information	O
science	O
and	O
statistics	O
series	O
editors	O
:	O
m.	O
jordan	O
j.	O
kleinberg	O
b.	O
scho¨lkopf	O
information	O
science	O
and	O
statistics	O
akaike	O
and	O
kitagawa	O
:	O
the	O
practice	O
of	O
time	O
series	O
analysis	O
.	O
bishop	O
:	O
pattern	O
recognition	O
and	O
machine	O
learning	O
.	O
cowell	O
,	O
dawid	O
,	O
lauritzen	O
,	O
and	O
spiegelhalter	O
:	O
probabilistic	O
networks	O
and	O
expert	O
systems	O
.	O
doucet	O
,	O
de	O
freitas	O
,	O
and	O
gordon	O
:	O
sequential	O
monte	O
carlo	O
methods	O
in	O
practice	O
.	O
fine	O
:	O
feedforward	O
neural	B
network	I
methodology	O
.	O
hawkins	O
and	O
olwell	O
:	O
cumulative	O
sum	O
charts	O
and	O
charting	O
for	O
quality	O
improvement	O
.	O
jensen	O
:	O
bayesian	O
networks	O
and	O
decision	O
graphs	O
.	O
marchette	O
:	O
computer	O
intrusion	O
detection	O
and	O
network	O
monitoring	O
:	O
a	O
statistical	O
viewpoint	O
.	O
rubinstein	O
and	O
kroese	O
:	O
the	O
cross-entropy	O
method	O
:	O
a	O
unified	O
approach	O
to	O
combinatorial	O
optimization	O
,	O
monte	O
carlo	O
simulation	O
,	O
and	O
machine	O
learning	O
.	O
studený	O
:	O
probabilistic	O
conditional	O
independence	O
structures	O
.	O
vapnik	O
:	O
the	O
nature	O
of	O
statistical	B
learning	I
theory	I
,	O
second	O
edition	O
.	O
wallace	O
:	O
statistical	O
and	O
inductive	O
inference	B
by	O
minimum	O
massage	O
length	O
.	O
christopher	O
m.	O
bishop	O
pattern	O
recognition	O
and	O
machine	O
learning	O
christopher	O
m.	O
bishop	O
f.r.eng	O
.	O
assistant	O
director	O
microsoft	O
research	O
ltd	O
cambridge	O
cb3	O
0fb	O
,	O
u.k.	O
cmbishop	O
@	O
microsoft.com	O
http	O
:	O
//research.microsoft.com/⬃cmbishop	O
series	O
editors	O
michael	O
jordan	O
department	O
of	O
computer	O
science	O
and	O
department	O
of	O
statistics	O
university	O
of	O
california	O
,	O
berkeley	O
berkeley	O
,	O
ca	O
94720	O
usa	O
professor	O
jon	O
kleinberg	O
department	O
of	O
computer	O
science	O
cornell	O
university	O
ithaca	O
,	O
ny	O
14853	O
usa	O
bernhard	O
scho¨lkopf	O
max	O
planck	O
institute	O
for	O
biological	O
cybernetics	O
spemannstrasse	O
38	O
72076	O
tu¨bingen	O
germany	O
library	O
of	O
congress	O
control	O
number	O
:	O
2006922522	O
isbn-10	O
:	O
0-387-31073-8	O
isbn-13	O
:	O
978-0387-31073-2	O
printed	O
on	O
acid-free	O
paper	O
.	O
©	O
2006	O
springer	O
science+business	O
media	O
,	O
llc	O
all	O
rights	O
reserved	O
.	O
this	O
work	O
may	O
not	O
be	O
translated	O
or	O
copied	O
in	O
whole	O
or	O
in	O
part	O
without	O
the	O
written	O
permission	O
of	O
the	O
publisher	O
(	O
springer	O
science+business	O
media	O
,	O
llc	O
,	O
233	O
spring	O
street	O
,	O
new	O
york	O
,	O
ny	O
10013	O
,	O
usa	O
)	O
,	O
except	O
for	O
brief	O
excerpts	O
in	O
connection	O
with	O
reviews	O
or	O
scholarly	O
analysis	O
.	O
use	O
in	O
connection	O
with	O
any	O
form	O
of	O
information	O
storage	O
and	O
retrieval	O
,	O
electronic	O
adaptation	O
,	O
computer	O
software	O
,	O
or	O
by	O
similar	O
or	O
dissimilar	O
methodology	O
now	O
known	O
or	O
hereafter	O
developed	O
is	O
forbidden	O
.	O
the	O
use	O
in	O
this	O
publication	O
of	O
trade	O
names	O
,	O
trademarks	O
,	O
service	O
marks	O
,	O
and	O
similar	O
terms	O
,	O
even	O
if	O
they	O
are	O
not	O
identified	O
as	O
such	O
,	O
is	O
not	O
to	O
be	O
taken	O
as	O
an	O
expression	O
of	O
opinion	O
as	O
to	O
whether	O
or	O
not	O
they	O
are	O
subject	O
to	O
proprietary	O
rights	O
.	O
printed	O
in	O
singapore	O
.	O
(	O
kyo	O
)	O
9	O
8	O
7	O
6	O
5	O
4	O
3	O
2	O
1	O
springer.com	O
this	O
book	O
is	O
dedicated	O
to	O
my	O
family	O
:	O
jenna	O
,	O
mark	O
,	O
and	O
hugh	O
total	O
eclipse	O
of	O
the	O
sun	O
,	O
antalya	O
,	O
turkey	O
,	O
29	O
march	O
2006.	O
preface	O
pattern	O
recognition	O
has	O
its	O
origins	O
in	O
engineering	O
,	O
whereas	O
machine	O
learning	O
grew	O
out	O
of	O
computer	O
science	O
.	O
however	O
,	O
these	O
activities	O
can	O
be	O
viewed	O
as	O
two	O
facets	O
of	O
the	O
same	O
ﬁeld	O
,	O
and	O
together	O
they	O
have	O
undergone	O
substantial	O
development	O
over	O
the	O
past	O
ten	O
years	O
.	O
in	O
particular	O
,	O
bayesian	O
methods	O
have	O
grown	O
from	O
a	O
specialist	O
niche	O
to	O
become	O
mainstream	O
,	O
while	O
graphical	O
models	O
have	O
emerged	O
as	O
a	O
general	O
framework	O
for	O
describing	O
and	O
applying	O
probabilistic	O
models	O
.	O
also	O
,	O
the	O
practical	O
applicability	O
of	O
bayesian	O
methods	O
has	O
been	O
greatly	O
enhanced	O
through	O
the	O
development	O
of	O
a	O
range	O
of	O
approximate	O
inference	B
algorithms	O
such	O
as	O
variational	B
bayes	O
and	O
expectation	B
propa-	O
gation	O
.	O
similarly	O
,	O
new	O
models	O
based	O
on	O
kernels	O
have	O
had	O
signiﬁcant	O
impact	O
on	O
both	O
algorithms	O
and	O
applications	O
.	O
this	O
new	O
textbook	O
reﬂects	O
these	O
recent	O
developments	O
while	O
providing	O
a	O
compre-	O
hensive	O
introduction	O
to	O
the	O
ﬁelds	O
of	O
pattern	O
recognition	O
and	O
machine	O
learning	O
.	O
it	O
is	O
aimed	O
at	O
advanced	O
undergraduates	O
or	O
ﬁrst	O
year	O
phd	O
students	O
,	O
as	O
well	O
as	O
researchers	O
and	O
practitioners	O
,	O
and	O
assumes	O
no	O
previous	O
knowledge	O
of	O
pattern	O
recognition	O
or	O
ma-	O
chine	O
learning	B
concepts	O
.	O
knowledge	O
of	O
multivariate	O
calculus	O
and	O
basic	O
linear	O
algebra	O
is	O
required	O
,	O
and	O
some	O
familiarity	O
with	O
probabilities	O
would	O
be	O
helpful	O
though	O
not	O
es-	O
sential	O
as	O
the	O
book	O
includes	O
a	O
self-contained	O
introduction	O
to	O
basic	O
probability	B
theory	O
.	O
because	O
this	O
book	O
has	O
broad	O
scope	O
,	O
it	O
is	O
impossible	O
to	O
provide	O
a	O
complete	O
list	O
of	O
references	O
,	O
and	O
in	O
particular	O
no	O
attempt	O
has	O
been	O
made	O
to	O
provide	O
accurate	O
historical	O
attribution	O
of	O
ideas	O
.	O
instead	O
,	O
the	O
aim	O
has	O
been	O
to	O
give	O
references	O
that	O
offer	O
greater	O
detail	O
than	O
is	O
possible	O
here	O
and	O
that	O
hopefully	O
provide	O
entry	O
points	O
into	O
what	O
,	O
in	O
some	O
cases	O
,	O
is	O
a	O
very	O
extensive	O
literature	O
.	O
for	O
this	O
reason	O
,	O
the	O
references	O
are	O
often	O
to	O
more	O
recent	O
textbooks	O
and	O
review	O
articles	O
rather	O
than	O
to	O
original	O
sources	O
.	O
the	O
book	O
is	O
supported	O
by	O
a	O
great	O
deal	O
of	O
additional	O
material	O
,	O
including	O
lecture	O
slides	O
as	O
well	O
as	O
the	O
complete	O
set	O
of	O
ﬁgures	O
used	O
in	O
the	O
book	O
,	O
and	O
the	O
reader	O
is	O
encouraged	O
to	O
visit	O
the	O
book	O
web	O
site	O
for	O
the	O
latest	O
information	O
:	O
http	O
:	O
//research.microsoft.com/∼cmbishop/prml	O
vii	O
viii	O
preface	O
exercises	O
the	O
exercises	O
that	O
appear	O
at	O
the	O
end	O
of	O
every	O
chapter	O
form	O
an	O
important	O
com-	O
ponent	O
of	O
the	O
book	O
.	O
each	O
exercise	O
has	O
been	O
carefully	O
chosen	O
to	O
reinforce	O
concepts	O
explained	O
in	O
the	O
text	O
or	O
to	O
develop	O
and	O
generalize	O
them	O
in	O
signiﬁcant	O
ways	O
,	O
and	O
each	O
is	O
graded	O
according	O
to	O
difﬁculty	O
ranging	O
from	O
(	O
(	O
cid:1	O
)	O
)	O
,	O
which	O
denotes	O
a	O
simple	O
exercise	O
taking	O
a	O
few	O
minutes	O
to	O
complete	O
,	O
through	O
to	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
,	O
which	O
denotes	O
a	O
signiﬁcantly	O
more	O
complex	O
exercise	O
.	O
it	O
has	O
been	O
difﬁcult	O
to	O
know	O
to	O
what	O
extent	O
these	O
solutions	O
should	O
be	O
made	O
widely	O
available	O
.	O
those	O
engaged	O
in	O
self	O
study	O
will	O
ﬁnd	O
worked	O
solutions	O
very	O
ben-	O
eﬁcial	O
,	O
whereas	O
many	O
course	O
tutors	O
request	O
that	O
solutions	O
be	O
available	O
only	O
via	O
the	O
publisher	O
so	O
that	O
the	O
exercises	O
may	O
be	O
used	O
in	O
class	O
.	O
in	O
order	O
to	O
try	O
to	O
meet	O
these	O
conﬂicting	O
requirements	O
,	O
those	O
exercises	O
that	O
help	O
amplify	O
key	O
points	O
in	O
the	O
text	O
,	O
or	O
that	O
ﬁll	O
in	O
important	O
details	O
,	O
have	O
solutions	O
that	O
are	O
available	O
as	O
a	O
pdf	O
ﬁle	O
from	O
the	O
book	O
web	O
site	O
.	O
such	O
exercises	O
are	O
denoted	O
by	O
www	O
.	O
solutions	O
for	O
the	O
remaining	O
exercises	O
are	O
available	O
to	O
course	O
tutors	O
by	O
contacting	O
the	O
publisher	O
(	O
contact	O
details	O
are	O
given	O
on	O
the	O
book	O
web	O
site	O
)	O
.	O
readers	O
are	O
strongly	O
encouraged	O
to	O
work	O
through	O
the	O
exercises	O
unaided	O
,	O
and	O
to	O
turn	O
to	O
the	O
solutions	O
only	O
as	O
required	O
.	O
although	O
this	O
book	O
focuses	O
on	O
concepts	O
and	O
principles	O
,	O
in	O
a	O
taught	O
course	O
the	O
students	O
should	O
ideally	O
have	O
the	O
opportunity	O
to	O
experiment	O
with	O
some	O
of	O
the	O
key	O
algorithms	O
using	O
appropriate	O
data	O
sets	O
.	O
a	O
companion	O
volume	O
(	O
bishop	O
and	O
nabney	O
,	O
2008	O
)	O
will	O
deal	O
with	O
practical	O
aspects	O
of	O
pattern	O
recognition	O
and	O
machine	O
learning	O
,	O
and	O
will	O
be	O
accompanied	O
by	O
matlab	O
software	O
implementing	O
most	O
of	O
the	O
algorithms	O
discussed	O
in	O
this	O
book	O
.	O
acknowledgements	O
first	O
of	O
all	O
i	O
would	O
like	O
to	O
express	O
my	O
sincere	O
thanks	O
to	O
markus	O
svens´en	O
who	O
has	O
provided	O
immense	O
help	O
with	O
preparation	O
of	O
ﬁgures	O
and	O
with	O
the	O
typesetting	O
of	O
the	O
book	O
in	O
latex	O
.	O
his	O
assistance	O
has	O
been	O
invaluable	O
.	O
i	O
am	O
very	O
grateful	O
to	O
microsoft	O
research	O
for	O
providing	O
a	O
highly	O
stimulating	O
re-	O
search	O
environment	O
and	O
for	O
giving	O
me	O
the	O
freedom	O
to	O
write	O
this	O
book	O
(	O
the	O
views	O
and	O
opinions	O
expressed	O
in	O
this	O
book	O
,	O
however	O
,	O
are	O
my	O
own	O
and	O
are	O
therefore	O
not	O
neces-	O
sarily	O
the	O
same	O
as	O
those	O
of	O
microsoft	O
or	O
its	O
afﬁliates	O
)	O
.	O
springer	O
has	O
provided	O
excellent	O
support	O
throughout	O
the	O
ﬁnal	O
stages	O
of	O
prepara-	O
tion	O
of	O
this	O
book	O
,	O
and	O
i	O
would	O
like	O
to	O
thank	O
my	O
commissioning	O
editor	O
john	O
kimmel	O
for	O
his	O
support	O
and	O
professionalism	O
,	O
as	O
well	O
as	O
joseph	O
piliero	O
for	O
his	O
help	O
in	O
design-	O
ing	O
the	O
cover	O
and	O
the	O
text	O
format	O
and	O
maryann	O
brickner	O
for	O
her	O
numerous	O
contribu-	O
tions	O
during	O
the	O
production	O
phase	O
.	O
the	O
inspiration	O
for	O
the	O
cover	O
design	O
came	O
from	O
a	O
discussion	O
with	O
antonio	O
criminisi	O
.	O
i	O
also	O
wish	O
to	O
thank	O
oxford	O
university	O
press	O
for	O
permission	O
to	O
reproduce	O
ex-	O
cerpts	O
from	O
an	O
earlier	O
textbook	O
,	O
neural	O
networks	O
for	O
pattern	O
recognition	O
(	O
bishop	O
,	O
1995a	O
)	O
.	O
the	O
images	O
of	O
the	O
mark	O
1	O
perceptron	B
and	O
of	O
frank	O
rosenblatt	O
are	O
repro-	O
duced	O
with	O
the	O
permission	O
of	O
arvin	O
calspan	O
advanced	O
technology	O
center	O
.	O
i	O
would	O
also	O
like	O
to	O
thank	O
asela	O
gunawardana	O
for	O
plotting	O
the	O
spectrogram	B
in	O
figure	O
13.1	O
,	O
and	O
bernhard	O
sch¨olkopf	O
for	O
permission	O
to	O
use	O
his	O
kernel	O
pca	O
code	O
to	O
plot	O
fig-	O
ure	O
12.17.	O
preface	O
ix	O
many	O
people	O
have	O
helped	O
by	O
proofreading	O
draft	O
material	O
and	O
providing	O
com-	O
ments	O
and	O
suggestions	O
,	O
including	O
shivani	O
agarwal	O
,	O
c´edric	O
archambeau	O
,	O
arik	O
azran	O
,	O
andrew	O
blake	O
,	O
hakan	O
cevikalp	O
,	O
michael	O
fourman	O
,	O
brendan	O
frey	O
,	O
zoubin	O
ghahra-	O
mani	O
,	O
thore	O
graepel	O
,	O
katherine	O
heller	O
,	O
ralf	O
herbrich	O
,	O
geoffrey	O
hinton	O
,	O
adam	O
jo-	O
hansen	O
,	O
matthew	O
johnson	O
,	O
michael	O
jordan	O
,	O
eva	O
kalyvianaki	O
,	O
anitha	O
kannan	O
,	O
julia	O
lasserre	O
,	O
david	O
liu	O
,	O
tom	O
minka	O
,	O
ian	O
nabney	O
,	O
tonatiuh	O
pena	O
,	O
yuan	O
qi	O
,	O
sam	O
roweis	O
,	O
balaji	O
sanjiya	O
,	O
toby	O
sharp	O
,	O
ana	O
costa	O
e	O
silva	O
,	O
david	O
spiegelhalter	O
,	O
jay	O
stokes	O
,	O
tara	O
symeonides	O
,	O
martin	O
szummer	O
,	O
marshall	O
tappen	O
,	O
ilkay	O
ulusoy	O
,	O
chris	O
williams	O
,	O
john	O
winn	O
,	O
and	O
andrew	O
zisserman	O
.	O
finally	O
,	O
i	O
would	O
like	O
to	O
thank	O
my	O
wife	O
jenna	O
who	O
has	O
been	O
hugely	O
supportive	O
throughout	O
the	O
several	O
years	O
it	O
has	O
taken	O
to	O
write	O
this	O
book	O
.	O
chris	O
bishop	O
cambridge	O
february	O
2006	O
mathematical	O
notation	O
i	O
have	O
tried	O
to	O
keep	O
the	O
mathematical	O
content	O
of	O
the	O
book	O
to	O
the	O
minimum	O
neces-	O
sary	O
to	O
achieve	O
a	O
proper	O
understanding	O
of	O
the	O
ﬁeld	O
.	O
however	O
,	O
this	O
minimum	O
level	O
is	O
nonzero	O
,	O
and	O
it	O
should	O
be	O
emphasized	O
that	O
a	O
good	O
grasp	O
of	O
calculus	O
,	O
linear	O
algebra	O
,	O
and	O
probability	B
theory	O
is	O
essential	O
for	O
a	O
clear	O
understanding	O
of	O
modern	O
pattern	O
recog-	O
nition	O
and	O
machine	O
learning	O
techniques	O
.	O
nevertheless	O
,	O
the	O
emphasis	O
in	O
this	O
book	O
is	O
on	O
conveying	O
the	O
underlying	O
concepts	O
rather	O
than	O
on	O
mathematical	O
rigour	O
.	O
i	O
have	O
tried	O
to	O
use	O
a	O
consistent	B
notation	O
throughout	O
the	O
book	O
,	O
although	O
at	O
times	O
this	O
means	O
departing	O
from	O
some	O
of	O
the	O
conventions	O
used	O
in	O
the	O
corresponding	O
re-	O
search	O
literature	O
.	O
vectors	O
are	O
denoted	O
by	O
lower	O
case	O
bold	O
roman	O
letters	O
such	O
as	O
x	O
,	O
and	O
all	O
vectors	O
are	O
assumed	O
to	O
be	O
column	O
vectors	O
.	O
a	O
superscript	O
t	O
denotes	O
the	O
transpose	O
of	O
a	O
matrix	O
or	O
vector	O
,	O
so	O
that	O
xt	O
will	O
be	O
a	O
row	O
vector	O
.	O
uppercase	O
bold	O
roman	O
letters	O
,	O
such	O
as	O
m	O
,	O
denote	O
matrices	O
.	O
the	O
notation	O
(	O
w1	O
,	O
.	O
.	O
.	O
,	O
wm	O
)	O
denotes	O
a	O
row	O
vector	O
with	O
m	O
elements	O
,	O
while	O
the	O
corresponding	O
column	O
vector	O
is	O
written	O
as	O
w	O
=	O
(	O
w1	O
,	O
.	O
.	O
.	O
,	O
wm	O
)	O
t.	O
the	O
notation	O
[	O
a	O
,	O
b	O
]	O
is	O
used	O
to	O
denote	O
the	O
closed	O
interval	O
from	O
a	O
to	O
b	O
,	O
that	O
is	O
the	O
interval	O
including	O
the	O
values	O
a	O
and	O
b	O
themselves	O
,	O
while	O
(	O
a	O
,	O
b	O
)	O
denotes	O
the	O
correspond-	O
ing	O
open	O
interval	O
,	O
that	O
is	O
the	O
interval	O
excluding	O
a	O
and	O
b.	O
similarly	O
,	O
[	O
a	O
,	O
b	O
)	O
denotes	O
an	O
interval	O
that	O
includes	O
a	O
but	O
excludes	O
b.	O
for	O
the	O
most	O
part	O
,	O
however	O
,	O
there	O
will	O
be	O
little	O
need	O
to	O
dwell	O
on	O
such	O
reﬁnements	O
as	O
whether	O
the	O
end	O
points	O
of	O
an	O
interval	O
are	O
included	O
or	O
not	O
.	O
the	O
m	O
×	O
m	O
identity	O
matrix	O
(	O
also	O
known	O
as	O
the	O
unit	O
matrix	O
)	O
is	O
denoted	O
im	O
,	O
which	O
will	O
be	O
abbreviated	O
to	O
i	O
where	O
there	O
is	O
no	O
ambiguity	O
about	O
it	O
dimensionality	O
.	O
it	O
has	O
elements	O
iij	O
that	O
equal	O
1	O
if	O
i	O
=	O
j	O
and	O
0	O
if	O
i	O
(	O
cid:2	O
)	O
=	O
j.	O
functional	B
is	O
discussed	O
in	O
appendix	O
d.	O
a	O
functional	B
is	O
denoted	O
f	O
[	O
y	O
]	O
where	O
y	O
(	O
x	O
)	O
is	O
some	O
function	O
.	O
the	O
concept	O
of	O
a	O
the	O
notation	O
g	O
(	O
x	O
)	O
=	O
o	O
(	O
f	O
(	O
x	O
)	O
)	O
denotes	O
that	O
|f	O
(	O
x	O
)	O
/g	O
(	O
x	O
)	O
|	O
is	O
bounded	O
as	O
x	O
→	O
∞	O
.	O
for	O
instance	O
if	O
g	O
(	O
x	O
)	O
=	O
3x2	O
+	O
2	O
,	O
then	O
g	O
(	O
x	O
)	O
=	O
o	O
(	O
x2	O
)	O
.	O
the	O
expectation	B
of	O
a	O
function	O
f	O
(	O
x	O
,	O
y	O
)	O
with	O
respect	O
to	O
a	O
random	O
variable	O
x	O
is	O
de-	O
noted	O
by	O
ex	O
[	O
f	O
(	O
x	O
,	O
y	O
)	O
]	O
.	O
in	O
situations	O
where	O
there	O
is	O
no	O
ambiguity	O
as	O
to	O
which	O
variable	O
is	O
being	O
averaged	O
over	O
,	O
this	O
will	O
be	O
simpliﬁed	O
by	O
omitting	O
the	O
sufﬁx	O
,	O
for	O
instance	O
xi	O
xii	O
mathematical	O
notation	O
e	O
[	O
x	O
]	O
.	O
if	O
the	O
distribution	O
of	O
x	O
is	O
conditioned	O
on	O
another	O
variable	O
z	O
,	O
then	O
the	O
corre-	O
sponding	O
conditional	B
expectation	I
will	O
be	O
written	O
ex	O
[	O
f	O
(	O
x	O
)	O
|z	O
]	O
.	O
similarly	O
,	O
the	O
variance	B
is	O
denoted	O
var	O
[	O
f	O
(	O
x	O
)	O
]	O
,	O
and	O
for	O
vector	O
variables	O
the	O
covariance	B
is	O
written	O
cov	O
[	O
x	O
,	O
y	O
]	O
.	O
we	O
shall	O
also	O
use	O
cov	O
[	O
x	O
]	O
as	O
a	O
shorthand	O
notation	O
for	O
cov	O
[	O
x	O
,	O
x	O
]	O
.	O
the	O
concepts	O
of	O
expecta-	O
tions	O
and	O
covariances	O
are	O
introduced	O
in	O
section	O
1.2.2.	O
if	O
we	O
have	O
n	O
values	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
of	O
a	O
d-dimensional	O
vector	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
t	O
,	O
we	O
can	O
combine	O
the	O
observations	O
into	O
a	O
data	O
matrix	O
x	O
in	O
which	O
the	O
nth	O
row	O
of	O
x	O
corresponds	O
to	O
the	O
row	O
vector	O
xt	O
n.	O
thus	O
the	O
n	O
,	O
i	O
element	O
of	O
x	O
corresponds	O
to	O
the	O
ith	O
element	O
of	O
the	O
nth	O
observation	O
xn	O
.	O
for	O
the	O
case	O
of	O
one-dimensional	O
variables	O
we	O
shall	O
denote	O
such	O
a	O
matrix	O
by	O
x	O
,	O
which	O
is	O
a	O
column	O
vector	O
whose	O
nth	O
element	O
is	O
xn	O
.	O
note	O
that	O
x	O
(	O
which	O
has	O
dimensionality	O
n	O
)	O
uses	O
a	O
different	O
typeface	O
to	O
distinguish	O
it	O
from	O
x	O
(	O
which	O
has	O
dimensionality	O
d	O
)	O
.	O
contents	O
preface	O
mathematical	O
notation	O
contents	O
1	O
introduction	O
1.1	O
1.2	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
vii	O
xi	O
xiii	O
1	O
4	O
12	O
17	O
19	O
21	O
24	O
28	O
30	O
32	O
33	O
38	O
39	O
41	O
42	O
42	O
46	O
48	O
55	O
58	O
xiii	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
example	O
:	O
polynomial	O
curve	O
fitting	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
probability	B
theory	O
.	O
.	O
.	O
.	O
.	O
1.2.1	O
.	O
.	O
.	O
.	O
probability	B
densities	O
1.2.2	O
expectations	O
and	O
covariances	O
1.2.3	O
bayesian	O
probabilities	O
.	O
.	O
.	O
1.2.4	O
the	O
gaussian	O
distribution	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
1.2.5	O
curve	B
ﬁtting	I
re-visited	O
.	O
.	O
.	O
1.2.6	O
bayesian	O
curve	B
ﬁtting	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
1.3	O
model	B
selection	I
.	O
.	O
.	O
.	O
.	O
.	O
1.4	O
1.5	O
decision	B
theory	I
.	O
.	O
.	O
.	O
.	O
.	O
the	O
curse	B
of	I
dimensionality	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
1.5.1	O
minimizing	O
the	O
misclassiﬁcation	O
rate	O
1.5.2	O
minimizing	O
the	O
expected	O
loss	O
1.5.3	O
the	O
reject	B
option	I
.	O
.	O
.	O
.	O
.	O
.	O
1.5.4	O
inference	B
and	O
decision	O
.	O
.	O
.	O
1.5.5	O
loss	O
functions	O
for	B
regression	I
.	O
.	O
.	O
.	O
information	B
theory	I
.	O
.	O
.	O
.	O
.	O
1.6.1	O
relative	B
entropy	I
and	O
mutual	B
information	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
1.6	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
xiv	O
contents	O
2	O
probability	B
distributions	O
2.1	O
binary	O
variables	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.1.1	O
the	O
beta	B
distribution	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.2	O
multinomial	O
variables	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.3	O
2.4	O
sequential	B
estimation	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.2.1	O
the	O
dirichlet	O
distribution	O
.	O
.	O
.	O
.	O
.	O
.	O
the	O
gaussian	O
distribution	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.3.1	O
conditional	B
gaussian	O
distributions	O
.	O
.	O
.	O
.	O
2.3.2	O
marginal	B
gaussian	O
distributions	O
.	O
.	O
.	O
.	O
.	O
2.3.3	O
bayes	O
’	O
theorem	O
for	O
gaussian	O
variables	O
.	O
.	O
.	O
.	O
.	O
.	O
2.3.4	O
maximum	B
likelihood	I
for	O
the	O
gaussian	O
.	O
.	O
.	O
.	O
.	O
.	O
2.3.5	O
2.3.6	O
bayesian	O
inference	B
for	O
the	O
gaussian	O
.	O
.	O
.	O
student	O
’	O
s	O
t-distribution	O
.	O
.	O
.	O
2.3.7	O
2.3.8	O
periodic	O
variables	O
.	O
.	O
.	O
.	O
.	O
.	O
2.3.9	O
mixtures	O
of	O
gaussians	O
.	O
.	O
.	O
the	O
exponential	B
family	I
.	O
.	O
.	O
.	O
.	O
.	O
2.4.1	O
maximum	B
likelihood	I
and	O
sufﬁcient	B
statistics	I
2.4.2	O
conjugate	B
priors	O
2.4.3	O
noninformative	B
priors	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.5	O
nonparametric	B
methods	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
67	O
68	O
71	O
74	O
76	O
78	O
85	O
88	O
90	O
93	O
94	O
97	O
.	O
102	O
.	O
105	O
.	O
110	O
.	O
113	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
116	O
.	O
.	O
.	O
.	O
.	O
117	O
.	O
.	O
.	O
.	O
.	O
117	O
.	O
.	O
.	O
.	O
.	O
120	O
.	O
.	O
.	O
.	O
.	O
122	O
.	O
.	O
.	O
.	O
.	O
124	O
.	O
127	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.5.1	O
kernel	O
density	O
estimators	O
.	O
.	O
.	O
.	O
.	O
.	O
2.5.2	O
nearest-neighbour	B
methods	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
3	O
linear	O
models	O
for	B
regression	I
3.1	O
.	O
.	O
.	O
.	O
.	O
linear	O
basis	O
function	O
models	O
.	O
.	O
.	O
3.1.1	O
maximum	B
likelihood	I
and	O
least	O
squares	O
.	O
.	O
.	O
.	O
.	O
.	O
3.1.2	O
geometry	O
of	O
least	O
squares	O
3.1.3	O
3.1.4	O
regularized	B
least	I
squares	I
.	O
.	O
.	O
.	O
.	O
.	O
3.1.5	O
multiple	O
outputs	O
the	O
bias-variance	O
decomposition	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
sequential	B
learning	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
3.3.1	O
3.3.2	O
3.3.3	O
equivalent	B
kernel	I
.	O
.	O
.	O
3.2	O
3.3	O
bayesian	O
linear	B
regression	I
.	O
.	O
.	O
.	O
parameter	O
distribution	O
.	O
.	O
.	O
predictive	B
distribution	I
.	O
.	O
.	O
.	O
.	O
.	O
3.4	O
bayesian	O
model	B
comparison	I
.	O
.	O
.	O
.	O
the	O
evidence	B
approximation	I
.	O
.	O
.	O
3.5	O
3.5.1	O
evaluation	O
of	O
the	O
evidence	B
function	I
.	O
.	O
.	O
3.5.2	O
maximizing	O
the	O
evidence	B
function	I
.	O
.	O
.	O
.	O
3.5.3	O
effective	B
number	I
of	I
parameters	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
limitations	O
of	O
fixed	O
basis	O
functions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
3.6	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
137	O
.	O
138	O
.	O
140	O
.	O
.	O
.	O
.	O
.	O
143	O
.	O
.	O
.	O
.	O
.	O
143	O
.	O
.	O
.	O
.	O
.	O
144	O
.	O
.	O
.	O
.	O
.	O
146	O
.	O
.	O
.	O
.	O
.	O
147	O
.	O
.	O
.	O
.	O
.	O
152	O
.	O
152	O
.	O
.	O
.	O
.	O
.	O
156	O
.	O
.	O
.	O
.	O
.	O
159	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
161	O
.	O
.	O
.	O
.	O
.	O
165	O
.	O
.	O
.	O
.	O
.	O
166	O
.	O
.	O
.	O
.	O
.	O
168	O
.	O
.	O
.	O
.	O
.	O
170	O
.	O
.	O
.	O
.	O
.	O
172	O
.	O
173	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
4.2	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
fisher	O
’	O
s	O
linear	B
discriminant	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
fisher	O
’	O
s	O
discriminant	O
for	O
multiple	O
classes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
4	O
linear	O
models	O
for	O
classiﬁcation	O
4.1	O
discriminant	O
functions	O
.	O
.	O
.	O
.	O
.	O
.	O
4.1.1	O
two	O
classes	O
.	O
4.1.2	O
multiple	O
classes	O
.	O
.	O
.	O
4.1.3	O
least	O
squares	O
for	O
classiﬁcation	O
.	O
.	O
.	O
4.1.4	O
4.1.5	O
relation	O
to	O
least	O
squares	O
.	O
.	O
.	O
.	O
.	O
.	O
4.1.6	O
4.1.7	O
the	O
perceptron	B
algorithm	O
.	O
.	O
.	O
.	O
.	O
.	O
probabilistic	O
generative	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
4.2.1	O
continuous	O
inputs	O
4.2.2	O
maximum	B
likelihood	I
solution	O
.	O
.	O
.	O
.	O
.	O
.	O
4.2.3	O
discrete	O
features	O
.	O
.	O
.	O
.	O
.	O
.	O
4.2.4	O
exponential	B
family	I
.	O
.	O
.	O
.	O
.	O
probabilistic	O
discriminative	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
4.3.1	O
fixed	O
basis	O
functions	O
.	O
.	O
.	O
.	O
4.3.2	O
logistic	B
regression	I
.	O
.	O
.	O
.	O
.	O
4.3.3	O
.	O
.	O
.	O
4.3.4	O
multiclass	B
logistic	O
regression	B
.	O
.	O
.	O
.	O
.	O
.	O
4.3.5	O
4.3.6	O
canonical	O
link	O
functions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
the	O
laplace	O
approximation	O
.	O
.	O
.	O
.	O
4.4.1	O
model	B
comparison	I
and	O
bic	O
.	O
.	O
.	O
.	O
.	O
.	O
iterative	B
reweighted	I
least	I
squares	I
probit	O
regression	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
4.3	O
4.4	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
4.5	O
bayesian	O
logistic	B
regression	I
.	O
.	O
.	O
4.5.1	O
laplace	O
approximation	O
.	O
.	O
.	O
4.5.2	O
predictive	B
distribution	I
.	O
.	O
.	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
5	O
neural	O
networks	O
5.1	O
feed-forward	O
network	O
functions	O
5.1.1	O
weight-space	O
symmetries	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
5.2	O
network	O
training	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
parameter	O
optimization	O
.	O
.	O
.	O
5.2.1	O
5.2.2	O
local	B
quadratic	O
approximation	O
.	O
.	O
.	O
.	O
.	O
.	O
5.2.3	O
use	O
of	O
gradient	O
information	O
.	O
.	O
.	O
.	O
.	O
.	O
5.2.4	O
gradient	B
descent	I
optimization	O
.	O
.	O
.	O
.	O
.	O
.	O
error	B
backpropagation	I
.	O
.	O
.	O
5.3.1	O
evaluation	O
of	O
error-function	O
derivatives	O
.	O
.	O
.	O
.	O
.	O
5.3.2	O
a	O
simple	O
example	O
5.3.3	O
efﬁciency	O
of	O
backpropagation	B
.	O
.	O
.	O
.	O
.	O
.	O
5.3.4	O
the	O
jacobian	O
matrix	O
.	O
.	O
.	O
.	O
the	O
hessian	O
matrix	O
.	O
.	O
.	O
.	O
.	O
5.4.1	O
diagonal	B
approximation	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
5.4.2	O
outer	B
product	I
approximation	I
.	O
.	O
.	O
.	O
5.4.3	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
inverse	B
hessian	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
5.3	O
5.4	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
contents	O
xv	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
179	O
.	O
.	O
.	O
.	O
.	O
181	O
.	O
.	O
.	O
.	O
.	O
181	O
.	O
.	O
.	O
.	O
.	O
182	O
.	O
184	O
.	O
.	O
.	O
.	O
.	O
186	O
.	O
.	O
.	O
.	O
.	O
189	O
.	O
191	O
.	O
.	O
.	O
.	O
.	O
192	O
.	O
196	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
198	O
.	O
.	O
.	O
.	O
.	O
200	O
.	O
202	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
202	O
.	O
.	O
.	O
.	O
.	O
203	O
.	O
.	O
.	O
.	O
.	O
204	O
.	O
205	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
207	O
.	O
.	O
.	O
.	O
.	O
.	O
209	O
.	O
.	O
.	O
.	O
.	O
210	O
.	O
.	O
.	O
.	O
.	O
212	O
.	O
213	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
216	O
.	O
217	O
.	O
217	O
.	O
218	O
.	O
220	O
225	O
.	O
.	O
.	O
.	O
.	O
227	O
.	O
.	O
.	O
.	O
.	O
.	O
231	O
.	O
.	O
.	O
.	O
.	O
232	O
.	O
.	O
.	O
.	O
.	O
236	O
.	O
.	O
.	O
.	O
.	O
237	O
.	O
.	O
.	O
.	O
.	O
.	O
239	O
.	O
.	O
.	O
.	O
.	O
240	O
.	O
.	O
.	O
.	O
.	O
241	O
.	O
.	O
.	O
.	O
.	O
.	O
242	O
.	O
.	O
.	O
245	O
.	O
.	O
.	O
.	O
.	O
246	O
.	O
.	O
.	O
.	O
.	O
247	O
.	O
.	O
.	O
.	O
.	O
249	O
.	O
.	O
.	O
.	O
.	O
.	O
250	O
.	O
.	O
251	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
252	O
.	O
.	O
.	O
.	O
xvi	O
contents	O
.	O
.	O
.	O
.	O
252	O
.	O
.	O
.	O
.	O
.	O
253	O
.	O
.	O
.	O
.	O
.	O
254	O
.	O
.	O
.	O
.	O
.	O
256	O
.	O
.	O
.	O
.	O
.	O
.	O
257	O
.	O
.	O
.	O
259	O
.	O
.	O
.	O
261	O
.	O
.	O
.	O
.	O
.	O
263	O
.	O
.	O
.	O
.	O
.	O
265	O
.	O
.	O
.	O
.	O
.	O
267	O
.	O
269	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
272	O
.	O
.	O
.	O
.	O
.	O
277	O
.	O
.	O
.	O
.	O
.	O
278	O
.	O
.	O
.	O
.	O
.	O
280	O
.	O
281	O
.	O
284	O
291	O
.	O
.	O
.	O
.	O
.	O
293	O
.	O
.	O
.	O
.	O
.	O
294	O
.	O
299	O
.	O
.	O
.	O
.	O
.	O
301	O
.	O
.	O
.	O
.	O
.	O
303	O
.	O
.	O
.	O
304	O
.	O
.	O
.	O
306	O
.	O
.	O
.	O
311	O
.	O
.	O
.	O
312	O
.	O
.	O
.	O
313	O
.	O
.	O
.	O
.	O
.	O
315	O
.	O
.	O
.	O
.	O
.	O
319	O
.	O
320	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
finite	O
differences	O
.	O
.	O
.	O
.	O
.	O
.	O
5.4.4	O
5.4.5	O
exact	B
evaluation	I
of	O
the	O
hessian	O
.	O
.	O
.	O
.	O
.	O
5.4.6	O
fast	B
multiplication	I
by	O
the	O
hessian	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
5.5	O
regularization	B
in	O
neural	O
networks	O
.	O
.	O
.	O
.	O
.	O
invariances	O
.	O
.	O
5.5.1	O
consistent	B
gaussian	O
priors	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
5.5.2	O
early	B
stopping	I
.	O
.	O
.	O
5.5.3	O
.	O
.	O
.	O
5.5.4	O
tangent	B
propagation	I
.	O
.	O
.	O
.	O
5.5.5	O
training	B
with	O
transformed	O
data	O
.	O
.	O
.	O
.	O
.	O
.	O
5.5.6	O
convolutional	B
networks	O
.	O
.	O
.	O
5.5.7	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
soft	B
weight	I
sharing	I
.	O
.	O
.	O
.	O
.	O
5.6	O
mixture	O
density	O
networks	O
.	O
.	O
.	O
.	O
.	O
5.7	O
bayesian	O
neural	O
networks	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
5.7.1	O
posterior	O
parameter	O
distribution	O
.	O
.	O
.	O
.	O
.	O
5.7.2	O
hyperparameter	B
optimization	O
.	O
.	O
.	O
.	O
.	O
.	O
5.7.3	O
bayesian	O
neural	O
networks	O
for	O
classiﬁcation	O
.	O
.	O
.	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
6	O
kernel	O
methods	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
6.1	O
dual	O
representations	O
.	O
.	O
.	O
.	O
6.2	O
constructing	O
kernels	O
.	O
.	O
.	O
.	O
6.3	O
radial	B
basis	I
function	I
networks	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
6.3.1	O
nadaraya-watson	O
model	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
6.4	O
gaussian	O
processes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
6.4.1	O
linear	B
regression	I
revisited	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
6.4.2	O
gaussian	O
processes	O
for	B
regression	I
.	O
.	O
.	O
.	O
6.4.3	O
learning	B
the	O
hyperparameters	O
.	O
.	O
.	O
.	O
.	O
.	O
6.4.4	O
automatic	B
relevance	I
determination	I
.	O
.	O
.	O
6.4.5	O
gaussian	O
processes	O
for	O
classiﬁcation	O
.	O
.	O
.	O
6.4.6	O
laplace	O
approximation	O
.	O
.	O
.	O
6.4.7	O
connection	O
to	O
neural	O
networks	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
exercises	O
7	O
sparse	O
kernel	O
machines	O
7.1	O
maximum	B
margin	I
classiﬁers	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
7.1.1	O
overlapping	O
class	O
distributions	O
.	O
.	O
.	O
.	O
.	O
7.1.2	O
relation	O
to	O
logistic	O
regression	B
.	O
.	O
.	O
.	O
.	O
7.1.3	O
multiclass	B
svms	O
.	O
.	O
.	O
.	O
.	O
.	O
7.1.4	O
svms	O
for	B
regression	I
.	O
.	O
.	O
.	O
7.1.5	O
computational	B
learning	I
theory	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
7.2	O
relevance	B
vector	I
machines	O
.	O
.	O
.	O
.	O
7.2.1	O
rvm	O
for	B
regression	I
.	O
.	O
.	O
.	O
.	O
7.2.2	O
analysis	O
of	O
sparsity	B
.	O
.	O
.	O
.	O
.	O
7.2.3	O
rvm	O
for	O
classiﬁcation	O
.	O
.	O
.	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
325	O
.	O
326	O
.	O
.	O
.	O
.	O
.	O
.	O
331	O
.	O
.	O
.	O
.	O
.	O
.	O
336	O
.	O
338	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
339	O
.	O
.	O
.	O
.	O
.	O
344	O
.	O
.	O
.	O
.	O
.	O
345	O
.	O
345	O
.	O
.	O
.	O
.	O
.	O
349	O
.	O
.	O
.	O
.	O
.	O
353	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
357	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
contents	O
xvii	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
359	O
.	O
.	O
.	O
.	O
.	O
360	O
.	O
.	O
.	O
362	O
.	O
365	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
366	O
.	O
.	O
.	O
.	O
.	O
370	O
.	O
.	O
.	O
.	O
.	O
372	O
.	O
373	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
378	O
.	O
.	O
.	O
.	O
.	O
383	O
.	O
.	O
.	O
.	O
.	O
383	O
.	O
.	O
.	O
.	O
.	O
384	O
.	O
.	O
.	O
.	O
.	O
387	O
.	O
.	O
.	O
.	O
.	O
390	O
.	O
.	O
.	O
.	O
.	O
393	O
.	O
.	O
.	O
.	O
.	O
394	O
.	O
.	O
.	O
.	O
.	O
398	O
.	O
.	O
.	O
399	O
.	O
.	O
.	O
.	O
.	O
.	O
402	O
.	O
.	O
.	O
.	O
.	O
411	O
.	O
.	O
.	O
.	O
.	O
416	O
.	O
.	O
.	O
.	O
.	O
417	O
.	O
.	O
.	O
.	O
.	O
.	O
418	O
.	O
418	O
8	O
graphical	O
models	O
8.1	O
bayesian	O
networks	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
8.1.1	O
example	O
:	O
polynomial	O
regression	O
.	O
.	O
.	O
.	O
.	O
8.1.2	O
generative	O
models	O
.	O
.	O
.	O
.	O
.	O
8.1.3	O
discrete	O
variables	O
.	O
.	O
.	O
.	O
.	O
.	O
8.1.4	O
linear-gaussian	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
8.2	O
conditional	B
independence	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
8.2.1	O
three	O
example	O
graphs	O
8.2.2	O
d-separation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
8.3	O
markov	O
random	O
fields	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
8.4	O
.	O
.	O
.	O
.	O
.	O
.	O
inference	B
on	O
a	O
chain	O
.	O
8.3.1	O
conditional	B
independence	I
properties	O
.	O
.	O
.	O
factorization	B
properties	O
8.3.2	O
.	O
.	O
.	O
illustration	O
:	O
image	B
de-noising	I
.	O
.	O
.	O
.	O
.	O
.	O
8.3.3	O
8.3.4	O
relation	O
to	O
directed	O
graphs	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
inference	B
in	O
graphical	O
models	O
.	O
.	O
.	O
8.4.1	O
.	O
.	O
.	O
8.4.2	O
trees	O
8.4.3	O
8.4.4	O
the	O
sum-product	B
algorithm	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
8.4.5	O
the	O
max-sum	B
algorithm	I
.	O
.	O
.	O
.	O
.	O
.	O
8.4.6	O
exact	O
inference	O
in	O
general	O
graphs	O
8.4.7	O
loopy	B
belief	I
propagation	I
.	O
.	O
.	O
.	O
.	O
.	O
8.4.8	O
learning	B
the	O
graph	O
structure	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
factor	O
graphs	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
9	O
mixture	B
models	O
and	O
em	O
9.1	O
k-means	O
clustering	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
9.1.1	O
image	O
segmentation	O
and	O
compression	O
.	O
.	O
.	O
.	O
.	O
.	O
9.2	O
mixtures	O
of	O
gaussians	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
9.2.1	O
maximum	B
likelihood	I
.	O
.	O
.	O
.	O
9.2.2	O
em	O
for	O
gaussian	O
mixtures	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
9.3	O
an	O
alternative	O
view	O
of	O
em	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
9.3.1	O
gaussian	O
mixtures	O
revisited	O
.	O
.	O
.	O
.	O
.	O
.	O
9.3.2	O
relation	O
to	O
k-means	O
.	O
.	O
.	O
.	O
9.3.3	O
mixtures	O
of	O
bernoulli	O
distributions	O
.	O
.	O
.	O
.	O
9.3.4	O
em	O
for	O
bayesian	O
linear	B
regression	I
.	O
.	O
.	O
.	O
the	O
em	O
algorithm	O
in	O
general	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
9.4	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
423	O
.	O
.	O
.	O
.	O
.	O
424	O
.	O
.	O
.	O
.	O
.	O
.	O
428	O
.	O
.	O
.	O
.	O
.	O
430	O
.	O
.	O
.	O
.	O
.	O
432	O
.	O
.	O
.	O
.	O
.	O
435	O
.	O
439	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
441	O
.	O
.	O
.	O
.	O
.	O
443	O
.	O
.	O
.	O
.	O
.	O
444	O
.	O
.	O
.	O
.	O
.	O
448	O
.	O
.	O
.	O
.	O
.	O
450	O
.	O
455	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
10	O
approximate	O
inference	B
10.1	O
variational	B
inference	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
10.1.1	O
factorized	O
distributions	O
.	O
.	O
.	O
10.1.2	O
properties	O
of	O
factorized	O
approximations	O
.	O
.	O
.	O
.	O
.	O
10.1.3	O
example	O
:	O
the	O
univariate	O
gaussian	O
.	O
.	O
.	O
.	O
10.1.4	O
model	B
comparison	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
10.2	O
illustration	O
:	O
variational	B
mixture	O
of	O
gaussians	O
.	O
.	O
.	O
.	O
.	O
.	O
461	O
.	O
.	O
.	O
.	O
.	O
462	O
.	O
464	O
.	O
.	O
.	O
.	O
.	O
466	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
470	O
.	O
.	O
.	O
.	O
.	O
473	O
.	O
474	O
.	O
.	O
.	O
.	O
.	O
xviii	O
contents	O
10.2.1	O
variational	B
distribution	O
.	O
.	O
.	O
10.2.2	O
variational	B
lower	O
bound	O
.	O
.	O
.	O
.	O
.	O
.	O
10.2.3	O
predictive	O
density	O
.	O
.	O
.	O
.	O
.	O
.	O
10.2.4	O
determining	O
the	O
number	O
of	O
components	O
.	O
.	O
.	O
.	O
.	O
10.2.5	O
induced	O
factorizations	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
10.3	O
variational	B
linear	O
regression	B
.	O
.	O
.	O
10.3.1	O
variational	B
distribution	O
.	O
.	O
.	O
10.3.2	O
predictive	B
distribution	I
.	O
.	O
.	O
10.3.3	O
lower	B
bound	I
.	O
.	O
.	O
.	O
10.4	O
exponential	B
family	I
distributions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
10.4.1	O
variational	B
message	O
passing	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
10.5	O
local	B
variational	O
methods	O
.	O
.	O
.	O
.	O
.	O
10.6	O
variational	B
logistic	O
regression	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
10.6.1	O
variational	B
posterior	O
distribution	O
.	O
.	O
.	O
.	O
.	O
10.6.2	O
optimizing	O
the	O
variational	B
parameters	O
.	O
.	O
.	O
.	O
.	O
.	O
10.6.3	O
inference	B
of	O
hyperparameters	O
10.7	O
expectation	B
propagation	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
10.7.1	O
example	O
:	O
the	O
clutter	B
problem	I
.	O
.	O
.	O
.	O
.	O
.	O
10.7.2	O
expectation	B
propagation	I
on	O
graphs	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
475	O
.	O
.	O
.	O
.	O
.	O
481	O
.	O
.	O
.	O
.	O
.	O
482	O
.	O
483	O
.	O
.	O
.	O
.	O
.	O
.	O
485	O
.	O
.	O
.	O
.	O
.	O
486	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
486	O
.	O
488	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
489	O
.	O
.	O
.	O
.	O
.	O
490	O
.	O
.	O
.	O
.	O
.	O
.	O
491	O
.	O
493	O
.	O
498	O
.	O
.	O
.	O
498	O
.	O
.	O
.	O
.	O
.	O
.	O
500	O
.	O
502	O
.	O
.	O
.	O
.	O
.	O
.	O
505	O
.	O
.	O
.	O
.	O
.	O
511	O
.	O
.	O
.	O
.	O
.	O
513	O
.	O
517	O
523	O
.	O
526	O
.	O
.	O
.	O
.	O
.	O
526	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
528	O
.	O
.	O
.	O
.	O
.	O
530	O
.	O
.	O
.	O
.	O
.	O
532	O
.	O
.	O
.	O
.	O
.	O
534	O
.	O
.	O
.	O
.	O
.	O
536	O
.	O
537	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
539	O
.	O
.	O
.	O
.	O
.	O
541	O
.	O
.	O
.	O
.	O
.	O
542	O
.	O
.	O
.	O
.	O
.	O
546	O
.	O
.	O
.	O
.	O
.	O
548	O
.	O
.	O
.	O
.	O
.	O
548	O
.	O
.	O
.	O
.	O
.	O
552	O
.	O
.	O
.	O
.	O
.	O
554	O
.	O
556	O
559	O
.	O
.	O
.	O
.	O
.	O
561	O
.	O
.	O
.	O
.	O
.	O
561	O
.	O
.	O
.	O
.	O
.	O
563	O
.	O
.	O
.	O
.	O
.	O
565	O
.	O
.	O
.	O
.	O
.	O
569	O
11	O
sampling	B
methods	I
11.1	O
basic	O
sampling	O
algorithms	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
11.1.1	O
standard	O
distributions	O
11.1.2	O
rejection	B
sampling	I
.	O
.	O
.	O
.	O
.	O
11.1.3	O
adaptive	B
rejection	I
sampling	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
11.1.4	O
importance	B
sampling	I
.	O
.	O
.	O
.	O
11.1.5	O
sampling-importance-resampling	B
.	O
.	O
.	O
.	O
11.1.6	O
sampling	O
and	O
the	O
em	O
algorithm	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
11.2	O
markov	O
chain	O
monte	O
carlo	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
11.2.1	O
markov	O
chains	O
.	O
.	O
.	O
.	O
11.2.2	O
the	O
metropolis-hastings	O
algorithm	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
11.3	O
gibbs	O
sampling	O
.	O
.	O
.	O
.	O
.	O
.	O
11.4	O
slice	B
sampling	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
11.5	O
the	O
hybrid	O
monte	O
carlo	O
algorithm	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
11.5.1	O
dynamical	O
systems	O
.	O
.	O
.	O
.	O
.	O
11.5.2	O
hybrid	O
monte	O
carlo	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
11.6	O
estimating	O
the	O
partition	B
function	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
12	O
continuous	O
latent	O
variables	O
12.1	O
principal	B
component	I
analysis	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
12.1.1	O
maximum	O
variance	O
formulation	O
.	O
.	O
.	O
.	O
.	O
12.1.2	O
minimum-error	O
formulation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
12.1.3	O
applications	O
of	O
pca	O
.	O
.	O
.	O
.	O
12.1.4	O
pca	O
for	O
high-dimensional	O
data	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
contents	O
xix	O
12.2	O
probabilistic	O
pca	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
12.2.1	O
maximum	B
likelihood	I
pca	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
12.2.2	O
em	O
algorithm	O
for	O
pca	O
.	O
.	O
.	O
12.2.3	O
bayesian	O
pca	O
.	O
.	O
.	O
12.2.4	O
factor	B
analysis	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
12.3	O
kernel	O
pca	O
.	O
12.4	O
nonlinear	O
latent	B
variable	I
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
12.4.1	O
independent	B
component	I
analysis	I
.	O
.	O
.	O
.	O
.	O
12.4.2	O
autoassociative	O
neural	O
networks	O
.	O
.	O
.	O
.	O
.	O
12.4.3	O
modelling	O
nonlinear	O
manifolds	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
570	O
.	O
.	O
.	O
.	O
.	O
574	O
.	O
.	O
.	O
.	O
.	O
577	O
.	O
.	O
.	O
580	O
.	O
.	O
.	O
583	O
.	O
.	O
.	O
586	O
.	O
.	O
.	O
.	O
.	O
591	O
.	O
.	O
.	O
.	O
.	O
591	O
.	O
.	O
.	O
.	O
.	O
592	O
.	O
.	O
.	O
.	O
.	O
595	O
.	O
599	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
13	O
sequential	B
data	I
13.1	O
markov	O
models	O
.	O
.	O
13.2	O
hidden	O
markov	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
13.2.1	O
maximum	B
likelihood	I
for	O
the	O
hmm	O
.	O
.	O
.	O
13.2.2	O
the	O
forward-backward	B
algorithm	I
.	O
.	O
.	O
.	O
13.2.3	O
the	O
sum-product	B
algorithm	I
for	O
the	O
hmm	O
.	O
.	O
.	O
.	O
13.2.4	O
scaling	O
factors	O
13.2.5	O
the	O
viterbi	O
algorithm	O
.	O
.	O
.	O
.	O
13.2.6	O
extensions	O
of	O
the	O
hidden	O
markov	O
model	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
605	O
.	O
.	O
.	O
.	O
.	O
607	O
.	O
.	O
.	O
.	O
.	O
610	O
.	O
.	O
.	O
.	O
.	O
615	O
.	O
.	O
.	O
.	O
.	O
618	O
.	O
625	O
.	O
.	O
.	O
627	O
.	O
629	O
.	O
631	O
.	O
635	O
.	O
638	O
.	O
642	O
.	O
644	O
.	O
.	O
.	O
645	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
646	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
13.3	O
linear	O
dynamical	O
systems	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
13.3.1	O
inference	B
in	O
lds	O
.	O
.	O
.	O
.	O
.	O
.	O
13.3.2	O
learning	B
in	O
lds	O
.	O
.	O
.	O
13.3.3	O
extensions	O
of	O
lds	O
.	O
.	O
.	O
.	O
.	O
13.3.4	O
particle	O
ﬁlters	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
14	O
combining	B
models	I
14.1	O
bayesian	O
model	B
averaging	I
.	O
.	O
.	O
.	O
.	O
14.2	O
committees	O
.	O
.	O
.	O
.	O
.	O
.	O
14.3	O
boosting	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
14.3.1	O
minimizing	O
exponential	O
error	O
14.3.2	O
error	B
functions	O
for	O
boosting	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
14.4	O
tree-based	O
models	O
.	O
.	O
.	O
.	O
.	O
14.5	O
conditional	O
mixture	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
14.5.1	O
mixtures	O
of	O
linear	B
regression	I
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
14.5.2	O
mixtures	O
of	O
logistic	O
models	O
.	O
.	O
.	O
14.5.3	O
mixtures	O
of	O
experts	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
653	O
.	O
.	O
.	O
.	O
.	O
654	O
.	O
.	O
.	O
.	O
.	O
655	O
.	O
657	O
.	O
.	O
.	O
659	O
.	O
.	O
.	O
.	O
.	O
.	O
661	O
.	O
.	O
.	O
.	O
.	O
663	O
.	O
.	O
.	O
.	O
.	O
666	O
.	O
.	O
.	O
.	O
.	O
667	O
.	O
.	O
.	O
.	O
.	O
670	O
.	O
672	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
674	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
appendix	O
a	O
data	O
sets	O
appendix	O
b	O
probability	B
distributions	O
appendix	O
c	O
properties	O
of	O
matrices	O
677	O
685	O
695	O
xx	O
contents	O
appendix	O
d	O
calculus	B
of	I
variations	I
appendix	O
e	O
lagrange	O
multipliers	O
references	O
index	O
703	O
707	O
711	O
729	O
1	O
introduction	O
the	O
problem	O
of	O
searching	O
for	O
patterns	O
in	O
data	O
is	O
a	O
fundamental	O
one	O
and	O
has	O
a	O
long	O
and	O
successful	O
history	O
.	O
for	O
instance	O
,	O
the	O
extensive	O
astronomical	O
observations	O
of	O
tycho	O
brahe	O
in	O
the	O
16th	O
century	O
allowed	O
johannes	O
kepler	O
to	O
discover	O
the	O
empirical	O
laws	O
of	O
planetary	O
motion	O
,	O
which	O
in	O
turn	O
provided	O
a	O
springboard	O
for	O
the	O
development	O
of	O
clas-	O
sical	O
mechanics	O
.	O
similarly	O
,	O
the	O
discovery	O
of	O
regularities	O
in	O
atomic	O
spectra	O
played	O
a	O
key	O
role	O
in	O
the	O
development	O
and	O
veriﬁcation	O
of	O
quantum	O
physics	O
in	O
the	O
early	O
twenti-	O
eth	O
century	O
.	O
the	O
ﬁeld	O
of	O
pattern	O
recognition	O
is	O
concerned	O
with	O
the	O
automatic	O
discov-	O
ery	O
of	O
regularities	O
in	O
data	O
through	O
the	O
use	O
of	O
computer	O
algorithms	O
and	O
with	O
the	O
use	O
of	O
these	O
regularities	O
to	O
take	O
actions	O
such	O
as	O
classifying	O
the	O
data	O
into	O
different	O
categories	O
.	O
consider	O
the	O
example	O
of	O
recognizing	O
handwritten	O
digits	O
,	O
illustrated	O
in	O
figure	O
1.1.	O
each	O
digit	O
corresponds	O
to	O
a	O
28×28	O
pixel	O
image	O
and	O
so	O
can	O
be	O
represented	O
by	O
a	O
vector	O
x	O
comprising	O
784	O
real	O
numbers	O
.	O
the	O
goal	O
is	O
to	O
build	O
a	O
machine	O
that	O
will	O
take	O
such	O
a	O
vector	O
x	O
as	O
input	O
and	O
that	O
will	O
produce	O
the	O
identity	O
of	O
the	O
digit	O
0	O
,	O
.	O
.	O
.	O
,	O
9	O
as	O
the	O
output	O
.	O
this	O
is	O
a	O
nontrivial	O
problem	O
due	O
to	O
the	O
wide	O
variability	O
of	O
handwriting	O
.	O
it	O
could	O
be	O
1	O
2	O
1.	O
introduction	O
figure	O
1.1	O
examples	O
of	O
hand-written	O
dig-	O
its	O
taken	O
from	O
us	O
zip	O
codes	O
.	O
tackled	O
using	O
handcrafted	O
rules	O
or	O
heuristics	O
for	O
distinguishing	O
the	O
digits	O
based	O
on	O
the	O
shapes	O
of	O
the	O
strokes	O
,	O
but	O
in	O
practice	O
such	O
an	O
approach	O
leads	O
to	O
a	O
proliferation	O
of	O
rules	O
and	O
of	O
exceptions	O
to	O
the	O
rules	O
and	O
so	O
on	O
,	O
and	O
invariably	O
gives	O
poor	O
results	O
.	O
far	O
better	O
results	O
can	O
be	O
obtained	O
by	O
adopting	O
a	O
machine	O
learning	O
approach	O
in	O
which	O
a	O
large	O
set	O
of	O
n	O
digits	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
called	O
a	O
training	B
set	I
is	O
used	O
to	O
tune	O
the	O
parameters	O
of	O
an	O
adaptive	O
model	O
.	O
the	O
categories	O
of	O
the	O
digits	O
in	O
the	O
training	B
set	I
are	O
known	O
in	O
advance	O
,	O
typically	O
by	O
inspecting	O
them	O
individually	O
and	O
hand-labelling	O
them	O
.	O
we	O
can	O
express	O
the	O
category	O
of	O
a	O
digit	O
using	O
target	B
vector	I
t	O
,	O
which	O
represents	O
the	O
identity	O
of	O
the	O
corresponding	O
digit	O
.	O
suitable	O
techniques	O
for	O
representing	O
cate-	O
gories	O
in	O
terms	O
of	O
vectors	O
will	O
be	O
discussed	O
later	O
.	O
note	O
that	O
there	O
is	O
one	O
such	O
target	B
vector	I
t	O
for	O
each	O
digit	O
image	O
x.	O
the	O
result	O
of	O
running	O
the	O
machine	O
learning	O
algorithm	O
can	O
be	O
expressed	O
as	O
a	O
function	O
y	O
(	O
x	O
)	O
which	O
takes	O
a	O
new	O
digit	O
image	O
x	O
as	O
input	O
and	O
that	O
generates	O
an	O
output	O
vector	O
y	O
,	O
encoded	O
in	O
the	O
same	O
way	O
as	O
the	O
target	O
vectors	O
.	O
the	O
precise	O
form	O
of	O
the	O
function	O
y	O
(	O
x	O
)	O
is	O
determined	O
during	O
the	O
training	B
phase	O
,	O
also	O
known	O
as	O
the	O
learning	B
phase	O
,	O
on	O
the	O
basis	O
of	O
the	O
training	B
data	O
.	O
once	O
the	O
model	O
is	O
trained	O
it	O
can	O
then	O
de-	O
termine	O
the	O
identity	O
of	O
new	O
digit	O
images	O
,	O
which	O
are	O
said	O
to	O
comprise	O
a	O
test	B
set	I
.	O
the	O
ability	O
to	O
categorize	O
correctly	O
new	O
examples	O
that	O
differ	O
from	O
those	O
used	O
for	O
train-	O
ing	O
is	O
known	O
as	O
generalization	B
.	O
in	O
practical	O
applications	O
,	O
the	O
variability	O
of	O
the	O
input	O
vectors	O
will	O
be	O
such	O
that	O
the	O
training	B
data	O
can	O
comprise	O
only	O
a	O
tiny	O
fraction	O
of	O
all	O
possible	O
input	O
vectors	O
,	O
and	O
so	O
generalization	B
is	O
a	O
central	O
goal	O
in	O
pattern	O
recognition	O
.	O
for	O
most	O
practical	O
applications	O
,	O
the	O
original	O
input	O
variables	O
are	O
typically	O
prepro-	O
cessed	O
to	O
transform	O
them	O
into	O
some	O
new	O
space	O
of	O
variables	O
where	O
,	O
it	O
is	O
hoped	O
,	O
the	O
pattern	O
recognition	O
problem	O
will	O
be	O
easier	O
to	O
solve	O
.	O
for	O
instance	O
,	O
in	O
the	O
digit	O
recogni-	O
tion	O
problem	O
,	O
the	O
images	O
of	O
the	O
digits	O
are	O
typically	O
translated	O
and	O
scaled	O
so	O
that	O
each	O
digit	O
is	O
contained	O
within	O
a	O
box	O
of	O
a	O
ﬁxed	O
size	O
.	O
this	O
greatly	O
reduces	O
the	O
variability	O
within	O
each	O
digit	O
class	O
,	O
because	O
the	O
location	O
and	O
scale	O
of	O
all	O
the	O
digits	O
are	O
now	O
the	O
same	O
,	O
which	O
makes	O
it	O
much	O
easier	O
for	O
a	O
subsequent	O
pattern	O
recognition	O
algorithm	O
to	O
distinguish	O
between	O
the	O
different	O
classes	O
.	O
this	O
pre-processing	O
stage	O
is	O
sometimes	O
also	O
called	O
feature	B
extraction	I
.	O
note	O
that	O
new	O
test	O
data	O
must	O
be	O
pre-processed	O
using	O
the	O
same	O
steps	O
as	O
the	O
training	B
data	O
.	O
pre-processing	O
might	O
also	O
be	O
performed	O
in	O
order	O
to	O
speed	O
up	O
computation	O
.	O
for	O
example	O
,	O
if	O
the	O
goal	O
is	O
real-time	O
face	B
detection	I
in	O
a	O
high-resolution	O
video	O
stream	O
,	O
the	O
computer	O
must	O
handle	O
huge	O
numbers	O
of	O
pixels	O
per	O
second	O
,	O
and	O
presenting	O
these	O
directly	O
to	O
a	O
complex	O
pattern	O
recognition	O
algorithm	O
may	O
be	O
computationally	O
infeasi-	O
ble	O
.	O
instead	O
,	O
the	O
aim	O
is	O
to	O
ﬁnd	O
useful	O
features	O
that	O
are	O
fast	O
to	O
compute	O
,	O
and	O
yet	O
that	O
1.	O
introduction	O
3	O
also	O
preserve	O
useful	O
discriminatory	O
information	O
enabling	O
faces	O
to	O
be	O
distinguished	O
from	O
non-faces	O
.	O
these	O
features	O
are	O
then	O
used	O
as	O
the	O
inputs	O
to	O
the	O
pattern	O
recognition	O
algorithm	O
.	O
for	O
instance	O
,	O
the	O
average	O
value	O
of	O
the	O
image	O
intensity	O
over	O
a	O
rectangular	O
subregion	O
can	O
be	O
evaluated	O
extremely	O
efﬁciently	O
(	O
viola	O
and	O
jones	O
,	O
2004	O
)	O
,	O
and	O
a	O
set	O
of	O
such	O
features	O
can	O
prove	O
very	O
effective	O
in	O
fast	O
face	O
detection	O
.	O
because	O
the	O
number	O
of	O
such	O
features	O
is	O
smaller	O
than	O
the	O
number	O
of	O
pixels	O
,	O
this	O
kind	O
of	O
pre-processing	O
repre-	O
sents	O
a	O
form	O
of	O
dimensionality	O
reduction	O
.	O
care	O
must	O
be	O
taken	O
during	O
pre-processing	O
because	O
often	O
information	O
is	O
discarded	O
,	O
and	O
if	O
this	O
information	O
is	O
important	O
to	O
the	O
solution	O
of	O
the	O
problem	O
then	O
the	O
overall	O
accuracy	O
of	O
the	O
system	O
can	O
suffer	O
.	O
applications	O
in	O
which	O
the	O
training	B
data	O
comprises	O
examples	O
of	O
the	O
input	O
vectors	O
along	O
with	O
their	O
corresponding	O
target	O
vectors	O
are	O
known	O
as	O
supervised	B
learning	I
prob-	O
lems	O
.	O
cases	O
such	O
as	O
the	O
digit	O
recognition	O
example	O
,	O
in	O
which	O
the	O
aim	O
is	O
to	O
assign	O
each	O
input	O
vector	O
to	O
one	O
of	O
a	O
ﬁnite	O
number	O
of	O
discrete	O
categories	O
,	O
are	O
called	O
classiﬁcation	B
problems	O
.	O
if	O
the	O
desired	O
output	O
consists	O
of	O
one	O
or	O
more	O
continuous	O
variables	O
,	O
then	O
the	O
task	O
is	O
called	O
regression	B
.	O
an	O
example	O
of	O
a	O
regression	B
problem	O
would	O
be	O
the	O
pre-	O
diction	O
of	O
the	O
yield	O
in	O
a	O
chemical	O
manufacturing	O
process	O
in	O
which	O
the	O
inputs	O
consist	O
of	O
the	O
concentrations	O
of	O
reactants	O
,	O
the	O
temperature	O
,	O
and	O
the	O
pressure	O
.	O
in	O
other	O
pattern	O
recognition	O
problems	O
,	O
the	O
training	B
data	O
consists	O
of	O
a	O
set	O
of	O
input	O
vectors	O
x	O
without	O
any	O
corresponding	O
target	O
values	O
.	O
the	O
goal	O
in	O
such	O
unsupervised	B
learning	I
problems	O
may	O
be	O
to	O
discover	O
groups	O
of	O
similar	O
examples	O
within	O
the	O
data	O
,	O
where	O
it	O
is	O
called	O
clustering	B
,	O
or	O
to	O
determine	O
the	O
distribution	O
of	O
data	O
within	O
the	O
input	O
space	O
,	O
known	O
as	O
density	B
estimation	I
,	O
or	O
to	O
project	O
the	O
data	O
from	O
a	O
high-dimensional	O
space	O
down	O
to	O
two	O
or	O
three	O
dimensions	O
for	O
the	O
purpose	O
of	O
visualization	B
.	O
finally	O
,	O
the	O
technique	O
of	O
reinforcement	B
learning	I
(	O
sutton	O
and	O
barto	O
,	O
1998	O
)	O
is	O
con-	O
cerned	O
with	O
the	O
problem	O
of	O
ﬁnding	O
suitable	O
actions	O
to	O
take	O
in	O
a	O
given	O
situation	O
in	O
order	O
to	O
maximize	O
a	O
reward	O
.	O
here	O
the	O
learning	B
algorithm	O
is	O
not	O
given	O
examples	O
of	O
optimal	O
outputs	O
,	O
in	O
contrast	O
to	O
supervised	B
learning	I
,	O
but	O
must	O
instead	O
discover	O
them	O
by	O
a	O
process	O
of	O
trial	O
and	O
error	B
.	O
typically	O
there	O
is	O
a	O
sequence	O
of	O
states	O
and	O
actions	O
in	O
which	O
the	O
learning	B
algorithm	O
is	O
interacting	O
with	O
its	O
environment	O
.	O
in	O
many	O
cases	O
,	O
the	O
current	O
action	O
not	O
only	O
affects	O
the	O
immediate	O
reward	O
but	O
also	O
has	O
an	O
impact	O
on	O
the	O
re-	O
ward	O
at	O
all	O
subsequent	O
time	O
steps	O
.	O
for	O
example	O
,	O
by	O
using	O
appropriate	O
reinforcement	B
learning	I
techniques	O
a	O
neural	B
network	I
can	O
learn	O
to	O
play	O
the	O
game	O
of	O
backgammon	B
to	O
a	O
high	O
standard	O
(	O
tesauro	O
,	O
1994	O
)	O
.	O
here	O
the	O
network	O
must	O
learn	O
to	O
take	O
a	O
board	O
position	O
as	O
input	O
,	O
along	O
with	O
the	O
result	O
of	O
a	O
dice	O
throw	O
,	O
and	O
produce	O
a	O
strong	O
move	O
as	O
the	O
output	O
.	O
this	O
is	O
done	O
by	O
having	O
the	O
network	O
play	O
against	O
a	O
copy	O
of	O
itself	O
for	O
perhaps	O
a	O
million	O
games	O
.	O
a	O
major	O
challenge	O
is	O
that	O
a	O
game	O
of	O
backgammon	B
can	O
involve	O
dozens	O
of	O
moves	O
,	O
and	O
yet	O
it	O
is	O
only	O
at	O
the	O
end	O
of	O
the	O
game	O
that	O
the	O
reward	O
,	O
in	O
the	O
form	O
of	O
victory	O
,	O
is	O
achieved	O
.	O
the	O
reward	O
must	O
then	O
be	O
attributed	O
appropriately	O
to	O
all	O
of	O
the	O
moves	O
that	O
led	O
to	O
it	O
,	O
even	O
though	O
some	O
moves	O
will	O
have	O
been	O
good	O
ones	O
and	O
others	O
less	O
so	O
.	O
this	O
is	O
an	O
example	O
of	O
a	O
credit	B
assignment	I
problem	O
.	O
a	O
general	O
feature	O
of	O
re-	O
inforcement	O
learning	B
is	O
the	O
trade-off	O
between	O
exploration	B
,	O
in	O
which	O
the	O
system	O
tries	O
out	O
new	O
kinds	O
of	O
actions	O
to	O
see	O
how	O
effective	O
they	O
are	O
,	O
and	O
exploitation	B
,	O
in	O
which	O
the	O
system	O
makes	O
use	O
of	O
actions	O
that	O
are	O
known	O
to	O
yield	O
a	O
high	O
reward	O
.	O
too	O
strong	O
a	O
focus	O
on	O
either	O
exploration	B
or	O
exploitation	B
will	O
yield	O
poor	O
results	O
.	O
reinforcement	B
learning	I
continues	O
to	O
be	O
an	O
active	O
area	O
of	O
machine	O
learning	O
research	O
.	O
however	O
,	O
a	O
4	O
1.	O
introduction	O
figure	O
1.2	O
plot	O
of	O
a	O
training	B
data	O
set	O
of	O
n	O
=	O
10	O
points	O
,	O
shown	O
as	O
blue	O
circles	O
,	O
each	O
comprising	O
an	O
observation	O
of	O
the	O
input	O
variable	O
x	O
along	O
with	O
the	O
corresponding	O
target	O
variable	O
t.	O
the	O
green	O
curve	O
shows	O
the	O
function	O
sin	O
(	O
2πx	O
)	O
used	O
to	O
gener-	O
ate	O
the	O
data	O
.	O
our	O
goal	O
is	O
to	O
pre-	O
dict	O
the	O
value	O
of	O
t	O
for	O
some	O
new	O
value	O
of	O
x	O
,	O
without	O
knowledge	O
of	O
the	O
green	O
curve	O
.	O
t	O
1	O
0	O
−1	O
0	O
x	O
1	O
detailed	O
treatment	O
lies	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
.	O
although	O
each	O
of	O
these	O
tasks	O
needs	O
its	O
own	O
tools	O
and	O
techniques	O
,	O
many	O
of	O
the	O
key	O
ideas	O
that	O
underpin	O
them	O
are	O
common	O
to	O
all	O
such	O
problems	O
.	O
one	O
of	O
the	O
main	O
goals	O
of	O
this	O
chapter	O
is	O
to	O
introduce	O
,	O
in	O
a	O
relatively	O
informal	O
way	O
,	O
several	O
of	O
the	O
most	O
important	O
of	O
these	O
concepts	O
and	O
to	O
illustrate	O
them	O
using	O
simple	O
examples	O
.	O
later	O
in	O
the	O
book	O
we	O
shall	O
see	O
these	O
same	O
ideas	O
re-emerge	O
in	O
the	O
context	O
of	O
more	O
sophisti-	O
cated	O
models	O
that	O
are	O
applicable	O
to	O
real-world	O
pattern	O
recognition	O
applications	O
.	O
this	O
chapter	O
also	O
provides	O
a	O
self-contained	O
introduction	O
to	O
three	O
important	O
tools	O
that	O
will	O
be	O
used	O
throughout	O
the	O
book	O
,	O
namely	O
probability	B
theory	O
,	O
decision	B
theory	I
,	O
and	O
infor-	O
mation	B
theory	O
.	O
although	O
these	O
might	O
sound	O
like	O
daunting	O
topics	O
,	O
they	O
are	O
in	O
fact	O
straightforward	O
,	O
and	O
a	O
clear	O
understanding	O
of	O
them	O
is	O
essential	O
if	O
machine	O
learning	O
techniques	O
are	O
to	O
be	O
used	O
to	O
best	O
effect	O
in	O
practical	O
applications	O
.	O
1.1.	O
example	O
:	O
polynomial	O
curve	O
fitting	O
we	O
begin	O
by	O
introducing	O
a	O
simple	O
regression	B
problem	O
,	O
which	O
we	O
shall	O
use	O
as	O
a	O
run-	O
ning	O
example	O
throughout	O
this	O
chapter	O
to	O
motivate	O
a	O
number	O
of	O
key	O
concepts	O
.	O
sup-	O
pose	O
we	O
observe	O
a	O
real-valued	O
input	O
variable	O
x	O
and	O
we	O
wish	O
to	O
use	O
this	O
observation	O
to	O
predict	O
the	O
value	O
of	O
a	O
real-valued	O
target	O
variable	O
t.	O
for	O
the	O
present	O
purposes	O
,	O
it	O
is	O
in-	O
structive	O
to	O
consider	O
an	O
artiﬁcial	O
example	O
using	O
synthetically	O
generated	O
data	O
because	O
we	O
then	O
know	O
the	O
precise	O
process	O
that	O
generated	O
the	O
data	O
for	O
comparison	O
against	O
any	O
learned	O
model	O
.	O
the	O
data	O
for	O
this	O
example	O
is	O
generated	O
from	O
the	O
function	O
sin	O
(	O
2πx	O
)	O
with	O
random	O
noise	O
included	O
in	O
the	O
target	O
values	O
,	O
as	O
described	O
in	O
detail	O
in	O
appendix	O
a.	O
now	O
suppose	O
that	O
we	O
are	O
given	O
a	O
training	B
set	I
comprising	O
n	O
observations	O
of	O
x	O
,	O
written	O
x	O
≡	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
t	O
,	O
together	O
with	O
corresponding	O
observations	O
of	O
the	O
values	O
of	O
t	O
,	O
denoted	O
t	O
≡	O
(	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
)	O
t.	O
figure	O
1.2	O
shows	O
a	O
plot	O
of	O
a	O
training	B
set	I
comprising	O
n	O
=	O
10	O
data	O
points	O
.	O
the	O
input	O
data	O
set	O
x	O
in	O
figure	O
1.2	O
was	O
generated	O
by	O
choos-	O
ing	O
values	O
of	O
xn	O
,	O
for	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
spaced	O
uniformly	O
in	O
range	O
[	O
0	O
,	O
1	O
]	O
,	O
and	O
the	O
target	O
data	O
set	O
t	O
was	O
obtained	O
by	O
ﬁrst	O
computing	O
the	O
corresponding	O
values	O
of	O
the	O
function	O
1.1.	O
example	O
:	O
polynomial	O
curve	O
fitting	O
5	O
sin	O
(	O
2πx	O
)	O
and	O
then	O
adding	O
a	O
small	O
level	O
of	O
random	O
noise	O
having	O
a	O
gaussian	O
distri-	O
bution	O
(	O
the	O
gaussian	O
distribution	O
is	O
discussed	O
in	O
section	O
1.2.4	O
)	O
to	O
each	O
such	O
point	O
in	O
order	O
to	O
obtain	O
the	O
corresponding	O
value	O
tn	O
.	O
by	O
generating	O
data	O
in	O
this	O
way	O
,	O
we	O
are	O
capturing	O
a	O
property	O
of	O
many	O
real	O
data	O
sets	O
,	O
namely	O
that	O
they	O
possess	O
an	O
underlying	O
regularity	O
,	O
which	O
we	O
wish	O
to	O
learn	O
,	O
but	O
that	O
individual	O
observations	O
are	O
corrupted	O
by	O
random	O
noise	O
.	O
this	O
noise	O
might	O
arise	O
from	O
intrinsically	O
stochastic	B
(	O
i.e	O
.	O
random	O
)	O
pro-	O
cesses	O
such	O
as	O
radioactive	O
decay	O
but	O
more	O
typically	O
is	O
due	O
to	O
there	O
being	O
sources	O
of	O
variability	O
that	O
are	O
themselves	O
unobserved	O
.	O
(	O
cid:1	O
)	O
t	O
of	O
the	O
target	O
variable	O
for	O
some	O
new	O
value	O
(	O
cid:1	O
)	O
x	O
of	O
the	O
input	O
variable	O
.	O
as	O
we	O
shall	O
see	O
set	O
.	O
furthermore	O
the	O
observed	O
data	O
are	O
corrupted	O
with	O
noise	O
,	O
and	O
so	O
for	O
a	O
given	O
(	O
cid:1	O
)	O
x	O
there	O
is	O
uncertainty	O
as	O
to	O
the	O
appropriate	O
value	O
for	O
(	O
cid:1	O
)	O
t.	O
probability	B
theory	O
,	O
discussed	O
later	O
,	O
this	O
involves	O
implicitly	O
trying	O
to	O
discover	O
the	O
underlying	O
function	O
sin	O
(	O
2πx	O
)	O
.	O
this	O
is	O
intrinsically	O
a	O
difﬁcult	O
problem	O
as	O
we	O
have	O
to	O
generalize	O
from	O
a	O
ﬁnite	O
data	O
our	O
goal	O
is	O
to	O
exploit	O
this	O
training	B
set	I
in	O
order	O
to	O
make	O
predictions	O
of	O
the	O
value	O
in	O
section	O
1.2	O
,	O
provides	O
a	O
framework	O
for	O
expressing	O
such	O
uncertainty	O
in	O
a	O
precise	O
and	O
quantitative	O
manner	O
,	O
and	O
decision	B
theory	I
,	O
discussed	O
in	O
section	O
1.5	O
,	O
allows	O
us	O
to	O
exploit	O
this	O
probabilistic	O
representation	O
in	O
order	O
to	O
make	O
predictions	O
that	O
are	O
optimal	O
according	O
to	O
appropriate	O
criteria	O
.	O
for	O
the	O
moment	O
,	O
however	O
,	O
we	O
shall	O
proceed	O
rather	O
informally	O
and	O
consider	O
a	O
simple	O
approach	O
based	O
on	O
curve	B
ﬁtting	I
.	O
in	O
particular	O
,	O
we	O
shall	O
ﬁt	O
the	O
data	O
using	O
a	O
polynomial	O
function	O
of	O
the	O
form	O
m	O
(	O
cid:2	O
)	O
y	O
(	O
x	O
,	O
w	O
)	O
=	O
w0	O
+	O
w1x	O
+	O
w2x2	O
+	O
.	O
.	O
.	O
+	O
wm	O
xm	O
=	O
wjxj	O
(	O
1.1	O
)	O
j=0	O
where	O
m	O
is	O
the	O
order	O
of	O
the	O
polynomial	O
,	O
and	O
xj	O
denotes	O
x	O
raised	O
to	O
the	O
power	O
of	O
j.	O
the	O
polynomial	O
coefﬁcients	O
w0	O
,	O
.	O
.	O
.	O
,	O
wm	O
are	O
collectively	O
denoted	O
by	O
the	O
vector	O
w.	O
note	O
that	O
,	O
although	O
the	O
polynomial	O
function	O
y	O
(	O
x	O
,	O
w	O
)	O
is	O
a	O
nonlinear	O
function	O
of	O
x	O
,	O
it	O
is	O
a	O
linear	O
function	O
of	O
the	O
coefﬁcients	O
w.	O
functions	O
,	O
such	O
as	O
the	O
polynomial	O
,	O
which	O
are	O
linear	O
in	O
the	O
unknown	O
parameters	O
have	O
important	O
properties	O
and	O
are	O
called	O
linear	O
models	O
and	O
will	O
be	O
discussed	O
extensively	O
in	O
chapters	O
3	O
and	O
4.	O
the	O
values	O
of	O
the	O
coefﬁcients	O
will	O
be	O
determined	O
by	O
ﬁtting	O
the	O
polynomial	O
to	O
the	O
training	B
data	O
.	O
this	O
can	O
be	O
done	O
by	O
minimizing	O
an	O
error	B
function	I
that	O
measures	O
the	O
misﬁt	O
between	O
the	O
function	O
y	O
(	O
x	O
,	O
w	O
)	O
,	O
for	O
any	O
given	O
value	O
of	O
w	O
,	O
and	O
the	O
training	B
set	I
data	O
points	O
.	O
one	O
simple	O
choice	O
of	O
error	B
function	I
,	O
which	O
is	O
widely	O
used	O
,	O
is	O
given	O
by	O
the	O
sum	O
of	O
the	O
squares	O
of	O
the	O
errors	O
between	O
the	O
predictions	O
y	O
(	O
xn	O
,	O
w	O
)	O
for	O
each	O
data	O
point	O
xn	O
and	O
the	O
corresponding	O
target	O
values	O
tn	O
,	O
so	O
that	O
we	O
minimize	O
e	O
(	O
w	O
)	O
=	O
1	O
2	O
{	O
y	O
(	O
xn	O
,	O
w	O
)	O
−	O
tn	O
}	O
2	O
(	O
1.2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
where	O
the	O
factor	O
of	O
1/2	O
is	O
included	O
for	O
later	O
convenience	O
.	O
we	O
shall	O
discuss	O
the	O
mo-	O
tivation	O
for	O
this	O
choice	O
of	O
error	B
function	I
later	O
in	O
this	O
chapter	O
.	O
for	O
the	O
moment	O
we	O
simply	O
note	O
that	O
it	O
is	O
a	O
nonnegative	O
quantity	O
that	O
would	O
be	O
zero	O
if	O
,	O
and	O
only	O
if	O
,	O
the	O
6	O
1.	O
introduction	O
figure	O
1.3	O
the	O
error	B
function	I
(	O
1.2	O
)	O
corre-	O
sponds	O
to	O
(	O
one	O
half	O
of	O
)	O
the	O
sum	O
of	O
the	O
squares	O
of	O
the	O
displacements	O
(	O
shown	O
by	O
the	O
vertical	O
green	O
bars	O
)	O
of	O
each	O
data	O
point	O
from	O
the	O
function	O
y	O
(	O
x	O
,	O
w	O
)	O
.	O
t	O
tn	O
y	O
(	O
xn	O
,	O
w	O
)	O
xn	O
x	O
exercise	O
1.1	O
function	O
y	O
(	O
x	O
,	O
w	O
)	O
were	O
to	O
pass	O
exactly	O
through	O
each	O
training	B
data	O
point	O
.	O
the	O
geomet-	O
rical	O
interpretation	O
of	O
the	O
sum-of-squares	B
error	I
function	O
is	O
illustrated	O
in	O
figure	O
1.3.	O
we	O
can	O
solve	O
the	O
curve	B
ﬁtting	I
problem	O
by	O
choosing	O
the	O
value	O
of	O
w	O
for	O
which	O
e	O
(	O
w	O
)	O
is	O
as	O
small	O
as	O
possible	O
.	O
because	O
the	O
error	B
function	I
is	O
a	O
quadratic	O
function	O
of	O
the	O
coefﬁcients	O
w	O
,	O
its	O
derivatives	O
with	O
respect	O
to	O
the	O
coefﬁcients	O
will	O
be	O
linear	O
in	O
the	O
elements	O
of	O
w	O
,	O
and	O
so	O
the	O
minimization	O
of	O
the	O
error	B
function	I
has	O
a	O
unique	O
solution	O
,	O
denoted	O
by	O
w	O
(	O
cid:1	O
)	O
,	O
which	O
can	O
be	O
found	O
in	O
closed	O
form	O
.	O
the	O
resulting	O
polynomial	O
is	O
given	O
by	O
the	O
function	O
y	O
(	O
x	O
,	O
w	O
(	O
cid:1	O
)	O
)	O
.	O
there	O
remains	O
the	O
problem	O
of	O
choosing	O
the	O
order	O
m	O
of	O
the	O
polynomial	O
,	O
and	O
as	O
we	O
shall	O
see	O
this	O
will	O
turn	O
out	O
to	O
be	O
an	O
example	O
of	O
an	O
important	O
concept	O
called	O
model	B
comparison	I
or	O
model	B
selection	I
.	O
in	O
figure	O
1.4	O
,	O
we	O
show	O
four	O
examples	O
of	O
the	O
results	O
of	O
ﬁtting	O
polynomials	O
having	O
orders	O
m	O
=	O
0	O
,	O
1	O
,	O
3	O
,	O
and	O
9	O
to	O
the	O
data	O
set	O
shown	O
in	O
figure	O
1.2.	O
we	O
notice	O
that	O
the	O
constant	O
(	O
m	O
=	O
0	O
)	O
and	O
ﬁrst	B
order	I
(	O
m	O
=	O
1	O
)	O
polynomials	O
give	O
rather	O
poor	O
ﬁts	O
to	O
the	O
data	O
and	O
consequently	O
rather	O
poor	O
representations	O
of	O
the	O
function	O
sin	O
(	O
2πx	O
)	O
.	O
the	O
third	O
order	O
(	O
m	O
=	O
3	O
)	O
polynomial	O
seems	O
to	O
give	O
the	O
best	O
ﬁt	O
to	O
the	O
function	O
sin	O
(	O
2πx	O
)	O
of	O
the	O
examples	O
shown	O
in	O
figure	O
1.4.	O
when	O
we	O
go	O
to	O
a	O
much	O
higher	O
order	O
polynomial	O
(	O
m	O
=	O
9	O
)	O
,	O
we	O
obtain	O
an	O
excellent	O
ﬁt	O
to	O
the	O
training	B
data	O
.	O
in	O
fact	O
,	O
the	O
polynomial	O
passes	O
exactly	O
through	O
each	O
data	O
point	O
and	O
e	O
(	O
w	O
(	O
cid:1	O
)	O
)	O
=	O
0.	O
however	O
,	O
the	O
ﬁtted	O
curve	O
oscillates	O
wildly	O
and	O
gives	O
a	O
very	O
poor	O
representation	O
of	O
the	O
function	O
sin	O
(	O
2πx	O
)	O
.	O
this	O
latter	O
behaviour	O
is	O
known	O
as	O
over-ﬁtting	B
.	O
as	O
we	O
have	O
noted	O
earlier	O
,	O
the	O
goal	O
is	O
to	O
achieve	O
good	O
generalization	B
by	O
making	O
accurate	O
predictions	O
for	O
new	O
data	O
.	O
we	O
can	O
obtain	O
some	O
quantitative	O
insight	O
into	O
the	O
dependence	O
of	O
the	O
generalization	B
performance	O
on	O
m	O
by	O
considering	O
a	O
separate	O
test	B
set	I
comprising	O
100	O
data	O
points	O
generated	O
using	O
exactly	O
the	O
same	O
procedure	O
used	O
to	O
generate	O
the	O
training	B
set	I
points	O
but	O
with	O
new	O
choices	O
for	O
the	O
random	O
noise	O
values	O
included	O
in	O
the	O
target	O
values	O
.	O
for	O
each	O
choice	O
of	O
m	O
,	O
we	O
can	O
then	O
evaluate	O
the	O
residual	O
value	O
of	O
e	O
(	O
w	O
(	O
cid:1	O
)	O
)	O
given	O
by	O
(	O
1.2	O
)	O
for	O
the	O
training	B
data	O
,	O
and	O
we	O
can	O
also	O
evaluate	O
e	O
(	O
w	O
(	O
cid:1	O
)	O
)	O
for	O
the	O
test	O
data	O
set	O
.	O
it	O
is	O
sometimes	O
more	O
convenient	O
to	O
use	O
the	O
root-mean-square	O
t	O
1	O
0	O
−1	O
t	O
1	O
0	O
−1	O
0	O
0	O
1.1.	O
example	O
:	O
polynomial	O
curve	O
fitting	O
7	O
m	O
=	O
1	O
x	O
1	O
m	O
=	O
9	O
m	O
=	O
0	O
t	O
1	O
0	O
−1	O
x	O
1	O
0	O
m	O
=	O
3	O
t	O
1	O
0	O
−1	O
x	O
1	O
0	O
x	O
1	O
figure	O
1.4	O
plots	O
of	O
polynomials	O
having	O
various	O
orders	O
m	O
,	O
shown	O
as	O
red	O
curves	O
,	O
ﬁtted	O
to	O
the	O
data	O
set	O
shown	O
in	O
figure	O
1.2	O
.	O
(	O
rms	O
)	O
error	B
deﬁned	O
by	O
erms	O
=	O
2e	O
(	O
w	O
(	O
cid:1	O
)	O
)	O
/n	O
(	O
cid:3	O
)	O
(	O
1.3	O
)	O
in	O
which	O
the	O
division	O
by	O
n	O
allows	O
us	O
to	O
compare	O
different	O
sizes	O
of	O
data	O
sets	O
on	O
an	O
equal	O
footing	O
,	O
and	O
the	O
square	O
root	O
ensures	O
that	O
erms	O
is	O
measured	O
on	O
the	O
same	O
scale	O
(	O
and	O
in	O
the	O
same	O
units	O
)	O
as	O
the	O
target	O
variable	O
t.	O
graphs	O
of	O
the	O
training	B
and	O
test	B
set	I
rms	O
errors	O
are	O
shown	O
,	O
for	O
various	O
values	O
of	O
m	O
,	O
in	O
figure	O
1.5.	O
the	O
test	B
set	I
error	O
is	O
a	O
measure	O
of	O
how	O
well	O
we	O
are	O
doing	O
in	O
predicting	O
the	O
values	O
of	O
t	O
for	O
new	O
data	O
observations	O
of	O
x.	O
we	O
note	O
from	O
figure	O
1.5	O
that	O
small	O
values	O
of	O
m	O
give	O
relatively	O
large	O
values	O
of	O
the	O
test	B
set	I
error	O
,	O
and	O
this	O
can	O
be	O
attributed	O
to	O
the	O
fact	O
that	O
the	O
corresponding	O
polynomials	O
are	O
rather	O
inﬂexible	O
and	O
are	O
incapable	O
of	O
capturing	O
the	O
oscillations	O
in	O
the	O
function	O
sin	O
(	O
2πx	O
)	O
.	O
values	O
of	O
m	O
in	O
the	O
range	O
3	O
(	O
cid:1	O
)	O
m	O
(	O
cid:1	O
)	O
8	O
give	O
small	O
values	O
for	O
the	O
test	B
set	I
error	O
,	O
and	O
these	O
also	O
give	O
reasonable	O
representations	O
of	O
the	O
generating	O
function	O
sin	O
(	O
2πx	O
)	O
,	O
as	O
can	O
be	O
seen	O
,	O
for	O
the	O
case	O
of	O
m	O
=	O
3	O
,	O
from	O
figure	O
1.4	O
.	O
8	O
1.	O
introduction	O
figure	O
1.5	O
graphs	O
of	O
the	O
root-mean-square	B
error	I
,	O
deﬁned	O
by	O
(	O
1.3	O
)	O
,	O
evaluated	O
on	O
the	O
training	B
set	I
and	O
on	O
an	O
inde-	O
pendent	O
test	B
set	I
for	O
various	O
values	O
of	O
m.	O
training	B
test	O
1	O
s	O
m	O
r	O
e	O
0.5	O
0	O
0	O
3	O
m	O
6	O
9	O
for	O
m	O
=	O
9	O
,	O
the	O
training	B
set	I
error	O
goes	O
to	O
zero	O
,	O
as	O
we	O
might	O
expect	O
because	O
this	O
polynomial	O
contains	O
10	O
degrees	B
of	I
freedom	I
corresponding	O
to	O
the	O
10	O
coefﬁcients	O
w0	O
,	O
.	O
.	O
.	O
,	O
w9	O
,	O
and	O
so	O
can	O
be	O
tuned	O
exactly	O
to	O
the	O
10	O
data	O
points	O
in	O
the	O
training	B
set	I
.	O
however	O
,	O
the	O
test	B
set	I
error	O
has	O
become	O
very	O
large	O
and	O
,	O
as	O
we	O
saw	O
in	O
figure	O
1.4	O
,	O
the	O
corresponding	O
function	O
y	O
(	O
x	O
,	O
w	O
(	O
cid:1	O
)	O
)	O
exhibits	O
wild	O
oscillations	O
.	O
this	O
may	O
seem	O
paradoxical	O
because	O
a	O
polynomial	O
of	O
given	O
order	O
contains	O
all	O
lower	O
order	O
polynomials	O
as	O
special	O
cases	O
.	O
the	O
m	O
=	O
9	O
polynomial	O
is	O
therefore	O
capa-	O
ble	O
of	O
generating	O
results	O
at	O
least	O
as	O
good	O
as	O
the	O
m	O
=	O
3	O
polynomial	O
.	O
furthermore	O
,	O
we	O
might	O
suppose	O
that	O
the	O
best	O
predictor	O
of	O
new	O
data	O
would	O
be	O
the	O
function	O
sin	O
(	O
2πx	O
)	O
from	O
which	O
the	O
data	O
was	O
generated	O
(	O
and	O
we	O
shall	O
see	O
later	O
that	O
this	O
is	O
indeed	O
the	O
case	O
)	O
.	O
we	O
know	O
that	O
a	O
power	O
series	O
expansion	O
of	O
the	O
function	O
sin	O
(	O
2πx	O
)	O
contains	O
terms	O
of	O
all	O
orders	O
,	O
so	O
we	O
might	O
expect	O
that	O
results	O
should	O
improve	O
monotonically	O
as	O
we	O
increase	O
m.	O
we	O
can	O
gain	O
some	O
insight	O
into	O
the	O
problem	O
by	O
examining	O
the	O
values	O
of	O
the	O
co-	O
efﬁcients	O
w	O
(	O
cid:1	O
)	O
obtained	O
from	O
polynomials	O
of	O
various	O
order	O
,	O
as	O
shown	O
in	O
table	O
1.1.	O
we	O
see	O
that	O
,	O
as	O
m	O
increases	O
,	O
the	O
magnitude	O
of	O
the	O
coefﬁcients	O
typically	O
gets	O
larger	O
.	O
in	O
particular	O
for	O
the	O
m	O
=	O
9	O
polynomial	O
,	O
the	O
coefﬁcients	O
have	O
become	O
ﬁnely	O
tuned	O
to	O
the	O
data	O
by	O
developing	O
large	O
positive	O
and	O
negative	O
values	O
so	O
that	O
the	O
correspond-	O
table	O
1.1	O
table	O
of	O
the	O
coefﬁcients	O
w	O
(	O
cid:1	O
)	O
for	O
polynomials	O
of	O
various	O
order	O
.	O
observe	O
how	O
the	O
typical	O
mag-	O
nitude	O
of	O
the	O
coefﬁcients	O
in-	O
creases	O
dramatically	O
as	O
the	O
or-	O
der	O
of	O
the	O
polynomial	O
increases	O
.	O
w	O
(	O
cid:1	O
)	O
0	O
w	O
(	O
cid:1	O
)	O
1	O
w	O
(	O
cid:1	O
)	O
2	O
w	O
(	O
cid:1	O
)	O
3	O
w	O
(	O
cid:1	O
)	O
4	O
w	O
(	O
cid:1	O
)	O
5	O
w	O
(	O
cid:1	O
)	O
6	O
w	O
(	O
cid:1	O
)	O
7	O
w	O
(	O
cid:1	O
)	O
8	O
w	O
(	O
cid:1	O
)	O
9	O
0.19	O
m	O
=	O
0	O
m	O
=	O
1	O
m	O
=	O
6	O
0.31	O
7.99	O
-25.43	O
17.37	O
0.82	O
-1.27	O
m	O
=	O
9	O
0.35	O
232.37	O
-5321.83	O
48568.31	O
-231639.30	O
640042.26	O
-1061800.52	O
1042400.18	O
-557682.99	O
125201.43	O
1.1.	O
example	O
:	O
polynomial	O
curve	O
fitting	O
9	O
t	O
1	O
0	O
−1	O
n	O
=	O
15	O
t	O
1	O
0	O
−1	O
n	O
=	O
100	O
0	O
x	O
1	O
0	O
x	O
1	O
figure	O
1.6	O
plots	O
of	O
the	O
solutions	O
obtained	O
by	O
minimizing	O
the	O
sum-of-squares	B
error	I
function	O
using	O
the	O
m	O
=	O
9	O
polynomial	O
for	O
n	O
=	O
15	O
data	O
points	O
(	O
left	O
plot	O
)	O
and	O
n	O
=	O
100	O
data	O
points	O
(	O
right	O
plot	O
)	O
.	O
we	O
see	O
that	O
increasing	O
the	O
size	O
of	O
the	O
data	O
set	O
reduces	O
the	O
over-ﬁtting	B
problem	O
.	O
ing	O
polynomial	O
function	O
matches	O
each	O
of	O
the	O
data	O
points	O
exactly	O
,	O
but	O
between	O
data	O
points	O
(	O
particularly	O
near	O
the	O
ends	O
of	O
the	O
range	O
)	O
the	O
function	O
exhibits	O
the	O
large	O
oscilla-	O
tions	O
observed	O
in	O
figure	O
1.4.	O
intuitively	O
,	O
what	O
is	O
happening	O
is	O
that	O
the	O
more	O
ﬂexible	O
polynomials	O
with	O
larger	O
values	O
of	O
m	O
are	O
becoming	O
increasingly	O
tuned	O
to	O
the	O
random	O
noise	O
on	O
the	O
target	O
values	O
.	O
it	O
is	O
also	O
interesting	O
to	O
examine	O
the	O
behaviour	O
of	O
a	O
given	O
model	O
as	O
the	O
size	O
of	O
the	O
data	O
set	O
is	O
varied	O
,	O
as	O
shown	O
in	O
figure	O
1.6.	O
we	O
see	O
that	O
,	O
for	O
a	O
given	O
model	O
complexity	O
,	O
the	O
over-ﬁtting	B
problem	O
become	O
less	O
severe	O
as	O
the	O
size	O
of	O
the	O
data	O
set	O
increases	O
.	O
another	O
way	O
to	O
say	O
this	O
is	O
that	O
the	O
larger	O
the	O
data	O
set	O
,	O
the	O
more	O
complex	O
(	O
in	O
other	O
words	O
more	O
ﬂexible	O
)	O
the	O
model	O
that	O
we	O
can	O
afford	O
to	O
ﬁt	O
to	O
the	O
data	O
.	O
one	O
rough	O
heuristic	O
that	O
is	O
sometimes	O
advocated	O
is	O
that	O
the	O
number	O
of	O
data	O
points	O
should	O
be	O
no	O
less	O
than	O
some	O
multiple	O
(	O
say	O
5	O
or	O
10	O
)	O
of	O
the	O
number	O
of	O
adaptive	O
parameters	O
in	O
the	O
model	O
.	O
however	O
,	O
as	O
we	O
shall	O
see	O
in	O
chapter	O
3	O
,	O
the	O
number	O
of	O
parameters	O
is	O
not	O
necessarily	O
the	O
most	O
appropriate	O
measure	O
of	O
model	O
complexity	O
.	O
also	O
,	O
there	O
is	O
something	O
rather	O
unsatisfying	O
about	O
having	O
to	O
limit	O
the	O
number	O
of	O
parameters	O
in	O
a	O
model	O
according	O
to	O
the	O
size	O
of	O
the	O
available	O
training	B
set	I
.	O
it	O
would	O
seem	O
more	O
reasonable	O
to	O
choose	O
the	O
complexity	O
of	O
the	O
model	O
according	O
to	O
the	O
com-	O
plexity	O
of	O
the	O
problem	O
being	O
solved	O
.	O
we	O
shall	O
see	O
that	O
the	O
least	O
squares	O
approach	O
to	O
ﬁnding	O
the	O
model	O
parameters	O
represents	O
a	O
speciﬁc	O
case	O
of	O
maximum	B
likelihood	I
(	O
discussed	O
in	O
section	O
1.2.5	O
)	O
,	O
and	O
that	O
the	O
over-ﬁtting	B
problem	O
can	O
be	O
understood	O
as	O
a	O
general	O
property	O
of	O
maximum	B
likelihood	I
.	O
by	O
adopting	O
a	O
bayesian	O
approach	O
,	O
the	O
over-ﬁtting	B
problem	O
can	O
be	O
avoided	O
.	O
we	O
shall	O
see	O
that	O
there	O
is	O
no	O
difﬁculty	O
from	O
a	O
bayesian	O
perspective	O
in	O
employing	O
models	O
for	O
which	O
the	O
number	O
of	O
parameters	O
greatly	O
exceeds	O
the	O
number	O
of	O
data	O
points	O
.	O
indeed	O
,	O
in	O
a	O
bayesian	O
model	O
the	O
effective	B
number	I
of	I
parameters	I
adapts	O
automatically	O
to	O
the	O
size	O
of	O
the	O
data	O
set	O
.	O
for	O
the	O
moment	O
,	O
however	O
,	O
it	O
is	O
instructive	O
to	O
continue	O
with	O
the	O
current	O
approach	O
and	O
to	O
consider	O
how	O
in	O
practice	O
we	O
can	O
apply	O
it	O
to	O
data	O
sets	O
of	O
limited	O
size	O
where	O
we	O
section	O
3.4	O
10	O
1.	O
introduction	O
t	O
1	O
0	O
−1	O
ln	O
λ	O
=	O
−18	O
t	O
1	O
0	O
−1	O
ln	O
λ	O
=	O
0	O
0	O
x	O
1	O
0	O
x	O
1	O
figure	O
1.7	O
plots	O
of	O
m	O
=	O
9	O
polynomials	O
ﬁtted	O
to	O
the	O
data	O
set	O
shown	O
in	O
figure	O
1.2	O
using	O
the	O
regularized	O
error	O
function	O
(	O
1.4	O
)	O
for	O
two	O
values	O
of	O
the	O
regularization	B
parameter	O
λ	O
corresponding	O
to	O
ln	O
λ	O
=	O
−18	O
and	O
ln	O
λ	O
=	O
0.	O
the	O
case	O
of	O
no	O
regularizer	O
,	O
i.e.	O
,	O
λ	O
=	O
0	O
,	O
corresponding	O
to	O
ln	O
λ	O
=	O
−∞	O
,	O
is	O
shown	O
at	O
the	O
bottom	O
right	O
of	O
figure	O
1.4.	O
may	O
wish	O
to	O
use	O
relatively	O
complex	O
and	O
ﬂexible	O
models	O
.	O
one	O
technique	O
that	O
is	O
often	O
used	O
to	O
control	O
the	O
over-ﬁtting	B
phenomenon	O
in	O
such	O
cases	O
is	O
that	O
of	O
regularization	B
,	O
which	O
involves	O
adding	O
a	O
penalty	O
term	O
to	O
the	O
error	B
function	I
(	O
1.2	O
)	O
in	O
order	O
to	O
discourage	O
the	O
coefﬁcients	O
from	O
reaching	O
large	O
values	O
.	O
the	O
simplest	O
such	O
penalty	O
term	O
takes	O
the	O
form	O
of	O
a	O
sum	O
of	O
squares	O
of	O
all	O
of	O
the	O
coefﬁcients	O
,	O
leading	O
to	O
a	O
modiﬁed	O
error	B
function	I
of	O
the	O
form	O
n	O
(	O
cid:2	O
)	O
(	O
cid:4	O
)	O
e	O
(	O
w	O
)	O
=	O
1	O
2	O
{	O
y	O
(	O
xn	O
,	O
w	O
)	O
−	O
tn	O
}	O
2	O
+	O
λ	O
2	O
(	O
cid:6	O
)	O
w	O
(	O
cid:6	O
)	O
2	O
(	O
1.4	O
)	O
n=1	O
0	O
+	O
w2	O
1	O
+	O
.	O
.	O
.	O
+	O
w2	O
where	O
(	O
cid:6	O
)	O
w	O
(	O
cid:6	O
)	O
2	O
≡	O
wtw	O
=	O
w2	O
m	O
,	O
and	O
the	O
coefﬁcient	O
λ	O
governs	O
the	O
rel-	O
ative	O
importance	O
of	O
the	O
regularization	B
term	O
compared	O
with	O
the	O
sum-of-squares	B
error	I
term	O
.	O
note	O
that	O
often	O
the	O
coefﬁcient	O
w0	O
is	O
omitted	O
from	O
the	O
regularizer	O
because	O
its	O
inclusion	O
causes	O
the	O
results	O
to	O
depend	O
on	O
the	O
choice	O
of	O
origin	O
for	O
the	O
target	O
variable	O
(	O
hastie	O
et	O
al.	O
,	O
2001	O
)	O
,	O
or	O
it	O
may	O
be	O
included	O
but	O
with	O
its	O
own	O
regularization	B
coefﬁcient	O
(	O
we	O
shall	O
discuss	O
this	O
topic	O
in	O
more	O
detail	O
in	O
section	O
5.5.1	O
)	O
.	O
again	O
,	O
the	O
error	B
function	I
in	O
(	O
1.4	O
)	O
can	O
be	O
minimized	O
exactly	O
in	O
closed	O
form	O
.	O
techniques	O
such	O
as	O
this	O
are	O
known	O
in	O
the	O
statistics	O
literature	O
as	O
shrinkage	B
methods	O
because	O
they	O
reduce	O
the	O
value	O
of	O
the	O
coefﬁcients	O
.	O
the	O
particular	O
case	O
of	O
a	O
quadratic	O
regularizer	O
is	O
called	O
ridge	O
regres-	O
sion	B
(	O
hoerl	O
and	O
kennard	O
,	O
1970	O
)	O
.	O
in	O
the	O
context	O
of	O
neural	O
networks	O
,	O
this	O
approach	O
is	O
known	O
as	O
weight	B
decay	I
.	O
figure	O
1.7	O
shows	O
the	O
results	O
of	O
ﬁtting	O
the	O
polynomial	O
of	O
order	O
m	O
=	O
9	O
to	O
the	O
same	O
data	O
set	O
as	O
before	O
but	O
now	O
using	O
the	O
regularized	O
error	O
function	O
given	O
by	O
(	O
1.4	O
)	O
.	O
we	O
see	O
that	O
,	O
for	O
a	O
value	O
of	O
ln	O
λ	O
=	O
−18	O
,	O
the	O
over-ﬁtting	B
has	O
been	O
suppressed	O
and	O
we	O
now	O
obtain	O
a	O
much	O
closer	O
representation	O
of	O
the	O
underlying	O
function	O
sin	O
(	O
2πx	O
)	O
.	O
if	O
,	O
however	O
,	O
we	O
use	O
too	O
large	O
a	O
value	O
for	O
λ	O
then	O
we	O
again	O
obtain	O
a	O
poor	O
ﬁt	O
,	O
as	O
shown	O
in	O
figure	O
1.7	O
for	O
ln	O
λ	O
=	O
0.	O
the	O
corresponding	O
coefﬁcients	O
from	O
the	O
ﬁtted	O
polynomials	O
are	O
given	O
in	O
table	O
1.2	O
,	O
showing	O
that	O
regularization	B
has	O
the	O
desired	O
effect	O
of	O
reducing	O
exercise	O
1.2	O
1.1.	O
example	O
:	O
polynomial	O
curve	O
fitting	O
11	O
table	O
1.2	O
table	O
of	O
the	O
coefﬁcients	O
w	O
(	O
cid:1	O
)	O
for	O
m	O
=	O
9	O
polynomials	O
with	O
various	O
values	O
for	O
the	O
regularization	B
parameter	O
λ.	O
note	O
that	O
ln	O
λ	O
=	O
−∞	O
corresponds	O
to	O
a	O
model	O
with	O
no	O
regularization	B
,	O
i.e.	O
,	O
to	O
the	O
graph	O
at	O
the	O
bottom	O
right	O
in	O
fig-	O
ure	O
1.4.	O
we	O
see	O
that	O
,	O
as	O
the	O
value	O
of	O
λ	O
increases	O
,	O
the	O
typical	O
magnitude	O
of	O
the	O
coefﬁcients	O
gets	O
smaller	O
.	O
ln	O
λ	O
=	O
−∞	O
ln	O
λ	O
=	O
−18	O
0.35	O
4.74	O
-0.77	O
-31.97	O
-3.89	O
55.28	O
41.32	O
-45.95	O
-91.53	O
72.68	O
0.35	O
232.37	O
-5321.83	O
48568.31	O
-231639.30	O
640042.26	O
-1061800.52	O
1042400.18	O
-557682.99	O
125201.43	O
ln	O
λ	O
=	O
0	O
0.13	O
-0.05	O
-0.06	O
-0.05	O
-0.03	O
-0.02	O
-0.01	O
-0.00	O
0.00	O
0.01	O
w	O
(	O
cid:1	O
)	O
0	O
w	O
(	O
cid:1	O
)	O
1	O
w	O
(	O
cid:1	O
)	O
2	O
w	O
(	O
cid:1	O
)	O
3	O
w	O
(	O
cid:1	O
)	O
4	O
w	O
(	O
cid:1	O
)	O
5	O
w	O
(	O
cid:1	O
)	O
6	O
w	O
(	O
cid:1	O
)	O
7	O
w	O
(	O
cid:1	O
)	O
8	O
w	O
(	O
cid:1	O
)	O
9	O
the	O
magnitude	O
of	O
the	O
coefﬁcients	O
.	O
the	O
impact	O
of	O
the	O
regularization	B
term	O
on	O
the	O
generalization	B
error	O
can	O
be	O
seen	O
by	O
plotting	O
the	O
value	O
of	O
the	O
rms	O
error	B
(	O
1.3	O
)	O
for	O
both	O
training	B
and	O
test	O
sets	O
against	O
ln	O
λ	O
,	O
as	O
shown	O
in	O
figure	O
1.8.	O
we	O
see	O
that	O
in	O
effect	O
λ	O
now	O
controls	O
the	O
effective	O
complexity	O
of	O
the	O
model	O
and	O
hence	O
determines	O
the	O
degree	O
of	O
over-ﬁtting	B
.	O
the	O
issue	O
of	O
model	O
complexity	O
is	O
an	O
important	O
one	O
and	O
will	O
be	O
discussed	O
at	O
length	O
in	O
section	O
1.3.	O
here	O
we	O
simply	O
note	O
that	O
,	O
if	O
we	O
were	O
trying	O
to	O
solve	O
a	O
practical	O
application	O
using	O
this	O
approach	O
of	O
minimizing	O
an	O
error	B
function	I
,	O
we	O
would	O
have	O
to	O
ﬁnd	O
a	O
way	O
to	O
determine	O
a	O
suitable	O
value	O
for	O
the	O
model	O
complexity	O
.	O
the	O
results	O
above	O
suggest	O
a	O
simple	O
way	O
of	O
achieving	O
this	O
,	O
namely	O
by	O
taking	O
the	O
available	O
data	O
and	O
partitioning	O
it	O
into	O
a	O
training	B
set	I
,	O
used	O
to	O
determine	O
the	O
coefﬁcients	O
w	O
,	O
and	O
a	O
separate	O
validation	B
set	I
,	O
also	O
called	O
a	O
hold-out	B
set	I
,	O
used	O
to	O
optimize	O
the	O
model	O
complexity	O
(	O
either	O
m	O
or	O
λ	O
)	O
.	O
in	O
many	O
cases	O
,	O
however	O
,	O
this	O
will	O
prove	O
to	O
be	O
too	O
wasteful	O
of	O
valuable	O
training	B
data	O
,	O
and	O
we	O
have	O
to	O
seek	O
more	O
sophisticated	O
approaches	O
.	O
so	O
far	O
our	O
discussion	O
of	O
polynomial	B
curve	I
ﬁtting	I
has	O
appealed	O
largely	O
to	O
in-	O
tuition	O
.	O
we	O
now	O
seek	O
a	O
more	O
principled	O
approach	O
to	O
solving	O
problems	O
in	O
pattern	O
recognition	O
by	O
turning	O
to	O
a	O
discussion	O
of	O
probability	B
theory	O
.	O
as	O
well	O
as	O
providing	O
the	O
foundation	O
for	O
nearly	O
all	O
of	O
the	O
subsequent	O
developments	O
in	O
this	O
book	O
,	O
it	O
will	O
also	O
section	O
1.3	O
figure	O
1.8	O
graph	O
of	O
the	O
root-mean-square	O
er-	O
ror	O
(	O
1.3	O
)	O
versus	O
ln	O
λ	O
for	O
the	O
m	O
=	O
9	O
polynomial	O
.	O
1	O
training	B
test	O
s	O
m	O
r	O
e	O
0.5	O
0	O
−35	O
−30	O
ln	O
λ	O
−25	O
−20	O
12	O
1.	O
introduction	O
give	O
us	O
some	O
important	O
insights	O
into	O
the	O
concepts	O
we	O
have	O
introduced	O
in	O
the	O
con-	O
text	O
of	O
polynomial	B
curve	I
ﬁtting	I
and	O
will	O
allow	O
us	O
to	O
extend	O
these	O
to	O
more	O
complex	O
situations	O
.	O
1.2.	O
probability	B
theory	O
a	O
key	O
concept	O
in	O
the	O
ﬁeld	O
of	O
pattern	O
recognition	O
is	O
that	O
of	O
uncertainty	O
.	O
it	O
arises	O
both	O
through	O
noise	O
on	O
measurements	O
,	O
as	O
well	O
as	O
through	O
the	O
ﬁnite	O
size	O
of	O
data	O
sets	O
.	O
prob-	O
ability	O
theory	B
provides	O
a	O
consistent	B
framework	O
for	O
the	O
quantiﬁcation	O
and	O
manipula-	O
tion	O
of	O
uncertainty	O
and	O
forms	O
one	O
of	O
the	O
central	O
foundations	O
for	O
pattern	O
recognition	O
.	O
when	O
combined	O
with	O
decision	B
theory	I
,	O
discussed	O
in	O
section	O
1.5	O
,	O
it	O
allows	O
us	O
to	O
make	O
optimal	O
predictions	O
given	O
all	O
the	O
information	O
available	O
to	O
us	O
,	O
even	O
though	O
that	O
infor-	O
mation	B
may	O
be	O
incomplete	O
or	O
ambiguous	O
.	O
we	O
will	O
introduce	O
the	O
basic	O
concepts	O
of	O
probability	B
theory	O
by	O
considering	O
a	O
sim-	O
ple	O
example	O
.	O
imagine	O
we	O
have	O
two	O
boxes	O
,	O
one	O
red	O
and	O
one	O
blue	O
,	O
and	O
in	O
the	O
red	O
box	O
we	O
have	O
2	O
apples	O
and	O
6	O
oranges	O
,	O
and	O
in	O
the	O
blue	O
box	O
we	O
have	O
3	O
apples	O
and	O
1	O
orange	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
1.9.	O
now	O
suppose	O
we	O
randomly	O
pick	O
one	O
of	O
the	O
boxes	O
and	O
from	O
that	O
box	O
we	O
randomly	O
select	O
an	O
item	O
of	O
fruit	O
,	O
and	O
having	O
observed	O
which	O
sort	O
of	O
fruit	O
it	O
is	O
we	O
replace	O
it	O
in	O
the	O
box	O
from	O
which	O
it	O
came	O
.	O
we	O
could	O
imagine	O
repeating	O
this	O
process	O
many	O
times	O
.	O
let	O
us	O
suppose	O
that	O
in	O
so	O
doing	O
we	O
pick	O
the	O
red	O
box	O
40	O
%	O
of	O
the	O
time	O
and	O
we	O
pick	O
the	O
blue	O
box	O
60	O
%	O
of	O
the	O
time	O
,	O
and	O
that	O
when	O
we	O
remove	O
an	O
item	O
of	O
fruit	O
from	O
a	O
box	O
we	O
are	O
equally	O
likely	O
to	O
select	O
any	O
of	O
the	O
pieces	O
of	O
fruit	O
in	O
the	O
box	O
.	O
in	O
this	O
example	O
,	O
the	O
identity	O
of	O
the	O
box	O
that	O
will	O
be	O
chosen	O
is	O
a	O
random	O
variable	O
,	O
which	O
we	O
shall	O
denote	O
by	O
b.	O
this	O
random	O
variable	O
can	O
take	O
one	O
of	O
two	O
possible	O
values	O
,	O
namely	O
r	O
(	O
corresponding	O
to	O
the	O
red	O
box	O
)	O
or	O
b	O
(	O
corresponding	O
to	O
the	O
blue	O
box	O
)	O
.	O
similarly	O
,	O
the	O
identity	O
of	O
the	O
fruit	O
is	O
also	O
a	O
random	O
variable	O
and	O
will	O
be	O
denoted	O
by	O
f	O
.	O
it	O
can	O
take	O
either	O
of	O
the	O
values	O
a	O
(	O
for	O
apple	O
)	O
or	O
o	O
(	O
for	O
orange	O
)	O
.	O
to	O
begin	O
with	O
,	O
we	O
shall	O
deﬁne	O
the	O
probability	B
of	O
an	O
event	O
to	O
be	O
the	O
fraction	O
of	O
times	O
that	O
event	O
occurs	O
out	O
of	O
the	O
total	O
number	O
of	O
trials	O
,	O
in	O
the	O
limit	O
that	O
the	O
total	O
number	O
of	O
trials	O
goes	O
to	O
inﬁnity	O
.	O
thus	O
the	O
probability	B
of	O
selecting	O
the	O
red	O
box	O
is	O
4/10	O
figure	O
1.9	O
we	O
use	O
a	O
simple	O
example	O
of	O
two	O
coloured	O
boxes	O
each	O
containing	O
fruit	O
(	O
apples	O
shown	O
in	O
green	O
and	O
or-	O
anges	O
shown	O
in	O
orange	O
)	O
to	O
intro-	O
duce	O
the	O
basic	O
ideas	O
of	O
probability	B
.	O
1.2.	O
probability	B
theory	O
13	O
figure	O
1.10	O
we	O
can	O
derive	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
by	O
considering	O
two	O
random	O
variables	O
,	O
x	O
,	O
which	O
takes	O
the	O
values	O
{	O
xi	O
}	O
where	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
,	O
and	O
y	O
,	O
which	O
takes	O
the	O
values	O
{	O
yj	O
}	O
where	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
l.	O
in	O
this	O
illustration	O
we	O
have	O
m	O
=	O
5	O
and	O
l	O
=	O
3.	O
if	O
we	O
consider	O
a	O
total	O
number	O
n	O
of	O
instances	O
of	O
these	O
variables	O
,	O
then	O
we	O
denote	O
the	O
number	O
of	O
instances	O
where	O
x	O
=	O
xi	O
and	O
y	O
=	O
yj	O
by	O
nij	O
,	O
which	O
is	O
the	O
number	O
of	O
points	O
in	O
the	O
corresponding	O
cell	O
of	O
the	O
array	O
.	O
the	O
number	O
of	O
points	O
in	O
column	O
i	O
,	O
corresponding	O
to	O
x	O
=	O
xi	O
,	O
is	O
denoted	O
by	O
ci	O
,	O
and	O
the	O
number	O
of	O
points	O
in	O
row	O
j	O
,	O
corresponding	O
to	O
y	O
=	O
yj	O
,	O
is	O
denoted	O
by	O
rj	O
.	O
yj	O
ci	O
}	O
nij	O
xi	O
}	O
rj	O
and	O
the	O
probability	B
of	O
selecting	O
the	O
blue	O
box	O
is	O
6/10	O
.	O
we	O
write	O
these	O
probabilities	O
as	O
p	O
(	O
b	O
=	O
r	O
)	O
=	O
4/10	O
and	O
p	O
(	O
b	O
=	O
b	O
)	O
=	O
6/10	O
.	O
note	O
that	O
,	O
by	O
deﬁnition	O
,	O
probabilities	O
must	O
lie	O
in	O
the	O
interval	O
[	O
0	O
,	O
1	O
]	O
.	O
also	O
,	O
if	O
the	O
events	O
are	O
mutually	O
exclusive	O
and	O
if	O
they	O
include	O
all	O
possible	O
outcomes	O
(	O
for	O
instance	O
,	O
in	O
this	O
example	O
the	O
box	O
must	O
be	O
either	O
red	O
or	O
blue	O
)	O
,	O
then	O
we	O
see	O
that	O
the	O
probabilities	O
for	O
those	O
events	O
must	O
sum	O
to	O
one	O
.	O
we	O
can	O
now	O
ask	O
questions	O
such	O
as	O
:	O
“	O
what	O
is	O
the	O
overall	O
probability	B
that	O
the	O
se-	O
lection	O
procedure	O
will	O
pick	O
an	O
apple	O
?	O
”	O
,	O
or	O
“	O
given	O
that	O
we	O
have	O
chosen	O
an	O
orange	O
,	O
what	O
is	O
the	O
probability	B
that	O
the	O
box	O
we	O
chose	O
was	O
the	O
blue	O
one	O
?	O
”	O
.	O
we	O
can	O
answer	O
questions	O
such	O
as	O
these	O
,	O
and	O
indeed	O
much	O
more	O
complex	O
questions	O
associated	O
with	O
problems	O
in	O
pattern	O
recognition	O
,	O
once	O
we	O
have	O
equipped	O
ourselves	O
with	O
the	O
two	O
el-	O
ementary	O
rules	O
of	O
probability	B
,	O
known	O
as	O
the	O
sum	B
rule	I
and	O
the	O
product	B
rule	I
.	O
having	O
obtained	O
these	O
rules	O
,	O
we	O
shall	O
then	O
return	O
to	O
our	O
boxes	O
of	O
fruit	O
example	O
.	O
in	O
order	O
to	O
derive	O
the	O
rules	O
of	O
probability	B
,	O
consider	O
the	O
slightly	O
more	O
general	O
ex-	O
ample	O
shown	O
in	O
figure	O
1.10	O
involving	O
two	O
random	O
variables	O
x	O
and	O
y	O
(	O
which	O
could	O
for	O
instance	O
be	O
the	O
box	O
and	O
fruit	O
variables	O
considered	O
above	O
)	O
.	O
we	O
shall	O
suppose	O
that	O
x	O
can	O
take	O
any	O
of	O
the	O
values	O
xi	O
where	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
,	O
and	O
y	O
can	O
take	O
the	O
values	O
yj	O
where	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
l.	O
consider	O
a	O
total	O
of	O
n	O
trials	O
in	O
which	O
we	O
sample	O
both	O
of	O
the	O
variables	O
x	O
and	O
y	O
,	O
and	O
let	O
the	O
number	O
of	O
such	O
trials	O
in	O
which	O
x	O
=	O
xi	O
and	O
y	O
=	O
yj	O
be	O
nij	O
.	O
also	O
,	O
let	O
the	O
number	O
of	O
trials	O
in	O
which	O
x	O
takes	O
the	O
value	O
xi	O
(	O
irrespective	O
of	O
the	O
value	O
that	O
y	O
takes	O
)	O
be	O
denoted	O
by	O
ci	O
,	O
and	O
similarly	O
let	O
the	O
number	O
of	O
trials	O
in	O
which	O
y	O
takes	O
the	O
value	O
yj	O
be	O
denoted	O
by	O
rj	O
.	O
the	O
probability	B
that	O
x	O
will	O
take	O
the	O
value	O
xi	O
and	O
y	O
will	O
take	O
the	O
value	O
yj	O
is	O
written	O
p	O
(	O
x	O
=	O
xi	O
,	O
y	O
=	O
yj	O
)	O
and	O
is	O
called	O
the	O
joint	O
probability	B
of	O
x	O
=	O
xi	O
and	O
y	O
=	O
yj	O
.	O
it	O
is	O
given	O
by	O
the	O
number	O
of	O
points	O
falling	O
in	O
the	O
cell	O
i	O
,	O
j	O
as	O
a	O
fraction	O
of	O
the	O
total	O
number	O
of	O
points	O
,	O
and	O
hence	O
p	O
(	O
x	O
=	O
xi	O
,	O
y	O
=	O
yj	O
)	O
=	O
nij	O
n	O
(	O
1.5	O
)	O
here	O
we	O
are	O
implicitly	O
considering	O
the	O
limit	O
n	O
→	O
∞	O
.	O
similarly	O
,	O
the	O
probability	B
that	O
x	O
takes	O
the	O
value	O
xi	O
irrespective	O
of	O
the	O
value	O
of	O
y	O
is	O
written	O
as	O
p	O
(	O
x	O
=	O
xi	O
)	O
and	O
is	O
given	O
by	O
the	O
fraction	O
of	O
the	O
total	O
number	O
of	O
points	O
that	O
fall	O
in	O
column	O
i	O
,	O
so	O
that	O
.	O
p	O
(	O
x	O
=	O
xi	O
)	O
=	O
ci	O
n	O
.	O
because	O
the	O
number	O
of	O
instances	O
in	O
column	O
i	O
in	O
figure	O
1.10	O
is	O
just	O
the	O
sum	O
of	O
the	O
number	O
of	O
instances	O
in	O
each	O
cell	O
of	O
that	O
column	O
,	O
we	O
have	O
ci	O
=	O
j	O
nij	O
and	O
therefore	O
,	O
(	O
cid:5	O
)	O
(	O
1.6	O
)	O
14	O
1.	O
introduction	O
from	O
(	O
1.5	O
)	O
and	O
(	O
1.6	O
)	O
,	O
we	O
have	O
p	O
(	O
x	O
=	O
xi	O
)	O
=	O
l	O
(	O
cid:2	O
)	O
j=1	O
p	O
(	O
x	O
=	O
xi	O
,	O
y	O
=	O
yj	O
)	O
(	O
1.7	O
)	O
which	O
is	O
the	O
sum	B
rule	I
of	I
probability	I
.	O
note	O
that	O
p	O
(	O
x	O
=	O
xi	O
)	O
is	O
sometimes	O
called	O
the	O
marginal	B
probability	I
,	O
because	O
it	O
is	O
obtained	O
by	O
marginalizing	O
,	O
or	O
summing	O
out	O
,	O
the	O
other	O
variables	O
(	O
in	O
this	O
case	O
y	O
)	O
.	O
if	O
we	O
consider	O
only	O
those	O
instances	O
for	O
which	O
x	O
=	O
xi	O
,	O
then	O
the	O
fraction	O
of	O
such	O
instances	O
for	O
which	O
y	O
=	O
yj	O
is	O
written	O
p	O
(	O
y	O
=	O
yj|x	O
=	O
xi	O
)	O
and	O
is	O
called	O
the	O
conditional	B
probability	I
of	O
y	O
=	O
yj	O
given	O
x	O
=	O
xi	O
.	O
it	O
is	O
obtained	O
by	O
ﬁnding	O
the	O
fraction	O
of	O
those	O
points	O
in	O
column	O
i	O
that	O
fall	O
in	O
cell	O
i	O
,	O
j	O
and	O
hence	O
is	O
given	O
by	O
p	O
(	O
y	O
=	O
yj|x	O
=	O
xi	O
)	O
=	O
nij	O
ci	O
.	O
from	O
(	O
1.5	O
)	O
,	O
(	O
1.6	O
)	O
,	O
and	O
(	O
1.8	O
)	O
,	O
we	O
can	O
then	O
derive	O
the	O
following	O
relationship	O
p	O
(	O
x	O
=	O
xi	O
,	O
y	O
=	O
yj	O
)	O
=	O
nij	O
n	O
=	O
nij	O
ci	O
·	O
ci	O
n	O
=	O
p	O
(	O
y	O
=	O
yj|x	O
=	O
xi	O
)	O
p	O
(	O
x	O
=	O
xi	O
)	O
(	O
1.8	O
)	O
(	O
1.9	O
)	O
which	O
is	O
the	O
product	B
rule	I
of	I
probability	I
.	O
so	O
far	O
we	O
have	O
been	O
quite	O
careful	O
to	O
make	O
a	O
distinction	O
between	O
a	O
random	O
vari-	O
able	O
,	O
such	O
as	O
the	O
box	O
b	O
in	O
the	O
fruit	O
example	O
,	O
and	O
the	O
values	O
that	O
the	O
random	O
variable	O
can	O
take	O
,	O
for	O
example	O
r	O
if	O
the	O
box	O
were	O
the	O
red	O
one	O
.	O
thus	O
the	O
probability	B
that	O
b	O
takes	O
the	O
value	O
r	O
is	O
denoted	O
p	O
(	O
b	O
=	O
r	O
)	O
.	O
although	O
this	O
helps	O
to	O
avoid	O
ambiguity	O
,	O
it	O
leads	O
to	O
a	O
rather	O
cumbersome	O
notation	O
,	O
and	O
in	O
many	O
cases	O
there	O
will	O
be	O
no	O
need	O
for	O
such	O
pedantry	O
.	O
instead	O
,	O
we	O
may	O
simply	O
write	O
p	O
(	O
b	O
)	O
to	O
denote	O
a	O
distribution	O
over	O
the	O
ran-	O
dom	O
variable	O
b	O
,	O
or	O
p	O
(	O
r	O
)	O
to	O
denote	O
the	O
distribution	O
evaluated	O
for	O
the	O
particular	O
value	O
r	O
,	O
provided	O
that	O
the	O
interpretation	O
is	O
clear	O
from	O
the	O
context	O
.	O
with	O
this	O
more	O
compact	O
notation	O
,	O
we	O
can	O
write	O
the	O
two	O
fundamental	O
rules	O
of	O
probability	B
theory	O
in	O
the	O
following	O
form	O
.	O
the	O
rules	O
of	O
probability	B
sum	O
rule	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:2	O
)	O
y	O
p	O
(	O
x	O
,	O
y	O
)	O
product	B
rule	I
p	O
(	O
x	O
,	O
y	O
)	O
=	O
p	O
(	O
y	O
|x	O
)	O
p	O
(	O
x	O
)	O
.	O
(	O
1.10	O
)	O
(	O
1.11	O
)	O
here	O
p	O
(	O
x	O
,	O
y	O
)	O
is	O
a	O
joint	O
probability	B
and	O
is	O
verbalized	O
as	O
“	O
the	O
probability	B
of	O
x	O
and	O
y	O
”	O
.	O
similarly	O
,	O
the	O
quantity	O
p	O
(	O
y	O
|x	O
)	O
is	O
a	O
conditional	B
probability	I
and	O
is	O
verbalized	O
as	O
“	O
the	O
probability	B
of	O
y	O
given	O
x	O
”	O
,	O
whereas	O
the	O
quantity	O
p	O
(	O
x	O
)	O
is	O
a	O
marginal	B
probability	I
1.2.	O
probability	B
theory	O
15	O
and	O
is	O
simply	O
“	O
the	O
probability	B
of	O
x	O
”	O
.	O
these	O
two	O
simple	O
rules	O
form	O
the	O
basis	O
for	O
all	O
of	O
the	O
probabilistic	O
machinery	O
that	O
we	O
use	O
throughout	O
this	O
book	O
.	O
from	O
the	O
product	B
rule	I
,	O
together	O
with	O
the	O
symmetry	O
property	O
p	O
(	O
x	O
,	O
y	O
)	O
=	O
p	O
(	O
y	O
,	O
x	O
)	O
,	O
we	O
immediately	O
obtain	O
the	O
following	O
relationship	O
between	O
conditional	B
probabilities	O
p	O
(	O
y	O
|x	O
)	O
=	O
p	O
(	O
x|y	O
)	O
p	O
(	O
y	O
)	O
p	O
(	O
x	O
)	O
(	O
1.12	O
)	O
which	O
is	O
called	O
bayes	O
’	O
theorem	O
and	O
which	O
plays	O
a	O
central	O
role	O
in	O
pattern	O
recognition	O
and	O
machine	O
learning	O
.	O
using	O
the	O
sum	B
rule	I
,	O
the	O
denominator	O
in	O
bayes	O
’	O
theorem	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
quantities	O
appearing	O
in	O
the	O
numerator	O
(	O
cid:2	O
)	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
x|y	O
)	O
p	O
(	O
y	O
)	O
.	O
(	O
1.13	O
)	O
y	O
we	O
can	O
view	O
the	O
denominator	O
in	O
bayes	O
’	O
theorem	O
as	O
being	O
the	O
normalization	O
constant	O
required	O
to	O
ensure	O
that	O
the	O
sum	O
of	O
the	O
conditional	B
probability	I
on	O
the	O
left-hand	O
side	O
of	O
(	O
1.12	O
)	O
over	O
all	O
values	O
of	O
y	O
equals	O
one	O
.	O
in	O
figure	O
1.11	O
,	O
we	O
show	O
a	O
simple	O
example	O
involving	O
a	O
joint	O
distribution	O
over	O
two	O
variables	O
to	O
illustrate	O
the	O
concept	O
of	O
marginal	B
and	O
conditional	B
distributions	O
.	O
here	O
a	O
ﬁnite	O
sample	O
of	O
n	O
=	O
60	O
data	O
points	O
has	O
been	O
drawn	O
from	O
the	O
joint	O
distribution	O
and	O
is	O
shown	O
in	O
the	O
top	O
left	O
.	O
in	O
the	O
top	O
right	O
is	O
a	O
histogram	O
of	O
the	O
fractions	O
of	O
data	O
points	O
having	O
each	O
of	O
the	O
two	O
values	O
of	O
y	O
.	O
from	O
the	O
deﬁnition	O
of	O
probability	B
,	O
these	O
fractions	O
would	O
equal	O
the	O
corresponding	O
probabilities	O
p	O
(	O
y	O
)	O
in	O
the	O
limit	O
n	O
→	O
∞	O
.	O
we	O
can	O
view	O
the	O
histogram	O
as	O
a	O
simple	O
way	O
to	O
model	O
a	O
probability	B
distribution	O
given	O
only	O
a	O
ﬁnite	O
number	O
of	O
points	O
drawn	O
from	O
that	O
distribution	O
.	O
modelling	O
distributions	O
from	O
data	O
lies	O
at	O
the	O
heart	O
of	O
statistical	O
pattern	O
recognition	O
and	O
will	O
be	O
explored	O
in	O
great	O
detail	O
in	O
this	O
book	O
.	O
the	O
remaining	O
two	O
plots	O
in	O
figure	O
1.11	O
show	O
the	O
corresponding	O
histogram	O
estimates	O
of	O
p	O
(	O
x	O
)	O
and	O
p	O
(	O
x|y	O
=	O
1	O
)	O
.	O
let	O
us	O
now	O
return	O
to	O
our	O
example	O
involving	O
boxes	O
of	O
fruit	O
.	O
for	O
the	O
moment	O
,	O
we	O
shall	O
once	O
again	O
be	O
explicit	O
about	O
distinguishing	O
between	O
the	O
random	O
variables	O
and	O
their	O
instantiations	O
.	O
we	O
have	O
seen	O
that	O
the	O
probabilities	O
of	O
selecting	O
either	O
the	O
red	O
or	O
the	O
blue	O
boxes	O
are	O
given	O
by	O
p	O
(	O
b	O
=	O
r	O
)	O
=	O
4/10	O
p	O
(	O
b	O
=	O
b	O
)	O
=	O
6/10	O
(	O
1.14	O
)	O
(	O
1.15	O
)	O
respectively	O
.	O
note	O
that	O
these	O
satisfy	O
p	O
(	O
b	O
=	O
r	O
)	O
+	O
p	O
(	O
b	O
=	O
b	O
)	O
=	O
1.	O
now	O
suppose	O
that	O
we	O
pick	O
a	O
box	O
at	O
random	O
,	O
and	O
it	O
turns	O
out	O
to	O
be	O
the	O
blue	O
box	O
.	O
then	O
the	O
probability	B
of	O
selecting	O
an	O
apple	O
is	O
just	O
the	O
fraction	O
of	O
apples	O
in	O
the	O
blue	O
box	O
which	O
is	O
3/4	O
,	O
and	O
so	O
p	O
(	O
f	O
=	O
a|b	O
=	O
b	O
)	O
=	O
3/4	O
.	O
in	O
fact	O
,	O
we	O
can	O
write	O
out	O
all	O
four	O
conditional	B
probabilities	O
for	O
the	O
type	O
of	O
fruit	O
,	O
given	O
the	O
selected	O
box	O
p	O
(	O
f	O
=	O
a|b	O
=	O
r	O
)	O
=	O
1/4	O
p	O
(	O
f	O
=	O
o|b	O
=	O
r	O
)	O
=	O
3/4	O
p	O
(	O
f	O
=	O
a|b	O
=	O
b	O
)	O
=	O
3/4	O
p	O
(	O
f	O
=	O
o|b	O
=	O
b	O
)	O
=	O
1/4	O
.	O
(	O
1.16	O
)	O
(	O
1.17	O
)	O
(	O
1.18	O
)	O
(	O
1.19	O
)	O
16	O
1.	O
introduction	O
p	O
(	O
x	O
,	O
y	O
)	O
p	O
(	O
y	O
)	O
y	O
=	O
2	O
y	O
=	O
1	O
x	O
p	O
(	O
x	O
)	O
p	O
(	O
x|y	O
=	O
1	O
)	O
x	O
x	O
figure	O
1.11	O
an	O
illustration	O
of	O
a	O
distribution	O
over	O
two	O
variables	O
,	O
x	O
,	O
which	O
takes	O
9	O
possible	O
values	O
,	O
and	O
y	O
,	O
which	O
takes	O
two	O
possible	O
values	O
.	O
the	O
top	O
left	O
ﬁgure	O
shows	O
a	O
sample	O
of	O
60	O
points	O
drawn	O
from	O
a	O
joint	O
probability	B
distri-	O
bution	O
over	O
these	O
variables	O
.	O
the	O
remaining	O
ﬁgures	O
show	O
histogram	O
estimates	O
of	O
the	O
marginal	B
distributions	O
p	O
(	O
x	O
)	O
and	O
p	O
(	O
y	O
)	O
,	O
as	O
well	O
as	O
the	O
conditional	B
distribution	O
p	O
(	O
x|y	O
=	O
1	O
)	O
corresponding	O
to	O
the	O
bottom	O
row	O
in	O
the	O
top	O
left	O
ﬁgure	O
.	O
again	O
,	O
note	O
that	O
these	O
probabilities	O
are	O
normalized	O
so	O
that	O
p	O
(	O
f	O
=	O
a|b	O
=	O
r	O
)	O
+	O
p	O
(	O
f	O
=	O
o|b	O
=	O
r	O
)	O
=	O
1	O
and	O
similarly	O
p	O
(	O
f	O
=	O
a|b	O
=	O
b	O
)	O
+	O
p	O
(	O
f	O
=	O
o|b	O
=	O
b	O
)	O
=	O
1	O
.	O
(	O
1.20	O
)	O
(	O
1.21	O
)	O
we	O
can	O
now	O
use	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
to	O
evaluate	O
the	O
overall	O
probability	B
of	O
choosing	O
an	O
apple	O
p	O
(	O
f	O
=	O
a	O
)	O
=	O
p	O
(	O
f	O
=	O
a|b	O
=	O
r	O
)	O
p	O
(	O
b	O
=	O
r	O
)	O
+	O
p	O
(	O
f	O
=	O
a|b	O
=	O
b	O
)	O
p	O
(	O
b	O
=	O
b	O
)	O
=	O
=	O
11	O
20	O
from	O
which	O
it	O
follows	O
,	O
using	O
the	O
sum	B
rule	I
,	O
that	O
p	O
(	O
f	O
=	O
o	O
)	O
=	O
1	O
−	O
11/20	O
=	O
9/20	O
.	O
1	O
4	O
×	O
4	O
10	O
+	O
3	O
4	O
×	O
6	O
10	O
(	O
1.22	O
)	O
1.2.	O
probability	B
theory	O
17	O
suppose	O
instead	O
we	O
are	O
told	O
that	O
a	O
piece	O
of	O
fruit	O
has	O
been	O
selected	O
and	O
it	O
is	O
an	O
orange	O
,	O
and	O
we	O
would	O
like	O
to	O
know	O
which	O
box	O
it	O
came	O
from	O
.	O
this	O
requires	O
that	O
we	O
evaluate	O
the	O
probability	B
distribution	O
over	O
boxes	O
conditioned	O
on	O
the	O
identity	O
of	O
the	O
fruit	O
,	O
whereas	O
the	O
probabilities	O
in	O
(	O
1.16	O
)	O
–	O
(	O
1.19	O
)	O
give	O
the	O
probability	B
distribution	O
over	O
the	O
fruit	O
conditioned	O
on	O
the	O
identity	O
of	O
the	O
box	O
.	O
we	O
can	O
solve	O
the	O
problem	O
of	O
reversing	O
the	O
conditional	B
probability	I
by	O
using	O
bayes	O
’	O
theorem	O
to	O
give	O
×	O
20	O
9	O
2	O
3	O
.	O
from	O
the	O
sum	B
rule	I
,	O
it	O
then	O
follows	O
that	O
p	O
(	O
b	O
=	O
b|f	O
=	O
o	O
)	O
=	O
1	O
−	O
2/3	O
=	O
1/3	O
.	O
p	O
(	O
b	O
=	O
r|f	O
=	O
o	O
)	O
=	O
p	O
(	O
f	O
=	O
o|b	O
=	O
r	O
)	O
p	O
(	O
b	O
=	O
r	O
)	O
p	O
(	O
f	O
=	O
o	O
)	O
=	O
3	O
4	O
×	O
4	O
10	O
=	O
(	O
1.23	O
)	O
we	O
can	O
provide	O
an	O
important	O
interpretation	O
of	O
bayes	O
’	O
theorem	O
as	O
follows	O
.	O
if	O
we	O
had	O
been	O
asked	O
which	O
box	O
had	O
been	O
chosen	O
before	O
being	O
told	O
the	O
identity	O
of	O
the	O
selected	O
item	O
of	O
fruit	O
,	O
then	O
the	O
most	O
complete	O
information	O
we	O
have	O
available	O
is	O
provided	O
by	O
the	O
probability	B
p	O
(	O
b	O
)	O
.	O
we	O
call	O
this	O
the	O
prior	B
probability	O
because	O
it	O
is	O
the	O
probability	B
available	O
before	O
we	O
observe	O
the	O
identity	O
of	O
the	O
fruit	O
.	O
once	O
we	O
are	O
told	O
that	O
the	O
fruit	O
is	O
an	O
orange	O
,	O
we	O
can	O
then	O
use	O
bayes	O
’	O
theorem	O
to	O
compute	O
the	O
probability	B
p	O
(	O
b|f	O
)	O
,	O
which	O
we	O
shall	O
call	O
the	O
posterior	B
probability	I
because	O
it	O
is	O
the	O
probability	B
obtained	O
after	O
we	O
have	O
observed	O
f	O
.	O
note	O
that	O
in	O
this	O
example	O
,	O
the	O
prior	B
probability	O
of	O
selecting	O
the	O
red	O
box	O
was	O
4/10	O
,	O
so	O
that	O
we	O
were	O
more	O
likely	O
to	O
select	O
the	O
blue	O
box	O
than	O
the	O
red	O
one	O
.	O
however	O
,	O
once	O
we	O
have	O
observed	O
that	O
the	O
piece	O
of	O
selected	O
fruit	O
is	O
an	O
orange	O
,	O
we	O
ﬁnd	O
that	O
the	O
posterior	B
probability	I
of	O
the	O
red	O
box	O
is	O
now	O
2/3	O
,	O
so	O
that	O
it	O
is	O
now	O
more	O
likely	O
that	O
the	O
box	O
we	O
selected	O
was	O
in	O
fact	O
the	O
red	O
one	O
.	O
this	O
result	O
accords	O
with	O
our	O
intuition	O
,	O
as	O
the	O
proportion	O
of	O
oranges	O
is	O
much	O
higher	O
in	O
the	O
red	O
box	O
than	O
it	O
is	O
in	O
the	O
blue	O
box	O
,	O
and	O
so	O
the	O
observation	O
that	O
the	O
fruit	O
was	O
an	O
orange	O
provides	O
signiﬁcant	O
evidence	O
favouring	O
the	O
red	O
box	O
.	O
in	O
fact	O
,	O
the	O
evidence	O
is	O
sufﬁciently	O
strong	O
that	O
it	O
outweighs	O
the	O
prior	B
and	O
makes	O
it	O
more	O
likely	O
that	O
the	O
red	O
box	O
was	O
chosen	O
rather	O
than	O
the	O
blue	O
one	O
.	O
finally	O
,	O
we	O
note	O
that	O
if	O
the	O
joint	O
distribution	O
of	O
two	O
variables	O
factorizes	O
into	O
the	O
product	O
of	O
the	O
marginals	O
,	O
so	O
that	O
p	O
(	O
x	O
,	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
,	O
then	O
x	O
and	O
y	O
are	O
said	O
to	O
be	O
independent	B
.	O
from	O
the	O
product	B
rule	I
,	O
we	O
see	O
that	O
p	O
(	O
y	O
|x	O
)	O
=	O
p	O
(	O
y	O
)	O
,	O
and	O
so	O
the	O
conditional	B
distribution	O
of	O
y	O
given	O
x	O
is	O
indeed	O
independent	B
of	O
the	O
value	O
of	O
x.	O
for	O
instance	O
,	O
in	O
our	O
boxes	O
of	O
fruit	O
example	O
,	O
if	O
each	O
box	O
contained	O
the	O
same	O
fraction	O
of	O
apples	O
and	O
oranges	O
,	O
then	O
p	O
(	O
f|b	O
)	O
=	O
p	O
(	O
f	O
)	O
,	O
so	O
that	O
the	O
probability	B
of	O
selecting	O
,	O
say	O
,	O
an	O
apple	O
is	O
independent	B
of	O
which	O
box	O
is	O
chosen	O
.	O
1.2.1	O
probability	B
densities	O
as	O
well	O
as	O
considering	O
probabilities	O
deﬁned	O
over	O
discrete	O
sets	O
of	O
events	O
,	O
we	O
also	O
wish	O
to	O
consider	O
probabilities	O
with	O
respect	O
to	O
continuous	O
variables	O
.	O
we	O
shall	O
limit	O
ourselves	O
to	O
a	O
relatively	O
informal	O
discussion	O
.	O
if	O
the	O
probability	B
of	O
a	O
real-valued	O
variable	O
x	O
falling	O
in	O
the	O
interval	O
(	O
x	O
,	O
x	O
+	O
δx	O
)	O
is	O
given	O
by	O
p	O
(	O
x	O
)	O
δx	O
for	O
δx	O
→	O
0	O
,	O
then	O
p	O
(	O
x	O
)	O
is	O
called	O
the	O
probability	B
density	O
over	O
x.	O
this	O
is	O
illustrated	O
in	O
figure	O
1.12.	O
the	O
probability	B
that	O
x	O
will	O
lie	O
in	O
an	O
interval	O
(	O
a	O
,	O
b	O
)	O
is	O
then	O
given	O
by	O
(	O
cid:6	O
)	O
b	O
p	O
(	O
x	O
∈	O
(	O
a	O
,	O
b	O
)	O
)	O
=	O
p	O
(	O
x	O
)	O
dx	O
.	O
a	O
(	O
1.24	O
)	O
18	O
1.	O
introduction	O
figure	O
1.12	O
the	O
concept	O
of	O
probability	B
for	O
discrete	O
variables	O
can	O
be	O
ex-	O
tended	O
to	O
that	O
of	O
a	O
probability	B
density	O
p	O
(	O
x	O
)	O
over	O
a	O
continuous	O
variable	O
x	O
and	O
is	O
such	O
that	O
the	O
probability	B
of	O
x	O
lying	O
in	O
the	O
inter-	O
val	O
(	O
x	O
,	O
x	O
+	O
δx	O
)	O
is	O
given	O
by	O
p	O
(	O
x	O
)	O
δx	O
for	O
δx	O
→	O
0.	O
the	O
probability	B
density	O
can	O
be	O
expressed	O
as	O
the	O
derivative	B
of	O
a	O
cumulative	O
distri-	O
bution	O
function	O
p	O
(	O
x	O
)	O
.	O
p	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
δx	O
x	O
because	O
probabilities	O
are	O
nonnegative	O
,	O
and	O
because	O
the	O
value	O
of	O
x	O
must	O
lie	O
some-	O
where	O
on	O
the	O
real	O
axis	O
,	O
the	O
probability	B
density	O
p	O
(	O
x	O
)	O
must	O
satisfy	O
the	O
two	O
conditions	O
(	O
cid:6	O
)	O
∞	O
p	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
0	O
p	O
(	O
x	O
)	O
dx	O
=	O
1	O
.	O
−∞	O
(	O
1.25	O
)	O
(	O
1.26	O
)	O
under	O
a	O
nonlinear	O
change	O
of	O
variable	O
,	O
a	O
probability	B
density	O
transforms	O
differently	O
from	O
a	O
simple	O
function	O
,	O
due	O
to	O
the	O
jacobian	O
factor	O
.	O
for	O
instance	O
,	O
if	O
we	O
consider	O
a	O
change	O
of	O
variables	O
x	O
=	O
g	O
(	O
y	O
)	O
,	O
then	O
a	O
function	O
f	O
(	O
x	O
)	O
becomes	O
(	O
cid:4	O
)	O
f	O
(	O
y	O
)	O
=	O
f	O
(	O
g	O
(	O
y	O
)	O
)	O
.	O
now	O
consider	O
a	O
probability	B
density	O
px	O
(	O
x	O
)	O
that	O
corresponds	O
to	O
a	O
density	B
py	O
(	O
y	O
)	O
with	O
respect	O
to	O
the	O
new	O
variable	O
y	O
,	O
where	O
the	O
sufﬁces	O
denote	O
the	O
fact	O
that	O
px	O
(	O
x	O
)	O
and	O
py	O
(	O
y	O
)	O
are	O
different	O
densities	O
.	O
observations	O
falling	O
in	O
the	O
range	O
(	O
x	O
,	O
x	O
+	O
δx	O
)	O
will	O
,	O
for	O
small	O
values	O
of	O
δx	O
,	O
be	O
transformed	O
into	O
the	O
range	O
(	O
y	O
,	O
y	O
+	O
δy	O
)	O
where	O
px	O
(	O
x	O
)	O
δx	O
(	O
cid:8	O
)	O
py	O
(	O
y	O
)	O
δy	O
,	O
and	O
hence	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
dx	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
py	O
(	O
y	O
)	O
=	O
px	O
(	O
x	O
)	O
dy	O
=	O
px	O
(	O
g	O
(	O
y	O
)	O
)	O
|g	O
(	O
cid:2	O
)	O
(	O
y	O
)	O
|	O
.	O
(	O
1.27	O
)	O
exercise	O
1.4	O
one	O
consequence	O
of	O
this	O
property	O
is	O
that	O
the	O
concept	O
of	O
the	O
maximum	O
of	O
a	O
probability	B
density	O
is	O
dependent	O
on	O
the	O
choice	O
of	O
variable	O
.	O
the	O
probability	B
that	O
x	O
lies	O
in	O
the	O
interval	O
(	O
−∞	O
,	O
z	O
)	O
is	O
given	O
by	O
the	O
cumulative	B
distribution	I
function	I
deﬁned	O
by	O
(	O
cid:6	O
)	O
z	O
p	O
(	O
z	O
)	O
=	O
p	O
(	O
x	O
)	O
dx	O
−∞	O
(	O
1.28	O
)	O
which	O
satisﬁes	O
p	O
(	O
cid:2	O
)	O
(	O
x	O
)	O
=	O
p	O
(	O
x	O
)	O
,	O
as	O
shown	O
in	O
figure	O
1.12.	O
if	O
we	O
have	O
several	O
continuous	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
,	O
denoted	O
collectively	O
by	O
the	O
vector	O
x	O
,	O
then	O
we	O
can	O
deﬁne	O
a	O
joint	O
probability	B
density	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
such	O
1.2.	O
probability	B
theory	O
19	O
that	O
the	O
probability	B
of	O
x	O
falling	O
in	O
an	O
inﬁnitesimal	O
volume	O
δx	O
containing	O
the	O
point	O
x	O
is	O
given	O
by	O
p	O
(	O
x	O
)	O
δx	O
.	O
this	O
multivariate	O
probability	B
density	O
must	O
satisfy	O
(	O
cid:6	O
)	O
p	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
0	O
p	O
(	O
x	O
)	O
dx	O
=	O
1	O
(	O
1.29	O
)	O
(	O
1.30	O
)	O
in	O
which	O
the	O
integral	O
is	O
taken	O
over	O
the	O
whole	O
of	O
x	O
space	O
.	O
we	O
can	O
also	O
consider	O
joint	O
probability	B
distributions	O
over	O
a	O
combination	O
of	O
discrete	O
and	O
continuous	O
variables	O
.	O
note	O
that	O
if	O
x	O
is	O
a	O
discrete	O
variable	O
,	O
then	O
p	O
(	O
x	O
)	O
is	O
sometimes	O
called	O
a	O
probability	B
mass	O
function	O
because	O
it	O
can	O
be	O
regarded	O
as	O
a	O
set	O
of	O
‘	O
probability	B
masses	O
’	O
concentrated	O
at	O
the	O
allowed	O
values	O
of	O
x.	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
,	O
as	O
well	O
as	O
bayes	O
’	O
theorem	O
,	O
apply	O
equally	O
to	O
the	O
case	O
of	O
probability	B
densities	O
,	O
or	O
to	O
combinations	O
of	O
discrete	O
and	O
con-	O
tinuous	O
variables	O
.	O
for	O
instance	O
,	O
if	O
x	O
and	O
y	O
are	O
two	O
real	O
variables	O
,	O
then	O
the	O
sum	O
and	O
product	O
rules	O
take	O
the	O
form	O
(	O
cid:6	O
)	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
x	O
,	O
y	O
)	O
dy	O
p	O
(	O
x	O
,	O
y	O
)	O
=	O
p	O
(	O
y|x	O
)	O
p	O
(	O
x	O
)	O
.	O
(	O
1.31	O
)	O
(	O
1.32	O
)	O
a	O
formal	O
justiﬁcation	O
of	O
the	O
sum	O
and	O
product	O
rules	O
for	O
continuous	O
variables	O
(	O
feller	O
,	O
1966	O
)	O
requires	O
a	O
branch	O
of	O
mathematics	O
called	O
measure	B
theory	I
and	O
lies	O
outside	O
the	O
scope	O
of	O
this	O
book	O
.	O
its	O
validity	O
can	O
be	O
seen	O
informally	O
,	O
however	O
,	O
by	O
dividing	O
each	O
real	O
variable	O
into	O
intervals	O
of	O
width	O
∆	O
and	O
considering	O
the	O
discrete	O
probability	B
dis-	O
tribution	O
over	O
these	O
intervals	O
.	O
taking	O
the	O
limit	O
∆	O
→	O
0	O
then	O
turns	O
sums	O
into	O
integrals	O
and	O
gives	O
the	O
desired	O
result	O
.	O
1.2.2	O
expectations	O
and	O
covariances	O
one	O
of	O
the	O
most	O
important	O
operations	O
involving	O
probabilities	O
is	O
that	O
of	O
ﬁnding	O
weighted	O
averages	O
of	O
functions	O
.	O
the	O
average	O
value	O
of	O
some	O
function	O
f	O
(	O
x	O
)	O
under	O
a	O
probability	B
distribution	O
p	O
(	O
x	O
)	O
is	O
called	O
the	O
expectation	B
of	O
f	O
(	O
x	O
)	O
and	O
will	O
be	O
denoted	O
by	O
e	O
[	O
f	O
]	O
.	O
for	O
a	O
discrete	O
distribution	O
,	O
it	O
is	O
given	O
by	O
(	O
cid:2	O
)	O
e	O
[	O
f	O
]	O
=	O
p	O
(	O
x	O
)	O
f	O
(	O
x	O
)	O
(	O
1.33	O
)	O
x	O
so	O
that	O
the	O
average	O
is	O
weighted	O
by	O
the	O
relative	B
probabilities	O
of	O
the	O
different	O
values	O
of	O
x.	O
in	O
the	O
case	O
of	O
continuous	O
variables	O
,	O
expectations	O
are	O
expressed	O
in	O
terms	O
of	O
an	O
integration	O
with	O
respect	O
to	O
the	O
corresponding	O
probability	B
density	O
(	O
cid:6	O
)	O
e	O
[	O
f	O
]	O
=	O
p	O
(	O
x	O
)	O
f	O
(	O
x	O
)	O
dx	O
.	O
(	O
1.34	O
)	O
in	O
either	O
case	O
,	O
if	O
we	O
are	O
given	O
a	O
ﬁnite	O
number	O
n	O
of	O
points	O
drawn	O
from	O
the	O
probability	B
distribution	O
or	O
probability	B
density	O
,	O
then	O
the	O
expectation	B
can	O
be	O
approximated	O
as	O
a	O
20	O
1.	O
introduction	O
ﬁnite	O
sum	O
over	O
these	O
points	O
n	O
(	O
cid:2	O
)	O
n=1	O
e	O
[	O
f	O
]	O
(	O
cid:8	O
)	O
1	O
n	O
f	O
(	O
xn	O
)	O
.	O
(	O
1.35	O
)	O
we	O
shall	O
make	O
extensive	O
use	O
of	O
this	O
result	O
when	O
we	O
discuss	O
sampling	B
methods	I
in	O
chapter	O
11.	O
the	O
approximation	O
in	O
(	O
1.35	O
)	O
becomes	O
exact	O
in	O
the	O
limit	O
n	O
→	O
∞	O
.	O
sometimes	O
we	O
will	O
be	O
considering	O
expectations	O
of	O
functions	O
of	O
several	O
variables	O
,	O
in	O
which	O
case	O
we	O
can	O
use	O
a	O
subscript	O
to	O
indicate	O
which	O
variable	O
is	O
being	O
averaged	O
over	O
,	O
so	O
that	O
for	O
instance	O
(	O
1.36	O
)	O
denotes	O
the	O
average	O
of	O
the	O
function	O
f	O
(	O
x	O
,	O
y	O
)	O
with	O
respect	O
to	O
the	O
distribution	O
of	O
x.	O
note	O
that	O
ex	O
[	O
f	O
(	O
x	O
,	O
y	O
)	O
]	O
will	O
be	O
a	O
function	O
of	O
y.	O
ex	O
[	O
f	O
(	O
x	O
,	O
y	O
)	O
]	O
we	O
can	O
also	O
consider	O
a	O
conditional	B
expectation	I
with	O
respect	O
to	O
a	O
conditional	B
distribution	O
,	O
so	O
that	O
ex	O
[	O
f|y	O
]	O
=	O
p	O
(	O
x|y	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
(	O
cid:8	O
)	O
(	O
f	O
(	O
x	O
)	O
−	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
)	O
2	O
(	O
cid:9	O
)	O
x	O
with	O
an	O
analogous	O
deﬁnition	O
for	O
continuous	O
variables	O
.	O
the	O
variance	B
of	O
f	O
(	O
x	O
)	O
is	O
deﬁned	O
by	O
var	O
[	O
f	O
]	O
=	O
e	O
(	O
1.37	O
)	O
(	O
1.38	O
)	O
exercise	O
1.5	O
exercise	O
1.6	O
and	O
provides	O
a	O
measure	O
of	O
how	O
much	O
variability	O
there	O
is	O
in	O
f	O
(	O
x	O
)	O
around	O
its	O
mean	O
value	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
.	O
expanding	O
out	O
the	O
square	O
,	O
we	O
see	O
that	O
the	O
variance	B
can	O
also	O
be	O
written	O
in	O
terms	O
of	O
the	O
expectations	O
of	O
f	O
(	O
x	O
)	O
and	O
f	O
(	O
x	O
)	O
2	O
var	O
[	O
f	O
]	O
=	O
e	O
[	O
f	O
(	O
x	O
)	O
2	O
]	O
−	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
2	O
.	O
(	O
1.39	O
)	O
in	O
particular	O
,	O
we	O
can	O
consider	O
the	O
variance	B
of	O
the	O
variable	O
x	O
itself	O
,	O
which	O
is	O
given	O
by	O
var	O
[	O
x	O
]	O
=	O
e	O
[	O
x2	O
]	O
−	O
e	O
[	O
x	O
]	O
2.	O
for	O
two	O
random	O
variables	O
x	O
and	O
y	O
,	O
the	O
covariance	B
is	O
deﬁned	O
by	O
cov	O
[	O
x	O
,	O
y	O
]	O
=	O
ex	O
,	O
y	O
[	O
{	O
x	O
−	O
e	O
[	O
x	O
]	O
}	O
{	O
y	O
−	O
e	O
[	O
y	O
]	O
}	O
]	O
=	O
ex	O
,	O
y	O
[	O
xy	O
]	O
−	O
e	O
[	O
x	O
]	O
e	O
[	O
y	O
]	O
(	O
1.40	O
)	O
(	O
1.41	O
)	O
which	O
expresses	O
the	O
extent	O
to	O
which	O
x	O
and	O
y	O
vary	O
together	O
.	O
if	O
x	O
and	O
y	O
are	O
indepen-	O
dent	O
,	O
then	O
their	O
covariance	B
vanishes	O
.	O
in	O
the	O
case	O
of	O
two	O
vectors	O
of	O
random	O
variables	O
x	O
and	O
y	O
,	O
the	O
covariance	B
is	O
a	O
matrix	O
(	O
cid:8	O
)	O
{	O
x	O
−	O
e	O
[	O
x	O
]	O
}	O
{	O
yt	O
−	O
e	O
[	O
yt	O
]	O
}	O
(	O
cid:9	O
)	O
cov	O
[	O
x	O
,	O
y	O
]	O
=	O
ex	O
,	O
y	O
=	O
ex	O
,	O
y	O
[	O
xyt	O
]	O
−	O
e	O
[	O
x	O
]	O
e	O
[	O
yt	O
]	O
.	O
(	O
1.42	O
)	O
if	O
we	O
consider	O
the	O
covariance	B
of	O
the	O
components	O
of	O
a	O
vector	O
x	O
with	O
each	O
other	O
,	O
then	O
we	O
use	O
a	O
slightly	O
simpler	O
notation	O
cov	O
[	O
x	O
]	O
≡	O
cov	O
[	O
x	O
,	O
x	O
]	O
.	O
1.2.	O
probability	B
theory	O
21	O
1.2.3	O
bayesian	O
probabilities	O
so	O
far	O
in	O
this	O
chapter	O
,	O
we	O
have	O
viewed	O
probabilities	O
in	O
terms	O
of	O
the	O
frequencies	O
of	O
random	O
,	O
repeatable	O
events	O
.	O
we	O
shall	O
refer	O
to	O
this	O
as	O
the	O
classical	B
or	O
frequentist	B
interpretation	O
of	O
probability	B
.	O
now	O
we	O
turn	O
to	O
the	O
more	O
general	O
bayesian	O
view	O
,	O
in	O
which	O
probabilities	O
provide	O
a	O
quantiﬁcation	O
of	O
uncertainty	O
.	O
consider	O
an	O
uncertain	O
event	O
,	O
for	O
example	O
whether	O
the	O
moon	O
was	O
once	O
in	O
its	O
own	O
orbit	O
around	O
the	O
sun	O
,	O
or	O
whether	O
the	O
arctic	O
ice	O
cap	O
will	O
have	O
disappeared	O
by	O
the	O
end	O
of	O
the	O
century	O
.	O
these	O
are	O
not	O
events	O
that	O
can	O
be	O
repeated	O
numerous	O
times	O
in	O
order	O
to	O
deﬁne	O
a	O
notion	O
of	O
probability	B
as	O
we	O
did	O
earlier	O
in	O
the	O
context	O
of	O
boxes	O
of	O
fruit	O
.	O
nevertheless	O
,	O
we	O
will	O
generally	O
have	O
some	O
idea	O
,	O
for	O
example	O
,	O
of	O
how	O
quickly	O
we	O
think	O
the	O
polar	O
ice	O
is	O
melting	O
.	O
if	O
we	O
now	O
obtain	O
fresh	O
evidence	O
,	O
for	O
instance	O
from	O
a	O
new	O
earth	O
observation	O
satellite	O
gathering	O
novel	O
forms	O
of	O
diagnostic	O
information	O
,	O
we	O
may	O
revise	O
our	O
opinion	O
on	O
the	O
rate	O
of	O
ice	O
loss	O
.	O
our	O
assessment	O
of	O
such	O
matters	O
will	O
affect	O
the	O
actions	O
we	O
take	O
,	O
for	O
instance	O
the	O
extent	O
to	O
which	O
we	O
endeavour	O
to	O
reduce	O
the	O
emission	O
of	O
greenhouse	O
gasses	O
.	O
in	O
such	O
circumstances	O
,	O
we	O
would	O
like	O
to	O
be	O
able	O
to	O
quantify	O
our	O
expression	O
of	O
uncertainty	O
and	O
make	O
precise	O
revisions	O
of	O
uncertainty	O
in	O
the	O
light	O
of	O
new	O
evidence	O
,	O
as	O
well	O
as	O
subsequently	O
to	O
be	O
able	O
to	O
take	O
optimal	O
actions	O
or	O
decisions	O
as	O
a	O
consequence	O
.	O
this	O
can	O
all	O
be	O
achieved	O
through	O
the	O
elegant	O
,	O
and	O
very	O
general	O
,	O
bayesian	O
interpretation	O
of	O
probability	B
.	O
the	O
use	O
of	O
probability	B
to	O
represent	O
uncertainty	O
,	O
however	O
,	O
is	O
not	O
an	O
ad-hoc	O
choice	O
,	O
but	O
is	O
inevitable	O
if	O
we	O
are	O
to	O
respect	O
common	O
sense	O
while	O
making	O
rational	O
coherent	O
inferences	O
.	O
for	O
instance	O
,	O
cox	O
(	O
1946	O
)	O
showed	O
that	O
if	O
numerical	O
values	O
are	O
used	O
to	O
represent	O
degrees	O
of	O
belief	O
,	O
then	O
a	O
simple	O
set	O
of	O
axioms	O
encoding	O
common	O
sense	O
properties	O
of	O
such	O
beliefs	O
leads	O
uniquely	O
to	O
a	O
set	O
of	O
rules	O
for	O
manipulating	O
degrees	O
of	O
belief	O
that	O
are	O
equivalent	O
to	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
.	O
this	O
provided	O
the	O
ﬁrst	O
rigorous	O
proof	O
that	O
probability	B
theory	O
could	O
be	O
regarded	O
as	O
an	O
extension	O
of	O
boolean	O
logic	O
to	O
situations	O
involving	O
uncertainty	O
(	O
jaynes	O
,	O
2003	O
)	O
.	O
numerous	O
other	O
authors	O
have	O
proposed	O
different	O
sets	O
of	O
properties	O
or	O
axioms	O
that	O
such	O
measures	O
of	O
uncertainty	O
should	O
satisfy	O
(	O
ramsey	O
,	O
1931	O
;	O
good	O
,	O
1950	O
;	O
savage	O
,	O
1961	O
;	O
definetti	O
,	O
1970	O
;	O
lindley	O
,	O
1982	O
)	O
.	O
in	O
each	O
case	O
,	O
the	O
resulting	O
numerical	O
quantities	O
behave	O
pre-	O
cisely	O
according	O
to	O
the	O
rules	O
of	O
probability	B
.	O
it	O
is	O
therefore	O
natural	O
to	O
refer	O
to	O
these	O
quantities	O
as	O
(	O
bayesian	O
)	O
probabilities	O
.	O
in	O
the	O
ﬁeld	O
of	O
pattern	O
recognition	O
,	O
too	O
,	O
it	O
is	O
helpful	O
to	O
have	O
a	O
more	O
general	O
no-	O
thomas	O
bayes	O
1701–1761	O
thomas	O
bayes	O
was	O
born	O
in	O
tun-	O
bridge	O
wells	O
and	O
was	O
a	O
clergyman	O
as	O
well	O
as	O
an	O
amateur	O
scientist	O
and	O
a	O
mathematician	O
.	O
he	O
studied	O
logic	O
and	O
theology	O
at	O
edinburgh	O
univer-	O
sity	O
and	O
was	O
elected	O
fellow	O
of	O
the	O
royal	O
society	O
in	O
1742.	O
during	O
the	O
18th	O
century	O
,	O
is-	O
sues	O
regarding	O
probability	B
arose	O
in	O
connection	O
with	O
gambling	O
and	O
with	O
the	O
new	O
concept	O
of	O
insurance	O
.	O
one	O
particularly	O
important	O
problem	O
concerned	O
so-called	O
in-	O
verse	O
probability	B
.	O
a	O
solution	O
was	O
proposed	O
by	O
thomas	O
bayes	O
in	O
his	O
paper	O
‘	O
essay	O
towards	O
solving	O
a	O
problem	O
in	O
the	O
doctrine	O
of	O
chances	O
’	O
,	O
which	O
was	O
published	O
in	O
1764	O
,	O
some	O
three	O
years	O
after	O
his	O
death	O
,	O
in	O
the	O
philo-	O
sophical	O
transactions	O
of	O
the	O
royal	O
society	O
.	O
in	O
fact	O
,	O
bayes	O
only	O
formulated	O
his	O
theory	B
for	O
the	O
case	O
of	O
a	O
uni-	O
form	O
prior	B
,	O
and	O
it	O
was	O
pierre-simon	O
laplace	O
who	O
inde-	O
pendently	O
rediscovered	O
the	O
theory	B
in	O
general	O
form	O
and	O
who	O
demonstrated	O
its	O
broad	O
applicability	O
.	O
22	O
1.	O
introduction	O
tion	O
of	O
probability	B
.	O
consider	O
the	O
example	O
of	O
polynomial	B
curve	I
ﬁtting	I
discussed	O
in	O
section	O
1.1.	O
it	O
seems	O
reasonable	O
to	O
apply	O
the	O
frequentist	B
notion	O
of	O
probability	B
to	O
the	O
random	O
values	O
of	O
the	O
observed	O
variables	O
tn	O
.	O
however	O
,	O
we	O
would	O
like	O
to	O
address	O
and	O
quantify	O
the	O
uncertainty	O
that	O
surrounds	O
the	O
appropriate	O
choice	O
for	O
the	O
model	O
param-	O
eters	O
w.	O
we	O
shall	O
see	O
that	O
,	O
from	O
a	O
bayesian	O
perspective	O
,	O
we	O
can	O
use	O
the	O
machinery	O
of	O
probability	B
theory	O
to	O
describe	O
the	O
uncertainty	O
in	O
model	O
parameters	O
such	O
as	O
w	O
,	O
or	O
indeed	O
in	O
the	O
choice	O
of	O
model	O
itself	O
.	O
bayes	O
’	O
theorem	O
now	O
acquires	O
a	O
new	O
signiﬁcance	O
.	O
recall	O
that	O
in	O
the	O
boxes	O
of	O
fruit	O
example	O
,	O
the	O
observation	O
of	O
the	O
identity	O
of	O
the	O
fruit	O
provided	O
relevant	O
information	O
that	O
altered	O
the	O
probability	B
that	O
the	O
chosen	O
box	O
was	O
the	O
red	O
one	O
.	O
in	O
that	O
example	O
,	O
bayes	O
’	O
theorem	O
was	O
used	O
to	O
convert	O
a	O
prior	B
probability	O
into	O
a	O
posterior	B
probability	I
by	O
incorporating	O
the	O
evidence	O
provided	O
by	O
the	O
observed	O
data	O
.	O
as	O
we	O
shall	O
see	O
in	O
detail	O
later	O
,	O
we	O
can	O
adopt	O
a	O
similar	O
approach	O
when	O
making	O
inferences	O
about	O
quantities	O
such	O
as	O
the	O
parameters	O
w	O
in	O
the	O
polynomial	B
curve	I
ﬁtting	I
example	O
.	O
we	O
capture	O
our	O
assumptions	O
about	O
w	O
,	O
before	O
observing	O
the	O
data	O
,	O
in	O
the	O
form	O
of	O
a	O
prior	B
probability	O
distribution	O
p	O
(	O
w	O
)	O
.	O
the	O
effect	O
of	O
the	O
observed	O
data	O
d	O
=	O
{	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
}	O
is	O
expressed	O
through	O
the	O
conditional	B
probability	I
p	O
(	O
d|w	O
)	O
,	O
and	O
we	O
shall	O
see	O
later	O
,	O
in	O
section	O
1.2.5	O
,	O
how	O
this	O
can	O
be	O
represented	O
explicitly	O
.	O
bayes	O
’	O
theorem	O
,	O
which	O
takes	O
the	O
form	O
p	O
(	O
w|d	O
)	O
=	O
p	O
(	O
d|w	O
)	O
p	O
(	O
w	O
)	O
p	O
(	O
d	O
)	O
(	O
1.43	O
)	O
then	O
allows	O
us	O
to	O
evaluate	O
the	O
uncertainty	O
in	O
w	O
after	O
we	O
have	O
observed	O
d	O
in	O
the	O
form	O
of	O
the	O
posterior	B
probability	I
p	O
(	O
w|d	O
)	O
.	O
the	O
quantity	O
p	O
(	O
d|w	O
)	O
on	O
the	O
right-hand	O
side	O
of	O
bayes	O
’	O
theorem	O
is	O
evaluated	O
for	O
the	O
observed	O
data	O
set	O
d	O
and	O
can	O
be	O
viewed	O
as	O
a	O
function	O
of	O
the	O
parameter	O
vector	O
w	O
,	O
in	O
which	O
case	O
it	O
is	O
called	O
the	O
likelihood	B
function	I
.	O
it	O
expresses	O
how	O
probable	O
the	O
observed	O
data	O
set	O
is	O
for	O
different	O
settings	O
of	O
the	O
parameter	O
vector	O
w.	O
note	O
that	O
the	O
likelihood	O
is	O
not	O
a	O
probability	B
distribution	O
over	O
w	O
,	O
and	O
its	O
integral	O
with	O
respect	O
to	O
w	O
does	O
not	O
(	O
necessarily	O
)	O
equal	O
one	O
.	O
given	O
this	O
deﬁnition	O
of	O
likelihood	O
,	O
we	O
can	O
state	O
bayes	O
’	O
theorem	O
in	O
words	O
posterior	O
∝	O
likelihood	O
×	O
prior	B
(	O
1.44	O
)	O
where	O
all	O
of	O
these	O
quantities	O
are	O
viewed	O
as	O
functions	O
of	O
w.	O
the	O
denominator	O
in	O
(	O
1.43	O
)	O
is	O
the	O
normalization	O
constant	O
,	O
which	O
ensures	O
that	O
the	O
posterior	O
distribution	O
on	O
the	O
left-hand	O
side	O
is	O
a	O
valid	O
probability	B
density	O
and	O
integrates	O
to	O
one	O
.	O
indeed	O
,	O
integrating	O
both	O
sides	O
of	O
(	O
1.43	O
)	O
with	O
respect	O
to	O
w	O
,	O
we	O
can	O
express	O
the	O
denominator	O
in	O
bayes	O
’	O
theorem	O
in	O
terms	O
of	O
the	O
prior	B
distribution	O
and	O
the	O
likelihood	B
function	I
(	O
cid:6	O
)	O
p	O
(	O
d	O
)	O
=	O
p	O
(	O
d|w	O
)	O
p	O
(	O
w	O
)	O
dw	O
.	O
(	O
1.45	O
)	O
in	O
both	O
the	O
bayesian	O
and	O
frequentist	B
paradigms	O
,	O
the	O
likelihood	B
function	I
p	O
(	O
d|w	O
)	O
plays	O
a	O
central	O
role	O
.	O
however	O
,	O
the	O
manner	O
in	O
which	O
it	O
is	O
used	O
is	O
fundamentally	O
dif-	O
ferent	O
in	O
the	O
two	O
approaches	O
.	O
in	O
a	O
frequentist	B
setting	O
,	O
w	O
is	O
considered	O
to	O
be	O
a	O
ﬁxed	O
parameter	O
,	O
whose	O
value	O
is	O
determined	O
by	O
some	O
form	O
of	O
‘	O
estimator	O
’	O
,	O
and	O
error	B
bars	O
1.2.	O
probability	B
theory	O
23	O
on	O
this	O
estimate	O
are	O
obtained	O
by	O
considering	O
the	O
distribution	O
of	O
possible	O
data	O
sets	O
d.	O
by	O
contrast	O
,	O
from	O
the	O
bayesian	O
viewpoint	O
there	O
is	O
only	O
a	O
single	O
data	O
set	O
d	O
(	O
namely	O
the	O
one	O
that	O
is	O
actually	O
observed	O
)	O
,	O
and	O
the	O
uncertainty	O
in	O
the	O
parameters	O
is	O
expressed	O
through	O
a	O
probability	B
distribution	O
over	O
w.	O
a	O
widely	O
used	O
frequentist	B
estimator	O
is	O
maximum	B
likelihood	I
,	O
in	O
which	O
w	O
is	O
set	O
to	O
the	O
value	O
that	O
maximizes	O
the	O
likelihood	B
function	I
p	O
(	O
d|w	O
)	O
.	O
this	O
corresponds	O
to	O
choosing	O
the	O
value	O
of	O
w	O
for	O
which	O
the	O
probability	B
of	O
the	O
observed	O
data	O
set	O
is	O
maxi-	O
mized	O
.	O
in	O
the	O
machine	O
learning	O
literature	O
,	O
the	O
negative	O
log	O
of	O
the	O
likelihood	B
function	I
is	O
called	O
an	O
error	B
function	I
.	O
because	O
the	O
negative	O
logarithm	O
is	O
a	O
monotonically	O
de-	O
creasing	O
function	O
,	O
maximizing	O
the	O
likelihood	O
is	O
equivalent	O
to	O
minimizing	O
the	O
error	B
.	O
one	O
approach	O
to	O
determining	O
frequentist	B
error	O
bars	O
is	O
the	O
bootstrap	B
(	O
efron	O
,	O
1979	O
;	O
hastie	O
et	O
al.	O
,	O
2001	O
)	O
,	O
in	O
which	O
multiple	O
data	O
sets	O
are	O
created	O
as	O
follows	O
.	O
suppose	O
our	O
original	O
data	O
set	O
consists	O
of	O
n	O
data	O
points	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
.	O
we	O
can	O
create	O
a	O
new	O
data	O
set	O
xb	O
by	O
drawing	O
n	O
points	O
at	O
random	O
from	O
x	O
,	O
with	O
replacement	O
,	O
so	O
that	O
some	O
points	O
in	O
x	O
may	O
be	O
replicated	O
in	O
xb	O
,	O
whereas	O
other	O
points	O
in	O
x	O
may	O
be	O
absent	O
from	O
xb	O
.	O
this	O
process	O
can	O
be	O
repeated	O
l	O
times	O
to	O
generate	O
l	O
data	O
sets	O
each	O
of	O
size	O
n	O
and	O
each	O
obtained	O
by	O
sampling	O
from	O
the	O
original	O
data	O
set	O
x.	O
the	O
statistical	O
accuracy	O
of	O
parameter	O
estimates	O
can	O
then	O
be	O
evaluated	O
by	O
looking	O
at	O
the	O
variability	O
of	O
predictions	O
between	O
the	O
different	O
bootstrap	B
data	O
sets	O
.	O
one	O
advantage	O
of	O
the	O
bayesian	O
viewpoint	O
is	O
that	O
the	O
inclusion	O
of	O
prior	B
knowl-	O
edge	B
arises	O
naturally	O
.	O
suppose	O
,	O
for	O
instance	O
,	O
that	O
a	O
fair-looking	O
coin	O
is	O
tossed	O
three	O
times	O
and	O
lands	O
heads	O
each	O
time	O
.	O
a	O
classical	B
maximum	O
likelihood	O
estimate	O
of	O
the	O
probability	B
of	O
landing	O
heads	O
would	O
give	O
1	O
,	O
implying	O
that	O
all	O
future	O
tosses	O
will	O
land	O
heads	O
!	O
by	O
contrast	O
,	O
a	O
bayesian	O
approach	O
with	O
any	O
reasonable	O
prior	B
will	O
lead	O
to	O
a	O
much	O
less	O
extreme	O
conclusion	O
.	O
there	O
has	O
been	O
much	O
controversy	O
and	O
debate	O
associated	O
with	O
the	O
relative	B
mer-	O
its	O
of	O
the	O
frequentist	B
and	O
bayesian	O
paradigms	O
,	O
which	O
have	O
not	O
been	O
helped	O
by	O
the	O
fact	O
that	O
there	O
is	O
no	O
unique	O
frequentist	B
,	O
or	O
even	O
bayesian	O
,	O
viewpoint	O
.	O
for	O
instance	O
,	O
one	O
common	O
criticism	O
of	O
the	O
bayesian	O
approach	O
is	O
that	O
the	O
prior	B
distribution	O
is	O
of-	O
ten	O
selected	O
on	O
the	O
basis	O
of	O
mathematical	O
convenience	O
rather	O
than	O
as	O
a	O
reﬂection	O
of	O
any	O
prior	B
beliefs	O
.	O
even	O
the	O
subjective	O
nature	O
of	O
the	O
conclusions	O
through	O
their	O
de-	O
pendence	O
on	O
the	O
choice	O
of	O
prior	B
is	O
seen	O
by	O
some	O
as	O
a	O
source	O
of	O
difﬁculty	O
.	O
reducing	O
the	O
dependence	O
on	O
the	O
prior	B
is	O
one	O
motivation	O
for	O
so-called	O
noninformative	B
priors	O
.	O
however	O
,	O
these	O
lead	O
to	O
difﬁculties	O
when	O
comparing	O
different	O
models	O
,	O
and	O
indeed	O
bayesian	O
methods	O
based	O
on	O
poor	O
choices	O
of	O
prior	B
can	O
give	O
poor	O
results	O
with	O
high	O
conﬁdence	O
.	O
frequentist	B
evaluation	O
methods	O
offer	O
some	O
protection	O
from	O
such	O
prob-	O
lems	O
,	O
and	O
techniques	O
such	O
as	O
cross-validation	B
remain	O
useful	O
in	O
areas	O
such	O
as	O
model	B
comparison	I
.	O
this	O
book	O
places	O
a	O
strong	O
emphasis	O
on	O
the	O
bayesian	O
viewpoint	O
,	O
reﬂecting	O
the	O
huge	O
growth	O
in	O
the	O
practical	O
importance	O
of	O
bayesian	O
methods	O
in	O
the	O
past	O
few	O
years	O
,	O
while	O
also	O
discussing	O
useful	O
frequentist	B
concepts	O
as	O
required	O
.	O
although	O
the	O
bayesian	O
framework	O
has	O
its	O
origins	O
in	O
the	O
18th	O
century	O
,	O
the	O
prac-	O
tical	O
application	O
of	O
bayesian	O
methods	O
was	O
for	O
a	O
long	O
time	O
severely	O
limited	O
by	O
the	O
difﬁculties	O
in	O
carrying	O
through	O
the	O
full	O
bayesian	O
procedure	O
,	O
particularly	O
the	O
need	O
to	O
marginalize	O
(	O
sum	O
or	O
integrate	O
)	O
over	O
the	O
whole	O
of	O
parameter	O
space	O
,	O
which	O
,	O
as	O
we	O
shall	O
section	O
2.1	O
section	O
2.4.3	O
section	O
1.3	O
24	O
1.	O
introduction	O
see	O
,	O
is	O
required	O
in	O
order	O
to	O
make	O
predictions	O
or	O
to	O
compare	O
different	O
models	O
.	O
the	O
development	O
of	O
sampling	B
methods	I
,	O
such	O
as	O
markov	O
chain	O
monte	O
carlo	O
(	O
discussed	O
in	O
chapter	O
11	O
)	O
along	O
with	O
dramatic	O
improvements	O
in	O
the	O
speed	O
and	O
memory	O
capacity	O
of	O
computers	O
,	O
opened	O
the	O
door	O
to	O
the	O
practical	O
use	O
of	O
bayesian	O
techniques	O
in	O
an	O
im-	O
pressive	O
range	O
of	O
problem	O
domains	O
.	O
monte	O
carlo	O
methods	O
are	O
very	O
ﬂexible	O
and	O
can	O
be	O
applied	O
to	O
a	O
wide	O
range	O
of	O
models	O
.	O
however	O
,	O
they	O
are	O
computationally	O
intensive	O
and	O
have	O
mainly	O
been	O
used	O
for	O
small-scale	O
problems	O
.	O
more	O
recently	O
,	O
highly	O
efﬁcient	O
deterministic	O
approximation	O
schemes	O
such	O
as	O
variational	B
bayes	O
and	O
expectation	B
propagation	I
(	O
discussed	O
in	O
chapter	O
10	O
)	O
have	O
been	O
developed	O
.	O
these	O
offer	O
a	O
complementary	O
alternative	O
to	O
sampling	B
methods	I
and	O
have	O
allowed	O
bayesian	O
techniques	O
to	O
be	O
used	O
in	O
large-scale	O
applications	O
(	O
blei	O
et	O
al.	O
,	O
2003	O
)	O
.	O
1.2.4	O
the	O
gaussian	O
distribution	O
we	O
shall	O
devote	O
the	O
whole	O
of	O
chapter	O
2	O
to	O
a	O
study	O
of	O
various	O
probability	B
dis-	O
tributions	O
and	O
their	O
key	O
properties	O
.	O
it	O
is	O
convenient	O
,	O
however	O
,	O
to	O
introduce	O
here	O
one	O
of	O
the	O
most	O
important	O
probability	B
distributions	O
for	O
continuous	O
variables	O
,	O
called	O
the	O
normal	O
or	O
gaussian	O
distribution	O
.	O
we	O
shall	O
make	O
extensive	O
use	O
of	O
this	O
distribution	O
in	O
the	O
remainder	O
of	O
this	O
chapter	O
and	O
indeed	O
throughout	O
much	O
of	O
the	O
book	O
.	O
for	O
the	O
case	O
of	O
a	O
single	O
real-valued	O
variable	O
x	O
,	O
the	O
gaussian	O
distribution	O
is	O
de-	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
ﬁned	O
by	O
n	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
x|µ	O
,	O
σ2	O
=	O
1	O
(	O
2πσ2	O
)	O
1/2	O
exp	O
−	O
1	O
2σ2	O
(	O
x	O
−	O
µ	O
)	O
2	O
(	O
1.46	O
)	O
which	O
is	O
governed	O
by	O
two	O
parameters	O
:	O
µ	O
,	O
called	O
the	O
mean	B
,	O
and	O
σ2	O
,	O
called	O
the	O
vari-	O
ance	O
.	O
the	O
square	O
root	O
of	O
the	O
variance	B
,	O
given	O
by	O
σ	O
,	O
is	O
called	O
the	O
standard	B
deviation	I
,	O
and	O
the	O
reciprocal	O
of	O
the	O
variance	B
,	O
written	O
as	O
β	O
=	O
1/σ2	O
,	O
is	O
called	O
the	O
precision	O
.	O
we	O
shall	O
see	O
the	O
motivation	O
for	O
these	O
terms	O
shortly	O
.	O
figure	O
1.13	O
shows	O
a	O
plot	O
of	O
the	O
gaussian	O
distribution	O
.	O
from	O
the	O
form	O
of	O
(	O
1.46	O
)	O
we	O
see	O
that	O
the	O
gaussian	O
distribution	O
satisﬁes	O
n	O
(	O
x|µ	O
,	O
σ2	O
)	O
>	O
0	O
.	O
(	O
1.47	O
)	O
exercise	O
1.7	O
also	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
the	O
gaussian	O
is	O
normalized	O
,	O
so	O
that	O
pierre-simon	O
laplace	O
1749–1827	O
it	O
is	O
said	O
that	O
laplace	O
was	O
seri-	O
ously	O
lacking	O
in	O
modesty	O
and	O
at	O
one	O
point	O
declared	O
himself	O
to	O
be	O
the	O
best	O
mathematician	O
in	O
france	O
at	O
the	O
time	O
,	O
a	O
claim	O
that	O
was	O
arguably	O
true	O
.	O
as	O
well	O
as	O
being	O
proliﬁc	O
in	O
mathe-	O
matics	O
,	O
he	O
also	O
made	O
numerous	O
contributions	O
to	O
as-	O
tronomy	O
,	O
including	O
the	O
nebular	O
hypothesis	O
by	O
which	O
the	O
earth	O
is	O
thought	O
to	O
have	O
formed	O
from	O
the	O
condensa-	O
tion	O
and	O
cooling	O
of	O
a	O
large	O
rotating	O
disk	O
of	O
gas	O
and	O
dust	O
.	O
in	O
1812	O
he	O
published	O
the	O
ﬁrst	O
edition	O
of	O
th´eorie	O
analytique	O
des	O
probabilit´es	O
,	O
in	O
which	O
laplace	O
states	O
that	O
“	O
probability	B
theory	O
is	O
nothing	O
but	O
common	O
sense	O
reduced	O
to	O
calculation	O
”	O
.	O
this	O
work	O
included	O
a	O
discus-	O
sion	B
of	O
the	O
inverse	B
probability	O
calculation	O
(	O
later	O
termed	O
bayes	O
’	O
theorem	O
by	O
poincar´e	O
)	O
,	O
which	O
he	O
used	O
to	O
solve	O
problems	O
in	O
life	O
expectancy	O
,	O
jurisprudence	O
,	O
planetary	O
masses	O
,	O
triangulation	O
,	O
and	O
error	B
estimation	O
.	O
figure	O
1.13	O
plot	O
of	O
the	O
univariate	O
gaussian	O
showing	O
the	O
mean	B
µ	O
and	O
the	O
standard	B
deviation	I
σ.	O
n	O
(	O
x|µ	O
,	O
σ2	O
)	O
1.2.	O
probability	B
theory	O
25	O
2σ	O
µ	O
dx	O
=	O
1	O
.	O
(	O
cid:6	O
)	O
∞	O
−∞	O
n	O
(	O
cid:10	O
)	O
x|µ	O
,	O
σ2	O
(	O
cid:11	O
)	O
x	O
(	O
1.48	O
)	O
thus	O
(	O
1.46	O
)	O
satisﬁes	O
the	O
two	O
requirements	O
for	O
a	O
valid	O
probability	B
density	O
.	O
we	O
can	O
readily	O
ﬁnd	O
expectations	O
of	O
functions	O
of	O
x	O
under	O
the	O
gaussian	O
distribu-	O
tion	O
.	O
in	O
particular	O
,	O
the	O
average	O
value	O
of	O
x	O
is	O
given	O
by	O
(	O
cid:11	O
)	O
x|µ	O
,	O
σ2	O
(	O
cid:6	O
)	O
∞	O
−∞	O
n	O
(	O
cid:10	O
)	O
n	O
(	O
cid:10	O
)	O
(	O
cid:6	O
)	O
∞	O
(	O
cid:11	O
)	O
x|µ	O
,	O
σ2	O
exercise	O
1.8	O
exercise	O
1.9	O
e	O
[	O
x	O
]	O
=	O
x	O
dx	O
=	O
µ	O
.	O
(	O
1.49	O
)	O
because	O
the	O
parameter	O
µ	O
represents	O
the	O
average	O
value	O
of	O
x	O
under	O
the	O
distribution	O
,	O
it	O
is	O
referred	O
to	O
as	O
the	O
mean	B
.	O
similarly	O
,	O
for	O
the	O
second	B
order	I
moment	O
e	O
[	O
x2	O
]	O
=	O
−∞	O
x2	O
dx	O
=	O
µ2	O
+	O
σ2	O
.	O
(	O
1.50	O
)	O
from	O
(	O
1.49	O
)	O
and	O
(	O
1.50	O
)	O
,	O
it	O
follows	O
that	O
the	O
variance	B
of	O
x	O
is	O
given	O
by	O
var	O
[	O
x	O
]	O
=	O
e	O
[	O
x2	O
]	O
−	O
e	O
[	O
x	O
]	O
2	O
=	O
σ2	O
(	O
1.51	O
)	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
and	O
hence	O
σ2	O
is	O
referred	O
to	O
as	O
the	O
variance	B
parameter	O
.	O
the	O
maximum	O
of	O
a	O
distribution	O
is	O
known	O
as	O
its	O
mode	O
.	O
for	O
a	O
gaussian	O
,	O
the	O
mode	O
coincides	O
with	O
the	O
mean	B
.	O
we	O
are	O
also	O
interested	O
in	O
the	O
gaussian	O
distribution	O
deﬁned	O
over	O
a	O
d-dimensional	O
vector	O
x	O
of	O
continuous	O
variables	O
,	O
which	O
is	O
given	O
by	O
n	O
(	O
x|µ	O
,	O
σ	O
)	O
=	O
1	O
1	O
(	O
x	O
−	O
µ	O
)	O
tς	O
−1	O
(	O
x	O
−	O
µ	O
)	O
|σ|1/2	O
exp	O
(	O
2π	O
)	O
d/2	O
(	O
1.52	O
)	O
where	O
the	O
d-dimensional	O
vector	O
µ	O
is	O
called	O
the	O
mean	B
,	O
the	O
d	O
×	O
d	O
matrix	O
σ	O
is	O
called	O
the	O
covariance	B
,	O
and	O
|σ|	O
denotes	O
the	O
determinant	O
of	O
σ.	O
we	O
shall	O
make	O
use	O
of	O
the	O
multivariate	O
gaussian	O
distribution	O
brieﬂy	O
in	O
this	O
chapter	O
,	O
although	O
its	O
properties	O
will	O
be	O
studied	O
in	O
detail	O
in	O
section	O
2.3	O
.	O
−1	O
2	O
26	O
1.	O
introduction	O
figure	O
1.14	O
illustration	O
of	O
the	O
likelihood	B
function	I
for	O
a	O
gaussian	O
distribution	O
,	O
shown	O
by	O
the	O
red	O
curve	O
.	O
here	O
the	O
black	O
points	O
de-	O
note	O
a	O
data	O
set	O
of	O
values	O
{	O
xn	O
}	O
,	O
and	O
the	O
likelihood	B
function	I
given	O
by	O
(	O
1.53	O
)	O
corresponds	O
to	O
the	O
product	O
of	O
the	O
blue	O
values	O
.	O
maximizing	O
the	O
likelihood	O
in-	O
volves	O
adjusting	O
the	O
mean	B
and	O
vari-	O
ance	O
of	O
the	O
gaussian	O
so	O
as	O
to	O
maxi-	O
mize	O
this	O
product	O
.	O
p	O
(	O
x	O
)	O
n	O
(	O
xn|µ	O
,	O
σ2	O
)	O
xn	O
x	O
now	O
suppose	O
that	O
we	O
have	O
a	O
data	O
set	O
of	O
observations	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
t	O
,	O
rep-	O
resenting	O
n	O
observations	O
of	O
the	O
scalar	O
variable	O
x.	O
note	O
that	O
we	O
are	O
using	O
the	O
type-	O
face	O
x	O
to	O
distinguish	O
this	O
from	O
a	O
single	O
observation	O
of	O
the	O
vector-valued	O
variable	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
t	O
,	O
which	O
we	O
denote	O
by	O
x.	O
we	O
shall	O
suppose	O
that	O
the	O
observations	O
are	O
drawn	O
independently	O
from	O
a	O
gaussian	O
distribution	O
whose	O
mean	B
µ	O
and	O
variance	B
σ2	O
are	O
unknown	O
,	O
and	O
we	O
would	O
like	O
to	O
determine	O
these	O
parameters	O
from	O
the	O
data	O
set	O
.	O
data	O
points	O
that	O
are	O
drawn	O
independently	O
from	O
the	O
same	O
distribution	O
are	O
said	O
to	O
be	O
independent	B
and	O
identically	O
distributed	O
,	O
which	O
is	O
often	O
abbreviated	O
to	O
i.i.d	O
.	O
we	O
have	O
seen	O
that	O
the	O
joint	O
probability	B
of	O
two	O
independent	B
events	O
is	O
given	O
by	O
the	O
product	O
of	O
the	O
marginal	B
probabilities	O
for	O
each	O
event	O
separately	O
.	O
because	O
our	O
data	O
set	O
x	O
is	O
i.i.d.	B
,	O
we	O
can	O
therefore	O
write	O
the	O
probability	B
of	O
the	O
data	O
set	O
,	O
given	O
µ	O
and	O
σ2	O
,	O
in	O
the	O
form	O
n	O
(	O
cid:14	O
)	O
n	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
p	O
(	O
x|µ	O
,	O
σ2	O
)	O
=	O
xn|µ	O
,	O
σ2	O
.	O
(	O
1.53	O
)	O
section	O
1.2.5	O
n=1	O
when	O
viewed	O
as	O
a	O
function	O
of	O
µ	O
and	O
σ2	O
,	O
this	O
is	O
the	O
likelihood	B
function	I
for	O
the	O
gaus-	O
sian	O
and	O
is	O
interpreted	O
diagrammatically	O
in	O
figure	O
1.14.	O
one	O
common	O
criterion	O
for	O
determining	O
the	O
parameters	O
in	O
a	O
probability	B
distribu-	O
tion	O
using	O
an	O
observed	O
data	O
set	O
is	O
to	O
ﬁnd	O
the	O
parameter	O
values	O
that	O
maximize	O
the	O
likelihood	B
function	I
.	O
this	O
might	O
seem	O
like	O
a	O
strange	O
criterion	O
because	O
,	O
from	O
our	O
fore-	O
going	O
discussion	O
of	O
probability	B
theory	O
,	O
it	O
would	O
seem	O
more	O
natural	O
to	O
maximize	O
the	O
probability	B
of	O
the	O
parameters	O
given	O
the	O
data	O
,	O
not	O
the	O
probability	B
of	O
the	O
data	O
given	O
the	O
parameters	O
.	O
in	O
fact	O
,	O
these	O
two	O
criteria	O
are	O
related	O
,	O
as	O
we	O
shall	O
discuss	O
in	O
the	O
context	O
of	O
curve	B
ﬁtting	I
.	O
for	O
the	O
moment	O
,	O
however	O
,	O
we	O
shall	O
determine	O
values	O
for	O
the	O
unknown	O
parame-	O
ters	O
µ	O
and	O
σ2	O
in	O
the	O
gaussian	O
by	O
maximizing	O
the	O
likelihood	B
function	I
(	O
1.53	O
)	O
.	O
in	O
prac-	O
tice	O
,	O
it	O
is	O
more	O
convenient	O
to	O
maximize	O
the	O
log	O
of	O
the	O
likelihood	B
function	I
.	O
because	O
the	O
logarithm	O
is	O
a	O
monotonically	O
increasing	O
function	O
of	O
its	O
argument	O
,	O
maximization	O
of	O
the	O
log	O
of	O
a	O
function	O
is	O
equivalent	O
to	O
maximization	O
of	O
the	O
function	O
itself	O
.	O
taking	O
the	O
log	O
not	O
only	O
simpliﬁes	O
the	O
subsequent	O
mathematical	O
analysis	O
,	O
but	O
it	O
also	O
helps	O
numerically	O
because	O
the	O
product	O
of	O
a	O
large	O
number	O
of	O
small	O
probabilities	O
can	O
easily	O
underﬂow	O
the	O
numerical	O
precision	O
of	O
the	O
computer	O
,	O
and	O
this	O
is	O
resolved	O
by	O
computing	O
instead	O
the	O
sum	O
of	O
the	O
log	O
probabilities	O
.	O
from	O
(	O
1.46	O
)	O
and	O
(	O
1.53	O
)	O
,	O
the	O
log	O
likelihood	O
exercise	O
1.11	O
section	O
1.1	O
exercise	O
1.12	O
function	O
can	O
be	O
written	O
in	O
the	O
form	O
(	O
cid:10	O
)	O
x|µ	O
,	O
σ2	O
(	O
cid:11	O
)	O
ln	O
p	O
=	O
−	O
1	O
2σ2	O
1.2.	O
probability	B
theory	O
27	O
(	O
xn	O
−	O
µ	O
)	O
2	O
−	O
n	O
2	O
ln	O
σ2	O
−	O
n	O
2	O
ln	O
(	O
2π	O
)	O
.	O
(	O
1.54	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
maximizing	O
(	O
1.54	O
)	O
with	O
respect	O
to	O
µ	O
,	O
we	O
obtain	O
the	O
maximum	B
likelihood	I
solution	O
given	O
by	O
µml	O
=	O
(	O
1.55	O
)	O
which	O
is	O
the	O
sample	B
mean	I
,	O
i.e.	O
,	O
the	O
mean	B
of	O
the	O
observed	O
values	O
{	O
xn	O
}	O
.	O
similarly	O
,	O
maximizing	O
(	O
1.54	O
)	O
with	O
respect	O
to	O
σ2	O
,	O
we	O
obtain	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
variance	B
in	O
the	O
form	O
xn	O
n=1	O
σ2	O
ml	O
=	O
1	O
n	O
(	O
xn	O
−	O
µml	O
)	O
2	O
(	O
1.56	O
)	O
n	O
(	O
cid:2	O
)	O
1	O
n	O
n	O
(	O
cid:2	O
)	O
n=1	O
which	O
is	O
the	O
sample	B
variance	I
measured	O
with	O
respect	O
to	O
the	O
sample	B
mean	I
µml	O
.	O
note	O
that	O
we	O
are	O
performing	O
a	O
joint	O
maximization	O
of	O
(	O
1.54	O
)	O
with	O
respect	O
to	O
µ	O
and	O
σ2	O
,	O
but	O
in	O
the	O
case	O
of	O
the	O
gaussian	O
distribution	O
the	O
solution	O
for	O
µ	O
decouples	O
from	O
that	O
for	O
σ2	O
so	O
that	O
we	O
can	O
ﬁrst	O
evaluate	O
(	O
1.55	O
)	O
and	O
then	O
subsequently	O
use	O
this	O
result	O
to	O
evaluate	O
(	O
1.56	O
)	O
.	O
later	O
in	O
this	O
chapter	O
,	O
and	O
also	O
in	O
subsequent	O
chapters	O
,	O
we	O
shall	O
highlight	O
the	O
sig-	O
niﬁcant	O
limitations	O
of	O
the	O
maximum	B
likelihood	I
approach	O
.	O
here	O
we	O
give	O
an	O
indication	O
of	O
the	O
problem	O
in	O
the	O
context	O
of	O
our	O
solutions	O
for	O
the	O
maximum	B
likelihood	I
param-	O
eter	O
settings	O
for	O
the	O
univariate	O
gaussian	O
distribution	O
.	O
in	O
particular	O
,	O
we	O
shall	O
show	O
that	O
the	O
maximum	B
likelihood	I
approach	O
systematically	O
underestimates	O
the	O
variance	B
of	O
the	O
distribution	O
.	O
this	O
is	O
an	O
example	O
of	O
a	O
phenomenon	O
called	O
bias	B
and	O
is	O
related	O
to	O
the	O
problem	O
of	O
over-ﬁtting	B
encountered	O
in	O
the	O
context	O
of	O
polynomial	B
curve	I
ﬁtting	I
.	O
we	O
ﬁrst	O
note	O
that	O
the	O
maximum	B
likelihood	I
solutions	O
µml	O
and	O
σ2	O
ml	O
are	O
functions	O
of	O
the	O
data	O
set	O
values	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
.	O
consider	O
the	O
expectations	O
of	O
these	O
quantities	O
with	O
respect	O
to	O
the	O
data	O
set	O
values	O
,	O
which	O
themselves	O
come	O
from	O
a	O
gaussian	O
distribution	O
with	O
parameters	O
µ	O
and	O
σ2	O
.	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
e	O
[	O
µml	O
]	O
=	O
µ	O
e	O
[	O
σ2	O
ml	O
]	O
=	O
n	O
−	O
1	O
n	O
σ2	O
(	O
1.57	O
)	O
(	O
1.58	O
)	O
so	O
that	O
on	O
average	O
the	O
maximum	B
likelihood	I
estimate	O
will	O
obtain	O
the	O
correct	O
mean	B
but	O
will	O
underestimate	O
the	O
true	O
variance	B
by	O
a	O
factor	O
(	O
n	O
−	O
1	O
)	O
/n	O
.	O
the	O
intuition	O
behind	O
this	O
result	O
is	O
given	O
by	O
figure	O
1.15.	O
from	O
(	O
1.58	O
)	O
it	O
follows	O
that	O
the	O
following	O
estimate	O
for	O
the	O
variance	B
parameter	O
is	O
unbiased	O
(	O
cid:4	O
)	O
σ2	O
=	O
n	O
n	O
−	O
1	O
σ2	O
ml	O
=	O
1	O
n	O
−	O
1	O
(	O
xn	O
−	O
µml	O
)	O
2	O
.	O
(	O
1.59	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
28	O
1.	O
introduction	O
figure	O
1.15	O
illustration	O
of	O
how	O
bias	B
arises	O
in	O
using	O
max-	O
imum	O
likelihood	O
to	O
determine	O
the	O
variance	B
of	O
a	O
gaussian	O
.	O
the	O
green	O
curve	O
shows	O
the	O
true	O
gaussian	O
distribution	O
from	O
which	O
data	O
is	O
generated	O
,	O
and	O
the	O
three	O
red	O
curves	O
show	O
the	O
gaussian	O
distributions	O
obtained	O
by	O
ﬁtting	O
to	O
three	O
data	O
sets	O
,	O
each	O
consist-	O
ing	O
of	O
two	O
data	O
points	O
shown	O
in	O
blue	O
,	O
us-	O
ing	O
the	O
maximum	B
likelihood	I
results	O
(	O
1.55	O
)	O
and	O
(	O
1.56	O
)	O
.	O
averaged	O
across	O
the	O
three	O
data	O
sets	O
,	O
the	O
mean	B
is	O
correct	O
,	O
but	O
the	O
variance	B
is	O
systematically	O
under-estimated	O
because	O
it	O
is	O
measured	O
relative	B
to	O
the	O
sample	B
mean	I
and	O
not	O
relative	B
to	O
the	O
true	O
mean	B
.	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
in	O
section	O
10.1.3	O
,	O
we	O
shall	O
see	O
how	O
this	O
result	O
arises	O
automatically	O
when	O
we	O
adopt	O
a	O
bayesian	O
approach	O
.	O
note	O
that	O
the	O
bias	B
of	O
the	O
maximum	B
likelihood	I
solution	O
becomes	O
less	O
signiﬁcant	O
as	O
the	O
number	O
n	O
of	O
data	O
points	O
increases	O
,	O
and	O
in	O
the	O
limit	O
n	O
→	O
∞	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
variance	B
equals	O
the	O
true	O
variance	B
of	O
the	O
distribution	O
that	O
generated	O
the	O
data	O
.	O
in	O
practice	O
,	O
for	O
anything	O
other	O
than	O
small	O
n	O
,	O
this	O
bias	B
will	O
not	O
prove	O
to	O
be	O
a	O
serious	O
problem	O
.	O
however	O
,	O
throughout	O
this	O
book	O
we	O
shall	O
be	O
interested	O
in	O
more	O
complex	O
models	O
with	O
many	O
parameters	O
,	O
for	O
which	O
the	O
bias	B
problems	O
asso-	O
ciated	O
with	O
maximum	B
likelihood	I
will	O
be	O
much	O
more	O
severe	O
.	O
in	O
fact	O
,	O
as	O
we	O
shall	O
see	O
,	O
the	O
issue	O
of	O
bias	B
in	O
maximum	B
likelihood	I
lies	O
at	O
the	O
root	O
of	O
the	O
over-ﬁtting	B
problem	O
that	O
we	O
encountered	O
earlier	O
in	O
the	O
context	O
of	O
polynomial	B
curve	I
ﬁtting	I
.	O
1.2.5	O
curve	B
ﬁtting	I
re-visited	O
we	O
have	O
seen	O
how	O
the	O
problem	O
of	O
polynomial	B
curve	I
ﬁtting	I
can	O
be	O
expressed	O
in	O
terms	O
of	O
error	B
minimization	O
.	O
here	O
we	O
return	O
to	O
the	O
curve	B
ﬁtting	I
example	O
and	O
view	O
it	O
from	O
a	O
probabilistic	O
perspective	O
,	O
thereby	O
gaining	O
some	O
insights	O
into	O
error	B
functions	O
and	O
regularization	B
,	O
as	O
well	O
as	O
taking	O
us	O
towards	O
a	O
full	O
bayesian	O
treatment	O
.	O
section	O
1.1	O
the	O
goal	O
in	O
the	O
curve	B
ﬁtting	I
problem	O
is	O
to	O
be	O
able	O
to	O
make	O
predictions	O
for	O
the	O
target	O
variable	O
t	O
given	O
some	O
new	O
value	O
of	O
the	O
input	O
variable	O
x	O
on	O
the	O
basis	O
of	O
a	O
set	O
of	O
training	B
data	O
comprising	O
n	O
input	O
values	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
t	O
and	O
their	O
corresponding	O
target	O
values	O
t	O
=	O
(	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
)	O
t.	O
we	O
can	O
express	O
our	O
uncertainty	O
over	O
the	O
value	O
of	O
the	O
target	O
variable	O
using	O
a	O
probability	B
distribution	O
.	O
for	O
this	O
purpose	O
,	O
we	O
shall	O
assume	O
that	O
,	O
given	O
the	O
value	O
of	O
x	O
,	O
the	O
corresponding	O
value	O
of	O
t	O
has	O
a	O
gaussian	O
distribution	O
with	O
a	O
mean	B
equal	O
to	O
the	O
value	O
y	O
(	O
x	O
,	O
w	O
)	O
of	O
the	O
polynomial	O
curve	O
given	O
by	O
(	O
1.1	O
)	O
.	O
thus	O
we	O
have	O
p	O
(	O
t|x	O
,	O
w	O
,	O
β	O
)	O
=	O
n	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
t|y	O
(	O
x	O
,	O
w	O
)	O
,	O
β	O
−1	O
(	O
1.60	O
)	O
where	O
,	O
for	O
consistency	O
with	O
the	O
notation	O
in	O
later	O
chapters	O
,	O
we	O
have	O
deﬁned	O
a	O
preci-	O
sion	B
parameter	O
β	O
corresponding	O
to	O
the	O
inverse	B
variance	O
of	O
the	O
distribution	O
.	O
this	O
is	O
illustrated	O
schematically	O
in	O
figure	O
1.16.	O
figure	O
1.16	O
schematic	O
illustration	O
of	O
a	O
gaus-	O
sian	O
conditional	B
distribution	O
for	O
t	O
given	O
x	O
given	O
by	O
(	O
1.60	O
)	O
,	O
in	O
which	O
the	O
mean	B
is	O
given	O
by	O
the	O
polyno-	O
mial	O
function	O
y	O
(	O
x	O
,	O
w	O
)	O
,	O
and	O
the	O
precision	O
is	O
given	O
by	O
the	O
parameter	O
β	O
,	O
which	O
is	O
related	O
to	O
the	O
vari-	O
ance	O
by	O
β−1	O
=	O
σ2	O
.	O
t	O
y	O
(	O
x0	O
,	O
w	O
)	O
1.2.	O
probability	B
theory	O
29	O
y	O
(	O
x	O
,	O
w	O
)	O
p	O
(	O
t|x0	O
,	O
w	O
,	O
β	O
)	O
x0	O
2σ	O
x	O
we	O
now	O
use	O
the	O
training	B
data	O
{	O
x	O
,	O
t	O
}	O
to	O
determine	O
the	O
values	O
of	O
the	O
unknown	O
parameters	O
w	O
and	O
β	O
by	O
maximum	B
likelihood	I
.	O
if	O
the	O
data	O
are	O
assumed	O
to	O
be	O
drawn	O
independently	O
from	O
the	O
distribution	O
(	O
1.60	O
)	O
,	O
then	O
the	O
likelihood	B
function	I
is	O
given	O
by	O
n	O
(	O
cid:14	O
)	O
n	O
(	O
cid:10	O
)	O
n=1	O
(	O
cid:11	O
)	O
p	O
(	O
t|x	O
,	O
w	O
,	O
β	O
)	O
=	O
tn|y	O
(	O
xn	O
,	O
w	O
)	O
,	O
β	O
−1	O
.	O
(	O
1.61	O
)	O
as	O
we	O
did	O
in	O
the	O
case	O
of	O
the	O
simple	O
gaussian	O
distribution	O
earlier	O
,	O
it	O
is	O
convenient	O
to	O
maximize	O
the	O
logarithm	O
of	O
the	O
likelihood	B
function	I
.	O
substituting	O
for	O
the	O
form	O
of	O
the	O
gaussian	O
distribution	O
,	O
given	O
by	O
(	O
1.46	O
)	O
,	O
we	O
obtain	O
the	O
log	O
likelihood	O
function	O
in	O
the	O
form	O
ln	O
p	O
(	O
t|x	O
,	O
w	O
,	O
β	O
)	O
=	O
−	O
β	O
2	O
{	O
y	O
(	O
xn	O
,	O
w	O
)	O
−	O
tn	O
}	O
2	O
+	O
n	O
2	O
ln	O
β	O
−	O
n	O
2	O
ln	O
(	O
2π	O
)	O
.	O
(	O
1.62	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
consider	O
ﬁrst	O
the	O
determination	O
of	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
polyno-	O
mial	O
coefﬁcients	O
,	O
which	O
will	O
be	O
denoted	O
by	O
wml	O
.	O
these	O
are	O
determined	O
by	O
maxi-	O
mizing	O
(	O
1.62	O
)	O
with	O
respect	O
to	O
w.	O
for	O
this	O
purpose	O
,	O
we	O
can	O
omit	O
the	O
last	O
two	O
terms	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
1.62	O
)	O
because	O
they	O
do	O
not	O
depend	O
on	O
w.	O
also	O
,	O
we	O
note	O
that	O
scaling	O
the	O
log	O
likelihood	O
by	O
a	O
positive	O
constant	O
coefﬁcient	O
does	O
not	O
alter	O
the	O
location	O
of	O
the	O
maximum	O
with	O
respect	O
to	O
w	O
,	O
and	O
so	O
we	O
can	O
replace	O
the	O
coefﬁcient	O
β/2	O
with	O
1/2	O
.	O
finally	O
,	O
instead	O
of	O
maximizing	O
the	O
log	O
likelihood	O
,	O
we	O
can	O
equivalently	O
minimize	O
the	O
negative	O
log	O
likelihood	O
.	O
we	O
therefore	O
see	O
that	O
maximizing	O
likelihood	O
is	O
equivalent	O
,	O
so	O
far	O
as	O
determining	O
w	O
is	O
concerned	O
,	O
to	O
minimizing	O
the	O
sum-of-squares	B
error	I
function	O
deﬁned	O
by	O
(	O
1.2	O
)	O
.	O
thus	O
the	O
sum-of-squares	B
error	I
function	O
has	O
arisen	O
as	O
a	O
consequence	O
of	O
maximizing	O
likelihood	O
under	O
the	O
assumption	O
of	O
a	O
gaussian	O
noise	O
distribution	O
.	O
we	O
can	O
also	O
use	O
maximum	B
likelihood	I
to	O
determine	O
the	O
precision	B
parameter	I
β	O
of	O
the	O
gaussian	O
conditional	B
distribution	O
.	O
maximizing	O
(	O
1.62	O
)	O
with	O
respect	O
to	O
β	O
gives	O
{	O
y	O
(	O
xn	O
,	O
wml	O
)	O
−	O
tn	O
}	O
2	O
.	O
(	O
1.63	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
1	O
βml	O
=	O
1	O
n	O
30	O
1.	O
introduction	O
section	O
1.2.4	O
n	O
(	O
cid:2	O
)	O
n=1	O
β	O
2	O
{	O
y	O
(	O
xn	O
,	O
w	O
)	O
−	O
tn	O
}	O
2	O
+	O
α	O
2	O
wtw	O
.	O
(	O
1.67	O
)	O
again	O
we	O
can	O
ﬁrst	O
determine	O
the	O
parameter	O
vector	O
wml	O
governing	O
the	O
mean	B
and	O
sub-	O
sequently	O
use	O
this	O
to	O
ﬁnd	O
the	O
precision	O
βml	O
as	O
was	O
the	O
case	O
for	O
the	O
simple	O
gaussian	O
distribution	O
.	O
having	O
determined	O
the	O
parameters	O
w	O
and	O
β	O
,	O
we	O
can	O
now	O
make	O
predictions	O
for	O
new	O
values	O
of	O
x.	O
because	O
we	O
now	O
have	O
a	O
probabilistic	O
model	O
,	O
these	O
are	O
expressed	O
in	O
terms	O
of	O
the	O
predictive	B
distribution	I
that	O
gives	O
the	O
probability	B
distribution	O
over	O
t	O
,	O
rather	O
than	O
simply	O
a	O
point	O
estimate	O
,	O
and	O
is	O
obtained	O
by	O
substituting	O
the	O
maximum	B
likelihood	I
parameters	O
into	O
(	O
1.60	O
)	O
to	O
give	O
(	O
cid:11	O
)	O
t|y	O
(	O
x	O
,	O
wml	O
)	O
,	O
β	O
−1	O
ml	O
.	O
(	O
1.64	O
)	O
p	O
(	O
t|x	O
,	O
wml	O
,	O
βml	O
)	O
=	O
n	O
(	O
cid:10	O
)	O
(	O
cid:17	O
)	O
(	O
cid:18	O
)	O
(	O
m	O
+1	O
)	O
/2	O
now	O
let	O
us	O
take	O
a	O
step	O
towards	O
a	O
more	O
bayesian	O
approach	O
and	O
introduce	O
a	O
prior	B
distribution	O
over	O
the	O
polynomial	O
coefﬁcients	O
w.	O
for	O
simplicity	O
,	O
let	O
us	O
consider	O
a	O
gaussian	O
distribution	O
of	O
the	O
form	O
(	O
cid:20	O
)	O
(	O
cid:19	O
)	O
−	O
α	O
2	O
exp	O
wtw	O
(	O
1.65	O
)	O
p	O
(	O
w|α	O
)	O
=	O
n	O
(	O
w|0	O
,	O
α	O
−1i	O
)	O
=	O
α	O
2π	O
where	O
α	O
is	O
the	O
precision	O
of	O
the	O
distribution	O
,	O
and	O
m	O
+1	O
is	O
the	O
total	O
number	O
of	O
elements	O
in	O
the	O
vector	O
w	O
for	O
an	O
m	O
th	O
order	O
polynomial	O
.	O
variables	O
such	O
as	O
α	O
,	O
which	O
control	O
the	O
distribution	O
of	O
model	O
parameters	O
,	O
are	O
called	O
hyperparameters	O
.	O
using	O
bayes	O
’	O
theorem	O
,	O
the	O
posterior	O
distribution	O
for	O
w	O
is	O
proportional	O
to	O
the	O
product	O
of	O
the	O
prior	B
distribution	O
and	O
the	O
likelihood	B
function	I
p	O
(	O
w|x	O
,	O
t	O
,	O
α	O
,	O
β	O
)	O
∝	O
p	O
(	O
t|x	O
,	O
w	O
,	O
β	O
)	O
p	O
(	O
w|α	O
)	O
.	O
(	O
1.66	O
)	O
we	O
can	O
now	O
determine	O
w	O
by	O
ﬁnding	O
the	O
most	O
probable	O
value	O
of	O
w	O
given	O
the	O
data	O
,	O
in	O
other	O
words	O
by	O
maximizing	O
the	O
posterior	O
distribution	O
.	O
this	O
technique	O
is	O
called	O
maximum	B
posterior	I
,	O
or	O
simply	O
map	O
.	O
taking	O
the	O
negative	O
logarithm	O
of	O
(	O
1.66	O
)	O
and	O
combining	O
with	O
(	O
1.62	O
)	O
and	O
(	O
1.65	O
)	O
,	O
we	O
ﬁnd	O
that	O
the	O
maximum	O
of	O
the	O
posterior	O
is	O
given	O
by	O
the	O
minimum	O
of	O
thus	O
we	O
see	O
that	O
maximizing	O
the	O
posterior	O
distribution	O
is	O
equivalent	O
to	O
minimizing	O
the	O
regularized	O
sum-of-squares	O
error	B
function	I
encountered	O
earlier	O
in	O
the	O
form	O
(	O
1.4	O
)	O
,	O
with	O
a	O
regularization	B
parameter	O
given	O
by	O
λ	O
=	O
α/β	O
.	O
1.2.6	O
bayesian	O
curve	B
ﬁtting	I
although	O
we	O
have	O
included	O
a	O
prior	B
distribution	O
p	O
(	O
w|α	O
)	O
,	O
we	O
are	O
so	O
far	O
still	O
mak-	O
ing	O
a	O
point	O
estimate	O
of	O
w	O
and	O
so	O
this	O
does	O
not	O
yet	O
amount	O
to	O
a	O
bayesian	O
treatment	O
.	O
in	O
a	O
fully	O
bayesian	O
approach	O
,	O
we	O
should	O
consistently	O
apply	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
,	O
which	O
requires	O
,	O
as	O
we	O
shall	O
see	O
shortly	O
,	O
that	O
we	O
integrate	O
over	O
all	O
val-	O
ues	O
of	O
w.	O
such	O
marginalizations	O
lie	O
at	O
the	O
heart	O
of	O
bayesian	O
methods	O
for	O
pattern	O
recognition	O
.	O
1.2.	O
probability	B
theory	O
31	O
in	O
the	O
curve	B
ﬁtting	I
problem	O
,	O
we	O
are	O
given	O
the	O
training	B
data	O
x	O
and	O
t	O
,	O
along	O
with	O
a	O
new	O
test	O
point	O
x	O
,	O
and	O
our	O
goal	O
is	O
to	O
predict	O
the	O
value	O
of	O
t.	O
we	O
therefore	O
wish	O
to	O
evaluate	O
the	O
predictive	B
distribution	I
p	O
(	O
t|x	O
,	O
x	O
,	O
t	O
)	O
.	O
here	O
we	O
shall	O
assume	O
that	O
the	O
parameters	O
α	O
and	O
β	O
are	O
ﬁxed	O
and	O
known	O
in	O
advance	O
(	O
in	O
later	O
chapters	O
we	O
shall	O
discuss	O
how	O
such	O
parameters	O
can	O
be	O
inferred	O
from	O
data	O
in	O
a	O
bayesian	O
setting	O
)	O
.	O
a	O
bayesian	O
treatment	O
simply	O
corresponds	O
to	O
a	O
consistent	B
application	O
of	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
,	O
which	O
allow	O
the	O
predictive	B
distribution	I
to	O
be	O
written	O
in	O
the	O
form	O
(	O
cid:6	O
)	O
p	O
(	O
t|x	O
,	O
x	O
,	O
t	O
)	O
=	O
p	O
(	O
t|x	O
,	O
w	O
)	O
p	O
(	O
w|x	O
,	O
t	O
)	O
dw	O
.	O
(	O
1.68	O
)	O
here	O
p	O
(	O
t|x	O
,	O
w	O
)	O
is	O
given	O
by	O
(	O
1.60	O
)	O
,	O
and	O
we	O
have	O
omitted	O
the	O
dependence	O
on	O
α	O
and	O
β	O
to	O
simplify	O
the	O
notation	O
.	O
here	O
p	O
(	O
w|x	O
,	O
t	O
)	O
is	O
the	O
posterior	O
distribution	O
over	O
param-	O
eters	O
,	O
and	O
can	O
be	O
found	O
by	O
normalizing	O
the	O
right-hand	O
side	O
of	O
(	O
1.66	O
)	O
.	O
we	O
shall	O
see	O
in	O
section	O
3.3	O
that	O
,	O
for	O
problems	O
such	O
as	O
the	O
curve-ﬁtting	O
example	O
,	O
this	O
posterior	O
distribution	O
is	O
a	O
gaussian	O
and	O
can	O
be	O
evaluated	O
analytically	O
.	O
similarly	O
,	O
the	O
integra-	O
tion	O
in	O
(	O
1.68	O
)	O
can	O
also	O
be	O
performed	O
analytically	O
with	O
the	O
result	O
that	O
the	O
predictive	B
distribution	I
is	O
given	O
by	O
a	O
gaussian	O
of	O
the	O
form	O
p	O
(	O
t|x	O
,	O
x	O
,	O
t	O
)	O
=	O
n	O
(	O
cid:10	O
)	O
t|m	O
(	O
x	O
)	O
,	O
s2	O
(	O
x	O
)	O
(	O
cid:11	O
)	O
where	O
the	O
mean	B
and	O
variance	B
are	O
given	O
by	O
n	O
(	O
cid:2	O
)	O
s2	O
(	O
x	O
)	O
=	O
β	O
here	O
the	O
matrix	O
s	O
is	O
given	O
by	O
m	O
(	O
x	O
)	O
=	O
βφ	O
(	O
x	O
)	O
ts	O
φ	O
(	O
xn	O
)	O
tn	O
−1	O
+	O
φ	O
(	O
x	O
)	O
tsφ	O
(	O
x	O
)	O
.	O
n=1	O
n	O
(	O
cid:2	O
)	O
n=1	O
φ	O
(	O
xn	O
)	O
φ	O
(	O
x	O
)	O
t	O
s−1	O
=	O
αi	O
+	O
β	O
(	O
1.69	O
)	O
(	O
1.70	O
)	O
(	O
1.71	O
)	O
(	O
1.72	O
)	O
where	O
i	O
is	O
the	O
unit	O
matrix	O
,	O
and	O
we	O
have	O
deﬁned	O
the	O
vector	O
φ	O
(	O
x	O
)	O
with	O
elements	O
φi	O
(	O
x	O
)	O
=	O
xi	O
for	O
i	O
=	O
0	O
,	O
.	O
.	O
.	O
,	O
m.	O
we	O
see	O
that	O
the	O
variance	B
,	O
as	O
well	O
as	O
the	O
mean	B
,	O
of	O
the	O
predictive	B
distribution	I
in	O
(	O
1.69	O
)	O
is	O
dependent	O
on	O
x.	O
the	O
ﬁrst	O
term	O
in	O
(	O
1.71	O
)	O
represents	O
the	O
uncertainty	O
in	O
the	O
predicted	O
value	O
of	O
t	O
due	O
to	O
the	O
noise	O
on	O
the	O
target	O
variables	O
and	O
was	O
expressed	O
already	O
−1	O
in	O
the	O
maximum	B
likelihood	I
predictive	O
distribution	O
(	O
1.64	O
)	O
through	O
β	O
ml	O
.	O
however	O
,	O
the	O
second	O
term	O
arises	O
from	O
the	O
uncertainty	O
in	O
the	O
parameters	O
w	O
and	O
is	O
a	O
consequence	O
of	O
the	O
bayesian	O
treatment	O
.	O
the	O
predictive	B
distribution	I
for	O
the	O
synthetic	O
sinusoidal	O
regression	B
problem	O
is	O
illustrated	O
in	O
figure	O
1.17	O
.	O
32	O
1.	O
introduction	O
figure	O
1.17	O
the	O
predictive	B
distribution	I
result-	O
ing	O
from	O
a	O
bayesian	O
treatment	O
of	O
polynomial	B
curve	I
ﬁtting	I
using	O
an	O
m	O
=	O
9	O
polynomial	O
,	O
with	O
the	O
ﬁxed	O
parameters	O
α	O
=	O
5	O
×	O
10−3	O
and	O
β	O
=	O
11.1	O
(	O
corresponding	O
to	O
the	O
known	O
noise	O
variance	B
)	O
,	O
in	O
which	O
the	O
red	O
curve	O
denotes	O
the	O
mean	B
of	O
the	O
predictive	B
distribution	I
and	O
the	O
red	O
region	O
corresponds	O
to	O
±1	O
stan-	O
dard	O
deviation	O
around	O
the	O
mean	B
.	O
t	O
1	O
0	O
−1	O
0	O
x	O
1	O
1.3.	O
model	B
selection	I
in	O
our	O
example	O
of	O
polynomial	B
curve	I
ﬁtting	I
using	O
least	O
squares	O
,	O
we	O
saw	O
that	O
there	O
was	O
an	O
optimal	O
order	O
of	O
polynomial	O
that	O
gave	O
the	O
best	O
generalization	B
.	O
the	O
order	O
of	O
the	O
polynomial	O
controls	O
the	O
number	O
of	O
free	O
parameters	O
in	O
the	O
model	O
and	O
thereby	O
governs	O
the	O
model	O
complexity	O
.	O
with	O
regularized	B
least	I
squares	I
,	O
the	O
regularization	B
coefﬁcient	O
λ	O
also	O
controls	O
the	O
effective	O
complexity	O
of	O
the	O
model	O
,	O
whereas	O
for	O
more	O
complex	O
models	O
,	O
such	O
as	O
mixture	B
distributions	O
or	O
neural	O
networks	O
there	O
may	O
be	O
multiple	O
pa-	O
rameters	O
governing	O
complexity	O
.	O
in	O
a	O
practical	O
application	O
,	O
we	O
need	O
to	O
determine	O
the	O
values	O
of	O
such	O
parameters	O
,	O
and	O
the	O
principal	O
objective	O
in	O
doing	O
so	O
is	O
usually	O
to	O
achieve	O
the	O
best	O
predictive	O
performance	O
on	O
new	O
data	O
.	O
furthermore	O
,	O
as	O
well	O
as	O
ﬁnd-	O
ing	O
the	O
appropriate	O
values	O
for	O
complexity	O
parameters	O
within	O
a	O
given	O
model	O
,	O
we	O
may	O
wish	O
to	O
consider	O
a	O
range	O
of	O
different	O
types	O
of	O
model	O
in	O
order	O
to	O
ﬁnd	O
the	O
best	O
one	O
for	O
our	O
particular	O
application	O
.	O
we	O
have	O
already	O
seen	O
that	O
,	O
in	O
the	O
maximum	B
likelihood	I
approach	O
,	O
the	O
perfor-	O
mance	O
on	O
the	O
training	B
set	I
is	O
not	O
a	O
good	O
indicator	O
of	O
predictive	O
performance	O
on	O
un-	O
seen	O
data	O
due	O
to	O
the	O
problem	O
of	O
over-ﬁtting	B
.	O
if	O
data	O
is	O
plentiful	O
,	O
then	O
one	O
approach	O
is	O
simply	O
to	O
use	O
some	O
of	O
the	O
available	O
data	O
to	O
train	O
a	O
range	O
of	O
models	O
,	O
or	O
a	O
given	O
model	O
with	O
a	O
range	O
of	O
values	O
for	O
its	O
complexity	O
parameters	O
,	O
and	O
then	O
to	O
compare	O
them	O
on	O
independent	B
data	O
,	O
sometimes	O
called	O
a	O
validation	B
set	I
,	O
and	O
select	O
the	O
one	O
having	O
the	O
best	O
predictive	O
performance	O
.	O
if	O
the	O
model	O
design	O
is	O
iterated	O
many	O
times	O
using	O
a	O
lim-	O
ited	O
size	O
data	O
set	O
,	O
then	O
some	O
over-ﬁtting	B
to	O
the	O
validation	O
data	O
can	O
occur	O
and	O
so	O
it	O
may	O
be	O
necessary	O
to	O
keep	O
aside	O
a	O
third	O
test	B
set	I
on	O
which	O
the	O
performance	O
of	O
the	O
selected	O
model	O
is	O
ﬁnally	O
evaluated	O
.	O
in	O
many	O
applications	O
,	O
however	O
,	O
the	O
supply	O
of	O
data	O
for	O
training	B
and	O
testing	O
will	O
be	O
limited	O
,	O
and	O
in	O
order	O
to	O
build	O
good	O
models	O
,	O
we	O
wish	O
to	O
use	O
as	O
much	O
of	O
the	O
available	O
data	O
as	O
possible	O
for	O
training	O
.	O
however	O
,	O
if	O
the	O
validation	B
set	I
is	O
small	O
,	O
it	O
will	O
give	O
a	O
relatively	O
noisy	O
estimate	O
of	O
predictive	O
performance	O
.	O
one	O
solution	O
to	O
this	O
dilemma	O
is	O
to	O
use	O
cross-validation	B
,	O
which	O
is	O
illustrated	O
in	O
figure	O
1.18.	O
this	O
allows	O
a	O
proportion	O
(	O
s	O
−	O
1	O
)	O
/s	O
of	O
the	O
available	O
data	O
to	O
be	O
used	O
for	O
training	O
while	O
making	O
use	O
of	O
all	O
of	O
the	O
1.4.	O
the	O
curse	B
of	I
dimensionality	I
33	O
figure	O
1.18	O
the	O
technique	O
of	O
s-fold	O
cross-validation	B
,	O
illus-	O
trated	O
here	O
for	O
the	O
case	O
of	O
s	O
=	O
4	O
,	O
involves	O
tak-	O
ing	O
the	O
available	O
data	O
and	O
partitioning	O
it	O
into	O
s	O
groups	O
(	O
in	O
the	O
simplest	O
case	O
these	O
are	O
of	O
equal	O
size	O
)	O
.	O
then	O
s	O
−	O
1	O
of	O
the	O
groups	O
are	O
used	O
to	O
train	O
a	O
set	O
of	O
models	O
that	O
are	O
then	O
evaluated	O
on	O
the	O
re-	O
maining	O
group	O
.	O
this	O
procedure	O
is	O
then	O
repeated	O
for	O
all	O
s	O
possible	O
choices	O
for	O
the	O
held-out	O
group	O
,	O
indicated	O
here	O
by	O
the	O
red	O
blocks	O
,	O
and	O
the	O
perfor-	O
mance	O
scores	O
from	O
the	O
s	O
runs	O
are	O
then	O
averaged	O
.	O
run	O
1	O
run	O
2	O
run	O
3	O
run	O
4	O
data	O
to	O
assess	O
performance	O
.	O
when	O
data	O
is	O
particularly	O
scarce	O
,	O
it	O
may	O
be	O
appropriate	O
to	O
consider	O
the	O
case	O
s	O
=	O
n	O
,	O
where	O
n	O
is	O
the	O
total	O
number	O
of	O
data	O
points	O
,	O
which	O
gives	O
the	O
leave-one-out	B
technique	O
.	O
one	O
major	O
drawback	O
of	O
cross-validation	B
is	O
that	O
the	O
number	O
of	O
training	B
runs	O
that	O
must	O
be	O
performed	O
is	O
increased	O
by	O
a	O
factor	O
of	O
s	O
,	O
and	O
this	O
can	O
prove	O
problematic	O
for	O
models	O
in	O
which	O
the	O
training	B
is	O
itself	O
computationally	O
expensive	O
.	O
a	O
further	O
problem	O
with	O
techniques	O
such	O
as	O
cross-validation	B
that	O
use	O
separate	O
data	O
to	O
assess	O
performance	O
is	O
that	O
we	O
might	O
have	O
multiple	O
complexity	O
parameters	O
for	O
a	O
single	O
model	O
(	O
for	O
in-	O
stance	O
,	O
there	O
might	O
be	O
several	O
regularization	B
parameters	O
)	O
.	O
exploring	O
combinations	O
of	O
settings	O
for	O
such	O
parameters	O
could	O
,	O
in	O
the	O
worst	O
case	O
,	O
require	O
a	O
number	O
of	O
training	B
runs	O
that	O
is	O
exponential	O
in	O
the	O
number	O
of	O
parameters	O
.	O
clearly	O
,	O
we	O
need	O
a	O
better	O
ap-	O
proach	O
.	O
ideally	O
,	O
this	O
should	O
rely	O
only	O
on	O
the	O
training	B
data	O
and	O
should	O
allow	O
multiple	O
hyperparameters	O
and	O
model	O
types	O
to	O
be	O
compared	O
in	O
a	O
single	O
training	B
run	O
.	O
we	O
there-	O
fore	O
need	O
to	O
ﬁnd	O
a	O
measure	O
of	O
performance	O
which	O
depends	O
only	O
on	O
the	O
training	B
data	O
and	O
which	O
does	O
not	O
suffer	O
from	O
bias	B
due	O
to	O
over-ﬁtting	B
.	O
historically	O
various	O
‘	O
information	O
criteria	O
’	O
have	O
been	O
proposed	O
that	O
attempt	O
to	O
correct	O
for	O
the	O
bias	B
of	O
maximum	B
likelihood	I
by	O
the	O
addition	O
of	O
a	O
penalty	O
term	O
to	O
compensate	O
for	O
the	O
over-ﬁtting	B
of	O
more	O
complex	O
models	O
.	O
for	O
example	O
,	O
the	O
akaike	O
information	B
criterion	I
,	O
or	O
aic	O
(	O
akaike	O
,	O
1974	O
)	O
,	O
chooses	O
the	O
model	O
for	O
which	O
the	O
quan-	O
tity	O
(	O
1.73	O
)	O
is	O
largest	O
.	O
here	O
p	O
(	O
d|wml	O
)	O
is	O
the	O
best-ﬁt	O
log	O
likelihood	O
,	O
and	O
m	O
is	O
the	O
number	O
of	O
adjustable	O
parameters	O
in	O
the	O
model	O
.	O
a	O
variant	O
of	O
this	O
quantity	O
,	O
called	O
the	O
bayesian	O
information	B
criterion	I
,	O
or	O
bic	O
,	O
will	O
be	O
discussed	O
in	O
section	O
4.4.1.	O
such	O
criteria	O
do	O
not	O
take	O
account	O
of	O
the	O
uncertainty	O
in	O
the	O
model	O
parameters	O
,	O
however	O
,	O
and	O
in	O
practice	O
they	O
tend	O
to	O
favour	O
overly	O
simple	O
models	O
.	O
we	O
therefore	O
turn	O
in	O
section	O
3.4	O
to	O
a	O
fully	O
bayesian	O
approach	O
where	O
we	O
shall	O
see	O
how	O
complexity	O
penalties	O
arise	O
in	O
a	O
natural	O
and	O
principled	O
way	O
.	O
ln	O
p	O
(	O
d|wml	O
)	O
−	O
m	O
1.4.	O
the	O
curse	B
of	I
dimensionality	I
in	O
the	O
polynomial	B
curve	I
ﬁtting	I
example	O
we	O
had	O
just	O
one	O
input	O
variable	O
x.	O
for	O
prac-	O
tical	O
applications	O
of	O
pattern	O
recognition	O
,	O
however	O
,	O
we	O
will	O
have	O
to	O
deal	O
with	O
spaces	O
34	O
1.	O
introduction	O
figure	O
1.19	O
scatter	O
plot	O
of	O
the	O
oil	B
ﬂow	I
data	I
for	O
input	O
variables	O
x6	O
and	O
x7	O
,	O
in	O
which	O
red	O
denotes	O
the	O
‘	O
homoge-	O
nous	O
’	O
class	O
,	O
green	O
denotes	O
the	O
‘	O
annular	O
’	O
class	O
,	O
and	O
blue	O
denotes	O
the	O
‘	O
laminar	O
’	O
class	O
.	O
our	O
goal	O
is	O
to	O
classify	O
the	O
new	O
test	O
point	O
de-	O
noted	O
by	O
‘	O
×	O
’	O
.	O
2	O
1.5	O
x7	O
1	O
0.5	O
0	O
0	O
0.25	O
0.5	O
x6	O
0.75	O
1	O
of	O
high	O
dimensionality	O
comprising	O
many	O
input	O
variables	O
.	O
as	O
we	O
now	O
discuss	O
,	O
this	O
poses	O
some	O
serious	O
challenges	O
and	O
is	O
an	O
important	O
factor	O
inﬂuencing	O
the	O
design	O
of	O
pattern	O
recognition	O
techniques	O
.	O
in	O
order	O
to	O
illustrate	O
the	O
problem	O
we	O
consider	O
a	O
synthetically	O
generated	O
data	O
set	O
representing	O
measurements	O
taken	O
from	O
a	O
pipeline	O
containing	O
a	O
mixture	O
of	O
oil	O
,	O
wa-	O
ter	O
,	O
and	O
gas	O
(	O
bishop	O
and	O
james	O
,	O
1993	O
)	O
.	O
these	O
three	O
materials	O
can	O
be	O
present	O
in	O
one	O
of	O
three	O
different	O
geometrical	O
conﬁgurations	O
known	O
as	O
‘	O
homogenous	O
’	O
,	O
‘	O
annular	O
’	O
,	O
and	O
‘	O
laminar	O
’	O
,	O
and	O
the	O
fractions	O
of	O
the	O
three	O
materials	O
can	O
also	O
vary	O
.	O
each	O
data	O
point	O
com-	O
prises	O
a	O
12-dimensional	O
input	O
vector	O
consisting	O
of	O
measurements	O
taken	O
with	O
gamma	O
ray	O
densitometers	O
that	O
measure	O
the	O
attenuation	O
of	O
gamma	O
rays	O
passing	O
along	O
nar-	O
row	O
beams	O
through	O
the	O
pipe	O
.	O
this	O
data	O
set	O
is	O
described	O
in	O
detail	O
in	O
appendix	O
a.	O
figure	O
1.19	O
shows	O
100	O
points	O
from	O
this	O
data	O
set	O
on	O
a	O
plot	O
showing	O
two	O
of	O
the	O
mea-	O
surements	O
x6	O
and	O
x7	O
(	O
the	O
remaining	O
ten	O
input	O
values	O
are	O
ignored	O
for	O
the	O
purposes	O
of	O
this	O
illustration	O
)	O
.	O
each	O
data	O
point	O
is	O
labelled	O
according	O
to	O
which	O
of	O
the	O
three	O
geomet-	O
rical	O
classes	O
it	O
belongs	O
to	O
,	O
and	O
our	O
goal	O
is	O
to	O
use	O
this	O
data	O
as	O
a	O
training	B
set	I
in	O
order	O
to	O
be	O
able	O
to	O
classify	O
a	O
new	O
observation	O
(	O
x6	O
,	O
x7	O
)	O
,	O
such	O
as	O
the	O
one	O
denoted	O
by	O
the	O
cross	O
in	O
figure	O
1.19.	O
we	O
observe	O
that	O
the	O
cross	O
is	O
surrounded	O
by	O
numerous	O
red	O
points	O
,	O
and	O
so	O
we	O
might	O
suppose	O
that	O
it	O
belongs	O
to	O
the	O
red	O
class	O
.	O
however	O
,	O
there	O
are	O
also	O
plenty	O
of	O
green	O
points	O
nearby	O
,	O
so	O
we	O
might	O
think	O
that	O
it	O
could	O
instead	O
belong	O
to	O
the	O
green	O
class	O
.	O
it	O
seems	O
unlikely	O
that	O
it	O
belongs	O
to	O
the	O
blue	O
class	O
.	O
the	O
intuition	O
here	O
is	O
that	O
the	O
identity	O
of	O
the	O
cross	O
should	O
be	O
determined	O
more	O
strongly	O
by	O
nearby	O
points	O
from	O
the	O
training	B
set	I
and	O
less	O
strongly	O
by	O
more	O
distant	O
points	O
.	O
in	O
fact	O
,	O
this	O
intuition	O
turns	O
out	O
to	O
be	O
reasonable	O
and	O
will	O
be	O
discussed	O
more	O
fully	O
in	O
later	O
chapters	O
.	O
how	O
can	O
we	O
turn	O
this	O
intuition	O
into	O
a	O
learning	B
algorithm	O
?	O
one	O
very	O
simple	O
ap-	O
proach	O
would	O
be	O
to	O
divide	O
the	O
input	O
space	O
into	O
regular	O
cells	O
,	O
as	O
indicated	O
in	O
fig-	O
ure	O
1.20.	O
when	O
we	O
are	O
given	O
a	O
test	O
point	O
and	O
we	O
wish	O
to	O
predict	O
its	O
class	O
,	O
we	O
ﬁrst	O
decide	O
which	O
cell	O
it	O
belongs	O
to	O
,	O
and	O
we	O
then	O
ﬁnd	O
all	O
of	O
the	O
training	B
data	O
points	O
that	O
1.4.	O
the	O
curse	B
of	I
dimensionality	I
35	O
figure	O
1.20	O
illustration	O
of	O
a	O
simple	O
approach	O
to	O
the	O
solution	O
of	O
a	O
classiﬁcation	B
problem	O
in	O
which	O
the	O
input	O
space	O
is	O
divided	O
into	O
cells	O
and	O
any	O
new	O
test	O
point	O
is	O
assigned	O
to	O
the	O
class	O
that	O
has	O
a	O
majority	O
number	O
of	O
rep-	O
resentatives	O
in	O
the	O
same	O
cell	O
as	O
the	O
test	O
point	O
.	O
as	O
we	O
shall	O
see	O
shortly	O
,	O
this	O
simplistic	O
approach	O
has	O
some	O
severe	O
shortcomings	O
.	O
2	O
1.5	O
x7	O
1	O
0.5	O
0	O
0	O
0.25	O
0.5	O
x6	O
0.75	O
1	O
fall	O
in	O
the	O
same	O
cell	O
.	O
the	O
identity	O
of	O
the	O
test	O
point	O
is	O
predicted	O
as	O
being	O
the	O
same	O
as	O
the	O
class	O
having	O
the	O
largest	O
number	O
of	O
training	B
points	O
in	O
the	O
same	O
cell	O
as	O
the	O
test	O
point	O
(	O
with	O
ties	O
being	O
broken	O
at	O
random	O
)	O
.	O
there	O
are	O
numerous	O
problems	O
with	O
this	O
naive	O
approach	O
,	O
but	O
one	O
of	O
the	O
most	O
se-	O
vere	O
becomes	O
apparent	O
when	O
we	O
consider	O
its	O
extension	O
to	O
problems	O
having	O
larger	O
numbers	O
of	O
input	O
variables	O
,	O
corresponding	O
to	O
input	O
spaces	O
of	O
higher	O
dimensionality	O
.	O
the	O
origin	O
of	O
the	O
problem	O
is	O
illustrated	O
in	O
figure	O
1.21	O
,	O
which	O
shows	O
that	O
,	O
if	O
we	O
divide	O
a	O
region	O
of	O
a	O
space	O
into	O
regular	O
cells	O
,	O
then	O
the	O
number	O
of	O
such	O
cells	O
grows	O
exponen-	O
tially	O
with	O
the	O
dimensionality	O
of	O
the	O
space	O
.	O
the	O
problem	O
with	O
an	O
exponentially	O
large	O
number	O
of	O
cells	O
is	O
that	O
we	O
would	O
need	O
an	O
exponentially	O
large	O
quantity	O
of	O
training	B
data	O
in	O
order	O
to	O
ensure	O
that	O
the	O
cells	O
are	O
not	O
empty	O
.	O
clearly	O
,	O
we	O
have	O
no	O
hope	O
of	O
applying	O
such	O
a	O
technique	O
in	O
a	O
space	O
of	O
more	O
than	O
a	O
few	O
variables	O
,	O
and	O
so	O
we	O
need	O
to	O
ﬁnd	O
a	O
more	O
sophisticated	O
approach	O
.	O
we	O
can	O
gain	O
further	O
insight	O
into	O
the	O
problems	O
of	O
high-dimensional	O
spaces	O
by	O
returning	O
to	O
the	O
example	O
of	O
polynomial	B
curve	I
ﬁtting	I
and	O
considering	O
how	O
we	O
would	O
section	O
1.1	O
of	O
figure	O
1.21	O
illustration	O
the	O
curse	B
of	I
dimensionality	I
,	O
showing	O
how	O
the	O
number	O
of	O
regions	O
of	O
a	O
regular	O
grid	O
grows	O
exponentially	O
with	O
the	O
dimensionality	O
d	O
of	O
the	O
space	O
.	O
for	O
clarity	O
,	O
only	O
a	O
subset	O
of	O
the	O
cubical	O
regions	O
are	O
shown	O
for	O
d	O
=	O
3.	O
x2	O
x2	O
x1	O
d	O
=	O
1	O
x1	O
x3	O
d	O
=	O
2	O
d	O
=	O
3	O
x1	O
36	O
1.	O
introduction	O
extend	O
this	O
approach	O
to	O
deal	O
with	O
input	O
spaces	O
having	O
several	O
variables	O
.	O
if	O
we	O
have	O
d	O
input	O
variables	O
,	O
then	O
a	O
general	O
polynomial	O
with	O
coefﬁcients	O
up	O
to	O
order	O
3	O
would	O
take	O
the	O
form	O
d	O
(	O
cid:2	O
)	O
d	O
(	O
cid:2	O
)	O
d	O
(	O
cid:2	O
)	O
d	O
(	O
cid:2	O
)	O
d	O
(	O
cid:2	O
)	O
d	O
(	O
cid:2	O
)	O
y	O
(	O
x	O
,	O
w	O
)	O
=	O
w0	O
+	O
wixi	O
+	O
wijxixj	O
+	O
wijkxixjxk	O
.	O
(	O
1.74	O
)	O
exercise	O
1.16	O
i=1	O
i=1	O
j=1	O
i=1	O
j=1	O
k=1	O
as	O
d	O
increases	O
,	O
so	O
the	O
number	O
of	O
independent	B
coefﬁcients	O
(	O
not	O
all	O
of	O
the	O
coefﬁcients	O
are	O
independent	B
due	O
to	O
interchange	O
symmetries	B
amongst	O
the	O
x	O
variables	O
)	O
grows	O
pro-	O
portionally	O
to	O
d3	O
.	O
in	O
practice	O
,	O
to	O
capture	O
complex	O
dependencies	O
in	O
the	O
data	O
,	O
we	O
may	O
need	O
to	O
use	O
a	O
higher-order	O
polynomial	O
.	O
for	O
a	O
polynomial	O
of	O
order	O
m	O
,	O
the	O
growth	O
in	O
the	O
number	O
of	O
coefﬁcients	O
is	O
like	O
dm	O
.	O
although	O
this	O
is	O
now	O
a	O
power	O
law	O
growth	O
,	O
rather	O
than	O
an	O
exponential	O
growth	O
,	O
it	O
still	O
points	O
to	O
the	O
method	O
becoming	O
rapidly	O
unwieldy	O
and	O
of	O
limited	O
practical	O
utility	O
.	O
our	O
geometrical	O
intuitions	O
,	O
formed	O
through	O
a	O
life	O
spent	O
in	O
a	O
space	O
of	O
three	O
di-	O
mensions	O
,	O
can	O
fail	O
badly	O
when	O
we	O
consider	O
spaces	O
of	O
higher	O
dimensionality	O
.	O
as	O
a	O
simple	O
example	O
,	O
consider	O
a	O
sphere	O
of	O
radius	O
r	O
=	O
1	O
in	O
a	O
space	O
of	O
d	O
dimensions	O
,	O
and	O
ask	O
what	O
is	O
the	O
fraction	O
of	O
the	O
volume	O
of	O
the	O
sphere	O
that	O
lies	O
between	O
radius	O
r	O
=	O
1−	O
and	O
r	O
=	O
1.	O
we	O
can	O
evaluate	O
this	O
fraction	O
by	O
noting	O
that	O
the	O
volume	O
of	O
a	O
sphere	O
of	O
radius	O
r	O
in	O
d	O
dimensions	O
must	O
scale	O
as	O
rd	O
,	O
and	O
so	O
we	O
write	O
vd	O
(	O
r	O
)	O
=	O
kdrd	O
(	O
1.75	O
)	O
exercise	O
1.18	O
where	O
the	O
constant	O
kd	O
depends	O
only	O
on	O
d.	O
thus	O
the	O
required	O
fraction	O
is	O
given	O
by	O
exercise	O
1.20	O
vd	O
(	O
1	O
)	O
−	O
vd	O
(	O
1	O
−	O
	O
)	O
vd	O
(	O
1	O
)	O
=	O
1	O
−	O
(	O
1	O
−	O
	O
)	O
d	O
(	O
1.76	O
)	O
which	O
is	O
plotted	O
as	O
a	O
function	O
of	O
	O
for	O
various	O
values	O
of	O
d	O
in	O
figure	O
1.22.	O
we	O
see	O
that	O
,	O
for	O
large	O
d	O
,	O
this	O
fraction	O
tends	O
to	O
1	O
even	O
for	O
small	O
values	O
of	O
	O
.	O
thus	O
,	O
in	O
spaces	O
of	O
high	O
dimensionality	O
,	O
most	O
of	O
the	O
volume	O
of	O
a	O
sphere	O
is	O
concentrated	O
in	O
a	O
thin	O
shell	O
near	O
the	O
surface	O
!	O
as	O
a	O
further	O
example	O
,	O
of	O
direct	O
relevance	O
to	O
pattern	O
recognition	O
,	O
consider	O
the	O
behaviour	O
of	O
a	O
gaussian	O
distribution	O
in	O
a	O
high-dimensional	O
space	O
.	O
if	O
we	O
transform	O
from	O
cartesian	O
to	O
polar	O
coordinates	O
,	O
and	O
then	O
integrate	O
out	O
the	O
directional	O
variables	O
,	O
we	O
obtain	O
an	O
expression	O
for	O
the	O
density	B
p	O
(	O
r	O
)	O
as	O
a	O
function	O
of	O
radius	O
r	O
from	O
the	O
origin	O
.	O
thus	O
p	O
(	O
r	O
)	O
δr	O
is	O
the	O
probability	B
mass	O
inside	O
a	O
thin	O
shell	O
of	O
thickness	O
δr	O
located	O
at	O
radius	O
r.	O
this	O
distribution	O
is	O
plotted	O
,	O
for	O
various	O
values	O
of	O
d	O
,	O
in	O
figure	O
1.23	O
,	O
and	O
we	O
see	O
that	O
for	O
large	O
d	O
the	O
probability	B
mass	O
of	O
the	O
gaussian	O
is	O
concentrated	O
in	O
a	O
thin	O
shell	O
.	O
the	O
severe	O
difﬁculty	O
that	O
can	O
arise	O
in	O
spaces	O
of	O
many	O
dimensions	O
is	O
sometimes	O
called	O
the	O
curse	B
of	I
dimensionality	I
(	O
bellman	O
,	O
1961	O
)	O
.	O
in	O
this	O
book	O
,	O
we	O
shall	O
make	O
ex-	O
tensive	O
use	O
of	O
illustrative	O
examples	O
involving	O
input	O
spaces	O
of	O
one	O
or	O
two	O
dimensions	O
,	O
because	O
this	O
makes	O
it	O
particularly	O
easy	O
to	O
illustrate	O
the	O
techniques	O
graphically	O
.	O
the	O
reader	O
should	O
be	O
warned	O
,	O
however	O
,	O
that	O
not	O
all	O
intuitions	O
developed	O
in	O
spaces	O
of	O
low	O
dimensionality	O
will	O
generalize	O
to	O
spaces	O
of	O
many	O
dimensions	O
.	O
figure	O
1.22	O
plot	O
of	O
the	O
fraction	O
of	O
the	O
volume	O
of	O
a	O
sphere	O
lying	O
in	O
the	O
range	O
r	O
=	O
1−	O
to	O
r	O
=	O
1	O
for	O
various	O
values	O
of	O
the	O
dimensionality	O
d.	O
1.4.	O
the	O
curse	B
of	I
dimensionality	I
37	O
n	O
o	O
i	O
t	O
c	O
a	O
r	O
f	O
e	O
m	O
u	O
o	O
v	O
l	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
0	O
d	O
=	O
20	O
d	O
=	O
5	O
d	O
=	O
2	O
d	O
=	O
1	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
	O
although	O
the	O
curse	B
of	I
dimensionality	I
certainly	O
raises	O
important	O
issues	O
for	O
pat-	O
tern	O
recognition	O
applications	O
,	O
it	O
does	O
not	O
prevent	O
us	O
from	O
ﬁnding	O
effective	O
techniques	O
applicable	O
to	O
high-dimensional	O
spaces	O
.	O
the	O
reasons	O
for	O
this	O
are	O
twofold	O
.	O
first	O
,	O
real	O
data	O
will	O
often	O
be	O
conﬁned	O
to	O
a	O
region	O
of	O
the	O
space	O
having	O
lower	O
effective	O
dimension-	O
ality	O
,	O
and	O
in	O
particular	O
the	O
directions	O
over	O
which	O
important	O
variations	O
in	O
the	O
target	O
variables	O
occur	O
may	O
be	O
so	O
conﬁned	O
.	O
second	O
,	O
real	O
data	O
will	O
typically	O
exhibit	O
some	O
smoothness	O
properties	O
(	O
at	O
least	O
locally	O
)	O
so	O
that	O
for	O
the	O
most	O
part	O
small	O
changes	O
in	O
the	O
input	O
variables	O
will	O
produce	O
small	O
changes	O
in	O
the	O
target	O
variables	O
,	O
and	O
so	O
we	O
can	O
ex-	O
ploit	O
local	B
interpolation-like	O
techniques	O
to	O
allow	O
us	O
to	O
make	O
predictions	O
of	O
the	O
target	O
variables	O
for	O
new	O
values	O
of	O
the	O
input	O
variables	O
.	O
successful	O
pattern	O
recognition	O
tech-	O
niques	O
exploit	O
one	O
or	O
both	O
of	O
these	O
properties	O
.	O
consider	O
,	O
for	O
example	O
,	O
an	O
application	O
in	O
manufacturing	O
in	O
which	O
images	O
are	O
captured	O
of	O
identical	O
planar	O
objects	O
on	O
a	O
con-	O
veyor	O
belt	O
,	O
in	O
which	O
the	O
goal	O
is	O
to	O
determine	O
their	O
orientation	O
.	O
each	O
image	O
is	O
a	O
point	O
figure	O
1.23	O
plot	O
of	O
the	O
probability	B
density	O
with	O
to	O
radius	O
r	O
of	O
a	O
gaus-	O
respect	O
sian	O
distribution	O
for	O
various	O
values	O
of	O
in	O
a	O
high-dimensional	O
space	O
,	O
most	O
of	O
the	O
probability	B
mass	O
of	O
a	O
gaussian	O
is	O
lo-	O
cated	O
within	O
a	O
thin	O
shell	O
at	O
a	O
speciﬁc	O
radius	O
.	O
the	O
dimensionality	O
d.	O
2	O
)	O
r	O
(	O
p	O
1	O
d	O
=	O
1	O
d	O
=	O
2	O
d	O
=	O
20	O
0	O
0	O
2	O
r	O
4	O
38	O
1.	O
introduction	O
in	O
a	O
high-dimensional	O
space	O
whose	O
dimensionality	O
is	O
determined	O
by	O
the	O
number	O
of	O
pixels	O
.	O
because	O
the	O
objects	O
can	O
occur	O
at	O
different	O
positions	O
within	O
the	O
image	O
and	O
in	O
different	O
orientations	O
,	O
there	O
are	O
three	O
degrees	B
of	I
freedom	I
of	O
variability	O
between	O
images	O
,	O
and	O
a	O
set	O
of	O
images	O
will	O
live	O
on	O
a	O
three	O
dimensional	O
manifold	B
embedded	O
within	O
the	O
high-dimensional	O
space	O
.	O
due	O
to	O
the	O
complex	O
relationships	O
between	O
the	O
object	O
position	O
or	O
orientation	O
and	O
the	O
pixel	O
intensities	O
,	O
this	O
manifold	B
will	O
be	O
highly	O
nonlinear	O
.	O
if	O
the	O
goal	O
is	O
to	O
learn	O
a	O
model	O
that	O
can	O
take	O
an	O
input	O
image	O
and	O
output	O
the	O
orientation	O
of	O
the	O
object	O
irrespective	O
of	O
its	O
position	O
,	O
then	O
there	O
is	O
only	O
one	O
degree	O
of	O
freedom	O
of	O
variability	O
within	O
the	O
manifold	B
that	O
is	O
signiﬁcant	O
.	O
1.5.	O
decision	B
theory	I
we	O
have	O
seen	O
in	O
section	O
1.2	O
how	O
probability	B
theory	O
provides	O
us	O
with	O
a	O
consistent	B
mathematical	O
framework	O
for	O
quantifying	O
and	O
manipulating	O
uncertainty	O
.	O
here	O
we	O
turn	O
to	O
a	O
discussion	O
of	O
decision	B
theory	I
that	O
,	O
when	O
combined	O
with	O
probability	B
theory	O
,	O
allows	O
us	O
to	O
make	O
optimal	O
decisions	O
in	O
situations	O
involving	O
uncertainty	O
such	O
as	O
those	O
encountered	O
in	O
pattern	O
recognition	O
.	O
suppose	O
we	O
have	O
an	O
input	O
vector	O
x	O
together	O
with	O
a	O
corresponding	O
vector	O
t	O
of	O
target	O
variables	O
,	O
and	O
our	O
goal	O
is	O
to	O
predict	O
t	O
given	O
a	O
new	O
value	O
for	O
x.	O
for	B
regression	I
problems	O
,	O
t	O
will	O
comprise	O
continuous	O
variables	O
,	O
whereas	O
for	O
classiﬁcation	O
problems	O
t	O
will	O
represent	O
class	O
labels	O
.	O
the	O
joint	O
probability	B
distribution	O
p	O
(	O
x	O
,	O
t	O
)	O
provides	O
a	O
complete	O
summary	O
of	O
the	O
uncertainty	O
associated	O
with	O
these	O
variables	O
.	O
determination	O
of	O
p	O
(	O
x	O
,	O
t	O
)	O
from	O
a	O
set	O
of	O
training	B
data	O
is	O
an	O
example	O
of	O
inference	B
and	O
is	O
typically	O
a	O
very	O
difﬁcult	O
problem	O
whose	O
solution	O
forms	O
the	O
subject	O
of	O
much	O
of	O
this	O
book	O
.	O
in	O
a	O
practical	O
application	O
,	O
however	O
,	O
we	O
must	O
often	O
make	O
a	O
speciﬁc	O
prediction	O
for	O
the	O
value	O
of	O
t	O
,	O
or	O
more	O
generally	O
take	O
a	O
speciﬁc	O
action	O
based	O
on	O
our	O
understanding	O
of	O
the	O
values	O
t	O
is	O
likely	O
to	O
take	O
,	O
and	O
this	O
aspect	O
is	O
the	O
subject	O
of	O
decision	B
theory	I
.	O
consider	O
,	O
for	O
example	O
,	O
a	O
medical	O
diagnosis	O
problem	O
in	O
which	O
we	O
have	O
taken	O
an	O
x-ray	O
image	O
of	O
a	O
patient	O
,	O
and	O
we	O
wish	O
to	O
determine	O
whether	O
the	O
patient	O
has	O
cancer	O
or	O
not	O
.	O
in	O
this	O
case	O
,	O
the	O
input	O
vector	O
x	O
is	O
the	O
set	O
of	O
pixel	O
intensities	O
in	O
the	O
image	O
,	O
and	O
output	O
variable	O
t	O
will	O
represent	O
the	O
presence	O
of	O
cancer	O
,	O
which	O
we	O
denote	O
by	O
the	O
class	O
c1	O
,	O
or	O
the	O
absence	O
of	O
cancer	O
,	O
which	O
we	O
denote	O
by	O
the	O
class	O
c2	O
.	O
we	O
might	O
,	O
for	O
instance	O
,	O
choose	O
t	O
to	O
be	O
a	O
binary	O
variable	O
such	O
that	O
t	O
=	O
0	O
corresponds	O
to	O
class	O
c1	O
and	O
t	O
=	O
1	O
corresponds	O
to	O
class	O
c2	O
.	O
we	O
shall	O
see	O
later	O
that	O
this	O
choice	O
of	O
label	O
values	O
is	O
particularly	O
convenient	O
for	O
probabilistic	O
models	O
.	O
the	O
general	O
inference	B
problem	O
then	O
involves	O
determining	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
ck	O
)	O
,	O
or	O
equivalently	O
p	O
(	O
x	O
,	O
t	O
)	O
,	O
which	O
gives	O
us	O
the	O
most	O
complete	O
probabilistic	O
description	O
of	O
the	O
situation	O
.	O
although	O
this	O
can	O
be	O
a	O
very	O
useful	O
and	O
informative	O
quantity	O
,	O
in	O
the	O
end	O
we	O
must	O
decide	O
either	O
to	O
give	O
treatment	O
to	O
the	O
patient	O
or	O
not	O
,	O
and	O
we	O
would	O
like	O
this	O
choice	O
to	O
be	O
optimal	O
in	O
some	O
appropriate	O
sense	O
(	O
duda	O
and	O
hart	O
,	O
1973	O
)	O
.	O
this	O
is	O
the	O
decision	O
step	O
,	O
and	O
it	O
is	O
the	O
subject	O
of	O
decision	B
theory	I
to	O
tell	O
us	O
how	O
to	O
make	O
optimal	O
decisions	O
given	O
the	O
appropriate	O
probabilities	O
.	O
we	O
shall	O
see	O
that	O
the	O
decision	O
stage	O
is	O
generally	O
very	O
simple	O
,	O
even	O
trivial	O
,	O
once	O
we	O
have	O
solved	O
the	O
inference	B
problem	O
.	O
here	O
we	O
give	O
an	O
introduction	O
to	O
the	O
key	O
ideas	O
of	O
decision	B
theory	I
as	O
required	O
for	O
1.5.	O
decision	B
theory	I
39	O
the	O
rest	O
of	O
the	O
book	O
.	O
further	O
background	O
,	O
as	O
well	O
as	O
more	O
detailed	O
accounts	O
,	O
can	O
be	O
found	O
in	O
berger	O
(	O
1985	O
)	O
and	O
bather	O
(	O
2000	O
)	O
.	O
before	O
giving	O
a	O
more	O
detailed	O
analysis	O
,	O
let	O
us	O
ﬁrst	O
consider	O
informally	O
how	O
we	O
might	O
expect	O
probabilities	O
to	O
play	O
a	O
role	O
in	O
making	O
decisions	O
.	O
when	O
we	O
obtain	O
the	O
x-ray	O
image	O
x	O
for	O
a	O
new	O
patient	O
,	O
our	O
goal	O
is	O
to	O
decide	O
which	O
of	O
the	O
two	O
classes	O
to	O
assign	O
to	O
the	O
image	O
.	O
we	O
are	O
interested	O
in	O
the	O
probabilities	O
of	O
the	O
two	O
classes	O
given	O
the	O
image	O
,	O
which	O
are	O
given	O
by	O
p	O
(	O
ck|x	O
)	O
.	O
using	O
bayes	O
’	O
theorem	O
,	O
these	O
probabilities	O
can	O
be	O
expressed	O
in	O
the	O
form	O
p	O
(	O
ck|x	O
)	O
=	O
p	O
(	O
x|ck	O
)	O
p	O
(	O
ck	O
)	O
p	O
(	O
x	O
)	O
.	O
(	O
1.77	O
)	O
note	O
that	O
any	O
of	O
the	O
quantities	O
appearing	O
in	O
bayes	O
’	O
theorem	O
can	O
be	O
obtained	O
from	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
ck	O
)	O
by	O
either	O
marginalizing	O
or	O
conditioning	O
with	O
respect	O
to	O
the	O
appropriate	O
variables	O
.	O
we	O
can	O
now	O
interpret	O
p	O
(	O
ck	O
)	O
as	O
the	O
prior	B
probability	O
for	O
the	O
class	O
ck	O
,	O
and	O
p	O
(	O
ck|x	O
)	O
as	O
the	O
corresponding	O
posterior	B
probability	I
.	O
thus	O
p	O
(	O
c1	O
)	O
repre-	O
sents	O
the	O
probability	B
that	O
a	O
person	O
has	O
cancer	O
,	O
before	O
we	O
take	O
the	O
x-ray	O
measurement	O
.	O
similarly	O
,	O
p	O
(	O
c1|x	O
)	O
is	O
the	O
corresponding	O
probability	B
,	O
revised	O
using	O
bayes	O
’	O
theorem	O
in	O
light	O
of	O
the	O
information	O
contained	O
in	O
the	O
x-ray	O
.	O
if	O
our	O
aim	O
is	O
to	O
minimize	O
the	O
chance	O
of	O
assigning	O
x	O
to	O
the	O
wrong	O
class	O
,	O
then	O
intuitively	O
we	O
would	O
choose	O
the	O
class	O
having	O
the	O
higher	O
posterior	B
probability	I
.	O
we	O
now	O
show	O
that	O
this	O
intuition	O
is	O
correct	O
,	O
and	O
we	O
also	O
discuss	O
more	O
general	O
criteria	O
for	O
making	O
decisions	O
.	O
1.5.1	O
minimizing	O
the	O
misclassiﬁcation	O
rate	O
suppose	O
that	O
our	O
goal	O
is	O
simply	O
to	O
make	O
as	O
few	O
misclassiﬁcations	O
as	O
possible	O
.	O
we	O
need	O
a	O
rule	O
that	O
assigns	O
each	O
value	O
of	O
x	O
to	O
one	O
of	O
the	O
available	O
classes	O
.	O
such	O
a	O
rule	O
will	O
divide	O
the	O
input	O
space	O
into	O
regions	O
rk	O
called	O
decision	O
regions	O
,	O
one	O
for	O
each	O
class	O
,	O
such	O
that	O
all	O
points	O
in	O
rk	O
are	O
assigned	O
to	O
class	O
ck	O
.	O
the	O
boundaries	O
between	O
decision	O
regions	O
are	O
called	O
decision	O
boundaries	O
or	O
decision	O
surfaces	O
.	O
note	O
that	O
each	O
decision	B
region	I
need	O
not	O
be	O
contiguous	O
but	O
could	O
comprise	O
some	O
number	O
of	O
disjoint	O
regions	O
.	O
we	O
shall	O
encounter	O
examples	O
of	O
decision	O
boundaries	O
and	O
decision	O
regions	O
in	O
later	O
chapters	O
.	O
in	O
order	O
to	O
ﬁnd	O
the	O
optimal	O
decision	O
rule	O
,	O
consider	O
ﬁrst	O
of	O
all	O
the	O
case	O
of	O
two	O
classes	O
,	O
as	O
in	O
the	O
cancer	O
problem	O
for	O
instance	O
.	O
a	O
mistake	O
occurs	O
when	O
an	O
input	O
vector	O
belonging	O
to	O
class	O
c1	O
is	O
assigned	O
to	O
class	O
c2	O
or	O
vice	O
versa	O
.	O
the	O
probability	B
of	O
this	O
occurring	O
is	O
given	O
by	O
p	O
(	O
mistake	O
)	O
=	O
p	O
(	O
x	O
∈	O
r1	O
,	O
c2	O
)	O
+	O
p	O
(	O
x	O
∈	O
r2	O
,	O
c1	O
)	O
p	O
(	O
x	O
,	O
c2	O
)	O
dx	O
+	O
p	O
(	O
x	O
,	O
c1	O
)	O
dx	O
.	O
(	O
1.78	O
)	O
(	O
cid:6	O
)	O
=	O
r1	O
(	O
cid:6	O
)	O
r2	O
we	O
are	O
free	O
to	O
choose	O
the	O
decision	O
rule	O
that	O
assigns	O
each	O
point	O
x	O
to	O
one	O
of	O
the	O
two	O
classes	O
.	O
clearly	O
to	O
minimize	O
p	O
(	O
mistake	O
)	O
we	O
should	O
arrange	O
that	O
each	O
x	O
is	O
assigned	O
to	O
whichever	O
class	O
has	O
the	O
smaller	O
value	O
of	O
the	O
integrand	O
in	O
(	O
1.78	O
)	O
.	O
thus	O
,	O
if	O
p	O
(	O
x	O
,	O
c1	O
)	O
>	O
p	O
(	O
x	O
,	O
c2	O
)	O
for	O
a	O
given	O
value	O
of	O
x	O
,	O
then	O
we	O
should	O
assign	O
that	O
x	O
to	O
class	O
c1	O
.	O
from	O
the	O
product	B
rule	I
of	I
probability	I
we	O
have	O
p	O
(	O
x	O
,	O
ck	O
)	O
=	O
p	O
(	O
ck|x	O
)	O
p	O
(	O
x	O
)	O
.	O
because	O
the	O
factor	O
p	O
(	O
x	O
)	O
is	O
common	O
to	O
both	O
terms	O
,	O
we	O
can	O
restate	O
this	O
result	O
as	O
saying	O
that	O
the	O
minimum	O
40	O
1.	O
introduction	O
(	O
cid:1	O
)	O
x	O
x0	O
p	O
(	O
x	O
,	O
c1	O
)	O
p	O
(	O
x	O
,	O
c2	O
)	O
r1	O
x	O
r2	O
figure	O
1.24	O
schematic	O
illustration	O
of	O
the	O
joint	O
probabilities	O
p	O
(	O
x	O
,	O
ck	O
)	O
for	O
each	O
of	O
two	O
classes	O
plotted	O
against	O
x	O
,	O
together	O
with	O
the	O
decision	B
boundary	I
x	O
=	O
bx	O
.	O
values	O
of	O
x	O
(	O
cid:2	O
)	O
bx	O
are	O
classiﬁed	O
as	O
class	O
c2	O
and	O
hence	O
belong	O
to	O
decision	B
region	I
r2	O
,	O
whereas	O
points	O
x	O
<	O
bx	O
are	O
classiﬁed	O
as	O
c1	O
and	O
belong	O
to	O
r1	O
.	O
errors	O
arise	O
from	O
the	O
blue	O
,	O
green	O
,	O
and	O
red	O
regions	O
,	O
so	O
that	O
for	O
x	O
<	O
bx	O
the	O
errors	O
are	O
due	O
to	O
points	O
from	O
class	O
c2	O
being	O
misclassiﬁed	O
as	O
c1	O
(	O
represented	O
by	O
the	O
sum	O
of	O
the	O
red	O
and	O
green	O
regions	O
)	O
,	O
and	O
conversely	O
for	O
points	O
in	O
the	O
region	O
x	O
(	O
cid:2	O
)	O
bx	O
the	O
errors	O
are	O
due	O
to	O
points	O
from	O
class	O
c1	O
being	O
misclassiﬁed	O
as	O
c2	O
(	O
represented	O
by	O
the	O
blue	O
region	O
)	O
.	O
as	O
we	O
vary	O
the	O
location	O
bx	O
of	O
the	O
decision	B
boundary	I
,	O
the	O
combined	O
areas	O
of	O
the	O
blue	O
and	O
green	O
regions	O
remains	O
constant	O
,	O
whereas	O
the	O
size	O
of	O
the	O
red	O
region	O
varies	O
.	O
the	O
optimal	O
choice	O
for	O
bx	O
is	O
where	O
the	O
curves	O
for	O
p	O
(	O
x	O
,	O
c1	O
)	O
and	O
p	O
(	O
x	O
,	O
c2	O
)	O
cross	O
,	O
corresponding	O
to	O
bx	O
=	O
x0	O
,	O
because	O
in	O
this	O
case	O
the	O
red	O
region	O
disappears	O
.	O
this	O
is	O
equivalent	O
to	O
the	O
minimum	O
misclassiﬁcation	O
rate	O
decision	O
rule	O
,	O
which	O
assigns	O
each	O
value	O
of	O
x	O
to	O
the	O
class	O
having	O
the	O
higher	O
posterior	B
probability	I
p	O
(	O
ck|x	O
)	O
.	O
probability	B
of	O
making	O
a	O
mistake	O
is	O
obtained	O
if	O
each	O
value	O
of	O
x	O
is	O
assigned	O
to	O
the	O
class	O
for	O
which	O
the	O
posterior	B
probability	I
p	O
(	O
ck|x	O
)	O
is	O
largest	O
.	O
this	O
result	O
is	O
illustrated	O
for	O
two	O
classes	O
,	O
and	O
a	O
single	O
input	O
variable	O
x	O
,	O
in	O
figure	O
1.24.	O
for	O
the	O
more	O
general	O
case	O
of	O
k	O
classes	O
,	O
it	O
is	O
slightly	O
easier	O
to	O
maximize	O
the	O
probability	B
of	O
being	O
correct	O
,	O
which	O
is	O
given	O
by	O
k	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
k=1	O
rk	O
k=1	O
p	O
(	O
x	O
∈	O
rk	O
,	O
ck	O
)	O
(	O
cid:6	O
)	O
p	O
(	O
correct	O
)	O
=	O
=	O
p	O
(	O
x	O
,	O
ck	O
)	O
dx	O
(	O
1.79	O
)	O
which	O
is	O
maximized	O
when	O
the	O
regions	O
rk	O
are	O
chosen	O
such	O
that	O
each	O
x	O
is	O
assigned	O
to	O
the	O
class	O
for	O
which	O
p	O
(	O
x	O
,	O
ck	O
)	O
is	O
largest	O
.	O
again	O
,	O
using	O
the	O
product	B
rule	I
p	O
(	O
x	O
,	O
ck	O
)	O
=	O
p	O
(	O
ck|x	O
)	O
p	O
(	O
x	O
)	O
,	O
and	O
noting	O
that	O
the	O
factor	O
of	O
p	O
(	O
x	O
)	O
is	O
common	O
to	O
all	O
terms	O
,	O
we	O
see	O
that	O
each	O
x	O
should	O
be	O
assigned	O
to	O
the	O
class	O
having	O
the	O
largest	O
posterior	B
probability	I
p	O
(	O
ck|x	O
)	O
.	O
figure	O
1.25	O
an	O
example	O
of	O
a	O
loss	B
matrix	I
with	O
ele-	O
ments	O
lkj	O
for	O
the	O
cancer	O
treatment	O
problem	O
.	O
the	O
rows	O
correspond	O
to	O
the	O
true	O
class	O
,	O
whereas	O
the	O
columns	O
cor-	O
respond	O
to	O
the	O
assignment	O
of	O
class	O
made	O
by	O
our	O
deci-	O
sion	B
criterion	O
.	O
cancer	O
normal	O
1.5.	O
decision	B
theory	I
(	O
cid:15	O
)	O
cancer	O
normal	O
(	O
cid:16	O
)	O
0	O
1	O
1000	O
0	O
41	O
1.5.2	O
minimizing	O
the	O
expected	O
loss	O
for	O
many	O
applications	O
,	O
our	O
objective	O
will	O
be	O
more	O
complex	O
than	O
simply	O
mini-	O
mizing	O
the	O
number	O
of	O
misclassiﬁcations	O
.	O
let	O
us	O
consider	O
again	O
the	O
medical	O
diagnosis	O
problem	O
.	O
we	O
note	O
that	O
,	O
if	O
a	O
patient	O
who	O
does	O
not	O
have	O
cancer	O
is	O
incorrectly	O
diagnosed	O
as	O
having	O
cancer	O
,	O
the	O
consequences	O
may	O
be	O
some	O
patient	O
distress	O
plus	O
the	O
need	O
for	O
further	O
investigations	O
.	O
conversely	O
,	O
if	O
a	O
patient	O
with	O
cancer	O
is	O
diagnosed	O
as	O
healthy	O
,	O
the	O
result	O
may	O
be	O
premature	O
death	O
due	O
to	O
lack	O
of	O
treatment	O
.	O
thus	O
the	O
consequences	O
of	O
these	O
two	O
types	O
of	O
mistake	O
can	O
be	O
dramatically	O
different	O
.	O
it	O
would	O
clearly	O
be	O
better	O
to	O
make	O
fewer	O
mistakes	O
of	O
the	O
second	O
kind	O
,	O
even	O
if	O
this	O
was	O
at	O
the	O
expense	O
of	O
making	O
more	O
mistakes	O
of	O
the	O
ﬁrst	O
kind	O
.	O
we	O
can	O
formalize	O
such	O
issues	O
through	O
the	O
introduction	O
of	O
a	O
loss	B
function	I
,	O
also	O
called	O
a	O
cost	B
function	I
,	O
which	O
is	O
a	O
single	O
,	O
overall	O
measure	O
of	O
loss	O
incurred	O
in	O
taking	O
any	O
of	O
the	O
available	O
decisions	O
or	O
actions	O
.	O
our	O
goal	O
is	O
then	O
to	O
minimize	O
the	O
total	O
loss	O
incurred	O
.	O
note	O
that	O
some	O
authors	O
consider	O
instead	O
a	O
utility	B
function	I
,	O
whose	O
value	O
they	O
aim	O
to	O
maximize	O
.	O
these	O
are	O
equivalent	O
concepts	O
if	O
we	O
take	O
the	O
utility	O
to	O
be	O
simply	O
the	O
negative	O
of	O
the	O
loss	O
,	O
and	O
throughout	O
this	O
text	O
we	O
shall	O
use	O
the	O
loss	B
function	I
convention	O
.	O
suppose	O
that	O
,	O
for	O
a	O
new	O
value	O
of	O
x	O
,	O
the	O
true	O
class	O
is	O
ck	O
and	O
that	O
we	O
assign	O
x	O
to	O
class	O
cj	O
(	O
where	O
j	O
may	O
or	O
may	O
not	O
be	O
equal	O
to	O
k	O
)	O
.	O
in	O
so	O
doing	O
,	O
we	O
incur	O
some	O
level	O
of	O
loss	O
that	O
we	O
denote	O
by	O
lkj	O
,	O
which	O
we	O
can	O
view	O
as	O
the	O
k	O
,	O
j	O
element	O
of	O
a	O
loss	B
matrix	I
.	O
for	O
instance	O
,	O
in	O
our	O
cancer	O
example	O
,	O
we	O
might	O
have	O
a	O
loss	B
matrix	I
of	O
the	O
form	O
shown	O
in	O
figure	O
1.25.	O
this	O
particular	O
loss	B
matrix	I
says	O
that	O
there	O
is	O
no	O
loss	O
incurred	O
if	O
the	O
correct	O
decision	O
is	O
made	O
,	O
there	O
is	O
a	O
loss	O
of	O
1	O
if	O
a	O
healthy	O
patient	O
is	O
diagnosed	O
as	O
having	O
cancer	O
,	O
whereas	O
there	O
is	O
a	O
loss	O
of	O
1000	O
if	O
a	O
patient	O
having	O
cancer	O
is	O
diagnosed	O
as	O
healthy	O
.	O
the	O
optimal	O
solution	O
is	O
the	O
one	O
which	O
minimizes	O
the	O
loss	B
function	I
.	O
however	O
,	O
the	O
loss	B
function	I
depends	O
on	O
the	O
true	O
class	O
,	O
which	O
is	O
unknown	O
.	O
for	O
a	O
given	O
input	O
vector	O
x	O
,	O
our	O
uncertainty	O
in	O
the	O
true	O
class	O
is	O
expressed	O
through	O
the	O
joint	O
probability	B
distribution	O
p	O
(	O
x	O
,	O
ck	O
)	O
and	O
so	O
we	O
seek	O
instead	O
to	O
minimize	O
the	O
average	O
loss	O
,	O
where	O
the	O
average	O
is	O
computed	O
with	O
respect	O
to	O
this	O
distribution	O
,	O
which	O
is	O
given	O
by	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:6	O
)	O
e	O
[	O
l	O
]	O
=	O
k	O
j	O
rj	O
lkjp	O
(	O
x	O
,	O
ck	O
)	O
dx	O
.	O
(	O
1.80	O
)	O
(	O
cid:5	O
)	O
each	O
x	O
can	O
be	O
assigned	O
independently	O
to	O
one	O
of	O
the	O
decision	O
regions	O
rj	O
.	O
our	O
goal	O
is	O
to	O
choose	O
the	O
regions	O
rj	O
in	O
order	O
to	O
minimize	O
the	O
expected	O
loss	O
(	O
1.80	O
)	O
,	O
which	O
k	O
lkjp	O
(	O
x	O
,	O
ck	O
)	O
.	O
as	O
before	O
,	O
we	O
can	O
use	O
implies	O
that	O
for	O
each	O
x	O
we	O
should	O
minimize	O
the	O
product	B
rule	I
p	O
(	O
x	O
,	O
ck	O
)	O
=	O
p	O
(	O
ck|x	O
)	O
p	O
(	O
x	O
)	O
to	O
eliminate	O
the	O
common	O
factor	O
of	O
p	O
(	O
x	O
)	O
.	O
thus	O
the	O
decision	O
rule	O
that	O
minimizes	O
the	O
expected	O
loss	O
is	O
the	O
one	O
that	O
assigns	O
each	O
42	O
1.	O
introduction	O
figure	O
1.26	O
illustration	O
of	O
the	O
reject	B
option	I
.	O
inputs	O
x	O
such	O
that	O
the	O
larger	O
of	O
the	O
two	O
poste-	O
rior	O
probabilities	O
is	O
less	O
than	O
or	O
equal	O
to	O
some	O
threshold	O
θ	O
will	O
be	O
rejected	O
.	O
1.0	O
θ	O
p	O
(	O
c1|x	O
)	O
p	O
(	O
c2|x	O
)	O
0.0	O
reject	O
region	O
x	O
new	O
x	O
to	O
the	O
class	O
j	O
for	O
which	O
the	O
quantity	O
(	O
cid:2	O
)	O
lkjp	O
(	O
ck|x	O
)	O
k	O
(	O
1.81	O
)	O
is	O
a	O
minimum	O
.	O
this	O
is	O
clearly	O
trivial	O
to	O
do	O
,	O
once	O
we	O
know	O
the	O
posterior	O
class	O
proba-	O
bilities	O
p	O
(	O
ck|x	O
)	O
.	O
1.5.3	O
the	O
reject	B
option	I
we	O
have	O
seen	O
that	O
classiﬁcation	B
errors	O
arise	O
from	O
the	O
regions	O
of	O
input	O
space	O
where	O
the	O
largest	O
of	O
the	O
posterior	O
probabilities	O
p	O
(	O
ck|x	O
)	O
is	O
signiﬁcantly	O
less	O
than	O
unity	O
,	O
or	O
equivalently	O
where	O
the	O
joint	O
distributions	O
p	O
(	O
x	O
,	O
ck	O
)	O
have	O
comparable	O
values	O
.	O
these	O
are	O
the	O
regions	O
where	O
we	O
are	O
relatively	O
uncertain	O
about	O
class	O
membership	O
.	O
in	O
some	O
applications	O
,	O
it	O
will	O
be	O
appropriate	O
to	O
avoid	O
making	O
decisions	O
on	O
the	O
difﬁcult	O
cases	O
in	O
anticipation	O
of	O
a	O
lower	O
error	O
rate	O
on	O
those	O
examples	O
for	O
which	O
a	O
classiﬁcation	B
de-	O
cision	O
is	O
made	O
.	O
this	O
is	O
known	O
as	O
the	O
reject	B
option	I
.	O
for	O
example	O
,	O
in	O
our	O
hypothetical	O
medical	O
illustration	O
,	O
it	O
may	O
be	O
appropriate	O
to	O
use	O
an	O
automatic	O
system	O
to	O
classify	O
those	O
x-ray	O
images	O
for	O
which	O
there	O
is	O
little	O
doubt	O
as	O
to	O
the	O
correct	O
class	O
,	O
while	O
leav-	O
ing	O
a	O
human	O
expert	O
to	O
classify	O
the	O
more	O
ambiguous	O
cases	O
.	O
we	O
can	O
achieve	O
this	O
by	O
introducing	O
a	O
threshold	O
θ	O
and	O
rejecting	O
those	O
inputs	O
x	O
for	O
which	O
the	O
largest	O
of	O
the	O
posterior	O
probabilities	O
p	O
(	O
ck|x	O
)	O
is	O
less	O
than	O
or	O
equal	O
to	O
θ.	O
this	O
is	O
illustrated	O
for	O
the	O
case	O
of	O
two	O
classes	O
,	O
and	O
a	O
single	O
continuous	O
input	O
variable	O
x	O
,	O
in	O
figure	O
1.26.	O
note	O
that	O
setting	O
θ	O
=	O
1	O
will	O
ensure	O
that	O
all	O
examples	O
are	O
rejected	O
,	O
whereas	O
if	O
there	O
are	O
k	O
classes	O
then	O
setting	O
θ	O
<	O
1/k	O
will	O
ensure	O
that	O
no	O
examples	O
are	O
rejected	O
.	O
thus	O
the	O
fraction	O
of	O
examples	O
that	O
get	O
rejected	O
is	O
controlled	O
by	O
the	O
value	O
of	O
θ.	O
we	O
can	O
easily	O
extend	O
the	O
reject	O
criterion	O
to	O
minimize	O
the	O
expected	O
loss	O
,	O
when	O
a	O
loss	B
matrix	I
is	O
given	O
,	O
taking	O
account	O
of	O
the	O
loss	O
incurred	O
when	O
a	O
reject	O
decision	O
is	O
made	O
.	O
1.5.4	O
inference	B
and	O
decision	O
we	O
have	O
broken	O
the	O
classiﬁcation	B
problem	O
down	O
into	O
two	O
separate	O
stages	O
,	O
the	O
inference	B
stage	O
in	O
which	O
we	O
use	O
training	B
data	O
to	O
learn	O
a	O
model	O
for	O
p	O
(	O
ck|x	O
)	O
,	O
and	O
the	O
exercise	O
1.24	O
1.5.	O
decision	B
theory	I
43	O
subsequent	O
decision	O
stage	O
in	O
which	O
we	O
use	O
these	O
posterior	O
probabilities	O
to	O
make	O
op-	O
timal	O
class	O
assignments	O
.	O
an	O
alternative	O
possibility	O
would	O
be	O
to	O
solve	O
both	O
problems	O
together	O
and	O
simply	O
learn	O
a	O
function	O
that	O
maps	O
inputs	O
x	O
directly	O
into	O
decisions	O
.	O
such	O
a	O
function	O
is	O
called	O
a	O
discriminant	B
function	I
.	O
in	O
fact	O
,	O
we	O
can	O
identify	O
three	O
distinct	O
approaches	O
to	O
solving	O
decision	O
problems	O
,	O
all	O
of	O
which	O
have	O
been	O
used	O
in	O
practical	O
applications	O
.	O
these	O
are	O
given	O
,	O
in	O
decreasing	O
order	O
of	O
complexity	O
,	O
by	O
:	O
(	O
a	O
)	O
first	O
solve	O
the	O
inference	B
problem	O
of	O
determining	O
the	O
class-conditional	O
densities	O
p	O
(	O
x|ck	O
)	O
for	O
each	O
class	O
ck	O
individually	O
.	O
also	O
separately	O
infer	O
the	O
prior	B
class	O
probabilities	O
p	O
(	O
ck	O
)	O
.	O
then	O
use	O
bayes	O
’	O
theorem	O
in	O
the	O
form	O
p	O
(	O
ck|x	O
)	O
=	O
p	O
(	O
x|ck	O
)	O
p	O
(	O
ck	O
)	O
(	O
1.82	O
)	O
to	O
ﬁnd	O
the	O
posterior	O
class	O
probabilities	O
p	O
(	O
ck|x	O
)	O
.	O
as	O
usual	O
,	O
the	O
denominator	O
in	O
bayes	O
’	O
theorem	O
can	O
be	O
found	O
in	O
terms	O
of	O
the	O
quantities	O
appearing	O
in	O
the	O
numerator	O
,	O
because	O
p	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
x|ck	O
)	O
p	O
(	O
ck	O
)	O
.	O
p	O
(	O
x	O
)	O
=	O
k	O
(	O
1.83	O
)	O
equivalently	O
,	O
we	O
can	O
model	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
ck	O
)	O
directly	O
and	O
then	O
normalize	O
to	O
obtain	O
the	O
posterior	O
probabilities	O
.	O
having	O
found	O
the	O
posterior	O
probabilities	O
,	O
we	O
use	O
decision	B
theory	I
to	O
determine	O
class	O
membership	O
for	O
each	O
new	O
input	O
x.	O
approaches	O
that	O
explicitly	O
or	O
implicitly	O
model	O
the	O
distribution	O
of	O
inputs	O
as	O
well	O
as	O
outputs	O
are	O
known	O
as	O
generative	O
models	O
,	O
because	O
by	O
sampling	O
from	O
them	O
it	O
is	O
possible	O
to	O
generate	O
synthetic	O
data	O
points	O
in	O
the	O
input	O
space	O
.	O
(	O
b	O
)	O
first	O
solve	O
the	O
inference	B
problem	O
of	O
determining	O
the	O
posterior	O
class	O
probabilities	O
p	O
(	O
ck|x	O
)	O
,	O
and	O
then	O
subsequently	O
use	O
decision	B
theory	I
to	O
assign	O
each	O
new	O
x	O
to	O
one	O
of	O
the	O
classes	O
.	O
approaches	O
that	O
model	O
the	O
posterior	O
probabilities	O
directly	O
are	O
called	O
discriminative	O
models	O
.	O
(	O
c	O
)	O
find	O
a	O
function	O
f	O
(	O
x	O
)	O
,	O
called	O
a	O
discriminant	B
function	I
,	O
which	O
maps	O
each	O
input	O
x	O
directly	O
onto	O
a	O
class	O
label	O
.	O
for	O
instance	O
,	O
in	O
the	O
case	O
of	O
two-class	O
problems	O
,	O
f	O
(	O
·	O
)	O
might	O
be	O
binary	O
valued	O
and	O
such	O
that	O
f	O
=	O
0	O
represents	O
class	O
c1	O
and	O
f	O
=	O
1	O
represents	O
class	O
c2	O
.	O
in	O
this	O
case	O
,	O
probabilities	O
play	O
no	O
role	O
.	O
let	O
us	O
consider	O
the	O
relative	B
merits	O
of	O
these	O
three	O
alternatives	O
.	O
approach	O
(	O
a	O
)	O
is	O
the	O
most	O
demanding	O
because	O
it	O
involves	O
ﬁnding	O
the	O
joint	O
distribution	O
over	O
both	O
x	O
and	O
ck	O
.	O
for	O
many	O
applications	O
,	O
x	O
will	O
have	O
high	O
dimensionality	O
,	O
and	O
consequently	O
we	O
may	O
need	O
a	O
large	O
training	O
set	O
in	O
order	O
to	O
be	O
able	O
to	O
determine	O
the	O
class-conditional	O
densities	O
to	O
reasonable	O
accuracy	O
.	O
note	O
that	O
the	O
class	O
priors	O
p	O
(	O
ck	O
)	O
can	O
often	O
be	O
esti-	O
mated	O
simply	O
from	O
the	O
fractions	O
of	O
the	O
training	B
set	I
data	O
points	O
in	O
each	O
of	O
the	O
classes	O
.	O
one	O
advantage	O
of	O
approach	O
(	O
a	O
)	O
,	O
however	O
,	O
is	O
that	O
it	O
also	O
allows	O
the	O
marginal	B
density	O
of	O
data	O
p	O
(	O
x	O
)	O
to	O
be	O
determined	O
from	O
(	O
1.83	O
)	O
.	O
this	O
can	O
be	O
useful	O
for	O
detecting	O
new	O
data	O
points	O
that	O
have	O
low	O
probability	B
under	O
the	O
model	O
and	O
for	O
which	O
the	O
predictions	O
may	O
44	O
1.	O
introduction	O
s	O
e	O
i	O
t	O
i	O
s	O
n	O
e	O
d	O
s	O
s	O
a	O
c	O
l	O
5	O
4	O
3	O
2	O
1	O
0	O
0	O
p	O
(	O
x|c2	O
)	O
p	O
(	O
x|c1	O
)	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
x	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
0	O
p	O
(	O
c1|x	O
)	O
p	O
(	O
c2|x	O
)	O
0.2	O
0.4	O
x	O
0.6	O
0.8	O
1	O
figure	O
1.27	O
example	O
of	O
the	O
class-conditional	O
densities	O
for	O
two	O
classes	O
having	O
a	O
single	O
input	O
variable	O
x	O
(	O
left	O
plot	O
)	O
together	O
with	O
the	O
corresponding	O
posterior	O
probabilities	O
(	O
right	O
plot	O
)	O
.	O
note	O
that	O
the	O
left-hand	O
mode	O
of	O
the	O
class-conditional	O
density	B
p	O
(	O
x|c1	O
)	O
,	O
shown	O
in	O
blue	O
on	O
the	O
left	O
plot	O
,	O
has	O
no	O
effect	O
on	O
the	O
posterior	O
probabilities	O
.	O
the	O
vertical	O
green	O
line	O
in	O
the	O
right	O
plot	O
shows	O
the	O
decision	B
boundary	I
in	O
x	O
that	O
gives	O
the	O
minimum	O
misclassiﬁcation	O
rate	O
.	O
be	O
of	O
low	O
accuracy	O
,	O
which	O
is	O
known	O
as	O
outlier	B
detection	O
or	O
novelty	B
detection	I
(	O
bishop	O
,	O
1994	O
;	O
tarassenko	O
,	O
1995	O
)	O
.	O
however	O
,	O
if	O
we	O
only	O
wish	O
to	O
make	O
classiﬁcation	B
decisions	O
,	O
then	O
it	O
can	O
be	O
waste-	O
ful	O
of	O
computational	O
resources	O
,	O
and	O
excessively	O
demanding	O
of	O
data	O
,	O
to	O
ﬁnd	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
ck	O
)	O
when	O
in	O
fact	O
we	O
only	O
really	O
need	O
the	O
posterior	O
probabilities	O
p	O
(	O
ck|x	O
)	O
,	O
which	O
can	O
be	O
obtained	O
directly	O
through	O
approach	O
(	O
b	O
)	O
.	O
indeed	O
,	O
the	O
class-	O
conditional	B
densities	O
may	O
contain	O
a	O
lot	O
of	O
structure	O
that	O
has	O
little	O
effect	O
on	O
the	O
pos-	O
terior	O
probabilities	O
,	O
as	O
illustrated	O
in	O
figure	O
1.27.	O
there	O
has	O
been	O
much	O
interest	O
in	O
exploring	O
the	O
relative	B
merits	O
of	O
generative	O
and	O
discriminative	O
approaches	O
to	O
machine	O
learning	O
,	O
and	O
in	O
ﬁnding	O
ways	O
to	O
combine	O
them	O
(	O
jebara	O
,	O
2004	O
;	O
lasserre	O
et	O
al.	O
,	O
2006	O
)	O
.	O
an	O
even	O
simpler	O
approach	O
is	O
(	O
c	O
)	O
in	O
which	O
we	O
use	O
the	O
training	B
data	O
to	O
ﬁnd	O
a	O
discriminant	B
function	I
f	O
(	O
x	O
)	O
that	O
maps	O
each	O
x	O
directly	O
onto	O
a	O
class	O
label	O
,	O
thereby	O
combining	O
the	O
inference	B
and	O
decision	O
stages	O
into	O
a	O
single	O
learning	B
problem	O
.	O
in	O
the	O
example	O
of	O
figure	O
1.27	O
,	O
this	O
would	O
correspond	O
to	O
ﬁnding	O
the	O
value	O
of	O
x	O
shown	O
by	O
the	O
vertical	O
green	O
line	O
,	O
because	O
this	O
is	O
the	O
decision	B
boundary	I
giving	O
the	O
minimum	O
probability	O
of	O
misclassiﬁcation	O
.	O
with	O
option	O
(	O
c	O
)	O
,	O
however	O
,	O
we	O
no	O
longer	O
have	O
access	O
to	O
the	O
posterior	O
probabilities	O
p	O
(	O
ck|x	O
)	O
.	O
there	O
are	O
many	O
powerful	O
reasons	O
for	O
wanting	O
to	O
compute	O
the	O
posterior	O
probabilities	O
,	O
even	O
if	O
we	O
subsequently	O
use	O
them	O
to	O
make	O
decisions	O
.	O
these	O
include	O
:	O
minimizing	O
risk	O
.	O
consider	O
a	O
problem	O
in	O
which	O
the	O
elements	O
of	O
the	O
loss	B
matrix	I
are	O
subjected	O
to	O
revision	O
from	O
time	O
to	O
time	O
(	O
such	O
as	O
might	O
occur	O
in	O
a	O
ﬁnancial	O
1.5.	O
decision	B
theory	I
45	O
application	O
)	O
.	O
if	O
we	O
know	O
the	O
posterior	O
probabilities	O
,	O
we	O
can	O
trivially	O
revise	O
the	O
minimum	B
risk	I
decision	O
criterion	O
by	O
modifying	O
(	O
1.81	O
)	O
appropriately	O
.	O
if	O
we	O
have	O
only	O
a	O
discriminant	B
function	I
,	O
then	O
any	O
change	O
to	O
the	O
loss	B
matrix	I
would	O
require	O
that	O
we	O
return	O
to	O
the	O
training	B
data	O
and	O
solve	O
the	O
classiﬁcation	B
problem	O
afresh	O
.	O
reject	B
option	I
.	O
posterior	O
probabilities	O
allow	O
us	O
to	O
determine	O
a	O
rejection	O
criterion	O
that	O
will	O
minimize	O
the	O
misclassiﬁcation	O
rate	O
,	O
or	O
more	O
generally	O
the	O
expected	O
loss	O
,	O
for	O
a	O
given	O
fraction	O
of	O
rejected	O
data	O
points	O
.	O
compensating	O
for	O
class	O
priors	O
.	O
consider	O
our	O
medical	O
x-ray	O
problem	O
again	O
,	O
and	O
suppose	O
that	O
we	O
have	O
collected	O
a	O
large	O
number	O
of	O
x-ray	O
images	O
from	O
the	O
gen-	O
eral	O
population	O
for	O
use	O
as	O
training	B
data	O
in	O
order	O
to	O
build	O
an	O
automated	O
screening	O
system	O
.	O
because	O
cancer	O
is	O
rare	O
amongst	O
the	O
general	O
population	O
,	O
we	O
might	O
ﬁnd	O
that	O
,	O
say	O
,	O
only	O
1	O
in	O
every	O
1,000	O
examples	O
corresponds	O
to	O
the	O
presence	O
of	O
can-	O
cer	O
.	O
if	O
we	O
used	O
such	O
a	O
data	O
set	O
to	O
train	O
an	O
adaptive	O
model	O
,	O
we	O
could	O
run	O
into	O
severe	O
difﬁculties	O
due	O
to	O
the	O
small	O
proportion	O
of	O
the	O
cancer	O
class	O
.	O
for	O
instance	O
,	O
a	O
classiﬁer	O
that	O
assigned	O
every	O
point	O
to	O
the	O
normal	O
class	O
would	O
already	O
achieve	O
99.9	O
%	O
accuracy	O
and	O
it	O
would	O
be	O
difﬁcult	O
to	O
avoid	O
this	O
trivial	O
solution	O
.	O
also	O
,	O
even	O
a	O
large	O
data	O
set	O
will	O
contain	O
very	O
few	O
examples	O
of	O
x-ray	O
images	O
corre-	O
sponding	O
to	O
cancer	O
,	O
and	O
so	O
the	O
learning	B
algorithm	O
will	O
not	O
be	O
exposed	O
to	O
a	O
broad	O
range	O
of	O
examples	O
of	O
such	O
images	O
and	O
hence	O
is	O
not	O
likely	O
to	O
generalize	O
well	O
.	O
a	O
balanced	O
data	O
set	O
in	O
which	O
we	O
have	O
selected	O
equal	O
numbers	O
of	O
exam-	O
ples	O
from	O
each	O
of	O
the	O
classes	O
would	O
allow	O
us	O
to	O
ﬁnd	O
a	O
more	O
accurate	O
model	O
.	O
however	O
,	O
we	O
then	O
have	O
to	O
compensate	O
for	O
the	O
effects	O
of	O
our	O
modiﬁcations	O
to	O
the	O
training	B
data	O
.	O
suppose	O
we	O
have	O
used	O
such	O
a	O
modiﬁed	O
data	O
set	O
and	O
found	O
models	O
for	O
the	O
posterior	O
probabilities	O
.	O
from	O
bayes	O
’	O
theorem	O
(	O
1.82	O
)	O
,	O
we	O
see	O
that	O
the	O
posterior	O
probabilities	O
are	O
proportional	O
to	O
the	O
prior	B
probabilities	O
,	O
which	O
we	O
can	O
interpret	O
as	O
the	O
fractions	O
of	O
points	O
in	O
each	O
class	O
.	O
we	O
can	O
therefore	O
simply	O
take	O
the	O
posterior	O
probabilities	O
obtained	O
from	O
our	O
artiﬁcially	O
balanced	O
data	O
set	O
and	O
ﬁrst	O
divide	O
by	O
the	O
class	O
fractions	O
in	O
that	O
data	O
set	O
and	O
then	O
multiply	O
by	O
the	O
class	O
fractions	O
in	O
the	O
population	O
to	O
which	O
we	O
wish	O
to	O
apply	O
the	O
model	O
.	O
finally	O
,	O
we	O
need	O
to	O
normalize	O
to	O
ensure	O
that	O
the	O
new	O
posterior	O
probabilities	O
sum	O
to	O
one	O
.	O
note	O
that	O
this	O
procedure	O
can	O
not	O
be	O
applied	O
if	O
we	O
have	O
learned	O
a	O
discriminant	B
function	I
directly	O
instead	O
of	O
determining	O
posterior	O
probabilities	O
.	O
combining	B
models	I
.	O
for	O
complex	O
applications	O
,	O
we	O
may	O
wish	O
to	O
break	O
the	O
problem	O
into	O
a	O
number	O
of	O
smaller	O
subproblems	O
each	O
of	O
which	O
can	O
be	O
tackled	O
by	O
a	O
sep-	O
arate	O
module	O
.	O
for	O
example	O
,	O
in	O
our	O
hypothetical	O
medical	O
diagnosis	O
problem	O
,	O
we	O
may	O
have	O
information	O
available	O
from	O
,	O
say	O
,	O
blood	O
tests	O
as	O
well	O
as	O
x-ray	O
im-	O
ages	O
.	O
rather	O
than	O
combine	O
all	O
of	O
this	O
heterogeneous	O
information	O
into	O
one	O
huge	O
input	O
space	O
,	O
it	O
may	O
be	O
more	O
effective	O
to	O
build	O
one	O
system	O
to	O
interpret	O
the	O
x-	O
ray	O
images	O
and	O
a	O
different	O
one	O
to	O
interpret	O
the	O
blood	O
data	O
.	O
as	O
long	O
as	O
each	O
of	O
the	O
two	O
models	O
gives	O
posterior	O
probabilities	O
for	O
the	O
classes	O
,	O
we	O
can	O
combine	O
the	O
outputs	O
systematically	O
using	O
the	O
rules	O
of	O
probability	B
.	O
one	O
simple	O
way	O
to	O
do	O
this	O
is	O
to	O
assume	O
that	O
,	O
for	O
each	O
class	O
separately	O
,	O
the	O
distributions	O
of	O
inputs	O
for	O
the	O
x-ray	O
images	O
,	O
denoted	O
by	O
xi	O
,	O
and	O
the	O
blood	O
data	O
,	O
denoted	O
by	O
xb	O
,	O
are	O
46	O
1.	O
introduction	O
independent	B
,	O
so	O
that	O
p	O
(	O
xi	O
,	O
xb|ck	O
)	O
=	O
p	O
(	O
xi|ck	O
)	O
p	O
(	O
xb|ck	O
)	O
.	O
(	O
1.84	O
)	O
section	O
8.2	O
section	O
8.2.2	O
section	O
1.1	O
appendix	O
d	O
this	O
is	O
an	O
example	O
of	O
conditional	B
independence	I
property	O
,	O
because	O
the	O
indepen-	O
dence	O
holds	O
when	O
the	O
distribution	O
is	O
conditioned	O
on	O
the	O
class	O
ck	O
.	O
the	O
posterior	B
probability	I
,	O
given	O
both	O
the	O
x-ray	O
and	O
blood	O
data	O
,	O
is	O
then	O
given	O
by	O
p	O
(	O
ck|xi	O
,	O
xb	O
)	O
∝	O
p	O
(	O
xi	O
,	O
xb|ck	O
)	O
p	O
(	O
ck	O
)	O
∝	O
p	O
(	O
xi|ck	O
)	O
p	O
(	O
xb|ck	O
)	O
p	O
(	O
ck	O
)	O
∝	O
p	O
(	O
ck|xi	O
)	O
p	O
(	O
ck|xb	O
)	O
p	O
(	O
ck	O
)	O
(	O
1.85	O
)	O
thus	O
we	O
need	O
the	O
class	O
prior	B
probabilities	O
p	O
(	O
ck	O
)	O
,	O
which	O
we	O
can	O
easily	O
estimate	O
from	O
the	O
fractions	O
of	O
data	O
points	O
in	O
each	O
class	O
,	O
and	O
then	O
we	O
need	O
to	O
normalize	O
the	O
resulting	O
posterior	O
probabilities	O
so	O
they	O
sum	O
to	O
one	O
.	O
the	O
particular	O
condi-	O
tional	O
independence	O
assumption	O
(	O
1.84	O
)	O
is	O
an	O
example	O
of	O
the	O
naive	O
bayes	O
model	O
.	O
note	O
that	O
the	O
joint	O
marginal	B
distribution	O
p	O
(	O
xi	O
,	O
xb	O
)	O
will	O
typically	O
not	O
factorize	O
under	O
this	O
model	O
.	O
we	O
shall	O
see	O
in	O
later	O
chapters	O
how	O
to	O
construct	O
models	O
for	O
combining	O
data	O
that	O
do	O
not	O
require	O
the	O
conditional	B
independence	I
assumption	O
(	O
1.84	O
)	O
.	O
1.5.5	O
loss	O
functions	O
for	B
regression	I
so	O
far	O
,	O
we	O
have	O
discussed	O
decision	B
theory	I
in	O
the	O
context	O
of	O
classiﬁcation	B
prob-	O
lems	O
.	O
we	O
now	O
turn	O
to	O
the	O
case	O
of	O
regression	B
problems	O
,	O
such	O
as	O
the	O
curve	B
ﬁtting	I
example	O
discussed	O
earlier	O
.	O
the	O
decision	O
stage	O
consists	O
of	O
choosing	O
a	O
speciﬁc	O
esti-	O
mate	O
y	O
(	O
x	O
)	O
of	O
the	O
value	O
of	O
t	O
for	O
each	O
input	O
x.	O
suppose	O
that	O
in	O
doing	O
so	O
,	O
we	O
incur	O
a	O
loss	O
l	O
(	O
t	O
,	O
y	O
(	O
x	O
)	O
)	O
.	O
the	O
average	O
,	O
or	O
expected	O
,	O
loss	O
is	O
then	O
given	O
by	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
e	O
[	O
l	O
]	O
=	O
l	O
(	O
t	O
,	O
y	O
(	O
x	O
)	O
)	O
p	O
(	O
x	O
,	O
t	O
)	O
dx	O
dt	O
.	O
(	O
1.86	O
)	O
a	O
common	O
choice	O
of	O
loss	B
function	I
in	O
regression	B
problems	O
is	O
the	O
squared	O
loss	O
given	O
by	O
l	O
(	O
t	O
,	O
y	O
(	O
x	O
)	O
)	O
=	O
{	O
y	O
(	O
x	O
)	O
−	O
t	O
}	O
2.	O
in	O
this	O
case	O
,	O
the	O
expected	O
loss	O
can	O
be	O
written	O
e	O
[	O
l	O
]	O
=	O
{	O
y	O
(	O
x	O
)	O
−	O
t	O
}	O
2p	O
(	O
x	O
,	O
t	O
)	O
dx	O
dt	O
.	O
(	O
1.87	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
our	O
goal	O
is	O
to	O
choose	O
y	O
(	O
x	O
)	O
so	O
as	O
to	O
minimize	O
e	O
[	O
l	O
]	O
.	O
if	O
we	O
assume	O
a	O
completely	O
ﬂexible	O
function	O
y	O
(	O
x	O
)	O
,	O
we	O
can	O
do	O
this	O
formally	O
using	O
the	O
calculus	B
of	I
variations	I
to	O
give	O
(	O
cid:6	O
)	O
{	O
y	O
(	O
x	O
)	O
−	O
t	O
}	O
p	O
(	O
x	O
,	O
t	O
)	O
dt	O
=	O
0	O
.	O
(	O
1.88	O
)	O
solving	O
for	O
y	O
(	O
x	O
)	O
,	O
and	O
using	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
,	O
we	O
obtain	O
δe	O
[	O
l	O
]	O
δy	O
(	O
x	O
)	O
=	O
2	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
y	O
(	O
x	O
)	O
=	O
tp	O
(	O
x	O
,	O
t	O
)	O
dt	O
=	O
p	O
(	O
x	O
)	O
tp	O
(	O
t|x	O
)	O
dt	O
=	O
et	O
[	O
t|x	O
]	O
(	O
1.89	O
)	O
figure	O
1.28	O
the	O
regression	B
function	I
y	O
(	O
x	O
)	O
,	O
which	O
minimizes	O
the	O
expected	O
squared	O
loss	O
,	O
is	O
given	O
by	O
the	O
mean	B
of	O
the	O
conditional	B
distri-	O
bution	O
p	O
(	O
t|x	O
)	O
.	O
t	O
y	O
(	O
x0	O
)	O
1.5.	O
decision	B
theory	I
47	O
y	O
(	O
x	O
)	O
p	O
(	O
t|x0	O
)	O
x0	O
x	O
which	O
is	O
the	O
conditional	B
average	O
of	O
t	O
conditioned	O
on	O
x	O
and	O
is	O
known	O
as	O
the	O
regression	B
function	I
.	O
this	O
result	O
is	O
illustrated	O
in	O
figure	O
1.28.	O
it	O
can	O
readily	O
be	O
extended	B
to	O
mul-	O
tiple	O
target	O
variables	O
represented	O
by	O
the	O
vector	O
t	O
,	O
in	O
which	O
case	O
the	O
optimal	O
solution	O
is	O
the	O
conditional	B
average	O
y	O
(	O
x	O
)	O
=	O
et	O
[	O
t|x	O
]	O
.	O
exercise	O
1.25	O
we	O
can	O
also	O
derive	O
this	O
result	O
in	O
a	O
slightly	O
different	O
way	O
,	O
which	O
will	O
also	O
shed	O
light	O
on	O
the	O
nature	O
of	O
the	O
regression	B
problem	O
.	O
armed	O
with	O
the	O
knowledge	O
that	O
the	O
optimal	O
solution	O
is	O
the	O
conditional	B
expectation	I
,	O
we	O
can	O
expand	O
the	O
square	O
term	O
as	O
follows	O
{	O
y	O
(	O
x	O
)	O
−	O
t	O
}	O
2	O
=	O
{	O
y	O
(	O
x	O
)	O
−	O
e	O
[	O
t|x	O
]	O
+	O
e	O
[	O
t|x	O
]	O
−	O
t	O
}	O
2	O
=	O
{	O
y	O
(	O
x	O
)	O
−	O
e	O
[	O
t|x	O
]	O
}	O
2	O
+	O
2	O
{	O
y	O
(	O
x	O
)	O
−	O
e	O
[	O
t|x	O
]	O
}	O
{	O
e	O
[	O
t|x	O
]	O
−	O
t	O
}	O
+	O
{	O
e	O
[	O
t|x	O
]	O
−	O
t	O
}	O
2	O
where	O
,	O
to	O
keep	O
the	O
notation	O
uncluttered	O
,	O
we	O
use	O
e	O
[	O
t|x	O
]	O
to	O
denote	O
et	O
[	O
t|x	O
]	O
.	O
substituting	O
into	O
the	O
loss	B
function	I
and	O
performing	O
the	O
integral	O
over	O
t	O
,	O
we	O
see	O
that	O
the	O
cross-term	O
vanishes	O
and	O
we	O
obtain	O
an	O
expression	O
for	O
the	O
loss	B
function	I
in	O
the	O
form	O
{	O
e	O
[	O
t|x	O
]	O
−	O
t	O
}	O
2p	O
(	O
x	O
)	O
dx	O
.	O
{	O
y	O
(	O
x	O
)	O
−	O
e	O
[	O
t|x	O
]	O
}	O
2	O
p	O
(	O
x	O
)	O
dx	O
+	O
(	O
1.90	O
)	O
(	O
cid:6	O
)	O
e	O
[	O
l	O
]	O
=	O
(	O
cid:6	O
)	O
the	O
function	O
y	O
(	O
x	O
)	O
we	O
seek	O
to	O
determine	O
enters	O
only	O
in	O
the	O
ﬁrst	O
term	O
,	O
which	O
will	O
be	O
minimized	O
when	O
y	O
(	O
x	O
)	O
is	O
equal	O
to	O
e	O
[	O
t|x	O
]	O
,	O
in	O
which	O
case	O
this	O
term	O
will	O
vanish	O
.	O
this	O
is	O
simply	O
the	O
result	O
that	O
we	O
derived	O
previously	O
and	O
that	O
shows	O
that	O
the	O
optimal	O
least	O
squares	O
predictor	O
is	O
given	O
by	O
the	O
conditional	B
mean	O
.	O
the	O
second	O
term	O
is	O
the	O
variance	B
of	O
the	O
distribution	O
of	O
t	O
,	O
averaged	O
over	O
x.	O
it	O
represents	O
the	O
intrinsic	O
variability	O
of	O
the	O
target	O
data	O
and	O
can	O
be	O
regarded	O
as	O
noise	O
.	O
because	O
it	O
is	O
independent	B
of	O
y	O
(	O
x	O
)	O
,	O
it	O
represents	O
the	O
irreducible	O
minimum	O
value	O
of	O
the	O
loss	B
function	I
.	O
as	O
with	O
the	O
classiﬁcation	B
problem	O
,	O
we	O
can	O
either	O
determine	O
the	O
appropriate	O
prob-	O
abilities	O
and	O
then	O
use	O
these	O
to	O
make	O
optimal	O
decisions	O
,	O
or	O
we	O
can	O
build	O
models	O
that	O
make	O
decisions	O
directly	O
.	O
indeed	O
,	O
we	O
can	O
identify	O
three	O
distinct	O
approaches	O
to	O
solving	O
regression	B
problems	O
given	O
,	O
in	O
order	O
of	O
decreasing	O
complexity	O
,	O
by	O
:	O
(	O
a	O
)	O
first	O
solve	O
the	O
inference	B
problem	O
of	O
determining	O
the	O
joint	O
density	B
p	O
(	O
x	O
,	O
t	O
)	O
.	O
then	O
normalize	O
to	O
ﬁnd	O
the	O
conditional	B
density	O
p	O
(	O
t|x	O
)	O
,	O
and	O
ﬁnally	O
marginalize	O
to	O
ﬁnd	O
the	O
conditional	B
mean	O
given	O
by	O
(	O
1.89	O
)	O
.	O
48	O
1.	O
introduction	O
(	O
b	O
)	O
first	O
solve	O
the	O
inference	B
problem	O
of	O
determining	O
the	O
conditional	B
density	O
p	O
(	O
t|x	O
)	O
,	O
and	O
then	O
subsequently	O
marginalize	O
to	O
ﬁnd	O
the	O
conditional	B
mean	O
given	O
by	O
(	O
1.89	O
)	O
.	O
(	O
c	O
)	O
find	O
a	O
regression	B
function	I
y	O
(	O
x	O
)	O
directly	O
from	O
the	O
training	B
data	O
.	O
the	O
relative	B
merits	O
of	O
these	O
three	O
approaches	O
follow	O
the	O
same	O
lines	O
as	O
for	O
classiﬁca-	O
tion	O
problems	O
above	O
.	O
the	O
squared	O
loss	O
is	O
not	O
the	O
only	O
possible	O
choice	O
of	O
loss	B
function	I
for	O
regression	B
.	O
indeed	O
,	O
there	O
are	O
situations	O
in	O
which	O
squared	O
loss	O
can	O
lead	O
to	O
very	O
poor	O
results	O
and	O
where	O
we	O
need	O
to	O
develop	O
more	O
sophisticated	O
approaches	O
.	O
an	O
important	O
example	O
concerns	O
situations	O
in	O
which	O
the	O
conditional	B
distribution	O
p	O
(	O
t|x	O
)	O
is	O
multimodal	O
,	O
as	O
often	O
arises	O
in	O
the	O
solution	O
of	O
inverse	B
problems	O
.	O
here	O
we	O
consider	O
brieﬂy	O
one	O
simple	O
generalization	B
of	O
the	O
squared	O
loss	O
,	O
called	O
the	O
minkowski	O
loss	O
,	O
whose	O
expectation	B
is	O
given	O
by	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
|y	O
(	O
x	O
)	O
−	O
t|qp	O
(	O
x	O
,	O
t	O
)	O
dx	O
dt	O
e	O
[	O
lq	O
]	O
=	O
(	O
1.91	O
)	O
which	O
reduces	O
to	O
the	O
expected	O
squared	O
loss	O
for	O
q	O
=	O
2.	O
the	O
function	O
|y	O
−	O
t|q	O
is	O
plotted	O
against	O
y	O
−	O
t	O
for	O
various	O
values	O
of	O
q	O
in	O
figure	O
1.29.	O
the	O
minimum	O
of	O
e	O
[	O
lq	O
]	O
is	O
given	O
by	O
the	O
conditional	B
mean	O
for	O
q	O
=	O
2	O
,	O
the	O
conditional	B
median	O
for	O
q	O
=	O
1	O
,	O
and	O
the	O
conditional	B
mode	O
for	O
q	O
→	O
0.	O
section	O
5.6	O
exercise	O
1.27	O
1.6.	O
information	B
theory	I
in	O
this	O
chapter	O
,	O
we	O
have	O
discussed	O
a	O
variety	O
of	O
concepts	O
from	O
probability	B
theory	O
and	O
decision	B
theory	I
that	O
will	O
form	O
the	O
foundations	O
for	O
much	O
of	O
the	O
subsequent	O
discussion	O
in	O
this	O
book	O
.	O
we	O
close	O
this	O
chapter	O
by	O
introducing	O
some	O
additional	O
concepts	O
from	O
the	O
ﬁeld	O
of	O
information	B
theory	I
,	O
which	O
will	O
also	O
prove	O
useful	O
in	O
our	O
development	O
of	O
pattern	O
recognition	O
and	O
machine	O
learning	O
techniques	O
.	O
again	O
,	O
we	O
shall	O
focus	O
only	O
on	O
the	O
key	O
concepts	O
,	O
and	O
we	O
refer	O
the	O
reader	O
elsewhere	O
for	O
more	O
detailed	O
discussions	O
(	O
viterbi	O
and	O
omura	O
,	O
1979	O
;	O
cover	O
and	O
thomas	O
,	O
1991	O
;	O
mackay	O
,	O
2003	O
)	O
.	O
we	O
begin	O
by	O
considering	O
a	O
discrete	O
random	O
variable	O
x	O
and	O
we	O
ask	O
how	O
much	O
information	O
is	O
received	O
when	O
we	O
observe	O
a	O
speciﬁc	O
value	O
for	O
this	O
variable	O
.	O
the	O
amount	O
of	O
information	O
can	O
be	O
viewed	O
as	O
the	O
‘	O
degree	O
of	O
surprise	O
’	O
on	O
learning	B
the	O
value	O
of	O
x.	O
if	O
we	O
are	O
told	O
that	O
a	O
highly	O
improbable	O
event	O
has	O
just	O
occurred	O
,	O
we	O
will	O
have	O
received	O
more	O
information	O
than	O
if	O
we	O
were	O
told	O
that	O
some	O
very	O
likely	O
event	O
has	O
just	O
occurred	O
,	O
and	O
if	O
we	O
knew	O
that	O
the	O
event	O
was	O
certain	O
to	O
happen	O
we	O
would	O
receive	O
no	O
information	O
.	O
our	O
measure	O
of	O
information	O
content	O
will	O
therefore	O
depend	O
on	O
the	O
probability	B
distribution	O
p	O
(	O
x	O
)	O
,	O
and	O
we	O
therefore	O
look	O
for	O
a	O
quantity	O
h	O
(	O
x	O
)	O
that	O
is	O
a	O
monotonic	O
function	O
of	O
the	O
probability	B
p	O
(	O
x	O
)	O
and	O
that	O
expresses	O
the	O
information	O
content	O
.	O
the	O
form	O
of	O
h	O
(	O
·	O
)	O
can	O
be	O
found	O
by	O
noting	O
that	O
if	O
we	O
have	O
two	O
events	O
x	O
and	O
y	O
that	O
are	O
unrelated	O
,	O
then	O
the	O
information	O
gain	O
from	O
observing	O
both	O
of	O
them	O
should	O
be	O
the	O
sum	O
of	O
the	O
information	O
gained	O
from	O
each	O
of	O
them	O
separately	O
,	O
so	O
that	O
h	O
(	O
x	O
,	O
y	O
)	O
=	O
h	O
(	O
x	O
)	O
+	O
h	O
(	O
y	O
)	O
.	O
two	O
unrelated	O
events	O
will	O
be	O
statistically	O
independent	B
and	O
so	O
p	O
(	O
x	O
,	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
.	O
from	O
these	O
two	O
relationships	O
,	O
it	O
is	O
easily	O
shown	O
that	O
h	O
(	O
x	O
)	O
must	O
be	O
given	O
by	O
the	O
logarithm	O
of	O
p	O
(	O
x	O
)	O
and	O
so	O
we	O
have	O
exercise	O
1.28	O
q	O
|	O
t	O
−	O
y	O
|	O
q	O
|	O
t	O
−	O
y	O
|	O
2	O
1	O
0	O
−2	O
2	O
1	O
0	O
−2	O
q	O
=	O
0.3	O
−1	O
0	O
y	O
−	O
t	O
1	O
2	O
q	O
=	O
2	O
−1	O
0	O
y	O
−	O
t	O
1	O
2	O
q	O
|	O
t	O
−	O
y	O
|	O
q	O
|	O
t	O
−	O
y	O
|	O
2	O
1	O
0	O
−2	O
2	O
1	O
0	O
−2	O
1.6.	O
information	B
theory	I
49	O
q	O
=	O
1	O
−1	O
0	O
y	O
−	O
t	O
1	O
2	O
q	O
=	O
10	O
−1	O
0	O
y	O
−	O
t	O
1	O
2	O
figure	O
1.29	O
plots	O
of	O
the	O
quantity	O
lq	O
=	O
|y	O
−	O
t|q	O
for	O
various	O
values	O
of	O
q.	O
h	O
(	O
x	O
)	O
=	O
−	O
log2	O
p	O
(	O
x	O
)	O
(	O
1.92	O
)	O
where	O
the	O
negative	O
sign	O
ensures	O
that	O
information	O
is	O
positive	O
or	O
zero	O
.	O
note	O
that	O
low	O
probability	B
events	O
x	O
correspond	O
to	O
high	O
information	O
content	O
.	O
the	O
choice	O
of	O
basis	O
for	O
the	O
logarithm	O
is	O
arbitrary	O
,	O
and	O
for	O
the	O
moment	O
we	O
shall	O
adopt	O
the	O
convention	O
prevalent	O
in	O
information	B
theory	I
of	O
using	O
logarithms	O
to	O
the	O
base	O
of	O
2.	O
in	O
this	O
case	O
,	O
as	O
we	O
shall	O
see	O
shortly	O
,	O
the	O
units	O
of	O
h	O
(	O
x	O
)	O
are	O
bits	B
(	O
‘	O
binary	O
digits	O
’	O
)	O
.	O
now	O
suppose	O
that	O
a	O
sender	O
wishes	O
to	O
transmit	O
the	O
value	O
of	O
a	O
random	O
variable	O
to	O
a	O
receiver	O
.	O
the	O
average	O
amount	O
of	O
information	O
that	O
they	O
transmit	O
in	O
the	O
process	O
is	O
obtained	O
by	O
taking	O
the	O
expectation	B
of	O
(	O
1.92	O
)	O
with	O
respect	O
to	O
the	O
distribution	O
p	O
(	O
x	O
)	O
and	O
is	O
given	O
by	O
(	O
cid:2	O
)	O
h	O
[	O
x	O
]	O
=	O
−	O
p	O
(	O
x	O
)	O
log2	O
p	O
(	O
x	O
)	O
.	O
(	O
1.93	O
)	O
x	O
this	O
important	O
quantity	O
is	O
called	O
the	O
entropy	B
of	O
the	O
random	O
variable	O
x.	O
note	O
that	O
limp→0	O
p	O
ln	O
p	O
=	O
0	O
and	O
so	O
we	O
shall	O
take	O
p	O
(	O
x	O
)	O
ln	O
p	O
(	O
x	O
)	O
=	O
0	O
whenever	O
we	O
encounter	O
a	O
value	O
for	O
x	O
such	O
that	O
p	O
(	O
x	O
)	O
=	O
0.	O
so	O
far	O
we	O
have	O
given	O
a	O
rather	O
heuristic	O
motivation	O
for	O
the	O
deﬁnition	O
of	O
informa-	O
50	O
1.	O
introduction	O
tion	O
(	O
1.92	O
)	O
and	O
the	O
corresponding	O
entropy	B
(	O
1.93	O
)	O
.	O
we	O
now	O
show	O
that	O
these	O
deﬁnitions	O
indeed	O
possess	O
useful	O
properties	O
.	O
consider	O
a	O
random	O
variable	O
x	O
having	O
8	O
possible	O
states	O
,	O
each	O
of	O
which	O
is	O
equally	O
likely	O
.	O
in	O
order	O
to	O
communicate	O
the	O
value	O
of	O
x	O
to	O
a	O
receiver	O
,	O
we	O
would	O
need	O
to	O
transmit	O
a	O
message	O
of	O
length	O
3	O
bits	B
.	O
notice	O
that	O
the	O
entropy	B
of	O
this	O
variable	O
is	O
given	O
by	O
h	O
[	O
x	O
]	O
=	O
−8	O
×	O
1	O
8	O
log2	O
1	O
8	O
=	O
3	O
bits	B
.	O
4	O
,	O
1	O
64	O
,	O
1	O
now	O
consider	O
an	O
example	O
(	O
cover	O
and	O
thomas	O
,	O
1991	O
)	O
of	O
a	O
variable	O
having	O
8	O
pos-	O
sible	O
states	O
{	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
,	O
g	O
,	O
h	O
}	O
for	O
which	O
the	O
respective	O
probabilities	O
are	O
given	O
by	O
16	O
,	O
1	O
(	O
1	O
2	O
,	O
1	O
8	O
,	O
1	O
h	O
[	O
x	O
]	O
=	O
−1	O
2	O
64	O
)	O
.	O
the	O
entropy	B
in	O
this	O
case	O
is	O
given	O
by	O
−	O
4	O
64	O
64	O
,	O
1	O
64	O
,	O
1	O
−	O
1	O
1	O
2	O
4	O
1	O
4	O
−	O
1	O
8	O
1	O
8	O
−	O
1	O
16	O
log2	O
=	O
2	O
bits	B
.	O
log2	O
1	O
16	O
1	O
64	O
log2	O
log2	O
log2	O
we	O
see	O
that	O
the	O
nonuniform	O
distribution	O
has	O
a	O
smaller	O
entropy	B
than	O
the	O
uniform	O
one	O
,	O
and	O
we	O
shall	O
gain	O
some	O
insight	O
into	O
this	O
shortly	O
when	O
we	O
discuss	O
the	O
interpretation	O
of	O
entropy	B
in	O
terms	O
of	O
disorder	O
.	O
for	O
the	O
moment	O
,	O
let	O
us	O
consider	O
how	O
we	O
would	O
transmit	O
the	O
identity	O
of	O
the	O
variable	O
’	O
s	O
state	O
to	O
a	O
receiver	O
.	O
we	O
could	O
do	O
this	O
,	O
as	O
before	O
,	O
using	O
a	O
3-bit	O
number	O
.	O
however	O
,	O
we	O
can	O
take	O
advantage	O
of	O
the	O
nonuniform	O
distribution	O
by	O
using	O
shorter	O
codes	O
for	O
the	O
more	O
probable	O
events	O
,	O
at	O
the	O
expense	O
of	O
longer	O
codes	O
for	O
the	O
less	O
probable	O
events	O
,	O
in	O
the	O
hope	O
of	O
getting	O
a	O
shorter	O
average	O
code	O
length	O
.	O
this	O
can	O
be	O
done	O
by	O
representing	O
the	O
states	O
{	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
,	O
g	O
,	O
h	O
}	O
using	O
,	O
for	O
instance	O
,	O
the	O
following	O
set	O
of	O
code	O
strings	O
:	O
0	O
,	O
10	O
,	O
110	O
,	O
1110	O
,	O
111100	O
,	O
111101	O
,	O
111110	O
,	O
111111.	O
the	O
average	O
length	O
of	O
the	O
code	O
that	O
has	O
to	O
be	O
transmitted	O
is	O
then	O
average	O
code	O
length	O
=	O
1	O
2	O
×	O
1	O
+	O
1	O
4	O
×	O
2	O
+	O
1	O
8	O
×	O
3	O
+	O
1	O
16	O
×	O
4	O
+	O
4	O
×	O
1	O
64	O
×	O
6	O
=	O
2	O
bits	B
which	O
again	O
is	O
the	O
same	O
as	O
the	O
entropy	B
of	O
the	O
random	O
variable	O
.	O
note	O
that	O
shorter	O
code	O
strings	O
can	O
not	O
be	O
used	O
because	O
it	O
must	O
be	O
possible	O
to	O
disambiguate	O
a	O
concatenation	O
of	O
such	O
strings	O
into	O
its	O
component	O
parts	O
.	O
for	O
instance	O
,	O
11001110	O
decodes	O
uniquely	O
into	O
the	O
state	O
sequence	O
c	O
,	O
a	O
,	O
d.	O
this	O
relation	O
between	O
entropy	B
and	O
shortest	O
coding	O
length	O
is	O
a	O
general	O
one	O
.	O
the	O
noiseless	B
coding	I
theorem	I
(	O
shannon	O
,	O
1948	O
)	O
states	O
that	O
the	O
entropy	B
is	O
a	O
lower	B
bound	I
on	O
the	O
number	O
of	O
bits	B
needed	O
to	O
transmit	O
the	O
state	O
of	O
a	O
random	O
variable	O
.	O
from	O
now	O
on	O
,	O
we	O
shall	O
switch	O
to	O
the	O
use	O
of	O
natural	O
logarithms	O
in	O
deﬁning	O
en-	O
tropy	O
,	O
as	O
this	O
will	O
provide	O
a	O
more	O
convenient	O
link	B
with	O
ideas	O
elsewhere	O
in	O
this	O
book	O
.	O
in	O
this	O
case	O
,	O
the	O
entropy	B
is	O
measured	O
in	O
units	O
of	O
‘	O
nats	B
’	O
instead	O
of	O
bits	B
,	O
which	O
differ	O
simply	O
by	O
a	O
factor	O
of	O
ln	O
2.	O
we	O
have	O
introduced	O
the	O
concept	O
of	O
entropy	B
in	O
terms	O
of	O
the	O
average	O
amount	O
of	O
information	O
needed	O
to	O
specify	O
the	O
state	O
of	O
a	O
random	O
variable	O
.	O
in	O
fact	O
,	O
the	O
concept	O
of	O
entropy	B
has	O
much	O
earlier	O
origins	O
in	O
physics	O
where	O
it	O
was	O
introduced	O
in	O
the	O
context	O
of	O
equilibrium	O
thermodynamics	O
and	O
later	O
given	O
a	O
deeper	O
interpretation	O
as	O
a	O
measure	O
of	O
disorder	O
through	O
developments	O
in	O
statistical	O
mechanics	O
.	O
we	O
can	O
understand	O
this	O
alternative	O
view	O
of	O
entropy	B
by	O
considering	O
a	O
set	O
of	O
n	O
identical	O
objects	O
that	O
are	O
to	O
be	O
divided	O
amongst	O
a	O
set	O
of	O
bins	O
,	O
such	O
that	O
there	O
are	O
ni	O
objects	O
in	O
the	O
ith	O
bin	O
.	O
consider	O
1.6.	O
information	B
theory	I
51	O
the	O
number	O
of	O
different	O
ways	O
of	O
allocating	O
the	O
objects	O
to	O
the	O
bins	O
.	O
there	O
are	O
n	O
ways	O
to	O
choose	O
the	O
ﬁrst	O
object	O
,	O
(	O
n	O
−	O
1	O
)	O
ways	O
to	O
choose	O
the	O
second	O
object	O
,	O
and	O
so	O
on	O
,	O
leading	O
to	O
a	O
total	O
of	O
n	O
!	O
ways	O
to	O
allocate	O
all	O
n	O
objects	O
to	O
the	O
bins	O
,	O
where	O
n	O
!	O
(	O
pronounced	O
‘	O
factorial	B
n	O
’	O
)	O
denotes	O
the	O
product	O
n	O
×	O
(	O
n	O
−1	O
)	O
×···×2×1	O
.	O
however	O
,	O
we	O
don	O
’	O
t	O
wish	O
to	O
distinguish	O
between	O
rearrangements	O
of	O
objects	O
within	O
each	O
bin	O
.	O
in	O
the	O
ith	O
bin	O
there	O
are	O
ni	O
!	O
ways	O
of	O
reordering	O
the	O
objects	O
,	O
and	O
so	O
the	O
total	O
number	O
of	O
ways	O
of	O
allocating	O
the	O
n	O
objects	O
to	O
the	O
bins	O
is	O
given	O
by	O
w	O
=	O
n	O
!	O
(	O
cid:21	O
)	O
i	O
ni	O
!	O
(	O
1.94	O
)	O
which	O
is	O
called	O
the	O
multiplicity	B
.	O
the	O
entropy	B
is	O
then	O
deﬁned	O
as	O
the	O
logarithm	O
of	O
the	O
multiplicity	B
scaled	O
by	O
an	O
appropriate	O
constant	O
h	O
=	O
1	O
n	O
ln	O
w	O
=	O
1	O
n	O
ln	O
n	O
!	O
−	O
1	O
n	O
ln	O
ni	O
!	O
.	O
(	O
1.95	O
)	O
we	O
now	O
consider	O
the	O
limit	O
n	O
→	O
∞	O
,	O
in	O
which	O
the	O
fractions	O
ni/n	O
are	O
held	O
ﬁxed	O
,	O
and	O
apply	O
stirling	O
’	O
s	O
approximation	O
(	O
cid:2	O
)	O
i	O
which	O
gives	O
h	O
=	O
−	O
lim	O
n→∞	O
(	O
cid:5	O
)	O
i	O
ln	O
n	O
!	O
(	O
cid:8	O
)	O
n	O
ln	O
n	O
−	O
n	O
(	O
cid:18	O
)	O
(	O
cid:17	O
)	O
(	O
cid:18	O
)	O
(	O
cid:2	O
)	O
(	O
cid:17	O
)	O
ni	O
n	O
ln	O
ni	O
n	O
=	O
−	O
(	O
cid:2	O
)	O
i	O
pi	O
ln	O
pi	O
(	O
1.96	O
)	O
(	O
1.97	O
)	O
i	O
ni	O
=	O
n.	O
here	O
pi	O
=	O
limn→∞	O
(	O
ni/n	O
)	O
is	O
the	O
probability	B
where	O
we	O
have	O
used	O
of	O
an	O
object	O
being	O
assigned	O
to	O
the	O
ith	O
bin	O
.	O
in	O
physics	O
terminology	O
,	O
the	O
speciﬁc	O
ar-	O
rangements	O
of	O
objects	O
in	O
the	O
bins	O
is	O
called	O
a	O
microstate	B
,	O
and	O
the	O
overall	O
distribution	O
of	O
occupation	O
numbers	O
,	O
expressed	O
through	O
the	O
ratios	O
ni/n	O
,	O
is	O
called	O
a	O
macrostate	B
.	O
the	O
multiplicity	B
w	O
is	O
also	O
known	O
as	O
the	O
weight	O
of	O
the	O
macrostate	B
.	O
we	O
can	O
interpret	O
the	O
bins	O
as	O
the	O
states	O
xi	O
of	O
a	O
discrete	O
random	O
variable	O
x	O
,	O
where	O
p	O
(	O
x	O
=	O
xi	O
)	O
=	O
pi	O
.	O
the	O
entropy	B
of	O
the	O
random	O
variable	O
x	O
is	O
then	O
h	O
[	O
p	O
]	O
=	O
−	O
p	O
(	O
xi	O
)	O
ln	O
p	O
(	O
xi	O
)	O
.	O
(	O
1.98	O
)	O
(	O
cid:2	O
)	O
i	O
appendix	O
e	O
distributions	O
p	O
(	O
xi	O
)	O
that	O
are	O
sharply	O
peaked	O
around	O
a	O
few	O
values	O
will	O
have	O
a	O
relatively	O
low	O
entropy	B
,	O
whereas	O
those	O
that	O
are	O
spread	O
more	O
evenly	O
across	O
many	O
values	O
will	O
have	O
higher	O
entropy	B
,	O
as	O
illustrated	O
in	O
figure	O
1.30.	O
because	O
0	O
(	O
cid:1	O
)	O
pi	O
(	O
cid:1	O
)	O
1	O
,	O
the	O
entropy	B
is	O
nonnegative	O
,	O
and	O
it	O
will	O
equal	O
its	O
minimum	O
value	O
of	O
0	O
when	O
one	O
of	O
the	O
pi	O
=	O
1	O
and	O
all	O
other	O
pj	O
(	O
cid:4	O
)	O
=i	O
=	O
0.	O
the	O
maximum	O
entropy	O
conﬁguration	O
can	O
be	O
found	O
by	O
maximizing	O
h	O
using	O
a	O
lagrange	O
multiplier	O
to	O
enforce	O
the	O
normalization	O
constraint	O
on	O
the	O
probabilities	O
.	O
thus	O
we	O
maximize	O
(	O
cid:22	O
)	O
(	O
cid:2	O
)	O
(	O
cid:23	O
)	O
p	O
(	O
xi	O
)	O
ln	O
p	O
(	O
xi	O
)	O
+	O
λ	O
i	O
i	O
p	O
(	O
xi	O
)	O
−	O
1	O
(	O
1.99	O
)	O
(	O
cid:2	O
)	O
(	O
cid:4	O
)	O
h	O
=	O
−	O
52	O
1.	O
introduction	O
s	O
e	O
i	O
t	O
i	O
l	O
i	O
b	O
a	O
b	O
o	O
r	O
p	O
h	O
=	O
1.77	O
0.5	O
0.25	O
0	O
s	O
e	O
i	O
t	O
i	O
l	O
i	O
b	O
a	O
b	O
o	O
r	O
p	O
h	O
=	O
3.09	O
0.5	O
0.25	O
0	O
figure	O
1.30	O
histograms	O
of	O
two	O
probability	B
distributions	O
over	O
30	O
bins	O
illustrating	O
the	O
higher	O
value	O
of	O
the	O
entropy	B
h	O
for	O
the	O
broader	O
distribution	O
.	O
the	O
largest	O
entropy	B
would	O
arise	O
from	O
a	O
uniform	B
distribution	I
that	O
would	O
give	O
h	O
=	O
−	O
ln	O
(	O
1/30	O
)	O
=	O
3.40.	O
exercise	O
1.29	O
from	O
which	O
we	O
ﬁnd	O
that	O
all	O
of	O
the	O
p	O
(	O
xi	O
)	O
are	O
equal	O
and	O
are	O
given	O
by	O
p	O
(	O
xi	O
)	O
=	O
1/m	O
where	O
m	O
is	O
the	O
total	O
number	O
of	O
states	O
xi	O
.	O
the	O
corresponding	O
value	O
of	O
the	O
entropy	B
is	O
then	O
h	O
=	O
ln	O
m.	O
this	O
result	O
can	O
also	O
be	O
derived	O
from	O
jensen	O
’	O
s	O
inequality	O
(	O
to	O
be	O
discussed	O
shortly	O
)	O
.	O
to	O
verify	O
that	O
the	O
stationary	B
point	O
is	O
indeed	O
a	O
maximum	O
,	O
we	O
can	O
evaluate	O
the	O
second	O
derivative	O
of	O
the	O
entropy	B
,	O
which	O
gives	O
∂	O
(	O
cid:4	O
)	O
h	O
∂p	O
(	O
xi	O
)	O
∂p	O
(	O
xj	O
)	O
=	O
−iij	O
1	O
pi	O
(	O
1.100	O
)	O
where	O
iij	O
are	O
the	O
elements	O
of	O
the	O
identity	O
matrix	O
.	O
we	O
can	O
extend	O
the	O
deﬁnition	O
of	O
entropy	B
to	O
include	O
distributions	O
p	O
(	O
x	O
)	O
over	O
con-	O
tinuous	O
variables	O
x	O
as	O
follows	O
.	O
first	O
divide	O
x	O
into	O
bins	O
of	O
width	O
∆	O
.	O
then	O
,	O
assuming	O
p	O
(	O
x	O
)	O
is	O
continuous	O
,	O
the	O
mean	B
value	I
theorem	I
(	O
weisstein	O
,	O
1999	O
)	O
tells	O
us	O
that	O
,	O
for	O
each	O
such	O
bin	O
,	O
there	O
must	O
exist	O
a	O
value	O
xi	O
such	O
that	O
(	O
cid:6	O
)	O
(	O
i+1	O
)	O
∆	O
p	O
(	O
x	O
)	O
dx	O
=	O
p	O
(	O
xi	O
)	O
∆	O
.	O
(	O
1.101	O
)	O
i∆	O
we	O
can	O
now	O
quantize	O
the	O
continuous	O
variable	O
x	O
by	O
assigning	O
any	O
value	O
x	O
to	O
the	O
value	O
xi	O
whenever	O
x	O
falls	O
in	O
the	O
ith	O
bin	O
.	O
the	O
probability	B
of	O
observing	O
the	O
value	O
xi	O
is	O
then	O
p	O
(	O
xi	O
)	O
∆	O
.	O
this	O
gives	O
a	O
discrete	O
distribution	O
for	O
which	O
the	O
entropy	B
takes	O
the	O
form	O
h∆	O
=	O
−	O
p	O
(	O
xi	O
)	O
∆	O
ln	O
(	O
p	O
(	O
xi	O
)	O
∆	O
)	O
=	O
−	O
p	O
(	O
xi	O
)	O
∆	O
ln	O
p	O
(	O
xi	O
)	O
−	O
ln	O
∆	O
(	O
1.102	O
)	O
(	O
cid:2	O
)	O
i	O
(	O
cid:2	O
)	O
i	O
(	O
cid:5	O
)	O
i	O
p	O
(	O
xi	O
)	O
∆	O
=	O
1	O
,	O
which	O
follows	O
from	O
(	O
1.101	O
)	O
.	O
we	O
now	O
omit	O
where	O
we	O
have	O
used	O
the	O
second	O
term	O
−	O
ln	O
∆	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
1.102	O
)	O
and	O
then	O
consider	O
the	O
limit	O
1.6.	O
information	B
theory	I
53	O
∆	O
→	O
0.	O
the	O
ﬁrst	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
1.102	O
)	O
will	O
approach	O
the	O
integral	O
of	O
p	O
(	O
x	O
)	O
ln	O
p	O
(	O
x	O
)	O
in	O
this	O
limit	O
so	O
that	O
(	O
cid:25	O
)	O
(	O
cid:6	O
)	O
(	O
cid:24	O
)	O
(	O
cid:2	O
)	O
i	O
lim	O
∆→0	O
p	O
(	O
xi	O
)	O
∆	O
ln	O
p	O
(	O
xi	O
)	O
=	O
−	O
p	O
(	O
x	O
)	O
ln	O
p	O
(	O
x	O
)	O
dx	O
(	O
1.103	O
)	O
where	O
the	O
quantity	O
on	O
the	O
right-hand	O
side	O
is	O
called	O
the	O
differential	B
entropy	I
.	O
we	O
see	O
that	O
the	O
discrete	O
and	O
continuous	O
forms	O
of	O
the	O
entropy	B
differ	O
by	O
a	O
quantity	O
ln	O
∆	O
,	O
which	O
diverges	O
in	O
the	O
limit	O
∆	O
→	O
0.	O
this	O
reﬂects	O
the	O
fact	O
that	O
to	O
specify	O
a	O
continuous	O
variable	O
very	O
precisely	O
requires	O
a	O
large	O
number	O
of	O
bits	B
.	O
for	O
a	O
density	B
deﬁned	O
over	O
multiple	O
continuous	O
variables	O
,	O
denoted	O
collectively	O
by	O
the	O
vector	O
x	O
,	O
the	O
differential	B
entropy	I
is	O
given	O
by	O
(	O
cid:6	O
)	O
h	O
[	O
x	O
]	O
=	O
−	O
p	O
(	O
x	O
)	O
ln	O
p	O
(	O
x	O
)	O
dx	O
.	O
(	O
1.104	O
)	O
in	O
the	O
case	O
of	O
discrete	O
distributions	O
,	O
we	O
saw	O
that	O
the	O
maximum	O
entropy	O
con-	O
ﬁguration	O
corresponded	O
to	O
an	O
equal	O
distribution	O
of	O
probabilities	O
across	O
the	O
possible	O
states	O
of	O
the	O
variable	O
.	O
let	O
us	O
now	O
consider	O
the	O
maximum	O
entropy	O
conﬁguration	O
for	O
a	O
continuous	O
variable	O
.	O
in	O
order	O
for	O
this	O
maximum	O
to	O
be	O
well	O
deﬁned	O
,	O
it	O
will	O
be	O
nec-	O
essary	O
to	O
constrain	O
the	O
ﬁrst	O
and	O
second	O
moments	O
of	O
p	O
(	O
x	O
)	O
as	O
well	O
as	O
preserving	O
the	O
normalization	O
constraint	O
.	O
we	O
therefore	O
maximize	O
the	O
differential	B
entropy	I
with	O
the	O
ludwig	O
boltzmann	O
1844–1906	O
ludwig	O
eduard	O
boltzmann	O
was	O
an	O
austrian	O
physicist	O
who	O
created	O
the	O
ﬁeld	O
of	O
statistical	O
mechanics	O
.	O
prior	B
to	O
boltzmann	O
,	O
the	O
concept	O
of	O
en-	O
tropy	O
was	O
already	O
known	O
from	O
classical	B
thermodynamics	O
where	O
it	O
quantiﬁes	O
the	O
fact	O
that	O
when	O
we	O
take	O
energy	O
from	O
a	O
system	O
,	O
not	O
all	O
of	O
that	O
energy	O
is	O
typically	O
available	O
to	O
do	O
useful	O
work	O
.	O
boltzmann	O
showed	O
that	O
the	O
ther-	O
modynamic	O
entropy	B
s	O
,	O
a	O
macroscopic	O
quantity	O
,	O
could	O
be	O
related	O
to	O
the	O
statistical	O
properties	O
at	O
the	O
micro-	O
scopic	O
level	O
.	O
this	O
is	O
expressed	O
through	O
the	O
famous	O
equation	O
s	O
=	O
k	O
ln	O
w	O
in	O
which	O
w	O
represents	O
the	O
number	O
of	O
possible	O
microstates	O
in	O
a	O
macrostate	B
,	O
and	O
k	O
(	O
cid:3	O
)	O
1.38	O
×	O
10−23	O
(	O
in	O
units	O
of	O
joules	O
per	O
kelvin	O
)	O
is	O
known	O
as	O
boltzmann	O
’	O
s	O
constant	O
.	O
boltzmann	O
’	O
s	O
ideas	O
were	O
disputed	O
by	O
many	O
scientists	O
of	O
they	O
day	O
.	O
one	O
dif-	O
ﬁculty	O
they	O
saw	O
arose	O
from	O
the	O
second	O
law	O
of	O
thermo-	O
dynamics	O
,	O
which	O
states	O
that	O
the	O
entropy	B
of	O
a	O
closed	O
system	O
tends	O
to	O
increase	O
with	O
time	O
.	O
by	O
contrast	O
,	O
at	O
the	O
microscopic	O
level	O
the	O
classical	B
newtonian	O
equa-	O
tions	O
of	O
physics	O
are	O
reversible	O
,	O
and	O
so	O
they	O
found	O
it	O
difﬁcult	O
to	O
see	O
how	O
the	O
latter	O
could	O
explain	O
the	O
for-	O
mer	O
.	O
they	O
didn	O
’	O
t	O
fully	O
appreciate	O
boltzmann	O
’	O
s	O
argu-	O
ments	O
,	O
which	O
were	O
statistical	O
in	O
nature	O
and	O
which	O
con-	O
cluded	O
not	O
that	O
entropy	B
could	O
never	O
decrease	O
over	O
time	O
but	O
simply	O
that	O
with	O
overwhelming	O
probability	B
it	O
would	O
generally	O
increase	O
.	O
boltzmann	O
even	O
had	O
a	O
long-	O
running	O
dispute	O
with	O
the	O
editor	O
of	O
the	O
leading	O
german	O
physics	O
journal	O
who	O
refused	O
to	O
let	O
him	O
refer	O
to	O
atoms	O
and	O
molecules	O
as	O
anything	O
other	O
than	O
convenient	O
the-	O
oretical	O
constructs	O
.	O
the	O
continued	O
attacks	O
on	O
his	O
work	O
lead	O
to	O
bouts	O
of	O
depression	O
,	O
and	O
eventually	O
he	O
com-	O
mitted	O
suicide	O
.	O
shortly	O
after	O
boltzmann	O
’	O
s	O
death	O
,	O
new	O
experiments	O
by	O
perrin	O
on	O
colloidal	O
suspensions	O
veri-	O
ﬁed	O
his	O
theories	O
and	O
conﬁrmed	O
the	O
value	O
of	O
the	O
boltz-	O
mann	O
constant	O
.	O
the	O
equation	O
s	O
=	O
k	O
ln	O
w	O
is	O
carved	O
on	O
boltzmann	O
’	O
s	O
tombstone	O
.	O
54	O
1.	O
introduction	O
three	O
constraints	O
(	O
cid:6	O
)	O
∞	O
(	O
cid:6	O
)	O
∞	O
−∞	O
p	O
(	O
x	O
)	O
dx	O
=	O
1	O
xp	O
(	O
x	O
)	O
dx	O
=	O
µ	O
−∞	O
(	O
x	O
−	O
µ	O
)	O
2p	O
(	O
x	O
)	O
dx	O
=	O
σ2	O
.	O
(	O
cid:6	O
)	O
∞	O
−∞	O
(	O
1.105	O
)	O
(	O
1.106	O
)	O
(	O
1.107	O
)	O
appendix	O
e	O
appendix	O
d	O
exercise	O
1.34	O
exercise	O
1.35	O
the	O
constrained	O
maximization	O
can	O
be	O
performed	O
using	O
lagrange	O
multipliers	O
so	O
that	O
we	O
maximize	O
the	O
following	O
functional	B
with	O
respect	O
to	O
p	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
dx	O
−	O
1	O
p	O
(	O
x	O
)	O
ln	O
p	O
(	O
x	O
)	O
dx	O
+	O
λ1	O
−	O
(	O
cid:16	O
)	O
(	O
cid:15	O
)	O
(	O
cid:6	O
)	O
∞	O
(	O
cid:15	O
)	O
(	O
cid:6	O
)	O
∞	O
−∞	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
xp	O
(	O
x	O
)	O
dx	O
−	O
µ	O
+	O
λ3	O
(	O
x	O
−	O
µ	O
)	O
2p	O
(	O
x	O
)	O
dx	O
−	O
σ2	O
.	O
(	O
cid:6	O
)	O
∞	O
(	O
cid:15	O
)	O
(	O
cid:6	O
)	O
∞	O
−∞	O
+λ2	O
−∞	O
using	O
the	O
calculus	B
of	I
variations	I
,	O
we	O
set	O
the	O
derivative	B
of	O
this	O
functional	B
to	O
zero	O
giving	O
p	O
(	O
x	O
)	O
=	O
exp	O
.	O
(	O
1.108	O
)	O
(	O
cid:27	O
)	O
−∞	O
(	O
cid:26	O
)	O
−1	O
+	O
λ1	O
+	O
λ2x	O
+	O
λ3	O
(	O
x	O
−	O
µ	O
)	O
2	O
(	O
cid:13	O
)	O
(	O
cid:12	O
)	O
−	O
(	O
x	O
−	O
µ	O
)	O
2	O
2σ2	O
(	O
2πσ2	O
)	O
1/2	O
exp	O
1	O
the	O
lagrange	O
multipliers	O
can	O
be	O
found	O
by	O
back	O
substitution	O
of	O
this	O
result	O
into	O
the	O
three	O
constraint	O
equations	O
,	O
leading	O
ﬁnally	O
to	O
the	O
result	O
p	O
(	O
x	O
)	O
=	O
(	O
1.109	O
)	O
and	O
so	O
the	O
distribution	O
that	O
maximizes	O
the	O
differential	B
entropy	I
is	O
the	O
gaussian	O
.	O
note	O
that	O
we	O
did	O
not	O
constrain	O
the	O
distribution	O
to	O
be	O
nonnegative	O
when	O
we	O
maximized	O
the	O
entropy	B
.	O
however	O
,	O
because	O
the	O
resulting	O
distribution	O
is	O
indeed	O
nonnegative	O
,	O
we	O
see	O
with	O
hindsight	O
that	O
such	O
a	O
constraint	O
is	O
not	O
necessary	O
.	O
if	O
we	O
evaluate	O
the	O
differential	B
entropy	I
of	O
the	O
gaussian	O
,	O
we	O
obtain	O
(	O
cid:26	O
)	O
h	O
[	O
x	O
]	O
=	O
1	O
2	O
(	O
cid:27	O
)	O
1	O
+	O
ln	O
(	O
2πσ2	O
)	O
.	O
(	O
1.110	O
)	O
thus	O
we	O
see	O
again	O
that	O
the	O
entropy	B
increases	O
as	O
the	O
distribution	O
becomes	O
broader	O
,	O
i.e.	O
,	O
as	O
σ2	O
increases	O
.	O
this	O
result	O
also	O
shows	O
that	O
the	O
differential	B
entropy	I
,	O
unlike	O
the	O
discrete	O
entropy	B
,	O
can	O
be	O
negative	O
,	O
because	O
h	O
(	O
x	O
)	O
<	O
0	O
in	O
(	O
1.110	O
)	O
for	O
σ2	O
<	O
1/	O
(	O
2πe	O
)	O
.	O
suppose	O
we	O
have	O
a	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
y	O
)	O
from	O
which	O
we	O
draw	O
pairs	O
of	O
values	O
of	O
x	O
and	O
y.	O
if	O
a	O
value	O
of	O
x	O
is	O
already	O
known	O
,	O
then	O
the	O
additional	O
information	O
needed	O
to	O
specify	O
the	O
corresponding	O
value	O
of	O
y	O
is	O
given	O
by	O
−	O
ln	O
p	O
(	O
y|x	O
)	O
.	O
thus	O
the	O
average	O
additional	O
information	O
needed	O
to	O
specify	O
y	O
can	O
be	O
written	O
as	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
h	O
[	O
y|x	O
]	O
=	O
−	O
p	O
(	O
y	O
,	O
x	O
)	O
ln	O
p	O
(	O
y|x	O
)	O
dy	O
dx	O
(	O
1.111	O
)	O
1.6.	O
information	B
theory	I
55	O
exercise	O
1.37	O
which	O
is	O
called	O
the	O
conditional	B
entropy	I
of	O
y	O
given	O
x.	O
it	O
is	O
easily	O
seen	O
,	O
using	O
the	O
product	B
rule	I
,	O
that	O
the	O
conditional	B
entropy	I
satisﬁes	O
the	O
relation	O
h	O
[	O
x	O
,	O
y	O
]	O
=	O
h	O
[	O
y|x	O
]	O
+	O
h	O
[	O
x	O
]	O
(	O
1.112	O
)	O
where	O
h	O
[	O
x	O
,	O
y	O
]	O
is	O
the	O
differential	B
entropy	I
of	O
p	O
(	O
x	O
,	O
y	O
)	O
and	O
h	O
[	O
x	O
]	O
is	O
the	O
differential	B
en-	O
tropy	O
of	O
the	O
marginal	B
distribution	O
p	O
(	O
x	O
)	O
.	O
thus	O
the	O
information	O
needed	O
to	O
describe	O
x	O
and	O
y	O
is	O
given	O
by	O
the	O
sum	O
of	O
the	O
information	O
needed	O
to	O
describe	O
x	O
alone	O
plus	O
the	O
additional	O
information	O
required	O
to	O
specify	O
y	O
given	O
x	O
.	O
1.6.1	O
relative	B
entropy	I
and	O
mutual	B
information	I
so	O
far	O
in	O
this	O
section	O
,	O
we	O
have	O
introduced	O
a	O
number	O
of	O
concepts	O
from	O
information	B
theory	I
,	O
including	O
the	O
key	O
notion	O
of	O
entropy	B
.	O
we	O
now	O
start	O
to	O
relate	O
these	O
ideas	O
to	O
pattern	O
recognition	O
.	O
consider	O
some	O
unknown	O
distribution	O
p	O
(	O
x	O
)	O
,	O
and	O
suppose	O
that	O
we	O
have	O
modelled	O
this	O
using	O
an	O
approximating	O
distribution	O
q	O
(	O
x	O
)	O
.	O
if	O
we	O
use	O
q	O
(	O
x	O
)	O
to	O
construct	O
a	O
coding	O
scheme	O
for	O
the	O
purpose	O
of	O
transmitting	O
values	O
of	O
x	O
to	O
a	O
receiver	O
,	O
then	O
the	O
average	O
additional	O
amount	O
of	O
information	O
(	O
in	O
nats	B
)	O
required	O
to	O
specify	O
the	O
value	O
of	O
x	O
(	O
assuming	O
we	O
choose	O
an	O
efﬁcient	O
coding	O
scheme	O
)	O
as	O
a	O
result	O
of	O
using	O
q	O
(	O
x	O
)	O
instead	O
of	O
the	O
true	O
distribution	O
p	O
(	O
x	O
)	O
is	O
given	O
by	O
p	O
(	O
x	O
)	O
ln	O
q	O
(	O
x	O
)	O
dx	O
−	O
kl	O
(	O
p	O
(	O
cid:6	O
)	O
q	O
)	O
=	O
−	O
p	O
(	O
x	O
)	O
ln	O
p	O
(	O
x	O
)	O
dx	O
(	O
cid:16	O
)	O
(	O
cid:15	O
)	O
(	O
cid:6	O
)	O
−	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
=	O
−	O
p	O
(	O
x	O
)	O
ln	O
q	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
dx	O
.	O
(	O
1.113	O
)	O
this	O
is	O
known	O
as	O
the	O
relative	B
entropy	I
or	O
kullback-leibler	O
divergence	O
,	O
or	O
kl	O
diver-	O
gence	O
(	O
kullback	O
and	O
leibler	O
,	O
1951	O
)	O
,	O
between	O
the	O
distributions	O
p	O
(	O
x	O
)	O
and	O
q	O
(	O
x	O
)	O
.	O
note	O
that	O
it	O
is	O
not	O
a	O
symmetrical	O
quantity	O
,	O
that	O
is	O
to	O
say	O
kl	O
(	O
p	O
(	O
cid:6	O
)	O
q	O
)	O
(	O
cid:2	O
)	O
≡	O
kl	O
(	O
q	O
(	O
cid:6	O
)	O
p	O
)	O
.	O
we	O
now	O
show	O
that	O
the	O
kullback-leibler	O
divergence	O
satisﬁes	O
kl	O
(	O
p	O
(	O
cid:6	O
)	O
q	O
)	O
(	O
cid:2	O
)	O
0	O
with	O
equality	O
if	O
,	O
and	O
only	O
if	O
,	O
p	O
(	O
x	O
)	O
=	O
q	O
(	O
x	O
)	O
.	O
to	O
do	O
this	O
we	O
ﬁrst	O
introduce	O
the	O
concept	O
of	O
convex	O
functions	O
.	O
a	O
function	O
f	O
(	O
x	O
)	O
is	O
said	O
to	O
be	O
convex	O
if	O
it	O
has	O
the	O
property	O
that	O
every	O
chord	O
lies	O
on	O
or	O
above	O
the	O
function	O
,	O
as	O
shown	O
in	O
figure	O
1.31.	O
any	O
value	O
of	O
x	O
in	O
the	O
interval	O
from	O
x	O
=	O
a	O
to	O
x	O
=	O
b	O
can	O
be	O
written	O
in	O
the	O
form	O
λa	O
+	O
(	O
1	O
−	O
λ	O
)	O
b	O
where	O
0	O
(	O
cid:1	O
)	O
λ	O
(	O
cid:1	O
)	O
1.	O
the	O
corresponding	O
point	O
on	O
the	O
chord	O
is	O
given	O
by	O
λf	O
(	O
a	O
)	O
+	O
(	O
1	O
−	O
λ	O
)	O
f	O
(	O
b	O
)	O
,	O
claude	O
shannon	O
1916–2001	O
after	O
graduating	O
from	O
michigan	O
and	O
mit	O
,	O
shannon	O
joined	O
the	O
at	O
&	O
t	O
bell	O
telephone	O
laboratories	O
in	O
1941.	O
his	O
paper	O
‘	O
a	O
mathematical	O
theory	B
of	O
communication	O
’	O
published	O
in	O
the	O
bell	O
system	O
technical	O
journal	O
in	O
1948	O
laid	O
the	O
foundations	O
for	O
modern	O
information	O
the-	O
ory	O
.	O
this	O
paper	O
introduced	O
the	O
word	O
‘	O
bit	O
’	O
,	O
and	O
his	O
con-	O
cept	O
that	O
information	O
could	O
be	O
sent	O
as	O
a	O
stream	O
of	O
1s	O
and	O
0s	O
paved	O
the	O
way	O
for	O
the	O
communications	O
revo-	O
lution	O
.	O
it	O
is	O
said	O
that	O
von	O
neumann	O
recommended	O
to	O
shannon	O
that	O
he	O
use	O
the	O
term	O
entropy	B
,	O
not	O
only	O
be-	O
cause	O
of	O
its	O
similarity	O
to	O
the	O
quantity	O
used	O
in	O
physics	O
,	O
but	O
also	O
because	O
“	O
nobody	O
knows	O
what	O
entropy	B
really	O
is	O
,	O
so	O
in	O
any	O
discussion	O
you	O
will	O
always	O
have	O
an	O
advan-	O
tage	O
”	O
.	O
56	O
1.	O
introduction	O
figure	O
1.31	O
a	O
convex	B
function	I
f	O
(	O
x	O
)	O
is	O
one	O
for	O
which	O
ev-	O
ery	O
chord	O
(	O
shown	O
in	O
blue	O
)	O
lies	O
on	O
or	O
above	O
the	O
function	O
(	O
shown	O
in	O
red	O
)	O
.	O
f	O
(	O
x	O
)	O
chord	O
a	O
xλ	O
xλ	O
b	O
x	O
and	O
the	O
corresponding	O
value	O
of	O
the	O
function	O
is	O
f	O
(	O
λa	O
+	O
(	O
1	O
−	O
λ	O
)	O
b	O
)	O
.	O
convexity	O
then	O
implies	O
f	O
(	O
λa	O
+	O
(	O
1	O
−	O
λ	O
)	O
b	O
)	O
(	O
cid:1	O
)	O
λf	O
(	O
a	O
)	O
+	O
(	O
1	O
−	O
λ	O
)	O
f	O
(	O
b	O
)	O
.	O
(	O
1.114	O
)	O
exercise	O
1.36	O
exercise	O
1.38	O
this	O
is	O
equivalent	O
to	O
the	O
requirement	O
that	O
the	O
second	O
derivative	O
of	O
the	O
function	O
be	O
everywhere	O
positive	O
.	O
examples	O
of	O
convex	O
functions	O
are	O
x	O
ln	O
x	O
(	O
for	O
x	O
>	O
0	O
)	O
and	O
x2	O
.	O
a	O
function	O
is	O
called	O
strictly	O
convex	O
if	O
the	O
equality	O
is	O
satisﬁed	O
only	O
for	O
λ	O
=	O
0	O
and	O
λ	O
=	O
1.	O
if	O
a	O
function	O
has	O
the	O
opposite	O
property	O
,	O
namely	O
that	O
every	O
chord	O
lies	O
on	O
or	O
below	O
the	O
function	O
,	O
it	O
is	O
called	O
concave	O
,	O
with	O
a	O
corresponding	O
deﬁnition	O
for	O
strictly	O
concave	O
.	O
if	O
a	O
function	O
f	O
(	O
x	O
)	O
is	O
convex	O
,	O
then	O
−f	O
(	O
x	O
)	O
will	O
be	O
concave	O
.	O
using	O
the	O
technique	O
of	O
proof	O
by	O
induction	O
,	O
we	O
can	O
show	O
from	O
(	O
1.114	O
)	O
that	O
a	O
convex	B
function	I
f	O
(	O
x	O
)	O
satisﬁes	O
(	O
cid:22	O
)	O
m	O
(	O
cid:2	O
)	O
(	O
cid:23	O
)	O
m	O
(	O
cid:2	O
)	O
i=1	O
f	O
λixi	O
(	O
cid:5	O
)	O
i	O
λi	O
=	O
1	O
,	O
for	O
any	O
set	O
of	O
points	O
{	O
xi	O
}	O
.	O
the	O
result	O
(	O
1.115	O
)	O
is	O
where	O
λi	O
(	O
cid:2	O
)	O
0	O
and	O
known	O
as	O
jensen	O
’	O
s	O
inequality	O
.	O
if	O
we	O
interpret	O
the	O
λi	O
as	O
the	O
probability	B
distribution	O
over	O
a	O
discrete	O
variable	O
x	O
taking	O
the	O
values	O
{	O
xi	O
}	O
,	O
then	O
(	O
1.115	O
)	O
can	O
be	O
written	O
λif	O
(	O
xi	O
)	O
(	O
1.115	O
)	O
(	O
cid:1	O
)	O
i=1	O
(	O
1.116	O
)	O
where	O
e	O
[	O
·	O
]	O
denotes	O
the	O
expectation	B
.	O
for	O
continuous	O
variables	O
,	O
jensen	O
’	O
s	O
inequality	O
takes	O
the	O
form	O
(	O
cid:15	O
)	O
(	O
cid:6	O
)	O
(	O
cid:16	O
)	O
(	O
cid:6	O
)	O
f	O
(	O
e	O
[	O
x	O
]	O
)	O
(	O
cid:1	O
)	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
f	O
xp	O
(	O
x	O
)	O
dx	O
f	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
dx	O
.	O
(	O
1.117	O
)	O
we	O
can	O
apply	O
jensen	O
’	O
s	O
inequality	O
in	O
the	O
form	O
(	O
1.117	O
)	O
to	O
the	O
kullback-leibler	O
divergence	O
(	O
1.113	O
)	O
to	O
give	O
(	O
cid:6	O
)	O
(	O
cid:12	O
)	O
kl	O
(	O
p	O
(	O
cid:6	O
)	O
q	O
)	O
=	O
−	O
p	O
(	O
x	O
)	O
ln	O
q	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
dx	O
(	O
cid:2	O
)	O
−	O
ln	O
q	O
(	O
x	O
)	O
dx	O
=	O
0	O
(	O
1.118	O
)	O
(	O
cid:1	O
)	O
(	O
cid:13	O
)	O
(	O
cid:6	O
)	O
1.6.	O
information	B
theory	I
57	O
(	O
cid:28	O
)	O
where	O
we	O
have	O
used	O
the	O
fact	O
that	O
−	O
ln	O
x	O
is	O
a	O
convex	B
function	I
,	O
together	O
with	O
the	O
nor-	O
q	O
(	O
x	O
)	O
dx	O
=	O
1.	O
in	O
fact	O
,	O
−	O
ln	O
x	O
is	O
a	O
strictly	O
convex	B
function	I
,	O
malization	O
condition	O
so	O
the	O
equality	O
will	O
hold	O
if	O
,	O
and	O
only	O
if	O
,	O
q	O
(	O
x	O
)	O
=	O
p	O
(	O
x	O
)	O
for	O
all	O
x.	O
thus	O
we	O
can	O
in-	O
terpret	O
the	O
kullback-leibler	O
divergence	O
as	O
a	O
measure	O
of	O
the	O
dissimilarity	O
of	O
the	O
two	O
distributions	O
p	O
(	O
x	O
)	O
and	O
q	O
(	O
x	O
)	O
.	O
we	O
see	O
that	O
there	O
is	O
an	O
intimate	O
relationship	O
between	O
data	B
compression	I
and	O
den-	O
sity	O
estimation	O
(	O
i.e.	O
,	O
the	O
problem	O
of	O
modelling	O
an	O
unknown	O
probability	B
distribution	O
)	O
because	O
the	O
most	O
efﬁcient	O
compression	O
is	O
achieved	O
when	O
we	O
know	O
the	O
true	O
distri-	O
bution	O
.	O
if	O
we	O
use	O
a	O
distribution	O
that	O
is	O
different	O
from	O
the	O
true	O
one	O
,	O
then	O
we	O
must	O
necessarily	O
have	O
a	O
less	O
efﬁcient	O
coding	O
,	O
and	O
on	O
average	O
the	O
additional	O
information	O
that	O
must	O
be	O
transmitted	O
is	O
(	O
at	O
least	O
)	O
equal	O
to	O
the	O
kullback-leibler	O
divergence	O
be-	O
tween	O
the	O
two	O
distributions	O
.	O
suppose	O
that	O
data	O
is	O
being	O
generated	O
from	O
an	O
unknown	O
distribution	O
p	O
(	O
x	O
)	O
that	O
we	O
wish	O
to	O
model	O
.	O
we	O
can	O
try	O
to	O
approximate	O
this	O
distribution	O
using	O
some	O
parametric	O
distribution	O
q	O
(	O
x|θ	O
)	O
,	O
governed	O
by	O
a	O
set	O
of	O
adjustable	O
parameters	O
θ	O
,	O
for	O
example	O
a	O
multivariate	O
gaussian	O
.	O
one	O
way	O
to	O
determine	O
θ	O
is	O
to	O
minimize	O
the	O
kullback-leibler	O
divergence	O
between	O
p	O
(	O
x	O
)	O
and	O
q	O
(	O
x|θ	O
)	O
with	O
respect	O
to	O
θ.	O
we	O
can	O
not	O
do	O
this	O
directly	O
because	O
we	O
don	O
’	O
t	O
know	O
p	O
(	O
x	O
)	O
.	O
suppose	O
,	O
however	O
,	O
that	O
we	O
have	O
observed	O
a	O
ﬁnite	O
set	O
of	O
training	B
points	O
xn	O
,	O
for	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
drawn	O
from	O
p	O
(	O
x	O
)	O
.	O
then	O
the	O
expectation	B
with	O
respect	O
to	O
p	O
(	O
x	O
)	O
can	O
be	O
approximated	O
by	O
a	O
ﬁnite	O
sum	O
over	O
these	O
points	O
,	O
using	O
(	O
1.35	O
)	O
,	O
so	O
that	O
{	O
−	O
ln	O
q	O
(	O
xn|θ	O
)	O
+	O
ln	O
p	O
(	O
xn	O
)	O
}	O
.	O
(	O
1.119	O
)	O
kl	O
(	O
p	O
(	O
cid:6	O
)	O
q	O
)	O
(	O
cid:8	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
1.119	O
)	O
is	O
independent	B
of	O
θ	O
,	O
and	O
the	O
ﬁrst	O
term	O
is	O
the	O
negative	O
log	O
likelihood	O
function	O
for	O
θ	O
under	O
the	O
distribution	O
q	O
(	O
x|θ	O
)	O
eval-	O
uated	O
using	O
the	O
training	B
set	I
.	O
thus	O
we	O
see	O
that	O
minimizing	O
this	O
kullback-leibler	O
divergence	O
is	O
equivalent	O
to	O
maximizing	O
the	O
likelihood	B
function	I
.	O
now	O
consider	O
the	O
joint	O
distribution	O
between	O
two	O
sets	O
of	O
variables	O
x	O
and	O
y	O
given	O
by	O
p	O
(	O
x	O
,	O
y	O
)	O
.	O
if	O
the	O
sets	O
of	O
variables	O
are	O
independent	B
,	O
then	O
their	O
joint	O
distribution	O
will	O
factorize	O
into	O
the	O
product	O
of	O
their	O
marginals	O
p	O
(	O
x	O
,	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
.	O
if	O
the	O
variables	O
are	O
not	O
independent	B
,	O
we	O
can	O
gain	O
some	O
idea	O
of	O
whether	O
they	O
are	O
‘	O
close	O
’	O
to	O
being	O
indepen-	O
dent	O
by	O
considering	O
the	O
kullback-leibler	O
divergence	O
between	O
the	O
joint	O
distribution	O
and	O
the	O
product	O
of	O
the	O
marginals	O
,	O
given	O
by	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
i	O
[	O
x	O
,	O
y	O
]	O
≡	O
kl	O
(	O
p	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:6	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
)	O
=	O
−	O
p	O
(	O
x	O
,	O
y	O
)	O
ln	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
p	O
(	O
x	O
,	O
y	O
)	O
dx	O
dy	O
(	O
1.120	O
)	O
which	O
is	O
called	O
the	O
mutual	B
information	I
between	O
the	O
variables	O
x	O
and	O
y.	O
from	O
the	O
properties	O
of	O
the	O
kullback-leibler	O
divergence	O
,	O
we	O
see	O
that	O
i	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:2	O
)	O
0	O
with	O
equal-	O
ity	O
if	O
,	O
and	O
only	O
if	O
,	O
x	O
and	O
y	O
are	O
independent	B
.	O
using	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
,	O
we	O
see	O
that	O
the	O
mutual	B
information	I
is	O
related	O
to	O
the	O
conditional	B
entropy	I
through	O
i	O
[	O
x	O
,	O
y	O
]	O
=	O
h	O
[	O
x	O
]	O
−	O
h	O
[	O
x|y	O
]	O
=	O
h	O
[	O
y	O
]	O
−	O
h	O
[	O
y|x	O
]	O
.	O
(	O
1.121	O
)	O
exercise	O
1.41	O
58	O
1.	O
introduction	O
thus	O
we	O
can	O
view	O
the	O
mutual	B
information	I
as	O
the	O
reduction	O
in	O
the	O
uncertainty	O
about	O
x	O
by	O
virtue	O
of	O
being	O
told	O
the	O
value	O
of	O
y	O
(	O
or	O
vice	O
versa	O
)	O
.	O
from	O
a	O
bayesian	O
perspective	O
,	O
we	O
can	O
view	O
p	O
(	O
x	O
)	O
as	O
the	O
prior	B
distribution	O
for	O
x	O
and	O
p	O
(	O
x|y	O
)	O
as	O
the	O
posterior	O
distribu-	O
tion	O
after	O
we	O
have	O
observed	O
new	O
data	O
y.	O
the	O
mutual	B
information	I
therefore	O
represents	O
the	O
reduction	O
in	O
uncertainty	O
about	O
x	O
as	O
a	O
consequence	O
of	O
the	O
new	O
observation	O
y.	O
exercises	O
1.1	O
(	O
(	O
cid:1	O
)	O
)	O
www	O
consider	O
the	O
sum-of-squares	B
error	I
function	O
given	O
by	O
(	O
1.2	O
)	O
in	O
which	O
the	O
function	O
y	O
(	O
x	O
,	O
w	O
)	O
is	O
given	O
by	O
the	O
polynomial	O
(	O
1.1	O
)	O
.	O
show	O
that	O
the	O
coefﬁcients	O
w	O
=	O
{	O
wi	O
}	O
that	O
minimize	O
this	O
error	B
function	I
are	O
given	O
by	O
the	O
solution	O
to	O
the	O
following	O
set	O
of	O
linear	O
equations	O
m	O
(	O
cid:2	O
)	O
where	O
n	O
(	O
cid:2	O
)	O
n=1	O
aij	O
=	O
aijwj	O
=	O
ti	O
j=0	O
(	O
xn	O
)	O
i+j	O
,	O
ti	O
=	O
(	O
1.122	O
)	O
(	O
xn	O
)	O
itn	O
.	O
(	O
1.123	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
here	O
a	O
sufﬁx	O
i	O
or	O
j	O
denotes	O
the	O
index	O
of	O
a	O
component	O
,	O
whereas	O
(	O
x	O
)	O
i	O
denotes	O
x	O
raised	O
to	O
the	O
power	O
of	O
i	O
.	O
1.2	O
(	O
(	O
cid:1	O
)	O
)	O
write	O
down	O
the	O
set	O
of	O
coupled	O
linear	O
equations	O
,	O
analogous	O
to	O
(	O
1.122	O
)	O
,	O
satisﬁed	O
by	O
the	O
coefﬁcients	O
wi	O
which	O
minimize	O
the	O
regularized	O
sum-of-squares	O
error	B
function	I
given	O
by	O
(	O
1.4	O
)	O
.	O
1.3	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
suppose	O
that	O
we	O
have	O
three	O
coloured	O
boxes	O
r	O
(	O
red	O
)	O
,	O
b	O
(	O
blue	O
)	O
,	O
and	O
g	O
(	O
green	O
)	O
.	O
box	O
r	O
contains	O
3	O
apples	O
,	O
4	O
oranges	O
,	O
and	O
3	O
limes	O
,	O
box	O
b	O
contains	O
1	O
apple	O
,	O
1	O
orange	O
,	O
and	O
0	O
limes	O
,	O
and	O
box	O
g	O
contains	O
3	O
apples	O
,	O
3	O
oranges	O
,	O
and	O
4	O
limes	O
.	O
if	O
a	O
box	O
is	O
chosen	O
at	O
random	O
with	O
probabilities	O
p	O
(	O
r	O
)	O
=	O
0.2	O
,	O
p	O
(	O
b	O
)	O
=	O
0.2	O
,	O
p	O
(	O
g	O
)	O
=	O
0.6	O
,	O
and	O
a	O
piece	O
of	O
fruit	O
is	O
removed	O
from	O
the	O
box	O
(	O
with	O
equal	O
probability	B
of	O
selecting	O
any	O
of	O
the	O
items	O
in	O
the	O
box	O
)	O
,	O
then	O
what	O
is	O
the	O
probability	B
of	O
selecting	O
an	O
apple	O
?	O
if	O
we	O
observe	O
that	O
the	O
selected	O
fruit	O
is	O
in	O
fact	O
an	O
orange	O
,	O
what	O
is	O
the	O
probability	B
that	O
it	O
came	O
from	O
the	O
green	O
box	O
?	O
1.4	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
www	O
consider	O
a	O
probability	B
density	O
px	O
(	O
x	O
)	O
deﬁned	O
over	O
a	O
continuous	O
vari-	O
able	O
x	O
,	O
and	O
suppose	O
that	O
we	O
make	O
a	O
nonlinear	O
change	O
of	O
variable	O
using	O
x	O
=	O
g	O
(	O
y	O
)	O
,	O
so	O
that	O
the	O
density	B
transforms	O
according	O
to	O
(	O
1.27	O
)	O
.	O
by	O
differentiating	O
(	O
1.27	O
)	O
,	O
show	O
that	O
the	O
location	O
(	O
cid:1	O
)	O
y	O
of	O
the	O
maximum	O
of	O
the	O
density	B
in	O
y	O
is	O
not	O
in	O
general	O
related	O
to	O
the	O
location	O
(	O
cid:1	O
)	O
x	O
of	O
the	O
maximum	O
of	O
the	O
density	B
over	O
x	O
by	O
the	O
simple	O
functional	B
relation	O
(	O
cid:1	O
)	O
x	O
=	O
g	O
(	O
(	O
cid:1	O
)	O
y	O
)	O
as	O
a	O
consequence	O
of	O
the	O
jacobian	O
factor	O
.	O
this	O
shows	O
that	O
the	O
maximum	O
of	O
a	O
probability	B
density	O
(	O
in	O
contrast	O
to	O
a	O
simple	O
function	O
)	O
is	O
dependent	O
on	O
the	O
choice	O
of	O
variable	O
.	O
verify	O
that	O
,	O
in	O
the	O
case	O
of	O
a	O
linear	O
transformation	O
,	O
the	O
location	O
of	O
the	O
maximum	O
transforms	O
in	O
the	O
same	O
way	O
as	O
the	O
variable	O
itself	O
.	O
1.5	O
(	O
(	O
cid:1	O
)	O
)	O
using	O
the	O
deﬁnition	O
(	O
1.38	O
)	O
show	O
that	O
var	O
[	O
f	O
(	O
x	O
)	O
]	O
satisﬁes	O
(	O
1.39	O
)	O
.	O
which	O
we	O
can	O
evaluate	O
by	O
ﬁrst	O
writing	O
its	O
square	O
in	O
the	O
form	O
(	O
cid:16	O
)	O
−	O
1	O
2σ2	O
x2	O
dx	O
(	O
cid:15	O
)	O
i	O
=	O
(	O
cid:6	O
)	O
∞	O
(	O
cid:6	O
)	O
∞	O
−∞	O
exp	O
(	O
cid:15	O
)	O
(	O
cid:6	O
)	O
∞	O
i	O
2	O
=	O
exp	O
−∞	O
−∞	O
2σ2	O
y2	O
2σ2	O
x2	O
−	O
1	O
−	O
1	O
(	O
cid:11	O
)	O
1/2	O
(	O
cid:10	O
)	O
(	O
cid:16	O
)	O
(	O
1.124	O
)	O
dx	O
dy	O
.	O
(	O
1.125	O
)	O
1.6	O
(	O
(	O
cid:1	O
)	O
)	O
show	O
that	O
if	O
two	O
variables	O
x	O
and	O
y	O
are	O
independent	B
,	O
then	O
their	O
covariance	B
is	O
zero	O
.	O
exercises	O
59	O
1.7	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
www	O
in	O
this	O
exercise	O
,	O
we	O
prove	O
the	O
normalization	O
condition	O
(	O
1.48	O
)	O
for	O
the	O
univariate	O
gaussian	O
.	O
to	O
do	O
this	O
consider	O
,	O
the	O
integral	O
now	O
make	O
the	O
transformation	O
from	O
cartesian	O
coordinates	O
(	O
x	O
,	O
y	O
)	O
to	O
polar	O
coordinates	O
(	O
r	O
,	O
θ	O
)	O
and	O
then	O
substitute	O
u	O
=	O
r2	O
.	O
show	O
that	O
,	O
by	O
performing	O
the	O
integrals	O
over	O
θ	O
and	O
u	O
,	O
and	O
then	O
taking	O
the	O
square	O
root	O
of	O
both	O
sides	O
,	O
we	O
obtain	O
(	O
1.126	O
)	O
finally	O
,	O
use	O
this	O
result	O
to	O
show	O
that	O
the	O
gaussian	O
distribution	O
n	O
(	O
x|µ	O
,	O
σ2	O
)	O
is	O
normal-	O
ized	O
.	O
.	O
i	O
=	O
2πσ2	O
1.8	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
www	O
by	O
using	O
a	O
change	O
of	O
variables	O
,	O
verify	O
that	O
the	O
univariate	O
gaussian	O
distribution	O
given	O
by	O
(	O
1.46	O
)	O
satisﬁes	O
(	O
1.49	O
)	O
.	O
next	O
,	O
by	O
differentiating	O
both	O
sides	O
of	O
the	O
normalization	O
condition	O
(	O
cid:6	O
)	O
∞	O
−∞	O
n	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
x|µ	O
,	O
σ2	O
dx	O
=	O
1	O
(	O
1.127	O
)	O
with	O
respect	O
to	O
σ2	O
,	O
verify	O
that	O
the	O
gaussian	O
satisﬁes	O
(	O
1.50	O
)	O
.	O
finally	O
,	O
show	O
that	O
(	O
1.51	O
)	O
holds	O
.	O
1.9	O
(	O
(	O
cid:1	O
)	O
)	O
www	O
show	O
that	O
the	O
mode	O
(	O
i.e	O
.	O
the	O
maximum	O
)	O
of	O
the	O
gaussian	O
distribution	O
(	O
1.46	O
)	O
is	O
given	O
by	O
µ.	O
similarly	O
,	O
show	O
that	O
the	O
mode	O
of	O
the	O
multivariate	O
gaussian	O
(	O
1.52	O
)	O
is	O
given	O
by	O
µ	O
.	O
1.10	O
(	O
(	O
cid:1	O
)	O
)	O
www	O
suppose	O
that	O
the	O
two	O
variables	O
x	O
and	O
z	O
are	O
statistically	O
independent	B
.	O
show	O
that	O
the	O
mean	B
and	O
variance	B
of	O
their	O
sum	O
satisﬁes	O
e	O
[	O
x	O
+	O
z	O
]	O
=	O
e	O
[	O
x	O
]	O
+	O
e	O
[	O
z	O
]	O
var	O
[	O
x	O
+	O
z	O
]	O
=	O
var	O
[	O
x	O
]	O
+	O
var	O
[	O
z	O
]	O
.	O
(	O
1.128	O
)	O
(	O
1.129	O
)	O
1.11	O
(	O
(	O
cid:1	O
)	O
)	O
by	O
setting	O
the	O
derivatives	O
of	O
the	O
log	O
likelihood	O
function	O
(	O
1.54	O
)	O
with	O
respect	O
to	O
µ	O
and	O
σ2	O
equal	O
to	O
zero	O
,	O
verify	O
the	O
results	O
(	O
1.55	O
)	O
and	O
(	O
1.56	O
)	O
.	O
60	O
1.	O
introduction	O
1.12	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
www	O
using	O
the	O
results	O
(	O
1.49	O
)	O
and	O
(	O
1.50	O
)	O
,	O
show	O
that	O
e	O
[	O
xnxm	O
]	O
=	O
µ2	O
+	O
inmσ2	O
(	O
1.130	O
)	O
where	O
xn	O
and	O
xm	O
denote	O
data	O
points	O
sampled	O
from	O
a	O
gaussian	O
distribution	O
with	O
mean	B
µ	O
and	O
variance	B
σ2	O
,	O
and	O
inm	O
satisﬁes	O
inm	O
=	O
1	O
if	O
n	O
=	O
m	O
and	O
inm	O
=	O
0	O
otherwise	O
.	O
hence	O
prove	O
the	O
results	O
(	O
1.57	O
)	O
and	O
(	O
1.58	O
)	O
.	O
1.13	O
(	O
(	O
cid:1	O
)	O
)	O
suppose	O
that	O
the	O
variance	B
of	O
a	O
gaussian	O
is	O
estimated	O
using	O
the	O
result	O
(	O
1.56	O
)	O
but	O
with	O
the	O
maximum	B
likelihood	I
estimate	O
µml	O
replaced	O
with	O
the	O
true	O
value	O
µ	O
of	O
the	O
mean	B
.	O
show	O
that	O
this	O
estimator	O
has	O
the	O
property	O
that	O
its	O
expectation	B
is	O
given	O
by	O
the	O
true	O
variance	B
σ2	O
.	O
1.14	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
show	O
that	O
an	O
arbitrary	O
square	O
matrix	O
with	O
elements	O
wij	O
can	O
be	O
written	O
in	O
the	O
form	O
wij	O
=	O
ws	O
ij	O
are	O
symmetric	O
and	O
anti-symmetric	O
matrices	O
,	O
respectively	O
,	O
satisfying	O
ws	O
ji	O
for	O
all	O
i	O
and	O
j.	O
now	O
consider	O
the	O
second	B
order	I
term	O
in	O
a	O
higher	O
order	O
polynomial	O
in	O
d	O
dimensions	O
,	O
given	O
by	O
ij	O
=	O
−wa	O
ij	O
and	O
wa	O
ij	O
+	O
wa	O
ij	O
where	O
ws	O
ij	O
=	O
ws	O
ji	O
and	O
wa	O
d	O
(	O
cid:2	O
)	O
d	O
(	O
cid:2	O
)	O
show	O
that	O
d	O
(	O
cid:2	O
)	O
d	O
(	O
cid:2	O
)	O
wijxixj	O
.	O
d	O
(	O
cid:2	O
)	O
d	O
(	O
cid:2	O
)	O
i=1	O
j=1	O
wijxixj	O
=	O
(	O
1.131	O
)	O
ws	O
ijxixj	O
(	O
1.132	O
)	O
i=1	O
j=1	O
i=1	O
j=1	O
so	O
that	O
the	O
contribution	O
from	O
the	O
anti-symmetric	O
matrix	O
vanishes	O
.	O
we	O
therefore	O
see	O
that	O
,	O
without	O
loss	O
of	O
generality	O
,	O
the	O
matrix	O
of	O
coefﬁcients	O
wij	O
can	O
be	O
chosen	O
to	O
be	O
symmetric	O
,	O
and	O
so	O
not	O
all	O
of	O
the	O
d2	O
elements	O
of	O
this	O
matrix	O
can	O
be	O
chosen	O
indepen-	O
dently	O
.	O
show	O
that	O
the	O
number	O
of	O
independent	B
parameters	O
in	O
the	O
matrix	O
ws	O
ij	O
is	O
given	O
by	O
d	O
(	O
d	O
+	O
1	O
)	O
/2	O
.	O
1.15	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
www	O
in	O
this	O
exercise	O
and	O
the	O
next	O
,	O
we	O
explore	O
how	O
the	O
number	O
of	O
indepen-	O
dent	O
parameters	O
in	O
a	O
polynomial	O
grows	O
with	O
the	O
order	O
m	O
of	O
the	O
polynomial	O
and	O
with	O
the	O
dimensionality	O
d	O
of	O
the	O
input	O
space	O
.	O
we	O
start	O
by	O
writing	O
down	O
the	O
m	O
th	O
order	O
term	O
for	O
a	O
polynomial	O
in	O
d	O
dimensions	O
in	O
the	O
form	O
d	O
(	O
cid:2	O
)	O
d	O
(	O
cid:2	O
)	O
···	O
d	O
(	O
cid:2	O
)	O
i1=1	O
i2=1	O
im	O
=1	O
wi1i2···im	O
xi1xi2	O
···	O
xim	O
.	O
(	O
1.133	O
)	O
the	O
coefﬁcients	O
wi1i2···im	O
comprise	O
dm	O
elements	O
,	O
but	O
the	O
number	O
of	O
independent	B
parameters	O
is	O
signiﬁcantly	O
fewer	O
due	O
to	O
the	O
many	O
interchange	O
symmetries	B
of	O
the	O
factor	O
xi1xi2	O
···	O
xim	O
.	O
begin	O
by	O
showing	O
that	O
the	O
redundancy	O
in	O
the	O
coefﬁcients	O
can	O
be	O
removed	O
by	O
rewriting	O
this	O
m	O
th	O
order	O
term	O
in	O
the	O
form	O
d	O
(	O
cid:2	O
)	O
i1	O
(	O
cid:2	O
)	O
···	O
im−1	O
(	O
cid:2	O
)	O
i1=1	O
i2=1	O
im	O
=1	O
(	O
cid:4	O
)	O
wi1i2···im	O
xi1xi2	O
···	O
xim	O
.	O
(	O
1.134	O
)	O
note	O
that	O
the	O
precise	O
relationship	O
between	O
the	O
(	O
cid:4	O
)	O
w	O
coefﬁcients	O
and	O
w	O
coefﬁcients	O
need	O
exercises	O
61	O
not	O
be	O
made	O
explicit	O
.	O
use	O
this	O
result	O
to	O
show	O
that	O
the	O
number	O
of	O
independent	B
param-	O
eters	O
n	O
(	O
d	O
,	O
m	O
)	O
,	O
which	O
appear	O
at	O
order	O
m	O
,	O
satisﬁes	O
the	O
following	O
recursion	O
relation	O
next	O
use	O
proof	O
by	O
induction	O
to	O
show	O
that	O
the	O
following	O
result	O
holds	O
d	O
(	O
cid:2	O
)	O
i=1	O
n	O
(	O
d	O
,	O
m	O
)	O
=	O
n	O
(	O
i	O
,	O
m	O
−	O
1	O
)	O
.	O
d	O
(	O
cid:2	O
)	O
i=1	O
(	O
i	O
+	O
m	O
−	O
2	O
)	O
!	O
(	O
i	O
−	O
1	O
)	O
!	O
(	O
m	O
−	O
1	O
)	O
!	O
=	O
(	O
d	O
+	O
m	O
−	O
1	O
)	O
!	O
(	O
d	O
−	O
1	O
)	O
!	O
m	O
!	O
(	O
1.135	O
)	O
(	O
1.136	O
)	O
which	O
can	O
be	O
done	O
by	O
ﬁrst	O
proving	O
the	O
result	O
for	O
d	O
=	O
1	O
and	O
arbitrary	O
m	O
by	O
making	O
use	O
of	O
the	O
result	O
0	O
!	O
=	O
1	O
,	O
then	O
assuming	O
it	O
is	O
correct	O
for	O
dimension	O
d	O
and	O
verifying	O
that	O
it	O
is	O
correct	O
for	O
dimension	O
d	O
+	O
1.	O
finally	O
,	O
use	O
the	O
two	O
previous	O
results	O
,	O
together	O
with	O
proof	O
by	O
induction	O
,	O
to	O
show	O
(	O
d	O
+	O
m	O
−	O
1	O
)	O
!	O
(	O
d	O
−	O
1	O
)	O
!	O
m	O
!	O
.	O
n	O
(	O
d	O
,	O
m	O
)	O
=	O
(	O
1.137	O
)	O
to	O
do	O
this	O
,	O
ﬁrst	O
show	O
that	O
the	O
result	O
is	O
true	O
for	O
m	O
=	O
2	O
,	O
and	O
any	O
value	O
of	O
d	O
(	O
cid:2	O
)	O
1	O
,	O
by	O
comparison	O
with	O
the	O
result	O
of	O
exercise	O
1.14.	O
then	O
make	O
use	O
of	O
(	O
1.135	O
)	O
,	O
together	O
with	O
(	O
1.136	O
)	O
,	O
to	O
show	O
that	O
,	O
if	O
the	O
result	O
holds	O
at	O
order	O
m	O
−	O
1	O
,	O
then	O
it	O
will	O
also	O
hold	O
at	O
order	O
m	O
1.16	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
in	O
exercise	O
1.15	O
,	O
we	O
proved	O
the	O
result	O
(	O
1.135	O
)	O
for	O
the	O
number	O
of	O
independent	B
parameters	O
in	O
the	O
m	O
th	O
order	O
term	O
of	O
a	O
d-dimensional	O
polynomial	O
.	O
we	O
now	O
ﬁnd	O
an	O
expression	O
for	O
the	O
total	O
number	O
n	O
(	O
d	O
,	O
m	O
)	O
of	O
independent	B
parameters	O
in	O
all	O
of	O
the	O
terms	O
up	O
to	O
and	O
including	O
the	O
m6th	O
order	O
.	O
first	O
show	O
that	O
n	O
(	O
d	O
,	O
m	O
)	O
satisﬁes	O
m	O
(	O
cid:2	O
)	O
n	O
(	O
d	O
,	O
m	O
)	O
=	O
n	O
(	O
d	O
,	O
m	O
)	O
(	O
1.138	O
)	O
where	O
n	O
(	O
d	O
,	O
m	O
)	O
is	O
the	O
number	O
of	O
independent	B
parameters	O
in	O
the	O
term	O
of	O
order	O
m.	O
now	O
make	O
use	O
of	O
the	O
result	O
(	O
1.137	O
)	O
,	O
together	O
with	O
proof	O
by	O
induction	O
,	O
to	O
show	O
that	O
m=0	O
n	O
(	O
d	O
,	O
m	O
)	O
=	O
(	O
d	O
+	O
m	O
)	O
!	O
d	O
!	O
m	O
!	O
.	O
(	O
1.139	O
)	O
this	O
can	O
be	O
done	O
by	O
ﬁrst	O
proving	O
that	O
the	O
result	O
holds	O
for	O
m	O
=	O
0	O
and	O
arbitrary	O
d	O
(	O
cid:2	O
)	O
1	O
,	O
then	O
assuming	O
that	O
it	O
holds	O
at	O
order	O
m	O
,	O
and	O
hence	O
showing	O
that	O
it	O
holds	O
at	O
order	O
m	O
+	O
1.	O
finally	O
,	O
make	O
use	O
of	O
stirling	O
’	O
s	O
approximation	O
in	O
the	O
form	O
n	O
!	O
(	O
cid:8	O
)	O
nne	O
−n	O
(	O
1.140	O
)	O
for	O
large	O
n	O
to	O
show	O
that	O
,	O
for	O
d	O
(	O
cid:10	O
)	O
m	O
,	O
the	O
quantity	O
n	O
(	O
d	O
,	O
m	O
)	O
grows	O
like	O
dm	O
,	O
and	O
for	O
m	O
(	O
cid:10	O
)	O
d	O
it	O
grows	O
like	O
m	O
d.	O
consider	O
a	O
cubic	O
(	O
m	O
=	O
3	O
)	O
polynomial	O
in	O
d	O
dimensions	O
,	O
and	O
evaluate	O
numerically	O
the	O
total	O
number	O
of	O
independent	B
parameters	O
for	O
(	O
i	O
)	O
d	O
=	O
10	O
and	O
(	O
ii	O
)	O
d	O
=	O
100	O
,	O
which	O
correspond	O
to	O
typical	O
small-scale	O
and	O
medium-scale	O
machine	O
learning	O
applications	O
.	O
62	O
1.	O
introduction	O
1.17	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
www	O
the	O
gamma	B
function	I
is	O
deﬁned	O
by	O
ux−1e	O
γ	O
(	O
x	O
)	O
≡	O
−u	O
du	O
.	O
(	O
cid:6	O
)	O
∞	O
(	O
1.141	O
)	O
using	O
integration	O
by	O
parts	O
,	O
prove	O
the	O
relation	O
γ	O
(	O
x	O
+	O
1	O
)	O
=	O
xγ	O
(	O
x	O
)	O
.	O
show	O
also	O
that	O
γ	O
(	O
1	O
)	O
=	O
1	O
and	O
hence	O
that	O
γ	O
(	O
x	O
+	O
1	O
)	O
=	O
x	O
!	O
when	O
x	O
is	O
an	O
integer	O
.	O
0	O
1.18	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
www	O
we	O
can	O
use	O
the	O
result	O
(	O
1.126	O
)	O
to	O
derive	O
an	O
expression	O
for	O
the	O
surface	O
area	O
sd	O
,	O
and	O
the	O
volume	O
vd	O
,	O
of	O
a	O
sphere	O
of	O
unit	O
radius	O
in	O
d	O
dimensions	O
.	O
to	O
do	O
this	O
,	O
consider	O
the	O
following	O
result	O
,	O
which	O
is	O
obtained	O
by	O
transforming	O
from	O
cartesian	O
to	O
polar	O
coordinates	O
(	O
cid:6	O
)	O
∞	O
d	O
(	O
cid:14	O
)	O
−∞	O
i=1	O
(	O
cid:6	O
)	O
∞	O
0	O
−x2	O
e	O
i	O
dxi	O
=	O
sd	O
−r2	O
e	O
rd−1	O
dr.	O
(	O
1.142	O
)	O
using	O
the	O
deﬁnition	O
(	O
1.141	O
)	O
of	O
the	O
gamma	B
function	I
,	O
together	O
with	O
(	O
1.126	O
)	O
,	O
evaluate	O
both	O
sides	O
of	O
this	O
equation	O
,	O
and	O
hence	O
show	O
that	O
sd	O
=	O
2πd/2	O
γ	O
(	O
d/2	O
)	O
.	O
(	O
1.143	O
)	O
next	O
,	O
by	O
integrating	O
with	O
respect	O
to	O
radius	O
from	O
0	O
to	O
1	O
,	O
show	O
that	O
the	O
volume	O
of	O
the	O
unit	O
sphere	O
in	O
d	O
dimensions	O
is	O
given	O
by	O
finally	O
,	O
use	O
the	O
results	O
γ	O
(	O
1	O
)	O
=	O
1	O
and	O
γ	O
(	O
3/2	O
)	O
=	O
(	O
1.144	O
)	O
reduce	O
to	O
the	O
usual	O
expressions	O
for	O
d	O
=	O
2	O
and	O
d	O
=	O
3.	O
vd	O
=	O
sd	O
d	O
.	O
√	O
π/2	O
to	O
show	O
that	O
(	O
1.143	O
)	O
and	O
(	O
1.144	O
)	O
1.19	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
consider	O
a	O
sphere	O
of	O
radius	O
a	O
in	O
d-dimensions	O
together	O
with	O
the	O
concentric	O
hypercube	O
of	O
side	O
2a	O
,	O
so	O
that	O
the	O
sphere	O
touches	O
the	O
hypercube	O
at	O
the	O
centres	O
of	O
each	O
of	O
its	O
sides	O
.	O
by	O
using	O
the	O
results	O
of	O
exercise	O
1.18	O
,	O
show	O
that	O
the	O
ratio	O
of	O
the	O
volume	O
of	O
the	O
sphere	O
to	O
the	O
volume	O
of	O
the	O
cube	O
is	O
given	O
by	O
volume	O
of	O
sphere	O
volume	O
of	O
cube	O
=	O
πd/2	O
d2d−1γ	O
(	O
d/2	O
)	O
.	O
(	O
1.145	O
)	O
now	O
make	O
use	O
of	O
stirling	O
’	O
s	O
formula	O
in	O
the	O
form	O
γ	O
(	O
x	O
+	O
1	O
)	O
(	O
cid:8	O
)	O
(	O
2π	O
)	O
1/2e	O
−xxx+1/2	O
(	O
1.146	O
)	O
which	O
is	O
valid	O
for	O
x	O
(	O
cid:10	O
)	O
1	O
,	O
to	O
show	O
that	O
,	O
as	O
d	O
→	O
∞	O
,	O
the	O
ratio	O
(	O
1.145	O
)	O
goes	O
to	O
zero	O
.	O
show	O
also	O
that	O
the	O
ratio	O
of	O
the	O
distance	O
from	O
the	O
centre	O
of	O
the	O
hypercube	O
to	O
one	O
of	O
d	O
,	O
which	O
the	O
corners	O
,	O
divided	O
by	O
the	O
perpendicular	O
distance	O
to	O
one	O
of	O
the	O
sides	O
,	O
is	O
therefore	O
goes	O
to	O
∞	O
as	O
d	O
→	O
∞	O
.	O
from	O
these	O
results	O
we	O
see	O
that	O
,	O
in	O
a	O
space	O
of	O
high	O
dimensionality	O
,	O
most	O
of	O
the	O
volume	O
of	O
a	O
cube	O
is	O
concentrated	O
in	O
the	O
large	O
number	O
of	O
corners	O
,	O
which	O
themselves	O
become	O
very	O
long	O
‘	O
spikes	O
’	O
!	O
√	O
exercises	O
63	O
1.20	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
www	O
in	O
this	O
exercise	O
,	O
we	O
explore	O
the	O
behaviour	O
of	O
the	O
gaussian	O
distribution	O
in	O
high-dimensional	O
spaces	O
.	O
consider	O
a	O
gaussian	O
distribution	O
in	O
d	O
dimensions	O
given	O
by	O
1	O
p	O
(	O
x	O
)	O
=	O
(	O
2πσ2	O
)	O
d/2	O
exp	O
.	O
(	O
1.147	O
)	O
(	O
cid:16	O
)	O
(	O
cid:15	O
)	O
−	O
(	O
cid:6	O
)	O
x	O
(	O
cid:6	O
)	O
2	O
2σ2	O
(	O
cid:15	O
)	O
−	O
r2	O
2σ2	O
(	O
2πσ2	O
)	O
d/2	O
exp	O
where	O
sd	O
is	O
the	O
surface	O
area	O
of	O
a	O
unit	O
sphere	O
in	O
d	O
dimensions	O
.	O
show	O
that	O
the	O
function	O
dσ	O
.	O
by	O
considering	O
we	O
wish	O
to	O
ﬁnd	O
the	O
density	B
with	O
respect	O
to	O
radius	O
in	O
polar	O
coordinates	O
in	O
which	O
the	O
direction	O
variables	O
have	O
been	O
integrated	O
out	O
.	O
to	O
do	O
this	O
,	O
show	O
that	O
the	O
integral	O
of	O
the	O
probability	B
density	O
over	O
a	O
thin	O
shell	O
of	O
radius	O
r	O
and	O
thickness	O
	O
,	O
where	O
	O
(	O
cid:12	O
)	O
1	O
,	O
is	O
given	O
by	O
p	O
(	O
r	O
)	O
	O
where	O
(	O
cid:16	O
)	O
p	O
(	O
r	O
)	O
=	O
sdrd−1	O
p	O
(	O
r	O
)	O
has	O
a	O
single	O
stationary	B
point	O
located	O
,	O
for	O
large	O
d	O
,	O
at	O
(	O
cid:1	O
)	O
r	O
(	O
cid:8	O
)	O
√	O
p	O
(	O
(	O
cid:1	O
)	O
r	O
+	O
	O
)	O
where	O
	O
(	O
cid:12	O
)	O
(	O
cid:1	O
)	O
r	O
,	O
show	O
that	O
for	O
large	O
d	O
,	O
p	O
(	O
(	O
cid:1	O
)	O
r	O
+	O
	O
)	O
=	O
p	O
(	O
(	O
cid:1	O
)	O
r	O
)	O
exp	O
which	O
shows	O
that	O
(	O
cid:1	O
)	O
r	O
is	O
a	O
maximum	O
of	O
the	O
radial	O
probability	O
density	B
and	O
also	O
that	O
p	O
(	O
r	O
)	O
decays	O
exponentially	O
away	O
from	O
its	O
maximum	O
at	O
(	O
cid:1	O
)	O
r	O
with	O
length	O
scale	O
σ.	O
we	O
have	O
already	O
seen	O
that	O
σ	O
(	O
cid:12	O
)	O
(	O
cid:1	O
)	O
r	O
for	O
large	O
d	O
,	O
and	O
so	O
we	O
see	O
that	O
most	O
of	O
the	O
probability	B
density	O
p	O
(	O
x	O
)	O
is	O
larger	O
at	O
the	O
origin	O
than	O
at	O
the	O
radius	O
(	O
cid:1	O
)	O
r	O
by	O
a	O
factor	O
of	O
exp	O
(	O
d/2	O
)	O
.	O
mass	O
is	O
concentrated	O
in	O
a	O
thin	O
shell	O
at	O
large	O
radius	O
.	O
finally	O
,	O
show	O
that	O
the	O
probability	B
−	O
32	O
2σ2	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
(	O
1.148	O
)	O
(	O
1.149	O
)	O
we	O
therefore	O
see	O
that	O
most	O
of	O
the	O
probability	B
mass	O
in	O
a	O
high-dimensional	O
gaussian	O
distribution	O
is	O
located	O
at	O
a	O
different	O
radius	O
from	O
the	O
region	O
of	O
high	O
probability	B
density	O
.	O
this	O
property	O
of	O
distributions	O
in	O
spaces	O
of	O
high	O
dimensionality	O
will	O
have	O
important	O
consequences	O
when	O
we	O
consider	O
bayesian	O
inference	B
of	O
model	O
parameters	O
in	O
later	O
chapters	O
.	O
1.21	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
consider	O
two	O
nonnegative	O
numbers	O
a	O
and	O
b	O
,	O
and	O
show	O
that	O
,	O
if	O
a	O
(	O
cid:1	O
)	O
b	O
,	O
then	O
a	O
(	O
cid:1	O
)	O
(	O
ab	O
)	O
1/2	O
.	O
use	O
this	O
result	O
to	O
show	O
that	O
,	O
if	O
the	O
decision	O
regions	O
of	O
a	O
two-class	O
classiﬁcation	B
problem	O
are	O
chosen	O
to	O
minimize	O
the	O
probability	B
of	O
misclassiﬁcation	O
,	O
this	O
probability	B
will	O
satisfy	O
(	O
cid:6	O
)	O
p	O
(	O
mistake	O
)	O
(	O
cid:1	O
)	O
{	O
p	O
(	O
x	O
,	O
c1	O
)	O
p	O
(	O
x	O
,	O
c2	O
)	O
}	O
1/2	O
dx	O
.	O
(	O
1.150	O
)	O
1.22	O
(	O
(	O
cid:1	O
)	O
)	O
www	O
given	O
a	O
loss	B
matrix	I
with	O
elements	O
lkj	O
,	O
the	O
expected	O
risk	O
is	O
minimized	O
if	O
,	O
for	O
each	O
x	O
,	O
we	O
choose	O
the	O
class	O
that	O
minimizes	O
(	O
1.81	O
)	O
.	O
verify	O
that	O
,	O
when	O
the	O
loss	B
matrix	I
is	O
given	O
by	O
lkj	O
=	O
1	O
−	O
ikj	O
,	O
where	O
ikj	O
are	O
the	O
elements	O
of	O
the	O
identity	O
matrix	O
,	O
this	O
reduces	O
to	O
the	O
criterion	O
of	O
choosing	O
the	O
class	O
having	O
the	O
largest	O
posterior	B
probability	I
.	O
what	O
is	O
the	O
interpretation	O
of	O
this	O
form	O
of	O
loss	B
matrix	I
?	O
1.23	O
(	O
(	O
cid:1	O
)	O
)	O
derive	O
the	O
criterion	O
for	O
minimizing	O
the	O
expected	O
loss	O
when	O
there	O
is	O
a	O
general	O
loss	B
matrix	I
and	O
general	O
prior	B
probabilities	O
for	O
the	O
classes	O
.	O
64	O
1.	O
introduction	O
1.24	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
www	O
consider	O
a	O
classiﬁcation	B
problem	O
in	O
which	O
the	O
loss	O
incurred	O
when	O
an	O
input	O
vector	O
from	O
class	O
ck	O
is	O
classiﬁed	O
as	O
belonging	O
to	O
class	O
cj	O
is	O
given	O
by	O
the	O
loss	B
matrix	I
lkj	O
,	O
and	O
for	O
which	O
the	O
loss	O
incurred	O
in	O
selecting	O
the	O
reject	B
option	I
is	O
λ.	O
find	O
the	O
decision	O
criterion	O
that	O
will	O
give	O
the	O
minimum	O
expected	O
loss	O
.	O
verify	O
that	O
this	O
reduces	O
to	O
the	O
reject	O
criterion	O
discussed	O
in	O
section	O
1.5.3	O
when	O
the	O
loss	B
matrix	I
is	O
given	O
by	O
lkj	O
=	O
1	O
−	O
ikj	O
.	O
what	O
is	O
the	O
relationship	O
between	O
λ	O
and	O
the	O
rejection	O
threshold	O
θ	O
?	O
1.25	O
(	O
(	O
cid:1	O
)	O
)	O
www	O
consider	O
the	O
generalization	B
of	O
the	O
squared	O
loss	B
function	I
(	O
1.87	O
)	O
for	O
a	O
single	O
target	O
variable	O
t	O
to	O
the	O
case	O
of	O
multiple	O
target	O
variables	O
described	O
by	O
the	O
vector	O
t	O
given	O
by	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
1.151	O
)	O
e	O
[	O
l	O
(	O
t	O
,	O
y	O
(	O
x	O
)	O
)	O
]	O
=	O
(	O
cid:6	O
)	O
y	O
(	O
x	O
)	O
−	O
t	O
(	O
cid:6	O
)	O
2p	O
(	O
x	O
,	O
t	O
)	O
dx	O
dt	O
.	O
using	O
the	O
calculus	B
of	I
variations	I
,	O
show	O
that	O
the	O
function	O
y	O
(	O
x	O
)	O
for	O
which	O
this	O
expected	O
loss	O
is	O
minimized	O
is	O
given	O
by	O
y	O
(	O
x	O
)	O
=	O
et	O
[	O
t|x	O
]	O
.	O
show	O
that	O
this	O
result	O
reduces	O
to	O
(	O
1.89	O
)	O
for	O
the	O
case	O
of	O
a	O
single	O
target	O
variable	O
t.	O
1.26	O
(	O
(	O
cid:1	O
)	O
)	O
by	O
expansion	O
of	O
the	O
square	O
in	O
(	O
1.151	O
)	O
,	O
derive	O
a	O
result	O
analogous	O
to	O
(	O
1.90	O
)	O
and	O
hence	O
show	O
that	O
the	O
function	O
y	O
(	O
x	O
)	O
that	O
minimizes	O
the	O
expected	O
squared	O
loss	O
for	O
the	O
case	O
of	O
a	O
vector	O
t	O
of	O
target	O
variables	O
is	O
again	O
given	O
by	O
the	O
conditional	B
expectation	I
of	O
t.	O
1.27	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
www	O
consider	O
the	O
expected	O
loss	O
for	O
regression	B
problems	O
under	O
the	O
lq	O
loss	B
function	I
given	O
by	O
(	O
1.91	O
)	O
.	O
write	O
down	O
the	O
condition	O
that	O
y	O
(	O
x	O
)	O
must	O
satisfy	O
in	O
order	O
to	O
minimize	O
e	O
[	O
lq	O
]	O
.	O
show	O
that	O
,	O
for	O
q	O
=	O
1	O
,	O
this	O
solution	O
represents	O
the	O
conditional	B
median	O
,	O
i.e.	O
,	O
the	O
function	O
y	O
(	O
x	O
)	O
such	O
that	O
the	O
probability	B
mass	O
for	O
t	O
<	O
y	O
(	O
x	O
)	O
is	O
the	O
same	O
as	O
for	O
t	O
(	O
cid:2	O
)	O
y	O
(	O
x	O
)	O
.	O
also	O
show	O
that	O
the	O
minimum	O
expected	O
lq	O
loss	O
for	O
q	O
→	O
0	O
is	O
given	O
by	O
the	O
conditional	B
mode	O
,	O
i.e.	O
,	O
by	O
the	O
function	O
y	O
(	O
x	O
)	O
equal	O
to	O
the	O
value	O
of	O
t	O
that	O
maximizes	O
p	O
(	O
t|x	O
)	O
for	O
each	O
x	O
.	O
1.28	O
(	O
(	O
cid:1	O
)	O
)	O
in	O
section	O
1.6	O
,	O
we	O
introduced	O
the	O
idea	O
of	O
entropy	B
h	O
(	O
x	O
)	O
as	O
the	O
information	O
gained	O
on	O
observing	O
the	O
value	O
of	O
a	O
random	O
variable	O
x	O
having	O
distribution	O
p	O
(	O
x	O
)	O
.	O
we	O
saw	O
that	O
,	O
for	O
independent	O
variables	O
x	O
and	O
y	O
for	O
which	O
p	O
(	O
x	O
,	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
,	O
the	O
entropy	B
functions	O
are	O
additive	O
,	O
so	O
that	O
h	O
(	O
x	O
,	O
y	O
)	O
=	O
h	O
(	O
x	O
)	O
+	O
h	O
(	O
y	O
)	O
.	O
in	O
this	O
exercise	O
,	O
we	O
derive	O
the	O
relation	O
between	O
h	O
and	O
p	O
in	O
the	O
form	O
of	O
a	O
function	O
h	O
(	O
p	O
)	O
.	O
first	O
show	O
that	O
h	O
(	O
p2	O
)	O
=	O
2h	O
(	O
p	O
)	O
,	O
and	O
hence	O
by	O
induction	O
that	O
h	O
(	O
pn	O
)	O
=	O
nh	O
(	O
p	O
)	O
where	O
n	O
is	O
a	O
positive	O
integer	O
.	O
hence	O
show	O
that	O
h	O
(	O
pn/m	O
)	O
=	O
(	O
n/m	O
)	O
h	O
(	O
p	O
)	O
where	O
m	O
is	O
also	O
a	O
positive	O
integer	O
.	O
this	O
implies	O
that	O
h	O
(	O
px	O
)	O
=	O
xh	O
(	O
p	O
)	O
where	O
x	O
is	O
a	O
positive	O
rational	O
number	O
,	O
and	O
hence	O
by	O
continuity	O
when	O
it	O
is	O
a	O
positive	O
real	O
number	O
.	O
finally	O
,	O
show	O
that	O
this	O
implies	O
h	O
(	O
p	O
)	O
must	O
take	O
the	O
form	O
h	O
(	O
p	O
)	O
∝	O
ln	O
p.	O
1.29	O
(	O
(	O
cid:1	O
)	O
)	O
www	O
consider	O
an	O
m-state	O
discrete	O
random	O
variable	O
x	O
,	O
and	O
use	O
jensen	O
’	O
s	O
in-	O
equality	O
in	O
the	O
form	O
(	O
1.115	O
)	O
to	O
show	O
that	O
the	O
entropy	B
of	O
its	O
distribution	O
p	O
(	O
x	O
)	O
satisﬁes	O
h	O
[	O
x	O
]	O
(	O
cid:1	O
)	O
ln	O
m.	O
1.30	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
evaluate	O
the	O
kullback-leibler	O
divergence	O
(	O
1.113	O
)	O
between	O
two	O
gaussians	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
x|µ	O
,	O
σ2	O
)	O
and	O
q	O
(	O
x	O
)	O
=	O
n	O
(	O
x|m	O
,	O
s2	O
)	O
.	O
table	O
1.3	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
y	O
)	O
for	O
two	O
binary	O
variables	O
x	O
and	O
y	O
used	O
in	O
exercise	O
1.39.	O
exercises	O
65	O
y	O
0	O
1/3	O
0	O
1	O
1/3	O
1/3	O
x	O
0	O
1	O
1.31	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
www	O
consider	O
two	O
variables	O
x	O
and	O
y	O
having	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
y	O
)	O
.	O
show	O
that	O
the	O
differential	B
entropy	I
of	O
this	O
pair	O
of	O
variables	O
satisﬁes	O
h	O
[	O
x	O
,	O
y	O
]	O
(	O
cid:1	O
)	O
h	O
[	O
x	O
]	O
+	O
h	O
[	O
y	O
]	O
(	O
1.152	O
)	O
with	O
equality	O
if	O
,	O
and	O
only	O
if	O
,	O
x	O
and	O
y	O
are	O
statistically	O
independent	B
.	O
1.32	O
(	O
(	O
cid:1	O
)	O
)	O
consider	O
a	O
vector	O
x	O
of	O
continuous	O
variables	O
with	O
distribution	O
p	O
(	O
x	O
)	O
and	O
corre-	O
sponding	O
entropy	B
h	O
[	O
x	O
]	O
.	O
suppose	O
that	O
we	O
make	O
a	O
nonsingular	O
linear	O
transformation	O
of	O
x	O
to	O
obtain	O
a	O
new	O
variable	O
y	O
=	O
ax	O
.	O
show	O
that	O
the	O
corresponding	O
entropy	B
is	O
given	O
by	O
h	O
[	O
y	O
]	O
=	O
h	O
[	O
x	O
]	O
+	O
ln|a|	O
where	O
|a|	O
denotes	O
the	O
determinant	O
of	O
a	O
.	O
1.33	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
suppose	O
that	O
the	O
conditional	B
entropy	I
h	O
[	O
y|x	O
]	O
between	O
two	O
discrete	O
random	O
variables	O
x	O
and	O
y	O
is	O
zero	O
.	O
show	O
that	O
,	O
for	O
all	O
values	O
of	O
x	O
such	O
that	O
p	O
(	O
x	O
)	O
>	O
0	O
,	O
the	O
variable	O
y	O
must	O
be	O
a	O
function	O
of	O
x	O
,	O
in	O
other	O
words	O
for	O
each	O
x	O
there	O
is	O
only	O
one	O
value	O
of	O
y	O
such	O
that	O
p	O
(	O
y|x	O
)	O
(	O
cid:2	O
)	O
=	O
0	O
.	O
1.34	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
www	O
use	O
the	O
calculus	B
of	I
variations	I
to	O
show	O
that	O
the	O
stationary	B
point	O
of	O
the	O
functional	B
(	O
1.108	O
)	O
is	O
given	O
by	O
(	O
1.108	O
)	O
.	O
then	O
use	O
the	O
constraints	O
(	O
1.105	O
)	O
,	O
(	O
1.106	O
)	O
,	O
and	O
(	O
1.107	O
)	O
to	O
eliminate	O
the	O
lagrange	O
multipliers	O
and	O
hence	O
show	O
that	O
the	O
maximum	O
entropy	O
solution	O
is	O
given	O
by	O
the	O
gaussian	O
(	O
1.109	O
)	O
.	O
1.35	O
(	O
(	O
cid:1	O
)	O
)	O
www	O
use	O
the	O
results	O
(	O
1.106	O
)	O
and	O
(	O
1.107	O
)	O
to	O
show	O
that	O
the	O
entropy	B
of	O
the	O
univariate	O
gaussian	O
(	O
1.109	O
)	O
is	O
given	O
by	O
(	O
1.110	O
)	O
.	O
1.36	O
(	O
(	O
cid:1	O
)	O
)	O
a	O
strictly	O
convex	B
function	I
is	O
deﬁned	O
as	O
one	O
for	O
which	O
every	O
chord	O
lies	O
above	O
the	O
function	O
.	O
show	O
that	O
this	O
is	O
equivalent	O
to	O
the	O
condition	O
that	O
the	O
second	O
derivative	O
of	O
the	O
function	O
be	O
positive	O
.	O
1.37	O
(	O
(	O
cid:1	O
)	O
)	O
using	O
the	O
deﬁnition	O
(	O
1.111	O
)	O
together	O
with	O
the	O
product	B
rule	I
of	I
probability	I
,	O
prove	O
the	O
result	O
(	O
1.112	O
)	O
.	O
1.38	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
www	O
using	O
proof	O
by	O
induction	O
,	O
show	O
that	O
the	O
inequality	O
(	O
1.114	O
)	O
for	O
convex	O
functions	O
implies	O
the	O
result	O
(	O
1.115	O
)	O
.	O
1.39	O
(	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
consider	O
two	O
binary	O
variables	O
x	O
and	O
y	O
having	O
the	O
joint	O
distribution	O
given	O
in	O
table	O
1.3.	O
evaluate	O
the	O
following	O
quantities	O
(	O
a	O
)	O
h	O
[	O
x	O
]	O
(	O
b	O
)	O
h	O
[	O
y	O
]	O
(	O
c	O
)	O
h	O
[	O
y|x	O
]	O
(	O
d	O
)	O
h	O
[	O
x|y	O
]	O
(	O
e	O
)	O
h	O
[	O
x	O
,	O
y	O
]	O
(	O
f	O
)	O
i	O
[	O
x	O
,	O
y	O
]	O
.	O
draw	O
a	O
diagram	O
to	O
show	O
the	O
relationship	O
between	O
these	O
various	O
quantities	O
.	O
66	O
1.	O
introduction	O
1.40	O
(	O
(	O
cid:1	O
)	O
)	O
by	O
applying	O
jensen	O
’	O
s	O
inequality	O
(	O
1.115	O
)	O
with	O
f	O
(	O
x	O
)	O
=	O
ln	O
x	O
,	O
show	O
that	O
the	O
arith-	O
metic	O
mean	B
of	O
a	O
set	O
of	O
real	O
numbers	O
is	O
never	O
less	O
than	O
their	O
geometrical	O
mean	B
.	O
1.41	O
(	O
(	O
cid:1	O
)	O
)	O
www	O
using	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
,	O
show	O
that	O
the	O
mutual	B
information	I
i	O
(	O
x	O
,	O
y	O
)	O
satisﬁes	O
the	O
relation	O
(	O
1.121	O
)	O
.	O
2	O
probability	B
distributions	O
in	O
chapter	O
1	O
,	O
we	O
emphasized	O
the	O
central	O
role	O
played	O
by	O
probability	B
theory	O
in	O
the	O
solution	O
of	O
pattern	O
recognition	O
problems	O
.	O
we	O
turn	O
now	O
to	O
an	O
exploration	B
of	O
some	O
particular	O
examples	O
of	O
probability	B
distributions	O
and	O
their	O
properties	O
.	O
as	O
well	O
as	O
be-	O
ing	O
of	O
great	O
interest	O
in	O
their	O
own	O
right	O
,	O
these	O
distributions	O
can	O
form	O
building	O
blocks	O
for	O
more	O
complex	O
models	O
and	O
will	O
be	O
used	O
extensively	O
throughout	O
the	O
book	O
.	O
the	O
distributions	O
introduced	O
in	O
this	O
chapter	O
will	O
also	O
serve	O
another	O
important	O
purpose	O
,	O
namely	O
to	O
provide	O
us	O
with	O
the	O
opportunity	O
to	O
discuss	O
some	O
key	O
statistical	O
concepts	O
,	O
such	O
as	O
bayesian	O
inference	B
,	O
in	O
the	O
context	O
of	O
simple	O
models	O
before	O
we	O
encounter	O
them	O
in	O
more	O
complex	O
situations	O
in	O
later	O
chapters	O
.	O
one	O
role	O
for	O
the	O
distributions	O
discussed	O
in	O
this	O
chapter	O
is	O
to	O
model	O
the	O
prob-	O
ability	O
distribution	O
p	O
(	O
x	O
)	O
of	O
a	O
random	O
variable	O
x	O
,	O
given	O
a	O
ﬁnite	O
set	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
of	O
observations	O
.	O
this	O
problem	O
is	O
known	O
as	O
density	B
estimation	I
.	O
for	O
the	O
purposes	O
of	O
this	O
chapter	O
,	O
we	O
shall	O
assume	O
that	O
the	O
data	O
points	O
are	O
independent	B
and	O
identically	O
distributed	O
.	O
it	O
should	O
be	O
emphasized	O
that	O
the	O
problem	O
of	O
density	B
estimation	I
is	O
fun-	O
67	O
68	O
2.	O
probability	B
distributions	O
damentally	O
ill-posed	O
,	O
because	O
there	O
are	O
inﬁnitely	O
many	O
probability	B
distributions	O
that	O
could	O
have	O
given	O
rise	O
to	O
the	O
observed	O
ﬁnite	O
data	O
set	O
.	O
indeed	O
,	O
any	O
distribution	O
p	O
(	O
x	O
)	O
that	O
is	O
nonzero	O
at	O
each	O
of	O
the	O
data	O
points	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
is	O
a	O
potential	O
candidate	O
.	O
the	O
issue	O
of	O
choosing	O
an	O
appropriate	O
distribution	O
relates	O
to	O
the	O
problem	O
of	O
model	O
selec-	O
tion	O
that	O
has	O
already	O
been	O
encountered	O
in	O
the	O
context	O
of	O
polynomial	B
curve	I
ﬁtting	I
in	O
chapter	O
1	O
and	O
that	O
is	O
a	O
central	O
issue	O
in	O
pattern	O
recognition	O
.	O
we	O
begin	O
by	O
considering	O
the	O
binomial	O
and	O
multinomial	O
distributions	O
for	O
discrete	O
random	O
variables	O
and	O
the	O
gaussian	O
distribution	O
for	O
continuous	O
random	O
variables	O
.	O
these	O
are	O
speciﬁc	O
examples	O
of	O
parametric	O
distributions	O
,	O
so-called	O
because	O
they	O
are	O
governed	O
by	O
a	O
small	O
number	O
of	O
adaptive	O
parameters	O
,	O
such	O
as	O
the	O
mean	B
and	O
variance	B
in	O
the	O
case	O
of	O
a	O
gaussian	O
for	O
example	O
.	O
to	O
apply	O
such	O
models	O
to	O
the	O
problem	O
of	O
density	B
estimation	I
,	O
we	O
need	O
a	O
procedure	O
for	O
determining	O
suitable	O
values	O
for	O
the	O
parameters	O
,	O
given	O
an	O
observed	O
data	O
set	O
.	O
in	O
a	O
frequentist	B
treatment	O
,	O
we	O
choose	O
speciﬁc	O
values	O
for	O
the	O
parameters	O
by	O
optimizing	O
some	O
criterion	O
,	O
such	O
as	O
the	O
likelihood	B
function	I
.	O
by	O
contrast	O
,	O
in	O
a	O
bayesian	O
treatment	O
we	O
introduce	O
prior	B
distributions	O
over	O
the	O
parameters	O
and	O
then	O
use	O
bayes	O
’	O
theorem	O
to	O
compute	O
the	O
corresponding	O
posterior	O
distribution	O
given	O
the	O
observed	O
data	O
.	O
we	O
shall	O
see	O
that	O
an	O
important	O
role	O
is	O
played	O
by	O
conjugate	B
priors	O
,	O
that	O
lead	O
to	O
posterior	O
distributions	O
having	O
the	O
same	O
functional	B
form	O
as	O
the	O
prior	B
,	O
and	O
that	O
there-	O
fore	O
lead	O
to	O
a	O
greatly	O
simpliﬁed	O
bayesian	O
analysis	O
.	O
for	O
example	O
,	O
the	O
conjugate	B
prior	I
for	O
the	O
parameters	O
of	O
the	O
multinomial	B
distribution	I
is	O
called	O
the	O
dirichlet	O
distribution	O
,	O
while	O
the	O
conjugate	B
prior	I
for	O
the	O
mean	B
of	O
a	O
gaussian	O
is	O
another	O
gaussian	O
.	O
all	O
of	O
these	O
distributions	O
are	O
examples	O
of	O
the	O
exponential	B
family	I
of	O
distributions	O
,	O
which	O
possess	O
a	O
number	O
of	O
important	O
properties	O
,	O
and	O
which	O
will	O
be	O
discussed	O
in	O
some	O
detail	O
.	O
one	O
limitation	O
of	O
the	O
parametric	O
approach	O
is	O
that	O
it	O
assumes	O
a	O
speciﬁc	O
functional	B
form	O
for	O
the	O
distribution	O
,	O
which	O
may	O
turn	O
out	O
to	O
be	O
inappropriate	O
for	O
a	O
particular	O
application	O
.	O
an	O
alternative	O
approach	O
is	O
given	O
by	O
nonparametric	O
density	O
estimation	O
methods	O
in	O
which	O
the	O
form	O
of	O
the	O
distribution	O
typically	O
depends	O
on	O
the	O
size	O
of	O
the	O
data	O
set	O
.	O
such	O
models	O
still	O
contain	O
parameters	O
,	O
but	O
these	O
control	O
the	O
model	O
complexity	O
rather	O
than	O
the	O
form	O
of	O
the	O
distribution	O
.	O
we	O
end	O
this	O
chapter	O
by	O
considering	O
three	O
nonparametric	B
methods	I
based	O
respectively	O
on	O
histograms	O
,	O
nearest-neighbours	O
,	O
and	O
kernels	O
.	O
2.1.	O
binary	O
variables	O
we	O
begin	O
by	O
considering	O
a	O
single	O
binary	O
random	O
variable	O
x	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
for	O
example	O
,	O
x	O
might	O
describe	O
the	O
outcome	O
of	O
ﬂipping	O
a	O
coin	O
,	O
with	O
x	O
=	O
1	O
representing	O
‘	O
heads	O
’	O
,	O
and	O
x	O
=	O
0	O
representing	O
‘	O
tails	O
’	O
.	O
we	O
can	O
imagine	O
that	O
this	O
is	O
a	O
damaged	O
coin	O
so	O
that	O
the	O
probability	B
of	O
landing	O
heads	O
is	O
not	O
necessarily	O
the	O
same	O
as	O
that	O
of	O
landing	O
tails	O
.	O
the	O
probability	B
of	O
x	O
=	O
1	O
will	O
be	O
denoted	O
by	O
the	O
parameter	O
µ	O
so	O
that	O
p	O
(	O
x	O
=	O
1|µ	O
)	O
=	O
µ	O
(	O
2.1	O
)	O
2.1.	O
binary	O
variables	O
69	O
where	O
0	O
(	O
cid:1	O
)	O
µ	O
(	O
cid:1	O
)	O
1	O
,	O
from	O
which	O
it	O
follows	O
that	O
p	O
(	O
x	O
=	O
0|µ	O
)	O
=	O
1	O
−	O
µ.	O
the	O
probability	B
distribution	O
over	O
x	O
can	O
therefore	O
be	O
written	O
in	O
the	O
form	O
bern	O
(	O
x|µ	O
)	O
=	O
µx	O
(	O
1	O
−	O
µ	O
)	O
1−x	O
(	O
2.2	O
)	O
exercise	O
2.1	O
which	O
is	O
known	O
as	O
the	O
bernoulli	O
distribution	O
.	O
it	O
is	O
easily	O
veriﬁed	O
that	O
this	O
distribution	O
is	O
normalized	O
and	O
that	O
it	O
has	O
mean	B
and	O
variance	B
given	O
by	O
e	O
[	O
x	O
]	O
=	O
µ	O
var	O
[	O
x	O
]	O
=	O
µ	O
(	O
1	O
−	O
µ	O
)	O
.	O
(	O
2.3	O
)	O
(	O
2.4	O
)	O
now	O
suppose	O
we	O
have	O
a	O
data	O
set	O
d	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
of	O
observed	O
values	O
of	O
x.	O
we	O
can	O
construct	O
the	O
likelihood	B
function	I
,	O
which	O
is	O
a	O
function	O
of	O
µ	O
,	O
on	O
the	O
assumption	O
that	O
the	O
observations	O
are	O
drawn	O
independently	O
from	O
p	O
(	O
x|µ	O
)	O
,	O
so	O
that	O
n	O
(	O
cid:14	O
)	O
n	O
(	O
cid:14	O
)	O
p	O
(	O
d|µ	O
)	O
=	O
p	O
(	O
xn|µ	O
)	O
=	O
µxn	O
(	O
1	O
−	O
µ	O
)	O
1−xn	O
.	O
(	O
2.5	O
)	O
n=1	O
n=1	O
in	O
a	O
frequentist	B
setting	O
,	O
we	O
can	O
estimate	O
a	O
value	O
for	O
µ	O
by	O
maximizing	O
the	O
likelihood	B
function	I
,	O
or	O
equivalently	O
by	O
maximizing	O
the	O
logarithm	O
of	O
the	O
likelihood	O
.	O
in	O
the	O
case	O
of	O
the	O
bernoulli	O
distribution	O
,	O
the	O
log	O
likelihood	O
function	O
is	O
given	O
by	O
ln	O
p	O
(	O
d|µ	O
)	O
=	O
ln	O
p	O
(	O
xn|µ	O
)	O
=	O
{	O
xn	O
ln	O
µ	O
+	O
(	O
1	O
−	O
xn	O
)	O
ln	O
(	O
1	O
−	O
µ	O
)	O
}	O
.	O
(	O
2.6	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
n	O
(	O
cid:2	O
)	O
(	O
cid:5	O
)	O
n=1	O
n	O
(	O
cid:2	O
)	O
section	O
2.4	O
at	O
this	O
point	O
,	O
it	O
is	O
worth	O
noting	O
that	O
the	O
log	O
likelihood	O
function	O
depends	O
on	O
the	O
n	O
observations	O
xn	O
only	O
through	O
their	O
sum	O
n	O
xn	O
.	O
this	O
sum	O
provides	O
an	O
example	O
of	O
a	O
sufﬁcient	O
statistic	O
for	O
the	O
data	O
under	O
this	O
distribution	O
,	O
and	O
we	O
shall	O
study	O
the	O
impor-	O
tant	O
role	O
of	O
sufﬁcient	B
statistics	I
in	O
some	O
detail	O
.	O
if	O
we	O
set	O
the	O
derivative	B
of	O
ln	O
p	O
(	O
d|µ	O
)	O
with	O
respect	O
to	O
µ	O
equal	O
to	O
zero	O
,	O
we	O
obtain	O
the	O
maximum	B
likelihood	I
estimator	O
µml	O
=	O
1	O
n	O
xn	O
n=1	O
(	O
2.7	O
)	O
jacob	O
bernoulli	O
1654–1705	O
jacob	O
bernoulli	O
,	O
also	O
known	O
as	O
jacques	O
or	O
james	O
bernoulli	O
,	O
was	O
a	O
swiss	O
mathematician	O
and	O
was	O
the	O
ﬁrst	O
of	O
many	O
in	O
the	O
bernoulli	O
family	O
to	O
pursue	O
a	O
career	O
in	O
science	O
and	O
mathematics	O
.	O
although	O
compelled	O
to	O
study	O
philosophy	O
and	O
theology	O
against	O
his	O
will	O
by	O
his	O
parents	O
,	O
he	O
travelled	O
extensively	O
after	O
graduating	O
in	O
order	O
to	O
meet	O
with	O
many	O
of	O
the	O
leading	O
scientists	O
of	O
his	O
time	O
,	O
including	O
boyle	O
and	O
hooke	O
in	O
england	O
.	O
when	O
he	O
returned	O
to	O
switzerland	O
,	O
he	O
taught	O
mechanics	O
and	O
became	O
professor	O
of	O
mathematics	O
at	O
basel	O
in	O
1687.	O
unfortunately	O
,	O
rivalry	O
between	O
jacob	O
and	O
his	O
younger	O
brother	O
johann	O
turned	O
an	O
initially	O
productive	O
collabora-	O
tion	O
into	O
a	O
bitter	O
and	O
public	O
dispute	O
.	O
jacob	O
’	O
s	O
most	O
sig-	O
niﬁcant	O
contributions	O
to	O
mathematics	O
appeared	O
in	O
the	O
artofconjecture	O
published	O
in	O
1713	O
,	O
eight	O
years	O
after	O
his	O
death	O
,	O
which	O
deals	O
with	O
topics	O
in	O
probability	B
the-	O
ory	O
including	O
what	O
has	O
become	O
known	O
as	O
the	O
bernoulli	O
distribution	O
.	O
70	O
2.	O
probability	B
distributions	O
figure	O
2.1	O
histogram	O
plot	O
of	O
the	O
binomial	O
dis-	O
tribution	O
(	O
2.9	O
)	O
as	O
a	O
function	O
of	O
m	O
for	O
n	O
=	O
10	O
and	O
µ	O
=	O
0.25	O
.	O
0.3	O
0.2	O
0.1	O
0	O
0	O
1	O
2	O
3	O
4	O
5	O
m	O
6	O
7	O
8	O
9	O
10	O
which	O
is	O
also	O
known	O
as	O
the	O
sample	B
mean	I
.	O
if	O
we	O
denote	O
the	O
number	O
of	O
observations	O
of	O
x	O
=	O
1	O
(	O
heads	O
)	O
within	O
this	O
data	O
set	O
by	O
m	O
,	O
then	O
we	O
can	O
write	O
(	O
2.7	O
)	O
in	O
the	O
form	O
µml	O
=	O
m	O
n	O
(	O
2.8	O
)	O
so	O
that	O
the	O
probability	B
of	O
landing	O
heads	O
is	O
given	O
,	O
in	O
this	O
maximum	B
likelihood	I
frame-	O
work	O
,	O
by	O
the	O
fraction	O
of	O
observations	O
of	O
heads	O
in	O
the	O
data	O
set	O
.	O
now	O
suppose	O
we	O
ﬂip	O
a	O
coin	O
,	O
say	O
,	O
3	O
times	O
and	O
happen	O
to	O
observe	O
3	O
heads	O
.	O
then	O
n	O
=	O
m	O
=	O
3	O
and	O
µml	O
=	O
1.	O
in	O
this	O
case	O
,	O
the	O
maximum	B
likelihood	I
result	O
would	O
predict	O
that	O
all	O
future	O
observations	O
should	O
give	O
heads	O
.	O
common	O
sense	O
tells	O
us	O
that	O
this	O
is	O
unreasonable	O
,	O
and	O
in	O
fact	O
this	O
is	O
an	O
extreme	O
example	O
of	O
the	O
over-ﬁtting	B
associ-	O
ated	O
with	O
maximum	B
likelihood	I
.	O
we	O
shall	O
see	O
shortly	O
how	O
to	O
arrive	O
at	O
more	O
sensible	O
conclusions	O
through	O
the	O
introduction	O
of	O
a	O
prior	B
distribution	O
over	O
µ.	O
we	O
can	O
also	O
work	O
out	O
the	O
distribution	O
of	O
the	O
number	O
m	O
of	O
observations	O
of	O
x	O
=	O
1	O
,	O
given	O
that	O
the	O
data	O
set	O
has	O
size	O
n.	O
this	O
is	O
called	O
the	O
binomial	B
distribution	I
,	O
and	O
from	O
(	O
2.5	O
)	O
we	O
see	O
that	O
it	O
is	O
proportional	O
to	O
µm	O
(	O
1	O
−	O
µ	O
)	O
n−m	O
.	O
in	O
order	O
to	O
obtain	O
the	O
normalization	O
coefﬁcient	O
we	O
note	O
that	O
out	O
of	O
n	O
coin	O
ﬂips	O
,	O
we	O
have	O
to	O
add	O
up	O
all	O
of	O
the	O
possible	O
ways	O
of	O
obtaining	O
m	O
heads	O
,	O
so	O
that	O
the	O
binomial	B
distribution	I
can	O
be	O
written	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
bin	O
(	O
m|n	O
,	O
µ	O
)	O
=	O
µm	O
(	O
1	O
−	O
µ	O
)	O
n−m	O
where	O
n	O
m	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
n	O
m	O
≡	O
n	O
!	O
(	O
n	O
−	O
m	O
)	O
!	O
m	O
!	O
(	O
2.9	O
)	O
(	O
2.10	O
)	O
exercise	O
2.3	O
is	O
the	O
number	O
of	O
ways	O
of	O
choosing	O
m	O
objects	O
out	O
of	O
a	O
total	O
of	O
n	O
identical	O
objects	O
.	O
figure	O
2.1	O
shows	O
a	O
plot	O
of	O
the	O
binomial	B
distribution	I
for	O
n	O
=	O
10	O
and	O
µ	O
=	O
0.25.	O
the	O
mean	B
and	O
variance	B
of	O
the	O
binomial	B
distribution	I
can	O
be	O
found	O
by	O
using	O
the	O
result	O
of	O
exercise	O
1.10	O
,	O
which	O
shows	O
that	O
for	O
independent	O
events	O
the	O
mean	B
of	O
the	O
sum	O
is	O
the	O
sum	O
of	O
the	O
means	O
,	O
and	O
the	O
variance	B
of	O
the	O
sum	O
is	O
the	O
sum	O
of	O
the	O
variances	O
.	O
because	O
m	O
=	O
x1	O
+	O
.	O
.	O
.	O
+	O
xn	O
,	O
and	O
for	O
each	O
observation	O
the	O
mean	B
and	O
variance	B
are	O
2.1.	O
binary	O
variables	O
71	O
given	O
by	O
(	O
2.3	O
)	O
and	O
(	O
2.4	O
)	O
,	O
respectively	O
,	O
we	O
have	O
e	O
[	O
m	O
]	O
≡	O
n	O
(	O
cid:2	O
)	O
var	O
[	O
m	O
]	O
≡	O
n	O
(	O
cid:2	O
)	O
mbin	O
(	O
m|n	O
,	O
µ	O
)	O
=	O
n	O
µ	O
m=0	O
(	O
m	O
−	O
e	O
[	O
m	O
]	O
)	O
2	O
bin	O
(	O
m|n	O
,	O
µ	O
)	O
=	O
n	O
µ	O
(	O
1	O
−	O
µ	O
)	O
.	O
(	O
2.11	O
)	O
(	O
2.12	O
)	O
exercise	O
2.4	O
these	O
results	O
can	O
also	O
be	O
proved	O
directly	O
using	O
calculus	O
.	O
m=0	O
2.1.1	O
the	O
beta	B
distribution	I
we	O
have	O
seen	O
in	O
(	O
2.8	O
)	O
that	O
the	O
maximum	B
likelihood	I
setting	O
for	O
the	O
parameter	O
µ	O
in	O
the	O
bernoulli	O
distribution	O
,	O
and	O
hence	O
in	O
the	O
binomial	B
distribution	I
,	O
is	O
given	O
by	O
the	O
fraction	O
of	O
the	O
observations	O
in	O
the	O
data	O
set	O
having	O
x	O
=	O
1.	O
as	O
we	O
have	O
already	O
noted	O
,	O
this	O
can	O
give	O
severely	O
over-ﬁtted	O
results	O
for	O
small	O
data	O
sets	O
.	O
in	O
order	O
to	O
develop	O
a	O
bayesian	O
treatment	O
for	O
this	O
problem	O
,	O
we	O
need	O
to	O
introduce	O
a	O
prior	B
distribution	O
p	O
(	O
µ	O
)	O
over	O
the	O
parameter	O
µ.	O
here	O
we	O
consider	O
a	O
form	O
of	O
prior	B
distribution	O
that	O
has	O
a	O
simple	O
interpretation	O
as	O
well	O
as	O
some	O
useful	O
analytical	O
properties	O
.	O
to	O
motivate	O
this	O
prior	B
,	O
we	O
note	O
that	O
the	O
likelihood	B
function	I
takes	O
the	O
form	O
of	O
the	O
product	O
of	O
factors	O
of	O
the	O
form	O
µx	O
(	O
1	O
−	O
µ	O
)	O
1−x	O
.	O
if	O
we	O
choose	O
a	O
prior	B
to	O
be	O
proportional	O
to	O
powers	O
of	O
µ	O
and	O
(	O
1	O
−	O
µ	O
)	O
,	O
then	O
the	O
posterior	O
distribution	O
,	O
which	O
is	O
proportional	O
to	O
the	O
product	O
of	O
the	O
prior	B
and	O
the	O
likelihood	B
function	I
,	O
will	O
have	O
the	O
same	O
functional	B
form	O
as	O
the	O
prior	B
.	O
this	O
property	O
is	O
called	O
conjugacy	O
and	O
we	O
will	O
see	O
several	O
examples	O
of	O
it	O
later	O
in	O
this	O
chapter	O
.	O
we	O
therefore	O
choose	O
a	O
prior	B
,	O
called	O
the	O
beta	B
distribution	I
,	O
given	O
by	O
exercise	O
2.5	O
beta	O
(	O
µ|a	O
,	O
b	O
)	O
=	O
γ	O
(	O
a	O
+	O
b	O
)	O
γ	O
(	O
a	O
)	O
γ	O
(	O
b	O
)	O
µa−1	O
(	O
1	O
−	O
µ	O
)	O
b−1	O
(	O
2.13	O
)	O
where	O
γ	O
(	O
x	O
)	O
is	O
the	O
gamma	B
function	I
deﬁned	O
by	O
(	O
1.141	O
)	O
,	O
and	O
the	O
coefﬁcient	O
in	O
(	O
2.13	O
)	O
ensures	O
that	O
the	O
beta	B
distribution	I
is	O
normalized	O
,	O
so	O
that	O
beta	O
(	O
µ|a	O
,	O
b	O
)	O
dµ	O
=	O
1	O
.	O
(	O
2.14	O
)	O
(	O
cid:6	O
)	O
1	O
exercise	O
2.6	O
the	O
mean	B
and	O
variance	B
of	O
the	O
beta	B
distribution	I
are	O
given	O
by	O
0	O
e	O
[	O
µ	O
]	O
=	O
var	O
[	O
µ	O
]	O
=	O
a	O
a	O
+	O
b	O
(	O
a	O
+	O
b	O
)	O
2	O
(	O
a	O
+	O
b	O
+	O
1	O
)	O
.	O
ab	O
(	O
2.15	O
)	O
(	O
2.16	O
)	O
the	O
parameters	O
a	O
and	O
b	O
are	O
often	O
called	O
hyperparameters	O
because	O
they	O
control	O
the	O
distribution	O
of	O
the	O
parameter	O
µ.	O
figure	O
2.2	O
shows	O
plots	O
of	O
the	O
beta	B
distribution	I
for	O
various	O
values	O
of	O
the	O
hyperparameters	O
.	O
the	O
posterior	O
distribution	O
of	O
µ	O
is	O
now	O
obtained	O
by	O
multiplying	O
the	O
beta	O
prior	O
(	O
2.13	O
)	O
by	O
the	O
binomial	O
likelihood	O
function	O
(	O
2.9	O
)	O
and	O
normalizing	O
.	O
keeping	O
only	O
the	O
factors	O
that	O
depend	O
on	O
µ	O
,	O
we	O
see	O
that	O
this	O
posterior	O
distribution	O
has	O
the	O
form	O
p	O
(	O
µ|m	O
,	O
l	O
,	O
a	O
,	O
b	O
)	O
∝	O
µm+a−1	O
(	O
1	O
−	O
µ	O
)	O
l+b−1	O
(	O
2.17	O
)	O
72	O
2.	O
probability	B
distributions	O
a	O
=	O
0.1	O
b	O
=	O
0.1	O
a	O
=	O
2	O
b	O
=	O
3	O
3	O
2	O
1	O
0	O
0	O
3	O
2	O
1	O
0	O
0	O
0.5	O
µ	O
1	O
a	O
=	O
1	O
b	O
=	O
1	O
0	O
0.5	O
µ	O
1	O
a	O
=	O
8	O
b	O
=	O
4	O
3	O
2	O
1	O
0	O
3	O
2	O
1	O
0.5	O
µ	O
1	O
0	O
0	O
0.5	O
µ	O
1	O
figure	O
2.2	O
plots	O
of	O
the	O
beta	B
distribution	I
beta	O
(	O
µ|a	O
,	O
b	O
)	O
given	O
by	O
(	O
2.13	O
)	O
as	O
a	O
function	O
of	O
µ	O
for	O
various	O
values	O
of	O
the	O
hyperparameters	O
a	O
and	O
b.	O
where	O
l	O
=	O
n	O
−	O
m	O
,	O
and	O
therefore	O
corresponds	O
to	O
the	O
number	O
of	O
‘	O
tails	O
’	O
in	O
the	O
coin	O
example	O
.	O
we	O
see	O
that	O
(	O
2.17	O
)	O
has	O
the	O
same	O
functional	B
dependence	O
on	O
µ	O
as	O
the	O
prior	B
distribution	O
,	O
reﬂecting	O
the	O
conjugacy	O
properties	O
of	O
the	O
prior	B
with	O
respect	O
to	O
the	O
like-	O
lihood	O
function	O
.	O
indeed	O
,	O
it	O
is	O
simply	O
another	O
beta	B
distribution	I
,	O
and	O
its	O
normalization	O
coefﬁcient	O
can	O
therefore	O
be	O
obtained	O
by	O
comparison	O
with	O
(	O
2.13	O
)	O
to	O
give	O
p	O
(	O
µ|m	O
,	O
l	O
,	O
a	O
,	O
b	O
)	O
=	O
γ	O
(	O
m	O
+	O
a	O
+	O
l	O
+	O
b	O
)	O
γ	O
(	O
m	O
+	O
a	O
)	O
γ	O
(	O
l	O
+	O
b	O
)	O
µm+a−1	O
(	O
1	O
−	O
µ	O
)	O
l+b−1	O
.	O
(	O
2.18	O
)	O
we	O
see	O
that	O
the	O
effect	O
of	O
observing	O
a	O
data	O
set	O
of	O
m	O
observations	O
of	O
x	O
=	O
1	O
and	O
l	O
observations	O
of	O
x	O
=	O
0	O
has	O
been	O
to	O
increase	O
the	O
value	O
of	O
a	O
by	O
m	O
,	O
and	O
the	O
value	O
of	O
b	O
by	O
l	O
,	O
in	O
going	O
from	O
the	O
prior	B
distribution	O
to	O
the	O
posterior	O
distribution	O
.	O
this	O
allows	O
us	O
to	O
provide	O
a	O
simple	O
interpretation	O
of	O
the	O
hyperparameters	O
a	O
and	O
b	O
in	O
the	O
prior	B
as	O
an	O
effective	B
number	I
of	I
observations	I
of	O
x	O
=	O
1	O
and	O
x	O
=	O
0	O
,	O
respectively	O
.	O
note	O
that	O
a	O
and	O
b	O
need	O
not	O
be	O
integers	O
.	O
furthermore	O
,	O
the	O
posterior	O
distribution	O
can	O
act	O
as	O
the	O
prior	B
if	O
we	O
subsequently	O
observe	O
additional	O
data	O
.	O
to	O
see	O
this	O
,	O
we	O
can	O
imagine	O
taking	O
observations	O
one	O
at	O
a	O
time	O
and	O
after	O
each	O
observation	O
updating	O
the	O
current	O
posterior	O
prior	O
2	O
1	O
0	O
0	O
2	O
1	O
0	O
0	O
0.5	O
µ	O
1	O
likelihood	B
function	I
0.5	O
µ	O
1	O
2.1.	O
binary	O
variables	O
73	O
posterior	O
2	O
1	O
0	O
0	O
0.5	O
µ	O
1	O
figure	O
2.3	O
illustration	O
of	O
one	O
step	O
of	O
sequential	O
bayesian	O
inference	B
.	O
the	O
prior	B
is	O
given	O
by	O
a	O
beta	B
distribution	I
with	O
parameters	O
a	O
=	O
2	O
,	O
b	O
=	O
2	O
,	O
and	O
the	O
likelihood	B
function	I
,	O
given	O
by	O
(	O
2.9	O
)	O
with	O
n	O
=	O
m	O
=	O
1	O
,	O
corresponds	O
to	O
a	O
single	O
observation	O
of	O
x	O
=	O
1	O
,	O
so	O
that	O
the	O
posterior	O
is	O
given	O
by	O
a	O
beta	B
distribution	I
with	O
parameters	O
a	O
=	O
3	O
,	O
b	O
=	O
2.	O
distribution	O
by	O
multiplying	O
by	O
the	O
likelihood	B
function	I
for	O
the	O
new	O
observation	O
and	O
then	O
normalizing	O
to	O
obtain	O
the	O
new	O
,	O
revised	O
posterior	O
distribution	O
.	O
at	O
each	O
stage	O
,	O
the	O
posterior	O
is	O
a	O
beta	B
distribution	I
with	O
some	O
total	O
number	O
of	O
(	O
prior	B
and	O
actual	O
)	O
observed	O
values	O
for	O
x	O
=	O
1	O
and	O
x	O
=	O
0	O
given	O
by	O
the	O
parameters	O
a	O
and	O
b.	O
incorporation	O
of	O
an	O
additional	O
observation	O
of	O
x	O
=	O
1	O
simply	O
corresponds	O
to	O
incrementing	O
the	O
value	O
of	O
a	O
by	O
1	O
,	O
whereas	O
for	O
an	O
observation	O
of	O
x	O
=	O
0	O
we	O
increment	O
b	O
by	O
1.	O
figure	O
2.3	O
illustrates	O
one	O
step	O
in	O
this	O
process	O
.	O
section	O
2.3.5	O
we	O
see	O
that	O
this	O
sequential	O
approach	O
to	O
learning	B
arises	O
naturally	O
when	O
we	O
adopt	O
a	O
bayesian	O
viewpoint	O
.	O
it	O
is	O
independent	B
of	O
the	O
choice	O
of	O
prior	B
and	O
of	O
the	O
likelihood	B
function	I
and	O
depends	O
only	O
on	O
the	O
assumption	O
of	O
i.i.d	O
.	O
data	O
.	O
sequential	O
methods	O
make	O
use	O
of	O
observations	O
one	O
at	O
a	O
time	O
,	O
or	O
in	O
small	O
batches	O
,	O
and	O
then	O
discard	O
them	O
before	O
the	O
next	O
observations	O
are	O
used	O
.	O
they	O
can	O
be	O
used	O
,	O
for	O
example	O
,	O
in	O
real-time	O
learning	B
scenarios	O
where	O
a	O
steady	O
stream	O
of	O
data	O
is	O
arriving	O
,	O
and	O
predictions	O
must	O
be	O
made	O
before	O
all	O
of	O
the	O
data	O
is	O
seen	O
.	O
because	O
they	O
do	O
not	O
require	O
the	O
whole	O
data	O
set	O
to	O
be	O
stored	O
or	O
loaded	O
into	O
memory	O
,	O
sequential	O
methods	O
are	O
also	O
useful	O
for	O
large	O
data	O
sets	O
.	O
maximum	B
likelihood	I
methods	O
can	O
also	O
be	O
cast	O
into	O
a	O
sequential	O
framework	O
.	O
if	O
our	O
goal	O
is	O
to	O
predict	O
,	O
as	O
best	O
we	O
can	O
,	O
the	O
outcome	O
of	O
the	O
next	O
trial	O
,	O
then	O
we	O
must	O
evaluate	O
the	O
predictive	B
distribution	I
of	O
x	O
,	O
given	O
the	O
observed	O
data	O
set	O
d.	O
from	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
,	O
this	O
takes	O
the	O
form	O
p	O
(	O
x	O
=	O
1|d	O
)	O
=	O
p	O
(	O
x	O
=	O
1|µ	O
)	O
p	O
(	O
µ|d	O
)	O
dµ	O
=	O
µp	O
(	O
µ|d	O
)	O
dµ	O
=	O
e	O
[	O
µ|d	O
]	O
.	O
(	O
cid:6	O
)	O
1	O
(	O
cid:6	O
)	O
1	O
0	O
0	O
(	O
2.19	O
)	O
using	O
the	O
result	O
(	O
2.18	O
)	O
for	O
the	O
posterior	O
distribution	O
p	O
(	O
µ|d	O
)	O
,	O
together	O
with	O
the	O
result	O
(	O
2.15	O
)	O
for	O
the	O
mean	B
of	O
the	O
beta	B
distribution	I
,	O
we	O
obtain	O
m	O
+	O
a	O
p	O
(	O
x	O
=	O
1|d	O
)	O
=	O
(	O
2.20	O
)	O
m	O
+	O
a	O
+	O
l	O
+	O
b	O
which	O
has	O
a	O
simple	O
interpretation	O
as	O
the	O
total	O
fraction	O
of	O
observations	O
(	O
both	O
real	O
ob-	O
servations	O
and	O
ﬁctitious	O
prior	B
observations	O
)	O
that	O
correspond	O
to	O
x	O
=	O
1.	O
note	O
that	O
in	O
the	O
limit	O
of	O
an	O
inﬁnitely	O
large	O
data	O
set	O
m	O
,	O
l	O
→	O
∞	O
the	O
result	O
(	O
2.20	O
)	O
reduces	O
to	O
the	O
maximum	B
likelihood	I
result	O
(	O
2.8	O
)	O
.	O
as	O
we	O
shall	O
see	O
,	O
it	O
is	O
a	O
very	O
general	O
property	O
that	O
the	O
bayesian	O
and	O
maximum	B
likelihood	I
results	O
will	O
agree	O
in	O
the	O
limit	O
of	O
an	O
inﬁnitely	O
74	O
2.	O
probability	B
distributions	O
exercise	O
2.7	O
exercise	O
2.8	O
large	O
data	O
set	O
.	O
for	O
a	O
ﬁnite	O
data	O
set	O
,	O
the	O
posterior	O
mean	O
for	O
µ	O
always	O
lies	O
between	O
the	O
prior	B
mean	O
and	O
the	O
maximum	B
likelihood	I
estimate	O
for	O
µ	O
corresponding	O
to	O
the	O
relative	B
frequencies	O
of	O
events	O
given	O
by	O
(	O
2.7	O
)	O
.	O
from	O
figure	O
2.2	O
,	O
we	O
see	O
that	O
as	O
the	O
number	O
of	O
observations	O
increases	O
,	O
so	O
the	O
posterior	O
distribution	O
becomes	O
more	O
sharply	O
peaked	O
.	O
this	O
can	O
also	O
be	O
seen	O
from	O
the	O
result	O
(	O
2.16	O
)	O
for	O
the	O
variance	B
of	O
the	O
beta	B
distribution	I
,	O
in	O
which	O
we	O
see	O
that	O
the	O
variance	B
goes	O
to	O
zero	O
for	O
a	O
→	O
∞	O
or	O
b	O
→	O
∞	O
.	O
in	O
fact	O
,	O
we	O
might	O
wonder	O
whether	O
it	O
is	O
a	O
general	O
property	O
of	O
bayesian	O
learning	B
that	O
,	O
as	O
we	O
observe	O
more	O
and	O
more	O
data	O
,	O
the	O
uncertainty	O
represented	O
by	O
the	O
posterior	O
distribution	O
will	O
steadily	O
decrease	O
.	O
to	O
address	O
this	O
,	O
we	O
can	O
take	O
a	O
frequentist	B
view	O
of	O
bayesian	O
learning	B
and	O
show	O
that	O
,	O
on	O
average	O
,	O
such	O
a	O
property	O
does	O
indeed	O
hold	O
.	O
consider	O
a	O
general	O
bayesian	O
inference	B
problem	O
for	O
a	O
parameter	O
θ	O
for	O
which	O
we	O
have	O
observed	O
a	O
data	O
set	O
d	O
,	O
de-	O
scribed	O
by	O
the	O
joint	O
distribution	O
p	O
(	O
θ	O
,	O
d	O
)	O
.	O
the	O
following	O
result	O
where	O
eθ	O
[	O
θ	O
]	O
≡	O
ed	O
[	O
eθ	O
[	O
θ|d	O
]	O
]	O
≡	O
(	O
2.21	O
)	O
(	O
2.22	O
)	O
(	O
2.23	O
)	O
(	O
cid:13	O
)	O
p	O
(	O
d	O
)	O
dd	O
θp	O
(	O
θ|d	O
)	O
dθ	O
eθ	O
[	O
θ	O
]	O
=	O
ed	O
[	O
eθ	O
[	O
θ|d	O
]	O
]	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:12	O
)	O
(	O
cid:6	O
)	O
p	O
(	O
θ	O
)	O
θ	O
dθ	O
says	O
that	O
the	O
posterior	O
mean	O
of	O
θ	O
,	O
averaged	O
over	O
the	O
distribution	O
generating	O
the	O
data	O
,	O
is	O
equal	O
to	O
the	O
prior	B
mean	O
of	O
θ.	O
similarly	O
,	O
we	O
can	O
show	O
that	O
varθ	O
[	O
θ	O
]	O
=	O
ed	O
[	O
varθ	O
[	O
θ|d	O
]	O
]	O
+	O
vard	O
[	O
eθ	O
[	O
θ|d	O
]	O
]	O
.	O
(	O
2.24	O
)	O
the	O
term	O
on	O
the	O
left-hand	O
side	O
of	O
(	O
2.24	O
)	O
is	O
the	O
prior	B
variance	O
of	O
θ.	O
on	O
the	O
right-	O
hand	O
side	O
,	O
the	O
ﬁrst	O
term	O
is	O
the	O
average	O
posterior	O
variance	O
of	O
θ	O
,	O
and	O
the	O
second	O
term	O
measures	O
the	O
variance	B
in	O
the	O
posterior	O
mean	O
of	O
θ.	O
because	O
this	O
variance	B
is	O
a	O
positive	O
quantity	O
,	O
this	O
result	O
shows	O
that	O
,	O
on	O
average	O
,	O
the	O
posterior	O
variance	O
of	O
θ	O
is	O
smaller	O
than	O
the	O
prior	B
variance	O
.	O
the	O
reduction	O
in	O
variance	B
is	O
greater	O
if	O
the	O
variance	B
in	O
the	O
posterior	O
mean	O
is	O
greater	O
.	O
note	O
,	O
however	O
,	O
that	O
this	O
result	O
only	O
holds	O
on	O
average	O
,	O
and	O
that	O
for	O
a	O
particular	O
observed	O
data	O
set	O
it	O
is	O
possible	O
for	O
the	O
posterior	O
variance	O
to	O
be	O
larger	O
than	O
the	O
prior	B
variance	O
.	O
2.2.	O
multinomial	O
variables	O
binary	O
variables	O
can	O
be	O
used	O
to	O
describe	O
quantities	O
that	O
can	O
take	O
one	O
of	O
two	O
possible	O
values	O
.	O
often	O
,	O
however	O
,	O
we	O
encounter	O
discrete	O
variables	O
that	O
can	O
take	O
on	O
one	O
of	O
k	O
possible	O
mutually	O
exclusive	O
states	O
.	O
although	O
there	O
are	O
various	O
alternative	O
ways	O
to	O
express	O
such	O
variables	O
,	O
we	O
shall	O
see	O
shortly	O
that	O
a	O
particularly	O
convenient	O
represen-	O
tation	O
is	O
the	O
1-of-k	O
scheme	O
in	O
which	O
the	O
variable	O
is	O
represented	O
by	O
a	O
k-dimensional	O
vector	O
x	O
in	O
which	O
one	O
of	O
the	O
elements	O
xk	O
equals	O
1	O
,	O
and	O
all	O
remaining	O
elements	O
equal	O
2.2.	O
multinomial	O
variables	O
75	O
k	O
(	O
cid:14	O
)	O
k=1	O
k	O
(	O
cid:2	O
)	O
k=1	O
k	O
(	O
cid:14	O
)	O
(	O
cid:2	O
)	O
k=1	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
x	O
n	O
(	O
cid:14	O
)	O
k	O
(	O
cid:14	O
)	O
n=1	O
k=1	O
0.	O
so	O
,	O
for	O
instance	O
if	O
we	O
have	O
a	O
variable	O
that	O
can	O
take	O
k	O
=	O
6	O
states	O
and	O
a	O
particular	O
observation	O
of	O
the	O
variable	O
happens	O
to	O
correspond	O
to	O
the	O
state	O
where	O
x3	O
=	O
1	O
,	O
then	O
x	O
will	O
be	O
represented	O
by	O
note	O
that	O
such	O
vectors	O
satisfy	O
by	O
the	O
parameter	O
µk	O
,	O
then	O
the	O
distribution	O
of	O
x	O
is	O
given	O
x	O
=	O
(	O
0	O
,	O
0	O
,	O
1	O
,	O
0	O
,	O
0	O
,	O
0	O
)	O
t.	O
(	O
2.25	O
)	O
k=1	O
xk	O
=	O
1.	O
if	O
we	O
denote	O
the	O
probability	B
of	O
xk	O
=	O
1	O
(	O
cid:5	O
)	O
k	O
p	O
(	O
x|µ	O
)	O
=	O
µxk	O
k	O
(	O
2.26	O
)	O
(	O
cid:5	O
)	O
where	O
µ	O
=	O
(	O
µ1	O
,	O
.	O
.	O
.	O
,	O
µk	O
)	O
t	O
,	O
and	O
the	O
parameters	O
µk	O
are	O
constrained	O
to	O
satisfy	O
µk	O
(	O
cid:2	O
)	O
0	O
k	O
µk	O
=	O
1	O
,	O
because	O
they	O
represent	O
probabilities	O
.	O
the	O
distribution	O
(	O
2.26	O
)	O
can	O
be	O
and	O
regarded	O
as	O
a	O
generalization	B
of	O
the	O
bernoulli	O
distribution	O
to	O
more	O
than	O
two	O
outcomes	O
.	O
it	O
is	O
easily	O
seen	O
that	O
the	O
distribution	O
is	O
normalized	O
p	O
(	O
x|µ	O
)	O
=	O
µk	O
=	O
1	O
(	O
2.27	O
)	O
and	O
that	O
e	O
[	O
x|µ	O
]	O
=	O
(	O
2.28	O
)	O
now	O
consider	O
a	O
data	O
set	O
d	O
of	O
n	O
independent	B
observations	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
.	O
the	O
x	O
p	O
(	O
x|µ	O
)	O
x	O
=	O
(	O
µ1	O
,	O
.	O
.	O
.	O
,	O
µm	O
)	O
t	O
=	O
µ.	O
corresponding	O
likelihood	B
function	I
takes	O
the	O
form	O
p	O
(	O
d|µ	O
)	O
=	O
µxnk	O
k	O
=	O
p	O
(	O
µ	O
k	O
n	O
xnk	O
)	O
=	O
µmk	O
k	O
.	O
(	O
2.29	O
)	O
k	O
(	O
cid:14	O
)	O
k=1	O
we	O
see	O
that	O
the	O
likelihood	B
function	I
depends	O
on	O
the	O
n	O
data	O
points	O
only	O
through	O
the	O
k	O
quantities	O
mk	O
=	O
xnk	O
n	O
(	O
2.30	O
)	O
which	O
represent	O
the	O
number	O
of	O
observations	O
of	O
xk	O
=	O
1.	O
these	O
are	O
called	O
the	O
sufﬁcient	B
statistics	I
for	O
this	O
distribution	O
.	O
in	O
order	O
to	O
ﬁnd	O
the	O
maximum	B
likelihood	I
solution	O
for	O
µ	O
,	O
we	O
need	O
to	O
maximize	O
ln	O
p	O
(	O
d|µ	O
)	O
with	O
respect	O
to	O
µk	O
taking	O
account	O
of	O
the	O
constraint	O
that	O
the	O
µk	O
must	O
sum	O
to	O
one	O
.	O
this	O
can	O
be	O
achieved	O
using	O
a	O
lagrange	O
multiplier	O
λ	O
and	O
maximizing	O
k	O
(	O
cid:2	O
)	O
(	O
cid:22	O
)	O
k	O
(	O
cid:2	O
)	O
(	O
cid:23	O
)	O
mk	O
ln	O
µk	O
+	O
λ	O
µk	O
−	O
1	O
.	O
k=1	O
k=1	O
setting	O
the	O
derivative	B
of	O
(	O
2.31	O
)	O
with	O
respect	O
to	O
µk	O
to	O
zero	O
,	O
we	O
obtain	O
µk	O
=	O
−mk/λ	O
.	O
(	O
2.31	O
)	O
(	O
2.32	O
)	O
section	O
2.4	O
appendix	O
e	O
76	O
2.	O
probability	B
distributions	O
(	O
cid:5	O
)	O
we	O
can	O
solve	O
for	O
the	O
lagrange	O
multiplier	O
λ	O
by	O
substituting	O
(	O
2.32	O
)	O
into	O
the	O
constraint	O
k	O
µk	O
=	O
1	O
to	O
give	O
λ	O
=	O
−n	O
.	O
thus	O
we	O
obtain	O
the	O
maximum	B
likelihood	I
solution	O
in	O
the	O
form	O
k	O
=	O
mk	O
µml	O
n	O
(	O
2.33	O
)	O
which	O
is	O
the	O
fraction	O
of	O
the	O
n	O
observations	O
for	O
which	O
xk	O
=	O
1.	O
we	O
can	O
consider	O
the	O
joint	O
distribution	O
of	O
the	O
quantities	O
m1	O
,	O
.	O
.	O
.	O
,	O
mk	O
,	O
conditioned	O
on	O
the	O
parameters	O
µ	O
and	O
on	O
the	O
total	O
number	O
n	O
of	O
observations	O
.	O
from	O
(	O
2.29	O
)	O
this	O
takes	O
the	O
form	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
k	O
(	O
cid:14	O
)	O
mult	O
(	O
m1	O
,	O
m2	O
,	O
.	O
.	O
.	O
,	O
mk|µ	O
,	O
n	O
)	O
=	O
n	O
m1m2	O
.	O
.	O
.	O
mk	O
k=1	O
µmk	O
k	O
(	O
2.34	O
)	O
which	O
is	O
known	O
as	O
the	O
multinomial	B
distribution	I
.	O
the	O
normalization	O
coefﬁcient	O
is	O
the	O
number	O
of	O
ways	O
of	O
partitioning	O
n	O
objects	O
into	O
k	O
groups	O
of	O
size	O
m1	O
,	O
.	O
.	O
.	O
,	O
mk	O
and	O
is	O
given	O
by	O
(	O
cid:15	O
)	O
n	O
m1m2	O
.	O
.	O
.	O
mk	O
n	O
!	O
=	O
m1	O
!	O
m2	O
!	O
.	O
.	O
.	O
mk	O
!	O
.	O
note	O
that	O
the	O
variables	O
mk	O
are	O
subject	O
to	O
the	O
constraint	O
(	O
cid:16	O
)	O
k	O
(	O
cid:2	O
)	O
k=1	O
mk	O
=	O
n.	O
(	O
2.35	O
)	O
(	O
2.36	O
)	O
2.2.1	O
the	O
dirichlet	O
distribution	O
we	O
now	O
introduce	O
a	O
family	O
of	O
prior	B
distributions	O
for	O
the	O
parameters	O
{	O
µk	O
}	O
of	O
the	O
multinomial	B
distribution	I
(	O
2.34	O
)	O
.	O
by	O
inspection	O
of	O
the	O
form	O
of	O
the	O
multinomial	B
distribution	I
,	O
we	O
see	O
that	O
the	O
conjugate	B
prior	I
is	O
given	O
by	O
µαk−1	O
k	O
(	O
2.37	O
)	O
p	O
(	O
µ|α	O
)	O
∝	O
k	O
(	O
cid:14	O
)	O
k=1	O
(	O
cid:5	O
)	O
where	O
0	O
(	O
cid:1	O
)	O
µk	O
(	O
cid:1	O
)	O
1	O
and	O
k	O
µk	O
=	O
1.	O
here	O
α1	O
,	O
.	O
.	O
.	O
,	O
αk	O
are	O
the	O
parameters	O
of	O
the	O
distribution	O
,	O
and	O
α	O
denotes	O
(	O
α1	O
,	O
.	O
.	O
.	O
,	O
αk	O
)	O
t.	O
note	O
that	O
,	O
because	O
of	O
the	O
summation	O
constraint	O
,	O
the	O
distribution	O
over	O
the	O
space	O
of	O
the	O
{	O
µk	O
}	O
is	O
conﬁned	O
to	O
a	O
simplex	B
of	O
dimensionality	O
k	O
−	O
1	O
,	O
as	O
illustrated	O
for	O
k	O
=	O
3	O
in	O
figure	O
2.4.	O
exercise	O
2.9	O
the	O
normalized	O
form	O
for	O
this	O
distribution	O
is	O
by	O
dir	O
(	O
µ|α	O
)	O
=	O
γ	O
(	O
α0	O
)	O
γ	O
(	O
α1	O
)	O
···	O
γ	O
(	O
αk	O
)	O
µαk−1	O
k	O
(	O
2.38	O
)	O
k	O
(	O
cid:14	O
)	O
k=1	O
which	O
is	O
called	O
the	O
dirichlet	O
distribution	O
.	O
here	O
γ	O
(	O
x	O
)	O
is	O
the	O
gamma	B
function	I
deﬁned	O
by	O
(	O
1.141	O
)	O
while	O
k	O
(	O
cid:2	O
)	O
α0	O
=	O
αk	O
.	O
k=1	O
(	O
2.39	O
)	O
2.2.	O
multinomial	O
variables	O
77	O
figure	O
2.4	O
the	O
dirichlet	O
distribution	O
over	O
three	O
variables	O
µ1	O
,	O
µ2	O
,	O
µ3	O
is	O
conﬁned	O
to	O
a	O
simplex	B
(	O
a	O
bounded	O
linear	O
manifold	O
)	O
of	O
the	O
form	O
shown	O
,	O
as	O
a	O
consequence	O
of	O
the	O
constraints	O
0	O
(	O
cid:1	O
)	O
µk	O
(	O
cid:1	O
)	O
1	O
and	O
k	O
µk	O
=	O
1.	O
p	O
µ2	O
µ3	O
µ1	O
plots	O
of	O
the	O
dirichlet	O
distribution	O
over	O
the	O
simplex	B
,	O
for	O
various	O
settings	O
of	O
the	O
param-	O
eters	O
αk	O
,	O
are	O
shown	O
in	O
figure	O
2.5.	O
posterior	O
distribution	O
for	O
the	O
parameters	O
{	O
µk	O
}	O
in	O
the	O
form	O
multiplying	O
the	O
prior	B
(	O
2.38	O
)	O
by	O
the	O
likelihood	B
function	I
(	O
2.34	O
)	O
,	O
we	O
obtain	O
the	O
p	O
(	O
µ|d	O
,	O
α	O
)	O
∝	O
p	O
(	O
d|µ	O
)	O
p	O
(	O
µ|α	O
)	O
∝	O
k	O
(	O
cid:14	O
)	O
k=1	O
µαk+mk−1	O
k	O
.	O
(	O
2.40	O
)	O
we	O
see	O
that	O
the	O
posterior	O
distribution	O
again	O
takes	O
the	O
form	O
of	O
a	O
dirichlet	O
distribution	O
,	O
conﬁrming	O
that	O
the	O
dirichlet	O
is	O
indeed	O
a	O
conjugate	B
prior	I
for	O
the	O
multinomial	O
.	O
this	O
allows	O
us	O
to	O
determine	O
the	O
normalization	O
coefﬁcient	O
by	O
comparison	O
with	O
(	O
2.38	O
)	O
so	O
that	O
p	O
(	O
µ|d	O
,	O
α	O
)	O
=	O
dir	O
(	O
µ|α	O
+	O
m	O
)	O
=	O
γ	O
(	O
α0	O
+	O
n	O
)	O
γ	O
(	O
α1	O
+	O
m1	O
)	O
···	O
γ	O
(	O
αk	O
+	O
mk	O
)	O
k	O
(	O
cid:14	O
)	O
k=1	O
µαk+mk−1	O
k	O
(	O
2.41	O
)	O
where	O
we	O
have	O
denoted	O
m	O
=	O
(	O
m1	O
,	O
.	O
.	O
.	O
,	O
mk	O
)	O
t.	O
as	O
for	O
the	O
case	O
of	O
the	O
binomial	B
distribution	I
with	O
its	O
beta	O
prior	O
,	O
we	O
can	O
interpret	O
the	O
parameters	O
αk	O
of	O
the	O
dirichlet	O
prior	B
as	O
an	O
effective	B
number	I
of	I
observations	I
of	O
xk	O
=	O
1.	O
note	O
that	O
two-state	O
quantities	O
can	O
either	O
be	O
represented	O
as	O
binary	O
variables	O
and	O
lejeune	O
dirichlet	O
1805–1859	O
johann	O
peter	O
gustav	O
lejeune	O
dirichlet	O
was	O
a	O
modest	O
and	O
re-	O
served	O
mathematician	O
who	O
made	O
contributions	O
in	O
number	O
theory	B
,	O
me-	O
chanics	O
,	O
and	O
astronomy	O
,	O
and	O
who	O
gave	O
the	O
ﬁrst	O
rigorous	O
analysis	O
of	O
fourier	O
series	O
.	O
his	O
family	O
originated	O
from	O
richelet	O
in	O
belgium	O
,	O
and	O
the	O
name	O
lejeune	O
dirichlet	O
comes	O
from	O
‘	O
le	O
jeune	O
de	O
richelet	O
’	O
(	O
the	O
young	O
person	O
from	O
richelet	O
)	O
.	O
dirichlet	O
’	O
s	O
ﬁrst	O
paper	O
,	O
which	O
was	O
published	O
in	O
1825	O
,	O
brought	O
him	O
instant	O
fame	O
.	O
it	O
concerned	O
fer-	O
mat	O
’	O
s	O
last	O
theorem	O
,	O
which	O
claims	O
that	O
there	O
are	O
no	O
positive	O
integer	O
solutions	O
to	O
xn	O
+	O
yn	O
=	O
zn	O
for	O
n	O
>	O
2.	O
dirichlet	O
gave	O
a	O
partial	O
proof	O
for	O
the	O
case	O
n	O
=	O
5	O
,	O
which	O
was	O
sent	O
to	O
legendre	O
for	O
review	O
and	O
who	O
in	O
turn	O
com-	O
pleted	O
the	O
proof	O
.	O
later	O
,	O
dirichlet	O
gave	O
a	O
complete	O
proof	O
for	O
n	O
=	O
14	O
,	O
although	O
a	O
full	O
proof	O
of	O
fermat	O
’	O
s	O
last	O
theo-	O
rem	O
for	O
arbitrary	O
n	O
had	O
to	O
wait	O
until	O
the	O
work	O
of	O
andrew	O
wiles	O
in	O
the	O
closing	O
years	O
of	O
the	O
20th	O
century	O
.	O
78	O
2.	O
probability	B
distributions	O
figure	O
2.5	O
plots	O
of	O
the	O
dirichlet	O
distribution	O
over	O
three	O
variables	O
,	O
where	O
the	O
two	O
horizontal	O
axes	O
are	O
coordinates	O
in	O
the	O
plane	O
of	O
the	O
simplex	B
and	O
the	O
vertical	O
axis	O
corresponds	O
to	O
the	O
value	O
of	O
the	O
density	B
.	O
here	O
{	O
αk	O
}	O
=	O
0.1	O
on	O
the	O
left	O
plot	O
,	O
{	O
αk	O
}	O
=	O
1	O
in	O
the	O
centre	O
plot	O
,	O
and	O
{	O
αk	O
}	O
=	O
10	O
in	O
the	O
right	O
plot	O
.	O
modelled	O
using	O
the	O
binomial	B
distribution	I
(	O
2.9	O
)	O
or	O
as	O
1-of-2	O
variables	O
and	O
modelled	O
using	O
the	O
multinomial	B
distribution	I
(	O
2.34	O
)	O
with	O
k	O
=	O
2	O
.	O
2.3.	O
the	O
gaussian	O
distribution	O
the	O
gaussian	O
,	O
also	O
known	O
as	O
the	O
normal	B
distribution	I
,	O
is	O
a	O
widely	O
used	O
model	O
for	O
the	O
distribution	O
of	O
continuous	O
variables	O
.	O
in	O
the	O
case	O
of	O
a	O
single	O
variable	O
x	O
,	O
the	O
gaussian	O
distribution	O
can	O
be	O
written	O
in	O
the	O
form	O
1	O
(	O
cid:1	O
)	O
(	O
cid:2	O
)	O
n	O
(	O
x|µ	O
,	O
σ2	O
)	O
=	O
(	O
2.42	O
)	O
−	O
1	O
2σ2	O
(	O
x	O
−	O
µ	O
)	O
2	O
(	O
2πσ2	O
)	O
1/2	O
exp	O
(	O
cid:1	O
)	O
n	O
(	O
x|µ	O
,	O
σ	O
)	O
=	O
where	O
µ	O
is	O
the	O
mean	B
and	O
σ2	O
is	O
the	O
variance	B
.	O
for	O
a	O
d-dimensional	O
vector	O
x	O
,	O
the	O
multivariate	O
gaussian	O
distribution	O
takes	O
the	O
form	O
−1	O
2	O
(	O
2.43	O
)	O
where	O
µ	O
is	O
a	O
d-dimensional	O
mean	B
vector	O
,	O
σ	O
is	O
a	O
d	O
×	O
d	O
covariance	B
matrix	I
,	O
and	O
|σ|	O
denotes	O
the	O
determinant	O
of	O
σ	O
.	O
(	O
x	O
−	O
µ	O
)	O
tς−1	O
(	O
x	O
−	O
µ	O
)	O
(	O
2π	O
)	O
d/2	O
1	O
1	O
|σ|1/2	O
exp	O
(	O
cid:2	O
)	O
section	O
1.6	O
exercise	O
2.14	O
the	O
gaussian	O
distribution	O
arises	O
in	O
many	O
different	O
contexts	O
and	O
can	O
be	O
motivated	O
from	O
a	O
variety	O
of	O
different	O
perspectives	O
.	O
for	O
example	O
,	O
we	O
have	O
already	O
seen	O
that	O
for	O
a	O
single	O
real	O
variable	O
,	O
the	O
distribution	O
that	O
maximizes	O
the	O
entropy	B
is	O
the	O
gaussian	O
.	O
this	O
property	O
applies	O
also	O
to	O
the	O
multivariate	O
gaussian	O
.	O
another	O
situation	O
in	O
which	O
the	O
gaussian	O
distribution	O
arises	O
is	O
when	O
we	O
consider	O
the	O
sum	O
of	O
multiple	O
random	O
variables	O
.	O
the	O
central	B
limit	I
theorem	I
(	O
due	O
to	O
laplace	O
)	O
tells	O
us	O
that	O
,	O
subject	O
to	O
certain	O
mild	O
conditions	O
,	O
the	O
sum	O
of	O
a	O
set	O
of	O
random	O
variables	O
,	O
which	O
is	O
of	O
course	O
itself	O
a	O
random	O
variable	O
,	O
has	O
a	O
distribution	O
that	O
becomes	O
increas-	O
ingly	O
gaussian	O
as	O
the	O
number	O
of	O
terms	O
in	O
the	O
sum	O
increases	O
(	O
walker	O
,	O
1969	O
)	O
.	O
we	O
can	O
2.3.	O
the	O
gaussian	O
distribution	O
79	O
n	O
=	O
1	O
3	O
2	O
1	O
0	O
0	O
n	O
=	O
2	O
3	O
2	O
1	O
0	O
0	O
n	O
=	O
10	O
3	O
2	O
1	O
0	O
0	O
0.5	O
1	O
0.5	O
1	O
0.5	O
1	O
figure	O
2.6	O
histogram	O
plots	O
of	O
the	O
mean	B
of	O
n	O
uniformly	O
distributed	O
numbers	O
for	O
various	O
values	O
of	O
n.	O
we	O
observe	O
that	O
as	O
n	O
increases	O
,	O
the	O
distribution	O
tends	O
towards	O
a	O
gaussian	O
.	O
illustrate	O
this	O
by	O
considering	O
n	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
each	O
of	O
which	O
has	O
a	O
uniform	B
distribution	I
over	O
the	O
interval	O
[	O
0	O
,	O
1	O
]	O
and	O
then	O
considering	O
the	O
distribution	O
of	O
the	O
mean	B
(	O
x1	O
+	O
···	O
+	O
xn	O
)	O
/n	O
.	O
for	O
large	O
n	O
,	O
this	O
distribution	O
tends	O
to	O
a	O
gaussian	O
,	O
as	O
illustrated	O
in	O
practice	O
,	O
the	O
convergence	O
to	O
a	O
gaussian	O
as	O
n	O
increases	O
can	O
be	O
in	O
figure	O
2.6.	O
very	O
rapid	O
.	O
one	O
consequence	O
of	O
this	O
result	O
is	O
that	O
the	O
binomial	B
distribution	I
(	O
2.9	O
)	O
,	O
which	O
is	O
a	O
distribution	O
over	O
m	O
deﬁned	O
by	O
the	O
sum	O
of	O
n	O
observations	O
of	O
the	O
random	O
binary	O
variable	O
x	O
,	O
will	O
tend	O
to	O
a	O
gaussian	O
as	O
n	O
→	O
∞	O
(	O
see	O
figure	O
2.1	O
for	O
the	O
case	O
of	O
n	O
=	O
10	O
)	O
.	O
the	O
gaussian	O
distribution	O
has	O
many	O
important	O
analytical	O
properties	O
,	O
and	O
we	O
shall	O
consider	O
several	O
of	O
these	O
in	O
detail	O
.	O
as	O
a	O
result	O
,	O
this	O
section	O
will	O
be	O
rather	O
more	O
tech-	O
nically	O
involved	O
than	O
some	O
of	O
the	O
earlier	O
sections	O
,	O
and	O
will	O
require	O
familiarity	O
with	O
various	O
matrix	O
identities	O
.	O
however	O
,	O
we	O
strongly	O
encourage	O
the	O
reader	O
to	O
become	O
pro-	O
ﬁcient	O
in	O
manipulating	O
gaussian	O
distributions	O
using	O
the	O
techniques	O
presented	O
here	O
as	O
this	O
will	O
prove	O
invaluable	O
in	O
understanding	O
the	O
more	O
complex	O
models	O
presented	O
in	O
later	O
chapters	O
.	O
we	O
begin	O
by	O
considering	O
the	O
geometrical	O
form	O
of	O
the	O
gaussian	O
distribution	O
.	O
the	O
appendix	O
c	O
carl	O
friedrich	O
gauss	O
1777–1855	O
it	O
is	O
said	O
that	O
when	O
gauss	O
went	O
to	O
elementary	O
school	O
at	O
age	O
7	O
,	O
his	O
teacher	O
b¨uttner	O
,	O
trying	O
to	O
keep	O
the	O
class	O
occupied	O
,	O
asked	O
the	O
pupils	O
to	O
sum	O
the	O
integers	O
from	O
1	O
to	O
100.	O
to	O
the	O
teacher	O
’	O
s	O
amazement	O
,	O
gauss	O
arrived	O
at	O
the	O
answer	O
in	O
a	O
matter	O
of	O
moments	O
by	O
noting	O
that	O
the	O
sum	O
can	O
be	O
represented	O
as	O
50	O
pairs	O
(	O
1	O
+	O
100	O
,	O
2+99	O
,	O
etc	O
.	O
)	O
each	O
of	O
which	O
added	O
to	O
101	O
,	O
giving	O
the	O
an-	O
swer	O
5,050.	O
it	O
is	O
now	O
believed	O
that	O
the	O
problem	O
which	O
was	O
actually	O
set	O
was	O
of	O
the	O
same	O
form	O
but	O
somewhat	O
harder	O
in	O
that	O
the	O
sequence	O
had	O
a	O
larger	O
starting	O
value	O
and	O
a	O
larger	O
increment	O
.	O
gauss	O
was	O
a	O
german	O
math-	O
ematician	O
and	O
scientist	O
with	O
a	O
reputation	O
for	O
being	O
a	O
hard-working	O
perfectionist	O
.	O
one	O
of	O
his	O
many	O
contribu-	O
tions	O
was	O
to	O
show	O
that	O
least	O
squares	O
can	O
be	O
derived	O
under	O
the	O
assumption	O
of	O
normally	O
distributed	O
errors	O
.	O
he	O
also	O
created	O
an	O
early	O
formulation	O
of	O
non-euclidean	O
geometry	O
(	O
a	O
self-consistent	O
geometrical	O
theory	B
that	O
vi-	O
olates	O
the	O
axioms	O
of	O
euclid	O
)	O
but	O
was	O
reluctant	O
to	O
dis-	O
cuss	O
it	O
openly	O
for	O
fear	O
that	O
his	O
reputation	O
might	O
suffer	O
if	O
it	O
were	O
seen	O
that	O
he	O
believed	O
in	O
such	O
a	O
geometry	O
.	O
at	O
one	O
point	O
,	O
gauss	O
was	O
asked	O
to	O
conduct	O
a	O
geodetic	O
survey	O
of	O
the	O
state	O
of	O
hanover	O
,	O
which	O
led	O
to	O
his	O
for-	O
mulation	O
of	O
the	O
normal	B
distribution	I
,	O
now	O
also	O
known	O
as	O
the	O
gaussian	O
.	O
after	O
his	O
death	O
,	O
a	O
study	O
of	O
his	O
di-	O
aries	O
revealed	O
that	O
he	O
had	O
discovered	O
several	O
impor-	O
tant	O
mathematical	O
results	O
years	O
or	O
even	O
decades	O
be-	O
fore	O
they	O
were	O
published	O
by	O
others	O
.	O
80	O
2.	O
probability	B
distributions	O
functional	B
dependence	O
of	O
the	O
gaussian	O
on	O
x	O
is	O
through	O
the	O
quadratic	O
form	O
∆2	O
=	O
(	O
x	O
−	O
µ	O
)	O
tς	O
−1	O
(	O
x	O
−	O
µ	O
)	O
(	O
2.44	O
)	O
which	O
appears	O
in	O
the	O
exponent	O
.	O
the	O
quantity	O
∆	O
is	O
called	O
the	O
mahalanobis	O
distance	O
from	O
µ	O
to	O
x	O
and	O
reduces	O
to	O
the	O
euclidean	O
distance	O
when	O
σ	O
is	O
the	O
identity	O
matrix	O
.	O
the	O
gaussian	O
distribution	O
will	O
be	O
constant	O
on	O
surfaces	O
in	O
x-space	O
for	O
which	O
this	O
quadratic	O
form	O
is	O
constant	O
.	O
first	O
of	O
all	O
,	O
we	O
note	O
that	O
the	O
matrix	O
σ	O
can	O
be	O
taken	O
to	O
be	O
symmetric	O
,	O
without	O
loss	O
of	O
generality	O
,	O
because	O
any	O
antisymmetric	O
component	O
would	O
disappear	O
from	O
the	O
exponent	O
.	O
now	O
consider	O
the	O
eigenvector	O
equation	O
for	O
the	O
covariance	B
matrix	I
σui	O
=	O
λiui	O
(	O
2.45	O
)	O
exercise	O
2.17	O
exercise	O
2.18	O
where	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
d.	O
because	O
σ	O
is	O
a	O
real	O
,	O
symmetric	O
matrix	O
its	O
eigenvalues	O
will	O
be	O
real	O
,	O
and	O
its	O
eigenvectors	O
can	O
be	O
chosen	O
to	O
form	O
an	O
orthonormal	O
set	O
,	O
so	O
that	O
where	O
iij	O
is	O
the	O
i	O
,	O
j	O
element	O
of	O
the	O
identity	O
matrix	O
and	O
satisﬁes	O
ut	O
i	O
uj	O
=	O
iij	O
(	O
cid:12	O
)	O
iij	O
=	O
if	O
i	O
=	O
j	O
1	O
,	O
0	O
,	O
otherwise	O
.	O
exercise	O
2.19	O
the	O
covariance	B
matrix	I
σ	O
can	O
be	O
expressed	O
as	O
an	O
expansion	O
in	O
terms	O
of	O
its	O
eigenvec-	O
tors	O
in	O
the	O
form	O
(	O
2.46	O
)	O
(	O
2.47	O
)	O
(	O
2.48	O
)	O
(	O
2.49	O
)	O
(	O
2.50	O
)	O
d	O
(	O
cid:2	O
)	O
i=1	O
d	O
(	O
cid:2	O
)	O
i=1	O
σ	O
=	O
λiuiut	O
i	O
−1	O
=	O
σ	O
1	O
λi	O
uiut	O
i	O
.	O
and	O
similarly	O
the	O
inverse	B
covariance	O
matrix	O
σ	O
−1	O
can	O
be	O
expressed	O
as	O
substituting	O
(	O
2.49	O
)	O
into	O
(	O
2.44	O
)	O
,	O
the	O
quadratic	O
form	O
becomes	O
d	O
(	O
cid:2	O
)	O
i=1	O
y2	O
i	O
λi	O
∆2	O
=	O
where	O
we	O
have	O
deﬁned	O
(	O
2.51	O
)	O
we	O
can	O
interpret	O
{	O
yi	O
}	O
as	O
a	O
new	O
coordinate	O
system	O
deﬁned	O
by	O
the	O
orthonormal	O
vectors	O
ui	O
that	O
are	O
shifted	O
and	O
rotated	O
with	O
respect	O
to	O
the	O
original	O
xi	O
coordinates	O
.	O
forming	O
the	O
vector	O
y	O
=	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yd	O
)	O
t	O
,	O
we	O
have	O
i	O
(	O
x	O
−	O
µ	O
)	O
.	O
yi	O
=	O
ut	O
y	O
=	O
u	O
(	O
x	O
−	O
µ	O
)	O
(	O
2.52	O
)	O
2.3.	O
the	O
gaussian	O
distribution	O
81	O
x2	O
figure	O
2.7	O
the	O
red	O
curve	O
shows	O
the	O
ellip-	O
tical	O
surface	O
of	O
constant	O
proba-	O
bility	O
density	B
for	O
a	O
gaussian	O
in	O
a	O
two-dimensional	O
space	O
x	O
=	O
(	O
x1	O
,	O
x2	O
)	O
on	O
which	O
the	O
density	B
is	O
exp	O
(	O
−1/2	O
)	O
of	O
its	O
value	O
at	O
x	O
=	O
µ.	O
the	O
major	O
axes	O
of	O
the	O
ellipse	O
are	O
deﬁned	O
by	O
the	O
eigenvectors	O
ui	O
of	O
the	O
covari-	O
ance	O
matrix	O
,	O
with	O
correspond-	O
ing	O
eigenvalues	O
λi	O
.	O
1/2	O
2	O
λ	O
u2	O
µ	O
y2	O
y1	O
u1	O
1/2	O
1	O
λ	O
x1	O
appendix	O
c	O
where	O
u	O
is	O
a	O
matrix	O
whose	O
rows	O
are	O
given	O
by	O
ut	O
i	O
.	O
from	O
(	O
2.46	O
)	O
it	O
follows	O
that	O
u	O
is	O
an	O
orthogonal	O
matrix	O
,	O
i.e.	O
,	O
it	O
satisﬁes	O
uut	O
=	O
i	O
,	O
and	O
hence	O
also	O
utu	O
=	O
i	O
,	O
where	O
i	O
is	O
the	O
identity	O
matrix	O
.	O
the	O
quadratic	O
form	O
,	O
and	O
hence	O
the	O
gaussian	O
density	B
,	O
will	O
be	O
constant	O
on	O
surfaces	O
if	O
all	O
of	O
the	O
eigenvalues	O
λi	O
are	O
positive	O
,	O
then	O
these	O
for	O
which	O
(	O
2.51	O
)	O
is	O
constant	O
.	O
surfaces	O
represent	O
ellipsoids	O
,	O
with	O
their	O
centres	O
at	O
µ	O
and	O
their	O
axes	O
oriented	O
along	O
ui	O
,	O
and	O
with	O
scaling	O
factors	O
in	O
the	O
directions	O
of	O
the	O
axes	O
given	O
by	O
λ	O
,	O
as	O
illustrated	O
in	O
figure	O
2.7	O
.	O
1/2	O
i	O
for	O
the	O
gaussian	O
distribution	O
to	O
be	O
well	O
deﬁned	O
,	O
it	O
is	O
necessary	O
for	O
all	O
of	O
the	O
eigenvalues	O
λi	O
of	O
the	O
covariance	B
matrix	I
to	O
be	O
strictly	O
positive	O
,	O
otherwise	O
the	O
dis-	O
tribution	O
can	O
not	O
be	O
properly	O
normalized	O
.	O
a	O
matrix	O
whose	O
eigenvalues	O
are	O
strictly	O
positive	O
is	O
said	O
to	O
be	O
positive	B
deﬁnite	I
.	O
in	O
chapter	O
12	O
,	O
we	O
will	O
encounter	O
gaussian	O
distributions	O
for	O
which	O
one	O
or	O
more	O
of	O
the	O
eigenvalues	O
are	O
zero	O
,	O
in	O
which	O
case	O
the	O
distribution	O
is	O
singular	O
and	O
is	O
conﬁned	O
to	O
a	O
subspace	O
of	O
lower	O
dimensionality	O
.	O
if	O
all	O
of	O
the	O
eigenvalues	O
are	O
nonnegative	O
,	O
then	O
the	O
covariance	B
matrix	I
is	O
said	O
to	O
be	O
positive	O
semideﬁnite	O
.	O
now	O
consider	O
the	O
form	O
of	O
the	O
gaussian	O
distribution	O
in	O
the	O
new	O
coordinate	O
system	O
deﬁned	O
by	O
the	O
yi	O
.	O
in	O
going	O
from	O
the	O
x	O
to	O
the	O
y	O
coordinate	O
system	O
,	O
we	O
have	O
a	O
jacobian	O
matrix	O
j	O
with	O
elements	O
given	O
by	O
jij	O
=	O
∂xi	O
∂yj	O
=	O
uji	O
(	O
2.53	O
)	O
where	O
uji	O
are	O
the	O
elements	O
of	O
the	O
matrix	O
ut	O
.	O
using	O
the	O
orthonormality	O
property	O
of	O
the	O
matrix	O
u	O
,	O
we	O
see	O
that	O
the	O
square	O
of	O
the	O
determinant	O
of	O
the	O
jacobian	O
matrix	O
is	O
(	O
2.54	O
)	O
and	O
hence	O
|j|	O
=	O
1.	O
also	O
,	O
the	O
determinant	O
|σ|	O
of	O
the	O
covariance	B
matrix	I
can	O
be	O
written	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
ut	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
2	O
=	O
|j|2	O
=	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
ut	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
|u|	O
=	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
utu	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
=	O
|i|	O
=	O
1	O
82	O
2.	O
probability	B
distributions	O
as	O
the	O
product	O
of	O
its	O
eigenvalues	O
,	O
and	O
hence	O
d	O
(	O
cid:14	O
)	O
j=1	O
|σ|1/2	O
=	O
1/2	O
j	O
.	O
λ	O
thus	O
in	O
the	O
yj	O
coordinate	O
system	O
,	O
the	O
gaussian	O
distribution	O
takes	O
the	O
form	O
d	O
(	O
cid:14	O
)	O
j=1	O
p	O
(	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
|j|	O
=	O
1	O
(	O
2πλj	O
)	O
1/2	O
exp	O
−	O
y2	O
j	O
2λj	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
(	O
2.55	O
)	O
(	O
2.56	O
)	O
e	O
[	O
x	O
]	O
=	O
1	O
(	O
2π	O
)	O
d/2	O
1	O
|σ|1/2	O
1	O
1	O
which	O
is	O
the	O
product	O
of	O
d	O
independent	B
univariate	O
gaussian	O
distributions	O
.	O
the	O
eigen-	O
vectors	O
therefore	O
deﬁne	O
a	O
new	O
set	O
of	O
shifted	O
and	O
rotated	O
coordinates	O
with	O
respect	O
to	O
which	O
the	O
joint	O
probability	B
distribution	O
factorizes	O
into	O
a	O
product	O
of	O
independent	B
distributions	O
.	O
the	O
integral	O
of	O
the	O
distribution	O
in	O
the	O
y	O
coordinate	O
system	O
is	O
then	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
(	O
cid:6	O
)	O
p	O
(	O
y	O
)	O
dy	O
=	O
1	O
(	O
2πλj	O
)	O
1/2	O
exp	O
−	O
y2	O
j	O
2λj	O
dyj	O
=	O
1	O
(	O
2.57	O
)	O
(	O
cid:6	O
)	O
∞	O
d	O
(	O
cid:14	O
)	O
−∞	O
j=1	O
where	O
we	O
have	O
used	O
the	O
result	O
(	O
1.48	O
)	O
for	O
the	O
normalization	O
of	O
the	O
univariate	O
gaussian	O
.	O
this	O
conﬁrms	O
that	O
the	O
multivariate	O
gaussian	O
(	O
2.43	O
)	O
is	O
indeed	O
normalized	O
.	O
we	O
now	O
look	O
at	O
the	O
moments	O
of	O
the	O
gaussian	O
distribution	O
and	O
thereby	O
provide	O
an	O
interpretation	O
of	O
the	O
parameters	O
µ	O
and	O
σ.	O
the	O
expectation	B
of	O
x	O
under	O
the	O
gaussian	O
distribution	O
is	O
given	O
by	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
−1	O
2	O
−1	O
2	O
(	O
cid:13	O
)	O
−1z	O
exp	O
(	O
x	O
−	O
µ	O
)	O
tς	O
−1	O
(	O
x	O
−	O
µ	O
)	O
x	O
dx	O
=	O
exp	O
|σ|1/2	O
(	O
2π	O
)	O
d/2	O
(	O
2.58	O
)	O
where	O
we	O
have	O
changed	O
variables	O
using	O
z	O
=	O
x	O
−	O
µ.	O
we	O
now	O
note	O
that	O
the	O
exponent	O
is	O
an	O
even	O
function	O
of	O
the	O
components	O
of	O
z	O
and	O
,	O
because	O
the	O
integrals	O
over	O
these	O
are	O
taken	O
over	O
the	O
range	O
(	O
−∞	O
,	O
∞	O
)	O
,	O
the	O
term	O
in	O
z	O
in	O
the	O
factor	O
(	O
z	O
+	O
µ	O
)	O
will	O
vanish	O
by	O
symmetry	O
.	O
thus	O
(	O
z	O
+	O
µ	O
)	O
dz	O
ztς	O
e	O
[	O
x	O
]	O
=	O
µ	O
(	O
2.59	O
)	O
and	O
so	O
we	O
refer	O
to	O
µ	O
as	O
the	O
mean	B
of	O
the	O
gaussian	O
distribution	O
.	O
we	O
now	O
consider	O
second	B
order	I
moments	O
of	O
the	O
gaussian	O
.	O
in	O
the	O
univariate	O
case	O
,	O
we	O
considered	O
the	O
second	B
order	I
moment	O
given	O
by	O
e	O
[	O
x2	O
]	O
.	O
for	O
the	O
multivariate	O
gaus-	O
sian	O
,	O
there	O
are	O
d2	O
second	B
order	I
moments	O
given	O
by	O
e	O
[	O
xixj	O
]	O
,	O
which	O
we	O
can	O
group	O
together	O
to	O
form	O
the	O
matrix	O
e	O
[	O
xxt	O
]	O
.	O
this	O
matrix	O
can	O
be	O
written	O
as	O
e	O
[	O
xxt	O
]	O
=	O
1	O
(	O
cid:6	O
)	O
|σ|1/2	O
1	O
(	O
2π	O
)	O
d/2	O
1	O
1	O
=	O
(	O
2π	O
)	O
d/2	O
|σ|1/2	O
exp	O
(	O
cid:6	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
exp	O
−1	O
2	O
(	O
cid:13	O
)	O
ztς	O
−1z	O
−1	O
2	O
(	O
x	O
−	O
µ	O
)	O
tς	O
−1	O
(	O
x	O
−	O
µ	O
)	O
xxt	O
dx	O
(	O
z	O
+	O
µ	O
)	O
(	O
z	O
+	O
µ	O
)	O
t	O
dz	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
2.3.	O
the	O
gaussian	O
distribution	O
83	O
where	O
again	O
we	O
have	O
changed	O
variables	O
using	O
z	O
=	O
x	O
−	O
µ.	O
note	O
that	O
the	O
cross-terms	O
involving	O
µzt	O
and	O
µtz	O
will	O
again	O
vanish	O
by	O
symmetry	O
.	O
the	O
term	O
µµt	O
is	O
constant	O
and	O
can	O
be	O
taken	O
outside	O
the	O
integral	O
,	O
which	O
itself	O
is	O
unity	O
because	O
the	O
gaussian	O
distribution	O
is	O
normalized	O
.	O
consider	O
the	O
term	O
involving	O
zzt	O
.	O
again	O
,	O
we	O
can	O
make	O
use	O
of	O
the	O
eigenvector	O
expansion	O
of	O
the	O
covariance	B
matrix	I
given	O
by	O
(	O
2.45	O
)	O
,	O
together	O
with	O
the	O
completeness	O
of	O
the	O
set	O
of	O
eigenvectors	O
,	O
to	O
write	O
d	O
(	O
cid:2	O
)	O
j=1	O
z	O
=	O
yjuj	O
(	O
2.60	O
)	O
(	O
cid:6	O
)	O
(	O
cid:12	O
)	O
d	O
(	O
cid:2	O
)	O
−1	O
2	O
d	O
(	O
cid:2	O
)	O
(	O
cid:13	O
)	O
(	O
cid:6	O
)	O
exp	O
ztς	O
−1z	O
zzt	O
dz	O
1	O
|σ|1/2	O
uiut	O
j	O
exp	O
i=1	O
j=1	O
k=1	O
(	O
cid:24	O
)	O
−	O
d	O
(	O
cid:2	O
)	O
(	O
cid:25	O
)	O
y2	O
k	O
2λk	O
yiyj	O
dy	O
where	O
yj	O
=	O
ut	O
j	O
z	O
,	O
which	O
gives	O
1	O
(	O
2π	O
)	O
d/2	O
1	O
|σ|1/2	O
1	O
(	O
2π	O
)	O
d/2	O
d	O
(	O
cid:2	O
)	O
=	O
=	O
uiut	O
i	O
λi	O
=	O
σ	O
(	O
2.61	O
)	O
i=1	O
where	O
we	O
have	O
made	O
use	O
of	O
the	O
eigenvector	O
equation	O
(	O
2.45	O
)	O
,	O
together	O
with	O
the	O
fact	O
that	O
the	O
integral	O
on	O
the	O
right-hand	O
side	O
of	O
the	O
middle	O
line	O
vanishes	O
by	O
symmetry	O
unless	O
i	O
=	O
j	O
,	O
and	O
in	O
the	O
ﬁnal	O
line	O
we	O
have	O
made	O
use	O
of	O
the	O
results	O
(	O
1.50	O
)	O
and	O
(	O
2.55	O
)	O
,	O
together	O
with	O
(	O
2.48	O
)	O
.	O
thus	O
we	O
have	O
e	O
[	O
xxt	O
]	O
=	O
µµt	O
+	O
σ	O
.	O
(	O
2.62	O
)	O
for	O
single	O
random	O
variables	O
,	O
we	O
subtracted	O
the	O
mean	B
before	O
taking	O
second	O
mo-	O
ments	O
in	O
order	O
to	O
deﬁne	O
a	O
variance	B
.	O
similarly	O
,	O
in	O
the	O
multivariate	O
case	O
it	O
is	O
again	O
convenient	O
to	O
subtract	O
off	O
the	O
mean	B
,	O
giving	O
rise	O
to	O
the	O
covariance	B
of	O
a	O
random	O
vector	O
x	O
deﬁned	O
by	O
cov	O
[	O
x	O
]	O
=	O
e	O
(	O
2.63	O
)	O
for	O
the	O
speciﬁc	O
case	O
of	O
a	O
gaussian	O
distribution	O
,	O
we	O
can	O
make	O
use	O
of	O
e	O
[	O
x	O
]	O
=	O
µ	O
,	O
together	O
with	O
the	O
result	O
(	O
2.62	O
)	O
,	O
to	O
give	O
(	O
x	O
−	O
e	O
[	O
x	O
]	O
)	O
(	O
x	O
−	O
e	O
[	O
x	O
]	O
)	O
t	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
.	O
cov	O
[	O
x	O
]	O
=	O
σ	O
.	O
(	O
2.64	O
)	O
because	O
the	O
parameter	O
matrix	O
σ	O
governs	O
the	O
covariance	B
of	O
x	O
under	O
the	O
gaussian	O
distribution	O
,	O
it	O
is	O
called	O
the	O
covariance	B
matrix	I
.	O
although	O
the	O
gaussian	O
distribution	O
(	O
2.43	O
)	O
is	O
widely	O
used	O
as	O
a	O
density	B
model	O
,	O
it	O
suffers	O
from	O
some	O
signiﬁcant	O
limitations	O
.	O
consider	O
the	O
number	O
of	O
free	O
parameters	O
in	O
the	O
distribution	O
.	O
a	O
general	O
symmetric	O
covariance	B
matrix	I
σ	O
will	O
have	O
d	O
(	O
d	O
+	O
1	O
)	O
/2	O
independent	B
parameters	O
,	O
and	O
there	O
are	O
another	O
d	O
independent	B
parameters	O
in	O
µ	O
,	O
giv-	O
ing	O
d	O
(	O
d	O
+	O
3	O
)	O
/2	O
parameters	O
in	O
total	O
.	O
for	O
large	O
d	O
,	O
the	O
total	O
number	O
of	O
parameters	O
exercise	O
2.21	O
84	O
2.	O
probability	B
distributions	O
figure	O
2.8	O
contours	O
of	O
constant	O
probability	B
density	O
for	O
a	O
gaussian	O
distribution	O
in	O
two	O
dimensions	O
in	O
which	O
the	O
covariance	B
matrix	I
is	O
(	O
a	O
)	O
of	O
general	O
form	O
,	O
(	O
b	O
)	O
diagonal	B
,	O
in	O
which	O
the	O
elliptical	O
contours	O
are	O
aligned	O
with	O
the	O
coordinate	O
axes	O
,	O
and	O
(	O
c	O
)	O
proportional	O
to	O
the	O
identity	O
matrix	O
,	O
in	O
which	O
the	O
contours	O
are	O
concentric	O
circles	O
.	O
x2	O
x2	O
x2	O
x1	O
x1	O
x1	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
therefore	O
grows	O
quadratically	O
with	O
d	O
,	O
and	O
the	O
computational	O
task	O
of	O
manipulating	O
and	O
inverting	O
large	O
matrices	O
can	O
become	O
prohibitive	O
.	O
one	O
way	O
to	O
address	O
this	O
prob-	O
lem	O
is	O
to	O
use	O
restricted	O
forms	O
of	O
the	O
covariance	B
matrix	I
.	O
if	O
we	O
consider	O
covariance	B
matrices	O
that	O
are	O
diagonal	B
,	O
so	O
that	O
σ	O
=	O
diag	O
(	O
σ2	O
i	O
)	O
,	O
we	O
then	O
have	O
a	O
total	O
of	O
2d	O
inde-	O
pendent	O
parameters	O
in	O
the	O
density	B
model	O
.	O
the	O
corresponding	O
contours	O
of	O
constant	O
density	B
are	O
given	O
by	O
axis-aligned	O
ellipsoids	O
.	O
we	O
could	O
further	O
restrict	O
the	O
covariance	B
matrix	I
to	O
be	O
proportional	O
to	O
the	O
identity	O
matrix	O
,	O
σ	O
=	O
σ2i	O
,	O
known	O
as	O
an	O
isotropic	B
co-	O
variance	B
,	O
giving	O
d	O
+	O
1	O
independent	B
parameters	O
in	O
the	O
model	O
and	O
spherical	O
surfaces	O
of	O
constant	O
density	B
.	O
the	O
three	O
possibilities	O
of	O
general	O
,	O
diagonal	B
,	O
and	O
isotropic	B
covari-	O
ance	O
matrices	O
are	O
illustrated	O
in	O
figure	O
2.8.	O
unfortunately	O
,	O
whereas	O
such	O
approaches	O
limit	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
in	O
the	O
distribution	O
and	O
make	O
inversion	O
of	O
the	O
covariance	B
matrix	I
a	O
much	O
faster	O
operation	O
,	O
they	O
also	O
greatly	O
restrict	O
the	O
form	O
of	O
the	O
probability	B
density	O
and	O
limit	O
its	O
ability	O
to	O
capture	O
interesting	O
correlations	O
in	O
the	O
data	O
.	O
a	O
further	O
limitation	O
of	O
the	O
gaussian	O
distribution	O
is	O
that	O
it	O
is	O
intrinsically	O
uni-	O
modal	O
(	O
i.e.	O
,	O
has	O
a	O
single	O
maximum	O
)	O
and	O
so	O
is	O
unable	O
to	O
provide	O
a	O
good	O
approximation	O
to	O
multimodal	O
distributions	O
.	O
thus	O
the	O
gaussian	O
distribution	O
can	O
be	O
both	O
too	O
ﬂexible	O
,	O
in	O
the	O
sense	O
of	O
having	O
too	O
many	O
parameters	O
,	O
while	O
also	O
being	O
too	O
limited	O
in	O
the	O
range	O
of	O
distributions	O
that	O
it	O
can	O
adequately	O
represent	O
.	O
we	O
will	O
see	O
later	O
that	O
the	O
introduc-	O
tion	O
of	O
latent	O
variables	O
,	O
also	O
called	O
hidden	O
variables	O
or	O
unobserved	O
variables	O
,	O
allows	O
both	O
of	O
these	O
problems	O
to	O
be	O
addressed	O
.	O
in	O
particular	O
,	O
a	O
rich	O
family	O
of	O
multimodal	O
distributions	O
is	O
obtained	O
by	O
introducing	O
discrete	O
latent	O
variables	O
leading	O
to	O
mixtures	O
of	O
gaussians	O
,	O
as	O
discussed	O
in	O
section	O
2.3.9.	O
similarly	O
,	O
the	O
introduction	O
of	O
continuous	O
latent	O
variables	O
,	O
as	O
described	O
in	O
chapter	O
12	O
,	O
leads	O
to	O
models	O
in	O
which	O
the	O
number	O
of	O
free	O
parameters	O
can	O
be	O
controlled	O
independently	O
of	O
the	O
dimensionality	O
d	O
of	O
the	O
data	O
space	O
while	O
still	O
allowing	O
the	O
model	O
to	O
capture	O
the	O
dominant	O
correlations	O
in	O
the	O
data	O
set	O
.	O
indeed	O
,	O
these	O
two	O
approaches	O
can	O
be	O
combined	O
and	O
further	O
extended	B
to	O
derive	O
a	O
very	O
rich	O
set	O
of	O
hierarchical	B
models	O
that	O
can	O
be	O
adapted	O
to	O
a	O
broad	O
range	O
of	O
prac-	O
tical	O
applications	O
.	O
for	O
instance	O
,	O
the	O
gaussian	O
version	O
of	O
the	O
markov	O
random	O
ﬁeld	O
,	O
which	O
is	O
widely	O
used	O
as	O
a	O
probabilistic	O
model	O
of	O
images	O
,	O
is	O
a	O
gaussian	O
distribution	O
over	O
the	O
joint	O
space	O
of	O
pixel	O
intensities	O
but	O
rendered	O
tractable	O
through	O
the	O
imposition	O
of	O
considerable	O
structure	O
reﬂecting	O
the	O
spatial	O
organization	O
of	O
the	O
pixels	O
.	O
similarly	O
,	O
the	O
linear	B
dynamical	I
system	I
,	O
used	O
to	O
model	O
time	O
series	O
data	O
for	O
applications	O
such	O
as	O
tracking	O
,	O
is	O
also	O
a	O
joint	O
gaussian	O
distribution	O
over	O
a	O
potentially	O
large	O
number	O
of	O
observed	O
and	O
latent	O
variables	O
and	O
again	O
is	O
tractable	O
due	O
to	O
the	O
structure	O
imposed	O
on	O
the	O
distribution	O
.	O
a	O
powerful	O
framework	O
for	O
expressing	O
the	O
form	O
and	O
properties	O
of	O
section	O
8.3	O
section	O
13.3	O
2.3.	O
the	O
gaussian	O
distribution	O
85	O
such	O
complex	O
distributions	O
is	O
that	O
of	O
probabilistic	O
graphical	O
models	O
,	O
which	O
will	O
form	O
the	O
subject	O
of	O
chapter	O
8	O
.	O
2.3.1	O
conditional	B
gaussian	O
distributions	O
an	O
important	O
property	O
of	O
the	O
multivariate	O
gaussian	O
distribution	O
is	O
that	O
if	O
two	O
sets	O
of	O
variables	O
are	O
jointly	O
gaussian	O
,	O
then	O
the	O
conditional	B
distribution	O
of	O
one	O
set	O
conditioned	O
on	O
the	O
other	O
is	O
again	O
gaussian	O
.	O
similarly	O
,	O
the	O
marginal	B
distribution	O
of	O
either	O
set	O
is	O
also	O
gaussian	O
.	O
consider	O
ﬁrst	O
the	O
case	O
of	O
conditional	B
distributions	O
.	O
suppose	O
x	O
is	O
a	O
d-dimensional	O
vector	O
with	O
gaussian	O
distribution	O
n	O
(	O
x|µ	O
,	O
σ	O
)	O
and	O
that	O
we	O
partition	O
x	O
into	O
two	O
dis-	O
joint	O
subsets	O
xa	O
and	O
xb	O
.	O
without	O
loss	O
of	O
generality	O
,	O
we	O
can	O
take	O
xa	O
to	O
form	O
the	O
ﬁrst	O
m	O
components	O
of	O
x	O
,	O
with	O
xb	O
comprising	O
the	O
remaining	O
d	O
−	O
m	O
components	O
,	O
so	O
that	O
we	O
also	O
deﬁne	O
corresponding	O
partitions	O
of	O
the	O
mean	B
vector	O
µ	O
given	O
by	O
and	O
of	O
the	O
covariance	B
matrix	I
σ	O
given	O
by	O
note	O
that	O
the	O
symmetry	O
σt	O
=	O
σ	O
of	O
the	O
covariance	B
matrix	I
implies	O
that	O
σaa	O
and	O
σbb	O
are	O
symmetric	O
,	O
while	O
σba	O
=	O
σt	O
ab	O
.	O
in	O
many	O
situations	O
,	O
it	O
will	O
be	O
convenient	O
to	O
work	O
with	O
the	O
inverse	B
of	O
the	O
covari-	O
ance	O
matrix	O
λ	O
≡	O
σ	O
−1	O
(	O
2.68	O
)	O
which	O
is	O
known	O
as	O
the	O
precision	B
matrix	I
.	O
in	O
fact	O
,	O
we	O
shall	O
see	O
that	O
some	O
properties	O
of	O
gaussian	O
distributions	O
are	O
most	O
naturally	O
expressed	O
in	O
terms	O
of	O
the	O
covariance	B
,	O
whereas	O
others	O
take	O
a	O
simpler	O
form	O
when	O
viewed	O
in	O
terms	O
of	O
the	O
precision	O
.	O
we	O
therefore	O
also	O
introduce	O
the	O
partitioned	B
form	O
of	O
the	O
precision	B
matrix	I
(	O
cid:15	O
)	O
λ	O
=	O
λaa	O
λab	O
λba	O
λbb	O
exercise	O
2.22	O
corresponding	O
to	O
the	O
partitioning	O
(	O
2.65	O
)	O
of	O
the	O
vector	O
x.	O
because	O
the	O
inverse	B
of	O
a	O
symmetric	O
matrix	O
is	O
also	O
symmetric	O
,	O
we	O
see	O
that	O
λaa	O
and	O
λbb	O
are	O
symmetric	O
,	O
while	O
λt	O
ab	O
=	O
λba	O
.	O
it	O
should	O
be	O
stressed	O
at	O
this	O
point	O
that	O
,	O
for	O
instance	O
,	O
λaa	O
is	O
not	O
simply	O
given	O
by	O
the	O
inverse	B
of	O
σaa	O
.	O
in	O
fact	O
,	O
we	O
shall	O
shortly	O
examine	O
the	O
relation	O
between	O
the	O
inverse	B
of	O
a	O
partitioned	B
matrix	O
and	O
the	O
inverses	O
of	O
its	O
partitions	O
.	O
let	O
us	O
begin	O
by	O
ﬁnding	O
an	O
expression	O
for	O
the	O
conditional	B
distribution	O
p	O
(	O
xa|xb	O
)	O
.	O
from	O
the	O
product	B
rule	I
of	I
probability	I
,	O
we	O
see	O
that	O
this	O
conditional	B
distribution	O
can	O
be	O
x	O
=	O
xa	O
xb	O
(	O
cid:15	O
)	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
.	O
µa	O
µb	O
µ	O
=	O
(	O
cid:15	O
)	O
σ	O
=	O
σaa	O
σab	O
σba	O
σbb	O
(	O
cid:16	O
)	O
.	O
(	O
cid:16	O
)	O
(	O
2.65	O
)	O
(	O
2.66	O
)	O
(	O
2.67	O
)	O
(	O
2.69	O
)	O
86	O
2.	O
probability	B
distributions	O
evaluated	O
from	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
xa	O
,	O
xb	O
)	O
simply	O
by	O
ﬁxing	O
xb	O
to	O
the	O
observed	O
value	O
and	O
normalizing	O
the	O
resulting	O
expression	O
to	O
obtain	O
a	O
valid	O
probability	B
distribution	O
over	O
xa	O
.	O
instead	O
of	O
performing	O
this	O
normalization	O
explicitly	O
,	O
we	O
can	O
obtain	O
the	O
solution	O
more	O
efﬁciently	O
by	O
considering	O
the	O
quadratic	O
form	O
in	O
the	O
exponent	O
of	O
the	O
gaussian	O
distribution	O
given	O
by	O
(	O
2.44	O
)	O
and	O
then	O
reinstating	O
the	O
normalization	O
coefﬁcient	O
at	O
the	O
end	O
of	O
the	O
calculation	O
.	O
if	O
we	O
make	O
use	O
of	O
the	O
partitioning	O
(	O
2.65	O
)	O
,	O
(	O
2.66	O
)	O
,	O
and	O
(	O
2.69	O
)	O
,	O
we	O
obtain	O
−1	O
(	O
x	O
−	O
µ	O
)	O
=	O
−1	O
2	O
(	O
x	O
−	O
µ	O
)	O
tς	O
−1	O
2	O
−1	O
2	O
(	O
xa	O
−	O
µa	O
)	O
tλab	O
(	O
xb	O
−	O
µb	O
)	O
(	O
xa	O
−	O
µa	O
)	O
tλaa	O
(	O
xa	O
−	O
µa	O
)	O
−	O
1	O
2	O
(	O
xb	O
−	O
µb	O
)	O
tλbb	O
(	O
xb	O
−	O
µb	O
)	O
.	O
(	O
xb	O
−	O
µb	O
)	O
tλba	O
(	O
xa	O
−	O
µa	O
)	O
−	O
1	O
2	O
(	O
2.70	O
)	O
we	O
see	O
that	O
as	O
a	O
function	O
of	O
xa	O
,	O
this	O
is	O
again	O
a	O
quadratic	O
form	O
,	O
and	O
hence	O
the	O
cor-	O
responding	O
conditional	B
distribution	O
p	O
(	O
xa|xb	O
)	O
will	O
be	O
gaussian	O
.	O
because	O
this	O
distri-	O
bution	O
is	O
completely	O
characterized	O
by	O
its	O
mean	B
and	O
its	O
covariance	B
,	O
our	O
goal	O
will	O
be	O
to	O
identify	O
expressions	O
for	O
the	O
mean	B
and	O
covariance	B
of	O
p	O
(	O
xa|xb	O
)	O
by	O
inspection	O
of	O
(	O
2.70	O
)	O
.	O
this	O
is	O
an	O
example	O
of	O
a	O
rather	O
common	O
operation	O
associated	O
with	O
gaussian	O
distributions	O
,	O
sometimes	O
called	O
‘	O
completing	B
the	I
square	I
’	O
,	O
in	O
which	O
we	O
are	O
given	O
a	O
quadratic	O
form	O
deﬁning	O
the	O
exponent	O
terms	O
in	O
a	O
gaussian	O
distribution	O
,	O
and	O
we	O
need	O
to	O
determine	O
the	O
corresponding	O
mean	B
and	O
covariance	B
.	O
such	O
problems	O
can	O
be	O
solved	O
straightforwardly	O
by	O
noting	O
that	O
the	O
exponent	O
in	O
a	O
general	O
gaussian	O
distribution	O
n	O
(	O
x|µ	O
,	O
σ	O
)	O
can	O
be	O
written	O
−1	O
2	O
(	O
x	O
−	O
µ	O
)	O
tς	O
−1	O
(	O
x	O
−	O
µ	O
)	O
=	O
−1	O
2	O
xtς	O
−1x	O
+	O
xtς	O
−1µ	O
+	O
const	O
(	O
2.71	O
)	O
−1	O
and	O
the	O
coefﬁcient	O
of	O
the	O
linear	O
term	O
in	O
x	O
to	O
σ	O
where	O
‘	O
const	O
’	O
denotes	O
terms	O
which	O
are	O
independent	B
of	O
x	O
,	O
and	O
we	O
have	O
made	O
use	O
of	O
the	O
symmetry	O
of	O
σ.	O
thus	O
if	O
we	O
take	O
our	O
general	O
quadratic	O
form	O
and	O
express	O
it	O
in	O
the	O
form	O
given	O
by	O
the	O
right-hand	O
side	O
of	O
(	O
2.71	O
)	O
,	O
then	O
we	O
can	O
immediately	O
equate	O
the	O
matrix	O
of	O
coefﬁcients	O
entering	O
the	O
second	B
order	I
term	O
in	O
x	O
to	O
the	O
inverse	B
covariance	O
−1µ	O
,	O
from	O
which	O
we	O
can	O
matrix	O
σ	O
obtain	O
µ.	O
now	O
let	O
us	O
apply	O
this	O
procedure	O
to	O
the	O
conditional	B
gaussian	O
distribution	O
p	O
(	O
xa|xb	O
)	O
for	O
which	O
the	O
quadratic	O
form	O
in	O
the	O
exponent	O
is	O
given	O
by	O
(	O
2.70	O
)	O
.	O
we	O
will	O
denote	O
the	O
mean	B
and	O
covariance	B
of	O
this	O
distribution	O
by	O
µa|b	O
and	O
σa|b	O
,	O
respectively	O
.	O
consider	O
the	O
functional	B
dependence	O
of	O
(	O
2.70	O
)	O
on	O
xa	O
in	O
which	O
xb	O
is	O
regarded	O
as	O
a	O
constant	O
.	O
if	O
we	O
pick	O
out	O
all	O
terms	O
that	O
are	O
second	B
order	I
in	O
xa	O
,	O
we	O
have	O
−1	O
2	O
xt	O
a	O
λaaxa	O
(	O
2.72	O
)	O
from	O
which	O
we	O
can	O
immediately	O
conclude	O
that	O
the	O
covariance	B
(	O
inverse	B
precision	O
)	O
of	O
p	O
(	O
xa|xb	O
)	O
is	O
given	O
by	O
σa|b	O
=	O
λ	O
−1	O
aa	O
.	O
(	O
2.73	O
)	O
2.3.	O
the	O
gaussian	O
distribution	O
87	O
now	O
consider	O
all	O
of	O
the	O
terms	O
in	O
(	O
2.70	O
)	O
that	O
are	O
linear	O
in	O
xa	O
a	O
{	O
λaaµa	O
−	O
λab	O
(	O
xb	O
−	O
µb	O
)	O
}	O
xt	O
(	O
2.74	O
)	O
where	O
we	O
have	O
used	O
λt	O
the	O
coefﬁcient	O
of	O
xa	O
in	O
this	O
expression	O
must	O
equal	O
σ	O
ba	O
=	O
λab	O
.	O
from	O
our	O
discussion	O
of	O
the	O
general	O
form	O
(	O
2.71	O
)	O
,	O
−1	O
a|bµa|b	O
and	O
hence	O
µa|b	O
=	O
σa|b	O
{	O
λaaµa	O
−	O
λab	O
(	O
xb	O
−	O
µb	O
)	O
}	O
=	O
µa	O
−	O
λ	O
aa	O
λab	O
(	O
xb	O
−	O
µb	O
)	O
−1	O
(	O
2.75	O
)	O
where	O
we	O
have	O
made	O
use	O
of	O
(	O
2.73	O
)	O
.	O
the	O
results	O
(	O
2.73	O
)	O
and	O
(	O
2.75	O
)	O
are	O
expressed	O
in	O
terms	O
of	O
the	O
partitioned	B
precision	O
matrix	O
of	O
the	O
original	O
joint	O
distribution	O
p	O
(	O
xa	O
,	O
xb	O
)	O
.	O
we	O
can	O
also	O
express	O
these	O
results	O
in	O
terms	O
of	O
the	O
corresponding	O
partitioned	B
covariance	O
matrix	O
.	O
to	O
do	O
this	O
,	O
we	O
make	O
use	O
of	O
the	O
following	O
identity	O
for	O
the	O
inverse	B
of	O
a	O
partitioned	B
matrix	O
−mbd−1	O
(	O
cid:16	O
)	O
−1	O
(	O
cid:15	O
)	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
m	O
=	O
−d−1cm	O
d−1	O
+	O
d−1cmbd−1	O
a	O
b	O
c	O
d	O
(	O
2.76	O
)	O
exercise	O
2.24	O
where	O
we	O
have	O
deﬁned	O
(	O
2.77	O
)	O
the	O
quantity	O
m−1	O
is	O
known	O
as	O
the	O
schur	O
complement	O
of	O
the	O
matrix	O
on	O
the	O
left-hand	O
side	O
of	O
(	O
2.76	O
)	O
with	O
respect	O
to	O
the	O
submatrix	O
d.	O
using	O
the	O
deﬁnition	O
m	O
=	O
(	O
a	O
−	O
bd−1c	O
)	O
−1	O
.	O
(	O
cid:15	O
)	O
(	O
2.78	O
)	O
(	O
2.79	O
)	O
(	O
2.80	O
)	O
(	O
cid:16	O
)	O
−1	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
σaa	O
σab	O
σba	O
σbb	O
=	O
λaa	O
λab	O
λba	O
λbb	O
and	O
making	O
use	O
of	O
(	O
2.76	O
)	O
,	O
we	O
have	O
λaa	O
=	O
(	O
σaa	O
−	O
σabς	O
−1	O
bb	O
σba	O
)	O
−1	O
λab	O
=	O
−	O
(	O
σaa	O
−	O
σabς	O
−1	O
bb	O
σba	O
)	O
−1σabς	O
−1	O
bb	O
.	O
from	O
these	O
we	O
obtain	O
the	O
following	O
expressions	O
for	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
conditional	B
distribution	O
p	O
(	O
xa|xb	O
)	O
bb	O
(	O
xb	O
−	O
µb	O
)	O
−1	O
µa|b	O
=	O
µa	O
+	O
σabς	O
σa|b	O
=	O
σaa	O
−	O
σabς	O
−1	O
bb	O
σba	O
.	O
(	O
2.81	O
)	O
(	O
2.82	O
)	O
comparing	O
(	O
2.73	O
)	O
and	O
(	O
2.82	O
)	O
,	O
we	O
see	O
that	O
the	O
conditional	B
distribution	O
p	O
(	O
xa|xb	O
)	O
takes	O
a	O
simpler	O
form	O
when	O
expressed	O
in	O
terms	O
of	O
the	O
partitioned	B
precision	O
matrix	O
than	O
when	O
it	O
is	O
expressed	O
in	O
terms	O
of	O
the	O
partitioned	B
covariance	O
matrix	O
.	O
note	O
that	O
the	O
mean	B
of	O
the	O
conditional	B
distribution	O
p	O
(	O
xa|xb	O
)	O
,	O
given	O
by	O
(	O
2.81	O
)	O
,	O
is	O
a	O
linear	O
function	O
of	O
xb	O
and	O
that	O
the	O
covariance	B
,	O
given	O
by	O
(	O
2.82	O
)	O
,	O
is	O
independent	B
of	O
xa	O
.	O
this	O
represents	O
an	O
example	O
of	O
a	O
linear-gaussian	O
model	O
.	O
section	O
8.1.4	O
88	O
2.	O
probability	B
distributions	O
2.3.2	O
marginal	B
gaussian	O
distributions	O
we	O
have	O
seen	O
that	O
if	O
a	O
joint	O
distribution	O
p	O
(	O
xa	O
,	O
xb	O
)	O
is	O
gaussian	O
,	O
then	O
the	O
condi-	O
tional	O
distribution	O
p	O
(	O
xa|xb	O
)	O
will	O
again	O
be	O
gaussian	O
.	O
now	O
we	O
turn	O
to	O
a	O
discussion	O
of	O
the	O
marginal	B
distribution	O
given	O
by	O
(	O
cid:6	O
)	O
p	O
(	O
xa	O
)	O
=	O
p	O
(	O
xa	O
,	O
xb	O
)	O
dxb	O
(	O
2.83	O
)	O
which	O
,	O
as	O
we	O
shall	O
see	O
,	O
is	O
also	O
gaussian	O
.	O
once	O
again	O
,	O
our	O
strategy	O
for	O
evaluating	O
this	O
distribution	O
efﬁciently	O
will	O
be	O
to	O
focus	O
on	O
the	O
quadratic	O
form	O
in	O
the	O
exponent	O
of	O
the	O
joint	O
distribution	O
and	O
thereby	O
to	O
identify	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
marginal	B
distribution	O
p	O
(	O
xa	O
)	O
.	O
the	O
quadratic	O
form	O
for	O
the	O
joint	O
distribution	O
can	O
be	O
expressed	O
,	O
using	O
the	O
par-	O
titioned	O
precision	B
matrix	I
,	O
in	O
the	O
form	O
(	O
2.70	O
)	O
.	O
because	O
our	O
goal	O
is	O
to	O
integrate	O
out	O
xb	O
,	O
this	O
is	O
most	O
easily	O
achieved	O
by	O
ﬁrst	O
considering	O
the	O
terms	O
involving	O
xb	O
and	O
then	O
completing	B
the	I
square	I
in	O
order	O
to	O
facilitate	O
integration	O
.	O
picking	O
out	O
just	O
those	O
terms	O
that	O
involve	O
xb	O
,	O
we	O
have	O
−1	O
2	O
bb	O
m	O
)	O
tλbb	O
(	O
xb−λ	O
−1	O
b	O
m	O
=	O
−1	O
2	O
−1	O
bb	O
m	O
(	O
2.84	O
)	O
b	O
λbbxb+xt	O
xt	O
−1	O
bb	O
m	O
)	O
+	O
(	O
xb−λ	O
mtλ	O
1	O
2	O
where	O
we	O
have	O
deﬁned	O
m	O
=	O
λbbµb	O
−	O
λba	O
(	O
xa	O
−	O
µa	O
)	O
.	O
(	O
2.85	O
)	O
we	O
see	O
that	O
the	O
dependence	O
on	O
xb	O
has	O
been	O
cast	O
into	O
the	O
standard	O
quadratic	O
form	O
of	O
a	O
gaussian	O
distribution	O
corresponding	O
to	O
the	O
ﬁrst	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
2.84	O
)	O
,	O
plus	O
a	O
term	O
that	O
does	O
not	O
depend	O
on	O
xb	O
(	O
but	O
that	O
does	O
depend	O
on	O
xa	O
)	O
.	O
thus	O
,	O
when	O
we	O
take	O
the	O
exponential	O
of	O
this	O
quadratic	O
form	O
,	O
we	O
see	O
that	O
the	O
integration	O
over	O
xb	O
required	O
by	O
(	O
2.83	O
)	O
will	O
take	O
the	O
form	O
(	O
xb	O
−	O
λ	O
bb	O
m	O
)	O
tλbb	O
(	O
xb	O
−	O
λ	O
−1	O
−1	O
bb	O
m	O
)	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
(	O
2.86	O
)	O
dxb	O
.	O
(	O
cid:6	O
)	O
exp	O
−1	O
2	O
this	O
integration	O
is	O
easily	O
performed	O
by	O
noting	O
that	O
it	O
is	O
the	O
integral	O
over	O
an	O
unnor-	O
malized	O
gaussian	O
,	O
and	O
so	O
the	O
result	O
will	O
be	O
the	O
reciprocal	O
of	O
the	O
normalization	O
co-	O
efﬁcient	O
.	O
we	O
know	O
from	O
the	O
form	O
of	O
the	O
normalized	O
gaussian	O
given	O
by	O
(	O
2.43	O
)	O
,	O
that	O
this	O
coefﬁcient	O
is	O
independent	B
of	O
the	O
mean	B
and	O
depends	O
only	O
on	O
the	O
determinant	O
of	O
the	O
covariance	B
matrix	I
.	O
thus	O
,	O
by	O
completing	B
the	I
square	I
with	O
respect	O
to	O
xb	O
,	O
we	O
can	O
integrate	O
out	O
xb	O
and	O
the	O
only	O
term	O
remaining	O
from	O
the	O
contributions	O
on	O
the	O
left-hand	O
side	O
of	O
(	O
2.84	O
)	O
that	O
depends	O
on	O
xa	O
is	O
the	O
last	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
2.84	O
)	O
in	O
which	O
m	O
is	O
given	O
by	O
(	O
2.85	O
)	O
.	O
combining	O
this	O
term	O
with	O
the	O
remaining	O
terms	O
from	O
2.3.	O
the	O
gaussian	O
distribution	O
89	O
(	O
2.70	O
)	O
that	O
depend	O
on	O
xa	O
,	O
we	O
obtain	O
1	O
2	O
[	O
λbbµb	O
−	O
λba	O
(	O
xa	O
−	O
µa	O
)	O
]	O
t	O
λ	O
bb	O
[	O
λbbµb	O
−	O
λba	O
(	O
xa	O
−	O
µa	O
)	O
]	O
−1	O
a	O
(	O
λaaµa	O
+	O
λabµb	O
)	O
+	O
const	O
−1	O
xt	O
a	O
λaaxa	O
+	O
xt	O
2	O
a	O
(	O
λaa	O
−	O
λabλ	O
=	O
−1	O
−1	O
xt	O
bb	O
λba	O
)	O
xa	O
2	O
a	O
(	O
λaa	O
−	O
λabλ	O
−1	O
bb	O
λba	O
)	O
−1µa	O
+	O
const	O
+xt	O
(	O
2.87	O
)	O
where	O
‘	O
const	O
’	O
denotes	O
quantities	O
independent	B
of	O
xa	O
.	O
again	O
,	O
by	O
comparison	O
with	O
(	O
2.71	O
)	O
,	O
we	O
see	O
that	O
the	O
covariance	B
of	O
the	O
marginal	B
distribution	O
of	O
p	O
(	O
xa	O
)	O
is	O
given	O
by	O
σa	O
=	O
(	O
λaa	O
−	O
λabλ	O
−1	O
bb	O
λba	O
)	O
−1	O
.	O
similarly	O
,	O
the	O
mean	B
is	O
given	O
by	O
σa	O
(	O
λaa	O
−	O
λabλ	O
−1	O
bb	O
λba	O
)	O
µa	O
=	O
µa	O
where	O
we	O
have	O
used	O
(	O
2.88	O
)	O
.	O
the	O
covariance	B
in	O
(	O
2.88	O
)	O
is	O
expressed	O
in	O
terms	O
of	O
the	O
partitioned	B
precision	O
matrix	O
given	O
by	O
(	O
2.69	O
)	O
.	O
we	O
can	O
rewrite	O
this	O
in	O
terms	O
of	O
the	O
corresponding	O
partitioning	O
of	O
the	O
covariance	B
matrix	I
given	O
by	O
(	O
2.67	O
)	O
,	O
as	O
we	O
did	O
for	O
the	O
conditional	B
distribution	O
.	O
these	O
partitioned	B
matrices	O
are	O
related	O
by	O
(	O
2.88	O
)	O
(	O
2.89	O
)	O
(	O
2.90	O
)	O
(	O
cid:16	O
)	O
−1	O
λaa	O
λab	O
λba	O
λbb	O
=	O
(	O
cid:15	O
)	O
(	O
cid:10	O
)	O
σaa	O
σab	O
σba	O
σbb	O
(	O
cid:16	O
)	O
(	O
cid:15	O
)	O
(	O
cid:11	O
)	O
−1	O
=	O
σaa	O
.	O
making	O
use	O
of	O
(	O
2.76	O
)	O
,	O
we	O
then	O
have	O
λaa	O
−	O
λabλ	O
−1	O
bb	O
λba	O
(	O
2.91	O
)	O
thus	O
we	O
obtain	O
the	O
intuitively	O
satisfying	O
result	O
that	O
the	O
marginal	B
distribution	O
p	O
(	O
xa	O
)	O
has	O
mean	B
and	O
covariance	B
given	O
by	O
e	O
[	O
xa	O
]	O
=	O
µa	O
cov	O
[	O
xa	O
]	O
=	O
σaa	O
.	O
(	O
2.92	O
)	O
(	O
2.93	O
)	O
we	O
see	O
that	O
for	O
a	O
marginal	B
distribution	O
,	O
the	O
mean	B
and	O
covariance	B
are	O
most	O
simply	O
ex-	O
pressed	O
in	O
terms	O
of	O
the	O
partitioned	B
covariance	O
matrix	O
,	O
in	O
contrast	O
to	O
the	O
conditional	B
distribution	O
for	O
which	O
the	O
partitioned	B
precision	O
matrix	O
gives	O
rise	O
to	O
simpler	O
expres-	O
sions	O
.	O
our	O
results	O
for	O
the	O
marginal	B
and	O
conditional	B
distributions	O
of	O
a	O
partitioned	B
gaus-	O
sian	O
are	O
summarized	O
below	O
.	O
partitioned	B
gaussians	O
given	O
a	O
joint	O
gaussian	O
distribution	O
n	O
(	O
x|µ	O
,	O
σ	O
)	O
with	O
λ	O
≡	O
σ	O
−1	O
and	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
xa	O
xb	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
µa	O
µb	O
x	O
=	O
,	O
µ	O
=	O
(	O
2.94	O
)	O
90	O
2.	O
probability	B
distributions	O
1	O
xb	O
0.5	O
xb	O
=	O
0.7	O
p	O
(	O
xa|xb	O
=	O
0.7	O
)	O
10	O
5	O
p	O
(	O
xa	O
,	O
xb	O
)	O
p	O
(	O
xa	O
)	O
0	O
0	O
0.5	O
xa	O
1	O
0	O
0	O
0.5	O
xa	O
1	O
figure	O
2.9	O
the	O
plot	O
on	O
the	O
left	O
shows	O
the	O
contours	O
of	O
a	O
gaussian	O
distribution	O
p	O
(	O
xa	O
,	O
xb	O
)	O
over	O
two	O
variables	O
,	O
and	O
the	O
plot	O
on	O
the	O
right	O
shows	O
the	O
marginal	B
distribution	O
p	O
(	O
xa	O
)	O
(	O
blue	O
curve	O
)	O
and	O
the	O
conditional	B
distribution	O
p	O
(	O
xa|xb	O
)	O
for	O
xb	O
=	O
0.7	O
(	O
red	O
curve	O
)	O
.	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
σ	O
=	O
σaa	O
σab	O
σba	O
σbb	O
,	O
λ	O
=	O
λaa	O
λab	O
λba	O
λbb	O
conditional	B
distribution	O
:	O
p	O
(	O
xa|xb	O
)	O
=	O
n	O
(	O
x|µa|b	O
,	O
λ	O
−1	O
aa	O
)	O
µa|b	O
=	O
µa	O
−	O
λ	O
aa	O
λab	O
(	O
xb	O
−	O
µb	O
)	O
.	O
−1	O
marginal	B
distribution	O
:	O
p	O
(	O
xa	O
)	O
=	O
n	O
(	O
xa|µa	O
,	O
σaa	O
)	O
.	O
.	O
(	O
2.95	O
)	O
(	O
2.96	O
)	O
(	O
2.97	O
)	O
(	O
2.98	O
)	O
we	O
illustrate	O
the	O
idea	O
of	O
conditional	B
and	O
marginal	B
distributions	O
associated	O
with	O
a	O
multivariate	O
gaussian	O
using	O
an	O
example	O
involving	O
two	O
variables	O
in	O
figure	O
2.9	O
.	O
2.3.3	O
bayes	O
’	O
theorem	O
for	O
gaussian	O
variables	O
in	O
sections	O
2.3.1	O
and	O
2.3.2	O
,	O
we	O
considered	O
a	O
gaussian	O
p	O
(	O
x	O
)	O
in	O
which	O
we	O
parti-	O
tioned	O
the	O
vector	O
x	O
into	O
two	O
subvectors	O
x	O
=	O
(	O
xa	O
,	O
xb	O
)	O
and	O
then	O
found	O
expressions	O
for	O
the	O
conditional	B
distribution	O
p	O
(	O
xa|xb	O
)	O
and	O
the	O
marginal	B
distribution	O
p	O
(	O
xa	O
)	O
.	O
we	O
noted	O
that	O
the	O
mean	B
of	O
the	O
conditional	B
distribution	O
p	O
(	O
xa|xb	O
)	O
was	O
a	O
linear	O
function	O
of	O
xb	O
.	O
here	O
we	O
shall	O
suppose	O
that	O
we	O
are	O
given	O
a	O
gaussian	O
marginal	B
distribution	O
p	O
(	O
x	O
)	O
and	O
a	O
gaussian	O
conditional	B
distribution	O
p	O
(	O
y|x	O
)	O
in	O
which	O
p	O
(	O
y|x	O
)	O
has	O
a	O
mean	B
that	O
is	O
a	O
linear	O
function	O
of	O
x	O
,	O
and	O
a	O
covariance	B
which	O
is	O
independent	B
of	O
x.	O
this	O
is	O
an	O
example	O
of	O
2.3.	O
the	O
gaussian	O
distribution	O
91	O
a	O
linear	O
gaussian	O
model	O
(	O
roweis	O
and	O
ghahramani	O
,	O
1999	O
)	O
,	O
which	O
we	O
shall	O
study	O
in	O
greater	O
generality	O
in	O
section	O
8.1.4.	O
we	O
wish	O
to	O
ﬁnd	O
the	O
marginal	B
distribution	O
p	O
(	O
y	O
)	O
and	O
the	O
conditional	B
distribution	O
p	O
(	O
x|y	O
)	O
.	O
this	O
is	O
a	O
problem	O
that	O
will	O
arise	O
frequently	O
in	O
subsequent	O
chapters	O
,	O
and	O
it	O
will	O
prove	O
convenient	O
to	O
derive	O
the	O
general	O
results	O
here	O
.	O
we	O
shall	O
take	O
the	O
marginal	B
and	O
conditional	B
distributions	O
to	O
be	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
cid:10	O
)	O
p	O
(	O
y|x	O
)	O
=	O
n	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
(	O
cid:11	O
)	O
−1	O
x|µ	O
,	O
λ	O
y|ax	O
+	O
b	O
,	O
l−1	O
where	O
µ	O
,	O
a	O
,	O
and	O
b	O
are	O
parameters	O
governing	O
the	O
means	O
,	O
and	O
λ	O
and	O
l	O
are	O
precision	O
matrices	O
.	O
if	O
x	O
has	O
dimensionality	O
m	O
and	O
y	O
has	O
dimensionality	O
d	O
,	O
then	O
the	O
matrix	O
a	O
has	O
size	O
d	O
×	O
m.	O
first	O
we	O
ﬁnd	O
an	O
expression	O
for	O
the	O
joint	O
distribution	O
over	O
x	O
and	O
y.	O
to	O
do	O
this	O
,	O
we	O
deﬁne	O
z	O
=	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
x	O
y	O
(	O
2.99	O
)	O
(	O
2.100	O
)	O
(	O
2.101	O
)	O
and	O
then	O
consider	O
the	O
log	O
of	O
the	O
joint	O
distribution	O
ln	O
p	O
(	O
z	O
)	O
=	O
ln	O
p	O
(	O
x	O
)	O
+	O
ln	O
p	O
(	O
y|x	O
)	O
=	O
−1	O
2	O
−1	O
2	O
(	O
x	O
−	O
µ	O
)	O
tλ	O
(	O
x	O
−	O
µ	O
)	O
(	O
y	O
−	O
ax	O
−	O
b	O
)	O
tl	O
(	O
y	O
−	O
ax	O
−	O
b	O
)	O
+	O
const	O
(	O
2.102	O
)	O
where	O
‘	O
const	O
’	O
denotes	O
terms	O
independent	B
of	O
x	O
and	O
y.	O
as	O
before	O
,	O
we	O
see	O
that	O
this	O
is	O
a	O
quadratic	O
function	O
of	O
the	O
components	O
of	O
z	O
,	O
and	O
hence	O
p	O
(	O
z	O
)	O
is	O
gaussian	O
distribution	O
.	O
to	O
ﬁnd	O
the	O
precision	O
of	O
this	O
gaussian	O
,	O
we	O
consider	O
the	O
second	B
order	I
terms	O
in	O
(	O
2.102	O
)	O
,	O
which	O
can	O
be	O
written	O
as	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
t	O
(	O
cid:15	O
)	O
xt	O
(	O
λ	O
+	O
atla	O
)	O
x	O
−	O
1	O
−1	O
2	O
2	O
=	O
−1	O
−la	O
2	O
x	O
y	O
ytly	O
+	O
ytlax	O
+	O
λ	O
+	O
atla	O
−atl	O
1	O
2	O
l	O
1	O
xtatly	O
2	O
=	O
−1	O
2	O
ztrz	O
(	O
2.103	O
)	O
and	O
so	O
the	O
gaussian	O
distribution	O
over	O
z	O
has	O
precision	O
(	O
inverse	B
covariance	O
)	O
matrix	O
given	O
by	O
r	O
=	O
λ	O
+	O
atla	O
−atl	O
−la	O
.	O
(	O
2.104	O
)	O
l	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
x	O
y	O
(	O
cid:15	O
)	O
(	O
cid:15	O
)	O
exercise	O
2.29	O
the	O
covariance	B
matrix	I
is	O
found	O
by	O
taking	O
the	O
inverse	B
of	O
the	O
precision	O
,	O
which	O
can	O
be	O
done	O
using	O
the	O
matrix	O
inversion	O
formula	O
(	O
2.76	O
)	O
to	O
give	O
cov	O
[	O
z	O
]	O
=	O
r−1	O
=	O
−1	O
−1at	O
−1	O
l−1	O
+	O
aλ	O
λ	O
λ	O
aλ	O
−1at	O
.	O
(	O
2.105	O
)	O
(	O
cid:16	O
)	O
92	O
2.	O
probability	B
distributions	O
similarly	O
,	O
we	O
can	O
ﬁnd	O
the	O
mean	B
of	O
the	O
gaussian	O
distribution	O
over	O
z	O
by	O
identify-	O
ing	O
the	O
linear	O
terms	O
in	O
(	O
2.102	O
)	O
,	O
which	O
are	O
given	O
by	O
xtλµ	O
−	O
xtatlb	O
+	O
ytlb	O
=	O
λµ	O
−	O
atlb	O
lb	O
.	O
(	O
2.106	O
)	O
(	O
cid:16	O
)	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
t	O
(	O
cid:15	O
)	O
x	O
y	O
(	O
cid:16	O
)	O
λµ	O
−	O
atlb	O
lb	O
(	O
cid:15	O
)	O
(	O
cid:15	O
)	O
using	O
our	O
earlier	O
result	O
(	O
2.71	O
)	O
obtained	O
by	O
completing	B
the	I
square	I
over	O
the	O
quadratic	O
form	O
of	O
a	O
multivariate	O
gaussian	O
,	O
we	O
ﬁnd	O
that	O
the	O
mean	B
of	O
z	O
is	O
given	O
by	O
e	O
[	O
z	O
]	O
=	O
r−1	O
exercise	O
2.30	O
making	O
use	O
of	O
(	O
2.105	O
)	O
,	O
we	O
then	O
obtain	O
e	O
[	O
z	O
]	O
=	O
(	O
cid:16	O
)	O
.	O
.	O
(	O
2.107	O
)	O
(	O
2.108	O
)	O
µ	O
aµ	O
+	O
b	O
section	O
2.3	O
section	O
2.3	O
next	O
we	O
ﬁnd	O
an	O
expression	O
for	O
the	O
marginal	B
distribution	O
p	O
(	O
y	O
)	O
in	O
which	O
we	O
have	O
marginalized	O
over	O
x.	O
recall	O
that	O
the	O
marginal	B
distribution	O
over	O
a	O
subset	O
of	O
the	O
com-	O
ponents	O
of	O
a	O
gaussian	O
random	O
vector	O
takes	O
a	O
particularly	O
simple	O
form	O
when	O
ex-	O
pressed	O
in	O
terms	O
of	O
the	O
partitioned	B
covariance	O
matrix	O
.	O
speciﬁcally	O
,	O
its	O
mean	B
and	O
covariance	B
are	O
given	O
by	O
(	O
2.92	O
)	O
and	O
(	O
2.93	O
)	O
,	O
respectively	O
.	O
making	O
use	O
of	O
(	O
2.105	O
)	O
and	O
(	O
2.108	O
)	O
we	O
see	O
that	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
marginal	B
distribution	O
p	O
(	O
y	O
)	O
are	O
given	O
by	O
e	O
[	O
y	O
]	O
=	O
aµ	O
+	O
b	O
cov	O
[	O
y	O
]	O
=	O
l−1	O
+	O
aλ	O
−1at	O
.	O
(	O
2.109	O
)	O
(	O
2.110	O
)	O
a	O
special	O
case	O
of	O
this	O
result	O
is	O
when	O
a	O
=	O
i	O
,	O
in	O
which	O
case	O
it	O
reduces	O
to	O
the	O
convolu-	O
tion	O
of	O
two	O
gaussians	O
,	O
for	O
which	O
we	O
see	O
that	O
the	O
mean	B
of	O
the	O
convolution	O
is	O
the	O
sum	O
of	O
the	O
mean	B
of	O
the	O
two	O
gaussians	O
,	O
and	O
the	O
covariance	B
of	O
the	O
convolution	O
is	O
the	O
sum	O
of	O
their	O
covariances	O
.	O
finally	O
,	O
we	O
seek	O
an	O
expression	O
for	O
the	O
conditional	B
p	O
(	O
x|y	O
)	O
.	O
recall	O
that	O
the	O
results	O
for	O
the	O
conditional	B
distribution	O
are	O
most	O
easily	O
expressed	O
in	O
terms	O
of	O
the	O
partitioned	B
precision	O
matrix	O
,	O
using	O
(	O
2.73	O
)	O
and	O
(	O
2.75	O
)	O
.	O
applying	O
these	O
results	O
to	O
(	O
2.105	O
)	O
and	O
(	O
2.108	O
)	O
we	O
see	O
that	O
the	O
conditional	B
distribution	O
p	O
(	O
x|y	O
)	O
has	O
mean	B
and	O
covariance	B
given	O
by	O
e	O
[	O
x|y	O
]	O
=	O
(	O
λ	O
+	O
atla	O
)	O
−1	O
cov	O
[	O
x|y	O
]	O
=	O
(	O
λ	O
+	O
atla	O
)	O
−1	O
.	O
atl	O
(	O
y	O
−	O
b	O
)	O
+	O
λµ	O
(	O
2.111	O
)	O
(	O
2.112	O
)	O
(	O
cid:26	O
)	O
(	O
cid:27	O
)	O
the	O
evaluation	O
of	O
this	O
conditional	B
can	O
be	O
seen	O
as	O
an	O
example	O
of	O
bayes	O
’	O
theorem	O
.	O
we	O
can	O
interpret	O
the	O
distribution	O
p	O
(	O
x	O
)	O
as	O
a	O
prior	B
distribution	O
over	O
x.	O
if	O
the	O
variable	O
y	O
is	O
observed	O
,	O
then	O
the	O
conditional	B
distribution	O
p	O
(	O
x|y	O
)	O
represents	O
the	O
corresponding	O
posterior	O
distribution	O
over	O
x.	O
having	O
found	O
the	O
marginal	B
and	O
conditional	B
distribu-	O
tions	O
,	O
we	O
effectively	O
expressed	O
the	O
joint	O
distribution	O
p	O
(	O
z	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y|x	O
)	O
in	O
the	O
form	O
p	O
(	O
x|y	O
)	O
p	O
(	O
y	O
)	O
.	O
these	O
results	O
are	O
summarized	O
below	O
.	O
2.3.	O
the	O
gaussian	O
distribution	O
93	O
marginal	B
and	O
conditional	B
gaussians	O
given	O
a	O
marginal	B
gaussian	O
distribution	O
for	O
x	O
and	O
a	O
conditional	B
gaussian	O
distri-	O
bution	O
for	O
y	O
given	O
x	O
in	O
the	O
form	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
x|µ	O
,	O
λ	O
p	O
(	O
y|x	O
)	O
=	O
n	O
(	O
y|ax	O
+	O
b	O
,	O
l−1	O
)	O
−1	O
)	O
(	O
2.113	O
)	O
(	O
2.114	O
)	O
the	O
marginal	B
distribution	O
of	O
y	O
and	O
the	O
conditional	B
distribution	O
of	O
x	O
given	O
y	O
are	O
given	O
by	O
p	O
(	O
y	O
)	O
=	O
n	O
(	O
y|aµ	O
+	O
b	O
,	O
l−1	O
+	O
aλ	O
−1at	O
)	O
p	O
(	O
x|y	O
)	O
=	O
n	O
(	O
x|σ	O
{	O
atl	O
(	O
y	O
−	O
b	O
)	O
+	O
λµ	O
}	O
,	O
σ	O
)	O
where	O
σ	O
=	O
(	O
λ	O
+	O
atla	O
)	O
−1	O
.	O
(	O
2.115	O
)	O
(	O
2.116	O
)	O
(	O
2.117	O
)	O
2.3.4	O
maximum	B
likelihood	I
for	O
the	O
gaussian	O
given	O
a	O
data	O
set	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
t	O
in	O
which	O
the	O
observations	O
{	O
xn	O
}	O
are	O
as-	O
sumed	O
to	O
be	O
drawn	O
independently	O
from	O
a	O
multivariate	O
gaussian	O
distribution	O
,	O
we	O
can	O
estimate	O
the	O
parameters	O
of	O
the	O
distribution	O
by	O
maximum	B
likelihood	I
.	O
the	O
log	O
likeli-	O
hood	O
function	O
is	O
given	O
by	O
ln	O
p	O
(	O
x|µ	O
,	O
σ	O
)	O
=	O
−	O
n	O
d	O
2	O
ln	O
(	O
2π	O
)	O
−	O
n	O
2	O
ln|σ|−1	O
2	O
(	O
xn−µ	O
)	O
tς	O
−1	O
(	O
xn−µ	O
)	O
.	O
(	O
2.118	O
)	O
by	O
simple	O
rearrangement	O
,	O
we	O
see	O
that	O
the	O
likelihood	B
function	I
depends	O
on	O
the	O
data	O
set	O
only	O
through	O
the	O
two	O
quantities	O
n	O
(	O
cid:2	O
)	O
xn	O
,	O
n=1	O
n=1	O
xnxt	O
n.	O
(	O
2.119	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
n	O
(	O
cid:2	O
)	O
n=1	O
appendix	O
c	O
these	O
are	O
known	O
as	O
the	O
sufﬁcient	B
statistics	I
for	O
the	O
gaussian	O
distribution	O
.	O
using	O
(	O
c.19	O
)	O
,	O
the	O
derivative	B
of	O
the	O
log	O
likelihood	O
with	O
respect	O
to	O
µ	O
is	O
given	O
by	O
ln	O
p	O
(	O
x|µ	O
,	O
σ	O
)	O
=	O
∂	O
∂µ	O
−1	O
(	O
xn	O
−	O
µ	O
)	O
σ	O
(	O
2.120	O
)	O
and	O
setting	O
this	O
derivative	B
to	O
zero	O
,	O
we	O
obtain	O
the	O
solution	O
for	O
the	O
maximum	B
likelihood	I
estimate	O
of	O
the	O
mean	B
given	O
by	O
µml	O
=	O
1	O
n	O
xn	O
(	O
2.121	O
)	O
94	O
2.	O
probability	B
distributions	O
exercise	O
2.34	O
which	O
is	O
the	O
mean	B
of	O
the	O
observed	O
set	O
of	O
data	O
points	O
.	O
the	O
maximization	O
of	O
(	O
2.118	O
)	O
with	O
respect	O
to	O
σ	O
is	O
rather	O
more	O
involved	O
.	O
the	O
simplest	O
approach	O
is	O
to	O
ignore	O
the	O
symmetry	O
constraint	O
and	O
show	O
that	O
the	O
resulting	O
solution	O
is	O
symmetric	O
as	O
required	O
.	O
alternative	O
derivations	O
of	O
this	O
result	O
,	O
which	O
impose	O
the	O
symmetry	O
and	O
positive	O
deﬁ-	O
niteness	O
constraints	O
explicitly	O
,	O
can	O
be	O
found	O
in	O
magnus	O
and	O
neudecker	O
(	O
1999	O
)	O
.	O
the	O
result	O
is	O
as	O
expected	O
and	O
takes	O
the	O
form	O
n	O
(	O
cid:2	O
)	O
n=1	O
σml	O
=	O
1	O
n	O
(	O
xn	O
−	O
µml	O
)	O
(	O
xn	O
−	O
µml	O
)	O
t	O
(	O
2.122	O
)	O
which	O
involves	O
µml	O
because	O
this	O
is	O
the	O
result	O
of	O
a	O
joint	O
maximization	O
with	O
respect	O
to	O
µ	O
and	O
σ.	O
note	O
that	O
the	O
solution	O
(	O
2.121	O
)	O
for	O
µml	O
does	O
not	O
depend	O
on	O
σml	O
,	O
and	O
so	O
we	O
can	O
ﬁrst	O
evaluate	O
µml	O
and	O
then	O
use	O
this	O
to	O
evaluate	O
σml	O
.	O
if	O
we	O
evaluate	O
the	O
expectations	O
of	O
the	O
maximum	B
likelihood	I
solutions	O
under	O
the	O
exercise	O
2.35	O
true	O
distribution	O
,	O
we	O
obtain	O
the	O
following	O
results	O
e	O
[	O
µml	O
]	O
=	O
µ	O
e	O
[	O
σml	O
]	O
=	O
n	O
−	O
1	O
this	O
bias	B
by	O
deﬁning	O
a	O
different	O
estimator	O
(	O
cid:4	O
)	O
σ	O
given	O
by	O
we	O
see	O
that	O
the	O
expectation	B
of	O
the	O
maximum	B
likelihood	I
estimate	O
for	O
the	O
mean	B
is	O
equal	O
to	O
the	O
true	O
mean	B
.	O
however	O
,	O
the	O
maximum	B
likelihood	I
estimate	O
for	O
the	O
covariance	B
has	O
an	O
expectation	B
that	O
is	O
less	O
than	O
the	O
true	O
value	O
,	O
and	O
hence	O
it	O
is	O
biased	O
.	O
we	O
can	O
correct	O
σ.	O
n	O
(	O
2.123	O
)	O
(	O
2.124	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:4	O
)	O
σ	O
=	O
clearly	O
from	O
(	O
2.122	O
)	O
and	O
(	O
2.124	O
)	O
,	O
the	O
expectation	B
of	O
(	O
cid:4	O
)	O
σ	O
is	O
equal	O
to	O
σ.	O
n=1	O
(	O
xn	O
−	O
µml	O
)	O
(	O
xn	O
−	O
µml	O
)	O
t.	O
1	O
n	O
−	O
1	O
(	O
2.125	O
)	O
2.3.5	O
sequential	B
estimation	I
our	O
discussion	O
of	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
parameters	O
of	O
a	O
gaus-	O
sian	O
distribution	O
provides	O
a	O
convenient	O
opportunity	O
to	O
give	O
a	O
more	O
general	O
discussion	O
of	O
the	O
topic	O
of	O
sequential	B
estimation	I
for	O
maximum	B
likelihood	I
.	O
sequential	O
methods	O
allow	O
data	O
points	O
to	O
be	O
processed	O
one	O
at	O
a	O
time	O
and	O
then	O
discarded	O
and	O
are	O
important	O
for	O
on-line	O
applications	O
,	O
and	O
also	O
where	O
large	O
data	O
sets	O
are	O
involved	O
so	O
that	O
batch	O
processing	O
of	O
all	O
data	O
points	O
at	O
once	O
is	O
infeasible	O
.	O
consider	O
the	O
result	O
(	O
2.121	O
)	O
for	O
the	O
maximum	B
likelihood	I
estimator	O
of	O
the	O
mean	B
ml	O
when	O
it	O
is	O
based	O
on	O
n	O
observations	O
.	O
if	O
we	O
µml	O
,	O
which	O
we	O
will	O
denote	O
by	O
µ	O
(	O
n	O
)	O
2.3.	O
the	O
gaussian	O
distribution	O
95	O
figure	O
2.10	O
a	O
schematic	O
illustration	O
of	O
two	O
correlated	O
ran-	O
dom	O
variables	O
z	O
and	O
θ	O
,	O
together	O
with	O
the	O
regression	B
function	I
f	O
(	O
θ	O
)	O
given	O
by	O
the	O
con-	O
ditional	O
expectation	B
e	O
[	O
z|θ	O
]	O
.	O
the	O
robbins-	O
monro	O
algorithm	O
provides	O
a	O
general	O
sequen-	O
tial	O
procedure	O
for	O
ﬁnding	O
the	O
root	O
θ	O
(	O
cid:1	O
)	O
of	O
such	O
functions	O
.	O
z	O
f	O
(	O
θ	O
)	O
θ	O
θ	O
(	O
cid:1	O
)	O
dissect	O
out	O
the	O
contribution	O
from	O
the	O
ﬁnal	O
data	O
point	O
xn	O
,	O
we	O
obtain	O
n	O
(	O
cid:2	O
)	O
µ	O
(	O
n	O
)	O
ml	O
=	O
=	O
1	O
n	O
n=1	O
xn	O
1	O
n	O
xn	O
+	O
n−1	O
(	O
cid:2	O
)	O
1	O
n	O
xn	O
+	O
n	O
−	O
1	O
1	O
n	O
ml	O
+	O
n=1	O
=	O
=	O
µ	O
(	O
n−1	O
)	O
xn	O
µ	O
(	O
n−1	O
)	O
ml	O
n	O
(	O
xn	O
−	O
µ	O
(	O
n−1	O
)	O
1	O
n	O
)	O
.	O
(	O
2.126	O
)	O
this	O
result	O
has	O
a	O
nice	O
interpretation	O
,	O
as	O
follows	O
.	O
after	O
observing	O
n	O
−	O
1	O
data	O
points	O
we	O
have	O
estimated	O
µ	O
by	O
µ	O
(	O
n−1	O
)	O
.	O
we	O
now	O
observe	O
data	O
point	O
xn	O
,	O
and	O
we	O
obtain	O
our	O
revised	O
estimate	O
µ	O
(	O
n	O
)	O
ml	O
by	O
moving	O
the	O
old	O
estimate	O
a	O
small	O
amount	O
,	O
proportional	O
to	O
1/n	O
,	O
in	O
the	O
direction	O
of	O
the	O
‘	O
error	B
signal	O
’	O
(	O
xn	O
−	O
µ	O
(	O
n−1	O
)	O
)	O
.	O
note	O
that	O
,	O
as	O
n	O
increases	O
,	O
so	O
the	O
contribution	O
from	O
successive	O
data	O
points	O
gets	O
smaller	O
.	O
ml	O
ml	O
ml	O
the	O
result	O
(	O
2.126	O
)	O
will	O
clearly	O
give	O
the	O
same	O
answer	O
as	O
the	O
batch	O
result	O
(	O
2.121	O
)	O
because	O
the	O
two	O
formulae	O
are	O
equivalent	O
.	O
however	O
,	O
we	O
will	O
not	O
always	O
be	O
able	O
to	O
de-	O
rive	O
a	O
sequential	O
algorithm	O
by	O
this	O
route	O
,	O
and	O
so	O
we	O
seek	O
a	O
more	O
general	O
formulation	O
of	O
sequential	B
learning	I
,	O
which	O
leads	O
us	O
to	O
the	O
robbins-monro	O
algorithm	O
.	O
consider	O
a	O
pair	O
of	O
random	O
variables	O
θ	O
and	O
z	O
governed	O
by	O
a	O
joint	O
distribution	O
p	O
(	O
z	O
,	O
θ	O
)	O
.	O
the	O
con-	O
ditional	O
expectation	B
of	O
z	O
given	O
θ	O
deﬁnes	O
a	O
deterministic	O
function	O
f	O
(	O
θ	O
)	O
that	O
is	O
given	O
by	O
(	O
cid:6	O
)	O
f	O
(	O
θ	O
)	O
≡	O
e	O
[	O
z|θ	O
]	O
=	O
zp	O
(	O
z|θ	O
)	O
dz	O
(	O
2.127	O
)	O
and	O
is	O
illustrated	O
schematically	O
in	O
figure	O
2.10.	O
functions	O
deﬁned	O
in	O
this	O
way	O
are	O
called	O
regression	B
functions	O
.	O
our	O
goal	O
is	O
to	O
ﬁnd	O
the	O
root	O
θ	O
(	O
cid:1	O
)	O
at	O
which	O
f	O
(	O
θ	O
(	O
cid:1	O
)	O
)	O
=	O
0.	O
if	O
we	O
had	O
a	O
large	O
data	O
set	O
of	O
observations	O
of	O
z	O
and	O
θ	O
,	O
then	O
we	O
could	O
model	O
the	O
regression	B
function	I
directly	O
and	O
then	O
obtain	O
an	O
estimate	O
of	O
its	O
root	O
.	O
suppose	O
,	O
however	O
,	O
that	O
we	O
observe	O
values	O
of	O
z	O
one	O
at	O
a	O
time	O
and	O
we	O
wish	O
to	O
ﬁnd	O
a	O
corresponding	O
sequential	B
estimation	I
scheme	O
for	O
θ	O
(	O
cid:1	O
)	O
.	O
the	O
following	O
general	O
procedure	O
for	O
solving	O
such	O
problems	O
was	O
given	O
by	O
96	O
2.	O
probability	B
distributions	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
robbins	O
and	O
monro	O
(	O
1951	O
)	O
.	O
we	O
shall	O
assume	O
that	O
the	O
conditional	B
variance	O
of	O
z	O
is	O
ﬁnite	O
so	O
that	O
(	O
2.128	O
)	O
and	O
we	O
shall	O
also	O
,	O
without	O
loss	O
of	O
generality	O
,	O
consider	O
the	O
case	O
where	O
f	O
(	O
θ	O
)	O
>	O
0	O
for	O
θ	O
>	O
θ	O
(	O
cid:1	O
)	O
and	O
f	O
(	O
θ	O
)	O
<	O
0	O
for	O
θ	O
<	O
θ	O
(	O
cid:1	O
)	O
,	O
as	O
is	O
the	O
case	O
in	O
figure	O
2.10.	O
the	O
robbins-monro	O
procedure	O
then	O
deﬁnes	O
a	O
sequence	O
of	O
successive	O
estimates	O
of	O
the	O
root	O
θ	O
(	O
cid:1	O
)	O
given	O
by	O
e	O
(	O
z	O
−	O
f	O
)	O
2	O
|	O
θ	O
<	O
∞	O
θ	O
(	O
n	O
)	O
=	O
θ	O
(	O
n−1	O
)	O
+	O
an−1z	O
(	O
θ	O
(	O
n−1	O
)	O
)	O
(	O
2.129	O
)	O
where	O
z	O
(	O
θ	O
(	O
n	O
)	O
)	O
is	O
an	O
observed	O
value	O
of	O
z	O
when	O
θ	O
takes	O
the	O
value	O
θ	O
(	O
n	O
)	O
.	O
the	O
coefﬁcients	O
{	O
an	O
}	O
represent	O
a	O
sequence	O
of	O
positive	O
numbers	O
that	O
satisfy	O
the	O
conditions	O
n→∞	O
an	O
=	O
0	O
lim	O
an	O
=	O
∞	O
∞	O
(	O
cid:2	O
)	O
∞	O
(	O
cid:2	O
)	O
n	O
=1	O
n	O
<	O
∞	O
.	O
a2	O
(	O
2.130	O
)	O
(	O
2.131	O
)	O
(	O
2.132	O
)	O
n	O
=1	O
it	O
can	O
then	O
be	O
shown	O
(	O
robbins	O
and	O
monro	O
,	O
1951	O
;	O
fukunaga	O
,	O
1990	O
)	O
that	O
the	O
sequence	O
of	O
estimates	O
given	O
by	O
(	O
2.129	O
)	O
does	O
indeed	O
converge	O
to	O
the	O
root	O
with	O
probability	B
one	O
.	O
note	O
that	O
the	O
ﬁrst	O
condition	O
(	O
2.130	O
)	O
ensures	O
that	O
the	O
successive	O
corrections	O
decrease	O
in	O
magnitude	O
so	O
that	O
the	O
process	O
can	O
converge	O
to	O
a	O
limiting	O
value	O
.	O
the	O
second	O
con-	O
dition	O
(	O
2.131	O
)	O
is	O
required	O
to	O
ensure	O
that	O
the	O
algorithm	O
does	O
not	O
converge	O
short	O
of	O
the	O
root	O
,	O
and	O
the	O
third	O
condition	O
(	O
2.132	O
)	O
is	O
needed	O
to	O
ensure	O
that	O
the	O
accumulated	O
noise	O
has	O
ﬁnite	O
variance	O
and	O
hence	O
does	O
not	O
spoil	O
convergence	O
.	O
now	O
let	O
us	O
consider	O
how	O
a	O
general	O
maximum	B
likelihood	I
problem	O
can	O
be	O
solved	O
sequentially	O
using	O
the	O
robbins-monro	O
algorithm	O
.	O
by	O
deﬁnition	O
,	O
the	O
maximum	O
like-	O
lihood	O
solution	O
θml	O
is	O
a	O
stationary	B
point	O
of	O
the	O
log	O
likelihood	O
function	O
and	O
hence	O
satisﬁes	O
(	O
cid:24	O
)	O
(	O
2.133	O
)	O
exchanging	O
the	O
derivative	B
and	O
the	O
summation	O
,	O
and	O
taking	O
the	O
limit	O
n	O
→	O
∞	O
we	O
have	O
θml	O
n=1	O
=	O
0	O
.	O
∂	O
∂θ	O
1	O
n	O
ln	O
p	O
(	O
xn|θ	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:25	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:29	O
)	O
(	O
cid:30	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
lim	O
n→∞	O
1	O
n	O
ln	O
p	O
(	O
xn|θ	O
)	O
=	O
ex	O
∂	O
∂θ	O
∂	O
∂θ	O
ln	O
p	O
(	O
x|θ	O
)	O
(	O
2.134	O
)	O
and	O
so	O
we	O
see	O
that	O
ﬁnding	O
the	O
maximum	B
likelihood	I
solution	O
corresponds	O
to	O
ﬁnd-	O
ing	O
the	O
root	O
of	O
a	O
regression	B
function	I
.	O
we	O
can	O
therefore	O
apply	O
the	O
robbins-monro	O
procedure	O
,	O
which	O
now	O
takes	O
the	O
form	O
θ	O
(	O
n	O
)	O
=	O
θ	O
(	O
n−1	O
)	O
+	O
an−1	O
∂	O
∂θ	O
(	O
n−1	O
)	O
ln	O
p	O
(	O
xn|θ	O
(	O
n−1	O
)	O
)	O
.	O
(	O
2.135	O
)	O
2.3.	O
the	O
gaussian	O
distribution	O
97	O
z	O
figure	O
2.11	O
in	O
the	O
case	O
of	O
a	O
gaussian	O
distribution	O
,	O
with	O
θ	O
corresponding	O
to	O
the	O
mean	B
µ	O
,	O
the	O
regression	B
function	I
illustrated	O
in	O
figure	O
2.10	O
takes	O
the	O
form	O
of	O
a	O
straight	O
line	O
,	O
as	O
shown	O
in	O
red	O
.	O
in	O
this	O
case	O
,	O
the	O
random	O
variable	O
z	O
corresponds	O
to	O
the	O
derivative	B
of	O
the	O
log	O
likelihood	O
function	O
and	O
is	O
given	O
by	O
(	O
x	O
−	O
µml	O
)	O
/σ2	O
,	O
and	O
its	O
expectation	B
that	O
deﬁnes	O
the	O
regression	B
function	I
is	O
a	O
straight	O
line	O
given	O
by	O
(	O
µ	O
−	O
µml	O
)	O
/σ2	O
.	O
the	O
root	O
of	O
the	O
regres-	O
sion	B
function	O
corresponds	O
to	O
the	O
maximum	O
like-	O
lihood	O
estimator	O
µml	O
.	O
µml	O
p	O
(	O
z|µ	O
)	O
µ	O
as	O
a	O
speciﬁc	O
example	O
,	O
we	O
consider	O
once	O
again	O
the	O
sequential	B
estimation	I
of	O
the	O
mean	B
of	O
a	O
gaussian	O
distribution	O
,	O
in	O
which	O
case	O
the	O
parameter	O
θ	O
(	O
n	O
)	O
is	O
the	O
estimate	O
(	O
n	O
)	O
ml	O
of	O
the	O
mean	B
of	O
the	O
gaussian	O
,	O
and	O
the	O
random	O
variable	O
z	O
is	O
given	O
by	O
µ	O
1	O
σ2	O
(	O
x	O
−	O
µml	O
)	O
.	O
z	O
=	O
∂	O
ln	O
p	O
(	O
x|µml	O
,	O
σ2	O
)	O
=	O
∂µml	O
(	O
2.136	O
)	O
thus	O
the	O
distribution	O
of	O
z	O
is	O
gaussian	O
with	O
mean	B
µ	O
−	O
µml	O
,	O
as	O
illustrated	O
in	O
fig-	O
ure	O
2.11.	O
substituting	O
(	O
2.136	O
)	O
into	O
(	O
2.135	O
)	O
,	O
we	O
obtain	O
the	O
univariate	O
form	O
of	O
(	O
2.126	O
)	O
,	O
provided	O
we	O
choose	O
the	O
coefﬁcients	O
an	O
to	O
have	O
the	O
form	O
an	O
=	O
σ2/n	O
.	O
note	O
that	O
although	O
we	O
have	O
focussed	O
on	O
the	O
case	O
of	O
a	O
single	O
variable	O
,	O
the	O
same	O
technique	O
,	O
together	O
with	O
the	O
same	O
restrictions	O
(	O
2.130	O
)	O
–	O
(	O
2.132	O
)	O
on	O
the	O
coefﬁcients	O
an	O
,	O
apply	O
equally	O
to	O
the	O
multivariate	O
case	O
(	O
blum	O
,	O
1965	O
)	O
.	O
2.3.6	O
bayesian	O
inference	B
for	O
the	O
gaussian	O
the	O
maximum	B
likelihood	I
framework	O
gave	O
point	O
estimates	O
for	O
the	O
parameters	O
µ	O
and	O
σ.	O
now	O
we	O
develop	O
a	O
bayesian	O
treatment	O
by	O
introducing	O
prior	B
distributions	O
over	O
these	O
parameters	O
.	O
let	O
us	O
begin	O
with	O
a	O
simple	O
example	O
in	O
which	O
we	O
consider	O
a	O
single	O
gaussian	O
random	O
variable	O
x.	O
we	O
shall	O
suppose	O
that	O
the	O
variance	B
σ2	O
is	O
known	O
,	O
and	O
we	O
consider	O
the	O
task	O
of	O
inferring	O
the	O
mean	B
µ	O
given	O
a	O
set	O
of	O
n	O
observations	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
.	O
the	O
likelihood	B
function	I
,	O
that	O
is	O
the	O
probability	B
of	O
the	O
observed	O
data	O
given	O
µ	O
,	O
viewed	O
as	O
a	O
function	O
of	O
µ	O
,	O
is	O
given	O
by	O
1	O
p	O
(	O
x|µ	O
)	O
=	O
p	O
(	O
xn|µ	O
)	O
=	O
(	O
2.137	O
)	O
again	O
we	O
emphasize	O
that	O
the	O
likelihood	B
function	I
p	O
(	O
x|µ	O
)	O
is	O
not	O
a	O
probability	B
distri-	O
bution	O
over	O
µ	O
and	O
is	O
not	O
normalized	O
.	O
(	O
2πσ2	O
)	O
n/2	O
exp	O
n=1	O
n=1	O
we	O
see	O
that	O
the	O
likelihood	B
function	I
takes	O
the	O
form	O
of	O
the	O
exponential	O
of	O
a	O
quad-	O
ratic	O
form	O
in	O
µ.	O
thus	O
if	O
we	O
choose	O
a	O
prior	B
p	O
(	O
µ	O
)	O
given	O
by	O
a	O
gaussian	O
,	O
it	O
will	O
be	O
a	O
−	O
1	O
2σ2	O
(	O
xn	O
−	O
µ	O
)	O
2	O
.	O
n	O
(	O
cid:14	O
)	O
(	O
cid:24	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:25	O
)	O
98	O
2.	O
probability	B
distributions	O
conjugate	B
distribution	O
for	O
this	O
likelihood	B
function	I
because	O
the	O
corresponding	O
poste-	O
rior	O
will	O
be	O
a	O
product	O
of	O
two	O
exponentials	O
of	O
quadratic	O
functions	O
of	O
µ	O
and	O
hence	O
will	O
also	O
be	O
gaussian	O
.	O
we	O
therefore	O
take	O
our	O
prior	B
distribution	O
to	O
be	O
p	O
(	O
µ	O
)	O
=	O
n	O
(	O
cid:10	O
)	O
µ|µ0	O
,	O
σ2	O
0	O
(	O
cid:11	O
)	O
and	O
the	O
posterior	O
distribution	O
is	O
given	O
by	O
p	O
(	O
µ|x	O
)	O
∝	O
p	O
(	O
x|µ	O
)	O
p	O
(	O
µ	O
)	O
.	O
exercise	O
2.38	O
simple	O
manipulation	O
involving	O
completing	B
the	I
square	I
in	O
the	O
exponent	O
shows	O
that	O
the	O
posterior	O
distribution	O
is	O
given	O
by	O
p	O
(	O
µ|x	O
)	O
=	O
n	O
(	O
cid:10	O
)	O
µ|µn	O
,	O
σ2	O
n	O
(	O
cid:11	O
)	O
(	O
2.138	O
)	O
(	O
2.139	O
)	O
(	O
2.140	O
)	O
(	O
2.141	O
)	O
(	O
2.142	O
)	O
where	O
µn	O
=	O
1	O
σ2	O
n	O
=	O
σ2	O
0	O
+	O
σ2	O
µ0	O
+	O
n	O
σ2	O
+	O
n	O
σ2	O
n	O
σ2	O
n	O
σ2	O
1	O
σ2	O
0	O
0	O
0	O
+	O
σ2	O
µml	O
in	O
which	O
µml	O
is	O
the	O
maximum	B
likelihood	I
solution	O
for	O
µ	O
given	O
by	O
the	O
sample	B
mean	I
n	O
(	O
cid:2	O
)	O
µml	O
=	O
1	O
n	O
xn	O
.	O
n=1	O
(	O
2.143	O
)	O
it	O
is	O
worth	O
spending	O
a	O
moment	O
studying	O
the	O
form	O
of	O
the	O
posterior	O
mean	O
and	O
variance	B
.	O
first	O
of	O
all	O
,	O
we	O
note	O
that	O
the	O
mean	B
of	O
the	O
posterior	O
distribution	O
given	O
by	O
(	O
2.141	O
)	O
is	O
a	O
compromise	O
between	O
the	O
prior	B
mean	O
µ0	O
and	O
the	O
maximum	B
likelihood	I
solution	O
µml	O
.	O
if	O
the	O
number	O
of	O
observed	O
data	O
points	O
n	O
=	O
0	O
,	O
then	O
(	O
2.141	O
)	O
reduces	O
to	O
the	O
prior	B
mean	O
as	O
expected	O
.	O
for	O
n	O
→	O
∞	O
,	O
the	O
posterior	O
mean	O
is	O
given	O
by	O
the	O
maximum	B
likelihood	I
solution	O
.	O
similarly	O
,	O
consider	O
the	O
result	O
(	O
2.142	O
)	O
for	O
the	O
variance	B
of	O
the	O
posterior	O
distribution	O
.	O
we	O
see	O
that	O
this	O
is	O
most	O
naturally	O
expressed	O
in	O
terms	O
of	O
the	O
inverse	B
variance	O
,	O
which	O
is	O
called	O
the	O
precision	O
.	O
furthermore	O
,	O
the	O
precisions	O
are	O
additive	O
,	O
so	O
that	O
the	O
precision	O
of	O
the	O
posterior	O
is	O
given	O
by	O
the	O
precision	O
of	O
the	O
prior	B
plus	O
one	O
contribution	O
of	O
the	O
data	O
precision	O
from	O
each	O
of	O
the	O
observed	O
data	O
points	O
.	O
as	O
we	O
increase	O
the	O
number	O
of	O
observed	O
data	O
points	O
,	O
the	O
precision	O
steadily	O
increases	O
,	O
corresponding	O
to	O
a	O
posterior	O
distribution	O
with	O
steadily	O
decreasing	O
variance	B
.	O
with	O
no	O
observed	O
data	O
points	O
,	O
we	O
have	O
the	O
prior	B
variance	O
,	O
whereas	O
if	O
the	O
number	O
of	O
data	O
points	O
n	O
→	O
∞	O
,	O
the	O
variance	B
σ2	O
n	O
goes	O
to	O
zero	O
and	O
the	O
posterior	O
distribution	O
becomes	O
inﬁnitely	O
peaked	O
around	O
the	O
maximum	B
likelihood	I
solution	O
.	O
we	O
therefore	O
see	O
that	O
the	O
maximum	B
likelihood	I
result	O
of	O
a	O
point	O
estimate	O
for	O
µ	O
given	O
by	O
(	O
2.143	O
)	O
is	O
recovered	O
precisely	O
from	O
the	O
bayesian	O
formalism	O
in	O
the	O
limit	O
of	O
an	O
inﬁnite	O
number	O
0	O
→	O
∞	O
in	O
which	O
the	O
of	O
observations	O
.	O
note	O
also	O
that	O
for	O
ﬁnite	O
n	O
,	O
if	O
we	O
take	O
the	O
limit	O
σ2	O
prior	B
has	O
inﬁnite	O
variance	B
then	O
the	O
posterior	O
mean	O
(	O
2.141	O
)	O
reduces	O
to	O
the	O
maximum	O
n	O
=	O
σ2/n	O
.	O
likelihood	O
result	O
,	O
while	O
from	O
(	O
2.142	O
)	O
the	O
posterior	O
variance	O
is	O
given	O
by	O
σ2	O
2.3.	O
the	O
gaussian	O
distribution	O
99	O
figure	O
2.12	O
illustration	O
of	O
bayesian	O
inference	B
for	O
the	O
mean	B
µ	O
of	O
a	O
gaussian	O
distri-	O
bution	O
,	O
in	O
which	O
the	O
variance	B
is	O
as-	O
sumed	O
to	O
be	O
known	O
.	O
the	O
curves	O
show	O
the	O
prior	B
distribution	O
over	O
µ	O
(	O
the	O
curve	O
labelled	O
n	O
=	O
0	O
)	O
,	O
which	O
in	O
this	O
case	O
is	O
itself	O
gaussian	O
,	O
along	O
with	O
the	O
posterior	O
distribution	O
given	O
by	O
(	O
2.140	O
)	O
for	O
increasing	O
numbers	O
n	O
of	O
data	O
points	O
.	O
the	O
data	O
points	O
are	O
generated	O
from	O
a	O
gaussian	O
of	O
mean	B
0.8	O
and	O
variance	B
0.1	O
,	O
and	O
the	O
prior	B
is	O
chosen	O
to	O
have	O
mean	B
0.	O
in	O
both	O
the	O
prior	B
and	O
the	O
likelihood	B
function	I
,	O
the	O
variance	B
is	O
set	O
to	O
the	O
true	O
value	O
.	O
5	O
0	O
−1	O
n	O
=	O
0	O
n	O
=	O
10	O
n	O
=	O
2	O
n	O
=	O
1	O
0	O
1	O
exercise	O
2.40	O
section	O
2.3.5	O
we	O
illustrate	O
our	O
analysis	O
of	O
bayesian	O
inference	B
for	O
the	O
mean	B
of	O
a	O
gaussian	O
distribution	O
in	O
figure	O
2.12.	O
the	O
generalization	B
of	O
this	O
result	O
to	O
the	O
case	O
of	O
a	O
d-	O
dimensional	O
gaussian	O
random	O
variable	O
x	O
with	O
known	O
covariance	B
and	O
unknown	O
mean	B
is	O
straightforward	O
.	O
we	O
have	O
already	O
seen	O
how	O
the	O
maximum	B
likelihood	I
expression	O
for	O
the	O
mean	B
of	O
a	O
gaussian	O
can	O
be	O
re-cast	O
as	O
a	O
sequential	O
update	O
formula	O
in	O
which	O
the	O
mean	B
after	O
observing	O
n	O
data	O
points	O
was	O
expressed	O
in	O
terms	O
of	O
the	O
mean	B
after	O
observing	O
n	O
−	O
1	O
data	O
points	O
together	O
with	O
the	O
contribution	O
from	O
data	O
point	O
xn	O
.	O
in	O
fact	O
,	O
the	O
bayesian	O
paradigm	O
leads	O
very	O
naturally	O
to	O
a	O
sequential	O
view	O
of	O
the	O
inference	B
problem	O
.	O
to	O
see	O
this	O
in	O
the	O
context	O
of	O
the	O
inference	B
of	O
the	O
mean	B
of	O
a	O
gaussian	O
,	O
we	O
write	O
the	O
posterior	O
distribution	O
with	O
the	O
contribution	O
from	O
the	O
ﬁnal	O
data	O
point	O
xn	O
separated	O
out	O
so	O
that	O
(	O
cid:31	O
)	O
n−1	O
(	O
cid:14	O
)	O
p	O
(	O
µ|d	O
)	O
∝	O
p	O
(	O
µ	O
)	O
p	O
(	O
xn|µ	O
)	O
p	O
(	O
xn|µ	O
)	O
.	O
(	O
2.144	O
)	O
n=1	O
the	O
term	O
in	O
square	O
brackets	O
is	O
(	O
up	O
to	O
a	O
normalization	O
coefﬁcient	O
)	O
just	O
the	O
posterior	O
distribution	O
after	O
observing	O
n	O
−	O
1	O
data	O
points	O
.	O
we	O
see	O
that	O
this	O
can	O
be	O
viewed	O
as	O
a	O
prior	B
distribution	O
,	O
which	O
is	O
combined	O
using	O
bayes	O
’	O
theorem	O
with	O
the	O
likelihood	B
function	I
associated	O
with	O
data	O
point	O
xn	O
to	O
arrive	O
at	O
the	O
posterior	O
distribution	O
after	O
observing	O
n	O
data	O
points	O
.	O
this	O
sequential	O
view	O
of	O
bayesian	O
inference	B
is	O
very	O
general	O
and	O
applies	O
to	O
any	O
problem	O
in	O
which	O
the	O
observed	O
data	O
are	O
assumed	O
to	O
be	O
independent	B
and	O
identically	O
distributed	O
.	O
so	O
far	O
,	O
we	O
have	O
assumed	O
that	O
the	O
variance	B
of	O
the	O
gaussian	O
distribution	O
over	O
the	O
data	O
is	O
known	O
and	O
our	O
goal	O
is	O
to	O
infer	O
the	O
mean	B
.	O
now	O
let	O
us	O
suppose	O
that	O
the	O
mean	B
is	O
known	O
and	O
we	O
wish	O
to	O
infer	O
the	O
variance	B
.	O
again	O
,	O
our	O
calculations	O
will	O
be	O
greatly	O
simpliﬁed	O
if	O
we	O
choose	O
a	O
conjugate	B
form	O
for	O
the	O
prior	B
distribution	O
.	O
it	O
turns	O
out	O
to	O
be	O
most	O
convenient	O
to	O
work	O
with	O
the	O
precision	O
λ	O
≡	O
1/σ2	O
.	O
the	O
likelihood	B
function	I
for	O
λ	O
takes	O
the	O
form	O
p	O
(	O
x|λ	O
)	O
=	O
−1	O
)	O
∝	O
λn/2	O
exp	O
n	O
(	O
xn|µ	O
,	O
λ	O
n	O
(	O
cid:2	O
)	O
(	O
xn	O
−	O
µ	O
)	O
2	O
.	O
(	O
cid:24	O
)	O
(	O
cid:25	O
)	O
(	O
2.145	O
)	O
−	O
λ	O
2	O
n=1	O
n	O
(	O
cid:14	O
)	O
n=1	O
100	O
2.	O
probability	B
distributions	O
2	O
1	O
0	O
0	O
a	O
=	O
0.1	O
b	O
=	O
0.1	O
2	O
1	O
a	O
=	O
1	O
b	O
=	O
1	O
2	O
1	O
a	O
=	O
4	O
b	O
=	O
6	O
λ	O
1	O
0	O
0	O
2	O
λ	O
1	O
0	O
0	O
2	O
λ	O
1	O
2	O
figure	O
2.13	O
plot	O
of	O
the	O
gamma	B
distribution	I
gam	O
(	O
λ|a	O
,	O
b	O
)	O
deﬁned	O
by	O
(	O
2.146	O
)	O
for	O
various	O
values	O
of	O
the	O
parameters	O
a	O
and	O
b.	O
the	O
corresponding	O
conjugate	B
prior	I
should	O
therefore	O
be	O
proportional	O
to	O
the	O
product	O
of	O
a	O
power	O
of	O
λ	O
and	O
the	O
exponential	O
of	O
a	O
linear	O
function	O
of	O
λ.	O
this	O
corresponds	O
to	O
the	O
gamma	B
distribution	I
which	O
is	O
deﬁned	O
by	O
gam	O
(	O
λ|a	O
,	O
b	O
)	O
=	O
1	O
γ	O
(	O
a	O
)	O
baλa−1	O
exp	O
(	O
−bλ	O
)	O
.	O
(	O
2.146	O
)	O
exercise	O
2.41	O
exercise	O
2.42	O
here	O
γ	O
(	O
a	O
)	O
is	O
the	O
gamma	B
function	I
that	O
is	O
deﬁned	O
by	O
(	O
1.141	O
)	O
and	O
that	O
ensures	O
that	O
(	O
2.146	O
)	O
is	O
correctly	O
normalized	O
.	O
the	O
gamma	B
distribution	I
has	O
a	O
ﬁnite	O
integral	O
if	O
a	O
>	O
0	O
,	O
and	O
the	O
distribution	O
itself	O
is	O
ﬁnite	O
if	O
a	O
(	O
cid:2	O
)	O
1.	O
it	O
is	O
plotted	O
,	O
for	O
various	O
values	O
of	O
a	O
and	O
b	O
,	O
in	O
figure	O
2.13.	O
the	O
mean	B
and	O
variance	B
of	O
the	O
gamma	B
distribution	I
are	O
given	O
by	O
(	O
2.147	O
)	O
e	O
[	O
λ	O
]	O
=	O
a	O
b	O
var	O
[	O
λ	O
]	O
=	O
a	O
b2	O
.	O
(	O
cid:24	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
(	O
2.148	O
)	O
consider	O
a	O
prior	B
distribution	O
gam	O
(	O
λ|a0	O
,	O
b0	O
)	O
.	O
if	O
we	O
multiply	O
by	O
the	O
likelihood	B
function	I
(	O
2.145	O
)	O
,	O
then	O
we	O
obtain	O
a	O
posterior	O
distribution	O
(	O
cid:25	O
)	O
p	O
(	O
λ|x	O
)	O
∝	O
λa0−1λn/2	O
exp	O
−b0λ	O
−	O
λ	O
2	O
(	O
xn	O
−	O
µ	O
)	O
2	O
(	O
2.149	O
)	O
which	O
we	O
recognize	O
as	O
a	O
gamma	B
distribution	I
of	O
the	O
form	O
gam	O
(	O
λ|an	O
,	O
bn	O
)	O
where	O
an	O
=	O
a0	O
+	O
n	O
2	O
1	O
2	O
bn	O
=	O
b0	O
+	O
n	O
(	O
cid:2	O
)	O
n=1	O
(	O
xn	O
−	O
µ	O
)	O
2	O
=	O
b0	O
+	O
n	O
2	O
σ2	O
ml	O
(	O
2.150	O
)	O
(	O
2.151	O
)	O
where	O
σ2	O
ml	O
is	O
the	O
maximum	B
likelihood	I
estimator	O
of	O
the	O
variance	B
.	O
note	O
that	O
in	O
(	O
2.149	O
)	O
there	O
is	O
no	O
need	O
to	O
keep	O
track	O
of	O
the	O
normalization	O
constants	O
in	O
the	O
prior	B
and	O
the	O
likelihood	B
function	I
because	O
,	O
if	O
required	O
,	O
the	O
correct	O
coefﬁcient	O
can	O
be	O
found	O
at	O
the	O
end	O
using	O
the	O
normalized	O
form	O
(	O
2.146	O
)	O
for	O
the	O
gamma	B
distribution	I
.	O
2.3.	O
the	O
gaussian	O
distribution	O
101	O
from	O
(	O
2.150	O
)	O
,	O
we	O
see	O
that	O
the	O
effect	O
of	O
observing	O
n	O
data	O
points	O
is	O
to	O
increase	O
the	O
value	O
of	O
the	O
coefﬁcient	O
a	O
by	O
n/2	O
.	O
thus	O
we	O
can	O
interpret	O
the	O
parameter	O
a0	O
in	O
the	O
prior	B
in	O
terms	O
of	O
2a0	O
‘	O
effective	O
’	O
prior	B
observations	O
.	O
similarly	O
,	O
from	O
(	O
2.151	O
)	O
we	O
see	O
that	O
the	O
n	O
data	O
points	O
contribute	O
n	O
σ2	O
ml	O
is	O
the	O
variance	B
,	O
and	O
so	O
we	O
can	O
interpret	O
the	O
parameter	O
b0	O
in	O
the	O
prior	B
as	O
arising	O
from	O
the	O
2a0	O
‘	O
effective	O
’	O
prior	B
observations	O
having	O
variance	B
2b0/	O
(	O
2a0	O
)	O
=	O
b0/a0	O
.	O
recall	O
that	O
we	O
made	O
an	O
analogous	O
interpretation	O
for	O
the	O
dirichlet	O
prior	B
.	O
these	O
distributions	O
are	O
examples	O
of	O
the	O
exponential	B
family	I
,	O
and	O
we	O
shall	O
see	O
that	O
the	O
interpretation	O
of	O
a	O
conjugate	B
prior	I
in	O
terms	O
of	O
effective	O
ﬁctitious	O
data	O
points	O
is	O
a	O
general	O
one	O
for	O
the	O
exponential	B
family	I
of	O
distributions	O
.	O
ml/2	O
to	O
the	O
parameter	O
b	O
,	O
where	O
σ2	O
section	O
2.2	O
instead	O
of	O
working	O
with	O
the	O
precision	O
,	O
we	O
can	O
consider	O
the	O
variance	B
itself	O
.	O
the	O
conjugate	B
prior	I
in	O
this	O
case	O
is	O
called	O
the	O
inverse	B
gamma	I
distribution	I
,	O
although	O
we	O
shall	O
not	O
discuss	O
this	O
further	O
because	O
we	O
will	O
ﬁnd	O
it	O
more	O
convenient	O
to	O
work	O
with	O
the	O
precision	O
.	O
now	O
suppose	O
that	O
both	O
the	O
mean	B
and	O
the	O
precision	O
are	O
unknown	O
.	O
to	O
ﬁnd	O
a	O
conjugate	B
prior	I
,	O
we	O
consider	O
the	O
dependence	O
of	O
the	O
likelihood	B
function	I
on	O
µ	O
and	O
λ	O
(	O
cid:13	O
)	O
n	O
(	O
cid:14	O
)	O
n=1	O
(	O
cid:15	O
)	O
(	O
cid:15	O
)	O
(	O
cid:12	O
)	O
(	O
cid:16	O
)	O
1/2	O
(	O
cid:16	O
)	O
(	O
cid:30	O
)	O
n	O
exp	O
−	O
λ	O
2	O
(	O
cid:24	O
)	O
p	O
(	O
x|µ	O
,	O
λ	O
)	O
=	O
(	O
cid:29	O
)	O
∝	O
λ1/2	O
exp	O
(	O
xn	O
−	O
µ	O
)	O
2	O
n	O
(	O
cid:2	O
)	O
n=1	O
exp	O
λµ	O
xn	O
−	O
λ	O
2	O
(	O
cid:25	O
)	O
x2	O
n	O
.	O
(	O
2.152	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
λ	O
2π	O
−	O
λµ2	O
2	O
(	O
cid:15	O
)	O
we	O
now	O
wish	O
to	O
identify	O
a	O
prior	B
distribution	O
p	O
(	O
µ	O
,	O
λ	O
)	O
that	O
has	O
the	O
same	O
functional	B
dependence	O
on	O
µ	O
and	O
λ	O
as	O
the	O
likelihood	B
function	I
and	O
that	O
should	O
therefore	O
take	O
the	O
form	O
p	O
(	O
µ	O
,	O
λ	O
)	O
∝	O
λ1/2	O
exp	O
(	O
cid:29	O
)	O
(	O
cid:12	O
)	O
(	O
cid:16	O
)	O
(	O
cid:30	O
)	O
β	O
(	O
cid:13	O
)	O
−	O
λµ2	O
2	O
(	O
µ	O
−	O
c/β	O
)	O
2	O
exp	O
{	O
cλµ	O
−	O
dλ	O
}	O
(	O
cid:12	O
)	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
(	O
cid:13	O
)	O
−	O
βλ	O
2	O
λβ/2	O
exp	O
=	O
exp	O
(	O
2.153	O
)	O
where	O
c	O
,	O
d	O
,	O
and	O
β	O
are	O
constants	O
.	O
since	O
we	O
can	O
always	O
write	O
p	O
(	O
µ	O
,	O
λ	O
)	O
=	O
p	O
(	O
µ|λ	O
)	O
p	O
(	O
λ	O
)	O
,	O
we	O
can	O
ﬁnd	O
p	O
(	O
µ|λ	O
)	O
and	O
p	O
(	O
λ	O
)	O
by	O
inspection	O
.	O
in	O
particular	O
,	O
we	O
see	O
that	O
p	O
(	O
µ|λ	O
)	O
is	O
a	O
gaussian	O
whose	O
precision	O
is	O
a	O
linear	O
function	O
of	O
λ	O
and	O
that	O
p	O
(	O
λ	O
)	O
is	O
a	O
gamma	O
distri-	O
bution	O
,	O
so	O
that	O
the	O
normalized	O
prior	O
takes	O
the	O
form	O
λ	O
−	O
d	O
−	O
c2	O
2β	O
p	O
(	O
µ	O
,	O
λ	O
)	O
=	O
n	O
(	O
µ|µ0	O
,	O
(	O
βλ	O
)	O
−1	O
)	O
gam	O
(	O
λ|a	O
,	O
b	O
)	O
(	O
2.154	O
)	O
where	O
we	O
have	O
deﬁned	O
new	O
constants	O
given	O
by	O
µ0	O
=	O
c/β	O
,	O
a	O
=	O
1	O
+	O
β/2	O
,	O
b	O
=	O
d−c2/2β	O
.	O
the	O
distribution	O
(	O
2.154	O
)	O
is	O
called	O
the	O
normal-gamma	O
or	O
gaussian-gamma	O
distribution	O
and	O
is	O
plotted	O
in	O
figure	O
2.14.	O
note	O
that	O
this	O
is	O
not	O
simply	O
the	O
product	O
of	O
an	O
independent	B
gaussian	O
prior	B
over	O
µ	O
and	O
a	O
gamma	O
prior	O
over	O
λ	O
,	O
because	O
the	O
precision	O
of	O
µ	O
is	O
a	O
linear	O
function	O
of	O
λ.	O
even	O
if	O
we	O
chose	O
a	O
prior	B
in	O
which	O
µ	O
and	O
λ	O
were	O
independent	B
,	O
the	O
posterior	O
distribution	O
would	O
exhibit	O
a	O
coupling	O
between	O
the	O
precision	O
of	O
µ	O
and	O
the	O
value	O
of	O
λ	O
.	O
102	O
2.	O
probability	B
distributions	O
figure	O
2.14	O
contour	O
plot	O
of	O
the	O
normal-gamma	B
distribution	I
(	O
2.154	O
)	O
for	O
parameter	O
values	O
µ0	O
=	O
0	O
,	O
β	O
=	O
2	O
,	O
a	O
=	O
5	O
and	O
b	O
=	O
6	O
.	O
2	O
λ	O
1	O
exercise	O
2.45	O
(	O
cid:22	O
)	O
0	O
−2	O
in	O
the	O
case	O
of	O
the	O
multivariate	O
gaussian	O
distribution	O
n	O
(	O
cid:10	O
)	O
0	O
µ	O
(	O
cid:11	O
)	O
x|µ	O
,	O
λ	O
−1	O
2	O
for	O
a	O
d-	O
dimensional	O
variable	O
x	O
,	O
the	O
conjugate	B
prior	I
distribution	O
for	O
the	O
mean	B
µ	O
,	O
assuming	O
the	O
precision	O
is	O
known	O
,	O
is	O
again	O
a	O
gaussian	O
.	O
for	O
known	O
mean	B
and	O
unknown	O
precision	B
matrix	I
λ	O
,	O
the	O
conjugate	B
prior	I
is	O
the	O
wishart	O
distribution	O
given	O
by	O
w	O
(	O
λ|w	O
,	O
ν	O
)	O
=	O
b|λ|	O
(	O
ν−d−1	O
)	O
/2	O
exp	O
(	O
2.155	O
)	O
where	O
ν	O
is	O
called	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
of	O
the	O
distribution	O
,	O
w	O
is	O
a	O
d×d	O
scale	O
matrix	O
,	O
and	O
tr	O
(	O
·	O
)	O
denotes	O
the	O
trace	O
.	O
the	O
normalization	O
constant	O
b	O
is	O
given	O
by	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
−1	O
2	O
tr	O
(	O
w−1λ	O
)	O
(	O
cid:15	O
)	O
ν	O
+	O
1	O
−	O
i	O
d	O
(	O
cid:14	O
)	O
i=1	O
(	O
cid:16	O
)	O
(	O
cid:23	O
)	O
−1	O
b	O
(	O
w	O
,	O
ν	O
)	O
=	O
|w|−ν/2	O
2νd/2	O
πd	O
(	O
d−1	O
)	O
/4	O
γ	O
2	O
.	O
(	O
2.156	O
)	O
again	O
,	O
it	O
is	O
also	O
possible	O
to	O
deﬁne	O
a	O
conjugate	B
prior	I
over	O
the	O
covariance	B
matrix	I
itself	O
,	O
rather	O
than	O
over	O
the	O
precision	B
matrix	I
,	O
which	O
leads	O
to	O
the	O
inverse	B
wishart	O
distribu-	O
tion	O
,	O
although	O
we	O
shall	O
not	O
discuss	O
this	O
further	O
.	O
if	O
both	O
the	O
mean	B
and	O
the	O
precision	O
are	O
unknown	O
,	O
then	O
,	O
following	O
a	O
similar	O
line	O
of	O
reasoning	O
to	O
the	O
univariate	O
case	O
,	O
the	O
conjugate	B
prior	I
is	O
given	O
by	O
p	O
(	O
µ	O
,	O
λ|µ0	O
,	O
β	O
,	O
w	O
,	O
ν	O
)	O
=	O
n	O
(	O
µ|µ0	O
,	O
(	O
βλ	O
)	O
−1	O
)	O
w	O
(	O
λ|w	O
,	O
ν	O
)	O
(	O
2.157	O
)	O
which	O
is	O
known	O
as	O
the	O
normal-wishart	O
or	O
gaussian-wishart	O
distribution	O
.	O
2.3.7	O
student	O
’	O
s	O
t-distribution	O
we	O
have	O
seen	O
that	O
the	O
conjugate	B
prior	I
for	O
the	O
precision	O
of	O
a	O
gaussian	O
is	O
given	O
by	O
a	O
gamma	B
distribution	I
.	O
if	O
we	O
have	O
a	O
univariate	O
gaussian	O
n	O
(	O
x|µ	O
,	O
τ	O
−1	O
)	O
together	O
with	O
a	O
gamma	O
prior	O
gam	O
(	O
τ|a	O
,	O
b	O
)	O
and	O
we	O
integrate	O
out	O
the	O
precision	O
,	O
we	O
obtain	O
the	O
marginal	B
distribution	O
of	O
x	O
in	O
the	O
form	O
section	O
2.3.6	O
exercise	O
2.46	O
figure	O
2.15	O
plot	O
of	O
student	O
’	O
s	O
t-distribution	O
(	O
2.159	O
)	O
for	O
µ	O
=	O
0	O
and	O
λ	O
=	O
1	O
for	O
various	O
values	O
of	O
ν.	O
the	O
limit	O
ν	O
→	O
∞	O
corresponds	O
to	O
a	O
gaussian	O
distribution	O
with	O
mean	B
µ	O
and	O
precision	O
λ	O
.	O
2.3.	O
the	O
gaussian	O
distribution	O
103	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−5	O
ν	O
→	O
∞	O
ν	O
=	O
1.0	O
ν	O
=	O
0.1	O
0	O
5	O
(	O
cid:6	O
)	O
∞	O
(	O
cid:6	O
)	O
∞	O
0	O
0	O
ba	O
γ	O
(	O
a	O
)	O
p	O
(	O
x|µ	O
,	O
a	O
,	O
b	O
)	O
=	O
=	O
=	O
n	O
(	O
x|µ	O
,	O
τ	O
bae	O
(	O
−bτ	O
)	O
τ	O
a−1	O
(	O
cid:16	O
)	O
1/2	O
(	O
cid:29	O
)	O
γ	O
(	O
a	O
)	O
1	O
2π	O
(	O
cid:15	O
)	O
−1	O
)	O
gam	O
(	O
τ|a	O
,	O
b	O
)	O
dτ	O
(	O
cid:17	O
)	O
(	O
cid:18	O
)	O
1/2	O
exp	O
τ	O
2π	O
(	O
x	O
−	O
µ	O
)	O
2	O
b	O
+	O
2	O
(	O
cid:20	O
)	O
(	O
2.158	O
)	O
dτ	O
(	O
x	O
−	O
µ	O
)	O
2	O
(	O
cid:19	O
)	O
(	O
cid:30	O
)	O
−a−1/2	O
−	O
τ	O
2	O
γ	O
(	O
a	O
+	O
1/2	O
)	O
where	O
we	O
have	O
made	O
the	O
change	O
of	O
variable	O
z	O
=	O
τ	O
[	O
b	O
+	O
(	O
x	O
−	O
µ	O
)	O
2/2	O
]	O
.	O
by	O
convention	O
(	O
cid:15	O
)	O
we	O
deﬁne	O
new	O
parameters	O
given	O
by	O
ν	O
=	O
2a	O
and	O
λ	O
=	O
a/b	O
,	O
in	O
terms	O
of	O
which	O
the	O
distribution	O
p	O
(	O
x|µ	O
,	O
a	O
,	O
b	O
)	O
takes	O
the	O
form	O
(	O
cid:30	O
)	O
−ν/2−1/2	O
(	O
cid:16	O
)	O
1/2	O
(	O
cid:29	O
)	O
(	O
2.159	O
)	O
st	O
(	O
x|µ	O
,	O
λ	O
,	O
ν	O
)	O
=	O
γ	O
(	O
ν/2	O
+	O
1/2	O
)	O
γ	O
(	O
ν/2	O
)	O
λ	O
πν	O
1	O
+	O
λ	O
(	O
x	O
−	O
µ	O
)	O
2	O
ν	O
which	O
is	O
known	O
as	O
student	O
’	O
s	O
t-distribution	O
.	O
the	O
parameter	O
λ	O
is	O
sometimes	O
called	O
the	O
precision	O
of	O
the	O
t-distribution	O
,	O
even	O
though	O
it	O
is	O
not	O
in	O
general	O
equal	O
to	O
the	O
inverse	B
of	O
the	O
variance	B
.	O
the	O
parameter	O
ν	O
is	O
called	O
the	O
degrees	B
of	I
freedom	I
,	O
and	O
its	O
effect	O
is	O
illustrated	O
in	O
figure	O
2.15.	O
for	O
the	O
particular	O
case	O
of	O
ν	O
=	O
1	O
,	O
the	O
t-distribution	O
reduces	O
to	O
the	O
cauchy	O
distribution	O
,	O
while	O
in	O
the	O
limit	O
ν	O
→	O
∞	O
the	O
t-distribution	O
st	O
(	O
x|µ	O
,	O
λ	O
,	O
ν	O
)	O
becomes	O
a	O
gaussian	O
n	O
(	O
x|µ	O
,	O
λ	O
−1	O
)	O
with	O
mean	B
µ	O
and	O
precision	O
λ.	O
from	O
(	O
2.158	O
)	O
,	O
we	O
see	O
that	O
student	O
’	O
s	O
t-distribution	O
is	O
obtained	O
by	O
adding	O
up	O
an	O
inﬁnite	O
number	O
of	O
gaussian	O
distributions	O
having	O
the	O
same	O
mean	B
but	O
different	O
preci-	O
sions	O
.	O
this	O
can	O
be	O
interpreted	O
as	O
an	O
inﬁnite	O
mixture	O
of	O
gaussians	O
(	O
gaussian	O
mixtures	O
will	O
be	O
discussed	O
in	O
detail	O
in	O
section	O
2.3.9.	O
the	O
result	O
is	O
a	O
distribution	O
that	O
in	O
gen-	O
eral	O
has	O
longer	O
‘	O
tails	O
’	O
than	O
a	O
gaussian	O
,	O
as	O
was	O
seen	O
in	O
figure	O
2.15.	O
this	O
gives	O
the	O
t-	O
distribution	O
an	O
important	O
property	O
called	O
robustness	B
,	O
which	O
means	O
that	O
it	O
is	O
much	O
less	O
sensitive	O
than	O
the	O
gaussian	O
to	O
the	O
presence	O
of	O
a	O
few	O
data	O
points	O
which	O
are	O
outliers	B
.	O
the	O
robustness	B
of	O
the	O
t-distribution	O
is	O
illustrated	O
in	O
figure	O
2.16	O
,	O
which	O
compares	O
the	O
maximum	B
likelihood	I
solutions	O
for	O
a	O
gaussian	O
and	O
a	O
t-distribution	O
.	O
note	O
that	O
the	O
max-	O
imum	O
likelihood	O
solution	O
for	O
the	O
t-distribution	O
can	O
be	O
found	O
using	O
the	O
expectation-	O
maximization	O
(	O
em	O
)	O
algorithm	O
.	O
here	O
we	O
see	O
that	O
the	O
effect	O
of	O
a	O
small	O
number	O
of	O
exercise	O
2.47	O
exercise	O
12.24	O
104	O
2.	O
probability	B
distributions	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−5	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−5	O
0	O
5	O
10	O
(	O
a	O
)	O
0	O
5	O
10	O
(	O
b	O
)	O
figure	O
2.16	O
illustration	O
of	O
the	O
robustness	B
of	O
student	O
’	O
s	O
t-distribution	O
compared	O
to	O
a	O
gaussian	O
.	O
(	O
a	O
)	O
histogram	O
distribution	O
of	O
30	O
data	O
points	O
drawn	O
from	O
a	O
gaussian	O
distribution	O
,	O
together	O
with	O
the	O
maximum	B
likelihood	I
ﬁt	O
ob-	O
tained	O
from	O
a	O
t-distribution	O
(	O
red	O
curve	O
)	O
and	O
a	O
gaussian	O
(	O
green	O
curve	O
,	O
largely	O
hidden	O
by	O
the	O
red	O
curve	O
)	O
.	O
because	O
the	O
t-distribution	O
contains	O
the	O
gaussian	O
as	O
a	O
special	O
case	O
it	O
gives	O
almost	O
the	O
same	O
solution	O
as	O
the	O
gaussian	O
.	O
(	O
b	O
)	O
the	O
same	O
data	O
set	O
but	O
with	O
three	O
additional	O
outlying	O
data	O
points	O
showing	O
how	O
the	O
gaussian	O
(	O
green	O
curve	O
)	O
is	O
strongly	O
distorted	O
by	O
the	O
outliers	B
,	O
whereas	O
the	O
t-distribution	O
(	O
red	O
curve	O
)	O
is	O
relatively	O
unaffected	O
.	O
outliers	B
is	O
much	O
less	O
signiﬁcant	O
for	O
the	O
t-distribution	O
than	O
for	O
the	O
gaussian	O
.	O
outliers	B
can	O
arise	O
in	O
practical	O
applications	O
either	O
because	O
the	O
process	O
that	O
generates	O
the	O
data	O
corresponds	O
to	O
a	O
distribution	O
having	O
a	O
heavy	O
tail	O
or	O
simply	O
through	O
mislabelled	O
data	O
.	O
robustness	B
is	O
also	O
an	O
important	O
property	O
for	B
regression	I
problems	O
.	O
unsurprisingly	O
,	O
the	O
least	O
squares	O
approach	O
to	O
regression	B
does	O
not	O
exhibit	O
robustness	B
,	O
because	O
it	O
cor-	O
responds	O
to	O
maximum	B
likelihood	I
under	O
a	O
(	O
conditional	B
)	O
gaussian	O
distribution	O
.	O
by	O
basing	O
a	O
regression	B
model	O
on	O
a	O
heavy-tailed	O
distribution	O
such	O
as	O
a	O
t-distribution	O
,	O
we	O
obtain	O
a	O
more	O
robust	O
model	O
.	O
if	O
we	O
go	O
back	O
to	O
(	O
2.158	O
)	O
and	O
substitute	O
the	O
alternative	O
parameters	O
ν	O
=	O
2a	O
,	O
λ	O
=	O
a/b	O
,	O
and	O
η	O
=	O
τ	O
b/a	O
,	O
we	O
see	O
that	O
the	O
t-distribution	O
can	O
be	O
written	O
in	O
the	O
form	O
st	O
(	O
x|µ	O
,	O
λ	O
,	O
ν	O
)	O
=	O
x|µ	O
,	O
(	O
ηλ	O
)	O
−1	O
gam	O
(	O
η|ν/2	O
,	O
ν/2	O
)	O
dη	O
.	O
(	O
2.160	O
)	O
we	O
can	O
then	O
generalize	O
this	O
to	O
a	O
multivariate	O
gaussian	O
n	O
(	O
x|µ	O
,	O
λ	O
)	O
to	O
obtain	O
the	O
cor-	O
responding	O
multivariate	O
student	O
’	O
s	O
t-distribution	O
in	O
the	O
form	O
st	O
(	O
x|µ	O
,	O
λ	O
,	O
ν	O
)	O
=	O
n	O
(	O
x|µ	O
,	O
(	O
ηλ	O
)	O
−1	O
)	O
gam	O
(	O
η|ν/2	O
,	O
ν/2	O
)	O
dη	O
.	O
(	O
2.161	O
)	O
exercise	O
2.48	O
using	O
the	O
same	O
technique	O
as	O
for	O
the	O
univariate	O
case	O
,	O
we	O
can	O
evaluate	O
this	O
integral	O
to	O
give	O
0	O
(	O
cid:11	O
)	O
(	O
cid:6	O
)	O
∞	O
n	O
(	O
cid:10	O
)	O
(	O
cid:6	O
)	O
∞	O
0	O
2.3.	O
the	O
gaussian	O
distribution	O
(	O
cid:29	O
)	O
(	O
cid:30	O
)	O
−d/2−ν/2	O
st	O
(	O
x|µ	O
,	O
λ	O
,	O
ν	O
)	O
=	O
γ	O
(	O
d/2	O
+	O
ν/2	O
)	O
γ	O
(	O
ν/2	O
)	O
|λ|1/2	O
(	O
πν	O
)	O
d/2	O
1	O
+	O
∆2	O
ν	O
105	O
(	O
2.162	O
)	O
(	O
2.163	O
)	O
where	O
d	O
is	O
the	O
dimensionality	O
of	O
x	O
,	O
and	O
∆2	O
is	O
the	O
squared	O
mahalanobis	O
distance	O
deﬁned	O
by	O
∆2	O
=	O
(	O
x	O
−	O
µ	O
)	O
tλ	O
(	O
x	O
−	O
µ	O
)	O
.	O
exercise	O
2.49	O
this	O
is	O
the	O
multivariate	O
form	O
of	O
student	O
’	O
s	O
t-distribution	O
and	O
satisﬁes	O
the	O
following	O
properties	O
e	O
[	O
x	O
]	O
=	O
µ	O
,	O
cov	O
[	O
x	O
]	O
=	O
ν	O
(	O
ν	O
−	O
2	O
)	O
−1	O
,	O
λ	O
mode	O
[	O
x	O
]	O
=	O
µ	O
if	O
if	O
ν	O
>	O
1	O
ν	O
>	O
2	O
(	O
2.164	O
)	O
(	O
2.165	O
)	O
(	O
2.166	O
)	O
with	O
corresponding	O
results	O
for	O
the	O
univariate	O
case	O
.	O
2.3.8	O
periodic	O
variables	O
although	O
gaussian	O
distributions	O
are	O
of	O
great	O
practical	O
signiﬁcance	O
,	O
both	O
in	O
their	O
own	O
right	O
and	O
as	O
building	O
blocks	O
for	O
more	O
complex	O
probabilistic	O
models	O
,	O
there	O
are	O
situations	O
in	O
which	O
they	O
are	O
inappropriate	O
as	O
density	B
models	O
for	O
continuous	O
vari-	O
ables	O
.	O
one	O
important	O
case	O
,	O
which	O
arises	O
in	O
practical	O
applications	O
,	O
is	O
that	O
of	O
periodic	O
variables	O
.	O
an	O
example	O
of	O
a	O
periodic	B
variable	I
would	O
be	O
the	O
wind	O
direction	O
at	O
a	O
particular	O
geographical	O
location	O
.	O
we	O
might	O
,	O
for	O
instance	O
,	O
measure	O
values	O
of	O
wind	O
direction	O
on	O
a	O
number	O
of	O
days	O
and	O
wish	O
to	O
summarize	O
this	O
using	O
a	O
parametric	O
distribution	O
.	O
another	O
example	O
is	O
calendar	O
time	O
,	O
where	O
we	O
may	O
be	O
interested	O
in	O
modelling	O
quantities	O
that	O
are	O
believed	O
to	O
be	O
periodic	O
over	O
24	O
hours	O
or	O
over	O
an	O
annual	O
cycle	O
.	O
such	O
quantities	O
can	O
conveniently	O
be	O
represented	O
using	O
an	O
angular	O
(	O
polar	O
)	O
coordinate	O
0	O
(	O
cid:1	O
)	O
θ	O
<	O
2π	O
.	O
and	O
θ2	O
=	O
359◦	O
we	O
might	O
be	O
tempted	O
to	O
treat	O
periodic	O
variables	O
by	O
choosing	O
some	O
direction	O
as	O
the	O
origin	O
and	O
then	O
applying	O
a	O
conventional	O
distribution	O
such	O
as	O
the	O
gaussian	O
.	O
such	O
an	O
approach	O
,	O
however	O
,	O
would	O
give	O
results	O
that	O
were	O
strongly	O
dependent	O
on	O
the	O
arbitrary	O
choice	O
of	O
origin	O
.	O
suppose	O
,	O
for	O
instance	O
,	O
that	O
we	O
have	O
two	O
observations	O
at	O
θ1	O
=	O
1◦	O
,	O
and	O
we	O
model	O
them	O
using	O
a	O
standard	O
univariate	O
gaussian	O
distribution	O
.	O
if	O
we	O
choose	O
the	O
origin	O
at	O
0◦	O
,	O
then	O
the	O
sample	B
mean	I
of	O
this	O
data	O
set	O
will	O
be	O
180◦	O
with	O
standard	B
deviation	I
179◦	O
,	O
whereas	O
if	O
we	O
choose	O
the	O
origin	O
at	O
180◦	O
,	O
then	O
the	O
mean	B
will	O
be	O
0◦	O
.	O
we	O
clearly	O
need	O
to	O
develop	O
a	O
special	O
approach	O
for	O
the	O
treatment	O
of	O
periodic	O
variables	O
.	O
let	O
us	O
consider	O
the	O
problem	O
of	O
evaluating	O
the	O
mean	B
of	O
a	O
set	O
of	O
observations	O
d	O
=	O
{	O
θ1	O
,	O
.	O
.	O
.	O
,	O
θn	O
}	O
of	O
a	O
periodic	B
variable	I
.	O
from	O
now	O
on	O
,	O
we	O
shall	O
assume	O
that	O
θ	O
is	O
measured	O
in	O
radians	O
.	O
we	O
have	O
already	O
seen	O
that	O
the	O
simple	O
average	O
(	O
θ1+···+θn	O
)	O
/n	O
will	O
be	O
strongly	O
coordinate	O
dependent	O
.	O
to	O
ﬁnd	O
an	O
invariant	O
measure	O
of	O
the	O
mean	B
,	O
we	O
note	O
that	O
the	O
observations	O
can	O
be	O
viewed	O
as	O
points	O
on	O
the	O
unit	O
circle	O
and	O
can	O
therefore	O
be	O
described	O
instead	O
by	O
two-dimensional	O
unit	O
vectors	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
where	O
(	O
cid:5	O
)	O
xn	O
(	O
cid:5	O
)	O
=	O
1	O
for	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
as	O
illustrated	O
in	O
figure	O
2.17.	O
we	O
can	O
average	O
the	O
vectors	O
{	O
xn	O
}	O
and	O
the	O
standard	B
deviation	I
will	O
be	O
1◦	O
106	O
2.	O
probability	B
distributions	O
figure	O
2.17	O
illustration	O
of	O
the	O
representation	O
of	O
val-	O
ues	O
θn	O
of	O
a	O
periodic	B
variable	I
as	O
two-	O
dimensional	O
vectors	O
xn	O
living	O
on	O
the	O
unit	O
circle	O
.	O
also	O
shown	O
is	O
the	O
average	O
x	O
of	O
those	O
vectors	O
.	O
x2	O
x3	O
x4	O
¯x	O
¯r	O
¯θ	O
x2	O
x1	O
x1	O
instead	O
to	O
give	O
n	O
(	O
cid:2	O
)	O
n=1	O
xn	O
x	O
=	O
1	O
n	O
(	O
2.167	O
)	O
and	O
then	O
ﬁnd	O
the	O
corresponding	O
angle	O
θ	O
of	O
this	O
average	O
.	O
clearly	O
,	O
this	O
deﬁnition	O
will	O
ensure	O
that	O
the	O
location	O
of	O
the	O
mean	B
is	O
independent	B
of	O
the	O
origin	O
of	O
the	O
angular	O
coor-	O
dinate	O
.	O
note	O
that	O
x	O
will	O
typically	O
lie	O
inside	O
the	O
unit	O
circle	O
.	O
the	O
cartesian	O
coordinates	O
of	O
the	O
observations	O
are	O
given	O
by	O
xn	O
=	O
(	O
cos	O
θn	O
,	O
sin	O
θn	O
)	O
,	O
and	O
we	O
can	O
write	O
the	O
carte-	O
sian	O
coordinates	O
of	O
the	O
sample	B
mean	I
in	O
the	O
form	O
x	O
=	O
(	O
r	O
cos	O
θ	O
,	O
r	O
sin	O
θ	O
)	O
.	O
substituting	O
into	O
(	O
2.167	O
)	O
and	O
equating	O
the	O
x1	O
and	O
x2	O
components	O
then	O
gives	O
r	O
cos	O
θ	O
=	O
1	O
n	O
cos	O
θn	O
,	O
r	O
sin	O
θ	O
=	O
1	O
n	O
sin	O
θn	O
.	O
(	O
2.168	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
n	O
(	O
cid:2	O
)	O
n=1	O
taking	O
the	O
ratio	O
,	O
and	O
using	O
the	O
identity	O
tan	O
θ	O
=	O
sin	O
θ/	O
cos	O
θ	O
,	O
we	O
can	O
solve	O
for	O
θ	O
to	O
give	O
θ	O
=	O
tan−1	O
n	O
sin	O
θn	O
n	O
cos	O
θn	O
.	O
(	O
2.169	O
)	O
(	O
cid:12	O
)	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
(	O
cid:13	O
)	O
shortly	O
,	O
we	O
shall	O
see	O
how	O
this	O
result	O
arises	O
naturally	O
as	O
the	O
maximum	B
likelihood	I
estimator	O
for	O
an	O
appropriately	O
deﬁned	O
distribution	O
over	O
a	O
periodic	B
variable	I
.	O
we	O
now	O
consider	O
a	O
periodic	O
generalization	O
of	O
the	O
gaussian	O
called	O
the	O
von	O
mises	O
distribution	O
.	O
here	O
we	O
shall	O
limit	O
our	O
attention	O
to	O
univariate	O
distributions	O
,	O
although	O
periodic	O
distributions	O
can	O
also	O
be	O
found	O
over	O
hyperspheres	O
of	O
arbitrary	O
dimension	O
.	O
for	O
an	O
extensive	O
discussion	O
of	O
periodic	O
distributions	O
,	O
see	O
mardia	O
and	O
jupp	O
(	O
2000	O
)	O
.	O
by	O
convention	O
,	O
we	O
will	O
consider	O
distributions	O
p	O
(	O
θ	O
)	O
that	O
have	O
period	O
2π	O
.	O
any	O
probability	B
density	O
p	O
(	O
θ	O
)	O
deﬁned	O
over	O
θ	O
must	O
not	O
only	O
be	O
nonnegative	O
and	O
integrate	O
2.3.	O
the	O
gaussian	O
distribution	O
107	O
figure	O
2.18	O
the	O
von	O
mises	O
distribution	O
can	O
be	O
derived	O
by	O
considering	O
a	O
two-dimensional	O
gaussian	O
of	O
the	O
form	O
(	O
2.173	O
)	O
,	O
whose	O
density	B
contours	O
are	O
shown	O
in	O
blue	O
and	O
conditioning	O
on	O
the	O
unit	O
circle	O
shown	O
in	O
red	O
.	O
x2	O
p	O
(	O
x	O
)	O
x1	O
r	O
=	O
1	O
to	O
one	O
,	O
but	O
it	O
must	O
also	O
be	O
periodic	O
.	O
thus	O
p	O
(	O
θ	O
)	O
must	O
satisfy	O
the	O
three	O
conditions	O
(	O
cid:6	O
)	O
2π	O
p	O
(	O
θ	O
)	O
(	O
cid:2	O
)	O
0	O
(	O
2.170	O
)	O
(	O
2.171	O
)	O
(	O
2.172	O
)	O
p	O
(	O
θ	O
)	O
dθ	O
=	O
1	O
0	O
p	O
(	O
θ	O
+	O
2π	O
)	O
=	O
p	O
(	O
θ	O
)	O
.	O
from	O
(	O
2.172	O
)	O
,	O
it	O
follows	O
that	O
p	O
(	O
θ	O
+	O
m2π	O
)	O
=	O
p	O
(	O
θ	O
)	O
for	O
any	O
integer	O
m.	O
we	O
can	O
easily	O
obtain	O
a	O
gaussian-like	O
distribution	O
that	O
satisﬁes	O
these	O
three	O
prop-	O
erties	O
as	O
follows	O
.	O
consider	O
a	O
gaussian	O
distribution	O
over	O
two	O
variables	O
x	O
=	O
(	O
x1	O
,	O
x2	O
)	O
having	O
mean	B
µ	O
=	O
(	O
µ1	O
,	O
µ2	O
)	O
and	O
a	O
covariance	B
matrix	I
σ	O
=	O
σ2i	O
where	O
i	O
is	O
the	O
2	O
×	O
2	O
identity	O
matrix	O
,	O
so	O
that	O
p	O
(	O
x1	O
,	O
x2	O
)	O
=	O
1	O
2πσ2	O
exp	O
−	O
(	O
x1	O
−	O
µ1	O
)	O
2	O
+	O
(	O
x2	O
−	O
µ2	O
)	O
2	O
2σ2	O
.	O
(	O
2.173	O
)	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
the	O
contours	O
of	O
constant	O
p	O
(	O
x	O
)	O
are	O
circles	O
,	O
as	O
illustrated	O
in	O
figure	O
2.18.	O
now	O
suppose	O
we	O
consider	O
the	O
value	O
of	O
this	O
distribution	O
along	O
a	O
circle	O
of	O
ﬁxed	O
radius	O
.	O
then	O
by	O
con-	O
struction	O
this	O
distribution	O
will	O
be	O
periodic	O
,	O
although	O
it	O
will	O
not	O
be	O
normalized	O
.	O
we	O
can	O
determine	O
the	O
form	O
of	O
this	O
distribution	O
by	O
transforming	O
from	O
cartesian	O
coordinates	O
(	O
x1	O
,	O
x2	O
)	O
to	O
polar	O
coordinates	O
(	O
r	O
,	O
θ	O
)	O
so	O
that	O
x2	O
=	O
r	O
sin	O
θ.	O
we	O
also	O
map	O
the	O
mean	B
µ	O
into	O
polar	O
coordinates	O
by	O
writing	O
x1	O
=	O
r	O
cos	O
θ	O
,	O
µ1	O
=	O
r0	O
cos	O
θ0	O
,	O
µ2	O
=	O
r0	O
sin	O
θ0	O
.	O
(	O
2.174	O
)	O
(	O
2.175	O
)	O
next	O
we	O
substitute	O
these	O
transformations	O
into	O
the	O
two-dimensional	O
gaussian	O
distribu-	O
tion	O
(	O
2.173	O
)	O
,	O
and	O
then	O
condition	O
on	O
the	O
unit	O
circle	O
r	O
=	O
1	O
,	O
noting	O
that	O
we	O
are	O
interested	O
only	O
in	O
the	O
dependence	O
on	O
θ.	O
focussing	O
on	O
the	O
exponent	O
in	O
the	O
gaussian	O
distribution	O
we	O
have	O
(	O
cid:26	O
)	O
(	O
r	O
cos	O
θ	O
−	O
r0	O
cos	O
θ0	O
)	O
2	O
+	O
(	O
r	O
sin	O
θ	O
−	O
r0	O
sin	O
θ0	O
)	O
2	O
(	O
cid:27	O
)	O
0	O
−	O
2r0	O
cos	O
θ	O
cos	O
θ0	O
−	O
2r0	O
sin	O
θ	O
sin	O
θ0	O
1	O
+	O
r2	O
(	O
cid:27	O
)	O
(	O
cid:26	O
)	O
−	O
1	O
2σ2	O
=	O
−	O
1	O
2σ2	O
=	O
r0	O
(	O
2.176	O
)	O
σ2	O
cos	O
(	O
θ	O
−	O
θ0	O
)	O
+	O
const	O
108	O
2.	O
probability	B
distributions	O
m	O
=	O
5	O
,	O
θ0	O
=	O
π/4	O
m	O
=	O
1	O
,	O
θ0	O
=	O
3π/4	O
3π/4	O
π/4	O
0	O
2π	O
m	O
=	O
5	O
,	O
θ0	O
=	O
π/4	O
m	O
=	O
1	O
,	O
θ0	O
=	O
3π/4	O
figure	O
2.19	O
the	O
von	O
mises	O
distribution	O
plotted	O
for	O
two	O
different	O
parameter	O
values	O
,	O
shown	O
as	O
a	O
cartesian	O
plot	O
on	O
the	O
left	O
and	O
as	O
the	O
corresponding	O
polar	O
plot	O
on	O
the	O
right	O
.	O
exercise	O
2.51	O
where	O
‘	O
const	O
’	O
denotes	O
terms	O
independent	B
of	O
θ	O
,	O
and	O
we	O
have	O
made	O
use	O
of	O
the	O
following	O
trigonometrical	O
identities	O
cos2	O
a	O
+	O
sin2	O
a	O
=	O
1	O
(	O
2.177	O
)	O
(	O
2.178	O
)	O
if	O
we	O
now	O
deﬁne	O
m	O
=	O
r0/σ2	O
,	O
we	O
obtain	O
our	O
ﬁnal	O
expression	O
for	O
the	O
distribution	O
of	O
p	O
(	O
θ	O
)	O
along	O
the	O
unit	O
circle	O
r	O
=	O
1	O
in	O
the	O
form	O
cos	O
a	O
cos	O
b	O
+	O
sin	O
a	O
sin	O
b	O
=	O
cos	O
(	O
a	O
−	O
b	O
)	O
.	O
p	O
(	O
θ|θ0	O
,	O
m	O
)	O
=	O
1	O
2πi0	O
(	O
m	O
)	O
exp	O
{	O
m	O
cos	O
(	O
θ	O
−	O
θ0	O
)	O
}	O
(	O
2.179	O
)	O
which	O
is	O
called	O
the	O
von	O
mises	O
distribution	O
,	O
or	O
the	O
circular	B
normal	I
.	O
here	O
the	O
param-	O
eter	O
θ0	O
corresponds	O
to	O
the	O
mean	B
of	O
the	O
distribution	O
,	O
while	O
m	O
,	O
which	O
is	O
known	O
as	O
the	O
concentration	B
parameter	I
,	O
is	O
analogous	O
to	O
the	O
inverse	B
variance	O
(	O
precision	O
)	O
for	O
the	O
gaussian	O
.	O
the	O
normalization	O
coefﬁcient	O
in	O
(	O
2.179	O
)	O
is	O
expressed	O
in	O
terms	O
of	O
i0	O
(	O
m	O
)	O
,	O
which	O
is	O
the	O
zeroth-order	O
bessel	O
function	O
of	O
the	O
ﬁrst	O
kind	O
(	O
abramowitz	O
and	O
stegun	O
,	O
1965	O
)	O
and	O
is	O
deﬁned	O
by	O
(	O
cid:6	O
)	O
2π	O
i0	O
(	O
m	O
)	O
=	O
1	O
2π	O
0	O
exp	O
{	O
m	O
cos	O
θ	O
}	O
dθ	O
.	O
(	O
2.180	O
)	O
exercise	O
2.52	O
for	O
large	O
m	O
,	O
the	O
distribution	O
becomes	O
approximately	O
gaussian	O
.	O
the	O
von	O
mises	O
dis-	O
tribution	O
is	O
plotted	O
in	O
figure	O
2.19	O
,	O
and	O
the	O
function	O
i0	O
(	O
m	O
)	O
is	O
plotted	O
in	O
figure	O
2.20.	O
now	O
consider	O
the	O
maximum	B
likelihood	I
estimators	O
for	O
the	O
parameters	O
θ0	O
and	O
m	O
for	O
the	O
von	O
mises	O
distribution	O
.	O
the	O
log	O
likelihood	O
function	O
is	O
given	O
by	O
cos	O
(	O
θn	O
−	O
θ0	O
)	O
.	O
ln	O
p	O
(	O
d|θ0	O
,	O
m	O
)	O
=	O
−n	O
ln	O
(	O
2π	O
)	O
−	O
n	O
ln	O
i0	O
(	O
m	O
)	O
+	O
m	O
(	O
2.181	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
i0	O
(	O
m	O
)	O
3000	O
2000	O
1000	O
0	O
0	O
2.3.	O
the	O
gaussian	O
distribution	O
109	O
1	O
a	O
(	O
m	O
)	O
0.5	O
5	O
m	O
10	O
0	O
0	O
5	O
m	O
10	O
figure	O
2.20	O
plot	O
of	O
the	O
bessel	O
function	O
i0	O
(	O
m	O
)	O
deﬁned	O
by	O
(	O
2.180	O
)	O
,	O
together	O
with	O
the	O
function	O
a	O
(	O
m	O
)	O
deﬁned	O
by	O
(	O
2.186	O
)	O
.	O
setting	O
the	O
derivative	B
with	O
respect	O
to	O
θ0	O
equal	O
to	O
zero	O
gives	O
n	O
(	O
cid:2	O
)	O
n=1	O
sin	O
(	O
θn	O
−	O
θ0	O
)	O
=	O
0.	O
to	O
solve	O
for	O
θ0	O
,	O
we	O
make	O
use	O
of	O
the	O
trigonometric	O
identity	O
sin	O
(	O
a	O
−	O
b	O
)	O
=	O
cos	O
b	O
sin	O
a	O
−	O
cos	O
a	O
sin	O
b	O
(	O
cid:13	O
)	O
(	O
cid:12	O
)	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
n	O
sin	O
θn	O
n	O
cos	O
θn	O
exercise	O
2.53	O
from	O
which	O
we	O
obtain	O
0	O
=	O
tan−1	O
θml	O
(	O
2.182	O
)	O
(	O
2.183	O
)	O
(	O
2.184	O
)	O
(	O
cid:4	O
)	O
0	O
(	O
m	O
)	O
=	O
(	O
2.186	O
)	O
which	O
we	O
recognize	O
as	O
the	O
result	O
(	O
2.169	O
)	O
obtained	O
earlier	O
for	O
the	O
mean	B
of	O
the	O
obser-	O
vations	O
viewed	O
in	O
a	O
two-dimensional	O
cartesian	O
space	O
.	O
similarly	O
,	O
maximizing	O
(	O
2.181	O
)	O
with	O
respect	O
to	O
m	O
,	O
and	O
making	O
use	O
of	O
i	O
i1	O
(	O
m	O
)	O
(	O
abramowitz	O
and	O
stegun	O
,	O
1965	O
)	O
,	O
we	O
have	O
a	O
(	O
m	O
)	O
=	O
1	O
n	O
cos	O
(	O
θn	O
−	O
θml	O
0	O
)	O
(	O
2.185	O
)	O
where	O
we	O
have	O
substituted	O
for	O
the	O
maximum	B
likelihood	I
solution	O
for	O
θml	O
that	O
we	O
are	O
performing	O
a	O
joint	O
optimization	O
over	O
θ	O
and	O
m	O
)	O
,	O
and	O
we	O
have	O
deﬁned	O
0	O
(	O
recalling	O
n	O
(	O
cid:2	O
)	O
n=1	O
a	O
(	O
m	O
)	O
=	O
i1	O
(	O
m	O
)	O
i0	O
(	O
m	O
)	O
.	O
the	O
function	O
a	O
(	O
m	O
)	O
is	O
plotted	O
in	O
figure	O
2.20.	O
making	O
use	O
of	O
the	O
trigonometric	O
iden-	O
tity	O
(	O
2.178	O
)	O
,	O
we	O
can	O
write	O
(	O
2.185	O
)	O
in	O
the	O
form	O
0	O
−	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
a	O
(	O
mml	O
)	O
=	O
cos	O
θml	O
(	O
2.187	O
)	O
sin	O
θn	O
sin	O
θml	O
0	O
.	O
cos	O
θn	O
(	O
cid:23	O
)	O
(	O
cid:22	O
)	O
1	O
n	O
n=1	O
(	O
cid:23	O
)	O
(	O
cid:22	O
)	O
1	O
n	O
n=1	O
110	O
2.	O
probability	B
distributions	O
100	O
80	O
60	O
40	O
1	O
on	O
the	O
left	O
figure	O
2.21	O
plots	O
of	O
the	O
‘	O
old	O
faith-	O
ful	O
’	O
data	O
in	O
which	O
the	O
blue	O
curves	O
show	O
contours	O
of	O
constant	O
proba-	O
bility	O
density	B
.	O
is	O
a	O
single	O
gaussian	O
distribution	O
which	O
has	O
been	O
ﬁtted	O
to	O
the	O
data	O
us-	O
ing	O
maximum	O
likelihood	O
.	O
note	O
that	O
this	O
distribution	O
fails	O
to	O
capture	O
the	O
two	O
clumps	O
in	O
the	O
data	O
and	O
indeed	O
places	O
much	O
of	O
its	O
probability	B
mass	O
in	O
the	O
central	O
region	O
between	O
the	O
clumps	O
where	O
the	O
data	O
are	O
relatively	O
sparse	O
.	O
on	O
the	O
right	O
the	O
distribution	O
is	O
given	O
by	O
a	O
linear	O
combination	O
of	O
two	O
gaussians	O
which	O
has	O
been	O
ﬁtted	O
to	O
the	O
data	O
by	O
maximum	B
likelihood	I
using	O
techniques	O
discussed	O
chap-	O
ter	O
9	O
,	O
and	O
which	O
gives	O
a	O
better	O
rep-	O
resentation	O
of	O
the	O
data	O
.	O
100	O
80	O
60	O
40	O
1	O
2	O
3	O
4	O
5	O
6	O
2	O
3	O
4	O
5	O
6	O
the	O
right-hand	O
side	O
of	O
(	O
2.187	O
)	O
is	O
easily	O
evaluated	O
,	O
and	O
the	O
function	O
a	O
(	O
m	O
)	O
can	O
be	O
inverted	O
numerically	O
.	O
for	O
completeness	O
,	O
we	O
mention	O
brieﬂy	O
some	O
alternative	O
techniques	O
for	O
the	O
con-	O
struction	O
of	O
periodic	O
distributions	O
.	O
the	O
simplest	O
approach	O
is	O
to	O
use	O
a	O
histogram	O
of	O
observations	O
in	O
which	O
the	O
angular	O
coordinate	O
is	O
divided	O
into	O
ﬁxed	O
bins	O
.	O
this	O
has	O
the	O
virtue	O
of	O
simplicity	O
and	O
ﬂexibility	O
but	O
also	O
suffers	O
from	O
signiﬁcant	O
limitations	O
,	O
as	O
we	O
shall	O
see	O
when	O
we	O
discuss	O
histogram	O
methods	O
in	O
more	O
detail	O
in	O
section	O
2.5.	O
another	O
approach	O
starts	O
,	O
like	O
the	O
von	O
mises	O
distribution	O
,	O
from	O
a	O
gaussian	O
distribution	O
over	O
a	O
euclidean	O
space	O
but	O
now	O
marginalizes	O
onto	O
the	O
unit	O
circle	O
rather	O
than	O
conditioning	O
(	O
mardia	O
and	O
jupp	O
,	O
2000	O
)	O
.	O
however	O
,	O
this	O
leads	O
to	O
more	O
complex	O
forms	O
of	O
distribution	O
and	O
will	O
not	O
be	O
discussed	O
further	O
.	O
finally	O
,	O
any	O
valid	O
distribution	O
over	O
the	O
real	O
axis	O
(	O
such	O
as	O
a	O
gaussian	O
)	O
can	O
be	O
turned	O
into	O
a	O
periodic	O
distribution	O
by	O
mapping	O
succes-	O
sive	O
intervals	O
of	O
width	O
2π	O
onto	O
the	O
periodic	B
variable	I
(	O
0	O
,	O
2π	O
)	O
,	O
which	O
corresponds	O
to	O
‘	O
wrapping	O
’	O
the	O
real	O
axis	O
around	O
unit	O
circle	O
.	O
again	O
,	O
the	O
resulting	O
distribution	O
is	O
more	O
complex	O
to	O
handle	O
than	O
the	O
von	O
mises	O
distribution	O
.	O
one	O
limitation	O
of	O
the	O
von	O
mises	O
distribution	O
is	O
that	O
it	O
is	O
unimodal	O
.	O
by	O
forming	O
mixtures	O
of	O
von	O
mises	O
distributions	O
,	O
we	O
obtain	O
a	O
ﬂexible	O
framework	O
for	O
modelling	O
periodic	O
variables	O
that	O
can	O
handle	O
multimodality	B
.	O
for	O
an	O
example	O
of	O
a	O
machine	O
learn-	O
ing	O
application	O
that	O
makes	O
use	O
of	O
von	O
mises	O
distributions	O
,	O
see	O
lawrence	O
et	O
al	O
.	O
(	O
2002	O
)	O
,	O
and	O
for	O
extensions	O
to	O
modelling	O
conditional	B
densities	O
for	B
regression	I
problems	O
,	O
see	O
bishop	O
and	O
nabney	O
(	O
1996	O
)	O
.	O
2.3.9	O
mixtures	O
of	O
gaussians	O
while	O
the	O
gaussian	O
distribution	O
has	O
some	O
important	O
analytical	O
properties	O
,	O
it	O
suf-	O
fers	O
from	O
signiﬁcant	O
limitations	O
when	O
it	O
comes	O
to	O
modelling	O
real	O
data	O
sets	O
.	O
consider	O
the	O
example	O
shown	O
in	O
figure	O
2.21.	O
this	O
is	O
known	O
as	O
the	O
‘	O
old	O
faithful	O
’	O
data	O
set	O
,	O
and	O
comprises	O
272	O
measurements	O
of	O
the	O
eruption	O
of	O
the	O
old	O
faithful	O
geyser	O
at	O
yel-	O
lowstone	O
national	O
park	O
in	O
the	O
usa	O
.	O
each	O
measurement	O
comprises	O
the	O
duration	O
of	O
appendix	O
a	O
2.3.	O
the	O
gaussian	O
distribution	O
111	O
figure	O
2.22	O
example	O
of	O
a	O
gaussian	O
mixture	B
distribution	I
in	O
one	O
dimension	O
showing	O
three	O
gaussians	O
(	O
each	O
scaled	O
by	O
a	O
coefﬁcient	O
)	O
in	O
blue	O
and	O
their	O
sum	O
in	O
red	O
.	O
p	O
(	O
x	O
)	O
the	O
eruption	O
in	O
minutes	O
(	O
horizontal	O
axis	O
)	O
and	O
the	O
time	O
in	O
minutes	O
to	O
the	O
next	O
erup-	O
tion	O
(	O
vertical	O
axis	O
)	O
.	O
we	O
see	O
that	O
the	O
data	O
set	O
forms	O
two	O
dominant	O
clumps	O
,	O
and	O
that	O
a	O
simple	O
gaussian	O
distribution	O
is	O
unable	O
to	O
capture	O
this	O
structure	O
,	O
whereas	O
a	O
linear	O
superposition	O
of	O
two	O
gaussians	O
gives	O
a	O
better	O
characterization	O
of	O
the	O
data	O
set	O
.	O
x	O
such	O
superpositions	O
,	O
formed	O
by	O
taking	O
linear	O
combinations	O
of	O
more	O
basic	O
dis-	O
tributions	O
such	O
as	O
gaussians	O
,	O
can	O
be	O
formulated	O
as	O
probabilistic	O
models	O
known	O
as	O
mixture	B
distributions	O
(	O
mclachlan	O
and	O
basford	O
,	O
1988	O
;	O
mclachlan	O
and	O
peel	O
,	O
2000	O
)	O
.	O
in	O
figure	O
2.22	O
we	O
see	O
that	O
a	O
linear	O
combination	O
of	O
gaussians	O
can	O
give	O
rise	O
to	O
very	O
complex	O
densities	O
.	O
by	O
using	O
a	O
sufﬁcient	O
number	O
of	O
gaussians	O
,	O
and	O
by	O
adjusting	O
their	O
means	O
and	O
covariances	O
as	O
well	O
as	O
the	O
coefﬁcients	O
in	O
the	O
linear	O
combination	O
,	O
almost	O
any	O
continuous	O
density	B
can	O
be	O
approximated	O
to	O
arbitrary	O
accuracy	O
.	O
we	O
therefore	O
consider	O
a	O
superposition	O
of	O
k	O
gaussian	O
densities	O
of	O
the	O
form	O
p	O
(	O
x	O
)	O
=	O
πkn	O
(	O
x|µk	O
,	O
σk	O
)	O
(	O
2.188	O
)	O
k=1	O
which	O
is	O
called	O
a	O
mixture	O
of	O
gaussians	O
.	O
each	O
gaussian	O
density	B
n	O
(	O
x|µk	O
,	O
σk	O
)	O
is	O
called	O
a	O
component	O
of	O
the	O
mixture	B
and	O
has	O
its	O
own	O
mean	B
µk	O
and	O
covariance	B
σk	O
.	O
contour	O
and	O
surface	O
plots	O
for	O
a	O
gaussian	O
mixture	B
having	O
3	O
components	O
are	O
shown	O
in	O
figure	O
2.23.	O
in	O
this	O
section	O
we	O
shall	O
consider	O
gaussian	O
components	O
to	O
illustrate	O
the	O
frame-	O
work	O
of	O
mixture	B
models	O
.	O
more	O
generally	O
,	O
mixture	B
models	O
can	O
comprise	O
linear	O
com-	O
binations	O
of	O
other	O
distributions	O
.	O
for	O
instance	O
,	O
in	O
section	O
9.3.3	O
we	O
shall	O
consider	O
mixtures	O
of	O
bernoulli	O
distributions	O
as	O
an	O
example	O
of	O
a	O
mixture	B
model	I
for	O
discrete	O
variables	O
.	O
section	O
9.3.3	O
the	O
parameters	O
πk	O
in	O
(	O
2.188	O
)	O
are	O
called	O
mixing	O
coefﬁcients	O
.	O
if	O
we	O
integrate	O
both	O
sides	O
of	O
(	O
2.188	O
)	O
with	O
respect	O
to	O
x	O
,	O
and	O
note	O
that	O
both	O
p	O
(	O
x	O
)	O
and	O
the	O
individual	O
gaussian	O
components	O
are	O
normalized	O
,	O
we	O
obtain	O
k	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
πk	O
=	O
1	O
.	O
(	O
2.189	O
)	O
also	O
,	O
the	O
requirement	O
that	O
p	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
0	O
,	O
together	O
with	O
n	O
(	O
x|µk	O
,	O
σk	O
)	O
(	O
cid:2	O
)	O
0	O
,	O
implies	O
πk	O
(	O
cid:2	O
)	O
0	O
for	O
all	O
k.	O
combining	O
this	O
with	O
the	O
condition	O
(	O
2.189	O
)	O
we	O
obtain	O
k=1	O
0	O
(	O
cid:1	O
)	O
πk	O
(	O
cid:1	O
)	O
1	O
.	O
(	O
2.190	O
)	O
112	O
2.	O
probability	B
distributions	O
1	O
(	O
a	O
)	O
1	O
(	O
b	O
)	O
0.5	O
0	O
0.5	O
0.3	O
0.2	O
0.5	O
0	O
0	O
0.5	O
1	O
0	O
0.5	O
1	O
figure	O
2.23	O
illustration	O
of	O
a	O
mixture	O
of	O
3	O
gaussians	O
in	O
a	O
two-dimensional	O
space	O
.	O
(	O
a	O
)	O
contours	O
of	O
constant	O
density	B
for	O
each	O
of	O
the	O
mixture	B
components	O
,	O
in	O
which	O
the	O
3	O
components	O
are	O
denoted	O
red	O
,	O
blue	O
and	O
green	O
,	O
and	O
the	O
values	O
of	O
the	O
mixing	O
coefﬁcients	O
are	O
shown	O
below	O
each	O
component	O
.	O
(	O
b	O
)	O
contours	O
of	O
the	O
marginal	B
probability	I
density	O
p	O
(	O
x	O
)	O
of	O
the	O
mixture	B
distribution	I
.	O
(	O
c	O
)	O
a	O
surface	O
plot	O
of	O
the	O
distribution	O
p	O
(	O
x	O
)	O
.	O
we	O
therefore	O
see	O
that	O
the	O
mixing	O
coefﬁcients	O
satisfy	O
the	O
requirements	O
to	O
be	O
probabil-	O
ities	O
.	O
from	O
the	O
sum	O
and	O
product	O
rules	O
,	O
the	O
marginal	B
density	O
is	O
given	O
by	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
k	O
)	O
p	O
(	O
x|k	O
)	O
(	O
2.191	O
)	O
k	O
(	O
cid:2	O
)	O
k=1	O
which	O
is	O
equivalent	O
to	O
(	O
2.188	O
)	O
in	O
which	O
we	O
can	O
view	O
πk	O
=	O
p	O
(	O
k	O
)	O
as	O
the	O
prior	B
prob-	O
ability	O
of	O
picking	O
the	O
kth	O
component	O
,	O
and	O
the	O
density	B
n	O
(	O
x|µk	O
,	O
σk	O
)	O
=	O
p	O
(	O
x|k	O
)	O
as	O
the	O
probability	B
of	O
x	O
conditioned	O
on	O
k.	O
as	O
we	O
shall	O
see	O
in	O
later	O
chapters	O
,	O
an	O
impor-	O
tant	O
role	O
is	O
played	O
by	O
the	O
posterior	O
probabilities	O
p	O
(	O
k|x	O
)	O
,	O
which	O
are	O
also	O
known	O
as	O
responsibilities	O
.	O
from	O
bayes	O
’	O
theorem	O
these	O
are	O
given	O
by	O
γk	O
(	O
x	O
)	O
≡	O
p	O
(	O
k|x	O
)	O
(	O
cid:5	O
)	O
p	O
(	O
k	O
)	O
p	O
(	O
x|k	O
)	O
(	O
cid:5	O
)	O
l	O
p	O
(	O
l	O
)	O
p	O
(	O
x|l	O
)	O
πkn	O
(	O
x|µk	O
,	O
σk	O
)	O
l	O
πln	O
(	O
x|µl	O
,	O
σl	O
)	O
.	O
=	O
=	O
we	O
shall	O
discuss	O
the	O
probabilistic	O
interpretation	O
of	O
the	O
mixture	B
distribution	I
in	O
greater	O
detail	O
in	O
chapter	O
9.	O
the	O
form	O
of	O
the	O
gaussian	O
mixture	B
distribution	I
is	O
governed	O
by	O
the	O
parameters	O
π	O
,	O
µ	O
and	O
σ	O
,	O
where	O
we	O
have	O
used	O
the	O
notation	O
π	O
≡	O
{	O
π1	O
,	O
.	O
.	O
.	O
,	O
πk	O
}	O
,	O
µ	O
≡	O
{	O
µ1	O
,	O
.	O
.	O
.	O
,	O
µk	O
}	O
and	O
σ	O
≡	O
{	O
σ1	O
,	O
.	O
.	O
.	O
σk	O
}	O
.	O
one	O
way	O
to	O
set	O
the	O
values	O
of	O
these	O
parameters	O
is	O
to	O
use	O
maximum	B
likelihood	I
.	O
from	O
(	O
2.188	O
)	O
the	O
log	O
of	O
the	O
likelihood	B
function	I
is	O
given	O
by	O
n	O
(	O
cid:2	O
)	O
(	O
cid:24	O
)	O
k	O
(	O
cid:2	O
)	O
(	O
cid:25	O
)	O
πkn	O
(	O
xn|µk	O
,	O
σk	O
)	O
n=1	O
k=1	O
ln	O
p	O
(	O
x|π	O
,	O
µ	O
,	O
σ	O
)	O
=	O
ln	O
(	O
2.192	O
)	O
(	O
2.193	O
)	O
2.4.	O
the	O
exponential	B
family	I
113	O
where	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
.	O
we	O
immediately	O
see	O
that	O
the	O
situation	O
is	O
now	O
much	O
more	O
complex	O
than	O
with	O
a	O
single	O
gaussian	O
,	O
due	O
to	O
the	O
presence	O
of	O
the	O
summation	O
over	O
k	O
inside	O
the	O
logarithm	O
.	O
as	O
a	O
result	O
,	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
parameters	O
no	O
longer	O
has	O
a	O
closed-form	O
analytical	O
solution	O
.	O
one	O
approach	O
to	O
maxi-	O
mizing	O
the	O
likelihood	B
function	I
is	O
to	O
use	O
iterative	O
numerical	O
optimization	O
techniques	O
(	O
fletcher	O
,	O
1987	O
;	O
nocedal	O
and	O
wright	O
,	O
1999	O
;	O
bishop	O
and	O
nabney	O
,	O
2008	O
)	O
.	O
alterna-	O
tively	O
we	O
can	O
employ	O
a	O
powerful	O
framework	O
called	O
expectation	B
maximization	I
,	O
which	O
will	O
be	O
discussed	O
at	O
length	O
in	O
chapter	O
9	O
.	O
2.4.	O
the	O
exponential	B
family	I
the	O
probability	B
distributions	O
that	O
we	O
have	O
studied	O
so	O
far	O
in	O
this	O
chapter	O
(	O
with	O
the	O
exception	O
of	O
the	O
gaussian	O
mixture	B
)	O
are	O
speciﬁc	O
examples	O
of	O
a	O
broad	O
class	O
of	O
distri-	O
butions	O
called	O
the	O
exponential	B
family	I
(	O
duda	O
and	O
hart	O
,	O
1973	O
;	O
bernardo	O
and	O
smith	O
,	O
1994	O
)	O
.	O
members	O
of	O
the	O
exponential	B
family	I
have	O
many	O
important	O
properties	O
in	O
com-	O
mon	O
,	O
and	O
it	O
is	O
illuminating	O
to	O
discuss	O
these	O
properties	O
in	O
some	O
generality	O
.	O
the	O
exponential	B
family	I
of	O
distributions	O
over	O
x	O
,	O
given	O
parameters	O
η	O
,	O
is	O
deﬁned	O
to	O
be	O
the	O
set	O
of	O
distributions	O
of	O
the	O
form	O
p	O
(	O
x|η	O
)	O
=	O
h	O
(	O
x	O
)	O
g	O
(	O
η	O
)	O
exp	O
ηtu	O
(	O
x	O
)	O
(	O
2.194	O
)	O
where	O
x	O
may	O
be	O
scalar	O
or	O
vector	O
,	O
and	O
may	O
be	O
discrete	O
or	O
continuous	O
.	O
here	O
η	O
are	O
called	O
the	O
natural	B
parameters	I
of	O
the	O
distribution	O
,	O
and	O
u	O
(	O
x	O
)	O
is	O
some	O
function	O
of	O
x.	O
the	O
function	O
g	O
(	O
η	O
)	O
can	O
be	O
interpreted	O
as	O
the	O
coefﬁcient	O
that	O
ensures	O
that	O
the	O
distribu-	O
tion	O
is	O
normalized	O
and	O
therefore	O
satisﬁes	O
(	O
cid:6	O
)	O
g	O
(	O
η	O
)	O
h	O
(	O
x	O
)	O
exp	O
ηtu	O
(	O
x	O
)	O
dx	O
=	O
1	O
(	O
2.195	O
)	O
(	O
cid:27	O
)	O
(	O
cid:26	O
)	O
(	O
cid:27	O
)	O
where	O
the	O
integration	O
is	O
replaced	O
by	O
summation	O
if	O
x	O
is	O
a	O
discrete	O
variable	O
.	O
we	O
begin	O
by	O
taking	O
some	O
examples	O
of	O
the	O
distributions	O
introduced	O
earlier	O
in	O
the	O
chapter	O
and	O
showing	O
that	O
they	O
are	O
indeed	O
members	O
of	O
the	O
exponential	B
family	I
.	O
consider	O
ﬁrst	O
the	O
bernoulli	O
distribution	O
p	O
(	O
x|µ	O
)	O
=	O
bern	O
(	O
x|µ	O
)	O
=	O
µx	O
(	O
1	O
−	O
µ	O
)	O
1−x	O
.	O
(	O
2.196	O
)	O
expressing	O
the	O
right-hand	O
side	O
as	O
the	O
exponential	O
of	O
the	O
logarithm	O
,	O
we	O
have	O
(	O
cid:26	O
)	O
(	O
cid:15	O
)	O
p	O
(	O
x|µ	O
)	O
=	O
exp	O
{	O
x	O
ln	O
µ	O
+	O
(	O
1	O
−	O
x	O
)	O
ln	O
(	O
1	O
−	O
µ	O
)	O
}	O
=	O
(	O
1	O
−	O
µ	O
)	O
exp	O
ln	O
µ	O
1	O
−	O
µ	O
(	O
cid:16	O
)	O
(	O
cid:13	O
)	O
(	O
cid:12	O
)	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
η	O
=	O
ln	O
µ	O
1	O
−	O
µ	O
x	O
.	O
(	O
2.197	O
)	O
(	O
2.198	O
)	O
comparison	O
with	O
(	O
2.194	O
)	O
allows	O
us	O
to	O
identify	O
114	O
2.	O
probability	B
distributions	O
which	O
we	O
can	O
solve	O
for	O
µ	O
to	O
give	O
µ	O
=	O
σ	O
(	O
η	O
)	O
,	O
where	O
σ	O
(	O
η	O
)	O
=	O
1	O
1	O
+	O
exp	O
(	O
−η	O
)	O
(	O
2.199	O
)	O
is	O
called	O
the	O
logistic	B
sigmoid	I
function	O
.	O
thus	O
we	O
can	O
write	O
the	O
bernoulli	O
distribution	O
using	O
the	O
standard	O
representation	O
(	O
2.194	O
)	O
in	O
the	O
form	O
p	O
(	O
x|η	O
)	O
=	O
σ	O
(	O
−η	O
)	O
exp	O
(	O
ηx	O
)	O
(	O
2.200	O
)	O
where	O
we	O
have	O
used	O
1	O
−	O
σ	O
(	O
η	O
)	O
=	O
σ	O
(	O
−η	O
)	O
,	O
which	O
is	O
easily	O
proved	O
from	O
(	O
2.199	O
)	O
.	O
com-	O
parison	O
with	O
(	O
2.194	O
)	O
shows	O
that	O
next	O
consider	O
the	O
multinomial	B
distribution	I
that	O
,	O
for	O
a	O
single	O
observation	O
x	O
,	O
takes	O
the	O
form	O
p	O
(	O
x|µ	O
)	O
=	O
m	O
(	O
cid:14	O
)	O
u	O
(	O
x	O
)	O
=	O
x	O
h	O
(	O
x	O
)	O
=	O
1	O
g	O
(	O
η	O
)	O
=	O
σ	O
(	O
−η	O
)	O
.	O
(	O
cid:24	O
)	O
m	O
(	O
cid:2	O
)	O
(	O
cid:25	O
)	O
(	O
2.201	O
)	O
(	O
2.202	O
)	O
(	O
2.203	O
)	O
µxk	O
k	O
=	O
exp	O
xk	O
ln	O
µk	O
(	O
2.204	O
)	O
k=1	O
k=1	O
where	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
t.	O
again	O
,	O
we	O
can	O
write	O
this	O
in	O
the	O
standard	O
representation	O
(	O
2.194	O
)	O
so	O
that	O
(	O
2.205	O
)	O
where	O
ηk	O
=	O
ln	O
µk	O
,	O
and	O
we	O
have	O
deﬁned	O
η	O
=	O
(	O
η1	O
,	O
.	O
.	O
.	O
,	O
ηm	O
)	O
t.	O
again	O
,	O
comparing	O
with	O
(	O
2.194	O
)	O
we	O
have	O
p	O
(	O
x|η	O
)	O
=	O
exp	O
(	O
ηtx	O
)	O
u	O
(	O
x	O
)	O
=	O
x	O
h	O
(	O
x	O
)	O
=	O
1	O
g	O
(	O
η	O
)	O
=	O
1	O
.	O
(	O
2.206	O
)	O
(	O
2.207	O
)	O
(	O
2.208	O
)	O
note	O
that	O
the	O
parameters	O
ηk	O
are	O
not	O
independent	B
because	O
the	O
parameters	O
µk	O
are	O
sub-	O
ject	O
to	O
the	O
constraint	O
m	O
(	O
cid:2	O
)	O
µk	O
=	O
1	O
k=1	O
(	O
2.209	O
)	O
so	O
that	O
,	O
given	O
any	O
m	O
−	O
1	O
of	O
the	O
parameters	O
µk	O
,	O
the	O
value	O
of	O
the	O
remaining	O
parameter	O
is	O
ﬁxed	O
.	O
in	O
some	O
circumstances	O
,	O
it	O
will	O
be	O
convenient	O
to	O
remove	O
this	O
constraint	O
by	O
expressing	O
the	O
distribution	O
in	O
terms	O
of	O
only	O
m	O
−	O
1	O
parameters	O
.	O
this	O
can	O
be	O
achieved	O
by	O
using	O
the	O
relationship	O
(	O
2.209	O
)	O
to	O
eliminate	O
µm	O
by	O
expressing	O
it	O
in	O
terms	O
of	O
the	O
remaining	O
{	O
µk	O
}	O
where	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
−	O
1	O
,	O
thereby	O
leaving	O
m	O
−	O
1	O
parameters	O
.	O
note	O
that	O
these	O
remaining	O
parameters	O
are	O
still	O
subject	O
to	O
the	O
constraints	O
0	O
(	O
cid:1	O
)	O
µk	O
(	O
cid:1	O
)	O
1	O
,	O
µk	O
(	O
cid:1	O
)	O
1	O
.	O
(	O
2.210	O
)	O
m−1	O
(	O
cid:2	O
)	O
k=1	O
(	O
cid:25	O
)	O
exp	O
(	O
cid:24	O
)	O
m	O
(	O
cid:2	O
)	O
k=1	O
=	O
exp	O
=	O
exp	O
xk	O
ln	O
µk	O
(	O
cid:24	O
)	O
(	O
cid:24	O
)	O
m−1	O
(	O
cid:2	O
)	O
m−1	O
(	O
cid:2	O
)	O
k=1	O
k=1	O
we	O
now	O
identify	O
(	O
cid:23	O
)	O
ln	O
+	O
ln	O
(	O
cid:22	O
)	O
1	O
−	O
m−1	O
(	O
cid:2	O
)	O
(	O
cid:22	O
)	O
1	O
−	O
m−1	O
(	O
cid:2	O
)	O
k=1	O
k=1	O
(	O
cid:23	O
)	O
(	O
cid:25	O
)	O
(	O
cid:23	O
)	O
(	O
cid:25	O
)	O
µk	O
µk	O
.	O
(	O
2.211	O
)	O
=	O
ηk	O
(	O
2.212	O
)	O
xk	O
ln	O
µk	O
+	O
(	O
cid:22	O
)	O
xk	O
ln	O
ln	O
xk	O
(	O
cid:23	O
)	O
(	O
cid:23	O
)	O
k=1	O
µk	O
j=1	O
µj	O
(	O
cid:22	O
)	O
1	O
−	O
m−1	O
(	O
cid:2	O
)	O
1	O
−	O
(	O
cid:5	O
)	O
m−1	O
(	O
cid:22	O
)	O
1	O
−	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
j	O
µj	O
µk	O
µk	O
=	O
exp	O
(	O
ηk	O
)	O
j	O
exp	O
(	O
ηj	O
)	O
.	O
1	O
+	O
(	O
cid:22	O
)	O
m−1	O
(	O
cid:2	O
)	O
(	O
cid:23	O
)	O
−1	O
2.4.	O
the	O
exponential	B
family	I
115	O
making	O
use	O
of	O
the	O
constraint	O
(	O
2.209	O
)	O
,	O
the	O
multinomial	B
distribution	I
in	O
this	O
representa-	O
tion	O
then	O
becomes	O
which	O
we	O
can	O
solve	O
for	O
µk	O
by	O
ﬁrst	O
summing	O
both	O
sides	O
over	O
k	O
and	O
then	O
rearranging	O
and	O
back-substituting	O
to	O
give	O
(	O
2.213	O
)	O
this	O
is	O
called	O
the	O
softmax	B
function	I
,	O
or	O
the	O
normalized	B
exponential	I
.	O
in	O
this	O
represen-	O
tation	O
,	O
the	O
multinomial	B
distribution	I
therefore	O
takes	O
the	O
form	O
p	O
(	O
x|η	O
)	O
=	O
1	O
+	O
exp	O
(	O
ηk	O
)	O
exp	O
(	O
ηtx	O
)	O
.	O
(	O
2.214	O
)	O
this	O
is	O
the	O
standard	O
form	O
of	O
the	O
exponential	B
family	I
,	O
with	O
parameter	O
vector	O
η	O
=	O
(	O
η1	O
,	O
.	O
.	O
.	O
,	O
ηm−1	O
)	O
t	O
in	O
which	O
k=1	O
u	O
(	O
x	O
)	O
=	O
x	O
h	O
(	O
x	O
)	O
=	O
1	O
(	O
cid:22	O
)	O
g	O
(	O
η	O
)	O
=	O
1	O
+	O
m−1	O
(	O
cid:2	O
)	O
k=1	O
(	O
cid:23	O
)	O
−1	O
exp	O
(	O
ηk	O
)	O
.	O
(	O
2.215	O
)	O
(	O
2.216	O
)	O
(	O
2.217	O
)	O
finally	O
,	O
let	O
us	O
consider	O
the	O
gaussian	O
distribution	O
.	O
for	O
the	O
univariate	O
gaussian	O
,	O
we	O
have	O
p	O
(	O
x|µ	O
,	O
σ2	O
)	O
=	O
=	O
1	O
(	O
2πσ2	O
)	O
1/2	O
exp	O
(	O
2πσ2	O
)	O
1/2	O
exp	O
1	O
−	O
1	O
2σ2	O
(	O
x	O
−	O
µ	O
)	O
2	O
−	O
1	O
2σ2	O
x2	O
+	O
µ	O
σ2	O
x	O
−	O
1	O
2σ2	O
µ2	O
(	O
cid:13	O
)	O
(	O
2.218	O
)	O
(	O
2.219	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
116	O
2.	O
probability	B
distributions	O
exercise	O
2.57	O
exercise	O
2.58	O
which	O
,	O
after	O
some	O
simple	O
rearrangement	O
,	O
can	O
be	O
cast	O
in	O
the	O
standard	O
exponential	O
family	O
form	O
(	O
2.194	O
)	O
with	O
η	O
=	O
(	O
cid:16	O
)	O
(	O
cid:15	O
)	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
µ/σ2	O
−1/2σ2	O
x	O
x2	O
u	O
(	O
x	O
)	O
=	O
h	O
(	O
x	O
)	O
=	O
(	O
2π	O
)	O
−1/2	O
g	O
(	O
η	O
)	O
=	O
(	O
−2η2	O
)	O
1/2	O
exp	O
(	O
2.220	O
)	O
(	O
2.221	O
)	O
(	O
2.222	O
)	O
(	O
2.223	O
)	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
.	O
η2	O
1	O
4η2	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:26	O
)	O
(	O
cid:26	O
)	O
(	O
cid:27	O
)	O
(	O
cid:27	O
)	O
2.4.1	O
maximum	B
likelihood	I
and	O
sufﬁcient	B
statistics	I
let	O
us	O
now	O
consider	O
the	O
problem	O
of	O
estimating	O
the	O
parameter	O
vector	O
η	O
in	O
the	O
gen-	O
eral	O
exponential	B
family	I
distribution	O
(	O
2.194	O
)	O
using	O
the	O
technique	O
of	O
maximum	O
likeli-	O
hood	O
.	O
taking	O
the	O
gradient	O
of	O
both	O
sides	O
of	O
(	O
2.195	O
)	O
with	O
respect	O
to	O
η	O
,	O
we	O
have	O
(	O
cid:26	O
)	O
(	O
cid:27	O
)	O
∇g	O
(	O
η	O
)	O
h	O
(	O
x	O
)	O
exp	O
ηtu	O
(	O
x	O
)	O
dx	O
+	O
g	O
(	O
η	O
)	O
h	O
(	O
x	O
)	O
exp	O
ηtu	O
(	O
x	O
)	O
u	O
(	O
x	O
)	O
dx	O
=	O
0	O
.	O
(	O
2.224	O
)	O
rearranging	O
,	O
and	O
making	O
use	O
again	O
of	O
(	O
2.195	O
)	O
then	O
gives	O
−	O
1	O
g	O
(	O
η	O
)	O
∇g	O
(	O
η	O
)	O
=	O
g	O
(	O
η	O
)	O
h	O
(	O
x	O
)	O
exp	O
ηtu	O
(	O
x	O
)	O
u	O
(	O
x	O
)	O
dx	O
=	O
e	O
[	O
u	O
(	O
x	O
)	O
]	O
where	O
we	O
have	O
used	O
(	O
2.194	O
)	O
.	O
we	O
therefore	O
obtain	O
the	O
result	O
−∇	O
ln	O
g	O
(	O
η	O
)	O
=	O
e	O
[	O
u	O
(	O
x	O
)	O
]	O
.	O
(	O
2.225	O
)	O
(	O
2.226	O
)	O
note	O
that	O
the	O
covariance	B
of	O
u	O
(	O
x	O
)	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
second	O
derivatives	O
of	O
g	O
(	O
η	O
)	O
,	O
and	O
similarly	O
for	O
higher	O
order	O
moments	O
.	O
thus	O
,	O
provided	O
we	O
can	O
normalize	O
a	O
distribution	O
from	O
the	O
exponential	B
family	I
,	O
we	O
can	O
always	O
ﬁnd	O
its	O
moments	O
by	O
simple	O
differentiation	O
.	O
n	O
(	O
cid:2	O
)	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
,	O
for	O
which	O
the	O
likelihood	B
function	I
is	O
given	O
by	O
now	O
consider	O
a	O
set	O
of	O
independent	B
identically	I
distributed	I
data	O
denoted	O
by	O
x	O
=	O
n	O
(	O
cid:14	O
)	O
(	O
cid:24	O
)	O
(	O
cid:25	O
)	O
(	O
cid:22	O
)	O
(	O
cid:23	O
)	O
h	O
(	O
xn	O
)	O
g	O
(	O
η	O
)	O
n	O
exp	O
ηt	O
n=1	O
n=1	O
u	O
(	O
xn	O
)	O
.	O
(	O
2.227	O
)	O
p	O
(	O
x|η	O
)	O
=	O
setting	O
the	O
gradient	O
of	O
ln	O
p	O
(	O
x|η	O
)	O
with	O
respect	O
to	O
η	O
to	O
zero	O
,	O
we	O
get	O
the	O
following	O
condition	O
to	O
be	O
satisﬁed	O
by	O
the	O
maximum	B
likelihood	I
estimator	O
ηml	O
−∇	O
ln	O
g	O
(	O
ηml	O
)	O
=	O
1	O
n	O
u	O
(	O
xn	O
)	O
(	O
2.228	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
which	O
can	O
in	O
principle	O
be	O
solved	O
to	O
obtain	O
ηml	O
.	O
we	O
see	O
that	O
the	O
solution	O
for	O
the	O
n	O
u	O
(	O
xn	O
)	O
,	O
which	O
maximum	B
likelihood	I
estimator	O
depends	O
on	O
the	O
data	O
only	O
through	O
is	O
therefore	O
called	O
the	O
sufﬁcient	O
statistic	O
of	O
the	O
distribution	O
(	O
2.194	O
)	O
.	O
we	O
do	O
not	O
need	O
to	O
store	O
the	O
entire	O
data	O
set	O
itself	O
but	O
only	O
the	O
value	O
of	O
the	O
sufﬁcient	O
statistic	O
.	O
for	O
the	O
bernoulli	O
distribution	O
,	O
for	O
example	O
,	O
the	O
function	O
u	O
(	O
x	O
)	O
is	O
given	O
just	O
by	O
x	O
and	O
so	O
we	O
need	O
only	O
keep	O
the	O
sum	O
of	O
the	O
data	O
points	O
{	O
xn	O
}	O
,	O
whereas	O
for	O
the	O
gaussian	O
u	O
(	O
x	O
)	O
=	O
(	O
x	O
,	O
x2	O
)	O
t	O
,	O
and	O
so	O
we	O
should	O
keep	O
both	O
the	O
sum	O
of	O
{	O
xn	O
}	O
and	O
the	O
sum	O
of	O
{	O
x2	O
n	O
}	O
.	O
if	O
we	O
consider	O
the	O
limit	O
n	O
→	O
∞	O
,	O
then	O
the	O
right-hand	O
side	O
of	O
(	O
2.228	O
)	O
becomes	O
e	O
[	O
u	O
(	O
x	O
)	O
]	O
,	O
and	O
so	O
by	O
comparing	O
with	O
(	O
2.226	O
)	O
we	O
see	O
that	O
in	O
this	O
limit	O
ηml	O
will	O
equal	O
the	O
true	O
value	O
η.	O
in	O
fact	O
,	O
this	O
sufﬁciency	O
property	O
holds	O
also	O
for	O
bayesian	O
inference	B
,	O
although	O
we	O
shall	O
defer	O
discussion	O
of	O
this	O
until	O
chapter	O
8	O
when	O
we	O
have	O
equipped	O
ourselves	O
with	O
the	O
tools	O
of	O
graphical	O
models	O
and	O
can	O
thereby	O
gain	O
a	O
deeper	O
insight	O
into	O
these	O
important	O
concepts	O
.	O
2.4.2	O
conjugate	B
priors	O
we	O
have	O
already	O
encountered	O
the	O
concept	O
of	O
a	O
conjugate	B
prior	I
several	O
times	O
,	O
for	O
example	O
in	O
the	O
context	O
of	O
the	O
bernoulli	O
distribution	O
(	O
for	O
which	O
the	O
conjugate	B
prior	I
is	O
the	O
beta	B
distribution	I
)	O
or	O
the	O
gaussian	O
(	O
where	O
the	O
conjugate	B
prior	I
for	O
the	O
mean	B
is	O
a	O
gaussian	O
,	O
and	O
the	O
conjugate	B
prior	I
for	O
the	O
precision	O
is	O
the	O
wishart	O
distribution	O
)	O
.	O
in	O
general	O
,	O
for	O
a	O
given	O
probability	B
distribution	O
p	O
(	O
x|η	O
)	O
,	O
we	O
can	O
seek	O
a	O
prior	B
p	O
(	O
η	O
)	O
that	O
is	O
conjugate	B
to	O
the	O
likelihood	B
function	I
,	O
so	O
that	O
the	O
posterior	O
distribution	O
has	O
the	O
same	O
functional	B
form	O
as	O
the	O
prior	B
.	O
for	O
any	O
member	O
of	O
the	O
exponential	B
family	I
(	O
2.194	O
)	O
,	O
there	O
exists	O
a	O
conjugate	B
prior	I
that	O
can	O
be	O
written	O
in	O
the	O
form	O
p	O
(	O
η|χ	O
,	O
ν	O
)	O
=	O
f	O
(	O
χ	O
,	O
ν	O
)	O
g	O
(	O
η	O
)	O
ν	O
exp	O
νηtχ	O
(	O
cid:26	O
)	O
(	O
cid:27	O
)	O
(	O
2.229	O
)	O
where	O
f	O
(	O
χ	O
,	O
ν	O
)	O
is	O
a	O
normalization	O
coefﬁcient	O
,	O
and	O
g	O
(	O
η	O
)	O
is	O
the	O
same	O
function	O
as	O
ap-	O
pears	O
in	O
(	O
2.194	O
)	O
.	O
to	O
see	O
that	O
this	O
is	O
indeed	O
conjugate	B
,	O
let	O
us	O
multiply	O
the	O
prior	B
(	O
2.229	O
)	O
by	O
the	O
likelihood	B
function	I
(	O
2.227	O
)	O
to	O
obtain	O
the	O
posterior	O
distribution	O
,	O
up	O
to	O
a	O
nor-	O
malization	O
coefﬁcient	O
,	O
in	O
the	O
form	O
(	O
cid:24	O
)	O
(	O
cid:22	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:23	O
)	O
(	O
cid:25	O
)	O
2.4.	O
the	O
exponential	B
family	I
117	O
(	O
cid:5	O
)	O
p	O
(	O
η|x	O
,	O
χ	O
,	O
ν	O
)	O
∝	O
g	O
(	O
η	O
)	O
ν+n	O
exp	O
ηt	O
u	O
(	O
xn	O
)	O
+	O
νχ	O
.	O
(	O
2.230	O
)	O
n=1	O
this	O
again	O
takes	O
the	O
same	O
functional	B
form	O
as	O
the	O
prior	B
(	O
2.229	O
)	O
,	O
conﬁrming	O
conjugacy	O
.	O
furthermore	O
,	O
we	O
see	O
that	O
the	O
parameter	O
ν	O
can	O
be	O
interpreted	O
as	O
a	O
effective	O
number	O
of	O
pseudo-observations	O
in	O
the	O
prior	B
,	O
each	O
of	O
which	O
has	O
a	O
value	O
for	O
the	O
sufﬁcient	O
statistic	O
u	O
(	O
x	O
)	O
given	O
by	O
χ	O
.	O
2.4.3	O
noninformative	B
priors	O
in	O
some	O
applications	O
of	O
probabilistic	O
inference	O
,	O
we	O
may	O
have	O
prior	B
knowledge	O
that	O
can	O
be	O
conveniently	O
expressed	O
through	O
the	O
prior	B
distribution	O
.	O
for	O
example	O
,	O
if	O
the	O
prior	B
assigns	O
zero	O
probability	B
to	O
some	O
value	O
of	O
variable	O
,	O
then	O
the	O
posterior	O
dis-	O
tribution	O
will	O
necessarily	O
also	O
assign	O
zero	O
probability	B
to	O
that	O
value	O
,	O
irrespective	O
of	O
118	O
2.	O
probability	B
distributions	O
any	O
subsequent	O
observations	O
of	O
data	O
.	O
in	O
many	O
cases	O
,	O
however	O
,	O
we	O
may	O
have	O
little	O
idea	O
of	O
what	O
form	O
the	O
distribution	O
should	O
take	O
.	O
we	O
may	O
then	O
seek	O
a	O
form	O
of	O
prior	B
distribution	O
,	O
called	O
a	O
noninformative	B
prior	I
,	O
which	O
is	O
intended	O
to	O
have	O
as	O
little	O
inﬂu-	O
ence	O
on	O
the	O
posterior	O
distribution	O
as	O
possible	O
(	O
jeffries	O
,	O
1946	O
;	O
box	O
and	O
tao	O
,	O
1973	O
;	O
bernardo	O
and	O
smith	O
,	O
1994	O
)	O
.	O
this	O
is	O
sometimes	O
referred	O
to	O
as	O
‘	O
letting	O
the	O
data	O
speak	O
for	O
themselves	O
’	O
.	O
if	O
we	O
have	O
a	O
distribution	O
p	O
(	O
x|λ	O
)	O
governed	O
by	O
a	O
parameter	O
λ	O
,	O
we	O
might	O
be	O
tempted	O
to	O
propose	O
a	O
prior	B
distribution	O
p	O
(	O
λ	O
)	O
=	O
const	O
as	O
a	O
suitable	O
prior	B
.	O
if	O
λ	O
is	O
a	O
discrete	O
variable	O
with	O
k	O
states	O
,	O
this	O
simply	O
amounts	O
to	O
setting	O
the	O
prior	B
probability	O
of	O
each	O
state	O
to	O
1/k	O
.	O
in	O
the	O
case	O
of	O
continuous	O
parameters	O
,	O
however	O
,	O
there	O
are	O
two	O
potential	O
difﬁculties	O
with	O
this	O
approach	O
.	O
the	O
ﬁrst	O
is	O
that	O
,	O
if	O
the	O
domain	O
of	O
λ	O
is	O
unbounded	O
,	O
this	O
prior	B
distribution	O
can	O
not	O
be	O
correctly	O
normalized	O
because	O
the	O
integral	O
over	O
λ	O
diverges	O
.	O
such	O
priors	O
are	O
called	O
improper	B
.	O
in	O
practice	O
,	O
improper	B
priors	O
can	O
often	O
be	O
used	O
provided	O
the	O
corresponding	O
posterior	O
distribution	O
is	O
proper	O
,	O
i.e.	O
,	O
that	O
it	O
can	O
be	O
correctly	O
normalized	O
.	O
for	O
instance	O
,	O
if	O
we	O
put	O
a	O
uniform	O
prior	O
distribution	O
over	O
the	O
mean	B
of	O
a	O
gaussian	O
,	O
then	O
the	O
posterior	O
distribution	O
for	O
the	O
mean	B
,	O
once	O
we	O
have	O
observed	O
at	O
least	O
one	O
data	O
point	O
,	O
will	O
be	O
proper	O
.	O
a	O
second	O
difﬁculty	O
arises	O
from	O
the	O
transformation	O
behaviour	O
of	O
a	O
probability	B
density	O
under	O
a	O
nonlinear	O
change	O
of	O
variables	O
,	O
given	O
by	O
(	O
1.27	O
)	O
.	O
if	O
a	O
function	O
h	O
(	O
λ	O
)	O
is	O
constant	O
,	O
and	O
we	O
change	O
variables	O
to	O
λ	O
=	O
η2	O
,	O
then	O
(	O
cid:1	O
)	O
h	O
(	O
η	O
)	O
=	O
h	O
(	O
η2	O
)	O
will	O
also	O
be	O
constant	O
.	O
however	O
,	O
if	O
we	O
choose	O
the	O
density	B
pλ	O
(	O
λ	O
)	O
to	O
be	O
constant	O
,	O
then	O
the	O
density	B
of	O
η	O
will	O
be	O
given	O
,	O
from	O
(	O
1.27	O
)	O
,	O
by	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
dλ	O
dη	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
=	O
pλ	O
(	O
η2	O
)	O
2η	O
∝	O
η	O
pη	O
(	O
η	O
)	O
=	O
pλ	O
(	O
λ	O
)	O
(	O
2.231	O
)	O
and	O
so	O
the	O
density	B
over	O
η	O
will	O
not	O
be	O
constant	O
.	O
this	O
issue	O
does	O
not	O
arise	O
when	O
we	O
use	O
maximum	B
likelihood	I
,	O
because	O
the	O
likelihood	B
function	I
p	O
(	O
x|λ	O
)	O
is	O
a	O
simple	O
function	O
of	O
λ	O
and	O
so	O
we	O
are	O
free	O
to	O
use	O
any	O
convenient	O
parameterization	O
.	O
if	O
,	O
however	O
,	O
we	O
are	O
to	O
choose	O
a	O
prior	B
distribution	O
that	O
is	O
constant	O
,	O
we	O
must	O
take	O
care	O
to	O
use	O
an	O
appropriate	O
representation	O
for	O
the	O
parameters	O
.	O
here	O
we	O
consider	O
two	O
simple	O
examples	O
of	O
noninformative	B
priors	O
(	O
berger	O
,	O
1985	O
)	O
.	O
first	O
of	O
all	O
,	O
if	O
a	O
density	B
takes	O
the	O
form	O
p	O
(	O
x|µ	O
)	O
=	O
f	O
(	O
x	O
−	O
µ	O
)	O
(	O
2.232	O
)	O
then	O
the	O
parameter	O
µ	O
is	O
known	O
as	O
a	O
location	B
parameter	I
.	O
this	O
family	O
of	O
densities	O
exhibits	O
translation	B
invariance	I
because	O
if	O
we	O
shift	O
x	O
by	O
a	O
constant	O
to	O
give	O
(	O
cid:1	O
)	O
x	O
=	O
x	O
+	O
c	O
,	O
where	O
we	O
have	O
deﬁned	O
(	O
cid:1	O
)	O
µ	O
=	O
µ	O
+	O
c.	O
thus	O
the	O
density	B
takes	O
the	O
same	O
form	O
in	O
the	O
p	O
(	O
(	O
cid:1	O
)	O
x|	O
(	O
cid:1	O
)	O
µ	O
)	O
=	O
f	O
(	O
(	O
cid:1	O
)	O
x	O
−	O
(	O
cid:1	O
)	O
µ	O
)	O
(	O
2.233	O
)	O
then	O
new	O
variable	O
as	O
in	O
the	O
original	O
one	O
,	O
and	O
so	O
the	O
density	B
is	O
independent	B
of	O
the	O
choice	O
of	O
origin	O
.	O
we	O
would	O
like	O
to	O
choose	O
a	O
prior	B
distribution	O
that	O
reﬂects	O
this	O
translation	B
invariance	I
property	O
,	O
and	O
so	O
we	O
choose	O
a	O
prior	B
that	O
assigns	O
equal	O
probability	B
mass	O
to	O
2.4.	O
the	O
exponential	B
family	I
119	O
an	O
interval	O
a	O
(	O
cid:1	O
)	O
µ	O
(	O
cid:1	O
)	O
b	O
as	O
to	O
the	O
shifted	O
interval	O
a	O
−	O
c	O
(	O
cid:1	O
)	O
µ	O
(	O
cid:1	O
)	O
b	O
−	O
c.	O
this	O
implies	O
(	O
cid:6	O
)	O
b	O
(	O
cid:6	O
)	O
b−c	O
(	O
cid:6	O
)	O
b	O
p	O
(	O
µ	O
)	O
dµ	O
=	O
a	O
a−c	O
p	O
(	O
µ	O
)	O
dµ	O
=	O
a	O
p	O
(	O
µ	O
−	O
c	O
)	O
dµ	O
(	O
2.234	O
)	O
and	O
because	O
this	O
must	O
hold	O
for	O
all	O
choices	O
of	O
a	O
and	O
b	O
,	O
we	O
have	O
p	O
(	O
µ	O
−	O
c	O
)	O
=	O
p	O
(	O
µ	O
)	O
(	O
2.235	O
)	O
which	O
implies	O
that	O
p	O
(	O
µ	O
)	O
is	O
constant	O
.	O
an	O
example	O
of	O
a	O
location	B
parameter	I
would	O
be	O
the	O
mean	B
µ	O
of	O
a	O
gaussian	O
distribution	O
.	O
as	O
we	O
have	O
seen	O
,	O
the	O
conjugate	B
prior	I
distri-	O
bution	O
for	O
µ	O
in	O
this	O
case	O
is	O
a	O
gaussian	O
p	O
(	O
µ|µ0	O
,	O
σ2	O
0	O
)	O
,	O
and	O
we	O
obtain	O
a	O
0	O
→	O
∞	O
.	O
indeed	O
,	O
from	O
(	O
2.141	O
)	O
and	O
(	O
2.142	O
)	O
noninformative	B
prior	I
by	O
taking	O
the	O
limit	O
σ2	O
we	O
see	O
that	O
this	O
gives	O
a	O
posterior	O
distribution	O
over	O
µ	O
in	O
which	O
the	O
contributions	O
from	O
(	O
cid:17	O
)	O
the	O
prior	B
vanish	O
.	O
0	O
)	O
=	O
n	O
(	O
µ|µ0	O
,	O
σ2	O
(	O
cid:18	O
)	O
as	O
a	O
second	O
example	O
,	O
consider	O
a	O
density	B
of	O
the	O
form	O
p	O
(	O
x|σ	O
)	O
=	O
1	O
σ	O
f	O
x	O
σ	O
(	O
2.236	O
)	O
exercise	O
2.59	O
where	O
σ	O
>	O
0.	O
note	O
that	O
this	O
will	O
be	O
a	O
normalized	O
density	O
provided	O
f	O
(	O
x	O
)	O
is	O
correctly	O
normalized	O
.	O
the	O
parameter	O
σ	O
is	O
known	O
as	O
a	O
scale	B
parameter	I
,	O
and	O
the	O
density	B
exhibits	O
scale	B
invariance	I
because	O
if	O
we	O
scale	O
x	O
by	O
a	O
constant	O
to	O
give	O
(	O
cid:1	O
)	O
x	O
=	O
cx	O
,	O
then	O
where	O
we	O
have	O
deﬁned	O
(	O
cid:1	O
)	O
σ	O
=	O
cσ	O
.	O
this	O
transformation	O
corresponds	O
to	O
a	O
change	O
of	O
p	O
(	O
(	O
cid:1	O
)	O
x|	O
(	O
cid:1	O
)	O
σ	O
)	O
=	O
(	O
cid:15	O
)	O
(	O
cid:1	O
)	O
x	O
(	O
cid:1	O
)	O
σ	O
(	O
2.237	O
)	O
(	O
cid:16	O
)	O
1	O
(	O
cid:1	O
)	O
σ	O
f	O
scale	O
,	O
for	O
example	O
from	O
meters	O
to	O
kilometers	O
if	O
x	O
is	O
a	O
length	O
,	O
and	O
we	O
would	O
like	O
to	O
choose	O
a	O
prior	B
distribution	O
that	O
reﬂects	O
this	O
scale	B
invariance	I
.	O
if	O
we	O
consider	O
an	O
interval	O
a	O
(	O
cid:1	O
)	O
σ	O
(	O
cid:1	O
)	O
b	O
,	O
and	O
a	O
scaled	O
interval	O
a/c	O
(	O
cid:1	O
)	O
σ	O
(	O
cid:1	O
)	O
b/c	O
,	O
then	O
the	O
prior	B
should	O
assign	O
equal	O
probability	B
mass	O
to	O
these	O
two	O
intervals	O
.	O
thus	O
we	O
have	O
(	O
cid:6	O
)	O
b	O
(	O
cid:6	O
)	O
b/c	O
p	O
(	O
σ	O
)	O
dσ	O
=	O
p	O
(	O
σ	O
)	O
dσ	O
=	O
a	O
a/c	O
1	O
c	O
σ	O
1	O
c	O
dσ	O
(	O
2.238	O
)	O
(	O
cid:16	O
)	O
(	O
cid:6	O
)	O
b	O
(	O
cid:15	O
)	O
p	O
a	O
and	O
because	O
this	O
must	O
hold	O
for	O
choices	O
of	O
a	O
and	O
b	O
,	O
we	O
have	O
(	O
cid:15	O
)	O
1	O
c	O
(	O
cid:16	O
)	O
1	O
c	O
p	O
(	O
σ	O
)	O
=	O
p	O
σ	O
(	O
2.239	O
)	O
and	O
hence	O
p	O
(	O
σ	O
)	O
∝	O
1/σ	O
.	O
note	O
that	O
again	O
this	O
is	O
an	O
improper	B
prior	I
because	O
the	O
integral	O
of	O
the	O
distribution	O
over	O
0	O
(	O
cid:1	O
)	O
σ	O
(	O
cid:1	O
)	O
∞	O
is	O
divergent	O
.	O
it	O
is	O
sometimes	O
also	O
convenient	O
to	O
think	O
of	O
the	O
prior	B
distribution	O
for	O
a	O
scale	B
parameter	I
in	O
terms	O
of	O
the	O
density	B
of	O
the	O
log	O
of	O
the	O
parameter	O
.	O
using	O
the	O
transformation	O
rule	O
(	O
1.27	O
)	O
for	O
densities	O
we	O
see	O
that	O
p	O
(	O
ln	O
σ	O
)	O
=	O
const	O
.	O
thus	O
,	O
for	O
this	O
prior	B
there	O
is	O
the	O
same	O
probability	B
mass	O
in	O
the	O
range	O
1	O
(	O
cid:1	O
)	O
σ	O
(	O
cid:1	O
)	O
10	O
as	O
in	O
the	O
range	O
10	O
(	O
cid:1	O
)	O
σ	O
(	O
cid:1	O
)	O
100	O
and	O
in	O
100	O
(	O
cid:1	O
)	O
σ	O
(	O
cid:1	O
)	O
1000	O
.	O
120	O
2.	O
probability	B
distributions	O
(	O
cid:26	O
)	O
−	O
(	O
(	O
cid:4	O
)	O
x/σ	O
)	O
2	O
(	O
cid:27	O
)	O
an	O
example	O
of	O
a	O
scale	B
parameter	I
would	O
be	O
the	O
standard	B
deviation	I
σ	O
of	O
a	O
gaussian	O
distribution	O
,	O
after	O
we	O
have	O
taken	O
account	O
of	O
the	O
location	B
parameter	I
µ	O
,	O
because	O
where	O
(	O
cid:4	O
)	O
x	O
=	O
x	O
−	O
µ.	O
as	O
discussed	O
earlier	O
,	O
it	O
is	O
often	O
more	O
convenient	O
to	O
work	O
in	O
terms	O
n	O
(	O
x|µ	O
,	O
σ2	O
)	O
∝	O
σ	O
−1	O
exp	O
of	O
the	O
precision	O
λ	O
=	O
1/σ2	O
rather	O
than	O
σ	O
itself	O
.	O
using	O
the	O
transformation	O
rule	O
for	O
densities	O
,	O
we	O
see	O
that	O
a	O
distribution	O
p	O
(	O
σ	O
)	O
∝	O
1/σ	O
corresponds	O
to	O
a	O
distribution	O
over	O
λ	O
of	O
the	O
form	O
p	O
(	O
λ	O
)	O
∝	O
1/λ	O
.	O
we	O
have	O
seen	O
that	O
the	O
conjugate	B
prior	I
for	O
λ	O
was	O
the	O
gamma	B
distribution	I
gam	O
(	O
λ|a0	O
,	O
b0	O
)	O
given	O
by	O
(	O
2.146	O
)	O
.	O
the	O
noninformative	B
prior	I
is	O
obtained	O
as	O
the	O
special	O
case	O
a0	O
=	O
b0	O
=	O
0.	O
again	O
,	O
if	O
we	O
examine	O
the	O
results	O
(	O
2.150	O
)	O
and	O
(	O
2.151	O
)	O
for	O
the	O
posterior	O
distribution	O
of	O
λ	O
,	O
we	O
see	O
that	O
for	O
a0	O
=	O
b0	O
=	O
0	O
,	O
the	O
posterior	O
depends	O
only	O
on	O
terms	O
arising	O
from	O
the	O
data	O
and	O
not	O
from	O
the	O
prior	B
.	O
(	O
2.240	O
)	O
section	O
2.3	O
2.5.	O
nonparametric	B
methods	I
throughout	O
this	O
chapter	O
,	O
we	O
have	O
focussed	O
on	O
the	O
use	O
of	O
probability	B
distributions	O
having	O
speciﬁc	O
functional	B
forms	O
governed	O
by	O
a	O
small	O
number	O
of	O
parameters	O
whose	O
values	O
are	O
to	O
be	O
determined	O
from	O
a	O
data	O
set	O
.	O
this	O
is	O
called	O
the	O
parametric	O
approach	O
to	O
density	B
modelling	O
.	O
an	O
important	O
limitation	O
of	O
this	O
approach	O
is	O
that	O
the	O
chosen	O
density	B
might	O
be	O
a	O
poor	O
model	O
of	O
the	O
distribution	O
that	O
generates	O
the	O
data	O
,	O
which	O
can	O
result	O
in	O
poor	O
predictive	O
performance	O
.	O
for	O
instance	O
,	O
if	O
the	O
process	O
that	O
generates	O
the	O
data	O
is	O
multimodal	O
,	O
then	O
this	O
aspect	O
of	O
the	O
distribution	O
can	O
never	O
be	O
captured	O
by	O
a	O
gaussian	O
,	O
which	O
is	O
necessarily	O
unimodal	O
.	O
in	O
this	O
ﬁnal	O
section	O
,	O
we	O
consider	O
some	O
nonparametric	O
approaches	O
to	O
density	B
es-	O
timation	O
that	O
make	O
few	O
assumptions	O
about	O
the	O
form	O
of	O
the	O
distribution	O
.	O
here	O
we	O
shall	O
focus	O
mainly	O
on	O
simple	O
frequentist	B
methods	O
.	O
the	O
reader	O
should	O
be	O
aware	O
,	O
however	O
,	O
that	O
nonparametric	O
bayesian	O
methods	O
are	O
attracting	O
increasing	O
interest	O
(	O
walker	O
et	O
al.	O
,	O
1999	O
;	O
neal	O
,	O
2000	O
;	O
m¨uller	O
and	O
quintana	O
,	O
2004	O
;	O
teh	O
et	O
al.	O
,	O
2006	O
)	O
.	O
let	O
us	O
start	O
with	O
a	O
discussion	O
of	O
histogram	O
methods	O
for	O
density	O
estimation	O
,	O
which	O
we	O
have	O
already	O
encountered	O
in	O
the	O
context	O
of	O
marginal	B
and	O
conditional	B
distributions	O
in	O
figure	O
1.11	O
and	O
in	O
the	O
context	O
of	O
the	O
central	B
limit	I
theorem	I
in	O
figure	O
2.6.	O
here	O
we	O
explore	O
the	O
properties	O
of	O
histogram	O
density	O
models	O
in	O
more	O
detail	O
,	O
focussing	O
on	O
the	O
case	O
of	O
a	O
single	O
continuous	O
variable	O
x.	O
standard	O
histograms	O
simply	O
partition	O
x	O
into	O
distinct	O
bins	O
of	O
width	O
∆i	O
and	O
then	O
count	O
the	O
number	O
ni	O
of	O
observations	O
of	O
x	O
falling	O
in	O
bin	O
i.	O
in	O
order	O
to	O
turn	O
this	O
count	O
into	O
a	O
normalized	O
probability	O
density	B
,	O
we	O
simply	O
divide	O
by	O
the	O
total	O
number	O
n	O
of	O
observations	O
and	O
by	O
the	O
width	O
∆i	O
of	O
the	O
bins	O
to	O
obtain	O
probability	B
values	O
for	O
each	O
bin	O
given	O
by	O
pi	O
=	O
ni	O
n∆i	O
(	O
2.241	O
)	O
p	O
(	O
x	O
)	O
dx	O
=	O
1.	O
this	O
gives	O
a	O
model	O
for	O
the	O
density	B
for	O
which	O
it	O
is	O
easily	O
seen	O
that	O
p	O
(	O
x	O
)	O
that	O
is	O
constant	O
over	O
the	O
width	O
of	O
each	O
bin	O
,	O
and	O
often	O
the	O
bins	O
are	O
chosen	O
to	O
have	O
the	O
same	O
width	O
∆i	O
=	O
∆	O
.	O
(	O
cid:28	O
)	O
figure	O
2.24	O
an	O
illustration	O
of	O
the	O
histogram	O
approach	O
to	O
density	B
estimation	I
,	O
in	O
which	O
a	O
data	O
set	O
of	O
50	O
data	O
points	O
is	O
generated	O
from	O
the	O
distribution	O
shown	O
by	O
the	O
green	O
curve	O
.	O
histogram	O
density	O
estimates	O
,	O
based	O
on	O
(	O
2.241	O
)	O
,	O
with	O
a	O
common	O
bin	O
width	O
∆	O
are	O
shown	O
for	O
various	O
values	O
of	O
∆	O
.	O
2.5.	O
nonparametric	B
methods	I
121	O
∆	O
=	O
0.04	O
∆	O
=	O
0.08	O
∆	O
=	O
0.25	O
5	O
0	O
5	O
0	O
0	O
5	O
0	O
0	O
0	O
0.5	O
0.5	O
0.5	O
1	O
1	O
1	O
in	O
figure	O
2.24	O
,	O
we	O
show	O
an	O
example	O
of	O
histogram	B
density	I
estimation	I
.	O
here	O
the	O
data	O
is	O
drawn	O
from	O
the	O
distribution	O
,	O
corresponding	O
to	O
the	O
green	O
curve	O
,	O
which	O
is	O
formed	O
from	O
a	O
mixture	O
of	O
two	O
gaussians	O
.	O
also	O
shown	O
are	O
three	O
examples	O
of	O
his-	O
togram	O
density	B
estimates	O
corresponding	O
to	O
three	O
different	O
choices	O
for	O
the	O
bin	O
width	O
∆	O
.	O
we	O
see	O
that	O
when	O
∆	O
is	O
very	O
small	O
(	O
top	O
ﬁgure	O
)	O
,	O
the	O
resulting	O
density	B
model	O
is	O
very	O
spiky	O
,	O
with	O
a	O
lot	O
of	O
structure	O
that	O
is	O
not	O
present	O
in	O
the	O
underlying	O
distribution	O
that	O
generated	O
the	O
data	O
set	O
.	O
conversely	O
,	O
if	O
∆	O
is	O
too	O
large	O
(	O
bottom	O
ﬁgure	O
)	O
then	O
the	O
result	O
is	O
a	O
model	O
that	O
is	O
too	O
smooth	O
and	O
that	O
consequently	O
fails	O
to	O
capture	O
the	O
bimodal	O
prop-	O
erty	O
of	O
the	O
green	O
curve	O
.	O
the	O
best	O
results	O
are	O
obtained	O
for	O
some	O
intermediate	O
value	O
of	O
∆	O
(	O
middle	O
ﬁgure	O
)	O
.	O
in	O
principle	O
,	O
a	O
histogram	O
density	O
model	O
is	O
also	O
dependent	O
on	O
the	O
choice	O
of	O
edge	B
location	O
for	O
the	O
bins	O
,	O
though	O
this	O
is	O
typically	O
much	O
less	O
signiﬁcant	O
than	O
the	O
value	O
of	O
∆	O
.	O
note	O
that	O
the	O
histogram	O
method	O
has	O
the	O
property	O
(	O
unlike	O
the	O
methods	O
to	O
be	O
dis-	O
cussed	O
shortly	O
)	O
that	O
,	O
once	O
the	O
histogram	O
has	O
been	O
computed	O
,	O
the	O
data	O
set	O
itself	O
can	O
be	O
discarded	O
,	O
which	O
can	O
be	O
advantageous	O
if	O
the	O
data	O
set	O
is	O
large	O
.	O
also	O
,	O
the	O
histogram	O
approach	O
is	O
easily	O
applied	O
if	O
the	O
data	O
points	O
are	O
arriving	O
sequentially	O
.	O
in	O
practice	O
,	O
the	O
histogram	O
technique	O
can	O
be	O
useful	O
for	O
obtaining	O
a	O
quick	O
visual-	O
ization	O
of	O
data	O
in	O
one	O
or	O
two	O
dimensions	O
but	O
is	O
unsuited	O
to	O
most	O
density	B
estimation	I
applications	O
.	O
one	O
obvious	O
problem	O
is	O
that	O
the	O
estimated	O
density	B
has	O
discontinuities	O
that	O
are	O
due	O
to	O
the	O
bin	O
edges	O
rather	O
than	O
any	O
property	O
of	O
the	O
underlying	O
distribution	O
that	O
generated	O
the	O
data	O
.	O
another	O
major	O
limitation	O
of	O
the	O
histogram	O
approach	O
is	O
its	O
scaling	O
with	O
dimensionality	O
.	O
if	O
we	O
divide	O
each	O
variable	O
in	O
a	O
d-dimensional	O
space	O
into	O
m	O
bins	O
,	O
then	O
the	O
total	O
number	O
of	O
bins	O
will	O
be	O
m	O
d.	O
this	O
exponential	O
scaling	O
with	O
d	O
is	O
an	O
example	O
of	O
the	O
curse	B
of	I
dimensionality	I
.	O
in	O
a	O
space	O
of	O
high	O
dimensional-	O
ity	O
,	O
the	O
quantity	O
of	O
data	O
needed	O
to	O
provide	O
meaningful	O
estimates	O
of	O
local	B
probability	O
density	B
would	O
be	O
prohibitive	O
.	O
the	O
histogram	O
approach	O
to	O
density	B
estimation	I
does	O
,	O
however	O
,	O
teach	O
us	O
two	O
im-	O
portant	O
lessons	O
.	O
first	O
,	O
to	O
estimate	O
the	O
probability	B
density	O
at	O
a	O
particular	O
location	O
,	O
we	O
should	O
consider	O
the	O
data	O
points	O
that	O
lie	O
within	O
some	O
local	B
neighbourhood	O
of	O
that	O
point	O
.	O
note	O
that	O
the	O
concept	O
of	O
locality	O
requires	O
that	O
we	O
assume	O
some	O
form	O
of	O
dis-	O
tance	O
measure	O
,	O
and	O
here	O
we	O
have	O
been	O
assuming	O
euclidean	O
distance	O
.	O
for	O
histograms	O
,	O
section	O
1.4	O
122	O
2.	O
probability	B
distributions	O
this	O
neighbourhood	O
property	O
was	O
deﬁned	O
by	O
the	O
bins	O
,	O
and	O
there	O
is	O
a	O
natural	O
‘	O
smooth-	O
ing	O
’	O
parameter	O
describing	O
the	O
spatial	O
extent	O
of	O
the	O
local	B
region	O
,	O
in	O
this	O
case	O
the	O
bin	O
width	O
.	O
second	O
,	O
the	O
value	O
of	O
the	O
smoothing	B
parameter	I
should	O
be	O
neither	O
too	O
large	O
nor	O
too	O
small	O
in	O
order	O
to	O
obtain	O
good	O
results	O
.	O
this	O
is	O
reminiscent	O
of	O
the	O
choice	O
of	O
model	O
complexity	O
in	O
polynomial	B
curve	I
ﬁtting	I
discussed	O
in	O
chapter	O
1	O
where	O
the	O
degree	O
m	O
of	O
the	O
polynomial	O
,	O
or	O
alternatively	O
the	O
value	O
α	O
of	O
the	O
regularization	B
parameter	O
,	O
was	O
optimal	O
for	O
some	O
intermediate	O
value	O
,	O
neither	O
too	O
large	O
nor	O
too	O
small	O
.	O
armed	O
with	O
these	O
insights	O
,	O
we	O
turn	O
now	O
to	O
a	O
discussion	O
of	O
two	O
widely	O
used	O
nonparametric	O
tech-	O
niques	O
for	O
density	O
estimation	O
,	O
kernel	O
estimators	O
and	O
nearest	O
neighbours	O
,	O
which	O
have	O
better	O
scaling	O
with	O
dimensionality	O
than	O
the	O
simple	O
histogram	O
model	O
.	O
2.5.1	O
kernel	O
density	O
estimators	O
let	O
us	O
suppose	O
that	O
observations	O
are	O
being	O
drawn	O
from	O
some	O
unknown	O
probabil-	O
ity	O
density	B
p	O
(	O
x	O
)	O
in	O
some	O
d-dimensional	O
space	O
,	O
which	O
we	O
shall	O
take	O
to	O
be	O
euclidean	O
,	O
and	O
we	O
wish	O
to	O
estimate	O
the	O
value	O
of	O
p	O
(	O
x	O
)	O
.	O
from	O
our	O
earlier	O
discussion	O
of	O
locality	O
,	O
let	O
us	O
consider	O
some	O
small	O
region	O
r	O
containing	O
x.	O
the	O
probability	B
mass	O
associated	O
with	O
this	O
region	O
is	O
given	O
by	O
(	O
cid:6	O
)	O
p	O
=	O
r	O
p	O
(	O
x	O
)	O
dx	O
.	O
(	O
2.242	O
)	O
section	O
2.1	O
now	O
suppose	O
that	O
we	O
have	O
collected	O
a	O
data	O
set	O
comprising	O
n	O
observations	O
drawn	O
from	O
p	O
(	O
x	O
)	O
.	O
because	O
each	O
data	O
point	O
has	O
a	O
probability	B
p	O
of	O
falling	O
within	O
r	O
,	O
the	O
total	O
number	O
k	O
of	O
points	O
that	O
lie	O
inside	O
r	O
will	O
be	O
distributed	O
according	O
to	O
the	O
binomial	B
distribution	I
bin	O
(	O
k|n	O
,	O
p	O
)	O
=	O
n	O
!	O
k	O
!	O
(	O
n	O
−	O
k	O
)	O
!	O
p	O
k	O
(	O
1	O
−	O
p	O
)	O
1−k	O
.	O
(	O
2.243	O
)	O
using	O
(	O
2.11	O
)	O
,	O
we	O
see	O
that	O
the	O
mean	B
fraction	O
of	O
points	O
falling	O
inside	O
the	O
region	O
is	O
e	O
[	O
k/n	O
]	O
=	O
p	O
,	O
and	O
similarly	O
using	O
(	O
2.12	O
)	O
we	O
see	O
that	O
the	O
variance	B
around	O
this	O
mean	B
is	O
var	O
[	O
k/n	O
]	O
=	O
p	O
(	O
1	O
−	O
p	O
)	O
/n	O
.	O
for	O
large	O
n	O
,	O
this	O
distribution	O
will	O
be	O
sharply	O
peaked	O
around	O
the	O
mean	B
and	O
so	O
(	O
2.244	O
)	O
if	O
,	O
however	O
,	O
we	O
also	O
assume	O
that	O
the	O
region	O
r	O
is	O
sufﬁciently	O
small	O
that	O
the	O
probability	B
density	O
p	O
(	O
x	O
)	O
is	O
roughly	O
constant	O
over	O
the	O
region	O
,	O
then	O
we	O
have	O
k	O
(	O
cid:7	O
)	O
n	O
p.	O
(	O
2.245	O
)	O
where	O
v	O
is	O
the	O
volume	O
of	O
r.	O
combining	O
(	O
2.244	O
)	O
and	O
(	O
2.245	O
)	O
,	O
we	O
obtain	O
our	O
density	B
estimate	O
in	O
the	O
form	O
.	O
(	O
2.246	O
)	O
p	O
(	O
cid:7	O
)	O
p	O
(	O
x	O
)	O
v	O
p	O
(	O
x	O
)	O
=	O
k	O
n	O
v	O
note	O
that	O
the	O
validity	O
of	O
(	O
2.246	O
)	O
depends	O
on	O
two	O
contradictory	O
assumptions	O
,	O
namely	O
that	O
the	O
region	O
r	O
be	O
sufﬁciently	O
small	O
that	O
the	O
density	B
is	O
approximately	O
constant	O
over	O
the	O
region	O
and	O
yet	O
sufﬁciently	O
large	O
(	O
in	O
relation	O
to	O
the	O
value	O
of	O
that	O
density	B
)	O
that	O
the	O
number	O
k	O
of	O
points	O
falling	O
inside	O
the	O
region	O
is	O
sufﬁcient	O
for	O
the	O
binomial	B
distribution	I
to	O
be	O
sharply	O
peaked	O
.	O
2.5.	O
nonparametric	B
methods	I
123	O
we	O
can	O
exploit	O
the	O
result	O
(	O
2.246	O
)	O
in	O
two	O
different	O
ways	O
.	O
either	O
we	O
can	O
ﬁx	O
k	O
and	O
determine	O
the	O
value	O
of	O
v	O
from	O
the	O
data	O
,	O
which	O
gives	O
rise	O
to	O
the	O
k-nearest-neighbour	O
technique	O
discussed	O
shortly	O
,	O
or	O
we	O
can	O
ﬁx	O
v	O
and	O
determine	O
k	O
from	O
the	O
data	O
,	O
giv-	O
ing	O
rise	O
to	O
the	O
kernel	O
approach	O
.	O
it	O
can	O
be	O
shown	O
that	O
both	O
the	O
k-nearest-neighbour	O
density	B
estimator	O
and	O
the	O
kernel	B
density	I
estimator	I
converge	O
to	O
the	O
true	O
probability	B
density	O
in	O
the	O
limit	O
n	O
→	O
∞	O
provided	O
v	O
shrinks	O
suitably	O
with	O
n	O
,	O
and	O
k	O
grows	O
with	O
n	O
(	O
duda	O
and	O
hart	O
,	O
1973	O
)	O
.	O
we	O
begin	O
by	O
discussing	O
the	O
kernel	O
method	O
in	O
detail	O
,	O
and	O
to	O
start	O
with	O
we	O
take	O
the	O
region	O
r	O
to	O
be	O
a	O
small	O
hypercube	O
centred	O
on	O
the	O
point	O
x	O
at	O
which	O
we	O
wish	O
to	O
determine	O
the	O
probability	B
density	O
.	O
in	O
order	O
to	O
count	O
the	O
number	O
k	O
of	O
points	O
falling	O
within	O
this	O
region	O
,	O
it	O
is	O
convenient	O
to	O
deﬁne	O
the	O
following	O
function	O
k	O
(	O
u	O
)	O
=	O
|ui|	O
(	O
cid:1	O
)	O
1/2	O
,	O
1	O
,	O
0	O
,	O
otherwise	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
,	O
(	O
2.247	O
)	O
(	O
cid:12	O
)	O
which	O
represents	O
a	O
unit	O
cube	O
centred	O
on	O
the	O
origin	O
.	O
the	O
function	O
k	O
(	O
u	O
)	O
is	O
an	O
example	O
of	O
a	O
kernel	B
function	I
,	O
and	O
in	O
this	O
context	O
is	O
also	O
called	O
a	O
parzen	O
window	O
.	O
from	O
(	O
2.247	O
)	O
,	O
the	O
quantity	O
k	O
(	O
(	O
x−	O
xn	O
)	O
/h	O
)	O
will	O
be	O
one	O
if	O
the	O
data	O
point	O
xn	O
lies	O
inside	O
a	O
cube	O
of	O
side	O
h	O
centred	O
on	O
x	O
,	O
and	O
zero	O
otherwise	O
.	O
the	O
total	O
number	O
of	O
data	O
points	O
lying	O
inside	O
this	O
cube	O
will	O
therefore	O
be	O
(	O
cid:17	O
)	O
x	O
−	O
xn	O
(	O
cid:18	O
)	O
k	O
=	O
k	O
.	O
(	O
2.248	O
)	O
substituting	O
this	O
expression	O
into	O
(	O
2.246	O
)	O
then	O
gives	O
the	O
following	O
result	O
for	O
the	O
esti-	O
mated	O
density	B
at	O
x	O
n	O
(	O
cid:2	O
)	O
n=1	O
n	O
(	O
cid:2	O
)	O
n=1	O
h	O
(	O
cid:17	O
)	O
(	O
cid:18	O
)	O
p	O
(	O
x	O
)	O
=	O
1	O
n	O
1	O
hd	O
k	O
x	O
−	O
xn	O
h	O
(	O
2.249	O
)	O
where	O
we	O
have	O
used	O
v	O
=	O
hd	O
for	O
the	O
volume	O
of	O
a	O
hypercube	O
of	O
side	O
h	O
in	O
d	O
di-	O
mensions	O
.	O
using	O
the	O
symmetry	O
of	O
the	O
function	O
k	O
(	O
u	O
)	O
,	O
we	O
can	O
now	O
re-interpret	O
this	O
equation	O
,	O
not	O
as	O
a	O
single	O
cube	O
centred	O
on	O
x	O
but	O
as	O
the	O
sum	O
over	O
n	O
cubes	O
centred	O
on	O
the	O
n	O
data	O
points	O
xn	O
.	O
as	O
it	O
stands	O
,	O
the	O
kernel	B
density	I
estimator	I
(	O
2.249	O
)	O
will	O
suffer	O
from	O
one	O
of	O
the	O
same	O
problems	O
that	O
the	O
histogram	O
method	O
suffered	O
from	O
,	O
namely	O
the	O
presence	O
of	O
artiﬁcial	O
discontinuities	O
,	O
in	O
this	O
case	O
at	O
the	O
boundaries	O
of	O
the	O
cubes	O
.	O
we	O
can	O
obtain	O
a	O
smoother	O
density	O
model	O
if	O
we	O
choose	O
a	O
smoother	O
kernel	O
function	O
,	O
and	O
a	O
common	O
choice	O
is	O
the	O
gaussian	O
,	O
which	O
gives	O
rise	O
to	O
the	O
following	O
kernel	O
density	O
model	O
n	O
(	O
cid:2	O
)	O
n=1	O
p	O
(	O
x	O
)	O
=	O
1	O
n	O
1	O
(	O
2πh2	O
)	O
1/2	O
exp	O
(	O
cid:12	O
)	O
−	O
(	O
cid:5	O
)	O
x	O
−	O
xn	O
(	O
cid:5	O
)	O
2	O
(	O
cid:13	O
)	O
2h2	O
(	O
2.250	O
)	O
where	O
h	O
represents	O
the	O
standard	B
deviation	I
of	O
the	O
gaussian	O
components	O
.	O
thus	O
our	O
density	B
model	O
is	O
obtained	O
by	O
placing	O
a	O
gaussian	O
over	O
each	O
data	O
point	O
and	O
then	O
adding	O
up	O
the	O
contributions	O
over	O
the	O
whole	O
data	O
set	O
,	O
and	O
then	O
dividing	O
by	O
n	O
so	O
that	O
the	O
den-	O
sity	O
is	O
correctly	O
normalized	O
.	O
in	O
figure	O
2.25	O
,	O
we	O
apply	O
the	O
model	O
(	O
2.250	O
)	O
to	O
the	O
data	O
124	O
2.	O
probability	B
distributions	O
figure	O
2.25	O
illustration	O
of	O
the	O
kernel	O
density	O
model	O
(	O
2.250	O
)	O
applied	O
to	O
the	O
same	O
data	O
set	O
used	O
to	O
demonstrate	O
the	O
histogram	O
approach	O
in	O
figure	O
2.24.	O
we	O
see	O
that	O
h	O
acts	O
as	O
a	O
smoothing	B
parameter	I
and	O
that	O
if	O
it	O
is	O
set	O
too	O
small	O
(	O
top	O
panel	O
)	O
,	O
the	O
result	O
is	O
a	O
very	O
noisy	O
density	B
model	O
,	O
whereas	O
if	O
it	O
is	O
set	O
too	O
large	O
(	O
bottom	O
panel	O
)	O
,	O
then	O
the	O
bimodal	O
nature	O
of	O
the	O
underlying	O
distribution	O
from	O
which	O
the	O
data	O
is	O
generated	O
(	O
shown	O
by	O
the	O
green	O
curve	O
)	O
is	O
washed	O
out	O
.	O
the	O
best	O
den-	O
sity	O
model	O
is	O
obtained	O
for	O
some	O
intermedi-	O
ate	O
value	O
of	O
h	O
(	O
middle	O
panel	O
)	O
.	O
h	O
=	O
0.005	O
h	O
=	O
0.07	O
h	O
=	O
0.2	O
5	O
0	O
5	O
0	O
0	O
5	O
0	O
0	O
0	O
0.5	O
0.5	O
0.5	O
1	O
1	O
1	O
set	O
used	O
earlier	O
to	O
demonstrate	O
the	O
histogram	O
technique	O
.	O
we	O
see	O
that	O
,	O
as	O
expected	O
,	O
the	O
parameter	O
h	O
plays	O
the	O
role	O
of	O
a	O
smoothing	B
parameter	I
,	O
and	O
there	O
is	O
a	O
trade-off	O
between	O
sensitivity	O
to	O
noise	O
at	O
small	O
h	O
and	O
over-smoothing	O
at	O
large	O
h.	O
again	O
,	O
the	O
optimization	O
of	O
h	O
is	O
a	O
problem	O
in	O
model	O
complexity	O
,	O
analogous	O
to	O
the	O
choice	O
of	O
bin	O
width	O
in	O
histogram	B
density	I
estimation	I
,	O
or	O
the	O
degree	O
of	O
the	O
polynomial	O
used	O
in	O
curve	B
ﬁtting	I
.	O
we	O
can	O
choose	O
any	O
other	O
kernel	B
function	I
k	O
(	O
u	O
)	O
in	O
(	O
2.249	O
)	O
subject	O
to	O
the	O
condi-	O
tions	O
(	O
cid:6	O
)	O
k	O
(	O
u	O
)	O
(	O
cid:2	O
)	O
0	O
,	O
k	O
(	O
u	O
)	O
du	O
=	O
1	O
(	O
2.251	O
)	O
(	O
2.252	O
)	O
which	O
ensure	O
that	O
the	O
resulting	O
probability	B
distribution	O
is	O
nonnegative	O
everywhere	O
and	O
integrates	O
to	O
one	O
.	O
the	O
class	O
of	O
density	B
model	O
given	O
by	O
(	O
2.249	O
)	O
is	O
called	O
a	O
kernel	B
density	I
estimator	I
,	O
or	O
parzen	O
estimator	O
.	O
it	O
has	O
a	O
great	O
merit	O
that	O
there	O
is	O
no	O
compu-	O
tation	O
involved	O
in	O
the	O
‘	O
training	B
’	O
phase	O
because	O
this	O
simply	O
requires	O
storage	O
of	O
the	O
training	B
set	I
.	O
however	O
,	O
this	O
is	O
also	O
one	O
of	O
its	O
great	O
weaknesses	O
because	O
the	O
computa-	O
tional	O
cost	O
of	O
evaluating	O
the	O
density	B
grows	O
linearly	O
with	O
the	O
size	O
of	O
the	O
data	O
set	O
.	O
2.5.2	O
nearest-neighbour	B
methods	I
one	O
of	O
the	O
difﬁculties	O
with	O
the	O
kernel	O
approach	O
to	O
density	B
estimation	I
is	O
that	O
the	O
parameter	O
h	O
governing	O
the	O
kernel	O
width	O
is	O
ﬁxed	O
for	O
all	O
kernels	O
.	O
in	O
regions	O
of	O
high	O
data	O
density	O
,	O
a	O
large	O
value	O
of	O
h	O
may	O
lead	O
to	O
over-smoothing	O
and	O
a	O
washing	O
out	O
of	O
structure	O
that	O
might	O
otherwise	O
be	O
extracted	O
from	O
the	O
data	O
.	O
however	O
,	O
reducing	O
h	O
may	O
lead	O
to	O
noisy	O
estimates	O
elsewhere	O
in	O
data	O
space	O
where	O
the	O
density	B
is	O
smaller	O
.	O
thus	O
the	O
optimal	O
choice	O
for	O
h	O
may	O
be	O
dependent	O
on	O
location	O
within	O
the	O
data	O
space	O
.	O
this	O
issue	O
is	O
addressed	O
by	O
nearest-neighbour	B
methods	I
for	O
density	B
estimation	I
.	O
we	O
therefore	O
return	O
to	O
our	O
general	O
result	O
(	O
2.246	O
)	O
for	O
local	O
density	B
estimation	I
,	O
and	O
instead	O
of	O
ﬁxing	O
v	O
and	O
determining	O
the	O
value	O
of	O
k	O
from	O
the	O
data	O
,	O
we	O
consider	O
a	O
ﬁxed	O
value	O
of	O
k	O
and	O
use	O
the	O
data	O
to	O
ﬁnd	O
an	O
appropriate	O
value	O
for	O
v	O
.	O
to	O
do	O
this	O
,	O
we	O
consider	O
a	O
small	O
sphere	O
centred	O
on	O
the	O
point	O
x	O
at	O
which	O
we	O
wish	O
to	O
estimate	O
the	O
2.5.	O
nonparametric	B
methods	I
125	O
figure	O
2.26	O
illustration	O
of	O
k-nearest-neighbour	O
den-	O
sity	O
estimation	O
using	O
the	O
same	O
data	O
set	O
as	O
in	O
figures	O
2.25	O
and	O
2.24.	O
we	O
see	O
that	O
the	O
parameter	O
k	O
governs	O
the	O
degree	O
of	O
smoothing	O
,	O
so	O
that	O
a	O
small	O
value	O
of	O
k	O
leads	O
to	O
a	O
very	O
noisy	O
density	B
model	O
(	O
top	O
panel	O
)	O
,	O
whereas	O
a	O
large	O
value	O
(	O
bot-	O
tom	O
panel	O
)	O
smoothes	O
out	O
the	O
bimodal	O
na-	O
ture	O
of	O
the	O
true	O
distribution	O
(	O
shown	O
by	O
the	O
green	O
curve	O
)	O
from	O
which	O
the	O
data	O
set	O
was	O
generated	O
.	O
k	O
=	O
1	O
k	O
=	O
5	O
k	O
=	O
30	O
5	O
0	O
5	O
0	O
0	O
5	O
0	O
0	O
0	O
0.5	O
0.5	O
0.5	O
1	O
1	O
1	O
exercise	O
2.61	O
density	B
p	O
(	O
x	O
)	O
,	O
and	O
we	O
allow	O
the	O
radius	O
of	O
the	O
sphere	O
to	O
grow	O
until	O
it	O
contains	O
precisely	O
k	O
data	O
points	O
.	O
the	O
estimate	O
of	O
the	O
density	B
p	O
(	O
x	O
)	O
is	O
then	O
given	O
by	O
(	O
2.246	O
)	O
with	O
v	O
set	O
to	O
the	O
volume	O
of	O
the	O
resulting	O
sphere	O
.	O
this	O
technique	O
is	O
known	O
as	O
k	O
nearest	O
neighbours	O
and	O
is	O
illustrated	O
in	O
figure	O
2.26	O
,	O
for	O
various	O
choices	O
of	O
the	O
parameter	O
k	O
,	O
using	O
the	O
same	O
data	O
set	O
as	O
used	O
in	O
figure	O
2.24	O
and	O
figure	O
2.25.	O
we	O
see	O
that	O
the	O
value	O
of	O
k	O
now	O
governs	O
the	O
degree	O
of	O
smoothing	O
and	O
that	O
again	O
there	O
is	O
an	O
optimum	O
choice	O
for	O
k	O
that	O
is	O
neither	O
too	O
large	O
nor	O
too	O
small	O
.	O
note	O
that	O
the	O
model	O
produced	O
by	O
k	O
nearest	O
neighbours	O
is	O
not	O
a	O
true	O
density	B
model	O
because	O
the	O
integral	O
over	O
all	O
space	O
diverges	O
.	O
we	O
close	O
this	O
chapter	O
by	O
showing	O
how	O
the	O
k-nearest-neighbour	O
technique	O
for	O
density	O
estimation	O
can	O
be	O
extended	B
to	O
the	O
problem	O
of	O
classiﬁcation	B
.	O
to	O
do	O
this	O
,	O
we	O
apply	O
the	O
k-nearest-neighbour	O
density	B
estimation	I
technique	O
to	O
each	O
class	O
separately	O
and	O
then	O
make	O
use	O
of	O
bayes	O
’	O
theorem	O
.	O
let	O
us	O
suppose	O
that	O
we	O
have	O
a	O
data	O
set	O
com-	O
prising	O
nk	O
points	O
in	O
class	O
ck	O
with	O
n	O
points	O
in	O
total	O
,	O
so	O
that	O
k	O
nk	O
=	O
n.	O
if	O
we	O
wish	O
to	O
classify	O
a	O
new	O
point	O
x	O
,	O
we	O
draw	O
a	O
sphere	O
centred	O
on	O
x	O
containing	O
precisely	O
k	O
points	O
irrespective	O
of	O
their	O
class	O
.	O
suppose	O
this	O
sphere	O
has	O
volume	O
v	O
and	O
contains	O
kk	O
points	O
from	O
class	O
ck	O
.	O
then	O
(	O
2.246	O
)	O
provides	O
an	O
estimate	O
of	O
the	O
density	B
associated	O
with	O
each	O
class	O
(	O
cid:5	O
)	O
p	O
(	O
x|ck	O
)	O
=	O
kk	O
nkv	O
similarly	O
,	O
the	O
unconditional	O
density	B
is	O
given	O
by	O
p	O
(	O
x	O
)	O
=	O
k	O
n	O
v	O
while	O
the	O
class	O
priors	O
are	O
given	O
by	O
p	O
(	O
ck	O
)	O
=	O
nk	O
n	O
.	O
.	O
(	O
2.253	O
)	O
(	O
2.254	O
)	O
(	O
2.255	O
)	O
we	O
can	O
now	O
combine	O
(	O
2.253	O
)	O
,	O
(	O
2.254	O
)	O
,	O
and	O
(	O
2.255	O
)	O
using	O
bayes	O
’	O
theorem	O
to	O
obtain	O
the	O
posterior	B
probability	I
of	O
class	O
membership	O
p	O
(	O
ck|x	O
)	O
=	O
p	O
(	O
x|ck	O
)	O
p	O
(	O
ck	O
)	O
p	O
(	O
x	O
)	O
=	O
kk	O
k	O
.	O
(	O
2.256	O
)	O
126	O
2.	O
probability	B
distributions	O
x2	O
x2	O
figure	O
2.27	O
(	O
a	O
)	O
in	O
the	O
k-nearest-	O
neighbour	O
classiﬁer	O
,	O
a	O
new	O
point	O
,	O
shown	O
by	O
the	O
black	O
diamond	O
,	O
is	O
clas-	O
siﬁed	O
according	O
to	O
the	O
majority	O
class	O
membership	O
of	O
the	O
k	O
closest	O
train-	O
ing	O
data	O
points	O
,	O
in	O
this	O
case	O
k	O
=	O
3.	O
in	O
the	O
nearest-neighbour	O
(	O
k	O
=	O
1	O
)	O
approach	O
to	O
classiﬁcation	B
,	O
the	O
resulting	O
decision	B
boundary	I
is	O
composed	O
of	O
hyperplanes	O
that	O
form	O
perpendicular	O
bisectors	O
of	O
pairs	O
of	O
points	O
from	O
different	O
classes	O
.	O
(	O
b	O
)	O
x1	O
x1	O
(	O
a	O
)	O
(	O
b	O
)	O
if	O
we	O
wish	O
to	O
minimize	O
the	O
probability	B
of	O
misclassiﬁcation	O
,	O
this	O
is	O
done	O
by	O
assigning	O
the	O
test	O
point	O
x	O
to	O
the	O
class	O
having	O
the	O
largest	O
posterior	B
probability	I
,	O
corresponding	O
to	O
the	O
largest	O
value	O
of	O
kk/k	O
.	O
thus	O
to	O
classify	O
a	O
new	O
point	O
,	O
we	O
identify	O
the	O
k	O
nearest	O
points	O
from	O
the	O
training	B
data	O
set	O
and	O
then	O
assign	O
the	O
new	O
point	O
to	O
the	O
class	O
having	O
the	O
largest	O
number	O
of	O
representatives	O
amongst	O
this	O
set	O
.	O
ties	O
can	O
be	O
broken	O
at	O
random	O
.	O
the	O
particular	O
case	O
of	O
k	O
=	O
1	O
is	O
called	O
the	O
nearest-neighbour	O
rule	O
,	O
because	O
a	O
test	O
point	O
is	O
simply	O
assigned	O
to	O
the	O
same	O
class	O
as	O
the	O
nearest	O
point	O
from	O
the	O
training	B
set	I
.	O
these	O
concepts	O
are	O
illustrated	O
in	O
figure	O
2.27.	O
in	O
figure	O
2.28	O
,	O
we	O
show	O
the	O
results	O
of	O
applying	O
the	O
k-nearest-neighbour	O
algo-	O
rithm	O
to	O
the	O
oil	B
ﬂow	I
data	I
,	O
introduced	O
in	O
chapter	O
1	O
,	O
for	O
various	O
values	O
of	O
k.	O
as	O
expected	O
,	O
we	O
see	O
that	O
k	O
controls	O
the	O
degree	O
of	O
smoothing	O
,	O
so	O
that	O
small	O
k	O
produces	O
many	O
small	O
regions	O
of	O
each	O
class	O
,	O
whereas	O
large	O
k	O
leads	O
to	O
fewer	O
larger	O
regions	O
.	O
k	O
=	O
1	O
x7	O
2	O
1	O
k	O
=	O
3	O
x7	O
2	O
1	O
x7	O
2	O
1	O
k	O
=	O
3	O
1	O
0	O
0	O
1	O
x6	O
2	O
0	O
0	O
1	O
x6	O
2	O
0	O
0	O
1	O
x6	O
2	O
figure	O
2.28	O
plot	O
of	O
200	O
data	O
points	O
from	O
the	O
oil	O
data	O
set	O
showing	O
values	O
of	O
x6	O
plotted	O
against	O
x7	O
,	O
where	O
the	O
red	O
,	O
green	O
,	O
and	O
blue	O
points	O
correspond	O
to	O
the	O
‘	O
laminar	O
’	O
,	O
‘	O
annular	O
’	O
,	O
and	O
‘	O
homogeneous	B
’	O
classes	O
,	O
respectively	O
.	O
also	O
shown	O
are	O
the	O
classiﬁcations	O
of	O
the	O
input	O
space	O
given	O
by	O
the	O
k-nearest-neighbour	O
algorithm	O
for	O
various	O
values	O
of	O
k.	O
exercises	O
127	O
an	O
interesting	O
property	O
of	O
the	O
nearest-neighbour	O
(	O
k	O
=	O
1	O
)	O
classiﬁer	O
is	O
that	O
,	O
in	O
the	O
limit	O
n	O
→	O
∞	O
,	O
the	O
error	B
rate	O
is	O
never	O
more	O
than	O
twice	O
the	O
minimum	O
achievable	O
error	B
rate	O
of	O
an	O
optimal	O
classiﬁer	O
,	O
i.e.	O
,	O
one	O
that	O
uses	O
the	O
true	O
class	O
distributions	O
(	O
cover	O
and	O
hart	O
,	O
1967	O
)	O
.	O
as	O
discussed	O
so	O
far	O
,	O
both	O
the	O
k-nearest-neighbour	O
method	O
,	O
and	O
the	O
kernel	O
den-	O
sity	O
estimator	O
,	O
require	O
the	O
entire	O
training	B
data	O
set	O
to	O
be	O
stored	O
,	O
leading	O
to	O
expensive	O
computation	O
if	O
the	O
data	O
set	O
is	O
large	O
.	O
this	O
effect	O
can	O
be	O
offset	O
,	O
at	O
the	O
expense	O
of	O
some	O
additional	O
one-off	O
computation	O
,	O
by	O
constructing	O
tree-based	O
search	O
structures	O
to	O
allow	O
(	O
approximate	O
)	O
near	O
neighbours	O
to	O
be	O
found	O
efﬁciently	O
without	O
doing	O
an	O
exhaustive	O
search	O
of	O
the	O
data	O
set	O
.	O
nevertheless	O
,	O
these	O
nonparametric	B
methods	I
are	O
still	O
severely	O
limited	O
.	O
on	O
the	O
other	O
hand	O
,	O
we	O
have	O
seen	O
that	O
simple	O
parametric	O
models	O
are	O
very	O
restricted	O
in	O
terms	O
of	O
the	O
forms	O
of	O
distribution	O
that	O
they	O
can	O
represent	O
.	O
we	O
therefore	O
need	O
to	O
ﬁnd	O
density	B
models	O
that	O
are	O
very	O
ﬂexible	O
and	O
yet	O
for	O
which	O
the	O
complexity	O
of	O
the	O
models	O
can	O
be	O
controlled	O
independently	O
of	O
the	O
size	O
of	O
the	O
training	B
set	I
,	O
and	O
we	O
shall	O
see	O
in	O
subsequent	O
chapters	O
how	O
to	O
achieve	O
this	O
.	O
exercises	O
erties	O
2.1	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
verify	O
that	O
the	O
bernoulli	O
distribution	O
(	O
2.2	O
)	O
satisﬁes	O
the	O
following	O
prop-	O
1	O
(	O
cid:2	O
)	O
p	O
(	O
x|µ	O
)	O
=	O
1	O
x=0	O
e	O
[	O
x	O
]	O
=	O
µ	O
var	O
[	O
x	O
]	O
=	O
µ	O
(	O
1	O
−	O
µ	O
)	O
.	O
(	O
2.257	O
)	O
(	O
2.258	O
)	O
(	O
2.259	O
)	O
show	O
that	O
the	O
entropy	B
h	O
[	O
x	O
]	O
of	O
a	O
bernoulli	O
distributed	O
random	O
binary	O
variable	O
x	O
is	O
given	O
by	O
h	O
[	O
x	O
]	O
=	O
−µ	O
ln	O
µ	O
−	O
(	O
1	O
−	O
µ	O
)	O
ln	O
(	O
1	O
−	O
µ	O
)	O
.	O
(	O
2.260	O
)	O
2.2	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
the	O
form	O
of	O
the	O
bernoulli	O
distribution	O
given	O
by	O
(	O
2.2	O
)	O
is	O
not	O
symmetric	O
be-	O
tween	O
the	O
two	O
values	O
of	O
x.	O
in	O
some	O
situations	O
,	O
it	O
will	O
be	O
more	O
convenient	O
to	O
use	O
an	O
equivalent	O
formulation	O
for	O
which	O
x	O
∈	O
{	O
−1	O
,	O
1	O
}	O
,	O
in	O
which	O
case	O
the	O
distribution	O
can	O
be	O
written	O
(	O
cid:16	O
)	O
(	O
1−x	O
)	O
/2	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
(	O
1+x	O
)	O
/2	O
(	O
cid:15	O
)	O
(	O
2.261	O
)	O
where	O
µ	O
∈	O
[	O
−1	O
,	O
1	O
]	O
.	O
show	O
that	O
the	O
distribution	O
(	O
2.261	O
)	O
is	O
normalized	O
,	O
and	O
evaluate	O
its	O
mean	B
,	O
variance	B
,	O
and	O
entropy	B
.	O
2	O
p	O
(	O
x|µ	O
)	O
=	O
1	O
−	O
µ	O
2	O
1	O
+	O
µ	O
2.3	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
in	O
this	O
exercise	O
,	O
we	O
prove	O
that	O
the	O
binomial	B
distribution	I
(	O
2.9	O
)	O
is	O
nor-	O
malized	O
.	O
first	O
use	O
the	O
deﬁnition	O
(	O
2.10	O
)	O
of	O
the	O
number	O
of	O
combinations	O
of	O
m	O
identical	O
objects	O
chosen	O
from	O
a	O
total	O
of	O
n	O
to	O
show	O
that	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
n	O
m	O
+	O
n	O
m	O
−	O
1	O
=	O
n	O
+	O
1	O
m	O
.	O
(	O
2.262	O
)	O
128	O
2.	O
probability	B
distributions	O
use	O
this	O
result	O
to	O
prove	O
by	O
induction	O
the	O
following	O
result	O
(	O
cid:15	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:16	O
)	O
n	O
m	O
m=0	O
(	O
1	O
+	O
x	O
)	O
n	O
=	O
xm	O
(	O
2.263	O
)	O
which	O
is	O
known	O
as	O
the	O
binomial	O
theorem	O
,	O
and	O
which	O
is	O
valid	O
for	O
all	O
real	O
values	O
of	O
x.	O
finally	O
,	O
show	O
that	O
the	O
binomial	B
distribution	I
is	O
normalized	O
,	O
so	O
that	O
µm	O
(	O
1	O
−	O
µ	O
)	O
n−m	O
=	O
1	O
(	O
2.264	O
)	O
(	O
cid:15	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:16	O
)	O
n	O
m	O
m=0	O
(	O
cid:6	O
)	O
1	O
(	O
cid:6	O
)	O
∞	O
0	O
which	O
can	O
be	O
done	O
by	O
ﬁrst	O
pulling	O
out	O
a	O
factor	O
(	O
1	O
−	O
µ	O
)	O
n	O
out	O
of	O
the	O
summation	O
and	O
then	O
making	O
use	O
of	O
the	O
binomial	O
theorem	O
.	O
2.4	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
the	O
mean	B
of	O
the	O
binomial	B
distribution	I
is	O
given	O
by	O
(	O
2.11	O
)	O
.	O
to	O
do	O
this	O
,	O
differentiate	O
both	O
sides	O
of	O
the	O
normalization	O
condition	O
(	O
2.264	O
)	O
with	O
respect	O
to	O
µ	O
and	O
then	O
rearrange	O
to	O
obtain	O
an	O
expression	O
for	O
the	O
mean	B
of	O
n.	O
similarly	O
,	O
by	O
differentiating	O
(	O
2.264	O
)	O
twice	O
with	O
respect	O
to	O
µ	O
and	O
making	O
use	O
of	O
the	O
result	O
(	O
2.11	O
)	O
for	O
the	O
mean	B
of	O
the	O
binomial	B
distribution	I
prove	O
the	O
result	O
(	O
2.12	O
)	O
for	O
the	O
variance	B
of	O
the	O
binomial	O
.	O
2.5	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
in	O
this	O
exercise	O
,	O
we	O
prove	O
that	O
the	O
beta	B
distribution	I
,	O
given	O
by	O
(	O
2.13	O
)	O
,	O
is	O
correctly	O
normalized	O
,	O
so	O
that	O
(	O
2.14	O
)	O
holds	O
.	O
this	O
is	O
equivalent	O
to	O
showing	O
that	O
µa−1	O
(	O
1	O
−	O
µ	O
)	O
b−1	O
dµ	O
=	O
γ	O
(	O
a	O
)	O
γ	O
(	O
b	O
)	O
γ	O
(	O
a	O
+	O
b	O
)	O
.	O
(	O
2.265	O
)	O
(	O
cid:6	O
)	O
∞	O
from	O
the	O
deﬁnition	O
(	O
1.141	O
)	O
of	O
the	O
gamma	B
function	I
,	O
we	O
have	O
γ	O
(	O
a	O
)	O
γ	O
(	O
b	O
)	O
=	O
exp	O
(	O
−x	O
)	O
xa−1	O
dx	O
exp	O
(	O
−y	O
)	O
yb−1	O
dy	O
.	O
(	O
2.266	O
)	O
0	O
0	O
use	O
this	O
expression	O
to	O
prove	O
(	O
2.265	O
)	O
as	O
follows	O
.	O
first	O
bring	O
the	O
integral	O
over	O
y	O
inside	O
the	O
integrand	O
of	O
the	O
integral	O
over	O
x	O
,	O
next	O
make	O
the	O
change	O
of	O
variable	O
t	O
=	O
y	O
+	O
x	O
where	O
x	O
is	O
ﬁxed	O
,	O
then	O
interchange	O
the	O
order	O
of	O
the	O
x	O
and	O
t	O
integrations	O
,	O
and	O
ﬁnally	O
make	O
the	O
change	O
of	O
variable	O
x	O
=	O
tµ	O
where	O
t	O
is	O
ﬁxed	O
.	O
2.6	O
(	O
(	O
cid:12	O
)	O
)	O
make	O
use	O
of	O
the	O
result	O
(	O
2.265	O
)	O
to	O
show	O
that	O
the	O
mean	B
,	O
variance	B
,	O
and	O
mode	O
of	O
the	O
beta	B
distribution	I
(	O
2.13	O
)	O
are	O
given	O
respectively	O
by	O
e	O
[	O
µ	O
]	O
=	O
var	O
[	O
µ	O
]	O
=	O
mode	O
[	O
µ	O
]	O
=	O
a	O
a	O
+	O
b	O
ab	O
(	O
a	O
+	O
b	O
)	O
2	O
(	O
a	O
+	O
b	O
+	O
1	O
)	O
a	O
−	O
1	O
a	O
+	O
b	O
−	O
2	O
.	O
(	O
2.267	O
)	O
(	O
2.268	O
)	O
(	O
2.269	O
)	O
exercises	O
129	O
2.7	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
binomial	O
random	O
variable	O
x	O
given	O
by	O
(	O
2.9	O
)	O
,	O
with	O
prior	B
distribution	O
for	O
µ	O
given	O
by	O
the	O
beta	B
distribution	I
(	O
2.13	O
)	O
,	O
and	O
suppose	O
we	O
have	O
observed	O
m	O
occur-	O
rences	O
of	O
x	O
=	O
1	O
and	O
l	O
occurrences	O
of	O
x	O
=	O
0.	O
show	O
that	O
the	O
posterior	O
mean	O
value	O
of	O
x	O
lies	O
between	O
the	O
prior	B
mean	O
and	O
the	O
maximum	B
likelihood	I
estimate	O
for	O
µ.	O
to	O
do	O
this	O
,	O
show	O
that	O
the	O
posterior	O
mean	O
can	O
be	O
written	O
as	O
λ	O
times	O
the	O
prior	B
mean	O
plus	O
(	O
1	O
−	O
λ	O
)	O
times	O
the	O
maximum	B
likelihood	I
estimate	O
,	O
where	O
0	O
(	O
cid:1	O
)	O
λ	O
(	O
cid:1	O
)	O
1.	O
this	O
illustrates	O
the	O
con-	O
cept	O
of	O
the	O
posterior	O
distribution	O
being	O
a	O
compromise	O
between	O
the	O
prior	B
distribution	O
and	O
the	O
maximum	B
likelihood	I
solution	O
.	O
2.8	O
(	O
(	O
cid:12	O
)	O
)	O
consider	O
two	O
variables	O
x	O
and	O
y	O
with	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
y	O
)	O
.	O
prove	O
the	O
follow-	O
ing	O
two	O
results	O
e	O
[	O
x	O
]	O
=	O
ey	O
[	O
ex	O
[	O
x|y	O
]	O
]	O
var	O
[	O
x	O
]	O
=	O
ey	O
[	O
varx	O
[	O
x|y	O
]	O
]	O
+	O
vary	O
[	O
ex	O
[	O
x|y	O
]	O
]	O
.	O
(	O
2.270	O
)	O
(	O
2.271	O
)	O
here	O
ex	O
[	O
x|y	O
]	O
denotes	O
the	O
expectation	B
of	O
x	O
under	O
the	O
conditional	B
distribution	O
p	O
(	O
x|y	O
)	O
,	O
with	O
a	O
similar	O
notation	O
for	O
the	O
conditional	B
variance	O
.	O
2.9	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
.	O
in	O
this	O
exercise	O
,	O
we	O
prove	O
the	O
normalization	O
of	O
the	O
dirichlet	O
dis-	O
tribution	O
(	O
2.38	O
)	O
using	O
induction	O
.	O
we	O
have	O
already	O
shown	O
in	O
exercise	O
2.5	O
that	O
the	O
beta	B
distribution	I
,	O
which	O
is	O
a	O
special	O
case	O
of	O
the	O
dirichlet	O
for	O
m	O
=	O
2	O
,	O
is	O
normalized	O
.	O
we	O
now	O
assume	O
that	O
the	O
dirichlet	O
distribution	O
is	O
normalized	O
for	O
m	O
−	O
1	O
variables	O
and	O
prove	O
that	O
it	O
is	O
normalized	O
for	O
m	O
variables	O
.	O
to	O
do	O
this	O
,	O
consider	O
the	O
dirichlet	O
k=1	O
µk	O
=	O
1	O
by	O
distribution	O
over	O
m	O
variables	O
,	O
and	O
take	O
account	O
of	O
the	O
constraint	O
eliminating	O
µm	O
,	O
so	O
that	O
the	O
dirichlet	O
is	O
written	O
pm	O
(	O
µ1	O
,	O
.	O
.	O
.	O
,	O
µm−1	O
)	O
=	O
cm	O
µj	O
(	O
2.272	O
)	O
k=1	O
j=1	O
and	O
our	O
goal	O
is	O
to	O
ﬁnd	O
an	O
expression	O
for	O
cm	O
.	O
to	O
do	O
this	O
,	O
integrate	O
over	O
µm−1	O
,	O
taking	O
care	O
over	O
the	O
limits	O
of	O
integration	O
,	O
and	O
then	O
make	O
a	O
change	O
of	O
variable	O
so	O
that	O
this	O
integral	O
has	O
limits	O
0	O
and	O
1.	O
by	O
assuming	O
the	O
correct	O
result	O
for	O
cm−1	O
and	O
making	O
use	O
of	O
(	O
2.265	O
)	O
,	O
derive	O
the	O
expression	O
for	O
cm	O
.	O
2.10	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
using	O
the	O
property	O
γ	O
(	O
x	O
+	O
1	O
)	O
=	O
xγ	O
(	O
x	O
)	O
of	O
the	O
gamma	B
function	I
,	O
derive	O
the	O
following	O
results	O
for	O
the	O
mean	B
,	O
variance	B
,	O
and	O
covariance	B
of	O
the	O
dirichlet	O
distribution	O
given	O
by	O
(	O
2.38	O
)	O
e	O
[	O
µj	O
]	O
=	O
αj	O
α0	O
var	O
[	O
µj	O
]	O
=	O
αj	O
(	O
α0	O
−	O
αj	O
)	O
0	O
(	O
α0	O
+	O
1	O
)	O
α2	O
cov	O
[	O
µjµl	O
]	O
=	O
−	O
αjαl	O
0	O
(	O
α0	O
+	O
1	O
)	O
,	O
α2	O
j	O
(	O
cid:9	O
)	O
=	O
l	O
(	O
2.273	O
)	O
(	O
2.274	O
)	O
(	O
2.275	O
)	O
where	O
α0	O
is	O
deﬁned	O
by	O
(	O
2.39	O
)	O
.	O
(	O
cid:5	O
)	O
m	O
(	O
cid:23	O
)	O
αm−1	O
m−1	O
(	O
cid:14	O
)	O
µαk−1	O
k	O
(	O
cid:22	O
)	O
1	O
−	O
m−1	O
(	O
cid:2	O
)	O
130	O
2.	O
probability	B
distributions	O
2.11	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
by	O
expressing	O
the	O
expectation	B
of	O
ln	O
µj	O
under	O
the	O
dirichlet	O
distribution	O
(	O
2.38	O
)	O
as	O
a	O
derivative	B
with	O
respect	O
to	O
αj	O
,	O
show	O
that	O
e	O
[	O
ln	O
µj	O
]	O
=	O
ψ	O
(	O
αj	O
)	O
−	O
ψ	O
(	O
α0	O
)	O
where	O
α0	O
is	O
given	O
by	O
(	O
2.39	O
)	O
and	O
ψ	O
(	O
a	O
)	O
≡	O
d	O
da	O
ln	O
γ	O
(	O
a	O
)	O
is	O
the	O
digamma	B
function	I
.	O
2.12	O
(	O
(	O
cid:12	O
)	O
)	O
the	O
uniform	B
distribution	I
for	O
a	O
continuous	O
variable	O
x	O
is	O
deﬁned	O
by	O
u	O
(	O
x|a	O
,	O
b	O
)	O
=	O
1	O
b	O
−	O
a	O
,	O
a	O
(	O
cid:1	O
)	O
x	O
(	O
cid:1	O
)	O
b	O
.	O
(	O
2.276	O
)	O
(	O
2.277	O
)	O
(	O
2.278	O
)	O
verify	O
that	O
this	O
distribution	O
is	O
normalized	O
,	O
and	O
ﬁnd	O
expressions	O
for	O
its	O
mean	B
and	O
variance	B
.	O
2.13	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
evaluate	O
the	O
kullback-leibler	O
divergence	O
(	O
1.113	O
)	O
between	O
two	O
gaussians	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
x|µ	O
,	O
σ	O
)	O
and	O
q	O
(	O
x	O
)	O
=	O
n	O
(	O
x|m	O
,	O
l	O
)	O
.	O
2.14	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
this	O
exercise	O
demonstrates	O
that	O
the	O
multivariate	O
distribution	O
with	O
max-	O
imum	O
entropy	B
,	O
for	O
a	O
given	O
covariance	B
,	O
is	O
a	O
gaussian	O
.	O
the	O
entropy	B
of	O
a	O
distribution	O
p	O
(	O
x	O
)	O
is	O
given	O
by	O
(	O
cid:6	O
)	O
h	O
[	O
x	O
]	O
=	O
−	O
p	O
(	O
x	O
)	O
ln	O
p	O
(	O
x	O
)	O
dx	O
.	O
(	O
2.279	O
)	O
we	O
wish	O
to	O
maximize	O
h	O
[	O
x	O
]	O
over	O
all	O
distributions	O
p	O
(	O
x	O
)	O
subject	O
to	O
the	O
constraints	O
that	O
p	O
(	O
x	O
)	O
be	O
normalized	O
and	O
that	O
it	O
have	O
a	O
speciﬁc	O
mean	B
and	O
covariance	B
,	O
so	O
that	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
p	O
(	O
x	O
)	O
dx	O
=	O
1	O
p	O
(	O
x	O
)	O
x	O
dx	O
=	O
µ	O
p	O
(	O
x	O
)	O
(	O
x	O
−	O
µ	O
)	O
(	O
x	O
−	O
µ	O
)	O
t	O
dx	O
=	O
σ	O
.	O
(	O
2.280	O
)	O
(	O
2.281	O
)	O
(	O
2.282	O
)	O
by	O
performing	O
a	O
variational	B
maximization	O
of	O
(	O
2.279	O
)	O
and	O
using	O
lagrange	O
multipliers	O
to	O
enforce	O
the	O
constraints	O
(	O
2.280	O
)	O
,	O
(	O
2.281	O
)	O
,	O
and	O
(	O
2.282	O
)	O
,	O
show	O
that	O
the	O
maximum	B
likelihood	I
distribution	O
is	O
given	O
by	O
the	O
gaussian	O
(	O
2.43	O
)	O
.	O
2.15	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
the	O
entropy	B
of	O
the	O
multivariate	O
gaussian	O
n	O
(	O
x|µ	O
,	O
σ	O
)	O
is	O
given	O
by	O
h	O
[	O
x	O
]	O
=	O
1	O
2	O
ln|σ|	O
+	O
d	O
2	O
(	O
1	O
+	O
ln	O
(	O
2π	O
)	O
)	O
(	O
2.283	O
)	O
where	O
d	O
is	O
the	O
dimensionality	O
of	O
x.	O
exercises	O
131	O
2.16	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
two	O
random	O
variables	O
x1	O
and	O
x2	O
having	O
gaussian	O
distri-	O
butions	O
with	O
means	O
µ1	O
,	O
µ2	O
and	O
precisions	O
τ1	O
,	O
τ2	O
respectively	O
.	O
derive	O
an	O
expression	O
for	O
the	O
differential	B
entropy	I
of	O
the	O
variable	O
x	O
=	O
x1	O
+	O
x2	O
.	O
to	O
do	O
this	O
,	O
ﬁrst	O
ﬁnd	O
the	O
distribution	O
of	O
x	O
by	O
using	O
the	O
relation	O
p	O
(	O
x|x2	O
)	O
p	O
(	O
x2	O
)	O
dx2	O
(	O
2.284	O
)	O
(	O
cid:6	O
)	O
∞	O
p	O
(	O
x	O
)	O
=	O
−∞	O
and	O
completing	B
the	I
square	I
in	O
the	O
exponent	O
.	O
then	O
observe	O
that	O
this	O
represents	O
the	O
convolution	O
of	O
two	O
gaussian	O
distributions	O
,	O
which	O
itself	O
will	O
be	O
gaussian	O
,	O
and	O
ﬁnally	O
make	O
use	O
of	O
the	O
result	O
(	O
1.110	O
)	O
for	O
the	O
entropy	B
of	O
the	O
univariate	O
gaussian	O
.	O
2.17	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
the	O
multivariate	O
gaussian	O
distribution	O
given	O
by	O
(	O
2.43	O
)	O
.	O
by	O
−1	O
as	O
the	O
sum	O
of	O
a	O
sym-	O
writing	O
the	O
precision	B
matrix	I
(	O
inverse	B
covariance	O
matrix	O
)	O
σ	O
metric	O
and	O
an	O
anti-symmetric	O
matrix	O
,	O
show	O
that	O
the	O
anti-symmetric	O
term	O
does	O
not	O
appear	O
in	O
the	O
exponent	O
of	O
the	O
gaussian	O
,	O
and	O
hence	O
that	O
the	O
precision	B
matrix	I
may	O
be	O
taken	O
to	O
be	O
symmetric	O
without	O
loss	O
of	O
generality	O
.	O
because	O
the	O
inverse	B
of	O
a	O
symmetric	O
matrix	O
is	O
also	O
symmetric	O
(	O
see	O
exercise	O
2.22	O
)	O
,	O
it	O
follows	O
that	O
the	O
covariance	B
matrix	I
may	O
also	O
be	O
chosen	O
to	O
be	O
symmetric	O
without	O
loss	O
of	O
generality	O
.	O
2.18	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
real	O
,	O
symmetric	O
matrix	O
σ	O
whose	O
eigenvalue	O
equation	O
is	O
given	O
by	O
(	O
2.45	O
)	O
.	O
by	O
taking	O
the	O
complex	O
conjugate	B
of	O
this	O
equation	O
and	O
subtracting	O
the	O
original	O
equation	O
,	O
and	O
then	O
forming	O
the	O
inner	O
product	O
with	O
eigenvector	O
ui	O
,	O
show	O
that	O
the	O
eigenvalues	O
λi	O
are	O
real	O
.	O
similarly	O
,	O
use	O
the	O
symmetry	O
property	O
of	O
σ	O
to	O
show	O
that	O
two	O
eigenvectors	O
ui	O
and	O
uj	O
will	O
be	O
orthogonal	O
provided	O
λj	O
(	O
cid:9	O
)	O
=	O
λi	O
.	O
finally	O
,	O
show	O
that	O
without	O
loss	O
of	O
generality	O
,	O
the	O
set	O
of	O
eigenvectors	O
can	O
be	O
chosen	O
to	O
be	O
orthonormal	O
,	O
so	O
that	O
they	O
satisfy	O
(	O
2.46	O
)	O
,	O
even	O
if	O
some	O
of	O
the	O
eigenvalues	O
are	O
zero	O
.	O
2.19	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
a	O
real	O
,	O
symmetric	O
matrix	O
σ	O
having	O
the	O
eigenvector	O
equation	O
(	O
2.45	O
)	O
can	O
be	O
expressed	O
as	O
an	O
expansion	O
in	O
the	O
eigenvectors	O
,	O
with	O
coefﬁcients	O
given	O
by	O
the	O
−1	O
has	O
a	O
eigenvalues	O
,	O
of	O
the	O
form	O
(	O
2.48	O
)	O
.	O
similarly	O
,	O
show	O
that	O
the	O
inverse	B
matrix	O
σ	O
representation	O
of	O
the	O
form	O
(	O
2.49	O
)	O
.	O
2.20	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
a	O
positive	B
deﬁnite	I
matrix	I
σ	O
can	O
be	O
deﬁned	O
as	O
one	O
for	O
which	O
the	O
quadratic	O
form	O
(	O
2.285	O
)	O
is	O
positive	O
for	O
any	O
real	O
value	O
of	O
the	O
vector	O
a.	O
show	O
that	O
a	O
necessary	O
and	O
sufﬁcient	O
condition	O
for	O
σ	O
to	O
be	O
positive	B
deﬁnite	I
is	O
that	O
all	O
of	O
the	O
eigenvalues	O
λi	O
of	O
σ	O
,	O
deﬁned	O
by	O
(	O
2.45	O
)	O
,	O
are	O
positive	O
.	O
atσa	O
2.21	O
(	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
a	O
real	O
,	O
symmetric	O
matrix	O
of	O
size	O
d×	O
d	O
has	O
d	O
(	O
d	O
+	O
1	O
)	O
/2	O
independent	B
parameters	O
.	O
2.22	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
the	O
inverse	B
of	O
a	O
symmetric	O
matrix	O
is	O
itself	O
symmetric	O
.	O
2.23	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
by	O
diagonalizing	O
the	O
coordinate	O
system	O
using	O
the	O
eigenvector	O
expansion	O
(	O
2.45	O
)	O
,	O
show	O
that	O
the	O
volume	O
contained	O
within	O
the	O
hyperellipsoid	O
corresponding	O
to	O
a	O
constant	O
132	O
2.	O
probability	B
distributions	O
mahalanobis	O
distance	O
∆	O
is	O
given	O
by	O
vd|σ|1/2∆d	O
(	O
2.286	O
)	O
where	O
vd	O
is	O
the	O
volume	O
of	O
the	O
unit	O
sphere	O
in	O
d	O
dimensions	O
,	O
and	O
the	O
mahalanobis	O
distance	O
is	O
deﬁned	O
by	O
(	O
2.44	O
)	O
.	O
2.24	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
prove	O
the	O
identity	O
(	O
2.76	O
)	O
by	O
multiplying	O
both	O
sides	O
by	O
the	O
matrix	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
a	O
b	O
c	O
d	O
(	O
2.287	O
)	O
and	O
making	O
use	O
of	O
the	O
deﬁnition	O
(	O
2.77	O
)	O
.	O
2.25	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
in	O
sections	O
2.3.1	O
and	O
2.3.2	O
,	O
we	O
considered	O
the	O
conditional	B
and	O
marginal	B
distri-	O
butions	O
for	O
a	O
multivariate	O
gaussian	O
.	O
more	O
generally	O
,	O
we	O
can	O
consider	O
a	O
partitioning	O
of	O
the	O
components	O
of	O
x	O
into	O
three	O
groups	O
xa	O
,	O
xb	O
,	O
and	O
xc	O
,	O
with	O
a	O
corresponding	O
par-	O
titioning	O
of	O
the	O
mean	B
vector	O
µ	O
and	O
of	O
the	O
covariance	B
matrix	I
σ	O
in	O
the	O
form	O
(	O
cid:22	O
)	O
(	O
cid:23	O
)	O
µa	O
µb	O
µc	O
(	O
cid:23	O
)	O
(	O
cid:22	O
)	O
σaa	O
σab	O
σac	O
σba	O
σbb	O
σbc	O
σca	O
σcb	O
σcc	O
µ	O
=	O
,	O
σ	O
=	O
.	O
(	O
2.288	O
)	O
by	O
making	O
use	O
of	O
the	O
results	O
of	O
section	O
2.3	O
,	O
ﬁnd	O
an	O
expression	O
for	O
the	O
conditional	B
distribution	O
p	O
(	O
xa|xb	O
)	O
in	O
which	O
xc	O
has	O
been	O
marginalized	O
out	O
.	O
2.26	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
a	O
very	O
useful	O
result	O
from	O
linear	O
algebra	O
is	O
the	O
woodbury	O
matrix	O
inversion	O
formula	O
given	O
by	O
(	O
a	O
+	O
bcd	O
)	O
−1	O
=	O
a−1	O
−	O
a−1b	O
(	O
c−1	O
+	O
da−1b	O
)	O
−1da−1	O
.	O
(	O
2.289	O
)	O
by	O
multiplying	O
both	O
sides	O
by	O
(	O
a	O
+	O
bcd	O
)	O
prove	O
the	O
correctness	O
of	O
this	O
result	O
.	O
2.27	O
(	O
(	O
cid:12	O
)	O
)	O
let	O
x	O
and	O
z	O
be	O
two	O
independent	B
random	O
vectors	O
,	O
so	O
that	O
p	O
(	O
x	O
,	O
z	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
z	O
)	O
.	O
show	O
that	O
the	O
mean	B
of	O
their	O
sum	O
y	O
=	O
x	O
+	O
z	O
is	O
given	O
by	O
the	O
sum	O
of	O
the	O
means	O
of	O
each	O
of	O
the	O
variable	O
separately	O
.	O
similarly	O
,	O
show	O
that	O
the	O
covariance	B
matrix	I
of	O
y	O
is	O
given	O
by	O
the	O
sum	O
of	O
the	O
covariance	B
matrices	O
of	O
x	O
and	O
z.	O
conﬁrm	O
that	O
this	O
result	O
agrees	O
with	O
that	O
of	O
exercise	O
1.10	O
.	O
2.28	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
joint	O
distribution	O
over	O
the	O
variable	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
x	O
y	O
z	O
=	O
(	O
2.290	O
)	O
whose	O
mean	B
and	O
covariance	B
are	O
given	O
by	O
(	O
2.108	O
)	O
and	O
(	O
2.105	O
)	O
respectively	O
.	O
by	O
mak-	O
ing	O
use	O
of	O
the	O
results	O
(	O
2.92	O
)	O
and	O
(	O
2.93	O
)	O
show	O
that	O
the	O
marginal	B
distribution	O
p	O
(	O
x	O
)	O
is	O
given	O
(	O
2.99	O
)	O
.	O
similarly	O
,	O
by	O
making	O
use	O
of	O
the	O
results	O
(	O
2.81	O
)	O
and	O
(	O
2.82	O
)	O
show	O
that	O
the	O
conditional	B
distribution	O
p	O
(	O
y|x	O
)	O
is	O
given	O
by	O
(	O
2.100	O
)	O
.	O
exercises	O
133	O
2.29	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
using	O
the	O
partitioned	B
matrix	O
inversion	O
formula	O
(	O
2.76	O
)	O
,	O
show	O
that	O
the	O
inverse	B
of	O
the	O
precision	B
matrix	I
(	O
2.104	O
)	O
is	O
given	O
by	O
the	O
covariance	B
matrix	I
(	O
2.105	O
)	O
.	O
2.30	O
(	O
(	O
cid:12	O
)	O
)	O
by	O
starting	O
from	O
(	O
2.107	O
)	O
and	O
making	O
use	O
of	O
the	O
result	O
(	O
2.105	O
)	O
,	O
verify	O
the	O
result	O
(	O
2.108	O
)	O
.	O
2.31	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
two	O
multidimensional	O
random	O
vectors	O
x	O
and	O
z	O
having	O
gaussian	O
distributions	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
x|µx	O
,	O
σx	O
)	O
and	O
p	O
(	O
z	O
)	O
=	O
n	O
(	O
z|µz	O
,	O
σz	O
)	O
respectively	O
,	O
together	O
with	O
their	O
sum	O
y	O
=	O
x+z	O
.	O
use	O
the	O
results	O
(	O
2.109	O
)	O
and	O
(	O
2.110	O
)	O
to	O
ﬁnd	O
an	O
expression	O
for	O
the	O
marginal	B
distribution	O
p	O
(	O
y	O
)	O
by	O
considering	O
the	O
linear-gaussian	O
model	O
comprising	O
the	O
product	O
of	O
the	O
marginal	B
distribution	O
p	O
(	O
x	O
)	O
and	O
the	O
conditional	B
distribution	O
p	O
(	O
y|x	O
)	O
.	O
2.32	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
this	O
exercise	O
and	O
the	O
next	O
provide	O
practice	O
at	O
manipulating	O
the	O
quadratic	O
forms	O
that	O
arise	O
in	O
linear-gaussian	O
models	O
,	O
as	O
well	O
as	O
giving	O
an	O
indepen-	O
dent	O
check	O
of	O
results	O
derived	O
in	O
the	O
main	O
text	O
.	O
consider	O
a	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
y	O
)	O
deﬁned	O
by	O
the	O
marginal	B
and	O
conditional	B
distributions	O
given	O
by	O
(	O
2.99	O
)	O
and	O
(	O
2.100	O
)	O
.	O
by	O
examining	O
the	O
quadratic	O
form	O
in	O
the	O
exponent	O
of	O
the	O
joint	O
distribution	O
,	O
and	O
using	O
the	O
technique	O
of	O
‘	O
completing	B
the	I
square	I
’	O
discussed	O
in	O
section	O
2.3	O
,	O
ﬁnd	O
expressions	O
for	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
marginal	B
distribution	O
p	O
(	O
y	O
)	O
in	O
which	O
the	O
variable	O
x	O
has	O
been	O
integrated	O
out	O
.	O
to	O
do	O
this	O
,	O
make	O
use	O
of	O
the	O
woodbury	O
matrix	O
inversion	O
formula	O
(	O
2.289	O
)	O
.	O
verify	O
that	O
these	O
results	O
agree	O
with	O
(	O
2.109	O
)	O
and	O
(	O
2.110	O
)	O
obtained	O
using	O
the	O
results	O
of	O
chapter	O
2	O
.	O
2.33	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
the	O
same	O
joint	O
distribution	O
as	O
in	O
exercise	O
2.32	O
,	O
but	O
now	O
use	O
the	O
technique	O
of	O
completing	B
the	I
square	I
to	O
ﬁnd	O
expressions	O
for	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
conditional	B
distribution	O
p	O
(	O
x|y	O
)	O
.	O
again	O
,	O
verify	O
that	O
these	O
agree	O
with	O
the	O
corre-	O
sponding	O
expressions	O
(	O
2.111	O
)	O
and	O
(	O
2.112	O
)	O
.	O
2.34	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
to	O
ﬁnd	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
covariance	B
matrix	I
of	O
a	O
multivariate	O
gaussian	O
,	O
we	O
need	O
to	O
maximize	O
the	O
log	O
likelihood	O
function	O
(	O
2.118	O
)	O
with	O
respect	O
to	O
σ	O
,	O
noting	O
that	O
the	O
covariance	B
matrix	I
must	O
be	O
symmetric	O
and	O
positive	B
deﬁnite	I
.	O
here	O
we	O
proceed	O
by	O
ignoring	O
these	O
constraints	O
and	O
doing	O
a	O
straightforward	O
maximization	O
.	O
using	O
the	O
results	O
(	O
c.21	O
)	O
,	O
(	O
c.26	O
)	O
,	O
and	O
(	O
c.28	O
)	O
from	O
appendix	O
c	O
,	O
show	O
that	O
the	O
covariance	B
matrix	I
σ	O
that	O
maximizes	O
the	O
log	O
likelihood	O
function	O
(	O
2.118	O
)	O
is	O
given	O
by	O
the	O
sample	O
covariance	O
(	O
2.122	O
)	O
.	O
we	O
note	O
that	O
the	O
ﬁnal	O
result	O
is	O
necessarily	O
symmetric	O
and	O
positive	B
deﬁnite	I
(	O
provided	O
the	O
sample	O
covariance	O
is	O
nonsingular	O
)	O
.	O
2.35	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
use	O
the	O
result	O
(	O
2.59	O
)	O
to	O
prove	O
(	O
2.62	O
)	O
.	O
now	O
,	O
using	O
the	O
results	O
(	O
2.59	O
)	O
,	O
and	O
(	O
2.62	O
)	O
,	O
show	O
that	O
e	O
[	O
xnxm	O
]	O
=	O
µµt	O
+	O
inmς	O
(	O
2.291	O
)	O
where	O
xn	O
denotes	O
a	O
data	O
point	O
sampled	O
from	O
a	O
gaussian	O
distribution	O
with	O
mean	B
µ	O
and	O
covariance	B
σ	O
,	O
and	O
inm	O
denotes	O
the	O
(	O
n	O
,	O
m	O
)	O
element	O
of	O
the	O
identity	O
matrix	O
.	O
hence	O
prove	O
the	O
result	O
(	O
2.124	O
)	O
.	O
2.36	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
using	O
an	O
analogous	O
procedure	O
to	O
that	O
used	O
to	O
obtain	O
(	O
2.126	O
)	O
,	O
derive	O
an	O
expression	O
for	O
the	O
sequential	B
estimation	I
of	O
the	O
variance	B
of	O
a	O
univariate	O
gaussian	O
134	O
2.	O
probability	B
distributions	O
distribution	O
,	O
by	O
starting	O
with	O
the	O
maximum	B
likelihood	I
expression	O
n	O
(	O
cid:2	O
)	O
n=1	O
σ2	O
ml	O
=	O
1	O
n	O
(	O
xn	O
−	O
µ	O
)	O
2	O
.	O
(	O
2.292	O
)	O
verify	O
that	O
substituting	O
the	O
expression	O
for	O
a	O
gaussian	O
distribution	O
into	O
the	O
robbins-	O
monro	O
sequential	B
estimation	I
formula	O
(	O
2.135	O
)	O
gives	O
a	O
result	O
of	O
the	O
same	O
form	O
,	O
and	O
hence	O
obtain	O
an	O
expression	O
for	O
the	O
corresponding	O
coefﬁcients	O
an	O
.	O
2.37	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
using	O
an	O
analogous	O
procedure	O
to	O
that	O
used	O
to	O
obtain	O
(	O
2.126	O
)	O
,	O
derive	O
an	O
ex-	O
pression	O
for	O
the	O
sequential	B
estimation	I
of	O
the	O
covariance	B
of	O
a	O
multivariate	O
gaussian	O
distribution	O
,	O
by	O
starting	O
with	O
the	O
maximum	B
likelihood	I
expression	O
(	O
2.122	O
)	O
.	O
verify	O
that	O
substituting	O
the	O
expression	O
for	O
a	O
gaussian	O
distribution	O
into	O
the	O
robbins-monro	O
se-	O
quential	O
estimation	O
formula	O
(	O
2.135	O
)	O
gives	O
a	O
result	O
of	O
the	O
same	O
form	O
,	O
and	O
hence	O
obtain	O
an	O
expression	O
for	O
the	O
corresponding	O
coefﬁcients	O
an	O
.	O
2.38	O
(	O
(	O
cid:12	O
)	O
)	O
use	O
the	O
technique	O
of	O
completing	B
the	I
square	I
for	O
the	O
quadratic	O
form	O
in	O
the	O
expo-	O
nent	O
to	O
derive	O
the	O
results	O
(	O
2.141	O
)	O
and	O
(	O
2.142	O
)	O
.	O
2.39	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
starting	O
from	O
the	O
results	O
(	O
2.141	O
)	O
and	O
(	O
2.142	O
)	O
for	O
the	O
posterior	O
distribution	O
of	O
the	O
mean	B
of	O
a	O
gaussian	O
random	O
variable	O
,	O
dissect	O
out	O
the	O
contributions	O
from	O
the	O
ﬁrst	O
n	O
−	O
1	O
data	O
points	O
and	O
hence	O
obtain	O
expressions	O
for	O
the	O
sequential	O
update	O
of	O
µn	O
and	O
σ2	O
n	O
.	O
now	O
derive	O
the	O
same	O
results	O
starting	O
from	O
the	O
posterior	O
distribution	O
p	O
(	O
µ|x1	O
,	O
.	O
.	O
.	O
,	O
xn−1	O
)	O
=	O
n	O
(	O
µ|µn−1	O
,	O
σ2	O
n−1	O
)	O
and	O
multiplying	O
by	O
the	O
likelihood	O
func-	O
tion	O
p	O
(	O
xn|µ	O
)	O
=	O
n	O
(	O
xn|µ	O
,	O
σ2	O
)	O
and	O
then	O
completing	B
the	I
square	I
and	O
normalizing	O
to	O
obtain	O
the	O
posterior	O
distribution	O
after	O
n	O
observations	O
.	O
2.40	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
d-dimensional	O
gaussian	O
random	O
variable	O
x	O
with	O
distribu-	O
tion	O
n	O
(	O
x|µ	O
,	O
σ	O
)	O
in	O
which	O
the	O
covariance	B
σ	O
is	O
known	O
and	O
for	O
which	O
we	O
wish	O
to	O
infer	O
the	O
mean	B
µ	O
from	O
a	O
set	O
of	O
observations	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
.	O
given	O
a	O
prior	B
distribution	O
p	O
(	O
µ	O
)	O
=	O
n	O
(	O
µ|µ0	O
,	O
σ0	O
)	O
,	O
ﬁnd	O
the	O
corresponding	O
posterior	O
distribution	O
p	O
(	O
µ|x	O
)	O
.	O
2.41	O
(	O
(	O
cid:12	O
)	O
)	O
use	O
the	O
deﬁnition	O
of	O
the	O
gamma	B
function	I
(	O
1.141	O
)	O
to	O
show	O
that	O
the	O
gamma	O
dis-	O
tribution	O
(	O
2.146	O
)	O
is	O
normalized	O
.	O
2.42	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
evaluate	O
the	O
mean	B
,	O
variance	B
,	O
and	O
mode	O
of	O
the	O
gamma	B
distribution	I
(	O
2.146	O
)	O
.	O
2.43	O
(	O
(	O
cid:12	O
)	O
)	O
the	O
following	O
distribution	O
p	O
(	O
x|σ2	O
,	O
q	O
)	O
=	O
(	O
cid:16	O
)	O
(	O
cid:15	O
)	O
−|x|q	O
2σ2	O
(	O
2.293	O
)	O
(	O
2.294	O
)	O
q	O
2	O
(	O
2σ2	O
)	O
1/qγ	O
(	O
1/q	O
)	O
exp	O
p	O
(	O
x|σ2	O
,	O
q	O
)	O
dx	O
=	O
1	O
(	O
cid:6	O
)	O
∞	O
−∞	O
is	O
a	O
generalization	B
of	O
the	O
univariate	O
gaussian	O
distribution	O
.	O
show	O
that	O
this	O
distribution	O
is	O
normalized	O
so	O
that	O
and	O
that	O
it	O
reduces	O
to	O
the	O
gaussian	O
when	O
q	O
=	O
2.	O
consider	O
a	O
regression	B
model	O
in	O
which	O
the	O
target	O
variable	O
is	O
given	O
by	O
t	O
=	O
y	O
(	O
x	O
,	O
w	O
)	O
+	O
	O
and	O
	O
is	O
a	O
random	O
noise	O
exercises	O
135	O
variable	O
drawn	O
from	O
the	O
distribution	O
(	O
2.293	O
)	O
.	O
show	O
that	O
the	O
log	O
likelihood	O
function	O
over	O
w	O
and	O
σ2	O
,	O
for	O
an	O
observed	O
data	O
set	O
of	O
input	O
vectors	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
and	O
corresponding	O
target	O
variables	O
t	O
=	O
(	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
)	O
t	O
,	O
is	O
given	O
by	O
n	O
(	O
cid:2	O
)	O
n=1	O
ln	O
p	O
(	O
t|x	O
,	O
w	O
,	O
σ2	O
)	O
=	O
−	O
1	O
2σ2	O
|y	O
(	O
xn	O
,	O
w	O
)	O
−	O
tn|q	O
−	O
n	O
q	O
ln	O
(	O
2σ2	O
)	O
+	O
const	O
(	O
2.295	O
)	O
where	O
‘	O
const	O
’	O
denotes	O
terms	O
independent	B
of	O
both	O
w	O
and	O
σ2	O
.	O
note	O
that	O
,	O
as	O
a	O
function	O
of	O
w	O
,	O
this	O
is	O
the	O
lq	O
error	B
function	I
considered	O
in	O
section	O
1.5.5	O
.	O
2.44	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
univariate	O
gaussian	O
distribution	O
n	O
(	O
x|µ	O
,	O
τ	O
−1	O
)	O
having	O
conjugate	B
gaussian-gamma	O
prior	B
given	O
by	O
(	O
2.154	O
)	O
,	O
and	O
a	O
data	O
set	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
of	O
i.i.d	O
.	O
observations	O
.	O
show	O
that	O
the	O
posterior	O
distribution	O
is	O
also	O
a	O
gaussian-gamma	O
distri-	O
bution	O
of	O
the	O
same	O
functional	B
form	O
as	O
the	O
prior	B
,	O
and	O
write	O
down	O
expressions	O
for	O
the	O
parameters	O
of	O
this	O
posterior	O
distribution	O
.	O
2.45	O
(	O
(	O
cid:12	O
)	O
)	O
verify	O
that	O
the	O
wishart	O
distribution	O
deﬁned	O
by	O
(	O
2.155	O
)	O
is	O
indeed	O
a	O
conjugate	B
prior	I
for	O
the	O
precision	B
matrix	I
of	O
a	O
multivariate	O
gaussian	O
.	O
2.46	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
verify	O
that	O
evaluating	O
the	O
integral	O
in	O
(	O
2.158	O
)	O
leads	O
to	O
the	O
result	O
(	O
2.159	O
)	O
.	O
2.47	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
in	O
the	O
limit	O
ν	O
→	O
∞	O
,	O
the	O
t-distribution	O
(	O
2.159	O
)	O
becomes	O
a	O
gaussian	O
.	O
hint	O
:	O
ignore	O
the	O
normalization	O
coefﬁcient	O
,	O
and	O
simply	O
look	O
at	O
the	O
depen-	O
dence	O
on	O
x	O
.	O
2.48	O
(	O
(	O
cid:12	O
)	O
)	O
by	O
following	O
analogous	O
steps	O
to	O
those	O
used	O
to	O
derive	O
the	O
univariate	O
student	O
’	O
s	O
t-distribution	O
(	O
2.159	O
)	O
,	O
verify	O
the	O
result	O
(	O
2.162	O
)	O
for	O
the	O
multivariate	O
form	O
of	O
the	O
stu-	O
dent	O
’	O
s	O
t-distribution	O
,	O
by	O
marginalizing	O
over	O
the	O
variable	O
η	O
in	O
(	O
2.161	O
)	O
.	O
using	O
the	O
deﬁnition	O
(	O
2.161	O
)	O
,	O
show	O
by	O
exchanging	O
integration	O
variables	O
that	O
the	O
multivariate	O
t-distribution	O
is	O
correctly	O
normalized	O
.	O
2.49	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
by	O
using	O
the	O
deﬁnition	O
(	O
2.161	O
)	O
of	O
the	O
multivariate	O
student	O
’	O
s	O
t-distribution	O
as	O
a	O
convolution	O
of	O
a	O
gaussian	O
with	O
a	O
gamma	B
distribution	I
,	O
verify	O
the	O
properties	O
(	O
2.164	O
)	O
,	O
(	O
2.165	O
)	O
,	O
and	O
(	O
2.166	O
)	O
for	O
the	O
multivariate	O
t-distribution	O
deﬁned	O
by	O
(	O
2.162	O
)	O
.	O
2.50	O
(	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
in	O
the	O
limit	O
ν	O
→	O
∞	O
,	O
the	O
multivariate	O
student	O
’	O
s	O
t-distribution	O
(	O
2.162	O
)	O
reduces	O
to	O
a	O
gaussian	O
with	O
mean	B
µ	O
and	O
precision	O
λ	O
.	O
2.51	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
the	O
various	O
trigonometric	O
identities	O
used	O
in	O
the	O
discussion	O
of	O
periodic	O
variables	O
in	O
this	O
chapter	O
can	O
be	O
proven	O
easily	O
from	O
the	O
relation	O
exp	O
(	O
ia	O
)	O
=	O
cos	O
a	O
+	O
i	O
sin	O
a	O
in	O
which	O
i	O
is	O
the	O
square	O
root	O
of	O
minus	O
one	O
.	O
by	O
considering	O
the	O
identity	O
exp	O
(	O
ia	O
)	O
exp	O
(	O
−ia	O
)	O
=	O
1	O
prove	O
the	O
result	O
(	O
2.177	O
)	O
.	O
similarly	O
,	O
using	O
the	O
identity	O
cos	O
(	O
a	O
−	O
b	O
)	O
=	O
(	O
cid:10	O
)	O
exp	O
{	O
i	O
(	O
a	O
−	O
b	O
)	O
}	O
(	O
2.296	O
)	O
(	O
2.297	O
)	O
(	O
2.298	O
)	O
136	O
2.	O
probability	B
distributions	O
where	O
(	O
cid:10	O
)	O
denotes	O
the	O
real	O
part	O
,	O
prove	O
(	O
2.178	O
)	O
.	O
finally	O
,	O
by	O
using	O
sin	O
(	O
a	O
−	O
b	O
)	O
=	O
(	O
cid:11	O
)	O
exp	O
{	O
i	O
(	O
a	O
−	O
b	O
)	O
}	O
,	O
where	O
(	O
cid:11	O
)	O
denotes	O
the	O
imaginary	O
part	O
,	O
prove	O
the	O
result	O
(	O
2.183	O
)	O
.	O
for	O
large	O
m	O
,	O
the	O
von	O
mises	O
distribution	O
(	O
2.179	O
)	O
becomes	O
sharply	O
peaked	O
around	O
the	O
mode	O
θ0	O
.	O
by	O
deﬁning	O
ξ	O
=	O
m1/2	O
(	O
θ	O
−	O
θ0	O
)	O
and	O
making	O
the	O
taylor	O
ex-	O
pansion	O
of	O
the	O
cosine	O
function	O
given	O
by	O
2.52	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
as	O
m	O
→	O
∞	O
,	O
the	O
von	O
mises	O
distribution	O
tends	O
to	O
a	O
gaussian	O
.	O
cos	O
α	O
=	O
1	O
−	O
α2	O
2	O
+	O
o	O
(	O
α4	O
)	O
(	O
2.299	O
)	O
2.53	O
(	O
(	O
cid:12	O
)	O
)	O
using	O
the	O
trigonometric	O
identity	O
(	O
2.183	O
)	O
,	O
show	O
that	O
solution	O
of	O
(	O
2.182	O
)	O
for	O
θ0	O
is	O
given	O
by	O
(	O
2.184	O
)	O
.	O
2.54	O
(	O
(	O
cid:12	O
)	O
)	O
by	O
computing	O
ﬁrst	O
and	O
second	O
derivatives	O
of	O
the	O
von	O
mises	O
distribution	O
(	O
2.179	O
)	O
,	O
and	O
using	O
i0	O
(	O
m	O
)	O
>	O
0	O
for	O
m	O
>	O
0	O
,	O
show	O
that	O
the	O
maximum	O
of	O
the	O
distribution	O
occurs	O
when	O
θ	O
=	O
θ0	O
and	O
that	O
the	O
minimum	O
occurs	O
when	O
θ	O
=	O
θ0	O
+	O
π	O
(	O
mod	O
2π	O
)	O
.	O
2.55	O
(	O
(	O
cid:12	O
)	O
)	O
by	O
making	O
use	O
of	O
the	O
result	O
(	O
2.168	O
)	O
,	O
together	O
with	O
(	O
2.184	O
)	O
and	O
the	O
trigonometric	O
identity	O
(	O
2.178	O
)	O
,	O
show	O
that	O
the	O
maximum	B
likelihood	I
solution	O
mml	O
for	O
the	O
concentra-	O
tion	O
of	O
the	O
von	O
mises	O
distribution	O
satisﬁes	O
a	O
(	O
mml	O
)	O
=	O
r	O
where	O
r	O
is	O
the	O
radius	O
of	O
the	O
mean	B
of	O
the	O
observations	O
viewed	O
as	O
unit	O
vectors	O
in	O
the	O
two-dimensional	O
euclidean	O
plane	O
,	O
as	O
illustrated	O
in	O
figure	O
2.17	O
.	O
2.56	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
express	O
the	O
beta	B
distribution	I
(	O
2.13	O
)	O
,	O
the	O
gamma	B
distribution	I
(	O
2.146	O
)	O
,	O
and	O
the	O
von	O
mises	O
distribution	O
(	O
2.179	O
)	O
as	O
members	O
of	O
the	O
exponential	B
family	I
(	O
2.194	O
)	O
and	O
thereby	O
identify	O
their	O
natural	B
parameters	I
.	O
2.57	O
(	O
(	O
cid:12	O
)	O
)	O
verify	O
that	O
the	O
multivariate	O
gaussian	O
distribution	O
can	O
be	O
cast	O
in	O
exponential	B
family	I
form	O
(	O
2.194	O
)	O
and	O
derive	O
expressions	O
for	O
η	O
,	O
u	O
(	O
x	O
)	O
,	O
h	O
(	O
x	O
)	O
and	O
g	O
(	O
η	O
)	O
analogous	O
to	O
(	O
2.220	O
)	O
–	O
(	O
2.223	O
)	O
.	O
2.58	O
(	O
(	O
cid:12	O
)	O
)	O
the	O
result	O
(	O
2.226	O
)	O
showed	O
that	O
the	O
negative	O
gradient	O
of	O
ln	O
g	O
(	O
η	O
)	O
for	O
the	O
exponen-	O
tial	O
family	O
is	O
given	O
by	O
the	O
expectation	B
of	O
u	O
(	O
x	O
)	O
.	O
by	O
taking	O
the	O
second	O
derivatives	O
of	O
(	O
2.195	O
)	O
,	O
show	O
that	O
−∇∇	O
ln	O
g	O
(	O
η	O
)	O
=	O
e	O
[	O
u	O
(	O
x	O
)	O
u	O
(	O
x	O
)	O
t	O
]	O
−	O
e	O
[	O
u	O
(	O
x	O
)	O
]	O
e	O
[	O
u	O
(	O
x	O
)	O
t	O
]	O
=	O
cov	O
[	O
u	O
(	O
x	O
)	O
]	O
.	O
(	O
2.300	O
)	O
2.59	O
(	O
(	O
cid:12	O
)	O
)	O
by	O
changing	O
variables	O
using	O
y	O
=	O
x/σ	O
,	O
show	O
that	O
the	O
density	B
(	O
2.236	O
)	O
will	O
be	O
correctly	O
normalized	O
,	O
provided	O
f	O
(	O
x	O
)	O
is	O
correctly	O
normalized	O
.	O
2.60	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
histogram-like	O
density	B
model	O
in	O
which	O
the	O
space	O
x	O
is	O
di-	O
vided	O
into	O
ﬁxed	O
regions	O
for	O
which	O
the	O
density	B
p	O
(	O
x	O
)	O
takes	O
the	O
constant	O
value	O
hi	O
over	O
the	O
ith	O
region	O
,	O
and	O
that	O
the	O
volume	O
of	O
region	O
i	O
is	O
denoted	O
∆i	O
.	O
suppose	O
we	O
have	O
a	O
set	O
of	O
n	O
observations	O
of	O
x	O
such	O
that	O
ni	O
of	O
these	O
observations	O
fall	O
in	O
region	O
i.	O
using	O
a	O
lagrange	O
multiplier	O
to	O
enforce	O
the	O
normalization	O
constraint	O
on	O
the	O
density	B
,	O
derive	O
an	O
expression	O
for	O
the	O
maximum	B
likelihood	I
estimator	O
for	O
the	O
{	O
hi	O
}	O
.	O
2.61	O
(	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
the	O
k-nearest-neighbour	O
density	B
model	O
deﬁnes	O
an	O
improper	B
distribu-	O
tion	O
whose	O
integral	O
over	O
all	O
space	O
is	O
divergent	O
.	O
3	O
linear	O
models	O
for	B
regression	I
the	O
focus	O
so	O
far	O
in	O
this	O
book	O
has	O
been	O
on	O
unsupervised	B
learning	I
,	O
including	O
topics	O
such	O
as	O
density	B
estimation	I
and	O
data	O
clustering	O
.	O
we	O
turn	O
now	O
to	O
a	O
discussion	O
of	O
super-	O
vised	O
learning	B
,	O
starting	O
with	O
regression	B
.	O
the	O
goal	O
of	O
regression	B
is	O
to	O
predict	O
the	O
value	O
of	O
one	O
or	O
more	O
continuous	O
target	O
variables	O
t	O
given	O
the	O
value	O
of	O
a	O
d-dimensional	O
vec-	O
tor	O
x	O
of	O
input	O
variables	O
.	O
we	O
have	O
already	O
encountered	O
an	O
example	O
of	O
a	O
regression	B
problem	O
when	O
we	O
considered	O
polynomial	B
curve	I
ﬁtting	I
in	O
chapter	O
1.	O
the	O
polynomial	O
is	O
a	O
speciﬁc	O
example	O
of	O
a	O
broad	O
class	O
of	O
functions	O
called	O
linear	B
regression	I
models	O
,	O
which	O
share	O
the	O
property	O
of	O
being	O
linear	O
functions	O
of	O
the	O
adjustable	O
parameters	O
,	O
and	O
which	O
will	O
form	O
the	O
focus	O
of	O
this	O
chapter	O
.	O
the	O
simplest	O
form	O
of	O
linear	B
regression	I
models	O
are	O
also	O
linear	O
functions	O
of	O
the	O
input	O
variables	O
.	O
however	O
,	O
we	O
can	O
obtain	O
a	O
much	O
more	O
useful	O
class	O
of	O
functions	O
by	O
taking	O
linear	O
combinations	O
of	O
a	O
ﬁxed	O
set	O
of	O
nonlinear	O
functions	O
of	O
the	O
input	O
variables	O
,	O
known	O
as	O
basis	O
functions	O
.	O
such	O
models	O
are	O
linear	O
functions	O
of	O
the	O
parameters	O
,	O
which	O
gives	O
them	O
simple	O
analytical	O
properties	O
,	O
and	O
yet	O
can	O
be	O
nonlinear	O
with	O
respect	O
to	O
the	O
input	O
variables	O
.	O
137	O
138	O
3.	O
linear	O
models	O
for	B
regression	I
given	O
a	O
training	B
data	O
set	O
comprising	O
n	O
observations	O
{	O
xn	O
}	O
,	O
where	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
together	O
with	O
corresponding	O
target	O
values	O
{	O
tn	O
}	O
,	O
the	O
goal	O
is	O
to	O
predict	O
the	O
value	O
of	O
t	O
for	O
a	O
new	O
value	O
of	O
x.	O
in	O
the	O
simplest	O
approach	O
,	O
this	O
can	O
be	O
done	O
by	O
directly	O
con-	O
structing	O
an	O
appropriate	O
function	O
y	O
(	O
x	O
)	O
whose	O
values	O
for	O
new	O
inputs	O
x	O
constitute	O
the	O
predictions	O
for	O
the	O
corresponding	O
values	O
of	O
t.	O
more	O
generally	O
,	O
from	O
a	O
probabilistic	O
perspective	O
,	O
we	O
aim	O
to	O
model	O
the	O
predictive	B
distribution	I
p	O
(	O
t|x	O
)	O
because	O
this	O
expresses	O
our	O
uncertainty	O
about	O
the	O
value	O
of	O
t	O
for	O
each	O
value	O
of	O
x.	O
from	O
this	O
conditional	B
dis-	O
tribution	O
we	O
can	O
make	O
predictions	O
of	O
t	O
,	O
for	O
any	O
new	O
value	O
of	O
x	O
,	O
in	O
such	O
a	O
way	O
as	O
to	O
minimize	O
the	O
expected	O
value	O
of	O
a	O
suitably	O
chosen	O
loss	B
function	I
.	O
as	O
discussed	O
in	O
sec-	O
tion	O
1.5.5	O
,	O
a	O
common	O
choice	O
of	O
loss	B
function	I
for	O
real-valued	O
variables	O
is	O
the	O
squared	O
loss	O
,	O
for	O
which	O
the	O
optimal	O
solution	O
is	O
given	O
by	O
the	O
conditional	B
expectation	I
of	O
t.	O
although	O
linear	O
models	O
have	O
signiﬁcant	O
limitations	O
as	O
practical	O
techniques	O
for	O
pattern	O
recognition	O
,	O
particularly	O
for	O
problems	O
involving	O
input	O
spaces	O
of	O
high	O
dimen-	O
sionality	O
,	O
they	O
have	O
nice	O
analytical	O
properties	O
and	O
form	O
the	O
foundation	O
for	O
more	O
so-	O
phisticated	O
models	O
to	O
be	O
discussed	O
in	O
later	O
chapters	O
.	O
3.1.	O
linear	O
basis	O
function	O
models	O
the	O
simplest	O
linear	O
model	O
for	B
regression	I
is	O
one	O
that	O
involves	O
a	O
linear	O
combination	O
of	O
the	O
input	O
variables	O
y	O
(	O
x	O
,	O
w	O
)	O
=	O
w0	O
+	O
w1x1	O
+	O
.	O
.	O
.	O
+	O
wdxd	O
(	O
3.1	O
)	O
where	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
t.	O
this	O
is	O
often	O
simply	O
known	O
as	O
linear	B
regression	I
.	O
the	O
key	O
property	O
of	O
this	O
model	O
is	O
that	O
it	O
is	O
a	O
linear	O
function	O
of	O
the	O
parameters	O
w0	O
,	O
.	O
.	O
.	O
,	O
wd	O
.	O
it	O
is	O
also	O
,	O
however	O
,	O
a	O
linear	O
function	O
of	O
the	O
input	O
variables	O
xi	O
,	O
and	O
this	O
imposes	O
signiﬁcant	O
limitations	O
on	O
the	O
model	O
.	O
we	O
therefore	O
extend	O
the	O
class	O
of	O
models	O
by	O
considering	O
linear	O
combinations	O
of	O
ﬁxed	O
nonlinear	O
functions	O
of	O
the	O
input	O
variables	O
,	O
of	O
the	O
form	O
m−1	O
(	O
cid:2	O
)	O
y	O
(	O
x	O
,	O
w	O
)	O
=	O
w0	O
+	O
wjφj	O
(	O
x	O
)	O
(	O
3.2	O
)	O
where	O
φj	O
(	O
x	O
)	O
are	O
known	O
as	O
basis	O
functions	O
.	O
by	O
denoting	O
the	O
maximum	O
value	O
of	O
the	O
index	O
j	O
by	O
m	O
−	O
1	O
,	O
the	O
total	O
number	O
of	O
parameters	O
in	O
this	O
model	O
will	O
be	O
m.	O
j=1	O
the	O
parameter	O
w0	O
allows	O
for	O
any	O
ﬁxed	O
offset	O
in	O
the	O
data	O
and	O
is	O
sometimes	O
called	O
a	O
bias	B
parameter	I
(	O
not	O
to	O
be	O
confused	O
with	O
‘	O
bias	B
’	O
in	O
a	O
statistical	O
sense	O
)	O
.	O
it	O
is	O
often	O
convenient	O
to	O
deﬁne	O
an	O
additional	O
dummy	O
‘	O
basis	B
function	I
’	O
φ0	O
(	O
x	O
)	O
=	O
1	O
so	O
that	O
m−1	O
(	O
cid:2	O
)	O
y	O
(	O
x	O
,	O
w	O
)	O
=	O
wjφj	O
(	O
x	O
)	O
=	O
wtφ	O
(	O
x	O
)	O
(	O
3.3	O
)	O
j=0	O
where	O
w	O
=	O
(	O
w0	O
,	O
.	O
.	O
.	O
,	O
wm−1	O
)	O
t	O
and	O
φ	O
=	O
(	O
φ0	O
,	O
.	O
.	O
.	O
,	O
φm−1	O
)	O
t.	O
in	O
many	O
practical	O
ap-	O
plications	O
of	O
pattern	O
recognition	O
,	O
we	O
will	O
apply	O
some	O
form	O
of	O
ﬁxed	O
pre-processing	O
,	O
3.1.	O
linear	O
basis	O
function	O
models	O
139	O
or	O
feature	B
extraction	I
,	O
to	O
the	O
original	O
data	O
variables	O
.	O
if	O
the	O
original	O
variables	O
com-	O
prise	O
the	O
vector	O
x	O
,	O
then	O
the	O
features	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
basis	O
functions	O
{	O
φj	O
(	O
x	O
)	O
}	O
.	O
by	O
using	O
nonlinear	O
basis	O
functions	O
,	O
we	O
allow	O
the	O
function	O
y	O
(	O
x	O
,	O
w	O
)	O
to	O
be	O
a	O
non-	O
linear	O
function	O
of	O
the	O
input	O
vector	O
x.	O
functions	O
of	O
the	O
form	O
(	O
3.2	O
)	O
are	O
called	O
linear	O
models	O
,	O
however	O
,	O
because	O
this	O
function	O
is	O
linear	O
in	O
w.	O
it	O
is	O
this	O
linearity	O
in	O
the	O
pa-	O
rameters	O
that	O
will	O
greatly	O
simplify	O
the	O
analysis	O
of	O
this	O
class	O
of	O
models	O
.	O
however	O
,	O
it	O
also	O
leads	O
to	O
some	O
signiﬁcant	O
limitations	O
,	O
as	O
we	O
discuss	O
in	O
section	O
3.6.	O
the	O
example	O
of	O
polynomial	O
regression	O
considered	O
in	O
chapter	O
1	O
is	O
a	O
particular	O
example	O
of	O
this	O
model	O
in	O
which	O
there	O
is	O
a	O
single	O
input	O
variable	O
x	O
,	O
and	O
the	O
basis	O
func-	O
tions	O
take	O
the	O
form	O
of	O
powers	O
of	O
x	O
so	O
that	O
φj	O
(	O
x	O
)	O
=	O
xj	O
.	O
one	O
limitation	O
of	O
polynomial	O
basis	O
functions	O
is	O
that	O
they	O
are	O
global	O
functions	O
of	O
the	O
input	O
variable	O
,	O
so	O
that	O
changes	O
in	O
one	O
region	O
of	O
input	O
space	O
affect	O
all	O
other	O
regions	O
.	O
this	O
can	O
be	O
resolved	O
by	O
dividing	O
the	O
input	O
space	O
up	O
into	O
regions	O
and	O
ﬁt	O
a	O
different	O
polynomial	O
in	O
each	O
region	O
,	O
leading	O
to	O
spline	B
functions	I
(	O
hastie	O
et	O
al.	O
,	O
2001	O
)	O
.	O
there	O
are	O
many	O
other	O
possible	O
choices	O
for	O
the	O
basis	O
functions	O
,	O
for	O
example	O
(	O
cid:12	O
)	O
−	O
(	O
x	O
−	O
µj	O
)	O
2	O
(	O
cid:13	O
)	O
2s2	O
φj	O
(	O
x	O
)	O
=	O
exp	O
(	O
3.4	O
)	O
(	O
3.5	O
)	O
(	O
3.6	O
)	O
where	O
the	O
µj	O
govern	O
the	O
locations	O
of	O
the	O
basis	O
functions	O
in	O
input	O
space	O
,	O
and	O
the	O
pa-	O
rameter	O
s	O
governs	O
their	O
spatial	O
scale	O
.	O
these	O
are	O
usually	O
referred	O
to	O
as	O
‘	O
gaussian	O
’	O
basis	O
functions	O
,	O
although	O
it	O
should	O
be	O
noted	O
that	O
they	O
are	O
not	O
required	O
to	O
have	O
a	O
prob-	O
abilistic	O
interpretation	O
,	O
and	O
in	O
particular	O
the	O
normalization	O
coefﬁcient	O
is	O
unimportant	O
because	O
these	O
basis	O
functions	O
will	O
be	O
multiplied	O
by	O
adaptive	O
parameters	O
wj	O
.	O
another	O
possibility	O
is	O
the	O
sigmoidal	O
basis	B
function	I
of	O
the	O
form	O
(	O
cid:18	O
)	O
(	O
cid:17	O
)	O
x	O
−	O
µj	O
s	O
φj	O
(	O
x	O
)	O
=	O
σ	O
where	O
σ	O
(	O
a	O
)	O
is	O
the	O
logistic	B
sigmoid	I
function	O
deﬁned	O
by	O
σ	O
(	O
a	O
)	O
=	O
1	O
1	O
+	O
exp	O
(	O
−a	O
)	O
.	O
equivalently	O
,	O
we	O
can	O
use	O
the	O
‘	O
tanh	O
’	O
function	O
because	O
this	O
is	O
related	O
to	O
the	O
logistic	B
sigmoid	I
by	O
tanh	O
(	O
a	O
)	O
=	O
2σ	O
(	O
a	O
)	O
−	O
1	O
,	O
and	O
so	O
a	O
general	O
linear	O
combination	O
of	O
logistic	B
sigmoid	I
functions	O
is	O
equivalent	O
to	O
a	O
general	O
linear	O
combination	O
of	O
‘	O
tanh	O
’	O
functions	O
.	O
these	O
various	O
choices	O
of	O
basis	B
function	I
are	O
illustrated	O
in	O
figure	O
3.1.	O
yet	O
another	O
possible	O
choice	O
of	O
basis	B
function	I
is	O
the	O
fourier	O
basis	O
,	O
which	O
leads	O
to	O
an	O
expansion	O
in	O
sinusoidal	O
functions	O
.	O
each	O
basis	B
function	I
represents	O
a	O
speciﬁc	O
fre-	O
quency	O
and	O
has	O
inﬁnite	O
spatial	O
extent	O
.	O
by	O
contrast	O
,	O
basis	O
functions	O
that	O
are	O
localized	O
to	O
ﬁnite	O
regions	O
of	O
input	O
space	O
necessarily	O
comprise	O
a	O
spectrum	O
of	O
different	O
spatial	O
frequencies	O
.	O
in	O
many	O
signal	O
processing	O
applications	O
,	O
it	O
is	O
of	O
interest	O
to	O
consider	O
ba-	O
sis	O
functions	O
that	O
are	O
localized	O
in	O
both	O
space	O
and	O
frequency	O
,	O
leading	O
to	O
a	O
class	O
of	O
functions	O
known	O
as	O
wavelets	B
.	O
these	O
are	O
also	O
deﬁned	O
to	O
be	O
mutually	O
orthogonal	O
,	O
to	O
simplify	O
their	O
application	O
.	O
wavelets	B
are	O
most	O
applicable	O
when	O
the	O
input	O
values	O
live	O
140	O
3.	O
linear	O
models	O
for	B
regression	I
1	O
0.5	O
0	O
−0.5	O
−1	O
−1	O
1	O
0.75	O
0.5	O
0.25	O
0	O
−1	O
0	O
1	O
1	O
0.75	O
0.5	O
0.25	O
0	O
−1	O
0	O
1	O
0	O
1	O
figure	O
3.1	O
examples	O
of	O
basis	O
functions	O
,	O
showing	O
polynomials	O
on	O
the	O
left	O
,	O
gaussians	O
of	O
the	O
form	O
(	O
3.4	O
)	O
in	O
the	O
centre	O
,	O
and	O
sigmoidal	O
of	O
the	O
form	O
(	O
3.5	O
)	O
on	O
the	O
right	O
.	O
on	O
a	O
regular	O
lattice	O
,	O
such	O
as	O
the	O
successive	O
time	O
points	O
in	O
a	O
temporal	O
sequence	O
,	O
or	O
the	O
pixels	O
in	O
an	O
image	O
.	O
useful	O
texts	O
on	O
wavelets	B
include	O
ogden	O
(	O
1997	O
)	O
,	O
mallat	O
(	O
1999	O
)	O
,	O
and	O
vidakovic	O
(	O
1999	O
)	O
.	O
most	O
of	O
the	O
discussion	O
in	O
this	O
chapter	O
,	O
however	O
,	O
is	O
independent	B
of	O
the	O
particular	O
choice	O
of	O
basis	B
function	I
set	O
,	O
and	O
so	O
for	O
most	O
of	O
our	O
discussion	O
we	O
shall	O
not	O
specify	O
the	O
particular	O
form	O
of	O
the	O
basis	O
functions	O
,	O
except	O
for	O
the	O
purposes	O
of	O
numerical	O
il-	O
lustration	O
.	O
indeed	O
,	O
much	O
of	O
our	O
discussion	O
will	O
be	O
equally	O
applicable	O
to	O
the	O
situation	O
in	O
which	O
the	O
vector	O
φ	O
(	O
x	O
)	O
of	O
basis	O
functions	O
is	O
simply	O
the	O
identity	O
φ	O
(	O
x	O
)	O
=	O
x.	O
fur-	O
thermore	O
,	O
in	O
order	O
to	O
keep	O
the	O
notation	O
simple	O
,	O
we	O
shall	O
focus	O
on	O
the	O
case	O
of	O
a	O
single	O
target	O
variable	O
t.	O
however	O
,	O
in	O
section	O
3.1.5	O
,	O
we	O
consider	O
brieﬂy	O
the	O
modiﬁcations	O
needed	O
to	O
deal	O
with	O
multiple	O
target	O
variables	O
.	O
3.1.1	O
maximum	B
likelihood	I
and	O
least	O
squares	O
in	O
chapter	O
1	O
,	O
we	O
ﬁtted	O
polynomial	O
functions	O
to	O
data	O
sets	O
by	O
minimizing	O
a	O
sum-	O
of-squares	O
error	B
function	I
.	O
we	O
also	O
showed	O
that	O
this	O
error	B
function	I
could	O
be	O
motivated	O
as	O
the	O
maximum	B
likelihood	I
solution	O
under	O
an	O
assumed	O
gaussian	O
noise	O
model	O
.	O
let	O
us	O
return	O
to	O
this	O
discussion	O
and	O
consider	O
the	O
least	O
squares	O
approach	O
,	O
and	O
its	O
relation	O
to	O
maximum	O
likelihood	O
,	O
in	O
more	O
detail	O
.	O
as	O
before	O
,	O
we	O
assume	O
that	O
the	O
target	O
variable	O
t	O
is	O
given	O
by	O
a	O
deterministic	O
func-	O
tion	O
y	O
(	O
x	O
,	O
w	O
)	O
with	O
additive	O
gaussian	O
noise	O
so	O
that	O
t	O
=	O
y	O
(	O
x	O
,	O
w	O
)	O
+	O
	O
(	O
3.7	O
)	O
where	O
	O
is	O
a	O
zero	O
mean	B
gaussian	O
random	O
variable	O
with	O
precision	O
(	O
inverse	B
variance	O
)	O
β.	O
thus	O
we	O
can	O
write	O
p	O
(	O
t|x	O
,	O
w	O
,	O
β	O
)	O
=	O
n	O
(	O
t|y	O
(	O
x	O
,	O
w	O
)	O
,	O
β	O
−1	O
)	O
.	O
(	O
3.8	O
)	O
section	O
1.5.5	O
recall	O
that	O
,	O
if	O
we	O
assume	O
a	O
squared	O
loss	B
function	I
,	O
then	O
the	O
optimal	O
prediction	O
,	O
for	O
a	O
new	O
value	O
of	O
x	O
,	O
will	O
be	O
given	O
by	O
the	O
conditional	B
mean	O
of	O
the	O
target	O
variable	O
.	O
in	O
the	O
case	O
of	O
a	O
gaussian	O
conditional	B
distribution	O
of	O
the	O
form	O
(	O
3.8	O
)	O
,	O
the	O
conditional	B
mean	O
will	O
be	O
simply	O
e	O
[	O
t|x	O
]	O
=	O
tp	O
(	O
t|x	O
)	O
dt	O
=	O
y	O
(	O
x	O
,	O
w	O
)	O
.	O
note	O
that	O
the	O
gaussian	O
noise	O
assumption	O
implies	O
that	O
the	O
conditional	B
distribution	O
of	O
t	O
given	O
x	O
is	O
unimodal	O
,	O
which	O
may	O
be	O
inappropriate	O
for	O
some	O
applications	O
.	O
an	O
ex-	O
tension	O
to	O
mixtures	O
of	O
conditional	B
gaussian	O
distributions	O
,	O
which	O
permit	O
multimodal	O
conditional	B
distributions	O
,	O
will	O
be	O
discussed	O
in	O
section	O
14.5.1.	O
now	O
consider	O
a	O
data	O
set	O
of	O
inputs	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
with	O
corresponding	O
target	O
values	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
.	O
we	O
group	O
the	O
target	O
variables	O
{	O
tn	O
}	O
into	O
a	O
column	O
vector	O
that	O
we	O
denote	O
by	O
t	O
where	O
the	O
typeface	O
is	O
chosen	O
to	O
distinguish	O
it	O
from	O
a	O
single	O
observation	O
of	O
a	O
multivariate	O
target	O
,	O
which	O
would	O
be	O
denoted	O
t.	O
making	O
the	O
assumption	O
that	O
these	O
data	O
points	O
are	O
drawn	O
independently	O
from	O
the	O
distribution	O
(	O
3.8	O
)	O
,	O
we	O
obtain	O
the	O
following	O
expression	O
for	O
the	O
likelihood	B
function	I
,	O
which	O
is	O
a	O
function	O
of	O
the	O
adjustable	O
parameters	O
w	O
and	O
β	O
,	O
in	O
the	O
form	O
p	O
(	O
t|x	O
,	O
w	O
,	O
β	O
)	O
=	O
n	O
(	O
tn|wtφ	O
(	O
xn	O
)	O
,	O
β	O
−1	O
)	O
(	O
3.10	O
)	O
where	O
we	O
have	O
used	O
(	O
3.3	O
)	O
.	O
note	O
that	O
in	O
supervised	B
learning	I
problems	O
such	O
as	O
regres-	O
sion	B
(	O
and	O
classiﬁcation	B
)	O
,	O
we	O
are	O
not	O
seeking	O
to	O
model	O
the	O
distribution	O
of	O
the	O
input	O
variables	O
.	O
thus	O
x	O
will	O
always	O
appear	O
in	O
the	O
set	O
of	O
conditioning	O
variables	O
,	O
and	O
so	O
from	O
now	O
on	O
we	O
will	O
drop	O
the	O
explicit	O
x	O
from	O
expressions	O
such	O
as	O
p	O
(	O
t|x	O
,	O
w	O
,	O
β	O
)	O
in	O
or-	O
der	O
to	O
keep	O
the	O
notation	O
uncluttered	O
.	O
taking	O
the	O
logarithm	O
of	O
the	O
likelihood	B
function	I
,	O
and	O
making	O
use	O
of	O
the	O
standard	O
form	O
(	O
1.46	O
)	O
for	O
the	O
univariate	O
gaussian	O
,	O
we	O
have	O
ln	O
p	O
(	O
t|w	O
,	O
β	O
)	O
=	O
lnn	O
(	O
tn|wtφ	O
(	O
xn	O
)	O
,	O
β	O
−1	O
)	O
=	O
n	O
2	O
ln	O
β	O
−	O
n	O
2	O
ln	O
(	O
2π	O
)	O
−	O
βed	O
(	O
w	O
)	O
(	O
3.11	O
)	O
where	O
the	O
sum-of-squares	B
error	I
function	O
is	O
deﬁned	O
by	O
ed	O
(	O
w	O
)	O
=	O
1	O
2	O
{	O
tn	O
−	O
wtφ	O
(	O
xn	O
)	O
}	O
2	O
.	O
(	O
3.12	O
)	O
having	O
written	O
down	O
the	O
likelihood	B
function	I
,	O
we	O
can	O
use	O
maximum	B
likelihood	I
to	O
determine	O
w	O
and	O
β.	O
consider	O
ﬁrst	O
the	O
maximization	O
with	O
respect	O
to	O
w.	O
as	O
observed	O
already	O
in	O
section	O
1.2.5	O
,	O
we	O
see	O
that	O
maximization	O
of	O
the	O
likelihood	B
function	I
under	O
a	O
conditional	B
gaussian	O
noise	O
distribution	O
for	O
a	O
linear	O
model	O
is	O
equivalent	O
to	O
minimizing	O
a	O
sum-of-squares	B
error	I
function	O
given	O
by	O
ed	O
(	O
w	O
)	O
.	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	O
function	O
(	O
3.11	O
)	O
takes	O
the	O
form	O
∇	O
ln	O
p	O
(	O
t|w	O
,	O
β	O
)	O
=	O
tn	O
−	O
wtφ	O
(	O
xn	O
)	O
φ	O
(	O
xn	O
)	O
t.	O
(	O
3.13	O
)	O
(	O
cid:27	O
)	O
3.1.	O
linear	O
basis	O
function	O
models	O
141	O
(	O
3.9	O
)	O
(	O
cid:6	O
)	O
n	O
(	O
cid:14	O
)	O
n=1	O
n	O
(	O
cid:2	O
)	O
n=1	O
n	O
(	O
cid:2	O
)	O
n=1	O
n	O
(	O
cid:2	O
)	O
(	O
cid:26	O
)	O
n=1	O
142	O
3.	O
linear	O
models	O
for	B
regression	I
setting	O
this	O
gradient	O
to	O
zero	O
gives	O
n	O
(	O
cid:2	O
)	O
n=1	O
solving	O
for	O
w	O
we	O
obtain	O
(	O
cid:22	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:11	O
)	O
−1	O
φtt	O
n=1	O
(	O
cid:10	O
)	O
0	O
=	O
tnφ	O
(	O
xn	O
)	O
t	O
−	O
wt	O
φ	O
(	O
xn	O
)	O
φ	O
(	O
xn	O
)	O
t	O
(	O
cid:23	O
)	O
.	O
(	O
3.14	O
)	O
φ	O
=	O
the	O
quantity	O
wml	O
=	O
(	O
3.15	O
)	O
which	O
are	O
known	O
as	O
the	O
normal	B
equations	I
for	O
the	O
least	O
squares	O
problem	O
.	O
here	O
φ	O
is	O
an	O
n×m	O
matrix	O
,	O
called	O
the	O
design	B
matrix	I
,	O
whose	O
elements	O
are	O
given	O
by	O
φnj	O
=	O
φj	O
(	O
xn	O
)	O
,	O
so	O
that	O
φtφ	O
⎛⎜⎜⎝	O
φ0	O
(	O
x1	O
)	O
φ0	O
(	O
x2	O
)	O
...	O
φ1	O
(	O
x1	O
)	O
φ1	O
(	O
x2	O
)	O
...	O
†	O
≡	O
(	O
cid:10	O
)	O
φm−1	O
(	O
x1	O
)	O
φm−1	O
(	O
x2	O
)	O
···	O
···	O
...	O
···	O
φm−1	O
(	O
xn	O
)	O
...	O
(	O
cid:11	O
)	O
−1	O
φt	O
φ0	O
(	O
xn	O
)	O
φ1	O
(	O
xn	O
)	O
⎞⎟⎟⎠	O
.	O
φ	O
φtφ	O
(	O
3.17	O
)	O
is	O
known	O
as	O
the	O
moore-penrose	O
pseudo-inverse	B
of	O
the	O
matrix	O
φ	O
(	O
rao	O
and	O
mitra	O
,	O
1971	O
;	O
golub	O
and	O
van	O
loan	O
,	O
1996	O
)	O
.	O
it	O
can	O
be	O
regarded	O
as	O
a	O
generalization	B
of	O
the	O
notion	O
of	O
matrix	O
inverse	B
to	O
nonsquare	O
matrices	O
.	O
indeed	O
,	O
if	O
φ	O
is	O
square	O
and	O
invertible	O
,	O
then	O
using	O
the	O
property	O
(	O
ab	O
)	O
−1	O
=	O
b−1a−1	O
we	O
see	O
that	O
φ	O
†	O
≡	O
φ	O
−1	O
.	O
at	O
this	O
point	O
,	O
we	O
can	O
gain	O
some	O
insight	O
into	O
the	O
role	O
of	O
the	O
bias	B
parameter	I
w0	O
.	O
if	O
we	O
make	O
the	O
bias	B
parameter	I
explicit	O
,	O
then	O
the	O
error	B
function	I
(	O
3.12	O
)	O
becomes	O
(	O
3.16	O
)	O
n	O
(	O
cid:2	O
)	O
{	O
tn	O
−	O
w0	O
−	O
m−1	O
(	O
cid:2	O
)	O
w0	O
=	O
t	O
−	O
m−1	O
(	O
cid:2	O
)	O
wjφj	O
n=1	O
j=1	O
ed	O
(	O
w	O
)	O
=	O
1	O
2	O
wjφj	O
(	O
xn	O
)	O
}	O
2	O
.	O
(	O
3.18	O
)	O
setting	O
the	O
derivative	B
with	O
respect	O
to	O
w0	O
equal	O
to	O
zero	O
,	O
and	O
solving	O
for	O
w0	O
,	O
we	O
obtain	O
(	O
3.19	O
)	O
where	O
we	O
have	O
deﬁned	O
t	O
=	O
1	O
n	O
n	O
(	O
cid:2	O
)	O
n=1	O
tn	O
,	O
j=1	O
φj	O
=	O
1	O
n	O
n	O
(	O
cid:2	O
)	O
n=1	O
φj	O
(	O
xn	O
)	O
.	O
(	O
3.20	O
)	O
thus	O
the	O
bias	B
w0	O
compensates	O
for	O
the	O
difference	O
between	O
the	O
averages	O
(	O
over	O
the	O
training	B
set	I
)	O
of	O
the	O
target	O
values	O
and	O
the	O
weighted	O
sum	O
of	O
the	O
averages	O
of	O
the	O
basis	B
function	I
values	O
.	O
we	O
can	O
also	O
maximize	O
the	O
log	O
likelihood	O
function	O
(	O
3.11	O
)	O
with	O
respect	O
to	O
the	O
noise	O
precision	B
parameter	I
β	O
,	O
giving	O
1	O
βml	O
=	O
1	O
n	O
n	O
(	O
cid:2	O
)	O
n=1	O
{	O
tn	O
−	O
wt	O
mlφ	O
(	O
xn	O
)	O
}	O
2	O
(	O
3.21	O
)	O
3.1.	O
linear	O
basis	O
function	O
models	O
143	O
figure	O
3.2	O
geometrical	O
interpretation	O
of	O
the	O
least-squares	O
solution	O
,	O
in	O
an	O
n-dimensional	O
space	O
whose	O
axes	O
are	O
the	O
values	O
of	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
.	O
the	O
least-squares	O
regression	B
function	I
is	O
obtained	O
by	O
ﬁnding	O
the	O
or-	O
thogonal	O
projection	O
of	O
the	O
data	O
vector	O
t	O
onto	O
the	O
subspace	O
spanned	O
by	O
the	O
basis	O
functions	O
φj	O
(	O
x	O
)	O
in	O
which	O
each	O
basis	B
function	I
is	O
viewed	O
as	O
a	O
vec-	O
tor	O
ϕj	O
of	O
length	O
n	O
with	O
elements	O
φj	O
(	O
xn	O
)	O
.	O
s	O
t	O
ϕ1	O
y	O
ϕ2	O
and	O
so	O
we	O
see	O
that	O
the	O
inverse	B
of	O
the	O
noise	O
precision	O
is	O
given	O
by	O
the	O
residual	O
variance	B
of	O
the	O
target	O
values	O
around	O
the	O
regression	B
function	I
.	O
3.1.2	O
geometry	O
of	O
least	O
squares	O
at	O
this	O
point	O
,	O
it	O
is	O
instructive	O
to	O
consider	O
the	O
geometrical	O
interpretation	O
of	O
the	O
least-squares	O
solution	O
.	O
to	O
do	O
this	O
we	O
consider	O
an	O
n-dimensional	O
space	O
whose	O
axes	O
are	O
given	O
by	O
the	O
tn	O
,	O
so	O
that	O
t	O
=	O
(	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
)	O
t	O
is	O
a	O
vector	O
in	O
this	O
space	O
.	O
each	O
basis	B
function	I
φj	O
(	O
xn	O
)	O
,	O
evaluated	O
at	O
the	O
n	O
data	O
points	O
,	O
can	O
also	O
be	O
represented	O
as	O
a	O
vector	O
in	O
the	O
same	O
space	O
,	O
denoted	O
by	O
ϕj	O
,	O
as	O
illustrated	O
in	O
figure	O
3.2.	O
note	O
that	O
ϕj	O
corresponds	O
to	O
the	O
jth	O
column	O
of	O
φ	O
,	O
whereas	O
φ	O
(	O
xn	O
)	O
corresponds	O
to	O
the	O
nth	O
row	O
of	O
φ.	O
if	O
the	O
number	O
m	O
of	O
basis	O
functions	O
is	O
smaller	O
than	O
the	O
number	O
n	O
of	O
data	O
points	O
,	O
then	O
the	O
m	O
vectors	O
φj	O
(	O
xn	O
)	O
will	O
span	O
a	O
linear	O
subspace	O
s	O
of	O
dimensionality	O
m.	O
we	O
deﬁne	O
y	O
to	O
be	O
an	O
n-dimensional	O
vector	O
whose	O
nth	O
element	O
is	O
given	O
by	O
y	O
(	O
xn	O
,	O
w	O
)	O
,	O
where	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
because	O
y	O
is	O
an	O
arbitrary	O
linear	O
combination	O
of	O
the	O
vectors	O
ϕj	O
,	O
it	O
can	O
live	O
anywhere	O
in	O
the	O
m-dimensional	O
subspace	O
.	O
the	O
sum-of-squares	B
error	I
(	O
3.12	O
)	O
is	O
then	O
equal	O
(	O
up	O
to	O
a	O
factor	O
of	O
1/2	O
)	O
to	O
the	O
squared	O
euclidean	O
distance	O
between	O
y	O
and	O
t.	O
thus	O
the	O
least-squares	O
solution	O
for	O
w	O
corresponds	O
to	O
that	O
choice	O
of	O
y	O
that	O
lies	O
in	O
subspace	O
s	O
and	O
that	O
is	O
closest	O
to	O
t.	O
intuitively	O
,	O
from	O
figure	O
3.2	O
,	O
we	O
anticipate	O
that	O
this	O
solution	O
corresponds	O
to	O
the	O
orthogonal	O
projection	O
of	O
t	O
onto	O
the	O
subspace	O
s.	O
this	O
is	O
indeed	O
the	O
case	O
,	O
as	O
can	O
easily	O
be	O
veriﬁed	O
by	O
noting	O
that	O
the	O
solution	O
for	O
y	O
is	O
given	O
by	O
φwml	O
,	O
and	O
then	O
conﬁrming	O
that	O
this	O
takes	O
the	O
form	O
of	O
an	O
orthogonal	O
projection	O
.	O
in	O
practice	O
,	O
a	O
direct	O
solution	O
of	O
the	O
normal	B
equations	I
can	O
lead	O
to	O
numerical	O
difﬁ-	O
culties	O
when	O
φtφ	O
is	O
close	O
to	O
singular	O
.	O
in	O
particular	O
,	O
when	O
two	O
or	O
more	O
of	O
the	O
basis	O
vectors	O
ϕj	O
are	O
co-linear	O
,	O
or	O
nearly	O
so	O
,	O
the	O
resulting	O
parameter	O
values	O
can	O
have	O
large	O
magnitudes	O
.	O
such	O
near	O
degeneracies	O
will	O
not	O
be	O
uncommon	O
when	O
dealing	O
with	O
real	O
data	O
sets	O
.	O
the	O
resulting	O
numerical	O
difﬁculties	O
can	O
be	O
addressed	O
using	O
the	O
technique	O
of	O
singular	B
value	I
decomposition	I
,	O
or	O
svd	O
(	O
press	O
et	O
al.	O
,	O
1992	O
;	O
bishop	O
and	O
nabney	O
,	O
2008	O
)	O
.	O
note	O
that	O
the	O
addition	O
of	O
a	O
regularization	B
term	O
ensures	O
that	O
the	O
matrix	O
is	O
non-	O
singular	O
,	O
even	O
in	O
the	O
presence	O
of	O
degeneracies	O
.	O
3.1.3	O
sequential	B
learning	I
batch	O
techniques	O
,	O
such	O
as	O
the	O
maximum	B
likelihood	I
solution	O
(	O
3.15	O
)	O
,	O
which	O
in-	O
volve	O
processing	O
the	O
entire	O
training	B
set	I
in	O
one	O
go	O
,	O
can	O
be	O
computationally	O
costly	O
for	O
large	O
data	O
sets	O
.	O
as	O
we	O
have	O
discussed	O
in	O
chapter	O
1	O
,	O
if	O
the	O
data	O
set	O
is	O
sufﬁciently	O
large	O
,	O
it	O
may	O
be	O
worthwhile	O
to	O
use	O
sequential	O
algorithms	O
,	O
also	O
known	O
as	O
on-line	O
algorithms	O
,	O
exercise	O
3.2	O
144	O
3.	O
linear	O
models	O
for	B
regression	I
in	O
which	O
the	O
data	O
points	O
are	O
considered	O
one	O
at	O
a	O
time	O
,	O
and	O
the	O
model	O
parameters	O
up-	O
dated	O
after	O
each	O
such	O
presentation	O
.	O
sequential	B
learning	I
is	O
also	O
appropriate	O
for	O
real-	O
time	O
applications	O
in	O
which	O
the	O
data	O
observations	O
are	O
arriving	O
in	O
a	O
continuous	O
stream	O
,	O
and	O
predictions	O
must	O
be	O
made	O
before	O
all	O
of	O
the	O
data	O
points	O
are	O
seen	O
.	O
we	O
can	O
obtain	O
a	O
sequential	B
learning	I
algorithm	O
by	O
applying	O
the	O
technique	O
of	O
stochastic	B
gradient	I
descent	I
,	O
also	O
known	O
as	O
sequential	B
gradient	I
descent	I
,	O
as	O
follows	O
.	O
if	O
the	O
error	B
function	I
comprises	O
a	O
sum	O
over	O
data	O
points	O
e	O
=	O
n	O
en	O
,	O
then	O
after	O
presen-	O
tation	O
of	O
pattern	O
n	O
,	O
the	O
stochastic	B
gradient	I
descent	I
algorithm	O
updates	O
the	O
parameter	O
vector	O
w	O
using	O
(	O
cid:5	O
)	O
(	O
3.22	O
)	O
where	O
τ	O
denotes	O
the	O
iteration	O
number	O
,	O
and	O
η	O
is	O
a	O
learning	B
rate	I
parameter	I
.	O
we	O
shall	O
discuss	O
the	O
choice	O
of	O
value	O
for	O
η	O
shortly	O
.	O
the	O
value	O
of	O
w	O
is	O
initialized	O
to	O
some	O
starting	O
vector	O
w	O
(	O
0	O
)	O
.	O
for	O
the	O
case	O
of	O
the	O
sum-of-squares	B
error	I
function	O
(	O
3.12	O
)	O
,	O
this	O
gives	O
w	O
(	O
τ	O
+1	O
)	O
=	O
w	O
(	O
τ	O
)	O
+	O
η	O
(	O
tn	O
−	O
w	O
(	O
τ	O
)	O
tφn	O
)	O
φn	O
(	O
3.23	O
)	O
where	O
φn	O
=	O
φ	O
(	O
xn	O
)	O
.	O
this	O
is	O
known	O
as	O
least-mean-squares	O
or	O
the	O
lms	O
algorithm	O
.	O
the	O
value	O
of	O
η	O
needs	O
to	O
be	O
chosen	O
with	O
care	O
to	O
ensure	O
that	O
the	O
algorithm	O
converges	O
(	O
bishop	O
and	O
nabney	O
,	O
2008	O
)	O
.	O
w	O
(	O
τ	O
+1	O
)	O
=	O
w	O
(	O
τ	O
)	O
−	O
η∇en	O
3.1.4	O
regularized	B
least	I
squares	I
in	O
section	O
1.1	O
,	O
we	O
introduced	O
the	O
idea	O
of	O
adding	O
a	O
regularization	B
term	O
to	O
an	O
error	B
function	I
in	O
order	O
to	O
control	O
over-ﬁtting	B
,	O
so	O
that	O
the	O
total	O
error	B
function	I
to	O
be	O
minimized	O
takes	O
the	O
form	O
ed	O
(	O
w	O
)	O
+	O
λew	O
(	O
w	O
)	O
(	O
3.24	O
)	O
where	O
λ	O
is	O
the	O
regularization	B
coefﬁcient	O
that	O
controls	O
the	O
relative	B
importance	O
of	O
the	O
data-dependent	O
error	B
ed	O
(	O
w	O
)	O
and	O
the	O
regularization	B
term	O
ew	O
(	O
w	O
)	O
.	O
one	O
of	O
the	O
sim-	O
plest	O
forms	O
of	O
regularizer	O
is	O
given	O
by	O
the	O
sum-of-squares	O
of	O
the	O
weight	B
vector	I
ele-	O
ments	O
if	O
we	O
also	O
consider	O
the	O
sum-of-squares	B
error	I
function	O
given	O
by	O
ew	O
(	O
w	O
)	O
=	O
wtw	O
.	O
1	O
2	O
n	O
(	O
cid:2	O
)	O
n=1	O
e	O
(	O
w	O
)	O
=	O
1	O
2	O
{	O
tn	O
−	O
wtφ	O
(	O
xn	O
)	O
}	O
2	O
(	O
3.25	O
)	O
(	O
3.26	O
)	O
(	O
3.27	O
)	O
then	O
the	O
total	O
error	B
function	I
becomes	O
n	O
(	O
cid:2	O
)	O
n=1	O
1	O
2	O
{	O
tn	O
−	O
wtφ	O
(	O
xn	O
)	O
}	O
2	O
+	O
λ	O
2	O
wtw	O
.	O
this	O
particular	O
choice	O
of	O
regularizer	O
is	O
known	O
in	O
the	O
machine	O
learning	O
literature	O
as	O
weight	B
decay	I
because	O
in	O
sequential	B
learning	I
algorithms	O
,	O
it	O
encourages	O
weight	O
values	O
to	O
decay	O
towards	O
zero	O
,	O
unless	O
supported	O
by	O
the	O
data	O
.	O
in	O
statistics	O
,	O
it	O
provides	O
an	O
ex-	O
ample	O
of	O
a	O
parameter	B
shrinkage	I
method	O
because	O
it	O
shrinks	O
parameter	O
values	O
towards	O
3.1.	O
linear	O
basis	O
function	O
models	O
145	O
q	O
=	O
0.5	O
q	O
=	O
1	O
q	O
=	O
2	O
q	O
=	O
4	O
figure	O
3.3	O
contours	O
of	O
the	O
regularization	B
term	O
in	O
(	O
3.29	O
)	O
for	O
various	O
values	O
of	O
the	O
parameter	O
q.	O
zero	O
.	O
it	O
has	O
the	O
advantage	O
that	O
the	O
error	B
function	I
remains	O
a	O
quadratic	O
function	O
of	O
w	O
,	O
and	O
so	O
its	O
exact	O
minimizer	O
can	O
be	O
found	O
in	O
closed	O
form	O
.	O
speciﬁcally	O
,	O
setting	O
the	O
gradient	O
of	O
(	O
3.27	O
)	O
with	O
respect	O
to	O
w	O
to	O
zero	O
,	O
and	O
solving	O
for	O
w	O
as	O
before	O
,	O
we	O
obtain	O
w	O
=	O
λi	O
+	O
φtφ	O
(	O
3.28	O
)	O
this	O
represents	O
a	O
simple	O
extension	O
of	O
the	O
least-squares	O
solution	O
(	O
3.15	O
)	O
.	O
a	O
more	O
general	O
regularizer	O
is	O
sometimes	O
used	O
,	O
for	O
which	O
the	O
regularized	O
error	O
(	O
cid:11	O
)	O
−1	O
φtt	O
.	O
m	O
(	O
cid:2	O
)	O
exercise	O
3.5	O
appendix	O
e	O
(	O
cid:10	O
)	O
m	O
(	O
cid:2	O
)	O
takes	O
the	O
form	O
1	O
2	O
n	O
(	O
cid:2	O
)	O
n=1	O
{	O
tn	O
−	O
wtφ	O
(	O
xn	O
)	O
}	O
2	O
+	O
λ	O
2	O
j=1	O
|wj|q	O
(	O
3.29	O
)	O
where	O
q	O
=	O
2	O
corresponds	O
to	O
the	O
quadratic	O
regularizer	O
(	O
3.27	O
)	O
.	O
figure	O
3.3	O
shows	O
con-	O
tours	O
of	O
the	O
regularization	B
function	O
for	O
different	O
values	O
of	O
q.	O
the	O
case	O
of	O
q	O
=	O
1	O
is	O
know	O
as	O
the	O
lasso	B
in	O
the	O
statistics	O
literature	O
(	O
tibshirani	O
,	O
it	O
has	O
the	O
property	O
that	O
if	O
λ	O
is	O
sufﬁciently	O
large	O
,	O
some	O
of	O
the	O
coefﬁcients	O
1996	O
)	O
.	O
wj	O
are	O
driven	O
to	O
zero	O
,	O
leading	O
to	O
a	O
sparse	O
model	O
in	O
which	O
the	O
corresponding	O
basis	O
functions	O
play	O
no	O
role	O
.	O
to	O
see	O
this	O
,	O
we	O
ﬁrst	O
note	O
that	O
minimizing	O
(	O
3.29	O
)	O
is	O
equivalent	O
to	O
minimizing	O
the	O
unregularized	O
sum-of-squares	B
error	I
(	O
3.12	O
)	O
subject	O
to	O
the	O
constraint	O
|wj|q	O
(	O
cid:1	O
)	O
η	O
(	O
3.30	O
)	O
j=1	O
for	O
an	O
appropriate	O
value	O
of	O
the	O
parameter	O
η	O
,	O
where	O
the	O
two	O
approaches	O
can	O
be	O
related	O
using	O
lagrange	O
multipliers	O
.	O
the	O
origin	O
of	O
the	O
sparsity	B
can	O
be	O
seen	O
from	O
figure	O
3.4	O
,	O
which	O
shows	O
that	O
the	O
minimum	O
of	O
the	O
error	B
function	I
,	O
subject	O
to	O
the	O
constraint	O
(	O
3.30	O
)	O
.	O
as	O
λ	O
is	O
increased	O
,	O
so	O
an	O
increasing	O
number	O
of	O
parameters	O
are	O
driven	O
to	O
zero	O
.	O
regularization	B
allows	O
complex	O
models	O
to	O
be	O
trained	O
on	O
data	O
sets	O
of	O
limited	O
size	O
without	O
severe	O
over-ﬁtting	B
,	O
essentially	O
by	O
limiting	O
the	O
effective	O
model	O
complexity	O
.	O
however	O
,	O
the	O
problem	O
of	O
determining	O
the	O
optimal	O
model	O
complexity	O
is	O
then	O
shifted	O
from	O
one	O
of	O
ﬁnding	O
the	O
appropriate	O
number	O
of	O
basis	O
functions	O
to	O
one	O
of	O
determining	O
a	O
suitable	O
value	O
of	O
the	O
regularization	B
coefﬁcient	O
λ.	O
we	O
shall	O
return	O
to	O
the	O
issue	O
of	O
model	O
complexity	O
later	O
in	O
this	O
chapter	O
.	O
146	O
3.	O
linear	O
models	O
for	B
regression	I
figure	O
3.4	O
plot	O
of	O
the	O
contours	O
of	O
the	O
unregularized	O
error	B
function	I
(	O
blue	O
)	O
along	O
with	O
the	O
constraint	O
re-	O
gion	O
(	O
3.30	O
)	O
for	O
the	O
quadratic	O
regular-	O
izer	O
q	O
=	O
2	O
on	O
the	O
left	O
and	O
the	O
lasso	B
regularizer	O
q	O
=	O
1	O
on	O
the	O
right	O
,	O
in	O
which	O
the	O
optimum	O
value	O
for	O
the	O
pa-	O
rameter	O
vector	O
w	O
is	O
denoted	O
by	O
w	O
(	O
cid:1	O
)	O
.	O
the	O
lasso	B
gives	O
a	O
sparse	O
solution	O
in	O
which	O
w	O
(	O
cid:1	O
)	O
1	O
=	O
0.	O
w2	O
w2	O
w	O
(	O
cid:1	O
)	O
w	O
(	O
cid:1	O
)	O
w1	O
w1	O
for	O
the	O
remainder	O
of	O
this	O
chapter	O
we	O
shall	O
focus	O
on	O
the	O
quadratic	O
regularizer	O
(	O
3.27	O
)	O
both	O
for	O
its	O
practical	O
importance	O
and	O
its	O
analytical	O
tractability	O
.	O
3.1.5	O
multiple	O
outputs	O
so	O
far	O
,	O
we	O
have	O
considered	O
the	O
case	O
of	O
a	O
single	O
target	O
variable	O
t.	O
in	O
some	O
applica-	O
tions	O
,	O
we	O
may	O
wish	O
to	O
predict	O
k	O
>	O
1	O
target	O
variables	O
,	O
which	O
we	O
denote	O
collectively	O
by	O
the	O
target	B
vector	I
t.	O
this	O
could	O
be	O
done	O
by	O
introducing	O
a	O
different	O
set	O
of	O
basis	O
func-	O
tions	O
for	O
each	O
component	O
of	O
t	O
,	O
leading	O
to	O
multiple	O
,	O
independent	B
regression	O
problems	O
.	O
however	O
,	O
a	O
more	O
interesting	O
,	O
and	O
more	O
common	O
,	O
approach	O
is	O
to	O
use	O
the	O
same	O
set	O
of	O
basis	O
functions	O
to	O
model	O
all	O
of	O
the	O
components	O
of	O
the	O
target	B
vector	I
so	O
that	O
y	O
(	O
x	O
,	O
w	O
)	O
=	O
wtφ	O
(	O
x	O
)	O
(	O
3.31	O
)	O
where	O
y	O
is	O
a	O
k-dimensional	O
column	O
vector	O
,	O
w	O
is	O
an	O
m	O
×	O
k	O
matrix	O
of	O
parameters	O
,	O
and	O
φ	O
(	O
x	O
)	O
is	O
an	O
m-dimensional	O
column	O
vector	O
with	O
elements	O
φj	O
(	O
x	O
)	O
,	O
with	O
φ0	O
(	O
x	O
)	O
=	O
1	O
as	O
before	O
.	O
suppose	O
we	O
take	O
the	O
conditional	B
distribution	O
of	O
the	O
target	B
vector	I
to	O
be	O
an	O
isotropic	B
gaussian	O
of	O
the	O
form	O
p	O
(	O
t|x	O
,	O
w	O
,	O
β	O
)	O
=	O
n	O
(	O
t|wtφ	O
(	O
x	O
)	O
,	O
β	O
−1i	O
)	O
.	O
(	O
3.32	O
)	O
if	O
we	O
have	O
a	O
set	O
of	O
observations	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
,	O
we	O
can	O
combine	O
these	O
into	O
a	O
matrix	O
t	O
of	O
size	O
n	O
×	O
k	O
such	O
that	O
the	O
nth	O
row	O
is	O
given	O
by	O
tt	O
n.	O
similarly	O
,	O
we	O
can	O
combine	O
the	O
input	O
vectors	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
into	O
a	O
matrix	O
x.	O
the	O
log	O
likelihood	O
function	O
is	O
then	O
given	O
by	O
ln	O
p	O
(	O
t|x	O
,	O
w	O
,	O
β	O
)	O
=	O
lnn	O
(	O
tn|wtφ	O
(	O
xn	O
)	O
,	O
β	O
−1i	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
n=1	O
=	O
n	O
k	O
2	O
ln	O
β	O
2π	O
−	O
β	O
2	O
’	O
’	O
tn	O
−	O
wtφ	O
(	O
xn	O
)	O
’	O
’	O
2	O
n	O
(	O
cid:2	O
)	O
n=1	O
.	O
(	O
3.33	O
)	O
as	O
before	O
,	O
we	O
can	O
maximize	O
this	O
function	O
with	O
respect	O
to	O
w	O
,	O
giving	O
if	O
we	O
examine	O
this	O
result	O
for	O
each	O
target	O
variable	O
tk	O
,	O
we	O
have	O
wml	O
=	O
(	O
cid:10	O
)	O
wk	O
=	O
φtφ	O
3.2.	O
the	O
bias-variance	O
decomposition	O
147	O
(	O
cid:10	O
)	O
φtφ	O
(	O
cid:11	O
)	O
−1	O
φtt	O
.	O
(	O
cid:11	O
)	O
−1	O
φttk	O
=	O
φ	O
†	O
tk	O
(	O
3.34	O
)	O
(	O
3.35	O
)	O
exercise	O
3.6	O
where	O
tk	O
is	O
an	O
n-dimensional	O
column	O
vector	O
with	O
components	O
tnk	O
for	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
n.	O
thus	O
the	O
solution	O
to	O
the	O
regression	B
problem	O
decouples	O
between	O
the	O
different	O
target	O
†	O
variables	O
,	O
and	O
we	O
need	O
only	O
compute	O
a	O
single	O
pseudo-inverse	B
matrix	O
φ	O
,	O
which	O
is	O
shared	O
by	O
all	O
of	O
the	O
vectors	O
wk	O
.	O
the	O
extension	O
to	O
general	O
gaussian	O
noise	O
distributions	O
having	O
arbitrary	O
covari-	O
ance	O
matrices	O
is	O
straightforward	O
.	O
again	O
,	O
this	O
leads	O
to	O
a	O
decoupling	O
into	O
k	O
inde-	O
pendent	O
regression	B
problems	O
.	O
this	O
result	O
is	O
unsurprising	O
because	O
the	O
parameters	O
w	O
deﬁne	O
only	O
the	O
mean	B
of	O
the	O
gaussian	O
noise	O
distribution	O
,	O
and	O
we	O
know	O
from	O
sec-	O
tion	O
2.3.4	O
that	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
mean	B
of	O
a	O
multivariate	O
gaus-	O
sian	O
is	O
independent	B
of	O
the	O
covariance	B
.	O
from	O
now	O
on	O
,	O
we	O
shall	O
therefore	O
consider	O
a	O
single	O
target	O
variable	O
t	O
for	O
simplicity	O
.	O
3.2.	O
the	O
bias-variance	O
decomposition	O
so	O
far	O
in	O
our	O
discussion	O
of	O
linear	O
models	O
for	B
regression	I
,	O
we	O
have	O
assumed	O
that	O
the	O
form	O
and	O
number	O
of	O
basis	O
functions	O
are	O
both	O
ﬁxed	O
.	O
as	O
we	O
have	O
seen	O
in	O
chapter	O
1	O
,	O
the	O
use	O
of	O
maximum	B
likelihood	I
,	O
or	O
equivalently	O
least	O
squares	O
,	O
can	O
lead	O
to	O
severe	O
over-ﬁtting	B
if	O
complex	O
models	O
are	O
trained	O
using	O
data	O
sets	O
of	O
limited	O
size	O
.	O
however	O
,	O
limiting	O
the	O
number	O
of	O
basis	O
functions	O
in	O
order	O
to	O
avoid	O
over-ﬁtting	B
has	O
the	O
side	O
effect	O
of	O
limiting	O
the	O
ﬂexibility	O
of	O
the	O
model	O
to	O
capture	O
interesting	O
and	O
important	O
trends	O
in	O
the	O
data	O
.	O
although	O
the	O
introduction	O
of	O
regularization	B
terms	O
can	O
control	O
over-ﬁtting	B
for	O
models	O
with	O
many	O
parameters	O
,	O
this	O
raises	O
the	O
question	O
of	O
how	O
to	O
determine	O
a	O
suitable	O
value	O
for	O
the	O
regularization	B
coefﬁcient	O
λ.	O
seeking	O
the	O
solution	O
that	O
minimizes	O
the	O
regularized	O
error	O
function	O
with	O
respect	O
to	O
both	O
the	O
weight	B
vector	I
w	O
and	O
the	O
regularization	B
coefﬁcient	O
λ	O
is	O
clearly	O
not	O
the	O
right	O
approach	O
since	O
this	O
leads	O
to	O
the	O
unregularized	O
solution	O
with	O
λ	O
=	O
0.	O
as	O
we	O
have	O
seen	O
in	O
earlier	O
chapters	O
,	O
the	O
phenomenon	O
of	O
over-ﬁtting	B
is	O
really	O
an	O
unfortunate	O
property	O
of	O
maximum	B
likelihood	I
and	O
does	O
not	O
arise	O
when	O
we	O
marginalize	O
over	O
parameters	O
in	O
a	O
bayesian	O
setting	O
.	O
in	O
this	O
chapter	O
,	O
we	O
shall	O
consider	O
the	O
bayesian	O
view	O
of	O
model	O
complexity	O
in	O
some	O
depth	O
.	O
before	O
doing	O
so	O
,	O
however	O
,	O
it	O
is	O
instructive	O
to	O
consider	O
a	O
frequentist	B
viewpoint	O
of	O
the	O
model	O
complexity	O
issue	O
,	O
known	O
as	O
the	O
bias-	O
variance	B
trade-off	O
.	O
although	O
we	O
shall	O
introduce	O
this	O
concept	O
in	O
the	O
context	O
of	O
linear	O
basis	O
function	O
models	O
,	O
where	O
it	O
is	O
easy	O
to	O
illustrate	O
the	O
ideas	O
using	O
simple	O
examples	O
,	O
the	O
discussion	O
has	O
more	O
general	O
applicability	O
.	O
in	O
section	O
1.5.5	O
,	O
when	O
we	O
discussed	O
decision	B
theory	I
for	O
regression	B
problems	O
,	O
we	O
considered	O
various	O
loss	O
functions	O
each	O
of	O
which	O
leads	O
to	O
a	O
corresponding	O
optimal	O
prediction	O
once	O
we	O
are	O
given	O
the	O
conditional	B
distribution	O
p	O
(	O
t|x	O
)	O
.	O
a	O
popular	O
choice	O
is	O
148	O
3.	O
linear	O
models	O
for	B
regression	I
the	O
squared	O
loss	B
function	I
,	O
for	O
which	O
the	O
optimal	O
prediction	O
is	O
given	O
by	O
the	O
conditional	B
expectation	I
,	O
which	O
we	O
denote	O
by	O
h	O
(	O
x	O
)	O
and	O
which	O
is	O
given	O
by	O
h	O
(	O
x	O
)	O
=	O
e	O
[	O
t|x	O
]	O
=	O
tp	O
(	O
t|x	O
)	O
dt	O
.	O
(	O
3.36	O
)	O
at	O
this	O
point	O
,	O
it	O
is	O
worth	O
distinguishing	O
between	O
the	O
squared	O
loss	B
function	I
arising	O
from	O
decision	B
theory	I
and	O
the	O
sum-of-squares	B
error	I
function	O
that	O
arose	O
in	O
the	O
maxi-	O
mum	O
likelihood	O
estimation	O
of	O
model	O
parameters	O
.	O
we	O
might	O
use	O
more	O
sophisticated	O
techniques	O
than	O
least	O
squares	O
,	O
for	O
example	O
regularization	B
or	O
a	O
fully	O
bayesian	O
ap-	O
proach	O
,	O
to	O
determine	O
the	O
conditional	B
distribution	O
p	O
(	O
t|x	O
)	O
.	O
these	O
can	O
all	O
be	O
combined	O
with	O
the	O
squared	O
loss	B
function	I
for	O
the	O
purpose	O
of	O
making	O
predictions	O
.	O
we	O
showed	O
in	O
section	O
1.5.5	O
that	O
the	O
expected	O
squared	O
loss	O
can	O
be	O
written	O
in	O
the	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
form	O
e	O
[	O
l	O
]	O
=	O
{	O
y	O
(	O
x	O
)	O
−	O
h	O
(	O
x	O
)	O
}	O
2	O
p	O
(	O
x	O
)	O
dx	O
+	O
{	O
h	O
(	O
x	O
)	O
−	O
t	O
}	O
2p	O
(	O
x	O
,	O
t	O
)	O
dx	O
dt	O
.	O
(	O
3.37	O
)	O
recall	O
that	O
the	O
second	O
term	O
,	O
which	O
is	O
independent	B
of	O
y	O
(	O
x	O
)	O
,	O
arises	O
from	O
the	O
intrinsic	O
noise	O
on	O
the	O
data	O
and	O
represents	O
the	O
minimum	O
achievable	O
value	O
of	O
the	O
expected	O
loss	O
.	O
the	O
ﬁrst	O
term	O
depends	O
on	O
our	O
choice	O
for	O
the	O
function	O
y	O
(	O
x	O
)	O
,	O
and	O
we	O
will	O
seek	O
a	O
so-	O
lution	O
for	O
y	O
(	O
x	O
)	O
which	O
makes	O
this	O
term	O
a	O
minimum	O
.	O
because	O
it	O
is	O
nonnegative	O
,	O
the	O
smallest	O
that	O
we	O
can	O
hope	O
to	O
make	O
this	O
term	O
is	O
zero	O
.	O
if	O
we	O
had	O
an	O
unlimited	O
supply	O
of	O
data	O
(	O
and	O
unlimited	O
computational	O
resources	O
)	O
,	O
we	O
could	O
in	O
principle	O
ﬁnd	O
the	O
regres-	O
sion	B
function	O
h	O
(	O
x	O
)	O
to	O
any	O
desired	O
degree	O
of	O
accuracy	O
,	O
and	O
this	O
would	O
represent	O
the	O
optimal	O
choice	O
for	O
y	O
(	O
x	O
)	O
.	O
however	O
,	O
in	O
practice	O
we	O
have	O
a	O
data	O
set	O
d	O
containing	O
only	O
a	O
ﬁnite	O
number	O
n	O
of	O
data	O
points	O
,	O
and	O
consequently	O
we	O
do	O
not	O
know	O
the	O
regression	B
function	I
h	O
(	O
x	O
)	O
exactly	O
.	O
if	O
we	O
model	O
the	O
h	O
(	O
x	O
)	O
using	O
a	O
parametric	O
function	O
y	O
(	O
x	O
,	O
w	O
)	O
governed	O
by	O
a	O
pa-	O
rameter	O
vector	O
w	O
,	O
then	O
from	O
a	O
bayesian	O
perspective	O
the	O
uncertainty	O
in	O
our	O
model	O
is	O
expressed	O
through	O
a	O
posterior	O
distribution	O
over	O
w.	O
a	O
frequentist	B
treatment	O
,	O
however	O
,	O
involves	O
making	O
a	O
point	O
estimate	O
of	O
w	O
based	O
on	O
the	O
data	O
set	O
d	O
,	O
and	O
tries	O
instead	O
to	O
interpret	O
the	O
uncertainty	O
of	O
this	O
estimate	O
through	O
the	O
following	O
thought	O
experi-	O
ment	O
.	O
suppose	O
we	O
had	O
a	O
large	O
number	O
of	O
data	O
sets	O
each	O
of	O
size	O
n	O
and	O
each	O
drawn	O
independently	O
from	O
the	O
distribution	O
p	O
(	O
t	O
,	O
x	O
)	O
.	O
for	O
any	O
given	O
data	O
set	O
d	O
,	O
we	O
can	O
run	O
our	O
learning	B
algorithm	O
and	O
obtain	O
a	O
prediction	O
function	O
y	O
(	O
x	O
;	O
d	O
)	O
.	O
different	O
data	O
sets	O
from	O
the	O
ensemble	O
will	O
give	O
different	O
functions	O
and	O
consequently	O
different	O
values	O
of	O
the	O
squared	O
loss	O
.	O
the	O
performance	O
of	O
a	O
particular	O
learning	B
algorithm	O
is	O
then	O
assessed	O
by	O
taking	O
the	O
average	O
over	O
this	O
ensemble	O
of	O
data	O
sets	O
.	O
d	O
takes	O
the	O
form	O
(	O
3.38	O
)	O
because	O
this	O
quantity	O
will	O
be	O
dependent	O
on	O
the	O
particular	O
data	O
set	O
d	O
,	O
we	O
take	O
its	O
aver-	O
age	O
over	O
the	O
ensemble	O
of	O
data	O
sets	O
.	O
if	O
we	O
add	O
and	O
subtract	O
the	O
quantity	O
ed	O
[	O
y	O
(	O
x	O
;	O
d	O
)	O
]	O
consider	O
the	O
integrand	O
of	O
the	O
ﬁrst	O
term	O
in	O
(	O
3.37	O
)	O
,	O
which	O
for	O
a	O
particular	O
data	O
set	O
{	O
y	O
(	O
x	O
;	O
d	O
)	O
−	O
h	O
(	O
x	O
)	O
}	O
2	O
.	O
3.2.	O
the	O
bias-variance	O
decomposition	O
149	O
inside	O
the	O
braces	O
,	O
and	O
then	O
expand	O
,	O
we	O
obtain	O
{	O
y	O
(	O
x	O
;	O
d	O
)	O
−	O
ed	O
[	O
y	O
(	O
x	O
;	O
d	O
)	O
]	O
+	O
ed	O
[	O
y	O
(	O
x	O
;	O
d	O
)	O
]	O
−	O
h	O
(	O
x	O
)	O
}	O
2	O
=	O
{	O
y	O
(	O
x	O
;	O
d	O
)	O
−	O
ed	O
[	O
y	O
(	O
x	O
;	O
d	O
)	O
]	O
}	O
2	O
+	O
{	O
ed	O
[	O
y	O
(	O
x	O
;	O
d	O
)	O
]	O
−	O
h	O
(	O
x	O
)	O
}	O
2	O
+2	O
{	O
y	O
(	O
x	O
;	O
d	O
)	O
−	O
ed	O
[	O
y	O
(	O
x	O
;	O
d	O
)	O
]	O
}	O
{	O
ed	O
[	O
y	O
(	O
x	O
;	O
d	O
)	O
]	O
−	O
h	O
(	O
x	O
)	O
}	O
.	O
(	O
3.39	O
)	O
we	O
now	O
take	O
the	O
expectation	B
of	O
this	O
expression	O
with	O
respect	O
to	O
d	O
and	O
note	O
that	O
the	O
ﬁnal	O
term	O
will	O
vanish	O
,	O
giving	O
(	O
cid:8	O
)	O
{	O
y	O
(	O
x	O
;	O
d	O
)	O
−	O
h	O
(	O
x	O
)	O
}	O
2	O
(	O
)	O
*	O
ed	O
=	O
{	O
ed	O
[	O
y	O
(	O
x	O
;	O
d	O
)	O
]	O
−	O
h	O
(	O
x	O
)	O
}	O
2	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
{	O
y	O
(	O
x	O
;	O
d	O
)	O
−	O
ed	O
[	O
y	O
(	O
x	O
;	O
d	O
)	O
]	O
}	O
2	O
)	O
*	O
(	O
cid:9	O
)	O
+	O
+	O
(	O
+	O
ed	O
.	O
(	O
3.40	O
)	O
(	O
bias	B
)	O
2	O
variance	B
we	O
see	O
that	O
the	O
expected	O
squared	O
difference	O
between	O
y	O
(	O
x	O
;	O
d	O
)	O
and	O
the	O
regression	B
function	I
h	O
(	O
x	O
)	O
can	O
be	O
expressed	O
as	O
the	O
sum	O
of	O
two	O
terms	O
.	O
the	O
ﬁrst	O
term	O
,	O
called	O
the	O
squared	O
bias	B
,	O
represents	O
the	O
extent	O
to	O
which	O
the	O
average	O
prediction	O
over	O
all	O
data	O
sets	O
differs	O
from	O
the	O
desired	O
regression	B
function	I
.	O
the	O
second	O
term	O
,	O
called	O
the	O
variance	B
,	O
measures	O
the	O
extent	O
to	O
which	O
the	O
solutions	O
for	O
individual	O
data	O
sets	O
vary	O
around	O
their	O
average	O
,	O
and	O
hence	O
this	O
measures	O
the	O
extent	O
to	O
which	O
the	O
function	O
y	O
(	O
x	O
;	O
d	O
)	O
is	O
sensitive	O
to	O
the	O
particular	O
choice	O
of	O
data	O
set	O
.	O
we	O
shall	O
provide	O
some	O
intuition	O
to	O
support	O
these	O
deﬁnitions	O
shortly	O
when	O
we	O
consider	O
a	O
simple	O
example	O
.	O
so	O
far	O
,	O
we	O
have	O
considered	O
a	O
single	O
input	O
value	O
x.	O
if	O
we	O
substitute	O
this	O
expansion	O
back	O
into	O
(	O
3.37	O
)	O
,	O
we	O
obtain	O
the	O
following	O
decomposition	O
of	O
the	O
expected	O
squared	O
loss	O
expected	O
loss	O
=	O
(	O
bias	B
)	O
2	O
+	O
variance	B
+	O
noise	O
where	O
(	O
bias	B
)	O
2	O
=	O
variance	B
=	O
noise	O
=	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
{	O
ed	O
[	O
y	O
(	O
x	O
;	O
d	O
)	O
]	O
−	O
h	O
(	O
x	O
)	O
}	O
2p	O
(	O
x	O
)	O
dx	O
(	O
cid:8	O
)	O
{	O
y	O
(	O
x	O
;	O
d	O
)	O
−	O
ed	O
[	O
y	O
(	O
x	O
;	O
d	O
)	O
]	O
}	O
2	O
ed	O
{	O
h	O
(	O
x	O
)	O
−	O
t	O
}	O
2p	O
(	O
x	O
,	O
t	O
)	O
dx	O
dt	O
(	O
cid:9	O
)	O
(	O
3.41	O
)	O
(	O
3.42	O
)	O
p	O
(	O
x	O
)	O
dx	O
(	O
3.43	O
)	O
(	O
3.44	O
)	O
and	O
the	O
bias	B
and	O
variance	B
terms	O
now	O
refer	O
to	O
integrated	O
quantities	O
.	O
our	O
goal	O
is	O
to	O
minimize	O
the	O
expected	O
loss	O
,	O
which	O
we	O
have	O
decomposed	O
into	O
the	O
sum	O
of	O
a	O
(	O
squared	O
)	O
bias	B
,	O
a	O
variance	B
,	O
and	O
a	O
constant	O
noise	O
term	O
.	O
as	O
we	O
shall	O
see	O
,	O
there	O
is	O
a	O
trade-off	O
between	O
bias	B
and	O
variance	B
,	O
with	O
very	O
ﬂexible	O
models	O
having	O
low	O
bias	B
and	O
high	O
variance	B
,	O
and	O
relatively	O
rigid	O
models	O
having	O
high	O
bias	B
and	O
low	O
variance	B
.	O
the	O
model	O
with	O
the	O
optimal	O
predictive	O
capability	O
is	O
the	O
one	O
that	O
leads	O
to	O
the	O
best	O
balance	O
between	O
bias	B
and	O
variance	B
.	O
this	O
is	O
illustrated	O
by	O
considering	O
the	O
sinusoidal	B
data	I
set	O
from	O
chapter	O
1.	O
here	O
we	O
generate	O
100	O
data	O
sets	O
,	O
each	O
containing	O
n	O
=	O
25	O
data	O
points	O
,	O
independently	O
from	O
the	O
sinusoidal	O
curve	O
h	O
(	O
x	O
)	O
=	O
sin	O
(	O
2πx	O
)	O
.	O
the	O
data	O
sets	O
are	O
indexed	O
by	O
l	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
l	O
,	O
where	O
l	O
=	O
100	O
,	O
and	O
for	O
each	O
data	O
set	O
d	O
(	O
l	O
)	O
we	O
appendix	O
a	O
150	O
3.	O
linear	O
models	O
for	B
regression	I
t	O
1	O
0	O
−1	O
t	O
1	O
0	O
−1	O
t	O
1	O
0	O
−1	O
0	O
0	O
0	O
ln	O
λ	O
=	O
2.6	O
t	O
1	O
0	O
−1	O
x	O
1	O
0	O
x	O
1	O
ln	O
λ	O
=	O
−0.31	O
t	O
1	O
0	O
−1	O
x	O
1	O
0	O
x	O
1	O
ln	O
λ	O
=	O
−2.4	O
t	O
1	O
0	O
−1	O
x	O
1	O
0	O
x	O
1	O
figure	O
3.5	O
illustration	O
of	O
the	O
dependence	O
of	O
bias	B
and	O
variance	B
on	O
model	O
complexity	O
,	O
governed	O
by	O
a	O
regulariza-	O
tion	O
parameter	O
λ	O
,	O
using	O
the	O
sinusoidal	B
data	I
set	O
from	O
chapter	O
1.	O
there	O
are	O
l	O
=	O
100	O
data	O
sets	O
,	O
each	O
having	O
n	O
=	O
25	O
data	O
points	O
,	O
and	O
there	O
are	O
24	O
gaussian	O
basis	O
functions	O
in	O
the	O
model	O
so	O
that	O
the	O
total	O
number	O
of	O
parameters	O
is	O
m	O
=	O
25	O
including	O
the	O
bias	B
parameter	I
.	O
the	O
left	O
column	O
shows	O
the	O
result	O
of	O
ﬁtting	O
the	O
model	O
to	O
the	O
data	O
sets	O
for	O
various	O
values	O
of	O
ln	O
λ	O
(	O
for	O
clarity	O
,	O
only	O
20	O
of	O
the	O
100	O
ﬁts	O
are	O
shown	O
)	O
.	O
the	O
right	O
column	O
shows	O
the	O
corresponding	O
average	O
of	O
the	O
100	O
ﬁts	O
(	O
red	O
)	O
along	O
with	O
the	O
sinusoidal	O
function	O
from	O
which	O
the	O
data	O
sets	O
were	O
generated	O
(	O
green	O
)	O
.	O
3.2.	O
the	O
bias-variance	O
decomposition	O
151	O
figure	O
3.6	O
plot	O
of	O
squared	O
bias	B
and	O
variance	B
,	O
together	O
with	O
their	O
sum	O
,	O
correspond-	O
ing	O
to	O
the	O
results	O
shown	O
in	O
fig-	O
ure	O
3.5.	O
also	O
shown	O
is	O
the	O
average	O
test	B
set	I
error	O
for	O
a	O
test	O
data	O
set	O
size	O
of	O
1000	O
points	O
.	O
the	O
minimum	O
value	O
of	O
(	O
bias	B
)	O
2	O
+	O
variance	B
occurs	O
around	O
ln	O
λ	O
=	O
−0.31	O
,	O
which	O
is	O
close	O
to	O
the	O
value	O
that	O
gives	O
the	O
minimum	O
error	O
on	O
the	O
test	O
data	O
.	O
0.15	O
0.12	O
0.09	O
0.06	O
0.03	O
0	O
−3	O
(	O
bias	B
)	O
2	O
variance	B
(	O
bias	B
)	O
2	O
+	O
variance	B
test	O
error	B
−2	O
−1	O
0	O
1	O
2	O
ln	O
λ	O
ﬁt	O
a	O
model	O
with	O
24	O
gaussian	O
basis	O
functions	O
by	O
minimizing	O
the	O
regularized	O
error	O
function	O
(	O
3.27	O
)	O
to	O
give	O
a	O
prediction	O
function	O
y	O
(	O
l	O
)	O
(	O
x	O
)	O
as	O
shown	O
in	O
figure	O
3.5.	O
the	O
top	O
row	O
corresponds	O
to	O
a	O
large	O
value	O
of	O
the	O
regularization	B
coefﬁcient	O
λ	O
that	O
gives	O
low	O
variance	B
(	O
because	O
the	O
red	O
curves	O
in	O
the	O
left	O
plot	O
look	O
similar	O
)	O
but	O
high	O
bias	B
(	O
because	O
the	O
two	O
curves	O
in	O
the	O
right	O
plot	O
are	O
very	O
different	O
)	O
.	O
conversely	O
on	O
the	O
bottom	O
row	O
,	O
for	O
which	O
λ	O
is	O
small	O
,	O
there	O
is	O
large	O
variance	O
(	O
shown	O
by	O
the	O
high	O
variability	O
between	O
the	O
red	O
curves	O
in	O
the	O
left	O
plot	O
)	O
but	O
low	O
bias	B
(	O
shown	O
by	O
the	O
good	O
ﬁt	O
between	O
the	O
average	O
model	O
ﬁt	O
and	O
the	O
original	O
sinusoidal	O
function	O
)	O
.	O
note	O
that	O
the	O
result	O
of	O
averaging	O
many	O
solutions	O
for	O
the	O
complex	O
model	O
with	O
m	O
=	O
25	O
is	O
a	O
very	O
good	O
ﬁt	O
to	O
the	O
regression	B
function	I
,	O
which	O
suggests	O
that	O
averaging	O
may	O
be	O
a	O
beneﬁcial	O
procedure	O
.	O
indeed	O
,	O
a	O
weighted	O
averaging	O
of	O
multiple	O
solutions	O
lies	O
at	O
the	O
heart	O
of	O
a	O
bayesian	O
approach	O
,	O
although	O
the	O
averaging	O
is	O
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
of	O
parameters	O
,	O
not	O
with	O
respect	O
to	O
multiple	O
data	O
sets	O
.	O
we	O
can	O
also	O
examine	O
the	O
bias-variance	B
trade-off	I
quantitatively	O
for	O
this	O
example	O
.	O
the	O
average	O
prediction	O
is	O
estimated	O
from	O
y	O
(	O
x	O
)	O
=	O
1	O
l	O
l	O
(	O
cid:2	O
)	O
l=1	O
y	O
(	O
l	O
)	O
(	O
x	O
)	O
and	O
the	O
integrated	O
squared	O
bias	B
and	O
integrated	O
variance	B
are	O
then	O
given	O
by	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
l	O
(	O
cid:2	O
)	O
(	O
cid:26	O
)	O
1	O
l	O
n=1	O
l=1	O
(	O
bias	B
)	O
2	O
=	O
variance	B
=	O
1	O
n	O
1	O
n	O
{	O
y	O
(	O
xn	O
)	O
−	O
h	O
(	O
xn	O
)	O
}	O
2	O
(	O
cid:27	O
)	O
2	O
y	O
(	O
l	O
)	O
(	O
xn	O
)	O
−	O
y	O
(	O
xn	O
)	O
(	O
3.45	O
)	O
(	O
3.46	O
)	O
(	O
3.47	O
)	O
where	O
the	O
integral	O
over	O
x	O
weighted	O
by	O
the	O
distribution	O
p	O
(	O
x	O
)	O
is	O
approximated	O
by	O
a	O
ﬁnite	O
sum	O
over	O
data	O
points	O
drawn	O
from	O
that	O
distribution	O
.	O
these	O
quantities	O
,	O
along	O
with	O
their	O
sum	O
,	O
are	O
plotted	O
as	O
a	O
function	O
of	O
ln	O
λ	O
in	O
figure	O
3.6.	O
we	O
see	O
that	O
small	O
values	O
of	O
λ	O
allow	O
the	O
model	O
to	O
become	O
ﬁnely	O
tuned	O
to	O
the	O
noise	O
on	O
each	O
individual	O
152	O
3.	O
linear	O
models	O
for	B
regression	I
data	O
set	O
leading	O
to	O
large	O
variance	O
.	O
conversely	O
,	O
a	O
large	O
value	O
of	O
λ	O
pulls	O
the	O
weight	O
parameters	O
towards	O
zero	O
leading	O
to	O
large	O
bias	O
.	O
although	O
the	O
bias-variance	O
decomposition	O
may	O
provide	O
some	O
interesting	O
in-	O
sights	O
into	O
the	O
model	O
complexity	O
issue	O
from	O
a	O
frequentist	B
perspective	O
,	O
it	O
is	O
of	O
lim-	O
ited	O
practical	O
value	O
,	O
because	O
the	O
bias-variance	O
decomposition	O
is	O
based	O
on	O
averages	O
with	O
respect	O
to	O
ensembles	O
of	O
data	O
sets	O
,	O
whereas	O
in	O
practice	O
we	O
have	O
only	O
the	O
single	O
observed	O
data	O
set	O
.	O
if	O
we	O
had	O
a	O
large	O
number	O
of	O
independent	B
training	O
sets	O
of	O
a	O
given	O
size	O
,	O
we	O
would	O
be	O
better	O
off	O
combining	O
them	O
into	O
a	O
single	O
large	O
training	O
set	O
,	O
which	O
of	O
course	O
would	O
reduce	O
the	O
level	O
of	O
over-ﬁtting	B
for	O
a	O
given	O
model	O
complexity	O
.	O
given	O
these	O
limitations	O
,	O
we	O
turn	O
in	O
the	O
next	O
section	O
to	O
a	O
bayesian	O
treatment	O
of	O
linear	O
basis	O
function	O
models	O
,	O
which	O
not	O
only	O
provides	O
powerful	O
insights	O
into	O
the	O
issues	O
of	O
over-ﬁtting	B
but	O
which	O
also	O
leads	O
to	O
practical	O
techniques	O
for	O
addressing	O
the	O
question	O
model	O
complexity	O
.	O
3.3.	O
bayesian	O
linear	B
regression	I
in	O
our	O
discussion	O
of	O
maximum	B
likelihood	I
for	O
setting	O
the	O
parameters	O
of	O
a	O
linear	O
re-	O
gression	O
model	O
,	O
we	O
have	O
seen	O
that	O
the	O
effective	O
model	O
complexity	O
,	O
governed	O
by	O
the	O
number	O
of	O
basis	O
functions	O
,	O
needs	O
to	O
be	O
controlled	O
according	O
to	O
the	O
size	O
of	O
the	O
data	O
set	O
.	O
adding	O
a	O
regularization	B
term	O
to	O
the	O
log	O
likelihood	O
function	O
means	O
the	O
effective	O
model	O
complexity	O
can	O
then	O
be	O
controlled	O
by	O
the	O
value	O
of	O
the	O
regularization	B
coefﬁ-	O
cient	O
,	O
although	O
the	O
choice	O
of	O
the	O
number	O
and	O
form	O
of	O
the	O
basis	O
functions	O
is	O
of	O
course	O
still	O
important	O
in	O
determining	O
the	O
overall	O
behaviour	O
of	O
the	O
model	O
.	O
this	O
leaves	O
the	O
issue	O
of	O
deciding	O
the	O
appropriate	O
model	O
complexity	O
for	O
the	O
par-	O
ticular	O
problem	O
,	O
which	O
can	O
not	O
be	O
decided	O
simply	O
by	O
maximizing	O
the	O
likelihood	O
func-	O
tion	O
,	O
because	O
this	O
always	O
leads	O
to	O
excessively	O
complex	O
models	O
and	O
over-ﬁtting	B
.	O
in-	O
dependent	O
hold-out	O
data	O
can	O
be	O
used	O
to	O
determine	O
model	O
complexity	O
,	O
as	O
discussed	O
in	O
section	O
1.3	O
,	O
but	O
this	O
can	O
be	O
both	O
computationally	O
expensive	O
and	O
wasteful	O
of	O
valu-	O
able	O
data	O
.	O
we	O
therefore	O
turn	O
to	O
a	O
bayesian	O
treatment	O
of	O
linear	B
regression	I
,	O
which	O
will	O
avoid	O
the	O
over-ﬁtting	B
problem	O
of	O
maximum	B
likelihood	I
,	O
and	O
which	O
will	O
also	O
lead	O
to	O
automatic	O
methods	O
of	O
determining	O
model	O
complexity	O
using	O
the	O
training	B
data	O
alone	O
.	O
again	O
,	O
for	O
simplicity	O
we	O
will	O
focus	O
on	O
the	O
case	O
of	O
a	O
single	O
target	O
variable	O
t.	O
ex-	O
tension	O
to	O
multiple	O
target	O
variables	O
is	O
straightforward	O
and	O
follows	O
the	O
discussion	O
of	O
section	O
3.1.5	O
.	O
3.3.1	O
parameter	O
distribution	O
we	O
begin	O
our	O
discussion	O
of	O
the	O
bayesian	O
treatment	O
of	O
linear	B
regression	I
by	O
in-	O
troducing	O
a	O
prior	B
probability	O
distribution	O
over	O
the	O
model	O
parameters	O
w.	O
for	O
the	O
mo-	O
ment	O
,	O
we	O
shall	O
treat	O
the	O
noise	O
precision	B
parameter	I
β	O
as	O
a	O
known	O
constant	O
.	O
first	O
note	O
that	O
the	O
likelihood	B
function	I
p	O
(	O
t|w	O
)	O
deﬁned	O
by	O
(	O
3.10	O
)	O
is	O
the	O
exponential	O
of	O
a	O
quadratic	O
function	O
of	O
w.	O
the	O
corresponding	O
conjugate	B
prior	I
is	O
therefore	O
given	O
by	O
a	O
gaussian	O
distribution	O
of	O
the	O
form	O
p	O
(	O
w	O
)	O
=	O
n	O
(	O
w|m0	O
,	O
s0	O
)	O
(	O
3.48	O
)	O
having	O
mean	B
m0	O
and	O
covariance	B
s0	O
.	O
exercise	O
3.7	O
exercise	O
3.8	O
3.3.	O
bayesian	O
linear	B
regression	I
153	O
next	O
we	O
compute	O
the	O
posterior	O
distribution	O
,	O
which	O
is	O
proportional	O
to	O
the	O
product	O
of	O
the	O
likelihood	B
function	I
and	O
the	O
prior	B
.	O
due	O
to	O
the	O
choice	O
of	O
a	O
conjugate	B
gaus-	O
sian	O
prior	B
distribution	O
,	O
the	O
posterior	O
will	O
also	O
be	O
gaussian	O
.	O
we	O
can	O
evaluate	O
this	O
distribution	O
by	O
the	O
usual	O
procedure	O
of	O
completing	B
the	I
square	I
in	O
the	O
exponential	O
,	O
and	O
then	O
ﬁnding	O
the	O
normalization	O
coefﬁcient	O
using	O
the	O
standard	O
result	O
for	O
a	O
normalized	O
gaussian	O
.	O
however	O
,	O
we	O
have	O
already	O
done	O
the	O
necessary	O
work	O
in	O
deriving	O
the	O
gen-	O
eral	O
result	O
(	O
2.116	O
)	O
,	O
which	O
allows	O
us	O
to	O
write	O
down	O
the	O
posterior	O
distribution	O
directly	O
in	O
the	O
form	O
p	O
(	O
w|t	O
)	O
=	O
n	O
(	O
w|mn	O
,	O
sn	O
)	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
mn	O
=	O
sn	O
−1	O
−1	O
0	O
+	O
βφtφ	O
.	O
n	O
=	O
s	O
s	O
s	O
−1	O
0	O
m0	O
+	O
βφtt	O
(	O
3.49	O
)	O
(	O
3.50	O
)	O
(	O
3.51	O
)	O
where	O
note	O
that	O
because	O
the	O
posterior	O
distribution	O
is	O
gaussian	O
,	O
its	O
mode	O
coincides	O
with	O
its	O
mean	B
.	O
thus	O
the	O
maximum	B
posterior	I
weight	O
vector	O
is	O
simply	O
given	O
by	O
wmap	O
=	O
mn	O
.	O
−1i	O
with	O
α	O
→	O
0	O
,	O
the	O
mean	B
mn	O
if	O
we	O
consider	O
an	O
inﬁnitely	O
broad	O
prior	B
s0	O
=	O
α	O
of	O
the	O
posterior	O
distribution	O
reduces	O
to	O
the	O
maximum	B
likelihood	I
value	O
wml	O
given	O
by	O
(	O
3.15	O
)	O
.	O
similarly	O
,	O
if	O
n	O
=	O
0	O
,	O
then	O
the	O
posterior	O
distribution	O
reverts	O
to	O
the	O
prior	B
.	O
furthermore	O
,	O
if	O
data	O
points	O
arrive	O
sequentially	O
,	O
then	O
the	O
posterior	O
distribution	O
at	O
any	O
stage	O
acts	O
as	O
the	O
prior	B
distribution	O
for	O
the	O
subsequent	O
data	O
point	O
,	O
such	O
that	O
the	O
new	O
posterior	O
distribution	O
is	O
again	O
given	O
by	O
(	O
3.49	O
)	O
.	O
for	O
the	O
remainder	O
of	O
this	O
chapter	O
,	O
we	O
shall	O
consider	O
a	O
particular	O
form	O
of	O
gaus-	O
sian	O
prior	B
in	O
order	O
to	O
simplify	O
the	O
treatment	O
.	O
speciﬁcally	O
,	O
we	O
consider	O
a	O
zero-mean	O
isotropic	B
gaussian	O
governed	O
by	O
a	O
single	O
precision	B
parameter	I
α	O
so	O
that	O
p	O
(	O
w|α	O
)	O
=	O
n	O
(	O
w|0	O
,	O
α	O
−1i	O
)	O
(	O
3.52	O
)	O
and	O
the	O
corresponding	O
posterior	O
distribution	O
over	O
w	O
is	O
then	O
given	O
by	O
(	O
3.49	O
)	O
with	O
mn	O
=	O
βsn	O
φtt	O
−1	O
n	O
=	O
αi	O
+	O
βφtφ	O
.	O
s	O
(	O
3.53	O
)	O
(	O
3.54	O
)	O
the	O
log	O
of	O
the	O
posterior	O
distribution	O
is	O
given	O
by	O
the	O
sum	O
of	O
the	O
log	O
likelihood	O
and	O
the	O
log	O
of	O
the	O
prior	B
and	O
,	O
as	O
a	O
function	O
of	O
w	O
,	O
takes	O
the	O
form	O
ln	O
p	O
(	O
w|t	O
)	O
=	O
−	O
β	O
2	O
{	O
tn	O
−	O
wtφ	O
(	O
xn	O
)	O
}	O
2	O
−	O
α	O
2	O
wtw	O
+	O
const	O
.	O
(	O
3.55	O
)	O
maximization	O
of	O
this	O
posterior	O
distribution	O
with	O
respect	O
to	O
w	O
is	O
therefore	O
equiva-	O
lent	O
to	O
the	O
minimization	O
of	O
the	O
sum-of-squares	B
error	I
function	O
with	O
the	O
addition	O
of	O
a	O
quadratic	O
regularization	O
term	O
,	O
corresponding	O
to	O
(	O
3.27	O
)	O
with	O
λ	O
=	O
α/β	O
.	O
we	O
can	O
illustrate	O
bayesian	O
learning	B
in	O
a	O
linear	O
basis	O
function	O
model	O
,	O
as	O
well	O
as	O
the	O
sequential	O
update	O
of	O
a	O
posterior	O
distribution	O
,	O
using	O
a	O
simple	O
example	O
involving	O
straight-line	O
ﬁtting	O
.	O
consider	O
a	O
single	O
input	O
variable	O
x	O
,	O
a	O
single	O
target	O
variable	O
t	O
and	O
n	O
(	O
cid:2	O
)	O
n=1	O
154	O
3.	O
linear	O
models	O
for	B
regression	I
a	O
linear	O
model	O
of	O
the	O
form	O
y	O
(	O
x	O
,	O
w	O
)	O
=	O
w0	O
+	O
w1x	O
.	O
because	O
this	O
has	O
just	O
two	O
adap-	O
tive	O
parameters	O
,	O
we	O
can	O
plot	O
the	O
prior	B
and	O
posterior	O
distributions	O
directly	O
in	O
parameter	O
space	O
.	O
we	O
generate	O
synthetic	O
data	O
from	O
the	O
function	O
f	O
(	O
x	O
,	O
a	O
)	O
=	O
a0	O
+	O
a1x	O
with	O
param-	O
eter	O
values	O
a0	O
=	O
−0.3	O
and	O
a1	O
=	O
0.5	O
by	O
ﬁrst	O
choosing	O
values	O
of	O
xn	O
from	O
the	O
uniform	B
distribution	I
u	O
(	O
x|−1	O
,	O
1	O
)	O
,	O
then	O
evaluating	O
f	O
(	O
xn	O
,	O
a	O
)	O
,	O
and	O
ﬁnally	O
adding	O
gaussian	O
noise	O
with	O
standard	B
deviation	I
of	O
0.2	O
to	O
obtain	O
the	O
target	O
values	O
tn	O
.	O
our	O
goal	O
is	O
to	O
recover	O
the	O
values	O
of	O
a0	O
and	O
a1	O
from	O
such	O
data	O
,	O
and	O
we	O
will	O
explore	O
the	O
dependence	O
on	O
the	O
size	O
of	O
the	O
data	O
set	O
.	O
we	O
assume	O
here	O
that	O
the	O
noise	O
variance	B
is	O
known	O
and	O
hence	O
we	O
set	O
the	O
precision	B
parameter	I
to	O
its	O
true	O
value	O
β	O
=	O
(	O
1/0.2	O
)	O
2	O
=	O
25.	O
similarly	O
,	O
we	O
ﬁx	O
the	O
parameter	O
α	O
to	O
2.0.	O
we	O
shall	O
shortly	O
discuss	O
strategies	O
for	O
determining	O
α	O
and	O
β	O
from	O
the	O
training	B
data	O
.	O
figure	O
3.7	O
shows	O
the	O
results	O
of	O
bayesian	O
learning	B
in	O
this	O
model	O
as	O
the	O
size	O
of	O
the	O
data	O
set	O
is	O
increased	O
and	O
demonstrates	O
the	O
sequential	O
nature	O
of	O
bayesian	O
learning	B
in	O
which	O
the	O
current	O
posterior	O
distribution	O
forms	O
the	O
prior	B
when	O
a	O
new	O
data	O
point	O
is	O
observed	O
.	O
it	O
is	O
worth	O
taking	O
time	O
to	O
study	O
this	O
ﬁgure	O
in	O
detail	O
as	O
it	O
illustrates	O
several	O
important	O
aspects	O
of	O
bayesian	O
inference	B
.	O
the	O
ﬁrst	O
row	O
of	O
this	O
ﬁgure	O
corresponds	O
to	O
the	O
situation	O
before	O
any	O
data	O
points	O
are	O
observed	O
and	O
shows	O
a	O
plot	O
of	O
the	O
prior	B
distribution	O
in	O
w	O
space	O
together	O
with	O
six	O
samples	O
of	O
the	O
function	O
y	O
(	O
x	O
,	O
w	O
)	O
in	O
which	O
the	O
values	O
of	O
w	O
are	O
drawn	O
from	O
the	O
prior	B
.	O
in	O
the	O
second	O
row	O
,	O
we	O
see	O
the	O
situation	O
after	O
observing	O
a	O
single	O
data	O
point	O
.	O
the	O
location	O
(	O
x	O
,	O
t	O
)	O
of	O
the	O
data	O
point	O
is	O
shown	O
by	O
a	O
blue	O
circle	O
in	O
the	O
right-hand	O
column	O
.	O
in	O
the	O
left-hand	O
column	O
is	O
a	O
plot	O
of	O
the	O
likelihood	B
function	I
p	O
(	O
t|x	O
,	O
w	O
)	O
for	O
this	O
data	O
point	O
as	O
a	O
function	O
of	O
w.	O
note	O
that	O
the	O
likelihood	B
function	I
provides	O
a	O
soft	B
constraint	O
that	O
the	O
line	O
must	O
pass	O
close	O
to	O
the	O
data	O
point	O
,	O
where	O
close	O
is	O
determined	O
by	O
the	O
noise	O
precision	O
β.	O
for	O
comparison	O
,	O
the	O
true	O
parameter	O
values	O
a0	O
=	O
−0.3	O
and	O
a1	O
=	O
0.5	O
used	O
to	O
generate	O
the	O
data	O
set	O
are	O
shown	O
by	O
a	O
white	O
cross	O
in	O
the	O
plots	O
in	O
the	O
left	O
column	O
of	O
figure	O
3.7.	O
when	O
we	O
multiply	O
this	O
likelihood	B
function	I
by	O
the	O
prior	B
from	O
the	O
top	O
row	O
,	O
and	O
normalize	O
,	O
we	O
obtain	O
the	O
posterior	O
distribution	O
shown	O
in	O
the	O
middle	O
plot	O
on	O
the	O
second	O
row	O
.	O
sam-	O
ples	O
of	O
the	O
regression	B
function	I
y	O
(	O
x	O
,	O
w	O
)	O
obtained	O
by	O
drawing	O
samples	O
of	O
w	O
from	O
this	O
posterior	O
distribution	O
are	O
shown	O
in	O
the	O
right-hand	O
plot	O
.	O
note	O
that	O
these	O
sample	O
lines	O
all	O
pass	O
close	O
to	O
the	O
data	O
point	O
.	O
the	O
third	O
row	O
of	O
this	O
ﬁgure	O
shows	O
the	O
effect	O
of	O
ob-	O
serving	O
a	O
second	O
data	O
point	O
,	O
again	O
shown	O
by	O
a	O
blue	O
circle	O
in	O
the	O
plot	O
in	O
the	O
right-hand	O
column	O
.	O
the	O
corresponding	O
likelihood	B
function	I
for	O
this	O
second	O
data	O
point	O
alone	O
is	O
shown	O
in	O
the	O
left	O
plot	O
.	O
when	O
we	O
multiply	O
this	O
likelihood	B
function	I
by	O
the	O
posterior	O
distribution	O
from	O
the	O
second	O
row	O
,	O
we	O
obtain	O
the	O
posterior	O
distribution	O
shown	O
in	O
the	O
middle	O
plot	O
of	O
the	O
third	O
row	O
.	O
note	O
that	O
this	O
is	O
exactly	O
the	O
same	O
posterior	O
distribution	O
as	O
would	O
be	O
obtained	O
by	O
combining	O
the	O
original	O
prior	B
with	O
the	O
likelihood	B
function	I
for	O
the	O
two	O
data	O
points	O
.	O
this	O
posterior	O
has	O
now	O
been	O
inﬂuenced	O
by	O
two	O
data	O
points	O
,	O
and	O
because	O
two	O
points	O
are	O
sufﬁcient	O
to	O
deﬁne	O
a	O
line	O
this	O
already	O
gives	O
a	O
relatively	O
compact	O
posterior	O
distribution	O
.	O
samples	O
from	O
this	O
posterior	O
distribution	O
give	O
rise	O
to	O
the	O
functions	O
shown	O
in	O
red	O
in	O
the	O
third	O
column	O
,	O
and	O
we	O
see	O
that	O
these	O
functions	O
pass	O
close	O
to	O
both	O
of	O
the	O
data	O
points	O
.	O
the	O
fourth	O
row	O
shows	O
the	O
effect	O
of	O
observing	O
a	O
total	O
of	O
20	O
data	O
points	O
.	O
the	O
left-hand	O
plot	O
shows	O
the	O
likelihood	B
function	I
for	O
the	O
20th	O
data	O
point	O
alone	O
,	O
and	O
the	O
middle	O
plot	O
shows	O
the	O
resulting	O
posterior	O
distribution	O
that	O
has	O
now	O
absorbed	O
information	O
from	O
all	O
20	O
observations	O
.	O
note	O
how	O
the	O
posterior	O
is	O
much	O
sharper	O
than	O
in	O
the	O
third	O
row	O
.	O
in	O
the	O
limit	O
of	O
an	O
inﬁnite	O
number	O
of	O
data	O
points	O
,	O
the	O
3.3.	O
bayesian	O
linear	B
regression	I
155	O
figure	O
3.7	O
illustration	O
of	O
sequential	O
bayesian	O
learning	B
for	O
a	O
simple	O
linear	O
model	O
of	O
the	O
form	O
y	O
(	O
x	O
,	O
w	O
)	O
=	O
w0	O
+	O
w1x	O
.	O
a	O
detailed	O
description	O
of	O
this	O
ﬁgure	O
is	O
given	O
in	O
the	O
text	O
.	O
156	O
3.	O
linear	O
models	O
for	B
regression	I
posterior	O
distribution	O
would	O
become	O
a	O
delta	O
function	O
centred	O
on	O
the	O
true	O
parameter	O
values	O
,	O
shown	O
by	O
the	O
white	O
cross	O
.	O
other	O
forms	O
of	O
prior	B
over	O
the	O
parameters	O
can	O
be	O
considered	O
.	O
for	O
instance	O
,	O
we	O
can	O
generalize	O
the	O
gaussian	O
prior	B
to	O
give	O
(	O
cid:29	O
)	O
(	O
cid:17	O
)	O
(	O
cid:18	O
)	O
1/q	O
(	O
cid:30	O
)	O
m	O
(	O
cid:22	O
)	O
p	O
(	O
w|α	O
)	O
=	O
q	O
2	O
α	O
2	O
1	O
γ	O
(	O
1/q	O
)	O
exp	O
−	O
α	O
2	O
|wj|q	O
(	O
3.56	O
)	O
(	O
cid:23	O
)	O
m	O
(	O
cid:2	O
)	O
j=1	O
in	O
which	O
q	O
=	O
2	O
corresponds	O
to	O
the	O
gaussian	O
distribution	O
,	O
and	O
only	O
in	O
this	O
case	O
is	O
the	O
prior	B
conjugate	O
to	O
the	O
likelihood	B
function	I
(	O
3.10	O
)	O
.	O
finding	O
the	O
maximum	O
of	O
the	O
poste-	O
rior	O
distribution	O
over	O
w	O
corresponds	O
to	O
minimization	O
of	O
the	O
regularized	O
error	O
function	O
(	O
3.29	O
)	O
.	O
in	O
the	O
case	O
of	O
the	O
gaussian	O
prior	B
,	O
the	O
mode	O
of	O
the	O
posterior	O
distribution	O
was	O
equal	O
to	O
the	O
mean	B
,	O
although	O
this	O
will	O
no	O
longer	O
hold	O
if	O
q	O
(	O
cid:9	O
)	O
=	O
2	O
.	O
3.3.2	O
predictive	B
distribution	I
in	O
practice	O
,	O
we	O
are	O
not	O
usually	O
interested	O
in	O
the	O
value	O
of	O
w	O
itself	O
but	O
rather	O
in	O
making	O
predictions	O
of	O
t	O
for	O
new	O
values	O
of	O
x.	O
this	O
requires	O
that	O
we	O
evaluate	O
the	O
predictive	B
distribution	I
deﬁned	O
by	O
p	O
(	O
t|t	O
,	O
α	O
,	O
β	O
)	O
=	O
p	O
(	O
t|w	O
,	O
β	O
)	O
p	O
(	O
w|t	O
,	O
α	O
,	O
β	O
)	O
dw	O
(	O
3.57	O
)	O
(	O
cid:6	O
)	O
exercise	O
3.10	O
exercise	O
3.11	O
in	O
which	O
t	O
is	O
the	O
vector	O
of	O
target	O
values	O
from	O
the	O
training	B
set	I
,	O
and	O
we	O
have	O
omitted	O
the	O
corresponding	O
input	O
vectors	O
from	O
the	O
right-hand	O
side	O
of	O
the	O
conditioning	O
statements	O
to	O
simplify	O
the	O
notation	O
.	O
the	O
conditional	B
distribution	O
p	O
(	O
t|x	O
,	O
w	O
,	O
β	O
)	O
of	O
the	O
target	O
vari-	O
able	O
is	O
given	O
by	O
(	O
3.8	O
)	O
,	O
and	O
the	O
posterior	O
weight	O
distribution	O
is	O
given	O
by	O
(	O
3.49	O
)	O
.	O
we	O
see	O
that	O
(	O
3.57	O
)	O
involves	O
the	O
convolution	O
of	O
two	O
gaussian	O
distributions	O
,	O
and	O
so	O
making	O
use	O
of	O
the	O
result	O
(	O
2.115	O
)	O
from	O
section	O
8.1.4	O
,	O
we	O
see	O
that	O
the	O
predictive	B
distribution	I
takes	O
the	O
form	O
where	O
the	O
variance	B
σ2	O
p	O
(	O
t|x	O
,	O
t	O
,	O
α	O
,	O
β	O
)	O
=	O
n	O
(	O
t|mt	O
n	O
(	O
x	O
)	O
of	O
the	O
predictive	B
distribution	I
is	O
given	O
by	O
n	O
φ	O
(	O
x	O
)	O
,	O
σ2	O
n	O
(	O
x	O
)	O
)	O
(	O
3.58	O
)	O
(	O
3.59	O
)	O
σ2	O
n	O
(	O
x	O
)	O
=	O
1	O
β	O
+	O
φ	O
(	O
x	O
)	O
tsn	O
φ	O
(	O
x	O
)	O
.	O
the	O
ﬁrst	O
term	O
in	O
(	O
3.59	O
)	O
represents	O
the	O
noise	O
on	O
the	O
data	O
whereas	O
the	O
second	O
term	O
reﬂects	O
the	O
uncertainty	O
associated	O
with	O
the	O
parameters	O
w.	O
because	O
the	O
noise	O
process	O
and	O
the	O
distribution	O
of	O
w	O
are	O
independent	B
gaussians	O
,	O
their	O
variances	O
are	O
additive	O
.	O
note	O
that	O
,	O
as	O
additional	O
data	O
points	O
are	O
observed	O
,	O
the	O
posterior	O
distribution	O
becomes	O
n	O
+1	O
(	O
x	O
)	O
(	O
cid:1	O
)	O
narrower	O
.	O
as	O
a	O
consequence	O
it	O
can	O
be	O
shown	O
(	O
qazaz	O
et	O
al.	O
,	O
1997	O
)	O
that	O
σ2	O
n	O
(	O
x	O
)	O
.	O
in	O
the	O
limit	O
n	O
→	O
∞	O
,	O
the	O
second	O
term	O
in	O
(	O
3.59	O
)	O
goes	O
to	O
zero	O
,	O
and	O
the	O
variance	B
σ2	O
of	O
the	O
predictive	B
distribution	I
arises	O
solely	O
from	O
the	O
additive	O
noise	O
governed	O
by	O
the	O
parameter	O
β.	O
as	O
an	O
illustration	O
of	O
the	O
predictive	B
distribution	I
for	O
bayesian	O
linear	B
regression	I
models	O
,	O
let	O
us	O
return	O
to	O
the	O
synthetic	O
sinusoidal	O
data	O
set	O
of	O
section	O
1.1.	O
in	O
figure	O
3.8	O
,	O
t	O
1	O
0	O
−1	O
t	O
1	O
0	O
−1	O
0	O
0	O
3.3.	O
bayesian	O
linear	B
regression	I
157	O
t	O
1	O
0	O
−1	O
x	O
1	O
0	O
x	O
1	O
t	O
1	O
0	O
−1	O
x	O
1	O
0	O
x	O
1	O
figure	O
3.8	O
examples	O
of	O
the	O
predictive	B
distribution	I
(	O
3.58	O
)	O
for	O
a	O
model	O
consisting	O
of	O
9	O
gaussian	O
basis	O
functions	O
of	O
the	O
form	O
(	O
3.4	O
)	O
using	O
the	O
synthetic	O
sinusoidal	O
data	O
set	O
of	O
section	O
1.1.	O
see	O
the	O
text	O
for	O
a	O
detailed	O
discussion	O
.	O
we	O
ﬁt	O
a	O
model	O
comprising	O
a	O
linear	O
combination	O
of	O
gaussian	O
basis	O
functions	O
to	O
data	O
sets	O
of	O
various	O
sizes	O
and	O
then	O
look	O
at	O
the	O
corresponding	O
posterior	O
distributions	O
.	O
here	O
the	O
green	O
curves	O
correspond	O
to	O
the	O
function	O
sin	O
(	O
2πx	O
)	O
from	O
which	O
the	O
data	O
points	O
were	O
generated	O
(	O
with	O
the	O
addition	O
of	O
gaussian	O
noise	O
)	O
.	O
data	O
sets	O
of	O
size	O
n	O
=	O
1	O
,	O
n	O
=	O
2	O
,	O
n	O
=	O
4	O
,	O
and	O
n	O
=	O
25	O
are	O
shown	O
in	O
the	O
four	O
plots	O
by	O
the	O
blue	O
circles	O
.	O
for	O
each	O
plot	O
,	O
the	O
red	O
curve	O
shows	O
the	O
mean	B
of	O
the	O
corresponding	O
gaussian	O
predictive	B
distribution	I
,	O
and	O
the	O
red	O
shaded	O
region	O
spans	O
one	O
standard	B
deviation	I
either	O
side	O
of	O
the	O
mean	B
.	O
note	O
that	O
the	O
predictive	O
uncertainty	O
depends	O
on	O
x	O
and	O
is	O
smallest	O
in	O
the	O
neighbourhood	O
of	O
the	O
data	O
points	O
.	O
also	O
note	O
that	O
the	O
level	O
of	O
uncertainty	O
decreases	O
as	O
more	O
data	O
points	O
are	O
observed	O
.	O
the	O
plots	O
in	O
figure	O
3.8	O
only	O
show	O
the	O
point-wise	O
predictive	O
variance	O
as	O
a	O
func-	O
tion	O
of	O
x.	O
in	O
order	O
to	O
gain	O
insight	O
into	O
the	O
covariance	B
between	O
the	O
predictions	O
at	O
different	O
values	O
of	O
x	O
,	O
we	O
can	O
draw	O
samples	O
from	O
the	O
posterior	O
distribution	O
over	O
w	O
,	O
and	O
then	O
plot	O
the	O
corresponding	O
functions	O
y	O
(	O
x	O
,	O
w	O
)	O
,	O
as	O
shown	O
in	O
figure	O
3.9	O
.	O
158	O
3.	O
linear	O
models	O
for	B
regression	I
t	O
1	O
0	O
−1	O
t	O
1	O
0	O
−1	O
0	O
0	O
t	O
1	O
0	O
−1	O
x	O
1	O
0	O
x	O
1	O
t	O
1	O
0	O
−1	O
x	O
1	O
0	O
x	O
1	O
figure	O
3.9	O
plots	O
of	O
the	O
function	O
y	O
(	O
x	O
,	O
w	O
)	O
using	O
samples	O
from	O
the	O
posterior	O
distributions	O
over	O
w	O
corresponding	O
to	O
the	O
plots	O
in	O
figure	O
3.8.	O
if	O
we	O
used	O
localized	O
basis	O
functions	O
such	O
as	O
gaussians	O
,	O
then	O
in	O
regions	O
away	O
from	O
the	O
basis	B
function	I
centres	O
,	O
the	O
contribution	O
from	O
the	O
second	O
term	O
in	O
the	O
predic-	O
−1	O
.	O
thus	O
,	O
tive	O
variance	B
(	O
3.59	O
)	O
will	O
go	O
to	O
zero	O
,	O
leaving	O
only	O
the	O
noise	O
contribution	O
β	O
the	O
model	O
becomes	O
very	O
conﬁdent	O
in	O
its	O
predictions	O
when	O
extrapolating	O
outside	O
the	O
region	O
occupied	O
by	O
the	O
basis	O
functions	O
,	O
which	O
is	O
generally	O
an	O
undesirable	O
behaviour	O
.	O
this	O
problem	O
can	O
be	O
avoided	O
by	O
adopting	O
an	O
alternative	O
bayesian	O
approach	O
to	O
re-	O
gression	O
known	O
as	O
a	O
gaussian	O
process	O
.	O
note	O
that	O
,	O
if	O
both	O
w	O
and	O
β	O
are	O
treated	O
as	O
unknown	O
,	O
then	O
we	O
can	O
introduce	O
a	O
conjugate	B
prior	I
distribution	O
p	O
(	O
w	O
,	O
β	O
)	O
that	O
,	O
from	O
the	O
discussion	O
in	O
section	O
2.3.6	O
,	O
will	O
be	O
given	O
by	O
a	O
gaussian-gamma	O
distribution	O
(	O
denison	O
et	O
al.	O
,	O
2002	O
)	O
.	O
in	O
this	O
case	O
,	O
the	O
predictive	B
distribution	I
is	O
a	O
student	O
’	O
s	O
t-distribution	O
.	O
section	O
6.4	O
exercise	O
3.12	O
exercise	O
3.13	O
3.3.	O
bayesian	O
linear	B
regression	I
159	O
figure	O
3.10	O
the	O
equivalent	O
ker-	O
nel	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:1	O
)	O
)	O
for	O
the	O
gaussian	O
basis	O
functions	O
in	O
figure	O
3.1	O
,	O
shown	O
as	O
a	O
plot	O
of	O
x	O
versus	O
x	O
(	O
cid:1	O
)	O
,	O
together	O
with	O
three	O
slices	O
through	O
this	O
matrix	O
cor-	O
responding	O
to	O
three	O
different	O
values	O
of	O
x.	O
the	O
data	O
set	O
used	O
to	O
generate	O
this	O
kernel	O
comprised	O
200	O
values	O
of	O
x	O
equally	O
spaced	O
over	O
the	O
interval	O
(	O
−1	O
,	O
1	O
)	O
.	O
3.3.3	O
equivalent	B
kernel	I
the	O
posterior	O
mean	O
solution	O
(	O
3.53	O
)	O
for	O
the	O
linear	O
basis	O
function	O
model	O
has	O
an	O
in-	O
teresting	O
interpretation	O
that	O
will	O
set	O
the	O
stage	O
for	O
kernel	O
methods	O
,	O
including	O
gaussian	O
processes	O
.	O
if	O
we	O
substitute	O
(	O
3.53	O
)	O
into	O
the	O
expression	O
(	O
3.3	O
)	O
,	O
we	O
see	O
that	O
the	O
predictive	O
mean	O
can	O
be	O
written	O
in	O
the	O
form	O
chapter	O
6	O
y	O
(	O
x	O
,	O
mn	O
)	O
=	O
mt	O
n	O
φ	O
(	O
x	O
)	O
=	O
βφ	O
(	O
x	O
)	O
tsn	O
φtt	O
=	O
βφ	O
(	O
x	O
)	O
tsn	O
φ	O
(	O
xn	O
)	O
tn	O
(	O
3.60	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
where	O
sn	O
is	O
deﬁned	O
by	O
(	O
3.51	O
)	O
.	O
thus	O
the	O
mean	B
of	O
the	O
predictive	B
distribution	I
at	O
a	O
point	O
x	O
is	O
given	O
by	O
a	O
linear	O
combination	O
of	O
the	O
training	B
set	I
target	O
variables	O
tn	O
,	O
so	O
that	O
we	O
can	O
write	O
n	O
(	O
cid:2	O
)	O
y	O
(	O
x	O
,	O
mn	O
)	O
=	O
k	O
(	O
x	O
,	O
xn	O
)	O
tn	O
where	O
the	O
function	O
n=1	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
βφ	O
(	O
x	O
)	O
tsn	O
φ	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
(	O
3.61	O
)	O
(	O
3.62	O
)	O
is	O
known	O
as	O
the	O
smoother	B
matrix	I
or	O
the	O
equivalent	B
kernel	I
.	O
regression	B
functions	O
,	O
such	O
as	O
this	O
,	O
which	O
make	O
predictions	O
by	O
taking	O
linear	O
combinations	O
of	O
the	O
training	B
set	I
target	O
values	O
are	O
known	O
as	O
linear	O
smoothers	O
.	O
note	O
that	O
the	O
equivalent	B
kernel	I
depends	O
on	O
the	O
input	O
values	O
xn	O
from	O
the	O
data	O
set	O
because	O
these	O
appear	O
in	O
the	O
deﬁnition	O
of	O
sn	O
.	O
the	O
equivalent	B
kernel	I
is	O
illustrated	O
for	O
the	O
case	O
of	O
gaussian	O
basis	O
functions	O
in	O
(	O
cid:4	O
)	O
)	O
have	O
been	O
plotted	O
as	O
a	O
function	O
of	O
figure	O
3.10	O
in	O
which	O
the	O
kernel	O
functions	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
for	O
three	O
different	O
values	O
of	O
x.	O
we	O
see	O
that	O
they	O
are	O
localized	O
around	O
x	O
,	O
and	O
so	O
the	O
x	O
mean	B
of	O
the	O
predictive	B
distribution	I
at	O
x	O
,	O
given	O
by	O
y	O
(	O
x	O
,	O
mn	O
)	O
,	O
is	O
obtained	O
by	O
forming	O
a	O
weighted	O
combination	O
of	O
the	O
target	O
values	O
in	O
which	O
data	O
points	O
close	O
to	O
x	O
are	O
given	O
higher	O
weight	O
than	O
points	O
further	O
removed	O
from	O
x.	O
intuitively	O
,	O
it	O
seems	O
reasonable	O
that	O
we	O
should	O
weight	O
local	O
evidence	O
more	O
strongly	O
than	O
distant	O
evidence	O
.	O
note	O
that	O
this	O
localization	O
property	O
holds	O
not	O
only	O
for	O
the	O
localized	O
gaussian	O
basis	O
functions	O
but	O
also	O
for	O
the	O
nonlocal	O
polynomial	O
and	O
sigmoidal	O
basis	O
functions	O
,	O
as	O
illustrated	O
in	O
figure	O
3.11	O
.	O
160	O
3.	O
linear	O
models	O
for	B
regression	I
figure	O
3.11	O
examples	O
of	O
equiva-	O
lent	O
kernels	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:1	O
)	O
)	O
for	O
x	O
=	O
0	O
plotted	O
as	O
a	O
function	O
of	O
x	O
(	O
cid:1	O
)	O
,	O
corre-	O
sponding	O
(	O
left	O
)	O
to	O
the	O
polynomial	O
ba-	O
sis	O
functions	O
and	O
(	O
right	O
)	O
to	O
the	O
sig-	O
moidal	O
basis	O
functions	O
shown	O
in	O
fig-	O
ure	O
3.1.	O
note	O
that	O
these	O
are	O
local-	O
ized	O
functions	O
of	O
x	O
(	O
cid:1	O
)	O
even	O
though	O
the	O
corresponding	O
basis	O
functions	O
are	O
nonlocal	O
.	O
0.04	O
0.02	O
0	O
−1	O
0.04	O
0.02	O
0	O
−1	O
0	O
1	O
0	O
1	O
further	O
insight	O
into	O
the	O
role	O
of	O
the	O
equivalent	B
kernel	I
can	O
be	O
obtained	O
by	O
consid-	O
ering	O
the	O
covariance	B
between	O
y	O
(	O
x	O
)	O
and	O
y	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
,	O
which	O
is	O
given	O
by	O
cov	O
[	O
y	O
(	O
x	O
)	O
,	O
y	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
]	O
=	O
cov	O
[	O
φ	O
(	O
x	O
)	O
tw	O
,	O
wtφ	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
]	O
=	O
φ	O
(	O
x	O
)	O
tsn	O
φ	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
β	O
−1k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
(	O
3.63	O
)	O
where	O
we	O
have	O
made	O
use	O
of	O
(	O
3.49	O
)	O
and	O
(	O
3.62	O
)	O
.	O
from	O
the	O
form	O
of	O
the	O
equivalent	B
kernel	I
,	O
we	O
see	O
that	O
the	O
predictive	O
mean	O
at	O
nearby	O
points	O
will	O
be	O
highly	O
correlated	O
,	O
whereas	O
for	O
more	O
distant	O
pairs	O
of	O
points	O
the	O
correlation	O
will	O
be	O
smaller	O
.	O
the	O
predictive	B
distribution	I
shown	O
in	O
figure	O
3.8	O
allows	O
us	O
to	O
visualize	O
the	O
point-	O
wise	O
uncertainty	O
in	O
the	O
predictions	O
,	O
governed	O
by	O
(	O
3.59	O
)	O
.	O
however	O
,	O
by	O
drawing	O
sam-	O
ples	O
from	O
the	O
posterior	O
distribution	O
over	O
w	O
,	O
and	O
plotting	O
the	O
corresponding	O
model	O
functions	O
y	O
(	O
x	O
,	O
w	O
)	O
as	O
in	O
figure	O
3.9	O
,	O
we	O
are	O
visualizing	O
the	O
joint	O
uncertainty	O
in	O
the	O
posterior	O
distribution	O
between	O
the	O
y	O
values	O
at	O
two	O
(	O
or	O
more	O
)	O
x	O
values	O
,	O
as	O
governed	O
by	O
the	O
equivalent	B
kernel	I
.	O
the	O
formulation	O
of	O
linear	B
regression	I
in	O
terms	O
of	O
a	O
kernel	B
function	I
suggests	O
an	O
alternative	O
approach	O
to	O
regression	B
as	O
follows	O
.	O
instead	O
of	O
introducing	O
a	O
set	O
of	O
basis	O
functions	O
,	O
which	O
implicitly	O
determines	O
an	O
equivalent	B
kernel	I
,	O
we	O
can	O
instead	O
deﬁne	O
a	O
localized	O
kernel	O
directly	O
and	O
use	O
this	O
to	O
make	O
predictions	O
for	O
new	O
input	O
vectors	O
x	O
,	O
given	O
the	O
observed	O
training	O
set	O
.	O
this	O
leads	O
to	O
a	O
practical	O
framework	O
for	B
regression	I
(	O
and	O
classiﬁcation	B
)	O
called	O
gaussian	O
processes	O
,	O
which	O
will	O
be	O
discussed	O
in	O
detail	O
in	O
section	O
6.4.	O
we	O
have	O
seen	O
that	O
the	O
effective	O
kernel	O
deﬁnes	O
the	O
weights	O
by	O
which	O
the	O
training	B
set	I
target	O
values	O
are	O
combined	O
in	O
order	O
to	O
make	O
a	O
prediction	O
at	O
a	O
new	O
value	O
of	O
x	O
,	O
and	O
it	O
can	O
be	O
shown	O
that	O
these	O
weights	O
sum	O
to	O
one	O
,	O
in	O
other	O
words	O
n	O
(	O
cid:2	O
)	O
k	O
(	O
x	O
,	O
xn	O
)	O
=	O
1	O
(	O
3.64	O
)	O
exercise	O
3.14	O
n=1	O
by	O
noting	O
that	O
the	O
summation	O
is	O
equivalent	O
to	O
considering	O
the	O
predictive	O
mean	O
(	O
cid:1	O
)	O
y	O
(	O
x	O
)	O
for	O
all	O
values	O
of	O
x.	O
this	O
intuitively	O
pleasing	O
result	O
can	O
easily	O
be	O
proven	O
informally	O
for	O
a	O
set	O
of	O
target	O
data	O
in	O
which	O
tn	O
=	O
1	O
for	O
all	O
n.	O
provided	O
the	O
basis	O
functions	O
are	O
linearly	O
independent	O
,	O
that	O
there	O
are	O
more	O
data	O
points	O
than	O
basis	O
functions	O
,	O
and	O
that	O
one	O
of	O
the	O
basis	O
functions	O
is	O
constant	O
(	O
corresponding	O
to	O
the	O
bias	B
parameter	I
)	O
,	O
then	O
it	O
is	O
clear	O
that	O
we	O
can	O
ﬁt	O
the	O
training	B
data	O
exactly	O
and	O
hence	O
that	O
the	O
predictive	O
mean	O
will	O
be	O
simply	O
(	O
cid:1	O
)	O
y	O
(	O
x	O
)	O
=	O
1	O
,	O
from	O
which	O
we	O
obtain	O
(	O
3.64	O
)	O
.	O
note	O
that	O
the	O
kernel	B
function	I
can	O
3.4.	O
bayesian	O
model	B
comparison	I
161	O
chapter	O
6	O
be	O
negative	O
as	O
well	O
as	O
positive	O
,	O
so	O
although	O
it	O
satisﬁes	O
a	O
summation	O
constraint	O
,	O
the	O
corresponding	O
predictions	O
are	O
not	O
necessarily	O
convex	O
combinations	O
of	O
the	O
training	B
set	I
target	O
variables	O
.	O
finally	O
,	O
we	O
note	O
that	O
the	O
equivalent	B
kernel	I
(	O
3.62	O
)	O
satisﬁes	O
an	O
important	O
property	O
shared	O
by	O
kernel	O
functions	O
in	O
general	O
,	O
namely	O
that	O
it	O
can	O
be	O
expressed	O
in	O
the	O
form	O
an	O
inner	O
product	O
with	O
respect	O
to	O
a	O
vector	O
ψ	O
(	O
x	O
)	O
of	O
nonlinear	O
functions	O
,	O
so	O
that	O
k	O
(	O
x	O
,	O
z	O
)	O
=	O
ψ	O
(	O
x	O
)	O
tψ	O
(	O
z	O
)	O
(	O
3.65	O
)	O
where	O
ψ	O
(	O
x	O
)	O
=	O
β1/2s1/2	O
n	O
φ	O
(	O
x	O
)	O
.	O
3.4.	O
bayesian	O
model	B
comparison	I
in	O
chapter	O
1	O
,	O
we	O
highlighted	O
the	O
problem	O
of	O
over-ﬁtting	B
as	O
well	O
as	O
the	O
use	O
of	O
cross-	O
validation	O
as	O
a	O
technique	O
for	O
setting	O
the	O
values	O
of	O
regularization	B
parameters	O
or	O
for	O
choosing	O
between	O
alternative	O
models	O
.	O
here	O
we	O
consider	O
the	O
problem	O
of	O
model	O
se-	O
lection	O
from	O
a	O
bayesian	O
perspective	O
.	O
in	O
this	O
section	O
,	O
our	O
discussion	O
will	O
be	O
very	O
general	O
,	O
and	O
then	O
in	O
section	O
3.5	O
we	O
shall	O
see	O
how	O
these	O
ideas	O
can	O
be	O
applied	O
to	O
the	O
determination	O
of	O
regularization	B
parameters	O
in	O
linear	B
regression	I
.	O
as	O
we	O
shall	O
see	O
,	O
the	O
over-ﬁtting	B
associated	O
with	O
maximum	B
likelihood	I
can	O
be	O
avoided	O
by	O
marginalizing	O
(	O
summing	O
or	O
integrating	O
)	O
over	O
the	O
model	O
parameters	O
in-	O
stead	O
of	O
making	O
point	O
estimates	O
of	O
their	O
values	O
.	O
models	O
can	O
then	O
be	O
compared	O
di-	O
rectly	O
on	O
the	O
training	B
data	O
,	O
without	O
the	O
need	O
for	O
a	O
validation	B
set	I
.	O
this	O
allows	O
all	O
available	O
data	O
to	O
be	O
used	O
for	O
training	O
and	O
avoids	O
the	O
multiple	O
training	B
runs	O
for	O
each	O
model	O
associated	O
with	O
cross-validation	B
.	O
it	O
also	O
allows	O
multiple	O
complexity	O
parame-	O
ters	O
to	O
be	O
determined	O
simultaneously	O
as	O
part	O
of	O
the	O
training	B
process	O
.	O
for	O
example	O
,	O
in	O
chapter	O
7	O
we	O
shall	O
introduce	O
the	O
relevance	B
vector	I
machine	I
,	O
which	O
is	O
a	O
bayesian	O
model	O
having	O
one	O
complexity	O
parameter	O
for	O
every	O
training	B
data	O
point	O
.	O
section	O
1.5.4	O
the	O
bayesian	O
view	O
of	O
model	B
comparison	I
simply	O
involves	O
the	O
use	O
of	O
probabilities	O
to	O
represent	O
uncertainty	O
in	O
the	O
choice	O
of	O
model	O
,	O
along	O
with	O
a	O
consistent	B
application	O
of	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
.	O
suppose	O
we	O
wish	O
to	O
compare	O
a	O
set	O
of	O
l	O
models	O
{	O
mi	O
}	O
where	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
l.	O
here	O
a	O
model	O
refers	O
to	O
a	O
probability	B
distribution	O
over	O
the	O
observed	O
data	O
d.	O
in	O
the	O
case	O
of	O
the	O
polynomial	O
curve-ﬁtting	O
problem	O
,	O
the	O
distribution	O
is	O
deﬁned	O
over	O
the	O
set	O
of	O
target	O
values	O
t	O
,	O
while	O
the	O
set	O
of	O
input	O
values	O
x	O
is	O
assumed	O
to	O
be	O
known	O
.	O
other	O
types	O
of	O
model	O
deﬁne	O
a	O
joint	O
distributions	O
over	O
x	O
and	O
t.	O
we	O
shall	O
suppose	O
that	O
the	O
data	O
is	O
generated	O
from	O
one	O
of	O
these	O
models	O
but	O
we	O
are	O
uncertain	O
which	O
one	O
.	O
our	O
uncertainty	O
is	O
expressed	O
through	O
a	O
prior	B
probability	O
distribution	O
p	O
(	O
mi	O
)	O
.	O
given	O
a	O
training	B
set	I
d	O
,	O
we	O
then	O
wish	O
to	O
evaluate	O
the	O
posterior	O
distribution	O
(	O
3.66	O
)	O
the	O
prior	B
allows	O
us	O
to	O
express	O
a	O
preference	O
for	O
different	O
models	O
.	O
let	O
us	O
simply	O
assume	O
that	O
all	O
models	O
are	O
given	O
equal	O
prior	B
probability	O
.	O
the	O
interesting	O
term	O
is	O
the	O
model	B
evidence	I
p	O
(	O
d|mi	O
)	O
which	O
expresses	O
the	O
preference	O
shown	O
by	O
the	O
data	O
for	O
p	O
(	O
mi|d	O
)	O
∝	O
p	O
(	O
mi	O
)	O
p	O
(	O
d|mi	O
)	O
.	O
162	O
3.	O
linear	O
models	O
for	B
regression	I
different	O
models	O
,	O
and	O
we	O
shall	O
examine	O
this	O
term	O
in	O
more	O
detail	O
shortly	O
.	O
the	O
model	B
evidence	I
is	O
sometimes	O
also	O
called	O
the	O
marginal	B
likelihood	I
because	O
it	O
can	O
be	O
viewed	O
as	O
a	O
likelihood	B
function	I
over	O
the	O
space	O
of	O
models	O
,	O
in	O
which	O
the	O
parameters	O
have	O
been	O
marginalized	O
out	O
.	O
the	O
ratio	O
of	O
model	O
evidences	O
p	O
(	O
d|mi	O
)	O
/p	O
(	O
d|mj	O
)	O
for	O
two	O
models	O
is	O
known	O
as	O
a	O
bayes	O
factor	O
(	O
kass	O
and	O
raftery	O
,	O
1995	O
)	O
.	O
once	O
we	O
know	O
the	O
posterior	O
distribution	O
over	O
models	O
,	O
the	O
predictive	B
distribution	I
is	O
given	O
,	O
from	O
the	O
sum	O
and	O
product	O
rules	O
,	O
by	O
l	O
(	O
cid:2	O
)	O
(	O
cid:6	O
)	O
p	O
(	O
t|x	O
,	O
d	O
)	O
=	O
p	O
(	O
t|x	O
,	O
mi	O
,	O
d	O
)	O
p	O
(	O
mi|d	O
)	O
.	O
(	O
3.67	O
)	O
i=1	O
this	O
is	O
an	O
example	O
of	O
a	O
mixture	B
distribution	I
in	O
which	O
the	O
overall	O
predictive	O
distribu-	O
tion	O
is	O
obtained	O
by	O
averaging	O
the	O
predictive	O
distributions	O
p	O
(	O
t|x	O
,	O
mi	O
,	O
d	O
)	O
of	O
individual	O
models	O
,	O
weighted	O
by	O
the	O
posterior	O
probabilities	O
p	O
(	O
mi|d	O
)	O
of	O
those	O
models	O
.	O
for	O
in-	O
stance	O
,	O
if	O
we	O
have	O
two	O
models	O
that	O
are	O
a-posteriori	O
equally	O
likely	O
and	O
one	O
predicts	O
a	O
narrow	O
distribution	O
around	O
t	O
=	O
a	O
while	O
the	O
other	O
predicts	O
a	O
narrow	O
distribution	O
around	O
t	O
=	O
b	O
,	O
the	O
overall	O
predictive	B
distribution	I
will	O
be	O
a	O
bimodal	O
distribution	O
with	O
modes	O
at	O
t	O
=	O
a	O
and	O
t	O
=	O
b	O
,	O
not	O
a	O
single	O
model	O
at	O
t	O
=	O
(	O
a	O
+	O
b	O
)	O
/2	O
.	O
a	O
simple	O
approximation	O
to	O
model	B
averaging	I
is	O
to	O
use	O
the	O
single	O
most	O
probable	O
model	O
alone	O
to	O
make	O
predictions	O
.	O
this	O
is	O
known	O
as	O
model	B
selection	I
.	O
for	O
a	O
model	O
governed	O
by	O
a	O
set	O
of	O
parameters	O
w	O
,	O
the	O
model	B
evidence	I
is	O
given	O
,	O
from	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
,	O
by	O
p	O
(	O
d|mi	O
)	O
=	O
p	O
(	O
d|w	O
,	O
mi	O
)	O
p	O
(	O
w|mi	O
)	O
dw	O
.	O
(	O
3.68	O
)	O
chapter	O
11	O
from	O
a	O
sampling	O
perspective	O
,	O
the	O
marginal	B
likelihood	I
can	O
be	O
viewed	O
as	O
the	O
proba-	O
bility	O
of	O
generating	O
the	O
data	O
set	O
d	O
from	O
a	O
model	O
whose	O
parameters	O
are	O
sampled	O
at	O
random	O
from	O
the	O
prior	B
.	O
it	O
is	O
also	O
interesting	O
to	O
note	O
that	O
the	O
evidence	O
is	O
precisely	O
the	O
normalizing	O
term	O
that	O
appears	O
in	O
the	O
denominator	O
in	O
bayes	O
’	O
theorem	O
when	O
evaluating	O
the	O
posterior	O
distribution	O
over	O
parameters	O
because	O
p	O
(	O
w|d	O
,	O
mi	O
)	O
=	O
p	O
(	O
d|w	O
,	O
mi	O
)	O
p	O
(	O
w|mi	O
)	O
p	O
(	O
d|mi	O
)	O
.	O
(	O
3.69	O
)	O
we	O
can	O
obtain	O
some	O
insight	O
into	O
the	O
model	B
evidence	I
by	O
making	O
a	O
simple	O
approx-	O
imation	O
to	O
the	O
integral	O
over	O
parameters	O
.	O
consider	O
ﬁrst	O
the	O
case	O
of	O
a	O
model	O
having	O
a	O
single	O
parameter	O
w.	O
the	O
posterior	O
distribution	O
over	O
parameters	O
is	O
proportional	O
to	O
p	O
(	O
d|w	O
)	O
p	O
(	O
w	O
)	O
,	O
where	O
we	O
omit	O
the	O
dependence	O
on	O
the	O
model	O
mi	O
to	O
keep	O
the	O
notation	O
uncluttered	O
.	O
if	O
we	O
assume	O
that	O
the	O
posterior	O
distribution	O
is	O
sharply	O
peaked	O
around	O
the	O
most	O
probable	O
value	O
wmap	O
,	O
with	O
width	O
∆wposterior	O
,	O
then	O
we	O
can	O
approximate	O
the	O
in-	O
tegral	O
by	O
the	O
value	O
of	O
the	O
integrand	O
at	O
its	O
maximum	O
times	O
the	O
width	O
of	O
the	O
peak	O
.	O
if	O
we	O
further	O
assume	O
that	O
the	O
prior	B
is	O
ﬂat	O
with	O
width	O
∆wprior	O
so	O
that	O
p	O
(	O
w	O
)	O
=	O
1/∆wprior	O
,	O
then	O
we	O
have	O
(	O
cid:6	O
)	O
p	O
(	O
d	O
)	O
=	O
p	O
(	O
d|w	O
)	O
p	O
(	O
w	O
)	O
dw	O
(	O
cid:7	O
)	O
p	O
(	O
d|wmap	O
)	O
∆wposterior	O
∆wprior	O
(	O
3.70	O
)	O
3.4.	O
bayesian	O
model	B
comparison	I
163	O
figure	O
3.12	O
we	O
can	O
obtain	O
a	O
rough	O
approximation	O
to	O
the	O
model	B
evidence	I
if	O
we	O
assume	O
that	O
the	O
posterior	O
distribution	O
over	O
parame-	O
ters	O
is	O
sharply	O
peaked	O
around	O
its	O
mode	O
wmap	O
.	O
∆wposterior	O
and	O
so	O
taking	O
logs	O
we	O
obtain	O
ln	O
p	O
(	O
d	O
)	O
(	O
cid:7	O
)	O
ln	O
p	O
(	O
d|wmap	O
)	O
+	O
ln	O
wmap	O
∆wprior	O
w	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
.	O
∆wposterior	O
∆wprior	O
(	O
3.71	O
)	O
this	O
approximation	O
is	O
illustrated	O
in	O
figure	O
3.12.	O
the	O
ﬁrst	O
term	O
represents	O
the	O
ﬁt	O
to	O
the	O
data	O
given	O
by	O
the	O
most	O
probable	O
parameter	O
values	O
,	O
and	O
for	O
a	O
ﬂat	O
prior	B
this	O
would	O
correspond	O
to	O
the	O
log	O
likelihood	O
.	O
the	O
second	O
term	O
penalizes	O
the	O
model	O
according	O
to	O
its	O
complexity	O
.	O
because	O
∆wposterior	O
<	O
∆wprior	O
this	O
term	O
is	O
negative	O
,	O
and	O
it	O
increases	O
in	O
magnitude	O
as	O
the	O
ratio	O
∆wposterior/∆wprior	O
gets	O
smaller	O
.	O
thus	O
,	O
if	O
parameters	O
are	O
ﬁnely	O
tuned	O
to	O
the	O
data	O
in	O
the	O
posterior	O
distribution	O
,	O
then	O
the	O
penalty	O
term	O
is	O
large	O
.	O
for	O
a	O
model	O
having	O
a	O
set	O
of	O
m	O
parameters	O
,	O
we	O
can	O
make	O
a	O
similar	O
approximation	O
for	O
each	O
parameter	O
in	O
turn	O
.	O
assuming	O
that	O
all	O
parameters	O
have	O
the	O
same	O
ratio	O
of	O
∆wposterior/∆wprior	O
,	O
we	O
obtain	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
ln	O
p	O
(	O
d	O
)	O
(	O
cid:7	O
)	O
ln	O
p	O
(	O
d|wmap	O
)	O
+	O
m	O
ln	O
∆wposterior	O
∆wprior	O
.	O
(	O
3.72	O
)	O
section	O
4.4.1	O
thus	O
,	O
in	O
this	O
very	O
simple	O
approximation	O
,	O
the	O
size	O
of	O
the	O
complexity	O
penalty	O
increases	O
linearly	O
with	O
the	O
number	O
m	O
of	O
adaptive	O
parameters	O
in	O
the	O
model	O
.	O
as	O
we	O
increase	O
the	O
complexity	O
of	O
the	O
model	O
,	O
the	O
ﬁrst	O
term	O
will	O
typically	O
decrease	O
,	O
because	O
a	O
more	O
complex	O
model	O
is	O
better	O
able	O
to	O
ﬁt	O
the	O
data	O
,	O
whereas	O
the	O
second	O
term	O
will	O
increase	O
due	O
to	O
the	O
dependence	O
on	O
m.	O
the	O
optimal	O
model	O
complexity	O
,	O
as	O
determined	O
by	O
the	O
maximum	O
evidence	O
,	O
will	O
be	O
given	O
by	O
a	O
trade-off	O
between	O
these	O
two	O
competing	O
terms	O
.	O
we	O
shall	O
later	O
develop	O
a	O
more	O
reﬁned	O
version	O
of	O
this	O
approximation	O
,	O
based	O
on	O
a	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
distribution	O
.	O
we	O
can	O
gain	O
further	O
insight	O
into	O
bayesian	O
model	B
comparison	I
and	O
understand	O
how	O
the	O
marginal	B
likelihood	I
can	O
favour	O
models	O
of	O
intermediate	O
complexity	O
by	O
con-	O
sidering	O
figure	O
3.13.	O
here	O
the	O
horizontal	O
axis	O
is	O
a	O
one-dimensional	O
representation	O
of	O
the	O
space	O
of	O
possible	O
data	O
sets	O
,	O
so	O
that	O
each	O
point	O
on	O
this	O
axis	O
corresponds	O
to	O
a	O
speciﬁc	O
data	O
set	O
.	O
we	O
now	O
consider	O
three	O
models	O
m1	O
,	O
m2	O
and	O
m3	O
of	O
successively	O
increasing	O
complexity	O
.	O
imagine	O
running	O
these	O
models	O
generatively	O
to	O
produce	O
exam-	O
ple	O
data	O
sets	O
,	O
and	O
then	O
looking	O
at	O
the	O
distribution	O
of	O
data	O
sets	O
that	O
result	O
.	O
any	O
given	O
164	O
3.	O
linear	O
models	O
for	B
regression	I
p	O
(	O
d	O
)	O
m1	O
figure	O
3.13	O
schematic	O
illustration	O
of	O
the	O
distribution	O
of	O
data	O
sets	O
for	O
three	O
models	O
of	O
different	O
com-	O
in	O
which	O
m1	O
is	O
the	O
plexity	O
,	O
simplest	O
and	O
m3	O
is	O
the	O
most	O
complex	O
.	O
note	O
that	O
the	O
dis-	O
tributions	O
are	O
normalized	O
.	O
in	O
for	O
the	O
partic-	O
this	O
example	O
,	O
ular	O
observed	O
data	O
set	O
d0	O
,	O
the	O
model	O
m2	O
with	O
intermedi-	O
ate	O
complexity	O
has	O
the	O
largest	O
evidence	O
.	O
m2	O
d0	O
m3	O
d	O
model	O
can	O
generate	O
a	O
variety	O
of	O
different	O
data	O
sets	O
since	O
the	O
parameters	O
are	O
governed	O
by	O
a	O
prior	B
probability	O
distribution	O
,	O
and	O
for	O
any	O
choice	O
of	O
the	O
parameters	O
there	O
may	O
be	O
random	O
noise	O
on	O
the	O
target	O
variables	O
.	O
to	O
generate	O
a	O
particular	O
data	O
set	O
from	O
a	O
spe-	O
ciﬁc	O
model	O
,	O
we	O
ﬁrst	O
choose	O
the	O
values	O
of	O
the	O
parameters	O
from	O
their	O
prior	B
distribution	O
p	O
(	O
w	O
)	O
,	O
and	O
then	O
for	O
these	O
parameter	O
values	O
we	O
sample	O
the	O
data	O
from	O
p	O
(	O
d|w	O
)	O
.	O
a	O
sim-	O
ple	O
model	O
(	O
for	O
example	O
,	O
based	O
on	O
a	O
ﬁrst	B
order	I
polynomial	O
)	O
has	O
little	O
variability	O
and	O
so	O
will	O
generate	O
data	O
sets	O
that	O
are	O
fairly	O
similar	O
to	O
each	O
other	O
.	O
its	O
distribution	O
p	O
(	O
d	O
)	O
is	O
therefore	O
conﬁned	O
to	O
a	O
relatively	O
small	O
region	O
of	O
the	O
horizontal	O
axis	O
.	O
by	O
contrast	O
,	O
a	O
complex	O
model	O
(	O
such	O
as	O
a	O
ninth	O
order	O
polynomial	O
)	O
can	O
generate	O
a	O
great	O
variety	O
of	O
different	O
data	O
sets	O
,	O
and	O
so	O
its	O
distribution	O
p	O
(	O
d	O
)	O
is	O
spread	O
over	O
a	O
large	O
region	O
of	O
the	O
space	O
of	O
data	O
sets	O
.	O
because	O
the	O
distributions	O
p	O
(	O
d|mi	O
)	O
are	O
normalized	O
,	O
we	O
see	O
that	O
the	O
particular	O
data	O
set	O
d0	O
can	O
have	O
the	O
highest	O
value	O
of	O
the	O
evidence	O
for	O
the	O
model	O
of	O
intermediate	O
complexity	O
.	O
essentially	O
,	O
the	O
simpler	O
model	O
can	O
not	O
ﬁt	O
the	O
data	O
well	O
,	O
whereas	O
the	O
more	O
complex	O
model	O
spreads	O
its	O
predictive	O
probability	O
over	O
too	O
broad	O
a	O
range	O
of	O
data	O
sets	O
and	O
so	O
assigns	O
relatively	O
small	O
probability	B
to	O
any	O
one	O
of	O
them	O
.	O
implicit	O
in	O
the	O
bayesian	O
model	B
comparison	I
framework	O
is	O
the	O
assumption	O
that	O
the	O
true	O
distribution	O
from	O
which	O
the	O
data	O
are	O
generated	O
is	O
contained	O
within	O
the	O
set	O
of	O
models	O
under	O
consideration	O
.	O
provided	O
this	O
is	O
so	O
,	O
we	O
can	O
show	O
that	O
bayesian	O
model	B
comparison	I
will	O
on	O
average	O
favour	O
the	O
correct	O
model	O
.	O
to	O
see	O
this	O
,	O
consider	O
two	O
models	O
m1	O
and	O
m2	O
in	O
which	O
the	O
truth	O
corresponds	O
to	O
m1	O
.	O
for	O
a	O
given	O
ﬁnite	O
data	O
set	O
,	O
it	O
is	O
possible	O
for	O
the	O
bayes	O
factor	O
to	O
be	O
larger	O
for	O
the	O
incorrect	O
model	O
.	O
however	O
,	O
if	O
bayes	O
factor	O
in	O
the	O
form	O
(	O
cid:6	O
)	O
we	O
average	O
the	O
bayes	O
factor	O
over	O
the	O
distribution	O
of	O
data	O
sets	O
,	O
we	O
obtain	O
the	O
expected	O
(	O
3.73	O
)	O
p	O
(	O
d|m1	O
)	O
ln	O
p	O
(	O
d|m1	O
)	O
p	O
(	O
d|m2	O
)	O
dd	O
section	O
1.6.1	O
where	O
the	O
average	O
has	O
been	O
taken	O
with	O
respect	O
to	O
the	O
true	O
distribution	O
of	O
the	O
data	O
.	O
this	O
quantity	O
is	O
an	O
example	O
of	O
the	O
kullback-leibler	O
divergence	O
and	O
satisﬁes	O
the	O
prop-	O
erty	O
of	O
always	O
being	O
positive	O
unless	O
the	O
two	O
distributions	O
are	O
equal	O
in	O
which	O
case	O
it	O
is	O
zero	O
.	O
thus	O
on	O
average	O
the	O
bayes	O
factor	O
will	O
always	O
favour	O
the	O
correct	O
model	O
.	O
we	O
have	O
seen	O
that	O
the	O
bayesian	O
framework	O
avoids	O
the	O
problem	O
of	O
over-ﬁtting	B
and	O
allows	O
models	O
to	O
be	O
compared	O
on	O
the	O
basis	O
of	O
the	O
training	B
data	O
alone	O
.	O
however	O
,	O
3.5.	O
the	O
evidence	B
approximation	I
165	O
a	O
bayesian	O
approach	O
,	O
like	O
any	O
approach	O
to	O
pattern	O
recognition	O
,	O
needs	O
to	O
make	O
as-	O
sumptions	O
about	O
the	O
form	O
of	O
the	O
model	O
,	O
and	O
if	O
these	O
are	O
invalid	O
then	O
the	O
results	O
can	O
be	O
misleading	O
.	O
in	O
particular	O
,	O
we	O
see	O
from	O
figure	O
3.12	O
that	O
the	O
model	B
evidence	I
can	O
be	O
sensitive	O
to	O
many	O
aspects	O
of	O
the	O
prior	B
,	O
such	O
as	O
the	O
behaviour	O
in	O
the	O
tails	O
.	O
indeed	O
,	O
the	O
evidence	O
is	O
not	O
deﬁned	O
if	O
the	O
prior	B
is	O
improper	B
,	O
as	O
can	O
be	O
seen	O
by	O
noting	O
that	O
an	O
improper	B
prior	I
has	O
an	O
arbitrary	O
scaling	B
factor	I
(	O
in	O
other	O
words	O
,	O
the	O
normalization	O
coefﬁcient	O
is	O
not	O
deﬁned	O
because	O
the	O
distribution	O
can	O
not	O
be	O
normalized	O
)	O
.	O
if	O
we	O
con-	O
sider	O
a	O
proper	O
prior	B
and	O
then	O
take	O
a	O
suitable	O
limit	O
in	O
order	O
to	O
obtain	O
an	O
improper	B
prior	I
(	O
for	O
example	O
,	O
a	O
gaussian	O
prior	B
in	O
which	O
we	O
take	O
the	O
limit	O
of	O
inﬁnite	O
variance	B
)	O
then	O
the	O
evidence	O
will	O
go	O
to	O
zero	O
,	O
as	O
can	O
be	O
seen	O
from	O
(	O
3.70	O
)	O
and	O
figure	O
3.12.	O
it	O
may	O
,	O
however	O
,	O
be	O
possible	O
to	O
consider	O
the	O
evidence	O
ratio	O
between	O
two	O
models	O
ﬁrst	O
and	O
then	O
take	O
a	O
limit	O
to	O
obtain	O
a	O
meaningful	O
answer	O
.	O
in	O
a	O
practical	O
application	O
,	O
therefore	O
,	O
it	O
will	O
be	O
wise	O
to	O
keep	O
aside	O
an	O
independent	B
test	O
set	O
of	O
data	O
on	O
which	O
to	O
evaluate	O
the	O
overall	O
performance	O
of	O
the	O
ﬁnal	O
system	O
.	O
3.5.	O
the	O
evidence	B
approximation	I
in	O
a	O
fully	O
bayesian	O
treatment	O
of	O
the	O
linear	O
basis	O
function	O
model	O
,	O
we	O
would	O
intro-	O
duce	O
prior	B
distributions	O
over	O
the	O
hyperparameters	O
α	O
and	O
β	O
and	O
make	O
predictions	O
by	O
marginalizing	O
with	O
respect	O
to	O
these	O
hyperparameters	O
as	O
well	O
as	O
with	O
respect	O
to	O
the	O
parameters	O
w.	O
however	O
,	O
although	O
we	O
can	O
integrate	O
analytically	O
over	O
either	O
w	O
or	O
over	O
the	O
hyperparameters	O
,	O
the	O
complete	O
marginalization	O
over	O
all	O
of	O
these	O
variables	O
is	O
analytically	O
intractable	O
.	O
here	O
we	O
discuss	O
an	O
approximation	O
in	O
which	O
we	O
set	O
the	O
hyperparameters	O
to	O
speciﬁc	O
values	O
determined	O
by	O
maximizing	O
the	O
marginal	B
likeli-	O
hood	O
function	O
obtained	O
by	O
ﬁrst	O
integrating	O
over	O
the	O
parameters	O
w.	O
this	O
framework	O
is	O
known	O
in	O
the	O
statistics	O
literature	O
as	O
empirical	O
bayes	O
(	O
bernardo	O
and	O
smith	O
,	O
1994	O
;	O
gelman	O
et	O
al.	O
,	O
2004	O
)	O
,	O
or	O
type	B
2	I
maximum	I
likelihood	I
(	O
berger	O
,	O
1985	O
)	O
,	O
or	O
generalized	B
maximum	I
likelihood	I
(	O
wahba	O
,	O
1975	O
)	O
,	O
and	O
in	O
the	O
machine	O
learning	O
literature	O
is	O
also	O
called	O
the	O
evidence	B
approximation	I
(	O
gull	O
,	O
1989	O
;	O
mackay	O
,	O
1992a	O
)	O
.	O
if	O
we	O
introduce	O
hyperpriors	O
over	O
α	O
and	O
β	O
,	O
the	O
predictive	B
distribution	I
is	O
obtained	O
by	O
marginalizing	O
over	O
w	O
,	O
α	O
and	O
β	O
so	O
that	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
p	O
(	O
t|t	O
)	O
=	O
p	O
(	O
t|w	O
,	O
β	O
)	O
p	O
(	O
w|t	O
,	O
α	O
,	O
β	O
)	O
p	O
(	O
α	O
,	O
β|t	O
)	O
dw	O
dα	O
dβ	O
(	O
3.74	O
)	O
where	O
p	O
(	O
t|w	O
,	O
β	O
)	O
is	O
given	O
by	O
(	O
3.8	O
)	O
and	O
p	O
(	O
w|t	O
,	O
α	O
,	O
β	O
)	O
is	O
given	O
by	O
(	O
3.49	O
)	O
with	O
mn	O
and	O
sn	O
deﬁned	O
by	O
(	O
3.53	O
)	O
and	O
(	O
3.54	O
)	O
respectively	O
.	O
here	O
we	O
have	O
omitted	O
the	O
dependence	O
on	O
the	O
input	O
variable	O
x	O
to	O
keep	O
the	O
notation	O
uncluttered	O
.	O
if	O
the	O
posterior	O
distribution	O
p	O
(	O
α	O
,	O
β|t	O
)	O
is	O
sharply	O
peaked	O
around	O
values	O
(	O
cid:1	O
)	O
α	O
and	O
(	O
cid:1	O
)	O
β	O
,	O
then	O
the	O
predictive	B
distribution	I
is	O
obtained	O
simply	O
by	O
marginalizing	O
over	O
w	O
in	O
which	O
α	O
and	O
β	O
are	O
ﬁxed	O
to	O
the	O
values	O
(	O
cid:1	O
)	O
α	O
and	O
(	O
cid:1	O
)	O
β	O
,	O
so	O
that	O
p	O
(	O
t|t	O
)	O
(	O
cid:7	O
)	O
p	O
(	O
t|t	O
,	O
(	O
cid:1	O
)	O
α	O
,	O
(	O
cid:1	O
)	O
β	O
)	O
=	O
p	O
(	O
t|w	O
,	O
(	O
cid:1	O
)	O
β	O
)	O
p	O
(	O
w|t	O
,	O
(	O
cid:1	O
)	O
α	O
,	O
(	O
cid:1	O
)	O
β	O
)	O
dw	O
.	O
(	O
3.75	O
)	O
(	O
cid:6	O
)	O
166	O
3.	O
linear	O
models	O
for	B
regression	I
from	O
bayes	O
’	O
theorem	O
,	O
the	O
posterior	O
distribution	O
for	O
α	O
and	O
β	O
is	O
given	O
by	O
if	O
the	O
prior	B
is	O
relatively	O
ﬂat	O
,	O
then	O
in	O
the	O
evidence	O
framework	O
the	O
values	O
of	O
(	O
cid:1	O
)	O
α	O
and	O
(	O
cid:1	O
)	O
β	O
are	O
obtained	O
by	O
maximizing	O
the	O
marginal	B
likelihood	I
function	O
p	O
(	O
t|α	O
,	O
β	O
)	O
.	O
we	O
shall	O
p	O
(	O
α	O
,	O
β|t	O
)	O
∝	O
p	O
(	O
t|α	O
,	O
β	O
)	O
p	O
(	O
α	O
,	O
β	O
)	O
.	O
(	O
3.76	O
)	O
proceed	O
by	O
evaluating	O
the	O
marginal	B
likelihood	I
for	O
the	O
linear	O
basis	O
function	O
model	O
and	O
then	O
ﬁnding	O
its	O
maxima	O
.	O
this	O
will	O
allow	O
us	O
to	O
determine	O
values	O
for	O
these	O
hyperpa-	O
rameters	O
from	O
the	O
training	B
data	O
alone	O
,	O
without	O
recourse	O
to	O
cross-validation	B
.	O
recall	O
that	O
the	O
ratio	O
α/β	O
is	O
analogous	O
to	O
a	O
regularization	B
parameter	O
.	O
as	O
an	O
aside	O
it	O
is	O
worth	O
noting	O
that	O
,	O
if	O
we	O
deﬁne	O
conjugate	B
(	O
gamma	O
)	O
prior	B
distri-	O
butions	O
over	O
α	O
and	O
β	O
,	O
then	O
the	O
marginalization	O
over	O
these	O
hyperparameters	O
in	O
(	O
3.74	O
)	O
can	O
be	O
performed	O
analytically	O
to	O
give	O
a	O
student	O
’	O
s	O
t-distribution	O
over	O
w	O
(	O
see	O
sec-	O
tion	O
2.3.7	O
)	O
.	O
although	O
the	O
resulting	O
integral	O
over	O
w	O
is	O
no	O
longer	O
analytically	O
tractable	O
,	O
it	O
might	O
be	O
thought	O
that	O
approximating	O
this	O
integral	O
,	O
for	O
example	O
using	O
the	O
laplace	O
approximation	O
discussed	O
(	O
section	O
4.4	O
)	O
which	O
is	O
based	O
on	O
a	O
local	B
gaussian	O
approxi-	O
mation	B
centred	O
on	O
the	O
mode	O
of	O
the	O
posterior	O
distribution	O
,	O
might	O
provide	O
a	O
practical	O
alternative	O
to	O
the	O
evidence	O
framework	O
(	O
buntine	O
and	O
weigend	O
,	O
1991	O
)	O
.	O
however	O
,	O
the	O
integrand	O
as	O
a	O
function	O
of	O
w	O
typically	O
has	O
a	O
strongly	O
skewed	O
mode	O
so	O
that	O
the	O
laplace	O
approximation	O
fails	O
to	O
capture	O
the	O
bulk	O
of	O
the	O
probability	B
mass	O
,	O
leading	O
to	O
poorer	O
re-	O
sults	O
than	O
those	O
obtained	O
by	O
maximizing	O
the	O
evidence	O
(	O
mackay	O
,	O
1999	O
)	O
.	O
returning	O
to	O
the	O
evidence	O
framework	O
,	O
we	O
note	O
that	O
there	O
are	O
two	O
approaches	O
that	O
we	O
can	O
take	O
to	O
the	O
maximization	O
of	O
the	O
log	O
evidence	O
.	O
we	O
can	O
evaluate	O
the	O
evidence	B
function	I
analytically	O
and	O
then	O
set	O
its	O
derivative	B
equal	O
to	O
zero	O
to	O
obtain	O
re-estimation	O
equations	O
for	O
α	O
and	O
β	O
,	O
which	O
we	O
shall	O
do	O
in	O
section	O
3.5.2.	O
alternatively	O
we	O
use	O
a	O
technique	O
called	O
the	O
expectation	B
maximization	I
(	O
em	O
)	O
algorithm	O
,	O
which	O
will	O
be	O
dis-	O
cussed	O
in	O
section	O
9.3.4	O
where	O
we	O
shall	O
also	O
show	O
that	O
these	O
two	O
approaches	O
converge	O
to	O
the	O
same	O
solution	O
.	O
3.5.1	O
evaluation	O
of	O
the	O
evidence	B
function	I
the	O
marginal	B
likelihood	I
function	O
p	O
(	O
t|α	O
,	O
β	O
)	O
is	O
obtained	O
by	O
integrating	O
over	O
the	O
weight	O
parameters	O
w	O
,	O
so	O
that	O
(	O
cid:6	O
)	O
p	O
(	O
t|α	O
,	O
β	O
)	O
=	O
p	O
(	O
t|w	O
,	O
β	O
)	O
p	O
(	O
w|α	O
)	O
dw	O
.	O
(	O
3.77	O
)	O
exercise	O
3.16	O
exercise	O
3.17	O
one	O
way	O
to	O
evaluate	O
this	O
integral	O
is	O
to	O
make	O
use	O
once	O
again	O
of	O
the	O
result	O
(	O
2.115	O
)	O
for	O
the	O
conditional	B
distribution	O
in	O
a	O
linear-gaussian	O
model	O
.	O
here	O
we	O
shall	O
evaluate	O
the	O
integral	O
instead	O
by	O
completing	B
the	I
square	I
in	O
the	O
exponent	O
and	O
making	O
use	O
of	O
the	O
standard	O
form	O
for	O
the	O
normalization	O
coefﬁcient	O
of	O
a	O
gaussian	O
.	O
from	O
(	O
3.11	O
)	O
,	O
(	O
3.12	O
)	O
,	O
and	O
(	O
3.52	O
)	O
,	O
we	O
can	O
write	O
the	O
evidence	B
function	I
in	O
the	O
form	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
n/2	O
(	O
cid:17	O
)	O
(	O
cid:18	O
)	O
m/2	O
(	O
cid:6	O
)	O
p	O
(	O
t|α	O
,	O
β	O
)	O
=	O
β	O
2π	O
α	O
2π	O
exp	O
{	O
−e	O
(	O
w	O
)	O
}	O
dw	O
(	O
3.78	O
)	O
exercise	O
3.18	O
exercise	O
3.19	O
3.5.	O
the	O
evidence	B
approximation	I
167	O
where	O
m	O
is	O
the	O
dimensionality	O
of	O
w	O
,	O
and	O
we	O
have	O
deﬁned	O
e	O
(	O
w	O
)	O
=	O
βed	O
(	O
w	O
)	O
+	O
αew	O
(	O
w	O
)	O
=	O
β	O
2	O
(	O
cid:5	O
)	O
t	O
−	O
φw	O
(	O
cid:5	O
)	O
2	O
+	O
α	O
2	O
wtw	O
.	O
(	O
3.79	O
)	O
we	O
recognize	O
(	O
3.79	O
)	O
as	O
being	O
equal	O
,	O
up	O
to	O
a	O
constant	O
of	O
proportionality	O
,	O
to	O
the	O
reg-	O
ularized	O
sum-of-squares	B
error	I
function	O
(	O
3.27	O
)	O
.	O
we	O
now	O
complete	O
the	O
square	O
over	O
w	O
giving	O
e	O
(	O
w	O
)	O
=	O
e	O
(	O
mn	O
)	O
+	O
(	O
w	O
−	O
mn	O
)	O
ta	O
(	O
w	O
−	O
mn	O
)	O
1	O
2	O
where	O
we	O
have	O
introduced	O
a	O
=	O
αi	O
+	O
βφtφ	O
together	O
with	O
e	O
(	O
mn	O
)	O
=	O
β	O
2	O
(	O
cid:5	O
)	O
t	O
−	O
φmn	O
(	O
cid:5	O
)	O
2	O
+	O
α	O
2	O
mt	O
n	O
mn	O
.	O
note	O
that	O
a	O
corresponds	O
to	O
the	O
matrix	O
of	O
second	O
derivatives	O
of	O
the	O
error	B
function	I
a	O
=	O
∇∇e	O
(	O
w	O
)	O
(	O
3.83	O
)	O
and	O
is	O
known	O
as	O
the	O
hessian	O
matrix	O
.	O
here	O
we	O
have	O
also	O
deﬁned	O
mn	O
given	O
by	O
mn	O
=	O
βa−1φtt	O
.	O
−1	O
n	O
,	O
and	O
hence	O
(	O
3.84	O
)	O
is	O
equivalent	O
to	O
the	O
previous	O
(	O
3.84	O
)	O
using	O
(	O
3.54	O
)	O
,	O
we	O
see	O
that	O
a	O
=	O
s	O
deﬁnition	O
(	O
3.53	O
)	O
,	O
and	O
therefore	O
represents	O
the	O
mean	B
of	O
the	O
posterior	O
distribution	O
.	O
the	O
integral	O
over	O
w	O
can	O
now	O
be	O
evaluated	O
simply	O
by	O
appealing	O
to	O
the	O
standard	O
result	O
for	O
the	O
normalization	O
coefﬁcient	O
of	O
a	O
multivariate	O
gaussian	O
,	O
giving	O
(	O
cid:6	O
)	O
(	O
3.80	O
)	O
(	O
3.81	O
)	O
(	O
3.82	O
)	O
(	O
cid:6	O
)	O
(	O
cid:12	O
)	O
exp	O
{	O
−e	O
(	O
w	O
)	O
}	O
dw	O
=	O
exp	O
{	O
−e	O
(	O
mn	O
)	O
}	O
=	O
exp	O
{	O
−e	O
(	O
mn	O
)	O
}	O
(	O
2π	O
)	O
m/2|a|−1/2	O
.	O
−1	O
2	O
exp	O
(	O
w	O
−	O
mn	O
)	O
ta	O
(	O
w	O
−	O
mn	O
)	O
(	O
cid:13	O
)	O
dw	O
(	O
3.85	O
)	O
using	O
(	O
3.78	O
)	O
we	O
can	O
then	O
write	O
the	O
log	O
of	O
the	O
marginal	B
likelihood	I
in	O
the	O
form	O
ln	O
p	O
(	O
t|α	O
,	O
β	O
)	O
=	O
m	O
2	O
ln	O
α	O
+	O
n	O
2	O
ln	O
β	O
−	O
e	O
(	O
mn	O
)	O
−	O
1	O
2	O
ln|a|	O
−	O
n	O
2	O
ln	O
(	O
2π	O
)	O
(	O
3.86	O
)	O
which	O
is	O
the	O
required	O
expression	O
for	O
the	O
evidence	B
function	I
.	O
returning	O
to	O
the	O
polynomial	O
regression	O
problem	O
,	O
we	O
can	O
plot	O
the	O
model	B
evidence	I
against	O
the	O
order	O
of	O
the	O
polynomial	O
,	O
as	O
shown	O
in	O
figure	O
3.14.	O
here	O
we	O
have	O
assumed	O
a	O
prior	B
of	O
the	O
form	O
(	O
1.65	O
)	O
with	O
the	O
parameter	O
α	O
ﬁxed	O
at	O
α	O
=	O
5	O
×	O
10−3	O
.	O
the	O
form	O
of	O
this	O
plot	O
is	O
very	O
instructive	O
.	O
referring	O
back	O
to	O
figure	O
1.4	O
,	O
we	O
see	O
that	O
the	O
m	O
=	O
0	O
polynomial	O
has	O
very	O
poor	O
ﬁt	O
to	O
the	O
data	O
and	O
consequently	O
gives	O
a	O
relatively	O
low	O
value	O
168	O
3.	O
linear	O
models	O
for	B
regression	I
figure	O
3.14	O
plot	O
of	O
the	O
model	B
evidence	I
versus	O
the	O
order	O
m	O
,	O
for	O
the	O
polynomial	O
re-	O
gression	O
model	O
,	O
showing	O
that	O
the	O
evidence	O
favours	O
the	O
model	O
with	O
m	O
=	O
3	O
.	O
−18	O
−20	O
−22	O
−24	O
−26	O
0	O
2	O
6	O
8	O
4	O
m	O
for	O
the	O
evidence	O
.	O
going	O
to	O
the	O
m	O
=	O
1	O
polynomial	O
greatly	O
improves	O
the	O
data	O
ﬁt	O
,	O
and	O
hence	O
the	O
evidence	O
is	O
signiﬁcantly	O
higher	O
.	O
however	O
,	O
in	O
going	O
to	O
m	O
=	O
2	O
,	O
the	O
data	O
ﬁt	O
is	O
improved	O
only	O
very	O
marginally	O
,	O
due	O
to	O
the	O
fact	O
that	O
the	O
underlying	O
sinusoidal	O
function	O
from	O
which	O
the	O
data	O
is	O
generated	O
is	O
an	O
odd	O
function	O
and	O
so	O
has	O
no	O
even	O
terms	O
in	O
a	O
polynomial	O
expansion	O
.	O
indeed	O
,	O
figure	O
1.5	O
shows	O
that	O
the	O
residual	O
data	O
error	O
is	O
reduced	O
only	O
slightly	O
in	O
going	O
from	O
m	O
=	O
1	O
to	O
m	O
=	O
2.	O
because	O
this	O
richer	O
model	O
suffers	O
a	O
greater	O
complexity	O
penalty	O
,	O
the	O
evidence	O
actually	O
falls	O
in	O
going	O
from	O
m	O
=	O
1	O
to	O
m	O
=	O
2.	O
when	O
we	O
go	O
to	O
m	O
=	O
3	O
we	O
obtain	O
a	O
signiﬁcant	O
further	O
improvement	O
in	O
data	O
ﬁt	O
,	O
as	O
seen	O
in	O
figure	O
1.4	O
,	O
and	O
so	O
the	O
evidence	O
is	O
increased	O
again	O
,	O
giving	O
the	O
highest	O
overall	O
evidence	O
for	O
any	O
of	O
the	O
polynomials	O
.	O
further	O
increases	O
in	O
the	O
value	O
of	O
m	O
produce	O
only	O
small	O
improvements	O
in	O
the	O
ﬁt	O
to	O
the	O
data	O
but	O
suffer	O
increasing	O
complexity	O
penalty	O
,	O
leading	O
overall	O
to	O
a	O
decrease	O
in	O
the	O
evidence	O
values	O
.	O
looking	O
again	O
at	O
figure	O
1.5	O
,	O
we	O
see	O
that	O
the	O
generalization	B
error	O
is	O
roughly	O
constant	O
between	O
m	O
=	O
3	O
and	O
m	O
=	O
8	O
,	O
and	O
it	O
would	O
be	O
difﬁcult	O
to	O
choose	O
between	O
these	O
models	O
on	O
the	O
basis	O
of	O
this	O
plot	O
alone	O
.	O
the	O
evidence	O
values	O
,	O
however	O
,	O
show	O
a	O
clear	O
preference	O
for	O
m	O
=	O
3	O
,	O
since	O
this	O
is	O
the	O
simplest	O
model	O
which	O
gives	O
a	O
good	O
explanation	O
for	O
the	O
observed	O
data	O
.	O
3.5.2	O
maximizing	O
the	O
evidence	B
function	I
let	O
us	O
ﬁrst	O
consider	O
the	O
maximization	O
of	O
p	O
(	O
t|α	O
,	O
β	O
)	O
with	O
respect	O
to	O
α.	O
this	O
can	O
be	O
done	O
by	O
ﬁrst	O
deﬁning	O
the	O
following	O
eigenvector	O
equation	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
(	O
3.87	O
)	O
(	O
cid:2	O
)	O
from	O
(	O
3.81	O
)	O
,	O
it	O
then	O
follows	O
that	O
a	O
has	O
eigenvalues	O
α	O
+	O
λi	O
.	O
now	O
consider	O
the	O
deriva-	O
tive	O
of	O
the	O
term	O
involving	O
ln|a|	O
in	O
(	O
3.86	O
)	O
with	O
respect	O
to	O
α.	O
we	O
have	O
(	O
cid:2	O
)	O
(	O
cid:14	O
)	O
βφtφ	O
ui	O
=	O
λiui	O
.	O
d	O
dα	O
ln|a|	O
=	O
d	O
dα	O
ln	O
(	O
λi	O
+	O
α	O
)	O
=	O
d	O
dα	O
i	O
i	O
ln	O
(	O
λi	O
+	O
α	O
)	O
=	O
.	O
(	O
3.88	O
)	O
1	O
λi	O
+	O
α	O
i	O
(	O
cid:2	O
)	O
thus	O
the	O
stationary	B
points	O
of	O
(	O
3.86	O
)	O
with	O
respect	O
to	O
α	O
satisfy	O
0	O
=	O
m	O
2α	O
−	O
1	O
2	O
n	O
mn	O
−	O
1	O
mt	O
2	O
1	O
λi	O
+	O
α	O
i	O
.	O
(	O
3.89	O
)	O
multiplying	O
through	O
by	O
2α	O
and	O
rearranging	O
,	O
we	O
obtain	O
3.5.	O
the	O
evidence	B
approximation	I
169	O
(	O
cid:2	O
)	O
1	O
λi	O
+	O
α	O
i	O
αmt	O
n	O
mn	O
=	O
m	O
−	O
α	O
(	O
cid:2	O
)	O
γ	O
=	O
λi	O
α	O
+	O
λi	O
i	O
=	O
γ	O
.	O
(	O
3.90	O
)	O
.	O
(	O
3.91	O
)	O
since	O
there	O
are	O
m	O
terms	O
in	O
the	O
sum	O
over	O
i	O
,	O
the	O
quantity	O
γ	O
can	O
be	O
written	O
exercise	O
3.20	O
the	O
interpretation	O
of	O
the	O
quantity	O
γ	O
will	O
be	O
discussed	O
shortly	O
.	O
from	O
(	O
3.90	O
)	O
we	O
see	O
that	O
the	O
value	O
of	O
α	O
that	O
maximizes	O
the	O
marginal	B
likelihood	I
satisﬁes	O
α	O
=	O
γ	O
n	O
mn	O
mt	O
.	O
(	O
3.92	O
)	O
note	O
that	O
this	O
is	O
an	O
implicit	O
solution	O
for	O
α	O
not	O
only	O
because	O
γ	O
depends	O
on	O
α	O
,	O
but	O
also	O
because	O
the	O
mode	O
mn	O
of	O
the	O
posterior	O
distribution	O
itself	O
depends	O
on	O
the	O
choice	O
of	O
α.	O
we	O
therefore	O
adopt	O
an	O
iterative	O
procedure	O
in	O
which	O
we	O
make	O
an	O
initial	O
choice	O
for	O
α	O
and	O
use	O
this	O
to	O
ﬁnd	O
mn	O
,	O
which	O
is	O
given	O
by	O
(	O
3.53	O
)	O
,	O
and	O
also	O
to	O
evaluate	O
γ	O
,	O
which	O
is	O
given	O
by	O
(	O
3.91	O
)	O
.	O
these	O
values	O
are	O
then	O
used	O
to	O
re-estimate	O
α	O
using	O
(	O
3.92	O
)	O
,	O
and	O
the	O
process	O
repeated	O
until	O
convergence	O
.	O
note	O
that	O
because	O
the	O
matrix	O
φtφ	O
is	O
ﬁxed	O
,	O
we	O
can	O
compute	O
its	O
eigenvalues	O
once	O
at	O
the	O
start	O
and	O
then	O
simply	O
multiply	O
these	O
by	O
β	O
to	O
obtain	O
the	O
λi	O
.	O
it	O
should	O
be	O
emphasized	O
that	O
the	O
value	O
of	O
α	O
has	O
been	O
determined	O
purely	O
by	O
look-	O
ing	O
at	O
the	O
training	B
data	O
.	O
in	O
contrast	O
to	O
maximum	B
likelihood	I
methods	O
,	O
no	O
independent	B
data	O
set	O
is	O
required	O
in	O
order	O
to	O
optimize	O
the	O
model	O
complexity	O
.	O
we	O
can	O
similarly	O
maximize	O
the	O
log	O
marginal	O
likelihood	O
(	O
3.86	O
)	O
with	O
respect	O
to	O
β.	O
to	O
do	O
this	O
,	O
we	O
note	O
that	O
the	O
eigenvalues	O
λi	O
deﬁned	O
by	O
(	O
3.87	O
)	O
are	O
proportional	O
to	O
β	O
,	O
and	O
hence	O
dλi/dβ	O
=	O
λi/β	O
giving	O
d	O
dβ	O
ln|a|	O
=	O
d	O
dβ	O
ln	O
(	O
λi	O
+	O
α	O
)	O
=	O
1	O
β	O
=	O
γ	O
β	O
.	O
(	O
3.93	O
)	O
the	O
stationary	B
point	O
of	O
the	O
marginal	B
likelihood	I
therefore	O
satisﬁes	O
0	O
=	O
n	O
2β	O
exercise	O
3.22	O
and	O
rearranging	O
we	O
obtain	O
1	O
β	O
=	O
−	O
1	O
2	O
tn	O
−	O
mt	O
n	O
φ	O
(	O
xn	O
)	O
n	O
−	O
γ	O
n=1	O
tn	O
−	O
mt	O
n	O
φ	O
(	O
xn	O
)	O
.	O
(	O
3.94	O
)	O
(	O
3.95	O
)	O
i	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:26	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
1	O
(	O
cid:26	O
)	O
λi	O
+	O
α	O
i	O
λi	O
(	O
cid:2	O
)	O
(	O
cid:27	O
)	O
2	O
−	O
γ	O
(	O
cid:27	O
)	O
2	O
2β	O
again	O
,	O
this	O
is	O
an	O
implicit	O
solution	O
for	O
β	O
and	O
can	O
be	O
solved	O
by	O
choosing	O
an	O
initial	O
value	O
for	O
β	O
and	O
then	O
using	O
this	O
to	O
calculate	O
mn	O
and	O
γ	O
and	O
then	O
re-estimate	O
β	O
using	O
(	O
3.95	O
)	O
,	O
repeating	O
until	O
convergence	O
.	O
if	O
both	O
α	O
and	O
β	O
are	O
to	O
be	O
determined	O
from	O
the	O
data	O
,	O
then	O
their	O
values	O
can	O
be	O
re-estimated	O
together	O
after	O
each	O
update	O
of	O
γ	O
.	O
170	O
3.	O
linear	O
models	O
for	B
regression	I
figure	O
3.15	O
contours	O
of	O
the	O
likelihood	B
function	I
(	O
red	O
)	O
and	O
the	O
prior	B
(	O
green	O
)	O
in	O
which	O
the	O
axes	O
in	O
parameter	O
space	O
have	O
been	O
rotated	O
to	O
align	O
with	O
the	O
eigenvectors	O
ui	O
of	O
the	O
hessian	O
.	O
for	O
α	O
=	O
0	O
,	O
the	O
mode	O
of	O
the	O
poste-	O
rior	O
is	O
given	O
by	O
the	O
maximum	B
likelihood	I
solution	O
wml	O
,	O
whereas	O
for	O
nonzero	O
α	O
the	O
mode	O
is	O
at	O
wmap	O
=	O
mn	O
.	O
in	O
the	O
direction	O
w1	O
the	O
eigenvalue	O
λ1	O
,	O
deﬁned	O
by	O
(	O
3.87	O
)	O
,	O
is	O
small	O
compared	O
with	O
α	O
and	O
so	O
the	O
quantity	O
λ1/	O
(	O
λ1	O
+	O
α	O
)	O
is	O
close	O
to	O
zero	O
,	O
and	O
the	O
corresponding	O
map	O
value	O
of	O
w1	O
is	O
also	O
close	O
to	O
zero	O
.	O
by	O
contrast	O
,	O
in	O
the	O
direction	O
w2	O
the	O
eigenvalue	O
λ2	O
is	O
large	O
compared	O
with	O
α	O
and	O
so	O
the	O
quantity	O
λ2/	O
(	O
λ2	O
+α	O
)	O
is	O
close	O
to	O
unity	O
,	O
and	O
the	O
map	O
value	O
of	O
w2	O
is	O
close	O
to	O
its	O
maximum	B
likelihood	I
value	O
.	O
w2	O
u2	O
wml	O
wmap	O
u1	O
w1	O
3.5.3	O
effective	B
number	I
of	I
parameters	I
the	O
result	O
(	O
3.92	O
)	O
has	O
an	O
elegant	O
interpretation	O
(	O
mackay	O
,	O
1992a	O
)	O
,	O
which	O
provides	O
insight	O
into	O
the	O
bayesian	O
solution	O
for	O
α.	O
to	O
see	O
this	O
,	O
consider	O
the	O
contours	O
of	O
the	O
like-	O
lihood	O
function	O
and	O
the	O
prior	B
as	O
illustrated	O
in	O
figure	O
3.15.	O
here	O
we	O
have	O
implicitly	O
transformed	O
to	O
a	O
rotated	O
set	O
of	O
axes	O
in	O
parameter	O
space	O
aligned	O
with	O
the	O
eigenvec-	O
tors	O
ui	O
deﬁned	O
in	O
(	O
3.87	O
)	O
.	O
contours	O
of	O
the	O
likelihood	B
function	I
are	O
then	O
axis-aligned	O
ellipses	O
.	O
the	O
eigenvalues	O
λi	O
measure	O
the	O
curvature	O
of	O
the	O
likelihood	B
function	I
,	O
and	O
so	O
in	O
figure	O
3.15	O
the	O
eigenvalue	O
λ1	O
is	O
small	O
compared	O
with	O
λ2	O
(	O
because	O
a	O
smaller	O
curvature	O
corresponds	O
to	O
a	O
greater	O
elongation	O
of	O
the	O
contours	O
of	O
the	O
likelihood	O
func-	O
tion	O
)	O
.	O
because	O
βφtφ	O
is	O
a	O
positive	B
deﬁnite	I
matrix	I
,	O
it	O
will	O
have	O
positive	O
eigenvalues	O
,	O
and	O
so	O
the	O
ratio	O
λi/	O
(	O
λi	O
+	O
α	O
)	O
will	O
lie	O
between	O
0	O
and	O
1.	O
consequently	O
,	O
the	O
quantity	O
γ	O
deﬁned	O
by	O
(	O
3.91	O
)	O
will	O
lie	O
in	O
the	O
range	O
0	O
(	O
cid:1	O
)	O
γ	O
(	O
cid:1	O
)	O
m.	O
for	O
directions	O
in	O
which	O
λi	O
(	O
cid:12	O
)	O
α	O
,	O
the	O
corresponding	O
parameter	O
wi	O
will	O
be	O
close	O
to	O
its	O
maximum	B
likelihood	I
value	O
,	O
and	O
the	O
ratio	O
λi/	O
(	O
λi	O
+	O
α	O
)	O
will	O
be	O
close	O
to	O
1.	O
such	O
parameters	O
are	O
called	O
well	O
determined	O
because	O
their	O
values	O
are	O
tightly	O
constrained	O
by	O
the	O
data	O
.	O
conversely	O
,	O
for	O
directions	O
in	O
which	O
λi	O
(	O
cid:13	O
)	O
α	O
,	O
the	O
corresponding	O
parameters	O
wi	O
will	O
be	O
close	O
to	O
zero	O
,	O
as	O
will	O
the	O
ratios	O
λi/	O
(	O
λi	O
+	O
α	O
)	O
.	O
these	O
are	O
directions	O
in	O
which	O
the	O
likelihood	B
function	I
is	O
relatively	O
insensitive	O
to	O
the	O
parameter	O
value	O
and	O
so	O
the	O
parameter	O
has	O
been	O
set	O
to	O
a	O
small	O
value	O
by	O
the	O
prior	B
.	O
the	O
quantity	O
γ	O
deﬁned	O
by	O
(	O
3.91	O
)	O
therefore	O
measures	O
the	O
effective	O
total	O
number	O
of	O
well	O
determined	O
parameters	O
.	O
we	O
can	O
obtain	O
some	O
insight	O
into	O
the	O
result	O
(	O
3.95	O
)	O
for	O
re-estimating	O
β	O
by	O
com-	O
paring	O
it	O
with	O
the	O
corresponding	O
maximum	B
likelihood	I
result	O
given	O
by	O
(	O
3.21	O
)	O
.	O
both	O
of	O
these	O
formulae	O
express	O
the	O
variance	B
(	O
the	O
inverse	B
precision	O
)	O
as	O
an	O
average	O
of	O
the	O
squared	O
differences	O
between	O
the	O
targets	O
and	O
the	O
model	O
predictions	O
.	O
however	O
,	O
they	O
differ	O
in	O
that	O
the	O
number	O
of	O
data	O
points	O
n	O
in	O
the	O
denominator	O
of	O
the	O
maximum	O
like-	O
lihood	O
result	O
is	O
replaced	O
by	O
n	O
−	O
γ	O
in	O
the	O
bayesian	O
result	O
.	O
we	O
recall	O
from	O
(	O
1.56	O
)	O
that	O
the	O
maximum	B
likelihood	I
estimate	O
of	O
the	O
variance	B
for	O
a	O
gaussian	O
distribution	O
over	O
a	O
3.5.	O
the	O
evidence	B
approximation	I
171	O
single	O
variable	O
x	O
is	O
given	O
by	O
σ2	O
ml	O
=	O
1	O
n	O
n	O
(	O
cid:2	O
)	O
(	O
xn	O
−	O
µml	O
)	O
2	O
n=1	O
(	O
3.96	O
)	O
and	O
that	O
this	O
estimate	O
is	O
biased	O
because	O
the	O
maximum	B
likelihood	I
solution	O
µml	O
for	O
the	O
mean	B
has	O
ﬁtted	O
some	O
of	O
the	O
noise	O
on	O
the	O
data	O
.	O
in	O
effect	O
,	O
this	O
has	O
used	O
up	O
one	O
degree	O
of	O
freedom	O
in	O
the	O
model	O
.	O
the	O
corresponding	O
unbiased	O
estimate	O
is	O
given	O
by	O
(	O
1.59	O
)	O
and	O
takes	O
the	O
form	O
n	O
(	O
cid:2	O
)	O
n=1	O
σ2	O
map	O
=	O
1	O
n	O
−	O
1	O
(	O
xn	O
−	O
µml	O
)	O
2	O
.	O
(	O
3.97	O
)	O
we	O
shall	O
see	O
in	O
section	O
10.1.3	O
that	O
this	O
result	O
can	O
be	O
obtained	O
from	O
a	O
bayesian	O
treat-	O
ment	O
in	O
which	O
we	O
marginalize	O
over	O
the	O
unknown	O
mean	B
.	O
the	O
factor	O
of	O
n	O
−	O
1	O
in	O
the	O
denominator	O
of	O
the	O
bayesian	O
result	O
takes	O
account	O
of	O
the	O
fact	O
that	O
one	O
degree	O
of	O
free-	O
dom	O
has	O
been	O
used	O
in	O
ﬁtting	O
the	O
mean	B
and	O
removes	O
the	O
bias	B
of	O
maximum	B
likelihood	I
.	O
now	O
consider	O
the	O
corresponding	O
results	O
for	O
the	O
linear	B
regression	I
model	O
.	O
the	O
mean	B
of	O
the	O
target	O
distribution	O
is	O
now	O
given	O
by	O
the	O
function	O
wtφ	O
(	O
x	O
)	O
,	O
which	O
contains	O
m	O
parameters	O
.	O
however	O
,	O
not	O
all	O
of	O
these	O
parameters	O
are	O
tuned	O
to	O
the	O
data	O
.	O
the	O
effective	B
number	I
of	I
parameters	I
that	O
are	O
determined	O
by	O
the	O
data	O
is	O
γ	O
,	O
with	O
the	O
remaining	O
m	O
−γ	O
parameters	O
set	O
to	O
small	O
values	O
by	O
the	O
prior	B
.	O
this	O
is	O
reﬂected	O
in	O
the	O
bayesian	O
result	O
for	O
the	O
variance	B
that	O
has	O
a	O
factor	O
n	O
−	O
γ	O
in	O
the	O
denominator	O
,	O
thereby	O
correcting	O
for	O
the	O
bias	B
of	O
the	O
maximum	B
likelihood	I
result	O
.	O
we	O
can	O
illustrate	O
the	O
evidence	O
framework	O
for	O
setting	O
hyperparameters	O
using	O
the	O
sinusoidal	O
synthetic	O
data	O
set	O
from	O
section	O
1.1	O
,	O
together	O
with	O
the	O
gaussian	O
basis	O
func-	O
tion	O
model	O
comprising	O
9	O
basis	O
functions	O
,	O
so	O
that	O
the	O
total	O
number	O
of	O
parameters	O
in	O
the	O
model	O
is	O
given	O
by	O
m	O
=	O
10	O
including	O
the	O
bias	B
.	O
here	O
,	O
for	O
simplicity	O
of	O
illustra-	O
tion	O
,	O
we	O
have	O
set	O
β	O
to	O
its	O
true	O
value	O
of	O
11.1	O
and	O
then	O
used	O
the	O
evidence	O
framework	O
to	O
determine	O
α	O
,	O
as	O
shown	O
in	O
figure	O
3.16.	O
we	O
can	O
also	O
see	O
how	O
the	O
parameter	O
α	O
controls	O
the	O
magnitude	O
of	O
the	O
parameters	O
{	O
wi	O
}	O
,	O
by	O
plotting	O
the	O
individual	O
parameters	O
versus	O
the	O
effective	O
number	O
γ	O
of	O
param-	O
eters	O
,	O
as	O
shown	O
in	O
figure	O
3.17.	O
if	O
we	O
consider	O
the	O
limit	O
n	O
(	O
cid:12	O
)	O
m	O
in	O
which	O
the	O
number	O
of	O
data	O
points	O
is	O
large	O
in	O
relation	O
to	O
the	O
number	O
of	O
parameters	O
,	O
then	O
from	O
(	O
3.87	O
)	O
all	O
of	O
the	O
parameters	O
will	O
be	O
well	O
determined	O
by	O
the	O
data	O
because	O
φtφ	O
involves	O
an	O
implicit	O
sum	O
over	O
data	O
points	O
,	O
and	O
so	O
the	O
eigenvalues	O
λi	O
increase	O
with	O
the	O
size	O
of	O
the	O
data	O
set	O
.	O
in	O
this	O
case	O
,	O
γ	O
=	O
m	O
,	O
and	O
the	O
re-estimation	O
equations	O
for	O
α	O
and	O
β	O
become	O
α	O
=	O
β	O
=	O
m	O
2ew	O
(	O
mn	O
)	O
n	O
2ed	O
(	O
mn	O
)	O
(	O
3.98	O
)	O
(	O
3.99	O
)	O
where	O
ew	O
and	O
ed	O
are	O
deﬁned	O
by	O
(	O
3.25	O
)	O
and	O
(	O
3.26	O
)	O
,	O
respectively	O
.	O
these	O
results	O
can	O
be	O
used	O
as	O
an	O
easy-to-compute	O
approximation	O
to	O
the	O
full	O
evidence	O
re-estimation	O
172	O
3.	O
linear	O
models	O
for	B
regression	I
−5	O
0	O
ln	O
α	O
5	O
−5	O
0	O
ln	O
α	O
5	O
figure	O
3.16	O
the	O
left	O
plot	O
shows	O
γ	O
(	O
red	O
curve	O
)	O
and	O
2αew	O
(	O
mn	O
)	O
(	O
blue	O
curve	O
)	O
versus	O
ln	O
α	O
for	O
the	O
sinusoidal	O
synthetic	O
data	O
set	O
.	O
it	O
is	O
the	O
intersection	O
of	O
these	O
two	O
curves	O
that	O
deﬁnes	O
the	O
optimum	O
value	O
for	O
α	O
given	O
by	O
the	O
evidence	O
procedure	O
.	O
the	O
right	O
plot	O
shows	O
the	O
corresponding	O
graph	O
of	O
log	O
evidence	O
ln	O
p	O
(	O
t|α	O
,	O
β	O
)	O
versus	O
ln	O
α	O
(	O
red	O
curve	O
)	O
showing	O
that	O
the	O
peak	O
coincides	O
with	O
the	O
crossing	O
point	O
of	O
the	O
curves	O
in	O
the	O
left	O
plot	O
.	O
also	O
shown	O
is	O
the	O
test	B
set	I
error	O
(	O
blue	O
curve	O
)	O
showing	O
that	O
the	O
evidence	O
maximum	O
occurs	O
close	O
to	O
the	O
point	O
of	O
best	O
generalization	B
.	O
formulae	O
,	O
because	O
they	O
do	O
not	O
require	O
evaluation	O
of	O
the	O
eigenvalue	O
spectrum	O
of	O
the	O
hessian	O
.	O
figure	O
3.17	O
plot	O
of	O
the	O
10	O
parameters	O
wi	O
from	O
the	O
gaussian	O
basis	B
function	I
model	O
versus	O
the	O
effective	O
num-	O
ber	O
of	O
parameters	O
γ	O
,	O
in	O
which	O
the	O
hyperparameter	B
α	O
is	O
varied	O
in	O
the	O
range	O
0	O
(	O
cid:1	O
)	O
α	O
(	O
cid:1	O
)	O
∞	O
causing	O
γ	O
to	O
vary	O
in	O
the	O
range	O
0	O
(	O
cid:1	O
)	O
γ	O
(	O
cid:1	O
)	O
m.	O
wi	O
2	O
1	O
0	O
−1	O
−2	O
0	O
8	O
4	O
5	O
2	O
6	O
3	O
1	O
7	O
9	O
0	O
2	O
4	O
6	O
8	O
γ	O
10	O
3.6.	O
limitations	O
of	O
fixed	O
basis	O
functions	O
throughout	O
this	O
chapter	O
,	O
we	O
have	O
focussed	O
on	O
models	O
comprising	O
a	O
linear	O
combina-	O
tion	O
of	O
ﬁxed	O
,	O
nonlinear	O
basis	O
functions	O
.	O
we	O
have	O
seen	O
that	O
the	O
assumption	O
of	O
linearity	O
in	O
the	O
parameters	O
led	O
to	O
a	O
range	O
of	O
useful	O
properties	O
including	O
closed-form	O
solutions	O
to	O
the	O
least-squares	O
problem	O
,	O
as	O
well	O
as	O
a	O
tractable	O
bayesian	O
treatment	O
.	O
furthermore	O
,	O
for	O
a	O
suitable	O
choice	O
of	O
basis	O
functions	O
,	O
we	O
can	O
model	O
arbitrary	O
nonlinearities	O
in	O
the	O
exercises	O
173	O
mapping	O
from	O
input	O
variables	O
to	O
targets	O
.	O
in	O
the	O
next	O
chapter	O
,	O
we	O
shall	O
study	O
an	O
anal-	O
ogous	O
class	O
of	O
models	O
for	O
classiﬁcation	O
.	O
it	O
might	O
appear	O
,	O
therefore	O
,	O
that	O
such	O
linear	O
models	O
constitute	O
a	O
general	O
purpose	O
framework	O
for	O
solving	O
problems	O
in	O
pattern	O
recognition	O
.	O
unfortunately	O
,	O
there	O
are	O
some	O
signiﬁcant	O
shortcomings	O
with	O
linear	O
models	O
,	O
which	O
will	O
cause	O
us	O
to	O
turn	O
in	O
later	O
chapters	O
to	O
more	O
complex	O
models	O
such	O
as	O
support	B
vector	I
machines	O
and	O
neural	O
networks	O
.	O
the	O
difﬁculty	O
stems	O
from	O
the	O
assumption	O
that	O
the	O
basis	O
functions	O
φj	O
(	O
x	O
)	O
are	O
ﬁxed	O
before	O
the	O
training	B
data	O
set	O
is	O
observed	O
and	O
is	O
a	O
manifestation	O
of	O
the	O
curse	O
of	O
dimen-	O
sionality	O
discussed	O
in	O
section	O
1.4.	O
as	O
a	O
consequence	O
,	O
the	O
number	O
of	O
basis	O
functions	O
needs	O
to	O
grow	O
rapidly	O
,	O
often	O
exponentially	O
,	O
with	O
the	O
dimensionality	O
d	O
of	O
the	O
input	O
space	O
.	O
fortunately	O
,	O
there	O
are	O
two	O
properties	O
of	O
real	O
data	O
sets	O
that	O
we	O
can	O
exploit	O
to	O
help	O
alleviate	O
this	O
problem	O
.	O
first	O
of	O
all	O
,	O
the	O
data	O
vectors	O
{	O
xn	O
}	O
typically	O
lie	O
close	O
to	O
a	O
non-	O
linear	O
manifold	O
whose	O
intrinsic	B
dimensionality	I
is	O
smaller	O
than	O
that	O
of	O
the	O
input	O
space	O
as	O
a	O
result	O
of	O
strong	O
correlations	O
between	O
the	O
input	O
variables	O
.	O
we	O
will	O
see	O
an	O
example	O
of	O
this	O
when	O
we	O
consider	O
images	O
of	O
handwritten	O
digits	O
in	O
chapter	O
12.	O
if	O
we	O
are	O
using	O
localized	O
basis	O
functions	O
,	O
we	O
can	O
arrange	O
that	O
they	O
are	O
scattered	O
in	O
input	O
space	O
only	O
in	O
regions	O
containing	O
data	O
.	O
this	O
approach	O
is	O
used	O
in	O
radial	B
basis	I
function	I
networks	O
and	O
also	O
in	O
support	B
vector	I
and	O
relevance	B
vector	I
machines	O
.	O
neural	B
network	I
models	O
,	O
which	O
use	O
adaptive	O
basis	O
functions	O
having	O
sigmoidal	O
nonlinearities	O
,	O
can	O
adapt	O
the	O
parameters	O
so	O
that	O
the	O
regions	O
of	O
input	O
space	O
over	O
which	O
the	O
basis	O
functions	O
vary	O
corresponds	O
to	O
the	O
data	O
manifold	O
.	O
the	O
second	O
property	O
is	O
that	O
target	O
variables	O
may	O
have	O
signiﬁcant	O
dependence	O
on	O
only	O
a	O
small	O
number	O
of	O
possible	O
directions	O
within	O
the	O
data	O
manifold	O
.	O
neural	O
networks	O
can	O
exploit	O
this	O
property	O
by	O
choosing	O
the	O
directions	O
in	O
input	O
space	O
to	O
which	O
the	O
basis	O
functions	O
respond	O
.	O
exercises	O
3.1	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
the	O
‘	O
tanh	O
’	O
function	O
and	O
the	O
logistic	B
sigmoid	I
function	O
(	O
3.6	O
)	O
are	O
related	O
by	O
tanh	O
(	O
a	O
)	O
=	O
2σ	O
(	O
2a	O
)	O
−	O
1.	O
hence	O
show	O
that	O
a	O
general	O
linear	O
combination	O
of	O
logistic	B
sigmoid	I
functions	O
of	O
the	O
form	O
m	O
(	O
cid:2	O
)	O
j=1	O
m	O
(	O
cid:2	O
)	O
j=1	O
(	O
cid:18	O
)	O
(	O
cid:18	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
x	O
−	O
µj	O
s	O
x	O
−	O
µj	O
s	O
(	O
3.100	O
)	O
(	O
3.101	O
)	O
(	O
3.102	O
)	O
is	O
equivalent	O
to	O
a	O
linear	O
combination	O
of	O
‘	O
tanh	O
’	O
functions	O
of	O
the	O
form	O
y	O
(	O
x	O
,	O
w	O
)	O
=	O
w0	O
+	O
wjσ	O
y	O
(	O
x	O
,	O
u	O
)	O
=	O
u0	O
+	O
uj	O
tanh	O
and	O
ﬁnd	O
expressions	O
to	O
relate	O
the	O
new	O
parameters	O
{	O
u1	O
,	O
.	O
.	O
.	O
,	O
um	O
}	O
to	O
the	O
original	O
pa-	O
rameters	O
{	O
w1	O
,	O
.	O
.	O
.	O
,	O
wm	O
}	O
.	O
174	O
3.	O
linear	O
models	O
for	B
regression	I
3.2	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
the	O
matrix	O
φ	O
(	O
φtφ	O
)	O
−1φt	O
(	O
3.103	O
)	O
takes	O
any	O
vector	O
v	O
and	O
projects	O
it	O
onto	O
the	O
space	O
spanned	O
by	O
the	O
columns	O
of	O
φ.	O
use	O
this	O
result	O
to	O
show	O
that	O
the	O
least-squares	O
solution	O
(	O
3.15	O
)	O
corresponds	O
to	O
an	O
orthogonal	O
projection	O
of	O
the	O
vector	O
t	O
onto	O
the	O
manifold	B
s	O
as	O
shown	O
in	O
figure	O
3.2	O
.	O
3.3	O
(	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
data	O
set	O
in	O
which	O
each	O
data	O
point	O
tn	O
is	O
associated	O
with	O
a	O
weighting	O
factor	O
rn	O
>	O
0	O
,	O
so	O
that	O
the	O
sum-of-squares	B
error	I
function	O
becomes	O
(	O
cid:26	O
)	O
n	O
(	O
cid:2	O
)	O
rn	O
n=1	O
ed	O
(	O
w	O
)	O
=	O
1	O
2	O
(	O
cid:27	O
)	O
2	O
tn	O
−	O
wtφ	O
(	O
xn	O
)	O
.	O
(	O
3.104	O
)	O
find	O
an	O
expression	O
for	O
the	O
solution	O
w	O
(	O
cid:1	O
)	O
that	O
minimizes	O
this	O
error	B
function	I
.	O
give	O
two	O
alternative	O
interpretations	O
of	O
the	O
weighted	O
sum-of-squares	O
error	B
function	I
in	O
terms	O
of	O
(	O
i	O
)	O
data	O
dependent	O
noise	O
variance	B
and	O
(	O
ii	O
)	O
replicated	O
data	O
points	O
.	O
3.4	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
linear	O
model	O
of	O
the	O
form	O
d	O
(	O
cid:2	O
)	O
y	O
(	O
x	O
,	O
w	O
)	O
=	O
w0	O
+	O
wixi	O
i=1	O
together	O
with	O
a	O
sum-of-squares	B
error	I
function	O
of	O
the	O
form	O
n	O
(	O
cid:2	O
)	O
n=1	O
ed	O
(	O
w	O
)	O
=	O
1	O
2	O
{	O
y	O
(	O
xn	O
,	O
w	O
)	O
−	O
tn	O
}	O
2	O
.	O
(	O
3.105	O
)	O
(	O
3.106	O
)	O
now	O
suppose	O
that	O
gaussian	O
noise	O
i	O
with	O
zero	O
mean	B
and	O
variance	B
σ2	O
is	O
added	O
in-	O
dependently	O
to	O
each	O
of	O
the	O
input	O
variables	O
xi	O
.	O
by	O
making	O
use	O
of	O
e	O
[	O
i	O
]	O
=	O
0	O
and	O
e	O
[	O
ij	O
]	O
=	O
δijσ2	O
,	O
show	O
that	O
minimizing	O
ed	O
averaged	O
over	O
the	O
noise	O
distribution	O
is	O
equivalent	O
to	O
minimizing	O
the	O
sum-of-squares	B
error	I
for	O
noise-free	O
input	O
variables	O
with	O
the	O
addition	O
of	O
a	O
weight-decay	O
regularization	B
term	O
,	O
in	O
which	O
the	O
bias	B
parameter	I
w0	O
is	O
omitted	O
from	O
the	O
regularizer	O
.	O
3.5	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
using	O
the	O
technique	O
of	O
lagrange	O
multipliers	O
,	O
discussed	O
in	O
appendix	O
e	O
,	O
show	O
that	O
minimization	O
of	O
the	O
regularized	O
error	O
function	O
(	O
3.29	O
)	O
is	O
equivalent	O
to	O
mini-	O
mizing	O
the	O
unregularized	O
sum-of-squares	B
error	I
(	O
3.12	O
)	O
subject	O
to	O
the	O
constraint	O
(	O
3.30	O
)	O
.	O
discuss	O
the	O
relationship	O
between	O
the	O
parameters	O
η	O
and	O
λ	O
.	O
3.6	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
linear	O
basis	O
function	O
regression	O
model	O
for	O
a	O
multivariate	O
target	O
variable	O
t	O
having	O
a	O
gaussian	O
distribution	O
of	O
the	O
form	O
p	O
(	O
t|w	O
,	O
σ	O
)	O
=	O
n	O
(	O
t|y	O
(	O
x	O
,	O
w	O
)	O
,	O
σ	O
)	O
where	O
y	O
(	O
x	O
,	O
w	O
)	O
=	O
wtφ	O
(	O
x	O
)	O
(	O
3.107	O
)	O
(	O
3.108	O
)	O
exercises	O
175	O
together	O
with	O
a	O
training	B
data	O
set	O
comprising	O
input	O
basis	O
vectors	O
φ	O
(	O
xn	O
)	O
and	O
corre-	O
sponding	O
target	O
vectors	O
tn	O
,	O
with	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
show	O
that	O
the	O
maximum	B
likelihood	I
solution	O
wml	O
for	O
the	O
parameter	O
matrix	O
w	O
has	O
the	O
property	O
that	O
each	O
column	O
is	O
given	O
by	O
an	O
expression	O
of	O
the	O
form	O
(	O
3.15	O
)	O
,	O
which	O
was	O
the	O
solution	O
for	O
an	O
isotropic	B
noise	O
distribution	O
.	O
note	O
that	O
this	O
is	O
independent	B
of	O
the	O
covariance	B
matrix	I
σ.	O
show	O
that	O
the	O
maximum	B
likelihood	I
solution	O
for	O
σ	O
is	O
given	O
by	O
n	O
(	O
cid:2	O
)	O
(	O
cid:10	O
)	O
n=1	O
σ	O
=	O
1	O
n	O
(	O
cid:11	O
)	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
t	O
tn	O
−	O
wt	O
mlφ	O
(	O
xn	O
)	O
tn	O
−	O
wt	O
mlφ	O
(	O
xn	O
)	O
.	O
(	O
3.109	O
)	O
3.7	O
(	O
(	O
cid:12	O
)	O
)	O
by	O
using	O
the	O
technique	O
of	O
completing	B
the	I
square	I
,	O
verify	O
the	O
result	O
(	O
3.49	O
)	O
for	O
the	O
posterior	O
distribution	O
of	O
the	O
parameters	O
w	O
in	O
the	O
linear	O
basis	O
function	O
model	O
in	O
which	O
mn	O
and	O
sn	O
are	O
deﬁned	O
by	O
(	O
3.50	O
)	O
and	O
(	O
3.51	O
)	O
respectively	O
.	O
3.8	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
the	O
linear	O
basis	O
function	O
model	O
in	O
section	O
3.1	O
,	O
and	O
suppose	O
that	O
we	O
have	O
already	O
observed	O
n	O
data	O
points	O
,	O
so	O
that	O
the	O
posterior	O
distribution	O
over	O
w	O
is	O
given	O
by	O
(	O
3.49	O
)	O
.	O
this	O
posterior	O
can	O
be	O
regarded	O
as	O
the	O
prior	B
for	O
the	O
next	O
obser-	O
vation	O
.	O
by	O
considering	O
an	O
additional	O
data	O
point	O
(	O
xn	O
+1	O
,	O
tn	O
+1	O
)	O
,	O
and	O
by	O
completing	B
the	I
square	I
in	O
the	O
exponential	O
,	O
show	O
that	O
the	O
resulting	O
posterior	O
distribution	O
is	O
again	O
given	O
by	O
(	O
3.49	O
)	O
but	O
with	O
sn	O
replaced	O
by	O
sn	O
+1	O
and	O
mn	O
replaced	O
by	O
mn	O
+1	O
.	O
3.9	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
repeat	O
the	O
previous	O
exercise	O
but	O
instead	O
of	O
completing	B
the	I
square	I
by	O
hand	O
,	O
make	O
use	O
of	O
the	O
general	O
result	O
for	O
linear-gaussian	O
models	O
given	O
by	O
(	O
2.116	O
)	O
.	O
3.10	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
by	O
making	O
use	O
of	O
the	O
result	O
(	O
2.115	O
)	O
to	O
evaluate	O
the	O
integral	O
in	O
(	O
3.57	O
)	O
,	O
verify	O
that	O
the	O
predictive	B
distribution	I
for	O
the	O
bayesian	O
linear	B
regression	I
model	O
is	O
given	O
by	O
(	O
3.58	O
)	O
in	O
which	O
the	O
input-dependent	O
variance	B
is	O
given	O
by	O
(	O
3.59	O
)	O
.	O
3.11	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
we	O
have	O
seen	O
that	O
,	O
as	O
the	O
size	O
of	O
a	O
data	O
set	O
increases	O
,	O
the	O
uncertainty	O
associated	O
with	O
the	O
posterior	O
distribution	O
over	O
model	O
parameters	O
decreases	O
.	O
make	O
use	O
of	O
the	O
matrix	O
identity	O
(	O
appendix	O
c	O
)	O
(	O
cid:10	O
)	O
m	O
+	O
vvt	O
(	O
cid:11	O
)	O
−1	O
=	O
m−1	O
−	O
(	O
m−1v	O
)	O
vtm−1	O
1	O
+	O
vtm−1v	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
to	O
show	O
that	O
the	O
uncertainty	O
σ2	O
given	O
by	O
(	O
3.59	O
)	O
satisﬁes	O
n	O
(	O
x	O
)	O
associated	O
with	O
the	O
linear	B
regression	I
function	O
n	O
+1	O
(	O
x	O
)	O
(	O
cid:1	O
)	O
σ2	O
σ2	O
n	O
(	O
x	O
)	O
.	O
(	O
3.110	O
)	O
(	O
3.111	O
)	O
3.12	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
we	O
saw	O
in	O
section	O
2.3.6	O
that	O
the	O
conjugate	B
prior	I
for	O
a	O
gaussian	O
distribution	O
with	O
unknown	O
mean	B
and	O
unknown	O
precision	O
(	O
inverse	B
variance	O
)	O
is	O
a	O
normal-gamma	B
distribution	I
.	O
this	O
property	O
also	O
holds	O
for	O
the	O
case	O
of	O
the	O
conditional	B
gaussian	O
dis-	O
tribution	O
p	O
(	O
t|x	O
,	O
w	O
,	O
β	O
)	O
of	O
the	O
linear	B
regression	I
model	O
.	O
if	O
we	O
consider	O
the	O
likelihood	B
function	I
(	O
3.10	O
)	O
,	O
then	O
the	O
conjugate	B
prior	I
for	O
w	O
and	O
β	O
is	O
given	O
by	O
−1s0	O
)	O
gam	O
(	O
β|a0	O
,	O
b0	O
)	O
.	O
p	O
(	O
w	O
,	O
β	O
)	O
=	O
n	O
(	O
w|m0	O
,	O
β	O
(	O
3.112	O
)	O
176	O
3.	O
linear	O
models	O
for	B
regression	I
show	O
that	O
the	O
corresponding	O
posterior	O
distribution	O
takes	O
the	O
same	O
functional	B
form	O
,	O
so	O
that	O
p	O
(	O
w	O
,	O
β|t	O
)	O
=	O
n	O
(	O
w|mn	O
,	O
β	O
−1sn	O
)	O
gam	O
(	O
β|an	O
,	O
bn	O
)	O
(	O
3.113	O
)	O
and	O
ﬁnd	O
expressions	O
for	O
the	O
posterior	O
parameters	O
mn	O
,	O
sn	O
,	O
an	O
,	O
and	O
bn	O
.	O
3.13	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
the	O
predictive	B
distribution	I
p	O
(	O
t|x	O
,	O
t	O
)	O
for	O
the	O
model	O
discussed	O
in	O
ex-	O
ercise	O
3.12	O
is	O
given	O
by	O
a	O
student	O
’	O
s	O
t-distribution	O
of	O
the	O
form	O
p	O
(	O
t|x	O
,	O
t	O
)	O
=	O
st	O
(	O
t|µ	O
,	O
λ	O
,	O
ν	O
)	O
(	O
3.114	O
)	O
and	O
obtain	O
expressions	O
for	O
µ	O
,	O
λ	O
and	O
ν	O
.	O
3.14	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
in	O
this	O
exercise	O
,	O
we	O
explore	O
in	O
more	O
detail	O
the	O
properties	O
of	O
the	O
equivalent	B
kernel	I
deﬁned	O
by	O
(	O
3.62	O
)	O
,	O
where	O
sn	O
is	O
deﬁned	O
by	O
(	O
3.54	O
)	O
.	O
suppose	O
that	O
the	O
basis	O
functions	O
φj	O
(	O
x	O
)	O
are	O
linearly	O
independent	O
and	O
that	O
the	O
number	O
n	O
of	O
data	O
points	O
is	O
greater	O
than	O
the	O
number	O
m	O
of	O
basis	O
functions	O
.	O
furthermore	O
,	O
let	O
one	O
of	O
the	O
basis	O
functions	O
be	O
constant	O
,	O
say	O
φ0	O
(	O
x	O
)	O
=	O
1.	O
by	O
taking	O
suitable	O
linear	O
combinations	O
of	O
these	O
basis	O
functions	O
,	O
we	O
can	O
construct	O
a	O
new	O
basis	O
set	O
ψj	O
(	O
x	O
)	O
spanning	O
the	O
same	O
space	O
but	O
that	O
are	O
orthonormal	O
,	O
so	O
that	O
n	O
(	O
cid:2	O
)	O
n=1	O
ψj	O
(	O
xn	O
)	O
ψk	O
(	O
xn	O
)	O
=	O
ijk	O
(	O
3.115	O
)	O
where	O
ijk	O
is	O
deﬁned	O
to	O
be	O
1	O
if	O
j	O
=	O
k	O
and	O
0	O
otherwise	O
,	O
and	O
we	O
take	O
ψ0	O
(	O
x	O
)	O
=	O
1.	O
show	O
that	O
for	O
α	O
=	O
0	O
,	O
the	O
equivalent	B
kernel	I
can	O
be	O
written	O
as	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
ψ	O
(	O
x	O
)	O
tψ	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
where	O
ψ	O
=	O
(	O
ψ1	O
,	O
.	O
.	O
.	O
,	O
ψm	O
)	O
t.	O
use	O
this	O
result	O
to	O
show	O
that	O
the	O
kernel	O
satisﬁes	O
the	O
summation	O
constraint	O
n	O
(	O
cid:2	O
)	O
k	O
(	O
x	O
,	O
xn	O
)	O
=	O
1	O
.	O
(	O
3.116	O
)	O
n=1	O
3.15	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
linear	O
basis	O
function	O
model	O
for	B
regression	I
in	O
which	O
the	O
pa-	O
rameters	O
α	O
and	O
β	O
are	O
set	O
using	O
the	O
evidence	O
framework	O
.	O
show	O
that	O
the	O
function	O
e	O
(	O
mn	O
)	O
deﬁned	O
by	O
(	O
3.82	O
)	O
satisﬁes	O
the	O
relation	O
2e	O
(	O
mn	O
)	O
=	O
n.	O
3.16	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
derive	O
the	O
result	O
(	O
3.86	O
)	O
for	O
the	O
log	O
evidence	O
function	O
p	O
(	O
t|α	O
,	O
β	O
)	O
of	O
the	O
linear	B
regression	I
model	O
by	O
making	O
use	O
of	O
(	O
2.115	O
)	O
to	O
evaluate	O
the	O
integral	O
(	O
3.77	O
)	O
directly	O
.	O
3.17	O
(	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
the	O
evidence	B
function	I
for	O
the	O
bayesian	O
linear	B
regression	I
model	O
can	O
be	O
written	O
in	O
the	O
form	O
(	O
3.78	O
)	O
in	O
which	O
e	O
(	O
w	O
)	O
is	O
deﬁned	O
by	O
(	O
3.79	O
)	O
.	O
3.18	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
by	O
completing	B
the	I
square	I
over	O
w	O
,	O
show	O
that	O
the	O
error	B
function	I
(	O
3.79	O
)	O
in	O
bayesian	O
linear	B
regression	I
can	O
be	O
written	O
in	O
the	O
form	O
(	O
3.80	O
)	O
.	O
3.19	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
the	O
integration	O
over	O
w	O
in	O
the	O
bayesian	O
linear	B
regression	I
model	O
gives	O
the	O
result	O
(	O
3.85	O
)	O
.	O
hence	O
show	O
that	O
the	O
log	O
marginal	O
likelihood	O
is	O
given	O
by	O
(	O
3.86	O
)	O
.	O
3.20	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
starting	O
from	O
(	O
3.86	O
)	O
verify	O
all	O
of	O
the	O
steps	O
needed	O
to	O
show	O
that	O
maxi-	O
mization	O
of	O
the	O
log	O
marginal	O
likelihood	B
function	I
(	O
3.86	O
)	O
with	O
respect	O
to	O
α	O
leads	O
to	O
the	O
re-estimation	O
equation	O
(	O
3.92	O
)	O
.	O
exercises	O
177	O
3.21	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
an	O
alternative	O
way	O
to	O
derive	O
the	O
result	O
(	O
3.92	O
)	O
for	O
the	O
optimal	O
value	O
of	O
α	O
in	O
the	O
evidence	O
framework	O
is	O
to	O
make	O
use	O
of	O
the	O
identity	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
ln|a|	O
=	O
tr	O
d	O
dα	O
a−1	O
d	O
dα	O
a	O
.	O
(	O
3.117	O
)	O
prove	O
this	O
identity	O
by	O
considering	O
the	O
eigenvalue	O
expansion	O
of	O
a	O
real	O
,	O
symmetric	O
matrix	O
a	O
,	O
and	O
making	O
use	O
of	O
the	O
standard	O
results	O
for	O
the	O
determinant	O
and	O
trace	O
of	O
a	O
expressed	O
in	O
terms	O
of	O
its	O
eigenvalues	O
(	O
appendix	O
c	O
)	O
.	O
then	O
make	O
use	O
of	O
(	O
3.117	O
)	O
to	O
derive	O
(	O
3.92	O
)	O
starting	O
from	O
(	O
3.86	O
)	O
.	O
3.22	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
starting	O
from	O
(	O
3.86	O
)	O
verify	O
all	O
of	O
the	O
steps	O
needed	O
to	O
show	O
that	O
maximiza-	O
tion	O
of	O
the	O
log	O
marginal	O
likelihood	B
function	I
(	O
3.86	O
)	O
with	O
respect	O
to	O
β	O
leads	O
to	O
the	O
re-estimation	O
equation	O
(	O
3.95	O
)	O
.	O
3.23	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
the	O
marginal	B
probability	I
of	O
the	O
data	O
,	O
in	O
other	O
words	O
the	O
model	B
evidence	I
,	O
for	O
the	O
model	O
described	O
in	O
exercise	O
3.12	O
is	O
given	O
by	O
p	O
(	O
t	O
)	O
=	O
1	O
(	O
2π	O
)	O
n/2	O
ba0	O
0	O
ban	O
n	O
γ	O
(	O
an	O
)	O
γ	O
(	O
a0	O
)	O
|sn|1/2	O
|s0|1/2	O
by	O
ﬁrst	O
marginalizing	O
with	O
respect	O
to	O
w	O
and	O
then	O
with	O
respect	O
to	O
β	O
.	O
3.24	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
repeat	O
the	O
previous	O
exercise	O
but	O
now	O
use	O
bayes	O
’	O
theorem	O
in	O
the	O
form	O
p	O
(	O
t	O
)	O
=	O
p	O
(	O
t|w	O
,	O
β	O
)	O
p	O
(	O
w	O
,	O
β	O
)	O
p	O
(	O
w	O
,	O
β|t	O
)	O
(	O
3.118	O
)	O
(	O
3.119	O
)	O
and	O
then	O
substitute	O
for	O
the	O
prior	B
and	O
posterior	O
distributions	O
and	O
the	O
likelihood	O
func-	O
tion	O
in	O
order	O
to	O
derive	O
the	O
result	O
(	O
3.118	O
)	O
.	O
4	O
linear	O
models	O
for	O
classiﬁcation	O
in	O
the	O
previous	O
chapter	O
,	O
we	O
explored	O
a	O
class	O
of	O
regression	B
models	O
having	O
particularly	O
simple	O
analytical	O
and	O
computational	O
properties	O
.	O
we	O
now	O
discuss	O
an	O
analogous	O
class	O
of	O
models	O
for	O
solving	O
classiﬁcation	B
problems	O
.	O
the	O
goal	O
in	O
classiﬁcation	B
is	O
to	O
take	O
an	O
input	O
vector	O
x	O
and	O
to	O
assign	O
it	O
to	O
one	O
of	O
k	O
discrete	O
classes	O
ck	O
where	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k.	O
in	O
the	O
most	O
common	O
scenario	O
,	O
the	O
classes	O
are	O
taken	O
to	O
be	O
disjoint	O
,	O
so	O
that	O
each	O
input	O
is	O
assigned	O
to	O
one	O
and	O
only	O
one	O
class	O
.	O
the	O
input	O
space	O
is	O
thereby	O
divided	O
into	O
decision	O
regions	O
whose	O
boundaries	O
are	O
called	O
decision	O
boundaries	O
or	O
decision	O
surfaces	O
.	O
in	O
this	O
chapter	O
,	O
we	O
consider	O
linear	O
models	O
for	O
classiﬁcation	O
,	O
by	O
which	O
we	O
mean	B
that	O
the	O
decision	O
surfaces	O
are	O
linear	O
functions	O
of	O
the	O
input	O
vector	O
x	O
and	O
hence	O
are	O
deﬁned	O
by	O
(	O
d	O
−	O
1	O
)	O
-dimensional	O
hyperplanes	O
within	O
the	O
d-dimensional	O
input	O
space	O
.	O
data	O
sets	O
whose	O
classes	O
can	O
be	O
separated	O
exactly	O
by	O
linear	O
decision	O
surfaces	O
are	O
said	O
to	O
be	O
linearly	B
separable	I
.	O
for	B
regression	I
problems	O
,	O
the	O
target	O
variable	O
t	O
was	O
simply	O
the	O
vector	O
of	O
real	O
num-	O
bers	O
whose	O
values	O
we	O
wish	O
to	O
predict	O
.	O
in	O
the	O
case	O
of	O
classiﬁcation	B
,	O
there	O
are	O
various	O
179	O
180	O
4.	O
linear	O
models	O
for	O
classification	O
ways	O
of	O
using	O
target	O
values	O
to	O
represent	O
class	O
labels	O
.	O
for	O
probabilistic	O
models	O
,	O
the	O
most	O
convenient	O
,	O
in	O
the	O
case	O
of	O
two-class	O
problems	O
,	O
is	O
the	O
binary	O
representation	O
in	O
which	O
there	O
is	O
a	O
single	O
target	O
variable	O
t	O
∈	O
{	O
0	O
,	O
1	O
}	O
such	O
that	O
t	O
=	O
1	O
represents	O
class	O
c1	O
and	O
t	O
=	O
0	O
represents	O
class	O
c2	O
.	O
we	O
can	O
interpret	O
the	O
value	O
of	O
t	O
as	O
the	O
probability	B
that	O
the	O
class	O
is	O
c1	O
,	O
with	O
the	O
values	O
of	O
probability	B
taking	O
only	O
the	O
extreme	O
values	O
of	O
0	O
and	O
1.	O
for	O
k	O
>	O
2	O
classes	O
,	O
it	O
is	O
convenient	O
to	O
use	O
a	O
1-of-k	O
coding	O
scheme	O
in	O
which	O
t	O
is	O
a	O
vector	O
of	O
length	O
k	O
such	O
that	O
if	O
the	O
class	O
is	O
cj	O
,	O
then	O
all	O
elements	O
tk	O
of	O
t	O
are	O
zero	O
except	O
element	O
tj	O
,	O
which	O
takes	O
the	O
value	O
1.	O
for	O
instance	O
,	O
if	O
we	O
have	O
k	O
=	O
5	O
classes	O
,	O
then	O
a	O
pattern	O
from	O
class	O
2	O
would	O
be	O
given	O
the	O
target	B
vector	I
t	O
=	O
(	O
0	O
,	O
1	O
,	O
0	O
,	O
0	O
,	O
0	O
)	O
t.	O
(	O
4.1	O
)	O
again	O
,	O
we	O
can	O
interpret	O
the	O
value	O
of	O
tk	O
as	O
the	O
probability	B
that	O
the	O
class	O
is	O
ck	O
.	O
for	O
nonprobabilistic	O
models	O
,	O
alternative	O
choices	O
of	O
target	O
variable	O
representation	O
will	O
sometimes	O
prove	O
convenient	O
.	O
in	O
chapter	O
1	O
,	O
we	O
identiﬁed	O
three	O
distinct	O
approaches	O
to	O
the	O
classiﬁcation	B
prob-	O
lem	O
.	O
the	O
simplest	O
involves	O
constructing	O
a	O
discriminant	B
function	I
that	O
directly	O
assigns	O
each	O
vector	O
x	O
to	O
a	O
speciﬁc	O
class	O
.	O
a	O
more	O
powerful	O
approach	O
,	O
however	O
,	O
models	O
the	O
conditional	B
probability	I
distribution	O
p	O
(	O
ck|x	O
)	O
in	O
an	O
inference	B
stage	O
,	O
and	O
then	O
subse-	O
quently	O
uses	O
this	O
distribution	O
to	O
make	O
optimal	O
decisions	O
.	O
by	O
separating	O
inference	B
and	O
decision	O
,	O
we	O
gain	O
numerous	O
beneﬁts	O
,	O
as	O
discussed	O
in	O
section	O
1.5.4.	O
there	O
are	O
two	O
different	O
approaches	O
to	O
determining	O
the	O
conditional	B
probabilities	O
p	O
(	O
ck|x	O
)	O
.	O
one	O
technique	O
is	O
to	O
model	O
them	O
directly	O
,	O
for	O
example	O
by	O
representing	O
them	O
as	O
parametric	O
models	O
and	O
then	O
optimizing	O
the	O
parameters	O
using	O
a	O
training	B
set	I
.	O
alternatively	O
,	O
we	O
can	O
adopt	O
a	O
generative	O
approach	O
in	O
which	O
we	O
model	O
the	O
class-conditional	O
densities	O
given	O
by	O
p	O
(	O
x|ck	O
)	O
,	O
together	O
with	O
the	O
prior	B
probabilities	O
p	O
(	O
ck	O
)	O
for	O
the	O
classes	O
,	O
and	O
then	O
we	O
compute	O
the	O
required	O
posterior	O
probabilities	O
using	O
bayes	O
’	O
theorem	O
p	O
(	O
ck|x	O
)	O
=	O
p	O
(	O
x|ck	O
)	O
p	O
(	O
ck	O
)	O
p	O
(	O
x	O
)	O
.	O
(	O
4.2	O
)	O
we	O
shall	O
discuss	O
examples	O
of	O
all	O
three	O
approaches	O
in	O
this	O
chapter	O
.	O
in	O
the	O
linear	B
regression	I
models	O
considered	O
in	O
chapter	O
3	O
,	O
the	O
model	O
prediction	O
y	O
(	O
x	O
,	O
w	O
)	O
was	O
given	O
by	O
a	O
linear	O
function	O
of	O
the	O
parameters	O
w.	O
in	O
the	O
simplest	O
case	O
,	O
the	O
model	O
is	O
also	O
linear	O
in	O
the	O
input	O
variables	O
and	O
therefore	O
takes	O
the	O
form	O
y	O
(	O
x	O
)	O
=	O
wtx	O
+	O
w0	O
,	O
so	O
that	O
y	O
is	O
a	O
real	O
number	O
.	O
for	O
classiﬁcation	O
problems	O
,	O
however	O
,	O
we	O
wish	O
to	O
predict	O
discrete	O
class	O
labels	O
,	O
or	O
more	O
generally	O
posterior	O
probabilities	O
that	O
lie	O
in	O
the	O
range	O
(	O
0	O
,	O
1	O
)	O
.	O
to	O
achieve	O
this	O
,	O
we	O
consider	O
a	O
generalization	B
of	O
this	O
model	O
in	O
which	O
we	O
transform	O
the	O
linear	O
function	O
of	O
w	O
using	O
a	O
nonlinear	O
function	O
f	O
(	O
·	O
)	O
so	O
that	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
y	O
(	O
x	O
)	O
=	O
f	O
wtx	O
+	O
w0	O
(	O
4.3	O
)	O
in	O
the	O
machine	O
learning	O
literature	O
f	O
(	O
·	O
)	O
is	O
known	O
as	O
an	O
activation	B
function	I
,	O
whereas	O
its	O
inverse	B
is	O
called	O
a	O
link	B
function	I
in	O
the	O
statistics	O
literature	O
.	O
the	O
decision	O
surfaces	O
correspond	O
to	O
y	O
(	O
x	O
)	O
=	O
constant	O
,	O
so	O
that	O
wtx	O
+	O
w0	O
=	O
constant	O
and	O
hence	O
the	O
deci-	O
sion	B
surfaces	O
are	O
linear	O
functions	O
of	O
x	O
,	O
even	O
if	O
the	O
function	O
f	O
(	O
·	O
)	O
is	O
nonlinear	O
.	O
for	O
this	O
reason	O
,	O
the	O
class	O
of	O
models	O
described	O
by	O
(	O
4.3	O
)	O
are	O
called	O
generalized	O
linear	O
models	O
.	O
4.1.	O
discriminant	O
functions	O
181	O
(	O
mccullagh	O
and	O
nelder	O
,	O
1989	O
)	O
.	O
note	O
,	O
however	O
,	O
that	O
in	O
contrast	O
to	O
the	O
models	O
used	O
for	B
regression	I
,	O
they	O
are	O
no	O
longer	O
linear	O
in	O
the	O
parameters	O
due	O
to	O
the	O
presence	O
of	O
the	O
nonlinear	O
function	O
f	O
(	O
·	O
)	O
.	O
this	O
will	O
lead	O
to	O
more	O
complex	O
analytical	O
and	O
computa-	O
tional	O
properties	O
than	O
for	O
linear	O
regression	B
models	O
.	O
nevertheless	O
,	O
these	O
models	O
are	O
still	O
relatively	O
simple	O
compared	O
to	O
the	O
more	O
general	O
nonlinear	O
models	O
that	O
will	O
be	O
studied	O
in	O
subsequent	O
chapters	O
.	O
the	O
algorithms	O
discussed	O
in	O
this	O
chapter	O
will	O
be	O
equally	O
applicable	O
if	O
we	O
ﬁrst	O
make	O
a	O
ﬁxed	O
nonlinear	O
transformation	O
of	O
the	O
input	O
variables	O
using	O
a	O
vector	O
of	O
basis	O
functions	O
φ	O
(	O
x	O
)	O
as	O
we	O
did	O
for	B
regression	I
models	O
in	O
chapter	O
3.	O
we	O
begin	O
by	O
consider-	O
ing	O
classiﬁcation	O
directly	O
in	O
the	O
original	O
input	O
space	O
x	O
,	O
while	O
in	O
section	O
4.3	O
we	O
shall	O
ﬁnd	O
it	O
convenient	O
to	O
switch	O
to	O
a	O
notation	O
involving	O
basis	O
functions	O
for	O
consistency	O
with	O
later	O
chapters	O
.	O
4.1.	O
discriminant	O
functions	O
a	O
discriminant	O
is	O
a	O
function	O
that	O
takes	O
an	O
input	O
vector	O
x	O
and	O
assigns	O
it	O
to	O
one	O
of	O
k	O
classes	O
,	O
denoted	O
ck	O
.	O
in	O
this	O
chapter	O
,	O
we	O
shall	O
restrict	O
attention	O
to	O
linear	O
discriminants	O
,	O
namely	O
those	O
for	O
which	O
the	O
decision	O
surfaces	O
are	O
hyperplanes	O
.	O
to	O
simplify	O
the	O
dis-	O
cussion	O
,	O
we	O
consider	O
ﬁrst	O
the	O
case	O
of	O
two	O
classes	O
and	O
then	O
investigate	O
the	O
extension	O
to	O
k	O
>	O
2	O
classes	O
.	O
4.1.1	O
two	O
classes	O
the	O
simplest	O
representation	O
of	O
a	O
linear	B
discriminant	I
function	O
is	O
obtained	O
by	O
tak-	O
ing	O
a	O
linear	O
function	O
of	O
the	O
input	O
vector	O
so	O
that	O
y	O
(	O
x	O
)	O
=	O
wtx	O
+	O
w0	O
(	O
4.4	O
)	O
where	O
w	O
is	O
called	O
a	O
weight	B
vector	I
,	O
and	O
w0	O
is	O
a	O
bias	B
(	O
not	O
to	O
be	O
confused	O
with	O
bias	B
in	O
the	O
statistical	O
sense	O
)	O
.	O
the	O
negative	O
of	O
the	O
bias	B
is	O
sometimes	O
called	O
a	O
threshold	O
.	O
an	O
input	O
vector	O
x	O
is	O
assigned	O
to	O
class	O
c1	O
if	O
y	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
0	O
and	O
to	O
class	O
c2	O
otherwise	O
.	O
the	O
cor-	O
responding	O
decision	B
boundary	I
is	O
therefore	O
deﬁned	O
by	O
the	O
relation	O
y	O
(	O
x	O
)	O
=	O
0	O
,	O
which	O
corresponds	O
to	O
a	O
(	O
d	O
−	O
1	O
)	O
-dimensional	O
hyperplane	O
within	O
the	O
d-dimensional	O
input	O
space	O
.	O
consider	O
two	O
points	O
xa	O
and	O
xb	O
both	O
of	O
which	O
lie	O
on	O
the	O
decision	B
surface	I
.	O
because	O
y	O
(	O
xa	O
)	O
=	O
y	O
(	O
xb	O
)	O
=	O
0	O
,	O
we	O
have	O
wt	O
(	O
xa	O
−	O
xb	O
)	O
=	O
0	O
and	O
hence	O
the	O
vector	O
w	O
is	O
orthogonal	O
to	O
every	O
vector	O
lying	O
within	O
the	O
decision	B
surface	I
,	O
and	O
so	O
w	O
determines	O
the	O
orientation	O
of	O
the	O
decision	B
surface	I
.	O
similarly	O
,	O
if	O
x	O
is	O
a	O
point	O
on	O
the	O
decision	B
surface	I
,	O
then	O
y	O
(	O
x	O
)	O
=	O
0	O
,	O
and	O
so	O
the	O
normal	O
distance	O
from	O
the	O
origin	O
to	O
the	O
decision	B
surface	I
is	O
given	O
by	O
(	O
4.5	O
)	O
wtx	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
=	O
−	O
w0	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
.	O
we	O
therefore	O
see	O
that	O
the	O
bias	B
parameter	I
w0	O
determines	O
the	O
location	O
of	O
the	O
decision	B
surface	I
.	O
these	O
properties	O
are	O
illustrated	O
for	O
the	O
case	O
of	O
d	O
=	O
2	O
in	O
figure	O
4.1.	O
furthermore	O
,	O
we	O
note	O
that	O
the	O
value	O
of	O
y	O
(	O
x	O
)	O
gives	O
a	O
signed	O
measure	O
of	O
the	O
per-	O
pendicular	O
distance	O
r	O
of	O
the	O
point	O
x	O
from	O
the	O
decision	B
surface	I
.	O
to	O
see	O
this	O
,	O
consider	O
182	O
4.	O
linear	O
models	O
for	O
classification	O
figure	O
4.1	O
illustration	O
of	O
the	O
geometry	O
of	O
a	O
linear	B
discriminant	I
function	O
in	O
two	O
dimensions	O
.	O
the	O
decision	B
surface	I
,	O
shown	O
in	O
red	O
,	O
is	O
perpen-	O
dicular	O
to	O
w	O
,	O
and	O
its	O
displacement	O
from	O
the	O
origin	O
is	O
controlled	O
by	O
the	O
bias	B
parameter	I
w0	O
.	O
also	O
,	O
the	O
signed	O
orthogonal	O
distance	O
of	O
a	O
gen-	O
eral	O
point	O
x	O
from	O
the	O
decision	B
surface	I
is	O
given	O
by	O
y	O
(	O
x	O
)	O
/	O
(	O
cid:3	O
)	O
w	O
(	O
cid:3	O
)	O
.	O
y	O
>	O
0	O
y	O
=	O
0	O
y	O
<	O
0	O
x2	O
r1	O
r2	O
w	O
x	O
y	O
(	O
x	O
)	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
x1	O
x⊥	O
−w0	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
an	O
arbitrary	O
point	O
x	O
and	O
let	O
x⊥	O
be	O
its	O
orthogonal	O
projection	O
onto	O
the	O
decision	B
surface	I
,	O
so	O
that	O
x	O
=	O
x⊥	O
+	O
r	O
w	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
.	O
(	O
4.6	O
)	O
(	O
4.8	O
)	O
multiplying	O
both	O
sides	O
of	O
this	O
result	O
by	O
wt	O
and	O
adding	O
w0	O
,	O
and	O
making	O
use	O
of	O
y	O
(	O
x	O
)	O
=	O
wtx	O
+	O
w0	O
and	O
y	O
(	O
x⊥	O
)	O
=	O
wtx⊥	O
+	O
w0	O
=	O
0	O
,	O
we	O
have	O
r	O
=	O
y	O
(	O
x	O
)	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
.	O
(	O
4.7	O
)	O
this	O
result	O
is	O
illustrated	O
in	O
figure	O
4.1.	O
as	O
with	O
the	O
linear	B
regression	I
models	O
in	O
chapter	O
3	O
,	O
it	O
is	O
sometimes	O
convenient	O
to	O
use	O
a	O
more	O
compact	O
notation	O
in	O
which	O
we	O
introduce	O
an	O
additional	O
dummy	O
‘	O
input	O
’	O
value	O
x0	O
=	O
1	O
and	O
then	O
deﬁne	O
(	O
cid:4	O
)	O
w	O
=	O
(	O
w0	O
,	O
w	O
)	O
and	O
(	O
cid:4	O
)	O
x	O
=	O
(	O
x0	O
,	O
x	O
)	O
so	O
that	O
y	O
(	O
x	O
)	O
=	O
(	O
cid:4	O
)	O
wt	O
(	O
cid:4	O
)	O
x.	O
in	O
this	O
case	O
,	O
the	O
decision	O
surfaces	O
are	O
d-dimensional	O
hyperplanes	O
passing	O
through	O
the	O
origin	O
of	O
the	O
d	O
+	O
1-dimensional	O
expanded	O
input	O
space	O
.	O
4.1.2	O
multiple	O
classes	O
now	O
consider	O
the	O
extension	O
of	O
linear	O
discriminants	O
to	O
k	O
>	O
2	O
classes	O
.	O
we	O
might	O
be	O
tempted	O
be	O
to	O
build	O
a	O
k-class	O
discriminant	O
by	O
combining	O
a	O
number	O
of	O
two-class	O
discriminant	O
functions	O
.	O
however	O
,	O
this	O
leads	O
to	O
some	O
serious	O
difﬁculties	O
(	O
duda	O
and	O
hart	O
,	O
1973	O
)	O
as	O
we	O
now	O
show	O
.	O
consider	O
the	O
use	O
of	O
k−1	O
classiﬁers	O
each	O
of	O
which	O
solves	O
a	O
two-class	O
problem	O
of	O
separating	O
points	O
in	O
a	O
particular	O
class	O
ck	O
from	O
points	O
not	O
in	O
that	O
class	O
.	O
this	O
is	O
known	O
as	O
a	O
one-versus-the-rest	B
classiﬁer	I
.	O
the	O
left-hand	O
example	O
in	O
figure	O
4.2	O
shows	O
an	O
4.1.	O
discriminant	O
functions	O
183	O
c3	O
r3	O
c1	O
r1	O
c1	O
c2	O
?	O
r2	O
c3	O
c2	O
?	O
r1	O
not	O
c1	O
r3	O
not	O
c2	O
r2	O
c2	O
c1	O
figure	O
4.2	O
attempting	O
to	O
construct	O
a	O
k	O
class	O
discriminant	O
from	O
a	O
set	O
of	O
two	O
class	O
discriminants	O
leads	O
to	O
am-	O
biguous	O
regions	O
,	O
shown	O
in	O
green	O
.	O
on	O
the	O
left	O
is	O
an	O
example	O
involving	O
the	O
use	O
of	O
two	O
discriminants	O
designed	O
to	O
distinguish	O
points	O
in	O
class	O
ck	O
from	O
points	O
not	O
in	O
class	O
ck	O
.	O
on	O
the	O
right	O
is	O
an	O
example	O
involving	O
three	O
discriminant	O
functions	O
each	O
of	O
which	O
is	O
used	O
to	O
separate	O
a	O
pair	O
of	O
classes	O
ck	O
and	O
cj	O
.	O
example	O
involving	O
three	O
classes	O
where	O
this	O
approach	O
leads	O
to	O
regions	O
of	O
input	O
space	O
that	O
are	O
ambiguously	O
classiﬁed	O
.	O
an	O
alternative	O
is	O
to	O
introduce	O
k	O
(	O
k	O
−	O
1	O
)	O
/2	O
binary	O
discriminant	O
functions	O
,	O
one	O
for	O
every	O
possible	O
pair	O
of	O
classes	O
.	O
this	O
is	O
known	O
as	O
a	O
one-versus-one	B
classiﬁer	I
.	O
each	O
point	O
is	O
then	O
classiﬁed	O
according	O
to	O
a	O
majority	O
vote	O
amongst	O
the	O
discriminant	O
func-	O
tions	O
.	O
however	O
,	O
this	O
too	O
runs	O
into	O
the	O
problem	O
of	O
ambiguous	O
regions	O
,	O
as	O
illustrated	O
in	O
the	O
right-hand	O
diagram	O
of	O
figure	O
4.2.	O
we	O
can	O
avoid	O
these	O
difﬁculties	O
by	O
considering	O
a	O
single	O
k-class	O
discriminant	O
comprising	O
k	O
linear	O
functions	O
of	O
the	O
form	O
yk	O
(	O
x	O
)	O
=	O
wt	O
(	O
4.9	O
)	O
and	O
then	O
assigning	O
a	O
point	O
x	O
to	O
class	O
ck	O
if	O
yk	O
(	O
x	O
)	O
>	O
yj	O
(	O
x	O
)	O
for	O
all	O
j	O
(	O
cid:9	O
)	O
=	O
k.	O
the	O
decision	B
boundary	I
between	O
class	O
ck	O
and	O
class	O
cj	O
is	O
therefore	O
given	O
by	O
yk	O
(	O
x	O
)	O
=	O
yj	O
(	O
x	O
)	O
and	O
hence	O
corresponds	O
to	O
a	O
(	O
d	O
−	O
1	O
)	O
-dimensional	O
hyperplane	O
deﬁned	O
by	O
k	O
x	O
+	O
wk0	O
(	O
wk	O
−	O
wj	O
)	O
tx	O
+	O
(	O
wk0	O
−	O
wj0	O
)	O
=	O
0	O
.	O
(	O
4.10	O
)	O
this	O
has	O
the	O
same	O
form	O
as	O
the	O
decision	B
boundary	I
for	O
the	O
two-class	O
case	O
discussed	O
in	O
section	O
4.1.1	O
,	O
and	O
so	O
analogous	O
geometrical	O
properties	O
apply	O
.	O
the	O
decision	O
regions	O
of	O
such	O
a	O
discriminant	O
are	O
always	O
singly	O
connected	O
and	O
convex	O
.	O
to	O
see	O
this	O
,	O
consider	O
two	O
points	O
xa	O
and	O
xb	O
both	O
of	O
which	O
lie	O
inside	O
decision	B
region	I
rk	O
,	O
as	O
illustrated	O
in	O
figure	O
4.3.	O
any	O
point	O
(	O
cid:1	O
)	O
x	O
that	O
lies	O
on	O
the	O
line	O
connecting	O
xa	O
and	O
xb	O
can	O
be	O
expressed	O
in	O
the	O
form	O
(	O
cid:1	O
)	O
x	O
=	O
λxa	O
+	O
(	O
1	O
−	O
λ	O
)	O
xb	O
(	O
4.11	O
)	O
184	O
4.	O
linear	O
models	O
for	O
classification	O
figure	O
4.3	O
illustration	O
of	O
the	O
decision	O
regions	O
for	O
a	O
mul-	O
ticlass	O
linear	B
discriminant	I
,	O
with	O
the	O
decision	O
if	O
two	O
points	O
xa	O
boundaries	O
shown	O
in	O
red	O
.	O
and	O
xb	O
both	O
lie	O
inside	O
the	O
same	O
decision	O
re-	O
gion	O
rk	O
,	O
then	O
any	O
point	O
bx	O
that	O
lies	O
on	O
the	O
line	O
connecting	O
these	O
two	O
points	O
must	O
also	O
lie	O
in	O
rk	O
,	O
and	O
hence	O
the	O
decision	B
region	I
must	O
be	O
singly	O
connected	O
and	O
convex	O
.	O
ri	O
xa	O
rj	O
rk	O
ˆx	O
xb	O
where	O
0	O
(	O
cid:1	O
)	O
λ	O
(	O
cid:1	O
)	O
1.	O
from	O
the	O
linearity	O
of	O
the	O
discriminant	O
functions	O
,	O
it	O
follows	O
that	O
yk	O
(	O
(	O
cid:1	O
)	O
x	O
)	O
=	O
λyk	O
(	O
xa	O
)	O
+	O
(	O
1	O
−	O
λ	O
)	O
yk	O
(	O
xb	O
)	O
.	O
yk	O
(	O
xb	O
)	O
>	O
yj	O
(	O
xb	O
)	O
,	O
for	O
all	O
j	O
(	O
cid:9	O
)	O
=	O
k	O
,	O
and	O
hence	O
yk	O
(	O
(	O
cid:1	O
)	O
x	O
)	O
>	O
yj	O
(	O
(	O
cid:1	O
)	O
x	O
)	O
,	O
and	O
so	O
(	O
cid:1	O
)	O
x	O
also	O
lies	O
(	O
4.12	O
)	O
because	O
both	O
xa	O
and	O
xb	O
lie	O
inside	O
rk	O
,	O
it	O
follows	O
that	O
yk	O
(	O
xa	O
)	O
>	O
yj	O
(	O
xa	O
)	O
,	O
and	O
inside	O
rk	O
.	O
thus	O
rk	O
is	O
singly	O
connected	O
and	O
convex	O
.	O
note	O
that	O
for	O
two	O
classes	O
,	O
we	O
can	O
either	O
employ	O
the	O
formalism	O
discussed	O
here	O
,	O
based	O
on	O
two	O
discriminant	O
functions	O
y1	O
(	O
x	O
)	O
and	O
y2	O
(	O
x	O
)	O
,	O
or	O
else	O
use	O
the	O
simpler	O
but	O
equivalent	O
formulation	O
described	O
in	O
section	O
4.1.1	O
based	O
on	O
a	O
single	O
discriminant	B
function	I
y	O
(	O
x	O
)	O
.	O
we	O
now	O
explore	O
three	O
approaches	O
to	O
learning	B
the	O
parameters	O
of	O
linear	O
discrimi-	O
nant	O
functions	O
,	O
based	O
on	O
least	O
squares	O
,	O
fisher	O
’	O
s	O
linear	B
discriminant	I
,	O
and	O
the	O
percep-	O
tron	O
algorithm	O
.	O
4.1.3	O
least	O
squares	O
for	O
classiﬁcation	O
in	O
chapter	O
3	O
,	O
we	O
considered	O
models	O
that	O
were	O
linear	O
functions	O
of	O
the	O
parame-	O
ters	O
,	O
and	O
we	O
saw	O
that	O
the	O
minimization	O
of	O
a	O
sum-of-squares	B
error	I
function	O
led	O
to	O
a	O
simple	O
closed-form	O
solution	O
for	O
the	O
parameter	O
values	O
.	O
it	O
is	O
therefore	O
tempting	O
to	O
see	O
if	O
we	O
can	O
apply	O
the	O
same	O
formalism	O
to	O
classiﬁcation	B
problems	O
.	O
consider	O
a	O
general	O
classiﬁcation	B
problem	O
with	O
k	O
classes	O
,	O
with	O
a	O
1-of-k	O
binary	O
coding	O
scheme	O
for	O
the	O
target	B
vector	I
t.	O
one	O
justiﬁcation	O
for	O
using	O
least	O
squares	O
in	O
such	O
a	O
context	O
is	O
that	O
it	O
approximates	O
the	O
conditional	B
expectation	I
e	O
[	O
t|x	O
]	O
of	O
the	O
target	O
values	O
given	O
the	O
input	O
vector	O
.	O
for	O
the	O
binary	O
coding	O
scheme	O
,	O
this	O
conditional	B
expectation	I
is	O
given	O
by	O
the	O
vector	O
of	O
posterior	O
class	O
probabilities	O
.	O
unfortunately	O
,	O
however	O
,	O
these	O
probabilities	O
are	O
typically	O
approximated	O
rather	O
poorly	O
,	O
indeed	O
the	O
approximations	O
can	O
have	O
values	O
outside	O
the	O
range	O
(	O
0	O
,	O
1	O
)	O
,	O
due	O
to	O
the	O
limited	O
ﬂexibility	O
of	O
a	O
linear	O
model	O
as	O
we	O
shall	O
see	O
shortly	O
.	O
each	O
class	O
ck	O
is	O
described	O
by	O
its	O
own	O
linear	O
model	O
so	O
that	O
yk	O
(	O
x	O
)	O
=	O
wt	O
k	O
x	O
+	O
wk0	O
(	O
4.13	O
)	O
where	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k.	O
we	O
can	O
conveniently	O
group	O
these	O
together	O
using	O
vector	O
nota-	O
tion	O
so	O
that	O
(	O
4.14	O
)	O
y	O
(	O
x	O
)	O
=	O
,	O
wt	O
(	O
cid:4	O
)	O
x	O
can	O
then	O
be	O
written	O
as	O
4.1.	O
discriminant	O
functions	O
185	O
a	O
dummy	O
input	O
x0	O
=	O
1.	O
this	O
representation	O
was	O
discussed	O
in	O
detail	O
in	O
section	O
3.1.	O
a	O
error	B
function	I
,	O
as	O
we	O
did	O
for	B
regression	I
in	O
chapter	O
3.	O
consider	O
a	O
training	B
data	O
set	O
{	O
xn	O
,	O
tn	O
}	O
where	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
and	O
deﬁne	O
a	O
matrix	O
t	O
whose	O
nth	O
row	O
is	O
the	O
vector	O
tt	O
n	O
,	O
n.	O
the	O
sum-of-squares	B
error	I
function	O
where	O
,	O
w	O
is	O
a	O
matrix	O
whose	O
kth	O
column	O
comprises	O
the	O
d	O
+	O
1-dimensional	O
vector	O
(	O
cid:4	O
)	O
wk	O
=	O
(	O
wk0	O
,	O
wt	O
k	O
)	O
t	O
and	O
(	O
cid:4	O
)	O
x	O
is	O
the	O
corresponding	O
augmented	O
input	O
vector	O
(	O
1	O
,	O
xt	O
)	O
t	O
with	O
new	O
input	O
x	O
is	O
then	O
assigned	O
to	O
the	O
class	O
for	O
which	O
the	O
output	O
yk	O
=	O
(	O
cid:4	O
)	O
wt	O
k	O
(	O
cid:4	O
)	O
x	O
is	O
largest	O
.	O
we	O
now	O
determine	O
the	O
parameter	O
matrix	O
,	O
w	O
by	O
minimizing	O
a	O
sum-of-squares	O
together	O
with	O
a	O
matrix	O
(	O
cid:4	O
)	O
x	O
whose	O
nth	O
row	O
is	O
(	O
cid:4	O
)	O
xt	O
(	O
cid:19	O
)	O
(	O
cid:20	O
)	O
(	O
(	O
cid:4	O
)	O
x	O
,	O
w	O
−	O
t	O
)	O
t	O
(	O
(	O
cid:4	O
)	O
x	O
,	O
w	O
−	O
t	O
)	O
setting	O
the	O
derivative	B
with	O
respect	O
to	O
,	O
w	O
to	O
zero	O
,	O
and	O
rearranging	O
,	O
we	O
then	O
obtain	O
the	O
solution	O
for	O
,	O
w	O
in	O
the	O
form	O
,	O
w	O
=	O
(	O
(	O
cid:4	O
)	O
xt	O
(	O
cid:4	O
)	O
x	O
)	O
−1	O
(	O
cid:4	O
)	O
xtt	O
=	O
(	O
cid:4	O
)	O
x†t	O
is	O
the	O
pseudo-inverse	B
of	O
the	O
matrix	O
(	O
cid:4	O
)	O
x	O
,	O
as	O
discussed	O
in	O
section	O
3.1.1.	O
we	O
where	O
(	O
cid:4	O
)	O
x†	O
(	O
cid:18	O
)	O
t	O
(	O
cid:4	O
)	O
x	O
.	O
(	O
cid:17	O
)	O
(	O
cid:4	O
)	O
x†	O
y	O
(	O
x	O
)	O
=	O
,	O
wt	O
(	O
cid:4	O
)	O
x	O
=	O
tt	O
then	O
obtain	O
the	O
discriminant	B
function	I
in	O
the	O
form	O
ed	O
(	O
,	O
w	O
)	O
=	O
1	O
2	O
tr	O
.	O
(	O
4.15	O
)	O
(	O
4.16	O
)	O
(	O
4.17	O
)	O
exercise	O
4.2	O
section	O
2.3.7	O
an	O
interesting	O
property	O
of	O
least-squares	O
solutions	O
with	O
multiple	O
target	O
variables	O
is	O
that	O
if	O
every	O
target	B
vector	I
in	O
the	O
training	B
set	I
satisﬁes	O
some	O
linear	O
constraint	O
attn	O
+	O
b	O
=	O
0	O
(	O
4.18	O
)	O
for	O
some	O
constants	O
a	O
and	O
b	O
,	O
then	O
the	O
model	O
prediction	O
for	O
any	O
value	O
of	O
x	O
will	O
satisfy	O
the	O
same	O
constraint	O
so	O
that	O
aty	O
(	O
x	O
)	O
+	O
b	O
=	O
0	O
.	O
(	O
4.19	O
)	O
thus	O
if	O
we	O
use	O
a	O
1-of-k	O
coding	O
scheme	O
for	O
k	O
classes	O
,	O
then	O
the	O
predictions	O
made	O
by	O
the	O
model	O
will	O
have	O
the	O
property	O
that	O
the	O
elements	O
of	O
y	O
(	O
x	O
)	O
will	O
sum	O
to	O
1	O
for	O
any	O
value	O
of	O
x.	O
however	O
,	O
this	O
summation	O
constraint	O
alone	O
is	O
not	O
sufﬁcient	O
to	O
allow	O
the	O
model	O
outputs	O
to	O
be	O
interpreted	O
as	O
probabilities	O
because	O
they	O
are	O
not	O
constrained	O
to	O
lie	O
within	O
the	O
interval	O
(	O
0	O
,	O
1	O
)	O
.	O
the	O
least-squares	O
approach	O
gives	O
an	O
exact	O
closed-form	O
solution	O
for	O
the	O
discrimi-	O
nant	O
function	O
parameters	O
.	O
however	O
,	O
even	O
as	O
a	O
discriminant	B
function	I
(	O
where	O
we	O
use	O
it	O
to	O
make	O
decisions	O
directly	O
and	O
dispense	O
with	O
any	O
probabilistic	O
interpretation	O
)	O
it	O
suf-	O
fers	O
from	O
some	O
severe	O
problems	O
.	O
we	O
have	O
already	O
seen	O
that	O
least-squares	O
solutions	O
lack	O
robustness	B
to	O
outliers	B
,	O
and	O
this	O
applies	O
equally	O
to	O
the	O
classiﬁcation	B
application	O
,	O
as	O
illustrated	O
in	O
figure	O
4.4.	O
here	O
we	O
see	O
that	O
the	O
additional	O
data	O
points	O
in	O
the	O
right-	O
hand	O
ﬁgure	O
produce	O
a	O
signiﬁcant	O
change	O
in	O
the	O
location	O
of	O
the	O
decision	B
boundary	I
,	O
even	O
though	O
these	O
point	O
would	O
be	O
correctly	O
classiﬁed	O
by	O
the	O
original	O
decision	O
bound-	O
ary	O
in	O
the	O
left-hand	O
ﬁgure	O
.	O
the	O
sum-of-squares	B
error	I
function	O
penalizes	O
predictions	O
that	O
are	O
‘	O
too	O
correct	O
’	O
in	O
that	O
they	O
lie	O
a	O
long	O
way	O
on	O
the	O
correct	O
side	O
of	O
the	O
decision	O
186	O
4.	O
linear	O
models	O
for	O
classification	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
−4	O
−2	O
0	O
2	O
4	O
6	O
8	O
−4	O
−2	O
0	O
2	O
4	O
6	O
8	O
figure	O
4.4	O
the	O
left	O
plot	O
shows	O
data	O
from	O
two	O
classes	O
,	O
denoted	O
by	O
red	O
crosses	O
and	O
blue	O
circles	O
,	O
together	O
with	O
the	O
decision	B
boundary	I
found	O
by	O
least	O
squares	O
(	O
magenta	O
curve	O
)	O
and	O
also	O
by	O
the	O
logistic	B
regression	I
model	O
(	O
green	O
curve	O
)	O
,	O
which	O
is	O
discussed	O
later	O
in	O
section	O
4.3.2.	O
the	O
right-hand	O
plot	O
shows	O
the	O
corresponding	O
results	O
obtained	O
when	O
extra	O
data	O
points	O
are	O
added	O
at	O
the	O
bottom	O
left	O
of	O
the	O
diagram	O
,	O
showing	O
that	O
least	O
squares	O
is	O
highly	O
sensitive	O
to	O
outliers	B
,	O
unlike	O
logistic	B
regression	I
.	O
boundary	O
.	O
in	O
section	O
7.1.2	O
,	O
we	O
shall	O
consider	O
several	O
alternative	O
error	B
functions	O
for	O
classiﬁcation	O
and	O
we	O
shall	O
see	O
that	O
they	O
do	O
not	O
suffer	O
from	O
this	O
difﬁculty	O
.	O
however	O
,	O
problems	O
with	O
least	O
squares	O
can	O
be	O
more	O
severe	O
than	O
simply	O
lack	O
of	O
robustness	B
,	O
as	O
illustrated	O
in	O
figure	O
4.5.	O
this	O
shows	O
a	O
synthetic	O
data	O
set	O
drawn	O
from	O
three	O
classes	O
in	O
a	O
two-dimensional	O
input	O
space	O
(	O
x1	O
,	O
x2	O
)	O
,	O
having	O
the	O
property	O
that	O
lin-	O
ear	O
decision	O
boundaries	O
can	O
give	O
excellent	O
separation	O
between	O
the	O
classes	O
.	O
indeed	O
,	O
the	O
technique	O
of	O
logistic	B
regression	I
,	O
described	O
later	O
in	O
this	O
chapter	O
,	O
gives	O
a	O
satisfac-	O
tory	O
solution	O
as	O
seen	O
in	O
the	O
right-hand	O
plot	O
.	O
however	O
,	O
the	O
least-squares	O
solution	O
gives	O
poor	O
results	O
,	O
with	O
only	O
a	O
small	O
region	O
of	O
the	O
input	O
space	O
assigned	O
to	O
the	O
green	O
class	O
.	O
the	O
failure	O
of	O
least	O
squares	O
should	O
not	O
surprise	O
us	O
when	O
we	O
recall	O
that	O
it	O
cor-	O
responds	O
to	O
maximum	B
likelihood	I
under	O
the	O
assumption	O
of	O
a	O
gaussian	O
conditional	B
distribution	O
,	O
whereas	O
binary	O
target	O
vectors	O
clearly	O
have	O
a	O
distribution	O
that	O
is	O
far	O
from	O
gaussian	O
.	O
by	O
adopting	O
more	O
appropriate	O
probabilistic	O
models	O
,	O
we	O
shall	O
obtain	O
clas-	O
siﬁcation	O
techniques	O
with	O
much	O
better	O
properties	O
than	O
least	O
squares	O
.	O
for	O
the	O
moment	O
,	O
however	O
,	O
we	O
continue	O
to	O
explore	O
alternative	O
nonprobabilistic	O
methods	O
for	O
setting	O
the	O
parameters	O
in	O
the	O
linear	O
classiﬁcation	O
models	O
.	O
4.1.4	O
fisher	O
’	O
s	O
linear	B
discriminant	I
one	O
way	O
to	O
view	O
a	O
linear	O
classiﬁcation	O
model	O
is	O
in	O
terms	O
of	O
dimensionality	O
reduction	O
.	O
consider	O
ﬁrst	O
the	O
case	O
of	O
two	O
classes	O
,	O
and	O
suppose	O
we	O
take	O
the	O
d-	O
4.1.	O
discriminant	O
functions	O
187	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−6	O
−4	O
−2	O
0	O
2	O
4	O
6	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−6	O
−4	O
−2	O
0	O
2	O
4	O
6	O
figure	O
4.5	O
example	O
of	O
a	O
synthetic	O
data	O
set	O
comprising	O
three	O
classes	O
,	O
with	O
training	B
data	O
points	O
denoted	O
in	O
red	O
(	O
×	O
)	O
,	O
green	O
(	O
+	O
)	O
,	O
and	O
blue	O
(	O
◦	O
)	O
.	O
lines	O
denote	O
the	O
decision	O
boundaries	O
,	O
and	O
the	O
background	O
colours	O
denote	O
the	O
respective	O
classes	O
of	O
the	O
decision	O
regions	O
.	O
on	O
the	O
left	O
is	O
the	O
result	O
of	O
using	O
a	O
least-squares	O
discriminant	O
.	O
we	O
see	O
that	O
the	O
region	O
of	O
input	O
space	O
assigned	O
to	O
the	O
green	O
class	O
is	O
too	O
small	O
and	O
so	O
most	O
of	O
the	O
points	O
from	O
this	O
class	O
are	O
misclassiﬁed	O
.	O
on	O
the	O
right	O
is	O
the	O
result	O
of	O
using	O
logistic	O
regressions	O
as	O
described	O
in	O
section	O
4.3.2	O
showing	O
correct	O
classiﬁcation	B
of	O
the	O
training	B
data	O
.	O
dimensional	O
input	O
vector	O
x	O
and	O
project	O
it	O
down	O
to	O
one	O
dimension	O
using	O
y	O
=	O
wtx	O
.	O
(	O
4.20	O
)	O
if	O
we	O
place	O
a	O
threshold	O
on	O
y	O
and	O
classify	O
y	O
(	O
cid:2	O
)	O
−w0	O
as	O
class	O
c1	O
,	O
and	O
otherwise	O
class	O
c2	O
,	O
then	O
we	O
obtain	O
our	O
standard	O
linear	O
classiﬁer	O
discussed	O
in	O
the	O
previous	O
section	O
.	O
in	O
general	O
,	O
the	O
projection	O
onto	O
one	O
dimension	O
leads	O
to	O
a	O
considerable	O
loss	O
of	O
infor-	O
mation	B
,	O
and	O
classes	O
that	O
are	O
well	O
separated	O
in	O
the	O
original	O
d-dimensional	O
space	O
may	O
become	O
strongly	O
overlapping	O
in	O
one	O
dimension	O
.	O
however	O
,	O
by	O
adjusting	O
the	O
com-	O
ponents	O
of	O
the	O
weight	B
vector	I
w	O
,	O
we	O
can	O
select	O
a	O
projection	O
that	O
maximizes	O
the	O
class	O
separation	O
.	O
to	O
begin	O
with	O
,	O
consider	O
a	O
two-class	O
problem	O
in	O
which	O
there	O
are	O
n1	O
points	O
of	O
class	O
c1	O
and	O
n2	O
points	O
of	O
class	O
c2	O
,	O
so	O
that	O
the	O
mean	B
vectors	O
of	O
the	O
two	O
classes	O
are	O
given	O
by	O
xn	O
,	O
m2	O
=	O
xn	O
.	O
(	O
4.21	O
)	O
(	O
cid:2	O
)	O
n	O
∈	O
c1	O
m1	O
=	O
1	O
n1	O
(	O
cid:2	O
)	O
n	O
∈	O
c2	O
1	O
n2	O
the	O
simplest	O
measure	O
of	O
the	O
separation	O
of	O
the	O
classes	O
,	O
when	O
projected	O
onto	O
w	O
,	O
is	O
the	O
separation	O
of	O
the	O
projected	O
class	O
means	O
.	O
this	O
suggests	O
that	O
we	O
might	O
choose	O
w	O
so	O
as	O
to	O
maximize	O
m2	O
−	O
m1	O
=	O
wt	O
(	O
m2	O
−	O
m1	O
)	O
where	O
mk	O
=	O
wtmk	O
(	O
4.22	O
)	O
(	O
4.23	O
)	O
188	O
4.	O
linear	O
models	O
for	O
classification	O
4	O
2	O
0	O
−2	O
4	O
2	O
0	O
−2	O
−2	O
2	O
6	O
−2	O
2	O
6	O
figure	O
4.6	O
the	O
left	O
plot	O
shows	O
samples	O
from	O
two	O
classes	O
(	O
depicted	O
in	O
red	O
and	O
blue	O
)	O
along	O
with	O
the	O
histograms	O
resulting	O
from	O
projection	O
onto	O
the	O
line	O
joining	O
the	O
class	O
means	O
.	O
note	O
that	O
there	O
is	O
considerable	O
class	O
overlap	O
in	O
the	O
projected	O
space	O
.	O
the	O
right	O
plot	O
shows	O
the	O
corresponding	O
projection	O
based	O
on	O
the	O
fisher	O
linear	B
discriminant	I
,	O
showing	O
the	O
greatly	O
improved	O
class	O
separation	O
.	O
appendix	O
e	O
exercise	O
4.4	O
(	O
cid:5	O
)	O
i	O
w2	O
is	O
the	O
mean	B
of	O
the	O
projected	O
data	O
from	O
class	O
ck	O
.	O
however	O
,	O
this	O
expression	O
can	O
be	O
made	O
arbitrarily	O
large	O
simply	O
by	O
increasing	O
the	O
magnitude	O
of	O
w.	O
to	O
solve	O
this	O
i	O
=	O
1.	O
using	O
problem	O
,	O
we	O
could	O
constrain	O
w	O
to	O
have	O
unit	O
length	O
,	O
so	O
that	O
a	O
lagrange	O
multiplier	O
to	O
perform	O
the	O
constrained	O
maximization	O
,	O
we	O
then	O
ﬁnd	O
that	O
w	O
∝	O
(	O
m2	O
−	O
m1	O
)	O
.	O
there	O
is	O
still	O
a	O
problem	O
with	O
this	O
approach	O
,	O
however	O
,	O
as	O
illustrated	O
in	O
figure	O
4.6.	O
this	O
shows	O
two	O
classes	O
that	O
are	O
well	O
separated	O
in	O
the	O
original	O
two-	O
dimensional	O
space	O
(	O
x1	O
,	O
x2	O
)	O
but	O
that	O
have	O
considerable	O
overlap	O
when	O
projected	O
onto	O
the	O
line	O
joining	O
their	O
means	O
.	O
this	O
difﬁculty	O
arises	O
from	O
the	O
strongly	O
nondiagonal	O
covariances	O
of	O
the	O
class	O
distributions	O
.	O
the	O
idea	O
proposed	O
by	O
fisher	O
is	O
to	O
maximize	O
a	O
function	O
that	O
will	O
give	O
a	O
large	O
separation	O
between	O
the	O
projected	O
class	O
means	O
while	O
also	O
giving	O
a	O
small	O
variance	B
within	O
each	O
class	O
,	O
thereby	O
minimizing	O
the	O
class	O
overlap	O
.	O
the	O
projection	O
formula	O
(	O
4.20	O
)	O
transforms	O
the	O
set	O
of	O
labelled	O
data	O
points	O
in	O
x	O
into	O
a	O
labelled	O
set	O
in	O
the	O
one-dimensional	O
space	O
y.	O
the	O
within-class	B
variance	O
of	O
the	O
transformed	O
data	O
from	O
class	O
ck	O
is	O
therefore	O
given	O
by	O
(	O
yn	O
−	O
mk	O
)	O
2	O
(	O
cid:2	O
)	O
s2	O
k	O
=	O
n∈ck	O
(	O
4.24	O
)	O
where	O
yn	O
=	O
wtxn	O
.	O
we	O
can	O
deﬁne	O
the	O
total	O
within-class	B
variance	O
for	O
the	O
whole	O
data	O
set	O
to	O
be	O
simply	O
s2	O
2.	O
the	O
fisher	O
criterion	O
is	O
deﬁned	O
to	O
be	O
the	O
ratio	O
of	O
the	O
between-class	B
variance	O
to	O
the	O
within-class	B
variance	O
and	O
is	O
given	O
by	O
1	O
+	O
s2	O
j	O
(	O
w	O
)	O
=	O
(	O
m2	O
−	O
m1	O
)	O
2	O
1	O
+	O
s2	O
s2	O
2	O
.	O
(	O
4.25	O
)	O
exercise	O
4.5	O
we	O
can	O
make	O
the	O
dependence	O
on	O
w	O
explicit	O
by	O
using	O
(	O
4.20	O
)	O
,	O
(	O
4.23	O
)	O
,	O
and	O
(	O
4.24	O
)	O
to	O
rewrite	O
the	O
fisher	O
criterion	O
in	O
the	O
form	O
4.1.	O
discriminant	O
functions	O
189	O
j	O
(	O
w	O
)	O
=	O
wtsbw	O
wtsww	O
where	O
sb	O
is	O
the	O
between-class	B
covariance	I
matrix	O
and	O
is	O
given	O
by	O
sb	O
=	O
(	O
m2	O
−	O
m1	O
)	O
(	O
m2	O
−	O
m1	O
)	O
t	O
and	O
sw	O
is	O
the	O
total	O
within-class	B
covariance	I
matrix	O
,	O
given	O
by	O
(	O
4.26	O
)	O
(	O
4.27	O
)	O
sw	O
=	O
(	O
xn	O
−	O
m1	O
)	O
(	O
xn	O
−	O
m1	O
)	O
t	O
+	O
(	O
xn	O
−	O
m2	O
)	O
(	O
xn	O
−	O
m2	O
)	O
t.	O
(	O
4.28	O
)	O
(	O
cid:2	O
)	O
n∈c1	O
(	O
cid:2	O
)	O
n∈c2	O
differentiating	O
(	O
4.26	O
)	O
with	O
respect	O
to	O
w	O
,	O
we	O
ﬁnd	O
that	O
j	O
(	O
w	O
)	O
is	O
maximized	O
when	O
(	O
wtsbw	O
)	O
sww	O
=	O
(	O
wtsww	O
)	O
sbw	O
.	O
(	O
4.29	O
)	O
from	O
(	O
4.27	O
)	O
,	O
we	O
see	O
that	O
sbw	O
is	O
always	O
in	O
the	O
direction	O
of	O
(	O
m2−m1	O
)	O
.	O
furthermore	O
,	O
we	O
do	O
not	O
care	O
about	O
the	O
magnitude	O
of	O
w	O
,	O
only	O
its	O
direction	O
,	O
and	O
so	O
we	O
can	O
drop	O
the	O
−1	O
scalar	O
factors	O
(	O
wtsbw	O
)	O
and	O
(	O
wtsww	O
)	O
.	O
multiplying	O
both	O
sides	O
of	O
(	O
4.29	O
)	O
by	O
s	O
w	O
we	O
then	O
obtain	O
(	O
4.30	O
)	O
note	O
that	O
if	O
the	O
within-class	B
covariance	I
is	O
isotropic	B
,	O
so	O
that	O
sw	O
is	O
proportional	O
to	O
the	O
unit	O
matrix	O
,	O
we	O
ﬁnd	O
that	O
w	O
is	O
proportional	O
to	O
the	O
difference	O
of	O
the	O
class	O
means	O
,	O
as	O
discussed	O
above	O
.	O
w	O
∝	O
s	O
w	O
(	O
m2	O
−	O
m1	O
)	O
.	O
−1	O
the	O
result	O
(	O
4.30	O
)	O
is	O
known	O
as	O
fisher	O
’	O
s	O
linear	B
discriminant	I
,	O
although	O
strictly	O
it	O
is	O
not	O
a	O
discriminant	O
but	O
rather	O
a	O
speciﬁc	O
choice	O
of	O
direction	O
for	O
projection	O
of	O
the	O
data	O
down	O
to	O
one	O
dimension	O
.	O
however	O
,	O
the	O
projected	O
data	O
can	O
subsequently	O
be	O
used	O
to	O
construct	O
a	O
discriminant	O
,	O
by	O
choosing	O
a	O
threshold	O
y0	O
so	O
that	O
we	O
classify	O
a	O
new	O
point	O
as	O
belonging	O
to	O
c1	O
if	O
y	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
y0	O
and	O
classify	O
it	O
as	O
belonging	O
to	O
c2	O
otherwise	O
.	O
for	O
example	O
,	O
we	O
can	O
model	O
the	O
class-conditional	O
densities	O
p	O
(	O
y|ck	O
)	O
using	O
gaussian	O
distributions	O
and	O
then	O
use	O
the	O
techniques	O
of	O
section	O
1.2.4	O
to	O
ﬁnd	O
the	O
parameters	O
of	O
the	O
gaussian	O
distributions	O
by	O
maximum	B
likelihood	I
.	O
having	O
found	O
gaussian	O
ap-	O
proximations	O
to	O
the	O
projected	O
classes	O
,	O
the	O
formalism	O
of	O
section	O
1.5.1	O
then	O
gives	O
an	O
expression	O
for	O
the	O
optimal	O
threshold	O
.	O
some	O
justiﬁcation	O
for	O
the	O
gaussian	O
assumption	O
comes	O
from	O
the	O
central	B
limit	I
theorem	I
by	O
noting	O
that	O
y	O
=	O
wtx	O
is	O
the	O
sum	O
of	O
a	O
set	O
of	O
random	O
variables	O
.	O
4.1.5	O
relation	O
to	O
least	O
squares	O
the	O
least-squares	O
approach	O
to	O
the	O
determination	O
of	O
a	O
linear	B
discriminant	I
was	O
based	O
on	O
the	O
goal	O
of	O
making	O
the	O
model	O
predictions	O
as	O
close	O
as	O
possible	O
to	O
a	O
set	O
of	O
target	O
values	O
.	O
by	O
contrast	O
,	O
the	O
fisher	O
criterion	O
was	O
derived	O
by	O
requiring	O
maximum	O
class	O
separation	O
in	O
the	O
output	O
space	O
.	O
it	O
is	O
interesting	O
to	O
see	O
the	O
relationship	O
between	O
these	O
two	O
approaches	O
.	O
in	O
particular	O
,	O
we	O
shall	O
show	O
that	O
,	O
for	O
the	O
two-class	O
problem	O
,	O
the	O
fisher	O
criterion	O
can	O
be	O
obtained	O
as	O
a	O
special	O
case	O
of	O
least	O
squares	O
.	O
so	O
far	O
we	O
have	O
considered	O
1-of-k	O
coding	O
for	O
the	O
target	O
values	O
.	O
if	O
,	O
however	O
,	O
we	O
adopt	O
a	O
slightly	O
different	O
target	O
coding	O
scheme	O
,	O
then	O
the	O
least-squares	O
solution	O
for	O
190	O
4.	O
linear	O
models	O
for	O
classification	O
the	O
weights	O
becomes	O
equivalent	O
to	O
the	O
fisher	O
solution	O
(	O
duda	O
and	O
hart	O
,	O
1973	O
)	O
.	O
in	O
particular	O
,	O
we	O
shall	O
take	O
the	O
targets	O
for	O
class	O
c1	O
to	O
be	O
n/n1	O
,	O
where	O
n1	O
is	O
the	O
number	O
of	O
patterns	O
in	O
class	O
c1	O
,	O
and	O
n	O
is	O
the	O
total	O
number	O
of	O
patterns	O
.	O
this	O
target	O
value	O
approximates	O
the	O
reciprocal	O
of	O
the	O
prior	B
probability	O
for	O
class	O
c1	O
.	O
for	O
class	O
c2	O
,	O
we	O
shall	O
take	O
the	O
targets	O
to	O
be	O
−n/n2	O
,	O
where	O
n2	O
is	O
the	O
number	O
of	O
patterns	O
in	O
class	O
c2	O
.	O
setting	O
the	O
derivatives	O
of	O
e	O
with	O
respect	O
to	O
w0	O
and	O
w	O
to	O
zero	O
,	O
we	O
obtain	O
respectively	O
.	O
(	O
4.31	O
)	O
the	O
sum-of-squares	B
error	I
function	O
can	O
be	O
written	O
wtxn	O
+	O
w0	O
−	O
tn	O
e	O
=	O
(	O
cid:11	O
)	O
2	O
n	O
(	O
cid:2	O
)	O
(	O
cid:10	O
)	O
n=1	O
1	O
2	O
(	O
cid:10	O
)	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:10	O
)	O
n=1	O
n=1	O
(	O
cid:11	O
)	O
(	O
cid:11	O
)	O
wtxn	O
+	O
w0	O
−	O
tn	O
=	O
0	O
wtxn	O
+	O
w0	O
−	O
tn	O
xn	O
=	O
0	O
.	O
(	O
4.32	O
)	O
(	O
4.33	O
)	O
(	O
4.34	O
)	O
(	O
4.35	O
)	O
from	O
(	O
4.32	O
)	O
,	O
and	O
making	O
use	O
of	O
our	O
choice	O
of	O
target	O
coding	O
scheme	O
for	O
the	O
tn	O
,	O
we	O
obtain	O
an	O
expression	O
for	O
the	O
bias	B
in	O
the	O
form	O
where	O
we	O
have	O
used	O
n	O
(	O
cid:2	O
)	O
n=1	O
w0	O
=	O
−wtm	O
tn	O
=	O
n1	O
n	O
n1	O
−	O
n2	O
n	O
n2	O
=	O
0	O
and	O
where	O
m	O
is	O
the	O
mean	B
of	O
the	O
total	O
data	O
set	O
and	O
is	O
given	O
by	O
n	O
(	O
cid:2	O
)	O
n=1	O
m	O
=	O
1	O
n	O
xn	O
=	O
1	O
n	O
(	O
n1m1	O
+	O
n2m2	O
)	O
.	O
(	O
4.36	O
)	O
exercise	O
4.6	O
after	O
some	O
straightforward	O
algebra	O
,	O
and	O
again	O
making	O
use	O
of	O
the	O
choice	O
of	O
tn	O
,	O
the	O
second	O
equation	O
(	O
4.33	O
)	O
becomes	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
sw	O
+	O
n1n2	O
n	O
sb	O
w	O
=	O
n	O
(	O
m1	O
−	O
m2	O
)	O
(	O
4.37	O
)	O
where	O
sw	O
is	O
deﬁned	O
by	O
(	O
4.28	O
)	O
,	O
sb	O
is	O
deﬁned	O
by	O
(	O
4.27	O
)	O
,	O
and	O
we	O
have	O
substituted	O
for	O
the	O
bias	B
using	O
(	O
4.34	O
)	O
.	O
using	O
(	O
4.27	O
)	O
,	O
we	O
note	O
that	O
sbw	O
is	O
always	O
in	O
the	O
direction	O
of	O
(	O
m2	O
−	O
m1	O
)	O
.	O
thus	O
we	O
can	O
write	O
w	O
∝	O
s	O
w	O
(	O
m2	O
−	O
m1	O
)	O
−1	O
(	O
4.38	O
)	O
where	O
we	O
have	O
ignored	O
irrelevant	O
scale	O
factors	O
.	O
thus	O
the	O
weight	B
vector	I
coincides	O
with	O
that	O
found	O
from	O
the	O
fisher	O
criterion	O
.	O
in	O
addition	O
,	O
we	O
have	O
also	O
found	O
an	O
expres-	O
sion	B
for	O
the	O
bias	B
value	O
w0	O
given	O
by	O
(	O
4.34	O
)	O
.	O
this	O
tells	O
us	O
that	O
a	O
new	O
vector	O
x	O
should	O
be	O
classiﬁed	O
as	O
belonging	O
to	O
class	O
c1	O
if	O
y	O
(	O
x	O
)	O
=	O
wt	O
(	O
x−m	O
)	O
>	O
0	O
and	O
class	O
c2	O
otherwise	O
.	O
4.1.	O
discriminant	O
functions	O
191	O
(	O
cid:4	O
)	O
4.1.6	O
fisher	O
’	O
s	O
discriminant	O
for	O
multiple	O
classes	O
we	O
now	O
consider	O
the	O
generalization	B
of	O
the	O
fisher	O
discriminant	O
to	O
k	O
>	O
2	O
classes	O
,	O
and	O
we	O
shall	O
assume	O
that	O
the	O
dimensionality	O
d	O
of	O
the	O
input	O
space	O
is	O
greater	O
than	O
the	O
k	O
x	O
,	O
where	O
number	O
k	O
of	O
classes	O
.	O
next	O
,	O
we	O
introduce	O
d	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
.	O
these	O
feature	O
values	O
can	O
conveniently	O
be	O
grouped	O
together	O
to	O
form	O
a	O
vector	O
y.	O
similarly	O
,	O
the	O
weight	O
vectors	O
{	O
wk	O
}	O
can	O
be	O
considered	O
to	O
be	O
the	O
columns	O
of	O
a	O
matrix	O
w	O
,	O
so	O
that	O
>	O
1	O
linear	O
‘	O
features	O
’	O
yk	O
=	O
wt	O
(	O
cid:4	O
)	O
(	O
4.39	O
)	O
note	O
that	O
again	O
we	O
are	O
not	O
including	O
any	O
bias	B
parameters	O
in	O
the	O
deﬁnition	O
of	O
y.	O
the	O
generalization	B
of	O
the	O
within-class	B
covariance	I
matrix	O
to	O
the	O
case	O
of	O
k	O
classes	O
follows	O
from	O
(	O
4.28	O
)	O
to	O
give	O
y	O
=	O
wtx	O
.	O
sw	O
=	O
sk	O
k	O
(	O
cid:2	O
)	O
k=1	O
xn	O
(	O
cid:2	O
)	O
n∈ck	O
1	O
nk	O
(	O
cid:2	O
)	O
n∈ck	O
(	O
xn	O
−	O
mk	O
)	O
(	O
xn	O
−	O
mk	O
)	O
t	O
where	O
sk	O
=	O
mk	O
=	O
and	O
nk	O
is	O
the	O
number	O
of	O
patterns	O
in	O
class	O
ck	O
.	O
in	O
order	O
to	O
ﬁnd	O
a	O
generalization	B
of	O
the	O
between-class	B
covariance	I
matrix	O
,	O
we	O
follow	O
duda	O
and	O
hart	O
(	O
1973	O
)	O
and	O
consider	O
ﬁrst	O
the	O
total	O
covariance	B
matrix	I
(	O
cid:5	O
)	O
where	O
m	O
is	O
the	O
mean	B
of	O
the	O
total	O
data	O
set	O
st	O
=	O
(	O
xn	O
−	O
m	O
)	O
(	O
xn	O
−	O
m	O
)	O
t	O
(	O
4.43	O
)	O
m	O
=	O
1	O
n	O
xn	O
=	O
1	O
n	O
k	O
(	O
cid:2	O
)	O
k=1	O
nkmk	O
(	O
4.44	O
)	O
and	O
n	O
=	O
k	O
nk	O
is	O
the	O
total	O
number	O
of	O
data	O
points	O
.	O
the	O
total	O
covariance	B
matrix	I
can	O
be	O
decomposed	O
into	O
the	O
sum	O
of	O
the	O
within-class	B
covariance	I
matrix	O
,	O
given	O
by	O
(	O
4.40	O
)	O
and	O
(	O
4.41	O
)	O
,	O
plus	O
an	O
additional	O
matrix	O
sb	O
,	O
which	O
we	O
identify	O
as	O
a	O
measure	O
of	O
the	O
between-class	B
covariance	I
st	O
=	O
sw	O
+	O
sb	O
where	O
sb	O
=	O
nk	O
(	O
mk	O
−	O
m	O
)	O
(	O
mk	O
−	O
m	O
)	O
t.	O
(	O
4.40	O
)	O
(	O
4.41	O
)	O
(	O
4.42	O
)	O
(	O
4.45	O
)	O
(	O
4.46	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
n	O
(	O
cid:2	O
)	O
n=1	O
k	O
(	O
cid:2	O
)	O
k=1	O
n∈ck	O
k=1	O
k	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
k=1	O
n∈ck	O
(	O
4.47	O
)	O
(	O
4.48	O
)	O
(	O
4.49	O
)	O
(	O
4.50	O
)	O
192	O
4.	O
linear	O
models	O
for	O
classification	O
these	O
covariance	B
matrices	O
have	O
been	O
deﬁned	O
in	O
the	O
original	O
x-space	O
.	O
we	O
can	O
now	O
deﬁne	O
similar	O
matrices	O
in	O
the	O
projected	O
d	O
-dimensional	O
y-space	O
(	O
cid:4	O
)	O
sw	O
=	O
(	O
yn	O
−	O
µk	O
)	O
(	O
yn	O
−	O
µk	O
)	O
t	O
and	O
where	O
sb	O
=	O
µk	O
=	O
1	O
nk	O
nk	O
(	O
µk	O
−	O
µ	O
)	O
(	O
µk	O
−	O
µ	O
)	O
t	O
k	O
(	O
cid:2	O
)	O
yn	O
,	O
µ	O
=	O
nkµk	O
.	O
k=1	O
1	O
n	O
(	O
cid:27	O
)	O
(	O
cid:26	O
)	O
j	O
(	O
w	O
)	O
=	O
tr	O
−1	O
s	O
w	O
sb	O
.	O
(	O
cid:26	O
)	O
again	O
we	O
wish	O
to	O
construct	O
a	O
scalar	O
that	O
is	O
large	O
when	O
the	O
between-class	B
covariance	I
is	O
large	O
and	O
when	O
the	O
within-class	B
covariance	I
is	O
small	O
.	O
there	O
are	O
now	O
many	O
possible	O
choices	O
of	O
criterion	O
(	O
fukunaga	O
,	O
1990	O
)	O
.	O
one	O
example	O
is	O
given	O
by	O
(	O
cid:27	O
)	O
this	O
criterion	O
can	O
then	O
be	O
rewritten	O
as	O
an	O
explicit	O
function	O
of	O
the	O
projection	O
matrix	O
w	O
in	O
the	O
form	O
j	O
(	O
w	O
)	O
=	O
tr	O
(	O
wswwt	O
)	O
−1	O
(	O
wsbwt	O
)	O
.	O
(	O
4.51	O
)	O
(	O
cid:4	O
)	O
maximization	O
of	O
such	O
criteria	O
is	O
straightforward	O
,	O
though	O
somewhat	O
involved	O
,	O
and	O
is	O
discussed	O
at	O
length	O
in	O
fukunaga	O
(	O
1990	O
)	O
.	O
the	O
weight	O
values	O
are	O
determined	O
by	O
those	O
eigenvectors	O
of	O
s	O
−1	O
w	O
sb	O
that	O
correspond	O
to	O
the	O
d	O
largest	O
eigenvalues	O
.	O
there	O
is	O
one	O
important	O
result	O
that	O
is	O
common	O
to	O
all	O
such	O
criteria	O
,	O
which	O
is	O
worth	O
emphasizing	O
.	O
we	O
ﬁrst	O
note	O
from	O
(	O
4.46	O
)	O
that	O
sb	O
is	O
composed	O
of	O
the	O
sum	O
of	O
k	O
ma-	O
trices	O
,	O
each	O
of	O
which	O
is	O
an	O
outer	O
product	O
of	O
two	O
vectors	O
and	O
therefore	O
of	O
rank	O
1.	O
in	O
addition	O
,	O
only	O
(	O
k	O
−	O
1	O
)	O
of	O
these	O
matrices	O
are	O
independent	B
as	O
a	O
result	O
of	O
the	O
constraint	O
(	O
4.44	O
)	O
.	O
thus	O
,	O
sb	O
has	O
rank	O
at	O
most	O
equal	O
to	O
(	O
k	O
−	O
1	O
)	O
and	O
so	O
there	O
are	O
at	O
most	O
(	O
k	O
−	O
1	O
)	O
nonzero	O
eigenvalues	O
.	O
this	O
shows	O
that	O
the	O
projection	O
onto	O
the	O
(	O
k	O
−	O
1	O
)	O
-dimensional	O
subspace	O
spanned	O
by	O
the	O
eigenvectors	O
of	O
sb	O
does	O
not	O
alter	O
the	O
value	O
of	O
j	O
(	O
w	O
)	O
,	O
and	O
so	O
we	O
are	O
therefore	O
unable	O
to	O
ﬁnd	O
more	O
than	O
(	O
k	O
−	O
1	O
)	O
linear	O
‘	O
features	O
’	O
by	O
this	O
means	O
(	O
fukunaga	O
,	O
1990	O
)	O
.	O
4.1.7	O
the	O
perceptron	B
algorithm	O
another	O
example	O
of	O
a	O
linear	B
discriminant	I
model	O
is	O
the	O
perceptron	B
of	O
rosenblatt	O
(	O
1962	O
)	O
,	O
which	O
occupies	O
an	O
important	O
place	O
in	O
the	O
history	O
of	O
pattern	O
recognition	O
al-	O
gorithms	O
.	O
it	O
corresponds	O
to	O
a	O
two-class	O
model	O
in	O
which	O
the	O
input	O
vector	O
x	O
is	O
ﬁrst	O
transformed	O
using	O
a	O
ﬁxed	O
nonlinear	O
transformation	O
to	O
give	O
a	O
feature	O
vector	O
φ	O
(	O
x	O
)	O
,	O
and	O
this	O
is	O
then	O
used	O
to	O
construct	O
a	O
generalized	B
linear	I
model	I
of	O
the	O
form	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
y	O
(	O
x	O
)	O
=	O
f	O
wtφ	O
(	O
x	O
)	O
(	O
4.52	O
)	O
4.1.	O
discriminant	O
functions	O
193	O
where	O
the	O
nonlinear	O
activation	B
function	I
f	O
(	O
·	O
)	O
is	O
given	O
by	O
a	O
step	O
function	O
of	O
the	O
form	O
f	O
(	O
a	O
)	O
=	O
+1	O
,	O
a	O
(	O
cid:2	O
)	O
0	O
−1	O
,	O
a	O
<	O
0	O
.	O
(	O
4.53	O
)	O
(	O
cid:12	O
)	O
the	O
vector	O
φ	O
(	O
x	O
)	O
will	O
typically	O
include	O
a	O
bias	B
component	O
φ0	O
(	O
x	O
)	O
=	O
1.	O
in	O
earlier	O
discussions	O
of	O
two-class	O
classiﬁcation	B
problems	O
,	O
we	O
have	O
focussed	O
on	O
a	O
target	O
coding	O
scheme	O
in	O
which	O
t	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
which	O
is	O
appropriate	O
in	O
the	O
context	O
of	O
probabilistic	O
models	O
.	O
for	O
the	O
perceptron	B
,	O
however	O
,	O
it	O
is	O
more	O
convenient	O
to	O
use	O
target	O
values	O
t	O
=	O
+1	O
for	O
class	O
c1	O
and	O
t	O
=	O
−1	O
for	O
class	O
c2	O
,	O
which	O
matches	O
the	O
choice	O
of	O
activation	B
function	I
.	O
the	O
algorithm	O
used	O
to	O
determine	O
the	O
parameters	O
w	O
of	O
the	O
perceptron	B
can	O
most	O
easily	O
be	O
motivated	O
by	O
error	B
function	I
minimization	O
.	O
a	O
natural	O
choice	O
of	O
error	B
func-	O
tion	O
would	O
be	O
the	O
total	O
number	O
of	O
misclassiﬁed	O
patterns	O
.	O
however	O
,	O
this	O
does	O
not	O
lead	O
to	O
a	O
simple	O
learning	B
algorithm	O
because	O
the	O
error	B
is	O
a	O
piecewise	O
constant	O
function	O
of	O
w	O
,	O
with	O
discontinuities	O
wherever	O
a	O
change	O
in	O
w	O
causes	O
the	O
decision	B
boundary	I
to	O
move	O
across	O
one	O
of	O
the	O
data	O
points	O
.	O
methods	O
based	O
on	O
changing	O
w	O
using	O
the	O
gradi-	O
ent	O
of	O
the	O
error	B
function	I
can	O
not	O
then	O
be	O
applied	O
,	O
because	O
the	O
gradient	O
is	O
zero	O
almost	O
everywhere	O
.	O
we	O
therefore	O
consider	O
an	O
alternative	O
error	B
function	I
known	O
as	O
the	O
perceptron	B
cri-	O
terion	O
.	O
to	O
derive	O
this	O
,	O
we	O
note	O
that	O
we	O
are	O
seeking	O
a	O
weight	B
vector	I
w	O
such	O
that	O
patterns	O
xn	O
in	O
class	O
c1	O
will	O
have	O
wtφ	O
(	O
xn	O
)	O
>	O
0	O
,	O
whereas	O
patterns	O
xn	O
in	O
class	O
c2	O
have	O
wtφ	O
(	O
xn	O
)	O
<	O
0.	O
using	O
the	O
t	O
∈	O
{	O
−1	O
,	O
+1	O
}	O
target	O
coding	O
scheme	O
it	O
follows	O
that	O
we	O
would	O
like	O
all	O
patterns	O
to	O
satisfy	O
wtφ	O
(	O
xn	O
)	O
tn	O
>	O
0.	O
the	O
perceptron	B
criterion	I
associates	O
zero	O
error	B
with	O
any	O
pattern	O
that	O
is	O
correctly	O
classiﬁed	O
,	O
whereas	O
for	O
a	O
mis-	O
classiﬁed	O
pattern	O
xn	O
it	O
tries	O
to	O
minimize	O
the	O
quantity	O
−wtφ	O
(	O
xn	O
)	O
tn	O
.	O
the	O
perceptron	B
criterion	I
is	O
therefore	O
given	O
by	O
ep	O
(	O
w	O
)	O
=	O
−	O
wtφntn	O
(	O
4.54	O
)	O
(	O
cid:2	O
)	O
n∈m	O
frank	O
rosenblatt	O
1928–1969	O
rosenblatt	O
’	O
s	O
perceptron	B
played	O
an	O
important	O
role	O
in	O
the	O
history	O
of	O
ma-	O
chine	O
learning	B
.	O
initially	O
,	O
rosenblatt	O
simulated	O
the	O
perceptron	B
on	O
an	O
ibm	O
704	O
computer	O
at	O
cornell	O
in	O
1957	O
,	O
but	O
by	O
the	O
early	O
1960s	O
he	O
had	O
built	O
special-purpose	O
hardware	B
that	O
provided	O
a	O
direct	O
,	O
par-	O
allel	O
implementation	O
of	O
perceptron	B
learning	O
.	O
many	O
of	O
his	O
ideas	O
were	O
encapsulated	O
in	O
“	O
principles	O
of	O
neuro-	O
dynamics	O
:	O
perceptrons	O
and	O
the	O
theory	B
of	O
brain	O
mech-	O
anisms	O
”	O
published	O
in	O
1962.	O
rosenblatt	O
’	O
s	O
work	O
was	O
criticized	O
by	O
marvin	O
minksy	O
,	O
whose	O
objections	O
were	O
published	O
in	O
the	O
book	O
“	O
perceptrons	O
”	O
,	O
co-authored	O
with	O
seymour	O
papert	O
.	O
this	O
book	O
was	O
widely	O
misinter-	O
preted	O
at	O
the	O
time	O
as	O
showing	O
that	O
neural	O
networks	O
were	O
fatally	O
ﬂawed	O
and	O
could	O
only	O
learn	O
solutions	O
for	O
linearly	O
separable	O
problems	O
.	O
in	O
fact	O
,	O
it	O
only	O
proved	O
such	O
limitations	O
in	O
the	O
case	O
of	O
single-layer	O
networks	O
such	O
as	O
the	O
perceptron	B
and	O
merely	O
conjectured	O
(	O
in-	O
correctly	O
)	O
that	O
they	O
applied	O
to	O
more	O
general	O
network	O
models	O
.	O
unfortunately	O
,	O
however	O
,	O
this	O
book	O
contributed	O
to	O
the	O
substantial	O
decline	O
in	O
research	O
funding	O
for	O
neu-	O
ral	O
computing	O
,	O
a	O
situation	O
that	O
was	O
not	O
reversed	O
un-	O
til	O
the	O
mid-1980s	O
.	O
today	O
,	O
there	O
are	O
many	O
hundreds	O
,	O
if	O
not	O
thousands	O
,	O
of	O
applications	O
of	O
neural	O
networks	O
in	O
widespread	O
use	O
,	O
with	O
examples	O
in	O
areas	O
such	O
as	O
handwriting	B
recognition	I
and	O
information	O
retrieval	O
be-	O
ing	O
used	O
routinely	O
by	O
millions	O
of	O
people	O
.	O
194	O
4.	O
linear	O
models	O
for	O
classification	O
section	O
3.1.3	O
where	O
m	O
denotes	O
the	O
set	O
of	O
all	O
misclassiﬁed	O
patterns	O
.	O
the	O
contribution	O
to	O
the	O
error	B
associated	O
with	O
a	O
particular	O
misclassiﬁed	O
pattern	O
is	O
a	O
linear	O
function	O
of	O
w	O
in	O
regions	O
of	O
w	O
space	O
where	O
the	O
pattern	O
is	O
misclassiﬁed	O
and	O
zero	O
in	O
regions	O
where	O
it	O
is	O
correctly	O
classiﬁed	O
.	O
the	O
total	O
error	B
function	I
is	O
therefore	O
piecewise	O
linear	O
.	O
we	O
now	O
apply	O
the	O
stochastic	B
gradient	I
descent	I
algorithm	O
to	O
this	O
error	B
function	I
.	O
the	O
change	O
in	O
the	O
weight	B
vector	I
w	O
is	O
then	O
given	O
by	O
w	O
(	O
τ	O
+1	O
)	O
=	O
w	O
(	O
τ	O
)	O
−	O
η∇ep	O
(	O
w	O
)	O
=	O
w	O
(	O
τ	O
)	O
+	O
ηφntn	O
(	O
4.55	O
)	O
where	O
η	O
is	O
the	O
learning	B
rate	I
parameter	I
and	O
τ	O
is	O
an	O
integer	O
that	O
indexes	O
the	O
steps	O
of	O
the	O
algorithm	O
.	O
because	O
the	O
perceptron	B
function	O
y	O
(	O
x	O
,	O
w	O
)	O
is	O
unchanged	O
if	O
we	O
multiply	O
w	O
by	O
a	O
constant	O
,	O
we	O
can	O
set	O
the	O
learning	B
rate	I
parameter	I
η	O
equal	O
to	O
1	O
without	O
of	O
generality	O
.	O
note	O
that	O
,	O
as	O
the	O
weight	B
vector	I
evolves	O
during	O
training	B
,	O
the	O
set	O
of	O
patterns	O
that	O
are	O
misclassiﬁed	O
will	O
change	O
.	O
the	O
perceptron	B
learning	O
algorithm	O
has	O
a	O
simple	O
interpretation	O
,	O
as	O
follows	O
.	O
we	O
cycle	O
through	O
the	O
training	B
patterns	O
in	O
turn	O
,	O
and	O
for	O
each	O
pattern	O
xn	O
we	O
evaluate	O
the	O
perceptron	B
function	O
(	O
4.52	O
)	O
.	O
if	O
the	O
pattern	O
is	O
correctly	O
classiﬁed	O
,	O
then	O
the	O
weight	B
vector	I
remains	O
unchanged	O
,	O
whereas	O
if	O
it	O
is	O
incorrectly	O
classiﬁed	O
,	O
then	O
for	O
class	O
c1	O
we	O
add	O
the	O
vector	O
φ	O
(	O
xn	O
)	O
onto	O
the	O
current	O
estimate	O
of	O
weight	B
vector	I
w	O
while	O
for	O
class	O
c2	O
we	O
subtract	O
the	O
vector	O
φ	O
(	O
xn	O
)	O
from	O
w.	O
the	O
perceptron	B
learning	O
algorithm	O
is	O
illustrated	O
in	O
figure	O
4.7.	O
if	O
we	O
consider	O
the	O
effect	O
of	O
a	O
single	O
update	O
in	O
the	O
perceptron	B
learning	O
algorithm	O
,	O
we	O
see	O
that	O
the	O
contribution	O
to	O
the	O
error	B
from	O
a	O
misclassiﬁed	O
pattern	O
will	O
be	O
reduced	O
because	O
from	O
(	O
4.55	O
)	O
we	O
have	O
−w	O
(	O
τ	O
+1	O
)	O
tφntn	O
=	O
−w	O
(	O
τ	O
)	O
tφntn	O
−	O
(	O
φntn	O
)	O
tφntn	O
<	O
−w	O
(	O
τ	O
)	O
tφntn	O
(	O
4.56	O
)	O
where	O
we	O
have	O
set	O
η	O
=	O
1	O
,	O
and	O
made	O
use	O
of	O
(	O
cid:5	O
)	O
φntn	O
(	O
cid:5	O
)	O
2	O
>	O
0.	O
of	O
course	O
,	O
this	O
does	O
not	O
imply	O
that	O
the	O
contribution	O
to	O
the	O
error	B
function	I
from	O
the	O
other	O
misclassiﬁed	O
patterns	O
will	O
have	O
been	O
reduced	O
.	O
furthermore	O
,	O
the	O
change	O
in	O
weight	B
vector	I
may	O
have	O
caused	O
some	O
previously	O
correctly	O
classiﬁed	O
patterns	O
to	O
become	O
misclassiﬁed	O
.	O
thus	O
the	O
perceptron	B
learning	O
rule	O
is	O
not	O
guaranteed	O
to	O
reduce	O
the	O
total	O
error	B
function	I
at	O
each	O
stage	O
.	O
however	O
,	O
the	O
perceptron	B
convergence	O
theorem	O
states	O
that	O
if	O
there	O
exists	O
an	O
ex-	O
act	O
solution	O
(	O
in	O
other	O
words	O
,	O
if	O
the	O
training	B
data	O
set	O
is	O
linearly	B
separable	I
)	O
,	O
then	O
the	O
perceptron	B
learning	O
algorithm	O
is	O
guaranteed	O
to	O
ﬁnd	O
an	O
exact	O
solution	O
in	O
a	O
ﬁnite	O
num-	O
ber	O
of	O
steps	O
.	O
proofs	O
of	O
this	O
theorem	O
can	O
be	O
found	O
for	O
example	O
in	O
rosenblatt	O
(	O
1962	O
)	O
,	O
block	O
(	O
1962	O
)	O
,	O
nilsson	O
(	O
1965	O
)	O
,	O
minsky	O
and	O
papert	O
(	O
1969	O
)	O
,	O
hertz	O
et	O
al	O
.	O
(	O
1991	O
)	O
,	O
and	O
bishop	O
(	O
1995a	O
)	O
.	O
note	O
,	O
however	O
,	O
that	O
the	O
number	O
of	O
steps	O
required	O
to	O
achieve	O
con-	O
vergence	O
could	O
still	O
be	O
substantial	O
,	O
and	O
in	O
practice	O
,	O
until	O
convergence	O
is	O
achieved	O
,	O
we	O
will	O
not	O
be	O
able	O
to	O
distinguish	O
between	O
a	O
nonseparable	O
problem	O
and	O
one	O
that	O
is	O
simply	O
slow	O
to	O
converge	O
.	O
even	O
when	O
the	O
data	O
set	O
is	O
linearly	B
separable	I
,	O
there	O
may	O
be	O
many	O
solutions	O
,	O
and	O
which	O
one	O
is	O
found	O
will	O
depend	O
on	O
the	O
initialization	O
of	O
the	O
parameters	O
and	O
on	O
the	O
or-	O
der	O
of	O
presentation	O
of	O
the	O
data	O
points	O
.	O
furthermore	O
,	O
for	O
data	O
sets	O
that	O
are	O
not	O
linearly	B
separable	I
,	O
the	O
perceptron	B
learning	O
algorithm	O
will	O
never	O
converge	O
.	O
4.1.	O
discriminant	O
functions	O
195	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1	O
−0.5	O
0	O
0.5	O
1	O
−0.5	O
0	O
0.5	O
1	O
−0.5	O
0	O
0.5	O
1	O
−0.5	O
0	O
0.5	O
1	O
figure	O
4.7	O
illustration	O
of	O
the	O
convergence	O
of	O
the	O
perceptron	B
learning	O
algorithm	O
,	O
showing	O
data	O
points	O
from	O
two	O
classes	O
(	O
red	O
and	O
blue	O
)	O
in	O
a	O
two-dimensional	O
feature	B
space	I
(	O
φ1	O
,	O
φ2	O
)	O
.	O
the	O
top	O
left	O
plot	O
shows	O
the	O
initial	O
parameter	O
vector	O
w	O
shown	O
as	O
a	O
black	O
arrow	O
together	O
with	O
the	O
corresponding	O
decision	B
boundary	I
(	O
black	O
line	O
)	O
,	O
in	O
which	O
the	O
arrow	O
points	O
towards	O
the	O
decision	B
region	I
which	O
classiﬁed	O
as	O
belonging	O
to	O
the	O
red	O
class	O
.	O
the	O
data	O
point	O
circled	O
in	O
green	O
is	O
misclassiﬁed	O
and	O
so	O
its	O
feature	O
vector	O
is	O
added	O
to	O
the	O
current	O
weight	B
vector	I
,	O
giving	O
the	O
new	O
decision	B
boundary	I
shown	O
in	O
the	O
top	O
right	O
plot	O
.	O
the	O
bottom	O
left	O
plot	O
shows	O
the	O
next	O
misclassiﬁed	O
point	O
to	O
be	O
considered	O
,	O
indicated	O
by	O
the	O
green	O
circle	O
,	O
and	O
its	O
feature	O
vector	O
is	O
again	O
added	O
to	O
the	O
weight	B
vector	I
giving	O
the	O
decision	B
boundary	I
shown	O
in	O
the	O
bottom	O
right	O
plot	O
for	O
which	O
all	O
data	O
points	O
are	O
correctly	O
classiﬁed	O
.	O
196	O
4.	O
linear	O
models	O
for	O
classification	O
figure	O
4.8	O
illustration	O
of	O
the	O
mark	O
1	O
perceptron	B
hardware	O
.	O
the	O
photograph	O
on	O
the	O
left	O
shows	O
how	O
the	O
inputs	O
were	O
obtained	O
using	O
a	O
simple	O
camera	O
system	O
in	O
which	O
an	O
input	O
scene	O
,	O
in	O
this	O
case	O
a	O
printed	O
character	O
,	O
was	O
illuminated	O
by	O
powerful	O
lights	O
,	O
and	O
an	O
image	O
focussed	O
onto	O
a	O
20	O
×	O
20	O
array	O
of	O
cadmium	O
sulphide	O
photocells	O
,	O
giving	O
a	O
primitive	O
400	O
pixel	O
image	O
.	O
the	O
perceptron	B
also	O
had	O
a	O
patch	O
board	O
,	O
shown	O
in	O
the	O
middle	O
photograph	O
,	O
which	O
allowed	O
different	O
conﬁgurations	O
of	O
input	O
features	O
to	O
be	O
tried	O
.	O
often	O
these	O
were	O
wired	O
up	O
at	O
random	O
to	O
demonstrate	O
the	O
ability	O
of	O
the	O
perceptron	B
to	O
learn	O
without	O
the	O
need	O
for	O
precise	O
wiring	O
,	O
in	O
contrast	O
to	O
a	O
modern	O
digital	O
computer	O
.	O
the	O
photograph	O
on	O
the	O
right	O
shows	O
one	O
of	O
the	O
racks	O
of	O
adaptive	O
weights	O
.	O
each	O
weight	O
was	O
implemented	O
using	O
a	O
rotary	O
variable	O
resistor	O
,	O
also	O
called	O
a	O
potentiometer	O
,	O
driven	O
by	O
an	O
electric	O
motor	O
thereby	O
allowing	O
the	O
value	O
of	O
the	O
weight	O
to	O
be	O
adjusted	O
automatically	O
by	O
the	O
learning	B
algorithm	O
.	O
aside	O
from	O
difﬁculties	O
with	O
the	O
learning	B
algorithm	O
,	O
the	O
perceptron	B
does	O
not	O
pro-	O
vide	O
probabilistic	O
outputs	O
,	O
nor	O
does	O
it	O
generalize	O
readily	O
to	O
k	O
>	O
2	O
classes	O
.	O
the	O
most	O
important	O
limitation	O
,	O
however	O
,	O
arises	O
from	O
the	O
fact	O
that	O
(	O
in	O
common	O
with	O
all	O
of	O
the	O
models	O
discussed	O
in	O
this	O
chapter	O
and	O
the	O
previous	O
one	O
)	O
it	O
is	O
based	O
on	O
linear	O
com-	O
binations	O
of	O
ﬁxed	O
basis	O
functions	O
.	O
more	O
detailed	O
discussions	O
of	O
the	O
limitations	O
of	O
perceptrons	O
can	O
be	O
found	O
in	O
minsky	O
and	O
papert	O
(	O
1969	O
)	O
and	O
bishop	O
(	O
1995a	O
)	O
.	O
analogue	O
hardware	B
implementations	O
of	O
the	O
perceptron	B
were	O
built	O
by	O
rosenblatt	O
,	O
based	O
on	O
motor-driven	O
variable	O
resistors	O
to	O
implement	O
the	O
adaptive	O
parameters	O
wj	O
.	O
these	O
are	O
illustrated	O
in	O
figure	O
4.8.	O
the	O
inputs	O
were	O
obtained	O
from	O
a	O
simple	O
camera	O
system	O
based	O
on	O
an	O
array	O
of	O
photo-sensors	O
,	O
while	O
the	O
basis	O
functions	O
φ	O
could	O
be	O
chosen	O
in	O
a	O
variety	O
of	O
ways	O
,	O
for	O
example	O
based	O
on	O
simple	O
ﬁxed	O
functions	O
of	O
randomly	O
chosen	O
subsets	O
of	O
pixels	O
from	O
the	O
input	O
image	O
.	O
typical	O
applications	O
involved	O
learning	B
to	O
discriminate	O
simple	O
shapes	O
or	O
characters	O
.	O
at	O
the	O
same	O
time	O
that	O
the	O
perceptron	B
was	O
being	O
developed	O
,	O
a	O
closely	O
related	O
system	O
called	O
the	O
adaline	B
,	O
which	O
is	O
short	O
for	O
‘	O
adaptive	O
linear	O
element	O
’	O
,	O
was	O
being	O
explored	O
by	O
widrow	O
and	O
co-workers	O
.	O
the	O
functional	B
form	O
of	O
the	O
model	O
was	O
the	O
same	O
as	O
for	O
the	O
perceptron	B
,	O
but	O
a	O
different	O
approach	O
to	O
training	B
was	O
adopted	O
(	O
widrow	O
and	O
hoff	O
,	O
1960	O
;	O
widrow	O
and	O
lehr	O
,	O
1990	O
)	O
.	O
4.2.	O
probabilistic	O
generative	O
models	O
we	O
turn	O
next	O
to	O
a	O
probabilistic	O
view	O
of	O
classiﬁcation	O
and	O
show	O
how	O
models	O
with	O
linear	O
decision	O
boundaries	O
arise	O
from	O
simple	O
assumptions	O
about	O
the	O
distribution	O
of	O
the	O
data	O
.	O
in	O
section	O
1.5.4	O
,	O
we	O
discussed	O
the	O
distinction	O
between	O
the	O
discriminative	O
and	O
the	O
generative	O
approaches	O
to	O
classiﬁcation	B
.	O
here	O
we	O
shall	O
adopt	O
a	O
generative	O
4.2.	O
probabilistic	O
generative	O
models	O
197	O
figure	O
4.9	O
plot	O
of	O
the	O
logistic	B
sigmoid	I
function	O
σ	O
(	O
a	O
)	O
deﬁned	O
by	O
(	O
4.59	O
)	O
,	O
shown	O
in	O
red	O
,	O
together	O
with	O
the	O
scaled	O
pro-	O
bit	O
function	O
φ	O
(	O
λa	O
)	O
,	O
for	O
λ2	O
=	O
π/8	O
,	O
shown	O
in	O
dashed	O
blue	O
,	O
where	O
φ	O
(	O
a	O
)	O
is	O
deﬁned	O
by	O
(	O
4.114	O
)	O
.	O
the	O
scal-	O
ing	O
factor	O
π/8	O
is	O
chosen	O
so	O
that	O
the	O
derivatives	O
of	O
the	O
two	O
curves	O
are	O
equal	O
for	O
a	O
=	O
0	O
.	O
1	O
0.5	O
0	O
−5	O
0	O
5	O
approach	O
in	O
which	O
we	O
model	O
the	O
class-conditional	O
densities	O
p	O
(	O
x|ck	O
)	O
,	O
as	O
well	O
as	O
the	O
class	O
priors	O
p	O
(	O
ck	O
)	O
,	O
and	O
then	O
use	O
these	O
to	O
compute	O
posterior	O
probabilities	O
p	O
(	O
ck|x	O
)	O
through	O
bayes	O
’	O
theorem	O
.	O
c1	O
can	O
be	O
written	O
as	O
consider	O
ﬁrst	O
of	O
all	O
the	O
case	O
of	O
two	O
classes	O
.	O
the	O
posterior	B
probability	I
for	O
class	O
p	O
(	O
x|c1	O
)	O
p	O
(	O
c1	O
)	O
p	O
(	O
c1|x	O
)	O
=	O
p	O
(	O
x|c1	O
)	O
p	O
(	O
c1	O
)	O
+	O
p	O
(	O
x|c2	O
)	O
p	O
(	O
c2	O
)	O
1	O
+	O
exp	O
(	O
−a	O
)	O
a	O
=	O
ln	O
p	O
(	O
x|c1	O
)	O
p	O
(	O
c1	O
)	O
p	O
(	O
x|c2	O
)	O
p	O
(	O
c2	O
)	O
and	O
σ	O
(	O
a	O
)	O
is	O
the	O
logistic	B
sigmoid	I
function	O
deﬁned	O
by	O
where	O
we	O
have	O
deﬁned	O
=	O
σ	O
(	O
a	O
)	O
=	O
1	O
σ	O
(	O
a	O
)	O
=	O
1	O
1	O
+	O
exp	O
(	O
−a	O
)	O
(	O
4.57	O
)	O
(	O
4.58	O
)	O
(	O
4.59	O
)	O
which	O
is	O
plotted	O
in	O
figure	O
4.9.	O
the	O
term	O
‘	O
sigmoid	B
’	O
means	O
s-shaped	O
.	O
this	O
type	O
of	O
function	O
is	O
sometimes	O
also	O
called	O
a	O
‘	O
squashing	O
function	O
’	O
because	O
it	O
maps	O
the	O
whole	O
real	O
axis	O
into	O
a	O
ﬁnite	O
interval	O
.	O
the	O
logistic	B
sigmoid	I
has	O
been	O
encountered	O
already	O
in	O
earlier	O
chapters	O
and	O
plays	O
an	O
important	O
role	O
in	O
many	O
classiﬁcation	B
algorithms	O
.	O
it	O
satisﬁes	O
the	O
following	O
symmetry	O
property	O
as	O
is	O
easily	O
veriﬁed	O
.	O
the	O
inverse	B
of	O
the	O
logistic	B
sigmoid	I
is	O
given	O
by	O
σ	O
(	O
−a	O
)	O
=	O
1	O
−	O
σ	O
(	O
a	O
)	O
(	O
cid:17	O
)	O
(	O
cid:18	O
)	O
a	O
=	O
ln	O
σ	O
1	O
−	O
σ	O
(	O
4.60	O
)	O
(	O
4.61	O
)	O
and	O
is	O
known	O
as	O
the	O
logit	B
function	I
.	O
it	O
represents	O
the	O
log	O
of	O
the	O
ratio	O
of	O
probabilities	O
ln	O
[	O
p	O
(	O
c1|x	O
)	O
/p	O
(	O
c2|x	O
)	O
]	O
for	O
the	O
two	O
classes	O
,	O
also	O
known	O
as	O
the	O
log	B
odds	I
.	O
198	O
4.	O
linear	O
models	O
for	O
classification	O
note	O
that	O
in	O
(	O
4.57	O
)	O
we	O
have	O
simply	O
rewritten	O
the	O
posterior	O
probabilities	O
in	O
an	O
equivalent	O
form	O
,	O
and	O
so	O
the	O
appearance	O
of	O
the	O
logistic	B
sigmoid	I
may	O
seem	O
rather	O
vac-	O
uous	O
.	O
however	O
,	O
it	O
will	O
have	O
signiﬁcance	O
provided	O
a	O
(	O
x	O
)	O
takes	O
a	O
simple	O
functional	B
form	O
.	O
we	O
shall	O
shortly	O
consider	O
situations	O
in	O
which	O
a	O
(	O
x	O
)	O
is	O
a	O
linear	O
function	O
of	O
x	O
,	O
in	O
which	O
case	O
the	O
posterior	B
probability	I
is	O
governed	O
by	O
a	O
generalized	B
linear	I
model	I
.	O
for	O
the	O
case	O
of	O
k	O
>	O
2	O
classes	O
,	O
we	O
have	O
p	O
(	O
ck|x	O
)	O
=	O
=	O
(	O
cid:5	O
)	O
p	O
(	O
x|ck	O
)	O
p	O
(	O
ck	O
)	O
j	O
p	O
(	O
x|cj	O
)	O
p	O
(	O
cj	O
)	O
(	O
cid:5	O
)	O
exp	O
(	O
ak	O
)	O
j	O
exp	O
(	O
aj	O
)	O
(	O
4.62	O
)	O
which	O
is	O
known	O
as	O
the	O
normalized	B
exponential	I
and	O
can	O
be	O
regarded	O
as	O
a	O
multiclass	B
generalization	O
of	O
the	O
logistic	B
sigmoid	I
.	O
here	O
the	O
quantities	O
ak	O
are	O
deﬁned	O
by	O
ak	O
=	O
ln	O
p	O
(	O
x|ck	O
)	O
p	O
(	O
ck	O
)	O
.	O
(	O
4.63	O
)	O
the	O
normalized	B
exponential	I
is	O
also	O
known	O
as	O
the	O
softmax	B
function	I
,	O
as	O
it	O
represents	O
a	O
smoothed	O
version	O
of	O
the	O
‘	O
max	O
’	O
function	O
because	O
,	O
if	O
ak	O
(	O
cid:12	O
)	O
aj	O
for	O
all	O
j	O
(	O
cid:9	O
)	O
=	O
k	O
,	O
then	O
p	O
(	O
ck|x	O
)	O
(	O
cid:7	O
)	O
1	O
,	O
and	O
p	O
(	O
cj|x	O
)	O
(	O
cid:7	O
)	O
0.	O
we	O
now	O
investigate	O
the	O
consequences	O
of	O
choosing	O
speciﬁc	O
forms	O
for	O
the	O
class-	O
conditional	B
densities	O
,	O
looking	O
ﬁrst	O
at	O
continuous	O
input	O
variables	O
x	O
and	O
then	O
dis-	O
cussing	O
brieﬂy	O
the	O
case	O
of	O
discrete	O
inputs	O
.	O
4.2.1	O
continuous	O
inputs	O
let	O
us	O
assume	O
that	O
the	O
class-conditional	O
densities	O
are	O
gaussian	O
and	O
then	O
explore	O
the	O
resulting	O
form	O
for	O
the	O
posterior	O
probabilities	O
.	O
to	O
start	O
with	O
,	O
we	O
shall	O
assume	O
that	O
all	O
classes	O
share	O
the	O
same	O
covariance	B
matrix	I
.	O
thus	O
the	O
density	B
for	O
class	O
ck	O
is	O
given	O
by	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
−1	O
(	O
x	O
−	O
µk	O
)	O
p	O
(	O
x|ck	O
)	O
=	O
1	O
(	O
2π	O
)	O
d/2	O
1	O
|σ|1/2	O
exp	O
−1	O
2	O
(	O
x	O
−	O
µk	O
)	O
tς	O
consider	O
ﬁrst	O
the	O
case	O
of	O
two	O
classes	O
.	O
from	O
(	O
4.57	O
)	O
and	O
(	O
4.58	O
)	O
,	O
we	O
have	O
p	O
(	O
c1|x	O
)	O
=	O
σ	O
(	O
wtx	O
+	O
w0	O
)	O
where	O
we	O
have	O
deﬁned	O
w	O
=	O
σ	O
w0	O
=	O
−1	O
2	O
−1	O
(	O
µ1	O
−	O
µ2	O
)	O
−1µ1	O
+	O
µt	O
1	O
σ	O
1	O
2	O
−1µ2	O
+	O
ln	O
p	O
(	O
c1	O
)	O
p	O
(	O
c2	O
)	O
.	O
µt	O
2	O
σ	O
.	O
(	O
4.64	O
)	O
(	O
4.65	O
)	O
(	O
4.66	O
)	O
(	O
4.67	O
)	O
we	O
see	O
that	O
the	O
quadratic	O
terms	O
in	O
x	O
from	O
the	O
exponents	O
of	O
the	O
gaussian	O
densities	O
have	O
cancelled	O
(	O
due	O
to	O
the	O
assumption	O
of	O
common	O
covariance	B
matrices	O
)	O
leading	O
to	O
a	O
linear	O
function	O
of	O
x	O
in	O
the	O
argument	O
of	O
the	O
logistic	B
sigmoid	I
.	O
this	O
result	O
is	O
illus-	O
trated	O
for	O
the	O
case	O
of	O
a	O
two-dimensional	O
input	O
space	O
x	O
in	O
figure	O
4.10.	O
the	O
resulting	O
4.2.	O
probabilistic	O
generative	O
models	O
199	O
figure	O
4.10	O
the	O
left-hand	O
plot	O
shows	O
the	O
class-conditional	O
densities	O
for	O
two	O
classes	O
,	O
denoted	O
red	O
and	O
blue	O
.	O
on	O
the	O
right	O
is	O
the	O
corresponding	O
posterior	B
probability	I
p	O
(	O
c1|x	O
)	O
,	O
which	O
is	O
given	O
by	O
a	O
logistic	B
sigmoid	I
of	O
a	O
linear	O
function	O
of	O
x.	O
the	O
surface	O
in	O
the	O
right-hand	O
plot	O
is	O
coloured	O
using	O
a	O
proportion	O
of	O
red	O
ink	O
given	O
by	O
p	O
(	O
c1|x	O
)	O
and	O
a	O
proportion	O
of	O
blue	O
ink	O
given	O
by	O
p	O
(	O
c2|x	O
)	O
=	O
1	O
−	O
p	O
(	O
c1|x	O
)	O
.	O
decision	O
boundaries	O
correspond	O
to	O
surfaces	O
along	O
which	O
the	O
posterior	O
probabilities	O
p	O
(	O
ck|x	O
)	O
are	O
constant	O
and	O
so	O
will	O
be	O
given	O
by	O
linear	O
functions	O
of	O
x	O
,	O
and	O
therefore	O
the	O
decision	O
boundaries	O
are	O
linear	O
in	O
input	O
space	O
.	O
the	O
prior	B
probabilities	O
p	O
(	O
ck	O
)	O
enter	O
only	O
through	O
the	O
bias	B
parameter	I
w0	O
so	O
that	O
changes	O
in	O
the	O
priors	O
have	O
the	O
effect	O
of	O
making	O
parallel	O
shifts	O
of	O
the	O
decision	B
boundary	I
and	O
more	O
generally	O
of	O
the	O
parallel	O
contours	O
of	O
constant	O
posterior	B
probability	I
.	O
for	O
the	O
general	O
case	O
of	O
k	O
classes	O
we	O
have	O
,	O
from	O
(	O
4.62	O
)	O
and	O
(	O
4.63	O
)	O
,	O
ak	O
(	O
x	O
)	O
=	O
wt	O
k	O
x	O
+	O
wk0	O
where	O
we	O
have	O
deﬁned	O
wk	O
=	O
σ	O
wk0	O
=	O
−1	O
2	O
−1µk	O
µt	O
k	O
σ	O
−1µk	O
+	O
ln	O
p	O
(	O
ck	O
)	O
.	O
(	O
4.68	O
)	O
(	O
4.69	O
)	O
(	O
4.70	O
)	O
we	O
see	O
that	O
the	O
ak	O
(	O
x	O
)	O
are	O
again	O
linear	O
functions	O
of	O
x	O
as	O
a	O
consequence	O
of	O
the	O
cancel-	O
lation	O
of	O
the	O
quadratic	O
terms	O
due	O
to	O
the	O
shared	O
covariances	O
.	O
the	O
resulting	O
decision	O
boundaries	O
,	O
corresponding	O
to	O
the	O
minimum	O
misclassiﬁcation	O
rate	O
,	O
will	O
occur	O
when	O
two	O
of	O
the	O
posterior	O
probabilities	O
(	O
the	O
two	O
largest	O
)	O
are	O
equal	O
,	O
and	O
so	O
will	O
be	O
deﬁned	O
by	O
linear	O
functions	O
of	O
x	O
,	O
and	O
so	O
again	O
we	O
have	O
a	O
generalized	B
linear	I
model	I
.	O
if	O
we	O
relax	O
the	O
assumption	O
of	O
a	O
shared	O
covariance	O
matrix	O
and	O
allow	O
each	O
class-	O
conditional	B
density	O
p	O
(	O
x|ck	O
)	O
to	O
have	O
its	O
own	O
covariance	B
matrix	I
σk	O
,	O
then	O
the	O
earlier	O
cancellations	O
will	O
no	O
longer	O
occur	O
,	O
and	O
we	O
will	O
obtain	O
quadratic	O
functions	O
of	O
x	O
,	O
giv-	O
ing	O
rise	O
to	O
a	O
quadratic	B
discriminant	I
.	O
the	O
linear	O
and	O
quadratic	O
decision	O
boundaries	O
are	O
illustrated	O
in	O
figure	O
4.11	O
.	O
200	O
4.	O
linear	O
models	O
for	O
classification	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
−2	O
−2.5	O
−2	O
−1	O
0	O
1	O
2	O
figure	O
4.11	O
the	O
left-hand	O
plot	O
shows	O
the	O
class-conditional	O
densities	O
for	O
three	O
classes	O
each	O
having	O
a	O
gaussian	O
distribution	O
,	O
coloured	O
red	O
,	O
green	O
,	O
and	O
blue	O
,	O
in	O
which	O
the	O
red	O
and	O
green	O
classes	O
have	O
the	O
same	O
covariance	B
matrix	I
.	O
the	O
right-hand	O
plot	O
shows	O
the	O
corresponding	O
posterior	O
probabilities	O
,	O
in	O
which	O
the	O
rgb	O
colour	O
vector	O
represents	O
the	O
posterior	O
probabilities	O
for	O
the	O
respective	O
three	O
classes	O
.	O
the	O
decision	O
boundaries	O
are	O
also	O
shown	O
.	O
notice	O
that	O
the	O
boundary	O
between	O
the	O
red	O
and	O
green	O
classes	O
,	O
which	O
have	O
the	O
same	O
covariance	B
matrix	I
,	O
is	O
linear	O
,	O
whereas	O
those	O
between	O
the	O
other	O
pairs	O
of	O
classes	O
are	O
quadratic	O
.	O
4.2.2	O
maximum	B
likelihood	I
solution	O
once	O
we	O
have	O
speciﬁed	O
a	O
parametric	O
functional	B
form	O
for	O
the	O
class-conditional	O
densities	O
p	O
(	O
x|ck	O
)	O
,	O
we	O
can	O
then	O
determine	O
the	O
values	O
of	O
the	O
parameters	O
,	O
together	O
with	O
the	O
prior	B
class	O
probabilities	O
p	O
(	O
ck	O
)	O
,	O
using	O
maximum	B
likelihood	I
.	O
this	O
requires	O
a	O
data	O
set	O
comprising	O
observations	O
of	O
x	O
along	O
with	O
their	O
corresponding	O
class	O
labels	O
.	O
consider	O
ﬁrst	O
the	O
case	O
of	O
two	O
classes	O
,	O
each	O
having	O
a	O
gaussian	O
class-conditional	O
density	B
with	O
a	O
shared	O
covariance	O
matrix	O
,	O
and	O
suppose	O
we	O
have	O
a	O
data	O
set	O
{	O
xn	O
,	O
tn	O
}	O
where	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
here	O
tn	O
=	O
1	O
denotes	O
class	O
c1	O
and	O
tn	O
=	O
0	O
denotes	O
class	O
c2	O
.	O
we	O
denote	O
the	O
prior	B
class	O
probability	B
p	O
(	O
c1	O
)	O
=	O
π	O
,	O
so	O
that	O
p	O
(	O
c2	O
)	O
=	O
1	O
−	O
π.	O
for	O
a	O
data	O
point	O
xn	O
from	O
class	O
c1	O
,	O
we	O
have	O
tn	O
=	O
1	O
and	O
hence	O
p	O
(	O
xn	O
,	O
c1	O
)	O
=	O
p	O
(	O
c1	O
)	O
p	O
(	O
xn|c1	O
)	O
=	O
πn	O
(	O
xn|µ1	O
,	O
σ	O
)	O
.	O
similarly	O
for	O
class	O
c2	O
,	O
we	O
have	O
tn	O
=	O
0	O
and	O
hence	O
p	O
(	O
xn	O
,	O
c2	O
)	O
=	O
p	O
(	O
c2	O
)	O
p	O
(	O
xn|c2	O
)	O
=	O
(	O
1	O
−	O
π	O
)	O
n	O
(	O
xn|µ2	O
,	O
σ	O
)	O
.	O
thus	O
the	O
likelihood	B
function	I
is	O
given	O
by	O
n	O
(	O
cid:14	O
)	O
p	O
(	O
t|π	O
,	O
µ1	O
,	O
µ2	O
,	O
σ	O
)	O
=	O
[	O
πn	O
(	O
xn|µ1	O
,	O
σ	O
)	O
]	O
tn	O
[	O
(	O
1	O
−	O
π	O
)	O
n	O
(	O
xn|µ2	O
,	O
σ	O
)	O
]	O
1−tn	O
(	O
4.71	O
)	O
n=1	O
where	O
t	O
=	O
(	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
)	O
t.	O
as	O
usual	O
,	O
it	O
is	O
convenient	O
to	O
maximize	O
the	O
log	O
of	O
the	O
likelihood	B
function	I
.	O
consider	O
ﬁrst	O
the	O
maximization	O
with	O
respect	O
to	O
π.	O
the	O
terms	O
in	O
exercise	O
4.9	O
4.2.	O
probabilistic	O
generative	O
models	O
201	O
the	O
log	O
likelihood	O
function	O
that	O
depend	O
on	O
π	O
are	O
{	O
tn	O
ln	O
π	O
+	O
(	O
1	O
−	O
tn	O
)	O
ln	O
(	O
1	O
−	O
π	O
)	O
}	O
.	O
(	O
4.72	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
n	O
(	O
cid:2	O
)	O
1	O
n	O
setting	O
the	O
derivative	B
with	O
respect	O
to	O
π	O
equal	O
to	O
zero	O
and	O
rearranging	O
,	O
we	O
obtain	O
tn	O
=	O
n1	O
n	O
=	O
n1	O
π	O
=	O
n=1	O
n1	O
+	O
n2	O
(	O
4.73	O
)	O
where	O
n1	O
denotes	O
the	O
total	O
number	O
of	O
data	O
points	O
in	O
class	O
c1	O
,	O
and	O
n2	O
denotes	O
the	O
total	O
number	O
of	O
data	O
points	O
in	O
class	O
c2	O
.	O
thus	O
the	O
maximum	B
likelihood	I
estimate	O
for	O
π	O
is	O
simply	O
the	O
fraction	O
of	O
points	O
in	O
class	O
c1	O
as	O
expected	O
.	O
this	O
result	O
is	O
easily	O
generalized	B
to	O
the	O
multiclass	B
case	O
where	O
again	O
the	O
maximum	B
likelihood	I
estimate	O
of	O
the	O
prior	B
probability	O
associated	O
with	O
class	O
ck	O
is	O
given	O
by	O
the	O
fraction	O
of	O
the	O
training	B
set	I
points	O
assigned	O
to	O
that	O
class	O
.	O
n	O
(	O
cid:2	O
)	O
now	O
consider	O
the	O
maximization	O
with	O
respect	O
to	O
µ1	O
.	O
again	O
we	O
can	O
pick	O
out	O
of	O
the	O
log	O
likelihood	O
function	O
those	O
terms	O
that	O
depend	O
on	O
µ1	O
giving	O
n	O
(	O
cid:2	O
)	O
tn	O
(	O
xn	O
−	O
µ1	O
)	O
tς	O
−1	O
(	O
xn	O
−	O
µ1	O
)	O
+	O
const	O
.	O
(	O
4.74	O
)	O
tn	O
lnn	O
(	O
xn|µ1	O
,	O
σ	O
)	O
=	O
−1	O
2	O
n=1	O
n=1	O
setting	O
the	O
derivative	B
with	O
respect	O
to	O
µ1	O
to	O
zero	O
and	O
rearranging	O
,	O
we	O
obtain	O
(	O
4.75	O
)	O
which	O
is	O
simply	O
the	O
mean	B
of	O
all	O
the	O
input	O
vectors	O
xn	O
assigned	O
to	O
class	O
c1	O
.	O
by	O
a	O
similar	O
argument	O
,	O
the	O
corresponding	O
result	O
for	O
µ2	O
is	O
given	O
by	O
n=1	O
tnxn	O
µ1	O
=	O
µ2	O
=	O
1	O
n2	O
(	O
1	O
−	O
tn	O
)	O
xn	O
(	O
4.76	O
)	O
which	O
again	O
is	O
the	O
mean	B
of	O
all	O
the	O
input	O
vectors	O
xn	O
assigned	O
to	O
class	O
c2	O
.	O
finally	O
,	O
consider	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
shared	O
covariance	O
matrix	O
σ.	O
picking	O
out	O
the	O
terms	O
in	O
the	O
log	O
likelihood	O
function	O
that	O
depend	O
on	O
σ	O
,	O
we	O
have	O
n	O
(	O
cid:2	O
)	O
1	O
n1	O
n	O
(	O
cid:2	O
)	O
n=1	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
n	O
(	O
cid:2	O
)	O
n=1	O
−1	O
(	O
xn	O
−	O
µ1	O
)	O
−1	O
2	O
tn	O
ln|σ|	O
−	O
1	O
2	O
−1	O
2	O
=	O
−	O
n	O
2	O
n=1	O
(	O
1	O
−	O
tn	O
)	O
ln|σ|	O
−	O
1	O
2	O
−1s	O
ln|σ|	O
−	O
n	O
(	O
cid:26	O
)	O
σ	O
2	O
tr	O
tn	O
(	O
xn	O
−	O
µ1	O
)	O
tς	O
n	O
(	O
cid:2	O
)	O
(	O
cid:27	O
)	O
n=1	O
(	O
1	O
−	O
tn	O
)	O
(	O
xn	O
−	O
µ2	O
)	O
tς	O
−1	O
(	O
xn	O
−	O
µ2	O
)	O
(	O
4.77	O
)	O
202	O
4.	O
linear	O
models	O
for	O
classification	O
where	O
we	O
have	O
deﬁned	O
s1	O
=	O
s	O
=	O
n1	O
n	O
1	O
n1	O
1	O
n2	O
s2	O
=	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
n∈c1	O
n∈c2	O
s2	O
s1	O
+	O
n2	O
n	O
(	O
xn	O
−	O
µ1	O
)	O
(	O
xn	O
−	O
µ1	O
)	O
t	O
(	O
xn	O
−	O
µ2	O
)	O
(	O
xn	O
−	O
µ2	O
)	O
t.	O
(	O
4.78	O
)	O
(	O
4.79	O
)	O
(	O
4.80	O
)	O
using	O
the	O
standard	O
result	O
for	O
the	O
maximum	B
likelihood	I
solution	O
for	O
a	O
gaussian	O
distri-	O
bution	O
,	O
we	O
see	O
that	O
σ	O
=	O
s	O
,	O
which	O
represents	O
a	O
weighted	O
average	O
of	O
the	O
covariance	B
matrices	O
associated	O
with	O
each	O
of	O
the	O
two	O
classes	O
separately	O
.	O
this	O
result	O
is	O
easily	O
extended	B
to	O
the	O
k	O
class	O
problem	O
to	O
obtain	O
the	O
corresponding	O
maximum	B
likelihood	I
solutions	O
for	O
the	O
parameters	O
in	O
which	O
each	O
class-conditional	O
density	B
is	O
gaussian	O
with	O
a	O
shared	O
covariance	O
matrix	O
.	O
note	O
that	O
the	O
approach	O
of	O
ﬁtting	O
gaussian	O
distributions	O
to	O
the	O
classes	O
is	O
not	O
robust	O
to	O
outliers	B
,	O
because	O
the	O
maximum	B
likelihood	I
estimation	O
of	O
a	O
gaussian	O
is	O
not	O
robust	O
.	O
4.2.3	O
discrete	O
features	O
let	O
us	O
now	O
consider	O
the	O
case	O
of	O
discrete	O
feature	O
values	O
xi	O
.	O
for	O
simplicity	O
,	O
we	O
begin	O
by	O
looking	O
at	O
binary	O
feature	O
values	O
xi	O
∈	O
{	O
0	O
,	O
1	O
}	O
and	O
discuss	O
the	O
extension	O
to	O
more	O
general	O
discrete	O
features	O
shortly	O
.	O
if	O
there	O
are	O
d	O
inputs	O
,	O
then	O
a	O
general	O
distribu-	O
tion	O
would	O
correspond	O
to	O
a	O
table	O
of	O
2d	O
numbers	O
for	O
each	O
class	O
,	O
containing	O
2d	O
−	O
1	O
independent	B
variables	I
(	O
due	O
to	O
the	O
summation	O
constraint	O
)	O
.	O
because	O
this	O
grows	O
expo-	O
nentially	O
with	O
the	O
number	O
of	O
features	O
,	O
we	O
might	O
seek	O
a	O
more	O
restricted	O
representa-	O
tion	O
.	O
here	O
we	O
will	O
make	O
the	O
naive	O
bayes	O
assumption	O
in	O
which	O
the	O
feature	O
values	O
are	O
treated	O
as	O
independent	B
,	O
conditioned	O
on	O
the	O
class	O
ck	O
.	O
thus	O
we	O
have	O
class-conditional	O
distributions	O
of	O
the	O
form	O
p	O
(	O
x|ck	O
)	O
=	O
ki	O
(	O
1	O
−	O
µki	O
)	O
1−xi	O
µxi	O
(	O
4.81	O
)	O
d	O
(	O
cid:14	O
)	O
i=1	O
exercise	O
4.10	O
section	O
2.3.7	O
section	O
8.2.2	O
which	O
contain	O
d	O
independent	B
parameters	O
for	O
each	O
class	O
.	O
substituting	O
into	O
(	O
4.63	O
)	O
then	O
gives	O
ak	O
(	O
x	O
)	O
=	O
{	O
xi	O
ln	O
µki	O
+	O
(	O
1	O
−	O
xi	O
)	O
ln	O
(	O
1	O
−	O
µki	O
)	O
}	O
+	O
ln	O
p	O
(	O
ck	O
)	O
(	O
4.82	O
)	O
d	O
(	O
cid:2	O
)	O
exercise	O
4.11	O
i=1	O
which	O
again	O
are	O
linear	O
functions	O
of	O
the	O
input	O
values	O
xi	O
.	O
for	O
the	O
case	O
of	O
k	O
=	O
2	O
classes	O
,	O
we	O
can	O
alternatively	O
consider	O
the	O
logistic	B
sigmoid	I
formulation	O
given	O
by	O
(	O
4.57	O
)	O
.	O
anal-	O
ogous	O
results	O
are	O
obtained	O
for	O
discrete	O
variables	O
each	O
of	O
which	O
can	O
take	O
m	O
>	O
2	O
states	O
.	O
4.2.4	O
exponential	B
family	I
as	O
we	O
have	O
seen	O
,	O
for	O
both	O
gaussian	O
distributed	O
and	O
discrete	O
inputs	O
,	O
the	O
posterior	O
class	O
probabilities	O
are	O
given	O
by	O
generalized	O
linear	O
models	O
with	O
logistic	B
sigmoid	I
(	O
k	O
=	O
4.3.	O
probabilistic	O
discriminative	O
models	O
203	O
2	O
classes	O
)	O
or	O
softmax	O
(	O
k	O
(	O
cid:2	O
)	O
2	O
classes	O
)	O
activation	O
functions	O
.	O
these	O
are	O
particular	O
cases	O
of	O
a	O
more	O
general	O
result	O
obtained	O
by	O
assuming	O
that	O
the	O
class-conditional	O
densities	O
p	O
(	O
x|ck	O
)	O
are	O
members	O
of	O
the	O
exponential	B
family	I
of	O
distributions	O
.	O
(	O
cid:27	O
)	O
using	O
the	O
form	O
(	O
2.194	O
)	O
for	O
members	O
of	O
the	O
exponential	B
family	I
,	O
we	O
see	O
that	O
the	O
distribution	O
of	O
x	O
can	O
be	O
written	O
in	O
the	O
form	O
(	O
cid:26	O
)	O
p	O
(	O
x|λk	O
)	O
=	O
h	O
(	O
x	O
)	O
g	O
(	O
λk	O
)	O
exp	O
λt	O
k	O
u	O
(	O
x	O
)	O
.	O
(	O
4.83	O
)	O
we	O
now	O
restrict	O
attention	O
to	O
the	O
subclass	O
of	O
such	O
distributions	O
for	O
which	O
u	O
(	O
x	O
)	O
=	O
x.	O
then	O
we	O
make	O
use	O
of	O
(	O
2.236	O
)	O
to	O
introduce	O
a	O
scaling	O
parameter	O
s	O
,	O
so	O
that	O
we	O
obtain	O
the	O
restricted	O
set	O
of	O
exponential	B
family	I
class-conditional	O
densities	O
of	O
the	O
form	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
(	O
cid:13	O
)	O
(	O
cid:12	O
)	O
1	O
s	O
p	O
(	O
x|λk	O
,	O
s	O
)	O
=	O
1	O
s	O
h	O
1	O
s	O
x	O
g	O
(	O
λk	O
)	O
exp	O
λt	O
k	O
x	O
.	O
(	O
4.84	O
)	O
note	O
that	O
we	O
are	O
allowing	O
each	O
class	O
to	O
have	O
its	O
own	O
parameter	O
vector	O
λk	O
but	O
we	O
are	O
assuming	O
that	O
the	O
classes	O
share	O
the	O
same	O
scale	B
parameter	I
s.	O
for	O
the	O
two-class	O
problem	O
,	O
we	O
substitute	O
this	O
expression	O
for	O
the	O
class-conditional	O
densities	O
into	O
(	O
4.58	O
)	O
and	O
we	O
see	O
that	O
the	O
posterior	O
class	O
probability	B
is	O
again	O
given	O
by	O
a	O
logistic	B
sigmoid	I
acting	O
on	O
a	O
linear	O
function	O
a	O
(	O
x	O
)	O
which	O
is	O
given	O
by	O
a	O
(	O
x	O
)	O
=	O
(	O
λ1	O
−	O
λ2	O
)	O
tx	O
+	O
ln	O
g	O
(	O
λ1	O
)	O
−	O
ln	O
g	O
(	O
λ2	O
)	O
+	O
ln	O
p	O
(	O
c1	O
)	O
−	O
ln	O
p	O
(	O
c2	O
)	O
.	O
(	O
4.85	O
)	O
similarly	O
,	O
for	O
the	O
k-class	O
problem	O
,	O
we	O
substitute	O
the	O
class-conditional	O
density	B
ex-	O
pression	O
into	O
(	O
4.63	O
)	O
to	O
give	O
ak	O
(	O
x	O
)	O
=	O
λt	O
k	O
x	O
+	O
ln	O
g	O
(	O
λk	O
)	O
+	O
ln	O
p	O
(	O
ck	O
)	O
(	O
4.86	O
)	O
and	O
so	O
again	O
is	O
a	O
linear	O
function	O
of	O
x	O
.	O
4.3.	O
probabilistic	O
discriminative	O
models	O
for	O
the	O
two-class	O
classiﬁcation	B
problem	O
,	O
we	O
have	O
seen	O
that	O
the	O
posterior	B
probability	I
of	O
class	O
c1	O
can	O
be	O
written	O
as	O
a	O
logistic	B
sigmoid	I
acting	O
on	O
a	O
linear	O
function	O
of	O
x	O
,	O
for	O
a	O
wide	O
choice	O
of	O
class-conditional	O
distributions	O
p	O
(	O
x|ck	O
)	O
.	O
similarly	O
,	O
for	O
the	O
multiclass	B
case	O
,	O
the	O
posterior	B
probability	I
of	O
class	O
ck	O
is	O
given	O
by	O
a	O
softmax	O
transformation	O
of	O
a	O
linear	O
function	O
of	O
x.	O
for	O
speciﬁc	O
choices	O
of	O
the	O
class-conditional	O
densities	O
p	O
(	O
x|ck	O
)	O
,	O
we	O
have	O
used	O
maximum	B
likelihood	I
to	O
determine	O
the	O
parameters	O
of	O
the	O
densities	O
as	O
well	O
as	O
the	O
class	O
priors	O
p	O
(	O
ck	O
)	O
and	O
then	O
used	O
bayes	O
’	O
theorem	O
to	O
ﬁnd	O
the	O
posterior	O
class	O
probabilities	O
.	O
however	O
,	O
an	O
alternative	O
approach	O
is	O
to	O
use	O
the	O
functional	B
form	O
of	O
the	O
generalized	B
linear	I
model	I
explicitly	O
and	O
to	O
determine	O
its	O
parameters	O
directly	O
by	O
using	O
maximum	B
likelihood	I
.	O
we	O
shall	O
see	O
that	O
there	O
is	O
an	O
efﬁcient	O
algorithm	O
ﬁnding	O
such	O
solutions	O
known	O
as	O
iterative	B
reweighted	I
least	I
squares	I
,	O
or	O
irls	O
.	O
the	O
indirect	O
approach	O
to	O
ﬁnding	O
the	O
parameters	O
of	O
a	O
generalized	B
linear	I
model	I
,	O
by	O
ﬁtting	O
class-conditional	O
densities	O
and	O
class	O
priors	O
separately	O
and	O
then	O
applying	O
204	O
4.	O
linear	O
models	O
for	O
classification	O
x2	O
1	O
0	O
−1	O
−1	O
0	O
x1	O
1	O
1	O
φ2	O
0.5	O
0	O
0	O
0.5	O
φ1	O
1	O
figure	O
4.12	O
illustration	O
of	O
the	O
role	O
of	O
nonlinear	O
basis	O
functions	O
in	O
linear	O
classiﬁcation	O
models	O
.	O
the	O
left	O
plot	O
shows	O
the	O
original	O
input	O
space	O
(	O
x1	O
,	O
x2	O
)	O
together	O
with	O
data	O
points	O
from	O
two	O
classes	O
labelled	O
red	O
and	O
blue	O
.	O
two	O
‘	O
gaussian	O
’	O
basis	O
functions	O
φ1	O
(	O
x	O
)	O
and	O
φ2	O
(	O
x	O
)	O
are	O
deﬁned	O
in	O
this	O
space	O
with	O
centres	O
shown	O
by	O
the	O
green	O
crosses	O
and	O
with	O
contours	O
shown	O
by	O
the	O
green	O
circles	O
.	O
the	O
right-hand	O
plot	O
shows	O
the	O
corresponding	O
feature	B
space	I
(	O
φ1	O
,	O
φ2	O
)	O
together	O
with	O
the	O
linear	O
decision	O
boundary	O
obtained	O
given	O
by	O
a	O
logistic	B
regression	I
model	O
of	O
the	O
form	O
discussed	O
in	O
section	O
4.3.2.	O
this	O
corresponds	O
to	O
a	O
nonlinear	O
decision	B
boundary	I
in	O
the	O
original	O
input	O
space	O
,	O
shown	O
by	O
the	O
black	O
curve	O
in	O
the	O
left-hand	O
plot	O
.	O
bayes	O
’	O
theorem	O
,	O
represents	O
an	O
example	O
of	O
generative	O
modelling	O
,	O
because	O
we	O
could	O
take	O
such	O
a	O
model	O
and	O
generate	O
synthetic	O
data	O
by	O
drawing	O
values	O
of	O
x	O
from	O
the	O
marginal	B
distribution	O
p	O
(	O
x	O
)	O
.	O
in	O
the	O
direct	O
approach	O
,	O
we	O
are	O
maximizing	O
a	O
likelihood	B
function	I
deﬁned	O
through	O
the	O
conditional	B
distribution	O
p	O
(	O
ck|x	O
)	O
,	O
which	O
represents	O
a	O
form	O
of	O
discriminative	O
training	O
.	O
one	O
advantage	O
of	O
the	O
discriminative	O
approach	O
is	O
that	O
there	O
will	O
typically	O
be	O
fewer	O
adaptive	O
parameters	O
to	O
be	O
determined	O
,	O
as	O
we	O
shall	O
see	O
shortly	O
.	O
it	O
may	O
also	O
lead	O
to	O
improved	O
predictive	O
performance	O
,	O
particularly	O
when	O
the	O
class-conditional	O
density	B
assumptions	O
give	O
a	O
poor	O
approximation	O
to	O
the	O
true	O
dis-	O
tributions	O
.	O
4.3.1	O
fixed	O
basis	O
functions	O
so	O
far	O
in	O
this	O
chapter	O
,	O
we	O
have	O
considered	O
classiﬁcation	B
models	O
that	O
work	O
di-	O
rectly	O
with	O
the	O
original	O
input	O
vector	O
x.	O
however	O
,	O
all	O
of	O
the	O
algorithms	O
are	O
equally	O
applicable	O
if	O
we	O
ﬁrst	O
make	O
a	O
ﬁxed	O
nonlinear	O
transformation	O
of	O
the	O
inputs	O
using	O
a	O
vector	O
of	O
basis	O
functions	O
φ	O
(	O
x	O
)	O
.	O
the	O
resulting	O
decision	O
boundaries	O
will	O
be	O
linear	O
in	O
the	O
feature	B
space	I
φ	O
,	O
and	O
these	O
correspond	O
to	O
nonlinear	O
decision	O
boundaries	O
in	O
the	O
original	O
x	O
space	O
,	O
as	O
illustrated	O
in	O
figure	O
4.12.	O
classes	O
that	O
are	O
linearly	B
separable	I
in	O
the	O
feature	B
space	I
φ	O
(	O
x	O
)	O
need	O
not	O
be	O
linearly	B
separable	I
in	O
the	O
original	O
observation	O
space	O
x.	O
note	O
that	O
as	O
in	O
our	O
discussion	O
of	O
linear	O
models	O
for	B
regression	I
,	O
one	O
of	O
the	O
4.3.	O
probabilistic	O
discriminative	O
models	O
205	O
basis	O
functions	O
is	O
typically	O
set	O
to	O
a	O
constant	O
,	O
say	O
φ0	O
(	O
x	O
)	O
=	O
1	O
,	O
so	O
that	O
the	O
correspond-	O
ing	O
parameter	O
w0	O
plays	O
the	O
role	O
of	O
a	O
bias	B
.	O
for	O
the	O
remainder	O
of	O
this	O
chapter	O
,	O
we	O
shall	O
include	O
a	O
ﬁxed	O
basis	B
function	I
transformation	O
φ	O
(	O
x	O
)	O
,	O
as	O
this	O
will	O
highlight	O
some	O
useful	O
similarities	O
to	O
the	O
regression	B
models	O
discussed	O
in	O
chapter	O
3.	O
for	O
many	O
problems	O
of	O
practical	O
interest	O
,	O
there	O
is	O
signiﬁcant	O
overlap	O
between	O
the	O
class-conditional	O
densities	O
p	O
(	O
x|ck	O
)	O
.	O
this	O
corresponds	O
to	O
posterior	O
probabilities	O
p	O
(	O
ck|x	O
)	O
,	O
which	O
,	O
for	O
at	O
least	O
some	O
values	O
of	O
x	O
,	O
are	O
not	O
0	O
or	O
1.	O
in	O
such	O
cases	O
,	O
the	O
opti-	O
mal	O
solution	O
is	O
obtained	O
by	O
modelling	O
the	O
posterior	O
probabilities	O
accurately	O
and	O
then	O
applying	O
standard	O
decision	O
theory	B
,	O
as	O
discussed	O
in	O
chapter	O
1.	O
note	O
that	O
nonlinear	O
transformations	O
φ	O
(	O
x	O
)	O
can	O
not	O
remove	O
such	O
class	O
overlap	O
.	O
indeed	O
,	O
they	O
can	O
increase	O
the	O
level	O
of	O
overlap	O
,	O
or	O
create	O
overlap	O
where	O
none	O
existed	O
in	O
the	O
original	O
observation	O
space	O
.	O
however	O
,	O
suitable	O
choices	O
of	O
nonlinearity	O
can	O
make	O
the	O
process	O
of	O
modelling	O
the	O
posterior	O
probabilities	O
easier	O
.	O
such	O
ﬁxed	O
basis	B
function	I
models	O
have	O
important	O
limitations	O
,	O
and	O
these	O
will	O
be	O
resolved	O
in	O
later	O
chapters	O
by	O
allowing	O
the	O
basis	O
functions	O
themselves	O
to	O
adapt	O
to	O
the	O
data	O
.	O
notwithstanding	O
these	O
limitations	O
,	O
models	O
with	O
ﬁxed	O
nonlinear	O
basis	O
functions	O
play	O
an	O
important	O
role	O
in	O
applications	O
,	O
and	O
a	O
discussion	O
of	O
such	O
models	O
will	O
intro-	O
duce	O
many	O
of	O
the	O
key	O
concepts	O
needed	O
for	O
an	O
understanding	O
of	O
their	O
more	O
complex	O
counterparts	O
.	O
4.3.2	O
logistic	B
regression	I
we	O
begin	O
our	O
treatment	O
of	O
generalized	O
linear	O
models	O
by	O
considering	O
the	O
problem	O
of	O
two-class	O
classiﬁcation	B
.	O
in	O
our	O
discussion	O
of	O
generative	O
approaches	O
in	O
section	O
4.2	O
,	O
we	O
saw	O
that	O
under	O
rather	O
general	O
assumptions	O
,	O
the	O
posterior	B
probability	I
of	O
class	O
c1	O
can	O
be	O
written	O
as	O
a	O
logistic	B
sigmoid	I
acting	O
on	O
a	O
linear	O
function	O
of	O
the	O
feature	O
vector	O
φ	O
so	O
that	O
(	O
4.87	O
)	O
with	O
p	O
(	O
c2|φ	O
)	O
=	O
1	O
−	O
p	O
(	O
c1|φ	O
)	O
.	O
here	O
σ	O
(	O
·	O
)	O
is	O
the	O
logistic	B
sigmoid	I
function	O
deﬁned	O
by	O
(	O
4.59	O
)	O
.	O
in	O
the	O
terminology	O
of	O
statistics	O
,	O
this	O
model	O
is	O
known	O
as	O
logistic	B
regression	I
,	O
although	O
it	O
should	O
be	O
emphasized	O
that	O
this	O
is	O
a	O
model	O
for	O
classiﬁcation	B
rather	O
than	O
regression	B
.	O
p	O
(	O
c1|φ	O
)	O
=	O
y	O
(	O
φ	O
)	O
=	O
σ	O
wtφ	O
for	O
an	O
m-dimensional	O
feature	B
space	I
φ	O
,	O
this	O
model	O
has	O
m	O
adjustable	O
parameters	O
.	O
by	O
contrast	O
,	O
if	O
we	O
had	O
ﬁtted	O
gaussian	O
class	O
conditional	B
densities	O
using	O
maximum	B
likelihood	I
,	O
we	O
would	O
have	O
used	O
2m	O
parameters	O
for	O
the	O
means	O
and	O
m	O
(	O
m	O
+	O
1	O
)	O
/2	O
parameters	O
for	O
the	O
(	O
shared	O
)	O
covariance	B
matrix	I
.	O
together	O
with	O
the	O
class	O
prior	B
p	O
(	O
c1	O
)	O
,	O
this	O
gives	O
a	O
total	O
of	O
m	O
(	O
m	O
+5	O
)	O
/2+1	O
parameters	O
,	O
which	O
grows	O
quadratically	O
with	O
m	O
,	O
in	O
contrast	O
to	O
the	O
linear	O
dependence	O
on	O
m	O
of	O
the	O
number	O
of	O
parameters	O
in	O
logistic	B
regression	I
.	O
for	O
large	O
values	O
of	O
m	O
,	O
there	O
is	O
a	O
clear	O
advantage	O
in	O
working	O
with	O
the	O
logistic	B
regression	I
model	O
directly	O
.	O
we	O
now	O
use	O
maximum	B
likelihood	I
to	O
determine	O
the	O
parameters	O
of	O
the	O
logistic	B
regression	I
model	O
.	O
to	O
do	O
this	O
,	O
we	O
shall	O
make	O
use	O
of	O
the	O
derivative	B
of	O
the	O
logistic	O
sig-	O
moid	O
function	O
,	O
which	O
can	O
conveniently	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
sigmoid	B
function	O
itself	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
=	O
σ	O
(	O
1	O
−	O
σ	O
)	O
.	O
dσ	O
da	O
(	O
4.88	O
)	O
section	O
3.6	O
exercise	O
4.12	O
206	O
4.	O
linear	O
models	O
for	O
classification	O
for	O
a	O
data	O
set	O
{	O
φn	O
,	O
tn	O
}	O
,	O
where	O
tn	O
∈	O
{	O
0	O
,	O
1	O
}	O
and	O
φn	O
=	O
φ	O
(	O
xn	O
)	O
,	O
with	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
the	O
likelihood	B
function	I
can	O
be	O
written	O
p	O
(	O
t|w	O
)	O
=	O
n	O
{	O
1	O
−	O
yn	O
}	O
1−tn	O
ytn	O
(	O
4.89	O
)	O
where	O
t	O
=	O
(	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
)	O
t	O
and	O
yn	O
=	O
p	O
(	O
c1|φn	O
)	O
.	O
as	O
usual	O
,	O
we	O
can	O
deﬁne	O
an	O
error	B
function	I
by	O
taking	O
the	O
negative	O
logarithm	O
of	O
the	O
likelihood	O
,	O
which	O
gives	O
the	O
cross-	O
entropy	B
error	O
function	O
in	O
the	O
form	O
e	O
(	O
w	O
)	O
=	O
−	O
ln	O
p	O
(	O
t|w	O
)	O
=	O
−	O
n	O
(	O
cid:2	O
)	O
n=1	O
n	O
(	O
cid:14	O
)	O
n=1	O
n	O
(	O
cid:2	O
)	O
exercise	O
4.13	O
section	O
3.1.1	O
exercise	O
4.14	O
{	O
tn	O
ln	O
yn	O
+	O
(	O
1	O
−	O
tn	O
)	O
ln	O
(	O
1	O
−	O
yn	O
)	O
}	O
(	O
4.90	O
)	O
where	O
yn	O
=	O
σ	O
(	O
an	O
)	O
and	O
an	O
=	O
wtφn	O
.	O
taking	O
the	O
gradient	O
of	O
the	O
error	B
function	I
with	O
respect	O
to	O
w	O
,	O
we	O
obtain	O
∇e	O
(	O
w	O
)	O
=	O
(	O
yn	O
−	O
tn	O
)	O
φn	O
(	O
4.91	O
)	O
n=1	O
where	O
we	O
have	O
made	O
use	O
of	O
(	O
4.88	O
)	O
.	O
we	O
see	O
that	O
the	O
factor	O
involving	O
the	O
derivative	B
of	O
the	O
logistic	B
sigmoid	I
has	O
cancelled	O
,	O
leading	O
to	O
a	O
simpliﬁed	O
form	O
for	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	O
.	O
in	O
particular	O
,	O
the	O
contribution	O
to	O
the	O
gradient	O
from	O
data	O
point	O
n	O
is	O
given	O
by	O
the	O
‘	O
error	B
’	O
yn	O
−	O
tn	O
between	O
the	O
target	O
value	O
and	O
the	O
prediction	O
of	O
the	O
model	O
,	O
times	O
the	O
basis	B
function	I
vector	O
φn	O
.	O
furthermore	O
,	O
comparison	O
with	O
(	O
3.13	O
)	O
shows	O
that	O
this	O
takes	O
precisely	O
the	O
same	O
form	O
as	O
the	O
gradient	O
of	O
the	O
sum-of-squares	B
error	I
function	O
for	O
the	O
linear	B
regression	I
model	O
.	O
if	O
desired	O
,	O
we	O
could	O
make	O
use	O
of	O
the	O
result	O
(	O
4.91	O
)	O
to	O
give	O
a	O
sequential	O
algorithm	O
in	O
which	O
patterns	O
are	O
presented	O
one	O
at	O
a	O
time	O
,	O
in	O
which	O
each	O
of	O
the	O
weight	O
vectors	O
is	O
updated	O
using	O
(	O
3.22	O
)	O
in	O
which	O
∇en	O
is	O
the	O
nth	O
term	O
in	O
(	O
4.91	O
)	O
.	O
it	O
is	O
worth	O
noting	O
that	O
maximum	B
likelihood	I
can	O
exhibit	O
severe	O
over-ﬁtting	B
for	O
data	O
sets	O
that	O
are	O
linearly	B
separable	I
.	O
this	O
arises	O
because	O
the	O
maximum	B
likelihood	I
so-	O
lution	O
occurs	O
when	O
the	O
hyperplane	O
corresponding	O
to	O
σ	O
=	O
0.5	O
,	O
equivalent	O
to	O
wtφ	O
=	O
0	O
,	O
separates	O
the	O
two	O
classes	O
and	O
the	O
magnitude	O
of	O
w	O
goes	O
to	O
inﬁnity	O
.	O
in	O
this	O
case	O
,	O
the	O
logistic	B
sigmoid	I
function	O
becomes	O
inﬁnitely	O
steep	O
in	O
feature	B
space	I
,	O
corresponding	O
to	O
a	O
heaviside	O
step	O
function	O
,	O
so	O
that	O
every	O
training	B
point	O
from	O
each	O
class	O
k	O
is	O
assigned	O
a	O
posterior	B
probability	I
p	O
(	O
ck|x	O
)	O
=	O
1.	O
furthermore	O
,	O
there	O
is	O
typically	O
a	O
continuum	O
of	O
such	O
solutions	O
because	O
any	O
separating	O
hyperplane	O
will	O
give	O
rise	O
to	O
the	O
same	O
pos-	O
terior	O
probabilities	O
at	O
the	O
training	B
data	O
points	O
,	O
as	O
will	O
be	O
seen	O
later	O
in	O
figure	O
10.13.	O
maximum	B
likelihood	I
provides	O
no	O
way	O
to	O
favour	O
one	O
such	O
solution	O
over	O
another	O
,	O
and	O
which	O
solution	O
is	O
found	O
in	O
practice	O
will	O
depend	O
on	O
the	O
choice	O
of	O
optimization	O
algo-	O
rithm	O
and	O
on	O
the	O
parameter	O
initialization	O
.	O
note	O
that	O
the	O
problem	O
will	O
arise	O
even	O
if	O
the	O
number	O
of	O
data	O
points	O
is	O
large	O
compared	O
with	O
the	O
number	O
of	O
parameters	O
in	O
the	O
model	O
,	O
so	O
long	O
as	O
the	O
training	B
data	O
set	O
is	O
linearly	B
separable	I
.	O
the	O
singularity	O
can	O
be	O
avoided	O
by	O
inclusion	O
of	O
a	O
prior	B
and	O
ﬁnding	O
a	O
map	O
solution	O
for	O
w	O
,	O
or	O
equivalently	O
by	O
adding	O
a	O
regularization	B
term	O
to	O
the	O
error	B
function	I
.	O
4.3.	O
probabilistic	O
discriminative	O
models	O
207	O
4.3.3	O
iterative	B
reweighted	I
least	I
squares	I
in	O
the	O
case	O
of	O
the	O
linear	B
regression	I
models	O
discussed	O
in	O
chapter	O
3	O
,	O
the	O
maxi-	O
mum	O
likelihood	O
solution	O
,	O
on	O
the	O
assumption	O
of	O
a	O
gaussian	O
noise	O
model	O
,	O
leads	O
to	O
a	O
closed-form	O
solution	O
.	O
this	O
was	O
a	O
consequence	O
of	O
the	O
quadratic	O
dependence	O
of	O
the	O
log	O
likelihood	O
function	O
on	O
the	O
parameter	O
vector	O
w.	O
for	O
logistic	O
regression	B
,	O
there	O
is	O
no	O
longer	O
a	O
closed-form	O
solution	O
,	O
due	O
to	O
the	O
nonlinearity	O
of	O
the	O
logistic	B
sigmoid	I
function	O
.	O
however	O
,	O
the	O
departure	O
from	O
a	O
quadratic	O
form	O
is	O
not	O
substantial	O
.	O
to	O
be	O
precise	O
,	O
the	O
error	B
function	I
is	O
concave	O
,	O
as	O
we	O
shall	O
see	O
shortly	O
,	O
and	O
hence	O
has	O
a	O
unique	O
minimum	O
.	O
furthermore	O
,	O
the	O
error	B
function	I
can	O
be	O
minimized	O
by	O
an	O
efﬁcient	O
iterative	O
technique	O
based	O
on	O
the	O
newton-raphson	O
iterative	O
optimization	O
scheme	O
,	O
which	O
uses	O
a	O
local	B
quadratic	O
approximation	O
to	O
the	O
log	O
likelihood	O
function	O
.	O
the	O
newton-raphson	O
update	O
,	O
for	O
minimizing	O
a	O
function	O
e	O
(	O
w	O
)	O
,	O
takes	O
the	O
form	O
(	O
fletcher	O
,	O
1987	O
;	O
bishop	O
and	O
nabney	O
,	O
2008	O
)	O
(	O
4.92	O
)	O
where	O
h	O
is	O
the	O
hessian	O
matrix	O
whose	O
elements	O
comprise	O
the	O
second	O
derivatives	O
of	O
e	O
(	O
w	O
)	O
with	O
respect	O
to	O
the	O
components	O
of	O
w.	O
let	O
us	O
ﬁrst	O
of	O
all	O
apply	O
the	O
newton-raphson	O
method	O
to	O
the	O
linear	B
regression	I
model	O
(	O
3.3	O
)	O
with	O
the	O
sum-of-squares	B
error	I
function	O
(	O
3.12	O
)	O
.	O
the	O
gradient	O
and	O
hessian	O
of	O
this	O
error	B
function	I
are	O
given	O
by	O
w	O
(	O
new	O
)	O
=	O
w	O
(	O
old	O
)	O
−	O
h−1∇e	O
(	O
w	O
)	O
.	O
∇e	O
(	O
w	O
)	O
=	O
h	O
=	O
∇∇e	O
(	O
w	O
)	O
=	O
(	O
wtφn	O
−	O
tn	O
)	O
φn	O
=	O
φtφw	O
−	O
φtt	O
φnφt	O
n	O
=	O
φtφ	O
(	O
4.93	O
)	O
(	O
4.94	O
)	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
n=1	O
section	O
3.1.1	O
where	O
φ	O
is	O
the	O
n	O
×	O
m	O
design	B
matrix	I
,	O
whose	O
nth	O
row	O
is	O
given	O
by	O
φt	O
(	O
cid:27	O
)	O
raphson	O
update	O
then	O
takes	O
the	O
form	O
φtφw	O
(	O
old	O
)	O
−	O
φtt	O
w	O
(	O
new	O
)	O
=	O
w	O
(	O
old	O
)	O
−	O
(	O
φtφ	O
)	O
−1	O
(	O
cid:26	O
)	O
n.	O
the	O
newton-	O
=	O
(	O
φtφ	O
)	O
−1φtt	O
which	O
we	O
recognize	O
as	O
the	O
standard	O
least-squares	O
solution	O
.	O
note	O
that	O
the	O
error	B
func-	O
tion	O
in	O
this	O
case	O
is	O
quadratic	O
and	O
hence	O
the	O
newton-raphson	O
formula	O
gives	O
the	O
exact	O
solution	O
in	O
one	O
step	O
.	O
now	O
let	O
us	O
apply	O
the	O
newton-raphson	O
update	O
to	O
the	O
cross-entropy	B
error	I
function	I
(	O
4.90	O
)	O
for	O
the	O
logistic	B
regression	I
model	O
.	O
from	O
(	O
4.91	O
)	O
we	O
see	O
that	O
the	O
gradient	O
and	O
hessian	O
of	O
this	O
error	B
function	I
are	O
given	O
by	O
n	O
(	O
cid:2	O
)	O
∇e	O
(	O
w	O
)	O
=	O
(	O
yn	O
−	O
tn	O
)	O
φn	O
=	O
φt	O
(	O
y	O
−	O
t	O
)	O
n=1	O
h	O
=	O
∇∇e	O
(	O
w	O
)	O
=	O
yn	O
(	O
1	O
−	O
yn	O
)	O
φnφt	O
n	O
=	O
φtrφ	O
n	O
(	O
cid:2	O
)	O
n=1	O
(	O
4.95	O
)	O
(	O
4.96	O
)	O
(	O
4.97	O
)	O
208	O
4.	O
linear	O
models	O
for	O
classification	O
rnn	O
=	O
yn	O
(	O
1	O
−	O
yn	O
)	O
.	O
where	O
we	O
have	O
made	O
use	O
of	O
(	O
4.88	O
)	O
.	O
also	O
,	O
we	O
have	O
introduced	O
the	O
n	O
×	O
n	O
diagonal	B
matrix	O
r	O
with	O
elements	O
(	O
4.98	O
)	O
we	O
see	O
that	O
the	O
hessian	O
is	O
no	O
longer	O
constant	O
but	O
depends	O
on	O
w	O
through	O
the	O
weight-	O
ing	O
matrix	O
r	O
,	O
corresponding	O
to	O
the	O
fact	O
that	O
the	O
error	B
function	I
is	O
no	O
longer	O
quadratic	O
.	O
using	O
the	O
property	O
0	O
<	O
yn	O
<	O
1	O
,	O
which	O
follows	O
from	O
the	O
form	O
of	O
the	O
logistic	B
sigmoid	I
function	O
,	O
we	O
see	O
that	O
uthu	O
>	O
0	O
for	O
an	O
arbitrary	O
vector	O
u	O
,	O
and	O
so	O
the	O
hessian	O
matrix	O
h	O
is	O
positive	B
deﬁnite	I
.	O
it	O
follows	O
that	O
the	O
error	B
function	I
is	O
a	O
concave	B
function	I
of	O
w	O
and	O
hence	O
has	O
a	O
unique	O
minimum	O
.	O
exercise	O
4.15	O
the	O
newton-raphson	O
update	O
formula	O
for	O
the	O
logistic	B
regression	I
model	O
then	O
be-	O
comes	O
w	O
(	O
new	O
)	O
=	O
w	O
(	O
old	O
)	O
−	O
(	O
φtrφ	O
)	O
−1φt	O
(	O
y	O
−	O
t	O
)	O
(	O
cid:26	O
)	O
(	O
cid:27	O
)	O
φtrφw	O
(	O
old	O
)	O
−	O
φt	O
(	O
y	O
−	O
t	O
)	O
=	O
(	O
φtrφ	O
)	O
−1	O
=	O
(	O
φtrφ	O
)	O
−1φtrz	O
where	O
z	O
is	O
an	O
n-dimensional	O
vector	O
with	O
elements	O
z	O
=	O
φw	O
(	O
old	O
)	O
−	O
r−1	O
(	O
y	O
−	O
t	O
)	O
.	O
(	O
4.99	O
)	O
(	O
4.100	O
)	O
we	O
see	O
that	O
the	O
update	O
formula	O
(	O
4.99	O
)	O
takes	O
the	O
form	O
of	O
a	O
set	O
of	O
normal	B
equations	I
for	O
a	O
weighted	O
least-squares	O
problem	O
.	O
because	O
the	O
weighing	O
matrix	O
r	O
is	O
not	O
constant	O
but	O
depends	O
on	O
the	O
parameter	O
vector	O
w	O
,	O
we	O
must	O
apply	O
the	O
normal	B
equations	I
iteratively	O
,	O
each	O
time	O
using	O
the	O
new	O
weight	B
vector	I
w	O
to	O
compute	O
a	O
revised	O
weighing	O
matrix	O
r.	O
for	O
this	O
reason	O
,	O
the	O
algorithm	O
is	O
known	O
as	O
iterative	B
reweighted	I
least	I
squares	I
,	O
or	O
irls	O
(	O
rubin	O
,	O
1983	O
)	O
.	O
as	O
in	O
the	O
weighted	O
least-squares	O
problem	O
,	O
the	O
elements	O
of	O
the	O
diagonal	B
weighting	O
matrix	O
r	O
can	O
be	O
interpreted	O
as	O
variances	O
because	O
the	O
mean	B
and	O
variance	B
of	O
t	O
in	O
the	O
logistic	B
regression	I
model	O
are	O
given	O
by	O
e	O
[	O
t	O
]	O
=	O
σ	O
(	O
x	O
)	O
=	O
y	O
var	O
[	O
t	O
]	O
=	O
e	O
[	O
t2	O
]	O
−	O
e	O
[	O
t	O
]	O
2	O
=	O
σ	O
(	O
x	O
)	O
−	O
σ	O
(	O
x	O
)	O
2	O
=	O
y	O
(	O
1	O
−	O
y	O
)	O
(	O
4.101	O
)	O
(	O
4.102	O
)	O
where	O
we	O
have	O
used	O
the	O
property	O
t2	O
=	O
t	O
for	O
t	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
in	O
fact	O
,	O
we	O
can	O
interpret	O
irls	O
as	O
the	O
solution	O
to	O
a	O
linearized	O
problem	O
in	O
the	O
space	O
of	O
the	O
variable	O
a	O
=	O
wtφ	O
.	O
the	O
quantity	O
zn	O
,	O
which	O
corresponds	O
to	O
the	O
nth	O
element	O
of	O
z	O
,	O
can	O
then	O
be	O
given	O
a	O
simple	O
interpretation	O
as	O
an	O
effective	O
target	O
value	O
in	O
this	O
space	O
obtained	O
by	O
making	O
a	O
local	B
linear	O
approximation	O
to	O
the	O
logistic	B
sigmoid	I
function	O
around	O
the	O
current	O
operating	O
point	O
w	O
(	O
old	O
)	O
an	O
(	O
w	O
)	O
(	O
cid:7	O
)	O
an	O
(	O
w	O
(	O
old	O
)	O
)	O
+	O
dan	O
dyn	O
nw	O
(	O
old	O
)	O
−	O
(	O
yn	O
−	O
tn	O
)	O
yn	O
(	O
1	O
−	O
yn	O
)	O
w	O
(	O
old	O
)	O
(	O
tn	O
−	O
yn	O
)	O
=	O
zn	O
.	O
(	O
4.103	O
)	O
=	O
φt	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
4.3.	O
probabilistic	O
discriminative	O
models	O
209	O
section	O
4.2	O
4.3.4	O
multiclass	B
logistic	O
regression	B
in	O
our	O
discussion	O
of	O
generative	O
models	O
for	O
multiclass	O
classiﬁcation	B
,	O
we	O
have	O
seen	O
that	O
for	O
a	O
large	O
class	O
of	O
distributions	O
,	O
the	O
posterior	O
probabilities	O
are	O
given	O
by	O
a	O
softmax	O
transformation	O
of	O
linear	O
functions	O
of	O
the	O
feature	O
variables	O
,	O
so	O
that	O
p	O
(	O
ck|φ	O
)	O
=	O
yk	O
(	O
φ	O
)	O
=	O
(	O
cid:5	O
)	O
exp	O
(	O
ak	O
)	O
j	O
exp	O
(	O
aj	O
)	O
(	O
4.104	O
)	O
(	O
4.105	O
)	O
where	O
the	O
‘	O
activations	O
’	O
ak	O
are	O
given	O
by	O
ak	O
=	O
wt	O
k	O
φ.	O
there	O
we	O
used	O
maximum	B
likelihood	I
to	O
determine	O
separately	O
the	O
class-conditional	O
densities	O
and	O
the	O
class	O
priors	O
and	O
then	O
found	O
the	O
corresponding	O
posterior	O
probabilities	O
using	O
bayes	O
’	O
theorem	O
,	O
thereby	O
implicitly	O
determining	O
the	O
parameters	O
{	O
wk	O
}	O
.	O
here	O
we	O
consider	O
the	O
use	O
of	O
maximum	B
likelihood	I
to	O
determine	O
the	O
parameters	O
{	O
wk	O
}	O
of	O
this	O
model	O
directly	O
.	O
to	O
do	O
this	O
,	O
we	O
will	O
require	O
the	O
derivatives	O
of	O
yk	O
with	O
respect	O
to	O
all	O
of	O
the	O
activations	O
aj	O
.	O
these	O
are	O
given	O
by	O
exercise	O
4.17	O
=	O
yk	O
(	O
ikj	O
−	O
yj	O
)	O
∂yk	O
∂aj	O
(	O
4.106	O
)	O
where	O
ikj	O
are	O
the	O
elements	O
of	O
the	O
identity	O
matrix	O
.	O
next	O
we	O
write	O
down	O
the	O
likelihood	B
function	I
.	O
this	O
is	O
most	O
easily	O
done	O
using	O
the	O
1-of-k	O
coding	O
scheme	O
in	O
which	O
the	O
target	B
vector	I
tn	O
for	O
a	O
feature	O
vector	O
φn	O
belonging	O
to	O
class	O
ck	O
is	O
a	O
binary	O
vector	O
with	O
all	O
elements	O
zero	O
except	O
for	O
element	O
k	O
,	O
which	O
equals	O
one	O
.	O
the	O
likelihood	B
function	I
is	O
then	O
given	O
by	O
n	O
(	O
cid:14	O
)	O
k	O
(	O
cid:14	O
)	O
n	O
(	O
cid:14	O
)	O
k	O
(	O
cid:14	O
)	O
p	O
(	O
t|w1	O
,	O
.	O
.	O
.	O
,	O
wk	O
)	O
=	O
p	O
(	O
ck|φn	O
)	O
tnk	O
=	O
ytnk	O
nk	O
(	O
4.107	O
)	O
n=1	O
k=1	O
n=1	O
k=1	O
where	O
ynk	O
=	O
yk	O
(	O
φn	O
)	O
,	O
and	O
t	O
is	O
an	O
n	O
×	O
k	O
matrix	O
of	O
target	O
variables	O
with	O
elements	O
tnk	O
.	O
taking	O
the	O
negative	O
logarithm	O
then	O
gives	O
e	O
(	O
w1	O
,	O
.	O
.	O
.	O
,	O
wk	O
)	O
=	O
−	O
ln	O
p	O
(	O
t|w1	O
,	O
.	O
.	O
.	O
,	O
wk	O
)	O
=	O
−	O
n	O
(	O
cid:2	O
)	O
tnk	O
ln	O
ynk	O
(	O
4.108	O
)	O
k	O
(	O
cid:2	O
)	O
which	O
is	O
known	O
as	O
the	O
cross-entropy	B
error	I
function	I
for	O
the	O
multiclass	B
classiﬁcation	O
problem	O
.	O
n=1	O
k=1	O
we	O
now	O
take	O
the	O
gradient	O
of	O
the	O
error	B
function	I
with	O
respect	O
to	O
one	O
of	O
the	O
param-	O
eter	O
vectors	O
wj	O
.	O
making	O
use	O
of	O
the	O
result	O
(	O
4.106	O
)	O
for	O
the	O
derivatives	O
of	O
the	O
softmax	B
function	I
,	O
we	O
obtain	O
∇wj	O
e	O
(	O
w1	O
,	O
.	O
.	O
.	O
,	O
wk	O
)	O
=	O
(	O
ynj	O
−	O
tnj	O
)	O
φn	O
(	O
4.109	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
exercise	O
4.18	O
210	O
4.	O
linear	O
models	O
for	O
classification	O
(	O
cid:5	O
)	O
k	O
tnk	O
=	O
1.	O
once	O
again	O
,	O
we	O
see	O
the	O
same	O
form	O
arising	O
where	O
we	O
have	O
made	O
use	O
of	O
for	O
the	O
gradient	O
as	O
was	O
found	O
for	O
the	O
sum-of-squares	B
error	I
function	O
with	O
the	O
linear	O
model	O
and	O
the	O
cross-entropy	O
error	O
for	O
the	O
logistic	B
regression	I
model	O
,	O
namely	O
the	O
prod-	O
uct	O
of	O
the	O
error	B
(	O
ynj	O
−	O
tnj	O
)	O
times	O
the	O
basis	B
function	I
φn	O
.	O
again	O
,	O
we	O
could	O
use	O
this	O
to	O
formulate	O
a	O
sequential	O
algorithm	O
in	O
which	O
patterns	O
are	O
presented	O
one	O
at	O
a	O
time	O
,	O
in	O
which	O
each	O
of	O
the	O
weight	O
vectors	O
is	O
updated	O
using	O
(	O
3.22	O
)	O
.	O
we	O
have	O
seen	O
that	O
the	O
derivative	B
of	O
the	O
log	O
likelihood	O
function	O
for	O
a	O
linear	O
regres-	O
sion	B
model	O
with	O
respect	O
to	O
the	O
parameter	O
vector	O
w	O
for	O
a	O
data	O
point	O
n	O
took	O
the	O
form	O
of	O
the	O
‘	O
error	B
’	O
yn	O
−	O
tn	O
times	O
the	O
feature	O
vector	O
φn	O
.	O
similarly	O
,	O
for	O
the	O
combination	O
of	O
logistic	B
sigmoid	I
activation	O
function	O
and	O
cross-entropy	B
error	I
function	I
(	O
4.90	O
)	O
,	O
and	O
for	O
the	O
softmax	O
activation	O
function	O
with	O
the	O
multiclass	B
cross-entropy	O
error	B
function	I
(	O
4.108	O
)	O
,	O
we	O
again	O
obtain	O
this	O
same	O
simple	O
form	O
.	O
this	O
is	O
an	O
example	O
of	O
a	O
more	O
general	O
result	O
,	O
as	O
we	O
shall	O
see	O
in	O
section	O
4.3.6.	O
to	O
ﬁnd	O
a	O
batch	O
algorithm	O
,	O
we	O
again	O
appeal	O
to	O
the	O
newton-raphson	O
update	O
to	O
obtain	O
the	O
corresponding	O
irls	O
algorithm	O
for	O
the	O
multiclass	B
problem	O
.	O
this	O
requires	O
evaluation	O
of	O
the	O
hessian	O
matrix	O
that	O
comprises	O
blocks	O
of	O
size	O
m	O
×	O
m	O
in	O
which	O
block	O
j	O
,	O
k	O
is	O
given	O
by	O
∇wj	O
e	O
(	O
w1	O
,	O
.	O
.	O
.	O
,	O
wk	O
)	O
=	O
−	O
n	O
(	O
cid:2	O
)	O
∇wk	O
ynk	O
(	O
ikj	O
−	O
ynj	O
)	O
φnφt	O
n.	O
(	O
4.110	O
)	O
exercise	O
4.20	O
n=1	O
as	O
with	O
the	O
two-class	O
problem	O
,	O
the	O
hessian	O
matrix	O
for	O
the	O
multiclass	B
logistic	O
regres-	O
sion	B
model	O
is	O
positive	B
deﬁnite	I
and	O
so	O
the	O
error	B
function	I
again	O
has	O
a	O
unique	O
minimum	O
.	O
practical	O
details	O
of	O
irls	O
for	O
the	O
multiclass	B
case	O
can	O
be	O
found	O
in	O
bishop	O
and	O
nabney	O
(	O
2008	O
)	O
.	O
4.3.5	O
probit	B
regression	I
we	O
have	O
seen	O
that	O
,	O
for	O
a	O
broad	O
range	O
of	O
class-conditional	O
distributions	O
,	O
described	O
by	O
the	O
exponential	B
family	I
,	O
the	O
resulting	O
posterior	O
class	O
probabilities	O
are	O
given	O
by	O
a	O
logistic	O
(	O
or	O
softmax	O
)	O
transformation	O
acting	O
on	O
a	O
linear	O
function	O
of	O
the	O
feature	O
vari-	O
ables	O
.	O
however	O
,	O
not	O
all	O
choices	O
of	O
class-conditional	O
density	B
give	O
rise	O
to	O
such	O
a	O
simple	O
form	O
for	O
the	O
posterior	O
probabilities	O
(	O
for	O
instance	O
,	O
if	O
the	O
class-conditional	O
densities	O
are	O
modelled	O
using	O
gaussian	O
mixtures	O
)	O
.	O
this	O
suggests	O
that	O
it	O
might	O
be	O
worth	O
exploring	O
other	O
types	O
of	O
discriminative	O
probabilistic	O
model	O
.	O
for	O
the	O
purposes	O
of	O
this	O
chapter	O
,	O
however	O
,	O
we	O
shall	O
return	O
to	O
the	O
two-class	O
case	O
,	O
and	O
again	O
remain	O
within	O
the	O
frame-	O
work	O
of	O
generalized	O
linear	O
models	O
so	O
that	O
p	O
(	O
t	O
=	O
1|a	O
)	O
=	O
f	O
(	O
a	O
)	O
where	O
a	O
=	O
wtφ	O
,	O
and	O
f	O
(	O
·	O
)	O
is	O
the	O
activation	B
function	I
.	O
(	O
4.111	O
)	O
one	O
way	O
to	O
motivate	O
an	O
alternative	O
choice	O
for	O
the	O
link	B
function	I
is	O
to	O
consider	O
a	O
noisy	O
threshold	O
model	O
,	O
as	O
follows	O
.	O
for	O
each	O
input	O
φn	O
,	O
we	O
evaluate	O
an	O
=	O
wtφn	O
and	O
then	O
we	O
set	O
the	O
target	O
value	O
according	O
to	O
(	O
cid:12	O
)	O
tn	O
=	O
1	O
if	O
an	O
(	O
cid:2	O
)	O
θ	O
tn	O
=	O
0	O
otherwise	O
.	O
(	O
4.112	O
)	O
4.3.	O
probabilistic	O
discriminative	O
models	O
211	O
figure	O
4.13	O
schematic	O
example	O
of	O
a	O
probability	B
density	O
p	O
(	O
θ	O
)	O
shown	O
by	O
the	O
blue	O
curve	O
,	O
given	O
in	O
this	O
example	O
by	O
a	O
mixture	O
of	O
two	O
gaussians	O
,	O
along	O
with	O
its	O
cumulative	B
distribution	I
function	I
f	O
(	O
a	O
)	O
,	O
shown	O
by	O
the	O
red	O
curve	O
.	O
note	O
that	O
the	O
value	O
of	O
the	O
blue	O
curve	O
at	O
any	O
point	O
,	O
such	O
as	O
that	O
indicated	O
by	O
the	O
vertical	O
green	O
line	O
,	O
corresponds	O
to	O
the	O
slope	O
of	O
the	O
red	O
curve	O
at	O
the	O
same	O
point	O
.	O
conversely	O
,	O
the	O
value	O
of	O
the	O
red	O
curve	O
at	O
this	O
point	O
corresponds	O
to	O
the	O
area	O
under	O
the	O
blue	O
curve	O
indicated	O
by	O
the	O
shaded	O
green	O
region	O
.	O
in	O
the	O
stochastic	B
threshold	O
model	O
,	O
the	O
class	O
label	O
takes	O
the	O
value	O
t	O
=	O
1	O
if	O
the	O
value	O
of	O
a	O
=	O
wtφ	O
exceeds	O
a	O
threshold	O
,	O
oth-	O
erwise	O
it	O
takes	O
the	O
value	O
t	O
=	O
0.	O
this	O
is	O
equivalent	O
to	O
an	O
activation	B
function	I
given	O
by	O
the	O
cumulative	B
distribution	I
function	I
f	O
(	O
a	O
)	O
.	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
0	O
1	O
2	O
3	O
4	O
if	O
the	O
value	O
of	O
θ	O
is	O
drawn	O
from	O
a	O
probability	B
density	O
p	O
(	O
θ	O
)	O
,	O
then	O
the	O
corresponding	O
activation	B
function	I
will	O
be	O
given	O
by	O
the	O
cumulative	B
distribution	I
function	I
(	O
cid:6	O
)	O
a	O
f	O
(	O
a	O
)	O
=	O
p	O
(	O
θ	O
)	O
dθ	O
−∞	O
(	O
4.113	O
)	O
as	O
illustrated	O
in	O
figure	O
4.13.	O
as	O
a	O
speciﬁc	O
example	O
,	O
suppose	O
that	O
the	O
density	B
p	O
(	O
θ	O
)	O
is	O
given	O
by	O
a	O
zero	O
mean	B
,	O
unit	O
variance	B
gaussian	O
.	O
the	O
corresponding	O
cumulative	B
distribution	I
function	I
is	O
given	O
by	O
(	O
cid:6	O
)	O
a	O
φ	O
(	O
a	O
)	O
=	O
−∞	O
n	O
(	O
θ|0	O
,	O
1	O
)	O
dθ	O
(	O
4.114	O
)	O
which	O
is	O
known	O
as	O
the	O
probit	B
function	I
.	O
it	O
has	O
a	O
sigmoidal	O
shape	O
and	O
is	O
compared	O
with	O
the	O
logistic	B
sigmoid	I
function	O
in	O
figure	O
4.9.	O
note	O
that	O
the	O
use	O
of	O
a	O
more	O
gen-	O
eral	O
gaussian	O
distribution	O
does	O
not	O
change	O
the	O
model	O
because	O
this	O
is	O
equivalent	O
to	O
a	O
re-scaling	O
of	O
the	O
linear	O
coefﬁcients	O
w.	O
many	O
numerical	O
packages	O
provide	O
for	O
the	O
evaluation	O
of	O
a	O
closely	O
related	O
function	O
deﬁned	O
by	O
erf	O
(	O
a	O
)	O
=	O
2√	O
π	O
exp	O
(	O
−θ2/2	O
)	O
dθ	O
(	O
4.115	O
)	O
(	O
cid:6	O
)	O
a	O
0	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
exercise	O
4.21	O
and	O
known	O
as	O
the	O
erf	B
function	I
or	O
error	B
function	I
(	O
not	O
to	O
be	O
confused	O
with	O
the	O
error	B
function	I
of	O
a	O
machine	O
learning	O
model	O
)	O
.	O
it	O
is	O
related	O
to	O
the	O
probit	B
function	I
by	O
φ	O
(	O
a	O
)	O
=	O
1	O
2	O
1√	O
2	O
1	O
+	O
erf	O
(	O
a	O
)	O
.	O
(	O
4.116	O
)	O
the	O
generalized	B
linear	I
model	I
based	O
on	O
a	O
probit	O
activation	O
function	O
is	O
known	O
as	O
probit	B
regression	I
.	O
we	O
can	O
determine	O
the	O
parameters	O
of	O
this	O
model	O
using	O
maximum	B
likelihood	I
,	O
by	O
a	O
straightforward	O
extension	O
of	O
the	O
ideas	O
discussed	O
earlier	O
.	O
in	O
practice	O
,	O
the	O
results	O
found	O
using	O
probit	B
regression	I
tend	O
to	O
be	O
similar	O
to	O
those	O
of	O
logistic	B
regression	I
.	O
we	O
shall	O
,	O
212	O
4.	O
linear	O
models	O
for	O
classification	O
however	O
,	O
ﬁnd	O
another	O
use	O
for	O
the	O
probit	O
model	O
when	O
we	O
discuss	O
bayesian	O
treatments	O
of	O
logistic	B
regression	I
in	O
section	O
4.5.	O
one	O
issue	O
that	O
can	O
occur	O
in	O
practical	O
applications	O
is	O
that	O
of	O
outliers	B
,	O
which	O
can	O
arise	O
for	O
instance	O
through	O
errors	O
in	O
measuring	O
the	O
input	O
vector	O
x	O
or	O
through	O
misla-	O
belling	O
of	O
the	O
target	O
value	O
t.	O
because	O
such	O
points	O
can	O
lie	O
a	O
long	O
way	O
to	O
the	O
wrong	O
side	O
of	O
the	O
ideal	O
decision	B
boundary	I
,	O
they	O
can	O
seriously	O
distort	O
the	O
classiﬁer	O
.	O
note	O
that	O
the	O
logistic	O
and	O
probit	B
regression	I
models	O
behave	O
differently	O
in	O
this	O
respect	O
because	O
the	O
tails	O
of	O
the	O
logistic	B
sigmoid	I
decay	O
asymptotically	O
like	O
exp	O
(	O
−x	O
)	O
for	O
x	O
→	O
∞	O
,	O
whereas	O
for	O
the	O
probit	O
activation	O
function	O
they	O
decay	O
like	O
exp	O
(	O
−x2	O
)	O
,	O
and	O
so	O
the	O
probit	O
model	O
can	O
be	O
signiﬁcantly	O
more	O
sensitive	O
to	O
outliers	B
.	O
however	O
,	O
both	O
the	O
logistic	O
and	O
the	O
probit	O
models	O
assume	O
the	O
data	O
is	O
correctly	O
labelled	O
.	O
the	O
effect	O
of	O
mislabelling	O
is	O
easily	O
incorporated	O
into	O
a	O
probabilistic	O
model	O
by	O
introducing	O
a	O
probability	B
	O
that	O
the	O
target	O
value	O
t	O
has	O
been	O
ﬂipped	O
to	O
the	O
wrong	O
value	O
(	O
opper	O
and	O
winther	O
,	O
2000a	O
)	O
,	O
leading	O
to	O
a	O
target	O
value	O
distribution	O
for	O
data	O
point	O
x	O
of	O
the	O
form	O
p	O
(	O
t|x	O
)	O
=	O
(	O
1	O
−	O
	O
)	O
σ	O
(	O
x	O
)	O
+	O
	O
(	O
1	O
−	O
σ	O
(	O
x	O
)	O
)	O
=	O
	O
+	O
(	O
1	O
−	O
2	O
)	O
σ	O
(	O
x	O
)	O
(	O
4.117	O
)	O
where	O
σ	O
(	O
x	O
)	O
is	O
the	O
activation	B
function	I
with	O
input	O
vector	O
x.	O
here	O
	O
may	O
be	O
set	O
in	O
advance	O
,	O
or	O
it	O
may	O
be	O
treated	O
as	O
a	O
hyperparameter	B
whose	O
value	O
is	O
inferred	O
from	O
the	O
data	O
.	O
4.3.6	O
canonical	O
link	O
functions	O
for	O
the	O
linear	B
regression	I
model	O
with	O
a	O
gaussian	O
noise	O
distribution	O
,	O
the	O
error	B
function	I
,	O
corresponding	O
to	O
the	O
negative	O
log	O
likelihood	O
,	O
is	O
given	O
by	O
(	O
3.12	O
)	O
.	O
if	O
we	O
take	O
the	O
derivative	B
with	O
respect	O
to	O
the	O
parameter	O
vector	O
w	O
of	O
the	O
contribution	O
to	O
the	O
error	B
function	I
from	O
a	O
data	O
point	O
n	O
,	O
this	O
takes	O
the	O
form	O
of	O
the	O
‘	O
error	B
’	O
yn	O
−	O
tn	O
times	O
the	O
feature	O
vector	O
φn	O
,	O
where	O
yn	O
=	O
wtφn	O
.	O
similarly	O
,	O
for	O
the	O
combination	O
of	O
the	O
logistic	B
sigmoid	I
activation	O
function	O
and	O
the	O
cross-entropy	B
error	I
function	I
(	O
4.90	O
)	O
,	O
and	O
for	O
the	O
softmax	O
activation	O
function	O
with	O
the	O
multiclass	B
cross-entropy	O
error	B
function	I
(	O
4.108	O
)	O
,	O
we	O
again	O
obtain	O
this	O
same	O
simple	O
form	O
.	O
we	O
now	O
show	O
that	O
this	O
is	O
a	O
general	O
result	O
of	O
assuming	O
a	O
conditional	B
distribution	O
for	O
the	O
target	O
variable	O
from	O
the	O
exponential	B
family	I
,	O
along	O
with	O
a	O
corresponding	O
choice	O
for	O
the	O
activation	B
function	I
known	O
as	O
the	O
canonical	B
link	I
function	I
.	O
we	O
again	O
make	O
use	O
of	O
the	O
restricted	O
form	O
(	O
4.84	O
)	O
of	O
exponential	B
family	I
distribu-	O
tions	O
.	O
note	O
that	O
here	O
we	O
are	O
applying	O
the	O
assumption	O
of	O
exponential	B
family	I
distribu-	O
tion	O
to	O
the	O
target	O
variable	O
t	O
,	O
in	O
contrast	O
to	O
section	O
4.2.4	O
where	O
we	O
applied	O
it	O
to	O
the	O
input	O
vector	O
x.	O
we	O
therefore	O
consider	O
conditional	B
distributions	O
of	O
the	O
target	O
variable	O
of	O
the	O
form	O
(	O
cid:19	O
)	O
(	O
cid:20	O
)	O
(	O
cid:17	O
)	O
(	O
cid:18	O
)	O
p	O
(	O
t|η	O
,	O
s	O
)	O
=	O
1	O
s	O
h	O
t	O
s	O
ηt	O
s	O
g	O
(	O
η	O
)	O
exp	O
.	O
(	O
4.118	O
)	O
using	O
the	O
same	O
line	O
of	O
argument	O
as	O
led	O
to	O
the	O
derivation	O
of	O
the	O
result	O
(	O
2.226	O
)	O
,	O
we	O
see	O
that	O
the	O
conditional	B
mean	O
of	O
t	O
,	O
which	O
we	O
denote	O
by	O
y	O
,	O
is	O
given	O
by	O
y	O
≡	O
e	O
[	O
t|η	O
]	O
=	O
−s	O
d	O
dη	O
ln	O
g	O
(	O
η	O
)	O
.	O
(	O
4.119	O
)	O
4.4.	O
the	O
laplace	O
approximation	O
213	O
thus	O
y	O
and	O
η	O
must	O
related	O
,	O
and	O
we	O
denote	O
this	O
relation	O
through	O
η	O
=	O
ψ	O
(	O
y	O
)	O
.	O
following	O
nelder	O
and	O
wedderburn	O
(	O
1972	O
)	O
,	O
we	O
deﬁne	O
a	O
generalized	B
linear	I
model	I
to	O
be	O
one	O
for	O
which	O
y	O
is	O
a	O
nonlinear	O
function	O
of	O
a	O
linear	O
combination	O
of	O
the	O
input	O
(	O
or	O
feature	O
)	O
variables	O
so	O
that	O
y	O
=	O
f	O
(	O
wtφ	O
)	O
(	O
4.120	O
)	O
where	O
f	O
(	O
·	O
)	O
is	O
known	O
as	O
the	O
activation	B
function	I
in	O
the	O
machine	O
learning	O
literature	O
,	O
and	O
−1	O
(	O
·	O
)	O
is	O
known	O
as	O
the	O
link	B
function	I
in	O
statistics	O
.	O
f	O
(	O
cid:19	O
)	O
now	O
consider	O
the	O
log	O
likelihood	O
function	O
for	O
this	O
model	O
,	O
which	O
,	O
as	O
a	O
function	O
of	O
η	O
,	O
is	O
given	O
by	O
(	O
cid:20	O
)	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
ln	O
p	O
(	O
t|η	O
,	O
s	O
)	O
=	O
ln	O
p	O
(	O
tn|η	O
,	O
s	O
)	O
=	O
n=1	O
n=1	O
ln	O
g	O
(	O
ηn	O
)	O
+	O
ηntn	O
s	O
+	O
const	O
(	O
4.121	O
)	O
where	O
we	O
are	O
assuming	O
that	O
all	O
observations	O
share	O
a	O
common	O
scale	B
parameter	I
(	O
which	O
corresponds	O
to	O
the	O
noise	O
variance	B
for	O
a	O
gaussian	O
distribution	O
for	O
instance	O
)	O
and	O
so	O
s	O
is	O
independent	B
of	O
n.	O
the	O
derivative	B
of	O
the	O
log	O
likelihood	O
with	O
respect	O
to	O
the	O
model	O
parameters	O
w	O
is	O
then	O
given	O
by	O
(	O
cid:13	O
)	O
∇w	O
ln	O
p	O
(	O
t|η	O
,	O
s	O
)	O
=	O
=	O
d	O
dηn	O
ln	O
g	O
(	O
ηn	O
)	O
+	O
tn	O
s	O
dηn	O
dyn	O
dyn	O
dan	O
∇an	O
{	O
tn	O
−	O
yn	O
}	O
ψ	O
(	O
cid:4	O
)	O
(	O
yn	O
)	O
f	O
(	O
cid:4	O
)	O
(	O
an	O
)	O
φn	O
1	O
s	O
(	O
4.122	O
)	O
(	O
cid:12	O
)	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
n=1	O
where	O
an	O
=	O
wtφn	O
,	O
and	O
we	O
have	O
used	O
yn	O
=	O
f	O
(	O
an	O
)	O
together	O
with	O
the	O
result	O
(	O
4.119	O
)	O
for	O
e	O
[	O
t|η	O
]	O
.	O
we	O
now	O
see	O
that	O
there	O
is	O
a	O
considerable	O
simpliﬁcation	O
if	O
we	O
choose	O
a	O
particular	O
form	O
for	O
the	O
link	B
function	I
f	O
−1	O
(	O
y	O
)	O
given	O
by	O
which	O
gives	O
f	O
(	O
ψ	O
(	O
y	O
)	O
)	O
=	O
y	O
and	O
hence	O
f	O
we	O
have	O
a	O
=	O
ψ	O
and	O
hence	O
f	O
function	O
reduces	O
to	O
(	O
cid:4	O
)	O
(	O
a	O
)	O
ψ	O
−1	O
(	O
y	O
)	O
=	O
ψ	O
(	O
y	O
)	O
f	O
(	O
4.123	O
)	O
−1	O
(	O
y	O
)	O
,	O
(	O
cid:4	O
)	O
(	O
y	O
)	O
=	O
1.	O
in	O
this	O
case	O
,	O
the	O
gradient	O
of	O
the	O
error	B
(	O
cid:4	O
)	O
(	O
y	O
)	O
=	O
1.	O
also	O
,	O
because	O
a	O
=	O
f	O
(	O
cid:4	O
)	O
(	O
ψ	O
)	O
ψ	O
∇	O
ln	O
e	O
(	O
w	O
)	O
=	O
1	O
s	O
{	O
yn	O
−	O
tn	O
}	O
φn	O
.	O
(	O
4.124	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
for	O
the	O
gaussian	O
s	O
=	O
β	O
−1	O
,	O
whereas	O
for	O
the	O
logistic	O
model	O
s	O
=	O
1	O
.	O
4.4.	O
the	O
laplace	O
approximation	O
in	O
section	O
4.5	O
we	O
shall	O
discuss	O
the	O
bayesian	O
treatment	O
of	O
logistic	B
regression	I
.	O
as	O
we	O
shall	O
see	O
,	O
this	O
is	O
more	O
complex	O
than	O
the	O
bayesian	O
treatment	O
of	O
linear	B
regression	I
models	O
,	O
discussed	O
in	O
sections	O
3.3	O
and	O
3.5.	O
in	O
particular	O
,	O
we	O
can	O
not	O
integrate	O
exactly	O
214	O
4.	O
linear	O
models	O
for	O
classification	O
chapter	O
10	O
chapter	O
11	O
over	O
the	O
parameter	O
vector	O
w	O
since	O
the	O
posterior	O
distribution	O
is	O
no	O
longer	O
gaussian	O
.	O
it	O
is	O
therefore	O
necessary	O
to	O
introduce	O
some	O
form	O
of	O
approximation	O
.	O
later	O
in	O
the	O
book	O
we	O
shall	O
consider	O
a	O
range	O
of	O
techniques	O
based	O
on	O
analytical	O
approximations	O
and	O
numerical	O
sampling	O
.	O
here	O
we	O
introduce	O
a	O
simple	O
,	O
but	O
widely	O
used	O
,	O
framework	O
called	O
the	O
laplace	O
ap-	O
proximation	B
,	O
that	O
aims	O
to	O
ﬁnd	O
a	O
gaussian	O
approximation	O
to	O
a	O
probability	B
density	O
deﬁned	O
over	O
a	O
set	O
of	O
continuous	O
variables	O
.	O
consider	O
ﬁrst	O
the	O
case	O
of	O
a	O
single	O
contin-	O
uous	O
variable	O
z	O
,	O
and	O
suppose	O
the	O
distribution	O
p	O
(	O
z	O
)	O
is	O
deﬁned	O
by	O
(	O
cid:28	O
)	O
p	O
(	O
z	O
)	O
=	O
1	O
z	O
f	O
(	O
z	O
)	O
(	O
4.125	O
)	O
f	O
(	O
z	O
)	O
dz	O
is	O
the	O
normalization	O
coefﬁcient	O
.	O
we	O
shall	O
suppose	O
that	O
the	O
where	O
z	O
=	O
value	O
of	O
z	O
is	O
unknown	O
.	O
in	O
the	O
laplace	O
method	O
the	O
goal	O
is	O
to	O
ﬁnd	O
a	O
gaussian	O
approx-	O
imation	O
q	O
(	O
z	O
)	O
which	O
is	O
centred	O
on	O
a	O
mode	O
of	O
the	O
distribution	O
p	O
(	O
z	O
)	O
.	O
the	O
ﬁrst	O
step	O
is	O
to	O
(	O
cid:4	O
)	O
(	O
z0	O
)	O
=	O
0	O
,	O
or	O
equivalently	O
ﬁnd	O
a	O
mode	O
of	O
p	O
(	O
z	O
)	O
,	O
in	O
other	O
words	O
a	O
point	O
z0	O
such	O
that	O
p	O
=	O
0.	O
z=z0	O
(	O
4.126	O
)	O
a	O
gaussian	O
distribution	O
has	O
the	O
property	O
that	O
its	O
logarithm	O
is	O
a	O
quadratic	O
function	O
of	O
the	O
variables	O
.	O
we	O
therefore	O
consider	O
a	O
taylor	O
expansion	O
of	O
ln	O
f	O
(	O
z	O
)	O
centred	O
on	O
the	O
mode	O
z0	O
so	O
that	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
df	O
(	O
z	O
)	O
dz	O
where	O
(	O
4.127	O
)	O
(	O
4.128	O
)	O
ln	O
f	O
(	O
z	O
)	O
(	O
cid:7	O
)	O
ln	O
f	O
(	O
z0	O
)	O
−	O
1	O
2	O
a	O
(	O
z	O
−	O
z0	O
)	O
2	O
a	O
=	O
−	O
d2	O
dz2	O
ln	O
f	O
(	O
z	O
)	O
.	O
z=z0	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
note	O
that	O
the	O
ﬁrst-order	O
term	O
in	O
the	O
taylor	O
expansion	O
does	O
not	O
appear	O
since	O
z0	O
is	O
a	O
local	B
maximum	O
of	O
the	O
distribution	O
.	O
taking	O
the	O
exponential	O
we	O
obtain	O
f	O
(	O
z	O
)	O
(	O
cid:7	O
)	O
f	O
(	O
z0	O
)	O
exp	O
(	O
z	O
−	O
z0	O
)	O
2	O
−	O
a	O
2	O
.	O
(	O
4.129	O
)	O
we	O
can	O
then	O
obtain	O
a	O
normalized	O
distribution	O
q	O
(	O
z	O
)	O
by	O
making	O
use	O
of	O
the	O
standard	O
result	O
for	O
the	O
normalization	O
of	O
a	O
gaussian	O
,	O
so	O
that	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
1/2	O
(	O
cid:12	O
)	O
q	O
(	O
z	O
)	O
=	O
a	O
2π	O
exp	O
(	O
z	O
−	O
z0	O
)	O
2	O
−	O
a	O
2	O
.	O
(	O
4.130	O
)	O
the	O
laplace	O
approximation	O
is	O
illustrated	O
in	O
figure	O
4.14.	O
note	O
that	O
the	O
gaussian	O
approximation	O
will	O
only	O
be	O
well	O
deﬁned	O
if	O
its	O
precision	O
a	O
>	O
0	O
,	O
in	O
other	O
words	O
the	O
stationary	B
point	O
z0	O
must	O
be	O
a	O
local	B
maximum	O
,	O
so	O
that	O
the	O
second	O
derivative	O
of	O
f	O
(	O
z	O
)	O
at	O
the	O
point	O
z0	O
is	O
negative	O
.	O
4.4.	O
the	O
laplace	O
approximation	O
215	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−2	O
−1	O
0	O
1	O
2	O
3	O
4	O
40	O
30	O
20	O
10	O
0	O
−2	O
−1	O
0	O
1	O
2	O
3	O
4	O
figure	O
4.14	O
illustration	O
of	O
the	O
laplace	O
approximation	O
applied	O
to	O
the	O
distribution	O
p	O
(	O
z	O
)	O
∝	O
exp	O
(	O
−z2/2	O
)	O
σ	O
(	O
20z	O
+	O
4	O
)	O
where	O
σ	O
(	O
z	O
)	O
is	O
the	O
logistic	B
sigmoid	I
function	O
deﬁned	O
by	O
σ	O
(	O
z	O
)	O
=	O
(	O
1	O
+	O
e−z	O
)	O
−1	O
.	O
the	O
left	O
plot	O
shows	O
the	O
normalized	O
distribution	O
p	O
(	O
z	O
)	O
in	O
yellow	O
,	O
together	O
with	O
the	O
laplace	O
approximation	O
centred	O
on	O
the	O
mode	O
z0	O
of	O
p	O
(	O
z	O
)	O
in	O
red	O
.	O
the	O
right	O
plot	O
shows	O
the	O
negative	O
logarithms	O
of	O
the	O
corresponding	O
curves	O
.	O
we	O
can	O
extend	O
the	O
laplace	O
method	O
to	O
approximate	O
a	O
distribution	O
p	O
(	O
z	O
)	O
=	O
f	O
(	O
z	O
)	O
/z	O
deﬁned	O
over	O
an	O
m-dimensional	O
space	O
z.	O
at	O
a	O
stationary	B
point	O
z0	O
the	O
gradient	O
∇f	O
(	O
z	O
)	O
will	O
vanish	O
.	O
expanding	O
around	O
this	O
stationary	B
point	O
we	O
have	O
ln	O
f	O
(	O
z	O
)	O
(	O
cid:7	O
)	O
ln	O
f	O
(	O
z0	O
)	O
−	O
1	O
2	O
(	O
z	O
−	O
z0	O
)	O
ta	O
(	O
z	O
−	O
z0	O
)	O
where	O
the	O
m	O
×	O
m	O
hessian	O
matrix	O
a	O
is	O
deﬁned	O
by	O
a	O
=	O
−	O
∇∇	O
ln	O
f	O
(	O
z	O
)	O
|z=z0	O
(	O
4.131	O
)	O
(	O
4.132	O
)	O
and	O
∇	O
is	O
the	O
gradient	O
operator	O
.	O
taking	O
the	O
exponential	O
of	O
both	O
sides	O
we	O
obtain	O
f	O
(	O
z	O
)	O
(	O
cid:7	O
)	O
f	O
(	O
z0	O
)	O
exp	O
(	O
z	O
−	O
z0	O
)	O
ta	O
(	O
z	O
−	O
z0	O
)	O
.	O
(	O
4.133	O
)	O
(	O
cid:13	O
)	O
(	O
cid:12	O
)	O
−1	O
2	O
(	O
cid:13	O
)	O
(	O
cid:12	O
)	O
q	O
(	O
z	O
)	O
=	O
|a|1/2	O
(	O
2π	O
)	O
m/2	O
exp	O
the	O
distribution	O
q	O
(	O
z	O
)	O
is	O
proportional	O
to	O
f	O
(	O
z	O
)	O
and	O
the	O
appropriate	O
normalization	O
coef-	O
ﬁcient	O
can	O
be	O
found	O
by	O
inspection	O
,	O
using	O
the	O
standard	O
result	O
(	O
2.43	O
)	O
for	O
a	O
normalized	O
multivariate	O
gaussian	O
,	O
giving	O
−1	O
2	O
(	O
4.134	O
)	O
where	O
|a|	O
denotes	O
the	O
determinant	O
of	O
a.	O
this	O
gaussian	O
distribution	O
will	O
be	O
well	O
deﬁned	O
provided	O
its	O
precision	B
matrix	I
,	O
given	O
by	O
a	O
,	O
is	O
positive	B
deﬁnite	I
,	O
which	O
implies	O
that	O
the	O
stationary	B
point	O
z0	O
must	O
be	O
a	O
local	B
maximum	O
,	O
not	O
a	O
minimum	O
or	O
a	O
saddle	O
point	O
.	O
(	O
z	O
−	O
z0	O
)	O
ta	O
(	O
z	O
−	O
z0	O
)	O
=	O
n	O
(	O
z|z0	O
,	O
a−1	O
)	O
in	O
order	O
to	O
apply	O
the	O
laplace	O
approximation	O
we	O
ﬁrst	O
need	O
to	O
ﬁnd	O
the	O
mode	O
z0	O
,	O
and	O
then	O
evaluate	O
the	O
hessian	O
matrix	O
at	O
that	O
mode	O
.	O
in	O
practice	O
a	O
mode	O
will	O
typi-	O
cally	O
be	O
found	O
by	O
running	O
some	O
form	O
of	O
numerical	O
optimization	O
algorithm	O
(	O
bishop	O
216	O
4.	O
linear	O
models	O
for	O
classification	O
and	O
nabney	O
,	O
2008	O
)	O
.	O
many	O
of	O
the	O
distributions	O
encountered	O
in	O
practice	O
will	O
be	O
mul-	O
timodal	O
and	O
so	O
there	O
will	O
be	O
different	O
laplace	O
approximations	O
according	O
to	O
which	O
mode	O
is	O
being	O
considered	O
.	O
note	O
that	O
the	O
normalization	O
constant	O
z	O
of	O
the	O
true	O
distri-	O
bution	O
does	O
not	O
need	O
to	O
be	O
known	O
in	O
order	O
to	O
apply	O
the	O
laplace	O
method	O
.	O
as	O
a	O
result	O
of	O
the	O
central	B
limit	I
theorem	I
,	O
the	O
posterior	O
distribution	O
for	O
a	O
model	O
is	O
expected	O
to	O
become	O
increasingly	O
better	O
approximated	O
by	O
a	O
gaussian	O
as	O
the	O
number	O
of	O
observed	O
data	O
points	O
is	O
increased	O
,	O
and	O
so	O
we	O
would	O
expect	O
the	O
laplace	O
approximation	O
to	O
be	O
most	O
useful	O
in	O
situations	O
where	O
the	O
number	O
of	O
data	O
points	O
is	O
relatively	O
large	O
.	O
one	O
major	O
weakness	O
of	O
the	O
laplace	O
approximation	O
is	O
that	O
,	O
since	O
it	O
is	O
based	O
on	O
a	O
gaussian	O
distribution	O
,	O
it	O
is	O
only	O
directly	O
applicable	O
to	O
real	O
variables	O
.	O
in	O
other	O
cases	O
it	O
may	O
be	O
possible	O
to	O
apply	O
the	O
laplace	O
approximation	O
to	O
a	O
transformation	O
of	O
the	O
variable	O
.	O
for	O
instance	O
if	O
0	O
(	O
cid:1	O
)	O
τ	O
<	O
∞	O
then	O
we	O
can	O
consider	O
a	O
laplace	O
approximation	O
of	O
ln	O
τ	O
.	O
the	O
most	O
serious	O
limitation	O
of	O
the	O
laplace	O
framework	O
,	O
however	O
,	O
is	O
that	O
it	O
is	O
based	O
purely	O
on	O
the	O
aspects	O
of	O
the	O
true	O
distribution	O
at	O
a	O
speciﬁc	O
value	O
of	O
the	O
variable	O
,	O
and	O
so	O
can	O
fail	O
to	O
capture	O
important	O
global	O
properties	O
.	O
in	O
chapter	O
10	O
we	O
shall	O
consider	O
alternative	O
approaches	O
which	O
adopt	O
a	O
more	O
global	O
perspective	O
.	O
4.4.1	O
model	B
comparison	I
and	O
bic	O
as	O
well	O
as	O
approximating	O
the	O
distribution	O
p	O
(	O
z	O
)	O
we	O
can	O
also	O
obtain	O
an	O
approxi-	O
mation	B
to	O
the	O
normalization	O
constant	O
z.	O
using	O
the	O
approximation	O
(	O
4.133	O
)	O
we	O
have	O
(	O
cid:6	O
)	O
z	O
=	O
f	O
(	O
z	O
)	O
dz	O
(	O
cid:6	O
)	O
(	O
cid:12	O
)	O
(	O
cid:7	O
)	O
f	O
(	O
z0	O
)	O
=	O
f	O
(	O
z0	O
)	O
exp	O
(	O
2π	O
)	O
m/2	O
|a|1/2	O
(	O
cid:13	O
)	O
(	O
z	O
−	O
z0	O
)	O
ta	O
(	O
z	O
−	O
z0	O
)	O
−1	O
2	O
dz	O
(	O
4.135	O
)	O
where	O
we	O
have	O
noted	O
that	O
the	O
integrand	O
is	O
gaussian	O
and	O
made	O
use	O
of	O
the	O
standard	O
result	O
(	O
2.43	O
)	O
for	O
a	O
normalized	O
gaussian	O
distribution	O
.	O
we	O
can	O
use	O
the	O
result	O
(	O
4.135	O
)	O
to	O
obtain	O
an	O
approximation	O
to	O
the	O
model	B
evidence	I
which	O
,	O
as	O
discussed	O
in	O
section	O
3.4	O
,	O
plays	O
a	O
central	O
role	O
in	O
bayesian	O
model	B
comparison	I
.	O
consider	O
a	O
data	O
set	O
d	O
and	O
a	O
set	O
of	O
models	O
{	O
mi	O
}	O
having	O
parameters	O
{	O
θi	O
}	O
.	O
for	O
each	O
model	O
we	O
deﬁne	O
a	O
likelihood	B
function	I
p	O
(	O
d|θi	O
,	O
mi	O
)	O
.	O
if	O
we	O
introduce	O
a	O
prior	B
p	O
(	O
θi|mi	O
)	O
over	O
the	O
parameters	O
,	O
then	O
we	O
are	O
interested	O
in	O
computing	O
the	O
model	O
evi-	O
dence	O
p	O
(	O
d|mi	O
)	O
for	O
the	O
various	O
models	O
.	O
from	O
now	O
on	O
we	O
omit	O
the	O
conditioning	O
on	O
mi	O
to	O
keep	O
the	O
notation	O
uncluttered	O
.	O
from	O
bayes	O
’	O
theorem	O
the	O
model	B
evidence	I
is	O
given	O
by	O
(	O
cid:6	O
)	O
p	O
(	O
d	O
)	O
=	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ	O
)	O
dθ	O
.	O
(	O
4.136	O
)	O
identifying	O
f	O
(	O
θ	O
)	O
=	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ	O
)	O
and	O
z	O
=	O
p	O
(	O
d	O
)	O
,	O
and	O
applying	O
the	O
result	O
(	O
4.135	O
)	O
,	O
we	O
obtain	O
ln	O
p	O
(	O
d	O
)	O
(	O
cid:7	O
)	O
ln	O
p	O
(	O
d|θmap	O
)	O
+	O
ln	O
p	O
(	O
θmap	O
)	O
+	O
m	O
2	O
ln	O
(	O
2π	O
)	O
−	O
1	O
2	O
ln|a|	O
(	O
4.137	O
)	O
)	O
*	O
(	O
+	O
occam	O
factor	O
exercise	O
4.22	O
exercise	O
4.23	O
section	O
3.5.3	O
4.5.	O
bayesian	O
logistic	B
regression	I
217	O
where	O
θmap	O
is	O
the	O
value	O
of	O
θ	O
at	O
the	O
mode	O
of	O
the	O
posterior	O
distribution	O
,	O
and	O
a	O
is	O
the	O
hessian	O
matrix	O
of	O
second	O
derivatives	O
of	O
the	O
negative	O
log	O
posterior	O
a	O
=	O
−∇∇	O
ln	O
p	O
(	O
d|θmap	O
)	O
p	O
(	O
θmap	O
)	O
=	O
−∇∇	O
ln	O
p	O
(	O
θmap|d	O
)	O
.	O
(	O
4.138	O
)	O
the	O
ﬁrst	O
term	O
on	O
the	O
right	O
hand	O
side	O
of	O
(	O
4.137	O
)	O
represents	O
the	O
log	O
likelihood	O
evalu-	O
ated	O
using	O
the	O
optimized	O
parameters	O
,	O
while	O
the	O
remaining	O
three	O
terms	O
comprise	O
the	O
‘	O
occam	O
factor	O
’	O
which	O
penalizes	O
model	O
complexity	O
.	O
if	O
we	O
assume	O
that	O
the	O
gaussian	O
prior	B
distribution	O
over	O
parameters	O
is	O
broad	O
,	O
and	O
that	O
the	O
hessian	O
has	O
full	O
rank	O
,	O
then	O
we	O
can	O
approximate	O
(	O
4.137	O
)	O
very	O
roughly	O
using	O
ln	O
p	O
(	O
d	O
)	O
(	O
cid:7	O
)	O
ln	O
p	O
(	O
d|θmap	O
)	O
−	O
1	O
2	O
m	O
ln	O
n	O
(	O
4.139	O
)	O
where	O
n	O
is	O
the	O
number	O
of	O
data	O
points	O
,	O
m	O
is	O
the	O
number	O
of	O
parameters	O
in	O
θ	O
and	O
we	O
have	O
omitted	O
additive	O
constants	O
.	O
this	O
is	O
known	O
as	O
the	O
bayesian	O
information	B
criterion	I
(	O
bic	O
)	O
or	O
the	O
schwarz	O
criterion	O
(	O
schwarz	O
,	O
1978	O
)	O
.	O
note	O
that	O
,	O
compared	O
to	O
aic	O
given	O
by	O
(	O
1.73	O
)	O
,	O
this	O
penalizes	O
model	O
complexity	O
more	O
heavily	O
.	O
complexity	O
measures	O
such	O
as	O
aic	O
and	O
bic	O
have	O
the	O
virtue	O
of	O
being	O
easy	O
to	O
evaluate	O
,	O
but	O
can	O
also	O
give	O
misleading	O
results	O
.	O
in	O
particular	O
,	O
the	O
assumption	O
that	O
the	O
hessian	O
matrix	O
has	O
full	O
rank	O
is	O
often	O
not	O
valid	O
since	O
many	O
of	O
the	O
parameters	O
are	O
not	O
‘	O
well-determined	O
’	O
.	O
we	O
can	O
use	O
the	O
result	O
(	O
4.137	O
)	O
to	O
obtain	O
a	O
more	O
accurate	O
estimate	O
of	O
the	O
model	B
evidence	I
starting	O
from	O
the	O
laplace	O
approximation	O
,	O
as	O
we	O
illustrate	O
in	O
the	O
context	O
of	O
neural	O
networks	O
in	O
section	O
5.7	O
.	O
4.5.	O
bayesian	O
logistic	B
regression	I
we	O
now	O
turn	O
to	O
a	O
bayesian	O
treatment	O
of	O
logistic	B
regression	I
.	O
exact	O
bayesian	O
infer-	O
ence	O
for	O
logistic	O
regression	B
is	O
intractable	O
.	O
in	O
particular	O
,	O
evaluation	O
of	O
the	O
posterior	O
distribution	O
would	O
require	O
normalization	O
of	O
the	O
product	O
of	O
a	O
prior	B
distribution	O
and	O
a	O
likelihood	B
function	I
that	O
itself	O
comprises	O
a	O
product	O
of	O
logistic	B
sigmoid	I
functions	O
,	O
one	O
for	O
every	O
data	O
point	O
.	O
evaluation	O
of	O
the	O
predictive	B
distribution	I
is	O
similarly	O
intractable	O
.	O
here	O
we	O
consider	O
the	O
application	O
of	O
the	O
laplace	O
approximation	O
to	O
the	O
problem	O
of	O
bayesian	O
logistic	B
regression	I
(	O
spiegelhalter	O
and	O
lauritzen	O
,	O
1990	O
;	O
mackay	O
,	O
1992b	O
)	O
.	O
4.5.1	O
laplace	O
approximation	O
recall	O
from	O
section	O
4.4	O
that	O
the	O
laplace	O
approximation	O
is	O
obtained	O
by	O
ﬁnding	O
the	O
mode	O
of	O
the	O
posterior	O
distribution	O
and	O
then	O
ﬁtting	O
a	O
gaussian	O
centred	O
at	O
that	O
mode	O
.	O
this	O
requires	O
evaluation	O
of	O
the	O
second	O
derivatives	O
of	O
the	O
log	O
posterior	O
,	O
which	O
is	O
equivalent	O
to	O
ﬁnding	O
the	O
hessian	O
matrix	O
.	O
because	O
we	O
seek	O
a	O
gaussian	O
representation	O
for	O
the	O
posterior	O
distribution	O
,	O
it	O
is	O
natural	O
to	O
begin	O
with	O
a	O
gaussian	O
prior	B
,	O
which	O
we	O
write	O
in	O
the	O
general	O
form	O
p	O
(	O
w	O
)	O
=	O
n	O
(	O
w|m0	O
,	O
s0	O
)	O
(	O
4.140	O
)	O
218	O
4.	O
linear	O
models	O
for	O
classification	O
where	O
m0	O
and	O
s0	O
are	O
ﬁxed	O
hyperparameters	O
.	O
the	O
posterior	O
distribution	O
over	O
w	O
is	O
given	O
by	O
(	O
4.141	O
)	O
where	O
t	O
=	O
(	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
)	O
t.	O
taking	O
the	O
log	O
of	O
both	O
sides	O
,	O
and	O
substituting	O
for	O
the	O
prior	B
distribution	O
using	O
(	O
4.140	O
)	O
,	O
and	O
for	O
the	O
likelihood	B
function	I
using	O
(	O
4.89	O
)	O
,	O
we	O
obtain	O
p	O
(	O
w|t	O
)	O
∝	O
p	O
(	O
w	O
)	O
p	O
(	O
t|w	O
)	O
(	O
w	O
−	O
m0	O
)	O
ts	O
0	O
(	O
w	O
−	O
m0	O
)	O
−1	O
n	O
(	O
cid:2	O
)	O
ln	O
p	O
(	O
w|t	O
)	O
=	O
−1	O
2	O
+	O
n=1	O
{	O
tn	O
ln	O
yn	O
+	O
(	O
1	O
−	O
tn	O
)	O
ln	O
(	O
1	O
−	O
yn	O
)	O
}	O
+	O
const	O
(	O
4.142	O
)	O
where	O
yn	O
=	O
σ	O
(	O
wtφn	O
)	O
.	O
to	O
obtain	O
a	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
dis-	O
tribution	O
,	O
we	O
ﬁrst	O
maximize	O
the	O
posterior	O
distribution	O
to	O
give	O
the	O
map	O
(	O
maximum	B
posterior	I
)	O
solution	O
wmap	O
,	O
which	O
deﬁnes	O
the	O
mean	B
of	O
the	O
gaussian	O
.	O
the	O
covariance	B
is	O
then	O
given	O
by	O
the	O
inverse	B
of	O
the	O
matrix	O
of	O
second	O
derivatives	O
of	O
the	O
negative	O
log	O
likelihood	O
,	O
which	O
takes	O
the	O
form	O
sn	O
=	O
−∇∇	O
ln	O
p	O
(	O
w|t	O
)	O
=	O
s	O
−1	O
0	O
+	O
yn	O
(	O
1	O
−	O
yn	O
)	O
φnφt	O
n.	O
(	O
4.143	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
the	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
distribution	O
therefore	O
takes	O
the	O
form	O
q	O
(	O
w	O
)	O
=	O
n	O
(	O
w|wmap	O
,	O
sn	O
)	O
.	O
(	O
4.144	O
)	O
having	O
obtained	O
a	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
distribution	O
,	O
there	O
remains	O
the	O
task	O
of	O
marginalizing	O
with	O
respect	O
to	O
this	O
distribution	O
in	O
order	O
to	O
make	O
predictions	O
.	O
4.5.2	O
predictive	B
distribution	I
the	O
predictive	B
distribution	I
for	O
class	O
c1	O
,	O
given	O
a	O
new	O
feature	O
vector	O
φ	O
(	O
x	O
)	O
,	O
is	O
obtained	O
by	O
marginalizing	O
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
p	O
(	O
w|t	O
)	O
,	O
which	O
is	O
itself	O
approximated	O
by	O
a	O
gaussian	O
distribution	O
q	O
(	O
w	O
)	O
so	O
that	O
p	O
(	O
c1|φ	O
,	O
t	O
)	O
=	O
p	O
(	O
c1|φ	O
,	O
w	O
)	O
p	O
(	O
w|t	O
)	O
dw	O
(	O
cid:7	O
)	O
(	O
4.145	O
)	O
with	O
the	O
corresponding	O
probability	B
for	O
class	O
c2	O
given	O
by	O
p	O
(	O
c2|φ	O
,	O
t	O
)	O
=	O
1−	O
p	O
(	O
c1|φ	O
,	O
t	O
)	O
.	O
to	O
evaluate	O
the	O
predictive	B
distribution	I
,	O
we	O
ﬁrst	O
note	O
that	O
the	O
function	O
σ	O
(	O
wtφ	O
)	O
de-	O
pends	O
on	O
w	O
only	O
through	O
its	O
projection	O
onto	O
φ.	O
denoting	O
a	O
=	O
wtφ	O
,	O
we	O
have	O
σ	O
(	O
wtφ	O
)	O
q	O
(	O
w	O
)	O
dw	O
(	O
cid:6	O
)	O
δ	O
(	O
a	O
−	O
wtφ	O
)	O
σ	O
(	O
a	O
)	O
da	O
where	O
δ	O
(	O
·	O
)	O
is	O
the	O
dirac	O
delta	O
function	O
.	O
from	O
this	O
we	O
obtain	O
σ	O
(	O
wtφ	O
)	O
=	O
(	O
4.146	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
σ	O
(	O
wtφ	O
)	O
q	O
(	O
w	O
)	O
dw	O
=	O
σ	O
(	O
a	O
)	O
p	O
(	O
a	O
)	O
da	O
(	O
4.147	O
)	O
where	O
(	O
cid:6	O
)	O
p	O
(	O
a	O
)	O
=	O
4.5.	O
bayesian	O
logistic	B
regression	I
219	O
δ	O
(	O
a	O
−	O
wtφ	O
)	O
q	O
(	O
w	O
)	O
dw	O
.	O
(	O
4.148	O
)	O
we	O
can	O
evaluate	O
p	O
(	O
a	O
)	O
by	O
noting	O
that	O
the	O
delta	O
function	O
imposes	O
a	O
linear	O
constraint	O
on	O
w	O
and	O
so	O
forms	O
a	O
marginal	B
distribution	O
from	O
the	O
joint	O
distribution	O
q	O
(	O
w	O
)	O
by	O
inte-	O
grating	O
out	O
all	O
directions	O
orthogonal	O
to	O
φ.	O
because	O
q	O
(	O
w	O
)	O
is	O
gaussian	O
,	O
we	O
know	O
from	O
section	O
2.3.2	O
that	O
the	O
marginal	B
distribution	O
will	O
also	O
be	O
gaussian	O
.	O
we	O
can	O
evaluate	O
the	O
mean	B
and	O
covariance	B
of	O
this	O
distribution	O
by	O
taking	O
moments	O
,	O
and	O
interchanging	O
the	O
order	O
of	O
integration	O
over	O
a	O
and	O
w	O
,	O
so	O
that	O
µa	O
=	O
e	O
[	O
a	O
]	O
=	O
p	O
(	O
a	O
)	O
a	O
da	O
=	O
q	O
(	O
w	O
)	O
wtφ	O
dw	O
=	O
wt	O
mapφ	O
(	O
4.149	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:26	O
)	O
where	O
we	O
have	O
used	O
the	O
result	O
(	O
4.144	O
)	O
for	O
the	O
variational	B
posterior	O
distribution	O
q	O
(	O
w	O
)	O
.	O
similarly	O
σ2	O
a	O
=	O
var	O
[	O
a	O
]	O
=	O
(	O
cid:6	O
)	O
(	O
cid:26	O
)	O
(	O
cid:27	O
)	O
(	O
cid:27	O
)	O
da	O
a2	O
−	O
e	O
[	O
a	O
]	O
2	O
p	O
(	O
a	O
)	O
(	O
wtφ	O
)	O
2	O
−	O
(	O
mt	O
=	O
q	O
(	O
w	O
)	O
n	O
φ	O
)	O
2	O
dw	O
=	O
φtsn	O
φ	O
.	O
(	O
4.150	O
)	O
note	O
that	O
the	O
distribution	O
of	O
a	O
takes	O
the	O
same	O
form	O
as	O
the	O
predictive	B
distribution	I
(	O
3.58	O
)	O
for	O
the	O
linear	B
regression	I
model	O
,	O
with	O
the	O
noise	O
variance	B
set	O
to	O
zero	O
.	O
thus	O
our	O
variational	B
approximation	O
to	O
the	O
predictive	B
distribution	I
becomes	O
σ	O
(	O
a	O
)	O
n	O
(	O
a|µa	O
,	O
σ2	O
σ	O
(	O
a	O
)	O
p	O
(	O
a	O
)	O
da	O
=	O
p	O
(	O
c1|t	O
)	O
=	O
(	O
4.151	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
a	O
)	O
da	O
.	O
this	O
result	O
can	O
also	O
be	O
derived	O
directly	O
by	O
making	O
use	O
of	O
the	O
results	O
for	O
the	O
marginal	B
of	O
a	O
gaussian	O
distribution	O
given	O
in	O
section	O
2.3.2.	O
the	O
integral	O
over	O
a	O
represents	O
the	O
convolution	O
of	O
a	O
gaussian	O
with	O
a	O
logistic	O
sig-	O
moid	O
,	O
and	O
can	O
not	O
be	O
evaluated	O
analytically	O
.	O
we	O
can	O
,	O
however	O
,	O
obtain	O
a	O
good	O
approx-	O
imation	O
(	O
spiegelhalter	O
and	O
lauritzen	O
,	O
1990	O
;	O
mackay	O
,	O
1992b	O
;	O
barber	O
and	O
bishop	O
,	O
1998a	O
)	O
by	O
making	O
use	O
of	O
the	O
close	O
similarity	O
between	O
the	O
logistic	B
sigmoid	I
function	O
σ	O
(	O
a	O
)	O
deﬁned	O
by	O
(	O
4.59	O
)	O
and	O
the	O
probit	B
function	I
φ	O
(	O
a	O
)	O
deﬁned	O
by	O
(	O
4.114	O
)	O
.	O
in	O
order	O
to	O
obtain	O
the	O
best	O
approximation	O
to	O
the	O
logistic	O
function	O
we	O
need	O
to	O
re-scale	O
the	O
hori-	O
zontal	O
axis	O
,	O
so	O
that	O
we	O
approximate	O
σ	O
(	O
a	O
)	O
by	O
φ	O
(	O
λa	O
)	O
.	O
we	O
can	O
ﬁnd	O
a	O
suitable	O
value	O
of	O
λ	O
by	O
requiring	O
that	O
the	O
two	O
functions	O
have	O
the	O
same	O
slope	O
at	O
the	O
origin	O
,	O
which	O
gives	O
λ2	O
=	O
π/8	O
.	O
the	O
similarity	O
of	O
the	O
logistic	B
sigmoid	I
and	O
the	O
probit	B
function	I
,	O
for	O
this	O
choice	O
of	O
λ	O
,	O
is	O
illustrated	O
in	O
figure	O
4.9.	O
the	O
advantage	O
of	O
using	O
a	O
probit	B
function	I
is	O
that	O
its	O
convolution	O
with	O
a	O
gaussian	O
can	O
be	O
expressed	O
analytically	O
in	O
terms	O
of	O
another	O
probit	B
function	I
.	O
speciﬁcally	O
we	O
can	O
show	O
that	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
(	O
cid:6	O
)	O
φ	O
(	O
λa	O
)	O
n	O
(	O
a|µ	O
,	O
σ2	O
)	O
da	O
=	O
φ	O
µ	O
(	O
λ−2	O
+	O
σ2	O
)	O
1/2	O
.	O
(	O
4.152	O
)	O
exercise	O
4.24	O
exercise	O
4.25	O
exercise	O
4.26	O
220	O
4.	O
linear	O
models	O
for	O
classification	O
we	O
now	O
apply	O
the	O
approximation	O
σ	O
(	O
a	O
)	O
(	O
cid:7	O
)	O
φ	O
(	O
λa	O
)	O
to	O
the	O
probit	O
functions	O
appearing	O
on	O
both	O
sides	O
of	O
this	O
equation	O
,	O
leading	O
to	O
the	O
following	O
approximation	O
for	O
the	O
convo-	O
lution	O
of	O
a	O
logistic	B
sigmoid	I
with	O
a	O
gaussian	O
(	O
cid:6	O
)	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
σ	O
(	O
a	O
)	O
n	O
(	O
a|µ	O
,	O
σ2	O
)	O
da	O
(	O
cid:7	O
)	O
σ	O
κ	O
(	O
σ2	O
)	O
µ	O
(	O
4.153	O
)	O
exercises	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
where	O
we	O
have	O
deﬁned	O
(	O
4.154	O
)	O
applying	O
this	O
result	O
to	O
(	O
4.151	O
)	O
we	O
obtain	O
the	O
approximate	O
predictive	B
distribution	I
κ	O
(	O
σ2	O
)	O
=	O
(	O
1	O
+	O
πσ2/8	O
)	O
−1/2	O
.	O
in	O
the	O
form	O
p	O
(	O
c1|φ	O
,	O
t	O
)	O
=	O
σ	O
κ	O
(	O
σ2	O
a	O
)	O
µa	O
where	O
µa	O
and	O
σ2	O
ﬁned	O
by	O
(	O
4.154	O
)	O
.	O
a	O
are	O
deﬁned	O
by	O
(	O
4.149	O
)	O
and	O
(	O
4.150	O
)	O
,	O
respectively	O
,	O
and	O
κ	O
(	O
σ2	O
(	O
4.155	O
)	O
a	O
)	O
is	O
de-	O
note	O
that	O
the	O
decision	B
boundary	I
corresponding	O
to	O
p	O
(	O
c1|φ	O
,	O
t	O
)	O
=	O
0.5	O
is	O
given	O
by	O
µa	O
=	O
0	O
,	O
which	O
is	O
the	O
same	O
as	O
the	O
decision	B
boundary	I
obtained	O
by	O
using	O
the	O
map	O
value	O
for	O
w.	O
thus	O
if	O
the	O
decision	O
criterion	O
is	O
based	O
on	O
minimizing	O
misclassiﬁca-	O
tion	O
rate	O
,	O
with	O
equal	O
prior	B
probabilities	O
,	O
then	O
the	O
marginalization	O
over	O
w	O
has	O
no	O
ef-	O
fect	O
.	O
however	O
,	O
for	O
more	O
complex	O
decision	O
criteria	O
it	O
will	O
play	O
an	O
important	O
role	O
.	O
marginalization	O
of	O
the	O
logistic	B
sigmoid	I
model	O
under	O
a	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
distribution	O
will	O
be	O
illustrated	O
in	O
the	O
context	O
of	O
variational	B
inference	I
in	O
figure	O
10.13	O
.	O
(	O
cid:2	O
)	O
4.1	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
given	O
a	O
set	O
of	O
data	O
points	O
{	O
xn	O
}	O
,	O
we	O
can	O
deﬁne	O
the	O
convex	O
hull	O
to	O
be	O
the	O
set	O
of	O
all	O
points	O
x	O
given	O
by	O
x	O
=	O
(	O
cid:5	O
)	O
(	O
4.156	O
)	O
n	O
αn	O
=	O
1.	O
consider	O
a	O
second	O
set	O
of	O
points	O
{	O
yn	O
}	O
together	O
with	O
separable	O
if	O
there	O
exists	O
a	O
vector	O
(	O
cid:1	O
)	O
w	O
and	O
a	O
scalar	O
w0	O
such	O
that	O
(	O
cid:1	O
)	O
wtxn	O
+	O
w0	O
>	O
0	O
for	O
all	O
where	O
αn	O
(	O
cid:2	O
)	O
0	O
and	O
xn	O
,	O
and	O
(	O
cid:1	O
)	O
wtyn	O
+	O
w0	O
<	O
0	O
for	O
all	O
yn	O
.	O
show	O
that	O
if	O
their	O
convex	O
hulls	O
intersect	O
,	O
the	O
two	O
their	O
corresponding	O
convex	O
hull	O
.	O
by	O
deﬁnition	O
,	O
the	O
two	O
sets	O
of	O
points	O
will	O
be	O
linearly	O
αnxn	O
n	O
sets	O
of	O
points	O
can	O
not	O
be	O
linearly	B
separable	I
,	O
and	O
conversely	O
that	O
if	O
they	O
are	O
linearly	B
separable	I
,	O
their	O
convex	O
hulls	O
do	O
not	O
intersect	O
.	O
4.2	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
the	O
minimization	O
of	O
a	O
sum-of-squares	B
error	I
function	O
(	O
4.15	O
)	O
,	O
and	O
suppose	O
that	O
all	O
of	O
the	O
target	O
vectors	O
in	O
the	O
training	B
set	I
satisfy	O
a	O
linear	O
constraint	O
attn	O
+	O
b	O
=	O
0	O
(	O
4.157	O
)	O
where	O
tn	O
corresponds	O
to	O
the	O
nth	O
row	O
of	O
the	O
matrix	O
t	O
in	O
(	O
4.15	O
)	O
.	O
show	O
that	O
as	O
a	O
consequence	O
of	O
this	O
constraint	O
,	O
the	O
elements	O
of	O
the	O
model	O
prediction	O
y	O
(	O
x	O
)	O
given	O
by	O
the	O
least-squares	O
solution	O
(	O
4.17	O
)	O
also	O
satisfy	O
this	O
constraint	O
,	O
so	O
that	O
aty	O
(	O
x	O
)	O
+	O
b	O
=	O
0	O
.	O
(	O
4.158	O
)	O
exercises	O
221	O
to	O
do	O
so	O
,	O
assume	O
that	O
one	O
of	O
the	O
basis	O
functions	O
φ0	O
(	O
x	O
)	O
=	O
1	O
so	O
that	O
the	O
corresponding	O
parameter	O
w0	O
plays	O
the	O
role	O
of	O
a	O
bias	B
.	O
4.3	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
extend	O
the	O
result	O
of	O
exercise	O
4.2	O
to	O
show	O
that	O
if	O
multiple	O
linear	O
constraints	O
are	O
satisﬁed	O
simultaneously	O
by	O
the	O
target	O
vectors	O
,	O
then	O
the	O
same	O
constraints	O
will	O
also	O
be	O
satisﬁed	O
by	O
the	O
least-squares	O
prediction	O
of	O
a	O
linear	O
model	O
.	O
4.4	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
maximization	O
of	O
the	O
class	O
separation	O
criterion	O
given	O
by	O
(	O
4.23	O
)	O
with	O
respect	O
to	O
w	O
,	O
using	O
a	O
lagrange	O
multiplier	O
to	O
enforce	O
the	O
constraint	O
wtw	O
=	O
1	O
,	O
leads	O
to	O
the	O
result	O
that	O
w	O
∝	O
(	O
m2	O
−	O
m1	O
)	O
.	O
4.5	O
(	O
(	O
cid:12	O
)	O
)	O
by	O
making	O
use	O
of	O
(	O
4.20	O
)	O
,	O
(	O
4.23	O
)	O
,	O
and	O
(	O
4.24	O
)	O
,	O
show	O
that	O
the	O
fisher	O
criterion	O
(	O
4.25	O
)	O
can	O
be	O
written	O
in	O
the	O
form	O
(	O
4.26	O
)	O
.	O
4.6	O
(	O
(	O
cid:12	O
)	O
)	O
using	O
the	O
deﬁnitions	O
of	O
the	O
between-class	B
and	O
within-class	B
covariance	I
matrices	O
given	O
by	O
(	O
4.27	O
)	O
and	O
(	O
4.28	O
)	O
,	O
respectively	O
,	O
together	O
with	O
(	O
4.34	O
)	O
and	O
(	O
4.36	O
)	O
and	O
the	O
choice	O
of	O
target	O
values	O
described	O
in	O
section	O
4.1.5	O
,	O
show	O
that	O
the	O
expression	O
(	O
4.33	O
)	O
that	O
minimizes	O
the	O
sum-of-squares	B
error	I
function	O
can	O
be	O
written	O
in	O
the	O
form	O
(	O
4.37	O
)	O
.	O
4.7	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
the	O
logistic	B
sigmoid	I
function	O
(	O
4.59	O
)	O
satisﬁes	O
the	O
property	O
σ	O
(	O
−a	O
)	O
=	O
1	O
−	O
σ	O
(	O
a	O
)	O
and	O
that	O
its	O
inverse	B
is	O
given	O
by	O
σ	O
−1	O
(	O
y	O
)	O
=	O
ln	O
{	O
y/	O
(	O
1	O
−	O
y	O
)	O
}	O
.	O
4.8	O
(	O
(	O
cid:12	O
)	O
)	O
using	O
(	O
4.57	O
)	O
and	O
(	O
4.58	O
)	O
,	O
derive	O
the	O
result	O
(	O
4.65	O
)	O
for	O
the	O
posterior	O
class	O
probability	B
in	O
the	O
two-class	O
generative	B
model	I
with	O
gaussian	O
densities	O
,	O
and	O
verify	O
the	O
results	O
(	O
4.66	O
)	O
and	O
(	O
4.67	O
)	O
for	O
the	O
parameters	O
w	O
and	O
w0	O
.	O
4.9	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
generative	O
classiﬁcation	O
model	O
for	O
k	O
classes	O
deﬁned	O
by	O
prior	B
class	O
probabilities	O
p	O
(	O
ck	O
)	O
=	O
πk	O
and	O
general	O
class-conditional	O
densities	O
p	O
(	O
φ|ck	O
)	O
where	O
φ	O
is	O
the	O
input	O
feature	O
vector	O
.	O
suppose	O
we	O
are	O
given	O
a	O
training	B
data	O
set	O
{	O
φn	O
,	O
tn	O
}	O
where	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
and	O
tn	O
is	O
a	O
binary	O
target	O
vector	O
of	O
length	O
k	O
that	O
uses	O
the	O
1-of-	O
k	O
coding	O
scheme	O
,	O
so	O
that	O
it	O
has	O
components	O
tnj	O
=	O
ijk	O
if	O
pattern	O
n	O
is	O
from	O
class	O
ck	O
.	O
assuming	O
that	O
the	O
data	O
points	O
are	O
drawn	O
independently	O
from	O
this	O
model	O
,	O
show	O
that	O
the	O
maximum-likelihood	O
solution	O
for	O
the	O
prior	B
probabilities	O
is	O
given	O
by	O
where	O
nk	O
is	O
the	O
number	O
of	O
data	O
points	O
assigned	O
to	O
class	O
ck	O
.	O
πk	O
=	O
nk	O
n	O
(	O
4.159	O
)	O
4.10	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
the	O
classiﬁcation	B
model	O
of	O
exercise	O
4.9	O
and	O
now	O
suppose	O
that	O
the	O
class-conditional	O
densities	O
are	O
given	O
by	O
gaussian	O
distributions	O
with	O
a	O
shared	O
covari-	O
ance	O
matrix	O
,	O
so	O
that	O
p	O
(	O
φ|ck	O
)	O
=	O
n	O
(	O
φ|µk	O
,	O
σ	O
)	O
.	O
(	O
4.160	O
)	O
show	O
that	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
mean	B
of	O
the	O
gaussian	O
distribution	O
for	O
class	O
ck	O
is	O
given	O
by	O
tnkφn	O
(	O
4.161	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
µk	O
=	O
1	O
nk	O
222	O
4.	O
linear	O
models	O
for	O
classification	O
which	O
represents	O
the	O
mean	B
of	O
those	O
feature	O
vectors	O
assigned	O
to	O
class	O
ck	O
.	O
similarly	O
,	O
show	O
that	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
shared	O
covariance	O
matrix	O
is	O
given	O
by	O
k	O
(	O
cid:2	O
)	O
k=1	O
σ	O
=	O
nk	O
n	O
sk	O
where	O
sk	O
=	O
1	O
nk	O
tnk	O
(	O
φn	O
−	O
µk	O
)	O
(	O
φn	O
−	O
µk	O
)	O
t.	O
n	O
(	O
cid:2	O
)	O
n=1	O
(	O
4.162	O
)	O
(	O
4.163	O
)	O
thus	O
σ	O
is	O
given	O
by	O
a	O
weighted	O
average	O
of	O
the	O
covariances	O
of	O
the	O
data	O
associated	O
with	O
each	O
class	O
,	O
in	O
which	O
the	O
weighting	O
coefﬁcients	O
are	O
given	O
by	O
the	O
prior	B
probabilities	O
of	O
the	O
classes	O
.	O
4.11	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
classiﬁcation	B
problem	O
with	O
k	O
classes	O
for	O
which	O
the	O
feature	O
vector	O
φ	O
has	O
m	O
components	O
each	O
of	O
which	O
can	O
take	O
l	O
discrete	O
states	O
.	O
let	O
the	O
values	O
of	O
the	O
components	O
be	O
represented	O
by	O
a	O
1-of-l	O
binary	O
coding	O
scheme	O
.	O
further	O
suppose	O
that	O
,	O
conditioned	O
on	O
the	O
class	O
ck	O
,	O
the	O
m	O
components	O
of	O
φ	O
are	O
independent	B
,	O
so	O
that	O
the	O
class-conditional	O
density	B
factorizes	O
with	O
respect	O
to	O
the	O
feature	O
vector	O
components	O
.	O
show	O
that	O
the	O
quantities	O
ak	O
given	O
by	O
(	O
4.63	O
)	O
,	O
which	O
appear	O
in	O
the	O
argument	O
to	O
the	O
softmax	B
function	I
describing	O
the	O
posterior	O
class	O
probabilities	O
,	O
are	O
linear	O
functions	O
of	O
the	O
components	O
of	O
φ.	O
note	O
that	O
this	O
represents	O
an	O
example	O
of	O
the	O
naive	O
bayes	O
model	O
which	O
is	O
discussed	O
in	O
section	O
8.2.2	O
.	O
4.12	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
verify	O
the	O
relation	O
(	O
4.88	O
)	O
for	O
the	O
derivative	B
of	O
the	O
logistic	B
sigmoid	I
func-	O
tion	O
deﬁned	O
by	O
(	O
4.59	O
)	O
.	O
4.13	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
by	O
making	O
use	O
of	O
the	O
result	O
(	O
4.88	O
)	O
for	O
the	O
derivative	B
of	O
the	O
logistic	O
sig-	O
moid	O
,	O
show	O
that	O
the	O
derivative	B
of	O
the	O
error	B
function	I
(	O
4.90	O
)	O
for	O
the	O
logistic	B
regression	I
model	O
is	O
given	O
by	O
(	O
4.91	O
)	O
.	O
4.14	O
(	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
for	O
a	O
linearly	B
separable	I
data	O
set	O
,	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
logistic	B
regression	I
model	O
is	O
obtained	O
by	O
ﬁnding	O
a	O
vector	O
w	O
whose	O
decision	B
boundary	I
wtφ	O
(	O
x	O
)	O
=	O
0	O
separates	O
the	O
classes	O
and	O
then	O
taking	O
the	O
magnitude	O
of	O
w	O
to	O
inﬁnity	O
.	O
4.15	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
the	O
hessian	O
matrix	O
h	O
for	O
the	O
logistic	B
regression	I
model	O
,	O
given	O
by	O
(	O
4.97	O
)	O
,	O
is	O
positive	B
deﬁnite	I
.	O
here	O
r	O
is	O
a	O
diagonal	B
matrix	O
with	O
elements	O
yn	O
(	O
1	O
−	O
yn	O
)	O
,	O
and	O
yn	O
is	O
the	O
output	O
of	O
the	O
logistic	B
regression	I
model	O
for	O
input	O
vector	O
xn	O
.	O
hence	O
show	O
that	O
the	O
error	B
function	I
is	O
a	O
concave	B
function	I
of	O
w	O
and	O
that	O
it	O
has	O
a	O
unique	O
minimum	O
.	O
4.16	O
(	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
binary	O
classiﬁcation	O
problem	O
in	O
which	O
each	O
observation	O
xn	O
is	O
known	O
to	O
belong	O
to	O
one	O
of	O
two	O
classes	O
,	O
corresponding	O
to	O
t	O
=	O
0	O
and	O
t	O
=	O
1	O
,	O
and	O
suppose	O
that	O
the	O
procedure	O
for	O
collecting	O
training	B
data	O
is	O
imperfect	O
,	O
so	O
that	O
training	B
points	O
are	O
sometimes	O
mislabelled	O
.	O
for	O
every	O
data	O
point	O
xn	O
,	O
instead	O
of	O
having	O
a	O
value	O
t	O
for	O
the	O
class	O
label	O
,	O
we	O
have	O
instead	O
a	O
value	O
πn	O
representing	O
the	O
probability	B
that	O
tn	O
=	O
1.	O
given	O
a	O
probabilistic	O
model	O
p	O
(	O
t	O
=	O
1|φ	O
)	O
,	O
write	O
down	O
the	O
log	O
likelihood	O
function	O
appropriate	O
to	O
such	O
a	O
data	O
set	O
.	O
exercises	O
223	O
4.17	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
the	O
derivatives	O
of	O
the	O
softmax	O
activation	O
function	O
(	O
4.104	O
)	O
,	O
where	O
the	O
ak	O
are	O
deﬁned	O
by	O
(	O
4.105	O
)	O
,	O
are	O
given	O
by	O
(	O
4.106	O
)	O
.	O
4.18	O
(	O
(	O
cid:12	O
)	O
)	O
using	O
the	O
result	O
(	O
4.91	O
)	O
for	O
the	O
derivatives	O
of	O
the	O
softmax	O
activation	O
function	O
,	O
show	O
that	O
the	O
gradients	O
of	O
the	O
cross-entropy	O
error	O
(	O
4.108	O
)	O
are	O
given	O
by	O
(	O
4.109	O
)	O
.	O
4.19	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
write	O
down	O
expressions	O
for	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	O
,	O
as	O
well	O
as	O
the	O
corresponding	O
hessian	O
matrix	O
,	O
for	O
the	O
probit	B
regression	I
model	O
deﬁned	O
in	O
sec-	O
tion	O
4.3.5.	O
these	O
are	O
the	O
quantities	O
that	O
would	O
be	O
required	O
to	O
train	O
such	O
a	O
model	O
using	O
irls	O
.	O
4.20	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
the	O
hessian	O
matrix	O
for	O
the	O
multiclass	B
logistic	O
regression	B
problem	O
,	O
deﬁned	O
by	O
(	O
4.110	O
)	O
,	O
is	O
positive	O
semideﬁnite	O
.	O
note	O
that	O
the	O
full	O
hessian	O
matrix	O
for	O
this	O
problem	O
is	O
of	O
size	O
m	O
k	O
×	O
m	O
k	O
,	O
where	O
m	O
is	O
the	O
number	O
of	O
parameters	O
and	O
k	O
is	O
the	O
number	O
of	O
classes	O
.	O
to	O
prove	O
the	O
positive	O
semideﬁnite	O
property	O
,	O
consider	O
the	O
product	O
uthu	O
where	O
u	O
is	O
an	O
arbitrary	O
vector	O
of	O
length	O
m	O
k	O
,	O
and	O
then	O
apply	O
jensen	O
’	O
s	O
inequality	O
.	O
4.21	O
(	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
the	O
probit	B
function	I
(	O
4.114	O
)	O
and	O
the	O
erf	B
function	I
(	O
4.115	O
)	O
are	O
related	O
by	O
(	O
4.116	O
)	O
.	O
4.22	O
(	O
(	O
cid:12	O
)	O
)	O
using	O
the	O
result	O
(	O
4.135	O
)	O
,	O
derive	O
the	O
expression	O
(	O
4.137	O
)	O
for	O
the	O
log	O
model	O
evi-	O
dence	O
under	O
the	O
laplace	O
approximation	O
.	O
4.23	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
in	O
this	O
exercise	O
,	O
we	O
derive	O
the	O
bic	O
result	O
(	O
4.139	O
)	O
starting	O
from	O
the	O
laplace	O
approximation	O
to	O
the	O
model	B
evidence	I
given	O
by	O
(	O
4.137	O
)	O
.	O
show	O
that	O
if	O
the	O
prior	B
over	O
parameters	O
is	O
gaussian	O
of	O
the	O
form	O
p	O
(	O
θ	O
)	O
=	O
n	O
(	O
θ|m	O
,	O
v0	O
)	O
,	O
the	O
log	O
model	O
evidence	O
under	O
the	O
laplace	O
approximation	O
takes	O
the	O
form	O
ln	O
p	O
(	O
d	O
)	O
(	O
cid:7	O
)	O
ln	O
p	O
(	O
d|θmap	O
)	O
−	O
1	O
ln|h|	O
+	O
const	O
2	O
where	O
h	O
is	O
the	O
matrix	O
of	O
second	O
derivatives	O
of	O
the	O
log	O
likelihood	O
ln	O
p	O
(	O
d|θ	O
)	O
evaluated	O
at	O
θmap	O
.	O
now	O
assume	O
that	O
the	O
prior	B
is	O
broad	O
so	O
that	O
v	O
is	O
small	O
and	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
above	O
can	O
be	O
neglected	O
.	O
furthermore	O
,	O
consider	O
the	O
case	O
of	O
independent	B
,	O
identically	O
distributed	O
data	O
so	O
that	O
h	O
is	O
the	O
sum	O
of	O
terms	O
one	O
for	O
each	O
data	O
point	O
.	O
show	O
that	O
the	O
log	O
model	O
evidence	O
can	O
then	O
be	O
written	O
approximately	O
in	O
the	O
form	O
of	O
the	O
bic	O
expression	O
(	O
4.139	O
)	O
.	O
0	O
(	O
θmap	O
−	O
m	O
)	O
−	O
1	O
−1	O
2	O
(	O
θmap	O
−	O
m	O
)	O
tv	O
−1	O
0	O
4.24	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
use	O
the	O
results	O
from	O
section	O
2.3.2	O
to	O
derive	O
the	O
result	O
(	O
4.151	O
)	O
for	O
the	O
marginal-	O
ization	O
of	O
the	O
logistic	B
regression	I
model	O
with	O
respect	O
to	O
a	O
gaussian	O
posterior	O
distribu-	O
tion	O
over	O
the	O
parameters	O
w.	O
4.25	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
suppose	O
we	O
wish	O
to	O
approximate	O
the	O
logistic	B
sigmoid	I
σ	O
(	O
a	O
)	O
deﬁned	O
by	O
(	O
4.59	O
)	O
by	O
a	O
scaled	O
probit	B
function	I
φ	O
(	O
λa	O
)	O
,	O
where	O
φ	O
(	O
a	O
)	O
is	O
deﬁned	O
by	O
(	O
4.114	O
)	O
.	O
show	O
that	O
if	O
λ	O
is	O
chosen	O
so	O
that	O
the	O
derivatives	O
of	O
the	O
two	O
functions	O
are	O
equal	O
at	O
a	O
=	O
0	O
,	O
then	O
λ2	O
=	O
π/8	O
.	O
224	O
4.	O
linear	O
models	O
for	O
classification	O
4.26	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
in	O
this	O
exercise	O
,	O
we	O
prove	O
the	O
relation	O
(	O
4.152	O
)	O
for	O
the	O
convolution	O
of	O
a	O
probit	B
function	I
with	O
a	O
gaussian	O
distribution	O
.	O
to	O
do	O
this	O
,	O
show	O
that	O
the	O
derivative	B
of	O
the	O
left-	O
hand	O
side	O
with	O
respect	O
to	O
µ	O
is	O
equal	O
to	O
the	O
derivative	B
of	O
the	O
right-hand	O
side	O
,	O
and	O
then	O
integrate	O
both	O
sides	O
with	O
respect	O
to	O
µ	O
and	O
then	O
show	O
that	O
the	O
constant	O
of	O
integration	O
vanishes	O
.	O
note	O
that	O
before	O
differentiating	O
the	O
left-hand	O
side	O
,	O
it	O
is	O
convenient	O
ﬁrst	O
to	O
introduce	O
a	O
change	O
of	O
variable	O
given	O
by	O
a	O
=	O
µ	O
+	O
σz	O
so	O
that	O
the	O
integral	O
over	O
a	O
is	O
replaced	O
by	O
an	O
integral	O
over	O
z.	O
when	O
we	O
differentiate	O
the	O
left-hand	O
side	O
of	O
the	O
relation	O
(	O
4.152	O
)	O
,	O
we	O
will	O
then	O
obtain	O
a	O
gaussian	O
integral	O
over	O
z	O
that	O
can	O
be	O
evaluated	O
analytically	O
.	O
5	O
neural	O
networks	O
in	O
chapters	O
3	O
and	O
4	O
we	O
considered	O
models	O
for	B
regression	I
and	O
classiﬁcation	B
that	O
com-	O
prised	O
linear	O
combinations	O
of	O
ﬁxed	O
basis	O
functions	O
.	O
we	O
saw	O
that	O
such	O
models	O
have	O
useful	O
analytical	O
and	O
computational	O
properties	O
but	O
that	O
their	O
practical	O
applicability	O
was	O
limited	O
by	O
the	O
curse	B
of	I
dimensionality	I
.	O
in	O
order	O
to	O
apply	O
such	O
models	O
to	O
large-	O
scale	O
problems	O
,	O
it	O
is	O
necessary	O
to	O
adapt	O
the	O
basis	O
functions	O
to	O
the	O
data	O
.	O
support	B
vector	I
machines	O
(	O
svms	O
)	O
,	O
discussed	O
in	O
chapter	O
7	O
,	O
address	O
this	O
by	O
ﬁrst	O
deﬁning	O
basis	O
functions	O
that	O
are	O
centred	O
on	O
the	O
training	B
data	O
points	O
and	O
then	O
selecting	O
a	O
subset	O
of	O
these	O
during	O
training	B
.	O
one	O
advantage	O
of	O
svms	O
is	O
that	O
,	O
although	O
the	O
training	B
involves	O
nonlinear	O
optimization	O
,	O
the	O
objective	O
function	O
is	O
convex	O
,	O
and	O
so	O
the	O
solution	O
of	O
the	O
optimization	O
problem	O
is	O
relatively	O
straightforward	O
.	O
the	O
number	O
of	O
basis	O
functions	O
in	O
the	O
resulting	O
models	O
is	O
generally	O
much	O
smaller	O
than	O
the	O
number	O
of	O
training	B
points	O
,	O
although	O
it	O
is	O
often	O
still	O
relatively	O
large	O
and	O
typically	O
increases	O
with	O
the	O
size	O
of	O
the	O
training	B
set	I
.	O
the	O
relevance	B
vector	I
machine	I
,	O
discussed	O
in	O
section	O
7.2	O
,	O
also	O
chooses	O
a	O
subset	O
from	O
a	O
ﬁxed	O
set	O
of	O
basis	O
functions	O
and	O
typically	O
results	O
in	O
much	O
225	O
226	O
5.	O
neural	O
networks	O
sparser	O
models	O
.	O
unlike	O
the	O
svm	O
it	O
also	O
produces	O
probabilistic	O
outputs	O
,	O
although	O
this	O
is	O
at	O
the	O
expense	O
of	O
a	O
nonconvex	O
optimization	O
during	O
training	B
.	O
an	O
alternative	O
approach	O
is	O
to	O
ﬁx	O
the	O
number	O
of	O
basis	O
functions	O
in	O
advance	O
but	O
allow	O
them	O
to	O
be	O
adaptive	O
,	O
in	O
other	O
words	O
to	O
use	O
parametric	O
forms	O
for	O
the	O
basis	O
func-	O
tions	O
in	O
which	O
the	O
parameter	O
values	O
are	O
adapted	O
during	O
training	B
.	O
the	O
most	O
successful	O
model	O
of	O
this	O
type	O
in	O
the	O
context	O
of	O
pattern	O
recognition	O
is	O
the	O
feed-forward	O
neural	B
network	I
,	O
also	O
known	O
as	O
the	O
multilayer	B
perceptron	I
,	O
discussed	O
in	O
this	O
chapter	O
.	O
in	O
fact	O
,	O
‘	O
multilayer	B
perceptron	I
’	O
is	O
really	O
a	O
misnomer	O
,	O
because	O
the	O
model	O
comprises	O
multi-	O
ple	O
layers	O
of	O
logistic	B
regression	I
models	O
(	O
with	O
continuous	O
nonlinearities	O
)	O
rather	O
than	O
multiple	O
perceptrons	O
(	O
with	O
discontinuous	O
nonlinearities	O
)	O
.	O
for	O
many	O
applications	O
,	O
the	O
resulting	O
model	O
can	O
be	O
signiﬁcantly	O
more	O
compact	O
,	O
and	O
hence	O
faster	O
to	O
evaluate	O
,	O
than	O
a	O
support	B
vector	I
machine	I
having	O
the	O
same	O
generalization	B
performance	O
.	O
the	O
price	O
to	O
be	O
paid	O
for	O
this	O
compactness	O
,	O
as	O
with	O
the	O
relevance	B
vector	I
machine	I
,	O
is	O
that	O
the	O
like-	O
lihood	O
function	O
,	O
which	O
forms	O
the	O
basis	O
for	O
network	O
training	B
,	O
is	O
no	O
longer	O
a	O
convex	B
function	I
of	O
the	O
model	O
parameters	O
.	O
in	O
practice	O
,	O
however	O
,	O
it	O
is	O
often	O
worth	O
investing	O
substantial	O
computational	O
resources	O
during	O
the	O
training	B
phase	O
in	O
order	O
to	O
obtain	O
a	O
compact	O
model	O
that	O
is	O
fast	O
at	O
processing	O
new	O
data	O
.	O
the	O
term	O
‘	O
neural	B
network	I
’	O
has	O
its	O
origins	O
in	O
attempts	O
to	O
ﬁnd	O
mathematical	O
rep-	O
resentations	O
of	O
information	O
processing	O
in	O
biological	O
systems	O
(	O
mcculloch	O
and	O
pitts	O
,	O
1943	O
;	O
widrow	O
and	O
hoff	O
,	O
1960	O
;	O
rosenblatt	O
,	O
1962	O
;	O
rumelhart	O
et	O
al.	O
,	O
1986	O
)	O
.	O
indeed	O
,	O
it	O
has	O
been	O
used	O
very	O
broadly	O
to	O
cover	O
a	O
wide	O
range	O
of	O
different	O
models	O
,	O
many	O
of	O
which	O
have	O
been	O
the	O
subject	O
of	O
exaggerated	O
claims	O
regarding	O
their	O
biological	O
plau-	O
sibility	O
.	O
from	O
the	O
perspective	O
of	O
practical	O
applications	O
of	O
pattern	O
recognition	O
,	O
how-	O
ever	O
,	O
biological	O
realism	O
would	O
impose	O
entirely	O
unnecessary	O
constraints	O
.	O
our	O
focus	O
in	O
this	O
chapter	O
is	O
therefore	O
on	O
neural	O
networks	O
as	O
efﬁcient	O
models	O
for	O
statistical	O
pattern	O
recognition	O
.	O
in	O
particular	O
,	O
we	O
shall	O
restrict	O
our	O
attention	O
to	O
the	O
speciﬁc	O
class	O
of	O
neu-	O
ral	O
networks	O
that	O
have	O
proven	O
to	O
be	O
of	O
greatest	O
practical	O
value	O
,	O
namely	O
the	O
multilayer	B
perceptron	I
.	O
we	O
begin	O
by	O
considering	O
the	O
functional	B
form	O
of	O
the	O
network	O
model	O
,	O
including	O
the	O
speciﬁc	O
parameterization	O
of	O
the	O
basis	O
functions	O
,	O
and	O
we	O
then	O
discuss	O
the	O
prob-	O
lem	O
of	O
determining	O
the	O
network	O
parameters	O
within	O
a	O
maximum	B
likelihood	I
frame-	O
work	O
,	O
which	O
involves	O
the	O
solution	O
of	O
a	O
nonlinear	O
optimization	O
problem	O
.	O
this	O
requires	O
the	O
evaluation	O
of	O
derivatives	O
of	O
the	O
log	O
likelihood	O
function	O
with	O
respect	O
to	O
the	O
net-	O
work	O
parameters	O
,	O
and	O
we	O
shall	O
see	O
how	O
these	O
can	O
be	O
obtained	O
efﬁciently	O
using	O
the	O
technique	O
of	O
error	B
backpropagation	I
.	O
we	O
shall	O
also	O
show	O
how	O
the	O
backpropagation	B
framework	O
can	O
be	O
extended	B
to	O
allow	O
other	O
derivatives	O
to	O
be	O
evaluated	O
,	O
such	O
as	O
the	O
jacobian	O
and	O
hessian	O
matrices	O
.	O
next	O
we	O
discuss	O
various	O
approaches	O
to	O
regulariza-	O
tion	O
of	O
neural	B
network	I
training	O
and	O
the	O
relationships	O
between	O
them	O
.	O
we	O
also	O
consider	O
some	O
extensions	O
to	O
the	O
neural	B
network	I
model	O
,	O
and	O
in	O
particular	O
we	O
describe	O
a	O
gen-	O
eral	O
framework	O
for	O
modelling	O
conditional	B
probability	I
distributions	O
known	O
as	O
mixture	O
density	O
networks	O
.	O
finally	O
,	O
we	O
discuss	O
the	O
use	O
of	O
bayesian	O
treatments	O
of	O
neural	O
net-	O
works	O
.	O
additional	O
background	O
on	O
neural	B
network	I
models	O
can	O
be	O
found	O
in	O
bishop	O
(	O
1995a	O
)	O
.	O
5.1.	O
feed-forward	O
network	O
functions	O
227	O
5.1.	O
feed-forward	O
network	O
functions	O
the	O
linear	O
models	O
for	B
regression	I
and	O
classiﬁcation	B
discussed	O
in	O
chapters	O
3	O
and	O
4	O
,	O
re-	O
spectively	O
,	O
are	O
based	O
on	O
linear	O
combinations	O
of	O
ﬁxed	O
nonlinear	O
basis	O
functions	O
φj	O
(	O
x	O
)	O
and	O
take	O
the	O
form	O
(	O
cid:23	O
)	O
(	O
cid:22	O
)	O
m	O
(	O
cid:2	O
)	O
j=1	O
wjφj	O
(	O
x	O
)	O
y	O
(	O
x	O
,	O
w	O
)	O
=	O
f	O
(	O
5.1	O
)	O
where	O
f	O
(	O
·	O
)	O
is	O
a	O
nonlinear	O
activation	B
function	I
in	O
the	O
case	O
of	O
classiﬁcation	O
and	O
is	O
the	O
identity	O
in	O
the	O
case	O
of	O
regression	B
.	O
our	O
goal	O
is	O
to	O
extend	O
this	O
model	O
by	O
making	O
the	O
basis	O
functions	O
φj	O
(	O
x	O
)	O
depend	O
on	O
parameters	O
and	O
then	O
to	O
allow	O
these	O
parameters	O
to	O
be	O
adjusted	O
,	O
along	O
with	O
the	O
coefﬁcients	O
{	O
wj	O
}	O
,	O
during	O
training	B
.	O
there	O
are	O
,	O
of	O
course	O
,	O
many	O
ways	O
to	O
construct	O
parametric	O
nonlinear	O
basis	O
functions	O
.	O
neural	O
networks	O
use	O
basis	O
functions	O
that	O
follow	O
the	O
same	O
form	O
as	O
(	O
5.1	O
)	O
,	O
so	O
that	O
each	O
basis	B
function	I
is	O
itself	O
a	O
nonlinear	O
function	O
of	O
a	O
linear	O
combination	O
of	O
the	O
inputs	O
,	O
where	O
the	O
coefﬁcients	O
in	O
the	O
linear	O
combination	O
are	O
adaptive	O
parameters	O
.	O
this	O
leads	O
to	O
the	O
basic	O
neural	B
network	I
model	O
,	O
which	O
can	O
be	O
described	O
a	O
series	O
of	O
functional	B
transformations	O
.	O
first	O
we	O
construct	O
m	O
linear	O
combinations	O
of	O
the	O
input	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
in	O
the	O
form	O
d	O
(	O
cid:2	O
)	O
aj	O
=	O
(	O
1	O
)	O
ji	O
xi	O
+	O
w	O
(	O
1	O
)	O
j0	O
w	O
(	O
5.2	O
)	O
i=1	O
where	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
,	O
and	O
the	O
superscript	O
(	O
1	O
)	O
indicates	O
that	O
the	O
corresponding	O
param-	O
(	O
1	O
)	O
eters	O
are	O
in	O
the	O
ﬁrst	O
‘	O
layer	O
’	O
of	O
the	O
network	O
.	O
we	O
shall	O
refer	O
to	O
the	O
parameters	O
w	O
ji	O
as	O
(	O
1	O
)	O
j0	O
as	O
biases	O
,	O
following	O
the	O
nomenclature	O
of	O
chapter	O
3.	O
weights	O
and	O
the	O
parameters	O
w	O
the	O
quantities	O
aj	O
are	O
known	O
as	O
activations	O
.	O
each	O
of	O
them	O
is	O
then	O
transformed	O
using	O
a	O
differentiable	O
,	O
nonlinear	O
activation	B
function	I
h	O
(	O
·	O
)	O
to	O
give	O
zj	O
=	O
h	O
(	O
aj	O
)	O
.	O
(	O
5.3	O
)	O
these	O
quantities	O
correspond	O
to	O
the	O
outputs	O
of	O
the	O
basis	O
functions	O
in	O
(	O
5.1	O
)	O
that	O
,	O
in	O
the	O
context	O
of	O
neural	O
networks	O
,	O
are	O
called	O
hidden	O
units	O
.	O
the	O
nonlinear	O
functions	O
h	O
(	O
·	O
)	O
are	O
generally	O
chosen	O
to	O
be	O
sigmoidal	O
functions	O
such	O
as	O
the	O
logistic	B
sigmoid	I
or	O
the	O
‘	O
tanh	O
’	O
function	O
.	O
following	O
(	O
5.1	O
)	O
,	O
these	O
values	O
are	O
again	O
linearly	O
combined	O
to	O
give	O
output	O
unit	O
activations	O
m	O
(	O
cid:2	O
)	O
ak	O
=	O
w	O
(	O
2	O
)	O
kj	O
zj	O
+	O
w	O
(	O
2	O
)	O
k0	O
(	O
5.4	O
)	O
j=1	O
where	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
,	O
and	O
k	O
is	O
the	O
total	O
number	O
of	O
outputs	O
.	O
this	O
transformation	O
cor-	O
(	O
2	O
)	O
responds	O
to	O
the	O
second	O
layer	O
of	O
the	O
network	O
,	O
and	O
again	O
the	O
w	O
k0	O
are	O
bias	B
parameters	O
.	O
finally	O
,	O
the	O
output	O
unit	O
activations	O
are	O
transformed	O
using	O
an	O
appropriate	O
activation	B
function	I
to	O
give	O
a	O
set	O
of	O
network	O
outputs	O
yk	O
.	O
the	O
choice	O
of	O
activation	B
function	I
is	O
determined	O
by	O
the	O
nature	O
of	O
the	O
data	O
and	O
the	O
assumed	O
distribution	O
of	O
target	O
variables	O
exercise	O
5.1	O
228	O
5.	O
neural	O
networks	O
figure	O
5.1	O
network	O
diagram	O
for	O
the	O
two-	O
layer	O
neural	B
network	I
corre-	O
sponding	O
to	O
(	O
5.7	O
)	O
.	O
the	O
input	O
,	O
hidden	O
,	O
and	O
output	O
variables	O
are	O
represented	O
by	O
nodes	O
,	O
and	O
the	O
weight	O
parameters	O
are	O
rep-	O
resented	O
by	O
links	O
between	O
the	O
nodes	O
,	O
in	O
which	O
the	O
bias	B
pa-	O
rameters	O
are	O
denoted	O
by	O
links	O
coming	O
from	O
additional	O
input	O
and	O
hidden	O
variables	O
x0	O
and	O
z0	O
.	O
arrows	O
denote	O
the	O
direc-	O
tion	O
of	O
information	O
ﬂow	O
through	O
the	O
network	O
during	O
forward	B
propagation	I
.	O
xd	O
inputs	O
x1	O
x0	O
hidden	O
units	O
zm	O
(	O
1	O
)	O
m	O
d	O
w	O
(	O
2	O
)	O
km	O
w	O
yk	O
outputs	O
y1	O
(	O
2	O
)	O
10	O
w	O
z1	O
z0	O
and	O
follows	O
the	O
same	O
considerations	O
as	O
for	O
linear	O
models	O
discussed	O
in	O
chapters	O
3	O
and	O
4.	O
thus	O
for	O
standard	O
regression	B
problems	O
,	O
the	O
activation	B
function	I
is	O
the	O
identity	O
so	O
that	O
yk	O
=	O
ak	O
.	O
similarly	O
,	O
for	O
multiple	O
binary	O
classiﬁcation	O
problems	O
,	O
each	O
output	O
unit	O
activation	O
is	O
transformed	O
using	O
a	O
logistic	B
sigmoid	I
function	O
so	O
that	O
where	O
yk	O
=	O
σ	O
(	O
ak	O
)	O
σ	O
(	O
a	O
)	O
=	O
1	O
1	O
+	O
exp	O
(	O
−a	O
)	O
.	O
(	O
5.5	O
)	O
(	O
5.6	O
)	O
finally	O
,	O
for	O
multiclass	O
problems	O
,	O
a	O
softmax	O
activation	O
function	O
of	O
the	O
form	O
(	O
4.62	O
)	O
is	O
used	O
.	O
the	O
choice	O
of	O
output	O
unit	O
activation	B
function	I
is	O
discussed	O
in	O
detail	O
in	O
sec-	O
tion	O
5.2.	O
we	O
can	O
combine	O
these	O
various	O
stages	O
to	O
give	O
the	O
overall	O
network	O
function	O
that	O
,	O
for	O
sigmoidal	O
output	O
unit	O
activation	O
functions	O
,	O
takes	O
the	O
form	O
(	O
cid:23	O
)	O
(	O
cid:23	O
)	O
(	O
cid:22	O
)	O
m	O
(	O
cid:2	O
)	O
(	O
cid:22	O
)	O
d	O
(	O
cid:2	O
)	O
yk	O
(	O
x	O
,	O
w	O
)	O
=	O
σ	O
w	O
(	O
2	O
)	O
kj	O
h	O
(	O
1	O
)	O
ji	O
xi	O
+	O
w	O
(	O
1	O
)	O
j0	O
w	O
+	O
w	O
(	O
2	O
)	O
k0	O
(	O
5.7	O
)	O
j=1	O
i=1	O
where	O
the	O
set	O
of	O
all	O
weight	O
and	O
bias	B
parameters	O
have	O
been	O
grouped	O
together	O
into	O
a	O
vector	O
w.	O
thus	O
the	O
neural	B
network	I
model	O
is	O
simply	O
a	O
nonlinear	O
function	O
from	O
a	O
set	O
of	O
input	O
variables	O
{	O
xi	O
}	O
to	O
a	O
set	O
of	O
output	O
variables	O
{	O
yk	O
}	O
controlled	O
by	O
a	O
vector	O
w	O
of	O
adjustable	O
parameters	O
.	O
this	O
function	O
can	O
be	O
represented	O
in	O
the	O
form	O
of	O
a	O
network	O
diagram	O
as	O
shown	O
in	O
figure	O
5.1.	O
the	O
process	O
of	O
evaluating	O
(	O
5.7	O
)	O
can	O
then	O
be	O
interpreted	O
as	O
a	O
forward	B
propagation	I
of	O
information	O
through	O
the	O
network	O
.	O
it	O
should	O
be	O
emphasized	O
that	O
these	O
diagrams	O
do	O
not	O
represent	O
probabilistic	O
graphical	O
models	O
of	O
the	O
kind	O
to	O
be	O
consid-	O
ered	O
in	O
chapter	O
8	O
because	O
the	O
internal	O
nodes	O
represent	O
deterministic	O
variables	O
rather	O
than	O
stochastic	B
ones	O
.	O
for	O
this	O
reason	O
,	O
we	O
have	O
adopted	O
a	O
slightly	O
different	O
graphical	O
5.1.	O
feed-forward	O
network	O
functions	O
229	O
notation	O
for	O
the	O
two	O
kinds	O
of	O
model	O
.	O
we	O
shall	O
see	O
later	O
how	O
to	O
give	O
a	O
probabilistic	O
interpretation	O
to	O
a	O
neural	B
network	I
.	O
as	O
discussed	O
in	O
section	O
3.1	O
,	O
the	O
bias	B
parameters	O
in	O
(	O
5.2	O
)	O
can	O
be	O
absorbed	O
into	O
the	O
set	O
of	O
weight	O
parameters	O
by	O
deﬁning	O
an	O
additional	O
input	O
variable	O
x0	O
whose	O
value	O
is	O
clamped	O
at	O
x0	O
=	O
1	O
,	O
so	O
that	O
(	O
5.2	O
)	O
takes	O
the	O
form	O
aj	O
=	O
(	O
1	O
)	O
ji	O
xi	O
.	O
w	O
(	O
5.8	O
)	O
d	O
(	O
cid:2	O
)	O
i=0	O
we	O
can	O
similarly	O
absorb	O
the	O
second-layer	O
biases	O
into	O
the	O
second-layer	O
weights	O
,	O
so	O
that	O
the	O
overall	O
network	O
function	O
becomes	O
(	O
cid:22	O
)	O
m	O
(	O
cid:2	O
)	O
(	O
cid:22	O
)	O
d	O
(	O
cid:2	O
)	O
(	O
cid:23	O
)	O
(	O
cid:23	O
)	O
yk	O
(	O
x	O
,	O
w	O
)	O
=	O
σ	O
w	O
(	O
2	O
)	O
kj	O
h	O
(	O
1	O
)	O
ji	O
xi	O
w	O
.	O
(	O
5.9	O
)	O
j=0	O
i=0	O
as	O
can	O
be	O
seen	O
from	O
figure	O
5.1	O
,	O
the	O
neural	B
network	I
model	O
comprises	O
two	O
stages	O
of	O
processing	O
,	O
each	O
of	O
which	O
resembles	O
the	O
perceptron	B
model	O
of	O
section	O
4.1.7	O
,	O
and	O
for	O
this	O
reason	O
the	O
neural	B
network	I
is	O
also	O
known	O
as	O
the	O
multilayer	B
perceptron	I
,	O
or	O
mlp	O
.	O
a	O
key	O
difference	O
compared	O
to	O
the	O
perceptron	B
,	O
however	O
,	O
is	O
that	O
the	O
neural	O
net-	O
work	O
uses	O
continuous	O
sigmoidal	O
nonlinearities	O
in	O
the	O
hidden	O
units	O
,	O
whereas	O
the	O
per-	O
ceptron	O
uses	O
step-function	O
nonlinearities	O
.	O
this	O
means	O
that	O
the	O
neural	B
network	I
func-	O
tion	O
is	O
differentiable	O
with	O
respect	O
to	O
the	O
network	O
parameters	O
,	O
and	O
this	O
property	O
will	O
play	O
a	O
central	O
role	O
in	O
network	O
training	B
.	O
if	O
the	O
activation	O
functions	O
of	O
all	O
the	O
hidden	O
units	O
in	O
a	O
network	O
are	O
taken	O
to	O
be	O
linear	O
,	O
then	O
for	O
any	O
such	O
network	O
we	O
can	O
always	O
ﬁnd	O
an	O
equivalent	O
network	O
without	O
hidden	O
units	O
.	O
this	O
follows	O
from	O
the	O
fact	O
that	O
the	O
composition	O
of	O
successive	O
linear	O
transformations	O
is	O
itself	O
a	O
linear	O
transformation	O
.	O
however	O
,	O
if	O
the	O
number	O
of	O
hidden	O
units	O
is	O
smaller	O
than	O
either	O
the	O
number	O
of	O
input	O
or	O
output	O
units	O
,	O
then	O
the	O
transforma-	O
tions	O
that	O
the	O
network	O
can	O
generate	O
are	O
not	O
the	O
most	O
general	O
possible	O
linear	O
trans-	O
formations	O
from	O
inputs	O
to	O
outputs	O
because	O
information	O
is	O
lost	O
in	O
the	O
dimensionality	O
reduction	O
at	O
the	O
hidden	O
units	O
.	O
in	O
section	O
12.4.2	O
,	O
we	O
show	O
that	O
networks	O
of	O
linear	O
units	O
give	O
rise	O
to	O
principal	B
component	I
analysis	I
.	O
in	O
general	O
,	O
however	O
,	O
there	O
is	O
little	O
interest	O
in	O
multilayer	O
networks	O
of	O
linear	O
units	O
.	O
the	O
network	O
architecture	O
shown	O
in	O
figure	O
5.1	O
is	O
the	O
most	O
commonly	O
used	O
one	O
in	O
practice	O
.	O
however	O
,	O
it	O
is	O
easily	O
generalized	B
,	O
for	O
instance	O
by	O
considering	O
additional	O
layers	O
of	O
processing	O
each	O
consisting	O
of	O
a	O
weighted	O
linear	O
combination	O
of	O
the	O
form	O
(	O
5.4	O
)	O
followed	O
by	O
an	O
element-wise	O
transformation	O
using	O
a	O
nonlinear	O
activation	O
func-	O
tion	O
.	O
note	O
that	O
there	O
is	O
some	O
confusion	O
in	O
the	O
literature	O
regarding	O
the	O
terminology	O
for	O
counting	O
the	O
number	O
of	O
layers	O
in	O
such	O
networks	O
.	O
thus	O
the	O
network	O
in	O
figure	O
5.1	O
may	O
be	O
described	O
as	O
a	O
3-layer	O
network	O
(	O
which	O
counts	O
the	O
number	O
of	O
layers	O
of	O
units	O
,	O
and	O
treats	O
the	O
inputs	O
as	O
units	O
)	O
or	O
sometimes	O
as	O
a	O
single-hidden-layer	O
network	O
(	O
which	O
counts	O
the	O
number	O
of	O
layers	O
of	O
hidden	O
units	O
)	O
.	O
we	O
recommend	O
a	O
terminology	O
in	O
which	O
figure	O
5.1	O
is	O
called	O
a	O
two-layer	O
network	O
,	O
because	O
it	O
is	O
the	O
number	O
of	O
layers	O
of	O
adap-	O
tive	O
weights	O
that	O
is	O
important	O
for	O
determining	O
the	O
network	O
properties	O
.	O
another	O
generalization	B
of	O
the	O
network	O
architecture	O
is	O
to	O
include	O
skip-layer	O
con-	O
nections	O
,	O
each	O
of	O
which	O
is	O
associated	O
with	O
a	O
corresponding	O
adaptive	O
parameter	O
.	O
for	O
230	O
5.	O
neural	O
networks	O
figure	O
5.2	O
example	O
of	O
a	O
neural	B
network	I
having	O
a	O
general	O
feed-forward	O
topology	O
.	O
note	O
that	O
each	O
hidden	O
and	O
output	O
unit	O
has	O
an	O
associated	O
bias	B
parameter	I
(	O
omitted	O
for	O
clarity	O
)	O
.	O
x2	O
inputs	O
x1	O
z1	O
z2	O
z3	O
y2	O
outputs	O
y1	O
instance	O
,	O
in	O
a	O
two-layer	O
network	O
these	O
would	O
go	O
directly	O
from	O
inputs	O
to	O
outputs	O
.	O
in	O
principle	O
,	O
a	O
network	O
with	O
sigmoidal	O
hidden	O
units	O
can	O
always	O
mimic	O
skip	O
layer	O
con-	O
nections	O
(	O
for	O
bounded	O
input	O
values	O
)	O
by	O
using	O
a	O
sufﬁciently	O
small	O
ﬁrst-layer	O
weight	O
that	O
,	O
over	O
its	O
operating	O
range	O
,	O
the	O
hidden	B
unit	I
is	O
effectively	O
linear	O
,	O
and	O
then	O
com-	O
pensating	O
with	O
a	O
large	O
weight	O
value	O
from	O
the	O
hidden	B
unit	I
to	O
the	O
output	O
.	O
in	O
practice	O
,	O
however	O
,	O
it	O
may	O
be	O
advantageous	O
to	O
include	O
skip-layer	O
connections	O
explicitly	O
.	O
furthermore	O
,	O
the	O
network	O
can	O
be	O
sparse	O
,	O
with	O
not	O
all	O
possible	O
connections	O
within	O
a	O
layer	O
being	O
present	O
.	O
we	O
shall	O
see	O
an	O
example	O
of	O
a	O
sparse	O
network	O
architecture	O
when	O
we	O
consider	O
convolutional	O
neural	O
networks	O
in	O
section	O
5.5.6.	O
because	O
there	O
is	O
a	O
direct	O
correspondence	O
between	O
a	O
network	O
diagram	O
and	O
its	O
mathematical	O
function	O
,	O
we	O
can	O
develop	O
more	O
general	O
network	O
mappings	O
by	O
con-	O
sidering	O
more	O
complex	O
network	O
diagrams	O
.	O
however	O
,	O
these	O
must	O
be	O
restricted	O
to	O
a	O
feed-forward	O
architecture	O
,	O
in	O
other	O
words	O
to	O
one	O
having	O
no	O
closed	O
directed	B
cycles	O
,	O
to	O
ensure	O
that	O
the	O
outputs	O
are	O
deterministic	O
functions	O
of	O
the	O
inputs	O
.	O
this	O
is	O
illustrated	O
with	O
a	O
simple	O
example	O
in	O
figure	O
5.2.	O
each	O
(	O
hidden	O
or	O
output	O
)	O
unit	O
in	O
such	O
a	O
network	O
computes	O
a	O
function	O
given	O
by	O
(	O
cid:22	O
)	O
(	O
cid:2	O
)	O
(	O
cid:23	O
)	O
zk	O
=	O
h	O
wkjzj	O
j	O
(	O
5.10	O
)	O
where	O
the	O
sum	O
runs	O
over	O
all	O
units	O
that	O
send	O
connections	O
to	O
unit	O
k	O
(	O
and	O
a	O
bias	B
param-	O
eter	O
is	O
included	O
in	O
the	O
summation	O
)	O
.	O
for	O
a	O
given	O
set	O
of	O
values	O
applied	O
to	O
the	O
inputs	O
of	O
the	O
network	O
,	O
successive	O
application	O
of	O
(	O
5.10	O
)	O
allows	O
the	O
activations	O
of	O
all	O
units	O
in	O
the	O
network	O
to	O
be	O
evaluated	O
including	O
those	O
of	O
the	O
output	O
units	O
.	O
the	O
approximation	O
properties	O
of	O
feed-forward	O
networks	O
have	O
been	O
widely	O
stud-	O
ied	O
(	O
funahashi	O
,	O
1989	O
;	O
cybenko	O
,	O
1989	O
;	O
hornik	O
et	O
al.	O
,	O
1989	O
;	O
stinchecombe	O
and	O
white	O
,	O
1989	O
;	O
cotter	O
,	O
1990	O
;	O
ito	O
,	O
1991	O
;	O
hornik	O
,	O
1991	O
;	O
kreinovich	O
,	O
1991	O
;	O
ripley	O
,	O
1996	O
)	O
and	O
found	O
to	O
be	O
very	O
general	O
.	O
neural	O
networks	O
are	O
therefore	O
said	O
to	O
be	O
universal	O
ap-	O
proximators	O
.	O
for	O
example	O
,	O
a	O
two-layer	O
network	O
with	O
linear	O
outputs	O
can	O
uniformly	O
approximate	O
any	O
continuous	O
function	O
on	O
a	O
compact	O
input	O
domain	O
to	O
arbitrary	O
accu-	O
racy	O
provided	O
the	O
network	O
has	O
a	O
sufﬁciently	O
large	O
number	O
of	O
hidden	O
units	O
.	O
this	O
result	O
holds	O
for	O
a	O
wide	O
range	O
of	O
hidden	B
unit	I
activation	O
functions	O
,	O
but	O
excluding	O
polynomi-	O
als	O
.	O
although	O
such	O
theorems	O
are	O
reassuring	O
,	O
the	O
key	O
problem	O
is	O
how	O
to	O
ﬁnd	O
suitable	O
parameter	O
values	O
given	O
a	O
set	O
of	O
training	B
data	O
,	O
and	O
in	O
later	O
sections	O
of	O
this	O
chapter	O
we	O
figure	O
5.3	O
illustration	O
of	O
the	O
ca-	O
pability	O
of	O
a	O
multilayer	B
perceptron	I
to	O
approximate	O
four	O
different	O
func-	O
tions	O
comprising	O
(	O
a	O
)	O
f	O
(	O
x	O
)	O
=	O
x2	O
,	O
(	O
b	O
)	O
f	O
(	O
x	O
)	O
=	O
sin	O
(	O
x	O
)	O
,	O
(	O
c	O
)	O
,	O
f	O
(	O
x	O
)	O
=	O
|x|	O
,	O
and	O
(	O
d	O
)	O
f	O
(	O
x	O
)	O
=	O
h	O
(	O
x	O
)	O
where	O
h	O
(	O
x	O
)	O
is	O
the	O
heaviside	O
step	O
function	O
.	O
in	O
each	O
case	O
,	O
n	O
=	O
50	O
data	O
points	O
,	O
shown	O
as	O
blue	O
dots	O
,	O
have	O
been	O
sam-	O
pled	O
uniformly	O
in	O
x	O
over	O
the	O
interval	O
(	O
−1	O
,	O
1	O
)	O
and	O
the	O
corresponding	O
val-	O
ues	O
of	O
f	O
(	O
x	O
)	O
evaluated	O
.	O
these	O
data	O
points	O
are	O
then	O
used	O
to	O
train	O
a	O
two-	O
layer	O
network	O
having	O
3	O
hidden	O
units	O
with	O
‘	O
tanh	O
’	O
activation	O
functions	O
and	O
linear	O
output	O
units	O
.	O
the	O
resulting	O
network	O
functions	O
are	O
shown	O
by	O
the	O
red	O
curves	O
,	O
and	O
the	O
outputs	O
of	O
the	O
three	O
hidden	O
units	O
are	O
shown	O
by	O
the	O
three	O
dashed	O
curves	O
.	O
5.1.	O
feed-forward	O
network	O
functions	O
231	O
(	O
a	O
)	O
(	O
c	O
)	O
(	O
b	O
)	O
(	O
d	O
)	O
will	O
show	O
that	O
there	O
exist	O
effective	O
solutions	O
to	O
this	O
problem	O
based	O
on	O
both	O
maximum	B
likelihood	I
and	O
bayesian	O
approaches	O
.	O
the	O
capability	O
of	O
a	O
two-layer	O
network	O
to	O
model	O
a	O
broad	O
range	O
of	O
functions	O
is	O
illustrated	O
in	O
figure	O
5.3.	O
this	O
ﬁgure	O
also	O
shows	O
how	O
individual	O
hidden	O
units	O
work	O
collaboratively	O
to	O
approximate	O
the	O
ﬁnal	O
function	O
.	O
the	O
role	O
of	O
hidden	O
units	O
in	O
a	O
simple	O
classiﬁcation	B
problem	O
is	O
illustrated	O
in	O
figure	O
5.4	O
using	O
the	O
synthetic	O
classiﬁcation	O
data	O
set	O
described	O
in	O
appendix	O
a	O
.	O
5.1.1	O
weight-space	O
symmetries	O
one	O
property	O
of	O
feed-forward	O
networks	O
,	O
which	O
will	O
play	O
a	O
role	O
when	O
we	O
consider	O
bayesian	O
model	B
comparison	I
,	O
is	O
that	O
multiple	O
distinct	O
choices	O
for	O
the	O
weight	B
vector	I
w	O
can	O
all	O
give	O
rise	O
to	O
the	O
same	O
mapping	O
function	O
from	O
inputs	O
to	O
outputs	O
(	O
chen	O
et	O
al.	O
,	O
1993	O
)	O
.	O
consider	O
a	O
two-layer	O
network	O
of	O
the	O
form	O
shown	O
in	O
figure	O
5.1	O
with	O
m	O
hidden	O
units	O
having	O
‘	O
tanh	O
’	O
activation	O
functions	O
and	O
full	O
connectivity	O
in	O
both	O
layers	O
.	O
if	O
we	O
change	O
the	O
sign	O
of	O
all	O
of	O
the	O
weights	O
and	O
the	O
bias	B
feeding	O
into	O
a	O
particular	O
hidden	B
unit	I
,	O
then	O
,	O
for	O
a	O
given	O
input	O
pattern	O
,	O
the	O
sign	O
of	O
the	O
activation	O
of	O
the	O
hidden	B
unit	I
will	O
be	O
reversed	O
,	O
because	O
‘	O
tanh	O
’	O
is	O
an	O
odd	O
function	O
,	O
so	O
that	O
tanh	O
(	O
−a	O
)	O
=	O
−	O
tanh	O
(	O
a	O
)	O
.	O
this	O
transformation	O
can	O
be	O
exactly	O
compensated	O
by	O
changing	O
the	O
sign	O
of	O
all	O
of	O
the	O
weights	O
leading	O
out	O
of	O
that	O
hidden	B
unit	I
.	O
thus	O
,	O
by	O
changing	O
the	O
signs	O
of	O
a	O
particular	O
group	O
of	O
weights	O
(	O
and	O
a	O
bias	B
)	O
,	O
the	O
input–output	O
mapping	O
function	O
represented	O
by	O
the	O
network	O
is	O
unchanged	O
,	O
and	O
so	O
we	O
have	O
found	O
two	O
different	O
weight	O
vectors	O
that	O
give	O
rise	O
to	O
the	O
same	O
mapping	O
function	O
.	O
for	O
m	O
hidden	O
units	O
,	O
there	O
will	O
be	O
m	O
such	O
‘	O
sign-ﬂip	O
’	O
232	O
5.	O
neural	O
networks	O
figure	O
5.4	O
example	O
of	O
the	O
solution	O
of	O
a	O
simple	O
two-	O
class	O
classiﬁcation	B
problem	O
involving	O
synthetic	O
data	O
using	O
a	O
neural	B
network	I
having	O
two	O
inputs	O
,	O
two	O
hidden	O
units	O
with	O
‘	O
tanh	O
’	O
activation	O
functions	O
,	O
and	O
a	O
single	O
output	O
having	O
a	O
logistic	B
sigmoid	I
activa-	O
tion	O
function	O
.	O
the	O
dashed	O
blue	O
lines	O
show	O
the	O
z	O
=	O
0.5	O
contours	O
for	O
each	O
of	O
the	O
hidden	O
units	O
,	O
and	O
the	O
red	O
line	O
shows	O
the	O
y	O
=	O
0.5	O
decision	B
surface	I
for	O
the	O
net-	O
work	O
.	O
for	O
comparison	O
,	O
the	O
green	O
line	O
denotes	O
the	O
optimal	O
decision	B
boundary	I
computed	O
from	O
the	O
distributions	O
used	O
to	O
generate	O
the	O
data	O
.	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−2	O
−1	O
0	O
1	O
2	O
symmetries	B
,	O
and	O
thus	O
any	O
given	O
weight	B
vector	I
will	O
be	O
one	O
of	O
a	O
set	O
2m	O
equivalent	O
weight	O
vectors	O
.	O
similarly	O
,	O
imagine	O
that	O
we	O
interchange	O
the	O
values	O
of	O
all	O
of	O
the	O
weights	O
(	O
and	O
the	O
bias	B
)	O
leading	O
both	O
into	O
and	O
out	O
of	O
a	O
particular	O
hidden	B
unit	I
with	O
the	O
corresponding	O
values	O
of	O
the	O
weights	O
(	O
and	O
bias	B
)	O
associated	O
with	O
a	O
different	O
hidden	B
unit	I
.	O
again	O
,	O
this	O
clearly	O
leaves	O
the	O
network	O
input–output	O
mapping	O
function	O
unchanged	O
,	O
but	O
it	O
corre-	O
sponds	O
to	O
a	O
different	O
choice	O
of	O
weight	B
vector	I
.	O
for	O
m	O
hidden	O
units	O
,	O
any	O
given	O
weight	B
vector	I
will	O
belong	O
to	O
a	O
set	O
of	O
m	O
!	O
equivalent	O
weight	O
vectors	O
associated	O
with	O
this	O
inter-	O
change	O
symmetry	O
,	O
corresponding	O
to	O
the	O
m	O
!	O
different	O
orderings	O
of	O
the	O
hidden	O
units	O
.	O
the	O
network	O
will	O
therefore	O
have	O
an	O
overall	O
weight-space	B
symmetry	I
factor	O
of	O
m	O
!	O
2m	O
.	O
for	O
networks	O
with	O
more	O
than	O
two	O
layers	O
of	O
weights	O
,	O
the	O
total	O
level	O
of	O
symmetry	O
will	O
be	O
given	O
by	O
the	O
product	O
of	O
such	O
factors	O
,	O
one	O
for	O
each	O
layer	O
of	O
hidden	O
units	O
.	O
it	O
turns	O
out	O
that	O
these	O
factors	O
account	O
for	O
all	O
of	O
the	O
symmetries	B
in	O
weight	O
space	O
(	O
except	O
for	O
possible	O
accidental	O
symmetries	B
due	O
to	O
speciﬁc	O
choices	O
for	O
the	O
weight	O
val-	O
ues	O
)	O
.	O
furthermore	O
,	O
the	O
existence	O
of	O
these	O
symmetries	B
is	O
not	O
a	O
particular	O
property	O
of	O
the	O
‘	O
tanh	O
’	O
function	O
but	O
applies	O
to	O
a	O
wide	O
range	O
of	O
activation	O
functions	O
(	O
k˙urkov´a	O
and	O
kainen	O
,	O
1994	O
)	O
.	O
in	O
many	O
cases	O
,	O
these	O
symmetries	B
in	O
weight	O
space	O
are	O
of	O
little	O
practi-	O
cal	O
consequence	O
,	O
although	O
in	O
section	O
5.7	O
we	O
shall	O
encounter	O
a	O
situation	O
in	O
which	O
we	O
need	O
to	O
take	O
them	O
into	O
account	O
.	O
5.2.	O
network	O
training	B
so	O
far	O
,	O
we	O
have	O
viewed	O
neural	O
networks	O
as	O
a	O
general	O
class	O
of	O
parametric	O
nonlinear	O
functions	O
from	O
a	O
vector	O
x	O
of	O
input	O
variables	O
to	O
a	O
vector	O
y	O
of	O
output	O
variables	O
.	O
a	O
simple	O
approach	O
to	O
the	O
problem	O
of	O
determining	O
the	O
network	O
parameters	O
is	O
to	O
make	O
an	O
analogy	O
with	O
the	O
discussion	O
of	O
polynomial	B
curve	I
ﬁtting	I
in	O
section	O
1.1	O
,	O
and	O
therefore	O
to	O
minimize	O
a	O
sum-of-squares	B
error	I
function	O
.	O
given	O
a	O
training	B
set	I
comprising	O
a	O
set	O
of	O
input	O
vectors	O
{	O
xn	O
}	O
,	O
where	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
together	O
with	O
a	O
corresponding	O
set	O
of	O
5.2.	O
network	O
training	B
233	O
n	O
(	O
cid:2	O
)	O
n=1	O
target	O
vectors	O
{	O
tn	O
}	O
,	O
we	O
minimize	O
the	O
error	B
function	I
e	O
(	O
w	O
)	O
=	O
1	O
2	O
(	O
cid:5	O
)	O
y	O
(	O
xn	O
,	O
w	O
)	O
−	O
tn	O
(	O
cid:5	O
)	O
2	O
.	O
(	O
5.11	O
)	O
however	O
,	O
we	O
can	O
provide	O
a	O
much	O
more	O
general	O
view	O
of	O
network	O
training	B
by	O
ﬁrst	O
giving	O
a	O
probabilistic	O
interpretation	O
to	O
the	O
network	O
outputs	O
.	O
we	O
have	O
already	O
seen	O
many	O
advantages	O
of	O
using	O
probabilistic	O
predictions	O
in	O
section	O
1.5.4.	O
here	O
it	O
will	O
also	O
provide	O
us	O
with	O
a	O
clearer	O
motivation	O
both	O
for	O
the	O
choice	O
of	O
output	O
unit	O
nonlinearity	O
and	O
the	O
choice	O
of	O
error	B
function	I
.	O
we	O
start	O
by	O
discussing	O
regression	B
problems	O
,	O
and	O
for	O
the	O
moment	O
we	O
consider	O
a	O
single	O
target	O
variable	O
t	O
that	O
can	O
take	O
any	O
real	O
value	O
.	O
following	O
the	O
discussions	O
in	O
section	O
1.2.5	O
and	O
3.1	O
,	O
we	O
assume	O
that	O
t	O
has	O
a	O
gaussian	O
distribution	O
with	O
an	O
x-	O
dependent	O
mean	B
,	O
which	O
is	O
given	O
by	O
the	O
output	O
of	O
the	O
neural	B
network	I
,	O
so	O
that	O
p	O
(	O
t|x	O
,	O
w	O
)	O
=	O
n	O
(	O
cid:10	O
)	O
t|y	O
(	O
x	O
,	O
w	O
)	O
,	O
β	O
−1	O
(	O
cid:11	O
)	O
(	O
5.12	O
)	O
where	O
β	O
is	O
the	O
precision	O
(	O
inverse	B
variance	O
)	O
of	O
the	O
gaussian	O
noise	O
.	O
of	O
course	O
this	O
is	O
a	O
somewhat	O
restrictive	O
assumption	O
,	O
and	O
in	O
section	O
5.6	O
we	O
shall	O
see	O
how	O
to	O
extend	O
this	O
approach	O
to	O
allow	O
for	O
more	O
general	O
conditional	B
distributions	O
.	O
for	O
the	O
conditional	B
distribution	O
given	O
by	O
(	O
5.12	O
)	O
,	O
it	O
is	O
sufﬁcient	O
to	O
take	O
the	O
output	O
unit	O
activation	B
function	I
to	O
be	O
the	O
identity	O
,	O
because	O
such	O
a	O
network	O
can	O
approximate	O
any	O
continuous	O
function	O
from	O
x	O
to	O
y.	O
given	O
a	O
data	O
set	O
of	O
n	O
independent	B
,	O
identically	O
distributed	O
observations	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
,	O
along	O
with	O
corresponding	O
target	O
values	O
t	O
=	O
{	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
}	O
,	O
we	O
can	O
construct	O
the	O
corresponding	O
likelihood	B
function	I
n	O
(	O
cid:14	O
)	O
n=1	O
p	O
(	O
tn|xn	O
,	O
w	O
,	O
β	O
)	O
.	O
p	O
(	O
t|x	O
,	O
w	O
,	O
β	O
)	O
=	O
n	O
(	O
cid:2	O
)	O
taking	O
the	O
negative	O
logarithm	O
,	O
we	O
obtain	O
the	O
error	B
function	I
β	O
2	O
n=1	O
{	O
y	O
(	O
xn	O
,	O
w	O
)	O
−	O
tn	O
}	O
2	O
−	O
n	O
2	O
ln	O
β	O
+	O
n	O
2	O
ln	O
(	O
2π	O
)	O
(	O
5.13	O
)	O
which	O
can	O
be	O
used	O
to	O
learn	O
the	O
parameters	O
w	O
and	O
β.	O
in	O
section	O
5.7	O
,	O
we	O
shall	O
dis-	O
cuss	O
the	O
bayesian	O
treatment	O
of	O
neural	O
networks	O
,	O
while	O
here	O
we	O
consider	O
a	O
maximum	B
likelihood	I
approach	O
.	O
note	O
that	O
in	O
the	O
neural	O
networks	O
literature	O
,	O
it	O
is	O
usual	O
to	O
con-	O
sider	O
the	O
minimization	O
of	O
an	O
error	B
function	I
rather	O
than	O
the	O
maximization	O
of	O
the	O
(	O
log	O
)	O
likelihood	O
,	O
and	O
so	O
here	O
we	O
shall	O
follow	O
this	O
convention	O
.	O
consider	O
ﬁrst	O
the	O
determi-	O
nation	O
of	O
w.	O
maximizing	O
the	O
likelihood	B
function	I
is	O
equivalent	O
to	O
minimizing	O
the	O
sum-of-squares	B
error	I
function	O
given	O
by	O
e	O
(	O
w	O
)	O
=	O
1	O
2	O
{	O
y	O
(	O
xn	O
,	O
w	O
)	O
−	O
tn	O
}	O
2	O
(	O
5.14	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
234	O
5.	O
neural	O
networks	O
where	O
we	O
have	O
discarded	O
additive	O
and	O
multiplicative	O
constants	O
.	O
the	O
value	O
of	O
w	O
found	O
by	O
minimizing	O
e	O
(	O
w	O
)	O
will	O
be	O
denoted	O
wml	O
because	O
it	O
corresponds	O
to	O
the	O
maximum	B
likelihood	I
solution	O
.	O
in	O
practice	O
,	O
the	O
nonlinearity	O
of	O
the	O
network	O
function	O
y	O
(	O
xn	O
,	O
w	O
)	O
causes	O
the	O
error	B
e	O
(	O
w	O
)	O
to	O
be	O
nonconvex	O
,	O
and	O
so	O
in	O
practice	O
local	B
maxima	O
of	O
the	O
likelihood	O
may	O
be	O
found	O
,	O
corresponding	O
to	O
local	B
minima	O
of	O
the	O
error	B
function	I
,	O
as	O
discussed	O
in	O
section	O
5.2.1.	O
having	O
found	O
wml	O
,	O
the	O
value	O
of	O
β	O
can	O
be	O
found	O
by	O
minimizing	O
the	O
negative	O
log	O
likelihood	O
to	O
give	O
1	O
βml	O
=	O
1	O
n	O
n	O
(	O
cid:2	O
)	O
n=1	O
{	O
y	O
(	O
xn	O
,	O
wml	O
)	O
−	O
tn	O
}	O
2	O
.	O
(	O
5.15	O
)	O
note	O
that	O
this	O
can	O
be	O
evaluated	O
once	O
the	O
iterative	O
optimization	O
required	O
to	O
ﬁnd	O
wml	O
is	O
completed	O
.	O
if	O
we	O
have	O
multiple	O
target	O
variables	O
,	O
and	O
we	O
assume	O
that	O
they	O
are	O
inde-	O
pendent	O
conditional	B
on	O
x	O
and	O
w	O
with	O
shared	O
noise	O
precision	O
β	O
,	O
then	O
the	O
conditional	B
distribution	O
of	O
the	O
target	O
values	O
is	O
given	O
by	O
(	O
cid:11	O
)	O
t|y	O
(	O
x	O
,	O
w	O
)	O
,	O
β	O
−1i	O
.	O
(	O
5.16	O
)	O
exercise	O
5.2	O
following	O
the	O
same	O
argument	O
as	O
for	O
a	O
single	O
target	O
variable	O
,	O
we	O
see	O
that	O
the	O
maximum	B
likelihood	I
weights	O
are	O
determined	O
by	O
minimizing	O
the	O
sum-of-squares	B
error	I
function	O
(	O
5.11	O
)	O
.	O
the	O
noise	O
precision	O
is	O
then	O
given	O
by	O
p	O
(	O
t|x	O
,	O
w	O
)	O
=	O
n	O
(	O
cid:10	O
)	O
n	O
(	O
cid:2	O
)	O
1	O
βml	O
=	O
1	O
n	O
k	O
n=1	O
(	O
cid:5	O
)	O
y	O
(	O
xn	O
,	O
wml	O
)	O
−	O
tn	O
(	O
cid:5	O
)	O
2	O
(	O
5.17	O
)	O
exercise	O
5.3	O
where	O
k	O
is	O
the	O
number	O
of	O
target	O
variables	O
.	O
the	O
assumption	O
of	O
independence	O
can	O
be	O
dropped	O
at	O
the	O
expense	O
of	O
a	O
slightly	O
more	O
complex	O
optimization	O
problem	O
.	O
recall	O
from	O
section	O
4.3.6	O
that	O
there	O
is	O
a	O
natural	O
pairing	O
of	O
the	O
error	B
function	I
(	O
given	O
by	O
the	O
negative	O
log	O
likelihood	O
)	O
and	O
the	O
output	O
unit	O
activation	B
function	I
.	O
in	O
the	O
regression	B
case	O
,	O
we	O
can	O
view	O
the	O
network	O
as	O
having	O
an	O
output	O
activation	B
function	I
that	O
is	O
the	O
identity	O
,	O
so	O
that	O
yk	O
=	O
ak	O
.	O
the	O
corresponding	O
sum-of-squares	B
error	I
function	O
has	O
the	O
property	O
=	O
yk	O
−	O
tk	O
∂e	O
∂ak	O
(	O
5.18	O
)	O
which	O
we	O
shall	O
make	O
use	O
of	O
when	O
discussing	O
error	B
backpropagation	I
in	O
section	O
5.3.	O
now	O
consider	O
the	O
case	O
of	O
binary	O
classiﬁcation	O
in	O
which	O
we	O
have	O
a	O
single	O
target	O
variable	O
t	O
such	O
that	O
t	O
=	O
1	O
denotes	O
class	O
c1	O
and	O
t	O
=	O
0	O
denotes	O
class	O
c2	O
.	O
following	O
the	O
discussion	O
of	O
canonical	O
link	O
functions	O
in	O
section	O
4.3.6	O
,	O
we	O
consider	O
a	O
network	O
having	O
a	O
single	O
output	O
whose	O
activation	B
function	I
is	O
a	O
logistic	B
sigmoid	I
y	O
=	O
σ	O
(	O
a	O
)	O
≡	O
1	O
(	O
5.19	O
)	O
so	O
that	O
0	O
(	O
cid:1	O
)	O
y	O
(	O
x	O
,	O
w	O
)	O
(	O
cid:1	O
)	O
1.	O
we	O
can	O
interpret	O
y	O
(	O
x	O
,	O
w	O
)	O
as	O
the	O
conditional	B
probability	I
p	O
(	O
c1|x	O
)	O
,	O
with	O
p	O
(	O
c2|x	O
)	O
given	O
by	O
1	O
−	O
y	O
(	O
x	O
,	O
w	O
)	O
.	O
the	O
conditional	B
distribution	O
of	O
targets	O
given	O
inputs	O
is	O
then	O
a	O
bernoulli	O
distribution	O
of	O
the	O
form	O
1	O
+	O
exp	O
(	O
−a	O
)	O
p	O
(	O
t|x	O
,	O
w	O
)	O
=	O
y	O
(	O
x	O
,	O
w	O
)	O
t	O
{	O
1	O
−	O
y	O
(	O
x	O
,	O
w	O
)	O
}	O
1−t	O
.	O
(	O
5.20	O
)	O
5.2.	O
network	O
training	B
235	O
if	O
we	O
consider	O
a	O
training	B
set	I
of	O
independent	B
observations	O
,	O
then	O
the	O
error	B
function	I
,	O
which	O
is	O
given	O
by	O
the	O
negative	O
log	O
likelihood	O
,	O
is	O
then	O
a	O
cross-entropy	B
error	I
function	I
of	O
the	O
form	O
{	O
tn	O
ln	O
yn	O
+	O
(	O
1	O
−	O
tn	O
)	O
ln	O
(	O
1	O
−	O
yn	O
)	O
}	O
(	O
5.21	O
)	O
e	O
(	O
w	O
)	O
=	O
−	O
n	O
(	O
cid:2	O
)	O
exercise	O
5.4	O
n=1	O
where	O
yn	O
denotes	O
y	O
(	O
xn	O
,	O
w	O
)	O
.	O
note	O
that	O
there	O
is	O
no	O
analogue	O
of	O
the	O
noise	O
precision	O
β	O
because	O
the	O
target	O
values	O
are	O
assumed	O
to	O
be	O
correctly	O
labelled	O
.	O
however	O
,	O
the	O
model	O
is	O
easily	O
extended	B
to	O
allow	O
for	O
labelling	O
errors	O
.	O
simard	O
et	O
al	O
.	O
(	O
2003	O
)	O
found	O
that	O
using	O
the	O
cross-entropy	B
error	I
function	I
instead	O
of	O
the	O
sum-of-squares	O
for	O
a	O
classiﬁcation	B
problem	O
leads	O
to	O
faster	O
training	B
as	O
well	O
as	O
improved	O
generalization	B
.	O
if	O
we	O
have	O
k	O
separate	O
binary	O
classiﬁcations	O
to	O
perform	O
,	O
then	O
we	O
can	O
use	O
a	O
net-	O
work	O
having	O
k	O
outputs	O
each	O
of	O
which	O
has	O
a	O
logistic	B
sigmoid	I
activation	O
function	O
.	O
associated	O
with	O
each	O
output	O
is	O
a	O
binary	O
class	O
label	O
tk	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
where	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k.	O
if	O
we	O
assume	O
that	O
the	O
class	O
labels	O
are	O
independent	B
,	O
given	O
the	O
input	O
vector	O
,	O
then	O
the	O
conditional	B
distribution	O
of	O
the	O
targets	O
is	O
p	O
(	O
t|x	O
,	O
w	O
)	O
=	O
yk	O
(	O
x	O
,	O
w	O
)	O
tk	O
[	O
1	O
−	O
yk	O
(	O
x	O
,	O
w	O
)	O
]	O
1−tk	O
.	O
(	O
5.22	O
)	O
k	O
(	O
cid:14	O
)	O
k=1	O
exercise	O
5.5	O
exercise	O
5.6	O
e	O
(	O
w	O
)	O
=	O
−	O
n	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
taking	O
the	O
negative	O
logarithm	O
of	O
the	O
corresponding	O
likelihood	B
function	I
then	O
gives	O
the	O
following	O
error	B
function	I
{	O
tnk	O
ln	O
ynk	O
+	O
(	O
1	O
−	O
tnk	O
)	O
ln	O
(	O
1	O
−	O
ynk	O
)	O
}	O
(	O
5.23	O
)	O
n=1	O
k=1	O
where	O
ynk	O
denotes	O
yk	O
(	O
xn	O
,	O
w	O
)	O
.	O
again	O
,	O
the	O
derivative	B
of	O
the	O
error	B
function	I
with	O
re-	O
spect	O
to	O
the	O
activation	O
for	O
a	O
particular	O
output	O
unit	O
takes	O
the	O
form	O
(	O
5.18	O
)	O
just	O
as	O
in	O
the	O
regression	B
case	O
.	O
it	O
is	O
interesting	O
to	O
contrast	O
the	O
neural	B
network	I
solution	O
to	O
this	O
problem	O
with	O
the	O
corresponding	O
approach	O
based	O
on	O
a	O
linear	O
classiﬁcation	O
model	O
of	O
the	O
kind	O
discussed	O
in	O
chapter	O
4.	O
suppose	O
that	O
we	O
are	O
using	O
a	O
standard	O
two-layer	O
network	O
of	O
the	O
kind	O
shown	O
in	O
figure	O
5.1.	O
we	O
see	O
that	O
the	O
weight	O
parameters	O
in	O
the	O
ﬁrst	O
layer	O
of	O
the	O
network	O
are	O
shared	O
between	O
the	O
various	O
outputs	O
,	O
whereas	O
in	O
the	O
linear	O
model	O
each	O
classiﬁcation	B
problem	O
is	O
solved	O
independently	O
.	O
the	O
ﬁrst	O
layer	O
of	O
the	O
network	O
can	O
be	O
viewed	O
as	O
performing	O
a	O
nonlinear	O
feature	B
extraction	I
,	O
and	O
the	O
sharing	O
of	O
features	O
between	O
the	O
different	O
outputs	O
can	O
save	O
on	O
computation	O
and	O
can	O
also	O
lead	O
to	O
improved	O
generalization	B
.	O
finally	O
,	O
we	O
consider	O
the	O
standard	O
multiclass	O
classiﬁcation	B
problem	O
in	O
which	O
each	O
input	O
is	O
assigned	O
to	O
one	O
of	O
k	O
mutually	O
exclusive	O
classes	O
.	O
the	O
binary	O
target	O
variables	O
tk	O
∈	O
{	O
0	O
,	O
1	O
}	O
have	O
a	O
1-of-k	O
coding	O
scheme	O
indicating	O
the	O
class	O
,	O
and	O
the	O
network	O
outputs	O
are	O
interpreted	O
as	O
yk	O
(	O
x	O
,	O
w	O
)	O
=	O
p	O
(	O
tk	O
=	O
1|x	O
)	O
,	O
leading	O
to	O
the	O
following	O
error	B
function	I
e	O
(	O
w	O
)	O
=	O
−	O
n	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
n=1	O
k=1	O
tkn	O
ln	O
yk	O
(	O
xn	O
,	O
w	O
)	O
.	O
(	O
5.24	O
)	O
236	O
5.	O
neural	O
networks	O
figure	O
5.5	O
geometrical	O
view	O
of	O
the	O
error	B
function	I
e	O
(	O
w	O
)	O
as	O
a	O
surface	O
sitting	O
over	O
weight	O
space	O
.	O
point	O
wa	O
is	O
a	O
local	B
minimum	I
and	O
wb	O
is	O
the	O
global	B
minimum	I
.	O
at	O
any	O
point	O
wc	O
,	O
the	O
local	B
gradient	O
of	O
the	O
error	B
surface	O
is	O
given	O
by	O
the	O
vector	O
∇e	O
.	O
e	O
(	O
w	O
)	O
wa	O
wb	O
w2	O
w1	O
wc	O
∇e	O
following	O
the	O
discussion	O
of	O
section	O
4.3.4	O
,	O
we	O
see	O
that	O
the	O
output	O
unit	O
activation	B
function	I
,	O
which	O
corresponds	O
to	O
the	O
canonical	O
link	O
,	O
is	O
given	O
by	O
the	O
softmax	B
function	I
(	O
5.25	O
)	O
(	O
cid:2	O
)	O
j	O
exp	O
(	O
ak	O
(	O
x	O
,	O
w	O
)	O
)	O
exp	O
(	O
aj	O
(	O
x	O
,	O
w	O
)	O
)	O
yk	O
(	O
x	O
,	O
w	O
)	O
=	O
(	O
cid:5	O
)	O
which	O
satisﬁes	O
0	O
(	O
cid:1	O
)	O
yk	O
(	O
cid:1	O
)	O
1	O
and	O
k	O
yk	O
=	O
1.	O
note	O
that	O
the	O
yk	O
(	O
x	O
,	O
w	O
)	O
are	O
unchanged	O
if	O
a	O
constant	O
is	O
added	O
to	O
all	O
of	O
the	O
ak	O
(	O
x	O
,	O
w	O
)	O
,	O
causing	O
the	O
error	B
function	I
to	O
be	O
constant	O
for	O
some	O
directions	O
in	O
weight	O
space	O
.	O
this	O
degeneracy	O
is	O
removed	O
if	O
an	O
appropriate	O
regularization	B
term	O
(	O
section	O
5.5	O
)	O
is	O
added	O
to	O
the	O
error	B
function	I
.	O
exercise	O
5.7	O
a	O
particular	O
output	O
unit	O
takes	O
the	O
familiar	O
form	O
(	O
5.18	O
)	O
.	O
once	O
again	O
,	O
the	O
derivative	B
of	O
the	O
error	B
function	I
with	O
respect	O
to	O
the	O
activation	O
for	O
in	O
summary	O
,	O
there	O
is	O
a	O
natural	O
choice	O
of	O
both	O
output	O
unit	O
activation	B
function	I
and	O
matching	O
error	B
function	I
,	O
according	O
to	O
the	O
type	O
of	O
problem	O
being	O
solved	O
.	O
for	O
re-	O
gression	O
we	O
use	O
linear	O
outputs	O
and	O
a	O
sum-of-squares	B
error	I
,	O
for	O
(	O
multiple	O
independent	B
)	O
binary	O
classiﬁcations	O
we	O
use	O
logistic	B
sigmoid	I
outputs	O
and	O
a	O
cross-entropy	O
error	O
func-	O
tion	O
,	O
and	O
for	O
multiclass	O
classiﬁcation	B
we	O
use	O
softmax	O
outputs	O
with	O
the	O
corresponding	O
multiclass	B
cross-entropy	O
error	B
function	I
.	O
for	O
classiﬁcation	O
problems	O
involving	O
two	O
classes	O
,	O
we	O
can	O
use	O
a	O
single	O
logistic	B
sigmoid	I
output	O
,	O
or	O
alternatively	O
we	O
can	O
use	O
a	O
network	O
with	O
two	O
outputs	O
having	O
a	O
softmax	O
output	O
activation	B
function	I
.	O
5.2.1	O
parameter	O
optimization	O
we	O
turn	O
next	O
to	O
the	O
task	O
of	O
ﬁnding	O
a	O
weight	B
vector	I
w	O
which	O
minimizes	O
the	O
chosen	O
function	O
e	O
(	O
w	O
)	O
.	O
at	O
this	O
point	O
,	O
it	O
is	O
useful	O
to	O
have	O
a	O
geometrical	O
picture	O
of	O
the	O
error	B
function	I
,	O
which	O
we	O
can	O
view	O
as	O
a	O
surface	O
sitting	O
over	O
weight	O
space	O
as	O
shown	O
in	O
figure	O
5.5.	O
first	O
note	O
that	O
if	O
we	O
make	O
a	O
small	O
step	O
in	O
weight	O
space	O
from	O
w	O
to	O
w+	O
δw	O
then	O
the	O
change	O
in	O
the	O
error	B
function	I
is	O
δe	O
(	O
cid:7	O
)	O
δwt∇e	O
(	O
w	O
)	O
,	O
where	O
the	O
vector	O
∇e	O
(	O
w	O
)	O
points	O
in	O
the	O
direction	O
of	O
greatest	O
rate	O
of	O
increase	O
of	O
the	O
error	B
function	I
.	O
because	O
the	O
error	B
e	O
(	O
w	O
)	O
is	O
a	O
smooth	O
continuous	O
function	O
of	O
w	O
,	O
its	O
smallest	O
value	O
will	O
occur	O
at	O
a	O
section	O
5.1.1	O
point	O
in	O
weight	O
space	O
such	O
that	O
the	O
gradient	O
of	O
the	O
error	B
function	I
vanishes	O
,	O
so	O
that	O
5.2.	O
network	O
training	B
237	O
∇e	O
(	O
w	O
)	O
=	O
0	O
(	O
5.26	O
)	O
as	O
otherwise	O
we	O
could	O
make	O
a	O
small	O
step	O
in	O
the	O
direction	O
of	O
−∇e	O
(	O
w	O
)	O
and	O
thereby	O
further	O
reduce	O
the	O
error	B
.	O
points	O
at	O
which	O
the	O
gradient	O
vanishes	O
are	O
called	O
stationary	B
points	O
,	O
and	O
may	O
be	O
further	O
classiﬁed	O
into	O
minima	O
,	O
maxima	O
,	O
and	O
saddle	O
points	O
.	O
our	O
goal	O
is	O
to	O
ﬁnd	O
a	O
vector	O
w	O
such	O
that	O
e	O
(	O
w	O
)	O
takes	O
its	O
smallest	O
value	O
.	O
how-	O
ever	O
,	O
the	O
error	B
function	I
typically	O
has	O
a	O
highly	O
nonlinear	O
dependence	O
on	O
the	O
weights	O
and	O
bias	B
parameters	O
,	O
and	O
so	O
there	O
will	O
be	O
many	O
points	O
in	O
weight	O
space	O
at	O
which	O
the	O
gradient	O
vanishes	O
(	O
or	O
is	O
numerically	O
very	O
small	O
)	O
.	O
indeed	O
,	O
from	O
the	O
discussion	O
in	O
sec-	O
tion	O
5.1.1	O
we	O
see	O
that	O
for	O
any	O
point	O
w	O
that	O
is	O
a	O
local	B
minimum	I
,	O
there	O
will	O
be	O
other	O
points	O
in	O
weight	O
space	O
that	O
are	O
equivalent	O
minima	O
.	O
for	O
instance	O
,	O
in	O
a	O
two-layer	O
net-	O
work	O
of	O
the	O
kind	O
shown	O
in	O
figure	O
5.1	O
,	O
with	O
m	O
hidden	O
units	O
,	O
each	O
point	O
in	O
weight	O
space	O
is	O
a	O
member	O
of	O
a	O
family	O
of	O
m	O
!	O
2m	O
equivalent	O
points	O
.	O
furthermore	O
,	O
there	O
will	O
typically	O
be	O
multiple	O
inequivalent	O
stationary	B
points	O
and	O
in	O
particular	O
multiple	O
inequivalent	O
minima	O
.	O
a	O
minimum	O
that	O
corresponds	O
to	O
the	O
smallest	O
value	O
of	O
the	O
error	B
function	I
for	O
any	O
weight	B
vector	I
is	O
said	O
to	O
be	O
a	O
global	B
minimum	I
.	O
any	O
other	O
minima	O
corresponding	O
to	O
higher	O
values	O
of	O
the	O
error	B
function	I
are	O
said	O
to	O
be	O
local	B
minima	O
.	O
for	O
a	O
successful	O
application	O
of	O
neural	O
networks	O
,	O
it	O
may	O
not	O
be	O
necessary	O
to	O
ﬁnd	O
the	O
global	B
minimum	I
(	O
and	O
in	O
general	O
it	O
will	O
not	O
be	O
known	O
whether	O
the	O
global	B
minimum	I
has	O
been	O
found	O
)	O
but	O
it	O
may	O
be	O
necessary	O
to	O
compare	O
several	O
local	B
minima	O
in	O
order	O
to	O
ﬁnd	O
a	O
sufﬁciently	O
good	O
solution	O
.	O
because	O
there	O
is	O
clearly	O
no	O
hope	O
of	O
ﬁnding	O
an	O
analytical	O
solution	O
to	O
the	O
equa-	O
tion	O
∇e	O
(	O
w	O
)	O
=	O
0	O
we	O
resort	O
to	O
iterative	O
numerical	O
procedures	O
.	O
the	O
optimization	O
of	O
continuous	O
nonlinear	O
functions	O
is	O
a	O
widely	O
studied	O
problem	O
and	O
there	O
exists	O
an	O
ex-	O
tensive	O
literature	O
on	O
how	O
to	O
solve	O
it	O
efﬁciently	O
.	O
most	O
techniques	O
involve	O
choosing	O
some	O
initial	O
value	O
w	O
(	O
0	O
)	O
for	O
the	O
weight	B
vector	I
and	O
then	O
moving	O
through	O
weight	O
space	O
in	O
a	O
succession	O
of	O
steps	O
of	O
the	O
form	O
w	O
(	O
τ	O
+1	O
)	O
=	O
w	O
(	O
τ	O
)	O
+	O
∆w	O
(	O
τ	O
)	O
(	O
5.27	O
)	O
where	O
τ	O
labels	O
the	O
iteration	O
step	O
.	O
different	O
algorithms	O
involve	O
different	O
choices	O
for	O
the	O
weight	B
vector	I
update	O
∆w	O
(	O
τ	O
)	O
.	O
many	O
algorithms	O
make	O
use	O
of	O
gradient	O
information	O
and	O
therefore	O
require	O
that	O
,	O
after	O
each	O
update	O
,	O
the	O
value	O
of	O
∇e	O
(	O
w	O
)	O
is	O
evaluated	O
at	O
the	O
new	O
weight	B
vector	I
w	O
(	O
τ	O
+1	O
)	O
.	O
in	O
order	O
to	O
understand	O
the	O
importance	O
of	O
gradient	O
information	O
,	O
it	O
is	O
useful	O
to	O
consider	O
a	O
local	B
approximation	O
to	O
the	O
error	B
function	I
based	O
on	O
a	O
taylor	O
expansion	O
.	O
5.2.2	O
local	B
quadratic	O
approximation	O
insight	O
into	O
the	O
optimization	O
problem	O
,	O
and	O
into	O
the	O
various	O
techniques	O
for	O
solv-	O
ing	O
it	O
,	O
can	O
be	O
obtained	O
by	O
considering	O
a	O
local	B
quadratic	O
approximation	O
to	O
the	O
error	B
function	I
.	O
consider	O
the	O
taylor	O
expansion	O
of	O
e	O
(	O
w	O
)	O
around	O
some	O
point	O
(	O
cid:1	O
)	O
w	O
in	O
weight	O
space	O
e	O
(	O
w	O
)	O
(	O
cid:7	O
)	O
e	O
(	O
(	O
cid:1	O
)	O
w	O
)	O
+	O
(	O
w	O
−	O
(	O
cid:1	O
)	O
w	O
)	O
tb	O
+	O
(	O
w	O
−	O
(	O
cid:1	O
)	O
w	O
)	O
th	O
(	O
w	O
−	O
(	O
cid:1	O
)	O
w	O
)	O
1	O
2	O
(	O
5.28	O
)	O
238	O
5.	O
neural	O
networks	O
where	O
cubic	O
and	O
higher	O
terms	O
have	O
been	O
omitted	O
.	O
here	O
b	O
is	O
deﬁned	O
to	O
be	O
the	O
gradient	O
of	O
e	O
evaluated	O
at	O
(	O
cid:1	O
)	O
w	O
b	O
≡	O
∇e|w=bw	O
and	O
the	O
hessian	O
matrix	O
h	O
=	O
∇∇e	O
has	O
elements	O
(	O
5.29	O
)	O
(	O
5.30	O
)	O
(	O
h	O
)	O
ij	O
≡	O
∂e	O
∂wi∂wj	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
∇e	O
(	O
cid:7	O
)	O
b	O
+	O
h	O
(	O
w	O
−	O
(	O
cid:1	O
)	O
w	O
)	O
.	O
w=bw	O
.	O
from	O
(	O
5.28	O
)	O
,	O
the	O
corresponding	O
local	B
approximation	O
to	O
the	O
gradient	O
is	O
given	O
by	O
for	O
points	O
w	O
that	O
are	O
sufﬁciently	O
close	O
to	O
(	O
cid:1	O
)	O
w	O
,	O
these	O
expressions	O
will	O
give	O
reasonable	O
(	O
5.31	O
)	O
approximations	O
for	O
the	O
error	B
and	O
its	O
gradient	O
.	O
consider	O
the	O
particular	O
case	O
of	O
a	O
local	B
quadratic	O
approximation	O
around	O
a	O
point	O
w	O
(	O
cid:1	O
)	O
that	O
is	O
a	O
minimum	O
of	O
the	O
error	B
function	I
.	O
in	O
this	O
case	O
there	O
is	O
no	O
linear	O
term	O
,	O
because	O
∇e	O
=	O
0	O
at	O
w	O
(	O
cid:1	O
)	O
,	O
and	O
(	O
5.28	O
)	O
becomes	O
e	O
(	O
w	O
)	O
=	O
e	O
(	O
w	O
(	O
cid:1	O
)	O
)	O
+	O
(	O
w	O
−	O
w	O
(	O
cid:1	O
)	O
)	O
th	O
(	O
w	O
−	O
w	O
(	O
cid:1	O
)	O
)	O
1	O
2	O
(	O
5.32	O
)	O
where	O
the	O
hessian	O
h	O
is	O
evaluated	O
at	O
w	O
(	O
cid:1	O
)	O
.	O
in	O
order	O
to	O
interpret	O
this	O
geometrically	O
,	O
consider	O
the	O
eigenvalue	O
equation	O
for	O
the	O
hessian	O
matrix	O
hui	O
=	O
λiui	O
(	O
5.33	O
)	O
where	O
the	O
eigenvectors	O
ui	O
form	O
a	O
complete	O
orthonormal	O
set	O
(	O
appendix	O
c	O
)	O
so	O
that	O
(	O
5.34	O
)	O
we	O
now	O
expand	O
(	O
w	O
−	O
w	O
(	O
cid:1	O
)	O
)	O
as	O
a	O
linear	O
combination	O
of	O
the	O
eigenvectors	O
in	O
the	O
form	O
ut	O
i	O
uj	O
=	O
δij	O
.	O
w	O
−	O
w	O
(	O
cid:1	O
)	O
=	O
αiui	O
.	O
(	O
5.35	O
)	O
(	O
cid:2	O
)	O
i	O
this	O
can	O
be	O
regarded	O
as	O
a	O
transformation	O
of	O
the	O
coordinate	O
system	O
in	O
which	O
the	O
origin	O
is	O
translated	O
to	O
the	O
point	O
w	O
(	O
cid:1	O
)	O
,	O
and	O
the	O
axes	O
are	O
rotated	O
to	O
align	O
with	O
the	O
eigenvectors	O
(	O
through	O
the	O
orthogonal	O
matrix	O
whose	O
columns	O
are	O
the	O
ui	O
)	O
,	O
and	O
is	O
discussed	O
in	O
more	O
detail	O
in	O
appendix	O
c.	O
substituting	O
(	O
5.35	O
)	O
into	O
(	O
5.32	O
)	O
,	O
and	O
using	O
(	O
5.33	O
)	O
and	O
(	O
5.34	O
)	O
,	O
allows	O
the	O
error	B
function	I
to	O
be	O
written	O
in	O
the	O
form	O
e	O
(	O
w	O
)	O
=	O
e	O
(	O
w	O
(	O
cid:1	O
)	O
)	O
+	O
1	O
2	O
λiα2	O
i	O
.	O
(	O
cid:2	O
)	O
i	O
a	O
matrix	O
h	O
is	O
said	O
to	O
be	O
positive	B
deﬁnite	I
if	O
,	O
and	O
only	O
if	O
,	O
vthv	O
>	O
0	O
for	O
all	O
v.	O
(	O
5.36	O
)	O
(	O
5.37	O
)	O
w2	O
the	O
error	B
figure	O
5.6	O
in	O
the	O
neighbourhood	O
of	O
a	O
min-	O
imum	O
w	O
(	O
cid:1	O
)	O
,	O
function	O
can	O
be	O
approximated	O
by	O
a	O
quadratic	O
.	O
contours	O
of	O
con-	O
stant	O
error	B
are	O
then	O
ellipses	O
whose	O
axes	O
are	O
aligned	O
with	O
the	O
eigenvectors	O
ui	O
of	O
the	O
hes-	O
sian	O
matrix	O
,	O
with	O
lengths	O
that	O
are	O
inversely	O
proportional	O
to	O
the	O
square	O
roots	O
of	O
the	O
correspond-	O
ing	O
eigenvectors	O
λi	O
.	O
−1/2	O
2	O
λ	O
5.2.	O
network	O
training	B
239	O
u2	O
w	O
(	O
cid:1	O
)	O
u1	O
w1	O
−1/2	O
1	O
λ	O
because	O
the	O
eigenvectors	O
{	O
ui	O
}	O
form	O
a	O
complete	O
set	O
,	O
an	O
arbitrary	O
vector	O
v	O
can	O
be	O
written	O
in	O
the	O
form	O
from	O
(	O
5.33	O
)	O
and	O
(	O
5.34	O
)	O
,	O
we	O
then	O
have	O
(	O
5.38	O
)	O
(	O
5.39	O
)	O
v	O
=	O
ciui	O
.	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
i	O
i	O
vthv	O
=	O
c2	O
i	O
λi	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
∂2e	O
∂w2	O
exercise	O
5.10	O
exercise	O
5.11	O
and	O
so	O
h	O
will	O
be	O
positive	B
deﬁnite	I
if	O
,	O
and	O
only	O
if	O
,	O
all	O
of	O
its	O
eigenvalues	O
are	O
positive	O
.	O
in	O
the	O
new	O
coordinate	O
system	O
,	O
whose	O
basis	O
vectors	O
are	O
given	O
by	O
the	O
eigenvectors	O
{	O
ui	O
}	O
,	O
the	O
contours	O
of	O
constant	O
e	O
are	O
ellipses	O
centred	O
on	O
the	O
origin	O
,	O
as	O
illustrated	O
in	O
figure	O
5.6.	O
for	O
a	O
one-dimensional	O
weight	O
space	O
,	O
a	O
stationary	B
point	O
w	O
(	O
cid:1	O
)	O
will	O
be	O
a	O
minimum	O
if	O
>	O
0.	O
w	O
(	O
cid:1	O
)	O
(	O
5.40	O
)	O
exercise	O
5.12	O
the	O
corresponding	O
result	O
in	O
d-dimensions	O
is	O
that	O
the	O
hessian	O
matrix	O
,	O
evaluated	O
at	O
w	O
(	O
cid:1	O
)	O
,	O
should	O
be	O
positive	B
deﬁnite	I
.	O
5.2.3	O
use	O
of	O
gradient	O
information	O
as	O
we	O
shall	O
see	O
in	O
section	O
5.3	O
,	O
it	O
is	O
possible	O
to	O
evaluate	O
the	O
gradient	O
of	O
an	O
error	B
function	I
efﬁciently	O
by	O
means	O
of	O
the	O
backpropagation	B
procedure	O
.	O
the	O
use	O
of	O
this	O
gradient	O
information	O
can	O
lead	O
to	O
signiﬁcant	O
improvements	O
in	O
the	O
speed	O
with	O
which	O
the	O
minima	O
of	O
the	O
error	B
function	I
can	O
be	O
located	O
.	O
we	O
can	O
see	O
why	O
this	O
is	O
so	O
,	O
as	O
follows	O
.	O
in	O
the	O
quadratic	O
approximation	O
to	O
the	O
error	B
function	I
,	O
given	O
in	O
(	O
5.28	O
)	O
,	O
the	O
error	B
surface	O
is	O
speciﬁed	O
by	O
the	O
quantities	O
b	O
and	O
h	O
,	O
which	O
contain	O
a	O
total	O
of	O
w	O
(	O
w	O
+	O
3	O
)	O
/2	O
independent	B
elements	O
(	O
because	O
the	O
matrix	O
h	O
is	O
symmetric	O
)	O
,	O
where	O
w	O
is	O
the	O
dimensionality	O
of	O
w	O
(	O
i.e.	O
,	O
the	O
total	O
number	O
of	O
adaptive	O
parameters	O
in	O
the	O
network	O
)	O
.	O
the	O
location	O
of	O
the	O
minimum	O
of	O
this	O
quadratic	O
approximation	O
therefore	O
depends	O
on	O
o	O
(	O
w	O
2	O
)	O
parameters	O
,	O
and	O
we	O
should	O
not	O
expect	O
to	O
be	O
able	O
to	O
locate	O
the	O
minimum	O
until	O
we	O
have	O
gathered	O
o	O
(	O
w	O
2	O
)	O
independent	B
pieces	O
of	O
information	O
.	O
if	O
we	O
do	O
not	O
make	O
use	O
of	O
gradient	O
information	O
,	O
we	O
would	O
expect	O
to	O
have	O
to	O
perform	O
o	O
(	O
w	O
2	O
)	O
function	O
exercise	O
5.13	O
240	O
5.	O
neural	O
networks	O
evaluations	O
,	O
each	O
of	O
which	O
would	O
require	O
o	O
(	O
w	O
)	O
steps	O
.	O
thus	O
,	O
the	O
computational	O
effort	O
needed	O
to	O
ﬁnd	O
the	O
minimum	O
using	O
such	O
an	O
approach	O
would	O
be	O
o	O
(	O
w	O
3	O
)	O
.	O
now	O
compare	O
this	O
with	O
an	O
algorithm	O
that	O
makes	O
use	O
of	O
the	O
gradient	O
information	O
.	O
because	O
each	O
evaluation	O
of	O
∇e	O
brings	O
w	O
items	O
of	O
information	O
,	O
we	O
might	O
hope	O
to	O
ﬁnd	O
the	O
minimum	O
of	O
the	O
function	O
in	O
o	O
(	O
w	O
)	O
gradient	O
evaluations	O
.	O
as	O
we	O
shall	O
see	O
,	O
by	O
using	O
error	B
backpropagation	I
,	O
each	O
such	O
evaluation	O
takes	O
only	O
o	O
(	O
w	O
)	O
steps	O
and	O
so	O
the	O
minimum	O
can	O
now	O
be	O
found	O
in	O
o	O
(	O
w	O
2	O
)	O
steps	O
.	O
for	O
this	O
reason	O
,	O
the	O
use	O
of	O
gradient	O
information	O
forms	O
the	O
basis	O
of	O
practical	O
algorithms	O
for	O
training	O
neural	O
networks	O
.	O
w	O
(	O
τ	O
+1	O
)	O
=	O
w	O
(	O
τ	O
)	O
−	O
η∇e	O
(	O
w	O
(	O
τ	O
)	O
)	O
5.2.4	O
gradient	B
descent	I
optimization	O
the	O
simplest	O
approach	O
to	O
using	O
gradient	O
information	O
is	O
to	O
choose	O
the	O
weight	O
update	O
in	O
(	O
5.27	O
)	O
to	O
comprise	O
a	O
small	O
step	O
in	O
the	O
direction	O
of	O
the	O
negative	O
gradient	O
,	O
so	O
that	O
(	O
5.41	O
)	O
where	O
the	O
parameter	O
η	O
>	O
0	O
is	O
known	O
as	O
the	O
learning	O
rate	O
.	O
after	O
each	O
such	O
update	O
,	O
the	O
gradient	O
is	O
re-evaluated	O
for	O
the	O
new	O
weight	B
vector	I
and	O
the	O
process	O
repeated	O
.	O
note	O
that	O
the	O
error	B
function	I
is	O
deﬁned	O
with	O
respect	O
to	O
a	O
training	B
set	I
,	O
and	O
so	O
each	O
step	O
requires	O
that	O
the	O
entire	O
training	B
set	I
be	O
processed	O
in	O
order	O
to	O
evaluate	O
∇e	O
.	O
techniques	O
that	O
use	O
the	O
whole	O
data	O
set	O
at	O
once	O
are	O
called	O
batch	O
methods	O
.	O
at	O
each	O
step	O
the	O
weight	B
vector	I
is	O
moved	O
in	O
the	O
direction	O
of	O
the	O
greatest	O
rate	O
of	O
decrease	O
of	O
the	O
error	B
function	I
,	O
and	O
so	O
this	O
approach	O
is	O
known	O
as	O
gradient	B
descent	I
or	O
steepest	B
descent	I
.	O
although	O
such	O
an	O
approach	O
might	O
intuitively	O
seem	O
reasonable	O
,	O
in	O
fact	O
it	O
turns	O
out	O
to	O
be	O
a	O
poor	O
algorithm	O
,	O
for	O
reasons	O
discussed	O
in	O
bishop	O
and	O
nabney	O
(	O
2008	O
)	O
.	O
for	O
batch	O
optimization	O
,	O
there	O
are	O
more	O
efﬁcient	O
methods	O
,	O
such	O
as	O
conjugate	B
gra-	O
dients	O
and	O
quasi-newton	O
methods	O
,	O
which	O
are	O
much	O
more	O
robust	O
and	O
much	O
faster	O
than	O
simple	O
gradient	B
descent	I
(	O
gill	O
et	O
al.	O
,	O
1981	O
;	O
fletcher	O
,	O
1987	O
;	O
nocedal	O
and	O
wright	O
,	O
1999	O
)	O
.	O
unlike	O
gradient	B
descent	I
,	O
these	O
algorithms	O
have	O
the	O
property	O
that	O
the	O
error	B
function	I
always	O
decreases	O
at	O
each	O
iteration	O
unless	O
the	O
weight	B
vector	I
has	O
arrived	O
at	O
a	O
local	B
or	O
global	B
minimum	I
.	O
in	O
order	O
to	O
ﬁnd	O
a	O
sufﬁciently	O
good	O
minimum	O
,	O
it	O
may	O
be	O
necessary	O
to	O
run	O
a	O
gradient-based	O
algorithm	O
multiple	O
times	O
,	O
each	O
time	O
using	O
a	O
different	O
randomly	O
cho-	O
sen	O
starting	O
point	O
,	O
and	O
comparing	O
the	O
resulting	O
performance	O
on	O
an	O
independent	B
vali-	O
dation	O
set	O
.	O
there	O
is	O
,	O
however	O
,	O
an	O
on-line	O
version	O
of	O
gradient	B
descent	I
that	O
has	O
proved	O
useful	O
in	O
practice	O
for	O
training	O
neural	O
networks	O
on	O
large	O
data	O
sets	O
(	O
le	O
cun	O
et	O
al.	O
,	O
1989	O
)	O
.	O
error	B
functions	O
based	O
on	O
maximum	B
likelihood	I
for	O
a	O
set	O
of	O
independent	B
observations	O
comprise	O
a	O
sum	O
of	O
terms	O
,	O
one	O
for	O
each	O
data	O
point	O
n	O
(	O
cid:2	O
)	O
n=1	O
e	O
(	O
w	O
)	O
=	O
en	O
(	O
w	O
)	O
.	O
(	O
5.42	O
)	O
on-line	O
gradient	O
descent	O
,	O
also	O
known	O
as	O
sequential	B
gradient	I
descent	I
or	O
stochastic	B
gradient	I
descent	I
,	O
makes	O
an	O
update	O
to	O
the	O
weight	B
vector	I
based	O
on	O
one	O
data	O
point	O
at	O
a	O
time	O
,	O
so	O
that	O
w	O
(	O
τ	O
+1	O
)	O
=	O
w	O
(	O
τ	O
)	O
−	O
η∇en	O
(	O
w	O
(	O
τ	O
)	O
)	O
.	O
(	O
5.43	O
)	O
5.3.	O
error	B
backpropagation	I
241	O
this	O
update	O
is	O
repeated	O
by	O
cycling	O
through	O
the	O
data	O
either	O
in	O
sequence	O
or	O
by	O
selecting	O
points	O
at	O
random	O
with	O
replacement	O
.	O
there	O
are	O
of	O
course	O
intermediate	O
scenarios	O
in	O
which	O
the	O
updates	O
are	O
based	O
on	O
batches	O
of	O
data	O
points	O
.	O
one	O
advantage	O
of	O
on-line	O
methods	O
compared	O
to	O
batch	O
methods	O
is	O
that	O
the	O
former	O
handle	O
redundancy	O
in	O
the	O
data	O
much	O
more	O
efﬁciently	O
.	O
to	O
see	O
,	O
this	O
consider	O
an	O
ex-	O
treme	O
example	O
in	O
which	O
we	O
take	O
a	O
data	O
set	O
and	O
double	O
its	O
size	O
by	O
duplicating	O
every	O
data	O
point	O
.	O
note	O
that	O
this	O
simply	O
multiplies	O
the	O
error	B
function	I
by	O
a	O
factor	O
of	O
2	O
and	O
so	O
is	O
equivalent	O
to	O
using	O
the	O
original	O
error	B
function	I
.	O
batch	O
methods	O
will	O
require	O
double	O
the	O
computational	O
effort	O
to	O
evaluate	O
the	O
batch	O
error	O
function	O
gradient	O
,	O
whereas	O
on-	O
line	O
methods	O
will	O
be	O
unaffected	O
.	O
another	O
property	O
of	O
on-line	O
gradient	O
descent	O
is	O
the	O
possibility	O
of	O
escaping	O
from	O
local	B
minima	O
,	O
since	O
a	O
stationary	B
point	O
with	O
respect	O
to	O
the	O
error	B
function	I
for	O
the	O
whole	O
data	O
set	O
will	O
generally	O
not	O
be	O
a	O
stationary	B
point	O
for	O
each	O
data	O
point	O
individually	O
.	O
nonlinear	O
optimization	O
algorithms	O
,	O
and	O
their	O
practical	O
application	O
to	O
neural	O
net-	O
work	O
training	B
,	O
are	O
discussed	O
in	O
detail	O
in	O
bishop	O
and	O
nabney	O
(	O
2008	O
)	O
.	O
5.3.	O
error	B
backpropagation	I
our	O
goal	O
in	O
this	O
section	O
is	O
to	O
ﬁnd	O
an	O
efﬁcient	O
technique	O
for	O
evaluating	O
the	O
gradient	O
of	O
an	O
error	B
function	I
e	O
(	O
w	O
)	O
for	O
a	O
feed-forward	O
neural	B
network	I
.	O
we	O
shall	O
see	O
that	O
this	O
can	O
be	O
achieved	O
using	O
a	O
local	B
message	O
passing	O
scheme	O
in	O
which	O
information	O
is	O
sent	O
alternately	O
forwards	O
and	O
backwards	O
through	O
the	O
network	O
and	O
is	O
known	O
as	O
error	B
backpropagation	I
,	O
or	O
sometimes	O
simply	O
as	O
backprop	O
.	O
it	O
should	O
be	O
noted	O
that	O
the	O
term	O
backpropagation	B
is	O
used	O
in	O
the	O
neural	O
com-	O
puting	O
literature	O
to	O
mean	B
a	O
variety	O
of	O
different	O
things	O
.	O
for	O
instance	O
,	O
the	O
multilayer	B
perceptron	I
architecture	O
is	O
sometimes	O
called	O
a	O
backpropagation	B
network	O
.	O
the	O
term	O
backpropagation	B
is	O
also	O
used	O
to	O
describe	O
the	O
training	B
of	O
a	O
multilayer	B
perceptron	I
us-	O
ing	O
gradient	O
descent	O
applied	O
to	O
a	O
sum-of-squares	B
error	I
function	O
.	O
in	O
order	O
to	O
clarify	O
the	O
terminology	O
,	O
it	O
is	O
useful	O
to	O
consider	O
the	O
nature	O
of	O
the	O
training	B
process	O
more	O
care-	O
fully	O
.	O
most	O
training	B
algorithms	O
involve	O
an	O
iterative	O
procedure	O
for	O
minimization	O
of	O
an	O
error	B
function	I
,	O
with	O
adjustments	O
to	O
the	O
weights	O
being	O
made	O
in	O
a	O
sequence	O
of	O
steps	O
.	O
at	O
each	O
such	O
step	O
,	O
we	O
can	O
distinguish	O
between	O
two	O
distinct	O
stages	O
.	O
in	O
the	O
ﬁrst	O
stage	O
,	O
the	O
derivatives	O
of	O
the	O
error	B
function	I
with	O
respect	O
to	O
the	O
weights	O
must	O
be	O
evaluated	O
.	O
as	O
we	O
shall	O
see	O
,	O
the	O
important	O
contribution	O
of	O
the	O
backpropagation	B
technique	O
is	O
in	O
pro-	O
viding	O
a	O
computationally	O
efﬁcient	O
method	O
for	O
evaluating	O
such	O
derivatives	O
.	O
because	O
it	O
is	O
at	O
this	O
stage	O
that	O
errors	O
are	O
propagated	O
backwards	O
through	O
the	O
network	O
,	O
we	O
shall	O
use	O
the	O
term	O
backpropagation	B
speciﬁcally	O
to	O
describe	O
the	O
evaluation	O
of	O
derivatives	O
.	O
in	O
the	O
second	O
stage	O
,	O
the	O
derivatives	O
are	O
then	O
used	O
to	O
compute	O
the	O
adjustments	O
to	O
be	O
made	O
to	O
the	O
weights	O
.	O
the	O
simplest	O
such	O
technique	O
,	O
and	O
the	O
one	O
originally	O
considered	O
by	O
rumelhart	O
et	O
al	O
.	O
(	O
1986	O
)	O
,	O
involves	O
gradient	B
descent	I
.	O
it	O
is	O
important	O
to	O
recognize	O
that	O
the	O
two	O
stages	O
are	O
distinct	O
.	O
thus	O
,	O
the	O
ﬁrst	O
stage	O
,	O
namely	O
the	O
propagation	O
of	O
er-	O
rors	O
backwards	O
through	O
the	O
network	O
in	O
order	O
to	O
evaluate	O
derivatives	O
,	O
can	O
be	O
applied	O
to	O
many	O
other	O
kinds	O
of	O
network	O
and	O
not	O
just	O
the	O
multilayer	B
perceptron	I
.	O
it	O
can	O
also	O
be	O
applied	O
to	O
error	B
functions	O
other	O
that	O
just	O
the	O
simple	O
sum-of-squares	O
,	O
and	O
to	O
the	O
eval-	O
242	O
5.	O
neural	O
networks	O
uation	O
of	O
other	O
derivatives	O
such	O
as	O
the	O
jacobian	O
and	O
hessian	O
matrices	O
,	O
as	O
we	O
shall	O
see	O
later	O
in	O
this	O
chapter	O
.	O
similarly	O
,	O
the	O
second	O
stage	O
of	O
weight	O
adjustment	O
using	O
the	O
calculated	O
derivatives	O
can	O
be	O
tackled	O
using	O
a	O
variety	O
of	O
optimization	O
schemes	O
,	O
many	O
of	O
which	O
are	O
substantially	O
more	O
powerful	O
than	O
simple	O
gradient	B
descent	I
.	O
5.3.1	O
evaluation	O
of	O
error-function	O
derivatives	O
we	O
now	O
derive	O
the	O
backpropagation	B
algorithm	O
for	O
a	O
general	O
network	O
having	O
ar-	O
bitrary	O
feed-forward	O
topology	O
,	O
arbitrary	O
differentiable	O
nonlinear	O
activation	O
functions	O
,	O
and	O
a	O
broad	O
class	O
of	O
error	B
function	I
.	O
the	O
resulting	O
formulae	O
will	O
then	O
be	O
illustrated	O
using	O
a	O
simple	O
layered	O
network	O
structure	O
having	O
a	O
single	O
layer	O
of	O
sigmoidal	O
hidden	O
units	O
together	O
with	O
a	O
sum-of-squares	B
error	I
.	O
many	O
error	B
functions	O
of	O
practical	O
interest	O
,	O
for	O
instance	O
those	O
deﬁned	O
by	O
maxi-	O
mum	O
likelihood	O
for	O
a	O
set	O
of	O
i.i.d	O
.	O
data	O
,	O
comprise	O
a	O
sum	O
of	O
terms	O
,	O
one	O
for	O
each	O
data	O
point	O
in	O
the	O
training	B
set	I
,	O
so	O
that	O
n	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
i	O
e	O
(	O
w	O
)	O
=	O
(	O
5.44	O
)	O
here	O
we	O
shall	O
consider	O
the	O
problem	O
of	O
evaluating	O
∇en	O
(	O
w	O
)	O
for	O
one	O
such	O
term	O
in	O
the	O
error	B
function	I
.	O
this	O
may	O
be	O
used	O
directly	O
for	O
sequential	O
optimization	O
,	O
or	O
the	O
results	O
can	O
be	O
accumulated	O
over	O
the	O
training	B
set	I
in	O
the	O
case	O
of	O
batch	O
methods	O
.	O
n=1	O
en	O
(	O
w	O
)	O
.	O
consider	O
ﬁrst	O
a	O
simple	O
linear	O
model	O
in	O
which	O
the	O
outputs	O
yk	O
are	O
linear	O
combina-	O
tions	O
of	O
the	O
input	O
variables	O
xi	O
so	O
that	O
yk	O
=	O
wkixi	O
(	O
5.45	O
)	O
together	O
with	O
an	O
error	B
function	I
that	O
,	O
for	O
a	O
particular	O
input	O
pattern	O
n	O
,	O
takes	O
the	O
form	O
en	O
=	O
1	O
2	O
k	O
(	O
ynk	O
−	O
tnk	O
)	O
2	O
=	O
(	O
ynj	O
−	O
tnj	O
)	O
xni	O
∂en	O
∂wji	O
(	O
5.46	O
)	O
(	O
5.47	O
)	O
where	O
ynk	O
=	O
yk	O
(	O
xn	O
,	O
w	O
)	O
.	O
the	O
gradient	O
of	O
this	O
error	B
function	I
with	O
respect	O
to	O
a	O
weight	O
wji	O
is	O
given	O
by	O
which	O
can	O
be	O
interpreted	O
as	O
a	O
‘	O
local	B
’	O
computation	O
involving	O
the	O
product	O
of	O
an	O
‘	O
error	B
signal	O
’	O
ynj	O
−	O
tnj	O
associated	O
with	O
the	O
output	O
end	O
of	O
the	O
link	B
wji	O
and	O
the	O
variable	O
xni	O
associated	O
with	O
the	O
input	O
end	O
of	O
the	O
link	B
.	O
in	O
section	O
4.3.2	O
,	O
we	O
saw	O
how	O
a	O
similar	O
formula	O
arises	O
with	O
the	O
logistic	B
sigmoid	I
activation	O
function	O
together	O
with	O
the	O
cross	O
entropy	B
error	O
function	O
,	O
and	O
similarly	O
for	O
the	O
softmax	O
activation	O
function	O
together	O
with	O
its	O
matching	O
cross-entropy	B
error	I
function	I
.	O
we	O
shall	O
now	O
see	O
how	O
this	O
simple	O
result	O
extends	O
to	O
the	O
more	O
complex	O
setting	O
of	O
multilayer	O
feed-forward	O
networks	O
.	O
in	O
a	O
general	O
feed-forward	O
network	O
,	O
each	O
unit	O
computes	O
a	O
weighted	O
sum	O
of	O
its	O
(	O
cid:2	O
)	O
inputs	O
of	O
the	O
form	O
aj	O
=	O
wjizi	O
i	O
(	O
5.48	O
)	O
5.3.	O
error	B
backpropagation	I
243	O
where	O
zi	O
is	O
the	O
activation	O
of	O
a	O
unit	O
,	O
or	O
input	O
,	O
that	O
sends	O
a	O
connection	O
to	O
unit	O
j	O
,	O
and	O
wji	O
is	O
the	O
weight	O
associated	O
with	O
that	O
connection	O
.	O
in	O
section	O
5.1	O
,	O
we	O
saw	O
that	O
biases	O
can	O
be	O
included	O
in	O
this	O
sum	O
by	O
introducing	O
an	O
extra	O
unit	O
,	O
or	O
input	O
,	O
with	O
activation	O
ﬁxed	O
at	O
+1	O
.	O
we	O
therefore	O
do	O
not	O
need	O
to	O
deal	O
with	O
biases	O
explicitly	O
.	O
the	O
sum	O
in	O
(	O
5.48	O
)	O
is	O
transformed	O
by	O
a	O
nonlinear	O
activation	B
function	I
h	O
(	O
·	O
)	O
to	O
give	O
the	O
activation	O
zj	O
of	O
unit	O
j	O
in	O
the	O
form	O
zj	O
=	O
h	O
(	O
aj	O
)	O
.	O
(	O
5.49	O
)	O
note	O
that	O
one	O
or	O
more	O
of	O
the	O
variables	O
zi	O
in	O
the	O
sum	O
in	O
(	O
5.48	O
)	O
could	O
be	O
an	O
input	O
,	O
and	O
similarly	O
,	O
the	O
unit	O
j	O
in	O
(	O
5.49	O
)	O
could	O
be	O
an	O
output	O
.	O
for	O
each	O
pattern	O
in	O
the	O
training	B
set	I
,	O
we	O
shall	O
suppose	O
that	O
we	O
have	O
supplied	O
the	O
corresponding	O
input	O
vector	O
to	O
the	O
network	O
and	O
calculated	O
the	O
activations	O
of	O
all	O
of	O
the	O
hidden	O
and	O
output	O
units	O
in	O
the	O
network	O
by	O
successive	O
application	O
of	O
(	O
5.48	O
)	O
and	O
(	O
5.49	O
)	O
.	O
this	O
process	O
is	O
often	O
called	O
forward	B
propagation	I
because	O
it	O
can	O
be	O
regarded	O
as	O
a	O
forward	O
ﬂow	O
of	O
information	O
through	O
the	O
network	O
.	O
now	O
consider	O
the	O
evaluation	O
of	O
the	O
derivative	B
of	O
en	O
with	O
respect	O
to	O
a	O
weight	O
wji	O
.	O
the	O
outputs	O
of	O
the	O
various	O
units	O
will	O
depend	O
on	O
the	O
particular	O
input	O
pattern	O
n.	O
however	O
,	O
in	O
order	O
to	O
keep	O
the	O
notation	O
uncluttered	O
,	O
we	O
shall	O
omit	O
the	O
subscript	O
n	O
from	O
the	O
network	O
variables	O
.	O
first	O
we	O
note	O
that	O
en	O
depends	O
on	O
the	O
weight	O
wji	O
only	O
via	O
the	O
summed	O
input	O
aj	O
to	O
unit	O
j.	O
we	O
can	O
therefore	O
apply	O
the	O
chain	O
rule	O
for	O
partial	O
derivatives	O
to	O
give	O
we	O
now	O
introduce	O
a	O
useful	O
notation	O
δj	O
≡	O
∂en	O
∂aj	O
∂en	O
∂wji	O
=	O
∂en	O
∂aj	O
∂aj	O
∂wji	O
.	O
(	O
5.50	O
)	O
(	O
5.51	O
)	O
where	O
the	O
δ	O
’	O
s	O
are	O
often	O
referred	O
to	O
as	O
errors	O
for	O
reasons	O
we	O
shall	O
see	O
shortly	O
.	O
using	O
(	O
5.48	O
)	O
,	O
we	O
can	O
write	O
∂aj	O
∂wji	O
=	O
zi	O
.	O
substituting	O
(	O
5.51	O
)	O
and	O
(	O
5.52	O
)	O
into	O
(	O
5.50	O
)	O
,	O
we	O
then	O
obtain	O
∂en	O
∂wji	O
=	O
δjzi	O
.	O
(	O
5.52	O
)	O
(	O
5.53	O
)	O
equation	O
(	O
5.53	O
)	O
tells	O
us	O
that	O
the	O
required	O
derivative	B
is	O
obtained	O
simply	O
by	O
multiplying	O
the	O
value	O
of	O
δ	O
for	O
the	O
unit	O
at	O
the	O
output	O
end	O
of	O
the	O
weight	O
by	O
the	O
value	O
of	O
z	O
for	O
the	O
unit	O
at	O
the	O
input	O
end	O
of	O
the	O
weight	O
(	O
where	O
z	O
=	O
1	O
in	O
the	O
case	O
of	O
a	O
bias	B
)	O
.	O
note	O
that	O
this	O
takes	O
the	O
same	O
form	O
as	O
for	O
the	O
simple	O
linear	O
model	O
considered	O
at	O
the	O
start	O
of	O
this	O
section	O
.	O
thus	O
,	O
in	O
order	O
to	O
evaluate	O
the	O
derivatives	O
,	O
we	O
need	O
only	O
to	O
calculate	O
the	O
value	O
of	O
δj	O
for	O
each	O
hidden	O
and	O
output	O
unit	O
in	O
the	O
network	O
,	O
and	O
then	O
apply	O
(	O
5.53	O
)	O
.	O
as	O
we	O
have	O
seen	O
already	O
,	O
for	O
the	O
output	O
units	O
,	O
we	O
have	O
δk	O
=	O
yk	O
−	O
tk	O
(	O
5.54	O
)	O
244	O
5.	O
neural	O
networks	O
figure	O
5.7	O
illustration	O
of	O
the	O
calculation	O
of	O
δj	O
for	O
hidden	O
unit	O
j	O
by	O
backpropagation	B
of	O
the	O
δ	O
’	O
s	O
from	O
those	O
units	O
k	O
to	O
which	O
unit	O
j	O
sends	O
connections	O
.	O
the	O
blue	O
arrow	O
denotes	O
the	O
direction	O
of	O
information	O
ﬂow	O
during	O
forward	B
propagation	I
,	O
and	O
the	O
red	O
arrows	O
indicate	O
the	O
backward	O
propagation	O
of	O
error	B
information	O
.	O
zi	O
wji	O
δj	O
zj	O
wkj	O
δk	O
δ1	O
provided	O
we	O
are	O
using	O
the	O
canonical	O
link	O
as	O
the	O
output-unit	O
activation	B
function	I
.	O
to	O
evaluate	O
the	O
δ	O
’	O
s	O
for	O
hidden	O
units	O
,	O
we	O
again	O
make	O
use	O
of	O
the	O
chain	O
rule	O
for	O
partial	O
derivatives	O
,	O
δj	O
≡	O
∂en	O
∂aj	O
=	O
∂en	O
∂ak	O
∂ak	O
∂aj	O
(	O
5.55	O
)	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
where	O
the	O
sum	O
runs	O
over	O
all	O
units	O
k	O
to	O
which	O
unit	O
j	O
sends	O
connections	O
.	O
the	O
arrange-	O
ment	O
of	O
units	O
and	O
weights	O
is	O
illustrated	O
in	O
figure	O
5.7.	O
note	O
that	O
the	O
units	O
labelled	O
k	O
could	O
include	O
other	O
hidden	O
units	O
and/or	O
output	O
units	O
.	O
in	O
writing	O
down	O
(	O
5.55	O
)	O
,	O
we	O
are	O
making	O
use	O
of	O
the	O
fact	O
that	O
variations	O
in	O
aj	O
give	O
rise	O
to	O
variations	O
in	O
the	O
error	B
func-	O
tion	O
only	O
through	O
variations	O
in	O
the	O
variables	O
ak	O
.	O
if	O
we	O
now	O
substitute	O
the	O
deﬁnition	O
of	O
δ	O
given	O
by	O
(	O
5.51	O
)	O
into	O
(	O
5.55	O
)	O
,	O
and	O
make	O
use	O
of	O
(	O
5.48	O
)	O
and	O
(	O
5.49	O
)	O
,	O
we	O
obtain	O
the	O
following	O
backpropagation	B
formula	O
δj	O
=	O
h	O
(	O
cid:4	O
)	O
(	O
aj	O
)	O
wkjδk	O
k	O
(	O
5.56	O
)	O
which	O
tells	O
us	O
that	O
the	O
value	O
of	O
δ	O
for	O
a	O
particular	O
hidden	B
unit	I
can	O
be	O
obtained	O
by	O
propagating	O
the	O
δ	O
’	O
s	O
backwards	O
from	O
units	O
higher	O
up	O
in	O
the	O
network	O
,	O
as	O
illustrated	O
in	O
figure	O
5.7.	O
note	O
that	O
the	O
summation	O
in	O
(	O
5.56	O
)	O
is	O
taken	O
over	O
the	O
ﬁrst	O
index	O
on	O
wkj	O
(	O
corresponding	O
to	O
backward	O
propagation	O
of	O
information	O
through	O
the	O
network	O
)	O
,	O
whereas	O
in	O
the	O
forward	B
propagation	I
equation	O
(	O
5.10	O
)	O
it	O
is	O
taken	O
over	O
the	O
second	O
index	O
.	O
because	O
we	O
already	O
know	O
the	O
values	O
of	O
the	O
δ	O
’	O
s	O
for	O
the	O
output	O
units	O
,	O
it	O
follows	O
that	O
by	O
recursively	O
applying	O
(	O
5.56	O
)	O
we	O
can	O
evaluate	O
the	O
δ	O
’	O
s	O
for	O
all	O
of	O
the	O
hidden	O
units	O
in	O
a	O
feed-forward	O
network	O
,	O
regardless	O
of	O
its	O
topology	O
.	O
the	O
backpropagation	B
procedure	O
can	O
therefore	O
be	O
summarized	O
as	O
follows	O
.	O
error	B
backpropagation	I
1.	O
apply	O
an	O
input	O
vector	O
xn	O
to	O
the	O
network	O
and	O
forward	O
propagate	O
through	O
the	O
network	O
using	O
(	O
5.48	O
)	O
and	O
(	O
5.49	O
)	O
to	O
ﬁnd	O
the	O
activations	O
of	O
all	O
the	O
hidden	O
and	O
output	O
units	O
.	O
2.	O
evaluate	O
the	O
δk	O
for	O
all	O
the	O
output	O
units	O
using	O
(	O
5.54	O
)	O
.	O
3.	O
backpropagate	O
the	O
δ	O
’	O
s	O
using	O
(	O
5.56	O
)	O
to	O
obtain	O
δj	O
for	O
each	O
hidden	B
unit	I
in	O
the	O
network	O
.	O
4.	O
use	O
(	O
5.53	O
)	O
to	O
evaluate	O
the	O
required	O
derivatives	O
.	O
5.3.	O
error	B
backpropagation	I
245	O
for	O
batch	O
methods	O
,	O
the	O
derivative	B
of	O
the	O
total	O
error	B
e	O
can	O
then	O
be	O
obtained	O
by	O
repeating	O
the	O
above	O
steps	O
for	O
each	O
pattern	O
in	O
the	O
training	B
set	I
and	O
then	O
summing	O
over	O
all	O
patterns	O
:	O
∂e	O
∂wji	O
=	O
∂en	O
∂wji	O
.	O
(	O
5.57	O
)	O
(	O
cid:2	O
)	O
n	O
in	O
the	O
above	O
derivation	O
we	O
have	O
implicitly	O
assumed	O
that	O
each	O
hidden	O
or	O
output	O
unit	O
in	O
the	O
network	O
has	O
the	O
same	O
activation	B
function	I
h	O
(	O
·	O
)	O
.	O
the	O
derivation	O
is	O
easily	O
general-	O
ized	O
,	O
however	O
,	O
to	O
allow	O
different	O
units	O
to	O
have	O
individual	O
activation	O
functions	O
,	O
simply	O
by	O
keeping	O
track	O
of	O
which	O
form	O
of	O
h	O
(	O
·	O
)	O
goes	O
with	O
which	O
unit	O
.	O
5.3.2	O
a	O
simple	O
example	O
the	O
above	O
derivation	O
of	O
the	O
backpropagation	B
procedure	O
allowed	O
for	O
general	O
forms	O
for	O
the	O
error	B
function	I
,	O
the	O
activation	O
functions	O
,	O
and	O
the	O
network	O
topology	O
.	O
in	O
order	O
to	O
illustrate	O
the	O
application	O
of	O
this	O
algorithm	O
,	O
we	O
shall	O
consider	O
a	O
particular	O
example	O
.	O
this	O
is	O
chosen	O
both	O
for	O
its	O
simplicity	O
and	O
for	O
its	O
practical	O
importance	O
,	O
be-	O
cause	O
many	O
applications	O
of	O
neural	O
networks	O
reported	O
in	O
the	O
literature	O
make	O
use	O
of	O
this	O
type	O
of	O
network	O
.	O
speciﬁcally	O
,	O
we	O
shall	O
consider	O
a	O
two-layer	O
network	O
of	O
the	O
form	O
illustrated	O
in	O
figure	O
5.1	O
,	O
together	O
with	O
a	O
sum-of-squares	B
error	I
,	O
in	O
which	O
the	O
output	O
units	O
have	O
linear	O
activation	O
functions	O
,	O
so	O
that	O
yk	O
=	O
ak	O
,	O
while	O
the	O
hidden	O
units	O
have	O
logistic	B
sigmoid	I
activation	O
functions	O
given	O
by	O
where	O
(	O
5.58	O
)	O
(	O
5.59	O
)	O
(	O
5.62	O
)	O
(	O
5.63	O
)	O
(	O
5.64	O
)	O
a	O
useful	O
feature	O
of	O
this	O
function	O
is	O
that	O
its	O
derivative	B
can	O
be	O
expressed	O
in	O
a	O
par-	O
ticularly	O
simple	O
form	O
:	O
(	O
5.60	O
)	O
we	O
also	O
consider	O
a	O
standard	O
sum-of-squares	O
error	B
function	I
,	O
so	O
that	O
for	O
pattern	O
n	O
the	O
error	B
is	O
given	O
by	O
h	O
(	O
cid:4	O
)	O
(	O
a	O
)	O
=	O
1	O
−	O
h	O
(	O
a	O
)	O
2	O
.	O
(	O
yk	O
−	O
tk	O
)	O
2	O
(	O
5.61	O
)	O
where	O
yk	O
is	O
the	O
activation	O
of	O
output	O
unit	O
k	O
,	O
and	O
tk	O
is	O
the	O
corresponding	O
target	O
,	O
for	O
a	O
particular	O
input	O
pattern	O
xn	O
.	O
for	O
each	O
pattern	O
in	O
the	O
training	B
set	I
in	O
turn	O
,	O
we	O
ﬁrst	O
perform	O
a	O
forward	B
propagation	I
using	O
aj	O
=	O
h	O
(	O
a	O
)	O
≡	O
tanh	O
(	O
a	O
)	O
tanh	O
(	O
a	O
)	O
=	O
ea	O
−	O
e	O
−a	O
ea	O
+	O
e−a	O
.	O
k	O
(	O
cid:2	O
)	O
k=1	O
en	O
=	O
1	O
2	O
d	O
(	O
cid:2	O
)	O
m	O
(	O
cid:2	O
)	O
i=0	O
j=0	O
(	O
1	O
)	O
ji	O
xi	O
w	O
zj	O
=	O
tanh	O
(	O
aj	O
)	O
yk	O
=	O
w	O
(	O
2	O
)	O
kj	O
zj	O
.	O
246	O
5.	O
neural	O
networks	O
next	O
we	O
compute	O
the	O
δ	O
’	O
s	O
for	O
each	O
output	O
unit	O
using	O
δk	O
=	O
yk	O
−	O
tk	O
.	O
then	O
we	O
backpropagate	O
these	O
to	O
obtain	O
δs	O
for	O
the	O
hidden	O
units	O
using	O
k	O
(	O
cid:2	O
)	O
δj	O
=	O
(	O
1	O
−	O
z2	O
j	O
)	O
wkjδk	O
.	O
k=1	O
(	O
5.65	O
)	O
(	O
5.66	O
)	O
finally	O
,	O
the	O
derivatives	O
with	O
respect	O
to	O
the	O
ﬁrst-layer	O
and	O
second-layer	O
weights	O
are	O
given	O
by	O
∂en	O
(	O
1	O
)	O
ji	O
∂w	O
=	O
δjxi	O
,	O
∂en	O
(	O
2	O
)	O
kj	O
∂w	O
=	O
δkzj	O
.	O
(	O
5.67	O
)	O
5.3.3	O
efﬁciency	O
of	O
backpropagation	B
one	O
of	O
the	O
most	O
important	O
aspects	O
of	O
backpropagation	B
is	O
its	O
computational	O
efﬁ-	O
ciency	O
.	O
to	O
understand	O
this	O
,	O
let	O
us	O
examine	O
how	O
the	O
number	O
of	O
computer	O
operations	O
required	O
to	O
evaluate	O
the	O
derivatives	O
of	O
the	O
error	B
function	I
scales	O
with	O
the	O
total	O
number	O
w	O
of	O
weights	O
and	O
biases	O
in	O
the	O
network	O
.	O
a	O
single	O
evaluation	O
of	O
the	O
error	B
function	I
(	O
for	O
a	O
given	O
input	O
pattern	O
)	O
would	O
require	O
o	O
(	O
w	O
)	O
operations	O
,	O
for	O
sufﬁciently	O
large	O
w	O
.	O
this	O
follows	O
from	O
the	O
fact	O
that	O
,	O
except	O
for	O
a	O
network	O
with	O
very	O
sparse	O
connections	O
,	O
the	O
number	O
of	O
weights	O
is	O
typically	O
much	O
greater	O
than	O
the	O
number	O
of	O
units	O
,	O
and	O
so	O
the	O
bulk	O
of	O
the	O
computational	O
effort	O
in	O
forward	B
propagation	I
is	O
concerned	O
with	O
evaluat-	O
ing	O
the	O
sums	O
in	O
(	O
5.48	O
)	O
,	O
with	O
the	O
evaluation	O
of	O
the	O
activation	O
functions	O
representing	O
a	O
small	O
overhead	O
.	O
each	O
term	O
in	O
the	O
sum	O
in	O
(	O
5.48	O
)	O
requires	O
one	O
multiplication	O
and	O
one	O
addition	O
,	O
leading	O
to	O
an	O
overall	O
computational	O
cost	O
that	O
is	O
o	O
(	O
w	O
)	O
.	O
an	O
alternative	O
approach	O
to	O
backpropagation	B
for	O
computing	O
the	O
derivatives	O
of	O
the	O
error	B
function	I
is	O
to	O
use	O
ﬁnite	B
differences	I
.	O
this	O
can	O
be	O
done	O
by	O
perturbing	O
each	O
weight	O
in	O
turn	O
,	O
and	O
approximating	O
the	O
derivatives	O
by	O
the	O
expression	O
=	O
en	O
(	O
wji	O
+	O
	O
)	O
−	O
en	O
(	O
wji	O
)	O
∂en	O
∂wji	O
(	O
5.68	O
)	O
where	O
	O
(	O
cid:13	O
)	O
1.	O
in	O
a	O
software	O
simulation	O
,	O
the	O
accuracy	O
of	O
the	O
approximation	O
to	O
the	O
derivatives	O
can	O
be	O
improved	O
by	O
making	O
	O
smaller	O
,	O
until	O
numerical	O
roundoff	O
problems	O
arise	O
.	O
the	O
accuracy	O
of	O
the	O
ﬁnite	B
differences	I
method	O
can	O
be	O
improved	O
signiﬁcantly	O
by	O
using	O
symmetrical	O
central	B
differences	I
of	O
the	O
form	O
	O
+	O
o	O
(	O
	O
)	O
=	O
en	O
(	O
wji	O
+	O
	O
)	O
−	O
en	O
(	O
wji	O
−	O
	O
)	O
2	O
∂en	O
∂wji	O
+	O
o	O
(	O
2	O
)	O
.	O
(	O
5.69	O
)	O
exercise	O
5.14	O
in	O
this	O
case	O
,	O
the	O
o	O
(	O
	O
)	O
corrections	O
cancel	O
,	O
as	O
can	O
be	O
veriﬁed	O
by	O
taylor	O
expansion	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
5.69	O
)	O
,	O
and	O
so	O
the	O
residual	O
corrections	O
are	O
o	O
(	O
2	O
)	O
.	O
the	O
number	O
of	O
computational	O
steps	O
is	O
,	O
however	O
,	O
roughly	O
doubled	O
compared	O
with	O
(	O
5.68	O
)	O
.	O
the	O
main	O
problem	O
with	O
numerical	O
differentiation	O
is	O
that	O
the	O
highly	O
desirable	O
o	O
(	O
w	O
)	O
scaling	O
has	O
been	O
lost	O
.	O
each	O
forward	B
propagation	I
requires	O
o	O
(	O
w	O
)	O
steps	O
,	O
and	O
5.3.	O
error	B
backpropagation	I
247	O
figure	O
5.8	O
illustration	O
of	O
a	O
modular	O
pattern	O
recognition	O
system	O
in	O
which	O
the	O
jacobian	O
matrix	O
can	O
be	O
used	O
to	O
backpropagate	O
error	B
signals	O
from	O
the	O
outputs	O
through	O
to	O
ear-	O
lier	O
modules	O
in	O
the	O
system	O
.	O
u	O
x	O
v	O
z	O
w	O
y	O
there	O
are	O
w	O
weights	O
in	O
the	O
network	O
each	O
of	O
which	O
must	O
be	O
perturbed	O
individually	O
,	O
so	O
that	O
the	O
overall	O
scaling	O
is	O
o	O
(	O
w	O
2	O
)	O
.	O
however	O
,	O
numerical	O
differentiation	O
plays	O
an	O
important	O
role	O
in	O
practice	O
,	O
because	O
a	O
comparison	O
of	O
the	O
derivatives	O
calculated	O
by	O
backpropagation	B
with	O
those	O
obtained	O
us-	O
ing	O
central	O
differences	O
provides	O
a	O
powerful	O
check	O
on	O
the	O
correctness	O
of	O
any	O
software	O
implementation	O
of	O
the	O
backpropagation	B
algorithm	O
.	O
when	O
training	B
networks	O
in	O
prac-	O
tice	O
,	O
derivatives	O
should	O
be	O
evaluated	O
using	O
backpropagation	B
,	O
because	O
this	O
gives	O
the	O
greatest	O
accuracy	O
and	O
numerical	O
efﬁciency	O
.	O
however	O
,	O
the	O
results	O
should	O
be	O
compared	O
with	O
numerical	O
differentiation	O
using	O
(	O
5.69	O
)	O
for	O
some	O
test	O
cases	O
in	O
order	O
to	O
check	O
the	O
correctness	O
of	O
the	O
implementation	O
.	O
5.3.4	O
the	O
jacobian	O
matrix	O
we	O
have	O
seen	O
how	O
the	O
derivatives	O
of	O
an	O
error	B
function	I
with	O
respect	O
to	O
the	O
weights	O
can	O
be	O
obtained	O
by	O
the	O
propagation	O
of	O
errors	O
backwards	O
through	O
the	O
network	O
.	O
the	O
technique	O
of	O
backpropagation	B
can	O
also	O
be	O
applied	O
to	O
the	O
calculation	O
of	O
other	O
deriva-	O
tives	O
.	O
here	O
we	O
consider	O
the	O
evaluation	O
of	O
the	O
jacobian	O
matrix	O
,	O
whose	O
elements	O
are	O
given	O
by	O
the	O
derivatives	O
of	O
the	O
network	O
outputs	O
with	O
respect	O
to	O
the	O
inputs	O
jki	O
≡	O
∂yk	O
∂xi	O
(	O
5.70	O
)	O
where	O
each	O
such	O
derivative	B
is	O
evaluated	O
with	O
all	O
other	O
inputs	O
held	O
ﬁxed	O
.	O
jacobian	O
matrices	O
play	O
a	O
useful	O
role	O
in	O
systems	O
built	O
from	O
a	O
number	O
of	O
distinct	O
modules	O
,	O
as	O
illustrated	O
in	O
figure	O
5.8.	O
each	O
module	O
can	O
comprise	O
a	O
ﬁxed	O
or	O
adaptive	O
function	O
,	O
which	O
can	O
be	O
linear	O
or	O
nonlinear	O
,	O
so	O
long	O
as	O
it	O
is	O
differentiable	O
.	O
suppose	O
we	O
wish	O
to	O
minimize	O
an	O
error	B
function	I
e	O
with	O
respect	O
to	O
the	O
parameter	O
w	O
in	O
figure	O
5.8.	O
the	O
derivative	B
of	O
the	O
error	B
function	I
is	O
given	O
by	O
(	O
cid:2	O
)	O
k	O
,	O
j	O
∂e	O
∂w	O
=	O
∂e	O
∂yk	O
∂yk	O
∂zj	O
∂zj	O
∂w	O
(	O
5.71	O
)	O
in	O
which	O
the	O
jacobian	O
matrix	O
for	O
the	O
red	O
module	O
in	O
figure	O
5.8	O
appears	O
in	O
the	O
middle	O
term	O
.	O
because	O
the	O
jacobian	O
matrix	O
provides	O
a	O
measure	O
of	O
the	O
local	B
sensitivity	O
of	O
the	O
outputs	O
to	O
changes	O
in	O
each	O
of	O
the	O
input	O
variables	O
,	O
it	O
also	O
allows	O
any	O
known	O
errors	O
∆xi	O
248	O
5.	O
neural	O
networks	O
(	O
cid:2	O
)	O
associated	O
with	O
the	O
inputs	O
to	O
be	O
propagated	O
through	O
the	O
trained	O
network	O
in	O
order	O
to	O
estimate	O
their	O
contribution	O
∆yk	O
to	O
the	O
errors	O
at	O
the	O
outputs	O
,	O
through	O
the	O
relation	O
∆yk	O
(	O
cid:7	O
)	O
∂yk	O
∂xi	O
∆xi	O
(	O
5.72	O
)	O
which	O
is	O
valid	O
provided	O
the	O
|∆xi|	O
are	O
small	O
.	O
in	O
general	O
,	O
the	O
network	O
mapping	O
rep-	O
resented	O
by	O
a	O
trained	O
neural	B
network	I
will	O
be	O
nonlinear	O
,	O
and	O
so	O
the	O
elements	O
of	O
the	O
jacobian	O
matrix	O
will	O
not	O
be	O
constants	O
but	O
will	O
depend	O
on	O
the	O
particular	O
input	O
vector	O
used	O
.	O
thus	O
(	O
5.72	O
)	O
is	O
valid	O
only	O
for	O
small	O
perturbations	O
of	O
the	O
inputs	O
,	O
and	O
the	O
jacobian	O
itself	O
must	O
be	O
re-evaluated	O
for	O
each	O
new	O
input	O
vector	O
.	O
i	O
the	O
jacobian	O
matrix	O
can	O
be	O
evaluated	O
using	O
a	O
backpropagation	B
procedure	O
that	O
is	O
similar	O
to	O
the	O
one	O
derived	O
earlier	O
for	O
evaluating	O
the	O
derivatives	O
of	O
an	O
error	B
function	I
with	O
respect	O
to	O
the	O
weights	O
.	O
we	O
start	O
by	O
writing	O
the	O
element	O
jki	O
in	O
the	O
form	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
j	O
j	O
jki	O
=	O
∂yk	O
∂xi	O
=	O
=	O
∂yk	O
∂aj	O
∂aj	O
∂xi	O
wji	O
∂yk	O
∂aj	O
(	O
5.73	O
)	O
where	O
we	O
have	O
made	O
use	O
of	O
(	O
5.48	O
)	O
.	O
the	O
sum	O
in	O
(	O
5.73	O
)	O
runs	O
over	O
all	O
units	O
j	O
to	O
which	O
the	O
input	O
unit	O
i	O
sends	O
connections	O
(	O
for	O
example	O
,	O
over	O
all	O
units	O
in	O
the	O
ﬁrst	O
hidden	O
layer	O
in	O
the	O
layered	O
topology	O
considered	O
earlier	O
)	O
.	O
we	O
now	O
write	O
down	O
a	O
recursive	O
backpropagation	B
formula	O
to	O
determine	O
the	O
derivatives	O
∂yk/∂aj	O
(	O
cid:2	O
)	O
∂yk	O
∂aj	O
=	O
∂al	O
∂aj	O
(	O
cid:2	O
)	O
∂yk	O
∂al	O
(	O
cid:4	O
)	O
(	O
aj	O
)	O
l	O
l	O
=	O
h	O
wlj	O
∂yk	O
∂al	O
(	O
5.74	O
)	O
where	O
the	O
sum	O
runs	O
over	O
all	O
units	O
l	O
to	O
which	O
unit	O
j	O
sends	O
connections	O
(	O
corresponding	O
to	O
the	O
ﬁrst	O
index	O
of	O
wlj	O
)	O
.	O
again	O
,	O
we	O
have	O
made	O
use	O
of	O
(	O
5.48	O
)	O
and	O
(	O
5.49	O
)	O
.	O
this	O
backpropagation	B
starts	O
at	O
the	O
output	O
units	O
for	O
which	O
the	O
required	O
derivatives	O
can	O
be	O
found	O
directly	O
from	O
the	O
functional	B
form	O
of	O
the	O
output-unit	O
activation	B
function	I
.	O
for	O
instance	O
,	O
if	O
we	O
have	O
individual	O
sigmoidal	O
activation	O
functions	O
at	O
each	O
output	O
unit	O
,	O
then	O
∂yk	O
∂aj	O
whereas	O
for	O
softmax	O
outputs	O
we	O
have	O
=	O
δkjσ	O
(	O
cid:4	O
)	O
(	O
aj	O
)	O
(	O
5.75	O
)	O
(	O
5.76	O
)	O
=	O
δkjyk	O
−	O
ykyj	O
.	O
∂yk	O
∂aj	O
we	O
can	O
summarize	O
the	O
procedure	O
for	O
evaluating	O
the	O
jacobian	O
matrix	O
as	O
follows	O
.	O
apply	O
the	O
input	O
vector	O
corresponding	O
to	O
the	O
point	O
in	O
input	O
space	O
at	O
which	O
the	O
ja-	O
cobian	O
matrix	O
is	O
to	O
be	O
found	O
,	O
and	O
forward	O
propagate	O
in	O
the	O
usual	O
way	O
to	O
obtain	O
the	O
5.4.	O
the	O
hessian	O
matrix	O
249	O
activations	O
of	O
all	O
of	O
the	O
hidden	O
and	O
output	O
units	O
in	O
the	O
network	O
.	O
next	O
,	O
for	O
each	O
row	O
k	O
of	O
the	O
jacobian	O
matrix	O
,	O
corresponding	O
to	O
the	O
output	O
unit	O
k	O
,	O
backpropagate	O
using	O
the	O
recursive	O
relation	O
(	O
5.74	O
)	O
,	O
starting	O
with	O
(	O
5.75	O
)	O
or	O
(	O
5.76	O
)	O
,	O
for	O
all	O
of	O
the	O
hidden	O
units	O
in	O
the	O
network	O
.	O
finally	O
,	O
use	O
(	O
5.73	O
)	O
to	O
do	O
the	O
backpropagation	B
to	O
the	O
inputs	O
.	O
the	O
jacobian	O
can	O
also	O
be	O
evaluated	O
using	O
an	O
alternative	O
forward	B
propagation	I
formalism	O
,	O
which	O
can	O
be	O
derived	O
in	O
an	O
analogous	O
way	O
to	O
the	O
backpropagation	B
approach	O
given	O
here	O
.	O
again	O
,	O
the	O
implementation	O
of	O
such	O
algorithms	O
can	O
be	O
checked	O
by	O
using	O
numeri-	O
exercise	O
5.15	O
cal	O
differentiation	O
in	O
the	O
form	O
=	O
yk	O
(	O
xi	O
+	O
	O
)	O
−	O
yk	O
(	O
xi	O
−	O
	O
)	O
2	O
∂yk	O
∂xi	O
+	O
o	O
(	O
2	O
)	O
(	O
5.77	O
)	O
which	O
involves	O
2d	O
forward	O
propagations	O
for	O
a	O
network	O
having	O
d	O
inputs	O
.	O
5.4.	O
the	O
hessian	O
matrix	O
we	O
have	O
shown	O
how	O
the	O
technique	O
of	O
backpropagation	B
can	O
be	O
used	O
to	O
obtain	O
the	O
ﬁrst	O
derivatives	O
of	O
an	O
error	B
function	I
with	O
respect	O
to	O
the	O
weights	O
in	O
the	O
network	O
.	O
back-	O
propagation	O
can	O
also	O
be	O
used	O
to	O
evaluate	O
the	O
second	O
derivatives	O
of	O
the	O
error	B
,	O
given	O
by	O
∂2e	O
∂wji∂wlk	O
.	O
(	O
5.78	O
)	O
note	O
that	O
it	O
is	O
sometimes	O
convenient	O
to	O
consider	O
all	O
of	O
the	O
weight	O
and	O
bias	B
parameters	O
as	O
elements	O
wi	O
of	O
a	O
single	O
vector	O
,	O
denoted	O
w	O
,	O
in	O
which	O
case	O
the	O
second	O
derivatives	O
form	O
the	O
elements	O
hij	O
of	O
the	O
hessian	O
matrix	O
h	O
,	O
where	O
i	O
,	O
j	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
w	O
}	O
and	O
w	O
is	O
the	O
total	O
number	O
of	O
weights	O
and	O
biases	O
.	O
the	O
hessian	O
plays	O
an	O
important	O
role	O
in	O
many	O
aspects	O
of	O
neural	O
computing	O
,	O
including	O
the	O
following	O
:	O
1.	O
several	O
nonlinear	O
optimization	O
algorithms	O
used	O
for	O
training	O
neural	O
networks	O
are	O
based	O
on	O
considerations	O
of	O
the	O
second-order	O
properties	O
of	O
the	O
error	B
surface	O
,	O
which	O
are	O
controlled	O
by	O
the	O
hessian	O
matrix	O
(	O
bishop	O
and	O
nabney	O
,	O
2008	O
)	O
.	O
2.	O
the	O
hessian	O
forms	O
the	O
basis	O
of	O
a	O
fast	O
procedure	O
for	O
re-training	O
a	O
feed-forward	O
network	O
following	O
a	O
small	O
change	O
in	O
the	O
training	B
data	O
(	O
bishop	O
,	O
1991	O
)	O
.	O
3.	O
the	O
inverse	B
of	O
the	O
hessian	O
has	O
been	O
used	O
to	O
identify	O
the	O
least	O
signiﬁcant	O
weights	O
in	O
a	O
network	O
as	O
part	O
of	O
network	O
‘	O
pruning	O
’	O
algorithms	O
(	O
le	O
cun	O
et	O
al.	O
,	O
1990	O
)	O
.	O
4.	O
the	O
hessian	O
plays	O
a	O
central	O
role	O
in	O
the	O
laplace	O
approximation	O
for	O
a	O
bayesian	O
neural	B
network	I
(	O
see	O
section	O
5.7	O
)	O
.	O
its	O
inverse	B
is	O
used	O
to	O
determine	O
the	O
predic-	O
tive	O
distribution	O
for	O
a	O
trained	O
network	O
,	O
its	O
eigenvalues	O
determine	O
the	O
values	O
of	O
hyperparameters	O
,	O
and	O
its	O
determinant	O
is	O
used	O
to	O
evaluate	O
the	O
model	B
evidence	I
.	O
various	O
approximation	O
schemes	O
have	O
been	O
used	O
to	O
evaluate	O
the	O
hessian	O
matrix	O
for	O
a	O
neural	B
network	I
.	O
however	O
,	O
the	O
hessian	O
can	O
also	O
be	O
calculated	O
exactly	O
using	O
an	O
extension	O
of	O
the	O
backpropagation	B
technique	O
.	O
250	O
5.	O
neural	O
networks	O
an	O
important	O
consideration	O
for	O
many	O
applications	O
of	O
the	O
hessian	O
is	O
the	O
efﬁciency	O
with	O
which	O
it	O
can	O
be	O
evaluated	O
.	O
if	O
there	O
are	O
w	O
parameters	O
(	O
weights	O
and	O
biases	O
)	O
in	O
the	O
network	O
,	O
then	O
the	O
hessian	O
matrix	O
has	O
dimensions	O
w	O
×	O
w	O
and	O
so	O
the	O
computational	O
effort	O
needed	O
to	O
evaluate	O
the	O
hessian	O
will	O
scale	O
like	O
o	O
(	O
w	O
2	O
)	O
for	O
each	O
pattern	O
in	O
the	O
data	O
set	O
.	O
as	O
we	O
shall	O
see	O
,	O
there	O
are	O
efﬁcient	O
methods	O
for	O
evaluating	O
the	O
hessian	O
whose	O
scaling	O
is	O
indeed	O
o	O
(	O
w	O
2	O
)	O
.	O
5.4.1	O
diagonal	B
approximation	I
some	O
of	O
the	O
applications	O
for	O
the	O
hessian	O
matrix	O
discussed	O
above	O
require	O
the	O
inverse	B
of	O
the	O
hessian	O
,	O
rather	O
than	O
the	O
hessian	O
itself	O
.	O
for	O
this	O
reason	O
,	O
there	O
has	O
been	O
some	O
interest	O
in	O
using	O
a	O
diagonal	B
approximation	I
to	O
the	O
hessian	O
,	O
in	O
other	O
words	O
one	O
that	O
simply	O
replaces	O
the	O
off-diagonal	O
elements	O
with	O
zeros	O
,	O
because	O
its	O
inverse	B
is	O
trivial	O
to	O
evaluate	O
.	O
again	O
,	O
we	O
shall	O
consider	O
an	O
error	B
function	I
that	O
consists	O
of	O
a	O
sum	O
of	O
terms	O
,	O
one	O
for	O
each	O
pattern	O
in	O
the	O
data	O
set	O
,	O
so	O
that	O
e	O
=	O
n	O
en	O
.	O
the	O
hessian	O
can	O
then	O
be	O
obtained	O
by	O
considering	O
one	O
pattern	O
at	O
a	O
time	O
,	O
and	O
then	O
summing	O
the	O
results	O
over	O
all	O
patterns	O
.	O
from	O
(	O
5.48	O
)	O
,	O
the	O
diagonal	B
elements	O
of	O
the	O
hessian	O
,	O
for	O
pattern	O
n	O
,	O
can	O
be	O
written	O
(	O
cid:5	O
)	O
∂2en	O
∂w2	O
ji	O
=	O
∂2en	O
∂a2	O
j	O
z2	O
i	O
.	O
(	O
5.79	O
)	O
using	O
(	O
5.48	O
)	O
and	O
(	O
5.49	O
)	O
,	O
the	O
second	O
derivatives	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
5.79	O
)	O
can	O
be	O
found	O
recursively	O
using	O
the	O
chain	O
rule	O
of	O
differential	B
calculus	O
to	O
give	O
a	O
backprop-	O
agation	O
equation	O
of	O
the	O
form	O
∂2en	O
∂a2	O
j	O
(	O
cid:4	O
)	O
(	O
aj	O
)	O
2	O
=	O
h	O
wkjwk	O
(	O
cid:1	O
)	O
j	O
∂2en	O
∂ak∂ak	O
(	O
cid:1	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
aj	O
)	O
+	O
h	O
wkj	O
∂en	O
∂ak	O
.	O
(	O
5.80	O
)	O
if	O
we	O
now	O
neglect	O
off-diagonal	O
elements	O
in	O
the	O
second-derivative	O
terms	O
,	O
we	O
obtain	O
(	O
becker	O
and	O
le	O
cun	O
,	O
1989	O
;	O
le	O
cun	O
et	O
al.	O
,	O
1990	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
k	O
k	O
(	O
cid:1	O
)	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
k	O
∂2en	O
∂a2	O
j	O
(	O
cid:4	O
)	O
(	O
aj	O
)	O
2	O
=	O
h	O
w2	O
kj	O
∂2en	O
∂a2	O
k	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
aj	O
)	O
+	O
h	O
wkj	O
∂en	O
∂ak	O
.	O
(	O
5.81	O
)	O
note	O
that	O
the	O
number	O
of	O
computational	O
steps	O
required	O
to	O
evaluate	O
this	O
approximation	O
is	O
o	O
(	O
w	O
)	O
,	O
where	O
w	O
is	O
the	O
total	O
number	O
of	O
weight	O
and	O
bias	B
parameters	O
in	O
the	O
network	O
,	O
compared	O
with	O
o	O
(	O
w	O
2	O
)	O
for	O
the	O
full	O
hessian	O
.	O
ricotti	O
et	O
al	O
.	O
(	O
1988	O
)	O
also	O
used	O
the	O
diagonal	B
approximation	I
to	O
the	O
hessian	O
,	O
but	O
they	O
retained	O
all	O
terms	O
in	O
the	O
evaluation	O
of	O
∂2en/∂a2	O
j	O
and	O
so	O
obtained	O
exact	O
expres-	O
sions	O
for	O
the	O
diagonal	B
terms	O
.	O
note	O
that	O
this	O
no	O
longer	O
has	O
o	O
(	O
w	O
)	O
scaling	O
.	O
the	O
major	O
problem	O
with	O
diagonal	B
approximations	O
,	O
however	O
,	O
is	O
that	O
in	O
practice	O
the	O
hessian	O
is	O
typically	O
found	O
to	O
be	O
strongly	O
nondiagonal	O
,	O
and	O
so	O
these	O
approximations	O
,	O
which	O
are	O
driven	O
mainly	O
be	O
computational	O
convenience	O
,	O
must	O
be	O
treated	O
with	O
care	O
.	O
5.4.	O
the	O
hessian	O
matrix	O
251	O
5.4.2	O
outer	B
product	I
approximation	I
when	O
neural	O
networks	O
are	O
applied	O
to	O
regression	B
problems	O
,	O
it	O
is	O
common	O
to	O
use	O
a	O
sum-of-squares	B
error	I
function	O
of	O
the	O
form	O
(	O
yn	O
−	O
tn	O
)	O
2	O
(	O
5.82	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
e	O
=	O
1	O
2	O
n	O
(	O
cid:2	O
)	O
exercise	O
5.20	O
an	O
analogous	O
result	O
can	O
be	O
obtained	O
for	O
multiclass	O
networks	O
having	O
softmax	O
output-	O
unit	O
activation	O
functions	O
.	O
n=1	O
yn	O
(	O
1	O
−	O
yn	O
)	O
bnbt	O
n.	O
(	O
5.85	O
)	O
exercise	O
5.16	O
exercise	O
5.17	O
exercise	O
5.19	O
n	O
(	O
cid:2	O
)	O
where	O
we	O
have	O
considered	O
the	O
case	O
of	O
a	O
single	O
output	O
in	O
order	O
to	O
keep	O
the	O
notation	O
simple	O
(	O
the	O
extension	O
to	O
several	O
outputs	O
is	O
straightforward	O
)	O
.	O
we	O
can	O
then	O
write	O
the	O
hessian	O
matrix	O
in	O
the	O
form	O
h	O
=	O
∇∇e	O
=	O
∇yn∇yn	O
+	O
(	O
yn	O
−	O
tn	O
)	O
∇∇yn	O
.	O
(	O
5.83	O
)	O
n=1	O
n=1	O
if	O
the	O
network	O
has	O
been	O
trained	O
on	O
the	O
data	O
set	O
,	O
and	O
its	O
outputs	O
yn	O
happen	O
to	O
be	O
very	O
close	O
to	O
the	O
target	O
values	O
tn	O
,	O
then	O
the	O
second	O
term	O
in	O
(	O
5.83	O
)	O
will	O
be	O
small	O
and	O
can	O
be	O
neglected	O
.	O
more	O
generally	O
,	O
however	O
,	O
it	O
may	O
be	O
appropriate	O
to	O
neglect	O
this	O
term	O
by	O
the	O
following	O
argument	O
.	O
recall	O
from	O
section	O
1.5.5	O
that	O
the	O
optimal	O
function	O
that	O
minimizes	O
a	O
sum-of-squares	O
loss	O
is	O
the	O
conditional	B
average	O
of	O
the	O
target	O
data	O
.	O
the	O
quantity	O
(	O
yn	O
−	O
tn	O
)	O
is	O
then	O
a	O
random	O
variable	O
with	O
zero	O
mean	B
.	O
if	O
we	O
assume	O
that	O
its	O
value	O
is	O
uncorrelated	O
with	O
the	O
value	O
of	O
the	O
second	O
derivative	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
5.83	O
)	O
,	O
then	O
the	O
whole	O
term	O
will	O
average	O
to	O
zero	O
in	O
the	O
summation	O
over	O
n.	O
by	O
neglecting	O
the	O
second	O
term	O
in	O
(	O
5.83	O
)	O
,	O
we	O
arrive	O
at	O
the	O
levenberg–marquardt	O
approximation	O
or	O
outer	B
product	I
approximation	I
(	O
because	O
the	O
hessian	O
matrix	O
is	O
built	O
up	O
from	O
a	O
sum	O
of	O
outer	O
products	O
of	O
vectors	O
)	O
,	O
given	O
by	O
bnbt	O
n	O
n=1	O
(	O
5.84	O
)	O
where	O
bn	O
=	O
∇yn	O
=	O
∇an	O
because	O
the	O
activation	B
function	I
for	O
the	O
output	O
units	O
is	O
simply	O
the	O
identity	O
.	O
evaluation	O
of	O
the	O
outer	B
product	I
approximation	I
for	O
the	O
hessian	O
is	O
straightforward	O
as	O
it	O
only	O
involves	O
ﬁrst	O
derivatives	O
of	O
the	O
error	B
function	I
,	O
which	O
can	O
be	O
evaluated	O
efﬁciently	O
in	O
o	O
(	O
w	O
)	O
steps	O
using	O
standard	O
backpropagation	O
.	O
the	O
elements	O
of	O
the	O
matrix	O
can	O
then	O
be	O
found	O
in	O
o	O
(	O
w	O
2	O
)	O
steps	O
by	O
simple	O
multiplication	O
.	O
it	O
is	O
important	O
to	O
emphasize	O
that	O
this	O
approximation	O
is	O
only	O
likely	O
to	O
be	O
valid	O
for	O
a	O
network	O
that	O
has	O
been	O
trained	O
appropriately	O
,	O
and	O
that	O
for	O
a	O
general	O
network	O
mapping	O
the	O
second	O
derivative	O
terms	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
5.83	O
)	O
will	O
typically	O
not	O
be	O
negligible	O
.	O
in	O
the	O
case	O
of	O
the	O
cross-entropy	B
error	I
function	I
for	O
a	O
network	O
with	O
logistic	B
sigmoid	I
output-unit	O
activation	O
functions	O
,	O
the	O
corresponding	O
approximation	O
is	O
given	O
by	O
h	O
(	O
cid:7	O
)	O
n	O
(	O
cid:2	O
)	O
h	O
(	O
cid:7	O
)	O
n	O
(	O
cid:2	O
)	O
252	O
5.	O
neural	O
networks	O
5.4.3	O
inverse	B
hessian	O
we	O
can	O
use	O
the	O
outer-product	O
approximation	O
to	O
develop	O
a	O
computationally	O
ef-	O
ﬁcient	O
procedure	O
for	O
approximating	O
the	O
inverse	B
of	O
the	O
hessian	O
(	O
hassibi	O
and	O
stork	O
,	O
1993	O
)	O
.	O
first	O
we	O
write	O
the	O
outer-product	O
approximation	O
in	O
matrix	O
notation	O
as	O
n	O
(	O
cid:2	O
)	O
bnbt	O
n	O
hn	O
=	O
(	O
5.86	O
)	O
where	O
bn	O
≡	O
∇wan	O
is	O
the	O
contribution	O
to	O
the	O
gradient	O
of	O
the	O
output	O
unit	O
activation	O
arising	O
from	O
data	O
point	O
n.	O
we	O
now	O
derive	O
a	O
sequential	O
procedure	O
for	O
building	O
up	O
the	O
hessian	O
by	O
including	O
data	O
points	O
one	O
at	O
a	O
time	O
.	O
suppose	O
we	O
have	O
already	O
obtained	O
the	O
inverse	B
hessian	O
using	O
the	O
ﬁrst	O
l	O
data	O
points	O
.	O
by	O
separating	O
off	O
the	O
contribution	O
from	O
data	O
point	O
l	O
+	O
1	O
,	O
we	O
obtain	O
n=1	O
in	O
order	O
to	O
evaluate	O
the	O
inverse	B
of	O
the	O
hessian	O
,	O
we	O
now	O
consider	O
the	O
matrix	O
identity	O
(	O
cid:10	O
)	O
m	O
+	O
vvt	O
l+1	O
.	O
hl+1	O
=	O
hl	O
+	O
bl+1bt	O
(	O
cid:11	O
)	O
−1	O
=	O
m−1	O
−	O
(	O
m−1v	O
)	O
(	O
cid:10	O
)	O
vtm−1	O
1	O
+	O
vtm−1v	O
(	O
cid:11	O
)	O
(	O
5.87	O
)	O
(	O
5.88	O
)	O
where	O
i	O
is	O
the	O
unit	O
matrix	O
,	O
which	O
is	O
simply	O
a	O
special	O
case	O
of	O
the	O
woodbury	O
identity	O
(	O
c.7	O
)	O
.	O
if	O
we	O
now	O
identify	O
hl	O
with	O
m	O
and	O
bl+1	O
with	O
v	O
,	O
we	O
obtain	O
−1	O
l+1	O
=	O
h	O
h	O
−1	O
−1	O
l	O
bl+1bt	O
l	O
−	O
h	O
l+1h	O
−1	O
l	O
−1	O
l+1h	O
l	O
bl+1	O
1	O
+	O
bt	O
.	O
(	O
5.89	O
)	O
exercise	O
5.21	O
in	O
this	O
way	O
,	O
data	O
points	O
are	O
sequentially	O
absorbed	O
until	O
l+1	O
=	O
n	O
and	O
the	O
whole	O
data	O
set	O
has	O
been	O
processed	O
.	O
this	O
result	O
therefore	O
represents	O
a	O
procedure	O
for	O
evaluating	O
the	O
inverse	B
of	O
the	O
hessian	O
using	O
a	O
single	O
pass	O
through	O
the	O
data	O
set	O
.	O
the	O
initial	O
matrix	O
h0	O
is	O
chosen	O
to	O
be	O
αi	O
,	O
where	O
α	O
is	O
a	O
small	O
quantity	O
,	O
so	O
that	O
the	O
algorithm	O
actually	O
ﬁnds	O
the	O
inverse	B
of	O
h	O
+	O
αi	O
.	O
the	O
results	O
are	O
not	O
particularly	O
sensitive	O
to	O
the	O
precise	O
value	O
of	O
α.	O
extension	O
of	O
this	O
algorithm	O
to	O
networks	O
having	O
more	O
than	O
one	O
output	O
is	O
straightforward	O
.	O
we	O
note	O
here	O
that	O
the	O
hessian	O
matrix	O
can	O
sometimes	O
be	O
calculated	O
indirectly	O
as	O
part	O
of	O
the	O
network	O
training	B
algorithm	O
.	O
in	O
particular	O
,	O
quasi-newton	O
nonlinear	O
opti-	O
mization	O
algorithms	O
gradually	O
build	O
up	O
an	O
approximation	O
to	O
the	O
inverse	B
of	O
the	O
hes-	O
sian	O
during	O
training	B
.	O
such	O
algorithms	O
are	O
discussed	O
in	O
detail	O
in	O
bishop	O
and	O
nabney	O
(	O
2008	O
)	O
.	O
5.4.4	O
finite	O
differences	O
as	O
in	O
the	O
case	O
of	O
the	O
ﬁrst	O
derivatives	O
of	O
the	O
error	B
function	I
,	O
we	O
can	O
ﬁnd	O
the	O
second	O
derivatives	O
by	O
using	O
ﬁnite	B
differences	I
,	O
with	O
accuracy	O
limited	O
by	O
numerical	O
precision	O
.	O
if	O
we	O
perturb	O
each	O
possible	O
pair	O
of	O
weights	O
in	O
turn	O
,	O
we	O
obtain	O
∂wji∂wlk	O
∂2e	O
=	O
1	O
42	O
{	O
e	O
(	O
wji	O
+	O
	O
,	O
wlk	O
+	O
	O
)	O
−	O
e	O
(	O
wji	O
+	O
	O
,	O
wlk	O
−	O
	O
)	O
−e	O
(	O
wji	O
−	O
	O
,	O
wlk	O
+	O
	O
)	O
+	O
e	O
(	O
wji	O
−	O
	O
,	O
wlk	O
−	O
	O
)	O
}	O
+	O
o	O
(	O
2	O
)	O
.	O
(	O
5.90	O
)	O
5.4.	O
the	O
hessian	O
matrix	O
253	O
again	O
,	O
by	O
using	O
a	O
symmetrical	O
central	B
differences	I
formulation	O
,	O
we	O
ensure	O
that	O
the	O
residual	O
errors	O
are	O
o	O
(	O
2	O
)	O
rather	O
than	O
o	O
(	O
	O
)	O
.	O
because	O
there	O
are	O
w	O
2	O
elements	O
in	O
the	O
hessian	O
matrix	O
,	O
and	O
because	O
the	O
evaluation	O
of	O
each	O
element	O
requires	O
four	O
forward	O
propagations	O
each	O
needing	O
o	O
(	O
w	O
)	O
operations	O
(	O
per	O
pattern	O
)	O
,	O
we	O
see	O
that	O
this	O
approach	O
will	O
require	O
o	O
(	O
w	O
3	O
)	O
operations	O
to	O
evaluate	O
the	O
complete	O
hessian	O
.	O
it	O
therefore	O
has	O
poor	O
scaling	O
properties	O
,	O
although	O
in	O
practice	O
it	O
is	O
very	O
useful	O
as	O
a	O
check	O
on	O
the	O
soft-	O
ware	O
implementation	O
of	O
backpropagation	B
methods	O
.	O
a	O
more	O
efﬁcient	O
version	O
of	O
numerical	O
differentiation	O
can	O
be	O
found	O
by	O
applying	O
central	B
differences	I
to	O
the	O
ﬁrst	O
derivatives	O
of	O
the	O
error	B
function	I
,	O
which	O
are	O
themselves	O
calculated	O
using	O
backpropagation	B
.	O
this	O
gives	O
(	O
cid:13	O
)	O
(	O
cid:12	O
)	O
∂2e	O
∂wji∂wlk	O
=	O
1	O
2	O
∂e	O
∂wji	O
(	O
wlk	O
+	O
	O
)	O
−	O
∂e	O
∂wji	O
(	O
wlk	O
−	O
	O
)	O
+	O
o	O
(	O
2	O
)	O
.	O
(	O
5.91	O
)	O
because	O
there	O
are	O
now	O
only	O
w	O
weights	O
to	O
be	O
perturbed	O
,	O
and	O
because	O
the	O
gradients	O
can	O
be	O
evaluated	O
in	O
o	O
(	O
w	O
)	O
steps	O
,	O
we	O
see	O
that	O
this	O
method	O
gives	O
the	O
hessian	O
in	O
o	O
(	O
w	O
2	O
)	O
operations	O
.	O
5.4.5	O
exact	B
evaluation	I
of	O
the	O
hessian	O
so	O
far	O
,	O
we	O
have	O
considered	O
various	O
approximation	O
schemes	O
for	O
evaluating	O
the	O
hessian	O
matrix	O
or	O
its	O
inverse	B
.	O
the	O
hessian	O
can	O
also	O
be	O
evaluated	O
exactly	O
,	O
for	O
a	O
net-	O
work	O
of	O
arbitrary	O
feed-forward	O
topology	O
,	O
using	O
extension	O
of	O
the	O
technique	O
of	O
back-	O
propagation	O
used	O
to	O
evaluate	O
ﬁrst	O
derivatives	O
,	O
which	O
shares	O
many	O
of	O
its	O
desirable	O
features	O
including	O
computational	O
efﬁciency	O
(	O
bishop	O
,	O
1991	O
;	O
bishop	O
,	O
1992	O
)	O
.	O
it	O
can	O
be	O
applied	O
to	O
any	O
differentiable	O
error	B
function	I
that	O
can	O
be	O
expressed	O
as	O
a	O
function	O
of	O
the	O
network	O
outputs	O
and	O
to	O
networks	O
having	O
arbitrary	O
differentiable	O
activation	O
func-	O
tions	O
.	O
the	O
number	O
of	O
computational	O
steps	O
needed	O
to	O
evaluate	O
the	O
hessian	O
scales	O
like	O
o	O
(	O
w	O
2	O
)	O
.	O
similar	O
algorithms	O
have	O
also	O
been	O
considered	O
by	O
buntine	O
and	O
weigend	O
(	O
1993	O
)	O
.	O
here	O
we	O
consider	O
the	O
speciﬁc	O
case	O
of	O
a	O
network	O
having	O
two	O
layers	O
of	O
weights	O
,	O
(	O
cid:4	O
)	O
for	O
which	O
the	O
required	O
equations	O
are	O
easily	O
derived	O
.	O
we	O
shall	O
use	O
indices	O
i	O
and	O
i	O
to	O
to	O
denote	O
inputs	O
,	O
indices	O
j	O
and	O
j	O
denote	O
outputs	O
.	O
we	O
ﬁrst	O
deﬁne	O
to	O
denoted	O
hidden	O
units	O
,	O
and	O
indices	O
k	O
and	O
k	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
δk	O
=	O
∂en	O
∂ak	O
,	O
mkk	O
(	O
cid:1	O
)	O
≡	O
∂2en	O
∂ak∂ak	O
(	O
cid:1	O
)	O
(	O
5.92	O
)	O
where	O
en	O
is	O
the	O
contribution	O
to	O
the	O
error	B
from	O
data	O
point	O
n.	O
the	O
hessian	O
matrix	O
for	O
this	O
network	O
can	O
then	O
be	O
considered	O
in	O
three	O
separate	O
blocks	O
as	O
follows	O
.	O
1.	O
both	O
weights	O
in	O
the	O
second	O
layer	O
:	O
exercise	O
5.22	O
∂2en	O
(	O
2	O
)	O
kj	O
∂w	O
(	O
2	O
)	O
k	O
(	O
cid:1	O
)	O
j	O
(	O
cid:1	O
)	O
∂w	O
=	O
zjzj	O
(	O
cid:1	O
)	O
mkk	O
(	O
cid:1	O
)	O
.	O
(	O
5.93	O
)	O
254	O
5.	O
neural	O
networks	O
2.	O
both	O
weights	O
in	O
the	O
ﬁrst	O
layer	O
:	O
∂w	O
∂2en	O
(	O
1	O
)	O
(	O
1	O
)	O
ji	O
∂w	O
j	O
(	O
cid:1	O
)	O
i	O
(	O
cid:1	O
)	O
+xixi	O
(	O
cid:1	O
)	O
h	O
3.	O
one	O
weight	O
in	O
each	O
layer	O
:	O
(	O
cid:4	O
)	O
(	O
aj	O
(	O
cid:1	O
)	O
)	O
h	O
(	O
cid:4	O
)	O
(	O
aj	O
)	O
(	O
cid:24	O
)	O
∂2en	O
(	O
1	O
)	O
ji	O
∂w	O
(	O
2	O
)	O
kj	O
(	O
cid:1	O
)	O
∂w	O
=	O
xih	O
(	O
cid:4	O
)	O
(	O
aj	O
(	O
cid:1	O
)	O
)	O
δkijj	O
(	O
cid:1	O
)	O
+	O
zj	O
=	O
xixi	O
(	O
cid:1	O
)	O
h	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
aj	O
(	O
cid:1	O
)	O
)	O
ijj	O
(	O
cid:1	O
)	O
w	O
(	O
2	O
)	O
kj	O
(	O
cid:1	O
)	O
δk	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
k	O
k	O
(	O
cid:1	O
)	O
w	O
(	O
2	O
)	O
k	O
(	O
cid:1	O
)	O
j	O
(	O
cid:1	O
)	O
w	O
(	O
2	O
)	O
kj	O
mkk	O
(	O
cid:1	O
)	O
.	O
(	O
5.94	O
)	O
(	O
cid:25	O
)	O
w	O
(	O
2	O
)	O
k	O
(	O
cid:1	O
)	O
j	O
(	O
cid:1	O
)	O
hkk	O
(	O
cid:1	O
)	O
.	O
(	O
5.95	O
)	O
(	O
cid:2	O
)	O
k	O
(	O
cid:1	O
)	O
exercise	O
5.23	O
(	O
cid:4	O
)	O
here	O
ijj	O
(	O
cid:1	O
)	O
is	O
the	O
j	O
,	O
j	O
element	O
of	O
the	O
identity	O
matrix	O
.	O
if	O
one	O
or	O
both	O
of	O
the	O
weights	O
is	O
a	O
bias	B
term	O
,	O
then	O
the	O
corresponding	O
expressions	O
are	O
obtained	O
simply	O
by	O
setting	O
the	O
appropriate	O
activation	O
(	O
s	O
)	O
to	O
1.	O
inclusion	O
of	O
skip-layer	O
connections	O
is	O
straightforward	O
.	O
5.4.6	O
fast	B
multiplication	I
by	O
the	O
hessian	O
for	O
many	O
applications	O
of	O
the	O
hessian	O
,	O
the	O
quantity	O
of	O
interest	O
is	O
not	O
the	O
hessian	O
matrix	O
h	O
itself	O
but	O
the	O
product	O
of	O
h	O
with	O
some	O
vector	O
v.	O
we	O
have	O
seen	O
that	O
the	O
evaluation	O
of	O
the	O
hessian	O
takes	O
o	O
(	O
w	O
2	O
)	O
operations	O
,	O
and	O
it	O
also	O
requires	O
storage	O
that	O
is	O
o	O
(	O
w	O
2	O
)	O
.	O
the	O
vector	O
vth	O
that	O
we	O
wish	O
to	O
calculate	O
,	O
however	O
,	O
has	O
only	O
w	O
elements	O
,	O
so	O
instead	O
of	O
computing	O
the	O
hessian	O
as	O
an	O
intermediate	O
step	O
,	O
we	O
can	O
instead	O
try	O
to	O
ﬁnd	O
an	O
efﬁcient	O
approach	O
to	O
evaluating	O
vth	O
directly	O
in	O
a	O
way	O
that	O
requires	O
only	O
o	O
(	O
w	O
)	O
operations	O
.	O
to	O
do	O
this	O
,	O
we	O
ﬁrst	O
note	O
that	O
vth	O
=	O
vt∇	O
(	O
∇e	O
)	O
(	O
5.96	O
)	O
where	O
∇	O
denotes	O
the	O
gradient	O
operator	O
in	O
weight	O
space	O
.	O
we	O
can	O
then	O
write	O
down	O
the	O
standard	O
forward-propagation	O
and	O
backpropagation	B
equations	O
for	O
the	O
evaluation	O
of	O
∇e	O
and	O
apply	O
(	O
5.96	O
)	O
to	O
these	O
equations	O
to	O
give	O
a	O
set	O
of	O
forward-propagation	O
and	O
backpropagation	B
equations	O
for	O
the	O
evaluation	O
of	O
vth	O
(	O
møller	O
,	O
1993	O
;	O
pearlmutter	O
,	O
1994	O
)	O
.	O
this	O
corresponds	O
to	O
acting	O
on	O
the	O
original	O
forward-propagation	O
and	O
back-	O
propagation	O
equations	O
with	O
a	O
differential	B
operator	O
vt∇	O
.	O
pearlmutter	O
(	O
1994	O
)	O
used	O
the	O
notation	O
r	O
{	O
·	O
}	O
to	O
denote	O
the	O
operator	O
vt∇	O
,	O
and	O
we	O
shall	O
follow	O
this	O
convention	O
.	O
the	O
analysis	O
is	O
straightforward	O
and	O
makes	O
use	O
of	O
the	O
usual	O
rules	O
of	O
differential	B
calculus	O
,	O
together	O
with	O
the	O
result	O
r	O
{	O
w	O
}	O
=	O
v.	O
(	O
5.97	O
)	O
the	O
technique	O
is	O
best	O
illustrated	O
with	O
a	O
simple	O
example	O
,	O
and	O
again	O
we	O
choose	O
a	O
two-layer	O
network	O
of	O
the	O
form	O
shown	O
in	O
figure	O
5.1	O
,	O
with	O
linear	O
output	O
units	O
and	O
a	O
sum-of-squares	B
error	I
function	O
.	O
as	O
before	O
,	O
we	O
consider	O
the	O
contribution	O
to	O
the	O
error	B
function	I
from	O
one	O
pattern	O
in	O
the	O
data	O
set	O
.	O
the	O
required	O
vector	O
is	O
then	O
obtained	O
as	O
5.4.	O
the	O
hessian	O
matrix	O
255	O
usual	O
by	O
summing	O
over	O
the	O
contributions	O
from	O
each	O
of	O
the	O
patterns	O
separately	O
.	O
for	O
the	O
two-layer	O
network	O
,	O
the	O
forward-propagation	O
equations	O
are	O
given	O
by	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
i	O
j	O
aj	O
=	O
wjixi	O
zj	O
=	O
h	O
(	O
aj	O
)	O
yk	O
=	O
wkjzj	O
.	O
(	O
5.98	O
)	O
(	O
5.99	O
)	O
(	O
5.100	O
)	O
we	O
now	O
act	O
on	O
these	O
equations	O
using	O
the	O
r	O
{	O
·	O
}	O
operator	O
to	O
obtain	O
a	O
set	O
of	O
forward	B
propagation	I
equations	O
in	O
the	O
form	O
r	O
{	O
aj	O
}	O
=	O
r	O
{	O
zj	O
}	O
=	O
h	O
r	O
{	O
yk	O
}	O
=	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
wkjr	O
{	O
zj	O
}	O
+	O
(	O
cid:4	O
)	O
(	O
aj	O
)	O
r	O
{	O
aj	O
}	O
(	O
cid:2	O
)	O
(	O
5.101	O
)	O
(	O
5.102	O
)	O
(	O
5.103	O
)	O
vkjzj	O
vjixi	O
i	O
j	O
j	O
where	O
vji	O
is	O
the	O
element	O
of	O
the	O
vector	O
v	O
that	O
corresponds	O
to	O
the	O
weight	O
wji	O
.	O
quan-	O
tities	O
of	O
the	O
form	O
r	O
{	O
zj	O
}	O
,	O
r	O
{	O
aj	O
}	O
and	O
r	O
{	O
yk	O
}	O
are	O
to	O
be	O
regarded	O
as	O
new	O
variables	O
whose	O
values	O
are	O
found	O
using	O
the	O
above	O
equations	O
.	O
because	O
we	O
are	O
considering	O
a	O
sum-of-squares	B
error	I
function	O
,	O
we	O
have	O
the	O
fol-	O
lowing	O
standard	O
backpropagation	O
expressions	O
:	O
δk	O
=	O
yk	O
−	O
tk	O
(	O
cid:4	O
)	O
(	O
aj	O
)	O
δj	O
=	O
h	O
(	O
cid:2	O
)	O
wkjδk	O
.	O
(	O
5.104	O
)	O
(	O
5.105	O
)	O
again	O
,	O
we	O
act	O
on	O
these	O
equations	O
with	O
the	O
r	O
{	O
·	O
}	O
operator	O
to	O
obtain	O
a	O
set	O
of	O
backprop-	O
agation	O
equations	O
in	O
the	O
form	O
k	O
(	O
cid:2	O
)	O
r	O
{	O
δk	O
}	O
=	O
r	O
{	O
yk	O
}	O
r	O
{	O
δj	O
}	O
=	O
h	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
aj	O
)	O
r	O
{	O
aj	O
}	O
wkjδk	O
k	O
vkjδk	O
+	O
h	O
(	O
cid:4	O
)	O
(	O
aj	O
)	O
(	O
cid:4	O
)	O
(	O
aj	O
)	O
+	O
h	O
(	O
cid:2	O
)	O
k	O
(	O
5.106	O
)	O
wkjr	O
{	O
δk	O
}	O
.	O
(	O
5.107	O
)	O
(	O
cid:2	O
)	O
k	O
finally	O
,	O
we	O
have	O
the	O
usual	O
equations	O
for	O
the	O
ﬁrst	O
derivatives	O
of	O
the	O
error	B
∂e	O
∂wkj	O
∂e	O
∂wji	O
=	O
δkzj	O
=	O
δjxi	O
(	O
5.108	O
)	O
(	O
5.109	O
)	O
256	O
5.	O
neural	O
networks	O
and	O
acting	O
on	O
these	O
with	O
the	O
r	O
{	O
·	O
}	O
operator	O
,	O
we	O
obtain	O
expressions	O
for	O
the	O
elements	O
of	O
the	O
vector	O
vth	O
=	O
r	O
{	O
δk	O
}	O
zj	O
+	O
δkr	O
{	O
zj	O
}	O
=	O
xir	O
{	O
δj	O
}	O
.	O
(	O
5.110	O
)	O
(	O
5.111	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
r	O
r	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
∂e	O
∂wkj	O
∂e	O
∂wji	O
the	O
implementation	O
of	O
this	O
algorithm	O
involves	O
the	O
introduction	O
of	O
additional	O
variables	O
r	O
{	O
aj	O
}	O
,	O
r	O
{	O
zj	O
}	O
and	O
r	O
{	O
δj	O
}	O
for	O
the	O
hidden	O
units	O
and	O
r	O
{	O
δk	O
}	O
and	O
r	O
{	O
yk	O
}	O
for	O
the	O
output	O
units	O
.	O
for	O
each	O
input	O
pattern	O
,	O
the	O
values	O
of	O
these	O
quantities	O
can	O
be	O
found	O
using	O
the	O
above	O
results	O
,	O
and	O
the	O
elements	O
of	O
vth	O
are	O
then	O
given	O
by	O
(	O
5.110	O
)	O
and	O
(	O
5.111	O
)	O
.	O
an	O
elegant	O
aspect	O
of	O
this	O
technique	O
is	O
that	O
the	O
equations	O
for	O
evaluating	O
vth	O
mirror	O
closely	O
those	O
for	O
standard	O
forward	O
and	O
backward	O
propagation	O
,	O
and	O
so	O
the	O
extension	O
of	O
existing	O
software	O
to	O
compute	O
this	O
product	O
is	O
typically	O
straightforward	O
.	O
if	O
desired	O
,	O
the	O
technique	O
can	O
be	O
used	O
to	O
evaluate	O
the	O
full	O
hessian	O
matrix	O
by	O
choosing	O
the	O
vector	O
v	O
to	O
be	O
given	O
successively	O
by	O
a	O
series	O
of	O
unit	O
vectors	O
of	O
the	O
form	O
(	O
0	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
0	O
)	O
each	O
of	O
which	O
picks	O
out	O
one	O
column	O
of	O
the	O
hessian	O
.	O
this	O
leads	O
to	O
a	O
formalism	O
that	O
is	O
analytically	O
equivalent	O
to	O
the	O
backpropagation	B
procedure	O
of	O
bishop	O
(	O
1992	O
)	O
,	O
as	O
described	O
in	O
section	O
5.4.5	O
,	O
though	O
with	O
some	O
loss	O
of	O
efﬁciency	O
due	O
to	O
redundant	O
calculations	O
.	O
5.5.	O
regularization	B
in	O
neural	O
networks	O
the	O
number	O
of	O
input	O
and	O
outputs	O
units	O
in	O
a	O
neural	B
network	I
is	O
generally	O
determined	O
by	O
the	O
dimensionality	O
of	O
the	O
data	O
set	O
,	O
whereas	O
the	O
number	O
m	O
of	O
hidden	O
units	O
is	O
a	O
free	O
parameter	O
that	O
can	O
be	O
adjusted	O
to	O
give	O
the	O
best	O
predictive	O
performance	O
.	O
note	O
that	O
m	O
controls	O
the	O
number	O
of	O
parameters	O
(	O
weights	O
and	O
biases	O
)	O
in	O
the	O
network	O
,	O
and	O
so	O
we	O
might	O
expect	O
that	O
in	O
a	O
maximum	B
likelihood	I
setting	O
there	O
will	O
be	O
an	O
optimum	O
value	O
of	O
m	O
that	O
gives	O
the	O
best	O
generalization	B
performance	O
,	O
corresponding	O
to	O
the	O
optimum	O
balance	O
between	O
under-ﬁtting	O
and	O
over-ﬁtting	B
.	O
figure	O
5.9	O
shows	O
an	O
example	O
of	O
the	O
effect	O
of	O
different	O
values	O
of	O
m	O
for	O
the	O
sinusoidal	O
regression	O
problem	O
.	O
the	O
generalization	B
error	O
,	O
however	O
,	O
is	O
not	O
a	O
simple	O
function	O
of	O
m	O
due	O
to	O
the	O
presence	O
of	O
local	B
minima	O
in	O
the	O
error	B
function	I
,	O
as	O
illustrated	O
in	O
figure	O
5.10.	O
here	O
we	O
see	O
the	O
effect	O
of	O
choosing	O
multiple	O
random	O
initializations	O
for	O
the	O
weight	B
vector	I
for	O
a	O
range	O
of	O
values	O
of	O
m.	O
the	O
overall	O
best	O
validation	B
set	I
performance	O
in	O
this	O
case	O
occurred	O
for	O
a	O
particular	O
solution	O
having	O
m	O
=	O
8.	O
in	O
practice	O
,	O
one	O
approach	O
to	O
choosing	O
m	O
is	O
in	O
fact	O
to	O
plot	O
a	O
graph	O
of	O
the	O
kind	O
shown	O
in	O
figure	O
5.10	O
and	O
then	O
to	O
choose	O
the	O
speciﬁc	O
solution	O
having	O
the	O
smallest	O
validation	B
set	I
error	O
.	O
there	O
are	O
,	O
however	O
,	O
other	O
ways	O
to	O
control	O
the	O
complexity	O
of	O
a	O
neural	B
network	I
model	O
in	O
order	O
to	O
avoid	O
over-ﬁtting	B
.	O
from	O
our	O
discussion	O
of	O
polynomial	B
curve	I
ﬁtting	I
in	O
chapter	O
1	O
,	O
we	O
see	O
that	O
an	O
alternative	O
approach	O
is	O
to	O
choose	O
a	O
relatively	O
large	O
value	O
for	O
m	O
and	O
then	O
to	O
control	O
complexity	O
by	O
the	O
addition	O
of	O
a	O
regularization	B
term	O
to	O
the	O
error	B
function	I
.	O
the	O
simplest	O
regularizer	O
is	O
the	O
quadratic	O
,	O
giving	O
a	O
regularized	O
error	O
5.5.	O
regularization	B
in	O
neural	O
networks	O
257	O
1	O
0	O
−1	O
m	O
=	O
1	O
1	O
0	O
−1	O
m	O
=	O
3	O
1	O
0	O
−1	O
m	O
=	O
10	O
0	O
1	O
0	O
1	O
0	O
1	O
figure	O
5.9	O
examples	O
of	O
two-layer	O
networks	O
trained	O
on	O
10	O
data	O
points	O
drawn	O
from	O
the	O
sinusoidal	B
data	I
set	O
.	O
the	O
graphs	O
show	O
the	O
result	O
of	O
ﬁtting	O
networks	O
having	O
m	O
=	O
1	O
,	O
3	O
and	O
10	O
hidden	O
units	O
,	O
respectively	O
,	O
by	O
minimizing	O
a	O
sum-of-squares	B
error	I
function	O
using	O
a	O
scaled	O
conjugate-gradient	O
algorithm	O
.	O
of	O
the	O
form	O
(	O
cid:4	O
)	O
e	O
(	O
w	O
)	O
=	O
e	O
(	O
w	O
)	O
+	O
λ	O
2	O
wtw	O
.	O
(	O
5.112	O
)	O
this	O
regularizer	O
is	O
also	O
known	O
as	O
weight	B
decay	I
and	O
has	O
been	O
discussed	O
at	O
length	O
in	O
chapter	O
3.	O
the	O
effective	O
model	O
complexity	O
is	O
then	O
determined	O
by	O
the	O
choice	O
of	O
the	O
regularization	B
coefﬁcient	O
λ.	O
as	O
we	O
have	O
seen	O
previously	O
,	O
this	O
regularizer	O
can	O
be	O
interpreted	O
as	O
the	O
negative	O
logarithm	O
of	O
a	O
zero-mean	O
gaussian	O
prior	B
distribution	O
over	O
the	O
weight	B
vector	I
w.	O
5.5.1	O
consistent	B
gaussian	O
priors	O
one	O
of	O
the	O
limitations	O
of	O
simple	O
weight	B
decay	I
in	O
the	O
form	O
(	O
5.112	O
)	O
is	O
that	O
is	O
inconsistent	O
with	O
certain	O
scaling	O
properties	O
of	O
network	O
mappings	O
.	O
to	O
illustrate	O
this	O
,	O
consider	O
a	O
multilayer	B
perceptron	I
network	O
having	O
two	O
layers	O
of	O
weights	O
and	O
linear	O
output	O
units	O
,	O
which	O
performs	O
a	O
mapping	O
from	O
a	O
set	O
of	O
input	O
variables	O
{	O
xi	O
}	O
to	O
a	O
set	O
of	O
output	O
variables	O
{	O
yk	O
}	O
.	O
the	O
activations	O
of	O
the	O
hidden	O
units	O
in	O
the	O
ﬁrst	O
hidden	O
layer	O
figure	O
5.10	O
plot	O
of	O
the	O
sum-of-squares	O
test-set	O
error	B
for	O
the	O
polynomial	O
data	O
set	O
ver-	O
sus	O
the	O
number	O
of	O
hidden	O
units	O
in	O
the	O
network	O
,	O
with	O
30	O
random	O
starts	O
for	O
each	O
network	O
size	O
,	O
showing	O
the	O
ef-	O
fect	O
of	O
local	B
minima	O
.	O
for	O
each	O
new	O
start	O
,	O
the	O
weight	B
vector	I
was	O
initial-	O
ized	O
by	O
sampling	O
from	O
an	O
isotropic	B
gaussian	O
distribution	O
having	O
a	O
mean	B
of	O
zero	O
and	O
a	O
variance	B
of	O
10	O
.	O
160	O
140	O
120	O
100	O
80	O
60	O
0	O
2	O
4	O
6	O
8	O
10	O
258	O
5.	O
neural	O
networks	O
take	O
the	O
form	O
zj	O
=	O
h	O
(	O
cid:23	O
)	O
wjixi	O
+	O
wj0	O
while	O
the	O
activations	O
of	O
the	O
output	O
units	O
are	O
given	O
by	O
(	O
cid:22	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
i	O
yk	O
=	O
wkjzj	O
+	O
wk0	O
.	O
suppose	O
we	O
perform	O
a	O
linear	O
transformation	O
of	O
the	O
input	O
data	O
of	O
the	O
form	O
j	O
xi	O
→	O
(	O
cid:4	O
)	O
xi	O
=	O
axi	O
+	O
b.	O
exercise	O
5.24	O
then	O
we	O
can	O
arrange	O
for	O
the	O
mapping	O
performed	O
by	O
the	O
network	O
to	O
be	O
unchanged	O
by	O
making	O
a	O
corresponding	O
linear	O
transformation	O
of	O
the	O
weights	O
and	O
biases	O
from	O
the	O
inputs	O
to	O
the	O
units	O
in	O
the	O
hidden	O
layer	O
of	O
the	O
form	O
(	O
5.113	O
)	O
(	O
5.114	O
)	O
(	O
5.115	O
)	O
(	O
5.116	O
)	O
(	O
5.117	O
)	O
(	O
5.118	O
)	O
(	O
5.119	O
)	O
(	O
5.120	O
)	O
wji	O
.	O
a	O
1	O
a	O
wji	O
wji	O
→	O
(	O
cid:4	O
)	O
wji	O
=	O
(	O
cid:2	O
)	O
wj0	O
→	O
(	O
cid:4	O
)	O
wj0	O
=	O
wj0	O
−	O
b	O
yk	O
→	O
(	O
cid:4	O
)	O
yk	O
=	O
cyk	O
+	O
d	O
wkj	O
→	O
(	O
cid:4	O
)	O
wkj	O
=	O
cwkj	O
wk0	O
→	O
(	O
cid:4	O
)	O
wk0	O
=	O
cwk0	O
+	O
d.	O
i	O
similarly	O
,	O
a	O
linear	O
transformation	O
of	O
the	O
output	O
variables	O
of	O
the	O
network	O
of	O
the	O
form	O
can	O
be	O
achieved	O
by	O
making	O
a	O
transformation	O
of	O
the	O
second-layer	O
weights	O
and	O
biases	O
using	O
if	O
we	O
train	O
one	O
network	O
using	O
the	O
original	O
data	O
and	O
one	O
network	O
using	O
data	O
for	O
which	O
the	O
input	O
and/or	O
target	O
variables	O
are	O
transformed	O
by	O
one	O
of	O
the	O
above	O
linear	O
transfor-	O
mations	O
,	O
then	O
consistency	O
requires	O
that	O
we	O
should	O
obtain	O
equivalent	O
networks	O
that	O
differ	O
only	O
by	O
the	O
linear	O
transformation	O
of	O
the	O
weights	O
as	O
given	O
.	O
any	O
regularizer	O
should	O
be	O
consistent	B
with	O
this	O
property	O
,	O
otherwise	O
it	O
arbitrarily	O
favours	O
one	O
solution	O
over	O
another	O
,	O
equivalent	O
one	O
.	O
clearly	O
,	O
simple	O
weight	B
decay	I
(	O
5.112	O
)	O
,	O
that	O
treats	O
all	O
weights	O
and	O
biases	O
on	O
an	O
equal	O
footing	O
,	O
does	O
not	O
satisfy	O
this	O
property	O
.	O
we	O
therefore	O
look	O
for	O
a	O
regularizer	O
which	O
is	O
invariant	O
under	O
the	O
linear	O
trans-	O
formations	O
(	O
5.116	O
)	O
,	O
(	O
5.117	O
)	O
,	O
(	O
5.119	O
)	O
and	O
(	O
5.120	O
)	O
.	O
these	O
require	O
that	O
the	O
regularizer	O
should	O
be	O
invariant	O
to	O
re-scaling	O
of	O
the	O
weights	O
and	O
to	O
shifts	O
of	O
the	O
biases	O
.	O
such	O
a	O
regularizer	O
is	O
given	O
by	O
(	O
5.121	O
)	O
where	O
w1	O
denotes	O
the	O
set	O
of	O
weights	O
in	O
the	O
ﬁrst	O
layer	O
,	O
w2	O
denotes	O
the	O
set	O
of	O
weights	O
in	O
the	O
second	O
layer	O
,	O
and	O
biases	O
are	O
excluded	O
from	O
the	O
summations	O
.	O
this	O
regularizer	O
w∈w2	O
w∈w1	O
w2	O
w2	O
+	O
λ2	O
2	O
(	O
cid:2	O
)	O
λ1	O
2	O
(	O
cid:2	O
)	O
p	O
(	O
w	O
)	O
∝	O
exp	O
αk	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
2	O
k	O
(	O
cid:23	O
)	O
(	O
cid:22	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
k	O
−1	O
2	O
w2	O
j	O
.	O
j∈wk	O
(	O
5.123	O
)	O
(	O
5.124	O
)	O
5.5.	O
regularization	B
in	O
neural	O
networks	O
259	O
will	O
remain	O
unchanged	O
under	O
the	O
weight	O
transformations	O
provided	O
the	O
regularization	B
parameters	O
are	O
re-scaled	O
using	O
λ1	O
→	O
a1/2λ1	O
and	O
λ2	O
→	O
c	O
(	O
cid:2	O
)	O
the	O
regularizer	O
(	O
5.121	O
)	O
corresponds	O
to	O
a	O
prior	B
of	O
the	O
form	O
(	O
cid:2	O
)	O
−1/2λ2	O
.	O
(	O
cid:22	O
)	O
(	O
cid:23	O
)	O
p	O
(	O
w|α1	O
,	O
α2	O
)	O
∝	O
exp	O
−	O
α1	O
2	O
w2	O
−	O
α2	O
2	O
w∈w1	O
w∈w2	O
w2	O
.	O
(	O
5.122	O
)	O
note	O
that	O
priors	O
of	O
this	O
form	O
are	O
improper	B
(	O
they	O
can	O
not	O
be	O
normalized	O
)	O
because	O
the	O
bias	B
parameters	O
are	O
unconstrained	O
.	O
the	O
use	O
of	O
improper	B
priors	O
can	O
lead	O
to	O
difﬁculties	O
in	O
selecting	O
regularization	B
coefﬁcients	O
and	O
in	O
model	B
comparison	I
within	O
the	O
bayesian	O
framework	O
,	O
because	O
the	O
corresponding	O
evidence	O
is	O
zero	O
.	O
it	O
is	O
therefore	O
common	O
to	O
include	O
separate	O
priors	O
for	O
the	O
biases	O
(	O
which	O
then	O
break	O
shift	O
invariance	B
)	O
having	O
their	O
own	O
hyperparameters	O
.	O
we	O
can	O
illustrate	O
the	O
effect	O
of	O
the	O
resulting	O
four	O
hyperpa-	O
rameters	O
by	O
drawing	O
samples	O
from	O
the	O
prior	B
and	O
plotting	O
the	O
corresponding	O
network	O
functions	O
,	O
as	O
shown	O
in	O
figure	O
5.11.	O
any	O
number	O
of	O
groups	O
wk	O
so	O
that	O
more	O
generally	O
,	O
we	O
can	O
consider	O
priors	O
in	O
which	O
the	O
weights	O
are	O
divided	O
into	O
where	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
2	O
k	O
=	O
as	O
a	O
special	O
case	O
of	O
this	O
prior	B
,	O
if	O
we	O
choose	O
the	O
groups	O
to	O
correspond	O
to	O
the	O
sets	O
of	O
weights	O
associated	O
with	O
each	O
of	O
the	O
input	O
units	O
,	O
and	O
we	O
optimize	O
the	O
marginal	B
likelihood	I
with	O
respect	O
to	O
the	O
corresponding	O
parameters	O
αk	O
,	O
we	O
obtain	O
automatic	B
relevance	I
determination	I
as	O
discussed	O
in	O
section	O
7.2.2	O
.	O
5.5.2	O
early	B
stopping	I
an	O
alternative	O
to	O
regularization	B
as	O
a	O
way	O
of	O
controlling	O
the	O
effective	O
complexity	O
of	O
a	O
network	O
is	O
the	O
procedure	O
of	O
early	B
stopping	I
.	O
the	O
training	B
of	O
nonlinear	O
network	O
models	O
corresponds	O
to	O
an	O
iterative	O
reduction	O
of	O
the	O
error	B
function	I
deﬁned	O
with	O
re-	O
spect	O
to	O
a	O
set	O
of	O
training	B
data	O
.	O
for	O
many	O
of	O
the	O
optimization	O
algorithms	O
used	O
for	O
network	O
training	B
,	O
such	O
as	O
conjugate	B
gradients	O
,	O
the	O
error	B
is	O
a	O
nonincreasing	O
function	O
of	O
the	O
iteration	O
index	O
.	O
however	O
,	O
the	O
error	B
measured	O
with	O
respect	O
to	O
independent	B
data	O
,	O
generally	O
called	O
a	O
validation	B
set	I
,	O
often	O
shows	O
a	O
decrease	O
at	O
ﬁrst	O
,	O
followed	O
by	O
an	O
in-	O
crease	O
as	O
the	O
network	O
starts	O
to	O
over-ﬁt	O
.	O
training	B
can	O
therefore	O
be	O
stopped	O
at	O
the	O
point	O
of	O
smallest	O
error	B
with	O
respect	O
to	O
the	O
validation	O
data	O
set	O
,	O
as	O
indicated	O
in	O
figure	O
5.12	O
,	O
in	O
order	O
to	O
obtain	O
a	O
network	O
having	O
good	O
generalization	B
performance	O
.	O
the	O
behaviour	O
of	O
the	O
network	O
in	O
this	O
case	O
is	O
sometimes	O
explained	O
qualitatively	O
in	O
terms	O
of	O
the	O
effective	O
number	O
of	O
degrees	O
of	O
freedom	O
in	O
the	O
network	O
,	O
in	O
which	O
this	O
number	O
starts	O
out	O
small	O
and	O
then	O
to	O
grows	O
during	O
the	O
training	B
process	O
,	O
corresponding	O
to	O
a	O
steady	O
increase	O
in	O
the	O
effective	O
complexity	O
of	O
the	O
model	O
.	O
halting	O
training	B
before	O
260	O
5.	O
neural	O
networks	O
αw	O
1	O
=	O
1	O
,	O
αb	O
1	O
=	O
1	O
,	O
αw	O
2	O
=	O
1	O
,	O
αb	O
2	O
=	O
1	O
−0.5	O
0	O
0.5	O
1	O
αw	O
1	O
=	O
1000	O
,	O
αb	O
1	O
=	O
100	O
,	O
αw	O
2	O
=	O
1	O
,	O
αb	O
2	O
=	O
1	O
−0.5	O
0	O
0.5	O
1	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−1	O
5	O
0	O
−5	O
−10	O
−1	O
40	O
20	O
0	O
−20	O
−40	O
−60	O
−1	O
5	O
0	O
−5	O
−10	O
−1	O
αw	O
1	O
=	O
1	O
,	O
αb	O
1	O
=	O
1	O
,	O
αw	O
2	O
=	O
10	O
,	O
αb	O
2	O
=	O
1	O
−0.5	O
0	O
0.5	O
1	O
αw	O
1	O
=	O
1000	O
,	O
αb	O
1	O
=	O
1000	O
,	O
αw	O
2	O
=	O
1	O
,	O
αb	O
2	O
=	O
1	O
−0.5	O
0	O
0.5	O
1	O
figure	O
5.11	O
illustration	O
of	O
the	O
effect	O
of	O
the	O
hyperparameters	O
governing	O
the	O
prior	B
distribution	O
over	O
weights	O
and	O
biases	O
in	O
a	O
two-layer	O
network	O
having	O
a	O
single	O
input	O
,	O
a	O
single	O
linear	O
output	O
,	O
and	O
12	O
hidden	O
units	O
having	O
‘	O
tanh	O
’	O
activation	O
functions	O
.	O
the	O
priors	O
are	O
governed	O
by	O
four	O
hyperparameters	O
αb	O
2	O
,	O
which	O
represent	O
the	O
precisions	O
of	O
the	O
gaussian	O
distributions	O
of	O
the	O
ﬁrst-layer	O
biases	O
,	O
ﬁrst-layer	O
weights	O
,	O
second-layer	O
biases	O
,	O
and	O
second-layer	O
weights	O
,	O
respectively	O
.	O
we	O
see	O
that	O
the	O
parameter	O
αw	O
2	O
governs	O
the	O
vertical	O
scale	O
of	O
functions	O
(	O
note	O
the	O
different	O
vertical	O
axis	O
ranges	O
on	O
the	O
top	O
two	O
diagrams	O
)	O
,	O
αw	O
1	O
governs	O
the	O
horizontal	O
scale	O
of	O
variations	O
in	O
the	O
function	O
values	O
,	O
and	O
αb	O
2	O
,	O
whose	O
effect	O
is	O
not	O
illustrated	O
here	O
,	O
governs	O
the	O
range	O
of	O
vertical	O
offsets	O
of	O
the	O
functions	O
.	O
1	O
governs	O
the	O
horizontal	O
range	O
over	O
which	O
variations	O
occur	O
.	O
the	O
parameter	O
αb	O
2	O
,	O
and	O
αw	O
1	O
,	O
αw	O
1	O
,	O
αb	O
a	O
minimum	O
of	O
the	O
training	B
error	O
has	O
been	O
reached	O
then	O
represents	O
a	O
way	O
of	O
limiting	O
the	O
effective	O
network	O
complexity	O
.	O
in	O
the	O
case	O
of	O
a	O
quadratic	O
error	O
function	O
,	O
we	O
can	O
verify	O
this	O
insight	O
,	O
and	O
show	O
that	O
early	B
stopping	I
should	O
exhibit	O
similar	O
behaviour	O
to	O
regularization	B
using	O
a	O
sim-	O
ple	O
weight-decay	O
term	O
.	O
this	O
can	O
be	O
understood	O
from	O
figure	O
5.13	O
,	O
in	O
which	O
the	O
axes	O
in	O
weight	O
space	O
have	O
been	O
rotated	O
to	O
be	O
parallel	O
to	O
the	O
eigenvectors	O
of	O
the	O
hessian	O
matrix	O
.	O
if	O
,	O
in	O
the	O
absence	O
of	O
weight	B
decay	I
,	O
the	O
weight	B
vector	I
starts	O
at	O
the	O
origin	O
and	O
proceeds	O
during	O
training	B
along	O
a	O
path	O
that	O
follows	O
the	O
local	B
negative	O
gradient	O
vec-	O
tor	O
,	O
then	O
the	O
weight	B
vector	I
will	O
move	O
initially	O
parallel	O
to	O
the	O
w2	O
axis	O
through	O
a	O
point	O
corresponding	O
roughly	O
to	O
(	O
cid:4	O
)	O
w	O
and	O
then	O
move	O
towards	O
the	O
minimum	O
of	O
the	O
error	B
func-	O
eigenvalues	O
of	O
the	O
hessian	O
.	O
stopping	O
at	O
a	O
point	O
near	O
(	O
cid:4	O
)	O
w	O
is	O
therefore	O
similar	O
to	O
weight	O
tion	O
wml	O
.	O
this	O
follows	O
from	O
the	O
shape	O
of	O
the	O
error	B
surface	O
and	O
the	O
widely	O
differing	O
exercise	O
5.25	O
decay	O
.	O
the	O
relationship	O
between	O
early	B
stopping	I
and	O
weight	B
decay	I
can	O
be	O
made	O
quan-	O
titative	O
,	O
thereby	O
showing	O
that	O
the	O
quantity	O
τ	O
η	O
(	O
where	O
τ	O
is	O
the	O
iteration	O
index	O
,	O
and	O
η	O
is	O
the	O
learning	B
rate	I
parameter	I
)	O
plays	O
the	O
role	O
of	O
the	O
reciprocal	O
of	O
the	O
regularization	B
5.5.	O
regularization	B
in	O
neural	O
networks	O
261	O
0.25	O
0.2	O
0.15	O
0	O
10	O
20	O
30	O
40	O
50	O
0.45	O
0.4	O
0.35	O
0	O
10	O
20	O
30	O
40	O
50	O
figure	O
5.12	O
an	O
illustration	O
of	O
the	O
behaviour	O
of	O
training	B
set	I
error	O
(	O
left	O
)	O
and	O
validation	B
set	I
error	O
(	O
right	O
)	O
during	O
a	O
typical	O
training	B
session	O
,	O
as	O
a	O
function	O
of	O
the	O
iteration	O
step	O
,	O
for	O
the	O
sinusoidal	B
data	I
set	O
.	O
the	O
goal	O
of	O
achieving	O
the	O
best	O
generalization	B
performance	O
suggests	O
that	O
training	B
should	O
be	O
stopped	O
at	O
the	O
point	O
shown	O
by	O
the	O
vertical	O
dashed	O
lines	O
,	O
corresponding	O
to	O
the	O
minimum	O
of	O
the	O
validation	B
set	I
error	O
.	O
parameter	O
λ.	O
the	O
effective	B
number	I
of	I
parameters	I
in	O
the	O
network	O
therefore	O
grows	O
during	O
the	O
course	O
of	O
training	B
.	O
5.5.3	O
invariances	O
in	O
many	O
applications	O
of	O
pattern	O
recognition	O
,	O
it	O
is	O
known	O
that	O
predictions	O
should	O
be	O
unchanged	O
,	O
or	O
invariant	O
,	O
under	O
one	O
or	O
more	O
transformations	O
of	O
the	O
input	O
vari-	O
ables	O
.	O
for	O
example	O
,	O
in	O
the	O
classiﬁcation	B
of	O
objects	O
in	O
two-dimensional	O
images	O
,	O
such	O
as	O
handwritten	O
digits	O
,	O
a	O
particular	O
object	O
should	O
be	O
assigned	O
the	O
same	O
classiﬁcation	B
irrespective	O
of	O
its	O
position	O
within	O
the	O
image	O
(	O
translation	B
invariance	I
)	O
or	O
of	O
its	O
size	O
(	O
scale	B
invariance	I
)	O
.	O
such	O
transformations	O
produce	O
signiﬁcant	O
changes	O
in	O
the	O
raw	O
data	O
,	O
expressed	O
in	O
terms	O
of	O
the	O
intensities	O
at	O
each	O
of	O
the	O
pixels	O
in	O
the	O
image	O
,	O
and	O
yet	O
should	O
give	O
rise	O
to	O
the	O
same	O
output	O
from	O
the	O
classiﬁcation	B
system	O
.	O
similarly	O
in	O
speech	B
recognition	I
,	O
small	O
levels	O
of	O
nonlinear	O
warping	O
along	O
the	O
time	O
axis	O
,	O
which	O
preserve	O
temporal	O
ordering	O
,	O
should	O
not	O
change	O
the	O
interpretation	O
of	O
the	O
signal	O
.	O
if	O
sufﬁciently	O
large	O
numbers	O
of	O
training	B
patterns	O
are	O
available	O
,	O
then	O
an	O
adaptive	O
model	O
such	O
as	O
a	O
neural	B
network	I
can	O
learn	O
the	O
invariance	B
,	O
at	O
least	O
approximately	O
.	O
this	O
involves	O
including	O
within	O
the	O
training	B
set	I
a	O
sufﬁciently	O
large	O
number	O
of	O
examples	O
of	O
the	O
effects	O
of	O
the	O
various	O
transformations	O
.	O
thus	O
,	O
for	O
translation	O
invariance	B
in	O
an	O
im-	O
age	O
,	O
the	O
training	B
set	I
should	O
include	O
examples	O
of	O
objects	O
at	O
many	O
different	O
positions	O
.	O
this	O
approach	O
may	O
be	O
impractical	O
,	O
however	O
,	O
if	O
the	O
number	O
of	O
training	B
examples	O
is	O
limited	O
,	O
or	O
if	O
there	O
are	O
several	O
invariants	O
(	O
because	O
the	O
number	O
of	O
combinations	O
of	O
transformations	O
grows	O
exponentially	O
with	O
the	O
number	O
of	O
such	O
transformations	O
)	O
.	O
we	O
therefore	O
seek	O
alternative	O
approaches	O
for	O
encouraging	O
an	O
adaptive	O
model	O
to	O
exhibit	O
the	O
required	O
invariances	O
.	O
these	O
can	O
broadly	O
be	O
divided	O
into	O
four	O
categories	O
:	O
1.	O
the	O
training	B
set	I
is	O
augmented	O
using	O
replicas	O
of	O
the	O
training	B
patterns	O
,	O
trans-	O
formed	O
according	O
to	O
the	O
desired	O
invariances	O
.	O
for	O
instance	O
,	O
in	O
our	O
digit	O
recog-	O
nition	O
example	O
,	O
we	O
could	O
make	O
multiple	O
copies	O
of	O
each	O
example	O
in	O
which	O
the	O
262	O
5.	O
neural	O
networks	O
figure	O
5.13	O
a	O
schematic	O
illustration	O
of	O
why	O
early	B
stopping	I
can	O
give	O
similar	O
results	O
to	O
weight	B
decay	I
in	O
the	O
case	O
of	O
a	O
quadratic	O
error	O
func-	O
tion	O
.	O
the	O
ellipse	O
shows	O
a	O
con-	O
tour	O
of	O
constant	O
error	B
,	O
and	O
wml	O
denotes	O
the	O
minimum	O
of	O
the	O
er-	O
ror	O
function	O
.	O
if	O
the	O
weight	B
vector	I
starts	O
at	O
the	O
origin	O
and	O
moves	O
ac-	O
cording	O
to	O
the	O
local	B
negative	O
gra-	O
dient	O
direction	O
,	O
then	O
it	O
will	O
follow	O
the	O
path	O
shown	O
by	O
the	O
curve	O
.	O
by	O
stopping	O
training	B
early	O
,	O
a	O
weight	B
vector	I
ew	O
is	O
found	O
that	O
is	O
qual-	O
itatively	O
similar	O
to	O
that	O
obtained	O
with	O
a	O
simple	O
weight-decay	O
reg-	O
ularizer	O
and	O
training	B
to	O
the	O
mini-	O
mum	O
of	O
the	O
regularized	O
error	O
,	O
as	O
can	O
be	O
seen	O
by	O
comparing	O
with	O
figure	O
3.15.	O
w2	O
(	O
cid:4	O
)	O
w	O
wml	O
w1	O
digit	O
is	O
shifted	O
to	O
a	O
different	O
position	O
in	O
each	O
image	O
.	O
2.	O
a	O
regularization	B
term	O
is	O
added	O
to	O
the	O
error	B
function	I
that	O
penalizes	O
changes	O
in	O
the	O
model	O
output	O
when	O
the	O
input	O
is	O
transformed	O
.	O
this	O
leads	O
to	O
the	O
technique	O
of	O
tangent	B
propagation	I
,	O
discussed	O
in	O
section	O
5.5.4	O
.	O
3.	O
invariance	B
is	O
built	O
into	O
the	O
pre-processing	O
by	O
extracting	O
features	O
that	O
are	O
invari-	O
ant	O
under	O
the	O
required	O
transformations	O
.	O
any	O
subsequent	O
regression	B
or	O
classi-	O
ﬁcation	O
system	O
that	O
uses	O
such	O
features	O
as	O
inputs	O
will	O
necessarily	O
also	O
respect	O
these	O
invariances	O
.	O
4.	O
the	O
ﬁnal	O
option	O
is	O
to	O
build	O
the	O
invariance	B
properties	O
into	O
the	O
structure	O
of	O
a	O
neu-	O
ral	O
network	O
(	O
or	O
into	O
the	O
deﬁnition	O
of	O
a	O
kernel	B
function	I
in	O
the	O
case	O
of	O
techniques	O
such	O
as	O
the	O
relevance	B
vector	I
machine	I
)	O
.	O
one	O
way	O
to	O
achieve	O
this	O
is	O
through	O
the	O
use	O
of	O
local	O
receptive	O
ﬁelds	O
and	O
shared	O
weights	O
,	O
as	O
discussed	O
in	O
the	O
context	O
of	O
convolutional	O
neural	O
networks	O
in	O
section	O
5.5.6.	O
approach	O
1	O
is	O
often	O
relatively	O
easy	O
to	O
implement	O
and	O
can	O
be	O
used	O
to	O
encourage	O
com-	O
plex	O
invariances	O
such	O
as	O
those	O
illustrated	O
in	O
figure	O
5.14.	O
for	O
sequential	O
training	B
algorithms	O
,	O
this	O
can	O
be	O
done	O
by	O
transforming	O
each	O
input	O
pattern	O
before	O
it	O
is	O
presented	O
to	O
the	O
model	O
so	O
that	O
,	O
if	O
the	O
patterns	O
are	O
being	O
recycled	O
,	O
a	O
different	O
transformation	O
(	O
drawn	O
from	O
an	O
appropriate	O
distribution	O
)	O
is	O
added	O
each	O
time	O
.	O
for	O
batch	O
methods	O
,	O
a	O
similar	O
effect	O
can	O
be	O
achieved	O
by	O
replicating	O
each	O
data	O
point	O
a	O
number	O
of	O
times	O
and	O
transforming	O
each	O
copy	O
independently	O
.	O
the	O
use	O
of	O
such	O
augmented	O
data	O
can	O
lead	O
to	O
signiﬁcant	O
improvements	O
in	O
generalization	B
(	O
simard	O
et	O
al.	O
,	O
2003	O
)	O
,	O
although	O
it	O
can	O
also	O
be	O
computationally	O
costly	O
.	O
approach	O
2	O
leaves	O
the	O
data	O
set	O
unchanged	O
but	O
modiﬁes	O
the	O
error	B
function	I
through	O
the	O
addition	O
of	O
a	O
regularizer	O
.	O
in	O
section	O
5.5.5	O
,	O
we	O
shall	O
show	O
that	O
this	O
approach	O
is	O
closely	O
related	O
to	O
approach	O
2	O
.	O
5.5.	O
regularization	B
in	O
neural	O
networks	O
263	O
figure	O
5.14	O
illustration	O
of	O
the	O
synthetic	O
warping	O
of	O
a	O
handwritten	B
digit	I
.	O
the	O
original	O
image	O
is	O
shown	O
on	O
the	O
left	O
.	O
on	O
the	O
right	O
,	O
the	O
top	O
row	O
shows	O
three	O
examples	O
of	O
warped	O
digits	O
,	O
with	O
the	O
corresponding	O
displacement	O
ﬁelds	O
shown	O
on	O
the	O
bottom	O
row	O
.	O
these	O
displacement	O
ﬁelds	O
are	O
generated	O
by	O
sampling	O
random	O
displacements	O
∆x	O
,	O
∆y	O
∈	O
(	O
0	O
,	O
1	O
)	O
at	O
each	O
pixel	O
and	O
then	O
smoothing	O
by	O
convolution	O
with	O
gaussians	O
of	O
width	O
0.01	O
,	O
30	O
and	O
60	O
respectively	O
.	O
one	O
advantage	O
of	O
approach	O
3	O
is	O
that	O
it	O
can	O
correctly	O
extrapolate	O
well	O
beyond	O
the	O
range	O
of	O
transformations	O
included	O
in	O
the	O
training	B
set	I
.	O
however	O
,	O
it	O
can	O
be	O
difﬁcult	O
to	O
ﬁnd	O
hand-crafted	O
features	O
with	O
the	O
required	O
invariances	O
that	O
do	O
not	O
also	O
discard	O
information	O
that	O
can	O
be	O
useful	O
for	O
discrimination	O
.	O
5.5.4	O
tangent	B
propagation	I
we	O
can	O
use	O
regularization	B
to	O
encourage	O
models	O
to	O
be	O
invariant	O
to	O
transformations	O
of	O
the	O
input	O
through	O
the	O
technique	O
of	O
tangent	B
propagation	I
(	O
simard	O
et	O
al.	O
,	O
1992	O
)	O
.	O
consider	O
the	O
effect	O
of	O
a	O
transformation	O
on	O
a	O
particular	O
input	O
vector	O
xn	O
.	O
provided	O
the	O
transformation	O
is	O
continuous	O
(	O
such	O
as	O
translation	O
or	O
rotation	O
,	O
but	O
not	O
mirror	O
reﬂection	O
for	O
instance	O
)	O
,	O
then	O
the	O
transformed	O
pattern	O
will	O
sweep	O
out	O
a	O
manifold	B
m	O
within	O
the	O
d-dimensional	O
input	O
space	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
5.15	O
,	O
for	O
the	O
case	O
of	O
d	O
=	O
2	O
for	O
simplicity	O
.	O
suppose	O
the	O
transformation	O
is	O
governed	O
by	O
a	O
single	O
parameter	O
ξ	O
(	O
which	O
might	O
be	O
rotation	O
angle	O
for	O
instance	O
)	O
.	O
then	O
the	O
subspace	O
m	O
swept	O
out	O
by	O
xn	O
figure	O
5.15	O
illustration	O
of	O
a	O
two-dimensional	O
input	O
space	O
showing	O
the	O
effect	O
of	O
a	O
continuous	O
transforma-	O
tion	O
on	O
a	O
particular	O
input	O
vector	O
xn	O
.	O
a	O
one-	O
dimensional	O
transformation	O
,	O
parameterized	O
by	O
the	O
continuous	O
variable	O
ξ	O
,	O
applied	O
to	O
xn	O
causes	O
it	O
to	O
sweep	O
out	O
a	O
one-dimensional	O
manifold	B
m.	O
locally	O
,	O
the	O
effect	O
of	O
the	O
transformation	O
can	O
be	O
approximated	O
by	O
the	O
tangent	O
vector	O
τ	O
n.	O
x2	O
τ	O
n	O
xn	O
m	O
ξ	O
x1	O
264	O
5.	O
neural	O
networks	O
will	O
be	O
one-dimensional	O
,	O
and	O
will	O
be	O
parameterized	O
by	O
ξ.	O
let	O
the	O
vector	O
that	O
results	O
from	O
acting	O
on	O
xn	O
by	O
this	O
transformation	O
be	O
denoted	O
by	O
s	O
(	O
xn	O
,	O
ξ	O
)	O
,	O
which	O
is	O
deﬁned	O
so	O
that	O
s	O
(	O
x	O
,	O
0	O
)	O
=	O
x.	O
then	O
the	O
tangent	O
to	O
the	O
curve	O
m	O
is	O
given	O
by	O
the	O
directional	O
derivative	O
τ	O
=	O
∂s/∂ξ	O
,	O
and	O
the	O
tangent	O
vector	O
at	O
the	O
point	O
xn	O
is	O
given	O
by	O
τ	O
n	O
=	O
∂s	O
(	O
xn	O
,	O
ξ	O
)	O
∂ξ	O
.	O
ξ=0	O
(	O
5.125	O
)	O
under	O
a	O
transformation	O
of	O
the	O
input	O
vector	O
,	O
the	O
network	O
output	O
vector	O
will	O
,	O
in	O
general	O
,	O
change	O
.	O
the	O
derivative	B
of	O
output	O
k	O
with	O
respect	O
to	O
ξ	O
is	O
given	O
by	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
d	O
(	O
cid:2	O
)	O
d	O
(	O
cid:2	O
)	O
∂yk	O
∂ξ	O
=	O
ξ=0	O
i=1	O
∂yk	O
∂xi	O
∂xi	O
∂ξ	O
ξ=0	O
i=1	O
=	O
jkiτi	O
(	O
5.126	O
)	O
where	O
jki	O
is	O
the	O
(	O
k	O
,	O
i	O
)	O
element	O
of	O
the	O
jacobian	O
matrix	O
j	O
,	O
as	O
discussed	O
in	O
section	O
5.3.4.	O
the	O
result	O
(	O
5.126	O
)	O
can	O
be	O
used	O
to	O
modify	O
the	O
standard	O
error	O
function	O
,	O
so	O
as	O
to	O
encour-	O
age	O
local	B
invariance	O
in	O
the	O
neighbourhood	O
of	O
the	O
data	O
points	O
,	O
by	O
the	O
addition	O
to	O
the	O
original	O
error	B
function	I
e	O
of	O
a	O
regularization	B
function	O
ω	O
to	O
give	O
a	O
total	O
error	B
function	I
of	O
the	O
form	O
(	O
cid:4	O
)	O
e	O
=	O
e	O
+	O
λω	O
(	O
cid:23	O
)	O
2	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
5.127	O
)	O
(	O
cid:22	O
)	O
d	O
(	O
cid:2	O
)	O
(	O
cid:23	O
)	O
2	O
jnkiτni	O
.	O
(	O
5.128	O
)	O
n	O
k	O
i=1	O
where	O
λ	O
is	O
a	O
regularization	B
coefﬁcient	O
and	O
(	O
cid:22	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
n	O
k	O
ω	O
=	O
1	O
2	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
∂ynk	O
∂ξ	O
ξ=0	O
=	O
1	O
2	O
exercise	O
5.26	O
the	O
regularization	B
function	O
will	O
be	O
zero	O
when	O
the	O
network	O
mapping	O
function	O
is	O
in-	O
variant	O
under	O
the	O
transformation	O
in	O
the	O
neighbourhood	O
of	O
each	O
pattern	O
vector	O
,	O
and	O
the	O
value	O
of	O
the	O
parameter	O
λ	O
determines	O
the	O
balance	O
between	O
ﬁtting	O
the	O
training	B
data	O
and	O
learning	B
the	O
invariance	B
property	O
.	O
in	O
a	O
practical	O
implementation	O
,	O
the	O
tangent	O
vector	O
τ	O
n	O
can	O
be	O
approximated	O
us-	O
ing	O
ﬁnite	O
differences	O
,	O
by	O
subtracting	O
the	O
original	O
vector	O
xn	O
from	O
the	O
corresponding	O
vector	O
after	O
transformation	O
using	O
a	O
small	O
value	O
of	O
ξ	O
,	O
and	O
then	O
dividing	O
by	O
ξ.	O
this	O
is	O
illustrated	O
in	O
figure	O
5.16.	O
the	O
regularization	B
function	O
depends	O
on	O
the	O
network	O
weights	O
through	O
the	O
jaco-	O
bian	O
j.	O
a	O
backpropagation	B
formalism	O
for	O
computing	O
the	O
derivatives	O
of	O
the	O
regu-	O
larizer	O
with	O
respect	O
to	O
the	O
network	O
weights	O
is	O
easily	O
obtained	O
by	O
extension	O
of	O
the	O
techniques	O
introduced	O
in	O
section	O
5.3.	O
if	O
the	O
transformation	O
is	O
governed	O
by	O
l	O
parameters	O
(	O
e.g.	O
,	O
l	O
=	O
3	O
for	O
the	O
case	O
of	O
translations	O
combined	O
with	O
in-plane	O
rotations	O
in	O
a	O
two-dimensional	O
image	O
)	O
,	O
then	O
the	O
manifold	B
m	O
will	O
have	O
dimensionality	O
l	O
,	O
and	O
the	O
corresponding	O
regularizer	O
is	O
given	O
by	O
the	O
sum	O
of	O
terms	O
of	O
the	O
form	O
(	O
5.128	O
)	O
,	O
one	O
for	O
each	O
transformation	O
.	O
if	O
several	O
transformations	O
are	O
considered	O
at	O
the	O
same	O
time	O
,	O
and	O
the	O
network	O
mapping	O
is	O
made	O
invariant	O
to	O
each	O
separately	O
,	O
then	O
it	O
will	O
be	O
(	O
locally	O
)	O
invariant	O
to	O
combinations	O
of	O
the	O
transformations	O
(	O
simard	O
et	O
al.	O
,	O
1992	O
)	O
.	O
5.5.	O
regularization	B
in	O
neural	O
networks	O
265	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
5.16	O
illustration	O
showing	O
(	O
a	O
)	O
the	O
original	O
image	O
x	O
of	O
a	O
hand-	O
written	O
digit	O
,	O
(	O
b	O
)	O
the	O
tangent	O
vector	O
τ	O
corresponding	O
to	O
an	O
inﬁnitesimal	O
clockwise	O
rotation	O
,	O
(	O
c	O
)	O
the	O
result	O
of	O
adding	O
a	O
small	O
contribution	O
from	O
the	O
tangent	O
vector	O
to	O
the	O
original	O
image	O
giving	O
x	O
+	O
τ	O
with	O
	O
=	O
15	O
degrees	O
,	O
and	O
(	O
d	O
)	O
the	O
true	O
image	O
rotated	O
for	O
comparison	O
.	O
(	O
c	O
)	O
(	O
d	O
)	O
a	O
related	O
technique	O
,	O
called	O
tangent	B
distance	I
,	O
can	O
be	O
used	O
to	O
build	O
invariance	B
properties	O
into	O
distance-based	O
methods	O
such	O
as	O
nearest-neighbour	O
classiﬁers	O
(	O
simard	O
et	O
al.	O
,	O
1993	O
)	O
.	O
5.5.5	O
training	B
with	O
transformed	O
data	O
we	O
have	O
seen	O
that	O
one	O
way	O
to	O
encourage	O
invariance	B
of	O
a	O
model	O
to	O
a	O
set	O
of	O
trans-	O
formations	O
is	O
to	O
expand	O
the	O
training	B
set	I
using	O
transformed	O
versions	O
of	O
the	O
original	O
input	O
patterns	O
.	O
here	O
we	O
show	O
that	O
this	O
approach	O
is	O
closely	O
related	O
to	O
the	O
technique	O
of	O
tangent	B
propagation	I
(	O
bishop	O
,	O
1995b	O
;	O
leen	O
,	O
1995	O
)	O
.	O
as	O
in	O
section	O
5.5.4	O
,	O
we	O
shall	O
consider	O
a	O
transformation	O
governed	O
by	O
a	O
single	O
parameter	O
ξ	O
and	O
described	O
by	O
the	O
function	O
s	O
(	O
x	O
,	O
ξ	O
)	O
,	O
with	O
s	O
(	O
x	O
,	O
0	O
)	O
=	O
x.	O
we	O
shall	O
also	O
consider	O
a	O
sum-of-squares	B
error	I
function	O
.	O
the	O
error	B
function	I
for	O
untransformed	O
inputs	O
can	O
be	O
written	O
(	O
in	O
the	O
inﬁnite	O
data	O
set	O
limit	O
)	O
in	O
the	O
form	O
{	O
y	O
(	O
x	O
)	O
−	O
t	O
}	O
2p	O
(	O
t|x	O
)	O
p	O
(	O
x	O
)	O
dx	O
dt	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
5.129	O
)	O
e	O
=	O
1	O
2	O
as	O
discussed	O
in	O
section	O
1.5.5.	O
here	O
we	O
have	O
considered	O
a	O
network	O
having	O
a	O
single	O
output	O
,	O
in	O
order	O
to	O
keep	O
the	O
notation	O
uncluttered	O
.	O
if	O
we	O
now	O
consider	O
an	O
inﬁnite	O
number	O
of	O
copies	O
of	O
each	O
data	O
point	O
,	O
each	O
of	O
which	O
is	O
perturbed	O
by	O
the	O
transformation	O
266	O
5.	O
neural	O
networks	O
in	O
which	O
the	O
parameter	O
ξ	O
is	O
drawn	O
from	O
a	O
distribution	O
p	O
(	O
ξ	O
)	O
,	O
then	O
the	O
error	B
function	I
deﬁned	O
over	O
this	O
expanded	O
data	O
set	O
can	O
be	O
written	O
as	O
{	O
y	O
(	O
s	O
(	O
x	O
,	O
ξ	O
)	O
)	O
−	O
t	O
}	O
2p	O
(	O
t|x	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
ξ	O
)	O
dx	O
dt	O
dξ	O
.	O
(	O
5.130	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:4	O
)	O
e	O
=	O
1	O
2	O
we	O
now	O
assume	O
that	O
the	O
distribution	O
p	O
(	O
ξ	O
)	O
has	O
zero	O
mean	B
with	O
small	O
variance	B
,	O
so	O
that	O
we	O
are	O
only	O
considering	O
small	O
transformations	O
of	O
the	O
original	O
input	O
vectors	O
.	O
we	O
can	O
then	O
expand	O
the	O
transformation	O
function	O
as	O
a	O
taylor	O
series	O
in	O
powers	O
of	O
ξ	O
to	O
give	O
s	O
(	O
x	O
,	O
ξ	O
)	O
=	O
s	O
(	O
x	O
,	O
0	O
)	O
+	O
ξ	O
∂	O
∂ξ	O
s	O
(	O
x	O
,	O
ξ	O
)	O
+	O
ξ2	O
2	O
ξ=0	O
∂2	O
∂ξ2	O
s	O
(	O
x	O
,	O
ξ	O
)	O
+	O
o	O
(	O
ξ3	O
)	O
ξ=0	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
denotes	O
the	O
second	O
derivative	O
of	O
s	O
(	O
x	O
,	O
ξ	O
)	O
with	O
respect	O
to	O
ξ	O
evaluated	O
at	O
ξ	O
=	O
0	O
.	O
=	O
x	O
+	O
ξτ	O
+	O
1	O
2	O
ξ2τ	O
(	O
cid:4	O
)	O
+	O
o	O
(	O
ξ3	O
)	O
-	O
where	O
τ	O
(	O
cid:4	O
)	O
this	O
allows	O
us	O
to	O
expand	O
the	O
model	O
function	O
to	O
give	O
y	O
(	O
s	O
(	O
x	O
,	O
ξ	O
)	O
)	O
=	O
y	O
(	O
x	O
)	O
+	O
ξτ	O
t∇y	O
(	O
x	O
)	O
+	O
ξ2	O
2	O
(	O
τ	O
(	O
cid:4	O
)	O
)	O
t	O
∇y	O
(	O
x	O
)	O
+	O
τ	O
t∇∇y	O
(	O
x	O
)	O
τ	O
+	O
o	O
(	O
ξ3	O
)	O
.	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:19	O
)	O
.	O
(	O
cid:20	O
)	O
(	O
cid:20	O
)	O
substituting	O
into	O
the	O
mean	B
error	O
function	O
(	O
5.130	O
)	O
and	O
expanding	O
,	O
we	O
then	O
have	O
(	O
cid:4	O
)	O
e	O
=	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:29	O
)	O
1	O
2	O
{	O
y	O
(	O
x	O
)	O
−	O
t	O
}	O
2p	O
(	O
t|x	O
)	O
p	O
(	O
x	O
)	O
dx	O
dt	O
+	O
e	O
[	O
ξ	O
]	O
{	O
y	O
(	O
x	O
)	O
−	O
t	O
}	O
τ	O
t∇y	O
(	O
x	O
)	O
p	O
(	O
t|x	O
)	O
p	O
(	O
x	O
)	O
dx	O
dt	O
+	O
e	O
[	O
ξ2	O
]	O
(	O
cid:10	O
)	O
{	O
y	O
(	O
x	O
)	O
−	O
t	O
}	O
1	O
2	O
.	O
(	O
cid:11	O
)	O
2	O
τ	O
t∇y	O
(	O
x	O
)	O
+	O
p	O
(	O
t|x	O
)	O
p	O
(	O
x	O
)	O
dx	O
dt	O
+	O
o	O
(	O
ξ3	O
)	O
.	O
(	O
τ	O
(	O
cid:4	O
)	O
)	O
t	O
∇y	O
(	O
x	O
)	O
+	O
τ	O
t∇∇y	O
(	O
x	O
)	O
τ	O
because	O
the	O
distribution	O
of	O
transformations	O
has	O
zero	O
mean	B
we	O
have	O
e	O
[	O
ξ	O
]	O
=	O
0.	O
also	O
,	O
we	O
shall	O
denote	O
e	O
[	O
ξ2	O
]	O
by	O
λ.	O
omitting	O
terms	O
of	O
o	O
(	O
ξ3	O
)	O
,	O
the	O
average	O
error	B
function	I
then	O
becomes	O
(	O
5.131	O
)	O
where	O
e	O
is	O
the	O
original	O
sum-of-squares	B
error	I
,	O
and	O
the	O
regularization	B
term	O
ω	O
takes	O
the	O
form	O
(	O
cid:4	O
)	O
e	O
=	O
e	O
+	O
λω	O
ω	O
=	O
(	O
τ	O
(	O
cid:4	O
)	O
)	O
t	O
∇y	O
(	O
x	O
)	O
+	O
τ	O
t∇∇y	O
(	O
x	O
)	O
τ	O
(	O
cid:19	O
)	O
(	O
cid:6	O
)	O
(	O
cid:29	O
)	O
(	O
cid:10	O
)	O
+	O
{	O
y	O
(	O
x	O
)	O
−	O
e	O
[	O
t|x	O
]	O
}	O
1	O
2	O
(	O
cid:30	O
)	O
(	O
cid:11	O
)	O
2	O
τ	O
t∇y	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
dx	O
(	O
5.132	O
)	O
in	O
which	O
we	O
have	O
performed	O
the	O
integration	O
over	O
t.	O
5.5.	O
regularization	B
in	O
neural	O
networks	O
267	O
we	O
can	O
further	O
simplify	O
this	O
regularization	B
term	O
as	O
follows	O
.	O
in	O
section	O
1.5.5	O
we	O
saw	O
that	O
the	O
function	O
that	O
minimizes	O
the	O
sum-of-squares	B
error	I
is	O
given	O
by	O
the	O
condi-	O
tional	O
average	O
e	O
[	O
t|x	O
]	O
of	O
the	O
target	O
values	O
t.	O
from	O
(	O
5.131	O
)	O
we	O
see	O
that	O
the	O
regularized	O
error	O
will	O
equal	O
the	O
unregularized	O
sum-of-squares	O
plus	O
terms	O
which	O
are	O
o	O
(	O
ξ	O
)	O
,	O
and	O
so	O
the	O
network	O
function	O
that	O
minimizes	O
the	O
total	O
error	B
will	O
have	O
the	O
form	O
y	O
(	O
x	O
)	O
=	O
e	O
[	O
t|x	O
]	O
+	O
o	O
(	O
ξ	O
)	O
.	O
(	O
5.133	O
)	O
thus	O
,	O
to	O
leading	O
order	O
in	O
ξ	O
,	O
the	O
ﬁrst	O
term	O
in	O
the	O
regularizer	O
vanishes	O
and	O
we	O
are	O
left	O
with	O
p	O
(	O
x	O
)	O
dx	O
(	O
5.134	O
)	O
(	O
cid:6	O
)	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
2	O
ω	O
=	O
1	O
2	O
τ	O
t∇y	O
(	O
x	O
)	O
exercise	O
5.27	O
which	O
is	O
equivalent	O
to	O
the	O
tangent	B
propagation	I
regularizer	O
(	O
5.128	O
)	O
.	O
if	O
we	O
consider	O
the	O
special	O
case	O
in	O
which	O
the	O
transformation	O
of	O
the	O
inputs	O
simply	O
consists	O
of	O
the	O
addition	O
of	O
random	O
noise	O
,	O
so	O
that	O
x	O
→	O
x	O
+	O
ξ	O
,	O
then	O
the	O
regularizer	O
takes	O
the	O
form	O
(	O
cid:6	O
)	O
ω	O
=	O
1	O
2	O
(	O
cid:5	O
)	O
∇y	O
(	O
x	O
)	O
(	O
cid:5	O
)	O
2	O
p	O
(	O
x	O
)	O
dx	O
(	O
5.135	O
)	O
which	O
is	O
known	O
as	O
tikhonov	O
regularization	B
(	O
tikhonov	O
and	O
arsenin	O
,	O
1977	O
;	O
bishop	O
,	O
1995b	O
)	O
.	O
derivatives	O
of	O
this	O
regularizer	O
with	O
respect	O
to	O
the	O
network	O
weights	O
can	O
be	O
found	O
using	O
an	O
extended	B
backpropagation	O
algorithm	O
(	O
bishop	O
,	O
1993	O
)	O
.	O
we	O
see	O
that	O
,	O
for	O
small	O
noise	O
amplitudes	O
,	O
tikhonov	O
regularization	B
is	O
related	O
to	O
the	O
addition	O
of	O
random	O
noise	O
to	O
the	O
inputs	O
,	O
which	O
has	O
been	O
shown	O
to	O
improve	O
generalization	B
in	O
appropriate	O
circumstances	O
(	O
sietsma	O
and	O
dow	O
,	O
1991	O
)	O
.	O
5.5.6	O
convolutional	B
networks	O
another	O
approach	O
to	O
creating	O
models	O
that	O
are	O
invariant	O
to	O
certain	O
transformation	O
of	O
the	O
inputs	O
is	O
to	O
build	O
the	O
invariance	B
properties	O
into	O
the	O
structure	O
of	O
a	O
neural	O
net-	O
work	O
.	O
this	O
is	O
the	O
basis	O
for	O
the	O
convolutional	B
neural	I
network	I
(	O
le	O
cun	O
et	O
al.	O
,	O
1989	O
;	O
lecun	O
et	O
al.	O
,	O
1998	O
)	O
,	O
which	O
has	O
been	O
widely	O
applied	O
to	O
image	O
data	O
.	O
consider	O
the	O
speciﬁc	O
task	O
of	O
recognizing	O
handwritten	O
digits	O
.	O
each	O
input	O
image	O
comprises	O
a	O
set	O
of	O
pixel	O
intensity	O
values	O
,	O
and	O
the	O
desired	O
output	O
is	O
a	O
posterior	O
proba-	O
bility	O
distribution	O
over	O
the	O
ten	O
digit	O
classes	O
.	O
we	O
know	O
that	O
the	O
identity	O
of	O
the	O
digit	O
is	O
invariant	O
under	O
translations	O
and	O
scaling	O
as	O
well	O
as	O
(	O
small	O
)	O
rotations	O
.	O
furthermore	O
,	O
the	O
network	O
must	O
also	O
exhibit	O
invariance	B
to	O
more	O
subtle	O
transformations	O
such	O
as	O
elastic	O
deformations	O
of	O
the	O
kind	O
illustrated	O
in	O
figure	O
5.14.	O
one	O
simple	O
approach	O
would	O
be	O
to	O
treat	O
the	O
image	O
as	O
the	O
input	O
to	O
a	O
fully	B
connected	I
network	O
,	O
such	O
as	O
the	O
kind	O
shown	O
in	O
figure	O
5.1.	O
given	O
a	O
sufﬁciently	O
large	O
training	O
set	O
,	O
such	O
a	O
network	O
could	O
in	O
principle	O
yield	O
a	O
good	O
solution	O
to	O
this	O
problem	O
and	O
would	O
learn	O
the	O
appropriate	O
invariances	O
by	O
example	O
.	O
however	O
,	O
this	O
approach	O
ignores	O
a	O
key	O
property	O
of	O
images	O
,	O
which	O
is	O
that	O
nearby	O
pixels	O
are	O
more	O
strongly	O
correlated	O
than	O
more	O
distant	O
pixels	O
.	O
many	O
of	O
the	O
modern	O
approaches	O
to	O
computer	O
vision	O
exploit	O
this	O
property	O
by	O
extracting	O
local	B
features	O
that	O
depend	O
only	O
on	O
small	O
subregions	O
of	O
the	O
image	O
.	O
information	O
from	O
such	O
features	O
can	O
then	O
be	O
merged	O
in	O
later	O
stages	O
of	O
processing	O
in	O
order	O
to	O
detect	O
higher-order	O
features	O
268	O
5.	O
neural	O
networks	O
input	O
image	O
convolutional	O
layer	O
sub-sampling	O
layer	O
figure	O
5.17	O
diagram	O
illustrating	O
part	O
of	O
a	O
convolutional	B
neural	I
network	I
,	O
showing	O
a	O
layer	O
of	O
convolu-	O
tional	O
units	O
followed	O
by	O
a	O
layer	O
of	O
subsampling	B
units	O
.	O
several	O
successive	O
pairs	O
of	O
such	O
layers	O
may	O
be	O
used	O
.	O
and	O
ultimately	O
to	O
yield	O
information	O
about	O
the	O
image	O
as	O
whole	O
.	O
also	O
,	O
local	B
features	O
that	O
are	O
useful	O
in	O
one	O
region	O
of	O
the	O
image	O
are	O
likely	O
to	O
be	O
useful	O
in	O
other	O
regions	O
of	O
the	O
image	O
,	O
for	O
instance	O
if	O
the	O
object	O
of	O
interest	O
is	O
translated	O
.	O
these	O
notions	O
are	O
incorporated	O
into	O
convolutional	O
neural	O
networks	O
through	O
three	O
mechanisms	O
:	O
(	O
i	O
)	O
local	O
receptive	O
ﬁelds	O
,	O
(	O
ii	O
)	O
weight	B
sharing	I
,	O
and	O
(	O
iii	O
)	O
subsampling	B
.	O
the	O
structure	O
of	O
a	O
convolutional	B
network	O
is	O
illustrated	O
in	O
figure	O
5.17.	O
in	O
the	O
convolutional	B
layer	O
the	O
units	O
are	O
organized	O
into	O
planes	O
,	O
each	O
of	O
which	O
is	O
called	O
a	O
feature	B
map	I
.	O
units	O
in	O
a	O
feature	B
map	I
each	O
take	O
inputs	O
only	O
from	O
a	O
small	O
subregion	O
of	O
the	O
image	O
,	O
and	O
all	O
of	O
the	O
units	O
in	O
a	O
feature	B
map	I
are	O
constrained	O
to	O
share	O
the	O
same	O
weight	O
values	O
.	O
for	O
instance	O
,	O
a	O
feature	B
map	I
might	O
consist	O
of	O
100	O
units	O
arranged	O
in	O
a	O
10	O
×	O
10	O
grid	O
,	O
with	O
each	O
unit	O
taking	O
inputs	O
from	O
a	O
5×5	O
pixel	O
patch	O
of	O
the	O
image	O
.	O
the	O
whole	O
feature	B
map	I
therefore	O
has	O
25	O
adjustable	O
weight	O
parameters	O
plus	O
one	O
adjustable	O
bias	B
parameter	I
.	O
input	O
values	O
from	O
a	O
patch	O
are	O
linearly	O
combined	O
using	O
the	O
weights	O
and	O
the	O
bias	B
,	O
and	O
the	O
result	O
transformed	O
by	O
a	O
sigmoidal	O
nonlinearity	O
using	O
(	O
5.1	O
)	O
.	O
if	O
we	O
think	O
of	O
the	O
units	O
as	O
feature	O
detectors	O
,	O
then	O
all	O
of	O
the	O
units	O
in	O
a	O
feature	B
map	I
detect	O
the	O
same	O
pattern	O
but	O
at	O
different	O
locations	O
in	O
the	O
input	O
image	O
.	O
due	O
to	O
the	O
weight	B
sharing	I
,	O
the	O
evaluation	O
of	O
the	O
activations	O
of	O
these	O
units	O
is	O
equivalent	O
to	O
a	O
convolution	O
of	O
the	O
image	O
pixel	O
intensities	O
with	O
a	O
‘	O
kernel	O
’	O
comprising	O
the	O
weight	O
parameters	O
.	O
if	O
the	O
input	O
image	O
is	O
shifted	O
,	O
the	O
activations	O
of	O
the	O
feature	B
map	I
will	O
be	O
shifted	O
by	O
the	O
same	O
amount	O
but	O
will	O
otherwise	O
be	O
unchanged	O
.	O
this	O
provides	O
the	O
basis	O
for	O
the	O
(	O
approximate	O
)	O
invariance	B
of	O
5.5.	O
regularization	B
in	O
neural	O
networks	O
269	O
the	O
network	O
outputs	O
to	O
translations	O
and	O
distortions	O
of	O
the	O
input	O
image	O
.	O
because	O
we	O
will	O
typically	O
need	O
to	O
detect	O
multiple	O
features	O
in	O
order	O
to	O
build	O
an	O
effective	O
model	O
,	O
there	O
will	O
generally	O
be	O
multiple	O
feature	O
maps	O
in	O
the	O
convolutional	B
layer	O
,	O
each	O
having	O
its	O
own	O
set	O
of	O
weight	O
and	O
bias	B
parameters	O
.	O
the	O
outputs	O
of	O
the	O
convolutional	B
units	O
form	O
the	O
inputs	O
to	O
the	O
subsampling	B
layer	O
of	O
the	O
network	O
.	O
for	O
each	O
feature	B
map	I
in	O
the	O
convolutional	B
layer	O
,	O
there	O
is	O
a	O
plane	O
of	O
units	O
in	O
the	O
subsampling	B
layer	O
and	O
each	O
unit	O
takes	O
inputs	O
from	O
a	O
small	O
receptive	O
ﬁeld	O
in	O
the	O
corresponding	O
feature	B
map	I
of	O
the	O
convolutional	B
layer	O
.	O
these	O
units	O
perform	O
subsampling	B
.	O
for	O
instance	O
,	O
each	O
subsampling	B
unit	O
might	O
take	O
inputs	O
from	O
a	O
2	O
×	O
2	O
unit	O
region	O
in	O
the	O
corresponding	O
feature	B
map	I
and	O
would	O
compute	O
the	O
average	O
of	O
those	O
inputs	O
,	O
multiplied	O
by	O
an	O
adaptive	O
weight	O
with	O
the	O
addition	O
of	O
an	O
adaptive	O
bias	O
parameter	O
,	O
and	O
then	O
transformed	O
using	O
a	O
sigmoidal	O
nonlinear	O
activation	B
function	I
.	O
the	O
receptive	O
ﬁelds	O
are	O
chosen	O
to	O
be	O
contiguous	O
and	O
nonoverlapping	O
so	O
that	O
there	O
are	O
half	O
the	O
number	O
of	O
rows	O
and	O
columns	O
in	O
the	O
subsampling	B
layer	O
compared	O
with	O
the	O
convolutional	B
layer	O
.	O
in	O
this	O
way	O
,	O
the	O
response	O
of	O
a	O
unit	O
in	O
the	O
subsampling	B
layer	O
will	O
be	O
relatively	O
insensitive	O
to	O
small	O
shifts	O
of	O
the	O
image	O
in	O
the	O
corresponding	O
regions	O
of	O
the	O
input	O
space	O
.	O
in	O
a	O
practical	O
architecture	O
,	O
there	O
may	O
be	O
several	O
pairs	O
of	O
convolutional	B
and	O
sub-	O
sampling	O
layers	O
.	O
at	O
each	O
stage	O
there	O
is	O
a	O
larger	O
degree	O
of	O
invariance	B
to	O
input	O
trans-	O
formations	O
compared	O
to	O
the	O
previous	O
layer	O
.	O
there	O
may	O
be	O
several	O
feature	O
maps	O
in	O
a	O
given	O
convolutional	B
layer	O
for	O
each	O
plane	O
of	O
units	O
in	O
the	O
previous	O
subsampling	B
layer	O
,	O
so	O
that	O
the	O
gradual	O
reduction	O
in	O
spatial	O
resolution	O
is	O
then	O
compensated	O
by	O
an	O
increas-	O
ing	O
number	O
of	O
features	O
.	O
the	O
ﬁnal	O
layer	O
of	O
the	O
network	O
would	O
typically	O
be	O
a	O
fully	B
connected	I
,	O
fully	O
adaptive	O
layer	O
,	O
with	O
a	O
softmax	O
output	O
nonlinearity	O
in	O
the	O
case	O
of	O
multiclass	B
classiﬁcation	O
.	O
the	O
whole	O
network	O
can	O
be	O
trained	O
by	O
error	B
minimization	O
using	O
backpropagation	B
to	O
evaluate	O
the	O
gradient	O
of	O
the	O
error	B
function	I
.	O
this	O
involves	O
a	O
slight	O
modiﬁcation	O
of	O
the	O
usual	O
backpropagation	B
algorithm	O
to	O
ensure	O
that	O
the	O
shared-weight	O
constraints	O
are	O
satisﬁed	O
.	O
due	O
to	O
the	O
use	O
of	O
local	O
receptive	O
ﬁelds	O
,	O
the	O
number	O
of	O
weights	O
in	O
the	O
network	O
is	O
smaller	O
than	O
if	O
the	O
network	O
were	O
fully	B
connected	I
.	O
furthermore	O
,	O
the	O
number	O
of	O
independent	B
parameters	O
to	O
be	O
learned	O
from	O
the	O
data	O
is	O
much	O
smaller	O
still	O
,	O
due	O
to	O
the	O
substantial	O
numbers	O
of	O
constraints	O
on	O
the	O
weights	O
.	O
5.5.7	O
soft	B
weight	I
sharing	I
one	O
way	O
to	O
reduce	O
the	O
effective	O
complexity	O
of	O
a	O
network	O
with	O
a	O
large	O
number	O
of	O
weights	O
is	O
to	O
constrain	O
weights	O
within	O
certain	O
groups	O
to	O
be	O
equal	O
.	O
this	O
is	O
the	O
technique	O
of	O
weight	B
sharing	I
that	O
was	O
discussed	O
in	O
section	O
5.5.6	O
as	O
a	O
way	O
of	O
building	O
translation	B
invariance	I
into	O
networks	O
used	O
for	O
image	O
interpretation	O
.	O
it	O
is	O
only	O
appli-	O
cable	O
,	O
however	O
,	O
to	O
particular	O
problems	O
in	O
which	O
the	O
form	O
of	O
the	O
constraints	O
can	O
be	O
speciﬁed	O
in	O
advance	O
.	O
here	O
we	O
consider	O
a	O
form	O
of	O
soft	B
weight	I
sharing	I
(	O
nowlan	O
and	O
hinton	O
,	O
1992	O
)	O
in	O
which	O
the	O
hard	O
constraint	O
of	O
equal	O
weights	O
is	O
replaced	O
by	O
a	O
form	O
of	O
regularization	B
in	O
which	O
groups	O
of	O
weights	O
are	O
encouraged	O
to	O
have	O
similar	O
values	O
.	O
furthermore	O
,	O
the	O
division	O
of	O
weights	O
into	O
groups	O
,	O
the	O
mean	B
weight	O
value	O
for	O
each	O
group	O
,	O
and	O
the	O
spread	O
of	O
values	O
within	O
the	O
groups	O
are	O
all	O
determined	O
as	O
part	O
of	O
the	O
learning	B
process	O
.	O
exercise	O
5.28	O
270	O
5.	O
neural	O
networks	O
section	O
2.3.9	O
exercise	O
5.29	O
recall	O
that	O
the	O
simple	O
weight	B
decay	I
regularizer	O
,	O
given	O
in	O
(	O
5.112	O
)	O
,	O
can	O
be	O
viewed	O
as	O
the	O
negative	O
log	O
of	O
a	O
gaussian	O
prior	B
distribution	O
over	O
the	O
weights	O
.	O
we	O
can	O
encour-	O
age	O
the	O
weight	O
values	O
to	O
form	O
several	O
groups	O
,	O
rather	O
than	O
just	O
one	O
group	O
,	O
by	O
consid-	O
ering	O
instead	O
a	O
probability	B
distribution	O
that	O
is	O
a	O
mixture	O
of	O
gaussians	O
.	O
the	O
centres	O
and	O
variances	O
of	O
the	O
gaussian	O
components	O
,	O
as	O
well	O
as	O
the	O
mixing	O
coefﬁcients	O
,	O
will	O
be	O
considered	O
as	O
adjustable	O
parameters	O
to	O
be	O
determined	O
as	O
part	O
of	O
the	O
learning	B
process	O
.	O
thus	O
,	O
we	O
have	O
a	O
probability	B
density	O
of	O
the	O
form	O
(	O
cid:14	O
)	O
p	O
(	O
w	O
)	O
=	O
p	O
(	O
wi	O
)	O
where	O
i	O
πjn	O
(	O
wi|µj	O
,	O
σ2	O
j	O
)	O
m	O
(	O
cid:2	O
)	O
j=1	O
p	O
(	O
wi	O
)	O
=	O
(	O
5.136	O
)	O
(	O
5.137	O
)	O
and	O
πj	O
are	O
the	O
mixing	O
coefﬁcients	O
.	O
taking	O
the	O
negative	O
logarithm	O
then	O
leads	O
to	O
a	O
regularization	B
function	O
of	O
the	O
form	O
(	O
cid:2	O
)	O
(	O
cid:22	O
)	O
m	O
(	O
cid:2	O
)	O
(	O
cid:23	O
)	O
ω	O
(	O
w	O
)	O
=	O
−	O
ln	O
πjn	O
(	O
wi|µj	O
,	O
σ2	O
j	O
)	O
.	O
(	O
5.138	O
)	O
i	O
j=1	O
the	O
total	O
error	B
function	I
is	O
then	O
given	O
by	O
(	O
cid:4	O
)	O
e	O
(	O
w	O
)	O
=	O
e	O
(	O
w	O
)	O
+	O
λω	O
(	O
w	O
)	O
(	O
5.139	O
)	O
where	O
λ	O
is	O
the	O
regularization	B
coefﬁcient	O
.	O
this	O
error	B
is	O
minimized	O
both	O
with	O
respect	O
to	O
the	O
weights	O
wi	O
and	O
with	O
respect	O
to	O
the	O
parameters	O
{	O
πj	O
,	O
µj	O
,	O
σj	O
}	O
of	O
the	O
mixture	B
model	I
.	O
if	O
the	O
weights	O
were	O
constant	O
,	O
then	O
the	O
parameters	O
of	O
the	O
mixture	B
model	I
could	O
be	O
determined	O
by	O
using	O
the	O
em	O
algorithm	O
discussed	O
in	O
chapter	O
9.	O
however	O
,	O
the	O
dis-	O
tribution	O
of	O
weights	O
is	O
itself	O
evolving	O
during	O
the	O
learning	B
process	O
,	O
and	O
so	O
to	O
avoid	O
nu-	O
merical	O
instability	O
,	O
a	O
joint	O
optimization	O
is	O
performed	O
simultaneously	O
over	O
the	O
weights	O
and	O
the	O
mixture-model	O
parameters	O
.	O
this	O
can	O
be	O
done	O
using	O
a	O
standard	O
optimization	O
algorithm	O
such	O
as	O
conjugate	B
gradients	O
or	O
quasi-newton	O
methods	O
.	O
in	O
order	O
to	O
minimize	O
the	O
total	O
error	B
function	I
,	O
it	O
is	O
necessary	O
to	O
be	O
able	O
to	O
evaluate	O
its	O
derivatives	O
with	O
respect	O
to	O
the	O
various	O
adjustable	O
parameters	O
.	O
to	O
do	O
this	O
it	O
is	O
con-	O
venient	O
to	O
regard	O
the	O
{	O
πj	O
}	O
as	O
prior	B
probabilities	O
and	O
to	O
introduce	O
the	O
corresponding	O
posterior	O
probabilities	O
which	O
,	O
following	O
(	O
2.192	O
)	O
,	O
are	O
given	O
by	O
bayes	O
’	O
theorem	O
in	O
the	O
form	O
γj	O
(	O
w	O
)	O
=	O
(	O
5.140	O
)	O
(	O
cid:5	O
)	O
πjn	O
(	O
w|µj	O
,	O
σ2	O
j	O
)	O
k	O
πkn	O
(	O
w|µk	O
,	O
σ2	O
k	O
)	O
.	O
(	O
cid:2	O
)	O
(	O
wi	O
−	O
µj	O
)	O
σ2	O
j	O
=	O
∂e	O
∂wi	O
+	O
λ	O
γj	O
(	O
wi	O
)	O
j	O
∂	O
(	O
cid:4	O
)	O
e	O
∂wi	O
the	O
derivatives	O
of	O
the	O
total	O
error	B
function	I
with	O
respect	O
to	O
the	O
weights	O
are	O
then	O
given	O
by	O
.	O
(	O
5.141	O
)	O
5.5.	O
regularization	B
in	O
neural	O
networks	O
271	O
the	O
effect	O
of	O
the	O
regularization	B
term	O
is	O
therefore	O
to	O
pull	O
each	O
weight	O
towards	O
the	O
centre	O
of	O
the	O
jth	O
gaussian	O
,	O
with	O
a	O
force	O
proportional	O
to	O
the	O
posterior	B
probability	I
of	O
that	O
gaussian	O
for	O
the	O
given	O
weight	O
.	O
this	O
is	O
precisely	O
the	O
kind	O
of	O
effect	O
that	O
we	O
are	O
seeking	O
.	O
derivatives	O
of	O
the	O
error	B
with	O
respect	O
to	O
the	O
centres	O
of	O
the	O
gaussians	O
are	O
also	O
∂	O
(	O
cid:4	O
)	O
e	O
∂µj	O
(	O
cid:2	O
)	O
i	O
=	O
λ	O
γj	O
(	O
wi	O
)	O
(	O
µi	O
−	O
wj	O
)	O
σ2	O
j	O
(	O
5.142	O
)	O
exercise	O
5.30	O
easily	O
computed	O
to	O
give	O
which	O
has	O
a	O
simple	O
intuitive	O
interpretation	O
,	O
because	O
it	O
pushes	O
µj	O
towards	O
an	O
aver-	O
age	O
of	O
the	O
weight	O
values	O
,	O
weighted	O
by	O
the	O
posterior	O
probabilities	O
that	O
the	O
respective	O
weight	O
parameters	O
were	O
generated	O
by	O
component	O
j.	O
similarly	O
,	O
the	O
derivatives	O
with	O
respect	O
to	O
the	O
variances	O
are	O
given	O
by	O
exercise	O
5.31	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
∂	O
(	O
cid:4	O
)	O
e	O
∂σj	O
(	O
cid:2	O
)	O
i	O
=	O
λ	O
γj	O
(	O
wi	O
)	O
−	O
(	O
wi	O
−	O
µj	O
)	O
2	O
σ3	O
j	O
1	O
σj	O
(	O
5.143	O
)	O
which	O
drives	O
σj	O
towards	O
the	O
weighted	O
average	O
of	O
the	O
squared	O
deviations	O
of	O
the	O
weights	O
around	O
the	O
corresponding	O
centre	O
µj	O
,	O
where	O
the	O
weighting	O
coefﬁcients	O
are	O
again	O
given	O
by	O
the	O
posterior	B
probability	I
that	O
each	O
weight	O
is	O
generated	O
by	O
component	O
j.	O
note	O
that	O
in	O
a	O
practical	O
implementation	O
,	O
new	O
variables	O
ηj	O
deﬁned	O
by	O
σ2	O
j	O
=	O
exp	O
(	O
ηj	O
)	O
(	O
5.144	O
)	O
are	O
introduced	O
,	O
and	O
the	O
minimization	O
is	O
performed	O
with	O
respect	O
to	O
the	O
ηj	O
.	O
this	O
en-	O
sures	O
that	O
the	O
parameters	O
σj	O
remain	O
positive	O
.	O
it	O
also	O
has	O
the	O
effect	O
of	O
discouraging	O
pathological	O
solutions	O
in	O
which	O
one	O
or	O
more	O
of	O
the	O
σj	O
goes	O
to	O
zero	O
,	O
corresponding	O
to	O
a	O
gaussian	O
component	O
collapsing	O
onto	O
one	O
of	O
the	O
weight	B
parameter	I
values	O
.	O
such	O
solutions	O
are	O
discussed	O
in	O
more	O
detail	O
in	O
the	O
context	O
of	O
gaussian	O
mixture	B
models	O
in	O
section	O
9.2.1.	O
for	O
the	O
derivatives	O
with	O
respect	O
to	O
the	O
mixing	O
coefﬁcients	O
πj	O
,	O
we	O
need	O
to	O
take	O
account	O
of	O
the	O
constraints	O
(	O
cid:2	O
)	O
πj	O
=	O
1	O
,	O
0	O
(	O
cid:1	O
)	O
πi	O
(	O
cid:1	O
)	O
1	O
(	O
5.145	O
)	O
j	O
which	O
follow	O
from	O
the	O
interpretation	O
of	O
the	O
πj	O
as	O
prior	B
probabilities	O
.	O
this	O
can	O
be	O
done	O
by	O
expressing	O
the	O
mixing	O
coefﬁcients	O
in	O
terms	O
of	O
a	O
set	O
of	O
auxiliary	O
variables	O
{	O
ηj	O
}	O
using	O
the	O
softmax	B
function	I
given	O
by	O
(	O
cid:5	O
)	O
m	O
πj	O
=	O
exp	O
(	O
ηj	O
)	O
k=1	O
exp	O
(	O
ηk	O
)	O
.	O
(	O
5.146	O
)	O
exercise	O
5.32	O
the	O
derivatives	O
of	O
the	O
regularized	O
error	O
function	O
with	O
respect	O
to	O
the	O
{	O
ηj	O
}	O
then	O
take	O
the	O
form	O
272	O
5.	O
neural	O
networks	O
figure	O
5.18	O
the	O
left	O
ﬁgure	O
shows	O
a	O
two-link	O
robot	B
arm	I
,	O
in	O
which	O
the	O
cartesian	O
coordinates	O
(	O
x1	O
,	O
x2	O
)	O
of	O
the	O
end	O
ef-	O
fector	O
are	O
determined	O
uniquely	O
by	O
the	O
two	O
joint	O
angles	O
θ1	O
and	O
θ2	O
and	O
the	O
(	O
ﬁxed	O
)	O
lengths	O
l1	O
and	O
l2	O
of	O
the	O
arms	O
.	O
this	O
is	O
know	O
as	O
the	O
forward	B
kinematics	I
of	O
the	O
arm	O
.	O
in	O
prac-	O
tice	O
,	O
we	O
have	O
to	O
ﬁnd	O
the	O
joint	O
angles	O
that	O
will	O
give	O
rise	O
to	O
a	O
desired	O
end	O
effector	O
position	O
and	O
,	O
as	O
shown	O
in	O
the	O
right	O
ﬁg-	O
ure	O
,	O
this	O
inversekinematicshas	O
two	O
solutions	O
correspond-	O
ing	O
to	O
‘	O
elbow	O
up	O
’	O
and	O
‘	O
elbow	O
down	O
’	O
.	O
∂	O
(	O
cid:4	O
)	O
e	O
∂ηj	O
(	O
x1	O
,	O
x2	O
)	O
(	O
x1	O
,	O
x2	O
)	O
l2	O
θ2	O
elbow	O
up	O
l1	O
θ1	O
{	O
πj	O
−	O
γj	O
(	O
wi	O
)	O
}	O
.	O
(	O
cid:2	O
)	O
i	O
=	O
elbow	O
down	O
(	O
5.147	O
)	O
we	O
see	O
that	O
πj	O
is	O
therefore	O
driven	O
towards	O
the	O
average	O
posterior	B
probability	I
for	O
com-	O
ponent	O
j	O
.	O
5.6.	O
mixture	O
density	O
networks	O
exercise	O
5.33	O
the	O
goal	O
of	O
supervised	B
learning	I
is	O
to	O
model	O
a	O
conditional	B
distribution	O
p	O
(	O
t|x	O
)	O
,	O
which	O
for	O
many	O
simple	O
regression	B
problems	O
is	O
chosen	O
to	O
be	O
gaussian	O
.	O
however	O
,	O
practical	O
machine	O
learning	O
problems	O
can	O
often	O
have	O
signiﬁcantly	O
non-gaussian	O
distributions	O
.	O
these	O
can	O
arise	O
,	O
for	O
example	O
,	O
with	O
inverse	B
problems	O
in	O
which	O
the	O
distribution	O
can	O
be	O
multimodal	O
,	O
in	O
which	O
case	O
the	O
gaussian	O
assumption	O
can	O
lead	O
to	O
very	O
poor	O
predic-	O
tions	O
.	O
as	O
a	O
simple	O
example	O
of	O
an	O
inverse	B
problem	I
,	O
consider	O
the	O
kinematics	O
of	O
a	O
robot	B
arm	I
,	O
as	O
illustrated	O
in	O
figure	O
5.18.	O
the	O
forward	B
problem	I
involves	O
ﬁnding	O
the	O
end	O
ef-	O
fector	O
position	O
given	O
the	O
joint	O
angles	O
and	O
has	O
a	O
unique	O
solution	O
.	O
however	O
,	O
in	O
practice	O
we	O
wish	O
to	O
move	O
the	O
end	O
effector	O
of	O
the	O
robot	O
to	O
a	O
speciﬁc	O
position	O
,	O
and	O
to	O
do	O
this	O
we	O
must	O
set	O
appropriate	O
joint	O
angles	O
.	O
we	O
therefore	O
need	O
to	O
solve	O
the	O
inverse	B
problem	I
,	O
which	O
has	O
two	O
solutions	O
as	O
seen	O
in	O
figure	O
5.18.	O
forward	O
problems	O
often	O
corresponds	O
to	O
causality	B
in	O
a	O
physical	O
system	O
and	O
gen-	O
erally	O
have	O
a	O
unique	O
solution	O
.	O
for	O
instance	O
,	O
a	O
speciﬁc	O
pattern	O
of	O
symptoms	O
in	O
the	O
human	O
body	O
may	O
be	O
caused	O
by	O
the	O
presence	O
of	O
a	O
particular	O
disease	O
.	O
in	O
pattern	O
recog-	O
nition	O
,	O
however	O
,	O
we	O
typically	O
have	O
to	O
solve	O
an	O
inverse	B
problem	I
,	O
such	O
as	O
trying	O
to	O
predict	O
the	O
presence	O
of	O
a	O
disease	O
given	O
a	O
set	O
of	O
symptoms	O
.	O
if	O
the	O
forward	B
problem	I
involves	O
a	O
many-to-one	O
mapping	O
,	O
then	O
the	O
inverse	B
problem	I
will	O
have	O
multiple	O
solu-	O
tions	O
.	O
for	O
instance	O
,	O
several	O
different	O
diseases	O
may	O
result	O
in	O
the	O
same	O
symptoms	O
.	O
in	O
the	O
robotics	O
example	O
,	O
the	O
kinematics	O
is	O
deﬁned	O
by	O
geometrical	O
equations	O
,	O
and	O
the	O
multimodality	B
is	O
readily	O
apparent	O
.	O
however	O
,	O
in	O
many	O
machine	O
learning	O
problems	O
the	O
presence	O
of	O
multimodality	B
,	O
particularly	O
in	O
problems	O
involving	O
spaces	O
of	O
high	O
di-	O
mensionality	O
,	O
can	O
be	O
less	O
obvious	O
.	O
for	O
tutorial	O
purposes	O
,	O
however	O
,	O
we	O
shall	O
consider	O
a	O
simple	O
toy	O
problem	O
for	O
which	O
we	O
can	O
easily	O
visualize	O
the	O
multimodality	B
.	O
data	O
for	O
this	O
problem	O
is	O
generated	O
by	O
sampling	O
a	O
variable	O
x	O
uniformly	O
over	O
the	O
interval	O
(	O
0	O
,	O
1	O
)	O
,	O
to	O
give	O
a	O
set	O
of	O
values	O
{	O
xn	O
}	O
,	O
and	O
the	O
corresponding	O
target	O
values	O
tn	O
are	O
obtained	O
figure	O
5.19	O
on	O
the	O
left	O
is	O
the	O
data	O
set	O
for	O
a	O
simple	O
‘	O
forward	B
problem	I
’	O
in	O
which	O
the	O
red	O
curve	O
shows	O
the	O
result	O
of	O
ﬁtting	O
a	O
two-layer	O
neural	B
network	I
by	O
minimizing	O
the	O
sum-of-squares	B
error	I
function	O
.	O
the	O
corresponding	O
inverse	B
problem	I
,	O
shown	O
on	O
the	O
right	O
,	O
is	O
obtained	O
by	O
exchanging	O
the	O
roles	O
of	O
x	O
and	O
t.	O
here	O
the	O
same	O
net-	O
work	O
trained	O
again	O
by	O
minimizing	O
the	O
sum-of-squares	B
error	I
function	O
gives	O
a	O
very	O
poor	O
ﬁt	O
to	O
the	O
data	O
due	O
to	O
the	O
multimodality	B
of	O
the	O
data	O
set	O
.	O
1	O
0	O
5.6.	O
mixture	O
density	O
networks	O
273	O
1	O
0	O
0	O
1	O
0	O
1	O
by	O
computing	O
the	O
function	O
xn	O
+	O
0.3	O
sin	O
(	O
2πxn	O
)	O
and	O
then	O
adding	O
uniform	O
noise	O
over	O
the	O
interval	O
(	O
−0.1	O
,	O
0.1	O
)	O
.	O
the	O
inverse	B
problem	I
is	O
then	O
obtained	O
by	O
keeping	O
the	O
same	O
data	O
points	O
but	O
exchanging	O
the	O
roles	O
of	O
x	O
and	O
t.	O
figure	O
5.19	O
shows	O
the	O
data	O
sets	O
for	O
the	O
forward	O
and	O
inverse	B
problems	O
,	O
along	O
with	O
the	O
results	O
of	O
ﬁtting	O
two-layer	O
neural	O
networks	O
having	O
6	O
hidden	O
units	O
and	O
a	O
single	O
linear	O
output	O
unit	O
by	O
minimizing	O
a	O
sum-	O
of-squares	O
error	B
function	I
.	O
least	O
squares	O
corresponds	O
to	O
maximum	B
likelihood	I
under	O
a	O
gaussian	O
assumption	O
.	O
we	O
see	O
that	O
this	O
leads	O
to	O
a	O
very	O
poor	O
model	O
for	O
the	O
highly	O
non-gaussian	O
inverse	B
problem	I
.	O
we	O
therefore	O
seek	O
a	O
general	O
framework	O
for	O
modelling	O
conditional	B
probability	I
distributions	O
.	O
this	O
can	O
be	O
achieved	O
by	O
using	O
a	O
mixture	B
model	I
for	O
p	O
(	O
t|x	O
)	O
in	O
which	O
both	O
the	O
mixing	O
coefﬁcients	O
as	O
well	O
as	O
the	O
component	O
densities	O
are	O
ﬂexible	O
functions	O
of	O
the	O
input	O
vector	O
x	O
,	O
giving	O
rise	O
to	O
the	O
mixture	B
density	I
network	I
.	O
for	O
any	O
given	O
value	O
of	O
x	O
,	O
the	O
mixture	B
model	I
provides	O
a	O
general	O
formalism	O
for	O
modelling	O
an	O
arbitrary	O
conditional	B
density	O
function	O
p	O
(	O
t|x	O
)	O
.	O
provided	O
we	O
consider	O
a	O
sufﬁciently	O
ﬂexible	O
network	O
,	O
we	O
then	O
have	O
a	O
framework	O
for	O
approximating	O
arbitrary	O
conditional	B
distri-	O
butions	O
.	O
here	O
we	O
shall	O
develop	O
the	O
model	O
explicitly	O
for	O
gaussian	O
components	O
,	O
so	O
that	O
k	O
(	O
cid:2	O
)	O
πk	O
(	O
x	O
)	O
n	O
(	O
cid:10	O
)	O
p	O
(	O
t|x	O
)	O
=	O
(	O
cid:11	O
)	O
t|µk	O
(	O
x	O
)	O
,	O
σ2	O
k	O
(	O
x	O
)	O
.	O
(	O
5.148	O
)	O
k=1	O
this	O
is	O
an	O
example	O
of	O
a	O
heteroscedastic	B
model	O
since	O
the	O
noise	O
variance	B
on	O
the	O
data	O
is	O
a	O
function	O
of	O
the	O
input	O
vector	O
x.	O
instead	O
of	O
gaussians	O
,	O
we	O
can	O
use	O
other	O
distribu-	O
tions	O
for	O
the	O
components	O
,	O
such	O
as	O
bernoulli	O
distributions	O
if	O
the	O
target	O
variables	O
are	O
binary	O
rather	O
than	O
continuous	O
.	O
we	O
have	O
also	O
specialized	O
to	O
the	O
case	O
of	O
isotropic	B
co-	O
variances	O
for	O
the	O
components	O
,	O
although	O
the	O
mixture	B
density	I
network	I
can	O
readily	O
be	O
extended	B
to	O
allow	O
for	O
general	O
covariance	B
matrices	O
by	O
representing	O
the	O
covariances	O
using	O
a	O
cholesky	O
factorization	B
(	O
williams	O
,	O
1996	O
)	O
.	O
even	O
with	O
isotropic	B
components	O
,	O
the	O
conditional	B
distribution	O
p	O
(	O
t|x	O
)	O
does	O
not	O
assume	O
factorization	B
with	O
respect	O
to	O
the	O
components	O
of	O
t	O
(	O
in	O
contrast	O
to	O
the	O
standard	O
sum-of-squares	O
regression	B
model	O
)	O
as	O
a	O
consequence	O
of	O
the	O
mixture	B
distribution	I
.	O
we	O
now	O
take	O
the	O
various	O
parameters	O
of	O
the	O
mixture	B
model	I
,	O
namely	O
the	O
mixing	O
k	O
(	O
x	O
)	O
,	O
to	O
be	O
governed	O
by	O
coefﬁcients	O
πk	O
(	O
x	O
)	O
,	O
the	O
means	O
µk	O
(	O
x	O
)	O
,	O
and	O
the	O
variances	O
σ2	O
274	O
5.	O
neural	O
networks	O
xd	O
x1	O
p	O
(	O
t|x	O
)	O
θm	O
θ1	O
θ	O
figure	O
5.20	O
the	O
mixturedensitynetwork	O
can	O
represent	O
general	O
conditional	B
probability	I
densities	O
p	O
(	O
t|x	O
)	O
by	O
considering	O
a	O
parametric	O
mixture	B
model	I
for	O
the	O
distribution	O
of	O
t	O
whose	O
parameters	O
are	O
determined	O
by	O
the	O
outputs	O
of	O
a	O
neural	B
network	I
that	O
takes	O
x	O
as	O
its	O
input	O
vector	O
.	O
t	O
the	O
outputs	O
of	O
a	O
conventional	O
neural	B
network	I
that	O
takes	O
x	O
as	O
its	O
input	O
.	O
the	O
structure	O
of	O
this	O
mixture	B
density	I
network	I
is	O
illustrated	O
in	O
figure	O
5.20.	O
the	O
mixture	B
density	I
network	I
is	O
closely	O
related	O
to	O
the	O
mixture	B
of	I
experts	I
discussed	O
in	O
section	O
14.5.3.	O
the	O
principle	O
difference	O
is	O
that	O
in	O
the	O
mixture	B
density	I
network	I
the	O
same	O
function	O
is	O
used	O
to	O
predict	O
the	O
parameters	O
of	O
all	O
of	O
the	O
component	O
densities	O
as	O
well	O
as	O
the	O
mixing	O
co-	O
efﬁcients	O
,	O
and	O
so	O
the	O
nonlinear	O
hidden	O
units	O
are	O
shared	O
amongst	O
the	O
input-dependent	O
functions	O
.	O
the	O
neural	B
network	I
in	O
figure	O
5.20	O
can	O
,	O
for	O
example	O
,	O
be	O
a	O
two-layer	O
network	O
having	O
sigmoidal	O
(	O
‘	O
tanh	O
’	O
)	O
hidden	O
units	O
.	O
if	O
there	O
are	O
l	O
components	O
in	O
the	O
mixture	B
model	I
(	O
5.148	O
)	O
,	O
and	O
if	O
t	O
has	O
k	O
components	O
,	O
then	O
the	O
network	O
will	O
have	O
l	O
output	O
unit	O
activations	O
denoted	O
by	O
aπ	O
k	O
that	O
determine	O
the	O
mixing	O
coefﬁcients	O
πk	O
(	O
x	O
)	O
,	O
k	O
outputs	O
k	O
that	O
determine	O
the	O
kernel	O
widths	O
σk	O
(	O
x	O
)	O
,	O
and	O
l	O
×	O
k	O
outputs	O
denoted	O
denoted	O
by	O
aσ	O
µ	O
kj	O
that	O
determine	O
the	O
components	O
µkj	O
(	O
x	O
)	O
of	O
the	O
kernel	O
centres	O
µk	O
(	O
x	O
)	O
.	O
the	O
total	O
by	O
a	O
number	O
of	O
network	O
outputs	O
is	O
given	O
by	O
(	O
k	O
+	O
2	O
)	O
l	O
,	O
as	O
compared	O
with	O
the	O
usual	O
k	O
outputs	O
for	O
a	O
network	O
,	O
which	O
simply	O
predicts	O
the	O
conditional	B
means	O
of	O
the	O
target	O
variables	O
.	O
the	O
mixing	O
coefﬁcients	O
must	O
satisfy	O
the	O
constraints	O
πk	O
(	O
x	O
)	O
=	O
1	O
,	O
0	O
(	O
cid:1	O
)	O
πk	O
(	O
x	O
)	O
(	O
cid:1	O
)	O
1	O
(	O
5.149	O
)	O
k	O
(	O
cid:2	O
)	O
k=1	O
which	O
can	O
be	O
achieved	O
using	O
a	O
set	O
of	O
softmax	O
outputs	O
(	O
cid:5	O
)	O
k	O
exp	O
(	O
aπ	O
k	O
)	O
l=1	O
exp	O
(	O
aπ	O
l	O
)	O
πk	O
(	O
x	O
)	O
=	O
.	O
(	O
5.150	O
)	O
similarly	O
,	O
the	O
variances	O
must	O
satisfy	O
σ2	O
of	O
the	O
exponentials	O
of	O
the	O
corresponding	O
network	O
activations	O
using	O
k	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
0	O
and	O
so	O
can	O
be	O
represented	O
in	O
terms	O
σk	O
(	O
x	O
)	O
=	O
exp	O
(	O
aσ	O
k	O
)	O
.	O
(	O
5.151	O
)	O
finally	O
,	O
because	O
the	O
means	O
µk	O
(	O
x	O
)	O
have	O
real	O
components	O
,	O
they	O
can	O
be	O
represented	O
5.6.	O
mixture	O
density	O
networks	O
275	O
directly	O
by	O
the	O
network	O
output	O
activations	O
µ	O
µkj	O
(	O
x	O
)	O
=	O
a	O
kj	O
.	O
(	O
5.152	O
)	O
the	O
adaptive	O
parameters	O
of	O
the	O
mixture	B
density	I
network	I
comprise	O
the	O
vector	O
w	O
of	O
weights	O
and	O
biases	O
in	O
the	O
neural	B
network	I
,	O
that	O
can	O
be	O
set	O
by	O
maximum	B
likelihood	I
,	O
or	O
equivalently	O
by	O
minimizing	O
an	O
error	B
function	I
deﬁned	O
to	O
be	O
the	O
negative	O
logarithm	O
of	O
the	O
likelihood	O
.	O
for	O
independent	O
data	O
,	O
this	O
error	B
function	I
takes	O
the	O
form	O
e	O
(	O
w	O
)	O
=	O
−	O
n	O
(	O
cid:2	O
)	O
ln	O
(	O
cid:24	O
)	O
k	O
(	O
cid:2	O
)	O
πk	O
(	O
xn	O
,	O
w	O
)	O
n	O
(	O
cid:10	O
)	O
tn|µk	O
(	O
xn	O
,	O
w	O
)	O
,	O
σ2	O
k	O
(	O
xn	O
,	O
w	O
)	O
(	O
5.153	O
)	O
(	O
cid:11	O
)	O
(	O
cid:25	O
)	O
n=1	O
k=1	O
where	O
we	O
have	O
made	O
the	O
dependencies	O
on	O
w	O
explicit	O
.	O
in	O
order	O
to	O
minimize	O
the	O
error	B
function	I
,	O
we	O
need	O
to	O
calculate	O
the	O
derivatives	O
of	O
the	O
error	B
e	O
(	O
w	O
)	O
with	O
respect	O
to	O
the	O
components	O
of	O
w.	O
these	O
can	O
be	O
evaluated	O
by	O
using	O
the	O
standard	O
backpropagation	O
procedure	O
,	O
provided	O
we	O
obtain	O
suitable	O
expres-	O
sions	O
for	O
the	O
derivatives	O
of	O
the	O
error	B
with	O
respect	O
to	O
the	O
output-unit	O
activations	O
.	O
these	O
represent	O
error	B
signals	O
δ	O
for	O
each	O
pattern	O
and	O
for	O
each	O
output	O
unit	O
,	O
and	O
can	O
be	O
back-	O
propagated	O
to	O
the	O
hidden	O
units	O
and	O
the	O
error	B
function	I
derivatives	O
evaluated	O
in	O
the	O
usual	O
way	O
.	O
because	O
the	O
error	B
function	I
(	O
5.153	O
)	O
is	O
composed	O
of	O
a	O
sum	O
of	O
terms	O
,	O
one	O
for	O
each	O
training	B
data	O
point	O
,	O
we	O
can	O
consider	O
the	O
derivatives	O
for	O
a	O
particular	O
pattern	O
n	O
and	O
then	O
ﬁnd	O
the	O
derivatives	O
of	O
e	O
by	O
summing	O
over	O
all	O
patterns	O
.	O
because	O
we	O
are	O
dealing	O
with	O
mixture	B
distributions	O
,	O
it	O
is	O
convenient	O
to	O
view	O
the	O
mixing	O
coefﬁcients	O
πk	O
(	O
x	O
)	O
as	O
x-dependent	O
prior	B
probabilities	O
and	O
to	O
introduce	O
the	O
corresponding	O
posterior	O
probabilities	O
given	O
by	O
(	O
cid:5	O
)	O
k	O
γk	O
(	O
t|x	O
)	O
=	O
πknnk	O
l=1	O
πlnnl	O
(	O
5.154	O
)	O
where	O
nnk	O
denotes	O
n	O
(	O
tn|µk	O
(	O
xn	O
)	O
,	O
σ2	O
k	O
(	O
xn	O
)	O
)	O
.	O
the	O
derivatives	O
with	O
respect	O
to	O
the	O
network	O
output	O
activations	O
governing	O
the	O
mix-	O
exercise	O
5.34	O
ing	O
coefﬁcients	O
are	O
given	O
by	O
∂en	O
∂aπ	O
k	O
=	O
πk	O
−	O
γk	O
.	O
(	O
5.155	O
)	O
similarly	O
,	O
the	O
derivatives	O
with	O
respect	O
to	O
the	O
output	O
activations	O
controlling	O
the	O
com-	O
ponent	O
means	O
are	O
given	O
by	O
exercise	O
5.35	O
exercise	O
5.36	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
∂en	O
µ	O
∂a	O
kl	O
=	O
γk	O
µkl	O
−	O
tl	O
σ2	O
k	O
(	O
cid:12	O
)	O
(	O
cid:5	O
)	O
t	O
−	O
µk	O
(	O
cid:5	O
)	O
2	O
σ3	O
k	O
−	O
1	O
σk	O
(	O
cid:13	O
)	O
=	O
−γk	O
∂en	O
∂aσ	O
k	O
finally	O
,	O
the	O
derivatives	O
with	O
respect	O
to	O
the	O
output	O
activations	O
controlling	O
the	O
compo-	O
nent	O
variances	O
are	O
given	O
by	O
.	O
(	O
5.156	O
)	O
.	O
(	O
5.157	O
)	O
276	O
5.	O
neural	O
networks	O
figure	O
5.21	O
(	O
a	O
)	O
plot	O
of	O
the	O
mixing	O
coefﬁcients	O
πk	O
(	O
x	O
)	O
as	O
a	O
function	O
of	O
x	O
for	O
the	O
three	O
kernel	O
functions	O
in	O
a	O
mixture	B
density	I
network	I
trained	O
on	O
the	O
data	O
shown	O
in	O
figure	O
5.19.	O
the	O
model	O
has	O
three	O
gaussian	O
compo-	O
nents	O
,	O
and	O
uses	O
a	O
two-layer	O
multi-	O
layer	O
perceptron	B
with	O
ﬁve	O
‘	O
tanh	O
’	O
sig-	O
moidal	O
units	O
in	O
the	O
hidden	O
layer	O
,	O
and	O
nine	O
outputs	O
(	O
corresponding	O
to	O
the	O
3	O
means	O
and	O
3	O
variances	O
of	O
the	O
gaus-	O
sian	O
components	O
and	O
the	O
3	O
mixing	O
coefﬁcients	O
)	O
.	O
at	O
both	O
small	O
and	O
large	O
values	O
of	O
x	O
,	O
where	O
the	O
conditional	B
probability	I
density	O
of	O
the	O
target	O
data	O
is	O
unimodal	O
,	O
only	O
one	O
of	O
the	O
ker-	O
nels	O
has	O
a	O
high	O
value	O
for	O
its	O
prior	B
probability	O
,	O
while	O
at	O
intermediate	O
val-	O
ues	O
of	O
x	O
,	O
where	O
the	O
conditional	B
den-	O
sity	O
is	O
trimodal	O
,	O
the	O
three	O
mixing	O
co-	O
efﬁcients	O
have	O
comparable	O
values	O
.	O
(	O
b	O
)	O
plots	O
of	O
the	O
means	O
µk	O
(	O
x	O
)	O
using	O
the	O
same	O
colour	O
coding	O
as	O
for	O
the	O
mixing	O
coefﬁcients	O
.	O
(	O
c	O
)	O
plot	O
of	O
the	O
contours	O
of	O
the	O
corresponding	O
con-	O
ditional	O
probability	B
density	O
of	O
the	O
tar-	O
get	O
data	O
for	O
the	O
same	O
mixture	B
den-	O
sity	O
network	O
.	O
the	O
ap-	O
proximate	O
conditional	B
mode	O
,	O
shown	O
by	O
the	O
red	O
points	O
,	O
of	O
the	O
conditional	B
density	O
.	O
(	O
d	O
)	O
plot	O
of	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
(	O
a	O
)	O
0	O
(	O
b	O
)	O
1	O
0	O
1	O
0	O
1	O
0	O
(	O
c	O
)	O
(	O
d	O
)	O
1	O
1	O
we	O
illustrate	O
the	O
use	O
of	O
a	O
mixture	B
density	I
network	I
by	O
returning	O
to	O
the	O
toy	O
ex-	O
ample	O
of	O
an	O
inverse	B
problem	I
shown	O
in	O
figure	O
5.19.	O
plots	O
of	O
the	O
mixing	O
coefﬁ-	O
cients	O
πk	O
(	O
x	O
)	O
,	O
the	O
means	O
µk	O
(	O
x	O
)	O
,	O
and	O
the	O
conditional	B
density	O
contours	O
corresponding	O
to	O
p	O
(	O
t|x	O
)	O
,	O
are	O
shown	O
in	O
figure	O
5.21.	O
the	O
outputs	O
of	O
the	O
neural	B
network	I
,	O
and	O
hence	O
the	O
parameters	O
in	O
the	O
mixture	B
model	I
,	O
are	O
necessarily	O
continuous	O
single-valued	O
functions	O
of	O
the	O
input	O
variables	O
.	O
however	O
,	O
we	O
see	O
from	O
figure	O
5.21	O
(	O
c	O
)	O
that	O
the	O
model	O
is	O
able	O
to	O
produce	O
a	O
conditional	B
density	O
that	O
is	O
unimodal	O
for	O
some	O
values	O
of	O
x	O
and	O
trimodal	O
for	O
other	O
values	O
by	O
modulating	O
the	O
amplitudes	O
of	O
the	O
mixing	O
components	O
πk	O
(	O
x	O
)	O
.	O
once	O
a	O
mixture	B
density	I
network	I
has	O
been	O
trained	O
,	O
it	O
can	O
predict	O
the	O
conditional	B
density	O
function	O
of	O
the	O
target	O
data	O
for	O
any	O
given	O
value	O
of	O
the	O
input	O
vector	O
.	O
this	O
conditional	B
density	O
represents	O
a	O
complete	O
description	O
of	O
the	O
generator	O
of	O
the	O
data	O
,	O
so	O
far	O
as	O
the	O
problem	O
of	O
predicting	O
the	O
value	O
of	O
the	O
output	O
vector	O
is	O
concerned	O
.	O
from	O
this	O
density	B
function	O
we	O
can	O
calculate	O
more	O
speciﬁc	O
quantities	O
that	O
may	O
be	O
of	O
interest	O
in	O
different	O
applications	O
.	O
one	O
of	O
the	O
simplest	O
of	O
these	O
is	O
the	O
mean	B
,	O
corresponding	O
to	O
the	O
conditional	B
average	O
of	O
the	O
target	O
data	O
,	O
and	O
is	O
given	O
by	O
(	O
cid:6	O
)	O
k	O
(	O
cid:2	O
)	O
k=1	O
e	O
[	O
t|x	O
]	O
=	O
tp	O
(	O
t|x	O
)	O
dt	O
=	O
πk	O
(	O
x	O
)	O
µk	O
(	O
x	O
)	O
(	O
5.158	O
)	O
5.7.	O
bayesian	O
neural	O
networks	O
277	O
where	O
we	O
have	O
used	O
(	O
5.148	O
)	O
.	O
because	O
a	O
standard	O
network	O
trained	O
by	O
least	O
squares	O
is	O
approximating	O
the	O
conditional	B
mean	O
,	O
we	O
see	O
that	O
a	O
mixture	B
density	I
network	I
can	O
reproduce	O
the	O
conventional	O
least-squares	O
result	O
as	O
a	O
special	O
case	O
.	O
of	O
course	O
,	O
as	O
we	O
have	O
already	O
noted	O
,	O
for	O
a	O
multimodal	O
distribution	O
the	O
conditional	B
mean	O
is	O
of	O
limited	O
value	O
.	O
we	O
can	O
similarly	O
evaluate	O
the	O
variance	B
of	O
the	O
density	B
function	O
about	O
the	O
condi-	O
(	O
cid:8	O
)	O
(	O
cid:5	O
)	O
t	O
−	O
e	O
[	O
t|x	O
]	O
(	O
cid:5	O
)	O
2	O
|x	O
(	O
cid:9	O
)	O
⎧⎨⎩σ2	O
k	O
(	O
cid:2	O
)	O
πk	O
(	O
x	O
)	O
k	O
(	O
x	O
)	O
+	O
k=1	O
’	O
’	O
’	O
’	O
’	O
µk	O
(	O
x	O
)	O
−	O
k	O
(	O
cid:2	O
)	O
l=1	O
’	O
’	O
’	O
’	O
’	O
2	O
πl	O
(	O
x	O
)	O
µl	O
(	O
x	O
)	O
(	O
5.159	O
)	O
⎫⎬⎭	O
(	O
5.160	O
)	O
exercise	O
5.37	O
tional	O
average	O
,	O
to	O
give	O
s2	O
(	O
x	O
)	O
=	O
e	O
=	O
where	O
we	O
have	O
used	O
(	O
5.148	O
)	O
and	O
(	O
5.158	O
)	O
.	O
this	O
is	O
more	O
general	O
than	O
the	O
corresponding	O
least-squares	O
result	O
because	O
the	O
variance	B
is	O
a	O
function	O
of	O
x.	O
we	O
have	O
seen	O
that	O
for	O
multimodal	O
distributions	O
,	O
the	O
conditional	B
mean	O
can	O
give	O
a	O
poor	O
representation	O
of	O
the	O
data	O
.	O
for	O
instance	O
,	O
in	O
controlling	O
the	O
simple	O
robot	B
arm	I
shown	O
in	O
figure	O
5.18	O
,	O
we	O
need	O
to	O
pick	O
one	O
of	O
the	O
two	O
possible	O
joint	O
angle	O
settings	O
in	O
order	O
to	O
achieve	O
the	O
desired	O
end-effector	O
location	O
,	O
whereas	O
the	O
average	O
of	O
the	O
two	O
solutions	O
is	O
not	O
itself	O
a	O
solution	O
.	O
in	O
such	O
cases	O
,	O
the	O
conditional	B
mode	O
may	O
be	O
of	O
more	O
value	O
.	O
because	O
the	O
conditional	B
mode	O
for	O
the	O
mixture	B
density	I
network	I
does	O
not	O
have	O
a	O
simple	O
analytical	O
solution	O
,	O
this	O
would	O
require	O
numerical	O
iteration	O
.	O
a	O
simple	O
alternative	O
is	O
to	O
take	O
the	O
mean	B
of	O
the	O
most	O
probable	O
component	O
(	O
i.e.	O
,	O
the	O
one	O
with	O
the	O
largest	O
mixing	B
coefﬁcient	I
)	O
at	O
each	O
value	O
of	O
x.	O
this	O
is	O
shown	O
for	O
the	O
toy	O
data	O
set	O
in	O
figure	O
5.21	O
(	O
d	O
)	O
.	O
5.7.	O
bayesian	O
neural	O
networks	O
so	O
far	O
,	O
our	O
discussion	O
of	O
neural	O
networks	O
has	O
focussed	O
on	O
the	O
use	O
of	O
maximum	O
like-	O
lihood	O
to	O
determine	O
the	O
network	O
parameters	O
(	O
weights	O
and	O
biases	O
)	O
.	O
regularized	O
max-	O
imum	O
likelihood	O
can	O
be	O
interpreted	O
as	O
a	O
map	O
(	O
maximum	B
posterior	I
)	O
approach	O
in	O
which	O
the	O
regularizer	O
can	O
be	O
viewed	O
as	O
the	O
logarithm	O
of	O
a	O
prior	B
parameter	O
distribu-	O
tion	O
.	O
however	O
,	O
in	O
a	O
bayesian	O
treatment	O
we	O
need	O
to	O
marginalize	O
over	O
the	O
distribution	O
of	O
parameters	O
in	O
order	O
to	O
make	O
predictions	O
.	O
in	O
section	O
3.3	O
,	O
we	O
developed	O
a	O
bayesian	O
solution	O
for	O
a	O
simple	O
linear	B
regression	I
model	O
under	O
the	O
assumption	O
of	O
gaussian	O
noise	O
.	O
we	O
saw	O
that	O
the	O
posterior	O
distribu-	O
tion	O
,	O
which	O
is	O
gaussian	O
,	O
could	O
be	O
evaluated	O
exactly	O
and	O
that	O
the	O
predictive	O
distribu-	O
tion	O
could	O
also	O
be	O
found	O
in	O
closed	O
form	O
.	O
in	O
the	O
case	O
of	O
a	O
multilayered	O
network	O
,	O
the	O
highly	O
nonlinear	O
dependence	O
of	O
the	O
network	O
function	O
on	O
the	O
parameter	O
values	O
means	O
that	O
an	O
exact	O
bayesian	O
treatment	O
can	O
no	O
longer	O
be	O
found	O
.	O
in	O
fact	O
,	O
the	O
log	O
of	O
the	O
pos-	O
terior	O
distribution	O
will	O
be	O
nonconvex	O
,	O
corresponding	O
to	O
the	O
multiple	O
local	B
minima	O
in	O
the	O
error	B
function	I
.	O
the	O
technique	O
of	O
variational	B
inference	I
,	O
to	O
be	O
discussed	O
in	O
chapter	O
10	O
,	O
has	O
been	O
applied	O
to	O
bayesian	O
neural	O
networks	O
using	O
a	O
factorized	O
gaussian	O
approximation	O
278	O
5.	O
neural	O
networks	O
to	O
the	O
posterior	O
distribution	O
(	O
hinton	O
and	O
van	O
camp	O
,	O
1993	O
)	O
and	O
also	O
using	O
a	O
full-	O
covariance	B
gaussian	O
(	O
barber	O
and	O
bishop	O
,	O
1998a	O
;	O
barber	O
and	O
bishop	O
,	O
1998b	O
)	O
.	O
the	O
most	O
complete	O
treatment	O
,	O
however	O
,	O
has	O
been	O
based	O
on	O
the	O
laplace	O
approximation	O
(	O
mackay	O
,	O
1992c	O
;	O
mackay	O
,	O
1992b	O
)	O
and	O
forms	O
the	O
basis	O
for	O
the	O
discussion	O
given	O
here	O
.	O
we	O
will	O
approximate	O
the	O
posterior	O
distribution	O
by	O
a	O
gaussian	O
,	O
centred	O
at	O
a	O
mode	O
of	O
the	O
true	O
posterior	O
.	O
furthermore	O
,	O
we	O
shall	O
assume	O
that	O
the	O
covariance	B
of	O
this	O
gaus-	O
sian	O
is	O
small	O
so	O
that	O
the	O
network	O
function	O
is	O
approximately	O
linear	O
with	O
respect	O
to	O
the	O
parameters	O
over	O
the	O
region	O
of	O
parameter	O
space	O
for	O
which	O
the	O
posterior	B
probability	I
is	O
signiﬁcantly	O
nonzero	O
.	O
with	O
these	O
two	O
approximations	O
,	O
we	O
will	O
obtain	O
models	O
that	O
are	O
analogous	O
to	O
the	O
linear	B
regression	I
and	O
classiﬁcation	B
models	O
discussed	O
in	O
earlier	O
chapters	O
and	O
so	O
we	O
can	O
exploit	O
the	O
results	O
obtained	O
there	O
.	O
we	O
can	O
then	O
make	O
use	O
of	O
the	O
evidence	O
framework	O
to	O
provide	O
point	O
estimates	O
for	O
the	O
hyperparameters	O
and	O
to	O
compare	O
alternative	O
models	O
(	O
for	O
example	O
,	O
networks	O
having	O
different	O
numbers	O
of	O
hid-	O
den	O
units	O
)	O
.	O
to	O
start	O
with	O
,	O
we	O
shall	O
discuss	O
the	O
regression	B
case	O
and	O
then	O
later	O
consider	O
the	O
modiﬁcations	O
needed	O
for	O
solving	O
classiﬁcation	B
tasks	O
.	O
5.7.1	O
posterior	O
parameter	O
distribution	O
consider	O
the	O
problem	O
of	O
predicting	O
a	O
single	O
continuous	O
target	O
variable	O
t	O
from	O
a	O
vector	O
x	O
of	O
inputs	O
(	O
the	O
extension	O
to	O
multiple	O
targets	O
is	O
straightforward	O
)	O
.	O
we	O
shall	O
suppose	O
that	O
the	O
conditional	B
distribution	O
p	O
(	O
t|x	O
)	O
is	O
gaussian	O
,	O
with	O
an	O
x-dependent	O
mean	B
given	O
by	O
the	O
output	O
of	O
a	O
neural	B
network	I
model	O
y	O
(	O
x	O
,	O
w	O
)	O
,	O
and	O
with	O
precision	O
(	O
inverse	B
variance	O
)	O
β	O
p	O
(	O
t|x	O
,	O
w	O
,	O
β	O
)	O
=	O
n	O
(	O
t|y	O
(	O
x	O
,	O
w	O
)	O
,	O
β	O
−1	O
)	O
.	O
(	O
5.161	O
)	O
similarly	O
,	O
we	O
shall	O
choose	O
a	O
prior	B
distribution	O
over	O
the	O
weights	O
w	O
that	O
is	O
gaussian	O
of	O
the	O
form	O
(	O
5.162	O
)	O
for	O
an	O
i.i.d	O
.	O
data	O
set	O
of	O
n	O
observations	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
with	O
a	O
corresponding	O
set	O
of	O
target	O
values	O
d	O
=	O
{	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
}	O
,	O
the	O
likelihood	B
function	I
is	O
given	O
by	O
p	O
(	O
w|α	O
)	O
=	O
n	O
(	O
w|0	O
,	O
α	O
−1i	O
)	O
.	O
n	O
(	O
cid:14	O
)	O
p	O
(	O
d|w	O
,	O
β	O
)	O
=	O
n	O
(	O
tn|y	O
(	O
xn	O
,	O
w	O
)	O
,	O
β	O
−1	O
)	O
n=1	O
and	O
so	O
the	O
resulting	O
posterior	O
distribution	O
is	O
then	O
p	O
(	O
w|d	O
,	O
α	O
,	O
β	O
)	O
∝	O
p	O
(	O
w|α	O
)	O
p	O
(	O
d|w	O
,	O
β	O
)	O
.	O
(	O
5.163	O
)	O
(	O
5.164	O
)	O
which	O
,	O
as	O
a	O
consequence	O
of	O
the	O
nonlinear	O
dependence	O
of	O
y	O
(	O
x	O
,	O
w	O
)	O
on	O
w	O
,	O
will	O
be	O
non-	O
gaussian	O
.	O
we	O
can	O
ﬁnd	O
a	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
distribution	O
by	O
using	O
the	O
laplace	O
approximation	O
.	O
to	O
do	O
this	O
,	O
we	O
must	O
ﬁrst	O
ﬁnd	O
a	O
(	O
local	B
)	O
maximum	O
of	O
the	O
posterior	O
,	O
and	O
this	O
must	O
be	O
done	O
using	O
iterative	O
numerical	O
optimization	O
.	O
as	O
usual	O
,	O
it	O
is	O
convenient	O
to	O
maximize	O
the	O
logarithm	O
of	O
the	O
posterior	O
,	O
which	O
can	O
be	O
written	O
in	O
the	O
5.7.	O
bayesian	O
neural	O
networks	O
279	O
form	O
ln	O
p	O
(	O
w|d	O
)	O
=	O
−	O
α	O
2	O
wtw	O
−	O
β	O
2	O
n	O
(	O
cid:2	O
)	O
n=1	O
{	O
y	O
(	O
xn	O
,	O
w	O
)	O
−	O
tn	O
}	O
2	O
+	O
const	O
(	O
5.165	O
)	O
which	O
corresponds	O
to	O
a	O
regularized	O
sum-of-squares	O
error	B
function	I
.	O
assuming	O
for	O
the	O
moment	O
that	O
α	O
and	O
β	O
are	O
ﬁxed	O
,	O
we	O
can	O
ﬁnd	O
a	O
maximum	O
of	O
the	O
posterior	O
,	O
which	O
we	O
denote	O
wmap	O
,	O
by	O
standard	O
nonlinear	O
optimization	O
algorithms	O
such	O
as	O
conjugate	B
gradients	O
,	O
using	O
error	B
backpropagation	I
to	O
evaluate	O
the	O
required	O
derivatives	O
.	O
having	O
found	O
a	O
mode	O
wmap	O
,	O
we	O
can	O
then	O
build	O
a	O
local	B
gaussian	O
approximation	O
by	O
evaluating	O
the	O
matrix	O
of	O
second	O
derivatives	O
of	O
the	O
negative	O
log	O
posterior	O
distribu-	O
tion	O
.	O
from	O
(	O
5.165	O
)	O
,	O
this	O
is	O
given	O
by	O
a	O
=	O
−∇∇	O
ln	O
p	O
(	O
w|d	O
,	O
α	O
,	O
β	O
)	O
=	O
αi	O
+	O
βh	O
(	O
5.166	O
)	O
where	O
h	O
is	O
the	O
hessian	O
matrix	O
comprising	O
the	O
second	O
derivatives	O
of	O
the	O
sum-of-	O
squares	O
error	B
function	I
with	O
respect	O
to	O
the	O
components	O
of	O
w.	O
algorithms	O
for	O
comput-	O
ing	O
and	O
approximating	O
the	O
hessian	O
were	O
discussed	O
in	O
section	O
5.4.	O
the	O
corresponding	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
is	O
then	O
given	O
from	O
(	O
4.134	O
)	O
by	O
q	O
(	O
w|d	O
)	O
=	O
n	O
(	O
w|wmap	O
,	O
a−1	O
)	O
.	O
(	O
5.167	O
)	O
similarly	O
,	O
the	O
predictive	B
distribution	I
is	O
obtained	O
by	O
marginalizing	O
with	O
respect	O
to	O
this	O
posterior	O
distribution	O
p	O
(	O
t|x	O
,	O
d	O
)	O
=	O
(	O
cid:6	O
)	O
p	O
(	O
t|x	O
,	O
w	O
)	O
q	O
(	O
w|d	O
)	O
dw	O
.	O
(	O
5.168	O
)	O
however	O
,	O
even	O
with	O
the	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
,	O
this	O
integration	O
is	O
still	O
analytically	O
intractable	O
due	O
to	O
the	O
nonlinearity	O
of	O
the	O
network	O
function	O
y	O
(	O
x	O
,	O
w	O
)	O
as	O
a	O
function	O
of	O
w.	O
to	O
make	O
progress	O
,	O
we	O
now	O
assume	O
that	O
the	O
posterior	O
distribution	O
has	O
small	O
variance	B
compared	O
with	O
the	O
characteristic	O
scales	O
of	O
w	O
over	O
which	O
y	O
(	O
x	O
,	O
w	O
)	O
is	O
varying	O
.	O
this	O
allows	O
us	O
to	O
make	O
a	O
taylor	O
series	O
expansion	O
of	O
the	O
network	O
function	O
around	O
wmap	O
and	O
retain	O
only	O
the	O
linear	O
terms	O
y	O
(	O
x	O
,	O
w	O
)	O
(	O
cid:7	O
)	O
y	O
(	O
x	O
,	O
wmap	O
)	O
+	O
gt	O
(	O
w	O
−	O
wmap	O
)	O
(	O
5.169	O
)	O
where	O
we	O
have	O
deﬁned	O
g	O
=	O
∇wy	O
(	O
x	O
,	O
w	O
)	O
|w=wmap	O
.	O
(	O
5.170	O
)	O
with	O
this	O
approximation	O
,	O
we	O
now	O
have	O
a	O
linear-gaussian	O
model	O
with	O
a	O
gaussian	O
distribution	O
for	O
p	O
(	O
w	O
)	O
and	O
a	O
gaussian	O
for	O
p	O
(	O
t|w	O
)	O
whose	O
mean	B
is	O
a	O
linear	O
function	O
of	O
w	O
of	O
the	O
form	O
t|y	O
(	O
x	O
,	O
wmap	O
)	O
+	O
gt	O
(	O
w	O
−	O
wmap	O
)	O
,	O
β	O
−1	O
.	O
(	O
5.171	O
)	O
p	O
(	O
t|x	O
,	O
w	O
,	O
β	O
)	O
(	O
cid:7	O
)	O
n	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
(	O
cid:11	O
)	O
exercise	O
5.38	O
we	O
can	O
therefore	O
make	O
use	O
of	O
the	O
general	O
result	O
(	O
2.115	O
)	O
for	O
the	O
marginal	B
p	O
(	O
t	O
)	O
to	O
give	O
p	O
(	O
t|x	O
,	O
d	O
,	O
α	O
,	O
β	O
)	O
=	O
n	O
(	O
cid:10	O
)	O
t|y	O
(	O
x	O
,	O
wmap	O
)	O
,	O
σ2	O
(	O
x	O
)	O
(	O
5.172	O
)	O
280	O
5.	O
neural	O
networks	O
where	O
the	O
input-dependent	O
variance	B
is	O
given	O
by	O
−1	O
+	O
gta−1g	O
.	O
σ2	O
(	O
x	O
)	O
=	O
β	O
(	O
5.173	O
)	O
we	O
see	O
that	O
the	O
predictive	B
distribution	I
p	O
(	O
t|x	O
,	O
d	O
)	O
is	O
a	O
gaussian	O
whose	O
mean	B
is	O
given	O
by	O
the	O
network	O
function	O
y	O
(	O
x	O
,	O
wmap	O
)	O
with	O
the	O
parameter	O
set	O
to	O
their	O
map	O
value	O
.	O
the	O
variance	B
has	O
two	O
terms	O
,	O
the	O
ﬁrst	O
of	O
which	O
arises	O
from	O
the	O
intrinsic	O
noise	O
on	O
the	O
target	O
variable	O
,	O
whereas	O
the	O
second	O
is	O
an	O
x-dependent	O
term	O
that	O
expresses	O
the	O
uncertainty	O
in	O
the	O
interpolant	O
due	O
to	O
the	O
uncertainty	O
in	O
the	O
model	O
parameters	O
w.	O
this	O
should	O
be	O
compared	O
with	O
the	O
corresponding	O
predictive	B
distribution	I
for	O
the	O
linear	B
regression	I
model	O
,	O
given	O
by	O
(	O
3.58	O
)	O
and	O
(	O
3.59	O
)	O
.	O
5.7.2	O
hyperparameter	B
optimization	O
so	O
far	O
,	O
we	O
have	O
assumed	O
that	O
the	O
hyperparameters	O
α	O
and	O
β	O
are	O
ﬁxed	O
and	O
known	O
.	O
we	O
can	O
make	O
use	O
of	O
the	O
evidence	O
framework	O
,	O
discussed	O
in	O
section	O
3.5	O
,	O
together	O
with	O
the	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
obtained	O
using	O
the	O
laplace	O
approxima-	O
tion	O
,	O
to	O
obtain	O
a	O
practical	O
procedure	O
for	O
choosing	O
the	O
values	O
of	O
such	O
hyperparameters	O
.	O
the	O
marginal	B
likelihood	I
,	O
or	O
evidence	O
,	O
for	O
the	O
hyperparameters	O
is	O
obtained	O
by	O
integrating	O
over	O
the	O
network	O
weights	O
p	O
(	O
d|α	O
,	O
β	O
)	O
=	O
(	O
cid:6	O
)	O
p	O
(	O
d|w	O
,	O
β	O
)	O
p	O
(	O
w|α	O
)	O
dw	O
.	O
(	O
5.174	O
)	O
exercise	O
5.39	O
this	O
is	O
easily	O
evaluated	O
by	O
making	O
use	O
of	O
the	O
laplace	O
approximation	O
result	O
(	O
4.135	O
)	O
.	O
taking	O
logarithms	O
then	O
gives	O
ln	O
p	O
(	O
d|α	O
,	O
β	O
)	O
(	O
cid:7	O
)	O
−e	O
(	O
wmap	O
)	O
−	O
1	O
2	O
ln|a|	O
+	O
w	O
2	O
ln	O
β	O
−	O
n	O
2	O
ln	O
α	O
+	O
n	O
2	O
ln	O
(	O
2π	O
)	O
(	O
5.175	O
)	O
where	O
w	O
is	O
the	O
total	O
number	O
of	O
parameters	O
in	O
w	O
,	O
and	O
the	O
regularized	O
error	O
function	O
is	O
deﬁned	O
by	O
n	O
(	O
cid:2	O
)	O
n=1	O
e	O
(	O
wmap	O
)	O
=	O
β	O
2	O
{	O
y	O
(	O
xn	O
,	O
wmap	O
)	O
−	O
tn	O
}	O
2	O
+	O
α	O
2	O
wt	O
mapwmap	O
.	O
(	O
5.176	O
)	O
we	O
see	O
that	O
this	O
takes	O
the	O
same	O
form	O
as	O
the	O
corresponding	O
result	O
(	O
3.86	O
)	O
for	O
the	O
linear	B
regression	I
model	O
.	O
in	O
the	O
evidence	O
framework	O
,	O
we	O
make	O
point	O
estimates	O
for	O
α	O
and	O
β	O
by	O
maximizing	O
ln	O
p	O
(	O
d|α	O
,	O
β	O
)	O
.	O
consider	O
ﬁrst	O
the	O
maximization	O
with	O
respect	O
to	O
α	O
,	O
which	O
can	O
be	O
done	O
by	O
analogy	O
with	O
the	O
linear	B
regression	I
case	O
discussed	O
in	O
section	O
3.5.2.	O
we	O
ﬁrst	O
deﬁne	O
the	O
eigenvalue	O
equation	O
(	O
5.177	O
)	O
where	O
h	O
is	O
the	O
hessian	O
matrix	O
comprising	O
the	O
second	O
derivatives	O
of	O
the	O
sum-of-	O
squares	O
error	B
function	I
,	O
evaluated	O
at	O
w	O
=	O
wmap	O
.	O
by	O
analogy	O
with	O
(	O
3.92	O
)	O
,	O
we	O
obtain	O
βhui	O
=	O
λiui	O
α	O
=	O
γ	O
wt	O
mapwmap	O
(	O
5.178	O
)	O
section	O
3.5.3	O
section	O
5.1.1	O
5.7.	O
bayesian	O
neural	O
networks	O
281	O
where	O
γ	O
represents	O
the	O
effective	B
number	I
of	I
parameters	I
and	O
is	O
deﬁned	O
by	O
w	O
(	O
cid:2	O
)	O
i=1	O
γ	O
=	O
λi	O
α	O
+	O
λi	O
.	O
(	O
5.179	O
)	O
note	O
that	O
this	O
result	O
was	O
exact	O
for	O
the	O
linear	B
regression	I
case	O
.	O
for	O
the	O
nonlinear	O
neural	B
network	I
,	O
however	O
,	O
it	O
ignores	O
the	O
fact	O
that	O
changes	O
in	O
α	O
will	O
cause	O
changes	O
in	O
the	O
hessian	O
h	O
,	O
which	O
in	O
turn	O
will	O
change	O
the	O
eigenvalues	O
.	O
we	O
have	O
therefore	O
implicitly	O
ignored	O
terms	O
involving	O
the	O
derivatives	O
of	O
λi	O
with	O
respect	O
to	O
α.	O
similarly	O
,	O
from	O
(	O
3.95	O
)	O
we	O
see	O
that	O
maximizing	O
the	O
evidence	O
with	O
respect	O
to	O
β	O
gives	O
the	O
re-estimation	O
formula	O
1	O
β	O
=	O
1	O
n	O
−	O
γ	O
n	O
(	O
cid:2	O
)	O
n=1	O
{	O
y	O
(	O
xn	O
,	O
wmap	O
)	O
−	O
tn	O
}	O
2	O
.	O
(	O
5.180	O
)	O
as	O
with	O
the	O
linear	O
model	O
,	O
we	O
need	O
to	O
alternate	O
between	O
re-estimation	O
of	O
the	O
hyper-	O
parameters	O
α	O
and	O
β	O
and	O
updating	O
of	O
the	O
posterior	O
distribution	O
.	O
the	O
situation	O
with	O
a	O
neural	B
network	I
model	O
is	O
more	O
complex	O
,	O
however	O
,	O
due	O
to	O
the	O
multimodality	B
of	O
the	O
posterior	O
distribution	O
.	O
as	O
a	O
consequence	O
,	O
the	O
solution	O
for	O
wmap	O
found	O
by	O
maximiz-	O
ing	O
the	O
log	O
posterior	O
will	O
depend	O
on	O
the	O
initialization	O
of	O
w.	O
solutions	O
that	O
differ	O
only	O
as	O
a	O
consequence	O
of	O
the	O
interchange	O
and	O
sign	O
reversal	O
symmetries	B
in	O
the	O
hidden	O
units	O
are	O
identical	O
so	O
far	O
as	O
predictions	O
are	O
concerned	O
,	O
and	O
it	O
is	O
irrelevant	O
which	O
of	O
the	O
equivalent	O
solutions	O
is	O
found	O
.	O
however	O
,	O
there	O
may	O
be	O
inequivalent	O
solutions	O
as	O
well	O
,	O
and	O
these	O
will	O
generally	O
yield	O
different	O
values	O
for	O
the	O
optimized	O
hyperparameters	O
.	O
in	O
order	O
to	O
compare	O
different	O
models	O
,	O
for	O
example	O
neural	O
networks	O
having	O
differ-	O
ent	O
numbers	O
of	O
hidden	O
units	O
,	O
we	O
need	O
to	O
evaluate	O
the	O
model	B
evidence	I
p	O
(	O
d	O
)	O
.	O
this	O
can	O
be	O
approximated	O
by	O
taking	O
(	O
5.175	O
)	O
and	O
substituting	O
the	O
values	O
of	O
α	O
and	O
β	O
obtained	O
from	O
the	O
iterative	O
optimization	O
of	O
these	O
hyperparameters	O
.	O
a	O
more	O
careful	O
evaluation	O
is	O
obtained	O
by	O
marginalizing	O
over	O
α	O
and	O
β	O
,	O
again	O
by	O
making	O
a	O
gaussian	O
approxima-	O
tion	O
(	O
mackay	O
,	O
1992c	O
;	O
bishop	O
,	O
1995a	O
)	O
.	O
in	O
either	O
case	O
,	O
it	O
is	O
necessary	O
to	O
evaluate	O
the	O
determinant	O
|a|	O
of	O
the	O
hessian	O
matrix	O
.	O
this	O
can	O
be	O
problematic	O
in	O
practice	O
because	O
the	O
determinant	O
,	O
unlike	O
the	O
trace	O
,	O
is	O
sensitive	O
to	O
the	O
small	O
eigenvalues	O
that	O
are	O
often	O
difﬁcult	O
to	O
determine	O
accurately	O
.	O
the	O
laplace	O
approximation	O
is	O
based	O
on	O
a	O
local	B
quadratic	O
expansion	O
around	O
a	O
mode	O
of	O
the	O
posterior	O
distribution	O
over	O
weights	O
.	O
we	O
have	O
seen	O
in	O
section	O
5.1.1	O
that	O
any	O
given	O
mode	O
in	O
a	O
two-layer	O
network	O
is	O
a	O
member	O
of	O
a	O
set	O
of	O
m	O
!	O
2m	O
equivalent	O
modes	O
that	O
differ	O
by	O
interchange	O
and	O
sign-change	O
symmetries	B
,	O
where	O
m	O
is	O
the	O
num-	O
ber	O
of	O
hidden	O
units	O
.	O
when	O
comparing	O
networks	O
having	O
different	O
numbers	O
of	O
hid-	O
den	O
units	O
,	O
this	O
can	O
be	O
taken	O
into	O
account	O
by	O
multiplying	O
the	O
evidence	O
by	O
a	O
factor	O
of	O
m	O
!	O
2m	O
.	O
5.7.3	O
bayesian	O
neural	O
networks	O
for	O
classiﬁcation	O
so	O
far	O
,	O
we	O
have	O
used	O
the	O
laplace	O
approximation	O
to	O
develop	O
a	O
bayesian	O
treat-	O
ment	O
of	O
neural	B
network	I
regression	O
models	O
.	O
we	O
now	O
discuss	O
the	O
modiﬁcations	O
to	O
282	O
5.	O
neural	O
networks	O
exercise	O
5.40	O
exercise	O
5.41	O
(	O
cid:2	O
)	O
this	O
framework	O
that	O
arise	O
when	O
it	O
is	O
applied	O
to	O
classiﬁcation	B
.	O
here	O
we	O
shall	O
con-	O
sider	O
a	O
network	O
having	O
a	O
single	O
logistic	B
sigmoid	I
output	O
corresponding	O
to	O
a	O
two-class	O
classiﬁcation	B
problem	O
.	O
the	O
extension	O
to	O
networks	O
with	O
multiclass	B
softmax	O
outputs	O
is	O
straightforward	O
.	O
we	O
shall	O
build	O
extensively	O
on	O
the	O
analogous	O
results	O
for	O
linear	O
classiﬁcation	B
models	O
discussed	O
in	O
section	O
4.5	O
,	O
and	O
so	O
we	O
encourage	O
the	O
reader	O
to	O
familiarize	O
themselves	O
with	O
that	O
material	O
before	O
studying	O
this	O
section	O
.	O
the	O
log	O
likelihood	O
function	O
for	O
this	O
model	O
is	O
given	O
by	O
ln	O
p	O
(	O
d|w	O
)	O
=	O
=	O
1n	O
{	O
tn	O
ln	O
yn	O
+	O
(	O
1	O
−	O
tn	O
)	O
ln	O
(	O
1	O
−	O
yn	O
)	O
}	O
(	O
5.181	O
)	O
n	O
where	O
tn	O
∈	O
{	O
0	O
,	O
1	O
}	O
are	O
the	O
target	O
values	O
,	O
and	O
yn	O
≡	O
y	O
(	O
xn	O
,	O
w	O
)	O
.	O
note	O
that	O
there	O
is	O
no	O
hyperparameter	B
β	O
,	O
because	O
the	O
data	O
points	O
are	O
assumed	O
to	O
be	O
correctly	O
labelled	O
.	O
as	O
before	O
,	O
the	O
prior	B
is	O
taken	O
to	O
be	O
an	O
isotropic	B
gaussian	O
of	O
the	O
form	O
(	O
5.162	O
)	O
.	O
the	O
ﬁrst	O
stage	O
in	O
applying	O
the	O
laplace	O
framework	O
to	O
this	O
model	O
is	O
to	O
initialize	O
the	O
hyperparameter	B
α	O
,	O
and	O
then	O
to	O
determine	O
the	O
parameter	O
vector	O
w	O
by	O
maximizing	O
the	O
log	O
posterior	O
distribution	O
.	O
this	O
is	O
equivalent	O
to	O
minimizing	O
the	O
regularized	O
error	O
function	O
e	O
(	O
w	O
)	O
=	O
−	O
ln	O
p	O
(	O
d|w	O
)	O
+	O
α	O
2	O
wtw	O
(	O
5.182	O
)	O
and	O
can	O
be	O
achieved	O
using	O
error	B
backpropagation	I
combined	O
with	O
standard	O
optimiza-	O
tion	O
algorithms	O
,	O
as	O
discussed	O
in	O
section	O
5.3.	O
having	O
found	O
a	O
solution	O
wmap	O
for	O
the	O
weight	B
vector	I
,	O
the	O
next	O
step	O
is	O
to	O
eval-	O
uate	O
the	O
hessian	O
matrix	O
h	O
comprising	O
the	O
second	O
derivatives	O
of	O
the	O
negative	O
log	O
likelihood	O
function	O
.	O
this	O
can	O
be	O
done	O
,	O
for	O
instance	O
,	O
using	O
the	O
exact	O
method	O
of	O
sec-	O
tion	O
5.4.5	O
,	O
or	O
using	O
the	O
outer	B
product	I
approximation	I
given	O
by	O
(	O
5.85	O
)	O
.	O
the	O
second	O
derivatives	O
of	O
the	O
negative	O
log	O
posterior	O
can	O
again	O
be	O
written	O
in	O
the	O
form	O
(	O
5.166	O
)	O
,	O
and	O
the	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
is	O
then	O
given	O
by	O
(	O
5.167	O
)	O
.	O
to	O
optimize	O
the	O
hyperparameter	B
α	O
,	O
we	O
again	O
maximize	O
the	O
marginal	B
likelihood	I
,	O
which	O
is	O
easily	O
shown	O
to	O
take	O
the	O
form	O
ln	O
p	O
(	O
d|α	O
)	O
(	O
cid:7	O
)	O
−e	O
(	O
wmap	O
)	O
−	O
1	O
2	O
where	O
the	O
regularized	O
error	O
function	O
is	O
deﬁned	O
by	O
e	O
(	O
wmap	O
)	O
=	O
−	O
n	O
(	O
cid:2	O
)	O
n=1	O
ln|a|	O
+	O
w	O
2	O
ln	O
α	O
+	O
const	O
(	O
5.183	O
)	O
{	O
tn	O
ln	O
yn	O
+	O
(	O
1	O
−	O
tn	O
)	O
ln	O
(	O
1	O
−	O
yn	O
)	O
}	O
+	O
α	O
2	O
wt	O
mapwmap	O
(	O
5.184	O
)	O
in	O
which	O
yn	O
≡	O
y	O
(	O
xn	O
,	O
wmap	O
)	O
.	O
maximizing	O
this	O
evidence	B
function	I
with	O
respect	O
to	O
α	O
again	O
leads	O
to	O
the	O
re-estimation	O
equation	O
given	O
by	O
(	O
5.178	O
)	O
.	O
the	O
use	O
of	O
the	O
evidence	O
procedure	O
to	O
determine	O
α	O
is	O
illustrated	O
in	O
figure	O
5.22	O
for	O
the	O
synthetic	O
two-dimensional	O
data	O
discussed	O
in	O
appendix	O
a.	O
finally	O
,	O
we	O
need	O
the	O
predictive	B
distribution	I
,	O
which	O
is	O
deﬁned	O
by	O
(	O
5.168	O
)	O
.	O
again	O
,	O
this	O
integration	O
is	O
intractable	O
due	O
to	O
the	O
nonlinearity	O
of	O
the	O
network	O
function	O
.	O
the	O
5.7.	O
bayesian	O
neural	O
networks	O
283	O
figure	O
5.22	O
illustration	O
of	O
the	O
evidence	O
framework	O
applied	O
to	O
a	O
synthetic	O
two-class	O
data	O
set	O
.	O
the	O
green	O
curve	O
shows	O
the	O
optimal	O
de-	O
cision	O
boundary	O
,	O
the	O
black	O
curve	O
shows	O
the	O
result	O
of	O
ﬁtting	O
a	O
two-layer	O
network	O
with	O
8	O
hidden	O
units	O
by	O
maximum	O
likeli-	O
hood	O
,	O
and	O
the	O
red	O
curve	O
shows	O
the	O
re-	O
sult	O
of	O
including	O
a	O
regularizer	O
in	O
which	O
α	O
is	O
optimized	O
using	O
the	O
evidence	O
pro-	O
cedure	O
,	O
starting	O
from	O
the	O
initial	O
value	O
α	O
=	O
0.	O
note	O
that	O
the	O
evidence	O
proce-	O
dure	O
greatly	O
reduces	O
the	O
over-ﬁtting	B
of	O
the	O
network	O
.	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−2	O
−1	O
0	O
1	O
2	O
simplest	O
approximation	O
is	O
to	O
assume	O
that	O
the	O
posterior	O
distribution	O
is	O
very	O
narrow	O
and	O
hence	O
make	O
the	O
approximation	O
p	O
(	O
t|x	O
,	O
d	O
)	O
(	O
cid:7	O
)	O
p	O
(	O
t|x	O
,	O
wmap	O
)	O
.	O
(	O
5.185	O
)	O
we	O
can	O
improve	O
on	O
this	O
,	O
however	O
,	O
by	O
taking	O
account	O
of	O
the	O
variance	B
of	O
the	O
posterior	O
distribution	O
.	O
in	O
this	O
case	O
,	O
a	O
linear	O
approximation	O
for	O
the	O
network	O
outputs	O
,	O
as	O
was	O
used	O
in	O
the	O
case	O
of	O
regression	B
,	O
would	O
be	O
inappropriate	O
due	O
to	O
the	O
logistic	B
sigmoid	I
output-	O
unit	O
activation	B
function	I
that	O
constrains	O
the	O
output	O
to	O
lie	O
in	O
the	O
range	O
(	O
0	O
,	O
1	O
)	O
.	O
instead	O
,	O
we	O
make	O
a	O
linear	O
approximation	O
for	O
the	O
output	O
unit	O
activation	O
in	O
the	O
form	O
a	O
(	O
x	O
,	O
w	O
)	O
(	O
cid:7	O
)	O
amap	O
(	O
x	O
)	O
+	O
bt	O
(	O
w	O
−	O
wmap	O
)	O
(	O
5.186	O
)	O
where	O
amap	O
(	O
x	O
)	O
=	O
a	O
(	O
x	O
,	O
wmap	O
)	O
,	O
and	O
the	O
vector	O
b	O
≡	O
∇a	O
(	O
x	O
,	O
wmap	O
)	O
can	O
be	O
found	O
by	O
backpropagation	B
.	O
(	O
cid:6	O
)	O
(	O
cid:10	O
)	O
δ	O
because	O
we	O
now	O
have	O
a	O
gaussian	O
approximation	O
for	O
the	O
posterior	O
distribution	O
over	O
w	O
,	O
and	O
a	O
model	O
for	O
a	O
that	O
is	O
a	O
linear	O
function	O
of	O
w	O
,	O
we	O
can	O
now	O
appeal	O
to	O
the	O
results	O
of	O
section	O
4.5.2.	O
the	O
distribution	O
of	O
output	O
unit	O
activation	O
values	O
,	O
induced	O
by	O
the	O
distribution	O
over	O
network	O
weights	O
,	O
is	O
given	O
by	O
p	O
(	O
a|x	O
,	O
d	O
)	O
=	O
a	O
−	O
amap	O
(	O
x	O
)	O
−	O
bt	O
(	O
x	O
)	O
(	O
w	O
−	O
wmap	O
)	O
q	O
(	O
w|d	O
)	O
dw	O
(	O
5.187	O
)	O
where	O
q	O
(	O
w|d	O
)	O
is	O
the	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
distribution	O
given	O
by	O
(	O
5.167	O
)	O
.	O
from	O
section	O
4.5.2	O
,	O
we	O
see	O
that	O
this	O
distribution	O
is	O
gaussian	O
with	O
mean	B
amap	O
≡	O
a	O
(	O
x	O
,	O
wmap	O
)	O
,	O
and	O
variance	B
(	O
cid:11	O
)	O
a	O
(	O
x	O
)	O
=	O
bt	O
(	O
x	O
)	O
a−1b	O
(	O
x	O
)	O
.	O
σ2	O
(	O
5.188	O
)	O
finally	O
,	O
to	O
obtain	O
the	O
predictive	B
distribution	I
,	O
we	O
must	O
marginalize	O
over	O
a	O
using	O
p	O
(	O
t	O
=	O
1|x	O
,	O
d	O
)	O
=	O
σ	O
(	O
a	O
)	O
p	O
(	O
a|x	O
,	O
d	O
)	O
da	O
.	O
(	O
5.189	O
)	O
(	O
cid:6	O
)	O
284	O
5.	O
neural	O
networks	O
3	O
2	O
1	O
0	O
−1	O
−2	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−2	O
−1	O
0	O
1	O
2	O
−2	O
−1	O
0	O
1	O
2	O
figure	O
5.23	O
an	O
illustration	O
of	O
the	O
laplace	O
approximation	O
for	O
a	O
bayesian	O
neural	B
network	I
having	O
8	O
hidden	O
units	O
with	O
‘	O
tanh	O
’	O
activation	O
functions	O
and	O
a	O
single	O
logistic-sigmoid	O
output	O
unit	O
.	O
the	O
weight	O
parameters	O
were	O
found	O
using	O
scaled	O
conjugate	B
gradients	O
,	O
and	O
the	O
hyperparameter	B
α	O
was	O
optimized	O
using	O
the	O
evidence	O
framework	O
.	O
on	O
the	O
left	O
is	O
the	O
result	O
of	O
using	O
the	O
simple	O
approximation	O
(	O
5.185	O
)	O
based	O
on	O
a	O
point	O
estimate	O
wmap	O
of	O
the	O
parameters	O
,	O
in	O
which	O
the	O
green	O
curve	O
shows	O
the	O
y	O
=	O
0.5	O
decision	B
boundary	I
,	O
and	O
the	O
other	O
contours	O
correspond	O
to	O
output	O
probabilities	O
of	O
y	O
=	O
0.1	O
,	O
0.3	O
,	O
0.7	O
,	O
and	O
0.9.	O
on	O
the	O
right	O
is	O
the	O
corresponding	O
result	O
obtained	O
using	O
(	O
5.190	O
)	O
.	O
note	O
that	O
the	O
effect	O
of	O
marginalization	O
is	O
to	O
spread	O
out	O
the	O
contours	O
and	O
to	O
make	O
the	O
predictions	O
less	O
conﬁdent	O
,	O
so	O
that	O
at	O
each	O
input	O
point	O
x	O
,	O
the	O
posterior	O
probabilities	O
are	O
shifted	O
towards	O
0.5	O
,	O
while	O
the	O
y	O
=	O
0.5	O
contour	O
itself	O
is	O
unaffected	O
.	O
the	O
convolution	O
of	O
a	O
gaussian	O
with	O
a	O
logistic	B
sigmoid	I
is	O
intractable	O
.	O
we	O
therefore	O
apply	O
the	O
approximation	O
(	O
4.153	O
)	O
to	O
(	O
5.189	O
)	O
giving	O
p	O
(	O
t	O
=	O
1|x	O
,	O
d	O
)	O
=	O
σ	O
κ	O
(	O
σ2	O
a	O
)	O
btwmap	O
(	O
5.190	O
)	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
where	O
κ	O
(	O
·	O
)	O
is	O
deﬁned	O
by	O
(	O
4.154	O
)	O
.	O
recall	O
that	O
both	O
σ2	O
a	O
and	O
b	O
are	O
functions	O
of	O
x.	O
figure	O
5.23	O
shows	O
an	O
example	O
of	O
this	O
framework	O
applied	O
to	O
the	O
synthetic	O
classi-	O
ﬁcation	O
data	O
set	O
described	O
in	O
appendix	O
a.	O
exercises	O
5.1	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
two-layer	O
network	O
function	O
of	O
the	O
form	O
(	O
5.7	O
)	O
in	O
which	O
the	O
hidden-	O
unit	O
nonlinear	O
activation	O
functions	O
g	O
(	O
·	O
)	O
are	O
given	O
by	O
logistic	B
sigmoid	I
functions	O
of	O
the	O
form	O
σ	O
(	O
a	O
)	O
=	O
{	O
1	O
+	O
exp	O
(	O
−a	O
)	O
}	O
−1	O
.	O
(	O
5.191	O
)	O
show	O
that	O
there	O
exists	O
an	O
equivalent	O
network	O
,	O
which	O
computes	O
exactly	O
the	O
same	O
func-	O
tion	O
,	O
but	O
with	O
hidden	B
unit	I
activation	O
functions	O
given	O
by	O
tanh	O
(	O
a	O
)	O
where	O
the	O
tanh	O
func-	O
tion	O
is	O
deﬁned	O
by	O
(	O
5.59	O
)	O
.	O
hint	O
:	O
ﬁrst	O
ﬁnd	O
the	O
relation	O
between	O
σ	O
(	O
a	O
)	O
and	O
tanh	O
(	O
a	O
)	O
,	O
and	O
then	O
show	O
that	O
the	O
parameters	O
of	O
the	O
two	O
networks	O
differ	O
by	O
linear	O
transformations	O
.	O
5.2	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
maximizing	O
the	O
likelihood	B
function	I
under	O
the	O
conditional	B
distribution	O
(	O
5.16	O
)	O
for	O
a	O
multioutput	O
neural	B
network	I
is	O
equivalent	O
to	O
minimizing	O
the	O
sum-of-squares	B
error	I
function	O
(	O
5.11	O
)	O
.	O
exercises	O
285	O
5.3	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
regression	B
problem	O
involving	O
multiple	O
target	O
variables	O
in	O
which	O
it	O
is	O
assumed	O
that	O
the	O
distribution	O
of	O
the	O
targets	O
,	O
conditioned	O
on	O
the	O
input	O
vector	O
x	O
,	O
is	O
a	O
gaussian	O
of	O
the	O
form	O
p	O
(	O
t|x	O
,	O
w	O
)	O
=	O
n	O
(	O
t|y	O
(	O
x	O
,	O
w	O
)	O
,	O
σ	O
)	O
(	O
5.192	O
)	O
where	O
y	O
(	O
x	O
,	O
w	O
)	O
is	O
the	O
output	O
of	O
a	O
neural	B
network	I
with	O
input	O
vector	O
x	O
and	O
weight	B
vector	I
w	O
,	O
and	O
σ	O
is	O
the	O
covariance	B
of	O
the	O
assumed	O
gaussian	O
noise	O
on	O
the	O
targets	O
.	O
given	O
a	O
set	O
of	O
independent	B
observations	O
of	O
x	O
and	O
t	O
,	O
write	O
down	O
the	O
error	B
function	I
that	O
must	O
be	O
minimized	O
in	O
order	O
to	O
ﬁnd	O
the	O
maximum	B
likelihood	I
solution	O
for	O
w	O
,	O
if	O
we	O
assume	O
that	O
σ	O
is	O
ﬁxed	O
and	O
known	O
.	O
now	O
assume	O
that	O
σ	O
is	O
also	O
to	O
be	O
determined	O
from	O
the	O
data	O
,	O
and	O
write	O
down	O
an	O
expression	O
for	O
the	O
maximum	B
likelihood	I
solution	O
for	O
σ.	O
note	O
that	O
the	O
optimizations	O
of	O
w	O
and	O
σ	O
are	O
now	O
coupled	O
,	O
in	O
contrast	O
to	O
the	O
case	O
of	O
independent	B
target	O
variables	O
discussed	O
in	O
section	O
5.2	O
.	O
5.4	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
binary	O
classiﬁcation	O
problem	O
in	O
which	O
the	O
target	O
values	O
are	O
t	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
with	O
a	O
network	O
output	O
y	O
(	O
x	O
,	O
w	O
)	O
that	O
represents	O
p	O
(	O
t	O
=	O
1|x	O
)	O
,	O
and	O
suppose	O
that	O
there	O
is	O
a	O
probability	B
	O
that	O
the	O
class	O
label	O
on	O
a	O
training	B
data	O
point	O
has	O
been	O
incorrectly	O
set	O
.	O
assuming	O
independent	B
and	O
identically	O
distributed	O
data	O
,	O
write	O
down	O
the	O
error	B
function	I
corresponding	O
to	O
the	O
negative	O
log	O
likelihood	O
.	O
verify	O
that	O
the	O
error	B
function	I
(	O
5.21	O
)	O
is	O
obtained	O
when	O
	O
=	O
0.	O
note	O
that	O
this	O
error	B
function	I
makes	O
the	O
model	O
robust	O
to	O
incorrectly	O
labelled	O
data	O
,	O
in	O
contrast	O
to	O
the	O
usual	O
error	B
function	I
.	O
5.5	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
maximizing	O
likelihood	O
for	O
a	O
multiclass	B
neural	O
network	O
model	O
in	O
which	O
the	O
network	O
outputs	O
have	O
the	O
interpretation	O
yk	O
(	O
x	O
,	O
w	O
)	O
=	O
p	O
(	O
tk	O
=	O
1|x	O
)	O
is	O
equivalent	O
to	O
the	O
minimization	O
of	O
the	O
cross-entropy	B
error	I
function	I
(	O
5.24	O
)	O
.	O
5.6	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
the	O
derivative	B
of	O
the	O
error	B
function	I
(	O
5.21	O
)	O
with	O
respect	O
to	O
the	O
activation	O
ak	O
for	O
an	O
output	O
unit	O
having	O
a	O
logistic	B
sigmoid	I
activation	O
function	O
satisﬁes	O
(	O
5.18	O
)	O
.	O
5.7	O
(	O
(	O
cid:12	O
)	O
)	O
show	O
the	O
derivative	B
of	O
the	O
error	B
function	I
(	O
5.24	O
)	O
with	O
respect	O
to	O
the	O
activation	O
ak	O
for	O
output	O
units	O
having	O
a	O
softmax	O
activation	O
function	O
satisﬁes	O
(	O
5.18	O
)	O
.	O
5.8	O
(	O
(	O
cid:12	O
)	O
)	O
we	O
saw	O
in	O
(	O
4.88	O
)	O
that	O
the	O
derivative	B
of	O
the	O
logistic	B
sigmoid	I
activation	O
function	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
function	O
value	O
itself	O
.	O
derive	O
the	O
corresponding	O
result	O
for	O
the	O
‘	O
tanh	O
’	O
activation	B
function	I
deﬁned	O
by	O
(	O
5.59	O
)	O
.	O
5.9	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
the	O
error	B
function	I
(	O
5.21	O
)	O
for	O
binary	O
classiﬁcation	B
problems	O
was	O
de-	O
rived	O
for	O
a	O
network	O
having	O
a	O
logistic-sigmoid	O
output	O
activation	B
function	I
,	O
so	O
that	O
0	O
(	O
cid:1	O
)	O
y	O
(	O
x	O
,	O
w	O
)	O
(	O
cid:1	O
)	O
1	O
,	O
and	O
data	O
having	O
target	O
values	O
t	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
derive	O
the	O
correspond-	O
ing	O
error	O
function	O
if	O
we	O
consider	O
a	O
network	O
having	O
an	O
output	O
−1	O
(	O
cid:1	O
)	O
y	O
(	O
x	O
,	O
w	O
)	O
(	O
cid:1	O
)	O
1	O
and	O
target	O
values	O
t	O
=	O
1	O
for	O
class	O
c1	O
and	O
t	O
=	O
−1	O
for	O
class	O
c2	O
.	O
what	O
would	O
be	O
the	O
appropriate	O
choice	O
of	O
output	O
unit	O
activation	B
function	I
?	O
5.10	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
hessian	O
matrix	O
h	O
with	O
eigenvector	O
equation	O
(	O
5.33	O
)	O
.	O
by	O
setting	O
the	O
vector	O
v	O
in	O
(	O
5.39	O
)	O
equal	O
to	O
each	O
of	O
the	O
eigenvectors	O
ui	O
in	O
turn	O
,	O
show	O
that	O
h	O
is	O
positive	B
deﬁnite	I
if	O
,	O
and	O
only	O
if	O
,	O
all	O
of	O
its	O
eigenvalues	O
are	O
positive	O
.	O
286	O
5.	O
neural	O
networks	O
5.11	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
quadratic	O
error	O
function	O
deﬁned	O
by	O
(	O
5.32	O
)	O
,	O
in	O
which	O
the	O
hessian	O
matrix	O
h	O
has	O
an	O
eigenvalue	O
equation	O
given	O
by	O
(	O
5.33	O
)	O
.	O
show	O
that	O
the	O
con-	O
tours	O
of	O
constant	O
error	B
are	O
ellipses	O
whose	O
axes	O
are	O
aligned	O
with	O
the	O
eigenvectors	O
ui	O
,	O
with	O
lengths	O
that	O
are	O
inversely	O
proportional	O
to	O
the	O
square	O
root	O
of	O
the	O
corresponding	O
eigenvalues	O
λi	O
.	O
5.12	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
by	O
considering	O
the	O
local	B
taylor	O
expansion	O
(	O
5.32	O
)	O
of	O
an	O
error	B
function	I
about	O
a	O
stationary	B
point	O
w	O
(	O
cid:1	O
)	O
,	O
show	O
that	O
the	O
necessary	O
and	O
sufﬁcient	O
condition	O
for	O
the	O
stationary	B
point	O
to	O
be	O
a	O
local	B
minimum	I
of	O
the	O
error	B
function	I
is	O
that	O
the	O
hessian	O
matrix	O
h	O
,	O
deﬁned	O
by	O
(	O
5.30	O
)	O
with	O
(	O
cid:1	O
)	O
w	O
=	O
w	O
(	O
cid:1	O
)	O
,	O
be	O
positive	B
deﬁnite	I
.	O
5.13	O
(	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
as	O
a	O
consequence	O
of	O
the	O
symmetry	O
of	O
the	O
hessian	O
matrix	O
h	O
,	O
the	O
number	O
of	O
independent	B
elements	O
in	O
the	O
quadratic	O
error	O
function	O
(	O
5.28	O
)	O
is	O
given	O
by	O
w	O
(	O
w	O
+	O
3	O
)	O
/2	O
.	O
5.14	O
(	O
(	O
cid:12	O
)	O
)	O
by	O
making	O
a	O
taylor	O
expansion	O
,	O
verify	O
that	O
the	O
terms	O
that	O
are	O
o	O
(	O
	O
)	O
cancel	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
5.69	O
)	O
.	O
5.15	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
in	O
section	O
5.3.4	O
,	O
we	O
derived	O
a	O
procedure	O
for	O
evaluating	O
the	O
jacobian	O
matrix	O
of	O
a	O
neural	B
network	I
using	O
a	O
backpropagation	B
procedure	O
.	O
derive	O
an	O
alternative	O
formalism	O
for	O
ﬁnding	O
the	O
jacobian	O
based	O
on	O
forward	B
propagation	I
equations	O
.	O
5.16	O
(	O
(	O
cid:12	O
)	O
)	O
the	O
outer	B
product	I
approximation	I
to	O
the	O
hessian	O
matrix	O
for	O
a	O
neural	B
network	I
using	O
a	O
sum-of-squares	B
error	I
function	O
is	O
given	O
by	O
(	O
5.84	O
)	O
.	O
extend	O
this	O
result	O
to	O
the	O
case	O
of	O
multiple	O
outputs	O
.	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
1	O
2	O
5.17	O
(	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
squared	O
loss	B
function	I
of	O
the	O
form	O
{	O
y	O
(	O
x	O
,	O
w	O
)	O
−	O
t	O
}	O
2	O
e	O
=	O
p	O
(	O
x	O
,	O
t	O
)	O
dx	O
dt	O
(	O
5.193	O
)	O
where	O
y	O
(	O
x	O
,	O
w	O
)	O
is	O
a	O
parametric	O
function	O
such	O
as	O
a	O
neural	B
network	I
.	O
the	O
result	O
(	O
1.89	O
)	O
shows	O
that	O
the	O
function	O
y	O
(	O
x	O
,	O
w	O
)	O
that	O
minimizes	O
this	O
error	B
is	O
given	O
by	O
the	O
conditional	B
expectation	I
of	O
t	O
given	O
x.	O
use	O
this	O
result	O
to	O
show	O
that	O
the	O
second	O
derivative	O
of	O
e	O
with	O
respect	O
to	O
two	O
elements	O
wr	O
and	O
ws	O
of	O
the	O
vector	O
w	O
,	O
is	O
given	O
by	O
(	O
cid:6	O
)	O
∂2e	O
∂wr∂ws	O
=	O
∂y	O
∂wr	O
∂y	O
∂ws	O
p	O
(	O
x	O
)	O
dx	O
.	O
(	O
5.194	O
)	O
note	O
that	O
,	O
for	O
a	O
ﬁnite	O
sample	O
from	O
p	O
(	O
x	O
)	O
,	O
we	O
obtain	O
(	O
5.84	O
)	O
.	O
5.18	O
(	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
two-layer	O
network	O
of	O
the	O
form	O
shown	O
in	O
figure	O
5.1	O
with	O
the	O
addition	O
of	O
extra	O
parameters	O
corresponding	O
to	O
skip-layer	O
connections	O
that	O
go	O
directly	O
from	O
the	O
inputs	O
to	O
the	O
outputs	O
.	O
by	O
extending	O
the	O
discussion	O
of	O
section	O
5.3.2	O
,	O
write	O
down	O
the	O
equations	O
for	O
the	O
derivatives	O
of	O
the	O
error	B
function	I
with	O
respect	O
to	O
these	O
additional	O
parameters	O
.	O
5.19	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
derive	O
the	O
expression	O
(	O
5.85	O
)	O
for	O
the	O
outer	B
product	I
approximation	I
to	O
the	O
hessian	O
matrix	O
for	O
a	O
network	O
having	O
a	O
single	O
output	O
with	O
a	O
logistic	B
sigmoid	I
output-unit	O
activation	B
function	I
and	O
a	O
cross-entropy	B
error	I
function	I
,	O
corresponding	O
to	O
the	O
result	O
(	O
5.84	O
)	O
for	O
the	O
sum-of-squares	B
error	I
function	O
.	O
exercises	O
287	O
5.20	O
(	O
(	O
cid:12	O
)	O
)	O
derive	O
an	O
expression	O
for	O
the	O
outer	B
product	I
approximation	I
to	O
the	O
hessian	O
matrix	O
for	O
a	O
network	O
having	O
k	O
outputs	O
with	O
a	O
softmax	O
output-unit	O
activation	B
function	I
and	O
a	O
cross-entropy	B
error	I
function	I
,	O
corresponding	O
to	O
the	O
result	O
(	O
5.84	O
)	O
for	O
the	O
sum-of-	O
squares	O
error	B
function	I
.	O
5.21	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
extend	O
the	O
expression	O
(	O
5.86	O
)	O
for	O
the	O
outer	B
product	I
approximation	I
of	O
the	O
hes-	O
sian	O
matrix	O
to	O
the	O
case	O
of	O
k	O
>	O
1	O
output	O
units	O
.	O
hence	O
,	O
derive	O
a	O
recursive	O
expression	O
analogous	O
to	O
(	O
5.87	O
)	O
for	O
incrementing	O
the	O
number	O
n	O
of	O
patterns	O
and	O
a	O
similar	O
expres-	O
sion	B
for	O
incrementing	O
the	O
number	O
k	O
of	O
outputs	O
.	O
use	O
these	O
results	O
,	O
together	O
with	O
the	O
identity	O
(	O
5.88	O
)	O
,	O
to	O
ﬁnd	O
sequential	O
update	O
expressions	O
analogous	O
to	O
(	O
5.89	O
)	O
for	O
ﬁnding	O
the	O
inverse	B
of	O
the	O
hessian	O
by	O
incrementally	O
including	O
both	O
extra	O
patterns	O
and	O
extra	O
outputs	O
.	O
5.22	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
derive	O
the	O
results	O
(	O
5.93	O
)	O
,	O
(	O
5.94	O
)	O
,	O
and	O
(	O
5.95	O
)	O
for	O
the	O
elements	O
of	O
the	O
hessian	O
matrix	O
of	O
a	O
two-layer	O
feed-forward	O
network	O
by	O
application	O
of	O
the	O
chain	O
rule	O
of	O
cal-	O
culus	O
.	O
5.23	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
extend	O
the	O
results	O
of	O
section	O
5.4.5	O
for	O
the	O
exact	O
hessian	O
of	O
a	O
two-layer	O
network	O
to	O
include	O
skip-layer	O
connections	O
that	O
go	O
directly	O
from	O
inputs	O
to	O
outputs	O
.	O
5.24	O
(	O
(	O
cid:12	O
)	O
)	O
verify	O
that	O
the	O
network	O
function	O
deﬁned	O
by	O
(	O
5.113	O
)	O
and	O
(	O
5.114	O
)	O
is	O
invariant	O
un-	O
der	O
the	O
transformation	O
(	O
5.115	O
)	O
applied	O
to	O
the	O
inputs	O
,	O
provided	O
the	O
weights	O
and	O
biases	O
are	O
simultaneously	O
transformed	O
using	O
(	O
5.116	O
)	O
and	O
(	O
5.117	O
)	O
.	O
similarly	O
,	O
show	O
that	O
the	O
network	O
outputs	O
can	O
be	O
transformed	O
according	O
(	O
5.118	O
)	O
by	O
applying	O
the	O
transforma-	O
tion	O
(	O
5.119	O
)	O
and	O
(	O
5.120	O
)	O
to	O
the	O
second-layer	O
weights	O
and	O
biases	O
.	O
5.25	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
quadratic	O
error	O
function	O
of	O
the	O
form	O
(	O
w	O
−	O
w	O
(	O
cid:1	O
)	O
)	O
th	O
(	O
w	O
−	O
w	O
(	O
cid:1	O
)	O
)	O
e	O
=	O
e0	O
+	O
1	O
2	O
(	O
5.195	O
)	O
where	O
w	O
(	O
cid:1	O
)	O
represents	O
the	O
minimum	O
,	O
and	O
the	O
hessian	O
matrix	O
h	O
is	O
positive	B
deﬁnite	I
and	O
constant	O
.	O
suppose	O
the	O
initial	O
weight	B
vector	I
w	O
(	O
0	O
)	O
is	O
chosen	O
to	O
be	O
at	O
the	O
origin	O
and	O
is	O
updated	O
using	O
simple	O
gradient	B
descent	I
w	O
(	O
τ	O
)	O
=	O
w	O
(	O
τ−1	O
)	O
−	O
ρ∇e	O
(	O
5.196	O
)	O
where	O
τ	O
denotes	O
the	O
step	O
number	O
,	O
and	O
ρ	O
is	O
the	O
learning	O
rate	O
(	O
which	O
is	O
assumed	O
to	O
be	O
small	O
)	O
.	O
show	O
that	O
,	O
after	O
τ	O
steps	O
,	O
the	O
components	O
of	O
the	O
weight	B
vector	I
parallel	O
to	O
the	O
eigenvectors	O
of	O
h	O
can	O
be	O
written	O
(	O
τ	O
)	O
j	O
=	O
{	O
1	O
−	O
(	O
1	O
−	O
ρηj	O
)	O
τ	O
}	O
w	O
(	O
cid:1	O
)	O
w	O
j	O
(	O
5.197	O
)	O
where	O
wj	O
=	O
wtuj	O
,	O
and	O
uj	O
and	O
ηj	O
are	O
the	O
eigenvectors	O
and	O
eigenvalues	O
,	O
respectively	O
,	O
of	O
h	O
so	O
that	O
(	O
5.198	O
)	O
show	O
that	O
as	O
τ	O
→	O
∞	O
,	O
this	O
gives	O
w	O
(	O
τ	O
)	O
→	O
w	O
(	O
cid:1	O
)	O
as	O
expected	O
,	O
provided	O
|1	O
−	O
ρηj|	O
<	O
1.	O
now	O
suppose	O
that	O
training	B
is	O
halted	O
after	O
a	O
ﬁnite	O
number	O
τ	O
of	O
steps	O
.	O
show	O
that	O
the	O
huj	O
=	O
ηjuj	O
.	O
288	O
5.	O
neural	O
networks	O
components	O
of	O
the	O
weight	B
vector	I
parallel	O
to	O
the	O
eigenvectors	O
of	O
the	O
hessian	O
satisfy	O
(	O
τ	O
)	O
j	O
(	O
cid:7	O
)	O
w	O
(	O
cid:1	O
)	O
|	O
(	O
cid:13	O
)	O
|w	O
(	O
cid:1	O
)	O
j	O
when	O
ηj	O
(	O
cid:12	O
)	O
(	O
ρτ	O
)	O
−1	O
j|	O
when	O
ηj	O
(	O
cid:13	O
)	O
(	O
ρτ	O
)	O
−1	O
.	O
w	O
(	O
τ	O
)	O
j	O
|w	O
(	O
5.199	O
)	O
(	O
5.200	O
)	O
compare	O
this	O
result	O
with	O
the	O
discussion	O
in	O
section	O
3.5.3	O
of	O
regularization	B
with	O
simple	O
weight	B
decay	I
,	O
and	O
hence	O
show	O
that	O
(	O
ρτ	O
)	O
−1	O
is	O
analogous	O
to	O
the	O
regularization	B
param-	O
eter	O
λ.	O
the	O
above	O
results	O
also	O
show	O
that	O
the	O
effective	B
number	I
of	I
parameters	I
in	O
the	O
network	O
,	O
as	O
deﬁned	O
by	O
(	O
3.91	O
)	O
,	O
grows	O
as	O
the	O
training	B
progresses	O
.	O
5.26	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
multilayer	B
perceptron	I
with	O
arbitrary	O
feed-forward	O
topology	O
,	O
which	O
is	O
to	O
be	O
trained	O
by	O
minimizing	O
the	O
tangent	B
propagation	I
error	O
function	O
(	O
5.127	O
)	O
in	O
which	O
the	O
regularizing	O
function	O
is	O
given	O
by	O
(	O
5.128	O
)	O
.	O
show	O
that	O
the	O
regularization	B
term	O
ω	O
can	O
be	O
written	O
as	O
a	O
sum	O
over	O
patterns	O
of	O
terms	O
of	O
the	O
form	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
where	O
g	O
is	O
a	O
differential	B
operator	O
deﬁned	O
by	O
ωn	O
=	O
1	O
2	O
k	O
(	O
gyk	O
)	O
2	O
g	O
≡	O
∂	O
∂xi	O
τi	O
i	O
by	O
acting	O
on	O
the	O
forward	B
propagation	I
equations	O
.	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
zj	O
=	O
h	O
(	O
aj	O
)	O
,	O
(	O
5.203	O
)	O
with	O
the	O
operator	O
g	O
,	O
show	O
that	O
ωn	O
can	O
be	O
evaluated	O
by	O
forward	B
propagation	I
using	O
the	O
following	O
equations	O
:	O
wjizi	O
aj	O
=	O
i	O
αj	O
=	O
h	O
(	O
cid:4	O
)	O
(	O
aj	O
)	O
βj	O
,	O
βj	O
=	O
wjiαi	O
.	O
i	O
(	O
5.204	O
)	O
where	O
we	O
have	O
deﬁned	O
the	O
new	O
variables	O
αj	O
≡	O
gzj	O
,	O
βj	O
≡	O
gaj	O
.	O
(	O
5.205	O
)	O
now	O
show	O
that	O
the	O
derivatives	O
of	O
ωn	O
with	O
respect	O
to	O
a	O
weight	O
wrs	O
in	O
the	O
network	O
can	O
be	O
written	O
in	O
the	O
form	O
(	O
5.201	O
)	O
(	O
5.202	O
)	O
(	O
5.206	O
)	O
(	O
5.207	O
)	O
(	O
cid:2	O
)	O
k	O
∂ωn	O
∂wrs	O
=	O
where	O
we	O
have	O
deﬁned	O
αk	O
{	O
φkrzs	O
+	O
δkrαs	O
}	O
δkr	O
≡	O
∂yk	O
∂ar	O
,	O
φkr	O
≡	O
gδkr	O
.	O
write	O
down	O
the	O
backpropagation	B
equations	O
for	O
δkr	O
,	O
and	O
hence	O
derive	O
a	O
set	O
of	O
back-	O
propagation	O
equations	O
for	O
the	O
evaluation	O
of	O
the	O
φkr	O
.	O
exercises	O
289	O
5.27	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
the	O
framework	O
for	O
training	O
with	O
transformed	O
data	O
in	O
the	O
special	O
case	O
in	O
which	O
the	O
transformation	O
consists	O
simply	O
of	O
the	O
addition	O
of	O
random	O
noise	O
x	O
→	O
x	O
+	O
ξ	O
where	O
ξ	O
has	O
a	O
gaussian	O
distribution	O
with	O
zero	O
mean	B
and	O
unit	O
covariance	B
.	O
by	O
following	O
an	O
argument	O
analogous	O
to	O
that	O
of	O
section	O
5.5.5	O
,	O
show	O
that	O
the	O
resulting	O
regularizer	O
reduces	O
to	O
the	O
tikhonov	O
form	O
(	O
5.135	O
)	O
.	O
5.28	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
neural	B
network	I
,	O
such	O
as	O
the	O
convolutional	B
network	O
discussed	O
in	O
section	O
5.5.6	O
,	O
in	O
which	O
multiple	O
weights	O
are	O
constrained	O
to	O
have	O
the	O
same	O
value	O
.	O
discuss	O
how	O
the	O
standard	O
backpropagation	O
algorithm	O
must	O
be	O
modiﬁed	O
in	O
order	O
to	O
ensure	O
that	O
such	O
constraints	O
are	O
satisﬁed	O
when	O
evaluating	O
the	O
derivatives	O
of	O
an	O
error	B
function	I
with	O
respect	O
to	O
the	O
adjustable	O
parameters	O
in	O
the	O
network	O
.	O
5.29	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
verify	O
the	O
result	O
(	O
5.141	O
)	O
.	O
5.30	O
(	O
(	O
cid:12	O
)	O
)	O
verify	O
the	O
result	O
(	O
5.142	O
)	O
.	O
5.31	O
(	O
(	O
cid:12	O
)	O
)	O
verify	O
the	O
result	O
(	O
5.143	O
)	O
.	O
5.32	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
the	O
derivatives	O
of	O
the	O
mixing	O
coefﬁcients	O
{	O
πk	O
}	O
,	O
deﬁned	O
by	O
(	O
5.146	O
)	O
,	O
with	O
respect	O
to	O
the	O
auxiliary	O
parameters	O
{	O
ηj	O
}	O
are	O
given	O
by	O
=	O
δjkπj	O
−	O
πjπk	O
.	O
∂πk	O
∂ηj	O
(	O
cid:5	O
)	O
(	O
5.208	O
)	O
hence	O
,	O
by	O
making	O
use	O
of	O
the	O
constraint	O
k	O
πk	O
=	O
1	O
,	O
derive	O
the	O
result	O
(	O
5.147	O
)	O
.	O
5.33	O
(	O
(	O
cid:12	O
)	O
)	O
write	O
down	O
a	O
pair	O
of	O
equations	O
that	O
express	O
the	O
cartesian	O
coordinates	O
(	O
x1	O
,	O
x2	O
)	O
for	O
the	O
robot	B
arm	I
shown	O
in	O
figure	O
5.18	O
in	O
terms	O
of	O
the	O
joint	O
angles	O
θ1	O
and	O
θ2	O
and	O
the	O
lengths	O
l1	O
and	O
l2	O
of	O
the	O
links	O
.	O
assume	O
the	O
origin	O
of	O
the	O
coordinate	O
system	O
is	O
given	O
by	O
the	O
attachment	O
point	O
of	O
the	O
lower	O
arm	O
.	O
these	O
equations	O
deﬁne	O
the	O
‘	O
forward	B
kinematics	I
’	O
of	O
the	O
robot	B
arm	I
.	O
5.34	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
derive	O
the	O
result	O
(	O
5.155	O
)	O
for	O
the	O
derivative	B
of	O
the	O
error	B
function	I
with	O
respect	O
to	O
the	O
network	O
output	O
activations	O
controlling	O
the	O
mixing	O
coefﬁcients	O
in	O
the	O
mixture	B
density	I
network	I
.	O
5.35	O
(	O
(	O
cid:12	O
)	O
)	O
derive	O
the	O
result	O
(	O
5.156	O
)	O
for	O
the	O
derivative	B
of	O
the	O
error	B
function	I
with	O
respect	O
to	O
the	O
network	O
output	O
activations	O
controlling	O
the	O
component	O
means	O
in	O
the	O
mixture	B
density	I
network	I
.	O
5.36	O
(	O
(	O
cid:12	O
)	O
)	O
derive	O
the	O
result	O
(	O
5.157	O
)	O
for	O
the	O
derivative	B
of	O
the	O
error	B
function	I
with	O
respect	O
to	O
the	O
network	O
output	O
activations	O
controlling	O
the	O
component	O
variances	O
in	O
the	O
mixture	B
density	I
network	I
.	O
5.37	O
(	O
(	O
cid:12	O
)	O
)	O
verify	O
the	O
results	O
(	O
5.158	O
)	O
and	O
(	O
5.160	O
)	O
for	O
the	O
conditional	B
mean	O
and	O
variance	B
of	O
the	O
mixture	B
density	I
network	I
model	O
.	O
5.38	O
(	O
(	O
cid:12	O
)	O
)	O
using	O
the	O
general	O
result	O
(	O
2.115	O
)	O
,	O
derive	O
the	O
predictive	B
distribution	I
(	O
5.172	O
)	O
for	O
the	O
laplace	O
approximation	O
to	O
the	O
bayesian	O
neural	B
network	I
model	O
.	O
290	O
5.	O
neural	O
networks	O
5.39	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
make	O
use	O
of	O
the	O
laplace	O
approximation	O
result	O
(	O
4.135	O
)	O
to	O
show	O
that	O
the	O
evidence	B
function	I
for	O
the	O
hyperparameters	O
α	O
and	O
β	O
in	O
the	O
bayesian	O
neural	B
network	I
model	O
can	O
be	O
approximated	O
by	O
(	O
5.175	O
)	O
.	O
5.40	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
outline	O
the	O
modiﬁcations	O
needed	O
to	O
the	O
framework	O
for	O
bayesian	O
neural	O
networks	O
,	O
discussed	O
in	O
section	O
5.7.3	O
,	O
to	O
handle	O
multiclass	B
problems	O
using	O
networks	O
having	O
softmax	O
output-unit	O
activation	O
functions	O
.	O
5.41	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
by	O
following	O
analogous	O
steps	O
to	O
those	O
given	O
in	O
section	O
5.7.1	O
for	B
regression	I
networks	O
,	O
derive	O
the	O
result	O
(	O
5.183	O
)	O
for	O
the	O
marginal	B
likelihood	I
in	O
the	O
case	O
of	O
a	O
net-	O
work	O
having	O
a	O
cross-entropy	B
error	I
function	I
and	O
logistic-sigmoid	O
output-unit	O
activa-	O
tion	O
function	O
.	O
6	O
kernel	O
methods	O
in	O
chapters	O
3	O
and	O
4	O
,	O
we	O
considered	O
linear	O
parametric	O
models	O
for	B
regression	I
and	O
classiﬁcation	B
in	O
which	O
the	O
form	O
of	O
the	O
mapping	O
y	O
(	O
x	O
,	O
w	O
)	O
from	O
input	O
x	O
to	O
output	O
y	O
is	O
governed	O
by	O
a	O
vector	O
w	O
of	O
adaptive	O
parameters	O
.	O
during	O
the	O
learning	B
phase	O
,	O
a	O
set	O
of	O
training	B
data	O
is	O
used	O
either	O
to	O
obtain	O
a	O
point	O
estimate	O
of	O
the	O
parameter	O
vector	O
or	O
to	O
determine	O
a	O
posterior	O
distribution	O
over	O
this	O
vector	O
.	O
the	O
training	B
data	O
is	O
then	O
discarded	O
,	O
and	O
predictions	O
for	O
new	O
inputs	O
are	O
based	O
purely	O
on	O
the	O
learned	O
parameter	O
vector	O
w.	O
this	O
approach	O
is	O
also	O
used	O
in	O
nonlinear	O
parametric	O
models	O
such	O
as	O
neural	O
networks	O
.	O
however	O
,	O
there	O
is	O
a	O
class	O
of	O
pattern	O
recognition	O
techniques	O
,	O
in	O
which	O
the	O
training	B
data	O
points	O
,	O
or	O
a	O
subset	O
of	O
them	O
,	O
are	O
kept	O
and	O
used	O
also	O
during	O
the	O
prediction	O
phase	O
.	O
for	O
instance	O
,	O
the	O
parzen	O
probability	B
density	O
model	O
comprised	O
a	O
linear	O
combination	O
of	O
‘	O
kernel	O
’	O
functions	O
each	O
one	O
centred	O
on	O
one	O
of	O
the	O
training	B
data	O
points	O
.	O
similarly	O
,	O
in	O
section	O
2.5.2	O
we	O
introduced	O
a	O
simple	O
technique	O
for	O
classiﬁcation	O
called	O
nearest	O
neighbours	O
,	O
which	O
involved	O
assigning	O
to	O
each	O
new	O
test	O
vector	O
the	O
same	O
label	O
as	O
the	O
chapter	O
5	O
section	O
2.5.1	O
291	O
292	O
6.	O
kernel	O
methods	O
closest	O
example	O
from	O
the	O
training	B
set	I
.	O
these	O
are	O
examples	O
of	O
memory-based	B
methods	I
that	O
involve	O
storing	O
the	O
entire	O
training	B
set	I
in	O
order	O
to	O
make	O
predictions	O
for	O
future	O
data	O
points	O
.	O
they	O
typically	O
require	O
a	O
metric	O
to	O
be	O
deﬁned	O
that	O
measures	O
the	O
similarity	O
of	O
any	O
two	O
vectors	O
in	O
input	O
space	O
,	O
and	O
are	O
generally	O
fast	O
to	O
‘	O
train	O
’	O
but	O
slow	O
at	O
making	O
predictions	O
for	O
test	O
data	O
points	O
.	O
many	O
linear	O
parametric	O
models	O
can	O
be	O
re-cast	O
into	O
an	O
equivalent	O
‘	O
dual	O
represen-	O
tation	O
’	O
in	O
which	O
the	O
predictions	O
are	O
also	O
based	O
on	O
linear	O
combinations	O
of	O
a	O
kernel	B
function	I
evaluated	O
at	O
the	O
training	B
data	O
points	O
.	O
as	O
we	O
shall	O
see	O
,	O
for	O
models	O
which	O
are	O
based	O
on	O
a	O
ﬁxed	O
nonlinear	O
feature	B
space	I
mapping	O
φ	O
(	O
x	O
)	O
,	O
the	O
kernel	B
function	I
is	O
given	O
by	O
the	O
relation	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
φ	O
(	O
x	O
)	O
tφ	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
.	O
(	O
6.1	O
)	O
from	O
this	O
deﬁnition	O
,	O
we	O
see	O
that	O
the	O
kernel	O
is	O
a	O
symmetric	O
function	O
of	O
its	O
arguments	O
so	O
that	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
k	O
(	O
x	O
(	O
cid:4	O
)	O
,	O
x	O
)	O
.	O
the	O
kernel	O
concept	O
was	O
introduced	O
into	O
the	O
ﬁeld	O
of	O
pat-	O
tern	O
recognition	O
by	O
aizerman	O
et	O
al	O
.	O
(	O
1964	O
)	O
in	O
the	O
context	O
of	O
the	O
method	O
of	O
potential	O
functions	O
,	O
so-called	O
because	O
of	O
an	O
analogy	O
with	O
electrostatics	O
.	O
although	O
neglected	O
for	O
many	O
years	O
,	O
it	O
was	O
re-introduced	O
into	O
machine	O
learning	O
in	O
the	O
context	O
of	O
large-	O
margin	B
classiﬁers	O
by	O
boser	O
et	O
al	O
.	O
(	O
1992	O
)	O
giving	O
rise	O
to	O
the	O
technique	O
of	O
support	B
vector	I
machines	O
.	O
since	O
then	O
,	O
there	O
has	O
been	O
considerable	O
interest	O
in	O
this	O
topic	O
,	O
both	O
in	O
terms	O
of	O
theory	B
and	O
applications	O
.	O
one	O
of	O
the	O
most	O
signiﬁcant	O
developments	O
has	O
been	O
the	O
extension	O
of	O
kernels	O
to	O
handle	O
symbolic	O
objects	O
,	O
thereby	O
greatly	O
expanding	O
the	O
range	O
of	O
problems	O
that	O
can	O
be	O
addressed	O
.	O
the	O
simplest	O
example	O
of	O
a	O
kernel	B
function	I
is	O
obtained	O
by	O
considering	O
the	O
identity	O
mapping	O
for	O
the	O
feature	B
space	I
in	O
(	O
6.1	O
)	O
so	O
that	O
φ	O
(	O
x	O
)	O
=	O
x	O
,	O
in	O
which	O
case	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
xtx	O
(	O
cid:4	O
)	O
.	O
we	O
shall	O
refer	O
to	O
this	O
as	O
the	O
linear	O
kernel	O
.	O
the	O
concept	O
of	O
a	O
kernel	O
formulated	O
as	O
an	O
inner	O
product	O
in	O
a	O
feature	B
space	I
allows	O
us	O
to	O
build	O
interesting	O
extensions	O
of	O
many	O
well-known	O
algorithms	O
by	O
making	O
use	O
of	O
the	O
kernel	B
trick	I
,	O
also	O
known	O
as	O
kernel	B
substitution	I
.	O
the	O
general	O
idea	O
is	O
that	O
,	O
if	O
we	O
have	O
an	O
algorithm	O
formulated	O
in	O
such	O
a	O
way	O
that	O
the	O
input	O
vector	O
x	O
enters	O
only	O
in	O
the	O
form	O
of	O
scalar	O
products	O
,	O
then	O
we	O
can	O
replace	O
that	O
scalar	O
product	O
with	O
some	O
other	O
choice	O
of	O
kernel	O
.	O
for	O
instance	O
,	O
the	O
technique	O
of	O
kernel	B
substitution	I
can	O
be	O
applied	O
to	O
principal	B
component	I
analysis	I
in	O
order	O
to	O
develop	O
a	O
nonlinear	O
variant	O
of	O
pca	O
(	O
sch¨olkopf	O
et	O
al.	O
,	O
1998	O
)	O
.	O
other	O
examples	O
of	O
kernel	B
substitution	I
include	O
nearest-neighbour	O
classiﬁers	O
and	O
the	O
kernel	O
fisher	O
discriminant	O
(	O
mika	O
et	O
al.	O
,	O
1999	O
;	O
roth	O
and	O
steinhage	O
,	O
2000	O
;	O
baudat	O
and	O
anouar	O
,	O
2000	O
)	O
.	O
there	O
are	O
numerous	O
forms	O
of	O
kernel	O
functions	O
in	O
common	O
use	O
,	O
and	O
we	O
shall	O
en-	O
counter	O
several	O
examples	O
in	O
this	O
chapter	O
.	O
many	O
have	O
the	O
property	O
of	O
being	O
a	O
function	O
only	O
of	O
the	O
difference	O
between	O
the	O
arguments	O
,	O
so	O
that	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
k	O
(	O
x	O
−	O
x	O
(	O
cid:4	O
)	O
)	O
,	O
which	O
are	O
known	O
as	O
stationary	B
kernels	O
because	O
they	O
are	O
invariant	O
to	O
translations	O
in	O
input	O
space	O
.	O
a	O
further	O
specialization	O
involves	O
homogeneous	B
kernels	O
,	O
also	O
known	O
as	O
ra-	O
dial	O
basis	O
functions	O
,	O
which	O
depend	O
only	O
on	O
the	O
magnitude	O
of	O
the	O
distance	O
(	O
typically	O
euclidean	O
)	O
between	O
the	O
arguments	O
so	O
that	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
k	O
(	O
(	O
cid:5	O
)	O
x	O
−	O
x	O
(	O
cid:4	O
)	O
(	O
cid:5	O
)	O
)	O
.	O
for	O
recent	O
textbooks	O
on	O
kernel	O
methods	O
,	O
see	O
sch¨olkopf	O
and	O
smola	O
(	O
2002	O
)	O
,	O
her-	O
brich	O
(	O
2002	O
)	O
,	O
and	O
shawe-taylor	O
and	O
cristianini	O
(	O
2004	O
)	O
.	O
chapter	O
7	O
section	O
12.3	O
section	O
6.3	O
6.1.	O
dual	O
representations	O
293	O
6.1.	O
dual	O
representations	O
n	O
(	O
cid:2	O
)	O
(	O
cid:26	O
)	O
n=1	O
(	O
cid:27	O
)	O
(	O
cid:26	O
)	O
(	O
cid:27	O
)	O
2	O
+	O
λ	O
2	O
n	O
(	O
cid:2	O
)	O
(	O
cid:27	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:26	O
)	O
many	O
linear	O
models	O
for	B
regression	I
and	O
classiﬁcation	B
can	O
be	O
reformulated	O
in	O
terms	O
of	O
a	O
dual	B
representation	I
in	O
which	O
the	O
kernel	B
function	I
arises	O
naturally	O
.	O
this	O
concept	O
will	O
play	O
an	O
important	O
role	O
when	O
we	O
consider	O
support	B
vector	I
machines	O
in	O
the	O
next	O
chapter	O
.	O
here	O
we	O
consider	O
a	O
linear	B
regression	I
model	O
whose	O
parameters	O
are	O
determined	O
by	O
minimizing	O
a	O
regularized	O
sum-of-squares	O
error	B
function	I
given	O
by	O
j	O
(	O
w	O
)	O
=	O
1	O
2	O
wtφ	O
(	O
xn	O
)	O
−	O
tn	O
wtw	O
(	O
6.2	O
)	O
where	O
λ	O
(	O
cid:2	O
)	O
0.	O
if	O
we	O
set	O
the	O
gradient	O
of	O
j	O
(	O
w	O
)	O
with	O
respect	O
to	O
w	O
equal	O
to	O
zero	O
,	O
we	O
see	O
that	O
the	O
solution	O
for	O
w	O
takes	O
the	O
form	O
of	O
a	O
linear	O
combination	O
of	O
the	O
vectors	O
φ	O
(	O
xn	O
)	O
,	O
with	O
coefﬁcients	O
that	O
are	O
functions	O
of	O
w	O
,	O
of	O
the	O
form	O
w	O
=	O
−	O
1	O
λ	O
wtφ	O
(	O
xn	O
)	O
−	O
tn	O
n=1	O
n=1	O
φ	O
(	O
xn	O
)	O
=	O
anφ	O
(	O
xn	O
)	O
=	O
φta	O
(	O
6.3	O
)	O
where	O
φ	O
is	O
the	O
design	B
matrix	I
,	O
whose	O
nth	O
row	O
is	O
given	O
by	O
φ	O
(	O
xn	O
)	O
t.	O
here	O
the	O
vector	O
a	O
=	O
(	O
a1	O
,	O
.	O
.	O
.	O
,	O
an	O
)	O
t	O
,	O
and	O
we	O
have	O
deﬁned	O
an	O
=	O
−	O
1	O
λ	O
wtφ	O
(	O
xn	O
)	O
−	O
tn	O
.	O
(	O
6.4	O
)	O
instead	O
of	O
working	O
with	O
the	O
parameter	O
vector	O
w	O
,	O
we	O
can	O
now	O
reformulate	O
the	O
least-	O
squares	O
algorithm	O
in	O
terms	O
of	O
the	O
parameter	O
vector	O
a	O
,	O
giving	O
rise	O
to	O
a	O
dual	O
represen-	O
tation	O
.	O
if	O
we	O
substitute	O
w	O
=	O
φta	O
into	O
j	O
(	O
w	O
)	O
,	O
we	O
obtain	O
j	O
(	O
a	O
)	O
=	O
1	O
2	O
atφφtφφta	O
−	O
atφφtt	O
+	O
1	O
2	O
ttt	O
+	O
λ	O
2	O
atφφta	O
(	O
6.5	O
)	O
where	O
t	O
=	O
(	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
)	O
t.	O
we	O
now	O
deﬁne	O
the	O
gram	O
matrix	O
k	O
=	O
φφt	O
,	O
which	O
is	O
an	O
n	O
×	O
n	O
symmetric	O
matrix	O
with	O
elements	O
knm	O
=	O
φ	O
(	O
xn	O
)	O
tφ	O
(	O
xm	O
)	O
=	O
k	O
(	O
xn	O
,	O
xm	O
)	O
(	O
6.6	O
)	O
where	O
we	O
have	O
introduced	O
the	O
kernel	B
function	I
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
deﬁned	O
by	O
(	O
6.1	O
)	O
.	O
in	O
terms	O
of	O
the	O
gram	O
matrix	O
,	O
the	O
sum-of-squares	B
error	I
function	O
can	O
be	O
written	O
as	O
j	O
(	O
a	O
)	O
=	O
1	O
2	O
atkka	O
−	O
atkt	O
+	O
1	O
2	O
ttt	O
+	O
λ	O
2	O
atka	O
.	O
(	O
6.7	O
)	O
setting	O
the	O
gradient	O
of	O
j	O
(	O
a	O
)	O
with	O
respect	O
to	O
a	O
to	O
zero	O
,	O
we	O
obtain	O
the	O
following	O
solu-	O
tion	O
a	O
=	O
(	O
k	O
+	O
λin	O
)	O
−1	O
t.	O
(	O
6.8	O
)	O
294	O
6.	O
kernel	O
methods	O
if	O
we	O
substitute	O
this	O
back	O
into	O
the	O
linear	B
regression	I
model	O
,	O
we	O
obtain	O
the	O
following	O
prediction	O
for	O
a	O
new	O
input	O
x	O
y	O
(	O
x	O
)	O
=	O
wtφ	O
(	O
x	O
)	O
=	O
atφφ	O
(	O
x	O
)	O
=	O
k	O
(	O
x	O
)	O
t	O
(	O
k	O
+	O
λin	O
)	O
−1	O
t	O
(	O
6.9	O
)	O
where	O
we	O
have	O
deﬁned	O
the	O
vector	O
k	O
(	O
x	O
)	O
with	O
elements	O
kn	O
(	O
x	O
)	O
=	O
k	O
(	O
xn	O
,	O
x	O
)	O
.	O
thus	O
we	O
see	O
that	O
the	O
dual	O
formulation	O
allows	O
the	O
solution	O
to	O
the	O
least-squares	O
problem	O
to	O
be	O
expressed	O
entirely	O
in	O
terms	O
of	O
the	O
kernel	B
function	I
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
.	O
this	O
is	O
known	O
as	O
a	O
dual	O
formulation	O
because	O
,	O
by	O
noting	O
that	O
the	O
solution	O
for	O
a	O
can	O
be	O
expressed	O
as	O
a	O
linear	O
combination	O
of	O
the	O
elements	O
of	O
φ	O
(	O
x	O
)	O
,	O
we	O
recover	O
the	O
original	O
formulation	O
in	O
terms	O
of	O
the	O
parameter	O
vector	O
w.	O
note	O
that	O
the	O
prediction	O
at	O
x	O
is	O
given	O
by	O
a	O
linear	O
combination	O
of	O
the	O
target	O
values	O
from	O
the	O
training	B
set	I
.	O
in	O
fact	O
,	O
we	O
have	O
already	O
obtained	O
this	O
result	O
,	O
using	O
a	O
slightly	O
different	O
notation	O
,	O
in	O
section	O
3.3.3.	O
in	O
the	O
dual	O
formulation	O
,	O
we	O
determine	O
the	O
parameter	O
vector	O
a	O
by	O
inverting	O
an	O
n	O
×	O
n	O
matrix	O
,	O
whereas	O
in	O
the	O
original	O
parameter	O
space	O
formulation	O
we	O
had	O
to	O
invert	O
an	O
m	O
×	O
m	O
matrix	O
in	O
order	O
to	O
determine	O
w.	O
because	O
n	O
is	O
typically	O
much	O
larger	O
than	O
m	O
,	O
the	O
dual	O
formulation	O
does	O
not	O
seem	O
to	O
be	O
particularly	O
useful	O
.	O
however	O
,	O
the	O
advantage	O
of	O
the	O
dual	O
formulation	O
,	O
as	O
we	O
shall	O
see	O
,	O
is	O
that	O
it	O
is	O
expressed	O
entirely	O
in	O
terms	O
of	O
the	O
kernel	B
function	I
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
.	O
we	O
can	O
therefore	O
work	O
directly	O
in	O
terms	O
of	O
kernels	O
and	O
avoid	O
the	O
explicit	O
introduction	O
of	O
the	O
feature	O
vector	O
φ	O
(	O
x	O
)	O
,	O
which	O
allows	O
us	O
implicitly	O
to	O
use	O
feature	O
spaces	O
of	O
high	O
,	O
even	O
inﬁnite	O
,	O
dimensionality	O
.	O
the	O
existence	O
of	O
a	O
dual	B
representation	I
based	O
on	O
the	O
gram	O
matrix	O
is	O
a	O
property	O
of	O
many	O
linear	O
models	O
,	O
including	O
the	O
perceptron	B
.	O
in	O
section	O
6.4	O
,	O
we	O
will	O
develop	O
a	O
dual-	O
ity	O
between	O
probabilistic	O
linear	O
models	O
for	B
regression	I
and	O
the	O
technique	O
of	O
gaussian	O
processes	O
.	O
duality	O
will	O
also	O
play	O
an	O
important	O
role	O
when	O
we	O
discuss	O
support	B
vector	I
machines	O
in	O
chapter	O
7.	O
exercise	O
6.1	O
exercise	O
6.2	O
6.2.	O
constructing	O
kernels	O
in	O
order	O
to	O
exploit	O
kernel	B
substitution	I
,	O
we	O
need	O
to	O
be	O
able	O
to	O
construct	O
valid	O
kernel	O
functions	O
.	O
one	O
approach	O
is	O
to	O
choose	O
a	O
feature	B
space	I
mapping	O
φ	O
(	O
x	O
)	O
and	O
then	O
use	O
this	O
to	O
ﬁnd	O
the	O
corresponding	O
kernel	O
,	O
as	O
is	O
illustrated	O
in	O
figure	O
6.1.	O
here	O
the	O
kernel	B
function	I
is	O
deﬁned	O
for	O
a	O
one-dimensional	O
input	O
space	O
by	O
m	O
(	O
cid:2	O
)	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
2	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
φ	O
(	O
x	O
)	O
tφ	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
(	O
cid:4	O
)	O
)	O
φi	O
(	O
x	O
)	O
φi	O
(	O
x	O
(	O
6.10	O
)	O
where	O
φi	O
(	O
x	O
)	O
are	O
the	O
basis	O
functions	O
.	O
i=1	O
an	O
alternative	O
approach	O
is	O
to	O
construct	O
kernel	O
functions	O
directly	O
.	O
in	O
this	O
case	O
,	O
we	O
must	O
ensure	O
that	O
the	O
function	O
we	O
choose	O
is	O
a	O
valid	O
kernel	O
,	O
in	O
other	O
words	O
that	O
it	O
corresponds	O
to	O
a	O
scalar	O
product	O
in	O
some	O
(	O
perhaps	O
inﬁnite	O
dimensional	O
)	O
feature	B
space	I
.	O
as	O
a	O
simple	O
example	O
,	O
consider	O
a	O
kernel	B
function	I
given	O
by	O
k	O
(	O
x	O
,	O
z	O
)	O
=	O
xtz	O
.	O
(	O
6.11	O
)	O
6.2.	O
constructing	O
kernels	O
295	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1	O
0.04	O
0.02	O
0	O
−1	O
1	O
0.75	O
0.5	O
0.25	O
0	O
−1	O
0.04	O
0.02	O
0	O
−1	O
1	O
0.75	O
0.5	O
0.25	O
0	O
−1	O
0.04	O
0.02	O
0	O
−1	O
0	O
0	O
1	O
1	O
0	O
1	O
0	O
1	O
0	O
0	O
1	O
1	O
figure	O
6.1	O
illustration	O
of	O
the	O
construction	O
of	O
kernel	O
functions	O
starting	O
from	O
a	O
corresponding	O
set	O
of	O
basis	O
func-	O
tions	O
.	O
in	O
each	O
column	O
the	O
lower	O
plot	O
shows	O
the	O
kernel	B
function	I
k	O
(	O
x	O
,	O
x	O
(	O
cid:1	O
)	O
)	O
deﬁned	O
by	O
(	O
6.10	O
)	O
plotted	O
as	O
a	O
function	O
of	O
x	O
for	O
x	O
(	O
cid:1	O
)	O
=	O
0	O
,	O
while	O
the	O
upper	O
plot	O
shows	O
the	O
corresponding	O
basis	O
functions	O
given	O
by	O
polynomials	O
(	O
left	O
column	O
)	O
,	O
‘	O
gaussians	O
’	O
(	O
centre	O
column	O
)	O
,	O
and	O
logistic	O
sigmoids	O
(	O
right	O
column	O
)	O
.	O
if	O
we	O
take	O
the	O
particular	O
case	O
of	O
a	O
two-dimensional	O
input	O
space	O
x	O
=	O
(	O
x1	O
,	O
x2	O
)	O
we	O
can	O
expand	O
out	O
the	O
terms	O
and	O
thereby	O
identify	O
the	O
corresponding	O
nonlinear	O
feature	O
mapping	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
2	O
=	O
(	O
x1z1	O
+	O
x2z2	O
)	O
2	O
k	O
(	O
x	O
,	O
z	O
)	O
=	O
xtz	O
2z2	O
1	O
+	O
2x1z1x2z2	O
+	O
x2	O
1z2	O
=	O
x2	O
√	O
√	O
2	O
=	O
(	O
x2	O
2x1x2	O
,	O
x2	O
2z1z2	O
,	O
z2	O
1	O
,	O
=	O
φ	O
(	O
x	O
)	O
tφ	O
(	O
z	O
)	O
.	O
2	O
)	O
(	O
z2	O
1	O
,	O
2	O
)	O
t	O
√	O
(	O
6.12	O
)	O
2	O
)	O
t	O
and	O
we	O
see	O
that	O
the	O
feature	O
mapping	O
takes	O
the	O
form	O
φ	O
(	O
x	O
)	O
=	O
(	O
x2	O
1	O
,	O
therefore	O
comprises	O
all	O
possible	O
second	B
order	I
terms	O
,	O
with	O
a	O
speciﬁc	O
weighting	O
be-	O
tween	O
them	O
.	O
2x1x2	O
,	O
x2	O
more	O
generally	O
,	O
however	O
,	O
we	O
need	O
a	O
simple	O
way	O
to	O
test	O
whether	O
a	O
function	O
con-	O
stitutes	O
a	O
valid	O
kernel	O
without	O
having	O
to	O
construct	O
the	O
function	O
φ	O
(	O
x	O
)	O
explicitly	O
.	O
a	O
necessary	O
and	O
sufﬁcient	O
condition	O
for	O
a	O
function	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
to	O
be	O
a	O
valid	O
kernel	O
(	O
shawe-	O
taylor	O
and	O
cristianini	O
,	O
2004	O
)	O
is	O
that	O
the	O
gram	O
matrix	O
k	O
,	O
whose	O
elements	O
are	O
given	O
by	O
k	O
(	O
xn	O
,	O
xm	O
)	O
,	O
should	O
be	O
positive	O
semideﬁnite	O
for	O
all	O
possible	O
choices	O
of	O
the	O
set	O
{	O
xn	O
}	O
.	O
note	O
that	O
a	O
positive	B
semideﬁnite	I
matrix	I
is	O
not	O
the	O
same	O
thing	O
as	O
a	O
matrix	O
whose	O
elements	O
are	O
nonnegative	O
.	O
one	O
powerful	O
technique	O
for	O
constructing	O
new	O
kernels	O
is	O
to	O
build	O
them	O
out	O
of	O
simpler	O
kernels	O
as	O
building	O
blocks	O
.	O
this	O
can	O
be	O
done	O
using	O
the	O
following	O
properties	O
:	O
appendix	O
c	O
296	O
6.	O
kernel	O
methods	O
techniques	O
for	O
constructing	O
new	O
kernels	O
.	O
given	O
valid	O
kernels	O
k1	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
and	O
k2	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
,	O
the	O
following	O
new	O
kernels	O
will	O
also	O
be	O
valid	O
:	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
ck1	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
f	O
(	O
x	O
)	O
k1	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
f	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
q	O
(	O
k1	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
)	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
exp	O
(	O
k1	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
)	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
k1	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
+	O
k2	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
k1	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
k2	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
k3	O
(	O
φ	O
(	O
x	O
)	O
,	O
φ	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
)	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
xtax	O
(	O
cid:4	O
)	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
ka	O
(	O
xa	O
,	O
x	O
(	O
cid:4	O
)	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
ka	O
(	O
xa	O
,	O
x	O
(	O
cid:4	O
)	O
(	O
6.13	O
)	O
(	O
6.14	O
)	O
(	O
6.15	O
)	O
(	O
6.16	O
)	O
(	O
6.17	O
)	O
(	O
6.18	O
)	O
(	O
6.19	O
)	O
(	O
6.20	O
)	O
(	O
6.21	O
)	O
(	O
6.22	O
)	O
where	O
c	O
>	O
0	O
is	O
a	O
constant	O
,	O
f	O
(	O
·	O
)	O
is	O
any	O
function	O
,	O
q	O
(	O
·	O
)	O
is	O
a	O
polynomial	O
with	O
nonneg-	O
m	O
,	O
k3	O
(	O
·	O
,	O
·	O
)	O
is	O
a	O
valid	O
kernel	O
in	O
ative	O
coefﬁcients	O
,	O
φ	O
(	O
x	O
)	O
is	O
a	O
function	O
from	O
x	O
to	O
r	O
m	O
,	O
a	O
is	O
a	O
symmetric	O
positive	B
semideﬁnite	I
matrix	I
,	O
xa	O
and	O
xb	O
are	O
variables	O
(	O
not	O
r	O
necessarily	O
disjoint	O
)	O
with	O
x	O
=	O
(	O
xa	O
,	O
xb	O
)	O
,	O
and	O
ka	O
and	O
kb	O
are	O
valid	O
kernel	O
functions	O
over	O
their	O
respective	O
spaces	O
.	O
a	O
)	O
+	O
kb	O
(	O
xb	O
,	O
x	O
(	O
cid:4	O
)	O
b	O
)	O
a	O
)	O
kb	O
(	O
xb	O
,	O
x	O
(	O
cid:4	O
)	O
b	O
)	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
2	O
equipped	O
with	O
these	O
properties	O
,	O
we	O
can	O
now	O
embark	O
on	O
the	O
construction	O
of	O
more	O
complex	O
kernels	O
appropriate	O
to	O
speciﬁc	O
applications	O
.	O
we	O
require	O
that	O
the	O
kernel	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
be	O
symmetric	O
and	O
positive	O
semideﬁnite	O
and	O
that	O
it	O
expresses	O
the	O
appropriate	O
form	O
of	O
similarity	O
between	O
x	O
and	O
x	O
(	O
cid:4	O
)	O
according	O
to	O
the	O
intended	O
application	O
.	O
here	O
we	O
consider	O
a	O
few	O
common	O
examples	O
of	O
kernel	O
functions	O
.	O
for	O
a	O
more	O
extensive	O
discus-	O
sion	B
of	O
‘	O
kernel	O
engineering	O
’	O
,	O
see	O
shawe-taylor	O
and	O
cristianini	O
(	O
2004	O
)	O
.	O
we	O
saw	O
that	O
the	O
simple	O
polynomial	O
kernel	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
contains	O
only	O
if	O
we	O
consider	O
the	O
slightly	O
generalized	B
kernel	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
with	O
c	O
>	O
0	O
,	O
then	O
the	O
corresponding	O
feature	O
mapping	O
φ	O
(	O
x	O
)	O
contains	O
con-	O
terms	O
of	O
degree	O
two	O
.	O
xtx	O
(	O
cid:4	O
)	O
+	O
c	O
stant	O
and	O
linear	O
terms	O
as	O
well	O
as	O
terms	O
of	O
order	O
two	O
.	O
similarly	O
,	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
contains	O
all	O
monomials	O
of	O
order	O
m.	O
for	O
instance	O
,	O
if	O
x	O
and	O
x	O
(	O
cid:4	O
)	O
are	O
two	O
images	O
,	O
then	O
the	O
kernel	O
represents	O
a	O
particular	O
weighted	O
sum	O
of	O
all	O
possible	O
products	O
of	O
m	O
pixels	O
in	O
the	O
ﬁrst	O
image	O
with	O
m	O
pixels	O
in	O
the	O
second	O
image	O
.	O
this	O
can	O
similarly	O
be	O
gener-	O
alized	O
to	O
include	O
all	O
terms	O
up	O
to	O
degree	O
m	O
by	O
considering	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
with	O
c	O
>	O
0.	O
using	O
the	O
results	O
(	O
6.17	O
)	O
and	O
(	O
6.18	O
)	O
for	O
combining	O
kernels	O
we	O
see	O
that	O
these	O
will	O
all	O
be	O
valid	O
kernel	O
functions	O
.	O
xtx	O
(	O
cid:4	O
)	O
(	O
cid:11	O
)	O
m	O
(	O
cid:11	O
)	O
m	O
(	O
cid:10	O
)	O
xtx	O
(	O
cid:4	O
)	O
+	O
c	O
(	O
cid:10	O
)	O
xtx	O
(	O
cid:4	O
)	O
(	O
cid:11	O
)	O
2	O
(	O
cid:10	O
)	O
another	O
commonly	O
used	O
kernel	O
takes	O
the	O
form	O
(	O
cid:10	O
)	O
−	O
(	O
cid:5	O
)	O
x	O
−	O
x	O
(	O
cid:4	O
)	O
(	O
cid:5	O
)	O
2/2σ2	O
(	O
cid:11	O
)	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
exp	O
(	O
6.23	O
)	O
and	O
is	O
often	O
called	O
a	O
‘	O
gaussian	O
’	O
kernel	O
.	O
note	O
,	O
however	O
,	O
that	O
in	O
this	O
context	O
it	O
is	O
not	O
interpreted	O
as	O
a	O
probability	B
density	O
,	O
and	O
hence	O
the	O
normalization	O
coefﬁcient	O
is	O
6.2.	O
constructing	O
kernels	O
297	O
omitted	O
.	O
we	O
can	O
see	O
that	O
this	O
is	O
a	O
valid	O
kernel	O
by	O
expanding	O
the	O
square	O
to	O
give	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
exp	O
(	O
cid:5	O
)	O
x	O
−	O
x	O
(	O
cid:4	O
)	O
(	O
cid:5	O
)	O
2	O
=	O
xtx	O
+	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
tx	O
(	O
cid:4	O
)	O
−	O
2xtx	O
(	O
cid:4	O
)	O
(	O
cid:10	O
)	O
−xtx/2σ2	O
(	O
cid:11	O
)	O
(	O
cid:10	O
)	O
exp	O
xtx	O
(	O
cid:4	O
)	O
/σ2	O
exp	O
(	O
cid:11	O
)	O
(	O
cid:10	O
)	O
−	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
tx	O
(	O
cid:4	O
)	O
(	O
cid:11	O
)	O
/2σ2	O
(	O
6.24	O
)	O
(	O
6.25	O
)	O
exercise	O
6.11	O
exercise	O
6.12	O
and	O
then	O
making	O
use	O
of	O
(	O
6.14	O
)	O
and	O
(	O
6.16	O
)	O
,	O
together	O
with	O
the	O
validity	O
of	O
the	O
linear	O
kernel	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
xtx	O
(	O
cid:4	O
)	O
.	O
note	O
that	O
the	O
feature	O
vector	O
that	O
corresponds	O
to	O
the	O
gaussian	O
kernel	O
has	O
inﬁnite	O
dimensionality	O
.	O
kernel	B
substitution	I
in	O
(	O
6.24	O
)	O
to	O
replace	O
xtx	O
(	O
cid:4	O
)	O
obtain	O
the	O
gaussian	O
kernel	O
is	O
not	O
restricted	O
to	O
the	O
use	O
of	O
euclidean	O
distance	O
.	O
if	O
we	O
use	O
with	O
a	O
nonlinear	O
kernel	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
,	O
we	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
exp	O
−	O
1	O
2σ2	O
(	O
κ	O
(	O
x	O
,	O
x	O
)	O
+	O
κ	O
(	O
x	O
(	O
cid:4	O
)	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
−	O
2κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
)	O
.	O
(	O
6.26	O
)	O
an	O
important	O
contribution	O
to	O
arise	O
from	O
the	O
kernel	O
viewpoint	O
has	O
been	O
the	O
exten-	O
sion	B
to	O
inputs	O
that	O
are	O
symbolic	O
,	O
rather	O
than	O
simply	O
vectors	O
of	O
real	O
numbers	O
.	O
kernel	O
functions	O
can	O
be	O
deﬁned	O
over	O
objects	O
as	O
diverse	O
as	O
graphs	O
,	O
sets	O
,	O
strings	O
,	O
and	O
text	O
doc-	O
uments	O
.	O
consider	O
,	O
for	O
instance	O
,	O
a	O
ﬁxed	O
set	O
and	O
deﬁne	O
a	O
nonvectorial	O
space	O
consisting	O
of	O
all	O
possible	O
subsets	O
of	O
this	O
set	O
.	O
if	O
a1	O
and	O
a2	O
are	O
two	O
such	O
subsets	O
then	O
one	O
simple	O
choice	O
of	O
kernel	O
would	O
be	O
k	O
(	O
a1	O
,	O
a2	O
)	O
=	O
2|a1∩a2|	O
(	O
6.27	O
)	O
where	O
a1	O
∩	O
a2	O
denotes	O
the	O
intersection	O
of	O
sets	O
a1	O
and	O
a2	O
,	O
and	O
|a|	O
denotes	O
the	O
number	O
of	O
subsets	O
in	O
a.	O
this	O
is	O
a	O
valid	O
kernel	B
function	I
because	O
it	O
can	O
be	O
shown	O
to	O
correspond	O
to	O
an	O
inner	O
product	O
in	O
a	O
feature	B
space	I
.	O
one	O
powerful	O
approach	O
to	O
the	O
construction	O
of	O
kernels	O
starts	O
from	O
a	O
probabilistic	O
generative	O
model	O
(	O
haussler	O
,	O
1999	O
)	O
,	O
which	O
allows	O
us	O
to	O
apply	O
generative	O
models	O
in	O
a	O
discriminative	O
setting	O
.	O
generative	O
models	O
can	O
deal	O
naturally	O
with	O
missing	B
data	I
and	O
in	O
the	O
case	O
of	O
hidden	O
markov	O
models	O
can	O
handle	O
sequences	O
of	O
varying	O
length	O
.	O
by	O
contrast	O
,	O
discriminative	O
models	O
generally	O
give	O
better	O
performance	O
on	O
discriminative	O
tasks	O
than	O
generative	O
models	O
.	O
it	O
is	O
therefore	O
of	O
some	O
interest	O
to	O
combine	O
these	O
two	O
approaches	O
(	O
lasserre	O
et	O
al.	O
,	O
2006	O
)	O
.	O
one	O
way	O
to	O
combine	O
them	O
is	O
to	O
use	O
a	O
generative	B
model	I
to	O
deﬁne	O
a	O
kernel	O
,	O
and	O
then	O
use	O
this	O
kernel	O
in	O
a	O
discriminative	O
approach	O
.	O
given	O
a	O
generative	B
model	I
p	O
(	O
x	O
)	O
we	O
can	O
deﬁne	O
a	O
kernel	O
by	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
.	O
(	O
6.28	O
)	O
this	O
is	O
clearly	O
a	O
valid	O
kernel	B
function	I
because	O
we	O
can	O
interpret	O
it	O
as	O
an	O
inner	O
product	O
in	O
the	O
one-dimensional	O
feature	B
space	I
deﬁned	O
by	O
the	O
mapping	O
p	O
(	O
x	O
)	O
.	O
it	O
says	O
that	O
two	O
inputs	O
x	O
and	O
x	O
(	O
cid:4	O
)	O
are	O
similar	O
if	O
they	O
both	O
have	O
high	O
probabilities	O
.	O
we	O
can	O
use	O
(	O
6.13	O
)	O
and	O
(	O
6.17	O
)	O
to	O
extend	O
this	O
class	O
of	O
kernels	O
by	O
considering	O
sums	O
over	O
products	O
of	O
different	O
probability	B
distributions	O
,	O
with	O
positive	O
weighting	O
coefﬁcients	O
p	O
(	O
i	O
)	O
,	O
of	O
the	O
form	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
p	O
(	O
x|i	O
)	O
p	O
(	O
x	O
(	O
cid:4	O
)	O
|i	O
)	O
p	O
(	O
i	O
)	O
.	O
(	O
6.29	O
)	O
(	O
cid:2	O
)	O
i	O
298	O
6.	O
kernel	O
methods	O
section	O
9.2	O
section	O
13.2	O
exercise	O
6.13	O
this	O
is	O
equivalent	O
,	O
up	O
to	O
an	O
overall	O
multiplicative	O
constant	O
,	O
to	O
a	O
mixture	B
distribution	I
in	O
which	O
the	O
components	O
factorize	O
,	O
with	O
the	O
index	O
i	O
playing	O
the	O
role	O
of	O
a	O
‘	O
latent	O
’	O
variable	O
.	O
two	O
inputs	O
x	O
and	O
x	O
(	O
cid:4	O
)	O
will	O
give	O
a	O
large	O
value	O
for	O
the	O
kernel	B
function	I
,	O
and	O
hence	O
appear	O
similar	O
,	O
if	O
they	O
have	O
signiﬁcant	O
probability	B
under	O
a	O
range	O
of	O
different	O
components	O
.	O
taking	O
the	O
limit	O
of	O
an	O
inﬁnite	O
sum	O
,	O
we	O
can	O
also	O
consider	O
kernels	O
of	O
the	O
form	O
(	O
cid:6	O
)	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
p	O
(	O
x|z	O
)	O
p	O
(	O
x	O
(	O
cid:4	O
)	O
|z	O
)	O
p	O
(	O
z	O
)	O
dz	O
(	O
6.30	O
)	O
where	O
z	O
is	O
a	O
continuous	O
latent	B
variable	I
.	O
now	O
suppose	O
that	O
our	O
data	O
consists	O
of	O
ordered	O
sequences	O
of	O
length	O
l	O
so	O
that	O
an	O
observation	O
is	O
given	O
by	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xl	O
}	O
.	O
a	O
popular	O
generative	B
model	I
for	O
sequences	O
is	O
the	O
hidden	O
markov	O
model	O
,	O
which	O
expresses	O
the	O
distribution	O
p	O
(	O
x	O
)	O
as	O
a	O
marginalization	O
over	O
a	O
corresponding	O
sequence	O
of	O
hidden	O
states	O
z	O
=	O
{	O
z1	O
,	O
.	O
.	O
.	O
,	O
zl	O
}	O
.	O
we	O
can	O
use	O
this	O
approach	O
to	O
deﬁne	O
a	O
kernel	B
function	I
measuring	O
the	O
similarity	O
of	O
two	O
sequences	O
x	O
and	O
x	O
(	O
cid:4	O
)	O
by	O
extending	O
the	O
mixture	B
representation	O
(	O
6.29	O
)	O
to	O
give	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
p	O
(	O
x|z	O
)	O
p	O
(	O
x	O
(	O
cid:4	O
)	O
|z	O
)	O
p	O
(	O
z	O
)	O
(	O
cid:2	O
)	O
(	O
6.31	O
)	O
z	O
so	O
that	O
both	O
observed	O
sequences	O
are	O
generated	O
by	O
the	O
same	O
hidden	O
sequence	O
z.	O
this	O
model	O
can	O
easily	O
be	O
extended	B
to	O
allow	O
sequences	O
of	O
differing	O
length	O
to	O
be	O
compared	O
.	O
an	O
alternative	O
technique	O
for	O
using	O
generative	O
models	O
to	O
deﬁne	O
kernel	O
functions	O
is	O
known	O
as	O
the	O
fisher	O
kernel	O
(	O
jaakkola	O
and	O
haussler	O
,	O
1999	O
)	O
.	O
consider	O
a	O
parametric	O
generative	B
model	I
p	O
(	O
x|θ	O
)	O
where	O
θ	O
denotes	O
the	O
vector	O
of	O
parameters	O
.	O
the	O
goal	O
is	O
to	O
ﬁnd	O
a	O
kernel	O
that	O
measures	O
the	O
similarity	O
of	O
two	O
input	O
vectors	O
x	O
and	O
x	O
(	O
cid:4	O
)	O
induced	O
by	O
the	O
generative	B
model	I
.	O
jaakkola	O
and	O
haussler	O
(	O
1999	O
)	O
consider	O
the	O
gradient	O
with	O
respect	O
to	O
θ	O
,	O
which	O
deﬁnes	O
a	O
vector	O
in	O
a	O
‘	O
feature	O
’	O
space	O
having	O
the	O
same	O
dimensionality	O
as	O
θ.	O
in	O
particular	O
,	O
they	O
consider	O
the	O
fisher	O
score	O
g	O
(	O
θ	O
,	O
x	O
)	O
=	O
∇θ	O
ln	O
p	O
(	O
x|θ	O
)	O
from	O
which	O
the	O
fisher	O
kernel	O
is	O
deﬁned	O
by	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
g	O
(	O
θ	O
,	O
x	O
)	O
tf−1g	O
(	O
θ	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
.	O
here	O
f	O
is	O
the	O
fisher	O
information	O
matrix	O
,	O
given	O
by	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
(	O
6.32	O
)	O
(	O
6.33	O
)	O
f	O
=	O
ex	O
g	O
(	O
θ	O
,	O
x	O
)	O
g	O
(	O
θ	O
,	O
x	O
)	O
t	O
(	O
6.34	O
)	O
where	O
the	O
expectation	B
is	O
with	O
respect	O
to	O
x	O
under	O
the	O
distribution	O
p	O
(	O
x|θ	O
)	O
.	O
this	O
can	O
be	O
motivated	O
from	O
the	O
perspective	O
of	O
information	B
geometry	I
(	O
amari	O
,	O
1998	O
)	O
,	O
which	O
considers	O
the	O
differential	B
geometry	O
of	O
the	O
space	O
of	O
model	O
parameters	O
.	O
here	O
we	O
sim-	O
ply	O
note	O
that	O
the	O
presence	O
of	O
the	O
fisher	O
information	O
matrix	O
causes	O
this	O
kernel	O
to	O
be	O
invariant	O
under	O
a	O
nonlinear	O
re-parameterization	O
of	O
the	O
density	B
model	O
θ	O
→	O
ψ	O
(	O
θ	O
)	O
.	O
in	O
practice	O
,	O
it	O
is	O
often	O
infeasible	O
to	O
evaluate	O
the	O
fisher	O
information	O
matrix	O
.	O
one	O
approach	O
is	O
simply	O
to	O
replace	O
the	O
expectation	B
in	O
the	O
deﬁnition	O
of	O
the	O
fisher	O
informa-	O
tion	O
with	O
the	O
sample	O
average	O
,	O
giving	O
g	O
(	O
θ	O
,	O
xn	O
)	O
g	O
(	O
θ	O
,	O
xn	O
)	O
t.	O
(	O
6.35	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
f	O
(	O
cid:7	O
)	O
1	O
n	O
6.3.	O
radial	B
basis	I
function	I
networks	O
299	O
section	O
12.1.3	O
this	O
is	O
the	O
covariance	B
matrix	I
of	O
the	O
fisher	O
scores	O
,	O
and	O
so	O
the	O
fisher	O
kernel	O
corre-	O
sponds	O
to	O
a	O
whitening	B
of	O
these	O
scores	O
.	O
more	O
simply	O
,	O
we	O
can	O
just	O
omit	O
the	O
fisher	O
information	O
matrix	O
altogether	O
and	O
use	O
the	O
noninvariant	O
kernel	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
g	O
(	O
θ	O
,	O
x	O
)	O
tg	O
(	O
θ	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
.	O
(	O
6.36	O
)	O
an	O
application	O
of	O
fisher	O
kernels	O
to	O
document	B
retrieval	I
is	O
given	O
by	O
hofmann	O
(	O
2000	O
)	O
.	O
a	O
ﬁnal	O
example	O
of	O
a	O
kernel	B
function	I
is	O
the	O
sigmoidal	O
kernel	O
given	O
by	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
tanh	O
axtx	O
(	O
cid:4	O
)	O
+	O
b	O
(	O
6.37	O
)	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
whose	O
gram	O
matrix	O
in	O
general	O
is	O
not	O
positive	O
semideﬁnite	O
.	O
this	O
form	O
of	O
kernel	O
has	O
,	O
however	O
,	O
been	O
used	O
in	O
practice	O
(	O
vapnik	O
,	O
1995	O
)	O
,	O
possibly	O
because	O
it	O
gives	O
kernel	O
expansions	O
such	O
as	O
the	O
support	B
vector	I
machine	I
a	O
superﬁcial	O
resemblance	O
to	O
neural	B
network	I
models	O
.	O
as	O
we	O
shall	O
see	O
,	O
in	O
the	O
limit	O
of	O
an	O
inﬁnite	O
number	O
of	O
basis	O
functions	O
,	O
a	O
bayesian	O
neural	B
network	I
with	O
an	O
appropriate	O
prior	B
reduces	O
to	O
a	O
gaussian	O
process	O
,	O
thereby	O
providing	O
a	O
deeper	O
link	B
between	O
neural	O
networks	O
and	O
kernel	O
methods	O
.	O
section	O
6.4.7	O
6.3.	O
radial	B
basis	I
function	I
networks	O
in	O
chapter	O
3	O
,	O
we	O
discussed	O
regression	B
models	O
based	O
on	O
linear	O
combinations	O
of	O
ﬁxed	O
basis	O
functions	O
,	O
although	O
we	O
did	O
not	O
discuss	O
in	O
detail	O
what	O
form	O
those	O
basis	O
functions	O
might	O
take	O
.	O
one	O
choice	O
that	O
has	O
been	O
widely	O
used	O
is	O
that	O
of	O
radial	O
basis	O
functions	O
,	O
which	O
have	O
the	O
property	O
that	O
each	O
basis	B
function	I
depends	O
only	O
on	O
the	O
radial	O
distance	O
(	O
typically	O
euclidean	O
)	O
from	O
a	O
centre	O
µj	O
,	O
so	O
that	O
φj	O
(	O
x	O
)	O
=	O
h	O
(	O
(	O
cid:5	O
)	O
x	O
−	O
µj	O
(	O
cid:5	O
)	O
)	O
.	O
historically	O
,	O
radial	O
basis	O
functions	O
were	O
introduced	O
for	O
the	O
purpose	O
of	O
exact	O
func-	O
tion	O
interpolation	O
(	O
powell	O
,	O
1987	O
)	O
.	O
given	O
a	O
set	O
of	O
input	O
vectors	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
along	O
with	O
corresponding	O
target	O
values	O
{	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
}	O
,	O
the	O
goal	O
is	O
to	O
ﬁnd	O
a	O
smooth	O
function	O
f	O
(	O
x	O
)	O
that	O
ﬁts	O
every	O
target	O
value	O
exactly	O
,	O
so	O
that	O
f	O
(	O
xn	O
)	O
=	O
tn	O
for	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
this	O
is	O
achieved	O
by	O
expressing	O
f	O
(	O
x	O
)	O
as	O
a	O
linear	O
combination	O
of	O
radial	O
basis	O
functions	O
,	O
one	O
centred	O
on	O
every	O
data	O
point	O
f	O
(	O
x	O
)	O
=	O
wnh	O
(	O
(	O
cid:5	O
)	O
x	O
−	O
xn	O
(	O
cid:5	O
)	O
)	O
.	O
(	O
6.38	O
)	O
n=1	O
the	O
values	O
of	O
the	O
coefﬁcients	O
{	O
wn	O
}	O
are	O
found	O
by	O
least	O
squares	O
,	O
and	O
because	O
there	O
are	O
the	O
same	O
number	O
of	O
coefﬁcients	O
as	O
there	O
are	O
constraints	O
,	O
the	O
result	O
is	O
a	O
function	O
that	O
ﬁts	O
every	O
target	O
value	O
exactly	O
.	O
in	O
pattern	O
recognition	O
applications	O
,	O
however	O
,	O
the	O
target	O
values	O
are	O
generally	O
noisy	O
,	O
and	O
exact	O
interpolation	O
is	O
undesirable	O
because	O
this	O
corresponds	O
to	O
an	O
over-ﬁtted	O
solution	O
.	O
expansions	O
in	O
radial	O
basis	O
functions	O
also	O
arise	O
from	O
regularization	B
theory	O
(	O
pog-	O
gio	O
and	O
girosi	O
,	O
1990	O
;	O
bishop	O
,	O
1995a	O
)	O
.	O
for	O
a	O
sum-of-squares	B
error	I
function	O
with	O
a	O
regularizer	O
deﬁned	O
in	O
terms	O
of	O
a	O
differential	B
operator	O
,	O
the	O
optimal	O
solution	O
is	O
given	O
by	O
an	O
expansion	O
in	O
the	O
green	O
’	O
s	O
functions	O
of	O
the	O
operator	O
(	O
which	O
are	O
analogous	O
to	O
the	O
eigenvectors	O
of	O
a	O
discrete	O
matrix	O
)	O
,	O
again	O
with	O
one	O
basis	B
function	I
centred	O
on	O
each	O
data	O
n	O
(	O
cid:2	O
)	O
300	O
6.	O
kernel	O
methods	O
point	O
.	O
if	O
the	O
differential	B
operator	O
is	O
isotropic	B
then	O
the	O
green	O
’	O
s	O
functions	O
depend	O
only	O
on	O
the	O
radial	O
distance	O
from	O
the	O
corresponding	O
data	O
point	O
.	O
due	O
to	O
the	O
presence	O
of	O
the	O
regularizer	O
,	O
the	O
solution	O
no	O
longer	O
interpolates	O
the	O
training	B
data	O
exactly	O
.	O
another	O
motivation	O
for	O
radial	O
basis	O
functions	O
comes	O
from	O
a	O
consideration	O
of	O
the	O
interpolation	O
problem	O
when	O
the	O
input	O
(	O
rather	O
than	O
the	O
target	O
)	O
variables	O
are	O
noisy	O
if	O
the	O
noise	O
on	O
the	O
input	O
variable	O
x	O
is	O
described	O
(	O
webb	O
,	O
1994	O
;	O
bishop	O
,	O
1995a	O
)	O
.	O
by	O
a	O
variable	O
ξ	O
having	O
a	O
distribution	O
ν	O
(	O
ξ	O
)	O
,	O
then	O
the	O
sum-of-squares	B
error	I
function	O
becomes	O
{	O
y	O
(	O
xn	O
+	O
ξ	O
)	O
−	O
tn	O
}	O
2	O
ν	O
(	O
ξ	O
)	O
dξ	O
.	O
(	O
6.39	O
)	O
(	O
cid:6	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
e	O
=	O
1	O
2	O
appendix	O
d	O
exercise	O
6.17	O
using	O
the	O
calculus	B
of	I
variations	I
,	O
we	O
can	O
optimize	O
with	O
respect	O
to	O
the	O
function	O
f	O
(	O
x	O
)	O
to	O
give	O
n	O
(	O
cid:2	O
)	O
y	O
(	O
xn	O
)	O
=	O
tnh	O
(	O
x	O
−	O
xn	O
)	O
(	O
6.40	O
)	O
n=1	O
where	O
the	O
basis	O
functions	O
are	O
given	O
by	O
h	O
(	O
x	O
−	O
xn	O
)	O
=	O
ν	O
(	O
x	O
−	O
xn	O
)	O
ν	O
(	O
x	O
−	O
xn	O
)	O
n	O
(	O
cid:2	O
)	O
.	O
(	O
6.41	O
)	O
n=1	O
we	O
see	O
that	O
there	O
is	O
one	O
basis	B
function	I
centred	O
on	O
every	O
data	O
point	O
.	O
this	O
is	O
known	O
as	O
the	O
nadaraya-watson	O
model	O
and	O
will	O
be	O
derived	O
again	O
from	O
a	O
different	O
perspective	O
in	O
section	O
6.3.1.	O
if	O
the	O
noise	O
distribution	O
ν	O
(	O
ξ	O
)	O
is	O
isotropic	B
,	O
so	O
that	O
it	O
is	O
a	O
function	O
only	O
of	O
(	O
cid:5	O
)	O
ξ	O
(	O
cid:5	O
)	O
,	O
then	O
the	O
basis	O
functions	O
will	O
be	O
radial	O
.	O
n	O
h	O
(	O
x	O
−	O
xn	O
)	O
=	O
1	O
for	O
any	O
value	O
of	O
x.	O
the	O
effect	O
of	O
such	O
normalization	O
is	O
shown	O
in	O
figure	O
6.2.	O
normal-	O
ization	O
is	O
sometimes	O
used	O
in	O
practice	O
as	O
it	O
avoids	O
having	O
regions	O
of	O
input	O
space	O
where	O
all	O
of	O
the	O
basis	O
functions	O
take	O
small	O
values	O
,	O
which	O
would	O
necessarily	O
lead	O
to	O
predic-	O
tions	O
in	O
such	O
regions	O
that	O
are	O
either	O
small	O
or	O
controlled	O
purely	O
by	O
the	O
bias	B
parameter	I
.	O
note	O
that	O
the	O
basis	O
functions	O
(	O
6.41	O
)	O
are	O
normalized	O
,	O
so	O
that	O
(	O
cid:5	O
)	O
another	O
situation	O
in	O
which	O
expansions	O
in	O
normalized	O
radial	O
basis	O
functions	O
arise	O
is	O
in	O
the	O
application	O
of	O
kernel	O
density	O
estimation	O
to	O
the	O
problem	O
of	O
regression	B
,	O
as	O
we	O
shall	O
discuss	O
in	O
section	O
6.3.1.	O
because	O
there	O
is	O
one	O
basis	B
function	I
associated	O
with	O
every	O
data	O
point	O
,	O
the	O
corre-	O
sponding	O
model	O
can	O
be	O
computationally	O
costly	O
to	O
evaluate	O
when	O
making	O
predictions	O
for	O
new	O
data	O
points	O
.	O
models	O
have	O
therefore	O
been	O
proposed	O
(	O
broomhead	O
and	O
lowe	O
,	O
1988	O
;	O
moody	O
and	O
darken	O
,	O
1989	O
;	O
poggio	O
and	O
girosi	O
,	O
1990	O
)	O
,	O
which	O
retain	O
the	O
expan-	O
sion	B
in	O
radial	O
basis	O
functions	O
but	O
where	O
the	O
number	O
m	O
of	O
basis	O
functions	O
is	O
smaller	O
than	O
the	O
number	O
n	O
of	O
data	O
points	O
.	O
typically	O
,	O
the	O
number	O
of	O
basis	O
functions	O
,	O
and	O
the	O
locations	O
µi	O
of	O
their	O
centres	O
,	O
are	O
determined	O
based	O
on	O
the	O
input	O
data	O
{	O
xn	O
}	O
alone	O
.	O
the	O
basis	O
functions	O
are	O
then	O
kept	O
ﬁxed	O
and	O
the	O
coefﬁcients	O
{	O
wi	O
}	O
are	O
determined	O
by	O
least	O
squares	O
by	O
solving	O
the	O
usual	O
set	O
of	O
linear	O
equations	O
,	O
as	O
discussed	O
in	O
section	O
3.1.1	O
.	O
6.3.	O
radial	B
basis	I
function	I
networks	O
301	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−1	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−1	O
−0.5	O
0	O
0.5	O
1	O
−0.5	O
0	O
0.5	O
1	O
figure	O
6.2	O
plot	O
of	O
a	O
set	O
of	O
gaussian	O
basis	O
functions	O
on	O
the	O
left	O
,	O
together	O
with	O
the	O
corresponding	O
normalized	O
basis	O
functions	O
on	O
the	O
right	O
.	O
one	O
of	O
the	O
simplest	O
ways	O
of	O
choosing	O
basis	B
function	I
centres	O
is	O
to	O
use	O
a	O
randomly	O
chosen	O
subset	O
of	O
the	O
data	O
points	O
.	O
a	O
more	O
systematic	O
approach	O
is	O
called	O
orthogonal	B
least	I
squares	I
(	O
chen	O
et	O
al.	O
,	O
1991	O
)	O
.	O
this	O
is	O
a	O
sequential	O
selection	O
process	O
in	O
which	O
at	O
each	O
step	O
the	O
next	O
data	O
point	O
to	O
be	O
chosen	O
as	O
a	O
basis	B
function	I
centre	O
corresponds	O
to	O
the	O
one	O
that	O
gives	O
the	O
greatest	O
reduction	O
in	O
the	O
sum-of-squares	B
error	I
.	O
values	O
for	O
the	O
expansion	O
coefﬁcients	O
are	O
determined	O
as	O
part	O
of	O
the	O
algorithm	O
.	O
clustering	B
algorithms	O
such	O
as	O
k-means	O
have	O
also	O
been	O
used	O
,	O
which	O
give	O
a	O
set	O
of	O
basis	B
function	I
centres	O
that	O
no	O
longer	O
coincide	O
with	O
training	B
data	O
points	O
.	O
6.3.1	O
nadaraya-watson	O
model	O
in	O
section	O
3.3.3	O
,	O
we	O
saw	O
that	O
the	O
prediction	O
of	O
a	O
linear	B
regression	I
model	O
for	O
a	O
new	O
input	O
x	O
takes	O
the	O
form	O
of	O
a	O
linear	O
combination	O
of	O
the	O
training	B
set	I
target	O
values	O
with	O
coefﬁcients	O
given	O
by	O
the	O
‘	O
equivalent	B
kernel	I
’	O
(	O
3.62	O
)	O
where	O
the	O
equivalent	B
kernel	I
satisﬁes	O
the	O
summation	O
constraint	O
(	O
3.64	O
)	O
.	O
we	O
can	O
motivate	O
the	O
kernel	B
regression	I
model	O
(	O
3.61	O
)	O
from	O
a	O
different	O
perspective	O
,	O
starting	O
with	O
kernel	O
density	O
estimation	O
.	O
suppose	O
we	O
have	O
a	O
training	B
set	I
{	O
xn	O
,	O
tn	O
}	O
and	O
we	O
use	O
a	O
parzen	O
density	B
estimator	O
to	O
model	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
t	O
)	O
,	O
so	O
that	O
section	O
9.1	O
section	O
2.5.1	O
n	O
(	O
cid:2	O
)	O
n=1	O
p	O
(	O
x	O
,	O
t	O
)	O
=	O
1	O
n	O
f	O
(	O
x	O
−	O
xn	O
,	O
t	O
−	O
tn	O
)	O
(	O
6.42	O
)	O
where	O
f	O
(	O
x	O
,	O
t	O
)	O
is	O
the	O
component	O
density	B
function	O
,	O
and	O
there	O
is	O
one	O
such	O
component	O
centred	O
on	O
each	O
data	O
point	O
.	O
we	O
now	O
ﬁnd	O
an	O
expression	O
for	O
the	O
regression	B
function	I
y	O
(	O
x	O
)	O
,	O
corresponding	O
to	O
the	O
conditional	B
average	O
of	O
the	O
target	O
variable	O
conditioned	O
on	O
302	O
6.	O
kernel	O
methods	O
the	O
input	O
variable	O
,	O
which	O
is	O
given	O
by	O
y	O
(	O
x	O
)	O
=	O
e	O
[	O
t|x	O
]	O
=	O
(	O
cid:6	O
)	O
∞	O
−∞	O
tp	O
(	O
t|x	O
)	O
dt	O
tp	O
(	O
x	O
,	O
t	O
)	O
dt	O
p	O
(	O
x	O
,	O
t	O
)	O
dt	O
=	O
=	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:6	O
)	O
∞	O
m	O
n	O
−∞	O
g	O
(	O
x	O
−	O
xn	O
)	O
tn	O
g	O
(	O
x	O
−	O
xm	O
)	O
k	O
(	O
x	O
,	O
xn	O
)	O
tn	O
y	O
(	O
x	O
)	O
=	O
=	O
k	O
(	O
x	O
,	O
xn	O
)	O
=	O
g	O
(	O
x	O
−	O
xn	O
)	O
g	O
(	O
x	O
−	O
xm	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
m	O
n	O
n	O
(	O
cid:2	O
)	O
(	O
cid:6	O
)	O
∞	O
m	O
(	O
6.44	O
)	O
(	O
6.45	O
)	O
(	O
6.46	O
)	O
(	O
6.47	O
)	O
tf	O
(	O
x	O
−	O
xn	O
,	O
t	O
−	O
tn	O
)	O
dt	O
f	O
(	O
x	O
−	O
xm	O
,	O
t	O
−	O
tm	O
)	O
dt	O
.	O
(	O
6.43	O
)	O
we	O
now	O
assume	O
for	O
simplicity	O
that	O
the	O
component	O
density	B
functions	O
have	O
zero	O
mean	B
so	O
that	O
f	O
(	O
x	O
,	O
t	O
)	O
t	O
dt	O
=	O
0	O
for	O
all	O
values	O
of	O
x.	O
using	O
a	O
simple	O
change	O
of	O
variable	O
,	O
we	O
then	O
obtain	O
where	O
n	O
,	O
m	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
and	O
the	O
kernel	B
function	I
k	O
(	O
x	O
,	O
xn	O
)	O
is	O
given	O
by	O
and	O
we	O
have	O
deﬁned	O
g	O
(	O
x	O
)	O
=	O
f	O
(	O
x	O
,	O
t	O
)	O
dt	O
.	O
−∞	O
the	O
result	O
(	O
6.45	O
)	O
is	O
known	O
as	O
the	O
nadaraya-watson	O
model	O
,	O
or	O
kernel	B
regression	I
(	O
nadaraya	O
,	O
1964	O
;	O
watson	O
,	O
1964	O
)	O
.	O
for	O
a	O
localized	O
kernel	B
function	I
,	O
it	O
has	O
the	O
prop-	O
erty	O
of	O
giving	O
more	O
weight	O
to	O
the	O
data	O
points	O
xn	O
that	O
are	O
close	O
to	O
x.	O
note	O
that	O
the	O
kernel	O
(	O
6.46	O
)	O
satisﬁes	O
the	O
summation	O
constraint	O
n	O
(	O
cid:2	O
)	O
n=1	O
k	O
(	O
x	O
,	O
xn	O
)	O
=	O
1	O
.	O
6.4.	O
gaussian	O
processes	O
303	O
figure	O
6.3	O
illustration	O
of	O
the	O
nadaraya-watson	O
kernel	B
regression	I
model	O
using	O
isotropic	B
gaussian	O
kernels	O
,	O
for	O
the	O
sinusoidal	B
data	I
set	O
.	O
the	O
original	O
sine	O
function	O
is	O
shown	O
by	O
the	O
green	O
curve	O
,	O
the	O
data	O
points	O
are	O
shown	O
in	O
blue	O
,	O
and	O
each	O
is	O
the	O
centre	O
of	O
an	O
isotropic	B
gaussian	O
kernel	O
.	O
the	O
resulting	O
regression	B
function	I
,	O
given	O
by	O
the	O
condi-	O
tional	O
mean	B
,	O
is	O
shown	O
by	O
the	O
red	O
line	O
,	O
along	O
with	O
the	O
two-	O
standard-deviation	O
region	O
for	O
the	O
conditional	B
distribution	O
p	O
(	O
t|x	O
)	O
shown	O
by	O
the	O
red	O
shading	O
.	O
the	O
blue	O
ellipse	O
around	O
each	O
data	O
point	O
shows	O
one	O
standard	B
deviation	I
contour	O
for	O
the	O
corresponding	O
kernel	O
.	O
these	O
appear	O
noncircular	O
due	O
to	O
the	O
different	O
scales	O
on	O
the	O
horizontal	O
and	O
vertical	O
axes	O
.	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
in	O
fact	O
,	O
this	O
model	O
deﬁnes	O
not	O
only	O
a	O
conditional	B
expectation	I
but	O
also	O
a	O
full	O
conditional	B
distribution	O
given	O
by	O
p	O
(	O
t|x	O
)	O
=	O
p	O
(	O
t	O
,	O
x	O
)	O
=	O
(	O
cid:6	O
)	O
p	O
(	O
t	O
,	O
x	O
)	O
dt	O
(	O
cid:2	O
)	O
(	O
cid:6	O
)	O
(	O
cid:2	O
)	O
n	O
f	O
(	O
x	O
−	O
xn	O
,	O
t	O
−	O
tn	O
)	O
f	O
(	O
x	O
−	O
xm	O
,	O
t	O
−	O
tm	O
)	O
dt	O
(	O
6.48	O
)	O
exercise	O
6.18	O
m	O
from	O
which	O
other	O
expectations	O
can	O
be	O
evaluated	O
.	O
as	O
an	O
illustration	O
we	O
consider	O
the	O
case	O
of	O
a	O
single	O
input	O
variable	O
x	O
in	O
which	O
f	O
(	O
x	O
,	O
t	O
)	O
is	O
given	O
by	O
a	O
zero-mean	O
isotropic	B
gaussian	O
over	O
the	O
variable	O
z	O
=	O
(	O
x	O
,	O
t	O
)	O
with	O
variance	B
σ2	O
.	O
the	O
corresponding	O
conditional	B
distribution	O
(	O
6.48	O
)	O
is	O
given	O
by	O
a	O
gaus-	O
sian	O
mixture	B
,	O
and	O
is	O
shown	O
,	O
together	O
with	O
the	O
conditional	B
mean	O
,	O
for	O
the	O
sinusoidal	O
synthetic	O
data	O
set	O
in	O
figure	O
6.3.	O
an	O
obvious	O
extension	O
of	O
this	O
model	O
is	O
to	O
allow	O
for	O
more	O
ﬂexible	O
forms	O
of	O
gaus-	O
sian	O
components	O
,	O
for	O
instance	O
having	O
different	O
variance	B
parameters	O
for	O
the	O
input	O
and	O
target	O
variables	O
.	O
more	O
generally	O
,	O
we	O
could	O
model	O
the	O
joint	O
distribution	O
p	O
(	O
t	O
,	O
x	O
)	O
using	O
a	O
gaussian	O
mixture	B
model	I
,	O
trained	O
using	O
techniques	O
discussed	O
in	O
chapter	O
9	O
(	O
ghahra-	O
mani	O
and	O
jordan	O
,	O
1994	O
)	O
,	O
and	O
then	O
ﬁnd	O
the	O
corresponding	O
conditional	B
distribution	O
p	O
(	O
t|x	O
)	O
.	O
in	O
this	O
latter	O
case	O
we	O
no	O
longer	O
have	O
a	O
representation	O
in	O
terms	O
of	O
kernel	O
func-	O
tions	O
evaluated	O
at	O
the	O
training	B
set	I
data	O
points	O
.	O
however	O
,	O
the	O
number	O
of	O
components	O
in	O
the	O
mixture	B
model	I
can	O
be	O
smaller	O
than	O
the	O
number	O
of	O
training	B
set	I
points	O
,	O
resulting	O
in	O
a	O
model	O
that	O
is	O
faster	O
to	O
evaluate	O
for	O
test	O
data	O
points	O
.	O
we	O
have	O
thereby	O
accepted	O
an	O
increased	O
computational	O
cost	O
during	O
the	O
training	B
phase	O
in	O
order	O
to	O
have	O
a	O
model	O
that	O
is	O
faster	O
at	O
making	O
predictions	O
.	O
6.4.	O
gaussian	O
processes	O
in	O
section	O
6.1	O
,	O
we	O
introduced	O
kernels	O
by	O
applying	O
the	O
concept	O
of	O
duality	O
to	O
a	O
non-	O
probabilistic	O
model	O
for	B
regression	I
.	O
here	O
we	O
extend	O
the	O
role	O
of	O
kernels	O
to	O
probabilis-	O
304	O
6.	O
kernel	O
methods	O
tic	O
discriminative	O
models	O
,	O
leading	O
to	O
the	O
framework	O
of	O
gaussian	O
processes	O
.	O
we	O
shall	O
thereby	O
see	O
how	O
kernels	O
arise	O
naturally	O
in	O
a	O
bayesian	O
setting	O
.	O
in	O
chapter	O
3	O
,	O
we	O
considered	O
linear	B
regression	I
models	O
of	O
the	O
form	O
y	O
(	O
x	O
,	O
w	O
)	O
=	O
wtφ	O
(	O
x	O
)	O
in	O
which	O
w	O
is	O
a	O
vector	O
of	O
parameters	O
and	O
φ	O
(	O
x	O
)	O
is	O
a	O
vector	O
of	O
ﬁxed	O
nonlinear	O
basis	O
functions	O
that	O
depend	O
on	O
the	O
input	O
vector	O
x.	O
we	O
showed	O
that	O
a	O
prior	B
distribution	O
over	O
w	O
induced	O
a	O
corresponding	O
prior	B
distribution	O
over	O
functions	O
y	O
(	O
x	O
,	O
w	O
)	O
.	O
given	O
a	O
training	B
data	O
set	O
,	O
we	O
then	O
evaluated	O
the	O
posterior	O
distribution	O
over	O
w	O
and	O
thereby	O
obtained	O
the	O
corresponding	O
posterior	O
distribution	O
over	O
regression	B
functions	O
,	O
which	O
in	O
turn	O
(	O
with	O
the	O
addition	O
of	O
noise	O
)	O
implies	O
a	O
predictive	B
distribution	I
p	O
(	O
t|x	O
)	O
for	O
new	O
input	O
vectors	O
x.	O
in	O
the	O
gaussian	O
process	O
viewpoint	O
,	O
we	O
dispense	O
with	O
the	O
parametric	O
model	O
and	O
instead	O
deﬁne	O
a	O
prior	B
probability	O
distribution	O
over	O
functions	O
directly	O
.	O
at	O
ﬁrst	O
sight	O
,	O
it	O
might	O
seem	O
difﬁcult	O
to	O
work	O
with	O
a	O
distribution	O
over	O
the	O
uncountably	O
inﬁnite	O
space	O
of	O
functions	O
.	O
however	O
,	O
as	O
we	O
shall	O
see	O
,	O
for	O
a	O
ﬁnite	O
training	O
set	O
we	O
only	O
need	O
to	O
consider	O
the	O
values	O
of	O
the	O
function	O
at	O
the	O
discrete	O
set	O
of	O
input	O
values	O
xn	O
corresponding	O
to	O
the	O
training	B
set	I
and	O
test	B
set	I
data	O
points	O
,	O
and	O
so	O
in	O
practice	O
we	O
can	O
work	O
in	O
a	O
ﬁnite	O
space	O
.	O
models	O
equivalent	O
to	O
gaussian	O
processes	O
have	O
been	O
widely	O
studied	O
in	O
many	O
dif-	O
ferent	O
ﬁelds	O
.	O
for	O
instance	O
,	O
in	O
the	O
geostatistics	O
literature	O
gaussian	O
process	O
regression	B
is	O
known	O
as	O
kriging	B
(	O
cressie	O
,	O
1993	O
)	O
.	O
similarly	O
,	O
arma	O
(	O
autoregressive	O
moving	O
aver-	O
age	O
)	O
models	O
,	O
kalman	O
ﬁlters	O
,	O
and	O
radial	B
basis	I
function	I
networks	O
can	O
all	O
be	O
viewed	O
as	O
forms	O
of	O
gaussian	O
process	O
models	O
.	O
reviews	O
of	O
gaussian	O
processes	O
from	O
a	O
machine	O
learning	O
perspective	O
can	O
be	O
found	O
in	O
mackay	O
(	O
1998	O
)	O
,	O
williams	O
(	O
1999	O
)	O
,	O
and	O
mackay	O
(	O
2003	O
)	O
,	O
and	O
a	O
comparison	O
of	O
gaussian	O
process	O
models	O
with	O
alternative	O
approaches	O
is	O
given	O
in	O
rasmussen	O
(	O
1996	O
)	O
.	O
see	O
also	O
rasmussen	O
and	O
williams	O
(	O
2006	O
)	O
for	O
a	O
recent	O
textbook	O
on	O
gaussian	O
processes	O
.	O
6.4.1	O
linear	B
regression	I
revisited	O
in	O
order	O
to	O
motivate	O
the	O
gaussian	O
process	O
viewpoint	O
,	O
let	O
us	O
return	O
to	O
the	O
linear	B
regression	I
example	O
and	O
re-derive	O
the	O
predictive	B
distribution	I
by	O
working	O
in	O
terms	O
of	O
distributions	O
over	O
functions	O
y	O
(	O
x	O
,	O
w	O
)	O
.	O
this	O
will	O
provide	O
a	O
speciﬁc	O
example	O
of	O
a	O
gaussian	O
process	O
.	O
consider	O
a	O
model	O
deﬁned	O
in	O
terms	O
of	O
a	O
linear	O
combination	O
of	O
m	O
ﬁxed	O
basis	O
functions	O
given	O
by	O
the	O
elements	O
of	O
the	O
vector	O
φ	O
(	O
x	O
)	O
so	O
that	O
y	O
(	O
x	O
)	O
=	O
wtφ	O
(	O
x	O
)	O
(	O
6.49	O
)	O
where	O
x	O
is	O
the	O
input	O
vector	O
and	O
w	O
is	O
the	O
m-dimensional	O
weight	B
vector	I
.	O
now	O
consider	O
a	O
prior	B
distribution	O
over	O
w	O
given	O
by	O
an	O
isotropic	B
gaussian	O
of	O
the	O
form	O
p	O
(	O
w	O
)	O
=	O
n	O
(	O
w|0	O
,	O
α	O
−1i	O
)	O
(	O
6.50	O
)	O
governed	O
by	O
the	O
hyperparameter	B
α	O
,	O
which	O
represents	O
the	O
precision	O
(	O
inverse	B
variance	O
)	O
of	O
the	O
distribution	O
.	O
for	O
any	O
given	O
value	O
of	O
w	O
,	O
the	O
deﬁnition	O
(	O
6.49	O
)	O
deﬁnes	O
a	O
partic-	O
ular	O
function	O
of	O
x.	O
the	O
probability	B
distribution	O
over	O
w	O
deﬁned	O
by	O
(	O
6.50	O
)	O
therefore	O
induces	O
a	O
probability	B
distribution	O
over	O
functions	O
y	O
(	O
x	O
)	O
.	O
in	O
practice	O
,	O
we	O
wish	O
to	O
eval-	O
uate	O
this	O
function	O
at	O
speciﬁc	O
values	O
of	O
x	O
,	O
for	O
example	O
at	O
the	O
training	B
data	O
points	O
6.4.	O
gaussian	O
processes	O
305	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
.	O
we	O
are	O
therefore	O
interested	O
in	O
the	O
joint	O
distribution	O
of	O
the	O
function	O
val-	O
ues	O
y	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
y	O
(	O
xn	O
)	O
,	O
which	O
we	O
denote	O
by	O
the	O
vector	O
y	O
with	O
elements	O
yn	O
=	O
y	O
(	O
xn	O
)	O
for	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
from	O
(	O
6.49	O
)	O
,	O
this	O
vector	O
is	O
given	O
by	O
y	O
=	O
φw	O
(	O
6.51	O
)	O
where	O
φ	O
is	O
the	O
design	B
matrix	I
with	O
elements	O
φnk	O
=	O
φk	O
(	O
xn	O
)	O
.	O
we	O
can	O
ﬁnd	O
the	O
proba-	O
bility	O
distribution	O
of	O
y	O
as	O
follows	O
.	O
first	O
of	O
all	O
we	O
note	O
that	O
y	O
is	O
a	O
linear	O
combination	O
of	O
gaussian	O
distributed	O
variables	O
given	O
by	O
the	O
elements	O
of	O
w	O
and	O
hence	O
is	O
itself	O
gaus-	O
sian	O
.	O
we	O
therefore	O
need	O
only	O
to	O
ﬁnd	O
its	O
mean	B
and	O
covariance	B
,	O
which	O
are	O
given	O
from	O
(	O
6.50	O
)	O
by	O
e	O
[	O
y	O
]	O
=	O
φe	O
[	O
w	O
]	O
=	O
0	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
exercise	O
2.31	O
(	O
6.52	O
)	O
(	O
6.53	O
)	O
(	O
6.54	O
)	O
cov	O
[	O
y	O
]	O
=	O
e	O
yyt	O
=	O
φe	O
wwt	O
φt	O
=	O
1	O
α	O
φφt	O
=	O
k	O
where	O
k	O
is	O
the	O
gram	O
matrix	O
with	O
elements	O
knm	O
=	O
k	O
(	O
xn	O
,	O
xm	O
)	O
=	O
and	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
is	O
the	O
kernel	B
function	I
.	O
1	O
α	O
φ	O
(	O
xn	O
)	O
tφ	O
(	O
xm	O
)	O
this	O
model	O
provides	O
us	O
with	O
a	O
particular	O
example	O
of	O
a	O
gaussian	O
process	O
.	O
in	O
gen-	O
eral	O
,	O
a	O
gaussian	O
process	O
is	O
deﬁned	O
as	O
a	O
probability	B
distribution	O
over	O
functions	O
y	O
(	O
x	O
)	O
such	O
that	O
the	O
set	O
of	O
values	O
of	O
y	O
(	O
x	O
)	O
evaluated	O
at	O
an	O
arbitrary	O
set	O
of	O
points	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
jointly	O
have	O
a	O
gaussian	O
distribution	O
.	O
in	O
cases	O
where	O
the	O
input	O
vector	O
x	O
is	O
two	O
di-	O
mensional	O
,	O
this	O
may	O
also	O
be	O
known	O
as	O
a	O
gaussian	O
random	O
ﬁeld	O
.	O
more	O
generally	O
,	O
a	O
stochastic	B
process	I
y	O
(	O
x	O
)	O
is	O
speciﬁed	O
by	O
giving	O
the	O
joint	O
probability	B
distribution	O
for	O
any	O
ﬁnite	O
set	O
of	O
values	O
y	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
y	O
(	O
xn	O
)	O
in	O
a	O
consistent	B
manner	O
.	O
a	O
key	O
point	O
about	O
gaussian	O
stochastic	B
processes	O
is	O
that	O
the	O
joint	O
distribution	O
over	O
n	O
variables	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
is	O
speciﬁed	O
completely	O
by	O
the	O
second-order	O
statistics	O
,	O
namely	O
the	O
mean	B
and	O
the	O
covariance	B
.	O
in	O
most	O
applications	O
,	O
we	O
will	O
not	O
have	O
any	O
prior	B
knowledge	O
about	O
the	O
mean	B
of	O
y	O
(	O
x	O
)	O
and	O
so	O
by	O
symmetry	O
we	O
take	O
it	O
to	O
be	O
zero	O
.	O
this	O
is	O
equivalent	O
to	O
choosing	O
the	O
mean	B
of	O
the	O
prior	B
over	O
weight	O
values	O
p	O
(	O
w|α	O
)	O
to	O
be	O
zero	O
in	O
the	O
basis	B
function	I
viewpoint	O
.	O
the	O
speciﬁcation	O
of	O
the	O
gaussian	O
process	O
is	O
then	O
completed	O
by	O
giving	O
the	O
covariance	B
of	O
y	O
(	O
x	O
)	O
evaluated	O
at	O
any	O
two	O
values	O
of	O
x	O
,	O
which	O
is	O
given	O
by	O
the	O
kernel	B
function	I
e	O
[	O
y	O
(	O
xn	O
)	O
y	O
(	O
xm	O
)	O
]	O
=	O
k	O
(	O
xn	O
,	O
xm	O
)	O
.	O
(	O
6.55	O
)	O
for	O
the	O
speciﬁc	O
case	O
of	O
a	O
gaussian	O
process	O
deﬁned	O
by	O
the	O
linear	B
regression	I
model	O
(	O
6.49	O
)	O
with	O
a	O
weight	O
prior	O
(	O
6.50	O
)	O
,	O
the	O
kernel	B
function	I
is	O
given	O
by	O
(	O
6.54	O
)	O
.	O
we	O
can	O
also	O
deﬁne	O
the	O
kernel	B
function	I
directly	O
,	O
rather	O
than	O
indirectly	O
through	O
a	O
choice	O
of	O
basis	B
function	I
.	O
figure	O
6.4	O
shows	O
samples	O
of	O
functions	O
drawn	O
from	O
gaus-	O
sian	O
processes	O
for	O
two	O
different	O
choices	O
of	O
kernel	B
function	I
.	O
the	O
ﬁrst	O
of	O
these	O
is	O
a	O
‘	O
gaussian	O
’	O
kernel	O
of	O
the	O
form	O
(	O
6.23	O
)	O
,	O
and	O
the	O
second	O
is	O
the	O
exponential	O
kernel	O
given	O
by	O
(	O
6.56	O
)	O
which	O
corresponds	O
to	O
the	O
ornstein-uhlenbeck	O
process	O
originally	O
introduced	O
by	O
uh-	O
lenbeck	O
and	O
ornstein	O
(	O
1930	O
)	O
to	O
describe	O
brownian	O
motion	O
.	O
(	O
cid:4	O
)	O
)	O
=	O
exp	O
(	O
−θ	O
|x	O
−	O
x	O
(	O
cid:4	O
)	O
|	O
)	O
k	O
(	O
x	O
,	O
x	O
306	O
6.	O
kernel	O
methods	O
figure	O
6.4	O
samples	O
from	O
gaus-	O
sian	O
processes	O
for	O
a	O
‘	O
gaussian	O
’	O
ker-	O
nel	O
(	O
left	O
)	O
and	O
an	O
exponential	O
kernel	O
(	O
right	O
)	O
.	O
3	O
1.5	O
0	O
−1.5	O
−3	O
−1	O
−0.5	O
0	O
0.5	O
1	O
3	O
1.5	O
0	O
−1.5	O
−3	O
−1	O
−0.5	O
0	O
0.5	O
1	O
6.4.2	O
gaussian	O
processes	O
for	B
regression	I
in	O
order	O
to	O
apply	O
gaussian	O
process	O
models	O
to	O
the	O
problem	O
of	O
regression	B
,	O
we	O
need	O
to	O
take	O
account	O
of	O
the	O
noise	O
on	O
the	O
observed	O
target	O
values	O
,	O
which	O
are	O
given	O
by	O
tn	O
=	O
yn	O
+	O
n	O
(	O
6.57	O
)	O
where	O
yn	O
=	O
y	O
(	O
xn	O
)	O
,	O
and	O
n	O
is	O
a	O
random	O
noise	O
variable	O
whose	O
value	O
is	O
chosen	O
inde-	O
pendently	O
for	O
each	O
observation	O
n.	O
here	O
we	O
shall	O
consider	O
noise	O
processes	O
that	O
have	O
a	O
gaussian	O
distribution	O
,	O
so	O
that	O
p	O
(	O
tn|yn	O
)	O
=	O
n	O
(	O
tn|yn	O
,	O
β	O
−1	O
)	O
(	O
6.58	O
)	O
where	O
β	O
is	O
a	O
hyperparameter	B
representing	O
the	O
precision	O
of	O
the	O
noise	O
.	O
because	O
the	O
noise	O
is	O
independent	B
for	O
each	O
data	O
point	O
,	O
the	O
joint	O
distribution	O
of	O
the	O
target	O
values	O
t	O
=	O
(	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
)	O
t	O
conditioned	O
on	O
the	O
values	O
of	O
y	O
=	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
)	O
t	O
is	O
given	O
by	O
an	O
isotropic	B
gaussian	O
of	O
the	O
form	O
p	O
(	O
t|y	O
)	O
=	O
n	O
(	O
t|y	O
,	O
β	O
−1in	O
)	O
(	O
6.59	O
)	O
where	O
in	O
denotes	O
the	O
n	O
×	O
n	O
unit	O
matrix	O
.	O
from	O
the	O
deﬁnition	O
of	O
a	O
gaussian	O
process	O
,	O
the	O
marginal	B
distribution	O
p	O
(	O
y	O
)	O
is	O
given	O
by	O
a	O
gaussian	O
whose	O
mean	B
is	O
zero	O
and	O
whose	O
covariance	B
is	O
deﬁned	O
by	O
a	O
gram	O
matrix	O
k	O
so	O
that	O
p	O
(	O
y	O
)	O
=	O
n	O
(	O
y|0	O
,	O
k	O
)	O
.	O
(	O
6.60	O
)	O
the	O
kernel	B
function	I
that	O
determines	O
k	O
is	O
typically	O
chosen	O
to	O
express	O
the	O
property	O
that	O
,	O
for	O
points	O
xn	O
and	O
xm	O
that	O
are	O
similar	O
,	O
the	O
corresponding	O
values	O
y	O
(	O
xn	O
)	O
and	O
y	O
(	O
xm	O
)	O
will	O
be	O
more	O
strongly	O
correlated	O
than	O
for	O
dissimilar	O
points	O
.	O
here	O
the	O
notion	O
of	O
similarity	O
will	O
depend	O
on	O
the	O
application	O
.	O
in	O
order	O
to	O
ﬁnd	O
the	O
marginal	B
distribution	O
p	O
(	O
t	O
)	O
,	O
conditioned	O
on	O
the	O
input	O
values	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
we	O
need	O
to	O
integrate	O
over	O
y.	O
this	O
can	O
be	O
done	O
by	O
making	O
use	O
of	O
the	O
results	O
from	O
section	O
2.3.3	O
for	O
the	O
linear-gaussian	O
model	O
.	O
using	O
(	O
2.115	O
)	O
,	O
we	O
see	O
that	O
the	O
marginal	B
distribution	O
of	O
t	O
is	O
given	O
by	O
(	O
cid:6	O
)	O
p	O
(	O
t	O
)	O
=	O
p	O
(	O
t|y	O
)	O
p	O
(	O
y	O
)	O
dy	O
=	O
n	O
(	O
t|0	O
,	O
c	O
)	O
(	O
6.61	O
)	O
6.4.	O
gaussian	O
processes	O
307	O
where	O
the	O
covariance	B
matrix	I
c	O
has	O
elements	O
c	O
(	O
xn	O
,	O
xm	O
)	O
=	O
k	O
(	O
xn	O
,	O
xm	O
)	O
+	O
β	O
−1δnm	O
.	O
(	O
6.62	O
)	O
this	O
result	O
reﬂects	O
the	O
fact	O
that	O
the	O
two	O
gaussian	O
sources	O
of	O
randomness	O
,	O
namely	O
that	O
associated	O
with	O
y	O
(	O
x	O
)	O
and	O
that	O
associated	O
with	O
	O
,	O
are	O
independent	B
and	O
so	O
their	O
covariances	O
simply	O
add	O
.	O
one	O
widely	O
used	O
kernel	B
function	I
for	O
gaussian	O
process	O
regression	B
is	O
given	O
by	O
the	O
exponential	O
of	O
a	O
quadratic	O
form	O
,	O
with	O
the	O
addition	O
of	O
constant	O
and	O
linear	O
terms	O
to	O
give	O
k	O
(	O
xn	O
,	O
xm	O
)	O
=	O
θ0	O
exp	O
+	O
θ2	O
+	O
θ3xt	O
nxm	O
.	O
(	O
6.63	O
)	O
(	O
cid:5	O
)	O
xn	O
−	O
xm	O
(	O
cid:5	O
)	O
2	O
−	O
θ1	O
2	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
note	O
that	O
the	O
term	O
involving	O
θ3	O
corresponds	O
to	O
a	O
parametric	O
model	O
that	O
is	O
a	O
linear	O
function	O
of	O
the	O
input	O
variables	O
.	O
samples	O
from	O
this	O
prior	B
are	O
plotted	O
for	O
various	O
values	O
of	O
the	O
parameters	O
θ0	O
,	O
.	O
.	O
.	O
,	O
θ3	O
in	O
figure	O
6.5	O
,	O
and	O
figure	O
6.6	O
shows	O
a	O
set	O
of	O
points	O
sam-	O
pled	O
from	O
the	O
joint	O
distribution	O
(	O
6.60	O
)	O
along	O
with	O
the	O
corresponding	O
values	O
deﬁned	O
by	O
(	O
6.61	O
)	O
.	O
so	O
far	O
,	O
we	O
have	O
used	O
the	O
gaussian	O
process	O
viewpoint	O
to	O
build	O
a	O
model	O
of	O
the	O
joint	O
distribution	O
over	O
sets	O
of	O
data	O
points	O
.	O
our	O
goal	O
in	O
regression	B
,	O
however	O
,	O
is	O
to	O
make	O
predictions	O
of	O
the	O
target	O
variables	O
for	O
new	O
inputs	O
,	O
given	O
a	O
set	O
of	O
training	B
data	O
.	O
let	O
us	O
suppose	O
that	O
tn	O
=	O
(	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
)	O
t	O
,	O
corresponding	O
to	O
input	O
values	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
comprise	O
the	O
observed	O
training	O
set	O
,	O
and	O
our	O
goal	O
is	O
to	O
predict	O
the	O
target	O
variable	O
tn	O
+1	O
for	O
a	O
new	O
input	O
vector	O
xn	O
+1	O
.	O
this	O
requires	O
that	O
we	O
evaluate	O
the	O
predictive	O
distri-	O
bution	O
p	O
(	O
tn	O
+1|tn	O
)	O
.	O
note	O
that	O
this	O
distribution	O
is	O
conditioned	O
also	O
on	O
the	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
and	O
xn	O
+1	O
.	O
however	O
,	O
to	O
keep	O
the	O
notation	O
simple	O
we	O
will	O
not	O
show	O
these	O
conditioning	O
variables	O
explicitly	O
.	O
to	O
ﬁnd	O
the	O
conditional	B
distribution	O
p	O
(	O
tn	O
+1|t	O
)	O
,	O
we	O
begin	O
by	O
writing	O
down	O
the	O
joint	O
distribution	O
p	O
(	O
tn	O
+1	O
)	O
,	O
where	O
tn	O
+1	O
denotes	O
the	O
vector	O
(	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
,	O
tn	O
+1	O
)	O
t.	O
we	O
then	O
apply	O
the	O
results	O
from	O
section	O
2.3.1	O
to	O
obtain	O
the	O
required	O
conditional	B
distribu-	O
tion	O
,	O
as	O
illustrated	O
in	O
figure	O
6.7.	O
from	O
(	O
6.61	O
)	O
,	O
the	O
joint	O
distribution	O
over	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
+1	O
will	O
be	O
given	O
by	O
p	O
(	O
tn	O
+1	O
)	O
=	O
n	O
(	O
tn	O
+1|0	O
,	O
cn	O
+1	O
)	O
(	O
6.64	O
)	O
where	O
cn	O
+1	O
is	O
an	O
(	O
n	O
+	O
1	O
)	O
×	O
(	O
n	O
+	O
1	O
)	O
covariance	B
matrix	I
with	O
elements	O
given	O
by	O
(	O
6.62	O
)	O
.	O
because	O
this	O
joint	O
distribution	O
is	O
gaussian	O
,	O
we	O
can	O
apply	O
the	O
results	O
from	O
section	O
2.3.1	O
to	O
ﬁnd	O
the	O
conditional	B
gaussian	O
distribution	O
.	O
to	O
do	O
this	O
,	O
we	O
partition	O
the	O
covariance	B
matrix	I
as	O
follows	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
cn	O
+1	O
=	O
(	O
6.65	O
)	O
where	O
cn	O
is	O
the	O
n	O
×	O
n	O
covariance	B
matrix	I
with	O
elements	O
given	O
by	O
(	O
6.62	O
)	O
for	O
n	O
,	O
m	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
the	O
vector	O
k	O
has	O
elements	O
k	O
(	O
xn	O
,	O
xn	O
+1	O
)	O
for	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
and	O
the	O
scalar	O
cn	O
k	O
kt	O
c	O
308	O
6.	O
kernel	O
methods	O
(	O
1.00	O
,	O
4.00	O
,	O
0.00	O
,	O
0.00	O
)	O
(	O
9.00	O
,	O
4.00	O
,	O
0.00	O
,	O
0.00	O
)	O
(	O
1.00	O
,	O
64.00	O
,	O
0.00	O
,	O
0.00	O
)	O
3	O
1.5	O
0	O
−1.5	O
−3	O
−1	O
3	O
1.5	O
0	O
−1.5	O
−3	O
−1	O
9	O
4.5	O
0	O
−4.5	O
0	O
−0.5	O
0.5	O
(	O
1.00	O
,	O
0.25	O
,	O
0.00	O
,	O
0.00	O
)	O
1	O
−9	O
−1	O
9	O
4.5	O
0	O
−4.5	O
−9	O
−1	O
−0.5	O
0	O
0.5	O
1	O
−0.5	O
0	O
0.5	O
1	O
(	O
1.00	O
,	O
4.00	O
,	O
10.00	O
,	O
0.00	O
)	O
−0.5	O
0	O
0.5	O
1	O
3	O
1.5	O
0	O
−1.5	O
−3	O
−1	O
4	O
2	O
0	O
−2	O
−4	O
−1	O
0	O
−0.5	O
0.5	O
(	O
1.00	O
,	O
4.00	O
,	O
0.00	O
,	O
5.00	O
)	O
1	O
−0.5	O
0	O
0.5	O
1	O
figure	O
6.5	O
samples	O
from	O
a	O
gaussian	O
process	O
prior	B
deﬁned	O
by	O
the	O
covariance	B
function	O
(	O
6.63	O
)	O
.	O
the	O
title	O
above	O
each	O
plot	O
denotes	O
(	O
θ0	O
,	O
θ1	O
,	O
θ2	O
,	O
θ3	O
)	O
.	O
−1	O
.	O
using	O
the	O
results	O
(	O
2.81	O
)	O
and	O
(	O
2.82	O
)	O
,	O
we	O
see	O
that	O
the	O
con-	O
c	O
=	O
k	O
(	O
xn	O
+1	O
,	O
xn	O
+1	O
)	O
+	O
β	O
ditional	O
distribution	O
p	O
(	O
tn	O
+1|t	O
)	O
is	O
a	O
gaussian	O
distribution	O
with	O
mean	B
and	O
covariance	B
given	O
by	O
−1	O
m	O
(	O
xn	O
+1	O
)	O
=	O
ktc	O
n	O
t	O
σ2	O
(	O
xn	O
+1	O
)	O
=	O
c	O
−	O
ktc	O
−1	O
n	O
k.	O
(	O
6.66	O
)	O
(	O
6.67	O
)	O
these	O
are	O
the	O
key	O
results	O
that	O
deﬁne	O
gaussian	O
process	O
regression	B
.	O
because	O
the	O
vector	O
k	O
is	O
a	O
function	O
of	O
the	O
test	O
point	O
input	O
value	O
xn	O
+1	O
,	O
we	O
see	O
that	O
the	O
predictive	O
distribu-	O
tion	O
is	O
a	O
gaussian	O
whose	O
mean	B
and	O
variance	B
both	O
depend	O
on	O
xn	O
+1	O
.	O
an	O
example	O
of	O
gaussian	O
process	O
regression	B
is	O
shown	O
in	O
figure	O
6.8.	O
the	O
only	O
restriction	O
on	O
the	O
kernel	B
function	I
is	O
that	O
the	O
covariance	B
matrix	I
given	O
by	O
(	O
6.62	O
)	O
must	O
be	O
positive	B
deﬁnite	I
.	O
if	O
λi	O
is	O
an	O
eigenvalue	O
of	O
k	O
,	O
then	O
the	O
corresponding	O
−1	O
.	O
it	O
is	O
therefore	O
sufﬁcient	O
that	O
the	O
kernel	O
matrix	O
eigenvalue	O
of	O
c	O
will	O
be	O
λi	O
+	O
β	O
k	O
(	O
xn	O
,	O
xm	O
)	O
be	O
positive	O
semideﬁnite	O
for	O
any	O
pair	O
of	O
points	O
xn	O
and	O
xm	O
,	O
so	O
that	O
λi	O
(	O
cid:2	O
)	O
0	O
,	O
because	O
any	O
eigenvalue	O
λi	O
that	O
is	O
zero	O
will	O
still	O
give	O
rise	O
to	O
a	O
positive	O
eigenvalue	O
for	O
c	O
because	O
β	O
>	O
0.	O
this	O
is	O
the	O
same	O
restriction	O
on	O
the	O
kernel	B
function	I
discussed	O
earlier	O
,	O
and	O
so	O
we	O
can	O
again	O
exploit	O
all	O
of	O
the	O
techniques	O
in	O
section	O
6.2	O
to	O
construct	O
figure	O
6.6	O
illustration	O
of	O
the	O
sampling	O
of	O
data	O
points	O
{	O
tn	O
}	O
from	O
a	O
gaussian	O
process	O
.	O
the	O
blue	O
curve	O
shows	O
a	O
sample	O
func-	O
tion	O
from	O
the	O
gaussian	O
process	O
prior	B
over	O
functions	O
,	O
and	O
the	O
red	O
points	O
show	O
the	O
values	O
of	O
yn	O
obtained	O
by	O
evaluating	O
the	O
function	O
at	O
a	O
set	O
of	O
in-	O
put	O
values	O
{	O
xn	O
}	O
.	O
the	O
correspond-	O
ing	O
values	O
of	O
{	O
tn	O
}	O
,	O
shown	O
in	O
green	O
,	O
are	O
obtained	O
by	O
adding	O
independent	B
gaussian	O
noise	O
to	O
each	O
of	O
the	O
{	O
yn	O
}	O
.	O
t	O
3	O
0	O
−3	O
−1	O
suitable	O
kernels	O
.	O
6.4.	O
gaussian	O
processes	O
309	O
0	O
x	O
1	O
note	O
that	O
the	O
mean	B
(	O
6.66	O
)	O
of	O
the	O
predictive	B
distribution	I
can	O
be	O
written	O
,	O
as	O
a	O
func-	O
tion	O
of	O
xn	O
+1	O
,	O
in	O
the	O
form	O
n	O
(	O
cid:2	O
)	O
m	O
(	O
xn	O
+1	O
)	O
=	O
ank	O
(	O
xn	O
,	O
xn	O
+1	O
)	O
(	O
6.68	O
)	O
exercise	O
6.21	O
n=1	O
−1	O
n	O
t.	O
thus	O
,	O
if	O
the	O
kernel	B
function	I
k	O
(	O
xn	O
,	O
xm	O
)	O
where	O
an	O
is	O
the	O
nth	O
component	O
of	O
c	O
depends	O
only	O
on	O
the	O
distance	O
(	O
cid:5	O
)	O
xn	O
−	O
xm	O
(	O
cid:5	O
)	O
,	O
then	O
we	O
obtain	O
an	O
expansion	O
in	O
radial	O
basis	O
functions	O
.	O
the	O
results	O
(	O
6.66	O
)	O
and	O
(	O
6.67	O
)	O
deﬁne	O
the	O
predictive	B
distribution	I
for	O
gaussian	O
pro-	O
cess	O
regression	B
with	O
an	O
arbitrary	O
kernel	B
function	I
k	O
(	O
xn	O
,	O
xm	O
)	O
.	O
in	O
the	O
particular	O
case	O
in	O
which	O
the	O
kernel	B
function	I
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
is	O
deﬁned	O
in	O
terms	O
of	O
a	O
ﬁnite	O
set	O
of	O
basis	O
functions	O
,	O
we	O
can	O
derive	O
the	O
results	O
obtained	O
previously	O
in	O
section	O
3.3.2	O
for	O
linear	O
regression	B
starting	O
from	O
the	O
gaussian	O
process	O
viewpoint	O
.	O
for	O
such	O
models	O
,	O
we	O
can	O
therefore	O
obtain	O
the	O
predictive	B
distribution	I
either	O
by	O
taking	O
a	O
parameter	O
space	O
viewpoint	O
and	O
using	O
the	O
linear	B
regression	I
result	O
or	O
by	O
taking	O
a	O
function	O
space	O
viewpoint	O
and	O
using	O
the	O
gaussian	O
process	O
result	O
.	O
the	O
central	O
computational	O
operation	O
in	O
using	O
gaussian	O
processes	O
will	O
involve	O
the	O
inversion	O
of	O
a	O
matrix	O
of	O
size	O
n	O
×	O
n	O
,	O
for	O
which	O
standard	O
methods	O
require	O
o	O
(	O
n	O
3	O
)	O
computations	O
.	O
by	O
contrast	O
,	O
in	O
the	O
basis	B
function	I
model	O
we	O
have	O
to	O
invert	O
a	O
matrix	O
sn	O
of	O
size	O
m	O
×	O
m	O
,	O
which	O
has	O
o	O
(	O
m	O
3	O
)	O
computational	O
complexity	O
.	O
note	O
that	O
for	O
both	O
viewpoints	O
,	O
the	O
matrix	O
inversion	O
must	O
be	O
performed	O
once	O
for	O
the	O
given	O
training	B
set	I
.	O
for	O
each	O
new	O
test	O
point	O
,	O
both	O
methods	O
require	O
a	O
vector-matrix	O
multiply	O
,	O
which	O
has	O
cost	O
o	O
(	O
n	O
2	O
)	O
in	O
the	O
gaussian	O
process	O
case	O
and	O
o	O
(	O
m	O
2	O
)	O
for	O
the	O
linear	O
basis	O
func-	O
tion	O
model	O
.	O
if	O
the	O
number	O
m	O
of	O
basis	O
functions	O
is	O
smaller	O
than	O
the	O
number	O
n	O
of	O
data	O
points	O
,	O
it	O
will	O
be	O
computationally	O
more	O
efﬁcient	O
to	O
work	O
in	O
the	O
basis	B
function	I
310	O
6.	O
kernel	O
methods	O
figure	O
6.7	O
illustration	O
of	O
the	O
mechanism	O
of	O
gaussian	O
process	O
regression	B
for	O
the	O
case	O
of	O
one	O
training	B
point	O
and	O
one	O
test	O
point	O
,	O
in	O
which	O
the	O
red	O
el-	O
lipses	O
show	O
contours	O
of	O
the	O
joint	O
dis-	O
tribution	O
p	O
(	O
t1	O
,	O
t2	O
)	O
.	O
here	O
t1	O
is	O
the	O
training	B
data	O
point	O
,	O
and	O
condition-	O
ing	O
on	O
the	O
value	O
of	O
t1	O
,	O
correspond-	O
ing	O
to	O
the	O
vertical	O
blue	O
line	O
,	O
we	O
ob-	O
tain	O
p	O
(	O
t2|t1	O
)	O
shown	O
as	O
a	O
function	O
of	O
t2	O
by	O
the	O
green	O
curve	O
.	O
1	O
0	O
−1	O
t2	O
m	O
(	O
x2	O
)	O
t1	O
−1	O
0	O
1	O
framework	O
.	O
however	O
,	O
an	O
advantage	O
of	O
a	O
gaussian	O
processes	O
viewpoint	O
is	O
that	O
we	O
can	O
consider	O
covariance	B
functions	O
that	O
can	O
only	O
be	O
expressed	O
in	O
terms	O
of	O
an	O
inﬁnite	O
number	O
of	O
basis	O
functions	O
.	O
for	O
large	O
training	B
data	O
sets	O
,	O
however	O
,	O
the	O
direct	O
application	O
of	O
gaussian	O
process	O
methods	O
can	O
become	O
infeasible	O
,	O
and	O
so	O
a	O
range	O
of	O
approximation	O
schemes	O
have	O
been	O
developed	O
that	O
have	O
better	O
scaling	O
with	O
training	B
set	I
size	O
than	O
the	O
exact	O
approach	O
(	O
gibbs	O
,	O
1997	O
;	O
tresp	O
,	O
2001	O
;	O
smola	O
and	O
bartlett	O
,	O
2001	O
;	O
williams	O
and	O
seeger	O
,	O
2001	O
;	O
csat´o	O
and	O
opper	O
,	O
2002	O
;	O
seeger	O
et	O
al.	O
,	O
2003	O
)	O
.	O
practical	O
issues	O
in	O
the	O
application	O
of	O
gaussian	O
processes	O
are	O
discussed	O
in	O
bishop	O
and	O
nabney	O
(	O
2008	O
)	O
.	O
we	O
have	O
introduced	O
gaussian	O
process	O
regression	B
for	O
the	O
case	O
of	O
a	O
single	O
tar-	O
get	O
variable	O
.	O
the	O
extension	O
of	O
this	O
formalism	O
to	O
multiple	O
target	O
variables	O
,	O
known	O
as	O
co-kriging	O
(	O
cressie	O
,	O
1993	O
)	O
,	O
is	O
straightforward	O
.	O
various	O
other	O
extensions	O
of	O
gaus-	O
exercise	O
6.23	O
figure	O
6.8	O
illustration	O
of	O
gaussian	O
process	O
re-	O
gression	O
applied	O
to	O
the	O
sinusoidal	B
data	I
set	O
in	O
figure	O
a.6	O
in	O
which	O
the	O
three	O
right-most	O
data	O
points	O
have	O
been	O
omitted	O
.	O
the	O
green	O
curve	O
shows	O
the	O
sinusoidal	O
function	O
from	O
which	O
the	O
data	O
points	O
,	O
shown	O
in	O
blue	O
,	O
are	O
obtained	O
by	O
sampling	O
and	O
addition	O
of	O
gaussian	O
noise	O
.	O
the	O
red	O
line	O
shows	O
the	O
mean	B
of	O
the	O
gaussian	O
process	O
predictive	O
distri-	O
bution	O
,	O
and	O
the	O
shaded	O
region	O
cor-	O
responds	O
to	O
plus	O
and	O
minus	O
two	O
standard	O
deviations	O
.	O
notice	O
how	O
the	O
uncertainty	O
increases	O
in	O
the	O
re-	O
gion	O
to	O
the	O
right	O
of	O
the	O
data	O
points	O
.	O
1	O
0.5	O
0	O
−0.5	O
−1	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
6.4.	O
gaussian	O
processes	O
311	O
sian	O
process	O
regression	B
have	O
also	O
been	O
considered	O
,	O
for	O
purposes	O
such	O
as	O
modelling	O
the	O
distribution	O
over	O
low-dimensional	O
manifolds	O
for	O
unsupervised	O
learning	B
(	O
bishop	O
et	O
al.	O
,	O
1998a	O
)	O
and	O
the	O
solution	O
of	O
stochastic	B
differential	O
equations	O
(	O
graepel	O
,	O
2003	O
)	O
.	O
6.4.3	O
learning	B
the	O
hyperparameters	O
the	O
predictions	O
of	O
a	O
gaussian	O
process	O
model	O
will	O
depend	O
,	O
in	O
part	O
,	O
on	O
the	O
choice	O
of	O
covariance	B
function	O
.	O
in	O
practice	O
,	O
rather	O
than	O
ﬁxing	O
the	O
covariance	B
function	O
,	O
we	O
may	O
prefer	O
to	O
use	O
a	O
parametric	O
family	O
of	O
functions	O
and	O
then	O
infer	O
the	O
parameter	O
values	O
from	O
the	O
data	O
.	O
these	O
parameters	O
govern	O
such	O
things	O
as	O
the	O
length	O
scale	O
of	O
the	O
correlations	O
and	O
the	O
precision	O
of	O
the	O
noise	O
and	O
correspond	O
to	O
the	O
hyperparameters	O
in	O
a	O
standard	O
parametric	O
model	O
.	O
techniques	O
for	O
learning	O
the	O
hyperparameters	O
are	O
based	O
on	O
the	O
evaluation	O
of	O
the	O
likelihood	B
function	I
p	O
(	O
t|θ	O
)	O
where	O
θ	O
denotes	O
the	O
hyperparameters	O
of	O
the	O
gaussian	O
pro-	O
cess	O
model	O
.	O
the	O
simplest	O
approach	O
is	O
to	O
make	O
a	O
point	O
estimate	O
of	O
θ	O
by	O
maximizing	O
the	O
log	O
likelihood	O
function	O
.	O
because	O
θ	O
represents	O
a	O
set	O
of	O
hyperparameters	O
for	O
the	O
regression	B
problem	O
,	O
this	O
can	O
be	O
viewed	O
as	O
analogous	O
to	O
the	O
type	O
2	O
maximum	O
like-	O
lihood	O
procedure	O
for	O
linear	O
regression	B
models	O
.	O
maximization	O
of	O
the	O
log	O
likelihood	O
can	O
be	O
done	O
using	O
efﬁcient	O
gradient-based	O
optimization	O
algorithms	O
such	O
as	O
conjugate	B
gradients	O
(	O
fletcher	O
,	O
1987	O
;	O
nocedal	O
and	O
wright	O
,	O
1999	O
;	O
bishop	O
and	O
nabney	O
,	O
2008	O
)	O
.	O
the	O
log	O
likelihood	O
function	O
for	O
a	O
gaussian	O
process	O
regression	B
model	O
is	O
easily	O
evaluated	O
using	O
the	O
standard	O
form	O
for	O
a	O
multivariate	O
gaussian	O
distribution	O
,	O
giving	O
ln	O
p	O
(	O
t|θ	O
)	O
=	O
−1	O
2	O
ln|cn|	O
−	O
1	O
2	O
ttc	O
n	O
t	O
−	O
n	O
−1	O
2	O
ln	O
(	O
2π	O
)	O
.	O
(	O
6.69	O
)	O
section	O
3.5	O
for	O
nonlinear	O
optimization	O
,	O
we	O
also	O
need	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	O
func-	O
tion	O
with	O
respect	O
to	O
the	O
parameter	O
vector	O
θ.	O
we	O
shall	O
assume	O
that	O
evaluation	O
of	O
the	O
derivatives	O
of	O
cn	O
is	O
straightforward	O
,	O
as	O
would	O
be	O
the	O
case	O
for	O
the	O
covariance	B
func-	O
tions	O
considered	O
in	O
this	O
chapter	O
.	O
making	O
use	O
of	O
the	O
result	O
(	O
c.21	O
)	O
for	O
the	O
derivative	B
of	O
n	O
,	O
together	O
with	O
the	O
result	O
(	O
c.22	O
)	O
for	O
the	O
derivative	B
of	O
ln|cn|	O
,	O
we	O
obtain	O
−1	O
c	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
−1	O
n	O
c	O
∂cn	O
∂θi	O
+	O
1	O
2	O
−1	O
ttc	O
n	O
∂cn	O
∂θi	O
−1	O
n	O
t.	O
c	O
ln	O
p	O
(	O
t|θ	O
)	O
=	O
−1	O
∂	O
∂θi	O
(	O
6.70	O
)	O
because	O
ln	O
p	O
(	O
t|θ	O
)	O
will	O
in	O
general	O
be	O
a	O
nonconvex	O
function	O
,	O
it	O
can	O
have	O
multiple	O
max-	O
ima	O
.	O
2	O
tr	O
it	O
is	O
straightforward	O
to	O
introduce	O
a	O
prior	B
over	O
θ	O
and	O
to	O
maximize	O
the	O
log	O
poste-	O
rior	O
using	O
gradient-based	O
methods	O
.	O
in	O
a	O
fully	O
bayesian	O
treatment	O
,	O
we	O
need	O
to	O
evaluate	O
marginals	O
over	O
θ	O
weighted	O
by	O
the	O
product	O
of	O
the	O
prior	B
p	O
(	O
θ	O
)	O
and	O
the	O
likelihood	O
func-	O
tion	O
p	O
(	O
t|θ	O
)	O
.	O
in	O
general	O
,	O
however	O
,	O
exact	O
marginalization	O
will	O
be	O
intractable	O
,	O
and	O
we	O
must	O
resort	O
to	O
approximations	O
.	O
the	O
gaussian	O
process	O
regression	B
model	O
gives	O
a	O
predictive	B
distribution	I
whose	O
mean	B
and	O
variance	B
are	O
functions	O
of	O
the	O
input	O
vector	O
x.	O
however	O
,	O
we	O
have	O
assumed	O
that	O
the	O
contribution	O
to	O
the	O
predictive	O
variance	O
arising	O
from	O
the	O
additive	O
noise	O
,	O
gov-	O
erned	O
by	O
the	O
parameter	O
β	O
,	O
is	O
a	O
constant	O
.	O
for	O
some	O
problems	O
,	O
known	O
as	O
heteroscedas-	O
tic	O
,	O
the	O
noise	O
variance	B
itself	O
will	O
also	O
depend	O
on	O
x.	O
to	O
model	O
this	O
,	O
we	O
can	O
extend	O
the	O
312	O
6.	O
kernel	O
methods	O
for	O
gaussian	O
processes	O
,	O
figure	O
6.9	O
samples	O
from	O
the	O
ard	O
prior	B
in	O
which	O
the	O
kernel	B
function	I
is	O
given	O
by	O
(	O
6.71	O
)	O
.	O
the	O
left	O
plot	O
corresponds	O
to	O
η1	O
=	O
η2	O
=	O
1	O
,	O
and	O
the	O
right	O
plot	O
cor-	O
responds	O
to	O
η1	O
=	O
1	O
,	O
η2	O
=	O
0.01.	O
gaussian	O
process	O
framework	O
by	O
introducing	O
a	O
second	O
gaussian	O
process	O
to	O
represent	O
the	O
dependence	O
of	O
β	O
on	O
the	O
input	O
x	O
(	O
goldberg	O
et	O
al.	O
,	O
1998	O
)	O
.	O
because	O
β	O
is	O
a	O
variance	B
,	O
and	O
hence	O
nonnegative	O
,	O
we	O
use	O
the	O
gaussian	O
process	O
to	O
model	O
ln	O
β	O
(	O
x	O
)	O
.	O
6.4.4	O
automatic	B
relevance	I
determination	I
in	O
the	O
previous	O
section	O
,	O
we	O
saw	O
how	O
maximum	B
likelihood	I
could	O
be	O
used	O
to	O
de-	O
termine	O
a	O
value	O
for	O
the	O
correlation	O
length-scale	O
parameter	O
in	O
a	O
gaussian	O
process	O
.	O
this	O
technique	O
can	O
usefully	O
be	O
extended	B
by	O
incorporating	O
a	O
separate	O
parameter	O
for	O
each	O
input	O
variable	O
(	O
rasmussen	O
and	O
williams	O
,	O
2006	O
)	O
.	O
the	O
result	O
,	O
as	O
we	O
shall	O
see	O
,	O
is	O
that	O
the	O
optimization	O
of	O
these	O
parameters	O
by	O
maximum	B
likelihood	I
allows	O
the	O
relative	B
importance	O
of	O
different	O
inputs	O
to	O
be	O
inferred	O
from	O
the	O
data	O
.	O
this	O
represents	O
an	O
exam-	O
ple	O
in	O
the	O
gaussian	O
process	O
context	O
of	O
automatic	B
relevance	I
determination	I
,	O
or	O
ard	O
,	O
which	O
was	O
originally	O
formulated	O
in	O
the	O
framework	O
of	O
neural	O
networks	O
(	O
mackay	O
,	O
1994	O
;	O
neal	O
,	O
1996	O
)	O
.	O
the	O
mechanism	O
by	O
which	O
appropriate	O
inputs	O
are	O
preferred	O
is	O
discussed	O
in	O
section	O
7.2.2.	O
consider	O
a	O
gaussian	O
process	O
with	O
a	O
two-dimensional	O
input	O
space	O
x	O
=	O
(	O
x1	O
,	O
x2	O
)	O
,	O
having	O
a	O
kernel	B
function	I
of	O
the	O
form	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
θ0	O
exp	O
−1	O
2	O
ηi	O
(	O
xi	O
−	O
x	O
(	O
cid:4	O
)	O
i	O
)	O
2	O
.	O
(	O
6.71	O
)	O
(	O
cid:24	O
)	O
2	O
(	O
cid:2	O
)	O
i=1	O
(	O
cid:25	O
)	O
samples	O
from	O
the	O
resulting	O
prior	B
over	O
functions	O
y	O
(	O
x	O
)	O
are	O
shown	O
for	O
two	O
different	O
settings	O
of	O
the	O
precision	O
parameters	O
ηi	O
in	O
figure	O
6.9.	O
we	O
see	O
that	O
,	O
as	O
a	O
particu-	O
lar	O
parameter	O
ηi	O
becomes	O
small	O
,	O
the	O
function	O
becomes	O
relatively	O
insensitive	O
to	O
the	O
corresponding	O
input	O
variable	O
xi	O
.	O
by	O
adapting	O
these	O
parameters	O
to	O
a	O
data	O
set	O
using	O
maximum	B
likelihood	I
,	O
it	O
becomes	O
possible	O
to	O
detect	O
input	O
variables	O
that	O
have	O
little	O
effect	O
on	O
the	O
predictive	B
distribution	I
,	O
because	O
the	O
corresponding	O
values	O
of	O
ηi	O
will	O
be	O
small	O
.	O
this	O
can	O
be	O
useful	O
in	O
practice	O
because	O
it	O
allows	O
such	O
inputs	O
to	O
be	O
discarded	O
.	O
ard	O
is	O
illustrated	O
using	O
a	O
simple	O
synthetic	O
data	O
set	O
having	O
three	O
inputs	O
x1	O
,	O
x2	O
and	O
x3	O
(	O
nabney	O
,	O
2002	O
)	O
in	O
figure	O
6.10.	O
the	O
target	O
variable	O
t	O
,	O
is	O
generated	O
by	O
sampling	O
100	O
values	O
of	O
x1	O
from	O
a	O
gaussian	O
,	O
evaluating	O
the	O
function	O
sin	O
(	O
2πx1	O
)	O
,	O
and	O
then	O
adding	O
6.4.	O
gaussian	O
processes	O
313	O
figure	O
6.10	O
illustration	O
of	O
automatic	O
rele-	O
vance	O
determination	O
in	O
a	O
gaus-	O
sian	O
process	O
for	O
a	O
synthetic	O
prob-	O
lem	O
having	O
three	O
inputs	O
x1	O
,	O
x2	O
,	O
and	O
x3	O
,	O
for	O
which	O
the	O
curves	O
show	O
the	O
corresponding	O
values	O
of	O
the	O
hyperparameters	O
η1	O
(	O
red	O
)	O
,	O
η2	O
(	O
green	O
)	O
,	O
and	O
η3	O
(	O
blue	O
)	O
as	O
a	O
func-	O
tion	O
of	O
the	O
number	O
of	O
iterations	O
when	O
optimizing	O
the	O
marginal	B
likelihood	I
.	O
details	O
are	O
given	O
in	O
the	O
text	O
.	O
note	O
the	O
logarithmic	O
scale	O
on	O
the	O
vertical	O
axis	O
.	O
102	O
100	O
10−2	O
10−4	O
0	O
20	O
40	O
60	O
80	O
100	O
gaussian	O
noise	O
.	O
values	O
of	O
x2	O
are	O
given	O
by	O
copying	O
the	O
corresponding	O
values	O
of	O
x1	O
and	O
adding	O
noise	O
,	O
and	O
values	O
of	O
x3	O
are	O
sampled	O
from	O
an	O
independent	B
gaussian	O
dis-	O
tribution	O
.	O
thus	O
x1	O
is	O
a	O
good	O
predictor	O
of	O
t	O
,	O
x2	O
is	O
a	O
more	O
noisy	O
predictor	O
of	O
t	O
,	O
and	O
x3	O
has	O
only	O
chance	O
correlations	O
with	O
t.	O
the	O
marginal	B
likelihood	I
for	O
a	O
gaussian	O
process	O
with	O
ard	O
parameters	O
η1	O
,	O
η2	O
,	O
η3	O
is	O
optimized	O
using	O
the	O
scaled	O
conjugate	B
gradients	O
algorithm	O
.	O
we	O
see	O
from	O
figure	O
6.10	O
that	O
η1	O
converges	O
to	O
a	O
relatively	O
large	O
value	O
,	O
η2	O
converges	O
to	O
a	O
much	O
smaller	O
value	O
,	O
and	O
η3	O
becomes	O
very	O
small	O
indicating	O
that	O
x3	O
is	O
irrelevant	O
for	O
predicting	O
t.	O
the	O
ard	O
framework	O
is	O
easily	O
incorporated	O
into	O
the	O
exponential-quadratic	O
kernel	O
(	O
6.63	O
)	O
to	O
give	O
the	O
following	O
form	O
of	O
kernel	B
function	I
,	O
which	O
has	O
been	O
found	O
useful	O
for	O
applications	O
of	O
gaussian	O
processes	O
to	O
a	O
range	O
of	O
regression	B
problems	O
(	O
cid:24	O
)	O
d	O
(	O
cid:2	O
)	O
(	O
cid:25	O
)	O
d	O
(	O
cid:2	O
)	O
k	O
(	O
xn	O
,	O
xm	O
)	O
=	O
θ0	O
exp	O
−1	O
2	O
ηi	O
(	O
xni	O
−	O
xmi	O
)	O
2	O
+	O
θ2	O
+	O
θ3	O
xnixmi	O
(	O
6.72	O
)	O
i=1	O
i=1	O
where	O
d	O
is	O
the	O
dimensionality	O
of	O
the	O
input	O
space	O
.	O
6.4.5	O
gaussian	O
processes	O
for	O
classiﬁcation	O
in	O
a	O
probabilistic	O
approach	O
to	O
classiﬁcation	B
,	O
our	O
goal	O
is	O
to	O
model	O
the	O
posterior	O
probabilities	O
of	O
the	O
target	O
variable	O
for	O
a	O
new	O
input	O
vector	O
,	O
given	O
a	O
set	O
of	O
training	B
data	O
.	O
these	O
probabilities	O
must	O
lie	O
in	O
the	O
interval	O
(	O
0	O
,	O
1	O
)	O
,	O
whereas	O
a	O
gaussian	O
process	O
model	O
makes	O
predictions	O
that	O
lie	O
on	O
the	O
entire	O
real	O
axis	O
.	O
however	O
,	O
we	O
can	O
easily	O
adapt	O
gaussian	O
processes	O
to	O
classiﬁcation	B
problems	O
by	O
transforming	O
the	O
output	O
of	O
the	O
gaussian	O
process	O
using	O
an	O
appropriate	O
nonlinear	O
activation	B
function	I
.	O
consider	O
ﬁrst	O
the	O
two-class	O
problem	O
with	O
a	O
target	O
variable	O
t	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
if	O
we	O
de-	O
ﬁne	O
a	O
gaussian	O
process	O
over	O
a	O
function	O
a	O
(	O
x	O
)	O
and	O
then	O
transform	O
the	O
function	O
using	O
a	O
logistic	B
sigmoid	I
y	O
=	O
σ	O
(	O
a	O
)	O
,	O
given	O
by	O
(	O
4.59	O
)	O
,	O
then	O
we	O
will	O
obtain	O
a	O
non-gaussian	O
stochastic	B
process	I
over	O
functions	O
y	O
(	O
x	O
)	O
where	O
y	O
∈	O
(	O
0	O
,	O
1	O
)	O
.	O
this	O
is	O
illustrated	O
for	O
the	O
case	O
of	O
a	O
one-dimensional	O
input	O
space	O
in	O
figure	O
6.11	O
in	O
which	O
the	O
probability	B
distri-	O
314	O
6.	O
kernel	O
methods	O
10	O
5	O
0	O
−5	O
−10	O
−1	O
−0.5	O
0	O
0.5	O
1	O
1	O
0.75	O
0.5	O
0.25	O
0	O
−1	O
−0.5	O
0	O
0.5	O
1	O
figure	O
6.11	O
the	O
left	O
plot	O
shows	O
a	O
sample	O
from	O
a	O
gaussian	O
process	O
prior	B
over	O
functions	O
a	O
(	O
x	O
)	O
,	O
and	O
the	O
right	O
plot	O
shows	O
the	O
result	O
of	O
transforming	O
this	O
sample	O
using	O
a	O
logistic	B
sigmoid	I
function	O
.	O
bution	O
over	O
the	O
target	O
variable	O
t	O
is	O
then	O
given	O
by	O
the	O
bernoulli	O
distribution	O
p	O
(	O
t|a	O
)	O
=	O
σ	O
(	O
a	O
)	O
t	O
(	O
1	O
−	O
σ	O
(	O
a	O
)	O
)	O
1−t	O
.	O
(	O
6.73	O
)	O
as	O
usual	O
,	O
we	O
denote	O
the	O
training	B
set	I
inputs	O
by	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
with	O
corresponding	O
observed	O
target	O
variables	O
t	O
=	O
(	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
)	O
t.	O
we	O
also	O
consider	O
a	O
single	O
test	O
point	O
xn	O
+1	O
with	O
target	O
value	O
tn	O
+1	O
.	O
our	O
goal	O
is	O
to	O
determine	O
the	O
predictive	B
distribution	I
p	O
(	O
tn	O
+1|t	O
)	O
,	O
where	O
we	O
have	O
left	O
the	O
conditioning	O
on	O
the	O
input	O
variables	O
implicit	O
.	O
to	O
do	O
this	O
we	O
introduce	O
a	O
gaussian	O
process	O
prior	B
over	O
the	O
vector	O
an	O
+1	O
,	O
which	O
has	O
compo-	O
nents	O
a	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
a	O
(	O
xn	O
+1	O
)	O
.	O
this	O
in	O
turn	O
deﬁnes	O
a	O
non-gaussian	O
process	O
over	O
tn	O
+1	O
,	O
and	O
by	O
conditioning	O
on	O
the	O
training	B
data	O
tn	O
we	O
obtain	O
the	O
required	O
predictive	O
distri-	O
bution	O
.	O
the	O
gaussian	O
process	O
prior	B
for	O
an	O
+1	O
takes	O
the	O
form	O
p	O
(	O
an	O
+1	O
)	O
=	O
n	O
(	O
an	O
+1|0	O
,	O
cn	O
+1	O
)	O
.	O
(	O
6.74	O
)	O
unlike	O
the	O
regression	B
case	O
,	O
the	O
covariance	B
matrix	I
no	O
longer	O
includes	O
a	O
noise	O
term	O
because	O
we	O
assume	O
that	O
all	O
of	O
the	O
training	B
data	O
points	O
are	O
correctly	O
labelled	O
.	O
how-	O
ever	O
,	O
for	O
numerical	O
reasons	O
it	O
is	O
convenient	O
to	O
introduce	O
a	O
noise-like	O
term	O
governed	O
by	O
a	O
parameter	O
ν	O
that	O
ensures	O
that	O
the	O
covariance	B
matrix	I
is	O
positive	B
deﬁnite	I
.	O
thus	O
the	O
covariance	B
matrix	I
cn	O
+1	O
has	O
elements	O
given	O
by	O
c	O
(	O
xn	O
,	O
xm	O
)	O
=	O
k	O
(	O
xn	O
,	O
xm	O
)	O
+	O
νδnm	O
(	O
6.75	O
)	O
where	O
k	O
(	O
xn	O
,	O
xm	O
)	O
is	O
any	O
positive	O
semideﬁnite	O
kernel	O
function	O
of	O
the	O
kind	O
considered	O
in	O
section	O
6.2	O
,	O
and	O
the	O
value	O
of	O
ν	O
is	O
typically	O
ﬁxed	O
in	O
advance	O
.	O
we	O
shall	O
assume	O
that	O
the	O
kernel	B
function	I
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
is	O
governed	O
by	O
a	O
vector	O
θ	O
of	O
parameters	O
,	O
and	O
we	O
shall	O
later	O
discuss	O
how	O
θ	O
may	O
be	O
learned	O
from	O
the	O
training	B
data	O
.	O
for	O
two-class	O
problems	O
,	O
it	O
is	O
sufﬁcient	O
to	O
predict	O
p	O
(	O
tn	O
+1	O
=	O
1|tn	O
)	O
because	O
the	O
value	O
of	O
p	O
(	O
tn	O
+1	O
=	O
0|tn	O
)	O
is	O
then	O
given	O
by	O
1	O
−	O
p	O
(	O
tn	O
+1	O
=	O
1|tn	O
)	O
.	O
the	O
required	O
6.4.	O
gaussian	O
processes	O
315	O
predictive	B
distribution	I
is	O
given	O
by	O
(	O
cid:6	O
)	O
p	O
(	O
tn	O
+1	O
=	O
1|tn	O
)	O
=	O
p	O
(	O
tn	O
+1	O
=	O
1|an	O
+1	O
)	O
p	O
(	O
an	O
+1|tn	O
)	O
dan	O
+1	O
(	O
6.76	O
)	O
where	O
p	O
(	O
tn	O
+1	O
=	O
1|an	O
+1	O
)	O
=	O
σ	O
(	O
an	O
+1	O
)	O
.	O
this	O
integral	O
is	O
analytically	O
intractable	O
,	O
and	O
so	O
may	O
be	O
approximated	O
using	O
sam-	O
pling	O
methods	O
(	O
neal	O
,	O
1997	O
)	O
.	O
alternatively	O
,	O
we	O
can	O
consider	O
techniques	O
based	O
on	O
an	O
analytical	O
approximation	O
.	O
in	O
section	O
4.5.2	O
,	O
we	O
derived	O
the	O
approximate	O
formula	O
(	O
4.153	O
)	O
for	O
the	O
convolution	O
of	O
a	O
logistic	B
sigmoid	I
with	O
a	O
gaussian	O
distribution	O
.	O
we	O
can	O
use	O
this	O
result	O
to	O
evaluate	O
the	O
integral	O
in	O
(	O
6.76	O
)	O
provided	O
we	O
have	O
a	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
distribution	O
p	O
(	O
an	O
+1|tn	O
)	O
.	O
the	O
usual	O
justiﬁcation	O
for	O
a	O
gaussian	O
approximation	O
to	O
a	O
posterior	O
distribution	O
is	O
that	O
the	O
true	O
posterior	O
will	O
tend	O
to	O
a	O
gaussian	O
as	O
the	O
number	O
of	O
data	O
points	O
increases	O
as	O
a	O
consequence	O
of	O
the	O
central	B
limit	I
theorem	I
.	O
in	O
the	O
case	O
of	O
gaussian	O
processes	O
,	O
the	O
number	O
of	O
variables	O
grows	O
with	O
the	O
number	O
of	O
data	O
points	O
,	O
and	O
so	O
this	O
argument	O
does	O
not	O
apply	O
directly	O
.	O
however	O
,	O
if	O
we	O
consider	O
increasing	O
the	O
number	O
of	O
data	O
points	O
falling	O
in	O
a	O
ﬁxed	O
region	O
of	O
x	O
space	O
,	O
then	O
the	O
corresponding	O
uncertainty	O
in	O
the	O
function	O
a	O
(	O
x	O
)	O
will	O
decrease	O
,	O
again	O
leading	O
asymptotically	O
to	O
a	O
gaussian	O
(	O
williams	O
and	O
barber	O
,	O
1998	O
)	O
.	O
three	O
different	O
approaches	O
to	O
obtaining	O
a	O
gaussian	O
approximation	O
have	O
been	O
considered	O
.	O
one	O
technique	O
is	O
based	O
on	O
variational	B
inference	I
(	O
gibbs	O
and	O
mackay	O
,	O
2000	O
)	O
and	O
makes	O
use	O
of	O
the	O
local	B
variational	O
bound	O
(	O
10.144	O
)	O
on	O
the	O
logistic	B
sigmoid	I
.	O
this	O
allows	O
the	O
product	O
of	O
sigmoid	B
functions	O
to	O
be	O
approximated	O
by	O
a	O
product	O
of	O
gaussians	O
thereby	O
allowing	O
the	O
marginalization	O
over	O
an	O
to	O
be	O
performed	O
analyti-	O
cally	O
.	O
the	O
approach	O
also	O
yields	O
a	O
lower	B
bound	I
on	O
the	O
likelihood	B
function	I
p	O
(	O
tn|θ	O
)	O
.	O
the	O
variational	B
framework	O
for	O
gaussian	O
process	O
classiﬁcation	B
can	O
also	O
be	O
extended	B
to	O
multiclass	B
(	O
k	O
>	O
2	O
)	O
problems	O
by	O
using	O
a	O
gaussian	O
approximation	O
to	O
the	O
softmax	B
function	I
(	O
gibbs	O
,	O
1997	O
)	O
.	O
a	O
second	O
approach	O
uses	O
expectation	B
propagation	I
(	O
opper	O
and	O
winther	O
,	O
2000b	O
;	O
minka	O
,	O
2001b	O
;	O
seeger	O
,	O
2003	O
)	O
.	O
because	O
the	O
true	O
posterior	O
distribution	O
is	O
unimodal	O
,	O
as	O
we	O
shall	O
see	O
shortly	O
,	O
the	O
expectation	B
propagation	I
approach	O
can	O
give	O
good	O
results	O
.	O
6.4.6	O
laplace	O
approximation	O
the	O
third	O
approach	O
to	O
gaussian	O
process	O
classiﬁcation	B
is	O
based	O
on	O
the	O
laplace	O
approximation	O
,	O
which	O
we	O
now	O
consider	O
in	O
detail	O
.	O
in	O
order	O
to	O
evaluate	O
the	O
predictive	B
distribution	I
(	O
6.76	O
)	O
,	O
we	O
seek	O
a	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
distribution	O
over	O
an	O
+1	O
,	O
which	O
,	O
using	O
bayes	O
’	O
theorem	O
,	O
is	O
given	O
by	O
(	O
cid:6	O
)	O
p	O
(	O
an	O
+1	O
,	O
an|tn	O
)	O
dan	O
(	O
cid:6	O
)	O
1	O
p	O
(	O
an	O
+1|tn	O
)	O
=	O
p	O
(	O
an	O
+1	O
,	O
an	O
)	O
p	O
(	O
tn|an	O
+1	O
,	O
an	O
)	O
dan	O
p	O
(	O
an	O
+1|an	O
)	O
p	O
(	O
an	O
)	O
p	O
(	O
tn|an	O
)	O
dan	O
p	O
(	O
tn	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
1	O
p	O
(	O
tn	O
)	O
p	O
(	O
an	O
+1|an	O
)	O
p	O
(	O
an|tn	O
)	O
dan	O
(	O
6.77	O
)	O
=	O
=	O
=	O
section	O
2.3	O
section	O
10.1	O
section	O
10.7	O
section	O
4.4	O
316	O
6.	O
kernel	O
methods	O
where	O
we	O
have	O
used	O
p	O
(	O
tn|an	O
+1	O
,	O
an	O
)	O
=	O
p	O
(	O
tn|an	O
)	O
.	O
the	O
conditional	B
distribution	O
p	O
(	O
an	O
+1|an	O
)	O
is	O
obtained	O
by	O
invoking	O
the	O
results	O
(	O
6.66	O
)	O
and	O
(	O
6.67	O
)	O
for	O
gaussian	O
pro-	O
cess	O
regression	B
,	O
to	O
give	O
p	O
(	O
an	O
+1|an	O
)	O
=	O
n	O
(	O
an	O
+1|ktc	O
n	O
an	O
,	O
c	O
−	O
ktc	O
−1	O
−1	O
n	O
k	O
)	O
.	O
(	O
6.78	O
)	O
we	O
can	O
therefore	O
evaluate	O
the	O
integral	O
in	O
(	O
6.77	O
)	O
by	O
ﬁnding	O
a	O
laplace	O
approximation	O
for	O
the	O
posterior	O
distribution	O
p	O
(	O
an|tn	O
)	O
,	O
and	O
then	O
using	O
the	O
standard	O
result	O
for	O
the	O
convolution	O
of	O
two	O
gaussian	O
distributions	O
.	O
the	O
prior	B
p	O
(	O
an	O
)	O
is	O
given	O
by	O
a	O
zero-mean	O
gaussian	O
process	O
with	O
covariance	B
ma-	O
trix	O
cn	O
,	O
and	O
the	O
data	O
term	O
(	O
assuming	O
independence	O
of	O
the	O
data	O
points	O
)	O
is	O
given	O
by	O
p	O
(	O
tn|an	O
)	O
=	O
σ	O
(	O
an	O
)	O
tn	O
(	O
1	O
−	O
σ	O
(	O
an	O
)	O
)	O
1−tn	O
=	O
eantnσ	O
(	O
−an	O
)	O
.	O
(	O
6.79	O
)	O
n	O
(	O
cid:14	O
)	O
n	O
(	O
cid:14	O
)	O
n=1	O
n=1	O
we	O
then	O
obtain	O
the	O
laplace	O
approximation	O
by	O
taylor	O
expanding	O
the	O
logarithm	O
of	O
p	O
(	O
an|tn	O
)	O
,	O
which	O
up	O
to	O
an	O
additive	O
normalization	O
constant	O
is	O
given	O
by	O
the	O
quantity	O
ψ	O
(	O
an	O
)	O
=	O
ln	O
p	O
(	O
an	O
)	O
+	O
ln	O
p	O
(	O
tn|an	O
)	O
=	O
−1	O
2	O
−	O
n	O
(	O
cid:2	O
)	O
at	O
n	O
c	O
n	O
an	O
−	O
n	O
−1	O
2	O
ln	O
(	O
2π	O
)	O
−	O
1	O
2	O
ln|cn|	O
+	O
tt	O
n	O
an	O
ln	O
(	O
1	O
+	O
ean	O
)	O
+	O
const	O
.	O
(	O
6.80	O
)	O
n=1	O
first	O
we	O
need	O
to	O
ﬁnd	O
the	O
mode	O
of	O
the	O
posterior	O
distribution	O
,	O
and	O
this	O
requires	O
that	O
we	O
evaluate	O
the	O
gradient	O
of	O
ψ	O
(	O
an	O
)	O
,	O
which	O
is	O
given	O
by	O
∇ψ	O
(	O
an	O
)	O
=	O
tn	O
−	O
σn	O
−	O
c	O
−1	O
n	O
an	O
(	O
6.81	O
)	O
where	O
σn	O
is	O
a	O
vector	O
with	O
elements	O
σ	O
(	O
an	O
)	O
.	O
we	O
can	O
not	O
simply	O
ﬁnd	O
the	O
mode	O
by	O
setting	O
this	O
gradient	O
to	O
zero	O
,	O
because	O
σn	O
depends	O
nonlinearly	O
on	O
an	O
,	O
and	O
so	O
we	O
resort	O
to	O
an	O
iterative	O
scheme	O
based	O
on	O
the	O
newton-raphson	O
method	O
,	O
which	O
gives	O
rise	O
to	O
an	O
iterative	B
reweighted	I
least	I
squares	I
(	O
irls	O
)	O
algorithm	O
.	O
this	O
requires	O
the	O
second	O
derivatives	O
of	O
ψ	O
(	O
an	O
)	O
,	O
which	O
we	O
also	O
require	O
for	O
the	O
laplace	O
approximation	O
anyway	O
,	O
and	O
which	O
are	O
given	O
by	O
∇∇ψ	O
(	O
an	O
)	O
=	O
−wn	O
−	O
c	O
−1	O
n	O
(	O
6.82	O
)	O
where	O
wn	O
is	O
a	O
diagonal	B
matrix	O
with	O
elements	O
σ	O
(	O
an	O
)	O
(	O
1−	O
σ	O
(	O
an	O
)	O
)	O
,	O
and	O
we	O
have	O
used	O
the	O
result	O
(	O
4.88	O
)	O
for	O
the	O
derivative	B
of	O
the	O
logistic	B
sigmoid	I
function	O
.	O
note	O
that	O
these	O
diagonal	B
elements	O
lie	O
in	O
the	O
range	O
(	O
0	O
,	O
1/4	O
)	O
,	O
and	O
hence	O
wn	O
is	O
a	O
positive	B
deﬁnite	I
matrix	I
.	O
because	O
cn	O
(	O
and	O
hence	O
its	O
inverse	B
)	O
is	O
positive	B
deﬁnite	I
by	O
construction	O
,	O
and	O
because	O
the	O
sum	O
of	O
two	O
positive	B
deﬁnite	I
matrices	O
is	O
also	O
positive	B
deﬁnite	I
,	O
we	O
see	O
that	O
the	O
hessian	O
matrix	O
a	O
=	O
−∇∇ψ	O
(	O
an	O
)	O
is	O
positive	B
deﬁnite	I
and	O
so	O
the	O
posterior	O
distribution	O
p	O
(	O
an|tn	O
)	O
is	O
log	O
convex	O
and	O
therefore	O
has	O
a	O
single	O
mode	O
that	O
is	O
the	O
global	O
section	O
4.3.3	O
exercise	O
6.24	O
6.4.	O
gaussian	O
processes	O
317	O
maximum	O
.	O
the	O
posterior	O
distribution	O
is	O
not	O
gaussian	O
,	O
however	O
,	O
because	O
the	O
hessian	O
is	O
a	O
function	O
of	O
an	O
.	O
using	O
the	O
newton-raphson	O
formula	O
(	O
4.92	O
)	O
,	O
the	O
iterative	O
update	O
equation	O
for	O
an	O
exercise	O
6.25	O
is	O
given	O
by	O
n	O
=	O
cn	O
(	O
i	O
+	O
wn	O
cn	O
)	O
−1	O
{	O
tn	O
−	O
σn	O
+	O
wn	O
an	O
}	O
.	O
anew	O
(	O
6.83	O
)	O
these	O
equations	O
are	O
iterated	O
until	O
they	O
converge	O
to	O
the	O
mode	O
which	O
we	O
denote	O
by	O
n	O
.	O
at	O
the	O
mode	O
,	O
the	O
gradient	O
∇ψ	O
(	O
an	O
)	O
will	O
vanish	O
,	O
and	O
hence	O
a	O
(	O
cid:1	O
)	O
a	O
(	O
cid:1	O
)	O
n	O
will	O
satisfy	O
n	O
=	O
cn	O
(	O
tn	O
−	O
σn	O
)	O
.	O
a	O
(	O
cid:1	O
)	O
(	O
6.84	O
)	O
once	O
we	O
have	O
found	O
the	O
mode	O
a	O
(	O
cid:1	O
)	O
n	O
of	O
the	O
posterior	O
,	O
we	O
can	O
evaluate	O
the	O
hessian	O
matrix	O
given	O
by	O
h	O
=	O
−∇∇ψ	O
(	O
an	O
)	O
=	O
wn	O
+	O
c	O
−1	O
n	O
(	O
6.85	O
)	O
where	O
the	O
elements	O
of	O
wn	O
are	O
evaluated	O
using	O
a	O
(	O
cid:1	O
)	O
proximation	B
to	O
the	O
posterior	O
distribution	O
p	O
(	O
an|tn	O
)	O
given	O
by	O
n	O
.	O
this	O
deﬁnes	O
our	O
gaussian	O
ap-	O
q	O
(	O
an	O
)	O
=	O
n	O
(	O
an|a	O
(	O
cid:1	O
)	O
n	O
,	O
h−1	O
)	O
.	O
(	O
6.86	O
)	O
exercise	O
6.26	O
we	O
can	O
now	O
combine	O
this	O
with	O
(	O
6.78	O
)	O
and	O
hence	O
evaluate	O
the	O
integral	O
(	O
6.77	O
)	O
.	O
because	O
this	O
corresponds	O
to	O
a	O
linear-gaussian	O
model	O
,	O
we	O
can	O
use	O
the	O
general	O
result	O
(	O
2.115	O
)	O
to	O
give	O
e	O
[	O
an	O
+1|tn	O
]	O
=	O
kt	O
(	O
tn	O
−	O
σn	O
)	O
var	O
[	O
an	O
+1|tn	O
]	O
=	O
c	O
−	O
kt	O
(	O
w	O
−1	O
n	O
+	O
cn	O
)	O
−1k	O
.	O
(	O
6.87	O
)	O
(	O
6.88	O
)	O
now	O
that	O
we	O
have	O
a	O
gaussian	O
distribution	O
for	O
p	O
(	O
an	O
+1|tn	O
)	O
,	O
we	O
can	O
approximate	O
the	O
integral	O
(	O
6.76	O
)	O
using	O
the	O
result	O
(	O
4.153	O
)	O
.	O
as	O
with	O
the	O
bayesian	O
logistic	B
regression	I
model	O
of	O
section	O
4.5	O
,	O
if	O
we	O
are	O
only	O
interested	O
in	O
the	O
decision	B
boundary	I
correspond-	O
ing	O
to	O
p	O
(	O
tn	O
+1|tn	O
)	O
=	O
0.5	O
,	O
then	O
we	O
need	O
only	O
consider	O
the	O
mean	B
and	O
we	O
can	O
ignore	O
the	O
effect	O
of	O
the	O
variance	B
.	O
we	O
also	O
need	O
to	O
determine	O
the	O
parameters	O
θ	O
of	O
the	O
covariance	B
function	O
.	O
one	O
approach	O
is	O
to	O
maximize	O
the	O
likelihood	B
function	I
given	O
by	O
p	O
(	O
tn|θ	O
)	O
for	O
which	O
we	O
need	O
expressions	O
for	O
the	O
log	O
likelihood	O
and	O
its	O
gradient	O
.	O
if	O
desired	O
,	O
suitable	O
regularization	B
terms	O
can	O
also	O
be	O
added	O
,	O
leading	O
to	O
a	O
penalized	O
maximum	B
likelihood	I
solution	O
.	O
the	O
likelihood	B
function	I
is	O
deﬁned	O
by	O
p	O
(	O
tn|θ	O
)	O
=	O
p	O
(	O
tn|an	O
)	O
p	O
(	O
an|θ	O
)	O
dan	O
.	O
(	O
6.89	O
)	O
(	O
cid:6	O
)	O
this	O
integral	O
is	O
analytically	O
intractable	O
,	O
so	O
again	O
we	O
make	O
use	O
of	O
the	O
laplace	O
approx-	O
imation	O
.	O
using	O
the	O
result	O
(	O
4.135	O
)	O
,	O
we	O
obtain	O
the	O
following	O
approximation	O
for	O
the	O
log	O
of	O
the	O
likelihood	B
function	I
ln	O
p	O
(	O
tn|θ	O
)	O
=	O
ψ	O
(	O
a	O
(	O
cid:1	O
)	O
n	O
)	O
−	O
1	O
2	O
ln|wn	O
+	O
c	O
n	O
|	O
+	O
n	O
−1	O
2	O
ln	O
(	O
2π	O
)	O
(	O
6.90	O
)	O
318	O
6.	O
kernel	O
methods	O
n|θ	O
)	O
+	O
ln	O
p	O
(	O
tn|a	O
(	O
cid:1	O
)	O
n	O
)	O
=	O
ln	O
p	O
(	O
a	O
(	O
cid:1	O
)	O
where	O
ψ	O
(	O
a	O
(	O
cid:1	O
)	O
n	O
)	O
.	O
we	O
also	O
need	O
to	O
evaluate	O
the	O
gradient	O
of	O
ln	O
p	O
(	O
tn|θ	O
)	O
with	O
respect	O
to	O
the	O
parameter	O
vector	O
θ.	O
note	O
that	O
changes	O
in	O
θ	O
will	O
cause	O
changes	O
in	O
a	O
(	O
cid:1	O
)	O
n	O
,	O
leading	O
to	O
additional	O
terms	O
in	O
the	O
gradient	O
.	O
thus	O
,	O
when	O
we	O
differentiate	O
(	O
6.90	O
)	O
with	O
respect	O
to	O
θ	O
,	O
we	O
obtain	O
two	O
sets	O
of	O
terms	O
,	O
the	O
ﬁrst	O
arising	O
from	O
the	O
dependence	O
of	O
the	O
covariance	B
matrix	I
cn	O
on	O
θ	O
,	O
and	O
the	O
rest	O
arising	O
from	O
dependence	O
of	O
a	O
(	O
cid:1	O
)	O
n	O
on	O
θ.	O
the	O
terms	O
arising	O
from	O
the	O
explicit	O
dependence	O
on	O
θ	O
can	O
be	O
found	O
by	O
using	O
(	O
6.80	O
)	O
together	O
with	O
the	O
results	O
(	O
c.21	O
)	O
and	O
(	O
c.22	O
)	O
,	O
and	O
are	O
given	O
by	O
∂	O
ln	O
p	O
(	O
tn|θ	O
)	O
∂θj	O
=	O
−1	O
a	O
(	O
cid:1	O
)	O
t	O
n	O
c	O
n	O
∂cn	O
∂θj	O
−1	O
n	O
a	O
(	O
cid:1	O
)	O
n	O
c	O
(	O
cid:29	O
)	O
1	O
2	O
−1	O
2	O
tr	O
(	O
i	O
+	O
cn	O
wn	O
)	O
−1wn	O
(	O
cid:30	O
)	O
∂cn	O
∂θj	O
.	O
(	O
6.91	O
)	O
n	O
,	O
and	O
so	O
ψ	O
(	O
a	O
(	O
cid:1	O
)	O
to	O
compute	O
the	O
terms	O
arising	O
from	O
the	O
dependence	O
of	O
a	O
(	O
cid:1	O
)	O
n	O
on	O
θ	O
,	O
we	O
note	O
that	O
the	O
laplace	O
approximation	O
has	O
been	O
constructed	O
such	O
that	O
ψ	O
(	O
an	O
)	O
has	O
zero	O
gradient	O
at	O
an	O
=	O
a	O
(	O
cid:1	O
)	O
n	O
)	O
gives	O
no	O
contribution	O
to	O
the	O
gradient	O
as	O
a	O
result	O
of	O
its	O
dependence	O
on	O
a	O
(	O
cid:1	O
)	O
n	O
.	O
this	O
leaves	O
the	O
following	O
contribution	O
to	O
the	O
derivative	B
with	O
respect	O
to	O
a	O
component	O
θj	O
of	O
θ	O
n	O
|	O
∂	O
ln|wn	O
+	O
c	O
−1	O
n	O
(	O
cid:2	O
)	O
−1	O
2	O
∂a	O
(	O
cid:1	O
)	O
n	O
n	O
(	O
cid:2	O
)	O
(	O
cid:8	O
)	O
n=1	O
n=1	O
=	O
−1	O
2	O
∂a	O
(	O
cid:1	O
)	O
n	O
∂θj	O
(	O
cid:9	O
)	O
(	O
i	O
+	O
cn	O
wn	O
)	O
−1cn	O
n	O
(	O
1	O
−	O
σ	O
(	O
cid:1	O
)	O
n	O
)	O
(	O
1	O
−	O
2σ	O
(	O
cid:1	O
)	O
nn	O
σ	O
(	O
cid:1	O
)	O
n	O
)	O
∂a	O
(	O
cid:1	O
)	O
n	O
∂θj	O
(	O
6.92	O
)	O
n	O
=	O
σ	O
(	O
a	O
(	O
cid:1	O
)	O
where	O
σ	O
(	O
cid:1	O
)	O
deﬁnition	O
of	O
wn	O
.	O
we	O
can	O
evaluate	O
the	O
derivative	B
of	O
a	O
(	O
cid:1	O
)	O
entiating	O
the	O
relation	O
(	O
6.84	O
)	O
with	O
respect	O
to	O
θj	O
to	O
give	O
n	O
)	O
,	O
and	O
again	O
we	O
have	O
used	O
the	O
result	O
(	O
c.22	O
)	O
together	O
with	O
the	O
n	O
with	O
respect	O
to	O
θj	O
by	O
differ-	O
∂a	O
(	O
cid:1	O
)	O
n	O
∂θj	O
=	O
∂cn	O
∂θj	O
(	O
tn	O
−	O
σn	O
)	O
−	O
cn	O
wn	O
∂a	O
(	O
cid:1	O
)	O
n	O
∂θj	O
.	O
rearranging	O
then	O
gives	O
∂a	O
(	O
cid:1	O
)	O
n	O
∂θj	O
=	O
(	O
i	O
+	O
wn	O
cn	O
)	O
−1	O
∂cn	O
∂θj	O
(	O
tn	O
−	O
σn	O
)	O
.	O
(	O
6.93	O
)	O
(	O
6.94	O
)	O
combining	O
(	O
6.91	O
)	O
,	O
(	O
6.92	O
)	O
,	O
and	O
(	O
6.94	O
)	O
,	O
we	O
can	O
evaluate	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	O
function	O
,	O
which	O
can	O
be	O
used	O
with	O
standard	O
nonlinear	O
optimization	O
algo-	O
rithms	O
in	O
order	O
to	O
determine	O
a	O
value	O
for	O
θ.	O
we	O
can	O
illustrate	O
the	O
application	O
of	O
the	O
laplace	O
approximation	O
for	O
gaussian	O
pro-	O
cesses	O
using	O
the	O
synthetic	O
two-class	O
data	O
set	O
shown	O
in	O
figure	O
6.12.	O
extension	O
of	O
the	O
laplace	O
approximation	O
to	O
gaussian	O
processes	O
involving	O
k	O
>	O
2	O
classes	O
,	O
using	O
the	O
softmax	O
activation	O
function	O
,	O
is	O
straightforward	O
(	O
williams	O
and	O
barber	O
,	O
1998	O
)	O
.	O
appendix	O
a	O
6.4.	O
gaussian	O
processes	O
319	O
2	O
0	O
−2	O
−2	O
0	O
2	O
figure	O
6.12	O
illustration	O
of	O
the	O
use	O
of	O
a	O
gaussian	O
process	O
for	O
classiﬁcation	O
,	O
showing	O
the	O
data	O
on	O
the	O
left	O
together	O
with	O
the	O
optimal	O
decision	B
boundary	I
from	O
the	O
true	O
distribution	O
in	O
green	O
,	O
and	O
the	O
decision	B
boundary	I
from	O
the	O
gaussian	O
process	O
classiﬁer	O
in	O
black	O
.	O
on	O
the	O
right	O
is	O
the	O
predicted	O
posterior	B
probability	I
for	O
the	O
blue	O
and	O
red	O
classes	O
together	O
with	O
the	O
gaussian	O
process	O
decision	B
boundary	I
.	O
6.4.7	O
connection	O
to	O
neural	O
networks	O
we	O
have	O
seen	O
that	O
the	O
range	O
of	O
functions	O
which	O
can	O
be	O
represented	O
by	O
a	O
neural	B
network	I
is	O
governed	O
by	O
the	O
number	O
m	O
of	O
hidden	O
units	O
,	O
and	O
that	O
,	O
for	O
sufﬁciently	O
large	O
m	O
,	O
a	O
two-layer	O
network	O
can	O
approximate	O
any	O
given	O
function	O
with	O
arbitrary	O
accuracy	O
.	O
in	O
the	O
framework	O
of	O
maximum	B
likelihood	I
,	O
the	O
number	O
of	O
hidden	O
units	O
needs	O
to	O
be	O
limited	O
(	O
to	O
a	O
level	O
dependent	O
on	O
the	O
size	O
of	O
the	O
training	B
set	I
)	O
in	O
order	O
to	O
avoid	O
over-ﬁtting	B
.	O
however	O
,	O
from	O
a	O
bayesian	O
perspective	O
it	O
makes	O
little	O
sense	O
to	O
limit	O
the	O
number	O
of	O
parameters	O
in	O
the	O
network	O
according	O
to	O
the	O
size	O
of	O
the	O
training	B
set	I
.	O
in	O
a	O
bayesian	O
neural	B
network	I
,	O
the	O
prior	B
distribution	O
over	O
the	O
parameter	O
vector	O
w	O
,	O
in	O
conjunction	O
with	O
the	O
network	O
function	O
f	O
(	O
x	O
,	O
w	O
)	O
,	O
produces	O
a	O
prior	B
distribution	O
over	O
functions	O
from	O
y	O
(	O
x	O
)	O
where	O
y	O
is	O
the	O
vector	O
of	O
network	O
outputs	O
.	O
neal	O
(	O
1996	O
)	O
has	O
shown	O
that	O
,	O
for	O
a	O
broad	O
class	O
of	O
prior	B
distributions	O
over	O
w	O
,	O
the	O
distribution	O
of	O
functions	O
generated	O
by	O
a	O
neural	B
network	I
will	O
tend	O
to	O
a	O
gaussian	O
process	O
in	O
the	O
limit	O
m	O
→	O
∞	O
.	O
it	O
should	O
be	O
noted	O
,	O
however	O
,	O
that	O
in	O
this	O
limit	O
the	O
output	O
variables	O
of	O
the	O
neural	B
network	I
become	O
independent	B
.	O
one	O
of	O
the	O
great	O
merits	O
of	O
neural	O
networks	O
is	O
that	O
the	O
outputs	O
share	O
the	O
hidden	O
units	O
and	O
so	O
they	O
can	O
‘	O
borrow	O
statistical	O
strength	O
’	O
from	O
each	O
other	O
,	O
that	O
is	O
,	O
the	O
weights	O
associated	O
with	O
each	O
hidden	B
unit	I
are	O
inﬂuenced	O
by	O
all	O
of	O
the	O
output	O
variables	O
not	O
just	O
by	O
one	O
of	O
them	O
.	O
this	O
property	O
is	O
therefore	O
lost	O
in	O
the	O
gaussian	O
process	O
limit	O
.	O
we	O
have	O
seen	O
that	O
a	O
gaussian	O
process	O
is	O
determined	O
by	O
its	O
covariance	B
(	O
kernel	O
)	O
function	O
.	O
williams	O
(	O
1998	O
)	O
has	O
given	O
explicit	O
forms	O
for	O
the	O
covariance	B
in	O
the	O
case	O
of	O
two	O
speciﬁc	O
choices	O
for	O
the	O
hidden	B
unit	I
activation	O
function	O
(	O
probit	O
and	O
gaussian	O
)	O
.	O
these	O
kernel	O
functions	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
are	O
nonstationary	O
,	O
i.e	O
.	O
they	O
can	O
not	O
be	O
expressed	O
as	O
a	O
function	O
of	O
the	O
difference	O
x	O
−	O
x	O
(	O
cid:4	O
)	O
,	O
as	O
a	O
consequence	O
of	O
the	O
gaussian	O
weight	O
prior	O
being	O
centred	O
on	O
zero	O
which	O
breaks	O
translation	B
invariance	I
in	O
weight	O
space	O
.	O
320	O
6.	O
kernel	O
methods	O
exercises	O
by	O
working	O
directly	O
with	O
the	O
covariance	B
function	O
we	O
have	O
implicitly	O
marginal-	O
ized	O
over	O
the	O
distribution	O
of	O
weights	O
.	O
if	O
the	O
weight	O
prior	O
is	O
governed	O
by	O
hyperpa-	O
rameters	O
,	O
then	O
their	O
values	O
will	O
determine	O
the	O
length	O
scales	O
of	O
the	O
distribution	O
over	O
functions	O
,	O
as	O
can	O
be	O
understood	O
by	O
studying	O
the	O
examples	O
in	O
figure	O
5.11	O
for	O
the	O
case	O
of	O
a	O
ﬁnite	O
number	O
of	O
hidden	O
units	O
.	O
note	O
that	O
we	O
can	O
not	O
marginalize	O
out	O
the	O
hyperpa-	O
rameters	O
analytically	O
,	O
and	O
must	O
instead	O
resort	O
to	O
techniques	O
of	O
the	O
kind	O
discussed	O
in	O
section	O
6.4	O
.	O
6.1	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
the	O
dual	O
formulation	O
of	O
the	O
least	O
squares	O
linear	B
regression	I
problem	O
given	O
in	O
section	O
6.1.	O
show	O
that	O
the	O
solution	O
for	O
the	O
components	O
an	O
of	O
the	O
vector	O
a	O
can	O
be	O
expressed	O
as	O
a	O
linear	O
combination	O
of	O
the	O
elements	O
of	O
the	O
vector	O
φ	O
(	O
xn	O
)	O
.	O
denoting	O
these	O
coefﬁcients	O
by	O
the	O
vector	O
w	O
,	O
show	O
that	O
the	O
dual	O
of	O
the	O
dual	O
formulation	O
is	O
given	O
by	O
the	O
original	O
representation	O
in	O
terms	O
of	O
the	O
parameter	O
vector	O
w.	O
6.2	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
in	O
this	O
exercise	O
,	O
we	O
develop	O
a	O
dual	O
formulation	O
of	O
the	O
perceptron	B
learning	O
algorithm	O
.	O
using	O
the	O
perceptron	B
learning	O
rule	O
(	O
4.55	O
)	O
,	O
show	O
that	O
the	O
learned	O
weight	B
vector	I
w	O
can	O
be	O
written	O
as	O
a	O
linear	O
combination	O
of	O
the	O
vectors	O
tnφ	O
(	O
xn	O
)	O
where	O
tn	O
∈	O
{	O
−1	O
,	O
+1	O
}	O
.	O
denote	O
the	O
coefﬁcients	O
of	O
this	O
linear	O
combination	O
by	O
αn	O
and	O
derive	O
a	O
formulation	O
of	O
the	O
perceptron	B
learning	O
algorithm	O
,	O
and	O
the	O
predictive	O
function	O
for	O
the	O
perceptron	B
,	O
in	O
terms	O
of	O
the	O
αn	O
.	O
show	O
that	O
the	O
feature	O
vector	O
φ	O
(	O
x	O
)	O
enters	O
only	O
in	O
the	O
form	O
of	O
the	O
kernel	B
function	I
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
φ	O
(	O
x	O
)	O
tφ	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
.	O
6.3	O
(	O
(	O
cid:12	O
)	O
)	O
the	O
nearest-neighbour	O
classiﬁer	O
(	O
section	O
2.5.2	O
)	O
assigns	O
a	O
new	O
input	O
vector	O
x	O
to	O
the	O
same	O
class	O
as	O
that	O
of	O
the	O
nearest	O
input	O
vector	O
xn	O
from	O
the	O
training	B
set	I
,	O
where	O
in	O
the	O
simplest	O
case	O
,	O
the	O
distance	O
is	O
deﬁned	O
by	O
the	O
euclidean	O
metric	O
(	O
cid:5	O
)	O
x	O
−	O
xn	O
(	O
cid:5	O
)	O
2.	O
by	O
expressing	O
this	O
rule	O
in	O
terms	O
of	O
scalar	O
products	O
and	O
then	O
making	O
use	O
of	O
kernel	O
sub-	O
stitution	O
,	O
formulate	O
the	O
nearest-neighbour	O
classiﬁer	O
for	O
a	O
general	O
nonlinear	O
kernel	O
.	O
6.4	O
(	O
(	O
cid:12	O
)	O
)	O
in	O
appendix	O
c	O
,	O
we	O
give	O
an	O
example	O
of	O
a	O
matrix	O
that	O
has	O
positive	O
elements	O
but	O
that	O
has	O
a	O
negative	O
eigenvalue	O
and	O
hence	O
that	O
is	O
not	O
positive	B
deﬁnite	I
.	O
find	O
an	O
example	O
of	O
the	O
converse	O
property	O
,	O
namely	O
a	O
2	O
×	O
2	O
matrix	O
with	O
positive	O
eigenvalues	O
yet	O
that	O
has	O
at	O
least	O
one	O
negative	O
element	O
.	O
6.5	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
verify	O
the	O
results	O
(	O
6.13	O
)	O
and	O
(	O
6.14	O
)	O
for	O
constructing	O
valid	O
kernels	O
.	O
6.6	O
(	O
(	O
cid:12	O
)	O
)	O
verify	O
the	O
results	O
(	O
6.15	O
)	O
and	O
(	O
6.16	O
)	O
for	O
constructing	O
valid	O
kernels	O
.	O
6.7	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
verify	O
the	O
results	O
(	O
6.17	O
)	O
and	O
(	O
6.18	O
)	O
for	O
constructing	O
valid	O
kernels	O
.	O
6.8	O
(	O
(	O
cid:12	O
)	O
)	O
verify	O
the	O
results	O
(	O
6.19	O
)	O
and	O
(	O
6.20	O
)	O
for	O
constructing	O
valid	O
kernels	O
.	O
6.9	O
(	O
(	O
cid:12	O
)	O
)	O
verify	O
the	O
results	O
(	O
6.21	O
)	O
and	O
(	O
6.22	O
)	O
for	O
constructing	O
valid	O
kernels	O
.	O
6.10	O
(	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
an	O
excellent	O
choice	O
of	O
kernel	O
for	O
learning	B
a	O
function	O
f	O
(	O
x	O
)	O
is	O
given	O
by	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
f	O
(	O
x	O
)	O
f	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
by	O
showing	O
that	O
a	O
linear	O
learning	O
machine	O
based	O
on	O
this	O
kernel	O
will	O
always	O
ﬁnd	O
a	O
solution	O
proportional	O
to	O
f	O
(	O
x	O
)	O
.	O
6.11	O
(	O
(	O
cid:12	O
)	O
)	O
by	O
making	O
use	O
of	O
the	O
expansion	O
(	O
6.25	O
)	O
,	O
and	O
then	O
expanding	O
the	O
middle	O
factor	O
as	O
a	O
power	O
series	O
,	O
show	O
that	O
the	O
gaussian	O
kernel	O
(	O
6.23	O
)	O
can	O
be	O
expressed	O
as	O
the	O
inner	O
product	O
of	O
an	O
inﬁnite-dimensional	O
feature	O
vector	O
.	O
exercises	O
321	O
6.12	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
the	O
space	O
of	O
all	O
possible	O
subsets	O
a	O
of	O
a	O
given	O
ﬁxed	O
set	O
d.	O
show	O
that	O
the	O
kernel	B
function	I
(	O
6.27	O
)	O
corresponds	O
to	O
an	O
inner	O
product	O
in	O
a	O
feature	B
space	I
of	O
dimensionality	O
2|d|	O
deﬁned	O
by	O
the	O
mapping	O
φ	O
(	O
a	O
)	O
where	O
a	O
is	O
a	O
subset	O
of	O
d	O
and	O
the	O
element	O
φu	O
(	O
a	O
)	O
,	O
indexed	O
by	O
the	O
subset	O
u	O
,	O
is	O
given	O
by	O
(	O
cid:12	O
)	O
φu	O
(	O
a	O
)	O
=	O
if	O
u	O
⊆	O
a	O
;	O
1	O
,	O
0	O
,	O
otherwise	O
.	O
(	O
6.95	O
)	O
here	O
u	O
⊆	O
a	O
denotes	O
that	O
u	O
is	O
either	O
a	O
subset	O
of	O
a	O
or	O
is	O
equal	O
to	O
a	O
.	O
6.13	O
(	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
the	O
fisher	O
kernel	O
,	O
deﬁned	O
by	O
(	O
6.33	O
)	O
,	O
remains	O
invariant	O
if	O
we	O
make	O
a	O
nonlinear	O
transformation	O
of	O
the	O
parameter	O
vector	O
θ	O
→	O
ψ	O
(	O
θ	O
)	O
,	O
where	O
the	O
function	O
ψ	O
(	O
·	O
)	O
is	O
invertible	O
and	O
differentiable	O
.	O
6.14	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
write	O
down	O
the	O
form	O
of	O
the	O
fisher	O
kernel	O
,	O
deﬁned	O
by	O
(	O
6.33	O
)	O
,	O
for	O
the	O
case	O
of	O
a	O
distribution	O
p	O
(	O
x|µ	O
)	O
=	O
n	O
(	O
x|µ	O
,	O
s	O
)	O
that	O
is	O
gaussian	O
with	O
mean	B
µ	O
and	O
ﬁxed	O
covariance	B
s.	O
6.15	O
(	O
(	O
cid:12	O
)	O
)	O
by	O
considering	O
the	O
determinant	O
of	O
a	O
2	O
×	O
2	O
gram	O
matrix	O
,	O
show	O
that	O
a	O
positive-	O
deﬁnite	O
kernel	B
function	I
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
satisﬁes	O
the	O
cauchy-schwartz	O
inequality	O
k	O
(	O
x1	O
,	O
x2	O
)	O
2	O
(	O
cid:1	O
)	O
k	O
(	O
x1	O
,	O
x1	O
)	O
k	O
(	O
x2	O
,	O
x2	O
)	O
.	O
(	O
6.96	O
)	O
6.16	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
parametric	O
model	O
governed	O
by	O
the	O
parameter	O
vector	O
w	O
together	O
with	O
a	O
data	O
set	O
of	O
input	O
values	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
and	O
a	O
nonlinear	O
feature	O
mapping	O
φ	O
(	O
x	O
)	O
.	O
suppose	O
that	O
the	O
dependence	O
of	O
the	O
error	B
function	I
on	O
w	O
takes	O
the	O
form	O
j	O
(	O
w	O
)	O
=	O
f	O
(	O
wtφ	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
wtφ	O
(	O
xn	O
)	O
)	O
+	O
g	O
(	O
wtw	O
)	O
(	O
6.97	O
)	O
where	O
g	O
(	O
·	O
)	O
is	O
a	O
monotonically	O
increasing	O
function	O
.	O
by	O
writing	O
w	O
in	O
the	O
form	O
n	O
(	O
cid:2	O
)	O
w	O
=	O
αnφ	O
(	O
xn	O
)	O
+	O
w⊥	O
(	O
6.98	O
)	O
n=1	O
show	O
that	O
the	O
value	O
of	O
w	O
that	O
minimizes	O
j	O
(	O
w	O
)	O
takes	O
the	O
form	O
of	O
a	O
linear	O
combination	O
of	O
the	O
basis	O
functions	O
φ	O
(	O
xn	O
)	O
for	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
6.17	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
the	O
sum-of-squares	B
error	I
function	O
(	O
6.39	O
)	O
for	O
data	O
having	O
noisy	O
inputs	O
,	O
where	O
ν	O
(	O
ξ	O
)	O
is	O
the	O
distribution	O
of	O
the	O
noise	O
.	O
use	O
the	O
calculus	O
of	O
vari-	O
ations	O
to	O
minimize	O
this	O
error	B
function	I
with	O
respect	O
to	O
the	O
function	O
y	O
(	O
x	O
)	O
,	O
and	O
hence	O
show	O
that	O
the	O
optimal	O
solution	O
is	O
given	O
by	O
an	O
expansion	O
of	O
the	O
form	O
(	O
6.40	O
)	O
in	O
which	O
the	O
basis	O
functions	O
are	O
given	O
by	O
(	O
6.41	O
)	O
.	O
322	O
6.	O
kernel	O
methods	O
6.18	O
(	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
nadaraya-watson	O
model	O
with	O
one	O
input	O
variable	O
x	O
and	O
one	O
target	O
variable	O
t	O
having	O
gaussian	O
components	O
with	O
isotropic	B
covariances	O
,	O
so	O
that	O
the	O
co-	O
variance	B
matrix	O
is	O
given	O
by	O
σ2i	O
where	O
i	O
is	O
the	O
unit	O
matrix	O
.	O
write	O
down	O
expressions	O
for	O
the	O
conditional	B
density	O
p	O
(	O
t|x	O
)	O
and	O
for	O
the	O
conditional	B
mean	O
e	O
[	O
t|x	O
]	O
and	O
variance	B
var	O
[	O
t|x	O
]	O
,	O
in	O
terms	O
of	O
the	O
kernel	B
function	I
k	O
(	O
x	O
,	O
xn	O
)	O
.	O
6.19	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
another	O
viewpoint	O
on	O
kernel	B
regression	I
comes	O
from	O
a	O
consideration	O
of	O
re-	O
gression	O
problems	O
in	O
which	O
the	O
input	O
variables	O
as	O
well	O
as	O
the	O
target	O
variables	O
are	O
corrupted	O
with	O
additive	O
noise	O
.	O
suppose	O
each	O
target	O
value	O
tn	O
is	O
generated	O
as	O
usual	O
by	O
taking	O
a	O
function	O
y	O
(	O
zn	O
)	O
evaluated	O
at	O
a	O
point	O
zn	O
,	O
and	O
adding	O
gaussian	O
noise	O
.	O
the	O
value	O
of	O
zn	O
is	O
not	O
directly	O
observed	O
,	O
however	O
,	O
but	O
only	O
a	O
noise	O
corrupted	O
version	O
xn	O
=	O
zn	O
+	O
ξn	O
where	O
the	O
random	O
variable	O
ξ	O
is	O
governed	O
by	O
some	O
distribution	O
g	O
(	O
ξ	O
)	O
.	O
consider	O
a	O
set	O
of	O
observations	O
{	O
xn	O
,	O
tn	O
}	O
,	O
where	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
together	O
with	O
a	O
cor-	O
responding	O
sum-of-squares	B
error	I
function	O
deﬁned	O
by	O
averaging	O
over	O
the	O
distribution	O
of	O
input	O
noise	O
to	O
give	O
(	O
cid:6	O
)	O
n	O
(	O
cid:2	O
)	O
e	O
=	O
1	O
2	O
n=1	O
{	O
y	O
(	O
xn	O
−	O
ξn	O
)	O
−	O
tn	O
}	O
2	O
g	O
(	O
ξn	O
)	O
dξn	O
.	O
(	O
6.99	O
)	O
by	O
minimizing	O
e	O
with	O
respect	O
to	O
the	O
function	O
y	O
(	O
z	O
)	O
using	O
the	O
calculus	B
of	I
variations	I
(	O
appendix	O
d	O
)	O
,	O
show	O
that	O
optimal	O
solution	O
for	O
y	O
(	O
x	O
)	O
is	O
given	O
by	O
a	O
nadaraya-watson	O
kernel	B
regression	I
solution	O
of	O
the	O
form	O
(	O
6.45	O
)	O
with	O
a	O
kernel	O
of	O
the	O
form	O
(	O
6.46	O
)	O
.	O
6.20	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
verify	O
the	O
results	O
(	O
6.66	O
)	O
and	O
(	O
6.67	O
)	O
.	O
6.21	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
gaussian	O
process	O
regression	B
model	O
in	O
which	O
the	O
kernel	B
function	I
is	O
deﬁned	O
in	O
terms	O
of	O
a	O
ﬁxed	O
set	O
of	O
nonlinear	O
basis	O
functions	O
.	O
show	O
that	O
the	O
predictive	B
distribution	I
is	O
identical	O
to	O
the	O
result	O
(	O
3.58	O
)	O
obtained	O
in	O
section	O
3.3.2	O
for	O
the	O
bayesian	O
linear	B
regression	I
model	O
.	O
to	O
do	O
this	O
,	O
note	O
that	O
both	O
models	O
have	O
gaussian	O
predictive	O
distributions	O
,	O
and	O
so	O
it	O
is	O
only	O
necessary	O
to	O
show	O
that	O
the	O
conditional	B
mean	O
and	O
variance	B
are	O
the	O
same	O
.	O
for	O
the	O
mean	B
,	O
make	O
use	O
of	O
the	O
matrix	O
identity	O
(	O
c.6	O
)	O
,	O
and	O
for	O
the	O
variance	B
,	O
make	O
use	O
of	O
the	O
matrix	O
identity	O
(	O
c.7	O
)	O
.	O
6.22	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
regression	B
problem	O
with	O
n	O
training	B
set	I
input	O
vectors	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
and	O
l	O
test	B
set	I
input	O
vectors	O
xn	O
+1	O
,	O
.	O
.	O
.	O
,	O
xn	O
+l	O
,	O
and	O
suppose	O
we	O
deﬁne	O
a	O
gaussian	O
process	O
prior	B
over	O
functions	O
t	O
(	O
x	O
)	O
.	O
derive	O
an	O
expression	O
for	O
the	O
joint	O
predictive	O
dis-	O
tribution	O
for	O
t	O
(	O
xn	O
+1	O
)	O
,	O
.	O
.	O
.	O
,	O
t	O
(	O
xn	O
+l	O
)	O
,	O
given	O
the	O
values	O
of	O
t	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
t	O
(	O
xn	O
)	O
.	O
show	O
the	O
marginal	B
of	O
this	O
distribution	O
for	O
one	O
of	O
the	O
test	O
observations	O
tj	O
where	O
n	O
+	O
1	O
(	O
cid:1	O
)	O
j	O
(	O
cid:1	O
)	O
n	O
+	O
l	O
is	O
given	O
by	O
the	O
usual	O
gaussian	O
process	O
regression	B
result	O
(	O
6.66	O
)	O
and	O
(	O
6.67	O
)	O
.	O
6.23	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
gaussian	O
process	O
regression	B
model	O
in	O
which	O
the	O
target	O
variable	O
t	O
has	O
dimensionality	O
d.	O
write	O
down	O
the	O
conditional	B
distribution	O
of	O
tn	O
+1	O
for	O
a	O
test	O
input	O
vector	O
xn	O
+1	O
,	O
given	O
a	O
training	B
set	I
of	O
input	O
vectors	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
+1	O
and	O
corresponding	O
target	O
observations	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
.	O
6.24	O
(	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
a	O
diagonal	B
matrix	O
w	O
whose	O
elements	O
satisfy	O
0	O
<	O
wii	O
<	O
1	O
is	O
positive	B
deﬁnite	I
.	O
show	O
that	O
the	O
sum	O
of	O
two	O
positive	B
deﬁnite	I
matrices	O
is	O
itself	O
positive	B
deﬁnite	I
.	O
exercises	O
323	O
6.25	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
using	O
the	O
newton-raphson	O
formula	O
(	O
4.92	O
)	O
,	O
derive	O
the	O
iterative	O
update	O
n	O
of	O
the	O
posterior	O
distribution	O
in	O
the	O
gaussian	O
formula	O
(	O
6.83	O
)	O
for	O
ﬁnding	O
the	O
mode	O
a	O
(	O
cid:1	O
)	O
process	O
classiﬁcation	B
model	O
.	O
6.26	O
(	O
(	O
cid:12	O
)	O
)	O
using	O
the	O
result	O
(	O
2.115	O
)	O
,	O
derive	O
the	O
expressions	O
(	O
6.87	O
)	O
and	O
(	O
6.88	O
)	O
for	O
the	O
mean	B
and	O
variance	B
of	O
the	O
posterior	O
distribution	O
p	O
(	O
an	O
+1|tn	O
)	O
in	O
the	O
gaussian	O
process	O
clas-	O
siﬁcation	O
model	O
.	O
6.27	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
derive	O
the	O
result	O
(	O
6.90	O
)	O
for	O
the	O
log	O
likelihood	O
function	O
in	O
the	O
laplace	O
approx-	O
imation	O
framework	O
for	O
gaussian	O
process	O
classiﬁcation	B
.	O
similarly	O
,	O
derive	O
the	O
results	O
(	O
6.91	O
)	O
,	O
(	O
6.92	O
)	O
,	O
and	O
(	O
6.94	O
)	O
for	O
the	O
terms	O
in	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	O
.	O
7	O
sparse	O
kernel	O
machines	O
in	O
the	O
previous	O
chapter	O
,	O
we	O
explored	O
a	O
variety	O
of	O
learning	B
algorithms	O
based	O
on	O
non-	O
linear	O
kernels	O
.	O
one	O
of	O
the	O
signiﬁcant	O
limitations	O
of	O
many	O
such	O
algorithms	O
is	O
that	O
the	O
kernel	B
function	I
k	O
(	O
xn	O
,	O
xm	O
)	O
must	O
be	O
evaluated	O
for	O
all	O
possible	O
pairs	O
xn	O
and	O
xm	O
of	O
training	B
points	O
,	O
which	O
can	O
be	O
computationally	O
infeasible	O
during	O
training	B
and	O
can	O
lead	O
to	O
excessive	O
computation	O
times	O
when	O
making	O
predictions	O
for	O
new	O
data	O
points	O
.	O
in	O
this	O
chapter	O
we	O
shall	O
look	O
at	O
kernel-based	O
algorithms	O
that	O
have	O
sparse	O
solutions	O
,	O
so	O
that	O
predictions	O
for	O
new	O
inputs	O
depend	O
only	O
on	O
the	O
kernel	B
function	I
evaluated	O
at	O
a	O
subset	O
of	O
the	O
training	B
data	O
points	O
.	O
we	O
begin	O
by	O
looking	O
in	O
some	O
detail	O
at	O
the	O
support	B
vector	I
machine	I
(	O
svm	O
)	O
,	O
which	O
became	O
popular	O
in	O
some	O
years	O
ago	O
for	O
solving	O
problems	O
in	O
classiﬁcation	B
,	O
regression	B
,	O
and	O
novelty	B
detection	I
.	O
an	O
important	O
property	O
of	O
support	B
vector	I
machines	O
is	O
that	O
the	O
determination	O
of	O
the	O
model	O
parameters	O
corresponds	O
to	O
a	O
convex	O
optimization	O
prob-	O
lem	O
,	O
and	O
so	O
any	O
local	B
solution	O
is	O
also	O
a	O
global	O
optimum	O
.	O
because	O
the	O
discussion	O
of	O
support	B
vector	I
machines	O
makes	O
extensive	O
use	O
of	O
lagrange	O
multipliers	O
,	O
the	O
reader	O
is	O
325	O
326	O
7.	O
sparse	O
kernel	O
machines	O
encouraged	O
to	O
review	O
the	O
key	O
concepts	O
covered	O
in	O
appendix	O
e.	O
additional	O
infor-	O
mation	B
on	O
support	B
vector	I
machines	O
can	O
be	O
found	O
in	O
vapnik	O
(	O
1995	O
)	O
,	O
burges	O
(	O
1998	O
)	O
,	O
cristianini	O
and	O
shawe-taylor	O
(	O
2000	O
)	O
,	O
m¨uller	O
et	O
al	O
.	O
(	O
2001	O
)	O
,	O
sch¨olkopf	O
and	O
smola	O
(	O
2002	O
)	O
,	O
and	O
herbrich	O
(	O
2002	O
)	O
.	O
the	O
svm	O
is	O
a	O
decision	O
machine	O
and	O
so	O
does	O
not	O
provide	O
posterior	O
probabilities	O
.	O
we	O
have	O
already	O
discussed	O
some	O
of	O
the	O
beneﬁts	O
of	O
determining	O
probabilities	O
in	O
sec-	O
tion	O
1.5.4.	O
an	O
alternative	O
sparse	O
kernel	O
technique	O
,	O
known	O
as	O
the	O
relevance	B
vector	I
machine	I
(	O
rvm	O
)	O
,	O
is	O
based	O
on	O
a	O
bayesian	O
formulation	O
and	O
provides	O
posterior	O
proba-	O
bilistic	O
outputs	O
,	O
as	O
well	O
as	O
having	O
typically	O
much	O
sparser	O
solutions	O
than	O
the	O
svm	O
.	O
section	O
7.2	O
7.1.	O
maximum	B
margin	I
classiﬁers	O
we	O
begin	O
our	O
discussion	O
of	O
support	B
vector	I
machines	O
by	O
returning	O
to	O
the	O
two-class	O
classiﬁcation	B
problem	O
using	O
linear	O
models	O
of	O
the	O
form	O
y	O
(	O
x	O
)	O
=	O
wtφ	O
(	O
x	O
)	O
+	O
b	O
(	O
7.1	O
)	O
where	O
φ	O
(	O
x	O
)	O
denotes	O
a	O
ﬁxed	O
feature-space	O
transformation	O
,	O
and	O
we	O
have	O
made	O
the	O
bias	B
parameter	I
b	O
explicit	O
.	O
note	O
that	O
we	O
shall	O
shortly	O
introduce	O
a	O
dual	B
representation	I
expressed	O
in	O
terms	O
of	O
kernel	O
functions	O
,	O
which	O
avoids	O
having	O
to	O
work	O
explicitly	O
in	O
feature	B
space	I
.	O
the	O
training	B
data	O
set	O
comprises	O
n	O
input	O
vectors	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
with	O
corresponding	O
target	O
values	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
where	O
tn	O
∈	O
{	O
−1	O
,	O
1	O
}	O
,	O
and	O
new	O
data	O
points	O
x	O
are	O
classiﬁed	O
according	O
to	O
the	O
sign	O
of	O
y	O
(	O
x	O
)	O
.	O
we	O
shall	O
assume	O
for	O
the	O
moment	O
that	O
the	O
training	B
data	O
set	O
is	O
linearly	B
separable	I
in	O
feature	B
space	I
,	O
so	O
that	O
by	O
deﬁnition	O
there	O
exists	O
at	O
least	O
one	O
choice	O
of	O
the	O
parameters	O
w	O
and	O
b	O
such	O
that	O
a	O
function	O
of	O
the	O
form	O
(	O
7.1	O
)	O
satisﬁes	O
y	O
(	O
xn	O
)	O
>	O
0	O
for	O
points	O
having	O
tn	O
=	O
+1	O
and	O
y	O
(	O
xn	O
)	O
<	O
0	O
for	O
points	O
having	O
tn	O
=	O
−1	O
,	O
so	O
that	O
tny	O
(	O
xn	O
)	O
>	O
0	O
for	O
all	O
training	B
data	O
points	O
.	O
there	O
may	O
of	O
course	O
exist	O
many	O
such	O
solutions	O
that	O
separate	O
the	O
classes	O
exactly	O
.	O
in	O
section	O
4.1.7	O
,	O
we	O
described	O
the	O
perceptron	B
algorithm	O
that	O
is	O
guaranteed	O
to	O
ﬁnd	O
a	O
solution	O
in	O
a	O
ﬁnite	O
number	O
of	O
steps	O
.	O
the	O
solution	O
that	O
it	O
ﬁnds	O
,	O
however	O
,	O
will	O
be	O
dependent	O
on	O
the	O
(	O
arbitrary	O
)	O
initial	O
values	O
chosen	O
for	O
w	O
and	O
b	O
as	O
well	O
as	O
on	O
the	O
order	O
in	O
which	O
the	O
data	O
points	O
are	O
presented	O
.	O
if	O
there	O
are	O
multiple	O
solutions	O
all	O
of	O
which	O
classify	O
the	O
training	B
data	O
set	O
exactly	O
,	O
then	O
we	O
should	O
try	O
to	O
ﬁnd	O
the	O
one	O
that	O
will	O
give	O
the	O
smallest	O
generalization	B
error	O
.	O
the	O
support	B
vector	I
machine	I
approaches	O
this	O
problem	O
through	O
the	O
concept	O
of	O
the	O
margin	B
,	O
which	O
is	O
deﬁned	O
to	O
be	O
the	O
smallest	O
distance	O
between	O
the	O
decision	B
boundary	I
and	O
any	O
of	O
the	O
samples	O
,	O
as	O
illustrated	O
in	O
figure	O
7.1.	O
in	O
support	B
vector	I
machines	O
the	O
decision	B
boundary	I
is	O
chosen	O
to	O
be	O
the	O
one	O
for	O
which	O
the	O
margin	B
is	O
maximized	O
.	O
the	O
maximum	B
margin	I
solution	O
can	O
be	O
motivated	O
us-	O
ing	O
computational	O
learning	B
theory	O
,	O
also	O
known	O
as	O
statistical	B
learning	I
theory	I
.	O
how-	O
ever	O
,	O
a	O
simple	O
insight	O
into	O
the	O
origins	O
of	O
maximum	B
margin	I
has	O
been	O
given	O
by	O
tong	O
and	O
koller	O
(	O
2000	O
)	O
who	O
consider	O
a	O
framework	O
for	O
classiﬁcation	O
based	O
on	O
a	O
hybrid	O
of	O
generative	O
and	O
discriminative	O
approaches	O
.	O
they	O
ﬁrst	O
model	O
the	O
distribution	O
over	O
in-	O
put	O
vectors	O
x	O
for	O
each	O
class	O
using	O
a	O
parzen	O
density	B
estimator	O
with	O
gaussian	O
kernels	O
section	O
7.1.5	O
y	O
=	O
1	O
y	O
=	O
0	O
y	O
=	O
−1	O
7.1.	O
maximum	B
margin	I
classiﬁers	O
327	O
y	O
=	O
−1	O
y	O
=	O
0	O
y	O
=	O
1	O
margin	B
figure	O
7.1	O
the	O
margin	B
is	O
deﬁned	O
as	O
the	O
perpendicular	O
distance	O
between	O
the	O
decision	B
boundary	I
and	O
the	O
closest	O
of	O
the	O
data	O
points	O
,	O
as	O
shown	O
on	O
the	O
left	O
ﬁgure	O
.	O
maximizing	O
the	O
margin	B
leads	O
to	O
a	O
particular	O
choice	O
of	O
decision	B
boundary	I
,	O
as	O
shown	O
on	O
the	O
right	O
.	O
the	O
location	O
of	O
this	O
boundary	O
is	O
determined	O
by	O
a	O
subset	O
of	O
the	O
data	O
points	O
,	O
known	O
as	O
support	O
vectors	O
,	O
which	O
are	O
indicated	O
by	O
the	O
circles	O
.	O
having	O
a	O
common	O
parameter	O
σ2	O
.	O
together	O
with	O
the	O
class	O
priors	O
,	O
this	O
deﬁnes	O
an	O
opti-	O
mal	O
misclassiﬁcation-rate	O
decision	B
boundary	I
.	O
however	O
,	O
instead	O
of	O
using	O
this	O
optimal	O
boundary	O
,	O
they	O
determine	O
the	O
best	O
hyperplane	O
by	O
minimizing	O
the	O
probability	B
of	O
error	B
relative	O
to	O
the	O
learned	O
density	B
model	O
.	O
in	O
the	O
limit	O
σ2	O
→	O
0	O
,	O
the	O
optimal	O
hyperplane	O
is	O
shown	O
to	O
be	O
the	O
one	O
having	O
maximum	B
margin	I
.	O
the	O
intuition	O
behind	O
this	O
result	O
is	O
that	O
as	O
σ2	O
is	O
reduced	O
,	O
the	O
hyperplane	O
is	O
increasingly	O
dominated	O
by	O
nearby	O
data	O
points	O
relative	B
to	O
more	O
distant	O
ones	O
.	O
in	O
the	O
limit	O
,	O
the	O
hyperplane	O
becomes	O
independent	B
of	O
data	O
points	O
that	O
are	O
not	O
support	O
vectors	O
.	O
we	O
shall	O
see	O
in	O
figure	O
10.13	O
that	O
marginalization	O
with	O
respect	O
to	O
the	O
prior	B
distri-	O
bution	O
of	O
the	O
parameters	O
in	O
a	O
bayesian	O
approach	O
for	O
a	O
simple	O
linearly	B
separable	I
data	O
set	O
leads	O
to	O
a	O
decision	B
boundary	I
that	O
lies	O
in	O
the	O
middle	O
of	O
the	O
region	O
separating	O
the	O
data	O
points	O
.	O
the	O
large	B
margin	I
solution	O
has	O
similar	O
behaviour	O
.	O
recall	O
from	O
figure	O
4.1	O
that	O
the	O
perpendicular	O
distance	O
of	O
a	O
point	O
x	O
from	O
a	O
hyper-	O
plane	O
deﬁned	O
by	O
y	O
(	O
x	O
)	O
=	O
0	O
where	O
y	O
(	O
x	O
)	O
takes	O
the	O
form	O
(	O
7.1	O
)	O
is	O
given	O
by	O
|y	O
(	O
x	O
)	O
|/	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
.	O
furthermore	O
,	O
we	O
are	O
only	O
interested	O
in	O
solutions	O
for	O
which	O
all	O
data	O
points	O
are	O
cor-	O
rectly	O
classiﬁed	O
,	O
so	O
that	O
tny	O
(	O
xn	O
)	O
>	O
0	O
for	O
all	O
n.	O
thus	O
the	O
distance	O
of	O
a	O
point	O
xn	O
to	O
the	O
decision	B
surface	I
is	O
given	O
by	O
tny	O
(	O
xn	O
)	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
=	O
tn	O
(	O
wtφ	O
(	O
xn	O
)	O
+	O
b	O
)	O
(	O
cid:12	O
)	O
(	O
cid:8	O
)	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
(	O
cid:9	O
)	O
(	O
cid:13	O
)	O
.	O
(	O
7.2	O
)	O
the	O
margin	B
is	O
given	O
by	O
the	O
perpendicular	O
distance	O
to	O
the	O
closest	O
point	O
xn	O
from	O
the	O
data	O
set	O
,	O
and	O
we	O
wish	O
to	O
optimize	O
the	O
parameters	O
w	O
and	O
b	O
in	O
order	O
to	O
maximize	O
this	O
distance	O
.	O
thus	O
the	O
maximum	B
margin	I
solution	O
is	O
found	O
by	O
solving	O
arg	O
max	O
(	O
7.3	O
)	O
where	O
we	O
have	O
taken	O
the	O
factor	O
1/	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
outside	O
the	O
optimization	O
over	O
n	O
because	O
w	O
wtφ	O
(	O
xn	O
)	O
+	O
b	O
tn	O
w	O
,	O
b	O
1	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
min	O
n	O
328	O
7.	O
sparse	O
kernel	O
machines	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
(	O
cid:10	O
)	O
does	O
not	O
depend	O
on	O
n.	O
direct	O
solution	O
of	O
this	O
optimization	O
problem	O
would	O
be	O
very	O
complex	O
,	O
and	O
so	O
we	O
shall	O
convert	O
it	O
into	O
an	O
equivalent	O
problem	O
that	O
is	O
much	O
easier	O
to	O
solve	O
.	O
to	O
do	O
this	O
we	O
note	O
that	O
if	O
we	O
make	O
the	O
rescaling	O
w	O
→	O
κw	O
and	O
b	O
→	O
κb	O
,	O
then	O
the	O
distance	O
from	O
any	O
point	O
xn	O
to	O
the	O
decision	B
surface	I
,	O
given	O
by	O
tny	O
(	O
xn	O
)	O
/	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
,	O
is	O
unchanged	O
.	O
we	O
can	O
use	O
this	O
freedom	O
to	O
set	O
(	O
cid:11	O
)	O
tn	O
wtφ	O
(	O
xn	O
)	O
+	O
b	O
=	O
1	O
(	O
7.4	O
)	O
tn	O
(	O
cid:2	O
)	O
1	O
,	O
for	O
the	O
point	O
that	O
is	O
closest	O
to	O
the	O
surface	O
.	O
in	O
this	O
case	O
,	O
all	O
data	O
points	O
will	O
satisfy	O
the	O
constraints	O
wtφ	O
(	O
xn	O
)	O
+	O
b	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
(	O
7.5	O
)	O
this	O
is	O
known	O
as	O
the	O
canonical	O
representation	O
of	O
the	O
decision	O
hyperplane	O
.	O
in	O
the	O
case	O
of	O
data	O
points	O
for	O
which	O
the	O
equality	O
holds	O
,	O
the	O
constraints	O
are	O
said	O
to	O
be	O
active	O
,	O
whereas	O
for	O
the	O
remainder	O
they	O
are	O
said	O
to	O
be	O
inactive	O
.	O
by	O
deﬁnition	O
,	O
there	O
will	O
always	O
be	O
at	O
least	O
one	O
active	B
constraint	I
,	O
because	O
there	O
will	O
always	O
be	O
a	O
closest	O
point	O
,	O
and	O
once	O
the	O
margin	B
has	O
been	O
maximized	O
there	O
will	O
be	O
at	O
least	O
two	O
active	O
constraints	O
.	O
the	O
optimization	O
problem	O
then	O
simply	O
requires	O
that	O
we	O
maximize	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
−1	O
,	O
which	O
is	O
equivalent	O
to	O
minimizing	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
2	O
,	O
and	O
so	O
we	O
have	O
to	O
solve	O
the	O
optimization	O
problem	O
arg	O
min	O
w	O
,	O
b	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
2	O
1	O
2	O
(	O
7.6	O
)	O
subject	O
to	O
the	O
constraints	O
given	O
by	O
(	O
7.5	O
)	O
.	O
the	O
factor	O
of	O
1/2	O
in	O
(	O
7.6	O
)	O
is	O
included	O
for	O
later	O
convenience	O
.	O
this	O
is	O
an	O
example	O
of	O
a	O
quadratic	O
programming	O
problem	O
in	O
which	O
we	O
are	O
trying	O
to	O
minimize	O
a	O
quadratic	O
function	O
subject	O
to	O
a	O
set	O
of	O
linear	O
inequality	O
constraints	O
.	O
it	O
appears	O
that	O
the	O
bias	B
parameter	I
b	O
has	O
disappeared	O
from	O
the	O
optimiza-	O
tion	O
.	O
however	O
,	O
it	O
is	O
determined	O
implicitly	O
via	O
the	O
constraints	O
,	O
because	O
these	O
require	O
that	O
changes	O
to	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
be	O
compensated	O
by	O
changes	O
to	O
b.	O
we	O
shall	O
see	O
how	O
this	O
works	O
shortly	O
.	O
in	O
order	O
to	O
solve	O
this	O
constrained	O
optimization	O
problem	O
,	O
we	O
introduce	O
lagrange	O
multipliers	O
an	O
(	O
cid:2	O
)	O
0	O
,	O
with	O
one	O
multiplier	O
an	O
for	O
each	O
of	O
the	O
constraints	O
in	O
(	O
7.5	O
)	O
,	O
giving	O
the	O
lagrangian	O
function	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
2	O
−	O
n	O
(	O
cid:2	O
)	O
n=1	O
(	O
cid:26	O
)	O
l	O
(	O
w	O
,	O
b	O
,	O
a	O
)	O
=	O
1	O
2	O
(	O
cid:27	O
)	O
tn	O
(	O
wtφ	O
(	O
xn	O
)	O
+	O
b	O
)	O
−	O
1	O
an	O
(	O
7.7	O
)	O
appendix	O
e	O
where	O
a	O
=	O
(	O
a1	O
,	O
.	O
.	O
.	O
,	O
an	O
)	O
t.	O
note	O
the	O
minus	O
sign	O
in	O
front	O
of	O
the	O
lagrange	O
multiplier	O
term	O
,	O
because	O
we	O
are	O
minimizing	O
with	O
respect	O
to	O
w	O
and	O
b	O
,	O
and	O
maximizing	O
with	O
respect	O
to	O
a.	O
setting	O
the	O
derivatives	O
of	O
l	O
(	O
w	O
,	O
b	O
,	O
a	O
)	O
with	O
respect	O
to	O
w	O
and	O
b	O
equal	O
to	O
zero	O
,	O
we	O
obtain	O
the	O
following	O
two	O
conditions	O
antnφ	O
(	O
xn	O
)	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
antn	O
.	O
n=1	O
w	O
=	O
0	O
=	O
(	O
7.8	O
)	O
(	O
7.9	O
)	O
7.1.	O
maximum	B
margin	I
classiﬁers	O
329	O
eliminating	O
w	O
and	O
b	O
from	O
l	O
(	O
w	O
,	O
b	O
,	O
a	O
)	O
using	O
these	O
conditions	O
then	O
gives	O
the	O
dual	B
representation	I
of	O
the	O
maximum	B
margin	I
problem	O
in	O
which	O
we	O
maximize	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
an	O
−	O
1	O
2	O
n	O
(	O
cid:2	O
)	O
(	O
cid:4	O
)	O
l	O
(	O
a	O
)	O
=	O
n	O
(	O
cid:2	O
)	O
n=1	O
with	O
respect	O
to	O
a	O
subject	O
to	O
the	O
constraints	O
n=1	O
m=1	O
anamtntmk	O
(	O
xn	O
,	O
xm	O
)	O
(	O
7.10	O
)	O
an	O
(	O
cid:2	O
)	O
0	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
antn	O
=	O
0	O
.	O
(	O
7.11	O
)	O
(	O
7.12	O
)	O
n=1	O
here	O
the	O
kernel	B
function	I
is	O
deﬁned	O
by	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
φ	O
(	O
x	O
)	O
tφ	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
.	O
again	O
,	O
this	O
takes	O
the	O
form	O
of	O
a	O
quadratic	O
programming	O
problem	O
in	O
which	O
we	O
optimize	O
a	O
quadratic	O
function	O
of	O
a	O
subject	O
to	O
a	O
set	O
of	O
inequality	O
constraints	O
.	O
we	O
shall	O
discuss	O
techniques	O
for	O
solving	O
such	O
quadratic	O
programming	O
problems	O
in	O
section	O
7.1.1.	O
the	O
solution	O
to	O
a	O
quadratic	O
programming	O
problem	O
in	O
m	O
variables	O
in	O
general	O
has	O
computational	O
complexity	O
that	O
is	O
o	O
(	O
m	O
3	O
)	O
.	O
in	O
going	O
to	O
the	O
dual	O
formulation	O
we	O
have	O
turned	O
the	O
original	O
optimization	O
problem	O
,	O
which	O
involved	O
minimizing	O
(	O
7.6	O
)	O
over	O
m	O
variables	O
,	O
into	O
the	O
dual	O
problem	O
(	O
7.10	O
)	O
,	O
which	O
has	O
n	O
variables	O
.	O
for	O
a	O
ﬁxed	O
set	O
of	O
basis	O
functions	O
whose	O
number	O
m	O
is	O
smaller	O
than	O
the	O
number	O
n	O
of	O
data	O
points	O
,	O
the	O
move	O
to	O
the	O
dual	O
problem	O
appears	O
disadvantageous	O
.	O
however	O
,	O
it	O
allows	O
the	O
model	O
to	O
be	O
reformulated	O
using	O
kernels	O
,	O
and	O
so	O
the	O
maximum	B
margin	I
classiﬁer	O
can	O
be	O
applied	O
efﬁciently	O
to	O
feature	O
spaces	O
whose	O
dimensionality	O
exceeds	O
the	O
number	O
of	O
data	O
points	O
,	O
including	O
inﬁnite	O
feature	O
spaces	O
.	O
the	O
kernel	O
formulation	O
also	O
makes	O
clear	O
the	O
role	O
of	O
the	O
constraint	O
that	O
the	O
kernel	B
function	I
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
be	O
positive	B
deﬁnite	I
,	O
because	O
this	O
ensures	O
that	O
the	O
lagrangian	O
function	O
(	O
cid:4	O
)	O
l	O
(	O
a	O
)	O
is	O
bounded	O
below	O
,	O
giving	O
rise	O
to	O
a	O
well-	O
deﬁned	O
optimization	O
problem	O
.	O
in	O
order	O
to	O
classify	O
new	O
data	O
points	O
using	O
the	O
trained	O
model	O
,	O
we	O
evaluate	O
the	O
sign	O
of	O
y	O
(	O
x	O
)	O
deﬁned	O
by	O
(	O
7.1	O
)	O
.	O
this	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
parameters	O
{	O
an	O
}	O
and	O
the	O
kernel	B
function	I
by	O
substituting	O
for	O
w	O
using	O
(	O
7.8	O
)	O
to	O
give	O
n	O
(	O
cid:2	O
)	O
y	O
(	O
x	O
)	O
=	O
antnk	O
(	O
x	O
,	O
xn	O
)	O
+	O
b	O
.	O
(	O
7.13	O
)	O
n=1	O
joseph-louis	O
lagrange	O
1736–1813	O
although	O
widely	O
considered	O
to	O
be	O
a	O
french	O
mathematician	O
,	O
lagrange	O
was	O
born	O
in	O
turin	O
in	O
italy	O
.	O
by	O
the	O
age	O
of	O
nineteen	O
,	O
he	O
had	O
already	O
made	O
important	O
contributions	O
mathemat-	O
ics	O
and	O
had	O
been	O
appointed	O
as	O
pro-	O
fessor	O
at	O
the	O
royal	O
artillery	O
school	O
in	O
turin	O
.	O
for	O
many	O
years	O
,	O
euler	O
worked	O
hard	O
to	O
persuade	O
lagrange	O
to	O
move	O
to	O
berlin	O
,	O
which	O
he	O
eventually	O
did	O
in	O
1766	O
where	O
he	O
succeeded	O
euler	O
as	O
director	O
of	O
mathematics	O
at	O
the	O
berlin	O
academy	O
.	O
later	O
he	O
moved	O
to	O
paris	O
,	O
nar-	O
rowly	O
escaping	O
with	O
his	O
life	O
during	O
the	O
french	O
revo-	O
lution	O
thanks	O
to	O
the	O
personal	O
intervention	O
of	O
lavoisier	O
(	O
the	O
french	O
chemist	O
who	O
discovered	O
oxygen	O
)	O
who	O
him-	O
self	O
was	O
later	O
executed	O
at	O
the	O
guillotine	O
.	O
lagrange	O
made	O
key	O
contributions	O
to	O
the	O
calculus	B
of	I
variations	I
and	O
the	O
foundations	O
of	O
dynamics	O
.	O
330	O
7.	O
sparse	O
kernel	O
machines	O
in	O
appendix	O
e	O
,	O
we	O
show	O
that	O
a	O
constrained	O
optimization	O
of	O
this	O
form	O
satisﬁes	O
the	O
karush-kuhn-tucker	O
(	O
kkt	O
)	O
conditions	O
,	O
which	O
in	O
this	O
case	O
require	O
that	O
the	O
following	O
three	O
properties	O
hold	O
an	O
(	O
cid:2	O
)	O
0	O
tny	O
(	O
xn	O
)	O
−	O
1	O
(	O
cid:2	O
)	O
0	O
an	O
{	O
tny	O
(	O
xn	O
)	O
−	O
1	O
}	O
=	O
0	O
.	O
(	O
7.14	O
)	O
(	O
7.15	O
)	O
(	O
7.16	O
)	O
thus	O
for	O
every	O
data	O
point	O
,	O
either	O
an	O
=	O
0	O
or	O
tny	O
(	O
xn	O
)	O
=	O
1.	O
any	O
data	O
point	O
for	O
which	O
an	O
=	O
0	O
will	O
not	O
appear	O
in	O
the	O
sum	O
in	O
(	O
7.13	O
)	O
and	O
hence	O
plays	O
no	O
role	O
in	O
making	O
predictions	O
for	O
new	O
data	O
points	O
.	O
the	O
remaining	O
data	O
points	O
are	O
called	O
support	O
vectors	O
,	O
and	O
because	O
they	O
satisfy	O
tny	O
(	O
xn	O
)	O
=	O
1	O
,	O
they	O
correspond	O
to	O
points	O
that	O
lie	O
on	O
the	O
maximum	B
margin	I
hyperplanes	O
in	O
feature	B
space	I
,	O
as	O
illustrated	O
in	O
figure	O
7.1.	O
this	O
property	O
is	O
central	O
to	O
the	O
practical	O
applicability	O
of	O
support	B
vector	I
machines	O
.	O
once	O
the	O
model	O
is	O
trained	O
,	O
a	O
signiﬁcant	O
proportion	O
of	O
the	O
data	O
points	O
can	O
be	O
discarded	O
and	O
only	O
the	O
support	O
vectors	O
retained	O
.	O
having	O
solved	O
the	O
quadratic	O
programming	O
problem	O
and	O
found	O
a	O
value	O
for	O
a	O
,	O
we	O
can	O
then	O
determine	O
the	O
value	O
of	O
the	O
threshold	B
parameter	I
b	O
by	O
noting	O
that	O
any	O
support	B
vector	I
xn	O
satisﬁes	O
tny	O
(	O
xn	O
)	O
=	O
1.	O
using	O
(	O
7.13	O
)	O
this	O
gives	O
(	O
cid:23	O
)	O
amtmk	O
(	O
xn	O
,	O
xm	O
)	O
+	O
b	O
=	O
1	O
(	O
7.17	O
)	O
(	O
cid:22	O
)	O
(	O
cid:2	O
)	O
tn	O
m∈s	O
where	O
s	O
denotes	O
the	O
set	O
of	O
indices	O
of	O
the	O
support	O
vectors	O
.	O
although	O
we	O
can	O
solve	O
this	O
equation	O
for	O
b	O
using	O
an	O
arbitrarily	O
chosen	O
support	B
vector	I
xn	O
,	O
a	O
numerically	O
more	O
n	O
=	O
1	O
,	O
stable	O
solution	O
is	O
obtained	O
by	O
ﬁrst	O
multiplying	O
through	O
by	O
tn	O
,	O
making	O
use	O
of	O
t2	O
and	O
then	O
averaging	O
these	O
equations	O
over	O
all	O
support	O
vectors	O
and	O
solving	O
for	O
b	O
to	O
give	O
(	O
cid:23	O
)	O
(	O
cid:22	O
)	O
(	O
cid:2	O
)	O
n∈s	O
(	O
cid:2	O
)	O
m∈s	O
b	O
=	O
1	O
ns	O
tn	O
−	O
amtmk	O
(	O
xn	O
,	O
xm	O
)	O
(	O
7.18	O
)	O
n	O
(	O
cid:2	O
)	O
where	O
ns	O
is	O
the	O
total	O
number	O
of	O
support	O
vectors	O
.	O
for	O
later	O
comparison	O
with	O
alternative	O
models	O
,	O
we	O
can	O
express	O
the	O
maximum-	O
margin	B
classiﬁer	O
in	O
terms	O
of	O
the	O
minimization	O
of	O
an	O
error	B
function	I
,	O
with	O
a	O
simple	O
quadratic	O
regularizer	O
,	O
in	O
the	O
form	O
e∞	O
(	O
y	O
(	O
xn	O
)	O
tn	O
−	O
1	O
)	O
+	O
λ	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
2	O
(	O
7.19	O
)	O
n=1	O
where	O
e∞	O
(	O
z	O
)	O
is	O
a	O
function	O
that	O
is	O
zero	O
if	O
z	O
(	O
cid:2	O
)	O
0	O
and	O
∞	O
otherwise	O
and	O
ensures	O
that	O
the	O
constraints	O
(	O
7.5	O
)	O
are	O
satisﬁed	O
.	O
note	O
that	O
as	O
long	O
as	O
the	O
regularization	B
parameter	O
satisﬁes	O
λ	O
>	O
0	O
,	O
its	O
precise	O
value	O
plays	O
no	O
role	O
.	O
figure	O
7.2	O
shows	O
an	O
example	O
of	O
the	O
classiﬁcation	B
resulting	O
from	O
training	B
a	O
sup-	O
port	O
vector	O
machine	O
on	O
a	O
simple	O
synthetic	O
data	O
set	O
using	O
a	O
gaussian	O
kernel	O
of	O
the	O
7.1.	O
maximum	B
margin	I
classiﬁers	O
331	O
figure	O
7.2	O
example	O
of	O
synthetic	O
data	O
from	O
two	O
classes	O
in	O
two	O
dimensions	O
showing	O
contours	O
of	O
constant	O
y	O
(	O
x	O
)	O
obtained	O
from	O
a	O
support	B
vector	I
machine	I
having	O
a	O
gaus-	O
sian	O
kernel	B
function	I
.	O
also	O
shown	O
are	O
the	O
decision	B
boundary	I
,	O
the	O
margin	B
boundaries	O
,	O
and	O
the	O
sup-	O
port	O
vectors	O
.	O
form	O
(	O
6.23	O
)	O
.	O
although	O
the	O
data	O
set	O
is	O
not	O
linearly	B
separable	I
in	O
the	O
two-dimensional	O
data	O
space	O
x	O
,	O
it	O
is	O
linearly	B
separable	I
in	O
the	O
nonlinear	O
feature	B
space	I
deﬁned	O
implicitly	O
by	O
the	O
nonlinear	O
kernel	B
function	I
.	O
thus	O
the	O
training	B
data	O
points	O
are	O
perfectly	O
separated	O
in	O
the	O
original	O
data	O
space	O
.	O
this	O
example	O
also	O
provides	O
a	O
geometrical	O
insight	O
into	O
the	O
origin	O
of	O
sparsity	B
in	O
the	O
svm	O
.	O
the	O
maximum	B
margin	I
hyperplane	O
is	O
deﬁned	O
by	O
the	O
location	O
of	O
the	O
support	O
vectors	O
.	O
other	O
data	O
points	O
can	O
be	O
moved	O
around	O
freely	O
(	O
so	O
long	O
as	O
they	O
remain	O
out-	O
side	O
the	O
margin	B
region	O
)	O
without	O
changing	O
the	O
decision	B
boundary	I
,	O
and	O
so	O
the	O
solution	O
will	O
be	O
independent	B
of	O
such	O
data	O
points	O
.	O
7.1.1	O
overlapping	O
class	O
distributions	O
so	O
far	O
,	O
we	O
have	O
assumed	O
that	O
the	O
training	B
data	O
points	O
are	O
linearly	B
separable	I
in	O
the	O
feature	B
space	I
φ	O
(	O
x	O
)	O
.	O
the	O
resulting	O
support	B
vector	I
machine	I
will	O
give	O
exact	O
separation	O
of	O
the	O
training	B
data	O
in	O
the	O
original	O
input	O
space	O
x	O
,	O
although	O
the	O
corresponding	O
decision	B
boundary	I
will	O
be	O
nonlinear	O
.	O
in	O
practice	O
,	O
however	O
,	O
the	O
class-conditional	O
distributions	O
may	O
overlap	O
,	O
in	O
which	O
case	O
exact	O
separation	O
of	O
the	O
training	B
data	O
can	O
lead	O
to	O
poor	O
generalization	B
.	O
we	O
therefore	O
need	O
a	O
way	O
to	O
modify	O
the	O
support	B
vector	I
machine	I
so	O
as	O
to	O
allow	O
some	O
of	O
the	O
training	B
points	O
to	O
be	O
misclassiﬁed	O
.	O
from	O
(	O
7.19	O
)	O
we	O
see	O
that	O
in	O
the	O
case	O
of	O
separable	O
classes	O
,	O
we	O
implicitly	O
used	O
an	O
error	B
function	I
that	O
gave	O
inﬁnite	O
error	B
if	O
a	O
data	O
point	O
was	O
misclassiﬁed	O
and	O
zero	O
error	B
if	O
it	O
was	O
classiﬁed	O
correctly	O
,	O
and	O
then	O
optimized	O
the	O
model	O
parameters	O
to	O
maximize	O
the	O
margin	B
.	O
we	O
now	O
modify	O
this	O
approach	O
so	O
that	O
data	O
points	O
are	O
allowed	O
to	O
be	O
on	O
the	O
‘	O
wrong	O
side	O
’	O
of	O
the	O
margin	B
boundary	O
,	O
but	O
with	O
a	O
penalty	O
that	O
increases	O
with	O
the	O
distance	O
from	O
that	O
boundary	O
.	O
for	O
the	O
subsequent	O
optimization	O
problem	O
,	O
it	O
is	O
convenient	O
to	O
make	O
this	O
penalty	O
a	O
linear	O
function	O
of	O
this	O
distance	O
.	O
to	O
do	O
this	O
,	O
we	O
introduce	O
slack	O
variables	O
,	O
ξn	O
(	O
cid:2	O
)	O
0	O
where	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
with	O
one	O
slack	B
variable	I
for	O
each	O
training	B
data	O
point	O
(	O
bennett	O
,	O
1992	O
;	O
cortes	O
and	O
vapnik	O
,	O
1995	O
)	O
.	O
these	O
are	O
deﬁned	O
by	O
ξn	O
=	O
0	O
for	O
data	O
points	O
that	O
are	O
on	O
or	O
inside	O
the	O
correct	O
margin	B
boundary	O
and	O
ξn	O
=	O
|tn	O
−	O
y	O
(	O
xn	O
)	O
|	O
for	O
other	O
points	O
.	O
thus	O
a	O
data	O
point	O
that	O
is	O
on	O
the	O
decision	B
boundary	I
y	O
(	O
xn	O
)	O
=	O
0	O
will	O
have	O
ξn	O
=	O
1	O
,	O
and	O
points	O
332	O
7.	O
sparse	O
kernel	O
machines	O
figure	O
7.3	O
illustration	O
of	O
the	O
slack	O
variables	O
ξn	O
(	O
cid:2	O
)	O
0.	O
data	O
points	O
with	O
circles	O
around	O
them	O
are	O
support	O
vectors	O
.	O
y	O
=	O
−1	O
y	O
=	O
0	O
ξ	O
>	O
1	O
y	O
=	O
1	O
ξ	O
<	O
1	O
ξ	O
=	O
0	O
ξ	O
=	O
0	O
tny	O
(	O
xn	O
)	O
(	O
cid:2	O
)	O
1	O
−	O
ξn	O
,	O
with	O
ξn	O
>	O
1	O
will	O
be	O
misclassiﬁed	O
.	O
the	O
exact	O
classiﬁcation	O
constraints	O
(	O
7.5	O
)	O
are	O
then	O
replaced	O
with	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
(	O
7.20	O
)	O
in	O
which	O
the	O
slack	O
variables	O
are	O
constrained	O
to	O
satisfy	O
ξn	O
(	O
cid:2	O
)	O
0.	O
data	O
points	O
for	O
which	O
ξn	O
=	O
0	O
are	O
correctly	O
classiﬁed	O
and	O
are	O
either	O
on	O
the	O
margin	B
or	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
.	O
points	O
for	O
which	O
0	O
<	O
ξn	O
(	O
cid:1	O
)	O
1	O
lie	O
inside	O
the	O
margin	B
,	O
but	O
on	O
the	O
cor-	O
rect	O
side	O
of	O
the	O
decision	B
boundary	I
,	O
and	O
those	O
data	O
points	O
for	O
which	O
ξn	O
>	O
1	O
lie	O
on	O
the	O
wrong	O
side	O
of	O
the	O
decision	B
boundary	I
and	O
are	O
misclassiﬁed	O
,	O
as	O
illustrated	O
in	O
fig-	O
ure	O
7.3.	O
this	O
is	O
sometimes	O
described	O
as	O
relaxing	O
the	O
hard	O
margin	B
constraint	O
to	O
give	O
a	O
soft	B
margin	I
and	O
allows	O
some	O
of	O
the	O
training	B
set	I
data	O
points	O
to	O
be	O
misclassiﬁed	O
.	O
note	O
that	O
while	O
slack	O
variables	O
allow	O
for	O
overlapping	O
class	O
distributions	O
,	O
this	O
framework	O
is	O
still	O
sensitive	O
to	O
outliers	B
because	O
the	O
penalty	O
for	O
misclassiﬁcation	O
increases	O
linearly	O
with	O
ξ.	O
our	O
goal	O
is	O
now	O
to	O
maximize	O
the	O
margin	B
while	O
softly	O
penalizing	O
points	O
that	O
lie	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
boundary	O
.	O
we	O
therefore	O
minimize	O
n	O
(	O
cid:2	O
)	O
c	O
n=1	O
ξn	O
+	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
2	O
1	O
2	O
(	O
7.21	O
)	O
(	O
cid:5	O
)	O
where	O
the	O
parameter	O
c	O
>	O
0	O
controls	O
the	O
trade-off	O
between	O
the	O
slack	B
variable	I
penalty	O
and	O
the	O
margin	B
.	O
because	O
any	O
point	O
that	O
is	O
misclassiﬁed	O
has	O
ξn	O
>	O
1	O
,	O
it	O
follows	O
that	O
n	O
ξn	O
is	O
an	O
upper	O
bound	O
on	O
the	O
number	O
of	O
misclassiﬁed	O
points	O
.	O
the	O
parameter	O
c	O
is	O
therefore	O
analogous	O
to	O
(	O
the	O
inverse	B
of	O
)	O
a	O
regularization	B
coefﬁcient	O
because	O
it	O
controls	O
the	O
trade-off	O
between	O
minimizing	O
training	B
errors	O
and	O
controlling	O
model	O
complexity	O
.	O
in	O
the	O
limit	O
c	O
→	O
∞	O
,	O
we	O
will	O
recover	O
the	O
earlier	O
support	B
vector	I
machine	I
for	O
separable	O
data	O
.	O
ξn	O
(	O
cid:2	O
)	O
0.	O
the	O
corresponding	O
lagrangian	O
is	O
given	O
by	O
we	O
now	O
wish	O
to	O
minimize	O
(	O
7.21	O
)	O
subject	O
to	O
the	O
constraints	O
(	O
7.20	O
)	O
together	O
with	O
n	O
(	O
cid:2	O
)	O
ξn−	O
n	O
(	O
cid:2	O
)	O
an	O
{	O
tny	O
(	O
xn	O
)	O
−	O
1	O
+	O
ξn	O
}	O
−	O
n	O
(	O
cid:2	O
)	O
n=1	O
n=1	O
n=1	O
µnξn	O
(	O
7.22	O
)	O
l	O
(	O
w	O
,	O
b	O
,	O
a	O
)	O
=	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
2	O
+	O
c	O
1	O
2	O
appendix	O
e	O
where	O
{	O
an	O
(	O
cid:2	O
)	O
0	O
}	O
and	O
{	O
µn	O
(	O
cid:2	O
)	O
0	O
}	O
are	O
lagrange	O
multipliers	O
.	O
the	O
corresponding	O
set	O
of	O
kkt	O
conditions	O
are	O
given	O
by	O
7.1.	O
maximum	B
margin	I
classiﬁers	O
333	O
an	O
(	O
cid:2	O
)	O
0	O
tny	O
(	O
xn	O
)	O
−	O
1	O
+	O
ξn	O
(	O
cid:2	O
)	O
0	O
an	O
(	O
tny	O
(	O
xn	O
)	O
−	O
1	O
+	O
ξn	O
)	O
=	O
0	O
µn	O
(	O
cid:2	O
)	O
0	O
ξn	O
(	O
cid:2	O
)	O
0	O
µnξn	O
=	O
0	O
(	O
7.23	O
)	O
(	O
7.24	O
)	O
(	O
7.25	O
)	O
(	O
7.26	O
)	O
(	O
7.27	O
)	O
(	O
7.28	O
)	O
where	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
we	O
now	O
optimize	O
out	O
w	O
,	O
b	O
,	O
and	O
{	O
ξn	O
}	O
making	O
use	O
of	O
the	O
deﬁnition	O
(	O
7.1	O
)	O
of	O
y	O
(	O
x	O
)	O
0	O
(	O
cid:1	O
)	O
an	O
(	O
cid:1	O
)	O
c	O
n	O
(	O
cid:2	O
)	O
antn	O
=	O
0	O
n=1	O
for	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
where	O
(	O
7.33	O
)	O
are	O
known	O
as	O
box	B
constraints	I
.	O
this	O
again	O
represents	O
a	O
quadratic	O
programming	O
problem	O
.	O
if	O
we	O
substitute	O
(	O
7.29	O
)	O
into	O
(	O
7.1	O
)	O
,	O
we	O
see	O
that	O
predictions	O
for	O
new	O
data	O
points	O
are	O
again	O
made	O
by	O
using	O
(	O
7.13	O
)	O
.	O
we	O
can	O
now	O
interpret	O
the	O
resulting	O
solution	O
.	O
as	O
before	O
,	O
a	O
subset	O
of	O
the	O
data	O
points	O
may	O
have	O
an	O
=	O
0	O
,	O
in	O
which	O
case	O
they	O
do	O
not	O
contribute	O
to	O
the	O
predictive	O
to	O
give	O
∂l	O
∂w	O
∂l	O
∂b	O
n	O
(	O
cid:2	O
)	O
antnφ	O
(	O
xn	O
)	O
=	O
0	O
⇒	O
w	O
=	O
=	O
0	O
⇒	O
n	O
(	O
cid:2	O
)	O
n=1	O
n=1	O
antn	O
=	O
0	O
=	O
0	O
⇒	O
an	O
=	O
c	O
−	O
µn	O
.	O
(	O
7.31	O
)	O
using	O
these	O
results	O
to	O
eliminate	O
w	O
,	O
b	O
,	O
and	O
{	O
ξn	O
}	O
from	O
the	O
lagrangian	O
,	O
we	O
obtain	O
the	O
dual	O
lagrangian	O
in	O
the	O
form	O
∂l	O
∂ξn	O
n	O
(	O
cid:2	O
)	O
n=1	O
(	O
cid:4	O
)	O
l	O
(	O
a	O
)	O
=	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
m=1	O
an	O
−	O
1	O
2	O
anamtntmk	O
(	O
xn	O
,	O
xm	O
)	O
(	O
7.32	O
)	O
which	O
is	O
identical	O
to	O
the	O
separable	O
case	O
,	O
except	O
that	O
the	O
constraints	O
are	O
somewhat	O
different	O
.	O
to	O
see	O
what	O
these	O
constraints	O
are	O
,	O
we	O
note	O
that	O
an	O
(	O
cid:2	O
)	O
0	O
is	O
required	O
because	O
these	O
are	O
lagrange	O
multipliers	O
.	O
furthermore	O
,	O
(	O
7.31	O
)	O
together	O
with	O
µn	O
(	O
cid:2	O
)	O
0	O
implies	O
an	O
(	O
cid:1	O
)	O
c.	O
we	O
therefore	O
have	O
to	O
minimize	O
(	O
7.32	O
)	O
with	O
respect	O
to	O
the	O
dual	O
variables	O
{	O
an	O
}	O
subject	O
to	O
(	O
7.29	O
)	O
(	O
7.30	O
)	O
(	O
7.33	O
)	O
(	O
7.34	O
)	O
334	O
7.	O
sparse	O
kernel	O
machines	O
model	O
(	O
7.13	O
)	O
.	O
the	O
remaining	O
data	O
points	O
constitute	O
the	O
support	O
vectors	O
.	O
these	O
have	O
an	O
>	O
0	O
and	O
hence	O
from	O
(	O
7.25	O
)	O
must	O
satisfy	O
tny	O
(	O
xn	O
)	O
=	O
1	O
−	O
ξn	O
.	O
(	O
7.35	O
)	O
if	O
an	O
<	O
c	O
,	O
then	O
(	O
7.31	O
)	O
implies	O
that	O
µn	O
>	O
0	O
,	O
which	O
from	O
(	O
7.28	O
)	O
requires	O
ξn	O
=	O
0	O
and	O
hence	O
such	O
points	O
lie	O
on	O
the	O
margin	B
.	O
points	O
with	O
an	O
=	O
c	O
can	O
lie	O
inside	O
the	O
margin	B
and	O
can	O
either	O
be	O
correctly	O
classiﬁed	O
if	O
ξn	O
(	O
cid:1	O
)	O
1	O
or	O
misclassiﬁed	O
if	O
ξn	O
>	O
1.	O
to	O
determine	O
the	O
parameter	O
b	O
in	O
(	O
7.1	O
)	O
,	O
we	O
note	O
that	O
those	O
support	O
vectors	O
for	O
which	O
0	O
<	O
an	O
<	O
c	O
have	O
ξn	O
=	O
0	O
so	O
that	O
tny	O
(	O
xn	O
)	O
=	O
1	O
and	O
hence	O
will	O
satisfy	O
(	O
cid:23	O
)	O
tn	O
amtmk	O
(	O
xn	O
,	O
xm	O
)	O
+	O
b	O
=	O
1	O
.	O
(	O
7.36	O
)	O
again	O
,	O
a	O
numerically	O
stable	O
solution	O
is	O
obtained	O
by	O
averaging	O
to	O
give	O
(	O
cid:23	O
)	O
(	O
cid:22	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
m∈s	O
(	O
cid:22	O
)	O
(	O
cid:2	O
)	O
m∈s	O
b	O
=	O
1	O
nm	O
n∈m	O
tn	O
−	O
(	O
cid:4	O
)	O
l	O
(	O
a	O
)	O
=	O
−1	O
2	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
m=1	O
amtmk	O
(	O
xn	O
,	O
xm	O
)	O
(	O
7.37	O
)	O
where	O
m	O
denotes	O
the	O
set	O
of	O
indices	O
of	O
data	O
points	O
having	O
0	O
<	O
an	O
<	O
c.	O
an	O
alternative	O
,	O
equivalent	O
formulation	O
of	O
the	O
support	B
vector	I
machine	I
,	O
known	O
as	O
the	O
ν-svm	O
,	O
has	O
been	O
proposed	O
by	O
sch¨olkopf	O
et	O
al	O
.	O
(	O
2000	O
)	O
.	O
this	O
involves	O
maximizing	O
anamtntmk	O
(	O
xn	O
,	O
xm	O
)	O
(	O
7.38	O
)	O
subject	O
to	O
the	O
constraints	O
0	O
(	O
cid:1	O
)	O
an	O
(	O
cid:1	O
)	O
1/n	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
antn	O
=	O
0	O
an	O
(	O
cid:2	O
)	O
ν	O
.	O
(	O
7.39	O
)	O
(	O
7.40	O
)	O
(	O
7.41	O
)	O
n=1	O
this	O
approach	O
has	O
the	O
advantage	O
that	O
the	O
parameter	O
ν	O
,	O
which	O
replaces	O
c	O
,	O
can	O
be	O
interpreted	O
as	O
both	O
an	O
upper	O
bound	O
on	O
the	O
fraction	O
of	O
margin	B
errors	O
(	O
points	O
for	O
which	O
ξn	O
>	O
0	O
and	O
hence	O
which	O
lie	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
boundary	O
and	O
which	O
may	O
or	O
may	O
not	O
be	O
misclassiﬁed	O
)	O
and	O
a	O
lower	B
bound	I
on	O
the	O
fraction	O
of	O
support	O
vectors	O
.	O
an	O
example	O
of	O
the	O
ν-svm	O
applied	O
to	O
a	O
synthetic	O
data	O
set	O
is	O
shown	O
in	O
figure	O
7.4.	O
here	O
gaussian	O
kernels	O
of	O
the	O
form	O
exp	O
(	O
−γ	O
(	O
cid:5	O
)	O
x	O
−	O
x	O
(	O
cid:4	O
)	O
(	O
cid:5	O
)	O
2	O
)	O
have	O
been	O
used	O
,	O
with	O
γ	O
=	O
0.45.	O
although	O
predictions	O
for	O
new	O
inputs	O
are	O
made	O
using	O
only	O
the	O
support	O
vectors	O
,	O
the	O
training	B
phase	O
(	O
i.e.	O
,	O
the	O
determination	O
of	O
the	O
parameters	O
a	O
and	O
b	O
)	O
makes	O
use	O
of	O
the	O
whole	O
data	O
set	O
,	O
and	O
so	O
it	O
is	O
important	O
to	O
have	O
efﬁcient	O
algorithms	O
for	O
solving	O
7.1.	O
maximum	B
margin	I
classiﬁers	O
335	O
figure	O
7.4	O
illustration	O
of	O
the	O
ν-svm	O
applied	O
to	O
a	O
nonseparable	O
data	O
set	O
in	O
two	O
dimensions	O
.	O
the	O
support	O
vectors	O
are	O
indicated	O
by	O
circles	O
.	O
2	O
0	O
−2	O
the	O
quadratic	O
programming	O
problem	O
.	O
we	O
ﬁrst	O
note	O
that	O
the	O
objective	O
function	O
(	O
cid:4	O
)	O
l	O
(	O
a	O
)	O
−2	O
0	O
2	O
given	O
by	O
(	O
7.10	O
)	O
or	O
(	O
7.32	O
)	O
is	O
quadratic	O
and	O
so	O
any	O
local	B
optimum	O
will	O
also	O
be	O
a	O
global	O
optimum	O
provided	O
the	O
constraints	O
deﬁne	O
a	O
convex	O
region	O
(	O
which	O
they	O
do	O
as	O
a	O
conse-	O
quence	O
of	O
being	O
linear	O
)	O
.	O
direct	O
solution	O
of	O
the	O
quadratic	O
programming	O
problem	O
us-	O
ing	O
traditional	O
techniques	O
is	O
often	O
infeasible	O
due	O
to	O
the	O
demanding	O
computation	O
and	O
memory	O
requirements	O
,	O
and	O
so	O
more	O
practical	O
approaches	O
need	O
to	O
be	O
found	O
.	O
the	O
tech-	O
nique	O
of	O
chunking	B
(	O
vapnik	O
,	O
1982	O
)	O
exploits	O
the	O
fact	O
that	O
the	O
value	O
of	O
the	O
lagrangian	O
is	O
unchanged	O
if	O
we	O
remove	O
the	O
rows	O
and	O
columns	O
of	O
the	O
kernel	O
matrix	O
corresponding	O
to	O
lagrange	O
multipliers	O
that	O
have	O
value	O
zero	O
.	O
this	O
allows	O
the	O
full	O
quadratic	O
pro-	O
gramming	O
problem	O
to	O
be	O
broken	O
down	O
into	O
a	O
series	O
of	O
smaller	O
ones	O
,	O
whose	O
goal	O
is	O
eventually	O
to	O
identify	O
all	O
of	O
the	O
nonzero	O
lagrange	O
multipliers	O
and	O
discard	O
the	O
others	O
.	O
chunking	B
can	O
be	O
implemented	O
using	O
protected	B
conjugate	I
gradients	I
(	O
burges	O
,	O
1998	O
)	O
.	O
although	O
chunking	B
reduces	O
the	O
size	O
of	O
the	O
matrix	O
in	O
the	O
quadratic	O
function	O
from	O
the	O
number	O
of	O
data	O
points	O
squared	O
to	O
approximately	O
the	O
number	O
of	O
nonzero	O
lagrange	O
multipliers	O
squared	O
,	O
even	O
this	O
may	O
be	O
too	O
big	O
to	O
ﬁt	O
in	O
memory	O
for	O
large-scale	O
appli-	O
cations	O
.	O
decomposition	B
methods	I
(	O
osuna	O
et	O
al.	O
,	O
1996	O
)	O
also	O
solve	O
a	O
series	O
of	O
smaller	O
quadratic	O
programming	O
problems	O
but	O
are	O
designed	O
so	O
that	O
each	O
of	O
these	O
is	O
of	O
a	O
ﬁxed	O
size	O
,	O
and	O
so	O
the	O
technique	O
can	O
be	O
applied	O
to	O
arbitrarily	O
large	O
data	O
sets	O
.	O
however	O
,	O
it	O
still	O
involves	O
numerical	O
solution	O
of	O
quadratic	O
programming	O
subproblems	O
and	O
these	O
can	O
be	O
problematic	O
and	O
expensive	O
.	O
one	O
of	O
the	O
most	O
popular	O
approaches	O
to	O
training	B
support	O
vector	O
machines	O
is	O
called	O
sequential	B
minimal	I
optimization	I
,	O
or	O
smo	O
(	O
platt	O
,	O
1999	O
)	O
.	O
it	O
takes	O
the	O
concept	O
of	O
chunking	B
to	O
the	O
extreme	O
limit	O
and	O
considers	O
just	O
two	O
lagrange	O
multipliers	O
at	O
a	O
time	O
.	O
in	O
this	O
case	O
,	O
the	O
subproblem	O
can	O
be	O
solved	O
analyti-	O
cally	O
,	O
thereby	O
avoiding	O
numerical	O
quadratic	O
programming	O
altogether	O
.	O
heuristics	O
are	O
given	O
for	O
choosing	O
the	O
pair	O
of	O
lagrange	O
multipliers	O
to	O
be	O
considered	O
at	O
each	O
step	O
.	O
in	O
practice	O
,	O
smo	O
is	O
found	O
to	O
have	O
a	O
scaling	O
with	O
the	O
number	O
of	O
data	O
points	O
that	O
is	O
somewhere	O
between	O
linear	O
and	O
quadratic	O
depending	O
on	O
the	O
particular	O
application	O
.	O
we	O
have	O
seen	O
that	O
kernel	O
functions	O
correspond	O
to	O
inner	O
products	O
in	O
feature	O
spaces	O
that	O
can	O
have	O
high	O
,	O
or	O
even	O
inﬁnite	O
,	O
dimensionality	O
.	O
by	O
working	O
directly	O
in	O
terms	O
of	O
the	O
kernel	B
function	I
,	O
without	O
introducing	O
the	O
feature	B
space	I
explicitly	O
,	O
it	O
might	O
there-	O
fore	O
seem	O
that	O
support	B
vector	I
machines	O
somehow	O
manage	O
to	O
avoid	O
the	O
curse	O
of	O
di-	O
336	O
7.	O
sparse	O
kernel	O
machines	O
section	O
1.4	O
section	O
4.3.2	O
mensionality	O
.	O
this	O
is	O
not	O
the	O
case	O
,	O
however	O
,	O
because	O
there	O
are	O
constraints	O
amongst	O
the	O
feature	O
values	O
that	O
restrict	O
the	O
effective	O
dimensionality	O
of	O
feature	B
space	I
.	O
to	O
see	O
this	O
consider	O
a	O
simple	O
second-order	O
polynomial	O
kernel	O
that	O
we	O
can	O
expand	O
in	O
terms	O
of	O
its	O
components	O
(	O
cid:11	O
)	O
2	O
=	O
(	O
1	O
+	O
x1z1	O
+	O
x2z2	O
)	O
2	O
k	O
(	O
x	O
,	O
z	O
)	O
=	O
(	O
cid:10	O
)	O
1	O
+	O
xtz	O
√	O
2x1	O
,	O
1z2	O
=	O
1	O
+	O
2x1z1	O
+	O
2x2z2	O
+	O
x2	O
√	O
2x1x2	O
,	O
x2	O
=	O
(	O
1	O
,	O
=	O
φ	O
(	O
x	O
)	O
tφ	O
(	O
z	O
)	O
.	O
√	O
2x2	O
,	O
x2	O
1	O
,	O
2z2	O
1	O
+	O
2x1z1x2z2	O
+	O
x2	O
√	O
2	O
2z2	O
,	O
z2	O
1	O
,	O
2	O
)	O
(	O
1	O
,	O
√	O
2z1	O
,	O
√	O
2z1z2	O
,	O
z2	O
2	O
)	O
t	O
(	O
7.42	O
)	O
this	O
kernel	B
function	I
therefore	O
represents	O
an	O
inner	O
product	O
in	O
a	O
feature	B
space	I
having	O
six	O
dimensions	O
,	O
in	O
which	O
the	O
mapping	O
from	O
input	O
space	O
to	O
feature	B
space	I
is	O
described	O
by	O
the	O
vector	O
function	O
φ	O
(	O
x	O
)	O
.	O
however	O
,	O
the	O
coefﬁcients	O
weighting	O
these	O
different	O
features	O
are	O
constrained	O
to	O
have	O
speciﬁc	O
forms	O
.	O
thus	O
any	O
set	O
of	O
points	O
in	O
the	O
original	O
two-dimensional	O
space	O
x	O
would	O
be	O
constrained	O
to	O
lie	O
exactly	O
on	O
a	O
two-dimensional	O
nonlinear	O
manifold	B
embedded	O
in	O
the	O
six-dimensional	O
feature	B
space	I
.	O
we	O
have	O
already	O
highlighted	O
the	O
fact	O
that	O
the	O
support	B
vector	I
machine	I
does	O
not	O
provide	O
probabilistic	O
outputs	O
but	O
instead	O
makes	O
classiﬁcation	B
decisions	O
for	O
new	O
in-	O
put	O
vectors	O
.	O
veropoulos	O
et	O
al	O
.	O
(	O
1999	O
)	O
discuss	O
modiﬁcations	O
to	O
the	O
svm	O
to	O
allow	O
the	O
trade-off	O
between	O
false	O
positive	O
and	O
false	O
negative	O
errors	O
to	O
be	O
controlled	O
.	O
how-	O
ever	O
,	O
if	O
we	O
wish	O
to	O
use	O
the	O
svm	O
as	O
a	O
module	O
in	O
a	O
larger	O
probabilistic	O
system	O
,	O
then	O
probabilistic	O
predictions	O
of	O
the	O
class	O
label	O
t	O
for	O
new	O
inputs	O
x	O
are	O
required	O
.	O
to	O
address	O
this	O
issue	O
,	O
platt	O
(	O
2000	O
)	O
has	O
proposed	O
ﬁtting	O
a	O
logistic	B
sigmoid	I
to	O
the	O
outputs	O
of	O
a	O
previously	O
trained	O
support	B
vector	I
machine	I
.	O
speciﬁcally	O
,	O
the	O
required	O
conditional	B
probability	I
is	O
assumed	O
to	O
be	O
of	O
the	O
form	O
p	O
(	O
t	O
=	O
1|x	O
)	O
=	O
σ	O
(	O
ay	O
(	O
x	O
)	O
+	O
b	O
)	O
(	O
7.43	O
)	O
where	O
y	O
(	O
x	O
)	O
is	O
deﬁned	O
by	O
(	O
7.1	O
)	O
.	O
values	O
for	O
the	O
parameters	O
a	O
and	O
b	O
are	O
found	O
by	O
minimizing	O
the	O
cross-entropy	B
error	I
function	I
deﬁned	O
by	O
a	O
training	B
set	I
consisting	O
of	O
pairs	O
of	O
values	O
y	O
(	O
xn	O
)	O
and	O
tn	O
.	O
the	O
data	O
used	O
to	O
ﬁt	O
the	O
sigmoid	B
needs	O
to	O
be	O
independent	B
of	O
that	O
used	O
to	O
train	O
the	O
original	O
svm	O
in	O
order	O
to	O
avoid	O
severe	O
over-ﬁtting	B
.	O
this	O
two-	O
stage	O
approach	O
is	O
equivalent	O
to	O
assuming	O
that	O
the	O
output	O
y	O
(	O
x	O
)	O
of	O
the	O
support	B
vector	I
machine	I
represents	O
the	O
log-odds	O
of	O
x	O
belonging	O
to	O
class	O
t	O
=	O
1.	O
because	O
the	O
svm	O
training	B
procedure	O
is	O
not	O
speciﬁcally	O
intended	O
to	O
encourage	O
this	O
,	O
the	O
svm	O
can	O
give	O
a	O
poor	O
approximation	O
to	O
the	O
posterior	O
probabilities	O
(	O
tipping	O
,	O
2001	O
)	O
.	O
7.1.2	O
relation	O
to	O
logistic	O
regression	B
as	O
with	O
the	O
separable	O
case	O
,	O
we	O
can	O
re-cast	O
the	O
svm	O
for	O
nonseparable	O
distri-	O
butions	O
in	O
terms	O
of	O
the	O
minimization	O
of	O
a	O
regularized	O
error	O
function	O
.	O
this	O
will	O
also	O
allow	O
us	O
to	O
highlight	O
similarities	O
,	O
and	O
differences	O
,	O
compared	O
to	O
the	O
logistic	B
regression	I
model	O
.	O
we	O
have	O
seen	O
that	O
for	O
data	O
points	O
that	O
are	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
boundary	O
,	O
and	O
which	O
therefore	O
satisfy	O
yntn	O
(	O
cid:2	O
)	O
1	O
,	O
we	O
have	O
ξn	O
=	O
0	O
,	O
and	O
for	O
the	O
7.1.	O
maximum	B
margin	I
classiﬁers	O
337	O
e	O
(	O
z	O
)	O
figure	O
7.5	O
plot	O
of	O
the	O
‘	O
hinge	O
’	O
error	B
function	I
used	O
in	O
support	B
vector	I
machines	O
,	O
shown	O
in	O
blue	O
,	O
along	O
with	O
the	O
error	B
function	I
for	O
logistic	B
regression	I
,	O
rescaled	O
by	O
a	O
factor	O
of	O
1/	O
ln	O
(	O
2	O
)	O
so	O
that	O
it	O
passes	O
through	O
the	O
point	O
(	O
0	O
,	O
1	O
)	O
,	O
shown	O
in	O
red	O
.	O
also	O
shown	O
are	O
the	O
misclassiﬁcation	O
error	B
in	O
black	O
and	O
the	O
squared	O
error	B
in	O
green	O
.	O
−2	O
−1	O
0	O
1	O
z	O
2	O
remaining	O
points	O
we	O
have	O
ξn	O
=	O
1	O
−	O
yntn	O
.	O
thus	O
the	O
objective	O
function	O
(	O
7.21	O
)	O
can	O
be	O
written	O
(	O
up	O
to	O
an	O
overall	O
multiplicative	O
constant	O
)	O
in	O
the	O
form	O
n	O
(	O
cid:2	O
)	O
esv	O
(	O
yntn	O
)	O
+	O
λ	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
2	O
(	O
7.44	O
)	O
n=1	O
where	O
λ	O
=	O
(	O
2c	O
)	O
−1	O
,	O
and	O
esv	O
(	O
·	O
)	O
is	O
the	O
hinge	B
error	I
function	I
deﬁned	O
by	O
esv	O
(	O
yntn	O
)	O
=	O
[	O
1	O
−	O
yntn	O
]	O
+	O
(	O
7.45	O
)	O
where	O
[	O
·	O
]	O
+	O
denotes	O
the	O
positive	O
part	O
.	O
the	O
hinge	B
error	I
function	I
,	O
so-called	O
because	O
of	O
its	O
shape	O
,	O
is	O
plotted	O
in	O
figure	O
7.5.	O
it	O
can	O
be	O
viewed	O
as	O
an	O
approximation	O
to	O
the	O
misclassiﬁcation	O
error	B
,	O
i.e.	O
,	O
the	O
error	B
function	I
that	O
ideally	O
we	O
would	O
like	O
to	O
minimize	O
,	O
which	O
is	O
also	O
shown	O
in	O
figure	O
7.5.	O
when	O
we	O
considered	O
the	O
logistic	B
regression	I
model	O
in	O
section	O
4.3.2	O
,	O
we	O
found	O
it	O
convenient	O
to	O
work	O
with	O
target	O
variable	O
t	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
for	O
comparison	O
with	O
the	O
support	B
vector	I
machine	I
,	O
we	O
ﬁrst	O
reformulate	O
maximum	B
likelihood	I
logistic	O
regression	B
using	O
the	O
target	O
variable	O
t	O
∈	O
{	O
−1	O
,	O
1	O
}	O
.	O
to	O
do	O
this	O
,	O
we	O
note	O
that	O
p	O
(	O
t	O
=	O
1|y	O
)	O
=	O
σ	O
(	O
y	O
)	O
where	O
y	O
(	O
x	O
)	O
is	O
given	O
by	O
(	O
7.1	O
)	O
,	O
and	O
σ	O
(	O
y	O
)	O
is	O
the	O
logistic	B
sigmoid	I
function	O
deﬁned	O
by	O
(	O
4.59	O
)	O
.	O
it	O
follows	O
that	O
p	O
(	O
t	O
=	O
−1|y	O
)	O
=	O
1	O
−	O
σ	O
(	O
y	O
)	O
=	O
σ	O
(	O
−y	O
)	O
,	O
where	O
we	O
have	O
used	O
the	O
properties	O
of	O
the	O
logistic	B
sigmoid	I
function	O
,	O
and	O
so	O
we	O
can	O
write	O
p	O
(	O
t|y	O
)	O
=	O
σ	O
(	O
yt	O
)	O
.	O
(	O
7.46	O
)	O
exercise	O
7.6	O
from	O
this	O
we	O
can	O
construct	O
an	O
error	B
function	I
by	O
taking	O
the	O
negative	O
logarithm	O
of	O
the	O
likelihood	B
function	I
that	O
,	O
with	O
a	O
quadratic	O
regularizer	O
,	O
takes	O
the	O
form	O
n	O
(	O
cid:2	O
)	O
elr	O
(	O
yntn	O
)	O
+	O
λ	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
2.	O
where	O
n=1	O
elr	O
(	O
yt	O
)	O
=	O
ln	O
(	O
1	O
+	O
exp	O
(	O
−yt	O
)	O
)	O
.	O
(	O
7.47	O
)	O
(	O
7.48	O
)	O
338	O
7.	O
sparse	O
kernel	O
machines	O
for	O
comparison	O
with	O
other	O
error	B
functions	O
,	O
we	O
can	O
divide	O
by	O
ln	O
(	O
2	O
)	O
so	O
that	O
the	O
error	B
function	I
passes	O
through	O
the	O
point	O
(	O
0	O
,	O
1	O
)	O
.	O
this	O
rescaled	O
error	B
function	I
is	O
also	O
plotted	O
in	O
figure	O
7.5	O
and	O
we	O
see	O
that	O
it	O
has	O
a	O
similar	O
form	O
to	O
the	O
support	B
vector	I
error	O
function	O
.	O
the	O
key	O
difference	O
is	O
that	O
the	O
ﬂat	O
region	O
in	O
esv	O
(	O
yt	O
)	O
leads	O
to	O
sparse	O
solutions	O
.	O
both	O
the	O
logistic	O
error	O
and	O
the	O
hinge	O
loss	O
can	O
be	O
viewed	O
as	O
continuous	O
approx-	O
imations	O
to	O
the	O
misclassiﬁcation	O
error	B
.	O
another	O
continuous	O
error	B
function	I
that	O
has	O
sometimes	O
been	O
used	O
to	O
solve	O
classiﬁcation	B
problems	O
is	O
the	O
squared	O
error	B
,	O
which	O
is	O
again	O
plotted	O
in	O
figure	O
7.5.	O
it	O
has	O
the	O
property	O
,	O
however	O
,	O
of	O
placing	O
increasing	O
emphasis	O
on	O
data	O
points	O
that	O
are	O
correctly	O
classiﬁed	O
but	O
that	O
are	O
a	O
long	O
way	O
from	O
the	O
decision	B
boundary	I
on	O
the	O
correct	O
side	O
.	O
such	O
points	O
will	O
be	O
strongly	O
weighted	O
at	O
the	O
expense	O
of	O
misclassiﬁed	O
points	O
,	O
and	O
so	O
if	O
the	O
objective	O
is	O
to	O
minimize	O
the	O
mis-	O
classiﬁcation	B
rate	O
,	O
then	O
a	O
monotonically	O
decreasing	O
error	B
function	I
would	O
be	O
a	O
better	O
choice	O
.	O
7.1.3	O
multiclass	B
svms	O
the	O
support	B
vector	I
machine	I
is	O
fundamentally	O
a	O
two-class	O
classiﬁer	O
.	O
in	O
practice	O
,	O
however	O
,	O
we	O
often	O
have	O
to	O
tackle	O
problems	O
involving	O
k	O
>	O
2	O
classes	O
.	O
various	O
meth-	O
ods	O
have	O
therefore	O
been	O
proposed	O
for	O
combining	O
multiple	O
two-class	O
svms	O
in	O
order	O
to	O
build	O
a	O
multiclass	B
classiﬁer	O
.	O
one	O
commonly	O
used	O
approach	O
(	O
vapnik	O
,	O
1998	O
)	O
is	O
to	O
construct	O
k	O
separate	O
svms	O
,	O
in	O
which	O
the	O
kth	O
model	O
yk	O
(	O
x	O
)	O
is	O
trained	O
using	O
the	O
data	O
from	O
class	O
ck	O
as	O
the	O
positive	O
examples	O
and	O
the	O
data	O
from	O
the	O
remaining	O
k	O
−	O
1	O
classes	O
as	O
the	O
negative	O
examples	O
.	O
this	O
is	O
known	O
as	O
the	O
one-versus-the-rest	O
approach	O
.	O
however	O
,	O
in	O
figure	O
4.2	O
we	O
saw	O
that	O
using	O
the	O
decisions	O
of	O
the	O
individual	O
classiﬁers	O
can	O
lead	O
to	O
inconsistent	O
results	O
in	O
which	O
an	O
input	O
is	O
assigned	O
to	O
multiple	O
classes	O
simultaneously	O
.	O
this	O
problem	O
is	O
sometimes	O
addressed	O
by	O
making	O
predictions	O
for	O
new	O
inputs	O
x	O
using	O
y	O
(	O
x	O
)	O
=	O
max	O
k	O
yk	O
(	O
x	O
)	O
.	O
(	O
7.49	O
)	O
unfortunately	O
,	O
this	O
heuristic	O
approach	O
suffers	O
from	O
the	O
problem	O
that	O
the	O
different	O
classiﬁers	O
were	O
trained	O
on	O
different	O
tasks	O
,	O
and	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
real-	O
valued	O
quantities	O
yk	O
(	O
x	O
)	O
for	O
different	O
classiﬁers	O
will	O
have	O
appropriate	O
scales	O
.	O
another	O
problem	O
with	O
the	O
one-versus-the-rest	O
approach	O
is	O
that	O
the	O
training	B
sets	O
are	O
imbalanced	O
.	O
for	O
instance	O
,	O
if	O
we	O
have	O
ten	O
classes	O
each	O
with	O
equal	O
numbers	O
of	O
training	B
data	O
points	O
,	O
then	O
the	O
individual	O
classiﬁers	O
are	O
trained	O
on	O
data	O
sets	O
comprising	O
90	O
%	O
negative	O
examples	O
and	O
only	O
10	O
%	O
positive	O
examples	O
,	O
and	O
the	O
symmetry	O
of	O
the	O
original	O
problem	O
is	O
lost	O
.	O
a	O
variant	O
of	O
the	O
one-versus-the-rest	O
scheme	O
was	O
proposed	O
by	O
lee	O
et	O
al	O
.	O
(	O
2001	O
)	O
who	O
modify	O
the	O
target	O
values	O
so	O
that	O
the	O
positive	O
class	O
has	O
target	O
+1	O
and	O
the	O
negative	O
class	O
has	O
target	O
−1/	O
(	O
k	O
−	O
1	O
)	O
.	O
weston	O
and	O
watkins	O
(	O
1999	O
)	O
deﬁne	O
a	O
single	O
objective	O
function	O
for	O
training	B
all	O
k	O
svms	O
simultaneously	O
,	O
based	O
on	O
maximizing	O
the	O
margin	B
from	O
each	O
to	O
remaining	O
classes	O
.	O
however	O
,	O
this	O
can	O
result	O
in	O
much	O
slower	O
training	B
because	O
,	O
instead	O
of	O
solving	O
k	O
separate	O
optimization	O
problems	O
each	O
over	O
n	O
data	O
points	O
with	O
an	O
overall	O
cost	O
of	O
o	O
(	O
kn	O
2	O
)	O
,	O
a	O
single	O
optimization	O
problem	O
of	O
size	O
(	O
k	O
−	O
1	O
)	O
n	O
must	O
be	O
solved	O
giving	O
an	O
overall	O
cost	O
of	O
o	O
(	O
k	O
2n	O
2	O
)	O
.	O
7.1.	O
maximum	B
margin	I
classiﬁers	O
339	O
another	O
approach	O
is	O
to	O
train	O
k	O
(	O
k	O
−1	O
)	O
/2	O
different	O
2-class	O
svms	O
on	O
all	O
possible	O
pairs	O
of	O
classes	O
,	O
and	O
then	O
to	O
classify	O
test	O
points	O
according	O
to	O
which	O
class	O
has	O
the	O
high-	O
est	O
number	O
of	O
‘	O
votes	O
’	O
,	O
an	O
approach	O
that	O
is	O
sometimes	O
called	O
one-versus-one	O
.	O
again	O
,	O
we	O
saw	O
in	O
figure	O
4.2	O
that	O
this	O
can	O
lead	O
to	O
ambiguities	O
in	O
the	O
resulting	O
classiﬁcation	B
.	O
also	O
,	O
for	O
large	O
k	O
this	O
approach	O
requires	O
signiﬁcantly	O
more	O
training	B
time	O
than	O
the	O
one-versus-the-rest	O
approach	O
.	O
similarly	O
,	O
to	O
evaluate	O
test	O
points	O
,	O
signiﬁcantly	O
more	O
computation	O
is	O
required	O
.	O
the	O
latter	O
problem	O
can	O
be	O
alleviated	O
by	O
organizing	O
the	O
pairwise	O
classiﬁers	O
into	O
a	O
directed	B
acyclic	I
graph	I
(	O
not	O
to	O
be	O
confused	O
with	O
a	O
probabilistic	B
graphical	I
model	I
)	O
leading	O
to	O
the	O
dagsvm	O
(	O
platt	O
et	O
al.	O
,	O
2000	O
)	O
.	O
for	O
k	O
classes	O
,	O
the	O
dagsvm	O
has	O
a	O
total	O
of	O
k	O
(	O
k	O
−	O
1	O
)	O
/2	O
classiﬁers	O
,	O
and	O
to	O
classify	O
a	O
new	O
test	O
point	O
only	O
k	O
−	O
1	O
pairwise	O
classiﬁers	O
need	O
to	O
be	O
evaluated	O
,	O
with	O
the	O
particular	O
classiﬁers	O
used	O
depending	O
on	O
which	O
path	O
through	O
the	O
graph	O
is	O
traversed	O
.	O
a	O
different	O
approach	O
to	O
multiclass	B
classiﬁcation	O
,	O
based	O
on	O
error-correcting	O
out-	O
put	O
codes	O
,	O
was	O
developed	O
by	O
dietterich	O
and	O
bakiri	O
(	O
1995	O
)	O
and	O
applied	O
to	O
support	B
vector	I
machines	O
by	O
allwein	O
et	O
al	O
.	O
(	O
2000	O
)	O
.	O
this	O
can	O
be	O
viewed	O
as	O
a	O
generalization	B
of	O
the	O
voting	O
scheme	O
of	O
the	O
one-versus-one	O
approach	O
in	O
which	O
more	O
general	O
partitions	O
of	O
the	O
classes	O
are	O
used	O
to	O
train	O
the	O
individual	O
classiﬁers	O
.	O
the	O
k	O
classes	O
themselves	O
are	O
represented	O
as	O
particular	O
sets	O
of	O
responses	O
from	O
the	O
two-class	O
classiﬁers	O
chosen	O
,	O
and	O
together	O
with	O
a	O
suitable	O
decoding	O
scheme	O
,	O
this	O
gives	O
robustness	B
to	O
errors	O
and	O
to	O
ambiguity	O
in	O
the	O
outputs	O
of	O
the	O
individual	O
classiﬁers	O
.	O
although	O
the	O
application	O
of	O
svms	O
to	O
multiclass	B
classiﬁcation	O
problems	O
remains	O
an	O
open	O
issue	O
,	O
in	O
practice	O
the	O
one-versus-the-rest	O
approach	O
is	O
the	O
most	O
widely	O
used	O
in	O
spite	O
of	O
its	O
ad-hoc	O
formula-	O
tion	O
and	O
its	O
practical	O
limitations	O
.	O
there	O
are	O
also	O
single-class	O
support	O
vector	O
machines	O
,	O
which	O
solve	O
an	O
unsuper-	O
vised	O
learning	B
problem	O
related	O
to	O
probability	B
density	O
estimation	O
.	O
instead	O
of	O
mod-	O
elling	O
the	O
density	B
of	O
data	O
,	O
however	O
,	O
these	O
methods	O
aim	O
to	O
ﬁnd	O
a	O
smooth	O
boundary	O
enclosing	O
a	O
region	O
of	O
high	O
density	B
.	O
the	O
boundary	O
is	O
chosen	O
to	O
represent	O
a	O
quantile	O
of	O
the	O
density	B
,	O
that	O
is	O
,	O
the	O
probability	B
that	O
a	O
data	O
point	O
drawn	O
from	O
the	O
distribution	O
will	O
land	O
inside	O
that	O
region	O
is	O
given	O
by	O
a	O
ﬁxed	O
number	O
between	O
0	O
and	O
1	O
that	O
is	O
speciﬁed	O
in	O
advance	O
.	O
this	O
is	O
a	O
more	O
restricted	O
problem	O
than	O
estimating	O
the	O
full	O
density	B
but	O
may	O
be	O
sufﬁcient	O
in	O
speciﬁc	O
applications	O
.	O
two	O
approaches	O
to	O
this	O
problem	O
using	O
support	B
vector	I
machines	O
have	O
been	O
proposed	O
.	O
the	O
algorithm	O
of	O
sch¨olkopf	O
et	O
al	O
.	O
(	O
2001	O
)	O
tries	O
to	O
ﬁnd	O
a	O
hyperplane	O
that	O
separates	O
all	O
but	O
a	O
ﬁxed	O
fraction	O
ν	O
of	O
the	O
training	B
data	O
from	O
the	O
origin	O
while	O
at	O
the	O
same	O
time	O
maximizing	O
the	O
distance	O
(	O
margin	B
)	O
of	O
the	O
hyperplane	O
from	O
the	O
origin	O
,	O
while	O
tax	O
and	O
duin	O
(	O
1999	O
)	O
look	O
for	O
the	O
smallest	O
sphere	O
in	O
feature	B
space	I
that	O
contains	O
all	O
but	O
a	O
fraction	O
ν	O
of	O
the	O
data	O
points	O
.	O
for	O
kernels	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
that	O
are	O
functions	O
only	O
of	O
x	O
−	O
x	O
(	O
cid:4	O
)	O
,	O
the	O
two	O
algorithms	O
are	O
equivalent	O
.	O
7.1.4	O
svms	O
for	B
regression	I
we	O
now	O
extend	O
support	B
vector	I
machines	O
to	O
regression	B
problems	O
while	O
at	O
the	O
same	O
time	O
preserving	O
the	O
property	O
of	O
sparseness	O
.	O
in	O
simple	O
linear	B
regression	I
,	O
we	O
section	O
3.1.4	O
340	O
7.	O
sparse	O
kernel	O
machines	O
figure	O
7.6	O
plot	O
of	O
an	O
-insensitive	B
error	I
function	I
(	O
in	O
red	O
)	O
in	O
which	O
the	O
error	B
increases	O
lin-	O
early	O
with	O
distance	O
beyond	O
the	O
insen-	O
sitive	O
region	O
.	O
also	O
shown	O
for	O
compar-	O
ison	O
is	O
the	O
quadratic	O
error	O
function	O
(	O
in	O
green	O
)	O
.	O
e	O
(	O
z	O
)	O
−	O
0	O
	O
z	O
minimize	O
a	O
regularized	O
error	O
function	O
given	O
by	O
{	O
yn	O
−	O
tn	O
}	O
2	O
+	O
λ	O
2	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
2	O
.	O
(	O
7.50	O
)	O
to	O
obtain	O
sparse	O
solutions	O
,	O
the	O
quadratic	O
error	O
function	O
is	O
replaced	O
by	O
an	O
-insensitive	B
error	I
function	I
(	O
vapnik	O
,	O
1995	O
)	O
,	O
which	O
gives	O
zero	O
error	B
if	O
the	O
absolute	O
difference	O
be-	O
tween	O
the	O
prediction	O
y	O
(	O
x	O
)	O
and	O
the	O
target	O
t	O
is	O
less	O
than	O
	O
where	O
	O
>	O
0.	O
a	O
simple	O
example	O
of	O
an	O
-insensitive	B
error	I
function	I
,	O
having	O
a	O
linear	O
cost	O
associated	O
with	O
errors	O
outside	O
the	O
insensitive	O
region	O
,	O
is	O
given	O
by	O
e	O
(	O
y	O
(	O
x	O
)	O
−	O
t	O
)	O
=	O
0	O
,	O
|y	O
(	O
x	O
)	O
−	O
t|	O
−	O
	O
,	O
otherwise	O
if	O
|y	O
(	O
x	O
)	O
−	O
t|	O
<	O
	O
;	O
n	O
(	O
cid:2	O
)	O
n=1	O
1	O
2	O
(	O
cid:12	O
)	O
n	O
(	O
cid:2	O
)	O
c	O
n=1	O
and	O
is	O
illustrated	O
in	O
figure	O
7.6.	O
we	O
therefore	O
minimize	O
a	O
regularized	O
error	O
function	O
given	O
by	O
e	O
(	O
y	O
(	O
xn	O
)	O
−	O
tn	O
)	O
+	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
2	O
1	O
2	O
where	O
y	O
(	O
x	O
)	O
is	O
given	O
by	O
(	O
7.1	O
)	O
.	O
by	O
convention	O
the	O
(	O
inverse	B
)	O
regularization	B
parameter	O
,	O
denoted	O
c	O
,	O
appears	O
in	O
front	O
of	O
the	O
error	B
term	O
.	O
as	O
before	O
,	O
we	O
can	O
re-express	O
the	O
optimization	O
problem	O
by	O
introducing	O
slack	O
variables	O
.	O
for	O
each	O
data	O
point	O
xn	O
,	O
we	O
now	O
need	O
two	O
slack	O
variables	O
ξn	O
(	O
cid:2	O
)	O
0	O
and	O
corresponds	O
to	O
a	O
point	O
for	O
which	O
tn	O
<	O
y	O
(	O
xn	O
)	O
−	O
	O
,	O
as	O
illustrated	O
in	O
figure	O
7.7	O
.	O
(	O
cid:1	O
)	O
ξn	O
(	O
cid:2	O
)	O
0	O
,	O
where	O
ξn	O
>	O
0	O
corresponds	O
to	O
a	O
point	O
for	O
which	O
tn	O
>	O
y	O
(	O
xn	O
)	O
+	O
	O
,	O
and	O
(	O
cid:1	O
)	O
ξn	O
>	O
0	O
the	O
condition	O
for	O
a	O
target	O
point	O
to	O
lie	O
inside	O
the	O
-tube	B
is	O
that	O
yn	O
−	O
	O
(	O
cid:1	O
)	O
tn	O
(	O
cid:1	O
)	O
yn+	O
,	O
where	O
yn	O
=	O
y	O
(	O
xn	O
)	O
.	O
introducing	O
the	O
slack	O
variables	O
allows	O
points	O
to	O
lie	O
outside	O
the	O
tube	O
provided	O
the	O
slack	O
variables	O
are	O
nonzero	O
,	O
and	O
the	O
corresponding	O
conditions	O
are	O
tn	O
(	O
cid:1	O
)	O
y	O
(	O
xn	O
)	O
+	O
	O
+	O
ξn	O
tn	O
(	O
cid:2	O
)	O
y	O
(	O
xn	O
)	O
−	O
	O
−	O
(	O
cid:1	O
)	O
ξn	O
.	O
(	O
7.51	O
)	O
(	O
7.52	O
)	O
(	O
7.53	O
)	O
(	O
7.54	O
)	O
7.1.	O
maximum	B
margin	I
classiﬁers	O
341	O
figure	O
7.7	O
illustration	O
of	O
svm	O
regression	B
,	O
showing	O
the	O
regression	B
curve	O
together	O
with	O
the	O
-	O
insensitive	O
‘	O
tube	O
’	O
.	O
also	O
shown	O
are	O
exam-	O
ples	O
of	O
the	O
slack	O
variables	O
ξ	O
and	O
bξ	O
.	O
points	O
above	O
the	O
-tube	B
have	O
ξ	O
>	O
0	O
and	O
bξ	O
=	O
0	O
,	O
points	O
below	O
the	O
-tube	B
have	O
ξ	O
=	O
0	O
and	O
bξ	O
>	O
0	O
,	O
and	O
points	O
inside	O
the	O
-tube	B
have	O
ξ	O
=	O
bξ	O
=	O
0.	O
y	O
(	O
x	O
)	O
ξ	O
>	O
0	O
(	O
cid:1	O
)	O
ξ	O
>	O
0	O
y	O
+	O
	O
y	O
y	O
−	O
	O
x	O
the	O
error	B
function	I
for	O
support	B
vector	I
regression	O
can	O
then	O
be	O
written	O
as	O
n	O
(	O
cid:2	O
)	O
n=1	O
c	O
(	O
ξn	O
+	O
(	O
cid:1	O
)	O
ξn	O
)	O
+	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
2	O
1	O
2	O
(	O
7.55	O
)	O
(	O
7.53	O
)	O
and	O
(	O
7.54	O
)	O
.	O
this	O
can	O
be	O
achieved	O
by	O
introducing	O
lagrange	O
multipliers	O
an	O
(	O
cid:2	O
)	O
0	O
,	O
which	O
must	O
be	O
minimized	O
subject	O
to	O
the	O
constraints	O
ξn	O
(	O
cid:2	O
)	O
0	O
and	O
(	O
cid:1	O
)	O
ξn	O
(	O
cid:2	O
)	O
0	O
as	O
well	O
as	O
(	O
cid:1	O
)	O
an	O
(	O
cid:2	O
)	O
0	O
,	O
µn	O
(	O
cid:2	O
)	O
0	O
,	O
and	O
(	O
cid:1	O
)	O
µn	O
(	O
cid:2	O
)	O
0	O
and	O
optimizing	O
the	O
lagrangian	O
(	O
µnξn	O
+	O
(	O
cid:1	O
)	O
µn	O
(	O
cid:1	O
)	O
an	O
(	O
	O
+	O
(	O
cid:1	O
)	O
ξn	O
−	O
yn	O
+	O
tn	O
)	O
.	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
2	O
−	O
n	O
(	O
cid:2	O
)	O
(	O
ξn	O
+	O
(	O
cid:1	O
)	O
ξn	O
)	O
+	O
an	O
(	O
	O
+	O
ξn	O
+	O
yn	O
−	O
tn	O
)	O
−	O
n	O
(	O
cid:2	O
)	O
grangian	O
with	O
respect	O
to	O
w	O
,	O
b	O
,	O
ξn	O
,	O
and	O
(	O
cid:1	O
)	O
ξn	O
to	O
zero	O
,	O
giving	O
we	O
now	O
substitute	O
for	O
y	O
(	O
x	O
)	O
using	O
(	O
7.1	O
)	O
and	O
then	O
set	O
the	O
derivatives	O
of	O
the	O
la-	O
(	O
cid:1	O
)	O
ξn	O
)	O
n	O
(	O
cid:2	O
)	O
−	O
n	O
(	O
cid:2	O
)	O
n=1	O
l	O
=	O
c	O
(	O
7.56	O
)	O
1	O
2	O
n=1	O
n=1	O
n=1	O
(	O
an	O
−	O
(	O
cid:1	O
)	O
an	O
)	O
φ	O
(	O
xn	O
)	O
n=1	O
=	O
0	O
⇒	O
w	O
=	O
n	O
(	O
cid:2	O
)	O
=	O
0	O
⇒	O
n	O
(	O
cid:2	O
)	O
(	O
an	O
−	O
(	O
cid:1	O
)	O
an	O
)	O
=	O
0	O
=	O
0	O
⇒	O
(	O
cid:1	O
)	O
an	O
+	O
(	O
cid:1	O
)	O
µn	O
=	O
c.	O
=	O
0	O
⇒	O
an	O
+	O
µn	O
=	O
c	O
n=1	O
∂l	O
∂w	O
∂l	O
∂b	O
∂l	O
∂ξn	O
∂l	O
∂	O
(	O
cid:1	O
)	O
ξn	O
(	O
7.57	O
)	O
(	O
7.58	O
)	O
(	O
7.59	O
)	O
(	O
7.60	O
)	O
exercise	O
7.7	O
using	O
these	O
results	O
to	O
eliminate	O
the	O
corresponding	O
variables	O
from	O
the	O
lagrangian	O
,	O
we	O
see	O
that	O
the	O
dual	O
problem	O
involves	O
maximizing	O
342	O
7.	O
sparse	O
kernel	O
machines	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:4	O
)	O
l	O
(	O
a	O
,	O
(	O
cid:1	O
)	O
a	O
)	O
=	O
−1	O
n	O
(	O
cid:2	O
)	O
(	O
an	O
+	O
(	O
cid:1	O
)	O
an	O
)	O
+	O
−	O
m=1	O
n=1	O
2	O
(	O
an	O
−	O
(	O
cid:1	O
)	O
an	O
)	O
(	O
am	O
−	O
(	O
cid:1	O
)	O
am	O
)	O
k	O
(	O
xn	O
,	O
xm	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
an	O
−	O
(	O
cid:1	O
)	O
an	O
)	O
tn	O
n=1	O
with	O
respect	O
to	O
{	O
an	O
}	O
and	O
{	O
(	O
cid:1	O
)	O
an	O
}	O
,	O
where	O
we	O
have	O
introduced	O
the	O
kernel	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
we	O
note	O
that	O
an	O
(	O
cid:2	O
)	O
0	O
and	O
(	O
cid:1	O
)	O
an	O
(	O
cid:2	O
)	O
0	O
are	O
both	O
required	O
because	O
these	O
are	O
lagrange	O
multipliers	O
.	O
also	O
µn	O
(	O
cid:2	O
)	O
0	O
and	O
(	O
cid:1	O
)	O
µn	O
(	O
cid:2	O
)	O
0	O
together	O
with	O
(	O
7.59	O
)	O
and	O
(	O
7.60	O
)	O
,	O
require	O
an	O
(	O
cid:1	O
)	O
c	O
and	O
(	O
cid:1	O
)	O
an	O
(	O
cid:1	O
)	O
c	O
,	O
and	O
so	O
again	O
we	O
have	O
the	O
box	B
constraints	I
φ	O
(	O
x	O
)	O
tφ	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
.	O
again	O
,	O
this	O
is	O
a	O
constrained	O
maximization	O
,	O
and	O
to	O
ﬁnd	O
the	O
constraints	O
n=1	O
(	O
7.61	O
)	O
together	O
with	O
the	O
condition	O
(	O
7.58	O
)	O
.	O
substituting	O
(	O
7.57	O
)	O
into	O
(	O
7.1	O
)	O
,	O
we	O
see	O
that	O
predictions	O
for	O
new	O
inputs	O
can	O
be	O
made	O
using	O
y	O
(	O
x	O
)	O
=	O
(	O
an	O
−	O
(	O
cid:1	O
)	O
an	O
)	O
k	O
(	O
x	O
,	O
xn	O
)	O
+	O
b	O
which	O
is	O
again	O
expressed	O
in	O
terms	O
of	O
the	O
kernel	B
function	I
.	O
n=1	O
the	O
corresponding	O
karush-kuhn-tucker	O
(	O
kkt	O
)	O
conditions	O
,	O
which	O
state	O
that	O
at	O
the	O
solution	O
the	O
product	O
of	O
the	O
dual	O
variables	O
and	O
the	O
constraints	O
must	O
vanish	O
,	O
are	O
given	O
by	O
and	O
such	O
points	O
must	O
lie	O
either	O
on	O
or	O
below	O
the	O
lower	O
boundary	O
of	O
the	O
-tube	B
.	O
from	O
these	O
we	O
can	O
obtain	O
several	O
useful	O
results	O
.	O
first	O
of	O
all	O
,	O
we	O
note	O
that	O
a	O
coefﬁcient	O
an	O
can	O
only	O
be	O
nonzero	O
if	O
	O
+	O
ξn	O
+	O
yn	O
−	O
tn	O
=	O
0	O
,	O
which	O
implies	O
that	O
the	O
data	O
point	O
either	O
lies	O
on	O
the	O
upper	O
boundary	O
of	O
the	O
-tube	B
(	O
ξn	O
=	O
0	O
)	O
or	O
lies	O
above	O
the	O
upper	O
boundary	O
(	O
ξn	O
>	O
0	O
)	O
.	O
similarly	O
,	O
a	O
nonzero	O
value	O
for	O
(	O
cid:1	O
)	O
an	O
implies	O
	O
+	O
(	O
cid:1	O
)	O
ξn	O
−	O
yn	O
+	O
tn	O
=	O
0	O
,	O
furthermore	O
,	O
the	O
two	O
constraints	O
	O
+	O
ξn	O
+	O
yn	O
−	O
tn	O
=	O
0	O
and	O
	O
+	O
(	O
cid:1	O
)	O
ξn	O
−	O
yn	O
+	O
tn	O
=	O
0	O
(	O
cid:1	O
)	O
ξn	O
are	O
nonnegative	O
while	O
	O
is	O
strictly	O
positive	O
,	O
and	O
so	O
for	O
every	O
data	O
point	O
xn	O
,	O
either	O
an	O
or	O
(	O
cid:1	O
)	O
an	O
(	O
or	O
both	O
)	O
must	O
be	O
zero	O
.	O
(	O
7.64	O
)	O
,	O
in	O
other	O
words	O
those	O
for	O
which	O
either	O
an	O
(	O
cid:9	O
)	O
=	O
0	O
or	O
(	O
cid:1	O
)	O
an	O
(	O
cid:9	O
)	O
=	O
0.	O
these	O
are	O
points	O
that	O
are	O
incompatible	O
,	O
as	O
is	O
easily	O
seen	O
by	O
adding	O
them	O
together	O
and	O
noting	O
that	O
ξn	O
and	O
the	O
support	O
vectors	O
are	O
those	O
data	O
points	O
that	O
contribute	O
to	O
predictions	O
given	O
by	O
lie	O
on	O
the	O
boundary	O
of	O
the	O
-tube	B
or	O
outside	O
the	O
tube	O
.	O
all	O
points	O
within	O
the	O
tube	O
have	O
0	O
(	O
cid:1	O
)	O
an	O
(	O
cid:1	O
)	O
c	O
0	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
an	O
(	O
cid:1	O
)	O
c	O
n	O
(	O
cid:2	O
)	O
an	O
(	O
	O
+	O
ξn	O
+	O
yn	O
−	O
tn	O
)	O
=	O
0	O
(	O
cid:1	O
)	O
an	O
(	O
	O
+	O
(	O
cid:1	O
)	O
ξn	O
−	O
yn	O
+	O
tn	O
)	O
=	O
0	O
(	O
c	O
−	O
(	O
cid:1	O
)	O
an	O
)	O
(	O
cid:1	O
)	O
ξn	O
=	O
0	O
.	O
(	O
c	O
−	O
an	O
)	O
ξn	O
=	O
0	O
(	O
7.62	O
)	O
(	O
7.63	O
)	O
(	O
7.64	O
)	O
(	O
7.65	O
)	O
(	O
7.66	O
)	O
(	O
7.67	O
)	O
(	O
7.68	O
)	O
an	O
=	O
(	O
cid:1	O
)	O
an	O
=	O
0.	O
we	O
again	O
have	O
a	O
sparse	O
solution	O
,	O
and	O
the	O
only	O
terms	O
that	O
have	O
to	O
be	O
7.1.	O
maximum	B
margin	I
classiﬁers	O
343	O
evaluated	O
in	O
the	O
predictive	O
model	O
(	O
7.64	O
)	O
are	O
those	O
that	O
involve	O
the	O
support	O
vectors	O
.	O
the	O
parameter	O
b	O
can	O
be	O
found	O
by	O
considering	O
a	O
data	O
point	O
for	O
which	O
0	O
<	O
an	O
<	O
c	O
,	O
which	O
from	O
(	O
7.67	O
)	O
must	O
have	O
ξn	O
=	O
0	O
,	O
and	O
from	O
(	O
7.65	O
)	O
must	O
therefore	O
satisfy	O
	O
+	O
yn	O
−	O
tn	O
=	O
0.	O
using	O
(	O
7.1	O
)	O
and	O
solving	O
for	O
b	O
,	O
we	O
obtain	O
b	O
=	O
tn	O
−	O
	O
−	O
wtφ	O
(	O
xn	O
)	O
=	O
tn	O
−	O
	O
−	O
n	O
(	O
cid:2	O
)	O
(	O
am	O
−	O
(	O
cid:1	O
)	O
am	O
)	O
k	O
(	O
xn	O
,	O
xm	O
)	O
(	O
7.69	O
)	O
for	O
which	O
0	O
<	O
(	O
cid:1	O
)	O
an	O
<	O
c.	O
in	O
practice	O
,	O
it	O
is	O
better	O
to	O
average	O
over	O
all	O
such	O
estimates	O
of	O
where	O
we	O
have	O
used	O
(	O
7.57	O
)	O
.	O
we	O
can	O
obtain	O
an	O
analogous	O
result	O
by	O
considering	O
a	O
point	O
m=1	O
b.	O
as	O
with	O
the	O
classiﬁcation	B
case	O
,	O
there	O
is	O
an	O
alternative	O
formulation	O
of	O
the	O
svm	O
for	B
regression	I
in	O
which	O
the	O
parameter	O
governing	O
complexity	O
has	O
a	O
more	O
intuitive	O
interpretation	O
(	O
sch¨olkopf	O
et	O
al.	O
,	O
2000	O
)	O
.	O
in	O
particular	O
,	O
instead	O
of	O
ﬁxing	O
the	O
width	O
	O
of	O
the	O
insensitive	O
region	O
,	O
we	O
ﬁx	O
instead	O
a	O
parameter	O
ν	O
that	O
bounds	O
the	O
fraction	O
of	O
points	O
lying	O
outside	O
the	O
tube	O
.	O
this	O
involves	O
maximizing	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:4	O
)	O
l	O
(	O
a	O
,	O
(	O
cid:1	O
)	O
a	O
)	O
=	O
−1	O
n	O
(	O
cid:2	O
)	O
(	O
an	O
−	O
(	O
cid:1	O
)	O
an	O
)	O
tn	O
m=1	O
n=1	O
+	O
2	O
(	O
an	O
−	O
(	O
cid:1	O
)	O
an	O
)	O
(	O
am	O
−	O
(	O
cid:1	O
)	O
am	O
)	O
k	O
(	O
xn	O
,	O
xm	O
)	O
subject	O
to	O
the	O
constraints	O
n=1	O
0	O
(	O
cid:1	O
)	O
an	O
(	O
cid:1	O
)	O
c/n	O
0	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
an	O
(	O
cid:1	O
)	O
c/n	O
n	O
(	O
cid:2	O
)	O
(	O
an	O
−	O
(	O
cid:1	O
)	O
an	O
)	O
=	O
0	O
n	O
(	O
cid:2	O
)	O
(	O
an	O
+	O
(	O
cid:1	O
)	O
an	O
)	O
(	O
cid:1	O
)	O
νc	O
.	O
n=1	O
(	O
7.70	O
)	O
(	O
7.71	O
)	O
(	O
7.72	O
)	O
(	O
7.73	O
)	O
(	O
7.74	O
)	O
n=1	O
it	O
can	O
be	O
shown	O
that	O
there	O
are	O
at	O
most	O
νn	O
data	O
points	O
falling	O
outside	O
the	O
insensitive	O
tube	O
,	O
while	O
at	O
least	O
νn	O
data	O
points	O
are	O
support	O
vectors	O
and	O
so	O
lie	O
either	O
on	O
the	O
tube	O
or	O
outside	O
it	O
.	O
the	O
use	O
of	O
a	O
support	B
vector	I
machine	I
to	O
solve	O
a	O
regression	B
problem	O
is	O
illustrated	O
using	O
the	O
sinusoidal	B
data	I
set	O
in	O
figure	O
7.8.	O
here	O
the	O
parameters	O
ν	O
and	O
c	O
have	O
been	O
chosen	O
by	O
hand	O
.	O
in	O
practice	O
,	O
their	O
values	O
would	O
typically	O
be	O
determined	O
by	O
cross-	O
validation	O
.	O
appendix	O
a	O
344	O
7.	O
sparse	O
kernel	O
machines	O
figure	O
7.8	O
illustration	O
of	O
the	O
ν-svm	O
for	O
re-	O
gression	O
applied	O
to	O
the	O
sinusoidal	O
synthetic	O
data	O
set	O
using	O
gaussian	O
kernels	O
.	O
the	O
predicted	O
regression	B
curve	O
is	O
shown	O
by	O
the	O
red	O
line	O
,	O
and	O
the	O
-insensitive	O
tube	O
corresponds	O
to	O
the	O
shaded	O
region	O
.	O
also	O
,	O
the	O
data	O
points	O
are	O
shown	O
in	O
green	O
,	O
and	O
those	O
with	O
support	O
vectors	O
are	O
indicated	O
by	O
blue	O
circles	O
.	O
t	O
1	O
0	O
−1	O
0	O
x	O
1	O
7.1.5	O
computational	B
learning	I
theory	I
historically	O
,	O
support	B
vector	I
machines	O
have	O
largely	O
been	O
motivated	O
and	O
analysed	O
using	O
a	O
theoretical	O
framework	O
known	O
as	O
computational	B
learning	I
theory	I
,	O
also	O
some-	O
times	O
called	O
statistical	B
learning	I
theory	I
(	O
anthony	O
and	O
biggs	O
,	O
1992	O
;	O
kearns	O
and	O
vazi-	O
rani	O
,	O
1994	O
;	O
vapnik	O
,	O
1995	O
;	O
vapnik	O
,	O
1998	O
)	O
.	O
this	O
has	O
its	O
origins	O
with	O
valiant	O
(	O
1984	O
)	O
who	O
formulated	O
the	O
probably	B
approximately	I
correct	I
,	O
or	O
pac	O
,	O
learning	B
framework	O
.	O
the	O
goal	O
of	O
the	O
pac	O
framework	O
is	O
to	O
understand	O
how	O
large	O
a	O
data	O
set	O
needs	O
to	O
be	O
in	O
order	O
to	O
give	O
good	O
generalization	B
.	O
it	O
also	O
gives	O
bounds	O
for	O
the	O
computational	O
cost	O
of	O
learning	B
,	O
although	O
we	O
do	O
not	O
consider	O
these	O
here	O
.	O
suppose	O
that	O
a	O
data	O
set	O
d	O
of	O
size	O
n	O
is	O
drawn	O
from	O
some	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
t	O
)	O
where	O
x	O
is	O
the	O
input	O
variable	O
and	O
t	O
represents	O
the	O
class	O
label	O
,	O
and	O
that	O
we	O
restrict	O
attention	O
to	O
‘	O
noise	O
free	O
’	O
situations	O
in	O
which	O
the	O
class	O
labels	O
are	O
determined	O
by	O
some	O
(	O
unknown	O
)	O
deterministic	O
function	O
t	O
=	O
g	O
(	O
x	O
)	O
.	O
in	O
pac	O
learning	B
we	O
say	O
that	O
a	O
function	O
f	O
(	O
x	O
;	O
d	O
)	O
,	O
drawn	O
from	O
a	O
space	O
f	O
of	O
such	O
functions	O
on	O
the	O
basis	O
of	O
the	O
training	B
set	I
d	O
,	O
has	O
good	O
generalization	B
if	O
its	O
expected	O
error	B
rate	O
is	O
below	O
some	O
pre-speciﬁed	O
threshold	O
	O
,	O
so	O
that	O
(	O
7.75	O
)	O
where	O
i	O
(	O
·	O
)	O
is	O
the	O
indicator	O
function	O
,	O
and	O
the	O
expectation	B
is	O
with	O
respect	O
to	O
the	O
dis-	O
tribution	O
p	O
(	O
x	O
,	O
t	O
)	O
.	O
the	O
quantity	O
on	O
the	O
left-hand	O
side	O
is	O
a	O
random	O
variable	O
,	O
because	O
it	O
depends	O
on	O
the	O
training	B
set	I
d	O
,	O
and	O
the	O
pac	O
framework	O
requires	O
that	O
(	O
7.75	O
)	O
holds	O
,	O
with	O
probability	B
greater	O
than	O
1	O
−	O
δ	O
,	O
for	O
a	O
data	O
set	O
d	O
drawn	O
randomly	O
from	O
p	O
(	O
x	O
,	O
t	O
)	O
.	O
here	O
δ	O
is	O
another	O
pre-speciﬁed	O
parameter	O
,	O
and	O
the	O
terminology	O
‘	O
probably	O
approxi-	O
mately	O
correct	O
’	O
comes	O
from	O
the	O
requirement	O
that	O
with	O
high	O
probability	B
(	O
greater	O
than	O
1−	O
δ	O
)	O
,	O
the	O
error	B
rate	O
be	O
small	O
(	O
less	O
than	O
	O
)	O
.	O
for	O
a	O
given	O
choice	O
of	O
model	O
space	O
f	O
,	O
and	O
for	O
given	O
parameters	O
	O
and	O
δ	O
,	O
pac	O
learning	B
aims	O
to	O
provide	O
bounds	O
on	O
the	O
minimum	O
size	O
n	O
of	O
data	O
set	O
needed	O
to	O
meet	O
this	O
criterion	O
.	O
a	O
key	O
quantity	O
in	O
pac	O
learning	B
is	O
the	O
vapnik-chervonenkis	O
dimension	O
,	O
or	O
vc	O
dimension	O
,	O
which	O
provides	O
a	O
measure	O
of	O
the	O
complexity	O
of	O
a	O
space	O
of	O
functions	O
,	O
and	O
which	O
allows	O
the	O
pac	O
framework	O
to	O
be	O
extended	B
to	O
spaces	O
containing	O
an	O
inﬁnite	O
number	O
of	O
functions	O
.	O
ex	O
,	O
t	O
[	O
i	O
(	O
f	O
(	O
x	O
;	O
d	O
)	O
(	O
cid:9	O
)	O
=	O
t	O
)	O
]	O
<	O
	O
the	O
bounds	O
derived	O
within	O
the	O
pac	O
framework	O
are	O
often	O
described	O
as	O
worst-	O
7.2.	O
relevance	B
vector	I
machines	O
345	O
case	O
,	O
because	O
they	O
apply	O
to	O
any	O
choice	O
for	O
the	O
distribution	O
p	O
(	O
x	O
,	O
t	O
)	O
,	O
so	O
long	O
as	O
both	O
the	O
training	B
and	O
the	O
test	O
examples	O
are	O
drawn	O
(	O
independently	O
)	O
from	O
the	O
same	O
distribu-	O
tion	O
,	O
and	O
for	O
any	O
choice	O
for	O
the	O
function	O
f	O
(	O
x	O
)	O
so	O
long	O
as	O
it	O
belongs	O
to	O
f.	O
in	O
real-world	O
applications	O
of	O
machine	O
learning	O
,	O
we	O
deal	O
with	O
distributions	O
that	O
have	O
signiﬁcant	O
reg-	O
ularity	O
,	O
for	O
example	O
in	O
which	O
large	O
regions	O
of	O
input	O
space	O
carry	O
the	O
same	O
class	O
label	O
.	O
as	O
a	O
consequence	O
of	O
the	O
lack	O
of	O
any	O
assumptions	O
about	O
the	O
form	O
of	O
the	O
distribution	O
,	O
the	O
pac	O
bounds	O
are	O
very	O
conservative	O
,	O
in	O
other	O
words	O
they	O
strongly	O
over-estimate	O
the	O
size	O
of	O
data	O
sets	O
required	O
to	O
achieve	O
a	O
given	O
generalization	B
performance	O
.	O
for	O
this	O
reason	O
,	O
pac	O
bounds	O
have	O
found	O
few	O
,	O
if	O
any	O
,	O
practical	O
applications	O
.	O
one	O
attempt	O
to	O
improve	O
the	O
tightness	O
of	O
the	O
pac	O
bounds	O
is	O
the	O
pac-bayesian	O
framework	O
(	O
mcallester	O
,	O
2003	O
)	O
,	O
which	O
considers	O
a	O
distribution	O
over	O
the	O
space	O
f	O
of	O
functions	O
,	O
somewhat	O
analogous	O
to	O
the	O
prior	B
in	O
a	O
bayesian	O
treatment	O
.	O
this	O
still	O
con-	O
siders	O
any	O
possible	O
choice	O
for	O
p	O
(	O
x	O
,	O
t	O
)	O
,	O
and	O
so	O
although	O
the	O
bounds	O
are	O
tighter	O
,	O
they	O
are	O
still	O
very	O
conservative	O
.	O
7.2.	O
relevance	B
vector	I
machines	O
support	B
vector	I
machines	O
have	O
been	O
used	O
in	O
a	O
variety	O
of	O
classiﬁcation	O
and	O
regres-	O
sion	B
applications	O
.	O
nevertheless	O
,	O
they	O
suffer	O
from	O
a	O
number	O
of	O
limitations	O
,	O
several	O
of	O
which	O
have	O
been	O
highlighted	O
already	O
in	O
this	O
chapter	O
.	O
in	O
particular	O
,	O
the	O
outputs	O
of	O
an	O
svm	O
represent	O
decisions	O
rather	O
than	O
posterior	O
probabilities	O
.	O
also	O
,	O
the	O
svm	O
was	O
originally	O
formulated	O
for	O
two	O
classes	O
,	O
and	O
the	O
extension	O
to	O
k	O
>	O
2	O
classes	O
is	O
prob-	O
lematic	O
.	O
there	O
is	O
a	O
complexity	O
parameter	O
c	O
,	O
or	O
ν	O
(	O
as	O
well	O
as	O
a	O
parameter	O
	O
in	O
the	O
case	O
of	O
regression	B
)	O
,	O
that	O
must	O
be	O
found	O
using	O
a	O
hold-out	O
method	O
such	O
as	O
cross-validation	B
.	O
finally	O
,	O
predictions	O
are	O
expressed	O
as	O
linear	O
combinations	O
of	O
kernel	O
functions	O
that	O
are	O
centred	O
on	O
training	B
data	O
points	O
and	O
that	O
are	O
required	O
to	O
be	O
positive	B
deﬁnite	I
.	O
the	O
relevance	B
vector	I
machine	I
or	O
rvm	O
(	O
tipping	O
,	O
2001	O
)	O
is	O
a	O
bayesian	O
sparse	O
ker-	O
nel	O
technique	O
for	B
regression	I
and	O
classiﬁcation	B
that	O
shares	O
many	O
of	O
the	O
characteristics	O
of	O
the	O
svm	O
whilst	O
avoiding	O
its	O
principal	O
limitations	O
.	O
additionally	O
,	O
it	O
typically	O
leads	O
to	O
much	O
sparser	O
models	O
resulting	O
in	O
correspondingly	O
faster	O
performance	O
on	O
test	O
data	O
whilst	O
maintaining	O
comparable	O
generalization	B
error	O
.	O
in	O
contrast	O
to	O
the	O
svm	O
we	O
shall	O
ﬁnd	O
it	O
more	O
convenient	O
to	O
introduce	O
the	O
regres-	O
sion	B
form	O
of	O
the	O
rvm	O
ﬁrst	O
and	O
then	O
consider	O
the	O
extension	O
to	O
classiﬁcation	B
tasks	O
.	O
7.2.1	O
rvm	O
for	B
regression	I
the	O
relevance	B
vector	I
machine	I
for	O
regression	B
is	O
a	O
linear	O
model	O
of	O
the	O
form	O
studied	O
in	O
chapter	O
3	O
but	O
with	O
a	O
modiﬁed	O
prior	B
that	O
results	O
in	O
sparse	O
solutions	O
.	O
the	O
model	O
deﬁnes	O
a	O
conditional	B
distribution	O
for	O
a	O
real-valued	O
target	O
variable	O
t	O
,	O
given	O
an	O
input	O
vector	O
x	O
,	O
which	O
takes	O
the	O
form	O
p	O
(	O
t|x	O
,	O
w	O
,	O
β	O
)	O
=	O
n	O
(	O
t|y	O
(	O
x	O
)	O
,	O
β	O
−1	O
)	O
(	O
7.76	O
)	O
346	O
7.	O
sparse	O
kernel	O
machines	O
m	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
where	O
β	O
=	O
σ	O
by	O
a	O
linear	O
model	O
of	O
the	O
form	O
−2	O
is	O
the	O
noise	O
precision	O
(	O
inverse	B
noise	O
variance	B
)	O
,	O
and	O
the	O
mean	B
is	O
given	O
y	O
(	O
x	O
)	O
=	O
wiφi	O
(	O
x	O
)	O
=	O
wtφ	O
(	O
x	O
)	O
(	O
7.77	O
)	O
i=1	O
with	O
ﬁxed	O
nonlinear	O
basis	O
functions	O
φi	O
(	O
x	O
)	O
,	O
which	O
will	O
typically	O
include	O
a	O
constant	O
term	O
so	O
that	O
the	O
corresponding	O
weight	B
parameter	I
represents	O
a	O
‘	O
bias	B
’	O
.	O
the	O
relevance	B
vector	I
machine	I
is	O
a	O
speciﬁc	O
instance	O
of	O
this	O
model	O
,	O
which	O
is	O
in-	O
tended	O
to	O
mirror	O
the	O
structure	O
of	O
the	O
support	B
vector	I
machine	I
.	O
in	O
particular	O
,	O
the	O
basis	O
functions	O
are	O
given	O
by	O
kernels	O
,	O
with	O
one	O
kernel	O
associated	O
with	O
each	O
of	O
the	O
data	O
points	O
from	O
the	O
training	B
set	I
.	O
the	O
general	O
expression	O
(	O
7.77	O
)	O
then	O
takes	O
the	O
svm-like	O
form	O
y	O
(	O
x	O
)	O
=	O
wnk	O
(	O
x	O
,	O
xn	O
)	O
+	O
b	O
(	O
7.78	O
)	O
where	O
b	O
is	O
a	O
bias	B
parameter	I
.	O
the	O
number	O
of	O
parameters	O
in	O
this	O
case	O
is	O
m	O
=	O
n	O
+	O
1	O
,	O
and	O
y	O
(	O
x	O
)	O
has	O
the	O
same	O
form	O
as	O
the	O
predictive	O
model	O
(	O
7.64	O
)	O
for	O
the	O
svm	O
,	O
except	O
that	O
the	O
coefﬁcients	O
an	O
are	O
here	O
denoted	O
wn	O
.	O
it	O
should	O
be	O
emphasized	O
that	O
the	O
subsequent	O
analysis	O
is	O
valid	O
for	O
arbitrary	O
choices	O
of	O
basis	B
function	I
,	O
and	O
for	O
generality	O
we	O
shall	O
work	O
with	O
the	O
form	O
(	O
7.77	O
)	O
.	O
in	O
contrast	O
to	O
the	O
svm	O
,	O
there	O
is	O
no	O
restriction	O
to	O
positive-	O
deﬁnite	O
kernels	O
,	O
nor	O
are	O
the	O
basis	O
functions	O
tied	O
in	O
either	O
number	O
or	O
location	O
to	O
the	O
training	B
data	O
points	O
.	O
suppose	O
we	O
are	O
given	O
a	O
set	O
of	O
n	O
observations	O
of	O
the	O
input	O
vector	O
x	O
,	O
which	O
we	O
n	O
with	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
the	O
denote	O
collectively	O
by	O
a	O
data	O
matrix	O
x	O
whose	O
nth	O
row	O
is	O
xt	O
corresponding	O
target	O
values	O
are	O
given	O
by	O
t	O
=	O
(	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
)	O
t.	O
thus	O
,	O
the	O
likelihood	B
function	I
is	O
given	O
by	O
n	O
(	O
cid:14	O
)	O
n=1	O
m	O
(	O
cid:14	O
)	O
p	O
(	O
t|x	O
,	O
w	O
,	O
β	O
)	O
=	O
p	O
(	O
tn|xn	O
,	O
w	O
,	O
β	O
−1	O
)	O
.	O
(	O
7.79	O
)	O
next	O
we	O
introduce	O
a	O
prior	B
distribution	O
over	O
the	O
parameter	O
vector	O
w	O
and	O
as	O
in	O
chapter	O
3	O
,	O
we	O
shall	O
consider	O
a	O
zero-mean	O
gaussian	O
prior	B
.	O
however	O
,	O
the	O
key	O
differ-	O
ence	O
in	O
the	O
rvm	O
is	O
that	O
we	O
introduce	O
a	O
separate	O
hyperparameter	B
αi	O
for	O
each	O
of	O
the	O
weight	O
parameters	O
wi	O
instead	O
of	O
a	O
single	O
shared	O
hyperparameter	O
.	O
thus	O
the	O
weight	O
prior	O
takes	O
the	O
form	O
p	O
(	O
w|α	O
)	O
=	O
n	O
(	O
wi|0	O
,	O
α	O
−1	O
i	O
)	O
(	O
7.80	O
)	O
i=1	O
where	O
αi	O
represents	O
the	O
precision	O
of	O
the	O
corresponding	O
parameter	O
wi	O
,	O
and	O
α	O
denotes	O
(	O
α1	O
,	O
.	O
.	O
.	O
,	O
αm	O
)	O
t.	O
we	O
shall	O
see	O
that	O
,	O
when	O
we	O
maximize	O
the	O
evidence	O
with	O
respect	O
to	O
these	O
hyperparameters	O
,	O
a	O
signiﬁcant	O
proportion	O
of	O
them	O
go	O
to	O
inﬁnity	O
,	O
and	O
the	O
corresponding	O
weight	O
parameters	O
have	O
posterior	O
distributions	O
that	O
are	O
concentrated	O
at	O
zero	O
.	O
the	O
basis	O
functions	O
associated	O
with	O
these	O
parameters	O
therefore	O
play	O
no	O
role	O
7.2.	O
relevance	B
vector	I
machines	O
347	O
in	O
the	O
predictions	O
made	O
by	O
the	O
model	O
and	O
so	O
are	O
effectively	O
pruned	O
out	O
,	O
resulting	O
in	O
a	O
sparse	O
model	O
.	O
using	O
the	O
result	O
(	O
3.49	O
)	O
for	O
linear	O
regression	B
models	O
,	O
we	O
see	O
that	O
the	O
posterior	O
distribution	O
for	O
the	O
weights	O
is	O
again	O
gaussian	O
and	O
takes	O
the	O
form	O
p	O
(	O
w|t	O
,	O
x	O
,	O
α	O
,	O
β	O
)	O
=	O
n	O
(	O
w|m	O
,	O
σ	O
)	O
(	O
7.81	O
)	O
where	O
the	O
mean	B
and	O
covariance	B
are	O
given	O
by	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
−1	O
m	O
=	O
βσφtt	O
σ	O
=	O
(	O
7.82	O
)	O
(	O
7.83	O
)	O
where	O
φ	O
is	O
the	O
n	O
×	O
m	O
design	B
matrix	I
with	O
elements	O
φni	O
=	O
φi	O
(	O
xn	O
)	O
,	O
and	O
a	O
=	O
diag	O
(	O
αi	O
)	O
.	O
note	O
that	O
in	O
the	O
speciﬁc	O
case	O
of	O
the	O
model	O
(	O
7.78	O
)	O
,	O
we	O
have	O
φ	O
=	O
k	O
,	O
where	O
k	O
is	O
the	O
symmetric	O
(	O
n	O
+	O
1	O
)	O
×	O
(	O
n	O
+	O
1	O
)	O
kernel	O
matrix	O
with	O
elements	O
k	O
(	O
xn	O
,	O
xm	O
)	O
.	O
a	O
+	O
βφtφ	O
the	O
values	O
of	O
α	O
and	O
β	O
are	O
determined	O
using	O
type-2	O
maximum	B
likelihood	I
,	O
also	O
known	O
as	O
the	O
evidence	B
approximation	I
,	O
in	O
which	O
we	O
maximize	O
the	O
marginal	B
likeli-	O
hood	O
function	O
obtained	O
by	O
integrating	O
out	O
the	O
weight	O
parameters	O
p	O
(	O
t|x	O
,	O
w	O
,	O
β	O
)	O
p	O
(	O
w|α	O
)	O
dw	O
.	O
p	O
(	O
t|x	O
,	O
α	O
,	O
β	O
)	O
=	O
(	O
7.84	O
)	O
(	O
cid:6	O
)	O
because	O
this	O
represents	O
the	O
convolution	O
of	O
two	O
gaussians	O
,	O
it	O
is	O
readily	O
evaluated	O
to	O
give	O
the	O
log	O
marginal	O
likelihood	O
in	O
the	O
form	O
ln	O
p	O
(	O
t|x	O
,	O
α	O
,	O
β	O
)	O
=	O
lnn	O
(	O
t|0	O
,	O
c	O
)	O
(	O
cid:26	O
)	O
(	O
cid:27	O
)	O
n	O
ln	O
(	O
2π	O
)	O
+	O
ln|c|	O
+	O
ttc−1t	O
=	O
−1	O
2	O
where	O
t	O
=	O
(	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
)	O
t	O
,	O
and	O
we	O
have	O
deﬁned	O
the	O
n	O
×	O
n	O
matrix	O
c	O
given	O
by	O
c	O
=	O
β	O
−1i	O
+	O
φa−1φt	O
.	O
(	O
7.85	O
)	O
(	O
7.86	O
)	O
section	O
3.5	O
exercise	O
7.10	O
our	O
goal	O
is	O
now	O
to	O
maximize	O
(	O
7.85	O
)	O
with	O
respect	O
to	O
the	O
hyperparameters	O
α	O
and	O
β.	O
this	O
requires	O
only	O
a	O
small	O
modiﬁcation	O
to	O
the	O
results	O
obtained	O
in	O
section	O
3.5	O
for	O
the	O
evidence	B
approximation	I
in	O
the	O
linear	B
regression	I
model	O
.	O
again	O
,	O
we	O
can	O
identify	O
two	O
approaches	O
.	O
in	O
the	O
ﬁrst	O
,	O
we	O
simply	O
set	O
the	O
required	O
derivatives	O
of	O
the	O
marginal	B
likelihood	I
to	O
zero	O
and	O
obtain	O
the	O
following	O
re-estimation	O
equations	O
exercise	O
7.12	O
αnew	O
i	O
(	O
βnew	O
)	O
−1	O
=	O
=	O
γi	O
m2	O
i	O
(	O
cid:5	O
)	O
t	O
−	O
φm	O
(	O
cid:5	O
)	O
2	O
i	O
γi	O
n	O
−	O
(	O
cid:5	O
)	O
(	O
7.87	O
)	O
(	O
7.88	O
)	O
section	O
3.5.3	O
where	O
mi	O
is	O
the	O
ith	O
component	O
of	O
the	O
posterior	O
mean	O
m	O
deﬁned	O
by	O
(	O
7.82	O
)	O
.	O
the	O
quantity	O
γi	O
measures	O
how	O
well	O
the	O
corresponding	O
parameter	O
wi	O
is	O
determined	O
by	O
the	O
data	O
and	O
is	O
deﬁned	O
by	O
348	O
7.	O
sparse	O
kernel	O
machines	O
γi	O
=	O
1	O
−	O
αiσii	O
(	O
7.89	O
)	O
in	O
which	O
σii	O
is	O
the	O
ith	O
diagonal	B
component	O
of	O
the	O
posterior	O
covariance	O
σ	O
given	O
by	O
(	O
7.83	O
)	O
.	O
learning	B
therefore	O
proceeds	O
by	O
choosing	O
initial	O
values	O
for	O
α	O
and	O
β	O
,	O
evalu-	O
ating	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
posterior	O
using	O
(	O
7.82	O
)	O
and	O
(	O
7.83	O
)	O
,	O
respectively	O
,	O
and	O
then	O
alternately	O
re-estimating	O
the	O
hyperparameters	O
,	O
using	O
(	O
7.87	O
)	O
and	O
(	O
7.88	O
)	O
,	O
and	O
re-estimating	O
the	O
posterior	O
mean	O
and	O
covariance	B
,	O
using	O
(	O
7.82	O
)	O
and	O
(	O
7.83	O
)	O
,	O
until	O
a	O
suit-	O
able	O
convergence	O
criterion	O
is	O
satisﬁed	O
.	O
exercise	O
9.23	O
section	O
7.2.2	O
exercise	O
7.14	O
section	O
6.4.2	O
the	O
second	O
approach	O
is	O
to	O
use	O
the	O
em	O
algorithm	O
,	O
and	O
is	O
discussed	O
in	O
sec-	O
tion	O
9.3.4.	O
these	O
two	O
approaches	O
to	O
ﬁnding	O
the	O
values	O
of	O
the	O
hyperparameters	O
that	O
maximize	O
the	O
evidence	O
are	O
formally	O
equivalent	O
.	O
numerically	O
,	O
however	O
,	O
it	O
is	O
found	O
that	O
the	O
direct	O
optimization	O
approach	O
corresponding	O
to	O
(	O
7.87	O
)	O
and	O
(	O
7.88	O
)	O
gives	O
some-	O
what	O
faster	O
convergence	O
(	O
tipping	O
,	O
2001	O
)	O
.	O
as	O
a	O
result	O
of	O
the	O
optimization	O
,	O
we	O
ﬁnd	O
that	O
a	O
proportion	O
of	O
the	O
hyperparameters	O
{	O
αi	O
}	O
are	O
driven	O
to	O
large	O
(	O
in	O
principle	O
inﬁnite	O
)	O
values	O
,	O
and	O
so	O
the	O
weight	O
parameters	O
wi	O
corresponding	O
to	O
these	O
hyperparameters	O
have	O
posterior	O
distributions	O
with	O
mean	B
and	O
variance	B
both	O
zero	O
.	O
thus	O
those	O
parameters	O
,	O
and	O
the	O
corresponding	O
basis	O
func-	O
tions	O
φi	O
(	O
x	O
)	O
,	O
are	O
removed	O
from	O
the	O
model	O
and	O
play	O
no	O
role	O
in	O
making	O
predictions	O
for	O
new	O
inputs	O
.	O
in	O
the	O
case	O
of	O
models	O
of	O
the	O
form	O
(	O
7.78	O
)	O
,	O
the	O
inputs	O
xn	O
corresponding	O
to	O
the	O
remaining	O
nonzero	O
weights	O
are	O
called	O
relevance	O
vectors	O
,	O
because	O
they	O
are	O
iden-	O
tiﬁed	O
through	O
the	O
mechanism	O
of	O
automatic	B
relevance	I
determination	I
,	O
and	O
are	O
analo-	O
gous	O
to	O
the	O
support	O
vectors	O
of	O
an	O
svm	O
.	O
it	O
is	O
worth	O
emphasizing	O
,	O
however	O
,	O
that	O
this	O
mechanism	O
for	O
achieving	O
sparsity	B
in	O
probabilistic	O
models	O
through	O
automatic	O
rele-	O
vance	O
determination	O
is	O
quite	O
general	O
and	O
can	O
be	O
applied	O
to	O
any	O
model	O
expressed	O
as	O
an	O
adaptive	O
linear	O
combination	O
of	O
basis	O
functions	O
.	O
having	O
found	O
values	O
α	O
(	O
cid:1	O
)	O
and	O
β	O
(	O
cid:1	O
)	O
for	O
the	O
hyperparameters	O
that	O
maximize	O
the	O
marginal	B
likelihood	I
,	O
we	O
can	O
evaluate	O
the	O
predictive	B
distribution	I
over	O
t	O
for	O
a	O
new	O
input	O
x.	O
using	O
(	O
7.76	O
)	O
and	O
(	O
7.81	O
)	O
,	O
this	O
is	O
given	O
by	O
p	O
(	O
t|x	O
,	O
x	O
,	O
t	O
,	O
α	O
(	O
cid:1	O
)	O
,	O
β	O
(	O
cid:1	O
)	O
)	O
=	O
p	O
(	O
t|x	O
,	O
w	O
,	O
β	O
(	O
cid:1	O
)	O
)	O
p	O
(	O
w|x	O
,	O
t	O
,	O
α	O
(	O
cid:1	O
)	O
,	O
β	O
(	O
cid:1	O
)	O
)	O
dw	O
t|mtφ	O
(	O
x	O
)	O
,	O
σ2	O
(	O
x	O
)	O
.	O
(	O
cid:6	O
)	O
=	O
n	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
(	O
7.90	O
)	O
thus	O
the	O
predictive	O
mean	O
is	O
given	O
by	O
(	O
7.76	O
)	O
with	O
w	O
set	O
equal	O
to	O
the	O
posterior	O
mean	O
m	O
,	O
and	O
the	O
variance	B
of	O
the	O
predictive	B
distribution	I
is	O
given	O
by	O
σ2	O
(	O
x	O
)	O
=	O
(	O
β	O
(	O
cid:1	O
)	O
)	O
−1	O
+	O
φ	O
(	O
x	O
)	O
tσφ	O
(	O
x	O
)	O
(	O
7.91	O
)	O
where	O
σ	O
is	O
given	O
by	O
(	O
7.83	O
)	O
in	O
which	O
α	O
and	O
β	O
are	O
set	O
to	O
their	O
optimized	O
values	O
α	O
(	O
cid:1	O
)	O
and	O
β	O
(	O
cid:1	O
)	O
.	O
this	O
is	O
just	O
the	O
familiar	O
result	O
(	O
3.59	O
)	O
obtained	O
in	O
the	O
context	O
of	O
linear	B
regression	I
.	O
recall	O
that	O
for	O
localized	O
basis	O
functions	O
,	O
the	O
predictive	O
variance	O
for	O
linear	O
regression	B
models	O
becomes	O
small	O
in	O
regions	O
of	O
input	O
space	O
where	O
there	O
are	O
no	O
basis	O
functions	O
.	O
in	O
the	O
case	O
of	O
an	O
rvm	O
with	O
the	O
basis	O
functions	O
centred	O
on	O
data	O
points	O
,	O
the	O
model	O
will	O
therefore	O
become	O
increasingly	O
certain	O
of	O
its	O
predictions	O
when	O
extrapolating	O
outside	O
the	O
domain	O
of	O
the	O
data	O
(	O
rasmussen	O
and	O
qui˜nonero-candela	O
,	O
2005	O
)	O
,	O
which	O
of	O
course	O
is	O
undesirable	O
.	O
the	O
predictive	B
distribution	I
in	O
gaussian	O
process	O
regression	B
does	O
not	O
7.2.	O
relevance	B
vector	I
machines	O
349	O
figure	O
7.9	O
illustration	O
of	O
rvm	O
regression	B
us-	O
ing	O
the	O
same	O
data	O
set	O
,	O
and	O
the	O
same	O
gaussian	O
kernel	O
functions	O
,	O
as	O
used	O
in	O
figure	O
7.8	O
for	O
the	O
ν-svm	O
regression	B
model	O
.	O
the	O
mean	B
of	O
the	O
predictive	O
distribu-	O
tion	O
for	O
the	O
rvm	O
is	O
shown	O
by	O
the	O
red	O
line	O
,	O
and	O
the	O
one	O
standard-	O
deviation	O
predictive	B
distribution	I
is	O
shown	O
by	O
the	O
shaded	O
region	O
.	O
also	O
,	O
the	O
data	O
points	O
are	O
shown	O
in	O
green	O
,	O
and	O
the	O
relevance	O
vec-	O
tors	O
are	O
indicated	O
by	O
blue	O
circles	O
.	O
note	O
that	O
there	O
are	O
only	O
3	O
rele-	O
vance	O
vectors	O
compared	O
to	O
7	O
sup-	O
port	O
vectors	O
for	O
the	O
ν-svm	O
in	O
fig-	O
ure	O
7.8.	O
t	O
1	O
0	O
−1	O
0	O
x	O
1	O
suffer	O
from	O
this	O
problem	O
.	O
however	O
,	O
the	O
computational	O
cost	O
of	O
making	O
predictions	O
with	O
a	O
gaussian	O
processes	O
is	O
typically	O
much	O
higher	O
than	O
with	O
an	O
rvm	O
.	O
figure	O
7.9	O
shows	O
an	O
example	O
of	O
the	O
rvm	O
applied	O
to	O
the	O
sinusoidal	O
regression	O
data	O
set	O
.	O
here	O
the	O
noise	O
precision	B
parameter	I
β	O
is	O
also	O
determined	O
through	O
evidence	O
maximization	O
.	O
we	O
see	O
that	O
the	O
number	O
of	O
relevance	O
vectors	O
in	O
the	O
rvm	O
is	O
signif-	O
icantly	O
smaller	O
than	O
the	O
number	O
of	O
support	O
vectors	O
used	O
by	O
the	O
svm	O
.	O
for	O
a	O
wide	O
range	O
of	O
regression	B
and	O
classiﬁcation	B
tasks	O
,	O
the	O
rvm	O
is	O
found	O
to	O
give	O
models	O
that	O
are	O
typically	O
an	O
order	O
of	O
magnitude	O
more	O
compact	O
than	O
the	O
corresponding	O
support	B
vector	I
machine	I
,	O
resulting	O
in	O
a	O
signiﬁcant	O
improvement	O
in	O
the	O
speed	O
of	O
processing	O
on	O
test	O
data	O
.	O
remarkably	O
,	O
this	O
greater	O
sparsity	B
is	O
achieved	O
with	O
little	O
or	O
no	O
reduction	O
in	O
generalization	B
error	O
compared	O
with	O
the	O
corresponding	O
svm	O
.	O
the	O
principal	O
disadvantage	O
of	O
the	O
rvm	O
compared	O
to	O
the	O
svm	O
is	O
that	O
training	B
involves	O
optimizing	O
a	O
nonconvex	O
function	O
,	O
and	O
training	B
times	O
can	O
be	O
longer	O
than	O
for	O
a	O
comparable	O
svm	O
.	O
for	O
a	O
model	O
with	O
m	O
basis	O
functions	O
,	O
the	O
rvm	O
requires	O
inversion	O
of	O
a	O
matrix	O
of	O
size	O
m	O
×	O
m	O
,	O
which	O
in	O
general	O
requires	O
o	O
(	O
m	O
3	O
)	O
computation	O
.	O
in	O
the	O
speciﬁc	O
case	O
of	O
the	O
svm-like	O
model	O
(	O
7.78	O
)	O
,	O
we	O
have	O
m	O
=	O
n	O
+1	O
.	O
as	O
we	O
have	O
noted	O
,	O
there	O
are	O
techniques	O
for	O
training	O
svms	O
whose	O
cost	O
is	O
roughly	O
quadratic	O
in	O
n.	O
of	O
course	O
,	O
in	O
the	O
case	O
of	O
the	O
rvm	O
we	O
always	O
have	O
the	O
option	O
of	O
starting	O
with	O
a	O
smaller	O
number	O
of	O
basis	O
functions	O
than	O
n	O
+	O
1.	O
more	O
signiﬁcantly	O
,	O
in	O
the	O
relevance	B
vector	I
machine	I
the	O
parameters	O
governing	O
complexity	O
and	O
noise	O
variance	B
are	O
determined	O
automatically	O
from	O
a	O
single	O
training	B
run	O
,	O
whereas	O
in	O
the	O
support	B
vector	I
machine	I
the	O
parameters	O
c	O
and	O
	O
(	O
or	O
ν	O
)	O
are	O
generally	O
found	O
using	O
cross-validation	B
,	O
which	O
involves	O
multiple	O
training	B
runs	O
.	O
furthermore	O
,	O
in	O
the	O
next	O
section	O
we	O
shall	O
derive	O
an	O
alternative	O
procedure	O
for	O
training	O
the	O
relevance	B
vector	I
machine	I
that	O
improves	O
training	B
speed	O
signiﬁcantly	O
.	O
7.2.2	O
analysis	O
of	O
sparsity	B
we	O
have	O
noted	O
earlier	O
that	O
the	O
mechanism	O
of	O
automatic	B
relevance	I
determination	I
causes	O
a	O
subset	O
of	O
parameters	O
to	O
be	O
driven	O
to	O
zero	O
.	O
we	O
now	O
examine	O
in	O
more	O
detail	O
350	O
7.	O
sparse	O
kernel	O
machines	O
t2	O
c	O
t	O
t1	O
t2	O
ϕ	O
c	O
t	O
t1	O
figure	O
7.10	O
illustration	O
of	O
the	O
mechanism	O
for	O
sparsity	O
in	O
a	O
bayesian	O
linear	B
regression	I
model	O
,	O
showing	O
a	O
training	B
set	I
vector	O
of	O
target	O
values	O
given	O
by	O
t	O
=	O
(	O
t1	O
,	O
t2	O
)	O
t	O
,	O
indicated	O
by	O
the	O
cross	O
,	O
for	O
a	O
model	O
with	O
one	O
basis	O
vector	O
ϕ	O
=	O
(	O
φ	O
(	O
x1	O
)	O
,	O
φ	O
(	O
x2	O
)	O
)	O
t	O
,	O
which	O
is	O
poorly	O
aligned	O
with	O
the	O
target	O
data	O
vector	O
t.	O
on	O
the	O
left	O
we	O
see	O
a	O
model	O
having	O
only	O
isotropic	B
noise	O
,	O
so	O
that	O
c	O
=	O
β−1i	O
,	O
corresponding	O
to	O
α	O
=	O
∞	O
,	O
with	O
β	O
set	O
to	O
its	O
most	O
probable	O
value	O
.	O
on	O
the	O
right	O
we	O
see	O
the	O
same	O
model	O
but	O
with	O
a	O
ﬁnite	O
value	O
of	O
α.	O
in	O
each	O
case	O
the	O
red	O
ellipse	O
corresponds	O
to	O
unit	O
mahalanobis	O
distance	O
,	O
with	O
|c|	O
taking	O
the	O
same	O
value	O
for	O
both	O
plots	O
,	O
while	O
the	O
dashed	O
green	O
circle	O
shows	O
the	O
contrition	O
arising	O
from	O
the	O
noise	O
term	O
β−1	O
.	O
we	O
see	O
that	O
any	O
ﬁnite	O
value	O
of	O
α	O
reduces	O
the	O
probability	B
of	O
the	O
observed	O
data	O
,	O
and	O
so	O
for	O
the	O
most	O
probable	O
solution	O
the	O
basis	O
vector	O
is	O
removed	O
.	O
the	O
mechanism	O
of	O
sparsity	B
in	O
the	O
context	O
of	O
the	O
relevance	B
vector	I
machine	I
.	O
in	O
the	O
process	O
,	O
we	O
will	O
arrive	O
at	O
a	O
signiﬁcantly	O
faster	O
procedure	O
for	O
optimizing	O
the	O
hyper-	O
parameters	O
compared	O
to	O
the	O
direct	O
techniques	O
given	O
above	O
.	O
before	O
proceeding	O
with	O
a	O
mathematical	O
analysis	O
,	O
we	O
ﬁrst	O
give	O
some	O
informal	O
insight	O
into	O
the	O
origin	O
of	O
sparsity	B
in	O
bayesian	O
linear	O
models	O
.	O
consider	O
a	O
data	O
set	O
comprising	O
n	O
=	O
2	O
observations	O
t1	O
and	O
t2	O
,	O
together	O
with	O
a	O
model	O
having	O
a	O
single	O
basis	B
function	I
φ	O
(	O
x	O
)	O
,	O
with	O
hyperparameter	B
α	O
,	O
along	O
with	O
isotropic	B
noise	O
having	O
pre-	O
cision	O
β.	O
from	O
(	O
7.85	O
)	O
,	O
the	O
marginal	B
likelihood	I
is	O
given	O
by	O
p	O
(	O
t|α	O
,	O
β	O
)	O
=	O
n	O
(	O
t|0	O
,	O
c	O
)	O
in	O
which	O
the	O
covariance	B
matrix	I
takes	O
the	O
form	O
c	O
=	O
1	O
β	O
i	O
+	O
1	O
α	O
ϕϕt	O
(	O
7.92	O
)	O
where	O
ϕ	O
denotes	O
the	O
n-dimensional	O
vector	O
(	O
φ	O
(	O
x1	O
)	O
,	O
φ	O
(	O
x2	O
)	O
)	O
t	O
,	O
and	O
similarly	O
t	O
=	O
(	O
t1	O
,	O
t2	O
)	O
t.	O
notice	O
that	O
this	O
is	O
just	O
a	O
zero-mean	O
gaussian	O
process	O
model	O
over	O
t	O
with	O
covariance	B
c.	O
given	O
a	O
particular	O
observation	O
for	O
t	O
,	O
our	O
goal	O
is	O
to	O
ﬁnd	O
α	O
(	O
cid:1	O
)	O
and	O
β	O
(	O
cid:1	O
)	O
by	O
maximizing	O
the	O
marginal	B
likelihood	I
.	O
we	O
see	O
from	O
figure	O
7.10	O
that	O
,	O
if	O
there	O
is	O
a	O
poor	O
alignment	O
between	O
the	O
direction	O
of	O
ϕ	O
and	O
that	O
of	O
the	O
training	B
data	O
vector	O
t	O
,	O
then	O
the	O
corresponding	O
hyperparameter	B
α	O
will	O
be	O
driven	O
to	O
∞	O
,	O
and	O
the	O
basis	O
vector	O
will	O
be	O
pruned	O
from	O
the	O
model	O
.	O
this	O
arises	O
because	O
any	O
ﬁnite	O
value	O
for	O
α	O
will	O
always	O
assign	O
a	O
lower	O
probability	O
to	O
the	O
data	O
,	O
thereby	O
decreasing	O
the	O
value	O
of	O
the	O
density	B
at	O
t	O
,	O
pro-	O
vided	O
that	O
β	O
is	O
set	O
to	O
its	O
optimal	O
value	O
.	O
we	O
see	O
that	O
any	O
ﬁnite	O
value	O
for	O
α	O
would	O
cause	O
the	O
distribution	O
to	O
be	O
elongated	O
in	O
a	O
direction	O
away	O
from	O
the	O
data	O
,	O
thereby	O
increasing	O
the	O
probability	B
mass	O
in	O
regions	O
away	O
from	O
the	O
observed	O
data	O
and	O
hence	O
reducing	O
the	O
value	O
of	O
the	O
density	B
at	O
the	O
target	O
data	O
vector	O
itself	O
.	O
for	O
the	O
more	O
general	O
case	O
of	O
m	O
7.2.	O
relevance	B
vector	I
machines	O
351	O
basis	O
vectors	O
ϕ1	O
,	O
.	O
.	O
.	O
,	O
ϕm	O
a	O
similar	O
intuition	O
holds	O
,	O
namely	O
that	O
if	O
a	O
particular	O
basis	O
vector	O
is	O
poorly	O
aligned	O
with	O
the	O
data	O
vector	O
t	O
,	O
then	O
it	O
is	O
likely	O
to	O
be	O
pruned	O
from	O
the	O
model	O
.	O
we	O
now	O
investigate	O
the	O
mechanism	O
for	O
sparsity	O
from	O
a	O
more	O
mathematical	O
per-	O
spective	O
,	O
for	O
a	O
general	O
case	O
involving	O
m	O
basis	O
functions	O
.	O
to	O
motivate	O
this	O
analysis	O
we	O
ﬁrst	O
note	O
that	O
,	O
in	O
the	O
result	O
(	O
7.87	O
)	O
for	O
re-estimating	O
the	O
parameter	O
αi	O
,	O
the	O
terms	O
on	O
the	O
right-hand	O
side	O
are	O
themselves	O
also	O
functions	O
of	O
αi	O
.	O
these	O
results	O
therefore	O
rep-	O
resent	O
implicit	O
solutions	O
,	O
and	O
iteration	O
would	O
be	O
required	O
even	O
to	O
determine	O
a	O
single	O
αi	O
with	O
all	O
other	O
αj	O
for	O
j	O
(	O
cid:9	O
)	O
=	O
i	O
ﬁxed	O
.	O
this	O
suggests	O
a	O
different	O
approach	O
to	O
solving	O
the	O
optimization	O
problem	O
for	O
the	O
rvm	O
,	O
in	O
which	O
we	O
make	O
explicit	O
all	O
of	O
the	O
dependence	O
of	O
the	O
marginal	B
likelihood	I
(	O
7.85	O
)	O
on	O
a	O
particular	O
αi	O
and	O
then	O
determine	O
its	O
stationary	B
points	O
explicitly	O
(	O
faul	O
and	O
tipping	O
,	O
2002	O
;	O
tipping	O
and	O
faul	O
,	O
2003	O
)	O
.	O
to	O
do	O
this	O
,	O
we	O
ﬁrst	O
pull	O
out	O
the	O
contribution	O
from	O
αi	O
in	O
the	O
matrix	O
c	O
deﬁned	O
by	O
(	O
7.86	O
)	O
to	O
give	O
(	O
cid:2	O
)	O
c	O
=	O
β	O
−1i	O
+	O
−1	O
j	O
ϕjϕt	O
j	O
+	O
α	O
α	O
=	O
c−i	O
+	O
α	O
j	O
(	O
cid:9	O
)	O
=i	O
−1	O
i	O
ϕiϕt	O
i	O
−1	O
i	O
ϕiϕt	O
i	O
(	O
7.93	O
)	O
where	O
ϕi	O
denotes	O
the	O
ith	O
column	O
of	O
φ	O
,	O
in	O
other	O
words	O
the	O
n-dimensional	O
vector	O
with	O
elements	O
(	O
φi	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
φi	O
(	O
xn	O
)	O
)	O
,	O
in	O
contrast	O
to	O
φn	O
,	O
which	O
denotes	O
the	O
nth	O
row	O
of	O
φ.	O
the	O
matrix	O
c−i	O
represents	O
the	O
matrix	O
c	O
with	O
the	O
contribution	O
from	O
basis	B
function	I
i	O
removed	O
.	O
using	O
the	O
matrix	O
identities	O
(	O
c.7	O
)	O
and	O
(	O
c.15	O
)	O
,	O
the	O
determinant	O
and	O
inverse	B
of	O
c	O
can	O
then	O
be	O
written	O
|c|	O
=	O
|c−i||1	O
+	O
α	O
−1−i	O
−	O
c	O
c−1	O
=	O
c	O
−1−i	O
ϕi|	O
−1	O
i	O
ϕt	O
i	O
c	O
−1−i	O
ϕiϕt	O
−1−i	O
i	O
c	O
−1−i	O
ϕi	O
αi	O
+	O
ϕt	O
i	O
c	O
.	O
(	O
7.94	O
)	O
(	O
7.95	O
)	O
using	O
these	O
results	O
,	O
we	O
can	O
then	O
write	O
the	O
log	O
marginal	O
likelihood	B
function	I
(	O
7.85	O
)	O
in	O
the	O
form	O
(	O
7.96	O
)	O
where	O
l	O
(	O
α−i	O
)	O
is	O
simply	O
the	O
log	O
marginal	O
likelihood	O
with	O
basis	B
function	I
ϕi	O
omitted	O
,	O
and	O
the	O
quantity	O
λ	O
(	O
αi	O
)	O
is	O
deﬁned	O
by	O
l	O
(	O
α	O
)	O
=	O
l	O
(	O
α−i	O
)	O
+	O
λ	O
(	O
αi	O
)	O
(	O
cid:29	O
)	O
(	O
cid:30	O
)	O
λ	O
(	O
αi	O
)	O
=	O
1	O
2	O
ln	O
αi	O
−	O
ln	O
(	O
αi	O
+	O
si	O
)	O
+	O
q2	O
i	O
αi	O
+	O
si	O
(	O
7.97	O
)	O
and	O
contains	O
all	O
of	O
the	O
dependence	O
on	O
αi	O
.	O
here	O
we	O
have	O
introduced	O
the	O
two	O
quantities	O
si	O
=	O
ϕt	O
qi	O
=	O
ϕt	O
i	O
c	O
i	O
c	O
−1−i	O
ϕi	O
−1−i	O
t.	O
(	O
7.98	O
)	O
(	O
7.99	O
)	O
here	O
si	O
is	O
called	O
the	O
sparsity	B
and	O
qi	O
is	O
known	O
as	O
the	O
quality	O
of	O
ϕi	O
,	O
and	O
as	O
we	O
shall	O
see	O
,	O
a	O
large	O
value	O
of	O
si	O
relative	B
to	O
the	O
value	O
of	O
qi	O
means	O
that	O
the	O
basis	B
function	I
ϕi	O
exercise	O
7.15	O
352	O
7.	O
sparse	O
kernel	O
machines	O
of	O
the	O
figure	O
7.11	O
plots	O
log	O
likelihood	O
λ	O
(	O
αi	O
)	O
versus	O
marginal	B
ln	O
αi	O
showing	O
on	O
the	O
left	O
,	O
the	O
single	O
maximum	O
at	O
a	O
ﬁnite	O
αi	O
for	O
q2	O
i	O
=	O
4	O
and	O
si	O
=	O
1	O
(	O
so	O
that	O
q2	O
i	O
>	O
si	O
)	O
and	O
on	O
the	O
right	O
,	O
the	O
maximum	O
at	O
αi	O
=	O
∞	O
for	O
q2	O
i	O
=	O
1	O
and	O
si	O
=	O
2	O
(	O
so	O
that	O
q2	O
i	O
<	O
si	O
)	O
.	O
2	O
0	O
−2	O
−4	O
2	O
0	O
−2	O
−4	O
−5	O
0	O
5	O
−5	O
0	O
5	O
is	O
more	O
likely	O
to	O
be	O
pruned	O
from	O
the	O
model	O
.	O
the	O
‘	O
sparsity	B
’	O
measures	O
the	O
extent	O
to	O
which	O
basis	B
function	I
ϕi	O
overlaps	O
with	O
the	O
other	O
basis	O
vectors	O
in	O
the	O
model	O
,	O
and	O
the	O
‘	O
quality	O
’	O
represents	O
a	O
measure	O
of	O
the	O
alignment	O
of	O
the	O
basis	O
vector	O
ϕn	O
with	O
the	O
error	B
between	O
the	O
training	B
set	I
values	O
t	O
=	O
(	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
)	O
t	O
and	O
the	O
vector	O
y−i	O
of	O
predictions	O
that	O
would	O
result	O
from	O
the	O
model	O
with	O
the	O
vector	O
ϕi	O
excluded	O
(	O
tipping	O
and	O
faul	O
,	O
2003	O
)	O
.	O
the	O
stationary	B
points	O
of	O
the	O
marginal	B
likelihood	I
with	O
respect	O
to	O
αi	O
occur	O
when	O
the	O
derivative	B
dλ	O
(	O
αi	O
)	O
=	O
α	O
−1	O
i	O
s2	O
i	O
−	O
(	O
q2	O
2	O
(	O
αi	O
+	O
si	O
)	O
2	O
i	O
−	O
si	O
)	O
(	O
7.100	O
)	O
is	O
equal	O
to	O
zero	O
.	O
there	O
are	O
two	O
possible	O
forms	O
for	O
the	O
solution	O
.	O
recalling	O
that	O
αi	O
(	O
cid:2	O
)	O
0	O
,	O
we	O
see	O
that	O
if	O
q2	O
i	O
>	O
si	O
,	O
we	O
can	O
solve	O
for	O
αi	O
to	O
obtain	O
i	O
<	O
si	O
,	O
then	O
αi	O
→	O
∞	O
provides	O
a	O
solution	O
.	O
conversely	O
,	O
if	O
q2	O
dαi	O
αi	O
=	O
s2	O
i	O
i	O
−	O
si	O
q2	O
.	O
(	O
7.101	O
)	O
exercise	O
7.16	O
these	O
two	O
solutions	O
are	O
illustrated	O
in	O
figure	O
7.11.	O
we	O
see	O
that	O
the	O
relative	B
size	O
of	O
the	O
quality	O
and	O
sparsity	B
terms	O
determines	O
whether	O
a	O
particular	O
basis	O
vector	O
will	O
be	O
pruned	O
from	O
the	O
model	O
or	O
not	O
.	O
a	O
more	O
complete	O
analysis	O
(	O
faul	O
and	O
tipping	O
,	O
2002	O
)	O
,	O
based	O
on	O
the	O
second	O
derivatives	O
of	O
the	O
marginal	B
likelihood	I
,	O
conﬁrms	O
these	O
solutions	O
are	O
indeed	O
the	O
unique	O
maxima	O
of	O
λ	O
(	O
αi	O
)	O
.	O
note	O
that	O
this	O
approach	O
has	O
yielded	O
a	O
closed-form	O
solution	O
for	O
αi	O
,	O
for	O
given	O
values	O
of	O
the	O
other	O
hyperparameters	O
.	O
as	O
well	O
as	O
providing	O
insight	O
into	O
the	O
origin	O
of	O
sparsity	B
in	O
the	O
rvm	O
,	O
this	O
analysis	O
also	O
leads	O
to	O
a	O
practical	O
algorithm	O
for	O
optimizing	O
the	O
hyperparameters	O
that	O
has	O
signiﬁcant	O
speed	O
advantages	O
.	O
this	O
uses	O
a	O
ﬁxed	O
set	O
of	O
candidate	O
basis	O
vectors	O
,	O
and	O
then	O
cycles	O
through	O
them	O
in	O
turn	O
to	O
decide	O
whether	O
each	O
vector	O
should	O
be	O
included	O
in	O
the	O
model	O
or	O
not	O
.	O
the	O
resulting	O
sequential	O
sparse	O
bayesian	O
learning	B
algorithm	O
is	O
described	O
below	O
.	O
sequential	O
sparse	O
bayesian	O
learning	B
algorithm	O
1.	O
if	O
solving	O
a	O
regression	B
problem	O
,	O
initialize	O
β	O
.	O
2.	O
initialize	O
using	O
one	O
basis	B
function	I
ϕ1	O
,	O
with	O
hyperparameter	B
α1	O
set	O
using	O
(	O
7.101	O
)	O
,	O
with	O
the	O
remaining	O
hyperparameters	O
αj	O
for	O
j	O
(	O
cid:9	O
)	O
=	O
i	O
initialized	O
to	O
inﬁnity	O
,	O
so	O
that	O
only	O
ϕ1	O
is	O
included	O
in	O
the	O
model	O
.	O
7.2.	O
relevance	B
vector	I
machines	O
353	O
3.	O
evaluate	O
σ	O
and	O
m	O
,	O
along	O
with	O
qi	O
and	O
si	O
for	O
all	O
basis	O
functions	O
.	O
4.	O
select	O
a	O
candidate	O
basis	B
function	I
ϕi	O
.	O
5.	O
if	O
q2	O
i	O
>	O
si	O
,	O
and	O
αi	O
<	O
∞	O
,	O
so	O
that	O
the	O
basis	O
vector	O
ϕi	O
is	O
already	O
included	O
in	O
i	O
>	O
si	O
,	O
and	O
αi	O
=	O
∞	O
,	O
then	O
add	O
ϕi	O
to	O
the	O
model	O
,	O
and	O
evaluate	O
hyperpa-	O
i	O
(	O
cid:1	O
)	O
si	O
,	O
and	O
αi	O
<	O
∞	O
then	O
remove	O
basis	B
function	I
ϕi	O
from	O
the	O
model	O
,	O
the	O
model	O
,	O
then	O
update	O
αi	O
using	O
(	O
7.101	O
)	O
.	O
6.	O
if	O
q2	O
7.	O
if	O
q2	O
rameter	O
αi	O
using	O
(	O
7.101	O
)	O
.	O
and	O
set	O
αi	O
=	O
∞	O
.	O
8.	O
if	O
solving	O
a	O
regression	B
problem	O
,	O
update	O
β	O
.	O
9.	O
if	O
converged	O
terminate	O
,	O
otherwise	O
go	O
to	O
3.	O
note	O
that	O
if	O
q2	O
from	O
the	O
model	O
and	O
no	O
action	O
is	O
required	O
.	O
i	O
(	O
cid:1	O
)	O
si	O
and	O
αi	O
=	O
∞	O
,	O
then	O
the	O
basis	B
function	I
ϕi	O
is	O
already	O
excluded	O
in	O
practice	O
,	O
it	O
is	O
convenient	O
to	O
evaluate	O
the	O
quantities	O
qi	O
=	O
ϕt	O
si	O
=	O
ϕt	O
i	O
c−1t	O
i	O
c−1ϕi	O
.	O
the	O
quality	O
and	O
sparseness	O
variables	O
can	O
then	O
be	O
expressed	O
in	O
the	O
form	O
(	O
7.102	O
)	O
(	O
7.103	O
)	O
(	O
7.104	O
)	O
qi	O
=	O
αiqi	O
αi	O
−	O
si	O
αisi	O
αi	O
−	O
si	O
(	O
7.105	O
)	O
note	O
that	O
when	O
αi	O
=	O
∞	O
,	O
we	O
have	O
qi	O
=	O
qi	O
and	O
si	O
=	O
si	O
.	O
using	O
(	O
c.7	O
)	O
,	O
we	O
can	O
write	O
.	O
si	O
=	O
qi	O
=	O
βϕt	O
si	O
=	O
βϕt	O
i	O
t	O
−	O
β2ϕt	O
i	O
ϕi	O
−	O
β2ϕt	O
i	O
φσφtt	O
i	O
φσφtϕi	O
(	O
7.106	O
)	O
(	O
7.107	O
)	O
where	O
φ	O
and	O
σ	O
involve	O
only	O
those	O
basis	O
vectors	O
that	O
correspond	O
to	O
ﬁnite	O
hyperpa-	O
rameters	O
αi	O
.	O
at	O
each	O
stage	O
the	O
required	O
computations	O
therefore	O
scale	O
like	O
o	O
(	O
m	O
3	O
)	O
,	O
where	O
m	O
is	O
the	O
number	O
of	O
active	O
basis	O
vectors	O
in	O
the	O
model	O
and	O
is	O
typically	O
much	O
smaller	O
than	O
the	O
number	O
n	O
of	O
training	B
patterns	O
.	O
7.2.3	O
rvm	O
for	O
classiﬁcation	O
we	O
can	O
extend	O
the	O
relevance	B
vector	I
machine	I
framework	O
to	O
classiﬁcation	B
prob-	O
lems	O
by	O
applying	O
the	O
ard	O
prior	B
over	O
weights	O
to	O
a	O
probabilistic	O
linear	O
classiﬁcation	B
model	O
of	O
the	O
kind	O
studied	O
in	O
chapter	O
4.	O
to	O
start	O
with	O
,	O
we	O
consider	O
two-class	O
prob-	O
lems	O
with	O
a	O
binary	O
target	O
variable	O
t	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
the	O
model	O
now	O
takes	O
the	O
form	O
of	O
a	O
linear	O
combination	O
of	O
basis	O
functions	O
transformed	O
by	O
a	O
logistic	B
sigmoid	I
function	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
y	O
(	O
x	O
,	O
w	O
)	O
=	O
σ	O
wtφ	O
(	O
x	O
)	O
(	O
7.108	O
)	O
exercise	O
7.17	O
354	O
7.	O
sparse	O
kernel	O
machines	O
section	O
4.4	O
exercise	O
7.18	O
where	O
σ	O
(	O
·	O
)	O
is	O
the	O
logistic	B
sigmoid	I
function	O
deﬁned	O
by	O
(	O
4.59	O
)	O
.	O
if	O
we	O
introduce	O
a	O
gaussian	O
prior	B
over	O
the	O
weight	B
vector	I
w	O
,	O
then	O
we	O
obtain	O
the	O
model	O
that	O
has	O
been	O
considered	O
already	O
in	O
chapter	O
4.	O
the	O
difference	O
here	O
is	O
that	O
in	O
the	O
rvm	O
,	O
this	O
model	O
uses	O
the	O
ard	O
prior	B
(	O
7.80	O
)	O
in	O
which	O
there	O
is	O
a	O
separate	O
precision	O
hyperparameter	O
associated	O
with	O
each	O
weight	B
parameter	I
.	O
in	O
contrast	O
to	O
the	O
regression	B
model	O
,	O
we	O
can	O
no	O
longer	O
integrate	O
analytically	O
over	O
the	O
parameter	O
vector	O
w.	O
here	O
we	O
follow	O
tipping	O
(	O
2001	O
)	O
and	O
use	O
the	O
laplace	O
ap-	O
proximation	B
,	O
which	O
was	O
applied	O
to	O
the	O
closely	O
related	O
problem	O
of	O
bayesian	O
logistic	B
regression	I
in	O
section	O
4.5.1.	O
we	O
begin	O
by	O
initializing	O
the	O
hyperparameter	B
vector	O
α.	O
for	O
this	O
given	O
value	O
of	O
α	O
,	O
we	O
then	O
build	O
a	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
distribution	O
and	O
thereby	O
obtain	O
an	O
approximation	O
to	O
the	O
marginal	B
likelihood	I
.	O
maximization	O
of	O
this	O
approxi-	O
mate	O
marginal	B
likelihood	I
then	O
leads	O
to	O
a	O
re-estimated	O
value	O
for	O
α	O
,	O
and	O
the	O
process	O
is	O
repeated	O
until	O
convergence	O
.	O
let	O
us	O
consider	O
the	O
laplace	O
approximation	O
for	O
this	O
model	O
in	O
more	O
detail	O
.	O
for	O
a	O
ﬁxed	O
value	O
of	O
α	O
,	O
the	O
mode	O
of	O
the	O
posterior	O
distribution	O
over	O
w	O
is	O
obtained	O
by	O
maximizing	O
ln	O
p	O
(	O
w|t	O
,	O
α	O
)	O
=	O
ln	O
{	O
p	O
(	O
t|w	O
)	O
p	O
(	O
w|α	O
)	O
}	O
−	O
ln	O
p	O
(	O
t|α	O
)	O
{	O
tn	O
ln	O
yn	O
+	O
(	O
1	O
−	O
tn	O
)	O
ln	O
(	O
1	O
−	O
yn	O
)	O
}	O
−	O
1	O
2	O
=	O
n	O
(	O
cid:2	O
)	O
n=1	O
wtaw	O
+	O
const	O
(	O
7.109	O
)	O
where	O
a	O
=	O
diag	O
(	O
αi	O
)	O
.	O
this	O
can	O
be	O
done	O
using	O
iterative	B
reweighted	I
least	I
squares	I
(	O
irls	O
)	O
as	O
discussed	O
in	O
section	O
4.3.3.	O
for	O
this	O
,	O
we	O
need	O
the	O
gradient	O
vector	O
and	O
hessian	O
matrix	O
of	O
the	O
log	O
posterior	O
distribution	O
,	O
which	O
from	O
(	O
7.109	O
)	O
are	O
given	O
by	O
∇∇	O
ln	O
p	O
(	O
w|t	O
,	O
α	O
)	O
=	O
−	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
∇	O
ln	O
p	O
(	O
w|t	O
,	O
α	O
)	O
=	O
φt	O
(	O
t	O
−	O
y	O
)	O
−	O
aw	O
φtbφ	O
+	O
a	O
(	O
7.110	O
)	O
(	O
7.111	O
)	O
where	O
b	O
is	O
an	O
n	O
×	O
n	O
diagonal	B
matrix	O
with	O
elements	O
bn	O
=	O
yn	O
(	O
1	O
−	O
yn	O
)	O
,	O
the	O
vector	O
y	O
=	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
)	O
t	O
,	O
and	O
φ	O
is	O
the	O
design	B
matrix	I
with	O
elements	O
φni	O
=	O
φi	O
(	O
xn	O
)	O
.	O
here	O
we	O
have	O
used	O
the	O
property	O
(	O
4.88	O
)	O
for	O
the	O
derivative	B
of	O
the	O
logistic	B
sigmoid	I
function	O
.	O
at	O
convergence	O
of	O
the	O
irls	O
algorithm	O
,	O
the	O
negative	O
hessian	O
represents	O
the	O
inverse	B
covariance	O
matrix	O
for	O
the	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
distribution	O
.	O
the	O
mode	O
of	O
the	O
resulting	O
approximation	O
to	O
the	O
posterior	O
distribution	O
,	O
corre-	O
sponding	O
to	O
the	O
mean	B
of	O
the	O
gaussian	O
approximation	O
,	O
is	O
obtained	O
setting	O
(	O
7.110	O
)	O
to	O
zero	O
,	O
giving	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
laplace	O
approximation	O
in	O
the	O
form	O
(	O
cid:10	O
)	O
w	O
(	O
cid:1	O
)	O
=	O
a−1φt	O
(	O
t	O
−	O
y	O
)	O
φtbφ	O
+	O
a	O
σ	O
=	O
(	O
cid:11	O
)	O
−1	O
.	O
(	O
7.112	O
)	O
(	O
7.113	O
)	O
we	O
can	O
now	O
use	O
this	O
laplace	O
approximation	O
to	O
evaluate	O
the	O
marginal	B
likelihood	I
.	O
using	O
the	O
general	O
result	O
(	O
4.135	O
)	O
for	O
an	O
integral	O
evaluated	O
using	O
the	O
laplace	O
approxi-	O
7.2.	O
relevance	B
vector	I
machines	O
355	O
mation	B
,	O
we	O
have	O
(	O
cid:6	O
)	O
(	O
cid:7	O
)	O
p	O
(	O
t|w	O
(	O
cid:1	O
)	O
)	O
p	O
(	O
w	O
(	O
cid:1	O
)	O
|α	O
)	O
(	O
2π	O
)	O
m/2|σ|1/2	O
.	O
p	O
(	O
t|w	O
)	O
p	O
(	O
w|α	O
)	O
dw	O
p	O
(	O
t|α	O
)	O
=	O
(	O
7.114	O
)	O
if	O
we	O
substitute	O
for	O
p	O
(	O
t|w	O
(	O
cid:1	O
)	O
)	O
and	O
p	O
(	O
w	O
(	O
cid:1	O
)	O
|α	O
)	O
and	O
then	O
set	O
the	O
derivative	B
of	O
the	O
marginal	B
likelihood	I
with	O
respect	O
to	O
αi	O
equal	O
to	O
zero	O
,	O
we	O
obtain	O
exercise	O
7.19	O
−1	O
2	O
(	O
w	O
(	O
cid:1	O
)	O
i	O
)	O
2	O
+	O
1	O
2αi	O
−	O
1	O
2	O
σii	O
=	O
0	O
.	O
(	O
7.115	O
)	O
deﬁning	O
γi	O
=	O
1	O
−	O
αiσii	O
and	O
rearranging	O
then	O
gives	O
i	O
=	O
γi	O
αnew	O
(	O
w	O
(	O
cid:1	O
)	O
i	O
)	O
2	O
(	O
7.116	O
)	O
which	O
is	O
identical	O
to	O
the	O
re-estimation	O
formula	O
(	O
7.87	O
)	O
obtained	O
for	O
the	O
regression	B
rvm	O
.	O
if	O
we	O
deﬁne	O
we	O
can	O
write	O
the	O
approximate	O
log	O
marginal	O
likelihood	O
in	O
the	O
form	O
(	O
cid:1	O
)	O
t	O
=	O
φw	O
(	O
cid:1	O
)	O
+	O
b−1	O
(	O
t	O
−	O
y	O
)	O
(	O
cid:19	O
)	O
(	O
cid:20	O
)	O
n	O
ln	O
(	O
2π	O
)	O
+	O
ln|c|	O
+	O
(	O
(	O
cid:1	O
)	O
t	O
)	O
tc−1	O
(	O
cid:1	O
)	O
t	O
ln	O
p	O
(	O
t|α	O
,	O
β	O
)	O
=	O
−1	O
2	O
(	O
7.117	O
)	O
(	O
7.118	O
)	O
(	O
7.119	O
)	O
where	O
c	O
=	O
b	O
+	O
φaφt	O
.	O
appendix	O
a	O
section	O
13.3	O
this	O
takes	O
the	O
same	O
form	O
as	O
(	O
7.85	O
)	O
in	O
the	O
regression	B
case	O
,	O
and	O
so	O
we	O
can	O
apply	O
the	O
same	O
analysis	O
of	O
sparsity	B
and	O
obtain	O
the	O
same	O
fast	O
learning	O
algorithm	O
in	O
which	O
we	O
fully	O
optimize	O
a	O
single	O
hyperparameter	B
αi	O
at	O
each	O
step	O
.	O
figure	O
7.12	O
shows	O
the	O
relevance	B
vector	I
machine	I
applied	O
to	O
a	O
synthetic	O
classiﬁ-	O
cation	O
data	O
set	O
.	O
we	O
see	O
that	O
the	O
relevance	O
vectors	O
tend	O
not	O
to	O
lie	O
in	O
the	O
region	O
of	O
the	O
decision	B
boundary	I
,	O
in	O
contrast	O
to	O
the	O
support	B
vector	I
machine	I
.	O
this	O
is	O
consistent	B
with	O
our	O
earlier	O
discussion	O
of	O
sparsity	B
in	O
the	O
rvm	O
,	O
because	O
a	O
basis	B
function	I
φi	O
(	O
x	O
)	O
centred	O
on	O
a	O
data	O
point	O
near	O
the	O
boundary	O
will	O
have	O
a	O
vector	O
ϕi	O
that	O
is	O
poorly	O
aligned	O
with	O
the	O
training	B
data	O
vector	O
t.	O
one	O
of	O
the	O
potential	O
advantages	O
of	O
the	O
relevance	B
vector	I
machine	I
compared	O
with	O
the	O
svm	O
is	O
that	O
it	O
makes	O
probabilistic	O
predictions	O
.	O
for	O
example	O
,	O
this	O
allows	O
the	O
rvm	O
to	O
be	O
used	O
to	O
help	O
construct	O
an	O
emission	O
density	O
in	O
a	O
nonlinear	O
extension	O
of	O
the	O
linear	B
dynamical	I
system	I
for	O
tracking	O
faces	O
in	O
video	O
sequences	O
(	O
williams	O
et	O
al.	O
,	O
2005	O
)	O
.	O
so	O
far	O
,	O
we	O
have	O
considered	O
the	O
rvm	O
for	O
binary	O
classiﬁcation	B
problems	O
.	O
for	O
k	O
>	O
2	O
classes	O
,	O
we	O
again	O
make	O
use	O
of	O
the	O
probabilistic	O
approach	O
in	O
section	O
4.3.4	O
in	O
which	O
there	O
are	O
k	O
linear	O
models	O
of	O
the	O
form	O
ak	O
=	O
wt	O
k	O
x	O
(	O
7.120	O
)	O
356	O
7.	O
sparse	O
kernel	O
machines	O
2	O
0	O
−2	O
−2	O
0	O
2	O
figure	O
7.12	O
example	O
of	O
the	O
relevance	B
vector	I
machine	I
applied	O
to	O
a	O
synthetic	O
data	O
set	O
,	O
in	O
which	O
the	O
left-hand	O
plot	O
shows	O
the	O
decision	B
boundary	I
and	O
the	O
data	O
points	O
,	O
with	O
the	O
relevance	O
vectors	O
indicated	O
by	O
circles	O
.	O
comparison	O
with	O
the	O
results	O
shown	O
in	O
figure	O
7.4	O
for	O
the	O
corresponding	O
support	B
vector	I
machine	I
shows	O
that	O
the	O
rvm	O
gives	O
a	O
much	O
sparser	O
model	O
.	O
the	O
right-hand	O
plot	O
shows	O
the	O
posterior	B
probability	I
given	O
by	O
the	O
rvm	O
output	O
in	O
which	O
the	O
proportion	O
of	O
red	O
(	O
blue	O
)	O
ink	O
indicates	O
the	O
probability	B
of	O
that	O
point	O
belonging	O
to	O
the	O
red	O
(	O
blue	O
)	O
class	O
.	O
which	O
are	O
combined	O
using	O
a	O
softmax	B
function	I
to	O
give	O
outputs	O
exp	O
(	O
ak	O
)	O
(	O
cid:2	O
)	O
exp	O
(	O
aj	O
)	O
yk	O
(	O
x	O
)	O
=	O
.	O
(	O
7.121	O
)	O
the	O
log	O
likelihood	O
function	O
is	O
then	O
given	O
by	O
j	O
ln	O
p	O
(	O
t|w1	O
,	O
.	O
.	O
.	O
,	O
wk	O
)	O
=	O
n	O
(	O
cid:14	O
)	O
k	O
(	O
cid:14	O
)	O
n=1	O
k=1	O
ytnk	O
nk	O
(	O
7.122	O
)	O
where	O
the	O
target	O
values	O
tnk	O
have	O
a	O
1-of-k	O
coding	O
for	O
each	O
data	O
point	O
n	O
,	O
and	O
t	O
is	O
a	O
matrix	O
with	O
elements	O
tnk	O
.	O
again	O
,	O
the	O
laplace	O
approximation	O
can	O
be	O
used	O
to	O
optimize	O
the	O
hyperparameters	O
(	O
tipping	O
,	O
2001	O
)	O
,	O
in	O
which	O
the	O
model	O
and	O
its	O
hessian	O
are	O
found	O
using	O
irls	O
.	O
this	O
gives	O
a	O
more	O
principled	O
approach	O
to	O
multiclass	B
classiﬁcation	O
than	O
the	O
pairwise	O
method	O
used	O
in	O
the	O
support	B
vector	I
machine	I
and	O
also	O
provides	O
probabilis-	O
tic	O
predictions	O
for	O
new	O
data	O
points	O
.	O
the	O
principal	O
disadvantage	O
is	O
that	O
the	O
hessian	O
matrix	O
has	O
size	O
m	O
k×m	O
k	O
,	O
where	O
m	O
is	O
the	O
number	O
of	O
active	O
basis	O
functions	O
,	O
which	O
gives	O
an	O
additional	O
factor	O
of	O
k	O
3	O
in	O
the	O
computational	O
cost	O
of	O
training	B
compared	O
with	O
the	O
two-class	O
rvm	O
.	O
the	O
principal	O
disadvantage	O
of	O
the	O
relevance	B
vector	I
machine	I
is	O
the	O
relatively	O
long	O
training	B
times	O
compared	O
with	O
the	O
svm	O
.	O
this	O
is	O
offset	O
,	O
however	O
,	O
by	O
the	O
avoidance	O
of	O
cross-validation	B
runs	O
to	O
set	O
the	O
model	O
complexity	O
parameters	O
.	O
furthermore	O
,	O
because	O
it	O
yields	O
sparser	O
models	O
,	O
the	O
computation	O
time	O
on	O
test	O
points	O
,	O
which	O
is	O
usually	O
the	O
more	O
important	O
consideration	O
in	O
practice	O
,	O
is	O
typically	O
much	O
less	O
.	O
exercises	O
357	O
exercises	O
7.1	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
suppose	O
we	O
have	O
a	O
data	O
set	O
of	O
input	O
vectors	O
{	O
xn	O
}	O
with	O
corresponding	O
target	O
values	O
tn	O
∈	O
{	O
−1	O
,	O
1	O
}	O
,	O
and	O
suppose	O
that	O
we	O
model	O
the	O
density	B
of	O
input	O
vec-	O
tors	O
within	O
each	O
class	O
separately	O
using	O
a	O
parzen	O
kernel	B
density	I
estimator	I
(	O
see	O
sec-	O
tion	O
2.5.1	O
)	O
with	O
a	O
kernel	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
.	O
write	O
down	O
the	O
minimum	O
misclassiﬁcation-rate	O
decision	O
rule	O
assuming	O
the	O
two	O
classes	O
have	O
equal	O
prior	B
probability	O
.	O
show	O
also	O
that	O
,	O
if	O
the	O
kernel	O
is	O
chosen	O
to	O
be	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
xtx	O
(	O
cid:4	O
)	O
,	O
then	O
the	O
classiﬁcation	B
rule	O
reduces	O
to	O
simply	O
assigning	O
a	O
new	O
input	O
vector	O
to	O
the	O
class	O
having	O
the	O
closest	O
mean	B
.	O
finally	O
,	O
show	O
that	O
,	O
if	O
the	O
kernel	O
takes	O
the	O
form	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
φ	O
(	O
x	O
)	O
tφ	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
,	O
that	O
the	O
classiﬁcation	B
is	O
based	O
on	O
the	O
closest	O
mean	B
in	O
the	O
feature	B
space	I
φ	O
(	O
x	O
)	O
.	O
7.2	O
(	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
,	O
if	O
the	O
1	O
on	O
the	O
right-hand	O
side	O
of	O
the	O
constraint	O
(	O
7.5	O
)	O
is	O
replaced	O
by	O
some	O
arbitrary	O
constant	O
γ	O
>	O
0	O
,	O
the	O
solution	O
for	O
the	O
maximum	B
margin	I
hyperplane	O
is	O
unchanged	O
.	O
7.3	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
,	O
irrespective	O
of	O
the	O
dimensionality	O
of	O
the	O
data	O
space	O
,	O
a	O
data	O
set	O
consisting	O
of	O
just	O
two	O
data	O
points	O
,	O
one	O
from	O
each	O
class	O
,	O
is	O
sufﬁcient	O
to	O
determine	O
the	O
location	O
of	O
the	O
maximum-margin	O
hyperplane	O
.	O
7.4	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
the	O
value	O
ρ	O
of	O
the	O
margin	B
for	O
the	O
maximum-margin	O
hyper-	O
plane	O
is	O
given	O
by	O
n	O
(	O
cid:2	O
)	O
1	O
ρ2	O
=	O
(	O
7.123	O
)	O
where	O
{	O
an	O
}	O
are	O
given	O
by	O
maximizing	O
(	O
7.10	O
)	O
subject	O
to	O
the	O
constraints	O
(	O
7.11	O
)	O
and	O
(	O
7.12	O
)	O
.	O
an	O
n=1	O
7.5	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
the	O
values	O
of	O
ρ	O
and	O
{	O
an	O
}	O
in	O
the	O
previous	O
exercise	O
also	O
satisfy	O
ρ2	O
=	O
2	O
(	O
cid:4	O
)	O
l	O
(	O
a	O
)	O
where	O
(	O
cid:4	O
)	O
l	O
(	O
a	O
)	O
is	O
deﬁned	O
by	O
(	O
7.10	O
)	O
.	O
similarly	O
,	O
show	O
that	O
ρ2	O
=	O
(	O
cid:5	O
)	O
w	O
(	O
cid:5	O
)	O
2	O
.	O
1	O
1	O
(	O
7.125	O
)	O
7.6	O
(	O
(	O
cid:12	O
)	O
)	O
consider	O
the	O
logistic	B
regression	I
model	O
with	O
a	O
target	O
variable	O
t	O
∈	O
{	O
−1	O
,	O
1	O
}	O
.	O
if	O
we	O
deﬁne	O
p	O
(	O
t	O
=	O
1|y	O
)	O
=	O
σ	O
(	O
y	O
)	O
where	O
y	O
(	O
x	O
)	O
is	O
given	O
by	O
(	O
7.1	O
)	O
,	O
show	O
that	O
the	O
negative	O
log	O
likelihood	O
,	O
with	O
the	O
addition	O
of	O
a	O
quadratic	O
regularization	O
term	O
,	O
takes	O
the	O
form	O
(	O
7.47	O
)	O
.	O
7.7	O
(	O
(	O
cid:12	O
)	O
)	O
consider	O
the	O
lagrangian	O
(	O
7.56	O
)	O
for	O
the	O
regression	B
support	O
vector	O
machine	O
.	O
by	O
setting	O
the	O
derivatives	O
of	O
the	O
lagrangian	O
with	O
respect	O
to	O
w	O
,	O
b	O
,	O
ξn	O
,	O
and	O
(	O
cid:1	O
)	O
ξn	O
to	O
zero	O
and	O
then	O
back	O
substituting	O
to	O
eliminate	O
the	O
corresponding	O
variables	O
,	O
show	O
that	O
the	O
dual	O
lagrangian	O
is	O
given	O
by	O
(	O
7.61	O
)	O
.	O
(	O
7.124	O
)	O
358	O
7.	O
sparse	O
kernel	O
machines	O
7.8	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
for	O
the	O
regression	B
support	O
vector	O
machine	O
considered	O
in	O
section	O
7.1.4	O
,	O
show	O
that	O
all	O
training	B
data	O
points	O
for	O
which	O
ξn	O
>	O
0	O
will	O
have	O
an	O
=	O
c	O
,	O
and	O
similarly	O
all	O
points	O
for	O
which	O
(	O
cid:1	O
)	O
ξn	O
>	O
0	O
will	O
have	O
(	O
cid:1	O
)	O
an	O
=	O
c.	O
7.9	O
(	O
(	O
cid:12	O
)	O
)	O
verify	O
the	O
results	O
(	O
7.82	O
)	O
and	O
(	O
7.83	O
)	O
for	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
posterior	O
distribution	O
over	O
weights	O
in	O
the	O
regression	B
rvm	O
.	O
7.10	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
derive	O
the	O
result	O
(	O
7.85	O
)	O
for	O
the	O
marginal	B
likelihood	I
function	O
in	O
the	O
regression	B
rvm	O
,	O
by	O
performing	O
the	O
gaussian	O
integral	O
over	O
w	O
in	O
(	O
7.84	O
)	O
using	O
the	O
technique	O
of	O
completing	B
the	I
square	I
in	O
the	O
exponential	O
.	O
7.11	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
repeat	O
the	O
above	O
exercise	O
,	O
but	O
this	O
time	O
make	O
use	O
of	O
the	O
general	O
result	O
(	O
2.115	O
)	O
.	O
7.12	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
direct	O
maximization	O
of	O
the	O
log	O
marginal	O
likelihood	O
(	O
7.85	O
)	O
for	O
the	O
regression	B
relevance	O
vector	O
machine	O
leads	O
to	O
the	O
re-estimation	O
equations	O
(	O
7.87	O
)	O
and	O
(	O
7.88	O
)	O
where	O
γi	O
is	O
deﬁned	O
by	O
(	O
7.89	O
)	O
.	O
7.13	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
in	O
the	O
evidence	O
framework	O
for	O
rvm	O
regression	B
,	O
we	O
obtained	O
the	O
re-estimation	O
formulae	O
(	O
7.87	O
)	O
and	O
(	O
7.88	O
)	O
by	O
maximizing	O
the	O
marginal	B
likelihood	I
given	O
by	O
(	O
7.85	O
)	O
.	O
extend	O
this	O
approach	O
by	O
inclusion	O
of	O
hyperpriors	O
given	O
by	O
gamma	O
distributions	O
of	O
the	O
form	O
(	O
b.26	O
)	O
and	O
obtain	O
the	O
corresponding	O
re-estimation	O
formulae	O
for	O
α	O
and	O
β	O
by	O
maximizing	O
the	O
corresponding	O
posterior	B
probability	I
p	O
(	O
t	O
,	O
α	O
,	O
β|x	O
)	O
with	O
respect	O
to	O
α	O
and	O
β	O
.	O
7.14	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
derive	O
the	O
result	O
(	O
7.90	O
)	O
for	O
the	O
predictive	B
distribution	I
in	O
the	O
relevance	B
vector	I
machine	I
for	O
regression	B
.	O
show	O
that	O
the	O
predictive	O
variance	O
is	O
given	O
by	O
(	O
7.91	O
)	O
.	O
7.15	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
using	O
the	O
results	O
(	O
7.94	O
)	O
and	O
(	O
7.95	O
)	O
,	O
show	O
that	O
the	O
marginal	B
likelihood	I
(	O
7.85	O
)	O
can	O
be	O
written	O
in	O
the	O
form	O
(	O
7.96	O
)	O
,	O
where	O
λ	O
(	O
αn	O
)	O
is	O
deﬁned	O
by	O
(	O
7.97	O
)	O
and	O
the	O
sparsity	B
and	O
quality	O
factors	O
are	O
deﬁned	O
by	O
(	O
7.98	O
)	O
and	O
(	O
7.99	O
)	O
,	O
respectively	O
.	O
7.16	O
(	O
(	O
cid:12	O
)	O
)	O
by	O
taking	O
the	O
second	O
derivative	O
of	O
the	O
log	O
marginal	O
likelihood	O
(	O
7.97	O
)	O
for	O
the	O
regression	B
rvm	O
with	O
respect	O
to	O
the	O
hyperparameter	B
αi	O
,	O
show	O
that	O
the	O
stationary	B
point	O
given	O
by	O
(	O
7.101	O
)	O
is	O
a	O
maximum	O
of	O
the	O
marginal	B
likelihood	I
.	O
7.17	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
using	O
(	O
7.83	O
)	O
and	O
(	O
7.86	O
)	O
,	O
together	O
with	O
the	O
matrix	O
identity	O
(	O
c.7	O
)	O
,	O
show	O
that	O
the	O
quantities	O
sn	O
and	O
qn	O
deﬁned	O
by	O
(	O
7.102	O
)	O
and	O
(	O
7.103	O
)	O
can	O
be	O
written	O
in	O
the	O
form	O
(	O
7.106	O
)	O
and	O
(	O
7.107	O
)	O
.	O
7.18	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
the	O
gradient	O
vector	O
and	O
hessian	O
matrix	O
of	O
the	O
log	O
poste-	O
rior	O
distribution	O
(	O
7.109	O
)	O
for	O
the	O
classiﬁcation	B
relevance	O
vector	O
machine	O
are	O
given	O
by	O
(	O
7.110	O
)	O
and	O
(	O
7.111	O
)	O
.	O
7.19	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
verify	O
that	O
maximization	O
of	O
the	O
approximate	O
log	O
marginal	O
likelihood	B
function	I
(	O
7.114	O
)	O
for	O
the	O
classiﬁcation	B
relevance	O
vector	O
machine	O
leads	O
to	O
the	O
result	O
(	O
7.116	O
)	O
for	O
re-estimation	O
of	O
the	O
hyperparameters	O
.	O
8	O
graphical	O
models	O
probabilities	O
play	O
a	O
central	O
role	O
in	O
modern	O
pattern	O
recognition	O
.	O
we	O
have	O
seen	O
in	O
chapter	O
1	O
that	O
probability	B
theory	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
two	O
simple	O
equations	O
corresponding	O
to	O
the	O
sum	B
rule	I
and	O
the	O
product	B
rule	I
.	O
all	O
of	O
the	O
probabilistic	O
infer-	O
ence	O
and	O
learning	B
manipulations	O
discussed	O
in	O
this	O
book	O
,	O
no	O
matter	O
how	O
complex	O
,	O
amount	O
to	O
repeated	O
application	O
of	O
these	O
two	O
equations	O
.	O
we	O
could	O
therefore	O
proceed	O
to	O
formulate	O
and	O
solve	O
complicated	O
probabilistic	O
models	O
purely	O
by	O
algebraic	O
ma-	O
nipulation	O
.	O
however	O
,	O
we	O
shall	O
ﬁnd	O
it	O
highly	O
advantageous	O
to	O
augment	O
the	O
analysis	O
using	O
diagrammatic	O
representations	O
of	O
probability	B
distributions	O
,	O
called	O
probabilistic	O
graphical	O
models	O
.	O
these	O
offer	O
several	O
useful	O
properties	O
:	O
1.	O
they	O
provide	O
a	O
simple	O
way	O
to	O
visualize	O
the	O
structure	O
of	O
a	O
probabilistic	O
model	O
and	O
can	O
be	O
used	O
to	O
design	O
and	O
motivate	O
new	O
models	O
.	O
2.	O
insights	O
into	O
the	O
properties	O
of	O
the	O
model	O
,	O
including	O
conditional	B
independence	I
properties	O
,	O
can	O
be	O
obtained	O
by	O
inspection	O
of	O
the	O
graph	O
.	O
359	O
360	O
8.	O
graphical	O
models	O
3.	O
complex	O
computations	O
,	O
required	O
to	O
perform	O
inference	B
and	O
learning	B
in	O
sophis-	O
ticated	O
models	O
,	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
graphical	O
manipulations	O
,	O
in	O
which	O
underlying	O
mathematical	O
expressions	O
are	O
carried	O
along	O
implicitly	O
.	O
a	O
graph	O
comprises	O
nodes	O
(	O
also	O
called	O
vertices	O
)	O
connected	O
by	O
links	O
(	O
also	O
known	O
as	O
edges	O
or	O
arcs	O
)	O
.	O
in	O
a	O
probabilistic	B
graphical	I
model	I
,	O
each	O
node	B
represents	O
a	O
random	O
variable	O
(	O
or	O
group	O
of	O
random	O
variables	O
)	O
,	O
and	O
the	O
links	O
express	O
probabilistic	O
relation-	O
ships	O
between	O
these	O
variables	O
.	O
the	O
graph	O
then	O
captures	O
the	O
way	O
in	O
which	O
the	O
joint	O
distribution	O
over	O
all	O
of	O
the	O
random	O
variables	O
can	O
be	O
decomposed	O
into	O
a	O
product	O
of	O
factors	O
each	O
depending	O
only	O
on	O
a	O
subset	O
of	O
the	O
variables	O
.	O
we	O
shall	O
begin	O
by	O
dis-	O
cussing	O
bayesian	O
networks	O
,	O
also	O
known	O
as	O
directed	B
graphical	O
models	O
,	O
in	O
which	O
the	O
links	O
of	O
the	O
graphs	O
have	O
a	O
particular	O
directionality	O
indicated	O
by	O
arrows	O
.	O
the	O
other	O
major	O
class	O
of	O
graphical	O
models	O
are	O
markov	O
random	O
ﬁelds	O
,	O
also	O
known	O
as	O
undirected	B
graphical	O
models	O
,	O
in	O
which	O
the	O
links	O
do	O
not	O
carry	O
arrows	O
and	O
have	O
no	O
directional	O
signiﬁcance	O
.	O
directed	B
graphs	O
are	O
useful	O
for	O
expressing	O
causal	O
relationships	O
between	O
random	O
variables	O
,	O
whereas	O
undirected	B
graphs	O
are	O
better	O
suited	O
to	O
expressing	O
soft	B
con-	O
straints	O
between	O
random	O
variables	O
.	O
for	O
the	O
purposes	O
of	O
solving	O
inference	B
problems	O
,	O
it	O
is	O
often	O
convenient	O
to	O
convert	O
both	O
directed	B
and	O
undirected	B
graphs	O
into	O
a	O
different	O
representation	O
called	O
a	O
factor	B
graph	I
.	O
in	O
this	O
chapter	O
,	O
we	O
shall	O
focus	O
on	O
the	O
key	O
aspects	O
of	O
graphical	O
models	O
as	O
needed	O
for	O
applications	O
in	O
pattern	O
recognition	O
and	O
machine	O
learning	O
.	O
more	O
general	O
treat-	O
ments	O
of	O
graphical	O
models	O
can	O
be	O
found	O
in	O
the	O
books	O
by	O
whittaker	O
(	O
1990	O
)	O
,	O
lauritzen	O
(	O
1996	O
)	O
,	O
jensen	O
(	O
1996	O
)	O
,	O
castillo	O
et	O
al	O
.	O
(	O
1997	O
)	O
,	O
jordan	O
(	O
1999	O
)	O
,	O
cowell	O
et	O
al	O
.	O
(	O
1999	O
)	O
,	O
and	O
jordan	O
(	O
2007	O
)	O
.	O
8.1.	O
bayesian	O
networks	O
in	O
order	O
to	O
motivate	O
the	O
use	O
of	O
directed	B
graphs	O
to	O
describe	O
probability	B
distributions	O
,	O
consider	O
ﬁrst	O
an	O
arbitrary	O
joint	O
distribution	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
over	O
three	O
variables	O
a	O
,	O
b	O
,	O
and	O
c.	O
note	O
that	O
at	O
this	O
stage	O
,	O
we	O
do	O
not	O
need	O
to	O
specify	O
anything	O
further	O
about	O
these	O
vari-	O
ables	O
,	O
such	O
as	O
whether	O
they	O
are	O
discrete	O
or	O
continuous	O
.	O
indeed	O
,	O
one	O
of	O
the	O
powerful	O
aspects	O
of	O
graphical	O
models	O
is	O
that	O
a	O
speciﬁc	O
graph	O
can	O
make	O
probabilistic	O
statements	O
for	O
a	O
broad	O
class	O
of	O
distributions	O
.	O
by	O
application	O
of	O
the	O
product	B
rule	I
of	I
probability	I
(	O
1.11	O
)	O
,	O
we	O
can	O
write	O
the	O
joint	O
distribution	O
in	O
the	O
form	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
p	O
(	O
c|a	O
,	O
b	O
)	O
p	O
(	O
a	O
,	O
b	O
)	O
.	O
(	O
8.1	O
)	O
a	O
second	O
application	O
of	O
the	O
product	B
rule	I
,	O
this	O
time	O
to	O
the	O
second	O
term	O
on	O
the	O
right-	O
hand	O
side	O
of	O
(	O
8.1	O
)	O
,	O
gives	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
p	O
(	O
c|a	O
,	O
b	O
)	O
p	O
(	O
b|a	O
)	O
p	O
(	O
a	O
)	O
.	O
(	O
8.2	O
)	O
note	O
that	O
this	O
decomposition	O
holds	O
for	O
any	O
choice	O
of	O
the	O
joint	O
distribution	O
.	O
we	O
now	O
represent	O
the	O
right-hand	O
side	O
of	O
(	O
8.2	O
)	O
in	O
terms	O
of	O
a	O
simple	O
graphical	B
model	I
as	O
follows	O
.	O
first	O
we	O
introduce	O
a	O
node	B
for	O
each	O
of	O
the	O
random	O
variables	O
a	O
,	O
b	O
,	O
and	O
c	O
and	O
associate	O
each	O
node	B
with	O
the	O
corresponding	O
conditional	B
distribution	O
on	O
the	O
right-hand	O
side	O
of	O
figure	O
8.1	O
a	O
directed	B
graphical	O
model	O
representing	O
the	O
joint	O
probabil-	O
ity	O
distribution	O
over	O
three	O
variables	O
a	O
,	O
b	O
,	O
and	O
c	O
,	O
correspond-	O
ing	O
to	O
the	O
decomposition	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
8.2	O
)	O
.	O
a	O
b	O
8.1.	O
bayesian	O
networks	O
361	O
c	O
(	O
8.2	O
)	O
.	O
then	O
,	O
for	O
each	O
conditional	B
distribution	O
we	O
add	O
directed	B
links	O
(	O
arrows	O
)	O
to	O
the	O
graph	O
from	O
the	O
nodes	O
corresponding	O
to	O
the	O
variables	O
on	O
which	O
the	O
distribution	O
is	O
conditioned	O
.	O
thus	O
for	O
the	O
factor	O
p	O
(	O
c|a	O
,	O
b	O
)	O
,	O
there	O
will	O
be	O
links	O
from	O
nodes	O
a	O
and	O
b	O
to	O
node	B
c	O
,	O
whereas	O
for	O
the	O
factor	O
p	O
(	O
a	O
)	O
there	O
will	O
be	O
no	O
incoming	O
links	O
.	O
the	O
result	O
is	O
the	O
graph	O
shown	O
in	O
figure	O
8.1.	O
if	O
there	O
is	O
a	O
link	B
going	O
from	O
a	O
node	B
a	O
to	O
a	O
node	B
b	O
,	O
then	O
we	O
say	O
that	O
node	B
a	O
is	O
the	O
parent	O
of	O
node	B
b	O
,	O
and	O
we	O
say	O
that	O
node	B
b	O
is	O
the	O
child	O
of	O
node	B
a.	O
note	O
that	O
we	O
shall	O
not	O
make	O
any	O
formal	O
distinction	O
between	O
a	O
node	B
and	O
the	O
variable	O
to	O
which	O
it	O
corresponds	O
but	O
will	O
simply	O
use	O
the	O
same	O
symbol	O
to	O
refer	O
to	O
both	O
.	O
an	O
interesting	O
point	O
to	O
note	O
about	O
(	O
8.2	O
)	O
is	O
that	O
the	O
left-hand	O
side	O
is	O
symmetrical	O
with	O
respect	O
to	O
the	O
three	O
variables	O
a	O
,	O
b	O
,	O
and	O
c	O
,	O
whereas	O
the	O
right-hand	O
side	O
is	O
not	O
.	O
indeed	O
,	O
in	O
making	O
the	O
decomposition	O
in	O
(	O
8.2	O
)	O
,	O
we	O
have	O
implicitly	O
chosen	O
a	O
particular	O
ordering	O
,	O
namely	O
a	O
,	O
b	O
,	O
c	O
,	O
and	O
had	O
we	O
chosen	O
a	O
different	O
ordering	O
we	O
would	O
have	O
obtained	O
a	O
different	O
decomposition	O
and	O
hence	O
a	O
different	O
graphical	O
representation	O
.	O
we	O
shall	O
return	O
to	O
this	O
point	O
later	O
.	O
for	O
the	O
moment	O
let	O
us	O
extend	O
the	O
example	O
of	O
figure	O
8.1	O
by	O
considering	O
the	O
joint	O
distribution	O
over	O
k	O
variables	O
given	O
by	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xk	O
)	O
.	O
by	O
repeated	O
application	O
of	O
the	O
product	B
rule	I
of	I
probability	I
,	O
this	O
joint	O
distribution	O
can	O
be	O
written	O
as	O
a	O
product	O
of	O
conditional	B
distributions	O
,	O
one	O
for	O
each	O
of	O
the	O
variables	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xk	O
)	O
=	O
p	O
(	O
xk|x1	O
,	O
.	O
.	O
.	O
,	O
xk−1	O
)	O
.	O
.	O
.	O
p	O
(	O
x2|x1	O
)	O
p	O
(	O
x1	O
)	O
.	O
(	O
8.3	O
)	O
for	O
a	O
given	O
choice	O
of	O
k	O
,	O
we	O
can	O
again	O
represent	O
this	O
as	O
a	O
directed	B
graph	O
having	O
k	O
nodes	O
,	O
one	O
for	O
each	O
conditional	B
distribution	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
8.3	O
)	O
,	O
with	O
each	O
node	B
having	O
incoming	O
links	O
from	O
all	O
lower	O
numbered	O
nodes	O
.	O
we	O
say	O
that	O
this	O
graph	O
is	O
fully	B
connected	I
because	O
there	O
is	O
a	O
link	B
between	O
every	O
pair	O
of	O
nodes	O
.	O
so	O
far	O
,	O
we	O
have	O
worked	O
with	O
completely	O
general	O
joint	O
distributions	O
,	O
so	O
that	O
the	O
decompositions	O
,	O
and	O
their	O
representations	O
as	O
fully	B
connected	I
graphs	O
,	O
will	O
be	O
applica-	O
ble	O
to	O
any	O
choice	O
of	O
distribution	O
.	O
as	O
we	O
shall	O
see	O
shortly	O
,	O
it	O
is	O
the	O
absence	O
of	O
links	O
in	O
the	O
graph	O
that	O
conveys	O
interesting	O
information	O
about	O
the	O
properties	O
of	O
the	O
class	O
of	O
distributions	O
that	O
the	O
graph	O
represents	O
.	O
consider	O
the	O
graph	O
shown	O
in	O
figure	O
8.2.	O
this	O
is	O
not	O
a	O
fully	B
connected	I
graph	O
because	O
,	O
for	O
instance	O
,	O
there	O
is	O
no	O
link	B
from	O
x1	O
to	O
x2	O
or	O
from	O
x3	O
to	O
x7	O
.	O
we	O
shall	O
now	O
go	O
from	O
this	O
graph	O
to	O
the	O
corresponding	O
representation	O
of	O
the	O
joint	O
probability	B
distribution	O
written	O
in	O
terms	O
of	O
the	O
product	O
of	O
a	O
set	O
of	O
conditional	B
dis-	O
tributions	O
,	O
one	O
for	O
each	O
node	B
in	O
the	O
graph	O
.	O
each	O
such	O
conditional	B
distribution	O
will	O
be	O
conditioned	O
only	O
on	O
the	O
parents	O
of	O
the	O
corresponding	O
node	B
in	O
the	O
graph	O
.	O
for	O
in-	O
stance	O
,	O
x5	O
will	O
be	O
conditioned	O
on	O
x1	O
and	O
x3	O
.	O
the	O
joint	O
distribution	O
of	O
all	O
7	O
variables	O
362	O
8.	O
graphical	O
models	O
figure	O
8.2	O
example	O
of	O
a	O
directed	B
acyclic	I
graph	I
describing	O
the	O
joint	O
distribution	O
over	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
x7	O
.	O
the	O
corresponding	O
decomposition	O
of	O
the	O
joint	O
distribution	O
is	O
given	O
by	O
(	O
8.4	O
)	O
.	O
x1	O
x2	O
x4	O
x3	O
x5	O
x6	O
x7	O
is	O
therefore	O
given	O
by	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2	O
)	O
p	O
(	O
x3	O
)	O
p	O
(	O
x4|x1	O
,	O
x2	O
,	O
x3	O
)	O
p	O
(	O
x5|x1	O
,	O
x3	O
)	O
p	O
(	O
x6|x4	O
)	O
p	O
(	O
x7|x4	O
,	O
x5	O
)	O
.	O
(	O
8.4	O
)	O
the	O
reader	O
should	O
take	O
a	O
moment	O
to	O
study	O
carefully	O
the	O
correspondence	O
between	O
(	O
8.4	O
)	O
and	O
figure	O
8.2.	O
k	O
(	O
cid:14	O
)	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
xk|pak	O
)	O
we	O
can	O
now	O
state	O
in	O
general	O
terms	O
the	O
relationship	O
between	O
a	O
given	O
directed	B
graph	O
and	O
the	O
corresponding	O
distribution	O
over	O
the	O
variables	O
.	O
the	O
joint	O
distribution	O
deﬁned	O
by	O
a	O
graph	O
is	O
given	O
by	O
the	O
product	O
,	O
over	O
all	O
of	O
the	O
nodes	O
of	O
the	O
graph	O
,	O
of	O
a	O
conditional	B
distribution	O
for	O
each	O
node	B
conditioned	O
on	O
the	O
variables	O
corresponding	O
to	O
the	O
parents	O
of	O
that	O
node	B
in	O
the	O
graph	O
.	O
thus	O
,	O
for	O
a	O
graph	O
with	O
k	O
nodes	O
,	O
the	O
joint	O
distribution	O
is	O
given	O
by	O
k=1	O
(	O
8.5	O
)	O
where	O
pak	O
denotes	O
the	O
set	O
of	O
parents	O
of	O
xk	O
,	O
and	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xk	O
}	O
.	O
this	O
key	O
equation	O
expresses	O
the	O
factorization	B
properties	O
of	O
the	O
joint	O
distribution	O
for	O
a	O
directed	B
graphical	O
model	O
.	O
although	O
we	O
have	O
considered	O
each	O
node	B
to	O
correspond	O
to	O
a	O
single	O
variable	O
,	O
we	O
can	O
equally	O
well	O
associate	O
sets	O
of	O
variables	O
and	O
vector-valued	O
variables	O
with	O
the	O
nodes	O
of	O
a	O
graph	O
.	O
it	O
is	O
easy	O
to	O
show	O
that	O
the	O
representation	O
on	O
the	O
right-	O
hand	O
side	O
of	O
(	O
8.5	O
)	O
is	O
always	O
correctly	O
normalized	O
provided	O
the	O
individual	O
conditional	B
distributions	O
are	O
normalized	O
.	O
the	O
directed	B
graphs	O
that	O
we	O
are	O
considering	O
are	O
subject	O
to	O
an	O
important	O
restric-	O
tion	O
namely	O
that	O
there	O
must	O
be	O
no	O
directed	B
cycles	O
,	O
in	O
other	O
words	O
there	O
are	O
no	O
closed	O
paths	O
within	O
the	O
graph	O
such	O
that	O
we	O
can	O
move	O
from	O
node	B
to	O
node	B
along	O
links	O
follow-	O
ing	O
the	O
direction	O
of	O
the	O
arrows	O
and	O
end	O
up	O
back	O
at	O
the	O
starting	O
node	B
.	O
such	O
graphs	O
are	O
also	O
called	O
directed	O
acyclic	O
graphs	O
,	O
or	O
dags	O
.	O
this	O
is	O
equivalent	O
to	O
the	O
statement	O
that	O
there	O
exists	O
an	O
ordering	O
of	O
the	O
nodes	O
such	O
that	O
there	O
are	O
no	O
links	O
that	O
go	O
from	O
any	O
node	B
to	O
any	O
lower	O
numbered	O
node	B
.	O
8.1.1	O
example	O
:	O
polynomial	O
regression	O
as	O
an	O
illustration	O
of	O
the	O
use	O
of	O
directed	B
graphs	O
to	O
describe	O
probability	B
distri-	O
butions	O
,	O
we	O
consider	O
the	O
bayesian	O
polynomial	O
regression	O
model	O
introduced	O
in	O
sec-	O
exercise	O
8.1	O
exercise	O
8.2	O
8.1.	O
bayesian	O
networks	O
363	O
figure	O
8.3	O
directed	B
graphical	O
model	O
representing	O
the	O
joint	O
distribution	O
(	O
8.6	O
)	O
corresponding	O
to	O
the	O
bayesian	O
polynomial	O
regression	O
model	O
introduced	O
in	O
sec-	O
tion	O
1.2.6.	O
w	O
t1	O
tn	O
tion	O
1.2.6.	O
the	O
random	O
variables	O
in	O
this	O
model	O
are	O
the	O
vector	O
of	O
polynomial	O
coefﬁ-	O
cients	O
w	O
and	O
the	O
observed	O
data	O
t	O
=	O
(	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
)	O
t.	O
in	O
addition	O
,	O
this	O
model	O
contains	O
the	O
input	O
data	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
t	O
,	O
the	O
noise	O
variance	B
σ2	O
,	O
and	O
the	O
hyperparameter	B
α	O
representing	O
the	O
precision	O
of	O
the	O
gaussian	O
prior	B
over	O
w	O
,	O
all	O
of	O
which	O
are	O
parameters	O
of	O
the	O
model	O
rather	O
than	O
random	O
variables	O
.	O
focussing	O
just	O
on	O
the	O
random	O
variables	O
for	O
the	O
moment	O
,	O
we	O
see	O
that	O
the	O
joint	O
distribution	O
is	O
given	O
by	O
the	O
product	O
of	O
the	O
prior	B
p	O
(	O
w	O
)	O
and	O
n	O
conditional	B
distributions	O
p	O
(	O
tn|w	O
)	O
for	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
so	O
that	O
p	O
(	O
t	O
,	O
w	O
)	O
=	O
p	O
(	O
w	O
)	O
p	O
(	O
tn|w	O
)	O
.	O
(	O
8.6	O
)	O
n=1	O
this	O
joint	O
distribution	O
can	O
be	O
represented	O
by	O
a	O
graphical	B
model	I
shown	O
in	O
figure	O
8.3.	O
when	O
we	O
start	O
to	O
deal	O
with	O
more	O
complex	O
models	O
later	O
in	O
the	O
book	O
,	O
we	O
shall	O
ﬁnd	O
it	O
inconvenient	O
to	O
have	O
to	O
write	O
out	O
multiple	O
nodes	O
of	O
the	O
form	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
explicitly	O
as	O
in	O
figure	O
8.3.	O
we	O
therefore	O
introduce	O
a	O
graphical	O
notation	O
that	O
allows	O
such	O
multiple	O
nodes	O
to	O
be	O
expressed	O
more	O
compactly	O
,	O
in	O
which	O
we	O
draw	O
a	O
single	O
representative	O
node	B
tn	O
and	O
then	O
surround	O
this	O
with	O
a	O
box	O
,	O
called	O
a	O
plate	B
,	O
labelled	O
with	O
n	O
indicating	O
that	O
there	O
are	O
n	O
nodes	O
of	O
this	O
kind	O
.	O
re-writing	O
the	O
graph	O
of	O
figure	O
8.3	O
in	O
this	O
way	O
,	O
we	O
obtain	O
the	O
graph	O
shown	O
in	O
figure	O
8.4.	O
we	O
shall	O
sometimes	O
ﬁnd	O
it	O
helpful	O
to	O
make	O
the	O
parameters	O
of	O
a	O
model	O
,	O
as	O
well	O
as	O
its	O
stochastic	B
variables	O
,	O
explicit	O
.	O
in	O
this	O
case	O
,	O
(	O
8.6	O
)	O
becomes	O
n	O
(	O
cid:14	O
)	O
n	O
(	O
cid:14	O
)	O
p	O
(	O
t	O
,	O
w|x	O
,	O
α	O
,	O
σ2	O
)	O
=	O
p	O
(	O
w|α	O
)	O
p	O
(	O
tn|w	O
,	O
xn	O
,	O
σ2	O
)	O
.	O
n=1	O
correspondingly	O
,	O
we	O
can	O
make	O
x	O
and	O
α	O
explicit	O
in	O
the	O
graphical	O
representation	O
.	O
to	O
do	O
this	O
,	O
we	O
shall	O
adopt	O
the	O
convention	O
that	O
random	O
variables	O
will	O
be	O
denoted	O
by	O
open	O
circles	O
,	O
and	O
deterministic	O
parameters	O
will	O
be	O
denoted	O
by	O
smaller	O
solid	O
circles	O
.	O
if	O
we	O
take	O
the	O
graph	O
of	O
figure	O
8.4	O
and	O
include	O
the	O
deterministic	O
parameters	O
,	O
we	O
obtain	O
the	O
graph	O
shown	O
in	O
figure	O
8.5.	O
when	O
we	O
apply	O
a	O
graphical	B
model	I
to	O
a	O
problem	O
in	O
machine	O
learning	O
or	O
pattern	O
recognition	O
,	O
we	O
will	O
typically	O
set	O
some	O
of	O
the	O
random	O
variables	O
to	O
speciﬁc	O
observed	O
figure	O
8.4	O
an	O
alternative	O
,	O
more	O
compact	O
,	O
representation	O
of	O
the	O
graph	O
shown	O
in	O
figure	O
8.3	O
in	O
which	O
we	O
have	O
introduced	O
a	O
plate	B
(	O
the	O
box	O
labelled	O
n	O
)	O
that	O
represents	O
n	O
nodes	O
of	O
which	O
only	O
a	O
single	O
example	O
tn	O
is	O
shown	O
explicitly	O
.	O
tn	O
n	O
w	O
364	O
8.	O
graphical	O
models	O
figure	O
8.5	O
this	O
shows	O
the	O
same	O
model	O
as	O
in	O
figure	O
8.4	O
but	O
with	O
the	O
deterministic	O
parameters	O
shown	O
explicitly	O
by	O
the	O
smaller	O
solid	O
nodes	O
.	O
xn	O
α	O
σ2	O
tn	O
n	O
w	O
values	O
,	O
for	O
example	O
the	O
variables	O
{	O
tn	O
}	O
from	O
the	O
training	B
set	I
in	O
the	O
case	O
of	O
polynomial	B
curve	I
ﬁtting	I
.	O
in	O
a	O
graphical	B
model	I
,	O
we	O
will	O
denote	O
such	O
observed	O
variables	O
by	O
shad-	O
ing	O
the	O
corresponding	O
nodes	O
.	O
thus	O
the	O
graph	O
corresponding	O
to	O
figure	O
8.5	O
in	O
which	O
the	O
variables	O
{	O
tn	O
}	O
are	O
observed	O
is	O
shown	O
in	O
figure	O
8.6.	O
note	O
that	O
the	O
value	O
of	O
w	O
is	O
not	O
observed	O
,	O
and	O
so	O
w	O
is	O
an	O
example	O
of	O
a	O
latent	B
variable	I
,	O
also	O
known	O
as	O
a	O
hidden	B
variable	I
.	O
such	O
variables	O
play	O
a	O
crucial	O
role	O
in	O
many	O
probabilistic	O
models	O
and	O
will	O
form	O
the	O
focus	O
of	O
chapters	O
9	O
and	O
12.	O
having	O
observed	O
the	O
values	O
{	O
tn	O
}	O
we	O
can	O
,	O
if	O
desired	O
,	O
evaluate	O
the	O
posterior	O
dis-	O
tribution	O
of	O
the	O
polynomial	O
coefﬁcients	O
w	O
as	O
discussed	O
in	O
section	O
1.2.5.	O
for	O
the	O
moment	O
,	O
we	O
note	O
that	O
this	O
involves	O
a	O
straightforward	O
application	O
of	O
bayes	O
’	O
theorem	O
n	O
(	O
cid:14	O
)	O
p	O
(	O
w|t	O
)	O
∝	O
p	O
(	O
w	O
)	O
p	O
(	O
tn|w	O
)	O
(	O
8.7	O
)	O
n=1	O
where	O
again	O
we	O
have	O
omitted	O
the	O
deterministic	O
parameters	O
in	O
order	O
to	O
keep	O
the	O
nota-	O
tion	O
uncluttered	O
.	O
in	O
general	O
,	O
model	O
parameters	O
such	O
as	O
w	O
are	O
of	O
little	O
direct	O
interest	O
in	O
themselves	O
,	O
because	O
our	O
ultimate	O
goal	O
is	O
to	O
make	O
predictions	O
for	O
new	O
input	O
values	O
.	O
suppose	O
we	O
are	O
given	O
a	O
new	O
input	O
value	O
(	O
cid:1	O
)	O
x	O
and	O
we	O
wish	O
to	O
ﬁnd	O
the	O
corresponding	O
probability	B
dis-	O
tribution	O
for	O
(	O
cid:1	O
)	O
t	O
conditioned	O
on	O
the	O
observed	O
data	O
.	O
the	O
graphical	B
model	I
that	O
describes	O
this	O
problem	O
is	O
shown	O
in	O
figure	O
8.7	O
,	O
and	O
the	O
corresponding	O
joint	O
distribution	O
of	O
all	O
of	O
the	O
random	O
variables	O
in	O
this	O
model	O
,	O
conditioned	O
on	O
the	O
deterministic	O
parameters	O
,	O
is	O
then	O
given	O
by	O
p	O
(	O
(	O
cid:1	O
)	O
t	O
,	O
t	O
,	O
w|	O
(	O
cid:1	O
)	O
x	O
,	O
x	O
,	O
α	O
,	O
σ2	O
)	O
=	O
(	O
cid:31	O
)	O
n	O
(	O
cid:14	O
)	O
p	O
(	O
tn|xn	O
,	O
w	O
,	O
σ2	O
)	O
p	O
(	O
w|α	O
)	O
p	O
(	O
(	O
cid:1	O
)	O
t|	O
(	O
cid:1	O
)	O
x	O
,	O
w	O
,	O
σ2	O
)	O
.	O
(	O
8.8	O
)	O
n=1	O
figure	O
8.6	O
as	O
in	O
figure	O
8.5	O
but	O
with	O
the	O
nodes	O
{	O
tn	O
}	O
shaded	O
to	O
indicate	O
that	O
the	O
corresponding	O
random	O
vari-	O
ables	O
have	O
been	O
set	O
to	O
their	O
observed	O
(	O
training	B
set	I
)	O
values	O
.	O
xn	O
α	O
σ2	O
tn	O
n	O
w	O
8.1.	O
bayesian	O
networks	O
365	O
figure	O
8.7	O
the	O
polynomial	O
regression	O
model	O
,	O
corresponding	O
to	O
figure	O
8.6	O
,	O
showing	O
also	O
a	O
new	O
input	O
value	O
bx	O
together	O
with	O
the	O
corresponding	O
model	O
prediction	O
bt	O
.	O
xn	O
α	O
w	O
tn	O
n	O
the	O
required	O
predictive	B
distribution	I
for	O
(	O
cid:1	O
)	O
t	O
is	O
then	O
obtained	O
,	O
from	O
the	O
sum	O
rule	O
of	O
ˆt	O
σ2	O
ˆx	O
probability	B
,	O
by	O
integrating	O
out	O
the	O
model	O
parameters	O
w	O
so	O
that	O
p	O
(	O
(	O
cid:1	O
)	O
t|	O
(	O
cid:1	O
)	O
x	O
,	O
x	O
,	O
t	O
,	O
α	O
,	O
σ2	O
)	O
∝	O
p	O
(	O
(	O
cid:1	O
)	O
t	O
,	O
t	O
,	O
w|	O
(	O
cid:1	O
)	O
x	O
,	O
x	O
,	O
α	O
,	O
σ2	O
)	O
dw	O
(	O
cid:6	O
)	O
where	O
we	O
are	O
implicitly	O
setting	O
the	O
random	O
variables	O
in	O
t	O
to	O
the	O
speciﬁc	O
values	O
ob-	O
served	O
in	O
the	O
data	O
set	O
.	O
the	O
details	O
of	O
this	O
calculation	O
were	O
discussed	O
in	O
chapter	O
3	O
.	O
8.1.2	O
generative	O
models	O
there	O
are	O
many	O
situations	O
in	O
which	O
we	O
wish	O
to	O
draw	O
samples	O
from	O
a	O
given	O
prob-	O
ability	O
distribution	O
.	O
although	O
we	O
shall	O
devote	O
the	O
whole	O
of	O
chapter	O
11	O
to	O
a	O
detailed	O
discussion	O
of	O
sampling	B
methods	I
,	O
it	O
is	O
instructive	O
to	O
outline	O
here	O
one	O
technique	O
,	O
called	O
ancestral	B
sampling	I
,	O
which	O
is	O
particularly	O
relevant	O
to	O
graphical	O
models	O
.	O
consider	O
a	O
joint	O
distribution	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xk	O
)	O
over	O
k	O
variables	O
that	O
factorizes	O
according	O
to	O
(	O
8.5	O
)	O
corresponding	O
to	O
a	O
directed	B
acyclic	I
graph	I
.	O
we	O
shall	O
suppose	O
that	O
the	O
variables	O
have	O
been	O
ordered	O
such	O
that	O
there	O
are	O
no	O
links	O
from	O
any	O
node	B
to	O
any	O
lower	O
numbered	O
node	B
,	O
in	O
other	O
words	O
each	O
node	B
has	O
a	O
higher	O
number	O
than	O
any	O
of	O
its	O
parents	O
.	O
our	O
goal	O
is	O
to	O
draw	O
a	O
sample	O
(	O
cid:1	O
)	O
x1	O
,	O
.	O
.	O
.	O
,	O
(	O
cid:1	O
)	O
xk	O
from	O
the	O
joint	O
distribution	O
.	O
distribution	O
p	O
(	O
x1	O
)	O
,	O
which	O
we	O
call	O
(	O
cid:1	O
)	O
x1	O
.	O
we	O
then	O
work	O
through	O
each	O
of	O
the	O
nodes	O
in	O
or-	O
to	O
do	O
this	O
,	O
we	O
start	O
with	O
the	O
lowest-numbered	O
node	B
and	O
draw	O
a	O
sample	O
from	O
the	O
der	O
,	O
so	O
that	O
for	O
node	O
n	O
we	O
draw	O
a	O
sample	O
from	O
the	O
conditional	B
distribution	O
p	O
(	O
xn|pan	O
)	O
in	O
which	O
the	O
parent	O
variables	O
have	O
been	O
set	O
to	O
their	O
sampled	O
values	O
.	O
note	O
that	O
at	O
each	O
stage	O
,	O
these	O
parent	O
values	O
will	O
always	O
be	O
available	O
because	O
they	O
correspond	O
to	O
lower-	O
numbered	O
nodes	O
that	O
have	O
already	O
been	O
sampled	O
.	O
techniques	O
for	O
sampling	O
from	O
speciﬁc	O
distributions	O
will	O
be	O
discussed	O
in	O
detail	O
in	O
chapter	O
11.	O
once	O
we	O
have	O
sam-	O
pled	O
from	O
the	O
ﬁnal	O
variable	O
xk	O
,	O
we	O
will	O
have	O
achieved	O
our	O
objective	O
of	O
obtaining	O
a	O
sample	O
from	O
the	O
joint	O
distribution	O
.	O
to	O
obtain	O
a	O
sample	O
from	O
some	O
marginal	B
distribu-	O
tion	O
corresponding	O
to	O
a	O
subset	O
of	O
the	O
variables	O
,	O
we	O
simply	O
take	O
the	O
sampled	O
values	O
for	O
the	O
required	O
nodes	O
and	O
ignore	O
the	O
sampled	O
values	O
for	O
the	O
remaining	O
nodes	O
.	O
for	O
example	O
,	O
to	O
draw	O
a	O
sample	O
from	O
the	O
distribution	O
p	O
(	O
x2	O
,	O
x4	O
)	O
,	O
we	O
simply	O
sample	O
from	O
the	O
full	O
joint	O
distribution	O
and	O
then	O
retain	O
the	O
values	O
(	O
cid:1	O
)	O
x2	O
,	O
(	O
cid:1	O
)	O
x4	O
and	O
discard	O
the	O
remaining	O
values	O
{	O
(	O
cid:1	O
)	O
xj	O
(	O
cid:9	O
)	O
=2,4	O
}	O
.	O
366	O
8.	O
graphical	O
models	O
figure	O
8.8	O
a	O
graphical	B
model	I
representing	O
the	O
process	O
by	O
which	O
images	O
of	O
objects	O
are	O
created	O
,	O
in	O
which	O
the	O
identity	O
of	O
an	O
object	O
(	O
a	O
discrete	O
variable	O
)	O
and	O
the	O
position	O
and	O
orientation	O
of	O
that	O
object	O
(	O
continuous	O
variables	O
)	O
have	O
independent	B
prior	O
probabilities	O
.	O
the	O
image	O
(	O
a	O
vector	O
of	O
pixel	O
intensities	O
)	O
has	O
a	O
probability	B
distribution	O
that	O
is	O
dependent	O
on	O
the	O
identity	O
of	O
the	O
object	O
as	O
well	O
as	O
on	O
its	O
position	O
and	O
orientation	O
.	O
object	O
position	O
orientation	O
image	O
for	O
practical	O
applications	O
of	O
probabilistic	O
models	O
,	O
it	O
will	O
typically	O
be	O
the	O
higher-	O
numbered	O
variables	O
corresponding	O
to	O
terminal	O
nodes	O
of	O
the	O
graph	O
that	O
represent	O
the	O
observations	O
,	O
with	O
lower-numbered	O
nodes	O
corresponding	O
to	O
latent	O
variables	O
.	O
the	O
primary	O
role	O
of	O
the	O
latent	O
variables	O
is	O
to	O
allow	O
a	O
complicated	O
distribution	O
over	O
the	O
observed	O
variables	O
to	O
be	O
represented	O
in	O
terms	O
of	O
a	O
model	O
constructed	O
from	O
simpler	O
(	O
typically	O
exponential	B
family	I
)	O
conditional	B
distributions	O
.	O
we	O
can	O
interpret	O
such	O
models	O
as	O
expressing	O
the	O
processes	O
by	O
which	O
the	O
observed	O
data	O
arose	O
.	O
for	O
instance	O
,	O
consider	O
an	O
object	B
recognition	I
task	O
in	O
which	O
each	O
observed	O
data	O
point	O
corresponds	O
to	O
an	O
image	O
(	O
comprising	O
a	O
vector	O
of	O
pixel	O
intensities	O
)	O
of	O
one	O
of	O
the	O
objects	O
.	O
in	O
this	O
case	O
,	O
the	O
latent	O
variables	O
might	O
have	O
an	O
interpretation	O
as	O
the	O
position	O
and	O
orientation	O
of	O
the	O
object	O
.	O
given	O
a	O
particular	O
observed	O
image	O
,	O
our	O
goal	O
is	O
to	O
ﬁnd	O
the	O
posterior	O
distribution	O
over	O
objects	O
,	O
in	O
which	O
we	O
integrate	O
over	O
all	O
possible	O
positions	O
and	O
orientations	O
.	O
we	O
can	O
represent	O
this	O
problem	O
using	O
a	O
graphical	B
model	I
of	O
the	O
form	O
show	O
in	O
figure	O
8.8.	O
the	O
graphical	B
model	I
captures	O
the	O
causal	O
process	O
(	O
pearl	O
,	O
1988	O
)	O
by	O
which	O
the	O
ob-	O
served	O
data	O
was	O
generated	O
.	O
for	O
this	O
reason	O
,	O
such	O
models	O
are	O
often	O
called	O
generative	O
models	O
.	O
by	O
contrast	O
,	O
the	O
polynomial	O
regression	O
model	O
described	O
by	O
figure	O
8.5	O
is	O
not	O
generative	O
because	O
there	O
is	O
no	O
probability	B
distribution	O
associated	O
with	O
the	O
input	O
variable	O
x	O
,	O
and	O
so	O
it	O
is	O
not	O
possible	O
to	O
generate	O
synthetic	O
data	O
points	O
from	O
this	O
model	O
.	O
we	O
could	O
make	O
it	O
generative	O
by	O
introducing	O
a	O
suitable	O
prior	B
distribution	O
p	O
(	O
x	O
)	O
,	O
at	O
the	O
expense	O
of	O
a	O
more	O
complex	O
model	O
.	O
the	O
hidden	O
variables	O
in	O
a	O
probabilistic	O
model	O
need	O
not	O
,	O
however	O
,	O
have	O
any	O
ex-	O
plicit	O
physical	O
interpretation	O
but	O
may	O
be	O
introduced	O
simply	O
to	O
allow	O
a	O
more	O
complex	O
joint	O
distribution	O
to	O
be	O
constructed	O
from	O
simpler	O
components	O
.	O
in	O
either	O
case	O
,	O
the	O
technique	O
of	O
ancestral	B
sampling	I
applied	O
to	O
a	O
generative	B
model	I
mimics	O
the	O
creation	O
of	O
the	O
observed	O
data	O
and	O
would	O
therefore	O
give	O
rise	O
to	O
‘	O
fantasy	O
’	O
data	O
whose	O
probability	B
distribution	O
(	O
if	O
the	O
model	O
were	O
a	O
perfect	O
representation	O
of	O
reality	O
)	O
would	O
be	O
the	O
same	O
as	O
that	O
of	O
the	O
observed	O
data	O
.	O
in	O
practice	O
,	O
producing	O
synthetic	O
observations	O
from	O
a	O
generative	B
model	I
can	O
prove	O
informative	O
in	O
understanding	O
the	O
form	O
of	O
the	O
probability	B
distribution	O
represented	O
by	O
that	O
model	O
.	O
8.1.3	O
discrete	O
variables	O
we	O
have	O
discussed	O
the	O
importance	O
of	O
probability	B
distributions	O
that	O
are	O
members	O
of	O
the	O
exponential	B
family	I
,	O
and	O
we	O
have	O
seen	O
that	O
this	O
family	O
includes	O
many	O
well-	O
known	O
distributions	O
as	O
particular	O
cases	O
.	O
although	O
such	O
distributions	O
are	O
relatively	O
simple	O
,	O
they	O
form	O
useful	O
building	O
blocks	O
for	O
constructing	O
more	O
complex	O
probability	B
section	O
2.4	O
8.1.	O
bayesian	O
networks	O
367	O
figure	O
8.9	O
(	O
a	O
)	O
this	O
fully-connected	O
graph	O
describes	O
a	O
general	O
distribu-	O
tion	O
over	O
two	O
k-state	O
discrete	O
variables	O
having	O
a	O
total	O
of	O
k	O
2	O
−	O
1	O
parameters	O
.	O
(	O
b	O
)	O
by	O
dropping	O
the	O
link	B
between	O
the	O
nodes	O
,	O
the	O
number	O
of	O
parameters	O
is	O
reduced	O
to	O
2	O
(	O
k	O
−	O
1	O
)	O
.	O
x1	O
x1	O
(	O
a	O
)	O
(	O
b	O
)	O
x2	O
x2	O
distributions	O
,	O
and	O
the	O
framework	O
of	O
graphical	O
models	O
is	O
very	O
useful	O
in	O
expressing	O
the	O
way	O
in	O
which	O
these	O
building	O
blocks	O
are	O
linked	O
together	O
.	O
such	O
models	O
have	O
particularly	O
nice	O
properties	O
if	O
we	O
choose	O
the	O
relationship	O
be-	O
tween	O
each	O
parent-child	O
pair	O
in	O
a	O
directed	B
graph	O
to	O
be	O
conjugate	B
,	O
and	O
we	O
shall	O
ex-	O
plore	O
several	O
examples	O
of	O
this	O
shortly	O
.	O
two	O
cases	O
are	O
particularly	O
worthy	O
of	O
note	O
,	O
namely	O
when	O
the	O
parent	O
and	O
child	B
node	I
each	O
correspond	O
to	O
discrete	O
variables	O
and	O
when	O
they	O
each	O
correspond	O
to	O
gaussian	O
variables	O
,	O
because	O
in	O
these	O
two	O
cases	O
the	O
relationship	O
can	O
be	O
extended	B
hierarchically	O
to	O
construct	O
arbitrarily	O
complex	O
directed	O
acyclic	O
graphs	O
.	O
we	O
begin	O
by	O
examining	O
the	O
discrete	O
case	O
.	O
the	O
probability	B
distribution	O
p	O
(	O
x|µ	O
)	O
for	O
a	O
single	O
discrete	O
variable	O
x	O
having	O
k	O
possible	O
states	O
(	O
using	O
the	O
1-of-k	O
representation	O
)	O
is	O
given	O
by	O
p	O
(	O
x|µ	O
)	O
=	O
µxk	O
k	O
k=1	O
(	O
cid:5	O
)	O
and	O
is	O
governed	O
by	O
the	O
parameters	O
µ	O
=	O
(	O
µ1	O
,	O
.	O
.	O
.	O
,	O
µk	O
)	O
t.	O
due	O
to	O
the	O
constraint	O
k	O
µk	O
=	O
1	O
,	O
only	O
k	O
−	O
1	O
values	O
for	O
µk	O
need	O
to	O
be	O
speciﬁed	O
in	O
order	O
to	O
deﬁne	O
the	O
now	O
suppose	O
that	O
we	O
have	O
two	O
discrete	O
variables	O
,	O
x1	O
and	O
x2	O
,	O
each	O
of	O
which	O
has	O
k	O
states	O
,	O
and	O
we	O
wish	O
to	O
model	O
their	O
joint	O
distribution	O
.	O
we	O
denote	O
the	O
probability	B
of	O
observing	O
both	O
x1k	O
=	O
1	O
and	O
x2l	O
=	O
1	O
by	O
the	O
parameter	O
µkl	O
,	O
where	O
x1k	O
denotes	O
the	O
kth	O
component	O
of	O
x1	O
,	O
and	O
similarly	O
for	O
x2l	O
.	O
the	O
joint	O
distribution	O
can	O
be	O
written	O
distribution	O
.	O
(	O
8.9	O
)	O
k	O
(	O
cid:14	O
)	O
p	O
(	O
x1	O
,	O
x2|µ	O
)	O
=	O
k	O
(	O
cid:14	O
)	O
k	O
(	O
cid:14	O
)	O
k=1	O
l=1	O
µx1kx2l	O
kl	O
.	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
k	O
l	O
µkl	O
=	O
1	O
,	O
this	O
distri-	O
because	O
the	O
parameters	O
µkl	O
are	O
subject	O
to	O
the	O
constraint	O
bution	O
is	O
governed	O
by	O
k	O
2	O
−	O
1	O
parameters	O
.	O
it	O
is	O
easily	O
seen	O
that	O
the	O
total	O
number	O
of	O
parameters	O
that	O
must	O
be	O
speciﬁed	O
for	O
an	O
arbitrary	O
joint	O
distribution	O
over	O
m	O
variables	O
is	O
km	O
−	O
1	O
and	O
therefore	O
grows	O
exponentially	O
with	O
the	O
number	O
m	O
of	O
variables	O
.	O
using	O
the	O
product	B
rule	I
,	O
we	O
can	O
factor	O
the	O
joint	O
distribution	O
p	O
(	O
x1	O
,	O
x2	O
)	O
in	O
the	O
form	O
p	O
(	O
x2|x1	O
)	O
p	O
(	O
x1	O
)	O
,	O
which	O
corresponds	O
to	O
a	O
two-node	O
graph	O
with	O
a	O
link	B
going	O
from	O
the	O
x1	O
node	B
to	O
the	O
x2	O
node	B
as	O
shown	O
in	O
figure	O
8.9	O
(	O
a	O
)	O
.	O
the	O
marginal	B
distribution	O
p	O
(	O
x1	O
)	O
is	O
governed	O
by	O
k	O
−	O
1	O
parameters	O
,	O
as	O
before	O
,	O
similarly	O
,	O
the	O
conditional	B
distribution	O
p	O
(	O
x2|x1	O
)	O
requires	O
the	O
speciﬁcation	O
of	O
k	O
−	O
1	O
parameters	O
for	O
each	O
of	O
the	O
k	O
possible	O
values	O
of	O
x1	O
.	O
the	O
total	O
number	O
of	O
parameters	O
that	O
must	O
be	O
speciﬁed	O
in	O
the	O
joint	O
distribution	O
is	O
therefore	O
(	O
k	O
−	O
1	O
)	O
+	O
k	O
(	O
k	O
−	O
1	O
)	O
=	O
k	O
2	O
−	O
1	O
as	O
before	O
.	O
now	O
suppose	O
that	O
the	O
variables	O
x1	O
and	O
x2	O
were	O
independent	B
,	O
corresponding	O
to	O
the	O
graphical	B
model	I
shown	O
in	O
figure	O
8.9	O
(	O
b	O
)	O
.	O
each	O
variable	O
is	O
then	O
described	O
by	O
368	O
8.	O
graphical	O
models	O
figure	O
8.10	O
this	O
chain	O
of	O
m	O
discrete	O
nodes	O
,	O
each	O
having	O
k	O
states	O
,	O
requires	O
the	O
speciﬁcation	O
of	O
k	O
−	O
1	O
+	O
(	O
m	O
−	O
1	O
)	O
k	O
(	O
k	O
−	O
1	O
)	O
parameters	O
,	O
which	O
grows	O
linearly	O
with	O
the	O
length	O
m	O
of	O
the	O
chain	O
.	O
in	O
contrast	O
,	O
a	O
fully	O
con-	O
nected	O
graph	O
of	O
m	O
nodes	O
would	O
have	O
km	O
−	O
1	O
param-	O
eters	O
,	O
which	O
grows	O
exponentially	O
with	O
m.	O
x1	O
x2	O
xm	O
a	O
separate	O
multinomial	B
distribution	I
,	O
and	O
the	O
total	O
number	O
of	O
parameters	O
would	O
be	O
2	O
(	O
k	O
−	O
1	O
)	O
.	O
for	O
a	O
distribution	O
over	O
m	O
independent	B
discrete	O
variables	O
,	O
each	O
having	O
k	O
states	O
,	O
the	O
total	O
number	O
of	O
parameters	O
would	O
be	O
m	O
(	O
k	O
−	O
1	O
)	O
,	O
which	O
therefore	O
grows	O
linearly	O
with	O
the	O
number	O
of	O
variables	O
.	O
from	O
a	O
graphical	O
perspective	O
,	O
we	O
have	O
reduced	O
the	O
number	O
of	O
parameters	O
by	O
dropping	O
links	O
in	O
the	O
graph	O
,	O
at	O
the	O
expense	O
of	O
having	O
a	O
restricted	O
class	O
of	O
distributions	O
.	O
more	O
generally	O
,	O
if	O
we	O
have	O
m	O
discrete	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
,	O
we	O
can	O
model	O
the	O
joint	O
distribution	O
using	O
a	O
directed	B
graph	O
with	O
one	O
variable	O
corresponding	O
to	O
each	O
node	B
.	O
the	O
conditional	B
distribution	O
at	O
each	O
node	B
is	O
given	O
by	O
a	O
set	O
of	O
nonnegative	O
pa-	O
rameters	O
subject	O
to	O
the	O
usual	O
normalization	O
constraint	O
.	O
if	O
the	O
graph	O
is	O
fully	B
connected	I
then	O
we	O
have	O
a	O
completely	O
general	O
distribution	O
having	O
km	O
−	O
1	O
parameters	O
,	O
whereas	O
if	O
there	O
are	O
no	O
links	O
in	O
the	O
graph	O
the	O
joint	O
distribution	O
factorizes	O
into	O
the	O
product	O
of	O
the	O
marginals	O
,	O
and	O
the	O
total	O
number	O
of	O
parameters	O
is	O
m	O
(	O
k	O
−	O
1	O
)	O
.	O
graphs	O
having	O
in-	O
termediate	O
levels	O
of	O
connectivity	O
allow	O
for	O
more	O
general	O
distributions	O
than	O
the	O
fully	O
factorized	O
one	O
while	O
requiring	O
fewer	O
parameters	O
than	O
the	O
general	O
joint	O
distribution	O
.	O
as	O
an	O
illustration	O
,	O
consider	O
the	O
chain	O
of	O
nodes	O
shown	O
in	O
figure	O
8.10.	O
the	O
marginal	B
distribution	O
p	O
(	O
x1	O
)	O
requires	O
k	O
−	O
1	O
parameters	O
,	O
whereas	O
each	O
of	O
the	O
m	O
−	O
1	O
condi-	O
tional	O
distributions	O
p	O
(	O
xi|xi−1	O
)	O
,	O
for	O
i	O
=	O
2	O
,	O
.	O
.	O
.	O
,	O
m	O
,	O
requires	O
k	O
(	O
k	O
−	O
1	O
)	O
parameters	O
.	O
this	O
gives	O
a	O
total	O
parameter	O
count	O
of	O
k	O
−	O
1	O
+	O
(	O
m	O
−	O
1	O
)	O
k	O
(	O
k	O
−	O
1	O
)	O
,	O
which	O
is	O
quadratic	O
in	O
k	O
and	O
which	O
grows	O
linearly	O
(	O
rather	O
than	O
exponentially	O
)	O
with	O
the	O
length	O
m	O
of	O
the	O
chain	O
.	O
an	O
alternative	O
way	O
to	O
reduce	O
the	O
number	O
of	O
independent	B
parameters	O
in	O
a	O
model	O
is	O
by	O
sharing	O
parameters	O
(	O
also	O
known	O
as	O
tying	O
of	O
parameters	O
)	O
.	O
for	O
instance	O
,	O
in	O
the	O
chain	O
example	O
of	O
figure	O
8.10	O
,	O
we	O
can	O
arrange	O
that	O
all	O
of	O
the	O
conditional	B
distributions	O
p	O
(	O
xi|xi−1	O
)	O
,	O
for	O
i	O
=	O
2	O
,	O
.	O
.	O
.	O
,	O
m	O
,	O
are	O
governed	O
by	O
the	O
same	O
set	O
of	O
k	O
(	O
k−1	O
)	O
parameters	O
.	O
together	O
with	O
the	O
k−1	O
parameters	O
governing	O
the	O
distribution	O
of	O
x1	O
,	O
this	O
gives	O
a	O
total	O
of	O
k	O
2	O
−	O
1	O
parameters	O
that	O
must	O
be	O
speciﬁed	O
in	O
order	O
to	O
deﬁne	O
the	O
joint	O
distribution	O
.	O
we	O
can	O
turn	O
a	O
graph	O
over	O
discrete	O
variables	O
into	O
a	O
bayesian	O
model	O
by	O
introduc-	O
ing	O
dirichlet	O
priors	O
for	O
the	O
parameters	O
.	O
from	O
a	O
graphical	O
point	O
of	O
view	O
,	O
each	O
node	B
then	O
acquires	O
an	O
additional	O
parent	O
representing	O
the	O
dirichlet	O
distribution	O
over	O
the	O
pa-	O
rameters	O
associated	O
with	O
the	O
corresponding	O
discrete	O
node	B
.	O
this	O
is	O
illustrated	O
for	O
the	O
chain	O
model	O
in	O
figure	O
8.11.	O
the	O
corresponding	O
model	O
in	O
which	O
we	O
tie	O
the	O
parame-	O
ters	O
governing	O
the	O
conditional	B
distributions	O
p	O
(	O
xi|xi−1	O
)	O
,	O
for	O
i	O
=	O
2	O
,	O
.	O
.	O
.	O
,	O
m	O
,	O
is	O
shown	O
in	O
figure	O
8.12.	O
another	O
way	O
of	O
controlling	O
the	O
exponential	O
growth	O
in	O
the	O
number	O
of	O
parameters	O
in	O
models	O
of	O
discrete	O
variables	O
is	O
to	O
use	O
parameterized	O
models	O
for	O
the	O
conditional	B
distributions	O
instead	O
of	O
complete	O
tables	O
of	O
conditional	B
probability	I
values	O
.	O
to	O
illus-	O
trate	O
this	O
idea	O
,	O
consider	O
the	O
graph	O
in	O
figure	O
8.13	O
in	O
which	O
all	O
of	O
the	O
nodes	O
represent	O
binary	O
variables	O
.	O
each	O
of	O
the	O
parent	O
variables	O
xi	O
is	O
governed	O
by	O
a	O
single	O
parame-	O
8.1.	O
bayesian	O
networks	O
figure	O
8.11	O
an	O
extension	O
of	O
the	O
model	O
of	O
figure	O
8.10	O
to	O
include	O
dirich-	O
let	O
priors	O
over	O
the	O
param-	O
eters	O
governing	O
the	O
discrete	O
distributions	O
.	O
figure	O
8.12	O
as	O
in	O
figure	O
8.11	O
but	O
with	O
a	O
sin-	O
gle	O
set	O
of	O
parameters	O
µ	O
shared	O
amongst	O
all	O
of	O
the	O
conditional	B
distributions	O
p	O
(	O
xi|xi−1	O
)	O
.	O
µ1	O
x1	O
µ1	O
x1	O
µ2	O
x2	O
x2	O
µ	O
369	O
µm	O
xm	O
xm	O
ter	O
µi	O
representing	O
the	O
probability	B
p	O
(	O
xi	O
=	O
1	O
)	O
,	O
giving	O
m	O
parameters	O
in	O
total	O
for	O
the	O
parent	O
nodes	O
.	O
the	O
conditional	B
distribution	O
p	O
(	O
y|x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
,	O
however	O
,	O
would	O
require	O
2m	O
parameters	O
representing	O
the	O
probability	B
p	O
(	O
y	O
=	O
1	O
)	O
for	O
each	O
of	O
the	O
2m	O
possible	O
settings	O
of	O
the	O
parent	O
variables	O
.	O
thus	O
in	O
general	O
the	O
number	O
of	O
parameters	O
required	O
to	O
specify	O
this	O
conditional	B
distribution	O
will	O
grow	O
exponentially	O
with	O
m.	O
we	O
can	O
ob-	O
tain	O
a	O
more	O
parsimonious	O
form	O
for	O
the	O
conditional	B
distribution	O
by	O
using	O
a	O
logistic	B
sigmoid	I
function	O
acting	O
on	O
a	O
linear	O
combination	O
of	O
the	O
parent	O
variables	O
,	O
giving	O
(	O
cid:22	O
)	O
(	O
cid:23	O
)	O
m	O
(	O
cid:2	O
)	O
section	O
2.4	O
p	O
(	O
y	O
=	O
1|x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
=	O
σ	O
w0	O
+	O
wixi	O
=	O
σ	O
(	O
wtx	O
)	O
(	O
8.10	O
)	O
i=1	O
where	O
σ	O
(	O
a	O
)	O
=	O
(	O
1+exp	O
(	O
−a	O
)	O
)	O
−1	O
is	O
the	O
logistic	B
sigmoid	I
,	O
x	O
=	O
(	O
x0	O
,	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
t	O
is	O
an	O
(	O
m	O
+	O
1	O
)	O
-dimensional	O
vector	O
of	O
parent	O
states	O
augmented	O
with	O
an	O
additional	O
variable	O
x0	O
whose	O
value	O
is	O
clamped	O
to	O
1	O
,	O
and	O
w	O
=	O
(	O
w0	O
,	O
w1	O
,	O
.	O
.	O
.	O
,	O
wm	O
)	O
t	O
is	O
a	O
vector	O
of	O
m	O
+	O
1	O
parameters	O
.	O
this	O
is	O
a	O
more	O
restricted	O
form	O
of	O
conditional	B
distribution	O
than	O
the	O
general	O
case	O
but	O
is	O
now	O
governed	O
by	O
a	O
number	O
of	O
parameters	O
that	O
grows	O
linearly	O
with	O
m.	O
in	O
this	O
sense	O
,	O
it	O
is	O
analogous	O
to	O
the	O
choice	O
of	O
a	O
restrictive	O
form	O
of	O
covariance	B
matrix	I
(	O
for	O
example	O
,	O
a	O
diagonal	B
matrix	O
)	O
in	O
a	O
multivariate	O
gaussian	O
distribution	O
.	O
the	O
motivation	O
for	O
the	O
logistic	B
sigmoid	I
representation	O
was	O
discussed	O
in	O
section	O
4.2.	O
figure	O
8.13	O
a	O
graph	O
comprising	O
m	O
parents	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
and	O
a	O
sin-	O
gle	O
child	O
y	O
,	O
used	O
to	O
illustrate	O
the	O
idea	O
of	O
parameterized	O
conditional	B
distributions	O
for	O
discrete	O
variables	O
.	O
x1	O
xm	O
y	O
370	O
8.	O
graphical	O
models	O
8.1.4	O
linear-gaussian	O
models	O
in	O
the	O
previous	O
section	O
,	O
we	O
saw	O
how	O
to	O
construct	O
joint	O
probability	B
distributions	O
over	O
a	O
set	O
of	O
discrete	O
variables	O
by	O
expressing	O
the	O
variables	O
as	O
nodes	O
in	O
a	O
directed	B
acyclic	I
graph	I
.	O
here	O
we	O
show	O
how	O
a	O
multivariate	O
gaussian	O
can	O
be	O
expressed	O
as	O
a	O
directed	B
graph	O
corresponding	O
to	O
a	O
linear-gaussian	O
model	O
over	O
the	O
component	O
vari-	O
ables	O
.	O
this	O
allows	O
us	O
to	O
impose	O
interesting	O
structure	O
on	O
the	O
distribution	O
,	O
with	O
the	O
general	O
gaussian	O
and	O
the	O
diagonal	B
covariance	O
gaussian	O
representing	O
opposite	O
ex-	O
tremes	O
.	O
several	O
widely	O
used	O
techniques	O
are	O
examples	O
of	O
linear-gaussian	O
models	O
,	O
such	O
as	O
probabilistic	O
principal	O
component	O
analysis	O
,	O
factor	B
analysis	I
,	O
and	O
linear	O
dy-	O
namical	O
systems	O
(	O
roweis	O
and	O
ghahramani	O
,	O
1999	O
)	O
.	O
we	O
shall	O
make	O
extensive	O
use	O
of	O
the	O
results	O
of	O
this	O
section	O
in	O
later	O
chapters	O
when	O
we	O
consider	O
some	O
of	O
these	O
techniques	O
in	O
detail	O
.	O
consider	O
an	O
arbitrary	O
directed	B
acyclic	I
graph	I
over	O
d	O
variables	O
in	O
which	O
node	B
i	O
represents	O
a	O
single	O
continuous	O
random	O
variable	O
xi	O
having	O
a	O
gaussian	O
distribution	O
.	O
the	O
mean	B
of	O
this	O
distribution	O
is	O
taken	O
to	O
be	O
a	O
linear	O
combination	O
of	O
the	O
states	O
of	O
its	O
parent	O
nodes	O
pai	O
of	O
node	B
i	O
⎛⎝xi	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:2	O
)	O
j∈pai	O
⎞⎠	O
p	O
(	O
xi|pai	O
)	O
=	O
n	O
wijxj	O
+	O
bi	O
,	O
vi	O
(	O
8.11	O
)	O
where	O
wij	O
and	O
bi	O
are	O
parameters	O
governing	O
the	O
mean	B
,	O
and	O
vi	O
is	O
the	O
variance	B
of	O
the	O
conditional	B
distribution	O
for	O
xi	O
.	O
the	O
log	O
of	O
the	O
joint	O
distribution	O
is	O
then	O
the	O
log	O
of	O
the	O
product	O
of	O
these	O
conditionals	O
over	O
all	O
nodes	O
in	O
the	O
graph	O
and	O
hence	O
takes	O
the	O
form	O
d	O
(	O
cid:2	O
)	O
=	O
−	O
d	O
(	O
cid:2	O
)	O
i=1	O
i=1	O
ln	O
p	O
(	O
x	O
)	O
=	O
ln	O
p	O
(	O
xi|pai	O
)	O
⎛⎝xi	O
−	O
⎞⎠2	O
wijxj	O
−	O
bi	O
(	O
cid:2	O
)	O
j∈pai	O
(	O
8.12	O
)	O
+	O
const	O
(	O
8.13	O
)	O
1	O
2vi	O
where	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
t	O
and	O
‘	O
const	O
’	O
denotes	O
terms	O
independent	B
of	O
x.	O
we	O
see	O
that	O
this	O
is	O
a	O
quadratic	O
function	O
of	O
the	O
components	O
of	O
x	O
,	O
and	O
hence	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
)	O
is	O
a	O
multivariate	O
gaussian	O
.	O
we	O
can	O
determine	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
joint	O
distribution	O
recursively	O
as	O
follows	O
.	O
each	O
variable	O
xi	O
has	O
(	O
conditional	B
on	O
the	O
states	O
of	O
its	O
parents	O
)	O
a	O
gaussian	O
distribution	O
of	O
the	O
form	O
(	O
8.11	O
)	O
and	O
so	O
√	O
xi	O
=	O
wijxj	O
+	O
bi	O
+	O
vii	O
(	O
8.14	O
)	O
where	O
i	O
is	O
a	O
zero	O
mean	B
,	O
unit	O
variance	B
gaussian	O
random	O
variable	O
satisfying	O
e	O
[	O
i	O
]	O
=	O
0	O
and	O
e	O
[	O
ij	O
]	O
=	O
iij	O
,	O
where	O
iij	O
is	O
the	O
i	O
,	O
j	O
element	O
of	O
the	O
identity	O
matrix	O
.	O
taking	O
the	O
expectation	B
of	O
(	O
8.14	O
)	O
,	O
we	O
have	O
e	O
[	O
xi	O
]	O
=	O
wij	O
e	O
[	O
xj	O
]	O
+	O
bi	O
.	O
(	O
8.15	O
)	O
(	O
cid:2	O
)	O
j∈pai	O
(	O
cid:2	O
)	O
j∈pai	O
figure	O
8.14	O
a	O
directed	B
graph	O
over	O
three	O
gaussian	O
variables	O
,	O
with	O
one	O
missing	O
link	O
.	O
x1	O
x2	O
8.1.	O
bayesian	O
networks	O
371	O
x3	O
thus	O
we	O
can	O
ﬁnd	O
the	O
components	O
of	O
e	O
[	O
x	O
]	O
=	O
(	O
e	O
[	O
x1	O
]	O
,	O
.	O
.	O
.	O
,	O
e	O
[	O
xd	O
]	O
)	O
t	O
by	O
starting	O
at	O
the	O
lowest	O
numbered	O
node	B
and	O
working	O
recursively	O
through	O
the	O
graph	O
(	O
here	O
we	O
again	O
assume	O
that	O
the	O
nodes	O
are	O
numbered	O
such	O
that	O
each	O
node	B
has	O
a	O
higher	O
number	O
than	O
its	O
parents	O
)	O
.	O
similarly	O
,	O
we	O
can	O
use	O
(	O
8.14	O
)	O
and	O
(	O
8.15	O
)	O
to	O
obtain	O
the	O
i	O
,	O
j	O
element	O
of	O
the	O
covariance	B
matrix	I
for	O
p	O
(	O
x	O
)	O
in	O
the	O
form	O
of	O
a	O
recursion	O
relation	O
cov	O
[	O
xi	O
,	O
xj	O
]	O
=	O
e	O
[	O
(	O
xi	O
−	O
e	O
[	O
xi	O
]	O
)	O
(	O
xj	O
−	O
e	O
[	O
xj	O
]	O
)	O
]	O
⎡⎣	O
(	O
xi	O
−	O
e	O
[	O
xi	O
]	O
)	O
(	O
cid:2	O
)	O
⎧⎨⎩	O
(	O
cid:2	O
)	O
k∈paj	O
=	O
e	O
=	O
wjkcov	O
[	O
xi	O
,	O
xk	O
]	O
+	O
iijvj	O
k∈paj	O
wjk	O
(	O
xk	O
−	O
e	O
[	O
xk	O
]	O
)	O
+	O
√	O
vjj	O
⎤⎦	O
⎫⎬⎭	O
(	O
8.16	O
)	O
and	O
so	O
the	O
covariance	B
can	O
similarly	O
be	O
evaluated	O
recursively	O
starting	O
from	O
the	O
lowest	O
numbered	O
node	B
.	O
let	O
us	O
consider	O
two	O
extreme	O
cases	O
.	O
first	O
of	O
all	O
,	O
suppose	O
that	O
there	O
are	O
no	O
links	O
in	O
the	O
graph	O
,	O
which	O
therefore	O
comprises	O
d	O
isolated	O
nodes	O
.	O
in	O
this	O
case	O
,	O
there	O
are	O
no	O
parameters	O
wij	O
and	O
so	O
there	O
are	O
just	O
d	O
parameters	O
bi	O
and	O
d	O
parameters	O
vi	O
.	O
from	O
the	O
recursion	O
relations	O
(	O
8.15	O
)	O
and	O
(	O
8.16	O
)	O
,	O
we	O
see	O
that	O
the	O
mean	B
of	O
p	O
(	O
x	O
)	O
is	O
given	O
by	O
(	O
b1	O
,	O
.	O
.	O
.	O
,	O
bd	O
)	O
t	O
and	O
the	O
covariance	B
matrix	I
is	O
diagonal	B
of	O
the	O
form	O
diag	O
(	O
v1	O
,	O
.	O
.	O
.	O
,	O
vd	O
)	O
.	O
the	O
joint	O
distribution	O
has	O
a	O
total	O
of	O
2d	O
parameters	O
and	O
represents	O
a	O
set	O
of	O
d	O
inde-	O
pendent	O
univariate	O
gaussian	O
distributions	O
.	O
now	O
consider	O
a	O
fully	B
connected	I
graph	O
in	O
which	O
each	O
node	B
has	O
all	O
lower	O
num-	O
bered	O
nodes	O
as	O
parents	O
.	O
the	O
matrix	O
wij	O
then	O
has	O
i	O
−	O
1	O
entries	O
on	O
the	O
ith	O
row	O
and	O
hence	O
is	O
a	O
lower	O
triangular	O
matrix	O
(	O
with	O
no	O
entries	O
on	O
the	O
leading	O
diagonal	B
)	O
.	O
then	O
the	O
total	O
number	O
of	O
parameters	O
wij	O
is	O
obtained	O
by	O
taking	O
the	O
number	O
d2	O
of	O
elements	O
in	O
a	O
d×	O
d	O
matrix	O
,	O
subtracting	O
d	O
to	O
account	O
for	O
the	O
absence	O
of	O
elements	O
on	O
the	O
lead-	O
ing	O
diagonal	O
,	O
and	O
then	O
dividing	O
by	O
2	O
because	O
the	O
matrix	O
has	O
elements	O
only	O
below	O
the	O
diagonal	B
,	O
giving	O
a	O
total	O
of	O
d	O
(	O
d−1	O
)	O
/2	O
.	O
the	O
total	O
number	O
of	O
independent	B
parameters	O
{	O
wij	O
}	O
and	O
{	O
vi	O
}	O
in	O
the	O
covariance	B
matrix	I
is	O
therefore	O
d	O
(	O
d	O
+	O
1	O
)	O
/2	O
corresponding	O
to	O
a	O
general	O
symmetric	O
covariance	B
matrix	I
.	O
graphs	O
having	O
some	O
intermediate	O
level	O
of	O
complexity	O
correspond	O
to	O
joint	O
gaus-	O
sian	O
distributions	O
with	O
partially	O
constrained	O
covariance	B
matrices	O
.	O
consider	O
for	O
ex-	O
ample	O
the	O
graph	O
shown	O
in	O
figure	O
8.14	O
,	O
which	O
has	O
a	O
link	B
missing	O
between	O
variables	O
x1	O
and	O
x3	O
.	O
using	O
the	O
recursion	O
relations	O
(	O
8.15	O
)	O
and	O
(	O
8.16	O
)	O
,	O
we	O
see	O
that	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
joint	O
distribution	O
are	O
given	O
by	O
(	O
cid:22	O
)	O
µ	O
=	O
(	O
b1	O
,	O
b2	O
+	O
w21b1	O
,	O
b3	O
+	O
w32b2	O
+	O
w32w21b1	O
)	O
t	O
σ	O
=	O
v1	O
w21v1	O
v2	O
+	O
w2	O
w21v1	O
21v1	O
w32w21v1	O
w32	O
(	O
v2	O
+	O
w2	O
21v1	O
)	O
v3	O
+	O
w2	O
w32w21v1	O
w32	O
(	O
v2	O
+	O
w2	O
21v1	O
)	O
32	O
(	O
v2	O
+	O
w2	O
21v1	O
)	O
(	O
cid:23	O
)	O
(	O
8.17	O
)	O
.	O
(	O
8.18	O
)	O
section	O
2.3	O
exercise	O
8.7	O
372	O
8.	O
graphical	O
models	O
we	O
can	O
readily	O
extend	O
the	O
linear-gaussian	O
graphical	B
model	I
to	O
the	O
case	O
in	O
which	O
the	O
nodes	O
of	O
the	O
graph	O
represent	O
multivariate	O
gaussian	O
variables	O
.	O
in	O
this	O
case	O
,	O
we	O
can	O
write	O
the	O
conditional	B
distribution	O
for	O
node	O
i	O
in	O
the	O
form	O
p	O
(	O
xi|pai	O
)	O
=	O
n	O
wijxj	O
+	O
bi	O
,	O
σi	O
(	O
8.19	O
)	O
⎛⎝xi	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:2	O
)	O
j∈pai	O
⎞⎠	O
section	O
2.3.6	O
where	O
now	O
wij	O
is	O
a	O
matrix	O
(	O
which	O
is	O
nonsquare	O
if	O
xi	O
and	O
xj	O
have	O
different	O
dimen-	O
sionalities	O
)	O
.	O
again	O
it	O
is	O
easy	O
to	O
verify	O
that	O
the	O
joint	O
distribution	O
over	O
all	O
variables	O
is	O
gaussian	O
.	O
note	O
that	O
we	O
have	O
already	O
encountered	O
a	O
speciﬁc	O
example	O
of	O
the	O
linear-gaussian	O
relationship	O
when	O
we	O
saw	O
that	O
the	O
conjugate	B
prior	I
for	O
the	O
mean	B
µ	O
of	O
a	O
gaussian	O
variable	O
x	O
is	O
itself	O
a	O
gaussian	O
distribution	O
over	O
µ.	O
the	O
joint	O
distribution	O
over	O
x	O
and	O
µ	O
is	O
therefore	O
gaussian	O
.	O
this	O
corresponds	O
to	O
a	O
simple	O
two-node	O
graph	O
in	O
which	O
the	O
node	B
representing	O
µ	O
is	O
the	O
parent	O
of	O
the	O
node	B
representing	O
x.	O
the	O
mean	B
of	O
the	O
distribution	O
over	O
µ	O
is	O
a	O
parameter	O
controlling	O
a	O
prior	B
,	O
and	O
so	O
it	O
can	O
be	O
viewed	O
as	O
a	O
hyperparameter	B
.	O
because	O
the	O
value	O
of	O
this	O
hyperparameter	B
may	O
itself	O
be	O
unknown	O
,	O
we	O
can	O
again	O
treat	O
it	O
from	O
a	O
bayesian	O
perspective	O
by	O
introducing	O
a	O
prior	B
over	O
the	O
hyperparameter	B
,	O
sometimes	O
called	O
a	O
hyperprior	B
,	O
which	O
is	O
again	O
given	O
by	O
a	O
gaussian	O
distribution	O
.	O
this	O
type	O
of	O
construction	O
can	O
be	O
extended	B
in	O
principle	O
to	O
any	O
level	O
and	O
is	O
an	O
illustration	O
of	O
a	O
hierarchical	B
bayesian	O
model	O
,	O
of	O
which	O
we	O
shall	O
encounter	O
further	O
examples	O
in	O
later	O
chapters	O
.	O
8.2.	O
conditional	B
independence	I
an	O
important	O
concept	O
for	O
probability	O
distributions	O
over	O
multiple	O
variables	O
is	O
that	O
of	O
conditional	B
independence	I
(	O
dawid	O
,	O
1980	O
)	O
.	O
consider	O
three	O
variables	O
a	O
,	O
b	O
,	O
and	O
c	O
,	O
and	O
suppose	O
that	O
the	O
conditional	B
distribution	O
of	O
a	O
,	O
given	O
b	O
and	O
c	O
,	O
is	O
such	O
that	O
it	O
does	O
not	O
depend	O
on	O
the	O
value	O
of	O
b	O
,	O
so	O
that	O
p	O
(	O
a|b	O
,	O
c	O
)	O
=	O
p	O
(	O
a|c	O
)	O
.	O
(	O
8.20	O
)	O
we	O
say	O
that	O
a	O
is	O
conditionally	O
independent	B
of	O
b	O
given	O
c.	O
this	O
can	O
be	O
expressed	O
in	O
a	O
slightly	O
different	O
way	O
if	O
we	O
consider	O
the	O
joint	O
distribution	O
of	O
a	O
and	O
b	O
conditioned	O
on	O
c	O
,	O
which	O
we	O
can	O
write	O
in	O
the	O
form	O
p	O
(	O
a	O
,	O
b|c	O
)	O
=	O
p	O
(	O
a|b	O
,	O
c	O
)	O
p	O
(	O
b|c	O
)	O
=	O
p	O
(	O
a|c	O
)	O
p	O
(	O
b|c	O
)	O
.	O
(	O
8.21	O
)	O
where	O
we	O
have	O
used	O
the	O
product	B
rule	I
of	I
probability	I
together	O
with	O
(	O
8.20	O
)	O
.	O
thus	O
we	O
see	O
that	O
,	O
conditioned	O
on	O
c	O
,	O
the	O
joint	O
distribution	O
of	O
a	O
and	O
b	O
factorizes	O
into	O
the	O
prod-	O
uct	O
of	O
the	O
marginal	B
distribution	O
of	O
a	O
and	O
the	O
marginal	B
distribution	O
of	O
b	O
(	O
again	O
both	O
conditioned	O
on	O
c	O
)	O
.	O
this	O
says	O
that	O
the	O
variables	O
a	O
and	O
b	O
are	O
statistically	O
independent	B
,	O
given	O
c.	O
note	O
that	O
our	O
deﬁnition	O
of	O
conditional	B
independence	I
will	O
require	O
that	O
(	O
8.20	O
)	O
,	O
8.2.	O
conditional	B
independence	I
373	O
figure	O
8.15	O
the	O
ﬁrst	O
of	O
three	O
examples	O
of	O
graphs	O
over	O
three	O
variables	O
a	O
,	O
b	O
,	O
and	O
c	O
used	O
to	O
discuss	O
conditional	B
independence	I
properties	O
of	O
directed	B
graphical	O
models	O
.	O
c	O
a	O
b	O
or	O
equivalently	O
(	O
8.21	O
)	O
,	O
must	O
hold	O
for	O
every	O
possible	O
value	O
of	O
c	O
,	O
and	O
not	O
just	O
for	O
some	O
values	O
.	O
we	O
shall	O
sometimes	O
use	O
a	O
shorthand	O
notation	O
for	O
conditional	O
independence	O
(	O
dawid	O
,	O
1979	O
)	O
in	O
which	O
a	O
⊥⊥	O
b	O
|	O
c	O
(	O
8.22	O
)	O
denotes	O
that	O
a	O
is	O
conditionally	O
independent	B
of	O
b	O
given	O
c	O
and	O
is	O
equivalent	O
to	O
(	O
8.20	O
)	O
.	O
conditional	B
independence	I
properties	O
play	O
an	O
important	O
role	O
in	O
using	O
probabilis-	O
tic	O
models	O
for	O
pattern	O
recognition	O
by	O
simplifying	O
both	O
the	O
structure	O
of	O
a	O
model	O
and	O
the	O
computations	O
needed	O
to	O
perform	O
inference	B
and	O
learning	B
under	O
that	O
model	O
.	O
we	O
shall	O
see	O
examples	O
of	O
this	O
shortly	O
.	O
if	O
we	O
are	O
given	O
an	O
expression	O
for	O
the	O
joint	O
distribution	O
over	O
a	O
set	O
of	O
variables	O
in	O
terms	O
of	O
a	O
product	O
of	O
conditional	B
distributions	O
(	O
i.e.	O
,	O
the	O
mathematical	O
representation	O
underlying	O
a	O
directed	B
graph	O
)	O
,	O
then	O
we	O
could	O
in	O
principle	O
test	O
whether	O
any	O
poten-	O
tial	O
conditional	B
independence	I
property	O
holds	O
by	O
repeated	O
application	O
of	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
.	O
in	O
practice	O
,	O
such	O
an	O
approach	O
would	O
be	O
very	O
time	O
con-	O
suming	O
.	O
an	O
important	O
and	O
elegant	O
feature	O
of	O
graphical	O
models	O
is	O
that	O
conditional	B
independence	I
properties	O
of	O
the	O
joint	O
distribution	O
can	O
be	O
read	O
directly	O
from	O
the	O
graph	O
without	O
having	O
to	O
perform	O
any	O
analytical	O
manipulations	O
.	O
the	O
general	O
framework	O
for	O
achieving	O
this	O
is	O
called	O
d-separation	B
,	O
where	O
the	O
‘	O
d	O
’	O
stands	O
for	O
‘	O
directed	B
’	O
(	O
pearl	O
,	O
1988	O
)	O
.	O
here	O
we	O
shall	O
motivate	O
the	O
concept	O
of	O
d-separation	B
and	O
give	O
a	O
general	O
state-	O
ment	O
of	O
the	O
d-separation	B
criterion	O
.	O
a	O
formal	O
proof	O
can	O
be	O
found	O
in	O
lauritzen	O
(	O
1996	O
)	O
.	O
8.2.1	O
three	O
example	O
graphs	O
we	O
begin	O
our	O
discussion	O
of	O
the	O
conditional	B
independence	I
properties	O
of	O
directed	B
graphs	O
by	O
considering	O
three	O
simple	O
examples	O
each	O
involving	O
graphs	O
having	O
just	O
three	O
nodes	O
.	O
together	O
,	O
these	O
will	O
motivate	O
and	O
illustrate	O
the	O
key	O
concepts	O
of	O
d-separation	B
.	O
the	O
ﬁrst	O
of	O
the	O
three	O
examples	O
is	O
shown	O
in	O
figure	O
8.15	O
,	O
and	O
the	O
joint	O
distribution	O
corresponding	O
to	O
this	O
graph	O
is	O
easily	O
written	O
down	O
using	O
the	O
general	O
result	O
(	O
8.5	O
)	O
to	O
give	O
(	O
8.23	O
)	O
if	O
none	O
of	O
the	O
variables	O
are	O
observed	O
,	O
then	O
we	O
can	O
investigate	O
whether	O
a	O
and	O
b	O
are	O
independent	B
by	O
marginalizing	O
both	O
sides	O
of	O
(	O
8.23	O
)	O
with	O
respect	O
to	O
c	O
to	O
give	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
p	O
(	O
a|c	O
)	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c	O
)	O
.	O
(	O
cid:2	O
)	O
p	O
(	O
a	O
,	O
b	O
)	O
=	O
p	O
(	O
a|c	O
)	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c	O
)	O
.	O
c	O
in	O
general	O
,	O
this	O
does	O
not	O
factorize	O
into	O
the	O
product	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
,	O
and	O
so	O
a	O
(	O
cid:9	O
)	O
⊥⊥	O
b	O
|	O
∅	O
(	O
8.24	O
)	O
(	O
8.25	O
)	O
374	O
8.	O
graphical	O
models	O
figure	O
8.16	O
as	O
in	O
figure	O
8.15	O
but	O
where	O
we	O
have	O
conditioned	O
on	O
the	O
value	O
of	O
variable	O
c.	O
c	O
a	O
b	O
where	O
∅	O
denotes	O
the	O
empty	O
set	O
,	O
and	O
the	O
symbol	O
(	O
cid:9	O
)	O
⊥⊥	O
means	O
that	O
the	O
conditional	B
inde-	O
pendence	O
property	O
does	O
not	O
hold	O
in	O
general	O
.	O
of	O
course	O
,	O
it	O
may	O
hold	O
for	O
a	O
particular	O
distribution	O
by	O
virtue	O
of	O
the	O
speciﬁc	O
numerical	O
values	O
associated	O
with	O
the	O
various	O
conditional	B
probabilities	O
,	O
but	O
it	O
does	O
not	O
follow	O
in	O
general	O
from	O
the	O
structure	O
of	O
the	O
graph	O
.	O
now	O
suppose	O
we	O
condition	O
on	O
the	O
variable	O
c	O
,	O
as	O
represented	O
by	O
the	O
graph	O
of	O
figure	O
8.16.	O
from	O
(	O
8.23	O
)	O
,	O
we	O
can	O
easily	O
write	O
down	O
the	O
conditional	B
distribution	O
of	O
a	O
and	O
b	O
,	O
given	O
c	O
,	O
in	O
the	O
form	O
p	O
(	O
a	O
,	O
b|c	O
)	O
=	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
p	O
(	O
c	O
)	O
=	O
p	O
(	O
a|c	O
)	O
p	O
(	O
b|c	O
)	O
and	O
so	O
we	O
obtain	O
the	O
conditional	B
independence	I
property	O
a	O
⊥⊥	O
b	O
|	O
c.	O
we	O
can	O
provide	O
a	O
simple	O
graphical	O
interpretation	O
of	O
this	O
result	O
by	O
considering	O
the	O
path	O
from	O
node	B
a	O
to	O
node	B
b	O
via	O
c.	O
the	O
node	B
c	O
is	O
said	O
to	O
be	O
tail-to-tail	O
with	O
re-	O
spect	O
to	O
this	O
path	O
because	O
the	O
node	B
is	O
connected	O
to	O
the	O
tails	O
of	O
the	O
two	O
arrows	O
,	O
and	O
the	O
presence	O
of	O
such	O
a	O
path	O
connecting	O
nodes	O
a	O
and	O
b	O
causes	O
these	O
nodes	O
to	O
be	O
de-	O
pendent	O
.	O
however	O
,	O
when	O
we	O
condition	O
on	O
node	B
c	O
,	O
as	O
in	O
figure	O
8.16	O
,	O
the	O
conditioned	O
node	B
‘	O
blocks	O
’	O
the	O
path	O
from	O
a	O
to	O
b	O
and	O
causes	O
a	O
and	O
b	O
to	O
become	O
(	O
conditionally	O
)	O
independent	B
.	O
we	O
can	O
similarly	O
consider	O
the	O
graph	O
shown	O
in	O
figure	O
8.17.	O
the	O
joint	O
distribution	O
corresponding	O
to	O
this	O
graph	O
is	O
again	O
obtained	O
from	O
our	O
general	O
formula	O
(	O
8.5	O
)	O
to	O
give	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
c|a	O
)	O
p	O
(	O
b|c	O
)	O
.	O
(	O
8.26	O
)	O
first	O
of	O
all	O
,	O
suppose	O
that	O
none	O
of	O
the	O
variables	O
are	O
observed	O
.	O
again	O
,	O
we	O
can	O
test	O
to	O
see	O
if	O
a	O
and	O
b	O
are	O
independent	B
by	O
marginalizing	O
over	O
c	O
to	O
give	O
(	O
cid:2	O
)	O
p	O
(	O
a	O
,	O
b	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
c|a	O
)	O
p	O
(	O
b|c	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b|a	O
)	O
.	O
c	O
figure	O
8.17	O
the	O
second	O
of	O
our	O
three	O
examples	O
of	O
3-node	O
graphs	O
used	O
to	O
motivate	O
the	O
conditional	B
indepen-	O
dence	O
framework	O
for	O
directed	O
graphical	O
models	O
.	O
a	O
c	O
b	O
figure	O
8.18	O
as	O
in	O
figure	O
8.17	O
but	O
now	O
conditioning	O
on	O
node	B
c.	O
a	O
c	O
8.2.	O
conditional	B
independence	I
which	O
in	O
general	O
does	O
not	O
factorize	O
into	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
,	O
and	O
so	O
a	O
(	O
cid:9	O
)	O
⊥⊥	O
b	O
|	O
∅	O
375	O
b	O
(	O
8.27	O
)	O
as	O
before	O
.	O
now	O
suppose	O
we	O
condition	O
on	O
node	B
c	O
,	O
as	O
shown	O
in	O
figure	O
8.18.	O
using	O
bayes	O
’	O
theorem	O
,	O
together	O
with	O
(	O
8.26	O
)	O
,	O
we	O
obtain	O
p	O
(	O
a	O
,	O
b|c	O
)	O
=	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
p	O
(	O
c	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
c|a	O
)	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c	O
)	O
=	O
p	O
(	O
a|c	O
)	O
p	O
(	O
b|c	O
)	O
and	O
so	O
again	O
we	O
obtain	O
the	O
conditional	B
independence	I
property	O
a	O
⊥⊥	O
b	O
|	O
c.	O
as	O
before	O
,	O
we	O
can	O
interpret	O
these	O
results	O
graphically	O
.	O
the	O
node	B
c	O
is	O
said	O
to	O
be	O
head-to-tail	O
with	O
respect	O
to	O
the	O
path	O
from	O
node	B
a	O
to	O
node	B
b.	O
such	O
a	O
path	O
connects	O
nodes	O
a	O
and	O
b	O
and	O
renders	O
them	O
dependent	O
.	O
if	O
we	O
now	O
observe	O
c	O
,	O
as	O
in	O
figure	O
8.18	O
,	O
then	O
this	O
observation	O
‘	O
blocks	O
’	O
the	O
path	O
from	O
a	O
to	O
b	O
and	O
so	O
we	O
obtain	O
the	O
conditional	B
independence	I
property	O
a	O
⊥⊥	O
b	O
|	O
c.	O
finally	O
,	O
we	O
consider	O
the	O
third	O
of	O
our	O
3-node	O
examples	O
,	O
shown	O
by	O
the	O
graph	O
in	O
figure	O
8.19.	O
as	O
we	O
shall	O
see	O
,	O
this	O
has	O
a	O
more	O
subtle	O
behaviour	O
than	O
the	O
two	O
previous	O
graphs	O
.	O
the	O
joint	O
distribution	O
can	O
again	O
be	O
written	O
down	O
using	O
our	O
general	O
result	O
(	O
8.5	O
)	O
to	O
give	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
p	O
(	O
c|a	O
,	O
b	O
)	O
.	O
(	O
8.28	O
)	O
consider	O
ﬁrst	O
the	O
case	O
where	O
none	O
of	O
the	O
variables	O
are	O
observed	O
.	O
marginalizing	O
both	O
sides	O
of	O
(	O
8.28	O
)	O
over	O
c	O
we	O
obtain	O
p	O
(	O
a	O
,	O
b	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
figure	O
8.19	O
the	O
last	O
of	O
our	O
three	O
examples	O
of	O
3-node	O
graphs	O
used	O
to	O
explore	O
conditional	B
independence	I
properties	O
in	O
graphi-	O
cal	O
models	O
.	O
this	O
graph	O
has	O
rather	O
different	O
properties	O
from	O
the	O
two	O
previous	O
examples	O
.	O
a	O
b	O
c	O
376	O
8.	O
graphical	O
models	O
figure	O
8.20	O
as	O
in	O
figure	O
8.19	O
but	O
conditioning	O
on	O
the	O
value	O
of	O
node	B
c.	O
in	O
this	O
graph	O
,	O
the	O
act	O
of	O
conditioning	O
induces	O
a	O
depen-	O
dence	O
between	O
a	O
and	O
b.	O
a	O
b	O
c	O
and	O
so	O
a	O
and	O
b	O
are	O
independent	B
with	O
no	O
variables	O
observed	O
,	O
in	O
contrast	O
to	O
the	O
two	O
previous	O
examples	O
.	O
we	O
can	O
write	O
this	O
result	O
as	O
a	O
⊥⊥	O
b	O
|	O
∅	O
.	O
(	O
8.29	O
)	O
now	O
suppose	O
we	O
condition	O
on	O
c	O
,	O
as	O
indicated	O
in	O
figure	O
8.20.	O
the	O
conditional	B
distri-	O
bution	O
of	O
a	O
and	O
b	O
is	O
then	O
given	O
by	O
p	O
(	O
a	O
,	O
b|c	O
)	O
=	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
p	O
(	O
c	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
p	O
(	O
c|a	O
,	O
b	O
)	O
p	O
(	O
c	O
)	O
exercise	O
8.10	O
which	O
in	O
general	O
does	O
not	O
factorize	O
into	O
the	O
product	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
,	O
and	O
so	O
a	O
(	O
cid:9	O
)	O
⊥⊥	O
b	O
|	O
c.	O
thus	O
our	O
third	O
example	O
has	O
the	O
opposite	O
behaviour	O
from	O
the	O
ﬁrst	O
two	O
.	O
graphically	O
,	O
we	O
say	O
that	O
node	B
c	O
is	O
head-to-head	O
with	O
respect	O
to	O
the	O
path	O
from	O
a	O
to	O
b	O
because	O
it	O
connects	O
to	O
the	O
heads	O
of	O
the	O
two	O
arrows	O
.	O
when	O
node	B
c	O
is	O
unobserved	O
,	O
it	O
‘	O
blocks	O
’	O
the	O
path	O
,	O
and	O
the	O
variables	O
a	O
and	O
b	O
are	O
independent	B
.	O
however	O
,	O
conditioning	O
on	O
c	O
‘	O
unblocks	O
’	O
the	O
path	O
and	O
renders	O
a	O
and	O
b	O
dependent	O
.	O
there	O
is	O
one	O
more	O
subtlety	O
associated	O
with	O
this	O
third	O
example	O
that	O
we	O
need	O
to	O
consider	O
.	O
first	O
we	O
introduce	O
some	O
more	O
terminology	O
.	O
we	O
say	O
that	O
node	B
y	O
is	O
a	O
de-	O
scendant	O
of	O
node	B
x	O
if	O
there	O
is	O
a	O
path	O
from	O
x	O
to	O
y	O
in	O
which	O
each	O
step	O
of	O
the	O
path	O
follows	O
the	O
directions	O
of	O
the	O
arrows	O
.	O
then	O
it	O
can	O
be	O
shown	O
that	O
a	O
head-to-head	B
path	I
will	O
become	O
unblocked	O
if	O
either	O
the	O
node	B
,	O
or	O
any	O
of	O
its	O
descendants	O
,	O
is	O
observed	O
.	O
in	O
summary	O
,	O
a	O
tail-to-tail	O
node	O
or	O
a	O
head-to-tail	O
node	O
leaves	O
a	O
path	O
unblocked	O
unless	O
it	O
is	O
observed	O
in	O
which	O
case	O
it	O
blocks	O
the	O
path	O
.	O
by	O
contrast	O
,	O
a	O
head-to-head	O
node	O
blocks	O
a	O
path	O
if	O
it	O
is	O
unobserved	O
,	O
but	O
once	O
the	O
node	B
,	O
and/or	O
at	O
least	O
one	O
of	O
its	O
descendants	O
,	O
is	O
observed	O
the	O
path	O
becomes	O
unblocked	O
.	O
it	O
is	O
worth	O
spending	O
a	O
moment	O
to	O
understand	O
further	O
the	O
unusual	O
behaviour	O
of	O
the	O
graph	O
of	O
figure	O
8.20.	O
consider	O
a	O
particular	O
instance	O
of	O
such	O
a	O
graph	O
corresponding	O
to	O
a	O
problem	O
with	O
three	O
binary	O
random	O
variables	O
relating	O
to	O
the	O
fuel	B
system	I
on	O
a	O
car	O
,	O
as	O
shown	O
in	O
figure	O
8.21.	O
the	O
variables	O
are	O
called	O
b	O
,	O
representing	O
the	O
state	O
of	O
a	O
battery	O
that	O
is	O
either	O
charged	O
(	O
b	O
=	O
1	O
)	O
or	O
ﬂat	O
(	O
b	O
=	O
0	O
)	O
,	O
f	O
representing	O
the	O
state	O
of	O
the	O
fuel	O
tank	O
that	O
is	O
either	O
full	O
of	O
fuel	O
(	O
f	O
=	O
1	O
)	O
or	O
empty	O
(	O
f	O
=	O
0	O
)	O
,	O
and	O
g	O
,	O
which	O
is	O
the	O
state	O
of	O
an	O
electric	O
fuel	O
gauge	O
and	O
which	O
indicates	O
either	O
full	O
(	O
g	O
=	O
1	O
)	O
or	O
empty	O
b	O
f	O
b	O
f	O
b	O
f	O
8.2.	O
conditional	B
independence	I
377	O
g	O
g	O
g	O
figure	O
8.21	O
an	O
example	O
of	O
a	O
3-node	O
graph	O
used	O
to	O
illustrate	O
the	O
phenomenon	O
of	O
‘	O
explaining	B
away	I
’	O
.	O
the	O
three	O
nodes	O
represent	O
the	O
state	O
of	O
the	O
battery	O
(	O
b	O
)	O
,	O
the	O
state	O
of	O
the	O
fuel	O
tank	O
(	O
f	O
)	O
and	O
the	O
reading	O
on	O
the	O
electric	O
fuel	O
gauge	O
(	O
g	O
)	O
.	O
see	O
the	O
text	O
for	O
details	O
.	O
(	O
g	O
=	O
0	O
)	O
.	O
the	O
battery	O
is	O
either	O
charged	O
or	O
ﬂat	O
,	O
and	O
independently	O
the	O
fuel	O
tank	O
is	O
either	O
full	O
or	O
empty	O
,	O
with	O
prior	B
probabilities	O
p	O
(	O
b	O
=	O
1	O
)	O
=	O
0.9	O
p	O
(	O
f	O
=	O
1	O
)	O
=	O
0.9.	O
given	O
the	O
state	O
of	O
the	O
fuel	O
tank	O
and	O
the	O
battery	O
,	O
the	O
fuel	O
gauge	O
reads	O
full	O
with	O
proba-	O
bilities	O
given	O
by	O
p	O
(	O
g	O
=	O
1|b	O
=	O
1	O
,	O
f	O
=	O
1	O
)	O
=	O
0.8	O
p	O
(	O
g	O
=	O
1|b	O
=	O
1	O
,	O
f	O
=	O
0	O
)	O
=	O
0.2	O
p	O
(	O
g	O
=	O
1|b	O
=	O
0	O
,	O
f	O
=	O
1	O
)	O
=	O
0.2	O
p	O
(	O
g	O
=	O
1|b	O
=	O
0	O
,	O
f	O
=	O
0	O
)	O
=	O
0.1	O
so	O
this	O
is	O
a	O
rather	O
unreliable	O
fuel	O
gauge	O
!	O
all	O
remaining	O
probabilities	O
are	O
determined	O
by	O
the	O
requirement	O
that	O
probabilities	O
sum	O
to	O
one	O
,	O
and	O
so	O
we	O
have	O
a	O
complete	O
speciﬁ-	O
cation	O
of	O
the	O
probabilistic	O
model	O
.	O
before	O
we	O
observe	O
any	O
data	O
,	O
the	O
prior	B
probability	O
of	O
the	O
fuel	O
tank	O
being	O
empty	O
is	O
p	O
(	O
f	O
=	O
0	O
)	O
=	O
0.1.	O
now	O
suppose	O
that	O
we	O
observe	O
the	O
fuel	O
gauge	O
and	O
discover	O
that	O
it	O
reads	O
empty	O
,	O
i.e.	O
,	O
g	O
=	O
0	O
,	O
corresponding	O
to	O
the	O
middle	O
graph	O
in	O
figure	O
8.21.	O
we	O
can	O
use	O
bayes	O
’	O
theorem	O
to	O
evaluate	O
the	O
posterior	B
probability	I
of	O
the	O
fuel	O
tank	O
being	O
empty	O
.	O
first	O
we	O
evaluate	O
the	O
denominator	O
for	O
bayes	O
’	O
theorem	O
given	O
by	O
p	O
(	O
g	O
=	O
0|b	O
,	O
f	O
)	O
p	O
(	O
b	O
)	O
p	O
(	O
f	O
)	O
=	O
0.315	O
p	O
(	O
g	O
=	O
0	O
)	O
=	O
(	O
cid:2	O
)	O
(	O
8.30	O
)	O
b∈	O
{	O
0,1	O
}	O
f∈	O
{	O
0,1	O
}	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
b∈	O
{	O
0,1	O
}	O
and	O
similarly	O
we	O
evaluate	O
p	O
(	O
g	O
=	O
0|f	O
=	O
0	O
)	O
=	O
p	O
(	O
g	O
=	O
0|b	O
,	O
f	O
=	O
0	O
)	O
p	O
(	O
b	O
)	O
=	O
0.81	O
(	O
8.31	O
)	O
and	O
using	O
these	O
results	O
we	O
have	O
p	O
(	O
f	O
=	O
0|g	O
=	O
0	O
)	O
=	O
p	O
(	O
g	O
=	O
0|f	O
=	O
0	O
)	O
p	O
(	O
f	O
=	O
0	O
)	O
p	O
(	O
g	O
=	O
0	O
)	O
(	O
cid:7	O
)	O
0.257	O
(	O
8.32	O
)	O
378	O
8.	O
graphical	O
models	O
and	O
so	O
p	O
(	O
f	O
=	O
0|g	O
=	O
0	O
)	O
>	O
p	O
(	O
f	O
=	O
0	O
)	O
.	O
thus	O
observing	O
that	O
the	O
gauge	O
reads	O
empty	O
makes	O
it	O
more	O
likely	O
that	O
the	O
tank	O
is	O
indeed	O
empty	O
,	O
as	O
we	O
would	O
intuitively	O
expect	O
.	O
next	O
suppose	O
that	O
we	O
also	O
check	O
the	O
state	O
of	O
the	O
battery	O
and	O
ﬁnd	O
that	O
it	O
is	O
ﬂat	O
,	O
i.e.	O
,	O
b	O
=	O
0.	O
we	O
have	O
now	O
observed	O
the	O
states	O
of	O
both	O
the	O
fuel	O
gauge	O
and	O
the	O
battery	O
,	O
as	O
shown	O
by	O
the	O
right-hand	O
graph	O
in	O
figure	O
8.21.	O
the	O
posterior	B
probability	I
that	O
the	O
fuel	O
tank	O
is	O
empty	O
given	O
the	O
observations	O
of	O
both	O
the	O
fuel	O
gauge	O
and	O
the	O
battery	O
state	O
is	O
then	O
given	O
by	O
(	O
cid:5	O
)	O
p	O
(	O
f	O
=	O
0|g	O
=	O
0	O
,	O
b	O
=	O
0	O
)	O
=	O
p	O
(	O
g	O
=	O
0|b	O
=	O
0	O
,	O
f	O
=	O
0	O
)	O
p	O
(	O
f	O
=	O
0	O
)	O
f∈	O
{	O
0,1	O
}	O
p	O
(	O
g	O
=	O
0|b	O
=	O
0	O
,	O
f	O
)	O
p	O
(	O
f	O
)	O
(	O
cid:7	O
)	O
0.111	O
(	O
8.33	O
)	O
where	O
the	O
prior	B
probability	O
p	O
(	O
b	O
=	O
0	O
)	O
has	O
cancelled	O
between	O
numerator	O
and	O
denom-	O
inator	O
.	O
thus	O
the	O
probability	B
that	O
the	O
tank	O
is	O
empty	O
has	O
decreased	O
(	O
from	O
0.257	O
to	O
0.111	O
)	O
as	O
a	O
result	O
of	O
the	O
observation	O
of	O
the	O
state	O
of	O
the	O
battery	O
.	O
this	O
accords	O
with	O
our	O
intuition	O
that	O
ﬁnding	O
out	O
that	O
the	O
battery	O
is	O
ﬂat	O
explains	O
away	O
the	O
observation	O
that	O
the	O
fuel	O
gauge	O
reads	O
empty	O
.	O
we	O
see	O
that	O
the	O
state	O
of	O
the	O
fuel	O
tank	O
and	O
that	O
of	O
the	O
battery	O
have	O
indeed	O
become	O
dependent	O
on	O
each	O
other	O
as	O
a	O
result	O
of	O
observing	O
the	O
reading	O
on	O
the	O
fuel	O
gauge	O
.	O
in	O
fact	O
,	O
this	O
would	O
also	O
be	O
the	O
case	O
if	O
,	O
instead	O
of	O
observing	O
the	O
fuel	O
gauge	O
directly	O
,	O
we	O
observed	O
the	O
state	O
of	O
some	O
descendant	O
of	O
g.	O
note	O
that	O
the	O
probability	B
p	O
(	O
f	O
=	O
0|g	O
=	O
0	O
,	O
b	O
=	O
0	O
)	O
(	O
cid:7	O
)	O
0.111	O
is	O
greater	O
than	O
the	O
prior	B
probability	O
p	O
(	O
f	O
=	O
0	O
)	O
=	O
0.1	O
because	O
the	O
observation	O
that	O
the	O
fuel	O
gauge	O
reads	O
zero	O
still	O
provides	O
some	O
evidence	O
in	O
favour	O
of	O
an	O
empty	O
fuel	O
tank	O
.	O
8.2.2	O
d-separation	B
we	O
now	O
give	O
a	O
general	O
statement	O
of	O
the	O
d-separation	B
property	O
(	O
pearl	O
,	O
1988	O
)	O
for	O
directed	O
graphs	O
.	O
consider	O
a	O
general	O
directed	B
graph	O
in	O
which	O
a	O
,	O
b	O
,	O
and	O
c	O
are	O
arbi-	O
trary	O
nonintersecting	O
sets	O
of	O
nodes	O
(	O
whose	O
union	O
may	O
be	O
smaller	O
than	O
the	O
complete	O
set	O
of	O
nodes	O
in	O
the	O
graph	O
)	O
.	O
we	O
wish	O
to	O
ascertain	O
whether	O
a	O
particular	O
conditional	B
independence	I
statement	O
a	O
⊥⊥	O
b	O
|	O
c	O
is	O
implied	O
by	O
a	O
given	O
directed	B
acyclic	I
graph	I
.	O
to	O
do	O
so	O
,	O
we	O
consider	O
all	O
possible	O
paths	O
from	O
any	O
node	B
in	O
a	O
to	O
any	O
node	B
in	O
b.	O
any	O
such	O
path	O
is	O
said	O
to	O
be	O
blocked	O
if	O
it	O
includes	O
a	O
node	B
such	O
that	O
either	O
(	O
a	O
)	O
the	O
arrows	O
on	O
the	O
path	O
meet	O
either	O
head-to-tail	O
or	O
tail-to-tail	O
at	O
the	O
node	B
,	O
and	O
the	O
node	B
is	O
in	O
the	O
set	O
c	O
,	O
or	O
(	O
b	O
)	O
the	O
arrows	O
meet	O
head-to-head	O
at	O
the	O
node	B
,	O
and	O
neither	O
the	O
node	B
,	O
nor	O
any	O
of	O
its	O
descendants	O
,	O
is	O
in	O
the	O
set	O
c.	O
if	O
all	O
paths	O
are	O
blocked	O
,	O
then	O
a	O
is	O
said	O
to	O
be	O
d-separated	O
from	O
b	O
by	O
c	O
,	O
and	O
the	O
joint	O
distribution	O
over	O
all	O
of	O
the	O
variables	O
in	O
the	O
graph	O
will	O
satisfy	O
a	O
⊥⊥	O
b	O
|	O
c.	O
the	O
concept	O
of	O
d-separation	B
is	O
illustrated	O
in	O
figure	O
8.22.	O
in	O
graph	O
(	O
a	O
)	O
,	O
the	O
path	O
from	O
a	O
to	O
b	O
is	O
not	O
blocked	O
by	O
node	B
f	O
because	O
it	O
is	O
a	O
tail-to-tail	O
node	O
for	O
this	O
path	O
and	O
is	O
not	O
observed	O
,	O
nor	O
is	O
it	O
blocked	O
by	O
node	B
e	O
because	O
,	O
although	O
the	O
latter	O
is	O
a	O
head-to-head	O
node	O
,	O
it	O
has	O
a	O
descendant	O
c	O
because	O
is	O
in	O
the	O
conditioning	O
set	O
.	O
thus	O
the	O
conditional	B
independence	I
statement	O
a	O
⊥⊥	O
b	O
|	O
c	O
does	O
not	O
follow	O
from	O
this	O
graph	O
.	O
in	O
graph	O
(	O
b	O
)	O
,	O
the	O
path	O
from	O
a	O
to	O
b	O
is	O
blocked	O
by	O
node	B
f	O
because	O
this	O
is	O
a	O
tail-to-tail	O
node	O
that	O
is	O
observed	O
,	O
and	O
so	O
the	O
conditional	B
independence	I
property	O
a	O
⊥⊥	O
b	O
|	O
f	O
will	O
figure	O
8.22	O
illustration	O
of	O
the	O
con-	O
cept	O
of	O
d-separation	B
.	O
see	O
the	O
text	O
for	O
details	O
.	O
a	O
f	O
a	O
f	O
8.2.	O
conditional	B
independence	I
379	O
b	O
e	O
c	O
b	O
e	O
c	O
(	O
a	O
)	O
(	O
b	O
)	O
be	O
satisﬁed	O
by	O
any	O
distribution	O
that	O
factorizes	O
according	O
to	O
this	O
graph	O
.	O
note	O
that	O
this	O
path	O
is	O
also	O
blocked	O
by	O
node	B
e	O
because	O
e	O
is	O
a	O
head-to-head	O
node	O
and	O
neither	O
it	O
nor	O
its	O
descendant	O
are	O
in	O
the	O
conditioning	O
set	O
.	O
for	O
the	O
purposes	O
of	O
d-separation	B
,	O
parameters	O
such	O
as	O
α	O
and	O
σ2	O
in	O
figure	O
8.5	O
,	O
indicated	O
by	O
small	O
ﬁlled	O
circles	O
,	O
behave	O
in	O
the	O
same	O
was	O
as	O
observed	O
nodes	O
.	O
how-	O
ever	O
,	O
there	O
are	O
no	O
marginal	B
distributions	O
associated	O
with	O
such	O
nodes	O
.	O
consequently	O
parameter	O
nodes	O
never	O
themselves	O
have	O
parents	O
and	O
so	O
all	O
paths	O
through	O
these	O
nodes	O
will	O
always	O
be	O
tail-to-tail	O
and	O
hence	O
blocked	O
.	O
consequently	O
they	O
play	O
no	O
role	O
in	O
d-separation	B
.	O
section	O
2.3	O
another	O
example	O
of	O
conditional	B
independence	I
and	O
d-separation	B
is	O
provided	O
by	O
the	O
concept	O
of	O
i.i.d	O
.	O
(	O
independent	B
identically	I
distributed	I
)	O
data	O
introduced	O
in	O
sec-	O
tion	O
1.2.4.	O
consider	O
the	O
problem	O
of	O
ﬁnding	O
the	O
posterior	O
distribution	O
for	O
the	O
mean	B
of	O
a	O
univariate	O
gaussian	O
distribution	O
.	O
this	O
can	O
be	O
represented	O
by	O
the	O
directed	B
graph	O
shown	O
in	O
figure	O
8.23	O
in	O
which	O
the	O
joint	O
distribution	O
is	O
deﬁned	O
by	O
a	O
prior	B
p	O
(	O
µ	O
)	O
to-	O
gether	O
with	O
a	O
set	O
of	O
conditional	B
distributions	O
p	O
(	O
xn|µ	O
)	O
for	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
in	O
practice	O
,	O
we	O
observe	O
d	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
and	O
our	O
goal	O
is	O
to	O
infer	O
µ.	O
suppose	O
,	O
for	O
a	O
moment	O
,	O
that	O
we	O
condition	O
on	O
µ	O
and	O
consider	O
the	O
joint	O
distribution	O
of	O
the	O
observations	O
.	O
using	O
d-separation	B
,	O
we	O
note	O
that	O
there	O
is	O
a	O
unique	O
path	O
from	O
any	O
xi	O
to	O
any	O
other	O
xj	O
(	O
cid:9	O
)	O
=i	O
and	O
that	O
this	O
path	O
is	O
tail-to-tail	O
with	O
respect	O
to	O
the	O
observed	O
node	O
µ.	O
every	O
such	O
path	O
is	O
blocked	O
and	O
so	O
the	O
observations	O
d	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
are	O
independent	B
given	O
µ	O
,	O
so	O
that	O
figure	O
8.23	O
(	O
a	O
)	O
directed	B
graph	O
corre-	O
sponding	O
to	O
the	O
problem	O
of	O
inferring	O
the	O
mean	B
µ	O
of	O
a	O
univariate	O
gaussian	O
dis-	O
tribution	O
from	O
observations	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
.	O
(	O
b	O
)	O
the	O
same	O
graph	O
drawn	O
using	O
the	O
plate	B
notation	O
.	O
n	O
(	O
cid:14	O
)	O
n=1	O
µ	O
p	O
(	O
d|µ	O
)	O
=	O
p	O
(	O
xn|µ	O
)	O
.	O
(	O
8.34	O
)	O
µ	O
x1	O
xn	O
xn	O
(	O
a	O
)	O
n	O
(	O
b	O
)	O
n	O
380	O
8.	O
graphical	O
models	O
figure	O
8.24	O
a	O
graphical	O
representation	O
of	O
the	O
‘	O
naive	O
bayes	O
’	O
conditioned	O
on	O
the	O
model	O
class	O
label	O
z	O
,	O
the	O
components	O
of	O
the	O
observed	O
vector	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
t	O
are	O
assumed	O
to	O
be	O
independent	B
.	O
for	O
classiﬁcation	O
.	O
z	O
x1	O
xd	O
however	O
,	O
if	O
we	O
integrate	O
over	O
µ	O
,	O
the	O
observations	O
are	O
in	O
general	O
no	O
longer	O
indepen-	O
dent	O
p	O
(	O
d	O
)	O
=	O
p	O
(	O
d|µ	O
)	O
p	O
(	O
µ	O
)	O
dµ	O
(	O
cid:9	O
)	O
=	O
0	O
n=1	O
p	O
(	O
xn	O
)	O
.	O
(	O
8.35	O
)	O
(	O
cid:6	O
)	O
∞	O
n	O
(	O
cid:14	O
)	O
section	O
3.3	O
independence	O
property	O
here	O
µ	O
is	O
a	O
latent	B
variable	I
,	O
because	O
its	O
value	O
is	O
not	O
observed	O
.	O
another	O
example	O
of	O
a	O
model	O
representing	O
i.i.d	O
.	O
data	O
is	O
the	O
graph	O
in	O
figure	O
8.7	O
corresponding	O
to	O
bayesian	O
polynomial	O
regression	O
.	O
here	O
the	O
stochastic	B
nodes	O
corre-	O
spond	O
to	O
{	O
tn	O
}	O
,	O
w	O
and	O
(	O
cid:1	O
)	O
t.	O
we	O
see	O
that	O
the	O
node	B
for	O
w	O
is	O
tail-to-tail	O
with	O
respect	O
to	O
the	O
path	O
from	O
(	O
cid:1	O
)	O
t	O
to	O
any	O
one	O
of	O
the	O
nodes	O
tn	O
and	O
so	O
we	O
have	O
the	O
following	O
conditional	B
(	O
cid:1	O
)	O
t	O
⊥⊥	O
tn	O
|	O
w.	O
(	O
cid:1	O
)	O
t	O
is	O
independent	B
of	O
the	O
training	B
data	O
{	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
}	O
.	O
we	O
can	O
therefore	O
ﬁrst	O
use	O
the	O
predictions	O
of	O
(	O
cid:1	O
)	O
t	O
for	O
new	O
input	O
observations	O
(	O
cid:1	O
)	O
x	O
.	O
(	O
8.36	O
)	O
thus	O
,	O
conditioned	O
on	O
the	O
polynomial	O
coefﬁcients	O
w	O
,	O
the	O
predictive	B
distribution	I
for	O
training	B
data	O
to	O
determine	O
the	O
posterior	O
distribution	O
over	O
the	O
coefﬁcients	O
w	O
and	O
then	O
we	O
can	O
discard	O
the	O
training	B
data	O
and	O
use	O
the	O
posterior	O
distribution	O
for	O
w	O
to	O
make	O
a	O
related	O
graphical	O
structure	O
arises	O
in	O
an	O
approach	O
to	O
classiﬁcation	B
called	O
the	O
naive	O
bayes	O
model	O
,	O
in	O
which	O
we	O
use	O
conditional	B
independence	I
assumptions	O
to	O
sim-	O
plify	O
the	O
model	O
structure	O
.	O
suppose	O
our	O
observed	B
variable	I
consists	O
of	O
a	O
d-dimensional	O
vector	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
t	O
,	O
and	O
we	O
wish	O
to	O
assign	O
observed	O
values	O
of	O
x	O
to	O
one	O
of	O
k	O
classes	O
.	O
using	O
the	O
1-of-k	O
encoding	O
scheme	O
,	O
we	O
can	O
represent	O
these	O
classes	O
by	O
a	O
k-	O
dimensional	O
binary	O
vector	O
z.	O
we	O
can	O
then	O
deﬁne	O
a	O
generative	B
model	I
by	O
introducing	O
a	O
multinomial	O
prior	O
p	O
(	O
z|µ	O
)	O
over	O
the	O
class	O
labels	O
,	O
where	O
the	O
kth	O
component	O
µk	O
of	O
µ	O
is	O
the	O
prior	B
probability	O
of	O
class	O
ck	O
,	O
together	O
with	O
a	O
conditional	B
distribution	O
p	O
(	O
x|z	O
)	O
for	O
the	O
observed	O
vector	O
x.	O
the	O
key	O
assumption	O
of	O
the	O
naive	O
bayes	O
model	O
is	O
that	O
,	O
conditioned	O
on	O
the	O
class	O
z	O
,	O
the	O
distributions	O
of	O
the	O
input	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
are	O
in-	O
dependent	O
.	O
the	O
graphical	O
representation	O
of	O
this	O
model	O
is	O
shown	O
in	O
figure	O
8.24.	O
we	O
see	O
that	O
observation	O
of	O
z	O
blocks	O
the	O
path	O
between	O
xi	O
and	O
xj	O
for	O
j	O
(	O
cid:9	O
)	O
=	O
i	O
(	O
because	O
such	O
paths	O
are	O
tail-to-tail	O
at	O
the	O
node	B
z	O
)	O
and	O
so	O
xi	O
and	O
xj	O
are	O
conditionally	O
independent	B
given	O
z.	O
if	O
,	O
however	O
,	O
we	O
marginalize	O
out	O
z	O
(	O
so	O
that	O
z	O
is	O
unobserved	O
)	O
the	O
tail-to-tail	B
path	I
from	O
xi	O
to	O
xj	O
is	O
no	O
longer	O
blocked	O
.	O
this	O
tells	O
us	O
that	O
in	O
general	O
the	O
marginal	B
density	O
p	O
(	O
x	O
)	O
will	O
not	O
factorize	O
with	O
respect	O
to	O
the	O
components	O
of	O
x.	O
we	O
encountered	O
a	O
simple	O
application	O
of	O
the	O
naive	O
bayes	O
model	O
in	O
the	O
context	O
of	O
fusing	O
data	O
from	O
different	O
sources	O
for	O
medical	O
diagnosis	O
in	O
section	O
1.5.	O
if	O
we	O
are	O
given	O
a	O
labelled	O
training	B
set	I
,	O
comprising	O
inputs	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
together	O
with	O
their	O
class	O
labels	O
,	O
then	O
we	O
can	O
ﬁt	O
the	O
naive	O
bayes	O
model	O
to	O
the	O
training	B
data	O
8.2.	O
conditional	B
independence	I
381	O
using	O
maximum	B
likelihood	I
assuming	O
that	O
the	O
data	O
are	O
drawn	O
independently	O
from	O
the	O
model	O
.	O
the	O
solution	O
is	O
obtained	O
by	O
ﬁtting	O
the	O
model	O
for	O
each	O
class	O
separately	O
using	O
the	O
correspondingly	O
labelled	O
data	O
.	O
as	O
an	O
example	O
,	O
suppose	O
that	O
the	O
probability	B
density	O
within	O
each	O
class	O
is	O
chosen	O
to	O
be	O
gaussian	O
.	O
in	O
this	O
case	O
,	O
the	O
naive	O
bayes	O
assumption	O
then	O
implies	O
that	O
the	O
covariance	B
matrix	I
for	O
each	O
gaussian	O
is	O
diagonal	B
,	O
and	O
the	O
contours	O
of	O
constant	O
density	B
within	O
each	O
class	O
will	O
be	O
axis-aligned	O
ellipsoids	O
.	O
the	O
marginal	B
density	O
,	O
however	O
,	O
is	O
given	O
by	O
a	O
superposition	O
of	O
diagonal	B
gaussians	O
(	O
with	O
weighting	O
coefﬁcients	O
given	O
by	O
the	O
class	O
priors	O
)	O
and	O
so	O
will	O
no	O
longer	O
factorize	O
with	O
respect	O
to	O
its	O
components	O
.	O
the	O
naive	O
bayes	O
assumption	O
is	O
helpful	O
when	O
the	O
dimensionality	O
d	O
of	O
the	O
input	O
space	O
is	O
high	O
,	O
making	O
density	B
estimation	I
in	O
the	O
full	O
d-dimensional	O
space	O
more	O
chal-	O
lenging	O
.	O
it	O
is	O
also	O
useful	O
if	O
the	O
input	O
vector	O
contains	O
both	O
discrete	O
and	O
continuous	O
variables	O
,	O
since	O
each	O
can	O
be	O
represented	O
separately	O
using	O
appropriate	O
models	O
(	O
e.g.	O
,	O
bernoulli	O
distributions	O
for	O
binary	O
observations	O
or	O
gaussians	O
for	O
real-valued	O
vari-	O
ables	O
)	O
.	O
the	O
conditional	B
independence	I
assumption	O
of	O
this	O
model	O
is	O
clearly	O
a	O
strong	O
one	O
that	O
may	O
lead	O
to	O
rather	O
poor	O
representations	O
of	O
the	O
class-conditional	O
densities	O
.	O
nevertheless	O
,	O
even	O
if	O
this	O
assumption	O
is	O
not	O
precisely	O
satisﬁed	O
,	O
the	O
model	O
may	O
still	O
give	O
good	O
classiﬁcation	B
performance	O
in	O
practice	O
because	O
the	O
decision	O
boundaries	O
can	O
be	O
insensitive	O
to	O
some	O
of	O
the	O
details	O
in	O
the	O
class-conditional	O
densities	O
,	O
as	O
illustrated	O
in	O
figure	O
1.27.	O
we	O
have	O
seen	O
that	O
a	O
particular	O
directed	B
graph	O
represents	O
a	O
speciﬁc	O
decomposition	O
of	O
a	O
joint	O
probability	B
distribution	O
into	O
a	O
product	O
of	O
conditional	B
probabilities	O
.	O
the	O
graph	O
also	O
expresses	O
a	O
set	O
of	O
conditional	B
independence	I
statements	O
obtained	O
through	O
the	O
d-separation	B
criterion	O
,	O
and	O
the	O
d-separation	B
theorem	O
is	O
really	O
an	O
expression	O
of	O
the	O
equivalence	O
of	O
these	O
two	O
properties	O
.	O
in	O
order	O
to	O
make	O
this	O
clear	O
,	O
it	O
is	O
helpful	O
to	O
think	O
of	O
a	O
directed	B
graph	O
as	O
a	O
ﬁlter	O
.	O
suppose	O
we	O
consider	O
a	O
particular	O
joint	O
probability	B
distribution	O
p	O
(	O
x	O
)	O
over	O
the	O
variables	O
x	O
corresponding	O
to	O
the	O
(	O
nonobserved	O
)	O
nodes	O
of	O
the	O
graph	O
.	O
the	O
ﬁlter	O
will	O
allow	O
this	O
distribution	O
to	O
pass	O
through	O
if	O
,	O
and	O
only	O
if	O
,	O
it	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
factorization	B
(	O
8.5	O
)	O
implied	O
by	O
the	O
graph	O
.	O
if	O
we	O
present	O
to	O
the	O
ﬁlter	O
the	O
set	O
of	O
all	O
possible	O
distributions	O
p	O
(	O
x	O
)	O
over	O
the	O
set	O
of	O
variables	O
x	O
,	O
then	O
the	O
subset	O
of	O
distributions	O
that	O
are	O
passed	O
by	O
the	O
ﬁlter	O
will	O
be	O
denoted	O
df	O
,	O
for	O
directed	O
factorization	B
.	O
this	O
is	O
illustrated	O
in	O
figure	O
8.25.	O
alternatively	O
,	O
we	O
can	O
use	O
the	O
graph	O
as	O
a	O
different	O
kind	O
of	O
ﬁlter	O
by	O
ﬁrst	O
listing	O
all	O
of	O
the	O
conditional	B
independence	I
properties	O
obtained	O
by	O
applying	O
the	O
d-separation	B
criterion	O
to	O
the	O
graph	O
,	O
and	O
then	O
allowing	O
a	O
distribution	O
to	O
pass	O
only	O
if	O
it	O
satisﬁes	O
all	O
of	O
these	O
properties	O
.	O
if	O
we	O
present	O
all	O
possible	O
distributions	O
p	O
(	O
x	O
)	O
to	O
this	O
second	O
kind	O
of	O
ﬁlter	O
,	O
then	O
the	O
d-separation	B
theorem	O
tells	O
us	O
that	O
the	O
set	O
of	O
distributions	O
that	O
will	O
be	O
allowed	O
through	O
is	O
precisely	O
the	O
set	O
df	O
.	O
it	O
should	O
be	O
emphasized	O
that	O
the	O
conditional	B
independence	I
properties	O
obtained	O
from	O
d-separation	B
apply	O
to	O
any	O
probabilistic	O
model	O
described	O
by	O
that	O
particular	O
di-	O
rected	O
graph	O
.	O
this	O
will	O
be	O
true	O
,	O
for	O
instance	O
,	O
whether	O
the	O
variables	O
are	O
discrete	O
or	O
continuous	O
or	O
a	O
combination	O
of	O
these	O
.	O
again	O
,	O
we	O
see	O
that	O
a	O
particular	O
graph	O
is	O
de-	O
scribing	O
a	O
whole	O
family	O
of	O
probability	B
distributions	O
.	O
at	O
one	O
extreme	O
we	O
have	O
a	O
fully	B
connected	I
graph	O
that	O
exhibits	O
no	O
conditional	B
in-	O
dependence	O
properties	O
at	O
all	O
,	O
and	O
which	O
can	O
represent	O
any	O
possible	O
joint	O
probability	B
distribution	O
over	O
the	O
given	O
variables	O
.	O
the	O
set	O
df	O
will	O
contain	O
all	O
possible	O
distribu-	O
382	O
8.	O
graphical	O
models	O
p	O
(	O
x	O
)	O
df	O
figure	O
8.25	O
we	O
can	O
view	O
a	O
graphical	B
model	I
(	O
in	O
this	O
case	O
a	O
directed	B
graph	O
)	O
as	O
a	O
ﬁlter	O
in	O
which	O
a	O
prob-	O
ability	O
distribution	O
p	O
(	O
x	O
)	O
is	O
allowed	O
through	O
the	O
ﬁlter	O
if	O
,	O
and	O
only	O
if	O
,	O
it	O
satisﬁes	O
the	O
directed	B
factorization	I
property	O
(	O
8.5	O
)	O
.	O
the	O
set	O
of	O
all	O
possible	O
probability	B
distributions	O
p	O
(	O
x	O
)	O
that	O
pass	O
through	O
the	O
ﬁlter	O
is	O
denoted	O
df	O
.	O
we	O
can	O
alternatively	O
use	O
the	O
graph	O
to	O
ﬁlter	O
distributions	O
according	O
to	O
whether	O
they	O
respect	O
all	O
of	O
the	O
conditional	B
independencies	O
implied	O
by	O
the	O
d-separation	B
properties	O
of	O
the	O
graph	O
.	O
the	O
d-separation	B
theorem	O
says	O
that	O
it	O
is	O
the	O
same	O
set	O
of	O
distributions	O
df	O
that	O
will	O
be	O
allowed	O
through	O
this	O
second	O
kind	O
of	O
ﬁlter	O
.	O
tions	O
p	O
(	O
x	O
)	O
.	O
at	O
the	O
other	O
extreme	O
,	O
we	O
have	O
the	O
fully	O
disconnected	O
graph	O
,	O
i.e.	O
,	O
one	O
having	O
no	O
links	O
at	O
all	O
.	O
this	O
corresponds	O
to	O
joint	O
distributions	O
which	O
factorize	O
into	O
the	O
product	O
of	O
the	O
marginal	B
distributions	O
over	O
the	O
variables	O
comprising	O
the	O
nodes	O
of	O
the	O
graph	O
.	O
note	O
that	O
for	O
any	O
given	O
graph	O
,	O
the	O
set	O
of	O
distributions	O
df	O
will	O
include	O
any	O
dis-	O
tributions	O
that	O
have	O
additional	O
independence	O
properties	O
beyond	O
those	O
described	O
by	O
the	O
graph	O
.	O
for	O
instance	O
,	O
a	O
fully	O
factorized	O
distribution	O
will	O
always	O
be	O
passed	O
through	O
the	O
ﬁlter	O
implied	O
by	O
any	O
graph	O
over	O
the	O
corresponding	O
set	O
of	O
variables	O
.	O
we	O
end	O
our	O
discussion	O
of	O
conditional	B
independence	I
properties	O
by	O
exploring	O
the	O
concept	O
of	O
a	O
markov	O
blanket	O
or	O
markov	O
boundary	O
.	O
consider	O
a	O
joint	O
distribution	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
represented	O
by	O
a	O
directed	B
graph	O
having	O
d	O
nodes	O
,	O
and	O
consider	O
the	O
conditional	B
distribution	O
of	O
a	O
particular	O
node	B
with	O
variables	O
xi	O
conditioned	O
on	O
all	O
of	O
the	O
remaining	O
variables	O
xj	O
(	O
cid:9	O
)	O
=i	O
.	O
using	O
the	O
factorization	B
property	O
(	O
8.5	O
)	O
,	O
we	O
can	O
express	O
this	O
conditional	B
distribution	O
in	O
the	O
form	O
p	O
(	O
xi|x	O
{	O
j	O
(	O
cid:9	O
)	O
=i	O
}	O
)	O
=	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
(	O
cid:6	O
)	O
(	O
cid:14	O
)	O
(	O
cid:6	O
)	O
(	O
cid:14	O
)	O
k	O
=	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
dxi	O
p	O
(	O
xk|pak	O
)	O
p	O
(	O
xk|pak	O
)	O
dxi	O
k	O
in	O
which	O
the	O
integral	O
is	O
replaced	O
by	O
a	O
summation	O
in	O
the	O
case	O
of	O
discrete	O
variables	O
.	O
we	O
now	O
observe	O
that	O
any	O
factor	O
p	O
(	O
xk|pak	O
)	O
that	O
does	O
not	O
have	O
any	O
functional	B
dependence	O
on	O
xi	O
can	O
be	O
taken	O
outside	O
the	O
integral	O
over	O
xi	O
,	O
and	O
will	O
therefore	O
cancel	O
between	O
numerator	O
and	O
denominator	O
.	O
the	O
only	O
factors	O
that	O
remain	O
will	O
be	O
the	O
conditional	B
distribution	O
p	O
(	O
xi|pai	O
)	O
for	O
node	O
xi	O
itself	O
,	O
together	O
with	O
the	O
conditional	B
distributions	O
for	O
any	O
nodes	O
xk	O
such	O
that	O
node	B
xi	O
is	O
in	O
the	O
conditioning	O
set	O
of	O
p	O
(	O
xk|pak	O
)	O
,	O
in	O
other	O
words	O
for	O
which	O
xi	O
is	O
a	O
parent	O
of	O
xk	O
.	O
the	O
conditional	B
p	O
(	O
xi|pai	O
)	O
will	O
depend	O
on	O
the	O
parents	O
of	O
node	B
xi	O
,	O
whereas	O
the	O
conditionals	O
p	O
(	O
xk|pak	O
)	O
will	O
depend	O
on	O
the	O
children	O
8.3.	O
markov	O
random	O
fields	O
383	O
figure	O
8.26	O
the	O
markov	O
blanket	O
of	O
a	O
node	B
xi	O
comprises	O
the	O
set	O
of	O
parents	O
,	O
children	O
and	O
co-parents	B
of	O
the	O
node	B
.	O
it	O
has	O
the	O
property	O
that	O
the	O
conditional	B
distribution	O
of	O
xi	O
,	O
conditioned	O
on	O
all	O
the	O
remaining	O
variables	O
in	O
the	O
graph	O
,	O
is	O
dependent	O
only	O
on	O
the	O
variables	O
in	O
the	O
markov	O
blanket	O
.	O
xi	O
of	O
xi	O
as	O
well	O
as	O
on	O
the	O
co-parents	B
,	O
in	O
other	O
words	O
variables	O
corresponding	O
to	O
parents	O
of	O
node	B
xk	O
other	O
than	O
node	B
xi	O
.	O
the	O
set	O
of	O
nodes	O
comprising	O
the	O
parents	O
,	O
the	O
children	O
and	O
the	O
co-parents	B
is	O
called	O
the	O
markov	O
blanket	O
and	O
is	O
illustrated	O
in	O
figure	O
8.26.	O
we	O
can	O
think	O
of	O
the	O
markov	O
blanket	O
of	O
a	O
node	B
xi	O
as	O
being	O
the	O
minimal	O
set	O
of	O
nodes	O
that	O
isolates	O
xi	O
from	O
the	O
rest	O
of	O
the	O
graph	O
.	O
note	O
that	O
it	O
is	O
not	O
sufﬁcient	O
to	O
include	O
only	O
the	O
parents	O
and	O
children	O
of	O
node	B
xi	O
because	O
the	O
phenomenon	O
of	O
explaining	B
away	I
means	O
that	O
observations	O
of	O
the	O
child	O
nodes	O
will	O
not	O
block	O
paths	O
to	O
the	O
co-parents	B
.	O
we	O
must	O
therefore	O
observe	O
the	O
co-parent	O
nodes	O
also	O
.	O
8.3.	O
markov	O
random	O
fields	O
we	O
have	O
seen	O
that	O
directed	B
graphical	O
models	O
specify	O
a	O
factorization	B
of	O
the	O
joint	O
dis-	O
tribution	O
over	O
a	O
set	O
of	O
variables	O
into	O
a	O
product	O
of	O
local	B
conditional	O
distributions	O
.	O
they	O
also	O
deﬁne	O
a	O
set	O
of	O
conditional	B
independence	I
properties	O
that	O
must	O
be	O
satisﬁed	O
by	O
any	O
distribution	O
that	O
factorizes	O
according	O
to	O
the	O
graph	O
.	O
we	O
turn	O
now	O
to	O
the	O
second	O
ma-	O
jor	O
class	O
of	O
graphical	O
models	O
that	O
are	O
described	O
by	O
undirected	B
graphs	O
and	O
that	O
again	O
specify	O
both	O
a	O
factorization	B
and	O
a	O
set	O
of	O
conditional	B
independence	I
relations	O
.	O
a	O
markov	O
random	O
ﬁeld	O
,	O
also	O
known	O
as	O
a	O
markov	O
network	O
or	O
an	O
undirected	B
graphical	O
model	O
(	O
kindermann	O
and	O
snell	O
,	O
1980	O
)	O
,	O
has	O
a	O
set	O
of	O
nodes	O
each	O
of	O
which	O
corresponds	O
to	O
a	O
variable	O
or	O
group	O
of	O
variables	O
,	O
as	O
well	O
as	O
a	O
set	O
of	O
links	O
each	O
of	O
which	O
connects	O
a	O
pair	O
of	O
nodes	O
.	O
the	O
links	O
are	O
undirected	B
,	O
that	O
is	O
they	O
do	O
not	O
carry	O
arrows	O
.	O
in	O
the	O
case	O
of	O
undirected	B
graphs	O
,	O
it	O
is	O
convenient	O
to	O
begin	O
with	O
a	O
discussion	O
of	O
conditional	B
independence	I
properties	O
.	O
8.3.1	O
conditional	B
independence	I
properties	O
in	O
the	O
case	O
of	O
directed	B
graphs	O
,	O
we	O
saw	O
that	O
it	O
was	O
possible	O
to	O
test	O
whether	O
a	O
par-	O
ticular	O
conditional	B
independence	I
property	O
holds	O
by	O
applying	O
a	O
graphical	O
test	O
called	O
d-separation	B
.	O
this	O
involved	O
testing	O
whether	O
or	O
not	O
the	O
paths	O
connecting	O
two	O
sets	O
of	O
nodes	O
were	O
‘	O
blocked	O
’	O
.	O
the	O
deﬁnition	O
of	O
blocked	O
,	O
however	O
,	O
was	O
somewhat	O
subtle	O
due	O
to	O
the	O
presence	O
of	O
paths	O
having	O
head-to-head	O
nodes	O
.	O
we	O
might	O
ask	O
whether	O
it	O
is	O
possible	O
to	O
deﬁne	O
an	O
alternative	O
graphical	O
semantics	O
for	O
probability	O
distributions	O
such	O
that	O
conditional	B
independence	I
is	O
determined	O
by	O
simple	O
graph	O
separation	O
.	O
this	O
is	O
indeed	O
the	O
case	O
and	O
corresponds	O
to	O
undirected	B
graphical	O
models	O
.	O
by	O
removing	O
the	O
section	O
8.2	O
384	O
8.	O
graphical	O
models	O
figure	O
8.27	O
an	O
example	O
of	O
an	O
undirected	B
graph	I
in	O
which	O
every	O
path	O
from	O
any	O
node	B
in	O
set	O
a	O
to	O
any	O
node	B
in	O
set	O
b	O
passes	O
through	O
at	O
least	O
one	O
node	B
in	O
set	O
c.	O
conse-	O
quently	O
the	O
conditional	B
independence	I
property	O
a	O
⊥⊥	O
b	O
|	O
c	O
holds	O
for	O
any	O
probability	B
distribution	O
described	O
by	O
this	O
graph	O
.	O
c	O
b	O
a	O
directionality	O
from	O
the	O
links	O
of	O
the	O
graph	O
,	O
the	O
asymmetry	O
between	O
parent	O
and	O
child	O
nodes	O
is	O
removed	O
,	O
and	O
so	O
the	O
subtleties	O
associated	O
with	O
head-to-head	O
nodes	O
no	O
longer	O
arise	O
.	O
suppose	O
that	O
in	O
an	O
undirected	B
graph	I
we	O
identify	O
three	O
sets	O
of	O
nodes	O
,	O
denoted	O
a	O
,	O
b	O
,	O
and	O
c	O
,	O
and	O
that	O
we	O
consider	O
the	O
conditional	B
independence	I
property	O
a	O
⊥⊥	O
b	O
|	O
c.	O
(	O
8.37	O
)	O
to	O
test	O
whether	O
this	O
property	O
is	O
satisﬁed	O
by	O
a	O
probability	B
distribution	O
deﬁned	O
by	O
a	O
graph	O
we	O
consider	O
all	O
possible	O
paths	O
that	O
connect	O
nodes	O
in	O
set	O
a	O
to	O
nodes	O
in	O
set	O
b.	O
if	O
all	O
such	O
paths	O
pass	O
through	O
one	O
or	O
more	O
nodes	O
in	O
set	O
c	O
,	O
then	O
all	O
such	O
paths	O
are	O
‘	O
blocked	O
’	O
and	O
so	O
the	O
conditional	B
independence	I
property	O
holds	O
.	O
however	O
,	O
if	O
there	O
is	O
at	O
least	O
one	O
such	O
path	O
that	O
is	O
not	O
blocked	O
,	O
then	O
the	O
property	O
does	O
not	O
necessarily	O
hold	O
,	O
or	O
more	O
precisely	O
there	O
will	O
exist	O
at	O
least	O
some	O
distributions	O
corresponding	O
to	O
the	O
graph	O
that	O
do	O
not	O
satisfy	O
this	O
conditional	B
independence	I
relation	O
.	O
this	O
is	O
illustrated	O
with	O
an	O
example	O
in	O
figure	O
8.27.	O
note	O
that	O
this	O
is	O
exactly	O
the	O
same	O
as	O
the	O
d-separation	B
crite-	O
rion	B
except	O
that	O
there	O
is	O
no	O
‘	O
explaining	B
away	I
’	O
phenomenon	O
.	O
testing	O
for	O
conditional	O
independence	O
in	O
undirected	B
graphs	O
is	O
therefore	O
simpler	O
than	O
in	O
directed	B
graphs	O
.	O
an	O
alternative	O
way	O
to	O
view	O
the	O
conditional	B
independence	I
test	O
is	O
to	O
imagine	O
re-	O
moving	O
all	O
nodes	O
in	O
set	O
c	O
from	O
the	O
graph	O
together	O
with	O
any	O
links	O
that	O
connect	O
to	O
those	O
nodes	O
.	O
we	O
then	O
ask	O
if	O
there	O
exists	O
a	O
path	O
that	O
connects	O
any	O
node	B
in	O
a	O
to	O
any	O
node	B
in	O
b.	O
if	O
there	O
are	O
no	O
such	O
paths	O
,	O
then	O
the	O
conditional	B
independence	I
property	O
must	O
hold	O
.	O
the	O
markov	O
blanket	O
for	O
an	O
undirected	B
graph	I
takes	O
a	O
particularly	O
simple	O
form	O
,	O
because	O
a	O
node	B
will	O
be	O
conditionally	O
independent	B
of	O
all	O
other	O
nodes	O
conditioned	O
only	O
on	O
the	O
neighbouring	O
nodes	O
,	O
as	O
illustrated	O
in	O
figure	O
8.28	O
.	O
8.3.2	O
factorization	B
properties	O
we	O
now	O
seek	O
a	O
factorization	B
rule	O
for	O
undirected	O
graphs	O
that	O
will	O
correspond	O
to	O
the	O
above	O
conditional	B
independence	I
test	O
.	O
again	O
,	O
this	O
will	O
involve	O
expressing	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
)	O
as	O
a	O
product	O
of	O
functions	O
deﬁned	O
over	O
sets	O
of	O
variables	O
that	O
are	O
local	B
to	O
the	O
graph	O
.	O
we	O
therefore	O
need	O
to	O
decide	O
what	O
is	O
the	O
appropriate	O
notion	O
of	O
locality	O
in	O
this	O
case	O
.	O
8.3.	O
markov	O
random	O
fields	O
385	O
figure	O
8.28	O
for	O
an	O
undirected	B
graph	I
,	O
the	O
markov	O
blanket	O
of	O
a	O
node	B
xi	O
consists	O
of	O
the	O
set	O
of	O
neighbouring	O
nodes	O
.	O
it	O
has	O
the	O
property	O
that	O
the	O
conditional	B
distribution	O
of	O
xi	O
,	O
conditioned	O
on	O
all	O
the	O
remaining	O
variables	O
in	O
the	O
graph	O
,	O
is	O
dependent	O
only	O
on	O
the	O
variables	O
in	O
the	O
markov	O
blanket	O
.	O
if	O
we	O
consider	O
two	O
nodes	O
xi	O
and	O
xj	O
that	O
are	O
not	O
connected	O
by	O
a	O
link	B
,	O
then	O
these	O
variables	O
must	O
be	O
conditionally	O
independent	B
given	O
all	O
other	O
nodes	O
in	O
the	O
graph	O
.	O
this	O
follows	O
from	O
the	O
fact	O
that	O
there	O
is	O
no	O
direct	O
path	O
between	O
the	O
two	O
nodes	O
,	O
and	O
all	O
other	O
paths	O
pass	O
through	O
nodes	O
that	O
are	O
observed	O
,	O
and	O
hence	O
those	O
paths	O
are	O
blocked	O
.	O
this	O
conditional	B
independence	I
property	O
can	O
be	O
expressed	O
as	O
p	O
(	O
xi	O
,	O
xj|x\	O
{	O
i	O
,	O
j	O
}	O
)	O
=	O
p	O
(	O
xi|x\	O
{	O
i	O
,	O
j	O
}	O
)	O
p	O
(	O
xj|x\	O
{	O
i	O
,	O
j	O
}	O
)	O
(	O
8.38	O
)	O
where	O
x\	O
{	O
i	O
,	O
j	O
}	O
denotes	O
the	O
set	O
x	O
of	O
all	O
variables	O
with	O
xi	O
and	O
xj	O
removed	O
.	O
the	O
factor-	O
ization	O
of	O
the	O
joint	O
distribution	O
must	O
therefore	O
be	O
such	O
that	O
xi	O
and	O
xj	O
do	O
not	O
appear	O
in	O
the	O
same	O
factor	O
in	O
order	O
for	O
the	O
conditional	B
independence	I
property	O
to	O
hold	O
for	O
all	O
possible	O
distributions	O
belonging	O
to	O
the	O
graph	O
.	O
this	O
leads	O
us	O
to	O
consider	O
a	O
graphical	O
concept	O
called	O
a	O
clique	B
,	O
which	O
is	O
deﬁned	O
as	O
a	O
subset	O
of	O
the	O
nodes	O
in	O
a	O
graph	O
such	O
that	O
there	O
exists	O
a	O
link	B
between	O
all	O
pairs	O
of	O
nodes	O
in	O
the	O
subset	O
.	O
in	O
other	O
words	O
,	O
the	O
set	O
of	O
nodes	O
in	O
a	O
clique	B
is	O
fully	B
connected	I
.	O
furthermore	O
,	O
a	O
maximal	B
clique	I
is	O
a	O
clique	B
such	O
that	O
it	O
is	O
not	O
possible	O
to	O
include	O
any	O
other	O
nodes	O
from	O
the	O
graph	O
in	O
the	O
set	O
without	O
it	O
ceasing	O
to	O
be	O
a	O
clique	B
.	O
these	O
concepts	O
are	O
illustrated	O
by	O
the	O
undirected	B
graph	I
over	O
four	O
variables	O
shown	O
in	O
figure	O
8.29.	O
this	O
graph	O
has	O
ﬁve	O
cliques	O
of	O
two	O
nodes	O
given	O
by	O
{	O
x1	O
,	O
x2	O
}	O
,	O
{	O
x2	O
,	O
x3	O
}	O
,	O
{	O
x3	O
,	O
x4	O
}	O
,	O
{	O
x4	O
,	O
x2	O
}	O
,	O
and	O
{	O
x1	O
,	O
x3	O
}	O
,	O
as	O
well	O
as	O
two	O
maximal	O
cliques	O
given	O
by	O
{	O
x1	O
,	O
x2	O
,	O
x3	O
}	O
and	O
{	O
x2	O
,	O
x3	O
,	O
x4	O
}	O
.	O
the	O
set	O
{	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
}	O
is	O
not	O
a	O
clique	B
because	O
of	O
the	O
missing	O
link	O
from	O
x1	O
to	O
x4	O
.	O
we	O
can	O
therefore	O
deﬁne	O
the	O
factors	O
in	O
the	O
decomposition	O
of	O
the	O
joint	O
distribution	O
to	O
be	O
functions	O
of	O
the	O
variables	O
in	O
the	O
cliques	O
.	O
in	O
fact	O
,	O
we	O
can	O
consider	O
functions	O
of	O
the	O
maximal	O
cliques	O
,	O
without	O
loss	O
of	O
generality	O
,	O
because	O
other	O
cliques	O
must	O
be	O
subsets	O
of	O
maximal	O
cliques	O
.	O
thus	O
,	O
if	O
{	O
x1	O
,	O
x2	O
,	O
x3	O
}	O
is	O
a	O
maximal	B
clique	I
and	O
we	O
deﬁne	O
an	O
arbitrary	O
function	O
over	O
this	O
clique	B
,	O
then	O
including	O
another	O
factor	O
deﬁned	O
over	O
a	O
subset	O
of	O
these	O
variables	O
would	O
be	O
redundant	O
.	O
let	O
us	O
denote	O
a	O
clique	B
by	O
c	O
and	O
the	O
set	O
of	O
variables	O
in	O
that	O
clique	B
by	O
xc	O
.	O
then	O
figure	O
8.29	O
a	O
four-node	O
undirected	B
graph	I
showing	O
a	O
clique	B
(	O
outlined	O
in	O
green	O
)	O
and	O
a	O
maximal	B
clique	I
(	O
outlined	O
in	O
blue	O
)	O
.	O
x1	O
x3	O
x2	O
x4	O
386	O
8.	O
graphical	O
models	O
the	O
joint	O
distribution	O
is	O
written	O
as	O
a	O
product	O
of	O
potential	O
functions	O
ψc	O
(	O
xc	O
)	O
over	O
the	O
maximal	O
cliques	O
of	O
the	O
graph	O
p	O
(	O
x	O
)	O
=	O
ψc	O
(	O
xc	O
)	O
.	O
(	O
8.39	O
)	O
(	O
cid:14	O
)	O
(	O
cid:14	O
)	O
c	O
1	O
z	O
(	O
cid:2	O
)	O
here	O
the	O
quantity	O
z	O
,	O
sometimes	O
called	O
the	O
partition	B
function	I
,	O
is	O
a	O
normalization	O
con-	O
stant	O
and	O
is	O
given	O
by	O
z	O
=	O
ψc	O
(	O
xc	O
)	O
(	O
8.40	O
)	O
x	O
c	O
which	O
ensures	O
that	O
the	O
distribution	O
p	O
(	O
x	O
)	O
given	O
by	O
(	O
8.39	O
)	O
is	O
correctly	O
normalized	O
.	O
by	O
considering	O
only	O
potential	O
functions	O
which	O
satisfy	O
ψc	O
(	O
xc	O
)	O
(	O
cid:2	O
)	O
0	O
we	O
ensure	O
that	O
p	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
0.	O
in	O
(	O
8.40	O
)	O
we	O
have	O
assumed	O
that	O
x	O
comprises	O
discrete	O
variables	O
,	O
but	O
the	O
framework	O
is	O
equally	O
applicable	O
to	O
continuous	O
variables	O
,	O
or	O
a	O
combination	O
of	O
the	O
two	O
,	O
in	O
which	O
the	O
summation	O
is	O
replaced	O
by	O
the	O
appropriate	O
combination	O
of	O
summation	O
and	O
integration	O
.	O
note	O
that	O
we	O
do	O
not	O
restrict	O
the	O
choice	O
of	O
potential	O
functions	O
to	O
those	O
that	O
have	O
a	O
speciﬁc	O
probabilistic	O
interpretation	O
as	O
marginal	B
or	O
conditional	B
distributions	O
.	O
this	O
is	O
in	O
contrast	O
to	O
directed	B
graphs	O
in	O
which	O
each	O
factor	O
represents	O
the	O
conditional	B
distribu-	O
tion	O
of	O
the	O
corresponding	O
variable	O
,	O
conditioned	O
on	O
the	O
state	O
of	O
its	O
parents	O
.	O
however	O
,	O
in	O
special	O
cases	O
,	O
for	O
instance	O
where	O
the	O
undirected	B
graph	I
is	O
constructed	O
by	O
starting	O
with	O
a	O
directed	B
graph	O
,	O
the	O
potential	O
functions	O
may	O
indeed	O
have	O
such	O
an	O
interpretation	O
,	O
as	O
we	O
shall	O
see	O
shortly	O
.	O
one	O
consequence	O
of	O
the	O
generality	O
of	O
the	O
potential	O
functions	O
ψc	O
(	O
xc	O
)	O
is	O
that	O
their	O
product	O
will	O
in	O
general	O
not	O
be	O
correctly	O
normalized	O
.	O
we	O
therefore	O
have	O
to	O
in-	O
troduce	O
an	O
explicit	O
normalization	O
factor	O
given	O
by	O
(	O
8.40	O
)	O
.	O
recall	O
that	O
for	O
directed	O
graphs	O
,	O
the	O
joint	O
distribution	O
was	O
automatically	O
normalized	O
as	O
a	O
consequence	O
of	O
the	O
normalization	O
of	O
each	O
of	O
the	O
conditional	B
distributions	O
in	O
the	O
factorization	B
.	O
the	O
presence	O
of	O
this	O
normalization	O
constant	O
is	O
one	O
of	O
the	O
major	O
limitations	O
of	O
undirected	B
graphs	O
.	O
if	O
we	O
have	O
a	O
model	O
with	O
m	O
discrete	O
nodes	O
each	O
having	O
k	O
states	O
,	O
then	O
the	O
evaluation	O
of	O
the	O
normalization	O
term	O
involves	O
summing	O
over	O
km	O
states	O
and	O
so	O
(	O
in	O
the	O
worst	O
case	O
)	O
is	O
exponential	O
in	O
the	O
size	O
of	O
the	O
model	O
.	O
the	O
partition	B
function	I
is	O
needed	O
for	O
parameter	O
learning	B
because	O
it	O
will	O
be	O
a	O
function	O
of	O
any	O
parameters	O
that	O
govern	O
the	O
potential	O
functions	O
ψc	O
(	O
xc	O
)	O
.	O
however	O
,	O
for	O
evaluation	O
of	O
local	B
conditional	O
distributions	O
,	O
the	O
partition	B
function	I
is	O
not	O
needed	O
because	O
a	O
conditional	B
is	O
the	O
ratio	O
of	O
two	O
marginals	O
,	O
and	O
the	O
partition	B
function	I
cancels	O
between	O
numerator	O
and	O
denom-	O
inator	O
when	O
evaluating	O
this	O
ratio	O
.	O
similarly	O
,	O
for	O
evaluating	O
local	B
marginal	O
probabil-	O
ities	O
we	O
can	O
work	O
with	O
the	O
unnormalized	O
joint	O
distribution	O
and	O
then	O
normalize	O
the	O
marginals	O
explicitly	O
at	O
the	O
end	O
.	O
provided	O
the	O
marginals	O
only	O
involves	O
a	O
small	O
number	O
of	O
variables	O
,	O
the	O
evaluation	O
of	O
their	O
normalization	O
coefﬁcient	O
will	O
be	O
feasible	O
.	O
so	O
far	O
,	O
we	O
have	O
discussed	O
the	O
notion	O
of	O
conditional	B
independence	I
based	O
on	O
sim-	O
ple	O
graph	O
separation	O
and	O
we	O
have	O
proposed	O
a	O
factorization	B
of	O
the	O
joint	O
distribution	O
that	O
is	O
intended	O
to	O
correspond	O
to	O
this	O
conditional	B
independence	I
structure	O
.	O
however	O
,	O
we	O
have	O
not	O
made	O
any	O
formal	O
connection	O
between	O
conditional	B
independence	I
and	O
factorization	B
for	O
undirected	B
graphs	O
.	O
to	O
do	O
so	O
we	O
need	O
to	O
restrict	O
attention	O
to	O
poten-	O
tial	O
functions	O
ψc	O
(	O
xc	O
)	O
that	O
are	O
strictly	O
positive	O
(	O
i.e.	O
,	O
never	O
zero	O
or	O
negative	O
for	O
any	O
8.3.	O
markov	O
random	O
fields	O
387	O
choice	O
of	O
xc	O
)	O
.	O
given	O
this	O
restriction	O
,	O
we	O
can	O
make	O
a	O
precise	O
relationship	O
between	O
factorization	B
and	O
conditional	B
independence	I
.	O
to	O
do	O
this	O
we	O
again	O
return	O
to	O
the	O
concept	O
of	O
a	O
graphical	B
model	I
as	O
a	O
ﬁlter	O
,	O
corre-	O
sponding	O
to	O
figure	O
8.25.	O
consider	O
the	O
set	O
of	O
all	O
possible	O
distributions	O
deﬁned	O
over	O
a	O
ﬁxed	O
set	O
of	O
variables	O
corresponding	O
to	O
the	O
nodes	O
of	O
a	O
particular	O
undirected	B
graph	I
.	O
we	O
can	O
deﬁne	O
ui	O
to	O
be	O
the	O
set	O
of	O
such	O
distributions	O
that	O
are	O
consistent	B
with	O
the	O
set	O
of	O
conditional	B
independence	I
statements	O
that	O
can	O
be	O
read	O
from	O
the	O
graph	O
using	O
graph	O
separation	O
.	O
similarly	O
,	O
we	O
can	O
deﬁne	O
uf	O
to	O
be	O
the	O
set	O
of	O
such	O
distributions	O
that	O
can	O
be	O
expressed	O
as	O
a	O
factorization	B
of	O
the	O
form	O
(	O
8.39	O
)	O
with	O
respect	O
to	O
the	O
maximal	O
cliques	O
of	O
the	O
graph	O
.	O
the	O
hammersley-clifford	O
theorem	O
(	O
clifford	O
,	O
1990	O
)	O
states	O
that	O
the	O
sets	O
ui	O
and	O
uf	O
are	O
identical	O
.	O
because	O
we	O
are	O
restricted	O
to	O
potential	O
functions	O
which	O
are	O
strictly	O
positive	O
it	O
is	O
convenient	O
to	O
express	O
them	O
as	O
exponentials	O
,	O
so	O
that	O
ψc	O
(	O
xc	O
)	O
=	O
exp	O
{	O
−e	O
(	O
xc	O
)	O
}	O
(	O
8.41	O
)	O
where	O
e	O
(	O
xc	O
)	O
is	O
called	O
an	O
energy	B
function	I
,	O
and	O
the	O
exponential	O
representation	O
is	O
called	O
the	O
boltzmann	O
distribution	O
.	O
the	O
joint	O
distribution	O
is	O
deﬁned	O
as	O
the	O
product	O
of	O
potentials	O
,	O
and	O
so	O
the	O
total	O
energy	O
is	O
obtained	O
by	O
adding	O
the	O
energies	O
of	O
each	O
of	O
the	O
maximal	O
cliques	O
.	O
in	O
contrast	O
to	O
the	O
factors	O
in	O
the	O
joint	O
distribution	O
for	O
a	O
directed	B
graph	O
,	O
the	O
po-	O
tentials	O
in	O
an	O
undirected	B
graph	I
do	O
not	O
have	O
a	O
speciﬁc	O
probabilistic	O
interpretation	O
.	O
although	O
this	O
gives	O
greater	O
ﬂexibility	O
in	O
choosing	O
the	O
potential	O
functions	O
,	O
because	O
there	O
is	O
no	O
normalization	O
constraint	O
,	O
it	O
does	O
raise	O
the	O
question	O
of	O
how	O
to	O
motivate	O
a	O
choice	O
of	O
potential	B
function	I
for	O
a	O
particular	O
application	O
.	O
this	O
can	O
be	O
done	O
by	O
view-	O
ing	O
the	O
potential	B
function	I
as	O
expressing	O
which	O
conﬁgurations	O
of	O
the	O
local	B
variables	O
are	O
preferred	O
to	O
others	O
.	O
global	O
conﬁgurations	O
that	O
have	O
a	O
relatively	O
high	O
probability	B
are	O
those	O
that	O
ﬁnd	O
a	O
good	O
balance	O
in	O
satisfying	O
the	O
(	O
possibly	O
conﬂicting	O
)	O
inﬂuences	O
of	O
the	O
clique	B
potentials	O
.	O
we	O
turn	O
now	O
to	O
a	O
speciﬁc	O
example	O
to	O
illustrate	O
the	O
use	O
of	O
undirected	B
graphs	O
.	O
8.3.3	O
illustration	O
:	O
image	B
de-noising	I
we	O
can	O
illustrate	O
the	O
application	O
of	O
undirected	B
graphs	O
using	O
an	O
example	O
of	O
noise	O
removal	O
from	O
a	O
binary	O
image	O
(	O
besag	O
,	O
1974	O
;	O
geman	O
and	O
geman	O
,	O
1984	O
;	O
besag	O
,	O
1986	O
)	O
.	O
although	O
a	O
very	O
simple	O
example	O
,	O
this	O
is	O
typical	O
of	O
more	O
sophisticated	O
applications	O
.	O
let	O
the	O
observed	O
noisy	O
image	O
be	O
described	O
by	O
an	O
array	O
of	O
binary	O
pixel	O
values	O
yi	O
∈	O
{	O
−1	O
,	O
+1	O
}	O
,	O
where	O
the	O
index	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
runs	O
over	O
all	O
pixels	O
.	O
we	O
shall	O
suppose	O
that	O
the	O
image	O
is	O
obtained	O
by	O
taking	O
an	O
unknown	O
noise-free	O
image	O
,	O
described	O
by	O
binary	O
pixel	O
values	O
xi	O
∈	O
{	O
−1	O
,	O
+1	O
}	O
and	O
randomly	O
ﬂipping	O
the	O
sign	O
of	O
pixels	O
with	O
some	O
small	O
probability	B
.	O
an	O
example	O
binary	O
image	O
,	O
together	O
with	O
a	O
noise	O
corrupted	O
image	O
obtained	O
by	O
ﬂipping	O
the	O
sign	O
of	O
the	O
pixels	O
with	O
probability	B
10	O
%	O
,	O
is	O
shown	O
in	O
figure	O
8.30.	O
given	O
the	O
noisy	O
image	O
,	O
our	O
goal	O
is	O
to	O
recover	O
the	O
original	O
noise-free	O
image	O
.	O
because	O
the	O
noise	O
level	O
is	O
small	O
,	O
we	O
know	O
that	O
there	O
will	O
be	O
a	O
strong	O
correlation	O
between	O
xi	O
and	O
yi	O
.	O
we	O
also	O
know	O
that	O
neighbouring	O
pixels	O
xi	O
and	O
xj	O
in	O
an	O
image	O
are	O
strongly	O
correlated	O
.	O
this	O
prior	B
knowledge	O
can	O
be	O
captured	O
using	O
the	O
markov	O
388	O
8.	O
graphical	O
models	O
figure	O
8.30	O
illustration	O
of	O
image	B
de-noising	I
using	O
a	O
markov	O
random	O
ﬁeld	O
.	O
the	O
top	O
row	O
shows	O
the	O
original	O
binary	O
image	O
on	O
the	O
left	O
and	O
the	O
corrupted	O
image	O
after	O
randomly	O
changing	O
10	O
%	O
of	O
the	O
pixels	O
on	O
the	O
right	O
.	O
the	O
bottom	O
row	O
shows	O
the	O
restored	O
images	O
obtained	O
using	O
iterated	O
conditional	O
models	O
(	O
icm	O
)	O
on	O
the	O
left	O
and	O
using	O
the	O
graph-cut	B
algorithm	I
on	O
the	O
right	O
.	O
icm	O
produces	O
an	O
image	O
where	O
96	O
%	O
of	O
the	O
pixels	O
agree	O
with	O
the	O
original	O
image	O
,	O
whereas	O
the	O
corresponding	O
number	O
for	O
graph-cut	O
is	O
99	O
%	O
.	O
random	O
ﬁeld	O
model	O
whose	O
undirected	B
graph	I
is	O
shown	O
in	O
figure	O
8.31.	O
this	O
graph	O
has	O
two	O
types	O
of	O
cliques	O
,	O
each	O
of	O
which	O
contains	O
two	O
variables	O
.	O
the	O
cliques	O
of	O
the	O
form	O
{	O
xi	O
,	O
yi	O
}	O
have	O
an	O
associated	O
energy	B
function	I
that	O
expresses	O
the	O
correlation	O
between	O
these	O
variables	O
.	O
we	O
choose	O
a	O
very	O
simple	O
energy	B
function	I
for	O
these	O
cliques	O
of	O
the	O
form	O
−ηxiyi	O
where	O
η	O
is	O
a	O
positive	O
constant	O
.	O
this	O
has	O
the	O
desired	O
effect	O
of	O
giving	O
a	O
lower	O
energy	O
(	O
thus	O
encouraging	O
a	O
higher	O
probability	B
)	O
when	O
xi	O
and	O
yi	O
have	O
the	O
same	O
sign	O
and	O
a	O
higher	O
energy	O
when	O
they	O
have	O
the	O
opposite	O
sign	O
.	O
the	O
remaining	O
cliques	O
comprise	O
pairs	O
of	O
variables	O
{	O
xi	O
,	O
xj	O
}	O
where	O
i	O
and	O
j	O
are	O
indices	O
of	O
neighbouring	O
pixels	O
.	O
again	O
,	O
we	O
want	O
the	O
energy	O
to	O
be	O
lower	O
when	O
the	O
pixels	O
have	O
the	O
same	O
sign	O
than	O
when	O
they	O
have	O
the	O
opposite	O
sign	O
,	O
and	O
so	O
we	O
choose	O
an	O
energy	O
given	O
by	O
−βxixj	O
where	O
β	O
is	O
a	O
positive	O
constant	O
.	O
because	O
a	O
potential	B
function	I
is	O
an	O
arbitrary	O
,	O
nonnegative	O
function	O
over	O
a	O
maximal	B
clique	I
,	O
we	O
can	O
multiply	O
it	O
by	O
any	O
nonnegative	O
functions	O
of	O
subsets	O
of	O
the	O
clique	B
,	O
or	O
8.3.	O
markov	O
random	O
fields	O
389	O
figure	O
8.31	O
an	O
undirected	B
graphical	O
model	O
representing	O
a	O
markov	O
random	O
ﬁeld	O
for	O
image	O
de-noising	O
,	O
in	O
which	O
xi	O
is	O
a	O
binary	O
variable	O
denoting	O
the	O
state	O
of	O
pixel	O
i	O
in	O
the	O
unknown	O
noise-free	O
image	O
,	O
and	O
yi	O
denotes	O
the	O
corresponding	O
value	O
of	O
pixel	O
i	O
in	O
the	O
observed	O
noisy	O
image	O
.	O
yi	O
xi	O
equivalently	O
we	O
can	O
add	O
the	O
corresponding	O
energies	O
.	O
in	O
this	O
example	O
,	O
this	O
allows	O
us	O
to	O
add	O
an	O
extra	O
term	O
hxi	O
for	O
each	O
pixel	O
i	O
in	O
the	O
noise-free	O
image	O
.	O
such	O
a	O
term	O
has	O
the	O
effect	O
of	O
biasing	O
the	O
model	O
towards	O
pixel	O
values	O
that	O
have	O
one	O
particular	O
sign	O
in	O
preference	O
to	O
the	O
other	O
.	O
the	O
complete	O
energy	O
function	O
for	O
the	O
model	O
then	O
takes	O
the	O
form	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
i	O
(	O
cid:2	O
)	O
{	O
i	O
,	O
j	O
}	O
e	O
(	O
x	O
,	O
y	O
)	O
=	O
h	O
xi	O
−	O
β	O
xixj	O
−	O
η	O
which	O
deﬁnes	O
a	O
joint	O
distribution	O
over	O
x	O
and	O
y	O
given	O
by	O
exp	O
{	O
−e	O
(	O
x	O
,	O
y	O
)	O
}	O
.	O
p	O
(	O
x	O
,	O
y	O
)	O
=	O
1	O
z	O
xiyi	O
i	O
(	O
8.42	O
)	O
(	O
8.43	O
)	O
we	O
now	O
ﬁx	O
the	O
elements	O
of	O
y	O
to	O
the	O
observed	O
values	O
given	O
by	O
the	O
pixels	O
of	O
the	O
noisy	O
image	O
,	O
which	O
implicitly	O
deﬁnes	O
a	O
conditional	B
distribution	O
p	O
(	O
x|y	O
)	O
over	O
noise-	O
free	O
images	O
.	O
this	O
is	O
an	O
example	O
of	O
the	O
ising	O
model	O
,	O
which	O
has	O
been	O
widely	O
studied	O
in	O
statistical	O
physics	O
.	O
for	O
the	O
purposes	O
of	O
image	O
restoration	O
,	O
we	O
wish	O
to	O
ﬁnd	O
an	O
image	O
x	O
having	O
a	O
high	O
probability	B
(	O
ideally	O
the	O
maximum	O
probability	O
)	O
.	O
to	O
do	O
this	O
we	O
shall	O
use	O
a	O
simple	O
iterative	O
technique	O
called	O
iterated	B
conditional	I
modes	I
,	O
or	O
icm	O
(	O
kittler	O
and	O
f¨oglein	O
,	O
1984	O
)	O
,	O
which	O
is	O
simply	O
an	O
application	O
of	O
coordinate-wise	O
gradient	O
ascent	O
.	O
the	O
idea	O
is	O
ﬁrst	O
to	O
initialize	O
the	O
variables	O
{	O
xi	O
}	O
,	O
which	O
we	O
do	O
by	O
simply	O
setting	O
xi	O
=	O
yi	O
for	O
all	O
i.	O
then	O
we	O
take	O
one	O
node	B
xj	O
at	O
a	O
time	O
and	O
we	O
evaluate	O
the	O
total	O
energy	O
for	O
the	O
two	O
possible	O
states	O
xj	O
=	O
+1	O
and	O
xj	O
=	O
−1	O
,	O
keeping	O
all	O
other	O
node	B
variables	O
ﬁxed	O
,	O
and	O
set	O
xj	O
to	O
whichever	O
state	O
has	O
the	O
lower	O
energy	O
.	O
this	O
will	O
either	O
leave	O
the	O
probability	B
unchanged	O
,	O
if	O
xj	O
is	O
unchanged	O
,	O
or	O
will	O
increase	O
it	O
.	O
because	O
only	O
one	O
variable	O
is	O
changed	O
,	O
this	O
is	O
a	O
simple	O
local	B
computation	O
that	O
can	O
be	O
performed	O
efﬁciently	O
.	O
we	O
then	O
repeat	O
the	O
update	O
for	O
another	O
site	O
,	O
and	O
so	O
on	O
,	O
until	O
some	O
suitable	O
stopping	O
criterion	O
is	O
satisﬁed	O
.	O
the	O
nodes	O
may	O
be	O
updated	O
in	O
a	O
systematic	O
way	O
,	O
for	O
instance	O
by	O
repeatedly	O
raster	O
scanning	O
through	O
the	O
image	O
,	O
or	O
by	O
choosing	O
nodes	O
at	O
random	O
.	O
if	O
we	O
have	O
a	O
sequence	O
of	O
updates	O
in	O
which	O
every	O
site	O
is	O
visited	O
at	O
least	O
once	O
,	O
and	O
in	O
which	O
no	O
changes	O
to	O
the	O
variables	O
are	O
made	O
,	O
then	O
by	O
deﬁnition	O
the	O
algorithm	O
exercise	O
8.13	O
390	O
8.	O
graphical	O
models	O
figure	O
8.32	O
(	O
a	O
)	O
example	O
of	O
a	O
directed	B
graph	O
.	O
(	O
b	O
)	O
the	O
equivalent	O
undirected	O
graph	O
.	O
x1	O
x1	O
(	O
a	O
)	O
(	O
b	O
)	O
x2	O
x2	O
xn−1	O
xn	O
xn	O
xn−1	O
will	O
have	O
converged	O
to	O
a	O
local	B
maximum	O
of	O
the	O
probability	B
.	O
this	O
need	O
not	O
,	O
however	O
,	O
correspond	O
to	O
the	O
global	O
maximum	O
.	O
for	O
the	O
purposes	O
of	O
this	O
simple	O
illustration	O
,	O
we	O
have	O
ﬁxed	O
the	O
parameters	O
to	O
be	O
β	O
=	O
1.0	O
,	O
η	O
=	O
2.1	O
and	O
h	O
=	O
0.	O
note	O
that	O
leaving	O
h	O
=	O
0	O
simply	O
means	O
that	O
the	O
prior	B
probabilities	O
of	O
the	O
two	O
states	O
of	O
xi	O
are	O
equal	O
.	O
starting	O
with	O
the	O
observed	O
noisy	O
image	O
as	O
the	O
initial	O
conﬁguration	O
,	O
we	O
run	O
icm	O
until	O
convergence	O
,	O
leading	O
to	O
the	O
de-noised	O
image	O
shown	O
in	O
the	O
lower	O
left	O
panel	O
of	O
figure	O
8.30.	O
note	O
that	O
if	O
we	O
set	O
β	O
=	O
0	O
,	O
which	O
effectively	O
removes	O
the	O
links	O
between	O
neighbouring	O
pixels	O
,	O
then	O
the	O
global	O
most	O
probable	O
solution	O
is	O
given	O
by	O
xi	O
=	O
yi	O
for	O
all	O
i	O
,	O
corresponding	O
to	O
the	O
observed	O
noisy	O
image	O
.	O
later	O
we	O
shall	O
discuss	O
a	O
more	O
effective	O
algorithm	O
for	O
ﬁnding	O
high	O
probability	B
so-	O
lutions	O
called	O
the	O
max-product	O
algorithm	O
,	O
which	O
typically	O
leads	O
to	O
better	O
solutions	O
,	O
although	O
this	O
is	O
still	O
not	O
guaranteed	O
to	O
ﬁnd	O
the	O
global	O
maximum	O
of	O
the	O
posterior	O
dis-	O
tribution	O
.	O
however	O
,	O
for	O
certain	O
classes	O
of	O
model	O
,	O
including	O
the	O
one	O
given	O
by	O
(	O
8.42	O
)	O
,	O
there	O
exist	O
efﬁcient	O
algorithms	O
based	O
on	O
graph	O
cuts	O
that	O
are	O
guaranteed	O
to	O
ﬁnd	O
the	O
global	O
maximum	O
(	O
greig	O
et	O
al.	O
,	O
1989	O
;	O
boykov	O
et	O
al.	O
,	O
2001	O
;	O
kolmogorov	O
and	O
zabih	O
,	O
2004	O
)	O
.	O
the	O
lower	O
right	O
panel	O
of	O
figure	O
8.30	O
shows	O
the	O
result	O
of	O
applying	O
a	O
graph-cut	B
algorithm	I
to	O
the	O
de-noising	O
problem	O
.	O
exercise	O
8.14	O
section	O
8.4	O
8.3.4	O
relation	O
to	O
directed	O
graphs	O
we	O
have	O
introduced	O
two	O
graphical	O
frameworks	O
for	O
representing	O
probability	B
dis-	O
tributions	O
,	O
corresponding	O
to	O
directed	B
and	O
undirected	B
graphs	O
,	O
and	O
it	O
is	O
instructive	O
to	O
discuss	O
the	O
relation	O
between	O
these	O
.	O
consider	O
ﬁrst	O
the	O
problem	O
of	O
taking	O
a	O
model	O
that	O
is	O
speciﬁed	O
using	O
a	O
directed	B
graph	O
and	O
trying	O
to	O
convert	O
it	O
to	O
an	O
undirected	B
graph	I
.	O
in	O
some	O
cases	O
this	O
is	O
straightforward	O
,	O
as	O
in	O
the	O
simple	O
example	O
in	O
figure	O
8.32.	O
here	O
the	O
joint	O
distribution	O
for	O
the	O
directed	B
graph	O
is	O
given	O
as	O
a	O
product	O
of	O
conditionals	O
in	O
the	O
form	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2|x1	O
)	O
p	O
(	O
x3|x2	O
)	O
···	O
p	O
(	O
xn|xn−1	O
)	O
.	O
(	O
8.44	O
)	O
now	O
let	O
us	O
convert	O
this	O
to	O
an	O
undirected	B
graph	I
representation	O
,	O
as	O
shown	O
in	O
fig-	O
ure	O
8.32.	O
in	O
the	O
undirected	B
graph	I
,	O
the	O
maximal	O
cliques	O
are	O
simply	O
the	O
pairs	O
of	O
neigh-	O
bouring	O
nodes	O
,	O
and	O
so	O
from	O
(	O
8.39	O
)	O
we	O
wish	O
to	O
write	O
the	O
joint	O
distribution	O
in	O
the	O
form	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
ψ1,2	O
(	O
x1	O
,	O
x2	O
)	O
ψ2,3	O
(	O
x2	O
,	O
x3	O
)	O
···	O
ψn−1	O
,	O
n	O
(	O
xn−1	O
,	O
xn	O
)	O
.	O
(	O
8.45	O
)	O
figure	O
8.33	O
example	O
of	O
a	O
simple	O
directed	B
graph	O
(	O
a	O
)	O
and	O
the	O
corre-	O
sponding	O
moral	O
graph	O
(	O
b	O
)	O
.	O
x1	O
x3	O
x1	O
x3	O
x2	O
x2	O
8.3.	O
markov	O
random	O
fields	O
391	O
x4	O
(	O
a	O
)	O
x4	O
(	O
b	O
)	O
this	O
is	O
easily	O
done	O
by	O
identifying	O
ψ1,2	O
(	O
x1	O
,	O
x2	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2|x1	O
)	O
ψ2,3	O
(	O
x2	O
,	O
x3	O
)	O
=	O
p	O
(	O
x3|x2	O
)	O
ψn−1	O
,	O
n	O
(	O
xn−1	O
,	O
xn	O
)	O
=	O
p	O
(	O
xn|xn−1	O
)	O
...	O
where	O
we	O
have	O
absorbed	O
the	O
marginal	B
p	O
(	O
x1	O
)	O
for	O
the	O
ﬁrst	O
node	O
into	O
the	O
ﬁrst	O
potential	O
function	O
.	O
note	O
that	O
in	O
this	O
case	O
,	O
the	O
partition	B
function	I
z	O
=	O
1.	O
let	O
us	O
consider	O
how	O
to	O
generalize	O
this	O
construction	O
,	O
so	O
that	O
we	O
can	O
convert	O
any	O
distribution	O
speciﬁed	O
by	O
a	O
factorization	B
over	O
a	O
directed	B
graph	O
into	O
one	O
speciﬁed	O
by	O
a	O
factorization	B
over	O
an	O
undirected	B
graph	I
.	O
this	O
can	O
be	O
achieved	O
if	O
the	O
clique	B
potentials	O
of	O
the	O
undirected	B
graph	I
are	O
given	O
by	O
the	O
conditional	B
distributions	O
of	O
the	O
directed	B
graph	O
.	O
in	O
order	O
for	O
this	O
to	O
be	O
valid	O
,	O
we	O
must	O
ensure	O
that	O
the	O
set	O
of	O
variables	O
that	O
appears	O
in	O
each	O
of	O
the	O
conditional	B
distributions	O
is	O
a	O
member	O
of	O
at	O
least	O
one	O
clique	B
of	O
the	O
undirected	B
graph	I
.	O
for	O
nodes	O
on	O
the	O
directed	B
graph	O
having	O
just	O
one	O
parent	O
,	O
this	O
is	O
achieved	O
simply	O
by	O
replacing	O
the	O
directed	B
link	O
with	O
an	O
undirected	B
link	O
.	O
however	O
,	O
for	O
nodes	O
in	O
the	O
directed	B
graph	O
having	O
more	O
than	O
one	O
parent	O
,	O
this	O
is	O
not	O
sufﬁcient	O
.	O
these	O
are	O
nodes	O
that	O
have	O
‘	O
head-to-head	O
’	O
paths	O
encountered	O
in	O
our	O
discussion	O
of	O
conditional	B
independence	I
.	O
consider	O
a	O
simple	O
directed	B
graph	O
over	O
4	O
nodes	O
shown	O
in	O
figure	O
8.33.	O
the	O
joint	O
distribution	O
for	O
the	O
directed	B
graph	O
takes	O
the	O
form	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2	O
)	O
p	O
(	O
x3	O
)	O
p	O
(	O
x4|x1	O
,	O
x2	O
,	O
x3	O
)	O
.	O
(	O
8.46	O
)	O
we	O
see	O
that	O
the	O
factor	O
p	O
(	O
x4|x1	O
,	O
x2	O
,	O
x3	O
)	O
involves	O
the	O
four	O
variables	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
and	O
x4	O
,	O
and	O
so	O
these	O
must	O
all	O
belong	O
to	O
a	O
single	O
clique	B
if	O
this	O
conditional	B
distribution	O
is	O
to	O
be	O
absorbed	O
into	O
a	O
clique	B
potential	O
.	O
to	O
ensure	O
this	O
,	O
we	O
add	O
extra	O
links	O
between	O
all	O
pairs	O
of	O
parents	O
of	O
the	O
node	B
x4	O
.	O
anachronistically	O
,	O
this	O
process	O
of	O
‘	O
marrying	O
the	O
parents	O
’	O
has	O
become	O
known	O
as	O
moralization	B
,	O
and	O
the	O
resulting	O
undirected	B
graph	I
,	O
after	O
dropping	O
the	O
arrows	O
,	O
is	O
called	O
the	O
moral	O
graph	O
.	O
it	O
is	O
important	O
to	O
observe	O
that	O
the	O
moral	O
graph	O
in	O
this	O
example	O
is	O
fully	B
connected	I
and	O
so	O
exhibits	O
no	O
conditional	B
independence	I
properties	O
,	O
in	O
contrast	O
to	O
the	O
original	O
directed	B
graph	O
.	O
thus	O
in	O
general	O
to	O
convert	O
a	O
directed	B
graph	O
into	O
an	O
undirected	B
graph	I
,	O
we	O
ﬁrst	O
add	O
additional	O
undirected	B
links	O
between	O
all	O
pairs	O
of	O
parents	O
for	O
each	O
node	B
in	O
the	O
graph	O
and	O
392	O
8.	O
graphical	O
models	O
section	O
8.4	O
section	O
8.2	O
then	O
drop	O
the	O
arrows	O
on	O
the	O
original	O
links	O
to	O
give	O
the	O
moral	O
graph	O
.	O
then	O
we	O
initialize	O
all	O
of	O
the	O
clique	B
potentials	O
of	O
the	O
moral	O
graph	O
to	O
1.	O
we	O
then	O
take	O
each	O
conditional	B
distribution	O
factor	O
in	O
the	O
original	O
directed	B
graph	O
and	O
multiply	O
it	O
into	O
one	O
of	O
the	O
clique	B
potentials	O
.	O
there	O
will	O
always	O
exist	O
at	O
least	O
one	O
maximal	B
clique	I
that	O
contains	O
all	O
of	O
the	O
variables	O
in	O
the	O
factor	O
as	O
a	O
result	O
of	O
the	O
moralization	B
step	O
.	O
note	O
that	O
in	O
all	O
cases	O
the	O
partition	B
function	I
is	O
given	O
by	O
z	O
=	O
1.	O
the	O
process	O
of	O
converting	O
a	O
directed	B
graph	O
into	O
an	O
undirected	B
graph	I
plays	O
an	O
important	O
role	O
in	O
exact	O
inference	O
techniques	O
such	O
as	O
the	O
junction	B
tree	I
algorithm	I
.	O
converting	O
from	O
an	O
undirected	B
to	O
a	O
directed	B
representation	O
is	O
much	O
less	O
common	O
and	O
in	O
general	O
presents	O
problems	O
due	O
to	O
the	O
normalization	O
constraints	O
.	O
we	O
saw	O
that	O
in	O
going	O
from	O
a	O
directed	B
to	O
an	O
undirected	B
representation	O
we	O
had	O
to	O
discard	O
some	O
conditional	B
independence	I
properties	O
from	O
the	O
graph	O
.	O
of	O
course	O
,	O
we	O
could	O
always	O
trivially	O
convert	O
any	O
distribution	O
over	O
a	O
directed	B
graph	O
into	O
one	O
over	O
an	O
undirected	B
graph	I
by	O
simply	O
using	O
a	O
fully	B
connected	I
undirected	O
graph	O
.	O
this	O
would	O
,	O
however	O
,	O
discard	O
all	O
conditional	B
independence	I
properties	O
and	O
so	O
would	O
be	O
vacuous	O
.	O
the	O
process	O
of	O
moralization	B
adds	O
the	O
fewest	O
extra	O
links	O
and	O
so	O
retains	O
the	O
maximum	O
number	O
of	O
independence	O
properties	O
.	O
we	O
have	O
seen	O
that	O
the	O
procedure	O
for	O
determining	O
the	O
conditional	B
independence	I
properties	O
is	O
different	O
between	O
directed	B
and	O
undirected	B
graphs	O
.	O
it	O
turns	O
out	O
that	O
the	O
two	O
types	O
of	O
graph	O
can	O
express	O
different	O
conditional	B
independence	I
properties	O
,	O
and	O
it	O
is	O
worth	O
exploring	O
this	O
issue	O
in	O
more	O
detail	O
.	O
to	O
do	O
so	O
,	O
we	O
return	O
to	O
the	O
view	O
of	O
a	O
speciﬁc	O
(	O
directed	B
or	O
undirected	B
)	O
graph	O
as	O
a	O
ﬁlter	O
,	O
so	O
that	O
the	O
set	O
of	O
all	O
possible	O
distributions	O
over	O
the	O
given	O
variables	O
could	O
be	O
reduced	O
to	O
a	O
subset	O
that	O
respects	O
the	O
conditional	B
independencies	O
implied	O
by	O
the	O
graph	O
.	O
a	O
graph	O
is	O
said	O
to	O
be	O
a	O
d	O
map	O
(	O
for	O
‘	O
dependency	B
map	I
’	O
)	O
of	O
a	O
distribution	O
if	O
every	O
conditional	B
independence	I
statement	O
satisﬁed	O
by	O
the	O
distribution	O
is	O
reﬂected	O
in	O
the	O
graph	O
.	O
thus	O
a	O
completely	O
disconnected	O
graph	O
(	O
no	O
links	O
)	O
will	O
be	O
a	O
trivial	O
d	O
map	O
for	O
any	O
distribution	O
.	O
alternatively	O
,	O
we	O
can	O
consider	O
a	O
speciﬁc	O
distribution	O
and	O
ask	O
which	O
graphs	O
have	O
the	O
appropriate	O
conditional	B
independence	I
properties	O
.	O
if	O
every	O
conditional	B
indepen-	O
dence	O
statement	O
implied	O
by	O
a	O
graph	O
is	O
satisﬁed	O
by	O
a	O
speciﬁc	O
distribution	O
,	O
then	O
the	O
graph	O
is	O
said	O
to	O
be	O
an	O
i	O
map	O
(	O
for	O
‘	O
independence	B
map	I
’	O
)	O
of	O
that	O
distribution	O
.	O
clearly	O
a	O
fully	B
connected	I
graph	O
will	O
be	O
a	O
trivial	O
i	O
map	O
for	O
any	O
distribution	O
.	O
if	O
it	O
is	O
the	O
case	O
that	O
every	O
conditional	B
independence	I
property	O
of	O
the	O
distribution	O
is	O
reﬂected	O
in	O
the	O
graph	O
,	O
and	O
vice	O
versa	O
,	O
then	O
the	O
graph	O
is	O
said	O
to	O
be	O
a	O
perfect	B
map	I
for	O
figure	O
8.34	O
venn	O
diagram	O
illustrating	O
the	O
set	O
of	O
all	O
distributions	O
p	O
over	O
a	O
given	O
set	O
of	O
variables	O
,	O
together	O
with	O
the	O
set	O
of	O
distributions	O
d	O
that	O
can	O
be	O
represented	O
as	O
a	O
perfect	B
map	I
using	O
a	O
directed	B
graph	O
,	O
and	O
the	O
set	O
u	O
that	O
can	O
be	O
represented	O
as	O
a	O
perfect	B
map	I
using	O
an	O
undirected	B
graph	I
.	O
d	O
u	O
p	O
8.4.	O
inference	B
in	O
graphical	O
models	O
figure	O
8.35	O
a	O
directed	B
graph	O
whose	O
conditional	B
independence	I
properties	O
can	O
not	O
be	O
expressed	O
using	O
an	O
undirected	B
graph	I
over	O
the	O
same	O
three	O
variables	O
.	O
a	O
393	O
b	O
c	O
that	O
distribution	O
.	O
a	O
perfect	B
map	I
is	O
therefore	O
both	O
an	O
i	O
map	O
and	O
a	O
d	O
map	O
.	O
consider	O
the	O
set	O
of	O
distributions	O
such	O
that	O
for	O
each	O
distribution	O
there	O
exists	O
a	O
directed	B
graph	O
that	O
is	O
a	O
perfect	B
map	I
.	O
this	O
set	O
is	O
distinct	O
from	O
the	O
set	O
of	O
distributions	O
such	O
that	O
for	O
each	O
distribution	O
there	O
exists	O
an	O
undirected	B
graph	I
that	O
is	O
a	O
perfect	B
map	I
.	O
in	O
addition	O
there	O
are	O
distributions	O
for	O
which	O
neither	O
directed	B
nor	O
undirected	B
graphs	O
offer	O
a	O
perfect	B
map	I
.	O
this	O
is	O
illustrated	O
as	O
a	O
venn	O
diagram	O
in	O
figure	O
8.34.	O
figure	O
8.35	O
shows	O
an	O
example	O
of	O
a	O
directed	B
graph	O
that	O
is	O
a	O
perfect	B
map	I
for	O
a	O
distribution	O
satisfying	O
the	O
conditional	B
independence	I
properties	O
a	O
⊥⊥	O
b	O
|	O
∅	O
and	O
a	O
(	O
cid:9	O
)	O
⊥⊥	O
b	O
|	O
c.	O
there	O
is	O
no	O
corresponding	O
undirected	B
graph	I
over	O
the	O
same	O
three	O
vari-	O
ables	O
that	O
is	O
a	O
perfect	B
map	I
.	O
conversely	O
,	O
consider	O
the	O
undirected	B
graph	I
over	O
four	O
variables	O
shown	O
in	O
fig-	O
ure	O
8.36.	O
this	O
graph	O
exhibits	O
the	O
properties	O
a	O
(	O
cid:9	O
)	O
⊥⊥	O
b	O
|	O
∅	O
,	O
c	O
⊥⊥	O
d	O
|	O
a	O
∪	O
b	O
and	O
a	O
⊥⊥	O
b	O
|	O
c	O
∪	O
d.	O
there	O
is	O
no	O
directed	B
graph	O
over	O
four	O
variables	O
that	O
implies	O
the	O
same	O
set	O
of	O
conditional	B
independence	I
properties	O
.	O
the	O
graphical	O
framework	O
can	O
be	O
extended	B
in	O
a	O
consistent	B
way	O
to	O
graphs	O
that	O
include	O
both	O
directed	B
and	O
undirected	B
links	O
.	O
these	O
are	O
called	O
chain	O
graphs	O
(	O
lauritzen	O
and	O
wermuth	O
,	O
1989	O
;	O
frydenberg	O
,	O
1990	O
)	O
,	O
and	O
contain	O
the	O
directed	B
and	O
undirected	B
graphs	O
considered	O
so	O
far	O
as	O
special	O
cases	O
.	O
although	O
such	O
graphs	O
can	O
represent	O
a	O
broader	O
class	O
of	O
distributions	O
than	O
either	O
directed	B
or	O
undirected	B
alone	O
,	O
there	O
remain	O
distributions	O
for	O
which	O
even	O
a	O
chain	B
graph	I
can	O
not	O
provide	O
a	O
perfect	B
map	I
.	O
chain	O
graphs	O
are	O
not	O
discussed	O
further	O
in	O
this	O
book	O
.	O
figure	O
8.36	O
an	O
undirected	B
graph	I
whose	O
conditional	B
independence	I
properties	O
can	O
not	O
be	O
expressed	O
in	O
terms	O
of	O
a	O
directed	B
graph	O
over	O
the	O
same	O
variables	O
.	O
a	O
b	O
c	O
d	O
8.4.	O
inference	B
in	O
graphical	O
models	O
we	O
turn	O
now	O
to	O
the	O
problem	O
of	O
inference	B
in	O
graphical	O
models	O
,	O
in	O
which	O
some	O
of	O
the	O
nodes	O
in	O
a	O
graph	O
are	O
clamped	O
to	O
observed	O
values	O
,	O
and	O
we	O
wish	O
to	O
compute	O
the	O
posterior	O
distributions	O
of	O
one	O
or	O
more	O
subsets	O
of	O
other	O
nodes	O
.	O
as	O
we	O
shall	O
see	O
,	O
we	O
can	O
exploit	O
the	O
graphical	O
structure	O
both	O
to	O
ﬁnd	O
efﬁcient	O
algorithms	O
for	O
inference	O
,	O
and	O
394	O
8.	O
graphical	O
models	O
figure	O
8.37	O
a	O
graphical	O
representation	O
of	O
bayes	O
’	O
see	O
the	O
text	O
for	O
details	O
.	O
theorem	O
.	O
x	O
y	O
x	O
y	O
x	O
y	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
to	O
make	O
the	O
structure	O
of	O
those	O
algorithms	O
transparent	O
.	O
speciﬁcally	O
,	O
we	O
shall	O
see	O
that	O
many	O
algorithms	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
propagation	O
of	O
local	B
messages	O
around	O
the	O
graph	O
.	O
in	O
this	O
section	O
,	O
we	O
shall	O
focus	O
primarily	O
on	O
techniques	O
for	O
exact	O
inference	B
,	O
and	O
in	O
chapter	O
10	O
we	O
shall	O
consider	O
a	O
number	O
of	O
approximate	O
inference	B
algorithms	O
.	O
to	O
start	O
with	O
,	O
let	O
us	O
consider	O
the	O
graphical	O
interpretation	O
of	O
bayes	O
’	O
theorem	O
.	O
suppose	O
we	O
decompose	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
y	O
)	O
over	O
two	O
variables	O
x	O
and	O
y	O
into	O
a	O
product	O
of	O
factors	O
in	O
the	O
form	O
p	O
(	O
x	O
,	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y|x	O
)	O
.	O
this	O
can	O
be	O
represented	O
by	O
the	O
directed	B
graph	O
shown	O
in	O
figure	O
8.37	O
(	O
a	O
)	O
.	O
now	O
suppose	O
we	O
observe	O
the	O
value	O
of	O
y	O
,	O
as	O
indicated	O
by	O
the	O
shaded	O
node	B
in	O
figure	O
8.37	O
(	O
b	O
)	O
.	O
we	O
can	O
view	O
the	O
marginal	B
distribution	O
p	O
(	O
x	O
)	O
as	O
a	O
prior	B
over	O
the	O
latent	B
variable	I
x	O
,	O
and	O
our	O
goal	O
is	O
to	O
infer	O
the	O
corresponding	O
posterior	O
distribution	O
over	O
x.	O
using	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
we	O
can	O
evaluate	O
(	O
cid:2	O
)	O
x	O
(	O
cid:1	O
)	O
p	O
(	O
y	O
)	O
=	O
p	O
(	O
y|x	O
(	O
cid:4	O
)	O
)	O
p	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
(	O
8.47	O
)	O
which	O
can	O
then	O
be	O
used	O
in	O
bayes	O
’	O
theorem	O
to	O
calculate	O
p	O
(	O
x|y	O
)	O
=	O
p	O
(	O
y|x	O
)	O
p	O
(	O
x	O
)	O
.	O
(	O
8.48	O
)	O
thus	O
the	O
joint	O
distribution	O
is	O
now	O
expressed	O
in	O
terms	O
of	O
p	O
(	O
y	O
)	O
and	O
p	O
(	O
x|y	O
)	O
.	O
from	O
a	O
graphical	O
perspective	O
,	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
y	O
)	O
is	O
now	O
represented	O
by	O
the	O
graph	O
shown	O
in	O
figure	O
8.37	O
(	O
c	O
)	O
,	O
in	O
which	O
the	O
direction	O
of	O
the	O
arrow	O
is	O
reversed	O
.	O
this	O
is	O
the	O
simplest	O
example	O
of	O
an	O
inference	B
problem	O
for	O
a	O
graphical	B
model	I
.	O
p	O
(	O
y	O
)	O
8.4.1	O
inference	B
on	O
a	O
chain	O
now	O
consider	O
a	O
more	O
complex	O
problem	O
involving	O
the	O
chain	O
of	O
nodes	O
of	O
the	O
form	O
shown	O
in	O
figure	O
8.32.	O
this	O
example	O
will	O
lay	O
the	O
foundation	O
for	O
a	O
discussion	O
of	O
exact	O
inference	O
in	O
more	O
general	O
graphs	O
later	O
in	O
this	O
section	O
.	O
speciﬁcally	O
,	O
we	O
shall	O
consider	O
the	O
undirected	B
graph	I
in	O
figure	O
8.32	O
(	O
b	O
)	O
.	O
we	O
have	O
already	O
seen	O
that	O
the	O
directed	B
chain	O
can	O
be	O
transformed	O
into	O
an	O
equivalent	O
undirected	O
chain	O
.	O
because	O
the	O
directed	B
graph	O
does	O
not	O
have	O
any	O
nodes	O
with	O
more	O
than	O
one	O
parent	O
,	O
this	O
does	O
not	O
require	O
the	O
addition	O
of	O
any	O
extra	O
links	O
,	O
and	O
the	O
directed	B
and	O
undirected	B
versions	O
of	O
this	O
graph	O
express	O
exactly	O
the	O
same	O
set	O
of	O
conditional	B
inde-	O
pendence	O
statements	O
.	O
8.4.	O
inference	B
in	O
graphical	O
models	O
395	O
the	O
joint	O
distribution	O
for	O
this	O
graph	O
takes	O
the	O
form	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
ψ1,2	O
(	O
x1	O
,	O
x2	O
)	O
ψ2,3	O
(	O
x2	O
,	O
x3	O
)	O
···	O
ψn−1	O
,	O
n	O
(	O
xn−1	O
,	O
xn	O
)	O
.	O
(	O
8.49	O
)	O
we	O
shall	O
consider	O
the	O
speciﬁc	O
case	O
in	O
which	O
the	O
n	O
nodes	O
represent	O
discrete	O
vari-	O
ables	O
each	O
having	O
k	O
states	O
,	O
in	O
which	O
case	O
each	O
potential	B
function	I
ψn−1	O
,	O
n	O
(	O
xn−1	O
,	O
xn	O
)	O
comprises	O
an	O
k	O
×	O
k	O
table	O
,	O
and	O
so	O
the	O
joint	O
distribution	O
has	O
(	O
n	O
−	O
1	O
)	O
k	O
2	O
parameters	O
.	O
let	O
us	O
consider	O
the	O
inference	B
problem	O
of	O
ﬁnding	O
the	O
marginal	B
distribution	O
p	O
(	O
xn	O
)	O
for	O
a	O
speciﬁc	O
node	B
xn	O
that	O
is	O
part	O
way	O
along	O
the	O
chain	O
.	O
note	O
that	O
,	O
for	O
the	O
moment	O
,	O
there	O
are	O
no	O
observed	O
nodes	O
.	O
by	O
deﬁnition	O
,	O
the	O
required	O
marginal	B
is	O
obtained	O
by	O
summing	O
the	O
joint	O
distribution	O
over	O
all	O
variables	O
except	O
xn	O
,	O
so	O
that	O
(	O
cid:2	O
)	O
···	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
···	O
p	O
(	O
xn	O
)	O
=	O
p	O
(	O
x	O
)	O
.	O
(	O
8.50	O
)	O
x1	O
xn−1	O
xn+1	O
xn	O
in	O
a	O
naive	O
implementation	O
,	O
we	O
would	O
ﬁrst	O
evaluate	O
the	O
joint	O
distribution	O
and	O
then	O
perform	O
the	O
summations	O
explicitly	O
.	O
the	O
joint	O
distribution	O
can	O
be	O
represented	O
as	O
a	O
set	O
of	O
numbers	O
,	O
one	O
for	O
each	O
possible	O
value	O
for	O
x.	O
because	O
there	O
are	O
n	O
variables	O
each	O
with	O
k	O
states	O
,	O
there	O
are	O
k	O
n	O
values	O
for	O
x	O
and	O
so	O
evaluation	O
and	O
storage	O
of	O
the	O
joint	O
distribution	O
,	O
as	O
well	O
as	O
marginalization	O
to	O
obtain	O
p	O
(	O
xn	O
)	O
,	O
all	O
involve	O
storage	O
and	O
computation	O
that	O
scale	O
exponentially	O
with	O
the	O
length	O
n	O
of	O
the	O
chain	O
.	O
we	O
can	O
,	O
however	O
,	O
obtain	O
a	O
much	O
more	O
efﬁcient	O
algorithm	O
by	O
exploiting	O
the	O
con-	O
ditional	O
independence	O
properties	O
of	O
the	O
graphical	B
model	I
.	O
if	O
we	O
substitute	O
the	O
factor-	O
ized	O
expression	O
(	O
8.49	O
)	O
for	O
the	O
joint	O
distribution	O
into	O
(	O
8.50	O
)	O
,	O
then	O
we	O
can	O
rearrange	O
the	O
order	O
of	O
the	O
summations	O
and	O
the	O
multiplications	O
to	O
allow	O
the	O
required	O
marginal	B
to	O
be	O
evaluated	O
much	O
more	O
efﬁciently	O
.	O
consider	O
for	O
instance	O
the	O
summation	O
over	O
xn	O
.	O
the	O
potential	O
ψn−1	O
,	O
n	O
(	O
xn−1	O
,	O
xn	O
)	O
is	O
the	O
only	O
one	O
that	O
depends	O
on	O
xn	O
,	O
and	O
so	O
we	O
can	O
perform	O
the	O
summation	O
(	O
cid:2	O
)	O
ψn−1	O
,	O
n	O
(	O
xn−1	O
,	O
xn	O
)	O
(	O
8.51	O
)	O
xn	O
ﬁrst	O
to	O
give	O
a	O
function	O
of	O
xn−1	O
.	O
we	O
can	O
then	O
use	O
this	O
to	O
perform	O
the	O
summation	O
over	O
xn−1	O
,	O
which	O
will	O
involve	O
only	O
this	O
new	O
function	O
together	O
with	O
the	O
potential	O
ψn−2	O
,	O
n−1	O
(	O
xn−2	O
,	O
xn−1	O
)	O
,	O
because	O
this	O
is	O
the	O
only	O
other	O
place	O
that	O
xn−1	O
appears	O
.	O
similarly	O
,	O
the	O
summation	O
over	O
x1	O
involves	O
only	O
the	O
potential	O
ψ1,2	O
(	O
x1	O
,	O
x2	O
)	O
and	O
so	O
can	O
be	O
performed	O
separately	O
to	O
give	O
a	O
function	O
of	O
x2	O
,	O
and	O
so	O
on	O
.	O
because	O
each	O
summation	O
effectively	O
removes	O
a	O
variable	O
from	O
the	O
distribution	O
,	O
this	O
can	O
be	O
viewed	O
as	O
the	O
removal	O
of	O
a	O
node	B
from	O
the	O
graph	O
.	O
if	O
we	O
group	O
the	O
potentials	O
and	O
summations	O
together	O
in	O
this	O
way	O
,	O
we	O
can	O
express	O
396	O
8.	O
graphical	O
models	O
the	O
desired	O
marginal	B
in	O
the	O
form	O
ψn−1	O
,	O
n	O
(	O
xn−1	O
,	O
xn	O
)	O
···	O
p	O
(	O
xn	O
)	O
=	O
1	O
xn−1	O
z⎡⎣	O
(	O
cid:2	O
)	O
(	O
⎡⎣	O
(	O
cid:2	O
)	O
(	O
xn+1	O
(	O
cid:31	O
)	O
(	O
cid:2	O
)	O
x1	O
ψ2,3	O
(	O
x2	O
,	O
x3	O
)	O
)	O
*	O
µα	O
(	O
xn	O
)	O
x2	O
(	O
cid:31	O
)	O
(	O
cid:2	O
)	O
(	O
cid:31	O
)	O
(	O
cid:2	O
)	O
)	O
*	O
xn	O
µβ	O
(	O
xn	O
)	O
⎤⎦	O
+	O
···	O
ψ1,2	O
(	O
x1	O
,	O
x2	O
)	O
⎤⎦	O
+	O
ψn	O
,	O
n+1	O
(	O
xn	O
,	O
xn+1	O
)	O
···	O
ψn−1	O
,	O
n	O
(	O
xn−1	O
,	O
xn	O
)	O
···	O
.	O
(	O
8.52	O
)	O
the	O
reader	O
is	O
encouraged	O
to	O
study	O
this	O
re-ordering	O
carefully	O
as	O
the	O
underlying	O
idea	O
forms	O
the	O
basis	O
for	O
the	O
later	O
discussion	O
of	O
the	O
general	O
sum-product	B
algorithm	I
.	O
here	O
the	O
key	O
concept	O
that	O
we	O
are	O
exploiting	O
is	O
that	O
multiplication	O
is	O
distributive	O
over	O
addi-	O
tion	O
,	O
so	O
that	O
ab	O
+	O
ac	O
=	O
a	O
(	O
b	O
+	O
c	O
)	O
(	O
8.53	O
)	O
in	O
which	O
the	O
left-hand	O
side	O
involves	O
three	O
arithmetic	O
operations	O
whereas	O
the	O
right-	O
hand	O
side	O
reduces	O
this	O
to	O
two	O
operations	O
.	O
let	O
us	O
work	O
out	O
the	O
computational	O
cost	O
of	O
evaluating	O
the	O
required	O
marginal	B
using	O
this	O
re-ordered	O
expression	O
.	O
we	O
have	O
to	O
perform	O
n	O
−	O
1	O
summations	O
each	O
of	O
which	O
is	O
over	O
k	O
states	O
and	O
each	O
of	O
which	O
involves	O
a	O
function	O
of	O
two	O
variables	O
.	O
for	O
instance	O
,	O
the	O
summation	O
over	O
x1	O
involves	O
only	O
the	O
function	O
ψ1,2	O
(	O
x1	O
,	O
x2	O
)	O
,	O
which	O
is	O
a	O
table	O
of	O
k	O
×	O
k	O
numbers	O
.	O
we	O
have	O
to	O
sum	O
this	O
table	O
over	O
x1	O
for	O
each	O
value	O
of	O
x2	O
and	O
so	O
this	O
has	O
o	O
(	O
k	O
2	O
)	O
cost	O
.	O
the	O
resulting	O
vector	O
of	O
k	O
numbers	O
is	O
multiplied	O
by	O
the	O
matrix	O
of	O
numbers	O
ψ2,3	O
(	O
x2	O
,	O
x3	O
)	O
and	O
so	O
is	O
again	O
o	O
(	O
k	O
2	O
)	O
.	O
because	O
there	O
are	O
n	O
−	O
1	O
summations	O
and	O
multiplications	O
of	O
this	O
kind	O
,	O
the	O
total	O
cost	O
of	O
evaluating	O
the	O
marginal	B
p	O
(	O
xn	O
)	O
is	O
o	O
(	O
n	O
k	O
2	O
)	O
.	O
this	O
is	O
linear	O
in	O
the	O
length	O
of	O
the	O
chain	O
,	O
in	O
contrast	O
to	O
the	O
exponential	O
cost	O
of	O
a	O
naive	O
approach	O
.	O
we	O
have	O
therefore	O
been	O
able	O
to	O
exploit	O
the	O
many	O
conditional	B
independence	I
properties	O
of	O
this	O
simple	O
graph	O
in	O
order	O
to	O
obtain	O
an	O
efﬁcient	O
calcula-	O
tion	O
.	O
if	O
the	O
graph	O
had	O
been	O
fully	B
connected	I
,	O
there	O
would	O
have	O
been	O
no	O
conditional	B
independence	I
properties	O
,	O
and	O
we	O
would	O
have	O
been	O
forced	O
to	O
work	O
directly	O
with	O
the	O
full	O
joint	O
distribution	O
.	O
we	O
now	O
give	O
a	O
powerful	O
interpretation	O
of	O
this	O
calculation	O
in	O
terms	O
of	O
the	O
passing	O
of	O
local	B
messages	O
around	O
on	O
the	O
graph	O
.	O
from	O
(	O
8.52	O
)	O
we	O
see	O
that	O
the	O
expression	O
for	O
the	O
marginal	B
p	O
(	O
xn	O
)	O
decomposes	O
into	O
the	O
product	O
of	O
two	O
factors	O
times	O
the	O
normalization	O
constant	O
p	O
(	O
xn	O
)	O
=	O
µα	O
(	O
xn	O
)	O
µβ	O
(	O
xn	O
)	O
.	O
(	O
8.54	O
)	O
1	O
z	O
we	O
shall	O
interpret	O
µα	O
(	O
xn	O
)	O
as	O
a	O
message	O
passed	O
forwards	O
along	O
the	O
chain	O
from	O
node	B
xn−1	O
to	O
node	B
xn	O
.	O
similarly	O
,	O
µβ	O
(	O
xn	O
)	O
can	O
be	O
viewed	O
as	O
a	O
message	O
passed	O
backwards	O
8.4.	O
inference	B
in	O
graphical	O
models	O
397	O
µα	O
(	O
xn−1	O
)	O
µα	O
(	O
xn	O
)	O
µβ	O
(	O
xn	O
)	O
µβ	O
(	O
xn+1	O
)	O
x1	O
xn−1	O
xn	O
xn+1	O
xn	O
figure	O
8.38	O
the	O
marginal	B
distribution	O
p	O
(	O
xn	O
)	O
for	O
a	O
node	B
xn	O
along	O
the	O
chain	O
is	O
ob-	O
tained	O
by	O
multiplying	O
the	O
two	O
messages	O
µα	O
(	O
xn	O
)	O
and	O
µβ	O
(	O
xn	O
)	O
,	O
and	O
then	O
normaliz-	O
ing	O
.	O
these	O
messages	O
can	O
themselves	O
be	O
evaluated	O
recursively	O
by	O
passing	O
mes-	O
sages	O
from	O
both	O
ends	O
of	O
the	O
chain	O
to-	O
wards	O
node	B
xn	O
.	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
xn−1	O
⎡⎣	O
(	O
cid:2	O
)	O
⎤⎦	O
···	O
along	O
the	O
chain	O
to	O
node	B
xn	O
from	O
node	B
xn+1	O
.	O
note	O
that	O
each	O
of	O
the	O
messages	O
com-	O
prises	O
a	O
set	O
of	O
k	O
values	O
,	O
one	O
for	O
each	O
choice	O
of	O
xn	O
,	O
and	O
so	O
the	O
product	O
of	O
two	O
mes-	O
sages	O
should	O
be	O
interpreted	O
as	O
the	O
point-wise	O
multiplication	O
of	O
the	O
elements	O
of	O
the	O
two	O
messages	O
to	O
give	O
another	O
set	O
of	O
k	O
values	O
.	O
the	O
message	O
µα	O
(	O
xn	O
)	O
can	O
be	O
evaluated	O
recursively	O
because	O
µα	O
(	O
xn	O
)	O
=	O
=	O
ψn−1	O
,	O
n	O
(	O
xn−1	O
,	O
xn	O
)	O
xn−2	O
ψn−1	O
,	O
n	O
(	O
xn−1	O
,	O
xn	O
)	O
µα	O
(	O
xn−1	O
)	O
.	O
(	O
8.55	O
)	O
we	O
therefore	O
ﬁrst	O
evaluate	O
xn−1	O
µα	O
(	O
x2	O
)	O
=	O
(	O
cid:2	O
)	O
x1	O
ψ1,2	O
(	O
x1	O
,	O
x2	O
)	O
(	O
8.56	O
)	O
and	O
then	O
apply	O
(	O
8.55	O
)	O
repeatedly	O
until	O
we	O
reach	O
the	O
desired	O
node	B
.	O
note	O
carefully	O
the	O
structure	O
of	O
the	O
message	B
passing	I
equation	O
.	O
the	O
outgoing	O
message	O
µα	O
(	O
xn	O
)	O
in	O
(	O
8.55	O
)	O
is	O
obtained	O
by	O
multiplying	O
the	O
incoming	O
message	O
µα	O
(	O
xn−1	O
)	O
by	O
the	O
local	B
potential	O
involving	O
the	O
node	B
variable	O
and	O
the	O
outgoing	O
variable	O
and	O
then	O
summing	O
over	O
the	O
node	B
variable	O
.	O
similarly	O
,	O
the	O
message	O
µβ	O
(	O
xn	O
)	O
can	O
be	O
evaluated	O
recursively	O
by	O
starting	O
with	O
node	B
xn	O
and	O
using	O
µβ	O
(	O
xn	O
)	O
=	O
=	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
xn+1	O
⎡⎣	O
(	O
cid:2	O
)	O
⎤⎦	O
···	O
ψn+1	O
,	O
n	O
(	O
xn+1	O
,	O
xn	O
)	O
xn+2	O
ψn+1	O
,	O
n	O
(	O
xn+1	O
,	O
xn	O
)	O
µβ	O
(	O
xn+1	O
)	O
.	O
(	O
8.57	O
)	O
xn+1	O
this	O
recursive	O
message	B
passing	I
is	O
illustrated	O
in	O
figure	O
8.38.	O
the	O
normalization	O
con-	O
stant	O
z	O
is	O
easily	O
evaluated	O
by	O
summing	O
the	O
right-hand	O
side	O
of	O
(	O
8.54	O
)	O
over	O
all	O
states	O
of	O
xn	O
,	O
an	O
operation	O
that	O
requires	O
only	O
o	O
(	O
k	O
)	O
computation	O
.	O
graphs	O
of	O
the	O
form	O
shown	O
in	O
figure	O
8.38	O
are	O
called	O
markov	O
chains	O
,	O
and	O
the	O
corresponding	O
message	B
passing	I
equations	O
represent	O
an	O
example	O
of	O
the	O
chapman-	O
kolmogorov	O
equations	O
for	O
markov	O
processes	O
(	O
papoulis	O
,	O
1984	O
)	O
.	O
398	O
8.	O
graphical	O
models	O
now	O
suppose	O
we	O
wish	O
to	O
evaluate	O
the	O
marginals	O
p	O
(	O
xn	O
)	O
for	O
every	O
node	B
n	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
in	O
the	O
chain	O
.	O
simply	O
applying	O
the	O
above	O
procedure	O
separately	O
for	O
each	O
node	B
will	O
have	O
computational	O
cost	O
that	O
is	O
o	O
(	O
n	O
2m	O
2	O
)	O
.	O
however	O
,	O
such	O
an	O
approach	O
would	O
be	O
very	O
wasteful	O
of	O
computation	O
.	O
for	O
instance	O
,	O
to	O
ﬁnd	O
p	O
(	O
x1	O
)	O
we	O
need	O
to	O
prop-	O
agate	O
a	O
message	O
µβ	O
(	O
·	O
)	O
from	O
node	B
xn	O
back	O
to	O
node	B
x2	O
.	O
similarly	O
,	O
to	O
evaluate	O
p	O
(	O
x2	O
)	O
we	O
need	O
to	O
propagate	O
a	O
messages	O
µβ	O
(	O
·	O
)	O
from	O
node	B
xn	O
back	O
to	O
node	B
x3	O
.	O
this	O
will	O
involve	O
much	O
duplicated	O
computation	O
because	O
most	O
of	O
the	O
messages	O
will	O
be	O
identical	O
in	O
the	O
two	O
cases	O
.	O
suppose	O
instead	O
we	O
ﬁrst	O
launch	O
a	O
message	O
µβ	O
(	O
xn−1	O
)	O
starting	O
from	O
node	B
xn	O
and	O
propagate	O
corresponding	O
messages	O
all	O
the	O
way	O
back	O
to	O
node	B
x1	O
,	O
and	O
suppose	O
we	O
similarly	O
launch	O
a	O
message	O
µα	O
(	O
x2	O
)	O
starting	O
from	O
node	B
x1	O
and	O
propagate	O
the	O
corre-	O
sponding	O
messages	O
all	O
the	O
way	O
forward	O
to	O
node	B
xn	O
.	O
provided	O
we	O
store	O
all	O
of	O
the	O
intermediate	O
messages	O
along	O
the	O
way	O
,	O
then	O
any	O
node	B
can	O
evaluate	O
its	O
marginal	B
sim-	O
ply	O
by	O
applying	O
(	O
8.54	O
)	O
.	O
the	O
computational	O
cost	O
is	O
only	O
twice	O
that	O
for	O
ﬁnding	O
the	O
marginal	B
of	O
a	O
single	O
node	B
,	O
rather	O
than	O
n	O
times	O
as	O
much	O
.	O
observe	O
that	O
a	O
message	O
has	O
passed	O
once	O
in	O
each	O
direction	O
across	O
each	O
link	B
in	O
the	O
graph	O
.	O
note	O
also	O
that	O
the	O
normalization	O
constant	O
z	O
need	O
be	O
evaluated	O
only	O
once	O
,	O
using	O
any	O
convenient	O
node	B
.	O
if	O
some	O
of	O
the	O
nodes	O
in	O
the	O
graph	O
are	O
observed	O
,	O
then	O
the	O
corresponding	O
variables	O
are	O
simply	O
clamped	O
to	O
their	O
observed	O
values	O
and	O
there	O
is	O
no	O
summation	O
.	O
to	O
see	O
this	O
,	O
note	O
that	O
the	O
effect	O
of	O
clamping	O
a	O
variable	O
xn	O
to	O
an	O
observed	O
value	O
(	O
cid:1	O
)	O
xn	O
can	O
additional	O
function	O
i	O
(	O
xn	O
,	O
(	O
cid:1	O
)	O
xn	O
)	O
,	O
which	O
takes	O
the	O
value	O
1	O
when	O
xn	O
=	O
(	O
cid:1	O
)	O
xn	O
and	O
the	O
value	O
contain	O
xn	O
.	O
summations	O
over	O
xn	O
then	O
contain	O
only	O
one	O
term	O
in	O
which	O
xn	O
=	O
(	O
cid:1	O
)	O
xn	O
.	O
0	O
otherwise	O
.	O
one	O
such	O
function	O
can	O
then	O
be	O
absorbed	O
into	O
each	O
of	O
the	O
potentials	O
that	O
be	O
expressed	O
by	O
multiplying	O
the	O
joint	O
distribution	O
by	O
(	O
one	O
or	O
more	O
copies	O
of	O
)	O
an	O
now	O
suppose	O
we	O
wish	O
to	O
calculate	O
the	O
joint	O
distribution	O
p	O
(	O
xn−1	O
,	O
xn	O
)	O
for	O
two	O
neighbouring	O
nodes	O
on	O
the	O
chain	O
.	O
this	O
is	O
similar	O
to	O
the	O
evaluation	O
of	O
the	O
marginal	B
for	O
a	O
single	O
node	B
,	O
except	O
that	O
there	O
are	O
now	O
two	O
variables	O
that	O
are	O
not	O
summed	O
out	O
.	O
a	O
few	O
moments	O
thought	O
will	O
show	O
that	O
the	O
required	O
joint	O
distribution	O
can	O
be	O
written	O
in	O
the	O
form	O
p	O
(	O
xn−1	O
,	O
xn	O
)	O
=	O
1	O
z	O
µα	O
(	O
xn−1	O
)	O
ψn−1	O
,	O
n	O
(	O
xn−1	O
,	O
xn	O
)	O
µβ	O
(	O
xn	O
)	O
.	O
(	O
8.58	O
)	O
thus	O
we	O
can	O
obtain	O
the	O
joint	O
distributions	O
over	O
all	O
of	O
the	O
sets	O
of	O
variables	O
in	O
each	O
of	O
the	O
potentials	O
directly	O
once	O
we	O
have	O
completed	O
the	O
message	B
passing	I
required	O
to	O
obtain	O
the	O
marginals	O
.	O
this	O
is	O
a	O
useful	O
result	O
because	O
in	O
practice	O
we	O
may	O
wish	O
to	O
use	O
parametric	O
forms	O
for	O
the	O
clique	B
potentials	O
,	O
or	O
equivalently	O
for	O
the	O
conditional	B
distributions	O
if	O
we	O
started	O
from	O
a	O
directed	B
graph	O
.	O
in	O
order	O
to	O
learn	O
the	O
parameters	O
of	O
these	O
potentials	O
in	O
situa-	O
tions	O
where	O
not	O
all	O
of	O
the	O
variables	O
are	O
observed	O
,	O
we	O
can	O
employ	O
the	O
em	O
algorithm	O
,	O
and	O
it	O
turns	O
out	O
that	O
the	O
local	B
joint	O
distributions	O
of	O
the	O
cliques	O
,	O
conditioned	O
on	O
any	O
observed	O
data	O
,	O
is	O
precisely	O
what	O
is	O
needed	O
in	O
the	O
e	O
step	O
.	O
we	O
shall	O
consider	O
some	O
examples	O
of	O
this	O
in	O
detail	O
in	O
chapter	O
13	O
.	O
8.4.2	O
trees	O
we	O
have	O
seen	O
that	O
exact	O
inference	O
on	O
a	O
graph	O
comprising	O
a	O
chain	O
of	O
nodes	O
can	O
be	O
performed	O
efﬁciently	O
in	O
time	O
that	O
is	O
linear	O
in	O
the	O
number	O
of	O
nodes	O
,	O
using	O
an	O
algorithm	O
exercise	O
8.15	O
chapter	O
9	O
8.4.	O
inference	B
in	O
graphical	O
models	O
399	O
figure	O
8.39	O
examples	O
tree-	O
structured	O
graphs	O
,	O
showing	O
(	O
a	O
)	O
an	O
undirected	B
tree	O
,	O
(	O
b	O
)	O
a	O
directed	B
tree	O
,	O
and	O
(	O
c	O
)	O
a	O
directed	B
polytree	O
.	O
of	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
that	O
can	O
be	O
interpreted	O
in	O
terms	O
of	O
messages	O
passed	O
along	O
the	O
chain	O
.	O
more	O
generally	O
,	O
inference	B
can	O
be	O
performed	O
efﬁciently	O
using	O
local	B
message	O
passing	O
on	O
a	O
broader	O
class	O
of	O
graphs	O
called	O
trees	O
.	O
in	O
particular	O
,	O
we	O
shall	O
shortly	O
generalize	O
the	O
message	B
passing	I
formalism	O
derived	O
above	O
for	O
chains	O
to	O
give	O
the	O
sum-product	B
algorithm	I
,	O
which	O
provides	O
an	O
efﬁcient	O
framework	O
for	O
exact	O
inference	B
in	O
tree-structured	O
graphs	O
.	O
in	O
the	O
case	O
of	O
an	O
undirected	B
graph	I
,	O
a	O
tree	B
is	O
deﬁned	O
as	O
a	O
graph	O
in	O
which	O
there	O
is	O
one	O
,	O
and	O
only	O
one	O
,	O
path	O
between	O
any	O
pair	O
of	O
nodes	O
.	O
such	O
graphs	O
therefore	O
do	O
not	O
have	O
loops	O
.	O
in	O
the	O
case	O
of	O
directed	B
graphs	O
,	O
a	O
tree	B
is	O
deﬁned	O
such	O
that	O
there	O
is	O
a	O
single	O
node	B
,	O
called	O
the	O
root	O
,	O
which	O
has	O
no	O
parents	O
,	O
and	O
all	O
other	O
nodes	O
have	O
one	O
parent	O
.	O
if	O
we	O
convert	O
a	O
directed	B
tree	O
into	O
an	O
undirected	B
graph	I
,	O
we	O
see	O
that	O
the	O
moralization	B
step	O
will	O
not	O
add	O
any	O
links	O
as	O
all	O
nodes	O
have	O
at	O
most	O
one	O
parent	O
,	O
and	O
as	O
a	O
consequence	O
the	O
corresponding	O
moralized	O
graph	O
will	O
be	O
an	O
undirected	B
tree	O
.	O
examples	O
of	O
undirected	B
and	O
directed	B
trees	O
are	O
shown	O
in	O
figure	O
8.39	O
(	O
a	O
)	O
and	O
8.39	O
(	O
b	O
)	O
.	O
note	O
that	O
a	O
distribution	O
represented	O
as	O
a	O
directed	B
tree	O
can	O
easily	O
be	O
converted	O
into	O
one	O
represented	O
by	O
an	O
undirected	B
tree	O
,	O
and	O
vice	O
versa	O
.	O
if	O
there	O
are	O
nodes	O
in	O
a	O
directed	B
graph	O
that	O
have	O
more	O
than	O
one	O
parent	O
,	O
but	O
there	O
is	O
still	O
only	O
one	O
path	O
(	O
ignoring	O
the	O
direction	O
of	O
the	O
arrows	O
)	O
between	O
any	O
two	O
nodes	O
,	O
then	O
the	O
graph	O
is	O
a	O
called	O
a	O
polytree	B
,	O
as	O
illustrated	O
in	O
figure	O
8.39	O
(	O
c	O
)	O
.	O
such	O
a	O
graph	O
will	O
have	O
more	O
than	O
one	O
node	B
with	O
the	O
property	O
of	O
having	O
no	O
parents	O
,	O
and	O
furthermore	O
,	O
the	O
corresponding	O
moralized	O
undirected	B
graph	I
will	O
have	O
loops	O
.	O
8.4.3	O
factor	O
graphs	O
the	O
sum-product	B
algorithm	I
that	O
we	O
derive	O
in	O
the	O
next	O
section	O
is	O
applicable	O
to	O
undirected	B
and	O
directed	B
trees	O
and	O
to	O
polytrees	O
.	O
it	O
can	O
be	O
cast	O
in	O
a	O
particularly	O
simple	O
and	O
general	O
form	O
if	O
we	O
ﬁrst	O
introduce	O
a	O
new	O
graphical	O
construction	O
called	O
a	O
factor	B
graph	I
(	O
frey	O
,	O
1998	O
;	O
kschischnang	O
et	O
al.	O
,	O
2001	O
)	O
.	O
exercise	O
8.18	O
both	O
directed	B
and	O
undirected	B
graphs	O
allow	O
a	O
global	O
function	O
of	O
several	O
vari-	O
ables	O
to	O
be	O
expressed	O
as	O
a	O
product	O
of	O
factors	O
over	O
subsets	O
of	O
those	O
variables	O
.	O
factor	O
graphs	O
make	O
this	O
decomposition	O
explicit	O
by	O
introducing	O
additional	O
nodes	O
for	O
the	O
fac-	O
tors	O
themselves	O
in	O
addition	O
to	O
the	O
nodes	O
representing	O
the	O
variables	O
.	O
they	O
also	O
allow	O
us	O
to	O
be	O
more	O
explicit	O
about	O
the	O
details	O
of	O
the	O
factorization	B
,	O
as	O
we	O
shall	O
see	O
.	O
let	O
us	O
write	O
the	O
joint	O
distribution	O
over	O
a	O
set	O
of	O
variables	O
in	O
the	O
form	O
of	O
a	O
product	O
(	O
cid:14	O
)	O
of	O
factors	O
p	O
(	O
x	O
)	O
=	O
fs	O
(	O
xs	O
)	O
(	O
8.59	O
)	O
where	O
xs	O
denotes	O
a	O
subset	O
of	O
the	O
variables	O
.	O
for	O
convenience	O
,	O
we	O
shall	O
denote	O
the	O
s	O
400	O
8.	O
graphical	O
models	O
figure	O
8.40	O
example	O
of	O
a	O
factor	B
graph	I
,	O
which	O
corresponds	O
to	O
the	O
factorization	B
(	O
8.60	O
)	O
.	O
x1	O
x2	O
x3	O
fa	O
fb	O
fc	O
fd	O
individual	O
variables	O
by	O
xi	O
,	O
however	O
,	O
as	O
in	O
earlier	O
discussions	O
,	O
these	O
can	O
comprise	O
groups	O
of	O
variables	O
(	O
such	O
as	O
vectors	O
or	O
matrices	O
)	O
.	O
each	O
factor	O
fs	O
is	O
a	O
function	O
of	O
a	O
corresponding	O
set	O
of	O
variables	O
xs	O
.	O
directed	B
graphs	O
,	O
whose	O
factorization	B
is	O
deﬁned	O
by	O
(	O
8.5	O
)	O
,	O
represent	O
special	O
cases	O
of	O
(	O
8.59	O
)	O
in	O
which	O
the	O
factors	O
fs	O
(	O
xs	O
)	O
are	O
local	B
conditional	O
distributions	O
.	O
similarly	O
,	O
undirected	B
graphs	O
,	O
given	O
by	O
(	O
8.39	O
)	O
,	O
are	O
a	O
special	O
case	O
in	O
which	O
the	O
factors	O
are	O
po-	O
tential	O
functions	O
over	O
the	O
maximal	O
cliques	O
(	O
the	O
normalizing	O
coefﬁcient	O
1/z	O
can	O
be	O
viewed	O
as	O
a	O
factor	O
deﬁned	O
over	O
the	O
empty	O
set	O
of	O
variables	O
)	O
.	O
in	O
a	O
factor	B
graph	I
,	O
there	O
is	O
a	O
node	B
(	O
depicted	O
as	O
usual	O
by	O
a	O
circle	O
)	O
for	O
every	O
variable	O
in	O
the	O
distribution	O
,	O
as	O
was	O
the	O
case	O
for	O
directed	O
and	O
undirected	B
graphs	O
.	O
there	O
are	O
also	O
additional	O
nodes	O
(	O
depicted	O
by	O
small	O
squares	O
)	O
for	O
each	O
factor	O
fs	O
(	O
xs	O
)	O
in	O
the	O
joint	O
dis-	O
tribution	O
.	O
finally	O
,	O
there	O
are	O
undirected	B
links	O
connecting	O
each	O
factor	O
node	O
to	O
all	O
of	O
the	O
variables	O
nodes	O
on	O
which	O
that	O
factor	O
depends	O
.	O
consider	O
,	O
for	O
example	O
,	O
a	O
distribution	O
that	O
is	O
expressed	O
in	O
terms	O
of	O
the	O
factorization	B
p	O
(	O
x	O
)	O
=	O
fa	O
(	O
x1	O
,	O
x2	O
)	O
fb	O
(	O
x1	O
,	O
x2	O
)	O
fc	O
(	O
x2	O
,	O
x3	O
)	O
fd	O
(	O
x3	O
)	O
.	O
(	O
8.60	O
)	O
this	O
can	O
be	O
expressed	O
by	O
the	O
factor	B
graph	I
shown	O
in	O
figure	O
8.40.	O
note	O
that	O
there	O
are	O
two	O
factors	O
fa	O
(	O
x1	O
,	O
x2	O
)	O
and	O
fb	O
(	O
x1	O
,	O
x2	O
)	O
that	O
are	O
deﬁned	O
over	O
the	O
same	O
set	O
of	O
variables	O
.	O
in	O
an	O
undirected	B
graph	I
,	O
the	O
product	O
of	O
two	O
such	O
factors	O
would	O
simply	O
be	O
lumped	O
together	O
into	O
the	O
same	O
clique	B
potential	O
.	O
similarly	O
,	O
fc	O
(	O
x2	O
,	O
x3	O
)	O
and	O
fd	O
(	O
x3	O
)	O
could	O
be	O
combined	O
into	O
a	O
single	O
potential	O
over	O
x2	O
and	O
x3	O
.	O
the	O
factor	B
graph	I
,	O
however	O
,	O
keeps	O
such	O
factors	O
explicit	O
and	O
so	O
is	O
able	O
to	O
convey	O
more	O
detailed	O
information	O
about	O
the	O
underlying	O
factorization	B
.	O
x1	O
x2	O
x1	O
x2	O
x1	O
f	O
x3	O
(	O
b	O
)	O
x3	O
(	O
a	O
)	O
x2	O
fb	O
fa	O
x3	O
(	O
c	O
)	O
figure	O
8.41	O
(	O
a	O
)	O
an	O
undirected	B
graph	I
with	O
a	O
single	O
clique	B
potential	O
ψ	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
.	O
(	O
b	O
)	O
a	O
factor	B
graph	I
with	O
factor	O
f	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
=	O
ψ	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
representing	O
the	O
same	O
distribution	O
as	O
the	O
undirected	B
graph	I
.	O
(	O
c	O
)	O
a	O
different	O
factor	B
graph	I
representing	O
the	O
same	O
distribution	O
,	O
whose	O
factors	O
satisfy	O
fa	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
fb	O
(	O
x1	O
,	O
x2	O
)	O
=	O
ψ	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
.	O
x1	O
x2	O
x1	O
x2	O
x1	O
x2	O
8.4.	O
inference	B
in	O
graphical	O
models	O
401	O
f	O
x3	O
(	O
b	O
)	O
fc	O
fa	O
fb	O
x3	O
(	O
c	O
)	O
x3	O
(	O
a	O
)	O
figure	O
8.42	O
(	O
a	O
)	O
a	O
directed	B
graph	O
with	O
the	O
factorization	B
p	O
(	O
x1	O
)	O
p	O
(	O
x2	O
)	O
p	O
(	O
x3|x1	O
,	O
x2	O
)	O
.	O
(	O
b	O
)	O
a	O
factor	B
graph	I
representing	O
the	O
same	O
distribution	O
as	O
the	O
directed	B
graph	O
,	O
whose	O
factor	O
satisﬁes	O
f	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2	O
)	O
p	O
(	O
x3|x1	O
,	O
x2	O
)	O
.	O
(	O
c	O
)	O
a	O
different	O
factor	B
graph	I
representing	O
the	O
same	O
distribution	O
with	O
factors	O
fa	O
(	O
x1	O
)	O
=	O
p	O
(	O
x1	O
)	O
,	O
fb	O
(	O
x2	O
)	O
=	O
p	O
(	O
x2	O
)	O
and	O
fc	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
=	O
p	O
(	O
x3|x1	O
,	O
x2	O
)	O
.	O
factor	O
graphs	O
are	O
said	O
to	O
be	O
bipartite	B
because	O
they	O
consist	O
of	O
two	O
distinct	O
kinds	O
of	O
nodes	O
,	O
and	O
all	O
links	O
go	O
between	O
nodes	O
of	O
opposite	O
type	O
.	O
in	O
general	O
,	O
factor	O
graphs	O
can	O
therefore	O
always	O
be	O
drawn	O
as	O
two	O
rows	O
of	O
nodes	O
(	O
variable	O
nodes	O
at	O
the	O
top	O
and	O
factor	O
nodes	O
at	O
the	O
bottom	O
)	O
with	O
links	O
between	O
the	O
rows	O
,	O
as	O
shown	O
in	O
the	O
example	O
in	O
figure	O
8.40.	O
in	O
some	O
situations	O
,	O
however	O
,	O
other	O
ways	O
of	O
laying	O
out	O
the	O
graph	O
may	O
be	O
more	O
intuitive	O
,	O
for	O
example	O
when	O
the	O
factor	B
graph	I
is	O
derived	O
from	O
a	O
directed	B
or	O
undirected	B
graph	I
,	O
as	O
we	O
shall	O
see	O
.	O
if	O
we	O
are	O
given	O
a	O
distribution	O
that	O
is	O
expressed	O
in	O
terms	O
of	O
an	O
undirected	B
graph	I
,	O
then	O
we	O
can	O
readily	O
convert	O
it	O
to	O
a	O
factor	B
graph	I
.	O
to	O
do	O
this	O
,	O
we	O
create	O
variable	O
nodes	O
corresponding	O
to	O
the	O
nodes	O
in	O
the	O
original	O
undirected	B
graph	I
,	O
and	O
then	O
create	O
addi-	O
tional	O
factor	O
nodes	O
corresponding	O
to	O
the	O
maximal	O
cliques	O
xs	O
.	O
the	O
factors	O
fs	O
(	O
xs	O
)	O
are	O
then	O
set	O
equal	O
to	O
the	O
clique	B
potentials	O
.	O
note	O
that	O
there	O
may	O
be	O
several	O
different	O
factor	O
graphs	O
that	O
correspond	O
to	O
the	O
same	O
undirected	B
graph	I
.	O
these	O
concepts	O
are	O
illustrated	O
in	O
figure	O
8.41.	O
similarly	O
,	O
to	O
convert	O
a	O
directed	B
graph	O
to	O
a	O
factor	B
graph	I
,	O
we	O
simply	O
create	O
variable	O
nodes	O
in	O
the	O
factor	B
graph	I
corresponding	O
to	O
the	O
nodes	O
of	O
the	O
directed	B
graph	O
,	O
and	O
then	O
create	O
factor	O
nodes	O
corresponding	O
to	O
the	O
conditional	B
distributions	O
,	O
and	O
then	O
ﬁnally	O
add	O
the	O
appropriate	O
links	O
.	O
again	O
,	O
there	O
can	O
be	O
multiple	O
factor	O
graphs	O
all	O
of	O
which	O
correspond	O
to	O
the	O
same	O
directed	B
graph	O
.	O
the	O
conversion	O
of	O
a	O
directed	B
graph	O
to	O
a	O
factor	B
graph	I
is	O
illustrated	O
in	O
figure	O
8.42.	O
we	O
have	O
already	O
noted	O
the	O
importance	O
of	O
tree-structured	O
graphs	O
for	O
performing	O
efﬁcient	O
inference	B
.	O
if	O
we	O
take	O
a	O
directed	B
or	O
undirected	B
tree	O
and	O
convert	O
it	O
into	O
a	O
factor	B
graph	I
,	O
then	O
the	O
result	O
will	O
again	O
be	O
a	O
tree	B
(	O
in	O
other	O
words	O
,	O
the	O
factor	B
graph	I
will	O
have	O
no	O
loops	O
,	O
and	O
there	O
will	O
be	O
one	O
and	O
only	O
one	O
path	O
connecting	O
any	O
two	O
nodes	O
)	O
.	O
in	O
the	O
case	O
of	O
a	O
directed	B
polytree	O
,	O
conversion	O
to	O
an	O
undirected	B
graph	I
results	O
in	O
loops	O
due	O
to	O
the	O
moralization	B
step	O
,	O
whereas	O
conversion	O
to	O
a	O
factor	B
graph	I
again	O
results	O
in	O
a	O
tree	B
,	O
as	O
illustrated	O
in	O
figure	O
8.43.	O
in	O
fact	O
,	O
local	B
cycles	O
in	O
a	O
directed	B
graph	O
due	O
to	O
links	O
connecting	O
parents	O
of	O
a	O
node	B
can	O
be	O
removed	O
on	O
conversion	O
to	O
a	O
factor	B
graph	I
by	O
deﬁning	O
the	O
appropriate	O
factor	O
function	O
,	O
as	O
shown	O
in	O
figure	O
8.44.	O
we	O
have	O
seen	O
that	O
multiple	O
different	O
factor	O
graphs	O
can	O
represent	O
the	O
same	O
di-	O
rected	O
or	O
undirected	B
graph	I
.	O
this	O
allows	O
factor	O
graphs	O
to	O
be	O
more	O
speciﬁc	O
about	O
the	O
402	O
8.	O
graphical	O
models	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
8.43	O
(	O
a	O
)	O
a	O
directed	B
polytree	O
.	O
(	O
b	O
)	O
the	O
result	O
of	O
converting	O
the	O
polytree	B
into	O
an	O
undirected	B
graph	I
showing	O
the	O
creation	O
of	O
loops	O
.	O
(	O
c	O
)	O
the	O
result	O
of	O
converting	O
the	O
polytree	B
into	O
a	O
factor	B
graph	I
,	O
which	O
retains	O
the	O
tree	B
structure	O
.	O
precise	O
form	O
of	O
the	O
factorization	B
.	O
figure	O
8.45	O
shows	O
an	O
example	O
of	O
a	O
fully	B
connected	I
undirected	O
graph	O
along	O
with	O
two	O
different	O
factor	O
graphs	O
.	O
in	O
(	O
b	O
)	O
,	O
the	O
joint	O
distri-	O
bution	O
is	O
given	O
by	O
a	O
general	O
form	O
p	O
(	O
x	O
)	O
=	O
f	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
,	O
whereas	O
in	O
(	O
c	O
)	O
,	O
it	O
is	O
given	O
by	O
the	O
more	O
speciﬁc	O
factorization	B
p	O
(	O
x	O
)	O
=	O
fa	O
(	O
x1	O
,	O
x2	O
)	O
fb	O
(	O
x1	O
,	O
x3	O
)	O
fc	O
(	O
x2	O
,	O
x3	O
)	O
.	O
it	O
should	O
be	O
emphasized	O
that	O
the	O
factorization	B
in	O
(	O
c	O
)	O
does	O
not	O
correspond	O
to	O
any	O
conditional	B
independence	I
properties	O
.	O
8.4.4	O
the	O
sum-product	B
algorithm	I
we	O
shall	O
now	O
make	O
use	O
of	O
the	O
factor	B
graph	I
framework	O
to	O
derive	O
a	O
powerful	O
class	O
of	O
efﬁcient	O
,	O
exact	O
inference	O
algorithms	O
that	O
are	O
applicable	O
to	O
tree-structured	O
graphs	O
.	O
here	O
we	O
shall	O
focus	O
on	O
the	O
problem	O
of	O
evaluating	O
local	B
marginals	O
over	O
nodes	O
or	O
subsets	O
of	O
nodes	O
,	O
which	O
will	O
lead	O
us	O
to	O
the	O
sum-product	B
algorithm	I
.	O
later	O
we	O
shall	O
modify	O
the	O
technique	O
to	O
allow	O
the	O
most	O
probable	O
state	O
to	O
be	O
found	O
,	O
giving	O
rise	O
to	O
the	O
max-sum	B
algorithm	I
.	O
also	O
we	O
shall	O
suppose	O
that	O
all	O
of	O
the	O
variables	O
in	O
the	O
model	O
are	O
discrete	O
,	O
and	O
so	O
marginalization	O
corresponds	O
to	O
performing	O
sums	O
.	O
the	O
framework	O
,	O
however	O
,	O
is	O
equally	O
applicable	O
to	O
linear-gaussian	O
models	O
in	O
which	O
case	O
marginalization	O
involves	O
integration	O
,	O
and	O
we	O
shall	O
consider	O
an	O
example	O
of	O
this	O
in	O
detail	O
when	O
we	O
discuss	O
linear	O
dynamical	O
systems	O
.	O
section	O
13.3	O
figure	O
8.44	O
(	O
a	O
)	O
a	O
fragment	O
of	O
a	O
di-	O
rected	O
graph	O
having	O
a	O
lo-	O
cal	O
cycle	O
.	O
(	O
b	O
)	O
conversion	O
to	O
a	O
fragment	O
of	O
a	O
factor	B
graph	I
having	O
a	O
tree	B
struc-	O
ture	O
,	O
in	O
which	O
f	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2|x1	O
)	O
p	O
(	O
x3|x1	O
,	O
x2	O
)	O
.	O
x1	O
x2	O
x1	O
x2	O
f	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
x3	O
(	O
a	O
)	O
x3	O
(	O
b	O
)	O
x1	O
x2	O
x1	O
x2	O
x1	O
fa	O
x2	O
8.4.	O
inference	B
in	O
graphical	O
models	O
403	O
x3	O
(	O
a	O
)	O
f	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
x3	O
(	O
b	O
)	O
fb	O
fc	O
x3	O
(	O
c	O
)	O
figure	O
8.45	O
(	O
a	O
)	O
a	O
fully	B
connected	I
undirected	O
graph	O
.	O
(	O
b	O
)	O
and	O
(	O
c	O
)	O
two	O
factor	O
graphs	O
each	O
of	O
which	O
corresponds	O
to	O
the	O
undirected	B
graph	I
in	O
(	O
a	O
)	O
.	O
there	O
is	O
an	O
algorithm	O
for	O
exact	O
inference	B
on	O
directed	B
graphs	O
without	O
loops	O
known	O
as	O
belief	B
propagation	I
(	O
pearl	O
,	O
1988	O
;	O
lauritzen	O
and	O
spiegelhalter	O
,	O
1988	O
)	O
,	O
and	O
is	O
equiv-	O
alent	O
to	O
a	O
special	O
case	O
of	O
the	O
sum-product	B
algorithm	I
.	O
here	O
we	O
shall	O
consider	O
only	O
the	O
sum-product	B
algorithm	I
because	O
it	O
is	O
simpler	O
to	O
derive	O
and	O
to	O
apply	O
,	O
as	O
well	O
as	O
being	O
more	O
general	O
.	O
we	O
shall	O
assume	O
that	O
the	O
original	O
graph	O
is	O
an	O
undirected	B
tree	O
or	O
a	O
directed	B
tree	O
or	O
polytree	B
,	O
so	O
that	O
the	O
corresponding	O
factor	B
graph	I
has	O
a	O
tree	B
structure	O
.	O
we	O
ﬁrst	O
convert	O
the	O
original	O
graph	O
into	O
a	O
factor	B
graph	I
so	O
that	O
we	O
can	O
deal	O
with	O
both	O
directed	B
and	O
undirected	B
models	O
using	O
the	O
same	O
framework	O
.	O
our	O
goal	O
is	O
to	O
exploit	O
the	O
structure	O
of	O
the	O
graph	O
to	O
achieve	O
two	O
things	O
:	O
(	O
i	O
)	O
to	O
obtain	O
an	O
efﬁcient	O
,	O
exact	O
inference	O
algorithm	O
for	O
ﬁnding	O
marginals	O
;	O
(	O
ii	O
)	O
in	O
situations	O
where	O
several	O
marginals	O
are	O
required	O
to	O
allow	O
computations	O
to	O
be	O
shared	O
efﬁciently	O
.	O
we	O
begin	O
by	O
considering	O
the	O
problem	O
of	O
ﬁnding	O
the	O
marginal	B
p	O
(	O
x	O
)	O
for	O
partic-	O
ular	O
variable	O
node	B
x.	O
for	O
the	O
moment	O
,	O
we	O
shall	O
suppose	O
that	O
all	O
of	O
the	O
variables	O
are	O
hidden	O
.	O
later	O
we	O
shall	O
see	O
how	O
to	O
modify	O
the	O
algorithm	O
to	O
incorporate	O
evidence	O
corresponding	O
to	O
observed	O
variables	O
.	O
by	O
deﬁnition	O
,	O
the	O
marginal	B
is	O
obtained	O
by	O
sum-	O
ming	O
the	O
joint	O
distribution	O
over	O
all	O
variables	O
except	O
x	O
so	O
that	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
x	O
)	O
(	O
8.61	O
)	O
(	O
cid:2	O
)	O
x\x	O
where	O
x	O
\	O
x	O
denotes	O
the	O
set	O
of	O
variables	O
in	O
x	O
with	O
variable	O
x	O
omitted	O
.	O
the	O
idea	O
is	O
to	O
substitute	O
for	O
p	O
(	O
x	O
)	O
using	O
the	O
factor	B
graph	I
expression	O
(	O
8.59	O
)	O
and	O
then	O
interchange	O
summations	O
and	O
products	O
in	O
order	O
to	O
obtain	O
an	O
efﬁcient	O
algorithm	O
.	O
consider	O
the	O
fragment	O
of	O
graph	O
shown	O
in	O
figure	O
8.46	O
in	O
which	O
we	O
see	O
that	O
the	O
tree	B
structure	O
of	O
the	O
graph	O
allows	O
us	O
to	O
partition	O
the	O
factors	O
in	O
the	O
joint	O
distribution	O
into	O
groups	O
,	O
with	O
one	O
group	O
associated	O
with	O
each	O
of	O
the	O
factor	O
nodes	O
that	O
is	O
a	O
neighbour	O
of	O
the	O
variable	O
node	B
x.	O
we	O
see	O
that	O
the	O
joint	O
distribution	O
can	O
be	O
written	O
as	O
a	O
product	O
of	O
the	O
form	O
p	O
(	O
x	O
)	O
=	O
fs	O
(	O
x	O
,	O
xs	O
)	O
(	O
8.62	O
)	O
(	O
cid:14	O
)	O
s∈ne	O
(	O
x	O
)	O
ne	O
(	O
x	O
)	O
denotes	O
the	O
set	O
of	O
factor	O
nodes	O
that	O
are	O
neighbours	O
of	O
x	O
,	O
and	O
xs	O
denotes	O
the	O
set	O
of	O
all	O
variables	O
in	O
the	O
subtree	O
connected	O
to	O
the	O
variable	O
node	B
x	O
via	O
the	O
factor	O
node	O
404	O
8.	O
graphical	O
models	O
figure	O
8.46	O
a	O
fragment	O
of	O
a	O
factor	B
graph	I
illustrating	O
the	O
evaluation	O
of	O
the	O
marginal	B
p	O
(	O
x	O
)	O
.	O
)	O
s	O
x	O
,	O
x	O
(	O
s	O
f	O
µfs→x	O
(	O
x	O
)	O
fs	O
x	O
fs	O
,	O
and	O
fs	O
(	O
x	O
,	O
xs	O
)	O
represents	O
the	O
product	O
of	O
all	O
the	O
factors	O
in	O
the	O
group	O
associated	O
with	O
factor	O
fs	O
.	O
substituting	O
(	O
8.62	O
)	O
into	O
(	O
8.61	O
)	O
and	O
interchanging	O
the	O
sums	O
and	O
products	O
,	O
we	O
ob-	O
tain	O
p	O
(	O
x	O
)	O
=	O
=	O
(	O
cid:31	O
)	O
(	O
cid:2	O
)	O
(	O
cid:14	O
)	O
(	O
cid:14	O
)	O
s∈ne	O
(	O
x	O
)	O
s∈ne	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
fs	O
(	O
x	O
,	O
xs	O
)	O
xs	O
µfs→x	O
(	O
x	O
)	O
.	O
µfs→x	O
(	O
x	O
)	O
≡	O
fs	O
(	O
x	O
,	O
xs	O
)	O
(	O
8.63	O
)	O
(	O
8.64	O
)	O
here	O
we	O
have	O
introduced	O
a	O
set	O
of	O
functions	O
µfs→x	O
(	O
x	O
)	O
,	O
deﬁned	O
by	O
xs	O
which	O
can	O
be	O
viewed	O
as	O
messages	O
from	O
the	O
factor	O
nodes	O
fs	O
to	O
the	O
variable	O
node	B
x.	O
we	O
see	O
that	O
the	O
required	O
marginal	B
p	O
(	O
x	O
)	O
is	O
given	O
by	O
the	O
product	O
of	O
all	O
the	O
incoming	O
messages	O
arriving	O
at	O
node	B
x.	O
in	O
order	O
to	O
evaluate	O
these	O
messages	O
,	O
we	O
again	O
turn	O
to	O
figure	O
8.46	O
and	O
note	O
that	O
each	O
factor	O
fs	O
(	O
x	O
,	O
xs	O
)	O
is	O
described	O
by	O
a	O
factor	O
(	O
sub-	O
)	O
graph	O
and	O
so	O
can	O
itself	O
be	O
fac-	O
torized	O
.	O
in	O
particular	O
,	O
we	O
can	O
write	O
fs	O
(	O
x	O
,	O
xs	O
)	O
=	O
fs	O
(	O
x	O
,	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
g1	O
(	O
x1	O
,	O
xs1	O
)	O
.	O
.	O
.	O
gm	O
(	O
xm	O
,	O
xsm	O
)	O
(	O
8.65	O
)	O
where	O
,	O
for	O
convenience	O
,	O
we	O
have	O
denoted	O
the	O
variables	O
associated	O
with	O
factor	O
fx	O
,	O
in	O
addition	O
to	O
x	O
,	O
by	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
.	O
this	O
factorization	B
is	O
illustrated	O
in	O
figure	O
8.47.	O
note	O
that	O
the	O
set	O
of	O
variables	O
{	O
x	O
,	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
}	O
is	O
the	O
set	O
of	O
variables	O
on	O
which	O
the	O
factor	O
fs	O
depends	O
,	O
and	O
so	O
it	O
can	O
also	O
be	O
denoted	O
xs	O
,	O
using	O
the	O
notation	O
of	O
(	O
8.59	O
)	O
.	O
substituting	O
(	O
8.65	O
)	O
into	O
(	O
8.64	O
)	O
we	O
obtain	O
(	O
cid:31	O
)	O
(	O
cid:2	O
)	O
µfs→x	O
(	O
x	O
)	O
=	O
=	O
fs	O
(	O
x	O
,	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
fs	O
(	O
x	O
,	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
gm	O
(	O
xm	O
,	O
xsm	O
)	O
xxm	O
µxm→fs	O
(	O
xm	O
)	O
(	O
8.66	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
x1	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
xm	O
.	O
.	O
.	O
.	O
.	O
.	O
x1	O
xm	O
(	O
cid:14	O
)	O
(	O
cid:14	O
)	O
m∈ne	O
(	O
fs	O
)	O
\x	O
m∈ne	O
(	O
fs	O
)	O
\x	O
8.4.	O
inference	B
in	O
graphical	O
models	O
405	O
figure	O
8.47	O
illustration	O
of	O
the	O
factorization	B
of	O
the	O
subgraph	O
as-	O
sociated	O
with	O
factor	O
node	O
fs	O
.	O
xm	O
µxm→fs	O
(	O
xm	O
)	O
fs	O
µfs→x	O
(	O
x	O
)	O
x	O
xm	O
gm	O
(	O
xm	O
,	O
xsm	O
)	O
where	O
ne	O
(	O
fs	O
)	O
denotes	O
the	O
set	O
of	O
variable	O
nodes	O
that	O
are	O
neighbours	O
of	O
the	O
factor	O
node	O
fs	O
,	O
and	O
ne	O
(	O
fs	O
)	O
\	O
x	O
denotes	O
the	O
same	O
set	O
but	O
with	O
node	B
x	O
removed	O
.	O
here	O
we	O
have	O
deﬁned	O
the	O
following	O
messages	O
from	O
variable	O
nodes	O
to	O
factor	O
nodes	O
(	O
cid:2	O
)	O
µxm→fs	O
(	O
xm	O
)	O
≡	O
gm	O
(	O
xm	O
,	O
xsm	O
)	O
.	O
(	O
8.67	O
)	O
xsm	O
we	O
have	O
therefore	O
introduced	O
two	O
distinct	O
kinds	O
of	O
message	O
,	O
those	O
that	O
go	O
from	O
factor	O
nodes	O
to	O
variable	O
nodes	O
denoted	O
µf→x	O
(	O
x	O
)	O
,	O
and	O
those	O
that	O
go	O
from	O
variable	O
nodes	O
to	O
factor	O
nodes	O
denoted	O
µx→f	O
(	O
x	O
)	O
.	O
in	O
each	O
case	O
,	O
we	O
see	O
that	O
messages	O
passed	O
along	O
a	O
link	B
are	O
always	O
a	O
function	O
of	O
the	O
variable	O
associated	O
with	O
the	O
variable	O
node	B
that	O
link	B
connects	O
to	O
.	O
the	O
result	O
(	O
8.66	O
)	O
says	O
that	O
to	O
evaluate	O
the	O
message	O
sent	O
by	O
a	O
factor	O
node	O
to	O
a	O
vari-	O
able	O
node	B
along	O
the	O
link	B
connecting	O
them	O
,	O
take	O
the	O
product	O
of	O
the	O
incoming	O
messages	O
along	O
all	O
other	O
links	O
coming	O
into	O
the	O
factor	O
node	O
,	O
multiply	O
by	O
the	O
factor	O
associated	O
with	O
that	O
node	B
,	O
and	O
then	O
marginalize	O
over	O
all	O
of	O
the	O
variables	O
associated	O
with	O
the	O
incoming	O
messages	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
8.47.	O
it	O
is	O
important	O
to	O
note	O
that	O
a	O
factor	O
node	O
can	O
send	O
a	O
message	O
to	O
a	O
variable	O
node	B
once	O
it	O
has	O
received	O
incoming	O
messages	O
from	O
all	O
other	O
neighbouring	O
variable	O
nodes	O
.	O
finally	O
,	O
we	O
derive	O
an	O
expression	O
for	O
evaluating	O
the	O
messages	O
from	O
variable	O
nodes	O
to	O
factor	O
nodes	O
,	O
again	O
by	O
making	O
use	O
of	O
the	O
(	O
sub-	O
)	O
graph	O
factorization	B
.	O
from	O
fig-	O
ure	O
8.48	O
,	O
we	O
see	O
that	O
term	O
gm	O
(	O
xm	O
,	O
xsm	O
)	O
associated	O
with	O
node	B
xm	O
is	O
given	O
by	O
a	O
product	O
of	O
terms	O
fl	O
(	O
xm	O
,	O
xml	O
)	O
each	O
associated	O
with	O
one	O
of	O
the	O
factor	O
nodes	O
fl	O
that	O
is	O
linked	O
to	O
node	B
xm	O
(	O
excluding	O
node	B
fs	O
)	O
,	O
so	O
that	O
gm	O
(	O
xm	O
,	O
xsm	O
)	O
=	O
fl	O
(	O
xm	O
,	O
xml	O
)	O
(	O
8.68	O
)	O
(	O
cid:14	O
)	O
l∈ne	O
(	O
xm	O
)	O
\fs	O
where	O
the	O
product	O
is	O
taken	O
over	O
all	O
neighbours	O
of	O
node	B
xm	O
except	O
for	O
node	O
fs	O
.	O
note	O
that	O
each	O
of	O
the	O
factors	O
fl	O
(	O
xm	O
,	O
xml	O
)	O
represents	O
a	O
subtree	O
of	O
the	O
original	O
graph	O
of	O
precisely	O
the	O
same	O
kind	O
as	O
introduced	O
in	O
(	O
8.62	O
)	O
.	O
substituting	O
(	O
8.68	O
)	O
into	O
(	O
8.67	O
)	O
,	O
we	O
406	O
8.	O
graphical	O
models	O
figure	O
8.48	O
illustration	O
of	O
the	O
evaluation	O
of	O
the	O
message	O
sent	O
by	O
a	O
variable	O
node	B
to	O
an	O
adjacent	O
factor	O
node	O
.	O
fl	O
fl	O
xm	O
fs	O
then	O
obtain	O
µxm→fs	O
(	O
xm	O
)	O
=	O
=	O
(	O
cid:14	O
)	O
(	O
cid:14	O
)	O
l∈ne	O
(	O
xm	O
)	O
\fs	O
l∈ne	O
(	O
xm	O
)	O
\fs	O
fl	O
(	O
xm	O
,	O
xml	O
)	O
fl	O
(	O
xm	O
,	O
xml	O
)	O
(	O
cid:31	O
)	O
(	O
cid:2	O
)	O
xml	O
µfl→xm	O
(	O
xm	O
)	O
(	O
8.69	O
)	O
where	O
we	O
have	O
used	O
the	O
deﬁnition	O
(	O
8.64	O
)	O
of	O
the	O
messages	O
passed	O
from	O
factor	O
nodes	O
to	O
variable	O
nodes	O
.	O
thus	O
to	O
evaluate	O
the	O
message	O
sent	O
by	O
a	O
variable	O
node	B
to	O
an	O
adjacent	O
factor	O
node	O
along	O
the	O
connecting	O
link	B
,	O
we	O
simply	O
take	O
the	O
product	O
of	O
the	O
incoming	O
messages	O
along	O
all	O
of	O
the	O
other	O
links	O
.	O
note	O
that	O
any	O
variable	O
node	B
that	O
has	O
only	O
two	O
neighbours	O
performs	O
no	O
computation	O
but	O
simply	O
passes	O
messages	O
through	O
un-	O
changed	O
.	O
also	O
,	O
we	O
note	O
that	O
a	O
variable	O
node	B
can	O
send	O
a	O
message	O
to	O
a	O
factor	O
node	O
once	O
it	O
has	O
received	O
incoming	O
messages	O
from	O
all	O
other	O
neighbouring	O
factor	O
nodes	O
.	O
recall	O
that	O
our	O
goal	O
is	O
to	O
calculate	O
the	O
marginal	B
for	O
variable	O
node	B
x	O
,	O
and	O
that	O
this	O
marginal	B
is	O
given	O
by	O
the	O
product	O
of	O
incoming	O
messages	O
along	O
all	O
of	O
the	O
links	O
arriving	O
at	O
that	O
node	B
.	O
each	O
of	O
these	O
messages	O
can	O
be	O
computed	O
recursively	O
in	O
terms	O
of	O
other	O
messages	O
.	O
in	O
order	O
to	O
start	O
this	O
recursion	O
,	O
we	O
can	O
view	O
the	O
node	B
x	O
as	O
the	O
root	O
of	O
the	O
tree	B
and	O
begin	O
at	O
the	O
leaf	O
nodes	O
.	O
from	O
the	O
deﬁnition	O
(	O
8.69	O
)	O
,	O
we	O
see	O
that	O
if	O
a	O
leaf	O
node	B
is	O
a	O
variable	O
node	B
,	O
then	O
the	O
message	O
that	O
it	O
sends	O
along	O
its	O
one	O
and	O
only	O
link	B
is	O
given	O
by	O
(	O
8.70	O
)	O
as	O
illustrated	O
in	O
figure	O
8.49	O
(	O
a	O
)	O
.	O
similarly	O
,	O
if	O
the	O
leaf	O
node	B
is	O
a	O
factor	O
node	O
,	O
we	O
see	O
from	O
(	O
8.66	O
)	O
that	O
the	O
message	O
sent	O
should	O
take	O
the	O
form	O
µx→f	O
(	O
x	O
)	O
=	O
1	O
µf→x	O
(	O
x	O
)	O
=	O
f	O
(	O
x	O
)	O
(	O
8.71	O
)	O
figure	O
8.49	O
the	O
sum-product	B
algorithm	I
begins	O
with	O
messages	O
sent	O
by	O
the	O
leaf	O
nodes	O
,	O
which	O
de-	O
pend	O
on	O
whether	O
the	O
leaf	O
node	B
is	O
(	O
a	O
)	O
a	O
variable	O
node	B
,	O
or	O
(	O
b	O
)	O
a	O
factor	O
node	O
.	O
µx→f	O
(	O
x	O
)	O
=	O
1	O
µf→x	O
(	O
x	O
)	O
=	O
f	O
(	O
x	O
)	O
x	O
f	O
f	O
x	O
(	O
a	O
)	O
(	O
b	O
)	O
8.4.	O
inference	B
in	O
graphical	O
models	O
407	O
as	O
illustrated	O
in	O
figure	O
8.49	O
(	O
b	O
)	O
.	O
at	O
this	O
point	O
,	O
it	O
is	O
worth	O
pausing	O
to	O
summarize	O
the	O
particular	O
version	O
of	O
the	O
sum-	O
product	O
algorithm	O
obtained	O
so	O
far	O
for	O
evaluating	O
the	O
marginal	B
p	O
(	O
x	O
)	O
.	O
we	O
start	O
by	O
viewing	O
the	O
variable	O
node	B
x	O
as	O
the	O
root	O
of	O
the	O
factor	B
graph	I
and	O
initiating	O
messages	O
at	O
the	O
leaves	O
of	O
the	O
graph	O
using	O
(	O
8.70	O
)	O
and	O
(	O
8.71	O
)	O
.	O
the	O
message	B
passing	I
steps	O
(	O
8.66	O
)	O
and	O
(	O
8.69	O
)	O
are	O
then	O
applied	O
recursively	O
until	O
messages	O
have	O
been	O
propagated	O
along	O
every	O
link	B
,	O
and	O
the	O
root	B
node	I
has	O
received	O
messages	O
from	O
all	O
of	O
its	O
neighbours	O
.	O
each	O
node	B
can	O
send	O
a	O
message	O
towards	O
the	O
root	O
once	O
it	O
has	O
received	O
messages	O
from	O
all	O
of	O
its	O
other	O
neighbours	O
.	O
once	O
the	O
root	B
node	I
has	O
received	O
messages	O
from	O
all	O
of	O
its	O
neighbours	O
,	O
the	O
required	O
marginal	B
can	O
be	O
evaluated	O
using	O
(	O
8.63	O
)	O
.	O
we	O
shall	O
illustrate	O
this	O
process	O
shortly	O
.	O
to	O
see	O
that	O
each	O
node	B
will	O
always	O
receive	O
enough	O
messages	O
to	O
be	O
able	O
to	O
send	O
out	O
a	O
message	O
,	O
we	O
can	O
use	O
a	O
simple	O
inductive	O
argument	O
as	O
follows	O
.	O
clearly	O
,	O
for	O
a	O
graph	O
comprising	O
a	O
variable	O
root	B
node	I
connected	O
directly	O
to	O
several	O
factor	O
leaf	O
nodes	O
,	O
the	O
algorithm	O
trivially	O
involves	O
sending	O
messages	O
of	O
the	O
form	O
(	O
8.71	O
)	O
directly	O
from	O
the	O
leaves	O
to	O
the	O
root	O
.	O
now	O
imagine	O
building	O
up	O
a	O
general	O
graph	O
by	O
adding	O
nodes	O
one	O
at	O
a	O
time	O
,	O
and	O
suppose	O
that	O
for	O
some	O
particular	O
graph	O
we	O
have	O
a	O
valid	O
algorithm	O
.	O
when	O
one	O
more	O
(	O
variable	O
or	O
factor	O
)	O
node	B
is	O
added	O
,	O
it	O
can	O
be	O
connected	O
only	O
by	O
a	O
single	O
link	B
because	O
the	O
overall	O
graph	O
must	O
remain	O
a	O
tree	B
,	O
and	O
so	O
the	O
new	O
node	B
will	O
be	O
a	O
leaf	O
node	B
.	O
it	O
therefore	O
sends	O
a	O
message	O
to	O
the	O
node	B
to	O
which	O
it	O
is	O
linked	O
,	O
which	O
in	O
turn	O
will	O
therefore	O
receive	O
all	O
the	O
messages	O
it	O
requires	O
in	O
order	O
to	O
send	O
its	O
own	O
message	O
towards	O
the	O
root	O
,	O
and	O
so	O
again	O
we	O
have	O
a	O
valid	O
algorithm	O
,	O
thereby	O
completing	O
the	O
proof	O
.	O
now	O
suppose	O
we	O
wish	O
to	O
ﬁnd	O
the	O
marginals	O
for	O
every	O
variable	O
node	B
in	O
the	O
graph	O
.	O
this	O
could	O
be	O
done	O
by	O
simply	O
running	O
the	O
above	O
algorithm	O
afresh	O
for	O
each	O
such	O
node	B
.	O
however	O
,	O
this	O
would	O
be	O
very	O
wasteful	O
as	O
many	O
of	O
the	O
required	O
computations	O
would	O
be	O
repeated	O
.	O
we	O
can	O
obtain	O
a	O
much	O
more	O
efﬁcient	O
procedure	O
by	O
‘	O
overlaying	O
’	O
these	O
multiple	O
message	B
passing	I
algorithms	O
to	O
obtain	O
the	O
general	O
sum-product	B
algorithm	I
as	O
follows	O
.	O
arbitrarily	O
pick	O
any	O
(	O
variable	O
or	O
factor	O
)	O
node	B
and	O
designate	O
it	O
as	O
the	O
root	O
.	O
propagate	O
messages	O
from	O
the	O
leaves	O
to	O
the	O
root	O
as	O
before	O
.	O
at	O
this	O
point	O
,	O
the	O
root	B
node	I
will	O
have	O
received	O
messages	O
from	O
all	O
of	O
its	O
neighbours	O
.	O
it	O
can	O
therefore	O
send	O
out	O
messages	O
to	O
all	O
of	O
its	O
neighbours	O
.	O
these	O
in	O
turn	O
will	O
then	O
have	O
received	O
messages	O
from	O
all	O
of	O
their	O
neighbours	O
and	O
so	O
can	O
send	O
out	O
messages	O
along	O
the	O
links	O
going	O
away	O
from	O
the	O
root	O
,	O
and	O
so	O
on	O
.	O
in	O
this	O
way	O
,	O
messages	O
are	O
passed	O
outwards	O
from	O
the	O
root	O
all	O
the	O
way	O
to	O
the	O
leaves	O
.	O
by	O
now	O
,	O
a	O
message	O
will	O
have	O
passed	O
in	O
both	O
directions	O
across	O
every	O
link	B
in	O
the	O
graph	O
,	O
and	O
every	O
node	B
will	O
have	O
received	O
a	O
message	O
from	O
all	O
of	O
its	O
neighbours	O
.	O
again	O
a	O
simple	O
inductive	O
argument	O
can	O
be	O
used	O
to	O
verify	O
the	O
validity	O
of	O
this	O
message	B
passing	I
protocol	O
.	O
because	O
every	O
variable	O
node	B
will	O
have	O
received	O
messages	O
from	O
all	O
of	O
its	O
neighbours	O
,	O
we	O
can	O
readily	O
calculate	O
the	O
marginal	B
distribution	O
for	O
every	O
variable	O
in	O
the	O
graph	O
.	O
the	O
number	O
of	O
messages	O
that	O
have	O
to	O
be	O
computed	O
is	O
given	O
by	O
twice	O
the	O
number	O
of	O
links	O
in	O
the	O
graph	O
and	O
so	O
involves	O
only	O
twice	O
the	O
computation	O
involved	O
in	O
ﬁnding	O
a	O
single	O
marginal	B
.	O
by	O
comparison	O
,	O
if	O
we	O
had	O
run	O
the	O
sum-product	B
algorithm	I
separately	O
for	O
each	O
node	B
,	O
the	O
amount	O
of	O
computation	O
would	O
grow	O
quadratically	O
with	O
the	O
size	O
of	O
the	O
graph	O
.	O
note	O
that	O
this	O
algorithm	O
is	O
in	O
fact	O
independent	B
of	O
which	O
node	B
was	O
designated	O
as	O
the	O
root	O
,	O
exercise	O
8.20	O
408	O
8.	O
graphical	O
models	O
figure	O
8.50	O
the	O
sum-product	B
algorithm	I
can	O
be	O
viewed	O
purely	O
in	O
terms	O
of	O
messages	O
sent	O
out	O
by	O
factor	O
nodes	O
to	O
other	O
factor	O
nodes	O
.	O
in	O
this	O
example	O
,	O
the	O
outgoing	O
message	O
shown	O
by	O
the	O
blue	O
arrow	O
is	O
obtained	O
by	O
taking	O
the	O
product	O
of	O
all	O
the	O
in-	O
coming	O
messages	O
shown	O
by	O
green	O
arrows	O
,	O
mul-	O
tiplying	O
by	O
the	O
factor	O
fs	O
,	O
and	O
marginalizing	O
over	O
the	O
variables	O
x1	O
and	O
x2	O
.	O
x1	O
x2	O
fs	O
x3	O
exercise	O
8.21	O
and	O
indeed	O
the	O
notion	O
of	O
one	O
node	B
having	O
a	O
special	O
status	O
was	O
introduced	O
only	O
as	O
a	O
convenient	O
way	O
to	O
explain	O
the	O
message	B
passing	I
protocol	O
.	O
next	O
suppose	O
we	O
wish	O
to	O
ﬁnd	O
the	O
marginal	B
distributions	O
p	O
(	O
xs	O
)	O
associated	O
with	O
the	O
sets	O
of	O
variables	O
belonging	O
to	O
each	O
of	O
the	O
factors	O
.	O
by	O
a	O
similar	O
argument	O
to	O
that	O
used	O
above	O
,	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
marginal	B
associated	O
with	O
a	O
factor	O
is	O
given	O
by	O
the	O
product	O
of	O
messages	O
arriving	O
at	O
the	O
factor	O
node	O
and	O
the	O
local	B
factor	O
at	O
that	O
node	B
p	O
(	O
xs	O
)	O
=	O
fs	O
(	O
xs	O
)	O
µxi→fs	O
(	O
xi	O
)	O
(	O
8.72	O
)	O
(	O
cid:14	O
)	O
i∈ne	O
(	O
fs	O
)	O
in	O
complete	O
analogy	O
with	O
the	O
marginals	O
at	O
the	O
variable	O
nodes	O
.	O
if	O
the	O
factors	O
are	O
parameterized	O
functions	O
and	O
we	O
wish	O
to	O
learn	O
the	O
values	O
of	O
the	O
parameters	O
using	O
the	O
em	O
algorithm	O
,	O
then	O
these	O
marginals	O
are	O
precisely	O
the	O
quantities	O
we	O
will	O
need	O
to	O
calculate	O
in	O
the	O
e	O
step	O
,	O
as	O
we	O
shall	O
see	O
in	O
detail	O
when	O
we	O
discuss	O
the	O
hidden	O
markov	O
model	O
in	O
chapter	O
13.	O
the	O
message	O
sent	O
by	O
a	O
variable	O
node	B
to	O
a	O
factor	O
node	O
,	O
as	O
we	O
have	O
seen	O
,	O
is	O
simply	O
the	O
product	O
of	O
the	O
incoming	O
messages	O
on	O
other	O
links	O
.	O
we	O
can	O
if	O
we	O
wish	O
view	O
the	O
sum-product	B
algorithm	I
in	O
a	O
slightly	O
different	O
form	O
by	O
eliminating	O
messages	O
from	O
variable	O
nodes	O
to	O
factor	O
nodes	O
and	O
simply	O
considering	O
messages	O
that	O
are	O
sent	O
out	O
by	O
factor	O
nodes	O
.	O
this	O
is	O
most	O
easily	O
seen	O
by	O
considering	O
the	O
example	O
in	O
figure	O
8.50.	O
so	O
far	O
,	O
we	O
have	O
rather	O
neglected	O
the	O
issue	O
of	O
normalization	O
.	O
if	O
the	O
factor	B
graph	I
was	O
derived	O
from	O
a	O
directed	B
graph	O
,	O
then	O
the	O
joint	O
distribution	O
is	O
already	O
correctly	O
nor-	O
malized	O
,	O
and	O
so	O
the	O
marginals	O
obtained	O
by	O
the	O
sum-product	B
algorithm	I
will	O
similarly	O
be	O
normalized	O
correctly	O
.	O
however	O
,	O
if	O
we	O
started	O
from	O
an	O
undirected	B
graph	I
,	O
then	O
in	O
general	O
there	O
will	O
be	O
an	O
unknown	O
normalization	O
coefﬁcient	O
1/z	O
.	O
as	O
with	O
the	O
simple	O
chain	O
example	O
of	O
figure	O
8.38	O
,	O
this	O
is	O
easily	O
handled	O
by	O
working	O
with	O
an	O
unnormal-	O
ized	O
version	O
(	O
cid:4	O
)	O
p	O
(	O
x	O
)	O
of	O
the	O
joint	O
distribution	O
,	O
where	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:4	O
)	O
p	O
(	O
x	O
)	O
/z	O
.	O
we	O
ﬁrst	O
run	O
the	O
sum-product	B
algorithm	I
to	O
ﬁnd	O
the	O
corresponding	O
unnormalized	O
marginals	O
(	O
cid:4	O
)	O
p	O
(	O
xi	O
)	O
.	O
the	O
(	O
cid:4	O
)	O
p	O
(	O
x	O
)	O
directly	O
.	O
coefﬁcient	O
1/z	O
is	O
then	O
easily	O
obtained	O
by	O
normalizing	O
any	O
one	O
of	O
these	O
marginals	O
,	O
and	O
this	O
is	O
computationally	O
efﬁcient	O
because	O
the	O
normalization	O
is	O
done	O
over	O
a	O
single	O
variable	O
rather	O
than	O
over	O
the	O
entire	O
set	O
of	O
variables	O
as	O
would	O
be	O
required	O
to	O
normalize	O
at	O
this	O
point	O
,	O
it	O
may	O
be	O
helpful	O
to	O
consider	O
a	O
simple	O
example	O
to	O
illustrate	O
the	O
operation	O
of	O
the	O
sum-product	B
algorithm	I
.	O
figure	O
8.51	O
shows	O
a	O
simple	O
4-node	O
factor	O
figure	O
8.51	O
a	O
simple	O
factor	B
graph	I
used	O
to	O
illustrate	O
the	O
sum-product	B
algorithm	I
.	O
x1	O
x2	O
8.4.	O
inference	B
in	O
graphical	O
models	O
409	O
x3	O
fa	O
fb	O
fc	O
x4	O
graph	O
whose	O
unnormalized	O
joint	O
distribution	O
is	O
given	O
by	O
(	O
cid:4	O
)	O
p	O
(	O
x	O
)	O
=	O
fa	O
(	O
x1	O
,	O
x2	O
)	O
fb	O
(	O
x2	O
,	O
x3	O
)	O
fc	O
(	O
x2	O
,	O
x4	O
)	O
.	O
(	O
8.73	O
)	O
in	O
order	O
to	O
apply	O
the	O
sum-product	B
algorithm	I
to	O
this	O
graph	O
,	O
let	O
us	O
designate	O
node	B
x3	O
as	O
the	O
root	O
,	O
in	O
which	O
case	O
there	O
are	O
two	O
leaf	O
nodes	O
x1	O
and	O
x4	O
.	O
starting	O
with	O
the	O
leaf	O
nodes	O
,	O
we	O
then	O
have	O
the	O
following	O
sequence	O
of	O
six	O
messages	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
x4	O
x2	O
x3	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
x2	O
x2	O
(	O
8.74	O
)	O
(	O
8.75	O
)	O
(	O
8.76	O
)	O
(	O
8.77	O
)	O
(	O
8.78	O
)	O
(	O
8.79	O
)	O
(	O
8.80	O
)	O
(	O
8.81	O
)	O
(	O
8.82	O
)	O
(	O
8.83	O
)	O
(	O
8.84	O
)	O
(	O
8.85	O
)	O
the	O
direction	O
of	O
ﬂow	O
of	O
these	O
messages	O
is	O
illustrated	O
in	O
figure	O
8.52.	O
once	O
this	O
mes-	O
sage	O
propagation	O
is	O
complete	O
,	O
we	O
can	O
then	O
propagate	O
messages	O
from	O
the	O
root	B
node	I
out	O
to	O
the	O
leaf	O
nodes	O
,	O
and	O
these	O
are	O
given	O
by	O
µx1→fa	O
(	O
x1	O
)	O
=	O
1	O
µfa→x2	O
(	O
x2	O
)	O
=	O
x1	O
µx4→fc	O
(	O
x4	O
)	O
=	O
1	O
µfc→x2	O
(	O
x2	O
)	O
=	O
fa	O
(	O
x1	O
,	O
x2	O
)	O
fc	O
(	O
x2	O
,	O
x4	O
)	O
µx2→fb	O
(	O
x2	O
)	O
=	O
µfa→x2	O
(	O
x2	O
)	O
µfc→x2	O
(	O
x2	O
)	O
fb	O
(	O
x2	O
,	O
x3	O
)	O
µx2→fb	O
.	O
µfb→x3	O
(	O
x3	O
)	O
=	O
µx3→fb	O
(	O
x3	O
)	O
=	O
1	O
µfb→x2	O
(	O
x2	O
)	O
=	O
fb	O
(	O
x2	O
,	O
x3	O
)	O
µx2→fa	O
(	O
x2	O
)	O
=	O
µfb→x2	O
(	O
x2	O
)	O
µfc→x2	O
(	O
x2	O
)	O
µfa→x1	O
(	O
x1	O
)	O
=	O
fa	O
(	O
x1	O
,	O
x2	O
)	O
µx2→fa	O
(	O
x2	O
)	O
µx2→fc	O
(	O
x2	O
)	O
=	O
µfa→x2	O
(	O
x2	O
)	O
µfb→x2	O
(	O
x2	O
)	O
µfc→x4	O
(	O
x4	O
)	O
=	O
fc	O
(	O
x2	O
,	O
x4	O
)	O
µx2→fc	O
(	O
x2	O
)	O
.	O
410	O
8.	O
graphical	O
models	O
x1	O
x2	O
x3	O
x1	O
x2	O
x3	O
x4	O
(	O
a	O
)	O
x4	O
(	O
b	O
)	O
figure	O
8.52	O
flow	O
of	O
messages	O
for	O
the	O
sum-product	B
algorithm	I
applied	O
to	O
the	O
example	O
graph	O
in	O
figure	O
8.51	O
.	O
(	O
a	O
)	O
from	O
the	O
leaf	O
nodes	O
x1	O
and	O
x4	O
towards	O
the	O
root	B
node	I
x3	O
.	O
(	O
b	O
)	O
from	O
the	O
root	B
node	I
towards	O
the	O
leaf	O
nodes	O
.	O
one	O
message	O
has	O
now	O
passed	O
in	O
each	O
direction	O
across	O
each	O
link	B
,	O
and	O
we	O
can	O
now	O
evaluate	O
the	O
marginals	O
.	O
as	O
a	O
simple	O
check	O
,	O
let	O
us	O
verify	O
that	O
the	O
marginal	B
p	O
(	O
x2	O
)	O
is	O
given	O
by	O
the	O
correct	O
expression	O
.	O
using	O
(	O
8.63	O
)	O
and	O
substituting	O
for	O
the	O
messages	O
using	O
the	O
above	O
results	O
,	O
we	O
have	O
(	O
cid:4	O
)	O
p	O
(	O
x2	O
)	O
=	O
µfa→x2	O
(	O
x2	O
)	O
µfb→x2	O
(	O
x2	O
)	O
µfc→x2	O
(	O
x2	O
)	O
(	O
cid:31	O
)	O
(	O
cid:2	O
)	O
(	O
cid:31	O
)	O
(	O
cid:2	O
)	O
fa	O
(	O
x1	O
,	O
x2	O
)	O
fb	O
(	O
x2	O
,	O
x3	O
)	O
fc	O
(	O
x2	O
,	O
x4	O
)	O
(	O
cid:31	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
x1	O
x2	O
x1	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
x4	O
=	O
=	O
=	O
x3	O
x4	O
fa	O
(	O
x1	O
,	O
x2	O
)	O
fb	O
(	O
x2	O
,	O
x3	O
)	O
fc	O
(	O
x2	O
,	O
x4	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
x	O
)	O
(	O
8.86	O
)	O
x1	O
x3	O
x4	O
as	O
required	O
.	O
so	O
far	O
,	O
we	O
have	O
assumed	O
that	O
all	O
of	O
the	O
variables	O
in	O
the	O
graph	O
are	O
hidden	O
.	O
in	O
most	O
practical	O
applications	O
,	O
a	O
subset	O
of	O
the	O
variables	O
will	O
be	O
observed	O
,	O
and	O
we	O
wish	O
to	O
cal-	O
culate	O
posterior	O
distributions	O
conditioned	O
on	O
these	O
observations	O
.	O
observed	O
nodes	O
are	O
easily	O
handled	O
within	O
the	O
sum-product	B
algorithm	I
as	O
follows	O
.	O
suppose	O
we	O
partition	O
x	O
into	O
hidden	O
variables	O
h	O
and	O
observed	O
variables	O
v	O
,	O
and	O
that	O
the	O
observed	O
value	O
of	O
v	O
(	O
cid:21	O
)	O
is	O
denoted	O
(	O
cid:1	O
)	O
v.	O
then	O
we	O
simply	O
multiply	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
)	O
by	O
i	O
i	O
(	O
vi	O
,	O
(	O
cid:1	O
)	O
vi	O
)	O
,	O
where	O
i	O
(	O
v	O
,	O
(	O
cid:1	O
)	O
v	O
)	O
=	O
1	O
if	O
v	O
=	O
(	O
cid:1	O
)	O
v	O
and	O
i	O
(	O
v	O
,	O
(	O
cid:1	O
)	O
v	O
)	O
=	O
0	O
otherwise	O
.	O
this	O
product	O
corresponds	O
to	O
p	O
(	O
h	O
,	O
v	O
=	O
(	O
cid:1	O
)	O
v	O
)	O
and	O
hence	O
is	O
an	O
unnormalized	O
version	O
of	O
p	O
(	O
h|v	O
=	O
(	O
cid:1	O
)	O
v	O
)	O
.	O
by	O
run-	O
p	O
(	O
hi|v	O
=	O
(	O
cid:1	O
)	O
v	O
)	O
up	O
to	O
a	O
normalization	O
coefﬁcient	O
whose	O
value	O
can	O
be	O
found	O
efﬁciently	O
ning	O
the	O
sum-product	B
algorithm	I
,	O
we	O
can	O
efﬁciently	O
calculate	O
the	O
posterior	O
marginals	O
using	O
a	O
local	B
computation	O
.	O
any	O
summations	O
over	O
variables	O
in	O
v	O
then	O
collapse	O
into	O
a	O
single	O
term	O
.	O
we	O
have	O
assumed	O
throughout	O
this	O
section	O
that	O
we	O
are	O
dealing	O
with	O
discrete	O
vari-	O
ables	O
.	O
however	O
,	O
there	O
is	O
nothing	O
speciﬁc	O
to	O
discrete	O
variables	O
either	O
in	O
the	O
graphical	O
framework	O
or	O
in	O
the	O
probabilistic	O
construction	O
of	O
the	O
sum-product	B
algorithm	I
.	O
for	O
8.4.	O
inference	B
in	O
graphical	O
models	O
411	O
table	O
8.1	O
example	O
of	O
a	O
joint	O
distribution	O
over	O
two	O
binary	O
variables	O
for	O
which	O
the	O
maximum	O
of	O
the	O
joint	O
distribution	O
occurs	O
for	O
dif-	O
ferent	O
variable	O
values	O
compared	O
to	O
the	O
maxima	O
of	O
the	O
two	O
marginals	O
.	O
y	O
=	O
0	O
y	O
=	O
1	O
x	O
=	O
0	O
x	O
=	O
1	O
0.4	O
0.3	O
0.3	O
0.0	O
section	O
13.3	O
continuous	O
variables	O
the	O
summations	O
are	O
simply	O
replaced	O
by	O
integrations	O
.	O
we	O
shall	O
give	O
an	O
example	O
of	O
the	O
sum-product	B
algorithm	I
applied	O
to	O
a	O
graph	O
of	O
linear-gaussian	O
variables	O
when	O
we	O
consider	O
linear	O
dynamical	O
systems	O
.	O
8.4.5	O
the	O
max-sum	B
algorithm	I
the	O
sum-product	B
algorithm	I
allows	O
us	O
to	O
take	O
a	O
joint	O
distribution	O
p	O
(	O
x	O
)	O
expressed	O
as	O
a	O
factor	B
graph	I
and	O
efﬁciently	O
ﬁnd	O
marginals	O
over	O
the	O
component	O
variables	O
.	O
two	O
other	O
common	O
tasks	O
are	O
to	O
ﬁnd	O
a	O
setting	O
of	O
the	O
variables	O
that	O
has	O
the	O
largest	O
prob-	O
ability	O
and	O
to	O
ﬁnd	O
the	O
value	O
of	O
that	O
probability	B
.	O
these	O
can	O
be	O
addressed	O
through	O
a	O
closely	O
related	O
algorithm	O
called	O
max-sum	O
,	O
which	O
can	O
be	O
viewed	O
as	O
an	O
application	O
of	O
dynamic	B
programming	I
in	O
the	O
context	O
of	O
graphical	O
models	O
(	O
cormen	O
et	O
al.	O
,	O
2001	O
)	O
.	O
a	O
simple	O
approach	O
to	O
ﬁnding	O
latent	B
variable	I
values	O
having	O
high	O
probability	B
would	O
be	O
to	O
run	O
the	O
sum-product	B
algorithm	I
to	O
obtain	O
the	O
marginals	O
p	O
(	O
xi	O
)	O
for	O
ev-	O
ery	O
variable	O
,	O
and	O
then	O
,	O
for	O
each	O
marginal	B
in	O
turn	O
,	O
to	O
ﬁnd	O
the	O
value	O
x	O
(	O
cid:1	O
)	O
i	O
that	O
maximizes	O
that	O
marginal	B
.	O
however	O
,	O
this	O
would	O
give	O
the	O
set	O
of	O
values	O
that	O
are	O
individually	O
the	O
most	O
probable	O
.	O
in	O
practice	O
,	O
we	O
typically	O
wish	O
to	O
ﬁnd	O
the	O
set	O
of	O
values	O
that	O
jointly	O
have	O
the	O
largest	O
probability	B
,	O
in	O
other	O
words	O
the	O
vector	O
xmax	O
that	O
maximizes	O
the	O
joint	O
distribution	O
,	O
so	O
that	O
xmax	O
=	O
arg	O
max	O
p	O
(	O
x	O
)	O
for	O
which	O
the	O
corresponding	O
value	O
of	O
the	O
joint	O
probability	B
will	O
be	O
given	O
by	O
p	O
(	O
xmax	O
)	O
=	O
max	O
x	O
p	O
(	O
x	O
)	O
.	O
x	O
(	O
8.87	O
)	O
(	O
8.88	O
)	O
in	O
general	O
,	O
xmax	O
is	O
not	O
the	O
same	O
as	O
the	O
set	O
of	O
x	O
(	O
cid:1	O
)	O
i	O
values	O
,	O
as	O
we	O
can	O
easily	O
show	O
using	O
a	O
simple	O
example	O
.	O
consider	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
y	O
)	O
over	O
two	O
binary	O
variables	O
x	O
,	O
y	O
∈	O
{	O
0	O
,	O
1	O
}	O
given	O
in	O
table	O
8.1.	O
the	O
joint	O
distribution	O
is	O
maximized	O
by	O
setting	O
x	O
=	O
1	O
and	O
y	O
=	O
0	O
,	O
corresponding	O
the	O
value	O
0.4.	O
however	O
,	O
the	O
marginal	B
for	O
p	O
(	O
x	O
)	O
,	O
obtained	O
by	O
summing	O
over	O
both	O
values	O
of	O
y	O
,	O
is	O
given	O
by	O
p	O
(	O
x	O
=	O
0	O
)	O
=	O
0.6	O
and	O
p	O
(	O
x	O
=	O
1	O
)	O
=	O
0.4	O
,	O
and	O
similarly	O
the	O
marginal	B
for	O
y	O
is	O
given	O
by	O
p	O
(	O
y	O
=	O
0	O
)	O
=	O
0.7	O
and	O
p	O
(	O
y	O
=	O
1	O
)	O
=	O
0.3	O
,	O
and	O
so	O
the	O
marginals	O
are	O
maximized	O
by	O
x	O
=	O
0	O
and	O
y	O
=	O
0	O
,	O
which	O
corresponds	O
to	O
a	O
value	O
of	O
0.3	O
for	O
the	O
joint	O
distribution	O
.	O
in	O
fact	O
,	O
it	O
is	O
not	O
difﬁcult	O
to	O
construct	O
examples	O
for	O
which	O
the	O
set	O
of	O
individually	O
most	O
probable	O
values	O
has	O
probability	B
zero	O
under	O
the	O
joint	O
distribution	O
.	O
we	O
therefore	O
seek	O
an	O
efﬁcient	O
algorithm	O
for	O
ﬁnding	O
the	O
value	O
of	O
x	O
that	O
maxi-	O
mizes	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
)	O
and	O
that	O
will	O
allow	O
us	O
to	O
obtain	O
the	O
value	O
of	O
the	O
joint	O
distribution	O
at	O
its	O
maximum	O
.	O
to	O
address	O
the	O
second	O
of	O
these	O
problems	O
,	O
we	O
shall	O
simply	O
write	O
out	O
the	O
max	O
operator	O
in	O
terms	O
of	O
its	O
components	O
max	O
x	O
p	O
(	O
x	O
)	O
=	O
max	O
x1	O
.	O
.	O
.	O
max	O
xm	O
p	O
(	O
x	O
)	O
(	O
8.89	O
)	O
exercise	O
8.27	O
412	O
8.	O
graphical	O
models	O
where	O
m	O
is	O
the	O
total	O
number	O
of	O
variables	O
,	O
and	O
then	O
substitute	O
for	O
p	O
(	O
x	O
)	O
using	O
its	O
expansion	O
in	O
terms	O
of	O
a	O
product	O
of	O
factors	O
.	O
in	O
deriving	O
the	O
sum-product	B
algorithm	I
,	O
we	O
made	O
use	O
of	O
the	O
distributive	O
law	O
(	O
8.53	O
)	O
for	O
multiplication	O
.	O
here	O
we	O
make	O
use	O
of	O
the	O
analogous	O
law	O
for	O
the	O
max	O
operator	O
(	O
8.90	O
)	O
which	O
holds	O
if	O
a	O
(	O
cid:2	O
)	O
0	O
(	O
as	O
will	O
always	O
be	O
the	O
case	O
for	O
the	O
factors	O
in	O
a	O
graphical	B
model	I
)	O
.	O
this	O
allows	O
us	O
to	O
exchange	O
products	O
with	O
maximizations	O
.	O
max	O
(	O
ab	O
,	O
ac	O
)	O
=	O
a	O
max	O
(	O
b	O
,	O
c	O
)	O
consider	O
ﬁrst	O
the	O
simple	O
example	O
of	O
a	O
chain	O
of	O
nodes	O
described	O
by	O
(	O
8.49	O
)	O
.	O
the	O
evaluation	O
of	O
the	O
probability	B
maximum	O
can	O
be	O
written	O
as	O
max	O
x	O
p	O
(	O
x	O
)	O
=	O
max	O
x1	O
(	O
cid:29	O
)	O
1	O
z	O
···	O
max	O
xn	O
(	O
cid:29	O
)	O
=	O
1	O
z	O
max	O
x1	O
ψ1,2	O
(	O
x1	O
,	O
x2	O
)	O
[	O
ψ1,2	O
(	O
x1	O
,	O
x2	O
)	O
···	O
ψn−1	O
,	O
n	O
(	O
xn−1	O
,	O
xn	O
)	O
]	O
···	O
max	O
ψn−1	O
,	O
n	O
(	O
xn−1	O
,	O
xn	O
)	O
.	O
xn	O
(	O
cid:30	O
)	O
(	O
cid:30	O
)	O
as	O
with	O
the	O
calculation	O
of	O
marginals	O
,	O
we	O
see	O
that	O
exchanging	O
the	O
max	O
and	O
product	O
operators	O
results	O
in	O
a	O
much	O
more	O
efﬁcient	O
computation	O
,	O
and	O
one	O
that	O
is	O
easily	O
inter-	O
preted	O
in	O
terms	O
of	O
messages	O
passed	O
from	O
node	B
xn	O
backwards	O
along	O
the	O
chain	O
to	O
node	B
x1	O
.	O
we	O
can	O
readily	O
generalize	O
this	O
result	O
to	O
arbitrary	O
tree-structured	O
factor	O
graphs	O
by	O
substituting	O
the	O
expression	O
(	O
8.59	O
)	O
for	O
the	O
factor	B
graph	I
expansion	O
into	O
(	O
8.89	O
)	O
and	O
again	O
exchanging	O
maximizations	O
with	O
products	O
.	O
the	O
structure	O
of	O
this	O
calculation	O
is	O
identical	O
to	O
that	O
of	O
the	O
sum-product	B
algorithm	I
,	O
and	O
so	O
we	O
can	O
simply	O
translate	O
those	O
results	O
into	O
the	O
present	O
context	O
.	O
in	O
particular	O
,	O
suppose	O
that	O
we	O
designate	O
a	O
particular	O
variable	O
node	B
as	O
the	O
‘	O
root	O
’	O
of	O
the	O
graph	O
.	O
then	O
we	O
start	O
a	O
set	O
of	O
messages	O
propagating	O
inwards	O
from	O
the	O
leaves	O
of	O
the	O
tree	B
towards	O
the	O
root	O
,	O
with	O
each	O
node	B
sending	O
its	O
message	O
towards	O
the	O
root	O
once	O
it	O
has	O
received	O
all	O
incoming	O
messages	O
from	O
its	O
other	O
neighbours	O
.	O
the	O
ﬁnal	O
maximization	O
is	O
performed	O
over	O
the	O
product	O
of	O
all	O
messages	O
arriving	O
at	O
the	O
root	B
node	I
,	O
and	O
gives	O
the	O
maximum	O
value	O
for	O
p	O
(	O
x	O
)	O
.	O
this	O
could	O
be	O
called	O
the	O
max-product	O
algorithm	O
and	O
is	O
identical	O
to	O
the	O
sum-product	B
algorithm	I
except	O
that	O
summations	O
are	O
replaced	O
by	O
maximizations	O
.	O
note	O
that	O
at	O
this	O
stage	O
,	O
messages	O
have	O
been	O
sent	O
from	O
leaves	O
to	O
the	O
root	O
,	O
but	O
not	O
in	O
the	O
other	O
direction	O
.	O
in	O
practice	O
,	O
products	O
of	O
many	O
small	O
probabilities	O
can	O
lead	O
to	O
numerical	O
under-	O
ﬂow	O
problems	O
,	O
and	O
so	O
it	O
is	O
convenient	O
to	O
work	O
with	O
the	O
logarithm	O
of	O
the	O
joint	O
distri-	O
bution	O
.	O
the	O
logarithm	O
is	O
a	O
monotonic	O
function	O
,	O
so	O
that	O
if	O
a	O
>	O
b	O
then	O
ln	O
a	O
>	O
ln	O
b	O
,	O
and	O
hence	O
the	O
max	O
operator	O
and	O
the	O
logarithm	O
function	O
can	O
be	O
interchanged	O
,	O
so	O
that	O
(	O
cid:17	O
)	O
(	O
cid:18	O
)	O
ln	O
max	O
x	O
p	O
(	O
x	O
)	O
=	O
max	O
x	O
ln	O
p	O
(	O
x	O
)	O
.	O
the	O
distributive	O
property	O
is	O
preserved	O
because	O
max	O
(	O
a	O
+	O
b	O
,	O
a	O
+	O
c	O
)	O
=	O
a	O
+	O
max	O
(	O
b	O
,	O
c	O
)	O
.	O
(	O
8.91	O
)	O
(	O
8.92	O
)	O
thus	O
taking	O
the	O
logarithm	O
simply	O
has	O
the	O
effect	O
of	O
replacing	O
the	O
products	O
in	O
the	O
max-product	O
algorithm	O
with	O
sums	O
,	O
and	O
so	O
we	O
obtain	O
the	O
max-sum	B
algorithm	I
.	O
from	O
8.4.	O
inference	B
in	O
graphical	O
models	O
413	O
the	O
results	O
(	O
8.66	O
)	O
and	O
(	O
8.69	O
)	O
derived	O
earlier	O
for	O
the	O
sum-product	B
algorithm	I
,	O
we	O
can	O
readily	O
write	O
down	O
the	O
max-sum	B
algorithm	I
in	O
terms	O
of	O
message	B
passing	I
simply	O
by	O
replacing	O
‘	O
sum	O
’	O
with	O
‘	O
max	O
’	O
and	O
replacing	O
products	O
with	O
sums	O
of	O
logarithms	O
to	O
give	O
⎡⎣ln	O
f	O
(	O
x	O
,	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
+	O
(	O
cid:2	O
)	O
µxm→f	O
(	O
xm	O
)	O
m∈ne	O
(	O
fs	O
)	O
\x	O
µx→f	O
(	O
x	O
)	O
=	O
µfl→x	O
(	O
x	O
)	O
.	O
(	O
8.94	O
)	O
µf→x	O
(	O
x	O
)	O
=	O
max	O
x1	O
,	O
...	O
,	O
xm	O
(	O
cid:2	O
)	O
l∈ne	O
(	O
x	O
)	O
\f	O
the	O
initial	O
messages	O
sent	O
by	O
the	O
leaf	O
nodes	O
are	O
obtained	O
by	O
analogy	O
with	O
(	O
8.70	O
)	O
and	O
(	O
8.71	O
)	O
and	O
are	O
given	O
by	O
⎤⎦	O
(	O
8.93	O
)	O
(	O
8.95	O
)	O
(	O
8.96	O
)	O
(	O
8.97	O
)	O
µx→f	O
(	O
x	O
)	O
=	O
0	O
µf→x	O
(	O
x	O
)	O
=	O
ln	O
f	O
(	O
x	O
)	O
pmax	O
=	O
max	O
x	O
µfs→x	O
(	O
x	O
)	O
⎡⎣	O
(	O
cid:2	O
)	O
s∈ne	O
(	O
x	O
)	O
⎡⎣	O
(	O
cid:2	O
)	O
s∈ne	O
(	O
x	O
)	O
⎤⎦	O
.	O
⎤⎦	O
.	O
while	O
at	O
the	O
root	B
node	I
the	O
maximum	O
probability	O
can	O
then	O
be	O
computed	O
,	O
by	O
analogy	O
with	O
(	O
8.63	O
)	O
,	O
using	O
so	O
far	O
,	O
we	O
have	O
seen	O
how	O
to	O
ﬁnd	O
the	O
maximum	O
of	O
the	O
joint	O
distribution	O
by	O
prop-	O
agating	O
messages	O
from	O
the	O
leaves	O
to	O
an	O
arbitrarily	O
chosen	O
root	B
node	I
.	O
the	O
result	O
will	O
be	O
the	O
same	O
irrespective	O
of	O
which	O
node	B
is	O
chosen	O
as	O
the	O
root	O
.	O
now	O
we	O
turn	O
to	O
the	O
second	O
problem	O
of	O
ﬁnding	O
the	O
conﬁguration	O
of	O
the	O
variables	O
for	O
which	O
the	O
joint	O
dis-	O
tribution	O
attains	O
this	O
maximum	O
value	O
.	O
so	O
far	O
,	O
we	O
have	O
sent	O
messages	O
from	O
the	O
leaves	O
to	O
the	O
root	O
.	O
the	O
process	O
of	O
evaluating	O
(	O
8.97	O
)	O
will	O
also	O
give	O
the	O
value	O
xmax	O
for	O
the	O
most	O
probable	O
value	O
of	O
the	O
root	B
node	I
variable	O
,	O
deﬁned	O
by	O
xmax	O
=	O
arg	O
max	O
x	O
µfs→x	O
(	O
x	O
)	O
(	O
8.98	O
)	O
at	O
this	O
point	O
,	O
we	O
might	O
be	O
tempted	O
simply	O
to	O
continue	O
with	O
the	O
message	B
passing	I
al-	O
gorithm	O
and	O
send	O
messages	O
from	O
the	O
root	O
back	O
out	O
to	O
the	O
leaves	O
,	O
using	O
(	O
8.93	O
)	O
and	O
(	O
8.94	O
)	O
,	O
then	O
apply	O
(	O
8.98	O
)	O
to	O
all	O
of	O
the	O
remaining	O
variable	O
nodes	O
.	O
however	O
,	O
because	O
we	O
are	O
now	O
maximizing	O
rather	O
than	O
summing	O
,	O
it	O
is	O
possible	O
that	O
there	O
may	O
be	O
mul-	O
tiple	O
conﬁgurations	O
of	O
x	O
all	O
of	O
which	O
give	O
rise	O
to	O
the	O
maximum	O
value	O
for	O
p	O
(	O
x	O
)	O
.	O
in	O
such	O
cases	O
,	O
this	O
strategy	O
can	O
fail	O
because	O
it	O
is	O
possible	O
for	O
the	O
individual	O
variable	O
values	O
obtained	O
by	O
maximizing	O
the	O
product	O
of	O
messages	O
at	O
each	O
node	B
to	O
belong	O
to	O
different	O
maximizing	O
conﬁgurations	O
,	O
giving	O
an	O
overall	O
conﬁguration	O
that	O
no	O
longer	O
corresponds	O
to	O
a	O
maximum	O
.	O
the	O
problem	O
can	O
be	O
resolved	O
by	O
adopting	O
a	O
rather	O
different	O
kind	O
of	O
message	B
passing	I
from	O
the	O
root	B
node	I
to	O
the	O
leaves	O
.	O
to	O
see	O
how	O
this	O
works	O
,	O
let	O
us	O
return	O
once	O
again	O
to	O
the	O
simple	O
chain	O
example	O
of	O
n	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
each	O
having	O
k	O
states	O
,	O
414	O
8.	O
graphical	O
models	O
figure	O
8.53	O
a	O
lattice	O
,	O
or	O
trellis	O
,	O
diagram	O
show-	O
ing	O
explicitly	O
the	O
k	O
possible	O
states	O
(	O
one	O
per	O
row	O
of	O
the	O
diagram	O
)	O
for	O
each	O
of	O
the	O
variables	O
xn	O
in	O
the	O
in	O
this	O
illustration	O
k	O
=	O
3.	O
the	O
ar-	O
chain	O
model	O
.	O
row	O
shows	O
the	O
direction	O
of	O
message	B
passing	I
in	O
the	O
max-product	O
algorithm	O
.	O
for	O
every	O
state	O
k	O
of	O
each	O
variable	O
xn	O
(	O
corresponding	O
to	O
column	O
n	O
of	O
the	O
dia-	O
gram	O
)	O
the	O
function	O
φ	O
(	O
xn	O
)	O
deﬁnes	O
a	O
unique	O
state	O
at	O
the	O
previous	O
variable	O
,	O
indicated	O
by	O
the	O
black	O
lines	O
.	O
the	O
two	O
paths	O
through	O
the	O
lattice	O
correspond	O
to	O
conﬁgurations	O
that	O
give	O
the	O
global	O
maximum	O
of	O
the	O
joint	O
probability	B
distribution	O
,	O
and	O
either	O
of	O
these	O
can	O
be	O
found	O
by	O
tracing	O
back	O
along	O
the	O
black	O
lines	O
in	O
the	O
opposite	O
direction	O
to	O
the	O
arrow	O
.	O
k	O
=	O
1	O
k	O
=	O
2	O
k	O
=	O
3	O
n	O
−	O
2	O
n	O
−	O
1	O
n	O
n	O
+	O
1	O
corresponding	O
to	O
the	O
graph	O
shown	O
in	O
figure	O
8.38.	O
suppose	O
we	O
take	O
node	B
xn	O
to	O
be	O
the	O
root	B
node	I
.	O
then	O
in	O
the	O
ﬁrst	O
phase	O
,	O
we	O
propagate	O
messages	O
from	O
the	O
leaf	O
node	B
x1	O
to	O
the	O
root	B
node	I
using	O
µxn→fn	O
,	O
n+1	O
(	O
xn	O
)	O
=	O
µfn−1	O
,	O
n→xn	O
(	O
xn	O
)	O
µfn−1	O
,	O
n→xn	O
(	O
xn	O
)	O
=	O
max	O
xn−1	O
ln	O
fn−1	O
,	O
n	O
(	O
xn−1	O
,	O
xn	O
)	O
+	O
µxn−1→f	O
n−1	O
,	O
n	O
(	O
xn	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
which	O
follow	O
from	O
applying	O
(	O
8.94	O
)	O
and	O
(	O
8.93	O
)	O
to	O
this	O
particular	O
graph	O
.	O
the	O
initial	O
message	O
sent	O
from	O
the	O
leaf	O
node	B
is	O
simply	O
the	O
most	O
probable	O
value	O
for	O
xn	O
is	O
then	O
given	O
by	O
µx1→f1,2	O
(	O
x1	O
)	O
=	O
0	O
.	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
(	O
8.99	O
)	O
.	O
(	O
8.100	O
)	O
xmax	O
n	O
=	O
arg	O
max	O
xn	O
µfn−1	O
,	O
n→xn	O
(	O
xn	O
)	O
now	O
we	O
need	O
to	O
determine	O
the	O
states	O
of	O
the	O
previous	O
variables	O
that	O
correspond	O
to	O
the	O
same	O
maximizing	O
conﬁguration	O
.	O
this	O
can	O
be	O
done	O
by	O
keeping	O
track	O
of	O
which	O
values	O
of	O
the	O
variables	O
gave	O
rise	O
to	O
the	O
maximum	O
state	O
of	O
each	O
variable	O
,	O
in	O
other	O
words	O
by	O
storing	O
quantities	O
given	O
by	O
φ	O
(	O
xn	O
)	O
=	O
arg	O
max	O
xn−1	O
ln	O
fn−1	O
,	O
n	O
(	O
xn−1	O
,	O
xn	O
)	O
+	O
µxn−1→f	O
n−1	O
,	O
n	O
(	O
xn	O
)	O
.	O
(	O
8.101	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
to	O
understand	O
better	O
what	O
is	O
happening	O
,	O
it	O
is	O
helpful	O
to	O
represent	O
the	O
chain	O
of	O
vari-	O
ables	O
in	O
terms	O
of	O
a	O
lattice	O
or	O
trellis	B
diagram	I
as	O
shown	O
in	O
figure	O
8.53.	O
note	O
that	O
this	O
is	O
not	O
a	O
probabilistic	B
graphical	I
model	I
because	O
the	O
nodes	O
represent	O
individual	O
states	O
of	O
variables	O
,	O
while	O
each	O
variable	O
corresponds	O
to	O
a	O
column	O
of	O
such	O
states	O
in	O
the	O
di-	O
agram	O
.	O
for	O
each	O
state	O
of	O
a	O
given	O
variable	O
,	O
there	O
is	O
a	O
unique	O
state	O
of	O
the	O
previous	O
variable	O
that	O
maximizes	O
the	O
probability	B
(	O
ties	O
are	O
broken	O
either	O
systematically	O
or	O
at	O
random	O
)	O
,	O
corresponding	O
to	O
the	O
function	O
φ	O
(	O
xn	O
)	O
given	O
by	O
(	O
8.101	O
)	O
,	O
and	O
this	O
is	O
indicated	O
8.4.	O
inference	B
in	O
graphical	O
models	O
415	O
by	O
the	O
lines	O
connecting	O
the	O
nodes	O
.	O
once	O
we	O
know	O
the	O
most	O
probable	O
value	O
of	O
the	O
ﬁ-	O
nal	O
node	B
xn	O
,	O
we	O
can	O
then	O
simply	O
follow	O
the	O
link	B
back	O
to	O
ﬁnd	O
the	O
most	O
probable	O
state	O
of	O
node	B
xn−1	O
and	O
so	O
on	O
back	O
to	O
the	O
initial	O
node	B
x1	O
.	O
this	O
corresponds	O
to	O
propagating	O
a	O
message	O
back	O
down	O
the	O
chain	O
using	O
n−1	O
=	O
φ	O
(	O
xmax	O
xmax	O
n	O
)	O
(	O
8.102	O
)	O
and	O
is	O
known	O
as	O
back-tracking	B
.	O
note	O
that	O
there	O
could	O
be	O
several	O
values	O
of	O
xn−1	O
all	O
of	O
which	O
give	O
the	O
maximum	O
value	O
in	O
(	O
8.101	O
)	O
.	O
provided	O
we	O
chose	O
one	O
of	O
these	O
values	O
when	O
we	O
do	O
the	O
back-tracking	B
,	O
we	O
are	O
assured	O
of	O
a	O
globally	O
consistent	B
maximizing	O
conﬁguration	O
.	O
in	O
figure	O
8.53	O
,	O
we	O
have	O
indicated	O
two	O
paths	O
,	O
each	O
of	O
which	O
we	O
shall	O
suppose	O
corresponds	O
to	O
a	O
global	O
maximum	O
of	O
the	O
joint	O
probability	B
distribution	O
.	O
if	O
k	O
=	O
2	O
and	O
k	O
=	O
3	O
each	O
represent	O
possible	O
values	O
of	O
xmax	O
n	O
,	O
then	O
starting	O
from	O
either	O
state	O
and	O
tracing	O
back	O
along	O
the	O
black	O
lines	O
,	O
which	O
corresponds	O
to	O
iterating	O
(	O
8.102	O
)	O
,	O
we	O
obtain	O
a	O
valid	O
global	O
maximum	O
conﬁguration	O
.	O
note	O
that	O
if	O
we	O
had	O
run	O
a	O
forward	O
pass	O
of	O
max-sum	O
message	O
passing	O
followed	O
by	O
a	O
backward	O
pass	O
and	O
then	O
applied	O
(	O
8.98	O
)	O
at	O
each	O
node	B
separately	O
,	O
we	O
could	O
end	O
up	O
selecting	O
some	O
states	O
from	O
one	O
path	O
and	O
some	O
from	O
the	O
other	O
path	O
,	O
giving	O
an	O
overall	O
conﬁguration	O
that	O
is	O
not	O
a	O
global	O
maximizer	O
.	O
we	O
see	O
that	O
it	O
is	O
necessary	O
instead	O
to	O
keep	O
track	O
of	O
the	O
maximizing	O
states	O
during	O
the	O
forward	O
pass	O
using	O
the	O
functions	O
φ	O
(	O
xn	O
)	O
and	O
then	O
use	O
back-tracking	B
to	O
ﬁnd	O
a	O
consistent	B
solution	O
.	O
the	O
extension	O
to	O
a	O
general	O
tree-structured	O
factor	B
graph	I
should	O
now	O
be	O
clear	O
.	O
if	O
a	O
message	O
is	O
sent	O
from	O
a	O
factor	O
node	O
f	O
to	O
a	O
variable	O
node	B
x	O
,	O
a	O
maximization	O
is	O
performed	O
over	O
all	O
other	O
variable	O
nodes	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
that	O
are	O
neighbours	O
of	O
that	O
fac-	O
tor	O
node	B
,	O
using	O
(	O
8.93	O
)	O
.	O
when	O
we	O
perform	O
this	O
maximization	O
,	O
we	O
keep	O
a	O
record	O
of	O
which	O
values	O
of	O
the	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
gave	O
rise	O
to	O
the	O
maximum	O
.	O
then	O
in	O
the	O
back-tracking	B
step	O
,	O
having	O
found	O
xmax	O
,	O
we	O
can	O
then	O
use	O
these	O
stored	O
values	O
to	O
as-	O
sign	O
consistent	B
maximizing	O
states	O
xmax	O
m	O
.	O
the	O
max-sum	B
algorithm	I
,	O
with	O
back-tracking	B
,	O
gives	O
an	O
exact	O
maximizing	O
conﬁguration	O
for	O
the	O
variables	O
provided	O
the	O
factor	B
graph	I
is	O
a	O
tree	B
.	O
an	O
important	O
application	O
of	O
this	O
technique	O
is	O
for	O
ﬁnding	O
the	O
most	O
probable	O
sequence	O
of	O
hidden	O
states	O
in	O
a	O
hidden	O
markov	O
model	O
,	O
in	O
which	O
case	O
it	O
is	O
known	O
as	O
the	O
viterbi	O
algorithm	O
.	O
,	O
.	O
.	O
.	O
,	O
xmax	O
1	O
as	O
with	O
the	O
sum-product	B
algorithm	I
,	O
the	O
inclusion	O
of	O
evidence	O
in	O
the	O
form	O
of	O
observed	O
variables	O
is	O
straightforward	O
.	O
the	O
observed	O
variables	O
are	O
clamped	O
to	O
their	O
observed	O
values	O
,	O
and	O
the	O
maximization	O
is	O
performed	O
over	O
the	O
remaining	O
hidden	O
vari-	O
ables	O
.	O
this	O
can	O
be	O
shown	O
formally	O
by	O
including	O
identity	O
functions	O
for	O
the	O
observed	O
variables	O
into	O
the	O
factor	O
functions	O
,	O
as	O
we	O
did	O
for	O
the	O
sum-product	B
algorithm	I
.	O
it	O
is	O
interesting	O
to	O
compare	O
max-sum	O
with	O
the	O
iterated	B
conditional	I
modes	I
(	O
icm	O
)	O
algorithm	O
described	O
on	O
page	O
389.	O
each	O
step	O
in	O
icm	O
is	O
computationally	O
simpler	O
be-	O
cause	O
the	O
‘	O
messages	O
’	O
that	O
are	O
passed	O
from	O
one	O
node	B
to	O
the	O
next	O
comprise	O
a	O
single	O
value	O
consisting	O
of	O
the	O
new	O
state	O
of	O
the	O
node	B
for	O
which	O
the	O
conditional	B
distribution	O
is	O
maximized	O
.	O
the	O
max-sum	B
algorithm	I
is	O
more	O
complex	O
because	O
the	O
messages	O
are	O
functions	O
of	O
node	B
variables	O
x	O
and	O
hence	O
comprise	O
a	O
set	O
of	O
k	O
values	O
for	O
each	O
pos-	O
sible	O
state	O
of	O
x.	O
unlike	O
max-sum	O
,	O
however	O
,	O
icm	O
is	O
not	O
guaranteed	O
to	O
ﬁnd	O
a	O
global	O
maximum	O
even	O
for	O
tree-structured	O
graphs	O
.	O
section	O
13.2	O
416	O
8.	O
graphical	O
models	O
8.4.6	O
exact	O
inference	O
in	O
general	O
graphs	O
the	O
sum-product	O
and	O
max-sum	O
algorithms	O
provide	O
efﬁcient	O
and	O
exact	O
solutions	O
to	O
inference	B
problems	O
in	O
tree-structured	O
graphs	O
.	O
for	O
many	O
practical	O
applications	O
,	O
however	O
,	O
we	O
have	O
to	O
deal	O
with	O
graphs	O
having	O
loops	O
.	O
the	O
message	B
passing	I
framework	O
can	O
be	O
generalized	B
to	O
arbitrary	O
graph	O
topolo-	O
gies	O
,	O
giving	O
an	O
exact	O
inference	O
procedure	O
known	O
as	O
the	O
junction	B
tree	I
algorithm	I
(	O
lau-	O
ritzen	O
and	O
spiegelhalter	O
,	O
1988	O
;	O
jordan	O
,	O
2007	O
)	O
.	O
here	O
we	O
give	O
a	O
brief	O
outline	O
of	O
the	O
key	O
steps	O
involved	O
.	O
this	O
is	O
not	O
intended	O
to	O
convey	O
a	O
detailed	O
understanding	O
of	O
the	O
algorithm	O
,	O
but	O
rather	O
to	O
give	O
a	O
ﬂavour	O
of	O
the	O
various	O
stages	O
involved	O
.	O
if	O
the	O
starting	O
point	O
is	O
a	O
directed	B
graph	O
,	O
it	O
is	O
ﬁrst	O
converted	O
to	O
an	O
undirected	B
graph	I
by	O
moraliza-	O
tion	O
,	O
whereas	O
if	O
starting	O
from	O
an	O
undirected	B
graph	I
this	O
step	O
is	O
not	O
required	O
.	O
next	O
the	O
graph	O
is	O
triangulated	B
,	O
which	O
involves	O
ﬁnding	O
chord-less	O
cycles	O
containing	O
four	O
or	O
more	O
nodes	O
and	O
adding	O
extra	O
links	O
to	O
eliminate	O
such	O
chord-less	O
cycles	O
.	O
for	O
instance	O
,	O
in	O
the	O
graph	O
in	O
figure	O
8.36	O
,	O
the	O
cycle	O
a–c–b–d–a	O
is	O
chord-less	O
a	O
link	B
could	O
be	O
added	O
between	O
a	O
and	O
b	O
or	O
alternatively	O
between	O
c	O
and	O
d.	O
note	O
that	O
the	O
joint	O
dis-	O
tribution	O
for	O
the	O
resulting	O
triangulated	B
graph	I
is	O
still	O
deﬁned	O
by	O
a	O
product	O
of	O
the	O
same	O
potential	O
functions	O
,	O
but	O
these	O
are	O
now	O
considered	O
to	O
be	O
functions	O
over	O
expanded	O
sets	O
of	O
variables	O
.	O
next	O
the	O
triangulated	B
graph	I
is	O
used	O
to	O
construct	O
a	O
new	O
tree-structured	O
undirected	B
graph	I
called	O
a	O
join	B
tree	I
,	O
whose	O
nodes	O
correspond	O
to	O
the	O
maximal	O
cliques	O
of	O
the	O
triangulated	B
graph	I
,	O
and	O
whose	O
links	O
connect	O
pairs	O
of	O
cliques	O
that	O
have	O
vari-	O
ables	O
in	O
common	O
.	O
the	O
selection	O
of	O
which	O
pairs	O
of	O
cliques	O
to	O
connect	O
in	O
this	O
way	O
is	O
important	O
and	O
is	O
done	O
so	O
as	O
to	O
give	O
a	O
maximal	B
spanning	I
tree	I
deﬁned	O
as	O
follows	O
.	O
of	O
all	O
possible	O
trees	O
that	O
link	B
up	O
the	O
cliques	O
,	O
the	O
one	O
that	O
is	O
chosen	O
is	O
one	O
for	O
which	O
the	O
weight	O
of	O
the	O
tree	B
is	O
largest	O
,	O
where	O
the	O
weight	O
for	O
a	O
link	B
is	O
the	O
number	O
of	O
nodes	O
shared	O
by	O
the	O
two	O
cliques	O
it	O
connects	O
,	O
and	O
the	O
weight	O
for	O
the	O
tree	B
is	O
the	O
sum	O
of	O
the	O
weights	O
for	O
the	O
links	O
.	O
if	O
the	O
tree	B
is	O
condensed	O
,	O
so	O
that	O
any	O
clique	B
that	O
is	O
a	O
subset	O
of	O
another	O
clique	B
is	O
absorbed	O
into	O
the	O
larger	O
clique	B
,	O
this	O
gives	O
a	O
junction	O
tree	O
.	O
as	O
a	O
consequence	O
of	O
the	O
triangulation	O
step	O
,	O
the	O
resulting	O
tree	B
satisﬁes	O
the	O
running	B
intersection	I
property	I
,	O
which	O
means	O
that	O
if	O
a	O
variable	O
is	O
contained	O
in	O
two	O
cliques	O
,	O
then	O
it	O
must	O
also	O
be	O
con-	O
tained	O
in	O
every	O
clique	B
on	O
the	O
path	O
that	O
connects	O
them	O
.	O
this	O
ensures	O
that	O
inference	B
about	O
variables	O
will	O
be	O
consistent	B
across	O
the	O
graph	O
.	O
finally	O
,	O
a	O
two-stage	O
message	B
passing	I
algorithm	O
,	O
essentially	O
equivalent	O
to	O
the	O
sum-product	B
algorithm	I
,	O
can	O
now	O
be	O
applied	O
to	O
this	O
junction	O
tree	O
in	O
order	O
to	O
ﬁnd	O
marginals	O
and	O
conditionals	O
.	O
although	O
the	O
junction	B
tree	I
algorithm	I
sounds	O
complicated	O
,	O
at	O
its	O
heart	O
is	O
the	O
simple	O
idea	O
that	O
we	O
have	O
used	O
already	O
of	O
exploiting	O
the	O
factorization	B
properties	O
of	O
the	O
distribution	O
to	O
allow	O
sums	O
and	O
products	O
to	O
be	O
interchanged	O
so	O
that	O
partial	O
summations	O
can	O
be	O
per-	O
formed	O
,	O
thereby	O
avoiding	O
having	O
to	O
work	O
directly	O
with	O
the	O
joint	O
distribution	O
.	O
the	O
role	O
of	O
the	O
junction	O
tree	O
is	O
to	O
provide	O
a	O
precise	O
and	O
efﬁcient	O
way	O
to	O
organize	O
these	O
computations	O
.	O
it	O
is	O
worth	O
emphasizing	O
that	O
this	O
is	O
achieved	O
using	O
purely	O
graphical	O
operations	O
!	O
the	O
junction	O
tree	O
is	O
exact	O
for	O
arbitrary	O
graphs	O
and	O
is	O
efﬁcient	O
in	O
the	O
sense	O
that	O
for	O
a	O
given	O
graph	O
there	O
does	O
not	O
in	O
general	O
exist	O
a	O
computationally	O
cheaper	O
approach	O
.	O
unfortunately	O
,	O
the	O
algorithm	O
must	O
work	O
with	O
the	O
joint	O
distributions	O
within	O
each	O
node	B
(	O
each	O
of	O
which	O
corresponds	O
to	O
a	O
clique	B
of	O
the	O
triangulated	B
graph	I
)	O
and	O
so	O
the	O
compu-	O
tational	O
cost	O
of	O
the	O
algorithm	O
is	O
determined	O
by	O
the	O
number	O
of	O
variables	O
in	O
the	O
largest	O
8.4.	O
inference	B
in	O
graphical	O
models	O
417	O
clique	B
and	O
will	O
grow	O
exponentially	O
with	O
this	O
number	O
in	O
the	O
case	O
of	O
discrete	O
variables	O
.	O
an	O
important	O
concept	O
is	O
the	O
treewidth	B
of	O
a	O
graph	O
(	O
bodlaender	O
,	O
1993	O
)	O
,	O
which	O
is	O
de-	O
ﬁned	O
in	O
terms	O
of	O
the	O
number	O
of	O
variables	O
in	O
the	O
largest	O
clique	B
.	O
in	O
fact	O
,	O
it	O
is	O
deﬁned	O
to	O
be	O
as	O
one	O
less	O
than	O
the	O
size	O
of	O
the	O
largest	O
clique	B
,	O
to	O
ensure	O
that	O
a	O
tree	B
has	O
a	O
treewidth	B
of	O
1.	O
because	O
there	O
in	O
general	O
there	O
can	O
be	O
multiple	O
different	O
junction	O
trees	O
that	O
can	O
be	O
constructed	O
from	O
a	O
given	O
starting	O
graph	O
,	O
the	O
treewidth	B
is	O
deﬁned	O
by	O
the	O
junction	O
tree	O
for	O
which	O
the	O
largest	O
clique	B
has	O
the	O
fewest	O
variables	O
.	O
if	O
the	O
treewidth	B
of	O
the	O
original	O
graph	O
is	O
high	O
,	O
the	O
junction	B
tree	I
algorithm	I
becomes	O
impractical	O
.	O
8.4.7	O
loopy	B
belief	I
propagation	I
for	O
many	O
problems	O
of	O
practical	O
interest	O
,	O
it	O
will	O
not	O
be	O
feasible	O
to	O
use	O
exact	O
in-	O
ference	O
,	O
and	O
so	O
we	O
need	O
to	O
exploit	O
effective	O
approximation	O
methods	O
.	O
an	O
important	O
class	O
of	O
such	O
approximations	O
,	O
that	O
can	O
broadly	O
be	O
called	O
variational	B
methods	O
,	O
will	O
be	O
discussed	O
in	O
detail	O
in	O
chapter	O
10.	O
complementing	O
these	O
deterministic	O
approaches	O
is	O
a	O
wide	O
range	O
of	O
sampling	B
methods	I
,	O
also	O
called	O
monte	O
carlo	O
methods	O
,	O
that	O
are	O
based	O
on	O
stochastic	B
numerical	O
sampling	O
from	O
distributions	O
and	O
that	O
will	O
be	O
discussed	O
at	O
length	O
in	O
chapter	O
11.	O
here	O
we	O
consider	O
one	O
simple	O
approach	O
to	O
approximate	O
inference	B
in	O
graphs	O
with	O
loops	O
,	O
which	O
builds	O
directly	O
on	O
the	O
previous	O
discussion	O
of	O
exact	O
inference	O
in	O
trees	O
.	O
the	O
idea	O
is	O
simply	O
to	O
apply	O
the	O
sum-product	B
algorithm	I
even	O
though	O
there	O
is	O
no	O
guar-	O
antee	O
that	O
it	O
will	O
yield	O
good	O
results	O
.	O
this	O
approach	O
is	O
known	O
as	O
loopy	O
belief	O
propa-	O
gation	O
(	O
frey	O
and	O
mackay	O
,	O
1998	O
)	O
and	O
is	O
possible	O
because	O
the	O
message	B
passing	I
rules	O
(	O
8.66	O
)	O
and	O
(	O
8.69	O
)	O
for	O
the	O
sum-product	B
algorithm	I
are	O
purely	O
local	B
.	O
however	O
,	O
because	O
the	O
graph	O
now	O
has	O
cycles	O
,	O
information	O
can	O
ﬂow	O
many	O
times	O
around	O
the	O
graph	O
.	O
for	O
some	O
models	O
,	O
the	O
algorithm	O
will	O
converge	O
,	O
whereas	O
for	O
others	O
it	O
will	O
not	O
.	O
in	O
order	O
to	O
apply	O
this	O
approach	O
,	O
we	O
need	O
to	O
deﬁne	O
a	O
message	B
passing	I
schedule	O
.	O
let	O
us	O
assume	O
that	O
one	O
message	O
is	O
passed	O
at	O
a	O
time	O
on	O
any	O
given	O
link	B
and	O
in	O
any	O
given	O
direction	O
.	O
each	O
message	O
sent	O
from	O
a	O
node	B
replaces	O
any	O
previous	O
message	O
sent	O
in	O
the	O
same	O
direction	O
across	O
the	O
same	O
link	B
and	O
will	O
itself	O
be	O
a	O
function	O
only	O
of	O
the	O
most	O
recent	O
messages	O
received	O
by	O
that	O
node	B
at	O
previous	O
steps	O
of	O
the	O
algorithm	O
.	O
we	O
have	O
seen	O
that	O
a	O
message	O
can	O
only	O
be	O
sent	O
across	O
a	O
link	B
from	O
a	O
node	B
when	O
all	O
other	O
messages	O
have	O
been	O
received	O
by	O
that	O
node	B
across	O
its	O
other	O
links	O
.	O
because	O
there	O
are	O
loops	O
in	O
the	O
graph	O
,	O
this	O
raises	O
the	O
problem	O
of	O
how	O
to	O
initiate	O
the	O
message	B
passing	I
algorithm	O
.	O
to	O
resolve	O
this	O
,	O
we	O
suppose	O
that	O
an	O
initial	O
message	O
given	O
by	O
the	O
unit	O
function	O
has	O
been	O
passed	O
across	O
every	O
link	B
in	O
each	O
direction	O
.	O
every	O
node	B
is	O
then	O
in	O
a	O
position	O
to	O
send	O
a	O
message	O
.	O
there	O
are	O
now	O
many	O
possible	O
ways	O
to	O
organize	O
the	O
message	B
passing	I
schedule	O
.	O
for	O
example	O
,	O
the	O
ﬂooding	B
schedule	I
simultaneously	O
passes	O
a	O
message	O
across	O
every	O
link	B
in	O
both	O
directions	O
at	O
each	O
time	O
step	O
,	O
whereas	O
schedules	O
that	O
pass	O
one	O
message	O
at	O
a	O
time	O
are	O
called	O
serial	O
schedules	O
.	O
following	O
kschischnang	O
et	O
al	O
.	O
(	O
2001	O
)	O
,	O
we	O
will	O
say	O
that	O
a	O
(	O
variable	O
or	O
factor	O
)	O
node	B
a	O
has	O
a	O
message	O
pending	O
on	O
its	O
link	B
to	O
a	O
node	B
b	O
if	O
node	B
a	O
has	O
received	O
any	O
message	O
on	O
any	O
of	O
its	O
other	O
links	O
since	O
the	O
last	O
time	O
it	O
send	O
a	O
message	O
to	O
b.	O
thus	O
,	O
when	O
a	O
node	B
receives	O
a	O
message	O
on	O
one	O
of	O
its	O
links	O
,	O
this	O
creates	O
pending	O
messages	O
on	O
all	O
of	O
its	O
other	O
links	O
.	O
only	O
pending	O
messages	O
need	O
to	O
be	O
transmitted	O
because	O
418	O
8.	O
graphical	O
models	O
exercise	O
8.29	O
other	O
messages	O
would	O
simply	O
duplicate	O
the	O
previous	O
message	O
on	O
the	O
same	O
link	B
.	O
for	O
graphs	O
that	O
have	O
a	O
tree	B
structure	O
,	O
any	O
schedule	B
that	O
sends	O
only	O
pending	O
messages	O
will	O
eventually	O
terminate	O
once	O
a	O
message	O
has	O
passed	O
in	O
each	O
direction	O
across	O
every	O
link	B
.	O
at	O
this	O
point	O
,	O
there	O
are	O
no	O
pending	O
messages	O
,	O
and	O
the	O
product	O
of	O
the	O
received	O
messages	O
at	O
every	O
variable	O
give	O
the	O
exact	O
marginal	O
.	O
in	O
graphs	O
having	O
loops	O
,	O
however	O
,	O
the	O
algorithm	O
may	O
never	O
terminate	O
because	O
there	O
might	O
always	O
be	O
pending	O
messages	O
,	O
although	O
in	O
practice	O
it	O
is	O
generally	O
found	O
to	O
converge	O
within	O
a	O
reasonable	O
time	O
for	O
most	O
applications	O
.	O
once	O
the	O
algorithm	O
has	O
converged	O
,	O
or	O
once	O
it	O
has	O
been	O
stopped	O
if	O
convergence	O
is	O
not	O
observed	O
,	O
the	O
(	O
approximate	O
)	O
local	B
marginals	O
can	O
be	O
computed	O
using	O
the	O
product	O
of	O
the	O
most	O
recently	O
received	O
incoming	O
messages	O
to	O
each	O
variable	O
node	B
or	O
factor	O
node	O
on	O
every	O
link	B
.	O
in	O
some	O
applications	O
,	O
the	O
loopy	B
belief	I
propagation	I
algorithm	O
can	O
give	O
poor	O
re-	O
sults	O
,	O
whereas	O
in	O
other	O
applications	O
it	O
has	O
proven	O
to	O
be	O
very	O
effective	O
.	O
in	O
particular	O
,	O
state-of-the-art	O
algorithms	O
for	O
decoding	O
certain	O
kinds	O
of	O
error-correcting	O
codes	O
are	O
equivalent	O
to	O
loopy	B
belief	I
propagation	I
(	O
gallager	O
,	O
1963	O
;	O
berrou	O
et	O
al.	O
,	O
1993	O
;	O
mceliece	O
et	O
al.	O
,	O
1998	O
;	O
mackay	O
and	O
neal	O
,	O
1999	O
;	O
frey	O
,	O
1998	O
)	O
.	O
8.4.8	O
learning	B
the	O
graph	O
structure	O
in	O
our	O
discussion	O
of	O
inference	B
in	O
graphical	O
models	O
,	O
we	O
have	O
assumed	O
that	O
the	O
structure	O
of	O
the	O
graph	O
is	O
known	O
and	O
ﬁxed	O
.	O
however	O
,	O
there	O
is	O
also	O
interest	O
in	O
go-	O
ing	O
beyond	O
the	O
inference	B
problem	O
and	O
learning	B
the	O
graph	O
structure	O
itself	O
from	O
data	O
(	O
friedman	O
and	O
koller	O
,	O
2003	O
)	O
.	O
this	O
requires	O
that	O
we	O
deﬁne	O
a	O
space	O
of	O
possible	O
struc-	O
tures	O
as	O
well	O
as	O
a	O
measure	O
that	O
can	O
be	O
used	O
to	O
score	O
each	O
structure	O
.	O
from	O
a	O
bayesian	O
viewpoint	O
,	O
we	O
would	O
ideally	O
like	O
to	O
compute	O
a	O
posterior	O
dis-	O
tribution	O
over	O
graph	O
structures	O
and	O
to	O
make	O
predictions	O
by	O
averaging	O
with	O
respect	O
to	O
this	O
distribution	O
.	O
if	O
we	O
have	O
a	O
prior	B
p	O
(	O
m	O
)	O
over	O
graphs	O
indexed	O
by	O
m	O
,	O
then	O
the	O
posterior	O
distribution	O
is	O
given	O
by	O
p	O
(	O
m|d	O
)	O
∝	O
p	O
(	O
m	O
)	O
p	O
(	O
d|m	O
)	O
(	O
8.103	O
)	O
where	O
d	O
is	O
the	O
observed	O
data	O
set	O
.	O
the	O
model	B
evidence	I
p	O
(	O
d|m	O
)	O
then	O
provides	O
the	O
score	O
for	O
each	O
model	O
.	O
however	O
,	O
evaluation	O
of	O
the	O
evidence	O
involves	O
marginalization	O
over	O
the	O
latent	O
variables	O
and	O
presents	O
a	O
challenging	O
computational	O
problem	O
for	O
many	O
models	O
.	O
exploring	O
the	O
space	O
of	O
structures	O
can	O
also	O
be	O
problematic	O
.	O
because	O
the	O
number	O
of	O
different	O
graph	O
structures	O
grows	O
exponentially	O
with	O
the	O
number	O
of	O
nodes	O
,	O
it	O
is	O
often	O
necessary	O
to	O
resort	O
to	O
heuristics	O
to	O
ﬁnd	O
good	O
candidates	O
.	O
exercises	O
8.1	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
by	O
marginalizing	O
out	O
the	O
variables	O
in	O
order	O
,	O
show	O
that	O
the	O
representation	O
(	O
8.5	O
)	O
for	O
the	O
joint	O
distribution	O
of	O
a	O
directed	B
graph	O
is	O
correctly	O
normalized	O
,	O
provided	O
each	O
of	O
the	O
conditional	B
distributions	O
is	O
normalized	O
.	O
8.2	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
the	O
property	O
of	O
there	O
being	O
no	O
directed	B
cycles	O
in	O
a	O
directed	B
graph	O
follows	O
from	O
the	O
statement	O
that	O
there	O
exists	O
an	O
ordered	O
numbering	O
of	O
the	O
nodes	O
such	O
that	O
for	O
each	O
node	B
there	O
are	O
no	O
links	O
going	O
to	O
a	O
lower-numbered	O
node	B
.	O
table	O
8.2	O
the	O
joint	O
distribution	O
over	O
three	O
binary	O
variables	O
.	O
exercises	O
419	O
a	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
1	O
b	O
0	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
c	O
0	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
0.192	O
0.144	O
0.048	O
0.216	O
0.192	O
0.064	O
0.048	O
0.096	O
8.3	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
three	O
binary	O
variables	O
a	O
,	O
b	O
,	O
c	O
∈	O
{	O
0	O
,	O
1	O
}	O
having	O
the	O
joint	O
distribution	O
given	O
in	O
table	O
8.2.	O
show	O
by	O
direct	O
evaluation	O
that	O
this	O
distribution	O
has	O
the	O
property	O
that	O
a	O
and	O
b	O
are	O
marginally	O
dependent	O
,	O
so	O
that	O
p	O
(	O
a	O
,	O
b	O
)	O
(	O
cid:9	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
,	O
but	O
that	O
they	O
become	O
independent	B
when	O
conditioned	O
on	O
c	O
,	O
so	O
that	O
p	O
(	O
a	O
,	O
b|c	O
)	O
=	O
p	O
(	O
a|c	O
)	O
p	O
(	O
b|c	O
)	O
for	O
both	O
c	O
=	O
0	O
and	O
c	O
=	O
1	O
.	O
8.4	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
evaluate	O
the	O
distributions	O
p	O
(	O
a	O
)	O
,	O
p	O
(	O
b|c	O
)	O
,	O
and	O
p	O
(	O
c|a	O
)	O
corresponding	O
to	O
the	O
joint	O
distribution	O
given	O
in	O
table	O
8.2.	O
hence	O
show	O
by	O
direct	O
evaluation	O
that	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
c|a	O
)	O
p	O
(	O
b|c	O
)	O
.	O
draw	O
the	O
corresponding	O
directed	B
graph	O
.	O
8.5	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
draw	O
a	O
directed	B
probabilistic	O
graphical	B
model	I
corresponding	O
to	O
the	O
relevance	B
vector	I
machine	I
described	O
by	O
(	O
7.79	O
)	O
and	O
(	O
7.80	O
)	O
.	O
8.6	O
(	O
(	O
cid:12	O
)	O
)	O
for	O
the	O
model	O
shown	O
in	O
figure	O
8.13	O
,	O
we	O
have	O
seen	O
that	O
the	O
number	O
of	O
parameters	O
required	O
to	O
specify	O
the	O
conditional	B
distribution	O
p	O
(	O
y|x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
,	O
where	O
xi	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
could	O
be	O
reduced	O
from	O
2m	O
to	O
m	O
+	O
1	O
by	O
making	O
use	O
of	O
the	O
logistic	B
sigmoid	I
represen-	O
tation	O
(	O
8.10	O
)	O
.	O
an	O
alternative	O
representation	O
(	O
pearl	O
,	O
1988	O
)	O
is	O
given	O
by	O
m	O
(	O
cid:14	O
)	O
p	O
(	O
y	O
=	O
1|x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
=	O
1	O
−	O
(	O
1	O
−	O
µ0	O
)	O
(	O
1	O
−	O
µi	O
)	O
xi	O
(	O
8.104	O
)	O
i=1	O
where	O
the	O
parameters	O
µi	O
represent	O
the	O
probabilities	O
p	O
(	O
xi	O
=	O
1	O
)	O
,	O
and	O
µ0	O
is	O
an	O
additional	O
parameters	O
satisfying	O
0	O
(	O
cid:1	O
)	O
µ0	O
(	O
cid:1	O
)	O
1.	O
the	O
conditional	B
distribution	O
(	O
8.104	O
)	O
is	O
known	O
as	O
the	O
noisy-or	O
.	O
show	O
that	O
this	O
can	O
be	O
interpreted	O
as	O
a	O
‘	O
soft	B
’	O
(	O
probabilistic	O
)	O
form	O
of	O
the	O
logical	O
or	O
function	O
(	O
i.e.	O
,	O
the	O
function	O
that	O
gives	O
y	O
=	O
1	O
whenever	O
at	O
least	O
one	O
of	O
the	O
xi	O
=	O
1	O
)	O
.	O
discuss	O
the	O
interpretation	O
of	O
µ0	O
.	O
8.7	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
using	O
the	O
recursion	O
relations	O
(	O
8.15	O
)	O
and	O
(	O
8.16	O
)	O
,	O
show	O
that	O
the	O
mean	B
and	O
covari-	O
ance	O
of	O
the	O
joint	O
distribution	O
for	O
the	O
graph	O
shown	O
in	O
figure	O
8.14	O
are	O
given	O
by	O
(	O
8.17	O
)	O
and	O
(	O
8.18	O
)	O
,	O
respectively	O
.	O
8.8	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
a	O
⊥⊥	O
b	O
,	O
c	O
|	O
d	O
implies	O
a	O
⊥⊥	O
b	O
|	O
d.	O
8.9	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
using	O
the	O
d-separation	B
criterion	O
,	O
show	O
that	O
the	O
conditional	B
distribution	O
for	O
a	O
node	B
x	O
in	O
a	O
directed	B
graph	O
,	O
conditioned	O
on	O
all	O
of	O
the	O
nodes	O
in	O
the	O
markov	O
blanket	O
,	O
is	O
independent	B
of	O
the	O
remaining	O
variables	O
in	O
the	O
graph	O
.	O
420	O
8.	O
graphical	O
models	O
figure	O
8.54	O
example	O
of	O
a	O
graphical	B
model	I
used	O
to	O
explore	O
the	O
con-	O
ditional	O
independence	O
properties	O
of	O
the	O
head-to-head	B
path	I
a–c–b	O
when	O
a	O
descendant	O
of	O
c	O
,	O
namely	O
the	O
node	B
d	O
,	O
is	O
observed	O
.	O
a	O
b	O
c	O
d	O
8.10	O
(	O
(	O
cid:12	O
)	O
)	O
consider	O
the	O
directed	B
graph	O
shown	O
in	O
figure	O
8.54	O
in	O
which	O
none	O
of	O
the	O
variables	O
is	O
observed	O
.	O
show	O
that	O
a	O
⊥⊥	O
b	O
|	O
∅	O
.	O
suppose	O
we	O
now	O
observe	O
the	O
variable	O
d.	O
show	O
that	O
in	O
general	O
a	O
(	O
cid:9	O
)	O
⊥⊥	O
b	O
|	O
d.	O
8.11	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
the	O
example	O
of	O
the	O
car	O
fuel	B
system	I
shown	O
in	O
figure	O
8.21	O
,	O
and	O
suppose	O
that	O
instead	O
of	O
observing	O
the	O
state	O
of	O
the	O
fuel	O
gauge	O
g	O
directly	O
,	O
the	O
gauge	O
is	O
seen	O
by	O
the	O
driver	O
d	O
who	O
reports	O
to	O
us	O
the	O
reading	O
on	O
the	O
gauge	O
.	O
this	O
report	O
is	O
either	O
that	O
the	O
gauge	O
shows	O
full	O
d	O
=	O
1	O
or	O
that	O
it	O
shows	O
empty	O
d	O
=	O
0.	O
our	O
driver	O
is	O
a	O
bit	O
unreliable	O
,	O
as	O
expressed	O
through	O
the	O
following	O
probabilities	O
p	O
(	O
d	O
=	O
1|g	O
=	O
1	O
)	O
=	O
0.9	O
p	O
(	O
d	O
=	O
0|g	O
=	O
0	O
)	O
=	O
0.9	O
.	O
(	O
8.105	O
)	O
(	O
8.106	O
)	O
suppose	O
that	O
the	O
driver	O
tells	O
us	O
that	O
the	O
fuel	O
gauge	O
shows	O
empty	O
,	O
in	O
other	O
words	O
that	O
we	O
observe	O
d	O
=	O
0.	O
evaluate	O
the	O
probability	B
that	O
the	O
tank	O
is	O
empty	O
given	O
only	O
this	O
observation	O
.	O
similarly	O
,	O
evaluate	O
the	O
corresponding	O
probability	B
given	O
also	O
the	O
observation	O
that	O
the	O
battery	O
is	O
ﬂat	O
,	O
and	O
note	O
that	O
this	O
second	O
probability	O
is	O
lower	O
.	O
discuss	O
the	O
intuition	O
behind	O
this	O
result	O
,	O
and	O
relate	O
the	O
result	O
to	O
figure	O
8.54	O
.	O
8.12	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
there	O
are	O
2m	O
(	O
m−1	O
)	O
/2	O
distinct	O
undirected	B
graphs	O
over	O
a	O
set	O
of	O
m	O
distinct	O
random	O
variables	O
.	O
draw	O
the	O
8	O
possibilities	O
for	O
the	O
case	O
of	O
m	O
=	O
3	O
.	O
8.13	O
(	O
(	O
cid:12	O
)	O
)	O
consider	O
the	O
use	O
of	O
iterated	B
conditional	I
modes	I
(	O
icm	O
)	O
to	O
minimize	O
the	O
energy	B
function	I
given	O
by	O
(	O
8.42	O
)	O
.	O
write	O
down	O
an	O
expression	O
for	O
the	O
difference	O
in	O
the	O
values	O
of	O
the	O
energy	O
associated	O
with	O
the	O
two	O
states	O
of	O
a	O
particular	O
variable	O
xj	O
,	O
with	O
all	O
other	O
variables	O
held	O
ﬁxed	O
,	O
and	O
show	O
that	O
it	O
depends	O
only	O
on	O
quantities	O
that	O
are	O
local	B
to	O
xj	O
in	O
the	O
graph	O
.	O
8.14	O
(	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
particular	O
case	O
of	O
the	O
energy	B
function	I
given	O
by	O
(	O
8.42	O
)	O
in	O
which	O
the	O
coefﬁcients	O
β	O
=	O
h	O
=	O
0.	O
show	O
that	O
the	O
most	O
probable	O
conﬁguration	O
of	O
the	O
latent	O
variables	O
is	O
given	O
by	O
xi	O
=	O
yi	O
for	O
all	O
i	O
.	O
8.15	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
the	O
joint	O
distribution	O
p	O
(	O
xn−1	O
,	O
xn	O
)	O
for	O
two	O
neighbouring	O
nodes	O
in	O
the	O
graph	O
shown	O
in	O
figure	O
8.38	O
is	O
given	O
by	O
an	O
expression	O
of	O
the	O
form	O
(	O
8.58	O
)	O
.	O
exercises	O
421	O
8.16	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
the	O
inference	B
problem	O
of	O
evaluating	O
p	O
(	O
xn|xn	O
)	O
for	O
the	O
graph	O
shown	O
in	O
figure	O
8.38	O
,	O
for	O
all	O
nodes	O
n	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
−	O
1	O
}	O
.	O
show	O
that	O
the	O
message	B
passing	I
algorithm	O
discussed	O
in	O
section	O
8.4.1	O
can	O
be	O
used	O
to	O
solve	O
this	O
efﬁciently	O
,	O
and	O
discuss	O
which	O
messages	O
are	O
modiﬁed	O
and	O
in	O
what	O
way	O
.	O
8.17	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
graph	O
of	O
the	O
form	O
shown	O
in	O
figure	O
8.38	O
having	O
n	O
=	O
5	O
nodes	O
,	O
in	O
which	O
nodes	O
x3	O
and	O
x5	O
are	O
observed	O
.	O
use	O
d-separation	B
to	O
show	O
that	O
x2	O
⊥⊥	O
x5	O
|	O
x3	O
.	O
show	O
that	O
if	O
the	O
message	B
passing	I
algorithm	O
of	O
section	O
8.4.1	O
is	O
applied	O
to	O
the	O
evalu-	O
ation	O
of	O
p	O
(	O
x2|x3	O
,	O
x5	O
)	O
,	O
the	O
result	O
will	O
be	O
independent	B
of	O
the	O
value	O
of	O
x5	O
.	O
8.18	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
a	O
distribution	O
represented	O
by	O
a	O
directed	B
tree	O
can	O
trivially	O
be	O
written	O
as	O
an	O
equivalent	O
distribution	O
over	O
the	O
corresponding	O
undirected	B
tree	O
.	O
also	O
show	O
that	O
a	O
distribution	O
expressed	O
as	O
an	O
undirected	B
tree	O
can	O
,	O
by	O
suitable	O
normaliza-	O
tion	O
of	O
the	O
clique	B
potentials	O
,	O
be	O
written	O
as	O
a	O
directed	B
tree	O
.	O
calculate	O
the	O
number	O
of	O
distinct	O
directed	B
trees	O
that	O
can	O
be	O
constructed	O
from	O
a	O
given	O
undirected	B
tree	O
.	O
8.19	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
apply	O
the	O
sum-product	B
algorithm	I
derived	O
in	O
section	O
8.4.4	O
to	O
the	O
chain-of-	O
nodes	O
model	O
discussed	O
in	O
section	O
8.4.1	O
and	O
show	O
that	O
the	O
results	O
(	O
8.54	O
)	O
,	O
(	O
8.55	O
)	O
,	O
and	O
(	O
8.57	O
)	O
are	O
recovered	O
as	O
a	O
special	O
case	O
.	O
8.20	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
the	O
message	B
passing	I
protocol	O
for	O
the	O
sum-product	B
algorithm	I
on	O
a	O
tree-structured	O
factor	B
graph	I
in	O
which	O
messages	O
are	O
ﬁrst	O
propagated	O
from	O
the	O
leaves	O
to	O
an	O
arbitrarily	O
chosen	O
root	B
node	I
and	O
then	O
from	O
the	O
root	B
node	I
out	O
to	O
the	O
leaves	O
.	O
use	O
proof	O
by	O
induction	O
to	O
show	O
that	O
the	O
messages	O
can	O
be	O
passed	O
in	O
such	O
an	O
order	O
that	O
at	O
every	O
step	O
,	O
each	O
node	B
that	O
must	O
send	O
a	O
message	O
has	O
received	O
all	O
of	O
the	O
incoming	O
messages	O
necessary	O
to	O
construct	O
its	O
outgoing	O
messages	O
.	O
8.21	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
the	O
marginal	B
distributions	O
p	O
(	O
xs	O
)	O
over	O
the	O
sets	O
of	O
variables	O
xs	O
associated	O
with	O
each	O
of	O
the	O
factors	O
fx	O
(	O
xs	O
)	O
in	O
a	O
factor	B
graph	I
can	O
be	O
found	O
by	O
ﬁrst	O
running	O
the	O
sum-product	O
message	O
passing	O
algorithm	O
and	O
then	O
evaluating	O
the	O
required	O
marginals	O
using	O
(	O
8.72	O
)	O
.	O
8.22	O
(	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
tree-structured	O
factor	B
graph	I
,	O
in	O
which	O
a	O
given	O
subset	O
of	O
the	O
variable	O
nodes	O
form	O
a	O
connected	O
subgraph	O
(	O
i.e.	O
,	O
any	O
variable	O
node	B
of	O
the	O
subset	O
is	O
connected	O
to	O
at	O
least	O
one	O
of	O
the	O
other	O
variable	O
nodes	O
via	O
a	O
single	O
factor	O
node	O
)	O
.	O
show	O
how	O
the	O
sum-product	B
algorithm	I
can	O
be	O
used	O
to	O
compute	O
the	O
marginal	B
distribution	O
over	O
that	O
subset	O
.	O
8.23	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
in	O
section	O
8.4.4	O
,	O
we	O
showed	O
that	O
the	O
marginal	B
distribution	O
p	O
(	O
xi	O
)	O
for	O
a	O
variable	O
node	B
xi	O
in	O
a	O
factor	B
graph	I
is	O
given	O
by	O
the	O
product	O
of	O
the	O
messages	O
arriving	O
at	O
this	O
node	B
from	O
neighbouring	O
factor	O
nodes	O
in	O
the	O
form	O
(	O
8.63	O
)	O
.	O
show	O
that	O
the	O
marginal	B
p	O
(	O
xi	O
)	O
can	O
also	O
be	O
written	O
as	O
the	O
product	O
of	O
the	O
incoming	O
message	O
along	O
any	O
one	O
of	O
the	O
links	O
with	O
the	O
outgoing	O
message	O
along	O
the	O
same	O
link	B
.	O
8.24	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
the	O
marginal	B
distribution	O
for	O
the	O
variables	O
xs	O
in	O
a	O
factor	O
fs	O
(	O
xs	O
)	O
in	O
a	O
tree-structured	O
factor	B
graph	I
,	O
after	O
running	O
the	O
sum-product	O
message	O
passing	O
algo-	O
rithm	O
,	O
can	O
be	O
written	O
as	O
the	O
product	O
of	O
the	O
message	O
arriving	O
at	O
the	O
factor	O
node	O
along	O
all	O
its	O
links	O
,	O
times	O
the	O
local	B
factor	O
f	O
(	O
xs	O
)	O
,	O
in	O
the	O
form	O
(	O
8.72	O
)	O
.	O
422	O
8.	O
graphical	O
models	O
8.25	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
in	O
(	O
8.86	O
)	O
,	O
we	O
veriﬁed	O
that	O
the	O
sum-product	B
algorithm	I
run	O
on	O
the	O
graph	O
in	O
figure	O
8.51	O
with	O
node	B
x3	O
designated	O
as	O
the	O
root	B
node	I
gives	O
the	O
correct	O
marginal	B
for	O
x2	O
.	O
show	O
that	O
the	O
correct	O
marginals	O
are	O
obtained	O
also	O
for	O
x1	O
and	O
x3	O
.	O
similarly	O
,	O
show	O
that	O
the	O
use	O
of	O
the	O
result	O
(	O
8.72	O
)	O
after	O
running	O
the	O
sum-product	B
algorithm	I
on	O
this	O
graph	O
gives	O
the	O
correct	O
joint	O
distribution	O
for	O
x1	O
,	O
x2	O
.	O
8.26	O
(	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
tree-structured	O
factor	B
graph	I
over	O
discrete	O
variables	O
,	O
and	O
suppose	O
we	O
wish	O
to	O
evaluate	O
the	O
joint	O
distribution	O
p	O
(	O
xa	O
,	O
xb	O
)	O
associated	O
with	O
two	O
variables	O
xa	O
and	O
xb	O
that	O
do	O
not	O
belong	O
to	O
a	O
common	O
factor	O
.	O
deﬁne	O
a	O
procedure	O
for	O
using	O
the	O
sum-	O
product	O
algorithm	O
to	O
evaluate	O
this	O
joint	O
distribution	O
in	O
which	O
one	O
of	O
the	O
variables	O
is	O
successively	O
clamped	O
to	O
each	O
of	O
its	O
allowed	O
values	O
.	O
8.27	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
two	O
discrete	O
variables	O
x	O
and	O
y	O
each	O
having	O
three	O
possible	O
states	O
,	O
for	O
example	O
x	O
,	O
y	O
∈	O
{	O
0	O
,	O
1	O
,	O
2	O
}	O
.	O
construct	O
a	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
y	O
)	O
over	O
these	O
variables	O
having	O
the	O
property	O
that	O
the	O
value	O
(	O
cid:1	O
)	O
x	O
that	O
maximizes	O
the	O
marginal	B
p	O
(	O
x	O
)	O
,	O
along	O
with	O
the	O
value	O
(	O
cid:1	O
)	O
y	O
that	O
maximizes	O
the	O
marginal	B
p	O
(	O
y	O
)	O
,	O
together	O
have	O
probability	B
zero	O
under	O
the	O
joint	O
distribution	O
,	O
so	O
that	O
p	O
(	O
(	O
cid:1	O
)	O
x	O
,	O
(	O
cid:1	O
)	O
y	O
)	O
=	O
0	O
.	O
8.28	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
the	O
concept	O
of	O
a	O
pending	B
message	I
in	O
the	O
sum-product	B
algorithm	I
for	O
a	O
factor	B
graph	I
was	O
deﬁned	O
in	O
section	O
8.4.7.	O
show	O
that	O
if	O
the	O
graph	O
has	O
one	O
or	O
more	O
cycles	O
,	O
there	O
will	O
always	O
be	O
at	O
least	O
one	O
pending	B
message	I
irrespective	O
of	O
how	O
long	O
the	O
algorithm	O
runs	O
.	O
8.29	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
if	O
the	O
sum-product	B
algorithm	I
is	O
run	O
on	O
a	O
factor	B
graph	I
with	O
a	O
tree	B
structure	O
(	O
no	O
loops	O
)	O
,	O
then	O
after	O
a	O
ﬁnite	O
number	O
of	O
messages	O
have	O
been	O
sent	O
,	O
there	O
will	O
be	O
no	O
pending	O
messages	O
.	O
9	O
mixture	B
models	O
and	O
em	O
if	O
we	O
deﬁne	O
a	O
joint	O
distribution	O
over	O
observed	O
and	O
latent	O
variables	O
,	O
the	O
correspond-	O
ing	O
distribution	O
of	O
the	O
observed	O
variables	O
alone	O
is	O
obtained	O
by	O
marginalization	O
.	O
this	O
allows	O
relatively	O
complex	O
marginal	B
distributions	O
over	O
observed	O
variables	O
to	O
be	O
ex-	O
pressed	O
in	O
terms	O
of	O
more	O
tractable	O
joint	O
distributions	O
over	O
the	O
expanded	O
space	O
of	O
observed	O
and	O
latent	O
variables	O
.	O
the	O
introduction	O
of	O
latent	O
variables	O
thereby	O
allows	O
complicated	O
distributions	O
to	O
be	O
formed	O
from	O
simpler	O
components	O
.	O
in	O
this	O
chapter	O
,	O
we	O
shall	O
see	O
that	O
mixture	B
distributions	O
,	O
such	O
as	O
the	O
gaussian	O
mixture	B
discussed	O
in	O
section	O
2.3.9	O
,	O
can	O
be	O
interpreted	O
in	O
terms	O
of	O
discrete	O
latent	O
variables	O
.	O
continuous	O
latent	O
variables	O
will	O
form	O
the	O
subject	O
of	O
chapter	O
12.	O
as	O
well	O
as	O
providing	O
a	O
framework	O
for	O
building	O
more	O
complex	O
probability	B
dis-	O
tributions	O
,	O
mixture	B
models	O
can	O
also	O
be	O
used	O
to	O
cluster	O
data	O
.	O
we	O
therefore	O
begin	O
our	O
discussion	O
of	O
mixture	B
distributions	O
by	O
considering	O
the	O
problem	O
of	O
ﬁnding	O
clusters	O
in	O
a	O
set	O
of	O
data	O
points	O
,	O
which	O
we	O
approach	O
ﬁrst	O
using	O
a	O
nonprobabilistic	O
technique	O
called	O
the	O
k-means	O
algorithm	O
(	O
lloyd	O
,	O
1982	O
)	O
.	O
then	O
we	O
introduce	O
the	O
latent	B
variable	I
423	O
section	O
9.1	O
424	O
9.	O
mixture	B
models	O
and	O
em	O
section	O
9.2	O
section	O
9.3	O
section	O
9.4	O
view	O
of	O
mixture	B
distributions	O
in	O
which	O
the	O
discrete	O
latent	O
variables	O
can	O
be	O
interpreted	O
as	O
deﬁning	O
assignments	O
of	O
data	O
points	O
to	O
speciﬁc	O
components	O
of	O
the	O
mixture	B
.	O
a	O
gen-	O
eral	O
technique	O
for	O
ﬁnding	O
maximum	B
likelihood	I
estimators	O
in	O
latent	B
variable	I
models	O
is	O
the	O
expectation-maximization	O
(	O
em	O
)	O
algorithm	O
.	O
we	O
ﬁrst	O
of	O
all	O
use	O
the	O
gaussian	O
mixture	B
distribution	I
to	O
motivate	O
the	O
em	O
algorithm	O
in	O
a	O
fairly	O
informal	O
way	O
,	O
and	O
then	O
we	O
give	O
a	O
more	O
careful	O
treatment	O
based	O
on	O
the	O
latent	B
variable	I
viewpoint	O
.	O
we	O
shall	O
see	O
that	O
the	O
k-means	O
algorithm	O
corresponds	O
to	O
a	O
particular	O
nonprobabilistic	O
limit	O
of	O
em	O
applied	O
to	O
mixtures	O
of	O
gaussians	O
.	O
finally	O
,	O
we	O
discuss	O
em	O
in	O
some	O
generality	O
.	O
gaussian	O
mixture	B
models	O
are	O
widely	O
used	O
in	O
data	O
mining	O
,	O
pattern	O
recognition	O
,	O
machine	O
learning	O
,	O
and	O
statistical	O
analysis	O
.	O
in	O
many	O
applications	O
,	O
their	O
parameters	O
are	O
determined	O
by	O
maximum	B
likelihood	I
,	O
typically	O
using	O
the	O
em	O
algorithm	O
.	O
however	O
,	O
as	O
we	O
shall	O
see	O
there	O
are	O
some	O
signiﬁcant	O
limitations	O
to	O
the	O
maximum	B
likelihood	I
ap-	O
proach	O
,	O
and	O
in	O
chapter	O
10	O
we	O
shall	O
show	O
that	O
an	O
elegant	O
bayesian	O
treatment	O
can	O
be	O
given	O
using	O
the	O
framework	O
of	O
variational	B
inference	I
.	O
this	O
requires	O
little	O
additional	O
computation	O
compared	O
with	O
em	O
,	O
and	O
it	O
resolves	O
the	O
principal	O
difﬁculties	O
of	O
maxi-	O
mum	O
likelihood	O
while	O
also	O
allowing	O
the	O
number	O
of	O
components	O
in	O
the	O
mixture	B
to	O
be	O
inferred	O
automatically	O
from	O
the	O
data	O
.	O
9.1.	O
k-means	O
clustering	B
we	O
begin	O
by	O
considering	O
the	O
problem	O
of	O
identifying	O
groups	O
,	O
or	O
clusters	O
,	O
of	O
data	O
points	O
in	O
a	O
multidimensional	O
space	O
.	O
suppose	O
we	O
have	O
a	O
data	O
set	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
consisting	O
of	O
n	O
observations	O
of	O
a	O
random	O
d-dimensional	O
euclidean	O
variable	O
x.	O
our	O
goal	O
is	O
to	O
partition	O
the	O
data	O
set	O
into	O
some	O
number	O
k	O
of	O
clusters	O
,	O
where	O
we	O
shall	O
suppose	O
for	O
the	O
moment	O
that	O
the	O
value	O
of	O
k	O
is	O
given	O
.	O
intuitively	O
,	O
we	O
might	O
think	O
of	O
a	O
cluster	O
as	O
comprising	O
a	O
group	O
of	O
data	O
points	O
whose	O
inter-point	O
distances	O
are	O
small	O
compared	O
with	O
the	O
distances	O
to	O
points	O
outside	O
of	O
the	O
cluster	O
.	O
we	O
can	O
formalize	O
this	O
notion	O
by	O
ﬁrst	O
introducing	O
a	O
set	O
of	O
d-dimensional	O
vectors	O
µk	O
,	O
where	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
,	O
in	O
which	O
µk	O
is	O
a	O
prototype	O
associated	O
with	O
the	O
kth	O
cluster	O
.	O
as	O
we	O
shall	O
see	O
shortly	O
,	O
we	O
can	O
think	O
of	O
the	O
µk	O
as	O
representing	O
the	O
centres	O
of	O
the	O
clusters	O
.	O
our	O
goal	O
is	O
then	O
to	O
ﬁnd	O
an	O
assignment	O
of	O
data	O
points	O
to	O
clusters	O
,	O
as	O
well	O
as	O
a	O
set	O
of	O
vectors	O
{	O
µk	O
}	O
,	O
such	O
that	O
the	O
sum	O
of	O
the	O
squares	O
of	O
the	O
distances	O
of	O
each	O
data	O
point	O
to	O
its	O
closest	O
vector	O
µk	O
,	O
is	O
a	O
minimum	O
.	O
it	O
is	O
convenient	O
at	O
this	O
point	O
to	O
deﬁne	O
some	O
notation	O
to	O
describe	O
the	O
assignment	O
of	O
data	O
points	O
to	O
clusters	O
.	O
for	O
each	O
data	O
point	O
xn	O
,	O
we	O
introduce	O
a	O
corresponding	O
set	O
of	O
binary	O
indicator	O
variables	O
rnk	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
where	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
describing	O
which	O
of	O
the	O
k	O
clusters	O
the	O
data	O
point	O
xn	O
is	O
assigned	O
to	O
,	O
so	O
that	O
if	O
data	O
point	O
xn	O
is	O
assigned	O
to	O
cluster	O
k	O
then	O
rnk	O
=	O
1	O
,	O
and	O
rnj	O
=	O
0	O
for	O
j	O
(	O
cid:9	O
)	O
=	O
k.	O
this	O
is	O
known	O
as	O
the	O
1-of-k	O
coding	O
scheme	O
.	O
we	O
can	O
then	O
deﬁne	O
an	O
objective	O
function	O
,	O
sometimes	O
called	O
a	O
distortion	B
measure	I
,	O
given	O
by	O
n	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
j	O
=	O
rnk	O
(	O
cid:5	O
)	O
xn	O
−	O
µk	O
(	O
cid:5	O
)	O
2	O
(	O
9.1	O
)	O
which	O
represents	O
the	O
sum	O
of	O
the	O
squares	O
of	O
the	O
distances	O
of	O
each	O
data	O
point	O
to	O
its	O
n=1	O
k=1	O
9.1.	O
k-means	O
clustering	B
425	O
assigned	O
vector	O
µk	O
.	O
our	O
goal	O
is	O
to	O
ﬁnd	O
values	O
for	O
the	O
{	O
rnk	O
}	O
and	O
the	O
{	O
µk	O
}	O
so	O
as	O
to	O
minimize	O
j.	O
we	O
can	O
do	O
this	O
through	O
an	O
iterative	O
procedure	O
in	O
which	O
each	O
iteration	O
involves	O
two	O
successive	O
steps	O
corresponding	O
to	O
successive	O
optimizations	O
with	O
respect	O
to	O
the	O
rnk	O
and	O
the	O
µk	O
.	O
first	O
we	O
choose	O
some	O
initial	O
values	O
for	O
the	O
µk	O
.	O
then	O
in	O
the	O
ﬁrst	O
phase	O
we	O
minimize	O
j	O
with	O
respect	O
to	O
the	O
rnk	O
,	O
keeping	O
the	O
µk	O
ﬁxed	O
.	O
in	O
the	O
second	O
phase	O
we	O
minimize	O
j	O
with	O
respect	O
to	O
the	O
µk	O
,	O
keeping	O
rnk	O
ﬁxed	O
.	O
this	O
two-stage	O
optimization	O
is	O
then	O
repeated	O
until	O
convergence	O
.	O
we	O
shall	O
see	O
that	O
these	O
two	O
stages	O
of	O
updating	O
rnk	O
and	O
updating	O
µk	O
correspond	O
respectively	O
to	O
the	O
e	O
(	O
expectation	B
)	O
and	O
m	O
(	O
maximization	O
)	O
steps	O
of	O
the	O
em	O
algorithm	O
,	O
and	O
to	O
emphasize	O
this	O
we	O
shall	O
use	O
the	O
terms	O
e	O
step	O
and	O
m	O
step	O
in	O
the	O
context	O
of	O
the	O
k-means	O
algorithm	O
.	O
section	O
9.4	O
consider	O
ﬁrst	O
the	O
determination	O
of	O
the	O
rnk	O
.	O
because	O
j	O
in	O
(	O
9.1	O
)	O
is	O
a	O
linear	O
func-	O
tion	O
of	O
rnk	O
,	O
this	O
optimization	O
can	O
be	O
performed	O
easily	O
to	O
give	O
a	O
closed	O
form	O
solution	O
.	O
the	O
terms	O
involving	O
different	O
n	O
are	O
independent	B
and	O
so	O
we	O
can	O
optimize	O
for	O
each	O
n	O
separately	O
by	O
choosing	O
rnk	O
to	O
be	O
1	O
for	O
whichever	O
value	O
of	O
k	O
gives	O
the	O
minimum	O
value	O
of	O
(	O
cid:5	O
)	O
xn	O
−	O
µk	O
(	O
cid:5	O
)	O
2.	O
in	O
other	O
words	O
,	O
we	O
simply	O
assign	O
the	O
nth	O
data	O
point	O
to	O
the	O
closest	O
cluster	O
centre	O
.	O
more	O
formally	O
,	O
this	O
can	O
be	O
expressed	O
as	O
if	O
k	O
=	O
arg	O
minj	O
(	O
cid:5	O
)	O
xn	O
−	O
µj	O
(	O
cid:5	O
)	O
2	O
otherwise	O
.	O
rnk	O
=	O
(	O
cid:12	O
)	O
(	O
9.2	O
)	O
1	O
0	O
now	O
consider	O
the	O
optimization	O
of	O
the	O
µk	O
with	O
the	O
rnk	O
held	O
ﬁxed	O
.	O
the	O
objective	O
function	O
j	O
is	O
a	O
quadratic	O
function	O
of	O
µk	O
,	O
and	O
it	O
can	O
be	O
minimized	O
by	O
setting	O
its	O
derivative	B
with	O
respect	O
to	O
µk	O
to	O
zero	O
giving	O
n	O
(	O
cid:2	O
)	O
n=1	O
2	O
µk	O
=	O
rnk	O
(	O
xn	O
−	O
µk	O
)	O
=	O
0	O
(	O
cid:5	O
)	O
n	O
rnkxn	O
(	O
cid:5	O
)	O
.	O
n	O
rnk	O
which	O
we	O
can	O
easily	O
solve	O
for	O
µk	O
to	O
give	O
(	O
9.3	O
)	O
(	O
9.4	O
)	O
the	O
denominator	O
in	O
this	O
expression	O
is	O
equal	O
to	O
the	O
number	O
of	O
points	O
assigned	O
to	O
cluster	O
k	O
,	O
and	O
so	O
this	O
result	O
has	O
a	O
simple	O
interpretation	O
,	O
namely	O
set	O
µk	O
equal	O
to	O
the	O
mean	B
of	O
all	O
of	O
the	O
data	O
points	O
xn	O
assigned	O
to	O
cluster	O
k.	O
for	O
this	O
reason	O
,	O
the	O
procedure	O
is	O
known	O
as	O
the	O
k-means	O
algorithm	O
.	O
the	O
two	O
phases	O
of	O
re-assigning	O
data	O
points	O
to	O
clusters	O
and	O
re-computing	O
the	O
clus-	O
ter	O
means	O
are	O
repeated	O
in	O
turn	O
until	O
there	O
is	O
no	O
further	O
change	O
in	O
the	O
assignments	O
(	O
or	O
until	O
some	O
maximum	O
number	O
of	O
iterations	O
is	O
exceeded	O
)	O
.	O
because	O
each	O
phase	O
reduces	O
the	O
value	O
of	O
the	O
objective	O
function	O
j	O
,	O
convergence	O
of	O
the	O
algorithm	O
is	O
assured	O
.	O
how-	O
ever	O
,	O
it	O
may	O
converge	O
to	O
a	O
local	B
rather	O
than	O
global	B
minimum	I
of	O
j.	O
the	O
convergence	O
properties	O
of	O
the	O
k-means	O
algorithm	O
were	O
studied	O
by	O
macqueen	O
(	O
1967	O
)	O
.	O
the	O
k-means	O
algorithm	O
is	O
illustrated	O
using	O
the	O
old	O
faithful	O
data	O
set	O
in	O
fig-	O
ure	O
9.1.	O
for	O
the	O
purposes	O
of	O
this	O
example	O
,	O
we	O
have	O
made	O
a	O
linear	O
re-scaling	O
of	O
the	O
data	O
,	O
known	O
as	O
standardizing	B
,	O
such	O
that	O
each	O
of	O
the	O
variables	O
has	O
zero	O
mean	B
and	O
unit	O
standard	B
deviation	I
.	O
for	O
this	O
example	O
,	O
we	O
have	O
chosen	O
k	O
=	O
2	O
,	O
and	O
so	O
in	O
this	O
exercise	O
9.1	O
appendix	O
a	O
426	O
9.	O
mixture	B
models	O
and	O
em	O
(	O
a	O
)	O
−2	O
(	O
d	O
)	O
−2	O
(	O
g	O
)	O
2	O
0	O
−2	O
2	O
0	O
−2	O
2	O
0	O
−2	O
0	O
2	O
0	O
2	O
(	O
b	O
)	O
−2	O
(	O
e	O
)	O
−2	O
(	O
h	O
)	O
2	O
0	O
−2	O
2	O
0	O
−2	O
2	O
0	O
−2	O
0	O
2	O
0	O
2	O
(	O
c	O
)	O
−2	O
(	O
f	O
)	O
−2	O
(	O
i	O
)	O
2	O
0	O
−2	O
2	O
0	O
−2	O
2	O
0	O
−2	O
0	O
2	O
0	O
2	O
−2	O
0	O
2	O
−2	O
0	O
2	O
−2	O
0	O
2	O
figure	O
9.1	O
illustration	O
of	O
the	O
k-means	O
algorithm	O
using	O
the	O
re-scaled	O
old	O
faithful	O
data	O
set	O
.	O
(	O
a	O
)	O
green	O
points	O
denote	O
the	O
data	O
set	O
in	O
a	O
two-dimensional	O
euclidean	O
space	O
.	O
the	O
initial	O
choices	O
for	O
centres	O
µ1	O
and	O
µ2	O
are	O
shown	O
by	O
the	O
red	O
and	O
blue	O
crosses	O
,	O
respectively	O
.	O
(	O
b	O
)	O
in	O
the	O
initial	O
e	O
step	O
,	O
each	O
data	O
point	O
is	O
assigned	O
either	O
to	O
the	O
red	O
cluster	O
or	O
to	O
the	O
blue	O
cluster	O
,	O
according	O
to	O
which	O
cluster	O
centre	O
is	O
nearer	O
.	O
this	O
is	O
equivalent	O
to	O
classifying	O
the	O
points	O
according	O
to	O
which	O
side	O
of	O
the	O
perpendicular	O
bisector	O
of	O
the	O
two	O
cluster	O
centres	O
,	O
shown	O
by	O
the	O
magenta	O
line	O
,	O
they	O
lie	O
on	O
.	O
(	O
c	O
)	O
in	O
the	O
subsequent	O
m	O
step	O
,	O
each	O
cluster	O
centre	O
is	O
re-computed	O
to	O
be	O
the	O
mean	B
of	O
the	O
points	O
assigned	O
to	O
the	O
corresponding	O
cluster	O
.	O
(	O
d	O
)	O
–	O
(	O
i	O
)	O
show	O
successive	O
e	O
and	O
m	O
steps	O
through	O
to	O
ﬁnal	O
convergence	O
of	O
the	O
algorithm	O
.	O
9.1.	O
k-means	O
clustering	B
427	O
figure	O
9.2	O
plot	O
of	O
the	O
cost	B
function	I
j	O
given	O
by	O
(	O
9.1	O
)	O
after	O
each	O
e	O
step	O
(	O
blue	O
points	O
)	O
and	O
m	O
step	O
(	O
red	O
points	O
)	O
of	O
the	O
k-	O
means	O
algorithm	O
for	O
the	O
example	O
shown	O
in	O
figure	O
9.1.	O
the	O
algo-	O
rithm	O
has	O
converged	O
after	O
the	O
third	O
m	O
step	O
,	O
and	O
the	O
ﬁnal	O
em	O
cycle	O
pro-	O
duces	O
no	O
changes	O
in	O
either	O
the	O
as-	O
signments	O
or	O
the	O
prototype	O
vectors	O
.	O
1000	O
j	O
500	O
0	O
1	O
2	O
3	O
4	O
section	O
9.2.2	O
section	O
2.3.5	O
exercise	O
9.2	O
case	O
,	O
the	O
assignment	O
of	O
each	O
data	O
point	O
to	O
the	O
nearest	O
cluster	O
centre	O
is	O
equivalent	O
to	O
a	O
classiﬁcation	B
of	O
the	O
data	O
points	O
according	O
to	O
which	O
side	O
they	O
lie	O
of	O
the	O
perpendicular	O
bisector	O
of	O
the	O
two	O
cluster	O
centres	O
.	O
a	O
plot	O
of	O
the	O
cost	B
function	I
j	O
given	O
by	O
(	O
9.1	O
)	O
for	O
the	O
old	O
faithful	O
example	O
is	O
shown	O
in	O
figure	O
9.2.	O
note	O
that	O
we	O
have	O
deliberately	O
chosen	O
poor	O
initial	O
values	O
for	O
the	O
cluster	O
centres	O
so	O
that	O
the	O
algorithm	O
takes	O
several	O
steps	O
before	O
convergence	O
.	O
in	O
practice	O
,	O
a	O
better	O
initialization	O
procedure	O
would	O
be	O
to	O
choose	O
the	O
cluster	O
centres	O
µk	O
to	O
be	O
equal	O
to	O
a	O
random	O
subset	O
of	O
k	O
data	O
points	O
.	O
it	O
is	O
also	O
worth	O
noting	O
that	O
the	O
k-means	O
algorithm	O
itself	O
is	O
often	O
used	O
to	O
initialize	O
the	O
parameters	O
in	O
a	O
gaussian	O
mixture	B
model	I
before	O
applying	O
the	O
em	O
algorithm	O
.	O
a	O
direct	O
implementation	O
of	O
the	O
k-means	O
algorithm	O
as	O
discussed	O
here	O
can	O
be	O
relatively	O
slow	O
,	O
because	O
in	O
each	O
e	O
step	O
it	O
is	O
necessary	O
to	O
compute	O
the	O
euclidean	O
dis-	O
tance	O
between	O
every	O
prototype	O
vector	O
and	O
every	O
data	O
point	O
.	O
various	O
schemes	O
have	O
been	O
proposed	O
for	O
speeding	O
up	O
the	O
k-means	O
algorithm	O
,	O
some	O
of	O
which	O
are	O
based	O
on	O
precomputing	O
a	O
data	O
structure	O
such	O
as	O
a	O
tree	B
such	O
that	O
nearby	O
points	O
are	O
in	O
the	O
same	O
subtree	O
(	O
ramasubramanian	O
and	O
paliwal	O
,	O
1990	O
;	O
moore	O
,	O
2000	O
)	O
.	O
other	O
approaches	O
make	O
use	O
of	O
the	O
triangle	O
inequality	O
for	O
distances	O
,	O
thereby	O
avoiding	O
unnecessary	O
dis-	O
tance	O
calculations	O
(	O
hodgson	O
,	O
1998	O
;	O
elkan	O
,	O
2003	O
)	O
.	O
so	O
far	O
,	O
we	O
have	O
considered	O
a	O
batch	O
version	O
of	O
k-means	O
in	O
which	O
the	O
whole	O
data	O
set	O
is	O
used	O
together	O
to	O
update	O
the	O
prototype	O
vectors	O
.	O
we	O
can	O
also	O
derive	O
an	O
on-line	O
stochastic	O
algorithm	O
(	O
macqueen	O
,	O
1967	O
)	O
by	O
applying	O
the	O
robbins-monro	O
procedure	O
to	O
the	O
problem	O
of	O
ﬁnding	O
the	O
roots	O
of	O
the	O
regression	B
function	I
given	O
by	O
the	O
derivatives	O
of	O
j	O
in	O
(	O
9.1	O
)	O
with	O
respect	O
to	O
µk	O
.	O
this	O
leads	O
to	O
a	O
sequential	O
update	O
in	O
which	O
,	O
for	O
each	O
data	O
point	O
xn	O
in	O
turn	O
,	O
we	O
update	O
the	O
nearest	O
prototype	O
µk	O
using	O
(	O
9.5	O
)	O
µnew	O
k	O
=	O
µold	O
k	O
+	O
ηn	O
(	O
xn	O
−	O
µold	O
k	O
)	O
where	O
ηn	O
is	O
the	O
learning	B
rate	I
parameter	I
,	O
which	O
is	O
typically	O
made	O
to	O
decrease	O
mono-	O
tonically	O
as	O
more	O
data	O
points	O
are	O
considered	O
.	O
the	O
k-means	O
algorithm	O
is	O
based	O
on	O
the	O
use	O
of	O
squared	O
euclidean	O
distance	O
as	O
the	O
measure	O
of	O
dissimilarity	O
between	O
a	O
data	O
point	O
and	O
a	O
prototype	O
vector	O
.	O
not	O
only	O
does	O
this	O
limit	O
the	O
type	O
of	O
data	O
variables	O
that	O
can	O
be	O
considered	O
(	O
it	O
would	O
be	O
inappropriate	O
for	O
cases	O
where	O
some	O
or	O
all	O
of	O
the	O
variables	O
represent	O
categorical	O
labels	O
for	O
instance	O
)	O
,	O
428	O
9.	O
mixture	B
models	O
and	O
em	O
section	O
2.3.7	O
but	O
it	O
can	O
also	O
make	O
the	O
determination	O
of	O
the	O
cluster	O
means	O
nonrobust	O
to	O
outliers	B
.	O
we	O
can	O
generalize	O
the	O
k-means	O
algorithm	O
by	O
introducing	O
a	O
more	O
general	O
dissimilarity	O
measure	O
v	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
between	O
two	O
vectors	O
x	O
and	O
x	O
(	O
cid:4	O
)	O
and	O
then	O
minimizing	O
the	O
following	O
distortion	B
measure	I
n	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
(	O
cid:4	O
)	O
j	O
=	O
rnkv	O
(	O
xn	O
,	O
µk	O
)	O
(	O
9.6	O
)	O
n=1	O
k=1	O
which	O
gives	O
the	O
k-medoids	O
algorithm	O
.	O
the	O
e	O
step	O
again	O
involves	O
,	O
for	O
given	O
cluster	O
prototypes	O
µk	O
,	O
assigning	O
each	O
data	O
point	O
to	O
the	O
cluster	O
for	O
which	O
the	O
dissimilarity	O
to	O
the	O
corresponding	O
prototype	O
is	O
smallest	O
.	O
the	O
computational	O
cost	O
of	O
this	O
is	O
o	O
(	O
kn	O
)	O
,	O
as	O
is	O
the	O
case	O
for	O
the	O
standard	O
k-means	O
algorithm	O
.	O
for	O
a	O
general	O
choice	O
of	O
dissimi-	O
larity	O
measure	O
,	O
the	O
m	O
step	O
is	O
potentially	O
more	O
complex	O
than	O
for	O
k-means	O
,	O
and	O
so	O
it	O
is	O
common	O
to	O
restrict	O
each	O
cluster	O
prototype	O
to	O
be	O
equal	O
to	O
one	O
of	O
the	O
data	O
vectors	O
as-	O
signed	O
to	O
that	O
cluster	O
,	O
as	O
this	O
allows	O
the	O
algorithm	O
to	O
be	O
implemented	O
for	O
any	O
choice	O
of	O
dissimilarity	O
measure	O
v	O
(	O
·	O
,	O
·	O
)	O
so	O
long	O
as	O
it	O
can	O
be	O
readily	O
evaluated	O
.	O
thus	O
the	O
m	O
step	O
involves	O
,	O
for	O
each	O
cluster	O
k	O
,	O
a	O
discrete	O
search	O
over	O
the	O
nk	O
points	O
assigned	O
to	O
that	O
cluster	O
,	O
which	O
requires	O
o	O
(	O
n	O
2	O
k	O
)	O
evaluations	O
of	O
v	O
(	O
·	O
,	O
·	O
)	O
.	O
one	O
notable	O
feature	O
of	O
the	O
k-means	O
algorithm	O
is	O
that	O
at	O
each	O
iteration	O
,	O
every	O
data	O
point	O
is	O
assigned	O
uniquely	O
to	O
one	O
,	O
and	O
only	O
one	O
,	O
of	O
the	O
clusters	O
.	O
whereas	O
some	O
data	O
points	O
will	O
be	O
much	O
closer	O
to	O
a	O
particular	O
centre	O
µk	O
than	O
to	O
any	O
other	O
centre	O
,	O
there	O
may	O
be	O
other	O
data	O
points	O
that	O
lie	O
roughly	O
midway	O
between	O
cluster	O
centres	O
.	O
in	O
the	O
latter	O
case	O
,	O
it	O
is	O
not	O
clear	O
that	O
the	O
hard	O
assignment	O
to	O
the	O
nearest	O
cluster	O
is	O
the	O
most	O
appropriate	O
.	O
we	O
shall	O
see	O
in	O
the	O
next	O
section	O
that	O
by	O
adopting	O
a	O
probabilistic	O
approach	O
,	O
we	O
obtain	O
‘	O
soft	B
’	O
assignments	O
of	O
data	O
points	O
to	O
clusters	O
in	O
a	O
way	O
that	O
reﬂects	O
the	O
level	O
of	O
uncertainty	O
over	O
the	O
most	O
appropriate	O
assignment	O
.	O
this	O
probabilistic	O
formulation	O
brings	O
with	O
it	O
numerous	O
beneﬁts	O
.	O
9.1.1	O
image	O
segmentation	O
and	O
compression	O
as	O
an	O
illustration	O
of	O
the	O
application	O
of	O
the	O
k-means	O
algorithm	O
,	O
we	O
consider	O
the	O
related	O
problems	O
of	O
image	O
segmentation	O
and	O
image	O
compression	O
.	O
the	O
goal	O
of	O
segmentation	O
is	O
to	O
partition	O
an	O
image	O
into	O
regions	O
each	O
of	O
which	O
has	O
a	O
reasonably	O
homogeneous	B
visual	O
appearance	O
or	O
which	O
corresponds	O
to	O
objects	O
or	O
parts	O
of	O
objects	O
(	O
forsyth	O
and	O
ponce	O
,	O
2003	O
)	O
.	O
each	O
pixel	O
in	O
an	O
image	O
is	O
a	O
point	O
in	O
a	O
3-dimensional	O
space	O
comprising	O
the	O
intensities	O
of	O
the	O
red	O
,	O
blue	O
,	O
and	O
green	O
channels	O
,	O
and	O
our	O
segmentation	O
algorithm	O
simply	O
treats	O
each	O
pixel	O
in	O
the	O
image	O
as	O
a	O
separate	O
data	O
point	O
.	O
note	O
that	O
strictly	O
this	O
space	O
is	O
not	O
euclidean	O
because	O
the	O
channel	O
intensities	O
are	O
bounded	O
by	O
the	O
interval	O
[	O
0	O
,	O
1	O
]	O
.	O
nevertheless	O
,	O
we	O
can	O
apply	O
the	O
k-means	O
algorithm	O
without	O
difﬁ-	O
culty	O
.	O
we	O
illustrate	O
the	O
result	O
of	O
running	O
k-means	O
to	O
convergence	O
,	O
for	O
any	O
particular	O
value	O
of	O
k	O
,	O
by	O
re-drawing	O
the	O
image	O
replacing	O
each	O
pixel	O
vector	O
with	O
the	O
{	O
r	O
,	O
g	O
,	O
b	O
}	O
intensity	O
triplet	O
given	O
by	O
the	O
centre	O
µk	O
to	O
which	O
that	O
pixel	O
has	O
been	O
assigned	O
.	O
results	O
for	O
various	O
values	O
of	O
k	O
are	O
shown	O
in	O
figure	O
9.3.	O
we	O
see	O
that	O
for	O
a	O
given	O
value	O
of	O
k	O
,	O
the	O
algorithm	O
is	O
representing	O
the	O
image	O
using	O
a	O
palette	O
of	O
only	O
k	O
colours	O
.	O
it	O
should	O
be	O
emphasized	O
that	O
this	O
use	O
of	O
k-means	O
is	O
not	O
a	O
particularly	O
sophisticated	O
approach	O
to	O
image	O
segmentation	O
,	O
not	O
least	O
because	O
it	O
takes	O
no	O
account	O
of	O
the	O
spatial	O
proximity	O
of	O
different	O
pixels	O
.	O
the	O
image	O
segmentation	O
problem	O
is	O
in	O
general	O
extremely	O
difﬁcult	O
k	O
=	O
2	O
k	O
=	O
3	O
k	O
=	O
10	O
original	O
image	O
9.1.	O
k-means	O
clustering	B
429	O
figure	O
9.3	O
two	O
examples	O
of	O
the	O
application	O
of	O
the	O
k-means	O
clustering	B
algorithm	O
to	O
image	O
segmentation	O
show-	O
ing	O
the	O
initial	O
images	O
together	O
with	O
their	O
k-means	O
segmentations	O
obtained	O
using	O
various	O
values	O
of	O
k.	O
this	O
also	O
illustrates	O
of	O
the	O
use	O
of	O
vector	B
quantization	I
for	O
data	B
compression	I
,	O
in	O
which	O
smaller	O
values	O
of	O
k	O
give	O
higher	O
compression	O
at	O
the	O
expense	O
of	O
poorer	O
image	O
quality	O
.	O
and	O
remains	O
the	O
subject	O
of	O
active	O
research	O
and	O
is	O
introduced	O
here	O
simply	O
to	O
illustrate	O
the	O
behaviour	O
of	O
the	O
k-means	O
algorithm	O
.	O
we	O
can	O
also	O
use	O
the	O
result	O
of	O
a	O
clustering	B
algorithm	O
to	O
perform	O
data	O
compres-	O
sion	B
.	O
it	O
is	O
important	O
to	O
distinguish	O
between	O
lossless	B
data	I
compression	I
,	O
in	O
which	O
the	O
goal	O
is	O
to	O
be	O
able	O
to	O
reconstruct	O
the	O
original	O
data	O
exactly	O
from	O
the	O
compressed	O
representation	O
,	O
and	O
lossy	B
data	I
compression	I
,	O
in	O
which	O
we	O
accept	O
some	O
errors	O
in	O
the	O
reconstruction	O
in	O
return	O
for	O
higher	O
levels	O
of	O
compression	O
than	O
can	O
be	O
achieved	O
in	O
the	O
lossless	O
case	O
.	O
we	O
can	O
apply	O
the	O
k-means	O
algorithm	O
to	O
the	O
problem	O
of	O
lossy	B
data	I
compression	I
as	O
follows	O
.	O
for	O
each	O
of	O
the	O
n	O
data	O
points	O
,	O
we	O
store	O
only	O
the	O
identity	O
k	O
of	O
the	O
cluster	O
to	O
which	O
it	O
is	O
assigned	O
.	O
we	O
also	O
store	O
the	O
values	O
of	O
the	O
k	O
clus-	O
ter	O
centres	O
µk	O
,	O
which	O
typically	O
requires	O
signiﬁcantly	O
less	O
data	O
,	O
provided	O
we	O
choose	O
k	O
(	O
cid:13	O
)	O
n.	O
each	O
data	O
point	O
is	O
then	O
approximated	O
by	O
its	O
nearest	O
centre	O
µk	O
.	O
new	O
data	O
points	O
can	O
similarly	O
be	O
compressed	O
by	O
ﬁrst	O
ﬁnding	O
the	O
nearest	O
µk	O
and	O
then	O
storing	O
the	O
label	O
k	O
instead	O
of	O
the	O
original	O
data	O
vector	O
.	O
this	O
framework	O
is	O
often	O
called	O
vector	B
quantization	I
,	O
and	O
the	O
vectors	O
µk	O
are	O
called	O
code-book	B
vectors	I
.	O
430	O
9.	O
mixture	B
models	O
and	O
em	O
the	O
image	O
segmentation	O
problem	O
discussed	O
above	O
also	O
provides	O
an	O
illustration	O
of	O
the	O
use	O
of	O
clustering	B
for	O
data	B
compression	I
.	O
suppose	O
the	O
original	O
image	O
has	O
n	O
pixels	O
comprising	O
{	O
r	O
,	O
g	O
,	O
b	O
}	O
values	O
each	O
of	O
which	O
is	O
stored	O
with	O
8	O
bits	B
of	O
precision	O
.	O
then	O
to	O
transmit	O
the	O
whole	O
image	O
directly	O
would	O
cost	O
24n	O
bits	B
.	O
now	O
suppose	O
we	O
ﬁrst	O
run	O
k-means	O
on	O
the	O
image	O
data	O
,	O
and	O
then	O
instead	O
of	O
transmitting	O
the	O
original	O
pixel	O
intensity	O
vectors	O
we	O
transmit	O
the	O
identity	O
of	O
the	O
nearest	O
vector	O
µk	O
.	O
because	O
there	O
are	O
k	O
such	O
vectors	O
,	O
this	O
requires	O
log2	O
k	O
bits	B
per	O
pixel	O
.	O
we	O
must	O
also	O
transmit	O
the	O
k	O
code	O
book	O
vectors	O
µk	O
,	O
which	O
requires	O
24k	O
bits	B
,	O
and	O
so	O
the	O
total	O
number	O
of	O
bits	B
required	O
to	O
transmit	O
the	O
image	O
is	O
24k	O
+	O
n	O
log2	O
k	O
(	O
rounding	O
up	O
to	O
the	O
nearest	O
integer	O
)	O
.	O
the	O
original	O
image	O
shown	O
in	O
figure	O
9.3	O
has	O
240	O
×	O
180	O
=	O
43	O
,	O
200	O
pixels	O
and	O
so	O
requires	O
24	O
×	O
43	O
,	O
200	O
=	O
1	O
,	O
036	O
,	O
800	O
bits	B
to	O
transmit	O
directly	O
.	O
by	O
comparison	O
,	O
the	O
compressed	O
images	O
require	O
43	O
,	O
248	O
bits	B
(	O
k	O
=	O
2	O
)	O
,	O
86	O
,	O
472	O
bits	B
(	O
k	O
=	O
3	O
)	O
,	O
and	O
173	O
,	O
040	O
bits	B
(	O
k	O
=	O
10	O
)	O
,	O
respectively	O
,	O
to	O
transmit	O
.	O
these	O
represent	O
compression	O
ratios	O
compared	O
to	O
the	O
original	O
image	O
of	O
4.2	O
%	O
,	O
8.3	O
%	O
,	O
and	O
16.7	O
%	O
,	O
respectively	O
.	O
we	O
see	O
that	O
there	O
is	O
a	O
trade-off	O
between	O
degree	O
of	O
compression	O
and	O
image	O
quality	O
.	O
note	O
that	O
our	O
aim	O
in	O
this	O
example	O
is	O
to	O
illustrate	O
the	O
k-means	O
algorithm	O
.	O
if	O
we	O
had	O
been	O
aiming	O
to	O
produce	O
a	O
good	O
image	O
compressor	O
,	O
then	O
it	O
would	O
be	O
more	O
fruitful	O
to	O
consider	O
small	O
blocks	O
of	O
adjacent	O
pixels	O
,	O
for	O
instance	O
5×	O
5	O
,	O
and	O
thereby	O
exploit	O
the	O
correlations	O
that	O
exist	O
in	O
natural	O
images	O
between	O
nearby	O
pixels	O
.	O
9.2.	O
mixtures	O
of	O
gaussians	O
in	O
section	O
2.3.9	O
we	O
motivated	O
the	O
gaussian	O
mixture	B
model	I
as	O
a	O
simple	O
linear	O
super-	O
position	O
of	O
gaussian	O
components	O
,	O
aimed	O
at	O
providing	O
a	O
richer	O
class	O
of	O
density	B
mod-	O
els	O
than	O
the	O
single	O
gaussian	O
.	O
we	O
now	O
turn	O
to	O
a	O
formulation	O
of	O
gaussian	O
mixtures	O
in	O
terms	O
of	O
discrete	O
latent	O
variables	O
.	O
this	O
will	O
provide	O
us	O
with	O
a	O
deeper	O
insight	O
into	O
this	O
important	O
distribution	O
,	O
and	O
will	O
also	O
serve	O
to	O
motivate	O
the	O
expectation-maximization	O
algorithm	O
.	O
recall	O
from	O
(	O
2.188	O
)	O
that	O
the	O
gaussian	O
mixture	B
distribution	I
can	O
be	O
written	O
as	O
a	O
linear	O
superposition	O
of	O
gaussians	O
in	O
the	O
form	O
p	O
(	O
x	O
)	O
=	O
πkn	O
(	O
x|µk	O
,	O
σk	O
)	O
.	O
(	O
9.7	O
)	O
let	O
us	O
introduce	O
a	O
k-dimensional	O
binary	O
random	O
variable	O
z	O
having	O
a	O
1-of-k	O
repre-	O
sentation	O
in	O
which	O
a	O
particular	O
element	O
zk	O
is	O
equal	O
to	O
1	O
and	O
all	O
other	O
elements	O
are	O
equal	O
to	O
0.	O
the	O
values	O
of	O
zk	O
therefore	O
satisfy	O
zk	O
∈	O
{	O
0	O
,	O
1	O
}	O
and	O
k	O
zk	O
=	O
1	O
,	O
and	O
we	O
see	O
that	O
there	O
are	O
k	O
possible	O
states	O
for	O
the	O
vector	O
z	O
according	O
to	O
which	O
element	O
is	O
nonzero	O
.	O
we	O
shall	O
deﬁne	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
z	O
)	O
in	O
terms	O
of	O
a	O
marginal	B
dis-	O
tribution	O
p	O
(	O
z	O
)	O
and	O
a	O
conditional	B
distribution	O
p	O
(	O
x|z	O
)	O
,	O
corresponding	O
to	O
the	O
graphical	B
model	I
in	O
figure	O
9.4.	O
the	O
marginal	B
distribution	O
over	O
z	O
is	O
speciﬁed	O
in	O
terms	O
of	O
the	O
mixing	O
coefﬁcients	O
πk	O
,	O
such	O
that	O
(	O
cid:5	O
)	O
k	O
(	O
cid:2	O
)	O
k=1	O
p	O
(	O
zk	O
=	O
1	O
)	O
=	O
πk	O
9.2.	O
mixtures	O
of	O
gaussians	O
431	O
figure	O
9.4	O
graphical	O
representation	O
of	O
a	O
mixture	B
model	I
,	O
in	O
which	O
the	O
joint	O
distribution	O
is	O
expressed	O
in	O
the	O
form	O
p	O
(	O
x	O
,	O
z	O
)	O
=	O
p	O
(	O
z	O
)	O
p	O
(	O
x|z	O
)	O
.	O
z	O
x	O
where	O
the	O
parameters	O
{	O
πk	O
}	O
must	O
satisfy	O
k	O
(	O
cid:2	O
)	O
together	O
with	O
0	O
(	O
cid:1	O
)	O
πk	O
(	O
cid:1	O
)	O
1	O
πk	O
=	O
1	O
k=1	O
(	O
9.8	O
)	O
(	O
9.9	O
)	O
in	O
order	O
to	O
be	O
valid	O
probabilities	O
.	O
because	O
z	O
uses	O
a	O
1-of-k	O
representation	O
,	O
we	O
can	O
also	O
write	O
this	O
distribution	O
in	O
the	O
form	O
p	O
(	O
z	O
)	O
=	O
πzk	O
k	O
.	O
(	O
9.10	O
)	O
similarly	O
,	O
the	O
conditional	B
distribution	O
of	O
x	O
given	O
a	O
particular	O
value	O
for	O
z	O
is	O
a	O
gaussian	O
k=1	O
p	O
(	O
x|zk	O
=	O
1	O
)	O
=	O
n	O
(	O
x|µk	O
,	O
σk	O
)	O
k	O
(	O
cid:14	O
)	O
k	O
(	O
cid:14	O
)	O
k=1	O
which	O
can	O
also	O
be	O
written	O
in	O
the	O
form	O
p	O
(	O
x|z	O
)	O
=	O
n	O
(	O
x|µk	O
,	O
σk	O
)	O
zk	O
.	O
(	O
9.11	O
)	O
exercise	O
9.3	O
the	O
joint	O
distribution	O
is	O
given	O
by	O
p	O
(	O
z	O
)	O
p	O
(	O
x|z	O
)	O
,	O
and	O
the	O
marginal	B
distribution	O
of	O
x	O
is	O
then	O
obtained	O
by	O
summing	O
the	O
joint	O
distribution	O
over	O
all	O
possible	O
states	O
of	O
z	O
to	O
give	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
z	O
)	O
p	O
(	O
x|z	O
)	O
=	O
πkn	O
(	O
x|µk	O
,	O
σk	O
)	O
(	O
9.12	O
)	O
z	O
k=1	O
(	O
cid:5	O
)	O
where	O
we	O
have	O
made	O
use	O
of	O
(	O
9.10	O
)	O
and	O
(	O
9.11	O
)	O
.	O
thus	O
the	O
marginal	B
distribution	O
of	O
x	O
is	O
a	O
gaussian	O
mixture	O
of	O
the	O
form	O
(	O
9.7	O
)	O
.	O
if	O
we	O
have	O
several	O
observations	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
then	O
,	O
because	O
we	O
have	O
represented	O
the	O
marginal	B
distribution	O
in	O
the	O
form	O
p	O
(	O
x	O
)	O
=	O
z	O
p	O
(	O
x	O
,	O
z	O
)	O
,	O
it	O
follows	O
that	O
for	O
every	O
observed	O
data	O
point	O
xn	O
there	O
is	O
a	O
corresponding	O
latent	B
variable	I
zn	O
.	O
we	O
have	O
therefore	O
found	O
an	O
equivalent	O
formulation	O
of	O
the	O
gaussian	O
mixture	B
in-	O
volving	O
an	O
explicit	O
latent	B
variable	I
.	O
it	O
might	O
seem	O
that	O
we	O
have	O
not	O
gained	O
much	O
by	O
doing	O
so	O
.	O
however	O
,	O
we	O
are	O
now	O
able	O
to	O
work	O
with	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
z	O
)	O
432	O
9.	O
mixture	B
models	O
and	O
em	O
instead	O
of	O
the	O
marginal	B
distribution	O
p	O
(	O
x	O
)	O
,	O
and	O
this	O
will	O
lead	O
to	O
signiﬁcant	O
simpliﬁca-	O
tions	O
,	O
most	O
notably	O
through	O
the	O
introduction	O
of	O
the	O
expectation-maximization	O
(	O
em	O
)	O
algorithm	O
.	O
another	O
quantity	O
that	O
will	O
play	O
an	O
important	O
role	O
is	O
the	O
conditional	B
probability	I
of	O
z	O
given	O
x.	O
we	O
shall	O
use	O
γ	O
(	O
zk	O
)	O
to	O
denote	O
p	O
(	O
zk	O
=	O
1|x	O
)	O
,	O
whose	O
value	O
can	O
be	O
found	O
using	O
bayes	O
’	O
theorem	O
γ	O
(	O
zk	O
)	O
≡	O
p	O
(	O
zk	O
=	O
1|x	O
)	O
=	O
=	O
k	O
(	O
cid:2	O
)	O
p	O
(	O
zk	O
=	O
1	O
)	O
p	O
(	O
x|zk	O
=	O
1	O
)	O
p	O
(	O
zj	O
=	O
1	O
)	O
p	O
(	O
x|zj	O
=	O
1	O
)	O
k	O
(	O
cid:2	O
)	O
πkn	O
(	O
x|µk	O
,	O
σk	O
)	O
πjn	O
(	O
x|µj	O
,	O
σj	O
)	O
j=1	O
.	O
(	O
9.13	O
)	O
section	O
8.1.2	O
j=1	O
we	O
shall	O
view	O
πk	O
as	O
the	O
prior	B
probability	O
of	O
zk	O
=	O
1	O
,	O
and	O
the	O
quantity	O
γ	O
(	O
zk	O
)	O
as	O
the	O
corresponding	O
posterior	B
probability	I
once	O
we	O
have	O
observed	O
x.	O
as	O
we	O
shall	O
see	O
later	O
,	O
γ	O
(	O
zk	O
)	O
can	O
also	O
be	O
viewed	O
as	O
the	O
responsibility	B
that	O
component	O
k	O
takes	O
for	O
‘	O
explain-	O
ing	O
’	O
the	O
observation	O
x.	O
we	O
can	O
use	O
the	O
technique	O
of	O
ancestral	B
sampling	I
to	O
generate	O
random	O
samples	O
distributed	O
according	O
to	O
the	O
gaussian	O
mixture	B
model	I
.	O
to	O
do	O
this	O
,	O
we	O
ﬁrst	O
generate	O
a	O
value	O
for	O
z	O
,	O
which	O
we	O
denote	O
(	O
cid:1	O
)	O
z	O
,	O
from	O
the	O
marginal	B
distribution	O
p	O
(	O
z	O
)	O
and	O
then	O
generate	O
a	O
value	O
for	O
x	O
from	O
the	O
conditional	B
distribution	O
p	O
(	O
x|	O
(	O
cid:1	O
)	O
z	O
)	O
.	O
techniques	O
for	O
sampling	O
from	O
standard	O
distributions	O
are	O
discussed	O
in	O
chapter	O
11.	O
we	O
can	O
depict	O
samples	O
from	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
z	O
)	O
by	O
plotting	O
points	O
at	O
the	O
corresponding	O
values	O
of	O
x	O
and	O
then	O
colouring	O
them	O
according	O
to	O
the	O
value	O
of	O
z	O
,	O
in	O
other	O
words	O
according	O
to	O
which	O
gaussian	O
component	O
was	O
responsible	O
for	O
generating	O
them	O
,	O
as	O
shown	O
in	O
figure	O
9.5	O
(	O
a	O
)	O
.	O
similarly	O
samples	O
from	O
the	O
marginal	B
distribution	O
p	O
(	O
x	O
)	O
are	O
obtained	O
by	O
taking	O
the	O
samples	O
from	O
the	O
joint	O
distribution	O
and	O
ignoring	O
the	O
values	O
of	O
z.	O
these	O
are	O
illustrated	O
in	O
figure	O
9.5	O
(	O
b	O
)	O
by	O
plotting	O
the	O
x	O
values	O
without	O
any	O
coloured	O
labels	O
.	O
we	O
can	O
also	O
use	O
this	O
synthetic	O
data	O
set	O
to	O
illustrate	O
the	O
‘	O
responsibilities	O
’	O
by	O
eval-	O
uating	O
,	O
for	O
every	O
data	O
point	O
,	O
the	O
posterior	B
probability	I
for	O
each	O
component	O
in	O
the	O
mixture	B
distribution	I
from	O
which	O
this	O
data	O
set	O
was	O
generated	O
.	O
in	O
particular	O
,	O
we	O
can	O
represent	O
the	O
value	O
of	O
the	O
responsibilities	O
γ	O
(	O
znk	O
)	O
associated	O
with	O
data	O
point	O
xn	O
by	O
plotting	O
the	O
corresponding	O
point	O
using	O
proportions	O
of	O
red	O
,	O
blue	O
,	O
and	O
green	O
ink	O
given	O
by	O
γ	O
(	O
znk	O
)	O
for	O
k	O
=	O
1	O
,	O
2	O
,	O
3	O
,	O
respectively	O
,	O
as	O
shown	O
in	O
figure	O
9.5	O
(	O
c	O
)	O
.	O
so	O
,	O
for	O
instance	O
,	O
a	O
data	O
point	O
for	O
which	O
γ	O
(	O
zn1	O
)	O
=	O
1	O
will	O
be	O
coloured	O
red	O
,	O
whereas	O
one	O
for	O
which	O
γ	O
(	O
zn2	O
)	O
=	O
γ	O
(	O
zn3	O
)	O
=	O
0.5	O
will	O
be	O
coloured	O
with	O
equal	O
proportions	O
of	O
blue	O
and	O
green	O
ink	O
and	O
so	O
will	O
appear	O
cyan	O
.	O
this	O
should	O
be	O
compared	O
with	O
figure	O
9.5	O
(	O
a	O
)	O
in	O
which	O
the	O
data	O
points	O
were	O
labelled	O
using	O
the	O
true	O
identity	O
of	O
the	O
component	O
from	O
which	O
they	O
were	O
generated	O
.	O
9.2.1	O
maximum	B
likelihood	I
suppose	O
we	O
have	O
a	O
data	O
set	O
of	O
observations	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
,	O
and	O
we	O
wish	O
to	O
model	O
this	O
data	O
using	O
a	O
mixture	O
of	O
gaussians	O
.	O
we	O
can	O
represent	O
this	O
data	O
set	O
as	O
an	O
n	O
×	O
d	O
9.2.	O
mixtures	O
of	O
gaussians	O
433	O
1	O
(	O
a	O
)	O
1	O
(	O
b	O
)	O
1	O
(	O
c	O
)	O
0.5	O
0	O
0.5	O
0	O
0.5	O
0	O
0	O
0.5	O
1	O
0	O
0.5	O
1	O
0	O
0.5	O
1	O
figure	O
9.5	O
example	O
of	O
500	O
points	O
drawn	O
from	O
the	O
mixture	O
of	O
3	O
gaussians	O
shown	O
in	O
figure	O
2.23	O
.	O
(	O
a	O
)	O
samples	O
from	O
the	O
joint	O
distribution	O
p	O
(	O
z	O
)	O
p	O
(	O
x|z	O
)	O
in	O
which	O
the	O
three	O
states	O
of	O
z	O
,	O
corresponding	O
to	O
the	O
three	O
components	O
of	O
the	O
mixture	B
,	O
are	O
depicted	O
in	O
red	O
,	O
green	O
,	O
and	O
blue	O
,	O
and	O
(	O
b	O
)	O
the	O
corresponding	O
samples	O
from	O
the	O
marginal	B
distribution	O
p	O
(	O
x	O
)	O
,	O
which	O
is	O
obtained	O
by	O
simply	O
ignoring	O
the	O
values	O
of	O
z	O
and	O
just	O
plotting	O
the	O
x	O
values	O
.	O
the	O
data	O
set	O
in	O
(	O
a	O
)	O
is	O
said	O
to	O
be	O
complete	O
,	O
whereas	O
that	O
in	O
(	O
b	O
)	O
is	O
incomplete	O
.	O
(	O
c	O
)	O
the	O
same	O
samples	O
in	O
which	O
the	O
colours	O
represent	O
the	O
value	O
of	O
the	O
responsibilities	O
γ	O
(	O
znk	O
)	O
associated	O
with	O
data	O
point	O
xn	O
,	O
obtained	O
by	O
plotting	O
the	O
corresponding	O
point	O
using	O
proportions	O
of	O
red	O
,	O
blue	O
,	O
and	O
green	O
ink	O
given	O
by	O
γ	O
(	O
znk	O
)	O
for	O
k	O
=	O
1	O
,	O
2	O
,	O
3	O
,	O
respectively	O
matrix	O
x	O
in	O
which	O
the	O
nth	O
row	O
is	O
given	O
by	O
xt	O
n.	O
similarly	O
,	O
the	O
corresponding	O
latent	O
variables	O
will	O
be	O
denoted	O
by	O
an	O
n	O
×	O
k	O
matrix	O
z	O
with	O
rows	O
zt	O
n.	O
if	O
we	O
assume	O
that	O
the	O
data	O
points	O
are	O
drawn	O
independently	O
from	O
the	O
distribution	O
,	O
then	O
we	O
can	O
express	O
the	O
gaussian	O
mixture	B
model	I
for	O
this	O
i.i.d	O
.	O
data	O
set	O
using	O
the	O
graphical	O
representation	O
shown	O
in	O
figure	O
9.6.	O
from	O
(	O
9.7	O
)	O
the	O
log	O
of	O
the	O
likelihood	B
function	I
is	O
given	O
by	O
n	O
(	O
cid:2	O
)	O
ln	O
p	O
(	O
x|π	O
,	O
µ	O
,	O
σ	O
)	O
=	O
ln	O
(	O
cid:24	O
)	O
k	O
(	O
cid:2	O
)	O
(	O
cid:25	O
)	O
πkn	O
(	O
xn|µk	O
,	O
σk	O
)	O
.	O
(	O
9.14	O
)	O
n=1	O
k=1	O
before	O
discussing	O
how	O
to	O
maximize	O
this	O
function	O
,	O
it	O
is	O
worth	O
emphasizing	O
that	O
there	O
is	O
a	O
signiﬁcant	O
problem	O
associated	O
with	O
the	O
maximum	B
likelihood	I
framework	O
applied	O
to	O
gaussian	O
mixture	B
models	O
,	O
due	O
to	O
the	O
presence	O
of	O
singularities	B
.	O
for	O
sim-	O
plicity	O
,	O
consider	O
a	O
gaussian	O
mixture	B
whose	O
components	O
have	O
covariance	B
matrices	O
given	O
by	O
σk	O
=	O
σ2	O
ki	O
,	O
where	O
i	O
is	O
the	O
unit	O
matrix	O
,	O
although	O
the	O
conclusions	O
will	O
hold	O
for	O
general	O
covariance	B
matrices	O
.	O
suppose	O
that	O
one	O
of	O
the	O
components	O
of	O
the	O
mixture	B
model	I
,	O
let	O
us	O
say	O
the	O
jth	O
component	O
,	O
has	O
its	O
mean	B
µj	O
exactly	O
equal	O
to	O
one	O
of	O
the	O
data	O
figure	O
9.6	O
graphical	O
representation	O
of	O
a	O
gaussian	O
mixture	B
model	I
for	O
a	O
set	O
of	O
n	O
i.i.d	O
.	O
data	O
points	O
{	O
xn	O
}	O
,	O
with	O
corresponding	O
latent	O
points	O
{	O
zn	O
}	O
,	O
where	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
zn	O
xn	O
π	O
µ	O
σ	O
n	O
434	O
9.	O
mixture	B
models	O
and	O
em	O
figure	O
9.7	O
illustration	O
of	O
how	O
singularities	B
in	O
the	O
likelihood	B
function	I
arise	O
with	O
mixtures	O
of	O
gaussians	O
.	O
this	O
should	O
be	O
com-	O
pared	O
with	O
the	O
case	O
of	O
a	O
single	O
gaus-	O
sian	O
shown	O
in	O
figure	O
1.14	O
for	O
which	O
no	O
singularities	B
arise	O
.	O
p	O
(	O
x	O
)	O
points	O
so	O
that	O
µj	O
=	O
xn	O
for	O
some	O
value	O
of	O
n.	O
this	O
data	O
point	O
will	O
then	O
contribute	O
a	O
term	O
in	O
the	O
likelihood	B
function	I
of	O
the	O
form	O
x	O
1	O
1	O
σj	O
.	O
n	O
(	O
xn|xn	O
,	O
σ2	O
j	O
i	O
)	O
=	O
(	O
2π	O
)	O
1/2	O
(	O
9.15	O
)	O
if	O
we	O
consider	O
the	O
limit	O
σj	O
→	O
0	O
,	O
then	O
we	O
see	O
that	O
this	O
term	O
goes	O
to	O
inﬁnity	O
and	O
so	O
the	O
log	O
likelihood	O
function	O
will	O
also	O
go	O
to	O
inﬁnity	O
.	O
thus	O
the	O
maximization	O
of	O
the	O
log	O
likelihood	O
function	O
is	O
not	O
a	O
well	O
posed	O
problem	O
because	O
such	O
singularities	B
will	O
always	O
be	O
present	O
and	O
will	O
occur	O
whenever	O
one	O
of	O
the	O
gaussian	O
components	O
‘	O
collapses	O
’	O
onto	O
a	O
speciﬁc	O
data	O
point	O
.	O
recall	O
that	O
this	O
problem	O
did	O
not	O
arise	O
in	O
the	O
case	O
of	O
a	O
single	O
gaussian	O
distribution	O
.	O
to	O
understand	O
the	O
difference	O
,	O
note	O
that	O
if	O
a	O
single	O
gaussian	O
collapses	O
onto	O
a	O
data	O
point	O
it	O
will	O
contribute	O
multiplicative	O
factors	O
to	O
the	O
likelihood	B
function	I
arising	O
from	O
the	O
other	O
data	O
points	O
and	O
these	O
factors	O
will	O
go	O
to	O
zero	O
exponentially	O
fast	O
,	O
giving	O
an	O
overall	O
likelihood	O
that	O
goes	O
to	O
zero	O
rather	O
than	O
inﬁnity	O
.	O
however	O
,	O
once	O
we	O
have	O
(	O
at	O
least	O
)	O
two	O
components	O
in	O
the	O
mixture	B
,	O
one	O
of	O
the	O
components	O
can	O
have	O
a	O
ﬁnite	O
variance	O
and	O
therefore	O
assign	O
ﬁnite	O
probability	O
to	O
all	O
of	O
the	O
data	O
points	O
while	O
the	O
other	O
component	O
can	O
shrink	O
onto	O
one	O
speciﬁc	O
data	O
point	O
and	O
thereby	O
contribute	O
an	O
ever	O
increasing	O
additive	O
value	O
to	O
the	O
log	O
likelihood	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
9.7.	O
these	O
singularities	B
provide	O
another	O
example	O
of	O
the	O
severe	O
over-ﬁtting	B
that	O
can	O
occur	O
in	O
a	O
maximum	B
likelihood	I
approach	O
.	O
we	O
shall	O
see	O
that	O
this	O
difﬁculty	O
does	O
not	O
occur	O
if	O
we	O
adopt	O
a	O
bayesian	O
approach	O
.	O
for	O
the	O
moment	O
,	O
however	O
,	O
we	O
simply	O
note	O
that	O
in	O
applying	O
maximum	B
likelihood	I
to	O
gaussian	O
mixture	B
models	O
we	O
must	O
take	O
steps	O
to	O
avoid	O
ﬁnding	O
such	O
pathological	O
solutions	O
and	O
instead	O
seek	O
local	B
maxima	O
of	O
the	O
likelihood	B
function	I
that	O
are	O
well	O
behaved	O
.	O
we	O
can	O
hope	O
to	O
avoid	O
the	O
singularities	B
by	O
using	O
suitable	O
heuristics	O
,	O
for	O
instance	O
by	O
detecting	O
when	O
a	O
gaussian	O
component	O
is	O
collapsing	O
and	O
resetting	O
its	O
mean	B
to	O
a	O
randomly	O
chosen	O
value	O
while	O
also	O
resetting	O
its	O
covariance	B
to	O
some	O
large	O
value	O
,	O
and	O
then	O
continuing	O
with	O
the	O
optimization	O
.	O
a	O
further	O
issue	O
in	O
ﬁnding	O
maximum	B
likelihood	I
solutions	O
arises	O
from	O
the	O
fact	O
that	O
for	O
any	O
given	O
maximum	B
likelihood	I
solution	O
,	O
a	O
k-component	O
mixture	B
will	O
have	O
a	O
total	O
of	O
k	O
!	O
equivalent	O
solutions	O
corresponding	O
to	O
the	O
k	O
!	O
ways	O
of	O
assigning	O
k	O
sets	O
of	O
parameters	O
to	O
k	O
components	O
.	O
in	O
other	O
words	O
,	O
for	O
any	O
given	O
(	O
nondegenerate	O
)	O
point	O
in	O
the	O
space	O
of	O
parameter	O
values	O
there	O
will	O
be	O
a	O
further	O
k	O
!	O
−1	O
additional	O
points	O
all	O
of	O
which	O
give	O
rise	O
to	O
exactly	O
the	O
same	O
distribution	O
.	O
this	O
problem	O
is	O
known	O
as	O
section	O
10.1	O
9.2.	O
mixtures	O
of	O
gaussians	O
435	O
identiﬁability	B
(	O
casella	O
and	O
berger	O
,	O
2002	O
)	O
and	O
is	O
an	O
important	O
issue	O
when	O
we	O
wish	O
to	O
interpret	O
the	O
parameter	O
values	O
discovered	O
by	O
a	O
model	O
.	O
identiﬁability	B
will	O
also	O
arise	O
when	O
we	O
discuss	O
models	O
having	O
continuous	O
latent	O
variables	O
in	O
chapter	O
12.	O
however	O
,	O
for	O
the	O
purposes	O
of	O
ﬁnding	O
a	O
good	O
density	B
model	O
,	O
it	O
is	O
irrelevant	O
because	O
any	O
of	O
the	O
equivalent	O
solutions	O
is	O
as	O
good	O
as	O
any	O
other	O
.	O
maximizing	O
the	O
log	O
likelihood	O
function	O
(	O
9.14	O
)	O
for	O
a	O
gaussian	O
mixture	B
model	I
turns	O
out	O
to	O
be	O
a	O
more	O
complex	O
problem	O
than	O
for	O
the	O
case	O
of	O
a	O
single	O
gaussian	O
.	O
the	O
difﬁculty	O
arises	O
from	O
the	O
presence	O
of	O
the	O
summation	O
over	O
k	O
that	O
appears	O
inside	O
the	O
logarithm	O
in	O
(	O
9.14	O
)	O
,	O
so	O
that	O
the	O
logarithm	O
function	O
no	O
longer	O
acts	O
directly	O
on	O
the	O
gaussian	O
.	O
if	O
we	O
set	O
the	O
derivatives	O
of	O
the	O
log	O
likelihood	O
to	O
zero	O
,	O
we	O
will	O
no	O
longer	O
obtain	O
a	O
closed	O
form	O
solution	O
,	O
as	O
we	O
shall	O
see	O
shortly	O
.	O
one	O
approach	O
is	O
to	O
apply	O
gradient-based	O
optimization	O
techniques	O
(	O
fletcher	O
,	O
1987	O
;	O
nocedal	O
and	O
wright	O
,	O
1999	O
;	O
bishop	O
and	O
nabney	O
,	O
2008	O
)	O
.	O
although	O
gradient-based	O
techniques	O
are	O
feasible	O
,	O
and	O
indeed	O
will	O
play	O
an	O
important	O
role	O
when	O
we	O
discuss	O
mixture	O
density	O
networks	O
in	O
chapter	O
5	O
,	O
we	O
now	O
consider	O
an	O
alternative	O
approach	O
known	O
as	O
the	O
em	O
algorithm	O
which	O
has	O
broad	O
applicability	O
and	O
which	O
will	O
lay	O
the	O
foundations	O
for	O
a	O
discussion	O
of	O
variational	B
inference	I
techniques	O
in	O
chapter	O
10	O
.	O
9.2.2	O
em	O
for	O
gaussian	O
mixtures	O
an	O
elegant	O
and	O
powerful	O
method	O
for	O
ﬁnding	O
maximum	B
likelihood	I
solutions	O
for	O
models	O
with	O
latent	O
variables	O
is	O
called	O
the	O
expectation-maximization	O
algorithm	O
,	O
or	O
em	O
algorithm	O
(	O
dempster	O
et	O
al.	O
,	O
1977	O
;	O
mclachlan	O
and	O
krishnan	O
,	O
1997	O
)	O
.	O
later	O
we	O
shall	O
give	O
a	O
general	O
treatment	O
of	O
em	O
,	O
and	O
we	O
shall	O
also	O
show	O
how	O
em	O
can	O
be	O
generalized	B
to	O
obtain	O
the	O
variational	B
inference	I
framework	O
.	O
initially	O
,	O
we	O
shall	O
motivate	O
the	O
em	O
algorithm	O
by	O
giving	O
a	O
relatively	O
informal	O
treatment	O
in	O
the	O
context	O
of	O
the	O
gaussian	O
mixture	B
model	I
.	O
we	O
emphasize	O
,	O
however	O
,	O
that	O
em	O
has	O
broad	O
applicability	O
,	O
and	O
indeed	O
it	O
will	O
be	O
encountered	O
in	O
the	O
context	O
of	O
a	O
variety	O
of	O
different	O
models	O
in	O
this	O
book	O
.	O
let	O
us	O
begin	O
by	O
writing	O
down	O
the	O
conditions	O
that	O
must	O
be	O
satisﬁed	O
at	O
a	O
maximum	O
of	O
the	O
likelihood	B
function	I
.	O
setting	O
the	O
derivatives	O
of	O
ln	O
p	O
(	O
x|π	O
,	O
µ	O
,	O
σ	O
)	O
in	O
(	O
9.14	O
)	O
with	O
respect	O
to	O
the	O
means	O
µk	O
of	O
the	O
gaussian	O
components	O
to	O
zero	O
,	O
we	O
obtain	O
σk	O
(	O
xn	O
−	O
µk	O
)	O
(	O
9.16	O
)	O
0	O
=	O
−	O
n	O
(	O
cid:2	O
)	O
n=1	O
(	O
cid:5	O
)	O
πkn	O
(	O
xn|µk	O
,	O
σk	O
)	O
(	O
+	O
j	O
πjn	O
(	O
xn|µj	O
,	O
σj	O
)	O
)	O
*	O
γ	O
(	O
znk	O
)	O
section	O
10.1	O
where	O
we	O
have	O
made	O
use	O
of	O
the	O
form	O
(	O
2.43	O
)	O
for	O
the	O
gaussian	O
distribution	O
.	O
note	O
that	O
the	O
posterior	O
probabilities	O
,	O
or	O
responsibilities	O
,	O
given	O
by	O
(	O
9.13	O
)	O
appear	O
naturally	O
on	O
the	O
right-hand	O
side	O
.	O
multiplying	O
by	O
σ	O
(	O
which	O
we	O
assume	O
to	O
be	O
nonsingular	O
)	O
and	O
rearranging	O
we	O
obtain	O
−1	O
k	O
µk	O
=	O
1	O
nk	O
γ	O
(	O
znk	O
)	O
xn	O
where	O
we	O
have	O
deﬁned	O
nk	O
=	O
γ	O
(	O
znk	O
)	O
.	O
(	O
9.17	O
)	O
(	O
9.18	O
)	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
n=1	O
436	O
9.	O
mixture	B
models	O
and	O
em	O
section	O
2.3.4	O
appendix	O
e	O
we	O
can	O
interpret	O
nk	O
as	O
the	O
effective	O
number	O
of	O
points	O
assigned	O
to	O
cluster	O
k.	O
note	O
carefully	O
the	O
form	O
of	O
this	O
solution	O
.	O
we	O
see	O
that	O
the	O
mean	B
µk	O
for	O
the	O
kth	O
gaussian	O
component	O
is	O
obtained	O
by	O
taking	O
a	O
weighted	O
mean	O
of	O
all	O
of	O
the	O
points	O
in	O
the	O
data	O
set	O
,	O
in	O
which	O
the	O
weighting	O
factor	O
for	O
data	O
point	O
xn	O
is	O
given	O
by	O
the	O
posterior	B
probability	I
γ	O
(	O
znk	O
)	O
that	O
component	O
k	O
was	O
responsible	O
for	O
generating	O
xn	O
.	O
if	O
we	O
set	O
the	O
derivative	B
of	O
ln	O
p	O
(	O
x|π	O
,	O
µ	O
,	O
σ	O
)	O
with	O
respect	O
to	O
σk	O
to	O
zero	O
,	O
and	O
follow	O
a	O
similar	O
line	O
of	O
reasoning	O
,	O
making	O
use	O
of	O
the	O
result	O
for	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
covariance	B
matrix	I
of	O
a	O
single	O
gaussian	O
,	O
we	O
obtain	O
γ	O
(	O
znk	O
)	O
(	O
xn	O
−	O
µk	O
)	O
(	O
xn	O
−	O
µk	O
)	O
t	O
(	O
9.19	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
σk	O
=	O
1	O
nk	O
which	O
has	O
the	O
same	O
form	O
as	O
the	O
corresponding	O
result	O
for	O
a	O
single	O
gaussian	O
ﬁtted	O
to	O
the	O
data	O
set	O
,	O
but	O
again	O
with	O
each	O
data	O
point	O
weighted	O
by	O
the	O
corresponding	O
poste-	O
rior	O
probability	B
and	O
with	O
the	O
denominator	O
given	O
by	O
the	O
effective	O
number	O
of	O
points	O
associated	O
with	O
the	O
corresponding	O
component	O
.	O
finally	O
,	O
we	O
maximize	O
ln	O
p	O
(	O
x|π	O
,	O
µ	O
,	O
σ	O
)	O
with	O
respect	O
to	O
the	O
mixing	O
coefﬁcients	O
πk	O
.	O
here	O
we	O
must	O
take	O
account	O
of	O
the	O
constraint	O
(	O
9.9	O
)	O
,	O
which	O
requires	O
the	O
mixing	O
coefﬁcients	O
to	O
sum	O
to	O
one	O
.	O
this	O
can	O
be	O
achieved	O
using	O
a	O
lagrange	O
multiplier	O
and	O
maximizing	O
the	O
following	O
quantity	O
(	O
cid:22	O
)	O
k	O
(	O
cid:2	O
)	O
(	O
cid:23	O
)	O
ln	O
p	O
(	O
x|π	O
,	O
µ	O
,	O
σ	O
)	O
+	O
λ	O
πk	O
−	O
1	O
(	O
9.20	O
)	O
which	O
gives	O
n	O
(	O
cid:2	O
)	O
n=1	O
k=1	O
(	O
cid:5	O
)	O
n	O
(	O
xn|µk	O
,	O
σk	O
)	O
j	O
πjn	O
(	O
xn|µj	O
,	O
σj	O
)	O
0	O
=	O
+	O
λ	O
(	O
9.21	O
)	O
where	O
again	O
we	O
see	O
the	O
appearance	O
of	O
the	O
responsibilities	O
.	O
if	O
we	O
now	O
multiply	O
both	O
sides	O
by	O
πk	O
and	O
sum	O
over	O
k	O
making	O
use	O
of	O
the	O
constraint	O
(	O
9.9	O
)	O
,	O
we	O
ﬁnd	O
λ	O
=	O
−n	O
.	O
using	O
this	O
to	O
eliminate	O
λ	O
and	O
rearranging	O
we	O
obtain	O
πk	O
=	O
nk	O
n	O
(	O
9.22	O
)	O
so	O
that	O
the	O
mixing	B
coefﬁcient	I
for	O
the	O
kth	O
component	O
is	O
given	O
by	O
the	O
average	O
respon-	O
sibility	O
which	O
that	O
component	O
takes	O
for	O
explaining	O
the	O
data	O
points	O
.	O
it	O
is	O
worth	O
emphasizing	O
that	O
the	O
results	O
(	O
9.17	O
)	O
,	O
(	O
9.19	O
)	O
,	O
and	O
(	O
9.22	O
)	O
do	O
not	O
con-	O
stitute	O
a	O
closed-form	O
solution	O
for	O
the	O
parameters	O
of	O
the	O
mixture	B
model	I
because	O
the	O
responsibilities	O
γ	O
(	O
znk	O
)	O
depend	O
on	O
those	O
parameters	O
in	O
a	O
complex	O
way	O
through	O
(	O
9.13	O
)	O
.	O
however	O
,	O
these	O
results	O
do	O
suggest	O
a	O
simple	O
iterative	O
scheme	O
for	O
ﬁnding	O
a	O
solution	O
to	O
the	O
maximum	B
likelihood	I
problem	O
,	O
which	O
as	O
we	O
shall	O
see	O
turns	O
out	O
to	O
be	O
an	O
instance	O
of	O
the	O
em	O
algorithm	O
for	O
the	O
particular	O
case	O
of	O
the	O
gaussian	O
mixture	B
model	I
.	O
we	O
ﬁrst	O
choose	O
some	O
initial	O
values	O
for	O
the	O
means	O
,	O
covariances	O
,	O
and	O
mixing	O
coefﬁcients	O
.	O
then	O
we	O
alternate	O
between	O
the	O
following	O
two	O
updates	O
that	O
we	O
shall	O
call	O
the	O
e	O
step	O
9.2.	O
mixtures	O
of	O
gaussians	O
437	O
2	O
0	O
−2	O
l	O
=	O
1	O
2	O
0	O
−2	O
−2	O
0	O
(	O
a	O
)	O
2	O
−2	O
0	O
(	O
b	O
)	O
2	O
−2	O
0	O
(	O
c	O
)	O
2	O
l	O
=	O
2	O
l	O
=	O
5	O
2	O
0	O
−2	O
l	O
=	O
20	O
2	O
0	O
−2	O
2	O
0	O
−2	O
2	O
0	O
−2	O
−2	O
0	O
(	O
d	O
)	O
2	O
−2	O
0	O
(	O
e	O
)	O
2	O
−2	O
0	O
(	O
f	O
)	O
2	O
figure	O
9.8	O
illustration	O
of	O
the	O
em	O
algorithm	O
using	O
the	O
old	O
faithful	O
set	O
as	O
used	O
for	O
the	O
illustration	O
of	O
the	O
k-means	O
algorithm	O
in	O
figure	O
9.1.	O
see	O
the	O
text	O
for	O
details	O
.	O
section	O
9.4	O
and	O
the	O
m	O
step	O
,	O
for	O
reasons	O
that	O
will	O
become	O
apparent	O
shortly	O
.	O
in	O
the	O
expectation	B
step	I
,	O
or	O
e	O
step	O
,	O
we	O
use	O
the	O
current	O
values	O
for	O
the	O
parameters	O
to	O
evaluate	O
the	O
posterior	O
probabilities	O
,	O
or	O
responsibilities	O
,	O
given	O
by	O
(	O
9.13	O
)	O
.	O
we	O
then	O
use	O
these	O
probabilities	O
in	O
the	O
maximization	B
step	I
,	O
or	O
m	O
step	O
,	O
to	O
re-estimate	O
the	O
means	O
,	O
covariances	O
,	O
and	O
mix-	O
ing	O
coefﬁcients	O
using	O
the	O
results	O
(	O
9.17	O
)	O
,	O
(	O
9.19	O
)	O
,	O
and	O
(	O
9.22	O
)	O
.	O
note	O
that	O
in	O
so	O
doing	O
we	O
ﬁrst	O
evaluate	O
the	O
new	O
means	O
using	O
(	O
9.17	O
)	O
and	O
then	O
use	O
these	O
new	O
values	O
to	O
ﬁnd	O
the	O
covariances	O
using	O
(	O
9.19	O
)	O
,	O
in	O
keeping	O
with	O
the	O
corresponding	O
result	O
for	O
a	O
single	O
gaussian	O
distribution	O
.	O
we	O
shall	O
show	O
that	O
each	O
update	O
to	O
the	O
parameters	O
resulting	O
from	O
an	O
e	O
step	O
followed	O
by	O
an	O
m	O
step	O
is	O
guaranteed	O
to	O
increase	O
the	O
log	O
likelihood	O
function	O
.	O
in	O
practice	O
,	O
the	O
algorithm	O
is	O
deemed	O
to	O
have	O
converged	O
when	O
the	O
change	O
in	O
the	O
log	O
likelihood	O
function	O
,	O
or	O
alternatively	O
in	O
the	O
parameters	O
,	O
falls	O
below	O
some	O
threshold	O
.	O
we	O
illustrate	O
the	O
em	O
algorithm	O
for	O
a	O
mixture	O
of	O
two	O
gaussians	O
applied	O
to	O
the	O
rescaled	O
old	O
faithful	O
data	O
set	O
in	O
figure	O
9.8.	O
here	O
a	O
mixture	O
of	O
two	O
gaussians	O
is	O
used	O
,	O
with	O
centres	O
initialized	O
using	O
the	O
same	O
values	O
as	O
for	O
the	O
k-means	O
algorithm	O
in	O
figure	O
9.1	O
,	O
and	O
with	O
precision	O
matrices	O
initialized	O
to	O
be	O
proportional	O
to	O
the	O
unit	O
matrix	O
.	O
plot	O
(	O
a	O
)	O
shows	O
the	O
data	O
points	O
in	O
green	O
,	O
together	O
with	O
the	O
initial	O
conﬁgura-	O
tion	O
of	O
the	O
mixture	B
model	I
in	O
which	O
the	O
one	O
standard-deviation	O
contours	O
for	O
the	O
two	O
438	O
9.	O
mixture	B
models	O
and	O
em	O
gaussian	O
components	O
are	O
shown	O
as	O
blue	O
and	O
red	O
circles	O
.	O
plot	O
(	O
b	O
)	O
shows	O
the	O
result	O
of	O
the	O
initial	O
e	O
step	O
,	O
in	O
which	O
each	O
data	O
point	O
is	O
depicted	O
using	O
a	O
proportion	O
of	O
blue	O
ink	O
equal	O
to	O
the	O
posterior	B
probability	I
of	O
having	O
been	O
generated	O
from	O
the	O
blue	O
com-	O
ponent	O
,	O
and	O
a	O
corresponding	O
proportion	O
of	O
red	O
ink	O
given	O
by	O
the	O
posterior	B
probability	I
of	O
having	O
been	O
generated	O
by	O
the	O
red	O
component	O
.	O
thus	O
,	O
points	O
that	O
have	O
a	O
signiﬁcant	O
probability	B
for	O
belonging	O
to	O
either	O
cluster	O
appear	O
purple	O
.	O
the	O
situation	O
after	O
the	O
ﬁrst	O
m	O
step	O
is	O
shown	O
in	O
plot	O
(	O
c	O
)	O
,	O
in	O
which	O
the	O
mean	B
of	O
the	O
blue	O
gaussian	O
has	O
moved	O
to	O
the	O
mean	B
of	O
the	O
data	O
set	O
,	O
weighted	O
by	O
the	O
probabilities	O
of	O
each	O
data	O
point	O
belonging	O
to	O
the	O
blue	O
cluster	O
,	O
in	O
other	O
words	O
it	O
has	O
moved	O
to	O
the	O
centre	O
of	O
mass	O
of	O
the	O
blue	O
ink	O
.	O
similarly	O
,	O
the	O
covariance	B
of	O
the	O
blue	O
gaussian	O
is	O
set	O
equal	O
to	O
the	O
covariance	B
of	O
the	O
blue	O
ink	O
.	O
analogous	O
results	O
hold	O
for	O
the	O
red	O
component	O
.	O
plots	O
(	O
d	O
)	O
,	O
(	O
e	O
)	O
,	O
and	O
(	O
f	O
)	O
show	O
the	O
results	O
after	O
2	O
,	O
5	O
,	O
and	O
20	O
complete	O
cycles	O
of	O
em	O
,	O
respectively	O
.	O
in	O
plot	O
(	O
f	O
)	O
the	O
algorithm	O
is	O
close	O
to	O
convergence	O
.	O
note	O
that	O
the	O
em	O
algorithm	O
takes	O
many	O
more	O
iterations	O
to	O
reach	O
(	O
approximate	O
)	O
convergence	O
compared	O
with	O
the	O
k-means	O
algorithm	O
,	O
and	O
that	O
each	O
cycle	O
requires	O
signiﬁcantly	O
more	O
computation	O
.	O
it	O
is	O
therefore	O
common	O
to	O
run	O
the	O
k-means	O
algo-	O
rithm	O
in	O
order	O
to	O
ﬁnd	O
a	O
suitable	O
initialization	O
for	O
a	O
gaussian	O
mixture	B
model	I
that	O
is	O
subsequently	O
adapted	O
using	O
em	O
.	O
the	O
covariance	B
matrices	O
can	O
conveniently	O
be	O
ini-	O
tialized	O
to	O
the	O
sample	O
covariances	O
of	O
the	O
clusters	O
found	O
by	O
the	O
k-means	O
algorithm	O
,	O
and	O
the	O
mixing	O
coefﬁcients	O
can	O
be	O
set	O
to	O
the	O
fractions	O
of	O
data	O
points	O
assigned	O
to	O
the	O
respective	O
clusters	O
.	O
as	O
with	O
gradient-based	O
approaches	O
for	O
maximizing	O
the	O
log	O
like-	O
lihood	O
,	O
techniques	O
must	O
be	O
employed	O
to	O
avoid	O
singularities	B
of	O
the	O
likelihood	B
function	I
in	O
which	O
a	O
gaussian	O
component	O
collapses	O
onto	O
a	O
particular	O
data	O
point	O
.	O
it	O
should	O
be	O
emphasized	O
that	O
there	O
will	O
generally	O
be	O
multiple	O
local	B
maxima	O
of	O
the	O
log	O
likelihood	O
function	O
,	O
and	O
that	O
em	O
is	O
not	O
guaranteed	O
to	O
ﬁnd	O
the	O
largest	O
of	O
these	O
maxima	O
.	O
because	O
the	O
em	O
algorithm	O
for	O
gaussian	O
mixtures	O
plays	O
such	O
an	O
important	O
role	O
,	O
we	O
summarize	O
it	O
below	O
.	O
em	O
for	O
gaussian	O
mixtures	O
given	O
a	O
gaussian	O
mixture	B
model	I
,	O
the	O
goal	O
is	O
to	O
maximize	O
the	O
likelihood	B
function	I
with	O
respect	O
to	O
the	O
parameters	O
(	O
comprising	O
the	O
means	O
and	O
covariances	O
of	O
the	O
components	O
and	O
the	O
mixing	O
coefﬁcients	O
)	O
.	O
1.	O
initialize	O
the	O
means	O
µk	O
,	O
covariances	O
σk	O
and	O
mixing	O
coefﬁcients	O
πk	O
,	O
and	O
evaluate	O
the	O
initial	O
value	O
of	O
the	O
log	O
likelihood	O
.	O
2.	O
e	O
step	O
.	O
evaluate	O
the	O
responsibilities	O
using	O
the	O
current	O
parameter	O
values	O
γ	O
(	O
znk	O
)	O
=	O
πkn	O
(	O
xn|µk	O
,	O
σk	O
)	O
πjn	O
(	O
xn|µj	O
,	O
σj	O
)	O
.	O
(	O
9.23	O
)	O
k	O
(	O
cid:2	O
)	O
j=1	O
9.3.	O
an	O
alternative	O
view	O
of	O
em	O
439	O
3.	O
m	O
step	O
.	O
re-estimate	O
the	O
parameters	O
using	O
the	O
current	O
responsibilities	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
n=1	O
µnew	O
k	O
=	O
1	O
nk	O
σnew	O
k	O
πnew	O
k	O
=	O
1	O
nk	O
=	O
nk	O
n	O
γ	O
(	O
znk	O
)	O
xn	O
γ	O
(	O
znk	O
)	O
(	O
xn	O
−	O
µnew	O
k	O
)	O
(	O
xn	O
−	O
µnew	O
k	O
)	O
t	O
(	O
9.24	O
)	O
(	O
9.25	O
)	O
(	O
9.26	O
)	O
(	O
9.27	O
)	O
(	O
9.28	O
)	O
where	O
4.	O
evaluate	O
the	O
log	O
likelihood	O
ln	O
p	O
(	O
x|µ	O
,	O
σ	O
,	O
π	O
)	O
=	O
nk	O
=	O
n	O
(	O
cid:2	O
)	O
ln	O
n	O
(	O
cid:2	O
)	O
(	O
cid:24	O
)	O
n=1	O
k	O
(	O
cid:2	O
)	O
γ	O
(	O
znk	O
)	O
.	O
(	O
cid:25	O
)	O
πkn	O
(	O
xn|µk	O
,	O
σk	O
)	O
n=1	O
k=1	O
and	O
check	O
for	O
convergence	O
of	O
either	O
the	O
parameters	O
or	O
the	O
log	O
likelihood	O
.	O
if	O
the	O
convergence	O
criterion	O
is	O
not	O
satisﬁed	O
return	O
to	O
step	O
2	O
.	O
9.3.	O
an	O
alternative	O
view	O
of	O
em	O
in	O
this	O
section	O
,	O
we	O
present	O
a	O
complementary	O
view	O
of	O
the	O
em	O
algorithm	O
that	O
recog-	O
nizes	O
the	O
key	O
role	O
played	O
by	O
latent	O
variables	O
.	O
we	O
discuss	O
this	O
approach	O
ﬁrst	O
of	O
all	O
in	O
an	O
abstract	O
setting	O
,	O
and	O
then	O
for	O
illustration	O
we	O
consider	O
once	O
again	O
the	O
case	O
of	O
gaussian	O
mixtures	O
.	O
the	O
goal	O
of	O
the	O
em	O
algorithm	O
is	O
to	O
ﬁnd	O
maximum	B
likelihood	I
solutions	O
for	O
mod-	O
els	O
having	O
latent	O
variables	O
.	O
we	O
denote	O
the	O
set	O
of	O
all	O
observed	O
data	O
by	O
x	O
,	O
in	O
which	O
the	O
nth	O
row	O
represents	O
xt	O
n	O
,	O
and	O
similarly	O
we	O
denote	O
the	O
set	O
of	O
all	O
latent	O
variables	O
by	O
z	O
,	O
with	O
a	O
corresponding	O
row	O
zt	O
n.	O
the	O
set	O
of	O
all	O
model	O
parameters	O
is	O
denoted	O
by	O
θ	O
,	O
and	O
so	O
the	O
log	O
likelihood	O
function	O
is	O
given	O
by	O
(	O
cid:24	O
)	O
(	O
cid:2	O
)	O
(	O
cid:25	O
)	O
ln	O
p	O
(	O
x|θ	O
)	O
=	O
ln	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
.	O
(	O
9.29	O
)	O
z	O
note	O
that	O
our	O
discussion	O
will	O
apply	O
equally	O
well	O
to	O
continuous	O
latent	O
variables	O
simply	O
by	O
replacing	O
the	O
sum	O
over	O
z	O
with	O
an	O
integral	O
.	O
a	O
key	O
observation	O
is	O
that	O
the	O
summation	O
over	O
the	O
latent	O
variables	O
appears	O
inside	O
the	O
logarithm	O
.	O
even	O
if	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
belongs	O
to	O
the	O
exponential	O
440	O
9.	O
mixture	B
models	O
and	O
em	O
family	O
,	O
the	O
marginal	B
distribution	O
p	O
(	O
x|θ	O
)	O
typically	O
does	O
not	O
as	O
a	O
result	O
of	O
this	O
sum-	O
mation	B
.	O
the	O
presence	O
of	O
the	O
sum	O
prevents	O
the	O
logarithm	O
from	O
acting	O
directly	O
on	O
the	O
joint	O
distribution	O
,	O
resulting	O
in	O
complicated	O
expressions	O
for	O
the	O
maximum	B
likelihood	I
solution	O
.	O
now	O
suppose	O
that	O
,	O
for	O
each	O
observation	O
in	O
x	O
,	O
we	O
were	O
told	O
the	O
corresponding	O
value	O
of	O
the	O
latent	B
variable	I
z.	O
we	O
shall	O
call	O
{	O
x	O
,	O
z	O
}	O
the	O
complete	B
data	I
set	I
,	O
and	O
we	O
shall	O
refer	O
to	O
the	O
actual	O
observed	O
data	O
x	O
as	O
incomplete	O
,	O
as	O
illustrated	O
in	O
figure	O
9.5.	O
the	O
likelihood	B
function	I
for	O
the	O
complete	B
data	I
set	I
simply	O
takes	O
the	O
form	O
ln	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
,	O
and	O
we	O
shall	O
suppose	O
that	O
maximization	O
of	O
this	O
complete-data	O
log	O
likelihood	O
function	O
is	O
straightforward	O
.	O
in	O
practice	O
,	O
however	O
,	O
we	O
are	O
not	O
given	O
the	O
complete	B
data	I
set	I
{	O
x	O
,	O
z	O
}	O
,	O
but	O
only	O
the	O
incomplete	O
data	O
x.	O
our	O
state	O
of	O
knowledge	O
of	O
the	O
values	O
of	O
the	O
latent	O
variables	O
in	O
z	O
is	O
given	O
only	O
by	O
the	O
posterior	O
distribution	O
p	O
(	O
z|x	O
,	O
θ	O
)	O
.	O
because	O
we	O
can	O
not	O
use	O
the	O
complete-data	O
log	O
likelihood	O
,	O
we	O
consider	O
instead	O
its	O
expected	O
value	O
under	O
the	O
posterior	O
distribution	O
of	O
the	O
latent	B
variable	I
,	O
which	O
corresponds	O
(	O
as	O
we	O
shall	O
see	O
)	O
to	O
the	O
e	O
step	O
of	O
the	O
em	O
algorithm	O
.	O
in	O
the	O
subsequent	O
m	O
step	O
,	O
we	O
maximize	O
this	O
expectation	B
.	O
if	O
the	O
current	O
estimate	O
for	O
the	O
parameters	O
is	O
denoted	O
θold	O
,	O
then	O
a	O
pair	O
of	O
successive	O
e	O
and	O
m	O
steps	O
gives	O
rise	O
to	O
a	O
revised	O
estimate	O
θnew	O
.	O
the	O
algorithm	O
is	O
initialized	O
by	O
choosing	O
some	O
starting	O
value	O
for	O
the	O
parameters	O
θ0	O
.	O
the	O
use	O
of	O
the	O
expectation	B
may	O
seem	O
somewhat	O
arbitrary	O
.	O
however	O
,	O
we	O
shall	O
see	O
the	O
motivation	O
for	O
this	O
choice	O
when	O
we	O
give	O
a	O
deeper	O
treatment	O
of	O
em	O
in	O
section	O
9.4.	O
in	O
the	O
e	O
step	O
,	O
we	O
use	O
the	O
current	O
parameter	O
values	O
θold	O
to	O
ﬁnd	O
the	O
posterior	O
distribution	O
of	O
the	O
latent	O
variables	O
given	O
by	O
p	O
(	O
z|x	O
,	O
θold	O
)	O
.	O
we	O
then	O
use	O
this	O
posterior	O
distribution	O
to	O
ﬁnd	O
the	O
expectation	B
of	O
the	O
complete-data	O
log	O
likelihood	O
evaluated	O
for	O
some	O
general	O
parameter	O
value	O
θ.	O
this	O
expectation	B
,	O
denoted	O
q	O
(	O
θ	O
,	O
θold	O
)	O
,	O
is	O
given	O
by	O
q	O
(	O
θ	O
,	O
θold	O
)	O
=	O
p	O
(	O
z|x	O
,	O
θold	O
)	O
ln	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
.	O
(	O
9.30	O
)	O
(	O
cid:2	O
)	O
z	O
in	O
the	O
m	O
step	O
,	O
we	O
determine	O
the	O
revised	O
parameter	O
estimate	O
θnew	O
by	O
maximizing	O
this	O
function	O
θnew	O
=	O
arg	O
max	O
(	O
9.31	O
)	O
note	O
that	O
in	O
the	O
deﬁnition	O
of	O
q	O
(	O
θ	O
,	O
θold	O
)	O
,	O
the	O
logarithm	O
acts	O
directly	O
on	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
,	O
and	O
so	O
the	O
corresponding	O
m-step	O
maximization	O
will	O
,	O
by	O
sup-	O
position	O
,	O
be	O
tractable	O
.	O
q	O
(	O
θ	O
,	O
θold	O
)	O
.	O
θ	O
section	O
9.4	O
the	O
general	O
em	O
algorithm	O
is	O
summarized	O
below	O
.	O
it	O
has	O
the	O
property	O
,	O
as	O
we	O
shall	O
show	O
later	O
,	O
that	O
each	O
cycle	O
of	O
em	O
will	O
increase	O
the	O
incomplete-data	O
log	O
likelihood	O
(	O
unless	O
it	O
is	O
already	O
at	O
a	O
local	B
maximum	O
)	O
.	O
the	O
general	O
em	O
algorithm	O
given	O
a	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
over	O
observed	O
variables	O
x	O
and	O
latent	O
vari-	O
ables	O
z	O
,	O
governed	O
by	O
parameters	O
θ	O
,	O
the	O
goal	O
is	O
to	O
maximize	O
the	O
likelihood	O
func-	O
tion	O
p	O
(	O
x|θ	O
)	O
with	O
respect	O
to	O
θ	O
.	O
1.	O
choose	O
an	O
initial	O
setting	O
for	O
the	O
parameters	O
θold	O
.	O
9.3.	O
an	O
alternative	O
view	O
of	O
em	O
441	O
2.	O
e	O
step	O
evaluate	O
p	O
(	O
z|x	O
,	O
θold	O
)	O
.	O
3.	O
m	O
step	O
evaluate	O
θnew	O
given	O
by	O
q	O
(	O
θ	O
,	O
θold	O
)	O
(	O
9.32	O
)	O
(	O
9.33	O
)	O
θnew	O
=	O
arg	O
max	O
θ	O
(	O
cid:2	O
)	O
where	O
q	O
(	O
θ	O
,	O
θold	O
)	O
=	O
p	O
(	O
z|x	O
,	O
θold	O
)	O
ln	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
.	O
exercise	O
9.4	O
z	O
4.	O
check	O
for	O
convergence	O
of	O
either	O
the	O
log	O
likelihood	O
or	O
the	O
parameter	O
values	O
.	O
if	O
the	O
convergence	O
criterion	O
is	O
not	O
satisﬁed	O
,	O
then	O
let	O
θold	O
←	O
θnew	O
(	O
9.34	O
)	O
and	O
return	O
to	O
step	O
2.	O
the	O
em	O
algorithm	O
can	O
also	O
be	O
used	O
to	O
ﬁnd	O
map	O
(	O
maximum	B
posterior	I
)	O
solutions	O
for	O
models	O
in	O
which	O
a	O
prior	B
p	O
(	O
θ	O
)	O
is	O
deﬁned	O
over	O
the	O
parameters	O
.	O
in	O
this	O
case	O
the	O
e	O
step	O
remains	O
the	O
same	O
as	O
in	O
the	O
maximum	B
likelihood	I
case	O
,	O
whereas	O
in	O
the	O
m	O
step	O
the	O
quantity	O
to	O
be	O
maximized	O
is	O
given	O
by	O
q	O
(	O
θ	O
,	O
θold	O
)	O
+	O
ln	O
p	O
(	O
θ	O
)	O
.	O
suitable	O
choices	O
for	O
the	O
prior	B
will	O
remove	O
the	O
singularities	B
of	O
the	O
kind	O
illustrated	O
in	O
figure	O
9.7.	O
here	O
we	O
have	O
considered	O
the	O
use	O
of	O
the	O
em	O
algorithm	O
to	O
maximize	O
a	O
likelihood	B
function	I
when	O
there	O
are	O
discrete	O
latent	O
variables	O
.	O
however	O
,	O
it	O
can	O
also	O
be	O
applied	O
when	O
the	O
unobserved	O
variables	O
correspond	O
to	O
missing	O
values	O
in	O
the	O
data	O
set	O
.	O
the	O
distribution	O
of	O
the	O
observed	O
values	O
is	O
obtained	O
by	O
taking	O
the	O
joint	O
distribution	O
of	O
all	O
the	O
variables	O
and	O
then	O
marginalizing	O
over	O
the	O
missing	O
ones	O
.	O
em	O
can	O
then	O
be	O
used	O
to	O
maximize	O
the	O
corresponding	O
likelihood	B
function	I
.	O
we	O
shall	O
show	O
an	O
example	O
of	O
the	O
application	O
of	O
this	O
technique	O
in	O
the	O
context	O
of	O
principal	B
component	I
analysis	I
in	O
figure	O
12.11.	O
this	O
will	O
be	O
a	O
valid	O
procedure	O
if	O
the	O
data	O
values	O
are	O
missing	B
at	I
random	I
,	O
meaning	O
that	O
the	O
mechanism	O
causing	O
values	O
to	O
be	O
missing	O
does	O
not	O
depend	O
on	O
the	O
unobserved	O
values	O
.	O
in	O
many	O
situations	O
this	O
will	O
not	O
be	O
the	O
case	O
,	O
for	O
instance	O
if	O
a	O
sensor	O
fails	O
to	O
return	O
a	O
value	O
whenever	O
the	O
quantity	O
it	O
is	O
measuring	O
exceeds	O
some	O
threshold	O
.	O
9.3.1	O
gaussian	O
mixtures	O
revisited	O
we	O
now	O
consider	O
the	O
application	O
of	O
this	O
latent	B
variable	I
view	O
of	O
em	O
to	O
the	O
spe-	O
ciﬁc	O
case	O
of	O
a	O
gaussian	O
mixture	B
model	I
.	O
recall	O
that	O
our	O
goal	O
is	O
to	O
maximize	O
the	O
log	O
likelihood	O
function	O
(	O
9.14	O
)	O
,	O
which	O
is	O
computed	O
using	O
the	O
observed	O
data	O
set	O
x	O
,	O
and	O
we	O
saw	O
that	O
this	O
was	O
more	O
difﬁcult	O
than	O
for	O
the	O
case	O
of	O
a	O
single	O
gaussian	O
distribution	O
due	O
to	O
the	O
presence	O
of	O
the	O
summation	O
over	O
k	O
that	O
occurs	O
inside	O
the	O
logarithm	O
.	O
sup-	O
pose	O
then	O
that	O
in	O
addition	O
to	O
the	O
observed	O
data	O
set	O
x	O
,	O
we	O
were	O
also	O
given	O
the	O
values	O
of	O
the	O
corresponding	O
discrete	O
variables	O
z.	O
recall	O
that	O
figure	O
9.5	O
(	O
a	O
)	O
shows	O
a	O
‘	O
com-	O
plete	O
’	O
data	O
set	O
(	O
i.e.	O
,	O
one	O
that	O
includes	O
labels	O
showing	O
which	O
component	O
generated	O
each	O
data	O
point	O
)	O
while	O
figure	O
9.5	O
(	O
b	O
)	O
shows	O
the	O
corresponding	O
‘	O
incomplete	O
’	O
data	O
set	O
.	O
the	O
graphical	B
model	I
for	O
the	O
complete	O
data	O
is	O
shown	O
in	O
figure	O
9.9	O
.	O
442	O
9.	O
mixture	B
models	O
and	O
em	O
figure	O
9.9	O
this	O
shows	O
the	O
same	O
graph	O
as	O
in	O
figure	O
9.6	O
except	O
that	O
we	O
now	O
suppose	O
that	O
the	O
discrete	O
variables	O
zn	O
are	O
ob-	O
served	O
,	O
as	O
well	O
as	O
the	O
data	O
variables	O
xn	O
.	O
zn	O
xn	O
π	O
µ	O
σ	O
n	O
now	O
consider	O
the	O
problem	O
of	O
maximizing	O
the	O
likelihood	O
for	O
the	O
complete	B
data	I
set	I
{	O
x	O
,	O
z	O
}	O
.	O
from	O
(	O
9.10	O
)	O
and	O
(	O
9.11	O
)	O
,	O
this	O
likelihood	B
function	I
takes	O
the	O
form	O
p	O
(	O
x	O
,	O
z|µ	O
,	O
σ	O
,	O
π	O
)	O
=	O
k	O
n	O
(	O
xn|µk	O
,	O
σk	O
)	O
znk	O
πznk	O
(	O
9.35	O
)	O
where	O
znk	O
denotes	O
the	O
kth	O
component	O
of	O
zn	O
.	O
taking	O
the	O
logarithm	O
,	O
we	O
obtain	O
ln	O
p	O
(	O
x	O
,	O
z|µ	O
,	O
σ	O
,	O
π	O
)	O
=	O
znk	O
{	O
ln	O
πk	O
+	O
lnn	O
(	O
xn|µk	O
,	O
σk	O
)	O
}	O
.	O
(	O
9.36	O
)	O
n	O
(	O
cid:14	O
)	O
k	O
(	O
cid:14	O
)	O
n=1	O
k=1	O
n	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
n=1	O
k=1	O
comparison	O
with	O
the	O
log	O
likelihood	O
function	O
(	O
9.14	O
)	O
for	O
the	O
incomplete	O
data	O
shows	O
that	O
the	O
summation	O
over	O
k	O
and	O
the	O
logarithm	O
have	O
been	O
interchanged	O
.	O
the	O
loga-	O
rithm	O
now	O
acts	O
directly	O
on	O
the	O
gaussian	O
distribution	O
,	O
which	O
itself	O
is	O
a	O
member	O
of	O
the	O
exponential	B
family	I
.	O
not	O
surprisingly	O
,	O
this	O
leads	O
to	O
a	O
much	O
simpler	O
solution	O
to	O
the	O
maximum	B
likelihood	I
problem	O
,	O
as	O
we	O
now	O
show	O
.	O
consider	O
ﬁrst	O
the	O
maximization	O
with	O
respect	O
to	O
the	O
means	O
and	O
covariances	O
.	O
because	O
zn	O
is	O
a	O
k-dimensional	O
vec-	O
tor	O
with	O
all	O
elements	O
equal	O
to	O
0	O
except	O
for	O
a	O
single	O
element	O
having	O
the	O
value	O
1	O
,	O
the	O
complete-data	O
log	O
likelihood	O
function	O
is	O
simply	O
a	O
sum	O
of	O
k	O
independent	B
contribu-	O
tions	O
,	O
one	O
for	O
each	O
mixture	B
component	I
.	O
thus	O
the	O
maximization	O
with	O
respect	O
to	O
a	O
mean	B
or	O
a	O
covariance	B
is	O
exactly	O
as	O
for	O
a	O
single	O
gaussian	O
,	O
except	O
that	O
it	O
involves	O
only	O
the	O
subset	O
of	O
data	O
points	O
that	O
are	O
‘	O
assigned	O
’	O
to	O
that	O
component	O
.	O
for	O
the	O
maximization	O
with	O
respect	O
to	O
the	O
mixing	O
coefﬁcients	O
,	O
we	O
note	O
that	O
these	O
are	O
coupled	O
for	O
different	O
values	O
of	O
k	O
by	O
virtue	O
of	O
the	O
summation	O
constraint	O
(	O
9.9	O
)	O
.	O
again	O
,	O
this	O
can	O
be	O
enforced	O
using	O
a	O
lagrange	O
multiplier	O
as	O
before	O
,	O
and	O
leads	O
to	O
the	O
result	O
n	O
(	O
cid:2	O
)	O
πk	O
=	O
1	O
n	O
znk	O
n=1	O
(	O
9.37	O
)	O
so	O
that	O
the	O
mixing	O
coefﬁcients	O
are	O
equal	O
to	O
the	O
fractions	O
of	O
data	O
points	O
assigned	O
to	O
the	O
corresponding	O
components	O
.	O
thus	O
we	O
see	O
that	O
the	O
complete-data	O
log	O
likelihood	O
function	O
can	O
be	O
maximized	O
trivially	O
in	O
closed	O
form	O
.	O
in	O
practice	O
,	O
however	O
,	O
we	O
do	O
not	O
have	O
values	O
for	O
the	O
latent	O
variables	O
so	O
,	O
as	O
discussed	O
earlier	O
,	O
we	O
consider	O
the	O
expectation	B
,	O
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
of	O
the	O
latent	O
variables	O
,	O
of	O
the	O
complete-data	O
log	O
likelihood	O
.	O
exercise	O
9.5	O
section	O
8.2	O
exercise	O
9.8	O
9.3.	O
an	O
alternative	O
view	O
of	O
em	O
443	O
using	O
(	O
9.10	O
)	O
and	O
(	O
9.11	O
)	O
together	O
with	O
bayes	O
’	O
theorem	O
,	O
we	O
see	O
that	O
this	O
posterior	O
distribution	O
takes	O
the	O
form	O
p	O
(	O
z|x	O
,	O
µ	O
,	O
σ	O
,	O
π	O
)	O
∝	O
n	O
(	O
cid:14	O
)	O
k	O
(	O
cid:14	O
)	O
[	O
πkn	O
(	O
xn|µk	O
,	O
σk	O
)	O
]	O
znk	O
.	O
(	O
9.38	O
)	O
n=1	O
k=1	O
and	O
hence	O
factorizes	O
over	O
n	O
so	O
that	O
under	O
the	O
posterior	O
distribution	O
the	O
{	O
zn	O
}	O
are	O
independent	B
.	O
this	O
is	O
easily	O
veriﬁed	O
by	O
inspection	O
of	O
the	O
directed	B
graph	O
in	O
figure	O
9.6	O
(	O
cid:2	O
)	O
and	O
making	O
use	O
of	O
the	O
d-separation	B
criterion	O
.	O
the	O
expected	O
value	O
of	O
the	O
indicator	O
variable	O
znk	O
under	O
this	O
posterior	O
distribution	O
is	O
then	O
given	O
by	O
(	O
cid:2	O
)	O
(	O
cid:8	O
)	O
znk	O
[	O
πkn	O
(	O
xn|µk	O
,	O
σk	O
)	O
]	O
znk	O
πjn	O
(	O
xn|µj	O
,	O
σj	O
)	O
k	O
(	O
cid:2	O
)	O
πkn	O
(	O
xn|µk	O
,	O
σk	O
)	O
πjn	O
(	O
xn|µj	O
,	O
σj	O
)	O
(	O
cid:9	O
)	O
znj	O
e	O
[	O
znk	O
]	O
=	O
=	O
γ	O
(	O
znk	O
)	O
(	O
9.39	O
)	O
znk	O
znj	O
=	O
which	O
is	O
just	O
the	O
responsibility	B
of	O
component	O
k	O
for	O
data	O
point	O
xn	O
.	O
the	O
expected	O
value	O
of	O
the	O
complete-data	O
log	O
likelihood	O
function	O
is	O
therefore	O
given	O
by	O
j=1	O
n	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
ez	O
[	O
ln	O
p	O
(	O
x	O
,	O
z|µ	O
,	O
σ	O
,	O
π	O
)	O
]	O
=	O
γ	O
(	O
znk	O
)	O
{	O
ln	O
πk	O
+	O
lnn	O
(	O
xn|µk	O
,	O
σk	O
)	O
}	O
.	O
(	O
9.40	O
)	O
n=1	O
k=1	O
we	O
can	O
now	O
proceed	O
as	O
follows	O
.	O
first	O
we	O
choose	O
some	O
initial	O
values	O
for	O
the	O
param-	O
eters	O
µold	O
,	O
σold	O
and	O
πold	O
,	O
and	O
use	O
these	O
to	O
evaluate	O
the	O
responsibilities	O
(	O
the	O
e	O
step	O
)	O
.	O
we	O
then	O
keep	O
the	O
responsibilities	O
ﬁxed	O
and	O
maximize	O
(	O
9.40	O
)	O
with	O
respect	O
to	O
µk	O
,	O
σk	O
and	O
πk	O
(	O
the	O
m	O
step	O
)	O
.	O
this	O
leads	O
to	O
closed	O
form	O
solutions	O
for	O
µnew	O
,	O
σnew	O
and	O
πnew	O
given	O
by	O
(	O
9.17	O
)	O
,	O
(	O
9.19	O
)	O
,	O
and	O
(	O
9.22	O
)	O
as	O
before	O
.	O
this	O
is	O
precisely	O
the	O
em	O
algorithm	O
for	O
gaussian	O
mixtures	O
as	O
derived	O
earlier	O
.	O
we	O
shall	O
gain	O
more	O
insight	O
into	O
the	O
role	O
of	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
function	O
when	O
we	O
give	O
a	O
proof	O
of	O
convergence	O
of	O
the	O
em	O
algorithm	O
in	O
section	O
9.4	O
.	O
9.3.2	O
relation	O
to	O
k-means	O
comparison	O
of	O
the	O
k-means	O
algorithm	O
with	O
the	O
em	O
algorithm	O
for	O
gaussian	O
mixtures	O
shows	O
that	O
there	O
is	O
a	O
close	O
similarity	O
.	O
whereas	O
the	O
k-means	O
algorithm	O
performs	O
a	O
hard	O
assignment	O
of	O
data	O
points	O
to	O
clusters	O
,	O
in	O
which	O
each	O
data	O
point	O
is	O
associated	O
uniquely	O
with	O
one	O
cluster	O
,	O
the	O
em	O
algorithm	O
makes	O
a	O
soft	B
assignment	O
based	O
on	O
the	O
posterior	O
probabilities	O
.	O
in	O
fact	O
,	O
we	O
can	O
derive	O
the	O
k-means	O
algorithm	O
as	O
a	O
particular	O
limit	O
of	O
em	O
for	O
gaussian	O
mixtures	O
as	O
follows	O
.	O
consider	O
a	O
gaussian	O
mixture	B
model	I
in	O
which	O
the	O
covariance	B
matrices	O
of	O
the	O
mixture	B
components	O
are	O
given	O
by	O
i	O
,	O
where	O
	O
is	O
a	O
variance	B
parameter	O
that	O
is	O
shared	O
444	O
9.	O
mixture	B
models	O
and	O
em	O
by	O
all	O
of	O
the	O
components	O
,	O
and	O
i	O
is	O
the	O
identity	O
matrix	O
,	O
so	O
that	O
p	O
(	O
x|µk	O
,	O
σk	O
)	O
=	O
1	O
(	O
2π	O
)	O
1/2	O
exp	O
−	O
1	O
2	O
(	O
cid:5	O
)	O
x	O
−	O
µk	O
(	O
cid:5	O
)	O
2	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
.	O
(	O
9.41	O
)	O
we	O
now	O
consider	O
the	O
em	O
algorithm	O
for	O
a	O
mixture	O
of	O
k	O
gaussians	O
of	O
this	O
form	O
in	O
which	O
we	O
treat	O
	O
as	O
a	O
ﬁxed	O
constant	O
,	O
instead	O
of	O
a	O
parameter	O
to	O
be	O
re-estimated	O
.	O
from	O
(	O
9.13	O
)	O
the	O
posterior	O
probabilities	O
,	O
or	O
responsibilities	O
,	O
for	O
a	O
particular	O
data	O
point	O
xn	O
,	O
are	O
given	O
by	O
(	O
cid:26	O
)	O
−	O
(	O
cid:5	O
)	O
xn	O
−	O
µj	O
(	O
cid:5	O
)	O
2/2	O
(	O
cid:27	O
)	O
.	O
γ	O
(	O
znk	O
)	O
=	O
πk	O
exp	O
{	O
−	O
(	O
cid:5	O
)	O
xn	O
−	O
µk	O
(	O
cid:5	O
)	O
2/2	O
}	O
(	O
cid:5	O
)	O
j	O
πj	O
exp	O
(	O
9.42	O
)	O
if	O
we	O
consider	O
the	O
limit	O
	O
→	O
0	O
,	O
we	O
see	O
that	O
in	O
the	O
denominator	O
the	O
term	O
for	O
which	O
(	O
cid:5	O
)	O
xn	O
−	O
µj	O
(	O
cid:5	O
)	O
2	O
is	O
smallest	O
will	O
go	O
to	O
zero	O
most	O
slowly	O
,	O
and	O
hence	O
the	O
responsibilities	O
γ	O
(	O
znk	O
)	O
for	O
the	O
data	O
point	O
xn	O
all	O
go	O
to	O
zero	O
except	O
for	O
term	O
j	O
,	O
for	O
which	O
the	O
responsi-	O
bility	O
γ	O
(	O
znj	O
)	O
will	O
go	O
to	O
unity	O
.	O
note	O
that	O
this	O
holds	O
independently	O
of	O
the	O
values	O
of	O
the	O
πk	O
so	O
long	O
as	O
none	O
of	O
the	O
πk	O
is	O
zero	O
.	O
thus	O
,	O
in	O
this	O
limit	O
,	O
we	O
obtain	O
a	O
hard	O
assignment	O
of	O
data	O
points	O
to	O
clusters	O
,	O
just	O
as	O
in	O
the	O
k-means	O
algorithm	O
,	O
so	O
that	O
γ	O
(	O
znk	O
)	O
→	O
rnk	O
where	O
rnk	O
is	O
deﬁned	O
by	O
(	O
9.2	O
)	O
.	O
each	O
data	O
point	O
is	O
thereby	O
assigned	O
to	O
the	O
cluster	O
having	O
the	O
closest	O
mean	B
.	O
the	O
em	O
re-estimation	O
equation	O
for	O
the	O
µk	O
,	O
given	O
by	O
(	O
9.17	O
)	O
,	O
then	O
reduces	O
to	O
the	O
k-means	O
result	O
(	O
9.4	O
)	O
.	O
note	O
that	O
the	O
re-estimation	O
formula	O
for	O
the	O
mixing	O
coefﬁcients	O
(	O
9.22	O
)	O
simply	O
re-sets	O
the	O
value	O
of	O
πk	O
to	O
be	O
equal	O
to	O
the	O
fraction	O
of	O
data	O
points	O
assigned	O
to	O
cluster	O
k	O
,	O
although	O
these	O
parameters	O
no	O
longer	O
play	O
an	O
active	O
role	O
in	O
the	O
algorithm	O
.	O
finally	O
,	O
in	O
the	O
limit	O
	O
→	O
0	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
,	O
given	O
by	O
exercise	O
9.11	O
(	O
9.40	O
)	O
,	O
becomes	O
ez	O
[	O
ln	O
p	O
(	O
x	O
,	O
z|µ	O
,	O
σ	O
,	O
π	O
)	O
]	O
→	O
−1	O
2	O
n	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
n=1	O
k=1	O
rnk	O
(	O
cid:5	O
)	O
xn	O
−	O
µk	O
(	O
cid:5	O
)	O
2	O
+	O
const	O
.	O
(	O
9.43	O
)	O
thus	O
we	O
see	O
that	O
in	O
this	O
limit	O
,	O
maximizing	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
is	O
equivalent	O
to	O
minimizing	O
the	O
distortion	B
measure	I
j	O
for	O
the	O
k-means	O
algorithm	O
given	O
by	O
(	O
9.1	O
)	O
.	O
note	O
that	O
the	O
k-means	O
algorithm	O
does	O
not	O
estimate	O
the	O
covariances	O
of	O
the	O
clus-	O
ters	O
but	O
only	O
the	O
cluster	O
means	O
.	O
a	O
hard-assignment	O
version	O
of	O
the	O
gaussian	O
mixture	B
model	I
with	O
general	O
covariance	B
matrices	O
,	O
known	O
as	O
the	O
elliptical	O
k-means	O
algorithm	O
,	O
has	O
been	O
considered	O
by	O
sung	O
and	O
poggio	O
(	O
1994	O
)	O
.	O
9.3.3	O
mixtures	O
of	O
bernoulli	O
distributions	O
so	O
far	O
in	O
this	O
chapter	O
,	O
we	O
have	O
focussed	O
on	O
distributions	O
over	O
continuous	O
vari-	O
ables	O
described	O
by	O
mixtures	O
of	O
gaussians	O
.	O
as	O
a	O
further	O
example	O
of	O
mixture	B
mod-	O
elling	O
,	O
and	O
to	O
illustrate	O
the	O
em	O
algorithm	O
in	O
a	O
different	O
context	O
,	O
we	O
now	O
discuss	O
mix-	O
tures	O
of	O
discrete	O
binary	O
variables	O
described	O
by	O
bernoulli	O
distributions	O
.	O
this	O
model	O
is	O
also	O
known	O
as	O
latent	B
class	I
analysis	I
(	O
lazarsfeld	O
and	O
henry	O
,	O
1968	O
;	O
mclachlan	O
and	O
peel	O
,	O
2000	O
)	O
.	O
as	O
well	O
as	O
being	O
of	O
practical	O
importance	O
in	O
its	O
own	O
right	O
,	O
our	O
discus-	O
sion	B
of	O
bernoulli	O
mixtures	O
will	O
also	O
lay	O
the	O
foundation	O
for	O
a	O
consideration	O
of	O
hidden	O
markov	O
models	O
over	O
discrete	O
variables	O
.	O
section	O
13.2	O
9.3.	O
an	O
alternative	O
view	O
of	O
em	O
445	O
d	O
(	O
cid:14	O
)	O
consider	O
a	O
set	O
of	O
d	O
binary	O
variables	O
xi	O
,	O
where	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
,	O
each	O
of	O
which	O
is	O
governed	O
by	O
a	O
bernoulli	O
distribution	O
with	O
parameter	O
µi	O
,	O
so	O
that	O
p	O
(	O
x|µ	O
)	O
=	O
i	O
(	O
1	O
−	O
µi	O
)	O
(	O
1−xi	O
)	O
µxi	O
(	O
9.44	O
)	O
i=1	O
where	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
t	O
and	O
µ	O
=	O
(	O
µ1	O
,	O
.	O
.	O
.	O
,	O
µd	O
)	O
t.	O
we	O
see	O
that	O
the	O
individual	O
variables	O
xi	O
are	O
independent	B
,	O
given	O
µ.	O
the	O
mean	B
and	O
covariance	B
of	O
this	O
distribution	O
are	O
easily	O
seen	O
to	O
be	O
e	O
[	O
x	O
]	O
=	O
µ	O
cov	O
[	O
x	O
]	O
=	O
diag	O
{	O
µi	O
(	O
1	O
−	O
µi	O
)	O
}	O
.	O
now	O
let	O
us	O
consider	O
a	O
ﬁnite	O
mixture	O
of	O
these	O
distributions	O
given	O
by	O
k	O
(	O
cid:2	O
)	O
k=1	O
p	O
(	O
x|µ	O
,	O
π	O
)	O
=	O
πkp	O
(	O
x|µk	O
)	O
where	O
µ	O
=	O
{	O
µ1	O
,	O
.	O
.	O
.	O
,	O
µk	O
}	O
,	O
π	O
=	O
{	O
π1	O
,	O
.	O
.	O
.	O
,	O
πk	O
}	O
,	O
and	O
ki	O
(	O
1	O
−	O
µki	O
)	O
(	O
1−xi	O
)	O
.	O
µxi	O
(	O
9.45	O
)	O
(	O
9.46	O
)	O
(	O
9.47	O
)	O
(	O
9.48	O
)	O
(	O
9.49	O
)	O
(	O
9.50	O
)	O
exercise	O
9.12	O
the	O
mean	B
and	O
covariance	B
of	O
this	O
mixture	B
distribution	I
are	O
given	O
by	O
d	O
(	O
cid:14	O
)	O
i=1	O
p	O
(	O
x|µk	O
)	O
=	O
k	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
k=1	O
πkµk	O
(	O
cid:26	O
)	O
e	O
[	O
x	O
]	O
=	O
(	O
cid:27	O
)	O
−	O
e	O
[	O
x	O
]	O
e	O
[	O
x	O
]	O
t	O
cov	O
[	O
x	O
]	O
=	O
σk	O
+	O
µkµt	O
k	O
πk	O
k=1	O
where	O
σk	O
=	O
diag	O
{	O
µki	O
(	O
1	O
−	O
µki	O
)	O
}	O
.	O
because	O
the	O
covariance	B
matrix	I
cov	O
[	O
x	O
]	O
is	O
no	O
longer	O
diagonal	B
,	O
the	O
mixture	B
distribution	I
can	O
capture	O
correlations	O
between	O
the	O
vari-	O
ables	O
,	O
unlike	O
a	O
single	O
bernoulli	O
distribution	O
.	O
if	O
we	O
are	O
given	O
a	O
data	O
set	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
then	O
the	O
log	O
likelihood	O
function	O
for	O
this	O
model	O
is	O
given	O
by	O
ln	O
p	O
(	O
x|µ	O
,	O
π	O
)	O
=	O
n	O
(	O
cid:2	O
)	O
(	O
cid:24	O
)	O
k	O
(	O
cid:2	O
)	O
(	O
cid:25	O
)	O
πkp	O
(	O
xn|µk	O
)	O
ln	O
.	O
(	O
9.51	O
)	O
n=1	O
k=1	O
again	O
we	O
see	O
the	O
appearance	O
of	O
the	O
summation	O
inside	O
the	O
logarithm	O
,	O
so	O
that	O
the	O
maximum	B
likelihood	I
solution	O
no	O
longer	O
has	O
closed	O
form	O
.	O
we	O
now	O
derive	O
the	O
em	O
algorithm	O
for	O
maximizing	O
the	O
likelihood	B
function	I
for	O
the	O
mixture	O
of	O
bernoulli	O
distributions	O
.	O
to	O
do	O
this	O
,	O
we	O
ﬁrst	O
introduce	O
an	O
explicit	O
latent	O
446	O
9.	O
mixture	B
models	O
and	O
em	O
exercise	O
9.14	O
variable	O
z	O
associated	O
with	O
each	O
instance	O
of	O
x.	O
as	O
in	O
the	O
case	O
of	O
the	O
gaussian	O
mixture	B
,	O
z	O
=	O
(	O
z1	O
,	O
.	O
.	O
.	O
,	O
zk	O
)	O
t	O
is	O
a	O
binary	O
k-dimensional	O
variable	O
having	O
a	O
single	O
component	O
equal	O
to	O
1	O
,	O
with	O
all	O
other	O
components	O
equal	O
to	O
0.	O
we	O
can	O
then	O
write	O
the	O
conditional	B
distribution	O
of	O
x	O
,	O
given	O
the	O
latent	B
variable	I
,	O
as	O
p	O
(	O
x|z	O
,	O
µ	O
)	O
=	O
(	O
9.52	O
)	O
k	O
(	O
cid:14	O
)	O
k=1	O
p	O
(	O
x|µk	O
)	O
zk	O
k	O
(	O
cid:14	O
)	O
πzk	O
k	O
.	O
while	O
the	O
prior	B
distribution	O
for	O
the	O
latent	O
variables	O
is	O
the	O
same	O
as	O
for	O
the	O
mixture	O
of	O
gaussians	O
model	O
,	O
so	O
that	O
p	O
(	O
z|π	O
)	O
=	O
(	O
9.53	O
)	O
if	O
we	O
form	O
the	O
product	O
of	O
p	O
(	O
x|z	O
,	O
µ	O
)	O
and	O
p	O
(	O
z|π	O
)	O
and	O
then	O
marginalize	O
over	O
z	O
,	O
then	O
we	O
recover	O
(	O
9.47	O
)	O
.	O
k=1	O
in	O
order	O
to	O
derive	O
the	O
em	O
algorithm	O
,	O
we	O
ﬁrst	O
write	O
down	O
the	O
complete-data	O
log	O
likelihood	O
function	O
,	O
which	O
is	O
given	O
by	O
ln	O
p	O
(	O
x	O
,	O
z|µ	O
,	O
π	O
)	O
=	O
(	O
cid:24	O
)	O
znk	O
ln	O
πk	O
n	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
n=1	O
k=1	O
+	O
[	O
xni	O
ln	O
µki	O
+	O
(	O
1	O
−	O
xni	O
)	O
ln	O
(	O
1	O
−	O
µki	O
)	O
]	O
(	O
9.54	O
)	O
where	O
x	O
=	O
{	O
xn	O
}	O
and	O
z	O
=	O
{	O
zn	O
}	O
.	O
next	O
we	O
take	O
the	O
expectation	B
of	O
the	O
complete-data	O
log	O
likelihood	O
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
of	O
the	O
latent	O
variables	O
to	O
give	O
i=1	O
(	O
cid:24	O
)	O
ez	O
[	O
ln	O
p	O
(	O
x	O
,	O
z|µ	O
,	O
π	O
)	O
]	O
=	O
γ	O
(	O
znk	O
)	O
ln	O
πk	O
n	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
n=1	O
k=1	O
+	O
[	O
xni	O
ln	O
µki	O
+	O
(	O
1	O
−	O
xni	O
)	O
ln	O
(	O
1	O
−	O
µki	O
)	O
]	O
(	O
9.55	O
)	O
(	O
cid:25	O
)	O
(	O
cid:25	O
)	O
d	O
(	O
cid:2	O
)	O
d	O
(	O
cid:2	O
)	O
i=1	O
where	O
γ	O
(	O
znk	O
)	O
=	O
e	O
[	O
znk	O
]	O
is	O
the	O
posterior	B
probability	I
,	O
or	O
responsibility	B
,	O
of	O
component	O
k	O
given	O
data	O
point	O
xn	O
.	O
in	O
the	O
e	O
step	O
,	O
these	O
responsibilities	O
are	O
evaluated	O
using	O
bayes	O
’	O
theorem	O
,	O
which	O
takes	O
the	O
form	O
γ	O
(	O
znk	O
)	O
=	O
e	O
[	O
znk	O
]	O
=	O
=	O
znk	O
(	O
cid:9	O
)	O
znj	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:8	O
)	O
znk	O
[	O
πkp	O
(	O
xn|µk	O
)	O
]	O
znk	O
πjp	O
(	O
xn|µj	O
)	O
k	O
(	O
cid:2	O
)	O
πkp	O
(	O
xn|µk	O
)	O
πjp	O
(	O
xn|µj	O
)	O
znj	O
.	O
j=1	O
(	O
9.56	O
)	O
9.3.	O
an	O
alternative	O
view	O
of	O
em	O
447	O
if	O
we	O
consider	O
the	O
sum	O
over	O
n	O
in	O
(	O
9.55	O
)	O
,	O
we	O
see	O
that	O
the	O
responsibilities	O
enter	O
only	O
through	O
two	O
terms	O
,	O
which	O
can	O
be	O
written	O
as	O
n	O
(	O
cid:2	O
)	O
n=1	O
1	O
nk	O
nk	O
=	O
xk	O
=	O
γ	O
(	O
znk	O
)	O
n	O
(	O
cid:2	O
)	O
γ	O
(	O
znk	O
)	O
xn	O
n=1	O
(	O
9.57	O
)	O
(	O
9.58	O
)	O
where	O
nk	O
is	O
the	O
effective	O
number	O
of	O
data	O
points	O
associated	O
with	O
component	O
k.	O
in	O
the	O
m	O
step	O
,	O
we	O
maximize	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
with	O
respect	O
to	O
the	O
parameters	O
µk	O
and	O
π.	O
if	O
we	O
set	O
the	O
derivative	B
of	O
(	O
9.55	O
)	O
with	O
respect	O
to	O
µk	O
equal	O
to	O
zero	O
and	O
rearrange	O
the	O
terms	O
,	O
we	O
obtain	O
exercise	O
9.15	O
exercise	O
9.16	O
exercise	O
9.17	O
section	O
9.4	O
µk	O
=	O
xk	O
.	O
(	O
9.59	O
)	O
we	O
see	O
that	O
this	O
sets	O
the	O
mean	B
of	O
component	O
k	O
equal	O
to	O
a	O
weighted	O
mean	O
of	O
the	O
data	O
,	O
with	O
weighting	O
coefﬁcients	O
given	O
by	O
the	O
responsibilities	O
that	O
component	O
k	O
takes	O
for	O
data	O
points	O
.	O
for	O
the	O
maximization	O
with	O
respect	O
to	O
πk	O
,	O
we	O
need	O
to	O
introduce	O
a	O
k	O
πk	O
=	O
1.	O
following	O
analogous	O
lagrange	O
multiplier	O
to	O
enforce	O
the	O
constraint	O
steps	O
to	O
those	O
used	O
for	O
the	O
mixture	O
of	O
gaussians	O
,	O
we	O
then	O
obtain	O
(	O
cid:5	O
)	O
πk	O
=	O
nk	O
n	O
(	O
9.60	O
)	O
which	O
represents	O
the	O
intuitively	O
reasonable	O
result	O
that	O
the	O
mixing	B
coefﬁcient	I
for	O
com-	O
ponent	O
k	O
is	O
given	O
by	O
the	O
effective	O
fraction	O
of	O
points	O
in	O
the	O
data	O
set	O
explained	O
by	O
that	O
component	O
.	O
note	O
that	O
in	O
contrast	O
to	O
the	O
mixture	O
of	O
gaussians	O
,	O
there	O
are	O
no	O
singularities	B
in	O
which	O
the	O
likelihood	B
function	I
goes	O
to	O
inﬁnity	O
.	O
this	O
can	O
be	O
seen	O
by	O
noting	O
that	O
the	O
likelihood	B
function	I
is	O
bounded	O
above	O
because	O
0	O
(	O
cid:1	O
)	O
p	O
(	O
xn|µk	O
)	O
(	O
cid:1	O
)	O
1.	O
there	O
exist	O
singularities	B
at	O
which	O
the	O
likelihood	B
function	I
goes	O
to	O
zero	O
,	O
but	O
these	O
will	O
not	O
be	O
found	O
by	O
em	O
provided	O
it	O
is	O
not	O
initialized	O
to	O
a	O
pathological	O
starting	O
point	O
,	O
because	O
the	O
em	O
algorithm	O
always	O
increases	O
the	O
value	O
of	O
the	O
likelihood	B
function	I
,	O
until	O
a	O
local	B
maximum	O
is	O
found	O
.	O
we	O
illustrate	O
the	O
bernoulli	O
mixture	B
model	I
in	O
figure	O
9.10	O
by	O
using	O
it	O
to	O
model	O
handwritten	O
digits	O
.	O
here	O
the	O
digit	O
images	O
have	O
been	O
turned	O
into	O
binary	O
vectors	O
by	O
setting	O
all	O
elements	O
whose	O
values	O
exceed	O
0.5	O
to	O
1	O
and	O
setting	O
the	O
remaining	O
elements	O
to	O
0.	O
we	O
now	O
ﬁt	O
a	O
data	O
set	O
of	O
n	O
=	O
600	O
such	O
digits	O
,	O
comprising	O
the	O
digits	O
‘	O
2	O
’	O
,	O
‘	O
3	O
’	O
,	O
and	O
‘	O
4	O
’	O
,	O
with	O
a	O
mixture	O
of	O
k	O
=	O
3	O
bernoulli	O
distributions	O
by	O
running	O
10	O
iterations	O
of	O
the	O
em	O
algorithm	O
.	O
the	O
mixing	O
coefﬁcients	O
were	O
initialized	O
to	O
πk	O
=	O
1/k	O
,	O
and	O
the	O
parameters	O
µkj	O
were	O
set	O
to	O
random	O
values	O
chosen	O
uniformly	O
in	O
j	O
µkj	O
=	O
1.	O
the	O
range	O
(	O
0.25	O
,	O
0.75	O
)	O
and	O
then	O
normalized	O
to	O
satisfy	O
the	O
constraint	O
that	O
we	O
see	O
that	O
a	O
mixture	O
of	O
3	O
bernoulli	O
distributions	O
is	O
able	O
to	O
ﬁnd	O
the	O
three	O
clusters	O
in	O
the	O
data	O
set	O
corresponding	O
to	O
the	O
different	O
digits	O
.	O
(	O
cid:5	O
)	O
the	O
conjugate	B
prior	I
for	O
the	O
parameters	O
of	O
a	O
bernoulli	O
distribution	O
is	O
given	O
by	O
the	O
beta	B
distribution	I
,	O
and	O
we	O
have	O
seen	O
that	O
a	O
beta	O
prior	O
is	O
equivalent	O
to	O
introducing	O
448	O
9.	O
mixture	B
models	O
and	O
em	O
figure	O
9.10	O
illustration	O
of	O
the	O
bernoulli	O
mixture	B
model	I
in	O
which	O
the	O
top	O
row	O
shows	O
examples	O
from	O
the	O
digits	O
data	O
set	O
after	O
converting	O
the	O
pixel	O
values	O
from	O
grey	O
scale	O
to	O
binary	O
using	O
a	O
threshold	O
of	O
0.5.	O
on	O
the	O
bottom	O
row	O
the	O
ﬁrst	O
three	O
images	O
show	O
the	O
parameters	O
µki	O
for	O
each	O
of	O
the	O
three	O
components	O
in	O
the	O
mixture	B
model	I
.	O
as	O
a	O
comparison	O
,	O
we	O
also	O
ﬁt	O
the	O
same	O
data	O
set	O
using	O
a	O
single	O
multivariate	O
bernoulli	O
distribution	O
,	O
again	O
using	O
maximum	B
likelihood	I
.	O
this	O
amounts	O
to	O
simply	O
averaging	O
the	O
counts	O
in	O
each	O
pixel	O
and	O
is	O
shown	O
by	O
the	O
right-most	O
image	O
on	O
the	O
bottom	O
row	O
.	O
section	O
2.1.1	O
exercise	O
9.18	O
exercise	O
9.19	O
additional	O
effective	O
observations	O
of	O
x.	O
we	O
can	O
similarly	O
introduce	O
priors	O
into	O
the	O
bernoulli	O
mixture	B
model	I
,	O
and	O
use	O
em	O
to	O
maximize	O
the	O
posterior	B
probability	I
distri-	O
butions	O
.	O
it	O
is	O
straightforward	O
to	O
extend	O
the	O
analysis	O
of	O
bernoulli	O
mixtures	O
to	O
the	O
case	O
of	O
multinomial	O
binary	O
variables	O
having	O
m	O
>	O
2	O
states	O
by	O
making	O
use	O
of	O
the	O
discrete	O
dis-	O
tribution	O
(	O
2.26	O
)	O
.	O
again	O
,	O
we	O
can	O
introduce	O
dirichlet	O
priors	O
over	O
the	O
model	O
parameters	O
if	O
desired	O
.	O
9.3.4	O
em	O
for	O
bayesian	O
linear	B
regression	I
as	O
a	O
third	O
example	O
of	O
the	O
application	O
of	O
em	O
,	O
we	O
return	O
to	O
the	O
evidence	O
ap-	O
proximation	B
for	O
bayesian	O
linear	B
regression	I
.	O
in	O
section	O
3.5.2	O
,	O
we	O
obtained	O
the	O
re-	O
estimation	O
equations	O
for	O
the	O
hyperparameters	O
α	O
and	O
β	O
by	O
evaluation	O
of	O
the	O
evidence	O
and	O
then	O
setting	O
the	O
derivatives	O
of	O
the	O
resulting	O
expression	O
to	O
zero	O
.	O
we	O
now	O
turn	O
to	O
an	O
alternative	O
approach	O
for	O
ﬁnding	O
α	O
and	O
β	O
based	O
on	O
the	O
em	O
algorithm	O
.	O
recall	O
that	O
our	O
goal	O
is	O
to	O
maximize	O
the	O
evidence	B
function	I
p	O
(	O
t|α	O
,	O
β	O
)	O
given	O
by	O
(	O
3.77	O
)	O
with	O
respect	O
to	O
α	O
and	O
β.	O
because	O
the	O
parameter	O
vector	O
w	O
is	O
marginalized	O
out	O
,	O
we	O
can	O
regard	O
it	O
as	O
a	O
latent	B
variable	I
,	O
and	O
hence	O
we	O
can	O
optimize	O
this	O
marginal	B
likelihood	I
function	O
using	O
em	O
.	O
in	O
the	O
e	O
step	O
,	O
we	O
compute	O
the	O
posterior	O
distribution	O
of	O
w	O
given	O
the	O
current	O
set-	O
ting	O
of	O
the	O
parameters	O
α	O
and	O
β	O
and	O
then	O
use	O
this	O
to	O
ﬁnd	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
.	O
in	O
the	O
m	O
step	O
,	O
we	O
maximize	O
this	O
quantity	O
with	O
respect	O
to	O
α	O
and	O
β.	O
we	O
have	O
already	O
derived	O
the	O
posterior	O
distribution	O
of	O
w	O
because	O
this	O
is	O
given	O
by	O
(	O
3.49	O
)	O
.	O
the	O
complete-data	O
log	O
likelihood	O
function	O
is	O
then	O
given	O
by	O
ln	O
p	O
(	O
t	O
,	O
w|α	O
,	O
β	O
)	O
=	O
ln	O
p	O
(	O
t|w	O
,	O
β	O
)	O
+	O
ln	O
p	O
(	O
w|α	O
)	O
(	O
9.61	O
)	O
9.3.	O
an	O
alternative	O
view	O
of	O
em	O
449	O
where	O
the	O
likelihood	O
p	O
(	O
t|w	O
,	O
β	O
)	O
and	O
the	O
prior	B
p	O
(	O
w|α	O
)	O
are	O
given	O
by	O
(	O
3.10	O
)	O
and	O
(	O
3.52	O
)	O
,	O
respectively	O
,	O
and	O
y	O
(	O
x	O
,	O
w	O
)	O
is	O
given	O
by	O
(	O
3.3	O
)	O
.	O
taking	O
the	O
expectation	B
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
of	O
w	O
then	O
gives	O
e	O
[	O
ln	O
p	O
(	O
t	O
,	O
w|α	O
,	O
β	O
)	O
]	O
=	O
m	O
2	O
−	O
β	O
2	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
+	O
n	O
2	O
ln	O
β	O
2π	O
(	O
cid:8	O
)	O
(	O
cid:18	O
)	O
(	O
cid:8	O
)	O
(	O
tn	O
−	O
wtφn	O
)	O
2	O
−	O
α	O
2	O
e	O
wtw	O
(	O
cid:9	O
)	O
(	O
cid:9	O
)	O
(	O
cid:17	O
)	O
n	O
(	O
cid:2	O
)	O
ln	O
α	O
2π	O
e	O
n=1	O
.	O
(	O
9.62	O
)	O
exercise	O
9.20	O
setting	O
the	O
derivatives	O
with	O
respect	O
to	O
α	O
to	O
zero	O
,	O
we	O
obtain	O
the	O
m	O
step	O
re-estimation	O
equation	O
α	O
=	O
m	O
e	O
[	O
wtw	O
]	O
=	O
m	O
n	O
mn	O
+	O
tr	O
(	O
sn	O
)	O
.	O
mt	O
(	O
9.63	O
)	O
exercise	O
9.21	O
an	O
analogous	O
result	O
holds	O
for	O
β.	O
note	O
that	O
this	O
re-estimation	O
equation	O
takes	O
a	O
slightly	O
different	O
form	O
from	O
the	O
corresponding	O
result	O
(	O
3.92	O
)	O
derived	O
by	O
direct	O
evaluation	O
of	O
the	O
evidence	B
function	I
.	O
however	O
,	O
they	O
each	O
involve	O
computation	O
and	O
inversion	O
(	O
or	O
eigen	O
decomposition	O
)	O
of	O
an	O
m	O
×	O
m	O
matrix	O
and	O
hence	O
will	O
have	O
comparable	O
computational	O
cost	O
per	O
iteration	O
.	O
these	O
two	O
approaches	O
to	O
determining	O
α	O
should	O
of	O
course	O
converge	O
to	O
the	O
same	O
result	O
(	O
assuming	O
they	O
ﬁnd	O
the	O
same	O
local	B
maximum	O
of	O
the	O
evidence	B
function	I
)	O
.	O
this	O
can	O
be	O
veriﬁed	O
by	O
ﬁrst	O
noting	O
that	O
the	O
quantity	O
γ	O
is	O
deﬁned	O
by	O
γ	O
=	O
m	O
−	O
α	O
1	O
=	O
m	O
−	O
αtr	O
(	O
sn	O
)	O
.	O
(	O
9.64	O
)	O
m	O
(	O
cid:2	O
)	O
λi	O
+	O
α	O
i=1	O
at	O
a	O
stationary	B
point	O
of	O
the	O
evidence	B
function	I
,	O
the	O
re-estimation	O
equation	O
(	O
3.92	O
)	O
will	O
be	O
self-consistently	O
satisﬁed	O
,	O
and	O
hence	O
we	O
can	O
substitute	O
for	O
γ	O
to	O
give	O
n	O
mn	O
=	O
γ	O
=	O
m	O
−	O
αtr	O
(	O
sn	O
)	O
αmt	O
(	O
9.65	O
)	O
and	O
solving	O
for	O
α	O
we	O
obtain	O
(	O
9.63	O
)	O
,	O
which	O
is	O
precisely	O
the	O
em	O
re-estimation	O
equation	O
.	O
as	O
a	O
ﬁnal	O
example	O
,	O
we	O
consider	O
a	O
closely	O
related	O
model	O
,	O
namely	O
the	O
relevance	B
vector	I
machine	I
for	O
regression	B
discussed	O
in	O
section	O
7.2.1.	O
there	O
we	O
used	O
direct	O
max-	O
imization	O
of	O
the	O
marginal	B
likelihood	I
to	O
derive	O
re-estimation	O
equations	O
for	O
the	O
hyper-	O
parameters	O
α	O
and	O
β.	O
here	O
we	O
consider	O
an	O
alternative	O
approach	O
in	O
which	O
we	O
view	O
the	O
weight	B
vector	I
w	O
as	O
a	O
latent	B
variable	I
and	O
apply	O
the	O
em	O
algorithm	O
.	O
the	O
e	O
step	O
involves	O
ﬁnding	O
the	O
posterior	O
distribution	O
over	O
the	O
weights	O
,	O
and	O
this	O
is	O
given	O
by	O
(	O
7.81	O
)	O
.	O
in	O
the	O
m	O
step	O
we	O
maximize	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
,	O
which	O
is	O
deﬁned	O
by	O
ew	O
[	O
ln	O
p	O
(	O
t|x	O
,	O
w	O
,	O
β	O
)	O
p	O
(	O
w|α	O
)	O
]	O
(	O
9.66	O
)	O
exercise	O
9.22	O
where	O
the	O
expectation	B
is	O
taken	O
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
computed	O
using	O
the	O
‘	O
old	O
’	O
parameter	O
values	O
.	O
to	O
compute	O
the	O
new	O
parameter	O
values	O
we	O
maximize	O
with	O
respect	O
to	O
α	O
and	O
β	O
to	O
give	O
450	O
9.	O
mixture	B
models	O
and	O
em	O
αnew	O
i	O
=	O
(	O
βnew	O
)	O
−1	O
=	O
1	O
i	O
+	O
σii	O
m2	O
(	O
cid:5	O
)	O
t	O
−	O
φmn	O
(	O
cid:5	O
)	O
2	O
+	O
β	O
−1	O
n	O
(	O
cid:5	O
)	O
i	O
γi	O
(	O
9.67	O
)	O
(	O
9.68	O
)	O
exercise	O
9.23	O
these	O
re-estimation	O
equations	O
are	O
formally	O
equivalent	O
to	O
those	O
obtained	O
by	O
direct	O
maxmization	O
.	O
9.4.	O
the	O
em	O
algorithm	O
in	O
general	O
the	O
expectation	B
maximization	I
algorithm	O
,	O
or	O
em	O
algorithm	O
,	O
is	O
a	O
general	O
technique	O
for	O
ﬁnding	O
maximum	B
likelihood	I
solutions	O
for	O
probabilistic	O
models	O
having	O
latent	O
vari-	O
ables	O
(	O
dempster	O
et	O
al.	O
,	O
1977	O
;	O
mclachlan	O
and	O
krishnan	O
,	O
1997	O
)	O
.	O
here	O
we	O
give	O
a	O
very	O
general	O
treatment	O
of	O
the	O
em	O
algorithm	O
and	O
in	O
the	O
process	O
provide	O
a	O
proof	O
that	O
the	O
em	O
algorithm	O
derived	O
heuristically	O
in	O
sections	O
9.2	O
and	O
9.3	O
for	O
gaussian	O
mixtures	O
does	O
indeed	O
maximize	O
the	O
likelihood	B
function	I
(	O
csisz`ar	O
and	O
tusn`ady	O
,	O
1984	O
;	O
hath-	O
away	O
,	O
1986	O
;	O
neal	O
and	O
hinton	O
,	O
1999	O
)	O
.	O
our	O
discussion	O
will	O
also	O
form	O
the	O
basis	O
for	O
the	O
derivation	O
of	O
the	O
variational	B
inference	I
framework	O
.	O
section	O
10.1	O
consider	O
a	O
probabilistic	O
model	O
in	O
which	O
we	O
collectively	O
denote	O
all	O
of	O
the	O
ob-	O
served	O
variables	O
by	O
x	O
and	O
all	O
of	O
the	O
hidden	O
variables	O
by	O
z.	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
is	O
governed	O
by	O
a	O
set	O
of	O
parameters	O
denoted	O
θ.	O
our	O
goal	O
is	O
to	O
maximize	O
the	O
likelihood	B
function	I
that	O
is	O
given	O
by	O
p	O
(	O
x|θ	O
)	O
=	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
.	O
(	O
cid:2	O
)	O
(	O
9.69	O
)	O
z	O
here	O
we	O
are	O
assuming	O
z	O
is	O
discrete	O
,	O
although	O
the	O
discussion	O
is	O
identical	O
if	O
z	O
com-	O
prises	O
continuous	O
variables	O
or	O
a	O
combination	O
of	O
discrete	O
and	O
continuous	O
variables	O
,	O
with	O
summation	O
replaced	O
by	O
integration	O
as	O
appropriate	O
.	O
we	O
shall	O
suppose	O
that	O
direct	O
optimization	O
of	O
p	O
(	O
x|θ	O
)	O
is	O
difﬁcult	O
,	O
but	O
that	O
opti-	O
mization	O
of	O
the	O
complete-data	O
likelihood	B
function	I
p	O
(	O
x	O
,	O
z|θ	O
)	O
is	O
signiﬁcantly	O
easier	O
.	O
next	O
we	O
introduce	O
a	O
distribution	O
q	O
(	O
z	O
)	O
deﬁned	O
over	O
the	O
latent	O
variables	O
,	O
and	O
we	O
ob-	O
serve	O
that	O
,	O
for	O
any	O
choice	O
of	O
q	O
(	O
z	O
)	O
,	O
the	O
following	O
decomposition	O
holds	O
ln	O
p	O
(	O
x|θ	O
)	O
=	O
l	O
(	O
q	O
,	O
θ	O
)	O
+	O
kl	O
(	O
q	O
(	O
cid:5	O
)	O
p	O
)	O
where	O
we	O
have	O
deﬁned	O
l	O
(	O
q	O
,	O
θ	O
)	O
=	O
q	O
(	O
z	O
)	O
ln	O
kl	O
(	O
q	O
(	O
cid:5	O
)	O
p	O
)	O
=	O
−	O
q	O
(	O
z	O
)	O
ln	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
z	O
z	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
q	O
(	O
z	O
)	O
p	O
(	O
z|x	O
,	O
θ	O
)	O
q	O
(	O
z	O
)	O
(	O
cid:13	O
)	O
(	O
9.70	O
)	O
(	O
9.71	O
)	O
(	O
9.72	O
)	O
(	O
cid:13	O
)	O
.	O
note	O
that	O
l	O
(	O
q	O
,	O
θ	O
)	O
is	O
a	O
functional	B
(	O
see	O
appendix	O
d	O
for	O
a	O
discussion	O
of	O
functionals	O
)	O
of	O
the	O
distribution	O
q	O
(	O
z	O
)	O
,	O
and	O
a	O
function	O
of	O
the	O
parameters	O
θ.	O
it	O
is	O
worth	O
studying	O
9.4.	O
the	O
em	O
algorithm	O
in	O
general	O
451	O
figure	O
9.11	O
illustration	O
of	O
the	O
decomposition	O
given	O
by	O
(	O
9.70	O
)	O
,	O
which	O
holds	O
for	O
any	O
choice	O
of	O
distribution	O
q	O
(	O
z	O
)	O
.	O
because	O
the	O
kullback-leibler	O
divergence	O
satisﬁes	O
kl	O
(	O
q	O
(	O
cid:3	O
)	O
p	O
)	O
(	O
cid:2	O
)	O
0	O
,	O
we	O
see	O
that	O
the	O
quan-	O
tity	O
l	O
(	O
q	O
,	O
θ	O
)	O
is	O
a	O
lower	B
bound	I
on	O
the	O
log	O
likelihood	O
function	O
ln	O
p	O
(	O
x|θ	O
)	O
.	O
kl	O
(	O
q||p	O
)	O
l	O
(	O
q	O
,	O
θ	O
)	O
ln	O
p	O
(	O
x|θ	O
)	O
exercise	O
9.24	O
section	O
1.6.1	O
carefully	O
the	O
forms	O
of	O
the	O
expressions	O
(	O
9.71	O
)	O
and	O
(	O
9.72	O
)	O
,	O
and	O
in	O
particular	O
noting	O
that	O
they	O
differ	O
in	O
sign	O
and	O
also	O
that	O
l	O
(	O
q	O
,	O
θ	O
)	O
contains	O
the	O
joint	O
distribution	O
of	O
x	O
and	O
z	O
while	O
kl	O
(	O
q	O
(	O
cid:5	O
)	O
p	O
)	O
contains	O
the	O
conditional	B
distribution	O
of	O
z	O
given	O
x.	O
to	O
verify	O
the	O
decomposition	O
(	O
9.70	O
)	O
,	O
we	O
ﬁrst	O
make	O
use	O
of	O
the	O
product	B
rule	I
of	I
probability	I
to	O
give	O
ln	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
=	O
ln	O
p	O
(	O
z|x	O
,	O
θ	O
)	O
+	O
ln	O
p	O
(	O
x|θ	O
)	O
(	O
9.73	O
)	O
which	O
we	O
then	O
substitute	O
into	O
the	O
expression	O
for	O
l	O
(	O
q	O
,	O
θ	O
)	O
.	O
this	O
gives	O
rise	O
to	O
two	O
terms	O
,	O
one	O
of	O
which	O
cancels	O
kl	O
(	O
q	O
(	O
cid:5	O
)	O
p	O
)	O
while	O
the	O
other	O
gives	O
the	O
required	O
log	O
likelihood	O
ln	O
p	O
(	O
x|θ	O
)	O
after	O
noting	O
that	O
q	O
(	O
z	O
)	O
is	O
a	O
normalized	O
distribution	O
that	O
sums	O
to	O
1.	O
from	O
(	O
9.72	O
)	O
,	O
we	O
see	O
that	O
kl	O
(	O
q	O
(	O
cid:5	O
)	O
p	O
)	O
is	O
the	O
kullback-leibler	O
divergence	O
between	O
q	O
(	O
z	O
)	O
and	O
the	O
posterior	O
distribution	O
p	O
(	O
z|x	O
,	O
θ	O
)	O
.	O
recall	O
that	O
the	O
kullback-leibler	O
di-	O
vergence	O
satisﬁes	O
kl	O
(	O
q	O
(	O
cid:5	O
)	O
p	O
)	O
(	O
cid:2	O
)	O
0	O
,	O
with	O
equality	O
if	O
,	O
and	O
only	O
if	O
,	O
q	O
(	O
z	O
)	O
=	O
p	O
(	O
z|x	O
,	O
θ	O
)	O
.	O
it	O
therefore	O
follows	O
from	O
(	O
9.70	O
)	O
that	O
l	O
(	O
q	O
,	O
θ	O
)	O
(	O
cid:1	O
)	O
ln	O
p	O
(	O
x|θ	O
)	O
,	O
in	O
other	O
words	O
that	O
l	O
(	O
q	O
,	O
θ	O
)	O
is	O
a	O
lower	B
bound	I
on	O
ln	O
p	O
(	O
x|θ	O
)	O
.	O
the	O
decomposition	O
(	O
9.70	O
)	O
is	O
illustrated	O
in	O
fig-	O
ure	O
9.11.	O
the	O
em	O
algorithm	O
is	O
a	O
two-stage	O
iterative	O
optimization	O
technique	O
for	O
ﬁnding	O
maximum	B
likelihood	I
solutions	O
.	O
we	O
can	O
use	O
the	O
decomposition	O
(	O
9.70	O
)	O
to	O
deﬁne	O
the	O
em	O
algorithm	O
and	O
to	O
demonstrate	O
that	O
it	O
does	O
indeed	O
maximize	O
the	O
log	O
likelihood	O
.	O
suppose	O
that	O
the	O
current	O
value	O
of	O
the	O
parameter	O
vector	O
is	O
θold	O
.	O
in	O
the	O
e	O
step	O
,	O
the	O
lower	B
bound	I
l	O
(	O
q	O
,	O
θold	O
)	O
is	O
maximized	O
with	O
respect	O
to	O
q	O
(	O
z	O
)	O
while	O
holding	O
θold	O
ﬁxed	O
.	O
the	O
solution	O
to	O
this	O
maximization	O
problem	O
is	O
easily	O
seen	O
by	O
noting	O
that	O
the	O
value	O
of	O
ln	O
p	O
(	O
x|θold	O
)	O
does	O
not	O
depend	O
on	O
q	O
(	O
z	O
)	O
and	O
so	O
the	O
largest	O
value	O
of	O
l	O
(	O
q	O
,	O
θold	O
)	O
will	O
occur	O
when	O
the	O
kullback-leibler	O
divergence	O
vanishes	O
,	O
in	O
other	O
words	O
when	O
q	O
(	O
z	O
)	O
is	O
equal	O
to	O
the	O
posterior	O
distribution	O
p	O
(	O
z|x	O
,	O
θold	O
)	O
.	O
in	O
this	O
case	O
,	O
the	O
lower	B
bound	I
will	O
equal	O
the	O
log	O
likelihood	O
,	O
as	O
illustrated	O
in	O
figure	O
9.12.	O
in	O
the	O
subsequent	O
m	O
step	O
,	O
the	O
distribution	O
q	O
(	O
z	O
)	O
is	O
held	O
ﬁxed	O
and	O
the	O
lower	B
bound	I
l	O
(	O
q	O
,	O
θ	O
)	O
is	O
maximized	O
with	O
respect	O
to	O
θ	O
to	O
give	O
some	O
new	O
value	O
θnew	O
.	O
this	O
will	O
cause	O
the	O
lower	B
bound	I
l	O
to	O
increase	O
(	O
unless	O
it	O
is	O
already	O
at	O
a	O
maximum	O
)	O
,	O
which	O
will	O
necessarily	O
cause	O
the	O
corresponding	O
log	O
likelihood	O
function	O
to	O
increase	O
.	O
because	O
the	O
distribution	O
q	O
is	O
determined	O
using	O
the	O
old	O
parameter	O
values	O
rather	O
than	O
the	O
new	O
values	O
and	O
is	O
held	O
ﬁxed	O
during	O
the	O
m	O
step	O
,	O
it	O
will	O
not	O
equal	O
the	O
new	O
posterior	O
distribution	O
p	O
(	O
z|x	O
,	O
θnew	O
)	O
,	O
and	O
hence	O
there	O
will	O
be	O
a	O
nonzero	O
kl	O
divergence	O
.	O
the	O
increase	O
in	O
the	O
log	O
likelihood	O
function	O
is	O
therefore	O
greater	O
than	O
the	O
increase	O
in	O
the	O
lower	B
bound	I
,	O
as	O
452	O
9.	O
mixture	B
models	O
and	O
em	O
kl	O
(	O
q||p	O
)	O
=	O
0	O
figure	O
9.12	O
illustration	O
of	O
the	O
e	O
step	O
of	O
the	O
em	O
algorithm	O
.	O
the	O
q	O
distribution	O
is	O
set	O
equal	O
to	O
the	O
posterior	O
distribution	O
for	O
the	O
current	O
parameter	O
val-	O
ues	O
θold	O
,	O
causing	O
the	O
lower	B
bound	I
to	O
move	O
up	O
to	O
the	O
same	O
value	O
as	O
the	O
log	O
like-	O
lihood	O
function	O
,	O
with	O
the	O
kl	O
divergence	O
vanishing	O
.	O
l	O
(	O
q	O
,	O
θold	O
)	O
ln	O
p	O
(	O
x|θold	O
)	O
shown	O
in	O
figure	O
9.13.	O
if	O
we	O
substitute	O
q	O
(	O
z	O
)	O
=	O
p	O
(	O
z|x	O
,	O
θold	O
)	O
into	O
(	O
9.71	O
)	O
,	O
we	O
see	O
that	O
,	O
after	O
the	O
e	O
step	O
,	O
the	O
lower	B
bound	I
takes	O
the	O
form	O
l	O
(	O
q	O
,	O
θ	O
)	O
=	O
p	O
(	O
z|x	O
,	O
θold	O
)	O
ln	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
−	O
p	O
(	O
z|x	O
,	O
θold	O
)	O
ln	O
p	O
(	O
z|x	O
,	O
θold	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
z	O
=	O
q	O
(	O
θ	O
,	O
θold	O
)	O
+	O
const	O
z	O
(	O
9.74	O
)	O
where	O
the	O
constant	O
is	O
simply	O
the	O
negative	O
entropy	B
of	O
the	O
q	O
distribution	O
and	O
is	O
there-	O
fore	O
independent	B
of	O
θ.	O
thus	O
in	O
the	O
m	O
step	O
,	O
the	O
quantity	O
that	O
is	O
being	O
maximized	O
is	O
the	O
expectation	B
of	O
the	O
complete-data	O
log	O
likelihood	O
,	O
as	O
we	O
saw	O
earlier	O
in	O
the	O
case	O
of	O
mix-	O
tures	O
of	O
gaussians	O
.	O
note	O
that	O
the	O
variable	O
θ	O
over	O
which	O
we	O
are	O
optimizing	O
appears	O
only	O
inside	O
the	O
logarithm	O
.	O
if	O
the	O
joint	O
distribution	O
p	O
(	O
z	O
,	O
x|θ	O
)	O
comprises	O
a	O
member	O
of	O
the	O
exponential	B
family	I
,	O
or	O
a	O
product	O
of	O
such	O
members	O
,	O
then	O
we	O
see	O
that	O
the	O
logarithm	O
will	O
cancel	O
the	O
exponential	O
and	O
lead	O
to	O
an	O
m	O
step	O
that	O
will	O
be	O
typically	O
much	O
simpler	O
than	O
the	O
maximization	O
of	O
the	O
corresponding	O
incomplete-data	O
log	O
likelihood	O
function	O
p	O
(	O
x|θ	O
)	O
.	O
the	O
operation	O
of	O
the	O
em	O
algorithm	O
can	O
also	O
be	O
viewed	O
in	O
the	O
space	O
of	O
parame-	O
ters	O
,	O
as	O
illustrated	O
schematically	O
in	O
figure	O
9.14.	O
here	O
the	O
red	O
curve	O
depicts	O
the	O
(	O
in-	O
kl	O
(	O
q||p	O
)	O
figure	O
9.13	O
illustration	O
of	O
the	O
m	O
step	O
of	O
the	O
em	O
the	O
distribution	O
q	O
(	O
z	O
)	O
algorithm	O
.	O
is	O
held	O
ﬁxed	O
and	O
the	O
lower	B
bound	I
l	O
(	O
q	O
,	O
θ	O
)	O
is	O
maximized	O
with	O
respect	O
to	O
the	O
parameter	O
vector	O
θ	O
to	O
give	O
a	O
revised	O
value	O
θnew	O
.	O
because	O
the	O
kl	O
divergence	O
is	O
nonnegative	O
,	O
this	O
causes	O
the	O
log	O
likelihood	O
ln	O
p	O
(	O
x|θ	O
)	O
to	O
increase	O
by	O
at	O
least	O
as	O
much	O
as	O
the	O
lower	B
bound	I
does	O
.	O
l	O
(	O
q	O
,	O
θnew	O
)	O
ln	O
p	O
(	O
x|θnew	O
)	O
figure	O
9.14	O
the	O
em	O
algorithm	O
involves	O
alter-	O
nately	O
computing	O
a	O
lower	B
bound	I
on	O
the	O
log	O
likelihood	O
for	O
the	O
cur-	O
rent	O
parameter	O
values	O
and	O
then	O
maximizing	O
this	O
bound	O
to	O
obtain	O
the	O
new	O
parameter	O
values	O
.	O
see	O
the	O
text	O
for	O
a	O
full	O
discussion	O
.	O
9.4.	O
the	O
em	O
algorithm	O
in	O
general	O
453	O
ln	O
p	O
(	O
x|θ	O
)	O
l	O
(	O
q	O
,	O
θ	O
)	O
θold	O
θnew	O
exercise	O
9.25	O
complete	O
data	O
)	O
log	O
likelihood	O
function	O
whose	O
value	O
we	O
wish	O
to	O
maximize	O
.	O
we	O
start	O
with	O
some	O
initial	O
parameter	O
value	O
θold	O
,	O
and	O
in	O
the	O
ﬁrst	O
e	O
step	O
we	O
evaluate	O
the	O
poste-	O
rior	O
distribution	O
over	O
latent	O
variables	O
,	O
which	O
gives	O
rise	O
to	O
a	O
lower	B
bound	I
l	O
(	O
θ	O
,	O
θ	O
(	O
old	O
)	O
)	O
whose	O
value	O
equals	O
the	O
log	O
likelihood	O
at	O
θ	O
(	O
old	O
)	O
,	O
as	O
shown	O
by	O
the	O
blue	O
curve	O
.	O
note	O
that	O
the	O
bound	O
makes	O
a	O
tangential	O
contact	O
with	O
the	O
log	O
likelihood	O
at	O
θ	O
(	O
old	O
)	O
,	O
so	O
that	O
both	O
curves	O
have	O
the	O
same	O
gradient	O
.	O
this	O
bound	O
is	O
a	O
convex	B
function	I
having	O
a	O
unique	O
maximum	O
(	O
for	O
mixture	O
components	O
from	O
the	O
exponential	B
family	I
)	O
.	O
in	O
the	O
m	O
step	O
,	O
the	O
bound	O
is	O
maximized	O
giving	O
the	O
value	O
θ	O
(	O
new	O
)	O
,	O
which	O
gives	O
a	O
larger	O
value	O
of	O
log	O
likeli-	O
hood	O
than	O
θ	O
(	O
old	O
)	O
.	O
the	O
subsequent	O
e	O
step	O
then	O
constructs	O
a	O
bound	O
that	O
is	O
tangential	O
at	O
θ	O
(	O
new	O
)	O
as	O
shown	O
by	O
the	O
green	O
curve	O
.	O
for	O
the	O
particular	O
case	O
of	O
an	O
independent	B
,	O
identically	O
distributed	O
data	O
set	O
,	O
x	O
will	O
comprise	O
n	O
data	O
points	O
{	O
xn	O
}	O
while	O
z	O
will	O
comprise	O
n	O
corresponding	O
latent	O
(	O
cid:21	O
)	O
variables	O
{	O
zn	O
}	O
,	O
where	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
from	O
the	O
independence	O
assumption	O
,	O
we	O
have	O
n	O
p	O
(	O
xn	O
,	O
zn	O
)	O
and	O
,	O
by	O
marginalizing	O
over	O
the	O
{	O
zn	O
}	O
we	O
have	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
x	O
,	O
z	O
)	O
=	O
n	O
p	O
(	O
xn	O
)	O
.	O
using	O
the	O
sum	O
and	O
product	O
rules	O
,	O
we	O
see	O
that	O
the	O
posterior	B
probability	I
(	O
cid:21	O
)	O
that	O
is	O
evaluated	O
in	O
the	O
e	O
step	O
takes	O
the	O
form	O
(	O
cid:2	O
)	O
p	O
(	O
z|x	O
,	O
θ	O
)	O
=	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
=	O
z	O
z	O
n=1	O
n	O
(	O
cid:14	O
)	O
n	O
(	O
cid:14	O
)	O
(	O
cid:2	O
)	O
n=1	O
p	O
(	O
xn	O
,	O
zn|θ	O
)	O
p	O
(	O
xn	O
,	O
zn|θ	O
)	O
n	O
(	O
cid:14	O
)	O
n=1	O
=	O
p	O
(	O
zn|xn	O
,	O
θ	O
)	O
(	O
9.75	O
)	O
and	O
so	O
the	O
posterior	O
distribution	O
also	O
factorizes	O
with	O
respect	O
to	O
n.	O
in	O
the	O
case	O
of	O
the	O
gaussian	O
mixture	B
model	I
this	O
simply	O
says	O
that	O
the	O
responsibility	B
that	O
each	O
of	O
the	O
mixture	B
components	O
takes	O
for	O
a	O
particular	O
data	O
point	O
xn	O
depends	O
only	O
on	O
the	O
value	O
of	O
xn	O
and	O
on	O
the	O
parameters	O
θ	O
of	O
the	O
mixture	B
components	O
,	O
not	O
on	O
the	O
values	O
of	O
the	O
other	O
data	O
points	O
.	O
we	O
have	O
seen	O
that	O
both	O
the	O
e	O
and	O
the	O
m	O
steps	O
of	O
the	O
em	O
algorithm	O
are	O
increas-	O
ing	O
the	O
value	O
of	O
a	O
well-deﬁned	O
bound	O
on	O
the	O
log	O
likelihood	O
function	O
and	O
that	O
the	O
454	O
9.	O
mixture	B
models	O
and	O
em	O
complete	O
em	O
cycle	O
will	O
change	O
the	O
model	O
parameters	O
in	O
such	O
a	O
way	O
as	O
to	O
cause	O
the	O
log	O
likelihood	O
to	O
increase	O
(	O
unless	O
it	O
is	O
already	O
at	O
a	O
maximum	O
,	O
in	O
which	O
case	O
the	O
parameters	O
remain	O
unchanged	O
)	O
.	O
we	O
can	O
also	O
use	O
the	O
em	O
algorithm	O
to	O
maximize	O
the	O
posterior	O
distribution	O
p	O
(	O
θ|x	O
)	O
for	O
models	O
in	O
which	O
we	O
have	O
introduced	O
a	O
prior	B
p	O
(	O
θ	O
)	O
over	O
the	O
parameters	O
.	O
to	O
see	O
this	O
,	O
we	O
note	O
that	O
as	O
a	O
function	O
of	O
θ	O
,	O
we	O
have	O
p	O
(	O
θ|x	O
)	O
=	O
p	O
(	O
θ	O
,	O
x	O
)	O
/p	O
(	O
x	O
)	O
and	O
so	O
ln	O
p	O
(	O
θ|x	O
)	O
=	O
ln	O
p	O
(	O
θ	O
,	O
x	O
)	O
−	O
ln	O
p	O
(	O
x	O
)	O
.	O
making	O
use	O
of	O
the	O
decomposition	O
(	O
9.70	O
)	O
,	O
we	O
have	O
ln	O
p	O
(	O
θ|x	O
)	O
=	O
l	O
(	O
q	O
,	O
θ	O
)	O
+	O
kl	O
(	O
q	O
(	O
cid:5	O
)	O
p	O
)	O
+	O
ln	O
p	O
(	O
θ	O
)	O
−	O
ln	O
p	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
l	O
(	O
q	O
,	O
θ	O
)	O
+	O
ln	O
p	O
(	O
θ	O
)	O
−	O
ln	O
p	O
(	O
x	O
)	O
.	O
(	O
9.76	O
)	O
(	O
9.77	O
)	O
where	O
ln	O
p	O
(	O
x	O
)	O
is	O
a	O
constant	O
.	O
we	O
can	O
again	O
optimize	O
the	O
right-hand	O
side	O
alternately	O
with	O
respect	O
to	O
q	O
and	O
θ.	O
the	O
optimization	O
with	O
respect	O
to	O
q	O
gives	O
rise	O
to	O
the	O
same	O
e-	O
step	O
equations	O
as	O
for	O
the	O
standard	O
em	O
algorithm	O
,	O
because	O
q	O
only	O
appears	O
in	O
l	O
(	O
q	O
,	O
θ	O
)	O
.	O
the	O
m-step	O
equations	O
are	O
modiﬁed	O
through	O
the	O
introduction	O
of	O
the	O
prior	B
term	O
ln	O
p	O
(	O
θ	O
)	O
,	O
which	O
typically	O
requires	O
only	O
a	O
small	O
modiﬁcation	O
to	O
the	O
standard	O
maximum	O
likeli-	O
hood	O
m-step	O
equations	O
.	O
the	O
em	O
algorithm	O
breaks	O
down	O
the	O
potentially	O
difﬁcult	O
problem	O
of	O
maximizing	O
the	O
likelihood	B
function	I
into	O
two	O
stages	O
,	O
the	O
e	O
step	O
and	O
the	O
m	O
step	O
,	O
each	O
of	O
which	O
will	O
often	O
prove	O
simpler	O
to	O
implement	O
.	O
nevertheless	O
,	O
for	O
complex	O
models	O
it	O
may	O
be	O
the	O
case	O
that	O
either	O
the	O
e	O
step	O
or	O
the	O
m	O
step	O
,	O
or	O
indeed	O
both	O
,	O
remain	O
intractable	O
.	O
this	O
leads	O
to	O
two	O
possible	O
extensions	O
of	O
the	O
em	O
algorithm	O
,	O
as	O
follows	O
.	O
the	O
generalized	B
em	O
,	O
or	O
gem	O
,	O
algorithm	O
addresses	O
the	O
problem	O
of	O
an	O
intractable	O
m	O
step	O
.	O
instead	O
of	O
aiming	O
to	O
maximize	O
l	O
(	O
q	O
,	O
θ	O
)	O
with	O
respect	O
to	O
θ	O
,	O
it	O
seeks	O
instead	O
to	O
change	O
the	O
parameters	O
in	O
such	O
a	O
way	O
as	O
to	O
increase	O
its	O
value	O
.	O
again	O
,	O
because	O
l	O
(	O
q	O
,	O
θ	O
)	O
is	O
a	O
lower	B
bound	I
on	O
the	O
log	O
likelihood	O
function	O
,	O
each	O
complete	O
em	O
cycle	O
of	O
the	O
gem	O
algorithm	O
is	O
guaranteed	O
to	O
increase	O
the	O
value	O
of	O
the	O
log	O
likelihood	O
(	O
unless	O
the	O
parameters	O
already	O
correspond	O
to	O
a	O
local	B
maximum	O
)	O
.	O
one	O
way	O
to	O
exploit	O
the	O
gem	O
approach	O
would	O
be	O
to	O
use	O
one	O
of	O
the	O
nonlinear	O
optimization	O
strategies	O
,	O
such	O
as	O
the	O
conjugate	B
gradients	O
algorithm	O
,	O
during	O
the	O
m	O
step	O
.	O
another	O
form	O
of	O
gem	O
algorithm	O
,	O
known	O
as	O
the	O
expectation	B
conditional	I
maximization	I
,	O
or	O
ecm	O
,	O
algorithm	O
,	O
involves	O
making	O
several	O
constrained	O
optimizations	O
within	O
each	O
m	O
step	O
(	O
meng	O
and	O
rubin	O
,	O
1993	O
)	O
.	O
for	O
instance	O
,	O
the	O
parameters	O
might	O
be	O
partitioned	B
into	O
groups	O
,	O
and	O
the	O
m	O
step	O
is	O
broken	O
down	O
into	O
multiple	O
steps	O
each	O
of	O
which	O
involves	O
optimizing	O
one	O
of	O
the	O
subset	O
with	O
the	O
remainder	O
held	O
ﬁxed	O
.	O
we	O
can	O
similarly	O
generalize	O
the	O
e	O
step	O
of	O
the	O
em	O
algorithm	O
by	O
performing	O
a	O
partial	O
,	O
rather	O
than	O
complete	O
,	O
optimization	O
of	O
l	O
(	O
q	O
,	O
θ	O
)	O
with	O
respect	O
to	O
q	O
(	O
z	O
)	O
(	O
neal	O
and	O
hinton	O
,	O
1999	O
)	O
.	O
as	O
we	O
have	O
seen	O
,	O
for	O
any	O
given	O
value	O
of	O
θ	O
there	O
is	O
a	O
unique	O
maximum	O
of	O
l	O
(	O
q	O
,	O
θ	O
)	O
with	O
respect	O
to	O
q	O
(	O
z	O
)	O
that	O
corresponds	O
to	O
the	O
posterior	O
distribution	O
qθ	O
(	O
z	O
)	O
=	O
p	O
(	O
z|x	O
,	O
θ	O
)	O
and	O
that	O
for	O
this	O
choice	O
of	O
q	O
(	O
z	O
)	O
the	O
bound	O
l	O
(	O
q	O
,	O
θ	O
)	O
is	O
equal	O
to	O
the	O
log	O
likelihood	O
function	O
ln	O
p	O
(	O
x|θ	O
)	O
.	O
it	O
follows	O
that	O
any	O
algorithm	O
that	O
converges	O
to	O
the	O
global	O
maximum	O
of	O
l	O
(	O
q	O
,	O
θ	O
)	O
will	O
ﬁnd	O
a	O
value	O
of	O
θ	O
that	O
is	O
also	O
a	O
global	O
maximum	O
of	O
the	O
log	O
likelihood	O
ln	O
p	O
(	O
x|θ	O
)	O
.	O
provided	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
is	O
a	O
continuous	O
function	O
of	O
θ	O
exercises	O
455	O
then	O
,	O
by	O
continuity	O
,	O
any	O
local	B
maximum	O
of	O
l	O
(	O
q	O
,	O
θ	O
)	O
will	O
also	O
be	O
a	O
local	B
maximum	O
of	O
ln	O
p	O
(	O
x|θ	O
)	O
.	O
consider	O
the	O
case	O
of	O
n	O
independent	B
data	O
points	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
with	O
corresponding	O
latent	O
variables	O
z1	O
,	O
.	O
.	O
.	O
,	O
zn	O
.	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
factorizes	O
over	O
the	O
data	O
points	O
,	O
and	O
this	O
structure	O
can	O
be	O
exploited	O
in	O
an	O
incremental	O
form	O
of	O
em	O
in	O
which	O
at	O
each	O
em	O
cycle	O
only	O
one	O
data	O
point	O
is	O
processed	O
at	O
a	O
time	O
.	O
in	O
the	O
e	O
step	O
,	O
instead	O
of	O
recomputing	O
the	O
responsibilities	O
for	O
all	O
of	O
the	O
data	O
points	O
,	O
we	O
just	O
re-evaluate	O
the	O
responsibilities	O
for	O
one	O
data	O
point	O
.	O
it	O
might	O
appear	O
that	O
the	O
subsequent	O
m	O
step	O
would	O
require	O
computation	O
involving	O
the	O
responsibilities	O
for	O
all	O
of	O
the	O
data	O
points	O
.	O
how-	O
ever	O
,	O
if	O
the	O
mixture	B
components	O
are	O
members	O
of	O
the	O
exponential	B
family	I
,	O
then	O
the	O
responsibilities	O
enter	O
only	O
through	O
simple	O
sufﬁcient	B
statistics	I
,	O
and	O
these	O
can	O
be	O
up-	O
dated	O
efﬁciently	O
.	O
consider	O
,	O
for	O
instance	O
,	O
the	O
case	O
of	O
a	O
gaussian	O
mixture	B
,	O
and	O
suppose	O
we	O
perform	O
an	O
update	O
for	O
data	O
point	O
m	O
in	O
which	O
the	O
corresponding	O
old	O
and	O
new	O
values	O
of	O
the	O
responsibilities	O
are	O
denoted	O
γold	O
(	O
zmk	O
)	O
and	O
γnew	O
(	O
zmk	O
)	O
.	O
in	O
the	O
m	O
step	O
,	O
the	O
required	O
sufﬁcient	B
statistics	I
can	O
be	O
updated	O
incrementally	O
.	O
for	O
instance	O
,	O
for	O
the	O
means	O
the	O
sufﬁcient	B
statistics	I
are	O
deﬁned	O
by	O
(	O
9.17	O
)	O
and	O
(	O
9.18	O
)	O
from	O
which	O
we	O
obtain	O
(	O
cid:16	O
)	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
µnew	O
k	O
=	O
µold	O
k	O
+	O
γnew	O
(	O
zmk	O
)	O
−	O
γold	O
(	O
zmk	O
)	O
n	O
new	O
k	O
xm	O
−	O
µold	O
k	O
(	O
cid:15	O
)	O
(	O
9.78	O
)	O
(	O
9.79	O
)	O
together	O
with	O
n	O
new	O
k	O
=	O
n	O
old	O
k	O
+	O
γnew	O
(	O
zmk	O
)	O
−	O
γold	O
(	O
zmk	O
)	O
.	O
the	O
corresponding	O
results	O
for	O
the	O
covariances	O
and	O
the	O
mixing	O
coefﬁcients	O
are	O
analo-	O
gous	O
.	O
thus	O
both	O
the	O
e	O
step	O
and	O
the	O
m	O
step	O
take	O
ﬁxed	O
time	O
that	O
is	O
independent	B
of	O
the	O
total	O
number	O
of	O
data	O
points	O
.	O
because	O
the	O
parameters	O
are	O
revised	O
after	O
each	O
data	O
point	O
,	O
rather	O
than	O
waiting	O
until	O
after	O
the	O
whole	O
data	O
set	O
is	O
processed	O
,	O
this	O
incremental	O
ver-	O
sion	B
can	O
converge	O
faster	O
than	O
the	O
batch	O
version	O
.	O
each	O
e	O
or	O
m	O
step	O
in	O
this	O
incremental	O
algorithm	O
is	O
increasing	O
the	O
value	O
of	O
l	O
(	O
q	O
,	O
θ	O
)	O
and	O
,	O
as	O
we	O
have	O
shown	O
above	O
,	O
if	O
the	O
algorithm	O
converges	O
to	O
a	O
local	B
(	O
or	O
global	O
)	O
maximum	O
of	O
l	O
(	O
q	O
,	O
θ	O
)	O
,	O
this	O
will	O
correspond	O
to	O
a	O
local	B
(	O
or	O
global	O
)	O
maximum	O
of	O
the	O
log	O
likelihood	O
function	O
ln	O
p	O
(	O
x|θ	O
)	O
.	O
9.1	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
the	O
k-means	O
algorithm	O
discussed	O
in	O
section	O
9.1.	O
show	O
that	O
as	O
a	O
consequence	O
of	O
there	O
being	O
a	O
ﬁnite	O
number	O
of	O
possible	O
assignments	O
for	O
the	O
set	O
of	O
discrete	O
indicator	O
variables	O
rnk	O
,	O
and	O
that	O
for	O
each	O
such	O
assignment	O
there	O
is	O
a	O
unique	O
optimum	O
for	O
the	O
{	O
µk	O
}	O
,	O
the	O
k-means	O
algorithm	O
must	O
converge	O
after	O
a	O
ﬁnite	O
number	O
of	O
iterations	O
.	O
9.2	O
(	O
(	O
cid:12	O
)	O
)	O
apply	O
the	O
robbins-monro	O
sequential	B
estimation	I
procedure	O
described	O
in	O
sec-	O
tion	O
2.3.5	O
to	O
the	O
problem	O
of	O
ﬁnding	O
the	O
roots	O
of	O
the	O
regression	B
function	I
given	O
by	O
the	O
derivatives	O
of	O
j	O
in	O
(	O
9.1	O
)	O
with	O
respect	O
to	O
µk	O
.	O
show	O
that	O
this	O
leads	O
to	O
a	O
stochastic	B
k-means	O
algorithm	O
in	O
which	O
,	O
for	O
each	O
data	O
point	O
xn	O
,	O
the	O
nearest	O
prototype	O
µk	O
is	O
updated	O
using	O
(	O
9.5	O
)	O
.	O
exercise	O
9.26	O
exercises	O
456	O
9.	O
mixture	B
models	O
and	O
em	O
9.3	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
gaussian	O
mixture	B
model	I
in	O
which	O
the	O
marginal	B
distribution	O
p	O
(	O
z	O
)	O
for	O
the	O
latent	B
variable	I
is	O
given	O
by	O
(	O
9.10	O
)	O
,	O
and	O
the	O
conditional	B
distribution	O
p	O
(	O
x|z	O
)	O
for	O
the	O
observed	B
variable	I
is	O
given	O
by	O
(	O
9.11	O
)	O
.	O
show	O
that	O
the	O
marginal	B
distribution	O
p	O
(	O
x	O
)	O
,	O
obtained	O
by	O
summing	O
p	O
(	O
z	O
)	O
p	O
(	O
x|z	O
)	O
over	O
all	O
possible	O
values	O
of	O
z	O
,	O
is	O
a	O
gaussian	O
mixture	O
of	O
the	O
form	O
(	O
9.7	O
)	O
.	O
9.4	O
(	O
(	O
cid:12	O
)	O
)	O
suppose	O
we	O
wish	O
to	O
use	O
the	O
em	O
algorithm	O
to	O
maximize	O
the	O
posterior	O
distri-	O
bution	O
over	O
parameters	O
p	O
(	O
θ|x	O
)	O
for	O
a	O
model	O
containing	O
latent	O
variables	O
,	O
where	O
x	O
is	O
the	O
observed	O
data	O
set	O
.	O
show	O
that	O
the	O
e	O
step	O
remains	O
the	O
same	O
as	O
in	O
the	O
maximum	B
likelihood	I
case	O
,	O
whereas	O
in	O
the	O
m	O
step	O
the	O
quantity	O
to	O
be	O
maximized	O
is	O
given	O
by	O
q	O
(	O
θ	O
,	O
θold	O
)	O
+	O
ln	O
p	O
(	O
θ	O
)	O
where	O
q	O
(	O
θ	O
,	O
θold	O
)	O
is	O
deﬁned	O
by	O
(	O
9.30	O
)	O
.	O
9.5	O
(	O
(	O
cid:12	O
)	O
)	O
consider	O
the	O
directed	B
graph	O
for	O
a	O
gaussian	O
mixture	B
model	I
shown	O
in	O
figure	O
9.6.	O
by	O
making	O
use	O
of	O
the	O
d-separation	B
criterion	O
discussed	O
in	O
section	O
8.2	O
,	O
show	O
that	O
the	O
posterior	O
distribution	O
of	O
the	O
latent	O
variables	O
factorizes	O
with	O
respect	O
to	O
the	O
different	O
data	O
points	O
so	O
that	O
p	O
(	O
z|x	O
,	O
µ	O
,	O
σ	O
,	O
π	O
)	O
=	O
p	O
(	O
zn|xn	O
,	O
µ	O
,	O
σ	O
,	O
π	O
)	O
.	O
(	O
9.80	O
)	O
n=1	O
9.6	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
special	O
case	O
of	O
a	O
gaussian	O
mixture	B
model	I
in	O
which	O
the	O
covari-	O
ance	O
matrices	O
σk	O
of	O
the	O
components	O
are	O
all	O
constrained	O
to	O
have	O
a	O
common	O
value	O
σ.	O
derive	O
the	O
em	O
equations	O
for	O
maximizing	O
the	O
likelihood	B
function	I
under	O
such	O
a	O
model	O
.	O
9.7	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
verify	O
that	O
maximization	O
of	O
the	O
complete-data	O
log	O
likelihood	O
(	O
9.36	O
)	O
for	O
a	O
gaussian	O
mixture	B
model	I
leads	O
to	O
the	O
result	O
that	O
the	O
means	O
and	O
covariances	O
of	O
each	O
component	O
are	O
ﬁtted	O
independently	O
to	O
the	O
corresponding	O
group	O
of	O
data	O
points	O
,	O
and	O
the	O
mixing	O
coefﬁcients	O
are	O
given	O
by	O
the	O
fractions	O
of	O
points	O
in	O
each	O
group	O
.	O
9.8	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
if	O
we	O
maximize	O
(	O
9.40	O
)	O
with	O
respect	O
to	O
µk	O
while	O
keeping	O
the	O
responsibilities	O
γ	O
(	O
znk	O
)	O
ﬁxed	O
,	O
we	O
obtain	O
the	O
closed	O
form	O
solution	O
given	O
by	O
(	O
9.17	O
)	O
.	O
9.9	O
(	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
if	O
we	O
maximize	O
(	O
9.40	O
)	O
with	O
respect	O
to	O
σk	O
and	O
πk	O
while	O
keeping	O
the	O
responsibilities	O
γ	O
(	O
znk	O
)	O
ﬁxed	O
,	O
we	O
obtain	O
the	O
closed	O
form	O
solutions	O
given	O
by	O
(	O
9.19	O
)	O
and	O
(	O
9.22	O
)	O
.	O
n	O
(	O
cid:14	O
)	O
k	O
(	O
cid:2	O
)	O
9.10	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
density	B
model	O
given	O
by	O
a	O
mixture	B
distribution	I
p	O
(	O
x	O
)	O
=	O
πkp	O
(	O
x|k	O
)	O
(	O
9.81	O
)	O
k=1	O
and	O
suppose	O
that	O
we	O
partition	O
the	O
vector	O
x	O
into	O
two	O
parts	O
so	O
that	O
x	O
=	O
(	O
xa	O
,	O
xb	O
)	O
.	O
show	O
that	O
the	O
conditional	B
density	O
p	O
(	O
xb|xa	O
)	O
is	O
itself	O
a	O
mixture	B
distribution	I
and	O
ﬁnd	O
expressions	O
for	O
the	O
mixing	O
coefﬁcients	O
and	O
for	O
the	O
component	O
densities	O
.	O
exercises	O
457	O
9.11	O
(	O
(	O
cid:12	O
)	O
)	O
in	O
section	O
9.3.2	O
,	O
we	O
obtained	O
a	O
relationship	O
between	O
k	O
means	O
and	O
em	O
for	O
gaussian	O
mixtures	O
by	O
considering	O
a	O
mixture	B
model	I
in	O
which	O
all	O
components	O
have	O
covariance	B
i	O
.	O
show	O
that	O
in	O
the	O
limit	O
	O
→	O
0	O
,	O
maximizing	O
the	O
expected	O
complete-	O
data	O
log	O
likelihood	O
for	O
this	O
model	O
,	O
given	O
by	O
(	O
9.40	O
)	O
,	O
is	O
equivalent	O
to	O
minimizing	O
the	O
distortion	B
measure	I
j	O
for	O
the	O
k-means	O
algorithm	O
given	O
by	O
(	O
9.1	O
)	O
.	O
9.12	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
mixture	B
distribution	I
of	O
the	O
form	O
p	O
(	O
x	O
)	O
=	O
πkp	O
(	O
x|k	O
)	O
(	O
9.82	O
)	O
k=1	O
where	O
the	O
elements	O
of	O
x	O
could	O
be	O
discrete	O
or	O
continuous	O
or	O
a	O
combination	O
of	O
these	O
.	O
denote	O
the	O
mean	B
and	O
covariance	B
of	O
p	O
(	O
x|k	O
)	O
by	O
µk	O
and	O
σk	O
,	O
respectively	O
.	O
show	O
that	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
mixture	B
distribution	I
are	O
given	O
by	O
(	O
9.49	O
)	O
and	O
(	O
9.50	O
)	O
.	O
9.13	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
using	O
the	O
re-estimation	O
equations	O
for	O
the	O
em	O
algorithm	O
,	O
show	O
that	O
a	O
mix-	O
ture	O
of	O
bernoulli	O
distributions	O
,	O
with	O
its	O
parameters	O
set	O
to	O
values	O
corresponding	O
to	O
a	O
maximum	O
of	O
the	O
likelihood	B
function	I
,	O
has	O
the	O
property	O
that	O
k	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
e	O
[	O
x	O
]	O
=	O
1	O
n	O
xn	O
≡	O
x	O
.	O
(	O
9.83	O
)	O
nents	O
have	O
the	O
same	O
mean	B
µk	O
=	O
(	O
cid:1	O
)	O
µ	O
for	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
,	O
then	O
the	O
em	O
algorithm	O
will	O
hence	O
show	O
that	O
if	O
the	O
parameters	O
of	O
this	O
model	O
are	O
initialized	O
such	O
that	O
all	O
compo-	O
converge	O
after	O
one	O
iteration	O
,	O
for	O
any	O
choice	O
of	O
the	O
initial	O
mixing	O
coefﬁcients	O
,	O
and	O
that	O
this	O
solution	O
has	O
the	O
property	O
µk	O
=	O
x.	O
note	O
that	O
this	O
represents	O
a	O
degenerate	O
case	O
of	O
the	O
mixture	B
model	I
in	O
which	O
all	O
of	O
the	O
components	O
are	O
identical	O
,	O
and	O
in	O
practice	O
we	O
try	O
to	O
avoid	O
such	O
solutions	O
by	O
using	O
an	O
appropriate	O
initialization	O
.	O
9.14	O
(	O
(	O
cid:12	O
)	O
)	O
consider	O
the	O
joint	O
distribution	O
of	O
latent	O
and	O
observed	O
variables	O
for	O
the	O
bernoulli	O
distribution	O
obtained	O
by	O
forming	O
the	O
product	O
of	O
p	O
(	O
x|z	O
,	O
µ	O
)	O
given	O
by	O
(	O
9.52	O
)	O
and	O
p	O
(	O
z|π	O
)	O
given	O
by	O
(	O
9.53	O
)	O
.	O
show	O
that	O
if	O
we	O
marginalize	O
this	O
joint	O
distribution	O
with	O
respect	O
to	O
z	O
,	O
then	O
we	O
obtain	O
(	O
9.47	O
)	O
.	O
9.15	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
if	O
we	O
maximize	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
function	O
(	O
9.55	O
)	O
for	O
a	O
mixture	O
of	O
bernoulli	O
distributions	O
with	O
respect	O
to	O
µk	O
,	O
we	O
obtain	O
the	O
m	O
step	O
equation	O
(	O
9.59	O
)	O
.	O
9.16	O
(	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
if	O
we	O
maximize	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
function	O
(	O
9.55	O
)	O
for	O
a	O
mixture	O
of	O
bernoulli	O
distributions	O
with	O
respect	O
to	O
the	O
mixing	O
coefﬁcients	O
πk	O
,	O
using	O
a	O
lagrange	O
multiplier	O
to	O
enforce	O
the	O
summation	O
constraint	O
,	O
we	O
obtain	O
the	O
m	O
step	O
equation	O
(	O
9.60	O
)	O
.	O
9.17	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
as	O
a	O
consequence	O
of	O
the	O
constraint	O
0	O
(	O
cid:1	O
)	O
p	O
(	O
xn|µk	O
)	O
(	O
cid:1	O
)	O
1	O
for	O
the	O
discrete	O
variable	O
xn	O
,	O
the	O
incomplete-data	O
log	O
likelihood	O
function	O
for	O
a	O
mixture	O
of	O
bernoulli	O
distributions	O
is	O
bounded	O
above	O
,	O
and	O
hence	O
that	O
there	O
are	O
no	O
singularities	B
for	O
which	O
the	O
likelihood	O
goes	O
to	O
inﬁnity	O
.	O
458	O
9.	O
mixture	B
models	O
and	O
em	O
9.18	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
bernoulli	O
mixture	B
model	I
as	O
discussed	O
in	O
section	O
9.3.3	O
,	O
together	O
with	O
a	O
prior	B
distribution	O
p	O
(	O
µk|ak	O
,	O
bk	O
)	O
over	O
each	O
of	O
the	O
parameter	O
vectors	O
µk	O
given	O
by	O
the	O
beta	B
distribution	I
(	O
2.13	O
)	O
,	O
and	O
a	O
dirichlet	O
prior	B
p	O
(	O
π|α	O
)	O
given	O
by	O
(	O
2.38	O
)	O
.	O
derive	O
the	O
em	O
algorithm	O
for	O
maximizing	O
the	O
posterior	B
probability	I
p	O
(	O
µ	O
,	O
π|x	O
)	O
.	O
(	O
cid:5	O
)	O
9.19	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
d-dimensional	O
variable	O
x	O
each	O
of	O
whose	O
components	O
i	O
is	O
itself	O
a	O
multinomial	O
variable	O
of	O
degree	O
m	O
so	O
that	O
x	O
is	O
a	O
binary	O
vector	O
with	O
components	O
xij	O
where	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
and	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
,	O
subject	O
to	O
the	O
constraint	O
that	O
j	O
xij	O
=	O
1	O
for	O
all	O
i.	O
suppose	O
that	O
the	O
distribution	O
of	O
these	O
variables	O
is	O
described	O
by	O
a	O
mixture	O
of	O
the	O
discrete	O
multinomial	O
distributions	O
considered	O
in	O
section	O
2.2	O
so	O
that	O
p	O
(	O
x	O
)	O
=	O
where	O
(	O
9.84	O
)	O
k=1	O
k	O
(	O
cid:2	O
)	O
πkp	O
(	O
x|µk	O
)	O
d	O
(	O
cid:14	O
)	O
m	O
(	O
cid:14	O
)	O
(	O
cid:5	O
)	O
i=1	O
xij	O
kij	O
.	O
p	O
(	O
x|µk	O
)	O
=	O
µ	O
(	O
9.85	O
)	O
the	O
parameters	O
µkij	O
represent	O
the	O
probabilities	O
p	O
(	O
xij	O
=	O
1|µk	O
)	O
and	O
must	O
satisfy	O
0	O
(	O
cid:1	O
)	O
µkij	O
(	O
cid:1	O
)	O
1	O
together	O
with	O
the	O
constraint	O
j	O
µkij	O
=	O
1	O
for	O
all	O
values	O
of	O
k	O
and	O
i.	O
given	O
an	O
observed	O
data	O
set	O
{	O
xn	O
}	O
,	O
where	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
derive	O
the	O
e	O
and	O
m	O
step	O
equations	O
of	O
the	O
em	O
algorithm	O
for	O
optimizing	O
the	O
mixing	O
coefﬁcients	O
πk	O
and	O
the	O
component	O
parameters	O
µkij	O
of	O
this	O
distribution	O
by	O
maximum	B
likelihood	I
.	O
j=1	O
9.20	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
maximization	O
of	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
function	O
(	O
9.62	O
)	O
for	O
the	O
bayesian	O
linear	B
regression	I
model	O
leads	O
to	O
the	O
m	O
step	O
re-	O
estimation	O
result	O
(	O
9.63	O
)	O
for	O
α	O
.	O
9.21	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
using	O
the	O
evidence	O
framework	O
of	O
section	O
3.5	O
,	O
derive	O
the	O
m-step	O
re-estimation	O
equations	O
for	O
the	O
parameter	O
β	O
in	O
the	O
bayesian	O
linear	B
regression	I
model	O
,	O
analogous	O
to	O
the	O
result	O
(	O
9.63	O
)	O
for	O
α	O
.	O
9.22	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
by	O
maximization	O
of	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
deﬁned	O
by	O
(	O
9.66	O
)	O
,	O
derive	O
the	O
m	O
step	O
equations	O
(	O
9.67	O
)	O
and	O
(	O
9.68	O
)	O
for	O
re-estimating	O
the	O
hyperpa-	O
rameters	O
of	O
the	O
relevance	B
vector	I
machine	I
for	O
regression	B
.	O
9.23	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
in	O
section	O
7.2.1	O
we	O
used	O
direct	O
maximization	O
of	O
the	O
marginal	B
like-	O
lihood	O
to	O
derive	O
the	O
re-estimation	O
equations	O
(	O
7.87	O
)	O
and	O
(	O
7.88	O
)	O
for	O
ﬁnding	O
values	O
of	O
the	O
hyperparameters	O
α	O
and	O
β	O
for	O
the	O
regression	B
rvm	O
.	O
similarly	O
,	O
in	O
section	O
9.3.4	O
we	O
used	O
the	O
em	O
algorithm	O
to	O
maximize	O
the	O
same	O
marginal	B
likelihood	I
,	O
giving	O
the	O
re-estimation	O
equations	O
(	O
9.67	O
)	O
and	O
(	O
9.68	O
)	O
.	O
show	O
that	O
these	O
two	O
sets	O
of	O
re-estimation	O
equations	O
are	O
formally	O
equivalent	O
.	O
9.24	O
(	O
(	O
cid:12	O
)	O
)	O
verify	O
the	O
relation	O
(	O
9.70	O
)	O
in	O
which	O
l	O
(	O
q	O
,	O
θ	O
)	O
and	O
kl	O
(	O
q	O
(	O
cid:5	O
)	O
p	O
)	O
are	O
deﬁned	O
by	O
(	O
9.71	O
)	O
and	O
(	O
9.72	O
)	O
,	O
respectively	O
.	O
exercises	O
459	O
9.25	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
the	O
lower	B
bound	I
l	O
(	O
q	O
,	O
θ	O
)	O
given	O
by	O
(	O
9.71	O
)	O
,	O
with	O
q	O
(	O
z	O
)	O
=	O
p	O
(	O
z|x	O
,	O
θ	O
(	O
old	O
)	O
)	O
,	O
has	O
the	O
same	O
gradient	O
with	O
respect	O
to	O
θ	O
as	O
the	O
log	O
likelihood	O
function	O
ln	O
p	O
(	O
x|θ	O
)	O
at	O
the	O
point	O
θ	O
=	O
θ	O
(	O
old	O
)	O
.	O
9.26	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
the	O
incremental	O
form	O
of	O
the	O
em	O
algorithm	O
for	O
a	O
mixture	O
of	O
gaussians	O
,	O
in	O
which	O
the	O
responsibilities	O
are	O
recomputed	O
only	O
for	O
a	O
speciﬁc	O
data	O
point	O
xm	O
.	O
starting	O
from	O
the	O
m-step	O
formulae	O
(	O
9.17	O
)	O
and	O
(	O
9.18	O
)	O
,	O
derive	O
the	O
results	O
(	O
9.78	O
)	O
and	O
(	O
9.79	O
)	O
for	O
updating	O
the	O
component	O
means	O
.	O
9.27	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
derive	O
m-step	O
formulae	O
for	O
updating	O
the	O
covariance	B
matrices	O
and	O
mixing	O
coefﬁcients	O
in	O
a	O
gaussian	O
mixture	B
model	I
when	O
the	O
responsibilities	O
are	O
updated	O
in-	O
crementally	O
,	O
analogous	O
to	O
the	O
result	O
(	O
9.78	O
)	O
for	O
updating	O
the	O
means	O
.	O
10	O
approximate	O
inference	B
a	O
central	O
task	O
in	O
the	O
application	O
of	O
probabilistic	O
models	O
is	O
the	O
evaluation	O
of	O
the	O
pos-	O
terior	O
distribution	O
p	O
(	O
z|x	O
)	O
of	O
the	O
latent	O
variables	O
z	O
given	O
the	O
observed	O
(	O
visible	O
)	O
data	O
variables	O
x	O
,	O
and	O
the	O
evaluation	O
of	O
expectations	O
computed	O
with	O
respect	O
to	O
this	O
dis-	O
tribution	O
.	O
the	O
model	O
might	O
also	O
contain	O
some	O
deterministic	O
parameters	O
,	O
which	O
we	O
will	O
leave	O
implicit	O
for	O
the	O
moment	O
,	O
or	O
it	O
may	O
be	O
a	O
fully	O
bayesian	O
model	O
in	O
which	O
any	O
unknown	O
parameters	O
are	O
given	O
prior	B
distributions	O
and	O
are	O
absorbed	O
into	O
the	O
set	O
of	O
latent	O
variables	O
denoted	O
by	O
the	O
vector	O
z.	O
for	O
instance	O
,	O
in	O
the	O
em	O
algorithm	O
we	O
need	O
to	O
evaluate	O
the	O
expectation	B
of	O
the	O
complete-data	O
log	O
likelihood	O
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
of	O
the	O
latent	O
variables	O
.	O
for	O
many	O
models	O
of	O
practical	O
interest	O
,	O
it	O
will	O
be	O
infeasible	O
to	O
evaluate	O
the	O
posterior	O
distribution	O
or	O
indeed	O
to	O
compute	O
expec-	O
tations	O
with	O
respect	O
to	O
this	O
distribution	O
.	O
this	O
could	O
be	O
because	O
the	O
dimensionality	O
of	O
the	O
latent	O
space	O
is	O
too	O
high	O
to	O
work	O
with	O
directly	O
or	O
because	O
the	O
posterior	O
distribution	O
has	O
a	O
highly	O
complex	O
form	O
for	O
which	O
expectations	O
are	O
not	O
analytically	O
tractable	O
.	O
in	O
the	O
case	O
of	O
continuous	O
variables	O
,	O
the	O
required	O
integrations	O
may	O
not	O
have	O
closed-form	O
461	O
462	O
10.	O
approximate	O
inference	B
analytical	O
solutions	O
,	O
while	O
the	O
dimensionality	O
of	O
the	O
space	O
and	O
the	O
complexity	O
of	O
the	O
integrand	O
may	O
prohibit	O
numerical	O
integration	O
.	O
for	O
discrete	O
variables	O
,	O
the	O
marginal-	O
izations	O
involve	O
summing	O
over	O
all	O
possible	O
conﬁgurations	O
of	O
the	O
hidden	O
variables	O
,	O
and	O
though	O
this	O
is	O
always	O
possible	O
in	O
principle	O
,	O
we	O
often	O
ﬁnd	O
in	O
practice	O
that	O
there	O
may	O
be	O
exponentially	O
many	O
hidden	O
states	O
so	O
that	O
exact	O
calculation	O
is	O
prohibitively	O
expensive	O
.	O
in	O
such	O
situations	O
,	O
we	O
need	O
to	O
resort	O
to	O
approximation	O
schemes	O
,	O
and	O
these	O
fall	O
broadly	O
into	O
two	O
classes	O
,	O
according	O
to	O
whether	O
they	O
rely	O
on	O
stochastic	B
or	O
determin-	O
istic	O
approximations	O
.	O
stochastic	B
techniques	O
such	O
as	O
markov	O
chain	O
monte	O
carlo	O
,	O
de-	O
scribed	O
in	O
chapter	O
11	O
,	O
have	O
enabled	O
the	O
widespread	O
use	O
of	O
bayesian	O
methods	O
across	O
many	O
domains	O
.	O
they	O
generally	O
have	O
the	O
property	O
that	O
given	O
inﬁnite	O
computational	O
resource	O
,	O
they	O
can	O
generate	O
exact	O
results	O
,	O
and	O
the	O
approximation	O
arises	O
from	O
the	O
use	O
of	O
a	O
ﬁnite	O
amount	O
of	O
processor	O
time	O
.	O
in	O
practice	O
,	O
sampling	B
methods	I
can	O
be	O
compu-	O
tationally	O
demanding	O
,	O
often	O
limiting	O
their	O
use	O
to	O
small-scale	O
problems	O
.	O
also	O
,	O
it	O
can	O
be	O
difﬁcult	O
to	O
know	O
whether	O
a	O
sampling	O
scheme	O
is	O
generating	O
independent	B
samples	O
from	O
the	O
required	O
distribution	O
.	O
in	O
this	O
chapter	O
,	O
we	O
introduce	O
a	O
range	O
of	O
deterministic	O
approximation	O
schemes	O
,	O
some	O
of	O
which	O
scale	O
well	O
to	O
large	O
applications	O
.	O
these	O
are	O
based	O
on	O
analytical	O
ap-	O
proximations	O
to	O
the	O
posterior	O
distribution	O
,	O
for	O
example	O
by	O
assuming	O
that	O
it	O
factorizes	O
in	O
a	O
particular	O
way	O
or	O
that	O
it	O
has	O
a	O
speciﬁc	O
parametric	O
form	O
such	O
as	O
a	O
gaussian	O
.	O
as	O
such	O
,	O
they	O
can	O
never	O
generate	O
exact	O
results	O
,	O
and	O
so	O
their	O
strengths	O
and	O
weaknesses	O
are	O
complementary	O
to	O
those	O
of	O
sampling	B
methods	I
.	O
in	O
section	O
4.4	O
,	O
we	O
discussed	O
the	O
laplace	O
approximation	O
,	O
which	O
is	O
based	O
on	O
a	O
local	B
gaussian	O
approximation	O
to	O
a	O
mode	O
(	O
i.e.	O
,	O
a	O
maximum	O
)	O
of	O
the	O
distribution	O
.	O
here	O
we	O
turn	O
to	O
a	O
family	O
of	O
approximation	O
techniques	O
called	O
variational	B
inference	I
or	O
vari-	O
ational	O
bayes	O
,	O
which	O
use	O
more	O
global	O
criteria	O
and	O
which	O
have	O
been	O
widely	O
applied	O
.	O
we	O
conclude	O
with	O
a	O
brief	O
introduction	O
to	O
an	O
alternative	O
variational	B
framework	O
known	O
as	O
expectation	B
propagation	I
.	O
10.1.	O
variational	B
inference	I
variational	O
methods	O
have	O
their	O
origins	O
in	O
the	O
18th	O
century	O
with	O
the	O
work	O
of	O
euler	O
,	O
lagrange	O
,	O
and	O
others	O
on	O
the	O
calculus	B
of	I
variations	I
.	O
standard	O
calculus	O
is	O
concerned	O
with	O
ﬁnding	O
derivatives	O
of	O
functions	O
.	O
we	O
can	O
think	O
of	O
a	O
function	O
as	O
a	O
mapping	O
that	O
takes	O
the	O
value	O
of	O
a	O
variable	O
as	O
the	O
input	O
and	O
returns	O
the	O
value	O
of	O
the	O
function	O
as	O
the	O
output	O
.	O
the	O
derivative	B
of	O
the	O
function	O
then	O
describes	O
how	O
the	O
output	O
value	O
varies	O
as	O
we	O
make	O
inﬁnitesimal	O
changes	O
to	O
the	O
input	O
value	O
.	O
similarly	O
,	O
we	O
can	O
deﬁne	O
a	O
functional	B
as	O
a	O
mapping	O
that	O
takes	O
a	O
function	O
as	O
the	O
input	O
and	O
that	O
returns	O
the	O
value	O
of	O
the	O
functional	B
as	O
the	O
output	O
.	O
an	O
example	O
would	O
be	O
the	O
entropy	B
h	O
[	O
p	O
]	O
,	O
which	O
takes	O
a	O
probability	B
distribution	O
p	O
(	O
x	O
)	O
as	O
the	O
input	O
and	O
returns	O
the	O
quantity	O
(	O
cid:6	O
)	O
h	O
[	O
p	O
]	O
=	O
p	O
(	O
x	O
)	O
ln	O
p	O
(	O
x	O
)	O
dx	O
(	O
10.1	O
)	O
10.1.	O
variational	B
inference	I
463	O
as	O
the	O
output	O
.	O
we	O
can	O
the	O
introduce	O
the	O
concept	O
of	O
a	O
functional	B
derivative	O
,	O
which	O
ex-	O
presses	O
how	O
the	O
value	O
of	O
the	O
functional	B
changes	O
in	O
response	O
to	O
inﬁnitesimal	O
changes	O
to	O
the	O
input	O
function	O
(	O
feynman	O
et	O
al.	O
,	O
1964	O
)	O
.	O
the	O
rules	O
for	O
the	O
calculus	B
of	I
variations	I
mirror	O
those	O
of	O
standard	O
calculus	O
and	O
are	O
discussed	O
in	O
appendix	O
d.	O
many	O
problems	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
an	O
optimization	O
problem	O
in	O
which	O
the	O
quantity	O
being	O
optimized	O
is	O
a	O
functional	B
.	O
the	O
solution	O
is	O
obtained	O
by	O
exploring	O
all	O
possible	O
input	O
functions	O
to	O
ﬁnd	O
the	O
one	O
that	O
maximizes	O
,	O
or	O
minimizes	O
,	O
the	O
functional	B
.	O
variational	B
methods	O
have	O
broad	O
applicability	O
and	O
include	O
such	O
areas	O
as	O
ﬁnite	O
element	O
methods	O
(	O
kapur	O
,	O
1989	O
)	O
and	O
maximum	O
entropy	O
(	O
schwarz	O
,	O
1988	O
)	O
.	O
although	O
there	O
is	O
nothing	O
intrinsically	O
approximate	O
about	O
variational	B
methods	O
,	O
they	O
do	O
naturally	O
lend	O
themselves	O
to	O
ﬁnding	O
approximate	O
solutions	O
.	O
this	O
is	O
done	O
by	O
restricting	O
the	O
range	O
of	O
functions	O
over	O
which	O
the	O
optimization	O
is	O
performed	O
,	O
for	O
instance	O
by	O
considering	O
only	O
quadratic	O
functions	O
or	O
by	O
considering	O
functions	O
com-	O
posed	O
of	O
a	O
linear	O
combination	O
of	O
ﬁxed	O
basis	O
functions	O
in	O
which	O
only	O
the	O
coefﬁcients	O
of	O
the	O
linear	O
combination	O
can	O
vary	O
.	O
in	O
the	O
case	O
of	O
applications	O
to	O
probabilistic	O
in-	O
ference	O
,	O
the	O
restriction	O
may	O
for	O
example	O
take	O
the	O
form	O
of	O
factorization	B
assumptions	O
(	O
jordan	O
et	O
al.	O
,	O
1999	O
;	O
jaakkola	O
,	O
2001	O
)	O
.	O
now	O
let	O
us	O
consider	O
in	O
more	O
detail	O
how	O
the	O
concept	O
of	O
variational	B
optimization	O
can	O
be	O
applied	O
to	O
the	O
inference	B
problem	O
.	O
suppose	O
we	O
have	O
a	O
fully	O
bayesian	O
model	O
in	O
which	O
all	O
parameters	O
are	O
given	O
prior	B
distributions	O
.	O
the	O
model	O
may	O
also	O
have	O
latent	O
variables	O
as	O
well	O
as	O
parameters	O
,	O
and	O
we	O
shall	O
denote	O
the	O
set	O
of	O
all	O
latent	O
variables	O
and	O
parameters	O
by	O
z.	O
similarly	O
,	O
we	O
denote	O
the	O
set	O
of	O
all	O
observed	O
variables	O
by	O
x.	O
for	O
example	O
,	O
we	O
might	O
have	O
a	O
set	O
of	O
n	O
independent	B
,	O
identically	O
distributed	O
data	O
,	O
for	O
which	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
and	O
z	O
=	O
{	O
z1	O
,	O
.	O
.	O
.	O
,	O
zn	O
}	O
.	O
our	O
probabilistic	O
model	O
speciﬁes	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
z	O
)	O
,	O
and	O
our	O
goal	O
is	O
to	O
ﬁnd	O
an	O
approximation	O
for	O
the	O
posterior	O
distribution	O
p	O
(	O
z|x	O
)	O
as	O
well	O
as	O
for	O
the	O
model	B
evidence	I
p	O
(	O
x	O
)	O
.	O
as	O
in	O
our	O
discussion	O
of	O
em	O
,	O
we	O
can	O
decompose	O
the	O
log	O
marginal	O
probability	B
using	O
ln	O
p	O
(	O
x	O
)	O
=	O
l	O
(	O
q	O
)	O
+	O
kl	O
(	O
q	O
(	O
cid:5	O
)	O
p	O
)	O
(	O
cid:6	O
)	O
(	O
cid:12	O
)	O
where	O
we	O
have	O
deﬁned	O
l	O
(	O
q	O
)	O
=	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
dz	O
dz	O
.	O
(	O
cid:6	O
)	O
q	O
(	O
z	O
)	O
ln	O
(	O
cid:12	O
)	O
p	O
(	O
x	O
,	O
z	O
)	O
q	O
(	O
z	O
)	O
p	O
(	O
z|x	O
)	O
q	O
(	O
z	O
)	O
kl	O
(	O
q	O
(	O
cid:5	O
)	O
p	O
)	O
=	O
−	O
q	O
(	O
z	O
)	O
ln	O
(	O
10.2	O
)	O
(	O
10.3	O
)	O
(	O
10.4	O
)	O
this	O
differs	O
from	O
our	O
discussion	O
of	O
em	O
only	O
in	O
that	O
the	O
parameter	O
vector	O
θ	O
no	O
longer	O
appears	O
,	O
because	O
the	O
parameters	O
are	O
now	O
stochastic	B
variables	O
and	O
are	O
absorbed	O
into	O
z.	O
since	O
in	O
this	O
chapter	O
we	O
will	O
mainly	O
be	O
interested	O
in	O
continuous	O
variables	O
we	O
have	O
used	O
integrations	O
rather	O
than	O
summations	O
in	O
formulating	O
this	O
decomposition	O
.	O
how-	O
ever	O
,	O
the	O
analysis	O
goes	O
through	O
unchanged	O
if	O
some	O
or	O
all	O
of	O
the	O
variables	O
are	O
discrete	O
simply	O
by	O
replacing	O
the	O
integrations	O
with	O
summations	O
as	O
required	O
.	O
as	O
before	O
,	O
we	O
can	O
maximize	O
the	O
lower	B
bound	I
l	O
(	O
q	O
)	O
by	O
optimization	O
with	O
respect	O
to	O
the	O
distribution	O
q	O
(	O
z	O
)	O
,	O
which	O
is	O
equivalent	O
to	O
minimizing	O
the	O
kl	O
divergence	O
.	O
if	O
we	O
allow	O
any	O
possible	O
choice	O
for	O
q	O
(	O
z	O
)	O
,	O
then	O
the	O
maximum	O
of	O
the	O
lower	B
bound	I
occurs	O
when	O
the	O
kl	O
diver-	O
gence	O
vanishes	O
,	O
which	O
occurs	O
when	O
q	O
(	O
z	O
)	O
equals	O
the	O
posterior	O
distribution	O
p	O
(	O
z|x	O
)	O
.	O
464	O
10.	O
approximate	O
inference	B
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−2	O
−1	O
0	O
1	O
2	O
3	O
4	O
40	O
30	O
20	O
10	O
0	O
−2	O
−1	O
0	O
1	O
2	O
3	O
4	O
figure	O
10.1	O
illustration	O
of	O
the	O
variational	B
approximation	O
for	O
the	O
example	O
considered	O
earlier	O
in	O
figure	O
4.14.	O
the	O
left-hand	O
plot	O
shows	O
the	O
original	O
distribution	O
(	O
yellow	O
)	O
along	O
with	O
the	O
laplace	O
(	O
red	O
)	O
and	O
variational	B
(	O
green	O
)	O
approx-	O
imations	O
,	O
and	O
the	O
right-hand	O
plot	O
shows	O
the	O
negative	O
logarithms	O
of	O
the	O
corresponding	O
curves	O
.	O
however	O
,	O
we	O
shall	O
suppose	O
the	O
model	O
is	O
such	O
that	O
working	O
with	O
the	O
true	O
posterior	O
distribution	O
is	O
intractable	O
.	O
we	O
therefore	O
consider	O
instead	O
a	O
restricted	O
family	O
of	O
distributions	O
q	O
(	O
z	O
)	O
and	O
then	O
seek	O
the	O
member	O
of	O
this	O
family	O
for	O
which	O
the	O
kl	O
divergence	O
is	O
minimized	O
.	O
our	O
goal	O
is	O
to	O
restrict	O
the	O
family	O
sufﬁciently	O
that	O
they	O
comprise	O
only	O
tractable	O
distributions	O
,	O
while	O
at	O
the	O
same	O
time	O
allowing	O
the	O
family	O
to	O
be	O
sufﬁciently	O
rich	O
and	O
ﬂexible	O
that	O
it	O
can	O
provide	O
a	O
good	O
approximation	O
to	O
the	O
true	O
posterior	O
distribution	O
.	O
it	O
is	O
important	O
to	O
emphasize	O
that	O
the	O
restriction	O
is	O
imposed	O
purely	O
to	O
achieve	O
tractability	O
,	O
and	O
that	O
sub-	O
ject	O
to	O
this	O
requirement	O
we	O
should	O
use	O
as	O
rich	O
a	O
family	O
of	O
approximating	O
distributions	O
as	O
possible	O
.	O
in	O
particular	O
,	O
there	O
is	O
no	O
‘	O
over-ﬁtting	B
’	O
associated	O
with	O
highly	O
ﬂexible	O
dis-	O
tributions	O
.	O
using	O
more	O
ﬂexible	O
approximations	O
simply	O
allows	O
us	O
to	O
approach	O
the	O
true	O
posterior	O
distribution	O
more	O
closely	O
.	O
one	O
way	O
to	O
restrict	O
the	O
family	O
of	O
approximating	O
distributions	O
is	O
to	O
use	O
a	O
paramet-	O
ric	O
distribution	O
q	O
(	O
z|ω	O
)	O
governed	O
by	O
a	O
set	O
of	O
parameters	O
ω.	O
the	O
lower	B
bound	I
l	O
(	O
q	O
)	O
then	O
becomes	O
a	O
function	O
of	O
ω	O
,	O
and	O
we	O
can	O
exploit	O
standard	O
nonlinear	O
optimization	O
techniques	O
to	O
determine	O
the	O
optimal	O
values	O
for	O
the	O
parameters	O
.	O
an	O
example	O
of	O
this	O
approach	O
,	O
in	O
which	O
the	O
variational	B
distribution	O
is	O
a	O
gaussian	O
and	O
we	O
have	O
optimized	O
with	O
respect	O
to	O
its	O
mean	B
and	O
variance	B
,	O
is	O
shown	O
in	O
figure	O
10.1	O
.	O
10.1.1	O
factorized	O
distributions	O
here	O
we	O
consider	O
an	O
alternative	O
way	O
in	O
which	O
to	O
restrict	O
the	O
family	O
of	O
distri-	O
butions	O
q	O
(	O
z	O
)	O
.	O
suppose	O
we	O
partition	O
the	O
elements	O
of	O
z	O
into	O
disjoint	O
groups	O
that	O
we	O
denote	O
by	O
zi	O
where	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m.	O
we	O
then	O
assume	O
that	O
the	O
q	O
distribution	O
factorizes	O
with	O
respect	O
to	O
these	O
groups	O
,	O
so	O
that	O
m	O
(	O
cid:14	O
)	O
i=1	O
q	O
(	O
z	O
)	O
=	O
qi	O
(	O
zi	O
)	O
.	O
(	O
10.5	O
)	O
10.1.	O
variational	B
inference	I
465	O
it	O
should	O
be	O
emphasized	O
that	O
we	O
are	O
making	O
no	O
further	O
assumptions	O
about	O
the	O
distri-	O
bution	O
.	O
in	O
particular	O
,	O
we	O
place	O
no	O
restriction	O
on	O
the	O
functional	B
forms	O
of	O
the	O
individual	O
factors	O
qi	O
(	O
zi	O
)	O
.	O
this	O
factorized	O
form	O
of	O
variational	B
inference	I
corresponds	O
to	O
an	O
ap-	O
proximation	B
framework	O
developed	O
in	O
physics	O
called	O
mean	B
ﬁeld	I
theory	I
(	O
parisi	O
,	O
1988	O
)	O
.	O
amongst	O
all	O
distributions	O
q	O
(	O
z	O
)	O
having	O
the	O
form	O
(	O
10.5	O
)	O
,	O
we	O
now	O
seek	O
that	O
distri-	O
bution	O
for	O
which	O
the	O
lower	B
bound	I
l	O
(	O
q	O
)	O
is	O
largest	O
.	O
we	O
therefore	O
wish	O
to	O
make	O
a	O
free	O
form	O
(	O
variational	B
)	O
optimization	O
of	O
l	O
(	O
q	O
)	O
with	O
respect	O
to	O
all	O
of	O
the	O
distributions	O
qi	O
(	O
zi	O
)	O
,	O
which	O
we	O
do	O
by	O
optimizing	O
with	O
respect	O
to	O
each	O
of	O
the	O
factors	O
in	O
turn	O
.	O
to	O
achieve	O
this	O
,	O
we	O
ﬁrst	O
substitute	O
(	O
10.5	O
)	O
into	O
(	O
10.3	O
)	O
and	O
then	O
dissect	O
out	O
the	O
dependence	O
on	O
one	O
of	O
the	O
factors	O
qj	O
(	O
zj	O
)	O
.	O
denoting	O
qj	O
(	O
zj	O
)	O
by	O
simply	O
qj	O
to	O
keep	O
the	O
notation	O
uncluttered	O
,	O
we	O
then	O
obtain	O
l	O
(	O
q	O
)	O
=	O
(	O
cid:24	O
)	O
(	O
cid:6	O
)	O
(	O
cid:14	O
)	O
(	O
cid:2	O
)	O
(	O
cid:24	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:14	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
qj	O
ln	O
(	O
cid:4	O
)	O
p	O
(	O
x	O
,	O
zj	O
)	O
dzj	O
−	O
where	O
we	O
have	O
deﬁned	O
a	O
new	O
distribution	O
(	O
cid:4	O
)	O
p	O
(	O
x	O
,	O
zj	O
)	O
by	O
the	O
relation	O
ln	O
(	O
cid:4	O
)	O
p	O
(	O
x	O
,	O
zj	O
)	O
=	O
ei	O
(	O
cid:9	O
)	O
=j	O
[	O
ln	O
p	O
(	O
x	O
,	O
z	O
)	O
]	O
+	O
const	O
.	O
(	O
10.7	O
)	O
here	O
the	O
notation	O
ei	O
(	O
cid:9	O
)	O
=j	O
[	O
···	O
]	O
denotes	O
an	O
expectation	B
with	O
respect	O
to	O
the	O
q	O
distributions	O
over	O
all	O
variables	O
zi	O
for	O
i	O
(	O
cid:9	O
)	O
=	O
j	O
,	O
so	O
that	O
(	O
cid:25	O
)	O
(	O
cid:25	O
)	O
ln	O
p	O
(	O
x	O
,	O
z	O
)	O
−	O
qj	O
ln	O
qj	O
dzj	O
+	O
const	O
qj	O
ln	O
qj	O
dzj	O
+	O
const	O
ln	O
p	O
(	O
x	O
,	O
z	O
)	O
qi	O
dzi	O
dzj	O
−	O
ln	O
qi	O
dz	O
i	O
qi	O
i	O
qj	O
=	O
=	O
(	O
10.6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:14	O
)	O
i	O
(	O
cid:9	O
)	O
=j	O
i	O
(	O
cid:9	O
)	O
=j	O
(	O
cid:6	O
)	O
ei	O
(	O
cid:9	O
)	O
=j	O
[	O
ln	O
p	O
(	O
x	O
,	O
z	O
)	O
]	O
=	O
ln	O
p	O
(	O
x	O
,	O
z	O
)	O
qi	O
dzi	O
.	O
(	O
10.8	O
)	O
now	O
suppose	O
we	O
keep	O
the	O
{	O
qi	O
(	O
cid:9	O
)	O
=j	O
}	O
ﬁxed	O
and	O
maximize	O
l	O
(	O
q	O
)	O
in	O
(	O
10.6	O
)	O
with	O
re-	O
spect	O
to	O
all	O
possible	O
forms	O
for	O
the	O
distribution	O
qj	O
(	O
zj	O
)	O
.	O
this	O
is	O
easily	O
done	O
by	O
rec-	O
ognizing	O
that	O
(	O
10.6	O
)	O
is	O
a	O
negative	O
kullback-leibler	O
divergence	O
between	O
qj	O
(	O
zj	O
)	O
and	O
(	O
cid:4	O
)	O
p	O
(	O
x	O
,	O
zj	O
)	O
.	O
thus	O
maximizing	O
(	O
10.6	O
)	O
is	O
equivalent	O
to	O
minimizing	O
the	O
kullback-leibler	O
leonhard	O
euler	O
1707–1783	O
euler	O
was	O
a	O
swiss	O
mathematician	O
and	O
physicist	O
who	O
worked	O
in	O
st.	O
petersburg	O
and	O
berlin	O
and	O
who	O
is	O
widely	O
considered	O
to	O
be	O
one	O
of	O
the	O
greatest	O
mathematicians	O
of	O
all	O
time	O
.	O
he	O
is	O
certainly	O
the	O
most	O
proliﬁc	O
,	O
and	O
his	O
collected	O
works	O
ﬁll	O
75	O
volumes	O
.	O
amongst	O
his	O
many	O
contributions	O
,	O
he	O
formulated	O
the	O
modern	O
theory	B
of	O
the	O
function	O
,	O
he	O
developed	O
(	O
together	O
with	O
lagrange	O
)	O
the	O
calculus	B
of	I
variations	I
,	O
and	O
he	O
discovered	O
the	O
formula	O
eiπ	O
=	O
−1	O
,	O
which	O
relates	O
four	O
of	O
the	O
most	O
important	O
numbers	O
in	O
mathematics	O
.	O
during	O
the	O
last	O
17	O
years	O
of	O
his	O
life	O
,	O
he	O
was	O
almost	O
totally	O
blind	O
,	O
and	O
yet	O
he	O
pro-	O
duced	O
nearly	O
half	O
of	O
his	O
results	O
during	O
this	O
period	O
.	O
466	O
10.	O
approximate	O
inference	B
divergence	O
,	O
and	O
the	O
minimum	O
occurs	O
when	O
qj	O
(	O
zj	O
)	O
=	O
(	O
cid:4	O
)	O
p	O
(	O
x	O
,	O
zj	O
)	O
.	O
thus	O
we	O
obtain	O
a	O
general	O
expression	O
for	O
the	O
optimal	O
solution	O
q	O
(	O
cid:1	O
)	O
j	O
(	O
zj	O
)	O
given	O
by	O
ln	O
q	O
(	O
cid:1	O
)	O
j	O
(	O
zj	O
)	O
=	O
ei	O
(	O
cid:9	O
)	O
=j	O
[	O
ln	O
p	O
(	O
x	O
,	O
z	O
)	O
]	O
+	O
const	O
.	O
(	O
10.9	O
)	O
it	O
is	O
worth	O
taking	O
a	O
few	O
moments	O
to	O
study	O
the	O
form	O
of	O
this	O
solution	O
as	O
it	O
provides	O
the	O
basis	O
for	O
applications	O
of	O
variational	B
methods	O
.	O
it	O
says	O
that	O
the	O
log	O
of	O
the	O
optimal	O
so-	O
lution	O
for	O
factor	O
qj	O
is	O
obtained	O
simply	O
by	O
considering	O
the	O
log	O
of	O
the	O
joint	O
distribution	O
over	O
all	O
hidden	O
and	O
visible	O
variables	O
and	O
then	O
taking	O
the	O
expectation	B
with	O
respect	O
to	O
all	O
of	O
the	O
other	O
factors	O
{	O
qi	O
}	O
for	O
i	O
(	O
cid:9	O
)	O
=	O
j	O
.	O
(	O
cid:6	O
)	O
the	O
additive	O
constant	O
in	O
(	O
10.9	O
)	O
is	O
set	O
by	O
normalizing	O
the	O
distribution	O
q	O
(	O
cid:1	O
)	O
thus	O
if	O
we	O
take	O
the	O
exponential	O
of	O
both	O
sides	O
and	O
normalize	O
,	O
we	O
have	O
exp	O
(	O
ei	O
(	O
cid:9	O
)	O
=j	O
[	O
ln	O
p	O
(	O
x	O
,	O
z	O
)	O
]	O
)	O
j	O
(	O
zj	O
)	O
.	O
q	O
(	O
cid:1	O
)	O
j	O
(	O
zj	O
)	O
=	O
.	O
exp	O
(	O
ei	O
(	O
cid:9	O
)	O
=j	O
[	O
ln	O
p	O
(	O
x	O
,	O
z	O
)	O
]	O
)	O
dzj	O
in	O
practice	O
,	O
we	O
shall	O
ﬁnd	O
it	O
more	O
convenient	O
to	O
work	O
with	O
the	O
form	O
(	O
10.9	O
)	O
and	O
then	O
re-	O
instate	O
the	O
normalization	O
constant	O
(	O
where	O
required	O
)	O
by	O
inspection	O
.	O
this	O
will	O
become	O
clear	O
from	O
subsequent	O
examples	O
.	O
the	O
set	O
of	O
equations	O
given	O
by	O
(	O
10.9	O
)	O
for	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
represent	O
a	O
set	O
of	O
con-	O
sistency	O
conditions	O
for	O
the	O
maximum	O
of	O
the	O
lower	B
bound	I
subject	O
to	O
the	O
factorization	B
constraint	O
.	O
however	O
,	O
they	O
do	O
not	O
represent	O
an	O
explicit	O
solution	O
because	O
the	O
expres-	O
sion	B
on	O
the	O
right-hand	O
side	O
of	O
(	O
10.9	O
)	O
for	O
the	O
optimum	O
q	O
(	O
cid:1	O
)	O
j	O
(	O
zj	O
)	O
depends	O
on	O
expectations	O
computed	O
with	O
respect	O
to	O
the	O
other	O
factors	O
qi	O
(	O
zi	O
)	O
for	O
i	O
(	O
cid:9	O
)	O
=	O
j.	O
we	O
will	O
therefore	O
seek	O
a	O
consistent	B
solution	O
by	O
ﬁrst	O
initializing	O
all	O
of	O
the	O
factors	O
qi	O
(	O
zi	O
)	O
appropriately	O
and	O
then	O
cycling	O
through	O
the	O
factors	O
and	O
replacing	O
each	O
in	O
turn	O
with	O
a	O
revised	O
estimate	O
given	O
by	O
the	O
right-hand	O
side	O
of	O
(	O
10.9	O
)	O
evaluated	O
using	O
the	O
current	O
estimates	O
for	O
all	O
of	O
the	O
other	O
factors	O
.	O
convergence	O
is	O
guaranteed	O
because	O
bound	O
is	O
convex	O
with	O
respect	O
to	O
each	O
of	O
the	O
factors	O
qi	O
(	O
zi	O
)	O
(	O
boyd	O
and	O
vandenberghe	O
,	O
2004	O
)	O
.	O
10.1.2	O
properties	O
of	O
factorized	O
approximations	O
our	O
approach	O
to	O
variational	B
inference	I
is	O
based	O
on	O
a	O
factorized	O
approximation	O
to	O
the	O
true	O
posterior	O
distribution	O
.	O
let	O
us	O
consider	O
for	O
a	O
moment	O
the	O
problem	O
of	O
approx-	O
imating	O
a	O
general	O
distribution	O
by	O
a	O
factorized	B
distribution	I
.	O
to	O
begin	O
with	O
,	O
we	O
discuss	O
the	O
problem	O
of	O
approximating	O
a	O
gaussian	O
distribution	O
using	O
a	O
factorized	O
gaussian	O
,	O
which	O
will	O
provide	O
useful	O
insight	O
into	O
the	O
types	O
of	O
inaccuracy	O
introduced	O
in	O
using	O
factorized	O
approximations	O
.	O
consider	O
a	O
gaussian	O
distribution	O
p	O
(	O
z	O
)	O
=	O
n	O
(	O
z|µ	O
,	O
λ	O
−1	O
)	O
over	O
two	O
correlated	O
variables	O
z	O
=	O
(	O
z1	O
,	O
z2	O
)	O
in	O
which	O
the	O
mean	B
and	O
precision	O
have	O
elements	O
(	O
cid:15	O
)	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
(	O
10.10	O
)	O
µ	O
=	O
,	O
λ	O
=	O
µ1	O
µ2	O
λ11	O
λ12	O
λ21	O
λ22	O
and	O
λ21	O
=	O
λ12	O
due	O
to	O
the	O
symmetry	O
of	O
the	O
precision	B
matrix	I
.	O
now	O
suppose	O
we	O
wish	O
to	O
approximate	O
this	O
distribution	O
using	O
a	O
factorized	O
gaussian	O
of	O
the	O
form	O
q	O
(	O
z	O
)	O
=	O
q1	O
(	O
z1	O
)	O
q2	O
(	O
z2	O
)	O
.	O
we	O
ﬁrst	O
apply	O
the	O
general	O
result	O
(	O
10.9	O
)	O
to	O
ﬁnd	O
an	O
expression	O
for	O
the	O
10.1.	O
variational	B
inference	I
467	O
optimal	O
factor	O
q	O
(	O
cid:1	O
)	O
1	O
(	O
z1	O
)	O
.	O
in	O
doing	O
so	O
it	O
is	O
useful	O
to	O
note	O
that	O
on	O
the	O
right-hand	O
side	O
we	O
only	O
need	O
to	O
retain	O
those	O
terms	O
that	O
have	O
some	O
functional	B
dependence	O
on	O
z1	O
because	O
all	O
other	O
terms	O
can	O
be	O
absorbed	O
into	O
the	O
normalization	O
constant	O
.	O
thus	O
we	O
have	O
ln	O
q	O
(	O
cid:1	O
)	O
1	O
(	O
z1	O
)	O
=	O
ez2	O
[	O
ln	O
p	O
(	O
z	O
)	O
]	O
+	O
const	O
(	O
cid:29	O
)	O
=	O
ez2	O
=	O
−1	O
2	O
z2	O
(	O
cid:30	O
)	O
(	O
z1	O
−	O
µ1	O
)	O
2λ11	O
−	O
(	O
z1	O
−	O
µ1	O
)	O
λ12	O
(	O
z2	O
−	O
µ2	O
)	O
−1	O
2	O
1λ11	O
+	O
z1µ1λ11	O
−	O
z1λ12	O
(	O
e	O
[	O
z2	O
]	O
−	O
µ2	O
)	O
+	O
const	O
.	O
+	O
const	O
(	O
10.11	O
)	O
next	O
we	O
observe	O
that	O
the	O
right-hand	O
side	O
of	O
this	O
expression	O
is	O
a	O
quadratic	O
function	O
of	O
z1	O
,	O
and	O
so	O
we	O
can	O
identify	O
q	O
(	O
cid:1	O
)	O
(	O
z1	O
)	O
as	O
a	O
gaussian	O
distribution	O
.	O
it	O
is	O
worth	O
emphasizing	O
that	O
we	O
did	O
not	O
assume	O
that	O
q	O
(	O
zi	O
)	O
is	O
gaussian	O
,	O
but	O
rather	O
we	O
derived	O
this	O
result	O
by	O
variational	B
optimization	O
of	O
the	O
kl	O
divergence	O
over	O
all	O
possible	O
distributions	O
q	O
(	O
zi	O
)	O
.	O
note	O
also	O
that	O
we	O
do	O
not	O
need	O
to	O
consider	O
the	O
additive	O
constant	O
in	O
(	O
10.9	O
)	O
explicitly	O
because	O
it	O
represents	O
the	O
normalization	O
constant	O
that	O
can	O
be	O
found	O
at	O
the	O
end	O
by	O
inspection	O
if	O
required	O
.	O
using	O
the	O
technique	O
of	O
completing	B
the	I
square	I
,	O
we	O
can	O
identify	O
the	O
mean	B
and	O
precision	O
of	O
this	O
gaussian	O
,	O
giving	O
section	O
2.3.1	O
q	O
(	O
cid:1	O
)	O
(	O
z1	O
)	O
=	O
n	O
(	O
z1|m1	O
,	O
λ−1	O
11	O
)	O
where	O
m1	O
=	O
µ1	O
−	O
λ−1	O
11	O
λ12	O
(	O
e	O
[	O
z2	O
]	O
−	O
µ2	O
)	O
.	O
2	O
(	O
z2	O
)	O
is	O
also	O
gaussian	O
and	O
can	O
be	O
written	O
as	O
by	O
symmetry	O
,	O
q	O
(	O
cid:1	O
)	O
2	O
(	O
z2	O
)	O
=	O
n	O
(	O
z2|m2	O
,	O
λ−1	O
q	O
(	O
cid:1	O
)	O
22	O
)	O
(	O
10.12	O
)	O
(	O
10.13	O
)	O
(	O
10.14	O
)	O
in	O
which	O
m2	O
=	O
µ2	O
−	O
λ−1	O
22	O
λ21	O
(	O
e	O
[	O
z1	O
]	O
−	O
µ1	O
)	O
.	O
(	O
10.15	O
)	O
note	O
that	O
these	O
solutions	O
are	O
coupled	O
,	O
so	O
that	O
q	O
(	O
cid:1	O
)	O
(	O
z1	O
)	O
depends	O
on	O
expectations	O
com-	O
puted	O
with	O
respect	O
to	O
q	O
(	O
cid:1	O
)	O
(	O
z2	O
)	O
and	O
vice	O
versa	O
.	O
in	O
general	O
,	O
we	O
address	O
this	O
by	O
treating	O
the	O
variational	B
solutions	O
as	O
re-estimation	O
equations	O
and	O
cycling	O
through	O
the	O
variables	O
in	O
turn	O
updating	O
them	O
until	O
some	O
convergence	O
criterion	O
is	O
satisﬁed	O
.	O
we	O
shall	O
see	O
an	O
example	O
of	O
this	O
shortly	O
.	O
here	O
,	O
however	O
,	O
we	O
note	O
that	O
the	O
problem	O
is	O
sufﬁciently	O
simple	O
that	O
a	O
closed	O
form	O
solution	O
can	O
be	O
found	O
.	O
in	O
particular	O
,	O
because	O
e	O
[	O
z1	O
]	O
=	O
m1	O
and	O
e	O
[	O
z2	O
]	O
=	O
m2	O
,	O
we	O
see	O
that	O
the	O
two	O
equations	O
are	O
satisﬁed	O
if	O
we	O
take	O
e	O
[	O
z1	O
]	O
=	O
µ1	O
and	O
e	O
[	O
z2	O
]	O
=	O
µ2	O
,	O
and	O
it	O
is	O
easily	O
shown	O
that	O
this	O
is	O
the	O
only	O
solution	O
provided	O
the	O
dis-	O
tribution	O
is	O
nonsingular	O
.	O
this	O
result	O
is	O
illustrated	O
in	O
figure	O
10.2	O
(	O
a	O
)	O
.	O
we	O
see	O
that	O
the	O
mean	B
is	O
correctly	O
captured	O
but	O
that	O
the	O
variance	B
of	O
q	O
(	O
z	O
)	O
is	O
controlled	O
by	O
the	O
direction	O
of	O
smallest	O
variance	B
of	O
p	O
(	O
z	O
)	O
,	O
and	O
that	O
the	O
variance	B
along	O
the	O
orthogonal	O
direction	O
is	O
signiﬁcantly	O
under-estimated	O
.	O
it	O
is	O
a	O
general	O
result	O
that	O
a	O
factorized	O
variational	O
ap-	O
proximation	B
tends	O
to	O
give	O
approximations	O
to	O
the	O
posterior	O
distribution	O
that	O
are	O
too	O
compact	O
.	O
by	O
way	O
of	O
comparison	O
,	O
suppose	O
instead	O
that	O
we	O
had	O
been	O
minimizing	O
the	O
reverse	O
kullback-leibler	O
divergence	O
kl	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
.	O
as	O
we	O
shall	O
see	O
,	O
this	O
form	O
of	O
kl	O
divergence	O
exercise	O
10.2	O
468	O
10.	O
approximate	O
inference	B
figure	O
10.2	O
comparison	O
of	O
the	O
the	O
two	O
alternative	O
forms	O
for	O
kullback-leibler	O
divergence	O
.	O
the	O
green	O
contours	O
corresponding	O
to	O
1	O
,	O
2	O
,	O
and	O
3	O
standard	O
deviations	O
for	O
a	O
correlated	O
gaussian	O
distribution	O
p	O
(	O
z	O
)	O
over	O
two	O
variables	O
z1	O
and	O
z2	O
,	O
and	O
the	O
red	O
contours	O
represent	O
the	O
corresponding	O
levels	O
for	O
an	O
q	O
(	O
z	O
)	O
approximating	O
over	O
the	O
same	O
variables	O
given	O
by	O
the	O
product	O
of	O
two	O
independent	B
univariate	O
gaussian	O
distributions	O
whose	O
parameters	O
are	O
obtained	O
by	O
minimization	O
of	O
the	O
kullback-	O
leibler	O
divergence	O
kl	O
(	O
q	O
(	O
cid:3	O
)	O
p	O
)	O
,	O
and	O
the	O
reverse	O
kullback-leibler	O
(	O
b	O
)	O
divergence	O
kl	O
(	O
p	O
(	O
cid:3	O
)	O
q	O
)	O
.	O
distribution	O
(	O
a	O
)	O
1	O
z2	O
0.5	O
0	O
0	O
1	O
z2	O
0.5	O
0	O
0	O
z1	O
1	O
0.5	O
(	O
a	O
)	O
z1	O
1	O
0.5	O
(	O
b	O
)	O
is	O
used	O
in	O
an	O
alternative	O
approximate	O
inference	B
framework	O
called	O
expectation	B
prop-	O
agation	O
.	O
we	O
therefore	O
consider	O
the	O
general	O
problem	O
of	O
minimizing	O
kl	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
when	O
q	O
(	O
z	O
)	O
is	O
a	O
factorized	O
approximation	O
of	O
the	O
form	O
(	O
10.5	O
)	O
.	O
the	O
kl	O
divergence	O
can	O
then	O
be	O
written	O
in	O
the	O
form	O
kl	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
=	O
−	O
p	O
(	O
z	O
)	O
ln	O
qi	O
(	O
zi	O
)	O
dz	O
+	O
const	O
(	O
10.16	O
)	O
section	O
10.7	O
exercise	O
10.3	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:31	O
)	O
m	O
(	O
cid:2	O
)	O
i=1	O
(	O
cid:14	O
)	O
i	O
(	O
cid:9	O
)	O
=j	O
where	O
the	O
constant	O
term	O
is	O
simply	O
the	O
entropy	B
of	O
p	O
(	O
z	O
)	O
and	O
so	O
does	O
not	O
depend	O
on	O
q	O
(	O
z	O
)	O
.	O
we	O
can	O
now	O
optimize	O
with	O
respect	O
to	O
each	O
of	O
the	O
factors	O
qj	O
(	O
zj	O
)	O
,	O
which	O
is	O
easily	O
done	O
using	O
a	O
lagrange	O
multiplier	O
to	O
give	O
q	O
(	O
cid:1	O
)	O
j	O
(	O
zj	O
)	O
=	O
p	O
(	O
z	O
)	O
dzi	O
=	O
p	O
(	O
zj	O
)	O
.	O
(	O
10.17	O
)	O
in	O
this	O
case	O
,	O
we	O
ﬁnd	O
that	O
the	O
optimal	O
solution	O
for	O
qj	O
(	O
zj	O
)	O
is	O
just	O
given	O
by	O
the	O
corre-	O
sponding	O
marginal	B
distribution	O
of	O
p	O
(	O
z	O
)	O
.	O
note	O
that	O
this	O
is	O
a	O
closed-form	O
solution	O
and	O
so	O
does	O
not	O
require	O
iteration	O
.	O
to	O
apply	O
this	O
result	O
to	O
the	O
illustrative	O
example	O
of	O
a	O
gaussian	O
distribution	O
p	O
(	O
z	O
)	O
over	O
a	O
vector	O
z	O
we	O
can	O
use	O
(	O
2.98	O
)	O
,	O
which	O
gives	O
the	O
result	O
shown	O
in	O
figure	O
10.2	O
(	O
b	O
)	O
.	O
we	O
see	O
that	O
once	O
again	O
the	O
mean	B
of	O
the	O
approximation	O
is	O
correct	O
,	O
but	O
that	O
it	O
places	O
signiﬁcant	O
probability	B
mass	O
in	O
regions	O
of	O
variable	O
space	O
that	O
have	O
very	O
low	O
probabil-	O
ity	O
.	O
the	O
difference	O
between	O
these	O
two	O
results	O
can	O
be	O
understood	O
by	O
noting	O
that	O
there	O
is	O
a	O
large	O
positive	O
contribution	O
to	O
the	O
kullback-leibler	O
divergence	O
kl	O
(	O
q	O
(	O
cid:5	O
)	O
p	O
)	O
=	O
−	O
q	O
(	O
z	O
)	O
ln	O
dz	O
(	O
10.18	O
)	O
(	O
cid:6	O
)	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
p	O
(	O
z	O
)	O
q	O
(	O
z	O
)	O
10.1.	O
variational	B
inference	I
469	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
10.3	O
another	O
comparison	O
of	O
the	O
two	O
alternative	O
forms	O
for	O
the	O
kullback-leibler	O
divergence	O
.	O
(	O
a	O
)	O
the	O
blue	O
contours	O
show	O
a	O
bimodal	O
distribution	O
p	O
(	O
z	O
)	O
given	O
by	O
a	O
mixture	O
of	O
two	O
gaussians	O
,	O
and	O
the	O
red	O
contours	O
correspond	O
to	O
the	O
single	O
gaussian	O
distribution	O
q	O
(	O
z	O
)	O
that	O
best	O
approximates	O
p	O
(	O
z	O
)	O
in	O
the	O
sense	O
of	O
minimizing	O
the	O
kullback-	O
leibler	O
divergence	O
kl	O
(	O
p	O
(	O
cid:3	O
)	O
q	O
)	O
.	O
(	O
b	O
)	O
as	O
in	O
(	O
a	O
)	O
but	O
now	O
the	O
red	O
contours	O
correspond	O
to	O
a	O
gaussian	O
distribution	O
q	O
(	O
z	O
)	O
found	O
by	O
numerical	O
minimization	O
of	O
the	O
kullback-leibler	O
divergence	O
kl	O
(	O
q	O
(	O
cid:3	O
)	O
p	O
)	O
.	O
(	O
c	O
)	O
as	O
in	O
(	O
b	O
)	O
but	O
showing	O
a	O
different	O
local	B
minimum	I
of	O
the	O
kullback-leibler	O
divergence	O
.	O
from	O
regions	O
of	O
z	O
space	O
in	O
which	O
p	O
(	O
z	O
)	O
is	O
near	O
zero	O
unless	O
q	O
(	O
z	O
)	O
is	O
also	O
close	O
to	O
zero	O
.	O
thus	O
minimizing	O
this	O
form	O
of	O
kl	O
divergence	O
leads	O
to	O
distributions	O
q	O
(	O
z	O
)	O
that	O
avoid	O
regions	O
in	O
which	O
p	O
(	O
z	O
)	O
is	O
small	O
.	O
conversely	O
,	O
the	O
kullback-leibler	O
divergence	O
kl	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
is	O
minimized	O
by	O
distributions	O
q	O
(	O
z	O
)	O
that	O
are	O
nonzero	O
in	O
regions	O
where	O
p	O
(	O
z	O
)	O
is	O
nonzero	O
.	O
we	O
can	O
gain	O
further	O
insight	O
into	O
the	O
different	O
behaviour	O
of	O
the	O
two	O
kl	O
diver-	O
gences	O
if	O
we	O
consider	O
approximating	O
a	O
multimodal	O
distribution	O
by	O
a	O
unimodal	O
one	O
,	O
as	O
illustrated	O
in	O
figure	O
10.3.	O
in	O
practical	O
applications	O
,	O
the	O
true	O
posterior	O
distri-	O
bution	O
will	O
often	O
be	O
multimodal	O
,	O
with	O
most	O
of	O
the	O
posterior	O
mass	O
concentrated	O
in	O
some	O
number	O
of	O
relatively	O
small	O
regions	O
of	O
parameter	O
space	O
.	O
these	O
multiple	O
modes	O
may	O
arise	O
through	O
nonidentiﬁability	B
in	O
the	O
latent	O
space	O
or	O
through	O
complex	O
nonlin-	O
ear	O
dependence	O
on	O
the	O
parameters	O
.	O
both	O
types	O
of	O
multimodality	B
were	O
encountered	O
in	O
chapter	O
9	O
in	O
the	O
context	O
of	O
gaussian	O
mixtures	O
,	O
where	O
they	O
manifested	O
themselves	O
as	O
multiple	O
maxima	O
in	O
the	O
likelihood	B
function	I
,	O
and	O
a	O
variational	B
treatment	O
based	O
on	O
the	O
minimization	O
of	O
kl	O
(	O
q	O
(	O
cid:5	O
)	O
p	O
)	O
will	O
tend	O
to	O
ﬁnd	O
one	O
of	O
these	O
modes	O
.	O
by	O
contrast	O
,	O
if	O
we	O
were	O
to	O
minimize	O
kl	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
,	O
the	O
resulting	O
approximations	O
would	O
average	O
across	O
all	O
of	O
the	O
modes	O
and	O
,	O
in	O
the	O
context	O
of	O
the	O
mixture	B
model	I
,	O
would	O
lead	O
to	O
poor	O
predictive	O
distributions	O
(	O
because	O
the	O
average	O
of	O
two	O
good	O
parameter	O
values	O
is	O
typically	O
itself	O
not	O
a	O
good	O
parameter	O
value	O
)	O
.	O
it	O
is	O
possible	O
to	O
make	O
use	O
of	O
kl	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
to	O
deﬁne	O
a	O
useful	O
inference	B
procedure	O
,	O
but	O
this	O
requires	O
a	O
rather	O
different	O
approach	O
to	O
the	O
one	O
discussed	O
here	O
,	O
and	O
will	O
be	O
considered	O
in	O
detail	O
when	O
we	O
discuss	O
expectation	B
propagation	I
.	O
the	O
two	O
forms	O
of	O
kullback-leibler	O
divergence	O
are	O
members	O
of	O
the	O
alpha	O
family	O
section	O
10.7	O
470	O
10.	O
approximate	O
inference	B
(	O
cid:16	O
)	O
(	O
cid:25	O
)	O
(	O
cid:11	O
)	O
(	O
cid:11	O
)	O
(	O
cid:15	O
)	O
(	O
cid:6	O
)	O
of	O
divergences	O
(	O
ali	O
and	O
silvey	O
,	O
1966	O
;	O
amari	O
,	O
1985	O
;	O
minka	O
,	O
2005	O
)	O
deﬁned	O
by	O
4	O
1	O
−	O
dα	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
=	O
p	O
(	O
x	O
)	O
(	O
1+α	O
)	O
/2q	O
(	O
x	O
)	O
(	O
1−α	O
)	O
/2	O
dx	O
1	O
−	O
α2	O
(	O
10.19	O
)	O
where	O
−∞	O
<	O
α	O
<	O
∞	O
is	O
a	O
continuous	O
parameter	O
.	O
the	O
kullback-leibler	O
divergence	O
kl	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
corresponds	O
to	O
the	O
limit	O
α	O
→	O
1	O
,	O
whereas	O
kl	O
(	O
q	O
(	O
cid:5	O
)	O
p	O
)	O
corresponds	O
to	O
the	O
limit	O
α	O
→	O
−1	O
.	O
for	O
all	O
values	O
of	O
α	O
we	O
have	O
dα	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
(	O
cid:2	O
)	O
0	O
,	O
with	O
equality	O
if	O
,	O
and	O
only	O
if	O
,	O
p	O
(	O
x	O
)	O
=	O
q	O
(	O
x	O
)	O
.	O
suppose	O
p	O
(	O
x	O
)	O
is	O
a	O
ﬁxed	O
distribution	O
,	O
and	O
we	O
minimize	O
dα	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
with	O
respect	O
to	O
some	O
set	O
of	O
distributions	O
q	O
(	O
x	O
)	O
.	O
then	O
for	O
α	O
(	O
cid:1	O
)	O
−1	O
the	O
divergence	O
is	O
zero	O
forcing	O
,	O
so	O
that	O
any	O
values	O
of	O
x	O
for	O
which	O
p	O
(	O
x	O
)	O
=	O
0	O
will	O
have	O
q	O
(	O
x	O
)	O
=	O
0	O
,	O
and	O
typically	O
q	O
(	O
x	O
)	O
will	O
under-estimate	O
the	O
support	O
of	O
p	O
(	O
x	O
)	O
and	O
will	O
tend	O
to	O
seek	O
the	O
mode	O
with	O
the	O
largest	O
mass	O
.	O
conversely	O
for	O
α	O
(	O
cid:2	O
)	O
1	O
the	O
divergence	O
is	O
zero-avoiding	O
,	O
so	O
that	O
values	O
of	O
x	O
for	O
which	O
p	O
(	O
x	O
)	O
>	O
0	O
will	O
have	O
q	O
(	O
x	O
)	O
>	O
0	O
,	O
and	O
typically	O
q	O
(	O
x	O
)	O
will	O
stretch	O
to	O
cover	O
all	O
of	O
p	O
(	O
x	O
)	O
,	O
and	O
will	O
over-estimate	O
the	O
support	O
of	O
p	O
(	O
x	O
)	O
.	O
when	O
α	O
=	O
0	O
we	O
obtain	O
a	O
symmetric	O
divergence	O
that	O
is	O
linearly	O
related	O
to	O
the	O
hellinger	O
distance	O
given	O
by	O
dh	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
=	O
p	O
(	O
x	O
)	O
1/2	O
−	O
q	O
(	O
x	O
)	O
1/2	O
dx	O
.	O
(	O
10.20	O
)	O
(	O
cid:6	O
)	O
(	O
cid:10	O
)	O
the	O
square	O
root	O
of	O
the	O
hellinger	O
distance	O
is	O
a	O
valid	O
distance	O
metric	O
.	O
10.1.3	O
example	O
:	O
the	O
univariate	O
gaussian	O
we	O
now	O
illustrate	O
the	O
factorized	O
variational	O
approximation	O
using	O
a	O
gaussian	O
dis-	O
tribution	O
over	O
a	O
single	O
variable	O
x	O
(	O
mackay	O
,	O
2003	O
)	O
.	O
our	O
goal	O
is	O
to	O
infer	O
the	O
posterior	O
distribution	O
for	O
the	O
mean	B
µ	O
and	O
precision	O
τ	O
,	O
given	O
a	O
data	O
set	O
d	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
of	O
observed	O
values	O
of	O
x	O
which	O
are	O
assumed	O
to	O
be	O
drawn	O
independently	O
from	O
the	O
gaus-	O
sian	O
.	O
the	O
likelihood	B
function	I
is	O
given	O
by	O
p	O
(	O
d|µ	O
,	O
τ	O
)	O
=	O
(	O
xn	O
−	O
µ	O
)	O
2	O
.	O
(	O
10.21	O
)	O
(	O
cid:24	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
−	O
τ	O
2	O
τ	O
2π	O
(	O
cid:18	O
)	O
n/2	O
(	O
cid:17	O
)	O
p	O
(	O
µ|τ	O
)	O
=	O
n	O
(	O
cid:10	O
)	O
exp	O
we	O
now	O
introduce	O
conjugate	B
prior	I
distributions	O
for	O
µ	O
and	O
τ	O
given	O
by	O
µ|µ0	O
,	O
(	O
λ0τ	O
)	O
−1	O
(	O
10.22	O
)	O
(	O
10.23	O
)	O
where	O
gam	O
(	O
τ|a0	O
,	O
b0	O
)	O
is	O
the	O
gamma	B
distribution	I
deﬁned	O
by	O
(	O
2.146	O
)	O
.	O
together	O
these	O
distributions	O
constitute	O
a	O
gaussian-gamma	O
conjugate	B
prior	I
distribution	O
.	O
p	O
(	O
τ	O
)	O
=	O
gam	O
(	O
τ|a0	O
,	O
b0	O
)	O
for	O
this	O
simple	O
problem	O
the	O
posterior	O
distribution	O
can	O
be	O
found	O
exactly	O
,	O
and	O
again	O
takes	O
the	O
form	O
of	O
a	O
gaussian-gamma	O
distribution	O
.	O
however	O
,	O
for	O
tutorial	O
purposes	O
we	O
will	O
consider	O
a	O
factorized	O
variational	O
approximation	O
to	O
the	O
posterior	O
distribution	O
given	O
by	O
q	O
(	O
µ	O
,	O
τ	O
)	O
=	O
qµ	O
(	O
µ	O
)	O
qτ	O
(	O
τ	O
)	O
.	O
(	O
10.24	O
)	O
exercise	O
10.6	O
section	O
2.3.6	O
exercise	O
2.44	O
10.1.	O
variational	B
inference	I
471	O
note	O
that	O
the	O
true	O
posterior	O
distribution	O
does	O
not	O
factorize	O
in	O
this	O
way	O
.	O
the	O
optimum	O
factors	O
qµ	O
(	O
µ	O
)	O
and	O
qτ	O
(	O
τ	O
)	O
can	O
be	O
obtained	O
from	O
the	O
general	O
result	O
(	O
10.9	O
)	O
as	O
follows	O
.	O
for	O
qµ	O
(	O
µ	O
)	O
we	O
have	O
ln	O
q	O
(	O
cid:1	O
)	O
(	O
cid:24	O
)	O
µ	O
(	O
µ	O
)	O
=	O
eτ	O
[	O
ln	O
p	O
(	O
d|µ	O
,	O
τ	O
)	O
+	O
ln	O
p	O
(	O
µ|τ	O
)	O
]	O
+	O
const	O
(	O
xn	O
−	O
µ	O
)	O
2	O
(	O
cid:25	O
)	O
completing	B
the	I
square	I
over	O
µ	O
we	O
see	O
that	O
qµ	O
(	O
µ	O
)	O
is	O
a	O
gaussian	O
n	O
(	O
cid:10	O
)	O
λ0	O
(	O
µ	O
−	O
µ0	O
)	O
2	O
+	O
=	O
−	O
e	O
[	O
τ	O
]	O
2	O
n	O
(	O
cid:2	O
)	O
n=1	O
(	O
cid:11	O
)	O
+	O
const	O
.	O
(	O
10.25	O
)	O
µ|µn	O
,	O
λ	O
−1	O
n	O
with	O
exercise	O
10.7	O
mean	B
and	O
precision	O
given	O
by	O
µn	O
=	O
λ0µ0	O
+	O
n	O
x	O
λ0	O
+	O
n	O
λn	O
=	O
(	O
λ0	O
+	O
n	O
)	O
e	O
[	O
τ	O
]	O
.	O
(	O
10.26	O
)	O
(	O
10.27	O
)	O
note	O
that	O
for	O
n	O
→	O
∞	O
this	O
gives	O
the	O
maximum	B
likelihood	I
result	O
in	O
which	O
µn	O
=	O
x	O
and	O
the	O
precision	O
is	O
inﬁnite	O
.	O
similarly	O
,	O
the	O
optimal	O
solution	O
for	O
the	O
factor	O
qτ	O
(	O
τ	O
)	O
is	O
given	O
by	O
τ	O
(	O
τ	O
)	O
=	O
eµ	O
[	O
ln	O
p	O
(	O
d|µ	O
,	O
τ	O
)	O
+	O
ln	O
p	O
(	O
µ|τ	O
)	O
]	O
+	O
ln	O
p	O
(	O
τ	O
)	O
+	O
const	O
ln	O
q	O
(	O
cid:1	O
)	O
=	O
(	O
a0	O
−	O
1	O
)	O
ln	O
τ	O
−	O
b0τ	O
+	O
n	O
2	O
ln	O
τ	O
(	O
cid:31	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
−	O
τ	O
2	O
eµ	O
(	O
xn	O
−	O
µ	O
)	O
2	O
+	O
λ0	O
(	O
µ	O
−	O
µ0	O
)	O
2	O
+	O
const	O
(	O
10.28	O
)	O
and	O
hence	O
qτ	O
(	O
τ	O
)	O
is	O
a	O
gamma	B
distribution	I
gam	O
(	O
τ|an	O
,	O
bn	O
)	O
with	O
parameters	O
an	O
=	O
a0	O
+	O
n	O
2	O
1	O
2	O
eµ	O
bn	O
=	O
b0	O
+	O
(	O
cid:31	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
(	O
xn	O
−	O
µ	O
)	O
2	O
+	O
λ0	O
(	O
µ	O
−	O
µ0	O
)	O
2	O
.	O
(	O
10.29	O
)	O
(	O
10.30	O
)	O
exercise	O
10.8	O
section	O
10.4.1	O
again	O
this	O
exhibits	O
the	O
expected	O
behaviour	O
when	O
n	O
→	O
∞	O
.	O
it	O
should	O
be	O
emphasized	O
that	O
we	O
did	O
not	O
assume	O
these	O
speciﬁc	O
functional	B
forms	O
for	O
the	O
optimal	O
distributions	O
qµ	O
(	O
µ	O
)	O
and	O
qτ	O
(	O
τ	O
)	O
.	O
they	O
arose	O
naturally	O
from	O
the	O
structure	O
of	O
the	O
likelihood	B
function	I
and	O
the	O
corresponding	O
conjugate	B
priors	O
.	O
thus	O
we	O
have	O
expressions	O
for	O
the	O
optimal	O
distributions	O
qµ	O
(	O
µ	O
)	O
and	O
qτ	O
(	O
τ	O
)	O
each	O
of	O
which	O
depends	O
on	O
moments	O
evaluated	O
with	O
respect	O
to	O
the	O
other	O
distribution	O
.	O
one	O
ap-	O
proach	O
to	O
ﬁnding	O
a	O
solution	O
is	O
therefore	O
to	O
make	O
an	O
initial	O
guess	O
for	O
,	O
say	O
,	O
the	O
moment	O
e	O
[	O
τ	O
]	O
and	O
use	O
this	O
to	O
re-compute	O
the	O
distribution	O
qµ	O
(	O
µ	O
)	O
.	O
given	O
this	O
revised	O
distri-	O
bution	O
we	O
can	O
then	O
extract	O
the	O
required	O
moments	O
e	O
[	O
µ	O
]	O
and	O
e	O
[	O
µ2	O
]	O
,	O
and	O
use	O
these	O
to	O
recompute	O
the	O
distribution	O
qτ	O
(	O
τ	O
)	O
,	O
and	O
so	O
on	O
.	O
since	O
the	O
space	O
of	O
hidden	O
variables	O
for	O
this	O
example	O
is	O
only	O
two	O
dimensional	O
,	O
we	O
can	O
illustrate	O
the	O
variational	B
approxima-	O
tion	O
to	O
the	O
posterior	O
distribution	O
by	O
plotting	O
contours	O
of	O
both	O
the	O
true	O
posterior	O
and	O
the	O
factorized	O
approximation	O
,	O
as	O
illustrated	O
in	O
figure	O
10.4	O
.	O
472	O
10.	O
approximate	O
inference	B
(	O
a	O
)	O
(	O
c	O
)	O
τ	O
2	O
1	O
0	O
−1	O
2	O
τ	O
1	O
0	O
−1	O
(	O
b	O
)	O
(	O
d	O
)	O
τ	O
2	O
1	O
0	O
−1	O
2	O
τ	O
1	O
0	O
µ	O
1	O
0	O
µ	O
1	O
0	O
µ	O
1	O
0	O
−1	O
0	O
µ	O
1	O
figure	O
10.4	O
illustration	O
of	O
variational	B
inference	I
for	O
the	O
mean	B
µ	O
and	O
precision	O
τ	O
of	O
a	O
univariate	O
gaussian	O
distribu-	O
tion	O
.	O
contours	O
of	O
the	O
true	O
posterior	O
distribution	O
p	O
(	O
µ	O
,	O
τ|d	O
)	O
are	O
shown	O
in	O
green	O
.	O
(	O
a	O
)	O
contours	O
of	O
the	O
initial	O
factorized	O
approximation	O
qµ	O
(	O
µ	O
)	O
qτ	O
(	O
τ	O
)	O
are	O
shown	O
in	O
blue	O
.	O
(	O
b	O
)	O
after	O
re-estimating	O
the	O
factor	O
qµ	O
(	O
µ	O
)	O
.	O
(	O
c	O
)	O
after	O
re-estimating	O
the	O
factor	O
qτ	O
(	O
τ	O
)	O
.	O
(	O
d	O
)	O
contours	O
of	O
the	O
optimal	O
factorized	O
approximation	O
,	O
to	O
which	O
the	O
iterative	O
scheme	O
converges	O
,	O
are	O
shown	O
in	O
red	O
.	O
appendix	O
b	O
in	O
general	O
,	O
we	O
will	O
need	O
to	O
use	O
an	O
iterative	O
approach	O
such	O
as	O
this	O
in	O
order	O
to	O
solve	O
for	O
the	O
optimal	O
factorized	O
posterior	O
distribution	O
.	O
for	O
the	O
very	O
simple	O
example	O
we	O
are	O
considering	O
here	O
,	O
however	O
,	O
we	O
can	O
ﬁnd	O
an	O
explicit	O
solution	O
by	O
solving	O
the	O
simultaneous	O
equations	O
for	O
the	O
optimal	O
factors	O
qµ	O
(	O
µ	O
)	O
and	O
qτ	O
(	O
τ	O
)	O
.	O
before	O
doing	O
this	O
,	O
we	O
can	O
simplify	O
these	O
expressions	O
by	O
considering	O
broad	O
,	O
noninformative	B
priors	O
in	O
which	O
µ0	O
=	O
a0	O
=	O
b0	O
=	O
λ0	O
=	O
0.	O
although	O
these	O
parameter	O
settings	O
correspond	O
to	O
improper	B
priors	O
,	O
we	O
see	O
that	O
the	O
posterior	O
distribution	O
is	O
still	O
well	O
deﬁned	O
.	O
using	O
the	O
standard	O
result	O
e	O
[	O
τ	O
]	O
=	O
an	O
/bn	O
for	O
the	O
mean	B
of	O
a	O
gamma	B
distribution	I
,	O
together	O
with	O
(	O
10.29	O
)	O
and	O
(	O
10.30	O
)	O
,	O
we	O
have	O
(	O
cid:31	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
1	O
e	O
[	O
τ	O
]	O
=	O
e	O
1	O
n	O
(	O
xn	O
−	O
µ	O
)	O
2	O
=	O
x2	O
−	O
2xe	O
[	O
µ	O
]	O
+	O
e	O
[	O
µ2	O
]	O
.	O
(	O
10.31	O
)	O
then	O
,	O
using	O
(	O
10.26	O
)	O
and	O
(	O
10.27	O
)	O
,	O
we	O
obtain	O
the	O
ﬁrst	O
and	O
second	B
order	I
moments	O
of	O
10.1.	O
variational	B
inference	I
473	O
qµ	O
(	O
µ	O
)	O
in	O
the	O
form	O
e	O
[	O
µ	O
]	O
=	O
x	O
,	O
e	O
[	O
µ2	O
]	O
=	O
x2	O
+	O
1	O
n	O
e	O
[	O
τ	O
]	O
.	O
(	O
10.32	O
)	O
exercise	O
10.9	O
we	O
can	O
now	O
substitute	O
these	O
moments	O
into	O
(	O
10.31	O
)	O
and	O
then	O
solve	O
for	O
e	O
[	O
τ	O
]	O
to	O
give	O
(	O
x2	O
−	O
x2	O
)	O
1	O
e	O
[	O
τ	O
]	O
=	O
=	O
1	O
n	O
−	O
1	O
1	O
n	O
−	O
1	O
n	O
(	O
cid:2	O
)	O
n=1	O
(	O
xn	O
−	O
x	O
)	O
2	O
.	O
(	O
10.33	O
)	O
section	O
1.2.4	O
we	O
recognize	O
the	O
right-hand	O
side	O
as	O
the	O
familiar	O
unbiased	O
estimator	O
for	O
the	O
variance	B
of	O
a	O
univariate	O
gaussian	O
distribution	O
,	O
and	O
so	O
we	O
see	O
that	O
the	O
use	O
of	O
a	O
bayesian	O
ap-	O
proach	O
has	O
avoided	O
the	O
bias	B
of	O
the	O
maximum	B
likelihood	I
solution	O
.	O
10.1.4	O
model	B
comparison	I
as	O
well	O
as	O
performing	O
inference	B
over	O
the	O
hidden	O
variables	O
z	O
,	O
we	O
may	O
also	O
wish	O
to	O
compare	O
a	O
set	O
of	O
candidate	O
models	O
,	O
labelled	O
by	O
the	O
index	O
m	O
,	O
and	O
having	O
prior	B
probabilities	O
p	O
(	O
m	O
)	O
.	O
our	O
goal	O
is	O
then	O
to	O
approximate	O
the	O
posterior	O
probabilities	O
p	O
(	O
m|x	O
)	O
,	O
where	O
x	O
is	O
the	O
observed	O
data	O
.	O
this	O
is	O
a	O
slightly	O
more	O
complex	O
situation	O
than	O
that	O
considered	O
so	O
far	O
because	O
different	O
models	O
may	O
have	O
different	O
structure	O
and	O
indeed	O
different	O
dimensionality	O
for	O
the	O
hidden	O
variables	O
z.	O
we	O
can	O
not	O
there-	O
fore	O
simply	O
consider	O
a	O
factorized	O
approximation	O
q	O
(	O
z	O
)	O
q	O
(	O
m	O
)	O
,	O
but	O
must	O
instead	O
recog-	O
nize	O
that	O
the	O
posterior	O
over	O
z	O
must	O
be	O
conditioned	O
on	O
m	O
,	O
and	O
so	O
we	O
must	O
consider	O
q	O
(	O
z	O
,	O
m	O
)	O
=	O
q	O
(	O
z|m	O
)	O
q	O
(	O
m	O
)	O
.	O
we	O
can	O
readily	O
verify	O
the	O
following	O
decomposition	O
based	O
on	O
this	O
variational	B
distribution	O
(	O
cid:13	O
)	O
p	O
(	O
z	O
,	O
m|x	O
)	O
q	O
(	O
z|m	O
)	O
q	O
(	O
m	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
(	O
10.34	O
)	O
.	O
(	O
10.35	O
)	O
ln	O
p	O
(	O
x	O
)	O
=	O
lm	O
−	O
q	O
(	O
z|m	O
)	O
q	O
(	O
m	O
)	O
ln	O
where	O
the	O
lm	O
is	O
a	O
lower	B
bound	I
on	O
ln	O
p	O
(	O
x	O
)	O
and	O
is	O
given	O
by	O
lm	O
=	O
q	O
(	O
z|m	O
)	O
q	O
(	O
m	O
)	O
ln	O
p	O
(	O
z	O
,	O
x	O
,	O
m	O
)	O
q	O
(	O
z|m	O
)	O
q	O
(	O
m	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
m	O
z	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
m	O
z	O
here	O
we	O
are	O
assuming	O
discrete	O
z	O
,	O
but	O
the	O
same	O
analysis	O
applies	O
to	O
continuous	O
latent	O
variables	O
provided	O
the	O
summations	O
are	O
replaced	O
with	O
integrations	O
.	O
we	O
can	O
maximize	O
lm	O
with	O
respect	O
to	O
the	O
distribution	O
q	O
(	O
m	O
)	O
using	O
a	O
lagrange	O
multiplier	O
,	O
with	O
the	O
result	O
q	O
(	O
m	O
)	O
∝	O
p	O
(	O
m	O
)	O
exp	O
{	O
lm	O
}	O
.	O
(	O
10.36	O
)	O
however	O
,	O
if	O
we	O
maximize	O
lm	O
with	O
respect	O
to	O
the	O
q	O
(	O
z|m	O
)	O
,	O
we	O
ﬁnd	O
that	O
the	O
solutions	O
for	O
different	O
m	O
are	O
coupled	O
,	O
as	O
we	O
expect	O
because	O
they	O
are	O
conditioned	O
on	O
m.	O
we	O
proceed	O
instead	O
by	O
ﬁrst	O
optimizing	O
each	O
of	O
the	O
q	O
(	O
z|m	O
)	O
individually	O
by	O
optimization	O
exercise	O
10.10	O
exercise	O
10.11	O
474	O
10.	O
approximate	O
inference	B
of	O
(	O
10.35	O
)	O
,	O
and	O
then	O
subsequently	O
determining	O
the	O
q	O
(	O
m	O
)	O
using	O
(	O
10.36	O
)	O
.	O
after	O
nor-	O
malization	O
the	O
resulting	O
values	O
for	O
q	O
(	O
m	O
)	O
can	O
be	O
used	O
for	O
model	O
selection	O
or	O
model	B
averaging	I
in	O
the	O
usual	O
way	O
.	O
10.2.	O
illustration	O
:	O
variational	B
mixture	O
of	O
gaussians	O
we	O
now	O
return	O
to	O
our	O
discussion	O
of	O
the	O
gaussian	O
mixture	B
model	I
and	O
apply	O
the	O
vari-	O
ational	O
inference	B
machinery	O
developed	O
in	O
the	O
previous	O
section	O
.	O
this	O
will	O
provide	O
a	O
good	O
illustration	O
of	O
the	O
application	O
of	O
variational	B
methods	O
and	O
will	O
also	O
demonstrate	O
how	O
a	O
bayesian	O
treatment	O
elegantly	O
resolves	O
many	O
of	O
the	O
difﬁculties	O
associated	O
with	O
the	O
maximum	B
likelihood	I
approach	O
(	O
attias	O
,	O
1999b	O
)	O
.	O
the	O
reader	O
is	O
encouraged	O
to	O
work	O
through	O
this	O
example	O
in	O
detail	O
as	O
it	O
provides	O
many	O
insights	O
into	O
the	O
practical	O
appli-	O
cation	O
of	O
variational	B
methods	O
.	O
many	O
bayesian	O
models	O
,	O
corresponding	O
to	O
much	O
more	O
sophisticated	O
distributions	O
,	O
can	O
be	O
solved	O
by	O
straightforward	O
extensions	O
and	O
general-	O
izations	O
of	O
this	O
analysis	O
.	O
our	O
starting	O
point	O
is	O
the	O
likelihood	B
function	I
for	O
the	O
gaussian	O
mixture	B
model	I
,	O
il-	O
lustrated	O
by	O
the	O
graphical	B
model	I
in	O
figure	O
9.6.	O
for	O
each	O
observation	O
xn	O
we	O
have	O
a	O
corresponding	O
latent	B
variable	I
zn	O
comprising	O
a	O
1-of-k	O
binary	O
vector	O
with	O
ele-	O
ments	O
znk	O
for	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k.	O
as	O
before	O
we	O
denote	O
the	O
observed	O
data	O
set	O
by	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
,	O
and	O
similarly	O
we	O
denote	O
the	O
latent	O
variables	O
by	O
z	O
=	O
{	O
z1	O
,	O
.	O
.	O
.	O
,	O
zn	O
}	O
.	O
from	O
(	O
9.10	O
)	O
we	O
can	O
write	O
down	O
the	O
conditional	B
distribution	O
of	O
z	O
,	O
given	O
the	O
mixing	O
coefﬁcients	O
π	O
,	O
in	O
the	O
form	O
p	O
(	O
z|π	O
)	O
=	O
πznk	O
k	O
.	O
(	O
10.37	O
)	O
n	O
(	O
cid:14	O
)	O
k	O
(	O
cid:14	O
)	O
n=1	O
k=1	O
n	O
(	O
cid:14	O
)	O
k	O
(	O
cid:14	O
)	O
n	O
(	O
cid:10	O
)	O
n=1	O
k=1	O
similarly	O
,	O
from	O
(	O
9.11	O
)	O
,	O
we	O
can	O
write	O
down	O
the	O
conditional	B
distribution	O
of	O
the	O
ob-	O
served	O
data	O
vectors	O
,	O
given	O
the	O
latent	O
variables	O
and	O
the	O
component	O
parameters	O
p	O
(	O
x|z	O
,	O
µ	O
,	O
λ	O
)	O
=	O
xn|µk	O
,	O
λ	O
−1	O
k	O
(	O
10.38	O
)	O
where	O
µ	O
=	O
{	O
µk	O
}	O
and	O
λ	O
=	O
{	O
λk	O
}	O
.	O
note	O
that	O
we	O
are	O
working	O
in	O
terms	O
of	O
precision	O
matrices	O
rather	O
than	O
covariance	B
matrices	O
as	O
this	O
somewhat	O
simpliﬁes	O
the	O
mathemat-	O
ics	O
.	O
next	O
we	O
introduce	O
priors	O
over	O
the	O
parameters	O
µ	O
,	O
λ	O
and	O
π.	O
the	O
analysis	O
is	O
con-	O
siderably	O
simpliﬁed	O
if	O
we	O
use	O
conjugate	B
prior	I
distributions	O
.	O
we	O
therefore	O
choose	O
a	O
dirichlet	O
distribution	O
over	O
the	O
mixing	O
coefﬁcients	O
π	O
p	O
(	O
π	O
)	O
=	O
dir	O
(	O
π|α0	O
)	O
=	O
c	O
(	O
α0	O
)	O
πα0−1	O
k	O
(	O
10.39	O
)	O
where	O
by	O
symmetry	O
we	O
have	O
chosen	O
the	O
same	O
parameter	O
α0	O
for	O
each	O
of	O
the	O
compo-	O
nents	O
,	O
and	O
c	O
(	O
α0	O
)	O
is	O
the	O
normalization	O
constant	O
for	O
the	O
dirichlet	O
distribution	O
deﬁned	O
k=1	O
(	O
cid:11	O
)	O
znk	O
k	O
(	O
cid:14	O
)	O
section	O
10.4.1	O
10.2.	O
illustration	O
:	O
variational	B
mixture	O
of	O
gaussians	O
475	O
figure	O
10.5	O
directed	B
acyclic	I
graph	I
representing	O
the	O
bayesian	O
mix-	O
ture	O
of	O
gaussians	O
model	O
,	O
in	O
which	O
the	O
box	O
(	O
plate	B
)	O
de-	O
notes	O
a	O
set	O
of	O
n	O
i.i.d	O
.	O
observations	O
.	O
here	O
µ	O
denotes	O
{	O
µk	O
}	O
and	O
λ	O
denotes	O
{	O
λk	O
}	O
.	O
π	O
λ	O
µ	O
zn	O
xn	O
n	O
section	O
2.2.1	O
by	O
(	O
b.23	O
)	O
.	O
as	O
we	O
have	O
seen	O
,	O
the	O
parameter	O
α0	O
can	O
be	O
interpreted	O
as	O
the	O
effective	O
prior	O
number	O
of	O
observations	O
associated	O
with	O
each	O
component	O
of	O
the	O
mixture	B
.	O
if	O
the	O
value	O
of	O
α0	O
is	O
small	O
,	O
then	O
the	O
posterior	O
distribution	O
will	O
be	O
inﬂuenced	O
primarily	O
by	O
the	O
data	O
rather	O
than	O
by	O
the	O
prior	B
.	O
similarly	O
,	O
we	O
introduce	O
an	O
independent	B
gaussian-wishart	O
prior	B
governing	O
the	O
mean	B
and	O
precision	O
of	O
each	O
gaussian	O
component	O
,	O
given	O
by	O
p	O
(	O
µ	O
,	O
λ	O
)	O
=	O
p	O
(	O
µ|λ	O
)	O
p	O
(	O
λ	O
)	O
k	O
(	O
cid:14	O
)	O
n	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
w	O
(	O
λk|w0	O
,	O
ν0	O
)	O
=	O
µk|m0	O
,	O
(	O
β0λk	O
)	O
−1	O
(	O
10.40	O
)	O
section	O
2.3.6	O
k=1	O
because	O
this	O
represents	O
the	O
conjugate	B
prior	I
distribution	O
when	O
both	O
the	O
mean	B
and	O
pre-	O
cision	O
are	O
unknown	O
.	O
typically	O
we	O
would	O
choose	O
m0	O
=	O
0	O
by	O
symmetry	O
.	O
the	O
resulting	O
model	O
can	O
be	O
represented	O
as	O
a	O
directed	B
graph	O
as	O
shown	O
in	O
fig-	O
ure	O
10.5.	O
note	O
that	O
there	O
is	O
a	O
link	B
from	O
λ	O
to	O
µ	O
since	O
the	O
variance	B
of	O
the	O
distribution	O
over	O
µ	O
in	O
(	O
10.40	O
)	O
is	O
a	O
function	O
of	O
λ.	O
this	O
example	O
provides	O
a	O
nice	O
illustration	O
of	O
the	O
distinction	O
between	O
latent	O
vari-	O
ables	O
and	O
parameters	O
.	O
variables	O
such	O
as	O
zn	O
that	O
appear	O
inside	O
the	O
plate	B
are	O
regarded	O
as	O
latent	O
variables	O
because	O
the	O
number	O
of	O
such	O
variables	O
grows	O
with	O
the	O
size	O
of	O
the	O
data	O
set	O
.	O
by	O
contrast	O
,	O
variables	O
such	O
as	O
µ	O
that	O
are	O
outside	O
the	O
plate	B
are	O
ﬁxed	O
in	O
number	O
independently	O
of	O
the	O
size	O
of	O
the	O
data	O
set	O
,	O
and	O
so	O
are	O
regarded	O
as	O
parameters	O
.	O
from	O
the	O
perspective	O
of	O
graphical	O
models	O
,	O
however	O
,	O
there	O
is	O
really	O
no	O
fundamental	O
difference	O
between	O
them	O
.	O
10.2.1	O
variational	B
distribution	O
in	O
order	O
to	O
formulate	O
a	O
variational	B
treatment	O
of	O
this	O
model	O
,	O
we	O
next	O
write	O
down	O
the	O
joint	O
distribution	O
of	O
all	O
of	O
the	O
random	O
variables	O
,	O
which	O
is	O
given	O
by	O
p	O
(	O
x	O
,	O
z	O
,	O
π	O
,	O
µ	O
,	O
λ	O
)	O
=	O
p	O
(	O
x|z	O
,	O
µ	O
,	O
λ	O
)	O
p	O
(	O
z|π	O
)	O
p	O
(	O
π	O
)	O
p	O
(	O
µ|λ	O
)	O
p	O
(	O
λ	O
)	O
(	O
10.41	O
)	O
in	O
which	O
the	O
various	O
factors	O
are	O
deﬁned	O
above	O
.	O
the	O
reader	O
should	O
take	O
a	O
moment	O
to	O
verify	O
that	O
this	O
decomposition	O
does	O
indeed	O
correspond	O
to	O
the	O
probabilistic	B
graphical	I
model	I
shown	O
in	O
figure	O
10.5.	O
note	O
that	O
only	O
the	O
variables	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
are	O
observed	O
.	O
476	O
10.	O
approximate	O
inference	B
we	O
now	O
consider	O
a	O
variational	B
distribution	O
which	O
factorizes	O
between	O
the	O
latent	O
variables	O
and	O
the	O
parameters	O
so	O
that	O
q	O
(	O
z	O
,	O
π	O
,	O
µ	O
,	O
λ	O
)	O
=	O
q	O
(	O
z	O
)	O
q	O
(	O
π	O
,	O
µ	O
,	O
λ	O
)	O
.	O
(	O
10.42	O
)	O
it	O
is	O
remarkable	O
that	O
this	O
is	O
the	O
only	O
assumption	O
that	O
we	O
need	O
to	O
make	O
in	O
order	O
to	O
obtain	O
a	O
tractable	O
practical	O
solution	O
to	O
our	O
bayesian	O
mixture	B
model	I
.	O
in	O
particular	O
,	O
the	O
functional	B
form	O
of	O
the	O
factors	O
q	O
(	O
z	O
)	O
and	O
q	O
(	O
π	O
,	O
µ	O
,	O
λ	O
)	O
will	O
be	O
determined	O
automatically	O
by	O
optimization	O
of	O
the	O
variational	B
distribution	O
.	O
note	O
that	O
we	O
are	O
omitting	O
the	O
sub-	O
scripts	O
on	O
the	O
q	O
distributions	O
,	O
much	O
as	O
we	O
do	O
with	O
the	O
p	O
distributions	O
in	O
(	O
10.41	O
)	O
,	O
and	O
are	O
relying	O
on	O
the	O
arguments	O
to	O
distinguish	O
the	O
different	O
distributions	O
.	O
the	O
corresponding	O
sequential	O
update	O
equations	O
for	O
these	O
factors	O
can	O
be	O
easily	O
derived	O
by	O
making	O
use	O
of	O
the	O
general	O
result	O
(	O
10.9	O
)	O
.	O
let	O
us	O
consider	O
the	O
derivation	O
of	O
the	O
update	O
equation	O
for	O
the	O
factor	O
q	O
(	O
z	O
)	O
.	O
the	O
log	O
of	O
the	O
optimized	O
factor	O
is	O
given	O
by	O
ln	O
q	O
(	O
cid:1	O
)	O
(	O
z	O
)	O
=	O
eπ	O
,	O
µ	O
,	O
λ	O
[	O
ln	O
p	O
(	O
x	O
,	O
z	O
,	O
π	O
,	O
µ	O
,	O
λ	O
)	O
]	O
+	O
const	O
.	O
(	O
10.43	O
)	O
we	O
now	O
make	O
use	O
of	O
the	O
decomposition	O
(	O
10.41	O
)	O
.	O
note	O
that	O
we	O
are	O
only	O
interested	O
in	O
the	O
functional	B
dependence	O
of	O
the	O
right-hand	O
side	O
on	O
the	O
variable	O
z.	O
thus	O
any	O
terms	O
that	O
do	O
not	O
depend	O
on	O
z	O
can	O
be	O
absorbed	O
into	O
the	O
additive	O
normalization	O
constant	O
,	O
giving	O
ln	O
q	O
(	O
cid:1	O
)	O
(	O
z	O
)	O
=	O
eπ	O
[	O
ln	O
p	O
(	O
z|π	O
)	O
]	O
+	O
eµ	O
,	O
λ	O
[	O
ln	O
p	O
(	O
x|z	O
,	O
µ	O
,	O
λ	O
)	O
]	O
+	O
const	O
.	O
(	O
10.44	O
)	O
substituting	O
for	O
the	O
two	O
conditional	B
distributions	O
on	O
the	O
right-hand	O
side	O
,	O
and	O
again	O
absorbing	O
any	O
terms	O
that	O
are	O
independent	B
of	O
z	O
into	O
the	O
additive	O
constant	O
,	O
we	O
have	O
ln	O
q	O
(	O
cid:1	O
)	O
(	O
z	O
)	O
=	O
znk	O
ln	O
ρnk	O
+	O
const	O
(	O
10.45	O
)	O
n	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
n=1	O
k=1	O
where	O
we	O
have	O
deﬁned	O
ln	O
ρnk	O
=	O
e	O
[	O
ln	O
πk	O
]	O
+	O
where	O
d	O
is	O
the	O
dimensionality	O
of	O
the	O
data	O
variable	O
x.	O
taking	O
the	O
exponential	O
of	O
both	O
sides	O
of	O
(	O
10.45	O
)	O
we	O
obtain	O
−1	O
2	O
1	O
2	O
eµk	O
,	O
λk	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
2	O
e	O
[	O
ln|λk|	O
]	O
−	O
d	O
ln	O
(	O
2π	O
)	O
(	O
xn	O
−	O
µk	O
)	O
tλk	O
(	O
xn	O
−	O
µk	O
)	O
k	O
(	O
cid:14	O
)	O
q	O
(	O
cid:1	O
)	O
(	O
z	O
)	O
∝	O
n	O
(	O
cid:14	O
)	O
n	O
(	O
cid:14	O
)	O
k	O
(	O
cid:14	O
)	O
ρznk	O
nk	O
.	O
n=1	O
k=1	O
q	O
(	O
cid:1	O
)	O
(	O
z	O
)	O
=	O
rznk	O
nk	O
n=1	O
k=1	O
(	O
10.46	O
)	O
(	O
10.47	O
)	O
(	O
10.48	O
)	O
exercise	O
10.12	O
requiring	O
that	O
this	O
distribution	O
be	O
normalized	O
,	O
and	O
noting	O
that	O
for	O
each	O
value	O
of	O
n	O
the	O
quantities	O
znk	O
are	O
binary	O
and	O
sum	O
to	O
1	O
over	O
all	O
values	O
of	O
k	O
,	O
we	O
obtain	O
where	O
10.2.	O
illustration	O
:	O
variational	B
mixture	O
of	O
gaussians	O
477	O
k	O
(	O
cid:2	O
)	O
rnk	O
=	O
ρnk	O
ρnj	O
.	O
j=1	O
(	O
10.49	O
)	O
we	O
see	O
that	O
the	O
optimal	O
solution	O
for	O
the	O
factor	O
q	O
(	O
z	O
)	O
takes	O
the	O
same	O
functional	B
form	O
as	O
the	O
prior	B
p	O
(	O
z|π	O
)	O
.	O
note	O
that	O
because	O
ρnk	O
is	O
given	O
by	O
the	O
exponential	O
of	O
a	O
real	O
quantity	O
,	O
the	O
quantities	O
rnk	O
will	O
be	O
nonnegative	O
and	O
will	O
sum	O
to	O
one	O
,	O
as	O
required	O
.	O
for	O
the	O
discrete	O
distribution	O
q	O
(	O
cid:1	O
)	O
(	O
z	O
)	O
we	O
have	O
the	O
standard	O
result	O
e	O
[	O
znk	O
]	O
=	O
rnk	O
(	O
10.50	O
)	O
from	O
which	O
we	O
see	O
that	O
the	O
quantities	O
rnk	O
are	O
playing	O
the	O
role	O
of	O
responsibilities	O
.	O
note	O
that	O
the	O
optimal	O
solution	O
for	O
q	O
(	O
cid:1	O
)	O
(	O
z	O
)	O
depends	O
on	O
moments	O
evaluated	O
with	O
respect	O
to	O
the	O
distributions	O
of	O
other	O
variables	O
,	O
and	O
so	O
again	O
the	O
variational	B
update	O
equations	O
are	O
coupled	O
and	O
must	O
be	O
solved	O
iteratively	O
.	O
at	O
this	O
point	O
,	O
we	O
shall	O
ﬁnd	O
it	O
convenient	O
to	O
deﬁne	O
three	O
statistics	O
of	O
the	O
observed	O
data	O
set	O
evaluated	O
with	O
respect	O
to	O
the	O
responsibilities	O
,	O
given	O
by	O
n	O
(	O
cid:2	O
)	O
n=1	O
1	O
nk	O
1	O
nk	O
nk	O
=	O
xk	O
=	O
sk	O
=	O
rnkxn	O
rnk	O
(	O
xn	O
−	O
xk	O
)	O
(	O
xn	O
−	O
xk	O
)	O
t.	O
(	O
10.51	O
)	O
(	O
10.52	O
)	O
(	O
10.53	O
)	O
rnk	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
n=1	O
note	O
that	O
these	O
are	O
analogous	O
to	O
quantities	O
evaluated	O
in	O
the	O
maximum	B
likelihood	I
em	O
algorithm	O
for	O
the	O
gaussian	O
mixture	B
model	I
.	O
now	O
let	O
us	O
consider	O
the	O
factor	O
q	O
(	O
π	O
,	O
µ	O
,	O
λ	O
)	O
in	O
the	O
variational	B
posterior	O
distribu-	O
tion	O
.	O
again	O
using	O
the	O
general	O
result	O
(	O
10.9	O
)	O
we	O
have	O
ln	O
q	O
(	O
cid:1	O
)	O
(	O
π	O
,	O
µ	O
,	O
λ	O
)	O
=	O
ln	O
p	O
(	O
π	O
)	O
+	O
ln	O
p	O
(	O
µk	O
,	O
λk	O
)	O
+	O
ez	O
[	O
ln	O
p	O
(	O
z|π	O
)	O
]	O
k	O
(	O
cid:2	O
)	O
e	O
[	O
znk	O
]	O
lnn	O
(	O
cid:10	O
)	O
k=1	O
k	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
k=1	O
n=1	O
+	O
(	O
cid:11	O
)	O
xn|µk	O
,	O
λ	O
−1	O
k	O
+	O
const	O
.	O
(	O
10.54	O
)	O
we	O
observe	O
that	O
the	O
right-hand	O
side	O
of	O
this	O
expression	O
decomposes	O
into	O
a	O
sum	O
of	O
terms	O
involving	O
only	O
π	O
together	O
with	O
terms	O
only	O
involving	O
µ	O
and	O
λ	O
,	O
which	O
implies	O
that	O
the	O
variational	B
posterior	O
q	O
(	O
π	O
,	O
µ	O
,	O
λ	O
)	O
factorizes	O
to	O
give	O
q	O
(	O
π	O
)	O
q	O
(	O
µ	O
,	O
λ	O
)	O
.	O
further-	O
more	O
,	O
the	O
terms	O
involving	O
µ	O
and	O
λ	O
themselves	O
comprise	O
a	O
sum	O
over	O
k	O
of	O
terms	O
involving	O
µk	O
and	O
λk	O
leading	O
to	O
the	O
further	O
factorization	B
q	O
(	O
π	O
,	O
µ	O
,	O
λ	O
)	O
=	O
q	O
(	O
π	O
)	O
q	O
(	O
µk	O
,	O
λk	O
)	O
.	O
(	O
10.55	O
)	O
k	O
(	O
cid:14	O
)	O
k=1	O
478	O
10.	O
approximate	O
inference	B
identifying	O
the	O
terms	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
10.54	O
)	O
that	O
depend	O
on	O
π	O
,	O
we	O
have	O
ln	O
q	O
(	O
cid:1	O
)	O
(	O
π	O
)	O
=	O
(	O
α0	O
−	O
1	O
)	O
ln	O
πk	O
+	O
rnk	O
ln	O
πk	O
+	O
const	O
(	O
10.56	O
)	O
k	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
k=1	O
k=1	O
n=1	O
where	O
we	O
have	O
used	O
(	O
10.50	O
)	O
.	O
taking	O
the	O
exponential	O
of	O
both	O
sides	O
,	O
we	O
recognize	O
q	O
(	O
cid:1	O
)	O
(	O
π	O
)	O
as	O
a	O
dirichlet	O
distribution	O
q	O
(	O
cid:1	O
)	O
(	O
π	O
)	O
=	O
dir	O
(	O
π|α	O
)	O
(	O
10.57	O
)	O
exercise	O
10.13	O
exercise	O
10.14	O
where	O
α	O
has	O
components	O
αk	O
given	O
by	O
αk	O
=	O
α0	O
+	O
nk	O
.	O
(	O
10.58	O
)	O
finally	O
,	O
the	O
variational	B
posterior	O
distribution	O
q	O
(	O
cid:1	O
)	O
(	O
µk	O
,	O
λk	O
)	O
does	O
not	O
factorize	O
into	O
the	O
product	O
of	O
the	O
marginals	O
,	O
but	O
we	O
can	O
always	O
use	O
the	O
product	B
rule	I
to	O
write	O
it	O
in	O
the	O
form	O
q	O
(	O
cid:1	O
)	O
(	O
µk	O
,	O
λk	O
)	O
=	O
q	O
(	O
cid:1	O
)	O
(	O
µk|λk	O
)	O
q	O
(	O
cid:1	O
)	O
(	O
λk	O
)	O
.	O
the	O
two	O
factors	O
can	O
be	O
found	O
by	O
inspecting	O
(	O
10.54	O
)	O
and	O
reading	O
off	O
those	O
terms	O
that	O
involve	O
µk	O
and	O
λk	O
.	O
the	O
result	O
,	O
as	O
expected	O
,	O
is	O
a	O
gaussian-wishart	O
distribution	O
and	O
is	O
given	O
by	O
µk|mk	O
,	O
(	O
βkλk	O
)	O
−1	O
q	O
(	O
cid:1	O
)	O
(	O
µk	O
,	O
λk	O
)	O
=	O
n	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
w	O
(	O
λk|wk	O
,	O
νk	O
)	O
(	O
10.59	O
)	O
where	O
we	O
have	O
deﬁned	O
βk	O
=	O
β0	O
+	O
nk	O
mk	O
=	O
1	O
βk	O
(	O
β0m0	O
+	O
nkxk	O
)	O
w	O
=	O
w	O
−1	O
k	O
νk	O
=	O
ν0	O
+	O
nk	O
.	O
−1	O
0	O
+	O
nksk	O
+	O
β0nk	O
β0	O
+	O
nk	O
(	O
xk	O
−	O
m0	O
)	O
(	O
xk	O
−	O
m0	O
)	O
t	O
(	O
10.60	O
)	O
(	O
10.61	O
)	O
(	O
10.62	O
)	O
(	O
10.63	O
)	O
these	O
update	O
equations	O
are	O
analogous	O
to	O
the	O
m-step	O
equations	O
of	O
the	O
em	O
algorithm	O
for	O
the	O
maximum	B
likelihood	I
solution	O
of	O
the	O
mixture	O
of	O
gaussians	O
.	O
we	O
see	O
that	O
the	O
computations	O
that	O
must	O
be	O
performed	O
in	O
order	O
to	O
update	O
the	O
variational	B
posterior	O
distribution	O
over	O
the	O
model	O
parameters	O
involve	O
evaluation	O
of	O
the	O
same	O
sums	O
over	O
the	O
data	O
set	O
,	O
as	O
arose	O
in	O
the	O
maximum	B
likelihood	I
treatment	O
.	O
in	O
order	O
to	O
perform	O
this	O
variational	B
m	O
step	O
,	O
we	O
need	O
the	O
expectations	O
e	O
[	O
znk	O
]	O
=	O
rnk	O
representing	O
the	O
responsibilities	O
.	O
these	O
are	O
obtained	O
by	O
normalizing	O
the	O
ρnk	O
that	O
are	O
given	O
by	O
(	O
10.46	O
)	O
.	O
we	O
see	O
that	O
this	O
expression	O
involves	O
expectations	O
with	O
respect	O
to	O
the	O
variational	B
distributions	O
of	O
the	O
parameters	O
,	O
and	O
these	O
are	O
easily	O
evaluated	O
to	O
give	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
(	O
xn	O
−	O
µk	O
)	O
tλk	O
(	O
xn	O
−	O
µk	O
)	O
eµk	O
,	O
λk	O
=	O
dβ	O
ln	O
(	O
cid:4	O
)	O
λk	O
≡	O
e	O
[	O
ln|λk|	O
]	O
=	O
ln	O
(	O
cid:4	O
)	O
πk	O
≡	O
e	O
[	O
ln	O
πk	O
]	O
=	O
ψ	O
(	O
αk	O
)	O
−	O
ψ	O
(	O
(	O
cid:1	O
)	O
α	O
)	O
i=1	O
ψ	O
2	O
(	O
cid:15	O
)	O
d	O
(	O
cid:2	O
)	O
k	O
+	O
νk	O
(	O
xn	O
−	O
mk	O
)	O
twk	O
(	O
xn	O
−	O
mk	O
)	O
−1	O
(	O
cid:16	O
)	O
νk	O
+	O
1	O
−	O
i	O
(	O
10.64	O
)	O
+	O
d	O
ln	O
2	O
+	O
ln|wk|	O
(	O
10.65	O
)	O
(	O
10.66	O
)	O
appendix	O
b	O
(	O
cid:5	O
)	O
10.2.	O
illustration	O
:	O
variational	B
mixture	O
of	O
gaussians	O
where	O
we	O
have	O
introduced	O
deﬁnitions	O
of	O
(	O
cid:4	O
)	O
λk	O
and	O
(	O
cid:4	O
)	O
πk	O
,	O
and	O
ψ	O
(	O
·	O
)	O
is	O
the	O
digamma	B
function	I
deﬁned	O
by	O
(	O
b.25	O
)	O
,	O
with	O
(	O
cid:1	O
)	O
α	O
=	O
(	O
cid:12	O
)	O
if	O
we	O
substitute	O
(	O
10.64	O
)	O
,	O
(	O
10.65	O
)	O
,	O
and	O
(	O
10.66	O
)	O
into	O
(	O
10.46	O
)	O
and	O
make	O
use	O
of	O
the	O
standard	O
properties	O
of	O
the	O
wishart	O
and	O
dirichlet	O
distributions	O
.	O
(	O
10.49	O
)	O
,	O
we	O
obtain	O
the	O
following	O
result	O
for	O
the	O
responsibilities	O
k	O
αk	O
.	O
the	O
results	O
(	O
10.65	O
)	O
and	O
(	O
10.66	O
)	O
follow	O
from	O
(	O
cid:13	O
)	O
479	O
rnk	O
∝	O
(	O
cid:4	O
)	O
πk	O
(	O
cid:4	O
)	O
λ1/2	O
k	O
exp	O
−	O
d	O
2βk	O
−	O
νk	O
2	O
(	O
xn	O
−	O
mk	O
)	O
twk	O
(	O
xn	O
−	O
mk	O
)	O
.	O
(	O
10.67	O
)	O
notice	O
the	O
similarity	O
to	O
the	O
corresponding	O
result	O
for	O
the	O
responsibilities	O
in	O
maximum	B
likelihood	I
em	O
,	O
which	O
from	O
(	O
9.13	O
)	O
can	O
be	O
written	O
in	O
the	O
form	O
(	O
cid:12	O
)	O
−1	O
2	O
(	O
cid:13	O
)	O
(	O
xn	O
−	O
µk	O
)	O
tλk	O
(	O
xn	O
−	O
µk	O
)	O
rnk	O
∝	O
πk|λk|1/2	O
exp	O
(	O
10.68	O
)	O
where	O
we	O
have	O
used	O
the	O
precision	O
in	O
place	O
of	O
the	O
covariance	B
to	O
highlight	O
the	O
similarity	O
to	O
(	O
10.67	O
)	O
.	O
thus	O
the	O
optimization	O
of	O
the	O
variational	B
posterior	O
distribution	O
involves	O
cycling	O
between	O
two	O
stages	O
analogous	O
to	O
the	O
e	O
and	O
m	O
steps	O
of	O
the	O
maximum	B
likelihood	I
em	O
algorithm	O
.	O
in	O
the	O
variational	B
equivalent	O
of	O
the	O
e	O
step	O
,	O
we	O
use	O
the	O
current	O
distributions	O
over	O
the	O
model	O
parameters	O
to	O
evaluate	O
the	O
moments	O
in	O
(	O
10.64	O
)	O
,	O
(	O
10.65	O
)	O
,	O
and	O
(	O
10.66	O
)	O
and	O
hence	O
evaluate	O
e	O
[	O
znk	O
]	O
=	O
rnk	O
.	O
then	O
in	O
the	O
subsequent	O
variational	B
equivalent	O
of	O
the	O
m	O
step	O
,	O
we	O
keep	O
these	O
responsibilities	O
ﬁxed	O
and	O
use	O
them	O
to	O
re-compute	O
the	O
variational	B
distribution	O
over	O
the	O
parameters	O
using	O
(	O
10.57	O
)	O
and	O
(	O
10.59	O
)	O
.	O
in	O
each	O
case	O
,	O
we	O
see	O
that	O
the	O
variational	B
posterior	O
distribution	O
has	O
the	O
same	O
functional	B
form	O
as	O
the	O
corresponding	O
factor	O
in	O
the	O
joint	O
distribution	O
(	O
10.41	O
)	O
.	O
this	O
is	O
a	O
general	O
result	O
and	O
is	O
a	O
consequence	O
of	O
the	O
choice	O
of	O
conjugate	B
distributions	O
.	O
figure	O
10.6	O
shows	O
the	O
results	O
of	O
applying	O
this	O
approach	O
to	O
the	O
rescaled	O
old	O
faith-	O
ful	O
data	O
set	O
for	O
a	O
gaussian	O
mixture	B
model	I
having	O
k	O
=	O
6	O
components	O
.	O
we	O
see	O
that	O
after	O
convergence	O
,	O
there	O
are	O
only	O
two	O
components	O
for	O
which	O
the	O
expected	O
values	O
of	O
the	O
mixing	O
coefﬁcients	O
are	O
numerically	O
distinguishable	O
from	O
their	O
prior	B
values	O
.	O
this	O
effect	O
can	O
be	O
understood	O
qualitatively	O
in	O
terms	O
of	O
the	O
automatic	O
trade-off	O
in	O
a	O
bayesian	O
model	O
between	O
ﬁtting	O
the	O
data	O
and	O
the	O
complexity	O
of	O
the	O
model	O
,	O
in	O
which	O
the	O
complexity	O
penalty	O
arises	O
from	O
components	O
whose	O
parameters	O
are	O
pushed	O
away	O
from	O
their	O
prior	B
values	O
.	O
components	O
that	O
take	O
essentially	O
no	O
responsibility	B
for	O
ex-	O
plaining	O
the	O
data	O
points	O
have	O
rnk	O
(	O
cid:7	O
)	O
0	O
and	O
hence	O
nk	O
(	O
cid:7	O
)	O
0.	O
from	O
(	O
10.58	O
)	O
,	O
we	O
see	O
that	O
αk	O
(	O
cid:7	O
)	O
α0	O
and	O
from	O
(	O
10.60	O
)	O
–	O
(	O
10.63	O
)	O
we	O
see	O
that	O
the	O
other	O
parameters	O
revert	O
to	O
their	O
prior	B
values	O
.	O
in	O
principle	O
such	O
components	O
are	O
ﬁtted	O
slightly	O
to	O
the	O
data	O
points	O
,	O
but	O
for	O
broad	O
priors	O
this	O
effect	O
is	O
too	O
small	O
to	O
be	O
seen	O
numerically	O
.	O
for	O
the	O
varia-	O
tional	O
gaussian	O
mixture	B
model	I
the	O
expected	O
values	O
of	O
the	O
mixing	O
coefﬁcients	O
in	O
the	O
posterior	O
distribution	O
are	O
given	O
by	O
section	O
10.4.1	O
section	O
3.4	O
exercise	O
10.15	O
(	O
10.69	O
)	O
consider	O
a	O
component	O
for	O
which	O
nk	O
(	O
cid:7	O
)	O
0	O
and	O
αk	O
(	O
cid:7	O
)	O
α0	O
.	O
if	O
the	O
prior	B
is	O
broad	O
so	O
that	O
α0	O
→	O
0	O
,	O
then	O
e	O
[	O
πk	O
]	O
→	O
0	O
and	O
the	O
component	O
plays	O
no	O
role	O
in	O
the	O
model	O
,	O
whereas	O
if	O
.	O
e	O
[	O
πk	O
]	O
=	O
αk	O
+	O
nk	O
kα0	O
+	O
n	O
480	O
10.	O
approximate	O
inference	B
figure	O
10.6	O
variational	B
bayesian	O
mixture	O
of	O
k	O
=	O
6	O
gaussians	O
ap-	O
plied	O
to	O
the	O
old	O
faithful	O
data	O
set	O
,	O
in	O
which	O
the	O
ellipses	O
denote	O
the	O
one	O
standard-deviation	O
density	B
contours	O
for	O
each	O
of	O
the	O
components	O
,	O
and	O
the	O
density	B
of	O
red	O
ink	O
inside	O
each	O
ellipse	O
corresponds	O
to	O
the	O
mean	O
value	O
of	O
the	O
mixing	B
coefﬁcient	I
for	O
each	O
com-	O
ponent	O
.	O
the	O
number	O
in	O
the	O
top	O
left	O
of	O
each	O
diagram	O
shows	O
the	O
num-	O
ber	O
of	O
iterations	O
of	O
variational	B
infer-	O
ence	O
.	O
components	O
whose	O
expected	O
mixing	B
coefﬁcient	I
are	O
numerically	O
in-	O
distinguishable	O
from	O
zero	O
are	O
not	O
plotted	O
.	O
0	O
60	O
15	O
120	O
the	O
prior	B
tightly	O
constrains	O
the	O
mixing	O
coefﬁcients	O
so	O
that	O
α0	O
→	O
∞	O
,	O
then	O
e	O
[	O
πk	O
]	O
→	O
1/k	O
.	O
in	O
figure	O
10.6	O
,	O
the	O
prior	B
over	O
the	O
mixing	O
coefﬁcients	O
is	O
a	O
dirichlet	O
of	O
the	O
form	O
(	O
10.39	O
)	O
.	O
recall	O
from	O
figure	O
2.5	O
that	O
for	O
α0	O
<	O
1	O
the	O
prior	B
favours	O
solutions	O
in	O
which	O
some	O
of	O
the	O
mixing	O
coefﬁcients	O
are	O
zero	O
.	O
figure	O
10.6	O
was	O
obtained	O
using	O
α0	O
=	O
10−3	O
,	O
and	O
resulted	O
in	O
two	O
components	O
having	O
nonzero	O
mixing	O
coefﬁcients	O
.	O
if	O
instead	O
we	O
choose	O
α0	O
=	O
1	O
we	O
obtain	O
three	O
components	O
with	O
nonzero	O
mixing	O
coefﬁcients	O
,	O
and	O
for	O
α	O
=	O
10	O
all	O
six	O
components	O
have	O
nonzero	O
mixing	O
coefﬁcients	O
.	O
as	O
we	O
have	O
seen	O
there	O
is	O
a	O
close	O
similarity	O
between	O
the	O
variational	B
solution	O
for	O
the	O
bayesian	O
mixture	O
of	O
gaussians	O
and	O
the	O
em	O
algorithm	O
for	O
maximum	O
likelihood	O
.	O
in	O
fact	O
if	O
we	O
consider	O
the	O
limit	O
n	O
→	O
∞	O
then	O
the	O
bayesian	O
treatment	O
converges	O
to	O
the	O
maximum	B
likelihood	I
em	O
algorithm	O
.	O
for	O
anything	O
other	O
than	O
very	O
small	O
data	O
sets	O
,	O
the	O
dominant	O
computational	O
cost	O
of	O
the	O
variational	B
algorithm	O
for	O
gaussian	O
mixtures	O
arises	O
from	O
the	O
evaluation	O
of	O
the	O
responsibilities	O
,	O
together	O
with	O
the	O
evaluation	O
and	O
inversion	O
of	O
the	O
weighted	O
data	O
covariance	B
matrices	O
.	O
these	O
computations	O
mirror	O
pre-	O
cisely	O
those	O
that	O
arise	O
in	O
the	O
maximum	B
likelihood	I
em	O
algorithm	O
,	O
and	O
so	O
there	O
is	O
little	O
computational	O
overhead	O
in	O
using	O
this	O
bayesian	O
approach	O
as	O
compared	O
to	O
the	O
tradi-	O
tional	O
maximum	B
likelihood	I
one	O
.	O
there	O
are	O
,	O
however	O
,	O
some	O
substantial	O
advantages	O
.	O
first	O
of	O
all	O
,	O
the	O
singularities	B
that	O
arise	O
in	O
maximum	B
likelihood	I
when	O
a	O
gaussian	O
com-	O
ponent	O
‘	O
collapses	O
’	O
onto	O
a	O
speciﬁc	O
data	O
point	O
are	O
absent	O
in	O
the	O
bayesian	O
treatment	O
.	O
10.2.	O
illustration	O
:	O
variational	B
mixture	O
of	O
gaussians	O
481	O
indeed	O
,	O
these	O
singularities	B
are	O
removed	O
if	O
we	O
simply	O
introduce	O
a	O
prior	B
and	O
then	O
use	O
a	O
map	O
estimate	O
instead	O
of	O
maximum	B
likelihood	I
.	O
furthermore	O
,	O
there	O
is	O
no	O
over-ﬁtting	B
if	O
we	O
choose	O
a	O
large	O
number	O
k	O
of	O
components	O
in	O
the	O
mixture	B
,	O
as	O
we	O
saw	O
in	O
fig-	O
ure	O
10.6.	O
finally	O
,	O
the	O
variational	B
treatment	O
opens	O
up	O
the	O
possibility	O
of	O
determining	O
the	O
optimal	O
number	O
of	O
components	O
in	O
the	O
mixture	B
without	O
resorting	O
to	O
techniques	O
such	O
as	O
cross	O
validation	O
.	O
section	O
10.2.4	O
10.2.2	O
variational	B
lower	O
bound	O
we	O
can	O
also	O
straightforwardly	O
evaluate	O
the	O
lower	B
bound	I
(	O
10.3	O
)	O
for	O
this	O
model	O
.	O
in	O
practice	O
,	O
it	O
is	O
useful	O
to	O
be	O
able	O
to	O
monitor	O
the	O
bound	O
during	O
the	O
re-estimation	O
in	O
order	O
to	O
test	O
for	O
convergence	O
.	O
it	O
can	O
also	O
provide	O
a	O
valuable	O
check	O
on	O
both	O
the	O
math-	O
ematical	O
expressions	O
for	O
the	O
solutions	O
and	O
their	O
software	O
implementation	O
,	O
because	O
at	O
each	O
step	O
of	O
the	O
iterative	O
re-estimation	O
procedure	O
the	O
value	O
of	O
this	O
bound	O
should	O
not	O
decrease	O
.	O
we	O
can	O
take	O
this	O
a	O
stage	O
further	O
to	O
provide	O
a	O
deeper	O
test	O
of	O
the	O
correctness	O
of	O
both	O
the	O
mathematical	O
derivation	O
of	O
the	O
update	O
equations	O
and	O
of	O
their	O
software	O
im-	O
plementation	O
by	O
using	O
ﬁnite	B
differences	I
to	O
check	O
that	O
each	O
update	O
does	O
indeed	O
give	O
a	O
(	O
constrained	O
)	O
maximum	O
of	O
the	O
bound	O
(	O
svens´en	O
and	O
bishop	O
,	O
2004	O
)	O
.	O
for	O
the	O
variational	B
mixture	O
of	O
gaussians	O
,	O
the	O
lower	B
bound	I
(	O
10.3	O
)	O
is	O
given	O
by	O
l	O
=	O
p	O
(	O
x	O
,	O
z	O
,	O
π	O
,	O
µ	O
,	O
λ	O
)	O
q	O
(	O
z	O
,	O
π	O
,	O
µ	O
,	O
λ	O
)	O
ln	O
q	O
(	O
z	O
,	O
π	O
,	O
µ	O
,	O
λ	O
)	O
=	O
e	O
[	O
ln	O
p	O
(	O
x	O
,	O
z	O
,	O
π	O
,	O
µ	O
,	O
λ	O
)	O
]	O
−	O
e	O
[	O
ln	O
q	O
(	O
z	O
,	O
π	O
,	O
µ	O
,	O
λ	O
)	O
]	O
=	O
e	O
[	O
ln	O
p	O
(	O
x|z	O
,	O
µ	O
,	O
λ	O
)	O
]	O
+	O
e	O
[	O
ln	O
p	O
(	O
z|π	O
)	O
]	O
+	O
e	O
[	O
ln	O
p	O
(	O
π	O
)	O
]	O
+	O
e	O
[	O
ln	O
p	O
(	O
µ	O
,	O
λ	O
)	O
]	O
dπ	O
dµ	O
dλ	O
z	O
−e	O
[	O
ln	O
q	O
(	O
z	O
)	O
]	O
−	O
e	O
[	O
ln	O
q	O
(	O
π	O
)	O
]	O
−	O
e	O
[	O
ln	O
q	O
(	O
µ	O
,	O
λ	O
)	O
]	O
(	O
10.70	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:2	O
)	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
where	O
,	O
to	O
keep	O
the	O
notation	O
uncluttered	O
,	O
we	O
have	O
omitted	O
the	O
(	O
cid:12	O
)	O
superscript	O
on	O
the	O
q	O
distributions	O
,	O
along	O
with	O
the	O
subscripts	O
on	O
the	O
expectation	B
operators	O
because	O
each	O
expectation	B
is	O
taken	O
with	O
respect	O
to	O
all	O
of	O
the	O
random	O
variables	O
in	O
its	O
argument	O
.	O
the	O
various	O
terms	O
in	O
the	O
bound	O
are	O
easily	O
evaluated	O
to	O
give	O
the	O
following	O
results	O
exercise	O
10.16	O
k	O
(	O
cid:2	O
)	O
(	O
cid:12	O
)	O
ln	O
(	O
cid:4	O
)	O
λk	O
−	O
dβ	O
nk	O
1	O
2	O
−νk	O
(	O
xk	O
−	O
mk	O
)	O
twk	O
(	O
xk	O
−	O
mk	O
)	O
−	O
d	O
ln	O
(	O
2π	O
)	O
k	O
−	O
νktr	O
(	O
skwk	O
)	O
−1	O
k=1	O
(	O
cid:13	O
)	O
n	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
rnk	O
ln	O
(	O
cid:4	O
)	O
πk	O
e	O
[	O
ln	O
p	O
(	O
x|z	O
,	O
µ	O
,	O
λ	O
)	O
]	O
=	O
e	O
[	O
ln	O
p	O
(	O
z|π	O
)	O
]	O
=	O
n=1	O
k=1	O
e	O
[	O
ln	O
p	O
(	O
π	O
)	O
]	O
=	O
ln	O
c	O
(	O
α0	O
)	O
+	O
(	O
α0	O
−	O
1	O
)	O
k	O
(	O
cid:2	O
)	O
k=1	O
ln	O
(	O
cid:4	O
)	O
πk	O
(	O
10.71	O
)	O
(	O
10.72	O
)	O
(	O
10.73	O
)	O
482	O
10.	O
approximate	O
inference	B
e	O
[	O
ln	O
p	O
(	O
µ	O
,	O
λ	O
)	O
]	O
=	O
1	O
2	O
k	O
(	O
cid:2	O
)	O
k=1	O
(	O
cid:12	O
)	O
d	O
ln	O
(	O
β0/2π	O
)	O
+	O
ln	O
(	O
cid:4	O
)	O
λk	O
−	O
dβ0	O
βk	O
(	O
cid:13	O
)	O
+	O
k	O
ln	O
b	O
(	O
w0	O
,	O
ν0	O
)	O
νktr	O
(	O
w	O
−1	O
0	O
wk	O
)	O
−β0νk	O
(	O
mk	O
−	O
m0	O
)	O
twk	O
(	O
mk	O
−	O
m0	O
)	O
(	O
ν0	O
−	O
d	O
−	O
1	O
)	O
k	O
(	O
cid:2	O
)	O
+	O
2	O
k=1	O
k=1	O
ln	O
(	O
cid:4	O
)	O
λk	O
−	O
1	O
k	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
(	O
αk	O
−	O
1	O
)	O
ln	O
(	O
cid:4	O
)	O
πk	O
+	O
ln	O
c	O
(	O
α	O
)	O
(	O
cid:16	O
)	O
(	O
cid:12	O
)	O
ln	O
(	O
cid:4	O
)	O
λk	O
+	O
d	O
rnk	O
ln	O
rnk	O
(	O
cid:15	O
)	O
ln	O
k=1	O
1	O
2	O
βk	O
2π	O
2	O
2	O
n	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
n=1	O
k=1	O
k=1	O
e	O
[	O
ln	O
q	O
(	O
z	O
)	O
]	O
=	O
e	O
[	O
ln	O
q	O
(	O
π	O
)	O
]	O
=	O
e	O
[	O
ln	O
q	O
(	O
µ	O
,	O
λ	O
)	O
]	O
=	O
(	O
10.74	O
)	O
(	O
10.75	O
)	O
(	O
10.76	O
)	O
(	O
10.77	O
)	O
(	O
cid:13	O
)	O
−	O
h	O
[	O
q	O
(	O
λk	O
)	O
]	O
−	O
d	O
2	O
where	O
d	O
is	O
the	O
dimensionality	O
of	O
x	O
,	O
h	O
[	O
q	O
(	O
λk	O
)	O
]	O
is	O
the	O
entropy	B
of	O
the	O
wishart	O
distribu-	O
tion	O
given	O
by	O
(	O
b.82	O
)	O
,	O
and	O
the	O
coefﬁcients	O
c	O
(	O
α	O
)	O
and	O
b	O
(	O
w	O
,	O
ν	O
)	O
are	O
deﬁned	O
by	O
(	O
b.23	O
)	O
and	O
(	O
b.79	O
)	O
,	O
respectively	O
.	O
note	O
that	O
the	O
terms	O
involving	O
expectations	O
of	O
the	O
logs	O
of	O
the	O
q	O
distributions	O
simply	O
represent	O
the	O
negative	O
entropies	O
of	O
those	O
distributions	O
.	O
some	O
simpliﬁcations	O
and	O
combination	O
of	O
terms	O
can	O
be	O
performed	O
when	O
these	O
expressions	O
are	O
summed	O
to	O
give	O
the	O
lower	B
bound	I
.	O
however	O
,	O
we	O
have	O
kept	O
the	O
expressions	O
sepa-	O
rate	O
for	O
ease	O
of	O
understanding	O
.	O
finally	O
,	O
it	O
is	O
worth	O
noting	O
that	O
the	O
lower	B
bound	I
provides	O
an	O
alternative	O
approach	O
for	O
deriving	O
the	O
variational	B
re-estimation	O
equations	O
obtained	O
in	O
section	O
10.2.1.	O
to	O
do	O
this	O
we	O
use	O
the	O
fact	O
that	O
,	O
since	O
the	O
model	O
has	O
conjugate	B
priors	O
,	O
the	O
functional	B
form	O
of	O
the	O
factors	O
in	O
the	O
variational	B
posterior	O
distribution	O
is	O
known	O
,	O
namely	O
discrete	O
for	O
z	O
,	O
dirichlet	O
for	O
π	O
,	O
and	O
gaussian-wishart	O
for	O
(	O
µk	O
,	O
λk	O
)	O
.	O
by	O
taking	O
general	O
parametric	O
forms	O
for	O
these	O
distributions	O
we	O
can	O
derive	O
the	O
form	O
of	O
the	O
lower	B
bound	I
as	O
a	O
function	O
of	O
the	O
parameters	O
of	O
the	O
distributions	O
.	O
maximizing	O
the	O
bound	O
with	O
respect	O
to	O
these	O
parameters	O
then	O
gives	O
the	O
required	O
re-estimation	O
equations	O
.	O
exercise	O
10.18	O
10.2.3	O
predictive	O
density	O
in	O
applications	O
of	O
the	O
bayesian	O
mixture	O
of	O
gaussians	O
model	O
we	O
will	O
often	O
be	O
interested	O
in	O
the	O
predictive	O
density	O
for	O
a	O
new	O
value	O
(	O
cid:1	O
)	O
x	O
of	O
the	O
observed	B
variable	I
.	O
as-	O
sociated	O
with	O
this	O
observation	O
will	O
be	O
a	O
corresponding	O
latent	B
variable	I
(	O
cid:1	O
)	O
z	O
,	O
and	O
the	O
pre-	O
p	O
(	O
(	O
cid:1	O
)	O
x|	O
(	O
cid:1	O
)	O
z	O
,	O
µ	O
,	O
λ	O
)	O
p	O
(	O
(	O
cid:1	O
)	O
z|π	O
)	O
p	O
(	O
π	O
,	O
µ	O
,	O
λ|x	O
)	O
dπ	O
dµ	O
dλ	O
(	O
10.78	O
)	O
dictive	O
density	B
is	O
then	O
given	O
by	O
p	O
(	O
(	O
cid:1	O
)	O
x|x	O
)	O
=	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:2	O
)	O
bz	O
10.2.	O
illustration	O
:	O
variational	B
mixture	O
of	O
gaussians	O
483	O
where	O
p	O
(	O
π	O
,	O
µ	O
,	O
λ|x	O
)	O
is	O
the	O
(	O
unknown	O
)	O
true	O
posterior	O
distribution	O
of	O
the	O
parameters	O
.	O
using	O
(	O
10.37	O
)	O
and	O
(	O
10.38	O
)	O
we	O
can	O
ﬁrst	O
perform	O
the	O
summation	O
over	O
(	O
cid:1	O
)	O
z	O
to	O
give	O
p	O
(	O
(	O
cid:1	O
)	O
x|x	O
)	O
=	O
πkn	O
(	O
cid:10	O
)	O
(	O
cid:1	O
)	O
x|µk	O
,	O
λ	O
p	O
(	O
π	O
,	O
µ	O
,	O
λ|x	O
)	O
dπ	O
dµ	O
dλ	O
.	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
k	O
(	O
cid:2	O
)	O
(	O
10.79	O
)	O
(	O
cid:11	O
)	O
−1	O
k	O
k=1	O
because	O
the	O
remaining	O
integrations	O
are	O
intractable	O
,	O
we	O
approximate	O
the	O
predictive	O
density	O
by	O
replacing	O
the	O
true	O
posterior	O
distribution	O
p	O
(	O
π	O
,	O
µ	O
,	O
λ|x	O
)	O
with	O
its	O
variational	B
approximation	O
q	O
(	O
π	O
)	O
q	O
(	O
µ	O
,	O
λ	O
)	O
to	O
give	O
p	O
(	O
(	O
cid:1	O
)	O
x|x	O
)	O
=	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
k	O
(	O
cid:2	O
)	O
k=1	O
πkn	O
(	O
cid:10	O
)	O
(	O
cid:1	O
)	O
x|µk	O
,	O
λ	O
−1	O
k	O
(	O
cid:11	O
)	O
q	O
(	O
π	O
)	O
q	O
(	O
µk	O
,	O
λk	O
)	O
dπ	O
dµk	O
dλk	O
(	O
10.80	O
)	O
exercise	O
10.19	O
where	O
we	O
have	O
made	O
use	O
of	O
the	O
factorization	B
(	O
10.55	O
)	O
and	O
in	O
each	O
term	O
we	O
have	O
im-	O
plicitly	O
integrated	O
out	O
all	O
variables	O
{	O
µj	O
,	O
λj	O
}	O
for	O
j	O
(	O
cid:9	O
)	O
=	O
k	O
the	O
remaining	O
integrations	O
can	O
now	O
be	O
evaluated	O
analytically	O
giving	O
a	O
mixture	O
of	O
student	O
’	O
s	O
t-distributions	O
p	O
(	O
(	O
cid:1	O
)	O
x|x	O
)	O
=	O
k	O
(	O
cid:2	O
)	O
k=1	O
1	O
(	O
cid:1	O
)	O
α	O
αkst	O
(	O
(	O
cid:1	O
)	O
x|mk	O
,	O
lk	O
,	O
νk	O
+	O
1	O
−	O
d	O
)	O
(	O
10.81	O
)	O
in	O
which	O
the	O
kth	O
component	O
has	O
mean	B
mk	O
,	O
and	O
the	O
precision	O
is	O
given	O
by	O
lk	O
=	O
(	O
νk	O
+	O
1	O
−	O
d	O
)	O
βk	O
(	O
1	O
+	O
βk	O
)	O
wk	O
(	O
10.82	O
)	O
exercise	O
10.20	O
in	O
which	O
νk	O
is	O
given	O
by	O
(	O
10.63	O
)	O
.	O
when	O
the	O
size	O
n	O
of	O
the	O
data	O
set	O
is	O
large	O
the	O
predictive	B
distribution	I
(	O
10.81	O
)	O
reduces	O
to	O
a	O
mixture	O
of	O
gaussians	O
.	O
section	O
10.1.4	O
exercise	O
10.21	O
10.2.4	O
determining	O
the	O
number	O
of	O
components	O
we	O
have	O
seen	O
that	O
the	O
variational	B
lower	O
bound	O
can	O
be	O
used	O
to	O
determine	O
a	O
pos-	O
terior	O
distribution	O
over	O
the	O
number	O
k	O
of	O
components	O
in	O
the	O
mixture	B
model	I
.	O
there	O
is	O
,	O
however	O
,	O
one	O
subtlety	O
that	O
needs	O
to	O
be	O
addressed	O
.	O
for	O
any	O
given	O
setting	O
of	O
the	O
parameters	O
in	O
a	O
gaussian	O
mixture	B
model	I
(	O
except	O
for	O
speciﬁc	O
degenerate	O
settings	O
)	O
,	O
there	O
will	O
exist	O
other	O
parameter	O
settings	O
for	O
which	O
the	O
density	B
over	O
the	O
observed	O
vari-	O
ables	O
will	O
be	O
identical	O
.	O
these	O
parameter	O
values	O
differ	O
only	O
through	O
a	O
re-labelling	O
of	O
the	O
components	O
.	O
for	O
instance	O
,	O
consider	O
a	O
mixture	O
of	O
two	O
gaussians	O
and	O
a	O
single	O
ob-	O
served	O
variable	O
x	O
,	O
in	O
which	O
the	O
parameters	O
have	O
the	O
values	O
π1	O
=	O
a	O
,	O
π2	O
=	O
b	O
,	O
µ1	O
=	O
c	O
,	O
µ2	O
=	O
d	O
,	O
σ1	O
=	O
e	O
,	O
σ2	O
=	O
f.	O
then	O
the	O
parameter	O
values	O
π1	O
=	O
b	O
,	O
π2	O
=	O
a	O
,	O
µ1	O
=	O
d	O
,	O
µ2	O
=	O
c	O
,	O
σ1	O
=	O
f	O
,	O
σ2	O
=	O
e	O
,	O
in	O
which	O
the	O
two	O
components	O
have	O
been	O
exchanged	O
,	O
will	O
by	O
symmetry	O
give	O
rise	O
to	O
the	O
same	O
value	O
of	O
p	O
(	O
x	O
)	O
.	O
if	O
we	O
have	O
a	O
mixture	B
model	I
com-	O
prising	O
k	O
components	O
,	O
then	O
each	O
parameter	O
setting	O
will	O
be	O
a	O
member	O
of	O
a	O
family	O
of	O
k	O
!	O
equivalent	O
settings	O
.	O
in	O
the	O
context	O
of	O
maximum	B
likelihood	I
,	O
this	O
redundancy	O
is	O
irrelevant	O
because	O
the	O
parameter	O
optimization	O
algorithm	O
(	O
for	O
example	O
em	O
)	O
will	O
,	O
depending	O
on	O
the	O
initial-	O
ization	O
of	O
the	O
parameters	O
,	O
ﬁnd	O
one	O
speciﬁc	O
solution	O
,	O
and	O
the	O
other	O
equivalent	O
solu-	O
tions	O
play	O
no	O
role	O
.	O
in	O
a	O
bayesian	O
setting	O
,	O
however	O
,	O
we	O
marginalize	O
over	O
all	O
possible	O
484	O
10.	O
approximate	O
inference	B
figure	O
10.7	O
plot	O
of	O
the	O
variational	B
lower	O
bound	O
l	O
versus	O
the	O
number	O
k	O
of	O
com-	O
ponents	O
in	O
the	O
gaussian	O
mixture	B
model	I
,	O
for	O
the	O
old	O
faithful	O
data	O
,	O
showing	O
a	O
distinct	O
peak	O
at	O
k	O
=	O
2	O
components	O
.	O
for	O
each	O
value	O
of	O
k	O
,	O
the	O
model	O
is	O
trained	O
from	O
100	O
different	O
random	O
starts	O
,	O
and	O
the	O
results	O
shown	O
as	O
‘	O
+	O
’	O
symbols	O
plotted	O
with	O
small	O
random	O
hori-	O
zontal	O
perturbations	O
so	O
that	O
they	O
can	O
be	O
distinguished	O
.	O
note	O
that	O
some	O
solutions	O
ﬁnd	O
suboptimal	O
local	B
maxima	O
,	O
but	O
that	O
this	O
hap-	O
pens	O
infrequently	O
.	O
p	O
(	O
d|k	O
)	O
1	O
2	O
3	O
4	O
5	O
6	O
k	O
parameter	O
values	O
.	O
we	O
have	O
seen	O
in	O
figure	O
10.2	O
that	O
if	O
the	O
true	O
posterior	O
distribution	O
is	O
multimodal	O
,	O
variational	B
inference	I
based	O
on	O
the	O
minimization	O
of	O
kl	O
(	O
q	O
(	O
cid:5	O
)	O
p	O
)	O
will	O
tend	O
to	O
approximate	O
the	O
distribution	O
in	O
the	O
neighbourhood	O
of	O
one	O
of	O
the	O
modes	O
and	O
ignore	O
the	O
others	O
.	O
again	O
,	O
because	O
equivalent	O
modes	O
have	O
equivalent	O
predictive	O
densities	O
,	O
this	O
is	O
of	O
no	O
concern	O
provided	O
we	O
are	O
considering	O
a	O
model	O
having	O
a	O
speciﬁc	O
number	O
k	O
of	O
components	O
.	O
if	O
,	O
however	O
,	O
we	O
wish	O
to	O
compare	O
different	O
values	O
of	O
k	O
,	O
then	O
we	O
need	O
to	O
take	O
account	O
of	O
this	O
multimodality	B
.	O
a	O
simple	O
approximate	O
solution	O
is	O
to	O
add	O
a	O
term	O
ln	O
k	O
!	O
onto	O
the	O
lower	B
bound	I
when	O
used	O
for	O
model	O
comparison	O
and	O
averaging	O
.	O
figure	O
10.7	O
shows	O
a	O
plot	O
of	O
the	O
lower	B
bound	I
,	O
including	O
the	O
multimodality	B
fac-	O
tor	O
,	O
versus	O
the	O
number	O
k	O
of	O
components	O
for	O
the	O
old	O
faithful	O
data	O
set	O
.	O
it	O
is	O
worth	O
emphasizing	O
once	O
again	O
that	O
maximum	B
likelihood	I
would	O
lead	O
to	O
values	O
of	O
the	O
likeli-	O
hood	O
function	O
that	O
increase	O
monotonically	O
with	O
k	O
(	O
assuming	O
the	O
singular	O
solutions	O
have	O
been	O
avoided	O
,	O
and	O
discounting	O
the	O
effects	O
of	O
local	B
maxima	O
)	O
and	O
so	O
can	O
not	O
be	O
used	O
to	O
determine	O
an	O
appropriate	O
model	O
complexity	O
.	O
by	O
contrast	O
,	O
bayesian	O
inference	B
automatically	O
makes	O
the	O
trade-off	O
between	O
model	O
complexity	O
and	O
ﬁtting	O
the	O
data	O
.	O
this	O
approach	O
to	O
the	O
determination	O
of	O
k	O
requires	O
that	O
a	O
range	O
of	O
models	O
having	O
different	O
k	O
values	O
be	O
trained	O
and	O
compared	O
.	O
an	O
alternative	O
approach	O
to	O
determining	O
a	O
suitable	O
value	O
for	O
k	O
is	O
to	O
treat	O
the	O
mixing	O
coefﬁcients	O
π	O
as	O
parameters	O
and	O
make	O
point	O
estimates	O
of	O
their	O
values	O
by	O
maximizing	O
the	O
lower	B
bound	I
(	O
corduneanu	O
and	O
bishop	O
,	O
2001	O
)	O
with	O
respect	O
to	O
π	O
instead	O
of	O
maintaining	O
a	O
probability	B
distribution	O
over	O
them	O
as	O
in	O
the	O
fully	O
bayesian	O
approach	O
.	O
this	O
leads	O
to	O
the	O
re-estimation	O
equation	O
n	O
(	O
cid:2	O
)	O
πk	O
=	O
1	O
n	O
rnk	O
n=1	O
(	O
10.83	O
)	O
and	O
this	O
maximization	O
is	O
interleaved	O
with	O
the	O
variational	B
updates	O
for	O
the	O
q	O
distribution	O
over	O
the	O
remaining	O
parameters	O
.	O
components	O
that	O
provide	O
insufﬁcient	O
contribution	O
exercise	O
10.22	O
section	O
3.4	O
exercise	O
10.23	O
section	O
7.2.2	O
10.2.	O
illustration	O
:	O
variational	B
mixture	O
of	O
gaussians	O
485	O
to	O
explaining	O
the	O
data	O
will	O
have	O
their	O
mixing	O
coefﬁcients	O
driven	O
to	O
zero	O
during	O
the	O
optimization	O
,	O
and	O
so	O
they	O
are	O
effectively	O
removed	O
from	O
the	O
model	O
through	O
automatic	B
relevance	I
determination	I
.	O
this	O
allows	O
us	O
to	O
make	O
a	O
single	O
training	B
run	O
in	O
which	O
we	O
start	O
with	O
a	O
relatively	O
large	O
initial	O
value	O
of	O
k	O
,	O
and	O
allow	O
surplus	O
components	O
to	O
be	O
pruned	O
out	O
of	O
the	O
model	O
.	O
the	O
origins	O
of	O
the	O
sparsity	B
when	O
optimizing	O
with	O
respect	O
to	O
hyperparameters	O
is	O
discussed	O
in	O
detail	O
in	O
the	O
context	O
of	O
the	O
relevance	B
vector	I
machine	I
.	O
10.2.5	O
induced	O
factorizations	O
in	O
deriving	O
these	O
variational	B
update	O
equations	O
for	O
the	O
gaussian	O
mixture	B
model	I
,	O
we	O
assumed	O
a	O
particular	O
factorization	B
of	O
the	O
variational	B
posterior	O
distribution	O
given	O
by	O
(	O
10.42	O
)	O
.	O
however	O
,	O
the	O
optimal	O
solutions	O
for	O
the	O
various	O
factors	O
exhibit	O
additional	O
factorizations	O
.	O
in	O
particular	O
,	O
the	O
solution	O
for	O
q	O
(	O
cid:1	O
)	O
(	O
µ	O
,	O
λ	O
)	O
is	O
given	O
by	O
the	O
product	O
of	O
an	O
independent	B
distribution	O
q	O
(	O
cid:1	O
)	O
(	O
µk	O
,	O
λk	O
)	O
over	O
each	O
of	O
the	O
components	O
k	O
of	O
the	O
mixture	B
,	O
whereas	O
the	O
variational	B
posterior	O
distribution	O
q	O
(	O
cid:1	O
)	O
(	O
z	O
)	O
over	O
the	O
latent	O
variables	O
,	O
given	O
by	O
(	O
10.48	O
)	O
,	O
factorizes	O
into	O
an	O
independent	B
distribution	O
q	O
(	O
cid:1	O
)	O
(	O
zn	O
)	O
for	O
each	O
observation	O
n	O
(	O
note	O
that	O
it	O
does	O
not	O
further	O
factorize	O
with	O
respect	O
to	O
k	O
because	O
,	O
for	O
each	O
value	O
of	O
n	O
,	O
the	O
znk	O
are	O
constrained	O
to	O
sum	O
to	O
one	O
over	O
k	O
)	O
.	O
these	O
additional	O
factorizations	O
are	O
a	O
consequence	O
of	O
the	O
interaction	O
between	O
the	O
assumed	O
factorization	O
and	O
the	O
conditional	B
independence	I
properties	O
of	O
the	O
true	O
distribution	O
,	O
as	O
characterized	O
by	O
the	O
directed	B
graph	O
in	O
figure	O
10.5.	O
we	O
shall	O
refer	O
to	O
these	O
additional	O
factorizations	O
as	O
induced	O
factorizations	O
be-	O
cause	O
they	O
arise	O
from	O
an	O
interaction	O
between	O
the	O
factorization	B
assumed	O
in	O
the	O
varia-	O
tional	O
posterior	O
distribution	O
and	O
the	O
conditional	B
independence	I
properties	O
of	O
the	O
true	O
joint	O
distribution	O
.	O
in	O
a	O
numerical	O
implementation	O
of	O
the	O
variational	B
approach	O
it	O
is	O
important	O
to	O
take	O
account	O
of	O
such	O
additional	O
factorizations	O
.	O
for	O
instance	O
,	O
it	O
would	O
be	O
very	O
inefﬁcient	O
to	O
maintain	O
a	O
full	O
precision	B
matrix	I
for	O
the	O
gaussian	O
distribution	O
over	O
a	O
set	O
of	O
variables	O
if	O
the	O
optimal	O
form	O
for	O
that	O
distribution	O
always	O
had	O
a	O
diago-	O
nal	O
precision	B
matrix	I
(	O
corresponding	O
to	O
a	O
factorization	B
with	O
respect	O
to	O
the	O
individual	O
variables	O
described	O
by	O
that	O
gaussian	O
)	O
.	O
such	O
induced	O
factorizations	O
can	O
easily	O
be	O
detected	O
using	O
a	O
simple	O
graphical	O
test	O
based	O
on	O
d-separation	B
as	O
follows	O
.	O
we	O
partition	O
the	O
latent	O
variables	O
into	O
three	O
disjoint	O
groups	O
a	O
,	O
b	O
,	O
c	O
and	O
then	O
let	O
us	O
suppose	O
that	O
we	O
are	O
assuming	O
a	O
factorization	B
between	O
c	O
and	O
the	O
remaining	O
latent	O
variables	O
,	O
so	O
that	O
q	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
q	O
(	O
a	O
,	O
b	O
)	O
q	O
(	O
c	O
)	O
.	O
(	O
10.84	O
)	O
using	O
the	O
general	O
result	O
(	O
10.9	O
)	O
,	O
together	O
with	O
the	O
product	B
rule	I
for	O
probabilities	O
,	O
we	O
see	O
that	O
the	O
optimal	O
solution	O
for	O
q	O
(	O
a	O
,	O
b	O
)	O
is	O
given	O
by	O
ln	O
q	O
(	O
cid:1	O
)	O
(	O
a	O
,	O
b	O
)	O
=	O
ec	O
[	O
ln	O
p	O
(	O
x	O
,	O
a	O
,	O
b	O
,	O
c	O
)	O
]	O
+	O
const	O
=	O
ec	O
[	O
ln	O
p	O
(	O
a	O
,	O
b|x	O
,	O
c	O
)	O
]	O
+	O
const	O
.	O
(	O
10.85	O
)	O
we	O
now	O
ask	O
whether	O
this	O
resulting	O
solution	O
will	O
factorize	O
between	O
a	O
and	O
b	O
,	O
in	O
other	O
words	O
whether	O
q	O
(	O
cid:1	O
)	O
(	O
a	O
,	O
b	O
)	O
=	O
q	O
(	O
cid:1	O
)	O
(	O
a	O
)	O
q	O
(	O
cid:1	O
)	O
(	O
b	O
)	O
.	O
this	O
will	O
happen	O
if	O
,	O
and	O
only	O
if	O
,	O
ln	O
p	O
(	O
a	O
,	O
b|x	O
,	O
c	O
)	O
=	O
ln	O
p	O
(	O
a|x	O
,	O
c	O
)	O
+	O
ln	O
p	O
(	O
b|x	O
,	O
c	O
)	O
,	O
that	O
is	O
,	O
if	O
the	O
conditional	B
inde-	O
pendence	O
relation	O
a	O
⊥⊥	O
b	O
|	O
x	O
,	O
c	O
(	O
10.86	O
)	O
486	O
10.	O
approximate	O
inference	B
is	O
satisﬁed	O
.	O
we	O
can	O
test	O
to	O
see	O
if	O
this	O
relation	O
does	O
hold	O
,	O
for	O
any	O
choice	O
of	O
a	O
and	O
b	O
by	O
making	O
use	O
of	O
the	O
d-separation	B
criterion	O
.	O
to	O
illustrate	O
this	O
,	O
consider	O
again	O
the	O
bayesian	O
mixture	O
of	O
gaussians	O
represented	O
by	O
the	O
directed	B
graph	O
in	O
figure	O
10.5	O
,	O
in	O
which	O
we	O
are	O
assuming	O
a	O
variational	B
fac-	O
torization	O
given	O
by	O
(	O
10.42	O
)	O
.	O
we	O
can	O
see	O
immediately	O
that	O
the	O
variational	B
posterior	O
distribution	O
over	O
the	O
parameters	O
must	O
factorize	O
between	O
π	O
and	O
the	O
remaining	O
param-	O
eters	O
µ	O
and	O
λ	O
because	O
all	O
paths	O
connecting	O
π	O
to	O
either	O
µ	O
or	O
λ	O
must	O
pass	O
through	O
one	O
of	O
the	O
nodes	O
zn	O
all	O
of	O
which	O
are	O
in	O
the	O
conditioning	O
set	O
for	O
our	O
conditional	B
inde-	O
pendence	O
test	O
and	O
all	O
of	O
which	O
are	O
head-to-tail	O
with	O
respect	O
to	O
such	O
paths	O
.	O
10.3.	O
variational	B
linear	O
regression	B
exercise	O
10.26	O
as	O
a	O
second	O
illustration	O
of	O
variational	B
inference	I
,	O
we	O
return	O
to	O
the	O
bayesian	O
linear	B
regression	I
model	O
of	O
section	O
3.3.	O
in	O
the	O
evidence	O
framework	O
,	O
we	O
approximated	O
the	O
integration	O
over	O
α	O
and	O
β	O
by	O
making	O
point	O
estimates	O
obtained	O
by	O
maximizing	O
the	O
log	O
marginal	O
likelihood	O
.	O
a	O
fully	O
bayesian	O
approach	O
would	O
integrate	O
over	O
the	O
hyperpa-	O
rameters	O
as	O
well	O
as	O
over	O
the	O
parameters	O
.	O
although	O
exact	O
integration	O
is	O
intractable	O
,	O
we	O
can	O
use	O
variational	B
methods	O
to	O
ﬁnd	O
a	O
tractable	O
approximation	O
.	O
in	O
order	O
to	O
sim-	O
plify	O
the	O
discussion	O
,	O
we	O
shall	O
suppose	O
that	O
the	O
noise	O
precision	B
parameter	I
β	O
is	O
known	O
,	O
and	O
is	O
ﬁxed	O
to	O
its	O
true	O
value	O
,	O
although	O
the	O
framework	O
is	O
easily	O
extended	B
to	O
include	O
the	O
distribution	O
over	O
β.	O
for	O
the	O
linear	B
regression	I
model	O
,	O
the	O
variational	B
treatment	O
will	O
turn	O
out	O
to	O
be	O
equivalent	O
to	O
the	O
evidence	O
framework	O
.	O
nevertheless	O
,	O
it	O
provides	O
a	O
good	O
exercise	O
in	O
the	O
use	O
of	O
variational	B
methods	O
and	O
will	O
also	O
lay	O
the	O
foundation	O
for	O
variational	O
treatment	O
of	O
bayesian	O
logistic	B
regression	I
in	O
section	O
10.6.	O
recall	O
that	O
the	O
likelihood	B
function	I
for	O
w	O
,	O
and	O
the	O
prior	B
over	O
w	O
,	O
are	O
given	O
by	O
n	O
(	O
cid:14	O
)	O
n	O
(	O
tn|wtφn	O
,	O
β	O
−1	O
)	O
(	O
10.87	O
)	O
p	O
(	O
t|w	O
)	O
=	O
p	O
(	O
w|α	O
)	O
=	O
n	O
(	O
w|0	O
,	O
α	O
n=1	O
−1i	O
)	O
(	O
10.88	O
)	O
where	O
φn	O
=	O
φ	O
(	O
xn	O
)	O
.	O
we	O
now	O
introduce	O
a	O
prior	B
distribution	O
over	O
α.	O
from	O
our	O
dis-	O
cussion	O
in	O
section	O
2.3.6	O
,	O
we	O
know	O
that	O
the	O
conjugate	B
prior	I
for	O
the	O
precision	O
of	O
a	O
gaussian	O
is	O
given	O
by	O
a	O
gamma	B
distribution	I
,	O
and	O
so	O
we	O
choose	O
(	O
10.89	O
)	O
where	O
gam	O
(	O
·|·	O
,	O
·	O
)	O
is	O
deﬁned	O
by	O
(	O
b.26	O
)	O
.	O
thus	O
the	O
joint	O
distribution	O
of	O
all	O
the	O
variables	O
is	O
given	O
by	O
p	O
(	O
α	O
)	O
=	O
gam	O
(	O
α|a0	O
,	O
b0	O
)	O
p	O
(	O
t	O
,	O
w	O
,	O
α	O
)	O
=	O
p	O
(	O
t|w	O
)	O
p	O
(	O
w|α	O
)	O
p	O
(	O
α	O
)	O
.	O
(	O
10.90	O
)	O
this	O
can	O
be	O
represented	O
as	O
a	O
directed	B
graphical	O
model	O
as	O
shown	O
in	O
figure	O
10.8	O
.	O
10.3.1	O
variational	B
distribution	O
our	O
ﬁrst	O
goal	O
is	O
to	O
ﬁnd	O
an	O
approximation	O
to	O
the	O
posterior	O
distribution	O
p	O
(	O
w	O
,	O
α|t	O
)	O
.	O
to	O
do	O
this	O
,	O
we	O
employ	O
the	O
variational	B
framework	O
of	O
section	O
10.1	O
,	O
with	O
a	O
variational	B
10.3.	O
variational	B
linear	O
regression	B
487	O
figure	O
10.8	O
probabilistic	B
graphical	I
model	I
representing	O
the	O
joint	O
dis-	O
regression	B
the	O
bayesian	O
linear	O
for	O
tribution	O
(	O
10.90	O
)	O
model	O
.	O
α	O
w	O
φn	O
β	O
tn	O
n	O
posterior	O
distribution	O
given	O
by	O
the	O
factorized	O
expression	O
q	O
(	O
w	O
,	O
α	O
)	O
=	O
q	O
(	O
w	O
)	O
q	O
(	O
α	O
)	O
.	O
(	O
10.91	O
)	O
we	O
can	O
ﬁnd	O
re-estimation	O
equations	O
for	O
the	O
factors	O
in	O
this	O
distribution	O
by	O
making	O
use	O
of	O
the	O
general	O
result	O
(	O
10.9	O
)	O
.	O
recall	O
that	O
for	O
each	O
factor	O
,	O
we	O
take	O
the	O
log	O
of	O
the	O
joint	O
distribution	O
over	O
all	O
variables	O
and	O
then	O
average	O
with	O
respect	O
to	O
those	O
variables	O
not	O
in	O
that	O
factor	O
.	O
consider	O
ﬁrst	O
the	O
distribution	O
over	O
α.	O
keeping	O
only	O
terms	O
that	O
have	O
a	O
functional	B
dependence	O
on	O
α	O
,	O
we	O
have	O
ln	O
q	O
(	O
cid:1	O
)	O
(	O
α	O
)	O
=	O
ln	O
p	O
(	O
α	O
)	O
+	O
ew	O
[	O
ln	O
p	O
(	O
w|α	O
)	O
]	O
+	O
const	O
=	O
(	O
a0	O
−	O
1	O
)	O
ln	O
α	O
−	O
b0α	O
+	O
m	O
2	O
ln	O
α	O
−	O
α	O
2	O
e	O
[	O
wtw	O
]	O
+	O
const	O
.	O
(	O
10.92	O
)	O
we	O
recognize	O
this	O
as	O
the	O
log	O
of	O
a	O
gamma	B
distribution	I
,	O
and	O
so	O
identifying	O
the	O
coefﬁ-	O
cients	O
of	O
α	O
and	O
ln	O
α	O
we	O
obtain	O
q	O
(	O
cid:1	O
)	O
(	O
α	O
)	O
=	O
gam	O
(	O
α|an	O
,	O
bn	O
)	O
where	O
an	O
=	O
a0	O
+	O
m	O
2	O
1	O
2	O
e	O
[	O
wtw	O
]	O
.	O
bn	O
=	O
b0	O
+	O
similarly	O
,	O
we	O
can	O
ﬁnd	O
the	O
variational	B
re-estimation	O
equation	O
for	O
the	O
posterior	O
distribution	O
over	O
w.	O
again	O
,	O
using	O
the	O
general	O
result	O
(	O
10.9	O
)	O
,	O
and	O
keeping	O
only	O
those	O
terms	O
that	O
have	O
a	O
functional	B
dependence	O
on	O
w	O
,	O
we	O
have	O
ln	O
q	O
(	O
cid:1	O
)	O
(	O
w	O
)	O
=	O
ln	O
p	O
(	O
t|w	O
)	O
+	O
eα	O
[	O
ln	O
p	O
(	O
w|α	O
)	O
]	O
+	O
const	O
(	O
10.96	O
)	O
(	O
10.93	O
)	O
(	O
10.94	O
)	O
(	O
10.95	O
)	O
(	O
10.97	O
)	O
(	O
10.98	O
)	O
(	O
10.99	O
)	O
n	O
(	O
cid:2	O
)	O
{	O
wtφn	O
−	O
tn	O
}	O
2	O
−	O
1	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
n=1	O
=	O
−	O
β	O
2	O
=	O
−1	O
2	O
2	O
e	O
[	O
α	O
]	O
wtw	O
+	O
const	O
wt	O
e	O
[	O
α	O
]	O
i	O
+	O
βφtφ	O
w	O
+	O
βwtφtt	O
+	O
const	O
.	O
because	O
this	O
is	O
a	O
quadratic	O
form	O
,	O
the	O
distribution	O
q	O
(	O
cid:1	O
)	O
(	O
w	O
)	O
is	O
gaussian	O
,	O
and	O
so	O
we	O
can	O
complete	O
the	O
square	O
in	O
the	O
usual	O
way	O
to	O
identify	O
the	O
mean	B
and	O
covariance	B
,	O
giving	O
q	O
(	O
cid:1	O
)	O
(	O
w	O
)	O
=	O
n	O
(	O
w|mn	O
,	O
sn	O
)	O
488	O
10.	O
approximate	O
inference	B
where	O
(	O
cid:10	O
)	O
mn	O
=	O
βsn	O
φtt	O
sn	O
=	O
e	O
[	O
α	O
]	O
i	O
+	O
βφtφ	O
(	O
cid:11	O
)	O
−1	O
.	O
(	O
10.100	O
)	O
(	O
10.101	O
)	O
note	O
the	O
close	O
similarity	O
to	O
the	O
posterior	O
distribution	O
(	O
3.52	O
)	O
obtained	O
when	O
α	O
was	O
treated	O
as	O
a	O
ﬁxed	O
parameter	O
.	O
the	O
difference	O
is	O
that	O
here	O
α	O
is	O
replaced	O
by	O
its	O
expecta-	O
tion	O
e	O
[	O
α	O
]	O
under	O
the	O
variational	B
distribution	O
.	O
indeed	O
,	O
we	O
have	O
chosen	O
to	O
use	O
the	O
same	O
notation	O
for	O
the	O
covariance	B
matrix	I
sn	O
in	O
both	O
cases	O
.	O
using	O
the	O
standard	O
results	O
(	O
b.27	O
)	O
,	O
(	O
b.38	O
)	O
,	O
and	O
(	O
b.39	O
)	O
,	O
we	O
can	O
obtain	O
the	O
required	O
moments	O
as	O
follows	O
e	O
[	O
α	O
]	O
=	O
an	O
/bn	O
e	O
[	O
wwt	O
]	O
=	O
mn	O
mt	O
n	O
+	O
sn	O
.	O
(	O
10.102	O
)	O
(	O
10.103	O
)	O
the	O
evaluation	O
of	O
the	O
variational	B
posterior	O
distribution	O
begins	O
by	O
initializing	O
the	O
pa-	O
rameters	O
of	O
one	O
of	O
the	O
distributions	O
q	O
(	O
w	O
)	O
or	O
q	O
(	O
α	O
)	O
,	O
and	O
then	O
alternately	O
re-estimates	O
these	O
factors	O
in	O
turn	O
until	O
a	O
suitable	O
convergence	O
criterion	O
is	O
satisﬁed	O
(	O
usually	O
speci-	O
ﬁed	O
in	O
terms	O
of	O
the	O
lower	B
bound	I
to	O
be	O
discussed	O
shortly	O
)	O
.	O
it	O
is	O
instructive	O
to	O
relate	O
the	O
variational	B
solution	O
to	O
that	O
found	O
using	O
the	O
evidence	O
framework	O
in	O
section	O
3.5.	O
to	O
do	O
this	O
consider	O
the	O
case	O
a0	O
=	O
b0	O
=	O
0	O
,	O
corresponding	O
to	O
the	O
limit	O
of	O
an	O
inﬁnitely	O
broad	O
prior	B
over	O
α.	O
the	O
mean	B
of	O
the	O
variational	B
posterior	O
distribution	O
q	O
(	O
α	O
)	O
is	O
then	O
given	O
by	O
e	O
[	O
α	O
]	O
=	O
an	O
bn	O
=	O
m/2	O
e	O
[	O
wtw	O
]	O
/2	O
=	O
m	O
n	O
mn	O
+	O
tr	O
(	O
sn	O
)	O
.	O
mt	O
(	O
10.104	O
)	O
comparison	O
with	O
(	O
9.63	O
)	O
shows	O
that	O
in	O
the	O
case	O
of	O
this	O
particularly	O
simple	O
model	O
,	O
the	O
variational	B
approach	O
gives	O
precisely	O
the	O
same	O
expression	O
as	O
that	O
obtained	O
by	O
maximizing	O
the	O
evidence	B
function	I
using	O
em	O
except	O
that	O
the	O
point	O
estimate	O
for	O
α	O
is	O
replaced	O
by	O
its	O
expected	O
value	O
.	O
because	O
the	O
distribution	O
q	O
(	O
w	O
)	O
depends	O
on	O
q	O
(	O
α	O
)	O
only	O
through	O
the	O
expectation	B
e	O
[	O
α	O
]	O
,	O
we	O
see	O
that	O
the	O
two	O
approaches	O
will	O
give	O
identical	O
results	O
for	O
the	O
case	O
of	O
an	O
inﬁnitely	O
broad	O
prior	B
.	O
10.3.2	O
predictive	B
distribution	I
the	O
predictive	B
distribution	I
over	O
t	O
,	O
given	O
a	O
new	O
input	O
x	O
,	O
is	O
easily	O
evaluated	O
for	O
this	O
model	O
using	O
the	O
gaussian	O
variational	B
posterior	O
for	O
the	O
parameters	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
p	O
(	O
t|x	O
,	O
t	O
)	O
=	O
(	O
cid:7	O
)	O
p	O
(	O
t|x	O
,	O
w	O
)	O
p	O
(	O
w|t	O
)	O
dw	O
p	O
(	O
t|x	O
,	O
w	O
)	O
q	O
(	O
w	O
)	O
dw	O
n	O
(	O
t|wtφ	O
(	O
x	O
)	O
,	O
β	O
n	O
φ	O
(	O
x	O
)	O
,	O
σ2	O
(	O
x	O
)	O
)	O
=	O
=	O
n	O
(	O
t|mt	O
−1	O
)	O
n	O
(	O
w|mn	O
,	O
sn	O
)	O
dw	O
(	O
10.105	O
)	O
10.3.	O
variational	B
linear	O
regression	B
489	O
where	O
we	O
have	O
evaluated	O
the	O
integral	O
by	O
making	O
use	O
of	O
the	O
result	O
(	O
2.115	O
)	O
for	O
the	O
linear-gaussian	O
model	O
.	O
here	O
the	O
input-dependent	O
variance	B
is	O
given	O
by	O
σ2	O
(	O
x	O
)	O
=	O
1	O
β	O
+	O
φ	O
(	O
x	O
)	O
tsn	O
φ	O
(	O
x	O
)	O
.	O
(	O
10.106	O
)	O
note	O
that	O
this	O
takes	O
the	O
same	O
form	O
as	O
the	O
result	O
(	O
3.59	O
)	O
obtained	O
with	O
ﬁxed	O
α	O
except	O
that	O
now	O
the	O
expected	O
value	O
e	O
[	O
α	O
]	O
appears	O
in	O
the	O
deﬁnition	O
of	O
sn	O
.	O
10.3.3	O
lower	B
bound	I
another	O
quantity	O
of	O
importance	O
is	O
the	O
lower	B
bound	I
l	O
deﬁned	O
by	O
l	O
(	O
q	O
)	O
=	O
e	O
[	O
ln	O
p	O
(	O
w	O
,	O
α	O
,	O
t	O
)	O
]	O
−	O
e	O
[	O
ln	O
q	O
(	O
w	O
,	O
α	O
)	O
]	O
−eα	O
[	O
ln	O
q	O
(	O
w	O
)	O
]	O
w	O
−	O
e	O
[	O
ln	O
q	O
(	O
α	O
)	O
]	O
.	O
=	O
ew	O
[	O
ln	O
p	O
(	O
t|w	O
)	O
]	O
+	O
ew	O
,	O
α	O
[	O
ln	O
p	O
(	O
w|α	O
)	O
]	O
+	O
eα	O
[	O
ln	O
p	O
(	O
α	O
)	O
]	O
(	O
10.107	O
)	O
exercise	O
10.27	O
evaluation	O
of	O
the	O
various	O
terms	O
is	O
straightforward	O
,	O
making	O
use	O
of	O
results	O
obtained	O
in	O
previous	O
chapters	O
,	O
and	O
gives	O
(	O
cid:16	O
)	O
(	O
cid:15	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
e	O
[	O
ln	O
p	O
(	O
t|w	O
)	O
]	O
w	O
=	O
n	O
ln	O
2	O
−	O
β	O
2	O
tr	O
e	O
[	O
ln	O
p	O
(	O
w|α	O
)	O
]	O
w	O
,	O
α	O
=	O
−	O
m	O
2	O
−	O
an	O
2bn	O
β	O
2π	O
−	O
β	O
2	O
ttt	O
+	O
βmt	O
n	O
φtt	O
(	O
cid:9	O
)	O
φtφ	O
(	O
mn	O
mt	O
ln	O
(	O
2π	O
)	O
+	O
m	O
2	O
n	O
+	O
sn	O
)	O
(	O
ψ	O
(	O
an	O
)	O
−	O
ln	O
bn	O
)	O
(	O
cid:9	O
)	O
e	O
[	O
ln	O
p	O
(	O
α	O
)	O
]	O
α	O
=	O
a0	O
ln	O
b0	O
+	O
(	O
a0	O
−	O
1	O
)	O
[	O
ψ	O
(	O
an	O
)	O
−	O
ln	O
bn	O
]	O
mt	O
n	O
mn	O
+	O
tr	O
(	O
sn	O
)	O
(	O
10.108	O
)	O
(	O
10.109	O
)	O
(	O
10.110	O
)	O
−	O
ln	O
γ	O
(	O
an	O
)	O
−b0	O
an	O
bn	O
ln|sn|	O
+	O
m	O
1	O
2	O
2	O
[	O
1	O
+	O
ln	O
(	O
2π	O
)	O
]	O
−e	O
[	O
ln	O
q	O
(	O
w	O
)	O
]	O
w	O
=	O
(	O
10.111	O
)	O
−e	O
[	O
ln	O
q	O
(	O
α	O
)	O
]	O
α	O
=	O
ln	O
γ	O
(	O
an	O
)	O
−	O
(	O
an	O
−	O
1	O
)	O
ψ	O
(	O
an	O
)	O
−	O
ln	O
bn	O
+	O
an	O
.	O
(	O
10.112	O
)	O
figure	O
10.9	O
shows	O
a	O
plot	O
of	O
the	O
lower	B
bound	I
l	O
(	O
q	O
)	O
versus	O
the	O
degree	O
of	O
a	O
polynomial	O
model	O
for	O
a	O
synthetic	O
data	O
set	O
generated	O
from	O
a	O
degree	O
three	O
polynomial	O
.	O
here	O
the	O
prior	B
parameters	O
have	O
been	O
set	O
to	O
a0	O
=	O
b0	O
=	O
0	O
,	O
corresponding	O
to	O
the	O
noninformative	B
prior	I
p	O
(	O
α	O
)	O
∝	O
1/α	O
,	O
which	O
is	O
uniform	O
over	O
ln	O
α	O
as	O
discussed	O
in	O
section	O
2.3.6.	O
as	O
we	O
saw	O
in	O
section	O
10.1	O
,	O
the	O
quantity	O
l	O
represents	O
lower	B
bound	I
on	O
the	O
log	O
marginal	O
likelihood	O
p	O
(	O
t|m	O
)	O
for	O
the	O
model	O
.	O
if	O
we	O
assign	O
equal	O
prior	B
probabilities	O
p	O
(	O
m	O
)	O
to	O
the	O
different	O
values	O
of	O
m	O
,	O
then	O
we	O
can	O
interpret	O
l	O
as	O
an	O
approximation	O
to	O
the	O
poste-	O
rior	O
model	O
probability	O
p	O
(	O
m|t	O
)	O
.	O
thus	O
the	O
variational	B
framework	O
assigns	O
the	O
highest	O
probability	B
to	O
the	O
model	O
with	O
m	O
=	O
3.	O
this	O
should	O
be	O
contrasted	O
with	O
the	O
maximum	B
likelihood	I
result	O
,	O
which	O
assigns	O
ever	O
smaller	O
residual	O
error	B
to	O
models	O
of	O
increasing	O
complexity	O
until	O
the	O
residual	O
error	B
is	O
driven	O
to	O
zero	O
,	O
causing	O
maximum	B
likelihood	I
to	O
favour	O
severely	O
over-ﬁtted	O
models	O
.	O
490	O
10.	O
approximate	O
inference	B
figure	O
10.9	O
plot	O
of	O
the	O
lower	B
bound	I
l	O
ver-	O
sus	O
the	O
order	O
m	O
of	O
the	O
polyno-	O
mial	O
,	O
for	O
a	O
polynomial	O
model	O
,	O
in	O
which	O
a	O
set	O
of	O
10	O
data	O
points	O
is	O
generated	O
from	O
a	O
polynomial	O
with	O
m	O
=	O
3	O
sampled	O
over	O
the	O
inter-	O
val	O
(	O
−5	O
,	O
5	O
)	O
with	O
additive	O
gaussian	O
noise	O
of	O
variance	B
0.09.	O
the	O
value	O
of	O
the	O
bound	O
gives	O
the	O
log	O
prob-	O
ability	O
of	O
the	O
model	O
,	O
and	O
we	O
see	O
that	O
the	O
value	O
of	O
the	O
bound	O
peaks	O
at	O
m	O
=	O
3	O
,	O
corresponding	O
to	O
the	O
true	O
model	O
from	O
which	O
the	O
data	O
set	O
was	O
generated	O
.	O
1	O
3	O
5	O
7	O
9	O
10.4.	O
exponential	B
family	I
distributions	O
in	O
chapter	O
2	O
,	O
we	O
discussed	O
the	O
important	O
role	O
played	O
by	O
the	O
exponential	B
family	I
of	O
distributions	O
and	O
their	O
conjugate	B
priors	O
.	O
for	O
many	O
of	O
the	O
models	O
discussed	O
in	O
this	O
book	O
,	O
the	O
complete-data	O
likelihood	O
is	O
drawn	O
from	O
the	O
exponential	B
family	I
.	O
however	O
,	O
in	O
general	O
this	O
will	O
not	O
be	O
the	O
case	O
for	O
the	O
marginal	B
likelihood	I
function	O
for	O
the	O
ob-	O
served	O
data	O
.	O
for	O
example	O
,	O
in	O
a	O
mixture	O
of	O
gaussians	O
,	O
the	O
joint	O
distribution	O
of	O
obser-	O
vations	O
xn	O
and	O
corresponding	O
hidden	O
variables	O
zn	O
is	O
a	O
member	O
of	O
the	O
exponential	B
family	I
,	O
whereas	O
the	O
marginal	B
distribution	O
of	O
xn	O
is	O
a	O
mixture	O
of	O
gaussians	O
and	O
hence	O
is	O
not	O
.	O
up	O
to	O
now	O
we	O
have	O
grouped	O
the	O
variables	O
in	O
the	O
model	O
into	O
observed	O
variables	O
and	O
hidden	O
variables	O
.	O
we	O
now	O
make	O
a	O
further	O
distinction	O
between	O
latent	O
variables	O
,	O
denoted	O
z	O
,	O
and	O
parameters	O
,	O
denoted	O
θ	O
,	O
where	O
parameters	O
are	O
intensive	O
(	O
ﬁxed	O
in	O
num-	O
ber	O
independent	B
of	O
the	O
size	O
of	O
the	O
data	O
set	O
)	O
,	O
whereas	O
latent	O
variables	O
are	O
extensive	O
(	O
scale	O
in	O
number	O
with	O
the	O
size	O
of	O
the	O
data	O
set	O
)	O
.	O
for	O
example	O
,	O
in	O
a	O
gaussian	O
mixture	B
model	I
,	O
the	O
indicator	O
variables	O
zkn	O
(	O
which	O
specify	O
which	O
component	O
k	O
is	O
responsible	O
for	O
generating	O
data	O
point	O
xn	O
)	O
represent	O
the	O
latent	O
variables	O
,	O
whereas	O
the	O
means	O
µk	O
,	O
precisions	O
λk	O
and	O
mixing	O
proportions	O
πk	O
represent	O
the	O
parameters	O
.	O
consider	O
the	O
case	O
of	O
independent	B
identically	I
distributed	I
data	O
.	O
we	O
denote	O
the	O
data	O
values	O
by	O
x	O
=	O
{	O
xn	O
}	O
,	O
where	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
n	O
,	O
with	O
corresponding	O
latent	O
variables	O
z	O
=	O
{	O
zn	O
}	O
.	O
now	O
suppose	O
that	O
the	O
joint	O
distribution	O
of	O
observed	O
and	O
latent	O
variables	O
is	O
a	O
member	O
of	O
the	O
exponential	B
family	I
,	O
parameterized	O
by	O
natural	B
parameters	I
η	O
so	O
that	O
p	O
(	O
x	O
,	O
z|η	O
)	O
=	O
h	O
(	O
xn	O
,	O
zn	O
)	O
g	O
(	O
η	O
)	O
exp	O
ηtu	O
(	O
xn	O
,	O
zn	O
)	O
n	O
(	O
cid:14	O
)	O
n=1	O
(	O
cid:26	O
)	O
(	O
cid:26	O
)	O
(	O
cid:27	O
)	O
(	O
cid:27	O
)	O
.	O
(	O
10.113	O
)	O
.	O
(	O
10.114	O
)	O
we	O
shall	O
also	O
use	O
a	O
conjugate	B
prior	I
for	O
η	O
,	O
which	O
can	O
be	O
written	O
as	O
p	O
(	O
η|ν0	O
,	O
v0	O
)	O
=	O
f	O
(	O
ν0	O
,	O
χ0	O
)	O
g	O
(	O
η	O
)	O
ν0	O
exp	O
νoηtχ0	O
recall	O
that	O
the	O
conjugate	B
prior	I
distribution	O
can	O
be	O
interpreted	O
as	O
a	O
prior	B
number	O
ν0	O
of	O
observations	O
all	O
having	O
the	O
value	O
χ0	O
for	O
the	O
u	O
vector	O
.	O
now	O
consider	O
a	O
variational	B
10.4.	O
exponential	B
family	I
distributions	O
491	O
distribution	O
that	O
factorizes	O
between	O
the	O
latent	O
variables	O
and	O
the	O
parameters	O
,	O
so	O
that	O
q	O
(	O
z	O
,	O
η	O
)	O
=	O
q	O
(	O
z	O
)	O
q	O
(	O
η	O
)	O
.	O
using	O
the	O
general	O
result	O
(	O
10.9	O
)	O
,	O
we	O
can	O
solve	O
for	O
the	O
two	O
factors	O
as	O
follows	O
ln	O
q	O
(	O
cid:1	O
)	O
(	O
z	O
)	O
=	O
eη	O
[	O
ln	O
p	O
(	O
x	O
,	O
z|η	O
)	O
]	O
+	O
const	O
=	O
ln	O
h	O
(	O
xn	O
,	O
zn	O
)	O
+	O
e	O
[	O
ηt	O
]	O
u	O
(	O
xn	O
,	O
zn	O
)	O
+	O
const	O
.	O
(	O
10.115	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:26	O
)	O
n=1	O
(	O
cid:27	O
)	O
(	O
cid:27	O
)	O
(	O
cid:27	O
)	O
(	O
cid:27	O
)	O
(	O
cid:21	O
)	O
thus	O
we	O
see	O
that	O
this	O
decomposes	O
into	O
a	O
sum	O
of	O
independent	B
terms	O
,	O
one	O
for	O
each	O
value	O
of	O
n	O
,	O
and	O
hence	O
the	O
solution	O
for	O
q	O
(	O
cid:1	O
)	O
(	O
z	O
)	O
will	O
factorize	O
over	O
n	O
so	O
that	O
q	O
(	O
cid:1	O
)	O
(	O
z	O
)	O
=	O
n	O
q	O
(	O
cid:1	O
)	O
(	O
zn	O
)	O
.	O
this	O
is	O
an	O
example	O
of	O
an	O
induced	B
factorization	I
.	O
taking	O
the	O
exponential	O
section	O
10.2.5	O
of	O
both	O
sides	O
,	O
we	O
have	O
(	O
cid:26	O
)	O
(	O
cid:26	O
)	O
q	O
(	O
cid:1	O
)	O
(	O
zn	O
)	O
=	O
h	O
(	O
xn	O
,	O
zn	O
)	O
g	O
(	O
e	O
[	O
η	O
]	O
)	O
exp	O
e	O
[	O
ηt	O
]	O
u	O
(	O
xn	O
,	O
zn	O
)	O
(	O
10.116	O
)	O
where	O
the	O
normalization	O
coefﬁcient	O
has	O
been	O
re-instated	O
by	O
comparison	O
with	O
the	O
standard	O
form	O
for	O
the	O
exponential	B
family	I
.	O
similarly	O
,	O
for	O
the	O
variational	B
distribution	O
over	O
the	O
parameters	O
,	O
we	O
have	O
ln	O
q	O
(	O
cid:1	O
)	O
(	O
η	O
)	O
=	O
ln	O
p	O
(	O
η|ν0	O
,	O
χ0	O
)	O
+	O
ez	O
[	O
ln	O
p	O
(	O
x	O
,	O
z|η	O
)	O
]	O
+	O
const	O
(	O
10.117	O
)	O
=	O
ν0	O
ln	O
g	O
(	O
η	O
)	O
+	O
ηtχ0	O
+	O
ln	O
g	O
(	O
η	O
)	O
+	O
ηt	O
ezn	O
[	O
u	O
(	O
xn	O
,	O
zn	O
)	O
]	O
+	O
const	O
.	O
(	O
10.118	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:26	O
)	O
n=1	O
again	O
,	O
taking	O
the	O
exponential	O
of	O
both	O
sides	O
,	O
and	O
re-instating	O
the	O
normalization	O
coef-	O
ﬁcient	O
by	O
inspection	O
,	O
we	O
have	O
q	O
(	O
cid:1	O
)	O
(	O
η	O
)	O
=	O
f	O
(	O
νn	O
,	O
χn	O
)	O
g	O
(	O
η	O
)	O
νn	O
exp	O
ηtχn	O
where	O
we	O
have	O
deﬁned	O
νn	O
=	O
ν0	O
+	O
n	O
n	O
(	O
cid:2	O
)	O
χn	O
=	O
χ0	O
+	O
ezn	O
[	O
u	O
(	O
xn	O
,	O
zn	O
)	O
]	O
.	O
(	O
10.119	O
)	O
(	O
10.120	O
)	O
(	O
10.121	O
)	O
n=1	O
note	O
that	O
the	O
solutions	O
for	O
q	O
(	O
cid:1	O
)	O
(	O
zn	O
)	O
and	O
q	O
(	O
cid:1	O
)	O
(	O
η	O
)	O
are	O
coupled	O
,	O
and	O
so	O
we	O
solve	O
them	O
iter-	O
atively	O
in	O
a	O
two-stage	O
procedure	O
.	O
in	O
the	O
variational	B
e	O
step	O
,	O
we	O
evaluate	O
the	O
expected	O
sufﬁcient	B
statistics	I
e	O
[	O
u	O
(	O
xn	O
,	O
zn	O
)	O
]	O
using	O
the	O
current	O
posterior	O
distribution	O
q	O
(	O
zn	O
)	O
over	O
the	O
latent	O
variables	O
and	O
use	O
this	O
to	O
compute	O
a	O
revised	O
posterior	O
distribution	O
q	O
(	O
η	O
)	O
over	O
the	O
parameters	O
.	O
then	O
in	O
the	O
subsequent	O
variational	B
m	O
step	O
,	O
we	O
use	O
this	O
revised	O
pa-	O
rameter	O
posterior	O
distribution	O
to	O
ﬁnd	O
the	O
expected	O
natural	B
parameters	I
e	O
[	O
ηt	O
]	O
,	O
which	O
gives	O
rise	O
to	O
a	O
revised	O
variational	B
distribution	O
over	O
the	O
latent	O
variables	O
.	O
10.4.1	O
variational	B
message	O
passing	O
we	O
have	O
illustrated	O
the	O
application	O
of	O
variational	B
methods	O
by	O
considering	O
a	O
spe-	O
ciﬁc	O
model	O
,	O
the	O
bayesian	O
mixture	O
of	O
gaussians	O
,	O
in	O
some	O
detail	O
.	O
this	O
model	O
can	O
be	O
492	O
10.	O
approximate	O
inference	B
described	O
by	O
the	O
directed	B
graph	O
shown	O
in	O
figure	O
10.5.	O
here	O
we	O
consider	O
more	O
gen-	O
erally	O
the	O
use	O
of	O
variational	B
methods	O
for	O
models	O
described	O
by	O
directed	B
graphs	O
and	O
derive	O
a	O
number	O
of	O
widely	O
applicable	O
results	O
.	O
the	O
joint	O
distribution	O
corresponding	O
to	O
a	O
directed	B
graph	O
can	O
be	O
written	O
using	O
the	O
decomposition	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
xi|pai	O
)	O
(	O
10.122	O
)	O
where	O
xi	O
denotes	O
the	O
variable	O
(	O
s	O
)	O
associated	O
with	O
node	B
i	O
,	O
and	O
pai	O
denotes	O
the	O
parent	O
set	O
corresponding	O
to	O
node	B
i.	O
note	O
that	O
xi	O
may	O
be	O
a	O
latent	B
variable	I
or	O
it	O
may	O
belong	O
to	O
the	O
set	O
of	O
observed	O
variables	O
.	O
now	O
consider	O
a	O
variational	B
approximation	O
in	O
which	O
the	O
distribution	O
q	O
(	O
x	O
)	O
is	O
assumed	O
to	O
factorize	O
with	O
respect	O
to	O
the	O
xi	O
so	O
that	O
q	O
(	O
x	O
)	O
=	O
qi	O
(	O
xi	O
)	O
.	O
(	O
10.123	O
)	O
(	O
cid:14	O
)	O
i	O
(	O
cid:14	O
)	O
i	O
(	O
cid:31	O
)	O
(	O
cid:2	O
)	O
ln	O
p	O
(	O
xi|pai	O
)	O
note	O
that	O
for	O
observed	O
nodes	O
,	O
there	O
is	O
no	O
factor	O
q	O
(	O
xi	O
)	O
in	O
the	O
variational	B
distribution	O
.	O
we	O
now	O
substitute	O
(	O
10.122	O
)	O
into	O
our	O
general	O
result	O
(	O
10.9	O
)	O
to	O
give	O
ln	O
q	O
(	O
cid:1	O
)	O
j	O
(	O
xj	O
)	O
=	O
ei	O
(	O
cid:9	O
)	O
=j	O
+	O
const	O
.	O
(	O
10.124	O
)	O
i	O
any	O
terms	O
on	O
the	O
right-hand	O
side	O
that	O
do	O
not	O
depend	O
on	O
xj	O
can	O
be	O
absorbed	O
into	O
in	O
fact	O
,	O
the	O
only	O
terms	O
that	O
do	O
depend	O
on	O
xj	O
are	O
the	O
con-	O
the	O
additive	O
constant	O
.	O
ditional	O
distribution	O
for	O
xj	O
given	O
by	O
p	O
(	O
xj|paj	O
)	O
together	O
with	O
any	O
other	O
conditional	B
distributions	O
that	O
have	O
xj	O
in	O
the	O
conditioning	O
set	O
.	O
by	O
deﬁnition	O
,	O
these	O
conditional	B
distributions	O
correspond	O
to	O
the	O
children	O
of	O
node	B
j	O
,	O
and	O
they	O
therefore	O
also	O
depend	O
on	O
the	O
co-parents	B
of	O
the	O
child	O
nodes	O
,	O
i.e.	O
,	O
the	O
other	O
parents	O
of	O
the	O
child	O
nodes	O
besides	O
node	B
xj	O
itself	O
.	O
we	O
see	O
that	O
the	O
set	O
of	O
all	O
nodes	O
on	O
which	O
q	O
(	O
cid:1	O
)	O
(	O
xj	O
)	O
depends	O
corresponds	O
to	O
the	O
markov	O
blanket	O
of	O
node	B
xj	O
,	O
as	O
illustrated	O
in	O
figure	O
8.26.	O
thus	O
the	O
update	O
of	O
the	O
factors	O
in	O
the	O
variational	B
posterior	O
distribution	O
represents	O
a	O
local	B
calculation	O
on	O
the	O
graph	O
.	O
this	O
makes	O
possible	O
the	O
construction	O
of	O
general	O
purpose	O
software	O
for	O
variational	O
inference	B
in	O
which	O
the	O
form	O
of	O
the	O
model	O
does	O
not	O
need	O
to	O
be	O
speciﬁed	O
in	O
advance	O
(	O
bishop	O
et	O
al.	O
,	O
2003	O
)	O
.	O
if	O
we	O
now	O
specialize	O
to	O
the	O
case	O
of	O
a	O
model	O
in	O
which	O
all	O
of	O
the	O
conditional	B
dis-	O
tributions	O
have	O
a	O
conjugate-exponential	O
structure	O
,	O
then	O
the	O
variational	B
update	O
proce-	O
dure	O
can	O
be	O
cast	O
in	O
terms	O
of	O
a	O
local	B
message	O
passing	O
algorithm	O
(	O
winn	O
and	O
bishop	O
,	O
2005	O
)	O
.	O
in	O
particular	O
,	O
the	O
distribution	O
associated	O
with	O
a	O
particular	O
node	B
can	O
be	O
updated	O
once	O
that	O
node	B
has	O
received	O
messages	O
from	O
all	O
of	O
its	O
parents	O
and	O
all	O
of	O
its	O
children	O
.	O
this	O
in	O
turn	O
requires	O
that	O
the	O
children	O
have	O
already	O
received	O
messages	O
from	O
their	O
co-	O
parents	O
.	O
the	O
evaluation	O
of	O
the	O
lower	B
bound	I
can	O
also	O
be	O
simpliﬁed	O
because	O
many	O
of	O
the	O
required	O
quantities	O
are	O
already	O
evaluated	O
as	O
part	O
of	O
the	O
message	B
passing	I
scheme	O
.	O
this	O
distributed	O
message	B
passing	I
formulation	O
has	O
good	O
scaling	O
properties	O
and	O
is	O
well	O
suited	O
to	O
large	O
networks	O
.	O
10.5.	O
local	B
variational	O
methods	O
493	O
10.5.	O
local	B
variational	O
methods	O
the	O
variational	B
framework	O
discussed	O
in	O
sections	O
10.1	O
and	O
10.2	O
can	O
be	O
considered	O
a	O
‘	O
global	O
’	O
method	O
in	O
the	O
sense	O
that	O
it	O
directly	O
seeks	O
an	O
approximation	O
to	O
the	O
full	O
poste-	O
rior	O
distribution	O
over	O
all	O
random	O
variables	O
.	O
an	O
alternative	O
‘	O
local	B
’	O
approach	O
involves	O
ﬁnding	O
bounds	O
on	O
functions	O
over	O
individual	O
variables	O
or	O
groups	O
of	O
variables	O
within	O
a	O
model	O
.	O
for	O
instance	O
,	O
we	O
might	O
seek	O
a	O
bound	O
on	O
a	O
conditional	B
distribution	O
p	O
(	O
y|x	O
)	O
,	O
which	O
is	O
itself	O
just	O
one	O
factor	O
in	O
a	O
much	O
larger	O
probabilistic	O
model	O
speciﬁed	O
by	O
a	O
directed	B
graph	O
.	O
the	O
purpose	O
of	O
introducing	O
the	O
bound	O
of	O
course	O
is	O
to	O
simplify	O
the	O
resulting	O
distribution	O
.	O
this	O
local	B
approximation	O
can	O
be	O
applied	O
to	O
multiple	O
variables	O
in	O
turn	O
until	O
a	O
tractable	O
approximation	O
is	O
obtained	O
,	O
and	O
in	O
section	O
10.6.1	O
we	O
shall	O
give	O
a	O
practical	O
example	O
of	O
this	O
approach	O
in	O
the	O
context	O
of	O
logistic	B
regression	I
.	O
here	O
we	O
focus	O
on	O
developing	O
the	O
bounds	O
themselves	O
.	O
we	O
have	O
already	O
seen	O
in	O
our	O
discussion	O
of	O
the	O
kullback-leibler	O
divergence	O
that	O
the	O
convexity	O
of	O
the	O
logarithm	O
function	O
played	O
a	O
key	O
role	O
in	O
developing	O
the	O
lower	B
bound	I
in	O
the	O
global	O
variational	O
approach	O
.	O
we	O
have	O
deﬁned	O
a	O
(	O
strictly	O
)	O
convex	O
func-	O
tion	O
as	O
one	O
for	O
which	O
every	O
chord	O
lies	O
above	O
the	O
function	O
.	O
convexity	O
also	O
plays	O
a	O
central	O
role	O
in	O
the	O
local	B
variational	O
framework	O
.	O
note	O
that	O
our	O
discussion	O
will	O
ap-	O
ply	O
equally	O
to	O
concave	O
functions	O
with	O
‘	O
min	O
’	O
and	O
‘	O
max	O
’	O
interchanged	O
and	O
with	O
lower	O
bounds	O
replaced	O
by	O
upper	O
bounds	O
.	O
let	O
us	O
begin	O
by	O
considering	O
a	O
simple	O
example	O
,	O
namely	O
the	O
function	O
f	O
(	O
x	O
)	O
=	O
exp	O
(	O
−x	O
)	O
,	O
which	O
is	O
a	O
convex	B
function	I
of	O
x	O
,	O
and	O
which	O
is	O
shown	O
in	O
the	O
left-hand	O
plot	O
of	O
figure	O
10.10.	O
our	O
goal	O
is	O
to	O
approximate	O
f	O
(	O
x	O
)	O
by	O
a	O
simpler	O
function	O
,	O
in	O
particular	O
a	O
linear	O
function	O
of	O
x.	O
from	O
figure	O
10.10	O
,	O
we	O
see	O
that	O
this	O
linear	O
function	O
will	O
be	O
a	O
lower	B
bound	I
on	O
f	O
(	O
x	O
)	O
if	O
it	O
corresponds	O
to	O
a	O
tangent	O
.	O
we	O
can	O
obtain	O
the	O
tangent	O
line	O
y	O
(	O
x	O
)	O
at	O
a	O
speciﬁc	O
value	O
of	O
x	O
,	O
say	O
x	O
=	O
ξ	O
,	O
by	O
making	O
a	O
ﬁrst	B
order	I
taylor	O
expansion	O
section	O
1.6.1	O
(	O
10.125	O
)	O
so	O
that	O
y	O
(	O
x	O
)	O
(	O
cid:1	O
)	O
f	O
(	O
x	O
)	O
with	O
equality	O
when	O
x	O
=	O
ξ.	O
for	O
our	O
example	O
function	O
f	O
(	O
x	O
)	O
=	O
y	O
(	O
x	O
)	O
=	O
f	O
(	O
ξ	O
)	O
+	O
f	O
(	O
cid:4	O
)	O
(	O
ξ	O
)	O
(	O
x	O
−	O
ξ	O
)	O
figure	O
10.10	O
in	O
the	O
left-hand	O
ﬁg-	O
ure	O
the	O
red	O
curve	O
shows	O
the	O
function	O
exp	O
(	O
−x	O
)	O
,	O
and	O
the	O
blue	O
line	O
shows	O
the	O
tangent	O
at	O
x	O
=	O
ξ	O
deﬁned	O
by	O
(	O
10.125	O
)	O
with	O
ξ	O
=	O
1.	O
this	O
line	O
has	O
slope	O
λ	O
=	O
f	O
(	O
cid:1	O
)	O
(	O
ξ	O
)	O
=	O
−	O
exp	O
(	O
−ξ	O
)	O
.	O
note	O
that	O
any	O
other	O
tangent	O
line	O
,	O
for	O
ex-	O
ample	O
the	O
ones	O
shown	O
in	O
green	O
,	O
will	O
have	O
a	O
smaller	O
value	O
of	O
y	O
at	O
x	O
=	O
ξ.	O
the	O
right-hand	O
ﬁgure	O
shows	O
the	O
corresponding	O
plot	O
of	O
the	O
function	O
λξ	O
−	O
g	O
(	O
λ	O
)	O
,	O
where	O
g	O
(	O
λ	O
)	O
is	O
given	O
by	O
(	O
10.131	O
)	O
,	O
versus	O
λ	O
for	O
ξ	O
=	O
1	O
,	O
in	O
which	O
the	O
maximum	O
corresponds	O
to	O
λ	O
=	O
−	O
exp	O
(	O
−ξ	O
)	O
=	O
−1/e	O
.	O
1	O
0.5	O
0	O
0	O
ξ	O
1.5	O
x	O
3	O
0.4	O
λξ	O
−	O
g	O
(	O
λ	O
)	O
0.2	O
0	O
−1	O
−0.5	O
λ	O
0	O
494	O
10.	O
approximate	O
inference	B
y	O
f	O
(	O
x	O
)	O
y	O
−g	O
(	O
λ	O
)	O
f	O
(	O
x	O
)	O
x	O
λx	O
x	O
λx	O
−	O
g	O
(	O
λ	O
)	O
figure	O
10.11	O
in	O
the	O
left-hand	O
plot	O
the	O
red	O
curve	O
shows	O
a	O
convex	B
function	I
f	O
(	O
x	O
)	O
,	O
and	O
the	O
blue	O
line	O
represents	O
the	O
linear	O
function	O
λx	O
,	O
which	O
is	O
a	O
lower	B
bound	I
on	O
f	O
(	O
x	O
)	O
because	O
f	O
(	O
x	O
)	O
>	O
λx	O
for	O
all	O
x.	O
for	O
the	O
given	O
value	O
of	O
slope	O
λ	O
the	O
contact	O
point	O
of	O
the	O
tangent	O
line	O
having	O
the	O
same	O
slope	O
is	O
found	O
by	O
minimizing	O
with	O
respect	O
to	O
x	O
the	O
discrepancy	O
(	O
shown	O
by	O
the	O
green	O
dashed	O
lines	O
)	O
given	O
by	O
f	O
(	O
x	O
)	O
−	O
λx	O
.	O
this	O
deﬁnes	O
the	O
dual	O
function	O
g	O
(	O
λ	O
)	O
,	O
which	O
corresponds	O
to	O
the	O
(	O
negative	O
of	O
the	O
)	O
intercept	O
of	O
the	O
tangent	O
line	O
having	O
slope	O
λ.	O
exp	O
(	O
−x	O
)	O
,	O
we	O
therefore	O
obtain	O
the	O
tangent	O
line	O
in	O
the	O
form	O
y	O
(	O
x	O
)	O
=	O
exp	O
(	O
−ξ	O
)	O
−	O
exp	O
(	O
−ξ	O
)	O
(	O
x	O
−	O
ξ	O
)	O
(	O
10.126	O
)	O
which	O
is	O
a	O
linear	O
function	O
parameterized	O
by	O
ξ.	O
for	O
consistency	O
with	O
subsequent	O
discussion	O
,	O
let	O
us	O
deﬁne	O
λ	O
=	O
−	O
exp	O
(	O
−ξ	O
)	O
so	O
that	O
y	O
(	O
x	O
,	O
λ	O
)	O
=	O
λx	O
−	O
λ	O
+	O
λ	O
ln	O
(	O
−λ	O
)	O
.	O
different	O
values	O
of	O
λ	O
correspond	O
to	O
different	O
tangent	O
lines	O
,	O
and	O
because	O
all	O
such	O
lines	O
are	O
lower	O
bounds	O
on	O
the	O
function	O
,	O
we	O
have	O
f	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
y	O
(	O
x	O
,	O
λ	O
)	O
.	O
thus	O
we	O
can	O
write	O
the	O
function	O
in	O
the	O
form	O
(	O
10.127	O
)	O
(	O
10.128	O
)	O
f	O
(	O
x	O
)	O
=	O
max	O
λ	O
{	O
λx	O
−	O
λ	O
+	O
λ	O
ln	O
(	O
−λ	O
)	O
}	O
.	O
we	O
have	O
succeeded	O
in	O
approximating	O
the	O
convex	B
function	I
f	O
(	O
x	O
)	O
by	O
a	O
simpler	O
,	O
lin-	O
ear	O
function	O
y	O
(	O
x	O
,	O
λ	O
)	O
.	O
the	O
price	O
we	O
have	O
paid	O
is	O
that	O
we	O
have	O
introduced	O
a	O
variational	B
parameter	O
λ	O
,	O
and	O
to	O
obtain	O
the	O
tightest	O
bound	O
we	O
must	O
optimize	O
with	O
respect	O
to	O
λ.	O
we	O
can	O
formulate	O
this	O
approach	O
more	O
generally	O
using	O
the	O
framework	O
of	O
convex	B
duality	I
(	O
rockafellar	O
,	O
1972	O
;	O
jordan	O
et	O
al.	O
,	O
1999	O
)	O
.	O
consider	O
the	O
illustration	O
of	O
a	O
convex	B
function	I
f	O
(	O
x	O
)	O
shown	O
in	O
the	O
left-hand	O
plot	O
in	O
figure	O
10.11.	O
in	O
this	O
example	O
,	O
the	O
function	O
λx	O
is	O
a	O
lower	B
bound	I
on	O
f	O
(	O
x	O
)	O
but	O
it	O
is	O
not	O
the	O
best	O
lower	B
bound	I
that	O
can	O
be	O
achieved	O
by	O
a	O
linear	O
function	O
having	O
slope	O
λ	O
,	O
because	O
the	O
tightest	O
bound	O
is	O
given	O
by	O
the	O
tangent	O
line	O
.	O
let	O
us	O
write	O
the	O
equation	O
of	O
the	O
tangent	O
line	O
,	O
having	O
slope	O
λ	O
as	O
λx	O
−	O
g	O
(	O
λ	O
)	O
where	O
the	O
(	O
negative	O
)	O
intercept	O
g	O
(	O
λ	O
)	O
clearly	O
depends	O
on	O
the	O
slope	O
λ	O
of	O
the	O
tangent	O
.	O
to	O
determine	O
the	O
intercept	O
,	O
we	O
note	O
that	O
the	O
line	O
must	O
be	O
moved	O
vertically	O
by	O
an	O
amount	O
equal	O
to	O
the	O
smallest	O
vertical	O
distance	O
between	O
the	O
line	O
and	O
the	O
function	O
,	O
as	O
shown	O
in	O
figure	O
10.11.	O
thus	O
g	O
(	O
λ	O
)	O
=	O
−	O
min	O
x	O
=	O
max	O
{	O
f	O
(	O
x	O
)	O
−	O
λx	O
}	O
{	O
λx	O
−	O
f	O
(	O
x	O
)	O
}	O
.	O
x	O
(	O
10.129	O
)	O
10.5.	O
local	B
variational	O
methods	O
495	O
now	O
,	O
instead	O
of	O
ﬁxing	O
λ	O
and	O
varying	O
x	O
,	O
we	O
can	O
consider	O
a	O
particular	O
x	O
and	O
then	O
adjust	O
λ	O
until	O
the	O
tangent	O
plane	O
is	O
tangent	O
at	O
that	O
particular	O
x.	O
because	O
the	O
y	O
value	O
of	O
the	O
tangent	O
line	O
at	O
a	O
particular	O
x	O
is	O
maximized	O
when	O
that	O
value	O
coincides	O
with	O
its	O
contact	O
point	O
,	O
we	O
have	O
{	O
λx	O
−	O
g	O
(	O
λ	O
)	O
}	O
.	O
(	O
10.130	O
)	O
f	O
(	O
x	O
)	O
=	O
max	O
λ	O
we	O
see	O
that	O
the	O
functions	O
f	O
(	O
x	O
)	O
and	O
g	O
(	O
λ	O
)	O
play	O
a	O
dual	O
role	O
,	O
and	O
are	O
related	O
through	O
(	O
10.129	O
)	O
and	O
(	O
10.130	O
)	O
.	O
let	O
us	O
apply	O
these	O
duality	O
relations	O
to	O
our	O
simple	O
example	O
f	O
(	O
x	O
)	O
=	O
exp	O
(	O
−x	O
)	O
.	O
from	O
(	O
10.129	O
)	O
we	O
see	O
that	O
the	O
maximizing	O
value	O
of	O
x	O
is	O
given	O
by	O
ξ	O
=	O
−	O
ln	O
(	O
−λ	O
)	O
,	O
and	O
back-substituting	O
we	O
obtain	O
the	O
conjugate	B
function	O
g	O
(	O
λ	O
)	O
in	O
the	O
form	O
g	O
(	O
λ	O
)	O
=	O
λ	O
−	O
λ	O
ln	O
(	O
−λ	O
)	O
(	O
10.131	O
)	O
as	O
obtained	O
previously	O
.	O
the	O
function	O
λξ	O
−	O
g	O
(	O
λ	O
)	O
is	O
shown	O
,	O
for	O
ξ	O
=	O
1	O
in	O
the	O
right-hand	O
plot	O
in	O
figure	O
10.10.	O
as	O
a	O
check	O
,	O
we	O
can	O
substitute	O
(	O
10.131	O
)	O
into	O
(	O
10.130	O
)	O
,	O
which	O
gives	O
the	O
maximizing	O
value	O
of	O
λ	O
=	O
−	O
exp	O
(	O
−x	O
)	O
,	O
and	O
back-substituting	O
then	O
recovers	O
the	O
original	O
function	O
f	O
(	O
x	O
)	O
=	O
exp	O
(	O
−x	O
)	O
.	O
for	O
concave	O
functions	O
,	O
we	O
can	O
follow	O
a	O
similar	O
argument	O
to	O
obtain	O
upper	O
bounds	O
,	O
in	O
which	O
max	O
’	O
is	O
replaced	O
with	O
‘	O
min	O
’	O
,	O
so	O
that	O
f	O
(	O
x	O
)	O
=	O
min	O
λ	O
g	O
(	O
λ	O
)	O
=	O
min	O
x	O
{	O
λx	O
−	O
g	O
(	O
λ	O
)	O
}	O
{	O
λx	O
−	O
f	O
(	O
x	O
)	O
}	O
.	O
(	O
10.132	O
)	O
(	O
10.133	O
)	O
if	O
the	O
function	O
of	O
interest	O
is	O
not	O
convex	O
(	O
or	O
concave	O
)	O
,	O
then	O
we	O
can	O
not	O
directly	O
apply	O
the	O
method	O
above	O
to	O
obtain	O
a	O
bound	O
.	O
however	O
,	O
we	O
can	O
ﬁrst	O
seek	O
invertible	O
transformations	O
either	O
of	O
the	O
function	O
or	O
of	O
its	O
argument	O
which	O
change	O
it	O
into	O
a	O
con-	O
vex	O
form	O
.	O
we	O
then	O
calculate	O
the	O
conjugate	B
function	O
and	O
then	O
transform	O
back	O
to	O
the	O
original	O
variables	O
.	O
an	O
important	O
example	O
,	O
which	O
arises	O
frequently	O
in	O
pattern	O
recognition	O
,	O
is	O
the	O
logistic	B
sigmoid	I
function	O
deﬁned	O
by	O
σ	O
(	O
x	O
)	O
=	O
1	O
1	O
+	O
e−x	O
.	O
(	O
10.134	O
)	O
as	O
it	O
stands	O
this	O
function	O
is	O
neither	O
convex	O
nor	O
concave	O
.	O
however	O
,	O
if	O
we	O
take	O
the	O
logarithm	O
we	O
obtain	O
a	O
function	O
which	O
is	O
concave	O
,	O
as	O
is	O
easily	O
veriﬁed	O
by	O
ﬁnding	O
the	O
second	O
derivative	O
.	O
from	O
(	O
10.133	O
)	O
the	O
corresponding	O
conjugate	B
function	O
then	O
takes	O
the	O
form	O
g	O
(	O
λ	O
)	O
=	O
min	O
x	O
{	O
λx	O
−	O
f	O
(	O
x	O
)	O
}	O
=	O
−λ	O
ln	O
λ	O
−	O
(	O
1	O
−	O
λ	O
)	O
ln	O
(	O
1	O
−	O
λ	O
)	O
(	O
10.135	O
)	O
which	O
we	O
recognize	O
as	O
the	O
binary	B
entropy	I
function	O
for	O
a	O
variable	O
whose	O
probability	B
of	O
having	O
the	O
value	O
1	O
is	O
λ.	O
using	O
(	O
10.132	O
)	O
,	O
we	O
then	O
obtain	O
an	O
upper	O
bound	O
on	O
the	O
log	O
sigmoid	O
ln	O
σ	O
(	O
x	O
)	O
(	O
cid:1	O
)	O
λx	O
−	O
g	O
(	O
λ	O
)	O
(	O
10.136	O
)	O
exercise	O
10.30	O
appendix	O
b	O
496	O
10.	O
approximate	O
inference	B
1	O
0.5	O
0	O
−6	O
λ	O
=	O
0.2	O
λ	O
=	O
0.7	O
0	O
1	O
0.5	O
ξ	O
=	O
2.5	O
6	O
0	O
−6	O
−ξ	O
0	O
ξ	O
6	O
figure	O
10.12	O
the	O
left-hand	O
plot	O
shows	O
the	O
logistic	B
sigmoid	I
function	O
σ	O
(	O
x	O
)	O
deﬁned	O
by	O
(	O
10.134	O
)	O
in	O
red	O
,	O
together	O
with	O
two	O
examples	O
of	O
the	O
exponential	O
upper	O
bound	O
(	O
10.137	O
)	O
shown	O
in	O
blue	O
.	O
the	O
right-hand	O
plot	O
shows	O
the	O
logistic	B
sigmoid	I
again	O
in	O
red	O
together	O
with	O
the	O
gaussian	O
lower	B
bound	I
(	O
10.144	O
)	O
shown	O
in	O
blue	O
.	O
here	O
the	O
parameter	O
ξ	O
=	O
2.5	O
,	O
and	O
the	O
bound	O
is	O
exact	O
at	O
x	O
=	O
ξ	O
and	O
x	O
=	O
−ξ	O
,	O
denoted	O
by	O
the	O
dashed	O
green	O
lines	O
.	O
and	O
taking	O
the	O
exponential	O
,	O
we	O
obtain	O
an	O
upper	O
bound	O
on	O
the	O
logistic	B
sigmoid	I
itself	O
of	O
the	O
form	O
σ	O
(	O
x	O
)	O
(	O
cid:1	O
)	O
exp	O
(	O
λx	O
−	O
g	O
(	O
λ	O
)	O
)	O
(	O
10.137	O
)	O
exercise	O
10.31	O
which	O
is	O
plotted	O
for	O
two	O
values	O
of	O
λ	O
on	O
the	O
left-hand	O
plot	O
in	O
figure	O
10.12.	O
we	O
can	O
also	O
obtain	O
a	O
lower	B
bound	I
on	O
the	O
sigmoid	B
having	O
the	O
functional	B
form	O
of	O
a	O
gaussian	O
.	O
to	O
do	O
this	O
,	O
we	O
follow	O
jaakkola	O
and	O
jordan	O
(	O
2000	O
)	O
and	O
make	O
transforma-	O
tions	O
both	O
of	O
the	O
input	O
variable	O
and	O
of	O
the	O
function	O
itself	O
.	O
first	O
we	O
take	O
the	O
log	O
of	O
the	O
logistic	O
function	O
and	O
then	O
decompose	O
it	O
so	O
that	O
−x	O
)	O
=	O
−	O
ln	O
ln	O
σ	O
(	O
x	O
)	O
=	O
−	O
ln	O
(	O
1	O
+	O
e	O
−x/2	O
)	O
(	O
cid:26	O
)	O
(	O
cid:27	O
)	O
=	O
x/2	O
−	O
ln	O
(	O
ex/2	O
+	O
e	O
(	O
10.138	O
)	O
we	O
now	O
note	O
that	O
the	O
function	O
f	O
(	O
x	O
)	O
=	O
−	O
ln	O
(	O
ex/2	O
+	O
e	O
−x/2	O
)	O
is	O
a	O
convex	B
function	I
of	O
the	O
variable	O
x2	O
,	O
as	O
can	O
again	O
be	O
veriﬁed	O
by	O
ﬁnding	O
the	O
second	O
derivative	O
.	O
this	O
leads	O
to	O
a	O
lower	B
bound	I
on	O
f	O
(	O
x	O
)	O
,	O
which	O
is	O
a	O
linear	O
function	O
of	O
x2	O
whose	O
conjugate	B
function	O
is	O
given	O
by	O
(	O
cid:18	O
)	O
(	O
cid:20	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
√	O
−x/2	O
(	O
ex/2	O
+	O
e	O
e	O
−x/2	O
)	O
.	O
g	O
(	O
λ	O
)	O
=	O
max	O
x2	O
λx2	O
−	O
f	O
x2	O
the	O
stationarity	O
condition	O
leads	O
to	O
0	O
=	O
λ	O
−	O
dx	O
dx2	O
d	O
dx	O
(	O
cid:18	O
)	O
.	O
(	O
10.139	O
)	O
(	O
10.140	O
)	O
.	O
(	O
cid:17	O
)	O
x	O
2	O
(	O
cid:30	O
)	O
f	O
(	O
x	O
)	O
=	O
λ	O
+	O
tanh	O
1	O
4x	O
(	O
cid:29	O
)	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
if	O
we	O
denote	O
this	O
value	O
of	O
x	O
,	O
corresponding	O
to	O
the	O
contact	O
point	O
of	O
the	O
tangent	O
line	O
for	O
this	O
particular	O
value	O
of	O
λ	O
,	O
by	O
ξ	O
,	O
then	O
we	O
have	O
λ	O
(	O
ξ	O
)	O
=	O
−	O
1	O
4ξ	O
tanh	O
ξ	O
2	O
=	O
−	O
1	O
2ξ	O
σ	O
(	O
ξ	O
)	O
−	O
1	O
2	O
.	O
(	O
10.141	O
)	O
10.5.	O
local	B
variational	O
methods	O
497	O
instead	O
of	O
thinking	O
of	O
λ	O
as	O
the	O
variational	B
parameter	O
,	O
we	O
can	O
let	O
ξ	O
play	O
this	O
role	O
as	O
this	O
leads	O
to	O
simpler	O
expressions	O
for	O
the	O
conjugate	B
function	O
,	O
which	O
is	O
then	O
given	O
by	O
g	O
(	O
λ	O
)	O
=	O
λ	O
(	O
ξ	O
)	O
ξ2	O
−	O
f	O
(	O
ξ	O
)	O
=	O
λ	O
(	O
ξ	O
)	O
ξ2	O
+	O
ln	O
(	O
eξ/2	O
+	O
e	O
−ξ/2	O
)	O
.	O
hence	O
the	O
bound	O
on	O
f	O
(	O
x	O
)	O
can	O
be	O
written	O
as	O
f	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
λx2	O
−	O
g	O
(	O
λ	O
)	O
=	O
λx2	O
−	O
λξ2	O
−	O
ln	O
(	O
eξ/2	O
+	O
e	O
−ξ/2	O
)	O
.	O
the	O
bound	O
on	O
the	O
sigmoid	B
then	O
becomes	O
σ	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
σ	O
(	O
ξ	O
)	O
exp	O
(	O
x	O
−	O
ξ	O
)	O
/2	O
−	O
λ	O
(	O
ξ	O
)	O
(	O
x2	O
−	O
ξ2	O
)	O
(	O
cid:26	O
)	O
(	O
10.142	O
)	O
(	O
10.143	O
)	O
(	O
10.144	O
)	O
(	O
cid:27	O
)	O
section	O
4.5	O
section	O
4.3	O
where	O
λ	O
(	O
ξ	O
)	O
is	O
deﬁned	O
by	O
(	O
10.141	O
)	O
.	O
this	O
bound	O
is	O
illustrated	O
in	O
the	O
right-hand	O
plot	O
of	O
figure	O
10.12.	O
we	O
see	O
that	O
the	O
bound	O
has	O
the	O
form	O
of	O
the	O
exponential	O
of	O
a	O
quadratic	O
function	O
of	O
x	O
,	O
which	O
will	O
prove	O
useful	O
when	O
we	O
seek	O
gaussian	O
representations	O
of	O
posterior	O
distributions	O
deﬁned	O
through	O
logistic	B
sigmoid	I
functions	O
.	O
the	O
logistic	B
sigmoid	I
arises	O
frequently	O
in	O
probabilistic	O
models	O
over	O
binary	O
vari-	O
ables	O
because	O
it	O
is	O
the	O
function	O
that	O
transforms	O
a	O
log	B
odds	I
ratio	O
into	O
a	O
posterior	O
prob-	O
ability	O
.	O
the	O
corresponding	O
transformation	O
for	O
a	O
multiclass	B
distribution	O
is	O
given	O
by	O
the	O
softmax	B
function	I
.	O
unfortunately	O
,	O
the	O
lower	B
bound	I
derived	O
here	O
for	O
the	O
logistic	B
sigmoid	I
does	O
not	O
directly	O
extend	O
to	O
the	O
softmax	O
.	O
gibbs	O
(	O
1997	O
)	O
proposes	O
a	O
method	O
for	O
constructing	O
a	O
gaussian	O
distribution	O
that	O
is	O
conjectured	O
to	O
be	O
a	O
bound	O
(	O
although	O
no	O
rigorous	O
proof	O
is	O
given	O
)	O
,	O
which	O
may	O
be	O
used	O
to	O
apply	O
local	B
variational	O
methods	O
to	O
multiclass	B
problems	O
.	O
we	O
shall	O
see	O
an	O
example	O
of	O
the	O
use	O
of	O
local	B
variational	O
bounds	O
in	O
sections	O
10.6.1.	O
for	O
the	O
moment	O
,	O
however	O
,	O
it	O
is	O
instructive	O
to	O
consider	O
in	O
general	O
terms	O
how	O
these	O
bounds	O
can	O
be	O
used	O
.	O
suppose	O
we	O
wish	O
to	O
evaluate	O
an	O
integral	O
of	O
the	O
form	O
(	O
cid:6	O
)	O
i	O
=	O
σ	O
(	O
a	O
)	O
p	O
(	O
a	O
)	O
da	O
(	O
10.145	O
)	O
where	O
σ	O
(	O
a	O
)	O
is	O
the	O
logistic	B
sigmoid	I
,	O
and	O
p	O
(	O
a	O
)	O
is	O
a	O
gaussian	O
probability	B
density	O
.	O
such	O
integrals	O
arise	O
in	O
bayesian	O
models	O
when	O
,	O
for	O
instance	O
,	O
we	O
wish	O
to	O
evaluate	O
the	O
pre-	O
dictive	O
distribution	O
,	O
in	O
which	O
case	O
p	O
(	O
a	O
)	O
represents	O
a	O
posterior	O
parameter	O
distribution	O
.	O
because	O
the	O
integral	O
is	O
intractable	O
,	O
we	O
employ	O
the	O
variational	B
bound	O
(	O
10.144	O
)	O
,	O
which	O
we	O
write	O
in	O
the	O
form	O
σ	O
(	O
a	O
)	O
(	O
cid:2	O
)	O
f	O
(	O
a	O
,	O
ξ	O
)	O
where	O
ξ	O
is	O
a	O
variational	B
parameter	O
.	O
the	O
inte-	O
gral	O
now	O
becomes	O
the	O
product	O
of	O
two	O
exponential-quadratic	O
functions	O
and	O
so	O
can	O
be	O
integrated	O
analytically	O
to	O
give	O
a	O
bound	O
on	O
i	O
f	O
(	O
a	O
,	O
ξ	O
)	O
p	O
(	O
a	O
)	O
da	O
=	O
f	O
(	O
ξ	O
)	O
.	O
(	O
10.146	O
)	O
(	O
cid:6	O
)	O
i	O
(	O
cid:2	O
)	O
we	O
now	O
have	O
the	O
freedom	O
to	O
choose	O
the	O
variational	B
parameter	O
ξ	O
,	O
which	O
we	O
do	O
by	O
ﬁnding	O
the	O
value	O
ξ	O
(	O
cid:1	O
)	O
that	O
maximizes	O
the	O
function	O
f	O
(	O
ξ	O
)	O
.	O
the	O
resulting	O
value	O
f	O
(	O
ξ	O
(	O
cid:1	O
)	O
)	O
represents	O
the	O
tightest	O
bound	O
within	O
this	O
family	O
of	O
bounds	O
and	O
can	O
be	O
used	O
as	O
an	O
approximation	O
to	O
i.	O
this	O
optimized	O
bound	O
,	O
however	O
,	O
will	O
in	O
general	O
not	O
be	O
exact	O
.	O
498	O
10.	O
approximate	O
inference	B
although	O
the	O
bound	O
σ	O
(	O
a	O
)	O
(	O
cid:2	O
)	O
f	O
(	O
a	O
,	O
ξ	O
)	O
on	O
the	O
logistic	B
sigmoid	I
can	O
be	O
optimized	O
exactly	O
,	O
the	O
required	O
choice	O
for	O
ξ	O
depends	O
on	O
the	O
value	O
of	O
a	O
,	O
so	O
that	O
the	O
bound	O
is	O
exact	O
for	O
one	O
value	O
of	O
a	O
only	O
.	O
because	O
the	O
quantity	O
f	O
(	O
ξ	O
)	O
is	O
obtained	O
by	O
integrating	O
over	O
all	O
values	O
of	O
a	O
,	O
the	O
value	O
of	O
ξ	O
(	O
cid:1	O
)	O
represents	O
a	O
compromise	O
,	O
weighted	O
by	O
the	O
distribution	O
p	O
(	O
a	O
)	O
.	O
10.6.	O
variational	B
logistic	O
regression	B
we	O
now	O
illustrate	O
the	O
use	O
of	O
local	B
variational	O
methods	O
by	O
returning	O
to	O
the	O
bayesian	O
logistic	B
regression	I
model	O
studied	O
in	O
section	O
4.5.	O
there	O
we	O
focussed	O
on	O
the	O
use	O
of	O
the	O
laplace	O
approximation	O
,	O
while	O
here	O
we	O
consider	O
a	O
variational	B
treatment	O
based	O
on	O
the	O
approach	O
of	O
jaakkola	O
and	O
jordan	O
(	O
2000	O
)	O
.	O
like	O
the	O
laplace	O
method	O
,	O
this	O
also	O
leads	O
to	O
a	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
distribution	O
.	O
however	O
,	O
the	O
greater	O
ﬂexibility	O
of	O
the	O
variational	B
approximation	O
leads	O
to	O
improved	O
accuracy	O
compared	O
to	O
the	O
laplace	O
method	O
.	O
furthermore	O
(	O
unlike	O
the	O
laplace	O
method	O
)	O
,	O
the	O
variational	B
approach	O
is	O
optimizing	O
a	O
well	O
deﬁned	O
objective	O
function	O
given	O
by	O
a	O
rigourous	O
bound	O
on	O
the	O
model	B
evidence	I
.	O
logistic	B
regression	I
has	O
also	O
been	O
treated	O
by	O
dybowski	O
and	O
roberts	O
(	O
2005	O
)	O
from	O
a	O
bayesian	O
perspective	O
using	O
monte	O
carlo	O
sampling	O
techniques	O
.	O
10.6.1	O
variational	B
posterior	O
distribution	O
here	O
we	O
shall	O
make	O
use	O
of	O
a	O
variational	B
approximation	O
based	O
on	O
the	O
local	B
bounds	O
introduced	O
in	O
section	O
10.5.	O
this	O
allows	O
the	O
likelihood	B
function	I
for	O
logistic	O
regres-	O
sion	B
,	O
which	O
is	O
governed	O
by	O
the	O
logistic	B
sigmoid	I
,	O
to	O
be	O
approximated	O
by	O
the	O
expo-	O
nential	O
of	O
a	O
quadratic	O
form	O
.	O
it	O
is	O
therefore	O
again	O
convenient	O
to	O
choose	O
a	O
conjugate	B
gaussian	O
prior	B
of	O
the	O
form	O
(	O
4.140	O
)	O
.	O
for	O
the	O
moment	O
,	O
we	O
shall	O
treat	O
the	O
hyperparam-	O
eters	O
m0	O
and	O
s0	O
as	O
ﬁxed	O
constants	O
.	O
in	O
section	O
10.6.3	O
,	O
we	O
shall	O
demonstrate	O
how	O
the	O
variational	B
formalism	O
can	O
be	O
extended	B
to	O
the	O
case	O
where	O
there	O
are	O
unknown	O
hyper-	O
parameters	O
whose	O
values	O
are	O
to	O
be	O
inferred	O
from	O
the	O
data	O
.	O
in	O
the	O
variational	B
framework	O
,	O
we	O
seek	O
to	O
maximize	O
a	O
lower	B
bound	I
on	O
the	O
marginal	B
likelihood	I
.	O
for	O
the	O
bayesian	O
logistic	B
regression	I
model	O
,	O
the	O
marginal	B
likelihood	I
takes	O
the	O
form	O
(	O
cid:6	O
)	O
(	O
cid:31	O
)	O
n	O
(	O
cid:14	O
)	O
p	O
(	O
tn|w	O
)	O
p	O
(	O
w	O
)	O
dw	O
.	O
(	O
10.147	O
)	O
we	O
ﬁrst	O
note	O
that	O
the	O
conditional	B
distribution	O
for	O
t	O
can	O
be	O
written	O
as	O
(	O
cid:6	O
)	O
p	O
(	O
t	O
)	O
=	O
1	O
(	O
cid:16	O
)	O
1−t	O
p	O
(	O
t|w	O
)	O
p	O
(	O
w	O
)	O
dw	O
=	O
(	O
cid:15	O
)	O
p	O
(	O
t|w	O
)	O
=	O
σ	O
(	O
a	O
)	O
t	O
{	O
1	O
−	O
σ	O
(	O
a	O
)	O
}	O
1−t	O
1	O
−	O
=	O
1	O
n=1	O
(	O
cid:16	O
)	O
t	O
(	O
cid:15	O
)	O
1	O
+	O
e−a	O
1	O
+	O
e−a	O
−a	O
1	O
+	O
e−a	O
=	O
eatσ	O
(	O
−a	O
)	O
e	O
=	O
eat	O
where	O
a	O
=	O
wtφ	O
.	O
in	O
order	O
to	O
obtain	O
a	O
lower	B
bound	I
on	O
p	O
(	O
t	O
)	O
,	O
we	O
make	O
use	O
of	O
the	O
variational	B
lower	O
bound	O
on	O
the	O
logistic	B
sigmoid	I
function	O
given	O
by	O
(	O
10.144	O
)	O
,	O
which	O
(	O
10.148	O
)	O
10.6.	O
variational	B
logistic	O
regression	B
499	O
(	O
cid:27	O
)	O
we	O
reproduce	O
here	O
for	O
convenience	O
σ	O
(	O
z	O
)	O
(	O
cid:2	O
)	O
σ	O
(	O
ξ	O
)	O
exp	O
(	O
cid:26	O
)	O
where	O
λ	O
(	O
ξ	O
)	O
=	O
we	O
can	O
therefore	O
write	O
p	O
(	O
t|w	O
)	O
=	O
eatσ	O
(	O
−a	O
)	O
(	O
cid:2	O
)	O
eatσ	O
(	O
ξ	O
)	O
exp	O
(	O
cid:30	O
)	O
(	O
z	O
−	O
ξ	O
)	O
/2	O
−	O
λ	O
(	O
ξ	O
)	O
(	O
z2	O
−	O
ξ2	O
)	O
(	O
cid:29	O
)	O
(	O
cid:26	O
)	O
−	O
(	O
a	O
+	O
ξ	O
)	O
/2	O
−	O
λ	O
(	O
ξ	O
)	O
(	O
a2	O
−	O
ξ2	O
)	O
(	O
cid:27	O
)	O
σ	O
(	O
ξ	O
)	O
−	O
1	O
2	O
1	O
2ξ	O
.	O
(	O
10.149	O
)	O
(	O
10.150	O
)	O
.	O
(	O
10.151	O
)	O
note	O
that	O
because	O
this	O
bound	O
is	O
applied	O
to	O
each	O
of	O
the	O
terms	O
in	O
the	O
likelihood	B
function	I
separately	O
,	O
there	O
is	O
a	O
variational	B
parameter	O
ξn	O
corresponding	O
to	O
each	O
training	B
set	I
observation	O
(	O
φn	O
,	O
tn	O
)	O
.	O
using	O
a	O
=	O
wtφ	O
,	O
and	O
multiplying	O
by	O
the	O
prior	B
distribution	O
,	O
we	O
obtain	O
the	O
following	O
bound	O
on	O
the	O
joint	O
distribution	O
of	O
t	O
and	O
w	O
p	O
(	O
t	O
,	O
w	O
)	O
=	O
p	O
(	O
t|w	O
)	O
p	O
(	O
w	O
)	O
(	O
cid:2	O
)	O
h	O
(	O
w	O
,	O
ξ	O
)	O
p	O
(	O
w	O
)	O
where	O
ξ	O
denotes	O
the	O
set	O
{	O
ξn	O
}	O
of	O
variational	B
parameters	O
,	O
and	O
n	O
(	O
cid:14	O
)	O
−	O
λ	O
(	O
ξn	O
)	O
(	O
[	O
wtφn	O
]	O
2	O
−	O
ξ2	O
n	O
)	O
wtφntn	O
−	O
(	O
wtφn	O
+	O
ξn	O
)	O
/2	O
h	O
(	O
w	O
,	O
ξ	O
)	O
=	O
σ	O
(	O
ξn	O
)	O
exp	O
(	O
10.152	O
)	O
(	O
10.153	O
)	O
(	O
cid:26	O
)	O
(	O
cid:27	O
)	O
n=1	O
.	O
evaluation	O
of	O
the	O
exact	O
posterior	O
distribution	O
would	O
require	O
normalization	O
of	O
the	O
left-	O
hand	O
side	O
of	O
this	O
inequality	O
.	O
because	O
this	O
is	O
intractable	O
,	O
we	O
work	O
instead	O
with	O
the	O
right-hand	O
side	O
.	O
note	O
that	O
the	O
function	O
on	O
the	O
right-hand	O
side	O
can	O
not	O
be	O
interpreted	O
as	O
a	O
probability	B
density	O
because	O
it	O
is	O
not	O
normalized	O
.	O
once	O
it	O
is	O
normalized	O
to	O
give	O
a	O
variational	B
posterior	O
distribution	O
q	O
(	O
w	O
)	O
,	O
however	O
,	O
it	O
no	O
longer	O
represents	O
a	O
bound	O
.	O
because	O
the	O
logarithm	O
function	O
is	O
monotonically	O
increasing	O
,	O
the	O
inequality	O
a	O
(	O
cid:2	O
)	O
b	O
implies	O
ln	O
a	O
(	O
cid:2	O
)	O
ln	O
b.	O
this	O
gives	O
a	O
lower	B
bound	I
on	O
the	O
log	O
of	O
the	O
joint	O
distribution	O
of	O
t	O
and	O
w	O
of	O
the	O
form	O
ln	O
{	O
p	O
(	O
t|w	O
)	O
p	O
(	O
w	O
)	O
}	O
(	O
cid:2	O
)	O
ln	O
p	O
(	O
w	O
)	O
+	O
ln	O
σ	O
(	O
ξn	O
)	O
+	O
wtφntn	O
−	O
(	O
wtφn	O
+	O
ξn	O
)	O
/2	O
−	O
λ	O
(	O
ξn	O
)	O
(	O
[	O
wtφn	O
]	O
2	O
−	O
ξ2	O
n	O
)	O
.	O
(	O
10.154	O
)	O
(	O
cid:27	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:26	O
)	O
n=1	O
substituting	O
for	O
the	O
prior	B
p	O
(	O
w	O
)	O
,	O
the	O
right-hand	O
side	O
of	O
this	O
inequality	O
becomes	O
,	O
as	O
a	O
function	O
of	O
w	O
n	O
(	O
cid:2	O
)	O
n=1	O
+	O
(	O
cid:26	O
)	O
(	O
w	O
−	O
m0	O
)	O
ts	O
0	O
(	O
w	O
−	O
m0	O
)	O
−1	O
−1	O
2	O
wtφn	O
(	O
tn	O
−	O
1/2	O
)	O
−	O
λ	O
(	O
ξn	O
)	O
wt	O
(	O
φnφt	O
n	O
)	O
w	O
(	O
cid:27	O
)	O
+	O
const	O
.	O
(	O
10.155	O
)	O
500	O
10.	O
approximate	O
inference	B
where	O
this	O
is	O
a	O
quadratic	O
function	O
of	O
w	O
,	O
and	O
so	O
we	O
can	O
obtain	O
the	O
corresponding	O
variational	B
approximation	O
to	O
the	O
posterior	O
distribution	O
by	O
identifying	O
the	O
linear	O
and	O
quadratic	O
terms	O
in	O
w	O
,	O
giving	O
a	O
gaussian	O
variational	B
posterior	O
of	O
the	O
form	O
q	O
(	O
w	O
)	O
=	O
n	O
(	O
w|mn	O
,	O
sn	O
)	O
(	O
cid:22	O
)	O
n	O
(	O
cid:2	O
)	O
mn	O
=	O
sn	O
s	O
−1	O
n	O
=	O
s	O
−1	O
0	O
+	O
2	O
s	O
(	O
tn	O
−	O
1/2	O
)	O
φn	O
−1	O
0	O
m0	O
+	O
n	O
(	O
cid:2	O
)	O
n=1	O
λ	O
(	O
ξn	O
)	O
φnφt	O
n.	O
(	O
cid:23	O
)	O
(	O
10.156	O
)	O
(	O
10.157	O
)	O
(	O
10.158	O
)	O
n=1	O
as	O
with	O
the	O
laplace	O
framework	O
,	O
we	O
have	O
again	O
obtained	O
a	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
distribution	O
.	O
however	O
,	O
the	O
additional	O
ﬂexibility	O
provided	O
by	O
the	O
vari-	O
ational	O
parameters	O
{	O
ξn	O
}	O
leads	O
to	O
improved	O
accuracy	O
in	O
the	O
approximation	O
(	O
jaakkola	O
and	O
jordan	O
,	O
2000	O
)	O
.	O
here	O
we	O
have	O
considered	O
a	O
batch	O
learning	O
context	O
in	O
which	O
all	O
of	O
the	O
training	B
data	O
is	O
available	O
at	O
once	O
.	O
however	O
,	O
bayesian	O
methods	O
are	O
intrinsically	O
well	O
suited	O
to	O
sequential	B
learning	I
in	O
which	O
the	O
data	O
points	O
are	O
processed	O
one	O
at	O
a	O
time	O
and	O
then	O
discarded	O
.	O
the	O
formulation	O
of	O
this	O
variational	B
approach	O
for	O
the	O
sequential	O
case	O
is	O
straightforward	O
.	O
note	O
that	O
the	O
bound	O
given	O
by	O
(	O
10.149	O
)	O
applies	O
only	O
to	O
the	O
two-class	O
problem	O
and	O
so	O
this	O
approach	O
does	O
not	O
directly	O
generalize	O
to	O
classiﬁcation	B
problems	O
with	O
k	O
>	O
2	O
classes	O
.	O
an	O
alternative	O
bound	O
for	O
the	O
multiclass	B
case	O
has	O
been	O
explored	O
by	O
gibbs	O
(	O
1997	O
)	O
.	O
exercise	O
10.32	O
10.6.2	O
optimizing	O
the	O
variational	B
parameters	O
we	O
now	O
have	O
a	O
normalized	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
distribution	O
,	O
which	O
we	O
shall	O
use	O
shortly	O
to	O
evaluate	O
the	O
predictive	B
distribution	I
for	O
new	O
data	O
points	O
.	O
first	O
,	O
however	O
,	O
we	O
need	O
to	O
determine	O
the	O
variational	B
parameters	O
{	O
ξn	O
}	O
by	O
maximizing	O
the	O
lower	B
bound	I
on	O
the	O
marginal	B
likelihood	I
.	O
to	O
do	O
this	O
,	O
we	O
substitute	O
the	O
inequality	O
(	O
10.152	O
)	O
back	O
into	O
the	O
marginal	B
likeli-	O
(	O
cid:6	O
)	O
hood	O
to	O
give	O
ln	O
p	O
(	O
t	O
)	O
=	O
ln	O
(	O
cid:6	O
)	O
p	O
(	O
t|w	O
)	O
p	O
(	O
w	O
)	O
dw	O
(	O
cid:2	O
)	O
ln	O
h	O
(	O
w	O
,	O
ξ	O
)	O
p	O
(	O
w	O
)	O
dw	O
=	O
l	O
(	O
ξ	O
)	O
.	O
(	O
10.159	O
)	O
as	O
with	O
the	O
optimization	O
of	O
the	O
hyperparameter	B
α	O
in	O
the	O
linear	B
regression	I
model	O
of	O
section	O
3.5	O
,	O
there	O
are	O
two	O
approaches	O
to	O
determining	O
the	O
ξn	O
.	O
in	O
the	O
ﬁrst	O
approach	O
,	O
we	O
recognize	O
that	O
the	O
function	O
l	O
(	O
ξ	O
)	O
is	O
deﬁned	O
by	O
an	O
integration	O
over	O
w	O
and	O
so	O
we	O
can	O
view	O
w	O
as	O
a	O
latent	B
variable	I
and	O
invoke	O
the	O
em	O
algorithm	O
.	O
in	O
the	O
second	O
approach	O
,	O
we	O
integrate	O
over	O
w	O
analytically	O
and	O
then	O
perform	O
a	O
direct	O
maximization	O
over	O
ξ.	O
let	O
us	O
begin	O
by	O
considering	O
the	O
em	O
approach	O
.	O
{	O
ξn	O
}	O
,	O
which	O
we	O
denote	O
collectively	O
by	O
ξold	O
.	O
the	O
em	O
algorithm	O
starts	O
by	O
choosing	O
some	O
initial	O
values	O
for	O
the	O
parameters	O
in	O
the	O
e	O
step	O
of	O
the	O
em	O
algorithm	O
,	O
n	O
(	O
cid:2	O
)	O
(	O
cid:26	O
)	O
(	O
cid:27	O
)	O
10.6.	O
variational	B
logistic	O
regression	B
501	O
we	O
then	O
use	O
these	O
parameter	O
values	O
to	O
ﬁnd	O
the	O
posterior	O
distribution	O
over	O
w	O
,	O
which	O
is	O
given	O
by	O
(	O
10.156	O
)	O
.	O
in	O
the	O
m	O
step	O
,	O
we	O
then	O
maximize	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
which	O
is	O
given	O
by	O
q	O
(	O
ξ	O
,	O
ξold	O
)	O
=	O
e	O
[	O
ln	O
h	O
(	O
w	O
,	O
ξ	O
)	O
p	O
(	O
w	O
)	O
]	O
(	O
10.160	O
)	O
where	O
the	O
expectation	B
is	O
taken	O
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
q	O
(	O
w	O
)	O
evalu-	O
ated	O
using	O
ξold	O
.	O
noting	O
that	O
p	O
(	O
w	O
)	O
does	O
not	O
depend	O
on	O
ξ	O
,	O
and	O
substituting	O
for	O
h	O
(	O
w	O
,	O
ξ	O
)	O
we	O
obtain	O
q	O
(	O
ξ	O
,	O
ξold	O
)	O
=	O
ln	O
σ	O
(	O
ξn	O
)	O
−	O
ξn/2	O
−	O
λ	O
(	O
ξn	O
)	O
(	O
φt	O
n	O
e	O
[	O
wwt	O
]	O
φn	O
−	O
ξ2	O
n	O
)	O
+	O
const	O
n=1	O
(	O
10.161	O
)	O
where	O
‘	O
const	O
’	O
denotes	O
terms	O
that	O
are	O
independent	B
of	O
ξ.	O
we	O
now	O
set	O
the	O
derivative	B
with	O
respect	O
to	O
ξn	O
equal	O
to	O
zero	O
.	O
a	O
few	O
lines	O
of	O
algebra	O
,	O
making	O
use	O
of	O
the	O
deﬁnitions	O
of	O
σ	O
(	O
ξ	O
)	O
and	O
λ	O
(	O
ξ	O
)	O
,	O
then	O
gives	O
(	O
cid:4	O
)	O
(	O
ξn	O
)	O
(	O
φt	O
n	O
e	O
[	O
wwt	O
]	O
φn	O
−	O
ξ2	O
n	O
)	O
.	O
0	O
=	O
λ	O
(	O
10.162	O
)	O
(	O
cid:4	O
)	O
(	O
ξ	O
)	O
is	O
a	O
monotonic	O
function	O
of	O
ξ	O
for	O
ξ	O
(	O
cid:2	O
)	O
0	O
,	O
and	O
that	O
we	O
can	O
we	O
now	O
note	O
that	O
λ	O
restrict	O
attention	O
to	O
nonnegative	O
values	O
of	O
ξ	O
without	O
loss	O
of	O
generality	O
due	O
to	O
the	O
(	O
cid:4	O
)	O
(	O
ξ	O
)	O
(	O
cid:9	O
)	O
=	O
0	O
,	O
and	O
hence	O
we	O
obtain	O
the	O
symmetry	O
of	O
the	O
bound	O
around	O
ξ	O
=	O
0.	O
thus	O
λ	O
following	O
re-estimation	O
equations	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
(	O
ξnew	O
n	O
)	O
2	O
=	O
φt	O
n	O
e	O
[	O
wwt	O
]	O
φn	O
=	O
φt	O
n	O
sn	O
+	O
mn	O
mt	O
n	O
φn	O
(	O
10.163	O
)	O
where	O
we	O
have	O
used	O
(	O
10.156	O
)	O
.	O
let	O
us	O
summarize	O
the	O
em	O
algorithm	O
for	O
ﬁnding	O
the	O
variational	B
posterior	O
distri-	O
bution	O
.	O
we	O
ﬁrst	O
initialize	O
the	O
variational	B
parameters	O
ξold	O
.	O
in	O
the	O
e	O
step	O
,	O
we	O
evaluate	O
the	O
posterior	O
distribution	O
over	O
w	O
given	O
by	O
(	O
10.156	O
)	O
,	O
in	O
which	O
the	O
mean	B
and	O
covari-	O
ance	O
are	O
deﬁned	O
by	O
(	O
10.157	O
)	O
and	O
(	O
10.158	O
)	O
.	O
in	O
the	O
m	O
step	O
,	O
we	O
then	O
use	O
this	O
variational	B
posterior	O
to	O
compute	O
a	O
new	O
value	O
for	O
ξ	O
given	O
by	O
(	O
10.163	O
)	O
.	O
the	O
e	O
and	O
m	O
steps	O
are	O
repeated	O
until	O
a	O
suitable	O
convergence	O
criterion	O
is	O
satisﬁed	O
,	O
which	O
in	O
practice	O
typically	O
requires	O
only	O
a	O
few	O
iterations	O
.	O
an	O
alternative	O
approach	O
to	O
obtaining	O
re-estimation	O
equations	O
for	O
ξ	O
is	O
to	O
note	O
that	O
in	O
the	O
integral	O
over	O
w	O
in	O
the	O
deﬁnition	O
(	O
10.159	O
)	O
of	O
the	O
lower	B
bound	I
l	O
(	O
ξ	O
)	O
,	O
the	O
integrand	O
has	O
a	O
gaussian-like	O
form	O
and	O
so	O
the	O
integral	O
can	O
be	O
evaluated	O
analytically	O
.	O
having	O
evaluated	O
the	O
integral	O
,	O
we	O
can	O
then	O
differentiate	O
with	O
respect	O
to	O
ξn	O
.	O
it	O
turns	O
out	O
that	O
this	O
gives	O
rise	O
to	O
exactly	O
the	O
same	O
re-estimation	O
equations	O
as	O
does	O
the	O
em	O
approach	O
given	O
by	O
(	O
10.163	O
)	O
.	O
as	O
we	O
have	O
emphasized	O
already	O
,	O
in	O
the	O
application	O
of	O
variational	B
methods	O
it	O
is	O
useful	O
to	O
be	O
able	O
to	O
evaluate	O
the	O
lower	B
bound	I
l	O
(	O
ξ	O
)	O
given	O
by	O
(	O
10.159	O
)	O
.	O
the	O
integration	O
over	O
w	O
can	O
be	O
performed	O
analytically	O
by	O
noting	O
that	O
p	O
(	O
w	O
)	O
is	O
gaussian	O
and	O
h	O
(	O
w	O
,	O
ξ	O
)	O
is	O
the	O
exponential	O
of	O
a	O
quadratic	O
function	O
of	O
w.	O
thus	O
,	O
by	O
completing	B
the	I
square	I
and	O
making	O
use	O
of	O
the	O
standard	O
result	O
for	O
the	O
normalization	O
coefﬁcient	O
of	O
a	O
gaussian	O
distribution	O
,	O
we	O
can	O
obtain	O
a	O
closed	O
form	O
solution	O
which	O
takes	O
the	O
form	O
exercise	O
10.33	O
exercise	O
10.34	O
exercise	O
10.35	O
502	O
10.	O
approximate	O
inference	B
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−4	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−4	O
−2	O
0	O
2	O
4	O
5	O
2	O
.	O
0	O
.	O
50	O
7	O
9	O
.	O
9	O
0	O
1	O
0.0	O
−2	O
0	O
2	O
4	O
figure	O
10.13	O
illustration	O
of	O
the	O
bayesian	O
approach	O
to	O
logistic	B
regression	I
for	O
a	O
simple	O
linearly	B
separable	I
data	O
set	O
.	O
the	O
plot	O
on	O
the	O
left	O
shows	O
the	O
predictive	B
distribution	I
obtained	O
using	O
variational	B
inference	I
.	O
we	O
see	O
that	O
the	O
decision	B
boundary	I
lies	O
roughly	O
mid	O
way	O
between	O
the	O
clusters	O
of	O
data	O
points	O
,	O
and	O
that	O
the	O
contours	O
of	O
the	O
predictive	B
distribution	I
splay	O
out	O
away	O
from	O
the	O
data	O
reﬂecting	O
the	O
greater	O
uncertainty	O
in	O
the	O
classiﬁcation	B
of	O
such	O
regions	O
.	O
the	O
plot	O
on	O
the	O
right	O
shows	O
the	O
decision	O
boundaries	O
corresponding	O
to	O
ﬁve	O
samples	O
of	O
the	O
parameter	O
vector	O
w	O
drawn	O
from	O
the	O
posterior	O
distribution	O
p	O
(	O
w|t	O
)	O
.	O
l	O
(	O
ξ	O
)	O
=	O
1	O
2	O
+	O
ln	O
mt	O
|sn|	O
(	O
cid:12	O
)	O
|s0|	O
−	O
1	O
n	O
(	O
cid:2	O
)	O
ln	O
σ	O
(	O
ξn	O
)	O
−	O
1	O
n	O
s	O
2	O
n=1	O
.	O
(	O
10.164	O
)	O
mt	O
0	O
s	O
−1	O
0	O
m0	O
(	O
cid:13	O
)	O
1	O
2	O
−1	O
n	O
mn	O
+	O
2	O
ξn	O
−	O
λ	O
(	O
ξn	O
)	O
ξ2	O
n	O
this	O
variational	B
framework	O
can	O
also	O
be	O
applied	O
to	O
situations	O
in	O
which	O
the	O
data	O
is	O
arriving	O
sequentially	O
(	O
jaakkola	O
and	O
jordan	O
,	O
2000	O
)	O
.	O
in	O
this	O
case	O
we	O
maintain	O
a	O
gaussian	O
posterior	O
distribution	O
over	O
w	O
,	O
which	O
is	O
initialized	O
using	O
the	O
prior	B
p	O
(	O
w	O
)	O
.	O
as	O
each	O
data	O
point	O
arrives	O
,	O
the	O
posterior	O
is	O
updated	O
by	O
making	O
use	O
of	O
the	O
bound	O
(	O
10.151	O
)	O
and	O
then	O
normalized	O
to	O
give	O
an	O
updated	O
posterior	O
distribution	O
.	O
the	O
predictive	B
distribution	I
is	O
obtained	O
by	O
marginalizing	O
over	O
the	O
posterior	O
dis-	O
tribution	O
,	O
and	O
takes	O
the	O
same	O
form	O
as	O
for	O
the	O
laplace	O
approximation	O
discussed	O
in	O
section	O
4.5.2.	O
figure	O
10.13	O
shows	O
the	O
variational	B
predictive	O
distributions	O
for	O
a	O
syn-	O
thetic	O
data	O
set	O
.	O
this	O
example	O
provides	O
interesting	O
insights	O
into	O
the	O
concept	O
of	O
‘	O
large	B
margin	I
’	O
,	O
which	O
was	O
discussed	O
in	O
section	O
7.1	O
and	O
which	O
has	O
qualitatively	O
similar	O
be-	O
haviour	O
to	O
the	O
bayesian	O
solution	O
.	O
10.6.3	O
inference	B
of	O
hyperparameters	O
so	O
far	O
,	O
we	O
have	O
treated	O
the	O
hyperparameter	B
α	O
in	O
the	O
prior	B
distribution	O
as	O
a	O
known	O
constant	O
.	O
we	O
now	O
extend	O
the	O
bayesian	O
logistic	B
regression	I
model	O
to	O
allow	O
the	O
value	O
of	O
this	O
parameter	O
to	O
be	O
inferred	O
from	O
the	O
data	O
set	O
.	O
this	O
can	O
be	O
achieved	O
by	O
combining	O
the	O
global	O
and	O
local	B
variational	O
approximations	O
into	O
a	O
single	O
framework	O
,	O
so	O
as	O
to	O
maintain	O
a	O
lower	B
bound	I
on	O
the	O
marginal	B
likelihood	I
at	O
each	O
stage	O
.	O
such	O
a	O
combined	O
approach	O
was	O
adopted	O
by	O
bishop	O
and	O
svens´en	O
(	O
2003	O
)	O
in	O
the	O
context	O
of	O
a	O
bayesian	O
treatment	O
of	O
the	O
hierarchical	B
mixture	I
of	I
experts	I
model	O
.	O
10.6.	O
variational	B
logistic	O
regression	B
503	O
speciﬁcally	O
,	O
we	O
consider	O
once	O
again	O
a	O
simple	O
isotropic	B
gaussian	O
prior	B
distribu-	O
tion	O
of	O
the	O
form	O
p	O
(	O
w|α	O
)	O
=	O
n	O
(	O
w|0	O
,	O
α	O
−1i	O
)	O
.	O
(	O
10.165	O
)	O
our	O
analysis	O
is	O
readily	O
extended	B
to	O
more	O
general	O
gaussian	O
priors	O
,	O
for	O
instance	O
if	O
we	O
wish	O
to	O
associate	O
a	O
different	O
hyperparameter	B
with	O
different	O
subsets	O
of	O
the	O
parame-	O
ters	O
wj	O
.	O
as	O
usual	O
,	O
we	O
consider	O
a	O
conjugate	B
hyperprior	O
over	O
α	O
given	O
by	O
a	O
gamma	B
distribution	I
p	O
(	O
α	O
)	O
=	O
gam	O
(	O
α|a0	O
,	O
b0	O
)	O
(	O
10.166	O
)	O
governed	O
by	O
the	O
constants	O
a0	O
and	O
b0	O
.	O
the	O
marginal	B
likelihood	I
for	O
this	O
model	O
now	O
takes	O
the	O
form	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
p	O
(	O
t	O
)	O
=	O
p	O
(	O
w	O
,	O
α	O
,	O
t	O
)	O
dw	O
dα	O
(	O
10.167	O
)	O
where	O
the	O
joint	O
distribution	O
is	O
given	O
by	O
p	O
(	O
w	O
,	O
α	O
,	O
t	O
)	O
=	O
p	O
(	O
t|w	O
)	O
p	O
(	O
w|α	O
)	O
p	O
(	O
α	O
)	O
.	O
(	O
10.168	O
)	O
we	O
are	O
now	O
faced	O
with	O
an	O
analytically	O
intractable	O
integration	O
over	O
w	O
and	O
α	O
,	O
which	O
we	O
shall	O
tackle	O
by	O
using	O
both	O
the	O
local	B
and	O
global	O
variational	O
approaches	O
in	O
the	O
same	O
model	O
to	O
begin	O
with	O
,	O
we	O
introduce	O
a	O
variational	B
distribution	O
q	O
(	O
w	O
,	O
α	O
)	O
,	O
and	O
then	O
apply	O
the	O
decomposition	O
(	O
10.2	O
)	O
,	O
which	O
in	O
this	O
instance	O
takes	O
the	O
form	O
(	O
10.169	O
)	O
where	O
the	O
lower	B
bound	I
l	O
(	O
q	O
)	O
and	O
the	O
kullback-leibler	O
divergence	O
kl	O
(	O
q	O
(	O
cid:5	O
)	O
p	O
)	O
are	O
de-	O
ﬁned	O
by	O
ln	O
p	O
(	O
t	O
)	O
=	O
l	O
(	O
q	O
)	O
+	O
kl	O
(	O
q	O
(	O
cid:5	O
)	O
p	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
q	O
(	O
w	O
,	O
α	O
)	O
ln	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
p	O
(	O
w	O
,	O
α	O
,	O
t	O
)	O
q	O
(	O
w	O
,	O
α	O
)	O
p	O
(	O
w	O
,	O
α|t	O
)	O
)	O
q	O
(	O
w	O
,	O
α	O
)	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
dw	O
dα	O
kl	O
(	O
q	O
(	O
cid:5	O
)	O
p	O
)	O
=	O
−	O
q	O
(	O
w	O
,	O
α	O
)	O
ln	O
(	O
10.171	O
)	O
at	O
this	O
point	O
,	O
the	O
lower	B
bound	I
l	O
(	O
q	O
)	O
is	O
still	O
intractable	O
due	O
to	O
the	O
form	O
of	O
the	O
likelihood	O
factor	O
p	O
(	O
t|w	O
)	O
.	O
we	O
therefore	O
apply	O
the	O
local	B
variational	O
bound	O
to	O
each	O
of	O
the	O
logistic	B
sigmoid	I
factors	O
as	O
before	O
.	O
this	O
allows	O
us	O
to	O
use	O
the	O
inequality	O
(	O
10.152	O
)	O
and	O
place	O
a	O
lower	B
bound	I
on	O
l	O
(	O
q	O
)	O
,	O
which	O
will	O
therefore	O
also	O
be	O
a	O
lower	B
bound	I
on	O
the	O
log	O
marginal	O
likelihood	O
dw	O
dα	O
.	O
l	O
(	O
q	O
)	O
=	O
(	O
10.170	O
)	O
ln	O
p	O
(	O
t	O
)	O
(	O
cid:2	O
)	O
l	O
(	O
q	O
)	O
(	O
cid:2	O
)	O
(	O
cid:4	O
)	O
l	O
(	O
q	O
,	O
ξ	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:12	O
)	O
=	O
q	O
(	O
w	O
,	O
α	O
)	O
ln	O
(	O
cid:13	O
)	O
h	O
(	O
w	O
,	O
ξ	O
)	O
p	O
(	O
w|α	O
)	O
p	O
(	O
α	O
)	O
q	O
(	O
w	O
,	O
α	O
)	O
dw	O
dα	O
.	O
(	O
10.172	O
)	O
next	O
we	O
assume	O
that	O
the	O
variational	B
distribution	O
factorizes	O
between	O
parameters	O
and	O
hyperparameters	O
so	O
that	O
q	O
(	O
w	O
,	O
α	O
)	O
=	O
q	O
(	O
w	O
)	O
q	O
(	O
α	O
)	O
.	O
(	O
10.173	O
)	O
504	O
10.	O
approximate	O
inference	B
with	O
this	O
factorization	B
we	O
can	O
appeal	O
to	O
the	O
general	O
result	O
(	O
10.9	O
)	O
to	O
ﬁnd	O
expressions	O
for	O
the	O
optimal	O
factors	O
.	O
consider	O
ﬁrst	O
the	O
distribution	O
q	O
(	O
w	O
)	O
.	O
discarding	O
terms	O
that	O
are	O
independent	B
of	O
w	O
,	O
we	O
have	O
ln	O
q	O
(	O
w	O
)	O
=	O
eα	O
[	O
ln	O
{	O
h	O
(	O
w	O
,	O
ξ	O
)	O
p	O
(	O
w|α	O
)	O
p	O
(	O
α	O
)	O
}	O
]	O
+	O
const	O
=	O
ln	O
h	O
(	O
w	O
,	O
ξ	O
)	O
+	O
eα	O
[	O
ln	O
p	O
(	O
w|α	O
)	O
]	O
+	O
const	O
.	O
we	O
now	O
substitute	O
for	O
ln	O
h	O
(	O
w	O
,	O
ξ	O
)	O
using	O
(	O
10.153	O
)	O
,	O
and	O
for	O
ln	O
p	O
(	O
w|α	O
)	O
using	O
(	O
10.165	O
)	O
,	O
giving	O
n	O
(	O
cid:2	O
)	O
(	O
cid:26	O
)	O
n=1	O
ln	O
q	O
(	O
w	O
)	O
=	O
−	O
e	O
[	O
α	O
]	O
2	O
wtw	O
+	O
(	O
tn	O
−	O
1/2	O
)	O
wtφn	O
−	O
λ	O
(	O
ξn	O
)	O
wtφnφt	O
nw	O
+	O
const	O
.	O
we	O
see	O
that	O
this	O
is	O
a	O
quadratic	O
function	O
of	O
w	O
and	O
so	O
the	O
solution	O
for	O
q	O
(	O
w	O
)	O
will	O
be	O
gaussian	O
.	O
completing	B
the	I
square	I
in	O
the	O
usual	O
way	O
,	O
we	O
obtain	O
(	O
cid:27	O
)	O
(	O
10.174	O
)	O
(	O
10.175	O
)	O
(	O
10.176	O
)	O
where	O
we	O
have	O
deﬁned	O
−1	O
n	O
µn	O
=	O
σ	O
q	O
(	O
w	O
)	O
=	O
n	O
(	O
w|µn	O
,	O
σn	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
(	O
tn	O
−	O
1/2	O
)	O
φn	O
n	O
(	O
cid:2	O
)	O
−1	O
n	O
=	O
e	O
[	O
α	O
]	O
i	O
+	O
2	O
σ	O
λ	O
(	O
ξn	O
)	O
φnφt	O
n.	O
similarly	O
,	O
the	O
optimal	O
solution	O
for	O
the	O
factor	O
q	O
(	O
α	O
)	O
is	O
obtained	O
from	O
n=1	O
ln	O
q	O
(	O
α	O
)	O
=	O
ew	O
[	O
ln	O
p	O
(	O
w|α	O
)	O
]	O
+	O
ln	O
p	O
(	O
α	O
)	O
+	O
const	O
.	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
substituting	O
for	O
ln	O
p	O
(	O
w|α	O
)	O
using	O
(	O
10.165	O
)	O
,	O
and	O
for	O
ln	O
p	O
(	O
α	O
)	O
using	O
(	O
10.166	O
)	O
,	O
we	O
obtain	O
ln	O
q	O
(	O
α	O
)	O
=	O
m	O
2	O
ln	O
α	O
−	O
α	O
2	O
e	O
wtw	O
+	O
(	O
a0	O
−	O
1	O
)	O
ln	O
α	O
−	O
b0α	O
+	O
const	O
.	O
we	O
recognize	O
this	O
as	O
the	O
log	O
of	O
a	O
gamma	B
distribution	I
,	O
and	O
so	O
we	O
obtain	O
q	O
(	O
α	O
)	O
=	O
gam	O
(	O
α|an	O
,	O
bn	O
)	O
=	O
where	O
0	O
αa0−1e	O
−b0α	O
1	O
γ	O
(	O
a0	O
)	O
ab0	O
(	O
cid:8	O
)	O
an	O
=	O
a0	O
+	O
m	O
2	O
1	O
2	O
ew	O
bn	O
=	O
b0	O
+	O
(	O
cid:9	O
)	O
.	O
wtw	O
(	O
10.177	O
)	O
(	O
10.178	O
)	O
(	O
10.179	O
)	O
10.7.	O
expectation	B
propagation	I
505	O
maximizing	O
the	O
lower	B
bound	I
(	O
cid:4	O
)	O
l	O
(	O
q	O
,	O
ξ	O
)	O
.	O
omitting	O
terms	O
that	O
are	O
independent	B
of	O
ξ	O
,	O
and	O
we	O
also	O
need	O
to	O
optimize	O
the	O
variational	B
parameters	O
ξn	O
,	O
and	O
this	O
is	O
also	O
done	O
by	O
integrating	O
over	O
α	O
,	O
we	O
have	O
(	O
cid:6	O
)	O
(	O
cid:4	O
)	O
l	O
(	O
q	O
,	O
ξ	O
)	O
=	O
q	O
(	O
w	O
)	O
ln	O
h	O
(	O
w	O
,	O
ξ	O
)	O
dw	O
+	O
const	O
.	O
(	O
10.180	O
)	O
note	O
that	O
this	O
has	O
precisely	O
the	O
same	O
form	O
as	O
(	O
10.159	O
)	O
,	O
and	O
so	O
we	O
can	O
again	O
appeal	O
to	O
our	O
earlier	O
result	O
(	O
10.163	O
)	O
,	O
which	O
can	O
be	O
obtained	O
by	O
direct	O
optimization	O
of	O
the	O
marginal	B
likelihood	I
function	O
,	O
leading	O
to	O
re-estimation	O
equations	O
of	O
the	O
form	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
(	O
ξnew	O
n	O
)	O
2	O
=	O
φt	O
n	O
σn	O
+	O
µn	O
µt	O
n	O
φn	O
.	O
(	O
10.181	O
)	O
appendix	O
b	O
we	O
have	O
obtained	O
re-estimation	O
equations	O
for	O
the	O
three	O
quantities	O
q	O
(	O
w	O
)	O
,	O
q	O
(	O
α	O
)	O
,	O
and	O
ξ	O
,	O
and	O
so	O
after	O
making	O
suitable	O
initializations	O
,	O
we	O
can	O
cycle	O
through	O
these	O
quan-	O
tities	O
,	O
updating	O
each	O
in	O
turn	O
.	O
the	O
required	O
moments	O
are	O
given	O
by	O
(	O
cid:8	O
)	O
e	O
(	O
cid:9	O
)	O
e	O
[	O
α	O
]	O
=	O
an	O
bn	O
wtw	O
=	O
σn	O
+	O
µt	O
n	O
µn	O
.	O
(	O
10.182	O
)	O
(	O
10.183	O
)	O
10.7.	O
expectation	B
propagation	I
we	O
conclude	O
this	O
chapter	O
by	O
discussing	O
an	O
alternative	O
form	O
of	O
deterministic	O
approx-	O
imate	O
inference	B
,	O
known	O
as	O
expectation	B
propagation	I
or	O
ep	O
(	O
minka	O
,	O
2001a	O
;	O
minka	O
,	O
2001b	O
)	O
.	O
as	O
with	O
the	O
variational	B
bayes	O
methods	O
discussed	O
so	O
far	O
,	O
this	O
too	O
is	O
based	O
on	O
the	O
minimization	O
of	O
a	O
kullback-leibler	O
divergence	O
but	O
now	O
of	O
the	O
reverse	O
form	O
,	O
which	O
gives	O
the	O
approximation	O
rather	O
different	O
properties	O
.	O
consider	O
for	O
a	O
moment	O
the	O
problem	O
of	O
minimizing	O
kl	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
with	O
respect	O
to	O
q	O
(	O
z	O
)	O
when	O
p	O
(	O
z	O
)	O
is	O
a	O
ﬁxed	O
distribution	O
and	O
q	O
(	O
z	O
)	O
is	O
a	O
member	O
of	O
the	O
exponential	B
family	I
and	O
so	O
,	O
from	O
(	O
2.194	O
)	O
,	O
can	O
be	O
written	O
in	O
the	O
form	O
(	O
cid:26	O
)	O
(	O
cid:27	O
)	O
q	O
(	O
z	O
)	O
=	O
h	O
(	O
z	O
)	O
g	O
(	O
η	O
)	O
exp	O
ηtu	O
(	O
z	O
)	O
.	O
(	O
10.184	O
)	O
as	O
a	O
function	O
of	O
η	O
,	O
the	O
kullback-leibler	O
divergence	O
then	O
becomes	O
kl	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
=	O
−	O
ln	O
g	O
(	O
η	O
)	O
−	O
ηt	O
ep	O
(	O
z	O
)	O
[	O
u	O
(	O
z	O
)	O
]	O
+	O
const	O
(	O
10.185	O
)	O
where	O
the	O
constant	O
terms	O
are	O
independent	B
of	O
the	O
natural	B
parameters	I
η.	O
we	O
can	O
mini-	O
mize	O
kl	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
within	O
this	O
family	O
of	O
distributions	O
by	O
setting	O
the	O
gradient	O
with	O
respect	O
to	O
η	O
to	O
zero	O
,	O
giving	O
(	O
10.186	O
)	O
however	O
,	O
we	O
have	O
already	O
seen	O
in	O
(	O
2.226	O
)	O
that	O
the	O
negative	O
gradient	O
of	O
ln	O
g	O
(	O
η	O
)	O
is	O
given	O
by	O
the	O
expectation	B
of	O
u	O
(	O
z	O
)	O
under	O
the	O
distribution	O
q	O
(	O
z	O
)	O
.	O
equating	O
these	O
two	O
results	O
,	O
we	O
obtain	O
−∇	O
ln	O
g	O
(	O
η	O
)	O
=	O
ep	O
(	O
z	O
)	O
[	O
u	O
(	O
z	O
)	O
]	O
.	O
eq	O
(	O
z	O
)	O
[	O
u	O
(	O
z	O
)	O
]	O
=	O
ep	O
(	O
z	O
)	O
[	O
u	O
(	O
z	O
)	O
]	O
.	O
(	O
10.187	O
)	O
506	O
10.	O
approximate	O
inference	B
we	O
see	O
that	O
the	O
optimum	O
solution	O
simply	O
corresponds	O
to	O
matching	O
the	O
expected	O
suf-	O
ﬁcient	O
statistics	O
.	O
so	O
,	O
for	O
instance	O
,	O
if	O
q	O
(	O
z	O
)	O
is	O
a	O
gaussian	O
n	O
(	O
z|µ	O
,	O
σ	O
)	O
then	O
we	O
minimize	O
the	O
kullback-leibler	O
divergence	O
by	O
setting	O
the	O
mean	B
µ	O
of	O
q	O
(	O
z	O
)	O
equal	O
to	O
the	O
mean	B
of	O
the	O
distribution	O
p	O
(	O
z	O
)	O
and	O
the	O
covariance	B
σ	O
equal	O
to	O
the	O
covariance	B
of	O
p	O
(	O
z	O
)	O
.	O
this	O
is	O
sometimes	O
called	O
moment	B
matching	I
.	O
an	O
example	O
of	O
this	O
was	O
seen	O
in	O
figure	O
10.3	O
(	O
a	O
)	O
.	O
now	O
let	O
us	O
exploit	O
this	O
result	O
to	O
obtain	O
a	O
practical	O
algorithm	O
for	O
approximate	O
inference	B
.	O
for	O
many	O
probabilistic	O
models	O
,	O
the	O
joint	O
distribution	O
of	O
data	O
d	O
and	O
hidden	O
variables	O
(	O
including	O
parameters	O
)	O
θ	O
comprises	O
a	O
product	O
of	O
factors	O
in	O
the	O
form	O
p	O
(	O
d	O
,	O
θ	O
)	O
=	O
fi	O
(	O
θ	O
)	O
.	O
(	O
10.188	O
)	O
(	O
cid:14	O
)	O
i	O
this	O
would	O
arise	O
,	O
for	O
example	O
,	O
in	O
a	O
model	O
for	O
independent	B
,	O
identically	O
distributed	O
data	O
in	O
which	O
there	O
is	O
one	O
factor	O
fn	O
(	O
θ	O
)	O
=	O
p	O
(	O
xn|θ	O
)	O
for	O
each	O
data	O
point	O
xn	O
,	O
along	O
with	O
a	O
factor	O
f0	O
(	O
θ	O
)	O
=	O
p	O
(	O
θ	O
)	O
corresponding	O
to	O
the	O
prior	B
.	O
more	O
generally	O
,	O
it	O
would	O
also	O
apply	O
to	O
any	O
model	O
deﬁned	O
by	O
a	O
directed	B
probabilistic	O
graph	O
in	O
which	O
each	O
factor	O
is	O
a	O
conditional	B
distribution	O
corresponding	O
to	O
one	O
of	O
the	O
nodes	O
,	O
or	O
an	O
undirected	B
graph	I
in	O
which	O
each	O
factor	O
is	O
a	O
clique	B
potential	O
.	O
we	O
are	O
interested	O
in	O
evaluating	O
the	O
posterior	O
distribution	O
p	O
(	O
θ|d	O
)	O
for	O
the	O
purpose	O
of	O
making	O
predictions	O
,	O
as	O
well	O
as	O
the	O
model	B
evidence	I
p	O
(	O
d	O
)	O
for	O
the	O
purpose	O
of	O
model	B
comparison	I
.	O
from	O
(	O
10.188	O
)	O
the	O
posterior	O
is	O
given	O
by	O
p	O
(	O
θ|d	O
)	O
=	O
fi	O
(	O
θ	O
)	O
(	O
10.189	O
)	O
(	O
cid:14	O
)	O
i	O
1	O
p	O
(	O
d	O
)	O
(	O
cid:6	O
)	O
(	O
cid:14	O
)	O
i	O
and	O
the	O
model	B
evidence	I
is	O
given	O
by	O
p	O
(	O
d	O
)	O
=	O
fi	O
(	O
θ	O
)	O
dθ	O
.	O
(	O
10.190	O
)	O
here	O
we	O
are	O
considering	O
continuous	O
variables	O
,	O
but	O
the	O
following	O
discussion	O
applies	O
equally	O
to	O
discrete	O
variables	O
with	O
integrals	O
replaced	O
by	O
summations	O
.	O
we	O
shall	O
sup-	O
pose	O
that	O
the	O
marginalization	O
over	O
θ	O
,	O
along	O
with	O
the	O
marginalizations	O
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
required	O
to	O
make	O
predictions	O
,	O
are	O
intractable	O
so	O
that	O
some	O
form	O
of	O
approximation	O
is	O
required	O
.	O
expectation	B
propagation	I
is	O
based	O
on	O
an	O
approximation	O
to	O
the	O
posterior	O
distribu-	O
tion	O
which	O
is	O
also	O
given	O
by	O
a	O
product	O
of	O
factors	O
q	O
(	O
θ	O
)	O
=	O
in	O
which	O
each	O
factor	O
(	O
cid:4	O
)	O
fi	O
(	O
θ	O
)	O
in	O
the	O
approximation	O
corresponds	O
to	O
one	O
of	O
the	O
factors	O
obtain	O
a	O
practical	O
algorithm	O
,	O
we	O
need	O
to	O
constrain	O
the	O
factors	O
(	O
cid:4	O
)	O
fi	O
(	O
θ	O
)	O
in	O
some	O
way	O
,	O
fi	O
(	O
θ	O
)	O
in	O
the	O
true	O
posterior	O
(	O
10.189	O
)	O
,	O
and	O
the	O
factor	O
1/z	O
is	O
the	O
normalizing	O
constant	O
needed	O
to	O
ensure	O
that	O
the	O
left-hand	O
side	O
of	O
(	O
10.191	O
)	O
integrates	O
to	O
unity	O
.	O
in	O
order	O
to	O
i	O
(	O
10.191	O
)	O
and	O
in	O
particular	O
we	O
shall	O
assume	O
that	O
they	O
come	O
from	O
the	O
exponential	B
family	I
.	O
the	O
product	O
of	O
the	O
factors	O
will	O
therefore	O
also	O
be	O
from	O
the	O
exponential	B
family	I
and	O
so	O
can	O
(	O
cid:14	O
)	O
(	O
cid:4	O
)	O
fi	O
(	O
θ	O
)	O
1	O
z	O
10.7.	O
expectation	B
propagation	I
be	O
described	O
by	O
a	O
ﬁnite	O
set	O
of	O
sufﬁcient	B
statistics	I
.	O
for	O
example	O
,	O
if	O
each	O
of	O
the	O
(	O
cid:4	O
)	O
fi	O
(	O
θ	O
)	O
ideally	O
we	O
would	O
like	O
to	O
determine	O
the	O
(	O
cid:4	O
)	O
fi	O
(	O
θ	O
)	O
by	O
minimizing	O
the	O
kullback-leibler	O
is	O
a	O
gaussian	O
,	O
then	O
the	O
overall	O
approximation	O
q	O
(	O
θ	O
)	O
will	O
also	O
be	O
gaussian	O
.	O
507	O
divergence	O
between	O
the	O
true	O
posterior	O
and	O
the	O
approximation	O
given	O
by	O
(	O
cid:22	O
)	O
(	O
cid:14	O
)	O
i	O
’	O
’	O
’	O
’	O
’	O
1	O
z	O
(	O
cid:14	O
)	O
i	O
(	O
cid:23	O
)	O
(	O
cid:4	O
)	O
fi	O
(	O
θ	O
)	O
kl	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
=	O
kl	O
1	O
p	O
(	O
d	O
)	O
fi	O
(	O
θ	O
)	O
.	O
(	O
10.192	O
)	O
note	O
that	O
this	O
is	O
the	O
reverse	O
form	O
of	O
kl	O
divergence	O
compared	O
with	O
that	O
used	O
in	O
varia-	O
tional	O
inference	B
.	O
in	O
general	O
,	O
this	O
minimization	O
will	O
be	O
intractable	O
because	O
the	O
kl	O
di-	O
vergence	O
involves	O
averaging	O
with	O
respect	O
to	O
the	O
true	O
distribution	O
.	O
as	O
a	O
rough	O
approx-	O
imation	O
,	O
we	O
could	O
instead	O
minimize	O
the	O
kl	O
divergences	O
between	O
the	O
corresponding	O
pairs	O
fi	O
(	O
θ	O
)	O
and	O
(	O
cid:4	O
)	O
fi	O
(	O
θ	O
)	O
of	O
factors	O
.	O
this	O
represents	O
a	O
much	O
simpler	O
problem	O
to	O
solve	O
,	O
and	O
has	O
the	O
advantage	O
that	O
the	O
algorithm	O
is	O
noniterative	O
.	O
however	O
,	O
because	O
each	O
fac-	O
tor	O
is	O
individually	O
approximated	O
,	O
the	O
product	O
of	O
the	O
factors	O
could	O
well	O
give	O
a	O
poor	O
approximation	O
.	O
this	O
is	O
similar	O
in	O
spirit	O
to	O
the	O
update	O
of	O
factors	O
in	O
the	O
variational	B
bayes	O
framework	O
expectation	B
propagation	I
makes	O
a	O
much	O
better	O
approximation	O
by	O
optimizing	O
each	O
factor	O
in	O
turn	O
in	O
the	O
context	O
of	O
all	O
of	O
the	O
remaining	O
factors	O
.	O
it	O
starts	O
by	O
initializing	O
the	O
factors	O
(	O
cid:4	O
)	O
fi	O
(	O
θ	O
)	O
,	O
and	O
then	O
cycles	O
through	O
the	O
factors	O
reﬁning	O
them	O
one	O
at	O
a	O
time	O
.	O
considered	O
earlier	O
.	O
suppose	O
we	O
wish	O
to	O
reﬁne	O
factor	O
(	O
cid:4	O
)	O
fj	O
(	O
θ	O
)	O
.	O
we	O
ﬁrst	O
remove	O
this	O
(	O
cid:4	O
)	O
fi	O
(	O
θ	O
)	O
.	O
conceptually	O
,	O
we	O
will	O
now	O
determine	O
a	O
revised	O
form	O
of	O
the	O
factor	O
(	O
cid:4	O
)	O
fj	O
(	O
θ	O
)	O
by	O
ensuring	O
that	O
the	O
product	O
(	O
cid:4	O
)	O
fi	O
(	O
θ	O
)	O
factor	O
from	O
the	O
product	O
to	O
give	O
(	O
10.193	O
)	O
i	O
(	O
cid:9	O
)	O
=j	O
(	O
cid:21	O
)	O
(	O
cid:14	O
)	O
qnew	O
(	O
θ	O
)	O
∝	O
(	O
cid:4	O
)	O
fj	O
(	O
θ	O
)	O
(	O
cid:14	O
)	O
(	O
cid:4	O
)	O
fi	O
(	O
θ	O
)	O
fj	O
(	O
θ	O
)	O
i	O
(	O
cid:9	O
)	O
=j	O
is	O
as	O
close	O
as	O
possible	O
to	O
in	O
which	O
we	O
keep	O
ﬁxed	O
all	O
of	O
the	O
factors	O
(	O
cid:4	O
)	O
fi	O
(	O
θ	O
)	O
for	O
i	O
(	O
cid:9	O
)	O
=	O
j.	O
this	O
ensures	O
that	O
the	O
to	O
the	O
‘	O
clutter	B
problem	I
’	O
.	O
to	O
achieve	O
this	O
,	O
we	O
ﬁrst	O
remove	O
the	O
factor	O
(	O
cid:4	O
)	O
fj	O
(	O
θ	O
)	O
from	O
the	O
approximation	O
is	O
most	O
accurate	O
in	O
the	O
regions	O
of	O
high	O
posterior	B
probability	I
as	O
deﬁned	O
by	O
the	O
remaining	O
factors	O
.	O
we	O
shall	O
see	O
an	O
example	O
of	O
this	O
effect	O
when	O
we	O
apply	O
ep	O
(	O
10.194	O
)	O
i	O
(	O
cid:9	O
)	O
=j	O
current	O
approximation	O
to	O
the	O
posterior	O
by	O
deﬁning	O
the	O
unnormalized	O
distribution	O
.	O
(	O
10.195	O
)	O
\j	O
(	O
θ	O
)	O
from	O
the	O
product	O
of	O
factors	O
i	O
(	O
cid:9	O
)	O
=	O
j	O
,	O
although	O
note	O
that	O
we	O
could	O
instead	O
ﬁnd	O
q	O
in	O
practice	O
division	O
is	O
usually	O
easier	O
.	O
this	O
is	O
now	O
combined	O
with	O
the	O
factor	O
fj	O
(	O
θ	O
)	O
to	O
give	O
a	O
distribution	O
(	O
10.196	O
)	O
\j	O
(	O
θ	O
)	O
=	O
q	O
(	O
θ	O
)	O
(	O
cid:4	O
)	O
fj	O
(	O
θ	O
)	O
q	O
fj	O
(	O
θ	O
)	O
q	O
\j	O
(	O
θ	O
)	O
1	O
zj	O
section	O
10.7.1	O
508	O
10.	O
approximate	O
inference	B
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−2	O
−1	O
0	O
1	O
2	O
3	O
4	O
40	O
30	O
20	O
10	O
0	O
−2	O
−1	O
0	O
1	O
2	O
3	O
4	O
figure	O
10.14	O
illustration	O
of	O
the	O
expectation	B
propagation	I
approximation	O
using	O
a	O
gaussian	O
distribution	O
for	O
the	O
example	O
considered	O
earlier	O
in	O
figures	O
4.14	O
and	O
10.1.	O
the	O
left-hand	O
plot	O
shows	O
the	O
original	O
distribution	O
(	O
yellow	O
)	O
along	O
with	O
the	O
laplace	O
(	O
red	O
)	O
,	O
global	O
variational	O
(	O
green	O
)	O
,	O
and	O
ep	O
(	O
blue	O
)	O
approximations	O
,	O
and	O
the	O
right-hand	O
plot	O
shows	O
the	O
corresponding	O
negative	O
logarithms	O
of	O
the	O
distributions	O
.	O
note	O
that	O
the	O
ep	O
distribution	O
is	O
broader	O
than	O
that	O
variational	B
inference	I
,	O
as	O
a	O
consequence	O
of	O
the	O
different	O
form	O
of	O
kl	O
divergence	O
.	O
where	O
zj	O
is	O
the	O
normalization	O
constant	O
given	O
by	O
we	O
now	O
determine	O
a	O
revised	O
factor	O
(	O
cid:4	O
)	O
fj	O
(	O
θ	O
)	O
by	O
minimizing	O
the	O
kullback-leibler	O
diver-	O
(	O
10.197	O
)	O
zj	O
=	O
fj	O
(	O
θ	O
)	O
q	O
\j	O
(	O
θ	O
)	O
dθ	O
.	O
gence	O
kl	O
\j	O
(	O
θ	O
)	O
fj	O
(	O
θ	O
)	O
q	O
zj	O
.	O
(	O
10.198	O
)	O
(	O
cid:6	O
)	O
(	O
cid:15	O
)	O
’	O
’	O
’	O
’	O
qnew	O
(	O
θ	O
)	O
(	O
cid:16	O
)	O
this	O
is	O
easily	O
solved	O
because	O
the	O
approximating	O
distribution	O
qnew	O
(	O
θ	O
)	O
is	O
from	O
the	O
ex-	O
ponential	O
family	O
,	O
and	O
so	O
we	O
can	O
appeal	O
to	O
the	O
result	O
(	O
10.187	O
)	O
,	O
which	O
tells	O
us	O
that	O
the	O
parameters	O
of	O
qnew	O
(	O
θ	O
)	O
are	O
obtained	O
by	O
matching	O
its	O
expected	O
sufﬁcient	B
statistics	I
to	O
the	O
corresponding	O
moments	O
of	O
(	O
10.196	O
)	O
.	O
we	O
shall	O
assume	O
that	O
this	O
is	O
a	O
tractable	O
oper-	O
ation	O
.	O
for	O
example	O
,	O
if	O
we	O
choose	O
q	O
(	O
θ	O
)	O
to	O
be	O
a	O
gaussian	O
distribution	O
n	O
(	O
θ|µ	O
,	O
σ	O
)	O
,	O
then	O
\j	O
(	O
θ	O
)	O
,	O
and	O
σ	O
is	O
µ	O
is	O
set	O
equal	O
to	O
the	O
mean	B
of	O
the	O
(	O
unnormalized	O
)	O
distribution	O
fj	O
(	O
θ	O
)	O
q	O
set	O
to	O
its	O
covariance	B
.	O
more	O
generally	O
,	O
it	O
is	O
straightforward	O
to	O
obtain	O
the	O
required	O
ex-	O
pectations	O
for	O
any	O
member	O
of	O
the	O
exponential	B
family	I
,	O
provided	O
it	O
can	O
be	O
normalized	O
,	O
because	O
the	O
expected	O
statistics	O
can	O
be	O
related	O
to	O
the	O
derivatives	O
of	O
the	O
normalization	O
coefﬁcient	O
,	O
as	O
given	O
by	O
(	O
2.226	O
)	O
.	O
the	O
ep	O
approximation	O
is	O
illustrated	O
in	O
figure	O
10.14.	O
from	O
(	O
10.193	O
)	O
,	O
we	O
see	O
that	O
the	O
revised	O
factor	O
(	O
cid:4	O
)	O
fj	O
(	O
θ	O
)	O
can	O
be	O
found	O
by	O
taking	O
qnew	O
(	O
θ	O
)	O
and	O
dividing	O
out	O
the	O
remaining	O
factors	O
so	O
that	O
(	O
cid:4	O
)	O
fj	O
(	O
θ	O
)	O
=	O
k	O
qnew	O
(	O
θ	O
)	O
q\j	O
(	O
θ	O
)	O
(	O
10.199	O
)	O
where	O
we	O
have	O
used	O
(	O
10.195	O
)	O
.	O
the	O
coefﬁcient	O
k	O
is	O
determined	O
by	O
multiplying	O
both	O
10.7.	O
expectation	B
propagation	I
509	O
(	O
cid:6	O
)	O
(	O
cid:4	O
)	O
fj	O
(	O
θ	O
)	O
q	O
(	O
cid:6	O
)	O
sides	O
of	O
(	O
10.199	O
)	O
by	O
q	O
\i	O
(	O
θ	O
)	O
and	O
integrating	O
to	O
give	O
k	O
=	O
\j	O
(	O
θ	O
)	O
dθ	O
(	O
10.200	O
)	O
where	O
we	O
have	O
used	O
the	O
fact	O
that	O
qnew	O
(	O
θ	O
)	O
is	O
normalized	O
.	O
the	O
value	O
of	O
k	O
can	O
therefore	O
be	O
found	O
by	O
matching	O
zeroth-order	O
moments	O
\j	O
(	O
θ	O
)	O
dθ	O
=	O
(	O
cid:6	O
)	O
(	O
cid:4	O
)	O
fj	O
(	O
θ	O
)	O
q	O
\j	O
(	O
θ	O
)	O
dθ	O
.	O
fj	O
(	O
θ	O
)	O
q	O
(	O
10.201	O
)	O
combining	O
this	O
with	O
(	O
10.197	O
)	O
,	O
we	O
then	O
see	O
that	O
k	O
=	O
zj	O
and	O
so	O
can	O
be	O
found	O
by	O
evaluating	O
the	O
integral	O
in	O
(	O
10.197	O
)	O
.	O
in	O
practice	O
,	O
several	O
passes	O
are	O
made	O
through	O
the	O
set	O
of	O
factors	O
,	O
revising	O
each	O
factor	O
in	O
turn	O
.	O
the	O
posterior	O
distribution	O
p	O
(	O
θ|d	O
)	O
is	O
then	O
approximated	O
using	O
(	O
10.191	O
)	O
,	O
and	O
the	O
model	B
evidence	I
p	O
(	O
d	O
)	O
can	O
be	O
approximated	O
by	O
using	O
(	O
10.190	O
)	O
with	O
the	O
factors	O
fi	O
(	O
θ	O
)	O
replaced	O
by	O
their	O
approximations	O
(	O
cid:4	O
)	O
fi	O
(	O
θ	O
)	O
.	O
expectation	B
propagation	I
we	O
are	O
given	O
a	O
joint	O
distribution	O
over	O
observed	O
data	O
d	O
and	O
stochastic	B
variables	O
θ	O
in	O
the	O
form	O
of	O
a	O
product	O
of	O
factors	O
p	O
(	O
d	O
,	O
θ	O
)	O
=	O
fi	O
(	O
θ	O
)	O
(	O
10.202	O
)	O
and	O
we	O
wish	O
to	O
approximate	O
the	O
posterior	O
distribution	O
p	O
(	O
θ|d	O
)	O
by	O
a	O
distribution	O
of	O
the	O
form	O
(	O
cid:14	O
)	O
(	O
cid:14	O
)	O
i	O
1	O
z	O
q	O
(	O
θ	O
)	O
=	O
we	O
also	O
wish	O
to	O
approximate	O
the	O
model	B
evidence	I
p	O
(	O
d	O
)	O
.	O
(	O
cid:4	O
)	O
fi	O
(	O
θ	O
)	O
.	O
1.	O
initialize	O
all	O
of	O
the	O
approximating	O
factors	O
(	O
cid:4	O
)	O
fi	O
(	O
θ	O
)	O
.	O
(	O
cid:4	O
)	O
fi	O
(	O
θ	O
)	O
.	O
2.	O
initialize	O
the	O
posterior	O
approximation	O
by	O
setting	O
q	O
(	O
θ	O
)	O
∝	O
(	O
cid:14	O
)	O
i	O
i	O
3.	O
until	O
convergence	O
:	O
(	O
a	O
)	O
choose	O
a	O
factor	O
(	O
cid:4	O
)	O
fj	O
(	O
θ	O
)	O
to	O
reﬁne	O
.	O
(	O
b	O
)	O
remove	O
(	O
cid:4	O
)	O
fj	O
(	O
θ	O
)	O
from	O
the	O
posterior	O
by	O
division	O
\j	O
(	O
θ	O
)	O
=	O
q	O
(	O
θ	O
)	O
(	O
cid:4	O
)	O
fj	O
(	O
θ	O
)	O
q	O
.	O
(	O
10.203	O
)	O
(	O
10.204	O
)	O
(	O
10.205	O
)	O
510	O
10.	O
approximate	O
inference	B
(	O
c	O
)	O
evaluate	O
the	O
new	O
posterior	O
by	O
setting	O
the	O
sufﬁcient	B
statistics	I
(	O
moments	O
)	O
\j	O
(	O
θ	O
)	O
fj	O
(	O
θ	O
)	O
,	O
including	O
evaluation	O
of	O
the	O
of	O
qnew	O
(	O
θ	O
)	O
equal	O
to	O
those	O
of	O
q	O
normalization	O
constant	O
(	O
cid:6	O
)	O
zj	O
=	O
\j	O
(	O
θ	O
)	O
fj	O
(	O
θ	O
)	O
dθ	O
.	O
q	O
(	O
10.206	O
)	O
(	O
d	O
)	O
evaluate	O
and	O
store	O
the	O
new	O
factor	O
qnew	O
(	O
θ	O
)	O
q\j	O
(	O
θ	O
)	O
.	O
4.	O
evaluate	O
the	O
approximation	O
to	O
the	O
model	B
evidence	I
(	O
cid:4	O
)	O
fj	O
(	O
θ	O
)	O
=	O
zj	O
(	O
cid:6	O
)	O
(	O
cid:14	O
)	O
p	O
(	O
d	O
)	O
(	O
cid:7	O
)	O
(	O
cid:4	O
)	O
fi	O
(	O
θ	O
)	O
dθ	O
.	O
(	O
10.207	O
)	O
(	O
10.208	O
)	O
i	O
a	O
special	O
case	O
of	O
ep	O
,	O
known	O
as	O
assumed	B
density	I
ﬁltering	I
(	O
adf	O
)	O
or	O
moment	B
matching	I
(	O
maybeck	O
,	O
1982	O
;	O
lauritzen	O
,	O
1992	O
;	O
boyen	O
and	O
koller	O
,	O
1998	O
;	O
opper	O
and	O
winther	O
,	O
1999	O
)	O
,	O
is	O
obtained	O
by	O
initializing	O
all	O
of	O
the	O
approximating	O
factors	O
except	O
the	O
ﬁrst	O
to	O
unity	O
and	O
then	O
making	O
one	O
pass	O
through	O
the	O
factors	O
updating	O
each	O
of	O
them	O
once	O
.	O
assumed	B
density	I
ﬁltering	I
can	O
be	O
appropriate	O
for	O
on-line	O
learning	B
in	O
which	O
data	O
points	O
are	O
arriving	O
in	O
a	O
sequence	O
and	O
we	O
need	O
to	O
learn	O
from	O
each	O
data	O
point	O
and	O
then	O
discard	O
it	O
before	O
considering	O
the	O
next	O
point	O
.	O
however	O
,	O
in	O
a	O
batch	O
setting	O
we	O
have	O
the	O
opportunity	O
to	O
re-use	O
the	O
data	O
points	O
many	O
times	O
in	O
order	O
to	O
achieve	O
improved	O
ac-	O
curacy	O
,	O
and	O
it	O
is	O
this	O
idea	O
that	O
is	O
exploited	O
in	O
expectation	B
propagation	I
.	O
furthermore	O
,	O
if	O
we	O
apply	O
adf	O
to	O
batch	O
data	O
,	O
the	O
results	O
will	O
have	O
an	O
undesirable	O
dependence	O
on	O
the	O
(	O
arbitrary	O
)	O
order	O
in	O
which	O
the	O
data	O
points	O
are	O
considered	O
,	O
which	O
again	O
ep	O
can	O
overcome	O
.	O
one	O
disadvantage	O
of	O
expectation	B
propagation	I
is	O
that	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
iterations	O
will	O
converge	O
.	O
however	O
,	O
for	O
approximations	O
q	O
(	O
θ	O
)	O
in	O
the	O
exponential	B
family	I
,	O
if	O
the	O
iterations	O
do	O
converge	O
,	O
the	O
resulting	O
solution	O
will	O
be	O
a	O
stationary	B
point	O
of	O
a	O
particular	O
energy	B
function	I
(	O
minka	O
,	O
2001a	O
)	O
,	O
although	O
each	O
iteration	O
of	O
ep	O
does	O
not	O
necessarily	O
decrease	O
the	O
value	O
of	O
this	O
energy	B
function	I
.	O
this	O
is	O
in	O
contrast	O
to	O
variational	B
bayes	O
,	O
which	O
iteratively	O
maximizes	O
a	O
lower	B
bound	I
on	O
the	O
log	O
marginal	O
likelihood	O
,	O
in	O
which	O
each	O
iteration	O
is	O
guaranteed	O
not	O
to	O
decrease	O
the	O
bound	O
.	O
it	O
is	O
possible	O
to	O
optimize	O
the	O
ep	O
cost	B
function	I
directly	O
,	O
in	O
which	O
case	O
it	O
is	O
guaranteed	O
to	O
converge	O
,	O
although	O
the	O
resulting	O
algorithms	O
can	O
be	O
slower	O
and	O
more	O
complex	O
to	O
implement	O
.	O
another	O
difference	O
between	O
variational	B
bayes	O
and	O
ep	O
arises	O
from	O
the	O
form	O
of	O
kl	O
divergence	O
that	O
is	O
minimized	O
by	O
the	O
two	O
algorithms	O
,	O
because	O
the	O
former	O
mini-	O
mizes	O
kl	O
(	O
q	O
(	O
cid:5	O
)	O
p	O
)	O
whereas	O
the	O
latter	O
minimizes	O
kl	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
.	O
as	O
we	O
saw	O
in	O
figure	O
10.3	O
,	O
for	O
distributions	O
p	O
(	O
θ	O
)	O
which	O
are	O
multimodal	O
,	O
minimizing	O
kl	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
can	O
lead	O
to	O
poor	O
approximations	O
.	O
in	O
particular	O
,	O
if	O
ep	O
is	O
applied	O
to	O
mixtures	O
the	O
results	O
are	O
not	O
sen-	O
sible	O
because	O
the	O
approximation	O
tries	O
to	O
capture	O
all	O
of	O
the	O
modes	O
of	O
the	O
posterior	O
distribution	O
.	O
conversely	O
,	O
in	O
logistic-type	O
models	O
,	O
ep	O
often	O
out-performs	O
both	O
local	B
variational	O
methods	O
and	O
the	O
laplace	O
approximation	O
(	O
kuss	O
and	O
rasmussen	O
,	O
2006	O
)	O
.	O
10.7.	O
expectation	B
propagation	I
511	O
figure	O
10.15	O
illustration	O
of	O
the	O
clutter	B
problem	I
for	O
a	O
data	O
space	O
dimensionality	O
of	O
d	O
=	O
1.	O
training	B
data	O
points	O
,	O
de-	O
noted	O
by	O
the	O
crosses	O
,	O
are	O
drawn	O
from	O
a	O
mixture	O
of	O
two	O
gaussians	O
with	O
components	O
shown	O
in	O
red	O
and	O
green	O
.	O
the	O
goal	O
is	O
to	O
infer	O
the	O
mean	B
of	O
the	O
green	O
gaussian	O
from	O
the	O
observed	O
data	O
.	O
−5	O
0	O
θ	O
5	O
x	O
10	O
10.7.1	O
example	O
:	O
the	O
clutter	B
problem	I
following	O
minka	O
(	O
2001b	O
)	O
,	O
we	O
illustrate	O
the	O
ep	O
algorithm	O
using	O
a	O
simple	O
exam-	O
ple	O
in	O
which	O
the	O
goal	O
is	O
to	O
infer	O
the	O
mean	B
θ	O
of	O
a	O
multivariate	O
gaussian	O
distribution	O
over	O
a	O
variable	O
x	O
given	O
a	O
set	O
of	O
observations	O
drawn	O
from	O
that	O
distribution	O
.	O
to	O
make	O
the	O
problem	O
more	O
interesting	O
,	O
the	O
observations	O
are	O
embedded	O
in	O
background	O
clutter	O
,	O
which	O
itself	O
is	O
also	O
gaussian	O
distributed	O
,	O
as	O
illustrated	O
in	O
figure	O
10.15.	O
the	O
distribu-	O
tion	O
of	O
observed	O
values	O
x	O
is	O
therefore	O
a	O
mixture	O
of	O
gaussians	O
,	O
which	O
we	O
take	O
to	O
be	O
of	O
the	O
form	O
p	O
(	O
x|θ	O
)	O
=	O
(	O
1	O
−	O
w	O
)	O
n	O
(	O
x|θ	O
,	O
i	O
)	O
+	O
wn	O
(	O
x|0	O
,	O
ai	O
)	O
(	O
10.209	O
)	O
where	O
w	O
is	O
the	O
proportion	O
of	O
background	O
clutter	O
and	O
is	O
assumed	O
to	O
be	O
known	O
.	O
the	O
prior	B
over	O
θ	O
is	O
taken	O
to	O
be	O
gaussian	O
p	O
(	O
θ	O
)	O
=	O
n	O
(	O
θ|0	O
,	O
bi	O
)	O
(	O
10.210	O
)	O
and	O
minka	O
(	O
2001a	O
)	O
chooses	O
the	O
parameter	O
values	O
a	O
=	O
10	O
,	O
b	O
=	O
100	O
and	O
w	O
=	O
0.5.	O
the	O
joint	O
distribution	O
of	O
n	O
observations	O
d	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
and	O
θ	O
is	O
given	O
by	O
n	O
(	O
cid:14	O
)	O
p	O
(	O
d	O
,	O
θ	O
)	O
=	O
p	O
(	O
θ	O
)	O
p	O
(	O
xn|θ	O
)	O
(	O
10.211	O
)	O
n=1	O
and	O
so	O
the	O
posterior	O
distribution	O
comprises	O
a	O
mixture	O
of	O
2n	O
gaussians	O
.	O
thus	O
the	O
computational	O
cost	O
of	O
solving	O
this	O
problem	O
exactly	O
would	O
grow	O
exponentially	O
with	O
the	O
size	O
of	O
the	O
data	O
set	O
,	O
and	O
so	O
an	O
exact	O
solution	O
is	O
intractable	O
for	O
moderately	O
large	O
n.	O
to	O
apply	O
ep	O
to	O
the	O
clutter	B
problem	I
,	O
we	O
ﬁrst	O
identify	O
the	O
factors	O
f0	O
(	O
θ	O
)	O
=	O
p	O
(	O
θ	O
)	O
and	O
fn	O
(	O
θ	O
)	O
=	O
p	O
(	O
xn|θ	O
)	O
.	O
next	O
we	O
select	O
an	O
approximating	O
distribution	O
from	O
the	O
expo-	O
nential	O
family	O
,	O
and	O
for	O
this	O
example	O
it	O
is	O
convenient	O
to	O
choose	O
a	O
spherical	O
gaussian	O
q	O
(	O
θ	O
)	O
=	O
n	O
(	O
θ|m	O
,	O
vi	O
)	O
.	O
(	O
10.212	O
)	O
512	O
10.	O
approximate	O
inference	B
(	O
10.213	O
)	O
(	O
cid:4	O
)	O
fn	O
(	O
θ	O
)	O
=	O
snn	O
(	O
θ|mn	O
,	O
vni	O
)	O
the	O
factor	O
approximations	O
will	O
therefore	O
take	O
the	O
form	O
of	O
exponential-quadratic	O
functions	O
of	O
the	O
form	O
where	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
and	O
we	O
set	O
(	O
cid:4	O
)	O
f0	O
(	O
θ	O
)	O
equal	O
to	O
the	O
prior	B
p	O
(	O
θ	O
)	O
.	O
note	O
that	O
the	O
use	O
of	O
convenient	O
shorthand	O
notation	O
.	O
the	O
approximations	O
(	O
cid:4	O
)	O
fn	O
(	O
θ	O
)	O
,	O
for	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
can	O
n	O
(	O
θ|·	O
,	O
·	O
)	O
does	O
not	O
imply	O
that	O
the	O
right-hand	O
side	O
is	O
a	O
well-deﬁned	O
gaussian	O
density	B
(	O
in	O
fact	O
,	O
as	O
we	O
shall	O
see	O
,	O
the	O
variance	B
parameter	O
vn	O
can	O
be	O
negative	O
)	O
but	O
is	O
simply	O
a	O
be	O
initialized	O
to	O
unity	O
,	O
corresponding	O
to	O
sn	O
=	O
(	O
2πvn	O
)	O
d/2	O
,	O
vn	O
→	O
∞	O
and	O
mn	O
=	O
0	O
,	O
where	O
d	O
is	O
the	O
dimensionality	O
of	O
x	O
and	O
hence	O
of	O
θ.	O
the	O
initial	O
q	O
(	O
θ	O
)	O
,	O
deﬁned	O
by	O
(	O
10.191	O
)	O
,	O
is	O
therefore	O
equal	O
to	O
the	O
prior	B
.	O
we	O
then	O
iteratively	O
reﬁne	O
the	O
factors	O
by	O
taking	O
one	O
factor	O
fn	O
(	O
θ	O
)	O
at	O
a	O
time	O
and	O
applying	O
(	O
10.205	O
)	O
,	O
(	O
10.206	O
)	O
,	O
and	O
(	O
10.207	O
)	O
.	O
note	O
that	O
we	O
do	O
not	O
need	O
to	O
revise	O
the	O
term	O
f0	O
(	O
θ	O
)	O
because	O
an	O
ep	O
update	O
will	O
leave	O
this	O
term	O
unchanged	O
.	O
here	O
we	O
state	O
the	O
results	O
and	O
leave	O
the	O
reader	O
to	O
ﬁll	O
in	O
the	O
details	O
.	O
first	O
we	O
remove	O
the	O
current	O
estimate	O
(	O
cid:4	O
)	O
fn	O
(	O
θ	O
)	O
from	O
q	O
(	O
θ	O
)	O
by	O
division	O
using	O
(	O
10.205	O
)	O
\n	O
(	O
θ	O
)	O
,	O
which	O
has	O
mean	B
and	O
inverse	B
variance	O
given	O
by	O
n	O
(	O
m	O
−	O
mn	O
)	O
\nv	O
m\n	O
=	O
m	O
+	O
v	O
−1	O
−1	O
−	O
v	O
−1	O
\n	O
)	O
−1	O
=	O
v	O
n	O
.	O
(	O
v	O
(	O
10.214	O
)	O
(	O
10.215	O
)	O
(	O
10.216	O
)	O
exercise	O
10.37	O
exercise	O
10.38	O
to	O
give	O
q	O
next	O
we	O
evaluate	O
the	O
normalization	O
constant	O
zn	O
using	O
(	O
10.206	O
)	O
to	O
give	O
zn	O
=	O
(	O
1	O
−	O
w	O
)	O
n	O
(	O
xn|m\n	O
,	O
(	O
v	O
\n	O
+	O
1	O
)	O
i	O
)	O
+	O
wn	O
(	O
xn|0	O
,	O
ai	O
)	O
.	O
exercise	O
10.39	O
similarly	O
,	O
we	O
compute	O
the	O
mean	B
and	O
variance	B
of	O
qnew	O
(	O
θ	O
)	O
by	O
ﬁnding	O
the	O
mean	B
and	O
variance	B
of	O
q	O
\n	O
(	O
θ	O
)	O
fn	O
(	O
θ	O
)	O
to	O
give	O
\n	O
v	O
m	O
=	O
m\n	O
+	O
ρn	O
v\n	O
+	O
1	O
\n	O
)	O
2	O
\n	O
−	O
ρn	O
(	O
v	O
v\n	O
+	O
1	O
v	O
=	O
v	O
(	O
xn	O
−	O
m\n	O
)	O
+	O
ρn	O
(	O
1	O
−	O
ρn	O
)	O
(	O
v	O
\n	O
)	O
2	O
(	O
cid:5	O
)	O
xn	O
−	O
m\n	O
(	O
cid:5	O
)	O
2	O
d	O
(	O
v\n	O
+	O
1	O
)	O
2	O
(	O
10.217	O
)	O
(	O
10.218	O
)	O
where	O
the	O
quantity	O
we	O
use	O
(	O
10.207	O
)	O
to	O
compute	O
the	O
reﬁned	O
factor	O
(	O
cid:4	O
)	O
fn	O
(	O
θ	O
)	O
whose	O
parameters	O
are	O
given	O
by	O
has	O
a	O
simple	O
interpretation	O
as	O
the	O
probability	B
of	O
the	O
point	O
xn	O
not	O
being	O
clutter	O
.	O
then	O
(	O
10.219	O
)	O
ρn	O
=	O
1	O
−	O
w	O
zn	O
n	O
(	O
xn|0	O
,	O
ai	O
)	O
n	O
=	O
(	O
vnew	O
)	O
−1	O
−	O
(	O
v	O
−1	O
v	O
mn	O
=	O
m\n	O
+	O
(	O
vn	O
+	O
v	O
sn	O
=	O
\n	O
)	O
−1	O
\n	O
)	O
(	O
v	O
zn	O
(	O
2πvn	O
)	O
d/2n	O
(	O
mn|m\n	O
,	O
(	O
vn	O
+	O
v\n	O
)	O
i	O
)	O
.	O
\n	O
)	O
−1	O
(	O
mnew	O
−	O
m\n	O
)	O
(	O
10.220	O
)	O
(	O
10.221	O
)	O
(	O
10.222	O
)	O
this	O
reﬁnement	O
process	O
is	O
repeated	O
until	O
a	O
suitable	O
termination	O
criterion	O
is	O
satisﬁed	O
,	O
for	O
instance	O
that	O
the	O
maximum	O
change	O
in	O
parameter	O
values	O
resulting	O
from	O
a	O
complete	O
10.7.	O
expectation	B
propagation	I
513	O
−5	O
0	O
5	O
θ	O
10	O
−5	O
0	O
5	O
θ	O
10	O
figure	O
10.16	O
examples	O
of	O
the	O
approximation	O
of	O
speciﬁc	O
factors	O
for	O
a	O
one-dimensional	O
version	O
of	O
the	O
clutter	B
problem	I
,	O
showing	O
fn	O
(	O
θ	O
)	O
in	O
blue	O
,	O
efn	O
(	O
θ	O
)	O
in	O
red	O
,	O
and	O
q\n	O
(	O
θ	O
)	O
in	O
green	O
.	O
notice	O
that	O
the	O
current	O
form	O
for	O
q\n	O
(	O
θ	O
)	O
controls	O
the	O
range	O
of	O
θ	O
over	O
which	O
efn	O
(	O
θ	O
)	O
will	O
be	O
a	O
good	O
approximation	O
to	O
fn	O
(	O
θ	O
)	O
.	O
pass	O
through	O
all	O
factors	O
is	O
less	O
than	O
some	O
threshold	O
.	O
finally	O
,	O
we	O
use	O
(	O
10.208	O
)	O
to	O
evaluate	O
the	O
approximation	O
to	O
the	O
model	B
evidence	I
,	O
given	O
by	O
p	O
(	O
d	O
)	O
(	O
cid:7	O
)	O
(	O
2πvnew	O
)	O
d/2	O
exp	O
(	O
b/2	O
)	O
sn	O
(	O
2πvn	O
)	O
−d/2	O
(	O
cid:27	O
)	O
n	O
(	O
cid:14	O
)	O
(	O
cid:26	O
)	O
−	O
n	O
(	O
cid:2	O
)	O
n=1	O
n=1	O
mt	O
nmn	O
vn	O
.	O
(	O
10.223	O
)	O
(	O
10.224	O
)	O
where	O
b	O
=	O
(	O
mnew	O
)	O
tmnew	O
v	O
examples	O
factor	O
approximations	O
for	O
the	O
clutter	B
problem	I
with	O
a	O
one-dimensional	O
pa-	O
rameter	O
space	O
θ	O
are	O
shown	O
in	O
figure	O
10.16.	O
note	O
that	O
the	O
factor	O
approximations	O
can	O
have	O
inﬁnite	O
or	O
even	O
negative	O
values	O
for	O
the	O
‘	O
variance	B
’	O
parameter	O
vn	O
.	O
this	O
simply	O
corresponds	O
to	O
approximations	O
that	O
curve	O
upwards	O
instead	O
of	O
downwards	O
and	O
are	O
not	O
necessarily	O
problematic	O
provided	O
the	O
overall	O
approximate	O
posterior	O
q	O
(	O
θ	O
)	O
has	O
posi-	O
tive	O
variance	B
.	O
figure	O
10.17	O
compares	O
the	O
performance	O
of	O
ep	O
with	O
variational	B
bayes	O
(	O
mean	B
ﬁeld	I
theory	I
)	O
and	O
the	O
laplace	O
approximation	O
on	O
the	O
clutter	B
problem	I
.	O
10.7.2	O
expectation	B
propagation	I
on	O
graphs	O
so	O
far	O
in	O
our	O
general	O
discussion	O
of	O
ep	O
,	O
we	O
have	O
allowed	O
the	O
factors	O
fi	O
(	O
θ	O
)	O
in	O
the	O
distribution	O
p	O
(	O
θ	O
)	O
to	O
be	O
functions	O
of	O
all	O
of	O
the	O
components	O
of	O
θ	O
,	O
and	O
similarly	O
for	O
the	O
approximating	O
factors	O
(	O
cid:4	O
)	O
f	O
(	O
θ	O
)	O
in	O
the	O
approximating	O
distribution	O
q	O
(	O
θ	O
)	O
.	O
we	O
now	O
consider	O
situations	O
in	O
which	O
the	O
factors	O
depend	O
only	O
on	O
subsets	O
of	O
the	O
variables	O
.	O
such	O
restric-	O
tions	O
can	O
be	O
conveniently	O
expressed	O
using	O
the	O
framework	O
of	O
probabilistic	O
graphical	O
models	O
,	O
as	O
discussed	O
in	O
chapter	O
8.	O
here	O
we	O
use	O
a	O
factor	B
graph	I
representation	O
because	O
this	O
encompasses	O
both	O
directed	B
and	O
undirected	B
graphs	O
.	O
514	O
10.	O
approximate	O
inference	B
posterior	O
mean	B
100	O
r	O
o	O
r	O
r	O
e	O
10−5	O
laplace	O
vb	O
104	O
ep	O
106	O
flops	O
evidence	O
vb	O
10−200	O
r	O
o	O
r	O
r	O
e	O
10−202	O
laplace	O
10−204	O
104	O
106	O
flops	O
ep	O
figure	O
10.17	O
comparison	O
of	O
expectation	B
propagation	I
,	O
variational	B
inference	I
,	O
and	O
the	O
laplace	O
approximation	O
on	O
the	O
clutter	B
problem	I
.	O
the	O
left-hand	O
plot	O
shows	O
the	O
error	B
in	O
the	O
predicted	O
posterior	O
mean	O
versus	O
the	O
number	O
of	O
ﬂoating	O
point	O
operations	O
,	O
and	O
the	O
right-hand	O
plot	O
shows	O
the	O
corresponding	O
results	O
for	O
the	O
model	B
evidence	I
.	O
we	O
shall	O
focus	O
on	O
the	O
case	O
in	O
which	O
the	O
approximating	O
distribution	O
is	O
fully	O
fac-	O
torized	O
,	O
and	O
we	O
shall	O
show	O
that	O
in	O
this	O
case	O
expectation	B
propagation	I
reduces	O
to	O
loopy	B
belief	I
propagation	I
(	O
minka	O
,	O
2001a	O
)	O
.	O
to	O
start	O
with	O
,	O
we	O
show	O
this	O
in	O
the	O
context	O
of	O
a	O
simple	O
example	O
,	O
and	O
then	O
we	O
shall	O
explore	O
the	O
general	O
case	O
.	O
first	O
of	O
all	O
,	O
recall	O
from	O
(	O
10.17	O
)	O
that	O
if	O
we	O
minimize	O
the	O
kullback-leibler	O
diver-	O
gence	O
kl	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
with	O
respect	O
to	O
a	O
factorized	B
distribution	I
q	O
,	O
then	O
the	O
optimal	O
solution	O
for	O
each	O
factor	O
is	O
simply	O
the	O
corresponding	O
marginal	B
of	O
p.	O
section	O
8.4.4	O
now	O
consider	O
the	O
factor	B
graph	I
shown	O
on	O
the	O
left	O
in	O
figure	O
10.18	O
,	O
which	O
was	O
introduced	O
earlier	O
in	O
the	O
context	O
of	O
the	O
sum-product	B
algorithm	I
.	O
the	O
joint	O
distribution	O
is	O
given	O
by	O
we	O
seek	O
an	O
approximation	O
q	O
(	O
x	O
)	O
that	O
has	O
the	O
same	O
factorization	B
,	O
so	O
that	O
p	O
(	O
x	O
)	O
=	O
fa	O
(	O
x1	O
,	O
x2	O
)	O
fb	O
(	O
x2	O
,	O
x3	O
)	O
fc	O
(	O
x2	O
,	O
x4	O
)	O
.	O
q	O
(	O
x	O
)	O
∝	O
(	O
cid:4	O
)	O
fa	O
(	O
x1	O
,	O
x2	O
)	O
(	O
cid:4	O
)	O
fb	O
(	O
x2	O
,	O
x3	O
)	O
(	O
cid:4	O
)	O
fc	O
(	O
x2	O
,	O
x4	O
)	O
.	O
(	O
10.225	O
)	O
(	O
10.226	O
)	O
note	O
that	O
normalization	O
constants	O
have	O
been	O
omitted	O
,	O
and	O
these	O
can	O
be	O
re-instated	O
at	O
the	O
end	O
by	O
local	B
normalization	O
,	O
as	O
is	O
generally	O
done	O
in	O
belief	B
propagation	I
.	O
now	O
sup-	O
pose	O
we	O
restrict	O
attention	O
to	O
approximations	O
in	O
which	O
the	O
factors	O
themselves	O
factorize	O
with	O
respect	O
to	O
the	O
individual	O
variables	O
so	O
that	O
q	O
(	O
x	O
)	O
∝	O
(	O
cid:4	O
)	O
fa1	O
(	O
x1	O
)	O
(	O
cid:4	O
)	O
fa2	O
(	O
x2	O
)	O
(	O
cid:4	O
)	O
fb2	O
(	O
x2	O
)	O
(	O
cid:4	O
)	O
fb3	O
(	O
x3	O
)	O
(	O
cid:4	O
)	O
fc2	O
(	O
x2	O
)	O
(	O
cid:4	O
)	O
fc4	O
(	O
x4	O
)	O
(	O
10.227	O
)	O
which	O
corresponds	O
to	O
the	O
factor	B
graph	I
shown	O
on	O
the	O
right	O
in	O
figure	O
10.18.	O
because	O
the	O
individual	O
factors	O
are	O
factorized	O
,	O
the	O
overall	O
distribution	O
q	O
(	O
x	O
)	O
is	O
itself	O
fully	O
fac-	O
torized	O
.	O
now	O
we	O
apply	O
the	O
ep	O
algorithm	O
using	O
the	O
fully	O
factorized	O
approximation	O
.	O
sup-	O
pose	O
that	O
we	O
have	O
initialized	O
all	O
of	O
the	O
factors	O
and	O
that	O
we	O
choose	O
to	O
reﬁne	O
factor	O
x1	O
x2	O
x3	O
x1	O
x2	O
x3	O
10.7.	O
expectation	B
propagation	I
515	O
fa	O
fb	O
˜fa1	O
˜fa2	O
fc	O
x4	O
˜fb2	O
˜fb3	O
˜fc2	O
˜fc4	O
x4	O
figure	O
10.18	O
on	O
the	O
left	O
is	O
a	O
simple	O
factor	B
graph	I
from	O
figure	O
8.51	O
and	O
reproduced	O
here	O
for	O
convenience	O
.	O
on	O
the	O
right	O
is	O
the	O
corresponding	O
factorized	O
approximation	O
.	O
(	O
cid:4	O
)	O
fb	O
(	O
x2	O
,	O
x3	O
)	O
=	O
(	O
cid:4	O
)	O
fb2	O
(	O
x2	O
)	O
(	O
cid:4	O
)	O
fb3	O
(	O
x3	O
)	O
.	O
we	O
ﬁrst	O
remove	O
this	O
factor	O
from	O
the	O
approximating	O
distribution	O
to	O
give	O
\b	O
(	O
x	O
)	O
=	O
(	O
cid:4	O
)	O
fa1	O
(	O
x1	O
)	O
(	O
cid:4	O
)	O
fa2	O
(	O
x2	O
)	O
(	O
cid:4	O
)	O
fc2	O
(	O
x2	O
)	O
(	O
cid:4	O
)	O
fc4	O
(	O
x4	O
)	O
q	O
x2	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
x3	O
x2	O
(	O
cid:4	O
)	O
fb2	O
(	O
x2	O
)	O
∝	O
(	O
cid:4	O
)	O
fb3	O
(	O
x3	O
)	O
∝	O
(	O
10.228	O
)	O
(	O
10.229	O
)	O
and	O
we	O
then	O
multiply	O
this	O
by	O
the	O
exact	O
factor	O
fb	O
(	O
x2	O
,	O
x3	O
)	O
to	O
give	O
the	O
result	O
,	O
as	O
noted	O
above	O
,	O
is	O
that	O
qnew	O
(	O
z	O
)	O
comprises	O
the	O
product	O
of	O
factors	O
,	O
one	O
for	O
each	O
variable	O
xi	O
,	O
in	O
which	O
each	O
factor	O
is	O
given	O
by	O
the	O
corresponding	O
marginal	B
of	O
\b	O
(	O
x	O
)	O
fb	O
(	O
x2	O
,	O
x3	O
)	O
=	O
(	O
cid:4	O
)	O
fa1	O
(	O
x1	O
)	O
(	O
cid:4	O
)	O
fa2	O
(	O
x2	O
)	O
(	O
cid:4	O
)	O
fc2	O
(	O
x2	O
)	O
(	O
cid:4	O
)	O
fc4	O
(	O
x4	O
)	O
fb	O
(	O
x2	O
,	O
x3	O
)	O
.	O
(	O
cid:1	O
)	O
p	O
(	O
x	O
)	O
=	O
q	O
we	O
now	O
ﬁnd	O
qnew	O
(	O
x	O
)	O
by	O
minimizing	O
the	O
kullback-leibler	O
divergence	O
kl	O
(	O
(	O
cid:1	O
)	O
p	O
(	O
cid:5	O
)	O
qnew	O
)	O
.	O
(	O
cid:1	O
)	O
p	O
(	O
x	O
)	O
.	O
these	O
four	O
marginals	O
are	O
given	O
by	O
(	O
cid:1	O
)	O
p	O
(	O
x1	O
)	O
∝	O
(	O
cid:4	O
)	O
fa1	O
(	O
x1	O
)	O
(	O
cid:1	O
)	O
p	O
(	O
x2	O
)	O
∝	O
(	O
cid:4	O
)	O
fa2	O
(	O
x2	O
)	O
(	O
cid:4	O
)	O
fc2	O
(	O
x2	O
)	O
(	O
cid:19	O
)	O
(	O
cid:20	O
)	O
(	O
cid:2	O
)	O
fb	O
(	O
x2	O
,	O
x3	O
)	O
(	O
cid:4	O
)	O
fa2	O
(	O
x2	O
)	O
(	O
cid:4	O
)	O
fc2	O
(	O
x2	O
)	O
(	O
cid:1	O
)	O
p	O
(	O
x3	O
)	O
∝	O
(	O
cid:1	O
)	O
p	O
(	O
x4	O
)	O
∝	O
(	O
cid:4	O
)	O
fc4	O
(	O
x4	O
)	O
(	O
cid:2	O
)	O
and	O
qnew	O
(	O
x	O
)	O
is	O
obtained	O
by	O
multiplying	O
these	O
marginals	O
together	O
.	O
we	O
see	O
that	O
the	O
only	O
factors	O
in	O
q	O
(	O
x	O
)	O
that	O
change	O
when	O
we	O
update	O
(	O
cid:4	O
)	O
fb	O
(	O
x2	O
,	O
x3	O
)	O
are	O
those	O
that	O
involve	O
the	O
variables	O
in	O
fb	O
namely	O
x2	O
and	O
x3	O
.	O
to	O
obtain	O
the	O
reﬁned	O
factor	O
(	O
cid:4	O
)	O
fb	O
(	O
x2	O
,	O
x3	O
)	O
=	O
(	O
cid:4	O
)	O
fb2	O
(	O
x2	O
)	O
(	O
cid:4	O
)	O
fb3	O
(	O
x3	O
)	O
we	O
simply	O
divide	O
qnew	O
(	O
x	O
)	O
by	O
q	O
\b	O
(	O
x	O
)	O
,	O
which	O
gives	O
fb	O
(	O
x2	O
,	O
x3	O
)	O
(	O
10.230	O
)	O
(	O
10.231	O
)	O
x3	O
(	O
10.232	O
)	O
(	O
10.233	O
)	O
fb	O
(	O
x2	O
,	O
x3	O
)	O
(	O
cid:19	O
)	O
(	O
cid:20	O
)	O
fb	O
(	O
x2	O
,	O
x3	O
)	O
(	O
cid:4	O
)	O
fa2	O
(	O
x2	O
)	O
(	O
cid:4	O
)	O
fc2	O
(	O
x2	O
)	O
.	O
(	O
10.234	O
)	O
(	O
10.235	O
)	O
516	O
10.	O
approximate	O
inference	B
section	O
8.4.4	O
µfb→x2	O
(	O
x2	O
)	O
sent	O
by	O
factor	O
node	O
fb	O
to	O
variable	O
node	B
x2	O
and	O
is	O
given	O
by	O
(	O
8.81	O
)	O
.	O
simi-	O
these	O
are	O
precisely	O
the	O
messages	O
obtained	O
using	O
belief	B
propagation	I
in	O
which	O
mes-	O
sages	O
from	O
variable	O
nodes	O
to	O
factor	O
nodes	O
have	O
been	O
folded	O
into	O
the	O
messages	O
from	O
factor	O
nodes	O
to	O
variable	O
nodes	O
.	O
in	O
particular	O
,	O
(	O
cid:4	O
)	O
fb2	O
(	O
x2	O
)	O
corresponds	O
to	O
the	O
message	O
larly	O
,	O
if	O
we	O
substitute	O
(	O
8.78	O
)	O
into	O
(	O
8.79	O
)	O
,	O
we	O
obtain	O
(	O
10.235	O
)	O
in	O
which	O
(	O
cid:4	O
)	O
fa2	O
(	O
x2	O
)	O
corre-	O
sponds	O
to	O
µfa→x2	O
(	O
x2	O
)	O
and	O
(	O
cid:4	O
)	O
fc2	O
(	O
x2	O
)	O
corresponds	O
to	O
µfc→x2	O
(	O
x2	O
)	O
,	O
giving	O
the	O
message	O
(	O
cid:4	O
)	O
fb3	O
(	O
x3	O
)	O
which	O
corresponds	O
to	O
µfb→x3	O
(	O
x3	O
)	O
.	O
factors	O
at	O
a	O
time	O
,	O
for	O
instance	O
if	O
we	O
reﬁne	O
only	O
(	O
cid:4	O
)	O
fb3	O
(	O
x3	O
)	O
,	O
then	O
(	O
cid:4	O
)	O
fb2	O
(	O
x2	O
)	O
is	O
unchanged	O
by	O
deﬁnition	O
,	O
while	O
the	O
reﬁned	O
version	O
of	O
(	O
cid:4	O
)	O
fb3	O
(	O
x3	O
)	O
is	O
again	O
given	O
by	O
(	O
10.235	O
)	O
.	O
if	O
this	O
result	O
differs	O
slightly	O
from	O
standard	O
belief	O
propagation	O
in	O
that	O
messages	O
are	O
passed	O
in	O
both	O
directions	O
at	O
the	O
same	O
time	O
.	O
we	O
can	O
easily	O
modify	O
the	O
ep	O
procedure	O
to	O
give	O
the	O
standard	O
form	O
of	O
the	O
sum-product	B
algorithm	I
by	O
updating	O
just	O
one	O
of	O
the	O
we	O
are	O
reﬁning	O
only	O
one	O
term	O
at	O
a	O
time	O
,	O
then	O
we	O
can	O
choose	O
the	O
order	O
in	O
which	O
the	O
reﬁnements	O
are	O
done	O
as	O
we	O
wish	O
.	O
in	O
particular	O
,	O
for	O
a	O
tree-structured	O
graph	O
we	O
can	O
follow	O
a	O
two-pass	O
update	O
scheme	O
,	O
corresponding	O
to	O
the	O
standard	O
belief	O
propagation	O
schedule	B
,	O
which	O
will	O
result	O
in	O
exact	O
inference	O
of	O
the	O
variable	O
and	O
factor	O
marginals	O
.	O
the	O
initialization	O
of	O
the	O
approximation	O
factors	O
in	O
this	O
case	O
is	O
unimportant	O
.	O
now	O
let	O
us	O
consider	O
a	O
general	O
factor	B
graph	I
corresponding	O
to	O
the	O
distribution	O
(	O
cid:14	O
)	O
p	O
(	O
θ	O
)	O
=	O
fi	O
(	O
θi	O
)	O
(	O
10.236	O
)	O
where	O
θi	O
represents	O
the	O
subset	O
of	O
variables	O
associated	O
with	O
factor	O
fi	O
.	O
we	O
approximate	O
this	O
using	O
a	O
fully	O
factorized	O
distribution	O
of	O
the	O
form	O
(	O
cid:14	O
)	O
i	O
(	O
cid:14	O
)	O
(	O
cid:4	O
)	O
fik	O
(	O
θk	O
)	O
q	O
(	O
θ	O
)	O
∝	O
(	O
10.237	O
)	O
i	O
k	O
where	O
θk	O
corresponds	O
to	O
an	O
individual	O
variable	O
node	B
.	O
suppose	O
that	O
we	O
wish	O
to	O
reﬁne	O
the	O
particular	O
term	O
(	O
cid:4	O
)	O
fjl	O
(	O
θl	O
)	O
keeping	O
all	O
other	O
terms	O
ﬁxed	O
.	O
we	O
ﬁrst	O
remove	O
the	O
term	O
(	O
cid:4	O
)	O
fj	O
(	O
θj	O
)	O
from	O
q	O
(	O
θ	O
)	O
to	O
give	O
and	O
then	O
multiply	O
by	O
the	O
exact	O
factor	O
fj	O
(	O
θj	O
)	O
.	O
to	O
determine	O
the	O
reﬁned	O
term	O
(	O
cid:4	O
)	O
fjl	O
(	O
θl	O
)	O
,	O
(	O
cid:4	O
)	O
fik	O
(	O
θk	O
)	O
\j	O
(	O
θ	O
)	O
∝	O
(	O
10.238	O
)	O
(	O
cid:14	O
)	O
(	O
cid:14	O
)	O
i	O
(	O
cid:9	O
)	O
=j	O
k	O
q	O
we	O
need	O
only	O
consider	O
the	O
functional	B
dependence	O
on	O
θl	O
,	O
and	O
so	O
we	O
simply	O
ﬁnd	O
the	O
corresponding	O
marginal	B
of	O
(	O
10.239	O
)	O
up	O
to	O
a	O
multiplicative	O
constant	O
,	O
this	O
involves	O
taking	O
the	O
marginal	B
of	O
fj	O
(	O
θj	O
)	O
multiplied	O
\j	O
(	O
θ	O
)	O
that	O
are	O
functions	O
of	O
any	O
of	O
the	O
variables	O
in	O
θj	O
.	O
terms	O
that	O
by	O
any	O
terms	O
from	O
q	O
correspond	O
to	O
other	O
factors	O
(	O
cid:4	O
)	O
fi	O
(	O
θi	O
)	O
for	O
i	O
(	O
cid:9	O
)	O
=	O
j	O
will	O
cancel	O
between	O
numerator	O
and	O
q	O
\j	O
(	O
θ	O
)	O
fj	O
(	O
θj	O
)	O
.	O
denominator	O
when	O
we	O
subsequently	O
divide	O
by	O
q	O
(	O
cid:2	O
)	O
(	O
cid:4	O
)	O
fjl	O
(	O
θl	O
)	O
∝	O
fj	O
(	O
θj	O
)	O
θm	O
(	O
cid:2	O
)	O
=l∈θj	O
\j	O
(	O
θ	O
)	O
.	O
we	O
therefore	O
obtain	O
(	O
cid:14	O
)	O
(	O
cid:14	O
)	O
m	O
(	O
cid:9	O
)	O
=l	O
k	O
(	O
cid:4	O
)	O
fkm	O
(	O
θm	O
)	O
.	O
(	O
10.240	O
)	O
exercises	O
517	O
we	O
recognize	O
this	O
as	O
the	O
sum-product	O
rule	O
in	O
the	O
form	O
in	O
which	O
messages	O
from	O
vari-	O
able	O
nodes	O
to	O
factor	O
nodes	O
have	O
been	O
eliminated	O
,	O
as	O
illustrated	O
by	O
the	O
example	O
shown	O
in	O
figure	O
8.50.	O
the	O
quantity	O
(	O
cid:4	O
)	O
fjm	O
(	O
θm	O
)	O
corresponds	O
to	O
the	O
message	O
µfj→θm	O
(	O
θm	O
)	O
,	O
which	O
factor	O
node	O
j	O
sends	O
to	O
variable	O
node	B
m	O
,	O
and	O
the	O
product	O
over	O
k	O
in	O
(	O
10.240	O
)	O
is	O
over	O
all	O
factors	O
that	O
depend	O
on	O
the	O
variables	O
θm	O
that	O
have	O
variables	O
(	O
other	O
than	O
variable	O
θl	O
)	O
in	O
common	O
with	O
factor	O
fj	O
(	O
θj	O
)	O
.	O
in	O
other	O
words	O
,	O
to	O
compute	O
the	O
outgoing	O
message	O
from	O
a	O
factor	O
node	O
,	O
we	O
take	O
the	O
product	O
of	O
all	O
the	O
incoming	O
messages	O
from	O
other	O
factor	O
nodes	O
,	O
multiply	O
by	O
the	O
local	B
factor	O
,	O
and	O
then	O
marginalize	O
.	O
thus	O
,	O
the	O
sum-product	B
algorithm	I
arises	O
as	O
a	O
special	O
case	O
of	O
expectation	B
propa-	O
gation	O
if	O
we	O
use	O
an	O
approximating	O
distribution	O
that	O
is	O
fully	O
factorized	O
.	O
this	O
suggests	O
that	O
more	O
ﬂexible	O
approximating	O
distributions	O
,	O
corresponding	O
to	O
partially	O
discon-	O
nected	O
graphs	O
,	O
could	O
be	O
used	O
to	O
achieve	O
higher	O
accuracy	O
.	O
another	O
generalization	B
is	O
to	O
group	O
factors	O
fi	O
(	O
θi	O
)	O
together	O
into	O
sets	O
and	O
to	O
reﬁne	O
all	O
the	O
factors	O
in	O
a	O
set	O
together	O
at	O
each	O
iteration	O
.	O
both	O
of	O
these	O
approaches	O
can	O
lead	O
to	O
improvements	O
in	O
accuracy	O
(	O
minka	O
,	O
2001b	O
)	O
.	O
in	O
general	O
,	O
the	O
problem	O
of	O
choosing	O
the	O
best	O
combination	O
of	O
group-	O
ing	O
and	O
disconnection	O
is	O
an	O
open	O
research	O
issue	O
.	O
we	O
have	O
seen	O
that	O
variational	B
message	O
passing	O
and	O
expectation	B
propagation	I
op-	O
timize	O
two	O
different	O
forms	O
of	O
the	O
kullback-leibler	O
divergence	O
.	O
minka	O
(	O
2005	O
)	O
has	O
shown	O
that	O
a	O
broad	O
range	O
of	O
message	B
passing	I
algorithms	O
can	O
be	O
derived	O
from	O
a	O
com-	O
mon	O
framework	O
involving	O
minimization	O
of	O
members	O
of	O
the	O
alpha	O
family	O
of	O
diver-	O
gences	O
,	O
given	O
by	O
(	O
10.19	O
)	O
.	O
these	O
include	O
variational	B
message	O
passing	O
,	O
loopy	B
belief	I
propagation	I
,	O
and	O
expectation	B
propagation	I
,	O
as	O
well	O
as	O
a	O
range	O
of	O
other	O
algorithms	O
,	O
which	O
we	O
do	O
not	O
have	O
space	O
to	O
discuss	O
here	O
,	O
such	O
as	O
tree-reweighted	O
message	O
pass-	O
ing	O
(	O
wainwright	O
et	O
al.	O
,	O
2005	O
)	O
,	O
fractional	B
belief	I
propagation	I
(	O
wiegerinck	O
and	O
heskes	O
,	O
2003	O
)	O
,	O
and	O
power	O
ep	O
(	O
minka	O
,	O
2004	O
)	O
.	O
exercises	O
10.1	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
verify	O
that	O
the	O
log	O
marginal	O
distribution	O
of	O
the	O
observed	O
data	O
ln	O
p	O
(	O
x	O
)	O
can	O
be	O
decomposed	O
into	O
two	O
terms	O
in	O
the	O
form	O
(	O
10.2	O
)	O
where	O
l	O
(	O
q	O
)	O
is	O
given	O
by	O
(	O
10.3	O
)	O
and	O
kl	O
(	O
q	O
(	O
cid:5	O
)	O
p	O
)	O
is	O
given	O
by	O
(	O
10.4	O
)	O
.	O
10.2	O
(	O
(	O
cid:12	O
)	O
)	O
use	O
the	O
properties	O
e	O
[	O
z1	O
]	O
=	O
m1	O
and	O
e	O
[	O
z2	O
]	O
=	O
m2	O
to	O
solve	O
the	O
simultaneous	O
equa-	O
tions	O
(	O
10.13	O
)	O
and	O
(	O
10.15	O
)	O
,	O
and	O
hence	O
show	O
that	O
,	O
provided	O
the	O
original	O
distribution	O
p	O
(	O
z	O
)	O
is	O
nonsingular	O
,	O
the	O
unique	O
solution	O
for	O
the	O
means	O
of	O
the	O
factors	O
in	O
the	O
approxi-	O
mation	B
distribution	O
is	O
given	O
by	O
e	O
[	O
z1	O
]	O
=	O
µ1	O
and	O
e	O
[	O
z2	O
]	O
=	O
µ2	O
.	O
10.3	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
factorized	O
variational	O
distribution	O
q	O
(	O
z	O
)	O
of	O
the	O
form	O
(	O
10.5	O
)	O
.	O
by	O
using	O
the	O
technique	O
of	O
lagrange	O
multipliers	O
,	O
verify	O
that	O
minimization	O
of	O
the	O
kullback-leibler	O
divergence	O
kl	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
with	O
respect	O
to	O
one	O
of	O
the	O
factors	O
qi	O
(	O
zi	O
)	O
,	O
keeping	O
all	O
other	O
factors	O
ﬁxed	O
,	O
leads	O
to	O
the	O
solution	O
(	O
10.17	O
)	O
.	O
10.4	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
suppose	O
that	O
p	O
(	O
x	O
)	O
is	O
some	O
ﬁxed	O
distribution	O
and	O
that	O
we	O
wish	O
to	O
approximate	O
it	O
using	O
a	O
gaussian	O
distribution	O
q	O
(	O
x	O
)	O
=	O
n	O
(	O
x|µ	O
,	O
σ	O
)	O
.	O
by	O
writing	O
down	O
the	O
form	O
of	O
the	O
kl	O
divergence	O
kl	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
for	O
a	O
gaussian	O
q	O
(	O
x	O
)	O
and	O
then	O
differentiating	O
,	O
show	O
that	O
518	O
10.	O
approximate	O
inference	B
minimization	O
of	O
kl	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
with	O
respect	O
to	O
µ	O
and	O
σ	O
leads	O
to	O
the	O
result	O
that	O
µ	O
is	O
given	O
by	O
the	O
expectation	B
of	O
x	O
under	O
p	O
(	O
x	O
)	O
and	O
that	O
σ	O
is	O
given	O
by	O
the	O
covariance	B
.	O
10.5	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
model	O
in	O
which	O
the	O
set	O
of	O
all	O
hidden	O
stochastic	O
variables	O
,	O
de-	O
noted	O
collectively	O
by	O
z	O
,	O
comprises	O
some	O
latent	O
variables	O
z	O
together	O
with	O
some	O
model	O
parameters	O
θ.	O
suppose	O
we	O
use	O
a	O
variational	B
distribution	O
that	O
factorizes	O
between	O
la-	O
tent	O
variables	O
and	O
parameters	O
so	O
that	O
q	O
(	O
z	O
,	O
θ	O
)	O
=	O
qz	O
(	O
z	O
)	O
qθ	O
(	O
θ	O
)	O
,	O
in	O
which	O
the	O
distribution	O
qθ	O
(	O
θ	O
)	O
is	O
approximated	O
by	O
a	O
point	O
estimate	O
of	O
the	O
form	O
qθ	O
(	O
θ	O
)	O
=	O
δ	O
(	O
θ	O
−	O
θ0	O
)	O
where	O
θ0	O
is	O
a	O
vector	O
of	O
free	O
parameters	O
.	O
show	O
that	O
variational	B
optimization	O
of	O
this	O
factorized	B
distribution	I
is	O
equivalent	O
to	O
an	O
em	O
algorithm	O
,	O
in	O
which	O
the	O
e	O
step	O
optimizes	O
qz	O
(	O
z	O
)	O
,	O
and	O
the	O
m	O
step	O
maximizes	O
the	O
expected	O
complete-data	O
log	O
posterior	O
distribution	O
of	O
θ	O
with	O
respect	O
to	O
θ0	O
.	O
10.6	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
the	O
alpha	O
family	O
of	O
divergences	O
is	O
deﬁned	O
by	O
(	O
10.19	O
)	O
.	O
show	O
that	O
the	O
kullback-	O
leibler	O
divergence	O
kl	O
(	O
p	O
(	O
cid:5	O
)	O
q	O
)	O
corresponds	O
to	O
α	O
→	O
1.	O
this	O
can	O
be	O
done	O
by	O
writing	O
p	O
=	O
exp	O
(	O
	O
ln	O
p	O
)	O
=	O
1	O
+	O
	O
ln	O
p	O
+	O
o	O
(	O
2	O
)	O
and	O
then	O
taking	O
	O
→	O
0.	O
similarly	O
show	O
that	O
kl	O
(	O
q	O
(	O
cid:5	O
)	O
p	O
)	O
corresponds	O
to	O
α	O
→	O
−1	O
.	O
10.7	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
the	O
problem	O
of	O
inferring	O
the	O
mean	B
and	O
precision	O
of	O
a	O
univariate	O
gaus-	O
sian	O
using	O
a	O
factorized	O
variational	O
approximation	O
,	O
as	O
considered	O
in	O
section	O
10.1.3.	O
show	O
that	O
the	O
factor	O
qµ	O
(	O
µ	O
)	O
is	O
a	O
gaussian	O
of	O
the	O
form	O
n	O
(	O
µ|µn	O
,	O
λ	O
−1	O
n	O
)	O
with	O
mean	B
and	O
precision	O
given	O
by	O
(	O
10.26	O
)	O
and	O
(	O
10.27	O
)	O
,	O
respectively	O
.	O
similarly	O
show	O
that	O
the	O
factor	O
qτ	O
(	O
τ	O
)	O
is	O
a	O
gamma	B
distribution	I
of	O
the	O
form	O
gam	O
(	O
τ|an	O
,	O
bn	O
)	O
with	O
parameters	O
given	O
by	O
(	O
10.29	O
)	O
and	O
(	O
10.30	O
)	O
.	O
10.8	O
(	O
(	O
cid:12	O
)	O
)	O
consider	O
the	O
variational	B
posterior	O
distribution	O
for	O
the	O
precision	O
of	O
a	O
univariate	O
gaussian	O
whose	O
parameters	O
are	O
given	O
by	O
(	O
10.29	O
)	O
and	O
(	O
10.30	O
)	O
.	O
by	O
using	O
the	O
standard	O
results	O
for	O
the	O
mean	B
and	O
variance	B
of	O
the	O
gamma	B
distribution	I
given	O
by	O
(	O
b.27	O
)	O
and	O
(	O
b.28	O
)	O
,	O
show	O
that	O
if	O
we	O
let	O
n	O
→	O
∞	O
,	O
this	O
variational	B
posterior	O
distribution	O
has	O
a	O
mean	B
given	O
by	O
the	O
inverse	B
of	O
the	O
maximum	B
likelihood	I
estimator	O
for	O
the	O
variance	B
of	O
the	O
data	O
,	O
and	O
a	O
variance	B
that	O
goes	O
to	O
zero	O
.	O
10.9	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
by	O
making	O
use	O
of	O
the	O
standard	O
result	O
e	O
[	O
τ	O
]	O
=	O
an	O
/bn	O
for	O
the	O
mean	B
of	O
a	O
gamma	B
distribution	I
,	O
together	O
with	O
(	O
10.26	O
)	O
,	O
(	O
10.27	O
)	O
,	O
(	O
10.29	O
)	O
,	O
and	O
(	O
10.30	O
)	O
,	O
derive	O
the	O
result	O
(	O
10.33	O
)	O
for	O
the	O
reciprocal	O
of	O
the	O
expected	O
precision	O
in	O
the	O
factorized	O
variational	O
treat-	O
ment	O
of	O
a	O
univariate	O
gaussian	O
.	O
10.10	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
derive	O
the	O
decomposition	O
given	O
by	O
(	O
10.34	O
)	O
that	O
is	O
used	O
to	O
ﬁnd	O
approxi-	O
mate	O
posterior	O
distributions	O
over	O
models	O
using	O
variational	B
inference	I
.	O
10.11	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
by	O
using	O
a	O
lagrange	O
multiplier	O
to	O
enforce	O
the	O
normalization	O
constraint	O
on	O
the	O
distribution	O
q	O
(	O
m	O
)	O
,	O
show	O
that	O
the	O
maximum	O
of	O
the	O
lower	B
bound	I
(	O
10.35	O
)	O
is	O
given	O
by	O
(	O
10.36	O
)	O
.	O
10.12	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
starting	O
from	O
the	O
joint	O
distribution	O
(	O
10.41	O
)	O
,	O
and	O
applying	O
the	O
general	O
result	O
(	O
10.9	O
)	O
,	O
show	O
that	O
the	O
optimal	O
variational	B
distribution	O
q	O
(	O
cid:1	O
)	O
(	O
z	O
)	O
over	O
the	O
latent	O
variables	O
for	O
the	O
bayesian	O
mixture	O
of	O
gaussians	O
is	O
given	O
by	O
(	O
10.48	O
)	O
by	O
verifying	O
the	O
steps	O
given	O
in	O
the	O
text	O
.	O
exercises	O
519	O
10.13	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
starting	O
from	O
(	O
10.54	O
)	O
,	O
derive	O
the	O
result	O
(	O
10.59	O
)	O
for	O
the	O
optimum	O
vari-	O
ational	O
posterior	O
distribution	O
over	O
µk	O
and	O
λk	O
in	O
the	O
bayesian	O
mixture	O
of	O
gaussians	O
,	O
and	O
hence	O
verify	O
the	O
expressions	O
for	O
the	O
parameters	O
of	O
this	O
distribution	O
given	O
by	O
(	O
10.60	O
)	O
–	O
(	O
10.63	O
)	O
.	O
10.14	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
using	O
the	O
distribution	O
(	O
10.59	O
)	O
,	O
verify	O
the	O
result	O
(	O
10.64	O
)	O
.	O
10.15	O
(	O
(	O
cid:12	O
)	O
)	O
using	O
the	O
result	O
(	O
b.17	O
)	O
,	O
show	O
that	O
the	O
expected	O
value	O
of	O
the	O
mixing	O
coefﬁcients	O
in	O
the	O
variational	B
mixture	O
of	O
gaussians	O
is	O
given	O
by	O
(	O
10.69	O
)	O
.	O
10.16	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
verify	O
the	O
results	O
(	O
10.71	O
)	O
and	O
(	O
10.72	O
)	O
for	O
the	O
ﬁrst	O
two	O
terms	O
in	O
the	O
lower	B
bound	I
for	O
the	O
variational	B
gaussian	O
mixture	B
model	I
given	O
by	O
(	O
10.70	O
)	O
.	O
10.17	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
verify	O
the	O
results	O
(	O
10.73	O
)	O
–	O
(	O
10.77	O
)	O
for	O
the	O
remaining	O
terms	O
in	O
the	O
lower	B
bound	I
for	O
the	O
variational	B
gaussian	O
mixture	B
model	I
given	O
by	O
(	O
10.70	O
)	O
.	O
10.18	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
in	O
this	O
exercise	O
,	O
we	O
shall	O
derive	O
the	O
variational	B
re-estimation	O
equations	O
for	O
the	O
gaussian	O
mixture	B
model	I
by	O
direct	O
differentiation	O
of	O
the	O
lower	B
bound	I
.	O
to	O
do	O
this	O
we	O
assume	O
that	O
the	O
variational	B
distribution	O
has	O
the	O
factorization	B
deﬁned	O
by	O
(	O
10.42	O
)	O
and	O
(	O
10.55	O
)	O
with	O
factors	O
given	O
by	O
(	O
10.48	O
)	O
,	O
(	O
10.57	O
)	O
,	O
and	O
(	O
10.59	O
)	O
.	O
substitute	O
these	O
into	O
(	O
10.70	O
)	O
and	O
hence	O
obtain	O
the	O
lower	B
bound	I
as	O
a	O
function	O
of	O
the	O
parameters	O
of	O
the	O
varia-	O
tional	O
distribution	O
.	O
then	O
,	O
by	O
maximizing	O
the	O
bound	O
with	O
respect	O
to	O
these	O
parameters	O
,	O
derive	O
the	O
re-estimation	O
equations	O
for	O
the	O
factors	O
in	O
the	O
variational	B
distribution	O
,	O
and	O
show	O
that	O
these	O
are	O
the	O
same	O
as	O
those	O
obtained	O
in	O
section	O
10.2.1	O
.	O
10.19	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
derive	O
the	O
result	O
(	O
10.81	O
)	O
for	O
the	O
predictive	B
distribution	I
in	O
the	O
variational	B
treat-	O
ment	O
of	O
the	O
bayesian	O
mixture	O
of	O
gaussians	O
model	O
.	O
10.20	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
this	O
exercise	O
explores	O
the	O
variational	B
bayes	O
solution	O
for	O
the	O
mixture	O
of	O
gaussians	O
model	O
when	O
the	O
size	O
n	O
of	O
the	O
data	O
set	O
is	O
large	O
and	O
shows	O
that	O
it	O
reduces	O
(	O
as	O
we	O
would	O
expect	O
)	O
to	O
the	O
maximum	B
likelihood	I
solution	O
based	O
on	O
em	O
derived	O
in	O
chap-	O
ter	O
9.	O
note	O
that	O
results	O
from	O
appendix	O
b	O
may	O
be	O
used	O
to	O
help	O
answer	O
this	O
exercise	O
.	O
first	O
show	O
that	O
the	O
posterior	O
distribution	O
q	O
(	O
cid:1	O
)	O
(	O
λk	O
)	O
of	O
the	O
precisions	O
becomes	O
sharply	O
peaked	O
around	O
the	O
maximum	B
likelihood	I
solution	O
.	O
do	O
the	O
same	O
for	O
the	O
posterior	O
dis-	O
tribution	O
of	O
the	O
means	O
q	O
(	O
cid:1	O
)	O
(	O
µk|λk	O
)	O
.	O
next	O
consider	O
the	O
posterior	O
distribution	O
q	O
(	O
cid:1	O
)	O
(	O
π	O
)	O
for	O
the	O
mixing	O
coefﬁcients	O
and	O
show	O
that	O
this	O
too	O
becomes	O
sharply	O
peaked	O
around	O
the	O
maximum	B
likelihood	I
solution	O
.	O
similarly	O
,	O
show	O
that	O
the	O
responsibilities	O
become	O
equal	O
to	O
the	O
corresponding	O
maximum	B
likelihood	I
values	O
for	O
large	O
n	O
,	O
by	O
making	O
use	O
of	O
the	O
following	O
asymptotic	O
result	O
for	O
the	O
digamma	B
function	I
for	O
large	O
x	O
ψ	O
(	O
x	O
)	O
=	O
ln	O
x	O
+	O
o	O
(	O
1/x	O
)	O
.	O
(	O
10.241	O
)	O
finally	O
,	O
by	O
making	O
use	O
of	O
(	O
10.80	O
)	O
,	O
show	O
that	O
for	O
large	O
n	O
,	O
the	O
predictive	B
distribution	I
becomes	O
a	O
mixture	O
of	O
gaussians	O
.	O
10.21	O
(	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
the	O
number	O
of	O
equivalent	O
parameter	O
settings	O
due	O
to	O
interchange	O
sym-	O
metries	O
in	O
a	O
mixture	B
model	I
with	O
k	O
components	O
is	O
k	O
!	O
.	O
520	O
10.	O
approximate	O
inference	B
10.22	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
we	O
have	O
seen	O
that	O
each	O
mode	O
of	O
the	O
posterior	O
distribution	O
in	O
a	O
gaussian	O
mix-	O
ture	O
model	O
is	O
a	O
member	O
of	O
a	O
family	O
of	O
k	O
!	O
equivalent	O
modes	O
.	O
suppose	O
that	O
the	O
result	O
of	O
running	O
the	O
variational	B
inference	I
algorithm	O
is	O
an	O
approximate	O
posterior	O
distribu-	O
tion	O
q	O
that	O
is	O
localized	O
in	O
the	O
neighbourhood	O
of	O
one	O
of	O
the	O
modes	O
.	O
we	O
can	O
then	O
approximate	O
the	O
full	O
posterior	O
distribution	O
as	O
a	O
mixture	O
of	O
k	O
!	O
such	O
q	O
distributions	O
,	O
once	O
centred	O
on	O
each	O
mode	O
and	O
having	O
equal	O
mixing	O
coefﬁcients	O
.	O
show	O
that	O
if	O
we	O
assume	O
negligible	O
overlap	O
between	O
the	O
components	O
of	O
the	O
q	O
mixture	B
,	O
the	O
resulting	O
lower	B
bound	I
differs	O
from	O
that	O
for	O
a	O
single	O
component	O
q	O
distribution	O
through	O
the	O
ad-	O
dition	O
of	O
an	O
extra	O
term	O
ln	O
k	O
!	O
.	O
10.23	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
variational	B
gaussian	O
mixture	B
model	I
in	O
which	O
there	O
is	O
no	O
prior	B
distribution	O
over	O
mixing	O
coefﬁcients	O
{	O
πk	O
}	O
.	O
instead	O
,	O
the	O
mixing	O
coefﬁcients	O
are	O
treated	O
as	O
parameters	O
,	O
whose	O
values	O
are	O
to	O
be	O
found	O
by	O
maximizing	O
the	O
variational	B
lower	O
bound	O
on	O
the	O
log	O
marginal	O
likelihood	O
.	O
show	O
that	O
maximizing	O
this	O
lower	B
bound	I
with	O
respect	O
to	O
the	O
mixing	O
coefﬁcients	O
,	O
using	O
a	O
lagrange	O
multiplier	O
to	O
enforce	O
the	O
constraint	O
that	O
the	O
mixing	O
coefﬁcients	O
sum	O
to	O
one	O
,	O
leads	O
to	O
the	O
re-estimation	O
result	O
(	O
10.83	O
)	O
.	O
note	O
that	O
there	O
is	O
no	O
need	O
to	O
consider	O
all	O
of	O
the	O
terms	O
in	O
the	O
lower	B
bound	I
but	O
only	O
the	O
dependence	O
of	O
the	O
bound	O
on	O
the	O
{	O
πk	O
}	O
.	O
10.24	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
we	O
have	O
seen	O
in	O
section	O
10.2	O
that	O
the	O
singularities	B
arising	O
in	O
the	O
max-	O
imum	O
likelihood	O
treatment	O
of	O
gaussian	O
mixture	B
models	O
do	O
not	O
arise	O
in	O
a	O
bayesian	O
treatment	O
.	O
discuss	O
whether	O
such	O
singularities	B
would	O
arise	O
if	O
the	O
bayesian	O
model	O
were	O
solved	O
using	O
maximum	B
posterior	I
(	O
map	O
)	O
estimation	O
.	O
10.25	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
the	O
variational	B
treatment	O
of	O
the	O
bayesian	O
mixture	O
of	O
gaussians	O
,	O
discussed	O
in	O
section	O
10.2	O
,	O
made	O
use	O
of	O
a	O
factorized	O
approximation	O
(	O
10.5	O
)	O
to	O
the	O
posterior	O
distribu-	O
tion	O
.	O
as	O
we	O
saw	O
in	O
figure	O
10.2	O
,	O
the	O
factorized	O
assumption	O
causes	O
the	O
variance	B
of	O
the	O
posterior	O
distribution	O
to	O
be	O
under-estimated	O
for	O
certain	O
directions	O
in	O
parameter	O
space	O
.	O
discuss	O
qualitatively	O
the	O
effect	O
this	O
will	O
have	O
on	O
the	O
variational	B
approximation	O
to	O
the	O
model	B
evidence	I
,	O
and	O
how	O
this	O
effect	O
will	O
vary	O
with	O
the	O
number	O
of	O
components	O
in	O
the	O
mixture	B
.	O
hence	O
explain	O
whether	O
the	O
variational	B
gaussian	O
mixture	B
will	O
tend	O
to	O
under-estimate	O
or	O
over-estimate	O
the	O
optimal	O
number	O
of	O
components	O
.	O
10.26	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
extend	O
the	O
variational	B
treatment	O
of	O
bayesian	O
linear	B
regression	I
to	O
include	O
a	O
gamma	O
hyperprior	O
gam	O
(	O
β|c0	O
,	O
d0	O
)	O
over	O
β	O
and	O
solve	O
variationally	O
,	O
by	O
assuming	O
a	O
factorized	O
variational	O
distribution	O
of	O
the	O
form	O
q	O
(	O
w	O
)	O
q	O
(	O
α	O
)	O
q	O
(	O
β	O
)	O
.	O
derive	O
the	O
variational	B
update	O
equations	O
for	O
the	O
three	O
factors	O
in	O
the	O
variational	B
distribution	O
and	O
also	O
obtain	O
an	O
expression	O
for	O
the	O
lower	B
bound	I
and	O
for	O
the	O
predictive	B
distribution	I
.	O
10.27	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
by	O
making	O
use	O
of	O
the	O
formulae	O
given	O
in	O
appendix	O
b	O
show	O
that	O
the	O
variational	B
lower	O
bound	O
for	O
the	O
linear	O
basis	O
function	O
regression	O
model	O
,	O
deﬁned	O
by	O
(	O
10.107	O
)	O
,	O
can	O
be	O
written	O
in	O
the	O
form	O
(	O
10.107	O
)	O
with	O
the	O
various	O
terms	O
deﬁned	O
by	O
(	O
10.108	O
)	O
–	O
(	O
10.112	O
)	O
.	O
10.28	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
rewrite	O
the	O
model	O
for	O
the	O
bayesian	O
mixture	O
of	O
gaussians	O
,	O
introduced	O
in	O
section	O
10.2	O
,	O
as	O
a	O
conjugate	B
model	O
from	O
the	O
exponential	B
family	I
,	O
as	O
discussed	O
in	O
section	O
10.4.	O
hence	O
use	O
the	O
general	O
results	O
(	O
10.115	O
)	O
and	O
(	O
10.119	O
)	O
to	O
derive	O
the	O
speciﬁc	O
results	O
(	O
10.48	O
)	O
,	O
(	O
10.57	O
)	O
,	O
and	O
(	O
10.59	O
)	O
.	O
exercises	O
521	O
10.29	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
the	O
function	O
f	O
(	O
x	O
)	O
=	O
ln	O
(	O
x	O
)	O
is	O
concave	O
for	O
0	O
<	O
x	O
<	O
∞	O
by	O
computing	O
its	O
second	O
derivative	O
.	O
determine	O
the	O
form	O
of	O
the	O
dual	O
function	O
g	O
(	O
λ	O
)	O
deﬁned	O
by	O
(	O
10.133	O
)	O
,	O
and	O
verify	O
that	O
minimization	O
of	O
λx	O
−	O
g	O
(	O
λ	O
)	O
with	O
respect	O
to	O
λ	O
according	O
to	O
(	O
10.132	O
)	O
indeed	O
recovers	O
the	O
function	O
ln	O
(	O
x	O
)	O
.	O
10.30	O
(	O
(	O
cid:12	O
)	O
)	O
by	O
evaluating	O
the	O
second	O
derivative	O
,	O
show	O
that	O
the	O
log	O
logistic	O
function	O
f	O
(	O
x	O
)	O
=	O
−	O
ln	O
(	O
1	O
+	O
e	O
−x	O
)	O
is	O
concave	O
.	O
derive	O
the	O
variational	B
upper	O
bound	O
(	O
10.137	O
)	O
directly	O
by	O
making	O
a	O
second	B
order	I
taylor	O
expansion	O
of	O
the	O
log	O
logistic	O
function	O
around	O
a	O
point	O
x	O
=	O
ξ	O
.	O
10.31	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
by	O
ﬁnding	O
the	O
second	O
derivative	O
with	O
respect	O
to	O
x	O
,	O
show	O
that	O
the	O
function	O
f	O
(	O
x	O
)	O
=	O
−	O
ln	O
(	O
ex/2	O
+	O
e	O
−x/2	O
)	O
is	O
a	O
concave	B
function	I
of	O
x.	O
now	O
consider	O
the	O
second	O
derivatives	O
with	O
respect	O
to	O
the	O
variable	O
x2	O
and	O
hence	O
show	O
that	O
it	O
is	O
a	O
convex	B
function	I
of	O
x2	O
.	O
plot	O
graphs	O
of	O
f	O
(	O
x	O
)	O
against	O
x	O
and	O
against	O
x2	O
.	O
derive	O
the	O
lower	B
bound	I
(	O
10.144	O
)	O
on	O
the	O
logistic	B
sigmoid	I
function	O
directly	O
by	O
making	O
a	O
ﬁrst	B
order	I
taylor	O
series	O
expan-	O
sion	B
of	O
the	O
function	O
f	O
(	O
x	O
)	O
in	O
the	O
variable	O
x2	O
centred	O
on	O
the	O
value	O
ξ2	O
.	O
10.32	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
the	O
variational	B
treatment	O
of	O
logistic	B
regression	I
with	O
sequen-	O
tial	O
learning	B
in	O
which	O
data	O
points	O
are	O
arriving	O
one	O
at	O
a	O
time	O
and	O
each	O
must	O
be	O
pro-	O
cessed	O
and	O
discarded	O
before	O
the	O
next	O
data	O
point	O
arrives	O
.	O
show	O
that	O
a	O
gaussian	O
ap-	O
proximation	B
to	O
the	O
posterior	O
distribution	O
can	O
be	O
maintained	O
through	O
the	O
use	O
of	O
the	O
lower	B
bound	I
(	O
10.151	O
)	O
,	O
in	O
which	O
the	O
distribution	O
is	O
initialized	O
using	O
the	O
prior	B
,	O
and	O
as	O
each	O
data	O
point	O
is	O
absorbed	O
its	O
corresponding	O
variational	B
parameter	O
ξn	O
is	O
optimized	O
.	O
10.33	O
(	O
(	O
cid:12	O
)	O
)	O
by	O
differentiating	O
the	O
quantity	O
q	O
(	O
ξ	O
,	O
ξold	O
)	O
deﬁned	O
by	O
(	O
10.161	O
)	O
with	O
respect	O
to	O
the	O
variational	B
parameter	O
ξn	O
show	O
that	O
the	O
update	O
equation	O
for	O
ξn	O
for	O
the	O
bayesian	O
logistic	B
regression	I
model	O
is	O
given	O
by	O
(	O
10.163	O
)	O
.	O
10.34	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
in	O
this	O
exercise	O
we	O
derive	O
re-estimation	O
equations	O
for	O
the	O
variational	B
parame-	O
ters	O
ξ	O
in	O
the	O
bayesian	O
logistic	B
regression	I
model	O
of	O
section	O
4.5	O
by	O
direct	O
maximization	O
of	O
the	O
lower	B
bound	I
given	O
by	O
(	O
10.164	O
)	O
.	O
to	O
do	O
this	O
set	O
the	O
derivative	B
of	O
l	O
(	O
ξ	O
)	O
with	O
re-	O
spect	O
to	O
ξn	O
equal	O
to	O
zero	O
,	O
making	O
use	O
of	O
the	O
result	O
(	O
3.117	O
)	O
for	O
the	O
derivative	B
of	O
the	O
log	O
of	O
a	O
determinant	O
,	O
together	O
with	O
the	O
expressions	O
(	O
10.157	O
)	O
and	O
(	O
10.158	O
)	O
which	O
deﬁne	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
variational	B
posterior	O
distribution	O
q	O
(	O
w	O
)	O
.	O
10.35	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
derive	O
the	O
result	O
(	O
10.164	O
)	O
for	O
the	O
lower	B
bound	I
l	O
(	O
ξ	O
)	O
in	O
the	O
variational	B
logistic	O
regression	B
model	O
.	O
this	O
is	O
most	O
easily	O
done	O
by	O
substituting	O
the	O
expressions	O
for	O
the	O
gaussian	O
prior	B
q	O
(	O
w	O
)	O
=	O
n	O
(	O
w|m0	O
,	O
s0	O
)	O
,	O
together	O
with	O
the	O
lower	B
bound	I
h	O
(	O
w	O
,	O
ξ	O
)	O
on	O
the	O
likelihood	B
function	I
,	O
into	O
the	O
integral	O
(	O
10.159	O
)	O
which	O
deﬁnes	O
l	O
(	O
ξ	O
)	O
.	O
next	O
gather	O
together	O
the	O
terms	O
which	O
depend	O
on	O
w	O
in	O
the	O
exponential	O
and	O
complete	O
the	O
square	O
to	O
give	O
a	O
gaussian	O
integral	O
,	O
which	O
can	O
then	O
be	O
evaluated	O
by	O
invoking	O
the	O
standard	O
result	O
for	O
the	O
normalization	O
coefﬁcient	O
of	O
a	O
multivariate	O
gaussian	O
.	O
finally	O
take	O
the	O
logarithm	O
to	O
obtain	O
(	O
10.164	O
)	O
.	O
10.36	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
the	O
adf	O
approximation	O
scheme	O
discussed	O
in	O
section	O
10.7	O
,	O
and	O
show	O
that	O
inclusion	O
of	O
the	O
factor	O
fj	O
(	O
θ	O
)	O
leads	O
to	O
an	O
update	O
of	O
the	O
model	B
evidence	I
of	O
the	O
form	O
pj	O
(	O
d	O
)	O
(	O
cid:7	O
)	O
pj−1	O
(	O
d	O
)	O
zj	O
(	O
10.242	O
)	O
522	O
10.	O
approximate	O
inference	B
where	O
zj	O
is	O
the	O
normalization	O
constant	O
deﬁned	O
by	O
(	O
10.197	O
)	O
.	O
by	O
applying	O
this	O
result	O
recursively	O
,	O
and	O
initializing	O
with	O
p0	O
(	O
d	O
)	O
=	O
1	O
,	O
derive	O
the	O
result	O
zj	O
.	O
j	O
(	O
10.243	O
)	O
(	O
cid:14	O
)	O
p	O
(	O
d	O
)	O
(	O
cid:7	O
)	O
10.37	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
the	O
expectation	B
propagation	I
algorithm	O
from	O
section	O
10.7	O
,	O
and	O
suppose	O
that	O
one	O
of	O
the	O
factors	O
f0	O
(	O
θ	O
)	O
in	O
the	O
deﬁnition	O
(	O
10.188	O
)	O
has	O
the	O
same	O
expo-	O
nential	O
family	O
functional	B
form	O
as	O
the	O
approximating	O
distribution	O
q	O
(	O
θ	O
)	O
.	O
show	O
that	O
if	O
the	O
factor	O
(	O
cid:4	O
)	O
f0	O
(	O
θ	O
)	O
is	O
initialized	O
to	O
be	O
f0	O
(	O
θ	O
)	O
,	O
then	O
an	O
ep	O
update	O
to	O
reﬁne	O
(	O
cid:4	O
)	O
f0	O
(	O
θ	O
)	O
leaves	O
(	O
cid:4	O
)	O
f0	O
(	O
θ	O
)	O
unchanged	O
.	O
this	O
situation	O
typically	O
arises	O
when	O
one	O
of	O
the	O
factors	O
is	O
the	O
prior	B
p	O
(	O
θ	O
)	O
,	O
and	O
so	O
we	O
see	O
that	O
the	O
prior	B
factor	O
can	O
be	O
incorporated	O
once	O
exactly	O
and	O
does	O
not	O
need	O
to	O
be	O
reﬁned	O
.	O
10.38	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
in	O
this	O
exercise	O
and	O
the	O
next	O
,	O
we	O
shall	O
verify	O
the	O
results	O
(	O
10.214	O
)	O
–	O
(	O
10.224	O
)	O
for	O
the	O
expectation	B
propagation	I
algorithm	O
applied	O
to	O
the	O
clutter	B
problem	I
.	O
begin	O
by	O
using	O
the	O
division	O
formula	O
(	O
10.205	O
)	O
to	O
derive	O
the	O
expressions	O
(	O
10.214	O
)	O
and	O
(	O
10.215	O
)	O
by	O
completing	B
the	I
square	I
inside	O
the	O
exponential	O
to	O
identify	O
the	O
mean	B
and	O
variance	B
.	O
also	O
,	O
show	O
that	O
the	O
normalization	O
constant	O
zn	O
,	O
deﬁned	O
by	O
(	O
10.206	O
)	O
,	O
is	O
given	O
for	O
the	O
clutter	B
problem	I
by	O
(	O
10.216	O
)	O
.	O
this	O
can	O
be	O
done	O
by	O
making	O
use	O
of	O
the	O
general	O
result	O
(	O
2.115	O
)	O
.	O
10.39	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
the	O
mean	B
and	O
variance	B
of	O
qnew	O
(	O
θ	O
)	O
for	O
ep	O
applied	O
to	O
the	O
clutter	B
problem	I
are	O
given	O
by	O
(	O
10.217	O
)	O
and	O
(	O
10.218	O
)	O
.	O
to	O
do	O
this	O
,	O
ﬁrst	O
prove	O
the	O
following	O
results	O
for	O
the	O
expectations	O
of	O
θ	O
and	O
θθt	O
under	O
qnew	O
(	O
θ	O
)	O
e	O
[	O
θ	O
]	O
=	O
m\n	O
+	O
v	O
\n∇m\n	O
ln	O
zn	O
e	O
[	O
θtθ	O
]	O
=	O
2	O
(	O
v	O
\n	O
)	O
2∇v\n	O
ln	O
zn	O
+	O
2e	O
[	O
θ	O
]	O
tm\n	O
−	O
(	O
cid:5	O
)	O
m\n	O
(	O
cid:5	O
)	O
2	O
(	O
10.244	O
)	O
(	O
10.245	O
)	O
and	O
then	O
make	O
use	O
of	O
the	O
result	O
(	O
10.216	O
)	O
for	O
zn	O
.	O
next	O
,	O
prove	O
the	O
results	O
(	O
10.220	O
)	O
–	O
(	O
10.222	O
)	O
by	O
using	O
(	O
10.207	O
)	O
and	O
completing	B
the	I
square	I
in	O
the	O
exponential	O
.	O
finally	O
,	O
use	O
(	O
10.208	O
)	O
to	O
derive	O
the	O
result	O
(	O
10.223	O
)	O
.	O
11	O
sampling	B
methods	I
for	O
most	O
probabilistic	O
models	O
of	O
practical	O
interest	O
,	O
exact	O
inference	O
is	O
intractable	O
,	O
and	O
so	O
we	O
have	O
to	O
resort	O
to	O
some	O
form	O
of	O
approximation	O
.	O
in	O
chapter	O
10	O
,	O
we	O
discussed	O
inference	B
algorithms	O
based	O
on	O
deterministic	O
approximations	O
,	O
which	O
include	O
methods	O
such	O
as	O
variational	B
bayes	O
and	O
expectation	B
propagation	I
.	O
here	O
we	O
consider	O
approxi-	O
mate	O
inference	B
methods	O
based	O
on	O
numerical	O
sampling	O
,	O
also	O
known	O
as	O
monte	O
carlo	O
techniques	O
.	O
although	O
for	O
some	O
applications	O
the	O
posterior	O
distribution	O
over	O
unobserved	O
vari-	O
ables	O
will	O
be	O
of	O
direct	O
interest	O
in	O
itself	O
,	O
for	O
most	O
situations	O
the	O
posterior	O
distribution	O
is	O
required	O
primarily	O
for	O
the	O
purpose	O
of	O
evaluating	O
expectations	O
,	O
for	O
example	O
in	O
order	O
to	O
make	O
predictions	O
.	O
the	O
fundamental	O
problem	O
that	O
we	O
therefore	O
wish	O
to	O
address	O
in	O
this	O
chapter	O
involves	O
ﬁnding	O
the	O
expectation	B
of	O
some	O
function	O
f	O
(	O
z	O
)	O
with	O
respect	O
to	O
a	O
probability	B
distribution	O
p	O
(	O
z	O
)	O
.	O
here	O
,	O
the	O
components	O
of	O
z	O
might	O
comprise	O
discrete	O
or	O
continuous	O
variables	O
or	O
some	O
combination	O
of	O
the	O
two	O
.	O
thus	O
in	O
the	O
case	O
of	O
continuous	O
523	O
524	O
11.	O
sampling	B
methods	I
figure	O
11.1	O
schematic	O
illustration	O
of	O
a	O
function	O
f	O
(	O
z	O
)	O
whose	O
expectation	B
is	O
to	O
be	O
evaluated	O
with	O
respect	O
to	O
a	O
distribution	O
p	O
(	O
z	O
)	O
.	O
p	O
(	O
z	O
)	O
f	O
(	O
z	O
)	O
z	O
variables	O
,	O
we	O
wish	O
to	O
evaluate	O
the	O
expectation	B
(	O
cid:6	O
)	O
e	O
[	O
f	O
]	O
=	O
f	O
(	O
z	O
)	O
p	O
(	O
z	O
)	O
dz	O
(	O
11.1	O
)	O
where	O
the	O
integral	O
is	O
replaced	O
by	O
summation	O
in	O
the	O
case	O
of	O
discrete	O
variables	O
.	O
this	O
is	O
illustrated	O
schematically	O
for	O
a	O
single	O
continuous	O
variable	O
in	O
figure	O
11.1.	O
we	O
shall	O
suppose	O
that	O
such	O
expectations	O
are	O
too	O
complex	O
to	O
be	O
evaluated	O
exactly	O
using	O
analyt-	O
ical	O
techniques	O
.	O
the	O
general	O
idea	O
behind	O
sampling	B
methods	I
is	O
to	O
obtain	O
a	O
set	O
of	O
samples	O
z	O
(	O
l	O
)	O
(	O
where	O
l	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
l	O
)	O
drawn	O
independently	O
from	O
the	O
distribution	O
p	O
(	O
z	O
)	O
.	O
this	O
allows	O
the	O
expectation	B
(	O
11.1	O
)	O
to	O
be	O
approximated	O
by	O
a	O
ﬁnite	O
sum	O
as	O
long	O
as	O
the	O
samples	O
z	O
(	O
l	O
)	O
are	O
drawn	O
from	O
the	O
distribution	O
p	O
(	O
z	O
)	O
,	O
then	O
e	O
[	O
(	O
cid:1	O
)	O
f	O
]	O
=	O
e	O
[	O
f	O
]	O
and	O
so	O
the	O
estimator	O
(	O
cid:1	O
)	O
f	O
has	O
the	O
correct	O
mean	B
.	O
the	O
variance	B
of	O
the	O
estimator	O
is	O
given	O
l=1	O
(	O
11.2	O
)	O
f	O
(	O
z	O
(	O
l	O
)	O
)	O
.	O
l	O
(	O
cid:2	O
)	O
(	O
cid:1	O
)	O
f	O
=	O
1	O
l	O
var	O
[	O
(	O
cid:1	O
)	O
f	O
]	O
=	O
(	O
cid:8	O
)	O
exercise	O
11.1	O
by	O
(	O
cid:9	O
)	O
(	O
f	O
−	O
e	O
[	O
f	O
]	O
)	O
2	O
1	O
l	O
e	O
(	O
11.3	O
)	O
is	O
the	O
variance	B
of	O
the	O
function	O
f	O
(	O
z	O
)	O
under	O
the	O
distribution	O
p	O
(	O
z	O
)	O
.	O
it	O
is	O
worth	O
emphasiz-	O
ing	O
that	O
the	O
accuracy	O
of	O
the	O
estimator	O
therefore	O
does	O
not	O
depend	O
on	O
the	O
dimension-	O
ality	O
of	O
z	O
,	O
and	O
that	O
,	O
in	O
principle	O
,	O
high	O
accuracy	O
may	O
be	O
achievable	O
with	O
a	O
relatively	O
small	O
number	O
of	O
samples	O
z	O
(	O
l	O
)	O
.	O
in	O
practice	O
,	O
ten	O
or	O
twenty	O
independent	B
samples	O
may	O
sufﬁce	O
to	O
estimate	O
an	O
expectation	B
to	O
sufﬁcient	O
accuracy	O
.	O
the	O
problem	O
,	O
however	O
,	O
is	O
that	O
the	O
samples	O
{	O
z	O
(	O
l	O
)	O
}	O
might	O
not	O
be	O
independent	B
,	O
and	O
so	O
the	O
effective	O
sample	O
size	O
might	O
be	O
much	O
smaller	O
than	O
the	O
apparent	O
sample	O
size	O
.	O
also	O
,	O
referring	O
back	O
to	O
figure	O
11.1	O
,	O
we	O
note	O
that	O
if	O
f	O
(	O
z	O
)	O
is	O
small	O
in	O
regions	O
where	O
p	O
(	O
z	O
)	O
is	O
large	O
,	O
and	O
vice	O
versa	O
,	O
then	O
the	O
expectation	B
may	O
be	O
dominated	O
by	O
regions	O
of	O
small	O
probability	B
,	O
implying	O
that	O
relatively	O
large	O
sample	O
sizes	O
will	O
be	O
required	O
to	O
achieve	O
sufﬁcient	O
accuracy	O
.	O
for	O
many	O
models	O
,	O
the	O
joint	O
distribution	O
p	O
(	O
z	O
)	O
is	O
conveniently	O
speciﬁed	O
in	O
terms	O
of	O
a	O
graphical	B
model	I
.	O
in	O
the	O
case	O
of	O
a	O
directed	B
graph	O
with	O
no	O
observed	O
variables	O
,	O
it	O
is	O
11.	O
sampling	B
methods	I
525	O
straightforward	O
to	O
sample	O
from	O
the	O
joint	O
distribution	O
(	O
assuming	O
that	O
it	O
is	O
possible	O
to	O
sample	O
from	O
the	O
conditional	B
distributions	O
at	O
each	O
node	B
)	O
using	O
the	O
following	O
ances-	O
tral	O
sampling	O
approach	O
,	O
discussed	O
brieﬂy	O
in	O
section	O
8.1.2.	O
the	O
joint	O
distribution	O
is	O
speciﬁed	O
by	O
m	O
(	O
cid:14	O
)	O
p	O
(	O
z	O
)	O
=	O
p	O
(	O
zi|pai	O
)	O
(	O
11.4	O
)	O
i=1	O
where	O
zi	O
are	O
the	O
set	O
of	O
variables	O
associated	O
with	O
node	B
i	O
,	O
and	O
pai	O
denotes	O
the	O
set	O
of	O
variables	O
associated	O
with	O
the	O
parents	O
of	O
node	B
i.	O
to	O
obtain	O
a	O
sample	O
from	O
the	O
joint	O
distribution	O
,	O
we	O
make	O
one	O
pass	O
through	O
the	O
set	O
of	O
variables	O
in	O
the	O
order	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
sampling	O
from	O
the	O
conditional	B
distributions	O
p	O
(	O
zi|pai	O
)	O
.	O
this	O
is	O
always	O
possible	O
be-	O
cause	O
at	O
each	O
step	O
all	O
of	O
the	O
parent	O
values	O
will	O
have	O
been	O
instantiated	O
.	O
after	O
one	O
pass	O
through	O
the	O
graph	O
,	O
we	O
will	O
have	O
obtained	O
a	O
sample	O
from	O
the	O
joint	O
distribution	O
.	O
now	O
consider	O
the	O
case	O
of	O
a	O
directed	B
graph	O
in	O
which	O
some	O
of	O
the	O
nodes	O
are	O
in-	O
stantiated	O
with	O
observed	O
values	O
.	O
we	O
can	O
in	O
principle	O
extend	O
the	O
above	O
procedure	O
,	O
at	O
least	O
in	O
the	O
case	O
of	O
nodes	O
representing	O
discrete	O
variables	O
,	O
to	O
give	O
the	O
following	O
logic	B
sampling	I
approach	O
(	O
henrion	O
,	O
1988	O
)	O
,	O
which	O
can	O
be	O
seen	O
as	O
a	O
special	O
case	O
of	O
impor-	O
tance	O
sampling	O
discussed	O
in	O
section	O
11.1.4.	O
at	O
each	O
step	O
,	O
when	O
a	O
sampled	O
value	O
is	O
obtained	O
for	O
a	O
variable	O
zi	O
whose	O
value	O
is	O
observed	O
,	O
the	O
sampled	O
value	O
is	O
compared	O
to	O
the	O
observed	O
value	O
,	O
and	O
if	O
they	O
agree	O
then	O
the	O
sample	O
value	O
is	O
retained	O
and	O
the	O
al-	O
gorithm	O
proceeds	O
to	O
the	O
next	O
variable	O
in	O
turn	O
.	O
however	O
,	O
if	O
the	O
sampled	O
value	O
and	O
the	O
observed	O
value	O
disagree	O
,	O
then	O
the	O
whole	O
sample	O
so	O
far	O
is	O
discarded	O
and	O
the	O
algorithm	O
starts	O
again	O
with	O
the	O
ﬁrst	O
node	O
in	O
the	O
graph	O
.	O
this	O
algorithm	O
samples	O
correctly	O
from	O
the	O
posterior	O
distribution	O
because	O
it	O
corresponds	O
simply	O
to	O
drawing	O
samples	O
from	O
the	O
joint	O
distribution	O
of	O
hidden	O
variables	O
and	O
data	O
variables	O
and	O
then	O
discarding	O
those	O
samples	O
that	O
disagree	O
with	O
the	O
observed	O
data	O
(	O
with	O
the	O
slight	O
saving	O
of	O
not	O
continu-	O
ing	O
with	O
the	O
sampling	O
from	O
the	O
joint	O
distribution	O
as	O
soon	O
as	O
one	O
contradictory	O
value	O
is	O
observed	O
)	O
.	O
however	O
,	O
the	O
overall	O
probability	B
of	O
accepting	O
a	O
sample	O
from	O
the	O
posterior	O
decreases	O
rapidly	O
as	O
the	O
number	O
of	O
observed	O
variables	O
increases	O
and	O
as	O
the	O
number	O
of	O
states	O
that	O
those	O
variables	O
can	O
take	O
increases	O
,	O
and	O
so	O
this	O
approach	O
is	O
rarely	O
used	O
in	O
practice	O
.	O
in	O
the	O
case	O
of	O
probability	B
distributions	O
deﬁned	O
by	O
an	O
undirected	B
graph	I
,	O
there	O
is	O
no	O
one-pass	O
sampling	O
strategy	O
that	O
will	O
sample	O
even	O
from	O
the	O
prior	B
distribution	O
with	O
no	O
observed	O
variables	O
.	O
instead	O
,	O
computationally	O
more	O
expensive	O
techniques	O
must	O
be	O
employed	O
,	O
such	O
as	O
gibbs	O
sampling	O
,	O
which	O
is	O
discussed	O
in	O
section	O
11.3.	O
as	O
well	O
as	O
sampling	O
from	O
conditional	B
distributions	O
,	O
we	O
may	O
also	O
require	O
samples	O
from	O
a	O
marginal	B
distribution	O
.	O
if	O
we	O
already	O
have	O
a	O
strategy	O
for	O
sampling	O
from	O
a	O
joint	O
distribution	O
p	O
(	O
u	O
,	O
v	O
)	O
,	O
then	O
it	O
is	O
straightforward	O
to	O
obtain	O
samples	O
from	O
the	O
marginal	B
distribution	O
p	O
(	O
u	O
)	O
simply	O
by	O
ignoring	O
the	O
values	O
for	O
v	O
in	O
each	O
sample	O
.	O
there	O
are	O
numerous	O
texts	O
dealing	O
with	O
monte	O
carlo	O
methods	O
.	O
those	O
of	O
partic-	O
ular	O
interest	O
from	O
the	O
statistical	O
inference	O
perspective	O
include	O
chen	O
et	O
al	O
.	O
(	O
2001	O
)	O
,	O
gamerman	O
(	O
1997	O
)	O
,	O
gilks	O
et	O
al	O
.	O
(	O
1996	O
)	O
,	O
liu	O
(	O
2001	O
)	O
,	O
neal	O
(	O
1996	O
)	O
,	O
and	O
robert	O
and	O
casella	O
(	O
1999	O
)	O
.	O
also	O
there	O
are	O
review	O
articles	O
by	O
besag	O
et	O
al	O
.	O
(	O
1995	O
)	O
,	O
brooks	O
(	O
1998	O
)	O
,	O
diaconis	O
and	O
saloff-coste	O
(	O
1998	O
)	O
,	O
jerrum	O
and	O
sinclair	O
(	O
1996	O
)	O
,	O
neal	O
(	O
1993	O
)	O
,	O
tierney	O
(	O
1994	O
)	O
,	O
and	O
andrieu	O
et	O
al	O
.	O
(	O
2003	O
)	O
that	O
provide	O
additional	O
information	O
on	O
sampling	O
526	O
11.	O
sampling	B
methods	I
methods	O
for	O
statistical	O
inference	B
.	O
diagnostic	O
tests	O
for	O
convergence	O
of	O
markov	O
chain	O
monte	O
carlo	O
algorithms	O
are	O
summarized	O
in	O
robert	O
and	O
casella	O
(	O
1999	O
)	O
,	O
and	O
some	O
practical	O
guidance	O
on	O
the	O
use	O
of	O
sampling	B
methods	I
in	O
the	O
context	O
of	O
machine	O
learning	O
is	O
given	O
in	O
bishop	O
and	O
nabney	O
(	O
2008	O
)	O
.	O
11.1.	O
basic	O
sampling	O
algorithms	O
in	O
this	O
section	O
,	O
we	O
consider	O
some	O
simple	O
strategies	O
for	O
generating	O
random	O
samples	O
from	O
a	O
given	O
distribution	O
.	O
because	O
the	O
samples	O
will	O
be	O
generated	O
by	O
a	O
computer	O
algorithm	O
they	O
will	O
in	O
fact	O
be	O
pseudo-random	B
numbers	I
,	O
that	O
is	O
,	O
they	O
will	O
be	O
deter-	O
ministically	O
calculated	O
,	O
but	O
must	O
nevertheless	O
pass	O
appropriate	O
tests	O
for	O
randomness	O
.	O
generating	O
such	O
numbers	O
raises	O
several	O
subtleties	O
(	O
press	O
et	O
al.	O
,	O
1992	O
)	O
that	O
lie	O
outside	O
the	O
scope	O
of	O
this	O
book	O
.	O
here	O
we	O
shall	O
assume	O
that	O
an	O
algorithm	O
has	O
been	O
provided	O
that	O
generates	O
pseudo-random	B
numbers	I
distributed	O
uniformly	O
over	O
(	O
0	O
,	O
1	O
)	O
,	O
and	O
indeed	O
most	O
software	O
environments	O
have	O
such	O
a	O
facility	O
built	O
in	O
.	O
11.1.1	O
standard	O
distributions	O
we	O
ﬁrst	O
consider	O
how	O
to	O
generate	O
random	O
numbers	O
from	O
simple	O
nonuniform	O
dis-	O
tributions	O
,	O
assuming	O
that	O
we	O
already	O
have	O
available	O
a	O
source	O
of	O
uniformly	O
distributed	O
random	O
numbers	O
.	O
suppose	O
that	O
z	O
is	O
uniformly	O
distributed	O
over	O
the	O
interval	O
(	O
0	O
,	O
1	O
)	O
,	O
and	O
that	O
we	O
transform	O
the	O
values	O
of	O
z	O
using	O
some	O
function	O
f	O
(	O
·	O
)	O
so	O
that	O
y	O
=	O
f	O
(	O
z	O
)	O
.	O
the	O
distribution	O
of	O
y	O
will	O
be	O
governed	O
by	O
p	O
(	O
y	O
)	O
=	O
p	O
(	O
z	O
)	O
(	O
11.5	O
)	O
dy	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
dz	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
p	O
(	O
(	O
cid:1	O
)	O
y	O
)	O
d	O
(	O
cid:1	O
)	O
y	O
exercise	O
11.2	O
z	O
=	O
h	O
(	O
y	O
)	O
≡	O
where	O
,	O
in	O
this	O
case	O
,	O
p	O
(	O
z	O
)	O
=	O
1.	O
our	O
goal	O
is	O
to	O
choose	O
the	O
function	O
f	O
(	O
z	O
)	O
such	O
that	O
the	O
resulting	O
values	O
of	O
y	O
have	O
some	O
speciﬁc	O
desired	O
distribution	O
p	O
(	O
y	O
)	O
.	O
integrating	O
(	O
11.5	O
)	O
we	O
obtain	O
(	O
cid:6	O
)	O
y	O
(	O
11.6	O
)	O
−1	O
(	O
z	O
)	O
,	O
and	O
so	O
we	O
have	O
to	O
which	O
is	O
the	O
indeﬁnite	O
integral	O
of	O
p	O
(	O
y	O
)	O
.	O
thus	O
,	O
y	O
=	O
h	O
transform	O
the	O
uniformly	O
distributed	O
random	O
numbers	O
using	O
a	O
function	O
which	O
is	O
the	O
inverse	B
of	O
the	O
indeﬁnite	O
integral	O
of	O
the	O
desired	O
distribution	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
11.2	O
.	O
−∞	O
consider	O
for	O
example	O
the	O
exponential	B
distribution	I
p	O
(	O
y	O
)	O
=	O
λ	O
exp	O
(	O
−λy	O
)	O
(	O
11.7	O
)	O
where	O
0	O
(	O
cid:1	O
)	O
y	O
<	O
∞	O
.	O
in	O
this	O
case	O
the	O
lower	O
limit	O
of	O
the	O
integral	O
in	O
(	O
11.6	O
)	O
is	O
0	O
,	O
and	O
so	O
h	O
(	O
y	O
)	O
=	O
1	O
−	O
exp	O
(	O
−λy	O
)	O
.	O
thus	O
,	O
if	O
we	O
transform	O
our	O
uniformly	O
distributed	O
variable	O
z	O
using	O
y	O
=	O
−λ	O
−1	O
ln	O
(	O
1	O
−	O
z	O
)	O
,	O
then	O
y	O
will	O
have	O
an	O
exponential	B
distribution	I
.	O
11.1.	O
basic	O
sampling	O
algorithms	O
527	O
figure	O
11.2	O
geometrical	O
interpretation	O
of	O
the	O
trans-	O
formation	O
method	O
for	O
generating	O
nonuni-	O
formly	O
distributed	O
random	O
numbers	O
.	O
h	O
(	O
y	O
)	O
is	O
the	O
indeﬁnite	O
integral	O
of	O
the	O
desired	O
dis-	O
tribution	O
p	O
(	O
y	O
)	O
.	O
if	O
a	O
uniformly	O
distributed	O
random	O
variable	O
z	O
is	O
transformed	O
using	O
y	O
=	O
h−1	O
(	O
z	O
)	O
,	O
then	O
y	O
will	O
be	O
distributed	O
ac-	O
cording	O
to	O
p	O
(	O
y	O
)	O
.	O
1	O
0	O
h	O
(	O
y	O
)	O
p	O
(	O
y	O
)	O
y	O
another	O
example	O
of	O
a	O
distribution	O
to	O
which	O
the	O
transformation	O
method	O
can	O
be	O
applied	O
is	O
given	O
by	O
the	O
cauchy	O
distribution	O
p	O
(	O
y	O
)	O
=	O
1	O
π	O
1	O
1	O
+	O
y2	O
.	O
(	O
11.8	O
)	O
exercise	O
11.3	O
in	O
this	O
case	O
,	O
the	O
inverse	B
of	O
the	O
indeﬁnite	O
integral	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
‘	O
tan	O
’	O
function	O
.	O
the	O
generalization	B
to	O
multiple	O
variables	O
is	O
straightforward	O
and	O
involves	O
the	O
ja-	O
cobian	O
of	O
the	O
change	O
of	O
variables	O
,	O
so	O
that	O
p	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
ym	O
)	O
=	O
p	O
(	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
∂	O
(	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
)	O
∂	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
ym	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
.	O
(	O
11.9	O
)	O
as	O
a	O
ﬁnal	O
example	O
of	O
the	O
transformation	O
method	O
we	O
consider	O
the	O
box-muller	O
method	O
for	O
generating	O
samples	O
from	O
a	O
gaussian	O
distribution	O
.	O
first	O
,	O
suppose	O
we	O
gen-	O
erate	O
pairs	O
of	O
uniformly	O
distributed	O
random	O
numbers	O
z1	O
,	O
z2	O
∈	O
(	O
−1	O
,	O
1	O
)	O
,	O
which	O
we	O
can	O
do	O
by	O
transforming	O
a	O
variable	O
distributed	O
uniformly	O
over	O
(	O
0	O
,	O
1	O
)	O
using	O
z	O
→	O
2z	O
−	O
1	O
.	O
2	O
(	O
cid:1	O
)	O
1.	O
this	O
leads	O
to	O
a	O
uniform	O
next	O
we	O
discard	O
each	O
pair	O
unless	O
it	O
satisﬁes	O
z2	O
distribution	O
of	O
points	O
inside	O
the	O
unit	O
circle	O
with	O
p	O
(	O
z1	O
,	O
z2	O
)	O
=	O
1/π	O
,	O
as	O
illustrated	O
in	O
figure	O
11.3.	O
then	O
,	O
for	O
each	O
pair	O
z1	O
,	O
z2	O
we	O
evaluate	O
the	O
quantities	O
1	O
+	O
z2	O
figure	O
11.3	O
the	O
box-muller	O
method	O
for	O
generating	O
gaussian	O
dis-	O
tributed	O
random	O
numbers	O
starts	O
by	O
generating	O
samples	O
from	O
a	O
uniform	B
distribution	I
inside	O
the	O
unit	O
circle	O
.	O
1	O
z2	O
−1	O
−1	O
z1	O
1	O
528	O
11.	O
sampling	B
methods	I
y1	O
=	O
z1	O
y2	O
=	O
z2	O
(	O
cid:16	O
)	O
1/2	O
(	O
cid:16	O
)	O
1/2	O
r2	O
(	O
cid:15	O
)	O
−2	O
ln	O
z1	O
(	O
cid:15	O
)	O
−2	O
ln	O
z2	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
∂	O
(	O
z1	O
,	O
z2	O
)	O
(	O
cid:30	O
)	O
(	O
cid:29	O
)	O
∂	O
(	O
y1	O
,	O
y2	O
)	O
r2	O
exercise	O
11.4	O
exercise	O
11.5	O
(	O
11.10	O
)	O
(	O
11.11	O
)	O
(	O
11.12	O
)	O
where	O
r2	O
=	O
z2	O
1	O
+	O
z2	O
2.	O
then	O
the	O
joint	O
distribution	O
of	O
y1	O
and	O
y2	O
is	O
given	O
by	O
p	O
(	O
y1	O
,	O
y2	O
)	O
=	O
p	O
(	O
z1	O
,	O
z2	O
)	O
(	O
cid:29	O
)	O
=	O
1√	O
2π	O
exp	O
(	O
−y2	O
1/2	O
)	O
(	O
cid:30	O
)	O
exp	O
(	O
−y2	O
2/2	O
)	O
1√	O
2π	O
and	O
so	O
y1	O
and	O
y2	O
are	O
independent	B
and	O
each	O
has	O
a	O
gaussian	O
distribution	O
with	O
zero	O
mean	B
and	O
unit	O
variance	B
.	O
if	O
y	O
has	O
a	O
gaussian	O
distribution	O
with	O
zero	O
mean	B
and	O
unit	O
variance	B
,	O
then	O
σy	O
+	O
µ	O
will	O
have	O
a	O
gaussian	O
distribution	O
with	O
mean	B
µ	O
and	O
variance	B
σ2	O
.	O
to	O
generate	O
vector-	O
valued	O
variables	O
having	O
a	O
multivariate	O
gaussian	O
distribution	O
with	O
mean	B
µ	O
and	O
co-	O
variance	B
σ	O
,	O
we	O
can	O
make	O
use	O
of	O
the	O
cholesky	O
decomposition	O
,	O
which	O
takes	O
the	O
form	O
σ	O
=	O
llt	O
(	O
press	O
et	O
al.	O
,	O
1992	O
)	O
.	O
then	O
,	O
if	O
z	O
is	O
a	O
vector	O
valued	O
random	O
variable	O
whose	O
components	O
are	O
independent	B
and	O
gaussian	O
distributed	O
with	O
zero	O
mean	B
and	O
unit	O
vari-	O
ance	O
,	O
then	O
y	O
=	O
µ	O
+	O
lz	O
will	O
have	O
mean	B
µ	O
and	O
covariance	B
σ.	O
obviously	O
,	O
the	O
transformation	O
technique	O
depends	O
for	O
its	O
success	O
on	O
the	O
ability	O
to	O
calculate	O
and	O
then	O
invert	O
the	O
indeﬁnite	O
integral	O
of	O
the	O
required	O
distribution	O
.	O
such	O
operations	O
will	O
only	O
be	O
feasible	O
for	O
a	O
limited	O
number	O
of	O
simple	O
distributions	O
,	O
and	O
so	O
we	O
must	O
turn	O
to	O
alternative	O
approaches	O
in	O
search	O
of	O
a	O
more	O
general	O
strategy	O
.	O
here	O
we	O
consider	O
two	O
techniques	O
called	O
rejection	B
sampling	I
and	O
importance	B
sampling	I
.	O
al-	O
though	O
mainly	O
limited	O
to	O
univariate	O
distributions	O
and	O
thus	O
not	O
directly	O
applicable	O
to	O
complex	O
problems	O
in	O
many	O
dimensions	O
,	O
they	O
do	O
form	O
important	O
components	O
in	O
more	O
general	O
strategies	O
.	O
11.1.2	O
rejection	B
sampling	I
the	O
rejection	B
sampling	I
framework	O
allows	O
us	O
to	O
sample	O
from	O
relatively	O
complex	O
distributions	O
,	O
subject	O
to	O
certain	O
constraints	O
.	O
we	O
begin	O
by	O
considering	O
univariate	O
dis-	O
tributions	O
and	O
discuss	O
the	O
extension	O
to	O
multiple	O
dimensions	O
subsequently	O
.	O
suppose	O
we	O
wish	O
to	O
sample	O
from	O
a	O
distribution	O
p	O
(	O
z	O
)	O
that	O
is	O
not	O
one	O
of	O
the	O
simple	O
,	O
standard	O
distributions	O
considered	O
so	O
far	O
,	O
and	O
that	O
sampling	O
directly	O
from	O
p	O
(	O
z	O
)	O
is	O
dif-	O
ﬁcult	O
.	O
furthermore	O
suppose	O
,	O
as	O
is	O
often	O
the	O
case	O
,	O
that	O
we	O
are	O
easily	O
able	O
to	O
evaluate	O
p	O
(	O
z	O
)	O
for	O
any	O
given	O
value	O
of	O
z	O
,	O
up	O
to	O
some	O
normalizing	O
constant	O
z	O
,	O
so	O
that	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
1	O
zp	O
where	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
can	O
readily	O
be	O
evaluated	O
,	O
but	O
zp	O
is	O
unknown	O
.	O
p	O
(	O
z	O
)	O
=	O
in	O
order	O
to	O
apply	O
rejection	B
sampling	I
,	O
we	O
need	O
some	O
simpler	O
distribution	O
q	O
(	O
z	O
)	O
,	O
sometimes	O
called	O
a	O
proposal	B
distribution	I
,	O
from	O
which	O
we	O
can	O
readily	O
draw	O
samples	O
.	O
(	O
11.13	O
)	O
figure	O
11.4	O
in	O
the	O
rejection	B
sampling	I
method	O
,	O
samples	O
are	O
drawn	O
from	O
a	O
sim-	O
ple	O
distribution	O
q	O
(	O
z	O
)	O
and	O
rejected	O
if	O
they	O
fall	O
in	O
the	O
grey	O
area	O
be-	O
tween	O
the	O
unnormalized	O
distribu-	O
tion	O
ep	O
(	O
z	O
)	O
and	O
the	O
scaled	O
distribu-	O
tion	O
kq	O
(	O
z	O
)	O
.	O
the	O
resulting	O
samples	O
are	O
distributed	O
according	O
to	O
p	O
(	O
z	O
)	O
,	O
which	O
is	O
the	O
normalized	O
version	O
of	O
ep	O
(	O
z	O
)	O
.	O
11.1.	O
basic	O
sampling	O
algorithms	O
529	O
kq	O
(	O
z0	O
)	O
kq	O
(	O
z	O
)	O
˜p	O
(	O
z	O
)	O
u0	O
exercise	O
11.6	O
z	O
z0	O
we	O
next	O
introduce	O
a	O
constant	O
k	O
whose	O
value	O
is	O
chosen	O
such	O
that	O
kq	O
(	O
z	O
)	O
(	O
cid:2	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
for	O
all	O
values	O
of	O
z.	O
the	O
function	O
kq	O
(	O
z	O
)	O
is	O
called	O
the	O
comparison	O
function	O
and	O
is	O
illus-	O
trated	O
for	O
a	O
univariate	O
distribution	O
in	O
figure	O
11.4.	O
each	O
step	O
of	O
the	O
rejection	O
sampler	O
involves	O
generating	O
two	O
random	O
numbers	O
.	O
first	O
,	O
we	O
generate	O
a	O
number	O
z0	O
from	O
the	O
distribution	O
q	O
(	O
z	O
)	O
.	O
next	O
,	O
we	O
generate	O
a	O
number	O
u0	O
from	O
the	O
uniform	B
distribution	I
over	O
[	O
0	O
,	O
kq	O
(	O
z0	O
)	O
]	O
.	O
this	O
pair	O
of	O
random	O
numbers	O
has	O
uniform	B
distribution	I
under	O
the	O
curve	O
of	O
the	O
function	O
kq	O
(	O
z	O
)	O
.	O
finally	O
,	O
if	O
u0	O
>	O
(	O
cid:4	O
)	O
p	O
(	O
z0	O
)	O
then	O
the	O
sample	O
is	O
rejected	O
,	O
otherwise	O
ure	O
11.4.	O
the	O
remaining	O
pairs	O
then	O
have	O
uniform	B
distribution	I
under	O
the	O
curve	O
of	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
,	O
ples	O
are	O
then	O
accepted	O
with	O
probability	B
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
/kq	O
(	O
z	O
)	O
,	O
and	O
so	O
the	O
probability	B
that	O
a	O
and	O
hence	O
the	O
corresponding	O
z	O
values	O
are	O
distributed	O
according	O
to	O
p	O
(	O
z	O
)	O
,	O
as	O
desired	O
.	O
the	O
original	O
values	O
of	O
z	O
are	O
generated	O
from	O
the	O
distribution	O
q	O
(	O
z	O
)	O
,	O
and	O
these	O
sam-	O
u0	O
is	O
retained	O
.	O
thus	O
the	O
pair	O
is	O
rejected	O
if	O
it	O
lies	O
in	O
the	O
grey	O
shaded	O
region	O
in	O
fig-	O
p	O
(	O
accept	O
)	O
=	O
sample	O
will	O
be	O
accepted	O
is	O
given	O
by	O
{	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
/kq	O
(	O
z	O
)	O
}	O
q	O
(	O
z	O
)	O
dz	O
(	O
cid:6	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
dz	O
.	O
the	O
area	O
under	O
the	O
unnormalized	O
distribution	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
to	O
the	O
area	O
under	O
the	O
curve	O
kq	O
(	O
z	O
)	O
.	O
limitation	O
that	O
kq	O
(	O
z	O
)	O
must	O
be	O
nowhere	O
less	O
than	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
.	O
we	O
therefore	O
see	O
that	O
the	O
constant	O
k	O
should	O
be	O
as	O
small	O
as	O
possible	O
subject	O
to	O
the	O
thus	O
the	O
fraction	O
of	O
points	O
that	O
are	O
rejected	O
by	O
this	O
method	O
depends	O
on	O
the	O
ratio	O
of	O
(	O
11.14	O
)	O
1	O
k	O
(	O
cid:6	O
)	O
=	O
as	O
an	O
illustration	O
of	O
the	O
use	O
of	O
rejection	B
sampling	I
,	O
consider	O
the	O
task	O
of	O
sampling	O
from	O
the	O
gamma	B
distribution	I
gam	O
(	O
z|a	O
,	O
b	O
)	O
=	O
baza−1	O
exp	O
(	O
−bz	O
)	O
γ	O
(	O
a	O
)	O
(	O
11.15	O
)	O
which	O
,	O
for	O
a	O
>	O
1	O
,	O
has	O
a	O
bell-shaped	O
form	O
,	O
as	O
shown	O
in	O
figure	O
11.5.	O
a	O
suitable	O
proposal	B
distribution	I
is	O
therefore	O
the	O
cauchy	O
(	O
11.8	O
)	O
because	O
this	O
too	O
is	O
bell-shaped	O
and	O
because	O
we	O
can	O
use	O
the	O
transformation	O
method	O
,	O
discussed	O
earlier	O
,	O
to	O
sample	O
from	O
it	O
.	O
we	O
need	O
to	O
generalize	O
the	O
cauchy	O
slightly	O
to	O
ensure	O
that	O
it	O
nowhere	O
has	O
a	O
smaller	O
value	O
than	O
the	O
gamma	B
distribution	I
.	O
this	O
can	O
be	O
achieved	O
by	O
transforming	O
a	O
uniform	O
random	O
variable	O
y	O
using	O
z	O
=	O
b	O
tan	O
y	O
+	O
c	O
,	O
which	O
gives	O
random	O
numbers	O
distributed	O
according	O
to	O
.	O
exercise	O
11.7	O
530	O
11.	O
sampling	B
methods	I
figure	O
11.5	O
plot	O
showing	O
the	O
gamma	O
distribu-	O
tion	O
given	O
by	O
(	O
11.15	O
)	O
as	O
the	O
green	O
curve	O
,	O
with	O
a	O
scaled	O
cauchy	O
pro-	O
posal	O
distribution	O
shown	O
by	O
the	O
red	O
curve	O
.	O
samples	O
from	O
the	O
gamma	B
distribution	I
can	O
be	O
obtained	O
by	O
sampling	O
from	O
the	O
cauchy	O
and	O
then	O
applying	O
the	O
rejection	O
sam-	O
pling	O
criterion	O
.	O
p	O
(	O
z	O
)	O
0.15	O
0.1	O
0.05	O
0	O
0	O
10	O
20	O
30	O
z	O
q	O
(	O
z	O
)	O
=	O
(	O
11.16	O
)	O
the	O
minimum	O
reject	O
rate	O
is	O
obtained	O
by	O
setting	O
c	O
=	O
a	O
−	O
1	O
,	O
b2	O
=	O
2a	O
−	O
1	O
and	O
choos-	O
ing	O
the	O
constant	O
k	O
to	O
be	O
as	O
small	O
as	O
possible	O
while	O
still	O
satisfying	O
the	O
requirement	O
kq	O
(	O
z	O
)	O
(	O
cid:2	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
.	O
the	O
resulting	O
comparison	O
function	O
is	O
also	O
illustrated	O
in	O
figure	O
11.5.	O
k	O
1	O
+	O
(	O
z	O
−	O
c	O
)	O
2/b2	O
.	O
11.1.3	O
adaptive	B
rejection	I
sampling	I
in	O
many	O
instances	O
where	O
we	O
might	O
wish	O
to	O
apply	O
rejection	B
sampling	I
,	O
it	O
proves	O
difﬁcult	O
to	O
determine	O
a	O
suitable	O
analytic	O
form	O
for	O
the	O
envelope	O
distribution	O
q	O
(	O
z	O
)	O
.	O
an	O
alternative	O
approach	O
is	O
to	O
construct	O
the	O
envelope	O
function	O
on	O
the	O
ﬂy	O
based	O
on	O
mea-	O
sured	O
values	O
of	O
the	O
distribution	O
p	O
(	O
z	O
)	O
(	O
gilks	O
and	O
wild	O
,	O
1992	O
)	O
.	O
construction	O
of	O
an	O
envelope	O
function	O
is	O
particularly	O
straightforward	O
for	O
cases	O
in	O
which	O
p	O
(	O
z	O
)	O
is	O
log	O
con-	O
cave	O
,	O
in	O
other	O
words	O
when	O
ln	O
p	O
(	O
z	O
)	O
has	O
derivatives	O
that	O
are	O
nonincreasing	O
functions	O
of	O
z.	O
the	O
construction	O
of	O
a	O
suitable	O
envelope	O
function	O
is	O
illustrated	O
graphically	O
in	O
figure	O
11.6.	O
the	O
function	O
ln	O
p	O
(	O
z	O
)	O
and	O
its	O
gradient	O
are	O
evaluated	O
at	O
some	O
initial	O
set	O
of	O
grid	O
points	O
,	O
and	O
the	O
intersections	O
of	O
the	O
resulting	O
tangent	O
lines	O
are	O
used	O
to	O
construct	O
the	O
envelope	O
function	O
.	O
next	O
a	O
sample	O
value	O
is	O
drawn	O
from	O
the	O
envelope	O
distribution	O
.	O
this	O
is	O
straightforward	O
because	O
the	O
log	O
of	O
the	O
envelope	O
distribution	O
is	O
a	O
succession	O
exercise	O
11.9	O
figure	O
11.6	O
in	O
the	O
case	O
of	O
distributions	O
that	O
are	O
log	O
concave	O
,	O
an	O
envelope	O
function	O
for	O
use	O
in	O
rejection	B
sampling	I
can	O
be	O
constructed	O
using	O
the	O
tangent	O
lines	O
computed	O
at	O
a	O
set	O
of	O
grid	O
points	O
.	O
if	O
a	O
sample	O
point	O
is	O
rejected	O
,	O
it	O
is	O
added	O
to	O
the	O
set	O
of	O
grid	O
points	O
and	O
used	O
to	O
reﬁne	O
the	O
envelope	O
distribution	O
.	O
ln	O
p	O
(	O
z	O
)	O
z1	O
z2	O
z3	O
z	O
11.1.	O
basic	O
sampling	O
algorithms	O
531	O
figure	O
11.7	O
illustrative	O
example	O
of	O
rejection	B
sampling	I
involving	O
sampling	O
from	O
a	O
gaussian	O
distribution	O
p	O
(	O
z	O
)	O
shown	O
by	O
the	O
green	O
curve	O
,	O
by	O
using	O
rejection	B
sampling	I
from	O
a	O
proposal	O
distri-	O
bution	O
q	O
(	O
z	O
)	O
that	O
is	O
also	O
gaussian	O
and	O
whose	O
scaled	O
version	O
kq	O
(	O
z	O
)	O
is	O
shown	O
by	O
the	O
red	O
curve	O
.	O
0.5	O
p	O
(	O
z	O
)	O
0.25	O
0	O
−5	O
0	O
z	O
5	O
of	O
linear	O
functions	O
,	O
and	O
hence	O
the	O
envelope	O
distribution	O
itself	O
comprises	O
a	O
piecewise	O
exponential	B
distribution	I
of	O
the	O
form	O
q	O
(	O
z	O
)	O
=	O
kiλi	O
exp	O
{	O
−λi	O
(	O
z	O
−	O
zi−1	O
)	O
}	O
zi−1	O
<	O
z	O
(	O
cid:1	O
)	O
zi	O
.	O
(	O
11.17	O
)	O
once	O
a	O
sample	O
has	O
been	O
drawn	O
,	O
the	O
usual	O
rejection	O
criterion	O
can	O
be	O
applied	O
.	O
if	O
the	O
sample	O
is	O
accepted	O
,	O
then	O
it	O
will	O
be	O
a	O
draw	O
from	O
the	O
desired	O
distribution	O
.	O
if	O
,	O
however	O
,	O
the	O
sample	O
is	O
rejected	O
,	O
then	O
it	O
is	O
incorporated	O
into	O
the	O
set	O
of	O
grid	O
points	O
,	O
a	O
new	O
tangent	O
line	O
is	O
computed	O
,	O
and	O
the	O
envelope	O
function	O
is	O
thereby	O
reﬁned	O
.	O
as	O
the	O
number	O
of	O
grid	O
points	O
increases	O
,	O
so	O
the	O
envelope	O
function	O
becomes	O
a	O
better	O
approximation	O
of	O
the	O
desired	O
distribution	O
p	O
(	O
z	O
)	O
and	O
the	O
probability	B
of	O
rejection	O
decreases	O
.	O
a	O
variant	O
of	O
the	O
algorithm	O
exists	O
that	O
avoids	O
the	O
evaluation	O
of	O
derivatives	O
(	O
gilks	O
,	O
1992	O
)	O
.	O
the	O
adaptive	B
rejection	I
sampling	I
framework	O
can	O
also	O
be	O
extended	B
to	O
distri-	O
butions	O
that	O
are	O
not	O
log	O
concave	O
,	O
simply	O
by	O
following	O
each	O
rejection	B
sampling	I
step	O
with	O
a	O
metropolis-hastings	O
step	O
(	O
to	O
be	O
discussed	O
in	O
section	O
11.2.2	O
)	O
,	O
giving	O
rise	O
to	O
adaptive	O
rejection	O
metropolis	O
sampling	O
(	O
gilks	O
et	O
al.	O
,	O
1995	O
)	O
.	O
clearly	O
for	O
rejection	O
sampling	O
to	O
be	O
of	O
practical	O
value	O
,	O
we	O
require	O
that	O
the	O
com-	O
parison	O
function	O
be	O
close	O
to	O
the	O
required	O
distribution	O
so	O
that	O
the	O
rate	O
of	O
rejection	O
is	O
kept	O
to	O
a	O
minimum	O
.	O
now	O
let	O
us	O
examine	O
what	O
happens	O
when	O
we	O
try	O
to	O
use	O
rejection	B
sampling	I
in	O
spaces	O
of	O
high	O
dimensionality	O
.	O
consider	O
,	O
for	O
the	O
sake	O
of	O
illustration	O
,	O
a	O
somewhat	O
artiﬁcial	O
problem	O
in	O
which	O
we	O
wish	O
to	O
sample	O
from	O
a	O
zero-mean	O
mul-	O
tivariate	O
gaussian	O
distribution	O
with	O
covariance	B
σ2	O
pi	O
,	O
where	O
i	O
is	O
the	O
unit	O
matrix	O
,	O
by	O
rejection	B
sampling	I
from	O
a	O
proposal	B
distribution	I
that	O
is	O
itself	O
a	O
zero-mean	O
gaussian	O
distribution	O
having	O
covariance	B
σ2	O
p	O
in	O
order	O
that	O
there	O
exists	O
a	O
k	O
such	O
that	O
kq	O
(	O
z	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
z	O
)	O
.	O
in	O
d-dimensions	O
the	O
optimum	O
value	O
of	O
k	O
is	O
given	O
by	O
k	O
=	O
(	O
σq/σp	O
)	O
d	O
,	O
as	O
illustrated	O
for	O
d	O
=	O
1	O
in	O
figure	O
11.7.	O
the	O
acceptance	O
rate	O
will	O
be	O
the	O
ratio	O
of	O
volumes	O
under	O
p	O
(	O
z	O
)	O
and	O
kq	O
(	O
z	O
)	O
,	O
which	O
,	O
because	O
both	O
distribu-	O
tions	O
are	O
normalized	O
,	O
is	O
just	O
1/k	O
.	O
thus	O
the	O
acceptance	O
rate	O
diminishes	O
exponentially	O
with	O
dimensionality	O
.	O
even	O
if	O
σq	O
exceeds	O
σp	O
by	O
just	O
one	O
percent	O
,	O
for	O
d	O
=	O
1	O
,	O
000	O
the	O
acceptance	O
ratio	O
will	O
be	O
approximately	O
1/20	O
,	O
000.	O
in	O
this	O
illustrative	O
example	O
the	O
comparison	O
function	O
is	O
close	O
to	O
the	O
required	O
distribution	O
.	O
for	O
more	O
practical	O
exam-	O
ples	O
,	O
where	O
the	O
desired	O
distribution	O
may	O
be	O
multimodal	O
and	O
sharply	O
peaked	O
,	O
it	O
will	O
be	O
extremely	O
difﬁcult	O
to	O
ﬁnd	O
a	O
good	O
proposal	B
distribution	I
and	O
comparison	O
function	O
.	O
qi	O
.	O
obviously	O
,	O
we	O
must	O
have	O
σ2	O
q	O
(	O
cid:2	O
)	O
σ2	O
532	O
11.	O
sampling	B
methods	I
figure	O
11.8	O
importance	B
sampling	I
addresses	O
the	O
prob-	O
lem	O
of	O
evaluating	O
the	O
expectation	B
of	O
a	O
func-	O
tion	O
f	O
(	O
z	O
)	O
with	O
respect	O
to	O
a	O
distribution	O
p	O
(	O
z	O
)	O
from	O
which	O
it	O
is	O
difﬁcult	O
to	O
draw	O
samples	O
di-	O
instead	O
,	O
samples	O
{	O
z	O
(	O
l	O
)	O
}	O
are	O
drawn	O
rectly	O
.	O
from	O
a	O
simpler	O
distribution	O
q	O
(	O
z	O
)	O
,	O
and	O
the	O
corresponding	O
terms	O
in	O
the	O
summation	O
are	O
weighted	O
by	O
the	O
ratios	O
p	O
(	O
z	O
(	O
l	O
)	O
)	O
/q	O
(	O
z	O
(	O
l	O
)	O
)	O
.	O
p	O
(	O
z	O
)	O
q	O
(	O
z	O
)	O
f	O
(	O
z	O
)	O
z	O
furthermore	O
,	O
the	O
exponential	O
decrease	O
of	O
acceptance	O
rate	O
with	O
dimensionality	O
is	O
a	O
generic	O
feature	O
of	O
rejection	B
sampling	I
.	O
although	O
rejection	O
can	O
be	O
a	O
useful	O
technique	O
in	O
one	O
or	O
two	O
dimensions	O
it	O
is	O
unsuited	O
to	O
problems	O
of	O
high	O
dimensionality	O
.	O
it	O
can	O
,	O
however	O
,	O
play	O
a	O
role	O
as	O
a	O
subroutine	O
in	O
more	O
sophisticated	O
algorithms	O
for	O
sampling	O
in	O
high	O
dimensional	O
spaces	O
.	O
11.1.4	O
importance	B
sampling	I
one	O
of	O
the	O
principal	O
reasons	O
for	O
wishing	O
to	O
sample	O
from	O
complicated	O
probability	B
distributions	O
is	O
to	O
be	O
able	O
to	O
evaluate	O
expectations	O
of	O
the	O
form	O
(	O
11.1	O
)	O
.	O
the	O
technique	O
of	O
importance	B
sampling	I
provides	O
a	O
framework	O
for	O
approximating	O
expectations	O
di-	O
rectly	O
but	O
does	O
not	O
itself	O
provide	O
a	O
mechanism	O
for	O
drawing	O
samples	O
from	O
distribution	O
p	O
(	O
z	O
)	O
.	O
the	O
ﬁnite	O
sum	O
approximation	O
to	O
the	O
expectation	B
,	O
given	O
by	O
(	O
11.2	O
)	O
,	O
depends	O
on	O
being	O
able	O
to	O
draw	O
samples	O
from	O
the	O
distribution	O
p	O
(	O
z	O
)	O
.	O
suppose	O
,	O
however	O
,	O
that	O
it	O
is	O
impractical	O
to	O
sample	O
directly	O
from	O
p	O
(	O
z	O
)	O
but	O
that	O
we	O
can	O
evaluate	O
p	O
(	O
z	O
)	O
easily	O
for	O
any	O
given	O
value	O
of	O
z.	O
one	O
simplistic	O
strategy	O
for	O
evaluating	O
expectations	O
would	O
be	O
to	O
discretize	O
z-space	O
into	O
a	O
uniform	O
grid	O
and	O
to	O
evaluate	O
the	O
integrand	O
as	O
a	O
sum	O
of	O
the	O
form	O
e	O
[	O
f	O
]	O
(	O
cid:7	O
)	O
l	O
(	O
cid:2	O
)	O
p	O
(	O
z	O
(	O
l	O
)	O
)	O
f	O
(	O
z	O
(	O
l	O
)	O
)	O
.	O
(	O
11.18	O
)	O
l=1	O
an	O
obvious	O
problem	O
with	O
this	O
approach	O
is	O
that	O
the	O
number	O
of	O
terms	O
in	O
the	O
summation	O
grows	O
exponentially	O
with	O
the	O
dimensionality	O
of	O
z.	O
furthermore	O
,	O
as	O
we	O
have	O
already	O
noted	O
,	O
the	O
kinds	O
of	O
probability	B
distributions	O
of	O
interest	O
will	O
often	O
have	O
much	O
of	O
their	O
mass	O
conﬁned	O
to	O
relatively	O
small	O
regions	O
of	O
z	O
space	O
and	O
so	O
uniform	B
sampling	I
will	O
be	O
very	O
inefﬁcient	O
because	O
in	O
high-dimensional	O
problems	O
,	O
only	O
a	O
very	O
small	O
proportion	O
of	O
the	O
samples	O
will	O
make	O
a	O
signiﬁcant	O
contribution	O
to	O
the	O
sum	O
.	O
we	O
would	O
really	O
like	O
to	O
choose	O
the	O
sample	O
points	O
to	O
fall	O
in	O
regions	O
where	O
p	O
(	O
z	O
)	O
is	O
large	O
,	O
or	O
ideally	O
where	O
the	O
product	O
p	O
(	O
z	O
)	O
f	O
(	O
z	O
)	O
is	O
large	O
.	O
as	O
in	O
the	O
case	O
of	O
rejection	B
sampling	I
,	O
importance	B
sampling	I
is	O
based	O
on	O
the	O
use	O
of	O
a	O
proposal	B
distribution	I
q	O
(	O
z	O
)	O
from	O
which	O
it	O
is	O
easy	O
to	O
draw	O
samples	O
,	O
as	O
illustrated	O
in	O
figure	O
11.8.	O
we	O
can	O
then	O
express	O
the	O
expectation	B
in	O
the	O
form	O
of	O
a	O
ﬁnite	O
sum	O
over	O
11.1.	O
basic	O
sampling	O
algorithms	O
533	O
samples	O
{	O
z	O
(	O
l	O
)	O
}	O
drawn	O
from	O
q	O
(	O
z	O
)	O
e	O
[	O
f	O
]	O
=	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
=	O
(	O
cid:7	O
)	O
1	O
l	O
f	O
(	O
z	O
)	O
p	O
(	O
z	O
)	O
dz	O
f	O
(	O
z	O
)	O
p	O
(	O
z	O
)	O
l	O
(	O
cid:2	O
)	O
l=1	O
q	O
(	O
z	O
)	O
q	O
(	O
z	O
)	O
dz	O
p	O
(	O
z	O
(	O
l	O
)	O
)	O
q	O
(	O
z	O
(	O
l	O
)	O
)	O
f	O
(	O
z	O
(	O
l	O
)	O
)	O
.	O
(	O
11.19	O
)	O
the	O
quantities	O
rl	O
=	O
p	O
(	O
z	O
(	O
l	O
)	O
)	O
/q	O
(	O
z	O
(	O
l	O
)	O
)	O
are	O
known	O
as	O
importance	B
weights	I
,	O
and	O
they	O
cor-	O
rect	O
the	O
bias	B
introduced	O
by	O
sampling	O
from	O
the	O
wrong	O
distribution	O
.	O
note	O
that	O
,	O
unlike	O
rejection	B
sampling	I
,	O
all	O
of	O
the	O
samples	O
generated	O
are	O
retained	O
.	O
normalization	O
constant	O
,	O
so	O
that	O
p	O
(	O
z	O
)	O
=	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
/zp	O
where	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
can	O
be	O
evaluated	O
easily	O
,	O
distribution	O
q	O
(	O
z	O
)	O
=	O
(	O
cid:4	O
)	O
q	O
(	O
z	O
)	O
/zq	O
,	O
which	O
has	O
the	O
same	O
property	O
.	O
we	O
then	O
have	O
it	O
will	O
often	O
be	O
the	O
case	O
that	O
the	O
distribution	O
p	O
(	O
z	O
)	O
can	O
only	O
be	O
evaluated	O
up	O
to	O
a	O
whereas	O
zp	O
is	O
unknown	O
.	O
similarly	O
,	O
we	O
may	O
wish	O
to	O
use	O
an	O
importance	B
sampling	I
where	O
(	O
cid:4	O
)	O
rl	O
=	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
(	O
l	O
)	O
)	O
/	O
(	O
cid:4	O
)	O
q	O
(	O
z	O
(	O
l	O
)	O
)	O
.	O
we	O
can	O
use	O
the	O
same	O
sample	O
set	O
to	O
evaluate	O
the	O
ratio	O
l=1	O
(	O
11.20	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
1	O
l	O
e	O
[	O
f	O
]	O
=	O
f	O
(	O
z	O
)	O
f	O
(	O
z	O
)	O
p	O
(	O
z	O
)	O
dz	O
=	O
zq	O
zp	O
(	O
cid:7	O
)	O
zq	O
zp	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
(	O
cid:4	O
)	O
q	O
(	O
z	O
)	O
q	O
(	O
z	O
)	O
dz	O
(	O
cid:4	O
)	O
rlf	O
(	O
z	O
(	O
l	O
)	O
)	O
.	O
(	O
cid:6	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
(	O
cid:4	O
)	O
q	O
(	O
z	O
)	O
q	O
(	O
z	O
)	O
dz	O
l	O
(	O
cid:2	O
)	O
(	O
cid:6	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
dz	O
=	O
l	O
(	O
cid:2	O
)	O
(	O
cid:4	O
)	O
rl	O
e	O
[	O
f	O
]	O
(	O
cid:7	O
)	O
l	O
(	O
cid:2	O
)	O
(	O
cid:4	O
)	O
rl	O
(	O
cid:5	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
(	O
l	O
)	O
)	O
/q	O
(	O
z	O
(	O
l	O
)	O
)	O
(	O
cid:5	O
)	O
m	O
(	O
cid:4	O
)	O
rm	O
m	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
(	O
m	O
)	O
)	O
/q	O
(	O
z	O
(	O
m	O
)	O
)	O
.	O
wlf	O
(	O
z	O
(	O
l	O
)	O
)	O
=	O
l=1	O
l=1	O
=	O
1	O
zq	O
(	O
cid:7	O
)	O
1	O
l	O
(	O
11.21	O
)	O
(	O
11.22	O
)	O
(	O
11.23	O
)	O
zp/zq	O
with	O
the	O
result	O
zp	O
zq	O
and	O
hence	O
where	O
we	O
have	O
deﬁned	O
wl	O
=	O
as	O
with	O
rejection	B
sampling	I
,	O
the	O
success	O
of	O
the	O
importance	B
sampling	I
approach	O
depends	O
crucially	O
on	O
how	O
well	O
the	O
sampling	O
distribution	O
q	O
(	O
z	O
)	O
matches	O
the	O
desired	O
534	O
11.	O
sampling	B
methods	I
distribution	O
p	O
(	O
z	O
)	O
.	O
if	O
,	O
as	O
is	O
often	O
the	O
case	O
,	O
p	O
(	O
z	O
)	O
f	O
(	O
z	O
)	O
is	O
strongly	O
varying	O
and	O
has	O
a	O
sig-	O
niﬁcant	O
proportion	O
of	O
its	O
mass	O
concentrated	O
over	O
relatively	O
small	O
regions	O
of	O
z	O
space	O
,	O
then	O
the	O
set	O
of	O
importance	B
weights	I
{	O
rl	O
}	O
may	O
be	O
dominated	O
by	O
a	O
few	O
weights	O
hav-	O
ing	O
large	O
values	O
,	O
with	O
the	O
remaining	O
weights	O
being	O
relatively	O
insigniﬁcant	O
.	O
thus	O
the	O
effective	O
sample	O
size	O
can	O
be	O
much	O
smaller	O
than	O
the	O
apparent	O
sample	O
size	O
l.	O
the	O
prob-	O
lem	O
is	O
even	O
more	O
severe	O
if	O
none	O
of	O
the	O
samples	O
falls	O
in	O
the	O
regions	O
where	O
p	O
(	O
z	O
)	O
f	O
(	O
z	O
)	O
is	O
large	O
.	O
in	O
that	O
case	O
,	O
the	O
apparent	O
variances	O
of	O
rl	O
and	O
rlf	O
(	O
z	O
(	O
l	O
)	O
)	O
may	O
be	O
small	O
even	O
though	O
the	O
estimate	O
of	O
the	O
expectation	B
may	O
be	O
severely	O
wrong	O
.	O
hence	O
a	O
major	O
draw-	O
back	O
of	O
the	O
importance	B
sampling	I
method	O
is	O
the	O
potential	O
to	O
produce	O
results	O
that	O
are	O
arbitrarily	O
in	O
error	B
and	O
with	O
no	O
diagnostic	O
indication	O
.	O
this	O
also	O
highlights	O
a	O
key	O
re-	O
quirement	O
for	O
the	O
sampling	O
distribution	O
q	O
(	O
z	O
)	O
,	O
namely	O
that	O
it	O
should	O
not	O
be	O
small	O
or	O
zero	O
in	O
regions	O
where	O
p	O
(	O
z	O
)	O
may	O
be	O
signiﬁcant	O
.	O
for	O
distributions	O
deﬁned	O
in	O
terms	O
of	O
a	O
graphical	B
model	I
,	O
we	O
can	O
apply	O
the	O
impor-	O
tance	O
sampling	O
technique	O
in	O
various	O
ways	O
.	O
for	O
discrete	O
variables	O
,	O
a	O
simple	O
approach	O
is	O
called	O
uniform	B
sampling	I
.	O
the	O
joint	O
distribution	O
for	O
a	O
directed	B
graph	O
is	O
deﬁned	O
by	O
(	O
11.4	O
)	O
.	O
each	O
sample	O
from	O
the	O
joint	O
distribution	O
is	O
obtained	O
by	O
ﬁrst	O
setting	O
those	O
variables	O
zi	O
that	O
are	O
in	O
the	O
evidence	O
set	O
equal	O
to	O
their	O
observed	O
values	O
.	O
each	O
of	O
the	O
remaining	O
variables	O
is	O
then	O
sampled	O
independently	O
from	O
a	O
uniform	B
distribution	I
over	O
the	O
space	O
of	O
possible	O
instantiations	O
.	O
to	O
determine	O
the	O
corresponding	O
weight	O
associ-	O
ated	O
with	O
a	O
sample	O
z	O
(	O
l	O
)	O
,	O
we	O
note	O
that	O
the	O
sampling	O
distribution	O
(	O
cid:4	O
)	O
q	O
(	O
z	O
)	O
is	O
uniform	O
over	O
the	O
possible	O
choices	O
for	O
z	O
,	O
and	O
that	O
(	O
cid:4	O
)	O
p	O
(	O
z|x	O
)	O
=	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
,	O
where	O
x	O
denotes	O
the	O
subset	O
of	O
variables	O
that	O
are	O
observed	O
,	O
and	O
the	O
equality	O
follows	O
from	O
the	O
fact	O
that	O
every	O
sample	O
z	O
that	O
is	O
generated	O
is	O
necessarily	O
consistent	B
with	O
the	O
evidence	O
.	O
thus	O
the	O
weights	O
rl	O
are	O
simply	O
proportional	O
to	O
p	O
(	O
z	O
)	O
.	O
note	O
that	O
the	O
variables	O
can	O
be	O
sampled	O
in	O
any	O
order	O
.	O
this	O
approach	O
can	O
yield	O
poor	O
results	O
if	O
the	O
posterior	O
distribution	O
is	O
far	O
from	O
uniform	O
,	O
as	O
is	O
often	O
the	O
case	O
in	O
practice	O
.	O
an	O
improvement	O
on	O
this	O
approach	O
is	O
called	O
likelihood	B
weighted	I
sampling	I
(	O
fung	O
and	O
chang	O
,	O
1990	O
;	O
shachter	O
and	O
peot	O
,	O
1990	O
)	O
and	O
is	O
based	O
on	O
ancestral	B
sampling	I
of	O
the	O
variables	O
.	O
for	O
each	O
variable	O
in	O
turn	O
,	O
if	O
that	O
variable	O
is	O
in	O
the	O
evidence	O
set	O
,	O
then	O
it	O
is	O
just	O
set	O
to	O
its	O
instantiated	O
value	O
.	O
if	O
it	O
is	O
not	O
in	O
the	O
evidence	O
set	O
,	O
then	O
it	O
is	O
sampled	O
from	O
the	O
conditional	B
distribution	O
p	O
(	O
zi|pai	O
)	O
in	O
which	O
the	O
conditioning	O
variables	O
are	O
set	O
to	O
their	O
currently	O
sampled	O
values	O
.	O
the	O
weighting	O
associated	O
with	O
the	O
resulting	O
sample	O
z	O
is	O
then	O
given	O
by	O
p	O
(	O
zi|pai	O
)	O
=	O
p	O
(	O
zi|pai	O
)	O
.	O
(	O
11.24	O
)	O
(	O
cid:14	O
)	O
zi	O
(	O
cid:9	O
)	O
∈e	O
r	O
(	O
z	O
)	O
=	O
(	O
cid:14	O
)	O
p	O
(	O
zi|pai	O
)	O
p	O
(	O
zi|pai	O
)	O
zi∈e	O
1	O
(	O
cid:14	O
)	O
zi∈e	O
this	O
method	O
can	O
be	O
further	O
extended	B
using	O
self-importance	O
sampling	O
(	O
shachter	O
and	O
peot	O
,	O
1990	O
)	O
in	O
which	O
the	O
importance	B
sampling	I
distribution	O
is	O
continually	O
updated	O
to	O
reﬂect	O
the	O
current	O
estimated	O
posterior	O
distribution	O
.	O
11.1.5	O
sampling-importance-resampling	B
the	O
rejection	B
sampling	I
method	O
discussed	O
in	O
section	O
11.1.2	O
depends	O
in	O
part	O
for	O
its	O
success	O
on	O
the	O
determination	O
of	O
a	O
suitable	O
value	O
for	O
the	O
constant	O
k.	O
for	O
many	O
pairs	O
of	O
distributions	O
p	O
(	O
z	O
)	O
and	O
q	O
(	O
z	O
)	O
,	O
it	O
will	O
be	O
impractical	O
to	O
determine	O
a	O
suitable	O
11.1.	O
basic	O
sampling	O
algorithms	O
535	O
value	O
for	O
k	O
in	O
that	O
any	O
value	O
that	O
is	O
sufﬁciently	O
large	O
to	O
guarantee	O
a	O
bound	O
on	O
the	O
desired	O
distribution	O
will	O
lead	O
to	O
impractically	O
small	O
acceptance	O
rates	O
.	O
as	O
in	O
the	O
case	O
of	O
rejection	B
sampling	I
,	O
the	O
sampling-importance-resampling	B
(	O
sir	O
)	O
approach	O
also	O
makes	O
use	O
of	O
a	O
sampling	O
distribution	O
q	O
(	O
z	O
)	O
but	O
avoids	O
having	O
to	O
de-	O
termine	O
the	O
constant	O
k.	O
there	O
are	O
two	O
stages	O
to	O
the	O
scheme	O
.	O
in	O
the	O
ﬁrst	O
stage	O
,	O
l	O
samples	O
z	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
z	O
(	O
l	O
)	O
are	O
drawn	O
from	O
q	O
(	O
z	O
)	O
.	O
then	O
in	O
the	O
second	O
stage	O
,	O
weights	O
w1	O
,	O
.	O
.	O
.	O
,	O
wl	O
are	O
constructed	O
using	O
(	O
11.23	O
)	O
.	O
finally	O
,	O
a	O
second	O
set	O
of	O
l	O
samples	O
is	O
drawn	O
from	O
the	O
discrete	O
distribution	O
(	O
z	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
z	O
(	O
l	O
)	O
)	O
with	O
probabilities	O
given	O
by	O
the	O
weights	O
(	O
w1	O
,	O
.	O
.	O
.	O
,	O
wl	O
)	O
.	O
the	O
resulting	O
l	O
samples	O
are	O
only	O
approximately	O
distributed	O
according	O
to	O
p	O
(	O
z	O
)	O
,	O
but	O
the	O
distribution	O
becomes	O
correct	O
in	O
the	O
limit	O
l	O
→	O
∞	O
.	O
to	O
see	O
this	O
,	O
consider	O
the	O
univariate	O
case	O
,	O
and	O
note	O
that	O
the	O
cumulative	O
distribution	O
of	O
the	O
resampled	O
values	O
is	O
given	O
by	O
(	O
cid:2	O
)	O
(	O
cid:5	O
)	O
l	O
i	O
(	O
z	O
(	O
l	O
)	O
(	O
cid:1	O
)	O
a	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
(	O
l	O
)	O
)	O
/q	O
(	O
z	O
(	O
l	O
)	O
)	O
(	O
cid:5	O
)	O
l	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
(	O
l	O
)	O
)	O
/q	O
(	O
z	O
(	O
l	O
)	O
)	O
l	O
:	O
z	O
(	O
l	O
)	O
(	O
cid:1	O
)	O
a	O
wl	O
p	O
(	O
z	O
(	O
cid:1	O
)	O
a	O
)	O
=	O
=	O
(	O
11.25	O
)	O
where	O
i	O
(	O
.	O
)	O
is	O
the	O
indicator	O
function	O
(	O
which	O
equals	O
1	O
if	O
its	O
argument	O
is	O
true	O
and	O
0	O
otherwise	O
)	O
.	O
taking	O
the	O
limit	O
l	O
→	O
∞	O
,	O
and	O
assuming	O
suitable	O
regularity	O
of	O
the	O
dis-	O
tributions	O
,	O
we	O
can	O
replace	O
the	O
sums	O
by	O
integrals	O
weighted	O
according	O
to	O
the	O
original	O
sampling	O
distribution	O
q	O
(	O
z	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
i	O
(	O
z	O
(	O
cid:1	O
)	O
a	O
)	O
{	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
/q	O
(	O
z	O
)	O
}	O
q	O
(	O
z	O
)	O
dz	O
(	O
cid:6	O
)	O
{	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
/q	O
(	O
z	O
)	O
}	O
q	O
(	O
z	O
)	O
dz	O
i	O
(	O
z	O
(	O
cid:1	O
)	O
a	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
dz	O
(	O
cid:6	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
dz	O
p	O
(	O
z	O
(	O
cid:1	O
)	O
a	O
)	O
=	O
=	O
=	O
i	O
(	O
z	O
(	O
cid:1	O
)	O
a	O
)	O
p	O
(	O
z	O
)	O
dz	O
(	O
11.26	O
)	O
which	O
is	O
the	O
cumulative	B
distribution	I
function	I
of	O
p	O
(	O
z	O
)	O
.	O
again	O
,	O
we	O
see	O
that	O
the	O
normal-	O
ization	O
of	O
p	O
(	O
z	O
)	O
is	O
not	O
required	O
.	O
for	O
a	O
ﬁnite	O
value	O
of	O
l	O
,	O
and	O
a	O
given	O
initial	O
sample	O
set	O
,	O
the	O
resampled	O
values	O
will	O
only	O
approximately	O
be	O
drawn	O
from	O
the	O
desired	O
distribution	O
.	O
as	O
with	O
rejection	O
sam-	O
pling	O
,	O
the	O
approximation	O
improves	O
as	O
the	O
sampling	O
distribution	O
q	O
(	O
z	O
)	O
gets	O
closer	O
to	O
the	O
desired	O
distribution	O
p	O
(	O
z	O
)	O
.	O
when	O
q	O
(	O
z	O
)	O
=	O
p	O
(	O
z	O
)	O
,	O
the	O
initial	O
samples	O
(	O
z	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
z	O
(	O
l	O
)	O
)	O
have	O
the	O
desired	O
distribution	O
,	O
and	O
the	O
weights	O
wn	O
=	O
1/l	O
so	O
that	O
the	O
resampled	O
values	O
also	O
have	O
the	O
desired	O
distribution	O
.	O
if	O
moments	O
with	O
respect	O
to	O
the	O
distribution	O
p	O
(	O
z	O
)	O
are	O
required	O
,	O
then	O
they	O
can	O
be	O
536	O
11.	O
sampling	B
methods	I
evaluated	O
directly	O
using	O
the	O
original	O
samples	O
together	O
with	O
the	O
weights	O
,	O
because	O
e	O
[	O
f	O
(	O
z	O
)	O
]	O
=	O
f	O
(	O
z	O
)	O
p	O
(	O
z	O
)	O
dz	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
f	O
(	O
z	O
)	O
[	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
/q	O
(	O
z	O
)	O
]	O
q	O
(	O
z	O
)	O
dz	O
(	O
cid:6	O
)	O
[	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
/q	O
(	O
z	O
)	O
]	O
q	O
(	O
z	O
)	O
dz	O
(	O
cid:7	O
)	O
l	O
(	O
cid:2	O
)	O
wlf	O
(	O
zl	O
)	O
.	O
=	O
l=1	O
(	O
11.27	O
)	O
11.1.6	O
sampling	O
and	O
the	O
em	O
algorithm	O
in	O
addition	O
to	O
providing	O
a	O
mechanism	O
for	O
direct	O
implementation	O
of	O
the	O
bayesian	O
framework	O
,	O
monte	O
carlo	O
methods	O
can	O
also	O
play	O
a	O
role	O
in	O
the	O
frequentist	B
paradigm	O
,	O
for	O
example	O
to	O
ﬁnd	O
maximum	B
likelihood	I
solutions	O
.	O
in	O
particular	O
,	O
sampling	B
methods	I
can	O
be	O
used	O
to	O
approximate	O
the	O
e	O
step	O
of	O
the	O
em	O
algorithm	O
for	O
models	O
in	O
which	O
the	O
e	O
step	O
can	O
not	O
be	O
performed	O
analytically	O
.	O
consider	O
a	O
model	O
with	O
hidden	O
variables	O
z	O
,	O
visible	O
(	O
observed	O
)	O
variables	O
x	O
,	O
and	O
parameters	O
θ.	O
the	O
function	O
that	O
is	O
optimized	O
with	O
respect	O
to	O
θ	O
in	O
the	O
m	O
step	O
is	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
,	O
given	O
by	O
(	O
cid:6	O
)	O
q	O
(	O
θ	O
,	O
θold	O
)	O
=	O
p	O
(	O
z|x	O
,	O
θold	O
)	O
ln	O
p	O
(	O
z	O
,	O
x|θ	O
)	O
dz	O
.	O
(	O
11.28	O
)	O
we	O
can	O
use	O
sampling	B
methods	I
to	O
approximate	O
this	O
integral	O
by	O
a	O
ﬁnite	O
sum	O
over	O
sam-	O
ples	O
{	O
z	O
(	O
l	O
)	O
}	O
,	O
which	O
are	O
drawn	O
from	O
the	O
current	O
estimate	O
for	O
the	O
posterior	O
distribution	O
p	O
(	O
z|x	O
,	O
θold	O
)	O
,	O
so	O
that	O
q	O
(	O
θ	O
,	O
θold	O
)	O
(	O
cid:7	O
)	O
1	O
l	O
ln	O
p	O
(	O
z	O
(	O
l	O
)	O
,	O
x|θ	O
)	O
.	O
(	O
11.29	O
)	O
l	O
(	O
cid:2	O
)	O
l=1	O
the	O
q	O
function	O
is	O
then	O
optimized	O
in	O
the	O
usual	O
way	O
in	O
the	O
m	O
step	O
.	O
this	O
procedure	O
is	O
called	O
the	O
monte	O
carlo	O
em	O
algorithm	O
.	O
it	O
is	O
straightforward	O
to	O
extend	O
this	O
to	O
the	O
problem	O
of	O
ﬁnding	O
the	O
mode	O
of	O
the	O
posterior	O
distribution	O
over	O
θ	O
(	O
the	O
map	O
estimate	O
)	O
when	O
a	O
prior	B
distribution	O
p	O
(	O
θ	O
)	O
has	O
been	O
deﬁned	O
,	O
simply	O
by	O
adding	O
ln	O
p	O
(	O
θ	O
)	O
to	O
the	O
function	O
q	O
(	O
θ	O
,	O
θold	O
)	O
before	O
performing	O
the	O
m	O
step	O
.	O
a	O
particular	O
instance	O
of	O
the	O
monte	O
carlo	O
em	O
algorithm	O
,	O
called	O
stochastic	B
em	O
,	O
arises	O
if	O
we	O
consider	O
a	O
ﬁnite	O
mixture	O
model	O
,	O
and	O
draw	O
just	O
one	O
sample	O
at	O
each	O
e	O
step	O
.	O
here	O
the	O
latent	B
variable	I
z	O
characterizes	O
which	O
of	O
the	O
k	O
components	O
of	O
the	O
mixture	B
is	O
responsible	O
for	O
generating	O
each	O
data	O
point	O
.	O
in	O
the	O
e	O
step	O
,	O
a	O
sample	O
of	O
z	O
is	O
taken	O
from	O
the	O
posterior	O
distribution	O
p	O
(	O
z|x	O
,	O
θold	O
)	O
where	O
x	O
is	O
the	O
data	O
set	O
.	O
this	O
effectively	O
makes	O
a	O
hard	O
assignment	O
of	O
each	O
data	O
point	O
to	O
one	O
of	O
the	O
components	O
in	O
the	O
mixture	B
.	O
in	O
the	O
m	O
step	O
,	O
this	O
sampled	O
approximation	O
to	O
the	O
posterior	O
distribution	O
is	O
used	O
to	O
update	O
the	O
model	O
parameters	O
in	O
the	O
usual	O
way	O
.	O
11.2.	O
markov	O
chain	O
monte	O
carlo	O
537	O
now	O
suppose	O
we	O
move	O
from	O
a	O
maximum	B
likelihood	I
approach	O
to	O
a	O
full	O
bayesian	O
treatment	O
in	O
which	O
we	O
wish	O
to	O
sample	O
from	O
the	O
posterior	O
distribution	O
over	O
the	O
param-	O
eter	O
vector	O
θ.	O
in	O
principle	O
,	O
we	O
would	O
like	O
to	O
draw	O
samples	O
from	O
the	O
joint	O
posterior	O
p	O
(	O
θ	O
,	O
z|x	O
)	O
,	O
but	O
we	O
shall	O
suppose	O
that	O
this	O
is	O
computationally	O
difﬁcult	O
.	O
suppose	O
fur-	O
ther	O
that	O
it	O
is	O
relatively	O
straightforward	O
to	O
sample	O
from	O
the	O
complete-data	O
parameter	O
posterior	O
p	O
(	O
θ|z	O
,	O
x	O
)	O
.	O
this	O
inspires	O
the	O
data	B
augmentation	I
algorithm	O
,	O
which	O
alter-	O
nates	O
between	O
two	O
steps	O
known	O
as	O
the	O
i-step	O
(	O
imputation	B
step	I
,	O
analogous	O
to	O
an	O
e	O
step	O
)	O
and	O
the	O
p-step	O
(	O
posterior	B
step	I
,	O
analogous	O
to	O
an	O
m	O
step	O
)	O
.	O
ip	O
algorithm	O
i-step	O
.	O
we	O
wish	O
to	O
sample	O
from	O
p	O
(	O
z|x	O
)	O
but	O
we	O
can	O
not	O
do	O
this	O
directly	O
.	O
we	O
therefore	O
note	O
the	O
relation	O
p	O
(	O
z|x	O
)	O
=	O
p	O
(	O
z|θ	O
,	O
x	O
)	O
p	O
(	O
θ|x	O
)	O
dθ	O
(	O
11.30	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
l	O
(	O
cid:2	O
)	O
l=1	O
and	O
hence	O
for	O
l	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
l	O
we	O
ﬁrst	O
draw	O
a	O
sample	O
θ	O
(	O
l	O
)	O
from	O
the	O
current	O
esti-	O
mate	O
for	O
p	O
(	O
θ|x	O
)	O
,	O
and	O
then	O
use	O
this	O
to	O
draw	O
a	O
sample	O
z	O
(	O
l	O
)	O
from	O
p	O
(	O
z|θ	O
(	O
l	O
)	O
,	O
x	O
)	O
.	O
p-step	O
.	O
given	O
the	O
relation	O
p	O
(	O
θ|x	O
)	O
=	O
p	O
(	O
θ|z	O
,	O
x	O
)	O
p	O
(	O
z|x	O
)	O
dz	O
(	O
11.31	O
)	O
we	O
use	O
the	O
samples	O
{	O
z	O
(	O
l	O
)	O
}	O
obtained	O
from	O
the	O
i-step	O
to	O
compute	O
a	O
revised	O
estimate	O
of	O
the	O
posterior	O
distribution	O
over	O
θ	O
given	O
by	O
p	O
(	O
θ|x	O
)	O
(	O
cid:7	O
)	O
1	O
l	O
p	O
(	O
θ|z	O
(	O
l	O
)	O
,	O
x	O
)	O
.	O
(	O
11.32	O
)	O
by	O
assumption	O
,	O
it	O
will	O
be	O
feasible	O
to	O
sample	O
from	O
this	O
approximation	O
in	O
the	O
i-step	O
.	O
note	O
that	O
we	O
are	O
making	O
a	O
(	O
somewhat	O
artiﬁcial	O
)	O
distinction	O
between	O
parameters	O
θ	O
and	O
hidden	O
variables	O
z.	O
from	O
now	O
on	O
,	O
we	O
blur	O
this	O
distinction	O
and	O
focus	O
simply	O
on	O
the	O
problem	O
of	O
drawing	O
samples	O
from	O
a	O
given	O
posterior	O
distribution	O
.	O
11.2.	O
markov	O
chain	O
monte	O
carlo	O
in	O
the	O
previous	O
section	O
,	O
we	O
discussed	O
the	O
rejection	B
sampling	I
and	O
importance	O
sam-	O
pling	O
strategies	O
for	O
evaluating	O
expectations	O
of	O
functions	O
,	O
and	O
we	O
saw	O
that	O
they	O
suffer	O
from	O
severe	O
limitations	O
particularly	O
in	O
spaces	O
of	O
high	O
dimensionality	O
.	O
we	O
therefore	O
turn	O
in	O
this	O
section	O
to	O
a	O
very	O
general	O
and	O
powerful	O
framework	O
called	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
,	O
which	O
allows	O
sampling	O
from	O
a	O
large	O
class	O
of	O
distributions	O
,	O
538	O
11.	O
sampling	B
methods	I
and	O
which	O
scales	O
well	O
with	O
the	O
dimensionality	O
of	O
the	O
sample	O
space	O
.	O
markov	O
chain	O
monte	O
carlo	O
methods	O
have	O
their	O
origins	O
in	O
physics	O
(	O
metropolis	O
and	O
ulam	O
,	O
1949	O
)	O
,	O
and	O
it	O
was	O
only	O
towards	O
the	O
end	O
of	O
the	O
1980s	O
that	O
they	O
started	O
to	O
have	O
a	O
signiﬁcant	O
impact	O
in	O
the	O
ﬁeld	O
of	O
statistics	O
.	O
as	O
with	O
rejection	O
and	O
importance	B
sampling	I
,	O
we	O
again	O
sample	O
from	O
a	O
proposal	B
distribution	I
.	O
this	O
time	O
,	O
however	O
,	O
we	O
maintain	O
a	O
record	O
of	O
the	O
current	O
state	O
z	O
(	O
τ	O
)	O
,	O
and	O
the	O
proposal	B
distribution	I
q	O
(	O
z|z	O
(	O
τ	O
)	O
)	O
depends	O
on	O
this	O
current	O
state	O
,	O
and	O
so	O
the	O
sequence	O
of	O
samples	O
z	O
(	O
1	O
)	O
,	O
z	O
(	O
2	O
)	O
,	O
.	O
.	O
.	O
forms	O
a	O
markov	O
chain	O
.	O
again	O
,	O
if	O
we	O
write	O
p	O
(	O
z	O
)	O
=	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
/zp	O
,	O
we	O
will	O
assume	O
that	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
can	O
readily	O
be	O
evaluated	O
for	O
any	O
given	O
value	O
of	O
z	O
,	O
although	O
section	O
11.2.1	O
the	O
value	O
of	O
zp	O
may	O
be	O
unknown	O
.	O
the	O
proposal	B
distribution	I
itself	O
is	O
chosen	O
to	O
be	O
sufﬁciently	O
simple	O
that	O
it	O
is	O
straightforward	O
to	O
draw	O
samples	O
from	O
it	O
directly	O
.	O
at	O
each	O
cycle	O
of	O
the	O
algorithm	O
,	O
we	O
generate	O
a	O
candidate	O
sample	O
z	O
(	O
cid:1	O
)	O
from	O
the	O
proposal	B
distribution	I
and	O
then	O
accept	O
the	O
sample	O
according	O
to	O
an	O
appropriate	O
criterion	O
.	O
in	O
the	O
basic	O
metropolis	O
algorithm	O
(	O
metropolis	O
et	O
al.	O
,	O
1953	O
)	O
,	O
we	O
assume	O
that	O
the	O
proposal	B
distribution	I
is	O
symmetric	O
,	O
that	O
is	O
q	O
(	O
za|zb	O
)	O
=	O
q	O
(	O
zb|za	O
)	O
for	O
all	O
values	O
of	O
za	O
and	O
zb	O
.	O
the	O
candidate	O
sample	O
is	O
then	O
accepted	O
with	O
probability	B
a	O
(	O
z	O
(	O
cid:1	O
)	O
,	O
z	O
(	O
τ	O
)	O
)	O
=	O
min	O
1	O
,	O
.	O
(	O
11.33	O
)	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
(	O
cid:1	O
)	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
(	O
τ	O
)	O
)	O
this	O
can	O
be	O
achieved	O
by	O
choosing	O
a	O
random	O
number	O
u	O
with	O
uniform	B
distribution	I
over	O
the	O
unit	O
interval	O
(	O
0	O
,	O
1	O
)	O
and	O
then	O
accepting	O
the	O
sample	O
if	O
a	O
(	O
z	O
(	O
cid:1	O
)	O
,	O
z	O
(	O
τ	O
)	O
)	O
>	O
u.	O
note	O
that	O
if	O
the	O
step	O
from	O
z	O
(	O
τ	O
)	O
to	O
z	O
(	O
cid:1	O
)	O
causes	O
an	O
increase	O
in	O
the	O
value	O
of	O
p	O
(	O
z	O
)	O
,	O
then	O
the	O
candidate	O
point	O
is	O
certain	O
to	O
be	O
kept	O
.	O
if	O
the	O
candidate	O
sample	O
is	O
accepted	O
,	O
then	O
z	O
(	O
τ	O
+1	O
)	O
=	O
z	O
(	O
cid:1	O
)	O
,	O
otherwise	O
the	O
candidate	O
point	O
z	O
(	O
cid:1	O
)	O
is	O
discarded	O
,	O
z	O
(	O
τ	O
+1	O
)	O
is	O
set	O
to	O
z	O
(	O
τ	O
)	O
and	O
another	O
candidate	O
sample	O
is	O
drawn	O
from	O
the	O
distribution	O
q	O
(	O
z|z	O
(	O
τ	O
+1	O
)	O
)	O
.	O
this	O
is	O
in	O
contrast	O
to	O
rejection	B
sampling	I
,	O
where	O
re-	O
jected	O
samples	O
are	O
simply	O
discarded	O
.	O
in	O
the	O
metropolis	O
algorithm	O
when	O
a	O
candidate	O
point	O
is	O
rejected	O
,	O
the	O
previous	O
sample	O
is	O
included	O
instead	O
in	O
the	O
ﬁnal	O
list	O
of	O
samples	O
,	O
leading	O
to	O
multiple	O
copies	O
of	O
samples	O
.	O
of	O
course	O
,	O
in	O
a	O
practical	O
implementation	O
,	O
only	O
a	O
single	O
copy	O
of	O
each	O
retained	O
sample	O
would	O
be	O
kept	O
,	O
along	O
with	O
an	O
integer	O
weighting	O
factor	O
recording	O
how	O
many	O
times	O
that	O
state	O
appears	O
.	O
as	O
we	O
shall	O
see	O
,	O
as	O
long	O
as	O
q	O
(	O
za|zb	O
)	O
is	O
positive	O
for	O
any	O
values	O
of	O
za	O
and	O
zb	O
(	O
this	O
is	O
a	O
sufﬁcient	O
but	O
not	O
necessary	O
condition	O
)	O
,	O
the	O
distribution	O
of	O
z	O
(	O
τ	O
)	O
tends	O
to	O
p	O
(	O
z	O
)	O
as	O
τ	O
→	O
∞	O
.	O
it	O
should	O
be	O
emphasized	O
,	O
however	O
,	O
that	O
the	O
sequence	O
z	O
(	O
1	O
)	O
,	O
z	O
(	O
2	O
)	O
,	O
.	O
.	O
.	O
is	O
not	O
a	O
set	O
of	O
independent	B
samples	O
from	O
p	O
(	O
z	O
)	O
because	O
successive	O
samples	O
are	O
highly	O
correlated	O
.	O
if	O
we	O
wish	O
to	O
obtain	O
independent	B
samples	O
,	O
then	O
we	O
can	O
discard	O
most	O
of	O
the	O
sequence	O
and	O
just	O
re-	O
tain	O
every	O
m	O
th	O
sample	O
.	O
for	O
m	O
sufﬁciently	O
large	O
,	O
the	O
retained	O
samples	O
will	O
for	O
all	O
practical	O
purposes	O
be	O
independent	B
.	O
figure	O
11.9	O
shows	O
a	O
simple	O
illustrative	O
exam-	O
ple	O
of	O
sampling	O
from	O
a	O
two-dimensional	O
gaussian	O
distribution	O
using	O
the	O
metropolis	O
algorithm	O
in	O
which	O
the	O
proposal	B
distribution	I
is	O
an	O
isotropic	B
gaussian	O
.	O
further	O
insight	O
into	O
the	O
nature	O
of	O
markov	O
chain	O
monte	O
carlo	O
algorithms	O
can	O
be	O
gleaned	O
by	O
looking	O
at	O
the	O
properties	O
of	O
a	O
speciﬁc	O
example	O
,	O
namely	O
a	O
simple	O
random	O
11.2.	O
markov	O
chain	O
monte	O
carlo	O
539	O
figure	O
11.9	O
a	O
simple	O
illustration	O
using	O
metropo-	O
lis	O
algorithm	O
to	O
sample	O
from	O
a	O
gaussian	O
distribution	O
whose	O
one	O
standard-deviation	O
contour	O
is	O
shown	O
by	O
the	O
ellipse	O
.	O
the	O
proposal	O
distribu-	O
tion	O
is	O
an	O
isotropic	B
gaussian	O
distri-	O
bution	O
whose	O
standard	B
deviation	I
is	O
0.2.	O
steps	O
that	O
are	O
accepted	O
are	O
shown	O
as	O
green	O
lines	O
,	O
and	O
rejected	O
steps	O
are	O
shown	O
in	O
red	O
.	O
a	O
total	O
of	O
150	O
candidate	O
samples	O
are	O
gener-	O
ated	O
,	O
of	O
which	O
43	O
are	O
rejected	O
.	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
0	O
0.5	O
1	O
1.5	O
2	O
2.5	O
3	O
exercise	O
11.10	O
walk	O
.	O
consider	O
a	O
state	O
space	O
z	O
consisting	O
of	O
the	O
integers	O
,	O
with	O
probabilities	O
p	O
(	O
z	O
(	O
τ	O
+1	O
)	O
=	O
z	O
(	O
τ	O
)	O
)	O
=	O
0.5	O
p	O
(	O
z	O
(	O
τ	O
+1	O
)	O
=	O
z	O
(	O
τ	O
)	O
+	O
1	O
)	O
=	O
0.25	O
p	O
(	O
z	O
(	O
τ	O
+1	O
)	O
=	O
z	O
(	O
τ	O
)	O
−	O
1	O
)	O
=	O
0.25	O
(	O
11.34	O
)	O
(	O
11.35	O
)	O
(	O
11.36	O
)	O
where	O
z	O
(	O
τ	O
)	O
denotes	O
the	O
state	O
at	O
step	O
τ	O
.	O
if	O
the	O
initial	O
state	O
is	O
z	O
(	O
1	O
)	O
=	O
0	O
,	O
then	O
by	O
sym-	O
metry	O
the	O
expected	O
state	O
at	O
time	O
τ	O
will	O
also	O
be	O
zero	O
e	O
[	O
z	O
(	O
τ	O
)	O
]	O
=	O
0	O
,	O
and	O
similarly	O
it	O
is	O
easily	O
seen	O
that	O
e	O
[	O
(	O
z	O
(	O
τ	O
)	O
)	O
2	O
]	O
=	O
τ	O
/2	O
.	O
thus	O
after	O
τ	O
steps	O
,	O
the	O
random	O
walk	O
has	O
only	O
trav-	O
elled	O
a	O
distance	O
that	O
on	O
average	O
is	O
proportional	O
to	O
the	O
square	O
root	O
of	O
τ	O
.	O
this	O
square	O
root	O
dependence	O
is	O
typical	O
of	O
random	O
walk	O
behaviour	O
and	O
shows	O
that	O
random	O
walks	O
are	O
very	O
inefﬁcient	O
in	O
exploring	O
the	O
state	O
space	O
.	O
as	O
we	O
shall	O
see	O
,	O
a	O
central	O
goal	O
in	O
designing	O
markov	O
chain	O
monte	O
carlo	O
methods	O
is	O
to	O
avoid	O
random	O
walk	O
behaviour	O
.	O
11.2.1	O
markov	O
chains	O
before	O
discussing	O
markov	O
chain	O
monte	O
carlo	O
methods	O
in	O
more	O
detail	O
,	O
it	O
is	O
use-	O
ful	O
to	O
study	O
some	O
general	O
properties	O
of	O
markov	O
chains	O
in	O
more	O
detail	O
.	O
in	O
particular	O
,	O
we	O
ask	O
under	O
what	O
circumstances	O
will	O
a	O
markov	O
chain	O
converge	O
to	O
the	O
desired	O
dis-	O
tribution	O
.	O
a	O
ﬁrst-order	O
markov	O
chain	O
is	O
deﬁned	O
to	O
be	O
a	O
series	O
of	O
random	O
variables	O
z	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
z	O
(	O
m	O
)	O
such	O
that	O
the	O
following	O
conditional	B
independence	I
property	O
holds	O
for	O
m	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
−	O
1	O
}	O
p	O
(	O
z	O
(	O
m+1	O
)	O
|z	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
z	O
(	O
m	O
)	O
)	O
=	O
p	O
(	O
z	O
(	O
m+1	O
)	O
|z	O
(	O
m	O
)	O
)	O
.	O
(	O
11.37	O
)	O
this	O
of	O
course	O
can	O
be	O
represented	O
as	O
a	O
directed	B
graph	O
in	O
the	O
form	O
of	O
a	O
chain	O
,	O
an	O
ex-	O
ample	O
of	O
which	O
is	O
shown	O
in	O
figure	O
8.38.	O
we	O
can	O
then	O
specify	O
the	O
markov	O
chain	O
by	O
giving	O
the	O
probability	B
distribution	O
for	O
the	O
initial	O
variable	O
p	O
(	O
z	O
(	O
0	O
)	O
)	O
together	O
with	O
the	O
540	O
11.	O
sampling	B
methods	I
(	O
cid:2	O
)	O
conditional	B
probabilities	O
for	O
subsequent	O
variables	O
in	O
the	O
form	O
of	O
transition	O
probabil-	O
ities	O
tm	O
(	O
z	O
(	O
m	O
)	O
,	O
z	O
(	O
m+1	O
)	O
)	O
≡	O
p	O
(	O
z	O
(	O
m+1	O
)	O
|z	O
(	O
m	O
)	O
)	O
.	O
a	O
markov	O
chain	O
is	O
called	O
homogeneous	B
if	O
the	O
transition	O
probabilities	O
are	O
the	O
same	O
for	O
all	O
m.	O
the	O
marginal	B
probability	I
for	O
a	O
particular	O
variable	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
marginal	B
probability	I
for	O
the	O
previous	O
variable	O
in	O
the	O
chain	O
in	O
the	O
form	O
p	O
(	O
z	O
(	O
m+1	O
)	O
)	O
=	O
p	O
(	O
z	O
(	O
m+1	O
)	O
|z	O
(	O
m	O
)	O
)	O
p	O
(	O
z	O
(	O
m	O
)	O
)	O
.	O
(	O
11.38	O
)	O
z	O
(	O
m	O
)	O
a	O
distribution	O
is	O
said	O
to	O
be	O
invariant	O
,	O
or	O
stationary	B
,	O
with	O
respect	O
to	O
a	O
markov	O
chain	O
if	O
each	O
step	O
in	O
the	O
chain	O
leaves	O
that	O
distribution	O
invariant	O
.	O
thus	O
,	O
for	O
a	O
homogeneous	B
markov	O
chain	O
with	O
transition	O
probabilities	O
t	O
(	O
z	O
(	O
cid:4	O
)	O
,	O
z	O
)	O
,	O
the	O
distribution	O
p	O
(	O
cid:1	O
)	O
(	O
z	O
)	O
is	O
invariant	O
if	O
,	O
z	O
)	O
p	O
(	O
cid:1	O
)	O
(	O
z	O
(	O
cid:4	O
)	O
)	O
.	O
t	O
(	O
z	O
(	O
cid:4	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
cid:1	O
)	O
(	O
z	O
)	O
=	O
(	O
11.39	O
)	O
z	O
(	O
cid:1	O
)	O
note	O
that	O
a	O
given	O
markov	O
chain	O
may	O
have	O
more	O
than	O
one	O
invariant	O
distribution	O
.	O
for	O
instance	O
,	O
if	O
the	O
transition	O
probabilities	O
are	O
given	O
by	O
the	O
identity	O
transformation	O
,	O
then	O
any	O
distribution	O
will	O
be	O
invariant	O
.	O
a	O
sufﬁcient	O
(	O
but	O
not	O
necessary	O
)	O
condition	O
for	O
ensuring	O
that	O
the	O
required	O
distribu-	O
tion	O
p	O
(	O
z	O
)	O
is	O
invariant	O
is	O
to	O
choose	O
the	O
transition	O
probabilities	O
to	O
satisfy	O
the	O
property	O
of	O
detailed	O
balance	O
,	O
deﬁned	O
by	O
p	O
(	O
cid:1	O
)	O
(	O
z	O
)	O
t	O
(	O
z	O
,	O
z	O
(	O
cid:4	O
)	O
)	O
=	O
p	O
(	O
cid:1	O
)	O
(	O
z	O
(	O
cid:4	O
)	O
)	O
t	O
(	O
z	O
(	O
cid:4	O
)	O
,	O
z	O
)	O
(	O
11.40	O
)	O
for	O
the	O
particular	O
distribution	O
p	O
(	O
cid:1	O
)	O
(	O
z	O
)	O
.	O
it	O
is	O
easily	O
seen	O
that	O
a	O
transition	B
probability	I
that	O
satisﬁes	O
detailed	O
balance	O
with	O
respect	O
to	O
a	O
particular	O
distribution	O
will	O
leave	O
that	O
distribution	O
invariant	O
,	O
because	O
p	O
(	O
cid:1	O
)	O
(	O
z	O
(	O
cid:4	O
)	O
)	O
t	O
(	O
z	O
(	O
cid:4	O
)	O
,	O
z	O
)	O
=	O
p	O
(	O
cid:1	O
)	O
(	O
z	O
)	O
t	O
(	O
z	O
,	O
z	O
(	O
cid:4	O
)	O
)	O
=	O
p	O
(	O
cid:1	O
)	O
(	O
z	O
)	O
p	O
(	O
z	O
(	O
cid:4	O
)	O
|z	O
)	O
=	O
p	O
(	O
cid:1	O
)	O
(	O
z	O
)	O
.	O
(	O
11.41	O
)	O
(	O
cid:2	O
)	O
z	O
(	O
cid:1	O
)	O
(	O
cid:2	O
)	O
z	O
(	O
cid:1	O
)	O
(	O
cid:2	O
)	O
z	O
(	O
cid:1	O
)	O
a	O
markov	O
chain	O
that	O
respects	O
detailed	O
balance	O
is	O
said	O
to	O
be	O
reversible	O
.	O
our	O
goal	O
is	O
to	O
use	O
markov	O
chains	O
to	O
sample	O
from	O
a	O
given	O
distribution	O
.	O
we	O
can	O
achieve	O
this	O
if	O
we	O
set	O
up	O
a	O
markov	O
chain	O
such	O
that	O
the	O
desired	O
distribution	O
is	O
invariant	O
.	O
however	O
,	O
we	O
must	O
also	O
require	O
that	O
for	O
m	O
→	O
∞	O
,	O
the	O
distribution	O
p	O
(	O
z	O
(	O
m	O
)	O
)	O
converges	O
to	O
the	O
required	O
invariant	O
distribution	O
p	O
(	O
cid:1	O
)	O
(	O
z	O
)	O
,	O
irrespective	O
of	O
the	O
choice	O
of	O
initial	O
dis-	O
tribution	O
p	O
(	O
z	O
(	O
0	O
)	O
)	O
.	O
this	O
property	O
is	O
called	O
ergodicity	O
,	O
and	O
the	O
invariant	O
distribution	O
is	O
then	O
called	O
the	O
equilibrium	O
distribution	O
.	O
clearly	O
,	O
an	O
ergodic	O
markov	O
chain	O
can	O
have	O
only	O
one	O
equilibrium	O
distribution	O
.	O
it	O
can	O
be	O
shown	O
that	O
a	O
homogeneous	B
markov	O
chain	O
will	O
be	O
ergodic	O
,	O
subject	O
only	O
to	O
weak	O
restrictions	O
on	O
the	O
invariant	O
distribution	O
and	O
the	O
transition	O
probabilities	O
(	O
neal	O
,	O
1993	O
)	O
.	O
in	O
practice	O
we	O
often	O
construct	O
the	O
transition	O
probabilities	O
from	O
a	O
set	O
of	O
‘	O
base	O
’	O
transitions	O
b1	O
,	O
.	O
.	O
.	O
,	O
bk	O
.	O
this	O
can	O
be	O
achieved	O
through	O
a	O
mixture	B
distribution	I
of	O
the	O
form	O
t	O
(	O
z	O
(	O
cid:4	O
)	O
,	O
z	O
)	O
=	O
αkbk	O
(	O
z	O
(	O
cid:4	O
)	O
,	O
z	O
)	O
(	O
11.42	O
)	O
k	O
(	O
cid:2	O
)	O
k=1	O
11.2.	O
markov	O
chain	O
monte	O
carlo	O
541	O
(	O
cid:5	O
)	O
for	O
some	O
set	O
of	O
mixing	O
coefﬁcients	O
α1	O
,	O
.	O
.	O
.	O
,	O
αk	O
satisfying	O
αk	O
(	O
cid:2	O
)	O
0	O
and	O
k	O
αk	O
=	O
1.	O
alternatively	O
,	O
the	O
base	O
transitions	O
may	O
be	O
combined	O
through	O
successive	O
application	O
,	O
so	O
that	O
t	O
(	O
z	O
(	O
cid:4	O
)	O
,	O
z1	O
)	O
.	O
.	O
.	O
bk−1	O
(	O
zk−2	O
,	O
zk−1	O
)	O
bk	O
(	O
zk−1	O
,	O
z	O
)	O
.	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
b1	O
(	O
z	O
(	O
cid:4	O
)	O
,	O
z	O
)	O
=	O
.	O
.	O
.	O
(	O
11.43	O
)	O
z1	O
zn−1	O
if	O
a	O
distribution	O
is	O
invariant	O
with	O
respect	O
to	O
each	O
of	O
the	O
base	O
transitions	O
,	O
then	O
obvi-	O
ously	O
it	O
will	O
also	O
be	O
invariant	O
with	O
respect	O
to	O
either	O
of	O
the	O
t	O
(	O
z	O
(	O
cid:4	O
)	O
,	O
z	O
)	O
given	O
by	O
(	O
11.42	O
)	O
or	O
(	O
11.43	O
)	O
.	O
for	O
the	O
case	O
of	O
the	O
mixture	B
(	O
11.42	O
)	O
,	O
if	O
each	O
of	O
the	O
base	O
transitions	O
sat-	O
isﬁes	O
detailed	O
balance	O
,	O
then	O
the	O
mixture	B
transition	O
t	O
will	O
also	O
satisfy	O
detailed	O
bal-	O
ance	O
.	O
this	O
does	O
not	O
hold	O
for	O
the	O
transition	B
probability	I
constructed	O
using	O
(	O
11.43	O
)	O
,	O
al-	O
though	O
by	O
symmetrizing	O
the	O
order	O
of	O
application	O
of	O
the	O
base	O
transitions	O
,	O
in	O
the	O
form	O
b1	O
,	O
b2	O
,	O
.	O
.	O
.	O
,	O
bk	O
,	O
bk	O
,	O
.	O
.	O
.	O
,	O
b2	O
,	O
b1	O
,	O
detailed	O
balance	O
can	O
be	O
restored	O
.	O
a	O
common	O
ex-	O
ample	O
of	O
the	O
use	O
of	O
composite	O
transition	O
probabilities	O
is	O
where	O
each	O
base	O
transition	O
changes	O
only	O
a	O
subset	O
of	O
the	O
variables	O
.	O
11.2.2	O
the	O
metropolis-hastings	O
algorithm	O
earlier	O
we	O
introduced	O
the	O
basic	O
metropolis	O
algorithm	O
,	O
without	O
actually	O
demon-	O
strating	O
that	O
it	O
samples	O
from	O
the	O
required	O
distribution	O
.	O
before	O
giving	O
a	O
proof	O
,	O
we	O
ﬁrst	O
discuss	O
a	O
generalization	B
,	O
known	O
as	O
the	O
metropolis-hastings	O
algorithm	O
(	O
hast-	O
ings	O
,	O
1970	O
)	O
,	O
to	O
the	O
case	O
where	O
the	O
proposal	B
distribution	I
is	O
no	O
longer	O
a	O
symmetric	O
function	O
of	O
its	O
arguments	O
.	O
in	O
particular	O
at	O
step	O
τ	O
of	O
the	O
algorithm	O
,	O
in	O
which	O
the	O
cur-	O
rent	O
state	O
is	O
z	O
(	O
τ	O
)	O
,	O
we	O
draw	O
a	O
sample	O
z	O
(	O
cid:1	O
)	O
from	O
the	O
distribution	O
qk	O
(	O
z|z	O
(	O
τ	O
)	O
)	O
and	O
then	O
accept	O
it	O
with	O
probability	B
ak	O
(	O
z	O
(	O
cid:1	O
)	O
,	O
zτ	O
)	O
where	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
(	O
cid:1	O
)	O
)	O
qk	O
(	O
z	O
(	O
τ	O
)	O
|z	O
(	O
cid:1	O
)	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
(	O
τ	O
)	O
)	O
qk	O
(	O
z	O
(	O
cid:1	O
)	O
|z	O
(	O
τ	O
)	O
)	O
ak	O
(	O
z	O
(	O
cid:1	O
)	O
,	O
z	O
(	O
τ	O
)	O
)	O
=	O
min	O
1	O
,	O
.	O
(	O
11.44	O
)	O
here	O
k	O
labels	O
the	O
members	O
of	O
the	O
set	O
of	O
possible	O
transitions	O
being	O
considered	O
.	O
again	O
,	O
the	O
evaluation	O
of	O
the	O
acceptance	B
criterion	I
does	O
not	O
require	O
knowledge	O
of	O
the	O
normal-	O
izing	O
constant	O
zp	O
in	O
the	O
probability	B
distribution	O
p	O
(	O
z	O
)	O
=	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
/zp	O
.	O
for	O
a	O
symmetric	O
proposal	B
distribution	I
the	O
metropolis-hastings	O
criterion	O
(	O
11.44	O
)	O
reduces	O
to	O
the	O
stan-	O
dard	O
metropolis	O
criterion	O
given	O
by	O
(	O
11.33	O
)	O
.	O
we	O
can	O
show	O
that	O
p	O
(	O
z	O
)	O
is	O
an	O
invariant	O
distribution	O
of	O
the	O
markov	O
chain	O
deﬁned	O
by	O
the	O
metropolis-hastings	O
algorithm	O
by	O
showing	O
that	O
detailed	O
balance	O
,	O
deﬁned	O
by	O
(	O
11.40	O
)	O
,	O
is	O
satisﬁed	O
.	O
using	O
(	O
11.44	O
)	O
we	O
have	O
p	O
(	O
z	O
)	O
qk	O
(	O
z|z	O
(	O
cid:4	O
)	O
)	O
ak	O
(	O
z	O
(	O
cid:4	O
)	O
,	O
z	O
)	O
=	O
min	O
(	O
p	O
(	O
z	O
)	O
qk	O
(	O
z|z	O
(	O
cid:4	O
)	O
)	O
,	O
p	O
(	O
z	O
(	O
cid:4	O
)	O
)	O
qk	O
(	O
z	O
(	O
cid:4	O
)	O
|z	O
)	O
)	O
=	O
min	O
(	O
p	O
(	O
z	O
(	O
cid:4	O
)	O
)	O
qk	O
(	O
z	O
(	O
cid:4	O
)	O
|z	O
)	O
,	O
p	O
(	O
z	O
)	O
qk	O
(	O
z|z	O
(	O
cid:4	O
)	O
)	O
)	O
=	O
p	O
(	O
z	O
(	O
cid:4	O
)	O
)	O
qk	O
(	O
z	O
(	O
cid:4	O
)	O
|z	O
)	O
ak	O
(	O
z	O
,	O
z	O
(	O
cid:4	O
)	O
)	O
(	O
11.45	O
)	O
as	O
required	O
.	O
the	O
speciﬁc	O
choice	O
of	O
proposal	B
distribution	I
can	O
have	O
a	O
marked	O
effect	O
on	O
the	O
performance	O
of	O
the	O
algorithm	O
.	O
for	O
continuous	O
state	O
spaces	O
,	O
a	O
common	O
choice	O
is	O
a	O
gaussian	O
centred	O
on	O
the	O
current	O
state	O
,	O
leading	O
to	O
an	O
important	O
trade-off	O
in	O
determin-	O
if	O
the	O
variance	B
is	O
small	O
,	O
then	O
the	O
ing	O
the	O
variance	B
parameter	O
of	O
this	O
distribution	O
.	O
542	O
11.	O
sampling	B
methods	I
figure	O
11.10	O
schematic	O
illustration	O
of	O
the	O
use	O
of	O
an	O
isotropic	B
gaussian	O
proposal	B
distribution	I
(	O
blue	O
circle	O
)	O
to	O
sample	O
from	O
a	O
correlated	O
multivariate	O
gaussian	O
distribution	O
(	O
red	O
ellipse	O
)	O
having	O
very	O
different	O
stan-	O
dard	O
deviations	O
in	O
different	O
directions	O
,	O
using	O
the	O
metropolis-hastings	O
algorithm	O
.	O
in	O
order	O
to	O
keep	O
the	O
rejection	O
rate	O
low	O
,	O
the	O
scale	O
ρ	O
of	O
the	O
proposal	B
distribution	I
should	O
be	O
on	O
the	O
order	O
of	O
the	O
smallest	O
standard	B
deviation	I
σmin	O
,	O
which	O
leads	O
to	O
random	O
walk	O
behaviour	O
in	O
which	O
the	O
number	O
of	O
steps	O
sep-	O
arating	O
states	O
that	O
are	O
approximately	O
independent	B
is	O
of	O
order	O
(	O
σmax/σmin	O
)	O
2	O
where	O
σmax	O
is	O
the	O
largest	O
standard	B
deviation	I
.	O
σmin	O
σmax	O
ρ	O
proportion	O
of	O
accepted	O
transitions	O
will	O
be	O
high	O
,	O
but	O
progress	O
through	O
the	O
state	O
space	O
takes	O
the	O
form	O
of	O
a	O
slow	O
random	O
walk	O
leading	O
to	O
long	O
correlation	O
times	O
.	O
however	O
,	O
if	O
the	O
variance	B
parameter	O
is	O
large	O
,	O
then	O
the	O
rejection	O
rate	O
will	O
be	O
high	O
because	O
,	O
in	O
the	O
kind	O
of	O
complex	O
problems	O
we	O
are	O
considering	O
,	O
many	O
of	O
the	O
proposed	O
steps	O
will	O
be	O
to	O
states	O
for	O
which	O
the	O
probability	B
p	O
(	O
z	O
)	O
is	O
low	O
.	O
consider	O
a	O
multivariate	O
distribution	O
p	O
(	O
z	O
)	O
having	O
strong	O
correlations	O
between	O
the	O
components	O
of	O
z	O
,	O
as	O
illustrated	O
in	O
fig-	O
ure	O
11.10.	O
the	O
scale	O
ρ	O
of	O
the	O
proposal	B
distribution	I
should	O
be	O
as	O
large	O
as	O
possible	O
without	O
incurring	O
high	O
rejection	O
rates	O
.	O
this	O
suggests	O
that	O
ρ	O
should	O
be	O
of	O
the	O
same	O
order	O
as	O
the	O
smallest	O
length	O
scale	O
σmin	O
.	O
the	O
system	O
then	O
explores	O
the	O
distribution	O
along	O
the	O
more	O
extended	B
direction	O
by	O
means	O
of	O
a	O
random	O
walk	O
,	O
and	O
so	O
the	O
number	O
of	O
steps	O
to	O
arrive	O
at	O
a	O
state	O
that	O
is	O
more	O
or	O
less	O
independent	B
of	O
the	O
original	O
state	O
is	O
of	O
order	O
(	O
σmax/σmin	O
)	O
2.	O
in	O
fact	O
in	O
two	O
dimensions	O
,	O
the	O
increase	O
in	O
rejection	O
rate	O
as	O
ρ	O
increases	O
is	O
offset	O
by	O
the	O
larger	O
steps	O
sizes	O
of	O
those	O
transitions	O
that	O
are	O
accepted	O
,	O
and	O
more	O
generally	O
for	O
a	O
multivariate	O
gaussian	O
the	O
number	O
of	O
steps	O
required	O
to	O
obtain	O
independent	B
samples	O
scales	O
like	O
(	O
σmax/σ2	O
)	O
2	O
where	O
σ2	O
is	O
the	O
second-smallest	O
stan-	O
dard	O
deviation	O
(	O
neal	O
,	O
1993	O
)	O
.	O
these	O
details	O
aside	O
,	O
it	O
remains	O
the	O
case	O
that	O
if	O
the	O
length	O
scales	O
over	O
which	O
the	O
distributions	O
vary	O
are	O
very	O
different	O
in	O
different	O
directions	O
,	O
then	O
the	O
metropolis	O
hastings	O
algorithm	O
can	O
have	O
very	O
slow	O
convergence	O
.	O
11.3.	O
gibbs	O
sampling	O
gibbs	O
sampling	O
(	O
geman	O
and	O
geman	O
,	O
1984	O
)	O
is	O
a	O
simple	O
and	O
widely	O
applicable	O
markov	O
chain	O
monte	O
carlo	O
algorithm	O
and	O
can	O
be	O
seen	O
as	O
a	O
special	O
case	O
of	O
the	O
metropolis-	O
hastings	O
algorithm	O
.	O
consider	O
the	O
distribution	O
p	O
(	O
z	O
)	O
=	O
p	O
(	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
)	O
from	O
which	O
we	O
wish	O
to	O
sample	O
,	O
and	O
suppose	O
that	O
we	O
have	O
chosen	O
some	O
initial	O
state	O
for	O
the	O
markov	O
chain	O
.	O
each	O
step	O
of	O
the	O
gibbs	O
sampling	O
procedure	O
involves	O
replacing	O
the	O
value	O
of	O
one	O
of	O
the	O
variables	O
by	O
a	O
value	O
drawn	O
from	O
the	O
distribution	O
of	O
that	O
variable	O
conditioned	O
on	O
the	O
values	O
of	O
the	O
remaining	O
variables	O
.	O
thus	O
we	O
replace	O
zi	O
by	O
a	O
value	O
drawn	O
from	O
the	O
distribution	O
p	O
(	O
zi|z\i	O
)	O
,	O
where	O
zi	O
denotes	O
the	O
ith	O
component	O
of	O
z	O
,	O
and	O
z\i	O
denotes	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
but	O
with	O
zi	O
omitted	O
.	O
this	O
procedure	O
is	O
repeated	O
either	O
by	O
cycling	O
through	O
the	O
variables	O
11.3.	O
gibbs	O
sampling	O
543	O
in	O
some	O
particular	O
order	O
or	O
by	O
choosing	O
the	O
variable	O
to	O
be	O
updated	O
at	O
each	O
step	O
at	O
random	O
from	O
some	O
distribution	O
.	O
and	O
at	O
step	O
τ	O
of	O
the	O
algorithm	O
we	O
have	O
selected	O
values	O
z	O
replace	O
z	O
bution	O
for	O
example	O
,	O
suppose	O
we	O
have	O
a	O
distribution	O
p	O
(	O
z1	O
,	O
z2	O
,	O
z3	O
)	O
over	O
three	O
variables	O
,	O
(	O
τ	O
)	O
3	O
.	O
we	O
ﬁrst	O
obtained	O
by	O
sampling	O
from	O
the	O
conditional	B
distri-	O
p	O
(	O
z1|z	O
(	O
τ	O
)	O
1	O
by	O
a	O
new	O
value	O
z	O
(	O
τ	O
+1	O
)	O
1	O
(	O
τ	O
)	O
1	O
,	O
z	O
and	O
z	O
(	O
τ	O
)	O
2	O
,	O
z	O
(	O
τ	O
)	O
3	O
)	O
.	O
(	O
11.46	O
)	O
(	O
τ	O
)	O
2	O
next	O
we	O
replace	O
z	O
distribution	O
(	O
τ	O
)	O
2	O
by	O
a	O
value	O
z	O
(	O
τ	O
+1	O
)	O
2	O
obtained	O
by	O
sampling	O
from	O
the	O
conditional	B
p	O
(	O
z2|z	O
(	O
τ	O
+1	O
)	O
1	O
(	O
τ	O
)	O
3	O
)	O
,	O
z	O
(	O
11.47	O
)	O
so	O
that	O
the	O
new	O
value	O
for	O
z1	O
is	O
used	O
straight	O
away	O
in	O
subsequent	O
sampling	O
steps	O
.	O
then	O
we	O
update	O
z3	O
with	O
a	O
sample	O
z	O
drawn	O
from	O
(	O
τ	O
+1	O
)	O
3	O
p	O
(	O
z3|z	O
(	O
τ	O
+1	O
)	O
1	O
(	O
τ	O
+1	O
)	O
2	O
)	O
,	O
z	O
(	O
11.48	O
)	O
and	O
so	O
on	O
,	O
cycling	O
through	O
the	O
three	O
variables	O
in	O
turn	O
.	O
gibbs	O
sampling	O
1.	O
initialize	O
{	O
zi	O
:	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
}	O
2.	O
for	O
τ	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
:	O
(	O
τ	O
+1	O
)	O
1	O
(	O
τ	O
+1	O
)	O
2	O
–	O
sample	O
z	O
–	O
sample	O
z	O
∼	O
p	O
(	O
z1|z	O
∼	O
p	O
(	O
z2|z	O
...	O
(	O
τ	O
)	O
2	O
,	O
z	O
(	O
τ	O
+1	O
)	O
1	O
(	O
τ	O
)	O
3	O
,	O
.	O
.	O
.	O
,	O
z	O
,	O
z	O
(	O
τ	O
)	O
3	O
,	O
.	O
.	O
.	O
,	O
z	O
(	O
τ	O
)	O
m	O
)	O
.	O
(	O
τ	O
)	O
m	O
)	O
.	O
–	O
sample	O
z	O
...	O
–	O
sample	O
z	O
(	O
τ	O
+1	O
)	O
j	O
∼	O
p	O
(	O
zj|z	O
(	O
τ	O
+1	O
)	O
1	O
,	O
.	O
.	O
.	O
,	O
z	O
(	O
τ	O
+1	O
)	O
j−1	O
,	O
z	O
(	O
τ	O
)	O
j+1	O
,	O
.	O
.	O
.	O
,	O
z	O
(	O
τ	O
)	O
m	O
)	O
.	O
(	O
τ	O
+1	O
)	O
m	O
∼	O
p	O
(	O
zm|z	O
(	O
τ	O
+1	O
)	O
1	O
(	O
τ	O
+1	O
)	O
2	O
,	O
z	O
,	O
.	O
.	O
.	O
,	O
z	O
(	O
τ	O
+1	O
)	O
m−1	O
)	O
.	O
josiah	O
willard	O
gibbs	O
1839–1903	O
gibbs	O
spent	O
almost	O
his	O
entire	O
life	O
liv-	O
ing	O
in	O
a	O
house	O
built	O
by	O
his	O
father	O
in	O
new	O
haven	O
,	O
connecticut	O
.	O
in	O
1863	O
,	O
gibbs	O
was	O
granted	O
the	O
ﬁrst	O
phd	O
in	O
engineering	O
in	O
the	O
united	O
states	O
,	O
and	O
in	O
1871	O
he	O
was	O
appointed	O
to	O
the	O
ﬁrst	O
chair	O
of	O
mathematical	O
physics	O
in	O
the	O
united	O
states	O
at	O
yale	O
,	O
a	O
post	O
for	O
which	O
he	O
received	O
no	O
salary	O
because	O
at	O
the	O
time	O
he	O
had	O
no	O
publications	O
.	O
he	O
de-	O
veloped	O
the	O
ﬁeld	O
of	O
vector	O
analysis	O
and	O
made	O
contri-	O
butions	O
to	O
crystallography	O
and	O
planetary	O
orbits	O
.	O
his	O
most	O
famous	O
work	O
,	O
entitled	O
ontheequilibriumofhet-	O
erogeneous	O
substances	O
,	O
laid	O
the	O
foundations	O
for	O
the	O
science	O
of	O
physical	O
chemistry	O
.	O
544	O
11.	O
sampling	B
methods	I
to	O
show	O
that	O
this	O
procedure	O
samples	O
from	O
the	O
required	O
distribution	O
,	O
we	O
ﬁrst	O
of	O
all	O
note	O
that	O
the	O
distribution	O
p	O
(	O
z	O
)	O
is	O
an	O
invariant	O
of	O
each	O
of	O
the	O
gibbs	O
sampling	O
steps	O
individually	O
and	O
hence	O
of	O
the	O
whole	O
markov	O
chain	O
.	O
this	O
follows	O
from	O
the	O
fact	O
that	O
when	O
we	O
sample	O
from	O
p	O
(	O
zi|	O
{	O
z\i	O
)	O
,	O
the	O
marginal	B
distribution	O
p	O
(	O
z\i	O
)	O
is	O
clearly	O
invariant	O
because	O
the	O
value	O
of	O
z\i	O
is	O
unchanged	O
.	O
also	O
,	O
each	O
step	O
by	O
deﬁnition	O
samples	O
from	O
the	O
correct	O
conditional	B
distribution	O
p	O
(	O
zi|z\i	O
)	O
.	O
because	O
these	O
conditional	B
and	O
marginal	B
distributions	O
together	O
specify	O
the	O
joint	O
distribution	O
,	O
we	O
see	O
that	O
the	O
joint	O
distribution	O
is	O
itself	O
invariant	O
.	O
the	O
second	O
requirement	O
to	O
be	O
satisﬁed	O
in	O
order	O
that	O
the	O
gibbs	O
sampling	O
proce-	O
dure	O
samples	O
from	O
the	O
correct	O
distribution	O
is	O
that	O
it	O
be	O
ergodic	O
.	O
a	O
sufﬁcient	O
condition	O
for	O
ergodicity	O
is	O
that	O
none	O
of	O
the	O
conditional	B
distributions	O
be	O
anywhere	O
zero	O
.	O
if	O
this	O
is	O
the	O
case	O
,	O
then	O
any	O
point	O
in	O
z	O
space	O
can	O
be	O
reached	O
from	O
any	O
other	O
point	O
in	O
a	O
ﬁnite	O
number	O
of	O
steps	O
involving	O
one	O
update	O
of	O
each	O
of	O
the	O
component	O
variables	O
.	O
if	O
this	O
requirement	O
is	O
not	O
satisﬁed	O
,	O
so	O
that	O
some	O
of	O
the	O
conditional	B
distributions	O
have	O
zeros	O
,	O
then	O
ergodicity	O
,	O
if	O
it	O
applies	O
,	O
must	O
be	O
proven	O
explicitly	O
.	O
the	O
distribution	O
of	O
initial	O
states	O
must	O
also	O
be	O
speciﬁed	O
in	O
order	O
to	O
complete	O
the	O
algorithm	O
,	O
although	O
samples	O
drawn	O
after	O
many	O
iterations	O
will	O
effectively	O
become	O
independent	B
of	O
this	O
distribution	O
.	O
of	O
course	O
,	O
successive	O
samples	O
from	O
the	O
markov	O
chain	O
will	O
be	O
highly	O
correlated	O
,	O
and	O
so	O
to	O
obtain	O
samples	O
that	O
are	O
nearly	O
independent	B
it	O
will	O
be	O
necessary	O
to	O
subsample	O
the	O
sequence	O
.	O
we	O
can	O
obtain	O
the	O
gibbs	O
sampling	O
procedure	O
as	O
a	O
particular	O
instance	O
of	O
the	O
metropolis-hastings	O
algorithm	O
as	O
follows	O
.	O
consider	O
a	O
metropolis-hastings	O
sampling	O
step	O
involving	O
the	O
variable	O
zk	O
in	O
which	O
the	O
remaining	O
variables	O
z\k	O
remain	O
ﬁxed	O
,	O
and	O
for	O
which	O
the	O
transition	B
probability	I
from	O
z	O
to	O
z	O
(	O
cid:1	O
)	O
is	O
given	O
by	O
qk	O
(	O
z	O
(	O
cid:1	O
)	O
|z	O
)	O
=	O
p	O
(	O
z	O
(	O
cid:1	O
)	O
k|z\k	O
)	O
.	O
we	O
note	O
that	O
z	O
(	O
cid:1	O
)	O
\k	O
=	O
z\k	O
because	O
these	O
components	O
are	O
unchanged	O
by	O
the	O
sampling	O
step	O
.	O
also	O
,	O
p	O
(	O
z	O
)	O
=	O
p	O
(	O
zk|z\k	O
)	O
p	O
(	O
z\k	O
)	O
.	O
thus	O
the	O
factor	O
that	O
determines	O
the	O
acceptance	O
probability	O
in	O
the	O
metropolis-hastings	O
(	O
11.44	O
)	O
is	O
given	O
by	O
a	O
(	O
z	O
(	O
cid:1	O
)	O
,	O
z	O
)	O
=	O
p	O
(	O
z	O
(	O
cid:1	O
)	O
)	O
qk	O
(	O
z|z	O
(	O
cid:1	O
)	O
)	O
p	O
(	O
z	O
)	O
qk	O
(	O
z	O
(	O
cid:1	O
)	O
|z	O
)	O
=	O
k|z	O
(	O
cid:1	O
)	O
\k	O
)	O
p	O
(	O
z	O
(	O
cid:1	O
)	O
\k	O
)	O
p	O
(	O
zk|z	O
(	O
cid:1	O
)	O
\k	O
)	O
k|z\k	O
)	O
p	O
(	O
z	O
(	O
cid:1	O
)	O
p	O
(	O
zk|z\k	O
)	O
p	O
(	O
z\k	O
)	O
p	O
(	O
z	O
(	O
cid:1	O
)	O
=	O
1	O
(	O
11.49	O
)	O
where	O
we	O
have	O
used	O
z	O
(	O
cid:1	O
)	O
\k	O
=	O
z\k	O
.	O
thus	O
the	O
metropolis-hastings	O
steps	O
are	O
always	O
accepted	O
.	O
as	O
with	O
the	O
metropolis	O
algorithm	O
,	O
we	O
can	O
gain	O
some	O
insight	O
into	O
the	O
behaviour	O
of	O
gibbs	O
sampling	O
by	O
investigating	O
its	O
application	O
to	O
a	O
gaussian	O
distribution	O
.	O
consider	O
a	O
correlated	O
gaussian	O
in	O
two	O
variables	O
,	O
as	O
illustrated	O
in	O
figure	O
11.11	O
,	O
having	O
con-	O
ditional	O
distributions	O
of	O
width	O
l	O
and	O
marginal	B
distributions	O
of	O
width	O
l.	O
the	O
typical	O
step	O
size	O
is	O
governed	O
by	O
the	O
conditional	B
distributions	O
and	O
will	O
be	O
of	O
order	O
l.	O
because	O
the	O
state	O
evolves	O
according	O
to	O
a	O
random	O
walk	O
,	O
the	O
number	O
of	O
steps	O
needed	O
to	O
obtain	O
independent	B
samples	O
from	O
the	O
distribution	O
will	O
be	O
of	O
order	O
(	O
l/l	O
)	O
2.	O
of	O
course	O
if	O
the	O
gaussian	O
distribution	O
were	O
uncorrelated	O
,	O
then	O
the	O
gibbs	O
sampling	O
procedure	O
would	O
be	O
optimally	O
efﬁcient	O
.	O
for	O
this	O
simple	O
problem	O
,	O
we	O
could	O
rotate	O
the	O
coordinate	O
sys-	O
tem	O
in	O
order	O
to	O
decorrelate	O
the	O
variables	O
.	O
however	O
,	O
in	O
practical	O
applications	O
it	O
will	O
generally	O
be	O
infeasible	O
to	O
ﬁnd	O
such	O
transformations	O
.	O
one	O
approach	O
to	O
reducing	O
random	O
walk	O
behaviour	O
in	O
gibbs	O
sampling	O
is	O
called	O
over-relaxation	B
(	O
adler	O
,	O
1981	O
)	O
.	O
in	O
its	O
original	O
form	O
,	O
this	O
applies	O
to	O
problems	O
for	O
which	O
11.3.	O
gibbs	O
sampling	O
545	O
z2	O
l	O
figure	O
11.11	O
illustration	O
of	O
gibbs	O
sampling	O
by	O
alter-	O
nate	O
updates	O
of	O
two	O
variables	O
whose	O
distribution	O
is	O
a	O
correlated	O
gaussian	O
.	O
the	O
step	O
size	O
is	O
governed	O
by	O
the	O
stan-	O
dard	O
deviation	O
of	O
the	O
conditional	B
distri-	O
bution	O
(	O
green	O
curve	O
)	O
,	O
and	O
is	O
o	O
(	O
l	O
)	O
,	O
lead-	O
ing	O
to	O
slow	O
progress	O
in	O
the	O
direction	O
of	O
elongation	O
of	O
the	O
joint	O
distribution	O
(	O
red	O
ellipse	O
)	O
.	O
the	O
number	O
of	O
steps	O
needed	O
to	O
obtain	O
an	O
independent	B
sample	O
from	O
the	O
distribution	O
is	O
o	O
(	O
(	O
l/l	O
)	O
2	O
)	O
.	O
l	O
z1	O
the	O
conditional	B
distributions	O
are	O
gaussian	O
,	O
which	O
represents	O
a	O
more	O
general	O
class	O
of	O
distributions	O
than	O
the	O
multivariate	O
gaussian	O
because	O
,	O
for	O
example	O
,	O
the	O
non-gaussian	O
distribution	O
p	O
(	O
z	O
,	O
y	O
)	O
∝	O
exp	O
(	O
−z2y2	O
)	O
has	O
gaussian	O
conditional	B
distributions	O
.	O
at	O
each	O
step	O
of	O
the	O
gibbs	O
sampling	O
algorithm	O
,	O
the	O
conditional	B
distribution	O
for	O
a	O
particular	O
component	O
zi	O
has	O
some	O
mean	B
µi	O
and	O
some	O
variance	B
σ2	O
i	O
.	O
in	O
the	O
over-relaxation	B
frame-	O
work	O
,	O
the	O
value	O
of	O
zi	O
is	O
replaced	O
with	O
i	O
=	O
µi	O
+	O
α	O
(	O
zi	O
−	O
µi	O
)	O
+	O
σi	O
(	O
1	O
−	O
α2	O
(	O
cid:4	O
)	O
i	O
)	O
1/2ν	O
z	O
(	O
11.50	O
)	O
i	O
,	O
then	O
so	O
too	O
does	O
z	O
where	O
ν	O
is	O
a	O
gaussian	O
random	O
variable	O
with	O
zero	O
mean	B
and	O
unit	O
variance	B
,	O
and	O
α	O
is	O
a	O
parameter	O
such	O
that	O
−1	O
<	O
α	O
<	O
1.	O
for	O
α	O
=	O
0	O
,	O
the	O
method	O
is	O
equivalent	O
to	O
standard	O
gibbs	O
sampling	O
,	O
and	O
for	O
α	O
<	O
0	O
the	O
step	O
is	O
biased	O
to	O
the	O
opposite	O
side	O
of	O
the	O
mean	B
.	O
this	O
step	O
leaves	O
the	O
desired	O
distribution	O
invariant	O
because	O
if	O
zi	O
has	O
mean	B
µi	O
(	O
cid:4	O
)	O
and	O
variance	B
σ2	O
i.	O
the	O
effect	O
of	O
over-relaxation	B
is	O
to	O
encourage	O
directed	B
motion	O
through	O
state	O
space	O
when	O
the	O
variables	O
are	O
highly	O
correlated	O
.	O
the	O
framework	O
of	O
ordered	B
over-relaxation	I
(	O
neal	O
,	O
1999	O
)	O
generalizes	O
this	O
approach	O
to	O
non-	O
gaussian	O
distributions	O
.	O
the	O
practical	O
applicability	O
of	O
gibbs	O
sampling	O
depends	O
on	O
the	O
ease	O
with	O
which	O
samples	O
can	O
be	O
drawn	O
from	O
the	O
conditional	B
distributions	O
p	O
(	O
zk|z\k	O
)	O
.	O
in	O
the	O
case	O
of	O
probability	B
distributions	O
speciﬁed	O
using	O
graphical	O
models	O
,	O
the	O
conditional	B
distribu-	O
tions	O
for	O
individual	O
nodes	O
depend	O
only	O
on	O
the	O
variables	O
in	O
the	O
corresponding	O
markov	O
blankets	O
,	O
as	O
illustrated	O
in	O
figure	O
11.12.	O
for	O
directed	O
graphs	O
,	O
a	O
wide	O
choice	O
of	O
condi-	O
tional	O
distributions	O
for	O
the	O
individual	O
nodes	O
conditioned	O
on	O
their	O
parents	O
will	O
lead	O
to	O
conditional	B
distributions	O
for	O
gibbs	O
sampling	O
that	O
are	O
log	O
concave	O
.	O
the	O
adaptive	O
re-	O
jection	O
sampling	B
methods	I
discussed	O
in	O
section	O
11.1.3	O
therefore	O
provide	O
a	O
framework	O
for	O
monte	O
carlo	O
sampling	O
from	O
directed	B
graphs	O
with	O
broad	O
applicability	O
.	O
if	O
the	O
graph	O
is	O
constructed	O
using	O
distributions	O
from	O
the	O
exponential	B
family	I
,	O
and	O
if	O
the	O
parent-child	O
relationships	O
preserve	O
conjugacy	O
,	O
then	O
the	O
full	O
conditional	B
distri-	O
butions	O
arising	O
in	O
gibbs	O
sampling	O
will	O
have	O
the	O
same	O
functional	B
form	O
as	O
the	O
orig-	O
546	O
11.	O
sampling	B
methods	I
figure	O
11.12	O
the	O
gibbs	O
sampling	O
method	O
requires	O
samples	O
to	O
be	O
drawn	O
from	O
the	O
conditional	B
distribution	O
of	O
a	O
variable	O
condi-	O
tioned	O
on	O
the	O
remaining	O
variables	O
.	O
for	O
graphical	O
models	O
,	O
this	O
conditional	B
distribution	O
is	O
a	O
function	O
only	O
of	O
the	O
states	O
of	O
the	O
nodes	O
in	O
the	O
markov	O
blanket	O
.	O
for	O
an	O
undirected	B
graph	I
this	O
com-	O
prises	O
the	O
set	O
of	O
neighbours	O
,	O
as	O
shown	O
on	O
the	O
left	O
,	O
while	O
for	O
a	O
directed	B
graph	O
the	O
markov	O
blanket	O
comprises	O
the	O
parents	O
,	O
the	O
children	O
,	O
and	O
the	O
co-parents	B
,	O
as	O
shown	O
on	O
the	O
right	O
.	O
inal	O
conditional	B
distributions	O
(	O
conditioned	O
on	O
the	O
parents	O
)	O
deﬁning	O
each	O
node	B
,	O
and	O
so	O
standard	O
sampling	O
techniques	O
can	O
be	O
employed	O
.	O
in	O
general	O
,	O
the	O
full	O
conditional	B
distributions	O
will	O
be	O
of	O
a	O
complex	O
form	O
that	O
does	O
not	O
permit	O
the	O
use	O
of	O
standard	O
sam-	O
pling	O
algorithms	O
.	O
however	O
,	O
if	O
these	O
conditionals	O
are	O
log	O
concave	O
,	O
then	O
sampling	O
can	O
be	O
done	O
efﬁciently	O
using	O
adaptive	B
rejection	I
sampling	I
(	O
assuming	O
the	O
corresponding	O
variable	O
is	O
a	O
scalar	O
)	O
.	O
if	O
,	O
at	O
each	O
stage	O
of	O
the	O
gibbs	O
sampling	O
algorithm	O
,	O
instead	O
of	O
drawing	O
a	O
sample	O
from	O
the	O
corresponding	O
conditional	B
distribution	O
,	O
we	O
make	O
a	O
point	O
estimate	O
of	O
the	O
variable	O
given	O
by	O
the	O
maximum	O
of	O
the	O
conditional	B
distribution	O
,	O
then	O
we	O
obtain	O
the	O
iterated	B
conditional	I
modes	I
(	O
icm	O
)	O
algorithm	O
discussed	O
in	O
section	O
8.3.3.	O
thus	O
icm	O
can	O
be	O
seen	O
as	O
a	O
greedy	O
approximation	O
to	O
gibbs	O
sampling	O
.	O
because	O
the	O
basic	O
gibbs	O
sampling	O
technique	O
considers	O
one	O
variable	O
at	O
a	O
time	O
,	O
there	O
are	O
strong	O
dependencies	O
between	O
successive	O
samples	O
.	O
at	O
the	O
opposite	O
extreme	O
,	O
if	O
we	O
could	O
draw	O
samples	O
directly	O
from	O
the	O
joint	O
distribution	O
(	O
an	O
operation	O
that	O
we	O
are	O
supposing	O
is	O
intractable	O
)	O
,	O
then	O
successive	O
samples	O
would	O
be	O
independent	B
.	O
we	O
can	O
hope	O
to	O
improve	O
on	O
the	O
simple	O
gibbs	O
sampler	O
by	O
adopting	O
an	O
intermediate	O
strategy	O
in	O
which	O
we	O
sample	O
successively	O
from	O
groups	O
of	O
variables	O
rather	O
than	O
individual	O
vari-	O
ables	O
.	O
this	O
is	O
achieved	O
in	O
the	O
blocking	B
gibbs	O
sampling	O
algorithm	O
by	O
choosing	O
blocks	O
of	O
variables	O
,	O
not	O
necessarily	O
disjoint	O
,	O
and	O
then	O
sampling	O
jointly	O
from	O
the	O
variables	O
in	O
each	O
block	O
in	O
turn	O
,	O
conditioned	O
on	O
the	O
remaining	O
variables	O
(	O
jensen	O
et	O
al.	O
,	O
1995	O
)	O
.	O
11.4.	O
slice	B
sampling	I
we	O
have	O
seen	O
that	O
one	O
of	O
the	O
difﬁculties	O
with	O
the	O
metropolis	O
algorithm	O
is	O
the	O
sensi-	O
tivity	O
to	O
step	O
size	O
.	O
if	O
this	O
is	O
too	O
small	O
,	O
the	O
result	O
is	O
slow	O
decorrelation	O
due	O
to	O
random	O
walk	O
behaviour	O
,	O
whereas	O
if	O
it	O
is	O
too	O
large	O
the	O
result	O
is	O
inefﬁciency	O
due	O
to	O
a	O
high	O
rejec-	O
tion	O
rate	O
.	O
the	O
technique	O
of	O
slice	B
sampling	I
(	O
neal	O
,	O
2003	O
)	O
provides	O
an	O
adaptive	O
step	O
size	O
that	O
is	O
automatically	O
adjusted	O
to	O
match	O
the	O
characteristics	O
of	O
the	O
distribution	O
.	O
again	O
it	O
requires	O
that	O
we	O
are	O
able	O
to	O
evaluate	O
the	O
unnormalized	O
distribution	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
.	O
consider	O
ﬁrst	O
the	O
univariate	O
case	O
.	O
slice	B
sampling	I
involves	O
augmenting	O
z	O
with	O
an	O
additional	O
variable	O
u	O
and	O
then	O
drawing	O
samples	O
from	O
the	O
joint	O
(	O
z	O
,	O
u	O
)	O
space	O
.	O
we	O
shall	O
see	O
another	O
example	O
of	O
this	O
approach	O
when	O
we	O
discuss	O
hybrid	O
monte	O
carlo	O
in	O
section	O
11.5.	O
the	O
goal	O
is	O
to	O
sample	O
uniformly	O
from	O
the	O
area	O
under	O
the	O
distribution	O
˜p	O
(	O
z	O
)	O
u	O
z	O
(	O
τ	O
)	O
11.4.	O
slice	B
sampling	I
547	O
˜p	O
(	O
z	O
)	O
zmin	O
u	O
zmax	O
z	O
(	O
a	O
)	O
z	O
(	O
τ	O
)	O
(	O
b	O
)	O
z	O
(	O
a	O
)	O
for	O
a	O
given	O
value	O
z	O
(	O
τ	O
)	O
,	O
a	O
value	O
of	O
u	O
is	O
chosen	O
uniformly	O
in	O
figure	O
11.13	O
illustration	O
of	O
slice	B
sampling	I
.	O
the	O
region	O
0	O
(	O
cid:1	O
)	O
u	O
(	O
cid:1	O
)	O
ep	O
(	O
z	O
(	O
τ	O
)	O
)	O
,	O
which	O
then	O
deﬁnes	O
a	O
‘	O
slice	O
’	O
through	O
the	O
distribution	O
,	O
shown	O
by	O
the	O
solid	O
horizontal	O
(	O
b	O
)	O
because	O
it	O
is	O
infeasible	O
to	O
sample	O
directly	O
from	O
a	O
slice	O
,	O
a	O
new	O
sample	O
of	O
z	O
is	O
drawn	O
from	O
a	O
region	O
lines	O
.	O
zmin	O
(	O
cid:1	O
)	O
z	O
(	O
cid:1	O
)	O
zmax	O
,	O
which	O
contains	O
the	O
previous	O
value	O
z	O
(	O
τ	O
)	O
.	O
(	O
11.51	O
)	O
given	O
by	O
where	O
zp	O
=	O
(	O
cid:12	O
)	O
(	O
cid:1	O
)	O
p	O
(	O
z	O
,	O
u	O
)	O
=	O
(	O
cid:28	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
dz	O
.	O
the	O
marginal	B
distribution	O
over	O
z	O
is	O
given	O
by	O
(	O
cid:6	O
)	O
ep	O
(	O
z	O
)	O
(	O
cid:6	O
)	O
(	O
cid:1	O
)	O
p	O
(	O
z	O
,	O
u	O
)	O
du	O
=	O
if	O
0	O
(	O
cid:1	O
)	O
u	O
(	O
cid:1	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
1/zp	O
0	O
otherwise	O
=	O
p	O
(	O
z	O
)	O
du	O
=	O
1	O
zp	O
0	O
zp	O
(	O
11.52	O
)	O
values	O
.	O
this	O
can	O
be	O
achieved	O
by	O
alternately	O
sampling	O
z	O
and	O
u.	O
given	O
the	O
value	O
of	O
z	O
and	O
so	O
we	O
can	O
sample	O
from	O
p	O
(	O
z	O
)	O
by	O
sampling	O
from	O
(	O
cid:1	O
)	O
p	O
(	O
z	O
,	O
u	O
)	O
and	O
then	O
ignoring	O
the	O
u	O
we	O
evaluate	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
and	O
then	O
sample	O
u	O
uniformly	O
in	O
the	O
range	O
0	O
(	O
cid:1	O
)	O
u	O
(	O
cid:1	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
,	O
which	O
is	O
distribution	O
deﬁned	O
by	O
{	O
z	O
:	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
>	O
u	O
}	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
11.13	O
(	O
a	O
)	O
.	O
under	O
(	O
cid:1	O
)	O
p	O
(	O
z	O
,	O
u	O
)	O
invariant	O
,	O
which	O
can	O
be	O
achieved	O
by	O
ensuring	O
that	O
detailed	O
balance	O
is	O
in	O
practice	O
,	O
it	O
can	O
be	O
difﬁcult	O
to	O
sample	O
directly	O
from	O
a	O
slice	O
through	O
the	O
distribu-	O
tion	O
and	O
so	O
instead	O
we	O
deﬁne	O
a	O
sampling	O
scheme	O
that	O
leaves	O
the	O
uniform	B
distribution	I
straightforward	O
.	O
then	O
we	O
ﬁx	O
u	O
and	O
sample	O
z	O
uniformly	O
from	O
the	O
‘	O
slice	O
’	O
through	O
the	O
satisﬁed	O
.	O
suppose	O
the	O
current	O
value	O
of	O
z	O
is	O
denoted	O
z	O
(	O
τ	O
)	O
and	O
that	O
we	O
have	O
obtained	O
a	O
corresponding	O
sample	O
u.	O
the	O
next	O
value	O
of	O
z	O
is	O
obtained	O
by	O
considering	O
a	O
region	O
zmin	O
(	O
cid:1	O
)	O
z	O
(	O
cid:1	O
)	O
zmax	O
that	O
contains	O
z	O
(	O
τ	O
)	O
.	O
it	O
is	O
in	O
the	O
choice	O
of	O
this	O
region	O
that	O
the	O
adap-	O
tation	O
to	O
the	O
characteristic	O
length	O
scales	O
of	O
the	O
distribution	O
takes	O
place	O
.	O
we	O
want	O
the	O
region	O
to	O
encompass	O
as	O
much	O
of	O
the	O
slice	O
as	O
possible	O
so	O
as	O
to	O
allow	O
large	O
moves	O
in	O
z	O
space	O
while	O
having	O
as	O
little	O
as	O
possible	O
of	O
this	O
region	O
lying	O
outside	O
the	O
slice	O
,	O
because	O
this	O
makes	O
the	O
sampling	O
less	O
efﬁcient	O
.	O
one	O
approach	O
to	O
the	O
choice	O
of	O
region	O
involves	O
starting	O
with	O
a	O
region	O
containing	O
z	O
(	O
τ	O
)	O
having	O
some	O
width	O
w	O
and	O
then	O
testing	O
each	O
of	O
the	O
end	O
points	O
to	O
see	O
if	O
they	O
lie	O
within	O
the	O
slice	O
.	O
if	O
either	O
end	O
point	O
does	O
not	O
,	O
then	O
the	O
region	O
is	O
extended	B
in	O
that	O
direction	O
by	O
increments	O
of	O
value	O
w	O
until	O
the	O
end	O
point	O
lies	O
outside	O
the	O
region	O
.	O
a	O
is	O
then	O
chosen	O
uniformly	O
from	O
this	O
region	O
,	O
and	O
if	O
it	O
lies	O
within	O
the	O
candidate	O
value	O
z	O
slice	O
,	O
then	O
it	O
forms	O
z	O
(	O
τ	O
+1	O
)	O
.	O
if	O
it	O
lies	O
outside	O
the	O
slice	O
,	O
then	O
the	O
region	O
is	O
shrunk	O
such	O
forms	O
an	O
end	O
point	O
and	O
such	O
that	O
the	O
region	O
still	O
contains	O
z	O
(	O
τ	O
)	O
.	O
then	O
another	O
that	O
z	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
548	O
11.	O
sampling	B
methods	I
candidate	O
point	O
is	O
drawn	O
uniformly	O
from	O
this	O
reduced	O
region	O
and	O
so	O
on	O
,	O
until	O
a	O
value	O
of	O
z	O
is	O
found	O
that	O
lies	O
within	O
the	O
slice	O
.	O
slice	B
sampling	I
can	O
be	O
applied	O
to	O
multivariate	O
distributions	O
by	O
repeatedly	O
sam-	O
pling	O
each	O
variable	O
in	O
turn	O
,	O
in	O
the	O
manner	O
of	O
gibbs	O
sampling	O
.	O
this	O
requires	O
that	O
we	O
are	O
able	O
to	O
compute	O
,	O
for	O
each	O
component	O
zi	O
,	O
a	O
function	O
that	O
is	O
proportional	O
to	O
p	O
(	O
zi|z\i	O
)	O
.	O
11.5.	O
the	O
hybrid	O
monte	O
carlo	O
algorithm	O
as	O
we	O
have	O
already	O
noted	O
,	O
one	O
of	O
the	O
major	O
limitations	O
of	O
the	O
metropolis	O
algorithm	O
is	O
that	O
it	O
can	O
exhibit	O
random	O
walk	O
behaviour	O
whereby	O
the	O
distance	O
traversed	O
through	O
the	O
state	O
space	O
grows	O
only	O
as	O
the	O
square	O
root	O
of	O
the	O
number	O
of	O
steps	O
.	O
the	O
problem	O
can	O
not	O
be	O
resolved	O
simply	O
by	O
taking	O
bigger	O
steps	O
as	O
this	O
leads	O
to	O
a	O
high	O
rejection	O
rate	O
.	O
in	O
this	O
section	O
,	O
we	O
introduce	O
a	O
more	O
sophisticated	O
class	O
of	O
transitions	O
based	O
on	O
an	O
analogy	O
with	O
physical	O
systems	O
and	O
that	O
has	O
the	O
property	O
of	O
being	O
able	O
to	O
make	O
large	O
changes	O
to	O
the	O
system	O
state	O
while	O
keeping	O
the	O
rejection	O
probability	O
small	O
.	O
it	O
is	O
ap-	O
plicable	O
to	O
distributions	O
over	O
continuous	O
variables	O
for	O
which	O
we	O
can	O
readily	O
evaluate	O
the	O
gradient	O
of	O
the	O
log	O
probability	O
with	O
respect	O
to	O
the	O
state	O
variables	O
.	O
we	O
will	O
discuss	O
the	O
dynamical	O
systems	O
framework	O
in	O
section	O
11.5.1	O
,	O
and	O
then	O
in	O
section	O
11.5.2	O
we	O
explain	O
how	O
this	O
may	O
be	O
combined	O
with	O
the	O
metropolis	O
algorithm	O
to	O
yield	O
the	O
pow-	O
erful	O
hybrid	O
monte	O
carlo	O
algorithm	O
.	O
a	O
background	O
in	O
physics	O
is	O
not	O
required	O
as	O
this	O
section	O
is	O
self-contained	O
and	O
the	O
key	O
results	O
are	O
all	O
derived	O
from	O
ﬁrst	O
principles	O
.	O
11.5.1	O
dynamical	O
systems	O
the	O
dynamical	O
approach	O
to	O
stochastic	B
sampling	O
has	O
its	O
origins	O
in	O
algorithms	O
for	O
simulating	O
the	O
behaviour	O
of	O
physical	O
systems	O
evolving	O
under	O
hamiltonian	O
dynam-	O
ics	O
.	O
in	O
a	O
markov	O
chain	O
monte	O
carlo	O
simulation	O
,	O
the	O
goal	O
is	O
to	O
sample	O
from	O
a	O
given	O
probability	B
distribution	O
p	O
(	O
z	O
)	O
.	O
the	O
framework	O
of	O
hamiltonian	O
dynamics	O
is	O
exploited	O
by	O
casting	O
the	O
probabilistic	O
simulation	O
in	O
the	O
form	O
of	O
a	O
hamiltonian	O
system	O
.	O
in	O
order	O
to	O
remain	O
in	O
keeping	O
with	O
the	O
literature	O
in	O
this	O
area	O
,	O
we	O
make	O
use	O
of	O
the	O
relevant	O
dynamical	O
systems	O
terminology	O
where	O
appropriate	O
,	O
which	O
will	O
be	O
deﬁned	O
as	O
we	O
go	O
along	O
.	O
the	O
dynamics	O
that	O
we	O
consider	O
corresponds	O
to	O
the	O
evolution	O
of	O
the	O
state	O
variable	O
z	O
=	O
{	O
zi	O
}	O
under	O
continuous	O
time	O
,	O
which	O
we	O
denote	O
by	O
τ	O
.	O
classical	B
dynamics	O
is	O
de-	O
scribed	O
by	O
newton	O
’	O
s	O
second	O
law	O
of	O
motion	O
in	O
which	O
the	O
acceleration	O
of	O
an	O
object	O
is	O
proportional	O
to	O
the	O
applied	O
force	O
,	O
corresponding	O
to	O
a	O
second-order	O
differential	B
equa-	O
tion	O
over	O
time	O
.	O
we	O
can	O
decompose	O
a	O
second-order	O
equation	O
into	O
two	O
coupled	O
ﬁrst-	O
order	O
equations	O
by	O
introducing	O
intermediate	O
momentum	O
variables	O
r	O
,	O
corresponding	O
to	O
the	O
rate	O
of	O
change	O
of	O
the	O
state	O
variables	O
z	O
,	O
having	O
components	O
ri	O
=	O
dzi	O
dτ	O
(	O
11.53	O
)	O
where	O
the	O
zi	O
can	O
be	O
regarded	O
as	O
position	O
variables	O
in	O
this	O
dynamics	O
perspective	O
.	O
thus	O
11.5.	O
the	O
hybrid	O
monte	O
carlo	O
algorithm	O
549	O
for	O
each	O
position	B
variable	I
there	O
is	O
a	O
corresponding	O
momentum	B
variable	I
,	O
and	O
the	O
joint	O
space	O
of	O
position	O
and	O
momentum	O
variables	O
is	O
called	O
phase	B
space	I
.	O
without	O
loss	O
of	O
generality	O
,	O
we	O
can	O
write	O
the	O
probability	B
distribution	O
p	O
(	O
z	O
)	O
in	O
the	O
form	O
p	O
(	O
z	O
)	O
=	O
exp	O
(	O
−e	O
(	O
z	O
)	O
)	O
1	O
zp	O
(	O
11.54	O
)	O
where	O
e	O
(	O
z	O
)	O
is	O
interpreted	O
as	O
the	O
potential	B
energy	I
of	O
the	O
system	O
when	O
in	O
state	O
z.	O
the	O
system	O
acceleration	O
is	O
the	O
rate	O
of	O
change	O
of	O
momentum	O
and	O
is	O
given	O
by	O
the	O
applied	O
force	O
,	O
which	O
itself	O
is	O
the	O
negative	O
gradient	O
of	O
the	O
potential	B
energy	I
dri	O
dτ	O
=	O
−	O
∂e	O
(	O
z	O
)	O
∂zi	O
.	O
(	O
11.55	O
)	O
it	O
is	O
convenient	O
to	O
reformulate	O
this	O
dynamical	B
system	I
using	O
the	O
hamiltonian	O
framework	O
.	O
to	O
do	O
this	O
,	O
we	O
ﬁrst	O
deﬁne	O
the	O
kinetic	B
energy	I
by	O
k	O
(	O
r	O
)	O
=	O
(	O
cid:5	O
)	O
r	O
(	O
cid:5	O
)	O
2	O
=	O
1	O
2	O
1	O
2	O
r2	O
i	O
.	O
(	O
11.56	O
)	O
(	O
cid:2	O
)	O
i	O
the	O
total	O
energy	O
of	O
the	O
system	O
is	O
then	O
the	O
sum	O
of	O
its	O
potential	O
and	O
kinetic	O
energies	O
h	O
(	O
z	O
,	O
r	O
)	O
=	O
e	O
(	O
z	O
)	O
+	O
k	O
(	O
r	O
)	O
(	O
11.57	O
)	O
exercise	O
11.15	O
where	O
h	O
is	O
the	O
hamiltonian	O
function	O
.	O
using	O
(	O
11.53	O
)	O
,	O
(	O
11.55	O
)	O
,	O
(	O
11.56	O
)	O
,	O
and	O
(	O
11.57	O
)	O
,	O
we	O
can	O
now	O
express	O
the	O
dynamics	O
of	O
the	O
system	O
in	O
terms	O
of	O
the	O
hamiltonian	O
equa-	O
tions	O
given	O
by	O
dzi	O
dτ	O
dri	O
dτ	O
=	O
∂h	O
∂ri	O
=	O
−	O
∂h	O
∂zi	O
.	O
(	O
11.58	O
)	O
(	O
11.59	O
)	O
william	O
hamilton	O
1805–1865	O
william	O
rowan	O
hamilton	O
was	O
an	O
irish	O
mathematician	O
and	O
physicist	O
,	O
and	O
child	O
prodigy	O
,	O
who	O
was	O
ap-	O
pointed	O
professor	O
of	O
astronomy	O
at	O
trinity	O
college	O
,	O
dublin	O
,	O
in	O
1827	O
,	O
be-	O
fore	O
he	O
had	O
even	O
graduated	O
.	O
one	O
of	O
hamilton	O
’	O
s	O
most	O
important	O
contributions	O
was	O
a	O
new	O
formulation	O
of	O
dynamics	O
,	O
which	O
played	O
a	O
signiﬁcant	O
role	O
in	O
the	O
later	O
development	O
of	O
quantum	O
mechanics	O
.	O
his	O
other	O
great	O
achievement	O
was	O
the	O
development	O
of	O
quaternions	O
,	O
which	O
generalize	O
the	O
concept	O
of	O
complex	O
numbers	O
by	O
introducing	O
three	O
distinct	O
square	O
roots	O
of	O
minus	O
one	O
,	O
which	O
satisfy	O
i2	O
=	O
j2	O
=	O
k2	O
=	O
ijk	O
=	O
−1	O
.	O
it	O
is	O
said	O
that	O
these	O
equations	O
occurred	O
to	O
him	O
while	O
walking	O
along	O
the	O
royal	O
canal	O
in	O
dublin	O
with	O
his	O
wife	O
,	O
on	O
16	O
october	O
1843	O
,	O
and	O
he	O
promptly	O
carved	O
the	O
equations	O
into	O
the	O
side	O
of	O
broome	O
bridge	O
.	O
although	O
there	O
is	O
no	O
longer	O
any	O
evidence	O
of	O
the	O
carving	O
,	O
there	O
is	O
now	O
a	O
stone	O
plaque	O
on	O
the	O
bridge	O
commemorating	O
the	O
discovery	O
and	O
displaying	O
the	O
quaternion	O
equations	O
.	O
550	O
11.	O
sampling	B
methods	I
during	O
the	O
evolution	O
of	O
this	O
dynamical	B
system	I
,	O
the	O
value	O
of	O
the	O
hamiltonian	O
h	O
is	O
constant	O
,	O
as	O
is	O
easily	O
seen	O
by	O
differentiation	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
i	O
i	O
dh	O
dτ	O
=	O
=	O
∂h	O
∂zi	O
dzi	O
dτ	O
∂h	O
∂zi	O
∂h	O
∂ri	O
+	O
∂h	O
∂ri	O
−	O
∂h	O
∂ri	O
dri	O
dτ	O
∂h	O
∂zi	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
=	O
0	O
.	O
(	O
11.60	O
)	O
a	O
second	O
important	O
property	O
of	O
hamiltonian	O
dynamical	O
systems	O
,	O
known	O
as	O
li-	O
ouville	O
’	O
s	O
theorem	O
,	O
is	O
that	O
they	O
preserve	O
volume	O
in	O
phase	B
space	I
.	O
in	O
other	O
words	O
,	O
if	O
we	O
consider	O
a	O
region	O
within	O
the	O
space	O
of	O
variables	O
(	O
z	O
,	O
r	O
)	O
,	O
then	O
as	O
this	O
region	O
evolves	O
under	O
the	O
equations	O
of	O
hamiltonian	O
dynamics	O
,	O
its	O
shape	O
may	O
change	O
but	O
its	O
volume	O
will	O
not	O
.	O
this	O
can	O
be	O
seen	O
by	O
noting	O
that	O
the	O
ﬂow	O
ﬁeld	O
(	O
rate	O
of	O
change	O
of	O
location	O
in	O
phase	B
space	I
)	O
is	O
given	O
by	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
dz	O
dτ	O
and	O
that	O
the	O
divergence	O
of	O
this	O
ﬁeld	O
vanishes	O
v	O
=	O
,	O
dr	O
dτ	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
i	O
i	O
∂	O
∂zi	O
−	O
∂	O
∂zi	O
dzi	O
dτ	O
+	O
∂	O
∂ri	O
dri	O
dτ	O
∂h	O
∂ri	O
+	O
∂	O
∂ri	O
∂h	O
∂zi	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
11.61	O
)	O
=	O
0	O
.	O
(	O
11.62	O
)	O
div	O
v	O
=	O
=	O
now	O
consider	O
the	O
joint	O
distribution	O
over	O
phase	B
space	I
whose	O
total	O
energy	O
is	O
the	O
hamiltonian	O
,	O
i.e.	O
,	O
the	O
distribution	O
given	O
by	O
p	O
(	O
z	O
,	O
r	O
)	O
=	O
exp	O
(	O
−h	O
(	O
z	O
,	O
r	O
)	O
)	O
.	O
1	O
zh	O
(	O
11.63	O
)	O
using	O
the	O
two	O
results	O
of	O
conservation	O
of	O
volume	O
and	O
conservation	O
of	O
h	O
,	O
it	O
follows	O
that	O
the	O
hamiltonian	O
dynamics	O
will	O
leave	O
p	O
(	O
z	O
,	O
r	O
)	O
invariant	O
.	O
this	O
can	O
be	O
seen	O
by	O
considering	O
a	O
small	O
region	O
of	O
phase	B
space	I
over	O
which	O
h	O
is	O
approximately	O
constant	O
.	O
if	O
we	O
follow	O
the	O
evolution	O
of	O
the	O
hamiltonian	O
equations	O
for	O
a	O
ﬁnite	O
time	O
,	O
then	O
the	O
volume	O
of	O
this	O
region	O
will	O
remain	O
unchanged	O
as	O
will	O
the	O
value	O
of	O
h	O
in	O
this	O
region	O
,	O
and	O
hence	O
the	O
probability	B
density	O
,	O
which	O
is	O
a	O
function	O
only	O
of	O
h	O
,	O
will	O
also	O
be	O
unchanged	O
.	O
although	O
h	O
is	O
invariant	O
,	O
the	O
values	O
of	O
z	O
and	O
r	O
will	O
vary	O
,	O
and	O
so	O
by	O
integrating	O
the	O
hamiltonian	O
dynamics	O
over	O
a	O
ﬁnite	O
time	O
duration	O
it	O
becomes	O
possible	O
to	O
make	O
large	O
changes	O
to	O
z	O
in	O
a	O
systematic	O
way	O
that	O
avoids	O
random	O
walk	O
behaviour	O
.	O
evolution	O
under	O
the	O
hamiltonian	O
dynamics	O
will	O
not	O
,	O
however	O
,	O
sample	O
ergodi-	O
cally	O
from	O
p	O
(	O
z	O
,	O
r	O
)	O
because	O
the	O
value	O
of	O
h	O
is	O
constant	O
.	O
in	O
order	O
to	O
arrive	O
at	O
an	O
ergodic	O
sampling	O
scheme	O
,	O
we	O
can	O
introduce	O
additional	O
moves	O
in	O
phase	B
space	I
that	O
change	O
the	O
value	O
of	O
h	O
while	O
also	O
leaving	O
the	O
distribution	O
p	O
(	O
z	O
,	O
r	O
)	O
invariant	O
.	O
the	O
simplest	O
way	O
to	O
achieve	O
this	O
is	O
to	O
replace	O
the	O
value	O
of	O
r	O
with	O
one	O
drawn	O
from	O
its	O
distribution	O
conditioned	O
on	O
z.	O
this	O
can	O
be	O
regarded	O
as	O
a	O
gibbs	O
sampling	O
step	O
,	O
and	O
hence	O
from	O
11.5.	O
the	O
hybrid	O
monte	O
carlo	O
algorithm	O
551	O
exercise	O
11.16	O
section	O
11.3	O
we	O
see	O
that	O
this	O
also	O
leaves	O
the	O
desired	O
distribution	O
invariant	O
.	O
noting	O
that	O
z	O
and	O
r	O
are	O
independent	B
in	O
the	O
distribution	O
p	O
(	O
z	O
,	O
r	O
)	O
,	O
we	O
see	O
that	O
the	O
conditional	B
distribution	O
p	O
(	O
r|z	O
)	O
is	O
a	O
gaussian	O
from	O
which	O
it	O
is	O
straightforward	O
to	O
sample	O
.	O
in	O
a	O
practical	O
application	O
of	O
this	O
approach	O
,	O
we	O
have	O
to	O
address	O
the	O
problem	O
of	O
performing	O
a	O
numerical	O
integration	O
of	O
the	O
hamiltonian	O
equations	O
.	O
this	O
will	O
neces-	O
sarily	O
introduce	O
numerical	O
errors	O
and	O
so	O
we	O
should	O
devise	O
a	O
scheme	O
that	O
minimizes	O
the	O
impact	O
of	O
such	O
errors	O
.	O
in	O
fact	O
,	O
it	O
turns	O
out	O
that	O
integration	O
schemes	O
can	O
be	O
devised	O
for	O
which	O
liouville	O
’	O
s	O
theorem	O
still	O
holds	O
exactly	O
.	O
this	O
property	O
will	O
be	O
important	O
in	O
the	O
hybrid	O
monte	O
carlo	O
algorithm	O
,	O
which	O
is	O
discussed	O
in	O
section	O
11.5.2.	O
one	O
scheme	O
for	O
achieving	O
this	O
is	O
called	O
the	O
leapfrog	B
discretization	I
and	O
involves	O
alternately	O
updat-	O
ing	O
discrete-time	O
approximations	O
(	O
cid:1	O
)	O
z	O
and	O
(	O
cid:1	O
)	O
r	O
to	O
the	O
position	O
and	O
momentum	O
variables	O
using	O
(	O
(	O
cid:1	O
)	O
z	O
(	O
τ	O
)	O
)	O
(	O
cid:1	O
)	O
ri	O
(	O
τ	O
+	O
/2	O
)	O
=	O
(	O
cid:1	O
)	O
ri	O
(	O
τ	O
)	O
−	O
	O
(	O
cid:1	O
)	O
zi	O
(	O
τ	O
+	O
	O
)	O
=	O
(	O
cid:1	O
)	O
zi	O
(	O
τ	O
)	O
+	O
	O
(	O
cid:1	O
)	O
ri	O
(	O
τ	O
+	O
/2	O
)	O
(	O
cid:1	O
)	O
ri	O
(	O
τ	O
+	O
	O
)	O
=	O
(	O
cid:1	O
)	O
ri	O
(	O
τ	O
+	O
/2	O
)	O
−	O
	O
∂e	O
∂zi	O
2	O
(	O
(	O
cid:1	O
)	O
z	O
(	O
τ	O
+	O
	O
)	O
)	O
.	O
∂e	O
∂zi	O
2	O
(	O
11.64	O
)	O
(	O
11.65	O
)	O
(	O
11.66	O
)	O
we	O
see	O
that	O
this	O
takes	O
the	O
form	O
of	O
a	O
half-step	O
update	O
of	O
the	O
momentum	O
variables	O
with	O
step	O
size	O
/2	O
,	O
followed	O
by	O
a	O
full-step	O
update	O
of	O
the	O
position	O
variables	O
with	O
step	O
size	O
	O
,	O
followed	O
by	O
a	O
second	O
half-step	O
update	O
of	O
the	O
momentum	O
variables	O
.	O
if	O
several	O
leapfrog	O
steps	O
are	O
applied	O
in	O
succession	O
,	O
it	O
can	O
be	O
seen	O
that	O
half-step	O
updates	O
to	O
the	O
momentum	O
variables	O
can	O
be	O
combined	O
into	O
full-step	O
updates	O
with	O
step	O
size	O
	O
.	O
the	O
successive	O
updates	O
to	O
position	O
and	O
momentum	O
variables	O
then	O
leapfrog	O
over	O
each	O
other	O
.	O
in	O
order	O
to	O
advance	O
the	O
dynamics	O
by	O
a	O
time	O
interval	O
τ	O
,	O
we	O
need	O
to	O
take	O
τ	O
/	O
steps	O
.	O
the	O
error	B
involved	O
in	O
the	O
discretized	O
approximation	O
to	O
the	O
continuous	O
time	O
dynamics	O
will	O
go	O
to	O
zero	O
,	O
assuming	O
a	O
smooth	O
function	O
e	O
(	O
z	O
)	O
,	O
in	O
the	O
limit	O
	O
→	O
0.	O
however	O
,	O
for	O
a	O
nonzero	O
	O
as	O
used	O
in	O
practice	O
,	O
some	O
residual	O
error	B
will	O
remain	O
.	O
we	O
shall	O
see	O
in	O
section	O
11.5.2	O
how	O
the	O
effects	O
of	O
such	O
errors	O
can	O
be	O
eliminated	O
in	O
the	O
hybrid	O
monte	O
carlo	O
algorithm	O
.	O
in	O
summary	O
then	O
,	O
the	O
hamiltonian	O
dynamical	O
approach	O
involves	O
alternating	O
be-	O
tween	O
a	O
series	O
of	O
leapfrog	O
updates	O
and	O
a	O
resampling	O
of	O
the	O
momentum	O
variables	O
from	O
their	O
marginal	B
distribution	O
.	O
note	O
that	O
the	O
hamiltonian	O
dynamics	O
method	O
,	O
unlike	O
the	O
basic	O
metropolis	O
algo-	O
rithm	O
,	O
is	O
able	O
to	O
make	O
use	O
of	O
information	O
about	O
the	O
gradient	O
of	O
the	O
log	O
probability	O
distribution	O
as	O
well	O
as	O
about	O
the	O
distribution	O
itself	O
.	O
an	O
analogous	O
situation	O
is	O
familiar	O
from	O
the	O
domain	O
of	O
function	O
optimization	O
.	O
in	O
most	O
cases	O
where	O
gradient	O
informa-	O
tion	O
is	O
available	O
,	O
it	O
is	O
highly	O
advantageous	O
to	O
make	O
use	O
of	O
it	O
.	O
informally	O
,	O
this	O
follows	O
from	O
the	O
fact	O
that	O
in	O
a	O
space	O
of	O
dimension	O
d	O
,	O
the	O
additional	O
computational	O
cost	O
of	O
evaluating	O
a	O
gradient	O
compared	O
with	O
evaluating	O
the	O
function	O
itself	O
will	O
typically	O
be	O
a	O
ﬁxed	O
factor	O
independent	O
of	O
d	O
,	O
whereas	O
the	O
d-dimensional	O
gradient	O
vector	O
conveys	O
d	O
pieces	O
of	O
information	O
compared	O
with	O
the	O
one	O
piece	O
of	O
information	O
given	O
by	O
the	O
function	O
itself	O
.	O
552	O
11.	O
sampling	B
methods	I
11.5.2	O
hybrid	O
monte	O
carlo	O
as	O
we	O
discussed	O
in	O
the	O
previous	O
section	O
,	O
for	O
a	O
nonzero	O
step	O
size	O
	O
,	O
the	O
discretiza-	O
tion	O
of	O
the	O
leapfrog	O
algorithm	O
will	O
introduce	O
errors	O
into	O
the	O
integration	O
of	O
the	O
hamil-	O
tonian	O
dynamical	O
equations	O
.	O
hybrid	O
monte	O
carlo	O
(	O
duane	O
et	O
al.	O
,	O
1987	O
;	O
neal	O
,	O
1996	O
)	O
combines	O
hamiltonian	O
dynamics	O
with	O
the	O
metropolis	O
algorithm	O
and	O
thereby	O
removes	O
any	O
bias	B
associated	O
with	O
the	O
discretization	O
.	O
speciﬁcally	O
,	O
the	O
algorithm	O
uses	O
a	O
markov	O
chain	O
consisting	O
of	O
alternate	O
stochastic	B
updates	O
of	O
the	O
momentum	B
variable	I
r	O
and	O
hamiltonian	O
dynamical	O
updates	O
using	O
the	O
leapfrog	O
algorithm	O
.	O
after	O
each	O
application	O
of	O
the	O
leapfrog	O
algorithm	O
,	O
the	O
resulting	O
candidate	O
state	O
is	O
accepted	O
or	O
rejected	O
according	O
to	O
the	O
metropolis	O
criterion	O
based	O
on	O
the	O
value	O
of	O
the	O
hamiltonian	O
h.	O
thus	O
if	O
(	O
z	O
,	O
r	O
)	O
is	O
the	O
initial	O
state	O
and	O
(	O
z	O
(	O
cid:1	O
)	O
,	O
r	O
(	O
cid:1	O
)	O
)	O
is	O
the	O
state	O
after	O
the	O
leapfrog	O
integration	O
,	O
then	O
this	O
candidate	O
state	O
is	O
accepted	O
with	O
probability	B
min	O
(	O
1	O
,	O
exp	O
{	O
h	O
(	O
z	O
,	O
r	O
)	O
−	O
h	O
(	O
z	O
(	O
cid:1	O
)	O
,	O
r	O
(	O
cid:1	O
)	O
)	O
}	O
)	O
.	O
(	O
11.67	O
)	O
if	O
the	O
leapfrog	O
integration	O
were	O
to	O
simulate	O
the	O
hamiltonian	O
dynamics	O
perfectly	O
,	O
then	O
every	O
such	O
candidate	O
step	O
would	O
automatically	O
be	O
accepted	O
because	O
the	O
value	O
of	O
h	O
would	O
be	O
unchanged	O
.	O
due	O
to	O
numerical	O
errors	O
,	O
the	O
value	O
of	O
h	O
may	O
sometimes	O
decrease	O
,	O
and	O
we	O
would	O
like	O
the	O
metropolis	O
criterion	O
to	O
remove	O
any	O
bias	B
due	O
to	O
this	O
effect	O
and	O
ensure	O
that	O
the	O
resulting	O
samples	O
are	O
indeed	O
drawn	O
from	O
the	O
required	O
dis-	O
tribution	O
.	O
in	O
order	O
for	O
this	O
to	O
be	O
the	O
case	O
,	O
we	O
need	O
to	O
ensure	O
that	O
the	O
update	O
equations	O
corresponding	O
to	O
the	O
leapfrog	O
integration	O
satisfy	O
detailed	O
balance	O
(	O
11.40	O
)	O
.	O
this	O
is	O
easily	O
achieved	O
by	O
modifying	O
the	O
leapfrog	O
scheme	O
as	O
follows	O
.	O
before	O
the	O
start	O
of	O
each	O
leapfrog	O
integration	O
sequence	O
,	O
we	O
choose	O
at	O
random	O
,	O
with	O
equal	O
probability	B
,	O
whether	O
to	O
integrate	O
forwards	O
in	O
time	O
(	O
using	O
step	O
size	O
	O
)	O
or	O
backwards	O
in	O
time	O
(	O
using	O
step	O
size	O
−	O
)	O
.	O
we	O
ﬁrst	O
note	O
that	O
the	O
leapfrog	O
integration	O
scheme	O
(	O
11.64	O
)	O
,	O
(	O
11.65	O
)	O
,	O
and	O
(	O
11.66	O
)	O
is	O
time-reversible	O
,	O
so	O
that	O
integration	O
for	O
l	O
steps	O
using	O
step	O
size	O
−	O
will	O
exactly	O
undo	O
the	O
effect	O
of	O
integration	O
for	O
l	O
steps	O
using	O
step	O
size	O
	O
.	O
next	O
we	O
show	O
that	O
the	O
leapfrog	O
integration	O
preserves	O
phase-space	O
volume	O
exactly	O
.	O
this	O
follows	O
from	O
the	O
fact	O
that	O
each	O
step	O
in	O
the	O
leapfrog	O
scheme	O
updates	O
either	O
a	O
zi	O
variable	O
or	O
an	O
ri	O
variable	O
by	O
an	O
amount	O
that	O
is	O
a	O
function	O
only	O
of	O
the	O
other	O
variable	O
.	O
as	O
shown	O
in	O
figure	O
11.14	O
,	O
this	O
has	O
the	O
effect	O
of	O
shearing	O
a	O
region	O
of	O
phase	B
space	I
while	O
not	O
altering	O
its	O
volume	O
.	O
finally	O
,	O
we	O
use	O
these	O
results	O
to	O
show	O
that	O
detailed	O
balance	O
holds	O
.	O
consider	O
a	O
small	O
region	O
r	O
of	O
phase	B
space	I
that	O
,	O
under	O
a	O
sequence	O
of	O
l	O
leapfrog	O
iterations	O
of	O
step	O
size	O
	O
,	O
maps	O
to	O
a	O
region	O
r	O
(	O
cid:4	O
)	O
.	O
using	O
conservation	O
of	O
volume	O
under	O
the	O
leapfrog	O
iteration	O
,	O
we	O
see	O
that	O
if	O
r	O
has	O
volume	O
δv	O
then	O
so	O
too	O
will	O
r	O
(	O
cid:4	O
)	O
.	O
if	O
we	O
choose	O
an	O
initial	O
point	O
from	O
the	O
distribution	O
(	O
11.63	O
)	O
and	O
then	O
update	O
it	O
using	O
l	O
leapfrog	O
interactions	O
,	O
the	O
probability	B
of	O
the	O
transition	O
going	O
from	O
r	O
to	O
r	O
(	O
cid:4	O
)	O
is	O
given	O
by	O
1	O
zh	O
exp	O
(	O
−h	O
(	O
r	O
)	O
)	O
δv	O
1	O
2	O
min	O
{	O
1	O
,	O
exp	O
(	O
−h	O
(	O
r	O
)	O
+	O
h	O
(	O
r	O
(	O
cid:4	O
)	O
)	O
)	O
}	O
.	O
(	O
11.68	O
)	O
where	O
the	O
factor	O
of	O
1/2	O
arises	O
from	O
the	O
probability	B
of	O
choosing	O
to	O
integrate	O
with	O
a	O
positive	O
step	O
size	O
rather	O
than	O
a	O
negative	O
one	O
.	O
similarly	O
,	O
the	O
probability	B
of	O
starting	O
in	O
11.5.	O
the	O
hybrid	O
monte	O
carlo	O
algorithm	O
553	O
ri	O
(	O
cid:4	O
)	O
i	O
r	O
zi	O
(	O
cid:4	O
)	O
i	O
z	O
figure	O
11.14	O
each	O
step	O
of	O
the	O
leapfrog	O
algorithm	O
(	O
11.64	O
)	O
–	O
(	O
11.66	O
)	O
modiﬁes	O
either	O
a	O
position	B
variable	I
zi	O
or	O
a	O
momentum	B
variable	I
ri	O
.	O
because	O
the	O
change	O
to	O
one	O
variable	O
is	O
a	O
function	O
only	O
of	O
the	O
other	O
,	O
any	O
region	O
in	O
phase	B
space	I
will	O
be	O
sheared	O
without	O
change	O
of	O
volume	O
.	O
region	O
r	O
(	O
cid:4	O
)	O
and	O
integrating	O
backwards	O
in	O
time	O
to	O
end	O
up	O
in	O
region	O
r	O
is	O
given	O
by	O
1	O
zh	O
min	O
{	O
1	O
,	O
exp	O
(	O
−h	O
(	O
r	O
(	O
cid:4	O
)	O
)	O
+	O
h	O
(	O
r	O
)	O
)	O
}	O
.	O
exp	O
(	O
−h	O
(	O
r	O
(	O
cid:4	O
)	O
)	O
)	O
δv	O
1	O
2	O
(	O
11.69	O
)	O
exercise	O
11.17	O
it	O
is	O
easily	O
seen	O
that	O
the	O
two	O
probabilities	O
(	O
11.68	O
)	O
and	O
(	O
11.69	O
)	O
are	O
equal	O
,	O
and	O
hence	O
detailed	O
balance	O
holds	O
.	O
note	O
that	O
this	O
proof	O
ignores	O
any	O
overlap	O
between	O
the	O
regions	O
r	O
and	O
r	O
(	O
cid:4	O
)	O
but	O
is	O
easily	O
generalized	B
to	O
allow	O
for	O
such	O
overlap	O
.	O
it	O
is	O
not	O
difﬁcult	O
to	O
construct	O
examples	O
for	O
which	O
the	O
leapfrog	O
algorithm	O
returns	O
to	O
its	O
starting	O
position	O
after	O
a	O
ﬁnite	O
number	O
of	O
iterations	O
.	O
in	O
such	O
cases	O
,	O
the	O
random	O
replacement	O
of	O
the	O
momentum	O
values	O
before	O
each	O
leapfrog	O
integration	O
will	O
not	O
be	O
sufﬁcient	O
to	O
ensure	O
ergodicity	O
because	O
the	O
position	O
variables	O
will	O
never	O
be	O
updated	O
.	O
such	O
phenomena	O
are	O
easily	O
avoided	O
by	O
choosing	O
the	O
magnitude	O
of	O
the	O
step	O
size	O
at	O
random	O
from	O
some	O
small	O
interval	O
,	O
before	O
each	O
leapfrog	O
integration	O
.	O
we	O
can	O
gain	O
some	O
insight	O
into	O
the	O
behaviour	O
of	O
the	O
hybrid	O
monte	O
carlo	O
algo-	O
rithm	O
by	O
considering	O
its	O
application	O
to	O
a	O
multivariate	O
gaussian	O
.	O
for	O
convenience	O
,	O
consider	O
a	O
gaussian	O
distribution	O
p	O
(	O
z	O
)	O
with	O
independent	B
components	O
,	O
for	O
which	O
the	O
hamiltonian	O
is	O
given	O
by	O
(	O
cid:2	O
)	O
i	O
(	O
cid:2	O
)	O
i	O
h	O
(	O
z	O
,	O
r	O
)	O
=	O
1	O
2	O
1	O
σ2	O
i	O
z2	O
i	O
+	O
1	O
2	O
r2	O
i	O
.	O
(	O
11.70	O
)	O
our	O
conclusions	O
will	O
be	O
equally	O
valid	O
for	O
a	O
gaussian	O
distribution	O
having	O
correlated	O
components	O
because	O
the	O
hybrid	O
monte	O
carlo	O
algorithm	O
exhibits	O
rotational	O
isotropy	O
.	O
during	O
the	O
leapfrog	O
integration	O
,	O
each	O
pair	O
of	O
phase-space	O
variables	O
zi	O
,	O
ri	O
evolves	O
in-	O
dependently	O
.	O
however	O
,	O
the	O
acceptance	O
or	O
rejection	O
of	O
the	O
candidate	O
point	O
is	O
based	O
on	O
the	O
value	O
of	O
h	O
,	O
which	O
depends	O
on	O
the	O
values	O
of	O
all	O
of	O
the	O
variables	O
.	O
thus	O
,	O
a	O
signiﬁcant	O
integration	O
error	B
in	O
any	O
one	O
of	O
the	O
variables	O
could	O
lead	O
to	O
a	O
high	O
prob-	O
ability	O
of	O
rejection	O
.	O
in	O
order	O
that	O
the	O
discrete	O
leapfrog	O
integration	O
be	O
a	O
reasonably	O
554	O
11.	O
sampling	B
methods	I
good	O
approximation	O
to	O
the	O
true	O
continuous-time	O
dynamics	O
,	O
it	O
is	O
necessary	O
for	O
the	O
leapfrog	O
integration	O
scale	O
	O
to	O
be	O
smaller	O
than	O
the	O
shortest	O
length-scale	O
over	O
which	O
the	O
potential	O
is	O
varying	O
signiﬁcantly	O
.	O
this	O
is	O
governed	O
by	O
the	O
smallest	O
value	O
of	O
σi	O
,	O
which	O
we	O
denote	O
by	O
σmin	O
.	O
recall	O
that	O
the	O
goal	O
of	O
the	O
leapfrog	O
integration	O
in	O
hybrid	O
monte	O
carlo	O
is	O
to	O
move	O
a	O
substantial	O
distance	O
through	O
phase	B
space	I
to	O
a	O
new	O
state	O
that	O
is	O
relatively	O
independent	B
of	O
the	O
initial	O
state	O
and	O
still	O
achieve	O
a	O
high	O
probability	B
of	O
acceptance	O
.	O
in	O
order	O
to	O
achieve	O
this	O
,	O
the	O
leapfrog	O
integration	O
must	O
be	O
continued	O
for	O
a	O
number	O
of	O
iterations	O
of	O
order	O
σmax/σmin	O
.	O
by	O
contrast	O
,	O
consider	O
the	O
behaviour	O
of	O
a	O
simple	O
metropolis	O
algorithm	O
with	O
an	O
isotropic	B
gaussian	O
proposal	B
distribution	I
of	O
variance	B
s2	O
,	O
considered	O
earlier	O
.	O
in	O
order	O
to	O
avoid	O
high	O
rejection	O
rates	O
,	O
the	O
value	O
of	O
s	O
must	O
be	O
of	O
order	O
σmin	O
.	O
the	O
exploration	B
of	O
state	O
space	O
then	O
proceeds	O
by	O
a	O
random	O
walk	O
and	O
takes	O
of	O
order	O
(	O
σmax/σmin	O
)	O
2	O
steps	O
to	O
arrive	O
at	O
a	O
roughly	O
independent	B
state	O
.	O
11.6.	O
estimating	O
the	O
partition	B
function	I
as	O
we	O
have	O
seen	O
,	O
most	O
of	O
the	O
sampling	O
algorithms	O
considered	O
in	O
this	O
chapter	O
re-	O
quire	O
only	O
the	O
functional	B
form	O
of	O
the	O
probability	B
distribution	O
up	O
to	O
a	O
multiplicative	O
constant	O
.	O
thus	O
if	O
we	O
write	O
pe	O
(	O
z	O
)	O
=	O
exp	O
(	O
−e	O
(	O
z	O
)	O
)	O
1	O
ze	O
(	O
11.71	O
)	O
then	O
the	O
value	O
of	O
the	O
normalization	O
constant	O
ze	O
,	O
also	O
known	O
as	O
the	O
partition	O
func-	O
tion	O
,	O
is	O
not	O
needed	O
in	O
order	O
to	O
draw	O
samples	O
from	O
p	O
(	O
z	O
)	O
.	O
however	O
,	O
knowledge	O
of	O
the	O
value	O
of	O
ze	O
can	O
be	O
useful	O
for	O
bayesian	O
model	B
comparison	I
since	O
it	O
represents	O
the	O
model	B
evidence	I
(	O
i.e.	O
,	O
the	O
probability	B
of	O
the	O
observed	O
data	O
given	O
the	O
model	O
)	O
,	O
and	O
so	O
it	O
is	O
of	O
interest	O
to	O
consider	O
how	O
its	O
value	O
might	O
be	O
obtained	O
.	O
we	O
assume	O
that	O
direct	O
evaluation	O
by	O
summing	O
,	O
or	O
integrating	O
,	O
the	O
function	O
exp	O
(	O
−e	O
(	O
z	O
)	O
)	O
over	O
the	O
state	O
space	O
of	O
z	O
is	O
intractable	O
.	O
for	O
model	O
comparison	O
,	O
it	O
is	O
actually	O
the	O
ratio	O
of	O
the	O
partition	O
functions	O
for	O
two	O
models	O
that	O
is	O
required	O
.	O
multiplication	O
of	O
this	O
ratio	O
by	O
the	O
ratio	O
of	O
prior	B
probabilities	O
gives	O
the	O
ratio	O
of	O
posterior	O
probabilities	O
,	O
which	O
can	O
then	O
be	O
used	O
for	O
model	O
selection	O
or	O
model	B
averaging	I
.	O
one	O
way	O
to	O
estimate	O
a	O
ratio	O
of	O
partition	O
functions	O
is	O
to	O
use	O
importance	B
sampling	I
(	O
11.72	O
)	O
ze	O
zg	O
=	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
from	O
a	O
distribution	O
with	O
energy	B
function	I
g	O
(	O
z	O
)	O
(	O
cid:5	O
)	O
z	O
exp	O
(	O
−e	O
(	O
z	O
)	O
)	O
(	O
cid:5	O
)	O
z	O
exp	O
(	O
−g	O
(	O
z	O
)	O
)	O
z	O
exp	O
(	O
−e	O
(	O
z	O
)	O
+	O
g	O
(	O
z	O
)	O
)	O
exp	O
(	O
−g	O
(	O
z	O
)	O
)	O
z	O
exp	O
(	O
−g	O
(	O
z	O
)	O
)	O
(	O
cid:2	O
)	O
=	O
=	O
eg	O
(	O
z	O
)	O
[	O
exp	O
(	O
−e	O
+	O
g	O
)	O
]	O
(	O
cid:7	O
)	O
exp	O
(	O
−e	O
(	O
z	O
(	O
l	O
)	O
)	O
+	O
g	O
(	O
z	O
(	O
l	O
)	O
)	O
)	O
l	O
11.6.	O
estimating	O
the	O
partition	B
function	I
555	O
where	O
{	O
z	O
(	O
l	O
)	O
}	O
are	O
samples	O
drawn	O
from	O
the	O
distribution	O
deﬁned	O
by	O
pg	O
(	O
z	O
)	O
.	O
if	O
the	O
dis-	O
tribution	O
pg	O
is	O
one	O
for	O
which	O
the	O
partition	B
function	I
can	O
be	O
evaluated	O
analytically	O
,	O
for	O
example	O
a	O
gaussian	O
,	O
then	O
the	O
absolute	O
value	O
of	O
ze	O
can	O
be	O
obtained	O
.	O
this	O
approach	O
will	O
only	O
yield	O
accurate	O
results	O
if	O
the	O
importance	B
sampling	I
distri-	O
bution	O
pg	O
is	O
closely	O
matched	O
to	O
the	O
distribution	O
pe	O
,	O
so	O
that	O
the	O
ratio	O
pe/pg	O
does	O
not	O
have	O
wide	O
variations	O
.	O
in	O
practice	O
,	O
suitable	O
analytically	O
speciﬁed	O
importance	B
sampling	I
distributions	O
can	O
not	O
readily	O
be	O
found	O
for	O
the	O
kinds	O
of	O
complex	O
models	O
considered	O
in	O
this	O
book	O
.	O
an	O
alternative	O
approach	O
is	O
therefore	O
to	O
use	O
the	O
samples	O
obtained	O
from	O
a	O
markov	O
chain	O
to	O
deﬁne	O
the	O
importance-sampling	O
distribution	O
.	O
if	O
the	O
transition	B
probability	I
for	O
the	O
markov	O
chain	O
is	O
given	O
by	O
t	O
(	O
z	O
,	O
z	O
(	O
cid:4	O
)	O
)	O
,	O
and	O
the	O
sample	O
set	O
is	O
given	O
by	O
z	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
z	O
(	O
l	O
)	O
,	O
then	O
the	O
sampling	O
distribution	O
can	O
be	O
written	O
as	O
l	O
(	O
cid:2	O
)	O
l=1	O
exp	O
(	O
−g	O
(	O
z	O
)	O
)	O
=	O
1	O
zg	O
which	O
can	O
be	O
used	O
directly	O
in	O
(	O
11.72	O
)	O
.	O
t	O
(	O
z	O
(	O
l	O
)	O
,	O
z	O
)	O
(	O
11.73	O
)	O
methods	O
for	O
estimating	O
the	O
ratio	O
of	O
two	O
partition	O
functions	O
require	O
for	O
their	O
suc-	O
cess	O
that	O
the	O
two	O
corresponding	O
distributions	O
be	O
reasonably	O
closely	O
matched	O
.	O
this	O
is	O
especially	O
problematic	O
if	O
we	O
wish	O
to	O
ﬁnd	O
the	O
absolute	O
value	O
of	O
the	O
partition	B
function	I
for	O
a	O
complex	O
distribution	O
because	O
it	O
is	O
only	O
for	O
relatively	O
simple	O
distributions	O
that	O
the	O
partition	B
function	I
can	O
be	O
evaluated	O
directly	O
,	O
and	O
so	O
attempting	O
to	O
estimate	O
the	O
ratio	O
of	O
partition	O
functions	O
directly	O
is	O
unlikely	O
to	O
be	O
successful	O
.	O
this	O
problem	O
can	O
be	O
tackled	O
using	O
a	O
technique	O
known	O
as	O
chaining	B
(	O
neal	O
,	O
1993	O
;	O
barber	O
and	O
bishop	O
,	O
1997	O
)	O
,	O
which	O
involves	O
introducing	O
a	O
succession	O
of	O
intermediate	O
distributions	O
p2	O
,	O
.	O
.	O
.	O
,	O
pm−1	O
that	O
interpolate	O
between	O
a	O
simple	O
distribution	O
p1	O
(	O
z	O
)	O
for	O
which	O
we	O
can	O
evaluate	O
the	O
normalization	O
coefﬁcient	O
z1	O
and	O
the	O
desired	O
complex	O
distribution	O
pm	O
(	O
z	O
)	O
.	O
we	O
then	O
have	O
(	O
11.74	O
)	O
zm	O
z1	O
=	O
z2	O
z1	O
z3	O
z2	O
···	O
zm	O
zm−1	O
in	O
which	O
the	O
intermediate	O
ratios	O
can	O
be	O
determined	O
using	O
monte	O
carlo	O
methods	O
as	O
discussed	O
above	O
.	O
one	O
way	O
to	O
construct	O
such	O
a	O
sequence	O
of	O
intermediate	O
systems	O
is	O
to	O
use	O
an	O
energy	B
function	I
containing	O
a	O
continuous	O
parameter	O
0	O
(	O
cid:1	O
)	O
α	O
(	O
cid:1	O
)	O
1	O
that	O
interpolates	O
between	O
the	O
two	O
distributions	O
eα	O
(	O
z	O
)	O
=	O
(	O
1	O
−	O
α	O
)	O
e1	O
(	O
z	O
)	O
+	O
αem	O
(	O
z	O
)	O
.	O
(	O
11.75	O
)	O
if	O
the	O
intermediate	O
ratios	O
in	O
(	O
11.74	O
)	O
are	O
to	O
be	O
found	O
using	O
monte	O
carlo	O
,	O
it	O
may	O
be	O
more	O
efﬁcient	O
to	O
use	O
a	O
single	O
markov	O
chain	O
run	O
than	O
to	O
restart	O
the	O
markov	O
chain	O
for	O
each	O
ratio	O
.	O
in	O
this	O
case	O
,	O
the	O
markov	O
chain	O
is	O
run	O
initially	O
for	O
the	O
system	O
p1	O
and	O
then	O
after	O
some	O
suitable	O
number	O
of	O
steps	O
moves	O
on	O
to	O
the	O
next	O
distribution	O
in	O
the	O
sequence	O
.	O
note	O
,	O
however	O
,	O
that	O
the	O
system	O
must	O
remain	O
close	O
to	O
the	O
equilibrium	O
distribution	O
at	O
each	O
stage	O
.	O
556	O
11.	O
sampling	B
methods	I
exercises	O
11.1	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
the	O
ﬁnite	O
sample	O
estimator	O
(	O
cid:1	O
)	O
f	O
deﬁned	O
by	O
(	O
11.2	O
)	O
has	O
mean	B
equal	O
to	O
e	O
[	O
f	O
]	O
and	O
variance	B
given	O
by	O
(	O
11.3	O
)	O
.	O
11.2	O
(	O
(	O
cid:12	O
)	O
)	O
suppose	O
that	O
z	O
is	O
a	O
random	O
variable	O
with	O
uniform	B
distribution	I
over	O
(	O
0	O
,	O
1	O
)	O
and	O
−1	O
(	O
z	O
)	O
where	O
h	O
(	O
y	O
)	O
is	O
given	O
by	O
(	O
11.6	O
)	O
.	O
show	O
that	O
y	O
that	O
we	O
transform	O
z	O
using	O
y	O
=	O
h	O
has	O
the	O
distribution	O
p	O
(	O
y	O
)	O
.	O
11.3	O
(	O
(	O
cid:12	O
)	O
)	O
given	O
a	O
random	O
variable	O
z	O
that	O
is	O
uniformly	O
distributed	O
over	O
(	O
0	O
,	O
1	O
)	O
,	O
ﬁnd	O
a	O
trans-	O
formation	O
y	O
=	O
f	O
(	O
z	O
)	O
such	O
that	O
y	O
has	O
a	O
cauchy	O
distribution	O
given	O
by	O
(	O
11.8	O
)	O
.	O
11.4	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
suppose	O
that	O
z1	O
and	O
z2	O
are	O
uniformly	O
distributed	O
over	O
the	O
unit	O
circle	O
,	O
as	O
shown	O
in	O
figure	O
11.3	O
,	O
and	O
that	O
we	O
make	O
the	O
change	O
of	O
variables	O
given	O
by	O
(	O
11.10	O
)	O
and	O
(	O
11.11	O
)	O
.	O
show	O
that	O
(	O
y1	O
,	O
y2	O
)	O
will	O
be	O
distributed	O
according	O
to	O
(	O
11.12	O
)	O
.	O
11.5	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
let	O
z	O
be	O
a	O
d-dimensional	O
random	O
variable	O
having	O
a	O
gaussian	O
distribu-	O
tion	O
with	O
zero	O
mean	B
and	O
unit	O
covariance	B
matrix	I
,	O
and	O
suppose	O
that	O
the	O
positive	B
deﬁnite	I
symmetric	O
matrix	O
σ	O
has	O
the	O
cholesky	O
decomposition	O
σ	O
=	O
llt	O
where	O
l	O
is	O
a	O
lower-	O
triangular	O
matrix	O
(	O
i.e.	O
,	O
one	O
with	O
zeros	O
above	O
the	O
leading	O
diagonal	B
)	O
.	O
show	O
that	O
the	O
variable	O
y	O
=	O
µ	O
+	O
lz	O
has	O
a	O
gaussian	O
distribution	O
with	O
mean	B
µ	O
and	O
covariance	B
σ.	O
this	O
provides	O
a	O
technique	O
for	O
generating	O
samples	O
from	O
a	O
general	O
multivariate	O
gaus-	O
sian	O
using	O
samples	O
from	O
a	O
univariate	O
gaussian	O
having	O
zero	O
mean	B
and	O
unit	O
variance	B
.	O
11.6	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
in	O
this	O
exercise	O
,	O
we	O
show	O
more	O
carefully	O
that	O
rejection	B
sampling	I
does	O
indeed	O
draw	O
samples	O
from	O
the	O
desired	O
distribution	O
p	O
(	O
z	O
)	O
.	O
suppose	O
the	O
proposal	O
dis-	O
tribution	O
is	O
q	O
(	O
z	O
)	O
and	O
show	O
that	O
the	O
probability	B
of	O
a	O
sample	O
value	O
z	O
being	O
accepted	O
is	O
given	O
by	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
/kq	O
(	O
z	O
)	O
where	O
(	O
cid:4	O
)	O
p	O
is	O
any	O
unnormalized	O
distribution	O
that	O
is	O
proportional	O
to	O
p	O
(	O
z	O
)	O
,	O
and	O
the	O
constant	O
k	O
is	O
set	O
to	O
the	O
smallest	O
value	O
that	O
ensures	O
kq	O
(	O
z	O
)	O
(	O
cid:2	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
)	O
for	O
all	O
values	O
of	O
z.	O
note	O
that	O
the	O
probability	B
of	O
drawing	O
a	O
value	O
z	O
is	O
given	O
by	O
the	O
probability	B
of	O
drawing	O
that	O
value	O
from	O
q	O
(	O
z	O
)	O
times	O
the	O
probability	B
of	O
accepting	O
that	O
value	O
given	O
that	O
it	O
has	O
been	O
drawn	O
.	O
make	O
use	O
of	O
this	O
,	O
along	O
with	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
,	O
to	O
write	O
down	O
the	O
normalized	O
form	O
for	O
the	O
distribution	O
over	O
z	O
,	O
and	O
show	O
that	O
it	O
equals	O
p	O
(	O
z	O
)	O
.	O
11.7	O
(	O
(	O
cid:12	O
)	O
)	O
suppose	O
that	O
z	O
has	O
a	O
uniform	B
distribution	I
over	O
the	O
interval	O
[	O
0	O
,	O
1	O
]	O
.	O
show	O
that	O
the	O
variable	O
y	O
=	O
b	O
tan	O
z	O
+	O
c	O
has	O
a	O
cauchy	O
distribution	O
given	O
by	O
(	O
11.16	O
)	O
.	O
11.8	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
determine	O
expressions	O
for	O
the	O
coefﬁcients	O
ki	O
in	O
the	O
envelope	O
distribution	O
(	O
11.17	O
)	O
for	O
adaptive	O
rejection	B
sampling	I
using	O
the	O
requirements	O
of	O
continuity	O
and	O
nor-	O
malization	O
.	O
11.9	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
by	O
making	O
use	O
of	O
the	O
technique	O
discussed	O
in	O
section	O
11.1.1	O
for	O
sampling	O
from	O
a	O
single	O
exponential	B
distribution	I
,	O
devise	O
an	O
algorithm	O
for	O
sampling	O
from	O
the	O
piecewise	O
exponential	B
distribution	I
deﬁned	O
by	O
(	O
11.17	O
)	O
.	O
11.10	O
(	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
the	O
simple	O
random	O
walk	O
over	O
the	O
integers	O
deﬁned	O
by	O
(	O
11.34	O
)	O
,	O
(	O
11.35	O
)	O
,	O
and	O
(	O
11.36	O
)	O
has	O
the	O
property	O
that	O
e	O
[	O
(	O
z	O
(	O
τ	O
)	O
)	O
2	O
]	O
=	O
e	O
[	O
(	O
z	O
(	O
τ−1	O
)	O
)	O
2	O
]	O
+	O
1/2	O
and	O
hence	O
by	O
induction	O
that	O
e	O
[	O
(	O
z	O
(	O
τ	O
)	O
)	O
2	O
]	O
=	O
τ	O
/2	O
.	O
figure	O
11.15	O
a	O
probability	B
distribution	O
over	O
two	O
variables	O
z1	O
and	O
z2	O
that	O
is	O
uniform	O
over	O
the	O
shaded	O
regions	O
and	O
that	O
is	O
zero	O
everywhere	O
else	O
.	O
z2	O
exercises	O
557	O
z1	O
11.11	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
the	O
gibbs	O
sampling	O
algorithm	O
,	O
discussed	O
in	O
section	O
11.3	O
,	O
satisﬁes	O
detailed	O
balance	O
as	O
deﬁned	O
by	O
(	O
11.40	O
)	O
.	O
11.12	O
(	O
(	O
cid:12	O
)	O
)	O
consider	O
the	O
distribution	O
shown	O
in	O
figure	O
11.15.	O
discuss	O
whether	O
the	O
standard	O
gibbs	O
sampling	O
procedure	O
for	O
this	O
distribution	O
is	O
ergodic	O
,	O
and	O
therefore	O
whether	O
it	O
would	O
sample	O
correctly	O
from	O
this	O
distribution	O
11.13	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
the	O
simple	O
3-node	O
graph	O
shown	O
in	O
figure	O
11.16	O
in	O
which	O
the	O
observed	O
node	O
x	O
is	O
given	O
by	O
a	O
gaussian	O
distribution	O
n	O
(	O
x|µ	O
,	O
τ	O
−1	O
)	O
with	O
mean	B
µ	O
and	O
precision	O
τ	O
.	O
suppose	O
that	O
the	O
marginal	B
distributions	O
over	O
the	O
mean	B
and	O
precision	O
are	O
given	O
by	O
n	O
(	O
µ|µ0	O
,	O
s0	O
)	O
and	O
gam	O
(	O
τ|a	O
,	O
b	O
)	O
,	O
where	O
gam	O
(	O
·|·	O
,	O
·	O
)	O
denotes	O
a	O
gamma	B
distribution	I
.	O
write	O
down	O
expressions	O
for	O
the	O
conditional	B
distributions	O
p	O
(	O
µ|x	O
,	O
τ	O
)	O
and	O
p	O
(	O
τ|x	O
,	O
µ	O
)	O
that	O
would	O
be	O
required	O
in	O
order	O
to	O
apply	O
gibbs	O
sampling	O
to	O
the	O
posterior	O
distribution	O
p	O
(	O
µ	O
,	O
τ|x	O
)	O
.	O
11.14	O
(	O
(	O
cid:12	O
)	O
)	O
verify	O
that	O
the	O
over-relaxation	B
update	O
(	O
11.50	O
)	O
,	O
in	O
which	O
zi	O
has	O
mean	B
µi	O
and	O
(	O
cid:4	O
)	O
i	O
with	O
variance	B
σi	O
,	O
and	O
where	O
ν	O
has	O
zero	O
mean	B
and	O
unit	O
variance	B
,	O
gives	O
a	O
value	O
z	O
mean	B
µi	O
and	O
variance	B
σ2	O
i	O
.	O
11.15	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
using	O
(	O
11.56	O
)	O
and	O
(	O
11.57	O
)	O
,	O
show	O
that	O
the	O
hamiltonian	O
equation	O
(	O
11.58	O
)	O
is	O
equivalent	O
to	O
(	O
11.53	O
)	O
.	O
similarly	O
,	O
using	O
(	O
11.57	O
)	O
show	O
that	O
(	O
11.59	O
)	O
is	O
equivalent	O
to	O
(	O
11.55	O
)	O
.	O
11.16	O
(	O
(	O
cid:12	O
)	O
)	O
by	O
making	O
use	O
of	O
(	O
11.56	O
)	O
,	O
(	O
11.57	O
)	O
,	O
and	O
(	O
11.63	O
)	O
,	O
show	O
that	O
the	O
conditional	B
dis-	O
tribution	O
p	O
(	O
r|z	O
)	O
is	O
a	O
gaussian	O
.	O
figure	O
11.16	O
a	O
graph	O
involving	O
an	O
observed	O
gaussian	O
variable	O
x	O
with	O
prior	B
distributions	O
over	O
its	O
mean	B
µ	O
and	O
precision	O
τ.	O
µ	O
τ	O
x	O
558	O
11.	O
sampling	B
methods	I
11.17	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
verify	O
that	O
the	O
two	O
probabilities	O
(	O
11.68	O
)	O
and	O
(	O
11.69	O
)	O
are	O
equal	O
,	O
and	O
hence	O
that	O
detailed	O
balance	O
holds	O
for	O
the	O
hybrid	O
monte	O
carlo	O
algorithm	O
.	O
appendix	O
a	O
in	O
chapter	O
9	O
,	O
we	O
discussed	O
probabilistic	O
models	O
having	O
discrete	O
latent	O
variables	O
,	O
such	O
as	O
the	O
mixture	O
of	O
gaussians	O
.	O
we	O
now	O
explore	O
models	O
in	O
which	O
some	O
,	O
or	O
all	O
,	O
of	O
the	O
latent	O
variables	O
are	O
continuous	O
.	O
an	O
important	O
motivation	O
for	O
such	O
models	O
is	O
that	O
many	O
data	O
sets	O
have	O
the	O
property	O
that	O
the	O
data	O
points	O
all	O
lie	O
close	O
to	O
a	O
manifold	B
of	O
much	O
lower	O
dimensionality	O
than	O
that	O
of	O
the	O
original	O
data	O
space	O
.	O
to	O
see	O
why	O
this	O
might	O
arise	O
,	O
consider	O
an	O
artificial	O
data	O
set	O
constructed	O
by	O
taking	O
one	O
of	O
the	O
off-line	O
digits	O
,	O
represented	O
by	O
a	O
64	O
x	O
64	O
pixel	O
grey-level	O
image	O
,	O
and	O
embedding	O
it	O
in	O
a	O
larger	O
image	O
of	O
size	O
100	O
x	O
100	O
by	O
padding	O
with	O
pixels	O
having	O
the	O
value	O
zero	O
(	O
corresponding	O
to	O
white	O
pixels	O
)	O
in	O
which	O
the	O
location	O
and	O
orientation	O
of	O
the	O
digit	O
is	O
varied	O
at	O
random	O
,	O
as	O
illustrated	O
in	O
figure	O
12.1.	O
each	O
of	O
the	O
resulting	O
images	O
is	O
represented	O
by	O
a	O
point	O
in	O
the	O
100	O
x	O
100	O
=	O
10	O
,	O
ooo-dimensional	O
data	O
space	O
.	O
however	O
,	O
across	O
a	O
data	O
set	O
of	O
such	O
images	O
,	O
there	O
are	O
only	O
three	O
degrees	O
offreedom	O
of	O
variability	O
,	O
corresponding	O
to	O
the	O
vertical	O
and	O
horizontal	O
translations	O
and	O
the	O
rotations	O
.	O
the	O
data	O
points	O
will	O
therefore	O
live	O
on	O
a	O
subspace	O
of	O
the	O
data	O
space	O
whose	O
intrinsic	B
dimensionality	I
is	O
three	O
.	O
note	O
559	O
560	O
12.	O
continuous	O
latent	O
variables	O
figure	O
12.1	O
a	O
synthetic	O
data	O
sel	O
obtained	O
by	O
taking	O
one	O
of	O
the	O
off-line	O
digit	O
images	O
and	O
creating	O
multi	O
(	O
cid:173	O
)	O
ple	O
copies	O
in	O
each	O
of	O
which	O
the	O
digit	O
has	O
undergone	O
a	O
random	O
displacement	O
and	O
rotation	O
within	O
some	O
larger	O
image	O
field	O
.	O
the	O
resulting	O
images	O
each	O
have	O
100	O
)	O
(	O
100	O
=	O
10.000	O
pixels	O
.	O
that	O
the	O
manifold	B
will	O
be	O
nonlinear	O
because	O
.	O
for	O
instance	O
.	O
if	O
we	O
translate	O
the	O
digit	O
past	O
a	O
particular	O
pixel	O
,	O
that	O
pixel	O
value	O
will	O
go	O
from	O
zero	O
(	O
white	O
)	O
10	O
one	O
(	O
black	O
)	O
and	O
back	O
to	O
zero	O
again	O
.	O
which	O
is	O
clearly	O
a	O
nonlinear	O
function	O
of	O
the	O
digit	O
position	O
.	O
in	O
this	O
example	O
.	O
!	O
.he	O
lranslation	O
and	O
rotation	O
parameters	O
are	O
latent	O
variables	O
because	O
we	O
observe	O
only	O
the	O
image	O
vectors	O
and	O
are	O
not	O
told	O
which	O
values	O
of	O
the	O
translation	O
or	O
rotation	O
variables	O
were	O
used	O
to	O
create	O
them	O
.	O
for	O
real	O
digit	O
image	O
data	O
,	O
there	O
will	O
be	O
a	O
funher	O
degree	O
of	O
freedom	O
arising	O
from	O
scaling	O
.	O
moreover	O
there	O
will	O
be	O
multiple	O
addilional	O
degrees	B
of	I
freedom	I
associaled	O
wilh	O
more	O
complex	O
deformations	O
due	O
to	O
the	O
variability	O
in	O
an	O
individual	O
's	O
wriling	O
3s	O
well	O
as	O
lhe	O
differences	O
in	O
writing	O
slyles	O
between	O
individuals	O
.	O
evenheless	O
.	O
the	O
number	O
of	O
such	O
degrees	B
of	I
freedom	I
will	O
be	O
small	O
compared	O
to	O
the	O
dimensionality	O
of	O
ihe	O
data	O
set	O
.	O
another	O
example	O
is	O
provided	O
by	O
the	O
oil	O
flow	O
data	O
set	O
.	O
in	O
which	O
(	O
for	O
a	O
given	O
ge-	O
ometrical	O
configuration	O
of	O
the	O
gas	O
,	O
woller	O
,	O
and	O
oil	O
phases	O
)	O
there	O
are	O
only	O
two	O
degrees	B
of	I
freedom	I
of	O
variability	O
corresponding	O
to	O
the	O
fraction	O
of	O
oil	O
in	O
the	O
pipe	O
and	O
the	O
frac	O
(	O
cid:173	O
)	O
tion	O
of	O
water	O
(	O
the	O
fraction	O
of	O
gas	O
ihen	O
being	O
determined	O
)	O
.	O
ahhough	O
the	O
data	O
space	O
comprises	O
12	O
measuremenls	O
,	O
a	O
data	O
set	O
of	O
points	O
will	O
lie	O
close	O
to	O
a	O
iwo-dimensional	O
manifold	B
embedded	O
within	O
this	O
space	O
.	O
in	O
this	O
case	O
,	O
the	O
manifold	B
comprises	O
scveral	O
distinct	O
segments	O
corresponding	O
to	O
different	O
flow	O
regimes	O
.	O
each	O
such	O
segment	O
being	O
a	O
(	O
noisy	O
)	O
continuous	O
two-dimensional	O
manifold	B
.	O
if	O
our	O
goal	O
is	O
data	B
compression	I
.	O
or	O
density	B
modelling	O
,	O
then	O
there	O
can	O
be	O
benefits	O
in	O
exploiling	O
this	O
manifold	B
struclure	O
.	O
the	O
data	O
points	O
will	O
not	O
be	O
confined	O
precisely	O
to	O
a	O
smooth	O
low	O
(	O
cid:173	O
)	O
dimensional	O
manifold	B
,	O
and	O
we	O
can	O
interpret	O
the	O
departures	O
of	O
data	O
points	O
from	O
the	O
manifold	B
as	O
·noise	O
'	O
.	O
this	O
leads	O
naturally	O
to	O
a	O
generative	O
view	O
of	O
such	O
models	O
in	O
which	O
we	O
first	O
select	O
a	O
poinl	O
within	O
the	O
manifold	B
according	O
to	O
some	O
latent	B
variable	I
distribution	O
and	O
then	O
generate	O
an	O
observed	O
data	O
point	O
by	O
:	O
ldding	O
noise	O
,	O
drawn	O
from	O
some	O
conditional	B
distribution	O
of	O
the	O
data	O
varillbles	O
given	O
the	O
latent	O
varillbles	O
.	O
in	O
praclice	O
.	O
thc	O
simplest	O
continuous	O
latent	B
variable	I
model	O
assumes	O
gaussian	O
distributions	O
for	O
both	O
thc	O
latent	O
and	O
observed	O
variables	O
and	O
makes	O
use	O
of	O
a	O
linear	O
,	O
gaussian	O
de-	O
pendence	O
of	O
the	O
observed	O
variables	O
on	O
ihe	O
slate	O
of	O
the	O
latent	O
variables	O
.	O
this	O
leads	O
to	O
a	O
probabilislic	O
fonnulation	O
of	O
the	O
well-known	O
technique	O
of	O
principal	B
component	I
analysis	I
(	O
pea	O
)	O
,	O
as	O
well	O
as	O
10	O
a	O
related	O
model	O
called	O
factor	B
analysis	I
.	O
in	O
this	O
chapter	O
w	O
will	O
begin	O
wilh	O
a	O
slandard	O
,	O
nonprobabilistic	O
treatment	O
of	O
pea	O
.	O
and	O
thcn	O
we	O
show	O
how	O
pea	O
arises	O
naturally	O
as	O
the	O
maximum	B
likelihood	I
solution	O
10	O
appendix	O
a	O
section	O
8.1..j	O
section	O
12.1	O
12.1.	O
principal	O
c01n	O
[	O
>	O
om	O
''	O
1	O
analjsis	O
561	O
flgu	O
,	O
e12.2	O
p'if	O
>	O
cipal	O
compooont	O
a	O
''	O
,	O
,~	O
''	O
seeks	O
''	O
$	O
pace	O
01	O
!	O
owe	O
,	O
dimensionality	O
.	O
kt	O
''	O
(	O
>	O
wil	O
as	O
!	O
he	O
p	O
<	O
lno	O
>	O
pal	O
subspace	O
``	O
nd	O
denoted	O
i	O
:	O
jy	O
the	O
magenta	O
``	O
1	O
line	O
.	O
such	O
itlet	O
the	O
grthogonet	O
[	O
jiojectioh	O
01	O
!	O
he	O
data	O
points	O
(	O
'ed	O
doisl	O
onto	O
tp'ns~	O
''	O
''	O
''	O
'imizes	O
the	O
varia	O
,	O
..	O
,	O
.	O
,	O
of	O
!	O
he	O
proja	O
<	O
:	O
ted	O
points	O
(	O
green	O
dois	O
)	O
.	O
an	O
``	O
it	O
''	O
,	O
nati	O
''	O
''	O
...	O
.finilion	O
01	O
pca	O
is	O
based	O
on	O
m..mizing	O
the	O
``	O
''	O
''	O
,	O
-	O
<	O
>	O
i·squares	O
of	O
!	O
he	O
projection	O
errors	O
.	O
ind'cated	O
by	O
the	O
bfi.	O
>	O
e	O
lines	O
.	O
s'crio	O
''	O
12.2	O
s'di	O
''	O
''	O
12.4	O
a	O
panlcula	O
,	O
fonn	O
of	O
linear-gau	O
''	O
ian	O
latem	O
``	O
ariable	O
model	O
.	O
this	O
probabilistic	O
refor	O
(	O
cid:173	O
)	O
mulation	O
bring~	O
many	O
ad\'imlag~s	O
,	O
su~h	O
as	O
tl	O
>	O
l	O
:	O
use	O
i	O
)	O
f	O
em	O
for	O
parameter	O
eslimalion	O
,	O
rrinciple	O
<	O
j	O
c~tensioos	O
10	O
oli~turc	O
,	O
of	O
pea	O
model	O
''	O
and	O
ba	O
)	O
'~sian	O
formulat	O
;	O
ons	O
that	O
allow	O
tbe	O
number	O
of	O
rrincipal	O
com	O
[	O
>	O
oncnts	O
to	O
be	O
detennined	O
aulomatically	O
from	O
!	O
be	O
data	O
.	O
finally	O
'	O
.	O
``	O
c	O
disl	O
;	O
us	O
<	O
briefly	O
``	O
''	O
'eral	O
gencrali	O
,	O
ation	O
,	O
of	O
the	O
latent	O
yariable	O
concept	O
that	O
g	O
<	O
l	O
be~ood	O
tbe	O
linear-gaussian	O
assumption	O
including	O
non·gau	O
''	O
i	O
''	O
n	O
i.tcnt	O
yari	O
(	O
cid:173	O
)	O
abies	O
...	O
..hich	O
lea	O
'	O
''	O
to	O
tbe	O
fr.me	O
...	O
.ork	O
of	O
indrl	O
''	O
'mj.m	O
compon.nl	O
anal	O
,	O
-	O
.	O
;	O
.	O
,	O
as	O
...	O
.ell	O
a	O
,	O
models	O
ha	O
''	O
ing	O
a	O
nonlinear	O
rclationship	O
bet	O
...	O
.een	O
latent	O
and	O
oose	O
''	O
,	O
e	O
<	O
j	O
,	O
'luiable	O
,	O
.	O
____'c2=.~1	O
.	O
principal	B
component	I
analysis	I
principal	O
compooem	O
analy	O
,	O
;	O
''	O
or	O
rca	O
.	O
;	O
s	O
a	O
technique	O
tha	O
!	O
is	O
``	O
'idely	O
u	O
<	O
ed	O
for	O
appli	O
.	O
cations	O
such	O
as	O
dimensionality	O
.-eduction	O
,	O
lossy	O
data	O
comprc	O
''	O
ion	O
,	O
feature	O
e	O
>	O
tracti	O
''	O
''	O
.	O
and	O
data	O
v	O
;	O
,	O
ualizatioll	O
(	O
jolliffe	O
,	O
2	O
(	O
02	O
)	O
.	O
it	O
;	O
s	O
also	O
kno	O
...	O
.	O
''	O
as	O
tile	O
karoan.n·i..	O
,	O
;	O
''	O
''	O
tran	O
,	O
·	O
f~	O
.	O
lbcrc	O
an	O
:	O
t	O
...	O
.o	O
commonly	O
used	O
definitions	O
of	O
pea	O
that	O
giye	O
rise	O
to	O
the	O
>	O
arne	O
algorithm	O
.	O
pea	O
can	O
be	O
defined	O
as	O
the	O
unhog	O
<	O
lnal	O
projtttion	O
of	O
the	O
data	O
o/1to	O
a	O
lo	O
...	O
.er	O
dimensionallincar	O
space	O
.	O
kno	O
...	O
.n	O
as	O
the	O
pri/lcip.al	O
$	O
uh.•p.aa	O
.	O
soch	O
that	O
the	O
\'ariance	O
of	O
the	O
projttted	O
data	O
i	O
'	O
ma~imi	O
,	O
e	O
<	O
j	O
(	O
1i	O
''	O
,	O
.liing	O
.	O
1933	O
)	O
.	O
equi	O
''	O
alemly	O
,	O
;	O
t	O
can	O
be	O
defined	O
as	O
tbe	O
linear	O
projection	O
that	O
minimi	O
''	O
'	O
.	O
the	O
average	O
projttlion	O
cost	O
.	O
defined	O
as	O
t~	O
mean	B
squa.-ed	O
distance	O
!	O
letween	O
the	O
data	O
[	O
>	O
oint	O
<	O
and	O
tbeir	O
p	O
<	O
ojtttioo	O
,	O
(	O
pearson	O
,	O
19	O
(	O
1	O
)	O
.	O
the	O
l	O
''	O
j	O
'	O
''	O
oc	O
''	O
s	O
<	O
of	O
onhogonal	O
projection	O
i	O
'	O
illustraled	O
in	O
figute	O
12.2.	O
we	O
con	O
,	O
ider	O
each	O
of	O
these	O
definitions	O
in	O
tum	O
.	O
12,1.1	O
mllximllm	O
variance	B
lormulation	O
con	O
,	O
ider	O
a	O
dala	O
set	O
<	O
if	O
obser	O
''	O
\lations	O
{	O
x	O
,	O
,	O
}	O
where	O
''	O
=	O
1	O
...	O
..	O
s	O
,	O
and	O
x	O
''	O
i	O
,	O
a	O
euclidean	O
variable	O
``	O
'ilh	O
dimen	O
,	O
ionality	O
d.	O
our	O
goal	O
is	O
to	O
project	O
if	O
>	O
/	O
:	O
:	O
data	O
onto	O
a	O
'pace	O
ha	O
''	O
ing	O
dimen	O
,	O
ionality	O
m	O
<	O
d	O
''	O
hile	O
ill3jli	O
''	O
,	O
i	O
,	O
illg	O
the	O
``	O
ariallce	O
of	O
the	O
projttted	O
data	O
.	O
for	O
the	O
!	O
noll..nl	O
.	O
we	O
'hall	O
assume	O
that	O
tbe	O
``	O
alue	O
of	O
m	O
is	O
g	O
;	O
\·en	O
.	O
latcr	O
in	O
this	O
562	O
12.	O
continuous	O
latent	O
variables	O
chapter	O
,	O
we	O
shall	O
consider	O
techniques	O
to	O
determine	O
an	O
appropriate	O
value	O
of	O
iv	O
!	O
from	O
the	O
data	O
.	O
to	O
begin	O
with	O
,	O
consider	O
the	O
projection	O
onto	O
a	O
one-dimensional	O
space	O
(	O
m	O
=	O
1	O
)	O
.	O
we	O
can	O
define	O
the	O
direction	O
of	O
this	O
space	O
using	O
a	O
d-dimensional	O
vector	O
ul	O
,	O
which	O
for	O
convenience	O
(	O
and	O
without	O
loss	O
of	O
generality	O
)	O
we	O
shall	O
choose	O
to	O
be	O
a	O
unit	O
vector	O
so	O
that	O
uf	O
ul	O
=	O
1	O
(	O
note	O
that	O
we	O
are	O
only	O
interested	O
in	O
the	O
direction	O
defined	O
by	O
ul	O
,	O
not	O
in	O
the	O
magnitude	O
of	O
ul	O
itself	O
)	O
.	O
each	O
data	O
point	O
x	O
n	O
is	O
then	O
projected	O
onto	O
a	O
scalar	O
value	O
uf	O
x	O
n	O
.	O
the	O
mean	B
of	O
the	O
projected	O
data	O
is	O
ufx	O
where	O
x	O
is	O
the	O
sample	O
set	O
mean	B
given	O
by	O
and	O
the	O
variance	B
of	O
the	O
projected	O
data	O
is	O
given	O
by	O
(	O
12.1	O
)	O
(	O
12.2	O
)	O
where	O
s	O
is	O
the	O
data	O
covariance	O
matrix	O
defined	O
by	O
s	O
=	O
-	O
``	O
(	O
xn	O
-	O
x	O
)	O
(	O
xn	O
-	O
x	O
)	O
t	O
1	O
n	O
nlj	O
n=l	O
.	O
(	O
12.3	O
)	O
appendix	O
e	O
we	O
now	O
maximize	O
the	O
projected	O
variance	B
ufsul	O
with	O
respect	O
to	O
ul	O
.	O
clearly	O
,	O
this	O
has	O
to	O
be	O
a	O
constrained	O
maximization	O
to	O
prevent	O
ilulll	O
...	O
..	O
00.	O
the	O
appropriate	O
constraint	O
comes	O
from	O
the	O
normalization	O
condition	O
uf	O
ul	O
=	O
1.	O
to	O
enforce	O
this	O
constraint	O
,	O
we	O
introduce	O
a	O
lagrange	O
multiplier	O
that	O
we	O
shall	O
denote	O
by	O
ai	O
,	O
and	O
then	O
make	O
an	O
unconstrained	O
maximization	O
of	O
(	O
12.4	O
)	O
by	O
setting	O
the	O
derivative	B
with	O
respect	O
to	O
ul	O
equal	O
to	O
zero	O
,	O
we	O
see	O
that	O
this	O
quantity	O
will	O
have	O
a	O
stationary	B
point	O
when	O
(	O
12.5	O
)	O
which	O
says	O
that	O
ul	O
must	O
be	O
an	O
eigenvector	O
of	O
s.	O
if	O
we	O
left-multiply	O
by	O
uf	O
and	O
make	O
use	O
of	O
uf	O
ul	O
=	O
1	O
,	O
we	O
see	O
that	O
the	O
variance	B
is	O
given	O
by	O
and	O
so	O
the	O
variance	B
will	O
be	O
a	O
maximum	O
when	O
we	O
set	O
ul	O
equal	O
to	O
the	O
eigenvector	O
having	O
the	O
largest	O
eigenvalue	O
ai	O
.	O
this	O
eigenvector	O
is	O
known	O
as	O
the	O
first	O
principal	O
component	O
.	O
we	O
can	O
define	O
additional	O
principal	O
components	O
in	O
an	O
incremental	O
fashion	O
by	O
choosing	O
each	O
new	O
direction	O
to	O
be	O
that	O
which	O
maximizes	O
the	O
projected	O
variance	B
(	O
12.6	O
)	O
exercise	O
12.1	O
section	O
12.2.2	O
appendix	O
c	O
12.1.	O
principal	B
component	I
analysis	I
563	O
amongst	O
all	O
possible	O
directions	O
orthogonal	O
to	O
those	O
already	O
considered	O
.	O
if	O
we	O
con	O
(	O
cid:173	O
)	O
sider	O
the	O
general	O
case	O
of	O
an	O
m	O
-dimensional	O
projection	O
space	O
,	O
the	O
optimal	O
linear	O
pro	O
(	O
cid:173	O
)	O
jection	O
for	O
which	O
the	O
variance	B
of	O
the	O
projected	O
data	O
is	O
maximized	O
is	O
now	O
defined	O
by	O
the	O
m	O
eigenvectors	O
u	O
1	O
,	O
...	O
,	O
u	O
m	O
of	O
the	O
data	O
covariance	O
matrix	O
s	O
corresponding	O
to	O
the	O
m	O
largest	O
eigenvalues	O
>	O
'1	O
,	O
...	O
,	O
am	O
.	O
this	O
is	O
easily	O
shown	O
using	O
proof	O
by	O
induction	O
.	O
to	O
summarize	O
,	O
principal	B
component	I
analysis	I
involves	O
evaluating	O
the	O
mean	B
x	O
and	O
the	O
covariance	B
matrix	I
s	O
of	O
the	O
data	O
set	O
and	O
then	O
finding	O
the	O
m	O
eigenvectors	O
of	O
s	O
corresponding	O
to	O
the	O
m	O
largest	O
eigenvalues	O
.	O
algorithms	O
for	O
finding	O
eigenvectors	O
and	O
eigenvalues	O
,	O
as	O
well	O
as	O
additional	O
theorems	O
related	O
to	O
eigenvector	O
decomposition	O
,	O
can	O
be	O
found	O
in	O
golub	O
and	O
van	O
loan	O
(	O
1996	O
)	O
.	O
note	O
that	O
the	O
computational	O
cost	O
of	O
computing	O
the	O
full	O
eigenvector	O
decomposition	O
for	O
a	O
matrix	O
of	O
size	O
d	O
x	O
dis	O
o	O
(	O
d3	O
)	O
.	O
if	O
we	O
plan	O
to	O
project	O
our	O
data	O
onto	O
the	O
first	O
m	O
principal	O
components	O
,	O
then	O
we	O
only	O
need	O
to	O
find	O
the	O
first	O
m	O
eigenvalues	O
and	O
eigenvectors	O
.	O
this	O
can	O
be	O
done	O
with	O
more	O
efficient	O
techniques	O
,	O
such	O
as	O
the	O
power	B
method	I
(	O
golub	O
and	O
van	O
loan	O
,	O
1996	O
)	O
,	O
that	O
scale	O
like	O
o	O
(	O
md	O
2	O
)	O
,	O
or	O
alternatively	O
we	O
can	O
make	O
use	O
of	O
the	O
em	O
algorithm	O
.	O
12.1.2	O
minimum-error	O
formulation	O
we	O
now	O
discuss	O
an	O
alternative	O
formulation	O
of	O
pea	O
based	O
on	O
projection	O
error	B
minimization	O
.	O
to	O
do	O
this	O
,	O
we	O
introduce	O
a	O
complete	O
orthonormal	O
set	O
of	O
d-dimensional	O
basis	O
vectors	O
{	O
ui	O
}	O
where	O
i	O
=	O
1	O
,	O
...	O
,	O
d	O
that	O
satisfy	O
(	O
12.7	O
)	O
because	O
this	O
basis	O
is	O
complete	O
,	O
each	O
data	O
point	O
can	O
be	O
represented	O
exactly	O
by	O
a	O
linear	O
combination	O
of	O
the	O
basis	O
vectors	O
d	O
x	O
n	O
=	O
laniui	O
i=l	O
(	O
12.8	O
)	O
where	O
the	O
coefficients	O
ani	O
will	O
be	O
different	O
for	O
different	O
data	O
points	O
.	O
this	O
simply	O
corresponds	O
to	O
a	O
rotation	O
of	O
the	O
coordinate	O
system	O
to	O
a	O
new	O
system	O
defined	O
by	O
the	O
{	O
ui	O
}	O
,	O
and	O
the	O
original	O
d	O
components	O
{	O
xnl	O
'	O
...	O
,	O
xnd	O
}	O
are	O
replaced	O
by	O
an	O
equivalent	O
set	O
{	O
anl	O
'	O
...	O
,	O
and	O
}	O
.	O
taking	O
the	O
inner	O
product	O
with	O
uj	O
,	O
and	O
making	O
use	O
of	O
the	O
or	O
(	O
cid:173	O
)	O
thonormality	O
property	O
,	O
we	O
obtain	O
anj	O
=	O
x	O
;	O
uj	O
,	O
and	O
so	O
without	O
loss	O
of	O
generality	O
we	O
can	O
write	O
d	O
x	O
n	O
=	O
l	O
(	O
x~ui	O
)	O
ui·	O
(	O
12.9	O
)	O
i=l	O
our	O
goal	O
,	O
however	O
,	O
is	O
to	O
approximate	O
this	O
data	O
point	O
using	O
a	O
representation	O
in	O
(	O
cid:173	O
)	O
volving	O
a	O
restricted	O
number	O
m	O
<	O
d	O
of	O
variables	O
corresponding	O
to	O
a	O
projection	O
onto	O
a	O
lower-dimensional	O
subspace	O
.	O
the	O
m	O
-dimensional	O
linear	O
subspace	O
can	O
be	O
repre	O
(	O
cid:173	O
)	O
sented	O
,	O
without	O
loss	O
of	O
generality	O
,	O
by	O
the	O
first	O
m	O
of	O
the	O
basis	O
vectors	O
,	O
and	O
so	O
we	O
approximate	O
each	O
data	O
point	O
x	O
n	O
by	O
m	O
xn	O
=	O
l	O
d	O
zniui	O
+	O
l	O
(	O
12.10	O
)	O
biui	O
i=l	O
i=m+l	O
564	O
12.	O
continuous	O
latent	O
variables	O
where	O
the	O
{	O
zni	O
}	O
depend	O
on	O
the	O
particular	O
data	O
point	O
,	O
whereas	O
the	O
{	O
bd	O
are	O
constants	O
that	O
are	O
the	O
same	O
for	O
all	O
data	O
points	O
.	O
we	O
are	O
free	O
to	O
choose	O
the	O
{	O
ui	O
}	O
,	O
the	O
{	O
zni	O
}	O
,	O
and	O
the	O
{	O
bd	O
so	O
as	O
to	O
minimize	O
the	O
distortion	O
introduced	O
by	O
the	O
reduction	O
in	O
dimensional	O
(	O
cid:173	O
)	O
ity	O
.	O
as	O
our	O
distortion	B
measure	I
,	O
we	O
shall	O
use	O
the	O
squared	O
distance	O
between	O
the	O
original	O
data	O
point	O
x	O
n	O
and	O
its	O
approximation	O
xn	O
,	O
averaged	O
over	O
the	O
data	O
set	O
,	O
so	O
that	O
our	O
goal	O
is	O
to	O
minimize	O
n	O
j	O
=	O
~	O
l	O
ilxn	O
-	O
xn	O
11	O
2	O
.	O
(	O
12.11	O
)	O
n=l	O
consider	O
first	O
of	O
all	O
the	O
minimization	O
with	O
respect	O
to	O
the	O
quantities	O
{	O
zni	O
}	O
.	O
sub	O
(	O
cid:173	O
)	O
stituting	O
for	O
xn	O
,	O
setting	O
the	O
derivative	B
with	O
respect	O
to	O
znj	O
to	O
zero	O
,	O
and	O
making	O
use	O
of	O
the	O
orthonormality	O
conditions	O
,	O
we	O
obtain	O
where	O
j	O
=	O
1	O
,	O
...	O
,	O
m.	O
similarly	O
,	O
setting	O
the	O
derivative	B
of	O
j	O
with	O
respect	O
to	O
bi	O
to	O
zero	O
,	O
and	O
again	O
making	O
use	O
of	O
the	O
orthonormality	O
relations	O
,	O
gives	O
(	O
12.12	O
)	O
(	O
12.13	O
)	O
where	O
j	O
=	O
m	O
+1	O
,	O
...	O
,	O
d.	O
if	O
we	O
substitute	O
for	O
zni	O
and	O
bi	O
,	O
and	O
make	O
use	O
of	O
the	O
general	O
expansion	O
(	O
12.9	O
)	O
,	O
we	O
obtain	O
b	O
j	O
=	O
x	O
uj	O
-t	O
x	O
n	O
-	O
x	O
n	O
=	O
l	O
{	O
(	O
x	O
n	O
-	O
x	O
)	O
tud	O
ui	O
d	O
i=m+l	O
(	O
12.14	O
)	O
from	O
which	O
we	O
see	O
that	O
the	O
displacement	O
vector	O
from	O
x	O
n	O
to	O
xn	O
lies	O
in	O
the	O
space	O
orthogonal	O
to	O
the	O
principal	B
subspace	I
,	O
because	O
it	O
is	O
a	O
linear	O
combination	O
of	O
{	O
ud	O
for	O
i	O
=	O
m	O
+	O
1	O
,	O
...	O
,	O
d	O
,	O
as	O
illustrated	O
in	O
figure	O
12.2.	O
this	O
is	O
to	O
be	O
expected	O
because	O
the	O
projected	O
points	O
xn	O
must	O
lie	O
within	O
the	O
principal	B
subspace	I
,	O
but	O
we	O
can	O
move	O
them	O
freely	O
within	O
that	O
subspace	O
,	O
and	O
so	O
the	O
minimum	O
error	O
is	O
given	O
by	O
the	O
orthogonal	O
projection	O
.	O
we	O
therefore	O
obtain	O
an	O
expression	O
for	O
the	O
distortion	B
measure	I
j	O
as	O
a	O
function	O
purely	O
of	O
the	O
{	O
ud	O
in	O
the	O
form	O
1	O
~	O
~	O
(	O
t	O
j	O
=	O
n	O
l	O
l	O
_t	O
)	O
2	O
=	O
l	O
u	O
i	O
sui	O
.	O
t	O
d	O
(	O
12.15	O
)	O
x	O
n	O
ui	O
-	O
x	O
ui	O
n=l	O
i=m+l	O
i=m+l	O
there	O
remains	O
the	O
task	O
of	O
minimizing	O
j	O
with	O
respect	O
to	O
the	O
{	O
ui	O
}	O
,	O
which	O
must	O
be	O
a	O
constrained	O
minimization	O
otherwise	O
we	O
will	O
obtain	O
the	O
vacuous	O
result	O
ui	O
=	O
o.	O
the	O
constraints	O
arise	O
from	O
the	O
orthonormality	O
conditions	O
and	O
,	O
as	O
we	O
shall	O
see	O
,	O
the	O
solution	O
will	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
eigenvector	O
expansion	O
of	O
the	O
covariance	B
matrix	I
.	O
before	O
considering	O
a	O
formal	O
solution	O
,	O
let	O
us	O
try	O
to	O
obtain	O
some	O
intuition	O
about	O
the	O
result	O
by	O
considering	O
the	O
case	O
of	O
a	O
two-dimensional	O
data	O
space	O
d	O
=	O
2	O
and	O
a	O
one	O
(	O
cid:173	O
)	O
dimensional	O
principal	B
subspace	I
m	O
=	O
1.	O
we	O
have	O
to	O
choose	O
a	O
direction	O
u2	O
so	O
as	O
to	O
12.1.	O
principal	B
component	I
analysis	I
565	O
minimize	O
j	O
=	O
uisu2	O
'	O
subject	O
to	O
the	O
normalization	O
constraint	O
ui	O
u2	O
=	O
1.	O
using	O
a	O
lagrange	O
multiplier	O
a2	O
to	O
enforce	O
the	O
constraint	O
,	O
we	O
consider	O
the	O
minimization	O
of	O
(	O
12.16	O
)	O
setting	O
the	O
derivative	B
with	O
respect	O
to	O
u2	O
to	O
zero	O
,	O
we	O
obtain	O
su2	O
=	O
a2u2	O
so	O
that	O
u2	O
is	O
an	O
eigenvector	O
of	O
s	O
with	O
eigenvalue	O
a2	O
.	O
thus	O
any	O
eigenvector	O
will	O
define	O
a	O
sta	O
(	O
cid:173	O
)	O
tionary	O
point	O
of	O
the	O
distortion	B
measure	I
.	O
to	O
find	O
the	O
value	O
of	O
j	O
at	O
the	O
minimum	O
,	O
we	O
back-substitute	O
the	O
solution	O
for	O
u2	O
into	O
the	O
distortion	B
measure	I
to	O
give	O
j	O
=	O
a2	O
.	O
we	O
therefore	O
obtain	O
the	O
minimum	O
value	O
of	O
j	O
by	O
choosing	O
u2	O
to	O
be	O
the	O
eigenvector	O
corre	O
(	O
cid:173	O
)	O
sponding	O
to	O
the	O
smaller	O
of	O
the	O
two	O
eigenvalues	O
.	O
thus	O
we	O
should	O
choose	O
the	O
principal	B
subspace	I
to	O
be	O
aligned	O
with	O
the	O
eigenvector	O
having	O
the	O
larger	O
eigenvalue	O
.	O
this	O
result	O
accords	O
with	O
our	O
intuition	O
that	O
,	O
in	O
order	O
to	O
minimize	O
the	O
average	O
squared	O
projection	O
distance	O
,	O
we	O
should	O
choose	O
the	O
principal	O
component	O
subspace	O
to	O
pass	O
through	O
the	O
mean	B
of	O
the	O
data	O
points	O
and	O
to	O
be	O
aligned	O
with	O
the	O
directions	O
of	O
maximum	O
variance	O
.	O
for	O
the	O
case	O
when	O
the	O
eigenvalues	O
are	O
equal	O
,	O
any	O
choice	O
of	O
principal	O
direction	O
will	O
give	O
rise	O
to	O
the	O
same	O
value	O
of	O
j.	O
the	O
general	O
solution	O
to	O
the	O
minimization	O
of	O
j	O
for	O
arbitrary	O
d	O
and	O
arbitrary	O
m	O
<	O
d	O
is	O
obtained	O
by	O
choosing	O
the	O
{	O
ui	O
}	O
to	O
be	O
eigenvectors	O
of	O
the	O
covariance	B
matrix	I
given	O
by	O
(	O
12.17	O
)	O
where	O
i	O
=	O
1	O
,	O
...	O
,	O
d	O
,	O
and	O
as	O
usual	O
the	O
eigenvectors	O
{	O
ui	O
}	O
are	O
chosen	O
to	O
be	O
orthonor	O
(	O
cid:173	O
)	O
mal	O
.	O
the	O
corresponding	O
value	O
of	O
the	O
distortion	B
measure	I
is	O
then	O
given	O
by	O
sui	O
=	O
aiui	O
d	O
j=	O
l	O
ai	O
i=m+l	O
(	O
12.18	O
)	O
which	O
is	O
simply	O
the	O
sum	O
of	O
the	O
eigenvalues	O
of	O
those	O
eigenvectors	O
that	O
are	O
orthogonal	O
to	O
the	O
principal	B
subspace	I
.	O
we	O
therefore	O
obtain	O
the	O
minimum	O
value	O
of	O
j	O
by	O
selecting	O
these	O
eigenvectors	O
to	O
be	O
those	O
having	O
the	O
d	O
-	O
m	O
smallest	O
eigenvalues	O
,	O
and	O
hence	O
the	O
eigenvectors	O
defining	O
the	O
principal	B
subspace	I
are	O
those	O
corresponding	O
to	O
the	O
m	O
largest	O
eigenvalues	O
.	O
although	O
we	O
have	O
considered	O
m	O
<	O
d	O
,	O
the	O
pca	O
analysis	O
still	O
holds	O
if	O
m	O
=	O
d	O
,	O
in	O
which	O
case	O
there	O
is	O
no	O
dimensionality	O
reduction	O
but	O
simply	O
a	O
rotation	O
of	O
the	O
coordinate	O
axes	O
to	O
align	O
with	O
principal	O
components	O
.	O
finally	O
,	O
it	O
is	O
worth	O
noting	O
that	O
there	O
exists	O
a	O
closely	O
related	O
linear	O
dimensionality	O
reduction	O
technique	O
called	O
canonical	B
correlation	I
analysis	I
,	O
or	O
cca	O
(	O
hotelling	O
,	O
1936	O
;	O
bach	O
and	O
jordan	O
,	O
2002	O
)	O
.	O
whereas	O
pca	O
works	O
with	O
a	O
single	O
random	O
variable	O
,	O
cca	O
considers	O
two	O
(	O
or	O
more	O
)	O
variables	O
and	O
tries	O
to	O
find	O
a	O
corresponding	O
pair	O
of	O
linear	O
subspaces	O
that	O
have	O
high	O
cross-correlation	O
,	O
so	O
that	O
each	O
component	O
within	O
one	O
of	O
the	O
subspaces	O
is	O
correlated	O
with	O
a	O
single	O
component	O
from	O
the	O
other	O
subspace	O
.	O
its	O
solution	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
a	O
generalized	B
eigenvector	O
problem	O
.	O
12.1.3	O
applications	O
of	O
pea	O
we	O
can	O
illustrate	O
the	O
use	O
of	O
pca	O
for	O
data	O
compression	O
by	O
considering	O
the	O
off	O
(	O
cid:173	O
)	O
line	O
digits	O
data	O
set	O
.	O
because	O
each	O
eigenvector	O
of	O
the	O
covariance	B
matrix	I
is	O
a	O
vector	O
exercise	O
12.2	O
appendix	O
a	O
566	O
12.	O
col\'tinuolis	O
latf	O
;	O
i\'t	O
\'ariaiiles	O
figure	O
12.3	O
the	O
mean	B
~	O
'	O
''	O
x	O
aklog	O
with	O
!	O
he	O
ii	O
''	O
't	O
lou	O
'	O
pca	O
e	O
;	O
gerrvecl	O
<	O
)	O
rll	O
ul	O
,	O
.	O
cligits	O
data	O
set	O
.	O
t	O
<	O
>	O
getl'ler	O
with	O
!	O
he	O
correspondi~~	O
.	O
..	O
'	O
''	O
lor	O
the	O
011-	O
...	O
..	O
;	O
n	O
the	O
oiigi	O
''	O
,	O
,1	O
d-	O
<	O
limensional	O
space	O
.	O
we	O
can	O
represent	O
tho	O
eigenw	O
:	O
cto	O
<	O
s	O
as	O
imago	O
<	O
of	O
tho	O
same	O
silo	O
as	O
,1	O
>	O
0	O
data	O
poi	O
''	O
,	O
,_	O
11	O
,	O
.	O
first	O
ih'e	O
.ig.n	O
,	O
'occofs	O
.	O
along	O
wich	O
tl	O
>	O
o	O
corre	O
(	O
cid:173	O
)	O
sponding	O
.igen	O
,	O
'slue	O
,	O
.	O
are	O
<	O
iio	O
''	O
'n	O
in	O
figure	O
12,3	O
,	O
a	O
plo	O
!	O
ofll	O
>	O
o	O
complete	O
spect	O
''	O
'm	O
uf	O
oigo	O
''	O
,	O
·alue	O
,	O
.	O
sone	O
<	O
!	O
into	O
decreasing	O
order	O
.	O
is	O
shown	O
in	O
figure	O
12.4	O
{	O
ai	O
.	O
the	O
di'tortion	O
measure	O
j	O
assqciated	O
wilh	O
choo	O
<	O
ing	O
a	O
particular	O
value	O
of	O
m	O
is	O
gi	O
.	O
'en	O
by	O
tho	O
sum	O
of	O
the	O
eig.n	O
''	O
,	O
lues	O
from	O
m	O
+	O
i	O
up	O
to	O
0	O
and	O
is	O
pto	O
!	O
ted	O
for	O
different	O
,	O
'aluo	O
<	O
of	O
.\1	O
in	O
figure	O
12,4	O
(	O
b	O
)	O
.	O
if	O
``	O
'e	O
<	O
utlslitut	O
.	O
(	O
12	O
,	O
12	O
)	O
and	O
(	O
12.13	O
)	O
into	O
(	O
12.10	O
)	O
.	O
we	O
can	O
write	O
the	O
i'ca	O
appro~­	O
imation	O
to	O
a	O
data	O
``	O
eel	O
'	O
''	O
x~	O
i	O
''	O
the	O
fonn	O
'-	O
~	O
l	O
{	O
x~	O
''	O
,	O
)	O
u	O
,	O
+	O
i	O
:	O
(	O
xl'u	O
,	O
)	O
u	O
,	O
._m+l	O
''	O
m	O
-	O
x+l	O
(	O
x~u	O
,	O
-xtu	O
,	O
)	O
u	O
;	O
m	O
.-	O
.	O
•	O
,	O
,	O
,	O
,	O
,	O
10'	O
(	O
12.19	O
)	O
(	O
12.20	O
)	O
''	O
,	O
~	O
,	O
.	O
,	O
~	O
;	O
''	O
-	O
''	O
0	O
''	O
,	O
~	O
,	O
.	O
,	O
~	O
``	O
,	O
to'	O
,	O
,	O
''	O
,	O
''	O
fiiiure	O
12,4	O
(	O
a	O
)	O
piol	O
at	O
!	O
he	O
ejoi	O
;	O
nv.loo	O
.	O
``	O
.	O
,	O
etrum	O
lor	O
the	O
off·1ine	O
digits	O
data	O
set	O
(	O
b	O
)	O
p10t	O
01	O
!	O
he	O
sum	O
at	O
the	O
<	O
:	O
liscarded	O
.	O
``	O
``	O
.ioos	O
,	O
which	O
``	O
'l'fesoots	O
!	O
he	O
s.um-ol·sq	O
''	O
''	O
,	O
es	O
distortlon	O
j	O
i	O
<	O
*~	O
by	O
projecti	O
<	O
xl	O
the	O
data	O
onto	O
a	O
p	O
<	O
incipal	O
componenl	O
slll	O
>	O
spaee	O
'	O
''	O
dimensionalitv	O
m.	O
11	O
.	O
1	O
.	O
i'rindpall	O
.	O
:	O
''	O
l11pon~nt	O
anal~·.i	O
.	O
567	O
fiiiur	O
.	O
1:1	O
:	O
.5	O
an	O
``	O
,	O
>	O
gi	O
''	O
,	O
,1	O
~mpie	O
irom	O
li	O
>	O
e	O
011·_	O
digils	O
data	O
...	O
ttoll	O
''	O
1her	O
with	O
its	O
pea	O
re	O
<	O
:	O
onstnxlions	O
oblair	O
...	O
:	O
!	O
by	O
'e1aio	O
''	O
li	O
!	O
xl	O
,	O
if	O
j	O
)	O
<	O
incipal	O
~n1s	O
10	O
<	O
various	O
val	O
,	O
,	O
''	O
01	O
,	O
if	O
.	O
as	O
,	O
ii	O
increason	O
!	O
tie	O
re	O
<	O
:	O
onst	O
,	O
uctiofi	O
~s	O
more	O
ao	O
:	O
:urate	O
and	O
woukl	O
~	O
portee	O
!	O
when	O
.-if	O
k	O
d	O
~	O
28	O
x	O
28	O
~	O
.	O
''	O
-1	O
.	O
ai	O
''	O
'	O
''	O
'	O
''	O
/	O
;	O
'\	O
'	O
a	O
seer/on	O
9.1	O
where	O
we	O
ha	O
''	O
e	O
made	O
moe	O
of	O
the	O
relation	O
''	O
,	O
-	O
,	O
x	O
=	O
l	O
(	O
x	O
'	O
''	O
,	O
)	O
u	O
;	O
(	O
12.21	O
)	O
which	O
follow	O
.	O
from	O
the	O
completene	O
''	O
of	O
the	O
{	O
u	O
,	O
i	O
,	O
thi	O
.	O
represent	O
.	O
a	O
contpre	O
''	O
ioo	O
''	O
f	O
the	O
data	O
>	O
ct	O
.	O
ilttau	O
>	O
e	O
for	O
each	O
data	O
poim	O
we	O
ha	O
,	O
..	O
repla	O
«	O
d	O
the	O
v·dimensiooal	O
''	O
o	O
<	O
:	O
lor	O
x	O
''	O
wilh	O
an	O
,	O
i	O
[	O
.din	O
>	O
en	O
,	O
ional	O
``	O
o	O
<	O
:	O
tor	O
having	O
componem	O
,	O
(	O
x~	O
'	O
''	O
_	O
x	O
'	O
''	O
,	O
)	O
.	O
11ie	O
'mailer	O
the	O
``	O
alue	O
of	O
m.	O
the	O
greater	O
the	O
degree	O
of	O
comp.-e	O
''	O
,	O
ion	O
.	O
example	O
.	O
of	O
pea	O
,	O
''	O
''	O
o	O
n't	O
''	O
''	O
tioos	O
of	O
data	O
points	O
for	O
the	O
digits	O
data	O
set	O
are	O
shown	O
in	O
figure	O
12.5	O
anolher	O
application	O
of	O
priocipal	O
compcmenl	O
analy	O
,	O
i	O
.	O
i	O
'	O
to	O
data	O
pre-processing	O
.	O
in	O
thi	O
'	O
case	O
,	O
lhe	O
goal	O
is	O
no	O
!	O
dimensionality	O
reduc1ion	O
but	O
rather	O
the	O
tmn	O
,	O
formmion	O
of	O
a	O
data	O
sel	O
in	O
or	O
<	O
k	O
'	O
to	O
standa'lli'.e	O
eenain	O
of	O
ils	O
pmpenies	O
.	O
this	O
can	O
be	O
in'portanl	O
in	O
allowing	O
.ubsequent	O
pallem	O
,	O
''	O
''	O
ognition	O
algorithm	O
.	O
10	O
be	O
applied	O
successfully	O
10	O
the	O
data	O
>	O
ct	O
.	O
typically	O
.	O
il	O
is	O
done	O
wilen	O
the	O
original	O
``	O
ariable	O
.	O
are	O
mea	O
,	O
ured	O
in	O
``	O
arioos	O
dif	O
.	O
ferent	O
unil	O
'	O
or	O
!	O
la	O
''	O
e	O
significantly	O
difterent	O
,	O
'ariabilil	O
}	O
'	O
.	O
for	O
instance	O
in	O
the	O
old	O
faithful	O
data	O
sel	O
.	O
the	O
time	O
betv.-een	O
eruption	O
.	O
i.	O
typicany	O
an	O
order	O
of	O
magni1ude	O
greater	O
than	O
lhe	O
durali	O
''	O
''	O
of.n	O
erupt	O
;	O
,	O
,	O
''	O
.	O
when	O
w'e	O
applied	O
the	O
``	O
.nlcans	O
algorill	O
''	O
''	O
10	O
thi	O
<	O
data	O
set	O
,	O
``	O
.-e	O
first	O
made	O
a	O
separ.te	O
linear	O
re-sealing	O
of	O
the	O
individual	O
``	O
anable	O
'	O
socb	O
thm	O
each	O
``	O
ariable	O
had	O
zero	O
mean	B
and	O
unit	O
``	O
ariance	O
.	O
llus	O
is	O
known	O
as	O
slllnlardiv·.	O
,	O
g	O
the	O
dota	O
.	O
and	O
the	O
co\'anance	O
matrix	O
for	O
lhe	O
'lando	O
,	O
di/	O
,	O
ed	O
dala	O
has	O
components	O
(	O
12,22	O
)	O
where	O
<	O
1	O
,	O
is	O
the	O
,	O
'anaoce	O
of	O
:	O
c	O
,	O
.	O
this	O
i	O
<	O
known	O
as	O
the	O
(	O
``	O
,	O
,	O
,	O
el	O
,	O
,	O
,	O
;	O
,	O
,	O
,	O
,	O
matri	O
.	O
'	O
of	O
the	O
original	O
dota	O
and	O
ha	O
'	O
the	O
propeny	O
thai	O
if	O
t	O
''	O
''	O
o	O
rompooent	O
,	O
x	O
;	O
and	O
x	O
,	O
of	O
the	O
data	O
are	O
perfee1ly	O
correl.ted	O
.	O
then	O
ai	O
_	O
i.•nd	O
if	O
they	O
a.-e	O
uocorrelated	O
.	O
then	O
ai	O
_	O
o	O
.	O
11	O
''	O
,	O
,'1	O
''	O
''	O
'	O
,	O
using	O
pea	O
we	O
can	O
make	O
a	O
it	O
>	O
of'e	O
subst.mial	O
nonnalizat	O
;	O
oo	O
of	O
the	O
data	O
to	O
gi\'c	O
it	O
zero	O
mean	B
and	O
unit	O
co'·ariance	O
.	O
so	O
that	O
different	O
``	O
anables	O
become	O
derorre	O
(	O
cid:173	O
)	O
late	O
<	O
l	O
to	O
do	O
this	O
.	O
we	O
first	O
``	O
''	O
rile	O
the	O
ei8cn	O
''	O
cclor	O
equation	O
(	O
12	O
,	O
17	O
)	O
in	O
the	O
form	O
su=	O
ul	O
(	O
12.23	O
)	O
568	O
12.	O
continuous	O
latent	O
variables	O
100	O
80	O
90	O
00'	O
,	O
=~o	O
70	O
60	O
50	O
40	O
2	O
0	O
-2	O
2	O
0	O
-2	O
0	O
b	O
o	O
000	O
0	O
08	O
0	O
0	O
0	O
tj	O
~	O
cpo	O
0	O
o~	O
~ooid	O
2	O
4	O
6	O
-2	O
0	O
2	O
-2	O
0	O
2	O
figure	O
12.6	O
illustration	O
of	O
the	O
effects	O
of	O
linear	O
pre-processing	O
applied	O
to	O
the	O
old	O
faithful	O
data	O
set	O
.	O
the	O
plot	O
on	O
the	O
left	O
shows	O
the	O
original	O
data	O
.	O
the	O
centre	O
plot	O
shows	O
the	O
result	O
of	O
standardizing	B
the	O
individual	O
variables	O
to	O
zero	O
mean	B
and	O
unit	O
variance	B
.	O
also	O
shown	O
are	O
the	O
principal	O
axes	O
of	O
this	O
normalized	O
data	O
set	O
,	O
plotted	O
over	O
the	O
range	O
±a~/2	O
.	O
the	O
plot	O
on	O
the	O
right	O
shows	O
the	O
result	O
of	O
whitening	B
of	O
the	O
data	O
to	O
give	O
it	O
zero	O
mean	B
and	O
unit	O
covariance	B
.	O
where	O
l	O
is	O
a	O
d	O
x	O
d	O
diagonal	B
matrix	O
with	O
elements	O
ai	O
,	O
and	O
u	O
is	O
a	O
d	O
x	O
d	O
orthog	O
(	O
cid:173	O
)	O
onal	O
matrix	O
with	O
columns	O
given	O
by	O
ui	O
.	O
then	O
we	O
define	O
,	O
for	O
each	O
data	O
point	O
x	O
n	O
,	O
a	O
transformed	O
value	O
given	O
by	O
where	O
x	O
is	O
the	O
sample	B
mean	I
defined	O
by	O
(	O
12.1	O
)	O
.	O
clearly	O
,	O
the	O
set	O
{	O
yn	O
}	O
has	O
zero	O
mean	B
,	O
and	O
its	O
covariance	B
is	O
given	O
by	O
the	O
identity	O
matrix	O
because	O
(	O
12.24	O
)	O
n	O
1~	O
l	O
l	O
-1/2ut	O
(	O
xn	O
-	O
x	O
)	O
(	O
xn	O
-	O
x	O
)	O
tul-1/2	O
l	O
~1/2utsul	O
-1/2	O
=	O
l	O
-1/2ll-1/2	O
=	O
i.	O
n=l	O
(	O
12.25	O
)	O
appendix	O
a	O
appendix	O
a	O
this	O
operation	O
is	O
known	O
as	O
whitening	B
or	O
sphereing	B
the	O
data	O
and	O
is	O
illustrated	O
for	O
the	O
old	O
faithful	O
data	O
set	O
in	O
figure	O
12.6.	O
it	O
is	O
interesting	O
to	O
compare	O
pca	O
with	O
the	O
fisher	O
linear	B
discriminant	I
which	O
was	O
discussed	O
in	O
section	O
4.1.4.	O
both	O
methods	O
can	O
be	O
viewed	O
as	O
techniques	O
for	O
linear	O
dimensionality	O
reduction	O
.	O
however	O
,	O
pca	O
is	O
unsupervised	O
and	O
depends	O
only	O
on	O
the	O
values	O
x	O
n	O
whereas	O
fisher	O
linear	B
discriminant	I
also	O
uses	O
class-label	O
information	O
.	O
this	O
difference	O
is	O
highlighted	O
by	O
the	O
example	O
in	O
figure	O
12.7.	O
another	O
common	O
application	O
of	O
principal	B
component	I
analysis	I
is	O
to	O
data	O
visual	O
(	O
cid:173	O
)	O
ization	O
.	O
here	O
each	O
data	O
point	O
is	O
projected	O
onto	O
a	O
two-dimensional	O
(	O
m	O
=	O
2	O
)	O
principal	B
subspace	I
,	O
so	O
that	O
a	O
data	O
point	O
x	O
n	O
is	O
plotted	O
at	O
cartesian	O
coordinates	O
given	O
by	O
x'j	O
.	O
u1	O
and	O
x'j	O
.	O
u2	O
,	O
where	O
ul	O
and	O
u2	O
are	O
the	O
eigenvectors	O
corresponding	O
to	O
the	O
largest	O
and	O
second	O
largest	O
eigenvalues	O
.	O
an	O
example	O
of	O
such	O
a	O
plot	O
,	O
for	O
the	O
oil	O
flow	O
data	O
set	O
,	O
is	O
shown	O
in	O
figure	O
12.8	O
.	O
12.1.	O
i	O
'	O
<	O
incipal	O
cl	O
)	O
m	O
...	O
''	O
n~nt	O
anal	O
}	O
's	O
;	O
,	O
569	O
''	O
fig	O
''	O
''	O
,	O
12.7	O
a	O
comparison	O
01	O
pro	O
:	O
ipal	O
compo	O
(	O
cid:173	O
)	O
mnt	O
analysis	O
...	O
.111	O
fisha	O
(	O
s	O
linaar	O
discriminant	O
101	O
``	O
''	O
''	O
''	O
,	O
<	O
*man	O
''	O
''	O
'	O
''	O
ality	O
r	O
&	O
duclion	O
.	O
here	O
too	O
data	O
in	O
two	O
dimansions	O
,	O
belonging	O
to	O
two	O
classes	O
siiowi1	O
in	O
red	O
and	O
blue	O
.	O
is	O
to	O
be	O
pfoi	O
''	O
cled	O
onto	O
a	O
s.ingle	O
di·	O
mension	O
.	O
pca	O
c/	O
>	O
xlsas	O
the	O
direc·	O
tion	O
01	O
maximum	O
varia	O
''	O
''	O
e.	O
siiowi1	O
try	O
tha	O
ma9	O
''	O
''	O
ta	O
co	O
''	O
''	O
'	O
.	O
wt11ch	O
leads	O
to	O
strong	O
class	O
overlap	O
.	O
whereas	O
!	O
he	O
fisl	O
>	O
ef	O
iimar	O
discfornillant	O
takes	O
accoun1	O
<	O
:	O
a	O
too	O
class	O
labels	O
and	O
leads	O
to	O
a	O
projection	O
onto	O
the	O
g	O
<	O
ean	O
cum	O
!	O
giving	O
much	O
t	O
>	O
etler	O
class	O
separation	O
.	O
``	O
.	O
,	O
-	O
.	O
.	O
'	O
~.•	O
''	O
''	O
'~	O
'	O
''	O
~	O
.•••._	O
'	O
,	O
..'	O
'	O
:	O
r-	O
'	O
--	O
-~+·_..-	O
:	O
--	O
'~-j	O
..	O
,	O
~	O
~	O
,	O
.	O
,	O
.	O
''	O
.	O
,	O
'	O
:	O
--	O
--	O
;	O
!	O
--	O
-	O
;	O
-	O
''	O
3	O
_.s	O
0	O
fig	O
''	O
''	O
,	O
12.8	O
visualilatlon	O
01	O
!	O
he	O
oill'low	O
<	O
lata	O
liet	O
obtained	O
try	O
projoecting	O
the	O
<	O
lata	O
onto	O
the	O
lirst	O
two	O
prin	O
.	O
cipal	O
compone	O
<	O
1ts	O
.	O
the	O
<	O
ed	O
,	O
blue	O
,	O
and	O
9r	O
&	O
en	O
points	O
corre-spond	O
to	O
!	O
he	O
'iamini	O
(	O
,	O
't	O
>	O
omo-	O
genoous	O
'	O
,	O
and	O
'8nnula~	O
flow	O
oonligurations	O
''	O
,	O
specriveiy	O
.	O
12.1.4	O
pea	O
for	O
high-dimensional	O
data	O
in	O
some	O
application	O
.	O
of	O
plitlcipal	O
component	O
analysis	O
.	O
the	O
number	O
of	O
data	O
points	O
is	O
smaller	O
than	O
t	O
!	O
>	O
c	O
dimensionality	O
of	O
troe	O
data	O
'pace	O
.	O
foi	O
''	O
example	O
.	O
``	O
,	O
e	O
might	O
want	O
to	O
apply	O
pea	O
to	O
a	O
data	O
<	O
el	O
of	O
a	O
few	O
hundred	O
images	O
,	O
each	O
of	O
,	O
,'hich	O
rorrespooos	O
to	O
a	O
''	O
eetor	O
in	O
a	O
'pace	O
of	O
poientially	O
...	O
..ml	O
million	O
dimensiolls	O
(	O
coitesponding	O
tn	O
thfl'e	O
enlour	O
``	O
alues	O
for	O
each	O
of	O
the	O
pi	O
.	O
``	O
,	O
ls	O
in	O
troe	O
image	O
)	O
,	O
noie	O
that	O
in	O
a	O
d-	O
<	O
limen	O
,	O
ional	O
space	O
a	O
set	O
of	O
jy	O
points	O
.	O
``	O
,	O
'here	O
n	O
<	O
d.	O
defines	O
a	O
linear	O
subspa	O
:	O
:e	O
``	O
,	O
hose	O
dimensi	O
''	O
nality	O
is	O
at	O
``	O
''	O
'st	O
n	O
-	O
1	O
,	O
and	O
so	O
there	O
is	O
linle	O
point	O
in	O
applying	O
pea	O
for	O
,	O
'alue	O
<	O
of	O
m	O
thai	O
''	O
'	O
''	O
greater	O
than	O
n	O
-	O
indeed	O
,	O
if	O
``	O
'e	O
pelf	O
''	O
''	O
,	O
,	O
pea	O
we	O
will	O
find	O
that	O
at	O
least	O
d	O
-	O
n	O
+	O
i	O
of	O
the	O
eigen	O
''	O
.lues	O
art	O
lero	O
.	O
eorrespnnding	O
tq	O
eigenvectors	O
aloog	O
``	O
,	O
hose	O
direclioos	O
the	O
data	O
<	O
el	O
has	O
10m	O
varianee	O
.	O
funhem	O
>	O
ore	O
.	O
typical	O
algol	O
''	O
ithm	O
,	O
for	O
finding	O
the	O
eigen	O
,	O
'eet	O
''	O
''	O
of	O
a	O
d	O
x	O
d	O
matrix	O
ha	O
''	O
e	O
a	O
computatiooal	O
eosl	O
thm	O
scales	O
like	O
o	O
(	O
d~j	O
.	O
aoo	O
so	O
for	O
appliealions	O
such	O
as	O
the	O
image	O
e	O
,	O
ample	O
.	O
a	O
direc	O
'	O
application	O
of	O
pea	O
will	O
be	O
computatiooally	O
infe	O
,	O
,-sibje	O
.	O
w.	O
can	O
resoh'e	O
this	O
problem	O
as	O
foil	O
''	O
,	O
''	O
,	O
'	O
''	O
fir	O
;	O
l.	O
let	O
us	O
define	O
x	O
to	O
be	O
the	O
(	O
n	O
``	O
dj·	O
i	O
,	O
570	O
12.	O
continuous	O
latent	O
variables	O
dimensional	O
centred	O
data	O
matrix	O
,	O
whose	O
nth	O
row	O
is	O
given	O
by	O
(	O
x	O
n	O
-	O
x	O
)	O
t.	O
the	O
covari	O
(	O
cid:173	O
)	O
ance	O
matrix	O
(	O
12.3	O
)	O
can	O
then	O
be	O
written	O
as	O
s	O
=	O
n-	O
1xtx	O
,	O
and	O
the	O
corresponding	O
eigenvector	O
equation	O
becomes	O
t	O
1	O
-x	O
xui	O
=	O
aiui	O
n	O
.	O
now	O
pre-multiply	O
both	O
sides	O
by	O
x	O
to	O
give	O
1	O
nxx	O
(	O
xui	O
)	O
=	O
ai	O
(	O
xui	O
)	O
'	O
t	O
if	O
we	O
now	O
define	O
vi	O
=	O
xui	O
,	O
we	O
obtain	O
t	O
1	O
-xx	O
vi	O
=	O
aivi	O
n	O
(	O
12.26	O
)	O
(	O
12.27	O
)	O
(	O
12.28	O
)	O
which	O
is	O
an	O
eigenvector	O
equation	O
for	O
the	O
n	O
x	O
n	O
matrix	O
n-	O
1xxt	O
.	O
we	O
see	O
that	O
this	O
has	O
the	O
same	O
n	O
-1	O
eigenvalues	O
as	O
the	O
original	O
covariance	B
matrix	I
(	O
which	O
itself	O
has	O
an	O
additional	O
d	O
-	O
n	O
+	O
1	O
eigenvalues	O
of	O
value	O
zero	O
)	O
.	O
thus	O
we	O
can	O
solve	O
the	O
eigenvector	O
problem	O
in	O
spaces	O
of	O
lower	O
dimensionality	O
with	O
computational	O
cost	O
o	O
(	O
n3	O
)	O
instead	O
of	O
o	O
(	O
d	O
3	O
)	O
.	O
in	O
order	O
to	O
determine	O
the	O
eigenvectors	O
,	O
we	O
multiply	O
both	O
sides	O
of	O
(	O
12.28	O
)	O
by	O
x	O
t	O
to	O
give	O
1	O
t	O
)	O
nx	O
x	O
(	O
x	O
vi	O
)	O
=	O
ai	O
(	O
x	O
vi	O
)	O
t	O
t	O
(	O
(	O
12.29	O
)	O
from	O
which	O
we	O
see	O
that	O
(	O
xtvi	O
)	O
is	O
an	O
eigenvector	O
of	O
s	O
with	O
eigenvalue	O
ai	O
.	O
note	O
,	O
however	O
,	O
that	O
these	O
eigenvectors	O
need	O
not	O
be	O
normalized	O
.	O
to	O
determine	O
the	O
appropri	O
(	O
cid:173	O
)	O
ate	O
normalization	O
,	O
we	O
re-scale	O
ui	O
ex	O
:	O
x	O
tvi	O
by	O
a	O
constant	O
such	O
that	O
ilui	O
ii	O
=	O
1	O
,	O
which	O
,	O
assuming	O
vi	O
has	O
been	O
normalized	O
to	O
unit	O
length	O
,	O
gives	O
1	O
ui	O
=	O
(	O
nai	O
)	O
1/2	O
x	O
vi·	O
t	O
(	O
12.30	O
)	O
in	O
summary	O
,	O
to	O
apply	O
this	O
approach	O
we	O
first	O
evaluate	O
xxt	O
and	O
then	O
find	O
its	O
eigen	O
(	O
cid:173	O
)	O
vectors	O
and	O
eigenvalues	O
and	O
then	O
compute	O
the	O
eigenvectors	O
in	O
the	O
original	O
data	O
space	O
using	O
(	O
12.30	O
)	O
.	O
12.2.	O
probabilistic	O
pea	O
the	O
formulation	O
of	O
pca	O
discussed	O
in	O
the	O
previous	O
section	O
was	O
based	O
on	O
a	O
linear	O
projection	O
of	O
the	O
data	O
onto	O
a	O
subspace	O
of	O
lower	O
dimensionality	O
than	O
the	O
original	O
data	O
space	O
.	O
we	O
now	O
show	O
that	O
pca	O
can	O
also	O
be	O
expressed	O
as	O
the	O
maximum	B
likelihood	I
solution	O
of	O
a	O
probabilistic	O
latent	O
variable	O
model	O
.	O
this	O
reformulation	O
of	O
pca	O
,	O
known	O
as	O
probabilistic	O
pea	O
,	O
brings	O
several	O
advantages	O
compared	O
with	O
conventional	O
pca	O
:	O
•	O
probabilistic	O
pca	O
represents	O
a	O
constrained	O
form	O
of	O
the	O
gaussian	O
distribution	O
in	O
which	O
the	O
number	O
of	O
free	O
parameters	O
can	O
be	O
restricted	O
while	O
still	O
allowing	O
the	O
model	O
to	O
capture	O
the	O
dominant	O
correlations	O
in	O
a	O
data	O
set	O
.	O
section	O
12.2.2	O
12.2.	O
probabilistic	O
pea	O
571	O
•	O
we	O
can	O
derive	O
an	O
em	O
algorithm	O
for	O
pca	O
that	O
is	O
computationally	O
efficient	O
in	O
situations	O
where	O
only	O
a	O
few	O
leading	O
eigenvectors	O
are	O
required	O
and	O
that	O
avoids	O
having	O
to	O
evaluate	O
the	O
data	O
covariance	O
matrix	O
as	O
an	O
intermediate	O
step	O
.	O
•	O
the	O
combination	O
of	O
a	O
probabilistic	O
model	O
and	O
em	O
allows	O
us	O
to	O
deal	O
with	O
miss	O
(	O
cid:173	O
)	O
ing	O
values	O
in	O
the	O
data	O
set	O
.	O
•	O
mixtures	O
of	O
probabilistic	O
pca	O
models	O
can	O
be	O
formulated	O
in	O
a	O
principled	O
way	O
and	O
trained	O
using	O
the	O
em	O
algorithm	O
.	O
section	O
12.2.3	O
•	O
probabilistic	O
pca	O
forms	O
the	O
basis	O
for	O
a	O
bayesian	O
treatment	O
of	O
pca	O
in	O
which	O
the	O
dimensionality	O
of	O
the	O
principal	B
subspace	I
can	O
be	O
found	O
automatically	O
from	O
the	O
data	O
.	O
•	O
the	O
existence	O
of	O
a	O
likelihood	B
function	I
allows	O
direct	O
comparison	O
with	O
other	O
probabilistic	O
density	O
models	O
.	O
by	O
contrast	O
,	O
conventional	O
pca	O
will	O
assign	O
a	O
low	O
reconstruction	O
cost	O
to	O
data	O
points	O
that	O
are	O
close	O
to	O
the	O
principal	B
subspace	I
even	O
if	O
they	O
lie	O
arbitrarily	O
far	O
from	O
the	O
training	B
data	O
.	O
•	O
probabilistic	O
pca	O
can	O
be	O
used	O
to	O
model	O
class-conditional	O
densities	O
and	O
hence	O
be	O
applied	O
to	O
classification	O
problems	O
.	O
•	O
the	O
probabilistic	O
pca	O
model	O
can	O
be	O
run	O
generatively	O
to	O
provide	O
samples	O
from	O
the	O
distribution	O
.	O
this	O
formulation	O
of	O
pca	O
as	O
a	O
probabilistic	O
model	O
was	O
proposed	O
independently	O
by	O
tipping	O
and	O
bishop	O
(	O
1997	O
,	O
1999b	O
)	O
and	O
by	O
roweis	O
(	O
1998	O
)	O
.	O
as	O
we	O
shall	O
see	O
later	O
,	O
it	O
is	O
closely	O
related	O
to	O
factor	B
analysis	I
(	O
basilevsky	O
,	O
1994	O
)	O
.	O
probabilistic	O
pca	O
is	O
a	O
simple	O
example	O
of	O
the	O
linear-gaussian	O
framework	O
,	O
in	O
which	O
all	O
of	O
the	O
marginal	B
and	O
conditional	B
distributions	O
are	O
gaussian	O
.	O
we	O
can	O
formu	O
(	O
cid:173	O
)	O
late	O
probabilistic	O
pca	O
by	O
first	O
introducing	O
an	O
explicit	O
latent	B
variable	I
z	O
corresponding	O
to	O
the	O
principal-component	O
subspace	O
.	O
next	O
we	O
define	O
a	O
gaussian	O
prior	B
distribution	O
p	O
(	O
z	O
)	O
over	O
the	O
latent	B
variable	I
,	O
together	O
with	O
a	O
gaussian	O
conditional	B
distribution	O
p	O
(	O
xl	O
z	O
)	O
for	O
the	O
observed	B
variable	I
x	O
conditioned	O
on	O
the	O
value	O
of	O
the	O
latent	B
variable	I
.	O
specifi	O
(	O
cid:173	O
)	O
cally	O
,	O
the	O
prior	B
distribution	O
over	O
z	O
is	O
given	O
by	O
a	O
zero-mean	O
unit-covariance	O
gaussian	O
p	O
(	O
z	O
)	O
=	O
n	O
(	O
zio	O
,	O
i	O
)	O
.	O
(	O
12.31	O
)	O
similarly	O
,	O
the	O
conditional	B
distribution	O
of	O
the	O
observed	B
variable	I
x	O
,	O
conditioned	O
on	O
the	O
value	O
of	O
the	O
latent	B
variable	I
z	O
,	O
is	O
again	O
gaussian	O
,	O
of	O
the	O
form	O
p	O
(	O
xlz	O
)	O
=	O
n	O
(	O
xlwz	O
+	O
j-l	O
,	O
a	O
2i	O
)	O
(	O
12.32	O
)	O
in	O
which	O
the	O
mean	B
of	O
x	O
is	O
a	O
general	O
linear	O
function	O
of	O
z	O
governed	O
by	O
the	O
d	O
x	O
m	O
matrix	O
wand	O
the	O
d-dimensional	O
vector	O
j-l.	O
note	O
that	O
this	O
factorizes	O
with	O
respect	O
to	O
the	O
elements	O
of	O
x	O
,	O
in	O
other	O
words	O
this	O
is	O
an	O
example	O
of	O
the	O
naive	O
bayes	O
model	O
.	O
as	O
we	O
shall	O
see	O
shortly	O
,	O
the	O
columns	O
of	O
w	O
span	O
a	O
linear	O
subspace	O
within	O
the	O
data	O
space	O
that	O
corresponds	O
to	O
the	O
principal	B
subspace	I
.	O
the	O
other	O
parameter	O
in	O
this	O
model	O
is	O
the	O
scalar	O
a	O
2	O
governing	O
the	O
variance	B
of	O
the	O
conditional	B
distribution	O
.	O
note	O
that	O
there	O
is	O
no	O
section	O
8.1.4	O
section	O
8.2.2	O
572	O
11.	O
continuous	O
lat	O
!	O
:	O
:nt	O
vanim1li	O
:	O
:s	O
/.-	O
,	O
,	O
,	O
,	O
,	O
,	O
,	O
,	O
,	O
flgu..	O
12.9	O
i\n	O
~i	O
''	O
'tfat	O
''	O
''	O
oilt	O
>	O
e	O
ii	O
''	O
''	O
''	O
fative	O
vi	O
&	O
w	O
oi1t	O
>	O
e	O
p	O
<	O
ot	O
>	O
abi	O
!	O
;	O
st	O
''	O
,	O
pea	O
modeifof	O
''	O
two-dimensiooal	O
<	O
!	O
ala	O
space	O
and	O
a	O
on	O
&	O
-	O
<	O
lirnent.ionallat/l	O
<	O
1t	O
space	O
,	O
an	O
ob	O
&	O
erved	O
<	O
!	O
ala	O
point	O
x	O
is	O
generated	O
by	O
first	O
drawing	O
a	O
value	O
i	O
fof	O
1t	O
>	O
e	O
iat	O
&	O
n1	O
vafiatlle	O
/f	O
(	O
lm	O
~s	O
prior	B
dist	O
,	O
~t	O
''	O
''	O
p	O
(	O
~	O
)	O
and	O
itlen	O
drawing	O
a	O
val	O
''	O
''	O
fof	O
x	O
lrom	O
an	O
iso/fopk	O
:	O
gaussian	O
distr~t	O
''	O
''	O
(	O
iijust	O
,	O
al	O
&	O
(	O
l	O
by	O
the	O
red	O
cir	O
<	O
:	O
ie	O
's	O
)	O
having	O
mean	B
wi	O
+	O
''	O
and	O
coy8r1.once	O
,	O
,'1	O
the	O
l/f	O
&	O
er\	O
ellips.	O
&	O
$	O
show	O
l	O
!	O
le	O
density	B
``	O
''	O
''	O
toors	O
!	O
of	O
the	O
marg	O
''	O
'	O
''	O
1	O
dis1r1bulion	O
pix	O
)	O
.	O
loss	O
of	O
ge	O
''	O
''	O
rajity	O
in	O
assuming	O
a	O
zero	O
mean	B
.	O
unit	O
co\'ariance	O
gau	O
''	O
ian	O
for	O
the	O
latent	O
distributi	O
''	O
n	O
ii	O
{	O
z	O
)	O
because	O
a	O
more	O
gcneral	O
gau	O
''	O
i3n	O
di	O
''	O
ributi	O
''	O
n	O
would	O
gi	O
''	O
e	O
rise	O
to	O
an	O
equivalent	O
probabili	O
''	O
ic	O
n	O
>	O
odel	O
.	O
we	O
can	O
view	O
the	O
probabilistic	O
pea	O
model	O
from	O
a	O
geoerati	O
''	O
e	O
\'iew	O
''	O
''	O
int	O
in	O
``	O
hich	O
a	O
sampled	O
'-alue	O
of	O
the	O
ob	O
''	O
''	O
yed	O
,	O
..riable	O
is	O
obiained	O
by	O
first	O
choo	O
,	O
ing	O
a	O
,	O
..iue	O
for	O
the	O
latent	O
,	O
'ariahle	O
aod	O
then	O
>	O
ampling	O
the	O
oo	O
''	O
,	O
,	O
,	O
'e	O
;	O
j	O
,	O
-ariable	O
cooditioned	O
on	O
this	O
lao	O
tent	O
\'alue	O
,	O
specifically	O
,	O
the	O
v-dimen'ional	O
oo	O
''	O
'	O
''	O
'ed	O
'-ariable	O
x	O
is	O
defined	O
by	O
a	O
lin·	O
ea	O
,	O
tran	O
,	O
formati	O
,	O
,	O
''	O
of	O
the	O
'\/·dimen	O
,	O
i	O
''	O
nal	O
latcnt	O
'-ariable	O
z	O
plu	O
,	O
additi'-e	O
gaussian	O
'noise	O
'	O
,	O
<	O
0	O
that	O
,	O
,=\vz+	O
,	O
,+~	O
(	O
12.33	O
)	O
w	O
!	O
>	O
ere	O
z	O
is	O
an	O
m-di	O
''	O
''	O
'nsional	O
gaussian	O
lalent	O
variable	O
.	O
and	O
..	O
is	O
a	O
v·dimensi	O
''	O
nal	O
,	O
ero-mean	O
gau..ian-distributed	O
noi..	O
,	O
``	O
ariable	O
witb	O
co'-ariance	O
,	O
,21.	O
this	O
generative	O
process	O
is	O
illustrated	O
in	O
figure	O
12.9.	O
noie	O
that	O
this	O
frame	O
''	O
.-orl	O
<	O
is	O
based	O
on	O
a	O
mapping	O
from	O
latent	O
,	O
pace	O
10	O
data	O
space	O
.	O
in	O
contrast	O
10	O
the	O
nl	O
(	O
)	O
l	O
'	O
(	O
:	O
c	O
(	O
``	O
''	O
'cnti	O
,	O
,	O
,	O
,	O
''	O
1	O
``	O
iew	O
``	O
f	O
i'ca	O
dis.cus	O
''	O
'd	O
alx	O
''	O
,	O
e	O
,	O
11ie	O
``	O
,	O
,'e=	O
mapping	O
,	O
from	O
data	O
space	O
to	O
the	O
latent	O
space	O
.	O
,	O
,-ill	O
he	O
oolained	O
,	O
honly	O
using	O
ha	O
ycs·	O
lhwn	O
:	O
m.	O
suf	O
!	O
ll'osc	O
we	O
wish	O
10	O
deten	O
''	O
ine	O
the	O
``	O
alues	O
ofll	O
>	O
o	O
parameters	O
\v	O
.	O
i	O
'	O
and	O
,	O
,	O
'	O
using	O
maximum	O
likelihuo	O
<	O
l	O
,	O
to	O
write	O
``	O
''	O
''	O
''	O
n	O
lhe	O
likeliltood	O
function	O
,	O
we	O
need	O
an	O
``	O
''	O
pression	O
for	O
tl	O
>	O
o	O
marginal	B
distributioo	O
p	O
{	O
``	O
)	O
of	O
tl	O
>	O
o	O
~	O
,	O
,'ed	O
...	O
riahle_	O
this	O
is	O
exprt__sed	O
.	O
fmn'	O
the	O
sum	O
aod	O
p	O
,	O
oduct	O
rules	O
``	O
fprobability	O
,	O
in	O
the	O
form	O
(	O
11,34	O
)	O
e	O
,	O
e	O
,	O
,-ise	O
12,7	O
ll	O
''	O
''	O
aus	O
(	O
:	O
this	O
corresponds	O
to	O
a	O
linear·gau	O
''	O
i	O
,	O
n	O
lt1	O
(	O
llicl	O
thi	O
<	O
marginal	B
di	O
,	O
tribulion	O
is	O
again	O
gaussian	O
.	O
atld	O
is	O
given	O
by	O
,	O
,	O
(	O
,	O
,	O
)	O
_	O
n	O
{	O
xllf	O
,	O
c	O
)	O
(	O
ius	O
)	O
12.2.	O
probabilistic	O
pea	O
573	O
where	O
the	O
d	O
x	O
d	O
covariance	B
matrix	I
c	O
is	O
defined	O
by	O
21.	O
c	O
=	O
wwt	O
+	O
0-	O
(	O
12.36	O
)	O
this	O
result	O
can	O
also	O
be	O
derived	O
more	O
directly	O
by	O
noting	O
that	O
the	O
predictive	B
distribution	I
will	O
be	O
gaussian	O
and	O
then	O
evaluating	O
its	O
mean	B
and	O
covariance	B
using	O
(	O
12.33	O
)	O
.	O
this	O
gives	O
ie	O
[	O
x	O
]	O
cov	O
[	O
x	O
]	O
ie	O
[	O
wz	O
+	O
jl	O
+	O
e	O
]	O
=	O
jl	O
ie	O
[	O
(	O
wz	O
+	O
e	O
)	O
(	O
wz	O
+	O
e	O
)	O
t	O
]	O
ie	O
[	O
wzztwt	O
]	O
+	O
ie	O
[	O
eet	O
]	O
=	O
wwt	O
+	O
0-	O
21	O
(	O
12.37	O
)	O
(	O
12.38	O
)	O
where	O
we	O
have	O
used	O
the	O
fact	O
that	O
z	O
and	O
e	O
are	O
independent	B
random	O
variables	O
and	O
hence	O
are	O
uncorrelated	O
.	O
intuitively	O
,	O
we	O
can	O
think	O
of	O
the	O
distribution	O
p	O
(	O
x	O
)	O
as	O
being	O
defined	O
by	O
taking	O
an	O
isotropic	B
gaussian	O
'spray	O
can	O
'	O
and	O
moving	O
it	O
across	O
the	O
principal	B
subspace	I
spraying	O
2	O
and	O
weighted	O
by	O
the	O
prior	B
distribution	O
.	O
gaussian	O
ink	O
with	O
density	B
determined	O
by	O
0-	O
the	O
accumulated	O
ink	O
density	B
gives	O
rise	O
to	O
a	O
'pancake	O
'	O
shaped	O
distribution	O
represent	O
(	O
cid:173	O
)	O
ing	O
the	O
marginal	B
density	O
p	O
(	O
x	O
)	O
.	O
the	O
predictive	B
distribution	I
p	O
(	O
x	O
)	O
is	O
governed	O
by	O
the	O
parameters	O
jl	O
,	O
w	O
,	O
and	O
0-	O
2	O
•	O
however	O
,	O
there	O
is	O
redundancy	O
in	O
this	O
parameterization	O
corresponding	O
to	O
rotations	O
of	O
the	O
latent	O
space	O
coordinates	O
.	O
to	O
see	O
this	O
,	O
consider	O
a	O
matrix	O
w	O
=	O
wr	O
where	O
r	O
is	O
an	O
orthogonal	O
matrix	O
.	O
using	O
the	O
orthogonality	O
property	O
rrt	O
=	O
i	O
,	O
we	O
see	O
that	O
the	O
quantity	O
wwt	O
that	O
appears	O
in	O
the	O
covariance	B
matrix	I
c	O
takes	O
the	O
form	O
(	O
12.39	O
)	O
and	O
hence	O
is	O
independent	B
of	O
r.	O
thus	O
there	O
is	O
a	O
whole	O
family	O
of	O
matrices	O
w	O
all	O
of	O
which	O
give	O
rise	O
to	O
the	O
same	O
predictive	B
distribution	I
.	O
this	O
invariance	B
can	O
be	O
understood	O
in	O
terms	O
of	O
rotations	O
within	O
the	O
latent	O
space	O
.	O
we	O
shall	O
return	O
to	O
a	O
discussion	O
of	O
the	O
number	O
of	O
independent	B
parameters	O
in	O
this	O
model	O
later	O
.	O
when	O
we	O
evaluate	O
the	O
predictive	B
distribution	I
,	O
we	O
require	O
c-	O
1	O
,	O
which	O
involves	O
the	O
inversion	O
of	O
a	O
d	O
x	O
d	O
matrix	O
.	O
the	O
computation	O
required	O
to	O
do	O
this	O
can	O
be	O
reduced	O
by	O
making	O
use	O
of	O
the	O
matrix	O
inversion	O
identity	O
(	O
c.7	O
)	O
to	O
give	O
c-	O
1	O
=	O
0	O
--	O
11	O
-	O
0	O
--	O
2wm-	O
1w	O
t	O
(	O
12.40	O
)	O
where	O
the	O
m	O
x	O
m	O
matrix	O
m	O
is	O
defined	O
by	O
m	O
=	O
wtw	O
+	O
0-	O
21	O
.	O
(	O
12.41	O
)	O
because	O
we	O
invert	O
m	O
rather	O
than	O
inverting	O
c	O
directly	O
,	O
the	O
cost	O
of	O
evaluating	O
c-	O
1	O
is	O
reduced	O
from	O
o	O
(	O
d3	O
)	O
to	O
o	O
(	O
m3	O
)	O
.	O
as	O
well	O
as	O
the	O
predictive	B
distribution	I
p	O
(	O
x	O
)	O
,	O
we	O
will	O
also	O
require	O
the	O
posterior	O
distributionp	O
(	O
zlx	O
)	O
,	O
which	O
can	O
again	O
be	O
written	O
down	O
directly	O
using	O
the	O
result	O
(	O
2.116	O
)	O
for	O
linear-gaussian	O
models	O
to	O
give	O
note	O
that	O
the	O
posterior	O
mean	O
depends	O
on	O
x	O
,	O
whereas	O
the	O
posterior	O
covariance	O
is	O
in	O
(	O
cid:173	O
)	O
dependent	O
of	O
x	O
.	O
(	O
12.42	O
)	O
exercise	O
12.8	O
574	O
12.	O
continuous	O
latent	O
variables	O
figure	O
12.10	O
the	O
probabilistic	O
pea	O
model	O
for	O
a	O
data	O
set	O
of	O
n	O
obser	O
(	O
cid:173	O
)	O
vations	O
of	O
x	O
can	O
be	O
expressed	O
as	O
a	O
directed	B
graph	O
in	O
which	O
each	O
observation	O
x	O
n	O
is	O
associated	O
with	O
a	O
value	O
zn	O
of	O
the	O
latent	B
variable	I
.	O
..-+	O
--	O
w	O
n	O
12.2.1	O
maximum	B
likelihood	I
pea	O
we	O
next	O
consider	O
the	O
determination	O
of	O
the	O
model	O
parameters	O
using	O
maximum	B
likelihood	I
.	O
given	O
a	O
data	O
set	O
x	O
=	O
{	O
xn	O
}	O
of	O
observed	O
data	O
points	O
,	O
the	O
probabilistic	O
pea	O
model	O
can	O
be	O
expressed	O
as	O
a	O
directed	B
graph	O
,	O
as	O
shown	O
in	O
figure	O
12.10.	O
the	O
corresponding	O
log	O
likelihood	O
function	O
is	O
given	O
,	O
from	O
(	O
12.35	O
)	O
,	O
by	O
inp	O
(	O
xijl	O
,	O
w	O
,	O
o	O
'	O
2	O
n	O
)	O
=	O
l	O
ln	O
p	O
(	O
xn	O
iw	O
,	O
jl	O
,	O
o'2	O
1	O
''	O
''	O
n=l	O
n	O
n	O
)	O
nd	O
--	O
2-	O
ln	O
(	O
2n	O
)	O
-	O
2	O
ln	O
ie	O
[	O
-	O
2	O
l	O
,	O
..	O
(	O
xn	O
-	O
jl	O
)	O
c-	O
(	O
xn	O
-	O
jl	O
)	O
.	O
t	O
1	O
(	O
12.43	O
)	O
n=l	O
setting	O
the	O
derivative	B
of	O
the	O
log	O
likelihood	O
with	O
respect	O
to	O
jl	O
equal	O
to	O
zero	O
gives	O
the	O
expected	O
result	O
jl	O
=	O
x	O
where	O
x	O
is	O
the	O
data	O
mean	O
defined	O
by	O
(	O
12.1	O
)	O
.	O
back-substituting	O
we	O
can	O
then	O
write	O
the	O
log	O
likelihood	O
function	O
in	O
the	O
form	O
inp	O
(	O
xiw	O
,	O
jl	O
,	O
0'2	O
)	O
=	O
-2	O
{	O
d	O
in	O
(	O
2n	O
)	O
+	O
in	O
ie	O
[	O
+	O
tr	O
(	O
c-1s	O
)	O
}	O
n	O
(	O
12.44	O
)	O
where	O
s	O
is	O
the	O
data	O
covariance	O
matrix	O
defined	O
by	O
(	O
12.3	O
)	O
.	O
because	O
the	O
log	O
likelihood	O
is	O
a	O
quadratic	O
function	O
of	O
jl	O
,	O
this	O
solution	O
represents	O
the	O
unique	O
maximum	O
,	O
as	O
can	O
be	O
confirmed	O
by	O
computing	O
second	O
derivatives	O
.	O
maximization	O
with	O
respect	O
to	O
w	O
and	O
0'2	O
is	O
more	O
complex	O
but	O
nonetheless	O
has	O
an	O
exact	O
closed-form	O
solution	O
.	O
it	O
was	O
shown	O
by	O
tipping	O
and	O
bishop	O
(	O
1999b	O
)	O
that	O
all	O
of	O
the	O
stationary	B
points	O
of	O
the	O
log	O
likelihood	O
function	O
can	O
be	O
written	O
as	O
(	O
12.45	O
)	O
where	O
u	O
m	O
is	O
a	O
d	O
x	O
m	O
matrix	O
whose	O
columns	O
are	O
given	O
by	O
any	O
subset	O
(	O
of	O
size	O
m	O
)	O
of	O
the	O
eigenvectors	O
of	O
the	O
data	O
covariance	O
matrix	O
s	O
,	O
the	O
m	O
x	O
m	O
diagonal	B
matrix	O
l	O
m	O
has	O
elements	O
given	O
by	O
the	O
corresponding	O
eigenvalues	O
..\	O
,	O
and	O
r	O
is	O
an	O
arbitrary	O
m	O
x	O
m	O
orthogonal	O
matrix	O
.	O
furthermore	O
,	O
tipping	O
and	O
bishop	O
(	O
1999b	O
)	O
showed	O
that	O
the	O
maximum	O
of	O
the	O
like	O
(	O
cid:173	O
)	O
lihood	O
function	O
is	O
obtained	O
when	O
the	O
m	O
eigenvectors	O
are	O
chosen	O
to	O
be	O
those	O
whose	O
eigenvalues	O
are	O
the	O
m	O
largest	O
(	O
all	O
other	O
solutions	O
being	O
saddle	O
points	O
)	O
.	O
a	O
similar	O
re	O
(	O
cid:173	O
)	O
sult	O
was	O
conjectured	O
independently	O
by	O
roweis	O
(	O
1998	O
)	O
,	O
although	O
no	O
proof	O
was	O
given	O
.	O
12.2.	O
probabilistic	O
pea	O
575	O
again	O
,	O
we	O
shall	O
assume	O
that	O
the	O
eigenvectors	O
have	O
been	O
arranged	O
in	O
order	O
of	O
decreas	O
(	O
cid:173	O
)	O
ing	O
values	O
of	O
the	O
corresponding	O
eigenvalues	O
,	O
so	O
that	O
the	O
m	O
principal	O
eigenvectors	O
are	O
ul	O
,	O
''	O
''	O
um	O
.	O
in	O
this	O
case	O
,	O
the	O
columns	O
of	O
w	O
define	O
the	O
principal	B
subspace	I
of	O
stan	O
(	O
cid:173	O
)	O
dard	O
pca	O
.	O
the	O
corresponding	O
maximum	B
likelihood	I
solution	O
for	O
(	O
j'2	O
is	O
then	O
given	O
by	O
(	O
j'~l	O
=	O
d-m	O
l	O
ai	O
d	O
1	O
i=m+l	O
(	O
12.46	O
)	O
so	O
that	O
(	O
j'~l	O
is	O
the	O
average	O
variance	B
associated	O
with	O
the	O
discarded	O
dimensions	O
.	O
because	O
r	O
is	O
orthogonal	O
,	O
it	O
can	O
be	O
interpreted	O
as	O
a	O
rotation	O
matrix	O
in	O
the	O
m	O
x	O
m	O
latent	O
space	O
.	O
if	O
we	O
substitute	O
the	O
solution	O
for	O
w	O
into	O
the	O
expression	O
for	O
c	O
,	O
and	O
make	O
use	O
of	O
the	O
orthogonality	O
property	O
rrt	O
=	O
i	O
,	O
we	O
see	O
that	O
c	O
is	O
independent	B
of	O
r.	O
this	O
simply	O
says	O
that	O
the	O
predictive	O
density	O
is	O
unchanged	O
by	O
rotations	O
in	O
the	O
latent	O
space	O
as	O
discussed	O
earlier	O
.	O
for	O
the	O
particular	O
case	O
of	O
r	O
=	O
i	O
,	O
we	O
see	O
that	O
the	O
columns	O
of	O
w	O
are	O
the	O
principal	O
component	O
eigenvectors	O
scaled	O
by	O
the	O
variance	B
parameters	O
ai	O
-	O
(	O
j'2	O
.	O
the	O
interpretation	O
of	O
these	O
scaling	O
factors	O
is	O
clear	O
once	O
we	O
recognize	O
that	O
for	O
a	O
convolution	O
of	O
independent	B
gaussian	O
distributions	O
(	O
in	O
this	O
case	O
the	O
latent	O
space	O
distribution	O
and	O
the	O
noise	O
model	O
)	O
the	O
variances	O
are	O
additive	O
.	O
thus	O
the	O
variance	B
ai	O
in	O
the	O
direction	O
of	O
an	O
eigenvector	O
ui	O
is	O
composed	O
of	O
the	O
sum	O
of	O
a	O
contribution	O
ai	O
(	O
cid:173	O
)	O
(	O
j'2	O
from	O
the	O
projection	O
of	O
the	O
unit-variance	O
latent	O
space	O
distribution	O
into	O
data	O
space	O
through	O
the	O
corresponding	O
column	O
of	O
w	O
,	O
plus	O
an	O
isotropic	B
contribution	O
of	O
variance	B
(	O
j'2	O
which	O
is	O
added	O
in	O
all	O
directions	O
by	O
the	O
noise	O
model	O
.	O
it	O
is	O
worth	O
taking	O
a	O
moment	O
to	O
study	O
the	O
form	O
of	O
the	O
covariance	B
matrix	I
given	O
by	O
(	O
12.36	O
)	O
.	O
consider	O
the	O
variance	B
of	O
the	O
predictive	B
distribution	I
along	O
some	O
direction	O
specified	O
by	O
the	O
unit	O
vector	O
v	O
,	O
where	O
vtv	O
=	O
1	O
,	O
which	O
is	O
given	O
by	O
vtcv	O
.	O
first	O
suppose	O
that	O
v	O
is	O
orthogonal	O
to	O
the	O
principal	B
subspace	I
,	O
in	O
other	O
words	O
it	O
is	O
given	O
by	O
some	O
linear	O
combination	O
of	O
the	O
discarded	O
eigenvectors	O
.	O
then	O
v	O
tv	O
=	O
0	O
and	O
hence	O
v	O
tcv	O
=	O
(	O
j'2	O
.	O
thus	O
the	O
model	O
predicts	O
a	O
noise	O
variance	B
orthogonal	O
to	O
the	O
principal	B
subspace	I
,	O
which	O
,	O
from	O
(	O
12.46	O
)	O
,	O
is	O
just	O
the	O
average	O
of	O
the	O
discarded	O
eigenvalues	O
.	O
now	O
suppose	O
that	O
v	O
=	O
ui	O
where	O
ui	O
is	O
one	O
of	O
the	O
retained	O
eigenvectors	O
defining	O
the	O
prin	O
(	O
cid:173	O
)	O
cipal	O
subspace	O
.	O
then	O
vtcv	O
=	O
(	O
ai	O
-	O
(	O
j'2	O
)	O
+	O
(	O
j'2	O
=	O
ai	O
.	O
in	O
other	O
words	O
,	O
this	O
model	O
correctly	O
captures	O
the	O
variance	B
of	O
the	O
data	O
along	O
the	O
principal	O
axes	O
,	O
and	O
approximates	O
the	O
variance	B
in	O
all	O
remaining	O
directions	O
with	O
a	O
single	O
average	O
value	O
(	O
j'2	O
.	O
one	O
way	O
to	O
construct	O
the	O
maximum	B
likelihood	I
density	O
model	O
would	O
simply	O
be	O
to	O
find	O
the	O
eigenvectors	O
and	O
eigenvalues	O
of	O
the	O
data	O
covariance	O
matrix	O
and	O
then	O
to	O
evaluate	O
wand	O
(	O
j'2	O
using	O
the	O
results	O
given	O
above	O
.	O
in	O
this	O
case	O
,	O
we	O
would	O
choose	O
r	O
=	O
i	O
for	O
convenience	O
.	O
however	O
,	O
if	O
the	O
maximum	B
likelihood	I
solution	O
is	O
found	O
by	O
numerical	O
optimization	O
of	O
the	O
likelihood	B
function	I
,	O
for	O
instance	O
using	O
an	O
algorithm	O
such	O
as	O
conjugate	B
gradients	O
(	O
fletcher	O
,	O
1987	O
;	O
nocedal	O
and	O
wright	O
,	O
1999	O
;	O
bishop	O
and	O
nabney	O
,	O
2008	O
)	O
or	O
through	O
the	O
em	O
algorithm	O
,	O
then	O
the	O
resulting	O
value	O
of	O
r	O
is	O
es	O
(	O
cid:173	O
)	O
sentially	O
arbitrary	O
.	O
this	O
implies	O
that	O
the	O
columns	O
of	O
w	O
need	O
not	O
be	O
orthogonal	O
.	O
if	O
an	O
orthogonal	O
basis	O
is	O
required	O
,	O
the	O
matrix	O
w	O
can	O
be	O
post-processed	O
appropriately	O
(	O
golub	O
and	O
van	O
loan	O
,	O
1996	O
)	O
.	O
alternatively	O
,	O
the	O
em	O
algorithm	O
can	O
be	O
modified	O
in	O
such	O
a	O
way	O
as	O
to	O
yield	O
orthonormal	O
principal	O
directions	O
,	O
sorted	O
in	O
descending	O
order	O
of	O
the	O
corresponding	O
eigenvalues	O
,	O
directly	O
(	O
ahn	O
and	O
oh	O
,	O
2003	O
)	O
.	O
section	O
12.2.2	O
576	O
12.	O
continuous	O
latent	O
variables	O
the	O
rotational	O
invariance	B
in	O
latent	O
space	O
represents	O
a	O
form	O
of	O
statistical	O
noniden	O
(	O
cid:173	O
)	O
tifiability	O
,	O
analogous	O
to	O
that	O
encountered	O
for	O
mixture	O
models	O
in	O
the	O
case	O
of	O
discrete	O
latent	O
variables	O
.	O
here	O
there	O
is	O
a	O
continuum	O
of	O
parameters	O
all	O
of	O
which	O
lead	O
to	O
the	O
same	O
predictive	O
density	O
,	O
in	O
contrast	O
to	O
the	O
discrete	O
nonidentifiability	O
associated	O
with	O
component	O
re-labelling	O
in	O
the	O
mixture	B
setting	O
.	O
if	O
we	O
consider	O
the	O
case	O
of	O
m	O
=	O
d	O
,	O
so	O
that	O
there	O
is	O
no	O
reduction	O
of	O
dimension	O
(	O
cid:173	O
)	O
ality	O
,	O
then	O
u	O
m	O
=	O
u	O
and	O
l	O
m	O
=	O
l.	O
making	O
use	O
of	O
the	O
orthogonality	O
properties	O
uut	O
=	O
i	O
and	O
rrt	O
=	O
i	O
,	O
we	O
see	O
that	O
the	O
covariance	B
c	O
of	O
the	O
marginal	B
distribution	O
for	O
x	O
becomes	O
(	O
12.47	O
)	O
and	O
so	O
we	O
obtain	O
the	O
standard	O
maximum	O
likelihood	O
solution	O
for	O
an	O
unconstrained	O
gaussian	O
distribution	O
in	O
which	O
the	O
covariance	B
matrix	I
is	O
given	O
by	O
the	O
sample	O
covari	O
(	O
cid:173	O
)	O
ance	O
.	O
conventional	O
pca	O
is	O
generally	O
formulated	O
as	O
a	O
projection	O
of	O
points	O
from	O
the	O
d	O
(	O
cid:173	O
)	O
dimensional	O
data	O
space	O
onto	O
an	O
m	O
-dimensional	O
linear	O
subspace	O
.	O
probabilistic	O
pca	O
,	O
however	O
,	O
is	O
most	O
naturally	O
expressed	O
as	O
a	O
mapping	O
from	O
the	O
latent	O
space	O
into	O
the	O
data	O
space	O
via	O
(	O
12.33	O
)	O
.	O
for	O
applications	O
such	O
as	O
visualization	B
and	O
data	B
compression	I
,	O
we	O
can	O
reverse	O
this	O
mapping	O
using	O
bayes	O
'	O
theorem	O
.	O
any	O
point	O
x	O
in	O
data	O
space	O
can	O
then	O
be	O
summarized	O
by	O
its	O
posterior	O
mean	O
and	O
covariance	B
in	O
latent	O
space	O
.	O
from	O
(	O
12.42	O
)	O
the	O
mean	B
is	O
given	O
by	O
where	O
m	O
is	O
given	O
by	O
(	O
12.41	O
)	O
.	O
this	O
projects	O
to	O
a	O
point	O
in	O
data	O
space	O
given	O
by	O
wle	O
[	O
zlx	O
]	O
+	O
j-l.	O
(	O
12.48	O
)	O
(	O
12.49	O
)	O
section	O
3.3.1	O
note	O
that	O
this	O
takes	O
the	O
same	O
form	O
as	O
the	O
equations	O
for	O
regularized	O
linear	B
regression	I
and	O
is	O
a	O
consequence	O
of	O
maximizing	O
the	O
likelihood	B
function	I
for	O
a	O
linear	O
gaussian	O
model	O
.	O
similarly	O
,	O
the	O
posterior	O
covariance	O
is	O
given	O
from	O
(	O
12.42	O
)	O
by	O
0-2m-	O
1	O
and	O
is	O
independent	B
of	O
x.	O
if	O
we	O
take	O
the	O
limit	O
0-	O
2	O
--	O
--	O
t	O
0	O
,	O
then	O
the	O
posterior	O
mean	O
reduces	O
to	O
(	O
12.50	O
)	O
exercise	O
12.11	O
exercise	O
12.12	O
section	O
2.3	O
which	O
represents	O
an	O
orthogonal	O
projection	O
of	O
the	O
data	O
point	O
onto	O
the	O
latent	O
space	O
,	O
and	O
so	O
we	O
recover	O
the	O
standard	O
pca	O
model	O
.	O
the	O
posterior	O
covariance	O
in	O
this	O
limit	O
is	O
2	O
>	O
0	O
,	O
the	O
latent	O
projection	O
zero	O
,	O
however	O
,	O
and	O
the	O
density	B
becomes	O
singular	O
.	O
for	O
0-	O
is	O
shifted	O
towards	O
the	O
origin	O
,	O
relative	B
to	O
the	O
orthogonal	O
projection	O
.	O
finally	O
,	O
we	O
note	O
that	O
an	O
important	O
role	O
for	O
the	O
probabilistic	O
pca	O
model	O
is	O
in	O
defining	O
a	O
multivariate	O
gaussian	O
distribution	O
in	O
which	O
the	O
number	O
of	O
degrees	O
of	O
free	O
(	O
cid:173	O
)	O
dom	O
,	O
in	O
other	O
words	O
the	O
number	O
of	O
independent	B
parameters	O
,	O
can	O
be	O
controlled	O
whilst	O
still	O
allowing	O
the	O
model	O
to	O
capture	O
the	O
dominant	O
correlations	O
in	O
the	O
data	O
.	O
recall	O
that	O
a	O
general	O
gaussian	O
distribution	O
has	O
d	O
(	O
d	O
+	O
1	O
)	O
/2	O
independent	B
parameters	O
in	O
its	O
covariance	B
matrix	I
(	O
plus	O
another	O
d	O
parameters	O
in	O
its	O
mean	B
)	O
.	O
thus	O
the	O
number	O
of	O
parameters	O
scales	O
quadratically	O
with	O
d	O
and	O
can	O
become	O
excessive	O
in	O
spaces	O
of	O
high	O
12.2.	O
probabilistic	O
pea	O
577	O
dimensionality	O
.	O
if	O
we	O
restrict	O
the	O
covariance	B
matrix	I
to	O
be	O
diagonal	B
,	O
then	O
it	O
has	O
only	O
d	O
independent	B
parameters	O
,	O
and	O
so	O
the	O
number	O
of	O
parameters	O
now	O
grows	O
linearly	O
with	O
dimensionality	O
.	O
however	O
,	O
it	O
now	O
treats	O
the	O
variables	O
as	O
if	O
they	O
were	O
independent	B
and	O
hence	O
can	O
no	O
longer	O
express	O
any	O
correlations	O
between	O
them	O
.	O
probabilistic	O
pea	O
pro	O
(	O
cid:173	O
)	O
vides	O
an	O
elegant	O
compromise	O
in	O
which	O
the	O
m	O
most	O
significant	O
correlations	O
can	O
be	O
captured	O
while	O
still	O
ensuring	O
that	O
the	O
total	O
number	O
of	O
parameters	O
grows	O
only	O
linearly	O
with	O
d.	O
we	O
can	O
see	O
this	O
by	O
evaluating	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
in	O
the	O
ppca	O
model	O
as	O
follows	O
.	O
the	O
covariance	B
matrix	I
c	O
depends	O
on	O
the	O
parameters	O
w	O
,	O
,	O
giving	O
a	O
total	O
parameter	O
count	O
of	O
dm	O
+	O
1.	O
however	O
,	O
which	O
has	O
size	O
d	O
x	O
m	O
,	O
and	O
a	O
2	O
we	O
have	O
seen	O
that	O
there	O
is	O
some	O
redundancy	O
in	O
this	O
parameterization	O
associated	O
with	O
rotations	O
of	O
the	O
coordinate	O
system	O
in	O
the	O
latent	O
space	O
.	O
the	O
orthogonal	O
matrix	O
r	O
that	O
expresses	O
these	O
rotations	O
has	O
size	O
m	O
x	O
m.	O
in	O
the	O
first	O
column	O
of	O
this	O
matrix	O
there	O
are	O
m	O
-	O
1	O
independent	B
parameters	O
,	O
because	O
the	O
column	O
vector	O
must	O
be	O
normalized	O
to	O
unit	O
length	O
.	O
in	O
the	O
second	O
column	O
there	O
are	O
m	O
-	O
2	O
independent	B
parameters	O
,	O
because	O
the	O
column	O
must	O
be	O
normalized	O
and	O
also	O
must	O
be	O
orthogonal	O
to	O
the	O
previous	O
column	O
,	O
and	O
so	O
on	O
.	O
summing	O
this	O
arithmetic	O
series	O
,	O
we	O
see	O
that	O
r	O
has	O
a	O
total	O
of	O
m	O
(	O
m	O
-1	O
)	O
/2	O
independent	B
parameters	O
.	O
thus	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
in	O
the	O
covariance	B
matrix	I
c	O
is	O
given	O
by	O
dm	O
+	O
1	O
-	O
m	O
(	O
m	O
-	O
1	O
)	O
/2	O
.	O
(	O
12.51	O
)	O
exercise	O
12.14	O
section	O
12.2.4	O
section	O
9.4	O
the	O
number	O
of	O
independent	B
parameters	O
in	O
this	O
model	O
therefore	O
only	O
grows	O
linearly	O
with	O
d	O
,	O
for	O
fixed	O
m.	O
if	O
we	O
take	O
m	O
=	O
d	O
-	O
1	O
,	O
then	O
we	O
recover	O
the	O
standard	O
result	O
for	O
a	O
full	O
covariance	B
gaussian	O
.	O
in	O
this	O
case	O
,	O
the	O
variance	B
along	O
d	O
-	O
1	O
linearly	O
in	O
(	O
cid:173	O
)	O
dependent	O
directions	O
is	O
controlled	O
by	O
the	O
columns	O
of	O
w	O
,	O
and	O
the	O
variance	B
along	O
the	O
remaining	O
direction	O
is	O
given	O
by	O
a	O
2	O
.	O
if	O
m	O
=	O
0	O
,	O
the	O
model	O
is	O
equivalent	O
to	O
the	O
isotropic	B
covariance	O
case	O
.	O
12.2.2	O
em	O
algorithm	O
for	O
pea	O
as	O
we	O
have	O
seen	O
,	O
the	O
probabilistic	O
pca	O
model	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
a	O
marginalization	O
over	O
a	O
continuous	O
latent	O
space	O
z	O
in	O
which	O
for	O
each	O
data	O
point	O
x	O
n	O
,	O
there	O
is	O
a	O
corresponding	O
latent	B
variable	I
zn	O
.	O
we	O
can	O
therefore	O
make	O
use	O
of	O
the	O
em	O
algorithm	O
to	O
find	O
maximum	B
likelihood	I
estimates	O
of	O
the	O
model	O
parameters	O
.	O
this	O
may	O
seem	O
rather	O
pointless	O
because	O
we	O
have	O
already	O
obtained	O
an	O
exact	O
closed-form	O
so	O
(	O
cid:173	O
)	O
lution	O
for	O
the	O
maximum	B
likelihood	I
parameter	O
values	O
.	O
however	O
,	O
in	O
spaces	O
of	O
high	O
dimensionality	O
,	O
there	O
may	O
be	O
computational	O
advantages	O
in	O
using	O
an	O
iterative	O
em	O
procedure	O
rather	O
than	O
working	O
directly	O
with	O
the	O
sample	O
covariance	O
matrix	O
.	O
this	O
em	O
procedure	O
can	O
also	O
be	O
extended	B
to	O
the	O
factor	B
analysis	I
model	O
,	O
for	O
which	O
there	O
is	O
no	O
closed-form	O
solution	O
.	O
finally	O
,	O
it	O
allows	O
missing	B
data	I
to	O
be	O
handled	O
in	O
a	O
principled	O
way	O
.	O
we	O
can	O
derive	O
the	O
em	O
algorithm	O
for	O
probabilistic	O
pca	O
by	O
following	O
the	O
general	O
framework	O
for	O
em	O
.	O
thus	O
we	O
write	O
down	O
the	O
complete-data	O
log	O
likelihood	O
and	O
take	O
its	O
expectation	B
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
of	O
the	O
latent	O
distribution	O
evaluated	O
using	O
'old	O
'	O
parameter	O
values	O
.	O
maximization	O
of	O
this	O
expected	O
complete	O
(	O
cid:173	O
)	O
data	O
log	O
likelihood	O
then	O
yields	O
the	O
'new	O
'	O
parameter	O
values	O
.	O
because	O
the	O
data	O
points	O
578	O
12.	O
continuous	O
latent	O
variables	O
are	O
assumed	O
independent	O
,	O
the	O
complete-data	O
log	O
likelihood	O
function	O
takes	O
the	O
form	O
inp	O
(	O
x	O
,	O
zijl	O
,	O
w	O
,	O
(	O
j2	O
)	O
=	O
l	O
{	O
lnp	O
(	O
xnlzn	O
)	O
+	O
lnp	O
(	O
zn	O
)	O
}	O
n	O
(	O
12.52	O
)	O
n=l	O
where	O
the	O
nth	O
row	O
of	O
the	O
matrix	O
z	O
is	O
given	O
by	O
zn	O
.	O
we	O
already	O
know	O
that	O
the	O
exact	O
maximum	O
likelihood	O
solution	O
for	O
jl	O
is	O
given	O
by	O
the	O
sample	B
mean	I
x	O
defined	O
by	O
(	O
12.1	O
)	O
,	O
and	O
it	O
is	O
convenient	O
to	O
substitute	O
for	O
jl	O
at	O
this	O
stage	O
.	O
making	O
use	O
of	O
the	O
expressions	O
(	O
12.31	O
)	O
and	O
(	O
12.32	O
)	O
for	O
the	O
latent	O
and	O
conditional	B
distributions	O
,	O
respectively	O
,	O
and	O
tak	O
(	O
cid:173	O
)	O
ing	O
the	O
expectation	B
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
over	O
the	O
latent	O
variables	O
,	O
we	O
obtain	O
note	O
that	O
this	O
depends	O
on	O
the	O
posterior	O
distribution	O
only	O
through	O
the	O
sufficient	O
statis	O
(	O
cid:173	O
)	O
tics	O
of	O
the	O
gaussian	O
.	O
thus	O
in	O
the	O
e	O
step	O
,	O
we	O
use	O
the	O
old	O
parameter	O
values	O
to	O
evaluate	O
m-1wt	O
(	O
xn	O
-	O
x	O
)	O
(	O
j2m-	O
1	O
+	O
le	O
[	O
zn	O
]	O
le	O
[	O
zn	O
]	O
t	O
(	O
12.54	O
)	O
(	O
12.55	O
)	O
which	O
follow	O
directly	O
from	O
the	O
posterior	O
distribution	O
(	O
12.42	O
)	O
together	O
with	O
the	O
stan	O
(	O
cid:173	O
)	O
dard	O
result	O
le	O
[	O
znz~	O
]	O
=	O
cov	O
[	O
zn	O
]	O
+	O
je	O
[	O
zn	O
]	O
je	O
[	O
zn	O
]	O
t.	O
here	O
m	O
is	O
defined	O
by	O
(	O
12.41	O
)	O
.	O
in	O
the	O
m	O
step	O
,	O
we	O
maximize	O
with	O
respect	O
to	O
wand	O
(	O
j2	O
,	O
keeping	O
the	O
posterior	O
statistics	O
fixed	O
.	O
maximization	O
with	O
respect	O
to	O
(	O
t2	O
is	O
straightforward	O
.	O
for	O
the	O
maxi	O
(	O
cid:173	O
)	O
mization	O
with	O
respect	O
to	O
w	O
we	O
make	O
use	O
of	O
(	O
c.24	O
)	O
,	O
and	O
obtain	O
the	O
m-step	O
equations	O
exercise	O
12.15	O
w	O
new	O
2	O
(	O
jnew	O
=	O
[	O
t	O
,	O
exn	O
-x	O
)	O
ilizn	O
]	O
t	O
]	O
[	O
t	O
,	O
il	O
[	O
znz~	O
]	O
]	O
-'	O
nd	O
l	O
{	O
llxn	O
-	O
xl1	O
2	O
+tr	O
(	O
je	O
[	O
znzj	O
]	O
w~eww	O
new	O
)	O
}	O
.	O
n=l	O
-	O
2le	O
[	O
zn	O
]	O
tw~ew	O
(	O
xn	O
-	O
x	O
)	O
1	O
n	O
(	O
12.56	O
)	O
(	O
12.57	O
)	O
the	O
em	O
algorithm	O
for	O
probabilistic	O
pca	O
proceeds	O
by	O
initializing	O
the	O
parameters	O
and	O
then	O
alternately	O
computing	O
the	O
sufficient	O
statistics	O
of	O
the	O
latent	O
space	O
posterior	O
distribution	O
using	O
(	O
12.54	O
)	O
and	O
(	O
12.55	O
)	O
in	O
the	O
e	O
step	O
and	O
revising	O
the	O
parameter	O
values	O
using	O
(	O
12.56	O
)	O
and	O
(	O
12.57	O
)	O
in	O
the	O
m	O
step	O
.	O
one	O
of	O
the	O
benefits	O
of	O
the	O
em	O
algorithm	O
for	O
pca	O
is	O
computational	O
efficiency	O
for	O
large-scale	O
applications	O
(	O
roweis	O
,	O
1998	O
)	O
.	O
unlike	O
conventional	O
pca	O
based	O
on	O
an	O
12.2.	O
probabilistic	O
pea	O
579	O
eigenvector	O
decomposition	O
of	O
the	O
sample	O
covariance	O
matrix	O
,	O
the	O
em	O
approach	O
is	O
iterative	O
and	O
so	O
might	O
appear	O
to	O
be	O
less	O
attractive	O
.	O
however	O
,	O
each	O
cycle	O
of	O
the	O
em	O
algorithm	O
can	O
be	O
computationally	O
much	O
more	O
efficient	O
than	O
conventional	O
pca	O
in	O
spaces	O
of	O
high	O
dimensionality	O
.	O
to	O
see	O
this	O
,	O
we	O
note	O
that	O
the	O
eigendecomposition	O
of	O
the	O
covariance	B
matrix	I
requires	O
o	O
(	O
d	O
3	O
)	O
computation	O
.	O
often	O
we	O
are	O
interested	O
only	O
in	O
the	O
first	O
m	O
eigenvectors	O
and	O
their	O
corresponding	O
eigenvalues	O
,	O
in	O
which	O
case	O
we	O
can	O
use	O
algorithms	O
that	O
are	O
0	O
(	O
md	O
2	O
)	O
.	O
however	O
,	O
the	O
evaluation	O
of	O
the	O
covariance	B
matrix	I
itself	O
takes	O
0	O
(	O
nd	O
2	O
)	O
computations	O
,	O
where	O
n	O
is	O
the	O
number	O
of	O
data	O
points	O
.	O
algorithms	O
such	O
as	O
the	O
snapshot	O
method	O
(	O
sirovich	O
,	O
1987	O
)	O
,	O
which	O
assume	O
that	O
the	O
eigenvectors	O
are	O
linear	O
combinations	O
of	O
the	O
data	O
vectors	O
,	O
avoid	O
direct	O
evaluation	O
of	O
the	O
covariance	B
matrix	I
but	O
are	O
o	O
(	O
n3	O
)	O
and	O
hence	O
unsuited	O
to	O
large	O
data	O
sets	O
.	O
the	O
em	O
algorithm	O
described	O
here	O
also	O
does	O
not	O
construct	O
the	O
covariance	B
matrix	I
explicitly	O
.	O
instead	O
,	O
the	O
most	O
computationally	O
demanding	O
steps	O
are	O
those	O
involving	O
sums	O
over	O
the	O
data	O
set	O
that	O
are	O
0	O
(	O
n	O
d	O
m	O
)	O
.	O
for	O
large	O
d	O
,	O
and	O
m	O
«	O
d	O
,	O
this	O
can	O
be	O
a	O
significant	O
saving	O
compared	O
to	O
0	O
(	O
nd	O
2	O
)	O
and	O
can	O
offset	O
the	O
iterative	O
nature	O
of	O
the	O
em	O
algorithm	O
.	O
note	O
that	O
this	O
em	O
algorithm	O
can	O
be	O
implemented	O
in	O
an	O
on-line	O
form	O
in	O
which	O
each	O
d-dimensional	O
data	O
point	O
is	O
read	O
in	O
and	O
processed	O
and	O
then	O
discarded	O
before	O
the	O
next	O
data	O
point	O
is	O
considered	O
.	O
to	O
see	O
this	O
,	O
note	O
that	O
the	O
quantities	O
evaluated	O
in	O
the	O
e	O
step	O
(	O
an	O
m-dimensional	O
vector	O
and	O
an	O
m	O
x	O
m	O
matrix	O
)	O
can	O
be	O
computed	O
for	O
each	O
data	O
point	O
separately	O
,	O
and	O
in	O
the	O
m	O
step	O
we	O
need	O
to	O
accumulate	O
sums	O
over	O
data	O
points	O
,	O
which	O
we	O
can	O
do	O
incrementally	O
.	O
this	O
approach	O
can	O
be	O
advantageous	O
if	O
both	O
nand	O
d	O
are	O
large	O
.	O
because	O
we	O
now	O
have	O
a	O
fully	O
probabilistic	O
model	O
for	O
pca	O
,	O
we	O
can	O
deal	O
with	O
missing	B
data	I
,	O
provided	O
that	O
it	O
is	O
missing	B
at	I
random	I
,	O
by	O
marginalizing	O
over	O
the	O
dis	O
(	O
cid:173	O
)	O
tribution	O
of	O
the	O
unobserved	O
variables	O
.	O
again	O
these	O
missing	O
values	O
can	O
be	O
treated	O
using	O
the	O
em	O
algorithm	O
.	O
we	O
give	O
an	O
example	O
of	O
the	O
use	O
of	O
this	O
approach	O
for	O
data	O
visualization	B
in	O
figure	O
12.11.	O
another	O
elegant	O
feature	O
ofthe	O
em	O
approach	O
is	O
that	O
we	O
can	O
take	O
the	O
limit	O
a	O
2	O
--	O
--	O
t	O
0	O
,	O
corresponding	O
to	O
standard	O
pca	O
,	O
and	O
still	O
obtain	O
a	O
valid	O
em-like	O
algorithm	O
(	O
roweis	O
,	O
1998	O
)	O
.	O
from	O
(	O
12.55	O
)	O
,	O
we	O
see	O
that	O
the	O
only	O
quantity	O
we	O
need	O
to	O
compute	O
in	O
the	O
estep	O
is	O
je	O
[	O
zn	O
]	O
.	O
furthermore	O
,	O
the	O
m	O
step	O
is	O
simplifie~	O
because	O
m	O
=	O
wtw	O
.	O
to	O
emphasize	O
the	O
simplicity	O
of	O
the	O
algorithm	O
,	O
let	O
us	O
define	O
x	O
to	O
be	O
a	O
matrix	O
of	O
size	O
n	O
x	O
d	O
whose	O
nth	O
row	O
is	O
given	O
by	O
the	O
vector	O
x	O
n	O
-	O
x	O
and	O
similarly	O
define	O
0	O
to	O
be	O
a	O
matrix	O
of	O
size	O
d	O
x	O
m	O
whose	O
nth	O
row	O
is	O
given	O
by	O
the	O
vector	O
je	O
[	O
zn	O
]	O
.	O
the	O
estep	O
(	O
12.54	O
)	O
of	O
the	O
em	O
algorithm	O
for	O
pca	O
then	O
becomes	O
o	O
=	O
(	O
w~d	O
wold	O
)	O
-lw~dx	O
and	O
the	O
m	O
step	O
(	O
12.56	O
)	O
takes	O
the	O
form	O
w	O
new	O
=	O
xtot	O
(	O
oot	O
)	O
-l.	O
(	O
12.58	O
)	O
(	O
12.59	O
)	O
again	O
these	O
can	O
be	O
implemented	O
in	O
an	O
on-line	O
form	O
.	O
these	O
equations	O
have	O
a	O
simple	O
interpretation	O
as	O
follows	O
.	O
from	O
our	O
earlier	O
discussion	O
,	O
we	O
see	O
that	O
the	O
e	O
step	O
involves	O
an	O
orthogonal	O
projection	O
of	O
the	O
data	O
points	O
onto	O
the	O
current	O
estimate	O
for	O
the	O
principal	B
subspace	I
.	O
correspondingly	O
,	O
the	O
m	O
step	O
represents	O
a	O
re-estimation	O
of	O
the	O
principal	O
580	O
12.	O
contlnljoljs	O
i	O
''	O
ht	O
;	O
i'it	O
vi\	O
riarles	O
fig	O
''	O
..	O
12.11	O
probabilistic	O
pca	O
visoo	O
,	O
zsbon	O
01	O
a	O
portion	O
0i1he	O
``	O
''	O
!	O
low	O
data	O
setlo	O
<	O
ihe	O
!	O
irsl	O
100	O
(	O
lata	O
»	O
einls	O
,	O
the	O
left..	O
,	O
...	O
nd	O
plot	O
oiiows	O
ihe	O
i'o	O
''	O
leoo	O
<	O
mean	B
proj9c1ions	O
oilhfi	O
(	O
lata	O
poims	O
on	O
lhe	O
principal	B
subspace	I
.	O
the	O
,	O
;	O
gtri·hi\nd	O
plot	O
is	O
obtained	O
by	O
firsl	O
ran	O
<	O
lomly	O
omitting	O
30	O
%	O
0i1he	O
variable	O
.aloo	O
.	O
and	O
lhen	O
us	O
>	O
rlg	O
em	O
10	O
mndie	O
i	O
''	O
''	O
mi	O
...	O
...	O
values	O
.	O
note	O
i	O
!	O
iai	O
eac/1	O
data	O
poinl1hen	O
nos	O
allea.	O
,	O
one	O
missing	O
mea.u	O
,	O
ement	O
but	O
lhoallhe	O
plot	O
i	O
.	O
``	O
``	O
ry	O
..mia	O
,	O
to	O
lhe	O
ona	O
obtained	O
wit	O
''	O
''	O
''	O
l	O
miss	O
...	O
.	O
vall	O
>	O
ll	O
$	O
ewrrise	O
/2	O
,	O
/7	O
subspace	O
to	O
minimize	O
!	O
he	O
squared	O
reoonslructioo	O
error	B
in	O
'oihich	O
the	O
proje	O
<	O
:	O
tion	O
,	O
are	O
c.	O
,	O
n	O
.	O
we	O
ean	O
gh'e	O
a	O
,	O
imple	O
physical	B
analogy	I
for	O
this	O
em	O
algorithm	O
.	O
which	O
is	O
easily	O
visualized	O
for	O
d	O
=	O
2	O
and	O
m	O
=	O
1.	O
coo	O
,	O
ider	O
a	O
collectioo	O
nf	O
data	O
point'	O
,	O
n	O
twi	O
)	O
dimension	O
'	O
,	O
aod	O
let	O
tile	O
u	O
''	O
''	O
'-dimensiunal	O
principal	B
subspace	I
be	O
represented	O
by	O
a	O
<	O
ohd	O
rod	O
.	O
now	O
atlach	O
each	O
data	O
point	O
to	O
the	O
nxi	O
via	O
a	O
,	O
pring	O
oo	O
<	O
:	O
)	O
''	O
ing	O
hooi	O
;	O
:	O
e	O
'	O
,	O
law	O
(	O
``	O
umj	O
energy	O
i	O
,	O
propol1ional	O
10	O
,	O
lie	O
square	O
of	O
lile	O
spring	O
''	O
.	O
length	O
)	O
.	O
in	O
ll1e	O
e	O
'tel	O
'	O
,	O
we	O
keep	O
the	O
nxi	O
hed	O
and	O
allow	O
the	O
attachment	O
point	O
'	O
tn	O
,	O
iide	O
up	O
and	O
<	O
i	O
<	O
,	O
wn	O
ll1e	O
nxi	O
'	O
''	O
a	O
,	O
to	O
minimize	O
ll1e	O
e	O
''	O
,	O
,'lly	O
,	O
this	O
cau	O
''	O
,	O
.	O
each	O
attachment	O
point	O
(	O
independently	O
)	O
10	O
position	O
itself	O
at	O
the	O
orthogonal	O
pmjeclion	O
of	O
the	O
c~sponding	O
data	O
point	O
onto	O
the	O
nxi	O
.	O
in	O
the	O
m	O
'tel	O
'	O
.	O
we	O
keep	O
the	O
attachment	O
poiol	O
'	O
fil	O
<	O
ed	O
and	O
then	O
release	O
tile	O
nxi	O
and	O
allow	O
it	O
to	O
m	O
'	O
>	O
,	O
'e	O
10	O
tile	O
minimum	O
energy	O
posilion	O
.	O
11ie	O
e	O
and	O
m	O
'teps	O
are	O
then	O
repeated	O
until	O
a	O
,	O
uitable	O
c	O
''	O
''	O
vergence	O
cri.eri	O
''	O
''	O
is	O
..	O
,	O
isfled	O
.	O
a.	O
is	O
illuslrated	O
in	O
figure	O
12.12	O
.	O
12.2.3	O
bayesian	O
pea	O
s	O
<	O
j	O
far	O
in	O
oilr	O
di	O
''	O
,	O
''	O
''	O
ioo	O
of	O
pea	O
.	O
we	O
have	O
``	O
,	O
'.nled	O
ihal	O
tile	O
'	O
''	O
ine	O
,	O
ii	O
for	O
,	O
lie	O
dl	O
,	O
nen	O
,	O
ionalit	O
)	O
''	O
of	O
tile	O
principal	O
.ubspace	O
is	O
gi	O
''	O
en	O
,	O
in	O
praclice	O
.	O
``	O
.-e	O
nlmt	O
cooose	O
a	O
suilable	O
,	O
..i	O
''	O
''	O
according	O
10	O
the	O
application	O
.	O
for	O
,	O
isuali	O
,	O
a	O
,	O
ion	O
.	O
we	O
ge	O
''	O
''	O
''	O
,	O
ny	O
choose	O
.\1	O
=	O
2.	O
whereas	O
for	O
oiher	O
application	O
,	O
the	O
approrrialc	O
choice	O
for	O
,1/	O
ma	O
)	O
''	O
be	O
less	O
dea	O
,	O
.	O
one	O
appmao	O
:	O
h	O
i	O
.	O
10	O
pi	O
''	O
,	O
the	O
eigen	O
''	O
alue	O
'peclrum	O
for	O
lhe	O
data	O
set	O
.	O
analog	O
,	O
•	O
.	O
''	O
10	O
the	O
example	O
in	O
figure	O
12.4	O
for	O
the	O
off_line	O
digits	O
dala	O
sci	O
,	O
and	O
look	O
to	O
see	O
if	O
lite	O
eige	O
''	O
,	O
,	O
,	O
i	O
...	O
.	O
nmurally	O
form	O
two	O
groups	O
comprising	O
a	O
set	O
of	O
,	O
mall	O
,	O
'alues	O
separated	O
by	O
a	O
,	O
ign	O
;	O
flcant	O
gap	O
from	O
a	O
``	O
,	O
t	O
of	O
relativel	O
)	O
''	O
large	O
,	O
'alues	O
,	O
indicating	O
a	O
natural	O
cholcc	O
f	O
<	O
>	O
r	O
ai	O
,	O
in	O
practice	O
.	O
such	O
a	O
gap	O
i	O
,	O
oflen	O
'	O
''	O
''	O
seen	O
,	O
``	O
,	O
0	O
-	O
,	O
,	O
,	O
``	O
,	O
o	O
-	O
,	O
-	O
,	O
•	O
,	O
,	O
,	O
0	O
-	O
,	O
,	O
-	O
,	O
-	O
,	O
•	O
,	O
...	O
.	O
~	O
,	O
•	O
0	O
o	O
,	O
,	O
<	O
,	O
0	O
-	O
,	O
,	O
-	O
,	O
-	O
,	O
0	O
o	O
,	O
,	O
,	O
0	O
o	O
flgu..	O
12.12	O
synt	O
''	O
'elic	O
<	O
lata	O
illustrating	O
too	O
em	O
algorithm	O
!	O
of	O
pca	O
defined	O
by	O
(	O
12.58	O
)	O
and	O
(	O
1259	O
)	O
.	O
(	O
8	O
)	O
a	O
data	O
set	O
x	O
with	O
the	O
data	O
points	O
shown	O
in	O
1ji'e	O
«	O
l	O
,	O
t	O
''	O
ll	O
''	O
tm	O
'	O
w'i1	O
!	O
1l	O
!	O
>	O
e	O
t'im	O
pmdpal	O
``	O
''	O
''	O
''	O
''	O
''	O
''	O
,	O
is	O
(	O
shown	O
as	O
eigenveclor1	O
scaled	O
by	O
it	O
>	O
e	O
squafll	O
'oois	O
04	O
the	O
eigej'l\lllluel	O
)	O
.	O
(	O
b	O
)	O
initial	O
configurat	O
''	O
'	O
''	O
01	O
too	O
principalsul	O
>	O
sl	O
>	O
a	O
<	O
:	O
<	O
t	O
defined	O
by	O
w	O
,	O
shown	O
in	O
md	O
,	O
too	O
''	O
lhfir	O
with	O
the	O
fk	O
'	O
(	O
ijeclions	O
01	O
the	O
latll	O
<	O
11	O
points	O
z	O
inlo	O
too	O
<	O
lata	O
space	O
,	O
giitoo	O
by	O
zwt	O
,	O
shown	O
in	O
cyan	O
,	O
(	O
oj	O
alter	O
''	O
''	O
''	O
m	O
step	O
,	O
too	O
laten	O
!	O
si'b	O
«	O
l	O
p	O
>	O
as	O
been	O
update	O
<	O
!	O
wiih	O
z	O
r	O
>	O
el	O
(	O
l	O
nxed	O
.	O
(	O
d	O
)	O
me	O
'	O
tt	O
>	O
e	O
success	O
...	O
.	O
e	O
slep	O
,	O
it	O
>	O
e	O
``	O
''	O
'-'eo	O
01	O
z	O
havu	O
been	O
up	O
<	O
!	O
atll	O
<	O
:1	O
.	O
~	O
'	O
''	O
ihogoooal	O
r	O
>	O
rojecliqn	O
$	O
,	O
with	O
w	O
h	O
&	O
k	O
!	O
fixed	O
.	O
(	O
e	O
)	O
aft	O
...	O
.	O
tile	O
se	O
<	O
:	O
o	O
<	O
l	O
<	O
l	O
m	O
s	O
!	O
flp	O
.	O
<	O
'	O
)	O
after	O
l	O
!	O
ie	O
mc	O
(	O
)	O
<	O
>	O
;	O
l	O
e	O
st	O
''	O
l'	O
s	O
,	O
uion	O
i.j	O
be	O
<	O
:	O
au	O
,	O
''	O
,	O
th~	O
pm/xlhi	O
li	O
>	O
lic	O
pea	O
modd	O
has	O
a	O
well·defined	O
likelillood	O
f	O
''	O
flction	O
,	O
we	O
<	O
wid	O
employ	O
cros	O
,	O
-	O
,	O
-.1idation	O
to	O
delermine	O
the	O
\	O
''	O
ajue	O
of	O
di	O
''	O
''	O
,	O
nsiooa	O
!	O
ity	O
by	O
``	O
'iecting	O
tit	O
<	O
:	O
large	O
,	O
t	O
log	O
likelihood	O
t	O
>	O
i1	O
a	O
'-alidation	O
data	O
set	O
such	O
an	O
opprooch	O
.	O
hov.·~\-er	O
.	O
can	O
become	O
computationally	O
ro	O
<	O
lly	O
.	O
p3rticularl	O
)	O
'	O
if	O
we	O
cqnsid	O
<	O
:	O
,	O
•	O
probabilistic	O
mixlure	O
of	O
pea	O
modds	O
(	O
tipping	O
and	O
bishop	O
.	O
1999a	O
)	O
in	O
``	O
hich	O
we	O
seek	O
10	O
<	O
!	O
etermi	O
'	O
''	O
the	O
appropriate	O
dimen	O
,	O
ionalily	O
``	O
,	O
paraltly	O
for	O
toch	O
componenl	O
in	O
lt1e	O
mixm	O
''	O
''	O
gi'-en	O
thai	O
w.	O
ha	O
,	O
-e	O
a	O
probabilislic	O
formulalion	O
of	O
pea	O
,	O
il	O
s	O
«	O
ms	O
natural	O
10	O
s	O
«	O
k	O
u	O
buye	O
,	O
ian	O
approach	O
10	O
model	O
seleclion	O
.	O
to	O
do	O
thi	O
,	O
.	O
,	O
,	O
''	O
'e	O
nee	O
<	O
!	O
10	O
marginalize	O
001	O
the	O
model	O
paramele	O
''	O
/	O
'	O
.	O
\v	O
.	O
und	O
,	O
,	O
'	O
wilh	O
``	O
''	O
peel	O
to	O
appropriate	O
prior	B
distribution	O
'	O
.	O
this	O
can	O
be	O
done	O
by	O
u	O
,	O
ing	O
a	O
,	O
-ariation.l	O
framework	O
to	O
.pproxim'le	O
the	O
allulylic.lly	O
intractable	O
murginaliuoi	O
;	O
oo	O
,	O
(	O
bi	O
,	O
hop	O
.	O
1mb	O
)	O
.	O
1lic	O
marginal	B
likelihood	I
v.lues	O
.	O
given	O
by	O
ttle	O
,	O
'ari.	O
,	O
ionallower	O
bour.d	O
,	O
cun	O
lhen	O
be	O
c	O
<	O
>	O
mpun	O
:	O
d	O
for	O
a	O
r.nge	O
of	O
different	O
'	O
''	O
tue'	O
''	O
f	O
;	O
\i	O
ar.d	O
itie	O
'	O
''	O
iue	O
giving	O
iht	O
largest	O
marginal	B
likelihood	I
``	O
,	O
iecloo_	O
l1ere	O
we	O
consider	O
.	O
simpler	O
approach	O
introducoo	O
by	O
b.ased	O
on	O
the	O
rddmu	O
``	O
p-	O
582	O
12.	O
continuous	O
latent	O
variables	O
figure	O
12.13	O
probabilistic	B
graphical	I
model	I
for	O
bayesian	O
pea	O
in	O
which	O
the	O
distribution	O
over	O
the	O
parameter	O
matrix	O
w	O
is	O
governed	O
by	O
a	O
vector	O
a	O
of	O
hyperparameters	O
.	O
w	O
n	O
proximation	B
,	O
which	O
is	O
appropriate	O
when	O
the	O
number	O
of	O
data	O
points	O
is	O
relatively	O
large	O
and	O
the	O
corresponding	O
posterior	O
distribution	O
is	O
tightly	O
peaked	O
(	O
bishop	O
,	O
1999a	O
)	O
.	O
it	O
involves	O
a	O
specific	O
choice	O
of	O
prior	B
over	O
w	O
that	O
allows	O
surplus	O
dimensions	O
in	O
the	O
principal	B
subspace	I
to	O
be	O
pruned	O
out	O
of	O
the	O
model	O
.	O
this	O
corresponds	O
to	O
an	O
example	O
of	O
automatic	B
relevance	I
determination	I
,	O
or	O
ard	O
,	O
discussed	O
in	O
section	O
7.2.2.	O
specifically	O
,	O
we	O
define	O
an	O
independent	B
gaussian	O
prior	B
over	O
each	O
column	O
of	O
w	O
,	O
which	O
represent	O
the	O
vectors	O
defining	O
the	O
principal	B
subspace	I
.	O
each	O
such	O
gaussian	O
has	O
an	O
independent	B
variance	O
governed	O
by	O
a	O
precision	O
hyperparameter	O
o	O
:	O
i	O
so	O
that	O
(	O
12.60	O
)	O
where	O
wi	O
is	O
the	O
i	O
th	O
column	O
of	O
w.	O
the	O
resulting	O
model	O
can	O
be	O
represented	O
using	O
the	O
directed	B
graph	O
shown	O
in	O
figure	O
12.13.	O
the	O
values	O
for	O
o	O
:	O
i	O
will	O
be	O
found	O
iteratively	O
by	O
maximizing	O
the	O
marginallikeli	O
(	O
cid:173	O
)	O
hood	O
function	O
in	O
which	O
w	O
has	O
been	O
integrated	O
out	O
.	O
as	O
a	O
result	O
of	O
this	O
optimization	O
,	O
some	O
of	O
the	O
o	O
:	O
i	O
may	O
be	O
driven	O
to	O
infinity	O
,	O
with	O
the	O
corresponding	O
parameters	O
vec	O
(	O
cid:173	O
)	O
tor	O
wi	O
being	O
driven	O
to	O
zero	O
(	O
the	O
posterior	O
distribution	O
becomes	O
a	O
delta	O
function	O
at	O
the	O
origin	O
)	O
giving	O
a	O
sparse	O
solution	O
.	O
the	O
effective	O
dimensionality	O
of	O
the	O
principal	B
subspace	I
is	O
then	O
determined	O
by	O
the	O
number	O
of	O
finite	O
o	O
:	O
i	O
values	O
,	O
and	O
the	O
correspond	O
(	O
cid:173	O
)	O
ing	O
vectors	O
wi	O
can	O
be	O
thought	O
of	O
as	O
'relevant	O
'	O
for	O
modelling	O
the	O
data	O
distribution	O
.	O
in	O
this	O
way	O
,	O
the	O
bayesian	O
approach	O
is	O
automatically	O
making	O
the	O
trade-off	O
between	O
improving	O
the	O
fit	O
to	O
the	O
data	O
,	O
by	O
using	O
a	O
larger	O
number	O
of	O
vectors	O
wi	O
with	O
their	O
cor	O
(	O
cid:173	O
)	O
responding	O
eigenvalues	O
ai	O
each	O
tuned	O
to	O
the	O
data	O
,	O
and	O
reducing	O
the	O
complexity	O
of	O
the	O
model	O
by	O
suppressing	O
some	O
of	O
the	O
wi	O
vectors	O
.	O
the	O
origins	O
of	O
this	O
sparsity	B
were	O
discussed	O
earlier	O
in	O
the	O
context	O
of	O
relevance	B
vector	I
machines	O
.	O
the	O
values	O
of	O
o	O
:	O
i	O
are	O
re-estimated	O
during	O
training	B
by	O
maximizing	O
the	O
log	O
marginal	O
likelihood	O
given	O
by	O
p	O
(	O
xla	O
,	O
j-l	O
,	O
0'2	O
)	O
=	O
jp	O
(	O
xiw	O
,	O
j-l	O
,	O
o'2	O
)	O
p	O
(	O
wla	O
)	O
dw	O
(	O
12.61	O
)	O
where	O
the	O
log	O
ofp	O
(	O
xiw	O
,	O
j-l	O
,	O
0'2	O
)	O
is	O
given	O
by	O
(	O
12.43	O
)	O
.	O
note	O
that	O
for	O
simplicity	O
we	O
also	O
treat	O
j-l	O
and	O
0'2	O
as	O
parameters	O
to	O
be	O
estimated	O
,	O
rather	O
than	O
defining	O
priors	O
over	O
these	O
parameters	O
.	O
section	O
7.2	O
12.2.	O
probabilistic	O
pea	O
583	O
section	O
4.4	O
section	O
3.5.3	O
because	O
this	O
integration	O
is	O
intractable	O
,	O
we	O
make	O
use	O
of	O
the	O
laplace	O
approxima	O
(	O
cid:173	O
)	O
tion	O
.	O
if	O
we	O
assume	O
that	O
the	O
posterior	O
distribution	O
is	O
sharply	O
peaked	O
,	O
as	O
will	O
occur	O
for	O
sufficiently	O
large	O
data	O
sets	O
,	O
then	O
the	O
re-estimation	O
equations	O
obtained	O
by	O
maximizing	O
the	O
marginal	B
likelihood	I
with	O
respect	O
to	O
ai	O
take	O
the	O
simple	O
form	O
which	O
follows	O
from	O
(	O
3.98	O
)	O
,	O
noting	O
that	O
the	O
dimensionality	O
of	O
wi	O
is	O
d.	O
these	O
re	O
(	O
cid:173	O
)	O
estimations	O
are	O
interleaved	O
with	O
the	O
em	O
algorithm	O
updates	O
for	O
determining	O
wand	O
a	O
2	O
•	O
the	O
e-step	O
equations	O
are	O
again	O
given	O
by	O
(	O
12.54	O
)	O
and	O
(	O
12.55	O
)	O
.	O
similarly	O
,	O
the	O
m	O
(	O
cid:173	O
)	O
step	O
equation	O
for	O
a	O
2	O
is	O
again	O
given	O
by	O
(	O
12.57	O
)	O
.	O
the	O
only	O
change	O
is	O
to	O
the	O
m-step	O
equation	O
for	O
w	O
,	O
which	O
is	O
modified	O
to	O
give	O
(	O
12.62	O
)	O
(	O
12.63	O
)	O
where	O
a	O
=	O
diag	O
(	O
ai	O
)	O
'	O
the	O
value	O
of	O
i-	O
''	O
is	O
given	O
by	O
the	O
sample	B
mean	I
,	O
as	O
before	O
.	O
if	O
we	O
choose	O
m	O
=	O
d	O
-	O
1	O
then	O
,	O
if	O
all	O
ai	O
values	O
are	O
finite	O
,	O
the	O
model	O
represents	O
a	O
full-covariance	O
gaussian	O
,	O
while	O
if	O
all	O
the	O
ai	O
go	O
to	O
infinity	O
the	O
model	O
is	O
equivalent	O
to	O
an	O
isotropic	B
gaussian	O
,	O
and	O
so	O
the	O
model	O
can	O
encompass	O
all	O
pennissible	O
values	O
for	O
the	O
effective	O
dimensionality	O
of	O
the	O
principal	B
subspace	I
.	O
it	O
is	O
also	O
possible	O
to	O
consider	O
smaller	O
values	O
of	O
m	O
,	O
which	O
will	O
save	O
on	O
computational	O
cost	O
but	O
which	O
will	O
limit	O
the	O
maximum	O
dimensionality	O
of	O
the	O
subspace	O
.	O
a	O
comparison	O
of	O
the	O
results	O
of	O
this	O
algorithm	O
with	O
standard	O
probabilistic	O
pca	O
is	O
shown	O
in	O
figure	O
12.14.	O
bayesian	O
pca	O
provides	O
an	O
opportunity	O
to	O
illustrate	O
the	O
gibbs	O
sampling	O
algo	O
(	O
cid:173	O
)	O
rithm	O
discussed	O
in	O
section	O
11.3.	O
figure	O
12.15	O
shows	O
an	O
example	O
of	O
the	O
samples	O
from	O
the	O
hyperparameters	O
in	O
ai	O
for	O
a	O
data	O
set	O
in	O
d	O
=	O
4	O
dimensions	O
in	O
which	O
the	O
di	O
(	O
cid:173	O
)	O
mensionality	O
of	O
the	O
latent	O
space	O
is	O
m	O
=	O
3	O
but	O
in	O
which	O
the	O
data	O
set	O
is	O
generated	O
from	O
a	O
probabilistic	O
pca	O
model	O
having	O
one	O
direction	O
of	O
high	O
variance	B
,	O
with	O
the	O
remaining	O
directions	O
comprising	O
low	O
variance	B
noise	O
.	O
this	O
result	O
shows	O
clearly	O
the	O
presence	O
of	O
three	O
distinct	O
modes	O
in	O
the	O
posterior	O
distribution	O
.	O
at	O
each	O
step	O
of	O
the	O
iteration	O
,	O
one	O
of	O
the	O
hyperparameters	O
has	O
a	O
small	O
value	O
and	O
the	O
remaining	O
two	O
have	O
large	O
values	O
,	O
so	O
that	O
two	O
of	O
the	O
three	O
latent	O
variables	O
are	O
suppressed	O
.	O
during	O
the	O
course	O
of	O
the	O
gibbs	O
sampling	O
,	O
the	O
solution	O
makes	O
sharp	O
transitions	O
between	O
the	O
three	O
modes	O
.	O
the	O
model	O
described	O
here	O
involves	O
a	O
prior	B
only	O
over	O
the	O
matrix	O
w.	O
a	O
fully	O
bayesian	O
treatment	O
of	O
pca	O
,	O
including	O
priors	O
over	O
1-	O
''	O
,	O
a	O
2	O
,	O
and	O
n	O
,	O
and	O
solved	O
us	O
(	O
cid:173	O
)	O
ing	O
variational	O
methods	O
,	O
is	O
described	O
in	O
bishop	O
(	O
1999b	O
)	O
.	O
for	O
a	O
discussion	O
of	O
vari	O
(	O
cid:173	O
)	O
ous	O
bayesian	O
approaches	O
to	O
detennining	O
the	O
appropriate	O
dimensionality	O
for	O
a	O
pca	O
model	O
,	O
see	O
minka	O
(	O
2001c	O
)	O
.	O
12.2.4	O
factor	B
analysis	I
factor	O
analysis	O
is	O
a	O
linear-gaussian	O
latent	B
variable	I
model	O
that	O
is	O
closely	O
related	O
to	O
probabilistic	O
pca	O
.	O
its	O
definition	O
differs	O
from	O
that	O
of	O
probabilistic	O
pca	O
only	O
in	O
that	O
the	O
conditional	B
distribution	O
of	O
the	O
observed	B
variable	I
x	O
given	O
the	O
latent	B
variable	I
z	O
is	O
584	O
12.	O
continuous	O
latent	O
variables	O
•	O
••	O
•	O
•	O
•	O
•	O
•	O
•	O
•	O
•	O
•	O
•	O
•	O
•	O
•	O
•	O
••	O
•	O
•	O
•	O
•	O
•	O
•	O
•	O
••	O
•	O
•	O
••	O
•	O
•	O
••	O
•	O
•	O
•	O
•	O
•	O
•	O
•	O
•	O
•••	O
••	O
•	O
•	O
•	O
•	O
•	O
•	O
figure	O
12.14	O
'hinloo	O
'	O
diagrams	O
of	O
the	O
matrix	O
w	O
in	O
which	O
each	O
element	O
01	O
the	O
matrix	O
is	O
depicted	O
as	O
a	O
square	O
(	O
white	O
lor	O
positive	O
and	O
black	O
lor	O
negative	O
values	O
)	O
whose	O
area	O
is	O
proportional	O
to	O
the	O
magnitude	O
of	O
that	O
element	O
.	O
the	O
synthetic	O
data	O
sel	O
comprises	O
300	O
data	O
points	O
in	O
d	O
=	O
10	O
dimensions	O
sampled	O
from	O
a	O
gaussian	O
distribution	O
having	O
standard	B
deviation	I
1.0	O
in	O
3	O
directions	O
and	O
standard	B
deviation	I
0.5	O
in	O
the	O
remaining	O
7	O
directions	O
for	O
a	O
data	O
set	O
in	O
d	O
=	O
10	O
dimensions	O
having	O
at	O
=	O
3	O
directions	O
with	O
larger	O
variance	B
than	O
the	O
remaining	O
7	O
directions	O
.	O
the	O
left-hand	O
plol	O
shows	O
the	O
result	O
irom	O
maximum	B
likelihood	I
probabilistic	O
pca	O
,	O
and	O
the	O
left·hand	O
plot	O
shows	O
the	O
corresponding	O
resuft	O
from	O
bayesian	O
pea	O
.	O
we	O
see	O
how	O
the	O
bayesian	O
model	O
is	O
able	O
to	O
discover	O
the	O
appropriate	O
dimensionality	O
by	O
suppressing	O
the	O
6	O
surplus	O
degrees	B
of	I
freedom	I
.	O
taken	O
to	O
have	O
a	O
diagonal	B
rather	O
than	O
an	O
isotropic	B
covariance	O
so	O
that	O
p	O
(	O
xlz	O
)	O
=	O
n	O
(	O
xlwz	O
+	O
1	O
'	O
.	O
\ii	O
)	O
(	O
12.64	O
)	O
where	O
ill	O
is	O
a	O
d	O
x	O
d	O
diagonal	B
matrix	O
.	O
note	O
that	O
the	O
factor	B
analysis	I
model	O
,	O
in	O
common	O
with	O
probabilistic	O
pca	O
.	O
assumes	O
that	O
the	O
observed	O
variables	O
xl	O
,	O
...	O
,	O
xo	O
are	O
indepen	O
(	O
cid:173	O
)	O
dent	O
.	O
given	O
the	O
latent	B
variable	I
z.	O
in	O
essence	O
.	O
the	O
factor	B
analysis	I
model	O
is	O
explaining	O
the	O
observed	O
covariance	O
structure	O
of	O
the	O
data	O
by	O
representing	O
the	O
independent	B
vari	O
(	O
cid:173	O
)	O
ance	O
associated	O
with	O
each	O
coordinate	O
in	O
the	O
matrix	O
1j	O
.	O
'	O
and	O
capturing	O
the	O
covariance	B
between	O
variables	O
in	O
the	O
matrix	O
w.	O
in	O
the	O
factor	B
analysis	I
literature	O
.	O
the	O
columns	O
of	O
w.	O
which	O
capture	O
the	O
correlations	O
between	O
observed	O
variables	O
.	O
are	O
calledfaclor	O
loadings	O
.	O
and	O
the	O
diagonal	B
elements	O
of	O
1j.	O
'	O
.	O
which	O
represent	O
the	O
independent	B
noise	O
variances	O
for	O
each	O
of	O
the	O
variables	O
,	O
are	O
called	O
llniqllenesses	O
.	O
the	O
origins	O
of	O
factor	B
analysis	I
are	O
as	O
old	O
as	O
those	O
of	O
pca	O
.	O
and	O
discussions	O
of	O
factor	B
analysis	I
can	O
be	O
found	O
in	O
the	O
books	O
by	O
everitt	O
(	O
1984	O
)	O
.	O
bartholomew	O
(	O
1987	O
)	O
,	O
and	O
basilevsky	O
(	O
1994	O
)	O
.	O
links	O
between	O
factor	B
analysis	I
and	O
pca	O
were	O
investigated	O
by	O
lilwley	O
(	O
1953	O
)	O
and	O
anderson	O
(	O
1963	O
)	O
who	O
showed	O
that	O
at	O
stationary	B
points	O
of	O
the	O
likelihood	B
function	I
.	O
for	O
a	O
faclor	O
analysis	O
model	O
with	O
1j	O
.	O
'	O
=	O
(	O
121	O
,	O
the	O
columns	O
of	O
w	O
are	O
scaled	O
eigenvectors	O
of	O
the	O
sample	O
covariance	O
matrix	O
.	O
and	O
(	O
12	O
is	O
the	O
average	O
of	O
the	O
discarded	O
eigenvalues	O
.	O
later	O
.	O
tipping	O
and	O
bishop	O
(	O
1999b	O
)	O
showed	O
that	O
the	O
maximum	O
of	O
the	O
log	O
likelihood	O
function	O
occurs	O
when	O
the	O
eigenvectors	O
comprising	O
ware	O
chosen	O
to	O
be	O
the	O
principal	O
eigenvectors	O
.	O
making	O
use	O
of	O
(	O
2.115	O
)	O
.	O
we	O
see	O
that	O
the	O
marginal	B
distribution	O
for	O
the	O
observed	O
12.2.l'ru	O
''	O
:	O
ohilislk	O
i'ca	O
585	O
flliure12.15	O
gillbs	O
.	O
,	O
,	O
,	O
,	O
>	O
p	O
!	O
j	O
''	O
lllo	O
<	O
bay	O
<	O
lslan	O
pca	O
sh	O
<	O
ming	O
plots	O
oj	O
ino	O
,	O
versus	O
~eralion	O
number	O
br	O
three	O
showing	O
tr	O
''	O
''	O
''	O
tions	O
tbe	O
th	O
''	O
'	O
''	O
moots	O
<	O
a	O
!	O
he	O
posterior	O
distribution	O
.	O
betw	O
...	O
..	O
''	O
values	O
.	O
eurr	O
''	O
e	O
12,19	O
s	O
''	O
na	O
''	O
12.4	O
,	O
-	O
''	O
riabl	O
,	O
i	O
'	O
gi	O
,	O
-	O
,	O
n	O
by	O
1	O
'	O
(	O
x	O
)	O
~	O
n	O
(	O
xlj	O
'	O
,	O
c	O
)	O
whe	O
...	O
now	O
c=wwt+'i	O
'	O
.	O
(	O
12,6~	O
)	O
as	O
with	O
probabilistic	O
pc	O
a	O
,	O
thi	O
,	O
momi	O
is	O
im-	O
''	O
ri.rrl	O
to	O
'olalions	O
in	O
11	O
>	O
<	O
0	O
latent	O
'pace	O
.	O
histoocally	O
,	O
factor	O
anal	O
)	O
'	O
,	O
;	O
s	O
has	O
been	O
lhe	O
,	O
ubjerl	O
of	O
col1tro	O
,	O
-ersy	O
wroe	O
''	O
a	O
!	O
tempt	O
<	O
h	O
''	O
,	O
-e	O
bttn	O
``	O
'a	O
<	O
k	O
:	O
to	O
place	O
an	O
intc'p	O
''	O
't	O
''	O
lioo	O
on	O
the	O
ind	O
;	O
vidual	O
faclon	O
(	O
the	O
coofdinates	O
in	O
z_space	O
)	O
.	O
which	O
h3.\	O
pm	O
''	O
en	O
problematic	O
due	O
to	O
lr.e	O
``	O
''	O
''	O
i	O
<	O
lcmifiabilily	O
of	O
factof	O
analysis	O
associmed	O
with	O
mation	B
'	O
in	O
this	O
'pace	O
.	O
from	O
oor	O
perspeoh-e	O
,	O
howe	O
,	O
-er	O
.	O
we	O
shall	O
.iew	O
factor	B
analysis	I
as	O
a	O
form	O
of	O
lalent	O
``	O
ariable	O
densily	O
model	O
.	O
in	O
which	O
the	O
form	O
of	O
tl	O
>	O
c	O
lalent	O
'pace	O
i	O
'	O
of	O
interest	O
but	O
no	O
!	O
the	O
particular	O
choicc	O
of	O
coordinates	O
used	O
to	O
descrit	O
>	O
c	O
il	O
.	O
if	O
we	O
wish	O
to	O
remove	O
the	O
degeneracy	O
a'sociated	O
with	O
latent	O
'pace	O
roiations	O
.	O
``	O
``	O
e	O
mu	O
,	O
t	O
con'ider	O
non-gaussian	O
latent	O
,	O
-	O
''	O
riable	O
di'tribution	O
,	O
.	O
gi	O
''	O
irrg	O
rise	O
10	O
independent	O
component	O
.n.lysi	O
,	O
(	O
lca	O
)	O
models	O
.	O
we	O
can	O
detenni	O
''	O
e	O
the	O
parameters	O
i	O
'	O
.	O
\v	O
.	O
``	O
nd	O
...	O
.	O
in	O
the	O
fac	O
!	O
of	O
an.ly	O
,	O
i	O
,	O
model	O
by	O
muimum	O
likelihood	O
.	O
11..	O
solution	O
for	O
i	O
'	O
i	O
'	O
ag	O
''	O
in	O
given	O
by	O
the	O
``	O
,	O
,	O
,	O
,pie	O
``	O
'ean	O
.	O
how·	O
eyc	O
'	O
.	O
``	O
nli~e	O
probabili	O
,	O
tic	O
l'ca.lllcre	O
i	O
'	O
no	O
longer	O
a	O
closed-form	O
maximum	B
likelihood	I
solution	O
for	O
\v	O
.	O
``	O
,	O
'hich	O
mu.\ltherdorc	O
be	O
found	O
i'er.li	O
,	O
'c1	O
)	O
'_	O
because	O
faclor	O
anal	O
)	O
'	O
,	O
i	O
.	O
is	O
a	O
latent	B
variable	I
model	O
thi	O
'	O
can	O
be	O
don	O
.	O
using	O
an	O
em	O
algorilhm	O
(	O
r.bin	O
and	O
thayer	O
.	O
1982	O
)	O
!	O
h	O
''	O
t	O
is	O
``	O
nalogou	O
,	O
to	O
the	O
one	O
used	O
(	O
of	O
pml	O
>	O
;	O
lbili.tie	O
pea	O
.	O
specihcally	O
.	O
lhe	O
e-'lep	O
eqnjtioo	O
,	O
are	O
g	O
;	O
'-en	O
by	O
e	O
[	O
zoj	O
=	O
gwt	O
''	O
,	O
-	O
'	O
(	O
xn	O
-	O
xl	O
e	O
[	O
z	O
''	O
z~j	O
_	O
g	O
+	O
e	O
[	O
zo	O
]	O
e	O
[	O
z	O
,	O
,	O
]	O
t	O
where	O
``	O
''	O
e	O
h	O
''	O
,	O
'e	O
defi	O
''	O
''	O
d	O
(	O
12,66	O
)	O
(	O
1267	O
)	O
(	O
l2,6h	O
)	O
noie	O
th	O
''	O
t	O
thi	O
'	O
i	O
'	O
e.pre	O
,	O
<	O
ed	O
in	O
a	O
for	O
'	O
''	O
thai	O
in	O
,	O
-oh'es	O
inycrsi	O
''	O
n	O
of	O
mal	O
rices	O
``	O
f	O
silo	O
,	O
\	O
i	O
x	O
,	O
if	O
rathe'lhan	O
d	O
x	O
d	O
(	O
ex	O
''	O
''	O
,	O
,	O
,	O
,	O
for	O
tbe	O
d	O
x	O
d	O
diagooal	O
matrix	O
oj	O
'	O
``	O
'hose	O
in	O
,	O
-erse	O
i	O
.	O
'	O
'ri	O
''	O
ial	O
586	O
12.	O
continuous	O
latent	O
variables	O
exercise	O
12.22	O
to	O
compute	O
in	O
o	O
(	O
d	O
)	O
steps	O
)	O
,	O
which	O
is	O
convenient	O
because	O
often	O
m	O
«	O
d.	O
similarly	O
,	O
the	O
m-step	O
equations	O
take	O
the	O
form	O
w	O
new	O
[	O
~	O
(	O
x	O
''	O
-xllllizn	O
]	O
''	O
]	O
[	O
~ill	O
[	O
znz~i	O
]	O
-'	O
diag	O
{	O
s-w.'w	O
~	O
~1ll	O
[	O
zn	O
]	O
(	O
xn	O
_	O
xl	O
''	O
}	O
(	O
12.69	O
)	O
(	O
12.70	O
)	O
where	O
the	O
'diag	O
'	O
operator	O
sets	O
all	O
of	O
the	O
nondiagonal	O
elements	O
of	O
a	O
matrix	O
to	O
zero	O
.	O
a	O
bayesian	O
treatment	O
of	O
the	O
factor	B
analysis	I
model	O
can	O
be	O
obtained	O
by	O
a	O
straightforward	O
application	O
of	O
the	O
techniques	O
discussed	O
in	O
this	O
book	O
.	O
another	O
difference	O
between	O
probabilistic	O
pca	O
and	O
factor	B
analysis	I
concerns	O
their	O
different	O
behaviour	O
under	O
transformations	O
of	O
the	O
data	O
set	O
.	O
for	O
pca	O
and	O
probabilis	O
(	O
cid:173	O
)	O
tic	O
pca	O
,	O
if	O
we	O
rotate	O
the	O
coordinate	O
system	O
in	O
data	O
space	O
,	O
then	O
we	O
obtain	O
exactly	O
the	O
same	O
fit	O
to	O
the	O
data	O
but	O
with	O
the	O
w	O
matrix	O
transformed	O
by	O
the	O
corresponding	O
rotation	O
matrix	O
.	O
however	O
,	O
for	O
factor	O
analysis	O
,	O
the	O
analogous	O
property	O
is	O
that	O
if	O
we	O
make	O
a	O
component-wise	O
re-scaling	O
of	O
the	O
data	O
vectors	O
,	O
then	O
this	O
is	O
absorbed	O
into	O
a	O
corresponding	O
re-scaling	O
of	O
the	O
elements	O
of	O
\	O
)	O
i.	O
exercise	O
12.25	O
12.3.	O
kernel	O
pea	O
in	O
chapter	O
6	O
,	O
we	O
saw	O
how	O
the	O
technique	O
of	O
kernel	B
substitution	I
allows	O
us	O
to	O
take	O
an	O
algorithm	O
expressed	O
in	O
terms	O
of	O
scalar	O
products	O
of	O
the	O
form	O
x	O
t	O
x	O
'	O
and	O
generalize	O
that	O
algorithm	O
by	O
replacing	O
the	O
scalar	O
products	O
with	O
a	O
nonlinear	O
kernel	O
.	O
here	O
we	O
apply	O
this	O
technique	O
of	O
kernel	B
substitution	I
to	O
principal	B
component	I
analysis	I
,	O
thereby	O
obtaining	O
a	O
nonlinear	O
generalization	B
called	O
kernel	O
pea	O
(	O
scholkopf	O
et	O
al.	O
,	O
1998	O
)	O
.	O
consider	O
a	O
data	O
set	O
{	O
xn	O
}	O
of	O
observations	O
,	O
where	O
n	O
=	O
1	O
,	O
...	O
,	O
n	O
,	O
in	O
a	O
space	O
of	O
dimensionality	O
d.	O
in	O
order	O
to	O
keep	O
the	O
notation	O
uncluttered	O
,	O
we	O
shall	O
assume	O
that	O
we	O
have	O
already	O
subtracted	O
the	O
sample	B
mean	I
from	O
each	O
of	O
the	O
vectors	O
x	O
n	O
,	O
so	O
that	O
ln	O
x	O
n	O
=	O
o.	O
the	O
first	O
step	O
is	O
to	O
express	O
conventional	O
pca	O
in	O
such	O
a	O
form	O
that	O
the	O
data	O
vectors	O
{	O
x	O
n	O
}	O
appear	O
only	O
in	O
the	O
form	O
of	O
the	O
scalar	O
products	O
x~	O
x	O
m	O
.	O
recall	O
that	O
the	O
principal	O
components	O
are	O
defined	O
by	O
the	O
eigenvectors	O
ui	O
of	O
the	O
covariance	B
matrix	I
where	O
i	O
=	O
1	O
,	O
...	O
,	O
d.	O
here	O
the	O
d	O
x	O
d	O
sample	O
covariance	O
matrix	O
s	O
is	O
defined	O
by	O
sui	O
=	O
aiui	O
(	O
12.71	O
)	O
and	O
the	O
eigenvectors	O
are	O
normalized	O
such	O
that	O
ut	O
ui	O
=	O
1.	O
now	O
consider	O
a	O
nonlinear	O
transformation	O
¢	O
(	O
x	O
)	O
into	O
an	O
m	O
-dimensional	O
feature	B
space	I
,	O
so	O
that	O
each	O
data	O
point	O
x	O
n	O
is	O
thereby	O
projected	O
onto	O
a	O
point	O
¢	O
(	O
xn	O
)	O
.	O
we	O
can	O
(	O
12.72	O
)	O
12.3.	O
k~mci	O
l'co	O
'	O
.	O
587	O
...	O
'\	O
figu'.12.16	O
sctiematic	O
_	O
,	O
.lion	O
01	O
kernel	O
pea	O
.	O
a	O
<	O
utll	O
hi	O
in	O
lhe	O
oflglnal	O
<	O
uta	O
space	O
l~'_	O
plot	O
}	O
..	O
pfoleled	O
by	O
'	O
``	O
''	O
''	O
'*-	O
tranllklfmalion	O
~	O
,	O
,	O
}	O
1nio	O
.	O
fa.tur	O
.	O
space	O
(	O
fight_	O
plot	O
)	O
.	O
by	O
i*b'~	O
pca	O
in	O
the	O
!	O
hue	O
111**	O
.	O
we	O
oblao'ilha	O
pmeiilai	O
~	O
''	O
lls	O
.	O
``	O
'who	O
<	O
:	O
tllha	O
tnt	O
ie	O
...	O
...	O
...	O
in	O
blue.	O
,	O
..	O
,	O
..	O
ojano4ed	O
by	O
lha	O
_	O
plolklio	O
''	O
.	O
onio	O
lhe	O
iirsl	O
poiridl*	O
'	O
''	O
''	O
''	O
...	O
...	O
...	O
11	O
.	O
``	O
``	O
''	O
'*	O
'	O
_od	O
110	O
``	O
''	O
'*-	O
poof	O
&	O
cllu	O
.	O
in	O
...	O
.	O
<	O
lfisionai	O
oillll	O
111**	O
.	O
hole	O
iiuiiin	O
gmefm	O
'	O
la	O
nol	O
pox	O
'	O
..	O
110	O
'	O
.	O
.	O
.	O
.	O
.	O
.	O
1ha	O
,	O
...	O
iiio_	O
poi	O
...	O
...	O
..	O
00i	O
'	O
...	O
...	O
..	O
by._1n	O
``	O
apam	O
.	O
v	O
,	O
tha	O
gr-	O
.	O
...	O
.	O
in	O
imiun	O
apam	O
indicma	O
iha	O
_	O
__	O
ptrform	O
)	O
l-.bnl	O
pea	O
ill	O
fnlllk	O
lopice	O
.	O
..-iudl	O
,	O
mpiiclily	O
«	O
lii'il	O
's	O
•	O
-	O
'_	O
...	O
.-..	O
model	O
ill	O
onpnll	O
cbuo	O
~	O
as	O
,1lu	O
>	O
tr*d	O
in	O
fllift	O
12-16.	O
princlpai	O
''	O
io..it	O
.	O
.	O
lei	O
'	O
''	O
los	O
!	O
oulllt	O
1nl	O
illt~	O
diu	O
.	O
~	O
lobo	O
halnro	O
mean	B
,	O
fu	O
ji	O
)	O
!	O
hal	O
l.	O
4	O
>	O
(	O
``	O
'.j	O
..	O
o.	O
we	O
dwl	O
rctlll'1l	O
10	O
itl	O
,	O
~	O
pol	O
'	O
''	O
.ihonly	O
.	O
1llt.1f	O
``	O
.if	O
``	O
'	O
''	O
''	O
''	O
*	O
co\-.ullcc	O
mmfu	O
,	O
n	O
(	O
~	O
.if*'e	O
,	O
~	O
l	O
''	O
''	O
by	O
,	O
..	O
,	O
.~	O
.	O
l	O
<	O
>	O
(	O
x.j4	O
>	O
(	O
x	O
.	O
)	O
t	O
c	O
-	O
and	O
,	O
l~	O
``	O
,	O
,	O
,	O
,n	O
'	O
''	O
mol	O
''	O
opan	O
,	O
ion	O
i	O
'	O
«	O
lined	O
by	O
cv	O
,	O
=	O
a	O
,	O
v	O
,	O
;	O
=	O
1	O
...	O
,	O
.	O
m.	O
our	O
goal	O
is	O
10	O
soh	O
'	O
''	O
lhis	O
eigen	O
''	O
lilue	O
problem	O
wilhoul	O
ha	O
''	O
inlllo	O
work	O
.	O
``	O
plici	O
,	O
ly	O
in	O
,	O
he	O
f.liture	O
'pace	O
.	O
from	O
!	O
he	O
definilion	O
of	O
c.	O
lhe	O
.ill	O
''	O
''	O
''	O
''	O
''	O
l	O
''	O
'	O
''	O
equal	O
ions	O
lell	O
'	O
u	O
,	O
thai	O
y	O
,	O
!	O
-ali	O
,	O
fies	O
.	O
``	O
s	O
l	O
<	O
bc	O
''	O
.	O
)	O
{	O
<	O
b	O
(	O
x.lt	O
v	O
,	O
}	O
-	O
)	O
''	O
y	O
,	O
(	O
12.7~	O
)	O
..	O
,	O
...	O
...	O
..	O
..-	O
''	O
.-	O
ln1	O
(	O
proo.idcd	O
a	O
,	O
>	O
0	O
)	O
tilt	O
``	O
cc'lor	O
v	O
,	O
is	O
li	O
''	O
n	O
by	O
•	O
ii_	O
rombllla	O
''	O
on	O
of	O
illt	O
d	O
>	O
(	O
j	O
...	O
..	O
jo	O
<	O
;	O
.-	O
he	O
``	O
-	O
''	O
llm	O
iillhc	O
(	O
orm	O
,	O
...	O
'	O
''	O
'	O
l	O
11	O
,	O
.4	O
>	O
(	O
``	O
.	O
)	O
.	O
v	O
,	O
(	O
12.73	O
)	O
(	O
12,74	O
)	O
(	O
12.76	O
)	O
588	O
12.	O
continuous	O
latent	O
variables	O
substituting	O
this	O
expansion	O
back	O
into	O
the	O
eigenvector	O
equation	O
,	O
we	O
obtain	O
1	O
n	O
n	O
l	O
n=l	O
n	O
¢	O
(	O
xn	O
)	O
¢	O
(	O
xn	O
)	O
t	O
l	O
n	O
aim¢	O
(	O
xm	O
)	O
=	O
ai	O
l	O
m=l	O
n=l	O
ain¢	O
(	O
xn	O
)	O
,	O
(	O
12.77	O
)	O
the	O
key	O
step	O
is	O
now	O
to	O
express	O
this	O
in	O
terms	O
of	O
the	O
kernel	B
function	I
k	O
(	O
xn	O
,	O
x	O
m	O
)	O
=	O
¢	O
(	O
xn	O
)	O
t¢	O
(	O
xm	O
)	O
,	O
which	O
we	O
do	O
by	O
multiplying	O
both	O
sides	O
by	O
¢	O
(	O
xz	O
)	O
t	O
to	O
give	O
1	O
n	O
m	O
n	O
lk	O
(	O
xi'xn	O
)	O
l	O
n=l	O
m=l	O
aimk	O
(	O
xn	O
,	O
xm	O
)	O
=	O
ai	O
laink	O
(	O
xi'xn	O
)	O
,	O
(	O
12.78	O
)	O
n	O
n=l	O
this	O
can	O
be	O
written	O
in	O
matrix	O
notation	O
as	O
where	O
ai	O
is	O
an	O
n-dimensional	O
column	O
vector	O
with	O
elements	O
ani	O
for	O
n	O
=	O
1	O
,	O
...	O
,	O
n.	O
we	O
can	O
find	O
solutions	O
for	O
ai	O
by	O
solving	O
the	O
following	O
eigenvalue	O
problem	O
(	O
12.80	O
)	O
(	O
12.79	O
)	O
exercise	O
12.26	O
in	O
which	O
we	O
have	O
removed	O
a	O
factor	O
of	O
k	O
from	O
both	O
sides	O
of	O
(	O
12.79	O
)	O
.	O
note	O
that	O
the	O
solutions	O
of	O
(	O
12.79	O
)	O
and	O
(	O
12.80	O
)	O
differ	O
only	O
by	O
eigenvectors	O
of	O
k	O
having	O
zero	O
eigenvalues	O
that	O
do	O
not	O
affect	O
the	O
principal	O
components	O
projection	O
.	O
the	O
normalization	O
condition	O
for	O
the	O
coefficients	O
ai	O
is	O
obtained	O
by	O
requiring	O
that	O
the	O
eigenvectors	O
in	O
feature	B
space	I
be	O
normalized	O
.	O
using	O
(	O
12.76	O
)	O
and	O
(	O
12.80	O
)	O
,	O
we	O
have	O
1	O
=	O
v	O
;	O
vi	O
=	O
l	O
l	O
n	O
n	O
n=l	O
m=l	O
ainaim¢	O
(	O
xn	O
)	O
t¢	O
(	O
xm	O
)	O
=	O
a	O
;	O
k~	O
=	O
aina	O
;	O
ai'	O
(	O
12.81	O
)	O
having	O
solved	O
the	O
eigenvector	O
problem	O
,	O
the	O
resulting	O
principal	O
component	O
pro	O
(	O
cid:173	O
)	O
jections	O
can	O
then	O
also	O
be	O
cast	O
in	O
terms	O
of	O
the	O
kernel	B
function	I
so	O
that	O
,	O
using	O
(	O
12.76	O
)	O
,	O
the	O
projection	O
of	O
a	O
point	O
x	O
onto	O
eigenvector	O
i	O
is	O
given	O
by	O
yi	O
(	O
x	O
)	O
=	O
¢	O
(	O
x	O
)	O
tvi	O
=	O
l	O
ain¢	O
(	O
x	O
)	O
t	O
¢	O
(	O
xn	O
)	O
=	O
l	O
aink	O
(	O
x	O
,	O
x	O
n	O
)	O
n	O
n	O
n=l	O
n=l	O
(	O
12.82	O
)	O
and	O
so	O
again	O
is	O
expressed	O
in	O
terms	O
of	O
the	O
kernel	B
function	I
.	O
in	O
the	O
original	O
d-dimensional	O
x	O
space	O
there	O
are	O
d	O
orthogonal	O
eigenvectors	O
and	O
hence	O
we	O
can	O
find	O
at	O
most	O
d	O
linear	O
principal	O
components	O
.	O
the	O
dimensionality	O
m	O
of	O
the	O
feature	B
space	I
,	O
however	O
,	O
can	O
be	O
much	O
larger	O
than	O
d	O
(	O
even	O
infinite	O
)	O
,	O
and	O
thus	O
we	O
can	O
find	O
a	O
number	O
of	O
nonlinear	O
principal	O
components	O
that	O
can	O
exceed	O
d.	O
note	O
,	O
however	O
,	O
that	O
the	O
number	O
of	O
nonzero	O
eigenvalues	O
can	O
not	O
exceed	O
the	O
number	O
n	O
of	O
data	O
points	O
,	O
because	O
(	O
even	O
if	O
m	O
>	O
n	O
)	O
the	O
covariance	B
matrix	I
in	O
feature	B
space	I
has	O
rank	O
at	O
most	O
equal	O
to	O
n.	O
this	O
is	O
reflected	O
in	O
the	O
fact	O
that	O
kernel	O
pca	O
involves	O
the	O
eigenvector	O
expansion	O
of	O
the	O
n	O
x	O
n	O
matrix	O
k.	O
12.3.	O
kernel	O
pca	O
589	O
so	O
far	O
we	O
have	O
assumed	O
that	O
the	O
projected	O
data	O
set	O
given	O
by	O
¢	O
(	O
xn	O
)	O
has	O
zero	O
mean	B
,	O
which	O
in	O
general	O
will	O
not	O
be	O
the	O
case	O
.	O
we	O
can	O
not	O
simply	O
compute	O
and	O
then	O
subtract	O
off	O
the	O
mean	B
,	O
since	O
we	O
wish	O
to	O
avoid	O
working	O
directly	O
in	O
feature	B
space	I
,	O
and	O
so	O
again	O
,	O
we	O
formulate	O
the	O
algorithm	O
purely	O
in-	O
!	O
erms	O
of	O
the	O
kernel	B
function	I
.	O
the	O
projected	O
data	O
points	O
after	O
centralizing	O
,	O
denoted	O
¢	O
(	O
xn	O
)	O
,	O
are	O
given	O
by	O
and	O
the	O
corresponding	O
elements	O
of	O
the	O
gram	O
matrix	O
are	O
given	O
by	O
k	O
nm	O
=	O
¢	O
(	O
xn	O
)	O
t¢	O
(	O
xm	O
)	O
¢	O
(	O
xn	O
)	O
t¢	O
(	O
xm	O
)	O
1	O
n	O
-	O
n	O
l	O
z=l	O
¢	O
(	O
xn	O
)	O
t¢	O
(	O
xz	O
)	O
1	O
n	O
1	O
n	O
n	O
-	O
n	O
l¢	O
(	O
xz	O
)	O
t¢	O
(	O
xm	O
)	O
+	O
n2	O
ll¢	O
(	O
xj	O
)	O
t¢	O
(	O
xz	O
)	O
z=l	O
j=l	O
z=l	O
1	O
n	O
k	O
(	O
xn	O
,	O
x	O
m	O
)	O
-	O
n	O
l	O
k	O
(	O
xz	O
,	O
x	O
m	O
)	O
z=l	O
-	O
n	O
lk	O
(	O
xn	O
,	O
xz	O
)	O
+	O
n2	O
llk	O
(	O
xj	O
,	O
xl	O
)	O
'	O
1	O
n	O
n	O
j=l	O
1=1	O
1	O
n	O
z=l	O
this	O
can	O
be	O
expressed	O
in	O
matrix	O
notation	O
as	O
(	O
12.83	O
)	O
(	O
12.84	O
)	O
(	O
12.85	O
)	O
exercise	O
12.27	O
~	O
where	O
in	O
denotes	O
the	O
n	O
x	O
n	O
matrix	O
in	O
which	O
every	O
element	O
takes	O
the	O
value	O
l/n	O
.	O
thus	O
we	O
can	O
evaluate	O
k	O
using	O
only	O
the	O
kernel	B
function	I
and	O
then	O
use	O
k	O
to	O
determine	O
the	O
eigenvalues	O
and	O
eigenvectors	O
.	O
note	O
that	O
the	O
standard	O
pca	O
algorithm	O
is	O
recovered	O
as	O
a	O
special	O
case	O
if	O
we	O
use	O
a	O
linear	O
kernel	O
k	O
(	O
x	O
,	O
x	O
'	O
)	O
=	O
xtx/	O
.	O
figure	O
12.17	O
shows	O
an	O
example	O
of	O
kernel	O
pca	O
applied	O
to	O
a	O
synthetic	O
data	O
set	O
(	O
scholkopf	O
et	O
al.	O
,	O
1998	O
)	O
.	O
here	O
a	O
'gaussian	O
'	O
kernel	O
of	O
the	O
form	O
~	O
k	O
(	O
x	O
,	O
x	O
'	O
)	O
=	O
exp	O
(	O
-llx	O
-	O
x/11	O
2/0.1	O
)	O
(	O
12.86	O
)	O
is	O
applied	O
to	O
a	O
synthetic	O
data	O
set	O
.	O
the	O
lines	O
correspond	O
to	O
contours	O
along	O
which	O
the	O
projection	O
onto	O
the	O
corresponding	O
principal	O
component	O
,	O
defined	O
by	O
n	O
¢	O
(	O
x	O
?	O
vi	O
=	O
l	O
n=l	O
aink	O
(	O
x	O
,	O
x	O
n	O
)	O
(	O
12.87	O
)	O
is	O
constant	O
.	O
590	O
12.	O
continuous	O
latent	O
valuables	O
_.	O
figure	O
12.11	O
e	O
''	O
llmple	O
01	O
kernel	O
pca	O
,	O
with	O
a	O
gaussian	O
kernel	O
awiioo	O
10	O
a	O
synthetic	O
<	O
lata	O
sat	O
in	O
two	O
<	O
:	O
iirnensions	O
,	O
showing	O
!	O
he	O
firsl	O
flight	O
eigenfunclions	O
along	O
w~h	O
l	O
!	O
>	O
eir	O
e9tnvailnls	O
.	O
the	O
oootours	O
am	O
lines	O
along	O
which	O
!	O
he	O
projoc1ion	O
onlo	O
t	O
''	O
''	O
coffaspmding	O
principal	O
componen1ls	O
co	O
<	O
>	O
stam	O
,	O
nola	O
haw	O
ihe	O
firsl	O
two~	O
...	O
.	O
,	O
..rat	O
.	O
!	O
he	O
th	O
''	O
'	O
''	O
dusters	O
.	O
!	O
he	O
``	O
''	O
'	O
''	O
ill	O
''	O
'	O
''	O
~	O
spiii	O
``	O
''	O
'*	O
'	O
oilhe	O
eluste	O
,	O
into	O
hamos	O
.	O
and	O
t	O
''	O
''	O
loliowing	O
ihree	O
~	O
again	O
spi~	O
!	O
he	O
duste	O
,	O
''	O
into	O
halves	O
along	O
directions	O
orthogonal	O
10	O
tho	O
premous	O
splils	O
,	O
one	O
obvioo	O
'	O
dls.ajmota~eof	O
i	O
:	O
emel	O
!	O
'ca	O
is	O
thaf	O
if	O
invoh'es	O
finding	O
lhe	O
elgen	O
''	O
e	O
<	O
(	O
cid:173	O
)	O
tors	O
of	O
the	O
n	O
x	O
n	O
malri	O
>	O
:	O
k	O
raw	O
.	O
ihan	O
lhe	O
d	O
x	O
d	O
malri	O
,	O
s	O
of	O
cor	O
,	O
..emionallinear	O
!	O
'ca	O
.	O
and	O
!	O
io	O
in	O
prac1lce	O
for	O
large	O
data	O
``	O
'1	O
'	O
appro	O
,	O
lmation	O
<	O
are	O
often	O
us	O
(	O
:	O
d	O
finally	O
.	O
``	O
``	O
e	O
ooie	O
that	O
i	O
''	O
<	O
tandard	O
linear	O
i'ca	O
,	O
we	O
often	O
retain	O
some	O
redoce	O
<	O
l	O
num·	O
ber	O
l	O
<	O
dof	O
eigenvectors	O
and	O
then	O
appro	O
,	O
lmale	O
0	O
data	O
vttl	O
<	O
:	O
>	O
r	O
xn	O
b	O
}	O
'	O
its	O
projection	O
i~	O
0	O
,	O
,1	O
''	O
lhe	O
l-dimensional	O
principal	B
subspace	I
,	O
defined	O
by	O
,	O
i~-l	O
:	O
«	O
``	O
,	O
)	O
''	O
''	O
(	O
12.88	O
)	O
i	O
''	O
kernell'ca	O
.	O
this	O
will	O
in	O
gencr~1	O
not	O
be	O
flo'slble	O
,	O
to	O
see	O
thl	O
'	O
,	O
ooie	O
ihat	O
the	O
map	O
(	O
cid:173	O
)	O
ping	O
4	O
'	O
(	O
x	O
)	O
maps	O
the	O
d-dimensional	O
x	O
space	O
i	O
''	O
t	O
''	O
0	O
d-dimensioo.l	O
manijqiii	O
in	O
lhe	O
m-dimemioo.l	O
femure	O
space	O
<	O
1	O
>	O
.	O
tlie	O
:	O
.	O
'ector	O
x	O
i	O
'	O
koown	O
a	O
<	O
lhe	O
f	O
'	O
''	O
,	O
.imagr	O
of	O
lhe	O
c	O
''	O
,	O
''	O
''	O
''	O
ponding	O
poi	O
''	O
l	O
4	O
'	O
(	O
x	O
)	O
.	O
however	O
,	O
fhe	O
projec1ioo	O
of	O
poinl	O
>	O
in	O
feature	O
<	O
j'3c	O
''	O
``	O
''	O
to	O
the	O
linear	O
rca	O
,	O
ub	O
,	O
p	O
''	O
''	O
''	O
in	O
that	O
'pace	O
will	O
typically	O
''	O
'	O
''	O
lie	O
on	O
fhe	O
nonlinear	O
d	O
(	O
cid:173	O
)	O
dimensional	O
manifold	B
and	O
!	O
io	O
will	O
nul	O
ha.	O
,	O
.	O
a	O
c	O
''	O
''	O
''	O
''	O
,	O
pondlng	O
p	O
''	O
,	O
.lmo~ein	O
dolo	O
spa	O
<	O
.	O
``	O
c	O
,	O
technlque	O
<	O
ho.-e	O
lherefore	O
bttn	O
proposed	O
for	O
finding	O
approximale	O
pre-image	O
<	O
ib	O
''	O
''	O
lr	O
nat..	O
2	O
(	O
04	O
)	O
.	O
12.4.	O
nonlinear	O
latent	B
variable	I
models	O
591	O
12.4.	O
nonlinear	O
latent	B
variable	I
models	O
in	O
this	O
chapter	O
,	O
we	O
have	O
focussed	O
on	O
the	O
simplest	O
class	O
of	O
models	O
having	O
continuous	O
latent	O
variables	O
,	O
namely	O
those	O
based	O
on	O
linear-gaussian	O
distributions	O
.	O
as	O
well	O
as	O
having	O
great	O
practical	O
importance	O
,	O
these	O
models	O
are	O
relatively	O
easy	O
to	O
analyse	O
and	O
to	O
fit	O
to	O
data	O
and	O
can	O
also	O
be	O
used	O
as	O
components	O
in	O
more	O
complex	O
models	O
.	O
here	O
we	O
consider	O
briefly	O
some	O
generalizations	O
of	O
this	O
framework	O
to	O
models	O
that	O
are	O
either	O
nonlinear	O
or	O
non-gaussian	O
,	O
or	O
both	O
.	O
in	O
fact	O
,	O
the	O
issues	O
of	O
nonlinearity	O
and	O
non-gaussianity	O
are	O
related	O
because	O
a	O
general	O
probability	B
density	O
can	O
be	O
obtained	O
from	O
a	O
simple	O
fixed	O
reference	O
density	B
,	O
such	O
as	O
a	O
gaussian	O
,	O
by	O
making	O
a	O
nonlinear	O
change	O
of	O
variables	O
.	O
this	O
idea	O
forms	O
the	O
basis	O
of	O
several	O
practical	O
latent	B
variable	I
models	O
as	O
we	O
shall	O
see	O
shortly	O
.	O
exercise	O
12.28	O
independent	B
component	I
analysis	I
12.4.1	O
we	O
begin	O
by	O
considering	O
models	O
in	O
which	O
the	O
observed	O
variables	O
are	O
related	O
linearly	O
to	O
the	O
latent	O
variables	O
,	O
but	O
for	O
which	O
the	O
latent	O
distribution	O
is	O
non-gaussian	O
.	O
an	O
important	O
class	O
of	O
such	O
models	O
,	O
known	O
as	O
independent	B
component	I
analysis	I
,	O
or	O
lea	O
,	O
arises	O
when	O
we	O
consider	O
a	O
distribution	O
over	O
the	O
latent	O
variables	O
that	O
factorizes	O
,	O
so	O
that	O
m	O
p	O
(	O
z	O
)	O
=	O
iip	O
(	O
zj	O
)	O
.	O
j=l	O
(	O
12.89	O
)	O
to	O
understand	O
the	O
role	O
of	O
such	O
models	O
,	O
consider	O
a	O
situation	O
in	O
which	O
two	O
people	O
are	O
talking	O
at	O
the	O
same	O
time	O
,	O
and	O
we	O
record	O
their	O
voices	O
using	O
two	O
microphones	O
.	O
if	O
we	O
ignore	O
effects	O
such	O
as	O
time	O
delay	O
and	O
echoes	O
,	O
then	O
the	O
signals	O
received	O
by	O
the	O
microphones	O
at	O
any	O
point	O
in	O
time	O
will	O
be	O
given	O
by	O
linear	O
combinations	O
of	O
the	O
amplitudes	O
of	O
the	O
two	O
voices	O
.	O
the	O
coefficients	O
of	O
this	O
linear	O
combination	O
will	O
be	O
constant	O
,	O
and	O
if	O
we	O
can	O
infer	O
their	O
values	O
from	O
sample	O
data	O
,	O
then	O
we	O
can	O
invert	O
the	O
mixing	O
process	O
(	O
assuming	O
it	O
is	O
nonsingular	O
)	O
and	O
thereby	O
obtain	O
two	O
clean	O
signals	O
each	O
of	O
which	O
contains	O
the	O
voice	O
of	O
just	O
one	O
person	O
.	O
this	O
is	O
an	O
example	O
of	O
a	O
problem	O
called	O
blind	B
source	I
separation	I
in	O
which	O
'blind	O
'	O
refers	O
to	O
the	O
fact	O
that	O
we	O
are	O
given	O
only	O
the	O
mixed	O
data	O
,	O
and	O
neither	O
the	O
original	O
sources	O
nor	O
the	O
mixing	O
coefficients	O
are	O
observed	O
(	O
cardoso	O
,	O
1998	O
)	O
.	O
this	O
type	O
of	O
problem	O
is	O
sometimes	O
addressed	O
using	O
the	O
following	O
approach	O
(	O
mackay	O
,	O
2003	O
)	O
in	O
which	O
we	O
ignore	O
the	O
temporal	O
nature	O
of	O
the	O
signals	O
and	O
treat	O
the	O
successive	O
samples	O
as	O
i.i.d	O
.	O
we	O
consider	O
a	O
generative	B
model	I
in	O
which	O
there	O
are	O
two	O
latent	O
variables	O
corresponding	O
to	O
the	O
unobserved	O
speech	O
signal	O
amplitudes	O
,	O
and	O
there	O
are	O
two	O
observed	O
variables	O
given	O
by	O
the	O
signal	O
values	O
at	O
the	O
microphones	O
.	O
the	O
latent	O
variables	O
have	O
a	O
joint	O
distribution	O
that	O
factorizes	O
as	O
above	O
,	O
and	O
the	O
observed	O
variables	O
are	O
given	O
by	O
a	O
linear	O
combination	O
of	O
the	O
latent	O
variables	O
.	O
there	O
is	O
no	O
need	O
to	O
include	O
a	O
noise	O
distribution	O
because	O
the	O
number	O
of	O
latent	O
variables	O
equals	O
the	O
number	O
of	O
ob	O
(	O
cid:173	O
)	O
served	O
variables	O
,	O
and	O
therefore	O
the	O
marginal	B
distribution	O
of	O
the	O
observed	O
variables	O
will	O
not	O
in	O
general	O
be	O
singular	O
,	O
so	O
the	O
observed	O
variables	O
are	O
simply	O
deterministic	O
linear	O
combinations	O
of	O
the	O
latent	O
variables	O
.	O
given	O
a	O
data	O
set	O
of	O
observations	O
,	O
the	O
592	O
12.	O
continuous	O
latent	O
variables	O
likelihood	B
function	I
for	O
this	O
model	O
is	O
a	O
function	O
of	O
the	O
coefficients	O
in	O
the	O
linear	O
com	O
(	O
cid:173	O
)	O
bination	O
.	O
the	O
log	O
likelihood	O
can	O
be	O
maximized	O
using	O
gradient-based	O
optimization	O
giving	O
rise	O
to	O
a	O
particular	O
version	O
of	O
independent	B
component	I
analysis	I
.	O
the	O
success	O
of	O
this	O
approach	O
requires	O
that	O
the	O
latent	O
variables	O
have	O
non-gaussian	O
distributions	O
.	O
to	O
see	O
this	O
,	O
recall	O
that	O
in	O
probabilistic	O
pca	O
(	O
and	O
in	O
factor	B
analysis	I
)	O
the	O
latent-space	O
distribution	O
is	O
given	O
by	O
a	O
zero-mean	O
isotropic	B
gaussian	O
.	O
the	O
model	O
therefore	O
can	O
not	O
distinguish	O
between	O
two	O
different	O
choices	O
for	O
the	O
latent	O
variables	O
where	O
these	O
differ	O
simply	O
by	O
a	O
rotation	O
in	O
latent	O
space	O
.	O
this	O
can	O
be	O
verified	O
directly	O
by	O
noting	O
that	O
the	O
marginal	B
density	O
(	O
12.35	O
)	O
,	O
and	O
hence	O
the	O
likelihood	B
function	I
,	O
is	O
unchanged	O
if	O
we	O
make	O
the	O
transformation	O
w	O
-	O
)	O
wr	O
where	O
r	O
is	O
an	O
orthogonal	O
matrix	O
satisfying	O
rrt	O
=	O
i	O
,	O
because	O
the	O
matrix	O
c	O
given	O
by	O
(	O
12.36	O
)	O
is	O
itself	O
invariant	O
.	O
extending	O
the	O
model	O
to	O
allow	O
more	O
general	O
gaussian	O
latent	O
distributions	O
does	O
not	O
change	O
this	O
conclusion	O
because	O
,	O
as	O
we	O
have	O
seen	O
,	O
such	O
a	O
model	O
is	O
equivalent	O
to	O
the	O
zero-mean	O
isotropic	B
gaussian	O
latent	B
variable	I
model	O
.	O
another	O
way	O
to	O
see	O
why	O
a	O
gaussian	O
latent	B
variable	I
distribution	O
in	O
a	O
linear	O
model	O
is	O
insufficient	O
to	O
find	O
independent	B
components	O
is	O
to	O
note	O
that	O
the	O
principal	O
compo	O
(	O
cid:173	O
)	O
nents	O
represent	O
a	O
rotation	O
of	O
the	O
coordinate	O
system	O
in	O
data	O
space	O
such	O
as	O
to	O
diagonal	B
(	O
cid:173	O
)	O
ize	O
the	O
covariance	B
matrix	I
,	O
so	O
that	O
the	O
data	O
distribution	O
in	O
the	O
new	O
coordinates	O
is	O
then	O
uncorrelated	O
.	O
although	O
zero	O
correlation	O
is	O
a	O
necessary	O
condition	O
for	O
independence	O
it	O
is	O
not	O
,	O
however	O
,	O
sufficient	O
.	O
in	O
practice	O
,	O
a	O
common	O
choice	O
for	O
the	O
latent-variable	O
distribution	O
is	O
given	O
by	O
exercise	O
12.29	O
1	O
p	O
(	O
z	O
)	O
=	O
--	O
,	O
.	O
--	O
--	O
-	O
,	O
-	O
7fcosh	O
(	O
zj	O
)	O
j	O
1	O
(	O
12.90	O
)	O
which	O
has	O
heavy	O
tails	O
compared	O
to	O
a	O
gaussian	O
,	O
reflecting	O
the	O
observation	O
that	O
many	O
real-world	O
distributions	O
also	O
exhibit	O
this	O
property	O
.	O
the	O
original	O
ica	O
model	O
(	O
bell	O
and	O
sejnowski	O
,	O
1995	O
)	O
was	O
based	O
on	O
the	O
optimiza	O
(	O
cid:173	O
)	O
tion	O
of	O
an	O
objective	O
function	O
defined	O
by	O
information	O
maximization	O
.	O
one	O
advantage	O
of	O
a	O
probabilistic	O
latent	O
variable	O
formulation	O
is	O
that	O
it	O
helps	O
to	O
motivate	O
and	O
formu	O
(	O
cid:173	O
)	O
late	O
generalizations	O
of	O
basic	O
ica	O
.	O
for	O
instance	O
,	O
independent	B
factor	I
analysis	I
(	O
attias	O
,	O
1999a	O
)	O
considers	O
a	O
model	O
in	O
which	O
the	O
number	O
of	O
latent	O
and	O
observed	O
variables	O
can	O
differ	O
,	O
the	O
observed	O
variables	O
are	O
noisy	O
,	O
and	O
the	O
individual	O
latent	O
variables	O
have	O
flex	O
(	O
cid:173	O
)	O
ible	O
distributions	O
modelled	O
by	O
mixtures	O
of	O
gaussians	O
.	O
the	O
log	O
likelihood	O
for	O
this	O
model	O
is	O
maximized	O
using	O
em	O
,	O
and	O
the	O
reconstruction	O
of	O
the	O
latent	O
variables	O
is	O
ap	O
(	O
cid:173	O
)	O
proximated	O
using	O
a	O
variational	B
approach	O
.	O
many	O
other	O
types	O
of	O
model	O
have	O
been	O
considered	O
,	O
and	O
there	O
is	O
now	O
a	O
huge	O
literature	O
on	O
ica	O
and	O
its	O
applications	O
(	O
jutten	O
and	O
herault	O
,	O
1991	O
;	O
comon	O
et	O
at.	O
,	O
1991	O
;	O
amari	O
et	O
at.	O
,	O
1996	O
;	O
pearlmutter	O
and	O
parra	O
,	O
1997	O
;	O
hyvarinen	O
and	O
oja	O
,	O
1997	O
;	O
hinton	O
et	O
at.	O
,	O
2001	O
;	O
miskin	O
and	O
mackay	O
,	O
2001	O
;	O
hojen-sorensen	O
et	O
at.	O
,	O
2002	O
;	O
choudrey	O
and	O
roberts	O
,	O
2003	O
;	O
chan	O
et	O
at.	O
,	O
2003	O
;	O
stone	O
,	O
2004	O
)	O
.	O
12.4.2	O
autoassociative	O
neural	O
networks	O
in	O
chapter	O
5	O
we	O
considered	O
neural	O
networks	O
in	O
the	O
context	O
of	O
supervised	O
learn	O
(	O
cid:173	O
)	O
ing	O
,	O
where	O
the	O
role	O
of	O
the	O
network	O
is	O
to	O
predict	O
the	O
output	O
variables	O
given	O
values	O
12.4.	O
nonlinear	O
latent	B
variable	I
models	O
593	O
figure	O
12.18	O
an	O
autoassociative	O
multilayer	O
perceptron	B
having	O
two	O
layers	O
of	O
weights	O
.	O
such	O
a	O
network	O
is	O
trained	O
to	O
map	O
input	O
vectors	O
onto	O
themselves	O
by	O
minimiza	O
(	O
cid:173	O
)	O
tion	O
ot	O
a	O
sum-ot-squares	O
error	B
.	O
even	O
with	O
non	O
(	O
cid:173	O
)	O
linear	O
units	O
in	O
the	O
hidden	O
layer	O
,	O
such	O
a	O
network	O
is	O
equivalent	O
to	O
linear	O
principal	O
component	O
anal	O
(	O
cid:173	O
)	O
ysis	O
.	O
links	O
representing	O
bias	B
parameters	O
have	O
been	O
omitted	O
for	O
clarity	O
.	O
inputs	O
outputs	O
for	O
the	O
input	O
variables	O
.	O
however	O
,	O
neural	O
networks	O
have	O
also	O
been	O
applied	O
to	O
un	O
(	O
cid:173	O
)	O
supervised	B
learning	I
where	O
they	O
have	O
been	O
used	O
for	O
dimensionality	O
reduction	O
.	O
this	O
is	O
achieved	O
by	O
using	O
a	O
network	O
having	O
the	O
same	O
number	O
of	O
outputs	O
as	O
inputs	O
,	O
and	O
optimizing	O
the	O
weights	O
so	O
as	O
to	O
minimize	O
some	O
measure	O
of	O
the	O
reconstruction	O
error	B
between	O
inputs	O
and	O
outputs	O
with	O
respect	O
to	O
a	O
set	O
of	O
training	B
data	O
.	O
consider	O
first	O
a	O
multilayer	B
perceptron	I
of	O
the	O
form	O
shown	O
in	O
figure	O
12.18	O
,	O
hav	O
(	O
cid:173	O
)	O
ing	O
d	O
inputs	O
,	O
d	O
output	O
units	O
and	O
m	O
hidden	O
units	O
,	O
with	O
m	O
<	O
d.	O
the	O
targets	O
used	O
to	O
train	O
the	O
network	O
are	O
simply	O
the	O
input	O
vectors	O
themselves	O
,	O
so	O
that	O
the	O
network	O
is	O
attempting	O
to	O
map	O
each	O
input	O
vector	O
onto	O
itself	O
.	O
such	O
a	O
network	O
is	O
said	O
to	O
form	O
an	O
autoassociative	O
mapping	O
.	O
since	O
the	O
number	O
of	O
hidden	O
units	O
is	O
smaller	O
than	O
the	O
number	O
of	O
inputs	O
,	O
a	O
perfect	O
reconstruction	O
of	O
all	O
input	O
vectors	O
is	O
not	O
in	O
general	O
pos	O
(	O
cid:173	O
)	O
sible	O
.	O
we	O
therefore	O
determine	O
the	O
network	O
parameters	O
w	O
by	O
minimizing	O
an	O
error	B
function	I
which	O
captures	O
the	O
degree	O
of	O
mismatch	O
between	O
the	O
input	O
vectors	O
and	O
their	O
reconstructions	O
.	O
in	O
particular	O
,	O
we	O
shall	O
choose	O
a	O
sum-of-squares	B
error	I
of	O
the	O
form	O
1	O
n	O
e	O
(	O
w	O
)	O
=	O
``	O
2	O
l	O
ily	O
(	O
xn	O
,	O
w	O
)	O
-	O
xn	O
11	O
2	O
•	O
n=l	O
(	O
12.91	O
)	O
if	O
the	O
hidden	O
units	O
have	O
linear	O
activations	O
functions	O
,	O
then	O
it	O
can	O
be	O
shown	O
that	O
the	O
error	B
function	I
has	O
a	O
unique	O
global	B
minimum	I
,	O
and	O
that	O
at	O
this	O
minimum	O
the	O
network	O
performs	O
a	O
projection	O
onto	O
the	O
m	O
-dimensional	O
subspace	O
which	O
is	O
spanned	O
by	O
the	O
first	O
m	O
principal	O
components	O
of	O
the	O
data	O
(	O
bourlard	O
and	O
kamp	O
,	O
1988	O
;	O
baldi	O
and	O
hornik	O
,	O
1989	O
)	O
.	O
thus	O
,	O
the	O
vectors	O
of	O
weights	O
which	O
lead	O
into	O
the	O
hidden	O
units	O
in	O
figure	O
12.18	O
form	O
a	O
basis	O
set	O
which	O
spans	O
the	O
principal	B
subspace	I
.	O
note	O
,	O
however	O
,	O
that	O
these	O
vec	O
(	O
cid:173	O
)	O
tors	O
need	O
not	O
be	O
orthogonal	O
or	O
normalized	O
.	O
this	O
result	O
is	O
unsurprising	O
,	O
since	O
both	O
principal	B
component	I
analysis	I
and	O
the	O
neural	B
network	I
are	O
using	O
linear	O
dimensionality	O
reduction	O
and	O
are	O
minimizing	O
the	O
same	O
sum-of-squares	B
error	I
function	O
.	O
it	O
might	O
be	O
thought	O
that	O
the	O
limitations	O
of	O
a	O
linear	O
dimensionality	O
reduction	O
could	O
be	O
overcome	O
by	O
using	O
nonlinear	O
(	O
sigmoidal	O
)	O
activation	O
functions	O
for	O
the	O
hidden	O
units	O
in	O
the	O
network	O
in	O
figure	O
12.18.	O
however	O
,	O
even	O
with	O
nonlinear	O
hidden	O
units	O
,	O
the	O
min	O
(	O
cid:173	O
)	O
imum	O
error	B
solution	O
is	O
again	O
given	O
by	O
the	O
projection	O
onto	O
the	O
principal	O
component	O
subspace	O
(	O
bourlard	O
and	O
kamp	O
,	O
1988	O
)	O
.	O
there	O
is	O
therefore	O
no	O
advantage	O
in	O
using	O
two	O
(	O
cid:173	O
)	O
layer	O
neural	O
networks	O
to	O
perform	O
dimensionality	O
reduction	O
.	O
standard	O
techniques	O
for	O
principal	O
component	O
analysis	O
(	O
based	O
on	O
singular	B
value	I
decomposition	I
)	O
are	O
guaran	O
(	O
cid:173	O
)	O
teed	O
to	O
give	O
the	O
correct	O
solution	O
in	O
finite	O
time	O
,	O
and	O
they	O
also	O
generate	O
an	O
ordered	O
set	O
of	O
eigenvalues	O
with	O
corresponding	O
orthonormal	O
eigenvectors	O
.	O
594	O
12.	O
continuous	O
latent	O
variables	O
figure	O
12.19	O
addition	O
of	O
extra	O
hidden	O
lay	O
(	O
cid:173	O
)	O
ers	O
of	O
noolinear	O
units	O
gives	O
an	O
auloassocialive	O
network	O
which	O
can	O
perform	O
a	O
noolinear	O
dimen	O
(	O
cid:173	O
)	O
siooality	O
reduction	O
.	O
f	O
,	O
•	O
f	O
,	O
•	O
inputs	O
x	O
,	O
outputs	O
x	O
,	O
the	O
situation	O
is	O
different	O
,	O
however	O
.	O
if	O
additional	O
hidden	O
layers	O
are	O
pcrmillcd	O
in	O
the	O
network	O
.	O
consider	O
the	O
four-layer	O
autoassociativc	O
network	O
shown	O
in	O
figure	O
12.19.	O
again	O
the	O
output	O
units	O
are	O
linear	O
,	O
and	O
the	O
m	O
units	O
in	O
the	O
second	O
hidden	O
layer	O
can	O
also	O
be	O
linear	O
.	O
however	O
,	O
the	O
first	O
and	O
third	O
hidden	O
layers	O
have	O
sigmoidal	O
nonlinear	O
activa	O
(	O
cid:173	O
)	O
tion	O
functions	O
.	O
the	O
network	O
is	O
again	O
trained	O
by	O
minimization	O
of	O
the	O
error	B
function	I
(	O
12.91	O
)	O
.	O
we	O
can	O
view	O
this	O
network	O
as	O
two	O
successive	O
functional	B
mappings	O
f	O
]	O
and	O
f	O
2	O
,	O
as	O
indicated	O
in	O
figure	O
12.19.	O
the	O
first	O
mapping	O
f	O
]	O
projects	O
the	O
original	O
d	O
(	O
cid:173	O
)	O
dimensional	O
data	O
onto	O
an	O
ai-dimensional	O
subspace	O
s	O
defined	O
by	O
the	O
activations	O
of	O
the	O
units	O
in	O
the	O
second	O
hidden	O
layer	O
.	O
because	O
of	O
the	O
presence	O
of	O
the	O
first	O
hidden	O
layer	O
of	O
nonlinear	O
units	O
.	O
this	O
mapping	O
is	O
very	O
general	O
.	O
and	O
in	O
particular	O
is	O
not	O
restricted	O
to	O
being	O
linear	O
.	O
similarly	O
.	O
the	O
second	O
half	O
of	O
the	O
network	O
defines	O
an	O
arbitrary	O
functional	B
mapping	O
from	O
the	O
m	O
-dimensional	O
space	O
back	O
into	O
the	O
original	O
d-dimensional	O
input	O
space	O
.	O
this	O
has	O
a	O
simple	O
geometrical	O
interpretation	O
.	O
as	O
indicated	O
for	O
the	O
case	O
d	O
=	O
3	O
and	O
m	O
=	O
2	O
in	O
figure	O
12.20.	O
such	O
a	O
network	O
effectively	O
perfonns	O
a	O
nonlinear	O
principal	B
component	I
analysis	I
.	O
x3	O
''	O
f	O
,	O
•	O
x	O
,	O
''	O
figure	O
12.20	O
geometrical	O
interpretation	O
of	O
the	O
mappings	O
performed	O
by	O
the	O
network	O
in	O
figure	O
12.1	O
g	O
for	O
the	O
case	O
of	O
0	O
=	O
3	O
inputs	O
and	O
ai	O
=	O
2	O
units	O
in	O
the	O
middle	O
hidden	O
layer	O
.	O
the	O
function	O
f	O
,	O
maps	O
from	O
an	O
m-dimensional	O
space	O
s	O
into	O
a	O
d-dimensiooal	O
space	O
and	O
therefore	O
defines	O
the	O
way	O
in	O
which	O
the	O
space	O
s	O
is	O
embedded	O
within	O
the	O
original	O
x-space	O
.	O
since	O
the	O
mapping	O
f	O
,	O
can	O
be	O
r	O
''	O
i	O
(	O
)	O
(	O
llinear	O
,	O
the	O
embedding	O
01	O
s	O
can	O
be	O
nonplanar	O
,	O
as	O
indicated	O
in	O
the	O
figure	O
.	O
the	O
mapping	O
f.	O
then	O
defines	O
a	O
projectiorl	O
of	O
points	O
in	O
the	O
original	O
d-dimensional	O
space	O
into	O
the	O
m	O
-dimensional	O
subspace	O
s.	O
12.4.	O
nonlinear	O
latent	B
variable	I
models	O
595	O
it	O
has	O
the	O
advantage	O
of	O
not	O
being	O
limited	O
to	O
linear	O
transformations	O
,	O
although	O
it	O
con	O
(	O
cid:173	O
)	O
tains	O
standard	O
principal	O
component	O
analysis	O
as	O
a	O
special	O
case	O
.	O
however	O
,	O
training	B
the	O
network	O
now	O
involves	O
a	O
nonlinear	O
optimization	O
problem	O
,	O
since	O
the	O
error	B
function	I
(	O
12.91	O
)	O
is	O
no	O
longer	O
a	O
quadratic	O
function	O
of	O
the	O
network	O
parameters	O
.	O
computation	O
(	O
cid:173	O
)	O
ally	O
intensive	O
nonlinear	O
optimization	O
techniques	O
must	O
be	O
used	O
,	O
and	O
there	O
is	O
the	O
risk	O
of	O
finding	O
a	O
suboptimal	O
local	B
minimum	I
of	O
the	O
error	B
function	I
.	O
also	O
,	O
the	O
dimensionality	O
of	O
the	O
subspace	O
must	O
be	O
specified	O
before	O
training	B
the	O
network	O
.	O
12.4.3	O
modelling	O
nonlinear	O
manifolds	O
as	O
we	O
have	O
already	O
noted	O
,	O
many	O
natural	O
sources	O
of	O
data	O
correspond	O
to	O
low	O
(	O
cid:173	O
)	O
dimensional	O
,	O
possibly	O
noisy	O
,	O
nonlinear	O
manifolds	O
embedded	O
within	O
the	O
higher	O
di	O
(	O
cid:173	O
)	O
mensional	O
observed	O
data	O
space	O
.	O
capturing	O
this	O
property	O
explicitly	O
can	O
lead	O
to	O
im	O
(	O
cid:173	O
)	O
proved	O
density	B
modelling	O
compared	O
with	O
more	O
general	O
methods	O
.	O
here	O
we	O
consider	O
briefly	O
a	O
range	O
of	O
techniques	O
that	O
attempt	O
to	O
do	O
this	O
.	O
one	O
way	O
to	O
model	O
the	O
nonlinear	O
structure	O
is	O
through	O
a	O
combination	O
of	O
linear	O
models	O
,	O
so	O
that	O
we	O
make	O
a	O
piece-wise	O
linear	O
approximation	O
to	O
the	O
manifold	B
.	O
this	O
can	O
be	O
obtained	O
,	O
for	O
instance	O
,	O
by	O
using	O
a	O
clustering	B
technique	O
such	O
as	O
k	O
-means	O
based	O
on	O
euclidean	O
distance	O
to	O
partition	O
the	O
data	O
set	O
into	O
local	B
groups	O
with	O
standard	O
pca	O
ap	O
(	O
cid:173	O
)	O
plied	O
to	O
each	O
group	O
.	O
a	O
better	O
approach	O
is	O
to	O
use	O
the	O
reconstruction	O
error	B
for	O
cluster	O
assignment	O
(	O
kambhatla	O
and	O
leen	O
,	O
1997	O
;	O
hinton	O
et	O
al.	O
,	O
1997	O
)	O
as	O
then	O
a	O
common	O
cost	B
function	I
is	O
being	O
optimized	O
in	O
each	O
stage	O
.	O
however	O
,	O
these	O
approaches	O
still	O
suffer	O
from	O
limitations	O
due	O
to	O
the	O
absence	O
of	O
an	O
overall	O
density	B
model	O
.	O
by	O
using	O
prob	O
(	O
cid:173	O
)	O
abilistic	O
pca	O
it	O
is	O
straightforward	O
to	O
define	O
a	O
fully	O
probabilistic	O
model	O
simply	O
by	O
considering	O
a	O
mixture	B
distribution	I
in	O
which	O
the	O
components	O
are	O
probabilistic	O
pca	O
models	O
(	O
tipping	O
and	O
bishop	O
,	O
1999a	O
)	O
.	O
such	O
a	O
model	O
has	O
both	O
discrete	O
latent	O
vari	O
(	O
cid:173	O
)	O
ables	O
,	O
corresponding	O
to	O
the	O
discrete	O
mixture	B
,	O
as	O
well	O
as	O
continuous	O
latent	O
variables	O
,	O
and	O
the	O
likelihood	B
function	I
can	O
be	O
maximized	O
using	O
the	O
em	O
algorithm	O
.	O
a	O
fully	O
bayesian	O
treatment	O
,	O
based	O
on	O
variational	B
inference	I
(	O
bishop	O
and	O
winn	O
,	O
2000	O
)	O
,	O
allows	O
the	O
number	O
of	O
components	O
in	O
the	O
mixture	B
,	O
as	O
well	O
as	O
the	O
effective	O
dimensionalities	O
of	O
the	O
individual	O
models	O
,	O
to	O
be	O
inferred	O
from	O
the	O
data	O
.	O
there	O
are	O
many	O
variants	O
of	O
this	O
model	O
in	O
which	O
parameters	O
such	O
as	O
the	O
w	O
matrix	O
or	O
the	O
noise	O
variances	O
are	O
tied	O
across	O
components	O
in	O
the	O
mixture	B
,	O
or	O
in	O
which	O
the	O
isotropic	B
noise	O
distributions	O
are	O
replaced	O
by	O
diagonal	B
ones	O
,	O
giving	O
rise	O
to	O
a	O
mixture	O
of	O
factor	O
analysers	O
(	O
ghahramani	O
and	O
hinton	O
,	O
1996a	O
;	O
ghahramani	O
and	O
beal	O
,	O
2000	O
)	O
.	O
the	O
mixture	O
of	O
probabilistic	O
pca	O
models	O
can	O
also	O
be	O
extended	B
hierarchically	O
to	O
produce	O
an	O
interactive	O
data	O
visualiza	O
(	O
cid:173	O
)	O
tion	O
algorithm	O
(	O
bishop	O
and	O
tipping	O
,	O
1998	O
)	O
.	O
an	O
alternative	O
to	O
considering	O
a	O
mixture	O
of	O
linear	O
models	O
is	O
to	O
consider	O
a	O
single	O
nonlinear	O
model	O
.	O
recall	O
that	O
conventional	O
pca	O
finds	O
a	O
linear	O
subspace	O
that	O
passes	O
close	O
to	O
the	O
data	O
in	O
a	O
least-squares	O
sense	O
.	O
this	O
concept	O
can	O
be	O
extended	B
to	O
one	O
(	O
cid:173	O
)	O
dimensional	O
nonlinear	O
surfaces	O
in	O
the	O
form	O
of	O
principal	O
curves	O
(	O
hastie	O
and	O
stuetzle	O
,	O
1989	O
)	O
.	O
we	O
can	O
describe	O
a	O
curve	O
in	O
a	O
d-dimensional	O
data	O
space	O
using	O
a	O
vector-valued	O
function	O
f	O
(	O
)	O
.	O
)	O
,	O
which	O
is	O
a	O
vector	O
each	O
of	O
whose	O
elements	O
is	O
a	O
function	O
of	O
the	O
scalar	O
)	O
..	O
there	O
are	O
many	O
possible	O
ways	O
to	O
parameterize	O
the	O
curve	O
,	O
of	O
which	O
a	O
natural	O
choice	O
is	O
the	O
arc	B
length	O
along	O
the	O
curve	O
.	O
for	O
any	O
given	O
point	O
xin	O
data	O
space	O
,	O
we	O
can	O
find	O
the	O
point	O
on	O
the	O
curve	O
that	O
is	O
closest	O
in	O
euclidean	O
distance	O
.	O
we	O
denote	O
this	O
point	O
by	O
596	O
12.	O
continuous	O
latent	O
variables	O
>	O
..	O
=	O
gf	O
(	O
x	O
)	O
because	O
it	O
depends	O
on	O
the	O
particular	O
curve	O
f	O
(	O
>	O
''	O
)	O
.	O
for	O
a	O
continuous	O
data	O
density	O
p	O
(	O
x	O
)	O
,	O
a	O
principal	B
curve	I
is	O
defined	O
as	O
one	O
for	O
which	O
every	O
point	O
on	O
the	O
curve	O
is	O
the	O
mean	B
of	O
all	O
those	O
points	O
in	O
data	O
space	O
that	O
project	O
to	O
it	O
,	O
so	O
that	O
je	O
[	O
xlgf	O
(	O
x	O
)	O
=	O
>	O
..	O
]	O
=	O
f	O
(	O
>	O
''	O
)	O
.	O
(	O
12.92	O
)	O
for	O
a	O
given	O
continuous	O
density	B
,	O
there	O
can	O
be	O
many	O
principal	O
curves	O
.	O
in	O
practice	O
,	O
we	O
are	O
interested	O
in	O
finite	O
data	O
sets	O
,	O
and	O
we	O
also	O
wish	O
to	O
restrict	O
attention	O
to	O
smooth	O
curves	O
.	O
hastie	O
and	O
stuetzle	O
(	O
1989	O
)	O
propose	O
a	O
two-stage	O
iterative	O
procedure	O
for	O
find	O
(	O
cid:173	O
)	O
ing	O
such	O
principal	O
curves	O
,	O
somewhat	O
reminiscent	O
of	O
the	O
em	O
algorithm	O
for	O
pca	O
.	O
the	O
curve	O
is	O
initialized	O
using	O
the	O
first	O
principal	O
component	O
,	O
and	O
then	O
the	O
algorithm	O
alter	O
(	O
cid:173	O
)	O
nates	O
between	O
a	O
data	O
projection	O
step	O
and	O
curve	O
re-estimation	O
step	O
.	O
in	O
the	O
projection	O
step	O
,	O
each	O
data	O
point	O
is	O
assigned	O
to	O
a	O
value	O
of	O
>	O
..	O
corresponding	O
to	O
the	O
closest	O
point	O
on	O
the	O
curve	O
.	O
then	O
in	O
the	O
re-estimation	O
step	O
,	O
each	O
point	O
on	O
the	O
curve	O
is	O
given	O
by	O
a	O
weighted	O
average	O
of	O
those	O
points	O
that	O
project	O
to	O
nearby	O
points	O
on	O
the	O
curve	O
,	O
with	O
points	O
closest	O
on	O
the	O
curve	O
given	O
the	O
greatest	O
weight	O
.	O
in	O
the	O
case	O
where	O
the	O
subspace	O
is	O
constrained	O
to	O
be	O
linear	O
,	O
the	O
procedure	O
converges	O
to	O
the	O
first	O
principal	O
component	O
and	O
is	O
equivalent	O
to	O
the	O
power	B
method	I
for	O
finding	O
the	O
largest	O
eigenvector	O
of	O
the	O
co	O
(	O
cid:173	O
)	O
variance	B
matrix	O
.	O
principal	O
curves	O
can	O
be	O
generalized	B
to	O
multidimensional	O
manifolds	O
called	O
principal	O
surfaces	O
although	O
these	O
have	O
found	O
limited	O
use	O
due	O
to	O
the	O
difficulty	O
of	O
data	O
smoothing	O
in	O
higher	O
dimensions	O
even	O
for	O
two-dimensional	O
manifolds	O
.	O
pca	O
is	O
often	O
used	O
to	O
project	O
a	O
data	O
set	O
onto	O
a	O
lower-dimensional	O
space	O
,	O
for	O
ex	O
(	O
cid:173	O
)	O
ample	O
two	O
dimensional	O
,	O
for	O
the	O
purposes	O
of	O
visualization	B
.	O
another	O
linear	O
technique	O
with	O
a	O
similar	O
aim	O
is	O
multidimensional	B
scaling	I
,	O
or	O
mds	O
(	O
cox	O
and	O
cox	O
,	O
2000	O
)	O
.	O
it	O
finds	O
a	O
low-dimensional	O
projection	O
of	O
the	O
data	O
such	O
as	O
to	O
preserve	O
,	O
as	O
closely	O
as	O
possible	O
,	O
the	O
pairwise	O
distances	O
between	O
data	O
points	O
,	O
and	O
involves	O
finding	O
the	O
eigenvectors	O
of	O
the	O
distance	O
matrix	O
.	O
in	O
the	O
case	O
where	O
the	O
distances	O
are	O
euclidean	O
,	O
it	O
gives	O
equivalent	O
results	O
to	O
pca	O
.	O
the	O
mds	O
concept	O
can	O
be	O
extended	B
to	O
a	O
wide	O
variety	O
of	O
data	O
types	O
specified	O
in	O
terms	O
of	O
a	O
similarity	O
matrix	O
,	O
giving	O
nonmetric	O
mds	O
.	O
two	O
other	O
nonprobabilistic	O
methods	O
for	O
dimensionality	O
reduction	O
and	O
data	O
vi	O
(	O
cid:173	O
)	O
sualization	O
are	O
worthy	O
of	O
mention	O
.	O
locally	B
linear	I
embedding	I
,	O
or	O
lle	O
(	O
roweis	O
and	O
saul	O
,	O
2000	O
)	O
first	O
computes	O
the	O
set	O
of	O
coefficients	O
that	O
best	O
reconstructs	O
each	O
data	O
point	O
from	O
its	O
neighbours	O
.	O
these	O
coefficients	O
are	O
arranged	O
to	O
be	O
invariant	O
to	O
rota	O
(	O
cid:173	O
)	O
tions	O
,	O
translations	O
,	O
and	O
scalings	O
of	O
that	O
data	O
point	O
and	O
its	O
neighbours	O
,	O
and	O
hence	O
they	O
characterize	O
the	O
local	B
geometrical	O
properties	O
of	O
the	O
neighbourhood	O
.	O
lle	O
then	O
maps	O
the	O
high-dimensional	O
data	O
points	O
down	O
to	O
a	O
lower	O
dimensional	O
space	O
while	O
preserv	O
(	O
cid:173	O
)	O
if	O
the	O
local	B
neighbourhood	O
for	O
a	O
particular	O
ing	O
these	O
neighbourhood	O
coefficients	O
.	O
data	O
point	O
can	O
be	O
considered	O
linear	O
,	O
then	O
the	O
transformation	O
can	O
be	O
achieved	O
using	O
a	O
combination	O
of	O
translation	O
,	O
rotation	O
,	O
and	O
scaling	O
,	O
such	O
as	O
to	O
preserve	O
the	O
angles	O
formed	O
between	O
the	O
data	O
points	O
and	O
their	O
neighbours	O
.	O
because	O
the	O
weights	O
are	O
in	O
(	O
cid:173	O
)	O
variant	O
to	O
these	O
transformations	O
,	O
we	O
expect	O
the	O
same	O
weight	O
values	O
to	O
reconstruct	O
the	O
data	O
points	O
in	O
the	O
low-dimensional	O
space	O
as	O
in	O
the	O
high-dimensional	O
data	O
space	O
.	O
in	O
spite	O
of	O
the	O
nonlinearity	O
,	O
the	O
optimization	O
for	O
lle	O
does	O
not	O
exhibit	O
local	B
minima	O
.	O
in	O
isometric	O
feature	O
mapping	O
,	O
or	O
isomap	B
(	O
tenenbaum	O
et	O
ai.	O
,	O
2000	O
)	O
,	O
the	O
goal	O
is	O
to	O
project	O
the	O
data	O
to	O
a	O
lower-dimensional	O
space	O
using	O
mds	O
,	O
but	O
where	O
the	O
dissim	O
(	O
cid:173	O
)	O
ilarities	O
are	O
defined	O
in	O
terms	O
of	O
the	O
geodesic	O
distances	O
measured	O
along	O
the	O
mani-	O
12.4.	O
nonlinear	O
latent	B
variable	I
models	O
597	O
fold	O
.	O
for	O
instance	O
,	O
if	O
two	O
points	O
lie	O
on	O
a	O
circle	O
,	O
then	O
the	O
geodesic	O
is	O
the	O
arc-length	O
distance	O
measured	O
around	O
the	O
circumference	O
of	O
the	O
circle	O
not	O
the	O
straight	O
line	O
dis	O
(	O
cid:173	O
)	O
tance	O
measured	O
along	O
the	O
chord	O
connecting	O
them	O
.	O
the	O
algorithm	O
first	O
defines	O
the	O
neighbourhood	O
for	O
each	O
data	O
point	O
,	O
either	O
by	O
finding	O
the	O
k	O
nearest	O
neighbours	O
or	O
by	O
finding	O
all	O
points	O
within	O
a	O
sphere	O
of	O
radius	O
e.	O
a	O
graph	O
is	O
then	O
constructed	O
by	O
link	B
(	O
cid:173	O
)	O
ing	O
all	O
neighbouring	O
points	O
and	O
labelling	O
them	O
with	O
their	O
euclidean	O
distance	O
.	O
the	O
geodesic	B
distance	I
between	O
any	O
pair	O
of	O
points	O
is	O
then	O
approximated	O
by	O
the	O
sum	O
of	O
the	O
arc	B
lengths	O
along	O
the	O
shortest	O
path	O
connecting	O
them	O
(	O
which	O
itself	O
is	O
found	O
using	O
standard	O
algorithms	O
)	O
.	O
finally	O
,	O
metric	O
mds	O
is	O
applied	O
to	O
the	O
geodesic	B
distance	I
matrix	O
to	O
find	O
the	O
low-dimensional	O
projection	O
.	O
our	O
focus	O
in	O
this	O
chapter	O
has	O
been	O
on	O
models	O
for	O
which	O
the	O
observed	O
vari	O
(	O
cid:173	O
)	O
ables	O
are	O
continuous	O
.	O
we	O
can	O
also	O
consider	O
models	O
having	O
continuous	O
latent	O
vari	O
(	O
cid:173	O
)	O
ables	O
together	O
with	O
discrete	O
observed	O
variables	O
,	O
giving	O
rise	O
to	O
latent	O
trait	O
models	O
(	O
bartholomew	O
,	O
1987	O
)	O
.	O
in	O
this	O
case	O
,	O
the	O
marginalization	O
over	O
the	O
continuous	O
latent	O
variables	O
,	O
even	O
for	O
a	O
linear	O
relationship	O
between	O
latent	O
and	O
observed	O
variables	O
,	O
can	O
(	O
cid:173	O
)	O
not	O
be	O
performed	O
analytically	O
,	O
and	O
so	O
more	O
sophisticated	O
techniques	O
are	O
required	O
.	O
tipping	O
(	O
1999	O
)	O
uses	O
variational	B
inference	I
in	O
a	O
model	O
with	O
a	O
two-dimensional	O
latent	O
space	O
,	O
allowing	O
a	O
binary	O
data	O
set	O
to	O
be	O
visualized	O
analogously	O
to	O
the	O
use	O
of	O
pca	O
to	O
visualize	O
continuous	O
data	O
.	O
note	O
that	O
this	O
model	O
is	O
the	O
dual	O
of	O
the	O
bayesian	O
logistic	B
regression	I
problem	O
discussed	O
in	O
section	O
4.5.	O
in	O
the	O
case	O
of	O
logistic	B
regression	I
we	O
have	O
n	O
observations	O
of	O
the	O
feature	O
vector	O
<	O
l	O
>	O
n	O
which	O
are	O
parameterized	O
by	O
a	O
single	O
parameter	O
vector	O
w	O
,	O
whereas	O
in	O
the	O
latent	O
space	O
visualization	B
model	O
there	O
is	O
a	O
single	O
latent	O
space	O
variable	O
x	O
(	O
analogous	O
to	O
<	O
1	O
»	O
and	O
n	O
copies	O
of	O
the	O
latent	B
variable	I
w	O
n	O
.	O
a	O
generalization	B
of	O
probabilistic	O
latent	O
variable	O
models	O
to	O
general	O
exponential	B
family	I
distributions	O
is	O
described	O
in	O
collins	O
et	O
al	O
.	O
(	O
2002	O
)	O
.	O
we	O
have	O
already	O
noted	O
that	O
an	O
arbitrary	O
distribution	O
can	O
be	O
formed	O
by	O
taking	O
a	O
gaussian	O
random	O
variable	O
and	O
transforming	O
it	O
through	O
a	O
suitable	O
nonlinearity	O
.	O
this	O
is	O
exploited	O
in	O
a	O
general	O
latent	B
variable	I
model	O
called	O
a	O
density	B
network	I
(	O
mackay	O
,	O
1995	O
;	O
mackay	O
and	O
gibbs	O
,	O
1999	O
)	O
in	O
which	O
the	O
nonlinear	O
function	O
is	O
governed	O
by	O
a	O
multilayered	O
neural	B
network	I
.	O
if	O
the	O
network	O
has	O
enough	O
hidden	O
units	O
,	O
it	O
can	O
approx	O
(	O
cid:173	O
)	O
imate	O
a	O
given	O
nonlinear	O
function	O
to	O
any	O
desired	O
accuracy	O
.	O
the	O
downside	O
of	O
having	O
such	O
a	O
flexible	O
model	O
is	O
that	O
the	O
marginalization	O
over	O
the	O
latent	O
variables	O
,	O
required	O
in	O
order	O
to	O
obtain	O
the	O
likelihood	B
function	I
,	O
is	O
no	O
longer	O
analytically	O
tractable	O
.	O
instead	O
,	O
the	O
likelihood	O
is	O
approximated	O
using	O
monte	O
carlo	O
techniques	O
by	O
drawing	O
samples	O
from	O
the	O
gaussian	O
prior	B
.	O
the	O
marginalization	O
over	O
the	O
latent	O
variables	O
then	O
becomes	O
a	O
simple	O
sum	O
with	O
one	O
term	O
for	O
each	O
sample	O
.	O
however	O
,	O
because	O
a	O
large	O
number	O
of	O
sample	O
points	O
may	O
be	O
required	O
in	O
order	O
to	O
give	O
an	O
accurate	O
representation	O
of	O
the	O
marginal	B
,	O
this	O
procedure	O
can	O
be	O
computationally	O
costly	O
.	O
if	O
we	O
consider	O
more	O
restricted	O
forms	O
for	O
the	O
nonlinear	O
function	O
,	O
and	O
make	O
an	O
ap	O
(	O
cid:173	O
)	O
propriate	O
choice	O
of	O
the	O
latent	B
variable	I
distribution	O
,	O
then	O
we	O
can	O
construct	O
a	O
latent	O
vari	O
(	O
cid:173	O
)	O
able	O
model	O
that	O
is	O
both	O
nonlinear	O
and	O
efficient	O
to	O
train	O
.	O
the	O
generative	B
topographic	I
mapping	I
,	O
or	O
gtm	O
(	O
bishop	O
et	O
ai.	O
,	O
1996	O
;	O
bishop	O
et	O
ai.	O
,	O
1997a	O
;	O
bishop	O
et	O
ai.	O
,	O
1998b	O
)	O
uses	O
a	O
latent	O
distribution	O
that	O
is	O
defined	O
by	O
a	O
finite	O
regular	O
grid	O
of	O
delta	O
functions	O
over	O
the	O
(	O
typically	O
two-dimensional	O
)	O
latent	O
space	O
.	O
marginalization	O
over	O
the	O
latent	O
space	O
then	O
simply	O
involves	O
summing	O
over	O
the	O
contributions	O
from	O
each	O
of	O
the	O
grid	O
locations	O
.	O
chapter	O
5	O
chapter	O
jj	O
598	O
12.	O
continuous	O
latent	O
va	O
k1ahu	O
'	O
:	O
s	O
?	O
lot	O
ot	O
trle	O
oillkyw	O
<	O
:	O
lata	O
wllisualiz.ed	O
using	O
pea	O
on	O
the	O
left	O
and	O
gtm	O
on	O
itle	O
ngr	O
,	O
t	O
fof	O
tile	O
gtm	O
flliu.e	O
12.21	O
model	O
.	O
each	O
<	O
lata	O
poinils	O
plollfld	O
at	O
tile	O
mean	B
ot	O
its	O
posm'k	O
>	O
<	O
dislribution	O
in	O
..tent	O
s	O
;	O
>	O
ace	O
,	O
tile	O
``	O
''	O
,	O
''	O
ineanty	O
mlhe	O
gtm	O
1tlod8i._lha	O
sepamlion	O
betwoon	O
the	O
groups	O
of	O
data	O
points	O
to	O
be	O
...	O
..n	O
``	O
''	O
''	O
.	O
ckl.arfy	O
,	O
.•	O
.	O
'	O
''	O
ch	O
''	O
l'l~f	O
j	O
s~etioo	O
/.4	O
the	O
no	O
''	O
liotar	O
mapping	O
is	O
gi	O
,	O
'en	O
by	O
a	O
linear	B
regression	I
model	O
thai	O
allow	O
,	O
for	O
general	O
iio/llinearily	O
while	O
being	O
a	O
linear	O
fuoction	O
of	O
tile	O
adapli'-e	O
parameler	O
<	O
,	O
noie	O
thai	O
tilt	O
usual	O
limitation	O
of	O
linear	B
regression	I
models	O
arising	O
from	O
the	O
en	O
''	O
''	O
,	O
of	O
dimen	O
,	O
iooalily	O
does	O
1101	O
arise	O
in	O
the	O
contr~1	O
of	O
lhe	O
gt~i	O
si	O
''	O
''	O
'e	O
the	O
``	O
\3nifold	O
generall	O
)	O
'	O
ha	O
<	O
t	O
,	O
,'o	O
di	O
''	O
ltn·	O
sions	O
irrespecti'-e	O
of	O
the	O
dimensionality	O
of	O
the	O
data	O
space	O
,	O
a	O
coo	O
''	O
''	O
!	O
``	O
,	O
,nce	O
of	O
illese	O
11	O
''	O
0	O
cooices	O
is	O
that	O
the	O
likelihood	O
funclion	O
can	O
be	O
e~pressed	O
analytically	O
in	O
dosed	O
form	O
and	O
can	O
be	O
optimilc	O
<	O
.	O
!	O
efficiently	O
o	O
,	O
ing	O
the	O
em	O
algorithm_	O
the	O
resolting	O
gtm	O
model	O
his	O
a	O
lwo-dimensional	O
nonlinear	O
manifold	B
10	O
tile	O
dala	O
sel	O
.	O
and	O
by	O
e	O
''	O
alualing	O
the	O
posterior	O
distrilj	O
''	O
,	O
lion	O
(	O
wer	O
latent	O
space	O
for	O
the	O
data	O
poi	O
''	O
''	O
,	O
they	O
can	O
he	O
projectt	O
<	O
j	O
back	O
to	O
the	O
lalent	O
'ji'k	O
'	O
''	O
for	O
.	O
'isualilalion	O
purposes	O
,	O
figure	O
12,21	O
sl	O
''	O
''	O
''	O
s	O
a	O
comparison	O
of	O
the	O
oil	O
data..,1	O
``	O
isualired	O
wilh	O
lincar	O
pea	O
and	O
wilh	O
lhe	O
iio/lhnear	O
gt~i	O
,	O
tilt	O
gtm	O
can	O
be	O
seen	O
as	O
a	O
probabilistic	O
``	O
'rsion	O
of	O
an	O
earlier	O
nlod	O
<	O
l	O
callm	O
the	O
'	O
''	O
if	O
org	O
''	O
nidng	O
``	O
''	O
'p	O
.	O
or	O
som	O
(	O
kohonen	O
.	O
1982	O
:	O
kobonen	O
.	O
(	O
995	O
)	O
.	O
which	O
also	O
represents	O
a	O
iwo-dimensiooal	O
iio/llincar	O
manifoid	O
as	O
a	O
regular	O
array	O
of	O
disc	O
''	O
'le	O
points	O
.	O
the	O
som	O
i	O
'	O
somewhat	O
remin	O
;	O
''	O
''	O
'nt	O
of	O
the	O
k·trlcan	O
,	O
algorithm	O
in	O
that	O
data	O
points	O
are	O
a.	O
,	O
igr.ed	O
to	O
nearby	O
prolol	O
)	O
'j	O
>	O
c	O
'-eclon	O
thai	O
are	O
lhen	O
subsc	O
<	O
juenlly	O
updale	O
<	O
!	O
.	O
initially	O
.	O
lhe	O
proioi	O
)	O
'jl	O
(	O
's	O
are	O
distribuled	O
at	O
random	O
,	O
and	O
during	O
the	O
training	B
process	O
they	O
'selr	O
organize	O
'	O
so	O
as	O
to	O
apl'ro~imalea	O
smoolh	O
manifold	B
.	O
unlike	O
k	O
-mean	O
'	O
.	O
how'e	O
''	O
e..	O
the	O
som	O
is	O
tioi	O
optimizing	O
any	O
well.ddine	O
<	O
!	O
cost	B
function	I
(	O
erwin	O
..	O
al..	O
1992	O
)	O
making	O
''	O
difficult	O
to	O
s.	O
''	O
the	O
parameters	O
of	O
the	O
model	O
and	O
10	O
assess	O
con'-ergence	O
.	O
there	O
i	O
'	O
also	O
no	O
guarantee	O
that	O
the	O
'	O
''	O
,	O
if-	O
<	O
>	O
rganilalion	O
'	O
will	O
take	O
place	O
..	O
this	O
is	O
depen	O
''	O
''	O
nl	O
00	O
the	O
choice	O
of	O
appropriate	O
paranlttcr	O
``	O
aloc	O
'	O
f	O
,	O
,	O
,	O
any	O
particular	O
data	O
sel	O
.	O
by	O
oofitrast	O
,	O
gtm	O
optimize	O
,	O
the	O
log	O
likelihood	O
functioo	O
,	O
and	O
the	O
resolting	O
model	O
define	O
'	O
a	O
probabilily	O
den	O
,	O
ity	O
in	O
dma	O
,	O
pace	O
,	O
in	O
fael	O
il	O
corre	O
,	O
ponds	O
to	O
a	O
con	O
,	O
m	O
,	O
incd	O
mi	O
,	O
ture	O
of	O
gaussian	O
,	O
in	O
which	O
the	O
component	O
.	O
'	O
,	O
h.re	O
a	O
conlnlon	O
``	O
.riance.•nd	O
the	O
mean	B
,	O
are	O
con'trained	O
to	O
lie	O
on	O
a	O
'mooih	O
tw-o-diitlcn	O
,	O
iooal	O
n1anifold	O
.	O
this	O
proba-	O
section	O
6.4	O
exercises	O
appendix	O
e	O
exercises	O
599	O
bilistic	O
foundation	O
also	O
makes	O
it	O
very	O
straightforward	O
to	O
define	O
generalizations	O
of	O
gtm	O
(	O
bishop	O
et	O
al.	O
,	O
1998a	O
)	O
such	O
as	O
a	O
bayesian	O
treatment	O
,	O
dealing	O
with	O
missing	O
val-	O
ues	O
,	O
a	O
principled	O
extension	O
to	O
discrete	O
variables	O
,	O
the	O
use	O
of	O
gaussian	O
processes	O
to	O
define	O
the	O
manifold	B
,	O
or	O
a	O
hierarchical	B
gtm	O
model	O
(	O
tino	O
and	O
nabney	O
,	O
2002	O
)	O
.	O
because	O
the	O
manifold	B
in	O
gtm	O
is	O
defined	O
as	O
a	O
continuous	O
surface	O
,	O
not	O
just	O
at	O
the	O
prototype	O
vectors	O
as	O
in	O
the	O
som	O
,	O
it	O
is	O
possible	O
to	O
compute	O
the	O
magnification	O
factors	O
corresponding	O
to	O
the	O
local	B
expansions	O
and	O
compressions	O
of	O
the	O
manifold	B
needed	O
to	O
fit	O
the	O
data	O
set	O
(	O
bishop	O
et	O
al.	O
,	O
1997b	O
)	O
as	O
well	O
as	O
the	O
directional	O
curvatures	O
of	O
the	O
manifold	B
(	O
tino	O
et	O
al.	O
,	O
2001	O
)	O
.	O
these	O
can	O
be	O
visualized	O
along	O
with	O
the	O
projected	O
data	O
and	O
provide	O
additional	O
insight	O
into	O
the	O
model	O
.	O
12.1	O
(	O
*	O
*	O
)	O
lib	O
in	O
this	O
exercise	O
,	O
we	O
use	O
proof	O
by	O
induction	O
to	O
show	O
that	O
the	O
linear	O
projection	O
onto	O
an	O
m	O
-dimensional	O
subspace	O
that	O
maximizes	O
the	O
variance	B
of	O
the	O
pro	O
(	O
cid:173	O
)	O
jected	O
data	O
is	O
defined	O
by	O
the	O
m	O
eigenvectors	O
of	O
the	O
data	O
covariance	O
matrix	O
s	O
,	O
given	O
by	O
(	O
12.3	O
)	O
,	O
corresponding	O
to	O
the	O
m	O
largest	O
eigenvalues	O
.	O
in	O
section	O
12.1	O
,	O
this	O
result	O
was	O
proven	O
for	O
the	O
case	O
of	O
m	O
=	O
1.	O
now	O
suppose	O
the	O
result	O
holds	O
for	O
some	O
general	O
value	O
of	O
m	O
and	O
show	O
that	O
it	O
consequently	O
holds	O
for	O
dimensionality	O
m	O
+	O
1.	O
to	O
do	O
this	O
,	O
first	O
set	O
the	O
derivative	B
of	O
the	O
variance	B
of	O
the	O
projected	O
data	O
with	O
respect	O
to	O
a	O
vector	O
um+1	O
defining	O
the	O
new	O
direction	O
in	O
data	O
space	O
equal	O
to	O
zero	O
.	O
this	O
should	O
be	O
done	O
subject	O
to	O
the	O
constraints	O
that	O
um	O
+l	O
be	O
orthogonal	O
to	O
the	O
existing	O
vectors	O
u1	O
,	O
''	O
''	O
um	O
,	O
and	O
also	O
that	O
it	O
be	O
normalized	O
to	O
unit	O
length	O
.	O
use	O
lagrange	O
multipli-	O
ers	O
to	O
enforce	O
these	O
constraints	O
.	O
then	O
make	O
use	O
of	O
the	O
orthonormality	O
properties	O
of	O
the	O
vectors	O
u1	O
,	O
''	O
''	O
um	O
to	O
show	O
that	O
the	O
new	O
vector	O
um+1	O
is	O
an	O
eigenvector	O
of	O
s.	O
finally	O
,	O
show	O
that	O
the	O
variance	B
is	O
maximized	O
if	O
the	O
eigenvector	O
is	O
chosen	O
to	O
be	O
the	O
one	O
corresponding	O
to	O
eigenvector	O
am+1	O
where	O
the	O
eigenvalues	O
have	O
been	O
ordered	O
in	O
decreasing	O
value	O
.	O
12.2	O
(	O
**	O
)	O
show	O
that	O
the	O
minimum	O
value	O
of	O
the	O
pca	O
distortion	B
measure	I
j	O
given	O
by	O
(	O
12.15	O
)	O
with	O
respect	O
to	O
the	O
ui	O
,	O
subject	O
to	O
the	O
orthonormality	O
constraints	O
(	O
12.7	O
)	O
,	O
is	O
obtained	O
when	O
the	O
ui	O
are	O
eigenvectors	O
of	O
the	O
data	O
covariance	O
matrix	O
s.	O
to	O
do	O
this	O
,	O
introduce	O
a	O
matrix	O
h	O
of	O
lagrange	O
multipliers	O
,	O
one	O
for	O
each	O
constraint	O
,	O
so	O
that	O
the	O
modified	O
distortion	B
measure	I
,	O
in	O
matrix	O
notation	O
reads	O
]	O
=	O
tr	O
{	O
utsu	O
}	O
+	O
tr	O
{	O
h	O
(	O
i	O
-	O
utu	O
)	O
}	O
(	O
12.93	O
)	O
where	O
uis	O
a	O
m~trix	O
of	O
dimensio~d	O
x	O
(	O
d	O
-	O
m	O
)	O
whose	O
columns	O
are	O
gi	O
:	O
:	O
:	O
..en	O
b~	O
ui	O
.	O
now	O
minimize	O
j	O
with	O
respect	O
to	O
u	O
and	O
show	O
that	O
the	O
s~ution	O
satisfies	O
su	O
=	O
uh	O
.	O
clearly	O
,	O
one	O
possible	O
solution	O
is	O
that	O
the	O
columns	O
of	O
u	O
are	O
eigenvectors	O
of	O
s	O
,	O
in	O
which	O
case	O
h	O
is	O
a	O
diagonal	B
matrix	O
containing	O
the	O
corresponding	O
eigenvalues	O
.	O
to	O
obtain	O
the	O
general	O
solution	O
,	O
show	O
that	O
h	O
can	O
be	O
assumed	O
to	O
be	O
a	O
symmetr~	O
ma~ix	O
,	O
and	O
by	O
using	O
its	O
eigenvect£r	O
expansion	O
show	O
that	O
the	O
general	O
solution	O
to	O
su	O
=~uh	O
gives	O
the	O
same	O
value	O
for	O
j	O
as	O
the	O
specific	O
solution	O
in	O
which	O
the	O
columns	O
of	O
u	O
are	O
600	O
12.	O
continuous	O
latent	O
variables	O
the	O
eigenvectors	O
of	O
s.	O
because	O
these	O
solutions	O
are	O
all	O
equivalent	O
,	O
it	O
is	O
convenient	O
to	O
choose	O
the	O
eigenvector	O
solution	O
.	O
12.3	O
(	O
*	O
)	O
verify	O
that	O
the	O
eigenvectors	O
defined	O
by	O
(	O
12.30	O
)	O
are	O
normalized	O
to	O
unit	O
length	O
,	O
assuming	O
that	O
the	O
eigenvectors	O
vi	O
have	O
unit	O
length	O
.	O
12.4	O
(	O
*	O
)	O
imm	O
suppose	O
we	O
replace	O
the	O
zero-mean	O
,	O
unit-covariance	O
latent	O
space	O
distri	O
(	O
cid:173	O
)	O
bution	O
(	O
12.31	O
)	O
in	O
the	O
probabilistic	O
pca	O
model	O
by	O
a	O
general	O
gaussian	O
distribution	O
of	O
the	O
formn	O
(	O
zlm	O
,	O
~	O
)	O
.	O
by	O
redefining	O
the	O
parameters	O
of	O
the	O
model	O
,	O
show	O
that	O
this	O
leads	O
to	O
an	O
identical	O
model	O
for	O
the	O
marginal	B
distribution	O
p	O
(	O
x	O
)	O
over	O
the	O
observed	O
variables	O
for	O
any	O
valid	O
choice	O
of	O
m	O
and	O
~	O
.	O
12.5	O
(	O
*	O
*	O
)	O
let	O
x	O
be	O
a	O
d-dimensional	O
random	O
variable	O
having	O
a	O
gaussian	O
distribution	O
given	O
by	O
n	O
(	O
xijl	O
,	O
~	O
)	O
,	O
and	O
consider	O
the	O
m-dimensional	O
random	O
variable	O
given	O
by	O
y	O
=	O
ax	O
+	O
b	O
where	O
a	O
is	O
an	O
m	O
x	O
d	O
matrix	O
.	O
show	O
that	O
y	O
also	O
has	O
a	O
gaussian	O
distribution	O
,	O
and	O
find	O
expressions	O
for	O
its	O
mean	B
and	O
covariance	B
.	O
discuss	O
the	O
form	O
of	O
this	O
gaussian	O
distribution	O
for	O
m	O
<	O
d	O
,	O
for	O
m	O
=	O
d	O
,	O
and	O
for	O
m	O
>	O
d.	O
12.6	O
(	O
*	O
)	O
imm	O
draw	O
a	O
directed	B
probabilistic	O
graph	O
for	O
the	O
probabilistic	O
pca	O
model	O
described	O
in	O
section	O
12.2	O
in	O
which	O
the	O
components	O
of	O
the	O
observed	B
variable	I
x	O
are	O
shown	O
explicitly	O
as	O
separate	O
nodes	O
.	O
hence	O
verify	O
that	O
the	O
probabilistic	O
pca	O
model	O
has	O
the	O
same	O
independence	O
structure	O
as	O
the	O
naive	O
bayes	O
model	O
discussed	O
in	O
sec	O
(	O
cid:173	O
)	O
tion	O
8.2.2	O
.	O
12.7	O
(	O
*	O
*	O
)	O
by	O
making	O
use	O
of	O
the	O
results	O
(	O
2.270	O
)	O
and	O
(	O
2.271	O
)	O
for	O
the	O
mean	B
and	O
covariance	B
of	O
a	O
general	O
distribution	O
,	O
derive	O
the	O
result	O
(	O
12.35	O
)	O
for	O
the	O
marginal	B
distribution	O
p	O
(	O
x	O
)	O
in	O
the	O
probabilistic	O
pca	O
model	O
.	O
12.8	O
(	O
*	O
*	O
)	O
imm	O
by	O
making	O
use	O
of	O
the	O
result	O
(	O
2.116	O
)	O
,	O
show	O
that	O
the	O
posterior	O
distribution	O
p	O
(	O
zlx	O
)	O
for	O
the	O
probabilistic	O
pca	O
model	O
is	O
given	O
by	O
(	O
12.42	O
)	O
.	O
12.9	O
(	O
*	O
)	O
verify	O
that	O
maximizing	O
the	O
log	O
likelihood	O
(	O
12.43	O
)	O
for	O
the	O
probabilistic	O
pca	O
model	O
with	O
respect	O
to	O
the	O
parameter	O
jl	O
gives	O
the	O
result	O
jlml	O
=	O
x	O
where	O
x	O
is	O
the	O
mean	B
of	O
the	O
data	O
vectors	O
.	O
12.10	O
(	O
**	O
)	O
by	O
evaluating	O
the	O
second	O
derivatives	O
of	O
the	O
log	O
likelihood	O
function	O
(	O
12.43	O
)	O
for	O
the	O
probabilistic	O
pca	O
model	O
with	O
respect	O
to	O
the	O
parameter	O
jl	O
,	O
show	O
that	O
the	O
stationary	B
point	O
jlml	O
=	O
x	O
represents	O
the	O
unique	O
maximum	O
.	O
(	O
*	O
*	O
)	O
imm	O
show	O
that	O
in	O
the	O
limit	O
(	O
y2	O
-	O
.	O
0	O
,	O
the	O
posterior	O
mean	O
for	O
the	O
probabilistic	O
pca	O
model	O
becomes	O
an	O
orthogonal	O
projection	O
onto	O
the	O
principal	B
subspace	I
,	O
as	O
in	O
conventional	O
pca	O
.	O
12.11	O
12.12	O
(	O
*	O
*	O
)	O
for	O
(	O
y2	O
>	O
0	O
show	O
that	O
the	O
posterior	O
mean	O
in	O
the	O
probabilistic	O
pca	O
model	O
is	O
shifted	O
towards	O
the	O
origin	O
relative	B
to	O
the	O
orthogonal	O
projection	O
.	O
12.13	O
(	O
*	O
*	O
)	O
show	O
that	O
the	O
optimal	O
reconstruction	O
of	O
a	O
data	O
point	O
under	O
probabilistic	O
pca	O
,	O
according	O
to	O
the	O
least	O
squares	O
projection	O
cost	O
of	O
conventional	O
pca	O
,	O
is	O
given	O
by	O
(	O
12.94	O
)	O
exercises	O
601	O
12.14	O
(	O
*	O
)	O
the	O
number	O
of	O
independent	B
parameters	O
in	O
the	O
covariance	B
matrix	I
for	O
the	O
proba	O
(	O
cid:173	O
)	O
bilistic	O
pca	O
model	O
with	O
an	O
m	O
-dimensional	O
latent	O
space	O
and	O
a	O
d-dimensional	O
data	O
space	O
is	O
given	O
by	O
(	O
12.51	O
)	O
.	O
verify	O
that	O
in	O
the	O
case	O
of	O
m	O
=	O
d	O
-	O
1	O
,	O
the	O
number	O
of	O
independent	B
parameters	O
is	O
the	O
same	O
as	O
in	O
a	O
general	O
covariance	B
gaussian	O
,	O
whereas	O
for	O
m	O
=	O
°it	O
is	O
the	O
same	O
as	O
for	O
a	O
gaussian	O
with	O
an	O
isotropic	B
covariance	O
.	O
12.15	O
(	O
**	O
)	O
iiii	O
!	O
i	O
derive	O
the	O
m-step	O
equations	O
(	O
12.56	O
)	O
and	O
(	O
12.57	O
)	O
for	O
the	O
probabilistic	O
pca	O
model	O
by	O
maximization	O
of	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
function	O
given	O
by	O
(	O
12.53	O
)	O
.	O
12.16	O
(	O
*	O
*	O
*	O
)	O
in	O
figure	O
12.11	O
,	O
we	O
showed	O
an	O
application	O
of	O
probabilistic	O
pca	O
to	O
a	O
data	O
set	O
in	O
which	O
some	O
of	O
the	O
data	O
values	O
were	O
missing	B
at	I
random	I
.	O
derive	O
the	O
em	O
algorithm	O
for	O
maximizing	O
the	O
likelihood	B
function	I
for	O
the	O
probabilistic	O
pca	O
model	O
in	O
this	O
situ	O
(	O
cid:173	O
)	O
ation	O
.	O
note	O
that	O
the	O
{	O
zn	O
}	O
,	O
as	O
well	O
as	O
the	O
missing	B
data	I
values	O
that	O
are	O
components	O
of	O
the	O
vectors	O
{	O
x	O
n	O
}	O
,	O
are	O
now	O
latent	O
variables	O
.	O
show	O
that	O
in	O
the	O
special	O
case	O
in	O
which	O
all	O
of	O
the	O
data	O
values	O
are	O
observed	O
,	O
this	O
reduces	O
to	O
the	O
em	O
algorithm	O
for	O
probabilistic	O
pca	O
derived	O
in	O
section	O
12.2.2	O
.	O
12.17	O
(	O
**	O
)	O
iiii	O
!	O
i	O
let	O
w	O
be	O
a	O
d	O
x	O
m	O
matrix	O
whose	O
columns	O
define	O
a	O
linear	O
subspace	O
of	O
dimensionality	O
m	O
embedded	O
within	O
a	O
data	O
space	O
of	O
dimensionality	O
d	O
,	O
and	O
let	O
j1	O
be	O
a	O
d-dimensional	O
vector	O
.	O
given	O
a	O
data	O
set	O
{	O
x	O
n	O
}	O
where	O
n	O
=	O
1	O
,	O
...	O
,	O
n	O
,	O
we	O
can	O
approximate	O
the	O
data	O
points	O
using	O
a	O
linear	O
mapping	O
from	O
a	O
set	O
of	O
m	O
-dimensional	O
vectors	O
{	O
zn	O
}	O
,	O
so	O
that	O
xn	O
is	O
approximated	O
by	O
w	O
zn	O
+	O
j1	O
.	O
the	O
associated	O
sum-of	O
(	O
cid:173	O
)	O
squares	O
reconstruction	O
cost	O
is	O
given	O
by	O
n	O
j	O
=	O
l	O
ilxn	O
-	O
j1-	O
wzn	O
11	O
2	O
.	O
n=l	O
(	O
12.95	O
)	O
first	O
show	O
that	O
minimizing	O
j	O
with	O
respect	O
to	O
j1leads	O
to	O
an	O
analogous	O
expression	O
with	O
x	O
n	O
and	O
zn	O
replaced	O
by	O
zero-mean	O
variables	O
x	O
n	O
-	O
x	O
and	O
zn	O
-	O
z	O
,	O
respectively	O
,	O
where	O
x	O
and	O
z	O
denote	O
sample	O
means	O
.	O
then	O
show	O
that	O
minimizing	O
j	O
with	O
respect	O
to	O
zn	O
,	O
where	O
w	O
is	O
kept	O
fixed	O
,	O
gives	O
rise	O
to	O
the	O
pca	O
estep	O
(	O
12.58	O
)	O
,	O
and	O
that	O
minimizing	O
j	O
with	O
respect	O
to	O
w	O
,	O
where	O
{	O
zn	O
}	O
is	O
kept	O
fixed	O
,	O
gives	O
rise	O
to	O
the	O
pca	O
m	O
step	O
(	O
12.59	O
)	O
.	O
12.18	O
(	O
*	O
)	O
derive	O
an	O
expression	O
for	O
the	O
number	O
of	O
independent	B
parameters	O
in	O
the	O
factor	B
analysis	I
model	O
described	O
in	O
section	O
12.2.4	O
.	O
12.19	O
(	O
**	O
)	O
iiii	O
!	O
i	O
show	O
that	O
the	O
factor	B
analysis	I
model	O
described	O
in	O
section	O
12.2.4	O
is	O
invariant	O
under	O
rotations	O
of	O
the	O
latent	O
space	O
coordinates	O
.	O
12.20	O
(	O
**	O
)	O
by	O
considering	O
second	O
derivatives	O
,	O
show	O
that	O
the	O
only	O
stationary	B
point	O
of	O
the	O
log	O
likelihood	O
function	O
for	O
the	O
factor	B
analysis	I
model	O
discussed	O
in	O
section	O
12.2.4	O
with	O
respect	O
to	O
the	O
parameter	O
j1	O
is	O
given	O
by	O
the	O
sample	B
mean	I
defined	O
by	O
(	O
12.1	O
)	O
.	O
furthermore	O
,	O
show	O
that	O
this	O
stationary	B
point	O
is	O
a	O
maximum	O
.	O
12.21	O
(	O
**	O
)	O
derive	O
the	O
formulae	O
(	O
12.66	O
)	O
and	O
(	O
12.67	O
)	O
for	O
the	O
e	O
step	O
of	O
the	O
em	O
algorithm	O
for	O
factor	O
analysis	O
.	O
note	O
that	O
from	O
the	O
result	O
of	O
exercise	O
12.20	O
,	O
the	O
parameter	O
j1	O
can	O
be	O
replaced	O
by	O
the	O
sample	B
mean	I
x	O
.	O
602	O
12.	O
continuous	O
latent	O
variables	O
12.22	O
(	O
*	O
*	O
)	O
write	O
down	O
an	O
expression	O
for	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
func	O
(	O
cid:173	O
)	O
tion	O
for	O
the	O
factor	B
analysis	I
model	O
,	O
and	O
hence	O
derive	O
the	O
corresponding	O
m	O
step	O
equa	O
(	O
cid:173	O
)	O
tions	O
(	O
12.69	O
)	O
and	O
(	O
12.70	O
)	O
.	O
12.23	O
(	O
*	O
)	O
iii	O
!	O
i	O
draw	O
a	O
directed	B
probabilistic	O
graphical	B
model	I
representing	O
a	O
discrete	O
mixture	O
of	O
probabilistic	O
pca	O
models	O
in	O
which	O
each	O
pca	O
model	O
has	O
its	O
own	O
values	O
of	O
w	O
,	O
jl	O
,	O
and	O
0-	O
•	O
now	O
draw	O
a	O
modified	O
graph	O
in	O
which	O
these	O
parameter	O
values	O
are	O
shared	O
between	O
the	O
components	O
of	O
the	O
mixture	B
.	O
2	O
12.24	O
(	O
***	O
)	O
we	O
saw	O
in	O
section	O
2.3.7	O
that	O
student	O
's	O
t-distribution	O
can	O
be	O
viewed	O
as	O
an	O
infinite	O
mixture	O
of	O
gaussians	O
in	O
which	O
we	O
marginalize	O
with	O
respect	O
to	O
a	O
continu	O
(	O
cid:173	O
)	O
ous	O
latent	B
variable	I
.	O
by	O
exploiting	O
this	O
representation	O
,	O
formulate	O
an	O
em	O
algorithm	O
for	O
maximizing	O
the	O
log	O
likelihood	O
function	O
for	O
a	O
multivariate	O
student	O
's	O
t-distribution	O
given	O
an	O
observed	O
set	O
of	O
data	O
points	O
,	O
and	O
derive	O
the	O
forms	O
of	O
the	O
e	O
and	O
m	O
step	O
equa	O
(	O
cid:173	O
)	O
tions	O
.	O
12.25	O
(	O
**	O
)	O
iii	O
!	O
i	O
consider	O
a	O
linear-gaussian	O
latent-variable	O
model	O
having	O
a	O
latent	O
space	O
distribution	O
p	O
(	O
z	O
)	O
=	O
n	O
(	O
xio	O
,	O
i	O
)	O
and	O
a	O
conditional	B
distribution	O
for	O
the	O
observed	O
vari	O
(	O
cid:173	O
)	O
able	O
p	O
(	O
xlz	O
)	O
=	O
n	O
(	O
xlwz	O
+	O
il	O
,	O
<	O
p	O
)	O
where	O
<	O
p	O
is	O
an	O
arbitrary	O
symmetric	O
,	O
positive	O
(	O
cid:173	O
)	O
definite	O
noise	O
covariance	B
matrix	I
.	O
now	O
suppose	O
that	O
we	O
make	O
a	O
nonsingular	O
linear	O
transformation	O
of	O
the	O
data	O
variables	O
x	O
--	O
-t	O
ax	O
,	O
where	O
a	O
is	O
a	O
d	O
x	O
d	O
matrix	O
.	O
if	O
jlml	O
'	O
w	O
ml	O
and	O
<	O
pml	O
represent	O
the	O
maximum	B
likelihood	I
solution	O
corresponding	O
to	O
the	O
original	O
untransformed	O
data	O
,	O
show	O
that	O
ajlml	O
'	O
awml	O
,	O
and	O
a	O
<	O
pmlat	O
will	O
rep	O
(	O
cid:173	O
)	O
resent	O
the	O
corresponding	O
maximum	B
likelihood	I
solution	O
for	O
the	O
transformed	O
data	O
set	O
.	O
finally	O
,	O
show	O
that	O
the	O
form	O
of	O
the	O
model	O
is	O
preserved	O
in	O
two	O
cases	O
:	O
(	O
i	O
)	O
a	O
is	O
a	O
diagonal	B
matrix	O
and	O
<	O
p	O
is	O
a	O
diagonal	B
matrix	O
.	O
this	O
corresponds	O
to	O
the	O
case	O
of	O
factor	B
analysis	I
.	O
the	O
transformed	O
<	O
p	O
remains	O
diagonal	B
,	O
and	O
hence	O
factor	B
analysis	I
is	O
covariant	O
under	O
component-wise	O
re-scaling	O
of	O
the	O
data	O
variables	O
;	O
(	O
ii	O
)	O
a	O
is	O
orthogonal	O
and	O
<	O
p	O
is	O
pro	O
(	O
cid:173	O
)	O
21.	O
this	O
corresponds	O
to	O
probabilistic	O
pca	O
.	O
portional	O
to	O
the	O
unit	O
matrix	O
so	O
that	O
<	O
p	O
=	O
0-	O
the	O
transformed	O
<	O
p	O
matrix	O
remains	O
proportional	O
to	O
the	O
unit	O
matrix	O
,	O
and	O
hence	O
proba	O
(	O
cid:173	O
)	O
bilistic	O
pca	O
is	O
covariant	O
under	O
a	O
rotation	O
of	O
the	O
axes	O
of	O
data	O
space	O
,	O
as	O
is	O
the	O
case	O
for	O
conventional	O
pca	O
.	O
\	O
12.26	O
(	O
**	O
)	O
show	O
that	O
any	O
vector	O
ai	O
that	O
satisfies	O
(	O
12.80	O
)	O
will	O
also	O
satisfy	O
(	O
12.79	O
)	O
.	O
also	O
,	O
show	O
that	O
for	O
any	O
solution	O
of	O
(	O
12.80	O
)	O
having	O
eigenvalue	O
a	O
,	O
we	O
can	O
add	O
any	O
multiple	O
of	O
an	O
eigenvector	O
of	O
k	O
having	O
zero	O
eigenvalue	O
,	O
and	O
obtain	O
a	O
solution	O
to	O
(	O
12.79	O
)	O
that	O
also	O
has	O
eigenvalue	O
a.	O
finally	O
,	O
show	O
that	O
such	O
modifications	O
do	O
not	O
affect	O
the	O
principal-component	O
projection	O
given	O
by	O
(	O
12.82	O
)	O
.	O
12.27	O
(	O
*	O
*	O
)	O
show	O
that	O
the	O
conventional	O
linear	O
pca	O
algorithm	O
is	O
recovered	O
as	O
a	O
special	O
case	O
of	O
kernel	O
pca	O
if	O
we	O
choose	O
the	O
linear	O
kernel	O
function	O
given	O
by	O
k	O
(	O
x	O
,	O
x	O
'	O
)	O
=	O
x	O
t	O
x	O
'	O
.	O
12.28	O
(	O
*	O
*	O
)	O
iii	O
!	O
i	O
use	O
the	O
transformation	O
property	O
(	O
1.27	O
)	O
of	O
a	O
probability	B
density	O
under	O
a	O
change	O
of	O
variable	O
to	O
show	O
that	O
any	O
density	B
p	O
(	O
y	O
)	O
can	O
be	O
obtained	O
from	O
a	O
fixed	O
density	B
q	O
(	O
x	O
)	O
that	O
is	O
everywhere	O
nonzero	O
by	O
making	O
a	O
nonlinear	O
change	O
of	O
variable	O
y	O
=	O
f	O
(	O
x	O
)	O
in	O
which	O
f	O
(	O
x	O
)	O
is	O
a	O
monotonic	O
function	O
so	O
that	O
0	O
:	O
:	O
:	O
;	O
j	O
'	O
(	O
x	O
)	O
<	O
00.	O
write	O
down	O
the	O
differential	B
equation	O
satisfied	O
by	O
f	O
(	O
x	O
)	O
and	O
draw	O
a	O
diagram	O
illustrating	O
the	O
transformation	O
of	O
the	O
density	B
.	O
exercises	O
603	O
12.29	O
(	O
**	O
)	O
em	O
suppose	O
that	O
two	O
variables	O
zl	O
and	O
z2	O
are	O
independent	B
so	O
thatp	O
(	O
zl	O
'	O
z2	O
)	O
=	O
p	O
(	O
zl	O
)	O
p	O
(	O
z2	O
)	O
'	O
show	O
that	O
the	O
covariance	B
matrix	I
between	O
these	O
variables	O
is	O
diagonal	B
.	O
this	O
shows	O
that	O
independence	O
is	O
a	O
sufficient	O
condition	O
for	O
two	O
variables	O
to	O
be	O
un	O
(	O
cid:173	O
)	O
correlated	O
.	O
now	O
consider	O
two	O
variables	O
yl	O
and	O
y2	O
in	O
which	O
-1	O
:0	O
;	O
;	O
yl	O
:0	O
;	O
;	O
1	O
and	O
y2	O
=	O
yg	O
.	O
write	O
down	O
the	O
conditional	B
distribution	O
p	O
(	O
y2iyl	O
)	O
and	O
observe	O
that	O
this	O
is	O
dependent	O
on	O
yb	O
showing	O
that	O
the	O
two	O
variables	O
are	O
not	O
independent	B
.	O
now	O
show	O
that	O
the	O
covariance	B
matrix	I
between	O
these	O
two	O
variables	O
is	O
again	O
diagonal	B
.	O
to	O
do	O
this	O
,	O
use	O
the	O
relation	O
p	O
(	O
yl	O
,	O
y2	O
)	O
=	O
p	O
(	O
yi	O
)	O
p	O
(	O
y2iyl	O
)	O
to	O
show	O
that	O
the	O
off-diagonal	O
terms	O
are	O
zero	O
.	O
this	O
counter-example	O
shows	O
that	O
zero	O
correlation	O
is	O
not	O
a	O
sufficient	O
condition	O
for	O
independence	O
.	O
13	O
sequential	B
data	I
so	O
far	O
in	O
this	O
book	O
,	O
we	O
have	O
focussed	O
primarily	O
on	O
sets	O
of	O
data	O
points	O
that	O
were	O
as-	O
sumed	O
to	O
be	O
independent	B
and	O
identically	O
distributed	O
(	O
i.i.d.	B
)	O
.	O
this	O
assumption	O
allowed	O
us	O
to	O
express	O
the	O
likelihood	B
function	I
as	O
the	O
product	O
over	O
all	O
data	O
points	O
of	O
the	O
prob-	O
ability	O
distribution	O
evaluated	O
at	O
each	O
data	O
point	O
.	O
for	O
many	O
applications	O
,	O
however	O
,	O
the	O
i.i.d	O
.	O
assumption	O
will	O
be	O
a	O
poor	O
one	O
.	O
here	O
we	O
consider	O
a	O
particularly	O
important	O
class	O
of	O
such	O
data	O
sets	O
,	O
namely	O
those	O
that	O
describe	O
sequential	B
data	I
.	O
these	O
often	O
arise	O
through	O
measurement	O
of	O
time	O
series	O
,	O
for	O
example	O
the	O
rainfall	O
measurements	O
on	O
suc-	O
cessive	O
days	O
at	O
a	O
particular	O
location	O
,	O
or	O
the	O
daily	O
values	O
of	O
a	O
currency	O
exchange	O
rate	O
,	O
or	O
the	O
acoustic	O
features	O
at	O
successive	O
time	O
frames	O
used	O
for	O
speech	O
recognition	O
.	O
an	O
example	O
involving	O
speech	O
data	O
is	O
shown	O
in	O
figure	O
13.1.	O
sequential	B
data	I
can	O
also	O
arise	O
in	O
contexts	O
other	O
than	O
time	O
series	O
,	O
for	O
example	O
the	O
sequence	O
of	O
nucleotide	O
base	O
pairs	O
along	O
a	O
strand	O
of	O
dna	O
or	O
the	O
sequence	O
of	O
characters	O
in	O
an	O
english	O
sentence	O
.	O
for	O
convenience	O
,	O
we	O
shall	O
sometimes	O
refer	O
to	O
‘	O
past	O
’	O
and	O
‘	O
future	O
’	O
observations	O
in	O
a	O
sequence	O
.	O
however	O
,	O
the	O
models	O
explored	O
in	O
this	O
chapter	O
are	O
equally	O
applicable	O
to	O
all	O
605	O
606	O
13.	O
sequential	B
data	I
figure	O
13.1	O
example	O
of	O
a	O
spectro-	O
gram	O
of	O
the	O
spoken	O
words	O
“	O
bayes	O
’	O
theo-	O
rem	O
”	O
showing	O
a	O
plot	O
of	O
the	O
intensity	O
of	O
the	O
spectral	O
coefﬁcients	O
versus	O
time	O
index	O
.	O
forms	O
of	O
sequential	B
data	I
,	O
not	O
just	O
temporal	O
sequences	O
.	O
it	O
is	O
useful	O
to	O
distinguish	O
between	O
stationary	B
and	O
nonstationary	O
sequential	O
dis-	O
tributions	O
.	O
in	O
the	O
stationary	B
case	O
,	O
the	O
data	O
evolves	O
in	O
time	O
,	O
but	O
the	O
distribution	O
from	O
which	O
it	O
is	O
generated	O
remains	O
the	O
same	O
.	O
for	O
the	O
more	O
complex	O
nonstationary	O
situa-	O
tion	O
,	O
the	O
generative	O
distribution	O
itself	O
is	O
evolving	O
with	O
time	O
.	O
here	O
we	O
shall	O
focus	O
on	O
the	O
stationary	B
case	O
.	O
for	O
many	O
applications	O
,	O
such	O
as	O
ﬁnancial	O
forecasting	O
,	O
we	O
wish	O
to	O
be	O
able	O
to	O
pre-	O
dict	O
the	O
next	O
value	O
in	O
a	O
time	O
series	O
given	O
observations	O
of	O
the	O
previous	O
values	O
.	O
in-	O
tuitively	O
,	O
we	O
expect	O
that	O
recent	O
observations	O
are	O
likely	O
to	O
be	O
more	O
informative	O
than	O
more	O
historical	O
observations	O
in	O
predicting	O
future	O
values	O
.	O
the	O
example	O
in	O
figure	O
13.1	O
shows	O
that	O
successive	O
observations	O
of	O
the	O
speech	O
spectrum	O
are	O
indeed	O
highly	O
cor-	O
related	O
.	O
furthermore	O
,	O
it	O
would	O
be	O
impractical	O
to	O
consider	O
a	O
general	O
dependence	O
of	O
future	O
observations	O
on	O
all	O
previous	O
observations	O
because	O
the	O
complexity	O
of	O
such	O
a	O
model	O
would	O
grow	O
without	O
limit	O
as	O
the	O
number	O
of	O
observations	O
increases	O
.	O
this	O
leads	O
us	O
to	O
consider	O
markov	O
models	O
in	O
which	O
we	O
assume	O
that	O
future	O
predictions	O
are	O
inde-	O
13.1.	O
markov	O
models	O
607	O
figure	O
13.2	O
the	O
simplest	O
approach	O
to	O
modelling	O
a	O
sequence	O
of	O
ob-	O
servations	O
is	O
to	O
treat	O
them	O
as	O
independent	B
,	O
correspond-	O
ing	O
to	O
a	O
graph	O
without	O
links	O
.	O
x1	O
x2	O
x3	O
x4	O
pendent	O
of	O
all	O
but	O
the	O
most	O
recent	O
observations	O
.	O
although	O
such	O
models	O
are	O
tractable	O
,	O
they	O
are	O
also	O
severely	O
limited	O
.	O
we	O
can	O
ob-	O
tain	O
a	O
more	O
general	O
framework	O
,	O
while	O
still	O
retaining	O
tractability	O
,	O
by	O
the	O
introduction	O
of	O
latent	O
variables	O
,	O
leading	O
to	O
state	O
space	O
models	O
.	O
as	O
in	O
chapters	O
9	O
and	O
12	O
,	O
we	O
shall	O
see	O
that	O
complex	O
models	O
can	O
thereby	O
be	O
constructed	O
from	O
simpler	O
components	O
(	O
in	O
particular	O
,	O
from	O
distributions	O
belonging	O
to	O
the	O
exponential	B
family	I
)	O
and	O
can	O
be	O
read-	O
ily	O
characterized	O
using	O
the	O
framework	O
of	O
probabilistic	O
graphical	O
models	O
.	O
here	O
we	O
focus	O
on	O
the	O
two	O
most	O
important	O
examples	O
of	O
state	O
space	O
models	O
,	O
namely	O
the	O
hid-	O
den	O
markov	O
model	O
,	O
in	O
which	O
the	O
latent	O
variables	O
are	O
discrete	O
,	O
and	O
linear	O
dynamical	O
systems	O
,	O
in	O
which	O
the	O
latent	O
variables	O
are	O
gaussian	O
.	O
both	O
models	O
are	O
described	O
by	O
di-	O
rected	O
graphs	O
having	O
a	O
tree	B
structure	O
(	O
no	O
loops	O
)	O
for	O
which	O
inference	B
can	O
be	O
performed	O
efﬁciently	O
using	O
the	O
sum-product	B
algorithm	I
.	O
13.1.	O
markov	O
models	O
the	O
easiest	O
way	O
to	O
treat	O
sequential	B
data	I
would	O
be	O
simply	O
to	O
ignore	O
the	O
sequential	O
aspects	O
and	O
treat	O
the	O
observations	O
as	O
i.i.d.	B
,	O
corresponding	O
to	O
the	O
graph	O
in	O
figure	O
13.2.	O
such	O
an	O
approach	O
,	O
however	O
,	O
would	O
fail	O
to	O
exploit	O
the	O
sequential	O
patterns	O
in	O
the	O
data	O
,	O
such	O
as	O
correlations	O
between	O
observations	O
that	O
are	O
close	O
in	O
the	O
sequence	O
.	O
suppose	O
,	O
for	O
instance	O
,	O
that	O
we	O
observe	O
a	O
binary	O
variable	O
denoting	O
whether	O
on	O
a	O
particular	O
day	O
it	O
rained	O
or	O
not	O
.	O
given	O
a	O
time	O
series	O
of	O
recent	O
observations	O
of	O
this	O
variable	O
,	O
we	O
wish	O
to	O
predict	O
whether	O
it	O
will	O
rain	O
on	O
the	O
next	O
day	O
.	O
if	O
we	O
treat	O
the	O
data	O
as	O
i.i.d.	B
,	O
then	O
the	O
only	O
information	O
we	O
can	O
glean	O
from	O
the	O
data	O
is	O
the	O
relative	B
frequency	O
of	O
rainy	O
days	O
.	O
however	O
,	O
we	O
know	O
in	O
practice	O
that	O
the	O
weather	O
often	O
exhibits	O
trends	O
that	O
may	O
last	O
for	O
several	O
days	O
.	O
observing	O
whether	O
or	O
not	O
it	O
rains	O
today	O
is	O
therefore	O
of	O
signiﬁcant	O
help	O
in	O
predicting	O
if	O
it	O
will	O
rain	O
tomorrow	O
.	O
to	O
express	O
such	O
effects	O
in	O
a	O
probabilistic	O
model	O
,	O
we	O
need	O
to	O
relax	O
the	O
i.i.d	O
.	O
as-	O
sumption	O
,	O
and	O
one	O
of	O
the	O
simplest	O
ways	O
to	O
do	O
this	O
is	O
to	O
consider	O
a	O
markov	O
model	O
.	O
first	O
of	O
all	O
we	O
note	O
that	O
,	O
without	O
loss	O
of	O
generality	O
,	O
we	O
can	O
use	O
the	O
product	B
rule	I
to	O
express	O
the	O
joint	O
distribution	O
for	O
a	O
sequence	O
of	O
observations	O
in	O
the	O
form	O
n	O
(	O
cid:14	O
)	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
=	O
p	O
(	O
xn|x1	O
,	O
.	O
.	O
.	O
,	O
xn−1	O
)	O
.	O
(	O
13.1	O
)	O
n=1	O
if	O
we	O
now	O
assume	O
that	O
each	O
of	O
the	O
conditional	B
distributions	O
on	O
the	O
right-hand	O
side	O
is	O
independent	B
of	O
all	O
previous	O
observations	O
except	O
the	O
most	O
recent	O
,	O
we	O
obtain	O
the	O
ﬁrst-order	O
markov	O
chain	O
,	O
which	O
is	O
depicted	O
as	O
a	O
graphical	B
model	I
in	O
figure	O
13.3.	O
the	O
608	O
13.	O
sequential	B
data	I
figure	O
13.3	O
a	O
ﬁrst-order	O
markov	O
chain	O
of	O
ob-	O
servations	O
{	O
xn	O
}	O
in	O
which	O
the	O
dis-	O
tribution	O
p	O
(	O
xn|xn−1	O
)	O
of	O
a	O
particu-	O
lar	O
observation	O
xn	O
is	O
conditioned	O
on	O
the	O
value	O
of	O
the	O
previous	O
ob-	O
servation	O
xn−1	O
.	O
x1	O
x2	O
x3	O
x4	O
joint	O
distribution	O
for	O
a	O
sequence	O
of	O
n	O
observations	O
under	O
this	O
model	O
is	O
given	O
by	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
xn|xn−1	O
)	O
.	O
(	O
13.2	O
)	O
section	O
8.2	O
exercise	O
13.1	O
n=2	O
from	O
the	O
d-separation	B
property	O
,	O
we	O
see	O
that	O
the	O
conditional	B
distribution	O
for	O
observa-	O
tion	O
xn	O
,	O
given	O
all	O
of	O
the	O
observations	O
up	O
to	O
time	O
n	O
,	O
is	O
given	O
by	O
p	O
(	O
xn|x1	O
,	O
.	O
.	O
.	O
,	O
xn−1	O
)	O
=	O
p	O
(	O
xn|xn−1	O
)	O
(	O
13.3	O
)	O
which	O
is	O
easily	O
veriﬁed	O
by	O
direct	O
evaluation	O
starting	O
from	O
(	O
13.2	O
)	O
and	O
using	O
the	O
prod-	O
uct	O
rule	O
of	O
probability	B
.	O
thus	O
if	O
we	O
use	O
such	O
a	O
model	O
to	O
predict	O
the	O
next	O
observation	O
in	O
a	O
sequence	O
,	O
the	O
distribution	O
of	O
predictions	O
will	O
depend	O
only	O
on	O
the	O
value	O
of	O
the	O
im-	O
mediately	O
preceding	O
observation	O
and	O
will	O
be	O
independent	B
of	O
all	O
earlier	O
observations	O
.	O
in	O
most	O
applications	O
of	O
such	O
models	O
,	O
the	O
conditional	B
distributions	O
p	O
(	O
xn|xn−1	O
)	O
that	O
deﬁne	O
the	O
model	O
will	O
be	O
constrained	O
to	O
be	O
equal	O
,	O
corresponding	O
to	O
the	O
assump-	O
tion	O
of	O
a	O
stationary	B
time	O
series	O
.	O
the	O
model	O
is	O
then	O
known	O
as	O
a	O
homogeneous	B
markov	O
chain	O
.	O
for	O
instance	O
,	O
if	O
the	O
conditional	B
distributions	O
depend	O
on	O
adjustable	O
parameters	O
(	O
whose	O
values	O
might	O
be	O
inferred	O
from	O
a	O
set	O
of	O
training	B
data	O
)	O
,	O
then	O
all	O
of	O
the	O
condi-	O
tional	O
distributions	O
in	O
the	O
chain	O
will	O
share	O
the	O
same	O
values	O
of	O
those	O
parameters	O
.	O
n	O
(	O
cid:14	O
)	O
n	O
(	O
cid:14	O
)	O
although	O
this	O
is	O
more	O
general	O
than	O
the	O
independence	O
model	O
,	O
it	O
is	O
still	O
very	O
re-	O
strictive	O
.	O
for	O
many	O
sequential	O
observations	O
,	O
we	O
anticipate	O
that	O
the	O
trends	O
in	O
the	O
data	O
over	O
several	O
successive	O
observations	O
will	O
provide	O
important	O
information	O
in	O
predict-	O
ing	O
the	O
next	O
value	O
.	O
one	O
way	O
to	O
allow	O
earlier	O
observations	O
to	O
have	O
an	O
inﬂuence	O
is	O
to	O
move	O
to	O
higher-order	O
markov	O
chains	O
.	O
if	O
we	O
allow	O
the	O
predictions	O
to	O
depend	O
also	O
on	O
the	O
previous-but-one	O
value	O
,	O
we	O
obtain	O
a	O
second-order	O
markov	O
chain	O
,	O
represented	O
by	O
the	O
graph	O
in	O
figure	O
13.4.	O
the	O
joint	O
distribution	O
is	O
now	O
given	O
by	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2|x1	O
)	O
p	O
(	O
xn|xn−1	O
,	O
xn−2	O
)	O
.	O
(	O
13.4	O
)	O
again	O
,	O
using	O
d-separation	B
or	O
by	O
direct	O
evaluation	O
,	O
we	O
see	O
that	O
the	O
conditional	B
distri-	O
bution	O
of	O
xn	O
given	O
xn−1	O
and	O
xn−2	O
is	O
independent	B
of	O
all	O
observations	O
x1	O
,	O
.	O
.	O
.	O
xn−3	O
.	O
n=3	O
figure	O
13.4	O
a	O
second-order	O
markov	O
chain	O
,	O
in	O
which	O
the	O
conditional	B
distribution	O
of	O
a	O
particular	O
observation	O
xn	O
depends	O
on	O
the	O
values	O
of	O
the	O
two	O
previous	O
observations	O
xn−1	O
and	O
xn−2	O
.	O
x1	O
x2	O
x3	O
x4	O
figure	O
13.5	O
we	O
can	O
represent	O
sequen-	O
tial	O
data	O
using	O
a	O
markov	O
chain	O
of	O
latent	O
variables	O
,	O
with	O
each	O
observation	O
condi-	O
tioned	O
on	O
the	O
state	O
of	O
the	O
corresponding	O
latent	B
variable	I
.	O
this	O
important	O
graphical	O
structure	O
forms	O
the	O
foundation	O
both	O
for	O
the	O
hidden	O
markov	O
model	O
and	O
for	O
linear	O
dy-	O
namical	O
systems	O
.	O
z1	O
x1	O
z2	O
x2	O
13.1.	O
markov	O
models	O
zn−1	O
zn	O
609	O
zn+1	O
xn−1	O
xn	O
xn+1	O
each	O
observation	O
is	O
now	O
inﬂuenced	O
by	O
two	O
previous	O
observations	O
.	O
we	O
can	O
similarly	O
consider	O
extensions	O
to	O
an	O
m	O
th	O
order	O
markov	O
chain	O
in	O
which	O
the	O
conditional	B
distri-	O
bution	O
for	O
a	O
particular	O
variable	O
depends	O
on	O
the	O
previous	O
m	O
variables	O
.	O
however	O
,	O
we	O
have	O
paid	O
a	O
price	O
for	O
this	O
increased	O
ﬂexibility	O
because	O
the	O
number	O
of	O
parameters	O
in	O
the	O
model	O
is	O
now	O
much	O
larger	O
.	O
suppose	O
the	O
observations	O
are	O
discrete	O
variables	O
hav-	O
ing	O
k	O
states	O
.	O
then	O
the	O
conditional	B
distribution	O
p	O
(	O
xn|xn−1	O
)	O
in	O
a	O
ﬁrst-order	O
markov	O
chain	O
will	O
be	O
speciﬁed	O
by	O
a	O
set	O
of	O
k	O
−	O
1	O
parameters	O
for	O
each	O
of	O
the	O
k	O
states	O
of	O
xn−1	O
giving	O
a	O
total	O
of	O
k	O
(	O
k	O
−	O
1	O
)	O
parameters	O
.	O
now	O
suppose	O
we	O
extend	O
the	O
model	O
to	O
an	O
m	O
th	O
order	O
markov	O
chain	O
,	O
so	O
that	O
the	O
joint	O
distribution	O
is	O
built	O
up	O
from	O
conditionals	O
p	O
(	O
xn|xn−m	O
,	O
.	O
.	O
.	O
,	O
xn−1	O
)	O
.	O
if	O
the	O
variables	O
are	O
discrete	O
,	O
and	O
if	O
the	O
conditional	B
distri-	O
butions	O
are	O
represented	O
by	O
general	O
conditional	B
probability	I
tables	O
,	O
then	O
the	O
number	O
of	O
parameters	O
in	O
such	O
a	O
model	O
will	O
have	O
km−1	O
(	O
k	O
−	O
1	O
)	O
parameters	O
.	O
because	O
this	O
grows	O
exponentially	O
with	O
m	O
,	O
it	O
will	O
often	O
render	O
this	O
approach	O
impractical	O
for	O
larger	O
values	O
of	O
m.	O
for	O
continuous	O
variables	O
,	O
we	O
can	O
use	O
linear-gaussian	O
conditional	B
distributions	O
in	O
which	O
each	O
node	B
has	O
a	O
gaussian	O
distribution	O
whose	O
mean	B
is	O
a	O
linear	O
function	O
of	O
its	O
parents	O
.	O
this	O
is	O
known	O
as	O
an	O
autoregressive	B
or	O
ar	O
model	O
(	O
box	O
et	O
al.	O
,	O
1994	O
;	O
thiesson	O
et	O
al.	O
,	O
2004	O
)	O
.	O
an	O
alternative	O
approach	O
is	O
to	O
use	O
a	O
parametric	O
model	O
for	O
p	O
(	O
xn|xn−m	O
,	O
.	O
.	O
.	O
,	O
xn−1	O
)	O
such	O
as	O
a	O
neural	B
network	I
.	O
this	O
technique	O
is	O
sometimes	O
called	O
a	O
tapped	B
delay	I
line	I
because	O
it	O
corresponds	O
to	O
storing	O
(	O
delaying	O
)	O
the	O
previous	O
m	O
values	O
of	O
the	O
observed	B
variable	I
in	O
order	O
to	O
predict	O
the	O
next	O
value	O
.	O
the	O
number	O
of	O
parameters	O
can	O
then	O
be	O
much	O
smaller	O
than	O
in	O
a	O
completely	O
general	O
model	O
(	O
for	O
ex-	O
ample	O
it	O
may	O
grow	O
linearly	O
with	O
m	O
)	O
,	O
although	O
this	O
is	O
achieved	O
at	O
the	O
expense	O
of	O
a	O
restricted	O
family	O
of	O
conditional	B
distributions	O
.	O
suppose	O
we	O
wish	O
to	O
build	O
a	O
model	O
for	O
sequences	O
that	O
is	O
not	O
limited	O
by	O
the	O
markov	O
assumption	O
to	O
any	O
order	O
and	O
yet	O
that	O
can	O
be	O
speciﬁed	O
using	O
a	O
limited	O
number	O
of	O
free	O
parameters	O
.	O
we	O
can	O
achieve	O
this	O
by	O
introducing	O
additional	O
latent	O
variables	O
to	O
permit	O
a	O
rich	O
class	O
of	O
models	O
to	O
be	O
constructed	O
out	O
of	O
simple	O
components	O
,	O
as	O
we	O
did	O
with	O
mixture	B
distributions	O
in	O
chapter	O
9	O
and	O
with	O
continuous	O
latent	B
variable	I
models	O
in	O
chapter	O
12.	O
for	O
each	O
observation	O
xn	O
,	O
we	O
introduce	O
a	O
corresponding	O
latent	B
variable	I
zn	O
(	O
which	O
may	O
be	O
of	O
different	O
type	O
or	O
dimensionality	O
to	O
the	O
observed	B
variable	I
)	O
.	O
we	O
now	O
assume	O
that	O
it	O
is	O
the	O
latent	O
variables	O
that	O
form	O
a	O
markov	O
chain	O
,	O
giving	O
rise	O
to	O
the	O
graphical	O
structure	O
known	O
as	O
a	O
state	B
space	I
model	I
,	O
which	O
is	O
shown	O
in	O
figure	O
13.5.	O
it	O
satisﬁes	O
the	O
key	O
conditional	B
independence	I
property	O
that	O
zn−1	O
and	O
zn+1	O
are	O
indepen-	O
dent	O
given	O
zn	O
,	O
so	O
that	O
zn+1	O
⊥⊥	O
zn−1	O
|	O
zn	O
.	O
(	O
13.5	O
)	O
610	O
13.	O
sequential	B
data	I
the	O
joint	O
distribution	O
for	O
this	O
model	O
is	O
given	O
by	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
z1	O
,	O
.	O
.	O
.	O
,	O
zn	O
)	O
=	O
p	O
(	O
z1	O
)	O
(	O
cid:31	O
)	O
n	O
(	O
cid:14	O
)	O
p	O
(	O
zn|zn−1	O
)	O
n=2	O
n	O
(	O
cid:14	O
)	O
n=1	O
p	O
(	O
xn|zn	O
)	O
.	O
(	O
13.6	O
)	O
using	O
the	O
d-separation	B
criterion	O
,	O
we	O
see	O
that	O
there	O
is	O
always	O
a	O
path	O
connecting	O
any	O
two	O
observed	O
variables	O
xn	O
and	O
xm	O
via	O
the	O
latent	O
variables	O
,	O
and	O
that	O
this	O
path	O
is	O
never	O
blocked	O
.	O
thus	O
the	O
predictive	B
distribution	I
p	O
(	O
xn+1|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
for	O
observation	O
xn+1	O
given	O
all	O
previous	O
observations	O
does	O
not	O
exhibit	O
any	O
conditional	B
independence	I
prop-	O
erties	O
,	O
and	O
so	O
our	O
predictions	O
for	O
xn+1	O
depends	O
on	O
all	O
previous	O
observations	O
.	O
the	O
observed	O
variables	O
,	O
however	O
,	O
do	O
not	O
satisfy	O
the	O
markov	O
property	O
at	O
any	O
order	O
.	O
we	O
shall	O
discuss	O
how	O
to	O
evaluate	O
the	O
predictive	B
distribution	I
in	O
later	O
sections	O
of	O
this	O
chap-	O
ter	O
.	O
there	O
are	O
two	O
important	O
models	O
for	O
sequential	O
data	O
that	O
are	O
described	O
by	O
this	O
graph	O
.	O
if	O
the	O
latent	O
variables	O
are	O
discrete	O
,	O
then	O
we	O
obtain	O
the	O
hidden	O
markov	O
model	O
,	O
or	O
hmm	O
(	O
elliott	O
et	O
al.	O
,	O
1995	O
)	O
.	O
note	O
that	O
the	O
observed	O
variables	O
in	O
an	O
hmm	O
may	O
be	O
discrete	O
or	O
continuous	O
,	O
and	O
a	O
variety	O
of	O
different	O
conditional	B
distributions	O
can	O
be	O
used	O
to	O
model	O
them	O
.	O
if	O
both	O
the	O
latent	O
and	O
the	O
observed	O
variables	O
are	O
gaussian	O
(	O
with	O
a	O
linear-gaussian	O
dependence	O
of	O
the	O
conditional	B
distributions	O
on	O
their	O
parents	O
)	O
,	O
then	O
we	O
obtain	O
the	O
linear	B
dynamical	I
system	I
.	O
section	O
13.2	O
section	O
13.3	O
13.2.	O
hidden	O
markov	O
models	O
the	O
hidden	O
markov	O
model	O
can	O
be	O
viewed	O
as	O
a	O
speciﬁc	O
instance	O
of	O
the	O
state	B
space	I
model	I
of	O
figure	O
13.5	O
in	O
which	O
the	O
latent	O
variables	O
are	O
discrete	O
.	O
however	O
,	O
if	O
we	O
examine	O
a	O
single	O
time	O
slice	O
of	O
the	O
model	O
,	O
we	O
see	O
that	O
it	O
corresponds	O
to	O
a	O
mixture	B
distribution	I
,	O
with	O
component	O
densities	O
given	O
by	O
p	O
(	O
x|z	O
)	O
.	O
it	O
can	O
therefore	O
also	O
be	O
interpreted	O
as	O
an	O
extension	O
of	O
a	O
mixture	B
model	I
in	O
which	O
the	O
choice	O
of	O
mixture	B
com-	O
ponent	O
for	O
each	O
observation	O
is	O
not	O
selected	O
independently	O
but	O
depends	O
on	O
the	O
choice	O
of	O
component	O
for	O
the	O
previous	O
observation	O
.	O
the	O
hmm	O
is	O
widely	O
used	O
in	O
speech	B
recognition	I
(	O
jelinek	O
,	O
1997	O
;	O
rabiner	O
and	O
juang	O
,	O
1993	O
)	O
,	O
natural	B
language	I
modelling	I
(	O
manning	O
and	O
sch¨utze	O
,	O
1999	O
)	O
,	O
on-line	O
handwriting	O
recognition	O
(	O
nag	O
et	O
al.	O
,	O
1986	O
)	O
,	O
and	O
for	O
the	O
analysis	O
of	O
biological	O
sequences	O
such	O
as	O
proteins	O
and	O
dna	O
(	O
krogh	O
et	O
al.	O
,	O
1994	O
;	O
durbin	O
et	O
al.	O
,	O
1998	O
;	O
baldi	O
and	O
brunak	O
,	O
2001	O
)	O
.	O
as	O
in	O
the	O
case	O
of	O
a	O
standard	O
mixture	O
model	O
,	O
the	O
latent	O
variables	O
are	O
the	O
discrete	O
multinomial	O
variables	O
zn	O
describing	O
which	O
component	O
of	O
the	O
mixture	B
is	O
responsible	O
for	O
generating	O
the	O
corresponding	O
observation	O
xn	O
.	O
again	O
,	O
it	O
is	O
convenient	O
to	O
use	O
a	O
1-of-k	O
coding	O
scheme	O
,	O
as	O
used	O
for	O
mixture	O
models	O
in	O
chapter	O
9.	O
we	O
now	O
allow	O
the	O
probability	B
distribution	O
of	O
zn	O
to	O
depend	O
on	O
the	O
state	O
of	O
the	O
previous	O
latent	B
variable	I
zn−1	O
through	O
a	O
conditional	B
distribution	O
p	O
(	O
zn|zn−1	O
)	O
.	O
because	O
the	O
latent	O
variables	O
are	O
k-dimensional	O
binary	O
variables	O
,	O
this	O
conditional	B
distribution	O
corresponds	O
to	O
a	O
table	O
of	O
numbers	O
that	O
we	O
denote	O
by	O
a	O
,	O
the	O
elements	O
of	O
which	O
are	O
known	O
as	O
transition	O
probabilities	O
.	O
they	O
are	O
given	O
by	O
ajk	O
≡	O
p	O
(	O
znk	O
=	O
1|zn−1	O
,	O
j	O
=	O
1	O
)	O
,	O
and	O
because	O
they	O
are	O
probabilities	O
,	O
they	O
satisfy	O
0	O
(	O
cid:1	O
)	O
ajk	O
(	O
cid:1	O
)	O
1	O
with	O
k	O
ajk	O
=	O
1	O
,	O
so	O
that	O
the	O
matrix	O
a	O
(	O
cid:5	O
)	O
13.2.	O
hidden	O
markov	O
models	O
611	O
figure	O
13.6	O
transition	O
diagram	O
showing	O
a	O
model	O
whose	O
la-	O
tent	O
variables	O
have	O
three	O
possible	O
states	O
corre-	O
sponding	O
to	O
the	O
three	O
boxes	O
.	O
the	O
black	O
lines	O
denote	O
the	O
elements	O
of	O
the	O
transition	O
matrix	O
ajk	O
.	O
a22	O
a21	O
a12	O
k	O
=	O
2	O
a32	O
a23	O
k	O
=	O
1	O
a11	O
k	O
=	O
3	O
a31	O
a13	O
a33	O
has	O
k	O
(	O
k−1	O
)	O
independent	B
parameters	O
.	O
we	O
can	O
then	O
write	O
the	O
conditional	B
distribution	O
explicitly	O
in	O
the	O
form	O
p	O
(	O
zn|zn−1	O
,	O
a	O
)	O
=	O
zn−1	O
,	O
j	O
znk	O
jk	O
.	O
a	O
(	O
13.7	O
)	O
k	O
(	O
cid:14	O
)	O
k	O
(	O
cid:14	O
)	O
k=1	O
j=1	O
k	O
(	O
cid:14	O
)	O
k=1	O
the	O
initial	O
latent	O
node	O
z1	O
is	O
special	O
in	O
that	O
it	O
does	O
not	O
have	O
a	O
parent	B
node	I
,	O
and	O
so	O
it	O
has	O
a	O
marginal	B
distribution	O
p	O
(	O
z1	O
)	O
represented	O
by	O
a	O
vector	O
of	O
probabilities	O
π	O
with	O
elements	O
πk	O
≡	O
p	O
(	O
z1k	O
=	O
1	O
)	O
,	O
so	O
that	O
p	O
(	O
z1|π	O
)	O
=	O
πz1k	O
k	O
(	O
13.8	O
)	O
(	O
cid:5	O
)	O
where	O
k	O
πk	O
=	O
1.	O
the	O
transition	O
matrix	O
is	O
sometimes	O
illustrated	O
diagrammatically	O
by	O
drawing	O
the	O
states	O
as	O
nodes	O
in	O
a	O
state	O
transition	O
diagram	O
as	O
shown	O
in	O
figure	O
13.6	O
for	O
the	O
case	O
of	O
k	O
=	O
3.	O
note	O
that	O
this	O
does	O
not	O
represent	O
a	O
probabilistic	B
graphical	I
model	I
,	O
because	O
the	O
nodes	O
are	O
not	O
separate	O
variables	O
but	O
rather	O
states	O
of	O
a	O
single	O
variable	O
,	O
and	O
so	O
we	O
have	O
shown	O
the	O
states	O
as	O
boxes	O
rather	O
than	O
circles	O
.	O
section	O
8.4.5	O
it	O
is	O
sometimes	O
useful	O
to	O
take	O
a	O
state	O
transition	O
diagram	O
,	O
of	O
the	O
kind	O
shown	O
in	O
figure	O
13.6	O
,	O
and	O
unfold	O
it	O
over	O
time	O
.	O
this	O
gives	O
an	O
alternative	O
representation	O
of	O
the	O
transitions	O
between	O
latent	O
states	O
,	O
known	O
as	O
a	O
lattice	O
or	O
trellis	B
diagram	I
,	O
and	O
which	O
is	O
shown	O
for	O
the	O
case	O
of	O
the	O
hidden	O
markov	O
model	O
in	O
figure	O
13.7.	O
the	O
speciﬁcation	O
of	O
the	O
probabilistic	O
model	O
is	O
completed	O
by	O
deﬁning	O
the	O
con-	O
ditional	O
distributions	O
of	O
the	O
observed	O
variables	O
p	O
(	O
xn|zn	O
,	O
φ	O
)	O
,	O
where	O
φ	O
is	O
a	O
set	O
of	O
pa-	O
rameters	O
governing	O
the	O
distribution	O
.	O
these	O
are	O
known	O
as	O
emission	O
probabilities	O
,	O
and	O
might	O
for	O
example	O
be	O
given	O
by	O
gaussians	O
of	O
the	O
form	O
(	O
9.11	O
)	O
if	O
the	O
elements	O
of	O
x	O
are	O
continuous	O
variables	O
,	O
or	O
by	O
conditional	B
probability	I
tables	O
if	O
x	O
is	O
discrete	O
.	O
because	O
xn	O
is	O
observed	O
,	O
the	O
distribution	O
p	O
(	O
xn|zn	O
,	O
φ	O
)	O
consists	O
,	O
for	O
a	O
given	O
value	O
of	O
φ	O
,	O
of	O
a	O
vector	O
of	O
k	O
numbers	O
corresponding	O
to	O
the	O
k	O
possible	O
states	O
of	O
the	O
binary	O
vector	O
zn	O
.	O
612	O
13.	O
sequential	B
data	I
figure	O
13.7	O
if	O
we	O
unfold	O
the	O
state	O
transition	O
dia-	O
gram	O
of	O
figure	O
13.6	O
over	O
time	O
,	O
we	O
obtain	O
a	O
lattice	O
,	O
or	O
trellis	O
,	O
representation	O
of	O
the	O
latent	O
states	O
.	O
each	O
column	O
of	O
this	O
diagram	O
corresponds	O
to	O
one	O
of	O
the	O
latent	O
variables	O
zn	O
.	O
k	O
=	O
1	O
k	O
=	O
2	O
k	O
=	O
3	O
a11	O
a11	O
a11	O
a33	O
n	O
−	O
2	O
n	O
−	O
1	O
a33	O
n	O
a33	O
n	O
+	O
1	O
we	O
can	O
represent	O
the	O
emission	O
probabilities	O
in	O
the	O
form	O
p	O
(	O
xn|zn	O
,	O
φ	O
)	O
=	O
p	O
(	O
xn|φk	O
)	O
znk	O
.	O
(	O
13.9	O
)	O
k	O
(	O
cid:14	O
)	O
k=1	O
we	O
shall	O
focuss	O
attention	O
on	O
homogeneous	B
models	O
for	O
which	O
all	O
of	O
the	O
condi-	O
tional	O
distributions	O
governing	O
the	O
latent	O
variables	O
share	O
the	O
same	O
parameters	O
a	O
,	O
and	O
similarly	O
all	O
of	O
the	O
emission	O
distributions	O
share	O
the	O
same	O
parameters	O
φ	O
(	O
the	O
extension	O
to	O
more	O
general	O
cases	O
is	O
straightforward	O
)	O
.	O
note	O
that	O
a	O
mixture	B
model	I
for	O
an	O
i.i.d	O
.	O
data	O
set	O
corresponds	O
to	O
the	O
special	O
case	O
in	O
which	O
the	O
parameters	O
ajk	O
are	O
the	O
same	O
for	O
all	O
values	O
of	O
j	O
,	O
so	O
that	O
the	O
conditional	B
distribution	O
p	O
(	O
zn|zn−1	O
)	O
is	O
independent	B
of	O
zn−1	O
.	O
this	O
corresponds	O
to	O
deleting	O
the	O
horizontal	O
links	O
in	O
the	O
graphical	B
model	I
shown	O
in	O
figure	O
13.5.	O
the	O
joint	O
probability	B
distribution	O
over	O
both	O
latent	O
and	O
observed	O
variables	O
is	O
then	O
given	O
by	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
=	O
p	O
(	O
z1|π	O
)	O
(	O
cid:31	O
)	O
n	O
(	O
cid:14	O
)	O
n	O
(	O
cid:14	O
)	O
p	O
(	O
zn|zn−1	O
,	O
a	O
)	O
p	O
(	O
xm|zm	O
,	O
φ	O
)	O
(	O
13.10	O
)	O
exercise	O
13.4	O
n=2	O
m=1	O
where	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
,	O
z	O
=	O
{	O
z1	O
,	O
.	O
.	O
.	O
,	O
zn	O
}	O
,	O
and	O
θ	O
=	O
{	O
π	O
,	O
a	O
,	O
φ	O
}	O
denotes	O
the	O
set	O
of	O
parameters	O
governing	O
the	O
model	O
.	O
most	O
of	O
our	O
discussion	O
of	O
the	O
hidden	O
markov	O
model	O
will	O
be	O
independent	B
of	O
the	O
particular	O
choice	O
of	O
the	O
emission	O
probabilities	O
.	O
indeed	O
,	O
the	O
model	O
is	O
tractable	O
for	O
a	O
wide	O
range	O
of	O
emission	O
distributions	O
including	O
discrete	O
tables	O
,	O
gaussians	O
,	O
and	O
mixtures	O
of	O
gaussians	O
.	O
it	O
is	O
also	O
possible	O
to	O
exploit	O
discriminative	O
models	O
such	O
as	O
neural	O
networks	O
.	O
these	O
can	O
be	O
used	O
to	O
model	O
the	O
emission	O
density	O
p	O
(	O
x|z	O
)	O
directly	O
,	O
or	O
to	O
provide	O
a	O
representation	O
for	O
p	O
(	O
z|x	O
)	O
that	O
can	O
be	O
converted	O
into	O
the	O
required	O
emission	O
density	O
p	O
(	O
x|z	O
)	O
using	O
bayes	O
’	O
theorem	O
(	O
bishop	O
et	O
al.	O
,	O
2004	O
)	O
.	O
we	O
can	O
gain	O
a	O
better	O
understanding	O
of	O
the	O
hidden	O
markov	O
model	O
by	O
considering	O
it	O
from	O
a	O
generative	O
point	O
of	O
view	O
.	O
recall	O
that	O
to	O
generate	O
samples	O
from	O
a	O
mixture	O
of	O
1	O
0.5	O
k	O
=	O
1	O
0	O
0	O
k	O
=	O
3	O
k	O
=	O
2	O
0.5	O
13.2.	O
hidden	O
markov	O
models	O
613	O
1	O
0.5	O
1	O
0	O
0	O
0.5	O
1	O
figure	O
13.8	O
illustration	O
of	O
sampling	O
from	O
a	O
hidden	O
markov	O
model	O
having	O
a	O
3-state	O
latent	B
variable	I
z	O
and	O
a	O
gaussian	O
emission	O
model	O
p	O
(	O
x|z	O
)	O
where	O
x	O
is	O
2-dimensional	O
.	O
(	O
a	O
)	O
contours	O
of	O
constant	O
probability	B
density	O
for	O
the	O
emission	O
distributions	O
corresponding	O
to	O
each	O
of	O
the	O
three	O
states	O
of	O
the	O
latent	B
variable	I
.	O
(	O
b	O
)	O
a	O
sample	O
of	O
50	O
points	O
drawn	O
from	O
the	O
hidden	O
markov	O
model	O
,	O
colour	O
coded	O
according	O
to	O
the	O
component	O
that	O
generated	O
them	O
and	O
with	O
lines	O
connecting	O
the	O
successive	O
observations	O
.	O
here	O
the	O
transition	O
matrix	O
was	O
ﬁxed	O
so	O
that	O
in	O
any	O
state	O
there	O
is	O
a	O
5	O
%	O
probability	B
of	O
making	O
a	O
transition	O
to	O
each	O
of	O
the	O
other	O
states	O
,	O
and	O
consequently	O
a	O
90	O
%	O
probability	B
of	O
remaining	O
in	O
the	O
same	O
state	O
.	O
gaussians	O
,	O
we	O
ﬁrst	O
chose	O
one	O
of	O
the	O
components	O
at	O
random	O
with	O
probability	B
given	O
by	O
the	O
mixing	O
coefﬁcients	O
πk	O
and	O
then	O
generate	O
a	O
sample	O
vector	O
x	O
from	O
the	O
correspond-	O
ing	O
gaussian	O
component	O
.	O
this	O
process	O
is	O
repeated	O
n	O
times	O
to	O
generate	O
a	O
data	O
set	O
of	O
n	O
independent	B
samples	O
.	O
in	O
the	O
case	O
of	O
the	O
hidden	O
markov	O
model	O
,	O
this	O
procedure	O
is	O
modiﬁed	O
as	O
follows	O
.	O
we	O
ﬁrst	O
choose	O
the	O
initial	O
latent	B
variable	I
z1	O
with	O
probabilities	O
governed	O
by	O
the	O
parameters	O
πk	O
and	O
then	O
sample	O
the	O
corresponding	O
observation	O
x1	O
.	O
now	O
we	O
choose	O
the	O
state	O
of	O
the	O
variable	O
z2	O
according	O
to	O
the	O
transition	O
probabilities	O
p	O
(	O
z2|z1	O
)	O
using	O
the	O
already	O
instantiated	O
value	O
of	O
z1	O
.	O
thus	O
suppose	O
that	O
the	O
sample	O
for	O
z1	O
corresponds	O
to	O
state	O
j.	O
then	O
we	O
choose	O
the	O
state	O
k	O
of	O
z2	O
with	O
probabilities	O
ajk	O
for	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k.	O
once	O
we	O
know	O
z2	O
we	O
can	O
draw	O
a	O
sample	O
for	O
x2	O
and	O
also	O
sample	O
the	O
next	O
latent	B
variable	I
z3	O
and	O
so	O
on	O
.	O
this	O
is	O
an	O
example	O
of	O
ancestral	B
sampling	I
for	O
a	O
directed	B
graphical	O
model	O
.	O
if	O
,	O
for	O
instance	O
,	O
we	O
have	O
a	O
model	O
in	O
which	O
the	O
diago-	O
nal	O
transition	O
elements	O
akk	O
are	O
much	O
larger	O
than	O
the	O
off-diagonal	O
elements	O
,	O
then	O
a	O
typical	O
data	O
sequence	O
will	O
have	O
long	O
runs	O
of	O
points	O
generated	O
from	O
a	O
single	O
compo-	O
nent	O
,	O
with	O
infrequent	O
transitions	O
from	O
one	O
component	O
to	O
another	O
.	O
the	O
generation	O
of	O
samples	O
from	O
a	O
hidden	O
markov	O
model	O
is	O
illustrated	O
in	O
figure	O
13.8.	O
there	O
are	O
many	O
variants	O
of	O
the	O
standard	O
hmm	O
model	O
,	O
obtained	O
for	O
instance	O
by	O
imposing	O
constraints	O
on	O
the	O
form	O
of	O
the	O
transition	O
matrix	O
a	O
(	O
rabiner	O
,	O
1989	O
)	O
.	O
here	O
we	O
mention	O
one	O
of	O
particular	O
practical	O
importance	O
called	O
the	O
left-to-right	B
hmm	O
,	O
which	O
is	O
obtained	O
by	O
setting	O
the	O
elements	O
ajk	O
of	O
a	O
to	O
zero	O
if	O
k	O
<	O
j	O
,	O
as	O
illustrated	O
in	O
the	O
section	O
8.1.2	O
614	O
13.	O
sequential	B
data	I
figure	O
13.9	O
example	O
of	O
the	O
state	O
transition	O
diagram	O
for	O
a	O
3-state	O
left-to-right	B
hidden	O
markov	O
model	O
.	O
note	O
that	O
once	O
a	O
state	O
has	O
been	O
vacated	O
,	O
it	O
can	O
not	O
later	O
be	O
re-entered	O
.	O
a11	O
a22	O
a33	O
a12	O
a23	O
k	O
=	O
1	O
a13	O
k	O
=	O
2	O
k	O
=	O
3	O
state	O
transition	O
diagram	O
for	O
a	O
3-state	O
hmm	O
in	O
figure	O
13.9.	O
typically	O
for	O
such	O
models	O
the	O
initial	O
state	O
probabilities	O
for	O
p	O
(	O
z1	O
)	O
are	O
modiﬁed	O
so	O
that	O
p	O
(	O
z11	O
)	O
=	O
1	O
and	O
p	O
(	O
z1j	O
)	O
=	O
0	O
for	O
j	O
(	O
cid:9	O
)	O
=	O
1	O
,	O
in	O
other	O
words	O
every	O
sequence	O
is	O
constrained	O
to	O
start	O
in	O
state	O
j	O
=	O
1.	O
the	O
transition	O
matrix	O
may	O
be	O
further	O
constrained	O
to	O
ensure	O
that	O
large	O
changes	O
in	O
the	O
state	O
index	O
do	O
not	O
occur	O
,	O
so	O
that	O
ajk	O
=	O
0	O
if	O
k	O
>	O
j	O
+	O
∆	O
.	O
this	O
type	O
of	O
model	O
is	O
illustrated	O
using	O
a	O
lattice	B
diagram	I
in	O
figure	O
13.10.	O
many	O
applications	O
of	O
hidden	O
markov	O
models	O
,	O
for	O
example	O
speech	B
recognition	I
,	O
or	O
on-line	O
character	O
recognition	O
,	O
make	O
use	O
of	O
left-to-right	B
architectures	O
.	O
as	O
an	O
illus-	O
tration	O
of	O
the	O
left-to-right	B
hidden	O
markov	O
model	O
,	O
we	O
consider	O
an	O
example	O
involving	O
handwritten	O
digits	O
.	O
this	O
uses	O
on-line	O
data	O
,	O
meaning	O
that	O
each	O
digit	O
is	O
represented	O
by	O
the	O
trajectory	O
of	O
the	O
pen	O
as	O
a	O
function	O
of	O
time	O
in	O
the	O
form	O
of	O
a	O
sequence	O
of	O
pen	O
coordinates	O
,	O
in	O
contrast	O
to	O
the	O
off-line	O
digits	O
data	O
,	O
discussed	O
in	O
appendix	O
a	O
,	O
which	O
comprises	O
static	O
two-dimensional	O
pixellated	O
images	O
of	O
the	O
ink	O
.	O
examples	O
of	O
the	O
on-	O
line	O
digits	O
are	O
shown	O
in	O
figure	O
13.11.	O
here	O
we	O
train	O
a	O
hidden	O
markov	O
model	O
on	O
a	O
subset	O
of	O
data	O
comprising	O
45	O
examples	O
of	O
the	O
digit	O
‘	O
2	O
’	O
.	O
there	O
are	O
k	O
=	O
16	O
states	O
,	O
each	O
of	O
which	O
can	O
generate	O
a	O
line	O
segment	O
of	O
ﬁxed	O
length	O
having	O
one	O
of	O
16	O
possible	O
angles	O
,	O
and	O
so	O
the	O
emission	O
distribution	O
is	O
simply	O
a	O
16	O
×	O
16	O
table	O
of	O
probabilities	O
associated	O
with	O
the	O
allowed	O
angle	O
values	O
for	O
each	O
state	O
index	O
value	O
.	O
transition	O
prob-	O
abilities	O
are	O
all	O
set	O
to	O
zero	O
except	O
for	O
those	O
that	O
keep	O
the	O
state	O
index	O
k	O
the	O
same	O
or	O
that	O
increment	O
it	O
by	O
1	O
,	O
and	O
the	O
model	O
parameters	O
are	O
optimized	O
using	O
25	O
iterations	O
of	O
em	O
.	O
we	O
can	O
gain	O
some	O
insight	O
into	O
the	O
resulting	O
model	O
by	O
running	O
it	O
generatively	O
,	O
as	O
shown	O
in	O
figure	O
13.11.	O
figure	O
13.10	O
lattice	B
diagram	I
for	O
a	O
3-state	O
left-	O
to-right	O
hmm	O
in	O
which	O
the	O
state	O
index	O
k	O
is	O
allowed	O
to	O
increase	O
by	O
at	O
most	O
1	O
at	O
each	O
transition	O
.	O
k	O
=	O
1	O
k	O
=	O
2	O
k	O
=	O
3	O
a11	O
a11	O
a11	O
a33	O
n	O
−	O
2	O
n	O
−	O
1	O
a33	O
n	O
a33	O
n	O
+	O
1	O
13.2.	O
hidden	O
markov	O
models	O
615	O
figure	O
13.11	O
top	O
row	O
:	O
examples	O
of	O
on-line	O
handwritten	O
digits	O
.	O
bottom	O
row	O
:	O
synthetic	O
digits	O
sam-	O
pled	O
generatively	O
from	O
a	O
left-to-right	B
hid-	O
den	O
markov	O
model	O
that	O
has	O
been	O
trained	O
on	O
a	O
data	O
set	O
of	O
45	O
handwritten	O
digits	O
.	O
one	O
of	O
the	O
most	O
powerful	O
properties	O
of	O
hidden	O
markov	O
models	O
is	O
their	O
ability	O
to	O
exhibit	O
some	O
degree	O
of	O
invariance	B
to	O
local	B
warping	O
(	O
compression	O
and	O
stretching	O
)	O
of	O
the	O
time	O
axis	O
.	O
to	O
understand	O
this	O
,	O
consider	O
the	O
way	O
in	O
which	O
the	O
digit	O
‘	O
2	O
’	O
is	O
written	O
in	O
the	O
on-line	O
handwritten	O
digits	O
example	O
.	O
a	O
typical	O
digit	O
comprises	O
two	O
distinct	O
sections	O
joined	O
at	O
a	O
cusp	O
.	O
the	O
ﬁrst	O
part	O
of	O
the	O
digit	O
,	O
which	O
starts	O
at	O
the	O
top	O
left	O
,	O
has	O
a	O
sweeping	O
arc	B
down	O
to	O
the	O
cusp	O
or	O
loop	O
at	O
the	O
bottom	O
left	O
,	O
followed	O
by	O
a	O
second	O
more-	O
or-less	O
straight	O
sweep	O
ending	O
at	O
the	O
bottom	O
right	O
.	O
natural	O
variations	O
in	O
writing	O
style	O
will	O
cause	O
the	O
relative	B
sizes	O
of	O
the	O
two	O
sections	O
to	O
vary	O
,	O
and	O
hence	O
the	O
location	O
of	O
the	O
cusp	O
or	O
loop	O
within	O
the	O
temporal	O
sequence	O
will	O
vary	O
.	O
from	O
a	O
generative	O
perspective	O
such	O
variations	O
can	O
be	O
accommodated	O
by	O
the	O
hidden	O
markov	O
model	O
through	O
changes	O
in	O
the	O
number	O
of	O
transitions	O
to	O
the	O
same	O
state	O
versus	O
the	O
number	O
of	O
transitions	O
to	O
the	O
successive	O
state	O
.	O
note	O
,	O
however	O
,	O
that	O
if	O
a	O
digit	O
‘	O
2	O
’	O
is	O
written	O
in	O
the	O
reverse	O
order	O
,	O
that	O
is	O
,	O
starting	O
at	O
the	O
bottom	O
right	O
and	O
ending	O
at	O
the	O
top	O
left	O
,	O
then	O
even	O
though	O
the	O
pen	O
tip	O
coordinates	O
may	O
be	O
identical	O
to	O
an	O
example	O
from	O
the	O
training	B
set	I
,	O
the	O
probability	B
of	O
the	O
observations	O
under	O
the	O
model	O
will	O
be	O
extremely	O
small	O
.	O
in	O
the	O
speech	B
recognition	I
context	O
,	O
warping	O
of	O
the	O
time	O
axis	O
is	O
associated	O
with	O
natural	O
variations	O
in	O
the	O
speed	O
of	O
speech	O
,	O
and	O
again	O
the	O
hidden	O
markov	O
model	O
can	O
accommodate	O
such	O
a	O
distortion	O
and	O
not	O
penalize	O
it	O
too	O
heavily	O
.	O
13.2.1	O
maximum	B
likelihood	I
for	O
the	O
hmm	O
if	O
we	O
have	O
observed	O
a	O
data	O
set	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
,	O
we	O
can	O
determine	O
the	O
param-	O
eters	O
of	O
an	O
hmm	O
using	O
maximum	B
likelihood	I
.	O
the	O
likelihood	B
function	I
is	O
obtained	O
from	O
the	O
joint	O
distribution	O
(	O
13.10	O
)	O
by	O
marginalizing	O
over	O
the	O
latent	O
variables	O
(	O
cid:2	O
)	O
p	O
(	O
x|θ	O
)	O
=	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
.	O
(	O
13.11	O
)	O
z	O
because	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
does	O
not	O
factorize	O
over	O
n	O
(	O
in	O
contrast	O
to	O
the	O
mixture	B
distribution	I
considered	O
in	O
chapter	O
9	O
)	O
,	O
we	O
can	O
not	O
simply	O
treat	O
each	O
of	O
the	O
summations	O
over	O
zn	O
independently	O
.	O
nor	O
can	O
we	O
perform	O
the	O
summations	O
explicitly	O
because	O
there	O
are	O
n	O
variables	O
to	O
be	O
summed	O
over	O
,	O
each	O
of	O
which	O
has	O
k	O
states	O
,	O
re-	O
sulting	O
in	O
a	O
total	O
of	O
k	O
n	O
terms	O
.	O
thus	O
the	O
number	O
of	O
terms	O
in	O
the	O
summation	O
grows	O
616	O
13.	O
sequential	B
data	I
exponentially	O
with	O
the	O
length	O
of	O
the	O
chain	O
.	O
in	O
fact	O
,	O
the	O
summation	O
in	O
(	O
13.11	O
)	O
cor-	O
responds	O
to	O
summing	O
over	O
exponentially	O
many	O
paths	O
through	O
the	O
lattice	B
diagram	I
in	O
figure	O
13.7.	O
we	O
have	O
already	O
encountered	O
a	O
similar	O
difﬁculty	O
when	O
we	O
considered	O
the	O
infer-	O
ence	O
problem	O
for	O
the	O
simple	O
chain	O
of	O
variables	O
in	O
figure	O
8.32.	O
there	O
we	O
were	O
able	O
to	O
make	O
use	O
of	O
the	O
conditional	B
independence	I
properties	O
of	O
the	O
graph	O
to	O
re-order	O
the	O
summations	O
in	O
order	O
to	O
obtain	O
an	O
algorithm	O
whose	O
cost	O
scales	O
linearly	O
,	O
instead	O
of	O
exponentially	O
,	O
with	O
the	O
length	O
of	O
the	O
chain	O
.	O
we	O
shall	O
apply	O
a	O
similar	O
technique	O
to	O
the	O
hidden	O
markov	O
model	O
.	O
a	O
further	O
difﬁculty	O
with	O
the	O
expression	O
(	O
13.11	O
)	O
for	O
the	O
likelihood	B
function	I
is	O
that	O
,	O
because	O
it	O
corresponds	O
to	O
a	O
generalization	B
of	O
a	O
mixture	B
distribution	I
,	O
it	O
represents	O
a	O
summation	O
over	O
the	O
emission	O
models	O
for	O
different	O
settings	O
of	O
the	O
latent	O
variables	O
.	O
direct	O
maximization	O
of	O
the	O
likelihood	B
function	I
will	O
therefore	O
lead	O
to	O
complex	O
ex-	O
pressions	O
with	O
no	O
closed-form	O
solutions	O
,	O
as	O
was	O
the	O
case	O
for	O
simple	O
mixture	B
models	O
(	O
recall	O
that	O
a	O
mixture	B
model	I
for	O
i.i.d	O
.	O
data	O
is	O
a	O
special	O
case	O
of	O
the	O
hmm	O
)	O
.	O
section	O
9.2	O
we	O
therefore	O
turn	O
to	O
the	O
expectation	B
maximization	I
algorithm	O
to	O
ﬁnd	O
an	O
efﬁcient	O
framework	O
for	O
maximizing	O
the	O
likelihood	B
function	I
in	O
hidden	O
markov	O
models	O
.	O
the	O
em	O
algorithm	O
starts	O
with	O
some	O
initial	O
selection	O
for	O
the	O
model	O
parameters	O
,	O
which	O
we	O
denote	O
by	O
θold	O
.	O
in	O
the	O
e	O
step	O
,	O
we	O
take	O
these	O
parameter	O
values	O
and	O
ﬁnd	O
the	O
posterior	O
distribution	O
of	O
the	O
latent	O
variables	O
p	O
(	O
z|x	O
,	O
θold	O
)	O
.	O
we	O
then	O
use	O
this	O
posterior	O
distri-	O
bution	O
to	O
evaluate	O
the	O
expectation	B
of	O
the	O
logarithm	O
of	O
the	O
complete-data	O
likelihood	B
function	I
,	O
as	O
a	O
function	O
of	O
the	O
parameters	O
θ	O
,	O
to	O
give	O
the	O
function	O
q	O
(	O
θ	O
,	O
θold	O
)	O
deﬁned	O
by	O
(	O
cid:2	O
)	O
q	O
(	O
θ	O
,	O
θold	O
)	O
=	O
p	O
(	O
z|x	O
,	O
θold	O
)	O
ln	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
.	O
(	O
13.12	O
)	O
z	O
at	O
this	O
point	O
,	O
it	O
is	O
convenient	O
to	O
introduce	O
some	O
notation	O
.	O
we	O
shall	O
use	O
γ	O
(	O
zn	O
)	O
to	O
denote	O
the	O
marginal	B
posterior	O
distribution	O
of	O
a	O
latent	B
variable	I
zn	O
,	O
and	O
ξ	O
(	O
zn−1	O
,	O
zn	O
)	O
to	O
denote	O
the	O
joint	O
posterior	O
distribution	O
of	O
two	O
successive	O
latent	O
variables	O
,	O
so	O
that	O
γ	O
(	O
zn	O
)	O
=	O
p	O
(	O
zn|x	O
,	O
θold	O
)	O
ξ	O
(	O
zn−1	O
,	O
zn	O
)	O
=	O
p	O
(	O
zn−1	O
,	O
zn|x	O
,	O
θold	O
)	O
.	O
(	O
13.13	O
)	O
(	O
13.14	O
)	O
for	O
each	O
value	O
of	O
n	O
,	O
we	O
can	O
store	O
γ	O
(	O
zn	O
)	O
using	O
a	O
set	O
of	O
k	O
nonnegative	O
numbers	O
that	O
sum	O
to	O
unity	O
,	O
and	O
similarly	O
we	O
can	O
store	O
ξ	O
(	O
zn−1	O
,	O
zn	O
)	O
using	O
a	O
k	O
×	O
k	O
matrix	O
of	O
nonnegative	O
numbers	O
that	O
again	O
sum	O
to	O
unity	O
.	O
we	O
shall	O
also	O
use	O
γ	O
(	O
znk	O
)	O
to	O
denote	O
the	O
conditional	B
probability	I
of	O
znk	O
=	O
1	O
,	O
with	O
a	O
similar	O
use	O
of	O
notation	O
for	O
ξ	O
(	O
zn−1	O
,	O
j	O
,	O
znk	O
)	O
and	O
for	O
other	O
probabilistic	O
variables	O
introduced	O
later	O
.	O
because	O
the	O
expectation	B
of	O
a	O
binary	O
random	O
variable	O
is	O
just	O
the	O
probability	B
that	O
it	O
takes	O
the	O
value	O
1	O
,	O
we	O
have	O
γ	O
(	O
znk	O
)	O
=	O
e	O
[	O
znk	O
]	O
=	O
γ	O
(	O
z	O
)	O
znk	O
(	O
13.15	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
ξ	O
(	O
zn−1	O
,	O
j	O
,	O
znk	O
)	O
=	O
e	O
[	O
zn−1	O
,	O
jznk	O
]	O
=	O
z	O
γ	O
(	O
z	O
)	O
zn−1	O
,	O
jznk	O
.	O
(	O
13.16	O
)	O
if	O
we	O
substitute	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
given	O
by	O
(	O
13.10	O
)	O
into	O
(	O
13.12	O
)	O
,	O
z	O
exercise	O
13.5	O
exercise	O
13.6	O
13.2.	O
hidden	O
markov	O
models	O
617	O
and	O
make	O
use	O
of	O
the	O
deﬁnitions	O
of	O
γ	O
and	O
ξ	O
,	O
we	O
obtain	O
n	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
q	O
(	O
θ	O
,	O
θold	O
)	O
=	O
γ	O
(	O
z1k	O
)	O
ln	O
πk	O
+	O
ξ	O
(	O
zn−1	O
,	O
j	O
,	O
znk	O
)	O
ln	O
ajk	O
n=2	O
j=1	O
k=1	O
γ	O
(	O
znk	O
)	O
ln	O
p	O
(	O
xn|φk	O
)	O
.	O
(	O
13.17	O
)	O
k	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
k=1	O
+	O
k	O
(	O
cid:2	O
)	O
n=1	O
k=1	O
the	O
goal	O
of	O
the	O
e	O
step	O
will	O
be	O
to	O
evaluate	O
the	O
quantities	O
γ	O
(	O
zn	O
)	O
and	O
ξ	O
(	O
zn−1	O
,	O
zn	O
)	O
efﬁ-	O
ciently	O
,	O
and	O
we	O
shall	O
discuss	O
this	O
in	O
detail	O
shortly	O
.	O
in	O
the	O
m	O
step	O
,	O
we	O
maximize	O
q	O
(	O
θ	O
,	O
θold	O
)	O
with	O
respect	O
to	O
the	O
parameters	O
θ	O
=	O
{	O
π	O
,	O
a	O
,	O
φ	O
}	O
in	O
which	O
we	O
treat	O
γ	O
(	O
zn	O
)	O
and	O
ξ	O
(	O
zn−1	O
,	O
zn	O
)	O
as	O
constant	O
.	O
maximization	O
with	O
respect	O
to	O
π	O
and	O
a	O
is	O
easily	O
achieved	O
using	O
appropriate	O
lagrange	O
multipliers	O
with	O
the	O
results	O
(	O
13.18	O
)	O
πk	O
=	O
ajk	O
=	O
γ	O
(	O
z1j	O
)	O
γ	O
(	O
z1k	O
)	O
k	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=2	O
j=1	O
ξ	O
(	O
zn−1	O
,	O
j	O
,	O
znk	O
)	O
.	O
(	O
13.19	O
)	O
ξ	O
(	O
zn−1	O
,	O
j	O
,	O
znl	O
)	O
l=1	O
n=2	O
the	O
em	O
algorithm	O
must	O
be	O
initialized	O
by	O
choosing	O
starting	O
values	O
for	O
π	O
and	O
a	O
,	O
which	O
should	O
of	O
course	O
respect	O
the	O
summation	O
constraints	O
associated	O
with	O
their	O
probabilis-	O
tic	O
interpretation	O
.	O
note	O
that	O
any	O
elements	O
of	O
π	O
or	O
a	O
that	O
are	O
set	O
to	O
zero	O
initially	O
will	O
remain	O
zero	O
in	O
subsequent	O
em	O
updates	O
.	O
a	O
typical	O
initialization	O
procedure	O
would	O
involve	O
selecting	O
random	O
starting	O
values	O
for	O
these	O
parameters	O
subject	O
to	O
the	O
summa-	O
tion	O
and	O
non-negativity	O
constraints	O
.	O
note	O
that	O
no	O
particular	O
modiﬁcation	O
to	O
the	O
em	O
results	O
are	O
required	O
for	O
the	O
case	O
of	O
left-to-right	B
models	O
beyond	O
choosing	O
initial	O
values	O
for	O
the	O
elements	O
ajk	O
in	O
which	O
the	O
appropriate	O
elements	O
are	O
set	O
to	O
zero	O
,	O
because	O
these	O
will	O
remain	O
zero	O
throughout	O
.	O
to	O
maximize	O
q	O
(	O
θ	O
,	O
θold	O
)	O
with	O
respect	O
to	O
φk	O
,	O
we	O
notice	O
that	O
only	O
the	O
ﬁnal	O
term	O
in	O
(	O
13.17	O
)	O
depends	O
on	O
φk	O
,	O
and	O
furthermore	O
this	O
term	O
has	O
exactly	O
the	O
same	O
form	O
as	O
the	O
data-dependent	O
term	O
in	O
the	O
corresponding	O
function	O
for	O
a	O
standard	O
mixture	O
dis-	O
tribution	O
for	O
i.i.d	O
.	O
data	O
,	O
as	O
can	O
be	O
seen	O
by	O
comparison	O
with	O
(	O
9.40	O
)	O
for	O
the	O
case	O
of	O
a	O
gaussian	O
mixture	B
.	O
here	O
the	O
quantities	O
γ	O
(	O
znk	O
)	O
are	O
playing	O
the	O
role	O
of	O
the	O
responsibil-	O
ities	O
.	O
if	O
the	O
parameters	O
φk	O
are	O
independent	B
for	O
the	O
different	O
components	O
,	O
then	O
this	O
term	O
decouples	O
into	O
a	O
sum	O
of	O
terms	O
one	O
for	O
each	O
value	O
of	O
k	O
,	O
each	O
of	O
which	O
can	O
be	O
maximized	O
independently	O
.	O
we	O
are	O
then	O
simply	O
maximizing	O
the	O
weighted	O
log	O
likeli-	O
hood	O
function	O
for	O
the	O
emission	O
density	O
p	O
(	O
x|φk	O
)	O
with	O
weights	O
γ	O
(	O
znk	O
)	O
.	O
here	O
we	O
shall	O
suppose	O
that	O
this	O
maximization	O
can	O
be	O
done	O
efﬁciently	O
.	O
for	O
instance	O
,	O
in	O
the	O
case	O
of	O
618	O
13.	O
sequential	B
data	I
gaussian	O
emission	O
densities	O
we	O
have	O
p	O
(	O
x|φk	O
)	O
=	O
n	O
(	O
x|µk	O
,	O
σk	O
)	O
,	O
and	O
maximization	O
of	O
the	O
function	O
q	O
(	O
θ	O
,	O
θold	O
)	O
then	O
gives	O
γ	O
(	O
znk	O
)	O
xn	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
n=1	O
µk	O
=	O
γ	O
(	O
znk	O
)	O
(	O
13.20	O
)	O
γ	O
(	O
znk	O
)	O
(	O
xn	O
−	O
µk	O
)	O
(	O
xn	O
−	O
µk	O
)	O
t	O
σk	O
=	O
n=1	O
.	O
(	O
13.21	O
)	O
γ	O
(	O
znk	O
)	O
exercise	O
13.8	O
section	O
8.4	O
n	O
(	O
cid:2	O
)	O
n=1	O
d	O
(	O
cid:14	O
)	O
k	O
(	O
cid:14	O
)	O
i=1	O
k=1	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
γ	O
(	O
znk	O
)	O
xni	O
for	O
the	O
case	O
of	O
discrete	O
multinomial	O
observed	O
variables	O
,	O
the	O
conditional	B
distribution	O
of	O
the	O
observations	O
takes	O
the	O
form	O
p	O
(	O
x|z	O
)	O
=	O
µxizk	O
ik	O
(	O
13.22	O
)	O
and	O
the	O
corresponding	O
m-step	O
equations	O
are	O
given	O
by	O
µik	O
=	O
.	O
γ	O
(	O
znk	O
)	O
(	O
13.23	O
)	O
n=1	O
an	O
analogous	O
result	O
holds	O
for	O
bernoulli	O
observed	O
variables	O
.	O
the	O
em	O
algorithm	O
requires	O
initial	O
values	O
for	O
the	O
parameters	O
of	O
the	O
emission	O
dis-	O
tribution	O
.	O
one	O
way	O
to	O
set	O
these	O
is	O
ﬁrst	O
to	O
treat	O
the	O
data	O
initially	O
as	O
i.i.d	O
.	O
and	O
ﬁt	O
the	O
emission	O
density	O
by	O
maximum	B
likelihood	I
,	O
and	O
then	O
use	O
the	O
resulting	O
values	O
to	O
ini-	O
tialize	O
the	O
parameters	O
for	O
em	O
.	O
13.2.2	O
the	O
forward-backward	B
algorithm	I
next	O
we	O
seek	O
an	O
efﬁcient	O
procedure	O
for	O
evaluating	O
the	O
quantities	O
γ	O
(	O
znk	O
)	O
and	O
ξ	O
(	O
zn−1	O
,	O
j	O
,	O
znk	O
)	O
,	O
corresponding	O
to	O
the	O
e	O
step	O
of	O
the	O
em	O
algorithm	O
.	O
the	O
graph	O
for	O
the	O
hidden	O
markov	O
model	O
,	O
shown	O
in	O
figure	O
13.5	O
,	O
is	O
a	O
tree	B
,	O
and	O
so	O
we	O
know	O
that	O
the	O
posterior	O
distribution	O
of	O
the	O
latent	O
variables	O
can	O
be	O
obtained	O
efﬁciently	O
using	O
a	O
two-	O
stage	O
message	B
passing	I
algorithm	O
.	O
in	O
the	O
particular	O
context	O
of	O
the	O
hidden	O
markov	O
model	O
,	O
this	O
is	O
known	O
as	O
the	O
forward-backward	B
algorithm	I
(	O
rabiner	O
,	O
1989	O
)	O
,	O
or	O
the	O
baum-welch	O
algorithm	O
(	O
baum	O
,	O
1972	O
)	O
.	O
there	O
are	O
in	O
fact	O
several	O
variants	O
of	O
the	O
basic	O
algorithm	O
,	O
all	O
of	O
which	O
lead	O
to	O
the	O
exact	O
marginals	O
,	O
according	O
to	O
the	O
precise	O
form	O
of	O
13.2.	O
hidden	O
markov	O
models	O
619	O
the	O
messages	O
that	O
are	O
propagated	O
along	O
the	O
chain	O
(	O
jordan	O
,	O
2007	O
)	O
.	O
we	O
shall	O
focus	O
on	O
the	O
most	O
widely	O
used	O
of	O
these	O
,	O
known	O
as	O
the	O
alpha-beta	O
algorithm	O
.	O
as	O
well	O
as	O
being	O
of	O
great	O
practical	O
importance	O
in	O
its	O
own	O
right	O
,	O
the	O
forward-	O
backward	O
algorithm	O
provides	O
us	O
with	O
a	O
nice	O
illustration	O
of	O
many	O
of	O
the	O
concepts	O
introduced	O
in	O
earlier	O
chapters	O
.	O
we	O
shall	O
therefore	O
begin	O
in	O
this	O
section	O
with	O
a	O
‘	O
con-	O
ventional	O
’	O
derivation	O
of	O
the	O
forward-backward	O
equations	O
,	O
making	O
use	O
of	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
,	O
and	O
exploiting	O
conditional	B
independence	I
properties	O
which	O
we	O
shall	O
obtain	O
from	O
the	O
corresponding	O
graphical	B
model	I
using	O
d-separation	B
.	O
then	O
in	O
section	O
13.2.3	O
,	O
we	O
shall	O
see	O
how	O
the	O
forward-backward	B
algorithm	I
can	O
be	O
obtained	O
very	O
simply	O
as	O
a	O
speciﬁc	O
example	O
of	O
the	O
sum-product	B
algorithm	I
introduced	O
in	O
section	O
8.4.4.	O
it	O
is	O
worth	O
emphasizing	O
that	O
evaluation	O
of	O
the	O
posterior	O
distributions	O
of	O
the	O
latent	O
variables	O
is	O
independent	B
of	O
the	O
form	O
of	O
the	O
emission	O
density	O
p	O
(	O
x|z	O
)	O
or	O
indeed	O
of	O
whether	O
the	O
observed	O
variables	O
are	O
continuous	O
or	O
discrete	O
.	O
all	O
we	O
require	O
is	O
the	O
values	O
of	O
the	O
quantities	O
p	O
(	O
xn|zn	O
)	O
for	O
each	O
value	O
of	O
zn	O
for	O
every	O
n.	O
also	O
,	O
in	O
this	O
section	O
and	O
the	O
next	O
we	O
shall	O
omit	O
the	O
explicit	O
dependence	O
on	O
the	O
model	O
parameters	O
θold	O
because	O
these	O
ﬁxed	O
throughout	O
.	O
we	O
therefore	O
begin	O
by	O
writing	O
down	O
the	O
following	O
conditional	B
independence	I
properties	O
(	O
jordan	O
,	O
2007	O
)	O
p	O
(	O
x|zn	O
)	O
=	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn|zn	O
)	O
p	O
(	O
xn	O
+1|x	O
,	O
zn	O
+1	O
)	O
=	O
p	O
(	O
xn	O
+1|zn	O
+1	O
)	O
p	O
(	O
zn	O
+1|zn	O
,	O
x	O
)	O
=	O
p	O
(	O
zn	O
+1|zn	O
)	O
p	O
(	O
xn+1	O
,	O
.	O
.	O
.	O
,	O
xn|zn	O
)	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn−1|xn	O
,	O
zn	O
)	O
=	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn−1|zn	O
)	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn−1|zn−1	O
,	O
zn	O
)	O
=	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn−1|zn−1	O
)	O
p	O
(	O
xn+1	O
,	O
.	O
.	O
.	O
,	O
xn|zn	O
,	O
zn+1	O
)	O
=	O
p	O
(	O
xn+1	O
,	O
.	O
.	O
.	O
,	O
xn|zn+1	O
)	O
p	O
(	O
xn+2	O
,	O
.	O
.	O
.	O
,	O
xn|zn+1	O
,	O
xn+1	O
)	O
=	O
p	O
(	O
xn+2	O
,	O
.	O
.	O
.	O
,	O
xn|zn+1	O
)	O
p	O
(	O
x|zn−1	O
,	O
zn	O
)	O
=	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn−1|zn−1	O
)	O
(	O
13.24	O
)	O
(	O
13.25	O
)	O
(	O
13.26	O
)	O
(	O
13.27	O
)	O
(	O
13.28	O
)	O
p	O
(	O
xn|zn	O
)	O
p	O
(	O
xn+1	O
,	O
.	O
.	O
.	O
,	O
xn|zn	O
)	O
(	O
13.29	O
)	O
(	O
13.30	O
)	O
(	O
13.31	O
)	O
where	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
.	O
these	O
relations	O
are	O
most	O
easily	O
proved	O
using	O
d-separation	B
.	O
for	O
instance	O
in	O
the	O
ﬁrst	O
of	O
these	O
results	O
,	O
we	O
note	O
that	O
every	O
path	O
from	O
any	O
one	O
of	O
the	O
nodes	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn−1	O
to	O
the	O
node	B
xn	O
passes	O
through	O
the	O
node	B
zn	O
,	O
which	O
is	O
observed	O
.	O
because	O
all	O
such	O
paths	O
are	O
head-to-tail	O
,	O
it	O
follows	O
that	O
the	O
conditional	B
independence	I
property	O
must	O
hold	O
.	O
the	O
reader	O
should	O
take	O
a	O
few	O
moments	O
to	O
verify	O
each	O
of	O
these	O
properties	O
in	O
turn	O
,	O
as	O
an	O
exercise	O
in	O
the	O
application	O
of	O
d-separation	B
.	O
these	O
relations	O
can	O
also	O
be	O
proved	O
directly	O
,	O
though	O
with	O
signiﬁcantly	O
greater	O
effort	O
,	O
from	O
the	O
joint	O
distribution	O
for	O
the	O
hidden	O
markov	O
model	O
using	O
the	O
sum	O
and	O
product	O
rules	O
of	O
proba-	O
bility	O
.	O
let	O
us	O
begin	O
by	O
evaluating	O
γ	O
(	O
znk	O
)	O
.	O
recall	O
that	O
for	O
a	O
discrete	O
multinomial	O
ran-	O
dom	O
variable	O
the	O
expected	O
value	O
of	O
one	O
of	O
its	O
components	O
is	O
just	O
the	O
probability	B
of	O
that	O
component	O
having	O
the	O
value	O
1.	O
thus	O
we	O
are	O
interested	O
in	O
ﬁnding	O
the	O
posterior	O
distribution	O
p	O
(	O
zn|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
of	O
zn	O
given	O
the	O
observed	O
data	O
set	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
.	O
this	O
exercise	O
13.10	O
620	O
13.	O
sequential	B
data	I
represents	O
a	O
vector	O
of	O
length	O
k	O
whose	O
entries	O
correspond	O
to	O
the	O
expected	O
values	O
of	O
znk	O
.	O
using	O
bayes	O
’	O
theorem	O
,	O
we	O
have	O
γ	O
(	O
zn	O
)	O
=	O
p	O
(	O
zn|x	O
)	O
=	O
p	O
(	O
x|zn	O
)	O
p	O
(	O
zn	O
)	O
p	O
(	O
x	O
)	O
.	O
(	O
13.32	O
)	O
note	O
that	O
the	O
denominator	O
p	O
(	O
x	O
)	O
is	O
implicitly	O
conditioned	O
on	O
the	O
parameters	O
θold	O
of	O
the	O
hmm	O
and	O
hence	O
represents	O
the	O
likelihood	B
function	I
.	O
using	O
the	O
conditional	B
independence	I
property	O
(	O
13.24	O
)	O
,	O
together	O
with	O
the	O
product	B
rule	I
of	I
probability	I
,	O
we	O
obtain	O
γ	O
(	O
zn	O
)	O
=	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
zn	O
)	O
p	O
(	O
xn+1	O
,	O
.	O
.	O
.	O
,	O
xn|zn	O
)	O
p	O
(	O
x	O
)	O
=	O
α	O
(	O
zn	O
)	O
β	O
(	O
zn	O
)	O
p	O
(	O
x	O
)	O
(	O
13.33	O
)	O
where	O
we	O
have	O
deﬁned	O
α	O
(	O
zn	O
)	O
≡	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
zn	O
)	O
β	O
(	O
zn	O
)	O
≡	O
p	O
(	O
xn+1	O
,	O
.	O
.	O
.	O
,	O
xn|zn	O
)	O
.	O
(	O
13.34	O
)	O
(	O
13.35	O
)	O
the	O
quantity	O
α	O
(	O
zn	O
)	O
represents	O
the	O
joint	O
probability	B
of	O
observing	O
all	O
of	O
the	O
given	O
data	O
up	O
to	O
time	O
n	O
and	O
the	O
value	O
of	O
zn	O
,	O
whereas	O
β	O
(	O
zn	O
)	O
represents	O
the	O
conditional	B
probability	I
of	O
all	O
future	O
data	O
from	O
time	O
n	O
+	O
1	O
up	O
to	O
n	O
given	O
the	O
value	O
of	O
zn	O
.	O
again	O
,	O
α	O
(	O
zn	O
)	O
and	O
β	O
(	O
zn	O
)	O
each	O
represent	O
set	O
of	O
k	O
numbers	O
,	O
one	O
for	O
each	O
of	O
the	O
possible	O
settings	O
of	O
the	O
1-of-k	O
coded	O
binary	O
vector	O
zn	O
.	O
we	O
shall	O
use	O
the	O
notation	O
α	O
(	O
znk	O
)	O
to	O
denote	O
the	O
value	O
of	O
α	O
(	O
zn	O
)	O
when	O
znk	O
=	O
1	O
,	O
with	O
an	O
analogous	O
interpretation	O
of	O
β	O
(	O
znk	O
)	O
.	O
we	O
now	O
derive	O
recursion	O
relations	O
that	O
allow	O
α	O
(	O
zn	O
)	O
and	O
β	O
(	O
zn	O
)	O
to	O
be	O
evaluated	O
efﬁciently	O
.	O
again	O
,	O
we	O
shall	O
make	O
use	O
of	O
conditional	B
independence	I
properties	O
,	O
in	O
particular	O
(	O
13.25	O
)	O
and	O
(	O
13.26	O
)	O
,	O
together	O
with	O
the	O
sum	O
and	O
product	O
rules	O
,	O
allowing	O
us	O
to	O
express	O
α	O
(	O
zn	O
)	O
in	O
terms	O
of	O
α	O
(	O
zn−1	O
)	O
as	O
follows	O
α	O
(	O
zn	O
)	O
=	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
zn	O
)	O
=	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn|zn	O
)	O
p	O
(	O
zn	O
)	O
=	O
p	O
(	O
xn|zn	O
)	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn−1|zn	O
)	O
p	O
(	O
zn	O
)	O
=	O
p	O
(	O
xn|zn	O
)	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn−1	O
,	O
zn	O
)	O
=	O
p	O
(	O
xn|zn	O
)	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn−1	O
,	O
zn−1	O
,	O
zn	O
)	O
zn−1	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
zn−1	O
zn−1	O
zn−1	O
=	O
p	O
(	O
xn|zn	O
)	O
=	O
p	O
(	O
xn|zn	O
)	O
=	O
p	O
(	O
xn|zn	O
)	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn−1	O
,	O
zn|zn−1	O
)	O
p	O
(	O
zn−1	O
)	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn−1|zn−1	O
)	O
p	O
(	O
zn|zn−1	O
)	O
p	O
(	O
zn−1	O
)	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn−1	O
,	O
zn−1	O
)	O
p	O
(	O
zn|zn−1	O
)	O
making	O
use	O
of	O
the	O
deﬁnition	O
(	O
13.34	O
)	O
for	O
α	O
(	O
zn	O
)	O
,	O
we	O
then	O
obtain	O
α	O
(	O
zn−1	O
)	O
p	O
(	O
zn|zn−1	O
)	O
.	O
α	O
(	O
zn	O
)	O
=	O
p	O
(	O
xn|zn	O
)	O
(	O
13.36	O
)	O
(	O
cid:2	O
)	O
zn−1	O
13.2.	O
hidden	O
markov	O
models	O
621	O
figure	O
13.12	O
illustration	O
of	O
the	O
forward	O
recursion	O
(	O
13.36	O
)	O
for	O
evaluation	O
of	O
the	O
α	O
variables	O
.	O
in	O
this	O
fragment	O
of	O
the	O
lattice	O
,	O
we	O
see	O
that	O
the	O
quantity	O
α	O
(	O
zn1	O
)	O
is	O
obtained	O
by	O
taking	O
the	O
elements	O
α	O
(	O
zn−1	O
,	O
j	O
)	O
of	O
α	O
(	O
zn−1	O
)	O
at	O
step	O
n−1	O
and	O
summing	O
them	O
up	O
with	O
weights	O
given	O
by	O
aj1	O
,	O
corresponding	O
to	O
the	O
val-	O
ues	O
of	O
p	O
(	O
zn|zn−1	O
)	O
,	O
and	O
then	O
multiplying	O
by	O
the	O
data	O
contribution	O
p	O
(	O
xn|zn1	O
)	O
.	O
α	O
(	O
zn−1,1	O
)	O
α	O
(	O
zn,1	O
)	O
k	O
=	O
1	O
a11	O
a21	O
p	O
(	O
xn|zn,1	O
)	O
α	O
(	O
zn−1,2	O
)	O
k	O
=	O
2	O
α	O
(	O
zn−1,3	O
)	O
k	O
=	O
3	O
n	O
−	O
1	O
a31	O
n	O
it	O
is	O
worth	O
taking	O
a	O
moment	O
to	O
study	O
this	O
recursion	O
relation	O
in	O
some	O
detail	O
.	O
note	O
that	O
there	O
are	O
k	O
terms	O
in	O
the	O
summation	O
,	O
and	O
the	O
right-hand	O
side	O
has	O
to	O
be	O
evaluated	O
for	O
each	O
of	O
the	O
k	O
values	O
of	O
zn	O
so	O
each	O
step	O
of	O
the	O
α	B
recursion	I
has	O
computational	O
cost	O
that	O
scaled	O
like	O
o	O
(	O
k	O
2	O
)	O
.	O
the	O
forward	O
recursion	O
equation	O
for	O
α	O
(	O
zn	O
)	O
is	O
illustrated	O
using	O
a	O
lattice	B
diagram	I
in	O
figure	O
13.12.	O
in	O
order	O
to	O
start	O
this	O
recursion	O
,	O
we	O
need	O
an	O
initial	O
condition	O
that	O
is	O
given	O
by	O
zn+1	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
zn+1	O
zn+1	O
zn+1	O
=	O
=	O
=	O
=	O
p	O
(	O
xn+1	O
,	O
.	O
.	O
.	O
,	O
xn|zn	O
,	O
zn+1	O
)	O
p	O
(	O
zn+1|zn	O
)	O
p	O
(	O
xn+1	O
,	O
.	O
.	O
.	O
,	O
xn|zn+1	O
)	O
p	O
(	O
zn+1|zn	O
)	O
p	O
(	O
xn+2	O
,	O
.	O
.	O
.	O
,	O
xn|zn+1	O
)	O
p	O
(	O
xn+1|zn+1	O
)	O
p	O
(	O
zn+1|zn	O
)	O
.	O
k	O
(	O
cid:14	O
)	O
k=1	O
α	O
(	O
z1	O
)	O
=	O
p	O
(	O
x1	O
,	O
z1	O
)	O
=	O
p	O
(	O
z1	O
)	O
p	O
(	O
x1|z1	O
)	O
=	O
{	O
πkp	O
(	O
x1|φk	O
)	O
}	O
z1k	O
(	O
13.37	O
)	O
which	O
tells	O
us	O
that	O
α	O
(	O
z1k	O
)	O
,	O
for	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
,	O
takes	O
the	O
value	O
πkp	O
(	O
x1|φk	O
)	O
.	O
starting	O
at	O
the	O
ﬁrst	O
node	O
of	O
the	O
chain	O
,	O
we	O
can	O
then	O
work	O
along	O
the	O
chain	O
and	O
evaluate	O
α	O
(	O
zn	O
)	O
for	O
every	O
latent	O
node	O
.	O
because	O
each	O
step	O
of	O
the	O
recursion	O
involves	O
multiplying	O
by	O
a	O
k	O
×	O
k	O
matrix	O
,	O
the	O
overall	O
cost	O
of	O
evaluating	O
these	O
quantities	O
for	O
the	O
whole	O
chain	O
is	O
of	O
o	O
(	O
k	O
2n	O
)	O
.	O
we	O
can	O
similarly	O
ﬁnd	O
a	O
recursion	O
relation	O
for	O
the	O
quantities	O
β	O
(	O
zn	O
)	O
by	O
making	O
use	O
of	O
the	O
conditional	B
independence	I
properties	O
(	O
13.27	O
)	O
and	O
(	O
13.28	O
)	O
giving	O
β	O
(	O
zn	O
)	O
=	O
p	O
(	O
xn+1	O
,	O
.	O
.	O
.	O
,	O
xn|zn	O
)	O
p	O
(	O
xn+1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
zn+1|zn	O
)	O
622	O
13.	O
sequential	B
data	I
figure	O
13.13	O
illustration	O
of	O
the	O
backward	O
recursion	O
(	O
13.38	O
)	O
for	O
evaluation	O
of	O
the	O
β	O
variables	O
.	O
in	O
this	O
fragment	O
of	O
the	O
lattice	O
,	O
we	O
see	O
that	O
the	O
quantity	O
β	O
(	O
zn1	O
)	O
is	O
obtained	O
by	O
taking	O
the	O
components	O
β	O
(	O
zn+1	O
,	O
k	O
)	O
of	O
β	O
(	O
zn+1	O
)	O
at	O
step	O
n	O
+	O
1	O
and	O
summing	O
them	O
up	O
with	O
weights	O
given	O
by	O
the	O
products	O
of	O
a1k	O
,	O
correspond-	O
ing	O
to	O
the	O
values	O
of	O
p	O
(	O
zn+1|zn	O
)	O
and	O
the	O
cor-	O
responding	O
values	O
of	O
the	O
emission	O
density	O
p	O
(	O
xn|zn+1	O
,	O
k	O
)	O
.	O
β	O
(	O
zn,1	O
)	O
β	O
(	O
zn+1,1	O
)	O
k	O
=	O
1	O
a11	O
a12	O
p	O
(	O
xn|zn+1,1	O
)	O
β	O
(	O
zn+1,2	O
)	O
k	O
=	O
2	O
a13	O
k	O
=	O
3	O
n	O
p	O
(	O
xn|zn+1,2	O
)	O
β	O
(	O
zn+1,3	O
)	O
n	O
+	O
1	O
p	O
(	O
xn|zn+1,3	O
)	O
(	O
cid:2	O
)	O
making	O
use	O
of	O
the	O
deﬁnition	O
(	O
13.35	O
)	O
for	O
β	O
(	O
zn	O
)	O
,	O
we	O
then	O
obtain	O
β	O
(	O
zn	O
)	O
=	O
β	O
(	O
zn+1	O
)	O
p	O
(	O
xn+1|zn+1	O
)	O
p	O
(	O
zn+1|zn	O
)	O
.	O
(	O
13.38	O
)	O
zn+1	O
note	O
that	O
in	O
this	O
case	O
we	O
have	O
a	O
backward	O
message	B
passing	I
algorithm	O
that	O
evaluates	O
β	O
(	O
zn	O
)	O
in	O
terms	O
of	O
β	O
(	O
zn+1	O
)	O
.	O
at	O
each	O
step	O
,	O
we	O
absorb	O
the	O
effect	O
of	O
observation	O
xn+1	O
through	O
the	O
emission	B
probability	I
p	O
(	O
xn+1|zn+1	O
)	O
,	O
multiply	O
by	O
the	O
transition	O
matrix	O
p	O
(	O
zn+1|zn	O
)	O
,	O
and	O
then	O
marginalize	O
out	O
zn+1	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
13.13.	O
again	O
we	O
need	O
a	O
starting	O
condition	O
for	O
the	O
recursion	O
,	O
namely	O
a	O
value	O
for	O
β	O
(	O
zn	O
)	O
.	O
this	O
can	O
be	O
obtained	O
by	O
setting	O
n	O
=	O
n	O
in	O
(	O
13.33	O
)	O
and	O
replacing	O
α	O
(	O
zn	O
)	O
with	O
its	O
deﬁnition	O
(	O
13.34	O
)	O
to	O
give	O
p	O
(	O
zn|x	O
)	O
=	O
p	O
(	O
x	O
,	O
zn	O
)	O
β	O
(	O
zn	O
)	O
p	O
(	O
x	O
)	O
(	O
13.39	O
)	O
which	O
we	O
see	O
will	O
be	O
correct	O
provided	O
we	O
take	O
β	O
(	O
zn	O
)	O
=	O
1	O
for	O
all	O
settings	O
of	O
zn	O
.	O
in	O
the	O
m	O
step	O
equations	O
,	O
the	O
quantity	O
p	O
(	O
x	O
)	O
will	O
cancel	O
out	O
,	O
as	O
can	O
be	O
seen	O
,	O
for	O
instance	O
,	O
in	O
the	O
m-step	O
equation	O
for	O
µk	O
given	O
by	O
(	O
13.20	O
)	O
,	O
which	O
takes	O
the	O
form	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
γ	O
(	O
znk	O
)	O
xn	O
α	O
(	O
znk	O
)	O
β	O
(	O
znk	O
)	O
xn	O
µk	O
=	O
=	O
γ	O
(	O
znk	O
)	O
.	O
(	O
13.40	O
)	O
α	O
(	O
znk	O
)	O
β	O
(	O
znk	O
)	O
n=1	O
n=1	O
however	O
,	O
the	O
quantity	O
p	O
(	O
x	O
)	O
represents	O
the	O
likelihood	B
function	I
whose	O
value	O
we	O
typ-	O
ically	O
wish	O
to	O
monitor	O
during	O
the	O
em	O
optimization	O
,	O
and	O
so	O
it	O
is	O
useful	O
to	O
be	O
able	O
to	O
evaluate	O
it	O
.	O
if	O
we	O
sum	O
both	O
sides	O
of	O
(	O
13.33	O
)	O
over	O
zn	O
,	O
and	O
use	O
the	O
fact	O
that	O
the	O
left-hand	O
side	O
is	O
a	O
normalized	O
distribution	O
,	O
we	O
obtain	O
p	O
(	O
x	O
)	O
=	O
α	O
(	O
zn	O
)	O
β	O
(	O
zn	O
)	O
.	O
(	O
13.41	O
)	O
(	O
cid:2	O
)	O
zn	O
13.2.	O
hidden	O
markov	O
models	O
623	O
thus	O
we	O
can	O
evaluate	O
the	O
likelihood	B
function	I
by	O
computing	O
this	O
sum	O
,	O
for	O
any	O
conve-	O
nient	O
choice	O
of	O
n.	O
for	O
instance	O
,	O
if	O
we	O
only	O
want	O
to	O
evaluate	O
the	O
likelihood	B
function	I
,	O
then	O
we	O
can	O
do	O
this	O
by	O
running	O
the	O
α	B
recursion	I
from	O
the	O
start	O
to	O
the	O
end	O
of	O
the	O
chain	O
,	O
and	O
then	O
use	O
this	O
result	O
for	O
n	O
=	O
n	O
,	O
making	O
use	O
of	O
the	O
fact	O
that	O
β	O
(	O
zn	O
)	O
is	O
a	O
vector	O
of	O
1s	O
.	O
in	O
this	O
case	O
no	O
β	O
recursion	O
is	O
required	O
,	O
and	O
we	O
simply	O
have	O
p	O
(	O
x	O
)	O
=	O
α	O
(	O
zn	O
)	O
.	O
(	O
13.42	O
)	O
(	O
cid:2	O
)	O
zn	O
let	O
us	O
take	O
a	O
moment	O
to	O
interpret	O
this	O
result	O
for	O
p	O
(	O
x	O
)	O
.	O
recall	O
that	O
to	O
compute	O
the	O
likelihood	O
we	O
should	O
take	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
z	O
)	O
and	O
sum	O
over	O
all	O
possible	O
values	O
of	O
z.	O
each	O
such	O
value	O
represents	O
a	O
particular	O
choice	O
of	O
hidden	O
state	O
for	O
every	O
time	O
step	O
,	O
in	O
other	O
words	O
every	O
term	O
in	O
the	O
summation	O
is	O
a	O
path	O
through	O
the	O
lattice	B
diagram	I
,	O
and	O
recall	O
that	O
there	O
are	O
exponentially	O
many	O
such	O
paths	O
.	O
by	O
expressing	O
the	O
likelihood	B
function	I
in	O
the	O
form	O
(	O
13.42	O
)	O
,	O
we	O
have	O
reduced	O
the	O
computational	O
cost	O
from	O
being	O
exponential	O
in	O
the	O
length	O
of	O
the	O
chain	O
to	O
being	O
linear	O
by	O
swapping	O
the	O
order	O
of	O
the	O
summation	O
and	O
multiplications	O
,	O
so	O
that	O
at	O
each	O
time	O
step	O
n	O
we	O
sum	O
the	O
contributions	O
from	O
all	O
paths	O
passing	O
through	O
each	O
of	O
the	O
states	O
znk	O
to	O
give	O
the	O
intermediate	O
quantities	O
α	O
(	O
zn	O
)	O
.	O
next	O
we	O
consider	O
the	O
evaluation	O
of	O
the	O
quantities	O
ξ	O
(	O
zn−1	O
,	O
zn	O
)	O
,	O
which	O
correspond	O
to	O
the	O
values	O
of	O
the	O
conditional	B
probabilities	O
p	O
(	O
zn−1	O
,	O
zn|x	O
)	O
for	O
each	O
of	O
the	O
k	O
×	O
k	O
settings	O
for	O
(	O
zn−1	O
,	O
zn	O
)	O
.	O
using	O
the	O
deﬁnition	O
of	O
ξ	O
(	O
zn−1	O
,	O
zn	O
)	O
,	O
and	O
applying	O
bayes	O
’	O
theorem	O
,	O
we	O
have	O
ξ	O
(	O
zn−1	O
,	O
zn	O
)	O
=	O
p	O
(	O
zn−1	O
,	O
zn|x	O
)	O
=	O
p	O
(	O
x|zn−1	O
,	O
zn	O
)	O
p	O
(	O
zn−1	O
,	O
zn	O
)	O
=	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn−1|zn−1	O
)	O
p	O
(	O
xn|zn	O
)	O
p	O
(	O
xn+1	O
,	O
.	O
.	O
.	O
,	O
xn|zn	O
)	O
p	O
(	O
zn|zn−1	O
)	O
p	O
(	O
zn−1	O
)	O
=	O
α	O
(	O
zn−1	O
)	O
p	O
(	O
xn|zn	O
)	O
p	O
(	O
zn|zn−1	O
)	O
β	O
(	O
zn	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
(	O
13.43	O
)	O
p	O
(	O
x	O
)	O
where	O
we	O
have	O
made	O
use	O
of	O
the	O
conditional	B
independence	I
property	O
(	O
13.29	O
)	O
together	O
with	O
the	O
deﬁnitions	O
of	O
α	O
(	O
zn	O
)	O
and	O
β	O
(	O
zn	O
)	O
given	O
by	O
(	O
13.34	O
)	O
and	O
(	O
13.35	O
)	O
.	O
thus	O
we	O
can	O
calculate	O
the	O
ξ	O
(	O
zn−1	O
,	O
zn	O
)	O
directly	O
by	O
using	O
the	O
results	O
of	O
the	O
α	O
and	O
β	O
recursions	O
.	O
let	O
us	O
summarize	O
the	O
steps	O
required	O
to	O
train	O
a	O
hidden	O
markov	O
model	O
using	O
the	O
em	O
algorithm	O
.	O
we	O
ﬁrst	O
make	O
an	O
initial	O
selection	O
of	O
the	O
parameters	O
θold	O
where	O
θ	O
≡	O
(	O
π	O
,	O
a	O
,	O
φ	O
)	O
.	O
the	O
a	O
and	O
π	O
parameters	O
are	O
often	O
initialized	O
either	O
uniformly	O
or	O
randomly	O
from	O
a	O
uniform	B
distribution	I
(	O
respecting	O
their	O
non-negativity	O
and	O
summa-	O
tion	O
constraints	O
)	O
.	O
initialization	O
of	O
the	O
parameters	O
φ	O
will	O
depend	O
on	O
the	O
form	O
of	O
the	O
distribution	O
.	O
for	O
instance	O
in	O
the	O
case	O
of	O
gaussians	O
,	O
the	O
parameters	O
µk	O
might	O
be	O
ini-	O
tialized	O
by	O
applying	O
the	O
k-means	O
algorithm	O
to	O
the	O
data	O
,	O
and	O
σk	O
might	O
be	O
initialized	O
to	O
the	O
covariance	B
matrix	I
of	O
the	O
corresponding	O
k	O
means	O
cluster	O
.	O
then	O
we	O
run	O
both	O
the	O
forward	O
α	O
recursion	O
and	O
the	O
backward	O
β	O
recursion	O
and	O
use	O
the	O
results	O
to	O
evaluate	O
γ	O
(	O
zn	O
)	O
and	O
ξ	O
(	O
zn−1	O
,	O
zn	O
)	O
.	O
at	O
this	O
stage	O
,	O
we	O
can	O
also	O
evaluate	O
the	O
likelihood	B
function	I
.	O
624	O
13.	O
sequential	B
data	I
this	O
completes	O
the	O
e	O
step	O
,	O
and	O
we	O
use	O
the	O
results	O
to	O
ﬁnd	O
a	O
revised	O
set	O
of	O
parameters	O
θnew	O
using	O
the	O
m-step	O
equations	O
from	O
section	O
13.2.1.	O
we	O
then	O
continue	O
to	O
alternate	O
between	O
e	O
and	O
m	O
steps	O
until	O
some	O
convergence	O
criterion	O
is	O
satisﬁed	O
,	O
for	O
instance	O
when	O
the	O
change	O
in	O
the	O
likelihood	B
function	I
is	O
below	O
some	O
threshold	O
.	O
note	O
that	O
in	O
these	O
recursion	O
relations	O
the	O
observations	O
enter	O
through	O
conditional	B
distributions	O
of	O
the	O
form	O
p	O
(	O
xn|zn	O
)	O
.	O
the	O
recursions	O
are	O
therefore	O
independent	B
of	O
the	O
type	O
or	O
dimensionality	O
of	O
the	O
observed	O
variables	O
or	O
the	O
form	O
of	O
this	O
conditional	B
distribution	O
,	O
so	O
long	O
as	O
its	O
value	O
can	O
be	O
computed	O
for	O
each	O
of	O
the	O
k	O
possible	O
states	O
of	O
zn	O
.	O
since	O
the	O
observed	O
variables	O
{	O
xn	O
}	O
are	O
ﬁxed	O
,	O
the	O
quantities	O
p	O
(	O
xn|zn	O
)	O
can	O
be	O
pre-computed	O
as	O
functions	O
of	O
zn	O
at	O
the	O
start	O
of	O
the	O
em	O
algorithm	O
,	O
and	O
remain	O
ﬁxed	O
throughout	O
.	O
exercise	O
13.12	O
we	O
have	O
seen	O
in	O
earlier	O
chapters	O
that	O
the	O
maximum	B
likelihood	I
approach	O
is	O
most	O
effective	O
when	O
the	O
number	O
of	O
data	O
points	O
is	O
large	O
in	O
relation	O
to	O
the	O
number	O
of	O
parame-	O
ters	O
.	O
here	O
we	O
note	O
that	O
a	O
hidden	O
markov	O
model	O
can	O
be	O
trained	O
effectively	O
,	O
using	O
max-	O
imum	O
likelihood	O
,	O
provided	O
the	O
training	B
sequence	O
is	O
sufﬁciently	O
long	O
.	O
alternatively	O
,	O
we	O
can	O
make	O
use	O
of	O
multiple	O
shorter	O
sequences	O
,	O
which	O
requires	O
a	O
straightforward	O
modiﬁcation	O
of	O
the	O
hidden	O
markov	O
model	O
em	O
algorithm	O
.	O
in	O
the	O
case	O
of	O
left-to-right	B
models	O
,	O
this	O
is	O
particularly	O
important	O
because	O
,	O
in	O
a	O
given	O
observation	O
sequence	O
,	O
a	O
given	O
state	O
transition	O
corresponding	O
to	O
a	O
nondiagonal	O
element	O
of	O
a	O
will	O
seen	O
at	O
most	O
once	O
.	O
another	O
quantity	O
of	O
interest	O
is	O
the	O
predictive	B
distribution	I
,	O
in	O
which	O
the	O
observed	O
data	O
is	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
and	O
we	O
wish	O
to	O
predict	O
xn	O
+1	O
,	O
which	O
would	O
be	O
important	O
for	O
real-time	O
applications	O
such	O
as	O
ﬁnancial	O
forecasting	O
.	O
again	O
we	O
make	O
use	O
of	O
the	O
sum	O
and	O
product	O
rules	O
together	O
with	O
the	O
conditional	B
independence	I
properties	O
(	O
13.29	O
)	O
and	O
(	O
13.31	O
)	O
giving	O
p	O
(	O
xn	O
+1|x	O
)	O
=	O
=	O
=	O
=	O
=	O
=	O
zn+1	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
zn+1	O
zn+1	O
zn+1	O
zn+1	O
1	O
p	O
(	O
x	O
)	O
p	O
(	O
xn	O
+1	O
,	O
zn	O
+1|x	O
)	O
p	O
(	O
xn	O
+1|zn	O
+1	O
)	O
p	O
(	O
zn	O
+1|x	O
)	O
p	O
(	O
xn	O
+1|zn	O
+1	O
)	O
p	O
(	O
xn	O
+1|zn	O
+1	O
)	O
p	O
(	O
zn	O
+1	O
,	O
zn|x	O
)	O
p	O
(	O
zn	O
+1|zn	O
)	O
p	O
(	O
zn|x	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
zn	O
zn	O
zn	O
p	O
(	O
xn	O
+1|zn	O
+1	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
zn	O
+1|zn	O
)	O
p	O
(	O
zn	O
,	O
x	O
)	O
p	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
xn	O
+1|zn	O
+1	O
)	O
p	O
(	O
zn	O
+1|zn	O
)	O
α	O
(	O
zn	O
)	O
(	O
13.44	O
)	O
zn+1	O
zn	O
which	O
can	O
be	O
evaluated	O
by	O
ﬁrst	O
running	O
a	O
forward	O
α	O
recursion	O
and	O
then	O
computing	O
the	O
ﬁnal	O
summations	O
over	O
zn	O
and	O
zn	O
+1	O
.	O
the	O
result	O
of	O
the	O
ﬁrst	O
summation	O
over	O
zn	O
can	O
be	O
stored	O
and	O
used	O
once	O
the	O
value	O
of	O
xn	O
+1	O
is	O
observed	O
in	O
order	O
to	O
run	O
the	O
α	B
recursion	I
forward	O
to	O
the	O
next	O
step	O
in	O
order	O
to	O
predict	O
the	O
subsequent	O
value	O
xn	O
+2	O
.	O
figure	O
13.14	O
a	O
fragment	O
of	O
the	O
fac-	O
tor	O
graph	O
representation	O
for	O
the	O
hidden	O
markov	O
model	O
.	O
χ	O
z1	O
13.2.	O
hidden	O
markov	O
models	O
625	O
zn−1	O
ψn	O
zn	O
g1	O
gn−1	O
gn	O
x1	O
xn−1	O
xn	O
note	O
that	O
in	O
(	O
13.44	O
)	O
,	O
the	O
inﬂuence	O
of	O
all	O
data	O
from	O
x1	O
to	O
xn	O
is	O
summarized	O
in	O
the	O
k	O
values	O
of	O
α	O
(	O
zn	O
)	O
.	O
thus	O
the	O
predictive	B
distribution	I
can	O
be	O
carried	O
forward	O
indeﬁnitely	O
using	O
a	O
ﬁxed	O
amount	O
of	O
storage	O
,	O
as	O
may	O
be	O
required	O
for	O
real-time	O
applications	O
.	O
here	O
we	O
have	O
discussed	O
the	O
estimation	O
of	O
the	O
parameters	O
of	O
an	O
hmm	O
using	O
max-	O
imum	O
likelihood	O
.	O
this	O
framework	O
is	O
easily	O
extended	B
to	O
regularized	O
maximum	O
likeli-	O
hood	O
by	O
introducing	O
priors	O
over	O
the	O
model	O
parameters	O
π	O
,	O
a	O
and	O
φ	O
whose	O
values	O
are	O
then	O
estimated	O
by	O
maximizing	O
their	O
posterior	B
probability	I
.	O
this	O
can	O
again	O
be	O
done	O
us-	O
ing	O
the	O
em	O
algorithm	O
in	O
which	O
the	O
e	O
step	O
is	O
the	O
same	O
as	O
discussed	O
above	O
,	O
and	O
the	O
m	O
step	O
involves	O
adding	O
the	O
log	O
of	O
the	O
prior	B
distribution	O
p	O
(	O
θ	O
)	O
to	O
the	O
function	O
q	O
(	O
θ	O
,	O
θold	O
)	O
before	O
maximization	O
and	O
represents	O
a	O
straightforward	O
application	O
of	O
the	O
techniques	O
developed	O
at	O
various	O
points	O
in	O
this	O
book	O
.	O
furthermore	O
,	O
we	O
can	O
use	O
variational	B
meth-	O
ods	O
to	O
give	O
a	O
fully	O
bayesian	O
treatment	O
of	O
the	O
hmm	O
in	O
which	O
we	O
marginalize	O
over	O
the	O
parameter	O
distributions	O
(	O
mackay	O
,	O
1997	O
)	O
.	O
as	O
with	O
maximum	B
likelihood	I
,	O
this	O
leads	O
to	O
a	O
two-pass	O
forward-backward	O
recursion	O
to	O
compute	O
posterior	O
probabilities	O
.	O
13.2.3	O
the	O
sum-product	B
algorithm	I
for	O
the	O
hmm	O
the	O
directed	B
graph	O
that	O
represents	O
the	O
hidden	O
markov	O
model	O
,	O
shown	O
in	O
fig-	O
ure	O
13.5	O
,	O
is	O
a	O
tree	B
and	O
so	O
we	O
can	O
solve	O
the	O
problem	O
of	O
ﬁnding	O
local	B
marginals	O
for	O
the	O
hidden	O
variables	O
using	O
the	O
sum-product	B
algorithm	I
.	O
not	O
surprisingly	O
,	O
this	O
turns	O
out	O
to	O
be	O
equivalent	O
to	O
the	O
forward-backward	B
algorithm	I
considered	O
in	O
the	O
previous	O
section	O
,	O
and	O
so	O
the	O
sum-product	B
algorithm	I
therefore	O
provides	O
us	O
with	O
a	O
simple	O
way	O
to	O
derive	O
the	O
alpha-beta	O
recursion	O
formulae	O
.	O
we	O
begin	O
by	O
transforming	O
the	O
directed	B
graph	O
of	O
figure	O
13.5	O
into	O
a	O
factor	B
graph	I
,	O
of	O
which	O
a	O
representative	O
fragment	O
is	O
shown	O
in	O
figure	O
13.14.	O
this	O
form	O
of	O
the	O
fac-	O
tor	O
graph	O
shows	O
all	O
variables	O
,	O
both	O
latent	O
and	O
observed	O
,	O
explicitly	O
.	O
however	O
,	O
for	O
the	O
purpose	O
of	O
solving	O
the	O
inference	B
problem	O
,	O
we	O
shall	O
always	O
be	O
conditioning	O
on	O
the	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
and	O
so	O
we	O
can	O
simplify	O
the	O
factor	B
graph	I
by	O
absorbing	O
the	O
emission	O
probabilities	O
into	O
the	O
transition	B
probability	I
factors	O
.	O
this	O
leads	O
to	O
the	O
sim-	O
pliﬁed	O
factor	B
graph	I
representation	O
in	O
figure	O
13.15	O
,	O
in	O
which	O
the	O
factors	O
are	O
given	O
by	O
h	O
(	O
z1	O
)	O
=	O
p	O
(	O
z1	O
)	O
p	O
(	O
x1|z1	O
)	O
fn	O
(	O
zn−1	O
,	O
zn	O
)	O
=	O
p	O
(	O
zn|zn−1	O
)	O
p	O
(	O
xn|zn	O
)	O
.	O
(	O
13.45	O
)	O
(	O
13.46	O
)	O
section	O
10.1	O
section	O
8.4.4	O
626	O
13.	O
sequential	B
data	I
figure	O
13.15	O
a	O
simpliﬁed	O
form	O
of	O
fac-	O
tor	O
graph	O
to	O
describe	O
the	O
hidden	O
markov	O
model	O
.	O
h	O
fn	O
z1	O
zn−1	O
zn	O
to	O
derive	O
the	O
alpha-beta	O
algorithm	O
,	O
we	O
denote	O
the	O
ﬁnal	O
hidden	B
variable	I
zn	O
as	O
the	O
root	B
node	I
,	O
and	O
ﬁrst	O
pass	O
messages	O
from	O
the	O
leaf	O
node	B
h	O
to	O
the	O
root	O
.	O
from	O
the	O
general	O
results	O
(	O
8.66	O
)	O
and	O
(	O
8.69	O
)	O
for	O
message	O
propagation	O
,	O
we	O
see	O
that	O
the	O
messages	O
which	O
are	O
propagated	O
in	O
the	O
hidden	O
markov	O
model	O
take	O
the	O
form	O
µzn−1→fn	O
(	O
zn−1	O
)	O
=	O
µfn−1→zn−1	O
(	O
zn−1	O
)	O
µfn→zn	O
(	O
zn	O
)	O
=	O
fn	O
(	O
zn−1	O
,	O
zn	O
)	O
µzn−1→fn	O
(	O
zn−1	O
)	O
(	O
13.47	O
)	O
(	O
13.48	O
)	O
(	O
cid:2	O
)	O
zn−1	O
these	O
equations	O
represent	O
the	O
propagation	O
of	O
messages	O
forward	O
along	O
the	O
chain	O
and	O
are	O
equivalent	O
to	O
the	O
alpha	O
recursions	O
derived	O
in	O
the	O
previous	O
section	O
,	O
as	O
we	O
shall	O
now	O
show	O
.	O
note	O
that	O
because	O
the	O
variable	O
nodes	O
zn	O
have	O
only	O
two	O
neighbours	O
,	O
they	O
perform	O
no	O
computation	O
.	O
sion	B
for	O
the	O
f	O
→	O
z	O
messages	O
of	O
the	O
form	O
we	O
can	O
eliminate	O
µzn−1→fn	O
(	O
zn−1	O
)	O
from	O
(	O
13.48	O
)	O
using	O
(	O
13.47	O
)	O
to	O
give	O
a	O
recur-	O
(	O
cid:2	O
)	O
µfn→zn	O
(	O
zn	O
)	O
=	O
fn	O
(	O
zn−1	O
,	O
zn	O
)	O
µfn−1→zn−1	O
(	O
zn−1	O
)	O
.	O
(	O
13.49	O
)	O
if	O
we	O
now	O
recall	O
the	O
deﬁnition	O
(	O
13.46	O
)	O
,	O
and	O
if	O
we	O
deﬁne	O
zn−1	O
α	O
(	O
zn	O
)	O
=	O
µfn→zn	O
(	O
zn	O
)	O
(	O
13.50	O
)	O
then	O
we	O
obtain	O
the	O
alpha	O
recursion	O
given	O
by	O
(	O
13.36	O
)	O
.	O
we	O
also	O
need	O
to	O
verify	O
that	O
the	O
quantities	O
α	O
(	O
zn	O
)	O
are	O
themselves	O
equivalent	O
to	O
those	O
deﬁned	O
previously	O
.	O
this	O
is	O
easily	O
done	O
by	O
using	O
the	O
initial	O
condition	O
(	O
8.71	O
)	O
and	O
noting	O
that	O
α	O
(	O
z1	O
)	O
is	O
given	O
by	O
h	O
(	O
z1	O
)	O
=	O
p	O
(	O
z1	O
)	O
p	O
(	O
x1|z1	O
)	O
which	O
is	O
identical	O
to	O
(	O
13.37	O
)	O
.	O
because	O
the	O
initial	O
α	O
is	O
the	O
same	O
,	O
and	O
because	O
they	O
are	O
iteratively	O
computed	O
using	O
the	O
same	O
equation	O
,	O
all	O
subsequent	O
α	O
quantities	O
must	O
be	O
the	O
same	O
.	O
next	O
we	O
consider	O
the	O
messages	O
that	O
are	O
propagated	O
from	O
the	O
root	B
node	I
back	O
to	O
the	O
leaf	O
node	B
.	O
these	O
take	O
the	O
form	O
(	O
cid:2	O
)	O
µfn+1→fn	O
(	O
zn	O
)	O
=	O
fn+1	O
(	O
zn	O
,	O
zn+1	O
)	O
µfn+2→fn+1	O
(	O
zn+1	O
)	O
(	O
13.51	O
)	O
zn+1	O
where	O
,	O
as	O
before	O
,	O
we	O
have	O
eliminated	O
the	O
messages	O
of	O
the	O
type	O
z	O
→	O
f	O
since	O
the	O
variable	O
nodes	O
perform	O
no	O
computation	O
.	O
using	O
the	O
deﬁnition	O
(	O
13.46	O
)	O
to	O
substitute	O
for	O
fn+1	O
(	O
zn	O
,	O
zn+1	O
)	O
,	O
and	O
deﬁning	O
β	O
(	O
zn	O
)	O
=	O
µfn+1→zn	O
(	O
zn	O
)	O
(	O
13.52	O
)	O
13.2.	O
hidden	O
markov	O
models	O
627	O
we	O
obtain	O
the	O
beta	B
recursion	I
given	O
by	O
(	O
13.38	O
)	O
.	O
again	O
,	O
we	O
can	O
verify	O
that	O
the	O
beta	O
variables	O
themselves	O
are	O
equivalent	O
by	O
noting	O
that	O
(	O
8.70	O
)	O
implies	O
that	O
the	O
initial	O
mes-	O
sage	O
send	O
by	O
the	O
root	O
variable	O
node	B
is	O
µzn→fn	O
(	O
zn	O
)	O
=	O
1	O
,	O
which	O
is	O
identical	O
to	O
the	O
initialization	O
of	O
β	O
(	O
zn	O
)	O
given	O
in	O
section	O
13.2.2.	O
the	O
sum-product	B
algorithm	I
also	O
speciﬁes	O
how	O
to	O
evaluate	O
the	O
marginals	O
once	O
all	O
the	O
messages	O
have	O
been	O
evaluated	O
.	O
in	O
particular	O
,	O
the	O
result	O
(	O
8.63	O
)	O
shows	O
that	O
the	O
local	B
marginal	O
at	O
the	O
node	B
zn	O
is	O
given	O
by	O
the	O
product	O
of	O
the	O
incoming	O
messages	O
.	O
because	O
we	O
have	O
conditioned	O
on	O
the	O
variables	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
,	O
we	O
are	O
computing	O
the	O
joint	O
distribution	O
p	O
(	O
zn	O
,	O
x	O
)	O
=	O
µfn→zn	O
(	O
zn	O
)	O
µfn+1→zn	O
(	O
zn	O
)	O
=	O
α	O
(	O
zn	O
)	O
β	O
(	O
zn	O
)	O
.	O
(	O
13.53	O
)	O
dividing	O
both	O
sides	O
by	O
p	O
(	O
x	O
)	O
,	O
we	O
then	O
obtain	O
γ	O
(	O
zn	O
)	O
=	O
p	O
(	O
zn	O
,	O
x	O
)	O
p	O
(	O
x	O
)	O
=	O
α	O
(	O
zn	O
)	O
β	O
(	O
zn	O
)	O
p	O
(	O
x	O
)	O
(	O
13.54	O
)	O
exercise	O
13.11	O
in	O
agreement	O
with	O
(	O
13.33	O
)	O
.	O
the	O
result	O
(	O
13.43	O
)	O
can	O
similarly	O
be	O
derived	O
from	O
(	O
8.72	O
)	O
.	O
13.2.4	O
scaling	O
factors	O
there	O
is	O
an	O
important	O
issue	O
that	O
must	O
be	O
addressed	O
before	O
we	O
can	O
make	O
use	O
of	O
the	O
forward	O
backward	O
algorithm	O
in	O
practice	O
.	O
from	O
the	O
recursion	O
relation	O
(	O
13.36	O
)	O
,	O
we	O
note	O
that	O
at	O
each	O
step	O
the	O
new	O
value	O
α	O
(	O
zn	O
)	O
is	O
obtained	O
from	O
the	O
previous	O
value	O
α	O
(	O
zn−1	O
)	O
by	O
multiplying	O
by	O
quantities	O
p	O
(	O
zn|zn−1	O
)	O
and	O
p	O
(	O
xn|zn	O
)	O
.	O
because	O
these	O
probabilities	O
are	O
often	O
signiﬁcantly	O
less	O
than	O
unity	O
,	O
as	O
we	O
work	O
our	O
way	O
forward	O
along	O
the	O
chain	O
,	O
the	O
values	O
of	O
α	O
(	O
zn	O
)	O
can	O
go	O
to	O
zero	O
exponentially	O
quickly	O
.	O
for	O
moderate	O
lengths	O
of	O
chain	O
(	O
say	O
100	O
or	O
so	O
)	O
,	O
the	O
calculation	O
of	O
the	O
α	O
(	O
zn	O
)	O
will	O
soon	O
exceed	O
the	O
dynamic	O
range	O
of	O
the	O
computer	O
,	O
even	O
if	O
double	O
precision	O
ﬂoating	O
point	O
is	O
used	O
.	O
in	O
the	O
case	O
of	O
i.i.d	O
.	O
data	O
,	O
we	O
implicitly	O
circumvented	O
this	O
problem	O
with	O
the	O
eval-	O
uation	O
of	O
likelihood	O
functions	O
by	O
taking	O
logarithms	O
.	O
unfortunately	O
,	O
this	O
will	O
not	O
help	O
here	O
because	O
we	O
are	O
forming	O
sums	O
of	O
products	O
of	O
small	O
numbers	O
(	O
we	O
are	O
in	O
fact	O
im-	O
plicitly	O
summing	O
over	O
all	O
possible	O
paths	O
through	O
the	O
lattice	B
diagram	I
of	O
figure	O
13.7	O
)	O
.	O
we	O
therefore	O
work	O
with	O
re-scaled	O
versions	O
of	O
α	O
(	O
zn	O
)	O
and	O
β	O
(	O
zn	O
)	O
whose	O
values	O
remain	O
of	O
order	O
unity	O
.	O
as	O
we	O
shall	O
see	O
,	O
the	O
corresponding	O
scaling	O
factors	O
cancel	O
out	O
when	O
we	O
use	O
these	O
re-scaled	O
quantities	O
in	O
the	O
em	O
algorithm	O
.	O
in	O
(	O
13.34	O
)	O
,	O
we	O
deﬁned	O
α	O
(	O
zn	O
)	O
=	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
zn	O
)	O
representing	O
the	O
joint	O
distri-	O
bution	O
of	O
all	O
the	O
observations	O
up	O
to	O
xn	O
and	O
the	O
latent	B
variable	I
zn	O
.	O
now	O
we	O
deﬁne	O
a	O
normalized	O
version	O
of	O
α	O
given	O
by	O
(	O
cid:1	O
)	O
α	O
(	O
zn	O
)	O
=	O
p	O
(	O
zn|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
=	O
α	O
(	O
zn	O
)	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
(	O
13.55	O
)	O
which	O
we	O
expect	O
to	O
be	O
well	O
behaved	O
numerically	O
because	O
it	O
is	O
a	O
probability	B
distribu-	O
tion	O
over	O
k	O
variables	O
for	O
any	O
value	O
of	O
n.	O
in	O
order	O
to	O
relate	O
the	O
scaled	O
and	O
original	O
al-	O
pha	O
variables	O
,	O
we	O
introduce	O
scaling	O
factors	O
deﬁned	O
by	O
conditional	B
distributions	O
over	O
the	O
observed	O
variables	O
cn	O
=	O
p	O
(	O
xn|x1	O
,	O
.	O
.	O
.	O
,	O
xn−1	O
)	O
.	O
(	O
13.56	O
)	O
628	O
13.	O
sequential	B
data	I
from	O
the	O
product	B
rule	I
,	O
we	O
then	O
have	O
n	O
(	O
cid:14	O
)	O
m=1	O
cm	O
(	O
cid:22	O
)	O
n	O
(	O
cid:14	O
)	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
=	O
(	O
13.57	O
)	O
and	O
so	O
(	O
13.58	O
)	O
cm	O
m=1	O
(	O
cid:2	O
)	O
α	O
(	O
zn	O
)	O
=	O
p	O
(	O
zn|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
=	O
cn	O
(	O
cid:1	O
)	O
α	O
(	O
zn	O
)	O
=	O
p	O
(	O
xn|zn	O
)	O
(	O
cid:23	O
)	O
(	O
cid:1	O
)	O
α	O
(	O
zn	O
)	O
.	O
we	O
can	O
then	O
turn	O
the	O
recursion	O
equation	O
(	O
13.36	O
)	O
for	O
α	O
into	O
one	O
for	O
(	O
cid:1	O
)	O
α	O
given	O
by	O
(	O
cid:1	O
)	O
α	O
(	O
zn−1	O
)	O
p	O
(	O
zn|zn−1	O
)	O
.	O
note	O
that	O
at	O
each	O
stage	O
of	O
the	O
forward	O
message	O
passing	O
phase	O
,	O
used	O
to	O
evaluate	O
(	O
cid:1	O
)	O
α	O
(	O
zn	O
)	O
,	O
that	O
normalizes	O
the	O
right-hand	O
side	O
of	O
(	O
13.59	O
)	O
to	O
give	O
(	O
cid:1	O
)	O
α	O
(	O
zn	O
)	O
.	O
we	O
can	O
similarly	O
deﬁne	O
re-scaled	O
variables	O
(	O
cid:1	O
)	O
β	O
(	O
zn	O
)	O
using	O
(	O
cid:23	O
)	O
(	O
cid:1	O
)	O
β	O
(	O
zn	O
)	O
tities	O
(	O
cid:1	O
)	O
β	O
(	O
zn	O
)	O
are	O
simply	O
the	O
ratio	O
of	O
two	O
conditional	B
probabilities	O
which	O
will	O
again	O
remain	O
within	O
machine	O
precision	O
because	O
,	O
from	O
(	O
13.35	O
)	O
,	O
the	O
quan-	O
we	O
have	O
to	O
evaluate	O
and	O
store	O
cn	O
,	O
which	O
is	O
easily	O
done	O
because	O
it	O
is	O
the	O
coefﬁcient	O
n	O
(	O
cid:14	O
)	O
β	O
(	O
zn	O
)	O
=	O
cm	O
m=n+1	O
(	O
13.59	O
)	O
(	O
13.60	O
)	O
(	O
cid:22	O
)	O
zn−1	O
p	O
(	O
xn+1	O
,	O
.	O
.	O
.	O
,	O
xn|zn	O
)	O
p	O
(	O
xn+1	O
,	O
.	O
.	O
.	O
,	O
xn|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
.	O
(	O
13.61	O
)	O
the	O
recursion	O
result	O
(	O
13.38	O
)	O
for	O
β	O
then	O
gives	O
the	O
following	O
recursion	O
for	O
the	O
re-scaled	O
variables	O
(	O
cid:1	O
)	O
β	O
(	O
zn+1	O
)	O
p	O
(	O
xn+1|zn+1	O
)	O
p	O
(	O
zn+1|zn	O
)	O
.	O
(	O
13.62	O
)	O
cn+1	O
(	O
cid:1	O
)	O
β	O
(	O
zn	O
)	O
=	O
(	O
cid:2	O
)	O
(	O
cid:1	O
)	O
β	O
(	O
zn	O
)	O
=	O
zn+1	O
in	O
applying	O
this	O
recursion	O
relation	O
,	O
we	O
make	O
use	O
of	O
the	O
scaling	O
factors	O
cn	O
that	O
were	O
previously	O
computed	O
in	O
the	O
α	O
phase	O
.	O
from	O
(	O
13.57	O
)	O
,	O
we	O
see	O
that	O
the	O
likelihood	B
function	I
can	O
be	O
found	O
using	O
n	O
(	O
cid:14	O
)	O
p	O
(	O
x	O
)	O
=	O
cn	O
.	O
n=1	O
(	O
13.63	O
)	O
exercise	O
13.15	O
similarly	O
,	O
using	O
(	O
13.33	O
)	O
and	O
(	O
13.43	O
)	O
,	O
together	O
with	O
(	O
13.63	O
)	O
,	O
we	O
see	O
that	O
the	O
required	O
marginals	O
are	O
given	O
by	O
γ	O
(	O
zn	O
)	O
=	O
(	O
cid:1	O
)	O
α	O
(	O
zn	O
)	O
(	O
cid:1	O
)	O
β	O
(	O
zn	O
)	O
ξ	O
(	O
zn−1	O
,	O
zn	O
)	O
=	O
cn	O
(	O
cid:1	O
)	O
α	O
(	O
zn−1	O
)	O
p	O
(	O
xn|zn	O
)	O
p	O
(	O
zn|z−1	O
)	O
(	O
cid:1	O
)	O
β	O
(	O
zn	O
)	O
.	O
(	O
13.64	O
)	O
(	O
13.65	O
)	O
section	O
13.3	O
13.2.	O
hidden	O
markov	O
models	O
629	O
finally	O
,	O
we	O
note	O
that	O
there	O
is	O
an	O
alternative	O
formulation	O
of	O
the	O
forward-backward	B
algorithm	I
(	O
jordan	O
,	O
2007	O
)	O
in	O
which	O
the	O
backward	O
pass	O
is	O
deﬁned	O
by	O
a	O
recursion	O
based	O
the	O
quantities	O
γ	O
(	O
zn	O
)	O
=	O
(	O
cid:1	O
)	O
α	O
(	O
zn	O
)	O
(	O
cid:1	O
)	O
β	O
(	O
zn	O
)	O
instead	O
of	O
using	O
(	O
cid:1	O
)	O
β	O
(	O
zn	O
)	O
.	O
this	O
α–γ	O
recursion	O
requires	O
that	O
the	O
forward	O
pass	O
be	O
completed	O
ﬁrst	O
so	O
that	O
all	O
the	O
quantities	O
(	O
cid:1	O
)	O
α	O
(	O
zn	O
)	O
are	O
available	O
for	O
the	O
backward	O
pass	O
,	O
whereas	O
the	O
forward	O
and	O
backward	O
passes	O
of	O
the	O
α–β	O
algorithm	O
can	O
be	O
done	O
independently	O
.	O
although	O
these	O
two	O
algorithms	O
have	O
comparable	O
computational	O
cost	O
,	O
the	O
α–β	O
version	O
is	O
the	O
most	O
commonly	O
encountered	O
one	O
in	O
the	O
case	O
of	O
hidden	O
markov	O
models	O
,	O
whereas	O
for	O
linear	O
dynamical	O
systems	O
a	O
recursion	O
analogous	O
to	O
the	O
α–γ	O
form	O
is	O
more	O
usual	O
.	O
13.2.5	O
the	O
viterbi	O
algorithm	O
in	O
many	O
applications	O
of	O
hidden	O
markov	O
models	O
,	O
the	O
latent	O
variables	O
have	O
some	O
meaningful	O
interpretation	O
,	O
and	O
so	O
it	O
is	O
often	O
of	O
interest	O
to	O
ﬁnd	O
the	O
most	O
probable	O
sequence	O
of	O
hidden	O
states	O
for	O
a	O
given	O
observation	O
sequence	O
.	O
for	O
instance	O
in	O
speech	B
recognition	I
,	O
we	O
might	O
wish	O
to	O
ﬁnd	O
the	O
most	O
probable	O
phoneme	O
sequence	O
for	O
a	O
given	O
series	O
of	O
acoustic	O
observations	O
.	O
because	O
the	O
graph	O
for	O
the	O
hidden	O
markov	O
model	O
is	O
a	O
directed	B
tree	O
,	O
this	O
problem	O
can	O
be	O
solved	O
exactly	O
using	O
the	O
max-sum	B
algorithm	I
.	O
we	O
recall	O
from	O
our	O
discussion	O
in	O
section	O
8.4.5	O
that	O
the	O
problem	O
of	O
ﬁnding	O
the	O
most	O
probable	O
sequence	O
of	O
latent	O
states	O
is	O
not	O
the	O
same	O
as	O
that	O
of	O
ﬁnding	O
the	O
set	O
of	O
states	O
that	O
are	O
individually	O
the	O
most	O
probable	O
.	O
the	O
latter	O
problem	O
can	O
be	O
solved	O
by	O
ﬁrst	O
running	O
the	O
forward-backward	O
(	O
sum-product	O
)	O
algorithm	O
to	O
ﬁnd	O
the	O
latent	B
variable	I
marginals	O
γ	O
(	O
zn	O
)	O
and	O
then	O
maximizing	O
each	O
of	O
these	O
individually	O
(	O
duda	O
et	O
al.	O
,	O
2001	O
)	O
.	O
however	O
,	O
the	O
set	O
of	O
such	O
states	O
will	O
not	O
,	O
in	O
general	O
,	O
correspond	O
to	O
the	O
most	O
probable	O
sequence	O
of	O
states	O
.	O
in	O
fact	O
,	O
this	O
set	O
of	O
states	O
might	O
even	O
represent	O
a	O
sequence	O
having	O
zero	O
probability	B
,	O
if	O
it	O
so	O
happens	O
that	O
two	O
successive	O
states	O
,	O
which	O
in	O
isolation	O
are	O
individually	O
the	O
most	O
probable	O
,	O
are	O
such	O
that	O
the	O
transition	O
matrix	O
element	O
connecting	O
them	O
is	O
zero	O
.	O
in	O
practice	O
,	O
we	O
are	O
usually	O
interested	O
in	O
ﬁnding	O
the	O
most	O
probable	O
sequence	O
of	O
states	O
,	O
and	O
this	O
can	O
be	O
solved	O
efﬁciently	O
using	O
the	O
max-sum	B
algorithm	I
,	O
which	O
in	O
the	O
context	O
of	O
hidden	O
markov	O
models	O
is	O
known	O
as	O
the	O
viterbi	O
algorithm	O
(	O
viterbi	O
,	O
1967	O
)	O
.	O
note	O
that	O
the	O
max-sum	B
algorithm	I
works	O
with	O
log	O
probabilities	O
and	O
so	O
there	O
is	O
no	O
need	O
to	O
use	O
re-scaled	O
variables	O
as	O
was	O
done	O
with	O
the	O
forward-backward	B
algorithm	I
.	O
figure	O
13.16	O
shows	O
a	O
fragment	O
of	O
the	O
hidden	O
markov	O
model	O
expanded	O
as	O
lattice	B
diagram	I
.	O
as	O
we	O
have	O
already	O
noted	O
,	O
the	O
number	O
of	O
possible	O
paths	O
through	O
the	O
lattice	O
grows	O
exponentially	O
with	O
the	O
length	O
of	O
the	O
chain	O
.	O
the	O
viterbi	O
algorithm	O
searches	O
this	O
space	O
of	O
paths	O
efﬁciently	O
to	O
ﬁnd	O
the	O
most	O
probable	O
path	O
with	O
a	O
computational	O
cost	O
that	O
grows	O
only	O
linearly	O
with	O
the	O
length	O
of	O
the	O
chain	O
.	O
as	O
with	O
the	O
sum-product	B
algorithm	I
,	O
we	O
ﬁrst	O
represent	O
the	O
hidden	O
markov	O
model	O
as	O
a	O
factor	B
graph	I
,	O
as	O
shown	O
in	O
figure	O
13.15.	O
again	O
,	O
we	O
treat	O
the	O
variable	O
node	B
zn	O
as	O
the	O
root	O
,	O
and	O
pass	O
messages	O
to	O
the	O
root	O
starting	O
with	O
the	O
leaf	O
nodes	O
.	O
using	O
the	O
results	O
(	O
8.93	O
)	O
and	O
(	O
8.94	O
)	O
,	O
we	O
see	O
that	O
the	O
messages	O
passed	O
in	O
the	O
max-sum	B
algorithm	I
are	O
given	O
by	O
(	O
cid:27	O
)	O
(	O
13.66	O
)	O
.	O
(	O
13.67	O
)	O
µzn→fn+1	O
(	O
zn	O
)	O
=	O
µfn→zn	O
(	O
zn	O
)	O
µfn+1→zn+1	O
(	O
zn+1	O
)	O
=	O
max	O
zn	O
ln	O
fn+1	O
(	O
zn	O
,	O
zn+1	O
)	O
+	O
µzn→fn+1	O
(	O
zn	O
)	O
(	O
cid:26	O
)	O
630	O
13.	O
sequential	B
data	I
figure	O
13.16	O
a	O
fragment	O
of	O
the	O
hmm	O
lattice	O
showing	O
two	O
possible	O
paths	O
.	O
the	O
viterbi	O
algorithm	O
efﬁciently	O
determines	O
the	O
most	O
probable	O
path	O
from	O
amongst	O
the	O
exponentially	O
many	O
possibilities	O
.	O
for	O
any	O
given	O
path	O
,	O
the	O
corresponding	O
probability	B
is	O
given	O
by	O
the	O
product	O
of	O
the	O
elements	O
of	O
the	O
tran-	O
sition	O
matrix	O
ajk	O
,	O
corresponding	O
to	O
the	O
probabil-	O
ities	O
p	O
(	O
zn+1|zn	O
)	O
for	O
each	O
segment	O
of	O
the	O
path	O
,	O
along	O
with	O
the	O
emission	O
densities	O
p	O
(	O
xn|k	O
)	O
asso-	O
ciated	O
with	O
each	O
node	B
on	O
the	O
path	O
.	O
k	O
=	O
1	O
k	O
=	O
2	O
k	O
=	O
3	O
n	O
−	O
2	O
n	O
−	O
1	O
n	O
n	O
+	O
1	O
if	O
we	O
eliminate	O
µzn→fn+1	O
(	O
zn	O
)	O
between	O
these	O
two	O
equations	O
,	O
and	O
make	O
use	O
of	O
(	O
13.46	O
)	O
,	O
we	O
obtain	O
a	O
recursion	O
for	O
the	O
f	O
→	O
z	O
messages	O
of	O
the	O
form	O
ω	O
(	O
zn+1	O
)	O
=	O
ln	O
p	O
(	O
xn+1|zn+1	O
)	O
+	O
max	O
zn	O
{	O
ln	O
p	O
(	O
x+1|zn	O
)	O
+	O
ω	O
(	O
zn	O
)	O
}	O
(	O
13.68	O
)	O
where	O
we	O
have	O
introduced	O
the	O
notation	O
ω	O
(	O
zn	O
)	O
≡	O
µfn→zn	O
(	O
zn	O
)	O
.	O
from	O
(	O
8.95	O
)	O
and	O
(	O
8.96	O
)	O
,	O
these	O
messages	O
are	O
initialized	O
using	O
ω	O
(	O
z1	O
)	O
=	O
ln	O
p	O
(	O
z1	O
)	O
+	O
ln	O
p	O
(	O
x1|z1	O
)	O
.	O
(	O
13.69	O
)	O
where	O
we	O
have	O
used	O
(	O
13.45	O
)	O
.	O
note	O
that	O
to	O
keep	O
the	O
notation	O
uncluttered	O
,	O
we	O
omit	O
the	O
dependence	O
on	O
the	O
model	O
parameters	O
θ	O
that	O
are	O
held	O
ﬁxed	O
when	O
ﬁnding	O
the	O
most	O
probable	O
sequence	O
.	O
exercise	O
13.16	O
the	O
viterbi	O
algorithm	O
can	O
also	O
be	O
derived	O
directly	O
from	O
the	O
deﬁnition	O
(	O
13.6	O
)	O
of	O
the	O
joint	O
distribution	O
by	O
taking	O
the	O
logarithm	O
and	O
then	O
exchanging	O
maximizations	O
and	O
summations	O
.	O
it	O
is	O
easily	O
seen	O
that	O
the	O
quantities	O
ω	O
(	O
zn	O
)	O
have	O
the	O
probabilistic	O
interpretation	O
ω	O
(	O
zn	O
)	O
=	O
max	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
z1	O
,	O
.	O
.	O
.	O
,	O
zn	O
)	O
.	O
z1	O
,	O
...	O
,	O
zn−1	O
(	O
13.70	O
)	O
once	O
we	O
have	O
completed	O
the	O
ﬁnal	O
maximization	O
over	O
zn	O
,	O
we	O
will	O
obtain	O
the	O
value	O
of	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
z	O
)	O
corresponding	O
to	O
the	O
most	O
probable	O
path	O
.	O
we	O
also	O
wish	O
to	O
ﬁnd	O
the	O
sequence	O
of	O
latent	B
variable	I
values	O
that	O
corresponds	O
to	O
this	O
path	O
.	O
to	O
do	O
this	O
,	O
we	O
simply	O
make	O
use	O
of	O
the	O
back-tracking	B
procedure	O
discussed	O
in	O
sec-	O
tion	O
8.4.5.	O
speciﬁcally	O
,	O
we	O
note	O
that	O
the	O
maximization	O
over	O
zn	O
must	O
be	O
performed	O
for	O
each	O
of	O
the	O
k	O
possible	O
values	O
of	O
zn+1	O
.	O
suppose	O
we	O
keep	O
a	O
record	O
of	O
the	O
values	O
of	O
zn	O
that	O
correspond	O
to	O
the	O
maxima	O
for	O
each	O
value	O
of	O
the	O
k	O
values	O
of	O
zn+1	O
.	O
let	O
us	O
denote	O
this	O
function	O
by	O
ψ	O
(	O
kn	O
)	O
where	O
k	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
.	O
once	O
we	O
have	O
passed	O
mes-	O
sages	O
to	O
the	O
end	O
of	O
the	O
chain	O
and	O
found	O
the	O
most	O
probable	O
state	O
of	O
zn	O
,	O
we	O
can	O
then	O
use	O
this	O
function	O
to	O
backtrack	O
along	O
the	O
chain	O
by	O
applying	O
it	O
recursively	O
kmax	O
n	O
=	O
ψ	O
(	O
kmax	O
n+1	O
)	O
.	O
(	O
13.71	O
)	O
13.2.	O
hidden	O
markov	O
models	O
631	O
intuitively	O
,	O
we	O
can	O
understand	O
the	O
viterbi	O
algorithm	O
as	O
follows	O
.	O
naively	O
,	O
we	O
could	O
consider	O
explicitly	O
all	O
of	O
the	O
exponentially	O
many	O
paths	O
through	O
the	O
lattice	O
,	O
evaluate	O
the	O
probability	B
for	O
each	O
,	O
and	O
then	O
select	O
the	O
path	O
having	O
the	O
highest	O
proba-	O
bility	O
.	O
however	O
,	O
we	O
notice	O
that	O
we	O
can	O
make	O
a	O
dramatic	O
saving	O
in	O
computational	O
cost	O
as	O
follows	O
.	O
suppose	O
that	O
for	O
each	O
path	O
we	O
evaluate	O
its	O
probability	B
by	O
summing	O
up	O
products	O
of	O
transition	O
and	O
emission	O
probabilities	O
as	O
we	O
work	O
our	O
way	O
forward	O
along	O
each	O
path	O
through	O
the	O
lattice	O
.	O
consider	O
a	O
particular	O
time	O
step	O
n	O
and	O
a	O
particular	O
state	O
k	O
at	O
that	O
time	O
step	O
.	O
there	O
will	O
be	O
many	O
possible	O
paths	O
converging	O
on	O
the	O
correspond-	O
ing	O
node	O
in	O
the	O
lattice	B
diagram	I
.	O
however	O
,	O
we	O
need	O
only	O
retain	O
that	O
particular	O
path	O
that	O
so	O
far	O
has	O
the	O
highest	O
probability	B
.	O
because	O
there	O
are	O
k	O
states	O
at	O
time	O
step	O
n	O
,	O
we	O
need	O
to	O
keep	O
track	O
of	O
k	O
such	O
paths	O
.	O
at	O
time	O
step	O
n	O
+	O
1	O
,	O
there	O
will	O
be	O
k	O
2	O
possible	O
paths	O
to	O
consider	O
,	O
comprising	O
k	O
possible	O
paths	O
leading	O
out	O
of	O
each	O
of	O
the	O
k	O
current	O
states	O
,	O
but	O
again	O
we	O
need	O
only	O
retain	O
k	O
of	O
these	O
corresponding	O
to	O
the	O
best	O
path	O
for	O
each	O
state	O
at	O
time	O
n+1	O
.	O
when	O
we	O
reach	O
the	O
ﬁnal	O
time	O
step	O
n	O
we	O
will	O
discover	O
which	O
state	O
corresponds	O
to	O
the	O
overall	O
most	O
probable	O
path	O
.	O
because	O
there	O
is	O
a	O
unique	O
path	O
coming	O
into	O
that	O
state	O
we	O
can	O
trace	O
the	O
path	O
back	O
to	O
step	O
n	O
−	O
1	O
to	O
see	O
what	O
state	O
it	O
occupied	O
at	O
that	O
time	O
,	O
and	O
so	O
on	O
back	O
through	O
the	O
lattice	O
to	O
the	O
state	O
n	O
=	O
1	O
.	O
13.2.6	O
extensions	O
of	O
the	O
hidden	O
markov	O
model	O
the	O
basic	O
hidden	O
markov	O
model	O
,	O
along	O
with	O
the	O
standard	O
training	O
algorithm	O
based	O
on	O
maximum	B
likelihood	I
,	O
has	O
been	O
extended	B
in	O
numerous	O
ways	O
to	O
meet	O
the	O
requirements	O
of	O
particular	O
applications	O
.	O
here	O
we	O
discuss	O
a	O
few	O
of	O
the	O
more	O
important	O
examples	O
.	O
we	O
see	O
from	O
the	O
digits	O
example	O
in	O
figure	O
13.11	O
that	O
hidden	O
markov	O
models	O
can	O
be	O
quite	O
poor	O
generative	O
models	O
for	O
the	O
data	O
,	O
because	O
many	O
of	O
the	O
synthetic	O
digits	O
look	O
quite	O
unrepresentative	O
of	O
the	O
training	B
data	O
.	O
if	O
the	O
goal	O
is	O
sequence	O
classiﬁca-	O
tion	O
,	O
there	O
can	O
be	O
signiﬁcant	O
beneﬁt	O
in	O
determining	O
the	O
parameters	O
of	O
hidden	O
markov	O
models	O
using	O
discriminative	O
rather	O
than	O
maximum	B
likelihood	I
techniques	O
.	O
suppose	O
we	O
have	O
a	O
training	B
set	I
of	O
r	O
observation	O
sequences	O
xr	O
,	O
where	O
r	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
r	O
,	O
each	O
of	O
which	O
is	O
labelled	O
according	O
to	O
its	O
class	O
m	O
,	O
where	O
m	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m.	O
for	O
each	O
class	O
,	O
we	O
have	O
a	O
separate	O
hidden	O
markov	O
model	O
with	O
its	O
own	O
parameters	O
θm	O
,	O
and	O
we	O
treat	O
the	O
problem	O
of	O
determining	O
the	O
parameter	O
values	O
as	O
a	O
standard	O
classiﬁcation	O
problem	O
in	O
which	O
we	O
optimize	O
the	O
cross-entropy	O
ln	O
p	O
(	O
mr|xr	O
)	O
.	O
(	O
13.72	O
)	O
using	O
bayes	O
’	O
theorem	O
this	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
sequence	O
probabilities	O
associated	O
with	O
the	O
hidden	O
markov	O
models	O
r	O
(	O
cid:2	O
)	O
r=1	O
(	O
cid:24	O
)	O
r	O
(	O
cid:2	O
)	O
r=1	O
ln	O
(	O
cid:25	O
)	O
(	O
cid:5	O
)	O
m	O
p	O
(	O
xr|θr	O
)	O
p	O
(	O
mr	O
)	O
l=1	O
p	O
(	O
xr|θl	O
)	O
p	O
(	O
lr	O
)	O
where	O
p	O
(	O
m	O
)	O
is	O
the	O
prior	B
probability	O
of	O
class	O
m.	O
optimization	O
of	O
this	O
cost	B
function	I
is	O
more	O
complex	O
than	O
for	O
maximum	O
likelihood	O
(	O
kapadia	O
,	O
1998	O
)	O
,	O
and	O
in	O
particular	O
(	O
13.73	O
)	O
632	O
13.	O
sequential	B
data	I
figure	O
13.17	O
section	O
of	O
an	O
autoregressive	O
hidden	O
markov	O
model	O
,	O
in	O
which	O
the	O
distribution	O
of	O
the	O
observation	O
xn	O
depends	O
on	O
a	O
subset	O
of	O
the	O
previous	O
observations	O
as	O
well	O
as	O
on	O
the	O
hidden	O
state	O
zn	O
.	O
in	O
this	O
example	O
,	O
the	O
distribution	O
of	O
xn	O
depends	O
on	O
the	O
two	O
previous	O
observations	O
xn−1	O
and	O
xn−2	O
.	O
zn−1	O
zn	O
zn+1	O
xn−1	O
xn	O
xn+1	O
requires	O
that	O
every	O
training	B
sequence	O
be	O
evaluated	O
under	O
each	O
of	O
the	O
models	O
in	O
or-	O
der	O
to	O
compute	O
the	O
denominator	O
in	O
(	O
13.73	O
)	O
.	O
hidden	O
markov	O
models	O
,	O
coupled	O
with	O
discriminative	O
training	O
methods	O
,	O
are	O
widely	O
used	O
in	O
speech	B
recognition	I
(	O
kapadia	O
,	O
1998	O
)	O
.	O
a	O
signiﬁcant	O
weakness	O
of	O
the	O
hidden	O
markov	O
model	O
is	O
the	O
way	O
in	O
which	O
it	O
rep-	O
resents	O
the	O
distribution	O
of	O
times	O
for	O
which	O
the	O
system	O
remains	O
in	O
a	O
given	O
state	O
.	O
to	O
see	O
the	O
problem	O
,	O
note	O
that	O
the	O
probability	B
that	O
a	O
sequence	O
sampled	O
from	O
a	O
given	O
hidden	O
markov	O
model	O
will	O
spend	O
precisely	O
t	O
steps	O
in	O
state	O
k	O
and	O
then	O
make	O
a	O
transition	O
to	O
a	O
different	O
state	O
is	O
given	O
by	O
p	O
(	O
t	O
)	O
=	O
(	O
akk	O
)	O
t	O
(	O
1	O
−	O
akk	O
)	O
∝	O
exp	O
(	O
−t	O
ln	O
akk	O
)	O
(	O
13.74	O
)	O
and	O
so	O
is	O
an	O
exponentially	O
decaying	O
function	O
of	O
t	O
.	O
for	O
many	O
applications	O
,	O
this	O
will	O
be	O
a	O
very	O
unrealistic	O
model	O
of	O
state	O
duration	O
.	O
the	O
problem	O
can	O
be	O
resolved	O
by	O
mod-	O
elling	O
state	O
duration	O
directly	O
in	O
which	O
the	O
diagonal	B
coefﬁcients	O
akk	O
are	O
all	O
set	O
to	O
zero	O
,	O
and	O
each	O
state	O
k	O
is	O
explicitly	O
associated	O
with	O
a	O
probability	B
distribution	O
p	O
(	O
t|k	O
)	O
of	O
pos-	O
sible	O
duration	O
times	O
.	O
from	O
a	O
generative	O
point	O
of	O
view	O
,	O
when	O
a	O
state	O
k	O
is	O
entered	O
,	O
a	O
value	O
t	O
representing	O
the	O
number	O
of	O
time	O
steps	O
that	O
the	O
system	O
will	O
remain	O
in	O
state	O
k	O
is	O
then	O
drawn	O
from	O
p	O
(	O
t|k	O
)	O
.	O
the	O
model	O
then	O
emits	O
t	O
values	O
of	O
the	O
observed	B
variable	I
xt	O
,	O
which	O
are	O
generally	O
assumed	O
to	O
be	O
independent	B
so	O
that	O
the	O
corresponding	O
emis-	O
t=1	O
p	O
(	O
xt|k	O
)	O
.	O
this	O
approach	O
requires	O
some	O
straightforward	O
sion	B
density	O
is	O
simply	O
modiﬁcations	O
to	O
the	O
em	O
optimization	O
procedure	O
(	O
rabiner	O
,	O
1989	O
)	O
.	O
(	O
cid:21	O
)	O
t	O
another	O
limitation	O
of	O
the	O
standard	O
hmm	O
is	O
that	O
it	O
is	O
poor	O
at	O
capturing	O
long-	O
range	O
correlations	O
between	O
the	O
observed	O
variables	O
(	O
i.e.	O
,	O
between	O
variables	O
that	O
are	O
separated	O
by	O
many	O
time	O
steps	O
)	O
because	O
these	O
must	O
be	O
mediated	O
via	O
the	O
ﬁrst-order	O
markov	O
chain	O
of	O
hidden	O
states	O
.	O
longer-range	O
effects	O
could	O
in	O
principle	O
be	O
included	O
by	O
adding	O
extra	O
links	O
to	O
the	O
graphical	B
model	I
of	O
figure	O
13.5.	O
one	O
way	O
to	O
address	O
this	O
is	O
to	O
generalize	O
the	O
hmm	O
to	O
give	O
the	O
autoregressive	O
hidden	O
markov	O
model	O
(	O
ephraim	O
et	O
al.	O
,	O
1989	O
)	O
,	O
an	O
example	O
of	O
which	O
is	O
shown	O
in	O
figure	O
13.17.	O
for	O
discrete	O
observa-	O
tions	O
,	O
this	O
corresponds	O
to	O
expanded	O
tables	O
of	O
conditional	B
probabilities	O
for	O
the	O
emis-	O
sion	B
distributions	O
.	O
in	O
the	O
case	O
of	O
a	O
gaussian	O
emission	O
density	O
,	O
we	O
can	O
use	O
the	O
linear-	O
gaussian	O
framework	O
in	O
which	O
the	O
conditional	B
distribution	O
for	O
xn	O
given	O
the	O
values	O
of	O
the	O
previous	O
observations	O
,	O
and	O
the	O
value	O
of	O
zn	O
,	O
is	O
a	O
gaussian	O
whose	O
mean	B
is	O
a	O
linear	O
combination	O
of	O
the	O
values	O
of	O
the	O
conditioning	O
variables	O
.	O
clearly	O
the	O
number	O
of	O
additional	O
links	O
in	O
the	O
graph	O
must	O
be	O
limited	O
to	O
avoid	O
an	O
excessive	O
the	O
number	O
of	O
free	O
parameters	O
.	O
in	O
the	O
example	O
shown	O
in	O
figure	O
13.17	O
,	O
each	O
observation	O
depends	O
on	O
figure	O
13.18	O
example	O
of	O
an	O
input-output	O
hidden	O
markov	O
model	O
.	O
in	O
this	O
case	O
,	O
both	O
the	O
emission	O
probabilities	O
and	O
the	O
transition	O
probabilities	O
depend	O
on	O
the	O
values	O
of	O
a	O
sequence	O
of	O
observations	O
u1	O
,	O
.	O
.	O
.	O
,	O
un	O
.	O
13.2.	O
hidden	O
markov	O
models	O
633	O
un−1	O
un	O
un+1	O
zn−1	O
zn	O
zn+1	O
xn−1	O
xn	O
xn+1	O
the	O
two	O
preceding	O
observed	O
variables	O
as	O
well	O
as	O
on	O
the	O
hidden	O
state	O
.	O
although	O
this	O
graph	O
looks	O
messy	O
,	O
we	O
can	O
again	O
appeal	O
to	O
d-separation	B
to	O
see	O
that	O
in	O
fact	O
it	O
still	O
has	O
a	O
simple	O
probabilistic	O
structure	O
.	O
in	O
particular	O
,	O
if	O
we	O
imagine	O
conditioning	O
on	O
zn	O
we	O
see	O
that	O
,	O
as	O
with	O
the	O
standard	O
hmm	O
,	O
the	O
values	O
of	O
zn−1	O
and	O
zn+1	O
are	O
independent	B
,	O
corresponding	O
to	O
the	O
conditional	B
independence	I
property	O
(	O
13.5	O
)	O
.	O
this	O
is	O
easily	O
veri-	O
ﬁed	O
by	O
noting	O
that	O
every	O
path	O
from	O
node	B
zn−1	O
to	O
node	B
zn+1	O
passes	O
through	O
at	O
least	O
one	O
observed	O
node	O
that	O
is	O
head-to-tail	O
with	O
respect	O
to	O
that	O
path	O
.	O
as	O
a	O
consequence	O
,	O
we	O
can	O
again	O
use	O
a	O
forward-backward	O
recursion	O
in	O
the	O
e	O
step	O
of	O
the	O
em	O
algorithm	O
to	O
determine	O
the	O
posterior	O
distributions	O
of	O
the	O
latent	O
variables	O
in	O
a	O
computational	O
time	O
that	O
is	O
linear	O
in	O
the	O
length	O
of	O
the	O
chain	O
.	O
similarly	O
,	O
the	O
m	O
step	O
involves	O
only	O
a	O
minor	O
modiﬁcation	O
of	O
the	O
standard	O
m-step	O
equations	O
.	O
in	O
the	O
case	O
of	O
gaussian	O
emission	O
densities	O
this	O
involves	O
estimating	O
the	O
parameters	O
using	O
the	O
standard	O
linear	O
regression	B
equations	O
,	O
discussed	O
in	O
chapter	O
3.	O
we	O
have	O
seen	O
that	O
the	O
autoregressive	B
hmm	O
appears	O
as	O
a	O
natural	O
extension	O
of	O
the	O
standard	O
hmm	O
when	O
viewed	O
as	O
a	O
graphical	B
model	I
.	O
in	O
fact	O
the	O
probabilistic	O
graphical	O
modelling	O
viewpoint	O
motivates	O
a	O
plethora	O
of	O
different	O
graphical	O
structures	O
based	O
on	O
the	O
hmm	O
.	O
another	O
example	O
is	O
the	O
input-output	O
hidden	O
markov	O
model	O
(	O
bengio	O
and	O
frasconi	O
,	O
1995	O
)	O
,	O
in	O
which	O
we	O
have	O
a	O
sequence	O
of	O
observed	O
variables	O
u1	O
,	O
.	O
.	O
.	O
,	O
un	O
,	O
in	O
addition	O
to	O
the	O
output	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
whose	O
values	O
inﬂuence	O
either	O
the	O
dis-	O
tribution	O
of	O
latent	O
variables	O
or	O
output	O
variables	O
,	O
or	O
both	O
.	O
an	O
example	O
is	O
shown	O
in	O
figure	O
13.18.	O
this	O
extends	O
the	O
hmm	O
framework	O
to	O
the	O
domain	O
of	O
supervised	O
learn-	O
ing	O
for	O
sequential	B
data	I
.	O
it	O
is	O
again	O
easy	O
to	O
show	O
,	O
through	O
the	O
use	O
of	O
the	O
d-separation	B
criterion	O
,	O
that	O
the	O
markov	O
property	O
(	O
13.5	O
)	O
for	O
the	O
chain	O
of	O
latent	O
variables	O
still	O
holds	O
.	O
to	O
verify	O
this	O
,	O
simply	O
note	O
that	O
there	O
is	O
only	O
one	O
path	O
from	O
node	B
zn−1	O
to	O
node	B
zn+1	O
and	O
this	O
is	O
head-to-tail	O
with	O
respect	O
to	O
the	O
observed	O
node	O
zn	O
.	O
this	O
conditional	B
inde-	O
pendence	O
property	O
again	O
allows	O
the	O
formulation	O
of	O
a	O
computationally	O
efﬁcient	O
learn-	O
ing	O
algorithm	O
.	O
in	O
particular	O
,	O
we	O
can	O
determine	O
the	O
parameters	O
θ	O
of	O
the	O
model	O
by	O
maximizing	O
the	O
likelihood	B
function	I
l	O
(	O
θ	O
)	O
=	O
p	O
(	O
x|u	O
,	O
θ	O
)	O
where	O
u	O
is	O
a	O
matrix	O
whose	O
rows	O
are	O
given	O
by	O
ut	O
n.	O
as	O
a	O
consequence	O
of	O
the	O
conditional	B
independence	I
property	O
(	O
13.5	O
)	O
this	O
likelihood	B
function	I
can	O
be	O
maximized	O
efﬁciently	O
using	O
an	O
em	O
algorithm	O
in	O
which	O
the	O
e	O
step	O
involves	O
forward	O
and	O
backward	O
recursions	O
.	O
another	O
variant	O
of	O
the	O
hmm	O
worthy	O
of	O
mention	O
is	O
the	O
factorial	O
hidden	O
markov	O
model	O
(	O
ghahramani	O
and	O
jordan	O
,	O
1997	O
)	O
,	O
in	O
which	O
there	O
are	O
multiple	O
independent	B
exercise	O
13.18	O
634	O
13.	O
sequential	B
data	I
figure	O
13.19	O
a	O
factorial	O
hidden	O
markov	O
model	O
com-	O
prising	O
two	O
markov	O
chains	O
of	O
latent	O
vari-	O
ables	O
.	O
for	O
continuous	O
observed	O
variables	O
x	O
,	O
one	O
possible	O
choice	O
of	O
emission	O
model	O
is	O
a	O
linear-gaussian	O
density	B
in	O
which	O
the	O
mean	B
of	O
the	O
gaussian	O
is	O
a	O
linear	O
combi-	O
nation	O
of	O
the	O
states	O
of	O
the	O
corresponding	O
latent	O
variables	O
.	O
z	O
(	O
2	O
)	O
n−1	O
z	O
(	O
2	O
)	O
n	O
z	O
(	O
2	O
)	O
n+1	O
z	O
(	O
1	O
)	O
n−1	O
z	O
(	O
1	O
)	O
n	O
z	O
(	O
1	O
)	O
n+1	O
xn−1	O
xn	O
xn+1	O
markov	O
chains	O
of	O
latent	O
variables	O
,	O
and	O
the	O
distribution	O
of	O
the	O
observed	B
variable	I
at	O
a	O
given	O
time	O
step	O
is	O
conditional	B
on	O
the	O
states	O
of	O
all	O
of	O
the	O
corresponding	O
latent	O
vari-	O
ables	O
at	O
that	O
same	O
time	O
step	O
.	O
figure	O
13.19	O
shows	O
the	O
corresponding	O
graphical	B
model	I
.	O
the	O
motivation	O
for	O
considering	O
factorial	B
hmm	O
can	O
be	O
seen	O
by	O
noting	O
that	O
in	O
order	O
to	O
represent	O
,	O
say	O
,	O
10	O
bits	B
of	O
information	O
at	O
a	O
given	O
time	O
step	O
,	O
a	O
standard	O
hmm	O
would	O
need	O
k	O
=	O
210	O
=	O
1024	O
latent	O
states	O
,	O
whereas	O
a	O
factorial	B
hmm	O
could	O
make	O
use	O
of	O
10	O
binary	O
latent	O
chains	O
.	O
the	O
primary	O
disadvantage	O
of	O
factorial	B
hmms	O
,	O
however	O
,	O
lies	O
in	O
the	O
additional	O
complexity	O
of	O
training	B
them	O
.	O
the	O
m	O
step	O
for	O
the	O
factorial	B
hmm	O
model	O
is	O
straightforward	O
.	O
however	O
,	O
observation	O
of	O
the	O
x	O
variables	O
introduces	O
dependencies	O
between	O
the	O
latent	O
chains	O
,	O
leading	O
to	O
difﬁculties	O
with	O
the	O
e	O
step	O
.	O
this	O
can	O
be	O
seen	O
by	O
noting	O
that	O
in	O
figure	O
13.19	O
,	O
the	O
variables	O
z	O
(	O
1	O
)	O
n	O
are	O
connected	O
by	O
a	O
path	O
which	O
is	O
head-to-head	O
at	O
node	B
xn	O
and	O
hence	O
they	O
are	O
not	O
d-separated	O
.	O
the	O
exact	O
e	O
step	O
for	O
this	O
model	O
does	O
not	O
correspond	O
to	O
running	O
forward	O
and	O
backward	O
recursions	O
along	O
the	O
m	O
markov	O
chains	O
independently	O
.	O
this	O
is	O
conﬁrmed	O
by	O
noting	O
that	O
the	O
key	O
conditional	B
independence	I
property	O
(	O
13.5	O
)	O
is	O
not	O
satisﬁed	O
for	O
the	O
individual	O
markov	O
chains	O
in	O
the	O
factorial	B
hmm	O
model	O
,	O
as	O
is	O
shown	O
using	O
d-separation	B
in	O
figure	O
13.20.	O
now	O
suppose	O
that	O
there	O
are	O
m	O
chains	O
of	O
hidden	O
nodes	O
and	O
for	O
simplicity	O
suppose	O
that	O
all	O
latent	O
variables	O
have	O
the	O
same	O
number	O
k	O
of	O
states	O
.	O
then	O
one	O
approach	O
would	O
be	O
to	O
note	O
that	O
there	O
are	O
km	O
combinations	O
of	O
latent	O
variables	O
at	O
a	O
given	O
time	O
step	O
n	O
and	O
z	O
(	O
2	O
)	O
figure	O
13.20	O
example	O
of	O
a	O
path	O
,	O
highlighted	O
in	O
green	O
,	O
which	O
is	O
head-to-head	O
at	O
the	O
observed	O
nodes	O
xn−1	O
and	O
xn+1	O
,	O
and	O
head-to-tail	O
at	O
the	O
unobserved	O
nodes	O
z	O
(	O
2	O
)	O
n	O
and	O
z	O
(	O
2	O
)	O
n+1	O
.	O
thus	O
the	O
path	O
is	O
not	O
blocked	O
and	O
so	O
the	O
conditional	B
independence	I
property	O
(	O
13.5	O
)	O
does	O
not	O
hold	O
for	O
the	O
individual	O
la-	O
tent	O
chains	O
of	O
the	O
factorial	B
hmm	O
model	O
.	O
as	O
a	O
consequence	O
,	O
there	O
is	O
no	O
efﬁcient	O
exact	O
e	O
step	O
for	O
this	O
model	O
.	O
n−1	O
,	O
z	O
(	O
2	O
)	O
z	O
(	O
2	O
)	O
n−1	O
z	O
(	O
2	O
)	O
n	O
z	O
(	O
2	O
)	O
n+1	O
z	O
(	O
1	O
)	O
n−1	O
z	O
(	O
1	O
)	O
n	O
z	O
(	O
1	O
)	O
n+1	O
xn−1	O
xn	O
xn+1	O
section	O
10.1	O
13.3.	O
linear	O
dynamical	O
systems	O
635	O
and	O
so	O
we	O
can	O
transform	O
the	O
model	O
into	O
an	O
equivalent	O
standard	O
hmm	O
having	O
a	O
single	O
chain	O
of	O
latent	O
variables	O
each	O
of	O
which	O
has	O
km	O
latent	O
states	O
.	O
we	O
can	O
then	O
run	O
the	O
standard	O
forward-backward	O
recursions	O
in	O
the	O
e	O
step	O
.	O
this	O
has	O
computational	O
com-	O
plexity	O
o	O
(	O
n	O
k	O
2m	O
)	O
that	O
is	O
exponential	O
in	O
the	O
number	O
m	O
of	O
latent	O
chains	O
and	O
so	O
will	O
be	O
intractable	O
for	O
anything	O
other	O
than	O
small	O
values	O
of	O
m.	O
one	O
solution	O
would	O
be	O
to	O
use	O
sampling	B
methods	I
(	O
discussed	O
in	O
chapter	O
11	O
)	O
.	O
as	O
an	O
elegant	O
deterministic	O
al-	O
ternative	O
,	O
ghahramani	O
and	O
jordan	O
(	O
1997	O
)	O
exploited	O
variational	B
inference	I
techniques	O
to	O
obtain	O
a	O
tractable	O
algorithm	O
for	O
approximate	O
inference	B
.	O
this	O
can	O
be	O
done	O
using	O
a	O
simple	O
variational	B
posterior	O
distribution	O
that	O
is	O
fully	O
factorized	O
with	O
respect	O
to	O
the	O
latent	O
variables	O
,	O
or	O
alternatively	O
by	O
using	O
a	O
more	O
powerful	O
approach	O
in	O
which	O
the	O
variational	B
distribution	O
is	O
described	O
by	O
independent	B
markov	O
chains	O
corresponding	O
to	O
the	O
chains	O
of	O
latent	O
variables	O
in	O
the	O
original	O
model	O
.	O
in	O
the	O
latter	O
case	O
,	O
the	O
variational	B
inference	I
algorithms	O
involves	O
running	O
independent	O
forward	O
and	O
backward	O
recursions	O
along	O
each	O
chain	O
,	O
which	O
is	O
computationally	O
efﬁcient	O
and	O
yet	O
is	O
also	O
able	O
to	O
capture	O
correlations	O
between	O
variables	O
within	O
the	O
same	O
chain	O
.	O
clearly	O
,	O
there	O
are	O
many	O
possible	O
probabilistic	O
structures	O
that	O
can	O
be	O
constructed	O
according	O
to	O
the	O
needs	O
of	O
particular	O
applications	O
.	O
graphical	O
models	O
provide	O
a	O
general	O
technique	O
for	O
motivating	O
,	O
describing	O
,	O
and	O
analysing	O
such	O
structures	O
,	O
and	O
variational	B
methods	O
provide	O
a	O
powerful	O
framework	O
for	O
performing	O
inference	B
in	O
those	O
models	O
for	O
which	O
exact	O
solution	O
is	O
intractable	O
.	O
13.3.	O
linear	O
dynamical	O
systems	O
in	O
order	O
to	O
motivate	O
the	O
concept	O
of	O
linear	O
dynamical	O
systems	O
,	O
let	O
us	O
consider	O
the	O
following	O
simple	O
problem	O
,	O
which	O
often	O
arises	O
in	O
practical	O
settings	O
.	O
suppose	O
we	O
wish	O
to	O
measure	O
the	O
value	O
of	O
an	O
unknown	O
quantity	O
z	O
using	O
a	O
noisy	O
sensor	O
that	O
returns	O
a	O
observation	O
x	O
representing	O
the	O
value	O
of	O
z	O
plus	O
zero-mean	O
gaussian	O
noise	O
.	O
given	O
a	O
single	O
measurement	O
,	O
our	O
best	O
guess	O
for	O
z	O
is	O
to	O
assume	O
that	O
z	O
=	O
x.	O
however	O
,	O
we	O
can	O
improve	O
our	O
estimate	O
for	O
z	O
by	O
taking	O
lots	O
of	O
measurements	O
and	O
averaging	O
them	O
,	O
because	O
the	O
random	O
noise	O
terms	O
will	O
tend	O
to	O
cancel	O
each	O
other	O
.	O
now	O
let	O
’	O
s	O
make	O
the	O
situation	O
more	O
complicated	O
by	O
assuming	O
that	O
we	O
wish	O
to	O
measure	O
a	O
quantity	O
z	O
that	O
is	O
changing	O
over	O
time	O
.	O
we	O
can	O
take	O
regular	O
measurements	O
of	O
x	O
so	O
that	O
at	O
some	O
point	O
in	O
time	O
we	O
have	O
obtained	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
and	O
we	O
wish	O
to	O
ﬁnd	O
the	O
corresponding	O
values	O
z1	O
,	O
.	O
.	O
.	O
,	O
xn	O
.	O
if	O
we	O
simply	O
average	O
the	O
measurements	O
,	O
the	O
error	B
due	O
to	O
random	O
noise	O
will	O
be	O
reduced	O
,	O
but	O
unfortunately	O
we	O
will	O
just	O
obtain	O
a	O
single	O
averaged	O
estimate	O
,	O
in	O
which	O
we	O
have	O
averaged	O
over	O
the	O
changing	O
value	O
of	O
z	O
,	O
thereby	O
introducing	O
a	O
new	O
source	O
of	O
error	B
.	O
intuitively	O
,	O
we	O
could	O
imagine	O
doing	O
a	O
bit	O
better	O
as	O
follows	O
.	O
to	O
estimate	O
the	O
value	O
of	O
zn	O
,	O
we	O
take	O
only	O
the	O
most	O
recent	O
few	O
measurements	O
,	O
say	O
xn−l	O
,	O
.	O
.	O
.	O
,	O
xn	O
and	O
just	O
average	O
these	O
.	O
if	O
z	O
is	O
changing	O
slowly	O
,	O
and	O
the	O
random	O
noise	O
level	O
in	O
the	O
sensor	O
is	O
high	O
,	O
it	O
would	O
make	O
sense	O
to	O
choose	O
a	O
relatively	O
long	O
window	O
of	O
observations	O
to	O
average	O
.	O
conversely	O
,	O
if	O
the	O
signal	O
is	O
changing	O
quickly	O
,	O
and	O
the	O
noise	O
levels	O
are	O
small	O
,	O
we	O
might	O
be	O
better	O
just	O
to	O
use	O
xn	O
directly	O
as	O
our	O
estimate	O
of	O
zn	O
.	O
perhaps	O
we	O
could	O
do	O
even	O
better	O
if	O
we	O
take	O
a	O
weighted	O
average	O
,	O
in	O
which	O
more	O
recent	O
measurements	O
636	O
13.	O
sequential	B
data	I
make	O
a	O
greater	O
contribution	O
than	O
less	O
recent	O
ones	O
.	O
although	O
this	O
sort	O
of	O
intuitive	O
argument	O
seems	O
plausible	O
,	O
it	O
does	O
not	O
tell	O
us	O
how	O
to	O
form	O
a	O
weighted	O
average	O
,	O
and	O
any	O
sort	O
of	O
hand-crafted	O
weighing	O
is	O
hardly	O
likely	O
to	O
be	O
optimal	O
.	O
fortunately	O
,	O
we	O
can	O
address	O
problems	O
such	O
as	O
this	O
much	O
more	O
sys-	O
tematically	O
by	O
deﬁning	O
a	O
probabilistic	O
model	O
that	O
captures	O
the	O
time	O
evolution	O
and	O
measurement	O
processes	O
and	O
then	O
applying	O
the	O
inference	B
and	O
learning	B
methods	O
devel-	O
oped	O
in	O
earlier	O
chapters	O
.	O
here	O
we	O
shall	O
focus	O
on	O
a	O
widely	O
used	O
model	O
known	O
as	O
a	O
linear	B
dynamical	I
system	I
.	O
as	O
we	O
have	O
seen	O
,	O
the	O
hmm	O
corresponds	O
to	O
the	O
state	B
space	I
model	I
shown	O
in	O
figure	O
13.5	O
in	O
which	O
the	O
latent	O
variables	O
are	O
discrete	O
but	O
with	O
arbitrary	O
emission	B
probability	I
distributions	O
.	O
this	O
graph	O
of	O
course	O
describes	O
a	O
much	O
broader	O
class	O
of	O
probability	B
distributions	O
,	O
all	O
of	O
which	O
factorize	O
according	O
to	O
(	O
13.6	O
)	O
.	O
we	O
now	O
consider	O
extensions	O
to	O
other	O
distributions	O
for	O
the	O
latent	O
variables	O
.	O
in	O
particular	O
,	O
we	O
consider	O
continuous	O
latent	O
variables	O
in	O
which	O
the	O
summations	O
of	O
the	O
sum-product	B
algorithm	I
become	O
integrals	O
.	O
the	O
general	O
form	O
of	O
the	O
inference	B
algorithms	O
will	O
,	O
however	O
,	O
be	O
the	O
same	O
as	O
for	O
the	O
hidden	O
markov	O
model	O
.	O
it	O
is	O
interesting	O
to	O
note	O
that	O
,	O
historically	O
,	O
hidden	O
markov	O
models	O
and	O
linear	O
dynamical	O
systems	O
were	O
developed	O
independently	O
.	O
once	O
they	O
are	O
both	O
expressed	O
as	O
graphical	O
models	O
,	O
however	O
,	O
the	O
deep	O
relationship	O
between	O
them	O
immediately	O
becomes	O
apparent	O
.	O
one	O
key	O
requirement	O
is	O
that	O
we	O
retain	O
an	O
efﬁcient	O
algorithm	O
for	O
inference	O
which	O
is	O
linear	O
in	O
the	O
length	O
of	O
the	O
chain	O
.	O
this	O
requires	O
that	O
,	O
for	O
instance	O
,	O
when	O
we	O
take	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
and	O
multiply	O
by	O
the	O
transition	B
probability	I
p	O
(	O
zn|zn−1	O
)	O
and	O
the	O
emission	B
probability	I
p	O
(	O
xn|zn	O
)	O
and	O
then	O
marginalize	O
over	O
zn−1	O
,	O
we	O
obtain	O
a	O
distribution	O
over	O
a	O
quantity	O
(	O
cid:1	O
)	O
α	O
(	O
zn−1	O
)	O
,	O
representing	O
the	O
posterior	B
probability	I
of	O
zn	O
given	O
observations	O
zn	O
that	O
is	O
of	O
the	O
same	O
functional	B
form	O
as	O
that	O
over	O
(	O
cid:1	O
)	O
α	O
(	O
zn−1	O
)	O
.	O
that	O
is	O
to	O
say	O
,	O
the	O
distribution	O
must	O
not	O
become	O
more	O
complex	O
at	O
each	O
stage	O
,	O
but	O
must	O
only	O
change	O
in	O
its	O
parameter	O
values	O
.	O
not	O
surprisingly	O
,	O
the	O
only	O
distributions	O
that	O
have	O
this	O
property	O
of	O
being	O
closed	O
under	O
multiplication	O
are	O
those	O
belonging	O
to	O
the	O
exponential	B
family	I
.	O
here	O
we	O
consider	O
the	O
most	O
important	O
example	O
from	O
a	O
practical	O
perspective	O
,	O
which	O
is	O
the	O
gaussian	O
.	O
in	O
particular	O
,	O
we	O
consider	O
a	O
linear-gaussian	O
state	B
space	I
model	I
so	O
that	O
the	O
latent	O
variables	O
{	O
zn	O
}	O
,	O
as	O
well	O
as	O
the	O
observed	O
variables	O
{	O
xn	O
}	O
,	O
are	O
multi-	O
variate	O
gaussian	O
distributions	O
whose	O
means	O
are	O
linear	O
functions	O
of	O
the	O
states	O
of	O
their	O
parents	O
in	O
the	O
graph	O
.	O
we	O
have	O
seen	O
that	O
a	O
directed	B
graph	O
of	O
linear-gaussian	O
units	O
is	O
equivalent	O
to	O
a	O
joint	O
gaussian	O
distribution	O
over	O
all	O
of	O
the	O
variables	O
.	O
furthermore	O
,	O
marginals	O
such	O
as	O
(	O
cid:1	O
)	O
α	O
(	O
zn	O
)	O
are	O
also	O
gaussian	O
,	O
so	O
that	O
the	O
functional	B
form	O
of	O
the	O
mes-	O
each	O
of	O
which	O
has	O
a	O
mean	B
that	O
is	O
linear	O
in	O
zn	O
.	O
then	O
even	O
if	O
(	O
cid:1	O
)	O
α	O
(	O
z1	O
)	O
is	O
gaussian	O
,	O
the	O
quantity	O
(	O
cid:1	O
)	O
α	O
(	O
z2	O
)	O
will	O
be	O
a	O
mixture	O
of	O
k	O
gaussians	O
,	O
(	O
cid:1	O
)	O
α	O
(	O
z3	O
)	O
will	O
be	O
a	O
mixture	O
of	O
k	O
2	O
sages	O
is	O
preserved	O
and	O
we	O
will	O
obtain	O
an	O
efﬁcient	O
inference	B
algorithm	O
.	O
by	O
contrast	O
,	O
suppose	O
that	O
the	O
emission	O
densities	O
p	O
(	O
xn|zn	O
)	O
comprise	O
a	O
mixture	O
of	O
k	O
gaussians	O
gaussians	O
,	O
and	O
so	O
on	O
,	O
and	O
exact	O
inference	O
will	O
not	O
be	O
of	O
practical	O
value	O
.	O
we	O
have	O
seen	O
that	O
the	O
hidden	O
markov	O
model	O
can	O
be	O
viewed	O
as	O
an	O
extension	O
of	O
the	O
mixture	B
models	O
of	O
chapter	O
9	O
to	O
allow	O
for	O
sequential	O
correlations	O
in	O
the	O
data	O
.	O
in	O
a	O
similar	O
way	O
,	O
we	O
can	O
view	O
the	O
linear	B
dynamical	I
system	I
as	O
a	O
generalization	B
of	O
the	O
continuous	O
latent	B
variable	I
models	O
of	O
chapter	O
12	O
such	O
as	O
probabilistic	O
pca	O
and	O
factor	B
analysis	I
.	O
each	O
pair	O
of	O
nodes	O
{	O
zn	O
,	O
xn	O
}	O
represents	O
a	O
linear-gaussian	O
latent	B
variable	I
13.3.	O
linear	O
dynamical	O
systems	O
637	O
model	O
for	O
that	O
particular	O
observation	O
.	O
however	O
,	O
the	O
latent	O
variables	O
{	O
zn	O
}	O
are	O
no	O
longer	O
treated	O
as	O
independent	B
but	O
now	O
form	O
a	O
markov	O
chain	O
.	O
because	O
the	O
model	O
is	O
represented	O
by	O
a	O
tree-structured	O
directed	B
graph	O
,	O
inference	B
problems	O
can	O
be	O
solved	O
efﬁciently	O
using	O
the	O
sum-product	B
algorithm	I
.	O
the	O
forward	O
re-	O
cursions	O
,	O
analogous	O
to	O
the	O
α	O
messages	O
of	O
the	O
hidden	O
markov	O
model	O
,	O
are	O
known	O
as	O
the	O
kalman	O
ﬁlter	O
equations	O
(	O
kalman	O
,	O
1960	O
;	O
zarchan	O
and	O
musoff	O
,	O
2005	O
)	O
,	O
and	O
the	O
back-	O
ward	O
recursions	O
,	O
analogous	O
to	O
the	O
β	O
messages	O
,	O
are	O
known	O
as	O
the	O
kalman	O
smoother	O
equations	O
,	O
or	O
the	O
rauch-tung-striebel	O
(	O
rts	O
)	O
equations	O
(	O
rauch	O
et	O
al.	O
,	O
1965	O
)	O
.	O
the	O
kalman	O
ﬁlter	O
is	O
widely	O
used	O
in	O
many	O
real-time	O
tracking	O
applications	O
.	O
because	O
the	O
linear	B
dynamical	I
system	I
is	O
a	O
linear-gaussian	O
model	O
,	O
the	O
joint	O
distri-	O
bution	O
over	O
all	O
variables	O
,	O
as	O
well	O
as	O
all	O
marginals	O
and	O
conditionals	O
,	O
will	O
be	O
gaussian	O
.	O
it	O
follows	O
that	O
the	O
sequence	O
of	O
individually	O
most	O
probable	O
latent	B
variable	I
values	O
is	O
the	O
same	O
as	O
the	O
most	O
probable	O
latent	O
sequence	O
.	O
there	O
is	O
thus	O
no	O
need	O
to	O
consider	O
the	O
analogue	O
of	O
the	O
viterbi	O
algorithm	O
for	O
the	O
linear	B
dynamical	I
system	I
.	O
because	O
the	O
model	O
has	O
linear-gaussian	O
conditional	B
distributions	O
,	O
we	O
can	O
write	O
the	O
transition	O
and	O
emission	O
distributions	O
in	O
the	O
general	O
form	O
p	O
(	O
zn|zn−1	O
)	O
=	O
n	O
(	O
zn|azn−1	O
,	O
γ	O
)	O
p	O
(	O
xn|zn	O
)	O
=	O
n	O
(	O
xn|czn	O
,	O
σ	O
)	O
.	O
the	O
initial	O
latent	B
variable	I
also	O
has	O
a	O
gaussian	O
distribution	O
which	O
we	O
write	O
as	O
p	O
(	O
z1	O
)	O
=	O
n	O
(	O
z1|µ0	O
,	O
v0	O
)	O
.	O
(	O
13.75	O
)	O
(	O
13.76	O
)	O
(	O
13.77	O
)	O
exercise	O
13.19	O
exercise	O
13.24	O
note	O
that	O
in	O
order	O
to	O
simplify	O
the	O
notation	O
,	O
we	O
have	O
omitted	O
additive	O
constant	O
terms	O
from	O
the	O
means	O
of	O
the	O
gaussians	O
.	O
in	O
fact	O
,	O
it	O
is	O
straightforward	O
to	O
include	O
them	O
if	O
desired	O
.	O
traditionally	O
,	O
these	O
distributions	O
are	O
more	O
commonly	O
expressed	O
in	O
an	O
equiv-	O
alent	O
form	O
in	O
terms	O
of	O
noisy	O
linear	O
equations	O
given	O
by	O
zn	O
=	O
azn−1	O
+	O
wn	O
xn	O
=	O
czn	O
+	O
vn	O
z1	O
=	O
µ0	O
+	O
u	O
(	O
13.78	O
)	O
(	O
13.79	O
)	O
(	O
13.80	O
)	O
where	O
the	O
noise	O
terms	O
have	O
the	O
distributions	O
w	O
∼	O
n	O
(	O
w|0	O
,	O
γ	O
)	O
v	O
∼	O
n	O
(	O
v|0	O
,	O
σ	O
)	O
u	O
∼	O
n	O
(	O
u|0	O
,	O
v0	O
)	O
.	O
(	O
13.81	O
)	O
(	O
13.82	O
)	O
(	O
13.83	O
)	O
the	O
parameters	O
of	O
the	O
model	O
,	O
denoted	O
by	O
θ	O
=	O
{	O
a	O
,	O
γ	O
,	O
c	O
,	O
σ	O
,	O
µ0	O
,	O
v0	O
}	O
,	O
can	O
be	O
determined	O
using	O
maximum	B
likelihood	I
through	O
the	O
em	O
algorithm	O
.	O
in	O
the	O
e	O
step	O
,	O
we	O
need	O
to	O
solve	O
the	O
inference	B
problem	O
of	O
determining	O
the	O
local	B
posterior	O
marginals	O
for	O
the	O
latent	O
variables	O
,	O
which	O
can	O
be	O
solved	O
efﬁciently	O
using	O
the	O
sum-product	B
algorithm	I
,	O
as	O
we	O
discuss	O
in	O
the	O
next	O
section	O
.	O
638	O
13.	O
sequential	B
data	I
13.3.1	O
inference	B
in	O
lds	O
we	O
now	O
turn	O
to	O
the	O
problem	O
of	O
ﬁnding	O
the	O
marginal	B
distributions	O
for	O
the	O
latent	O
variables	O
conditional	B
on	O
the	O
observation	O
sequence	O
.	O
for	O
given	O
parameter	O
settings	O
,	O
we	O
also	O
wish	O
to	O
make	O
predictions	O
of	O
the	O
next	O
latent	O
state	O
zn	O
and	O
of	O
the	O
next	O
observation	O
xn	O
conditioned	O
on	O
the	O
observed	O
data	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn−1	O
for	O
use	O
in	O
real-time	O
applications	O
.	O
these	O
inference	B
problems	O
can	O
be	O
solved	O
efﬁciently	O
using	O
the	O
sum-product	B
algorithm	I
,	O
which	O
in	O
the	O
context	O
of	O
the	O
linear	B
dynamical	I
system	I
gives	O
rise	O
to	O
the	O
kalman	O
ﬁlter	O
and	O
kalman	O
smoother	O
equations	O
.	O
it	O
is	O
worth	O
emphasizing	O
that	O
because	O
the	O
linear	B
dynamical	I
system	I
is	O
a	O
linear-	O
gaussian	O
model	O
,	O
the	O
joint	O
distribution	O
over	O
all	O
latent	O
and	O
observed	O
variables	O
is	O
simply	O
a	O
gaussian	O
,	O
and	O
so	O
in	O
principle	O
we	O
could	O
solve	O
inference	B
problems	O
by	O
using	O
the	O
standard	O
results	O
derived	O
in	O
previous	O
chapters	O
for	O
the	O
marginals	O
and	O
conditionals	O
of	O
a	O
multivariate	O
gaussian	O
.	O
the	O
role	O
of	O
the	O
sum-product	B
algorithm	I
is	O
to	O
provide	O
a	O
more	O
efﬁcient	O
way	O
to	O
perform	O
such	O
computations	O
.	O
linear	O
dynamical	O
systems	O
have	O
the	O
identical	O
factorization	B
,	O
given	O
by	O
(	O
13.6	O
)	O
,	O
to	O
hidden	O
markov	O
models	O
,	O
and	O
are	O
again	O
described	O
by	O
the	O
factor	O
graphs	O
in	O
figures	O
13.14	O
and	O
13.15.	O
inference	B
algorithms	O
therefore	O
take	O
precisely	O
the	O
same	O
form	O
except	O
that	O
summations	O
over	O
latent	O
variables	O
are	O
replaced	O
by	O
integrations	O
.	O
we	O
begin	O
by	O
consid-	O
ering	O
the	O
forward	O
equations	O
in	O
which	O
we	O
treat	O
zn	O
as	O
the	O
root	B
node	I
,	O
and	O
propagate	O
messages	O
from	O
the	O
leaf	O
node	B
h	O
(	O
z1	O
)	O
to	O
the	O
root	O
.	O
from	O
(	O
13.77	O
)	O
,	O
the	O
initial	O
message	O
will	O
be	O
gaussian	O
,	O
and	O
because	O
each	O
of	O
the	O
factors	O
is	O
gaussian	O
,	O
all	O
subsequent	O
messages	O
will	O
also	O
be	O
gaussian	O
.	O
by	O
convention	O
,	O
we	O
shall	O
propagate	O
messages	O
that	O
are	O
nor-	O
malized	O
marginal	B
distributions	O
corresponding	O
to	O
p	O
(	O
zn|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
,	O
which	O
we	O
denote	O
by	O
this	O
is	O
precisely	O
analogous	O
to	O
the	O
propagation	O
of	O
scaled	O
variables	O
(	O
cid:1	O
)	O
α	O
(	O
zn	O
)	O
given	O
by	O
(	O
cid:1	O
)	O
α	O
(	O
zn	O
)	O
=	O
n	O
(	O
zn|µn	O
,	O
vn	O
)	O
.	O
(	O
13.84	O
)	O
(	O
13.59	O
)	O
in	O
the	O
discrete	O
case	O
of	O
the	O
hidden	O
markov	O
model	O
,	O
and	O
so	O
the	O
recursion	O
equa-	O
tion	O
now	O
takes	O
the	O
form	O
cn	O
(	O
cid:1	O
)	O
α	O
(	O
zn	O
)	O
=	O
p	O
(	O
xn|zn	O
)	O
(	O
cid:6	O
)	O
(	O
cid:1	O
)	O
α	O
(	O
zn−1	O
)	O
p	O
(	O
zn|zn−1	O
)	O
dzn−1	O
.	O
(	O
13.85	O
)	O
substituting	O
for	O
the	O
conditionals	O
p	O
(	O
zn|zn−1	O
)	O
and	O
p	O
(	O
xn|zn	O
)	O
,	O
using	O
(	O
13.75	O
)	O
and	O
(	O
13.76	O
)	O
,	O
respectively	O
,	O
and	O
making	O
use	O
of	O
(	O
13.84	O
)	O
,	O
we	O
see	O
that	O
(	O
13.85	O
)	O
becomes	O
(	O
cid:6	O
)	O
cnn	O
(	O
zn|µn	O
,	O
vn	O
)	O
=	O
n	O
(	O
xn|czn	O
,	O
σ	O
)	O
n	O
(	O
zn|azn−1	O
,	O
γ	O
)	O
n	O
(	O
zn−1|µn−1	O
,	O
vn−1	O
)	O
dzn−1	O
.	O
(	O
cid:6	O
)	O
here	O
we	O
are	O
supposing	O
that	O
µn−1	O
and	O
vn−1	O
are	O
known	O
,	O
and	O
by	O
evaluating	O
the	O
inte-	O
gral	O
in	O
(	O
13.86	O
)	O
,	O
we	O
wish	O
to	O
determine	O
values	O
for	O
µn	O
and	O
vn	O
.	O
the	O
integral	O
is	O
easily	O
evaluated	O
by	O
making	O
use	O
of	O
the	O
result	O
(	O
2.115	O
)	O
,	O
from	O
which	O
it	O
follows	O
that	O
n	O
(	O
zn|azn−1	O
,	O
γ	O
)	O
n	O
(	O
zn−1|µn−1	O
,	O
vn−1	O
)	O
dzn−1	O
(	O
13.86	O
)	O
=	O
n	O
(	O
zn|aµn−1	O
,	O
pn−1	O
)	O
(	O
13.87	O
)	O
13.3.	O
linear	O
dynamical	O
systems	O
639	O
where	O
we	O
have	O
deﬁned	O
pn−1	O
=	O
avn−1at	O
+	O
γ	O
.	O
(	O
13.88	O
)	O
we	O
can	O
now	O
combine	O
this	O
result	O
with	O
the	O
ﬁrst	O
factor	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
13.86	O
)	O
by	O
making	O
use	O
of	O
(	O
2.115	O
)	O
and	O
(	O
2.116	O
)	O
to	O
give	O
µn	O
=	O
aµn−1	O
+	O
kn	O
(	O
xn	O
−	O
caµn−1	O
)	O
vn	O
=	O
(	O
i	O
−	O
knc	O
)	O
pn−1	O
cn	O
=	O
n	O
(	O
xn|caµn−1	O
,	O
cpn−1ct	O
+	O
σ	O
)	O
.	O
(	O
13.89	O
)	O
(	O
13.90	O
)	O
(	O
13.91	O
)	O
here	O
we	O
have	O
made	O
use	O
of	O
the	O
matrix	O
inverse	B
identities	O
(	O
c.5	O
)	O
and	O
(	O
c.7	O
)	O
and	O
also	O
deﬁned	O
the	O
kalman	O
gain	O
matrix	O
(	O
cid:11	O
)	O
−1	O
kn	O
=	O
pn−1ct	O
cpn−1ct	O
+	O
σ	O
.	O
(	O
13.92	O
)	O
thus	O
,	O
given	O
the	O
values	O
of	O
µn−1	O
and	O
vn−1	O
,	O
together	O
with	O
the	O
new	O
observation	O
xn	O
,	O
we	O
can	O
evaluate	O
the	O
gaussian	O
marginal	B
for	O
zn	O
having	O
mean	B
µn	O
and	O
covariance	B
vn	O
,	O
as	O
well	O
as	O
the	O
normalization	O
coefﬁcient	O
cn	O
.	O
the	O
initial	O
conditions	O
for	O
these	O
recursion	O
equations	O
are	O
obtained	O
from	O
c1	O
(	O
cid:1	O
)	O
α	O
(	O
z1	O
)	O
=	O
p	O
(	O
z1	O
)	O
p	O
(	O
x1|z1	O
)	O
.	O
(	O
13.93	O
)	O
because	O
p	O
(	O
z1	O
)	O
is	O
given	O
by	O
(	O
13.77	O
)	O
,	O
and	O
p	O
(	O
x1|z1	O
)	O
is	O
given	O
by	O
(	O
13.76	O
)	O
,	O
we	O
can	O
again	O
make	O
use	O
of	O
(	O
2.115	O
)	O
to	O
calculate	O
c1	O
and	O
(	O
2.116	O
)	O
to	O
calculate	O
µ1	O
and	O
v1	O
giving	O
(	O
cid:10	O
)	O
(	O
cid:10	O
)	O
µ1	O
=	O
µ0	O
+	O
k1	O
(	O
x1	O
−	O
cµ0	O
)	O
v1	O
=	O
(	O
i	O
−	O
k1c	O
)	O
v0	O
c1	O
=	O
n	O
(	O
x1|cµ0	O
,	O
cv0ct	O
+	O
σ	O
)	O
(	O
cid:11	O
)	O
−1	O
where	O
k1	O
=	O
v0ct	O
cv0ct	O
+	O
σ	O
.	O
(	O
13.94	O
)	O
(	O
13.95	O
)	O
(	O
13.96	O
)	O
(	O
13.97	O
)	O
similarly	O
,	O
the	O
likelihood	B
function	I
for	O
the	O
linear	B
dynamical	I
system	I
is	O
given	O
by	O
(	O
13.63	O
)	O
in	O
which	O
the	O
factors	O
cn	O
are	O
found	O
using	O
the	O
kalman	O
ﬁltering	O
equations	O
.	O
we	O
can	O
interpret	O
the	O
steps	O
involved	O
in	O
going	O
from	O
the	O
posterior	O
marginal	O
over	O
zn−1	O
to	O
the	O
posterior	O
marginal	O
over	O
zn	O
as	O
follows	O
.	O
in	O
(	O
13.89	O
)	O
,	O
we	O
can	O
view	O
the	O
quantity	O
aµn−1	O
as	O
the	O
prediction	O
of	O
the	O
mean	B
over	O
zn	O
obtained	O
by	O
simply	O
taking	O
the	O
mean	B
over	O
zn−1	O
and	O
projecting	O
it	O
forward	O
one	O
step	O
using	O
the	O
transition	B
probability	I
matrix	O
a.	O
this	O
predicted	O
mean	B
would	O
give	O
a	O
predicted	O
observation	O
for	O
xn	O
given	O
by	O
cazn−1	O
obtained	O
by	O
applying	O
the	O
emission	B
probability	I
matrix	O
c	O
to	O
the	O
predicted	O
hidden	O
state	O
mean	B
.	O
we	O
can	O
view	O
the	O
update	O
equation	O
(	O
13.89	O
)	O
for	O
the	O
mean	B
of	O
the	O
hidden	B
variable	I
distribution	O
as	O
taking	O
the	O
predicted	O
mean	B
aµn−1	O
and	O
then	O
adding	O
a	O
correction	O
that	O
is	O
proportional	O
to	O
the	O
error	B
xn	O
−	O
cazn−1	O
between	O
the	O
predicted	O
observation	O
and	O
the	O
actual	O
observation	O
.	O
the	O
coefﬁcient	O
of	O
this	O
correction	O
is	O
given	O
by	O
the	O
kalman	O
gain	O
matrix	O
.	O
thus	O
we	O
can	O
view	O
the	O
kalman	O
ﬁlter	O
as	O
a	O
process	O
of	O
making	O
successive	O
predictions	O
and	O
then	O
correcting	O
these	O
predictions	O
in	O
the	O
light	O
of	O
the	O
new	O
observations	O
.	O
this	O
is	O
illustrated	O
graphically	O
in	O
figure	O
13.21	O
.	O
640	O
13.	O
sequential	B
data	I
zn−1	O
zn	O
zn	O
figure	O
13.21	O
the	O
linear	B
dynamical	I
system	I
can	O
be	O
viewed	O
as	O
a	O
sequence	O
of	O
steps	O
in	O
which	O
increasing	O
un-	O
certainty	O
in	O
the	O
state	O
variable	O
due	O
to	O
diffusion	O
is	O
compensated	O
by	O
the	O
arrival	O
of	O
new	O
data	O
.	O
in	O
the	O
left-hand	O
plot	O
,	O
the	O
blue	O
curve	O
shows	O
the	O
distribution	O
p	O
(	O
zn−1|x1	O
,	O
.	O
.	O
.	O
,	O
xn−1	O
)	O
,	O
which	O
incorporates	O
all	O
the	O
data	O
up	O
to	O
step	O
n	O
−	O
1.	O
the	O
diffusion	O
arising	O
from	O
the	O
nonzero	O
variance	B
of	O
the	O
transition	B
probability	I
p	O
(	O
zn|zn−1	O
)	O
gives	O
the	O
distribution	O
p	O
(	O
zn|x1	O
,	O
.	O
.	O
.	O
,	O
xn−1	O
)	O
,	O
shown	O
in	O
red	O
in	O
the	O
centre	O
plot	O
.	O
note	O
that	O
this	O
is	O
broader	O
and	O
shifted	O
relative	B
to	O
the	O
blue	O
curve	O
(	O
which	O
is	O
shown	O
dashed	O
in	O
the	O
centre	O
plot	O
for	O
comparison	O
)	O
.	O
the	O
next	O
data	O
observation	O
xn	O
contributes	O
through	O
the	O
emission	O
density	O
p	O
(	O
xn|zn	O
)	O
,	O
which	O
is	O
shown	O
as	O
a	O
function	O
of	O
zn	O
in	O
green	O
on	O
the	O
right-hand	O
plot	O
.	O
note	O
that	O
this	O
is	O
not	O
a	O
density	B
with	O
respect	O
to	O
zn	O
and	O
so	O
is	O
not	O
normalized	O
to	O
one	O
.	O
inclusion	O
of	O
this	O
new	O
data	O
point	O
leads	O
to	O
a	O
revised	O
distribution	O
p	O
(	O
zn|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
for	O
the	O
state	O
density	O
shown	O
in	O
blue	O
.	O
we	O
see	O
that	O
observation	O
of	O
the	O
data	O
has	O
shifted	O
and	O
narrowed	O
the	O
distribution	O
compared	O
to	O
p	O
(	O
zn|x1	O
,	O
.	O
.	O
.	O
,	O
xn−1	O
)	O
(	O
which	O
is	O
shown	O
in	O
dashed	O
in	O
the	O
right-hand	O
plot	O
for	O
comparison	O
)	O
.	O
exercise	O
13.27	O
exercise	O
13.28	O
if	O
we	O
consider	O
a	O
situation	O
in	O
which	O
the	O
measurement	O
noise	O
is	O
small	O
compared	O
to	O
the	O
rate	O
at	O
which	O
the	O
latent	B
variable	I
is	O
evolving	O
,	O
then	O
we	O
ﬁnd	O
that	O
the	O
posterior	O
distribution	O
for	O
zn	O
depends	O
only	O
on	O
the	O
current	O
measurement	O
xn	O
,	O
in	O
accordance	O
with	O
the	O
intuition	O
from	O
our	O
simple	O
example	O
at	O
the	O
start	O
of	O
the	O
section	O
.	O
similarly	O
,	O
if	O
the	O
latent	B
variable	I
is	O
evolving	O
slowly	O
relative	B
to	O
the	O
observation	O
noise	O
level	O
,	O
we	O
ﬁnd	O
that	O
the	O
posterior	O
mean	O
for	O
zn	O
is	O
obtained	O
by	O
averaging	O
all	O
of	O
the	O
measurements	O
obtained	O
up	O
to	O
that	O
time	O
.	O
one	O
of	O
the	O
most	O
important	O
applications	O
of	O
the	O
kalman	O
ﬁlter	O
is	O
to	O
tracking	O
,	O
and	O
this	O
is	O
illustrated	O
using	O
a	O
simple	O
example	O
of	O
an	O
object	O
moving	O
in	O
two	O
dimensions	O
in	O
figure	O
13.22.	O
so	O
far	O
,	O
we	O
have	O
solved	O
the	O
inference	B
problem	O
of	O
ﬁnding	O
the	O
posterior	O
marginal	O
for	O
a	O
node	B
zn	O
given	O
observations	O
from	O
x1	O
up	O
to	O
xn	O
.	O
next	O
we	O
turn	O
to	O
the	O
problem	O
of	O
ﬁnding	O
the	O
marginal	B
for	O
a	O
node	B
zn	O
given	O
all	O
observations	O
x1	O
to	O
xn	O
.	O
for	O
temporal	O
data	O
,	O
this	O
corresponds	O
to	O
the	O
inclusion	O
of	O
future	O
as	O
well	O
as	O
past	O
observations	O
.	O
al-	O
though	O
this	O
can	O
not	O
be	O
used	O
for	O
real-time	O
prediction	O
,	O
it	O
plays	O
a	O
key	O
role	O
in	O
learning	B
the	O
parameters	O
of	O
the	O
model	O
.	O
by	O
analogy	O
with	O
the	O
hidden	O
markov	O
model	O
,	O
this	O
problem	O
can	O
be	O
solved	O
by	O
propagating	O
messages	O
from	O
node	B
xn	O
back	O
to	O
node	B
x1	O
and	O
com-	O
bining	O
this	O
information	O
with	O
that	O
obtained	O
during	O
the	O
forward	O
message	O
passing	O
stage	O
used	O
to	O
compute	O
the	O
(	O
cid:1	O
)	O
α	O
(	O
zn	O
)	O
.	O
of	O
γ	O
(	O
zn	O
)	O
=	O
(	O
cid:1	O
)	O
α	O
(	O
zn	O
)	O
(	O
cid:1	O
)	O
β	O
(	O
zn	O
)	O
rather	O
than	O
in	O
terms	O
of	O
(	O
cid:1	O
)	O
β	O
(	O
zn	O
)	O
.	O
because	O
γ	O
(	O
zn	O
)	O
must	O
also	O
be	O
in	O
the	O
lds	O
literature	O
,	O
it	O
is	O
usual	O
to	O
formulate	O
this	O
backward	O
recursion	O
in	O
terms	O
gaussian	O
,	O
we	O
write	O
it	O
in	O
the	O
form	O
γ	O
(	O
zn	O
)	O
=	O
(	O
cid:1	O
)	O
α	O
(	O
zn	O
)	O
(	O
cid:1	O
)	O
β	O
(	O
zn	O
)	O
=	O
n	O
(	O
zn|	O
(	O
cid:1	O
)	O
µn	O
,	O
(	O
cid:1	O
)	O
vn	O
)	O
.	O
(	O
13.98	O
)	O
to	O
derive	O
the	O
required	O
recursion	O
,	O
we	O
start	O
from	O
the	O
backward	O
recursion	O
(	O
13.62	O
)	O
for	O
13.3.	O
linear	O
dynamical	O
systems	O
641	O
figure	O
13.22	O
an	O
illustration	O
of	O
a	O
linear	O
dy-	O
namical	O
system	O
being	O
used	O
to	O
track	O
a	O
moving	O
object	O
.	O
the	O
blue	O
points	O
indicate	O
the	O
true	O
positions	O
of	O
the	O
object	O
in	O
a	O
two-dimensional	O
space	O
at	O
successive	O
time	O
steps	O
,	O
the	O
green	O
points	O
denote	O
noisy	O
measurements	O
of	O
the	O
positions	O
,	O
and	O
the	O
red	O
crosses	O
indicate	O
the	O
means	O
of	O
the	O
inferred	O
posterior	O
distributions	O
of	O
the	O
positions	O
ob-	O
tained	O
by	O
running	O
the	O
kalman	O
ﬁl-	O
tering	O
equations	O
.	O
the	O
covari-	O
the	O
inferred	O
positions	O
ances	O
of	O
are	O
indicated	O
by	O
the	O
red	O
ellipses	O
,	O
which	O
correspond	O
to	O
contours	O
having	O
one	O
standard	B
deviation	I
.	O
cn+1	O
(	O
cid:1	O
)	O
β	O
(	O
zn	O
)	O
=	O
(	O
cid:1	O
)	O
β	O
(	O
zn	O
)	O
,	O
which	O
,	O
for	O
continuous	O
latent	O
variables	O
,	O
can	O
be	O
written	O
in	O
the	O
form	O
(	O
cid:6	O
)	O
(	O
cid:1	O
)	O
β	O
(	O
zn+1	O
)	O
p	O
(	O
xn+1|zn+1	O
)	O
p	O
(	O
zn+1|zn	O
)	O
dzn+1	O
.	O
we	O
now	O
multiply	O
both	O
sides	O
of	O
(	O
13.99	O
)	O
by	O
(	O
cid:1	O
)	O
α	O
(	O
zn	O
)	O
and	O
substitute	O
for	O
p	O
(	O
xn+1|zn+1	O
)	O
(	O
cid:1	O
)	O
µn	O
=	O
µn	O
+	O
jn	O
(	O
cid:1	O
)	O
vn	O
=	O
vn	O
+	O
jn	O
and	O
p	O
(	O
zn+1|zn	O
)	O
using	O
(	O
13.75	O
)	O
and	O
(	O
13.76	O
)	O
.	O
then	O
we	O
make	O
use	O
of	O
(	O
13.89	O
)	O
,	O
(	O
13.90	O
)	O
and	O
(	O
13.91	O
)	O
,	O
together	O
with	O
(	O
13.98	O
)	O
,	O
and	O
after	O
some	O
manipulation	O
we	O
obtain	O
(	O
cid:10	O
)	O
(	O
cid:1	O
)	O
µn+1	O
−	O
aµn	O
(	O
cid:11	O
)	O
(	O
cid:18	O
)	O
(	O
cid:17	O
)	O
(	O
cid:1	O
)	O
vn+1	O
−	O
pn	O
(	O
13.100	O
)	O
(	O
13.99	O
)	O
(	O
13.101	O
)	O
jt	O
n	O
where	O
we	O
have	O
deﬁned	O
−1	O
exercise	O
13.29	O
exercise	O
13.31	O
(	O
13.102	O
)	O
and	O
we	O
have	O
made	O
use	O
of	O
avn	O
=	O
pnjt	O
n.	O
note	O
that	O
these	O
recursions	O
require	O
that	O
the	O
forward	O
pass	O
be	O
completed	O
ﬁrst	O
so	O
that	O
the	O
quantities	O
µn	O
and	O
vn	O
will	O
be	O
available	O
for	O
the	O
backward	O
pass	O
.	O
jn	O
=	O
vnat	O
(	O
pn	O
)	O
ξ	O
(	O
zn−1	O
,	O
zn	O
)	O
=	O
(	O
cn	O
)	O
can	O
be	O
obtained	O
from	O
(	O
13.65	O
)	O
in	O
the	O
form	O
for	O
the	O
em	O
algorithm	O
,	O
we	O
also	O
require	O
the	O
pairwise	O
posterior	O
marginals	O
,	O
which	O
−1	O
(	O
cid:1	O
)	O
α	O
(	O
zn−1	O
)	O
p	O
(	O
xn|zn	O
)	O
p	O
(	O
zn|z−1	O
)	O
(	O
cid:1	O
)	O
β	O
(	O
zn	O
)	O
n	O
(	O
zn−1|µn−1	O
,	O
vn−1	O
)	O
n	O
(	O
zn|azn−1	O
,	O
γ	O
)	O
n	O
(	O
xn|czn	O
,	O
σ	O
)	O
n	O
(	O
zn|	O
(	O
cid:1	O
)	O
µn	O
,	O
(	O
cid:1	O
)	O
vn	O
)	O
substituting	O
for	O
(	O
cid:1	O
)	O
α	O
(	O
zn	O
)	O
using	O
(	O
13.84	O
)	O
and	O
rearranging	O
,	O
we	O
see	O
that	O
ξ	O
(	O
zn−1	O
,	O
zn	O
)	O
is	O
a	O
cn	O
(	O
cid:1	O
)	O
α	O
(	O
zn	O
)	O
gaussian	O
with	O
mean	B
given	O
with	O
components	O
γ	O
(	O
zn−1	O
)	O
and	O
γ	O
(	O
zn	O
)	O
,	O
and	O
a	O
covariance	B
between	O
zn	O
and	O
zn−1	O
given	O
by	O
(	O
13.103	O
)	O
=	O
.	O
cov	O
[	O
zn	O
,	O
zn−1	O
]	O
=	O
jn−1	O
(	O
13.104	O
)	O
(	O
cid:1	O
)	O
vn	O
.	O
642	O
13.	O
sequential	B
data	I
13.3.2	O
learning	B
in	O
lds	O
so	O
far	O
,	O
we	O
have	O
considered	O
the	O
inference	B
problem	O
for	O
linear	O
dynamical	O
systems	O
,	O
assuming	O
that	O
the	O
model	O
parameters	O
θ	O
=	O
{	O
a	O
,	O
γ	O
,	O
c	O
,	O
σ	O
,	O
µ0	O
,	O
v0	O
}	O
are	O
known	O
.	O
next	O
,	O
we	O
consider	O
the	O
determination	O
of	O
these	O
parameters	O
using	O
maximum	B
likelihood	I
(	O
ghahra-	O
mani	O
and	O
hinton	O
,	O
1996b	O
)	O
.	O
because	O
the	O
model	O
has	O
latent	O
variables	O
,	O
this	O
can	O
be	O
ad-	O
dressed	O
using	O
the	O
em	O
algorithm	O
,	O
which	O
was	O
discussed	O
in	O
general	O
terms	O
in	O
chapter	O
9.	O
we	O
can	O
derive	O
the	O
em	O
algorithm	O
for	O
the	O
linear	B
dynamical	I
system	I
as	O
follows	O
.	O
let	O
us	O
denote	O
the	O
estimated	O
parameter	O
values	O
at	O
some	O
particular	O
cycle	O
of	O
the	O
algorithm	O
by	O
θold	O
.	O
for	O
these	O
parameter	O
values	O
,	O
we	O
can	O
run	O
the	O
inference	B
algorithm	O
to	O
determine	O
the	O
posterior	O
distribution	O
of	O
the	O
latent	O
variables	O
p	O
(	O
z|x	O
,	O
θold	O
)	O
,	O
or	O
more	O
precisely	O
those	O
in	O
particular	O
,	O
we	O
shall	O
local	B
posterior	O
marginals	O
that	O
are	O
required	O
in	O
the	O
m	O
step	O
.	O
require	O
the	O
following	O
expectations	O
(	O
cid:8	O
)	O
e	O
e	O
[	O
zn	O
]	O
=	O
(	O
cid:1	O
)	O
µn	O
(	O
cid:9	O
)	O
(	O
cid:9	O
)	O
(	O
cid:1	O
)	O
vn	O
+	O
(	O
cid:1	O
)	O
µn	O
(	O
cid:1	O
)	O
µt	O
=	O
(	O
cid:1	O
)	O
vn	O
+	O
(	O
cid:1	O
)	O
µn	O
(	O
cid:1	O
)	O
µt	O
=	O
jn−1	O
n	O
znzt	O
n−1	O
znzt	O
n	O
e	O
(	O
cid:8	O
)	O
n−1	O
(	O
13.105	O
)	O
(	O
13.106	O
)	O
(	O
13.107	O
)	O
where	O
we	O
have	O
used	O
(	O
13.104	O
)	O
.	O
now	O
we	O
consider	O
the	O
complete-data	O
log	O
likelihood	O
function	O
,	O
which	O
is	O
obtained	O
by	O
taking	O
the	O
logarithm	O
of	O
(	O
13.6	O
)	O
and	O
is	O
therefore	O
given	O
by	O
n	O
(	O
cid:2	O
)	O
ln	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
=	O
ln	O
p	O
(	O
z1|µ0	O
,	O
v0	O
)	O
+	O
ln	O
p	O
(	O
zn|zn−1	O
,	O
a	O
,	O
γ	O
)	O
n	O
(	O
cid:2	O
)	O
+	O
n=2	O
ln	O
p	O
(	O
xn|zn	O
,	O
c	O
,	O
σ	O
)	O
(	O
13.108	O
)	O
n=1	O
in	O
which	O
we	O
have	O
made	O
the	O
dependence	O
on	O
the	O
parameters	O
explicit	O
.	O
we	O
now	O
take	O
the	O
expectation	B
of	O
the	O
complete-data	O
log	O
likelihood	O
with	O
respect	O
to	O
the	O
posterior	O
distri-	O
bution	O
p	O
(	O
z|x	O
,	O
θold	O
)	O
which	O
deﬁnes	O
the	O
function	O
q	O
(	O
θ	O
,	O
θold	O
)	O
=	O
ez|θold	O
[	O
ln	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
]	O
.	O
(	O
13.109	O
)	O
in	O
the	O
m	O
step	O
,	O
this	O
function	O
is	O
maximized	O
with	O
respect	O
to	O
the	O
components	O
of	O
θ.	O
consider	O
ﬁrst	O
the	O
parameters	O
µ0	O
and	O
v0	O
.	O
if	O
we	O
substitute	O
for	O
p	O
(	O
z1|µ0	O
,	O
v0	O
)	O
in	O
(	O
13.108	O
)	O
using	O
(	O
13.77	O
)	O
,	O
and	O
then	O
take	O
the	O
expectation	B
with	O
respect	O
to	O
z	O
,	O
we	O
obtain	O
(	O
cid:29	O
)	O
(	O
cid:30	O
)	O
0	O
(	O
z1	O
−	O
µ0	O
)	O
−1	O
+	O
const	O
q	O
(	O
θ	O
,	O
θold	O
)	O
=	O
−1	O
2	O
ln|v0|	O
−	O
ez|θold	O
(	O
z1	O
−	O
µ0	O
)	O
tv	O
1	O
2	O
where	O
all	O
terms	O
not	O
dependent	O
on	O
µ0	O
or	O
v0	O
have	O
been	O
absorbed	O
into	O
the	O
additive	O
constant	O
.	O
maximization	O
with	O
respect	O
to	O
µ0	O
and	O
v0	O
is	O
easily	O
performed	O
by	O
making	O
use	O
of	O
the	O
maximum	B
likelihood	I
solution	O
for	O
a	O
gaussian	O
distribution	O
discussed	O
in	O
section	O
2.3.4	O
,	O
giving	O
exercise	O
13.32	O
13.3.	O
linear	O
dynamical	O
systems	O
643	O
(	O
13.110	O
)	O
(	O
13.111	O
)	O
similarly	O
,	O
to	O
optimize	O
a	O
and	O
γ	O
,	O
we	O
substitute	O
for	O
p	O
(	O
zn|zn−1	O
,	O
a	O
,	O
γ	O
)	O
in	O
(	O
13.108	O
)	O
1	O
]	O
−	O
e	O
[	O
z1	O
]	O
e	O
[	O
zt	O
1	O
]	O
.	O
=	O
e	O
[	O
z1	O
]	O
=	O
e	O
[	O
z1zt	O
µnew	O
vnew	O
0	O
0	O
using	O
(	O
13.75	O
)	O
giving	O
(	O
cid:31	O
)	O
q	O
(	O
θ	O
,	O
θold	O
)	O
=	O
−	O
n	O
−	O
1	O
−ez|θold	O
n	O
(	O
cid:2	O
)	O
2	O
1	O
2	O
n=2	O
ln|γ|	O
(	O
zn	O
−	O
azn−1	O
)	O
tγ	O
−1	O
(	O
zn	O
−	O
azn−1	O
)	O
+	O
const	O
(	O
13.112	O
)	O
in	O
which	O
the	O
constant	O
comprises	O
terms	O
that	O
are	O
independent	B
of	O
a	O
and	O
γ.	O
maximizing	O
with	O
respect	O
to	O
these	O
parameters	O
then	O
gives	O
anew	O
=	O
e	O
znzt	O
n−1	O
zn−1zt	O
n−1	O
(	O
cid:22	O
)	O
n	O
(	O
cid:2	O
)	O
γnew	O
=	O
n=2	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
1	O
n	O
−	O
1	O
znzt	O
n−1	O
−e	O
(	O
cid:8	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:26	O
)	O
e	O
(	O
cid:9	O
)	O
(	O
cid:23	O
)	O
(	O
cid:22	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
−	O
anew	O
(	O
cid:8	O
)	O
n=2	O
e	O
znzt	O
n	O
(	O
cid:8	O
)	O
n=2	O
anew	O
+	O
anew	O
(	O
cid:9	O
)	O
(	O
cid:23	O
)	O
−1	O
(	O
cid:9	O
)	O
(	O
cid:20	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
e	O
zn−1zt	O
n	O
e	O
zn−1zt	O
n−1	O
(	O
anew	O
)	O
t	O
(	O
13.113	O
)	O
.	O
(	O
13.114	O
)	O
note	O
that	O
anew	O
must	O
be	O
evaluated	O
ﬁrst	O
,	O
and	O
the	O
result	O
can	O
then	O
be	O
used	O
to	O
determine	O
γnew	O
.	O
p	O
(	O
xn|zn	O
,	O
c	O
,	O
σ	O
)	O
in	O
(	O
13.108	O
)	O
using	O
(	O
13.76	O
)	O
giving	O
finally	O
,	O
in	O
order	O
to	O
determine	O
the	O
new	O
values	O
of	O
c	O
and	O
σ	O
,	O
we	O
substitute	O
for	O
q	O
(	O
θ	O
,	O
θold	O
)	O
=	O
−	O
n	O
2	O
maximizing	O
with	O
respect	O
to	O
c	O
and	O
σ	O
then	O
gives	O
(	O
xn	O
−	O
czn	O
)	O
tς	O
−1	O
(	O
xn	O
−	O
czn	O
)	O
+	O
const	O
.	O
ln|σ|	O
(	O
cid:31	O
)	O
1	O
2	O
−ez|θold	O
(	O
cid:22	O
)	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
1	O
n	O
−xne	O
n=1	O
(	O
cid:26	O
)	O
(	O
cid:8	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
cnew	O
=	O
xne	O
zt	O
n	O
σnew	O
=	O
n	O
−	O
cnew	O
xnxt	O
e	O
[	O
zn	O
]	O
xt	O
n	O
(	O
cid:9	O
)	O
(	O
cid:23	O
)	O
(	O
cid:22	O
)	O
n	O
(	O
cid:2	O
)	O
e	O
n=1	O
(	O
cid:8	O
)	O
znzt	O
n	O
(	O
cid:9	O
)	O
(	O
cid:23	O
)	O
−1	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
zt	O
n	O
cnew	O
+	O
cnew	O
e	O
znzt	O
n	O
cnew	O
(	O
13.115	O
)	O
(	O
cid:27	O
)	O
.	O
(	O
13.116	O
)	O
exercise	O
13.33	O
exercise	O
13.34	O
644	O
13.	O
sequential	B
data	I
we	O
have	O
approached	O
parameter	O
learning	O
in	O
the	O
linear	B
dynamical	I
system	I
using	O
maximum	B
likelihood	I
.	O
inclusion	O
of	O
priors	O
to	O
give	O
a	O
map	O
estimate	O
is	O
straightforward	O
,	O
and	O
a	O
fully	O
bayesian	O
treatment	O
can	O
be	O
found	O
by	O
applying	O
the	O
analytical	O
approxima-	O
tion	O
techniques	O
discussed	O
in	O
chapter	O
10	O
,	O
though	O
a	O
detailed	O
treatment	O
is	O
precluded	O
here	O
due	O
to	O
lack	O
of	O
space	O
.	O
13.3.3	O
extensions	O
of	O
lds	O
as	O
with	O
the	O
hidden	O
markov	O
model	O
,	O
there	O
is	O
considerable	O
interest	O
in	O
extending	O
the	O
basic	O
linear	B
dynamical	I
system	I
in	O
order	O
to	O
increase	O
its	O
capabilities	O
.	O
although	O
the	O
assumption	O
of	O
a	O
linear-gaussian	O
model	O
leads	O
to	O
efﬁcient	O
algorithms	O
for	O
inference	O
and	O
learning	B
,	O
it	O
also	O
implies	O
that	O
the	O
marginal	B
distribution	O
of	O
the	O
observed	O
variables	O
is	O
simply	O
a	O
gaussian	O
,	O
which	O
represents	O
a	O
signiﬁcant	O
limitation	O
.	O
one	O
simple	O
extension	O
of	O
the	O
linear	B
dynamical	I
system	I
is	O
to	O
use	O
a	O
gaussian	O
mixture	B
as	O
the	O
initial	O
distribution	O
for	O
z1	O
.	O
if	O
this	O
mixture	B
has	O
k	O
components	O
,	O
then	O
the	O
forward	O
recursion	O
equations	O
(	O
13.85	O
)	O
will	O
lead	O
to	O
a	O
mixture	O
of	O
k	O
gaussians	O
over	O
each	O
hidden	B
variable	I
zn	O
,	O
and	O
so	O
the	O
model	O
is	O
again	O
tractable	O
.	O
for	O
many	O
applications	O
,	O
the	O
gaussian	O
emission	O
density	O
is	O
a	O
poor	O
approximation	O
.	O
if	O
instead	O
we	O
try	O
to	O
use	O
a	O
mixture	O
of	O
k	O
gaussians	O
as	O
the	O
emission	O
density	O
,	O
then	O
the	O
posterior	O
(	O
cid:1	O
)	O
α	O
(	O
z1	O
)	O
will	O
also	O
be	O
a	O
mixture	O
of	O
k	O
gaussians	O
.	O
however	O
,	O
from	O
(	O
13.85	O
)	O
the	O
posterior	O
(	O
cid:1	O
)	O
α	O
(	O
z2	O
)	O
will	O
comprise	O
a	O
mixture	O
of	O
k	O
2	O
gaussians	O
,	O
and	O
so	O
on	O
,	O
with	O
(	O
cid:1	O
)	O
α	O
(	O
zn	O
)	O
being	O
given	O
by	O
a	O
mixture	O
of	O
k	O
n	O
gaussians	O
.	O
thus	O
the	O
number	O
of	O
components	O
grows	O
exponentially	O
with	O
the	O
length	O
of	O
the	O
chain	O
,	O
and	O
so	O
this	O
model	O
is	O
impractical	O
.	O
chapter	O
10	O
more	O
generally	O
,	O
introducing	O
transition	O
or	O
emission	O
models	O
that	O
depart	O
from	O
the	O
linear-gaussian	O
(	O
or	O
other	O
exponential	B
family	I
)	O
model	O
leads	O
to	O
an	O
intractable	O
infer-	O
ence	O
problem	O
.	O
we	O
can	O
make	O
deterministic	O
approximations	O
such	O
as	O
assumed	O
den-	O
sity	O
ﬁltering	O
or	O
expectation	B
propagation	I
,	O
or	O
we	O
can	O
make	O
use	O
of	O
sampling	B
methods	I
,	O
as	O
discussed	O
in	O
section	O
13.3.4.	O
one	O
widely	O
used	O
approach	O
is	O
to	O
make	O
a	O
gaussian	O
approximation	O
by	O
linearizing	O
around	O
the	O
mean	B
of	O
the	O
predicted	O
distribution	O
,	O
which	O
gives	O
rise	O
to	O
the	O
extended	B
kalman	O
ﬁlter	O
(	O
zarchan	O
and	O
musoff	O
,	O
2005	O
)	O
.	O
as	O
with	O
hidden	O
markov	O
models	O
,	O
we	O
can	O
develop	O
interesting	O
extensions	O
of	O
the	O
ba-	O
sic	O
linear	B
dynamical	I
system	I
by	O
expanding	O
its	O
graphical	O
representation	O
.	O
for	O
example	O
,	O
the	O
switching	B
state	I
space	I
model	I
(	O
ghahramani	O
and	O
hinton	O
,	O
1998	O
)	O
can	O
be	O
viewed	O
as	O
a	O
combination	O
of	O
the	O
hidden	O
markov	O
model	O
with	O
a	O
set	O
of	O
linear	O
dynamical	O
systems	O
.	O
the	O
model	O
has	O
multiple	O
markov	O
chains	O
of	O
continuous	O
linear-gaussian	O
latent	O
vari-	O
ables	O
,	O
each	O
of	O
which	O
is	O
analogous	O
to	O
the	O
latent	O
chain	O
of	O
the	O
linear	B
dynamical	I
system	I
discussed	O
earlier	O
,	O
together	O
with	O
a	O
markov	O
chain	O
of	O
discrete	O
variables	O
of	O
the	O
form	O
used	O
in	O
a	O
hidden	O
markov	O
model	O
.	O
the	O
output	O
at	O
each	O
time	O
step	O
is	O
determined	O
by	O
stochas-	O
tically	O
choosing	O
one	O
of	O
the	O
continuous	O
latent	O
chains	O
,	O
using	O
the	O
state	O
of	O
the	O
discrete	O
latent	B
variable	I
as	O
a	O
switch	O
,	O
and	O
then	O
emitting	O
an	O
observation	O
from	O
the	O
corresponding	O
conditional	B
output	O
distribution	O
.	O
exact	O
inference	O
in	O
this	O
model	O
is	O
intractable	O
,	O
but	O
vari-	O
ational	O
methods	O
lead	O
to	O
an	O
efﬁcient	O
inference	B
scheme	O
involving	O
forward-backward	O
recursions	O
along	O
each	O
of	O
the	O
continuous	O
and	O
discrete	O
markov	O
chains	O
independently	O
.	O
note	O
that	O
,	O
if	O
we	O
consider	O
multiple	O
chains	O
of	O
discrete	O
latent	O
variables	O
,	O
and	O
use	O
one	O
as	O
the	O
switch	O
to	O
select	O
from	O
the	O
remainder	O
,	O
we	O
obtain	O
an	O
analogous	O
model	O
having	O
only	O
discrete	O
latent	O
variables	O
known	O
as	O
the	O
switching	O
hidden	O
markov	O
model	O
.	O
e	O
[	O
f	O
(	O
zn	O
)	O
]	O
=	O
f	O
(	O
zn	O
)	O
p	O
(	O
zn|xn	O
)	O
dzn	O
f	O
(	O
zn	O
)	O
p	O
(	O
zn|xn	O
,	O
xn−1	O
)	O
dzn	O
f	O
(	O
zn	O
)	O
p	O
(	O
xn|zn	O
)	O
p	O
(	O
zn|xn−1	O
)	O
dzn	O
p	O
(	O
xn|zn	O
)	O
p	O
(	O
zn|xn−1	O
)	O
dzn	O
=	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:7	O
)	O
l	O
(	O
cid:2	O
)	O
=	O
l=1	O
13.3.	O
linear	O
dynamical	O
systems	O
645	O
chapter	O
11	O
13.3.4	O
particle	O
ﬁlters	O
for	O
dynamical	O
systems	O
which	O
do	O
not	O
have	O
a	O
linear-gaussian	O
,	O
for	O
example	O
,	O
if	O
they	O
use	O
a	O
non-gaussian	O
emission	O
density	O
,	O
we	O
can	O
turn	O
to	O
sampling	B
methods	I
in	O
order	O
to	O
ﬁnd	O
a	O
tractable	O
inference	B
algorithm	O
.	O
in	O
particular	O
,	O
we	O
can	O
apply	O
the	O
sampling-	O
importance-resampling	O
formalism	O
of	O
section	O
11.1.5	O
to	O
obtain	O
a	O
sequential	O
monte	O
carlo	O
algorithm	O
known	O
as	O
the	O
particle	B
ﬁlter	I
.	O
consider	O
the	O
class	O
of	O
distributions	O
represented	O
by	O
the	O
graphical	B
model	I
in	O
fig-	O
ure	O
13.5	O
,	O
and	O
suppose	O
we	O
are	O
given	O
the	O
observed	O
values	O
xn	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
and	O
we	O
wish	O
to	O
draw	O
l	O
samples	O
from	O
the	O
posterior	O
distribution	O
p	O
(	O
zn|xn	O
)	O
.	O
using	O
bayes	O
’	O
theorem	O
,	O
we	O
have	O
w	O
(	O
l	O
)	O
n	O
f	O
(	O
z	O
(	O
l	O
)	O
n	O
)	O
(	O
13.117	O
)	O
where	O
{	O
z	O
(	O
l	O
)	O
n	O
}	O
is	O
a	O
set	O
of	O
samples	O
drawn	O
from	O
p	O
(	O
zn|xn−1	O
)	O
and	O
we	O
have	O
made	O
use	O
of	O
the	O
conditional	B
independence	I
property	O
p	O
(	O
xn|zn	O
,	O
xn−1	O
)	O
=	O
p	O
(	O
xn|zn	O
)	O
,	O
which	O
follows	O
from	O
the	O
graph	O
in	O
figure	O
13.5.	O
the	O
sampling	O
weights	O
{	O
w	O
n	O
}	O
are	O
deﬁned	O
by	O
(	O
l	O
)	O
(	O
cid:5	O
)	O
l	O
p	O
(	O
xn|z	O
(	O
l	O
)	O
n	O
)	O
m=1	O
p	O
(	O
xn|z	O
(	O
m	O
)	O
n	O
)	O
w	O
(	O
l	O
)	O
n	O
=	O
(	O
13.118	O
)	O
where	O
the	O
same	O
samples	O
are	O
used	O
in	O
the	O
numerator	O
as	O
in	O
the	O
denominator	O
.	O
thus	O
the	O
(	O
cid:5	O
)	O
posterior	O
distribution	O
p	O
(	O
zn|xn	O
)	O
is	O
represented	O
by	O
the	O
set	O
of	O
samples	O
{	O
z	O
(	O
l	O
)	O
n	O
}	O
together	O
with	O
the	O
corresponding	O
weights	O
{	O
w	O
(	O
l	O
)	O
n	O
1	O
and	O
n	O
}	O
.	O
note	O
that	O
these	O
weights	O
satisfy	O
0	O
(	O
cid:1	O
)	O
w	O
(	O
l	O
)	O
n	O
=	O
1	O
.	O
(	O
l	O
)	O
because	O
we	O
wish	O
to	O
ﬁnd	O
a	O
sequential	O
sampling	O
scheme	O
,	O
we	O
shall	O
suppose	O
that	O
a	O
set	O
of	O
samples	O
and	O
weights	O
have	O
been	O
obtained	O
at	O
time	O
step	O
n	O
,	O
and	O
that	O
we	O
have	O
subsequently	O
observed	O
the	O
value	O
of	O
xn+1	O
,	O
and	O
we	O
wish	O
to	O
ﬁnd	O
the	O
weights	O
and	O
sam-	O
ples	O
at	O
time	O
step	O
n	O
+	O
1.	O
we	O
ﬁrst	O
sample	O
from	O
the	O
distribution	O
p	O
(	O
zn+1|xn	O
)	O
.	O
this	O
is	O
l	O
w	O
646	O
13.	O
sequential	B
data	I
straightforward	O
since	O
,	O
again	O
using	O
bayes	O
’	O
theorem	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
zn+1|xn	O
)	O
=	O
=	O
=	O
=	O
=	O
p	O
(	O
zn+1|zn	O
,	O
xn	O
)	O
p	O
(	O
zn|xn	O
)	O
dzn	O
p	O
(	O
zn+1|zn	O
)	O
p	O
(	O
zn|xn	O
)	O
dzn	O
p	O
(	O
zn+1|zn	O
)	O
p	O
(	O
zn|xn	O
,	O
xn−1	O
)	O
dzn	O
p	O
(	O
zn+1|zn	O
)	O
p	O
(	O
xn|zn	O
)	O
p	O
(	O
zn|xn−1	O
)	O
dzn	O
(	O
cid:6	O
)	O
p	O
(	O
xn|zn	O
)	O
p	O
(	O
zn|xn−1	O
)	O
dzn	O
n	O
p	O
(	O
zn+1|z	O
(	O
l	O
)	O
w	O
(	O
l	O
)	O
n	O
)	O
where	O
we	O
have	O
made	O
use	O
of	O
the	O
conditional	B
independence	I
properties	O
l	O
p	O
(	O
zn+1|zn	O
,	O
xn	O
)	O
=	O
p	O
(	O
zn+1|zn	O
)	O
p	O
(	O
xn|zn	O
,	O
xn−1	O
)	O
=	O
p	O
(	O
xn|zn	O
)	O
(	O
13.119	O
)	O
(	O
13.120	O
)	O
(	O
13.121	O
)	O
which	O
follow	O
from	O
the	O
application	O
of	O
the	O
d-separation	B
criterion	O
to	O
the	O
graph	O
in	O
fig-	O
ure	O
13.5.	O
the	O
distribution	O
given	O
by	O
(	O
13.119	O
)	O
is	O
a	O
mixture	B
distribution	I
,	O
and	O
samples	O
can	O
be	O
drawn	O
by	O
choosing	O
a	O
component	O
l	O
with	O
probability	B
given	O
by	O
the	O
mixing	O
coef-	O
ﬁcients	O
w	O
(	O
l	O
)	O
and	O
then	O
drawing	O
a	O
sample	O
from	O
the	O
corresponding	O
component	O
.	O
in	O
summary	O
,	O
we	O
can	O
view	O
each	O
step	O
of	O
the	O
particle	B
ﬁlter	I
algorithm	O
as	O
comprising	O
two	O
stages	O
.	O
at	O
time	O
step	O
n	O
,	O
we	O
have	O
a	O
sample	O
representation	O
of	O
the	O
posterior	O
dis-	O
tribution	O
p	O
(	O
zn|xn	O
)	O
expressed	O
as	O
samples	O
{	O
z	O
(	O
l	O
)	O
n	O
}	O
.	O
this	O
can	O
be	O
viewed	O
as	O
a	O
mixture	B
representation	O
of	O
the	O
form	O
(	O
13.119	O
)	O
.	O
to	O
obtain	O
the	O
corresponding	O
representation	O
for	O
the	O
next	O
time	O
step	O
,	O
we	O
ﬁrst	O
draw	O
l	O
samples	O
from	O
the	O
mixture	B
distribution	I
(	O
13.119	O
)	O
,	O
and	O
then	O
for	O
each	O
sample	O
we	O
use	O
the	O
new	O
obser-	O
n+1	O
)	O
.	O
this	O
is	O
vation	O
xn+1	O
to	O
evaluate	O
the	O
corresponding	O
weights	O
w	O
illustrated	O
,	O
for	O
the	O
case	O
of	O
a	O
single	O
variable	O
z	O
,	O
in	O
figure	O
13.23.	O
n	O
}	O
with	O
corresponding	O
weights	O
{	O
w	O
(	O
l	O
)	O
(	O
l	O
)	O
n+1	O
∝	O
p	O
(	O
xn+1|z	O
(	O
l	O
)	O
the	O
particle	O
ﬁltering	O
,	O
or	O
sequential	O
monte	O
carlo	O
,	O
approach	O
has	O
appeared	O
in	O
the	O
literature	O
under	O
various	O
names	O
including	O
the	O
bootstrap	B
ﬁlter	I
(	O
gordon	O
et	O
al.	O
,	O
1993	O
)	O
,	O
survival	B
of	I
the	I
ﬁttest	I
(	O
kanazawa	O
et	O
al.	O
,	O
1995	O
)	O
,	O
and	O
the	O
condensation	B
algorithm	I
(	O
isard	O
and	O
blake	O
,	O
1998	O
)	O
.	O
exercises	O
13.1	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
use	O
the	O
technique	O
of	O
d-separation	B
,	O
discussed	O
in	O
section	O
8.2	O
,	O
to	O
verify	O
that	O
the	O
markov	O
model	O
shown	O
in	O
figure	O
13.3	O
having	O
n	O
nodes	O
in	O
total	O
satisﬁes	O
the	O
conditional	B
independence	I
properties	O
(	O
13.3	O
)	O
for	O
n	O
=	O
2	O
,	O
.	O
.	O
.	O
,	O
n.	O
similarly	O
,	O
show	O
that	O
a	O
model	O
described	O
by	O
the	O
graph	O
in	O
figure	O
13.4	O
in	O
which	O
there	O
are	O
n	O
nodes	O
in	O
total	O
p	O
(	O
zn|xn	O
)	O
p	O
(	O
zn+1|xn	O
)	O
p	O
(	O
xn+1|zn+1	O
)	O
p	O
(	O
zn+1|xn+1	O
)	O
exercises	O
647	O
z	O
figure	O
13.23	O
schematic	O
illustration	O
of	O
the	O
operation	O
of	O
the	O
particle	B
ﬁlter	I
for	O
a	O
one-dimensional	O
latent	O
space	O
.	O
at	O
time	O
step	O
n	O
,	O
the	O
posterior	O
p	O
(	O
zn|xn	O
)	O
is	O
represented	O
as	O
a	O
mixture	B
distribution	I
,	O
shown	O
schematically	O
as	O
circles	O
whose	O
sizes	O
are	O
proportional	O
to	O
the	O
weights	O
w	O
(	O
l	O
)	O
n	O
.	O
a	O
set	O
of	O
l	O
samples	O
is	O
then	O
drawn	O
from	O
this	O
distribution	O
and	O
the	O
new	O
weights	O
w	O
(	O
l	O
)	O
n+1	O
evaluated	O
using	O
p	O
(	O
xn+1|z	O
(	O
l	O
)	O
n+1	O
)	O
.	O
satisﬁes	O
the	O
conditional	B
independence	I
properties	O
p	O
(	O
xn|x1	O
,	O
.	O
.	O
.	O
,	O
xn−1	O
)	O
=	O
p	O
(	O
xn|xn−1	O
,	O
xn−2	O
)	O
(	O
13.122	O
)	O
for	O
n	O
=	O
3	O
,	O
.	O
.	O
.	O
,	O
n.	O
13.2	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
the	O
joint	O
probability	B
distribution	O
(	O
13.2	O
)	O
corresponding	O
to	O
the	O
directed	B
graph	O
of	O
figure	O
13.3.	O
using	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
,	O
verify	O
that	O
this	O
joint	O
distribution	O
satisﬁes	O
the	O
conditional	B
independence	I
property	O
(	O
13.3	O
)	O
for	O
n	O
=	O
2	O
,	O
.	O
.	O
.	O
,	O
n.	O
similarly	O
,	O
show	O
that	O
the	O
second-order	O
markov	O
model	O
described	O
by	O
the	O
joint	O
distribution	O
(	O
13.4	O
)	O
satisﬁes	O
the	O
conditional	B
independence	I
property	O
p	O
(	O
xn|x1	O
,	O
.	O
.	O
.	O
,	O
xn−1	O
)	O
=	O
p	O
(	O
xn|xn−1	O
,	O
xn−2	O
)	O
(	O
13.123	O
)	O
for	O
n	O
=	O
3	O
,	O
.	O
.	O
.	O
,	O
n.	O
13.3	O
(	O
(	O
cid:12	O
)	O
)	O
by	O
using	O
d-separation	B
,	O
show	O
that	O
the	O
distribution	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
of	O
the	O
observed	O
data	O
for	O
the	O
state	B
space	I
model	I
represented	O
by	O
the	O
directed	B
graph	O
in	O
figure	O
13.5	O
does	O
not	O
satisfy	O
any	O
conditional	B
independence	I
properties	O
and	O
hence	O
does	O
not	O
exhibit	O
the	O
markov	O
property	O
at	O
any	O
ﬁnite	O
order	O
.	O
13.4	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
hidden	O
markov	O
model	O
in	O
which	O
the	O
emission	O
densities	O
are	O
represented	O
by	O
a	O
parametric	O
model	O
p	O
(	O
x|z	O
,	O
w	O
)	O
,	O
such	O
as	O
a	O
linear	B
regression	I
model	O
or	O
a	O
neural	B
network	I
,	O
in	O
which	O
w	O
is	O
a	O
vector	O
of	O
adaptive	O
parameters	O
.	O
describe	O
how	O
the	O
parameters	O
w	O
can	O
be	O
learned	O
from	O
data	O
using	O
maximum	B
likelihood	I
.	O
648	O
13.	O
sequential	B
data	I
13.5	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
verify	O
the	O
m-step	O
equations	O
(	O
13.18	O
)	O
and	O
(	O
13.19	O
)	O
for	O
the	O
initial	O
state	O
probabili-	O
ties	O
and	O
transition	B
probability	I
parameters	O
of	O
the	O
hidden	O
markov	O
model	O
by	O
maximiza-	O
tion	O
of	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
function	O
(	O
13.17	O
)	O
,	O
using	O
appropriate	O
lagrange	O
multipliers	O
to	O
enforce	O
the	O
summation	O
constraints	O
on	O
the	O
components	O
of	O
π	O
and	O
a	O
.	O
13.6	O
(	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
if	O
any	O
elements	O
of	O
the	O
parameters	O
π	O
or	O
a	O
for	O
a	O
hidden	O
markov	O
model	O
are	O
initially	O
set	O
to	O
zero	O
,	O
then	O
those	O
elements	O
will	O
remain	O
zero	O
in	O
all	O
subsequent	O
updates	O
of	O
the	O
em	O
algorithm	O
.	O
13.7	O
(	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
hidden	O
markov	O
model	O
with	O
gaussian	O
emission	O
densities	O
.	O
show	O
that	O
maximization	O
of	O
the	O
function	O
q	O
(	O
θ	O
,	O
θold	O
)	O
with	O
respect	O
to	O
the	O
mean	B
and	O
covariance	B
parameters	O
of	O
the	O
gaussians	O
gives	O
rise	O
to	O
the	O
m-step	O
equations	O
(	O
13.20	O
)	O
and	O
(	O
13.21	O
)	O
.	O
13.8	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
for	O
a	O
hidden	O
markov	O
model	O
having	O
discrete	O
observations	O
governed	O
by	O
a	O
multinomial	B
distribution	I
,	O
show	O
that	O
the	O
conditional	B
distribution	O
of	O
the	O
observations	O
given	O
the	O
hidden	O
variables	O
is	O
given	O
by	O
(	O
13.22	O
)	O
and	O
the	O
corresponding	O
m	O
step	O
equa-	O
tions	O
are	O
given	O
by	O
(	O
13.23	O
)	O
.	O
write	O
down	O
the	O
analogous	O
equations	O
for	O
the	O
conditional	B
distribution	O
and	O
the	O
m	O
step	O
equations	O
for	O
the	O
case	O
of	O
a	O
hidden	O
markov	O
with	O
multiple	O
binary	O
output	O
variables	O
each	O
of	O
which	O
is	O
governed	O
by	O
a	O
bernoulli	O
conditional	B
dis-	O
tribution	O
.	O
hint	O
:	O
refer	O
to	O
sections	O
2.1	O
and	O
2.2	O
for	O
a	O
discussion	O
of	O
the	O
corresponding	O
maximum	B
likelihood	I
solutions	O
for	O
i.i.d	O
.	O
data	O
if	O
required	O
.	O
13.9	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
use	O
the	O
d-separation	B
criterion	O
to	O
verify	O
that	O
the	O
conditional	B
indepen-	O
dence	O
properties	O
(	O
13.24	O
)	O
–	O
(	O
13.31	O
)	O
are	O
satisﬁed	O
by	O
the	O
joint	O
distribution	O
for	O
the	O
hidden	O
markov	O
model	O
deﬁned	O
by	O
(	O
13.6	O
)	O
.	O
13.10	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
by	O
applying	O
the	O
sum	O
and	O
product	O
rules	O
of	O
probability	B
,	O
verify	O
that	O
the	O
condi-	O
tional	O
independence	O
properties	O
(	O
13.24	O
)	O
–	O
(	O
13.31	O
)	O
are	O
satisﬁed	O
by	O
the	O
joint	O
distribution	O
for	O
the	O
hidden	O
markov	O
model	O
deﬁned	O
by	O
(	O
13.6	O
)	O
.	O
13.11	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
starting	O
from	O
the	O
expression	O
(	O
8.72	O
)	O
for	O
the	O
marginal	B
distribution	O
over	O
the	O
vari-	O
ables	O
of	O
a	O
factor	O
in	O
a	O
factor	B
graph	I
,	O
together	O
with	O
the	O
results	O
for	O
the	O
messages	O
in	O
the	O
sum-product	B
algorithm	I
obtained	O
in	O
section	O
13.2.3	O
,	O
derive	O
the	O
result	O
(	O
13.43	O
)	O
for	O
the	O
joint	O
posterior	O
distribution	O
over	O
two	O
successive	O
latent	O
variables	O
in	O
a	O
hidden	O
markov	O
model	O
.	O
13.12	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
suppose	O
we	O
wish	O
to	O
train	O
a	O
hidden	O
markov	O
model	O
by	O
maximum	B
likelihood	I
using	O
data	O
that	O
comprises	O
r	O
independent	B
sequences	O
of	O
observations	O
,	O
which	O
we	O
de-	O
note	O
by	O
x	O
(	O
r	O
)	O
where	O
r	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
r.	O
show	O
that	O
in	O
the	O
e	O
step	O
of	O
the	O
em	O
algorithm	O
,	O
we	O
simply	O
evaluate	O
posterior	O
probabilities	O
for	O
the	O
latent	O
variables	O
by	O
running	O
the	O
α	O
and	O
β	O
recursions	O
independently	O
for	O
each	O
of	O
the	O
sequences	O
.	O
also	O
show	O
that	O
in	O
the	O
m	O
step	O
,	O
the	O
initial	O
probability	B
and	O
transition	B
probability	I
parameters	O
are	O
re-estimated	O
using	O
modiﬁed	O
forms	O
of	O
(	O
13.18	O
)	O
and	O
(	O
13.19	O
)	O
given	O
by	O
(	O
r	O
)	O
1k	O
)	O
r=1	O
γ	O
(	O
z	O
r	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
r	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
r	O
(	O
cid:2	O
)	O
r	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
γ	O
(	O
z	O
n=2	O
r=1	O
r=1	O
j=1	O
(	O
r	O
)	O
1j	O
)	O
πk	O
=	O
ajk	O
=	O
ξ	O
(	O
z	O
(	O
r	O
)	O
n−1	O
,	O
j	O
,	O
z	O
(	O
r	O
)	O
n	O
,	O
k	O
)	O
ξ	O
(	O
z	O
(	O
r	O
)	O
n−1	O
,	O
j	O
,	O
z	O
(	O
r	O
)	O
n	O
,	O
l	O
)	O
r=1	O
l=1	O
n=2	O
exercises	O
649	O
(	O
13.124	O
)	O
(	O
13.125	O
)	O
where	O
,	O
for	O
notational	O
convenience	O
,	O
we	O
have	O
assumed	O
that	O
the	O
sequences	O
are	O
of	O
the	O
same	O
length	O
(	O
the	O
generalization	B
to	O
sequences	O
of	O
different	O
lengths	O
is	O
straightforward	O
)	O
.	O
similarly	O
,	O
show	O
that	O
the	O
m-step	O
equation	O
for	O
re-estimation	O
of	O
the	O
means	O
of	O
gaussian	O
emission	O
models	O
is	O
given	O
by	O
r	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
r	O
(	O
cid:2	O
)	O
n=1	O
r=1	O
µk	O
=	O
γ	O
(	O
z	O
(	O
r	O
)	O
nk	O
)	O
x	O
(	O
r	O
)	O
n	O
.	O
γ	O
(	O
z	O
(	O
r	O
)	O
nk	O
)	O
(	O
13.126	O
)	O
r=1	O
n=1	O
note	O
that	O
the	O
m-step	O
equations	O
for	O
other	O
emission	O
model	O
parameters	O
and	O
distributions	O
take	O
an	O
analogous	O
form	O
.	O
13.13	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
use	O
the	O
deﬁnition	O
(	O
8.64	O
)	O
of	O
the	O
messages	O
passed	O
from	O
a	O
factor	O
node	O
to	O
a	O
variable	O
node	B
in	O
a	O
factor	B
graph	I
,	O
together	O
with	O
the	O
expression	O
(	O
13.6	O
)	O
for	O
the	O
joint	O
distribution	O
in	O
a	O
hidden	O
markov	O
model	O
,	O
to	O
show	O
that	O
the	O
deﬁnition	O
(	O
13.50	O
)	O
of	O
the	O
alpha	O
message	O
is	O
the	O
same	O
as	O
the	O
deﬁnition	O
(	O
13.34	O
)	O
.	O
13.14	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
use	O
the	O
deﬁnition	O
(	O
8.67	O
)	O
of	O
the	O
messages	O
passed	O
from	O
a	O
factor	O
node	O
to	O
a	O
variable	O
node	B
in	O
a	O
factor	B
graph	I
,	O
together	O
with	O
the	O
expression	O
(	O
13.6	O
)	O
for	O
the	O
joint	O
distribution	O
in	O
a	O
hidden	O
markov	O
model	O
,	O
to	O
show	O
that	O
the	O
deﬁnition	O
(	O
13.52	O
)	O
of	O
the	O
beta	O
message	O
is	O
the	O
same	O
as	O
the	O
deﬁnition	O
(	O
13.35	O
)	O
.	O
13.15	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
use	O
the	O
expressions	O
(	O
13.33	O
)	O
and	O
(	O
13.43	O
)	O
for	O
the	O
marginals	O
in	O
a	O
hidden	O
markov	O
model	O
to	O
derive	O
the	O
corresponding	O
results	O
(	O
13.64	O
)	O
and	O
(	O
13.65	O
)	O
expressed	O
in	O
terms	O
of	O
re-scaled	O
variables	O
.	O
13.16	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
in	O
this	O
exercise	O
,	O
we	O
derive	O
the	O
forward	O
message	O
passing	O
equation	O
for	O
the	O
viterbi	O
algorithm	O
directly	O
from	O
the	O
expression	O
(	O
13.6	O
)	O
for	O
the	O
joint	O
distribution	O
.	O
this	O
involves	O
maximizing	O
over	O
all	O
of	O
the	O
hidden	O
variables	O
z1	O
,	O
.	O
.	O
.	O
,	O
zn	O
.	O
by	O
taking	O
the	O
log-	O
arithm	O
and	O
then	O
exchanging	O
maximizations	O
and	O
summations	O
,	O
derive	O
the	O
recursion	O
650	O
13.	O
sequential	B
data	I
(	O
13.68	O
)	O
where	O
the	O
quantities	O
ω	O
(	O
zn	O
)	O
are	O
deﬁned	O
by	O
(	O
13.70	O
)	O
.	O
show	O
that	O
the	O
initial	O
condition	O
for	O
this	O
recursion	O
is	O
given	O
by	O
(	O
13.69	O
)	O
.	O
13.17	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
the	O
directed	B
graph	O
for	O
the	O
input-output	O
hidden	O
markov	O
model	O
,	O
given	O
in	O
figure	O
13.18	O
,	O
can	O
be	O
expressed	O
as	O
a	O
tree-structured	O
factor	B
graph	I
of	O
the	O
form	O
shown	O
in	O
figure	O
13.15	O
and	O
write	O
down	O
expressions	O
for	O
the	O
initial	O
factor	O
h	O
(	O
z1	O
)	O
and	O
for	O
the	O
general	O
factor	O
fn	O
(	O
zn−1	O
,	O
zn	O
)	O
where	O
2	O
(	O
cid:1	O
)	O
n	O
(	O
cid:1	O
)	O
n.	O
13.18	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
using	O
the	O
result	O
of	O
exercise	O
13.17	O
,	O
derive	O
the	O
recursion	O
equations	O
,	O
includ-	O
ing	O
the	O
initial	O
conditions	O
,	O
for	O
the	O
forward-backward	B
algorithm	I
for	O
the	O
input-output	O
hidden	O
markov	O
model	O
shown	O
in	O
figure	O
13.18	O
.	O
13.19	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
the	O
kalman	O
ﬁlter	O
and	O
smoother	O
equations	O
allow	O
the	O
posterior	O
distribu-	O
tions	O
over	O
individual	O
latent	O
variables	O
,	O
conditioned	O
on	O
all	O
of	O
the	O
observed	O
variables	O
,	O
to	O
be	O
found	O
efﬁciently	O
for	O
linear	O
dynamical	O
systems	O
.	O
show	O
that	O
the	O
sequence	O
of	O
latent	B
variable	I
values	O
obtained	O
by	O
maximizing	O
each	O
of	O
these	O
posterior	O
distributions	O
individually	O
is	O
the	O
same	O
as	O
the	O
most	O
probable	O
sequence	O
of	O
latent	O
values	O
.	O
to	O
do	O
this	O
,	O
simply	O
note	O
that	O
the	O
joint	O
distribution	O
of	O
all	O
latent	O
and	O
observed	O
variables	O
in	O
a	O
linear	B
dynamical	I
system	I
is	O
gaussian	O
,	O
and	O
hence	O
all	O
conditionals	O
and	O
marginals	O
will	O
also	O
be	O
gaussian	O
,	O
and	O
then	O
make	O
use	O
of	O
the	O
result	O
(	O
2.98	O
)	O
.	O
13.20	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
use	O
the	O
result	O
(	O
2.115	O
)	O
to	O
prove	O
(	O
13.87	O
)	O
.	O
13.21	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
use	O
the	O
results	O
(	O
2.115	O
)	O
and	O
(	O
2.116	O
)	O
,	O
together	O
with	O
the	O
matrix	O
identities	O
(	O
c.5	O
)	O
and	O
(	O
c.7	O
)	O
,	O
to	O
derive	O
the	O
results	O
(	O
13.89	O
)	O
,	O
(	O
13.90	O
)	O
,	O
and	O
(	O
13.91	O
)	O
,	O
where	O
the	O
kalman	O
gain	O
matrix	O
kn	O
is	O
deﬁned	O
by	O
(	O
13.92	O
)	O
.	O
13.22	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
using	O
(	O
13.93	O
)	O
,	O
together	O
with	O
the	O
deﬁnitions	O
(	O
13.76	O
)	O
and	O
(	O
13.77	O
)	O
and	O
the	O
result	O
(	O
2.115	O
)	O
,	O
derive	O
(	O
13.96	O
)	O
.	O
13.23	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
using	O
(	O
13.93	O
)	O
,	O
together	O
with	O
the	O
deﬁnitions	O
(	O
13.76	O
)	O
and	O
(	O
13.77	O
)	O
and	O
the	O
result	O
(	O
2.116	O
)	O
,	O
derive	O
(	O
13.94	O
)	O
,	O
(	O
13.95	O
)	O
and	O
(	O
13.97	O
)	O
.	O
13.24	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
generalization	B
of	O
(	O
13.75	O
)	O
and	O
(	O
13.76	O
)	O
in	O
which	O
we	O
include	O
constant	O
terms	O
a	O
and	O
c	O
in	O
the	O
gaussian	O
means	O
,	O
so	O
that	O
p	O
(	O
zn|zn−1	O
)	O
=	O
n	O
(	O
zn|azn−1	O
+	O
a	O
,	O
γ	O
)	O
p	O
(	O
xn|zn	O
)	O
=	O
n	O
(	O
xn|czn	O
+	O
c	O
,	O
σ	O
)	O
.	O
(	O
13.127	O
)	O
(	O
13.128	O
)	O
show	O
that	O
this	O
extension	O
can	O
be	O
re-case	O
in	O
the	O
framework	O
discussed	O
in	O
this	O
chapter	O
by	O
deﬁning	O
a	O
state	O
vector	O
z	O
with	O
an	O
additional	O
component	O
ﬁxed	O
at	O
unity	O
,	O
and	O
then	O
aug-	O
menting	O
the	O
matrices	O
a	O
and	O
c	O
using	O
extra	O
columns	O
corresponding	O
to	O
the	O
parameters	O
a	O
and	O
c.	O
13.25	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
in	O
this	O
exercise	O
,	O
we	O
show	O
that	O
when	O
the	O
kalman	O
ﬁlter	O
equations	O
are	O
applied	O
to	O
independent	B
observations	O
,	O
they	O
reduce	O
to	O
the	O
results	O
given	O
in	O
section	O
2.3	O
for	O
the	O
maximum	B
likelihood	I
solution	O
for	O
a	O
single	O
gaussian	O
distribution	O
.	O
consider	O
the	O
prob-	O
lem	O
of	O
ﬁnding	O
the	O
mean	B
µ	O
of	O
a	O
single	O
gaussian	O
random	O
variable	O
x	O
,	O
in	O
which	O
we	O
are	O
given	O
a	O
set	O
of	O
independent	B
observations	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
.	O
to	O
model	O
this	O
we	O
can	O
use	O
exercises	O
651	O
a	O
linear	B
dynamical	I
system	I
governed	O
by	O
(	O
13.75	O
)	O
and	O
(	O
13.76	O
)	O
,	O
with	O
latent	O
variables	O
{	O
z1	O
,	O
.	O
.	O
.	O
,	O
zn	O
}	O
in	O
which	O
c	O
becomes	O
the	O
identity	O
matrix	O
and	O
where	O
the	O
transition	O
prob-	O
ability	O
a	O
=	O
0	O
because	O
the	O
observations	O
are	O
independent	B
.	O
let	O
the	O
parameters	O
m0	O
and	O
v0	O
of	O
the	O
initial	O
state	O
be	O
denoted	O
by	O
µ0	O
and	O
σ2	O
0	O
,	O
respectively	O
,	O
and	O
suppose	O
that	O
σ	O
becomes	O
σ2	O
.	O
write	O
down	O
the	O
corresponding	O
kalman	O
ﬁlter	O
equations	O
starting	O
from	O
the	O
general	O
results	O
(	O
13.89	O
)	O
and	O
(	O
13.90	O
)	O
,	O
together	O
with	O
(	O
13.94	O
)	O
and	O
(	O
13.95	O
)	O
.	O
show	O
that	O
these	O
are	O
equivalent	O
to	O
the	O
results	O
(	O
2.141	O
)	O
and	O
(	O
2.142	O
)	O
obtained	O
directly	O
by	O
consider-	O
ing	O
independent	O
data	O
.	O
13.26	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
special	O
case	O
of	O
the	O
linear	B
dynamical	I
system	I
of	O
section	O
13.3	O
that	O
is	O
equivalent	O
to	O
probabilistic	O
pca	O
,	O
so	O
that	O
the	O
transition	O
matrix	O
a	O
=	O
0	O
,	O
the	O
covariance	B
γ	O
=	O
i	O
,	O
and	O
the	O
noise	O
covariance	B
σ	O
=	O
σ2i	O
.	O
by	O
making	O
use	O
of	O
the	O
matrix	O
inversion	O
identity	O
(	O
c.7	O
)	O
show	O
that	O
,	O
if	O
the	O
emission	O
density	O
matrix	O
c	O
is	O
denoted	O
w	O
,	O
then	O
the	O
posterior	O
distribution	O
over	O
the	O
hidden	O
states	O
deﬁned	O
by	O
(	O
13.89	O
)	O
and	O
(	O
13.90	O
)	O
reduces	O
to	O
the	O
result	O
(	O
12.42	O
)	O
for	O
probabilistic	O
pca	O
.	O
13.27	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
linear	B
dynamical	I
system	I
of	O
the	O
form	O
discussed	O
in	O
sec-	O
tion	O
13.3	O
in	O
which	O
the	O
amplitude	O
of	O
the	O
observation	O
noise	O
goes	O
to	O
zero	O
,	O
so	O
that	O
σ	O
=	O
0.	O
show	O
that	O
the	O
posterior	O
distribution	O
for	O
zn	O
has	O
mean	B
xn	O
and	O
zero	O
variance	B
.	O
this	O
accords	O
with	O
our	O
intuition	O
that	O
if	O
there	O
is	O
no	O
noise	O
,	O
we	O
should	O
just	O
use	O
the	O
current	O
observation	O
xn	O
to	O
estimate	O
the	O
state	O
variable	O
zn	O
and	O
ignore	O
all	O
previous	O
observations	O
.	O
13.28	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
special	O
case	O
of	O
the	O
linear	B
dynamical	I
system	I
of	O
section	O
13.3	O
in	O
which	O
the	O
state	O
variable	O
zn	O
is	O
constrained	O
to	O
be	O
equal	O
to	O
the	O
previous	O
state	O
variable	O
,	O
which	O
corresponds	O
to	O
a	O
=	O
i	O
and	O
γ	O
=	O
0.	O
for	O
simplicity	O
,	O
assume	O
also	O
that	O
v0	O
→	O
∞	O
so	O
that	O
the	O
initial	O
conditions	O
for	O
z	O
are	O
unimportant	O
,	O
and	O
the	O
predictions	O
are	O
determined	O
purely	O
by	O
the	O
data	O
.	O
use	O
proof	O
by	O
induction	O
to	O
show	O
that	O
the	O
posterior	O
mean	O
for	O
state	O
zn	O
is	O
determined	O
by	O
the	O
average	O
of	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
.	O
this	O
corresponds	O
to	O
the	O
intuitive	O
result	O
that	O
if	O
the	O
state	O
variable	O
is	O
constant	O
,	O
our	O
best	O
estimate	O
is	O
obtained	O
by	O
averaging	O
the	O
observations	O
.	O
13.29	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
starting	O
from	O
the	O
backwards	O
recursion	O
equation	O
(	O
13.99	O
)	O
,	O
derive	O
the	O
rts	O
smoothing	O
equations	O
(	O
13.100	O
)	O
and	O
(	O
13.101	O
)	O
for	O
the	O
gaussian	O
linear	O
dynamical	O
sys-	O
tem	O
.	O
13.30	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
starting	O
from	O
the	O
result	O
(	O
13.65	O
)	O
for	O
the	O
pairwise	O
posterior	O
marginal	O
in	O
a	O
state	B
space	I
model	I
,	O
derive	O
the	O
speciﬁc	O
form	O
(	O
13.103	O
)	O
for	O
the	O
case	O
of	O
the	O
gaussian	O
linear	B
dynamical	I
system	I
.	O
13.31	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
starting	O
from	O
the	O
result	O
(	O
13.103	O
)	O
and	O
by	O
substituting	O
for	O
(	O
cid:1	O
)	O
α	O
(	O
zn	O
)	O
using	O
(	O
13.84	O
)	O
,	O
verify	O
the	O
result	O
(	O
13.104	O
)	O
for	O
the	O
covariance	B
between	O
zn	O
and	O
zn−1	O
.	O
13.32	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
verify	O
the	O
results	O
(	O
13.110	O
)	O
and	O
(	O
13.111	O
)	O
for	O
the	O
m-step	O
equations	O
for	O
µ0	O
and	O
v0	O
in	O
the	O
linear	B
dynamical	I
system	I
.	O
13.33	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
verify	O
the	O
results	O
(	O
13.113	O
)	O
and	O
(	O
13.114	O
)	O
for	O
the	O
m-step	O
equations	O
for	O
a	O
and	O
γ	O
in	O
the	O
linear	B
dynamical	I
system	I
.	O
652	O
13.	O
sequential	B
data	I
13.34	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
verify	O
the	O
results	O
(	O
13.115	O
)	O
and	O
(	O
13.116	O
)	O
for	O
the	O
m-step	O
equations	O
for	O
c	O
and	O
σ	O
in	O
the	O
linear	B
dynamical	I
system	I
.	O
14	O
combining	B
models	I
in	O
earlier	O
chapters	O
,	O
we	O
have	O
explored	O
a	O
range	O
of	O
different	O
models	O
for	O
solving	O
classiﬁ-	O
cation	O
and	O
regression	B
problems	O
.	O
it	O
is	O
often	O
found	O
that	O
improved	O
performance	O
can	O
be	O
obtained	O
by	O
combining	O
multiple	O
models	O
together	O
in	O
some	O
way	O
,	O
instead	O
of	O
just	O
using	O
a	O
single	O
model	O
in	O
isolation	O
.	O
for	O
instance	O
,	O
we	O
might	O
train	O
l	O
different	O
models	O
and	O
then	O
make	O
predictions	O
using	O
the	O
average	O
of	O
the	O
predictions	O
made	O
by	O
each	O
model	O
.	O
such	O
combinations	O
of	O
models	O
are	O
sometimes	O
called	O
committees	O
.	O
in	O
section	O
14.2	O
,	O
we	O
dis-	O
cuss	O
ways	O
to	O
apply	O
the	O
committee	B
concept	O
in	O
practice	O
,	O
and	O
we	O
also	O
give	O
some	O
insight	O
into	O
why	O
it	O
can	O
sometimes	O
be	O
an	O
effective	O
procedure	O
.	O
one	O
important	O
variant	O
of	O
the	O
committee	B
method	O
,	O
known	O
as	O
boosting	B
,	O
involves	O
training	B
multiple	O
models	O
in	O
sequence	O
in	O
which	O
the	O
error	B
function	I
used	O
to	O
train	O
a	O
par-	O
ticular	O
model	O
depends	O
on	O
the	O
performance	O
of	O
the	O
previous	O
models	O
.	O
this	O
can	O
produce	O
substantial	O
improvements	O
in	O
performance	O
compared	O
to	O
the	O
use	O
of	O
a	O
single	O
model	O
and	O
is	O
discussed	O
in	O
section	O
14.3.	O
instead	O
of	O
averaging	O
the	O
predictions	O
of	O
a	O
set	O
of	O
models	O
,	O
an	O
alternative	O
form	O
of	O
653	O
654	O
14.	O
combining	B
models	I
model	O
combination	O
is	O
to	O
select	O
one	O
of	O
the	O
models	O
to	O
make	O
the	O
prediction	O
,	O
in	O
which	O
the	O
choice	O
of	O
model	O
is	O
a	O
function	O
of	O
the	O
input	O
variables	O
.	O
thus	O
different	O
models	O
be-	O
come	O
responsible	O
for	O
making	O
predictions	O
in	O
different	O
regions	O
of	O
input	O
space	O
.	O
one	O
widely	O
used	O
framework	O
of	O
this	O
kind	O
is	O
known	O
as	O
a	O
decision	B
tree	I
in	O
which	O
the	O
selec-	O
tion	O
process	O
can	O
be	O
described	O
as	O
a	O
sequence	O
of	O
binary	O
selections	O
corresponding	O
to	O
the	O
traversal	O
of	O
a	O
tree	B
structure	O
and	O
is	O
discussed	O
in	O
section	O
14.4.	O
in	O
this	O
case	O
,	O
the	O
individual	O
models	O
are	O
generally	O
chosen	O
to	O
be	O
very	O
simple	O
,	O
and	O
the	O
overall	O
ﬂexibility	O
of	O
the	O
model	O
arises	O
from	O
the	O
input-dependent	O
selection	O
process	O
.	O
decision	O
trees	O
can	O
be	O
applied	O
to	O
both	O
classiﬁcation	O
and	O
regression	O
problems	O
.	O
one	O
limitation	O
of	O
decision	O
trees	O
is	O
that	O
the	O
division	O
of	O
input	O
space	O
is	O
based	O
on	O
hard	O
splits	O
in	O
which	O
only	O
one	O
model	O
is	O
responsible	O
for	O
making	O
predictions	O
for	O
any	O
given	O
value	O
of	O
the	O
input	O
variables	O
.	O
the	O
decision	O
process	O
can	O
be	O
softened	O
by	O
moving	O
to	O
a	O
probabilistic	O
framework	O
for	O
combining	O
models	O
,	O
as	O
discussed	O
in	O
section	O
14.5.	O
for	O
example	O
,	O
if	O
we	O
have	O
a	O
set	O
of	O
k	O
models	O
for	O
a	O
conditional	B
distribution	O
p	O
(	O
t|x	O
,	O
k	O
)	O
where	O
x	O
is	O
the	O
input	O
variable	O
,	O
t	O
is	O
the	O
target	O
variable	O
,	O
and	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
indexes	O
the	O
model	O
,	O
then	O
we	O
can	O
form	O
a	O
probabilistic	O
mixture	O
of	O
the	O
form	O
k	O
(	O
cid:2	O
)	O
p	O
(	O
t|x	O
)	O
=	O
πk	O
(	O
x	O
)	O
p	O
(	O
t|x	O
,	O
k	O
)	O
(	O
14.1	O
)	O
k=1	O
in	O
which	O
πk	O
(	O
x	O
)	O
=	O
p	O
(	O
k|x	O
)	O
represent	O
the	O
input-dependent	O
mixing	O
coefﬁcients	O
.	O
such	O
models	O
can	O
be	O
viewed	O
as	O
mixture	B
distributions	O
in	O
which	O
the	O
component	O
densities	O
,	O
as	O
well	O
as	O
the	O
mixing	O
coefﬁcients	O
,	O
are	O
conditioned	O
on	O
the	O
input	O
variables	O
and	O
are	O
known	O
as	O
mixtures	O
of	O
experts	O
.	O
they	O
are	O
closely	O
related	O
to	O
the	O
mixture	B
density	I
network	I
model	O
discussed	O
in	O
section	O
5.6	O
.	O
14.1.	O
bayesian	O
model	B
averaging	I
it	O
is	O
important	O
to	O
distinguish	O
between	O
model	O
combination	O
methods	O
and	O
bayesian	O
model	B
averaging	I
,	O
as	O
the	O
two	O
are	O
often	O
confused	O
.	O
to	O
understand	O
the	O
difference	O
,	O
con-	O
sider	O
the	O
example	O
of	O
density	B
estimation	I
using	O
a	O
mixture	O
of	O
gaussians	O
in	O
which	O
several	O
gaussian	O
components	O
are	O
combined	O
probabilistically	O
.	O
the	O
model	O
contains	O
a	O
binary	O
latent	O
variable	O
z	O
that	O
indicates	O
which	O
component	O
of	O
the	O
mixture	B
is	O
responsible	O
for	O
generating	O
the	O
corresponding	O
data	O
point	O
.	O
thus	O
the	O
model	O
is	O
speciﬁed	O
in	O
terms	O
of	O
a	O
joint	O
distribution	O
(	O
14.2	O
)	O
and	O
the	O
corresponding	O
density	B
over	O
the	O
observed	B
variable	I
x	O
is	O
obtained	O
by	O
marginal-	O
izing	O
over	O
the	O
latent	B
variable	I
p	O
(	O
x	O
,	O
z	O
)	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
x	O
,	O
z	O
)	O
.	O
(	O
14.3	O
)	O
(	O
cid:2	O
)	O
z	O
section	O
9.2	O
14.2.	O
committees	O
655	O
in	O
the	O
case	O
of	O
our	O
gaussian	O
mixture	B
example	O
,	O
this	O
leads	O
to	O
a	O
distribution	O
of	O
the	O
form	O
p	O
(	O
x	O
)	O
=	O
πkn	O
(	O
x|µk	O
,	O
σk	O
)	O
(	O
14.4	O
)	O
k	O
(	O
cid:2	O
)	O
k=1	O
h	O
(	O
cid:2	O
)	O
with	O
the	O
usual	O
interpretation	O
of	O
the	O
symbols	O
.	O
this	O
is	O
an	O
example	O
of	O
model	O
combi-	O
nation	O
.	O
for	O
independent	O
,	O
identically	O
distributed	O
data	O
,	O
we	O
can	O
use	O
(	O
14.3	O
)	O
to	O
write	O
the	O
marginal	B
probability	I
of	O
a	O
data	O
set	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
in	O
the	O
form	O
(	O
cid:31	O
)	O
(	O
cid:2	O
)	O
n	O
(	O
cid:14	O
)	O
n	O
(	O
cid:14	O
)	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
xn	O
)	O
=	O
p	O
(	O
xn	O
,	O
zn	O
)	O
.	O
(	O
14.5	O
)	O
n=1	O
n=1	O
zn	O
thus	O
we	O
see	O
that	O
each	O
observed	O
data	O
point	O
xn	O
has	O
a	O
corresponding	O
latent	B
variable	I
zn	O
.	O
now	O
suppose	O
we	O
have	O
several	O
different	O
models	O
indexed	O
by	O
h	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
h	O
with	O
prior	B
probabilities	O
p	O
(	O
h	O
)	O
.	O
for	O
instance	O
one	O
model	O
might	O
be	O
a	O
mixture	O
of	O
gaussians	O
and	O
another	O
model	O
might	O
be	O
a	O
mixture	O
of	O
cauchy	O
distributions	O
.	O
the	O
marginal	B
distribution	O
over	O
the	O
data	O
set	O
is	O
given	O
by	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
x|h	O
)	O
p	O
(	O
h	O
)	O
.	O
(	O
14.6	O
)	O
h=1	O
this	O
is	O
an	O
example	O
of	O
bayesian	O
model	B
averaging	I
.	O
the	O
interpretation	O
of	O
this	O
summa-	O
tion	O
over	O
h	O
is	O
that	O
just	O
one	O
model	O
is	O
responsible	O
for	O
generating	O
the	O
whole	O
data	O
set	O
,	O
and	O
the	O
probability	B
distribution	O
over	O
h	O
simply	O
reﬂects	O
our	O
uncertainty	O
as	O
to	O
which	O
model	O
that	O
is	O
.	O
as	O
the	O
size	O
of	O
the	O
data	O
set	O
increases	O
,	O
this	O
uncertainty	O
reduces	O
,	O
and	O
the	O
posterior	O
probabilities	O
p	O
(	O
h|x	O
)	O
become	O
increasingly	O
focussed	O
on	O
just	O
one	O
of	O
the	O
models	O
.	O
this	O
highlights	O
the	O
key	O
difference	O
between	O
bayesian	O
model	B
averaging	I
and	O
model	O
combination	O
,	O
because	O
in	O
bayesian	O
model	B
averaging	I
the	O
whole	O
data	O
set	O
is	O
generated	O
by	O
a	O
single	O
model	O
.	O
by	O
contrast	O
,	O
when	O
we	O
combine	O
multiple	O
models	O
,	O
as	O
in	O
(	O
14.5	O
)	O
,	O
we	O
see	O
that	O
different	O
data	O
points	O
within	O
the	O
data	O
set	O
can	O
potentially	O
be	O
generated	O
from	O
different	O
values	O
of	O
the	O
latent	B
variable	I
z	O
and	O
hence	O
by	O
different	O
components	O
.	O
although	O
we	O
have	O
considered	O
the	O
marginal	B
probability	I
p	O
(	O
x	O
)	O
,	O
the	O
same	O
consid-	O
erations	O
apply	O
for	O
the	O
predictive	O
density	O
p	O
(	O
x|x	O
)	O
or	O
for	O
conditional	O
distributions	O
such	O
as	O
p	O
(	O
t|x	O
,	O
x	O
,	O
t	O
)	O
.	O
exercise	O
14.1	O
14.2.	O
committees	O
section	O
3.2	O
the	O
simplest	O
way	O
to	O
construct	O
a	O
committee	B
is	O
to	O
average	O
the	O
predictions	O
of	O
a	O
set	O
of	O
individual	O
models	O
.	O
such	O
a	O
procedure	O
can	O
be	O
motivated	O
from	O
a	O
frequentist	B
perspective	O
by	O
considering	O
the	O
trade-off	O
between	O
bias	B
and	O
variance	B
,	O
which	O
decomposes	O
the	O
er-	O
ror	O
due	O
to	O
a	O
model	O
into	O
the	O
bias	B
component	O
that	O
arises	O
from	O
differences	O
between	O
the	O
model	O
and	O
the	O
true	O
function	O
to	O
be	O
predicted	O
,	O
and	O
the	O
variance	B
component	O
that	O
repre-	O
sents	O
the	O
sensitivity	O
of	O
the	O
model	O
to	O
the	O
individual	O
data	O
points	O
.	O
recall	O
from	O
figure	O
3.5	O
656	O
14.	O
combining	B
models	I
that	O
when	O
we	O
trained	O
multiple	O
polynomials	O
using	O
the	O
sinusoidal	B
data	I
,	O
and	O
then	O
aver-	O
aged	O
the	O
resulting	O
functions	O
,	O
the	O
contribution	O
arising	O
from	O
the	O
variance	B
term	O
tended	O
to	O
cancel	O
,	O
leading	O
to	O
improved	O
predictions	O
.	O
when	O
we	O
averaged	O
a	O
set	O
of	O
low-bias	O
mod-	O
els	O
(	O
corresponding	O
to	O
higher	O
order	O
polynomials	O
)	O
,	O
we	O
obtained	O
accurate	O
predictions	O
for	O
the	O
underlying	O
sinusoidal	O
function	O
from	O
which	O
the	O
data	O
were	O
generated	O
.	O
in	O
practice	O
,	O
of	O
course	O
,	O
we	O
have	O
only	O
a	O
single	O
data	O
set	O
,	O
and	O
so	O
we	O
have	O
to	O
ﬁnd	O
a	O
way	O
to	O
introduce	O
variability	O
between	O
the	O
different	O
models	O
within	O
the	O
committee	B
.	O
one	O
approach	O
is	O
to	O
use	O
bootstrap	B
data	O
sets	O
,	O
discussed	O
in	O
section	O
1.2.3.	O
consider	O
a	O
regression	B
problem	O
in	O
which	O
we	O
are	O
trying	O
to	O
predict	O
the	O
value	O
of	O
a	O
single	O
continuous	O
variable	O
,	O
and	O
suppose	O
we	O
generate	O
m	O
bootstrap	B
data	O
sets	O
and	O
then	O
use	O
each	O
to	O
train	O
a	O
separate	O
copy	O
ym	O
(	O
x	O
)	O
of	O
a	O
predictive	O
model	O
where	O
m	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m.	O
the	O
committee	B
prediction	O
is	O
given	O
by	O
ym	O
(	O
x	O
)	O
.	O
(	O
14.7	O
)	O
m	O
(	O
cid:2	O
)	O
m=1	O
ycom	O
(	O
x	O
)	O
=	O
1	O
m	O
this	O
procedure	O
is	O
known	O
as	O
bootstrap	B
aggregation	O
or	O
bagging	B
(	O
breiman	O
,	O
1996	O
)	O
.	O
suppose	O
the	O
true	O
regression	B
function	I
that	O
we	O
are	O
trying	O
to	O
predict	O
is	O
given	O
by	O
h	O
(	O
x	O
)	O
,	O
so	O
that	O
the	O
output	O
of	O
each	O
of	O
the	O
models	O
can	O
be	O
written	O
as	O
the	O
true	O
value	O
plus	O
an	O
error	B
in	O
the	O
form	O
ym	O
(	O
x	O
)	O
=	O
h	O
(	O
x	O
)	O
+	O
m	O
(	O
x	O
)	O
.	O
(	O
14.8	O
)	O
the	O
average	O
sum-of-squares	B
error	I
then	O
takes	O
the	O
form	O
ex	O
(	O
14.9	O
)	O
where	O
ex	O
[	O
·	O
]	O
denotes	O
a	O
frequentist	B
expectation	O
with	O
respect	O
to	O
the	O
distribution	O
of	O
the	O
input	O
vector	O
x.	O
the	O
average	O
error	B
made	O
by	O
the	O
models	O
acting	O
individually	O
is	O
therefore	O
=	O
ex	O
m	O
(	O
x	O
)	O
2	O
m	O
(	O
x	O
)	O
2	O
.	O
(	O
14.10	O
)	O
(	O
cid:8	O
)	O
{	O
ym	O
(	O
x	O
)	O
−	O
h	O
(	O
x	O
)	O
}	O
2	O
(	O
cid:9	O
)	O
m	O
(	O
cid:2	O
)	O
eav	O
=	O
ex	O
m=1	O
(	O
cid:8	O
)	O
1	O
m	O
⎡⎣	O
(	O
cid:24	O
)	O
⎡⎣	O
(	O
cid:24	O
)	O
m	O
(	O
cid:2	O
)	O
m	O
(	O
cid:2	O
)	O
m=1	O
m=1	O
1	O
m	O
1	O
m	O
ecom	O
=	O
ex	O
=	O
ex	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
(	O
cid:9	O
)	O
(	O
cid:25	O
)	O
2	O
⎤⎦	O
ym	O
(	O
x	O
)	O
−	O
h	O
(	O
x	O
)	O
(	O
cid:25	O
)	O
2	O
⎤⎦	O
m	O
(	O
x	O
)	O
similarly	O
,	O
the	O
expected	O
error	B
from	O
the	O
committee	B
(	O
14.7	O
)	O
is	O
given	O
by	O
if	O
we	O
assume	O
that	O
the	O
errors	O
have	O
zero	O
mean	B
and	O
are	O
uncorrelated	O
,	O
so	O
that	O
ex	O
[	O
m	O
(	O
x	O
)	O
]	O
=	O
0	O
ex	O
[	O
m	O
(	O
x	O
)	O
l	O
(	O
x	O
)	O
]	O
=	O
0	O
,	O
m	O
(	O
cid:9	O
)	O
=	O
l	O
(	O
14.11	O
)	O
(	O
14.12	O
)	O
(	O
14.13	O
)	O
exercise	O
14.2	O
then	O
we	O
obtain	O
14.3.	O
boosting	B
657	O
ecom	O
=	O
1	O
m	O
eav	O
.	O
(	O
14.14	O
)	O
this	O
apparently	O
dramatic	O
result	O
suggests	O
that	O
the	O
average	O
error	B
of	O
a	O
model	O
can	O
be	O
reduced	O
by	O
a	O
factor	O
of	O
m	O
simply	O
by	O
averaging	O
m	O
versions	O
of	O
the	O
model	O
.	O
unfortu-	O
nately	O
,	O
it	O
depends	O
on	O
the	O
key	O
assumption	O
that	O
the	O
errors	O
due	O
to	O
the	O
individual	O
models	O
are	O
uncorrelated	O
.	O
in	O
practice	O
,	O
the	O
errors	O
are	O
typically	O
highly	O
correlated	O
,	O
and	O
the	O
reduc-	O
tion	O
in	O
overall	O
error	B
is	O
generally	O
small	O
.	O
it	O
can	O
,	O
however	O
,	O
be	O
shown	O
that	O
the	O
expected	O
committee	B
error	O
will	O
not	O
exceed	O
the	O
expected	O
error	B
of	O
the	O
constituent	O
models	O
,	O
so	O
that	O
ecom	O
(	O
cid:1	O
)	O
eav	O
.	O
in	O
order	O
to	O
achieve	O
more	O
signiﬁcant	O
improvements	O
,	O
we	O
turn	O
to	O
a	O
more	O
sophisticated	O
technique	O
for	O
building	O
committees	O
,	O
known	O
as	O
boosting	B
.	O
exercise	O
14.3	O
14.3.	O
boosting	B
boosting	O
is	O
a	O
powerful	O
technique	O
for	O
combining	O
multiple	O
‘	O
base	O
’	O
classiﬁers	O
to	O
produce	O
a	O
form	O
of	O
committee	B
whose	O
performance	O
can	O
be	O
signiﬁcantly	O
better	O
than	O
that	O
of	O
any	O
of	O
the	O
base	O
classiﬁers	O
.	O
here	O
we	O
describe	O
the	O
most	O
widely	O
used	O
form	O
of	O
boosting	B
algorithm	O
called	O
adaboost	O
,	O
short	O
for	O
‘	O
adaptive	O
boosting	O
’	O
,	O
developed	O
by	O
freund	O
and	O
schapire	O
(	O
1996	O
)	O
.	O
boosting	B
can	O
give	O
good	O
results	O
even	O
if	O
the	O
base	O
classiﬁers	O
have	O
a	O
performance	O
that	O
is	O
only	O
slightly	O
better	O
than	O
random	O
,	O
and	O
hence	O
sometimes	O
the	O
base	O
classiﬁers	O
are	O
known	O
as	O
weak	O
learners	O
.	O
originally	O
designed	O
for	O
solving	O
classiﬁcation	B
problems	O
,	O
boosting	B
can	O
also	O
be	O
extended	B
to	O
regression	B
(	O
friedman	O
,	O
2001	O
)	O
.	O
the	O
principal	O
difference	O
between	O
boosting	B
and	O
the	O
committee	B
methods	O
such	O
as	O
bagging	B
discussed	O
above	O
,	O
is	O
that	O
the	O
base	O
classiﬁers	O
are	O
trained	O
in	O
sequence	O
,	O
and	O
each	O
base	O
classiﬁer	O
is	O
trained	O
using	O
a	O
weighted	O
form	O
of	O
the	O
data	O
set	O
in	O
which	O
the	O
weighting	O
coefﬁcient	O
associated	O
with	O
each	O
data	O
point	O
depends	O
on	O
the	O
performance	O
of	O
the	O
previous	O
classiﬁers	O
.	O
in	O
particular	O
,	O
points	O
that	O
are	O
misclassiﬁed	O
by	O
one	O
of	O
the	O
base	O
classiﬁers	O
are	O
given	O
greater	O
weight	O
when	O
used	O
to	O
train	O
the	O
next	O
classiﬁer	O
in	O
the	O
sequence	O
.	O
once	O
all	O
the	O
classiﬁers	O
have	O
been	O
trained	O
,	O
their	O
predictions	O
are	O
then	O
combined	O
through	O
a	O
weighted	O
majority	O
voting	O
scheme	O
,	O
as	O
illustrated	O
schematically	O
in	O
figure	O
14.1.	O
consider	O
a	O
two-class	O
classiﬁcation	B
problem	O
,	O
in	O
which	O
the	O
training	B
data	O
comprises	O
input	O
vectors	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
along	O
with	O
corresponding	O
binary	O
target	O
variables	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
where	O
tn	O
∈	O
{	O
−1	O
,	O
1	O
}	O
.	O
each	O
data	O
point	O
is	O
given	O
an	O
associated	O
weighting	O
parameter	O
wn	O
,	O
which	O
is	O
initially	O
set	O
1/n	O
for	O
all	O
data	O
points	O
.	O
we	O
shall	O
suppose	O
that	O
we	O
have	O
a	O
procedure	O
available	O
for	O
training	O
a	O
base	O
classiﬁer	O
using	O
weighted	O
data	O
to	O
give	O
a	O
function	O
y	O
(	O
x	O
)	O
∈	O
{	O
−1	O
,	O
1	O
}	O
.	O
at	O
each	O
stage	O
of	O
the	O
algorithm	O
,	O
adaboost	O
trains	O
a	O
new	O
classiﬁer	O
using	O
a	O
data	O
set	O
in	O
which	O
the	O
weighting	O
coefﬁcients	O
are	O
adjusted	O
according	O
to	O
the	O
performance	O
of	O
the	O
previously	O
trained	O
classiﬁer	O
so	O
as	O
to	O
give	O
greater	O
weight	O
to	O
the	O
misclassiﬁed	O
data	O
points	O
.	O
finally	O
,	O
when	O
the	O
desired	O
number	O
of	O
base	O
classiﬁers	O
have	O
been	O
trained	O
,	O
they	O
are	O
combined	O
to	O
form	O
a	O
committee	B
using	O
coefﬁcients	O
that	O
give	O
different	O
weight	O
to	O
different	O
base	O
classiﬁers	O
.	O
the	O
precise	O
form	O
of	O
the	O
adaboost	O
algorithm	O
is	O
given	O
below	O
.	O
658	O
14.	O
combining	B
models	I
figure	O
14.1	O
schematic	O
illustration	O
of	O
the	O
boosting	B
framework	O
.	O
each	O
base	O
classiﬁer	O
ym	O
(	O
x	O
)	O
is	O
trained	O
on	O
a	O
weighted	O
form	O
of	O
the	O
train-	O
ing	O
set	O
(	O
blue	O
arrows	O
)	O
in	O
which	O
the	O
weights	O
w	O
(	O
m	O
)	O
depend	O
on	O
the	O
performance	O
of	O
the	O
pre-	O
vious	O
base	O
classiﬁer	O
ym−1	O
(	O
x	O
)	O
(	O
green	O
arrows	O
)	O
.	O
once	O
all	O
base	O
classiﬁers	O
have	O
been	O
trained	O
,	O
they	O
are	O
combined	O
to	O
give	O
the	O
ﬁnal	O
classiﬁer	O
ym	O
(	O
x	O
)	O
(	O
red	O
arrows	O
)	O
.	O
n	O
{	O
w	O
(	O
1	O
)	O
n	O
}	O
{	O
w	O
(	O
2	O
)	O
n	O
}	O
{	O
w	O
(	O
m	O
)	O
n	O
}	O
y1	O
(	O
x	O
)	O
y2	O
(	O
x	O
)	O
ym	O
(	O
x	O
)	O
(	O
cid:22	O
)	O
m	O
(	O
cid:2	O
)	O
ym	O
(	O
x	O
)	O
=	O
sign	O
(	O
cid:23	O
)	O
αmym	O
(	O
x	O
)	O
m	O
adaboost	O
1.	O
initialize	O
the	O
data	O
weighting	O
coefﬁcients	O
{	O
wn	O
}	O
by	O
setting	O
w	O
(	O
1	O
)	O
n	O
=	O
1/n	O
for	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
2.	O
for	O
m	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
:	O
(	O
a	O
)	O
fit	O
a	O
classiﬁer	O
ym	O
(	O
x	O
)	O
to	O
the	O
training	B
data	O
by	O
minimizing	O
the	O
weighted	O
error	O
function	O
jm	O
=	O
n	O
(	O
cid:2	O
)	O
n=1	O
n	O
i	O
(	O
ym	O
(	O
xn	O
)	O
(	O
cid:9	O
)	O
=	O
tn	O
)	O
w	O
(	O
m	O
)	O
(	O
14.15	O
)	O
where	O
i	O
(	O
ym	O
(	O
xn	O
)	O
(	O
cid:9	O
)	O
=	O
tn	O
)	O
is	O
the	O
indicator	O
function	O
and	O
equals	O
1	O
when	O
ym	O
(	O
xn	O
)	O
(	O
cid:9	O
)	O
=	O
tn	O
and	O
0	O
otherwise	O
.	O
n	O
(	O
cid:2	O
)	O
(	O
b	O
)	O
evaluate	O
the	O
quantities	O
m	O
=	O
n=1	O
(	O
14.16	O
)	O
n	O
i	O
(	O
ym	O
(	O
xn	O
)	O
(	O
cid:9	O
)	O
=	O
tn	O
)	O
n	O
(	O
cid:2	O
)	O
w	O
(	O
m	O
)	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
w	O
(	O
m	O
)	O
n=1	O
n	O
1	O
−	O
m	O
m	O
and	O
then	O
use	O
these	O
to	O
evaluate	O
αm	O
=	O
ln	O
.	O
(	O
14.17	O
)	O
(	O
c	O
)	O
update	O
the	O
data	O
weighting	O
coefﬁcients	O
w	O
(	O
m+1	O
)	O
n	O
=	O
w	O
(	O
m	O
)	O
n	O
exp	O
{	O
αmi	O
(	O
ym	O
(	O
xn	O
)	O
(	O
cid:9	O
)	O
=	O
tn	O
)	O
}	O
(	O
14.18	O
)	O
3.	O
make	O
predictions	O
using	O
the	O
ﬁnal	O
model	O
,	O
which	O
is	O
given	O
by	O
14.3.	O
boosting	B
659	O
(	O
cid:22	O
)	O
m	O
(	O
cid:2	O
)	O
(	O
cid:23	O
)	O
ym	O
(	O
x	O
)	O
=	O
sign	O
αmym	O
(	O
x	O
)	O
.	O
(	O
14.19	O
)	O
m=1	O
we	O
see	O
that	O
the	O
ﬁrst	O
base	O
classiﬁer	O
y1	O
(	O
x	O
)	O
is	O
trained	O
using	O
weighting	O
coefﬁ-	O
(	O
1	O
)	O
n	O
that	O
are	O
all	O
equal	O
,	O
which	O
therefore	O
corresponds	O
to	O
the	O
usual	O
procedure	O
cients	O
w	O
for	O
training	O
a	O
single	O
classiﬁer	O
.	O
from	O
(	O
14.18	O
)	O
,	O
we	O
see	O
that	O
in	O
subsequent	O
iterations	O
(	O
m	O
)	O
the	O
weighting	O
coefﬁcients	O
w	O
are	O
increased	O
for	O
data	O
points	O
that	O
are	O
misclassiﬁed	O
n	O
and	O
decreased	O
for	O
data	O
points	O
that	O
are	O
correctly	O
classiﬁed	O
.	O
successive	O
classiﬁers	O
are	O
therefore	O
forced	O
to	O
place	O
greater	O
emphasis	O
on	O
points	O
that	O
have	O
been	O
misclassiﬁed	O
by	O
previous	O
classiﬁers	O
,	O
and	O
data	O
points	O
that	O
continue	O
to	O
be	O
misclassiﬁed	O
by	O
successive	O
classiﬁers	O
receive	O
ever	O
greater	O
weight	O
.	O
the	O
quantities	O
m	O
represent	O
weighted	O
mea-	O
sures	O
of	O
the	O
error	B
rates	O
of	O
each	O
of	O
the	O
base	O
classiﬁers	O
on	O
the	O
data	O
set	O
.	O
we	O
therefore	O
see	O
that	O
the	O
weighting	O
coefﬁcients	O
αm	O
deﬁned	O
by	O
(	O
14.17	O
)	O
give	O
greater	O
weight	O
to	O
the	O
more	O
accurate	O
classiﬁers	O
when	O
computing	O
the	O
overall	O
output	O
given	O
by	O
(	O
14.19	O
)	O
.	O
the	O
adaboost	O
algorithm	O
is	O
illustrated	O
in	O
figure	O
14.2	O
,	O
using	O
a	O
subset	O
of	O
30	O
data	O
points	O
taken	O
from	O
the	O
toy	O
classiﬁcation	B
data	O
set	O
shown	O
in	O
figure	O
a.7	O
.	O
here	O
each	O
base	O
learners	O
consists	O
of	O
a	O
threshold	O
on	O
one	O
of	O
the	O
input	O
variables	O
.	O
this	O
simple	O
classiﬁer	O
corresponds	O
to	O
a	O
form	O
of	O
decision	B
tree	I
known	O
as	O
a	O
‘	O
decision	O
stumps	O
’	O
,	O
i.e.	O
,	O
a	O
deci-	O
sion	B
tree	O
with	O
a	O
single	O
node	B
.	O
thus	O
each	O
base	O
learner	O
classiﬁes	O
an	O
input	O
according	O
to	O
whether	O
one	O
of	O
the	O
input	O
features	O
exceeds	O
some	O
threshold	O
and	O
therefore	O
simply	O
parti-	O
tions	O
the	O
space	O
into	O
two	O
regions	O
separated	O
by	O
a	O
linear	O
decision	O
surface	O
that	O
is	O
parallel	O
to	O
one	O
of	O
the	O
axes	O
.	O
section	O
14.4	O
14.3.1	O
minimizing	O
exponential	O
error	O
boosting	B
was	O
originally	O
motivated	O
using	O
statistical	B
learning	I
theory	I
,	O
leading	O
to	O
upper	O
bounds	O
on	O
the	O
generalization	B
error	O
.	O
however	O
,	O
these	O
bounds	O
turn	O
out	O
to	O
be	O
too	O
loose	O
to	O
have	O
practical	O
value	O
,	O
and	O
the	O
actual	O
performance	O
of	O
boosting	B
is	O
much	O
better	O
than	O
the	O
bounds	O
alone	O
would	O
suggest	O
.	O
friedman	O
et	O
al	O
.	O
(	O
2000	O
)	O
gave	O
a	O
different	O
and	O
very	O
simple	O
interpretation	O
of	O
boosting	B
in	O
terms	O
of	O
the	O
sequential	O
minimization	O
of	O
an	O
exponential	O
error	O
function	O
.	O
consider	O
the	O
exponential	O
error	O
function	O
deﬁned	O
by	O
exp	O
{	O
−tnfm	O
(	O
xn	O
)	O
}	O
e	O
=	O
n	O
(	O
cid:2	O
)	O
n=1	O
(	O
14.20	O
)	O
where	O
fm	O
(	O
x	O
)	O
is	O
a	O
classiﬁer	O
deﬁned	O
in	O
terms	O
of	O
a	O
linear	O
combination	O
of	O
base	O
classiﬁers	O
yl	O
(	O
x	O
)	O
of	O
the	O
form	O
fm	O
(	O
x	O
)	O
=	O
(	O
14.21	O
)	O
and	O
tn	O
∈	O
{	O
−1	O
,	O
1	O
}	O
are	O
the	O
training	B
set	I
target	O
values	O
.	O
our	O
goal	O
is	O
to	O
minimize	O
e	O
with	O
respect	O
to	O
both	O
the	O
weighting	O
coefﬁcients	O
αl	O
and	O
the	O
parameters	O
of	O
the	O
base	O
classiﬁers	O
yl	O
(	O
x	O
)	O
.	O
l=1	O
αlyl	O
(	O
x	O
)	O
m	O
(	O
cid:2	O
)	O
1	O
2	O
660	O
14.	O
combining	B
models	I
m	O
=	O
2	O
−1	O
0	O
1	O
2	O
m	O
=	O
10	O
m	O
=	O
1	O
−1	O
0	O
1	O
2	O
m	O
=	O
6	O
2	O
0	O
−2	O
2	O
0	O
−2	O
2	O
0	O
−2	O
2	O
0	O
−2	O
2	O
0	O
−2	O
2	O
0	O
−2	O
m	O
=	O
3	O
−1	O
0	O
1	O
2	O
m	O
=	O
150	O
−1	O
0	O
1	O
2	O
−1	O
0	O
1	O
2	O
−1	O
0	O
1	O
2	O
figure	O
14.2	O
illustration	O
of	O
boosting	B
in	O
which	O
the	O
base	O
learners	O
consist	O
of	O
simple	O
thresholds	O
applied	O
to	O
one	O
or	O
other	O
of	O
the	O
axes	O
.	O
each	O
ﬁgure	O
shows	O
the	O
number	O
m	O
of	O
base	O
learners	O
trained	O
so	O
far	O
,	O
along	O
with	O
the	O
decision	B
boundary	I
of	O
the	O
most	O
recent	O
base	O
learner	O
(	O
dashed	O
black	O
line	O
)	O
and	O
the	O
combined	O
decision	B
boundary	I
of	O
the	O
en-	O
semble	O
(	O
solid	O
green	O
line	O
)	O
.	O
each	O
data	O
point	O
is	O
depicted	O
by	O
a	O
circle	O
whose	O
radius	O
indicates	O
the	O
weight	O
assigned	O
to	O
that	O
data	O
point	O
when	O
training	B
the	O
most	O
recently	O
added	O
base	O
learner	O
.	O
thus	O
,	O
for	O
instance	O
,	O
we	O
see	O
that	O
points	O
that	O
are	O
misclassiﬁed	O
by	O
the	O
m	O
=	O
1	O
base	O
learner	O
are	O
given	O
greater	O
weight	O
when	O
training	B
the	O
m	O
=	O
2	O
base	O
learner	O
.	O
instead	O
of	O
doing	O
a	O
global	O
error	O
function	O
minimization	O
,	O
however	O
,	O
we	O
shall	O
sup-	O
pose	O
that	O
the	O
base	O
classiﬁers	O
y1	O
(	O
x	O
)	O
,	O
.	O
.	O
.	O
,	O
ym−1	O
(	O
x	O
)	O
are	O
ﬁxed	O
,	O
as	O
are	O
their	O
coefﬁcients	O
α1	O
,	O
.	O
.	O
.	O
,	O
αm−1	O
,	O
and	O
so	O
we	O
are	O
minimizing	O
only	O
with	O
respect	O
to	O
αm	O
and	O
ym	O
(	O
x	O
)	O
.	O
sep-	O
arating	O
off	O
the	O
contribution	O
from	O
base	O
classiﬁer	O
ym	O
(	O
x	O
)	O
,	O
we	O
can	O
then	O
write	O
the	O
error	B
function	I
in	O
the	O
form	O
(	O
cid:13	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
n=1	O
e	O
=	O
=	O
exp	O
−tnfm−1	O
(	O
xn	O
)	O
−	O
1	O
2	O
tnαmym	O
(	O
xn	O
)	O
(	O
cid:13	O
)	O
w	O
(	O
m	O
)	O
n	O
exp	O
−1	O
2	O
tnαmym	O
(	O
xn	O
)	O
(	O
14.22	O
)	O
(	O
m	O
)	O
n	O
=	O
exp	O
{	O
−tnfm−1	O
(	O
xn	O
)	O
}	O
can	O
be	O
viewed	O
as	O
constants	O
where	O
the	O
coefﬁcients	O
w	O
if	O
we	O
denote	O
by	O
tm	O
the	O
set	O
of	O
because	O
we	O
are	O
optimizing	O
only	O
αm	O
and	O
ym	O
(	O
x	O
)	O
.	O
data	O
points	O
that	O
are	O
correctly	O
classiﬁed	O
by	O
ym	O
(	O
x	O
)	O
,	O
and	O
if	O
we	O
denote	O
the	O
remaining	O
misclassiﬁed	O
points	O
by	O
mm	O
,	O
then	O
we	O
can	O
in	O
turn	O
rewrite	O
the	O
error	B
function	I
in	O
the	O
14.3.	O
boosting	B
661	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
n∈mm	O
w	O
(	O
m	O
)	O
n	O
form	O
e	O
=	O
e	O
−αm/2	O
w	O
(	O
m	O
)	O
n	O
+	O
eαm/2	O
n∈tm	O
=	O
(	O
eαm/2	O
−	O
e	O
−αm/2	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
n	O
i	O
(	O
ym	O
(	O
xn	O
)	O
(	O
cid:9	O
)	O
=	O
tn	O
)	O
+	O
e	O
w	O
(	O
m	O
)	O
−αm/2	O
n	O
(	O
cid:2	O
)	O
n=1	O
w	O
(	O
m	O
)	O
n	O
.	O
(	O
14.23	O
)	O
exercise	O
14.6	O
exercise	O
14.7	O
when	O
we	O
minimize	O
this	O
with	O
respect	O
to	O
ym	O
(	O
x	O
)	O
,	O
we	O
see	O
that	O
the	O
second	O
term	O
is	O
con-	O
stant	O
,	O
and	O
so	O
this	O
is	O
equivalent	O
to	O
minimizing	O
(	O
14.15	O
)	O
because	O
the	O
overall	O
multiplica-	O
tive	O
factor	O
in	O
front	O
of	O
the	O
summation	O
does	O
not	O
affect	O
the	O
location	O
of	O
the	O
minimum	O
.	O
similarly	O
,	O
minimizing	O
with	O
respect	O
to	O
αm	O
,	O
we	O
obtain	O
(	O
14.17	O
)	O
in	O
which	O
m	O
is	O
deﬁned	O
by	O
(	O
14.16	O
)	O
.	O
from	O
(	O
14.22	O
)	O
we	O
see	O
that	O
,	O
having	O
found	O
αm	O
and	O
ym	O
(	O
x	O
)	O
,	O
the	O
weights	O
on	O
the	O
data	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
−1	O
2	O
tnαmym	O
(	O
xn	O
)	O
.	O
(	O
14.24	O
)	O
points	O
are	O
updated	O
using	O
w	O
(	O
m+1	O
)	O
n	O
=	O
w	O
(	O
m	O
)	O
n	O
exp	O
making	O
use	O
of	O
the	O
fact	O
that	O
tnym	O
(	O
xn	O
)	O
=	O
1	O
−	O
2i	O
(	O
ym	O
(	O
xn	O
)	O
(	O
cid:9	O
)	O
=	O
tn	O
)	O
(	O
14.25	O
)	O
(	O
m	O
)	O
we	O
see	O
that	O
the	O
weights	O
w	O
n	O
are	O
updated	O
at	O
the	O
next	O
iteration	O
using	O
exp	O
(	O
−αm/2	O
)	O
exp	O
{	O
αmi	O
(	O
ym	O
(	O
xn	O
)	O
(	O
cid:9	O
)	O
=	O
tn	O
)	O
}	O
.	O
w	O
(	O
m+1	O
)	O
=	O
w	O
(	O
m	O
)	O
n	O
n	O
(	O
14.26	O
)	O
because	O
the	O
term	O
exp	O
(	O
−αm/2	O
)	O
is	O
independent	B
of	O
n	O
,	O
we	O
see	O
that	O
it	O
weights	O
all	O
data	O
points	O
by	O
the	O
same	O
factor	O
and	O
so	O
can	O
be	O
discarded	O
.	O
thus	O
we	O
obtain	O
(	O
14.18	O
)	O
.	O
finally	O
,	O
once	O
all	O
the	O
base	O
classiﬁers	O
are	O
trained	O
,	O
new	O
data	O
points	O
are	O
classiﬁed	O
by	O
evaluating	O
the	O
sign	O
of	O
the	O
combined	O
function	O
deﬁned	O
according	O
to	O
(	O
14.21	O
)	O
.	O
because	O
the	O
factor	O
of	O
1/2	O
does	O
not	O
affect	O
the	O
sign	O
it	O
can	O
be	O
omitted	O
,	O
giving	O
(	O
14.19	O
)	O
.	O
14.3.2	O
error	B
functions	O
for	O
boosting	O
the	O
exponential	O
error	O
function	O
that	O
is	O
minimized	O
by	O
the	O
adaboost	O
algorithm	O
differs	O
from	O
those	O
considered	O
in	O
previous	O
chapters	O
.	O
to	O
gain	O
some	O
insight	O
into	O
the	O
nature	O
of	O
the	O
exponential	O
error	O
function	O
,	O
we	O
ﬁrst	O
consider	O
the	O
expected	O
error	B
given	O
by	O
(	O
cid:6	O
)	O
ex	O
,	O
t	O
[	O
exp	O
{	O
−ty	O
(	O
x	O
)	O
}	O
]	O
=	O
exp	O
{	O
−ty	O
(	O
x	O
)	O
}	O
p	O
(	O
t|x	O
)	O
p	O
(	O
x	O
)	O
dx	O
.	O
(	O
14.27	O
)	O
(	O
cid:2	O
)	O
t	O
if	O
we	O
perform	O
a	O
variational	B
minimization	O
with	O
respect	O
to	O
all	O
possible	O
functions	O
y	O
(	O
x	O
)	O
,	O
we	O
obtain	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
y	O
(	O
x	O
)	O
=	O
ln	O
1	O
2	O
p	O
(	O
t	O
=	O
1|x	O
)	O
p	O
(	O
t	O
=	O
−1|x	O
)	O
(	O
14.28	O
)	O
662	O
14.	O
combining	B
models	I
figure	O
14.3	O
plot	O
of	O
the	O
exponential	O
(	O
green	O
)	O
and	O
rescaled	O
cross-entropy	O
(	O
red	O
)	O
error	B
functions	O
along	O
with	O
the	O
hinge	O
er-	O
ror	O
(	O
blue	O
)	O
used	O
in	O
support	B
vector	I
machines	O
,	O
and	O
the	O
misclassiﬁcation	O
for	O
large	O
error	B
(	O
black	O
)	O
.	O
note	O
that	O
negative	O
values	O
of	O
z	O
=	O
ty	O
(	O
x	O
)	O
,	O
the	O
cross-entropy	O
gives	O
a	O
linearly	O
in-	O
creasing	O
penalty	O
,	O
whereas	O
the	O
expo-	O
nential	O
loss	O
gives	O
an	O
exponentially	O
in-	O
creasing	O
penalty	O
.	O
e	O
(	O
z	O
)	O
−2	O
−1	O
0	O
1	O
z	O
2	O
which	O
is	O
half	O
the	O
log-odds	O
.	O
thus	O
the	O
adaboost	O
algorithm	O
is	O
seeking	O
the	O
best	O
approx-	O
imation	O
to	O
the	O
log	B
odds	I
ratio	O
,	O
within	O
the	O
space	O
of	O
functions	O
represented	O
by	O
the	O
linear	O
combination	O
of	O
base	O
classiﬁers	O
,	O
subject	O
to	O
the	O
constrained	O
minimization	O
resulting	O
from	O
the	O
sequential	O
optimization	O
strategy	O
.	O
this	O
result	O
motivates	O
the	O
use	O
of	O
the	O
sign	O
function	O
in	O
(	O
14.19	O
)	O
to	O
arrive	O
at	O
the	O
ﬁnal	O
classiﬁcation	B
decision	O
.	O
we	O
have	O
already	O
seen	O
that	O
the	O
minimizer	O
y	O
(	O
x	O
)	O
of	O
the	O
cross-entropy	O
error	O
(	O
4.90	O
)	O
for	O
two-class	O
classiﬁcation	B
is	O
given	O
by	O
the	O
posterior	O
class	O
probability	B
.	O
in	O
the	O
case	O
of	O
a	O
target	O
variable	O
t	O
∈	O
{	O
−1	O
,	O
1	O
}	O
,	O
we	O
have	O
seen	O
that	O
the	O
error	B
function	I
is	O
given	O
by	O
ln	O
(	O
1	O
+	O
exp	O
(	O
−yt	O
)	O
)	O
.	O
this	O
is	O
compared	O
with	O
the	O
exponential	O
error	O
function	O
in	O
fig-	O
ure	O
14.3	O
,	O
where	O
we	O
have	O
divided	O
the	O
cross-entropy	O
error	O
by	O
a	O
constant	O
factor	O
ln	O
(	O
2	O
)	O
so	O
that	O
it	O
passes	O
through	O
the	O
point	O
(	O
0	O
,	O
1	O
)	O
for	O
ease	O
of	O
comparison	O
.	O
we	O
see	O
that	O
both	O
can	O
be	O
seen	O
as	O
continuous	O
approximations	O
to	O
the	O
ideal	O
misclassiﬁcation	O
error	B
func-	O
tion	O
.	O
an	O
advantage	O
of	O
the	O
exponential	O
error	O
is	O
that	O
its	O
sequential	O
minimization	O
leads	O
to	O
the	O
simple	O
adaboost	O
scheme	O
.	O
one	O
drawback	O
,	O
however	O
,	O
is	O
that	O
it	O
penalizes	O
large	O
negative	O
values	O
of	O
ty	O
(	O
x	O
)	O
much	O
more	O
strongly	O
than	O
cross-entropy	O
.	O
in	O
particular	O
,	O
we	O
see	O
that	O
for	O
large	O
negative	O
values	O
of	O
ty	O
,	O
the	O
cross-entropy	O
grows	O
linearly	O
with	O
|ty|	O
,	O
whereas	O
the	O
exponential	O
error	O
function	O
grows	O
exponentially	O
with	O
|ty|	O
.	O
thus	O
the	O
ex-	O
ponential	O
error	B
function	I
will	O
be	O
much	O
less	O
robust	O
to	O
outliers	B
or	O
misclassiﬁed	O
data	O
points	O
.	O
another	O
important	O
difference	O
between	O
cross-entropy	O
and	O
the	O
exponential	O
er-	O
ror	O
function	O
is	O
that	O
the	O
latter	O
can	O
not	O
be	O
interpreted	O
as	O
the	O
log	O
likelihood	O
function	O
of	O
any	O
well-deﬁned	O
probabilistic	O
model	O
.	O
furthermore	O
,	O
the	O
exponential	O
error	O
does	O
not	O
generalize	O
to	O
classiﬁcation	B
problems	O
having	O
k	O
>	O
2	O
classes	O
,	O
again	O
in	O
contrast	O
to	O
the	O
cross-entropy	O
for	O
a	O
probabilistic	O
model	O
,	O
which	O
is	O
easily	O
generalized	B
to	O
give	O
(	O
4.108	O
)	O
.	O
the	O
interpretation	O
of	O
boosting	B
as	O
the	O
sequential	O
optimization	O
of	O
an	O
additive	O
model	O
under	O
an	O
exponential	O
error	O
(	O
friedman	O
et	O
al.	O
,	O
2000	O
)	O
opens	O
the	O
door	O
to	O
a	O
wide	O
range	O
of	O
boosting-like	O
algorithms	O
,	O
including	O
multiclass	B
extensions	O
,	O
by	O
altering	O
the	O
choice	O
of	O
error	B
function	I
.	O
it	O
also	O
motivates	O
the	O
extension	O
to	O
regression	B
problems	O
(	O
friedman	O
,	O
2001	O
)	O
.	O
if	O
we	O
consider	O
a	O
sum-of-squares	B
error	I
function	O
for	B
regression	I
,	O
then	O
sequential	O
minimization	O
of	O
an	O
additive	O
model	O
of	O
the	O
form	O
(	O
14.21	O
)	O
simply	O
involves	O
ﬁtting	O
each	O
new	O
base	O
classiﬁer	O
to	O
the	O
residual	O
errors	O
tn−fm−1	O
(	O
xn	O
)	O
from	O
the	O
previous	O
model	O
.	O
as	O
we	O
have	O
noted	O
,	O
however	O
,	O
the	O
sum-of-squares	B
error	I
is	O
not	O
robust	O
to	O
outliers	B
,	O
and	O
this	O
section	O
7.1.2	O
exercise	O
14.8	O
section	O
4.3.4	O
exercise	O
14.9	O
figure	O
14.4	O
comparison	O
of	O
the	O
squared	O
error	B
(	O
green	O
)	O
with	O
the	O
absolute	O
error	B
(	O
red	O
)	O
showing	O
how	O
the	O
latter	O
places	O
much	O
less	O
emphasis	O
on	O
large	O
errors	O
and	O
hence	O
is	O
more	O
robust	O
to	O
outliers	B
and	O
mislabelled	O
data	O
points	O
.	O
14.4.	O
tree-based	O
models	O
663	O
e	O
(	O
z	O
)	O
−1	O
0	O
1	O
z	O
can	O
be	O
addressed	O
by	O
basing	O
the	O
boosting	B
algorithm	O
on	O
the	O
absolute	O
deviation	O
|y	O
−	O
t|	O
instead	O
.	O
these	O
two	O
error	B
functions	O
are	O
compared	O
in	O
figure	O
14.4	O
.	O
14.4.	O
tree-based	O
models	O
there	O
are	O
various	O
simple	O
,	O
but	O
widely	O
used	O
,	O
models	O
that	O
work	O
by	O
partitioning	O
the	O
input	O
space	O
into	O
cuboid	O
regions	O
,	O
whose	O
edges	O
are	O
aligned	O
with	O
the	O
axes	O
,	O
and	O
then	O
assigning	O
a	O
simple	O
model	O
(	O
for	O
example	O
,	O
a	O
constant	O
)	O
to	O
each	O
region	O
.	O
they	O
can	O
be	O
viewed	O
as	O
a	O
model	O
combination	O
method	O
in	O
which	O
only	O
one	O
model	O
is	O
responsible	O
for	O
making	O
predictions	O
at	O
any	O
given	O
point	O
in	O
input	O
space	O
.	O
the	O
process	O
of	O
selecting	O
a	O
speciﬁc	O
model	O
,	O
given	O
a	O
new	O
input	O
x	O
,	O
can	O
be	O
described	O
by	O
a	O
sequential	O
decision	O
making	O
process	O
corresponding	O
to	O
the	O
traversal	O
of	O
a	O
binary	O
tree	O
(	O
one	O
that	O
splits	O
into	O
two	O
branches	O
at	O
each	O
node	B
)	O
.	O
here	O
we	O
focus	O
on	O
a	O
particular	O
tree-based	O
framework	O
called	O
classiﬁcation	B
and	I
regression	I
trees	I
,	O
or	O
cart	O
(	O
breiman	O
et	O
al.	O
,	O
1984	O
)	O
,	O
although	O
there	O
are	O
many	O
other	O
variants	O
going	O
by	O
such	O
names	O
as	O
id3	O
and	O
c4.5	O
(	O
quinlan	O
,	O
1986	O
;	O
quinlan	O
,	O
1993	O
)	O
.	O
figure	O
14.5	O
shows	O
an	O
illustration	O
of	O
a	O
recursive	O
binary	O
partitioning	O
of	O
the	O
input	O
space	O
,	O
along	O
with	O
the	O
corresponding	O
tree	B
structure	O
.	O
in	O
this	O
example	O
,	O
the	O
ﬁrst	O
step	O
figure	O
14.5	O
illustration	O
of	O
a	O
two-dimensional	O
in-	O
put	O
space	O
that	O
has	O
been	O
partitioned	B
into	O
ﬁve	O
regions	O
using	O
axis-aligned	O
boundaries	O
.	O
x2	O
θ3	O
θ2	O
b	O
a	O
e	O
c	O
d	O
θ1	O
θ4	O
x1	O
664	O
14.	O
combining	B
models	I
figure	O
14.6	O
binary	O
tree	O
corresponding	O
to	O
the	O
par-	O
input	O
space	O
shown	O
in	O
fig-	O
titioning	O
of	O
ure	O
14.5.	O
x1	O
>	O
θ1	O
x2	O
(	O
cid:1	O
)	O
θ2	O
x2	O
>	O
θ3	O
x1	O
(	O
cid:1	O
)	O
θ4	O
a	O
b	O
c	O
d	O
e	O
divides	O
the	O
whole	O
of	O
the	O
input	O
space	O
into	O
two	O
regions	O
according	O
to	O
whether	O
x1	O
(	O
cid:1	O
)	O
θ1	O
or	O
x1	O
>	O
θ1	O
where	O
θ1	O
is	O
a	O
parameter	O
of	O
the	O
model	O
.	O
this	O
creates	O
two	O
subregions	O
,	O
each	O
of	O
which	O
can	O
then	O
be	O
subdivided	O
independently	O
.	O
for	O
instance	O
,	O
the	O
region	O
x1	O
(	O
cid:1	O
)	O
θ1	O
is	O
further	O
subdivided	O
according	O
to	O
whether	O
x2	O
(	O
cid:1	O
)	O
θ2	O
or	O
x2	O
>	O
θ2	O
,	O
giving	O
rise	O
to	O
the	O
regions	O
denoted	O
a	O
and	O
b.	O
the	O
recursive	O
subdivision	O
can	O
be	O
described	O
by	O
the	O
traversal	O
of	O
the	O
binary	O
tree	O
shown	O
in	O
figure	O
14.6.	O
for	O
any	O
new	O
input	O
x	O
,	O
we	O
determine	O
which	O
region	O
it	O
falls	O
into	O
by	O
starting	O
at	O
the	O
top	O
of	O
the	O
tree	B
at	O
the	O
root	B
node	I
and	O
following	O
a	O
path	O
down	O
to	O
a	O
speciﬁc	O
leaf	O
node	B
according	O
to	O
the	O
decision	O
criteria	O
at	O
each	O
node	B
.	O
note	O
that	O
such	O
decision	O
trees	O
are	O
not	O
probabilistic	O
graphical	O
models	O
.	O
within	O
each	O
region	O
,	O
there	O
is	O
a	O
separate	O
model	O
to	O
predict	O
the	O
target	O
variable	O
.	O
for	O
instance	O
,	O
in	O
regression	B
we	O
might	O
simply	O
predict	O
a	O
constant	O
over	O
each	O
region	O
,	O
or	O
in	O
classiﬁcation	B
we	O
might	O
assign	O
each	O
region	O
to	O
a	O
speciﬁc	O
class	O
.	O
a	O
key	O
property	O
of	O
tree-	O
based	O
models	O
,	O
which	O
makes	O
them	O
popular	O
in	O
ﬁelds	O
such	O
as	O
medical	O
diagnosis	O
,	O
for	O
example	O
,	O
is	O
that	O
they	O
are	O
readily	O
interpretable	O
by	O
humans	O
because	O
they	O
correspond	O
to	O
a	O
sequence	O
of	O
binary	O
decisions	O
applied	O
to	O
the	O
individual	O
input	O
variables	O
.	O
for	O
in-	O
stance	O
,	O
to	O
predict	O
a	O
patient	O
’	O
s	O
disease	O
,	O
we	O
might	O
ﬁrst	O
ask	O
“	O
is	O
their	O
temperature	O
greater	O
than	O
some	O
threshold	O
?	O
”	O
.	O
if	O
the	O
answer	O
is	O
yes	O
,	O
then	O
we	O
might	O
next	O
ask	O
“	O
is	O
their	O
blood	O
pressure	O
less	O
than	O
some	O
threshold	O
?	O
”	O
.	O
each	O
leaf	O
of	O
the	O
tree	B
is	O
then	O
associated	O
with	O
a	O
speciﬁc	O
diagnosis	O
.	O
in	O
order	O
to	O
learn	O
such	O
a	O
model	O
from	O
a	O
training	B
set	I
,	O
we	O
have	O
to	O
determine	O
the	O
structure	O
of	O
the	O
tree	B
,	O
including	O
which	O
input	O
variable	O
is	O
chosen	O
at	O
each	O
node	B
to	O
form	O
the	O
split	O
criterion	O
as	O
well	O
as	O
the	O
value	O
of	O
the	O
threshold	B
parameter	I
θi	O
for	O
the	O
split	O
.	O
we	O
also	O
have	O
to	O
determine	O
the	O
values	O
of	O
the	O
predictive	O
variable	O
within	O
each	O
region	O
.	O
consider	O
ﬁrst	O
a	O
regression	B
problem	O
in	O
which	O
the	O
goal	O
is	O
to	O
predict	O
a	O
single	O
target	O
variable	O
t	O
from	O
a	O
d-dimensional	O
vector	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
t	O
of	O
input	O
variables	O
.	O
the	O
training	B
data	O
consists	O
of	O
input	O
vectors	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
along	O
with	O
the	O
corresponding	O
continuous	O
labels	O
{	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
}	O
.	O
if	O
the	O
partitioning	O
of	O
the	O
input	O
space	O
is	O
given	O
,	O
and	O
we	O
minimize	O
the	O
sum-of-squares	B
error	I
function	O
,	O
then	O
the	O
optimal	O
value	O
of	O
the	O
predictive	O
variable	O
within	O
any	O
given	O
region	O
is	O
just	O
given	O
by	O
the	O
average	O
of	O
the	O
values	O
of	O
tn	O
for	O
those	O
data	O
points	O
that	O
fall	O
in	O
that	O
region	O
.	O
now	O
consider	O
how	O
to	O
determine	O
the	O
structure	O
of	O
the	O
decision	B
tree	I
.	O
even	O
for	O
a	O
ﬁxed	O
number	O
of	O
nodes	O
in	O
the	O
tree	B
,	O
the	O
problem	O
of	O
determining	O
the	O
optimal	O
structure	O
(	O
including	O
choice	O
of	O
input	O
variable	O
for	O
each	O
split	O
as	O
well	O
as	O
the	O
corresponding	O
thresh-	O
exercise	O
14.10	O
14.4.	O
tree-based	O
models	O
665	O
olds	O
)	O
to	O
minimize	O
the	O
sum-of-squares	B
error	I
is	O
usually	O
computationally	O
infeasible	O
due	O
to	O
the	O
combinatorially	O
large	O
number	O
of	O
possible	O
solutions	O
.	O
instead	O
,	O
a	O
greedy	O
opti-	O
mization	O
is	O
generally	O
done	O
by	O
starting	O
with	O
a	O
single	O
root	B
node	I
,	O
corresponding	O
to	O
the	O
whole	O
input	O
space	O
,	O
and	O
then	O
growing	O
the	O
tree	B
by	O
adding	O
nodes	O
one	O
at	O
a	O
time	O
.	O
at	O
each	O
step	O
there	O
will	O
be	O
some	O
number	O
of	O
candidate	O
regions	O
in	O
input	O
space	O
that	O
can	O
be	O
split	O
,	O
corresponding	O
to	O
the	O
addition	O
of	O
a	O
pair	O
of	O
leaf	O
nodes	O
to	O
the	O
existing	O
tree	B
.	O
for	O
each	O
of	O
these	O
,	O
there	O
is	O
a	O
choice	O
of	O
which	O
of	O
the	O
d	O
input	O
variables	O
to	O
split	O
,	O
as	O
well	O
as	O
the	O
value	O
of	O
the	O
threshold	O
.	O
the	O
joint	O
optimization	O
of	O
the	O
choice	O
of	O
region	O
to	O
split	O
,	O
and	O
the	O
choice	O
of	O
input	O
variable	O
and	O
threshold	O
,	O
can	O
be	O
done	O
efﬁciently	O
by	O
exhaustive	O
search	O
noting	O
that	O
,	O
for	O
a	O
given	O
choice	O
of	O
split	O
variable	O
and	O
threshold	O
,	O
the	O
optimal	O
choice	O
of	O
predictive	O
variable	O
is	O
given	O
by	O
the	O
local	B
average	O
of	O
the	O
data	O
,	O
as	O
noted	O
earlier	O
.	O
this	O
is	O
repeated	O
for	O
all	O
possible	O
choices	O
of	O
variable	O
to	O
be	O
split	O
,	O
and	O
the	O
one	O
that	O
gives	O
the	O
smallest	O
residual	O
sum-of-squares	B
error	I
is	O
retained	O
.	O
given	O
a	O
greedy	O
strategy	O
for	O
growing	O
the	O
tree	B
,	O
there	O
remains	O
the	O
issue	O
of	O
when	O
to	O
stop	O
adding	O
nodes	O
.	O
a	O
simple	O
approach	O
would	O
be	O
to	O
stop	O
when	O
the	O
reduction	O
in	O
residual	O
error	B
falls	O
below	O
some	O
threshold	O
.	O
however	O
,	O
it	O
is	O
found	O
empirically	O
that	O
often	O
none	O
of	O
the	O
available	O
splits	O
produces	O
a	O
signiﬁcant	O
reduction	O
in	O
error	B
,	O
and	O
yet	O
after	O
several	O
more	O
splits	O
a	O
substantial	O
error	B
reduction	O
is	O
found	O
.	O
for	O
this	O
reason	O
,	O
it	O
is	O
com-	O
mon	O
practice	O
to	O
grow	O
a	O
large	O
tree	O
,	O
using	O
a	O
stopping	O
criterion	O
based	O
on	O
the	O
number	O
of	O
data	O
points	O
associated	O
with	O
the	O
leaf	O
nodes	O
,	O
and	O
then	O
prune	O
back	O
the	O
resulting	O
tree	B
.	O
the	O
pruning	O
is	O
based	O
on	O
a	O
criterion	O
that	O
balances	O
residual	O
error	B
against	O
a	O
measure	O
of	O
model	O
complexity	O
.	O
if	O
we	O
denote	O
the	O
starting	O
tree	B
for	O
pruning	O
by	O
t0	O
,	O
then	O
we	O
deﬁne	O
t	O
⊂	O
t0	O
to	O
be	O
a	O
subtree	O
of	O
t0	O
if	O
it	O
can	O
be	O
obtained	O
by	O
pruning	O
nodes	O
from	O
t0	O
(	O
in	O
other	O
words	O
,	O
by	O
collapsing	O
internal	O
nodes	O
by	O
combining	O
the	O
corresponding	O
regions	O
)	O
.	O
suppose	O
the	O
leaf	O
nodes	O
are	O
indexed	O
by	O
τ	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
|t|	O
,	O
with	O
leaf	O
node	B
τ	O
representing	O
a	O
region	O
rτ	O
of	O
input	O
space	O
having	O
nτ	O
data	O
points	O
,	O
and	O
|t|	O
denoting	O
the	O
total	O
number	O
of	O
leaf	O
nodes	O
.	O
the	O
optimal	O
prediction	O
for	O
region	O
rτ	O
is	O
then	O
given	O
by	O
yτ	O
=	O
tn	O
(	O
14.29	O
)	O
and	O
the	O
corresponding	O
contribution	O
to	O
the	O
residual	O
sum-of-squares	O
is	O
then	O
qτ	O
(	O
t	O
)	O
=	O
{	O
tn	O
−	O
yτ	O
}	O
2	O
.	O
(	O
cid:2	O
)	O
xn∈rτ	O
1	O
nτ	O
(	O
cid:2	O
)	O
xn∈rτ	O
|t|	O
(	O
cid:2	O
)	O
(	O
14.30	O
)	O
(	O
14.31	O
)	O
the	O
pruning	O
criterion	O
is	O
then	O
given	O
by	O
c	O
(	O
t	O
)	O
=	O
qτ	O
(	O
t	O
)	O
+	O
λ|t|	O
τ	O
=1	O
the	O
regularization	B
parameter	O
λ	O
determines	O
the	O
trade-off	O
between	O
the	O
overall	O
residual	O
sum-of-squares	B
error	I
and	O
the	O
complexity	O
of	O
the	O
model	O
as	O
measured	O
by	O
the	O
number	O
|t|	O
of	O
leaf	O
nodes	O
,	O
and	O
its	O
value	O
is	O
chosen	O
by	O
cross-validation	B
.	O
for	O
classiﬁcation	O
problems	O
,	O
the	O
process	O
of	O
growing	O
and	O
pruning	O
the	O
tree	B
is	O
sim-	O
ilar	O
,	O
except	O
that	O
the	O
sum-of-squares	B
error	I
is	O
replaced	O
by	O
a	O
more	O
appropriate	O
measure	O
666	O
14.	O
combining	B
models	I
of	O
performance	O
.	O
if	O
we	O
deﬁne	O
pτ	O
k	O
to	O
be	O
the	O
proportion	O
of	O
data	O
points	O
in	O
region	O
rτ	O
assigned	O
to	O
class	O
k	O
,	O
where	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
,	O
then	O
two	O
commonly	O
used	O
choices	O
are	O
the	O
cross-entropy	O
qτ	O
(	O
t	O
)	O
=	O
pτ	O
k	O
ln	O
pτ	O
k	O
k	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
k=1	O
(	O
14.32	O
)	O
(	O
14.33	O
)	O
and	O
the	O
gini	O
index	O
qτ	O
(	O
t	O
)	O
=	O
pτ	O
k	O
(	O
1	O
−	O
pτ	O
k	O
)	O
.	O
exercise	O
14.11	O
k=1	O
these	O
both	O
vanish	O
for	O
pτ	O
k	O
=	O
0	O
and	O
pτ	O
k	O
=	O
1	O
and	O
have	O
a	O
maximum	O
at	O
pτ	O
k	O
=	O
0.5.	O
they	O
encourage	O
the	O
formation	O
of	O
regions	O
in	O
which	O
a	O
high	O
proportion	O
of	O
the	O
data	O
points	O
are	O
assigned	O
to	O
one	O
class	O
.	O
the	O
cross	O
entropy	B
and	O
the	O
gini	O
index	O
are	O
better	O
measures	O
than	O
the	O
misclassiﬁcation	O
rate	O
for	O
growing	O
the	O
tree	B
because	O
they	O
are	O
more	O
sensitive	O
to	O
the	O
node	B
probabilities	O
.	O
also	O
,	O
unlike	O
misclassiﬁcation	O
rate	O
,	O
they	O
are	O
differentiable	O
and	O
hence	O
better	O
suited	O
to	O
gradient	O
based	O
optimization	O
methods	O
.	O
for	O
subsequent	O
pruning	O
of	O
the	O
tree	B
,	O
the	O
misclassiﬁcation	O
rate	O
is	O
generally	O
used	O
.	O
the	O
human	O
interpretability	O
of	O
a	O
tree	B
model	O
such	O
as	O
cart	O
is	O
often	O
seen	O
as	O
its	O
major	O
strength	O
.	O
however	O
,	O
in	O
practice	O
it	O
is	O
found	O
that	O
the	O
particular	O
tree	B
structure	O
that	O
is	O
learned	O
is	O
very	O
sensitive	O
to	O
the	O
details	O
of	O
the	O
data	O
set	O
,	O
so	O
that	O
a	O
small	O
change	O
to	O
the	O
training	B
data	O
can	O
result	O
in	O
a	O
very	O
different	O
set	O
of	O
splits	O
(	O
hastie	O
et	O
al.	O
,	O
2001	O
)	O
.	O
there	O
are	O
other	O
problems	O
with	O
tree-based	O
methods	O
of	O
the	O
kind	O
considered	O
in	O
this	O
section	O
.	O
one	O
is	O
that	O
the	O
splits	O
are	O
aligned	O
with	O
the	O
axes	O
of	O
the	O
feature	B
space	I
,	O
which	O
may	O
be	O
very	O
suboptimal	O
.	O
for	O
instance	O
,	O
to	O
separate	O
two	O
classes	O
whose	O
optimal	O
decision	B
boundary	I
runs	O
at	O
45	O
degrees	O
to	O
the	O
axes	O
would	O
need	O
a	O
large	O
number	O
of	O
axis-parallel	O
splits	O
of	O
the	O
input	O
space	O
as	O
compared	O
to	O
a	O
single	O
non-axis-aligned	O
split	O
.	O
furthermore	O
,	O
the	O
splits	O
in	O
a	O
decision	B
tree	I
are	O
hard	O
,	O
so	O
that	O
each	O
region	O
of	O
input	O
space	O
is	O
associated	O
with	O
one	O
,	O
and	O
only	O
one	O
,	O
leaf	O
node	B
model	O
.	O
the	O
last	O
issue	O
is	O
particularly	O
problematic	O
in	O
regression	B
where	O
we	O
are	O
typically	O
aiming	O
to	O
model	O
smooth	O
functions	O
,	O
and	O
yet	O
the	O
tree	B
model	O
produces	O
piecewise-constant	O
predictions	O
with	O
discontinuities	O
at	O
the	O
split	O
boundaries	O
.	O
14.5.	O
conditional	O
mixture	O
models	O
we	O
have	O
seen	O
that	O
standard	O
decision	O
trees	O
are	O
restricted	O
by	O
hard	O
,	O
axis-aligned	O
splits	O
of	O
the	O
input	O
space	O
.	O
these	O
constraints	O
can	O
be	O
relaxed	O
,	O
at	O
the	O
expense	O
of	O
interpretability	O
,	O
by	O
allowing	O
soft	B
,	O
probabilistic	O
splits	O
that	O
can	O
be	O
functions	O
of	O
all	O
of	O
the	O
input	O
variables	O
,	O
not	O
just	O
one	O
of	O
them	O
at	O
a	O
time	O
.	O
if	O
we	O
also	O
give	O
the	O
leaf	O
models	O
a	O
probabilistic	O
inter-	O
pretation	O
,	O
we	O
arrive	O
at	O
a	O
fully	O
probabilistic	O
tree-based	O
model	O
called	O
the	O
hierarchical	B
mixture	I
of	I
experts	I
,	O
which	O
we	O
consider	O
in	O
section	O
14.5.3.	O
an	O
alternative	O
way	O
to	O
motivate	O
the	O
hierarchical	B
mixture	I
of	I
experts	I
model	O
is	O
to	O
start	O
with	O
a	O
standard	O
probabilistic	O
mixtures	O
of	O
unconditional	O
density	B
models	O
such	O
as	O
gaussians	O
and	O
replace	O
the	O
component	O
densities	O
with	O
conditional	B
distributions	O
.	O
here	O
we	O
consider	O
mixtures	O
of	O
linear	B
regression	I
models	O
(	O
section	O
14.5.1	O
)	O
and	O
mixtures	O
of	O
chapter	O
9	O
14.5.	O
conditional	O
mixture	O
models	O
667	O
logistic	B
regression	I
models	O
(	O
section	O
14.5.2	O
)	O
.	O
in	O
the	O
simplest	O
case	O
,	O
the	O
mixing	O
coefﬁ-	O
cients	O
are	O
independent	B
of	O
the	O
input	O
variables	O
.	O
if	O
we	O
make	O
a	O
further	O
generalization	B
to	O
allow	O
the	O
mixing	O
coefﬁcients	O
also	O
to	O
depend	O
on	O
the	O
inputs	O
then	O
we	O
obtain	O
a	O
mixture	B
of	I
experts	I
model	O
.	O
finally	O
,	O
if	O
we	O
allow	O
each	O
component	O
in	O
the	O
mixture	B
model	I
to	O
be	O
itself	O
a	O
mixture	B
of	I
experts	I
model	O
,	O
then	O
we	O
obtain	O
a	O
hierarchical	B
mixture	I
of	I
experts	I
.	O
14.5.1	O
mixtures	O
of	O
linear	B
regression	I
models	O
one	O
of	O
the	O
many	O
advantages	O
of	O
giving	O
a	O
probabilistic	O
interpretation	O
to	O
the	O
lin-	O
ear	O
regression	B
model	O
is	O
that	O
it	O
can	O
then	O
be	O
used	O
as	O
a	O
component	O
in	O
more	O
complex	O
probabilistic	O
models	O
.	O
this	O
can	O
be	O
done	O
,	O
for	O
instance	O
,	O
by	O
viewing	O
the	O
conditional	B
distribution	O
representing	O
the	O
linear	B
regression	I
model	O
as	O
a	O
node	B
in	O
a	O
directed	B
prob-	O
abilistic	O
graph	O
.	O
here	O
we	O
consider	O
a	O
simple	O
example	O
corresponding	O
to	O
a	O
mixture	O
of	O
linear	O
regression	B
models	O
,	O
which	O
represents	O
a	O
straightforward	O
extension	O
of	O
the	O
gaus-	O
sian	O
mixture	B
model	I
discussed	O
in	O
section	O
9.2	O
to	O
the	O
case	O
of	O
conditional	B
gaussian	O
distributions	O
.	O
we	O
therefore	O
consider	O
k	O
linear	B
regression	I
models	O
,	O
each	O
governed	O
by	O
its	O
own	O
weight	B
parameter	I
wk	O
.	O
in	O
many	O
applications	O
,	O
it	O
will	O
be	O
appropriate	O
to	O
use	O
a	O
common	O
noise	O
variance	B
,	O
governed	O
by	O
a	O
precision	B
parameter	I
β	O
,	O
for	O
all	O
k	O
components	O
,	O
and	O
this	O
is	O
the	O
case	O
we	O
consider	O
here	O
.	O
we	O
will	O
once	O
again	O
restrict	O
attention	O
to	O
a	O
single	O
target	O
variable	O
t	O
,	O
though	O
the	O
extension	O
to	O
multiple	O
outputs	O
is	O
straightforward	O
.	O
if	O
we	O
denote	O
the	O
mixing	O
coefﬁcients	O
by	O
πk	O
,	O
then	O
the	O
mixture	B
distribution	I
can	O
be	O
written	O
p	O
(	O
t|θ	O
)	O
=	O
πkn	O
(	O
t|wt	O
k	O
φ	O
,	O
β	O
−1	O
)	O
(	O
14.34	O
)	O
where	O
θ	O
denotes	O
the	O
set	O
of	O
all	O
adaptive	O
parameters	O
in	O
the	O
model	O
,	O
namely	O
w	O
=	O
{	O
wk	O
}	O
,	O
(	O
cid:22	O
)	O
π	O
=	O
{	O
πk	O
}	O
,	O
and	O
β.	O
the	O
log	O
likelihood	O
function	O
for	O
this	O
model	O
,	O
given	O
a	O
data	O
set	O
of	O
k	O
(	O
cid:2	O
)	O
observations	O
{	O
φn	O
,	O
tn	O
}	O
,	O
then	O
takes	O
the	O
form	O
n	O
(	O
cid:2	O
)	O
(	O
cid:23	O
)	O
k=1	O
ln	O
p	O
(	O
t|θ	O
)	O
=	O
ln	O
πkn	O
(	O
tn|wt	O
k	O
φn	O
,	O
β	O
−1	O
)	O
(	O
14.35	O
)	O
k	O
(	O
cid:2	O
)	O
where	O
t	O
=	O
(	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
)	O
t	O
denotes	O
the	O
vector	O
of	O
target	O
variables	O
.	O
n=1	O
k=1	O
in	O
order	O
to	O
maximize	O
this	O
likelihood	B
function	I
,	O
we	O
can	O
once	O
again	O
appeal	O
to	O
the	O
em	O
algorithm	O
,	O
which	O
will	O
turn	O
out	O
to	O
be	O
a	O
simple	O
extension	O
of	O
the	O
em	O
algorithm	O
for	O
unconditional	O
gaussian	O
mixtures	O
of	O
section	O
9.2.	O
we	O
can	O
therefore	O
build	O
on	O
our	O
expe-	O
rience	O
with	O
the	O
unconditional	O
mixture	B
and	O
introduce	O
a	O
set	O
z	O
=	O
{	O
zn	O
}	O
of	O
binary	O
latent	O
variables	O
where	O
znk	O
∈	O
{	O
0	O
,	O
1	O
}	O
in	O
which	O
,	O
for	O
each	O
data	O
point	O
n	O
,	O
all	O
of	O
the	O
elements	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
are	O
zero	O
except	O
for	O
a	O
single	O
value	O
of	O
1	O
indicating	O
which	O
component	O
of	O
the	O
mixture	B
was	O
responsible	O
for	O
generating	O
that	O
data	O
point	O
.	O
the	O
joint	O
distribution	O
over	O
latent	O
and	O
observed	O
variables	O
can	O
be	O
represented	O
by	O
the	O
graphical	B
model	I
shown	O
in	O
figure	O
14.7.	O
the	O
complete-data	O
log	O
likelihood	O
function	O
then	O
takes	O
the	O
form	O
ln	O
p	O
(	O
t	O
,	O
z|θ	O
)	O
=	O
znk	O
ln	O
πkn	O
(	O
tn|wt	O
k	O
φn	O
,	O
β	O
−1	O
)	O
.	O
(	O
14.36	O
)	O
n	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
n=1	O
k=1	O
(	O
cid:26	O
)	O
(	O
cid:27	O
)	O
exercise	O
14.12	O
exercise	O
14.13	O
668	O
14.	O
combining	B
models	I
figure	O
14.7	O
probabilistic	O
directed	O
graph	O
representing	O
a	O
mixture	O
of	O
linear	O
regression	B
models	O
,	O
deﬁned	O
by	O
(	O
14.35	O
)	O
.	O
π	O
β	O
w	O
zn	O
φn	O
tn	O
n	O
the	O
em	O
algorithm	O
begins	O
by	O
ﬁrst	O
choosing	O
an	O
initial	O
value	O
θold	O
for	O
the	O
model	O
param-	O
eters	O
.	O
in	O
the	O
e	O
step	O
,	O
these	O
parameter	O
values	O
are	O
then	O
used	O
to	O
evaluate	O
the	O
posterior	O
probabilities	O
,	O
or	O
responsibilities	O
,	O
of	O
each	O
component	O
k	O
for	O
every	O
data	O
point	O
n	O
given	O
by	O
(	O
cid:5	O
)	O
γnk	O
=	O
e	O
[	O
znk	O
]	O
=	O
p	O
(	O
k|φn	O
,	O
θold	O
)	O
=	O
πkn	O
(	O
tn|wt	O
j	O
πjn	O
(	O
tn|wt	O
k	O
φn	O
,	O
β	O
j	O
φn	O
,	O
β−1	O
)	O
.	O
(	O
14.37	O
)	O
−1	O
)	O
exercise	O
14.14	O
k	O
(	O
cid:2	O
)	O
the	O
responsibilities	O
are	O
then	O
used	O
to	O
determine	O
the	O
expectation	B
,	O
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
p	O
(	O
z|t	O
,	O
θold	O
)	O
,	O
of	O
the	O
complete-data	O
log	O
likelihood	O
,	O
which	O
takes	O
n	O
(	O
cid:2	O
)	O
(	O
cid:27	O
)	O
the	O
form	O
q	O
(	O
θ	O
,	O
θold	O
)	O
=	O
ez	O
[	O
ln	O
p	O
(	O
t	O
,	O
z|θ	O
)	O
]	O
=	O
(	O
cid:5	O
)	O
in	O
the	O
m	O
step	O
,	O
we	O
maximize	O
the	O
function	O
q	O
(	O
θ	O
,	O
θold	O
)	O
with	O
respect	O
to	O
θ	O
,	O
keeping	O
the	O
γnk	O
ﬁxed	O
.	O
for	O
the	O
optimization	O
with	O
respect	O
to	O
the	O
mixing	O
coefﬁcients	O
πk	O
we	O
need	O
k	O
πk	O
=	O
1	O
,	O
which	O
can	O
be	O
done	O
with	O
the	O
aid	O
of	O
a	O
to	O
take	O
account	O
of	O
the	O
constraint	O
lagrange	O
multiplier	O
,	O
leading	O
to	O
an	O
m-step	O
re-estimation	O
equation	O
for	O
πk	O
in	O
the	O
form	O
ln	O
πk	O
+	O
lnn	O
(	O
tn|wt	O
k	O
φn	O
,	O
β	O
−1	O
)	O
(	O
cid:26	O
)	O
n=1	O
k=1	O
γnk	O
.	O
n	O
(	O
cid:2	O
)	O
πk	O
=	O
1	O
n	O
γnk	O
.	O
n=1	O
(	O
14.38	O
)	O
note	O
that	O
this	O
has	O
exactly	O
the	O
same	O
form	O
as	O
the	O
corresponding	O
result	O
for	O
a	O
simple	O
mixture	O
of	O
unconditional	O
gaussians	O
given	O
by	O
(	O
9.22	O
)	O
.	O
next	O
consider	O
the	O
maximization	O
with	O
respect	O
to	O
the	O
parameter	O
vector	O
wk	O
of	O
the	O
kth	O
linear	B
regression	I
model	O
.	O
substituting	O
for	O
the	O
gaussian	O
distribution	O
,	O
we	O
see	O
that	O
the	O
function	O
q	O
(	O
θ	O
,	O
θold	O
)	O
,	O
as	O
a	O
function	O
of	O
the	O
parameter	O
vector	O
wk	O
,	O
takes	O
the	O
form	O
(	O
cid:12	O
)	O
n	O
(	O
cid:2	O
)	O
γnk	O
n=1	O
(	O
cid:10	O
)	O
(	O
cid:13	O
)	O
(	O
cid:11	O
)	O
2	O
q	O
(	O
θ	O
,	O
θold	O
)	O
=	O
−	O
β	O
2	O
tn	O
−	O
wt	O
k	O
φn	O
+	O
const	O
(	O
14.39	O
)	O
where	O
the	O
constant	O
term	O
includes	O
the	O
contributions	O
from	O
other	O
weight	O
vectors	O
wj	O
for	O
j	O
(	O
cid:9	O
)	O
=	O
k.	O
note	O
that	O
the	O
quantity	O
we	O
are	O
maximizing	O
is	O
similar	O
to	O
the	O
(	O
negative	O
of	O
the	O
)	O
standard	O
sum-of-squares	O
error	B
(	O
3.12	O
)	O
for	O
a	O
single	O
linear	B
regression	I
model	O
,	O
but	O
with	O
the	O
inclusion	O
of	O
the	O
responsibilities	O
γnk	O
.	O
this	O
represents	O
a	O
weighted	B
least	I
squares	I
14.5.	O
conditional	O
mixture	O
models	O
669	O
problem	O
,	O
in	O
which	O
the	O
term	O
corresponding	O
to	O
the	O
nth	O
data	O
point	O
carries	O
a	O
weighting	O
coefﬁcient	O
given	O
by	O
βγnk	O
,	O
which	O
could	O
be	O
interpreted	O
as	O
an	O
effective	O
precision	O
for	O
each	O
data	O
point	O
.	O
we	O
see	O
that	O
each	O
component	O
linear	B
regression	I
model	O
in	O
the	O
mixture	B
,	O
governed	O
by	O
its	O
own	O
parameter	O
vector	O
wk	O
,	O
is	O
ﬁtted	O
separately	O
to	O
the	O
whole	O
data	O
set	O
in	O
the	O
m	O
step	O
,	O
but	O
with	O
each	O
data	O
point	O
n	O
weighted	O
by	O
the	O
responsibility	B
γnk	O
that	O
model	O
k	O
takes	O
for	O
that	O
data	O
point	O
.	O
setting	O
the	O
derivative	B
of	O
(	O
14.39	O
)	O
with	O
respect	O
to	O
wk	O
equal	O
to	O
zero	O
gives	O
n	O
(	O
cid:2	O
)	O
(	O
cid:10	O
)	O
0	O
=	O
γnk	O
n=1	O
(	O
cid:11	O
)	O
tn	O
−	O
wt	O
k	O
φn	O
φn	O
(	O
14.40	O
)	O
which	O
we	O
can	O
write	O
in	O
matrix	O
notation	O
as	O
(	O
14.41	O
)	O
where	O
rk	O
=	O
diag	O
(	O
γnk	O
)	O
is	O
a	O
diagonal	B
matrix	O
of	O
size	O
n	O
×	O
n.	O
solving	O
for	O
wk	O
,	O
we	O
obtain	O
0	O
=	O
φtrk	O
(	O
t	O
−	O
φwk	O
)	O
(	O
cid:10	O
)	O
(	O
14.42	O
)	O
this	O
represents	O
a	O
set	O
of	O
modiﬁed	O
normal	B
equations	I
corresponding	O
to	O
the	O
weighted	B
least	I
squares	I
problem	O
,	O
of	O
the	O
same	O
form	O
as	O
(	O
4.99	O
)	O
found	O
in	O
the	O
context	O
of	O
logistic	B
regression	I
.	O
note	O
that	O
after	O
each	O
e	O
step	O
,	O
the	O
matrix	O
rk	O
will	O
change	O
and	O
so	O
we	O
will	O
have	O
to	O
solve	O
the	O
normal	B
equations	I
afresh	O
in	O
the	O
subsequent	O
m	O
step	O
.	O
finally	O
,	O
we	O
maximize	O
q	O
(	O
θ	O
,	O
θold	O
)	O
with	O
respect	O
to	O
β.	O
keeping	O
only	O
terms	O
that	O
(	O
cid:11	O
)	O
−1	O
φtrkt	O
.	O
wk	O
=	O
φtrkφ	O
depend	O
on	O
β	O
,	O
the	O
function	O
q	O
(	O
θ	O
,	O
θold	O
)	O
can	O
be	O
written	O
(	O
cid:10	O
)	O
(	O
cid:13	O
)	O
(	O
cid:11	O
)	O
2	O
q	O
(	O
θ	O
,	O
θold	O
)	O
=	O
1	O
2	O
ln	O
β	O
−	O
β	O
2	O
tn	O
−	O
wt	O
k	O
φn	O
.	O
(	O
14.43	O
)	O
(	O
cid:12	O
)	O
n	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
γnk	O
n=1	O
k=1	O
setting	O
the	O
derivative	B
with	O
respect	O
to	O
β	O
equal	O
to	O
zero	O
,	O
and	O
rearranging	O
,	O
we	O
obtain	O
the	O
m-step	O
equation	O
for	O
β	O
in	O
the	O
form	O
n	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
(	O
cid:10	O
)	O
1	O
β	O
=	O
1	O
n	O
γnk	O
n=1	O
k=1	O
tn	O
−	O
wt	O
k	O
φn	O
(	O
cid:11	O
)	O
2	O
.	O
(	O
14.44	O
)	O
in	O
figure	O
14.8	O
,	O
we	O
illustrate	O
this	O
em	O
algorithm	O
using	O
the	O
simple	O
example	O
of	O
ﬁtting	O
a	O
mixture	O
of	O
two	O
straight	O
lines	O
to	O
a	O
data	O
set	O
having	O
one	O
input	O
variable	O
x	O
and	O
one	O
target	O
variable	O
t.	O
the	O
predictive	O
density	O
(	O
14.34	O
)	O
is	O
plotted	O
in	O
figure	O
14.9	O
using	O
the	O
converged	O
parameter	O
values	O
obtained	O
from	O
the	O
em	O
algorithm	O
,	O
corresponding	O
to	O
the	O
right-hand	O
plot	O
in	O
figure	O
14.8.	O
also	O
shown	O
in	O
this	O
ﬁgure	O
is	O
the	O
result	O
of	O
ﬁtting	O
a	O
single	O
linear	B
regression	I
model	O
,	O
which	O
gives	O
a	O
unimodal	O
predictive	O
density	O
.	O
we	O
see	O
that	O
the	O
mixture	B
model	I
gives	O
a	O
much	O
better	O
representation	O
of	O
the	O
data	O
distribution	O
,	O
and	O
this	O
is	O
reﬂected	O
in	O
the	O
higher	O
likelihood	O
value	O
.	O
however	O
,	O
the	O
mixture	B
model	I
also	O
assigns	O
signiﬁcant	O
probability	B
mass	O
to	O
regions	O
where	O
there	O
is	O
no	O
data	O
because	O
its	O
predictive	B
distribution	I
is	O
bimodal	O
for	O
all	O
values	O
of	O
x.	O
this	O
problem	O
can	O
be	O
resolved	O
by	O
extending	O
the	O
model	O
to	O
allow	O
the	O
mixture	B
coefﬁcients	O
themselves	O
to	O
be	O
functions	O
of	O
x	O
,	O
leading	O
to	O
models	O
such	O
as	O
the	O
mixture	O
density	O
networks	O
discussed	O
in	O
section	O
5.6	O
,	O
and	O
hierarchical	B
mixture	I
of	I
experts	I
discussed	O
in	O
section	O
14.5.3	O
.	O
670	O
14.	O
combining	B
models	I
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
−1	O
−0.5	O
0	O
0.5	O
1	O
−1	O
−0.5	O
0	O
0.5	O
1	O
−1	O
−0.5	O
0	O
0.5	O
1	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−1	O
−0.5	O
0	O
0.5	O
1	O
0	O
−1	O
−0.5	O
0	O
0.5	O
1	O
0	O
−1	O
−0.5	O
0	O
0.5	O
1	O
figure	O
14.8	O
example	O
of	O
a	O
synthetic	O
data	O
set	O
,	O
shown	O
by	O
the	O
green	O
points	O
,	O
having	O
one	O
input	O
variable	O
x	O
and	O
one	O
target	O
variable	O
t	O
,	O
together	O
with	O
a	O
mixture	O
of	O
two	O
linear	B
regression	I
models	O
whose	O
mean	B
functions	O
y	O
(	O
x	O
,	O
wk	O
)	O
,	O
where	O
k	O
∈	O
{	O
1	O
,	O
2	O
}	O
,	O
are	O
shown	O
by	O
the	O
blue	O
and	O
red	O
lines	O
.	O
the	O
upper	O
three	O
plots	O
show	O
the	O
initial	O
conﬁguration	O
(	O
left	O
)	O
,	O
the	O
result	O
of	O
running	O
30	O
iterations	O
of	O
em	O
(	O
centre	O
)	O
,	O
and	O
the	O
result	O
after	O
50	O
iterations	O
of	O
em	O
(	O
right	O
)	O
.	O
here	O
β	O
was	O
initialized	O
to	O
the	O
reciprocal	O
of	O
the	O
true	O
variance	B
of	O
the	O
set	O
of	O
target	O
values	O
.	O
the	O
lower	O
three	O
plots	O
show	O
the	O
corresponding	O
responsibilities	O
plotted	O
as	O
a	O
vertical	O
line	O
for	O
each	O
data	O
point	O
in	O
which	O
the	O
length	O
of	O
the	O
blue	O
segment	O
gives	O
the	O
posterior	B
probability	I
of	O
the	O
blue	O
line	O
for	O
that	O
data	O
point	O
(	O
and	O
similarly	O
for	O
the	O
red	O
segment	O
)	O
.	O
14.5.2	O
mixtures	O
of	O
logistic	O
models	O
because	O
the	O
logistic	B
regression	I
model	O
deﬁnes	O
a	O
conditional	B
distribution	O
for	O
the	O
target	O
variable	O
,	O
given	O
the	O
input	O
vector	O
,	O
it	O
is	O
straightforward	O
to	O
use	O
it	O
as	O
the	O
component	O
distribution	O
in	O
a	O
mixture	B
model	I
,	O
thereby	O
giving	O
rise	O
to	O
a	O
richer	O
family	O
of	O
conditional	B
distributions	O
compared	O
to	O
a	O
single	O
logistic	B
regression	I
model	O
.	O
this	O
example	O
involves	O
a	O
straightforward	O
combination	O
of	O
ideas	O
encountered	O
in	O
earlier	O
sections	O
of	O
the	O
book	O
and	O
will	O
help	O
consolidate	O
these	O
for	O
the	O
reader	O
.	O
the	O
conditional	B
distribution	O
of	O
the	O
target	O
variable	O
,	O
for	O
a	O
probabilistic	O
mixture	O
of	O
k	O
logistic	B
regression	I
models	O
,	O
is	O
given	O
by	O
k	O
(	O
cid:2	O
)	O
(	O
cid:10	O
)	O
k=1	O
p	O
(	O
t|φ	O
,	O
θ	O
)	O
=	O
πkyt	O
k	O
[	O
1	O
−	O
yk	O
]	O
1−t	O
(	O
cid:11	O
)	O
where	O
φ	O
is	O
the	O
feature	O
vector	O
,	O
yk	O
=	O
σ	O
denotes	O
the	O
adjustable	O
parameters	O
namely	O
{	O
πk	O
}	O
and	O
{	O
wk	O
}	O
.	O
is	O
the	O
output	O
of	O
component	O
k	O
,	O
and	O
θ	O
now	O
suppose	O
we	O
are	O
given	O
a	O
data	O
set	O
{	O
φn	O
,	O
tn	O
}	O
.	O
the	O
corresponding	O
likelihood	O
k	O
φ	O
wt	O
(	O
14.45	O
)	O
14.5.	O
conditional	O
mixture	O
models	O
671	O
figure	O
14.9	O
the	O
left	O
plot	O
shows	O
the	O
predictive	O
conditional	O
density	B
corresponding	O
to	O
the	O
converged	O
solution	O
in	O
figure	O
14.8.	O
this	O
gives	O
a	O
log	O
likelihood	O
value	O
of	O
−3.0	O
.	O
a	O
vertical	O
slice	O
through	O
one	O
of	O
these	O
plots	O
at	O
a	O
particular	O
value	O
of	O
x	O
represents	O
the	O
corresponding	O
conditional	B
distribution	O
p	O
(	O
t|x	O
)	O
,	O
which	O
we	O
see	O
is	O
bimodal	O
.	O
the	O
plot	O
on	O
the	O
right	O
shows	O
the	O
predictive	O
density	O
for	O
a	O
single	O
linear	B
regression	I
model	O
ﬁtted	O
to	O
the	O
same	O
data	O
set	O
using	O
maximum	B
likelihood	I
.	O
this	O
model	O
has	O
a	O
smaller	O
log	O
likelihood	O
of	O
−27.6	O
.	O
function	O
is	O
then	O
given	O
by	O
p	O
(	O
t|θ	O
)	O
=	O
n	O
(	O
cid:14	O
)	O
(	O
cid:22	O
)	O
k	O
(	O
cid:2	O
)	O
πkytn	O
nk	O
[	O
1	O
−	O
ynk	O
]	O
1−tn	O
(	O
14.46	O
)	O
(	O
cid:23	O
)	O
(	O
cid:27	O
)	O
znk	O
n=1	O
k=1	O
n	O
(	O
cid:14	O
)	O
k	O
(	O
cid:14	O
)	O
(	O
cid:26	O
)	O
where	O
ynk	O
=	O
σ	O
(	O
wt	O
k	O
φn	O
)	O
and	O
t	O
=	O
(	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
)	O
t.	O
we	O
can	O
maximize	O
this	O
likelihood	B
function	I
iteratively	O
by	O
making	O
use	O
of	O
the	O
em	O
algorithm	O
.	O
this	O
involves	O
introducing	O
latent	O
variables	O
znk	O
that	O
correspond	O
to	O
a	O
1-of-k	O
coded	O
binary	O
indicator	O
variable	O
for	O
each	O
data	O
point	O
n.	O
the	O
complete-data	O
likelihood	B
function	I
is	O
then	O
given	O
by	O
p	O
(	O
t	O
,	O
z|θ	O
)	O
=	O
πkytn	O
nk	O
[	O
1	O
−	O
ynk	O
]	O
1−tn	O
(	O
14.47	O
)	O
n=1	O
k=1	O
where	O
z	O
is	O
the	O
matrix	O
of	O
latent	O
variables	O
with	O
elements	O
znk	O
.	O
we	O
initialize	O
the	O
em	O
algorithm	O
by	O
choosing	O
an	O
initial	O
value	O
θold	O
for	O
the	O
model	O
parameters	O
.	O
in	O
the	O
e	O
step	O
,	O
we	O
then	O
use	O
these	O
parameter	O
values	O
to	O
evaluate	O
the	O
posterior	O
probabilities	O
of	O
the	O
com-	O
ponents	O
k	O
for	O
each	O
data	O
point	O
n	O
,	O
which	O
are	O
given	O
by	O
γnk	O
=	O
e	O
[	O
znk	O
]	O
=	O
p	O
(	O
k|φn	O
,	O
θold	O
)	O
=	O
πkytn	O
j	O
πjytn	O
nk	O
[	O
1	O
−	O
ynk	O
]	O
1−tn	O
nj	O
[	O
1	O
−	O
ynj	O
]	O
1−tn	O
(	O
cid:5	O
)	O
(	O
14.48	O
)	O
.	O
these	O
responsibilities	O
are	O
then	O
used	O
to	O
ﬁnd	O
the	O
expected	O
complete-data	O
log	O
likelihood	O
as	O
a	O
function	O
of	O
θ	O
,	O
given	O
by	O
q	O
(	O
θ	O
,	O
θold	O
)	O
=	O
ez	O
[	O
ln	O
p	O
(	O
t	O
,	O
z|θ	O
)	O
]	O
n	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
n=1	O
k=1	O
=	O
γnk	O
{	O
ln	O
πk	O
+	O
tn	O
ln	O
ynk	O
+	O
(	O
1	O
−	O
tn	O
)	O
ln	O
(	O
1	O
−	O
ynk	O
)	O
}	O
.	O
(	O
14.49	O
)	O
672	O
14.	O
combining	B
models	I
(	O
cid:5	O
)	O
n	O
(	O
cid:2	O
)	O
1	O
n	O
the	O
m	O
step	O
involves	O
maximization	O
of	O
this	O
function	O
with	O
respect	O
to	O
θ	O
,	O
keeping	O
θold	O
,	O
and	O
hence	O
γnk	O
,	O
ﬁxed	O
.	O
maximization	O
with	O
respect	O
to	O
πk	O
can	O
be	O
done	O
in	O
the	O
usual	O
way	O
,	O
k	O
πk	O
=	O
1	O
,	O
giving	O
with	O
a	O
lagrange	O
multiplier	O
to	O
enforce	O
the	O
summation	O
constraint	O
the	O
familiar	O
result	O
πk	O
=	O
n=1	O
γnk	O
.	O
(	O
14.50	O
)	O
to	O
determine	O
the	O
{	O
wk	O
}	O
,	O
we	O
note	O
that	O
the	O
q	O
(	O
θ	O
,	O
θold	O
)	O
function	O
comprises	O
a	O
sum	O
over	O
terms	O
indexed	O
by	O
k	O
each	O
of	O
which	O
depends	O
only	O
on	O
one	O
of	O
the	O
vectors	O
wk	O
,	O
so	O
that	O
the	O
different	O
vectors	O
are	O
decoupled	O
in	O
the	O
m	O
step	O
of	O
the	O
em	O
algorithm	O
.	O
in	O
other	O
words	O
,	O
the	O
different	O
components	O
interact	O
only	O
via	O
the	O
responsibilities	O
,	O
which	O
are	O
ﬁxed	O
during	O
the	O
m	O
step	O
.	O
note	O
that	O
the	O
m	O
step	O
does	O
not	O
have	O
a	O
closed-form	O
solution	O
and	O
must	O
be	O
solved	O
iteratively	O
using	O
,	O
for	O
instance	O
,	O
the	O
iterative	B
reweighted	I
least	I
squares	I
(	O
irls	O
)	O
algorithm	O
.	O
the	O
gradient	O
and	O
the	O
hessian	O
for	O
the	O
vector	O
wk	O
are	O
given	O
by	O
∇kq	O
=	O
γnk	O
(	O
tn	O
−	O
ynk	O
)	O
φn	O
n	O
(	O
cid:2	O
)	O
hk	O
=	O
−∇k∇kq	O
=	O
γnkynk	O
(	O
1	O
−	O
ynk	O
)	O
φnφt	O
n	O
(	O
14.51	O
)	O
(	O
14.52	O
)	O
n	O
(	O
cid:2	O
)	O
n=1	O
section	O
4.3.3	O
n=1	O
where	O
∇k	O
denotes	O
the	O
gradient	O
with	O
respect	O
to	O
wk	O
.	O
for	O
ﬁxed	O
γnk	O
,	O
these	O
are	O
indepen-	O
dent	O
of	O
{	O
wj	O
}	O
for	O
j	O
(	O
cid:9	O
)	O
=	O
k	O
and	O
so	O
we	O
can	O
solve	O
for	O
each	O
wk	O
separately	O
using	O
the	O
irls	O
algorithm	O
.	O
thus	O
the	O
m-step	O
equations	O
for	O
component	O
k	O
correspond	O
simply	O
to	O
ﬁtting	O
a	O
single	O
logistic	B
regression	I
model	O
to	O
a	O
weighted	O
data	O
set	O
in	O
which	O
data	O
point	O
n	O
carries	O
a	O
weight	O
γnk	O
.	O
figure	O
14.10	O
shows	O
an	O
example	O
of	O
the	O
mixture	O
of	O
logistic	O
regression	B
models	O
applied	O
to	O
a	O
simple	O
classiﬁcation	B
problem	O
.	O
the	O
extension	O
of	O
this	O
model	O
to	O
a	O
mixture	O
of	O
softmax	O
models	O
for	O
more	O
than	O
two	O
classes	O
is	O
straightforward	O
.	O
section	O
4.3.3	O
exercise	O
14.16	O
14.5.3	O
mixtures	O
of	O
experts	O
in	O
section	O
14.5.1	O
,	O
we	O
considered	O
a	O
mixture	O
of	O
linear	O
regression	B
models	O
,	O
and	O
in	O
section	O
14.5.2	O
we	O
discussed	O
the	O
analogous	O
mixture	O
of	O
linear	O
classiﬁers	O
.	O
although	O
these	O
simple	O
mixtures	O
extend	O
the	O
ﬂexibility	O
of	O
linear	O
models	O
to	O
include	O
more	O
com-	O
plex	O
(	O
e.g.	O
,	O
multimodal	O
)	O
predictive	O
distributions	O
,	O
they	O
are	O
still	O
very	O
limited	O
.	O
we	O
can	O
further	O
increase	O
the	O
capability	O
of	O
such	O
models	O
by	O
allowing	O
the	O
mixing	O
coefﬁcients	O
themselves	O
to	O
be	O
functions	O
of	O
the	O
input	O
variable	O
,	O
so	O
that	O
p	O
(	O
t|x	O
)	O
=	O
πk	O
(	O
x	O
)	O
pk	O
(	O
t|x	O
)	O
.	O
(	O
14.53	O
)	O
k=1	O
this	O
is	O
known	O
as	O
a	O
mixture	B
of	I
experts	I
model	O
(	O
jacobs	O
et	O
al.	O
,	O
1991	O
)	O
in	O
which	O
the	O
mix-	O
ing	O
coefﬁcients	O
πk	O
(	O
x	O
)	O
are	O
known	O
as	O
gating	O
functions	O
and	O
the	O
individual	O
component	O
densities	O
pk	O
(	O
t|x	O
)	O
are	O
called	O
experts	O
.	O
the	O
notion	O
behind	O
the	O
terminology	O
is	O
that	O
differ-	O
ent	O
components	O
can	O
model	O
the	O
distribution	O
in	O
different	O
regions	O
of	O
input	O
space	O
(	O
they	O
k	O
(	O
cid:2	O
)	O
14.5.	O
conditional	O
mixture	O
models	O
673	O
figure	O
14.10	O
illustration	O
of	O
a	O
mixture	O
of	O
logistic	O
regression	B
models	O
.	O
the	O
left	O
plot	O
shows	O
data	O
points	O
drawn	O
from	O
two	O
classes	O
denoted	O
red	O
and	O
blue	O
,	O
in	O
which	O
the	O
background	O
colour	O
(	O
which	O
varies	O
from	O
pure	O
red	O
to	O
pure	O
blue	O
)	O
denotes	O
the	O
true	O
probability	B
of	O
the	O
class	O
label	O
.	O
the	O
centre	O
plot	O
shows	O
the	O
result	O
of	O
ﬁtting	O
a	O
single	O
logistic	B
regression	I
model	O
using	O
maximum	B
likelihood	I
,	O
in	O
which	O
the	O
background	O
colour	O
denotes	O
the	O
corresponding	O
probability	B
of	O
the	O
class	O
label	O
.	O
because	O
the	O
colour	O
is	O
a	O
near-uniform	O
purple	O
,	O
we	O
see	O
that	O
the	O
model	O
assigns	O
a	O
probability	B
of	O
around	O
0.5	O
to	O
each	O
of	O
the	O
classes	O
over	O
most	O
of	O
input	O
space	O
.	O
the	O
right	O
plot	O
shows	O
the	O
result	O
of	O
ﬁtting	O
a	O
mixture	O
of	O
two	O
logistic	B
regression	I
models	O
,	O
which	O
now	O
gives	O
much	O
higher	O
probability	B
to	O
the	O
correct	O
labels	O
for	O
many	O
of	O
the	O
points	O
in	O
the	O
blue	O
class	O
.	O
(	O
cid:5	O
)	O
are	O
‘	O
experts	O
’	O
at	O
making	O
predictions	O
in	O
their	O
own	O
regions	O
)	O
,	O
and	O
the	O
gating	O
functions	O
determine	O
which	O
components	O
are	O
dominant	O
in	O
which	O
region	O
.	O
the	O
gating	O
functions	O
πk	O
(	O
x	O
)	O
must	O
satisfy	O
the	O
usual	O
constraints	O
for	O
mixing	O
co-	O
efﬁcients	O
,	O
namely	O
0	O
(	O
cid:1	O
)	O
πk	O
(	O
x	O
)	O
(	O
cid:1	O
)	O
1	O
and	O
k	O
πk	O
(	O
x	O
)	O
=	O
1.	O
they	O
can	O
therefore	O
be	O
represented	O
,	O
for	O
example	O
,	O
by	O
linear	O
softmax	O
models	O
of	O
the	O
form	O
(	O
4.104	O
)	O
and	O
(	O
4.105	O
)	O
.	O
if	O
the	O
experts	O
are	O
also	O
linear	O
(	O
regression	B
or	O
classiﬁcation	B
)	O
models	O
,	O
then	O
the	O
whole	O
model	O
can	O
be	O
ﬁtted	O
efﬁciently	O
using	O
the	O
em	O
algorithm	O
,	O
with	O
iterative	B
reweighted	I
least	I
squares	I
being	O
employed	O
in	O
the	O
m	O
step	O
(	O
jordan	O
and	O
jacobs	O
,	O
1994	O
)	O
.	O
such	O
a	O
model	O
still	O
has	O
signiﬁcant	O
limitations	O
due	O
to	O
the	O
use	O
of	O
linear	O
models	O
for	O
the	O
gating	O
and	O
expert	O
functions	O
.	O
a	O
much	O
more	O
ﬂexible	O
model	O
is	O
obtained	O
by	O
using	O
a	O
multilevel	O
gating	B
function	I
to	O
give	O
the	O
hierarchical	B
mixture	I
of	I
experts	I
,	O
or	O
hme	O
model	O
(	O
jordan	O
and	O
jacobs	O
,	O
1994	O
)	O
.	O
to	O
understand	O
the	O
structure	O
of	O
this	O
model	O
,	O
imagine	O
a	O
mixture	B
distribution	I
in	O
which	O
each	O
component	O
in	O
the	O
mixture	B
is	O
itself	O
a	O
mixture	B
distribution	I
.	O
for	O
simple	O
unconditional	O
mixtures	O
,	O
this	O
hierarchical	O
mixture	O
is	O
trivially	O
equivalent	O
to	O
a	O
single	O
ﬂat	O
mixture	B
distribution	I
.	O
however	O
,	O
when	O
the	O
mixing	O
coefﬁcients	O
are	O
input	O
dependent	O
,	O
this	O
hierarchical	B
model	O
becomes	O
nontrivial	O
.	O
the	O
hme	O
model	O
can	O
also	O
be	O
viewed	O
as	O
a	O
probabilistic	O
version	O
of	O
decision	O
trees	O
discussed	O
in	O
section	O
14.4	O
and	O
can	O
again	O
be	O
trained	O
efﬁciently	O
by	O
maximum	B
likelihood	I
using	O
an	O
em	O
algorithm	O
with	O
irls	O
in	O
the	O
m	O
step	O
.	O
a	O
bayesian	O
treatment	O
of	O
the	O
hme	O
has	O
been	O
given	O
by	O
bishop	O
and	O
svens´en	O
(	O
2003	O
)	O
based	O
on	O
variational	B
inference	I
.	O
we	O
shall	O
not	O
discuss	O
the	O
hme	O
in	O
detail	O
here	O
.	O
however	O
,	O
it	O
is	O
worth	O
pointing	O
out	O
the	O
close	O
connection	O
with	O
the	O
mixture	B
density	I
network	I
discussed	O
in	O
section	O
5.6.	O
the	O
principal	O
advantage	O
of	O
the	O
mixtures	O
of	O
experts	O
model	O
is	O
that	O
it	O
can	O
be	O
optimized	O
by	O
em	O
in	O
which	O
the	O
m	O
step	O
for	O
each	O
mixture	B
component	I
and	O
gating	O
model	O
involves	O
a	O
convex	O
optimization	O
(	O
although	O
the	O
overall	O
optimization	O
is	O
nonconvex	O
)	O
.	O
by	O
con-	O
trast	O
,	O
the	O
advantage	O
of	O
the	O
mixture	B
density	I
network	I
approach	O
is	O
that	O
the	O
component	O
exercise	O
14.17	O
section	O
4.3.3	O
674	O
14.	O
combining	B
models	I
exercises	O
densities	O
and	O
the	O
mixing	O
coefﬁcients	O
share	O
the	O
hidden	O
units	O
of	O
the	O
neural	B
network	I
.	O
furthermore	O
,	O
in	O
the	O
mixture	B
density	I
network	I
,	O
the	O
splits	O
of	O
the	O
input	O
space	O
are	O
further	O
relaxed	O
compared	O
to	O
the	O
hierarchical	B
mixture	I
of	I
experts	I
in	O
that	O
they	O
are	O
not	O
only	O
soft	B
,	O
and	O
not	O
constrained	O
to	O
be	O
axis	O
aligned	O
,	O
but	O
they	O
can	O
also	O
be	O
nonlinear	O
.	O
14.1	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
set	O
models	O
of	O
the	O
form	O
p	O
(	O
t|x	O
,	O
zh	O
,	O
θh	O
,	O
h	O
)	O
in	O
which	O
x	O
is	O
the	O
input	O
vector	O
,	O
t	O
is	O
the	O
target	B
vector	I
,	O
h	O
indexes	O
the	O
different	O
models	O
,	O
zh	O
is	O
a	O
latent	O
vari-	O
able	O
for	O
model	O
h	O
,	O
and	O
θh	O
is	O
the	O
set	O
of	O
parameters	O
for	O
model	O
h.	O
suppose	O
the	O
models	O
have	O
prior	B
probabilities	O
p	O
(	O
h	O
)	O
and	O
that	O
we	O
are	O
given	O
a	O
training	B
set	I
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
and	O
t	O
=	O
{	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
}	O
.	O
write	O
down	O
the	O
formulae	O
needed	O
to	O
evaluate	O
the	O
predic-	O
tive	O
distribution	O
p	O
(	O
t|x	O
,	O
x	O
,	O
t	O
)	O
in	O
which	O
the	O
latent	O
variables	O
and	O
the	O
model	O
index	O
are	O
marginalized	O
out	O
.	O
use	O
these	O
formulae	O
to	O
highlight	O
the	O
difference	O
between	O
bayesian	O
averaging	O
of	O
different	O
models	O
and	O
the	O
use	O
of	O
latent	O
variables	O
within	O
a	O
single	O
model	O
.	O
14.2	O
(	O
(	O
cid:12	O
)	O
)	O
the	O
expected	O
sum-of-squares	B
error	I
eav	O
for	O
a	O
simple	O
committee	B
model	O
can	O
be	O
deﬁned	O
by	O
(	O
14.10	O
)	O
,	O
and	O
the	O
expected	O
error	B
of	O
the	O
committee	B
itself	O
is	O
given	O
by	O
(	O
14.11	O
)	O
.	O
assuming	O
that	O
the	O
individual	O
errors	O
satisfy	O
(	O
14.12	O
)	O
and	O
(	O
14.13	O
)	O
,	O
derive	O
the	O
result	O
(	O
14.14	O
)	O
.	O
14.3	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
by	O
making	O
use	O
of	O
jensen	O
’	O
s	O
inequality	O
(	O
1.115	O
)	O
,	O
for	O
the	O
special	O
case	O
of	O
the	O
convex	B
function	I
f	O
(	O
x	O
)	O
=	O
x2	O
,	O
show	O
that	O
the	O
average	O
expected	O
sum-of-squares	B
error	I
eav	O
of	O
the	O
members	O
of	O
a	O
simple	O
committee	B
model	O
,	O
given	O
by	O
(	O
14.10	O
)	O
,	O
and	O
the	O
expected	O
error	B
ecom	O
of	O
the	O
committee	B
itself	O
,	O
given	O
by	O
(	O
14.11	O
)	O
,	O
satisfy	O
ecom	O
(	O
cid:1	O
)	O
eav	O
.	O
(	O
14.54	O
)	O
14.4	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
by	O
making	O
use	O
of	O
jensen	O
’	O
s	O
in	O
equality	O
(	O
1.115	O
)	O
,	O
show	O
that	O
the	O
result	O
(	O
14.54	O
)	O
derived	O
in	O
the	O
previous	O
exercise	O
hods	O
for	O
any	O
error	B
function	I
e	O
(	O
y	O
)	O
,	O
not	O
just	O
sum-of-	O
squares	O
,	O
provided	O
it	O
is	O
a	O
convex	B
function	I
of	O
y	O
.	O
14.5	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
committee	B
in	O
which	O
we	O
allow	O
unequal	O
weighting	O
of	O
the	O
constituent	O
models	O
,	O
so	O
that	O
ycom	O
(	O
x	O
)	O
=	O
m	O
(	O
cid:2	O
)	O
m	O
(	O
cid:2	O
)	O
m=1	O
αmym	O
(	O
x	O
)	O
.	O
(	O
14.55	O
)	O
in	O
order	O
to	O
ensure	O
that	O
the	O
predictions	O
ycom	O
(	O
x	O
)	O
remain	O
within	O
sensible	O
limits	O
,	O
sup-	O
pose	O
that	O
we	O
require	O
that	O
they	O
be	O
bounded	O
at	O
each	O
value	O
of	O
x	O
by	O
the	O
minimum	O
and	O
maximum	O
values	O
given	O
by	O
any	O
of	O
the	O
members	O
of	O
the	O
committee	B
,	O
so	O
that	O
m=1	O
ymin	O
(	O
x	O
)	O
(	O
cid:1	O
)	O
ycom	O
(	O
x	O
)	O
(	O
cid:1	O
)	O
ymax	O
(	O
x	O
)	O
.	O
(	O
14.56	O
)	O
show	O
that	O
a	O
necessary	O
and	O
sufﬁcient	O
condition	O
for	O
this	O
constraint	O
is	O
that	O
the	O
coefﬁ-	O
cients	O
αm	O
satisfy	O
αm	O
(	O
cid:2	O
)	O
0	O
,	O
αm	O
=	O
1	O
.	O
(	O
14.57	O
)	O
exercises	O
675	O
14.6	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
by	O
differentiating	O
the	O
error	B
function	I
(	O
14.23	O
)	O
with	O
respect	O
to	O
αm	O
,	O
show	O
that	O
the	O
parameters	O
αm	O
in	O
the	O
adaboost	O
algorithm	O
are	O
updated	O
using	O
(	O
14.17	O
)	O
in	O
which	O
m	O
is	O
deﬁned	O
by	O
(	O
14.16	O
)	O
.	O
14.7	O
(	O
(	O
cid:12	O
)	O
)	O
by	O
making	O
a	O
variational	B
minimization	O
of	O
the	O
expected	O
exponential	O
error	O
function	O
given	O
by	O
(	O
14.27	O
)	O
with	O
respect	O
to	O
all	O
possible	O
functions	O
y	O
(	O
x	O
)	O
,	O
show	O
that	O
the	O
minimizing	O
function	O
is	O
given	O
by	O
(	O
14.28	O
)	O
.	O
14.8	O
(	O
(	O
cid:12	O
)	O
)	O
show	O
that	O
the	O
exponential	O
error	O
function	O
(	O
14.20	O
)	O
,	O
which	O
is	O
minimized	O
by	O
the	O
adaboost	O
algorithm	O
,	O
does	O
not	O
correspond	O
to	O
the	O
log	O
likelihood	O
of	O
any	O
well-behaved	O
probabilistic	O
model	O
.	O
this	O
can	O
be	O
done	O
by	O
showing	O
that	O
the	O
corresponding	O
conditional	B
distribution	O
p	O
(	O
t|x	O
)	O
can	O
not	O
be	O
correctly	O
normalized	O
.	O
14.9	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
show	O
that	O
the	O
sequential	O
minimization	O
of	O
the	O
sum-of-squares	B
error	I
func-	O
tion	O
for	O
an	O
additive	O
model	O
of	O
the	O
form	O
(	O
14.21	O
)	O
in	O
the	O
style	O
of	O
boosting	B
simply	O
involves	O
ﬁtting	O
each	O
new	O
base	O
classiﬁer	O
to	O
the	O
residual	O
errors	O
tn−fm−1	O
(	O
xn	O
)	O
from	O
the	O
previous	O
model	O
.	O
14.10	O
(	O
(	O
cid:12	O
)	O
)	O
verify	O
that	O
if	O
we	O
minimize	O
the	O
sum-of-squares	B
error	I
between	O
a	O
set	O
of	O
training	B
values	O
{	O
tn	O
}	O
and	O
a	O
single	O
predictive	O
value	O
t	O
,	O
then	O
the	O
optimal	O
solution	O
for	O
t	O
is	O
given	O
by	O
the	O
mean	B
of	O
the	O
{	O
tn	O
}	O
.	O
14.11	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
consider	O
a	O
data	O
set	O
comprising	O
400	O
data	O
points	O
from	O
class	O
c1	O
and	O
400	O
data	O
points	O
from	O
class	O
c2	O
.	O
suppose	O
that	O
a	O
tree	B
model	O
a	O
splits	O
these	O
into	O
(	O
300	O
,	O
100	O
)	O
at	O
the	O
ﬁrst	O
leaf	O
node	B
and	O
(	O
100	O
,	O
300	O
)	O
at	O
the	O
second	O
leaf	O
node	B
,	O
where	O
(	O
n	O
,	O
m	O
)	O
denotes	O
that	O
n	O
points	O
are	O
assigned	O
to	O
c1	O
and	O
m	O
points	O
are	O
assigned	O
to	O
c2	O
.	O
similarly	O
,	O
suppose	O
that	O
a	O
second	O
tree	O
model	O
b	O
splits	O
them	O
into	O
(	O
200	O
,	O
400	O
)	O
and	O
(	O
200	O
,	O
0	O
)	O
.	O
evaluate	O
the	O
misclassiﬁcation	O
rates	O
for	O
the	O
two	O
trees	O
and	O
hence	O
show	O
that	O
they	O
are	O
equal	O
.	O
similarly	O
,	O
evaluate	O
the	O
cross-entropy	O
(	O
14.32	O
)	O
and	O
gini	O
index	O
(	O
14.33	O
)	O
for	O
the	O
two	O
trees	O
and	O
show	O
that	O
they	O
are	O
both	O
lower	O
for	O
tree	B
b	O
than	O
for	O
tree	O
a	O
.	O
14.12	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
extend	O
the	O
results	O
of	O
section	O
14.5.1	O
for	O
a	O
mixture	O
of	O
linear	O
regression	B
models	O
to	O
the	O
case	O
of	O
multiple	O
target	O
values	O
described	O
by	O
a	O
vector	O
t.	O
to	O
do	O
this	O
,	O
make	O
use	O
of	O
the	O
results	O
of	O
section	O
3.1.5	O
.	O
14.13	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
verify	O
that	O
the	O
complete-data	O
log	O
likelihood	O
function	O
for	O
the	O
mixture	O
of	O
linear	O
regression	B
models	O
is	O
given	O
by	O
(	O
14.36	O
)	O
.	O
14.14	O
(	O
(	O
cid:12	O
)	O
)	O
use	O
the	O
technique	O
of	O
lagrange	O
multipliers	O
(	O
appendix	O
e	O
)	O
to	O
show	O
that	O
the	O
m-step	O
re-estimation	O
equation	O
for	O
the	O
mixing	O
coefﬁcients	O
in	O
the	O
mixture	O
of	O
linear	O
regression	B
models	O
trained	O
by	O
maximum	B
likelihood	I
em	O
is	O
given	O
by	O
(	O
14.38	O
)	O
.	O
14.15	O
(	O
(	O
cid:12	O
)	O
)	O
www	O
we	O
have	O
already	O
noted	O
that	O
if	O
we	O
use	O
a	O
squared	O
loss	B
function	I
in	O
a	O
regres-	O
sion	B
problem	O
,	O
the	O
corresponding	O
optimal	O
prediction	O
of	O
the	O
target	O
variable	O
for	O
a	O
new	O
input	O
vector	O
is	O
given	O
by	O
the	O
conditional	B
mean	O
of	O
the	O
predictive	B
distribution	I
.	O
show	O
that	O
the	O
conditional	B
mean	O
for	O
the	O
mixture	O
of	O
linear	O
regression	B
models	O
discussed	O
in	O
section	O
14.5.1	O
is	O
given	O
by	O
a	O
linear	O
combination	O
of	O
the	O
means	O
of	O
each	O
component	O
dis-	O
tribution	O
.	O
note	O
that	O
if	O
the	O
conditional	B
distribution	O
of	O
the	O
target	O
data	O
is	O
multimodal	O
,	O
the	O
conditional	B
mean	O
can	O
give	O
poor	O
predictions	O
.	O
676	O
14.	O
combining	B
models	I
14.16	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
extend	O
the	O
logistic	B
regression	I
mixture	O
model	O
of	O
section	O
14.5.2	O
to	O
a	O
mixture	O
of	O
softmax	O
classiﬁers	O
representing	O
c	O
(	O
cid:2	O
)	O
2	O
classes	O
.	O
write	O
down	O
the	O
em	O
algorithm	O
for	O
determining	O
the	O
parameters	O
of	O
this	O
model	O
through	O
maximum	B
likelihood	I
.	O
14.17	O
(	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
)	O
www	O
consider	O
a	O
mixture	B
model	I
for	O
a	O
conditional	B
distribution	O
p	O
(	O
t|x	O
)	O
of	O
the	O
form	O
k	O
(	O
cid:2	O
)	O
p	O
(	O
t|x	O
)	O
=	O
πkψk	O
(	O
t|x	O
)	O
k=1	O
(	O
14.58	O
)	O
in	O
which	O
each	O
mixture	B
component	I
ψk	O
(	O
t|x	O
)	O
is	O
itself	O
a	O
mixture	B
model	I
.	O
show	O
that	O
this	O
two-level	O
hierarchical	O
mixture	O
is	O
equivalent	O
to	O
a	O
conventional	O
single-level	O
mixture	B
model	I
.	O
now	O
suppose	O
that	O
the	O
mixing	O
coefﬁcients	O
in	O
both	O
levels	O
of	O
such	O
a	O
hierar-	O
chical	O
model	O
are	O
arbitrary	O
functions	O
of	O
x.	O
again	O
,	O
show	O
that	O
this	O
hierarchical	B
model	O
is	O
again	O
equivalent	O
to	O
a	O
single-level	O
model	O
with	O
x-dependent	O
mixing	O
coefﬁcients	O
.	O
finally	O
,	O
consider	O
the	O
case	O
in	O
which	O
the	O
mixing	O
coefﬁcients	O
at	O
both	O
levels	O
of	O
the	O
hi-	O
erarchical	O
mixture	B
are	O
constrained	O
to	O
be	O
linear	O
classiﬁcation	O
(	O
logistic	O
or	O
softmax	O
)	O
models	O
.	O
show	O
that	O
the	O
hierarchical	O
mixture	O
can	O
not	O
in	O
general	O
be	O
represented	O
by	O
a	O
single-level	O
mixture	B
having	O
linear	O
classiﬁcation	O
models	O
for	O
the	O
mixing	O
coefﬁcients	O
.	O
hint	O
:	O
to	O
do	O
this	O
it	O
is	O
sufﬁcient	O
to	O
construct	O
a	O
single	O
counter-example	O
,	O
so	O
consider	O
a	O
mixture	O
of	O
two	O
components	O
in	O
which	O
one	O
of	O
those	O
components	O
is	O
itself	O
a	O
mixture	O
of	O
two	O
components	O
,	O
with	O
mixing	O
coefﬁcients	O
given	O
by	O
linear-logistic	O
models	O
.	O
show	O
that	O
this	O
can	O
not	O
be	O
represented	O
by	O
a	O
single-level	O
mixture	O
of	O
3	O
components	O
having	O
mixing	O
coefﬁcients	O
determined	O
by	O
a	O
linear-softmax	O
model	O
.	O
appendix	O
a.	O
data	O
sets	O
in	O
this	O
appendix	O
,	O
we	O
give	O
a	O
brief	O
introduction	O
to	O
the	O
data	O
sets	O
used	O
to	O
illustrate	O
some	O
of	O
the	O
algorithms	O
described	O
in	O
this	O
book	O
.	O
detailed	O
information	O
on	O
ﬁle	O
formats	O
for	O
these	O
data	O
sets	O
,	O
as	O
well	O
as	O
the	O
data	O
ﬁles	O
themselves	O
,	O
can	O
be	O
obtained	O
from	O
the	O
book	O
web	O
site	O
:	O
http	O
:	O
//research.microsoft.com/∼cmbishop/prml	O
handwritten	O
digits	O
the	O
digits	O
data	O
used	O
in	O
this	O
book	O
is	O
taken	O
from	O
the	O
mnist	O
data	O
set	O
(	O
lecun	O
et	O
al.	O
,	O
1998	O
)	O
,	O
which	O
itself	O
was	O
constructed	O
by	O
modifying	O
a	O
subset	O
of	O
the	O
much	O
larger	O
data	O
set	O
produced	O
by	O
nist	O
(	O
the	O
national	O
institute	O
of	O
standards	O
and	O
technology	O
)	O
.	O
it	O
com-	O
prises	O
a	O
training	B
set	I
of	O
60	O
,	O
000	O
examples	O
and	O
a	O
test	B
set	I
of	O
10	O
,	O
000	O
examples	O
.	O
some	O
of	O
the	O
data	O
was	O
collected	O
from	O
census	O
bureau	O
employees	O
and	O
the	O
rest	O
was	O
collected	O
from	O
high-school	O
children	O
,	O
and	O
care	O
was	O
taken	O
to	O
ensure	O
that	O
the	O
test	O
examples	O
were	O
written	O
by	O
different	O
individuals	O
to	O
the	O
training	B
examples	O
.	O
the	O
original	O
nist	O
data	O
had	O
binary	O
(	O
black	O
or	O
white	O
)	O
pixels	O
.	O
to	O
create	O
mnist	O
,	O
these	O
images	O
were	O
size	O
normalized	O
to	O
ﬁt	O
in	O
a	O
20×20	O
pixel	O
box	O
while	O
preserving	O
their	O
aspect	O
ratio	O
.	O
as	O
a	O
consequence	O
of	O
the	O
anti-aliasing	O
used	O
to	O
change	O
the	O
resolution	O
of	O
the	O
images	O
,	O
the	O
resulting	O
mnist	O
digits	O
are	O
grey	O
scale	O
.	O
these	O
images	O
were	O
then	O
centred	O
in	O
a	O
28	O
×	O
28	O
box	O
.	O
examples	O
of	O
the	O
mnist	O
digits	O
are	O
shown	O
in	O
figure	O
a.1	O
.	O
error	B
rates	O
for	O
classifying	O
the	O
digits	O
range	O
from	O
12	O
%	O
for	O
a	O
simple	O
linear	O
classi-	O
ﬁer	O
,	O
through	O
0.56	O
%	O
for	O
a	O
carefully	O
designed	O
support	B
vector	I
machine	I
,	O
to	O
0.4	O
%	O
for	O
a	O
convolutional	B
neural	I
network	I
(	O
lecun	O
et	O
al.	O
,	O
1998	O
)	O
.	O
677	O
678	O
a.	O
data	O
sets	O
figure	O
a.1	O
one	O
hundred	O
examples	O
of	O
the	O
mnist	O
digits	O
chosen	O
at	O
ran-	O
dom	O
from	O
the	O
training	B
set	I
.	O
oil	O
flow	O
this	O
is	O
a	O
synthetic	O
data	O
set	O
that	O
arose	O
out	O
of	O
a	O
project	O
aimed	O
at	O
measuring	O
nonin-	O
vasively	O
the	O
proportions	O
of	O
oil	O
,	O
water	O
,	O
and	O
gas	O
in	O
north	O
sea	O
oil	O
transfer	O
pipelines	O
(	O
bishop	O
and	O
james	O
,	O
1993	O
)	O
.	O
it	O
is	O
based	O
on	O
the	O
principle	O
of	O
dual-energy	O
gamma	O
densit-	O
ometry	O
.	O
the	O
ideas	O
is	O
that	O
if	O
a	O
narrow	O
beam	O
of	O
gamma	O
rays	O
is	O
passed	O
through	O
the	O
pipe	O
,	O
the	O
attenuation	O
in	O
the	O
intensity	O
of	O
the	O
beam	O
provides	O
information	O
about	O
the	O
density	B
of	O
material	O
along	O
its	O
path	O
.	O
thus	O
,	O
for	O
instance	O
,	O
the	O
beam	O
will	O
be	O
attenuated	O
more	O
strongly	O
by	O
oil	O
than	O
by	O
gas	O
.	O
a	O
single	O
attenuation	O
measurement	O
alone	O
is	O
not	O
sufﬁcient	O
because	O
there	O
are	O
two	O
degrees	B
of	I
freedom	I
corresponding	O
to	O
the	O
fraction	O
of	O
oil	O
and	O
the	O
fraction	O
of	O
water	O
(	O
the	O
fraction	O
of	O
gas	O
is	O
redundant	O
because	O
the	O
three	O
fractions	O
must	O
add	O
to	O
one	O
)	O
.	O
to	O
address	O
this	O
,	O
two	O
gamma	O
beams	O
of	O
different	O
energies	O
(	O
in	O
other	O
words	O
different	O
frequencies	O
or	O
wavelengths	O
)	O
are	O
passed	O
through	O
the	O
pipe	O
along	O
the	O
same	O
path	O
,	O
and	O
the	O
attenuation	O
of	O
each	O
is	O
measured	O
.	O
because	O
the	O
absorbtion	O
properties	O
of	O
different	O
materials	O
vary	O
dif-	O
ferently	O
as	O
a	O
function	O
of	O
energy	O
,	O
measurement	O
of	O
the	O
attenuations	O
at	O
the	O
two	O
energies	O
provides	O
two	O
independent	B
pieces	O
of	O
information	O
.	O
given	O
the	O
known	O
absorbtion	O
prop-	O
erties	O
of	O
oil	O
,	O
water	O
,	O
and	O
gas	O
at	O
the	O
two	O
energies	O
,	O
it	O
is	O
then	O
a	O
simple	O
matter	O
to	O
calculate	O
the	O
average	O
fractions	O
of	O
oil	O
and	O
water	O
(	O
and	O
hence	O
of	O
gas	O
)	O
measured	O
along	O
the	O
path	O
of	O
the	O
gamma	O
beams	O
.	O
there	O
is	O
a	O
further	O
complication	O
,	O
however	O
,	O
associated	O
with	O
the	O
motion	O
of	O
the	O
ma-	O
terials	O
along	O
the	O
pipe	O
.	O
if	O
the	O
ﬂow	O
velocity	O
is	O
small	O
,	O
then	O
the	O
oil	O
ﬂoats	O
on	O
top	O
of	O
the	O
water	O
with	O
the	O
gas	O
sitting	O
above	O
the	O
oil	O
.	O
this	O
is	O
known	O
as	O
a	O
laminar	O
or	O
stratiﬁed	O
figure	O
a.2	O
the	O
three	O
geometrical	O
conﬁgurations	O
of	O
the	O
oil	O
,	O
water	O
,	O
and	O
gas	O
phases	O
used	O
to	O
generate	O
the	O
oil-	O
ﬂow	O
data	O
set	O
.	O
for	O
each	O
conﬁguration	O
,	O
the	O
pro-	O
portions	O
of	O
the	O
three	O
phases	O
can	O
vary	O
.	O
a.	O
data	O
sets	O
679	O
stratiﬁed	O
annular	O
oil	O
water	O
gas	O
mix	O
homogeneous	B
ﬂow	I
conﬁguration	O
and	O
is	O
illustrated	O
in	O
figure	O
a.2	O
.	O
as	O
the	O
ﬂow	O
velocity	O
is	O
increased	O
,	O
more	O
complex	O
geometrical	O
conﬁgurations	O
of	O
the	O
oil	O
,	O
water	O
,	O
and	O
gas	O
can	O
arise	O
.	O
for	O
the	O
purposes	O
of	O
this	O
data	O
set	O
,	O
two	O
speciﬁc	O
idealizations	O
are	O
considered	O
.	O
in	O
the	O
annular	O
conﬁguration	O
the	O
oil	O
,	O
water	O
,	O
and	O
gas	O
form	O
concentric	O
cylinders	O
with	O
the	O
water	O
around	O
the	O
outside	O
and	O
the	O
gas	O
in	O
the	O
centre	O
,	O
whereas	O
in	O
the	O
homogeneous	B
conﬁguration	O
the	O
oil	O
,	O
water	O
and	O
gas	O
are	O
assumed	O
to	O
be	O
intimately	O
mixed	O
as	O
might	O
occur	O
at	O
high	O
ﬂow	O
velocities	O
under	O
turbulent	O
conditions	O
.	O
these	O
conﬁgurations	O
are	O
also	O
illustrated	O
in	O
figure	O
a.2	O
.	O
we	O
have	O
seen	O
that	O
a	O
single	O
dual-energy	O
beam	O
gives	O
the	O
oil	O
and	O
water	O
fractions	O
measured	O
along	O
the	O
path	O
length	O
,	O
whereas	O
we	O
are	O
interested	O
in	O
the	O
volume	O
fractions	O
of	O
oil	O
and	O
water	O
.	O
this	O
can	O
be	O
addressed	O
by	O
using	O
multiple	O
dual-energy	O
gamma	O
densit-	O
ometers	O
whose	O
beams	O
pass	O
through	O
different	O
regions	O
of	O
the	O
pipe	O
.	O
for	O
this	O
particular	O
data	O
set	O
,	O
there	O
are	O
six	O
such	O
beams	O
,	O
and	O
their	O
spatial	O
arrangement	O
is	O
shown	O
in	O
fig-	O
ure	O
a.3	O
.	O
a	O
single	O
observation	O
is	O
therefore	O
represented	O
by	O
a	O
12-dimensional	O
vector	O
comprising	O
the	O
fractions	O
of	O
oil	O
and	O
water	O
measured	O
along	O
the	O
paths	O
of	O
each	O
of	O
the	O
beams	O
.	O
we	O
are	O
,	O
however	O
,	O
interested	O
in	O
obtaining	O
the	O
overall	O
volume	O
fractions	O
of	O
the	O
three	O
phases	O
in	O
the	O
pipe	O
.	O
this	O
is	O
much	O
like	O
the	O
classical	B
problem	O
of	O
tomographic	O
re-	O
construction	O
,	O
used	O
in	O
medical	O
imaging	O
for	O
example	O
,	O
in	O
which	O
a	O
two-dimensional	O
dis-	O
figure	O
a.3	O
cross	O
section	O
of	O
the	O
pipe	O
showing	O
the	O
arrangement	O
of	O
the	O
six	O
beam	O
lines	O
,	O
each	O
of	O
which	O
comprises	O
a	O
single	O
dual-	O
energy	O
gamma	O
densitometer	O
.	O
note	O
that	O
the	O
vertical	O
beams	O
are	O
asymmetrically	O
arranged	O
relative	B
to	O
the	O
central	O
axis	O
(	O
shown	O
by	O
the	O
dotted	O
line	O
)	O
.	O
680	O
a.	O
data	O
sets	O
tribution	O
is	O
to	O
be	O
reconstructed	O
from	O
an	O
number	O
of	O
one-dimensional	O
averages	O
.	O
here	O
there	O
are	O
far	O
fewer	O
line	O
measurements	O
than	O
in	O
a	O
typical	O
tomography	B
application	O
.	O
on	O
the	O
other	O
hand	O
the	O
range	O
of	O
geometrical	O
conﬁgurations	O
is	O
much	O
more	O
limited	O
,	O
and	O
so	O
the	O
conﬁguration	O
,	O
as	O
well	O
as	O
the	O
phase	O
fractions	O
,	O
can	O
be	O
predicted	O
with	O
reasonable	O
accuracy	O
from	O
the	O
densitometer	O
data	O
.	O
for	O
safety	O
reasons	O
,	O
the	O
intensity	O
of	O
the	O
gamma	O
beams	O
is	O
kept	O
relatively	O
weak	O
and	O
so	O
to	O
obtain	O
an	O
accurate	O
measurement	O
of	O
the	O
attenuation	O
,	O
the	O
measured	O
beam	O
intensity	O
is	O
integrated	O
over	O
a	O
speciﬁc	O
time	O
interval	O
.	O
for	O
a	O
ﬁnite	O
integration	O
time	O
,	O
there	O
are	O
random	O
ﬂuctuations	O
in	O
the	O
measured	O
intensity	O
due	O
to	O
the	O
fact	O
that	O
the	O
gamma	O
beams	O
comprise	O
discrete	O
packets	O
of	O
energy	O
called	O
photons	O
.	O
in	O
practice	O
,	O
the	O
integration	O
time	O
is	O
chosen	O
as	O
a	O
compromise	O
between	O
reducing	O
the	O
noise	O
level	O
(	O
which	O
requires	O
a	O
long	O
integration	O
time	O
)	O
and	O
detecting	O
temporal	O
variations	O
in	O
the	O
ﬂow	O
(	O
which	O
requires	O
a	O
short	O
integration	O
time	O
)	O
.	O
the	O
oil	B
ﬂow	I
data	I
set	O
is	O
generated	O
using	O
realistic	O
known	O
values	O
for	O
the	O
absorption	O
properties	O
of	O
oil	O
,	O
water	O
,	O
and	O
gas	O
at	O
the	O
two	O
gamma	O
energies	O
used	O
,	O
and	O
with	O
a	O
speciﬁc	O
choice	O
of	O
integration	O
time	O
(	O
10	O
seconds	O
)	O
chosen	O
as	O
characteristic	O
of	O
a	O
typical	O
practical	O
setup	O
.	O
each	O
point	O
in	O
the	O
data	O
set	O
is	O
generated	O
independently	O
using	O
the	O
following	O
steps	O
:	O
1.	O
choose	O
one	O
of	O
the	O
three	O
phase	O
conﬁgurations	O
at	O
random	O
with	O
equal	O
probability	B
.	O
2.	O
choose	O
three	O
random	O
numbers	O
f1	O
,	O
f2	O
and	O
f3	O
from	O
the	O
uniform	B
distribution	I
over	O
(	O
0	O
,	O
1	O
)	O
and	O
deﬁne	O
foil	O
=	O
f1	O
f1	O
+	O
f2	O
+	O
f3	O
,	O
fwater	O
=	O
f2	O
f1	O
+	O
f2	O
+	O
f3	O
.	O
(	O
a.1	O
)	O
this	O
treats	O
the	O
three	O
phases	O
on	O
an	O
equal	O
footing	O
and	O
ensures	O
that	O
the	O
volume	O
fractions	O
add	O
to	O
one	O
.	O
3.	O
for	O
each	O
of	O
the	O
six	O
beam	O
lines	O
,	O
calculate	O
the	O
effective	O
path	O
lengths	O
through	O
oil	O
and	O
water	O
for	O
the	O
given	O
phase	O
conﬁguration	O
.	O
4.	O
perturb	O
the	O
path	O
lengths	O
using	O
the	O
poisson	O
distribution	O
based	O
on	O
the	O
known	O
beam	O
intensities	O
and	O
integration	O
time	O
to	O
allow	O
for	O
the	O
effect	O
of	O
photon	O
statistics	O
.	O
each	O
point	O
in	O
the	O
data	O
set	O
comprises	O
the	O
12	O
path	O
length	O
measurements	O
,	O
together	O
with	O
the	O
fractions	O
of	O
oil	O
and	O
water	O
and	O
a	O
binary	O
label	O
describing	O
the	O
phase	O
conﬁgu-	O
ration	O
.	O
the	O
data	O
set	O
is	O
divided	O
into	O
training	B
,	O
validation	O
,	O
and	O
test	O
sets	O
,	O
each	O
of	O
which	O
comprises	O
1	O
,	O
000	O
independent	B
data	O
points	O
.	O
details	O
of	O
the	O
data	O
format	O
are	O
available	O
from	O
the	O
book	O
web	O
site	O
.	O
in	O
bishop	O
and	O
james	O
(	O
1993	O
)	O
,	O
statistical	O
machine	O
learning	B
techniques	O
were	O
used	O
to	O
predict	O
the	O
volume	O
fractions	O
and	O
also	O
the	O
geometrical	O
conﬁguration	O
of	O
the	O
phases	O
shown	O
in	O
figure	O
a.2	O
,	O
from	O
the	O
12-dimensional	O
vector	O
of	O
measurements	O
.	O
the	O
12-	O
dimensional	O
observation	O
vectors	O
can	O
also	O
be	O
used	O
to	O
test	O
data	O
visualization	B
algo-	O
rithms	O
.	O
this	O
data	O
set	O
has	O
a	O
rich	O
and	O
interesting	O
structure	O
,	O
as	O
follows	O
.	O
for	O
any	O
given	O
conﬁguration	O
there	O
are	O
two	O
degrees	B
of	I
freedom	I
corresponding	O
to	O
the	O
fractions	O
of	O
a.	O
data	O
sets	O
681	O
oil	O
and	O
water	O
,	O
and	O
so	O
for	O
inﬁnite	O
integration	O
time	O
the	O
data	O
will	O
locally	O
live	O
on	O
a	O
two-	O
dimensional	O
manifold	B
.	O
for	O
a	O
ﬁnite	O
integration	O
time	O
,	O
the	O
individual	O
data	O
points	O
will	O
be	O
perturbed	O
away	O
from	O
the	O
manifold	B
by	O
the	O
photon	B
noise	I
.	O
in	O
the	O
homogeneous	B
phase	O
conﬁguration	O
,	O
the	O
path	O
lengths	O
in	O
oil	O
and	O
water	O
are	O
linearly	O
related	O
to	O
the	O
fractions	O
of	O
oil	O
and	O
water	O
,	O
and	O
so	O
the	O
data	O
points	O
lie	O
close	O
to	O
a	O
linear	O
manifold	O
.	O
for	O
the	O
annular	O
conﬁguration	O
,	O
the	O
relationship	O
between	O
phase	O
fraction	O
and	O
path	O
length	O
is	O
nonlinear	O
and	O
so	O
the	O
manifold	B
will	O
be	O
nonlinear	O
.	O
in	O
the	O
case	O
of	O
the	O
laminar	O
conﬁguration	O
the	O
situation	O
is	O
even	O
more	O
complex	O
because	O
small	O
variations	O
in	O
the	O
phase	O
fractions	O
can	O
cause	O
one	O
of	O
the	O
horizontal	O
phase	O
boundaries	O
to	O
move	O
across	O
one	O
of	O
the	O
horizontal	O
beam	O
lines	O
leading	O
to	O
a	O
discontinuous	O
jump	O
in	O
the	O
12-dimensional	O
observation	O
space	O
.	O
in	O
this	O
way	O
,	O
the	O
two-dimensional	O
nonlinear	O
manifold	B
for	O
the	O
laminar	O
conﬁguration	O
is	O
broken	O
into	O
six	O
distinct	O
segments	O
.	O
note	O
also	O
that	O
some	O
of	O
the	O
manifolds	O
for	O
different	O
phase	O
conﬁgurations	O
meet	O
at	O
speciﬁc	O
points	O
,	O
for	O
example	O
if	O
the	O
pipe	O
is	O
ﬁlled	O
entirely	O
with	O
oil	O
,	O
it	O
corresponds	O
to	O
speciﬁc	O
instances	O
of	O
the	O
laminar	O
,	O
annular	O
,	O
and	O
homoge-	O
neous	O
conﬁgurations	O
.	O
old	O
faithful	O
old	O
faithful	O
,	O
shown	O
in	O
figure	O
a.4	O
,	O
is	O
a	O
hydrothermal	O
geyser	O
in	O
yellowstone	O
national	O
park	O
in	O
the	O
state	O
of	O
wyoming	O
,	O
u.s.a.	O
,	O
and	O
is	O
a	O
popular	O
tourist	O
attraction	O
.	O
its	O
name	O
stems	O
from	O
the	O
supposed	O
regularity	O
of	O
its	O
eruptions	O
.	O
the	O
data	O
set	O
comprises	O
272	O
observations	O
,	O
each	O
of	O
which	O
represents	O
a	O
single	O
erup-	O
tion	O
and	O
contains	O
two	O
variables	O
corresponding	O
to	O
the	O
duration	O
in	O
minutes	O
of	O
the	O
erup-	O
tion	O
,	O
and	O
the	O
time	O
until	O
the	O
next	O
eruption	O
,	O
also	O
in	O
minutes	O
.	O
figure	O
a.5	O
shows	O
a	O
plot	O
of	O
the	O
time	O
to	O
the	O
next	O
eruption	O
versus	O
the	O
duration	O
of	O
the	O
eruptions	O
.	O
it	O
can	O
be	O
seen	O
that	O
the	O
time	O
to	O
the	O
next	O
eruption	O
varies	O
considerably	O
,	O
although	O
knowledge	O
of	O
the	O
duration	O
of	O
the	O
current	O
eruption	O
allows	O
it	O
to	O
be	O
predicted	O
more	O
accurately	O
.	O
note	O
that	O
there	O
exist	O
several	O
other	O
data	O
sets	O
relating	O
to	O
the	O
eruptions	O
of	O
old	O
faithful	O
.	O
figure	O
a.4	O
the	O
old	O
faithful	O
geyser	O
national	O
c	O
(	O
cid:9	O
)	O
bruce	O
t.	O
gourley	O
in	O
park	O
.	O
www.brucegourley.com	O
.	O
yellowstone	O
682	O
a.	O
data	O
sets	O
figure	O
a.5	O
plot	O
of	O
the	O
time	O
to	O
the	O
next	O
eruption	O
in	O
minutes	O
(	O
vertical	O
axis	O
)	O
versus	O
the	O
duration	O
of	O
the	O
eruption	O
in	O
minutes	O
(	O
horizontal	O
axis	O
)	O
for	O
the	O
old	O
faithful	O
data	O
set	O
.	O
100	O
90	O
80	O
70	O
60	O
50	O
40	O
1	O
2	O
3	O
4	O
5	O
6	O
synthetic	O
data	O
throughout	O
the	O
book	O
,	O
we	O
use	O
two	O
simple	O
synthetic	B
data	I
sets	I
to	O
illustrate	O
many	O
of	O
the	O
algorithms	O
.	O
the	O
ﬁrst	O
of	O
these	O
is	O
a	O
regression	B
problem	O
,	O
based	O
on	O
the	O
sinusoidal	O
func-	O
tion	O
,	O
shown	O
in	O
figure	O
a.6	O
.	O
the	O
input	O
values	O
{	O
xn	O
}	O
are	O
generated	O
uniformly	O
in	O
range	O
(	O
0	O
,	O
1	O
)	O
,	O
and	O
the	O
corresponding	O
target	O
values	O
{	O
tn	O
}	O
are	O
obtained	O
by	O
ﬁrst	O
computing	O
the	O
corresponding	O
values	O
of	O
the	O
function	O
sin	O
(	O
2πx	O
)	O
,	O
and	O
then	O
adding	O
random	O
noise	O
with	O
a	O
gaussian	O
distribution	O
having	O
standard	B
deviation	I
0.3.	O
various	O
forms	O
of	O
this	O
data	O
set	O
,	O
having	O
different	O
numbers	O
of	O
data	O
points	O
,	O
are	O
used	O
in	O
the	O
book	O
.	O
the	O
second	O
data	O
set	O
is	O
a	O
classiﬁcation	B
problem	O
having	O
two	O
classes	O
,	O
with	O
equal	O
prior	B
probabilities	O
,	O
and	O
is	O
shown	O
in	O
figure	O
a.7	O
.	O
the	O
blue	O
class	O
is	O
generated	O
from	O
a	O
single	O
gaussian	O
while	O
the	O
red	O
class	O
comes	O
from	O
a	O
mixture	O
of	O
two	O
gaussians	O
.	O
be-	O
cause	O
we	O
know	O
the	O
class	O
priors	O
and	O
the	O
class-conditional	O
densities	O
,	O
it	O
is	O
straightfor-	O
ward	O
to	O
evaluate	O
and	O
plot	O
the	O
true	O
posterior	O
probabilities	O
as	O
well	O
as	O
the	O
minimum	O
misclassiﬁcation-rate	O
decision	B
boundary	I
,	O
as	O
shown	O
in	O
figure	O
a.7	O
.	O
a.	O
data	O
sets	O
683	O
t	O
1	O
0	O
−1	O
t	O
1	O
0	O
−1	O
0	O
x	O
1	O
0	O
x	O
1	O
figure	O
a.6	O
the	O
left-hand	O
plot	O
shows	O
the	O
synthetic	O
regression	O
data	O
set	O
along	O
with	O
the	O
underlying	O
sinusoidal	O
function	O
from	O
which	O
the	O
data	O
points	O
were	O
generated	O
.	O
the	O
right-hand	O
plot	O
shows	O
the	O
true	O
conditional	B
distribution	O
p	O
(	O
t|x	O
)	O
from	O
which	O
the	O
labels	O
are	O
generated	O
,	O
in	O
which	O
the	O
green	O
curve	O
denotes	O
the	O
mean	B
,	O
and	O
the	O
shaded	O
region	O
spans	O
one	O
standard	B
deviation	I
on	O
each	O
side	O
of	O
the	O
mean	B
.	O
2	O
0	O
−2	O
−2	O
0	O
2	O
figure	O
a.7	O
the	O
left	O
plot	O
shows	O
the	O
synthetic	O
classiﬁcation	O
data	O
set	O
with	O
data	O
from	O
the	O
two	O
classes	O
shown	O
in	O
red	O
and	O
blue	O
.	O
on	O
the	O
right	O
is	O
a	O
plot	O
of	O
the	O
true	O
posterior	O
probabilities	O
,	O
shown	O
on	O
a	O
colour	O
scale	O
going	O
from	O
pure	O
red	O
denoting	O
probability	B
of	O
the	O
red	O
class	O
is	O
1	O
to	O
pure	O
blue	O
denoting	O
probability	B
of	O
the	O
red	O
class	O
is	O
0.	O
because	O
these	O
probabilities	O
are	O
known	O
,	O
the	O
optimal	O
decision	B
boundary	I
for	O
minimizing	O
the	O
misclassiﬁcation	O
rate	O
(	O
which	O
corresponds	O
to	O
the	O
contour	O
along	O
which	O
the	O
posterior	O
probabilities	O
for	O
each	O
class	O
equal	O
0.5	O
)	O
can	O
be	O
evaluated	O
and	O
is	O
shown	O
by	O
the	O
green	O
curve	O
.	O
this	O
decision	B
boundary	I
is	O
also	O
plotted	O
on	O
the	O
left-hand	O
ﬁgure	O
.	O
appendix	O
b.	O
probability	B
distributions	O
in	O
this	O
appendix	O
,	O
we	O
summarize	O
the	O
main	O
properties	O
of	O
some	O
of	O
the	O
most	O
widely	O
used	O
probability	B
distributions	O
,	O
and	O
for	O
each	O
distribution	O
we	O
list	O
some	O
key	O
statistics	O
such	O
as	O
the	O
expectation	B
e	O
[	O
x	O
]	O
,	O
the	O
variance	B
(	O
or	O
covariance	B
)	O
,	O
the	O
mode	O
,	O
and	O
the	O
entropy	B
h	O
[	O
x	O
]	O
.	O
all	O
of	O
these	O
distributions	O
are	O
members	O
of	O
the	O
exponential	B
family	I
and	O
are	O
widely	O
used	O
as	O
building	O
blocks	O
for	O
more	O
sophisticated	O
probabilistic	O
models	O
.	O
bernoulli	O
this	O
is	O
the	O
distribution	O
for	O
a	O
single	O
binary	O
variable	O
x	O
∈	O
{	O
0	O
,	O
1	O
}	O
representing	O
,	O
for	O
example	O
,	O
the	O
result	O
of	O
ﬂipping	O
a	O
coin	O
.	O
it	O
is	O
governed	O
by	O
a	O
single	O
continuous	O
parameter	O
µ	O
∈	O
[	O
0	O
,	O
1	O
]	O
that	O
represents	O
the	O
probability	B
of	O
x	O
=	O
1.	O
bern	O
(	O
x|µ	O
)	O
=	O
µx	O
(	O
1	O
−	O
µ	O
)	O
1−x	O
(	O
b.1	O
)	O
(	O
b.2	O
)	O
(	O
b.3	O
)	O
(	O
b.4	O
)	O
(	O
b.5	O
)	O
e	O
[	O
x	O
]	O
=	O
µ	O
var	O
[	O
x	O
]	O
=	O
µ	O
(	O
1	O
−	O
µ	O
)	O
(	O
cid:12	O
)	O
h	O
[	O
x	O
]	O
=	O
−µ	O
ln	O
µ	O
−	O
(	O
1	O
−	O
µ	O
)	O
ln	O
(	O
1	O
−	O
µ	O
)	O
.	O
mode	O
[	O
x	O
]	O
=	O
1	O
if	O
µ	O
(	O
cid:2	O
)	O
0.5	O
,	O
0	O
otherwise	O
the	O
bernoulli	O
is	O
a	O
special	O
case	O
of	O
the	O
binomial	B
distribution	I
for	O
the	O
case	O
of	O
a	O
single	O
observation	O
.	O
its	O
conjugate	B
prior	I
for	O
µ	O
is	O
the	O
beta	B
distribution	I
.	O
685	O
686	O
b.	O
probability	B
distributions	O
beta	O
this	O
is	O
a	O
distribution	O
over	O
a	O
continuous	O
variable	O
µ	O
∈	O
[	O
0	O
,	O
1	O
]	O
,	O
which	O
is	O
often	O
used	O
to	O
represent	O
the	O
probability	B
for	O
some	O
binary	O
event	O
.	O
it	O
is	O
governed	O
by	O
two	O
parameters	O
a	O
and	O
b	O
that	O
are	O
constrained	O
by	O
a	O
>	O
0	O
and	O
b	O
>	O
0	O
to	O
ensure	O
that	O
the	O
distribution	O
can	O
be	O
normalized	O
.	O
beta	O
(	O
µ|a	O
,	O
b	O
)	O
=	O
γ	O
(	O
a	O
+	O
b	O
)	O
γ	O
(	O
a	O
)	O
γ	O
(	O
b	O
)	O
µa−1	O
(	O
1	O
−	O
µ	O
)	O
b−1	O
e	O
[	O
µ	O
]	O
=	O
var	O
[	O
µ	O
]	O
=	O
mode	O
[	O
µ	O
]	O
=	O
a	O
a	O
+	O
b	O
ab	O
(	O
a	O
+	O
b	O
)	O
2	O
(	O
a	O
+	O
b	O
+	O
1	O
)	O
a	O
−	O
1	O
a	O
+	O
b	O
−	O
2	O
.	O
(	O
b.6	O
)	O
(	O
b.7	O
)	O
(	O
b.8	O
)	O
(	O
b.9	O
)	O
the	O
beta	O
is	O
the	O
conjugate	B
prior	I
for	O
the	O
bernoulli	O
distribution	O
,	O
for	O
which	O
a	O
and	O
b	O
can	O
be	O
interpreted	O
as	O
the	O
effective	O
prior	O
number	O
of	O
observations	O
of	O
x	O
=	O
1	O
and	O
x	O
=	O
0	O
,	O
respectively	O
.	O
its	O
density	B
is	O
ﬁnite	O
if	O
a	O
(	O
cid:2	O
)	O
1	O
and	O
b	O
(	O
cid:2	O
)	O
1	O
,	O
otherwise	O
there	O
is	O
a	O
singularity	O
at	O
µ	O
=	O
0	O
and/or	O
µ	O
=	O
1.	O
for	O
a	O
=	O
b	O
=	O
1	O
,	O
it	O
reduces	O
to	O
a	O
uniform	B
distribution	I
.	O
the	O
beta	B
distribution	I
is	O
a	O
special	O
case	O
of	O
the	O
k-state	O
dirichlet	O
distribution	O
for	O
k	O
=	O
2.	O
binomial	O
the	O
binomial	B
distribution	I
gives	O
the	O
probability	B
of	O
observing	O
m	O
occurrences	O
of	O
x	O
=	O
1	O
in	O
a	O
set	O
of	O
n	O
samples	O
from	O
a	O
bernoulli	O
distribution	O
,	O
where	O
the	O
probability	B
of	O
observ-	O
ing	O
x	O
=	O
1	O
is	O
µ	O
∈	O
[	O
0	O
,	O
1	O
]	O
.	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
µm	O
(	O
1	O
−	O
µ	O
)	O
n−m	O
(	O
b.10	O
)	O
bin	O
(	O
m|n	O
,	O
µ	O
)	O
=	O
n	O
m	O
e	O
[	O
m	O
]	O
=	O
n	O
µ	O
var	O
[	O
m	O
]	O
=	O
n	O
µ	O
(	O
1	O
−	O
µ	O
)	O
mode	O
[	O
m	O
]	O
=	O
(	O
cid:24	O
)	O
(	O
n	O
+	O
1	O
)	O
µ	O
(	O
cid:25	O
)	O
(	O
b.11	O
)	O
(	O
b.12	O
)	O
(	O
b.13	O
)	O
where	O
(	O
cid:24	O
)	O
(	O
n	O
+	O
1	O
)	O
µ	O
(	O
cid:25	O
)	O
denotes	O
the	O
largest	O
integer	O
that	O
is	O
less	O
than	O
or	O
equal	O
to	O
(	O
n	O
+	O
1	O
)	O
µ	O
,	O
and	O
the	O
quantity	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
n	O
m	O
n	O
!	O
=	O
m	O
!	O
(	O
n	O
−	O
m	O
)	O
!	O
(	O
b.14	O
)	O
denotes	O
the	O
number	O
of	O
ways	O
of	O
choosing	O
m	O
objects	O
out	O
of	O
a	O
total	O
of	O
n	O
identical	O
objects	O
.	O
here	O
m	O
!	O
,	O
pronounced	O
‘	O
factorial	B
m	O
’	O
,	O
denotes	O
the	O
product	O
m	O
×	O
(	O
m	O
−	O
1	O
)	O
×	O
.	O
.	O
.	O
,	O
×2	O
×	O
1.	O
the	O
particular	O
case	O
of	O
the	O
binomial	B
distribution	I
for	O
n	O
=	O
1	O
is	O
known	O
as	O
the	O
bernoulli	O
distribution	O
,	O
and	O
for	O
large	O
n	O
the	O
binomial	B
distribution	I
is	O
approximately	O
gaussian	O
.	O
the	O
conjugate	B
prior	I
for	O
µ	O
is	O
the	O
beta	B
distribution	I
.	O
b.	O
probability	B
distributions	O
687	O
dirichlet	O
the	O
dirichlet	O
is	O
a	O
multivariate	O
distribution	O
over	O
k	O
random	O
variables	O
0	O
(	O
cid:1	O
)	O
µk	O
(	O
cid:1	O
)	O
1	O
,	O
where	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
,	O
subject	O
to	O
the	O
constraints	O
0	O
(	O
cid:1	O
)	O
µk	O
(	O
cid:1	O
)	O
1	O
,	O
µk	O
=	O
1	O
.	O
(	O
b.15	O
)	O
k	O
(	O
cid:2	O
)	O
k=1	O
denoting	O
µ	O
=	O
(	O
µ1	O
,	O
.	O
.	O
.	O
,	O
µk	O
)	O
t	O
and	O
α	O
=	O
(	O
α1	O
,	O
.	O
.	O
.	O
,	O
αk	O
)	O
t	O
,	O
we	O
have	O
k=1	O
dir	O
(	O
µ|α	O
)	O
=	O
c	O
(	O
α	O
)	O
µαk−1	O
k	O
e	O
[	O
µk	O
]	O
=	O
var	O
[	O
µk	O
]	O
=	O
k	O
(	O
cid:3	O
)	O
αk	O
(	O
cid:4	O
)	O
α	O
αk	O
(	O
(	O
cid:4	O
)	O
α	O
−	O
αk	O
)	O
(	O
cid:4	O
)	O
α2	O
(	O
(	O
cid:4	O
)	O
α	O
+	O
1	O
)	O
(	O
cid:4	O
)	O
α2	O
(	O
(	O
cid:4	O
)	O
α	O
+	O
1	O
)	O
(	O
cid:4	O
)	O
α	O
−	O
k	O
e	O
[	O
ln	O
µk	O
]	O
=	O
ψ	O
(	O
αk	O
)	O
−	O
ψ	O
(	O
(	O
cid:4	O
)	O
α	O
)	O
h	O
[	O
µ	O
]	O
=	O
−	O
k	O
(	O
cid:2	O
)	O
αk	O
−	O
1	O
cov	O
[	O
µjµk	O
]	O
=	O
−	O
αjαk	O
mode	O
[	O
µk	O
]	O
=	O
(	O
b.16	O
)	O
(	O
b.17	O
)	O
(	O
b.18	O
)	O
(	O
b.19	O
)	O
(	O
b.20	O
)	O
(	O
b.21	O
)	O
(	O
b.22	O
)	O
(	O
b.23	O
)	O
(	O
b.24	O
)	O
(	O
b.25	O
)	O
(	O
αk	O
−	O
1	O
)	O
{	O
ψ	O
(	O
αk	O
)	O
−	O
ψ	O
(	O
(	O
cid:4	O
)	O
α	O
)	O
}	O
−	O
ln	O
c	O
(	O
α	O
)	O
k=1	O
c	O
(	O
α	O
)	O
=	O
γ	O
(	O
(	O
cid:4	O
)	O
α	O
)	O
k	O
(	O
cid:2	O
)	O
αk	O
.	O
(	O
cid:4	O
)	O
α	O
=	O
γ	O
(	O
α1	O
)	O
···	O
γ	O
(	O
αk	O
)	O
k=1	O
ψ	O
(	O
a	O
)	O
≡	O
d	O
da	O
ln	O
γ	O
(	O
a	O
)	O
where	O
and	O
here	O
is	O
known	O
as	O
the	O
digamma	B
function	I
(	O
abramowitz	O
and	O
stegun	O
,	O
1965	O
)	O
.	O
the	O
parameters	O
αk	O
are	O
subject	O
to	O
the	O
constraint	O
αk	O
>	O
0	O
in	O
order	O
to	O
ensure	O
that	O
the	O
distribution	O
can	O
be	O
normalized	O
.	O
the	O
dirichlet	O
forms	O
the	O
conjugate	B
prior	I
for	O
the	O
multinomial	B
distribution	I
and	O
rep-	O
resents	O
a	O
generalization	B
of	O
the	O
beta	B
distribution	I
.	O
in	O
this	O
case	O
,	O
the	O
parameters	O
αk	O
can	O
be	O
interpreted	O
as	O
effective	O
numbers	O
of	O
observations	O
of	O
the	O
corresponding	O
values	O
of	O
the	O
k-dimensional	O
binary	O
observation	O
vector	O
x.	O
as	O
with	O
the	O
beta	B
distribution	I
,	O
the	O
dirichlet	O
has	O
ﬁnite	O
density	O
everywhere	O
provided	O
αk	O
(	O
cid:2	O
)	O
1	O
for	O
all	O
k.	O
688	O
b.	O
probability	B
distributions	O
gamma	O
the	O
gamma	O
is	O
a	O
probability	B
distribution	O
over	O
a	O
positive	O
random	O
variable	O
τ	O
>	O
0	O
governed	O
by	O
parameters	O
a	O
and	O
b	O
that	O
are	O
subject	O
to	O
the	O
constraints	O
a	O
>	O
0	O
and	O
b	O
>	O
0	O
to	O
ensure	O
that	O
the	O
distribution	O
can	O
be	O
normalized	O
.	O
gam	O
(	O
τ|a	O
,	O
b	O
)	O
=	O
1	O
γ	O
(	O
a	O
)	O
baτ	O
a−1e	O
−bτ	O
e	O
[	O
τ	O
]	O
=	O
a	O
b	O
var	O
[	O
τ	O
]	O
=	O
a	O
b2	O
mode	O
[	O
τ	O
]	O
=	O
a	O
−	O
1	O
e	O
[	O
ln	O
τ	O
]	O
=	O
ψ	O
(	O
a	O
)	O
−	O
ln	O
b	O
h	O
[	O
τ	O
]	O
=	O
ln	O
γ	O
(	O
a	O
)	O
−	O
(	O
a	O
−	O
1	O
)	O
ψ	O
(	O
a	O
)	O
−	O
ln	O
b	O
+	O
a	O
for	O
α	O
(	O
cid:2	O
)	O
1	O
b	O
(	O
b.26	O
)	O
(	O
b.27	O
)	O
(	O
b.28	O
)	O
(	O
b.29	O
)	O
(	O
b.30	O
)	O
(	O
b.31	O
)	O
where	O
ψ	O
(	O
·	O
)	O
is	O
the	O
digamma	B
function	I
deﬁned	O
by	O
(	O
b.25	O
)	O
.	O
the	O
gamma	B
distribution	I
is	O
the	O
conjugate	B
prior	I
for	O
the	O
precision	O
(	O
inverse	B
variance	O
)	O
of	O
a	O
univariate	O
gaussian	O
.	O
for	O
a	O
(	O
cid:2	O
)	O
1	O
the	O
density	B
is	O
everywhere	O
ﬁnite	O
,	O
and	O
the	O
special	O
case	O
of	O
a	O
=	O
1	O
is	O
known	O
as	O
the	O
exponential	B
distribution	I
.	O
gaussian	O
the	O
gaussian	O
is	O
the	O
most	O
widely	O
used	O
distribution	O
for	O
continuous	O
variables	O
.	O
it	O
is	O
also	O
known	O
as	O
the	O
normal	B
distribution	I
.	O
in	O
the	O
case	O
of	O
a	O
single	O
variable	O
x	O
∈	O
(	O
−∞	O
,	O
∞	O
)	O
it	O
is	O
governed	O
by	O
two	O
parameters	O
,	O
the	O
mean	B
µ	O
∈	O
(	O
−∞	O
,	O
∞	O
)	O
and	O
the	O
variance	B
σ2	O
>	O
0	O
.	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
n	O
(	O
x|µ	O
,	O
σ2	O
)	O
=	O
1	O
(	O
2πσ2	O
)	O
1/2	O
exp	O
−	O
1	O
2σ2	O
(	O
x	O
−	O
µ	O
)	O
2	O
e	O
[	O
x	O
]	O
=	O
µ	O
var	O
[	O
x	O
]	O
=	O
σ2	O
mode	O
[	O
x	O
]	O
=	O
µ	O
1	O
2	O
h	O
[	O
x	O
]	O
=	O
ln	O
σ2	O
+	O
1	O
2	O
(	O
1	O
+	O
ln	O
(	O
2π	O
)	O
)	O
.	O
(	O
b.32	O
)	O
(	O
b.33	O
)	O
(	O
b.34	O
)	O
(	O
b.35	O
)	O
(	O
b.36	O
)	O
the	O
inverse	B
of	O
the	O
variance	B
τ	O
=	O
1/σ2	O
is	O
called	O
the	O
precision	O
,	O
and	O
the	O
square	O
root	O
of	O
the	O
variance	B
σ	O
is	O
called	O
the	O
standard	B
deviation	I
.	O
the	O
conjugate	B
prior	I
for	O
µ	O
is	O
the	O
gaussian	O
,	O
and	O
the	O
conjugate	B
prior	I
for	O
τ	O
is	O
the	O
gamma	B
distribution	I
.	O
if	O
both	O
µ	O
and	O
τ	O
are	O
unknown	O
,	O
their	O
joint	O
conjugate	B
prior	I
is	O
the	O
gaussian-gamma	O
distribution	O
.	O
for	O
a	O
d-dimensional	O
vector	O
x	O
,	O
the	O
gaussian	O
is	O
governed	O
by	O
a	O
d-dimensional	O
mean	B
vector	O
µ	O
and	O
a	O
d	O
×	O
d	O
covariance	B
matrix	I
σ	O
that	O
must	O
be	O
symmetric	O
and	O
b.	O
probability	B
distributions	O
689	O
positive-deﬁnite	O
.	O
n	O
(	O
x|µ	O
,	O
σ	O
)	O
=	O
1	O
(	O
2π	O
)	O
d/2	O
1	O
|σ|1/2	O
exp	O
(	O
cid:12	O
)	O
−1	O
2	O
(	O
cid:13	O
)	O
−1	O
(	O
x	O
−	O
µ	O
)	O
(	O
x	O
−	O
µ	O
)	O
tς	O
e	O
[	O
x	O
]	O
=	O
µ	O
cov	O
[	O
x	O
]	O
=	O
σ	O
mode	O
[	O
x	O
]	O
=	O
µ	O
1	O
2	O
h	O
[	O
x	O
]	O
=	O
ln|σ|	O
+	O
d	O
2	O
(	O
1	O
+	O
ln	O
(	O
2π	O
)	O
)	O
.	O
(	O
b.41	O
)	O
−1	O
is	O
the	O
precision	B
matrix	I
,	O
which	O
is	O
also	O
the	O
inverse	B
of	O
the	O
covariance	B
matrix	I
λ	O
=	O
σ	O
symmetric	O
and	O
positive	B
deﬁnite	I
.	O
averages	O
of	O
random	O
variables	O
tend	O
to	O
a	O
gaussian	O
,	O
by	O
the	O
central	B
limit	I
theorem	I
,	O
and	O
the	O
sum	O
of	O
two	O
gaussian	O
variables	O
is	O
again	O
gaussian	O
.	O
the	O
gaussian	O
is	O
the	O
distribution	O
that	O
maximizes	O
the	O
entropy	B
for	O
a	O
given	O
variance	B
(	O
or	O
covariance	B
)	O
.	O
any	O
linear	O
transformation	O
of	O
a	O
gaussian	O
random	O
variable	O
is	O
again	O
gaussian	O
.	O
the	O
marginal	B
distribution	O
of	O
a	O
multivariate	O
gaussian	O
with	O
respect	O
to	O
a	O
subset	O
of	O
the	O
variables	O
is	O
itself	O
gaussian	O
,	O
and	O
similarly	O
the	O
conditional	B
distribution	O
is	O
also	O
gaussian	O
.	O
the	O
conjugate	B
prior	I
for	O
µ	O
is	O
the	O
gaussian	O
,	O
the	O
conjugate	B
prior	I
for	O
λ	O
is	O
the	O
wishart	O
,	O
and	O
the	O
conjugate	B
prior	I
for	O
(	O
µ	O
,	O
λ	O
)	O
is	O
the	O
gaussian-wishart	O
.	O
if	O
we	O
have	O
a	O
marginal	B
gaussian	O
distribution	O
for	O
x	O
and	O
a	O
conditional	B
gaussian	O
distribution	O
for	O
y	O
given	O
x	O
in	O
the	O
form	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
x|µ	O
,	O
λ	O
p	O
(	O
y|x	O
)	O
=	O
n	O
(	O
y|ax	O
+	O
b	O
,	O
l−1	O
)	O
−1	O
)	O
(	O
b.37	O
)	O
(	O
b.38	O
)	O
(	O
b.39	O
)	O
(	O
b.40	O
)	O
(	O
b.42	O
)	O
(	O
b.43	O
)	O
(	O
b.44	O
)	O
(	O
b.45	O
)	O
then	O
the	O
marginal	B
distribution	O
of	O
y	O
,	O
and	O
the	O
conditional	B
distribution	O
of	O
x	O
given	O
y	O
,	O
are	O
given	O
by	O
p	O
(	O
y	O
)	O
=	O
n	O
(	O
y|aµ	O
+	O
b	O
,	O
l−1	O
+	O
aλ	O
−1at	O
)	O
p	O
(	O
x|y	O
)	O
=	O
n	O
(	O
x|σ	O
{	O
atl	O
(	O
y	O
−	O
b	O
)	O
+	O
λµ	O
}	O
,	O
σ	O
)	O
where	O
if	O
we	O
have	O
a	O
joint	O
gaussian	O
distribution	O
n	O
(	O
x|µ	O
,	O
σ	O
)	O
with	O
λ	O
≡	O
σ	O
(	O
b.46	O
)	O
−1	O
and	O
we	O
σ	O
=	O
(	O
λ	O
+	O
atla	O
)	O
−1	O
.	O
deﬁne	O
the	O
following	O
partitions	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
x	O
=	O
xa	O
xb	O
(	O
cid:15	O
)	O
σ	O
=	O
σaa	O
σab	O
σba	O
σbb	O
,	O
µ	O
=	O
,	O
λ	O
=	O
(	O
cid:16	O
)	O
(	O
cid:15	O
)	O
(	O
cid:15	O
)	O
µa	O
µb	O
λaa	O
λab	O
λba	O
λbb	O
then	O
the	O
conditional	B
distribution	O
p	O
(	O
xa|xb	O
)	O
is	O
given	O
by	O
−1	O
aa	O
)	O
p	O
(	O
xa|xb	O
)	O
=	O
n	O
(	O
x|µa|b	O
,	O
λ	O
µa|b	O
=	O
µa	O
−	O
λ	O
aa	O
λab	O
(	O
xb	O
−	O
µb	O
)	O
−1	O
(	O
cid:16	O
)	O
(	O
b.47	O
)	O
(	O
b.48	O
)	O
(	O
b.49	O
)	O
(	O
b.50	O
)	O
690	O
b.	O
probability	B
distributions	O
and	O
the	O
marginal	B
distribution	O
p	O
(	O
xa	O
)	O
is	O
given	O
by	O
p	O
(	O
xa	O
)	O
=	O
n	O
(	O
xa|µa	O
,	O
σaa	O
)	O
.	O
(	O
b.51	O
)	O
gaussian-gamma	O
this	O
is	O
the	O
conjugate	B
prior	I
distribution	O
for	O
a	O
univariate	O
gaussian	O
n	O
(	O
x|µ	O
,	O
λ	O
−1	O
)	O
in	O
which	O
the	O
mean	B
µ	O
and	O
the	O
precision	O
λ	O
are	O
both	O
unknown	O
and	O
is	O
also	O
called	O
the	O
normal-gamma	B
distribution	I
.	O
it	O
comprises	O
the	O
product	O
of	O
a	O
gaussian	O
distribution	O
for	O
µ	O
,	O
whose	O
precision	O
is	O
proportional	O
to	O
λ	O
,	O
and	O
a	O
gamma	B
distribution	I
over	O
λ.	O
p	O
(	O
µ	O
,	O
λ|µ0	O
,	O
β	O
,	O
a	O
,	O
b	O
)	O
=	O
n	O
(	O
cid:10	O
)	O
µ|µo	O
,	O
(	O
βλ	O
)	O
−1	O
gam	O
(	O
λ|a	O
,	O
b	O
)	O
.	O
(	O
b.52	O
)	O
(	O
cid:11	O
)	O
gaussian-wishart	O
this	O
is	O
the	O
conjugate	B
prior	I
distribution	O
for	O
a	O
multivariate	O
gaussian	O
n	O
(	O
x|µ	O
,	O
λ	O
)	O
in	O
which	O
both	O
the	O
mean	B
µ	O
and	O
the	O
precision	O
λ	O
are	O
unknown	O
,	O
and	O
is	O
also	O
called	O
the	O
normal-wishart	O
distribution	O
.	O
it	O
comprises	O
the	O
product	O
of	O
a	O
gaussian	O
distribution	O
for	O
µ	O
,	O
whose	O
precision	O
is	O
proportional	O
to	O
λ	O
,	O
and	O
a	O
wishart	O
distribution	O
over	O
λ.	O
p	O
(	O
µ	O
,	O
λ|µ0	O
,	O
β	O
,	O
w	O
,	O
ν	O
)	O
=	O
n	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
w	O
(	O
λ|w	O
,	O
ν	O
)	O
.	O
µ|µ0	O
,	O
(	O
βλ	O
)	O
−1	O
(	O
b.53	O
)	O
for	O
the	O
particular	O
case	O
of	O
a	O
scalar	O
x	O
,	O
this	O
is	O
equivalent	O
to	O
the	O
gaussian-gamma	O
distri-	O
bution	O
.	O
multinomial	O
if	O
we	O
generalize	O
the	O
bernoulli	O
distribution	O
to	O
an	O
k-dimensional	O
binary	O
variable	O
x	O
with	O
components	O
xk	O
∈	O
{	O
0	O
,	O
1	O
}	O
such	O
that	O
k	O
xk	O
=	O
1	O
,	O
then	O
we	O
obtain	O
the	O
following	O
discrete	O
distribution	O
(	O
cid:5	O
)	O
k	O
(	O
cid:14	O
)	O
p	O
(	O
x	O
)	O
=	O
µxk	O
k	O
k=1	O
e	O
[	O
xk	O
]	O
=	O
µk	O
var	O
[	O
xk	O
]	O
=	O
µk	O
(	O
1	O
−	O
µk	O
)	O
cov	O
[	O
xjxk	O
]	O
=	O
ijkµk	O
h	O
[	O
x	O
]	O
=	O
−	O
m	O
(	O
cid:2	O
)	O
µk	O
ln	O
µk	O
k=1	O
(	O
b.54	O
)	O
(	O
b.55	O
)	O
(	O
b.56	O
)	O
(	O
b.57	O
)	O
(	O
b.58	O
)	O
b.	O
probability	B
distributions	O
691	O
where	O
ijk	O
is	O
the	O
j	O
,	O
k	O
element	O
of	O
the	O
identity	O
matrix	O
.	O
because	O
p	O
(	O
xk	O
=	O
1	O
)	O
=	O
µk	O
,	O
the	O
parameters	O
must	O
satisfy	O
0	O
(	O
cid:1	O
)	O
µk	O
(	O
cid:1	O
)	O
1	O
and	O
k	O
µk	O
=	O
1.	O
the	O
multinomial	B
distribution	I
is	O
a	O
multivariate	O
generalization	B
of	O
the	O
binomial	O
and	O
gives	O
the	O
distribution	O
over	O
counts	O
mk	O
for	O
a	O
k-state	O
discrete	O
variable	O
to	O
be	O
in	O
state	O
k	O
given	O
a	O
total	O
number	O
of	O
observations	O
n.	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
m	O
(	O
cid:14	O
)	O
mult	O
(	O
m1	O
,	O
m2	O
,	O
.	O
.	O
.	O
,	O
mk|µ	O
,	O
n	O
)	O
=	O
n	O
m1m2	O
.	O
.	O
.	O
mm	O
k=1	O
(	O
cid:5	O
)	O
(	O
cid:16	O
)	O
e	O
[	O
mk	O
]	O
=	O
n	O
µk	O
var	O
[	O
mk	O
]	O
=	O
n	O
µk	O
(	O
1	O
−	O
µk	O
)	O
cov	O
[	O
mjmk	O
]	O
=	O
−n	O
µjµk	O
where	O
µ	O
=	O
(	O
µ1	O
,	O
.	O
.	O
.	O
,	O
µk	O
)	O
t	O
,	O
and	O
the	O
quantity	O
(	O
cid:15	O
)	O
n	O
m1m2	O
.	O
.	O
.	O
mk	O
=	O
n	O
!	O
m1	O
!	O
.	O
.	O
.	O
mk	O
!	O
µmk	O
k	O
(	O
b.59	O
)	O
(	O
b.60	O
)	O
(	O
b.61	O
)	O
(	O
b.62	O
)	O
(	O
b.63	O
)	O
(	O
cid:5	O
)	O
gives	O
the	O
number	O
of	O
ways	O
of	O
taking	O
n	O
identical	O
objects	O
and	O
assigning	O
mk	O
of	O
them	O
to	O
bin	O
k	O
for	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k.	O
the	O
value	O
of	O
µk	O
gives	O
the	O
probability	B
of	O
the	O
random	O
variable	O
taking	O
state	O
k	O
,	O
and	O
so	O
these	O
parameters	O
are	O
subject	O
to	O
the	O
constraints	O
0	O
(	O
cid:1	O
)	O
µk	O
(	O
cid:1	O
)	O
1	O
k	O
µk	O
=	O
1.	O
the	O
conjugate	B
prior	I
distribution	O
for	O
the	O
parameters	O
{	O
µk	O
}	O
is	O
the	O
and	O
dirichlet	O
.	O
normal	O
the	O
normal	B
distribution	I
is	O
simply	O
another	O
name	O
for	O
the	O
gaussian	O
.	O
in	O
this	O
book	O
,	O
we	O
use	O
the	O
term	O
gaussian	O
throughout	O
,	O
although	O
we	O
retain	O
the	O
conventional	O
use	O
of	O
the	O
symbol	O
n	O
to	O
denote	O
this	O
distribution	O
.	O
for	O
consistency	O
,	O
we	O
shall	O
refer	O
to	O
the	O
normal-	O
gamma	B
distribution	I
as	O
the	O
gaussian-gamma	O
distribution	O
,	O
and	O
similarly	O
the	O
normal-	O
wishart	O
is	O
called	O
the	O
gaussian-wishart	O
.	O
student	O
’	O
s	O
t	O
this	O
distribution	O
was	O
published	O
by	O
william	O
gosset	O
in	O
1908	O
,	O
but	O
his	O
employer	O
,	O
gui-	O
ness	O
breweries	O
,	O
required	O
him	O
to	O
publish	O
under	O
a	O
pseudonym	O
,	O
so	O
he	O
chose	O
‘	O
student	O
’	O
.	O
in	O
the	O
univariate	O
form	O
,	O
student	O
’	O
s	O
t-distribution	O
is	O
obtained	O
by	O
placing	O
a	O
conjugate	B
gamma	O
prior	B
over	O
the	O
precision	O
of	O
a	O
univariate	O
gaussian	O
distribution	O
and	O
then	O
inte-	O
grating	O
out	O
the	O
precision	O
variable	O
.	O
it	O
can	O
therefore	O
be	O
viewed	O
as	O
an	O
inﬁnite	O
mixture	B
692	O
b.	O
probability	B
distributions	O
of	O
gaussians	O
having	O
the	O
same	O
mean	B
but	O
different	O
variances	O
.	O
st	O
(	O
x|µ	O
,	O
λ	O
,	O
ν	O
)	O
=	O
e	O
[	O
x	O
]	O
=	O
µ	O
1	O
λ	O
mode	O
[	O
x	O
]	O
=	O
µ.	O
var	O
[	O
x	O
]	O
=	O
γ	O
(	O
ν/2	O
+	O
1/2	O
)	O
γ	O
(	O
ν/2	O
)	O
for	O
ν	O
>	O
1	O
ν	O
ν	O
−	O
2	O
for	O
ν	O
>	O
2	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
1/2	O
(	O
cid:29	O
)	O
λ	O
πν	O
1	O
+	O
λ	O
(	O
x	O
−	O
µ	O
)	O
2	O
ν	O
(	O
cid:30	O
)	O
−ν/2−1/2	O
(	O
b.64	O
)	O
(	O
b.65	O
)	O
(	O
b.66	O
)	O
(	O
b.67	O
)	O
here	O
ν	O
>	O
0	O
is	O
called	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
of	O
the	O
distribution	O
.	O
the	O
particular	O
case	O
of	O
ν	O
=	O
1	O
is	O
called	O
the	O
cauchy	O
distribution	O
.	O
for	O
a	O
d-dimensional	O
variable	O
x	O
,	O
student	O
’	O
s	O
t-distribution	O
corresponds	O
to	O
marginal-	O
izing	O
the	O
precision	B
matrix	I
of	O
a	O
multivariate	O
gaussian	O
with	O
respect	O
to	O
a	O
conjugate	B
wishart	O
prior	B
and	O
takes	O
the	O
form	O
(	O
cid:29	O
)	O
(	O
cid:30	O
)	O
−ν/2−d/2	O
st	O
(	O
x|µ	O
,	O
λ	O
,	O
ν	O
)	O
=	O
γ	O
(	O
ν/2	O
+	O
d/2	O
)	O
γ	O
(	O
ν/2	O
)	O
|λ|1/2	O
(	O
νπ	O
)	O
d/2	O
1	O
+	O
∆2	O
ν	O
e	O
[	O
x	O
]	O
=	O
µ	O
for	O
ν	O
>	O
1	O
cov	O
[	O
x	O
]	O
=	O
mode	O
[	O
x	O
]	O
=	O
µ	O
−1	O
λ	O
ν	O
ν	O
−	O
2	O
for	O
ν	O
>	O
2	O
where	O
∆2	O
is	O
the	O
squared	O
mahalanobis	O
distance	O
deﬁned	O
by	O
∆2	O
=	O
(	O
x	O
−	O
µ	O
)	O
tλ	O
(	O
x	O
−	O
µ	O
)	O
.	O
(	O
b.72	O
)	O
in	O
the	O
limit	O
ν	O
→	O
∞	O
,	O
the	O
t-distribution	O
reduces	O
to	O
a	O
gaussian	O
with	O
mean	B
µ	O
and	O
pre-	O
cision	O
λ.	O
student	O
’	O
s	O
t-distribution	O
provides	O
a	O
generalization	B
of	O
the	O
gaussian	O
whose	O
maximum	B
likelihood	I
parameter	O
values	O
are	O
robust	O
to	O
outliers	B
.	O
uniform	O
this	O
is	O
a	O
simple	O
distribution	O
for	O
a	O
continuous	O
variable	O
x	O
deﬁned	O
over	O
a	O
ﬁnite	O
interval	O
x	O
∈	O
[	O
a	O
,	O
b	O
]	O
where	O
b	O
>	O
a	O
.	O
(	O
b.68	O
)	O
(	O
b.69	O
)	O
(	O
b.70	O
)	O
(	O
b.71	O
)	O
(	O
b.73	O
)	O
(	O
b.74	O
)	O
(	O
b.75	O
)	O
u	O
(	O
x|a	O
,	O
b	O
)	O
=	O
e	O
[	O
x	O
]	O
=	O
1	O
b	O
−	O
a	O
(	O
b	O
+	O
a	O
)	O
(	O
b	O
−	O
a	O
)	O
2	O
var	O
[	O
x	O
]	O
=	O
h	O
[	O
x	O
]	O
=	O
ln	O
(	O
b	O
−	O
a	O
)	O
.	O
12	O
2	O
(	O
b.76	O
)	O
if	O
x	O
has	O
distribution	O
u	O
(	O
x|0	O
,	O
1	O
)	O
,	O
then	O
a	O
+	O
(	O
b	O
−	O
a	O
)	O
x	O
will	O
have	O
distribution	O
u	O
(	O
x|a	O
,	O
b	O
)	O
.	O
b.	O
probability	B
distributions	O
693	O
von	O
mises	O
the	O
von	O
mises	O
distribution	O
,	O
also	O
known	O
as	O
the	O
circular	B
normal	I
or	O
the	O
circular	O
gaus-	O
sian	O
,	O
is	O
a	O
univariate	O
gaussian-like	O
periodic	O
distribution	O
for	O
a	O
variable	O
θ	O
∈	O
[	O
0	O
,	O
2π	O
)	O
.	O
p	O
(	O
θ|θ0	O
,	O
m	O
)	O
=	O
1	O
2πi0	O
(	O
m	O
)	O
exp	O
{	O
m	O
cos	O
(	O
θ	O
−	O
θ0	O
)	O
}	O
(	O
b.77	O
)	O
where	O
i0	O
(	O
m	O
)	O
is	O
the	O
zeroth-order	O
bessel	O
function	O
of	O
the	O
ﬁrst	O
kind	O
.	O
the	O
distribution	O
has	O
period	O
2π	O
so	O
that	O
p	O
(	O
θ	O
+	O
2π	O
)	O
=	O
p	O
(	O
θ	O
)	O
for	O
all	O
θ.	O
care	O
must	O
be	O
taken	O
in	O
interpret-	O
ing	O
this	O
distribution	O
because	O
simple	O
expectations	O
will	O
be	O
dependent	O
on	O
the	O
(	O
arbitrary	O
)	O
choice	O
of	O
origin	O
for	O
the	O
variable	O
θ.	O
the	O
parameter	O
θ0	O
is	O
analogous	O
to	O
the	O
mean	B
of	O
a	O
univariate	O
gaussian	O
,	O
and	O
the	O
parameter	O
m	O
>	O
0	O
,	O
known	O
as	O
the	O
concentration	O
param-	O
eter	O
,	O
is	O
analogous	O
to	O
the	O
precision	O
(	O
inverse	B
variance	O
)	O
.	O
for	O
large	O
m	O
,	O
the	O
von	O
mises	O
distribution	O
is	O
approximately	O
a	O
gaussian	O
centred	O
on	O
θ0	O
.	O
wishart	O
the	O
wishart	O
distribution	O
is	O
the	O
conjugate	B
prior	I
for	O
the	O
precision	B
matrix	I
of	O
a	O
multi-	O
variate	O
gaussian	O
.	O
w	O
(	O
λ|w	O
,	O
ν	O
)	O
=	O
b	O
(	O
w	O
,	O
ν	O
)	O
|λ|	O
(	O
ν−d−1	O
)	O
/2	O
exp	O
where	O
(	O
cid:22	O
)	O
b	O
(	O
w	O
,	O
ν	O
)	O
≡	O
|w|−ν/2	O
2νd/2	O
πd	O
(	O
d−1	O
)	O
/4	O
γ	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
−1	O
2	O
tr	O
(	O
w−1λ	O
)	O
(	O
cid:16	O
)	O
(	O
cid:23	O
)	O
−1	O
(	O
cid:15	O
)	O
d	O
(	O
cid:14	O
)	O
ν	O
+	O
1	O
−	O
i	O
(	O
b.78	O
)	O
(	O
b.79	O
)	O
(	O
b.80	O
)	O
(	O
b.81	O
)	O
e	O
[	O
λ	O
]	O
=	O
νw	O
e	O
[	O
ln|λ|	O
]	O
=	O
(	O
cid:15	O
)	O
d	O
(	O
cid:2	O
)	O
i=1	O
ψ	O
ν	O
+	O
1	O
−	O
i	O
2	O
2	O
(	O
cid:16	O
)	O
i=1	O
+	O
d	O
ln	O
2	O
+	O
ln|w|	O
h	O
[	O
λ	O
]	O
=	O
−	O
ln	O
b	O
(	O
w	O
,	O
ν	O
)	O
−	O
(	O
ν	O
−	O
d	O
−	O
1	O
)	O
2	O
(	O
b.82	O
)	O
where	O
w	O
is	O
a	O
d	O
×	O
d	O
symmetric	O
,	O
positive	B
deﬁnite	I
matrix	I
,	O
and	O
ψ	O
(	O
·	O
)	O
is	O
the	O
digamma	B
function	I
deﬁned	O
by	O
(	O
b.25	O
)	O
.	O
the	O
parameter	O
ν	O
is	O
called	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
of	O
the	O
distribution	O
and	O
is	O
restricted	O
to	O
ν	O
>	O
d	O
−	O
1	O
to	O
ensure	O
that	O
the	O
gamma	B
function	I
in	O
the	O
normalization	O
factor	O
is	O
well-deﬁned	O
.	O
in	O
one	O
dimension	O
,	O
the	O
wishart	O
reduces	O
to	O
the	O
gamma	B
distribution	I
gam	O
(	O
λ|a	O
,	O
b	O
)	O
given	O
by	O
(	O
b.26	O
)	O
with	O
parameters	O
a	O
=	O
ν/2	O
and	O
b	O
=	O
1/2w	O
.	O
e	O
[	O
ln|λ|	O
]	O
+	O
νd	O
2	O
appendix	O
c.	O
properties	O
of	O
matrices	O
in	O
this	O
appendix	O
,	O
we	O
gather	O
together	O
some	O
useful	O
properties	O
and	O
identities	O
involving	O
matrices	O
and	O
determinants	O
.	O
this	O
is	O
not	O
intended	O
to	O
be	O
an	O
introductory	O
tutorial	O
,	O
and	O
it	O
is	O
assumed	O
that	O
the	O
reader	O
is	O
already	O
familiar	O
with	O
basic	O
linear	O
algebra	O
.	O
for	O
some	O
results	O
,	O
we	O
indicate	O
how	O
to	O
prove	O
them	O
,	O
whereas	O
in	O
more	O
complex	O
cases	O
we	O
leave	O
the	O
interested	O
reader	O
to	O
refer	O
to	O
standard	O
textbooks	O
on	O
the	O
subject	O
.	O
in	O
all	O
cases	O
,	O
we	O
assume	O
that	O
inverses	O
exist	O
and	O
that	O
matrix	O
dimensions	O
are	O
such	O
that	O
the	O
formulae	O
are	O
correctly	O
deﬁned	O
.	O
a	O
comprehensive	O
discussion	O
of	O
linear	O
algebra	O
can	O
be	O
found	O
in	O
golub	O
and	O
van	O
loan	O
(	O
1996	O
)	O
,	O
and	O
an	O
extensive	O
collection	O
of	O
matrix	O
properties	O
is	O
given	O
by	O
l¨utkepohl	O
(	O
1996	O
)	O
.	O
matrix	O
derivatives	O
are	O
discussed	O
in	O
magnus	O
and	O
neudecker	O
(	O
1999	O
)	O
.	O
basic	O
matrix	O
identities	O
a	O
matrix	O
a	O
has	O
elements	O
aij	O
where	O
i	O
indexes	O
the	O
rows	O
,	O
and	O
j	O
indexes	O
the	O
columns	O
.	O
we	O
use	O
in	O
to	O
denote	O
the	O
n	O
×	O
n	O
identity	O
matrix	O
(	O
also	O
called	O
the	O
unit	O
matrix	O
)	O
,	O
and	O
where	O
there	O
is	O
no	O
ambiguity	O
over	O
dimensionality	O
we	O
simply	O
use	O
i.	O
the	O
transpose	O
matrix	O
at	O
has	O
elements	O
(	O
at	O
)	O
ij	O
=	O
aji	O
.	O
from	O
the	O
deﬁnition	O
of	O
transpose	O
,	O
we	O
have	O
(	O
c.1	O
)	O
which	O
can	O
be	O
veriﬁed	O
by	O
writing	O
out	O
the	O
indices	O
.	O
the	O
inverse	B
of	O
a	O
,	O
denoted	O
a−1	O
,	O
satisﬁes	O
(	O
ab	O
)	O
t	O
=	O
btat	O
aa−1	O
=	O
a−1a	O
=	O
i.	O
because	O
abb−1a−1	O
=	O
i	O
,	O
we	O
have	O
also	O
we	O
have	O
(	O
ab	O
)	O
−1	O
=	O
b−1a−1	O
.	O
(	O
cid:11	O
)	O
t	O
(	O
cid:10	O
)	O
at	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
−1	O
=	O
a−1	O
(	O
c.2	O
)	O
(	O
c.3	O
)	O
(	O
c.4	O
)	O
695	O
696	O
c.	O
properties	O
of	O
matrices	O
which	O
is	O
easily	O
proven	O
by	O
taking	O
the	O
transpose	O
of	O
(	O
c.2	O
)	O
and	O
applying	O
(	O
c.1	O
)	O
.	O
a	O
useful	O
identity	O
involving	O
matrix	O
inverses	O
is	O
the	O
following	O
(	O
p−1	O
+	O
btr−1b	O
)	O
−1btr−1	O
=	O
pbt	O
(	O
bpbt	O
+	O
r	O
)	O
−1	O
.	O
(	O
c.5	O
)	O
which	O
is	O
easily	O
veriﬁed	O
by	O
right	O
multiplying	O
both	O
sides	O
by	O
(	O
bpbt	O
+	O
r	O
)	O
.	O
suppose	O
that	O
p	O
has	O
dimensionality	O
n	O
×	O
n	O
while	O
r	O
has	O
dimensionality	O
m	O
×	O
m	O
,	O
so	O
that	O
b	O
is	O
m	O
×	O
n.	O
then	O
if	O
m	O
(	O
cid:5	O
)	O
n	O
,	O
it	O
will	O
be	O
much	O
cheaper	O
to	O
evaluate	O
the	O
right-hand	O
side	O
of	O
(	O
c.5	O
)	O
than	O
the	O
left-hand	O
side	O
.	O
a	O
special	O
case	O
that	O
sometimes	O
arises	O
is	O
(	O
i	O
+	O
ab	O
)	O
−1a	O
=	O
a	O
(	O
i	O
+	O
ba	O
)	O
−1	O
.	O
another	O
useful	O
identity	O
involving	O
inverses	O
is	O
the	O
following	O
:	O
(	O
a	O
+	O
bd−1c	O
)	O
−1	O
=	O
a−1	O
−	O
a−1b	O
(	O
d	O
+	O
ca−1b	O
)	O
−1ca−1	O
(	O
c.6	O
)	O
(	O
c.7	O
)	O
(	O
cid:5	O
)	O
which	O
is	O
known	O
as	O
the	O
woodbury	O
identity	O
and	O
which	O
can	O
be	O
veriﬁed	O
by	O
multiplying	O
both	O
sides	O
by	O
(	O
a	O
+	O
bd−1c	O
)	O
.	O
this	O
is	O
useful	O
,	O
for	O
instance	O
,	O
when	O
a	O
is	O
large	O
and	O
diagonal	B
,	O
and	O
hence	O
easy	O
to	O
invert	O
,	O
while	O
b	O
has	O
many	O
rows	O
but	O
few	O
columns	O
(	O
and	O
conversely	O
for	O
c	O
)	O
so	O
that	O
the	O
right-hand	O
side	O
is	O
much	O
cheaper	O
to	O
evaluate	O
than	O
the	O
left-hand	O
side	O
.	O
a	O
set	O
of	O
vectors	O
{	O
a1	O
,	O
.	O
.	O
.	O
,	O
an	O
}	O
is	O
said	O
to	O
be	O
linearly	O
independent	O
if	O
the	O
relation	O
n	O
αnan	O
=	O
0	O
holds	O
only	O
if	O
all	O
αn	O
=	O
0.	O
this	O
implies	O
that	O
none	O
of	O
the	O
vectors	O
can	O
be	O
expressed	O
as	O
a	O
linear	O
combination	O
of	O
the	O
remainder	O
.	O
the	O
rank	O
of	O
a	O
matrix	O
is	O
the	O
maximum	O
number	O
of	O
linearly	O
independent	O
rows	O
(	O
or	O
equivalently	O
the	O
maximum	O
number	O
of	O
linearly	O
independent	O
columns	O
)	O
.	O
traces	O
and	O
determinants	O
trace	O
and	O
determinant	O
apply	O
to	O
square	O
matrices	O
.	O
the	O
trace	O
tr	O
(	O
a	O
)	O
of	O
a	O
matrix	O
a	O
is	O
deﬁned	O
as	O
the	O
sum	O
of	O
the	O
elements	O
on	O
the	O
leading	O
diagonal	B
.	O
by	O
writing	O
out	O
the	O
indices	O
,	O
we	O
see	O
that	O
tr	O
(	O
ab	O
)	O
=	O
tr	O
(	O
ba	O
)	O
.	O
(	O
c.8	O
)	O
by	O
applying	O
this	O
formula	O
multiple	O
times	O
to	O
the	O
product	O
of	O
three	O
matrices	O
,	O
we	O
see	O
that	O
tr	O
(	O
abc	O
)	O
=	O
tr	O
(	O
cab	O
)	O
=	O
tr	O
(	O
bca	O
)	O
(	O
c.9	O
)	O
which	O
is	O
known	O
as	O
the	O
cyclic	O
property	O
of	O
the	O
trace	O
operator	O
and	O
which	O
clearly	O
extends	O
to	O
the	O
product	O
of	O
any	O
number	O
of	O
matrices	O
.	O
the	O
determinant	O
|a|	O
of	O
an	O
n	O
×	O
n	O
matrix	O
a	O
is	O
deﬁned	O
by	O
(	O
cid:2	O
)	O
|a|	O
=	O
(	O
±1	O
)	O
a1i1a2i2	O
···	O
an	O
in	O
(	O
c.10	O
)	O
in	O
which	O
the	O
sum	O
is	O
taken	O
over	O
all	O
products	O
consisting	O
of	O
precisely	O
one	O
element	O
from	O
each	O
row	O
and	O
one	O
element	O
from	O
each	O
column	O
,	O
with	O
a	O
coefﬁcient	O
+1	O
or	O
−1	O
according	O
c.	O
properties	O
of	O
matrices	O
697	O
to	O
whether	O
the	O
permutation	O
i1i2	O
.	O
.	O
.	O
in	O
is	O
even	O
or	O
odd	O
,	O
respectively	O
.	O
note	O
that	O
|i|	O
=	O
1.	O
thus	O
,	O
for	O
a	O
2	O
×	O
2	O
matrix	O
,	O
the	O
determinant	O
takes	O
the	O
form	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
a11	O
a12	O
a21	O
a22	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
=	O
a11a22	O
−	O
a12a21	O
.	O
|a|	O
=	O
the	O
determinant	O
of	O
a	O
product	O
of	O
two	O
matrices	O
is	O
given	O
by	O
as	O
can	O
be	O
shown	O
from	O
(	O
c.10	O
)	O
.	O
also	O
,	O
the	O
determinant	O
of	O
an	O
inverse	B
matrix	O
is	O
given	O
by	O
|ab|	O
=	O
|a||b|	O
1	O
|a|	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
=	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
a−1	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
im	O
+	O
atb	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
=	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
in	O
+	O
abt	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
=	O
1	O
+	O
atb	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
in	O
+	O
abt	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
.	O
a	O
useful	O
special	O
case	O
is	O
where	O
a	O
and	O
b	O
are	O
n-dimensional	O
column	O
vectors	O
.	O
matrix	O
derivatives	O
(	O
c.11	O
)	O
(	O
c.12	O
)	O
(	O
c.13	O
)	O
(	O
c.14	O
)	O
(	O
c.15	O
)	O
(	O
c.17	O
)	O
(	O
c.18	O
)	O
(	O
c.19	O
)	O
which	O
can	O
be	O
shown	O
by	O
taking	O
the	O
determinant	O
of	O
(	O
c.2	O
)	O
and	O
applying	O
(	O
c.12	O
)	O
.	O
if	O
a	O
and	O
b	O
are	O
matrices	O
of	O
size	O
n	O
×	O
m	O
,	O
then	O
sometimes	O
we	O
need	O
to	O
consider	O
derivatives	O
of	O
vectors	O
and	O
matrices	O
with	O
respect	O
to	O
scalars	O
.	O
the	O
derivative	B
of	O
a	O
vector	O
a	O
with	O
respect	O
to	O
a	O
scalar	O
x	O
is	O
itself	O
a	O
vector	O
whose	O
components	O
are	O
given	O
by	O
∂a	O
∂x	O
=	O
∂ai	O
∂x	O
(	O
c.16	O
)	O
with	O
an	O
analogous	O
deﬁnition	O
for	O
the	O
derivative	B
of	O
a	O
matrix	O
.	O
derivatives	O
with	O
respect	O
to	O
vectors	O
and	O
matrices	O
can	O
also	O
be	O
deﬁned	O
,	O
for	O
instance	O
(	O
cid:15	O
)	O
(	O
cid:15	O
)	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
i	O
i	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
∂x	O
∂a	O
=	O
∂x	O
∂ai	O
and	O
similarly	O
the	O
following	O
is	O
easily	O
proven	O
by	O
writing	O
out	O
the	O
components	O
∂a	O
∂b	O
(	O
cid:11	O
)	O
=	O
∂ai	O
∂bj	O
.	O
ij	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
(	O
cid:10	O
)	O
∂	O
∂x	O
xta	O
=	O
∂	O
∂x	O
atx	O
=	O
a	O
.	O
698	O
c.	O
properties	O
of	O
matrices	O
similarly	O
∂	O
∂x	O
(	O
ab	O
)	O
=	O
∂a	O
∂x	O
b	O
+	O
a	O
∂b	O
∂x	O
.	O
(	O
c.20	O
)	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
the	O
derivative	B
of	O
the	O
inverse	B
of	O
a	O
matrix	O
can	O
be	O
expressed	O
as	O
a−1	O
∂	O
∂x	O
=	O
−a−1	O
∂a	O
∂x	O
a−1	O
(	O
c.21	O
)	O
as	O
can	O
be	O
shown	O
by	O
differentiating	O
the	O
equation	O
a−1a	O
=	O
i	O
using	O
(	O
c.20	O
)	O
and	O
then	O
right	O
multiplying	O
by	O
a−1	O
.	O
also	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
ln|a|	O
=	O
tr	O
∂	O
∂x	O
a−1	O
∂a	O
∂x	O
which	O
we	O
shall	O
prove	O
later	O
.	O
if	O
we	O
choose	O
x	O
to	O
be	O
one	O
of	O
the	O
elements	O
of	O
a	O
,	O
we	O
have	O
∂	O
∂aij	O
tr	O
(	O
ab	O
)	O
=	O
bji	O
(	O
c.23	O
)	O
as	O
can	O
be	O
seen	O
by	O
writing	O
out	O
the	O
matrices	O
using	O
index	O
notation	O
.	O
we	O
can	O
write	O
this	O
result	O
more	O
compactly	O
in	O
the	O
form	O
(	O
c.22	O
)	O
(	O
c.24	O
)	O
(	O
c.25	O
)	O
(	O
c.26	O
)	O
(	O
c.27	O
)	O
(	O
c.28	O
)	O
with	O
this	O
notation	O
,	O
we	O
have	O
the	O
following	O
properties	O
tr	O
(	O
ab	O
)	O
=	O
bt	O
.	O
∂	O
∂a	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
∂	O
∂a	O
tr	O
atb	O
=	O
b	O
∂	O
∂a	O
tr	O
(	O
a	O
)	O
=	O
i	O
tr	O
(	O
abat	O
)	O
=	O
a	O
(	O
b	O
+	O
bt	O
)	O
∂	O
∂a	O
(	O
cid:11	O
)	O
t	O
(	O
cid:10	O
)	O
a−1	O
which	O
can	O
again	O
be	O
proven	O
by	O
writing	O
out	O
the	O
matrix	O
indices	O
.	O
we	O
also	O
have	O
ln|a|	O
=	O
∂	O
∂a	O
which	O
follows	O
from	O
(	O
c.22	O
)	O
and	O
(	O
c.26	O
)	O
.	O
eigenvector	O
equation	O
for	O
a	O
square	O
matrix	O
a	O
of	O
size	O
m	O
×	O
m	O
,	O
the	O
eigenvector	O
equation	O
is	O
deﬁned	O
by	O
aui	O
=	O
λiui	O
(	O
c.29	O
)	O
c.	O
properties	O
of	O
matrices	O
699	O
for	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
,	O
where	O
ui	O
is	O
an	O
eigenvector	O
and	O
λi	O
is	O
the	O
corresponding	O
eigenvalue	O
.	O
this	O
can	O
be	O
viewed	O
as	O
a	O
set	O
of	O
m	O
simultaneous	O
homogeneous	B
linear	O
equations	O
,	O
and	O
the	O
condition	O
for	O
a	O
solution	O
is	O
that	O
|a	O
−	O
λii|	O
=	O
0	O
(	O
c.30	O
)	O
which	O
is	O
known	O
as	O
the	O
characteristic	O
equation	O
.	O
because	O
this	O
is	O
a	O
polynomial	O
of	O
order	O
m	O
in	O
λi	O
,	O
it	O
must	O
have	O
m	O
solutions	O
(	O
though	O
these	O
need	O
not	O
all	O
be	O
distinct	O
)	O
.	O
the	O
rank	O
of	O
a	O
is	O
equal	O
to	O
the	O
number	O
of	O
nonzero	O
eigenvalues	O
.	O
of	O
particular	O
interest	O
are	O
symmetric	O
matrices	O
,	O
which	O
arise	O
as	O
covariance	B
ma-	O
trices	O
,	O
kernel	O
matrices	O
,	O
and	O
hessians	O
.	O
symmetric	O
matrices	O
have	O
the	O
property	O
that	O
aij	O
=	O
aji	O
,	O
or	O
equivalently	O
at	O
=	O
a.	O
the	O
inverse	B
of	O
a	O
symmetric	O
matrix	O
is	O
also	O
sym-	O
metric	O
,	O
as	O
can	O
be	O
seen	O
by	O
taking	O
the	O
transpose	O
of	O
a−1a	O
=	O
i	O
and	O
using	O
aa−1	O
=	O
i	O
together	O
with	O
the	O
symmetry	O
of	O
i.	O
in	O
general	O
,	O
the	O
eigenvalues	O
of	O
a	O
matrix	O
are	O
complex	O
numbers	O
,	O
but	O
for	O
symmetric	O
matrices	O
the	O
eigenvalues	O
λi	O
are	O
real	O
.	O
this	O
can	O
be	O
seen	O
by	O
ﬁrst	O
left	O
multiplying	O
(	O
c.29	O
)	O
by	O
(	O
u	O
(	O
cid:3	O
)	O
i	O
)	O
t	O
,	O
where	O
(	O
cid:11	O
)	O
denotes	O
the	O
complex	O
conjugate	B
,	O
to	O
give	O
(	O
u	O
(	O
cid:3	O
)	O
i	O
)	O
t	O
aui	O
=	O
λi	O
(	O
u	O
(	O
cid:3	O
)	O
i	O
)	O
t	O
ui	O
.	O
(	O
c.31	O
)	O
next	O
we	O
take	O
the	O
complex	O
conjugate	B
of	O
(	O
c.29	O
)	O
and	O
left	O
multiply	O
by	O
ut	O
i	O
to	O
give	O
i	O
au	O
(	O
cid:3	O
)	O
ut	O
i	O
=	O
λ	O
(	O
cid:3	O
)	O
i	O
ut	O
i	O
u	O
(	O
cid:3	O
)	O
i	O
.	O
(	O
c.32	O
)	O
where	O
we	O
have	O
used	O
a	O
(	O
cid:3	O
)	O
=	O
a	O
because	O
we	O
consider	O
only	O
real	O
matrices	O
a.	O
taking	O
the	O
transpose	O
of	O
the	O
second	O
of	O
these	O
equations	O
,	O
and	O
using	O
at	O
=	O
a	O
,	O
we	O
see	O
that	O
the	O
left-hand	O
sides	O
of	O
the	O
two	O
equations	O
are	O
equal	O
,	O
and	O
hence	O
that	O
λ	O
(	O
cid:3	O
)	O
i	O
=	O
λi	O
and	O
so	O
λi	O
must	O
be	O
real	O
.	O
the	O
eigenvectors	O
ui	O
of	O
a	O
real	O
symmetric	O
matrix	O
can	O
be	O
chosen	O
to	O
be	O
orthonormal	O
(	O
i.e.	O
,	O
orthogonal	O
and	O
of	O
unit	O
length	O
)	O
so	O
that	O
ut	O
i	O
uj	O
=	O
iij	O
(	O
c.33	O
)	O
where	O
iij	O
are	O
the	O
elements	O
of	O
the	O
identity	O
matrix	O
i.	O
to	O
show	O
this	O
,	O
we	O
ﬁrst	O
left	O
multiply	O
(	O
c.29	O
)	O
by	O
ut	O
j	O
to	O
give	O
and	O
hence	O
,	O
by	O
exchange	O
of	O
indices	O
,	O
we	O
have	O
ut	O
j	O
aui	O
=	O
λiut	O
j	O
ui	O
ut	O
i	O
auj	O
=	O
λjut	O
i	O
uj	O
.	O
(	O
c.34	O
)	O
(	O
c.35	O
)	O
we	O
now	O
take	O
the	O
transpose	O
of	O
the	O
second	O
equation	O
and	O
make	O
use	O
of	O
the	O
symmetry	O
property	O
at	O
=	O
a	O
,	O
and	O
then	O
subtract	O
the	O
two	O
equations	O
to	O
give	O
(	O
λi	O
−	O
λj	O
)	O
ut	O
hence	O
,	O
for	O
λi	O
(	O
cid:6	O
)	O
=	O
λj	O
,	O
we	O
have	O
ut	O
i	O
uj	O
=	O
0	O
,	O
and	O
hence	O
ui	O
and	O
uj	O
are	O
orthogonal	O
.	O
if	O
the	O
two	O
eigenvalues	O
are	O
equal	O
,	O
then	O
any	O
linear	O
combination	O
αui	O
+	O
βuj	O
is	O
also	O
an	O
eigen-	O
vector	O
with	O
the	O
same	O
eigenvalue	O
,	O
so	O
we	O
can	O
select	O
one	O
linear	O
combination	O
arbitrarily	O
,	O
i	O
uj	O
=	O
0	O
.	O
(	O
c.36	O
)	O
700	O
c.	O
properties	O
of	O
matrices	O
and	O
then	O
choose	O
the	O
second	O
to	O
be	O
orthogonal	O
to	O
the	O
ﬁrst	O
(	O
it	O
can	O
be	O
shown	O
that	O
the	O
de-	O
generate	O
eigenvectors	O
are	O
never	O
linearly	O
dependent	O
)	O
.	O
hence	O
the	O
eigenvectors	O
can	O
be	O
chosen	O
to	O
be	O
orthogonal	O
,	O
and	O
by	O
normalizing	O
can	O
be	O
set	O
to	O
unit	O
length	O
.	O
because	O
there	O
are	O
m	O
eigenvalues	O
,	O
the	O
corresponding	O
m	O
orthogonal	O
eigenvectors	O
form	O
a	O
complete	O
set	O
and	O
so	O
any	O
m-dimensional	O
vector	O
can	O
be	O
expressed	O
as	O
a	O
linear	O
combination	O
of	O
the	O
eigenvectors	O
.	O
we	O
can	O
take	O
the	O
eigenvectors	O
ui	O
to	O
be	O
the	O
columns	O
of	O
an	O
m	O
×	O
m	O
matrix	O
u	O
,	O
which	O
from	O
orthonormality	O
satisﬁes	O
utu	O
=	O
i	O
.	O
(	O
c.37	O
)	O
such	O
a	O
matrix	O
is	O
said	O
to	O
be	O
orthogonal	O
.	O
interestingly	O
,	O
the	O
rows	O
of	O
this	O
matrix	O
are	O
also	O
orthogonal	O
,	O
so	O
that	O
uut	O
=	O
i.	O
to	O
show	O
this	O
,	O
note	O
that	O
(	O
c.37	O
)	O
implies	O
utuu−1	O
=	O
u−1	O
=	O
ut	O
and	O
so	O
uu−1	O
=	O
uut	O
=	O
i.	O
using	O
(	O
c.12	O
)	O
,	O
it	O
also	O
follows	O
that	O
|u|	O
=	O
1.	O
the	O
eigenvector	O
equation	O
(	O
c.29	O
)	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
u	O
in	O
the	O
form	O
(	O
c.38	O
)	O
where	O
λ	O
is	O
an	O
m	O
×	O
m	O
diagonal	B
matrix	O
whose	O
diagonal	B
elements	O
are	O
given	O
by	O
the	O
eigenvalues	O
λi	O
.	O
au	O
=	O
uλ	O
if	O
we	O
consider	O
a	O
column	O
vector	O
x	O
that	O
is	O
transformed	O
by	O
an	O
orthogonal	O
matrix	O
u	O
to	O
give	O
a	O
new	O
vector	O
then	O
the	O
length	O
of	O
the	O
vector	O
is	O
preserved	O
because	O
and	O
similarly	O
the	O
angle	O
between	O
any	O
two	O
such	O
vectors	O
is	O
preserved	O
because	O
(	O
cid:4	O
)	O
x	O
=	O
ux	O
(	O
cid:4	O
)	O
xt	O
(	O
cid:4	O
)	O
x	O
=	O
xtutux	O
=	O
xtx	O
(	O
cid:4	O
)	O
xt	O
(	O
cid:4	O
)	O
y	O
=	O
xtutuy	O
=	O
xty	O
.	O
(	O
c.39	O
)	O
(	O
c.40	O
)	O
(	O
c.41	O
)	O
thus	O
,	O
multiplication	O
by	O
u	O
can	O
be	O
interpreted	O
as	O
a	O
rigid	O
rotation	O
of	O
the	O
coordinate	O
system	O
.	O
from	O
(	O
c.38	O
)	O
,	O
it	O
follows	O
that	O
utau	O
=	O
λ	O
(	O
c.42	O
)	O
and	O
because	O
λ	O
is	O
a	O
diagonal	B
matrix	O
,	O
we	O
say	O
that	O
the	O
matrix	O
a	O
is	O
diagonalized	O
by	O
the	O
matrix	O
u.	O
if	O
we	O
left	O
multiply	O
by	O
u	O
and	O
right	O
multiply	O
by	O
ut	O
,	O
we	O
obtain	O
(	O
c.43	O
)	O
taking	O
the	O
inverse	B
of	O
this	O
equation	O
,	O
and	O
using	O
(	O
c.3	O
)	O
together	O
with	O
u−1	O
=	O
ut	O
,	O
we	O
have	O
a	O
=	O
uλut	O
a−1	O
=	O
uλ	O
−1ut	O
.	O
(	O
c.44	O
)	O
a	O
=	O
a−1	O
=	O
λiuiut	O
i	O
1	O
λi	O
uiut	O
i	O
.	O
|a|	O
=	O
λi	O
.	O
i=1	O
m	O
(	O
cid:2	O
)	O
m	O
(	O
cid:2	O
)	O
i=1	O
i=1	O
m	O
(	O
cid:14	O
)	O
m	O
(	O
cid:2	O
)	O
(	O
c.45	O
)	O
(	O
c.46	O
)	O
(	O
c.47	O
)	O
c.	O
properties	O
of	O
matrices	O
701	O
these	O
last	O
two	O
equations	O
can	O
also	O
be	O
written	O
in	O
the	O
form	O
if	O
we	O
take	O
the	O
determinant	O
of	O
(	O
c.43	O
)	O
,	O
and	O
use	O
(	O
c.12	O
)	O
,	O
we	O
obtain	O
similarly	O
,	O
taking	O
the	O
trace	O
of	O
(	O
c.43	O
)	O
,	O
and	O
using	O
the	O
cyclic	O
property	O
(	O
c.8	O
)	O
of	O
the	O
trace	O
operator	O
together	O
with	O
utu	O
=	O
i	O
,	O
we	O
have	O
tr	O
(	O
a	O
)	O
=	O
λi	O
.	O
i=1	O
(	O
c.48	O
)	O
we	O
leave	O
it	O
as	O
an	O
exercise	O
for	O
the	O
reader	O
to	O
verify	O
(	O
c.22	O
)	O
by	O
making	O
use	O
of	O
the	O
results	O
(	O
c.33	O
)	O
,	O
(	O
c.45	O
)	O
,	O
(	O
c.46	O
)	O
,	O
and	O
(	O
c.47	O
)	O
.	O
a	O
matrix	O
a	O
is	O
said	O
to	O
be	O
positive	B
deﬁnite	I
,	O
denoted	O
by	O
a	O
(	O
cid:7	O
)	O
0	O
,	O
if	O
wtaw	O
>	O
0	O
for	O
all	O
values	O
of	O
the	O
vector	O
w.	O
equivalently	O
,	O
a	O
positive	B
deﬁnite	I
matrix	I
has	O
λi	O
>	O
0	O
for	O
all	O
of	O
its	O
eigenvalues	O
(	O
as	O
can	O
be	O
seen	O
by	O
setting	O
w	O
to	O
each	O
of	O
the	O
eigenvectors	O
in	O
turn	O
,	O
and	O
by	O
noting	O
that	O
an	O
arbitrary	O
vector	O
can	O
be	O
expanded	O
as	O
a	O
linear	O
combination	O
of	O
the	O
eigenvectors	O
)	O
.	O
note	O
that	O
positive	B
deﬁnite	I
is	O
not	O
the	O
same	O
as	O
all	O
the	O
elements	O
being	O
positive	O
.	O
for	O
example	O
,	O
the	O
matrix	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
(	O
c.49	O
)	O
has	O
eigenvalues	O
λ1	O
(	O
cid:8	O
)	O
5.37	O
and	O
λ2	O
(	O
cid:8	O
)	O
−0.37	O
.	O
a	O
matrix	O
is	O
said	O
to	O
be	O
positive	O
semidef-	O
inite	O
if	O
wtaw	O
(	O
cid:2	O
)	O
0	O
holds	O
for	O
all	O
values	O
of	O
w	O
,	O
which	O
is	O
denoted	O
a	O
(	O
cid:9	O
)	O
0	O
,	O
and	O
is	O
equivalent	O
to	O
λi	O
(	O
cid:2	O
)	O
0	O
.	O
1	O
2	O
3	O
4	O
appendix	O
d.	O
calculus	B
of	I
variations	I
we	O
can	O
think	O
of	O
a	O
function	O
y	O
(	O
x	O
)	O
as	O
being	O
an	O
operator	O
that	O
,	O
for	O
any	O
input	O
value	O
x	O
,	O
returns	O
an	O
output	O
value	O
y.	O
in	O
the	O
same	O
way	O
,	O
we	O
can	O
deﬁne	O
a	O
functional	B
f	O
[	O
y	O
]	O
to	O
be	O
an	O
operator	O
that	O
takes	O
a	O
function	O
y	O
(	O
x	O
)	O
and	O
returns	O
an	O
output	O
value	O
f	O
.	O
an	O
example	O
of	O
a	O
functional	B
is	O
the	O
length	O
of	O
a	O
curve	O
drawn	O
in	O
a	O
two-dimensional	O
plane	O
in	O
which	O
the	O
path	O
of	O
the	O
curve	O
is	O
deﬁned	O
in	O
terms	O
of	O
a	O
function	O
.	O
in	O
the	O
context	O
of	O
machine	O
learning	O
,	O
a	O
widely	O
used	O
functional	B
is	O
the	O
entropy	B
h	O
[	O
x	O
]	O
for	O
a	O
continuous	O
variable	O
x	O
because	O
,	O
for	O
any	O
choice	O
of	O
probability	B
density	O
function	O
p	O
(	O
x	O
)	O
,	O
it	O
returns	O
a	O
scalar	O
value	O
representing	O
the	O
entropy	B
of	O
x	O
under	O
that	O
density	B
.	O
thus	O
the	O
entropy	B
of	O
p	O
(	O
x	O
)	O
could	O
equally	O
well	O
have	O
been	O
written	O
as	O
h	O
[	O
p	O
]	O
.	O
a	O
common	O
problem	O
in	O
conventional	O
calculus	O
is	O
to	O
ﬁnd	O
a	O
value	O
of	O
x	O
that	O
max-	O
imizes	O
(	O
or	O
minimizes	O
)	O
a	O
function	O
y	O
(	O
x	O
)	O
.	O
similarly	O
,	O
in	O
the	O
calculus	B
of	I
variations	I
we	O
seek	O
a	O
function	O
y	O
(	O
x	O
)	O
that	O
maximizes	O
(	O
or	O
minimizes	O
)	O
a	O
functional	B
f	O
[	O
y	O
]	O
.	O
that	O
is	O
,	O
of	O
all	O
possible	O
functions	O
y	O
(	O
x	O
)	O
,	O
we	O
wish	O
to	O
ﬁnd	O
the	O
particular	O
function	O
for	O
which	O
the	O
func-	O
tional	O
f	O
[	O
y	O
]	O
is	O
a	O
maximum	O
(	O
or	O
minimum	O
)	O
.	O
the	O
calculus	B
of	I
variations	I
can	O
be	O
used	O
,	O
for	O
instance	O
,	O
to	O
show	O
that	O
the	O
shortest	O
path	O
between	O
two	O
points	O
is	O
a	O
straight	O
line	O
or	O
that	O
the	O
maximum	O
entropy	O
distribution	O
is	O
a	O
gaussian	O
.	O
if	O
we	O
weren	O
’	O
t	O
familiar	O
with	O
the	O
rules	O
of	O
ordinary	O
calculus	O
,	O
we	O
could	O
evaluate	O
a	O
conventional	O
derivative	B
dy/	O
dx	O
by	O
making	O
a	O
small	O
change	O
	O
to	O
the	O
variable	O
x	O
and	O
then	O
expanding	O
in	O
powers	O
of	O
	O
,	O
so	O
that	O
y	O
(	O
x	O
+	O
	O
)	O
=	O
y	O
(	O
x	O
)	O
+	O
(	O
d.1	O
)	O
and	O
ﬁnally	O
taking	O
the	O
limit	O
	O
→	O
0.	O
similarly	O
,	O
for	O
a	O
function	O
of	O
several	O
variables	O
y	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
,	O
the	O
corresponding	O
partial	O
derivatives	O
are	O
deﬁned	O
by	O
	O
+	O
o	O
(	O
2	O
)	O
dy	O
dx	O
d	O
(	O
cid:2	O
)	O
i=1	O
y	O
(	O
x1	O
+	O
1	O
,	O
.	O
.	O
.	O
,	O
xd	O
+	O
d	O
)	O
=	O
y	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
+	O
∂y	O
∂xi	O
i	O
+	O
o	O
(	O
2	O
)	O
.	O
(	O
d.2	O
)	O
the	O
analogous	O
deﬁnition	O
of	O
a	O
functional	B
derivative	O
arises	O
when	O
we	O
consider	O
how	O
much	O
a	O
functional	B
f	O
[	O
y	O
]	O
changes	O
when	O
we	O
make	O
a	O
small	O
change	O
η	O
(	O
x	O
)	O
to	O
the	O
function	O
703	O
704	O
d.	O
calculus	B
of	I
variations	I
figure	O
d.1	O
a	O
functional	B
derivative	O
can	O
be	O
deﬁned	O
by	O
considering	O
how	O
the	O
value	O
of	O
a	O
functional	B
f	O
[	O
y	O
]	O
changes	O
when	O
the	O
function	O
y	O
(	O
x	O
)	O
is	O
changed	O
to	O
y	O
(	O
x	O
)	O
+	O
η	O
(	O
x	O
)	O
where	O
η	O
(	O
x	O
)	O
is	O
an	O
arbitrary	O
function	O
of	O
x.	O
y	O
(	O
x	O
)	O
y	O
(	O
x	O
)	O
+	O
η	O
(	O
x	O
)	O
x	O
y	O
(	O
x	O
)	O
,	O
where	O
η	O
(	O
x	O
)	O
is	O
an	O
arbitrary	O
function	O
of	O
x	O
,	O
as	O
illustrated	O
in	O
figure	O
d.1	O
.	O
we	O
denote	O
the	O
functional	B
derivative	O
of	O
e	O
[	O
f	O
]	O
with	O
respect	O
to	O
f	O
(	O
x	O
)	O
by	O
δf/δf	O
(	O
x	O
)	O
,	O
and	O
deﬁne	O
it	O
by	O
the	O
following	O
relation	O
:	O
(	O
cid:6	O
)	O
f	O
[	O
y	O
(	O
x	O
)	O
+	O
η	O
(	O
x	O
)	O
]	O
=	O
f	O
[	O
y	O
(	O
x	O
)	O
]	O
+	O
	O
δf	O
δy	O
(	O
x	O
)	O
η	O
(	O
x	O
)	O
dx	O
+	O
o	O
(	O
2	O
)	O
.	O
(	O
d.3	O
)	O
this	O
can	O
be	O
seen	O
as	O
a	O
natural	O
extension	O
of	O
(	O
d.2	O
)	O
in	O
which	O
f	O
[	O
y	O
]	O
now	O
depends	O
on	O
a	O
continuous	O
set	O
of	O
variables	O
,	O
namely	O
the	O
values	O
of	O
y	O
at	O
all	O
points	O
x.	O
requiring	O
that	O
the	O
functional	B
be	O
stationary	B
with	O
respect	O
to	O
small	O
variations	O
in	O
the	O
function	O
y	O
(	O
x	O
)	O
gives	O
(	O
cid:6	O
)	O
δe	O
δy	O
(	O
x	O
)	O
η	O
(	O
x	O
)	O
dx	O
=	O
0	O
.	O
(	O
d.4	O
)	O
because	O
this	O
must	O
hold	O
for	O
an	O
arbitrary	O
choice	O
of	O
η	O
(	O
x	O
)	O
,	O
it	O
follows	O
that	O
the	O
functional	B
derivative	O
must	O
vanish	O
.	O
to	O
see	O
this	O
,	O
imagine	O
choosing	O
a	O
perturbation	O
η	O
(	O
x	O
)	O
that	O
is	O
zero	O
everywhere	O
except	O
in	O
the	O
neighbourhood	O
of	O
a	O
point	O
(	O
cid:1	O
)	O
x	O
,	O
in	O
which	O
case	O
the	O
functional	B
derivative	O
must	O
be	O
zero	O
at	O
x	O
=	O
(	O
cid:1	O
)	O
x.	O
however	O
,	O
because	O
this	O
must	O
be	O
true	O
for	O
every	O
choice	O
of	O
(	O
cid:1	O
)	O
x	O
,	O
the	O
functional	B
derivative	O
must	O
vanish	O
for	O
all	O
values	O
of	O
x.	O
consider	O
a	O
functional	B
that	O
is	O
deﬁned	O
by	O
an	O
integral	O
over	O
a	O
function	O
g	O
(	O
y	O
,	O
y	O
(	O
cid:1	O
)	O
that	O
depends	O
on	O
both	O
y	O
(	O
x	O
)	O
and	O
its	O
derivative	B
y	O
dence	O
on	O
x	O
f	O
[	O
y	O
]	O
=	O
g	O
(	O
y	O
(	O
x	O
)	O
,	O
y	O
,	O
x	O
)	O
(	O
cid:1	O
)	O
(	O
x	O
)	O
as	O
well	O
as	O
having	O
a	O
direct	O
depen-	O
(	O
cid:1	O
)	O
(	O
x	O
)	O
,	O
x	O
)	O
dx	O
(	O
d.5	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
where	O
the	O
value	O
of	O
y	O
(	O
x	O
)	O
is	O
assumed	O
to	O
be	O
ﬁxed	O
at	O
the	O
boundary	O
of	O
the	O
region	O
of	O
integration	O
(	O
which	O
might	O
be	O
at	O
inﬁnity	O
)	O
.	O
if	O
we	O
now	O
consider	O
variations	O
in	O
the	O
function	O
y	O
(	O
x	O
)	O
,	O
we	O
obtain	O
f	O
[	O
y	O
(	O
x	O
)	O
+	O
η	O
(	O
x	O
)	O
]	O
=	O
f	O
[	O
y	O
(	O
x	O
)	O
]	O
+	O
	O
∂g	O
∂y	O
η	O
(	O
x	O
)	O
+	O
∂g	O
∂y	O
(	O
cid:1	O
)	O
η	O
(	O
cid:1	O
)	O
(	O
x	O
)	O
dx	O
+	O
o	O
(	O
2	O
)	O
.	O
(	O
d.6	O
)	O
we	O
now	O
have	O
to	O
cast	O
this	O
in	O
the	O
form	O
(	O
d.3	O
)	O
.	O
to	O
do	O
so	O
,	O
we	O
integrate	O
the	O
second	O
term	O
by	O
parts	O
and	O
make	O
use	O
of	O
the	O
fact	O
that	O
η	O
(	O
x	O
)	O
must	O
vanish	O
at	O
the	O
boundary	O
of	O
the	O
integral	O
(	O
because	O
y	O
(	O
x	O
)	O
is	O
ﬁxed	O
at	O
the	O
boundary	O
)	O
.	O
this	O
gives	O
−	O
d	O
dx	O
f	O
[	O
y	O
(	O
x	O
)	O
+	O
η	O
(	O
x	O
)	O
]	O
=	O
f	O
[	O
y	O
(	O
x	O
)	O
]	O
+	O
	O
η	O
(	O
x	O
)	O
dx	O
+	O
o	O
(	O
2	O
)	O
(	O
d.7	O
)	O
(	O
cid:6	O
)	O
(	O
cid:12	O
)	O
(	O
cid:16	O
)	O
(	O
cid:13	O
)	O
∂g	O
∂y	O
(	O
cid:1	O
)	O
∂g	O
∂y	O
(	O
cid:15	O
)	O
d.	O
calculus	B
of	I
variations	I
705	O
from	O
which	O
we	O
can	O
read	O
off	O
the	O
functional	B
derivative	O
by	O
comparison	O
with	O
(	O
d.3	O
)	O
.	O
requiring	O
that	O
the	O
functional	B
derivative	O
vanishes	O
then	O
gives	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
∂g	O
∂y	O
−	O
d	O
dx	O
∂g	O
∂y	O
(	O
cid:1	O
)	O
=	O
0	O
which	O
are	O
known	O
as	O
the	O
euler-lagrange	O
equations	O
.	O
for	O
example	O
,	O
if	O
g	O
=	O
y	O
(	O
x	O
)	O
2	O
+	O
(	O
y	O
(	O
cid:1	O
)	O
(	O
x	O
)	O
)	O
2	O
then	O
the	O
euler-lagrange	O
equations	O
take	O
the	O
form	O
y	O
(	O
x	O
)	O
−	O
d2y	O
dx2	O
=	O
0	O
.	O
(	O
d.8	O
)	O
(	O
d.9	O
)	O
(	O
d.10	O
)	O
this	O
second	B
order	I
differential	O
equation	O
can	O
be	O
solved	O
for	O
y	O
(	O
x	O
)	O
by	O
making	O
use	O
of	O
the	O
boundary	O
conditions	O
on	O
y	O
(	O
x	O
)	O
.	O
often	O
,	O
we	O
consider	O
functionals	O
deﬁned	O
by	O
integrals	O
whose	O
integrands	O
take	O
the	O
form	O
g	O
(	O
y	O
,	O
x	O
)	O
and	O
that	O
do	O
not	O
depend	O
on	O
the	O
derivatives	O
of	O
y	O
(	O
x	O
)	O
.	O
in	O
this	O
case	O
,	O
station-	O
arity	O
simply	O
requires	O
that	O
∂g/∂y	O
(	O
x	O
)	O
=	O
0	O
for	O
all	O
values	O
of	O
x.	O
if	O
we	O
are	O
optimizing	O
a	O
functional	B
with	O
respect	O
to	O
a	O
probability	B
distribution	O
,	O
then	O
we	O
need	O
to	O
maintain	O
the	O
normalization	O
constraint	O
on	O
the	O
probabilities	O
.	O
this	O
is	O
often	O
most	O
conveniently	O
done	O
using	O
a	O
lagrange	O
multiplier	O
,	O
which	O
then	O
allows	O
an	O
uncon-	O
strained	O
optimization	O
to	O
be	O
performed	O
.	O
the	O
extension	O
of	O
the	O
above	O
results	O
to	O
a	O
multidimensional	O
variable	O
x	O
is	O
straight-	O
forward	O
.	O
for	O
a	O
more	O
comprehensive	O
discussion	O
of	O
the	O
calculus	B
of	I
variations	I
,	O
see	O
sagan	O
(	O
1969	O
)	O
.	O
appendix	O
e	O
appendix	O
e.	O
lagrange	O
multipliers	O
lagrange	O
multipliers	O
,	O
also	O
sometimes	O
called	O
undetermined	O
multipliers	O
,	O
are	O
used	O
to	O
ﬁnd	O
the	O
stationary	B
points	O
of	O
a	O
function	O
of	O
several	O
variables	O
subject	O
to	O
one	O
or	O
more	O
constraints	O
.	O
consider	O
the	O
problem	O
of	O
ﬁnding	O
the	O
maximum	O
of	O
a	O
function	O
f	O
(	O
x1	O
,	O
x2	O
)	O
subject	O
to	O
a	O
constraint	O
relating	O
x1	O
and	O
x2	O
,	O
which	O
we	O
write	O
in	O
the	O
form	O
g	O
(	O
x1	O
,	O
x2	O
)	O
=	O
0	O
.	O
(	O
e.1	O
)	O
one	O
approach	O
would	O
be	O
to	O
solve	O
the	O
constraint	O
equation	O
(	O
e.1	O
)	O
and	O
thus	O
express	O
x2	O
as	O
a	O
function	O
of	O
x1	O
in	O
the	O
form	O
x2	O
=	O
h	O
(	O
x1	O
)	O
.	O
this	O
can	O
then	O
be	O
substituted	O
into	O
f	O
(	O
x1	O
,	O
x2	O
)	O
to	O
give	O
a	O
function	O
of	O
x1	O
alone	O
of	O
the	O
form	O
f	O
(	O
x1	O
,	O
h	O
(	O
x1	O
)	O
)	O
.	O
the	O
maximum	O
with	O
respect	O
to	O
x1	O
could	O
then	O
be	O
found	O
by	O
differentiation	O
in	O
the	O
usual	O
way	O
,	O
to	O
give	O
the	O
stationary	B
value	O
x	O
(	O
cid:3	O
)	O
1	O
,	O
with	O
the	O
corresponding	O
value	O
of	O
x2	O
given	O
by	O
x	O
(	O
cid:3	O
)	O
one	O
problem	O
with	O
this	O
approach	O
is	O
that	O
it	O
may	O
be	O
difﬁcult	O
to	O
ﬁnd	O
an	O
analytic	O
solution	O
of	O
the	O
constraint	O
equation	O
that	O
allows	O
x2	O
to	O
be	O
expressed	O
as	O
an	O
explicit	O
func-	O
tion	O
of	O
x1	O
.	O
also	O
,	O
this	O
approach	O
treats	O
x1	O
and	O
x2	O
differently	O
and	O
so	O
spoils	O
the	O
natural	O
symmetry	O
between	O
these	O
variables	O
.	O
2	O
=	O
h	O
(	O
x	O
(	O
cid:3	O
)	O
1	O
)	O
.	O
a	O
more	O
elegant	O
,	O
and	O
often	O
simpler	O
,	O
approach	O
is	O
based	O
on	O
the	O
introduction	O
of	O
a	O
parameter	O
λ	O
called	O
a	O
lagrange	O
multiplier	O
.	O
we	O
shall	O
motivate	O
this	O
technique	O
from	O
a	O
geometrical	O
perspective	O
.	O
consider	O
a	O
d-dimensional	O
variable	O
x	O
with	O
components	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
.	O
the	O
constraint	O
equation	O
g	O
(	O
x	O
)	O
=	O
0	O
then	O
represents	O
a	O
(	O
d−1	O
)	O
-dimensional	O
surface	O
in	O
x-space	O
as	O
indicated	O
in	O
figure	O
e.1	O
.	O
we	O
ﬁrst	O
note	O
that	O
at	O
any	O
point	O
on	O
the	O
constraint	O
surface	O
the	O
gradient	O
∇g	O
(	O
x	O
)	O
of	O
the	O
constraint	O
function	O
will	O
be	O
orthogonal	O
to	O
the	O
surface	O
.	O
to	O
see	O
this	O
,	O
consider	O
a	O
point	O
x	O
that	O
lies	O
on	O
the	O
constraint	O
surface	O
,	O
and	O
consider	O
a	O
nearby	O
point	O
x	O
+	O
	O
that	O
also	O
lies	O
on	O
the	O
surface	O
.	O
if	O
we	O
make	O
a	O
taylor	O
expansion	O
around	O
x	O
,	O
we	O
have	O
g	O
(	O
x	O
+	O
	O
)	O
(	O
cid:8	O
)	O
g	O
(	O
x	O
)	O
+	O
t∇g	O
(	O
x	O
)	O
.	O
(	O
e.2	O
)	O
because	O
both	O
x	O
and	O
x+	O
	O
lie	O
on	O
the	O
constraint	O
surface	O
,	O
we	O
have	O
g	O
(	O
x	O
)	O
=	O
g	O
(	O
x+	O
	O
)	O
and	O
hence	O
t∇g	O
(	O
x	O
)	O
(	O
cid:8	O
)	O
0.	O
in	O
the	O
limit	O
(	O
cid:11	O
)	O
	O
(	O
cid:11	O
)	O
→	O
0	O
we	O
have	O
t∇g	O
(	O
x	O
)	O
=	O
0	O
,	O
and	O
because	O
	O
is	O
707	O
708	O
e.	O
lagrange	O
multipliers	O
figure	O
e.1	O
a	O
geometrical	O
picture	O
of	O
the	O
technique	O
of	O
la-	O
grange	O
multipliers	O
in	O
which	O
we	O
seek	O
to	O
maximize	O
a	O
function	O
f	O
(	O
x	O
)	O
,	O
subject	O
to	O
the	O
constraint	O
g	O
(	O
x	O
)	O
=	O
0.	O
if	O
x	O
is	O
d	O
dimensional	O
,	O
the	O
constraint	O
g	O
(	O
x	O
)	O
=	O
0	O
cor-	O
responds	O
to	O
a	O
subspace	O
of	O
dimensionality	O
d	O
−	O
1	O
,	O
indicated	O
by	O
the	O
red	O
curve	O
.	O
the	O
problem	O
can	O
be	O
solved	O
by	O
optimizing	O
the	O
lagrangian	O
function	O
l	O
(	O
x	O
,	O
λ	O
)	O
=	O
f	O
(	O
x	O
)	O
+	O
λg	O
(	O
x	O
)	O
.	O
∇f	O
(	O
x	O
)	O
xa	O
∇g	O
(	O
x	O
)	O
g	O
(	O
x	O
)	O
=	O
0	O
then	O
parallel	O
to	O
the	O
constraint	O
surface	O
g	O
(	O
x	O
)	O
=	O
0	O
,	O
we	O
see	O
that	O
the	O
vector	O
∇g	O
is	O
normal	O
to	O
the	O
surface	O
.	O
next	O
we	O
seek	O
a	O
point	O
x	O
(	O
cid:3	O
)	O
on	O
the	O
constraint	O
surface	O
such	O
that	O
f	O
(	O
x	O
)	O
is	O
maximized	O
.	O
such	O
a	O
point	O
must	O
have	O
the	O
property	O
that	O
the	O
vector	O
∇f	O
(	O
x	O
)	O
is	O
also	O
orthogonal	O
to	O
the	O
constraint	O
surface	O
,	O
as	O
illustrated	O
in	O
figure	O
e.1	O
,	O
because	O
otherwise	O
we	O
could	O
increase	O
the	O
value	O
of	O
f	O
(	O
x	O
)	O
by	O
moving	O
a	O
short	O
distance	O
along	O
the	O
constraint	O
surface	O
.	O
thus	O
∇f	O
and	O
∇g	O
are	O
parallel	O
(	O
or	O
anti-parallel	O
)	O
vectors	O
,	O
and	O
so	O
there	O
must	O
exist	O
a	O
parameter	O
λ	O
such	O
that	O
where	O
λ	O
(	O
cid:6	O
)	O
=	O
0	O
is	O
known	O
as	O
a	O
lagrange	O
multiplier	O
.	O
note	O
that	O
λ	O
can	O
have	O
either	O
sign	O
.	O
at	O
this	O
point	O
,	O
it	O
is	O
convenient	O
to	O
introduce	O
the	O
lagrangian	O
function	O
deﬁned	O
by	O
∇f	O
+	O
λ∇g	O
=	O
0	O
(	O
e.3	O
)	O
l	O
(	O
x	O
,	O
λ	O
)	O
≡	O
f	O
(	O
x	O
)	O
+	O
λg	O
(	O
x	O
)	O
.	O
(	O
e.4	O
)	O
the	O
constrained	O
stationarity	O
condition	O
(	O
e.3	O
)	O
is	O
obtained	O
by	O
setting	O
∇xl	O
=	O
0.	O
fur-	O
thermore	O
,	O
the	O
condition	O
∂l/∂λ	O
=	O
0	O
leads	O
to	O
the	O
constraint	O
equation	O
g	O
(	O
x	O
)	O
=	O
0.	O
thus	O
to	O
ﬁnd	O
the	O
maximum	O
of	O
a	O
function	O
f	O
(	O
x	O
)	O
subject	O
to	O
the	O
constraint	O
g	O
(	O
x	O
)	O
=	O
0	O
,	O
we	O
deﬁne	O
the	O
lagrangian	O
function	O
given	O
by	O
(	O
e.4	O
)	O
and	O
we	O
then	O
ﬁnd	O
the	O
stationary	B
point	O
of	O
l	O
(	O
x	O
,	O
λ	O
)	O
with	O
respect	O
to	O
both	O
x	O
and	O
λ.	O
for	O
a	O
d-dimensional	O
vector	O
x	O
,	O
this	O
gives	O
d	O
+	O
1	O
equations	O
that	O
determine	O
both	O
the	O
stationary	B
point	O
x	O
(	O
cid:3	O
)	O
and	O
the	O
value	O
of	O
λ.	O
if	O
we	O
are	O
only	O
interested	O
in	O
x	O
(	O
cid:3	O
)	O
,	O
then	O
we	O
can	O
eliminate	O
λ	O
from	O
the	O
stationarity	O
equa-	O
tions	O
without	O
needing	O
to	O
ﬁnd	O
its	O
value	O
(	O
hence	O
the	O
term	O
‘	O
undetermined	B
multiplier	I
’	O
)	O
.	O
as	O
a	O
simple	O
example	O
,	O
suppose	O
we	O
wish	O
to	O
ﬁnd	O
the	O
stationary	B
point	O
of	O
the	O
function	O
f	O
(	O
x1	O
,	O
x2	O
)	O
=	O
1	O
−	O
x2	O
2	O
subject	O
to	O
the	O
constraint	O
g	O
(	O
x1	O
,	O
x2	O
)	O
=	O
x1	O
+	O
x2	O
−	O
1	O
=	O
0	O
,	O
as	O
illustrated	O
in	O
figure	O
e.2	O
.	O
the	O
corresponding	O
lagrangian	O
function	O
is	O
given	O
by	O
the	O
conditions	O
for	O
this	O
lagrangian	O
to	O
be	O
stationary	B
with	O
respect	O
to	O
x1	O
,	O
x2	O
,	O
and	O
λ	O
give	O
the	O
following	O
coupled	O
equations	O
:	O
1	O
−	O
x2	O
l	O
(	O
x	O
,	O
λ	O
)	O
=	O
1	O
−	O
x2	O
1	O
−	O
x2	O
2	O
+	O
λ	O
(	O
x1	O
+	O
x2	O
−	O
1	O
)	O
.	O
−2x1	O
+	O
λ	O
=	O
0	O
−2x2	O
+	O
λ	O
=	O
0	O
x1	O
+	O
x2	O
−	O
1	O
=	O
0	O
.	O
(	O
e.5	O
)	O
(	O
e.6	O
)	O
(	O
e.7	O
)	O
(	O
e.8	O
)	O
e.	O
lagrange	O
multipliers	O
709	O
figure	O
e.2	O
a	O
simple	O
example	O
of	O
the	O
use	O
of	O
lagrange	O
multipli-	O
ers	O
in	O
which	O
the	O
aim	O
is	O
to	O
maximize	O
f	O
(	O
x1	O
,	O
x2	O
)	O
=	O
1	O
−	O
x2	O
2	O
subject	O
to	O
the	O
constraint	O
g	O
(	O
x1	O
,	O
x2	O
)	O
=	O
0	O
where	O
g	O
(	O
x1	O
,	O
x2	O
)	O
=	O
x1	O
+	O
x2	O
−	O
1.	O
the	O
circles	O
show	O
contours	O
of	O
the	O
function	O
f	O
(	O
x1	O
,	O
x2	O
)	O
,	O
and	O
the	O
diagonal	B
line	O
shows	O
the	O
constraint	O
surface	O
g	O
(	O
x1	O
,	O
x2	O
)	O
=	O
0	O
.	O
1	O
−	O
x2	O
x2	O
(	O
x	O
(	O
cid:3	O
)	O
1	O
,	O
x	O
(	O
cid:3	O
)	O
2	O
)	O
x1	O
g	O
(	O
x1	O
,	O
x2	O
)	O
=	O
0	O
solution	O
of	O
these	O
equations	O
then	O
gives	O
the	O
stationary	B
point	O
as	O
(	O
x	O
(	O
cid:3	O
)	O
the	O
corresponding	O
value	O
for	O
the	O
lagrange	O
multiplier	O
is	O
λ	O
=	O
1	O
.	O
1	O
,	O
x	O
(	O
cid:3	O
)	O
2	O
)	O
=	O
(	O
1	O
2	O
,	O
1	O
2	O
)	O
,	O
and	O
so	O
far	O
,	O
we	O
have	O
considered	O
the	O
problem	O
of	O
maximizing	O
a	O
function	O
subject	O
to	O
an	O
equality	B
constraint	I
of	O
the	O
form	O
g	O
(	O
x	O
)	O
=	O
0.	O
we	O
now	O
consider	O
the	O
problem	O
of	O
maxi-	O
mizing	O
f	O
(	O
x	O
)	O
subject	O
to	O
an	O
inequality	B
constraint	I
of	O
the	O
form	O
g	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
0	O
,	O
as	O
illustrated	O
in	O
figure	O
e.3	O
.	O
there	O
are	O
now	O
two	O
kinds	O
of	O
solution	O
possible	O
,	O
according	O
to	O
whether	O
the	O
con-	O
strained	O
stationary	B
point	O
lies	O
in	O
the	O
region	O
where	O
g	O
(	O
x	O
)	O
>	O
0	O
,	O
in	O
which	O
case	O
the	O
con-	O
straint	O
is	O
inactive	O
,	O
or	O
whether	O
it	O
lies	O
on	O
the	O
boundary	O
g	O
(	O
x	O
)	O
=	O
0	O
,	O
in	O
which	O
case	O
the	O
constraint	O
is	O
said	O
to	O
be	O
active	O
.	O
in	O
the	O
former	O
case	O
,	O
the	O
function	O
g	O
(	O
x	O
)	O
plays	O
no	O
role	O
and	O
so	O
the	O
stationary	B
condition	O
is	O
simply	O
∇f	O
(	O
x	O
)	O
=	O
0.	O
this	O
again	O
corresponds	O
to	O
a	O
stationary	B
point	O
of	O
the	O
lagrange	O
function	O
(	O
e.4	O
)	O
but	O
this	O
time	O
with	O
λ	O
=	O
0.	O
the	O
latter	O
case	O
,	O
where	O
the	O
solution	O
lies	O
on	O
the	O
boundary	O
,	O
is	O
analogous	O
to	O
the	O
equality	O
con-	O
straint	O
discussed	O
previously	O
and	O
corresponds	O
to	O
a	O
stationary	B
point	O
of	O
the	O
lagrange	O
function	O
(	O
e.4	O
)	O
with	O
λ	O
(	O
cid:6	O
)	O
=	O
0.	O
now	O
,	O
however	O
,	O
the	O
sign	O
of	O
the	O
lagrange	O
multiplier	O
is	O
crucial	O
,	O
because	O
the	O
function	O
f	O
(	O
x	O
)	O
will	O
only	O
be	O
at	O
a	O
maximum	O
if	O
its	O
gradient	O
is	O
ori-	O
ented	O
away	O
from	O
the	O
region	O
g	O
(	O
x	O
)	O
>	O
0	O
,	O
as	O
illustrated	O
in	O
figure	O
e.3	O
.	O
we	O
therefore	O
have	O
∇f	O
(	O
x	O
)	O
=	O
−λ∇g	O
(	O
x	O
)	O
for	O
some	O
value	O
of	O
λ	O
>	O
0.	O
for	O
either	O
of	O
these	O
two	O
cases	O
,	O
the	O
product	O
λg	O
(	O
x	O
)	O
=	O
0.	O
thus	O
the	O
solution	O
to	O
the	O
figure	O
e.3	O
illustration	O
of	O
f	O
(	O
x	O
)	O
subject	O
g	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
0.	O
the	O
problem	O
of	O
maximizing	O
to	O
the	O
inequality	B
constraint	I
∇f	O
(	O
x	O
)	O
xa	O
∇g	O
(	O
x	O
)	O
xb	O
g	O
(	O
x	O
)	O
>	O
0	O
g	O
(	O
x	O
)	O
=	O
0	O
710	O
e.	O
lagrange	O
multipliers	O
problem	O
of	O
maximizing	O
f	O
(	O
x	O
)	O
subject	O
to	O
g	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
0	O
is	O
obtained	O
by	O
optimizing	O
the	O
lagrange	O
function	O
(	O
e.4	O
)	O
with	O
respect	O
to	O
x	O
and	O
λ	O
subject	O
to	O
the	O
conditions	O
g	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
0	O
λ	O
(	O
cid:2	O
)	O
0	O
λg	O
(	O
x	O
)	O
=	O
0	O
(	O
e.9	O
)	O
(	O
e.10	O
)	O
(	O
e.11	O
)	O
these	O
are	O
known	O
as	O
the	O
karush-kuhn-tucker	O
(	O
kkt	O
)	O
conditions	O
(	O
karush	O
,	O
1939	O
;	O
kuhn	O
and	O
tucker	O
,	O
1951	O
)	O
.	O
note	O
that	O
if	O
we	O
wish	O
to	O
minimize	O
(	O
rather	O
than	O
maximize	O
)	O
the	O
function	O
f	O
(	O
x	O
)	O
sub-	O
ject	O
to	O
an	O
inequality	B
constraint	I
g	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
0	O
,	O
then	O
we	O
minimize	O
the	O
lagrangian	O
function	O
l	O
(	O
x	O
,	O
λ	O
)	O
=	O
f	O
(	O
x	O
)	O
−	O
λg	O
(	O
x	O
)	O
with	O
respect	O
to	O
x	O
,	O
again	O
subject	O
to	O
λ	O
(	O
cid:2	O
)	O
0.	O
finally	O
,	O
it	O
is	O
straightforward	O
to	O
extend	O
the	O
technique	O
of	O
lagrange	O
multipliers	O
to	O
the	O
case	O
of	O
multiple	O
equality	O
and	O
inequality	O
constraints	O
.	O
suppose	O
we	O
wish	O
to	O
maxi-	O
mize	O
f	O
(	O
x	O
)	O
subject	O
to	O
gj	O
(	O
x	O
)	O
=	O
0	O
for	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
j	O
,	O
and	O
hk	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
0	O
for	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k.	O
we	O
then	O
introduce	O
lagrange	O
multipliers	O
{	O
λj	O
}	O
and	O
{	O
µk	O
}	O
,	O
and	O
then	O
optimize	O
the	O
la-	O
grangian	O
function	O
given	O
by	O
j	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
l	O
(	O
x	O
,	O
{	O
λj	O
}	O
,	O
{	O
µk	O
}	O
)	O
=	O
f	O
(	O
x	O
)	O
+	O
λjgj	O
(	O
x	O
)	O
+	O
µkhk	O
(	O
x	O
)	O
(	O
e.12	O
)	O
appendix	O
d	O
subject	O
to	O
µk	O
(	O
cid:2	O
)	O
0	O
and	O
µkhk	O
(	O
x	O
)	O
=	O
0	O
for	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k.	O
extensions	O
to	O
constrained	O
functional	B
derivatives	O
are	O
similarly	O
straightforward	O
.	O
for	O
a	O
more	O
detailed	O
discussion	O
of	O
the	O
technique	O
of	O
lagrange	O
multipliers	O
,	O
see	O
nocedal	O
and	O
wright	O
(	O
1999	O
)	O
.	O
j=1	O
k=1	O
references	O
711	O
references	O
abramowitz	O
,	O
m.	O
and	O
i.	O
a.	O
stegun	O
(	O
1965	O
)	O
.	O
handbook	O
of	O
mathematical	O
functions	O
.	O
dover	O
.	O
adler	O
,	O
s.	O
l.	O
(	O
1981	O
)	O
.	O
over-relaxation	B
method	O
for	O
the	O
monte	O
carlo	O
evaluation	O
of	O
the	O
partition	O
func-	O
tion	O
for	O
multiquadratic	O
actions	O
.	O
physical	O
review	O
d	O
23	O
,	O
2901–2904	O
.	O
ahn	O
,	O
j.	O
h.	O
and	O
j.	O
h.	O
oh	O
(	O
2003	O
)	O
.	O
a	O
constrained	O
em	O
algorithm	O
for	O
principal	O
component	O
analysis	O
.	O
neu-	O
ral	O
computation	O
15	O
(	O
1	O
)	O
,	O
57–65	O
.	O
aizerman	O
,	O
m.	O
a.	O
,	O
e.	O
m.	O
braverman	O
,	O
and	O
l.	O
i.	O
rozo-	O
noer	O
(	O
1964	O
)	O
.	O
the	O
probability	B
problem	O
of	O
pattern	O
recognition	O
learning	B
and	O
the	O
method	O
of	O
potential	O
functions	O
.	O
automation	O
and	O
remote	O
control	O
25	O
,	O
1175–1190	O
.	O
akaike	O
,	O
h.	O
(	O
1974	O
)	O
.	O
a	O
new	O
look	O
at	O
statistical	O
model	O
identiﬁcation	O
.	O
ieee	O
transactions	O
on	O
automatic	O
control	O
19	O
,	O
716–723	O
.	O
ali	O
,	O
s.	O
m.	O
and	O
s.	O
d.	O
silvey	O
(	O
1966	O
)	O
.	O
a	O
general	O
class	O
of	O
coefﬁcients	O
of	O
divergence	O
of	O
one	O
distribution	O
from	O
another	O
.	O
journal	O
of	O
the	O
royal	O
statistical	O
so-	O
ciety	O
,	O
b	O
28	O
(	O
1	O
)	O
,	O
131–142	O
.	O
allwein	O
,	O
e.	O
l.	O
,	O
r.	O
e.	O
schapire	O
,	O
and	O
y.	O
singer	O
(	O
2000	O
)	O
.	O
reducing	O
multiclass	B
to	O
binary	O
:	O
a	O
unifying	O
ap-	O
proach	O
for	O
margin	O
classiﬁers	O
.	O
journal	O
of	O
machine	O
learning	O
research	O
1	O
,	O
113–141	O
.	O
amari	O
,	O
s.	O
(	O
1985	O
)	O
.	O
differential-geometrical	O
methods	O
in	O
statistics	O
.	O
springer	O
.	O
amari	O
,	O
s.	O
,	O
a.	O
cichocki	O
,	O
and	O
h.	O
h.	O
yang	O
(	O
1996	O
)	O
.	O
a	O
new	O
learning	B
algorithm	O
for	O
blind	O
signal	O
separa-	O
tion	O
.	O
in	O
d.	O
s.	O
touretzky	O
,	O
m.	O
c.	O
mozer	O
,	O
and	O
m.	O
e.	O
hasselmo	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
informa-	O
tion	O
processing	O
systems	O
,	O
volume	O
8	O
,	O
pp	O
.	O
757–763	O
.	O
mit	O
press	O
.	O
amari	O
,	O
s.	O
i	O
.	O
(	O
1998	O
)	O
.	O
natural	O
gradient	O
works	O
efﬁ-	O
10	O
,	O
ciently	O
in	O
learning	B
.	O
neural	O
computation	O
251–276	O
.	O
anderson	O
,	O
j.	O
a.	O
and	O
e.	O
rosenfeld	O
(	O
eds	O
.	O
)	O
(	O
1988	O
)	O
.	O
neurocomputing	O
:	O
foundations	O
of	O
research	O
.	O
mit	O
press	O
.	O
anderson	O
,	O
t.	O
w.	O
(	O
1963	O
)	O
.	O
asymptotic	O
theory	B
for	O
prin-	O
cipal	O
component	O
analysis	O
.	O
annals	O
of	O
mathemati-	O
cal	O
statistics	O
34	O
,	O
122–148	O
.	O
andrieu	O
,	O
c.	O
,	O
n.	O
de	O
freitas	O
,	O
a.	O
doucet	O
,	O
and	O
m.	O
i.	O
jor-	O
dan	O
(	O
2003	O
)	O
.	O
an	O
introduction	O
to	O
mcmc	O
for	O
ma-	O
chine	O
learning	B
.	O
machine	O
learning	O
50	O
,	O
5–43	O
.	O
anthony	O
,	O
m.	O
and	O
n.	O
biggs	O
(	O
1992	O
)	O
.	O
an	O
introduction	O
to	O
computational	B
learning	I
theory	I
.	O
cambridge	O
university	O
press	O
.	O
attias	O
,	O
h.	O
(	O
1999a	O
)	O
.	O
independent	B
factor	I
analysis	I
.	O
neu-	O
ral	O
computation	O
11	O
(	O
4	O
)	O
,	O
803–851	O
.	O
attias	O
,	O
h.	O
(	O
1999b	O
)	O
.	O
inferring	O
parameters	O
and	O
struc-	O
ture	O
of	O
latent	B
variable	I
models	O
by	O
variational	B
bayes	O
.	O
in	O
k.	O
b.	O
laskey	O
and	O
h.	O
prade	O
(	O
eds	O
.	O
)	O
,	O
712	O
references	O
uncertainty	O
in	O
artiﬁcial	O
intelligence	O
:	O
proceed-	O
ings	O
of	O
the	O
fifth	O
conference	O
,	O
pp	O
.	O
21–30	O
.	O
morgan	O
kaufmann	O
.	O
bach	O
,	O
f.	O
r.	O
and	O
m.	O
i.	O
jordan	O
(	O
2002	O
)	O
.	O
kernel	O
inde-	O
pendent	O
component	O
analysis	O
.	O
journal	O
of	O
machine	O
learning	O
research	O
3	O
,	O
1–48	O
.	O
bakir	O
,	O
g.	O
h.	O
,	O
j.	O
weston	O
,	O
and	O
b.	O
sch¨olkopf	O
(	O
2004	O
)	O
.	O
learning	B
to	O
ﬁnd	O
pre-images	O
.	O
in	O
s.	O
thrun	O
,	O
l.	O
k.	O
saul	O
,	O
and	O
b.	O
sch¨olkopf	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neu-	O
ral	O
information	O
processing	O
systems	O
,	O
volume	O
16	O
,	O
pp	O
.	O
449–456	O
.	O
mit	O
press	O
.	O
baldi	O
,	O
p.	O
and	O
s.	O
brunak	O
(	O
2001	O
)	O
.	O
bioinformatics	O
:	O
the	O
machine	O
learning	O
approach	O
(	O
second	O
ed.	O
)	O
.	O
mit	O
press	O
.	O
baldi	O
,	O
p.	O
and	O
k.	O
hornik	O
(	O
1989	O
)	O
.	O
neural	O
networks	O
and	O
principal	B
component	I
analysis	I
:	O
learning	B
from	O
examples	O
without	O
local	B
minima	O
.	O
neural	O
net-	O
works	O
2	O
(	O
1	O
)	O
,	O
53–58	O
.	O
barber	O
,	O
d.	O
and	O
c.	O
m.	O
bishop	O
(	O
1997	O
)	O
.	O
bayesian	O
model	B
comparison	I
by	O
monte	O
carlo	O
chaining	B
.	O
in	O
m.	O
mozer	O
,	O
m.	O
jordan	O
,	O
and	O
t.	O
petsche	O
(	O
eds	O
.	O
)	O
,	O
ad-	O
vances	O
in	O
neural	O
information	O
processing	O
sys-	O
tems	O
,	O
volume	O
9	O
,	O
pp	O
.	O
333–339	O
.	O
mit	O
press	O
.	O
barber	O
,	O
d.	O
and	O
c.	O
m.	O
bishop	O
(	O
1998a	O
)	O
.	O
ensemble	O
learning	B
for	O
multi-layer	O
networks	O
.	O
in	O
m.	O
i.	O
jor-	O
dan	O
,	O
k.	O
j.	O
kearns	O
,	O
and	O
s.	O
a.	O
solla	O
(	O
eds	O
.	O
)	O
,	O
ad-	O
vances	O
in	O
neural	O
information	O
processing	O
sys-	O
tems	O
,	O
volume	O
10	O
,	O
pp	O
.	O
395–401	O
.	O
barber	O
,	O
d.	O
and	O
c.	O
m.	O
bishop	O
(	O
1998b	O
)	O
.	O
ensemble	O
learning	B
in	O
bayesian	O
neural	O
networks	O
.	O
in	O
c.	O
m.	O
bishop	O
(	O
ed	O
.	O
)	O
,	O
generalization	B
in	O
neural	O
networks	O
and	O
machine	O
learning	O
,	O
pp	O
.	O
215–237	O
.	O
springer	O
.	O
bartholomew	O
,	O
d.	O
j	O
.	O
(	O
1987	O
)	O
.	O
latent	B
variable	I
models	O
and	O
factor	B
analysis	I
.	O
charles	O
grifﬁn	O
.	O
basilevsky	O
,	O
a	O
.	O
(	O
1994	O
)	O
.	O
statistical	O
factor	O
analysis	O
and	O
related	O
methods	O
:	O
theory	B
and	O
applications	O
.	O
wiley	O
.	O
baum	O
,	O
l.	O
e.	O
(	O
1972	O
)	O
.	O
an	O
inequality	O
and	O
associated	O
maximization	O
technique	O
in	O
statistical	O
estimation	O
of	O
probabilistic	O
functions	O
of	O
markov	O
processes	O
.	O
inequalities	O
3	O
,	O
1–8	O
.	O
becker	O
,	O
s.	O
and	O
y.	O
le	O
cun	O
(	O
1989	O
)	O
.	O
improving	O
the	O
con-	O
vergence	O
of	O
back-propagation	O
learning	B
with	O
sec-	O
ond	O
order	O
methods	O
.	O
in	O
d.	O
touretzky	O
,	O
g.	O
e.	O
hin-	O
ton	O
,	O
and	O
t.	O
j.	O
sejnowski	O
(	O
eds	O
.	O
)	O
,	O
proceedings	O
of	O
the	O
1988	O
connectionist	O
models	O
summer	O
school	O
,	O
pp	O
.	O
29–37	O
.	O
morgan	O
kaufmann	O
.	O
bell	O
,	O
a.	O
j.	O
and	O
t.	O
j.	O
sejnowski	O
(	O
1995	O
)	O
.	O
an	O
infor-	O
mation	B
maximization	O
approach	O
to	O
blind	O
separa-	O
tion	O
and	O
blind	O
deconvolution	O
.	O
neural	O
computa-	O
tion	O
7	O
(	O
6	O
)	O
,	O
1129–1159	O
.	O
bellman	O
,	O
r.	O
(	O
1961	O
)	O
.	O
adaptive	O
control	O
processes	O
:	O
a	O
guided	O
tour	O
.	O
princeton	O
university	O
press	O
.	O
bengio	O
,	O
y.	O
and	O
p.	O
frasconi	O
(	O
1995	O
)	O
.	O
an	O
input	O
output	O
hmm	O
architecture	O
.	O
in	O
g.	O
tesauro	O
,	O
d.	O
s.	O
touret-	O
zky	O
,	O
and	O
t.	O
k.	O
leen	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
volume	O
7	O
,	O
pp	O
.	O
427–434	O
.	O
mit	O
press	O
.	O
bennett	O
,	O
k.	O
p.	O
(	O
1992	O
)	O
.	O
robust	O
linear	O
programming	O
discrimination	O
of	O
two	O
linearly	B
separable	I
sets	O
.	O
op-	O
timization	O
methods	O
and	O
software	O
1	O
,	O
23–34	O
.	O
berger	O
,	O
j.	O
o	O
.	O
(	O
1985	O
)	O
.	O
statistical	O
decision	O
theory	B
and	O
bayesian	O
analysis	O
(	O
second	O
ed.	O
)	O
.	O
springer	O
.	O
bernardo	O
,	O
j.	O
m.	O
and	O
a.	O
f.	O
m.	O
smith	O
(	O
1994	O
)	O
.	O
bayesian	O
theory	B
.	O
wiley	O
.	O
berrou	O
,	O
c.	O
,	O
a.	O
glavieux	O
,	O
and	O
p.	O
thitimajshima	O
(	O
1993	O
)	O
.	O
near	O
shannon	O
limit	O
error-correcting	O
cod-	O
ing	O
and	O
decoding	O
:	O
turbo-codes	O
(	O
1	O
)	O
.	O
in	O
proceed-	O
ings	O
icc	O
’	O
93	O
,	O
pp	O
.	O
1064–1070	O
.	O
besag	O
,	O
j	O
.	O
(	O
1974	O
)	O
.	O
on	O
spatio-temporal	O
models	O
and	O
markov	O
ﬁelds	O
.	O
in	O
transactions	O
of	O
the	O
7th	O
prague	O
conference	O
on	O
information	B
theory	I
,	O
statistical	O
decision	O
functions	O
and	O
random	O
processes	O
,	O
pp	O
.	O
47–75	O
.	O
academia	O
.	O
bather	O
,	O
j	O
.	O
(	O
2000	O
)	O
.	O
decision	B
theory	I
:	O
an	O
introduction	O
to	O
dynamic	B
programming	I
and	O
sequential	O
deci-	O
sions	O
.	O
wiley	O
.	O
besag	O
,	O
j	O
.	O
(	O
1986	O
)	O
.	O
on	O
the	O
statistical	O
analysis	O
of	O
dirty	O
pictures	O
.	O
journal	O
of	O
the	O
royal	O
statistical	O
soci-	O
ety	O
b-48	O
,	O
259–302	O
.	O
baudat	O
,	O
g.	O
and	O
f.	O
anouar	O
(	O
2000	O
)	O
.	O
generalized	B
dis-	O
criminant	O
analysis	O
using	O
a	O
kernel	O
approach	O
.	O
neu-	O
ral	O
computation	O
12	O
(	O
10	O
)	O
,	O
2385–2404	O
.	O
besag	O
,	O
j.	O
,	O
p.	O
j.	O
green	O
,	O
d.	O
hidgon	O
,	O
and	O
k.	O
megersen	O
(	O
1995	O
)	O
.	O
bayesian	O
computation	O
and	O
stochastic	B
systems	O
.	O
statistical	O
science	O
10	O
(	O
1	O
)	O
,	O
3–66	O
.	O
references	O
713	O
bishop	O
,	O
c.	O
m.	O
(	O
1991	O
)	O
.	O
a	O
fast	O
procedure	O
for	O
retraining	O
the	O
multilayer	B
perceptron	I
.	O
international	O
journal	O
of	O
neural	O
systems	O
2	O
(	O
3	O
)	O
,	O
229–236	O
.	O
bishop	O
,	O
c.	O
m.	O
(	O
1992	O
)	O
.	O
exact	O
calculation	O
of	O
the	O
hes-	O
sian	O
matrix	O
for	O
the	O
multilayer	B
perceptron	I
.	O
neural	O
computation	O
4	O
(	O
4	O
)	O
,	O
494–501	O
.	O
bishop	O
,	O
c.	O
m.	O
(	O
1993	O
)	O
.	O
curvature-driven	O
smoothing	O
:	O
a	O
learning	B
algorithm	O
for	O
feedforward	O
networks	O
.	O
ieee	O
transactions	O
on	O
neural	O
networks	O
4	O
(	O
5	O
)	O
,	O
882–884	O
.	O
bishop	O
,	O
c.	O
m.	O
(	O
1994	O
)	O
.	O
novelty	B
detection	I
and	O
neu-	O
ral	O
network	O
validation	O
.	O
iee	O
proceedings	O
:	O
vision	O
,	O
image	O
and	O
signal	O
processing	O
141	O
(	O
4	O
)	O
,	O
217–222	O
.	O
special	O
issue	O
on	O
applications	O
of	O
neural	O
networks	O
.	O
bishop	O
,	O
c.	O
m.	O
(	O
1995a	O
)	O
.	O
neural	O
networks	O
for	O
pattern	O
recognition	O
.	O
oxford	O
university	O
press	O
.	O
bishop	O
,	O
c.	O
m.	O
(	O
1995b	O
)	O
.	O
training	B
with	O
noise	O
is	O
equiv-	O
alent	O
to	O
tikhonov	O
regularization	B
.	O
neural	O
compu-	O
tation	O
7	O
(	O
1	O
)	O
,	O
108–116	O
.	O
bishop	O
,	O
c.	O
m.	O
(	O
1999a	O
)	O
.	O
bayesian	O
pca	O
.	O
in	O
m.	O
s.	O
kearns	O
,	O
s.	O
a.	O
solla	O
,	O
and	O
d.	O
a.	O
cohn	O
(	O
eds	O
.	O
)	O
,	O
ad-	O
vances	O
in	O
neural	O
information	O
processing	O
sys-	O
tems	O
,	O
volume	O
11	O
,	O
pp	O
.	O
382–388	O
.	O
mit	O
press	O
.	O
(	O
1999b	O
)	O
.	O
variational	B
principal	O
bishop	O
,	O
c.	O
m.	O
components	O
.	O
in	O
proceedings	O
ninth	O
interna-	O
tional	O
conference	O
on	O
artiﬁcial	O
neural	O
networks	O
,	O
icann	O
’	O
99	O
,	O
volume	O
1	O
,	O
pp	O
.	O
509–514	O
.	O
iee	O
.	O
bishop	O
,	O
c.	O
m.	O
and	O
g.	O
d.	O
james	O
(	O
1993	O
)	O
.	O
analysis	O
of	O
multiphase	O
ﬂows	O
using	O
dual-energy	O
gamma	O
den-	O
sitometry	O
and	O
neural	O
networks	O
.	O
nuclear	O
instru-	O
ments	O
and	O
methods	O
in	O
physics	O
research	O
a327	O
,	O
580–593	O
.	O
bishop	O
,	O
c.	O
m.	O
and	O
i.	O
t.	O
nabney	O
(	O
1996	O
)	O
.	O
modelling	O
conditional	B
probability	I
distributions	O
for	O
periodic	O
variables	O
.	O
neural	O
computation	O
8	O
(	O
5	O
)	O
,	O
1123–1133	O
.	O
bishop	O
,	O
c.	O
m.	O
and	O
i.	O
t.	O
nabney	O
(	O
2008	O
)	O
.	O
pattern	O
recognition	O
and	O
machine	O
learning	O
:	O
a	O
matlab	O
companion	O
.	O
springer	O
.	O
in	O
preparation	O
.	O
bishop	O
,	O
c.	O
m.	O
,	O
d.	O
spiegelhalter	O
,	O
and	O
j.	O
winn	O
(	O
2003	O
)	O
.	O
vibes	O
:	O
a	O
variational	B
inference	I
engine	O
for	O
bayesian	O
networks	O
.	O
in	O
s.	O
becker	O
,	O
s.	O
thrun	O
,	O
and	O
k.	O
obermeyer	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
volume	O
15	O
,	O
pp	O
.	O
793–800	O
.	O
mit	O
press	O
.	O
bishop	O
,	O
c.	O
m.	O
and	O
m.	O
svens´en	O
(	O
2003	O
)	O
.	O
bayesian	O
hi-	O
erarchical	O
mixtures	O
of	O
experts	O
.	O
in	O
u.	O
kjaerulff	O
and	O
c.	O
meek	O
(	O
eds	O
.	O
)	O
,	O
proceedings	O
nineteenth	O
conference	O
on	O
uncertainty	O
in	O
artiﬁcial	O
intelli-	O
gence	O
,	O
pp	O
.	O
57–64	O
.	O
morgan	O
kaufmann	O
.	O
bishop	O
,	O
c.	O
m.	O
,	O
m.	O
svens´en	O
,	O
and	O
g.	O
e.	O
hinton	O
(	O
2004	O
)	O
.	O
distinguishing	O
text	O
from	O
graphics	O
in	O
on-	O
line	O
handwritten	O
ink	O
.	O
in	O
f.	O
kimura	O
and	O
h.	O
fu-	O
jisawa	O
(	O
eds	O
.	O
)	O
,	O
proceedings	O
ninth	O
international	O
workshop	O
on	O
frontiers	O
in	O
handwriting	O
recogni-	O
tion	O
,	O
iwfhr-9	O
,	O
tokyo	O
,	O
japan	O
,	O
pp	O
.	O
142–147	O
.	O
bishop	O
,	O
c.	O
m.	O
,	O
m.	O
svens´en	O
,	O
and	O
c.	O
k.	O
i.	O
williams	O
(	O
1996	O
)	O
.	O
em	O
optimization	O
of	O
latent	B
variable	I
den-	O
sity	O
models	O
.	O
in	O
d.	O
s.	O
touretzky	O
,	O
m.	O
c.	O
mozer	O
,	O
and	O
m.	O
e.	O
hasselmo	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
volume	O
8	O
,	O
pp	O
.	O
465–471	O
.	O
mit	O
press	O
.	O
bishop	O
,	O
c.	O
m.	O
,	O
m.	O
svens´en	O
,	O
and	O
c.	O
k.	O
i.	O
williams	O
(	O
1997a	O
)	O
.	O
gtm	O
:	O
a	O
principled	O
alternative	O
to	O
the	O
self-organizing	B
map	I
.	O
in	O
m.	O
c.	O
mozer	O
,	O
m.	O
i.	O
jor-	O
dan	O
,	O
and	O
t.	O
petche	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
volume	O
9	O
,	O
pp	O
.	O
354–360	O
.	O
mit	O
press	O
.	O
bishop	O
,	O
c.	O
m.	O
,	O
m.	O
svens´en	O
,	O
and	O
c.	O
k.	O
i.	O
williams	O
(	O
1997b	O
)	O
.	O
magniﬁcation	O
factors	O
for	O
the	O
gtm	O
al-	O
gorithm	O
.	O
in	O
proceedings	O
iee	O
fifth	O
international	O
conference	O
on	O
artiﬁcial	O
neural	O
networks	O
,	O
cam-	O
bridge	O
,	O
u.k.	O
,	O
pp	O
.	O
64–69	O
.	O
institute	O
of	O
electrical	O
engineers	O
.	O
bishop	O
,	O
c.	O
m.	O
,	O
m.	O
svens´en	O
,	O
and	O
c.	O
k.	O
i.	O
williams	O
(	O
1998a	O
)	O
.	O
developments	O
of	O
the	O
generative	O
to-	O
pographic	O
mapping	O
.	O
neurocomputing	O
21	O
,	O
203–	O
224.	O
bishop	O
,	O
c.	O
m.	O
,	O
m.	O
svens´en	O
,	O
and	O
c.	O
k.	O
i.	O
williams	O
(	O
1998b	O
)	O
.	O
gtm	O
:	O
the	O
generative	B
topographic	I
mapping	I
.	O
neural	O
computation	O
10	O
(	O
1	O
)	O
,	O
215–234	O
.	O
bishop	O
,	O
c.	O
m.	O
and	O
m.	O
e.	O
tipping	O
(	O
1998	O
)	O
.	O
a	O
hier-	O
archical	O
latent	B
variable	I
model	O
for	O
data	O
visualiza-	O
tion	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
20	O
(	O
3	O
)	O
,	O
281–293	O
.	O
714	O
references	O
bishop	O
,	O
c.	O
m.	O
and	O
j.	O
winn	O
(	O
2000	O
)	O
.	O
non-linear	O
bayesian	O
image	O
modelling	O
.	O
in	O
proceedings	O
sixth	O
european	O
conference	O
on	O
computer	O
vision	O
,	O
dublin	O
,	O
volume	O
1	O
,	O
pp	O
.	O
3–17	O
.	O
springer	O
.	O
blei	O
,	O
d.	O
m.	O
,	O
m.	O
i.	O
jordan	O
,	O
and	O
a.	O
y.	O
ng	O
(	O
2003	O
)	O
.	O
hi-	O
erarchical	O
bayesian	O
models	O
for	O
applications	O
in	O
information	O
retrieval	O
.	O
in	O
j.	O
m.	O
b.	O
et	O
al	O
.	O
(	O
ed	O
.	O
)	O
,	O
bayesian	O
statistics	O
,	O
7	O
,	O
pp	O
.	O
25–43	O
.	O
oxford	O
uni-	O
versity	O
press	O
.	O
block	O
,	O
h.	O
d.	O
(	O
1962	O
)	O
.	O
the	O
perceptron	B
:	O
a	O
model	O
for	O
brain	O
functioning	O
.	O
reviews	O
of	O
modern	O
physics	O
34	O
(	O
1	O
)	O
,	O
123–135	O
.	O
reprinted	O
in	O
anderson	O
and	O
rosenfeld	O
(	O
1988	O
)	O
.	O
blum	O
,	O
j.	O
a	O
.	O
(	O
1965	O
)	O
.	O
multidimensional	O
stochastic	O
ap-	O
proximation	B
methods	O
.	O
annals	O
of	O
mathematical	O
statistics	O
25	O
,	O
737–744	O
.	O
bodlaender	O
,	O
h.	O
(	O
1993	O
)	O
.	O
a	O
tourist	O
guide	O
through	O
treewidth	B
.	O
acta	O
cybernetica	O
11	O
,	O
1–21	O
.	O
boser	O
,	O
b.	O
e.	O
,	O
i.	O
m.	O
guyon	O
,	O
and	O
v.	O
n.	O
vapnik	O
(	O
1992	O
)	O
.	O
a	O
training	B
algorithm	O
for	O
optimal	O
margin	B
classi-	O
ﬁers	O
.	O
in	O
d.	O
haussler	O
(	O
ed	O
.	O
)	O
,	O
proceedings	O
fifth	O
an-	O
nual	O
workshop	O
on	O
computational	O
learning	O
the-	O
ory	O
(	O
colt	O
)	O
,	O
pp	O
.	O
144–152	O
.	O
acm	O
.	O
bourlard	O
,	O
h.	O
and	O
y.	O
kamp	O
(	O
1988	O
)	O
.	O
auto-association	O
by	O
multilayer	O
perceptrons	O
and	O
singular	O
value	O
de-	O
composition	O
.	O
biological	O
cybernetics	O
59	O
,	O
291–	O
294.	O
box	O
,	O
g.	O
e.	O
p.	O
,	O
g.	O
m.	O
jenkins	O
,	O
and	O
g.	O
c.	O
reinsel	O
(	O
1994	O
)	O
.	O
time	O
series	O
analysis	O
.	O
prentice	O
hall	O
.	O
box	O
,	O
g.	O
e.	O
p.	O
and	O
g.	O
c.	O
tao	O
(	O
1973	O
)	O
.	O
bayesian	O
infer-	O
ence	O
in	O
statistical	O
analysis	O
.	O
wiley	O
.	O
boyd	O
,	O
s.	O
and	O
l.	O
vandenberghe	O
(	O
2004	O
)	O
.	O
convex	O
opti-	O
mization	O
.	O
cambridge	O
university	O
press	O
.	O
boyen	O
,	O
x.	O
and	O
d.	O
koller	O
(	O
1998	O
)	O
.	O
tractable	O
inference	B
for	O
complex	O
stochastic	B
processes	O
.	O
in	O
g.	O
f.	O
cooper	O
and	O
s.	O
moral	O
(	O
eds	O
.	O
)	O
,	O
proceedings	O
14th	O
annual	O
conference	O
on	O
uncertainty	O
in	O
artiﬁcial	O
intelli-	O
gence	O
(	O
uai	O
)	O
,	O
pp	O
.	O
33–42	O
.	O
morgan	O
kaufmann	O
.	O
breiman	O
,	O
l.	O
(	O
1996	O
)	O
.	O
bagging	B
predictors	O
.	O
machine	O
learning	O
26	O
,	O
123–140	O
.	O
breiman	O
,	O
l.	O
,	O
j.	O
h.	O
friedman	O
,	O
r.	O
a.	O
olshen	O
,	O
and	O
p.	O
j.	O
stone	O
(	O
1984	O
)	O
.	O
classiﬁcation	B
and	I
regression	I
trees	I
.	O
wadsworth	O
.	O
brooks	O
,	O
s.	O
p.	O
(	O
1998	O
)	O
.	O
markov	O
chain	O
monte	O
carlo	O
method	O
and	O
its	O
application	O
.	O
the	O
statisti-	O
cian	O
47	O
(	O
1	O
)	O
,	O
69–100	O
.	O
broomhead	O
,	O
d.	O
s.	O
and	O
d.	O
lowe	O
(	O
1988	O
)	O
.	O
multivari-	O
able	O
functional	B
interpolation	O
and	O
adaptive	O
net-	O
works	O
.	O
complex	O
systems	O
2	O
,	O
321–355	O
.	O
buntine	O
,	O
w.	O
and	O
a.	O
weigend	O
(	O
1991	O
)	O
.	O
bayesian	O
back-	O
propagation	O
.	O
complex	O
systems	O
5	O
,	O
603–643	O
.	O
buntine	O
,	O
w.	O
l.	O
and	O
a.	O
s.	O
weigend	O
(	O
1993	O
)	O
.	O
com-	O
puting	O
second	O
derivatives	O
in	O
feed-forward	O
net-	O
works	O
:	O
a	O
review	O
.	O
ieee	O
transactions	O
on	O
neural	O
networks	O
5	O
(	O
3	O
)	O
,	O
480–488	O
.	O
burges	O
,	O
c.	O
j.	O
c.	O
(	O
1998	O
)	O
.	O
a	O
tutorial	O
on	O
support	O
vec-	O
tor	O
machines	O
for	O
pattern	O
recognition	O
.	O
knowledge	O
discovery	O
and	O
data	O
mining	O
2	O
(	O
2	O
)	O
,	O
121–167	O
.	O
cardoso	O
,	O
j.-f.	O
(	O
1998	O
)	O
.	O
blind	O
signal	O
separation	O
:	O
statis-	O
tical	O
principles	O
.	O
proceedings	O
of	O
the	O
ieee	O
9	O
(	O
10	O
)	O
,	O
2009–2025	O
.	O
casella	O
,	O
g.	O
and	O
r.	O
l.	O
berger	O
(	O
2002	O
)	O
.	O
statistical	O
in-	O
ference	O
(	O
second	O
ed.	O
)	O
.	O
duxbury	O
.	O
castillo	O
,	O
e.	O
,	O
j.	O
m.	O
guti´errez	O
,	O
and	O
a.	O
s.	O
hadi	O
(	O
1997	O
)	O
.	O
expert	O
systems	O
and	O
probabilistic	O
network	O
mod-	O
els	O
.	O
springer	O
.	O
chan	O
,	O
k.	O
,	O
t.	O
lee	O
,	O
and	O
t.	O
j.	O
sejnowski	O
(	O
2003	O
)	O
.	O
vari-	O
ational	O
bayesian	O
learning	B
of	O
ica	O
with	O
missing	B
data	I
.	O
neural	O
computation	O
15	O
(	O
8	O
)	O
,	O
1991–2011	O
.	O
chen	O
,	O
a.	O
m.	O
,	O
h.	O
lu	O
,	O
and	O
r.	O
hecht-nielsen	O
(	O
1993	O
)	O
.	O
on	O
the	O
geometry	O
of	O
feedforward	O
neural	B
network	I
error	O
surfaces	O
.	O
neural	O
computation	O
5	O
(	O
6	O
)	O
,	O
910–	O
927.	O
chen	O
,	O
m.	O
h.	O
,	O
q.	O
m.	O
shao	O
,	O
and	O
j.	O
g.	O
ibrahim	O
(	O
eds	O
.	O
)	O
(	O
2001	O
)	O
.	O
monte	O
carlo	O
methods	O
for	O
bayesian	O
com-	O
putation	O
.	O
springer	O
.	O
boykov	O
,	O
y.	O
,	O
o.	O
veksler	O
,	O
and	O
r.	O
zabih	O
(	O
2001	O
)	O
.	O
fast	O
approximate	O
energy	O
minimization	O
via	O
graph	O
cuts	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
ma-	O
chine	O
intelligence	O
23	O
(	O
11	O
)	O
,	O
1222–1239	O
.	O
chen	O
,	O
s.	O
,	O
c.	O
f.	O
n.	O
cowan	O
,	O
and	O
p.	O
m.	O
grant	O
(	O
1991	O
)	O
.	O
orthogonal	B
least	I
squares	I
learning	O
algorithm	O
for	O
radial	O
basis	B
function	I
networks	O
.	O
ieee	O
transac-	O
tions	O
on	O
neural	O
networks	O
2	O
(	O
2	O
)	O
,	O
302–309	O
.	O
choudrey	O
,	O
r.	O
a.	O
and	O
s.	O
j.	O
roberts	O
(	O
2003	O
)	O
.	O
variational	B
mixture	O
of	O
bayesian	O
independent	O
component	O
an-	O
alyzers	O
.	O
neural	O
computation	O
15	O
(	O
1	O
)	O
,	O
213–252	O
.	O
clifford	O
,	O
p.	O
(	O
1990	O
)	O
.	O
markov	O
random	O
ﬁelds	O
in	O
statis-	O
tics	O
.	O
in	O
g.	O
r.	O
grimmett	O
and	O
d.	O
j.	O
a.	O
welsh	O
(	O
eds	O
.	O
)	O
,	O
disorder	O
in	O
physical	O
systems	O
.	O
a	O
volume	O
in	O
hon-	O
our	O
of	O
john	O
m.	O
hammersley	O
,	O
pp	O
.	O
19–32	O
.	O
oxford	O
university	O
press	O
.	O
collins	O
,	O
m.	O
,	O
s.	O
dasgupta	O
,	O
and	O
r.	O
e.	O
schapire	O
(	O
2002	O
)	O
.	O
a	O
generalization	B
of	O
principal	O
component	O
analy-	O
sis	O
to	O
the	O
exponential	B
family	I
.	O
in	O
t.	O
g.	O
dietterich	O
,	O
s.	O
becker	O
,	O
and	O
z.	O
ghahramani	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
vol-	O
ume	O
14	O
,	O
pp	O
.	O
617–624	O
.	O
mit	O
press	O
.	O
comon	O
,	O
p.	O
,	O
c.	O
jutten	O
,	O
and	O
j.	O
herault	O
(	O
1991	O
)	O
.	O
blind	B
source	I
separation	I
,	O
2	O
:	O
problems	O
statement	O
.	O
signal	O
processing	O
24	O
(	O
1	O
)	O
,	O
11–20	O
.	O
corduneanu	O
,	O
a.	O
and	O
c.	O
m.	O
bishop	O
(	O
2001	O
)	O
.	O
vari-	O
ational	O
bayesian	O
model	B
selection	I
for	O
mixture	B
distributions	O
.	O
in	O
t.	O
richardson	O
and	O
t.	O
jaakkola	O
(	O
eds	O
.	O
)	O
,	O
proceedings	O
eighth	O
international	O
confer-	O
ence	O
on	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
,	O
pp	O
.	O
27–34	O
.	O
morgan	O
kaufmann	O
.	O
cormen	O
,	O
t.	O
h.	O
,	O
c.	O
e.	O
leiserson	O
,	O
r.	O
l.	O
rivest	O
,	O
and	O
c.	O
stein	O
(	O
2001	O
)	O
.	O
introduction	O
to	O
algorithms	O
(	O
sec-	O
ond	O
ed.	O
)	O
.	O
mit	O
press	O
.	O
cortes	O
,	O
c.	O
and	O
v.	O
n.	O
vapnik	O
(	O
1995	O
)	O
.	O
support	B
vector	I
networks	O
.	O
machine	O
learning	O
20	O
,	O
273–297	O
.	O
cotter	O
,	O
n.	O
e.	O
(	O
1990	O
)	O
.	O
the	O
stone-weierstrass	O
theo-	O
rem	O
and	O
its	O
application	O
to	O
neural	O
networks	O
.	O
ieee	O
transactions	O
on	O
neural	O
networks	O
1	O
(	O
4	O
)	O
,	O
290–295	O
.	O
cover	O
,	O
t.	O
and	O
p.	O
hart	O
(	O
1967	O
)	O
.	O
nearest	O
neighbor	O
pat-	O
tern	O
classiﬁcation	B
.	O
ieee	O
transactions	O
on	O
infor-	O
mation	B
theory	O
it-11	O
,	O
21–27	O
.	O
cover	O
,	O
t.	O
m.	O
and	O
j.	O
a.	O
thomas	O
(	O
1991	O
)	O
.	O
elements	O
of	O
information	B
theory	I
.	O
wiley	O
.	O
cowell	O
,	O
r.	O
g.	O
,	O
a.	O
p.	O
dawid	O
,	O
s.	O
l.	O
lauritzen	O
,	O
and	O
d.	O
j.	O
spiegelhalter	O
(	O
1999	O
)	O
.	O
probabilistic	O
networks	O
and	O
expert	O
systems	O
.	O
springer	O
.	O
cox	O
,	O
r.	O
t.	O
(	O
1946	O
)	O
.	O
probability	B
,	O
frequency	O
and	O
reasonable	O
expectation	B
.	O
american	O
journal	O
of	O
physics	O
14	O
(	O
1	O
)	O
,	O
1–13	O
.	O
references	O
715	O
cox	O
,	O
t.	O
f.	O
and	O
m.	O
a.	O
a.	O
cox	O
(	O
2000	O
)	O
.	O
multidimen-	O
sional	O
scaling	O
(	O
second	O
ed.	O
)	O
.	O
chapman	O
and	O
hall	O
.	O
cressie	O
,	O
n.	O
(	O
1993	O
)	O
.	O
statistics	O
for	O
spatial	O
data	O
.	O
wiley	O
.	O
cristianini	O
,	O
n.	O
and	O
j.	O
shawe-taylor	O
(	O
2000	O
)	O
.	O
support	B
vector	I
machines	O
and	O
other	O
kernel-based	O
learning	B
methods	O
.	O
cambridge	O
university	O
press	O
.	O
csat´o	O
,	O
l.	O
and	O
m.	O
opper	O
(	O
2002	O
)	O
.	O
sparse	O
on-line	O
gaus-	O
sian	O
processes	O
.	O
neural	O
computation	O
14	O
(	O
3	O
)	O
,	O
641–	O
668.	O
csisz`ar	O
,	O
i.	O
and	O
g.	O
tusn`ady	O
(	O
1984	O
)	O
.	O
information	O
ge-	O
ometry	O
and	O
alternating	O
minimization	O
procedures	O
.	O
statistics	O
and	O
decisions	O
1	O
(	O
1	O
)	O
,	O
205–237	O
.	O
cybenko	O
,	O
g.	O
(	O
1989	O
)	O
.	O
approximation	O
by	O
superposi-	O
tions	O
of	O
a	O
sigmoidal	O
function	O
.	O
mathematics	O
of	O
control	O
,	O
signals	O
and	O
systems	O
2	O
,	O
304–314	O
.	O
dawid	O
,	O
a.	O
p.	O
(	O
1979	O
)	O
.	O
conditional	B
independence	I
in	O
statistical	O
theory	O
(	O
with	O
discussion	O
)	O
.	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
,	O
series	O
b	O
4	O
,	O
1–31	O
.	O
dawid	O
,	O
a.	O
p.	O
(	O
1980	O
)	O
.	O
conditional	B
independence	I
for	O
statistical	O
operations	O
.	O
annals	O
of	O
statistics	O
8	O
,	O
598–	O
617.	O
definetti	O
,	O
b	O
.	O
(	O
1970	O
)	O
.	O
theory	B
of	O
probability	B
.	O
wiley	O
and	O
sons	O
.	O
dempster	O
,	O
a.	O
p.	O
,	O
n.	O
m.	O
laird	O
,	O
and	O
d.	O
b.	O
rubin	O
(	O
1977	O
)	O
.	O
maximum	B
likelihood	I
from	O
incomplete	O
data	O
via	O
the	O
em	O
algorithm	O
.	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
,	O
b	O
39	O
(	O
1	O
)	O
,	O
1–38	O
.	O
denison	O
,	O
d.	O
g.	O
t.	O
,	O
c.	O
c.	O
holmes	O
,	O
b.	O
k.	O
mallick	O
,	O
and	O
a.	O
f.	O
m.	O
smith	O
(	O
2002	O
)	O
.	O
bayesian	O
methods	O
for	O
nonlinear	O
classiﬁcation	O
and	O
regression	O
.	O
wiley	O
.	O
diaconis	O
,	O
p.	O
and	O
l.	O
saloff-coste	O
(	O
1998	O
)	O
.	O
what	O
do	O
we	O
know	O
about	O
the	O
metropolis	O
algorithm	O
?	O
journal	O
of	O
computer	O
and	O
system	O
sciences	O
57	O
,	O
20–36	O
.	O
dietterich	O
,	O
t.	O
g.	O
and	O
g.	O
bakiri	O
(	O
1995	O
)	O
.	O
solving	O
multiclass	B
learning	O
problems	O
via	O
error-correcting	B
output	I
codes	I
.	O
journal	O
of	O
artiﬁcial	O
intelligence	O
research	O
2	O
,	O
263–286	O
.	O
duane	O
,	O
s.	O
,	O
a.	O
d.	O
kennedy	O
,	O
b.	O
j.	O
pendleton	O
,	O
and	O
d.	O
roweth	O
(	O
1987	O
)	O
.	O
hybrid	O
monte	O
carlo	O
.	O
physics	O
letters	O
b	O
195	O
(	O
2	O
)	O
,	O
216–222	O
.	O
duda	O
,	O
r.	O
o.	O
and	O
p.	O
e.	O
hart	O
(	O
1973	O
)	O
.	O
pattern	O
classiﬁ-	O
cation	O
and	O
scene	O
analysis	O
.	O
wiley	O
.	O
716	O
references	O
duda	O
,	O
r.	O
o.	O
,	O
p.	O
e.	O
hart	O
,	O
and	O
d.	O
g.	O
stork	O
(	O
2001	O
)	O
.	O
pat-	O
fletcher	O
,	O
r.	O
(	O
1987	O
)	O
.	O
practical	O
methods	O
of	O
optimiza-	O
tern	O
classiﬁcation	B
(	O
second	O
ed.	O
)	O
.	O
wiley	O
.	O
tion	O
(	O
second	O
ed.	O
)	O
.	O
wiley	O
.	O
durbin	O
,	O
r.	O
,	O
s.	O
eddy	O
,	O
a.	O
krogh	O
,	O
and	O
g.	O
mitchi-	O
son	O
(	O
1998	O
)	O
.	O
biological	B
sequence	I
analysis	O
.	O
cam-	O
bridge	O
university	O
press	O
.	O
dybowski	O
,	O
r.	O
and	O
s.	O
roberts	O
(	O
2005	O
)	O
.	O
an	O
anthology	O
of	O
probabilistic	O
models	O
for	O
medical	O
informatics	O
.	O
in	O
d.	O
husmeier	O
,	O
r.	O
dybowski	O
,	O
and	O
s.	O
roberts	O
(	O
eds	O
.	O
)	O
,	O
probabilistic	O
modeling	O
in	O
bioinformatics	O
and	O
medical	O
informatics	O
,	O
pp	O
.	O
297–349	O
.	O
springer	O
.	O
efron	O
,	O
b	O
.	O
(	O
1979	O
)	O
.	O
bootstrap	B
methods	O
:	O
another	O
look	O
at	O
the	O
jackknife	O
.	O
annals	O
of	O
statistics	O
7	O
,	O
1–26	O
.	O
elkan	O
,	O
c.	O
(	O
2003	O
)	O
.	O
using	O
the	O
triangle	O
inequality	O
to	O
ac-	O
celerate	O
k-means	O
.	O
in	O
proceedings	O
of	O
the	O
twelfth	O
international	O
conference	O
on	O
machine	O
learning	O
,	O
pp	O
.	O
147–153	O
.	O
aaai	O
.	O
elliott	O
,	O
r.	O
j.	O
,	O
l.	O
aggoun	O
,	O
and	O
j.	O
b.	O
moore	O
(	O
1995	O
)	O
.	O
hidden	O
markov	O
models	O
:	O
estimation	O
and	O
con-	O
trol	O
.	O
springer	O
.	O
ephraim	O
,	O
y.	O
,	O
d.	O
malah	O
,	O
and	O
b.	O
h.	O
juang	O
(	O
1989	O
)	O
.	O
on	O
the	O
application	O
of	O
hidden	O
markov	O
models	O
for	O
enhancing	O
noisy	O
speech	O
.	O
ieee	O
transactions	O
on	O
acoustics	O
,	O
speech	O
and	O
signal	O
processing	O
37	O
(	O
12	O
)	O
,	O
1846–1856	O
.	O
erwin	O
,	O
e.	O
,	O
k.	O
obermayer	O
,	O
and	O
k.	O
schulten	O
(	O
1992	O
)	O
.	O
self-organizing	O
maps	O
:	O
ordering	O
,	O
convergence	O
properties	O
and	O
energy	O
functions	O
.	O
biological	O
cy-	O
bernetics	O
67	O
,	O
47–55	O
.	O
everitt	O
,	O
b.	O
s.	O
(	O
1984	O
)	O
.	O
an	O
introduction	O
to	O
latent	O
vari-	O
able	O
models	O
.	O
chapman	O
and	O
hall	O
.	O
faul	O
,	O
a.	O
c.	O
and	O
m.	O
e.	O
tipping	O
(	O
2002	O
)	O
.	O
analysis	O
of	O
sparse	O
bayesian	O
learning	B
.	O
in	O
t.	O
g.	O
dietterich	O
,	O
s.	O
becker	O
,	O
and	O
z.	O
ghahramani	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
vol-	O
ume	O
14	O
,	O
pp	O
.	O
383–389	O
.	O
mit	O
press	O
.	O
feller	O
,	O
w.	O
(	O
1966	O
)	O
.	O
an	O
introduction	O
to	O
probability	B
theory	O
and	O
its	O
applications	O
(	O
second	O
ed	O
.	O
)	O
,	O
vol-	O
ume	O
2.	O
wiley	O
.	O
feynman	O
,	O
r.	O
p.	O
,	O
r.	O
b.	O
leighton	O
,	O
and	O
m.	O
sands	O
(	O
1964	O
)	O
.	O
the	O
feynman	O
lectures	O
of	O
physics	O
,	O
vol-	O
ume	O
two	O
.	O
addison-wesley	O
.	O
chapter	O
19.	O
forsyth	O
,	O
d.	O
a.	O
and	O
j.	O
ponce	O
(	O
2003	O
)	O
.	O
computer	O
vi-	O
sion	B
:	O
a	O
modern	O
approach	O
.	O
prentice	O
hall	O
.	O
freund	O
,	O
y.	O
and	O
r.	O
e.	O
schapire	O
(	O
1996	O
)	O
.	O
experiments	O
with	O
a	O
new	O
boosting	B
algorithm	O
.	O
in	O
l.	O
saitta	O
(	O
ed	O
.	O
)	O
,	O
thirteenth	O
international	O
conference	O
on	O
machine	O
learning	O
,	O
pp	O
.	O
148–156	O
.	O
morgan	O
kaufmann	O
.	O
frey	O
,	O
b.	O
j	O
.	O
(	O
1998	O
)	O
.	O
graphical	O
models	O
for	O
ma-	O
chine	O
learning	B
and	O
digital	O
communication	O
.	O
mit	O
press	O
.	O
frey	O
,	O
b.	O
j.	O
and	O
d.	O
j.	O
c.	O
mackay	O
(	O
1998	O
)	O
.	O
a	O
revolu-	O
tion	O
:	O
belief	B
propagation	I
in	O
graphs	O
with	O
cycles	O
.	O
in	O
m.	O
i.	O
jordan	O
,	O
m.	O
j.	O
kearns	O
,	O
and	O
s.	O
a.	O
solla	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
sys-	O
tems	O
,	O
volume	O
10.	O
mit	O
press	O
.	O
friedman	O
,	O
j.	O
h.	O
(	O
2001	O
)	O
.	O
greedy	O
function	O
approxi-	O
mation	B
:	O
a	O
gradient	O
boosting	O
machine	O
.	O
annals	O
of	O
statistics	O
29	O
(	O
5	O
)	O
,	O
1189–1232	O
.	O
friedman	O
,	O
j.	O
h.	O
,	O
t.	O
hastie	O
,	O
and	O
r.	O
tibshirani	O
(	O
2000	O
)	O
.	O
additive	O
logistic	B
regression	I
:	O
a	O
statistical	O
view	O
of	O
boosting	B
.	O
annals	O
of	O
statistics	O
28	O
,	O
337–407	O
.	O
friedman	O
,	O
n.	O
and	O
d.	O
koller	O
(	O
2003	O
)	O
.	O
being	O
bayesian	O
about	O
network	O
structure	O
:	O
a	O
bayesian	O
approach	O
to	O
structure	O
discovery	O
in	O
bayesian	O
networks	O
.	O
ma-	O
chine	O
learning	B
50	O
,	O
95–126	O
.	O
frydenberg	O
,	O
m.	O
(	O
1990	O
)	O
.	O
the	O
chain	B
graph	I
markov	O
property	O
.	O
scandinavian	O
journal	O
of	O
statistics	O
17	O
,	O
333–353	O
.	O
fukunaga	O
,	O
k.	O
(	O
1990	O
)	O
.	O
introduction	O
to	O
statistical	O
pat-	O
tern	O
recognition	O
(	O
second	O
ed.	O
)	O
.	O
academic	O
press	O
.	O
funahashi	O
,	O
k.	O
(	O
1989	O
)	O
.	O
on	O
the	O
approximate	O
realiza-	O
tion	O
of	O
continuous	O
mappings	O
by	O
neural	O
networks	O
.	O
neural	O
networks	O
2	O
(	O
3	O
)	O
,	O
183–192	O
.	O
fung	O
,	O
r.	O
and	O
k.	O
c.	O
chang	O
(	O
1990	O
)	O
.	O
weighting	O
and	O
integrating	O
evidence	O
for	O
stochastic	B
simulation	O
in	O
bayesian	O
networks	O
.	O
in	O
p.	O
p.	O
bonissone	O
,	O
m.	O
hen-	O
rion	B
,	O
l.	O
n.	O
kanal	O
,	O
and	O
j.	O
f.	O
lemmer	O
(	O
eds	O
.	O
)	O
,	O
un-	O
certainty	O
in	O
artiﬁcial	O
intelligence	O
,	O
volume	O
5	O
,	O
pp	O
.	O
208–219	O
.	O
elsevier	O
.	O
gallager	O
,	O
r.	O
g.	O
(	O
1963	O
)	O
.	O
low-density	O
parity-check	O
codes	O
.	O
mit	O
press	O
.	O
gamerman	O
,	O
d.	O
(	O
1997	O
)	O
.	O
markov	O
chain	O
monte	O
carlo	O
:	O
stochastic	B
simulation	O
for	O
bayesian	O
inference	B
.	O
chapman	O
and	O
hall	O
.	O
gelman	O
,	O
a.	O
,	O
j.	O
b.	O
carlin	O
,	O
h.	O
s.	O
stern	O
,	O
and	O
d.	O
b.	O
ru-	O
bin	O
(	O
2004	O
)	O
.	O
bayesian	O
data	O
analysis	O
(	O
second	O
ed.	O
)	O
.	O
chapman	O
and	O
hall	O
.	O
geman	O
,	O
s.	O
and	O
d.	O
geman	O
(	O
1984	O
)	O
.	O
stochastic	B
re-	O
laxation	O
,	O
gibbs	O
distributions	O
,	O
and	O
the	O
bayesian	O
restoration	O
of	O
images	O
.	O
ieee	O
transactions	O
on	O
pat-	O
tern	O
analysis	O
and	O
machine	O
intelligence	O
6	O
(	O
1	O
)	O
,	O
721–741	O
.	O
ghahramani	O
,	O
z.	O
and	O
m.	O
j.	O
beal	O
(	O
2000	O
)	O
.	O
variational	B
inference	I
for	O
bayesian	O
mixtures	O
of	O
factor	O
ana-	O
lyzers	O
.	O
in	O
s.	O
a.	O
solla	O
,	O
t.	O
k.	O
leen	O
,	O
and	O
k.	O
r.	O
m¨uller	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
volume	O
12	O
,	O
pp	O
.	O
449–455	O
.	O
mit	O
press	O
.	O
ghahramani	O
,	O
z.	O
and	O
g.	O
e.	O
hinton	O
(	O
1996a	O
)	O
.	O
the	O
em	O
algorithm	O
for	O
mixtures	O
of	O
factor	O
analyzers	O
.	O
technical	O
report	O
crg-tr-96-1	O
,	O
university	O
of	O
toronto	O
.	O
ghahramani	O
,	O
z.	O
and	O
g.	O
e.	O
hinton	O
(	O
1996b	O
)	O
.	O
param-	O
eter	O
estimation	O
for	O
linear	O
dynamical	O
systems	O
.	O
technical	O
report	O
crg-tr-96-2	O
,	O
university	O
of	O
toronto	O
.	O
ghahramani	O
,	O
z.	O
and	O
g.	O
e.	O
hinton	O
(	O
1998	O
)	O
.	O
variational	B
learning	O
for	O
switching	O
state-space	O
models	O
.	O
neu-	O
ral	O
computation	O
12	O
(	O
4	O
)	O
,	O
963–996	O
.	O
ghahramani	O
,	O
z.	O
and	O
m.	O
i.	O
jordan	O
(	O
1994	O
)	O
.	O
super-	O
vised	O
learning	B
from	O
incomplete	O
data	O
via	O
an	O
em	O
appproach	O
.	O
in	O
j.	O
d.	O
cowan	O
,	O
g.	O
t.	O
tesauro	O
,	O
and	O
j.	O
alspector	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
informa-	O
tion	O
processing	O
systems	O
,	O
volume	O
6	O
,	O
pp	O
.	O
120–127	O
.	O
morgan	O
kaufmann	O
.	O
ghahramani	O
,	O
z.	O
and	O
m.	O
i.	O
jordan	O
(	O
1997	O
)	O
.	O
factorial	O
hidden	O
markov	O
models	O
.	O
machine	O
learning	O
29	O
,	O
245–275	O
.	O
gibbs	O
,	O
m.	O
n.	O
(	O
1997	O
)	O
.	O
bayesian	O
gaussian	O
processes	O
for	B
regression	I
and	O
classiﬁcation	B
.	O
phd	O
thesis	O
,	O
uni-	O
versity	O
of	O
cambridge	O
.	O
gibbs	O
,	O
m.	O
n.	O
and	O
d.	O
j.	O
c.	O
mackay	O
(	O
2000	O
)	O
.	O
varia-	O
tional	O
gaussian	O
process	O
classiﬁers	O
.	O
ieee	O
trans-	O
actions	O
on	O
neural	O
networks	O
11	O
,	O
1458–1464	O
.	O
references	O
717	O
gilks	O
,	O
w.	O
r.	O
(	O
1992	O
)	O
.	O
derivative-free	O
adaptive	B
rejection	I
sampling	I
for	O
gibbs	O
in	O
j.	O
bernardo	O
,	O
j.	O
berger	O
,	O
a.	O
p.	O
dawid	O
,	O
and	O
a.	O
f.	O
m.	O
smith	O
(	O
eds	O
.	O
)	O
,	O
bayesian	O
statistics	O
,	O
volume	O
4.	O
ox-	O
ford	O
university	O
press	O
.	O
sampling	O
.	O
gilks	O
,	O
w.	O
r.	O
,	O
n.	O
g.	O
best	O
,	O
and	O
k.	O
k.	O
c.	O
tan	O
(	O
1995	O
)	O
.	O
adaptive	O
rejection	O
metropolis	O
sampling	O
.	O
applied	O
statistics	O
44	O
,	O
455–472	O
.	O
gilks	O
,	O
w.	O
r.	O
,	O
s.	O
richardson	O
,	O
and	O
d.	O
j.	O
spiegelhal-	O
ter	O
(	O
eds	O
.	O
)	O
(	O
1996	O
)	O
.	O
markov	O
chain	O
monte	O
carlo	O
in	O
practice	O
.	O
chapman	O
and	O
hall	O
.	O
gilks	O
,	O
w.	O
r.	O
and	O
p.	O
wild	O
(	O
1992	O
)	O
.	O
adaptive	B
rejection	I
sampling	I
for	O
gibbs	O
sampling	O
.	O
applied	O
statis-	O
tics	O
41	O
,	O
337–348	O
.	O
gill	O
,	O
p.	O
e.	O
,	O
w.	O
murray	O
,	O
and	O
m.	O
h.	O
wright	O
(	O
1981	O
)	O
.	O
practical	O
optimization	O
.	O
academic	O
press	O
.	O
goldberg	O
,	O
p.	O
w.	O
,	O
c.	O
k.	O
i.	O
williams	O
,	O
and	O
c.	O
m.	O
bishop	O
(	O
1998	O
)	O
.	O
regression	B
with	O
input-dependent	O
noise	O
:	O
a	O
gaussian	O
process	O
treatment	O
.	O
in	O
ad-	O
vances	O
in	O
neural	O
information	O
processing	O
sys-	O
tems	O
,	O
volume	O
10	O
,	O
pp	O
.	O
493–499	O
.	O
mit	O
press	O
.	O
golub	O
,	O
g.	O
h.	O
and	O
c.	O
f.	O
van	O
loan	O
(	O
1996	O
)	O
.	O
matrix	O
computations	O
(	O
third	O
ed.	O
)	O
.	O
john	O
hopkins	O
univer-	O
sity	O
press	O
.	O
good	O
,	O
i	O
.	O
(	O
1950	O
)	O
.	O
probability	B
and	O
the	O
weighing	O
of	O
ev-	O
idence	O
.	O
hafners	O
.	O
gordon	O
,	O
n.	O
j.	O
,	O
d.	O
j.	O
salmond	O
,	O
and	O
a.	O
f.	O
m.	O
smith	O
approach	O
to	O
nonlinear/non-	O
iee	O
(	O
1993	O
)	O
.	O
novel	O
gaussian	O
bayesian	O
proceedings-f	O
140	O
(	O
2	O
)	O
,	O
107–113	O
.	O
estimation	O
.	O
state	O
graepel	O
,	O
t.	O
(	O
2003	O
)	O
.	O
solving	O
noisy	O
linear	O
operator	O
equations	O
by	O
gaussian	O
processes	O
:	O
application	O
to	O
ordinary	O
and	O
partial	O
differential	B
equations	O
.	O
in	O
proceedings	O
of	O
the	O
twentieth	O
international	O
con-	O
ference	O
on	O
machine	O
learning	O
,	O
pp	O
.	O
234–241	O
.	O
greig	O
,	O
d.	O
,	O
b.	O
porteous	O
,	O
and	O
a.	O
seheult	O
(	O
1989	O
)	O
.	O
ex-	O
act	O
maximum	O
a-posteriori	O
estimation	O
for	O
binary	O
images	O
.	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
,	O
series	O
b	O
51	O
(	O
2	O
)	O
,	O
271–279	O
.	O
gull	O
,	O
s.	O
f.	O
(	O
1989	O
)	O
.	O
developments	O
in	O
maximum	O
en-	O
tropy	O
data	O
analysis	O
.	O
in	O
j.	O
skilling	O
(	O
ed	O
.	O
)	O
,	O
maxi-	O
mum	O
entropy	B
and	O
bayesian	O
methods	O
,	O
pp	O
.	O
53–71	O
.	O
kluwer	O
.	O
718	O
references	O
hassibi	O
,	O
b.	O
and	O
d.	O
g.	O
stork	O
(	O
1993	O
)	O
.	O
second	B
order	I
derivatives	O
for	O
network	O
pruning	O
:	O
optimal	O
brain	O
surgeon	O
.	O
in	O
s.	O
j.	O
hanson	O
,	O
j.	O
d.	O
cowan	O
,	O
and	O
c.	O
l.	O
giles	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
informa-	O
tion	O
processing	O
systems	O
,	O
volume	O
5	O
,	O
pp	O
.	O
164–171	O
.	O
morgan	O
kaufmann	O
.	O
hastie	O
,	O
t.	O
and	O
w.	O
stuetzle	O
(	O
1989	O
)	O
.	O
principal	O
curves	O
.	O
the	O
american	O
statistical	O
associa-	O
journal	O
of	O
tion	O
84	O
(	O
106	O
)	O
,	O
502–516	O
.	O
hastie	O
,	O
t.	O
,	O
r.	O
tibshirani	O
,	O
and	O
j.	O
friedman	O
(	O
2001	O
)	O
.	O
the	O
elements	O
of	O
statistical	O
learning	O
.	O
springer	O
.	O
hastings	O
,	O
w.	O
k.	O
(	O
1970	O
)	O
.	O
monte	O
carlo	O
sampling	B
methods	I
using	O
markov	O
chains	O
and	O
their	O
applica-	O
tions	O
.	O
biometrika	O
57	O
,	O
97–109	O
.	O
hathaway	O
,	O
r.	O
j	O
.	O
(	O
1986	O
)	O
.	O
another	O
interpretation	O
of	O
the	O
em	O
algorithm	O
for	O
mixture	O
distributions	O
.	O
statistics	O
and	O
probability	B
letters	O
4	O
,	O
53–56	O
.	O
haussler	O
,	O
d.	O
(	O
1999	O
)	O
.	O
convolution	O
kernels	O
on	O
discrete	O
structures	O
.	O
technical	O
report	O
ucsc-crl-99-10	O
,	O
university	O
of	O
california	O
,	O
santa	O
cruz	O
,	O
computer	O
science	O
department	O
.	O
henrion	O
,	O
m.	O
(	O
1988	O
)	O
.	O
propagation	O
of	O
uncertainty	O
by	O
logic	B
sampling	I
in	O
bayes	O
’	O
networks	O
.	O
in	O
j.	O
f.	O
lem-	O
mer	O
and	O
l.	O
n.	O
kanal	O
(	O
eds	O
.	O
)	O
,	O
uncertainty	O
in	O
arti-	O
ﬁcial	O
intelligence	O
,	O
volume	O
2	O
,	O
pp	O
.	O
149–164	O
.	O
north	O
holland	O
.	O
herbrich	O
,	O
r.	O
(	O
2002	O
)	O
.	O
learning	B
kernel	O
classiﬁers	O
.	O
mit	O
press	O
.	O
hertz	O
,	O
j.	O
,	O
a.	O
krogh	O
,	O
and	O
r.	O
g.	O
palmer	O
(	O
1991	O
)	O
.	O
in-	O
troduction	O
to	O
the	O
theory	B
of	O
neural	O
computation	O
.	O
addison	O
wesley	O
.	O
hinton	O
,	O
g.	O
e.	O
,	O
p.	O
dayan	O
,	O
and	O
m.	O
revow	O
(	O
1997	O
)	O
.	O
modelling	O
the	O
manifolds	O
of	O
images	O
of	O
handwrit-	O
ten	O
digits	O
.	O
ieee	O
transactions	O
on	O
neural	O
net-	O
works	O
8	O
(	O
1	O
)	O
,	O
65–74	O
.	O
hinton	O
,	O
g.	O
e.	O
and	O
d.	O
van	O
camp	O
(	O
1993	O
)	O
.	O
keeping	O
neural	O
networks	O
simple	O
by	O
minimizing	O
the	O
de-	O
scription	O
length	O
of	O
the	O
weights	O
.	O
in	O
proceedings	O
of	O
the	O
sixth	O
annual	O
conference	O
on	O
computational	B
learning	I
theory	I
,	O
pp	O
.	O
5–13	O
.	O
acm	O
.	O
hinton	O
,	O
g.	O
e.	O
,	O
m.	O
welling	O
,	O
y.	O
w.	O
teh	O
,	O
and	O
s.	O
osin-	O
dero	O
(	O
2001	O
)	O
.	O
a	O
new	O
view	O
of	O
ica	O
.	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
independent	B
component	I
analysis	I
and	O
blind	O
signal	O
separa-	O
tion	O
,	O
volume	O
3.	O
hodgson	O
,	O
m.	O
e.	O
(	O
1998	O
)	O
.	O
reducing	O
computational	O
re-	O
quirements	O
of	O
the	O
minimum-distance	O
classiﬁer	O
.	O
remote	O
sensing	O
of	O
environments	O
25	O
,	O
117–128	O
.	O
hoerl	O
,	O
a.	O
e.	O
and	O
r.	O
kennard	O
(	O
1970	O
)	O
.	O
ridge	O
regres-	O
sion	B
:	O
biased	O
estimation	O
for	O
nonorthogonal	O
prob-	O
lems	O
.	O
technometrics	O
12	O
,	O
55–67	O
.	O
hofmann	O
,	O
t.	O
(	O
2000	O
)	O
.	O
learning	B
the	O
similarity	O
of	O
doc-	O
uments	O
:	O
an	O
information-geometric	O
approach	O
to	O
document	B
retrieval	I
and	O
classiﬁcation	B
.	O
in	O
s.	O
a.	O
solla	O
,	O
t.	O
k.	O
leen	O
,	O
and	O
k.	O
r.	O
m¨uller	O
(	O
eds	O
.	O
)	O
,	O
ad-	O
vances	O
in	O
neural	O
information	O
processing	O
sys-	O
tems	O
,	O
volume	O
12	O
,	O
pp	O
.	O
914–920	O
.	O
mit	O
press	O
.	O
hojen-sorensen	O
,	O
p.	O
a.	O
,	O
o.	O
winther	O
,	O
and	O
l.	O
k.	O
hansen	O
(	O
2002	O
)	O
.	O
mean	O
ﬁeld	O
approaches	O
to	O
independent	B
component	I
analysis	I
.	O
neural	O
computation	O
14	O
(	O
4	O
)	O
,	O
889–918	O
.	O
hornik	O
,	O
k.	O
(	O
1991	O
)	O
.	O
approximation	O
capabilities	O
of	O
multilayer	O
feedforward	O
networks	O
.	O
neural	O
net-	O
works	O
4	O
(	O
2	O
)	O
,	O
251–257	O
.	O
hornik	O
,	O
k.	O
,	O
m.	O
stinchcombe	O
,	O
and	O
h.	O
white	O
(	O
1989	O
)	O
.	O
multilayer	O
feedforward	O
networks	O
are	O
universal	O
approximators	O
.	O
neural	O
networks	O
2	O
(	O
5	O
)	O
,	O
359–366	O
.	O
hotelling	O
,	O
h.	O
(	O
1933	O
)	O
.	O
analysis	O
of	O
a	O
complex	O
of	O
statis-	O
tical	O
variables	O
into	O
principal	O
components	O
.	O
jour-	O
nal	O
of	O
educational	O
psychology	O
24	O
,	O
417–441	O
.	O
hotelling	O
,	O
h.	O
(	O
1936	O
)	O
.	O
relations	O
between	O
two	O
sets	O
of	O
variables	O
.	O
biometrika	O
28	O
,	O
321–377	O
.	O
hyv¨arinen	O
,	O
a.	O
and	O
e.	O
oja	O
(	O
1997	O
)	O
.	O
a	O
fast	O
ﬁxed-point	O
algorithm	O
for	O
independent	O
component	O
analysis	O
.	O
neural	O
computation	O
9	O
(	O
7	O
)	O
,	O
1483–1492	O
.	O
isard	O
,	O
m.	O
and	O
a.	O
blake	O
(	O
1998	O
)	O
.	O
condensation	O
–	O
conditional	B
density	O
propagation	O
for	O
visual	O
tracking	O
.	O
international	O
journal	O
of	O
computer	O
vi-	O
sion	B
29	O
(	O
1	O
)	O
,	O
5–18	O
.	O
ito	O
,	O
y	O
.	O
(	O
1991	O
)	O
.	O
representation	O
of	O
functions	O
by	O
su-	O
perpositions	O
of	O
a	O
step	O
or	O
sigmoid	B
function	O
and	O
their	O
applications	O
to	O
neural	B
network	I
theory	O
.	O
neu-	O
ral	O
networks	O
4	O
(	O
3	O
)	O
,	O
385–394	O
.	O
references	O
719	O
jaakkola	O
,	O
t.	O
and	O
m.	O
i.	O
jordan	O
(	O
2000	O
)	O
.	O
bayesian	O
parameter	O
estimation	O
via	O
variational	B
methods	O
.	O
statistics	O
and	O
computing	O
10	O
,	O
25–37	O
.	O
jaakkola	O
,	O
t.	O
s.	O
(	O
2001	O
)	O
.	O
tutorial	O
on	O
variational	B
ap-	O
proximation	B
methods	O
.	O
in	O
m.	O
opper	O
and	O
d.	O
saad	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
mean	B
field	O
methods	O
,	O
pp	O
.	O
129–159	O
.	O
mit	O
press	O
.	O
jaakkola	O
,	O
t.	O
s.	O
and	O
d.	O
haussler	O
(	O
1999	O
)	O
.	O
exploiting	O
generative	O
models	O
in	O
discriminative	O
classiﬁers	O
.	O
in	O
m.	O
s.	O
kearns	O
,	O
s.	O
a.	O
solla	O
,	O
and	O
d.	O
a.	O
cohn	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
sys-	O
tems	O
,	O
volume	O
11.	O
mit	O
press	O
.	O
jacobs	O
,	O
r.	O
a.	O
,	O
m.	O
i.	O
jordan	O
,	O
s.	O
j.	O
nowlan	O
,	O
and	O
g.	O
e.	O
hinton	O
(	O
1991	O
)	O
.	O
adaptive	O
mixtures	O
of	O
local	B
ex-	O
perts	O
.	O
neural	O
computation	O
3	O
(	O
1	O
)	O
,	O
79–87	O
.	O
jaynes	O
,	O
e.	O
t.	O
(	O
2003	O
)	O
.	O
probability	B
theory	O
:	O
the	O
logic	O
of	O
science	O
.	O
cambridge	O
university	O
press	O
.	O
jebara	O
,	O
t.	O
(	O
2004	O
)	O
.	O
machine	O
learning	O
:	O
discrimina-	O
tive	O
and	O
generative	O
.	O
kluwer	O
.	O
jeffries	O
,	O
h.	O
(	O
1946	O
)	O
.	O
an	O
invariant	O
form	O
for	O
the	O
prior	B
probability	O
in	O
estimation	O
problems	O
.	O
pro	O
.	O
roy	O
.	O
soc	O
.	O
aa	O
186	O
,	O
453–461	O
.	O
jelinek	O
,	O
f.	O
(	O
1997	O
)	O
.	O
statistical	O
methods	O
for	O
speech	O
recognition	O
.	O
mit	O
press	O
.	O
jensen	O
,	O
c.	O
,	O
a.	O
kong	O
,	O
and	O
u.	O
kjaerulff	O
(	O
1995	O
)	O
.	O
block-	O
ing	O
gibbs	O
sampling	O
in	O
very	O
large	O
probabilistic	O
expert	O
systems	O
.	O
international	O
journal	O
of	O
human	O
computer	O
studies	O
.	O
special	O
issue	O
on	O
real-world	O
applications	O
of	O
uncertain	O
reasoning	O
.	O
42	O
,	O
647–	O
666.	O
jordan	O
,	O
m.	O
i	O
.	O
(	O
2007	O
)	O
.	O
an	O
introduction	O
to	O
probabilis-	O
tic	O
graphical	O
models	O
.	O
in	O
preparation	O
.	O
jordan	O
,	O
m.	O
i.	O
,	O
z.	O
ghahramani	O
,	O
t.	O
s.	O
jaakkola	O
,	O
and	O
l.	O
k.	O
saul	O
(	O
1999	O
)	O
.	O
an	O
introduction	O
to	O
variational	B
methods	O
for	O
graphical	O
models	O
.	O
in	O
m.	O
i.	O
jordan	O
(	O
ed	O
.	O
)	O
,	O
learning	B
in	O
graphical	O
models	O
,	O
pp	O
.	O
105–	O
162.	O
mit	O
press	O
.	O
jordan	O
,	O
m.	O
i.	O
and	O
r.	O
a.	O
jacobs	O
(	O
1994	O
)	O
.	O
hierarchical	B
mixtures	O
of	O
experts	O
and	O
the	O
em	O
algorithm	O
.	O
neu-	O
ral	O
computation	O
6	O
(	O
2	O
)	O
,	O
181–214	O
.	O
jutten	O
,	O
c.	O
and	O
j.	O
herault	O
(	O
1991	O
)	O
.	O
blind	O
separation	O
of	O
sources	O
,	O
1	O
:	O
an	O
adaptive	O
algorithm	O
based	O
on	O
neu-	O
romimetic	O
architecture	O
.	O
signal	O
processing	O
24	O
(	O
1	O
)	O
,	O
1–10	O
.	O
kalman	O
,	O
r.	O
e.	O
(	O
1960	O
)	O
.	O
a	O
new	O
approach	O
to	O
linear	O
ﬁl-	O
tering	O
and	O
prediction	O
problems	O
.	O
transactions	O
of	O
the	O
american	O
society	O
for	O
mechanical	O
engineer-	O
ing	O
,	O
series	O
d	O
,	O
journal	O
of	O
basic	O
engineering	O
82	O
,	O
35–45	O
.	O
kambhatla	O
,	O
n.	O
and	O
t.	O
k.	O
leen	O
(	O
1997	O
)	O
.	O
dimension	O
reduction	O
by	O
local	B
principal	O
component	O
analysis	O
.	O
neural	O
computation	O
9	O
(	O
7	O
)	O
,	O
1493–1516	O
.	O
kanazawa	O
,	O
k.	O
,	O
d.	O
koller	O
,	O
and	O
s.	O
russel	O
(	O
1995	O
)	O
.	O
stochastic	B
simulation	O
algorithms	O
for	O
dynamic	O
probabilistic	O
networks	O
.	O
in	O
uncertainty	O
in	O
artiﬁ-	O
cial	O
intelligence	O
,	O
volume	O
11.	O
morgan	O
kaufmann	O
.	O
kapadia	O
,	O
s.	O
(	O
1998	O
)	O
.	O
discriminative	O
training	O
of	O
hid-	O
den	O
markov	O
models	O
.	O
phd	O
thesis	O
,	O
university	O
of	O
cambridge	O
,	O
u.k.	O
kapur	O
,	O
j	O
.	O
(	O
1989	O
)	O
.	O
maximum	O
entropy	O
methods	O
in	O
sci-	O
jensen	O
,	O
f.	O
v.	O
(	O
1996	O
)	O
.	O
an	O
introduction	O
to	O
bayesian	O
ence	O
and	O
engineering	O
.	O
wiley	O
.	O
networks	O
.	O
ucl	O
press	O
.	O
jerrum	O
,	O
m.	O
and	O
a.	O
sinclair	O
(	O
1996	O
)	O
.	O
the	O
markov	O
chain	O
monte	O
carlo	O
method	O
:	O
an	O
approach	O
to	O
ap-	O
proximate	O
counting	O
and	O
integration	O
.	O
in	O
d.	O
s.	O
hochbaum	O
(	O
ed	O
.	O
)	O
,	O
approximation	O
algorithms	O
for	O
np-hard	O
problems	O
.	O
pws	O
publishing	O
.	O
jolliffe	O
,	O
i.	O
t.	O
(	O
2002	O
)	O
.	O
principal	B
component	I
analysis	I
(	O
second	O
ed.	O
)	O
.	O
springer	O
.	O
jordan	O
,	O
m.	O
i	O
.	O
(	O
1999	O
)	O
.	O
learning	B
in	O
graphical	O
models	O
.	O
mit	O
press	O
.	O
karush	O
,	O
w.	O
(	O
1939	O
)	O
.	O
minima	O
of	O
functions	O
of	O
several	O
variables	O
with	O
inequalities	O
as	O
side	O
constraints	O
.	O
master	O
’	O
s	O
thesis	O
,	O
department	O
of	O
mathematics	O
,	O
university	O
of	O
chicago	O
.	O
kass	O
,	O
r.	O
e.	O
and	O
a.	O
e.	O
raftery	O
(	O
1995	O
)	O
.	O
bayes	O
fac-	O
tors	O
.	O
journal	O
of	O
the	O
american	O
statistical	O
associ-	O
ation	O
90	O
,	O
377–395	O
.	O
kearns	O
,	O
m.	O
j.	O
and	O
u.	O
v.	O
vazirani	O
(	O
1994	O
)	O
.	O
an	O
intro-	O
duction	O
to	O
computational	B
learning	I
theory	I
.	O
mit	O
press	O
.	O
720	O
references	O
kindermann	O
,	O
r.	O
and	O
j.	O
l.	O
snell	O
(	O
1980	O
)	O
.	O
markov	O
ran-	O
dom	O
fields	O
and	O
their	O
applications	O
.	O
american	O
mathematical	O
society	O
.	O
kittler	O
,	O
j.	O
and	O
j.	O
f¨oglein	O
(	O
1984	O
)	O
.	O
contextual	O
classiﬁ-	O
cation	O
of	O
multispectral	O
pixel	O
data	O
.	O
image	O
and	O
vi-	O
sion	B
computing	O
2	O
,	O
13–29	O
.	O
kohonen	O
,	O
t.	O
(	O
1982	O
)	O
.	O
self-organized	O
formation	O
of	O
topologically	O
correct	O
feature	O
maps	O
.	O
biological	O
cybernetics	O
43	O
,	O
59–69	O
.	O
kohonen	O
,	O
t.	O
(	O
1995	O
)	O
.	O
self-organizing	O
maps	O
.	O
springer	O
.	O
kolmogorov	O
,	O
v.	O
and	O
r.	O
zabih	O
(	O
2004	O
)	O
.	O
what	O
en-	O
ergy	O
functions	O
can	O
be	O
minimized	O
via	O
graph	O
cuts	O
?	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
ma-	O
chine	O
intelligence	O
26	O
(	O
2	O
)	O
,	O
147–159	O
.	O
kreinovich	O
,	O
v.	O
y	O
.	O
(	O
1991	O
)	O
.	O
arbitrary	O
nonlinearity	O
is	O
sufﬁcient	O
to	O
represent	O
all	O
functions	O
by	O
neural	O
net-	O
works	O
:	O
a	O
theorem	O
.	O
neural	O
networks	O
4	O
(	O
3	O
)	O
,	O
381–	O
383.	O
krogh	O
,	O
a.	O
,	O
m.	O
brown	O
,	O
i.	O
s.	O
mian	O
,	O
k.	O
sj¨olander	O
,	O
and	O
d.	O
haussler	O
(	O
1994	O
)	O
.	O
hidden	O
markov	O
models	O
in	O
computational	O
biology	O
:	O
applications	O
to	O
protein	O
modelling	O
.	O
journal	O
of	O
molecular	O
biology	O
235	O
,	O
1501–1531	O
.	O
kschischnang	O
,	O
f.	O
r.	O
,	O
b.	O
j.	O
frey	O
,	O
and	O
h.	O
a.	O
loeliger	O
(	O
2001	O
)	O
.	O
factor	O
graphs	O
and	O
the	O
sum-product	O
algo-	O
rithm	O
.	O
ieee	O
transactions	O
on	O
information	O
the-	O
ory	O
47	O
(	O
2	O
)	O
,	O
498–519	O
.	O
kuhn	O
,	O
h.	O
w.	O
and	O
a.	O
w.	O
tucker	O
(	O
1951	O
)	O
.	O
nonlinear	O
programming	O
.	O
in	O
proceedings	O
of	O
the	O
2nd	O
berke-	O
ley	O
symposium	O
on	O
mathematical	O
statistics	O
and	O
probabilities	O
,	O
pp	O
.	O
481–492	O
.	O
university	O
of	O
cali-	O
fornia	O
press	O
.	O
kullback	O
,	O
s.	O
and	O
r.	O
a.	O
leibler	O
(	O
1951	O
)	O
.	O
on	O
infor-	O
mation	B
and	O
sufﬁciency	O
.	O
annals	O
of	O
mathematical	O
statistics	O
22	O
(	O
1	O
)	O
,	O
79–86	O
.	O
k˙urkov´a	O
,	O
v.	O
and	O
p.	O
c.	O
kainen	O
(	O
1994	O
)	O
.	O
functionally	O
equivalent	O
feed-forward	O
neural	O
networks	O
.	O
neural	O
computation	O
6	O
(	O
3	O
)	O
,	O
543–558	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
number	O
18.	O
mit	O
press	O
.	O
in	O
press	O
.	O
lasserre	O
,	O
j.	O
,	O
c.	O
m.	O
bishop	O
,	O
and	O
t.	O
minka	O
(	O
2006	O
)	O
.	O
principled	O
hybrids	O
of	O
generative	O
and	O
discrimina-	O
tive	O
models	O
.	O
in	O
proceedings	O
2006	O
ieee	O
confer-	O
ence	O
on	O
computer	O
vision	O
and	O
pattern	O
recogni-	O
tion	O
,	O
new	O
york	O
.	O
lauritzen	O
,	O
s.	O
and	O
n.	O
wermuth	O
(	O
1989	O
)	O
.	O
graphical	O
models	O
for	O
association	O
between	O
variables	O
,	O
some	O
of	O
which	O
are	O
qualitative	O
some	O
quantitative	O
.	O
an-	O
nals	O
of	O
statistics	O
17	O
,	O
31–57	O
.	O
lauritzen	O
,	O
s.	O
l.	O
(	O
1992	O
)	O
.	O
propagation	O
of	O
probabilities	O
,	O
means	O
and	O
variances	O
in	O
mixed	O
graphical	O
associa-	O
tion	O
models	O
.	O
journal	O
of	O
the	O
american	O
statistical	O
association	O
87	O
,	O
1098–1108	O
.	O
lauritzen	O
,	O
s.	O
l.	O
(	O
1996	O
)	O
.	O
graphical	O
models	O
.	O
oxford	O
university	O
press	O
.	O
lauritzen	O
,	O
s.	O
l.	O
and	O
d.	O
j.	O
spiegelhalter	O
(	O
1988	O
)	O
.	O
lo-	O
cal	O
computations	O
with	O
probabailities	O
on	O
graphical	O
structures	O
and	O
their	O
application	O
to	O
expert	O
systems	O
.	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
50	O
,	O
157–	O
224.	O
lawley	O
,	O
d.	O
n.	O
(	O
1953	O
)	O
.	O
a	O
modiﬁed	O
method	O
of	O
esti-	O
mation	B
in	O
factor	B
analysis	I
and	O
some	O
large	O
sam-	O
ple	O
results	O
.	O
in	O
uppsala	O
symposium	O
on	O
psycho-	O
logical	O
factor	B
analysis	I
,	O
number	O
3	O
in	O
nordisk	O
psykologi	O
monograph	O
series	O
,	O
pp	O
.	O
35–42	O
.	O
upp-	O
sala	O
:	O
almqvist	O
and	O
wiksell	O
.	O
lawrence	O
,	O
n.	O
d.	O
,	O
a.	O
i.	O
t.	O
rowstron	O
,	O
c.	O
m.	O
bishop	O
,	O
and	O
m.	O
j.	O
taylor	O
(	O
2002	O
)	O
.	O
optimising	O
synchro-	O
nisation	O
times	O
for	O
mobile	O
devices	O
.	O
in	O
t.	O
g.	O
di-	O
etterich	O
,	O
s.	O
becker	O
,	O
and	O
z.	O
ghahramani	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
sys-	O
tems	O
,	O
volume	O
14	O
,	O
pp	O
.	O
1401–1408	O
.	O
mit	O
press	O
.	O
lazarsfeld	O
,	O
p.	O
f.	O
and	O
n.	O
w.	O
henry	O
(	O
1968	O
)	O
.	O
latent	O
structure	O
analysis	O
.	O
houghton	O
mifﬂin	O
.	O
le	O
cun	O
,	O
y.	O
,	O
b.	O
boser	O
,	O
j.	O
s.	O
denker	O
,	O
d.	O
henderson	O
,	O
r.	O
e.	O
howard	O
,	O
w.	O
hubbard	O
,	O
and	O
l.	O
d.	O
jackel	O
(	O
1989	O
)	O
.	O
backpropagation	B
applied	O
to	O
handwritten	O
zip	O
code	O
recognition	O
.	O
neural	O
computation	O
1	O
(	O
4	O
)	O
,	O
541–551	O
.	O
kuss	O
,	O
m.	O
and	O
c.	O
rasmussen	O
(	O
2006	O
)	O
.	O
assessing	O
ap-	O
proximations	O
for	O
gaussian	O
process	O
classiﬁcation	B
.	O
le	O
cun	O
,	O
y.	O
,	O
j.	O
s.	O
denker	O
,	O
and	O
s.	O
a.	O
solla	O
(	O
1990	O
)	O
.	O
optimal	O
brain	O
damage	O
.	O
in	O
d.	O
s.	O
touretzky	O
(	O
ed	O
.	O
)	O
,	O
references	O
721	O
advances	O
in	O
neural	O
information	O
processing	O
sys-	O
tems	O
,	O
volume	O
2	O
,	O
pp	O
.	O
598–605	O
.	O
morgan	O
kauf-	O
mann	O
.	O
lecun	O
,	O
y.	O
,	O
l.	O
bottou	O
,	O
y.	O
bengio	O
,	O
and	O
p.	O
haffner	O
(	O
1998	O
)	O
.	O
gradient-based	O
learning	B
applied	O
to	O
doc-	O
ument	O
recognition	O
.	O
proceedings	O
of	O
the	O
ieee	O
86	O
,	O
2278–2324	O
.	O
lee	O
,	O
y.	O
,	O
y.	O
lin	O
,	O
and	O
g.	O
wahba	O
(	O
2001	O
)	O
.	O
multicategory	O
support	B
vector	I
machines	O
.	O
technical	O
report	O
1040	O
,	O
department	O
of	O
statistics	O
,	O
university	O
of	O
madison	O
,	O
wisconsin	O
.	O
leen	O
,	O
t.	O
k.	O
(	O
1995	O
)	O
.	O
from	O
data	O
distributions	O
to	O
regu-	O
larization	O
in	O
invariant	O
learning	B
.	O
neural	O
computa-	O
tion	O
7	O
,	O
974–981	O
.	O
lindley	O
,	O
d.	O
v.	O
(	O
1982	O
)	O
.	O
scoring	O
rules	O
and	O
the	O
in-	O
evitability	O
of	O
probability	B
.	O
international	O
statisti-	O
cal	O
review	O
50	O
,	O
1–26	O
.	O
liu	O
,	O
j.	O
s	O
.	O
(	O
ed	O
.	O
)	O
(	O
2001	O
)	O
.	O
monte	O
carlo	O
strategies	O
in	O
scientiﬁc	O
computing	O
.	O
springer	O
.	O
lloyd	O
,	O
s.	O
p.	O
(	O
1982	O
)	O
.	O
least	O
squares	O
quantization	O
in	O
pcm	O
.	O
ieee	O
transactions	O
on	O
information	O
the-	O
ory	O
28	O
(	O
2	O
)	O
,	O
129–137	O
.	O
l¨utkepohl	O
,	O
h.	O
(	O
1996	O
)	O
.	O
handbook	O
of	O
matrices	O
.	O
wiley	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1992a	O
)	O
.	O
bayesian	O
interpolation	O
.	O
neural	O
computation	O
4	O
(	O
3	O
)	O
,	O
415–447	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1992b	O
)	O
.	O
the	O
evidence	O
framework	O
applied	O
to	O
classiﬁcation	B
networks	O
.	O
neural	O
com-	O
putation	O
4	O
(	O
5	O
)	O
,	O
720–736	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1992c	O
)	O
.	O
a	O
practical	O
bayesian	O
framework	O
for	O
back-propagation	O
networks	O
.	O
neu-	O
ral	O
computation	O
4	O
(	O
3	O
)	O
,	O
448–472	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1994	O
)	O
.	O
bayesian	O
methods	O
for	O
backprop	O
networks	O
.	O
in	O
e.	O
domany	O
,	O
j.	O
l.	O
van	O
hemmen	O
,	O
and	O
k.	O
schulten	O
(	O
eds	O
.	O
)	O
,	O
models	O
of	O
neural	O
networks	O
,	O
iii	O
,	O
chapter	O
6	O
,	O
pp	O
.	O
211–254	O
.	O
springer	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1995	O
)	O
.	O
bayesian	O
neural	O
networks	O
and	O
density	B
networks	O
.	O
nuclear	O
instruments	O
and	O
methods	O
in	O
physics	O
research	O
,	O
a	O
354	O
(	O
1	O
)	O
,	O
73–80	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1997	O
)	O
.	O
ensemble	O
learning	B
for	O
hid-	O
den	O
markov	O
models	O
.	O
unpublished	O
manuscript	O
,	O
department	O
of	O
physics	O
,	O
university	O
of	O
cam-	O
bridge	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1998	O
)	O
.	O
introduction	O
to	O
gaus-	O
sian	O
processes	O
.	O
in	O
c.	O
m.	O
bishop	O
(	O
ed	O
.	O
)	O
,	O
neural	O
networks	O
and	O
machine	O
learning	O
,	O
pp	O
.	O
133–166	O
.	O
springer	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1999	O
)	O
.	O
comparison	O
of	O
approx-	O
imate	O
methods	O
for	O
handling	O
hyperparameters	O
.	O
neural	O
computation	O
11	O
(	O
5	O
)	O
,	O
1035–1068	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
2003	O
)	O
.	O
information	B
theory	I
,	O
infer-	O
ence	O
and	O
learning	B
algorithms	O
.	O
cambridge	O
uni-	O
versity	O
press	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
and	O
m.	O
n.	O
gibbs	O
(	O
1999	O
)	O
.	O
den-	O
sity	O
networks	O
.	O
in	O
j.	O
w.	O
kay	O
and	O
d.	O
m.	O
tittering-	O
ton	O
(	O
eds	O
.	O
)	O
,	O
statistics	O
and	O
neural	O
networks	O
:	O
ad-	O
vances	O
at	O
the	O
interface	O
,	O
chapter	O
5	O
,	O
pp	O
.	O
129–145	O
.	O
oxford	O
university	O
press	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
and	O
r.	O
m.	O
neal	O
(	O
1999	O
)	O
.	O
good	O
error-	O
correcting	O
codes	O
based	O
on	O
very	O
sparse	O
matrices	O
.	O
ieee	O
transactions	O
on	O
information	B
theory	I
45	O
,	O
399–431	O
.	O
macqueen	O
,	O
j	O
.	O
(	O
1967	O
)	O
.	O
some	O
methods	O
for	O
classiﬁca-	O
tion	O
and	O
analysis	O
of	O
multivariate	O
observations	O
.	O
in	O
l.	O
m.	O
lecam	O
and	O
j.	O
neyman	O
(	O
eds	O
.	O
)	O
,	O
proceed-	O
ings	O
of	O
the	O
fifth	O
berkeley	O
symposium	O
on	O
mathe-	O
matical	O
statistics	O
and	O
probability	B
,	O
volume	O
i	O
,	O
pp	O
.	O
281–297	O
.	O
university	O
of	O
california	O
press	O
.	O
magnus	O
,	O
j.	O
r.	O
and	O
h.	O
neudecker	O
(	O
1999	O
)	O
.	O
matrix	O
dif-	O
ferential	O
calculus	O
with	O
applications	O
in	O
statistics	O
and	O
econometrics	O
.	O
wiley	O
.	O
mallat	O
,	O
s.	O
(	O
1999	O
)	O
.	O
a	O
wavelet	O
tour	O
of	O
signal	O
process-	O
ing	O
(	O
second	O
ed.	O
)	O
.	O
academic	O
press	O
.	O
manning	O
,	O
c.	O
d.	O
and	O
h.	O
sch¨utze	O
(	O
1999	O
)	O
.	O
foundations	O
of	O
statistical	O
natural	O
language	O
processing	O
.	O
mit	O
press	O
.	O
mardia	O
,	O
k.	O
v.	O
and	O
p.	O
e.	O
jupp	O
(	O
2000	O
)	O
.	O
directional	O
statistics	O
.	O
wiley	O
.	O
maybeck	O
,	O
p.	O
s.	O
(	O
1982	O
)	O
.	O
stochastic	B
models	O
,	O
estima-	O
tion	O
and	O
control	O
.	O
academic	O
press	O
.	O
mcallester	O
,	O
d.	O
a	O
.	O
(	O
2003	O
)	O
.	O
pac-bayesian	O
stochastic	B
model	O
selection	O
.	O
machine	O
learning	O
51	O
(	O
1	O
)	O
,	O
5–21	O
.	O
722	O
references	O
mccullagh	O
,	O
p.	O
and	O
j.	O
a.	O
nelder	O
(	O
1989	O
)	O
.	O
generalized	O
linear	O
models	O
(	O
second	O
ed.	O
)	O
.	O
chapman	O
and	O
hall	O
.	O
mcculloch	O
,	O
w.	O
s.	O
and	O
w.	O
pitts	O
(	O
1943	O
)	O
.	O
a	O
logical	O
calculus	O
of	O
the	O
ideas	O
immanent	O
in	O
nervous	O
ac-	O
tivity	O
.	O
bulletin	O
of	O
mathematical	O
biophysics	O
5	O
,	O
115–133	O
.	O
reprinted	O
in	O
anderson	O
and	O
rosenfeld	O
(	O
1988	O
)	O
.	O
mceliece	O
,	O
r.	O
j.	O
,	O
d.	O
j.	O
c.	O
mackay	O
,	O
and	O
j.	O
f.	O
cheng	O
(	O
1998	O
)	O
.	O
turbo	O
decoding	O
as	O
an	O
instance	O
of	O
pearl	O
’	O
s	O
‘	O
belief	O
ppropagation	O
’	O
algorithm	O
.	O
ieee	O
journal	O
on	O
selected	O
areas	O
in	O
communications	O
16	O
,	O
140–	O
152.	O
mclachlan	O
,	O
g.	O
j.	O
and	O
k.	O
e.	O
basford	O
(	O
1988	O
)	O
.	O
mixture	B
models	O
:	O
inference	B
and	O
applications	O
to	O
cluster-	O
ing	O
.	O
marcel	O
dekker	O
.	O
mclachlan	O
,	O
g.	O
j.	O
and	O
t.	O
krishnan	O
(	O
1997	O
)	O
.	O
the	O
em	O
algorithm	O
and	O
its	O
extensions	O
.	O
wiley	O
.	O
mclachlan	O
,	O
g.	O
j.	O
and	O
d.	O
peel	O
(	O
2000	O
)	O
.	O
finite	O
mixture	B
models	O
.	O
wiley	O
.	O
meng	O
,	O
x.	O
l.	O
and	O
d.	O
b.	O
rubin	O
(	O
1993	O
)	O
.	O
maximum	O
like-	O
lihood	O
estimation	O
via	O
the	O
ecm	O
algorithm	O
:	O
a	O
gen-	O
eral	O
framework	O
.	O
biometrika	O
80	O
,	O
267–278	O
.	O
metropolis	O
,	O
n.	O
,	O
a.	O
w.	O
rosenbluth	O
,	O
m.	O
n.	O
rosen-	O
bluth	O
,	O
a.	O
h.	O
teller	O
,	O
and	O
e.	O
teller	O
(	O
1953	O
)	O
.	O
equa-	O
tion	O
of	O
state	O
calculations	O
by	O
fast	O
computing	O
machines	O
.	O
journal	O
of	O
chemical	O
physics	O
21	O
(	O
6	O
)	O
,	O
1087–1092	O
.	O
metropolis	O
,	O
n.	O
and	O
s.	O
ulam	O
(	O
1949	O
)	O
.	O
the	O
monte	O
carlo	O
method	O
.	O
journal	O
of	O
the	O
american	O
statisti-	O
cal	O
association	O
44	O
(	O
247	O
)	O
,	O
335–341	O
.	O
mika	O
,	O
s.	O
,	O
g.	O
r¨atsch	O
,	O
j.	O
weston	O
,	O
and	O
b.	O
sch¨olkopf	O
(	O
1999	O
)	O
.	O
fisher	O
discriminant	O
analysis	O
with	O
ker-	O
nels	O
.	O
in	O
y.	O
h.	O
hu	O
,	O
j.	O
larsen	O
,	O
e.	O
wilson	O
,	O
and	O
s.	O
douglas	O
(	O
eds	O
.	O
)	O
,	O
neural	O
networks	O
for	O
signal	O
processing	O
ix	O
,	O
pp	O
.	O
41–48	O
.	O
ieee	O
.	O
minka	O
,	O
t.	O
(	O
2001a	O
)	O
.	O
expectation	B
propagation	I
for	O
ap-	O
proximate	O
bayesian	O
inference	B
.	O
in	O
j.	O
breese	O
and	O
d.	O
koller	O
(	O
eds	O
.	O
)	O
,	O
proceedings	O
of	O
the	O
seventeenth	O
conference	O
on	O
uncertainty	O
in	O
artiﬁcial	O
intelli-	O
gence	O
,	O
pp	O
.	O
362–369	O
.	O
morgan	O
kaufmann	O
.	O
minka	O
,	O
t.	O
(	O
2001b	O
)	O
.	O
a	O
family	O
of	O
approximate	O
al-	O
gorithms	O
for	O
bayesian	O
inference	B
.	O
ph	O
.	O
d.	O
thesis	O
,	O
mit	O
.	O
minka	O
,	O
t.	O
(	O
2004	O
)	O
.	O
power	O
ep	O
.	O
technical	O
report	O
msr-tr-2004-149	O
,	O
microsoft	O
research	O
cam-	O
bridge	O
.	O
minka	O
,	O
t.	O
(	O
2005	O
)	O
.	O
divergence	O
measures	O
and	O
mes-	O
sage	O
passing	O
.	O
technical	O
report	O
msr-tr-2005-	O
173	O
,	O
microsoft	O
research	O
cambridge	O
.	O
minka	O
,	O
t.	O
p.	O
(	O
2001c	O
)	O
.	O
automatic	O
choice	O
of	O
dimen-	O
sionality	O
for	O
pca	O
.	O
in	O
t.	O
k.	O
leen	O
,	O
t.	O
g.	O
diet-	O
terich	O
,	O
and	O
v.	O
tresp	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
volume	O
13	O
,	O
pp	O
.	O
598–604	O
.	O
mit	O
press	O
.	O
minsky	O
,	O
m.	O
l.	O
and	O
s.	O
a.	O
papert	O
(	O
1969	O
)	O
.	O
perceptrons	O
.	O
mit	O
press	O
.	O
expanded	O
edition	O
1990.	O
miskin	O
,	O
j.	O
w.	O
and	O
d.	O
j.	O
c.	O
mackay	O
(	O
2001	O
)	O
.	O
ensem-	O
ble	O
learning	B
for	O
blind	B
source	I
separation	I
.	O
in	O
s.	O
j.	O
roberts	O
and	O
r.	O
m.	O
everson	O
(	O
eds	O
.	O
)	O
,	O
independent	B
component	I
analysis	I
:	O
principles	O
and	O
practice	O
.	O
cambridge	O
university	O
press	O
.	O
møller	O
,	O
m.	O
(	O
1993	O
)	O
.	O
efﬁcient	O
training	B
of	O
feed-	O
forward	O
neural	O
networks	O
.	O
ph	O
.	O
d.	O
thesis	O
,	O
aarhus	O
university	O
,	O
denmark	O
.	O
moody	O
,	O
j.	O
and	O
c.	O
j.	O
darken	O
(	O
1989	O
)	O
.	O
fast	O
learning	O
in	O
networks	O
of	O
locally-tuned	O
processing	O
units	O
.	O
neu-	O
ral	O
computation	O
1	O
(	O
2	O
)	O
,	O
281–294	O
.	O
moore	O
,	O
a.	O
w.	O
(	O
2000	O
)	O
.	O
the	O
anchors	O
hierarch	O
:	O
us-	O
ing	O
the	O
triangle	O
inequality	O
to	O
survive	O
high	O
dimen-	O
sional	O
data	O
.	O
in	O
proceedings	O
of	O
the	O
twelfth	O
con-	O
ference	O
on	O
uncertainty	O
in	O
artiﬁcial	O
intelligence	O
,	O
pp	O
.	O
397–405	O
.	O
m¨uller	O
,	O
k.	O
r.	O
,	O
s.	O
mika	O
,	O
g.	O
r¨atsch	O
,	O
k.	O
tsuda	O
,	O
and	O
b.	O
sch¨olkopf	O
(	O
2001	O
)	O
.	O
an	O
introduction	O
to	O
kernel-	O
based	O
learning	B
algorithms	O
.	O
ieee	O
transactions	O
on	O
neural	O
networks	O
12	O
(	O
2	O
)	O
,	O
181–202	O
.	O
m¨uller	O
,	O
p.	O
and	O
f.	O
a.	O
quintana	O
(	O
2004	O
)	O
.	O
nonparametric	O
bayesian	O
data	O
analysis	O
.	O
statistical	O
science	O
19	O
(	O
1	O
)	O
,	O
95–110	O
.	O
nabney	O
,	O
i.	O
t.	O
(	O
2002	O
)	O
.	O
netlab	O
:	O
algorithms	O
for	O
pattern	O
recognition	O
.	O
springer	O
.	O
nadaraya	O
,	O
´e	O
.	O
a	O
.	O
(	O
1964	O
)	O
.	O
on	O
estimating	O
regression	B
.	O
theory	B
of	O
probability	B
and	O
its	O
applications	O
9	O
(	O
1	O
)	O
,	O
141–142	O
.	O
references	O
723	O
nag	O
,	O
r.	O
,	O
k.	O
wong	O
,	O
and	O
f.	O
fallside	O
(	O
1986	O
)	O
.	O
script	O
recognition	O
using	O
hidden	O
markov	O
models	O
.	O
in	O
icassp86	O
,	O
pp	O
.	O
2071–2074	O
.	O
ieee	O
.	O
neal	O
,	O
r.	O
m.	O
(	O
1993	O
)	O
.	O
probabilistic	O
inference	O
using	O
markov	O
chain	O
monte	O
carlo	O
methods	O
.	O
technical	O
report	O
crg-tr-93-1	O
,	O
department	O
of	O
computer	O
science	O
,	O
university	O
of	O
toronto	O
,	O
canada	O
.	O
neal	O
,	O
r.	O
m.	O
(	O
1996	O
)	O
.	O
bayesian	O
learning	B
for	O
neural	O
networks	O
.	O
springer	O
.	O
lecture	O
notes	O
in	O
statistics	O
118.	O
neal	O
,	O
r.	O
m.	O
(	O
1997	O
)	O
.	O
monte	O
carlo	O
implementation	O
of	O
gaussian	O
process	O
models	O
for	O
bayesian	O
regression	B
and	O
classiﬁcation	B
.	O
technical	O
report	O
9702	O
,	O
de-	O
partment	O
of	O
computer	O
statistics	O
,	O
university	O
of	O
toronto	O
.	O
neal	O
,	O
r.	O
m.	O
(	O
1999	O
)	O
.	O
suppressing	O
random	O
walks	O
in	O
markov	O
chain	O
monte	O
carlo	O
using	O
ordered	O
over-	O
relaxation	O
.	O
in	O
m.	O
i.	O
jordan	O
(	O
ed	O
.	O
)	O
,	O
learning	B
in	O
graphical	O
models	O
,	O
pp	O
.	O
205–228	O
.	O
mit	O
press	O
.	O
neal	O
,	O
r.	O
m.	O
(	O
2000	O
)	O
.	O
markov	O
chain	O
sampling	O
for	O
dirichlet	O
process	O
mixture	B
models	O
.	O
journal	O
of	O
computational	O
and	O
graphical	O
statistics	O
9	O
,	O
249–	O
265.	O
neal	O
,	O
r.	O
m.	O
(	O
2003	O
)	O
.	O
slice	B
sampling	I
.	O
annals	O
of	O
statis-	O
tics	O
31	O
,	O
705–767	O
.	O
neal	O
,	O
r.	O
m.	O
and	O
g.	O
e.	O
hinton	O
(	O
1999	O
)	O
.	O
a	O
new	O
view	O
of	O
the	O
em	O
algorithm	O
that	O
justiﬁes	O
incremental	O
and	O
other	O
variants	O
.	O
in	O
m.	O
i.	O
jordan	O
(	O
ed	O
.	O
)	O
,	O
learning	B
in	O
graphical	O
models	O
,	O
pp	O
.	O
355–368	O
.	O
mit	O
press	O
.	O
nelder	O
,	O
j.	O
a.	O
and	O
r.	O
w.	O
m.	O
wedderburn	O
(	O
1972	O
)	O
.	O
gen-	O
eralized	O
linear	O
models	O
.	O
journal	O
of	O
the	O
royal	O
sta-	O
tistical	O
society	O
,	O
a	O
135	O
,	O
370–384	O
.	O
nilsson	O
,	O
n.	O
j	O
.	O
(	O
1965	O
)	O
.	O
learning	B
machines	O
.	O
mcgraw-	O
hill	O
.	O
reprinted	O
as	O
the	O
mathematical	O
founda-	O
tions	O
of	O
learning	B
machines	O
,	O
morgan	O
kaufmann	O
,	O
(	O
1990	O
)	O
.	O
nocedal	O
,	O
j.	O
and	O
s.	O
j.	O
wright	O
(	O
1999	O
)	O
.	O
numerical	O
op-	O
timization	O
.	O
springer	O
.	O
nowlan	O
,	O
s.	O
j.	O
and	O
g.	O
e.	O
hinton	O
(	O
1992	O
)	O
.	O
simplifying	O
neural	O
networks	O
by	O
soft	B
weight	I
sharing	I
.	O
neural	O
computation	O
4	O
(	O
4	O
)	O
,	O
473–493	O
.	O
ogden	O
,	O
r.	O
t.	O
(	O
1997	O
)	O
.	O
essential	O
wavelets	B
for	O
statisti-	O
cal	O
applications	O
and	O
data	O
analysis	O
.	O
birkh¨auser	O
.	O
opper	O
,	O
m.	O
and	O
o.	O
winther	O
(	O
1999	O
)	O
.	O
a	O
bayesian	O
ap-	O
proach	O
to	O
on-line	B
learning	I
.	O
in	O
d.	O
saad	O
(	O
ed	O
.	O
)	O
,	O
on-	O
line	O
learning	B
in	O
neural	O
networks	O
,	O
pp	O
.	O
363–378	O
.	O
cambridge	O
university	O
press	O
.	O
opper	O
,	O
m.	O
and	O
o.	O
winther	O
(	O
2000a	O
)	O
.	O
gaussian	O
processes	O
and	O
svm	O
:	O
mean	B
ﬁeld	I
theory	I
and	O
leave-one-out	B
.	O
in	O
a.	O
j.	O
smola	O
,	O
p.	O
l.	O
bartlett	O
,	O
b.	O
sch¨olkopf	O
,	O
and	O
d.	O
shuurmans	O
(	O
eds	O
.	O
)	O
,	O
ad-	O
vances	O
in	O
large	B
margin	I
classiﬁers	O
,	O
pp	O
.	O
311–326	O
.	O
mit	O
press	O
.	O
opper	O
,	O
m.	O
and	O
o.	O
winther	O
(	O
2000b	O
)	O
.	O
gaussian	O
processes	O
for	O
classiﬁcation	O
.	O
neural	O
computa-	O
tion	O
12	O
(	O
11	O
)	O
,	O
2655–2684	O
.	O
osuna	O
,	O
e.	O
,	O
r.	O
freund	O
,	O
and	O
f.	O
girosi	O
(	O
1996	O
)	O
.	O
support	B
vector	I
machines	O
:	O
training	B
and	O
applications	O
.	O
a.i	O
.	O
memo	O
aim-1602	O
,	O
mit	O
.	O
papoulis	O
,	O
a	O
.	O
(	O
1984	O
)	O
.	O
probability	B
,	O
random	O
variables	O
,	O
and	O
stochastic	B
processes	O
(	O
second	O
ed.	O
)	O
.	O
mcgraw-	O
hill	O
.	O
parisi	O
,	O
g.	O
(	O
1988	O
)	O
.	O
statistical	O
field	O
theory	B
.	O
addison-	O
wesley	O
.	O
pearl	O
,	O
j	O
.	O
(	O
1988	O
)	O
.	O
probabilistic	O
reasoning	O
in	O
intelli-	O
gent	O
systems	O
.	O
morgan	O
kaufmann	O
.	O
pearlmutter	O
,	O
b.	O
a	O
.	O
(	O
1994	O
)	O
.	O
fast	O
exact	O
multiplication	O
by	O
the	O
hessian	O
.	O
neural	O
computation	O
6	O
(	O
1	O
)	O
,	O
147–	O
160.	O
pearlmutter	O
,	O
b.	O
a.	O
and	O
l.	O
c.	O
parra	O
(	O
1997	O
)	O
.	O
maximum	B
likelihood	I
source	O
separation	O
:	O
a	O
context-sensitive	O
generalization	B
of	O
ica	O
.	O
in	O
m.	O
c.	O
mozer	O
,	O
m.	O
i.	O
jor-	O
dan	O
,	O
and	O
t.	O
petsche	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
volume	O
9	O
,	O
pp	O
.	O
613–619	O
.	O
mit	O
press	O
.	O
pearson	O
,	O
k.	O
(	O
1901	O
)	O
.	O
on	O
lines	O
and	O
planes	O
of	O
closest	O
ﬁt	O
to	O
systems	O
of	O
points	O
in	O
space	O
.	O
the	O
london	O
,	O
edin-	O
burgh	O
and	O
dublin	O
philosophical	O
magazine	O
and	O
journal	O
of	O
science	O
,	O
sixth	O
series	O
2	O
,	O
559–572	O
.	O
platt	O
,	O
j.	O
c.	O
(	O
1999	O
)	O
.	O
fast	O
training	O
of	O
support	B
vector	I
machines	O
using	O
sequential	B
minimal	I
optimization	I
.	O
in	O
b.	O
sch¨olkopf	O
,	O
c.	O
j.	O
c.	O
burges	O
,	O
and	O
a.	O
j.	O
smola	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
kernel	O
methods	O
–	O
support	B
vector	I
learning	O
,	O
pp	O
.	O
185–208	O
.	O
mit	O
press	O
.	O
724	O
references	O
platt	O
,	O
j.	O
c.	O
(	O
2000	O
)	O
.	O
probabilities	O
for	O
sv	O
machines	O
.	O
in	O
a.	O
j.	O
smola	O
,	O
p.	O
l.	O
bartlett	O
,	O
b.	O
sch¨olkopf	O
,	O
and	O
d.	O
shuurmans	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
large	B
margin	I
classiﬁers	O
,	O
pp	O
.	O
61–73	O
.	O
mit	O
press	O
.	O
platt	O
,	O
j.	O
c.	O
,	O
n.	O
cristianini	O
,	O
and	O
j.	O
shawe-taylor	O
(	O
2000	O
)	O
.	O
large	B
margin	I
dags	O
for	O
multiclass	O
clas-	O
siﬁcation	O
.	O
in	O
s.	O
a.	O
solla	O
,	O
t.	O
k.	O
leen	O
,	O
and	O
k.	O
r.	O
m¨uller	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
volume	O
12	O
,	O
pp	O
.	O
547–553	O
.	O
mit	O
press	O
.	O
poggio	O
,	O
t.	O
and	O
f.	O
girosi	O
(	O
1990	O
)	O
.	O
networks	O
for	O
ap-	O
proximation	B
and	O
learning	B
.	O
proceedings	O
of	O
the	O
ieee	O
78	O
(	O
9	O
)	O
,	O
1481–1497	O
.	O
powell	O
,	O
m.	O
j.	O
d.	O
(	O
1987	O
)	O
.	O
radial	O
basis	O
functions	O
for	O
multivariable	O
interpolation	O
:	O
a	O
review	O
.	O
in	O
j.	O
c.	O
mason	O
and	O
m.	O
g.	O
cox	O
(	O
eds	O
.	O
)	O
,	O
algorithms	O
for	O
approximation	O
,	O
pp	O
.	O
143–167	O
.	O
oxford	O
university	O
press	O
.	O
press	O
,	O
w.	O
h.	O
,	O
s.	O
a.	O
teukolsky	O
,	O
w.	O
t.	O
vetterling	O
,	O
and	O
b.	O
p.	O
flannery	O
(	O
1992	O
)	O
.	O
numerical	O
recipes	O
in	O
c	O
:	O
the	O
art	O
of	O
scientiﬁc	O
computing	O
(	O
second	O
ed.	O
)	O
.	O
cambridge	O
university	O
press	O
.	O
qazaz	O
,	O
c.	O
s.	O
,	O
c.	O
k.	O
i.	O
williams	O
,	O
and	O
c.	O
m.	O
bishop	O
(	O
1997	O
)	O
.	O
an	O
upper	O
bound	O
on	O
the	O
bayesian	O
error	B
bars	O
for	O
generalized	O
linear	B
regression	I
.	O
in	O
s.	O
w.	O
ellacott	O
,	O
j.	O
c.	O
mason	O
,	O
and	O
i.	O
j.	O
anderson	O
(	O
eds	O
.	O
)	O
,	O
mathematics	O
of	O
neural	O
networks	O
:	O
models	O
,	O
algo-	O
rithms	O
and	O
applications	O
,	O
pp	O
.	O
295–299	O
.	O
kluwer	O
.	O
quinlan	O
,	O
j.	O
r.	O
(	O
1986	O
)	O
.	O
induction	O
of	O
decision	O
trees	O
.	O
machine	O
learning	O
1	O
(	O
1	O
)	O
,	O
81–106	O
.	O
quinlan	O
,	O
j.	O
r.	O
(	O
1993	O
)	O
.	O
c4.5	O
:	O
programs	O
for	O
machine	O
learning	B
.	O
morgan	O
kaufmann	O
.	O
rabiner	O
,	O
l.	O
and	O
b.	O
h.	O
juang	O
(	O
1993	O
)	O
.	O
fundamentals	O
of	O
speech	B
recognition	I
.	O
prentice	O
hall	O
.	O
rabiner	O
,	O
l.	O
r.	O
(	O
1989	O
)	O
.	O
a	O
tutorial	O
on	O
hidden	O
markov	O
models	O
and	O
selected	O
applications	O
in	O
speech	O
the	O
ieee	O
77	O
(	O
2	O
)	O
,	O
recognition	O
.	O
proceedings	O
of	O
257–285	O
.	O
ramasubramanian	O
,	O
v.	O
and	O
k.	O
k.	O
paliwal	O
(	O
1990	O
)	O
.	O
a	O
generalized	B
optimization	O
of	O
the	O
k-d	O
tree	B
for	O
fast	O
nearest-neighbour	O
search	O
.	O
in	O
proceedings	O
fourth	O
ieee	O
region	O
10	O
international	O
conference	O
(	O
ten-	O
con	O
’	O
89	O
)	O
,	O
pp	O
.	O
565–568	O
.	O
ramsey	O
,	O
f.	O
(	O
1931	O
)	O
.	O
truth	O
and	O
probability	B
.	O
in	O
r.	O
braithwaite	O
(	O
ed	O
.	O
)	O
,	O
the	O
foundations	O
of	O
math-	O
ematics	O
and	O
other	O
logical	O
essays	O
.	O
humanities	O
press	O
.	O
rao	O
,	O
c.	O
r.	O
and	O
s.	O
k.	O
mitra	O
(	O
1971	O
)	O
.	O
generalized	B
in-	O
verse	O
of	O
matrices	O
and	O
its	O
applications	O
.	O
wiley	O
.	O
rasmussen	O
,	O
c.	O
e.	O
(	O
1996	O
)	O
.	O
evaluation	O
of	O
gaussian	O
processes	O
and	O
other	O
methods	O
for	O
non-linear	O
regression	B
.	O
ph	O
.	O
d.	O
thesis	O
,	O
university	O
of	O
toronto	O
.	O
rasmussen	O
,	O
c.	O
e.	O
and	O
j.	O
qui˜nonero-candela	O
(	O
2005	O
)	O
.	O
healing	O
the	O
relevance	B
vector	I
machine	I
by	O
aug-	O
mentation	O
.	O
in	O
l.	O
d.	O
raedt	O
and	O
s.	O
wrobel	O
(	O
eds	O
.	O
)	O
,	O
proceedings	O
of	O
the	O
22nd	O
international	O
confer-	O
ence	O
on	O
machine	O
learning	O
,	O
pp	O
.	O
689–696	O
.	O
rasmussen	O
,	O
c.	O
e.	O
and	O
c.	O
k.	O
i.	O
williams	O
(	O
2006	O
)	O
.	O
gaussian	O
processes	O
for	O
machine	O
learning	B
.	O
mit	O
press	O
.	O
rauch	O
,	O
h.	O
e.	O
,	O
f.	O
tung	O
,	O
and	O
c.	O
t.	O
striebel	O
(	O
1965	O
)	O
.	O
maximum	B
likelihood	I
estimates	O
of	O
linear	O
dynam-	O
ical	O
systems	O
.	O
aiaa	O
journal	O
3	O
,	O
1445–1450	O
.	O
ricotti	O
,	O
l.	O
p.	O
,	O
s.	O
ragazzini	O
,	O
and	O
g.	O
martinelli	O
(	O
1988	O
)	O
.	O
learning	B
of	O
word	O
stress	O
in	O
a	O
sub-optimal	O
second	B
order	I
backpropagation	O
neural	B
network	I
.	O
in	O
pro-	O
ceedings	O
of	O
the	O
ieee	O
international	O
conference	O
on	O
neural	O
networks	O
,	O
volume	O
1	O
,	O
pp	O
.	O
355–361	O
.	O
ieee	O
.	O
ripley	O
,	O
b.	O
d.	O
(	O
1996	O
)	O
.	O
pattern	O
recognition	O
and	O
neu-	O
ral	O
networks	O
.	O
cambridge	O
university	O
press	O
.	O
robbins	O
,	O
h.	O
and	O
s.	O
monro	O
(	O
1951	O
)	O
.	O
a	O
stochastic	B
approximation	O
method	O
.	O
annals	O
of	O
mathematical	O
statistics	O
22	O
,	O
400–407	O
.	O
robert	O
,	O
c.	O
p.	O
and	O
g.	O
casella	O
(	O
1999	O
)	O
.	O
monte	O
carlo	O
statistical	O
methods	O
.	O
springer	O
.	O
rockafellar	O
,	O
r.	O
(	O
1972	O
)	O
.	O
convex	O
analysis	O
.	O
princeton	O
university	O
press	O
.	O
rosenblatt	O
,	O
f.	O
(	O
1962	O
)	O
.	O
principles	O
of	O
neurodynam-	O
ics	O
:	O
perceptrons	O
and	O
the	O
theory	B
of	O
brain	O
mech-	O
anisms	O
.	O
spartan	O
.	O
roth	O
,	O
v.	O
and	O
v.	O
steinhage	O
(	O
2000	O
)	O
.	O
nonlinear	O
discrim-	O
inant	O
analysis	O
using	O
kernel	O
functions	O
.	O
in	O
s.	O
a.	O
references	O
725	O
solla	O
,	O
t.	O
k.	O
leen	O
,	O
and	O
k.	O
r.	O
m¨uller	O
(	O
eds	O
.	O
)	O
,	O
ad-	O
vances	O
in	O
neural	O
information	O
processing	O
sys-	O
tems	O
,	O
volume	O
12.	O
mit	O
press	O
.	O
roweis	O
,	O
s.	O
(	O
1998	O
)	O
.	O
em	O
algorithms	O
for	O
pca	O
and	O
spca	O
.	O
in	O
m.	O
i.	O
jordan	O
,	O
m.	O
j.	O
kearns	O
,	O
and	O
s.	O
a.	O
solla	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
volume	O
10	O
,	O
pp	O
.	O
626–632	O
.	O
mit	O
press	O
.	O
roweis	O
,	O
s.	O
and	O
z.	O
ghahramani	O
(	O
1999	O
)	O
.	O
a	O
unifying	O
review	O
of	O
linear	O
gaussian	O
models	O
.	O
neural	O
com-	O
putation	O
11	O
(	O
2	O
)	O
,	O
305–345	O
.	O
roweis	O
,	O
s.	O
and	O
l.	O
saul	O
(	O
2000	O
,	O
december	O
)	O
.	O
nonlinear	O
dimensionality	O
reduction	O
by	O
locally	O
linear	O
em-	O
bedding	O
.	O
science	O
290	O
,	O
2323–2326	O
.	O
rubin	O
,	O
d.	O
b	O
.	O
(	O
1983	O
)	O
.	O
iteratively	O
reweighted	O
least	O
squares	O
.	O
in	O
encyclopedia	O
of	O
statistical	O
sciences	O
,	O
volume	O
4	O
,	O
pp	O
.	O
272–275	O
.	O
wiley	O
.	O
rubin	O
,	O
d.	O
b.	O
and	O
d.	O
t.	O
thayer	O
(	O
1982	O
)	O
.	O
em	O
al-	O
gorithms	O
for	O
ml	O
factor	B
analysis	I
.	O
psychome-	O
trika	O
47	O
(	O
1	O
)	O
,	O
69–76	O
.	O
rumelhart	O
,	O
d.	O
e.	O
,	O
g.	O
e.	O
hinton	O
,	O
and	O
r.	O
j.	O
williams	O
(	O
1986	O
)	O
.	O
learning	B
internal	O
representations	O
by	O
er-	O
ror	O
propagation	O
.	O
in	O
d.	O
e.	O
rumelhart	O
,	O
j.	O
l.	O
mc-	O
clelland	O
,	O
and	O
the	O
pdp	O
research	O
group	O
(	O
eds	O
.	O
)	O
,	O
parallel	O
distributed	O
processing	O
:	O
explorations	O
in	O
the	O
microstructure	O
of	O
cognition	O
,	O
volume	O
1	O
:	O
foundations	O
,	O
pp	O
.	O
318–362	O
.	O
mit	O
press	O
.	O
reprinted	O
in	O
anderson	O
and	O
rosenfeld	O
(	O
1988	O
)	O
.	O
rumelhart	O
,	O
d.	O
e.	O
,	O
j.	O
l.	O
mcclelland	O
,	O
and	O
the	O
pdp	O
re-	O
search	O
group	O
(	O
eds	O
.	O
)	O
(	O
1986	O
)	O
.	O
parallel	O
distributed	O
processing	O
:	O
explorations	O
in	O
the	O
microstruc-	O
ture	O
of	O
cognition	O
,	O
volume	O
1	O
:	O
foundations	O
.	O
mit	O
press	O
.	O
sagan	O
,	O
h.	O
(	O
1969	O
)	O
.	O
introduction	O
to	O
the	O
calculus	B
of	I
variations	I
.	O
dover	O
.	O
savage	O
,	O
l.	O
j	O
.	O
(	O
1961	O
)	O
.	O
the	O
subjective	O
basis	O
of	O
sta-	O
tistical	O
practice	O
.	O
technical	O
report	O
,	O
department	O
of	O
statistics	O
,	O
university	O
of	O
michigan	O
,	O
ann	O
arbor	O
.	O
sch¨olkopf	O
,	O
b.	O
,	O
j.	O
platt	O
,	O
j.	O
shawe-taylor	O
,	O
a.	O
smola	O
,	O
and	O
r.	O
c.	O
williamson	O
(	O
2001	O
)	O
.	O
estimating	O
the	O
sup-	O
port	O
of	O
a	O
high-dimensional	O
distribution	O
.	O
neural	O
computation	O
13	O
(	O
7	O
)	O
,	O
1433–1471	O
.	O
sch¨olkopf	O
,	O
b.	O
,	O
a.	O
smola	O
,	O
and	O
k.-r.	O
m¨uller	O
(	O
1998	O
)	O
.	O
nonlinear	O
component	O
analysis	O
as	O
a	O
kernel	O
eigenvalue	O
problem	O
.	O
neural	O
computation	O
10	O
(	O
5	O
)	O
,	O
1299–1319	O
.	O
sch¨olkopf	O
,	O
b.	O
,	O
a.	O
smola	O
,	O
r.	O
c.	O
williamson	O
,	O
and	O
p.	O
l.	O
bartlett	O
(	O
2000	O
)	O
.	O
new	O
support	B
vector	I
algorithms	O
.	O
neural	O
computation	O
12	O
(	O
5	O
)	O
,	O
1207–1245	O
.	O
sch¨olkopf	O
,	O
b.	O
and	O
a.	O
j.	O
smola	O
(	O
2002	O
)	O
.	O
learning	B
with	O
kernels	O
.	O
mit	O
press	O
.	O
schwarz	O
,	O
g.	O
(	O
1978	O
)	O
.	O
estimating	O
the	O
dimension	O
of	O
a	O
model	O
.	O
annals	O
of	O
statistics	O
6	O
,	O
461–464	O
.	O
schwarz	O
,	O
h.	O
r.	O
(	O
1988	O
)	O
.	O
finite	O
element	O
methods	O
.	O
aca-	O
demic	O
press	O
.	O
seeger	O
,	O
m.	O
(	O
2003	O
)	O
.	O
bayesian	O
gaussian	O
process	O
mod-	O
els	O
:	O
pac-bayesian	O
generalization	B
error	O
bounds	O
and	O
sparse	O
approximations	O
.	O
ph	O
.	O
d.	O
thesis	O
,	O
uni-	O
versity	O
of	O
edinburg	O
.	O
seeger	O
,	O
m.	O
,	O
c.	O
k.	O
i.	O
williams	O
,	O
and	O
n.	O
lawrence	O
(	O
2003	O
)	O
.	O
fast	O
forward	O
selection	O
to	O
speed	O
up	O
sparse	O
gaussian	O
processes	O
.	O
in	O
c.	O
m.	O
bishop	O
and	O
b.	O
frey	O
(	O
eds	O
.	O
)	O
,	O
proceedings	O
ninth	O
international	O
work-	O
shop	O
on	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
,	O
key	O
west	O
,	O
florida	O
.	O
shachter	O
,	O
r.	O
d.	O
and	O
m.	O
peot	O
(	O
1990	O
)	O
.	O
simulation	O
ap-	O
proaches	O
to	O
general	O
probabilistic	O
inference	O
on	O
be-	O
lief	O
networks	O
.	O
in	O
p.	O
p.	O
bonissone	O
,	O
m.	O
henrion	O
,	O
l.	O
n.	O
kanal	O
,	O
and	O
j.	O
f.	O
lemmer	O
(	O
eds	O
.	O
)	O
,	O
uncer-	O
tainty	O
in	O
artiﬁcial	O
intelligence	O
,	O
volume	O
5.	O
else-	O
vier	O
.	O
shannon	O
,	O
c.	O
e.	O
(	O
1948	O
)	O
.	O
a	O
mathematical	O
theory	B
of	O
communication	O
.	O
the	O
bell	O
system	O
technical	O
jour-	O
nal	O
27	O
(	O
3	O
)	O
,	O
379–423	O
and	O
623–656	O
.	O
shawe-taylor	O
,	O
j.	O
and	O
n.	O
cristianini	O
(	O
2004	O
)	O
.	O
kernel	O
methods	O
for	O
pattern	O
analysis	O
.	O
cambridge	O
uni-	O
versity	O
press	O
.	O
sietsma	O
,	O
j.	O
and	O
r.	O
j.	O
f.	O
dow	O
(	O
1991	O
)	O
.	O
creating	O
artiﬁ-	O
cial	O
neural	O
networks	O
that	O
generalize	O
.	O
neural	O
net-	O
works	O
4	O
(	O
1	O
)	O
,	O
67–79	O
.	O
simard	O
,	O
p.	O
,	O
y.	O
le	O
cun	O
,	O
and	O
j.	O
denker	O
(	O
1993	O
)	O
.	O
efﬁ-	O
cient	O
pattern	O
recognition	O
using	O
a	O
new	O
transforma-	O
tion	O
distance	O
.	O
in	O
s.	O
j.	O
hanson	O
,	O
j.	O
d.	O
cowan	O
,	O
and	O
726	O
references	O
c.	O
l.	O
giles	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
informa-	O
tion	O
processing	O
systems	O
,	O
volume	O
5	O
,	O
pp	O
.	O
50–58	O
.	O
morgan	O
kaufmann	O
.	O
simard	O
,	O
p.	O
,	O
b.	O
victorri	O
,	O
y.	O
le	O
cun	O
,	O
and	O
j.	O
denker	O
(	O
1992	O
)	O
.	O
tangent	O
prop	O
–	O
a	O
formalism	O
for	O
specify-	O
ing	O
selected	O
invariances	O
in	O
an	O
adaptive	O
network	O
.	O
in	O
j.	O
e.	O
moody	O
,	O
s.	O
j.	O
hanson	O
,	O
and	O
r.	O
p.	O
lippmann	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
information	O
process-	O
ing	O
systems	O
,	O
volume	O
4	O
,	O
pp	O
.	O
895–903	O
.	O
morgan	O
kaufmann	O
.	O
simard	O
,	O
p.	O
y.	O
,	O
d.	O
steinkraus	O
,	O
and	O
j.	O
platt	O
(	O
2003	O
)	O
.	O
best	O
practice	O
for	O
convolutional	O
neural	O
networks	O
applied	O
to	O
visual	O
document	O
analysis	O
.	O
in	O
pro-	O
ceedings	O
international	O
conference	O
on	O
document	O
analysis	O
and	O
recognition	O
(	O
icdar	O
)	O
,	O
pp	O
.	O
958–	O
962.	O
ieee	O
computer	O
society	O
.	O
sirovich	O
,	O
l.	O
(	O
1987	O
)	O
.	O
turbulence	O
and	O
the	O
dynamics	O
of	O
coherent	O
structures	O
.	O
quarterly	O
applied	O
math-	O
ematics	O
45	O
(	O
3	O
)	O
,	O
561–590	O
.	O
smola	O
,	O
a.	O
j.	O
and	O
p.	O
bartlett	O
(	O
2001	O
)	O
.	O
sparse	O
greedy	O
gaussian	O
process	O
regression	B
.	O
in	O
t.	O
k.	O
leen	O
,	O
t.	O
g.	O
dietterich	O
,	O
and	O
v.	O
tresp	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neu-	O
ral	O
information	O
processing	O
systems	O
,	O
volume	O
13	O
,	O
pp	O
.	O
619–625	O
.	O
mit	O
press	O
.	O
spiegelhalter	O
,	O
d.	O
and	O
s.	O
lauritzen	O
(	O
1990	O
)	O
.	O
sequential	O
updating	O
of	O
conditional	B
probabilities	O
on	O
directed	B
graphical	O
structures	O
.	O
networks	O
20	O
,	O
579–605	O
.	O
stinchecombe	O
,	O
m.	O
and	O
h.	O
white	O
(	O
1989	O
)	O
.	O
universal	O
approximation	O
using	O
feed-forward	O
networks	O
with	O
non-sigmoid	O
hidden	O
layer	O
activation	O
functions	O
.	O
in	O
international	O
joint	O
conference	O
on	O
neural	O
net-	O
works	O
,	O
volume	O
1	O
,	O
pp	O
.	O
613–618	O
.	O
ieee	O
.	O
stone	O
,	O
j.	O
v.	O
(	O
2004	O
)	O
.	O
independent	O
component	O
analy-	O
sis	O
:	O
a	O
tutorial	O
introduction	O
.	O
mit	O
press	O
.	O
sung	O
,	O
k.	O
k.	O
and	O
t.	O
poggio	O
(	O
1994	O
)	O
.	O
example-based	O
learning	B
for	O
view-based	O
human	O
face	B
detection	I
.	O
a.i	O
.	O
memo	O
1521	O
,	O
mit	O
.	O
sutton	O
,	O
r.	O
s.	O
and	O
a.	O
g.	O
barto	O
(	O
1998	O
)	O
.	O
reinforcement	B
learning	I
:	O
an	O
introduction	O
.	O
mit	O
press	O
.	O
tarassenko	O
,	O
l.	O
(	O
1995	O
)	O
.	O
novelty	B
detection	I
for	O
the	O
identiﬁcation	O
of	O
masses	O
in	O
mamograms	O
.	O
in	O
pro-	O
ceedings	O
fourth	O
iee	O
international	O
conference	O
on	O
artiﬁcial	O
neural	O
networks	O
,	O
volume	O
4	O
,	O
pp	O
.	O
442–447	O
.	O
iee	O
.	O
tax	O
,	O
d.	O
and	O
r.	O
duin	O
(	O
1999	O
)	O
.	O
data	O
domain	O
descrip-	O
tion	O
by	O
support	O
vectors	O
.	O
in	O
m.	O
verleysen	O
(	O
ed	O
.	O
)	O
,	O
proceedings	O
european	O
symposium	O
on	O
artiﬁcial	O
neural	O
networks	O
,	O
esann	O
,	O
pp	O
.	O
251–256	O
.	O
d.	O
facto	O
press	O
.	O
teh	O
,	O
y.	O
w.	O
,	O
m.	O
i.	O
jordan	O
,	O
m.	O
j.	O
beal	O
,	O
and	O
d.	O
m.	O
blei	O
(	O
2006	O
)	O
.	O
hierarchical	B
dirichlet	O
processes	O
.	O
journal	O
of	O
the	O
americal	O
statistical	O
association	O
.	O
to	O
appear	O
.	O
tenenbaum	O
,	O
j.	O
b.	O
,	O
v.	O
de	O
silva	O
,	O
and	O
j.	O
c.	O
langford	O
(	O
2000	O
,	O
december	O
)	O
.	O
a	O
global	O
framework	O
for	O
non-	O
linear	O
dimensionality	O
reduction	O
.	O
science	O
290	O
,	O
2319–2323	O
.	O
tesauro	O
,	O
g.	O
(	O
1994	O
)	O
.	O
td-gammon	O
,	O
a	O
self-teaching	O
backgammon	B
program	O
,	O
achieves	O
master-level	O
play	O
.	O
neural	O
computation	O
6	O
(	O
2	O
)	O
,	O
215–219	O
.	O
thiesson	O
,	O
b.	O
,	O
d.	O
m.	O
chickering	O
,	O
d.	O
heckerman	O
,	O
and	O
c.	O
meek	O
(	O
2004	O
)	O
.	O
arma	O
time-series	O
modelling	O
with	O
graphical	O
models	O
.	O
in	O
m.	O
chickering	O
and	O
j.	O
halpern	O
(	O
eds	O
.	O
)	O
,	O
proceedings	O
of	O
the	O
twentieth	O
conference	O
on	O
uncertainty	O
in	O
artiﬁcial	O
intelli-	O
gence	O
,	O
banff	O
,	O
canada	O
,	O
pp	O
.	O
552–560	O
.	O
auai	O
press	O
.	O
tibshirani	O
,	O
r.	O
(	O
1996	O
)	O
.	O
regression	B
shrinkage	O
and	O
se-	O
lection	O
via	O
the	O
lasso	B
.	O
journal	O
of	O
the	O
royal	O
statis-	O
tical	O
society	O
,	O
b	O
58	O
,	O
267–288	O
.	O
tierney	O
,	O
l.	O
(	O
1994	O
)	O
.	O
markov	O
chains	O
for	O
exploring	O
pos-	O
terior	O
distributions	O
.	O
annals	O
of	O
statistics	O
22	O
(	O
4	O
)	O
,	O
1701–1762	O
.	O
tikhonov	O
,	O
a.	O
n.	O
and	O
v.	O
y.	O
arsenin	O
(	O
1977	O
)	O
.	O
solutions	O
of	O
ill-posed	O
problems	O
.	O
v.	O
h.	O
winston	O
.	O
tino	O
,	O
p.	O
and	O
i.	O
t.	O
nabney	O
(	O
2002	O
)	O
.	O
hierarchical	B
gtm	O
:	O
constructing	O
localized	O
non-linear	O
projec-	O
tion	O
manifolds	O
in	O
a	O
principled	O
way	O
.	O
ieee	O
trans-	O
actions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelli-	O
gence	O
24	O
(	O
5	O
)	O
,	O
639–656	O
.	O
svens´en	O
,	O
m.	O
and	O
c.	O
m.	O
bishop	O
(	O
2004	O
)	O
.	O
ro-	O
bust	O
bayesian	O
mixture	B
modelling	O
.	O
neurocomput-	O
ing	O
64	O
,	O
235–252	O
.	O
tino	O
,	O
p.	O
,	O
i.	O
t.	O
nabney	O
,	O
and	O
y.	O
sun	O
(	O
2001	O
)	O
.	O
us-	O
ing	O
directional	O
curvatures	O
to	O
visualize	O
folding	O
patterns	O
of	O
the	O
gtm	O
projection	O
manifolds	O
.	O
in	O
references	O
727	O
g.	O
dorffner	O
,	O
h.	O
bischof	O
,	O
and	O
k.	O
hornik	O
(	O
eds	O
.	O
)	O
,	O
artiﬁcial	O
neural	O
networks	O
–	O
icann	O
2001	O
,	O
pp	O
.	O
421–428	O
.	O
springer	O
.	O
vapnik	O
,	O
v.	O
n.	O
(	O
1982	O
)	O
.	O
estimation	O
of	O
dependences	O
based	O
on	O
empirical	O
data	O
.	O
springer	O
.	O
vapnik	O
,	O
v.	O
n.	O
(	O
1995	O
)	O
.	O
the	O
nature	O
of	O
statistical	O
learn-	O
tipping	O
,	O
m.	O
e.	O
(	O
1999	O
)	O
.	O
probabilistic	O
visualisation	O
of	O
high-dimensional	O
binary	O
data	O
.	O
in	O
m.	O
s.	O
kearns	O
,	O
s.	O
a.	O
solla	O
,	O
and	O
d.	O
a.	O
cohn	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
vol-	O
ume	O
11	O
,	O
pp	O
.	O
592–598	O
.	O
mit	O
press	O
.	O
tipping	O
,	O
m.	O
e.	O
(	O
2001	O
)	O
.	O
sparse	O
bayesian	O
learning	B
and	O
the	O
relevance	B
vector	I
machine	I
.	O
journal	O
of	O
ma-	O
chine	O
learning	B
research	O
1	O
,	O
211–244	O
.	O
tipping	O
,	O
m.	O
e.	O
and	O
c.	O
m.	O
bishop	O
(	O
1997	O
)	O
.	O
probabilis-	O
tic	O
principal	B
component	I
analysis	I
.	O
technical	O
re-	O
port	O
ncrg/97/010	O
,	O
neural	O
computing	O
research	O
group	O
,	O
aston	O
university	O
.	O
tipping	O
,	O
m.	O
e.	O
and	O
c.	O
m.	O
bishop	O
(	O
1999a	O
)	O
.	O
mixtures	O
of	O
probabilistic	O
principal	O
component	O
analyzers	O
.	O
neural	O
computation	O
11	O
(	O
2	O
)	O
,	O
443–482	O
.	O
tipping	O
,	O
m.	O
e.	O
and	O
c.	O
m.	O
bishop	O
(	O
1999b	O
)	O
.	O
prob-	O
abilistic	O
principal	B
component	I
analysis	I
.	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
,	O
series	O
b	O
21	O
(	O
3	O
)	O
,	O
611–622	O
.	O
tipping	O
,	O
m.	O
e.	O
and	O
a.	O
faul	O
(	O
2003	O
)	O
.	O
fast	O
marginal	O
likelihood	O
maximization	O
for	O
sparse	O
bayesian	O
models	O
.	O
in	O
c.	O
m.	O
bishop	O
and	O
b.	O
frey	O
(	O
eds	O
.	O
)	O
,	O
proceedings	O
ninth	O
international	O
workshop	O
on	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
,	O
key	O
west	O
,	O
florida	O
.	O
tong	O
,	O
s.	O
and	O
d.	O
koller	O
(	O
2000	O
)	O
.	O
restricted	O
bayes	O
op-	O
timal	O
classiﬁers	O
.	O
in	O
proceedings	O
17th	O
national	O
conference	O
on	O
artiﬁcial	O
intelligence	O
,	O
pp	O
.	O
658–	O
664.	O
aaai	O
.	O
tresp	O
,	O
v.	O
(	O
2001	O
)	O
.	O
scaling	O
kernel-based	O
systems	O
to	O
large	O
data	O
sets	O
.	O
data	O
mining	O
and	O
knowledge	O
dis-	O
covery	O
5	O
(	O
3	O
)	O
,	O
197–211	O
.	O
uhlenbeck	O
,	O
g.	O
e.	O
and	O
l.	O
s.	O
ornstein	O
(	O
1930	O
)	O
.	O
on	O
the	O
theory	B
of	O
brownian	O
motion	O
.	O
phys	O
.	O
rev	O
.	O
36	O
,	O
823–	O
841.	O
ing	B
theory	I
.	O
springer	O
.	O
vapnik	O
,	O
v.	O
n.	O
(	O
1998	O
)	O
.	O
statistical	B
learning	I
theory	I
.	O
wi-	O
ley	O
.	O
veropoulos	O
,	O
k.	O
,	O
c.	O
campbell	O
,	O
and	O
n.	O
cristianini	O
(	O
1999	O
)	O
.	O
controlling	O
the	O
sensitivity	O
of	O
support	B
vector	I
machines	O
.	O
in	O
proceedings	O
of	O
the	O
interna-	O
tional	O
joint	O
conference	O
on	O
artiﬁcial	O
intelligence	O
(	O
ijcai99	O
)	O
,	O
workshop	O
ml3	O
,	O
pp	O
.	O
55–60	O
.	O
vidakovic	O
,	O
b	O
.	O
(	O
1999	O
)	O
.	O
statistical	O
modelling	O
by	O
wavelets	B
.	O
wiley	O
.	O
viola	O
,	O
p.	O
and	O
m.	O
jones	O
(	O
2004	O
)	O
.	O
robust	O
real-time	O
face	B
detection	I
.	O
international	O
journal	O
of	O
computer	O
vi-	O
sion	B
57	O
(	O
2	O
)	O
,	O
137–154	O
.	O
viterbi	O
,	O
a.	O
j	O
.	O
(	O
1967	O
)	O
.	O
error	B
bounds	O
for	O
convolu-	O
tional	O
codes	O
and	O
an	O
asymptotically	O
optimum	O
de-	O
coding	O
algorithm	O
.	O
ieee	O
transactions	O
on	O
infor-	O
mation	B
theory	O
it-13	O
,	O
260–267	O
.	O
viterbi	O
,	O
a.	O
j.	O
and	O
j.	O
k.	O
omura	O
(	O
1979	O
)	O
.	O
principles	O
of	O
digital	O
communication	O
and	O
coding	O
.	O
mcgraw-	O
hill	O
.	O
wahba	O
,	O
g.	O
(	O
1975	O
)	O
.	O
a	O
comparison	O
of	O
gcv	O
and	O
gml	O
for	O
choosing	O
the	O
smoothing	B
parameter	I
in	O
the	O
gen-	O
eralized	O
spline	O
smoothing	O
problem	O
.	O
numerical	O
mathematics	O
24	O
,	O
383–393	O
.	O
wainwright	O
,	O
m.	O
j.	O
,	O
t.	O
s.	O
jaakkola	O
,	O
and	O
a.	O
s.	O
willsky	O
(	O
2005	O
)	O
.	O
a	O
new	O
class	O
of	O
upper	O
bounds	O
on	O
the	O
log	O
partition	O
function	O
.	O
ieee	O
transactions	O
on	O
infor-	O
mation	B
theory	O
51	O
,	O
2313–2335	O
.	O
walker	O
,	O
a.	O
m.	O
(	O
1969	O
)	O
.	O
on	O
the	O
asymptotic	O
behaviour	O
of	O
posterior	O
distributions	O
.	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
,	O
b	O
31	O
(	O
1	O
)	O
,	O
80–88	O
.	O
walker	O
,	O
s.	O
g.	O
,	O
p.	O
damien	O
,	O
p.	O
w.	O
laud	O
,	O
and	O
a.	O
f.	O
m.	O
smith	O
(	O
1999	O
)	O
.	O
bayesian	O
nonparametric	O
inference	O
for	O
random	O
distributions	O
and	O
related	O
functions	O
(	O
with	O
discussion	O
)	O
.	O
journal	O
of	O
the	O
royal	O
statisti-	O
cal	O
society	O
,	O
b	O
61	O
(	O
3	O
)	O
,	O
485–527	O
.	O
valiant	O
,	O
l.	O
g.	O
(	O
1984	O
)	O
.	O
a	O
theory	B
of	O
the	O
learnable	O
.	O
communications	O
of	O
the	O
association	O
for	O
comput-	O
ing	O
machinery	O
27	O
,	O
1134–1142	O
.	O
watson	O
,	O
g.	O
s.	O
(	O
1964	O
)	O
.	O
smooth	O
regression	B
analysis	O
.	O
sankhy¯a	O
:	O
the	O
indian	O
journal	O
of	O
statistics	O
.	O
series	O
a	O
26	O
,	O
359–372	O
.	O
728	O
references	O
webb	O
,	O
a.	O
r.	O
(	O
1994	O
)	O
.	O
functional	B
approximation	O
by	O
feed-forward	O
networks	O
:	O
a	O
least-squares	O
approach	O
to	O
generalisation	O
.	O
ieee	O
transactions	O
on	O
neural	O
networks	O
5	O
(	O
3	O
)	O
,	O
363–371	O
.	O
williams	O
,	O
o.	O
,	O
a.	O
blake	O
,	O
and	O
r.	O
cipolla	O
(	O
2005	O
)	O
.	O
sparse	O
bayesian	O
learning	B
for	O
efﬁcient	O
visual	O
tracking	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
27	O
(	O
8	O
)	O
,	O
1292–1304	O
.	O
williams	O
,	O
p.	O
m.	O
(	O
1996	O
)	O
.	O
using	O
neural	O
networks	O
to	O
model	O
conditional	O
multivariate	O
densities	O
.	O
neural	O
computation	O
8	O
(	O
4	O
)	O
,	O
843–854	O
.	O
winn	O
,	O
j.	O
and	O
c.	O
m.	O
bishop	O
(	O
2005	O
)	O
.	O
variational	B
mes-	O
sage	O
passing	O
.	O
journal	O
of	O
machine	O
learning	O
re-	O
search	O
6	O
,	O
661–694	O
.	O
zarchan	O
,	O
p.	O
and	O
h.	O
musoff	O
(	O
2005	O
)	O
.	O
fundamentals	O
of	O
kalman	O
filtering	O
:	O
a	O
practical	O
approach	O
(	O
sec-	O
ond	O
ed.	O
)	O
.	O
aiaa	O
.	O
weisstein	O
,	O
e.	O
w.	O
(	O
1999	O
)	O
.	O
crc	O
concise	O
encyclopedia	O
of	O
mathematics	O
.	O
chapman	O
and	O
hall	O
,	O
and	O
crc	O
.	O
weston	O
,	O
j.	O
and	O
c.	O
watkins	O
(	O
1999	O
)	O
.	O
multi-class	O
sup-	O
port	O
vector	O
machines	O
.	O
in	O
m.	O
verlysen	O
(	O
ed	O
.	O
)	O
,	O
pro-	O
ceedings	O
esann	O
’	O
99	O
,	O
brussels	O
.	O
d-facto	O
publica-	O
tions	O
.	O
whittaker	O
,	O
j	O
.	O
(	O
1990	O
)	O
.	O
graphical	O
models	O
in	O
applied	O
multivariate	O
statistics	O
.	O
wiley	O
.	O
widrow	O
,	O
b.	O
and	O
m.	O
e.	O
hoff	O
(	O
1960	O
)	O
.	O
adaptive	O
switching	O
circuits	O
.	O
in	O
ire	O
wescon	O
convention	O
record	O
,	O
volume	O
4	O
,	O
pp	O
.	O
96–104	O
.	O
reprinted	O
in	O
an-	O
derson	O
and	O
rosenfeld	O
(	O
1988	O
)	O
.	O
widrow	O
,	O
b.	O
and	O
m.	O
a.	O
lehr	O
(	O
1990	O
)	O
.	O
30	O
years	O
of	O
adap-	O
tive	O
neural	O
networks	O
:	O
perceptron	B
,	O
madeline	O
,	O
and	O
backpropagation	B
.	O
proceedings	O
of	O
the	O
ieee	O
78	O
(	O
9	O
)	O
,	O
1415–1442	O
.	O
wiegerinck	O
,	O
w.	O
and	O
t.	O
heskes	O
(	O
2003	O
)	O
.	O
fractional	B
belief	I
propagation	I
.	O
in	O
s.	O
becker	O
,	O
s.	O
thrun	O
,	O
and	O
k.	O
obermayer	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
infor-	O
mation	B
processing	O
systems	O
,	O
volume	O
15	O
,	O
pp	O
.	O
455–	O
462.	O
mit	O
press	O
.	O
williams	O
,	O
c.	O
k.	O
i	O
.	O
(	O
1998	O
)	O
.	O
computation	O
with	O
inﬁ-	O
nite	O
neural	O
networks	O
.	O
neural	O
computation	O
10	O
(	O
5	O
)	O
,	O
1203–1216	O
.	O
williams	O
,	O
c.	O
k.	O
i	O
.	O
(	O
1999	O
)	O
.	O
prediction	O
with	O
gaussian	O
processes	O
:	O
from	O
linear	B
regression	I
to	O
linear	O
pre-	O
diction	O
and	O
beyond	O
.	O
in	O
m.	O
i.	O
jordan	O
(	O
ed	O
.	O
)	O
,	O
learn-	O
ing	O
in	O
graphical	O
models	O
,	O
pp	O
.	O
599–621	O
.	O
mit	O
press	O
.	O
williams	O
,	O
c.	O
k.	O
i.	O
and	O
d.	O
barber	O
(	O
1998	O
)	O
.	O
bayesian	O
classiﬁcation	B
with	O
gaussian	O
processes	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
20	O
,	O
1342–1351	O
.	O
williams	O
,	O
c.	O
k.	O
i.	O
and	O
m.	O
seeger	O
(	O
2001	O
)	O
.	O
using	O
the	O
nystrom	O
method	O
to	O
speed	O
up	O
kernel	O
machines	O
.	O
in	O
t.	O
k.	O
leen	O
,	O
t.	O
g.	O
dietterich	O
,	O
and	O
v.	O
tresp	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
sys-	O
tems	O
,	O
volume	O
13	O
,	O
pp	O
.	O
682–688	O
.	O
mit	O
press	O
.	O