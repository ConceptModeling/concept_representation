machine	O
learning	O
,	O
neural	O
and	O
statistical	B
introduction	O
of	O
leeds	O
d.	O
michie	O
(	O
1	O
)	O
,	O
d.	O
j.	O
spiegelhalter	O
(	O
2	O
)	O
and	O
c.	O
c.	O
taylor	O
(	O
3	O
)	O
(	O
1	O
)	O
university	O
of	O
strathclyde	O
,	O
(	O
2	O
)	O
mrc	O
biostatistics	O
unit	O
,	O
cambridge	O
and	O
(	O
3	O
)	O
university	O
1.1	O
introduction	O
the	O
aim	O
of	O
this	O
book	O
is	O
to	O
provide	O
an	O
up-to-date	O
review	O
of	O
different	O
approaches	O
to	O
clas-	O
siﬁcation	O
,	O
compare	O
their	O
performance	O
on	O
a	O
wide	O
range	O
of	O
challenging	O
data-sets	O
,	O
and	O
draw	O
conclusions	O
on	O
their	O
applicability	O
to	O
realistic	O
industrial	O
problems	O
.	O
before	O
describing	O
the	O
contents	O
,	O
we	O
ﬁrst	O
need	O
to	O
deﬁne	O
what	O
we	O
mean	O
by	O
classiﬁcation	B
,	O
give	O
some	O
background	O
to	O
the	O
different	O
perspectives	O
on	O
the	O
task	O
,	O
and	O
introduce	O
the	O
european	O
community	O
statlog	O
project	O
whose	O
results	O
form	O
the	O
basis	O
for	O
this	O
book	O
.	O
1.2	O
classification	O
the	O
task	O
of	O
classiﬁcation	B
occurs	O
in	O
a	O
wide	O
range	O
of	O
human	O
activity	O
.	O
at	O
its	O
broadest	O
,	O
the	O
term	O
could	O
cover	O
any	O
context	O
in	O
which	O
some	O
decision	O
or	O
forecast	O
is	O
made	O
on	O
the	O
basis	O
of	O
currently	O
available	O
information	O
,	O
and	O
a	O
classiﬁcation	B
procedure	O
is	O
then	O
some	O
formal	O
method	O
for	O
repeatedly	O
making	O
such	O
judgments	O
in	O
new	O
situations	O
.	O
in	O
this	O
book	O
we	O
shall	O
consider	O
a	O
more	O
restricted	O
interpretation	O
.	O
we	O
shall	O
assume	O
that	O
the	O
problem	O
concerns	O
the	O
construction	O
of	O
a	O
procedure	O
that	O
will	O
be	O
applied	O
to	O
a	O
continuing	O
sequence	O
of	O
cases	O
,	O
in	O
which	O
each	O
new	O
case	O
must	O
be	O
assigned	O
to	O
one	O
of	O
a	O
set	O
of	O
pre-deﬁned	O
classes	O
on	O
the	O
basis	O
of	O
observed	O
attributes	O
or	O
features	O
.	O
the	O
construction	O
of	O
a	O
classiﬁcation	B
procedure	O
from	O
a	O
set	O
of	O
data	O
for	O
which	O
the	O
true	O
classes	O
are	O
known	O
has	O
also	O
been	O
variously	O
termed	O
pattern	O
recognition	O
,	O
discrimination	O
,	O
or	O
supervised	O
learning	O
(	O
in	O
order	O
to	O
distinguish	O
it	O
from	O
unsupervised	O
learning	O
or	O
clustering	O
in	O
which	O
the	O
classes	O
are	O
inferred	O
from	O
the	O
data	O
)	O
.	O
contexts	O
in	O
which	O
a	O
classiﬁcation	B
task	O
is	O
fundamental	O
include	O
,	O
for	O
example	B
,	O
mechanical	O
procedures	O
for	O
sorting	O
letters	O
on	O
the	O
basis	O
of	O
machine-read	O
postcodes	O
,	O
assigning	O
individuals	O
to	O
credit	O
status	O
on	O
the	O
basis	O
of	O
ﬁnancial	O
and	O
other	O
personal	O
information	O
,	O
and	O
the	O
preliminary	O
diagnosis	O
of	O
a	O
patient	O
’	O
s	O
disease	O
in	O
order	O
to	O
select	O
immediate	O
treatment	O
while	O
awaiting	O
deﬁnitive	O
test	O
results	O
.	O
in	O
fact	O
,	O
some	O
of	O
the	O
most	O
urgent	O
problems	O
arising	O
in	O
science	O
,	O
industry	O
address	O
for	O
correspondence	O
:	O
mrc	O
biostatistics	O
unit	O
,	O
institute	O
of	O
public	O
health	O
,	O
university	O
forvie	O
site	O
,	O
robinson	O
way	O
,	O
cambridge	O
cb2	O
2sr	O
,	O
u.k.	O
2	O
introduction	O
[	O
ch	O
.	O
1	O
and	O
commerce	O
can	O
be	O
regarded	O
as	O
classiﬁcation	O
or	O
decision	O
problems	O
using	O
complex	O
and	O
often	O
very	O
extensive	O
data	O
.	O
we	O
note	O
that	O
many	O
other	O
topics	O
come	O
under	O
the	O
broad	O
heading	O
of	O
classiﬁcation	B
.	O
these	O
include	O
problems	O
of	O
control	O
,	O
which	O
is	O
brieﬂy	O
covered	O
in	O
chapter	O
13	O
.	O
1.3	O
perspectives	O
on	O
classification	O
as	O
the	O
book	O
’	O
s	O
title	O
suggests	O
,	O
a	O
wide	O
variety	O
of	O
approaches	O
has	O
been	O
taken	O
towards	O
this	O
task	O
.	O
three	O
main	O
historical	O
strands	O
of	O
research	O
can	O
be	O
identiﬁed	O
:	O
statistical	B
,	O
machine	O
learning	O
and	O
neural	O
network	O
.	O
these	O
have	O
largely	O
involved	O
different	O
professional	O
and	O
academic	O
groups	O
,	O
and	O
emphasised	O
different	O
issues	O
.	O
all	O
groups	O
have	O
,	O
however	O
,	O
had	O
some	O
objectives	B
in	O
common	O
.	O
they	O
have	O
all	O
attempted	O
to	O
derive	O
procedures	O
that	O
would	O
be	O
able	O
:	O
to	O
equal	O
,	O
if	O
not	O
exceed	O
,	O
a	O
human	O
decision-maker	O
’	O
s	O
behaviour	O
,	O
but	O
have	O
the	O
advantage	O
of	O
consistency	O
and	O
,	O
to	O
a	O
variable	O
extent	O
,	O
explicitness	O
,	O
to	O
handle	O
a	O
wide	O
variety	O
of	O
problems	O
and	O
,	O
given	O
enough	O
data	O
,	O
to	O
be	O
extremely	O
general	O
,	O
to	O
be	O
used	O
in	O
practical	O
settings	O
with	O
proven	O
success	O
.	O
1.3.1	O
statistical	B
approaches	O
two	O
main	O
phases	O
of	O
work	O
on	O
classiﬁcation	B
can	O
be	O
identiﬁed	O
within	O
the	O
statistical	B
community	O
.	O
the	O
ﬁrst	O
,	O
“	O
classical	O
”	O
phase	O
concentrated	O
on	O
derivatives	O
of	O
fisher	O
’	O
s	O
early	O
work	O
on	O
linear	O
discrimination	O
.	O
the	O
second	O
,	O
“	O
modern	O
”	O
phase	O
exploits	O
more	O
ﬂexible	O
classes	O
of	O
models	O
,	O
many	O
of	O
which	O
attempt	O
to	O
provide	O
an	O
estimate	O
of	O
the	O
joint	O
distribution	O
of	O
the	O
features	O
within	O
each	O
class	O
,	O
which	O
can	O
in	O
turn	O
provide	O
a	O
classiﬁcation	B
rule	O
.	O
statistical	B
approaches	O
are	O
generally	O
characterised	O
by	O
having	O
an	O
explicit	O
underlying	O
probability	O
model	O
,	O
which	O
provides	O
a	O
probability	O
of	O
being	O
in	O
each	O
class	O
rather	O
than	O
simply	O
a	O
classiﬁcation	B
.	O
in	O
addition	O
,	O
it	O
is	O
usually	O
assumed	O
that	O
the	O
techniques	O
will	O
be	O
used	O
by	O
statis-	O
ticians	O
,	O
and	O
hence	O
some	O
human	O
intervention	O
is	O
assumed	O
with	O
regard	O
to	O
variable	O
selection	O
and	O
transformation	O
,	O
and	O
overall	O
structuring	O
of	O
the	O
problem	O
.	O
1.3.2	O
machine	O
learning	O
machine	O
learning	O
is	O
generally	O
taken	O
to	O
encompass	O
automatic	O
computing	O
procedures	O
based	O
on	O
logical	O
or	O
binary	O
operations	O
,	O
that	O
learn	O
a	O
task	O
from	O
a	O
series	O
of	O
examples	O
.	O
here	O
we	O
are	O
just	O
concerned	O
with	O
classiﬁcation	B
,	O
and	O
it	O
is	O
arguable	O
what	O
should	O
come	O
under	O
the	O
machine	O
learning	O
umbrella	O
.	O
attention	O
has	O
focussed	O
on	O
decision-tree	O
approaches	O
,	O
in	O
which	O
classiﬁcation	B
results	O
from	O
a	O
sequence	O
of	O
logical	O
steps	O
.	O
these	O
are	O
capable	O
of	O
representing	O
the	O
most	O
complex	O
problem	O
given	O
sufﬁcient	O
data	O
(	O
but	O
this	O
may	O
mean	O
an	O
enormous	O
amount	O
!	O
)	O
.	O
other	O
techniques	O
,	O
such	O
as	O
genetic	O
algorithms	O
and	O
inductive	O
logic	O
procedures	O
(	O
ilp	O
)	O
,	O
are	O
currently	O
under	O
active	O
development	O
and	O
in	O
principle	O
would	O
allow	O
us	O
to	O
deal	O
with	O
more	O
general	O
types	O
of	O
data	O
,	O
including	O
cases	O
where	O
the	O
number	O
and	O
type	O
of	O
attributes	O
may	O
vary	O
,	O
and	O
where	O
additional	O
layers	O
of	O
learning	O
are	O
superimposed	O
,	O
with	O
hierarchical	O
structure	O
of	O
attributes	O
and	O
classes	O
and	O
so	O
on	O
.	O
machine	O
learning	O
aims	O
to	O
generate	O
classifying	O
expressions	O
simple	O
enough	O
to	O
be	O
un-	O
derstood	O
easily	O
by	O
the	O
human	O
.	O
they	O
must	O
mimic	O
human	O
reasoning	O
sufﬁciently	O
to	O
provide	O
insight	O
into	O
the	O
decision	O
process	O
.	O
like	O
statistical	B
approaches	O
,	O
background	O
knowledge	O
may	O
be	O
exploited	O
in	O
development	O
,	O
but	O
operation	O
is	O
assumed	O
without	O
human	O
intervention	O
.	O
sec	O
.	O
1.4	O
]	O
1.3.3	O
neural	O
networks	O
perspectives	O
on	O
classiﬁcation	B
3	O
the	O
ﬁeld	O
of	O
neural	O
networks	O
has	O
arisen	O
from	O
diverse	O
sources	O
,	O
ranging	O
from	O
the	O
fascination	O
of	O
mankind	O
with	O
understanding	O
and	O
emulating	O
the	O
human	O
brain	O
,	O
to	O
broader	O
issues	O
of	O
copying	O
human	O
abilities	O
such	O
as	O
speech	O
and	O
the	O
use	O
of	O
language	O
,	O
to	O
the	O
practical	O
commercial	O
,	O
scientiﬁc	O
,	O
and	O
engineering	O
disciplines	O
of	O
pattern	O
recognition	O
,	O
modelling	O
,	O
and	O
prediction	O
.	O
the	O
pursuit	O
of	O
technology	O
is	O
a	O
strong	O
driving	O
force	O
for	O
researchers	O
,	O
both	O
in	O
academia	O
and	O
industry	O
,	O
in	O
many	O
ﬁelds	O
of	O
science	O
and	O
engineering	O
.	O
in	O
neural	O
networks	O
,	O
as	O
in	O
machine	O
learning	O
,	O
the	O
excitement	O
of	O
technological	O
progress	O
is	O
supplemented	O
by	O
the	O
challenge	O
of	O
reproducing	O
intelligence	O
itself	O
.	O
a	O
broad	O
class	O
of	O
techniques	O
can	O
come	O
under	O
this	O
heading	O
,	O
but	O
,	O
generally	O
,	O
neural	O
networks	O
consist	O
of	O
layers	O
of	O
interconnected	O
nodes	O
,	O
each	O
node	O
producing	O
a	O
non-linear	O
function	O
of	O
its	O
input	B
.	O
the	O
input	B
to	O
a	O
node	O
may	O
come	O
from	O
other	O
nodes	O
or	O
directly	O
from	O
the	O
input	B
data	O
.	O
also	O
,	O
some	O
nodes	O
are	O
identiﬁed	O
with	O
the	O
output	B
of	O
the	O
network	O
.	O
the	O
complete	O
network	O
therefore	O
represents	O
a	O
very	O
complex	O
set	O
of	O
interdependencies	O
which	O
may	O
incorporate	O
any	O
degree	O
of	O
nonlinearity	O
,	O
allowing	O
very	O
general	O
functions	O
to	O
be	O
modelled	O
.	O
in	O
the	O
simplest	O
networks	O
,	O
the	O
output	B
from	O
one	O
node	O
is	O
fed	O
into	O
another	O
node	O
in	O
such	O
a	O
way	O
as	O
to	O
propagate	O
“	O
messages	O
”	O
through	O
layers	O
of	O
interconnecting	O
nodes	O
.	O
more	O
complex	O
behaviour	O
may	O
be	O
modelled	O
by	O
networks	O
in	O
which	O
the	O
ﬁnal	O
output	B
nodes	O
are	O
connected	O
with	O
earlier	O
nodes	O
,	O
and	O
then	O
the	O
system	O
has	O
the	O
characteristics	O
of	O
a	O
highly	O
nonlinear	O
system	O
with	O
feedback	O
.	O
it	O
has	O
been	O
argued	O
that	O
neural	O
networks	O
mirror	O
to	O
a	O
certain	O
extent	O
the	O
behaviour	O
of	O
networks	O
of	O
neurons	O
in	O
the	O
brain	O
.	O
neural	O
network	O
approaches	O
combine	O
the	O
complexity	O
of	O
some	O
of	O
the	O
statistical	B
techniques	O
with	O
the	O
machine	O
learning	O
objective	O
of	O
imitating	O
human	O
intelligence	O
:	O
however	O
,	O
this	O
is	O
done	O
at	O
a	O
more	O
“	O
unconscious	O
”	O
level	O
and	O
hence	O
there	O
is	O
no	O
accompanying	O
ability	O
to	O
make	O
learned	O
concepts	O
transparent	O
to	O
the	O
user	O
.	O
1.3.4	O
conclusions	O
the	O
three	O
broad	O
approachesoutlined	O
above	O
form	O
the	O
basis	O
of	O
the	O
grouping	O
of	O
procedures	O
used	O
in	O
this	O
book	O
.	O
the	O
correspondence	O
between	O
type	O
of	O
technique	O
and	O
professional	O
background	O
is	O
inexact	O
:	O
for	O
example	B
,	O
techniques	O
that	O
use	O
decision	O
trees	O
have	O
been	O
developed	O
in	O
parallel	O
both	O
within	O
the	O
machine	O
learning	O
community	O
,	O
motivated	O
by	O
psychological	O
research	O
or	O
knowledge	O
acquisition	O
for	O
expert	O
systems	O
,	O
and	O
within	O
the	O
statistical	B
profession	O
as	O
a	O
response	O
to	O
the	O
perceived	O
limitations	O
of	O
classical	O
discrimination	O
techniques	O
based	O
on	O
linear	O
functions	O
.	O
similarly	O
strong	O
parallels	O
may	O
be	O
drawn	O
between	O
advanced	O
regression	O
techniques	O
developed	O
in	O
statistics	O
,	O
and	O
neural	O
network	O
models	O
with	O
a	O
background	O
in	O
psychology	O
,	O
computer	O
science	O
and	O
artiﬁcial	O
intelligence	O
.	O
it	O
is	O
the	O
aim	O
of	O
this	O
book	O
to	O
put	O
all	O
methods	O
to	O
the	O
test	O
of	O
experiment	O
,	O
and	O
to	O
give	O
an	O
objective	O
assessment	O
of	O
their	O
strengths	O
and	O
weaknesses	O
.	O
techniques	O
have	O
been	O
grouped	O
according	O
to	O
the	O
above	O
categories	O
.	O
it	O
is	O
not	O
always	O
straightforward	O
to	O
select	O
a	O
group	O
:	O
for	O
example	B
some	O
procedures	O
can	O
be	O
considered	O
as	O
a	O
development	O
from	O
linear	O
regression	O
,	O
but	O
have	O
strong	O
afﬁnity	O
to	O
neural	O
networks	O
.	O
when	O
deciding	O
on	O
a	O
group	O
for	O
a	O
speciﬁc	O
technique	O
,	O
we	O
have	O
attempted	O
to	O
ignore	O
its	O
professional	O
pedigree	O
and	O
classify	O
according	O
to	O
its	O
essential	O
nature	O
.	O
4	O
introduction	O
[	O
ch	O
.	O
1	O
1.4	O
the	O
statlog	O
project	O
the	O
fragmentation	O
amongst	O
different	O
disciplines	O
has	O
almost	O
certainly	O
hindered	O
communi-	O
cation	O
and	O
progress	O
.	O
the	O
statlog	O
project	O
was	O
designed	O
to	O
break	O
down	O
these	O
divisions	O
by	O
selecting	O
classiﬁcation	B
procedures	O
regardless	O
of	O
historical	O
pedigree	O
,	O
testing	O
them	O
on	O
large-scale	O
and	O
commercially	O
important	O
problems	O
,	O
and	O
hence	O
to	O
determine	O
to	O
what	O
ex-	O
tent	O
the	O
various	O
techniques	O
met	O
the	O
needs	O
of	O
industry	O
.	O
this	O
depends	O
critically	O
on	O
a	O
clear	O
understanding	O
of	O
:	O
1	O
.	O
2	O
.	O
3.	O
measures	O
of	O
performance	O
or	O
benchmarks	O
to	O
monitor	O
the	O
success	O
of	O
the	O
method	O
in	O
a	O
the	O
aims	O
of	O
each	O
classiﬁcation/decision	O
procedure	O
;	O
the	O
class	O
of	O
problems	O
for	O
which	O
it	O
is	O
most	O
suited	O
;	O
particular	O
application	O
.	O
about	O
20	O
procedures	O
were	O
considered	O
for	O
about	O
20	O
datasets	O
,	O
so	O
that	O
results	O
were	O
obtained	O
from	O
around	O
20	O
20	O
=	O
400	O
large	O
scale	O
experiments	O
.	O
the	O
set	O
of	O
methods	O
to	O
be	O
considered	O
was	O
pruned	O
after	O
early	O
experiments	O
,	O
using	O
criteria	O
developed	O
for	O
multi-input	O
(	O
problems	O
)	O
,	O
many	O
treatments	O
(	O
algorithms	O
)	O
and	O
multiple	O
criteria	O
experiments	O
.	O
a	O
management	O
hierarchy	O
led	O
by	O
daimler-benz	O
controlled	O
the	O
full	O
project	O
.	O
the	O
objectives	B
of	O
the	O
project	O
were	O
threefold	O
:	O
to	O
provide	O
critical	O
performance	O
measurements	O
on	O
available	O
classiﬁcation	B
procedures	O
;	O
to	O
indicate	O
the	O
nature	O
and	O
scope	O
of	O
further	O
development	O
which	O
particular	O
methods	O
require	O
to	O
meet	O
the	O
expectations	O
of	O
industrial	O
users	O
;	O
to	O
indicate	O
the	O
most	O
promising	O
avenues	O
of	O
development	O
for	O
the	O
commercially	O
immature	O
approaches	O
.	O
1	O
.	O
2	O
.	O
3	O
.	O
1.4.1	O
quality	O
control	O
the	O
project	O
laid	O
down	O
strict	O
guidelines	O
for	O
the	O
testing	O
procedure	O
.	O
first	O
an	O
agreed	O
data	O
format	O
was	O
established	O
,	O
algorithms	O
were	O
“	O
deposited	O
”	O
at	O
one	O
site	O
,	O
with	O
appropriate	O
instructions	O
;	O
this	O
version	O
would	O
be	O
used	O
in	O
the	O
case	O
of	O
any	O
future	O
dispute	O
.	O
each	O
dataset	O
was	O
then	O
divided	O
into	O
a	O
training	O
set	O
and	O
a	O
testing	O
set	O
,	O
and	O
any	O
parameters	O
in	O
an	O
algorithm	O
could	O
be	O
“	O
tuned	O
”	O
or	O
estimated	O
only	O
by	O
reference	O
to	O
the	O
training	O
set	O
.	O
once	O
a	O
rule	O
had	O
been	O
determined	O
,	O
it	O
was	O
then	O
applied	O
to	O
the	O
test	O
data	O
.	O
this	O
procedure	O
was	O
validated	O
at	O
another	O
site	O
by	O
another	O
(	O
more	O
na¨ıve	O
)	O
user	O
for	O
each	O
dataset	O
in	O
the	O
ﬁrst	O
phase	O
of	O
the	O
project	O
.	O
this	O
ensured	O
that	O
the	O
guidelines	O
for	O
parameter	O
selection	O
were	O
not	O
violated	O
,	O
and	O
also	O
gave	O
some	O
information	O
on	O
the	O
ease-of-use	O
for	O
a	O
non-expert	O
in	O
the	O
domain	O
.	O
unfortunately	O
,	O
these	O
guidelines	O
were	O
not	O
followed	O
for	O
the	O
radial	O
basis	O
function	O
(	O
rbf	O
)	O
algorithm	O
which	O
for	O
some	O
datasets	O
determined	O
the	O
number	O
of	O
centres	O
and	O
locations	O
with	O
reference	O
to	O
the	O
test	O
set	O
,	O
so	O
these	O
results	O
should	O
be	O
viewed	O
with	O
some	O
caution	O
.	O
however	O
,	O
it	O
is	O
thought	O
that	O
the	O
conclusions	O
will	O
be	O
unaffected	O
.	O
1.4.2	O
caution	O
in	O
the	O
interpretations	O
of	O
comparisons	O
there	O
are	O
some	O
strong	O
caveats	O
that	O
must	O
be	O
made	O
concerning	O
comparisons	O
between	O
tech-	O
niques	O
in	O
a	O
project	O
such	O
as	O
this	O
.	O
first	O
,	O
the	O
exercise	O
is	O
necessarily	O
somewhat	O
contrived	O
.	O
in	O
any	O
real	O
application	O
,	O
there	O
should	O
be	O
an	O
iterative	O
process	O
in	O
which	O
the	O
constructor	O
of	O
the	O
classiﬁer	B
interacts	O
with	O
the	O
	O
esprit	O
project	O
5170.	O
comparative	O
testing	O
and	O
evaluation	O
of	O
statistical	B
and	O
logical	O
learning	O
algorithms	O
on	O
large-scale	O
applications	O
to	O
classiﬁcation	B
,	O
prediction	O
and	O
control	O
sec	O
.	O
1.5	O
]	O
the	O
structure	O
of	O
this	O
volume	O
5	O
expert	O
in	O
the	O
domain	O
,	O
gaining	O
understanding	O
of	O
the	O
problem	O
and	O
any	O
limitations	O
in	O
the	O
data	O
,	O
and	O
receiving	O
feedback	O
as	O
to	O
the	O
quality	O
of	O
preliminary	O
investigations	O
.	O
in	O
contrast	O
,	O
statlog	O
datasets	O
were	O
simply	O
distributed	O
and	O
used	O
as	O
test	O
cases	O
for	O
a	O
wide	O
variety	O
of	O
techniques	O
,	O
each	O
applied	O
in	O
a	O
somewhat	O
automatic	O
fashion	O
.	O
second	O
,	O
the	O
results	O
obtained	O
by	O
applying	O
a	O
technique	O
to	O
a	O
test	O
problem	O
depend	O
on	O
three	O
factors	O
:	O
1	O
.	O
2	O
.	O
3.	O
the	O
essential	O
quality	O
and	O
appropriateness	O
of	O
the	O
technique	O
;	O
the	O
actual	O
implementation	O
of	O
the	O
technique	O
as	O
a	O
computer	O
program	O
;	O
the	O
skill	O
of	O
the	O
user	O
in	O
coaxing	O
the	O
best	O
out	O
of	O
the	O
technique	O
.	O
in	O
appendix	O
b	O
we	O
have	O
described	O
the	O
implementations	O
used	O
for	O
each	O
technique	O
,	O
and	O
the	O
availability	O
of	O
more	O
advanced	O
versions	O
if	O
appropriate	O
.	O
however	O
,	O
it	O
is	O
extremely	O
difﬁcult	O
to	O
control	O
adequately	O
the	O
variations	O
in	O
the	O
background	O
and	O
ability	O
of	O
all	O
the	O
experimenters	O
in	O
statlog	O
,	O
particularly	O
with	O
regard	O
to	O
data	O
analysis	O
and	O
facility	O
in	O
“	O
tuning	O
”	O
procedures	O
to	O
give	O
their	O
best	O
.	O
individual	O
techniques	O
may	O
,	O
therefore	O
,	O
have	O
suffered	O
from	O
poor	O
implementation	O
and	O
use	O
,	O
but	O
we	O
hope	O
that	O
there	O
is	O
no	O
overall	O
bias	O
against	O
whole	O
classes	O
of	O
procedure	O
.	O
1.5	O
the	O
structure	O
of	O
this	O
volume	O
the	O
present	O
text	O
has	O
been	O
produced	O
by	O
a	O
variety	O
of	O
authors	O
,	O
from	O
widely	O
differing	O
back-	O
grounds	O
,	O
but	O
with	O
the	O
common	O
aim	O
of	O
making	O
the	O
results	O
of	O
the	O
statlog	O
project	O
accessible	O
to	O
a	O
wide	O
range	O
of	O
workers	O
in	O
the	O
ﬁelds	O
of	O
machine	O
learning	O
,	O
statistics	O
and	O
neural	O
networks	O
,	O
and	O
to	O
help	O
the	O
cross-fertilisation	O
of	O
ideas	O
between	O
these	O
groups	O
.	O
after	O
discussing	O
the	O
general	O
classiﬁcation	B
problem	O
in	O
chapter	O
2	O
,	O
the	O
next	O
4	O
chapters	O
detail	O
the	O
methods	O
that	O
have	O
been	O
investigated	O
,	O
divided	O
up	O
according	O
to	O
broad	O
headings	O
of	O
classical	O
statistics	O
,	O
modern	O
statistical	B
techniques	O
,	O
decision	O
trees	O
and	O
rules	O
,	O
and	O
neural	O
networks	O
.	O
the	O
next	O
part	O
of	O
the	O
book	O
concerns	O
the	O
evaluation	O
experiments	O
,	O
and	O
includes	O
chapters	O
on	O
evaluation	O
criteria	O
,	O
a	O
survey	O
of	O
previous	O
comparative	O
studies	O
,	O
a	O
description	O
of	O
the	O
data-sets	O
and	O
the	O
results	O
for	O
the	O
different	O
methods	O
,	O
and	O
an	O
analysis	O
of	O
the	O
results	O
which	O
explores	O
the	O
characteristics	O
of	O
data-sets	O
that	O
make	O
them	O
suitable	O
for	O
particular	O
approaches	O
:	O
we	O
might	O
call	O
this	O
“	O
machine	O
learning	O
on	O
machine	O
learning	O
”	O
.	O
the	O
conclusions	O
concerning	O
the	O
experiments	O
are	O
summarised	O
in	O
chapter	O
11.	O
the	O
ﬁnal	O
chapters	O
of	O
the	O
book	O
broaden	O
the	O
interpretation	O
of	O
the	O
basic	O
classiﬁcation	B
problem	O
.	O
the	O
fundamental	O
theme	O
of	O
representing	O
knowledge	O
using	O
different	O
formalisms	O
is	O
discussed	O
with	O
relation	O
to	O
constructing	O
classiﬁcation	B
techniques	O
,	O
followed	O
by	O
a	O
summary	O
of	O
current	O
approaches	O
to	O
dynamic	O
control	O
now	O
arising	O
from	O
a	O
rephrasing	O
of	O
the	O
problem	O
in	O
terms	O
of	O
classiﬁcation	B
and	O
learning	O
.	O
2	O
classiﬁcation	B
r.	O
j.	O
henery	O
university	O
of	O
strathclyde	O
2.1	O
definition	O
of	O
classification	O
classiﬁcation	B
has	O
two	O
distinct	O
meanings	O
.	O
we	O
may	O
be	O
given	O
a	O
set	O
of	O
observations	O
with	O
the	O
aim	O
of	O
establishing	O
the	O
existence	O
of	O
classes	O
or	O
clusters	O
in	O
the	O
data	O
.	O
or	O
we	O
may	O
know	O
for	O
certain	O
that	O
there	O
are	O
so	O
many	O
classes	O
,	O
and	O
the	O
aim	O
is	O
to	O
establish	O
a	O
rule	O
whereby	O
we	O
can	O
classify	O
a	O
new	O
observation	O
into	O
one	O
of	O
the	O
existing	O
classes	O
.	O
the	O
former	O
type	O
is	O
known	O
as	O
unsupervised	O
learning	O
(	O
or	O
clustering	O
)	O
,	O
the	O
latter	O
as	O
supervised	O
learning	O
.	O
in	O
this	O
book	O
when	O
we	O
use	O
the	O
term	O
classiﬁcation	B
,	O
we	O
are	O
talking	O
of	O
supervised	O
learning	O
.	O
in	O
the	O
statistical	B
literature	O
,	O
supervised	O
learning	O
is	O
usually	O
,	O
but	O
not	O
always	O
,	O
referred	O
to	O
as	O
discrimination	O
,	O
by	O
which	O
is	O
meant	O
the	O
establishing	O
of	O
the	O
classiﬁcation	B
rule	O
from	O
given	O
correctly	O
classiﬁed	O
data	O
.	O
the	O
existence	O
of	O
correctly	O
classiﬁed	O
data	O
presupposes	O
that	O
someone	O
(	O
the	O
supervisor	O
)	O
is	O
able	O
to	O
classify	O
without	O
error	O
,	O
so	O
the	O
question	O
naturally	O
arises	O
:	O
why	O
is	O
it	O
necessary	O
to	O
replace	O
this	O
exact	O
classiﬁcation	B
by	O
some	O
approximation	O
?	O
2.1.1	O
rationale	O
there	O
are	O
many	O
reasons	O
why	O
we	O
may	O
wish	O
to	O
set	O
up	O
a	O
classiﬁcation	B
procedure	O
,	O
and	O
some	O
of	O
these	O
are	O
discussed	O
later	O
in	O
relation	O
to	O
the	O
actual	O
datasets	O
used	O
in	O
this	O
book	O
.	O
here	O
we	O
outline	O
possible	O
reasons	O
for	O
the	O
examples	O
in	O
section	O
1.2	O
.	O
1.	O
mechanical	O
classiﬁcation	B
procedures	O
may	O
be	O
much	O
faster	O
:	O
for	O
example	B
,	O
postal	O
code	O
reading	O
machines	O
may	O
be	O
able	O
to	O
sort	O
the	O
majority	O
of	O
letters	O
,	O
leaving	O
the	O
difﬁcult	O
cases	O
to	O
human	O
readers	O
.	O
2.	O
a	O
mail	O
order	O
ﬁrm	O
must	O
take	O
a	O
decision	O
on	O
the	O
granting	O
of	O
credit	O
purely	O
on	O
the	O
basis	O
of	O
information	O
supplied	O
in	O
the	O
application	O
form	O
:	O
human	O
operators	O
may	O
well	O
have	O
biases	O
,	O
i.e	O
.	O
may	O
make	O
decisions	O
on	O
irrelevant	O
information	O
and	O
may	O
turn	O
away	O
good	O
customers	O
.	O
address	O
for	O
correspondence	O
:	O
department	O
of	O
statistics	O
and	O
modelling	O
science	O
,	O
university	O
of	O
strathclyde	O
,	O
glasgow	O
g1	O
1xh	O
,	O
u.k.	O
sec	O
.	O
2.1	O
]	O
deﬁnition	O
7	O
3.	O
in	O
the	O
medical	O
ﬁeld	O
,	O
we	O
may	O
wish	O
to	O
avoid	O
the	O
surgery	O
that	O
would	O
be	O
the	O
only	O
sure	O
way	O
of	O
making	O
an	O
exact	O
diagnosis	O
,	O
so	O
we	O
ask	O
if	O
a	O
reliable	O
diagnosis	O
can	O
be	O
made	O
on	O
purely	O
external	O
symptoms	O
.	O
4.	O
the	O
supervisor	O
(	O
refered	O
to	O
above	O
)	O
may	O
be	O
the	O
verdict	O
of	O
history	O
,	O
as	O
in	O
meteorology	O
or	O
stock-exchange	O
transaction	O
or	O
investment	O
and	O
loan	O
decisions	O
.	O
in	O
this	O
case	O
the	O
issue	O
is	O
one	O
of	O
forecasting	O
.	O
2.1.2	O
issues	O
there	O
are	O
also	O
many	O
issues	O
of	O
concern	O
to	O
the	O
would-be	O
classiﬁer	B
.	O
we	O
list	O
below	O
a	O
few	O
of	O
these	O
.	O
accuracy	O
.	O
there	O
is	O
the	O
reliability	O
of	O
the	O
rule	O
,	O
usually	O
represented	O
by	O
the	O
proportion	O
of	O
correct	O
classiﬁcations	O
,	O
although	O
it	O
may	O
be	O
that	O
some	O
errors	O
are	O
more	O
serious	O
than	O
others	O
,	O
and	O
it	O
may	O
be	O
important	O
to	O
control	O
the	O
error	O
rate	O
for	O
some	O
key	O
class	O
.	O
speed	O
.	O
in	O
some	O
circumstances	O
,	O
the	O
speed	O
of	O
the	O
classiﬁer	B
is	O
a	O
major	O
issue	O
.	O
a	O
classiﬁer	B
that	O
is	O
90	O
%	O
accurate	O
may	O
be	O
preferred	O
over	O
one	O
that	O
is	O
95	O
%	O
accurate	O
if	O
it	O
is	O
100	O
times	O
faster	O
in	O
testing	O
(	O
and	O
such	O
differences	O
in	O
time-scales	O
are	O
not	O
uncommon	O
in	O
neural	O
networks	O
for	O
example	B
)	O
.	O
such	O
considerations	O
would	O
be	O
important	O
for	O
the	O
automatic	O
reading	O
of	O
postal	O
codes	O
,	O
or	O
automatic	O
fault	O
detection	O
of	O
items	O
on	O
a	O
production	O
line	O
for	O
example	B
.	O
comprehensibility	O
.	O
if	O
it	O
is	O
a	O
human	O
operator	O
that	O
must	O
apply	O
the	O
classiﬁcation	B
proce-	O
dure	O
,	O
the	O
procedure	O
must	O
be	O
easily	O
understood	O
else	O
mistakes	O
will	O
be	O
made	O
in	O
applying	O
the	O
rule	O
.	O
it	O
is	O
important	O
also	O
,	O
that	O
human	O
operators	O
believe	O
the	O
system	O
.	O
an	O
oft-quoted	O
example	B
is	O
the	O
three-mile	O
island	O
case	O
,	O
where	O
the	O
automatic	O
devices	O
correctly	O
rec-	O
ommended	O
a	O
shutdown	O
,	O
but	O
this	O
recommendation	O
was	O
not	O
acted	O
upon	O
by	O
the	O
human	O
operators	O
who	O
did	O
not	O
believe	O
that	O
the	O
recommendation	O
was	O
well	O
founded	O
.	O
a	O
similar	O
story	O
applies	O
to	O
the	O
chernobyl	O
disaster	O
.	O
time	O
to	O
learn	O
.	O
especially	O
in	O
a	O
rapidly	O
changing	O
environment	O
,	O
it	O
may	O
be	O
necessary	O
to	O
learn	O
a	O
classiﬁcation	B
rule	O
quickly	O
,	O
or	O
make	O
adjustments	O
to	O
an	O
existing	O
rule	O
in	O
real	O
time	O
.	O
“	O
quickly	O
”	O
might	O
imply	O
also	O
that	O
we	O
need	O
only	O
a	O
small	O
number	O
of	O
observations	O
to	O
establish	O
our	O
rule	O
.	O
at	O
one	O
extreme	O
,	O
consider	O
the	O
na¨ıve	O
1-nearest	O
neighbour	O
rule	O
,	O
in	O
which	O
the	O
training	O
set	O
is	O
searched	O
for	O
the	O
‘	O
nearest	O
’	O
(	O
in	O
a	O
deﬁned	O
sense	O
)	O
previous	O
example	B
,	O
whose	O
class	O
is	O
then	O
assumed	O
for	O
the	O
new	O
case	O
.	O
this	O
is	O
very	O
fast	O
to	O
learn	O
(	O
no	O
time	O
at	O
all	O
!	O
)	O
,	O
but	O
is	O
very	O
slow	O
in	O
practice	O
if	O
all	O
the	O
data	O
are	O
used	O
(	O
although	O
if	O
you	O
have	O
a	O
massively	O
parallel	O
computer	O
you	O
might	O
speed	O
up	O
the	O
method	O
considerably	O
)	O
.	O
at	O
the	O
other	O
extreme	O
,	O
there	O
are	O
cases	O
where	O
it	O
is	O
very	O
useful	O
to	O
have	O
a	O
quick-and-dirty	O
method	O
,	O
possibly	O
for	O
eyeball	O
checking	O
of	O
data	O
,	O
or	O
for	O
providing	O
a	O
quick	O
cross-checking	O
on	O
the	O
results	O
of	O
another	O
procedure	O
.	O
for	O
example	B
,	O
a	O
bank	O
manager	O
might	O
know	O
that	O
the	O
simple	O
rule-of-thumb	O
“	O
only	O
give	O
credit	O
to	O
applicants	O
who	O
already	O
have	O
a	O
bank	O
account	O
”	O
is	O
a	O
fairly	O
reliable	O
rule	O
.	O
if	O
she	O
notices	O
that	O
the	O
new	O
assistant	O
(	O
or	O
the	O
new	O
automated	O
procedure	O
)	O
is	O
mostly	O
giving	O
credit	O
to	O
customers	O
who	O
do	O
not	O
have	O
a	O
bank	O
account	O
,	O
she	O
would	O
probably	O
wish	O
to	O
check	O
that	O
the	O
new	O
assistant	O
(	O
or	O
new	O
procedure	O
)	O
was	O
operating	O
correctly	O
.	O
8	O
classiﬁcation	B
[	O
ch	O
.	O
2	O
2.1.3	O
class	O
deﬁnitions	O
an	O
important	O
question	O
,	O
that	O
is	O
improperly	O
understood	O
in	O
many	O
studies	O
of	O
classiﬁcation	B
,	O
is	O
the	O
nature	O
of	O
the	O
classes	O
and	O
the	O
way	O
that	O
they	O
are	O
deﬁned	O
.	O
we	O
can	O
distinguish	O
three	O
common	O
cases	O
,	O
only	O
the	O
ﬁrst	O
leading	O
to	O
what	O
statisticians	O
would	O
term	O
classiﬁcation	B
:	O
1.	O
classes	O
correspond	O
to	O
labels	O
for	O
different	O
populations	O
:	O
membership	O
of	O
the	O
various	O
populations	O
is	O
not	O
in	O
question	O
.	O
for	O
example	B
,	O
dogs	O
and	O
cats	O
form	O
quite	O
separate	O
classes	O
or	O
populations	O
,	O
and	O
it	O
is	O
known	O
,	O
with	O
certainty	O
,	O
whether	O
an	O
animal	O
is	O
a	O
dog	O
or	O
a	O
cat	O
(	O
or	O
neither	O
)	O
.	O
membership	O
of	O
a	O
class	O
or	O
population	O
is	O
determined	O
by	O
an	O
independent	O
authority	O
(	O
the	O
supervisor	O
)	O
,	O
the	O
allocation	O
to	O
a	O
class	O
being	O
determined	O
independently	O
of	O
any	O
particular	O
attributes	O
or	O
variables	O
.	O
2.	O
classes	O
result	O
from	O
a	O
prediction	O
problem	O
.	O
here	O
class	O
is	O
essentially	O
an	O
outcome	O
that	O
must	O
be	O
predicted	O
from	O
a	O
knowledge	O
of	O
the	O
attributes	O
.	O
in	O
statistical	B
terms	O
,	O
the	O
class	O
is	O
a	O
random	O
variable	O
.	O
a	O
typical	O
example	B
is	O
in	O
the	O
prediction	O
of	O
interest	O
rates	O
.	O
frequently	O
the	O
question	O
is	O
put	O
:	O
will	O
interest	O
rates	O
rise	O
(	O
class=1	O
)	O
or	O
not	O
(	O
class=0	O
)	O
.	O
3.	O
classes	O
are	O
pre-deﬁned	O
by	O
a	O
partition	O
of	O
the	O
sample	O
space	O
,	O
i.e	O
.	O
of	O
the	O
attributes	O
themselves	O
.	O
we	O
may	O
say	O
that	O
class	O
is	O
a	O
function	O
of	O
the	O
attributes	O
.	O
thus	O
a	O
manufactured	O
item	O
may	O
be	O
classed	O
as	O
faulty	O
if	O
some	O
attributes	O
are	O
outside	O
predetermined	O
limits	O
,	O
and	O
not	O
faulty	O
otherwise	O
.	O
there	O
is	O
a	O
rule	O
that	O
has	O
already	O
classiﬁed	O
the	O
data	O
from	O
the	O
attributes	O
:	O
the	O
problem	O
is	O
to	O
create	O
a	O
rule	O
that	O
mimics	O
the	O
actual	O
rule	O
as	O
closely	O
as	O
possible	O
.	O
many	O
credit	O
datasets	O
are	O
of	O
this	O
type	O
.	O
in	O
practice	O
,	O
datasets	O
may	O
be	O
mixtures	O
of	O
these	O
types	O
,	O
or	O
may	O
be	O
somewhere	O
in	O
between	O
.	O
2.1.4	O
accuracy	O
on	O
the	O
question	O
of	O
accuracy	O
,	O
we	O
should	O
always	O
bear	O
in	O
mind	O
that	O
accuracy	O
as	O
measured	O
on	O
the	O
training	O
set	O
and	O
accuracy	O
as	O
measured	O
on	O
unseen	O
data	O
(	O
the	O
test	O
set	O
)	O
are	O
often	O
very	O
different	O
.	O
indeed	O
it	O
is	O
not	O
uncommon	O
,	O
especially	O
in	O
machine	O
learning	O
applications	O
,	O
for	O
the	O
training	O
set	O
to	O
be	O
perfectly	O
ﬁtted	O
,	O
but	O
performance	O
on	O
the	O
test	O
set	O
to	O
be	O
very	O
disappointing	O
.	O
usually	O
,	O
it	O
is	O
the	O
accuracy	O
on	O
the	O
unseen	O
data	O
,	O
when	O
the	O
true	O
classiﬁcation	B
is	O
unknown	O
,	O
that	O
is	O
of	O
practical	O
importance	O
.	O
the	O
generally	O
accepted	O
method	O
for	O
estimating	O
this	O
is	O
to	O
use	O
the	O
given	O
data	O
,	O
in	O
which	O
we	O
assume	O
that	O
all	O
class	O
memberships	O
are	O
known	O
,	O
as	O
follows	O
.	O
firstly	O
,	O
we	O
use	O
a	O
substantial	O
proportion	O
(	O
the	O
training	O
set	O
)	O
of	O
the	O
given	O
data	O
to	O
train	O
the	O
procedure	O
.	O
this	O
rule	O
is	O
then	O
tested	O
on	O
the	O
remaining	O
data	O
(	O
the	O
test	O
set	O
)	O
,	O
and	O
the	O
results	O
compared	O
with	O
the	O
known	O
classiﬁcations	O
.	O
the	O
proportion	O
correct	O
in	O
the	O
test	O
set	O
is	O
an	O
unbiased	O
estimate	O
of	O
the	O
accuracy	O
of	O
the	O
rule	O
provided	O
that	O
the	O
training	O
set	O
is	O
randomly	O
sampled	O
from	O
the	O
given	O
data	O
.	O
2.2	O
examples	O
of	O
classifiers	O
to	O
illustrate	O
the	O
basic	O
types	O
of	O
classiﬁers	O
,	O
we	O
will	O
use	O
the	O
well-known	O
iris	O
dataset	O
,	O
which	O
is	O
given	O
,	O
in	O
full	O
,	O
in	O
kendall	O
&	O
stuart	O
(	O
1983	O
)	O
.	O
there	O
are	O
three	O
varieties	O
of	O
iris	O
:	O
setosa	O
,	O
versicolor	O
and	O
virginica	O
.	O
the	O
length	O
and	O
breadth	O
of	O
both	O
petal	O
and	O
sepal	O
were	O
measured	O
on	O
50	O
ﬂowers	O
of	O
each	O
variety	O
.	O
the	O
original	O
problem	O
is	O
to	O
classify	O
a	O
new	O
iris	O
ﬂower	O
into	O
one	O
of	O
these	O
three	O
types	O
on	O
the	O
basis	O
of	O
the	O
four	O
attributes	O
(	O
petal	O
and	O
sepal	O
length	O
and	O
width	O
)	O
.	O
to	O
keep	O
this	O
example	B
simple	O
,	O
however	O
,	O
we	O
will	O
look	O
for	O
a	O
classiﬁcation	B
rule	O
by	O
which	O
the	O
varieties	O
can	O
be	O
distinguished	O
purely	O
on	O
the	O
basis	O
of	O
the	O
two	O
measurements	O
on	O
petal	O
length	O
sec	O
.	O
2.2	O
]	O
examples	O
of	O
classiﬁers	O
9	O
and	O
width	O
.	O
we	O
have	O
available	O
ﬁfty	O
pairs	O
of	O
measurements	O
of	O
each	O
variety	O
from	O
which	O
to	O
learn	O
the	O
classiﬁcation	B
rule	O
.	O
2.2.1	O
fisher	O
’	O
s	O
linear	O
discriminants	O
this	O
is	O
one	O
of	O
the	O
oldest	O
classiﬁcation	B
procedures	O
,	O
and	O
is	O
the	O
most	O
commonly	O
implemented	O
in	O
computer	O
packages	O
.	O
the	O
idea	O
is	O
to	O
divide	O
sample	O
space	O
by	O
a	O
series	O
of	O
lines	O
in	O
two	O
dimensions	O
,	O
planes	O
in	O
3-d	O
and	O
,	O
generally	O
hyperplanes	O
in	O
many	O
dimensions	O
.	O
the	O
line	O
dividing	O
two	O
classes	O
is	O
drawn	O
to	O
bisect	O
the	O
line	O
joining	O
the	O
centres	O
of	O
those	O
classes	O
,	O
the	O
direction	O
of	O
the	O
line	O
is	O
determined	O
by	O
the	O
shape	O
of	O
the	O
clusters	O
of	O
points	O
.	O
for	O
example	B
,	O
to	O
differentiate	O
between	O
versicolor	O
and	O
virginica	O
,	O
the	O
following	O
rule	O
is	O
applied	O
:	O
fisher	O
’	O
s	O
linear	O
discriminants	O
applied	O
to	O
the	O
iris	O
data	O
are	O
shown	O
in	O
figure	O
2.1.	O
six	O
of	O
the	O
observations	O
would	O
be	O
misclassiﬁed	O
.	O
if	O
petal	O
width	O
if	O
petal	O
width	O
	O
0	O
3	O
.	O
  	O
  	O
	O
	O
	O
	O
petal	O
length	O
,	O
then	O
versicolor	O
.	O
petal	O
length	O
,	O
then	O
virginica	O
.	O
virginica	O
5	O
.	O
2	O
0	O
.	O
2	O
t	O
h	O
d	O
w	O
i	O
l	O
t	O
a	O
e	O
p	O
5	O
.	O
1	O
0	O
.	O
1	O
5	O
.	O
0	O
0	O
.	O
0	O
setosa	O
s	O
s	O
s	O
s	O
s	O
ss	O
s	O
s	O
ss	O
ss	O
s	O
s	O
ssss	O
ss	O
ss	O
s	O
s	O
s	O
ss	O
ss	O
s	O
s	O
s	O
ss	O
ss	O
sss	O
ss	O
s	O
s	O
s	O
s	O
s	O
s	O
s	O
s	O
a	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
ee	O
e	O
e	O
e	O
e	O
ee	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
e	O
a	O
a	O
a	O
a	O
a	O
e	O
aa	O
a	O
e	O
e	O
e	O
a	O
e	O
versicolor	O
0	O
2	O
4	O
petal	O
length	O
6	O
8	O
fig	O
.	O
2.1	O
:	O
classiﬁcation	B
by	O
linear	O
discriminants	O
:	O
iris	O
data	O
.	O
2.2.2	O
decision	O
tree	O
and	O
rule-based	O
methods	O
one	O
class	O
of	O
classiﬁcation	B
procedures	O
is	O
based	O
on	O
recursive	O
partitioning	O
of	O
the	O
sample	O
space	O
.	O
space	O
is	O
divided	O
into	O
boxes	O
,	O
and	O
at	O
each	O
stage	O
in	O
the	O
procedure	O
,	O
each	O
box	O
is	O
examined	O
to	O
see	O
if	O
it	O
may	O
be	O
split	O
into	O
two	O
boxes	O
,	O
the	O
split	O
usually	O
being	O
parallel	O
to	O
the	O
coordinate	O
axes	O
.	O
an	O
example	B
for	O
the	O
iris	O
data	O
follows	O
.	O
if	O
petal	O
length	O
2.65	O
then	O
setosa	O
.	O
if	O
petal	O
length	O
4.95	O
then	O
virginica	O
.	O
	O
	O
10	O
classiﬁcation	B
[	O
ch	O
.	O
2	O
if	O
2.65	O
petal	O
length	O
4.95	O
then	O
:	O
if	O
petal	O
width	O
1.65	O
then	O
versicolor	O
;	O
if	O
petal	O
width	O
1.65	O
then	O
virginica	O
.	O
0	O
3	O
.	O
the	O
resulting	O
partition	O
is	O
shown	O
in	O
figure	O
2.2.	O
note	O
that	O
this	O
classiﬁcation	B
rule	O
has	O
three	O
mis-classiﬁcations	O
.	O
5	O
2	O
.	O
0	O
2	O
.	O
h	O
t	O
i	O
d	O
w	O
5	O
1	O
.	O
l	O
a	O
t	O
e	O
p	O
0	O
1	O
.	O
5	O
.	O
0	O
0	O
.	O
0	O
setosa	O
s	O
s	O
s	O
s	O
s	O
ss	O
s	O
s	O
ss	O
ss	O
s	O
s	O
ssss	O
ss	O
ss	O
s	O
s	O
s	O
ss	O
ss	O
s	O
s	O
ss	O
s	O
ss	O
sss	O
ss	O
s	O
s	O
s	O
s	O
s	O
s	O
s	O
s	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
e	O
a	O
a	O
a	O
a	O
a	O
a	O
aa	O
e	O
e	O
e	O
e	O
a	O
e	O
virginica	O
virginica	O
a	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
ee	O
e	O
e	O
e	O
e	O
e	O
ee	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
versicolor	O
0	O
2	O
4	O
petal	O
length	O
6	O
8	O
fig	O
.	O
2.2	O
:	O
classiﬁcation	B
by	O
decision	O
tree	O
:	O
iris	O
data	O
.	O
otherwise	O
versicolor	O
.	O
weiss	O
&	O
kapouleas	O
(	O
1989	O
)	O
give	O
an	O
alternative	O
classiﬁcation	B
rule	O
for	O
the	O
iris	O
data	O
that	O
is	O
very	O
directly	O
related	O
to	O
figure	O
2.2.	O
their	O
rule	O
can	O
be	O
obtained	O
from	O
figure	O
2.2	O
by	O
continuing	O
the	O
dotted	O
line	O
to	O
the	O
left	O
,	O
and	O
can	O
be	O
stated	O
thus	O
:	O
notice	O
that	O
this	O
rule	O
,	O
while	O
equivalent	O
to	O
the	O
rule	O
illustrated	O
in	O
figure	O
2.2	O
,	O
is	O
stated	O
more	O
concisely	O
,	O
and	O
this	O
formulation	O
may	O
be	O
preferred	O
for	O
this	O
reason	O
.	O
notice	O
also	O
that	O
the	O
rule	O
is	O
if	O
petal	O
length	O
2.65	O
then	O
setosa	O
.	O
if	O
petal	O
length	O
4.95	O
or	O
petal	O
width	O
1.65	O
then	O
virginica	O
.	O
ambiguous	O
if	O
petal	O
length	O
2.65	O
and	O
petal	O
width	O
1.65.	O
the	O
quoted	O
rules	O
may	O
be	O
made	O
unambiguous	O
by	O
applying	O
them	O
in	O
the	O
given	O
order	O
,	O
and	O
they	O
are	O
then	O
just	O
a	O
re-statement	O
of	O
the	O
previous	O
decision	O
tree	O
.	O
the	O
rule	O
discussed	O
here	O
is	O
an	O
instance	O
of	O
a	O
rule-based	O
method	O
:	O
such	O
methods	O
have	O
very	O
close	O
links	O
with	O
decision	O
trees	O
.	O
2.2.3	O
k-nearest-neighbour	O
we	O
illustrate	O
this	O
technique	O
on	O
the	O
iris	O
data	O
.	O
suppose	O
a	O
new	O
iris	O
is	O
to	O
be	O
classiﬁed	O
.	O
the	O
idea	O
is	O
that	O
it	O
is	O
most	O
likely	O
to	O
be	O
near	O
to	O
observations	O
from	O
its	O
own	O
proper	O
population	O
.	O
so	O
we	O
look	O
at	O
the	O
ﬁve	O
(	O
say	O
)	O
nearest	O
observations	O
from	O
all	O
previously	O
recorded	O
irises	O
,	O
and	O
classify	O
sec	O
.	O
2.3	O
]	O
variable	O
selection	O
11	O
the	O
observation	O
according	O
to	O
the	O
most	O
frequent	O
class	O
among	O
its	O
neighbours	O
.	O
in	O
figure	O
2.3	O
,	O
.	O
the	O
apparent	O
elliptical	O
shape	O
is	O
due	O
to	O
the	O
differing	O
horizontal	O
and	O
vertical	O
scales	O
,	O
but	O
the	O
proper	O
scaling	O
of	O
the	O
observations	O
is	O
a	O
major	O
difﬁculty	O
of	O
this	O
method	O
.	O
,	O
and	O
the	O
nearest	O
observations	O
lie	O
within	O
the	O
circle	O
the	O
new	O
observation	O
is	O
marked	O
by	O
a	O
centred	O
on	O
the	O
this	O
is	O
illustrated	O
in	O
figure	O
2.3	O
,	O
where	O
an	O
observation	O
centred	O
at	O
would	O
be	O
classiﬁed	O
as	O
virginica	O
since	O
it	O
has	O
virginica	O
among	O
its	O
nearest	O
neighbours	O
.	O
5	O
2	O
.	O
0	O
3	O
.	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
e	O
a	O
a	O
a	O
a	O
a	O
a	O
aa	O
e	O
e	O
e	O
e	O
a	O
e	O
virginica	O
a	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
ee	O
e	O
e	O
e	O
e	O
e	O
ee	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
e	O
0	O
2	O
.	O
h	O
t	O
i	O
d	O
w	O
l	O
a	O
5	O
1	O
.	O
t	O
e	O
p	O
0	O
.	O
1	O
5	O
.	O
0	O
0	O
.	O
0	O
s	O
s	O
s	O
s	O
s	O
ss	O
s	O
s	O
ss	O
ss	O
s	O
s	O
ssss	O
ss	O
ss	O
s	O
s	O
s	O
ss	O
ss	O
s	O
s	O
ss	O
s	O
ss	O
sss	O
ss	O
s	O
s	O
s	O
s	O
s	O
s	O
s	O
s	O
0	O
2	O
4	O
petal	O
length	O
6	O
8	O
fig	O
.	O
2.3	O
:	O
classiﬁcation	B
by	O
5-nearest-neighbours	O
:	O
iris	O
data	O
.	O
2.3	O
choice	O
of	O
variables	O
as	O
we	O
have	O
just	O
pointed	O
out	O
in	O
relation	O
to	O
k-nearest	O
neighbour	O
,	O
it	O
may	O
be	O
necessary	O
to	O
reduce	O
the	O
weight	O
attached	O
to	O
some	O
variables	O
by	O
suitable	O
scaling	O
.	O
at	O
one	O
extreme	O
,	O
we	O
might	O
remove	O
some	O
variables	O
altogether	O
if	O
they	O
do	O
not	O
contribute	O
usefully	O
to	O
the	O
discrimination	O
,	O
although	O
this	O
is	O
not	O
always	O
easy	O
to	O
decide	O
.	O
there	O
are	O
established	O
procedures	O
(	O
for	O
example	B
,	O
forward	B
stepwise	O
selection	O
)	O
for	O
removing	O
unnecessary	O
variables	O
in	O
linear	O
discriminants	O
,	O
but	O
,	O
for	O
large	O
datasets	O
,	O
the	O
performance	O
of	O
linear	O
discriminants	O
is	O
not	O
seriously	O
affected	O
by	O
including	O
such	O
unnecessary	O
variables	O
.	O
in	O
contrast	O
,	O
the	O
presence	O
of	O
irrelevant	O
variables	O
is	O
always	O
a	O
problem	O
with	O
k-nearest	O
neighbour	O
,	O
regardless	O
of	O
dataset	O
size	O
.	O
2.3.1	O
transformations	O
and	O
combinations	O
of	O
variables	O
often	O
problems	O
can	O
be	O
simpliﬁed	O
by	O
a	O
judicious	O
transformation	O
of	O
variables	O
.	O
with	O
statistical	B
procedures	O
,	O
the	O
aim	O
is	O
usually	O
to	O
transform	O
the	O
attributes	O
so	O
that	O
their	O
marginal	O
density	O
is	O
approximately	O
normal	O
,	O
usually	O
by	O
applying	O
a	O
monotonic	O
transformation	O
of	O
the	O
power	O
law	O
type	O
.	O
monotonic	O
transformations	O
do	O
not	O
affect	O
the	O
machine	O
learning	O
methods	O
,	O
but	O
they	O
can	O
beneﬁt	O
by	O
combining	O
variables	O
,	O
for	O
example	B
by	O
taking	O
ratios	O
or	O
differences	O
of	O
key	O
variables	O
.	O
background	O
knowledge	O
of	O
the	O
problem	O
is	O
of	O
help	O
in	O
determining	O
what	O
transformation	O
or	O
12	O
classiﬁcation	B
[	O
ch	O
.	O
2	O
combination	O
to	O
use	O
.	O
for	O
example	B
,	O
in	O
the	O
iris	O
data	O
,	O
the	O
product	O
of	O
the	O
variables	O
petal	O
length	O
and	O
petal	O
width	O
gives	O
a	O
single	O
attribute	O
which	O
has	O
the	O
dimensions	O
of	O
area	O
,	O
and	O
might	O
be	O
labelled	O
as	O
petal	O
area	O
.	O
it	O
so	O
happens	O
that	O
a	O
decision	O
rule	O
based	O
on	O
the	O
single	O
variable	O
petal	O
area	O
is	O
a	O
good	O
classiﬁer	B
with	O
only	O
four	O
errors	O
:	O
if	O
petal	O
area	O
2.0	O
then	O
setosa	O
.	O
if	O
2.0	O
petal	O
area	O
7.4	O
then	O
virginica	O
.	O
if	O
petal	O
area	O
7.4	O
then	O
virginica	O
.	O
this	O
tree	O
,	O
while	O
it	O
has	O
one	O
more	O
error	O
than	O
the	O
decision	O
tree	O
quoted	O
earlier	O
,	O
might	O
be	O
preferred	O
on	O
the	O
grounds	O
of	O
conceptual	O
simplicity	O
as	O
it	O
involves	O
only	O
one	O
“	O
concept	O
”	O
,	O
namely	O
petal	O
area	O
.	O
also	O
,	O
one	O
less	O
arbitrary	O
constant	O
need	O
be	O
remembered	O
(	O
i.e	O
.	O
there	O
is	O
one	O
less	O
node	O
or	O
cut-point	O
in	O
the	O
decision	O
trees	O
)	O
.	O
2.4	O
classification	O
of	O
classification	O
procedures	O
the	O
above	O
three	O
procedures	O
(	O
linear	O
discrimination	O
,	O
decision-tree	O
and	O
rule-based	O
,	O
k-nearest	O
neighbour	O
)	O
are	O
prototypes	O
for	O
three	O
types	O
of	O
classiﬁcation	B
procedure	O
.	O
not	O
surprisingly	O
,	O
they	O
have	O
been	O
reﬁned	O
and	O
extended	O
,	O
but	O
they	O
still	O
represent	O
the	O
major	O
strands	O
in	O
current	O
classiﬁcation	B
practice	O
and	O
research	O
.	O
the	O
23	O
procedures	O
investigated	O
in	O
this	O
book	O
can	O
be	O
directly	O
linked	O
to	O
one	O
or	O
other	O
of	O
the	O
above	O
.	O
however	O
,	O
within	O
this	O
book	O
the	O
methods	O
have	O
been	O
grouped	O
around	O
the	O
more	O
traditional	O
headings	O
of	O
classical	O
statistics	O
,	O
modern	O
statistical	B
techniques	O
,	O
machine	O
learning	O
and	O
neural	O
networks	O
.	O
chapters	O
3	O
–	O
6	O
,	O
respectively	O
,	O
are	O
devoted	O
to	O
each	O
of	O
these	O
.	O
for	O
some	O
methods	O
,	O
the	O
classiﬁcation	B
is	O
rather	O
abitrary	O
.	O
2.4.1	O
extensions	O
to	O
linear	O
discrimination	O
we	O
can	O
include	O
in	O
this	O
group	O
those	O
procedures	O
that	O
start	O
from	O
linear	O
combinations	O
of	O
the	O
measurements	O
,	O
even	O
if	O
these	O
combinations	O
are	O
subsequently	O
subjected	O
to	O
some	O
non-	O
linear	O
transformation	O
.	O
there	O
are	O
7	O
procedures	O
of	O
this	O
type	O
:	O
linear	O
discriminants	O
;	O
logistic	O
discriminants	O
;	O
quadratic	O
discriminants	O
;	O
multi-layer	O
perceptron	O
(	O
backprop	O
and	O
cascade	O
)	O
;	O
dipol92	O
;	O
and	O
projection	O
pursuit	O
.	O
note	O
that	O
this	O
group	O
consists	O
of	O
statistical	B
and	O
neural	O
network	O
(	O
speciﬁcally	O
multilayer	O
perceptron	O
)	O
methods	O
only	O
.	O
2.4.2	O
decision	O
trees	O
and	O
rule-based	O
methods	O
c4.5	O
;	O
cart	O
;	O
indcart	O
;	O
bayes	O
tree	O
;	O
and	O
itrule	O
(	O
see	O
chapter	O
5	O
)	O
.	O
this	O
is	O
the	O
most	O
numerous	O
group	O
in	O
the	O
book	O
with	O
9	O
procedures	O
:	O
newid	O
;	O
	O
;	O
cal5	O
;	O
cn2	O
;	O
2.4.3	O
density	O
estimates	O
this	O
group	O
is	O
a	O
little	O
less	O
homogeneous	O
,	O
but	O
the	O
7	O
members	O
have	O
this	O
in	O
common	O
:	O
the	O
procedure	O
is	O
intimately	O
linked	O
with	O
the	O
estimation	O
of	O
the	O
local	O
probability	O
density	O
at	O
each	O
point	O
in	O
sample	O
space	O
.	O
the	O
density	O
estimate	O
group	O
contains	O
:	O
k-nearest	O
neighbour	O
;	O
radial	O
basis	O
functions	O
;	O
naive	O
bayes	O
;	O
polytrees	O
;	O
kohonen	O
self-organising	O
net	O
;	O
lvq	O
;	O
and	O
the	O
kernel	O
density	O
method	O
.	O
this	O
group	O
also	O
contains	O
only	O
statistical	B
and	O
neural	O
net	O
methods	O
.	O
2.5	O
a	O
general	O
structure	O
for	O
classification	O
problems	O
there	O
are	O
three	O
essential	O
components	O
to	O
a	O
classiﬁcation	B
problem	O
.	O
1.	O
the	O
relative	O
frequency	O
with	O
which	O
the	O
classes	O
occur	O
in	O
the	O
population	O
of	O
interest	O
,	O
expressed	O
formally	O
as	O
the	O
prior	O
probability	O
distribution	O
.	O
sec	O
.	O
2.5	O
]	O
costs	O
of	O
misclassiﬁcation	O
13	O
2.	O
an	O
implicit	O
or	O
explicit	O
criterion	O
for	O
separating	O
the	O
classes	O
:	O
we	O
may	O
think	O
of	O
an	O
un-	O
derlying	O
input/output	O
relation	O
that	O
uses	O
observed	O
attributes	O
to	O
distinguish	O
a	O
random	O
individual	O
from	O
each	O
class	O
.	O
3.	O
the	O
cost	O
associated	O
with	O
making	O
a	O
wrong	O
classiﬁcation	B
.	O
most	O
techniques	O
implicitly	O
confound	O
components	O
and	O
,	O
for	O
example	B
,	O
produce	O
a	O
classiﬁ-	O
cation	O
rule	O
that	O
is	O
derived	O
conditional	O
on	O
a	O
particular	O
prior	O
distribution	O
and	O
can	O
not	O
easily	O
be	O
adapted	O
to	O
a	O
change	O
in	O
class	O
frequency	O
.	O
however	O
,	O
in	O
theory	O
each	O
of	O
these	O
components	O
may	O
be	O
individually	O
studied	O
and	O
then	O
the	O
results	O
formally	O
combined	O
into	O
a	O
classiﬁcation	B
rule	O
.	O
we	O
shall	O
describe	O
this	O
development	O
below	O
.	O
2.5.1	O
prior	O
probabilities	O
and	O
the	O
default	O
rule	O
 * + 	O
is	O
that	O
with	O
the	O
least	O
expected	O
cost	O
(	O
see	O
below	O
)	O
.	O
!	O
for	O
the	O
class	O
!	O
be	O
:	O
irrespective	O
of	O
the	O
attributes	O
of	O
the	O
example	B
.	O
this	O
no-data	O
or	O
default	O
rule	O
may	O
even	O
be	O
adopted	O
in	O
practice	O
if	O
the	O
cost	O
of	O
gathering	O
the	O
data	O
is	O
too	O
high	O
.	O
thus	O
,	O
banks	O
may	O
give	O
credit	O
to	O
all	O
their	O
established	O
customers	O
for	O
the	O
sake	O
of	O
good	O
customer	O
relations	O
:	O
here	O
the	O
cost	O
of	O
gathering	O
the	O
data	O
is	O
the	O
risk	O
of	O
losing	O
customers	O
.	O
the	O
default	O
rule	O
relies	O
only	O
on	O
knowledge	O
of	O
the	O
prior	O
probabilities	O
,	O
and	O
clearly	O
the	O
decision	O
rule	O
that	O
has	O
the	O
greatest	O
chance	O
of	O
success	O
is	O
to	O
allocate	O
every	O
new	O
observation	O
to	O
the	O
most	O
frequent	O
class	O
.	O
however	O
,	O
if	O
some	O
classiﬁcation	B
errors	O
are	O
more	O
serious	O
than	O
others	O
we	O
adopt	O
the	O
minimum	O
risk	O
(	O
least	O
we	O
need	O
to	O
introduce	O
some	O
notation	O
.	O
let	O
the	O
classes	O
be	O
denoted	O
!	O
#	O
''	O
%	O
$	O
'	O
&	O
)	O
(	O
''	O
''	O
-	O
,	O
,	O
and	O
let	O
the	O
prior	O
probability	O
.	O
.	O
!	O
/	O
&	O
10324	O
!	O
65	O
it	O
is	O
always	O
possible	O
to	O
use	O
the	O
no-data	O
rule	O
:	O
classify	O
any	O
new	O
observation	O
as	O
class7	O
,	O
expected	O
cost	O
)	O
rule	O
,	O
and	O
the	O
class8	O
suppose	O
we	O
are	O
able	O
to	O
observe	O
data	O
on	O
an	O
individual	O
,	O
and	O
that	O
we	O
know	O
the	O
probability	O
distribution	O
of	O
within	O
each	O
class9	O
!	O
=	O
<	O
	O
!	O
>	O
5	O
.	O
then	O
for	O
any	O
two	O
classes9	O
!	O
?	O
``	O
@	O
ba	O
to	O
be	O
:	O
;	O
2	O
=	O
<	O
likelihood	O
ratio	O
:	O
;	O
2	O
9	O
!	O
>	O
5	O
%	O
d	O
:	O
;	O
2	O
ea*5	O
provides	O
the	O
theoretical	O
optimal	O
form	O
for	O
discriminating	O
the	O
classes	O
on	O
the	O
basis	O
of	O
data	O
!	O
object	O
as	O
class	O
suppose	O
the	O
cost	O
of	O
misclassifying	O
a	O
class	O
isf*24	O
$	O
-	O
''	O
6gh5	O
.	O
decisions	O
should	O
all	O
new	O
observations	O
to	O
the	O
classi	O
,	O
using	O
sufﬁxj	O
as	O
label	O
for	O
the	O
decision	O
class	O
.	O
when	O
!	O
examples	O
decision	O
is	O
made	O
for	O
all	O
new	O
examples	O
,	O
a	O
cost	O
offk24	O
$	O
-	O
''	O
ljm5	O
is	O
incurred	O
for	O
class	O
i	O
of	O
making	O
decision	O
!	O
.	O
so	O
the	O
expected	O
cost	O
and	O
these	O
occur	O
with	O
probability	O
.	O
&	O
on	O
f*24	O
$	O
-	O
''	O
-jp5	O
the	O
bayes	O
minimum	O
cost	O
rule	O
chooses	O
that	O
class	O
that	O
has	O
the	O
lowest	O
expected	O
cost	O
.	O
to	O
see	O
the	O
relation	O
between	O
the	O
minimum	O
error	O
and	O
minimum	O
cost	O
rules	O
,	O
suppose	O
the	O
cost	O
of	O
.	O
the	O
majority	O
of	O
techniques	O
featured	O
in	O
this	O
book	O
can	O
be	O
thought	O
of	O
as	O
implicitly	O
or	O
explicitly	O
deriving	O
an	O
approximate	O
form	O
for	O
this	O
likelihood	O
ratio	O
.	O
be	O
based	O
on	O
the	O
principle	O
that	O
the	O
total	O
cost	O
of	O
misclassiﬁcations	O
should	O
be	O
minimised	O
:	O
for	O
a	O
new	O
observation	O
this	O
means	O
minimising	O
the	O
expected	O
cost	O
of	O
misclassiﬁcation	O
.	O
let	O
us	O
ﬁrst	O
consider	O
the	O
expected	O
cost	O
of	O
applying	O
the	O
default	O
decision	O
rule	O
:	O
allocate	O
the	O
is	O
:	O
2.5.2	O
separating	O
classes	O
c	O
<	O
2.5.3	O
misclassiﬁcation	O
costs	O
a	O
i	O
i	O
	O
i	O
!	O
.	O
!	O
14	O
classiﬁcation	B
[	O
ch	O
.	O
2	O
misclassiﬁcations	O
to	O
be	O
the	O
same	O
for	O
all	O
errors	O
and	O
zero	O
when	O
a	O
class	O
is	O
correctly	O
identiﬁed	O
,	O
for	O
$	O
y	O
&	O
zg	O
.	O
&	O
sfk2l	O
(	O
then	O
the	O
expected	O
cost	O
is	O
i.e	O
.	O
suppose	O
thatf*2q	O
$	O
l	O
''	O
6gh5r	O
&	O
sf	O
for	O
$	O
ut	O
f*24	O
$	O
-	O
''	O
-jp5	O
[	O
&	O
!	O
@	O
\	O
&	O
vg	O
andfk24	O
$	O
-	O
''	O
wg5x	O
&	O
f^	O
&	O
sf	O
!	O
6\	O
and	O
the	O
minimum	O
cost	O
rule	O
is	O
to	O
allocate	O
to	O
the	O
class	O
with	O
the	O
greatest	O
prior	O
probability	O
.	O
misclassiﬁcation	O
costs	O
are	O
very	O
difﬁcult	O
to	O
obtain	O
in	O
practice	O
.	O
even	O
in	O
situations	O
where	O
it	O
is	O
very	O
clear	O
that	O
there	O
are	O
very	O
great	O
inequalities	O
in	O
the	O
sizes	O
of	O
the	O
possible	O
penalties	O
or	O
rewards	O
for	O
making	O
the	O
wrong	O
or	O
right	O
decision	O
,	O
it	O
is	O
often	O
very	O
difﬁcult	O
to	O
quantify	O
them	O
.	O
typically	O
they	O
may	O
vary	O
from	O
individual	O
to	O
individual	O
,	O
as	O
in	O
the	O
case	O
of	O
applications	O
for	O
credit	O
of	O
varying	O
amounts	O
in	O
widely	O
differing	O
circumstances	O
.	O
in	O
one	O
dataset	O
we	O
have	O
assumed	O
the	O
misclassiﬁcation	O
costs	O
to	O
be	O
the	O
same	O
for	O
all	O
individuals	O
.	O
(	O
in	O
practice	O
,	O
credit-	O
granting	O
companies	O
must	O
assess	O
the	O
potential	O
costs	O
for	O
each	O
applicant	O
,	O
and	O
in	O
this	O
case	O
the	O
classiﬁcation	B
algorithm	O
usually	O
delivers	O
an	O
assessment	O
of	O
probabilities	O
,	O
and	O
the	O
decision	O
is	O
left	O
to	O
the	O
human	O
operator	O
.	O
)	O
.	O
if	O
we	O
wish	O
to	O
use	O
a	O
minimum	O
cost	O
rule	O
,	O
we	O
must	O
ﬁrst	O
calculate	O
the	O
expected	O
costs	O
of	O
the	O
we	O
can	O
now	O
see	O
how	O
the	O
three	O
components	O
introduced	O
above	O
may	O
be	O
combined	O
into	O
a	O
classiﬁcation	B
procedure	O
.	O
about	O
an	O
individual	O
,	O
the	O
situation	O
is	O
,	O
in	O
principle	O
,	O
unchanged	O
from	O
the	O
no-data	O
situation	O
.	O
the	O
difference	O
is	O
that	O
all	O
probabilities	O
must	O
now	O
.	O
again	O
,	O
the	O
decision	O
rule	O
with	O
least	O
probability	O
of	O
error	O
is	O
to	O
allocate	O
to	O
the	O
class	O
with	O
the	O
highest	O
probability	O
of	O
occurrence	O
,	O
but	O
now	O
the	O
2.6	O
bayes	O
rule	O
given	O
data	O
when	O
we	O
are	O
given	O
information	O
be	O
interpreted	O
as	O
conditional	O
on	O
the	O
data	O
!	O
given	O
the	O
data	O
:	O
5	O
of	O
class	O
relevant	O
probability	O
is	O
the	O
conditional	O
probability0_2q	O
prob	O
(	O
class9	O
!	O
given	O
0_2q9	O
!	O
5/	O
&	O
various	O
decisions	O
conditional	O
on	O
the	O
given	O
information	O
is	O
made	O
for	O
examples	O
with	O
attributes	O
now	O
,	O
when	O
decision	O
,	O
a	O
cost	O
off*2q	O
$	O
l	O
''	O
-jm5	O
!	O
examples	O
and	O
these	O
occur	O
with	O
probability0324	O
5	O
.	O
as	O
the	O
is	O
incurred	O
for	O
class	O
5	O
depend	O
on	O
probabilities0_2q	O
i	O
:	O
cost	O
5	O
of	O
making	O
decision	O
5	O
%	O
f*24	O
$	O
-	O
''	O
ljm5	O
5r	O
&	O
on	O
0_2q	O
when	O
bayes	O
theorem	O
is	O
used	O
to	O
calculate	O
the	O
conditional	O
probabilities03249	O
!	O
5	O
for	O
the	O
5	O
are	O
calculated	O
from	O
a	O
knowledge	O
of	O
the	O
prior	O
probabilities.	O
!	O
and	O
the	O
probabilities0324	O
!	O
c	O
<	O
	O
!	O
>	O
5	O
of	O
the	O
data	O
for	O
each	O
class	O
!	O
.	O
thus	O
,	O
for	O
class9	O
!	O
suppose	O
conditional	O
probabilities	O
:	O
`2	O
that	O
the	O
probability	O
of	O
observing	O
data	O
5	O
.	O
bayes	O
theorem	O
gives	O
the	O
posterior	O
!	O
as	O
:	O
5	O
for	O
class	O
probability0324	O
c	O
<	O
5	O
%	O
d	O
:	O
;	O
2	O
5/	O
&	O
a	O
.	O
0_2q	O
in	O
the	O
special	O
case	O
of	O
equal	O
misclassiﬁcation	O
costs	O
,	O
the	O
minimum	O
cost	O
rule	O
is	O
to	O
allocate	O
to	O
the	O
class	O
with	O
the	O
greatest	O
posterior	O
probability	O
.	O
classes	O
,	O
we	O
refer	O
to	O
them	O
as	O
the	O
posterior	O
probabilities	O
of	O
the	O
classes	O
.	O
then	O
the	O
posterior	O
,	O
so	O
too	O
will	O
the	O
decision	O
rule	O
.	O
so	O
too	O
will	O
the	O
expected	O
is	O
:	O
;	O
2	O
=	O
<	O
:	O
;	O
2	O
c	O
<	O
	O
	O
i	O
&	O
n	O
!	O
.	O
!	O
n	O
]	O
i	O
.	O
!	O
n	O
]	O
i	O
.	O
!	O
	O
.	O
i	O
5	O
!	O
<	O
	O
<	O
	O
5	O
i	O
!	O
<	O
	O
!	O
<	O
	O
i	O
2	O
	O
	O
i	O
2	O
	O
!	O
!	O
<	O
	O
<	O
	O
<	O
	O
	O
!	O
!	O
<	O
	O
!	O
<	O
	O
!	O
	O
!	O
n	O
a	O
.	O
a	O
	O
a	O
5	O
for	O
which	O
is	O
a	O
minimum	O
.	O
assuming	O
now	O
that	O
the	O
attributes	O
have	O
continuous	O
distributions	O
,	O
the	O
probabilities	O
above	O
if	O
sec	O
.	O
2.6	O
]	O
bayes	O
’	O
rule	O
15	O
is	O
proportional	O
the	O
divisor	O
is	O
common	O
to	O
all	O
classes	O
,	O
so	O
we	O
may	O
use	O
the	O
fact	O
that0_2q	O
=	O
<	O
i	O
with	O
minimum	O
expected	O
cost	O
(	O
minimum	O
risk	O
)	O
is	O
therefore	O
that	O
5	O
.	O
the	O
class	O
:	O
;	O
2	O
to	O
.	O
c	O
<	O
f*24	O
$	O
-	O
''	O
-jp5	O
%	O
:	O
;	O
2	O
!	O
have	O
become	O
probability	O
densities	O
.	O
suppose	O
that	O
observations	O
drawn	O
from	O
population	O
	O
;	O
<	O
probability	O
density	O
functionb	O
5	O
'	O
&	O
cb32	O
5	O
and	O
that	O
the	O
prior	O
probability	O
that	O
an	O
obser-	O
vation	O
belongs	O
to	O
class9	O
!	O
is.d	O
!	O
.	O
then	O
bayes	O
’	O
theorem	O
computes	O
the	O
probability	O
that	O
an	O
observation	O
belongs	O
to	O
class	O
!	O
as	O
5	O
@	O
d	O
5/	O
&	O
a	O
.	O
0_2q	O
a	O
classiﬁcation	B
rule	O
then	O
assigns	O
to	O
the	O
classi	O
with	O
maximal	O
a	O
posteriori	O
probability	O
given	O
:	O
5c	O
&	O
max	O
!	O
0_2q	O
0_2q	O
as	O
before	O
,	O
the	O
classi	O
with	O
minimum	O
expected	O
cost	O
(	O
minimum	O
risk	O
)	O
is	O
that	O
for	O
which	O
f*24	O
$	O
-	O
''	O
-jp5	O
%	O
b	O
a	O
.	O
then	O
!	O
and	O
consider	O
the	O
problem	O
of	O
discriminating	O
between	O
just	O
two	O
classes	O
	O
,	O
we	O
should	O
allocate	O
to	O
class	O
$	O
assuming	O
as	O
before	O
thatf*24	O
$	O
-	O
''	O
@	O
$	O
w5	O
'	O
&	O
ef*2fg	O
''	O
6gh5g	O
&	O
5'h	O
.	O
f*24	O
$	O
-	O
''	O
wg5	O
@	O
b	O
f*2ig	O
''	O
@	O
$	O
w5	O
@	O
b	O
f*2q	O
$	O
l	O
''	O
6g5	O
f*2fg	O
''	O
%	O
$	O
>	O
5	O
rather	O
than	O
deriving0_2q9	O
!	O
5	O
via	O
bayes	O
theorem	O
,	O
we	O
could	O
also	O
use	O
the	O
empirical	O
frequency	O
i	O
with	O
5	O
among	O
these	O
examples	O
.	O
the	O
minimum	O
error	O
rule	O
is	O
to	O
allocate	O
to	O
the	O
class	O
0324	O
which	O
shows	O
the	O
pivotal	O
role	O
of	O
the	O
likelihood	O
ratio	O
,	O
which	O
must	O
be	O
greater	O
than	O
the	O
ratio	O
of	O
prior	O
probabilities	O
times	O
the	O
relative	O
costs	O
of	O
the	O
errors	O
.	O
we	O
note	O
the	O
symmetry	O
in	O
the	O
above	O
expression	O
:	O
changes	O
in	O
costs	O
can	O
be	O
compensated	O
in	O
changes	O
in	O
prior	O
to	O
keep	O
constant	O
the	O
threshold	O
that	O
deﬁnes	O
the	O
classiﬁcation	B
rule	O
-	O
this	O
facility	O
is	O
exploited	O
in	O
some	O
techniques	O
,	O
although	O
for	O
more	O
than	O
two	O
groups	O
this	O
property	O
only	O
exists	O
under	O
restrictive	O
assumptions	O
(	O
see	O
breiman	O
et	O
al.	O
,	O
page	O
112	O
)	O
.	O
version	O
of	O
bayes	O
rule	O
,	O
which	O
,	O
in	O
practice	O
,	O
would	O
require	O
prohibitively	O
large	O
amounts	O
of	O
data	O
.	O
however	O
,	O
in	O
principle	O
,	O
the	O
procedure	O
is	O
to	O
gather	O
together	O
all	O
examples	O
in	O
the	O
training	O
set	O
that	O
have	O
the	O
same	O
attributes	O
(	O
exactly	O
)	O
as	O
the	O
given	O
example	B
,	O
and	O
to	O
ﬁnd	O
class	O
proportions	O
unless	O
the	O
number	O
of	O
attributes	O
is	O
very	O
small	O
and	O
the	O
training	O
dataset	O
very	O
large	O
,	O
it	O
will	O
be	O
necessary	O
to	O
use	O
approximations	O
to	O
estimate	O
the	O
posterior	O
class	O
probabilities	O
.	O
for	O
example	B
,	O
is	O
a	O
minimum	O
.	O
or	O
equivalently	O
2.6.1	O
bayes	O
rule	O
in	O
statistics	O
highest	O
posterior	O
probability	O
.	O
!	O
<	O
	O
5	O
!	O
	O
!	O
n	O
!	O
.	O
!	O
	O
!	O
5	O
!	O
2	O
	O
	O
!	O
!	O
<	O
	O
!	O
b	O
!	O
2	O
	O
n	O
a	O
.	O
a	O
b	O
a	O
2	O
	O
5	O
i	O
<	O
	O
!	O
<	O
	O
5	O
n	O
!	O
.	O
!	O
!	O
2	O
	O
5	O
.	O
a	O
a	O
2	O
	O
!	O
!	O
2	O
	O
5	O
b	O
!	O
2	O
	O
5	O
b	O
a	O
2	O
	O
5	O
.	O
a	O
.	O
!	O
<	O
	O
!	O
<	O
	O
16	O
classiﬁcation	B
[	O
ch	O
.	O
2	O
one	O
way	O
of	O
ﬁnding	O
an	O
approximate	O
bayes	O
rule	O
would	O
be	O
to	O
use	O
not	O
just	O
examples	O
with	O
attributes	O
matching	O
exactly	O
those	O
of	O
the	O
given	O
example	B
,	O
but	O
to	O
use	O
examples	O
that	O
were	O
near	O
the	O
given	O
example	B
in	O
some	O
sense	O
.	O
the	O
minimum	O
error	O
decision	O
rule	O
would	O
be	O
to	O
allocate	O
to	O
the	O
most	O
frequent	O
class	O
among	O
these	O
matching	O
examples	O
.	O
partitioning	O
algorithms	O
,	O
and	O
decision	O
trees	O
in	O
particular	O
,	O
divide	O
up	O
attribute	O
space	O
into	O
regions	O
of	O
self-similarity	O
:	O
all	O
data	O
within	O
a	O
given	O
box	O
are	O
treated	O
as	O
similar	O
,	O
and	O
posterior	O
class	O
probabilities	O
are	O
constant	O
within	O
the	O
box	O
.	O
decision	O
rules	O
based	O
on	O
bayes	O
rules	O
are	O
optimal	O
-	O
no	O
other	O
rule	O
has	O
lower	O
expected	O
error	O
rate	O
,	O
or	O
lower	O
expected	O
misclassiﬁcation	O
costs	O
.	O
although	O
unattainable	O
in	O
practice	O
,	O
they	O
provide	O
the	O
logical	O
basis	O
for	O
all	O
statistical	B
algorithms	O
.	O
they	O
are	O
unattainable	O
because	O
they	O
assume	O
complete	O
information	O
is	O
known	O
about	O
the	O
statistical	B
distributions	O
in	O
each	O
class	O
.	O
statistical	B
procedures	O
try	O
to	O
supply	O
the	O
missing	O
distributional	O
information	O
in	O
a	O
variety	O
of	O
ways	O
,	O
but	O
there	O
are	O
two	O
main	O
lines	O
:	O
parametric	O
and	O
non-parametric	O
.	O
parametric	O
methods	O
make	O
assumptions	O
about	O
the	O
nature	O
of	O
the	O
distributions	O
(	O
commonly	O
it	O
is	O
assumed	O
that	O
the	O
distributions	O
are	O
gaussian	O
)	O
,	O
and	O
the	O
problem	O
is	O
reduced	O
to	O
estimating	O
the	O
parameters	O
of	O
the	O
distributions	O
(	O
means	O
and	O
variances	O
in	O
the	O
case	O
of	O
gaussians	O
)	O
.	O
non-parametric	O
methods	O
make	O
no	O
assumptions	O
about	O
the	O
speciﬁc	O
distributions	O
involved	O
,	O
and	O
are	O
therefore	O
described	O
,	O
perhaps	O
more	O
accurately	O
,	O
as	O
distribution-free	O
.	O
2.7	O
reference	O
texts	O
there	O
are	O
several	O
good	O
textbooks	O
that	O
we	O
can	O
recommend	O
.	O
weiss	O
&	O
kulikowski	O
(	O
1991	O
)	O
give	O
an	O
overall	O
view	O
of	O
classiﬁcation	B
methods	O
in	O
a	O
text	O
that	O
is	O
probably	O
the	O
most	O
accessible	O
to	O
the	O
machine	O
learning	O
community	O
.	O
hand	O
(	O
1981	O
)	O
,	O
lachenbruch	O
&	O
mickey	O
(	O
1975	O
)	O
and	O
kendall	O
et	O
al	O
.	O
(	O
1983	O
)	O
give	O
the	O
statistical	B
approach	O
.	O
breiman	O
et	O
al	O
.	O
(	O
1984	O
)	O
describe	O
cart	O
,	O
which	O
is	O
a	O
partitioning	O
algorithm	O
developed	O
by	O
statisticians	O
,	O
and	O
silverman	O
(	O
1986	O
)	O
discusses	O
density	O
estimation	O
methods	O
.	O
for	O
neural	O
net	O
approaches	O
,	O
the	O
book	O
by	O
hertz	O
et	O
al	O
.	O
(	O
1991	O
)	O
is	O
probably	O
the	O
most	O
comprehensive	O
and	O
reliable	O
.	O
two	O
excellent	O
texts	O
on	O
pattern	O
recognition	O
are	O
those	O
of	O
fukunaga	O
(	O
1990	O
)	O
,	O
who	O
gives	O
a	O
thorough	O
treatment	O
of	O
classiﬁcation	B
problems	O
,	O
and	O
devijver	O
&	O
kittler	O
(	O
1982	O
)	O
who	O
concentrate	O
on	O
the	O
k-nearest	O
neighbour	O
approach	O
.	O
a	O
thorough	O
treatment	O
of	O
statistical	B
procedures	O
is	O
given	O
in	O
mclachlan	O
(	O
1992	O
)	O
,	O
who	O
also	O
mentions	O
the	O
more	O
important	O
alternative	O
approaches	O
.	O
a	O
recent	O
text	O
dealing	O
with	O
pattern	O
recognition	O
from	O
a	O
variety	O
of	O
perspectives	O
is	O
schalkoff	O
(	O
1992	O
)	O
.	O
3	O
classical	O
statistical	B
methods	O
j.	O
m.	O
o.	O
mitchell	O
university	O
of	O
strathclyde	O
3.1	O
introduction	O
this	O
chapter	O
provides	O
an	O
introduction	O
to	O
the	O
classical	O
statistical	B
discrimination	O
techniques	O
and	O
is	O
intended	O
for	O
the	O
non-statistical	O
reader	O
.	O
it	O
begins	O
with	O
fisher	O
’	O
s	O
linear	O
discriminant	O
,	O
which	O
requires	O
no	O
probability	O
assumptions	O
,	O
and	O
then	O
introduces	O
methods	O
based	O
on	O
maximum	O
likelihood	O
.	O
these	O
are	O
linear	O
discriminant	O
,	O
quadratic	O
discriminant	O
and	O
logistic	O
discriminant	O
.	O
next	O
there	O
is	O
a	O
brief	O
section	O
on	O
bayes	O
’	O
rules	O
,	O
which	O
indicates	O
how	O
each	O
of	O
the	O
methods	O
can	O
be	O
adapted	O
to	O
deal	O
with	O
unequal	O
prior	O
probabilities	O
and	O
unequal	O
misclassiﬁcation	O
costs	O
.	O
finally	O
there	O
is	O
an	O
illustrative	O
example	B
showing	O
the	O
result	O
of	O
applying	O
all	O
three	O
methods	O
to	O
a	O
two	O
class	O
and	O
two	O
attribute	O
problem	O
.	O
for	O
full	O
details	O
of	O
the	O
statistical	B
theory	O
involved	O
the	O
reader	O
should	O
consult	O
a	O
statistical	B
text	O
book	O
,	O
for	O
example	B
(	O
anderson	O
,	O
1958	O
)	O
.	O
	O
n	O
 * * 	O
methods	O
require	O
numerical	O
attribute	O
vectors	O
,	O
and	O
also	O
require	O
that	O
none	O
of	O
the	O
values	O
is	O
missing	O
.	O
where	O
an	O
attribute	O
is	O
categorical	O
with	O
two	O
values	O
,	O
an	O
indicator	O
is	O
used	O
,	O
i.e	O
.	O
an	O
attribute	O
which	O
takes	O
the	O
value	O
1	O
for	O
one	O
category	O
,	O
and	O
0	O
for	O
the	O
other	O
.	O
where	O
there	O
are	O
more	O
than	O
two	O
categorical	O
values	O
,	O
indicators	O
are	O
normally	O
set	O
up	O
for	O
each	O
of	O
the	O
values	O
.	O
however	O
there	O
is	O
then	O
redundancy	O
among	O
these	O
new	O
attributes	O
and	O
the	O
usual	O
procedure	O
is	O
the	O
training	O
set	O
will	O
consist	O
of	O
examples	O
drawn	O
from	O
,	O
known	O
classes	O
.	O
(	O
often	O
,	O
will	O
be	O
2	O
.	O
)	O
the	O
values	O
of0	O
numerically-valued	O
attributes	O
will	O
be	O
known	O
for	O
each	O
ofj	O
examples	O
,	O
and	O
these	O
form	O
the	O
attribute	O
vectorkl	O
&	O
m2	O
5	O
.	O
it	O
should	O
be	O
noted	O
that	O
these	O
to	O
drop	O
one	O
of	O
them	O
.	O
in	O
this	O
way	O
a	O
single	O
categorical	O
attribute	O
withg	O
values	O
is	O
replaced	O
by	O
(	O
attributes	O
whose	O
values	O
are	O
0	O
or	O
1.	O
where	O
the	O
attribute	O
values	O
are	O
ordered	O
,	O
it	O
may	O
be	O
address	O
for	O
correspondence	O
:	O
department	O
of	O
statistics	O
and	O
modelling	O
science	O
,	O
university	O
of	O
strathclyde	O
,	O
3.2	O
linear	O
discriminants	O
there	O
are	O
two	O
quite	O
different	O
justiﬁcations	O
for	O
using	O
fisher	O
’	O
s	O
linear	O
discriminant	O
rule	O
:	O
the	O
ﬁrst	O
,	O
as	O
given	O
by	O
fisher	O
(	O
1936	O
)	O
,	O
is	O
that	O
it	O
maximises	O
the	O
separation	O
between	O
the	O
classes	O
in	O
acceptable	O
to	O
use	O
a	O
single	O
numerical-valued	O
attribute	O
.	O
care	O
has	O
to	O
be	O
taken	O
that	O
the	O
numbers	O
used	O
reﬂect	O
the	O
spacing	O
of	O
the	O
categories	O
in	O
an	O
appropriate	O
fashion	O
.	O
glasgow	O
g1	O
1xh	O
,	O
u.k.	O
	O
''	O
	O
''	O
''	O
g	O
	O
18	O
classical	O
statistical	B
methods	O
[	O
ch	O
.	O
3	O
a	O
least-squares	O
sense	O
;	O
the	O
second	O
is	O
by	O
maximum	O
likelihood	O
(	O
see	O
section	O
3.2.3	O
)	O
.	O
we	O
will	O
give	O
a	O
brief	O
outline	O
of	O
these	O
approaches	O
.	O
for	O
a	O
proof	O
that	O
they	O
arrive	O
at	O
the	O
same	O
solution	O
,	O
we	O
refer	O
the	O
reader	O
to	O
mclachlan	O
(	O
1992	O
)	O
.	O
``	O
lp	O
24ks5	O
line	O
,	O
i.e	O
.	O
the	O
wrong	O
side	O
of	O
and	O
this	O
is	O
easily	O
seen	O
to	O
be	O
2	O
?	O
o	O
5w	O
2	O
?	O
o	O
,	O
o	O
,	O
o	O
3.2.1	O
linear	O
discriminants	O
by	O
least	O
squares	O
fisher	O
’	O
s	O
linear	O
discriminant	O
(	O
fisher	O
,	O
1936	O
)	O
is	O
an	O
empirical	O
method	O
for	O
classiﬁcation	B
based	O
purely	O
on	O
attribute	O
vectors	O
.	O
a	O
hyperplane	O
(	O
line	O
in	O
two	O
dimensions	O
,	O
plane	O
in	O
three	O
dimensions	O
,	O
the	O
discriminant	O
between	O
the	O
classes	O
.	O
we	O
wish	O
the	O
discriminants	O
for	O
the	O
two	O
classes	O
to	O
the	O
attribute	O
vectors	O
overall	O
and	O
for	O
the	O
two	O
classes	O
.	O
suppose	O
that	O
we	O
are	O
given	O
a	O
set	O
of	O
between	O
the	O
mean	O
discriminants	O
for	O
the	O
two	O
classes	O
divided	O
by	O
the	O
standard	O
deviation	O
of	O
as	O
possible	O
.	O
points	O
are	O
classiﬁed	O
according	O
to	O
the	O
side	O
of	O
the	O
hyperplane	O
that	O
they	O
fall	O
on	O
.	O
for	O
example	B
,	O
see	O
figure	O
3.1	O
,	O
which	O
illustrates	O
discrimination	O
between	O
two	O
“	O
digits	O
”	O
,	O
with	O
the	O
continuous	O
line	O
as	O
the	O
discriminating	O
hyperplane	O
between	O
the	O
two	O
populations	O
.	O
this	O
procedure	O
is	O
also	O
equivalent	O
to	O
a	O
t-test	O
or	O
f-test	O
for	O
a	O
signiﬁcant	O
difference	O
between	O
the	O
mean	O
discriminants	O
for	O
the	O
two	O
samples	O
,	O
the	O
t-statistic	O
or	O
f-statistic	O
being	O
constructed	O
to	O
have	O
the	O
largest	O
possible	O
value	O
.	O
etc	O
.	O
)	O
in	O
the0	O
-dimensional	O
attribute	O
space	O
is	O
chosen	O
to	O
separate	O
the	O
known	O
classes	O
as	O
well	O
more	O
precisely	O
,	O
in	O
the	O
case	O
of	O
two	O
classes	O
,	O
leto	O
be	O
respectively	O
the	O
means	O
of	O
n	O
and	O
let	O
us	O
call	O
the	O
particular	O
linear	O
combination	O
of	O
attributes	O
 q q 	O
coefﬁcientsp	O
2qks5e	O
&	O
en	O
differ	O
as	O
much	O
as	O
possible	O
,	O
and	O
one	O
measure	B
for	O
this	O
is	O
the	O
differencer	O
2to	O
2	O
?	O
o	O
the	O
discriminants	O
,	O
u	O
?	O
v	O
say	O
,	O
giving	O
the	O
following	O
measure	B
of	O
discrimination	O
:	O
the	O
assumption	O
of	O
a	O
multivariate	O
normal	O
distribution	O
forr	O
that	O
the	O
normal	O
random	O
variabler	O
where	O
we	O
assume	O
,	O
without	O
loss	O
of	O
generality	O
,	O
thatr	O
2	O
?	O
o	O
are	O
not	O
of	O
equal	O
sizes	O
,	O
or	O
if	O
,	O
as	O
is	O
very	O
frequently	O
the	O
case	O
,	O
the	O
variance	O
ofr	O
of	O
variance	O
.	O
the	O
sum	O
of	O
squares	O
ofr	O
24ks5	O
within	O
class	O
this	O
measure	B
of	O
discrimination	O
is	O
related	O
to	O
an	O
estimate	O
of	O
misclassiﬁcation	O
error	O
based	O
on	O
(	O
note	O
that	O
this	O
is	O
a	O
weaker	O
assumption	O
than	O
saying	O
that	O
x	O
has	O
a	O
normal	O
distribution	O
)	O
.	O
for	O
the	O
sake	O
of	O
argument	O
,	O
we	O
set	O
the	O
dividing	O
line	O
between	O
the	O
two	O
classes	O
at	O
the	O
midpoint	O
between	O
the	O
two	O
class	O
means	O
.	O
then	O
we	O
may	O
estimate	O
the	O
probability	O
of	O
misclassiﬁcation	O
for	O
one	O
class	O
as	O
the	O
probability	O
for	O
that	O
class	O
is	O
on	O
the	O
wrong	O
side	O
of	O
the	O
dividing	O
is	O
negative	O
.	O
if	O
the	O
classes	O
is	O
not	O
the	O
same	O
for	O
the	O
two	O
classes	O
,	O
the	O
dividing	O
line	O
is	O
best	O
drawn	O
at	O
some	O
point	O
other	O
than	O
the	O
midpoint	O
.	O
rather	O
than	O
use	O
the	O
simple	O
measure	B
quoted	O
above	O
,	O
it	O
is	O
more	O
convenient	O
algebraically	O
to	O
use	O
an	O
equivalent	O
measure	B
deﬁned	O
in	O
terms	O
of	O
sums	O
of	O
squared	O
deviations	O
,	O
as	O
in	O
analysis	O
2qks5	O
2	O
?	O
o	O
is	O
24ks5	O
k	O
k	O
k	O
''	O
r	O
p	O
a	O
	O
a	O
k	O
5	O
	O
r	O
k	O
5	O
r	O
2	O
o	O
k	O
5	O
	O
r	O
2	O
o	O
k	O
5	O
u	O
v	O
r	O
k	O
r	O
k	O
5	O
	O
x	O
2	O
r	O
2	O
o	O
k	O
5	O
	O
r	O
2	O
o	O
k	O
5	O
	O
u	O
v	O
5	O
k	O
5	O
	O
r	O
k	O
5	O
!	O
sec	O
.	O
3.2	O
]	O
linear	O
discrimination	O
19	O
say	O
,	O
is	O
the	O
sum	O
of	O
these	O
quantities	O
for	O
the	O
two	O
classes	O
(	O
this	O
is	O
the	O
quantity	O
that	O
would	O
give	O
where	O
this	O
last	O
sum	O
is	O
now	O
over	O
both	O
classes	O
.	O
by	O
subtraction	O
,	O
the	O
pooled	O
sum	O
of	O
squares	O
calculate	O
the	O
f-statistic	O
24ks5	O
2to	O
24ks5	O
2	O
?	O
o	O
ky	O
!	O
l5	O
@	O
5	O
!	O
.	O
the	O
pooled	O
sum	O
of	O
squares	O
within	O
classes	O
,	O
z	O
the	O
sum	O
being	O
over	O
the	O
examples	O
in	O
class	O
v	O
)	O
.	O
the	O
total	O
sum	O
of	O
squares	O
ofr	O
us	O
a	O
standard	O
deviationu	O
24ks5	O
is	O
{	O
|2	O
k=5	O
%	O
5	O
@	O
9	O
&	O
~	O
}	O
say	O
,	O
z	O
,	O
and	O
this	O
last	O
quantity	O
is	O
proportional	O
to2	O
between	O
classes	O
is	O
}	O
5	O
%	O
5	O
@	O
.	O
in	O
terms	O
of	O
the	O
f-test	O
for	O
the	O
signiﬁcance	O
of	O
the	O
differencer	O
2	O
?	O
o	O
5	O
,	O
we	O
would	O
2f	O
}	O
zp5	O
@	O
dd	O
(	O
	O
zmdm2w	O
clearly	O
maximising	O
the	O
f-ratio	O
statistic	O
is	O
equivalent	O
to	O
maximising	O
the	O
ratio	O
}	O
ldhz	O
,	O
so	O
the	O
 * + 	O
coefﬁcientsp*a	O
,	O
g	O
&	O
	O
(	O
''	O
''	O
40	O
may	O
be	O
chosen	O
to	O
maximise	O
the	O
ratio	O
}	O
ld	O
z	O
.	O
this	O
maximisation	O
!	O
.	O
problem	O
may	O
be	O
solved	O
analytically	O
,	O
giving	O
an	O
explicit	O
solution	O
for	O
the	O
coefﬁcientsp	O
is	O
to	O
normalise	O
thep	O
arbitrary	O
multiplicative	O
constant	O
so	O
that	O
the	O
separationr	O
5	O
between	O
the	O
class	O
ratio	O
is	O
now	O
equivalent	O
to	O
minimising	O
the	O
total	O
sum	O
of	O
squaresz	O
.	O
put	O
this	O
way	O
,	O
the	O
problem	O
there	O
is	O
however	O
an	O
arbitrary	O
multiplicative	O
constant	O
in	O
the	O
solution	O
,	O
and	O
the	O
usual	O
practice	O
in	O
some	O
way	O
so	O
that	O
the	O
solution	O
is	O
uniquely	O
determined	O
.	O
often	O
one	O
coefﬁcient	O
is	O
taken	O
to	O
be	O
unity	O
(	O
so	O
avoiding	O
a	O
multiplication	O
)	O
.	O
however	O
the	O
detail	O
of	O
this	O
need	O
not	O
concern	O
us	O
here	O
.	O
the	O
main	O
point	O
about	O
this	O
method	O
is	O
that	O
it	O
is	O
a	O
linear	O
function	O
of	O
the	O
attributes	O
that	O
is	O
used	O
to	O
carry	O
out	O
the	O
classiﬁcation	B
.	O
this	O
often	O
works	O
well	O
,	O
but	O
it	O
is	O
easy	O
to	O
see	O
that	O
it	O
may	O
work	O
badly	O
if	O
a	O
linear	O
separator	O
is	O
not	O
appropriate	O
.	O
this	O
could	O
happen	O
for	O
example	B
if	O
the	O
data	O
for	O
one	O
class	O
formed	O
a	O
tight	O
cluster	O
and	O
the	O
the	O
values	O
for	O
the	O
other	O
class	O
were	O
widely	O
spread	O
around	O
it	O
.	O
however	O
the	O
coordinate	O
system	O
used	O
is	O
of	O
no	O
importance	O
.	O
equivalent	O
results	O
will	O
be	O
obtained	O
after	O
any	O
linear	O
transformation	O
of	O
the	O
coordinates	O
.	O
is	O
identical	O
to	O
a	O
regression	O
of	O
class	O
(	O
treated	O
numerically	O
)	O
on	O
the	O
attributes	O
,	O
the	O
dependent	O
variable	O
class	O
being	O
zero	O
for	O
one	O
class	O
and	O
unity	O
for	O
the	O
other	O
.	O
mean	O
discriminants	O
is	O
equal	O
to	O
some	O
predetermined	O
value	O
(	O
say	O
unity	O
)	O
.	O
maximising	O
the	O
f-	O
to	O
justify	O
the	O
“	O
least	O
squares	O
”	O
of	O
the	O
title	O
for	O
this	O
section	O
,	O
note	O
that	O
we	O
may	O
choose	O
the	O
a	O
practical	O
complication	O
is	O
that	O
for	O
the	O
algorithm	O
to	O
work	O
the	O
pooled	O
sample	O
covariance	O
!	O
examples	O
from	O
matrix	O
must	O
be	O
invertible	O
.	O
the	O
covariance	O
matrix	O
for	O
a	O
dataset	O
withj	O
!	O
,	O
is	O
class	O
!	O
	O
&	O
jy	O
!	O
is	O
the0	O
-dimensional	O
row-vector	O
is	O
thejy	O
!	O
	O
0	O
matrix	O
of	O
attribute	O
values	O
,	O
ando	O
of	O
attribute	O
means	O
.	O
the	O
pooled	O
covariance	O
matrix	O
where	O
!	O
ldm24j	O
,	O
5	O
where	O
the	O
24jy	O
!	O
(	O
+5	O
is	O
{	O
summation	O
is	O
over	O
all	O
the	O
classes	O
,	O
and	O
the	O
divisorj	O
is	O
chosen	O
to	O
make	O
the	O
pooled	O
covariance	O
matrix	O
unbiased	O
.	O
for	O
invertibility	O
the	O
attributes	O
must	O
be	O
linearly	O
independent	O
,	O
which	O
means	O
that	O
no	O
attribute	O
may	O
be	O
an	O
exact	O
linear	O
combination	O
of	O
other	O
attributes	O
.	O
in	O
order	O
to	O
achieve	O
this	O
,	O
some	O
attributes	O
may	O
have	O
to	O
be	O
dropped	O
.	O
moreover	O
no	O
attribute	O
can	O
be	O
constant	O
within	O
each	O
class	O
.	O
of	O
course	O
an	O
attribute	O
which	O
is	O
constant	O
within	O
each	O
class	O
but	O
not	O
overall	O
may	O
be	O
an	O
excellent	O
discriminator	O
and	O
is	O
likely	O
to	O
be	O
utilised	O
in	O
decision	O
tree	O
algorithms	O
.	O
however	O
it	O
will	O
cause	O
the	O
linear	O
discriminant	O
algorithm	O
to	O
fail	O
.	O
this	O
situation	O
can	O
be	O
treated	O
by	O
adding	O
a	O
small	O
positive	O
constant	O
to	O
the	O
corresponding	O
diagonal	O
element	O
of	O
(	O
m	O
	O
n	O
2	O
r	O
	O
r	O
''	O
r	O
	O
r	O
2	O
o	O
	O
r	O
2	O
o	O
k	O
5	O
	O
r	O
2	O
o	O
k	O
k	O
5	O
	O
r	O
k	O
	O
&	O
	O
5	O
a	O
2	O
o	O
k	O
5	O
	O
r	O
2	O
o	O
k	O
(	O
	O
	O
o	O
k	O
	O
o	O
	O
''	O
k	O
	O
	O
	O
	O
,	O
20	O
classical	O
statistical	B
methods	O
[	O
ch	O
.	O
3	O
the	O
pooled	O
covariance	O
matrix	O
,	O
or	O
by	O
adding	O
random	O
noise	O
to	O
the	O
attribute	O
before	O
applying	O
the	O
algorithm	O
.	O
in	O
order	O
to	O
deal	O
with	O
the	O
case	O
of	O
more	O
than	O
two	O
classes	O
fisher	O
(	O
1938	O
)	O
suggested	O
the	O
use	O
of	O
canonical	O
variates	O
.	O
first	O
a	O
linear	O
combination	O
of	O
the	O
attributes	O
is	O
chosen	O
to	O
minimise	O
the	O
ratio	O
of	O
the	O
pooled	O
within	O
class	O
sum	O
of	O
squares	O
to	O
the	O
total	O
sum	O
of	O
squares	O
.	O
then	O
further	O
linear	O
functions	O
are	O
found	O
to	O
improve	O
the	O
discrimination	O
.	O
(	O
the	O
coefﬁcients	O
in	O
these	O
functions	O
are	O
the	O
eigenvectors	O
corresponding	O
to	O
the	O
non-zero	O
eigenvalues	O
of	O
a	O
certain	O
matrix	O
.	O
)	O
in	O
general	O
there	O
will	O
be	O
min2	O
>	O
,	O
(	O
''	O
40y5	O
canonical	O
variates	O
.	O
it	O
may	O
turn	O
out	O
that	O
only	O
a	O
few	O
of	O
the	O
canonical	O
variates	O
are	O
important	O
.	O
then	O
an	O
observation	O
can	O
be	O
assigned	O
to	O
the	O
class	O
whose	O
centroid	O
is	O
closest	O
in	O
the	O
subspace	O
deﬁned	O
by	O
these	O
variates	O
.	O
it	O
is	O
especially	O
useful	O
when	O
the	O
class	O
means	O
are	O
ordered	O
,	O
or	O
lie	O
along	O
a	O
simple	O
curve	O
in	O
attribute-space	O
.	O
in	O
the	O
simplest	O
case	O
,	O
the	O
class	O
means	O
lie	O
along	O
a	O
straight	O
line	O
.	O
this	O
is	O
the	O
case	O
for	O
the	O
head	B
injury	I
data	O
(	O
see	O
section	O
9.4.1	O
)	O
,	O
for	O
example	B
,	O
and	O
,	O
in	O
general	O
,	O
arises	O
when	O
the	O
classes	O
are	O
ordered	O
in	O
some	O
sense	O
.	O
in	O
this	O
book	O
,	O
this	O
procedure	O
was	O
not	O
used	O
as	O
a	O
classiﬁer	B
,	O
but	O
rather	O
in	O
a	O
qualitative	O
sense	O
to	O
give	O
some	O
measure	B
of	O
reduced	O
dimensionality	O
in	O
attribute	O
space	O
.	O
since	O
this	O
technique	O
can	O
also	O
be	O
used	O
as	O
a	O
basis	O
for	O
explaining	O
differences	O
in	O
mean	O
vectors	O
as	O
in	O
analysis	O
of	O
variance	O
,	O
the	O
procedure	O
may	O
be	O
called	O
manova	O
,	O
standing	O
for	O
multivariate	O
analysis	O
of	O
variance	O
.	O
3.2.2	O
special	O
case	O
of	O
two	O
classes	O
the	O
linear	O
discriminant	O
procedure	O
is	O
particularly	O
easy	O
to	O
program	O
when	O
there	O
are	O
just	O
two	O
classes	O
,	O
for	O
then	O
the	O
fisher	O
discriminant	O
problem	O
is	O
equivalent	O
to	O
a	O
multiple	O
regression	O
problem	O
,	O
with	O
the	O
attributes	O
being	O
used	O
to	O
predict	O
the	O
class	O
value	O
which	O
is	O
treated	O
as	O
for	O
a	O
numerical-valued	O
variable	O
.	O
the	O
class	O
values	O
are	O
converted	O
to	O
numerical	O
values	O
:	O
is	O
given	O
the	O
value	O
1.	O
a	O
standard	O
multiple	O
regression	O
package	O
is	O
then	O
used	O
to	O
predict	O
the	O
class	O
value	O
.	O
if	O
the	O
two	O
classes	O
are	O
equiprobable	O
,	O
the	O
discriminating	O
hyperplane	O
bisects	O
the	O
line	O
joining	O
the	O
class	O
centroids	O
.	O
otherwise	O
,	O
the	O
discriminating	O
hyperplane	O
is	O
closer	O
to	O
the	O
less	O
frequent	O
class	O
.	O
the	O
formulae	O
are	O
most	O
easily	O
derived	O
by	O
considering	O
the	O
multiple	O
regression	O
predictor	O
as	O
a	O
single	O
attribute	O
that	O
is	O
to	O
be	O
used	O
as	O
a	O
one-dimensional	O
discriminant	O
,	O
and	O
then	O
applying	O
the	O
formulae	O
of	O
the	O
following	O
section	O
.	O
the	O
procedure	O
is	O
simple	O
,	O
but	O
the	O
details	O
can	O
not	O
be	O
expressed	O
simply	O
.	O
see	O
ripley	O
(	O
1993	O
)	O
for	O
the	O
explicit	O
connection	O
between	O
discrimination	O
and	O
regression	O
.	O
is	O
given	O
the	O
value	O
0	O
and	O
class	O
example	B
,	O
class	O
3.2.3	O
linear	O
discriminants	O
by	O
maximum	O
likelihood	O
the	O
justiﬁcation	O
of	O
the	O
other	O
statistical	B
algorithms	O
depends	O
on	O
the	O
consideration	O
of	O
prob-	O
ability	O
distributions	O
,	O
and	O
the	O
linear	O
discriminant	O
procedure	O
itself	O
has	O
a	O
justiﬁcation	O
of	O
this	O
kind	O
.	O
new	O
point	O
with	O
attribute	O
vector	O
x	O
is	O
then	O
assigned	O
to	O
that	O
class	O
for	O
which	O
the	O
probability	O
is	O
greatest	O
.	O
this	O
is	O
a	O
maximum	O
likelihood	O
method	O
.	O
a	O
frequently	O
made	O
assumption	O
is	O
that	O
the	O
distributions	O
are	O
normal	O
(	O
or	O
gaussian	O
)	O
with	O
different	O
means	O
but	O
the	O
same	O
covariance	O
matrix	O
.	O
the	O
probability	O
density	O
function	O
of	O
the	O
normal	O
distribution	O
is	O
!	O
are	O
independent	O
it	O
is	O
assumed	O
that	O
the	O
attribute	O
vectors	O
for	O
examples	O
of	O
class	O
!	O
.	O
a	O
and	O
follow	O
a	O
certain	O
probability	O
distribution	O
with	O
probability	O
density	O
function	O
(	O
pdf	O
)	O
b	O
2	O
x5	O
density	O
functionb	O
<	O
exp2	O
.s	O
	O
x	O
5	O
%	O
5	O
?	O
''	O
(	O
3.1	O
)	O
24k	O
	O
24k	O
	O
!	O
(	O
	O
<	O
	O
	O
(	O
	O
5	O
	O
sec	O
.	O
3.3	O
]	O
linear	O
discrimination	O
21	O
is	O
a0	O
-dimensional	O
vector	O
denoting	O
the	O
(	O
theoretical	O
)	O
mean	O
for	O
a	O
class	O
and	O
,	O
(	O
necessarily	O
positive	O
deﬁnite	O
)	O
matrix	O
.	O
the	O
(	O
sample	O
)	O
covariance	O
matrix	O
that	O
we	O
saw	O
earlier	O
is	O
the	O
sample	O
analogue	O
of	O
this	O
covariance	O
matrix	O
,	O
which	O
is	O
best	O
thought	O
of	O
as	O
a	O
set	O
of	O
coefﬁcients	O
in	O
the	O
pdf	O
or	O
a	O
set	O
of	O
parameters	O
for	O
the	O
distribution	O
.	O
this	O
means	O
that	O
the	O
points	O
for	O
the	O
class	O
are	O
distributed	O
in	O
a	O
cluster	O
centered	O
.	O
each	O
cluster	O
has	O
the	O
same	O
orientation	O
and	O
spread	O
though	O
their	O
means	O
will	O
of	O
course	O
be	O
different	O
.	O
(	O
it	O
should	O
be	O
noted	O
that	O
there	O
is	O
in	O
theory	O
no	O
absolute	O
boundary	O
for	O
the	O
clusters	O
but	O
the	O
contours	O
for	O
the	O
probability	O
density	O
function	O
in	O
practice	O
occurrences	O
of	O
examples	O
outside	O
a	O
certain	O
ellipsoid	O
have	O
ellipsoidal	O
shape	O
.	O
will	O
be	O
extremely	O
rare	O
.	O
)	O
in	O
this	O
case	O
it	O
can	O
be	O
shown	O
that	O
the	O
boundary	O
separating	O
two	O
classes	O
,	O
deﬁned	O
by	O
equality	O
of	O
the	O
two	O
pdfs	O
,	O
is	O
indeed	O
a	O
hyperplane	O
and	O
it	O
passes	O
through	O
the	O
mid-point	O
of	O
the	O
two	O
centres	O
.	O
its	O
equation	O
is	O
where	O
the	O
(	O
theoretical	O
)	O
covariance	O
matrix	O
,	O
is	O
a0r	O
0	O
at	O
of	O
ellipsoidal	O
shape	O
described	O
by	O
	O
where	O
!	O
denotes	O
the	O
population	O
mean	O
for	O
class9	O
!	O
.	O
however	O
in	O
classiﬁcation	B
the	O
exact	O
the	O
distributions	O
.	O
with	O
two	O
classes	O
,	O
if	O
the	O
sample	O
means	O
are	O
substituted	O
for	O
!	O
and	O
the	O
pooled	O
sample	O
covariance	O
matrix	O
for	O
,	O
then	O
fisher	O
’	O
s	O
linear	O
discriminant	O
is	O
obtained	O
.	O
with	O
more	O
than	O
two	O
classes	O
,	O
this	O
method	O
does	O
not	O
in	O
general	O
give	O
the	O
same	O
results	O
as	O
fisher	O
’	O
s	O
discriminant	O
.	O
distribution	O
is	O
usually	O
not	O
known	O
,	O
and	O
it	O
becomes	O
necessary	O
to	O
estimate	O
the	O
parameters	O
for	O
^	O
^	O
	O
5	O
'	O
&	O
(	O
3.2	O
)	O
is	O
the	O
probability	O
density	O
3.2.4	O
more	O
than	O
two	O
classes	O
when	O
there	O
are	O
more	O
than	O
two	O
classes	O
,	O
it	O
is	O
no	O
longer	O
possible	O
to	O
use	O
a	O
single	O
linear	O
discriminant	O
score	O
to	O
separate	O
the	O
classes	O
.	O
the	O
simplest	O
procedure	O
is	O
to	O
calculate	O
a	O
linear	O
discriminant	O
for	O
each	O
class	O
,	O
this	O
discriminant	O
being	O
just	O
the	O
logarithm	O
of	O
the	O
estimated	O
probability	O
density	O
function	O
for	O
the	O
appropriate	O
class	O
,	O
with	O
constant	O
terms	O
dropped	O
.	O
sample	O
values	O
are	O
substituted	O
for	O
population	O
values	O
where	O
these	O
are	O
unknown	O
(	O
this	O
gives	O
the	O
“	O
plug-	O
in	O
”	O
estimates	O
)	O
.	O
where	O
the	O
prior	O
class	O
proportions	O
are	O
unknown	O
,	O
they	O
would	O
be	O
estimated	O
by	O
the	O
relative	O
frequencies	O
in	O
the	O
training	O
set	O
.	O
similarly	O
,	O
the	O
sample	O
means	O
and	O
pooled	O
covariance	O
matrix	O
are	O
substituted	O
for	O
the	O
population	O
means	O
and	O
covariance	O
matrix	O
.	O
suppose	O
the	O
prior	O
probability	O
of	O
class9	O
!	O
is.	O
!	O
,	O
and	O
thatbt	O
!	O
l2	O
of	O
in	O
class9	O
!	O
,	O
and	O
is	O
the	O
normal	O
density	O
given	O
in	O
equation	O
(	O
3.1	O
)	O
.	O
the	O
joint	O
probability	O
of	O
observing	O
class	O
!	O
and	O
attribute	O
is.d	O
!	O
6bt	O
!	O
-2	O
5	O
and	O
the	O
logarithm	O
of	O
the	O
probability	O
of	O
observing	O
class	O
!	O
and	O
attributek	O
log	O
.	O
k	O
	O
	O
!	O
are	O
given	O
by	O
the	O
coefﬁcients	O
of	O
x	O
to	O
within	O
an	O
additive	O
constant	O
.	O
so	O
the	O
coefﬁcients	O
&	O
s	O
!	O
by	O
and	O
the	O
additive	O
constant	O
log.	O
!	O
3	O
!	O
	O
&	O
,	O
	O
!	O
.	O
to	O
obtain	O
the	O
corresponding	O
“	O
plug-in	O
”	O
formulae	O
,	O
substitute	O
the	O
rameters	O
i	O
and	O
.	O
corresponding	O
sample	O
estimators	O
:	O
	O
!	O
for	O
!	O
for	O
.	O
;	O
o	O
i	O
;	O
and0	O
for	O
!	O
examples	O
.	O
proportion	O
of	O
class	O
the	O
above	O
formulae	O
are	O
stated	O
in	O
terms	O
of	O
the	O
(	O
generally	O
unknown	O
)	O
population	O
pa-	O
though	O
these	O
can	O
be	O
simpliﬁed	O
by	O
subtracting	O
the	O
coefﬁcients	O
for	O
the	O
last	O
class	O
.	O
!	O
,	O
where0	O
is	O
the	O
sample	O
is	O
k	O
	O
2	O
	O
5	O
	O
(	O
	O
2	O
	O
	O
5	O
	O
2	O
	O
	O
''	O
	O
5	O
	O
!	O
	O
	O
!	O
	O
(	O
	O
	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
(	O
	O
	O
	O
!	O
	O
	O
	O
!	O
k	O
!	O
22	O
classical	O
statistical	B
methods	O
[	O
ch	O
.	O
3	O
3.3	O
quadratic	O
discriminant	O
quadratic	O
discrimination	O
is	O
similar	O
to	O
linear	O
discrimination	O
,	O
but	O
the	O
boundary	O
between	O
two	O
discrimination	O
regions	O
is	O
now	O
allowed	O
to	O
be	O
a	O
quadratic	O
surface	O
.	O
when	O
the	O
assumption	O
of	O
equal	O
covariance	O
matrices	O
is	O
dropped	O
,	O
then	O
in	O
the	O
maximum	O
likelihood	O
argument	O
with	O
normal	O
distributions	O
a	O
quadratic	O
surface	O
(	O
for	O
example	B
,	O
ellipsoid	O
,	O
hyperboloid	O
,	O
etc	O
.	O
)	O
is	O
obtained	O
.	O
this	O
type	O
of	O
discrimination	O
can	O
deal	O
with	O
classiﬁcations	O
where	O
the	O
set	O
of	O
attribute	O
values	O
for	O
one	O
class	O
to	O
some	O
extent	O
surrounds	O
that	O
for	O
another	O
.	O
clarke	O
et	O
al	O
.	O
(	O
1979	O
)	O
ﬁnd	O
that	O
the	O
quadratic	O
discriminant	O
procedure	O
is	O
robust	O
to	O
small	O
departures	O
from	O
normality	O
and	O
that	O
heavy	O
kurtosis	O
(	O
heavier	O
tailed	O
distributions	O
than	O
gaussian	O
)	O
does	O
not	O
substantially	O
reduce	O
accuracy	O
.	O
however	O
,	O
the	O
number	O
of	O
parameters	O
to	O
be	O
estimated	O
becomes	O
,	O
-0_20	O
	O
(	O
*5	O
@	O
d	O
and	O
the	O
difference	O
between	O
the	O
variances	O
would	O
need	O
to	O
be	O
considerable	O
to	O
justify	O
the	O
use	O
of	O
this	O
method	O
,	O
especially	O
for	O
small	O
or	O
moderate	O
sized	O
datasets	O
(	O
marks	O
&	O
dunn	O
,	O
1974	O
)	O
.	O
occasionally	O
,	O
differences	O
in	O
the	O
covariances	O
are	O
of	O
scale	O
only	O
and	O
some	O
simpliﬁcation	O
may	O
occur	O
(	O
kendall	O
et	O
al.	O
,	O
1983	O
)	O
.	O
linear	O
discriminant	O
is	O
thought	O
to	O
be	O
still	O
effective	O
if	O
the	O
departure	O
from	O
equality	O
of	O
covariances	O
is	O
small	O
(	O
gilbert	O
,	O
1969	O
)	O
.	O
some	O
aspects	O
of	O
quadratic	O
dependence	O
may	O
be	O
included	O
in	O
the	O
linear	O
or	O
logistic	O
form	O
(	O
see	O
below	O
)	O
by	O
adjoining	O
new	O
attributes	O
that	O
are	O
quadratic	O
functions	O
of	O
the	O
given	O
attributes	O
.	O
	O
,	O
3.3.1	O
quadratic	O
discriminant	O
-	O
programming	O
details	O
the	O
quadratic	O
discriminant	O
function	O
is	O
most	O
simply	O
deﬁned	O
as	O
the	O
logarithm	O
of	O
the	O
ap-	O
propriate	O
probability	O
density	O
function	O
,	O
so	O
that	O
one	O
quadratic	O
discriminant	O
is	O
calculated	O
for	O
each	O
class	O
.	O
the	O
procedure	O
used	O
is	O
to	O
take	O
the	O
logarithm	O
of	O
the	O
probability	O
density	O
function	O
and	O
to	O
substitute	O
the	O
sample	O
means	O
and	O
covariance	O
matrices	O
in	O
place	O
of	O
the	O
population	O
values	O
,	O
giving	O
the	O
so-called	O
“	O
plug-in	O
”	O
estimates	O
.	O
taking	O
the	O
logarithm	O
of	O
equation	O
(	O
3.1	O
)	O
,	O
in	O
classiﬁcation	B
,	O
the	O
quadratic	O
discriminant	O
is	O
calculated	O
for	O
each	O
class	O
and	O
the	O
class	O
with	O
the	O
largest	O
discriminant	O
is	O
chosen	O
.	O
to	O
ﬁnd	O
the	O
a	O
posteriori	O
class	O
probabilities	O
explicitly	O
,	O
the	O
exponential	O
is	O
taken	O
of	O
the	O
discriminant	O
and	O
the	O
resulting	O
quantities	O
normalised	O
to	O
sum	O
!	O
,	O
we	O
obtain	O
and	O
allowing	O
for	O
differing	O
prior	O
class	O
probabilities	O
.	O
^	O
^	O
log2	O
log2q	O
.	O
log	O
.	O
24k	O
!	O
.	O
here	O
it	O
is	O
understood	O
that	O
the	O
sufﬁx	O
$	O
refers	O
to	O
as	O
the	O
quadratic	O
discriminant	O
for	O
class	O
!	O
.	O
the	O
sample	O
of	O
values	O
from	O
class	O
ks5	O
are	O
given	O
by	O
to	O
unity	O
(	O
see	O
section	O
2.6	O
)	O
.	O
thus	O
the	O
posterior	O
class	O
probabilities	O
:	O
`24	O
	O
^	O
log2	O
exp	O
log24	O
.	O
:	O
;	O
24	O
5	O
>	O
	O
24k	O
2qk	O
ks5	O
and	O
associated	O
expected	O
costs	O
explicitly	O
,	O
using	O
to	O
calculate	O
the	O
class	O
probabilities	O
:	O
;	O
24	O
the	O
formulae	O
of	O
section	O
2.6.	O
the	O
most	O
frequent	O
problem	O
with	O
quadratic	O
discriminants	O
is	O
caused	O
when	O
some	O
attribute	O
has	O
zero	O
variance	O
in	O
one	O
class	O
,	O
for	O
then	O
the	O
covariance	O
matrix	O
can	O
not	O
be	O
inverted	O
.	O
one	O
way	O
of	O
avoiding	O
this	O
problem	O
is	O
to	O
add	O
a	O
small	O
positive	O
constant	O
term	O
to	O
the	O
diagonal	O
terms	O
in	O
the	O
covariance	O
matrix	O
(	O
this	O
corresponds	O
to	O
adding	O
random	O
noise	O
to	O
the	O
attributes	O
)	O
.	O
another	O
way	O
,	O
adopted	O
in	O
our	O
own	O
implementation	O
,	O
is	O
to	O
use	O
some	O
combination	O
of	O
the	O
class	O
covariance	O
and	O
the	O
pooled	O
covariance	O
.	O
if	O
there	O
is	O
a	O
cost	O
matrix	O
,	O
then	O
,	O
no	O
matter	O
the	O
number	O
of	O
classes	O
,	O
the	O
simplest	O
procedure	O
is	O
24k	O
5	O
'	O
&	O
ks5/	O
&	O
apart	O
from	O
a	O
normalising	O
factor	O
.	O
!	O
b	O
!	O
2	O
	O
!	O
5	O
	O
(	O
	O
<	O
	O
!	O
<	O
5	O
	O
(	O
	O
!	O
5	O
	O
	O
	O
!	O
!	O
5	O
!	O
<	O
!	O
<	O
!	O
5	O
	O
(	O
	O
<	O
	O
!	O
<	O
5	O
	O
(	O
	O
!	O
5	O
	O
	O
	O
!	O
!	O
!	O
<	O
sec	O
.	O
3.3	O
]	O
quadratic	O
discrimination	O
23	O
is	O
the	O
sample	O
!	O
;	O
o	O
!	O
for	O
!	O
for	O
i	O
;	O
and0	O
once	O
again	O
,	O
the	O
above	O
formulae	O
are	O
stated	O
in	O
terms	O
of	O
the	O
unknown	O
population	O
pa-	O
has	O
an	O
option	O
for	O
quadratic	O
discrimination	O
,	O
sas	O
also	O
does	O
quadratic	O
discrimination	O
)	O
.	O
many	O
statistical	B
packages	O
allow	O
for	O
quadratic	O
discrimination	O
(	O
for	O
example	B
,	O
minitab	O
!	O
,	O
	O
!	O
.	O
to	O
obtain	O
the	O
corresponding	O
“	O
plug-in	O
”	O
formulae	O
,	O
substitute	O
the	O
rameters	O
i	O
and	O
.	O
corresponding	O
sample	O
estimators	O
:	O
	O
!	O
examples	O
.	O
proportion	O
of	O
class	O
3.3.2	O
regularisation	O
and	O
smoothed	O
estimates	O
the	O
main	O
problem	O
with	O
quadratic	O
discriminants	O
is	O
the	O
large	O
number	O
of	O
parameters	O
that	O
need	O
to	O
be	O
estimated	O
and	O
the	O
resulting	O
large	O
variance	O
of	O
the	O
estimated	O
discriminants	O
.	O
a	O
related	O
problem	O
is	O
the	O
presence	O
of	O
zero	O
or	O
near	O
zero	O
eigenvalues	O
of	O
the	O
sample	O
covariance	O
matrices	O
.	O
attempts	O
to	O
alleviate	O
this	O
problem	O
are	O
known	O
as	O
regularisation	O
methods	O
,	O
and	O
the	O
most	O
practically	O
useful	O
of	O
these	O
was	O
put	O
forward	B
by	O
friedman	O
(	O
1989	O
)	O
,	O
who	O
proposed	O
a	O
compromise	O
between	O
linear	O
and	O
quadratic	O
discriminants	O
via	O
a	O
two-parameter	O
family	O
of	O
estimates	O
.	O
one	O
parameter	O
controls	O
the	O
smoothing	O
of	O
the	O
class	O
covariance	O
matrix	O
estimates	O
.	O
!	O
,	O
where0	O
!	O
for	O
.	O
.	O
friedman	O
(	O
1989	O
)	O
makes	O
the	O
is	O
the	O
pooled	O
covariance	O
matrix	O
.	O
is	O
a	O
(	O
small	O
)	O
constant	O
term	O
that	O
is	O
added	O
to	O
the	O
diagonals	O
of	O
the	O
covariance	O
matrices	O
:	O
this	O
is	O
done	O
to	O
make	O
the	O
covariance	O
matrix	O
non-singular	O
,	O
and	O
also	O
has	O
the	O
effect	O
of	O
smoothing	O
out	O
the	O
covariance	O
matrices	O
.	O
as	O
we	O
have	O
already	O
mentioned	O
in	O
connection	O
with	O
linear	O
discriminants	O
,	O
any	O
singularity	O
of	O
the	O
covariance	O
matrix	O
will	O
cause	O
problems	O
,	O
and	O
as	O
there	O
is	O
now	O
one	O
covariance	O
matrix	O
for	O
each	O
class	O
the	O
likelihood	O
of	O
such	O
a	O
problem	O
is	O
much	O
greater	O
,	O
especially	O
for	O
the	O
classes	O
with	O
small	O
sample	O
sizes	O
.	O
the	O
smoothed	O
estimate	O
of	O
the	O
class	O
$	O
covariance	O
matrix	O
is	O
	O
2l	O
(	O
where	O
is	O
the	O
class	O
$	O
sample	O
covariance	O
matrix	O
and	O
when	O
is	O
zero	O
,	O
there	O
is	O
no	O
smoothing	O
and	O
the	O
estimated	O
class	O
$	O
covariance	O
matrix	O
is	O
just	O
the	O
i	O
’	O
th	O
sample	O
covariance	O
matrix	O
!	O
.	O
when	O
the	O
!	O
are	O
unity	O
,	O
all	O
classes	O
have	O
the	O
same	O
covariance	O
matrix	O
,	O
namely	O
the	O
pooled	O
covariance	O
matrix	O
value	O
of	O
!	O
smaller	O
for	O
classes	O
with	O
larger	O
numbers	O
.	O
for	O
the	O
i	O
’	O
th	O
sample	O
withj	O
!	O
observations	O
:	O
	O
2w	O
(	O
+5	O
%	O
	O
5	O
#	O
24j	O
,	O
5	O
2-	O
(	O
2	O
>	O
	O
,	O
5	O
%	O
d	O
 q 	O
where	O
&	O
lj	O
j	O
^js	O
.	O
the	O
other	O
parameter	O
ordinary	O
linear	O
discriminants	O
(	O
	O
	O
)	O
;	O
quadratic	O
discriminants	O
(	O
	O
	O
)	O
;	O
and	O
&	O
	O
(	O
	O
''	O
-	O
&	O
the	O
values	O
&	O
(	O
	O
''	O
-w	O
&	O
(	O
correspond	O
to	O
a	O
minimum	O
euclidean	O
distance	O
rule	O
.	O
error	O
,	O
to	O
choose	O
the	O
values	O
of	O
and	O
	O
and	O
[	O
&	O
the	O
default	O
values	O
of	O
	O
were	O
adopted	O
for	O
the	O
majority	O
of	O
statlog	O
datasets	O
,	O
default	O
values	O
were	O
used	O
for	O
the	O
head	B
injury	I
dataset	I
(	O
	O
=0.05	O
)	O
and	O
the	O
dna	O
dataset	O
(	O
	O
=0.3	O
this	O
type	O
of	O
regularisation	O
has	O
been	O
incorporated	O
in	O
the	O
strathclyde	O
version	O
of	O
quadisc	O
.	O
very	O
little	O
extra	O
programming	O
effort	O
is	O
required	O
.	O
however	O
,	O
it	O
is	O
up	O
to	O
the	O
user	O
,	O
by	O
trial	O
and	O
.	O
friedman	O
(	O
1989	O
)	O
gives	O
various	O
shortcut	O
methods	O
for	O
this	O
two-parameter	O
family	O
of	O
procedures	O
is	O
described	O
by	O
friedman	O
(	O
1989	O
)	O
as	O
“	O
regu-	O
larised	O
discriminant	O
analysis	O
”	O
.	O
various	O
simple	O
procedures	O
are	O
included	O
as	O
special	O
cases	O
:	O
the	O
exceptions	O
were	O
those	O
cases	O
where	O
a	O
covariance	O
matrix	O
was	O
not	O
invertible	O
.	O
non-	O
the	O
philosophy	O
being	O
to	O
keep	O
the	O
procedure	O
“	O
pure	O
”	O
quadratic	O
.	O
3.3.3	O
choice	O
of	O
regularisation	O
parameters	O
reducing	O
the	O
amount	O
of	O
computation	O
.	O
``	O
l	O
[	O
&	O
k	O
!	O
!	O
5	O
	O
!	O
	O
!	O
	O
!	O
!	O
	O
!	O
&	O
	O
	O
	O
	O
!	O
	O
&	O
	O
&	O
24	O
classical	O
statistical	B
methods	O
[	O
ch	O
.	O
3	O
approx.	O
)	O
.	O
in	O
practice	O
,	O
great	O
improvements	O
in	O
the	O
performance	O
of	O
quadratic	O
discriminants	O
may	O
result	O
from	O
the	O
use	O
of	O
regularisation	O
,	O
especially	O
in	O
the	O
smaller	O
datasets	O
.	O
3.4	O
logistic	O
discriminant	O
exactly	O
as	O
in	O
section	O
3.2	O
,	O
logistic	O
regression	O
operates	O
by	O
choosing	O
a	O
hyperplane	O
to	O
separate	O
the	O
classes	O
as	O
well	O
as	O
possible	O
,	O
but	O
the	O
criterion	O
for	O
a	O
good	O
separation	O
is	O
changed	O
.	O
fisher	O
’	O
s	O
linear	O
discriminants	O
optimises	O
a	O
quadratic	O
cost	O
function	O
whereas	O
in	O
logistic	O
discrimination	O
it	O
is	O
a	O
conditional	O
likelihood	O
that	O
is	O
maximised	O
.	O
however	O
,	O
in	O
practice	O
,	O
there	O
is	O
often	O
very	O
little	O
difference	O
between	O
the	O
two	O
,	O
and	O
the	O
linear	O
discriminants	O
provide	O
good	O
starting	O
values	O
for	O
the	O
logistic	O
.	O
logistic	O
discrimination	O
is	O
identical	O
,	O
in	O
theory	O
,	O
to	O
linear	O
discrimination	O
for	O
normal	O
distributions	O
with	O
equal	O
covariances	O
,	O
and	O
also	O
for	O
independent	O
binary	O
attributes	O
,	O
so	O
the	O
greatest	O
differences	O
between	O
the	O
two	O
are	O
to	O
be	O
expected	O
when	O
we	O
are	O
far	O
from	O
these	O
two	O
cases	O
,	O
for	O
example	B
when	O
the	O
attributes	O
have	O
very	O
non-normal	O
distributions	O
with	O
very	O
dissimilar	O
covariances	O
.	O
the	O
method	O
is	O
only	O
partially	O
parametric	O
,	O
as	O
the	O
actual	O
pdfs	O
for	O
the	O
classes	O
are	O
not	O
modelled	O
,	O
but	O
rather	O
the	O
ratios	O
between	O
them	O
.	O
likelihood	O
.	O
the	O
model	O
implies	O
that	O
,	O
given	O
attribute	O
values	O
x	O
,	O
the	O
conditional	O
class	O
probabilities	O
for	O
classes	O
times	O
the	O
ratios	O
of	O
the	O
probability	O
density	O
functions	O
for	O
the	O
classes	O
are	O
modelled	O
as	O
linear	O
functions	O
of	O
the	O
attributes	O
.	O
thus	O
,	O
for	O
two	O
classes	O
,	O
are	O
the	O
parameters	O
of	O
the	O
model	O
that	O
are	O
to	O
be	O
estimated	O
.	O
the	O
case	O
of	O
normal	O
distributions	O
with	O
equal	O
covariance	O
is	O
a	O
special	O
case	O
of	O
this	O
,	O
for	O
which	O
the	O
parameters	O
are	O
functions	O
of	O
the	O
prior	O
probabilities	O
,	O
the	O
class	O
means	O
and	O
the	O
common	O
covariance	O
matrix	O
.	O
however	O
the	O
model	O
covers	O
other	O
cases	O
too	O
,	O
such	O
as	O
that	O
where	O
the	O
attributes	O
are	O
independent	O
with	O
values	O
0	O
or	O
1.	O
one	O
of	O
the	O
attractions	O
is	O
that	O
the	O
is	O
speciﬁcally	O
,	O
the	O
logarithms	O
of	O
the	O
prior	O
odds	O
.	O
24ks5	O
log	O
.	O
&	O
l	O
c¡k	O
'	O
''	O
24ks5	O
and	O
the0	O
-dimensional	O
vector	O
where	O
discriminant	O
scale	O
covers	O
all	O
real	O
numbers	O
.	O
a	O
large	O
positive	O
value	O
indicates	O
that	O
class	O
likely	O
,	O
while	O
a	O
large	O
negative	O
value	O
indicates	O
that	O
class	O
in	O
practice	O
the	O
parameters	O
are	O
estimated	O
by	O
maximumf	O
?	O
¢*jsj	O
$	O
i	O
}	O
@	O
$	O
>	O
¢*jspm£	O
and	O
:	O
;	O
24	O
ks5¤	O
&	O
ks5¤	O
&	O
:	O
;	O
24	O
parameters	O
and	O
2	O
>	O
b	O
''	O
q'5x	O
&	O
exp2	O
>	O
	O
	O
exp2	O
>	O
	O
/	O
(	O
'	O
exp2	O
>	O
	O
/	O
(	O
'	O
:	O
;	O
2q	O
§	O
%	O
¨c©	O
sampleª	O
is	O
deﬁned	O
to	O
be	O
ks5	O
ks5	O
ks5	O
ks5	O
and	O
the	O
parameter	O
estimates	O
are	O
the	O
values	O
that	O
maximise	O
this	O
likelihood	O
.	O
they	O
are	O
found	O
by	O
iterative	O
methods	O
,	O
as	O
proposed	O
by	O
cox	O
(	O
1966	O
)	O
and	O
day	O
&	O
kerridge	O
(	O
1967	O
)	O
.	O
logistic	O
models	O
:	O
;	O
24	O
§	O
%	O
¨s	O
«	O
sampleª	O
ks5	O
respectively	O
.	O
given	O
independent	O
samples	O
from	O
the	O
two	O
classes	O
,	O
the	O
conditional	O
likelihood	O
for	O
the	O
d	O
.	O
is	O
likely	O
.	O
take	O
the	O
forms	O
:	O
b	O
.	O
b	O
	O
<	O
¡	O
¡	O
<	O
(	O
¡	O
¥	O
¦	O
<	O
¦	O
<	O
sec	O
.	O
3.4	O
]	O
logistic	O
discrimination	O
25	O
.	O
take	O
the	O
forms	O
:	O
is	O
deﬁned	O
to	O
be	O
belong	O
to	O
the	O
class	O
of	O
generalised	O
linear	O
models	O
(	O
glms	O
)	O
,	O
which	O
generalise	O
the	O
use	O
of	O
linear	O
regression	O
models	O
to	O
deal	O
with	O
non-normal	O
random	O
variables	O
,	O
and	O
in	O
particular	O
to	O
deal	O
with	O
binomial	O
variables	O
.	O
in	O
this	O
context	O
,	O
the	O
binomial	O
variable	O
is	O
an	O
indicator	O
variable	O
that	O
counts	O
again	O
,	O
the	O
parameters	O
are	O
estimated	O
by	O
maximum	O
conditional	O
likelihood	O
.	O
given	O
at-	O
whether	O
an	O
example	B
is	O
class	O
or	O
not	O
.	O
when	O
there	O
are	O
more	O
than	O
two	O
classes	O
,	O
one	O
class	O
is	O
taken	O
as	O
a	O
reference	O
class	O
,	O
and	O
there	O
are	O
,	O
(	O
sets	O
of	O
parameters	O
for	O
the	O
odds	O
of	O
each	O
class	O
relative	O
to	O
the	O
reference	O
class	O
.	O
to	O
discuss	O
this	O
case	O
,	O
we	O
abbreviate	O
the	O
notation	O
forw	O
/	O
.	O
for	O
the	O
remainder	O
of	O
this	O
section	O
,	O
therefore	O
,	O
x	O
is	O
a20	O
¬	O
(	O
+5	O
-dimensional	O
to	O
the	O
simpler	O
corresponds	O
to	O
the	O
constant	O
vector	O
with	O
leading	O
term	O
unity	O
,	O
and	O
the	O
leading	O
term	O
in	O
!	O
,	O
where	O
$	O
t	O
&	O
a	O
,	O
,	O
and	O
the	O
tribute	O
values	O
x	O
,	O
the	O
conditional	O
class	O
probability	O
for	O
class	O
conditional	O
class	O
probability	O
for	O
exp2	O
ks5	O
ks5­	O
&	O
:	O
;	O
24	O
ks5	O
	O
exp2qc¡	O
-®¯¯®	O
ks5­	O
&	O
:	O
;	O
24	O
ks5	O
	O
exp2qc¡	O
-®¯¯®	O
respectively	O
.	O
given	O
independent	O
samples	O
from	O
the	O
,	O
classes	O
,	O
the	O
conditional	O
likelihood	O
for	O
the	O
parametersy	O
!	O
 q q 	O
ks5	O
ks5	O
:	O
`24	O
''	O
4	O
5/	O
&	O
2	O
:	O
;	O
2q	O
«	O
sampleª	O
©	O
sampleª	O
§	O
%	O
¨	O
§	O
%	O
¨	O
	O
)	O
values	O
occur	O
,	O
it	O
will	O
generally	O
be	O
necessary	O
3.1	O
,	O
when	O
categorical	O
attributes	O
with±	O
(	O
(	O
binary	O
attributes	O
before	O
using	O
the	O
algorithm	O
,	O
especially	O
if	O
the	O
to	O
convert	O
them	O
into±	O
categories	O
are	O
not	O
ordered	O
.	O
anderson	O
(	O
1984	O
)	O
points	O
out	O
that	O
it	O
may	O
be	O
appropriate	O
to	O
include	O
transformations	O
or	O
products	O
of	O
the	O
attributes	O
in	O
the	O
linear	O
function	O
,	O
but	O
for	O
large	O
datasets	O
this	O
may	O
involve	O
much	O
computation	O
.	O
see	O
mclachlan	O
(	O
1992	O
)	O
for	O
useful	O
hints	O
.	O
one	O
way	O
to	O
increase	O
complexity	O
of	O
model	O
,	O
without	O
sacriﬁcing	O
intelligibility	O
,	O
is	O
to	O
add	O
parameters	O
in	O
a	O
hierarchical	O
fashion	O
,	O
and	O
there	O
are	O
then	O
links	O
with	O
graphical	O
models	O
and	O
polytrees	O
.	O
in	O
the	O
basic	O
form	O
of	O
the	O
algorithm	O
an	O
example	B
is	O
assigned	O
to	O
the	O
class	O
for	O
which	O
the	O
posterior	O
is	O
greatest	O
if	O
that	O
is	O
greater	O
than	O
0	O
,	O
or	O
to	O
the	O
reference	O
class	O
if	O
all	O
posteriors	O
are	O
negative	O
.	O
more	O
complicated	O
models	O
can	O
be	O
accommodated	O
by	O
adding	O
transformations	O
of	O
the	O
given	O
attributes	O
,	O
for	O
example	B
products	O
of	O
pairs	O
of	O
attributes	O
.	O
as	O
mentioned	O
in	O
section	O
once	O
again	O
,	O
the	O
parameter	O
estimates	O
are	O
the	O
values	O
that	O
maximise	O
this	O
likelihood	O
.	O
:	O
;	O
24	O
ks5	O
 + * 	O
§	O
%	O
¨s°	O
sampleª	O
3.4.1	O
logistic	O
discriminant	O
-	O
programming	O
details	O
most	O
statistics	O
packages	O
can	O
deal	O
with	O
linear	O
discriminant	O
analysis	O
for	O
two	O
classes	O
.	O
systat	O
has	O
,	O
in	O
addition	O
,	O
a	O
version	O
of	O
logistic	O
regression	O
capable	O
of	O
handling	O
problems	O
with	O
more	O
than	O
two	O
classes	O
.	O
if	O
a	O
package	O
has	O
only	O
binary	O
logistic	O
regression	O
(	O
i.e	O
.	O
can	O
only	O
deal	O
with	O
two	O
classes	O
)	O
,	O
begg	O
&	O
gray	O
(	O
1984	O
)	O
suggest	O
an	O
approximate	O
procedure	O
whereby	O
classes	O
are	O
all	O
compared	O
to	O
a	O
reference	O
class	O
by	O
means	O
of	O
logistic	O
regressions	O
,	O
and	O
the	O
results	O
then	O
combined	O
.	O
the	O
approximation	O
is	O
fairly	O
good	O
in	O
practice	O
according	O
to	O
begg	O
&	O
gray	O
(	O
1984	O
)	O
.	O
	O
¡	O
k	O
¡	O
k	O
	O
!	O
<	O
¡	O
!	O
n	O
a	O
]	O
a	O
<	O
(	O
n	O
a	O
]	O
a	O
¥	O
''	O
	O
	O
¦	O
<	O
¦	O
<	O
¦	O
	O
<	O
	O
26	O
classical	O
statistical	B
methods	O
[	O
ch	O
.	O
3	O
many	O
statistical	B
packages	O
(	O
glim	O
,	O
splus	O
,	O
genstat	O
)	O
now	O
include	O
a	O
generalised	O
linear	O
model	O
(	O
glm	O
)	O
function	O
,	O
enabling	O
logistic	O
regression	O
to	O
be	O
programmed	O
easily	O
,	O
in	O
two	O
or	O
three	O
lines	O
of	O
code	O
.	O
occurrences	O
.	O
the	O
indicator	O
variable	O
is	O
then	O
declared	O
to	O
be	O
a	O
“	O
binomial	O
”	O
variable	O
with	O
the	O
“	O
logit	O
”	O
link	O
function	O
,	O
and	O
generalised	O
regression	O
performed	O
on	O
the	O
attributes	O
.	O
we	O
used	O
the	O
package	O
splus	O
for	O
this	O
purpose	O
.	O
this	O
is	O
ﬁne	O
for	O
two	O
classes	O
,	O
and	O
has	O
the	O
merit	O
of	O
requiring	O
little	O
extra	O
programming	O
effort	O
.	O
for	O
more	O
than	O
two	O
classes	O
,	O
the	O
complexity	O
of	O
the	O
problem	O
increases	O
substantially	O
,	O
and	O
,	O
although	O
it	O
is	O
technically	O
still	O
possible	O
to	O
use	O
glm	O
procedures	O
,	O
the	O
programming	O
effort	O
is	O
substantially	O
greater	O
and	O
much	O
less	O
efﬁcient	O
.	O
the	O
procedure	O
is	O
to	O
deﬁne	O
an	O
indicator	O
variable	O
for	O
class	O
numbers	O
in	O
the	O
various	O
classes	O
:	O
i.e	O
.	O
the	O
maximum	O
likelihood	O
solution	O
can	O
be	O
found	O
via	O
a	O
newton-raphson	O
iterative	O
pro-	O
cedure	O
,	O
as	O
it	O
is	O
quite	O
easy	O
to	O
write	O
down	O
the	O
necessary	O
derivatives	O
of	O
the	O
likelihood	O
(	O
or	O
,	O
ﬁrst	O
iteration	O
.	O
of	O
course	O
,	O
an	O
alternative	O
would	O
be	O
to	O
use	O
the	O
linear	O
discriminant	O
parameters	O
as	O
starting	O
values	O
.	O
in	O
subsequent	O
iterations	O
,	O
the	O
step	O
size	O
may	O
occasionally	O
have	O
to	O
be	O
reduced	O
,	O
but	O
usually	O
the	O
procedure	O
converges	O
in	O
about	O
10	O
iterations	O
.	O
this	O
is	O
the	O
procedure	O
we	O
adopted	O
where	O
possible	O
.	O
!	O
coefﬁ-	O
equivalently	O
,	O
the	O
log-likelihood	O
)	O
.	O
the	O
simplest	O
starting	O
procedure	O
is	O
to	O
set	O
the	O
!	O
)	O
which	O
are	O
set	O
to	O
the	O
logarithms	O
of	O
the	O
cients	O
to	O
zero	O
except	O
for	O
the	O
leading	O
coefﬁcients	O
(	O
	O
!	O
,	O
wherej	O
is	O
the	O
number	O
of	O
class	O
logj	O
!	O
are	O
those	O
of	O
the	O
linear	O
discriminant	O
after	O
the	O
examples	O
.	O
this	O
ensures	O
that	O
the	O
values	O
of	O
2w	O
,	O
(	O
*5-2q0²	O
e	O
(	O
+5	O
rows	O
,	O
and	O
each	O
term	O
requires	O
a	O
summation	O
over	O
all	O
the	O
observations	O
in	O
the	O
thus	O
there	O
are	O
of	O
order	O
,	O
w0y	O
+	O
m	O
,	O
	O
,0e	O
&	O
in	O
the	O
kl	O
digits	O
dataset	O
(	O
see	O
section	O
9.3.2	O
)	O
,	O
for	O
example	B
,	O
,	O
&	O
³	O
(	O
	O
,	O
so	O
the	O
number	O
of	O
operations	O
is	O
of	O
order	O
(	O
ando	O
&	O
m´	O
in	O
each	O
iteration	O
.	O
in	O
such	O
cases	O
,	O
it	O
is	O
preferable	O
to	O
use	O
a	O
purely	O
numerical	O
search	O
procedure	O
,	O
or	O
,	O
as	O
we	O
did	O
when	O
the	O
newton-raphson	O
procedure	O
was	O
too	O
time-consuming	O
,	O
to	O
use	O
a	O
method	O
based	O
on	O
an	O
approximate	O
hessian	O
.	O
the	O
approximation	O
uses	O
the	O
fact	O
that	O
the	O
hessian	O
for	O
the	O
zero	O
’	O
th	O
order	O
iteration	O
is	O
simply	O
a	O
replicate	O
of	O
the	O
design	O
matrix	O
(	O
cf	O
.	O
covariance	O
matrix	O
)	O
used	O
by	O
the	O
linear	O
discriminant	O
rule	O
.	O
this	O
zero-order	O
hessian	O
is	O
used	O
for	O
all	O
iterations	O
.	O
in	O
situations	O
where	O
there	O
is	O
little	O
difference	O
between	O
the	O
linear	O
and	O
logistic	O
parameters	O
,	O
the	O
approximation	O
is	O
very	O
good	O
and	O
convergence	O
is	O
fairly	O
fast	O
(	O
although	O
a	O
few	O
more	O
iterations	O
are	O
generally	O
required	O
)	O
.	O
however	O
,	O
in	O
the	O
more	O
interesting	O
case	O
that	O
the	O
linear	O
and	O
logistic	O
parameters	O
are	O
very	O
different	O
,	O
convergence	O
using	O
this	O
procedure	O
is	O
very	O
slow	O
,	O
and	O
it	O
may	O
still	O
be	O
quite	O
far	O
from	O
convergence	O
after	O
,	O
say	O
,	O
100	O
iterations	O
.	O
we	O
generally	O
stopped	O
after	O
50	O
iterations	O
:	O
although	O
the	O
parameter	O
values	O
were	O
generally	O
not	O
stable	O
,	O
the	O
predicted	O
classes	O
for	O
the	O
data	O
were	O
reasonably	O
stable	O
,	O
so	O
the	O
predictive	O
power	O
of	O
the	O
resulting	O
rule	O
may	O
not	O
be	O
seriously	O
affected	O
.	O
this	O
aspect	O
of	O
logistic	O
regression	O
has	O
not	O
been	O
explored	O
.	O
however	O
,	O
each	O
iteration	O
requires	O
a	O
separate	O
calculation	O
of	O
the	O
hessian	O
,	O
and	O
it	O
is	O
here	O
that	O
the	O
bulk	O
of	O
the	O
computational	O
work	O
is	O
required	O
.	O
the	O
hessian	O
is	O
a	O
square	O
matrix	O
with	O
whole	O
dataset	O
(	O
although	O
some	O
saving	O
can	O
by	O
achieved	O
using	O
the	O
symmetries	O
of	O
the	O
hessian	O
)	O
.	O
computations	O
required	O
to	O
ﬁnd	O
the	O
hessian	O
matrix	O
at	O
each	O
iteration	O
.	O
	O
µ	O
the	O
ﬁnal	O
program	O
used	O
for	O
the	O
trials	O
reported	O
in	O
this	O
book	O
was	O
coded	O
in	O
fortran	O
,	O
since	O
the	O
splus	O
procedure	O
had	O
prohibitive	O
memory	O
requirements	O
.	O
availablility	O
of	O
the	O
fortran	O
code	O
can	O
be	O
found	O
in	O
appendix	O
b	O
.	O
	O
!	O
&	O
!	O
!	O
	O
linear	O
discrimination	O
has	O
the	O
equation	O
sec	O
.	O
3.6	O
]	O
bayes	O
’	O
rules	O
27	O
3.5	O
bayes	O
’	O
rules	O
methods	O
based	O
on	O
likelihood	O
ratios	O
can	O
be	O
adapted	O
to	O
cover	O
the	O
case	O
of	O
unequal	O
mis-	O
classiﬁcation	B
costs	O
and/or	O
unequal	O
prior	O
probabilities	O
.	O
let	O
the	O
prior	O
probabilities	O
be	O
when	O
there	O
are	O
more	O
than	O
two	O
classes	O
,	O
the	O
simplest	O
procedure	O
is	O
to	O
calculate	O
the	O
as	O
in	O
section	O
2.6	O
,	O
the	O
minimum	O
expected	O
cost	O
solution	O
is	O
to	O
assign	O
the	O
data	O
x	O
to	O
class	O
 * + 	O
''	O
l	O
,	O
	O
,	O
and	O
letf*2q	O
$	O
l	O
''	O
6g5	O
denote	O
the	O
cost	O
incurred	O
by	O
classifying	O
an	O
example	B
:	O
$	O
w¶	O
)	O
(	O
	O
''	O
h	O
.	O
a	O
.	O
into	O
class	O
of	O
class	O
i	O
chosen	O
to	O
minimise	O
{	O
5	O
.	O
in	O
the	O
case	O
of	O
two	O
classes	O
the	O
hyperplane	O
in	O
f*24	O
$	O
-	O
''	O
-jp5	O
%	O
b_2qk	O
	O
x	O
k_¡f	O
5c	O
&	O
ks5	O
and	O
associated	O
expected	O
costs	O
explicitly	O
,	O
using	O
the	O
formulae	O
of	O
class	O
probabilities	O
:	O
`249	O
!	O
3.6	O
example	B
as	O
illustration	O
of	O
the	O
differences	O
between	O
the	O
linear	O
,	O
quadratic	O
and	O
logistic	O
discriminants	O
,	O
we	O
consider	O
a	O
subset	O
of	O
the	O
karhunen-loeve	O
version	O
of	O
the	O
digits	O
data	O
later	O
studied	O
in	O
this	O
book	O
.	O
for	O
simplicity	O
,	O
we	O
consider	O
only	O
the	O
digits	O
‘	O
1	O
’	O
and	O
‘	O
2	O
’	O
,	O
and	O
to	O
differentiate	O
between	O
them	O
we	O
use	O
only	O
the	O
ﬁrst	O
two	O
attributes	O
(	O
40	O
are	O
available	O
,	O
so	O
this	O
is	O
a	O
substantial	O
reduction	O
in	O
potential	O
information	O
)	O
.	O
the	O
full	O
sample	O
of	O
900	O
points	O
for	O
each	O
digit	O
was	O
used	O
to	O
estimate	O
the	O
parameters	O
of	O
the	O
discriminants	O
,	O
although	O
only	O
a	O
subset	O
of	O
200	O
points	O
for	O
each	O
digit	O
is	O
plotted	O
in	O
figure	O
3.1	O
as	O
much	O
of	O
the	O
detail	O
is	O
obscured	O
when	O
the	O
full	O
set	O
is	O
plotted	O
.	O
log·	O
f*2	O
''	O
+	O
(	O
*5	O
f*2l	O
(	O
''	O
5	O
¸	O
the	O
right	O
hand	O
side	O
replacing	O
0	O
that	O
we	O
had	O
in	O
equation	O
(	O
3.2	O
)	O
.	O
section	O
2.6	O
.	O
5w¡q2	O
3.6.1	O
linear	O
discriminant	O
also	O
shown	O
in	O
figure	O
3.1	O
are	O
the	O
sample	O
centres	O
of	O
gravity	O
(	O
marked	O
by	O
a	O
cross	O
)	O
.	O
because	O
there	O
are	O
equal	O
numbers	O
in	O
the	O
samples	O
,	O
the	O
linear	O
discriminant	O
boundary	O
(	O
shown	O
on	O
the	O
diagram	O
by	O
a	O
full	O
line	O
)	O
intersects	O
the	O
line	O
joining	O
the	O
centres	O
of	O
gravity	O
at	O
its	O
mid-point	O
.	O
any	O
new	O
point	O
is	O
classiﬁed	O
as	O
a	O
‘	O
1	O
’	O
if	O
it	O
lies	O
below	O
the	O
line	O
i.e	O
.	O
is	O
on	O
the	O
same	O
side	O
as	O
the	O
centre	O
of	O
the	O
‘	O
1	O
’	O
s	O
)	O
.	O
in	O
the	O
diagram	O
,	O
there	O
are	O
18	O
‘	O
2	O
’	O
s	O
below	O
the	O
line	O
,	O
so	O
they	O
would	O
be	O
misclassiﬁed	O
.	O
3.6.2	O
logistic	O
discriminant	O
the	O
logistic	O
discriminant	O
procedure	O
usually	O
starts	O
with	O
the	O
linear	O
discriminant	O
line	O
and	O
then	O
adjusts	O
the	O
slope	O
and	O
intersect	O
to	O
maximise	O
the	O
conditional	O
likelihood	O
,	O
arriving	O
at	O
the	O
dashed	O
line	O
of	O
the	O
diagram	O
.	O
essentially	O
,	O
the	O
line	O
is	O
shifted	O
towards	O
the	O
centre	O
of	O
the	O
‘	O
1	O
’	O
s	O
so	O
as	O
to	O
reduce	O
the	O
number	O
of	O
misclassiﬁed	O
‘	O
2	O
’	O
s	O
.	O
this	O
gives	O
7	O
fewer	O
misclassiﬁed	O
‘	O
2	O
’	O
s	O
(	O
but	O
2	O
more	O
misclassiﬁed	O
‘	O
1	O
’	O
s	O
)	O
in	O
the	O
diagram	O
.	O
3.6.3	O
quadratic	O
discriminant	O
the	O
quadratic	O
discriminant	O
starts	O
by	O
constructing	O
,	O
for	O
each	O
sample	O
,	O
an	O
ellipse	O
centred	O
on	O
the	O
centre	O
of	O
gravity	O
of	O
the	O
points	O
.	O
in	O
figure	O
3.1	O
it	O
is	O
clear	O
that	O
the	O
distributions	O
are	O
of	O
different	O
shape	O
and	O
spread	O
,	O
with	O
the	O
distribution	O
of	O
‘	O
2	O
’	O
s	O
being	O
roughly	O
circular	O
in	O
shape	O
and	O
the	O
‘	O
1	O
’	O
s	O
being	O
more	O
elliptical	O
.	O
the	O
line	O
of	O
equal	O
likelihood	O
is	O
now	O
itself	O
an	O
ellipse	O
(	O
in	O
general	O
a	O
conic	O
section	O
)	O
as	O
shown	O
in	O
the	O
figure	O
.	O
all	O
points	O
within	O
the	O
ellipse	O
are	O
classiﬁed	O
!	O
!	O
	O
!	O
.	O
!	O
<	O
	O
!	O
2	O
	O
5	O
	O
(	O
	O
2	O
	O
	O
	O
.	O
.	O
	O
	O
''	O
<	O
28	O
classical	O
statistical	B
methods	O
[	O
ch	O
.	O
3	O
as	O
‘	O
1	O
’	O
s	O
.	O
relative	O
to	O
the	O
logistic	O
boundary	O
,	O
i.e	O
.	O
in	O
the	O
region	O
between	O
the	O
dashed	O
line	O
and	O
the	O
ellipse	O
,	O
the	O
quadratic	O
rule	O
misclassiﬁes	O
an	O
extra	O
7	O
‘	O
1	O
’	O
s	O
(	O
in	O
the	O
upper	O
half	O
of	O
the	O
diagram	O
)	O
but	O
correctly	O
classiﬁes	O
an	O
extra	O
8	O
‘	O
2	O
’	O
s	O
(	O
in	O
the	O
lower	O
half	O
of	O
the	O
diagram	O
)	O
.	O
so	O
the	O
performance	O
of	O
the	O
quadratic	O
classiﬁer	B
is	O
about	O
the	O
same	O
as	O
the	O
logistic	O
discriminant	O
in	O
this	O
case	O
,	O
probably	O
due	O
to	O
the	O
skewness	O
of	O
the	O
‘	O
1	O
’	O
distribution	O
.	O
linear	O
,	O
logistic	O
and	O
quadratic	O
discriminants	O
1	O
1	O
2	O
1	O
1	O
1	O
2	O
1	O
1	O
2	O
1	O
1	O
1	O
1	O
2	O
1	O
1	O
1	O
2	O
2	O
2	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
11	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
2	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
11	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
11	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
2	O
2	O
2	O
1	O
1	O
1	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
1	O
2	O
2	O
2	O
2	O
2	O
2	O
1	O
2	O
2	O
2	O
2	O
2	O
1	O
2	O
2	O
2	O
2	O
2	O
1	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
1	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
22	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
1	O
2	O
2	O
2	O
2	O
2	O
2	O
1	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
1	O
2	O
2	O
2	O
2	O
2	O
2	O
1	O
linear	O
logistic	O
2	O
150	O
200	O
1	O
1	O
0	O
5	O
1	O
1	O
e	O
t	O
a	O
i	O
r	O
a	O
v	O
-	O
l	O
k	O
d	O
n	O
2	O
0	O
0	O
1	O
0	O
5	O
quadratic	O
50	O
100	O
1st	O
kl-variate	O
fig	O
.	O
3.1	O
:	O
decision	O
boundaries	O
for	O
the	O
three	O
discriminants	O
:	O
quadratic	O
(	O
curved	O
)	O
;	O
linear	O
(	O
full	O
line	O
)	O
;	O
and	O
logistic	O
(	O
dashed	O
line	O
)	O
.	O
the	O
data	O
are	O
the	O
ﬁrst	O
two	O
karhunen-loeve	O
components	O
for	O
the	O
digits	O
‘	O
1	O
’	O
and	O
‘	O
2	O
’	O
.	O
¹	O
4	O
modern	O
statistical	B
techniques	O
r.	O
molina	O
(	O
1	O
)	O
,	O
n.	O
p´erez	O
de	O
la	O
blanca	O
(	O
1	O
)	O
and	O
c.	O
c.	O
taylor	O
(	O
2	O
)	O
(	O
1	O
)	O
university	O
of	O
granada	O
and	O
(	O
2	O
)	O
university	O
of	O
leeds	O
4.1	O
introduction	O
!	O
l	O
''	O
!	O
l	O
''	O
 + * 	O
.	O
in	O
the	O
previous	O
chapter	O
we	O
studied	O
the	O
classiﬁcation	B
problem	O
,	O
from	O
the	O
statistical	B
point	O
of	O
view	O
,	O
assuming	O
that	O
the	O
form	O
of	O
the	O
underlying	O
density	O
functions	O
(	O
or	O
their	O
ratio	O
)	O
was	O
known	O
.	O
however	O
,	O
in	O
most	O
real	O
problems	O
this	O
assumption	O
does	O
not	O
necessarily	O
hold	O
.	O
in	O
this	O
chapter	O
we	O
examine	O
distribution-free	O
(	O
often	O
called	O
nonparametric	O
)	O
classiﬁcation	B
procedures	O
that	O
can	O
be	O
used	O
without	O
assuming	O
that	O
the	O
form	O
of	O
the	O
underlying	O
densities	O
are	O
known	O
.	O
the	O
bayesian	O
approach	O
for	O
allocating	O
observations	O
to	O
classes	O
has	O
already	O
been	O
outlined	O
in	O
section	O
2.6.	O
it	O
is	O
clear	O
that	O
to	O
apply	O
the	O
bayesian	O
approach	O
to	O
classiﬁcation	B
we	O
have	O
recall	O
that	O
,	O
m	O
''	O
%	O
j	O
'	O
''	O
40	O
denote	O
the	O
number	O
of	O
classes	O
,	O
of	O
examples	O
and	O
attributes	O
,	O
respec-	O
 * + 	O
tively	O
.	O
classes	O
will	O
be	O
denoted	O
by	O
''	O
%	O
	O
and	O
attribute	O
values	O
for	O
example	B
$	O
''	O
%	O
	O
	O
n	O
 + * 	O
2q	O
$	O
=	O
&	O
(	O
	O
''	O
!	O
w5b¶	O
»	O
	O
n	O
 + * 	O
elements	O
in	O
»	O
will	O
be	O
denotedk/	O
&	O
|2	O
5	O
.	O
a	O
or0_2q	O
ks5	O
.	O
nonparametric	O
methods	O
to	O
do	O
this	O
job	O
will	O
be	O
to	O
estimateb_2qk	O
''	O
%	O
js5	O
will	O
be	O
denoted	O
by	O
the0	O
-dimensional	O
vectorky	O
!	O
º	O
&	O
|2	O
5	O
and	O
.	O
discussed	O
in	O
this	O
chapter	O
.	O
we	O
begin	O
in	O
section	O
4.2	O
with	O
kernel	O
density	O
estimation	O
;	O
a	O
close	O
relative	O
to	O
this	O
approach	O
is	O
the	O
k-nearest	O
neighbour	O
(	O
k-nn	O
)	O
which	O
is	O
outlined	O
in	O
section	O
4.3.	O
bayesian	O
methods	O
which	O
either	O
allow	O
for	O
,	O
or	O
prohibit	O
dependence	O
between	O
the	O
variables	O
are	O
discussed	O
in	O
sections	O
4.5	O
and	O
4.6.	O
a	O
ﬁnal	O
section	O
deals	O
with	O
promising	O
methods	O
which	O
have	O
been	O
developed	O
recently	O
,	O
but	O
,	O
for	O
various	O
reasons	O
,	O
must	O
be	O
regarded	O
as	O
methods	O
for	O
the	O
future	O
.	O
to	O
a	O
greater	O
or	O
lesser	O
extent	O
,	O
these	O
methods	O
have	O
been	O
tried	O
out	O
in	O
the	O
project	O
,	O
but	O
the	O
results	O
were	O
disappointing	O
.	O
in	O
some	O
cases	O
(	O
ace	O
)	O
,	O
this	O
is	O
due	O
to	O
limitations	O
of	O
size	O
and	O
memory	O
as	O
implemented	O
in	O
splus	O
.	O
the	O
pruned	O
implementation	O
of	O
mars	O
in	O
splus	O
(	O
statsci	O
,	O
1991	O
)	O
also	O
suffered	O
in	O
a	O
similar	O
way	O
,	O
but	O
a	O
standalone	O
version	O
which	O
also	O
does	O
classiﬁcation	B
is	O
expected	O
shortly	O
.	O
we	O
believe	O
that	O
these	O
methods	O
will	O
have	O
a	O
place	O
in	O
classiﬁcation	B
practice	O
,	O
once	O
some	O
relatively	O
minor	O
technical	B
problems	O
have	O
been	O
resolved	O
.	O
as	O
yet	O
,	O
however	O
,	O
we	O
can	O
not	O
recommend	O
them	O
on	O
the	O
basis	O
of	O
our	O
empirical	O
trials	O
.	O
address	O
for	O
correspondence	O
:	O
department	O
of	O
computer	O
science	O
and	O
ai	O
,	O
facultad	O
de	O
ciencas	O
,	O
university	O
of	O
granada	O
,	O
18071	O
granada	O
,	O
spain	O
''	O
	O
''	O
	O
	O
''	O
	O
''	O
	O
''	O
''	O
<	O
	O
a	O
a	O
<	O
30	O
modern	O
statistical	B
techniques	O
[	O
ch	O
.	O
4	O
is	O
given	O
by	O
.	O
furthermore	O
,	O
,	O
then	O
is	O
the	O
length	O
of	O
the	O
edge	O
of	O
the	O
hypercube	O
we	O
have	O
.	O
this	O
leads	O
to	O
the	O
following	O
procedure	O
to	O
estimate	O
4.2	O
density	O
estimation	O
a	O
nonparametric	O
approach	O
,	O
proposed	O
in	O
fix	O
&	O
hodges	O
(	O
1951	O
)	O
,	O
is	O
to	O
estimate	O
the	O
densities	O
 + * 	O
b-am2qks5	O
?	O
``	O
6g¼	O
&	O
(	O
	O
''	O
''	O
-	O
,	O
by	O
nonparametric	O
density	O
estimation	O
.	O
then	O
once	O
we	O
have	O
estimated	O
b-am2qks5	O
and	O
the	O
prior	O
probabilities.pa	O
we	O
can	O
use	O
the	O
formulae	O
of	O
section	O
2.6	O
and	O
the	O
costs	O
to	O
classifyk	O
by	O
minimum	O
risk	O
or	O
minimum	O
error	O
.	O
	O
dimensional	O
density	O
to	O
introduce	O
the	O
method	O
,	O
we	O
assume	O
that	O
we	O
have	O
to	O
estimate	O
the0	O
functionb324ks5	O
of	O
an	O
unknown	O
distribution	O
.	O
note	O
that	O
we	O
will	O
have	O
to	O
perform	O
this	O
process	O
 * + 	O
for	O
each	O
of	O
the	O
,	O
densitiesb	O
,	O
that	O
a	O
vectork	O
2qks5	O
?	O
``	O
6g²	O
&	O
	O
(	O
''	O
''	O
l	O
,	O
.	O
then	O
,	O
the	O
probability	O
,	O
:	O
will	O
fall	O
in	O
a	O
region½	O
:	O
c	O
&	O
e¾m¿xb324k_¡q5	O
@	O
j	O
k_¡	O
suppose	O
thatj	O
observations	O
are	O
drawn	O
independently	O
according	O
tob324ks5	O
.	O
then	O
we	O
can	O
is	O
the	O
number	O
of	O
thesej	O
observations	O
falling	O
in½	O
approach	O
:	O
by8dhj	O
where8	O
ifb_2qks5	O
does	O
not	O
vary	O
appreciably	O
within½	O
we	O
can	O
write	O
:	O
càeb324ks5	O
@	O
á	O
whereá	O
is	O
the	O
volume	O
enclosed	O
by½	O
,8hâ	O
be	O
the	O
number	O
of	O
samples	O
falling	O
in½wâ	O
the	O
density	O
atk	O
.	O
letáãâ	O
be	O
the	O
volume	O
of½wâ	O
the	O
estimate	O
ofb324ks5	O
based	O
on	O
a	O
sample	O
of	O
sizej	O
andä	O
b2qks5	O
8hâåd	O
j	O
b24ks5	O
'	O
&	O
ámâ	O
	O
dimensional	O
is	O
a0	O
equation	O
(	O
4.1	O
)	O
can	O
be	O
written	O
in	O
a	O
much	O
more	O
suggestive	O
way	O
.	O
if½wâ	O
hypercube	O
and	O
ifmâ	O
ky	O
!	O
b24ks5	O
'	O
&	O
mâ	O
ámâçæ|è	O
<	O
î	O
9 * + 	O
(	O
+d	O
24ë_5	O
'	O
&	O
íì	O
then	O
(	O
4.2	O
)	O
expresses	O
our	O
estimate	O
forb324ks5	O
as	O
an	O
average	O
function	O
ofk	O
and	O
the	O
samplesky	O
!	O
.	O
b24ks5	O
'	O
&	O
24k	O
'	O
''	O
%	O
ky	O
!	O
#	O
''	O
-	O
where	O
''	O
-ãâd5	O
are	O
kernel	O
functions	O
.	O
for	O
instance	O
,	O
we	O
could	O
use	O
,	O
instead	O
of	O
the	O
parzen	O
24k	O
'	O
''	O
%	O
k	O
n	O
expð	O
24k	O
'	O
''	O
%	O
ky	O
!	O
?	O
``	O
l	O
5c	O
&	O
2wï	O
.s	O
is	O
clear	O
.	O
for	O
(	O
4.3	O
)	O
,	O
ifãâ	O
the	O
role	O
played	O
bymâ	O
''	O
-mâå5	O
changes	O
very	O
slowly	O
withk	O
,	O
resulting	O
in	O
a	O
very	O
smooth	O
estimate	O
forb324ks5	O
.	O
on	O
the	O
other	O
hand	O
,	O
ifãâ	O
b2qks5	O
is	O
the	O
superposition	O
ofj	O
small	O
thenä	O
centered	O
at	O
the	O
samples	O
producing	O
a	O
very	O
erratic	O
estimate	O
ofb_2qks5	O
.	O
the	O
analysis	O
for	O
the	O
a	O
#	O
!	O
is	O
very	O
large	O
is	O
sharp	O
normal	O
distributions	O
with	O
small	O
variances	O
g¼	O
&	O
(	O
	O
''	O
''	O
40	O
(	O
4.1	O
)	O
(	O
4.2	O
)	O
(	O
4.3	O
)	O
where	O
otherwise	O
in	O
general	O
we	O
could	O
use	O
window	O
deﬁned	O
above	O
,	O
ó	O
24k	O
'	O
''	O
%	O
k	O
parzen	O
window	O
is	O
similar	O
.	O
	O
''	O
a	O
	O
''	O
ä	O
ä	O
(	O
j	O
â	O
n	O
!	O
]	O
(	O
k	O
	O
é	O
ê	O
(	O
<	O
ë	O
a	O
	O
	O
ä	O
(	O
j	O
â	O
n	O
!	O
]	O
	O
â	O
5	O
!	O
	O
â	O
(	O
	O
â	O
5	O
ñ	O
ò	O
	O
(	O
	O
n	O
a	O
è	O
	O
a	O
	O
	O
	O
â	O
é	O
ô	O
õ	O
!	O
sec	O
.	O
4.2	O
]	O
density	O
estimation	O
31	O
further	O
details	O
.	O
independent	O
coordinates	O
,	O
i.e	O
.	O
is	O
an	O
averaged	O
value	O
of	O
the	O
unknown	O
density	O
.	O
for	O
the	O
mean	O
and	O
variance	O
of	O
the	O
estimator	O
.	O
these	O
can	O
be	O
used	O
to	O
derive	O
plug-in	O
estimates	O
before	O
going	O
into	O
details	O
about	O
the	O
kernel	O
functions	O
we	O
use	O
in	O
the	O
classiﬁcation	B
problem	O
,	O
we	O
brieﬂy	O
comment	O
on	O
the	O
mean	O
we	O
now	O
consider	O
our	O
classiﬁcation	B
problem	O
.	O
two	O
choices	O
have	O
to	O
be	O
made	O
in	O
order	O
to	O
estimate	O
the	O
density	O
,	O
the	O
speciﬁcation	O
of	O
the	O
kernel	O
and	O
the	O
value	O
of	O
the	O
smoothing	O
parameter	O
.	O
it	O
is	O
fairly	O
widely	O
recognised	O
that	O
the	O
choice	O
of	O
the	O
smoothing	O
parameter	O
is	O
much	O
more	O
important	O
.	O
with	O
regard	O
to	O
the	O
kernel	O
function	O
we	O
will	O
restrict	O
our	O
attention	O
to	O
and	O
about	O
the	O
estimation	O
of	O
the	O
smoothing	O
parameterãâ	O
b_2qks5	O
.	O
we	O
have	O
behaviour	O
ofä	O
b24ks5	O
>	O
º	O
&	O
c¾	O
2qk	O
'	O
''	O
@	O
×9	O
''	O
-mâ5	O
@	O
b324×'5	O
@	O
j	O
×	O
b24ks5	O
and	O
so	O
the	O
expected	O
value	O
of	O
the	O
estimateä	O
in	O
a	O
taylor	O
series	O
(	O
inmâ	O
)	O
about	O
x	O
one	O
can	O
derive	O
asymptotic	O
formulae	O
b2qks5	O
by	O
expandingä	O
forãâ	O
which	O
are	O
well-suited	O
to	O
the	O
goal	O
of	O
density	O
estimation	O
,	O
see	O
silverman	O
(	O
1986	O
)	O
for	O
kernels	O
with0	O
24k	O
'	O
''	O
%	O
k	O
''	O
ly5	O
'	O
&	O
withwø	O
indicating	O
the	O
kernel	O
function	O
component	O
of	O
theg	O
th	O
attribute	O
and	O
being	O
not	O
a+ù	O
dependent	O
ong	O
.	O
it	O
is	O
very	O
important	O
to	O
note	O
that	O
as	O
stressed	O
by	O
aitchison	O
&	O
aitken	O
(	O
1976	O
)	O
,	O
wø	O
it	O
is	O
clear	O
that	O
kernels	O
could	O
have	O
a	O
more	O
complex	O
form	O
and	O
that	O
the	O
smoothing	O
parameter	O
could	O
be	O
coordinate	O
dependent	O
.	O
we	O
will	O
not	O
discuss	O
in	O
detail	O
that	O
possibility	O
here	O
(	O
see	O
mclachlan	O
,	O
1992	O
for	O
details	O
)	O
.	O
some	O
comments	O
will	O
be	O
made	O
at	O
the	O
end	O
of	O
this	O
section	O
.	O
this	O
factorisation	O
does	O
not	O
imply	O
the	O
independence	O
of	O
the	O
attributes	O
for	O
the	O
density	O
we	O
are	O
estimating	O
.	O
the	O
kernels	O
we	O
use	O
depend	O
on	O
the	O
type	O
of	O
variable	O
.	O
for	O
continuous	O
variables	O
a-	O
!	O
``	O
-å5	O
wø	O
a*ù	O
ad	O
''	O
for	O
binary	O
variables	O
a-	O
!	O
@	O
''	O
-å5¤	O
&	O
a-	O
!	O
``	O
-å5¤	O
&	O
a-	O
!	O
``	O
-å5¤	O
&	O
log	O
expð	O
a+ù	O
.sd	O
	O
(	O
ü*ý	O
log	O
.sd	O
ý4þ	O
a+ù	O
(	O
'	O
gé	O
ýqþ	O
(	O
'	O
	O
for	O
nominal	O
variables	O
withßma	O
nominal	O
values	O
a+ù	O
(	O
'	O
h2qß	O
(	O
+5	O
%	O
'é9à	O
(	O
+5	O
@	O
	O
(	O
'	O
l2qß	O
ú	O
ü*ý4þ	O
)	O
«	O
(	O
'	O
hgé	O
ý4þ	O
ü*ý4þ	O
ü+ý	O
a-	O
!	O
log'û	O
(	O
+d	O
ý4þ	O
(	O
'	O
¬24ß	O
(	O
+5	O
@	O
gé	O
ýqþ	O
ö	O
	O
ä	O
	O
	O
!	O
n	O
¦	O
a	O
]	O
2	O
	O
a	O
''	O
	O
2	O
	O
	O
(	O
	O
	O
ñ	O
ò	O
	O
a	O
	O
	O
	O
	O
ó	O
ô	O
õ	O
&	O
(	O
	O
	O
	O
	O
ø	O
2	O
	O
a	O
''	O
	O
è	O
	O
ø	O
ü	O
ý	O
	O
ü	O
ù	O
«	O
è	O
(	O
	O
ø	O
ü	O
ý	O
	O
ü	O
ù	O
«	O
&	O
(	O
	O
ø	O
ü	O
ý	O
	O
ü	O
ù	O
«	O
	O
ø	O
2	O
	O
a	O
''	O
	O
è	O
(	O
a	O
	O
ø	O
ü	O
ý	O
®	O
ü	O
ù	O
è	O
	O
a	O
	O
	O
à	O
ø	O
ü	O
ý	O
®	O
ü	O
ù	O
&	O
(	O
a	O
	O
	O
	O
à	O
ø	O
®	O
ù	O
32	O
modern	O
statistical	B
techniques	O
[	O
ch	O
.	O
4	O
continuous	O
binary	O
nominal	O
ordinal	O
5	O
%	O
a	O
5	O
a-	O
!	O
a	O
#	O
!	O
deﬁned	O
,	O
depending	O
on	O
the	O
type	O
of	O
variable	O
,	O
by	O
for	O
the	O
above	O
expressions	O
we	O
can	O
see	O
that	O
in	O
all	O
cases	O
we	O
can	O
write	O
,	O
for	O
all	O
the	O
is	O
«	O
whereu*	O
if	O
whereám2	O
''	O
%	O
âp5g	O
&	O
	O
(	O
&	O
¬âå	O
''	O
for	O
ordinal	O
variables	O
withß	O
wø	O
ad	O
''	O
a-	O
!	O
@	O
''	O
-å5¤	O
&	O
a+ù	O
a-	O
!	O
``	O
-å5¤	O
&	O
a+ù	O
the	O
problem	O
is	O
that	O
since	O
we	O
want	O
to	O
use	O
the	O
same	O
smoothing	O
parameter	O
,	O
	O
variables	O
,	O
we	O
have	O
to	O
normalise	O
them	O
.	O
to	O
do	O
so	O
we	O
substitute	O
by_	O
@	O
ä-å	O
	O
otherwise	O
.	O
a	O
nominal	O
values	O
ý4þ	O
ükã	O
ü+ýqþ	O
ý4þ	O
	O
;	O
2	O
>	O
y5	O
a	O
#	O
!	O
5	O
@	O
2	O
>	O
8d5	O
jc24j	O
(	O
*5	O
where9am2w85	O
denotes	O
the	O
number	O
of	O
examples	O
for	O
which	O
attributeg	O
has	O
the	O
value8	O
ando	O
is	O
the	O
sample	O
mean	O
of	O
theg	O
th	O
attribute	O
.	O
with	O
this	O
selection	O
ofu*	O
we	O
have	O
a-	O
!	O
5	O
@	O
d	O
u	O
average7	O
for	O
discrete	O
variables	O
the	O
range	O
of	O
the	O
smoothness	O
parameter	O
is	O
the	O
interval2	O
''	O
*	O
(	O
+5	O
.	O
one	O
a	O
''	O
a	O
#	O
!	O
@	O
''	O
+	O
(	O
*5=	O
&	O
(	O
+dhßpa	O
&	O
(	O
a-	O
!	O
a-	O
!	O
a	O
#	O
!	O
5=	O
&	O
(	O
	O
have	O
to	O
be	O
for	O
continuous	O
variables	O
the	O
range	O
is	O
ç~1	O
(	O
andu	O
&	O
1	O
(	O
and	O
&	O
regarded	O
as	O
limiting	O
cases	O
.	O
as	O
[	O
è¤	O
(	O
we	O
get	O
the	O
“	O
uniform	B
distribution	O
over	O
the	O
real	O
line	O
”	O
	O
we	O
get	O
the	O
dirac	O
spike	O
function	O
situated	O
at	O
the	O
a	O
#	O
!	O
.	O
and	O
asè	O
.	O
asè	O
having	O
deﬁned	O
the	O
kernels	O
we	O
will	O
use	O
,	O
we	O
need	O
to	O
choose	O
density	O
approaches	O
zero	O
at	O
allk	O
except	O
at	O
the	O
samples	O
where	O
it	O
is	O
(	O
+d	O
j	O
function	O
.	O
this	O
precludes	O
choosing	O
by	O
maximizing	O
the	O
log	O
likelihood	O
with	O
respect	O
to	O
bt	O
!	O
-24ky	O
!	O
@	O
5	O
where	O
to	O
maximiseé	O
(	O
1976	O
)	O
and	O
takes	O
''	O
%	O
ks7	O
''	O
-å5	O
5c	O
&	O
24k	O
ã	O
@	O
ê	O
ã*ë	O
êþ	O
the	O
estimated	O
times	O
the	O
dirac	O
delta	O
.	O
to	O
estimate	O
a	O
good	O
choice	O
of	O
smoothing	O
parameter	O
,	O
a	O
jackknife	O
modiﬁcation	O
of	O
the	O
maximum	O
likelihood	O
method	O
can	O
be	O
used	O
.	O
this	O
was	O
proposed	O
by	O
habbema	O
et	O
al	O
.	O
(	O
1974	O
)	O
and	O
duin	O
so	O
we	O
can	O
understand	O
the	O
above	O
process	O
as	O
rescaling	O
all	O
the	O
variables	O
to	O
the	O
same	O
scale	O
.	O
extreme	O
leads	O
to	O
the	O
uniform	B
distribution	O
and	O
the	O
other	O
to	O
a	O
one-point	O
distribution	O
:	O
if	O
if	O
7m	O
''	O
2qk	O
	O
2	O
	O
	O
	O
ø	O
ü	O
ý	O
	O
ü	O
ù	O
«	O
{	O
	O
ý	O
7	O
]	O
	O
ø	O
	O
ù	O
«	O
	O
ø	O
2	O
	O
a	O
''	O
	O
(	O
	O
i	O
«	O
ø	O
ü	O
ý	O
®	O
ü	O
ù	O
{	O
â	O
!	O
]	O
2	O
	O
	O
o	O
	O
a	O
{	O
â	O
!	O
]	O
2	O
	O
	O
o	O
	O
a	O
j	O
	O
{	O
	O
ý	O
7	O
]	O
	O
a	O
	O
	O
(	O
j	O
	O
(	O
â	O
n	O
!	O
]	O
2	O
	O
	O
o	O
	O
	O
a	O
\	O
]	O
!	O
j	O
2	O
	O
a	O
	O
&	O
	O
æ	O
g	O
	O
	O
	O
2	O
	O
	O
	O
&	O
	O
	O
2	O
	O
a	O
''	O
	O
''	O
	O
	O
a	O
&	O
	O
''	O
	O
	O
a	O
t	O
&	O
	O
	O
â	O
!	O
]	O
ä	O
ä	O
b	O
!	O
!	O
(	O
j	O
	O
(	O
â	O
n	O
©	O
	O
ø	O
n	O
ù	O
!	O
sec	O
.	O
4.3	O
]	O
density	O
estimation	O
33	O
this	O
criterion	O
makes	O
the	O
smoothness	O
data	O
dependent	O
,	O
leads	O
to	O
an	O
algorithm	O
for	O
an	O
arbi-	O
trary	O
dimensionality	O
of	O
the	O
data	O
and	O
possesses	O
consistency	O
requirements	O
as	O
discussed	O
by	O
aitchison	O
&	O
aitken	O
(	O
1976	O
)	O
.	O
tends	O
to	O
be	O
constant	O
over	O
large	O
regions	O
,	O
roughly	O
approximating	O
the	O
ﬁxed	O
kernel	O
model	O
.	O
the	O
variation	O
in	O
smoothness	O
of	O
the	O
estimated	O
density	O
over	O
the	O
different	O
regions	O
.	O
if	O
,	O
for	O
the	O
euclidean	O
distance	O
measured	O
after	O
standardisation	O
of	O
all	O
variables	O
.	O
the	O
proportionality	O
.	O
the	O
smoothing	O
value	O
is	O
now	O
determined	O
by	O
two	O
the	O
so-called	O
variable	O
kernel	O
model	O
.	O
an	O
extensive	O
description	O
of	O
this	O
model	O
was	O
ﬁrst	O
given	O
by	O
breiman	O
et	O
al	O
.	O
(	O
1977	O
)	O
.	O
this	O
method	O
has	O
promising	O
results	O
especially	O
when	O
lognormal	O
is	O
thus	O
proportional	O
to	O
the	O
!	O
dependent	O
on	O
the8	O
th	O
nearest	O
an	O
extension	O
of	O
the	O
above	O
model	O
for	O
is	O
to	O
make	O
!	O
,	O
so	O
that	O
we	O
have	O
a	O
!	O
for	O
each	O
sample	O
point	O
.	O
this	O
gives	O
rise	O
to	O
neighbour	O
distance	O
tok	O
or	O
skewed	O
distributions	O
are	O
estimated	O
.	O
the	O
kernel	O
width	O
!	O
denoted	O
byj	O
&	O
mcj	O
7	O
,	O
i.e	O
.	O
8	O
th	O
nearest	O
neighbour	O
distance	O
ink	O
7	O
.	O
we	O
take	O
forj	O
is	O
(	O
inversely	O
)	O
dependent	O
on8	O
factor	O
parameters	O
,	O
	O
and8	O
;	O
	O
can	O
be	O
though	O
of	O
as	O
an	O
overall	O
smoothing	O
parameter	O
,	O
while8	O
deﬁnes	O
example8w	O
&	O
	O
(	O
,	O
the	O
smoothness	O
will	O
vary	O
locally	O
while	O
for	O
larger8	O
values	O
the	O
smoothness	O
mí	O
wø	O
.	O
expì	O
a+ù	O
7*u	O
cj	O
7+u	O
cj	O
to	O
optimise	O
for	O
dimensional	O
optimisation	B
problem	O
of	O
the	O
likelihood	O
function¥	O
2we	O
''	O
-85	O
with	O
one	O
continuous	O
parameter	O
(	O
	O
)	O
and	O
one	O
discrete	O
parameter	O
(	O
8	O
)	O
.	O
silverman	O
(	O
1986	O
,	O
sections	O
2.6	O
and	O
5.3	O
)	O
studies	O
the	O
advantages	O
and	O
disadvantages	O
of	O
this	O
approach	O
.	O
he	O
also	O
proposes	O
another	O
method	O
to	O
estimate	O
the	O
smoothing	O
parameters	O
in	O
a	O
variable	O
kernel	O
model	O
(	O
see	O
silverman	O
,	O
1986	O
and	O
mclachlan	O
,	O
1992	O
for	O
details	O
)	O
.	O
the	O
jackknife	O
modiﬁcation	O
of	O
the	O
maximum	O
likelihood	O
method	O
can	O
again	O
be	O
applied	O
.	O
however	O
,	O
for	O
the	O
variable	O
kernel	O
this	O
leads	O
to	O
a	O
more	O
difﬁcult	O
two-	O
the	O
algorithm	O
we	O
mainly	O
used	O
in	O
our	O
trials	O
to	O
classify	O
by	O
density	O
estimation	O
is	O
alloc80	O
by	O
hermans	O
at	O
al	O
.	O
(	O
1982	O
)	O
(	O
see	O
appendix	O
b	O
for	O
source	O
)	O
.	O
we	O
use	O
a	O
normal	O
distribution	O
for	O
the	O
component	O
5g	O
&	O
a-	O
!	O
``	O
-	O
and8	O
a-	O
!	O
4.2.1	O
example	B
we	O
illustrate	O
the	O
kernel	O
classiﬁer	B
with	O
some	O
simulated	O
data	O
,	O
which	O
comprise	O
200	O
obser-	O
vations	O
from	O
a	O
standard	O
normal	O
distribution	O
(	O
class	O
1	O
,	O
say	O
)	O
and	O
100	O
(	O
in	O
total	O
)	O
values	O
from	O
an	O
equal	O
mixture	O
of2wî	O
''	O
+	O
(	O
*5	O
(	O
class	O
2	O
)	O
.	O
the	O
resulting	O
estimates	O
can	O
then	O
be	O
used	O
as	O
a	O
basis	O
for	O
classifying	O
future	O
observations	O
to	O
one	O
or	O
other	O
class	O
.	O
various	O
scenarios	O
are	O
given	O
in	O
figure	O
4.1	O
where	O
a	O
black	O
segment	O
indicates	O
that	O
observations	O
will	O
be	O
allocated	O
to	O
class	O
2	O
,	O
and	O
otherwise	O
to	O
class	O
1.	O
in	O
this	O
example	B
we	O
have	O
used	O
equal	O
priors	O
for	O
the	O
2	O
classes	O
(	O
although	O
they	O
are	O
not	O
equally	O
represented	O
)	O
,	O
and	O
hence	O
allocations	O
are	O
based	O
on	O
maximum	O
estimated	O
likelihood	O
.	O
it	O
is	O
clear	O
that	O
the	O
rule	O
will	O
depend	O
on	O
the	O
smoothing	O
parameters	O
,	O
and	O
can	O
result	O
in	O
very	O
disconnected	O
sets	O
.	O
in	O
higher	O
dimensions	O
these	O
segments	O
will	O
become	O
regions	O
,	O
with	O
potentially	O
very	O
nonlinear	O
boundaries	O
,	O
and	O
possibly	O
disconnected	O
,	O
depending	O
on	O
the	O
smoothing	O
parameters	O
used	O
.	O
for	O
comparison	O
we	O
also	O
draw	O
the	O
population	O
probability	O
densities	O
,	O
and	O
the	O
“	O
true	O
”	O
decision	O
regions	O
in	O
figure	O
4.1	O
(	O
top	O
)	O
,	O
which	O
are	O
still	O
disconnected	O
but	O
very	O
much	O
smoother	O
than	O
some	O
of	O
those	O
constructed	O
from	O
the	O
kernels	O
.	O
!	O
!	O
	O
!	O
!	O
!	O
7	O
2	O
	O
a	O
''	O
	O
!	O
(	O
!	O
a	O
ï	O
	O
	O
(	O
	O
è	O
	O
a	O
	O
	O
!	O
a	O
é	O
 	O
ï	O
34	O
modern	O
statistical	B
techniques	O
[	O
ch	O
.	O
4	O
true	O
probability	O
densities	O
with	O
decision	O
regions	O
4	O
.	O
0	O
3	O
.	O
0	O
2	O
.	O
0	O
1	O
.	O
0	O
fñ	O
0	O
.	O
0	O
11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111	O
-3	O
-2	O
-1	O
4	O
.	O
0	O
3	O
.	O
0	O
2	O
.	O
0	O
1	O
.	O
0	O
f	O
f	O
3	O
.	O
0	O
2	O
.	O
0	O
1	O
.	O
0	O
0	O
.	O
0	O
0ò	O
xð	O
0ò	O
xð	O
11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111	O
1	O
kernel	O
estimates	O
with	O
decision	O
regions	O
(	O
a	O
)	O
smoothing	O
values	O
=	O
0.3	O
,	O
0.8	O
(	O
b	O
)	O
smoothing	O
values	O
=	O
0.3	O
,	O
0.4	O
||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||	O
|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||	O
-3	O
-1	O
-2	O
||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||	O
1	O
||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||	O
-3	O
-2	O
-1	O
||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||	O
1	O
(	O
c	O
)	O
smoothing	O
values	O
=	O
0.1	O
,	O
1.0	O
(	O
d	O
)	O
smoothing	O
values	O
=	O
0.4	O
,	O
0.1	O
2ó	O
3ô	O
0ò	O
xð	O
f	O
f	O
6	O
.	O
0	O
4	O
.	O
0	O
2	O
.	O
0	O
0	O
.	O
0	O
0	O
.	O
2	O
5	O
.	O
1	O
0	O
.	O
1	O
5	O
.	O
0	O
0	O
.	O
0	O
3ô	O
2ó	O
3ô	O
2ó	O
0ò	O
xð	O
0ò	O
xð	O
-2	O
-1	O
1	O
1	O
||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||	O
-1	O
-2	O
||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||	O
-3	O
||||||||||||||||||||||||||||||||||||||	O
|||||||||||||||||||||||||||||||	O
||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||	O
|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||	O
||||||||||||||||||||	O
|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||	O
-3	O
fig	O
.	O
4.1	O
:	O
classiﬁcation	B
regions	O
for	O
kernel	O
classiﬁer	B
(	O
bottom	O
)	O
with	O
true	O
probability	O
densities	O
(	O
top	O
)	O
.	O
3ô	O
the	O
smoothing	O
parameters	O
quoted	O
in	O
(	O
a	O
)	O
–	O
(	O
d	O
)	O
are	O
the	O
values	O
ofõ	O
ö	O
used	O
in	O
equation	O
(	O
4.3	O
)	O
for	O
class	O
1	O
and	O
class	O
2	O
,	O
respectively	O
.	O
2ó	O
2ó	O
3ô	O
sec	O
.	O
4.3	O
]	O
k–nearest	O
neighbour	O
35	O
4.3	O
-nearest	O
neighbour	O
.	O
this	O
leads	O
immediately	O
there	O
is	O
a	O
problem	O
that	O
is	O
important	O
to	O
mention	O
.	O
in	O
the	O
above	O
analysis	O
it	O
is	O
assumed	O
that	O
.	O
however	O
,	O
it	O
could	O
be	O
the	O
case	O
that	O
our	O
sample	O
did	O
not	O
estimate	O
 * + 	O
''	O
l	O
,	O
by	O
a	O
nearest	O
neigh-	O
÷m5	O
,	O
ø	O
&	O
(	O
	O
''	O
suppose	O
we	O
consider	O
estimating	O
the	O
quantitiesb324k	O
bour	O
method	O
.	O
if	O
we	O
have	O
training	O
data	O
in	O
which	O
there	O
arejs÷	O
observations	O
from	O
class÷	O
with	O
÷	O
,	O
and	O
the	O
hypersphere	O
aroundk	O
containing	O
the8	O
nearest	O
observations	O
has	O
volume	O
j	O
[	O
&	O
c	O
{	O
ùj	O
 + * 	O
 * + 	O
	O
respectively	O
,	O
then	O
24ks5	O
observations	O
of	O
classes	O
zd24ks5	O
and	O
contains8	O
''	O
@	O
	O
24ks5	O
?	O
''	O
``	O
-8	O
is	O
estimated	O
by8	O
24ks5	O
%	O
dm24j	O
z2qks5	O
@	O
5	O
,	O
which	O
then	O
gives	O
dhj	O
andb_2qk	O
is	O
estimated	O
byj	O
ks5	O
by	O
substitution	O
asä	O
an	O
estimate	O
of0_2q	O
0y2q	O
ks5c	O
&	O
e8	O
24ks5	O
%	O
d8	O
if8mú	O
&	O
max÷	O
to	O
the	O
classiﬁcation	B
rule	O
:	O
classifyk	O
as	O
belonging	O
to	O
classú	O
2	O
>	O
8	O
5	O
.	O
this	O
is	O
known	O
as	O
the8	O
-nearest	O
neighbour	O
(	O
k-nn	O
)	O
classiﬁcation	B
rule	O
.	O
for	O
the	O
special	O
case	O
when	O
8	O
;	O
&	O
	O
(	O
,	O
it	O
is	O
simply	O
termed	O
the	O
nearest-neighbour	O
(	O
nn	O
)	O
classiﬁcation	B
rule	O
.	O
d	O
j	O
is	O
estimated	O
byj	O
the	O
nearest-neighbour	O
rule	O
should	O
work	O
.	O
to	O
begin	O
with	O
,	O
note	O
that	O
the	O
classûýüeü	O
with	O
the	O
nearest	O
neighbour	O
is	O
a	O
random	O
variable	O
and	O
the	O
probability	O
thatûýüeü³	O
&	O
v	O
k_üeü5	O
wherek_übü	O
is	O
merely0324	O
is	O
the	O
sample	O
nearest	O
tok	O
samples	O
is	O
very	O
large	O
,	O
it	O
is	O
reasonable	O
to	O
assume	O
thatk_übü	O
is	O
sufﬁciently	O
close	O
tok	O
k_übü5	O
.	O
in	O
this	O
case	O
,	O
we	O
can	O
view	O
the	O
nearest-neighbour	O
rule	O
as	O
a	O
ks5àþ0324	O
that0_24	O
!	O
with	O
probability	O
randomised	O
decision	O
rule	O
that	O
classiﬁesk	O
by	O
selecting	O
the	O
category	O
03249	O
!	O
ks5	O
.	O
as	O
a	O
nonparametric	O
density	O
estimator	O
the	O
nearest	O
neighbour	O
approach	O
yields	O
a	O
non-smooth	O
curve	O
which	O
does	O
not	O
integrate	O
to	O
unity	O
,	O
and	O
as	O
a	O
method	O
of	O
density	O
estimation	O
it	O
is	O
unlikely	O
to	O
be	O
appropriate	O
.	O
however	O
,	O
these	O
poor	O
qualities	O
need	O
not	O
extend	O
to	O
the	O
domain	O
of	O
classiﬁcation	B
.	O
note	O
also	O
that	O
the	O
nearest	O
neighbour	O
method	O
is	O
equivalent	O
to	O
the	O
kernel	O
density	O
estimate	O
as	O
the	O
smoothing	O
parameter	O
tends	O
to	O
zero	O
,	O
when	O
the	O
normal	O
kernel	O
function	O
is	O
used	O
.	O
see	O
scott	O
(	O
1992	O
)	O
for	O
details	O
.	O
.	O
when	O
the	O
number	O
of	O
so	O
properly	O
the	O
group-prior	O
probabilities	O
.	O
this	O
issue	O
is	O
studied	O
in	O
davies	O
(	O
1988	O
)	O
.	O
we	O
study	O
in	O
some	O
depth	O
the	O
nn	O
rule	O
.	O
we	O
ﬁrst	O
try	O
to	O
get	O
a	O
heuristic	O
understanding	O
of	O
why	O
associated	O
it	O
is	O
obvious	O
that	O
the	O
use	O
of	O
this	O
rule	O
involves	O
choice	O
of	O
a	O
suitable	O
metric	O
,	O
i.e	O
.	O
how	O
is	O
the	O
distance	O
to	O
the	O
nearest	O
points	O
to	O
be	O
measured	O
?	O
in	O
some	O
datasets	O
there	O
is	O
no	O
problem	O
,	O
but	O
for	O
multivariate	O
data	O
,	O
where	O
the	O
measurements	O
are	O
measured	O
on	O
different	O
scales	O
,	O
some	O
standardisation	O
is	O
usually	O
required	O
.	O
this	O
is	O
usually	O
taken	O
to	O
be	O
either	O
the	O
standard	O
deviation	O
or	O
the	O
range	O
of	O
the	O
variable	O
.	O
if	O
there	O
are	O
indicator	O
variables	O
(	O
as	O
will	O
occur	O
for	O
nominal	O
data	O
)	O
then	O
the	O
data	O
is	O
usually	O
transformed	O
so	O
that	O
all	O
observations	O
lie	O
in	O
the	O
unit	O
hypercube	O
.	O
note	O
that	O
the	O
metric	O
can	O
also	O
be	O
class	O
dependent	O
,	O
so	O
that	O
one	O
obtains	O
a	O
distance	O
conditional	O
on	O
the	O
class	O
.	O
this	O
will	O
increase	O
the	O
processing	O
and	O
classiﬁcation	B
time	O
,	O
but	O
may	O
lead	O
to	O
a	O
considerable	O
increase	O
in	O
performance	O
.	O
for	O
classes	O
with	O
few	O
samples	O
,	O
a	O
compromise	O
is	O
to	O
use	O
a	O
regularised	O
value	O
,	O
in	O
which	O
there	O
is	O
some	O
trade-off	O
between	O
the	O
within	O
–	O
class	O
value	O
,	O
and	O
the	O
global	O
value	O
of	O
the	O
rescaling	O
parameters	O
.	O
a	O
study	O
on	O
the	O
inﬂuence	O
of	O
data	O
transformation	O
and	O
metrics	O
on	O
the	O
k-nn	O
rule	O
can	O
be	O
found	O
in	O
todeschini	O
(	O
1989	O
)	O
.	O
to	O
speed	O
up	O
the	O
process	O
of	O
ﬁnding	O
the	O
nearest	O
neighbours	O
several	O
approaches	O
have	O
been	O
proposed	O
.	O
fukunaka	O
&	O
narendra	O
(	O
1975	O
)	O
used	O
a	O
branch	O
and	O
bound	O
algorithm	O
to	O
increase	O
the	O
speed	O
to	O
compute	O
the	O
nearest	O
neighbour	O
,	O
the	O
idea	O
is	O
to	O
divide	O
the	O
attribute	O
space	O
in	O
regions	O
and	O
explore	O
a	O
region	O
only	O
when	O
there	O
are	O
possibilities	O
of	O
ﬁnding	O
there	O
a	O
nearest	O
neighbour	O
.	O
the	O
regions	O
are	O
hierarchically	O
decomposed	O
to	O
subsets	O
,	O
sub-subsets	O
and	O
so	O
on	O
.	O
other	O
ways	O
to	O
speed	O
up	O
the	O
process	O
are	O
to	O
use	O
a	O
condensed-nearest-neighbour	O
rule	O
(	O
hart	O
,	O
	O
<	O
	O
''	O
.	O
÷	O
÷	O
<	O
	O
÷	O
5	O
÷	O
÷	O
÷	O
<	O
÷	O
<	O
÷	O
÷	O
.	O
÷	O
÷	O
!	O
!	O
<	O
!	O
<	O
!	O
<	O
<	O
36	O
modern	O
statistical	B
techniques	O
[	O
ch	O
.	O
4	O
1968	O
)	O
,	O
a	O
reduced-nearest-neighbour-rule	O
(	O
gates	O
,	O
1972	O
)	O
or	O
the	O
edited-nearest-neighbour-rule	O
(	O
hand	O
&	O
batchelor	O
,	O
1978	O
)	O
.	O
these	O
methods	O
all	O
reduce	O
the	O
training	O
set	O
by	O
retaining	O
those	O
observations	O
which	O
are	O
used	O
to	O
correctly	O
classify	O
the	O
discarded	O
points	O
,	O
thus	O
speeding	O
up	O
the	O
classiﬁcation	B
process	O
.	O
however	O
they	O
have	O
not	O
been	O
implemented	O
in	O
the	O
k-nn	O
programs	O
used	O
in	O
this	O
book	O
.	O
is	O
split	O
,	O
and	O
the	O
second	O
part	O
classiﬁed	O
using	O
a	O
k-nn	O
rule	O
.	O
however	O
,	O
in	O
large	O
datasets	O
,	O
this	O
method	O
can	O
be	O
prohibitive	O
in	O
cpu	O
time	O
.	O
indeed	O
for	O
large	O
datasets	O
,	O
the	O
method	O
is	O
very	O
time	O
classiﬁcation	B
.	O
enas	O
&	O
choi	O
(	O
1986	O
)	O
,	O
have	O
looked	O
at	O
this	O
problem	O
in	O
a	O
simulation	O
study	O
and	O
for	O
the	O
two	O
classes	O
problem	O
.	O
see	O
mclachlan	O
(	O
1992	O
)	O
for	O
the	O
choice	O
of8	O
can	O
be	O
made	O
by	O
cross-validation	O
methods	O
whereby	O
the	O
training	O
data	O
consuming	O
for8u	O
ÿ	O
(	O
since	O
all	O
the	O
training	O
data	O
must	O
be	O
stored	O
and	O
examined	O
for	O
each	O
proposed	O
rules	O
for	O
estimating8	O
in	O
the	O
trials	O
reported	O
in	O
this	O
book	O
,	O
we	O
used	O
the	O
nearest	O
neighbour	O
(	O
8	O
;	O
&	O
	O
(	O
)	O
classiﬁer	B
with	O
8	O
was	O
chosen	O
by	O
cross-validation	O
.	O
)	O
distances	O
were	O
scaled	O
using	O
the	O
standard	O
deviation	O
for	O
each	O
attribute	O
,	O
with	O
the	O
calculation	O
conditional	O
on	O
the	O
class	O
.	O
ties	O
were	O
broken	O
by	O
a	O
majority	O
vote	O
,	O
or	O
as	O
a	O
last	O
resort	O
,	O
the	O
default	O
rule	O
.	O
no	O
condensing	O
.	O
(	O
the	O
exception	O
to	O
this	O
was	O
the	O
satellite	O
dataset	O
-	O
see	O
section	O
9.3.6	O
-	O
in	O
which	O
details	O
.	O
4.3.1	O
example	B
nearest	O
neighbour	O
classifier	O
0	O
0	O
4	O
1	O
0	O
0	O
2	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
8	O
0	O
0	O
6	O
0	O
0	O
4	O
a	O
e	O
r	O
a	O
e	O
s	O
o	O
c	O
u	O
g	O
l	O
1	O
1	O
2	O
3	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
2	O
2	O
2	O
3	O
3	O
3	O
2	O
3	O
2	O
2	O
3	O
2	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
2	O
3	O
3	O
3	O
3	O
2	O
2	O
2	O
3	O
0.9	O
1.0	O
relative	O
weight	O
1.1	O
1.2	O
1	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
0.8	O
fig	O
.	O
4.2	O
:	O
nearest	O
neighbour	O
classiﬁer	B
for	O
one	O
test	O
example	B
.	O
the	O
following	O
example	B
shows	O
how	O
the	O
nearest	O
(	O
8/	O
&	O
1	O
(	O
)	O
neighbour	O
classiﬁer	B
works	O
.	O
the	O
data	O
are	O
a	O
random	O
subset	O
of	O
dataset	O
36	O
in	O
andrews	O
&	O
herzberg	O
(	O
1985	O
)	O
which	O
examines	O
the	O
relationship	O
between	O
chemical	O
subclinical	O
and	O
overt	O
nonketotic	O
diabetes	B
in	O
145	O
patients	O
(	O
see	O
above	O
for	O
more	O
details	O
)	O
.	O
for	O
ease	O
of	O
presentation	O
,	O
we	O
have	O
used	O
only	O
50	O
patients	O
and	O
two	O
of	O
the	O
six	O
variables	O
;	O
relative	O
weight	O
and	O
glucose	O
area	O
,	O
and	O
the	O
data	O
are	O
shown	O
in	O
figure	O
4.2	O
the	O
classiﬁcations	O
of	O
50	O
patients	O
are	O
one	O
of	O
overt	O
diabetic	O
(	O
1	O
)	O
,	O
chemical	O
diabetic	O
(	O
2	O
)	O
and	O
normal	O
(	O
3	O
)	O
are	O
labeled	O
on	O
the	O
graph	O
.	O
in	O
this	O
example	B
,	O
it	O
can	O
be	O
seen	O
that	O
glucose	O
area	O
sec	O
.	O
4.4	O
]	O
projection	O
pursuit	O
classiﬁcation	B
37	O
(	O
y-axis	O
)	O
is	O
more	O
useful	O
in	O
separating	O
the	O
three	O
classes	O
,	O
and	O
that	O
class	O
3	O
is	O
easier	O
to	O
distinguish	O
than	O
classes	O
1	O
and	O
2.	O
a	O
new	O
patient	O
,	O
whose	O
condition	O
is	O
supposed	O
unknown	O
is	O
assigned	O
the	O
same	O
classiﬁcation	B
as	O
his	O
nearest	O
neighbour	O
on	O
the	O
graph	O
.	O
the	O
distance	O
,	O
as	O
measured	O
to	O
each	O
point	O
,	O
needs	O
to	O
be	O
scaled	O
in	O
some	O
way	O
to	O
take	O
account	O
for	O
different	O
variability	O
in	O
the	O
different	O
directions	O
.	O
in	O
this	O
case	O
the	O
patient	O
is	O
classiﬁed	O
as	O
being	O
in	O
class	O
2	O
,	O
and	O
is	O
classiﬁed	O
correctly	O
.	O
the	O
decision	O
regions	O
for	O
the	O
nearest	O
neighbour	O
are	O
composed	O
of	O
piecewise	O
linear	O
bound-	O
aries	O
,	O
which	O
may	O
be	O
disconnected	O
regions	O
.	O
these	O
regions	O
are	O
the	O
union	O
of	O
dirichlet	O
cells	O
;	O
each	O
cell	O
consists	O
of	O
points	O
which	O
are	O
nearer	O
(	O
in	O
an	O
appropriate	O
metric	O
)	O
to	O
a	O
given	O
observa-	O
tion	B
than	O
to	O
any	O
other	O
.	O
for	O
this	O
data	O
we	O
have	O
shaded	O
each	O
cell	O
according	O
to	O
the	O
class	O
of	O
its	O
centre	O
,	O
and	O
the	O
resulting	O
decision	O
regions	O
are	O
shown	O
in	O
figure	O
4.3	O
nearest	O
neighbour	O
decision	O
regions	O
0	O
0	O
4	O
1	O
0	O
0	O
2	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
8	O
0	O
0	O
6	O
0	O
0	O
4	O
a	O
e	O
r	O
a	O
e	O
s	O
o	O
c	O
u	O
g	O
l	O
0.8	O
0.9	O
1.0	O
relative	O
weight	O
1.1	O
1.2	O
fig	O
.	O
4.3	O
:	O
decision	O
regions	O
for	O
nearest	O
neighbour	O
classiﬁer	B
.	O
4.4	O
projection	O
pursuit	O
classification	O
as	O
we	O
have	O
seen	O
in	O
the	O
previous	O
sections	O
our	O
goal	O
has	O
been	O
to	O
estimate	O
minimum	O
risk	O
decision	O
problem	O
into	O
a	O
minimum	O
error	O
decision	O
problem	O
.	O
to	O
do	O
so	O
we	O
 * + 	O
!	O
 	O
when	O
to	O
class	O
in	O
order	O
to	O
assignk	O
	O
b_2qk	O
5	O
?	O
''	O
%	O
.	O
``	O
6g	O
&	O
	O
(	O
''	O
''	O
l	O
,	O
	O
b_2qk	O
ea	O
5	O
f*24	O
$	O
	O
''	O
6g5	O
.ma	O
b_2qk	O
ba5	O
f*2q	O
$	O
l	O
''	O
6gh5	O
.pa	O
 * + 	O
''	O
l	O
,	O
and	O
to	O
simplify	O
problems	O
transform	O
our	O
''	O
wg^	O
&	O
	O
(	O
	O
''	O
we	O
assume	O
that	O
we	O
know	O
.	O
24	O
$	O
-	O
''	O
6gh5	O
%	O
	O
such	O
that	O
	O
andf*2q	O
$	O
l	O
''	O
6gh5	O
%	O
	O
simply	O
alterh	O
.	O
toh	O
.	O
	O
andf	O
$	O
-	O
''	O
6g	O
&	O
cf*2q	O
$	O
l	O
''	O
6g5w	O
.	O
f-¡q24	O
$	O
-	O
''	O
wg5w._¡	O
constrainingf	O
24	O
$	O
-	O
''	O
wg5	O
@	O
	O
ifgyt	O
&	O
¬	O
$	O
24	O
$	O
-	O
''	O
wg5	O
'	O
&	O
ì	O
then	O
an	O
approximation	O
to	O
.	O
to	O
be	O
of	O
the	O
form	O
constant	O
otherwise	O
is	O
<	O
	O
a	O
a	O
n	O
a	O
ä	O
ä	O
<	O
î	O
n	O
a	O
ä	O
ä	O
<	O
æ	O
$	O
a	O
a	O
¡	O
a	O
¡	O
a	O
a	O
æ	O
¡	O
f	O
¡	O
	O
¡	O
a	O
38	O
modern	O
statistical	B
techniques	O
[	O
ch	O
.	O
4	O
.pa	O
or	O
(	O
see	O
breiman	O
et	O
al.	O
,	O
1984	O
for	O
details	O
)	O
.	O
that	O
we	O
are	O
trying	O
to	O
estimate	O
.	O
the	O
problem	O
can	O
be	O
put	O
into	O
a	O
different	O
setting	O
that	O
resolves	O
the	O
difﬁculty	O
.	O
let	O
f*24	O
$	O
-	O
''	O
wg5	O
a	O
with	O
these	O
new	O
prior	O
and	O
costsk	O
is	O
assigned	O
to	O
classea	O
 	O
when	O
b2qk	O
b2qk	O
ba5	O
	O
!	O
!	O
 	O
ks5	O
0_2q	O
0324	O
ks5	O
 * + 	O
so	O
our	O
ﬁnal	O
goal	O
is	O
to	O
build	O
a	O
good	O
estimator	O
0y2q	O
ks5	O
?	O
``	O
6g	O
&	O
	O
(	O
''	O
''	O
-	O
,	O
	O
 + * 	O
''	O
l	O
,	O
	O
we	O
could	O
use	O
ks5t	O
''	O
wg	O
&	O
(	O
	O
''	O
0_2q	O
to	O
deﬁne	O
the	O
quality	O
of	O
an	O
estimatorjd2qks5	O
'	O
&	O
c	O
0324	O
ks5	O
n	O
ks5	O
%	O
5	O
20324	O
 + * 	O
''	O
-	O
,	O
	O
,	O
however	O
,	O
(	O
4.4	O
)	O
is	O
obviously	O
the	O
best	O
estimator	O
isj24ks5	O
[	O
&	O
	O
?	O
0324	O
ks5	O
?	O
``	O
6g	O
&	O
(	O
	O
''	O
 * + 	O
''	O
l	O
,	O
	O
ks5	O
?	O
``	O
6g	O
&	O
	O
(	O
''	O
useless	O
since	O
it	O
contains	O
the	O
unknown	O
quantitiest0_2q	O
 + * 	O
''	O
@	O
	O
a	O
random	O
vector	O
onh	O
	O
	O
»	O
with	O
distribution0_24	O
''	O
%	O
ks5	O
and	O
deﬁne	O
new	O
 * * 	O
''	O
-	O
,	O
by	O
''	O
6g	O
&	O
	O
(	O
''	O
variables	O
&	O
¬	O
if	O
_a	O
&	O
thenö	O
ks5	O
.	O
we	O
then	O
deﬁne	O
the	O
mean	O
square	O
error	O
	O
2wjm5	O
by	O
ky_	O
&	O
0324	O
	O
ks5	O
%	O
5	O
0_2qea	O
2	O
_a	O
estimatorj	O
we	O
have	O
ks5	O
@	O
5	O
0y2qea	O
2q0_2qea	O
ks5	O
n	O
5	O
'	O
&	O
2	O
>	O
j	O
2wjp5	O
 * * 	O
ks5t	O
''	O
wgc	O
&	O
­	O
(	O
''	O
''	O
-	O
,	O
	O
0y24	O
2qks5^	O
&	O
and	O
so	O
to	O
compare	O
two	O
estimatorsj	O
andj	O
<	O
%	O
	O
 * + 	O
''	O
l	O
,	O
	O
we	O
can	O
compare	O
the	O
values	O
of	O
	O
2wj	O
5t	O
''	O
wg	O
&	O
(	O
	O
''	O
5	O
and	O
	O
h2	O
>	O
j	O
t0	O
2q	O
5	O
.	O
when	O
projection	O
pursuit	O
techniques	O
are	O
used	O
in	O
classiﬁcation	B
problemsö	O
	O
'7	O
s7	O
7b	O
	O
'7	O
kyå	O
&	O
	O
	O
,	O
ö	O
7	O
,	O
ö	O
&	O
	O
(	O
.	O
the	O
coefﬁcients	O
(	O
and	O
{	O
=	O
with	O
and	O
the	O
functions	O
ya	O
4dhj	O
a	O
#	O
!	O
i5	O
js7	O
&	O
¬9	O
!	O
if	O
in	O
observation	O
$	O
,	O
!	O
=	O
&	O
ì	O
the	O
very	O
interesting	O
point	O
is	O
that	O
it	O
can	O
be	O
easily	O
shown	O
that	O
for	O
any	O
class	O
probability	O
are	O
parameters	O
of	O
the	O
model	O
and	O
are	O
estimated	O
by	O
least	O
squares	O
.	O
equation	O
(	O
4.5	O
)	O
is	O
approximated	O
by	O
(	O
4.4	O
)	O
(	O
4.5	O
)	O
is	O
24ks5^	O
&	O
ky	O
''	O
åa	O
(	O
4.6	O
)	O
with	O
otherwise	O
otherwise	O
modelled	O
as	O
.	O
¡	O
n	O
!	O
ä	O
.	O
¡	O
!	O
 	O
ä	O
<	O
 	O
5	O
	O
ä	O
.	O
¡	O
a	O
ä	O
<	O
æ	O
g	O
ä	O
<	O
	O
ä	O
a	O
<	O
æ	O
g	O
ä	O
a	O
<	O
ä	O
a	O
<	O
ö	O
a	O
a	O
<	O
	O
ä	O
a	O
<	O
	O
a	O
<	O
a	O
<	O
''	O
''	O
	O
a	O
a	O
ì	O
(	O
a	O
	O
a	O
<	O
a	O
<	O
ö	O
	O
n	O
a	O
	O
ä	O
<	O
	O
	O
	O
ö	O
a	O
<	O
	O
ä	O
<	O
	O
	O
ä	O
a	O
<	O
¡	O
a	O
<	O
ö	O
<	O
	O
n	O
	O
]	O
2	O
n	O
n	O
a	O
]	O
	O
a	O
	O
	O
a	O
5	O
7	O
&	O
ö	O
	O
	O
&	O
	O
	O
&	O
n	O
a	O
]	O
a	O
	O
7	O
	O
	O
	O
n	O
7	O
.	O
¡	O
7	O
j	O
n	O
!	O
	O
	O
7	O
!	O
	O
	O
7	O
	O
	O
n	O
	O
]	O
	O
7	O
	O
	O
	O
2	O
n	O
n	O
a	O
]	O
	O
	O
	O
7	O
(	O
	O
sec	O
.	O
4.4	O
]	O
projection	O
pursuit	O
classiﬁcation	B
39	O
.	O
``	O
-	O
.	O
,	O
 * + 	O
''	O
	O
the	O
“	O
projection	O
”	O
part	O
of	O
the	O
term	O
projection	O
pursuit	O
indicates	O
that	O
the	O
vector	O
x	O
is	O
of	O
the	O
projections	O
,	O
and	O
the	O
“	O
pursuit	O
”	O
part	O
indicates	O
that	O
the	O
optimization	O
technique	O
is	O
used	O
functions	O
are	O
in	O
order	O
.	O
they	O
are	O
special	O
scatterplot	O
smoother	O
designed	O
to	O
have	O
the	O
following	O
features	O
:	O
they	O
are	O
very	O
fast	O
to	O
compute	O
and	O
have	O
a	O
variable	O
span	O
.	O
aee	O
statsci	O
(	O
1991	O
for	O
details	O
.	O
it	O
is	O
the	O
purpose	O
of	O
the	O
projection	O
pursuit	O
algorithm	O
to	O
minimise	O
(	O
4.6	O
)	O
with	O
respect	O
to	O
,	O
given	O
,	O
the	O
number	O
of	O
predictive	O
terms	O
comprising	O
the	O
model	O
.	O
increasing	O
the	O
number	O
of	O
terms	O
decreases	O
the	O
bias	O
(	O
model	O
speciﬁcation	O
error	O
)	O
at	O
the	O
expense	O
of	O
increasing	O
the	O
variance	O
of	O
the	O
(	O
model	O
and	O
parameter	O
)	O
estimates	O
.	O
then	O
the	O
above	O
expression	O
is	O
minimised	O
with	O
respect	O
to	O
the	O
parameterss7	O
 * + 	O
5	O
and	O
the	O
functions	O
''	O
-	O
2w	O
	O
 * + 	O
''	O
@	O
$	O
c	O
&	O
þ	O
(	O
	O
''	O
projected	O
onto	O
the	O
direction	O
vectors	O
''	O
l	O
to	O
get	O
the	O
lengths	O
 * * 	O
to	O
ﬁnd	O
“	O
good	O
direction	O
”	O
vectors	O
''	O
l	O
''	O
-	O
a	O
few	O
words	O
on	O
the	O
î	O
''	O
s7	O
,	O
,	O
(	O
the	O
parameters	O
,	O
(	O
,	O
(	O
and	O
functions	O
the	O
training	O
data	O
.	O
the	O
principal	O
task	O
of	O
the	O
user	O
is	O
to	O
choose	O
&	O
	O
)	O
and	O
ﬁnd	O
and	O
less	O
.	O
that	O
is	O
,	O
solutions	O
that	O
minimise¥	O
the	O
strategy	O
is	O
to	O
start	O
with	O
a	O
relatively	O
large	O
value	O
of	O
(	O
say	O
are	O
found	O
for	O
all	O
models	O
of	O
size	O
~	O
 * + 	O
	O
'	O
''	O
	O
(	O
''	O
	O
''	O
+	O
(	O
in	O
order	O
of	O
decreasing	O
for	O
the	O
numerical	O
search	O
in	O
each	O
-term	O
model	O
are	O
the	O
solution	O
values	O
for	O
the	O
most	O
important	O
(	O
out	O
ofs	O
e	O
(	O
)	O
terms	O
of	O
the	O
previous	O
model	O
.	O
the	O
importance	O
is	O
measured	O
as	O
î	O
!	O
mî	O
þ5	O
2l	O
(	O
s7	O
&	O
''	O
	O
all	O
the	O
is	O
one	O
.	O
)	O
the	O
starting	O
point	O
for	O
the	O
minimisation	O
of	O
the	O
largest	O
model	O
,	O
	O
is	O
given	O
by	O
an	O
normalised	O
so	O
that	O
the	O
most	O
important	O
term	O
has	O
unit	O
importance	O
.	O
(	O
note	O
that	O
the	O
variance	O
of	O
,	O
term	O
stagewise	O
model	O
(	O
friedman	O
&	O
stuetzle	O
,	O
1981	O
and	O
statsci	O
,	O
1991	O
for	O
the	O
sequence	O
of	O
solutions	O
generated	O
in	O
this	O
manner	O
is	O
then	O
examined	O
by	O
the	O
user	O
and	O
a	O
ﬁnal	O
model	O
is	O
chosen	O
according	O
to	O
the	O
guidelines	O
above	O
.	O
a	O
very	O
precise	O
description	O
of	O
the	O
process	O
)	O
.	O
the	O
algorithm	O
we	O
used	O
in	O
the	O
trials	O
to	O
classify	O
by	O
projection	O
pursuit	O
is	O
smart	O
(	O
see	O
.	O
the	O
starting	O
parameter	O
values	O
friedman	O
,	O
1984	O
for	O
details	O
,	O
and	O
appendix	O
b	O
for	O
availability	O
)	O
4.4.1	O
example	B
this	O
method	O
is	O
illustrated	O
using	O
a	O
5-dimensional	O
dataset	O
with	O
three	O
classes	O
relating	O
to	O
chemical	O
and	O
overt	O
diabetes	B
.	O
the	O
data	O
can	O
be	O
found	O
in	O
dataset	O
36	O
of	O
andrews	O
&	O
herzberg	O
(	O
1985	O
)	O
and	O
were	O
ﬁrst	O
published	O
in	O
reaven	O
&	O
miller	O
(	O
1979	O
)	O
.	O
the	O
smart	O
model	O
can	O
be	O
examined	O
by	O
plotting	O
the	O
smooth	O
functions	O
in	O
the	O
two	O
projected	O
data	O
co-ordinates	O
:	O
these	O
are	O
given	O
in	O
figure	O
4.4	O
which	O
also	O
shows	O
the	O
class	O
values	O
given	O
by	O
the	O
projected	O
points	O
of	O
the	O
selected	O
training	O
data	O
(	O
100	O
of	O
the	O
145	O
patients	O
)	O
.	O
the	O
remainder	O
of	O
the	O
model	O
to	O
obtain	O
a	O
linear	O
combination	O
of	O
the	O
functions	O
which	O
can	O
then	O
be	O
used	O
to	O
model	O
the	O
conditional	O
probabilities	O
.	O
in	O
this	O
example	B
we	O
get	O
+	O
+	O
0.0010	O
$	O
%	O
0.0005	O
$	O
%	O
-	O
-	O
0.0044	O
'	O
&	O
0.0008	O
'	O
&	O
+	O
-	O
0.9998	O
0.0045	O
0.0065	O
chooses	O
the	O
values	O
ofy	O
!	O
-	O
t	O
-0.05	O
-0.40	O
0.46	O
=	O
=	O
=	O
-	O
-	O
0.0213	O
$	O
#	O
0.0001	O
$	O
#	O
@	O
-	O
=	O
=	O
=	O
-0.33	O
0.34	O
-0.01	O
	O
	O
	O
	O
&	O
	O
''	O
n	O
	O
''	O
	O
!	O
	O
''	O
''	O
	O
a	O
	O
	O
î	O
8	O
î	O
î	O
g	O
î	O
0	O
î	O
	O
&	O
	O
''	O
á	O
	O
&	O
	O
n	O
7	O
]	O
7	O
<	O
	O
<	O
	O
	O
	O
	O
	O
	O
	O
	O
#	O
	O
#	O
40	O
modern	O
statistical	B
techniques	O
[	O
ch	O
.	O
4	O
smooth	O
functions	O
with	O
training	O
data	O
projections	O
33333333333333333333333333333333333333333333333333333	O
2	O
2	O
22	O
22	O
22222222222222222	O
1111	O
1	O
1	O
1	O
1	O
11	O
1	O
11	O
1	O
111	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
-30	O
-25	O
-20	O
projected	O
point	O
-15	O
-10	O
-5	O
1	O
1	O
11	O
111	O
1	O
1	O
11	O
1	O
1	O
1	O
1	O
1	O
1	O
11	O
11	O
1	O
1	O
1	O
2	O
2	O
2	O
2	O
22	O
2	O
2	O
2	O
22	O
2	O
2	O
22	O
22	O
2	O
22	O
2	O
22	O
3	O
33	O
33	O
33	O
333333	O
3333	O
3	O
3	O
333333	O
3333	O
3	O
33333333	O
33	O
3	O
333	O
3	O
33333	O
333	O
1	O
f	O
2	O
f	O
0	O
1	O
.	O
5	O
.	O
0	O
0	O
0	O
.	O
.	O
5	O
0	O
-	O
.	O
0	O
1	O
-	O
5	O
0	O
.	O
0	O
0	O
.	O
.	O
0	O
1	O
-	O
.	O
0	O
2	O
-	O
-1.5	O
-1.0	O
-0.5	O
0.0	O
0.5	O
projected	O
point	O
fig	O
.	O
4.4	O
:	O
projected	O
training	O
data	O
with	O
smooth	O
functions	O
.	O
the	O
remaining	O
45	O
patients	O
were	O
used	O
as	O
a	O
test	O
data	O
set	O
,	O
and	O
for	O
each	O
class	O
the	O
unscaled	O
conditional	O
probability	O
can	O
be	O
obtained	O
using	O
the	O
relevant	O
coefﬁcients	O
for	O
that	O
class	O
.	O
these	O
are	O
shown	O
in	O
figure	O
4.5	O
,	O
where	O
we	O
have	O
plotted	O
the	O
predicted	O
value	O
against	O
only	O
one	O
of	O
the	O
projected	O
co-ordinate	O
axes	O
.	O
it	O
is	O
clear	O
that	O
if	O
we	O
choose	O
the	O
model	O
(	O
and	O
hence	O
the	O
class	O
)	O
to	O
maximise	O
this	O
value	O
,	O
then	O
we	O
will	O
choose	O
the	O
correct	O
class	O
each	O
time	O
.	O
4.5	O
naive	O
bayes	O
all	O
the	O
nonparametric	O
methods	O
described	O
so	O
far	O
in	O
this	O
chapter	O
suffer	O
from	O
the	O
requirements	O
that	O
all	O
of	O
the	O
sample	O
must	O
be	O
stored	O
.	O
since	O
a	O
large	O
number	O
of	O
observations	O
is	O
needed	O
to	O
obtain	O
good	O
estimates	O
,	O
the	O
memory	O
requirements	O
can	O
be	O
severe	O
.	O
in	O
this	O
section	O
we	O
will	O
make	O
independence	O
assumptions	O
,	O
to	O
be	O
described	O
later	O
,	O
among	O
the	O
variables	O
involved	O
in	O
the	O
classiﬁcation	B
problem	O
.	O
in	O
the	O
next	O
section	O
we	O
will	O
address	O
the	O
problem	O
of	O
estimating	O
the	O
relations	O
between	O
the	O
variables	O
involved	O
in	O
a	O
problem	O
and	O
display	O
such	O
relations	O
by	O
mean	O
of	O
a	O
directed	O
acyclic	O
graph	O
.	O
the	O
na¨ıve	O
bayes	O
classiﬁer	B
is	O
obtained	O
as	O
follows	O
.	O
we	O
assume	O
that	O
the	O
joint	O
distribution	O
 * * 	O
âã5	O
'	O
&	O
¬	O
.	O
b32	O
of	O
classes	O
and	O
attributes	O
can	O
be	O
written	O
as	O
:	O
;	O
24	O
the	O
problem	O
is	O
then	O
to	O
obtain	O
the	O
probabilities	O
.d	O
!	O
l	O
''	O
lb32	O
of	O
independence	O
makes	O
it	O
much	O
easier	O
to	O
estimate	O
these	O
probabilities	O
since	O
each	O
attribute	O
can	O
be	O
treated	O
separately	O
.	O
if	O
an	O
attribute	O
takes	O
a	O
continuous	O
value	O
,	O
the	O
usual	O
procedure	O
is	O
to	O
discretise	O
the	O
interval	O
and	O
to	O
use	O
the	O
appropriate	O
frequency	O
of	O
the	O
interval	O
,	O
although	O
there	O
is	O
an	O
option	O
to	O
use	O
the	O
normal	O
distribution	O
to	O
calculate	O
probabilities	O
.	O
$	O
l	O
''	O
6g	O
.	O
the	O
assumption	O
the	O
implementation	O
used	O
in	O
our	O
trials	O
to	O
obtain	O
a	O
na¨ıve	O
bayes	O
classiﬁer	B
comes	O
from	O
the	O
ind	O
package	O
of	O
machine	O
learning	O
algorithms	O
ind	O
1.0	O
by	O
wray	O
buntine	O
(	O
see	O
appendix	O
b	O
for	O
availability	O
)	O
.	O
9	O
!	O
>	O
5t	O
''	O
!	O
''	O
	O
''	O
''	O
	O
!	O
n	O
¦	O
a	O
]	O
	O
a	O
<	O
	O
!	O
5	O
æ	O
$	O
	O
a	O
<	O
æ	O
sec	O
.	O
4.6	O
]	O
causal	O
networks	O
41	O
estimated	O
(	O
unscaled	O
)	O
conditional	O
probabilities	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
-30	O
-25	O
-20	O
-15	O
-10	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
22	O
2	O
2	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
33	O
3	O
33	O
3	O
3	O
33	O
33	O
3	O
3	O
3	O
3	O
-5	O
2	O
2	O
22	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
1	O
1	O
1	O
1	O
-30	O
-25	O
1	O
1	O
1	O
1	O
-20	O
1	O
-15	O
3	O
3	O
-10	O
3	O
3	O
33	O
33	O
3	O
33	O
33	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
-5	O
.	O
.	O
.	O
0	O
1	O
8	O
0	O
6	O
0	O
4	O
0	O
2	O
0	O
0	O
0	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
0	O
1	O
8	O
0	O
6	O
0	O
4	O
0	O
2	O
0	O
0	O
0	O
2	O
0	O
-	O
.	O
.	O
.	O
0	O
1	O
.	O
.	O
8	O
0	O
6	O
0	O
.	O
4	O
0	O
.	O
.	O
2	O
0	O
0	O
0	O
.	O
1	O
1	O
1	O
1	O
-30	O
-25	O
3	O
333	O
3	O
33	O
33	O
3	O
333	O
3	O
33	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
2	O
2	O
22	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
-10	O
-5	O
1	O
1	O
1	O
1	O
1	O
-20	O
projected	O
point	O
-15	O
a	O
ﬁnite	O
state	O
,	O
where	O
we	O
use	O
the	O
short	O
notation	O
is	O
associated	O
.	O
the	O
total	O
set	O
of	O
conﬁguration	O
is	O
the	O
set	O
fig	O
.	O
4.5	O
:	O
projected	O
test	O
data	O
with	O
conditional	O
probablities	O
for	O
three	O
classes	O
.	O
class	O
1	O
(	O
top	O
)	O
,	O
class	O
2	O
(	O
middle	O
)	O
,	O
class	O
3	O
(	O
bottom	O
)	O
.	O
4.6	O
causal	O
networks	O
we	O
start	O
this	O
section	O
by	O
introducing	O
the	O
concept	O
of	O
causal	O
network	O
.	O
5	O
be	O
a	O
directed	O
acyclic	O
graph	O
(	O
dag	O
)	O
.	O
with	O
each	O
nodez¶/á	O
let	O
(	O
e	O
&	O
þ2wá	O
'	O
''	O
space	O
)	O
+*	O
)	O
h	O
&	O
)	O
*	O
,	O
.-	O
*	O
are	O
denoted	O
are2	O
''	O
%	O
zw¶á5	O
.	O
we	O
assume	O
that	O
typical	O
elements	O
of	O
)	O
*	O
and	O
elements	O
of	O
)	O
we	O
have	O
a	O
probability	O
distribution	O
:	O
`2	O
>	O
á5	O
over	O
)	O
:	O
;	O
2	O
>	O
á5	O
'	O
&	O
c	O
:	O
	O
''	O
@	O
zý¶á	O
5	O
be	O
a	O
directed	O
acyclic	O
graph	O
(	O
dag	O
)	O
.	O
for	O
eachzu¶~á	O
deﬁnition	O
1	O
let	O
(	O
m	O
&	O
m2wá	O
'	O
''	O
be	O
the	O
set	O
of	O
all	O
parents	O
ofz	O
andjd24zp51/íá	O
f*2qzp50/á	O
be	O
the	O
set	O
of	O
all	O
descendent	O
ofz	O
.	O
furthermore	O
forzý¶á	O
letpd24zm5	O
be	O
the	O
set	O
of	O
variables	O
iná	O
excludingz	O
andz	O
’	O
s	O
descendent	O
.	O
/~pd24zm5	O
,	O
andz	O
are	O
conditionally	O
independent	O
givenf*2qzp5	O
,	O
the	O
then	O
if	O
for	O
every	O
subset	O
''	O
-	O
:	O
5	O
e	O
&	O
þ2wá	O
'	O
''	O
there	O
are	O
two	O
key	O
results	O
establishing	O
the	O
relations	O
between	O
a	O
causal	O
network1	O
&	O
2wá	O
'	O
''	O
''	O
l	O
:	O
5	O
and	O
:	O
;	O
2wá5	O
.	O
the	O
proofs	O
can	O
be	O
found	O
in	O
neapolitan	O
(	O
1990	O
)	O
.	O
``	O
l	O
:	O
5	O
the	O
ﬁrst	O
theorem	O
establishes	O
that	O
ifc	O
&	O
|2	O
>	O
ág	O
''	O
is	O
a	O
causal	O
network	O
,	O
then	O
:	O
;	O
2	O
>	O
á5	O
can	O
fk24zp5	O
%	O
5	O
:	O
;	O
2qz	O
:	O
;	O
2	O
>	O
á5	O
'	O
&	O
thus	O
,	O
in	O
a	O
causal	O
network	O
,	O
if	O
one	O
knows	O
the	O
conditional	O
probability	O
distribution	O
of	O
each	O
variable	O
given	O
its	O
parents	O
,	O
one	O
can	O
compute	O
the	O
joint	O
probability	O
distribution	O
of	O
all	O
the	O
variables	O
in	O
the	O
network	O
.	O
this	O
obviously	O
can	O
reduce	O
the	O
complexity	O
of	O
determining	O
the	O
is	O
called	O
a	O
causal	O
or	O
bayesian	O
network	O
.	O
be	O
written	O
as	O
*2,3-	O
let	O
ö	O
)	O
*	O
	O
*	O
*	O
&	O
	O
*	O
ö	O
ö	O
ö	O
ö	O
¦	O
<	O
42	O
modern	O
statistical	B
techniques	O
[	O
ch	O
.	O
4	O
distribution	O
enormously	O
.	O
the	O
theorem	O
just	O
established	O
shows	O
that	O
if	O
we	O
know	O
that	O
a	O
dag	O
and	O
a	O
probability	O
distribution	O
constitute	O
a	O
causal	O
network	O
,	O
then	O
the	O
joint	O
distribution	O
can	O
be	O
retrieved	O
from	O
the	O
conditional	O
distribution	O
of	O
every	O
variable	O
given	O
its	O
parents	O
.	O
this	O
does	O
not	O
imply	O
,	O
however	O
,	O
that	O
if	O
we	O
arbitrarily	O
specify	O
a	O
dag	O
and	O
conditional	O
probability	O
distributions	O
of	O
every	O
variables	O
given	O
its	O
parents	O
we	O
will	O
necessary	O
have	O
a	O
causal	O
network	O
.	O
this	O
inverse	O
result	O
can	O
be	O
stated	O
as	O
follows	O
.	O
is	O
uniquely	O
determined	O
by	O
we	O
illustrate	O
the	O
notion	O
of	O
network	O
with	O
a	O
simple	O
example	B
taken	O
from	O
cooper	O
(	O
1984	O
)	O
.	O
suppose	O
that	O
metastatic	O
cancer	O
is	O
a	O
cause	O
of	O
brain	O
tumour	O
and	O
can	O
also	O
cause	O
an	O
increase	O
in	O
total	O
serum	O
calcium	O
.	O
suppose	O
further	O
that	O
either	O
a	O
brain	O
tumor	O
or	O
an	O
increase	O
in	O
total	O
serum	O
calcium	O
could	O
cause	O
a	O
patient	O
to	O
fall	O
into	O
a	O
coma	O
,	O
and	O
that	O
a	O
brain	O
tumor	O
could	O
cause	O
papilledema	O
.	O
let	O
be	O
a	O
set	O
of	O
ﬁnite	O
sets	O
of	O
alternatives	O
(	O
we	O
are	O
not	O
yet	O
calling	O
the	O
members	O
ofá	O
letá	O
variables	O
since	O
we	O
do	O
not	O
yet	O
have	O
a	O
probability	O
distribution	O
)	O
and	O
let	O
(	O
þ	O
&	O
e2wá	O
'	O
''	O
5	O
be	O
a	O
dag	O
.	O
in	O
addition	O
,	O
forz^¶~á	O
be	O
the	O
set	O
of	O
all	O
parents	O
ofz	O
,	O
and	O
let	O
a	O
conditional	O
letf*2qzp50/á	O
probability	O
distribution	O
ofz	O
givenf*24zm5	O
be	O
speciﬁed	O
for	O
every	O
event	O
inf*2qzp5	O
,	O
that	O
is	O
we	O
have	O
f*24zm5	O
@	O
5	O
.	O
then	O
a	O
joint	O
probability	O
distribution	O
:	O
of	O
the	O
vertices	O
:	O
;	O
2qz	O
a	O
probability	O
distributionä	O
iná	O
:	O
24z	O
:	O
;	O
2	O
>	O
á5	O
'	O
&	O
fk24zp5	O
%	O
5	O
*2,3-	O
''	O
-	O
:	O
5	O
constitutes	O
a	O
causal	O
network	O
.	O
andþ	O
&	O
e2wá	O
'	O
''	O
=	O
metastatic	O
cancer	O
present	O
=	O
serum	O
calcium	O
increased	O
=	O
brain	O
tumor	O
present	O
=	O
coma	O
present	O
=	O
papilledema	O
present	O
98	O
:	O
a	O
687	O
>	O
	O
?	O
90	O
:	O
b	O
607	O
>	O
	O
?	O
98	O
:	O
d	O
687	O
=metastatic	O
cancer	O
not	O
present	O
=	O
serum	O
calcium	O
not	O
increased	O
=	O
brain	O
tumor	O
not	O
present	O
=	O
coma	O
not	O
present	O
=	O
papilledema	O
not	O
present	O
>	O
	O
?	O
98	O
:	O
e	O
687	O
fig	O
.	O
4.6	O
:	O
dag	O
for	O
the	O
cancer	O
problem	O
.	O
then	O
,	O
the	O
structure	O
of	O
our	O
knowledge-base	O
is	O
represented	O
by	O
the	O
dag	O
in	O
figure	O
4.6.	O
this	O
structure	O
together	O
with	O
quantitative	O
knowledge	O
of	O
the	O
conditional	O
probability	O
of	O
every	O
variable	O
given	O
all	O
possible	O
parent	O
states	O
deﬁne	O
a	O
causal	O
network	O
that	O
can	O
be	O
used	O
as	O
device	O
to	O
perform	O
efﬁcient	O
(	O
probabilistic	O
)	O
inference	O
,	O
(	O
absorb	O
knowledge	O
about	O
variables	O
as	O
it	O
arrives	O
,	O
be	O
able	O
to	O
see	O
the	O
effect	O
on	O
the	O
other	O
variables	O
of	O
one	O
variable	O
taking	O
a	O
particular	O
value	O
and	O
so	O
on	O
)	O
.	O
see	O
pearl	O
(	O
1988	O
)	O
and	O
lauritzen	O
&	O
spiegelhalter	O
(	O
1988	O
)	O
.	O
9	O
;	O
:	O
c	O
607	O
ö	O
<	O
¦	O
ä	O
<	O
ö	O
p	O
p	O
4	O
4	O
f	O
f	O
j	O
j	O
5	O
5	O
<	O
<	O
<	O
=	O
>	O
>	O
>	O
>	O
<	O
<	O
<	O
=	O
>	O
>	O
sec	O
.	O
4.6	O
]	O
causal	O
networks	O
43	O
so	O
,	O
once	O
a	O
causal	O
network	O
has	O
been	O
built	O
,	O
it	O
constitutes	O
an	O
efﬁcient	O
device	O
to	O
perform	O
probabilistic	O
inference	O
.	O
however	O
,	O
there	O
remains	O
the	O
previous	O
problem	O
of	O
building	O
such	O
a	O
network	O
,	O
that	O
is	O
,	O
to	O
provide	O
the	O
structure	O
and	O
conditional	O
probabilities	O
necessary	O
for	O
characterizing	O
the	O
network	O
.	O
a	O
very	O
interesting	O
task	O
is	O
then	O
to	O
develop	O
methods	O
able	O
to	O
learn	O
the	O
net	O
directly	O
from	O
raw	O
data	O
,	O
as	O
an	O
alternative	O
to	O
the	O
method	O
of	O
eliciting	O
opinions	O
from	O
the	O
experts	O
.	O
in	O
the	O
problem	O
of	O
learning	O
graphical	O
representations	O
,	O
it	O
could	O
be	O
said	O
that	O
the	O
statistical	B
community	O
has	O
mainly	O
worked	O
in	O
the	O
direction	O
of	O
building	O
undirected	O
representations	O
:	O
chapter	O
8	O
of	O
whittaker	O
(	O
1990	O
)	O
provides	O
a	O
good	O
survey	O
on	O
selection	O
of	O
undirected	O
graphical	O
representations	O
up	O
to	O
1990	O
from	O
the	O
statistical	B
point	O
of	O
view	O
.	O
the	O
program	O
bifrost	O
(	O
højsgaard	O
et	O
al.	O
,	O
1992	O
)	O
has	O
been	O
developed	O
,	O
very	O
recently	O
,	O
to	O
obtain	O
causal	O
models	O
.	O
a	O
second	O
literature	O
on	O
model	O
selection	O
devoted	O
to	O
the	O
construction	O
of	O
directed	O
graphs	O
can	O
be	O
found	O
in	O
the	O
social	O
sciences	O
(	O
glymour	O
et	O
al.	O
,	O
1987	O
;	O
spirtes	O
et	O
al.	O
,	O
1991	O
)	O
and	O
the	O
artiﬁcial	O
intelligence	O
community	O
(	O
pearl	O
,	O
1988	O
;	O
herkovsits	O
&	O
cooper	O
,	O
1990	O
;	O
cooper	O
&	O
herkovsits	O
,	O
1991	O
and	O
fung	O
&	O
crawford	O
,	O
1991	O
)	O
.	O
in	O
this	O
section	O
we	O
will	O
concentrate	O
on	O
methods	O
to	O
build	O
a	O
simpliﬁed	O
kind	O
of	O
causal	O
structure	O
,	O
polytrees	O
(	O
singly	O
connected	O
networks	O
)	O
;	O
networks	O
where	O
no	O
more	O
than	O
one	O
path	O
exists	O
between	O
any	O
two	O
nodes	O
.	O
polytrees	O
,	O
are	O
directed	O
graphs	O
which	O
do	O
not	O
contain	O
loops	O
in	O
the	O
skeleton	O
(	O
the	O
network	O
without	O
the	O
arrows	O
)	O
that	O
allow	O
an	O
extremely	O
efﬁcient	O
local	O
propagation	O
procedure	O
.	O
before	O
describing	O
how	O
to	O
build	O
polytrees	O
from	O
data	O
,	O
we	O
comment	O
on	O
how	O
to	O
use	O
a	O
polytree	O
in	O
a	O
classiﬁcation	B
problem	O
.	O
in	O
any	O
classiﬁcation	B
problem	O
,	O
we	O
have	O
a	O
set	O
of	O
variables	O
that	O
(	O
possibly	O
)	O
have	O
inﬂuence	O
on	O
a	O
distinguished	O
classiﬁcation	B
.	O
the	O
problem	O
is	O
,	O
given	O
a	O
particular	O
instantiation	O
of	O
these	O
variables	O
,	O
to	O
predict	O
.	O
for	O
this	O
task	O
,	O
we	O
need	O
a	O
set	O
of	O
examples	O
and	O
their	O
correct	O
classiﬁcation	B
,	O
acting	O
as	O
a	O
training	O
sample	O
.	O
in	O
this	O
context	O
,	O
we	O
ﬁrst	O
estimate	O
from	O
this	O
training	O
sample	O
a	O
network	O
(	O
polytree	O
)	O
,	O
;	O
next	O
,	O
in	O
propagation	O
mode	O
,	O
given	O
a	O
new	O
case	O
with	O
unknown	O
classiﬁcation	B
,	O
we	O
will	O
instantiate	O
and	O
propagate	O
the	O
available	O
information	O
,	O
showing	O
the	O
more	O
likely	O
value	O
of	O
the	O
classiﬁcation	B
.	O
 * + 	O
''	O
@	O
$	O
c	O
&	O
þ	O
(	O
	O
''	O
''	O
q0å	O
&	O
l	O
variable	O
,	O
that	O
is	O
,	O
to	O
classify	O
this	O
particular	O
case	O
in	O
one	O
of	O
the	O
possible	O
categories	O
of	O
the	O
value	O
of	O
 q 	O
''	O
@	O
$	O
s	O
&	O
(	O
	O
''	O
''	O
q0å	O
$	O
@	O
'	O
structure	O
displaying	O
the	O
causal	O
relationships	O
among	O
the	O
variablesáþ	O
&	O
e	O
variable	O
.	O
moreover	O
,	O
the	O
network	O
shows	O
the	O
variables	O
iná	O
value	O
of	O
all	O
the	O
variables	O
iná	O
directly	O
have	O
inﬂuence	O
on	O
,	O
the	O
children	O
of	O
,	O
in	O
fact	O
the	O
parents	O
of	O
(	O
the	O
knowledge	O
of	O
these	O
variables	O
makes	O
parents	O
of	O
the	O
children	O
of	O
the	O
rest	O
of	O
variables	O
iná	O
 * * 	O
theory	O
to	O
build	O
polytree-based	O
representations	O
for	O
a	O
general	O
set	O
of	O
variables	O
assume	O
that	O
the	O
distribution	O
:	O
;	O
2ac5	O
of	O
to	O
estimate	O
)	O
can	O
be	O
represented	O
by	O
some	O
unknown	O
polytree	O
	O
,	O
that	O
is	O
,	O
:	O
;	O
2ba=5	O
has	O
the	O
form	O
it	O
is	O
important	O
to	O
note	O
that	O
this	O
classiﬁer	B
can	O
be	O
used	O
even	O
when	O
we	O
do	O
not	O
know	O
the	O
that	O
and	O
the	O
other	O
independent	O
of	O
)	O
(	O
pearl	O
,	O
1988	O
)	O
.	O
so	O
the	O
rest	O
of	O
the	O
network	O
could	O
be	O
pruned	O
,	O
thus	O
reducing	O
the	O
complexity	O
and	O
increasing	O
the	O
efﬁciency	O
of	O
the	O
classiﬁer	B
.	O
however	O
,	O
since	O
the	O
process	O
of	O
building	O
the	O
network	O
does	O
not	O
take	O
into	O
account	O
the	O
fact	O
that	O
we	O
are	O
only	O
interested	O
in	O
classifying	O
,	O
we	O
should	O
expect	O
as	O
a	O
classiﬁer	B
a	O
poorer	O
performance	O
than	O
other	O
classiﬁcation	B
oriented	O
methods	O
.	O
however	O
,	O
the	O
built	O
networks	O
are	O
able	O
to	O
display	O
insights	O
into	O
the	O
classiﬁcation	B
problem	O
that	O
other	O
methods	O
lack	O
.	O
we	O
now	O
proceed	O
to	O
describe	O
the	O
discrete-value	O
variables	O
(	O
which	O
we	O
are	O
trying	O
.	O
!	O
!	O
''	O
``	O
	O
44	O
modern	O
statistical	B
techniques	O
[	O
ch	O
.	O
4	O
!	O
4ù	O
!	O
4ù	O
''	O
@	O
â	O
!	O
4ù	O
.	O
skeleton	O
we	O
have	O
the	O
following	O
theorem	O
:	O
as	O
a	O
parent	O
.	O
is	O
deﬁned	O
by	O
!	O
+	O
''	O
is	O
the	O
(	O
possibly	O
empty	O
)	O
set	O
of	O
direct	O
parents	O
of	O
the	O
variable	O
the	O
ﬁrst	O
step	O
in	O
the	O
process	O
of	O
building	O
a	O
polytree	O
is	O
to	O
learn	O
the	O
skeleton	O
.	O
to	O
build	O
the	O
it	O
is	O
important	O
to	O
keep	O
in	O
mind	O
that	O
a	O
na¨ıve	O
bayes	O
classiﬁer	B
(	O
section	O
4.5	O
)	O
can	O
be	O
represented	O
by	O
a	O
polytree	O
,	O
more	O
precisely	O
a	O
tree	O
in	O
which	O
each	O
attribute	O
node	O
has	O
the	O
class	O
at	O
simpler	O
representations	O
than	O
the	O
one	O
displayed	O
in	O
figure	O
4.6.	O
the	O
skeleton	O
of	O
the	O
graph	O
involved	O
in	O
that	O
example	B
is	O
not	O
a	O
tree	O
.	O
theorem	O
1	O
any	O
maximum	O
weight	O
spanning	O
tree	O
(	O
mwst	O
)	O
where	O
the	O
weight	O
of	O
the	O
branch	O
connecting	O
then	O
,	O
according	O
to	O
key	O
results	O
seen	O
at	O
the	O
beginning	O
of	O
this	O
section	O
,	O
we	O
have	O
a	O
causal	O
is	O
nondegen-	O
erate	O
,	O
meaning	O
that	O
there	O
exists	O
a	O
connected	O
dag	O
that	O
displays	O
all	O
the	O
dependencies	O
and	O
''	O
dcdcec-	O
''	O
%	O
â	O
!	O
iù	O
is	O
a	O
polytree	O
.	O
we	O
will	O
assume	O
that	O
:	O
;	O
2ba=5	O
is	O
representable	O
by	O
a	O
polytree	O
:	O
;	O
24â*	O
!	O
:	O
;	O
2ac5	O
'	O
&	O
''	O
dcecdc-	O
''	O
@	O
â	O
''	O
%	O
â	O
where	O
â	O
!	O
4ù	O
!	O
4ù	O
in	O
	O
,	O
and	O
the	O
parents	O
of	O
each	O
variable	O
are	O
mutually	O
independent	O
.	O
so	O
we	O
are	O
aiming	O
networkþ	O
&	O
|2	O
''	O
-	O
:	O
5	O
and2	O
independencies	O
embedded	O
in	O
:	O
variable	O
if	O
a	O
nondegenerate	O
distribution	O
:	O
;	O
2	O
y5	O
!	O
and	O
''	O
@	O
â	O
:	O
;	O
24â*	O
!	O
l	O
''	O
%	O
â	O
?	O
a5	O
log	O
:	O
;	O
24â	O
a	O
5	O
'	O
&	O
ám2	O
:	O
`24â	O
5	O
%	O
:	O
;	O
24â	O
will	O
unambiguously	O
recover	O
the	O
skeleton	O
of	O
	O
.	O
eracy	O
implies	O
that	O
for	O
any	O
pairs	O
of	O
variables2	O
5b	O
ám2	O
7hg	O
7	O
5g	O
5	O
'	O
&	O
ám2	O
ám2	O
''	O
@	O
â7	O
5	O
log	O
p	O
jsj	O
7hg	O
7	O
5c	O
&	O
5b	O
ám2	O
having	O
found	O
the	O
skeleton	O
of	O
the	O
polytree	O
we	O
move	O
on	O
to	O
ﬁnd	O
the	O
directionality	O
of	O
the	O
branches	O
.	O
to	O
recover	O
the	O
directions	O
of	O
the	O
branches	O
we	O
use	O
the	O
following	O
facts	O
:	O
nondegen-	O
that	O
do	O
not	O
have	O
a	O
common	O
descendent	O
â7	O
5	O
''	O
@	O
â	O
â7	O
5	O
@	O
:	O
;	O
2qâ	O
â7	O
5	O
:	O
;	O
2qâ	O
:	O
`24â	O
7è	O
p	O
jsjhám2	O
p	O
jsjhám2	O
''	O
%	O
â	O
:	O
;	O
2qâ	O
7è	O
we	O
have	O
we	O
have	O
where	O
we	O
have	O
	O
,	O
then	O
(	O
4.7	O
)	O
and	O
for	O
any	O
of	O
the	O
patterns	O
7	O
5	O
'	O
&	O
furthermore	O
,	O
for	O
the	O
pattern	O
	O
¦	O
!	O
]	O
<	O
â	O
a	O
©	O
ø	O
a	O
«	O
ø	O
a	O
þ	O
ø	O
5	O
a	O
©	O
ø	O
a	O
«	O
ø	O
a	O
þ	O
ø	O
	O
!	O
''	O
ö	O
''	O
ö	O
5	O
a	O
n	O
f	O
þ	O
®	O
f	O
ý	O
!	O
a	O
5	O
!	O
a	O
5	O
!	O
''	O
a	O
5	O
!	O
''	O
a	O
	O
!	O
è	O
a	O
!	O
''	O
a	O
	O
!	O
''	O
a	O
<	O
	O
!	O
''	O
a	O
<	O
n	O
f	O
þ	O
®	O
f	O
ý	O
®	O
f	O
ã	O
!	O
a	O
!	O
a	O
<	O
!	O
<	O
a	O
<	O
!	O
g	O
a	O
''	O
!	O
g	O
a	O
!	O
è	O
a	O
!	O
''	O
a	O
	O
!	O
''	O
a	O
<	O
	O
sec	O
.	O
4.7	O
]	O
causal	O
networks	O
45	O
taking	O
all	O
these	O
facts	O
into	O
account	O
we	O
can	O
recover	O
the	O
head–to–head	O
patterns	O
,	O
(	O
4.7	O
)	O
,	O
which	O
are	O
the	O
really	O
important	O
ones	O
.	O
the	O
rest	O
of	O
the	O
branches	O
can	O
be	O
assigned	O
any	O
direction	O
as	O
long	O
as	O
we	O
do	O
not	O
produce	O
more	O
head–to–head	O
patterns	O
.	O
the	O
algorithm	O
to	O
direct	O
the	O
skeleton	O
can	O
be	O
found	O
in	O
pearl	O
(	O
1988	O
)	O
.	O
p	O
usal	O
the	O
program	O
to	O
estimate	O
causal	O
polytrees	O
used	O
in	O
our	O
trials	O
is	O
castle	O
,	O
(	O
	O
}	O
ructures	O
from	O
inductive¥	O
5	O
arning	O
)	O
.	O
it	O
has	O
been	O
developed	O
at	O
the	O
university	O
of	O
granada	O
for	O
the	O
esprit	O
project	O
statlog	O
(	O
acid	O
et	O
al	O
.	O
(	O
1991a	O
)	O
;	O
acid	O
et	O
al	O
.	O
(	O
1991b	O
)	O
)	O
.	O
see	O
appendix	O
b	O
for	O
availability	O
.	O
4.6.1	O
example	B
we	O
now	O
illustrate	O
the	O
use	O
of	O
the	O
bayesian	O
learning	O
methodology	O
in	O
a	O
simple	O
model	O
,	O
the	O
digit	O
recognition	O
in	O
a	O
calculator	O
.	O
digits	O
are	O
ordinarily	O
displayed	O
on	O
electronic	O
watches	O
and	O
calculators	O
using	O
seven	O
hor-	O
izontal	O
and	O
vertical	O
lights	O
in	O
on–off	O
conﬁgurations	O
(	O
see	O
figure	O
4.7	O
)	O
.	O
we	O
number	O
the	O
lights	O
to	O
be	O
an	O
eight–dimensional	O
as	O
shown	O
in	O
figure	O
4.7.	O
we	O
take	O
	O
&	O
2	O
>	O
£6	O
''	O
	O
''	O
	O
 + * 	O
''	O
ji*5	O
2	O
5	O
1	O
4	O
7	O
3	O
6	O
fig	O
.	O
4.7	O
:	O
digits	O
.	O
the	O
we	O
generate	O
examples	O
from	O
a	O
faulty	O
calculator	O
.	O
the	O
data	O
consist	O
of	O
outcomes	O
from	O
 + 	O
?	O
 	O
''	O
*	O
(	O
	O
''	O
vector	O
where£	O
'	O
&	O
|	O
$	O
denotes	O
the	O
$	O
i	O
}	O
lø	O
digit	O
,	O
$	O
9	O
&	O
''	O
-´	O
and	O
when	O
ﬁxing£	O
to	O
$	O
 * + 	O
remaining2	O
''	O
''	O
ji	O
?	O
5	O
&	O
	O
(	O
is	O
a	O
seven	O
dimensional	O
vector	O
of	O
zeros	O
and	O
ones	O
with	O
	O
otherwise	O
.	O
position	O
is	O
on	O
for	O
the	O
$	O
i	O
}	O
lø	O
digit	O
and	O
if	O
the	O
light	O
in	O
the	O
 * + 	O
the	O
random	O
vector£w	O
''	O
i	O
where£	O
is	O
the	O
class	O
label	O
,	O
the	O
digit	O
,	O
and	O
assumes	O
 + * 	O
 * 	O
?	O
 	O
the	O
values	O
in	O
''	O
-´	O
with	O
equal	O
probability	O
and	O
the	O
''	O
+	O
(	O
''	O
i	O
are	O
zero-one	O
 * + 	O
variables	O
.	O
given	O
the	O
value	O
of£	O
,	O
the	O
i	O
are	O
each	O
independently	O
equal	O
to	O
the	O
´	O
and	O
are	O
in	O
error	O
with	O
probability 	O
!	O
with	O
probabilityd 	O
(	O
.	O
value	O
corresponding	O
to	O
the	O
our	O
aim	O
is	O
to	O
build	O
up	O
the	O
polytree	O
displaying	O
the	O
(	O
in	O
)	O
dependencies	O
in	O
 * + 	O
j/£6	O
''	O
s	O
$	O
-	O
''	O
wg	O
&	O
þ	O
(	O
	O
''	O
$	O
4z	O
jy	O
}	O
jsj	O
ft¢*jsj	O
$	O
i	O
}	O
@	O
$	O
w¢*jspp£i£qâ	O
$	O
qjsj	O
p	O
±	O
p	O
jsjk	O
we	O
generate	O
four	O
hundred	O
samples	O
of	O
this	O
distributionand	O
use	O
them	O
as	O
a	O
learning	O
sample	O
.	O
after	O
reading	O
in	O
the	O
sample	O
,	O
estimating	O
the	O
skeleton	O
and	O
directing	O
the	O
skeleton	O
the	O
polytree	O
estimated	O
by	O
castle	O
is	O
the	O
one	O
shown	O
in	O
figure	O
4.8.	O
castle	O
then	O
tells	O
us	O
what	O
we	O
had	O
expected	O
:	O
finally	O
,	O
we	O
examine	O
the	O
predictive	O
power	O
of	O
this	O
polytree	O
.	O
the	O
posterior	O
probabilities	O
of	O
each	O
digit	O
given	O
some	O
observed	O
patterns	O
are	O
shown	O
in	O
figure	O
4.9.	O
.	O
	O
''	O
	O
	O
''	O
''	O
	O
	O
&	O
''	O
''	O
''	O
	O
''	O
''	O
''	O
''	O
''	O
''	O
''	O
!	O
a	O
5	O
5	O
0	O
5	O
5	O
r	O
5	O
	O
''	O
''	O
	O
46	O
modern	O
statistical	B
techniques	O
[	O
ch	O
.	O
4	O
fig	O
.	O
4.8	O
:	O
obtained	O
polytree	O
.	O
digit	O
0	O
463	O
0	O
1	O
1	O
0	O
290	O
1	O
0	O
749	O
0	O
0	O
21	O
0	O
2	O
2	O
0	O
971	O
0	O
0	O
0	O
3	O
0	O
0	O
0	O
280	O
0	O
0	O
4	O
0	O
0	O
6	O
0	O
913	O
0	O
5	O
0	O
0	O
0	O
699	O
0	O
644	O
6	O
519	O
0	O
1	O
19	O
0	O
51	O
7	O
0	O
251	O
12	O
2	O
1	O
5	O
8	O
16	O
0	O
0	O
0	O
2	O
10	O
9	O
0	O
0	O
0	O
0	O
63	O
0	O
4.7	O
other	O
recent	O
approaches	O
the	O
methods	O
discussed	O
in	O
this	O
section	O
are	O
available	O
via	O
anonymous	O
ftp	O
from	O
statlib	O
,	O
internet	O
address	O
128.2.241.142.	O
a	O
version	O
of	O
ace	O
for	O
nonlinear	O
discriminant	O
analysis	O
is	O
available	O
fig	O
.	O
4.9	O
:	O
probabilitiesl	O
1000	O
for	O
some	O
‘	O
digits	O
’	O
.	O
j	O
$	O
wu*f	O
.	O
mars	O
is	O
available	O
in	O
a	O
fortran	O
version	O
.	O
since	O
these	O
algorithms	O
were	O
not	O
formally	O
included	O
in	O
the	O
statlog	O
trials	O
(	O
for	O
various	O
reasons	O
)	O
,	O
we	O
give	O
only	O
a	O
brief	O
introduction	O
.	O
as	O
the	O
s	O
coded	O
functionr	O
4.7.1	O
ace	O
nonlinear	O
transformation	O
of	O
variables	O
is	O
a	O
commonly	O
used	O
practice	O
in	O
regression	O
problems	O
.	O
the	O
alternating	O
conditional	O
expectation	O
algorithm	O
(	O
breiman	O
&	O
friedman	O
,	O
1985	O
)	O
is	O
a	O
simple	O
iterative	O
scheme	O
using	O
only	O
bivariate	O
conditional	O
expectations	O
,	O
which	O
ﬁnds	O
those	O
transformations	O
that	O
produce	O
the	O
best	O
ﬁtting	O
additive	O
model	O
.	O
approaches	O
this	O
problem	O
by	O
minimising	O
the	O
squared-error	O
objective	O
suppose	O
we	O
have	O
two	O
random	O
variables	O
:	O
the	O
response	O
,	O
5	O
so	O
thatö	O
and	O
the	O
predictor	O
,	O
wà­b_2	O
5	O
.	O
the	O
ace	O
algorithm	O
2m	O
2	O
5	O
andb32	O
seek	O
transformationsm	O
2	O
m	O
2	O
5	O
@	O
	O
b32	O
5	O
[	O
&	O
for	O
ﬁxedm	O
,	O
the	O
minimisingb	O
	O
,	O
and	O
conversely	O
,	O
for	O
ﬁxedb	O
m	O
2	O
isb_2	O
ism	O
2	O
5g	O
&	O
	O
b_2	O
	O
.	O
the	O
key	O
idea	O
in	O
the	O
ace	O
algorithm	O
is	O
to	O
begin	O
with	O
minimisingm	O
(	O
4.8	O
)	O
the	O
,	O
and	O
we	O
5	O
<	O
ö	O
5	O
	O
 	O
ö	O
5	O
<	O
ö	O
5	O
<	O
using	O
an	O
automatic	O
smoothing	O
procedure	O
.	O
this	O
constitutes	O
one	O
iteration	O
of	O
the	O
algorithm	O
which	O
terminates	O
when	O
an	O
iteration	O
fails	O
to	O
ace	O
places	O
no	O
restriction	O
on	O
the	O
type	O
of	O
each	O
variable	O
.	O
the	O
transformation	O
functions	O
representing	O
the	O
class	O
labels	O
,	O
sec	O
.	O
4.7	O
]	O
other	O
recent	O
approaches	O
47	O
some	O
starting	O
functions	O
and	O
alternate	O
these	O
two	O
steps	O
until	O
convergence	O
.	O
with	O
multiple	O
(	O
4.9	O
)	O
(	O
4.10	O
)	O
(	O
4.11	O
)	O
n	O
,	O
ace	O
seeks	O
to	O
minimise	O
in	O
practice	O
,	O
given	O
a	O
dataset	O
,	O
estimates	O
of	O
the	O
conditional	O
expectations	O
are	O
constructed	O
in	O
order	O
to	O
stop	O
the	O
iterates	O
from	O
shrinking	O
is	O
scaled	O
to	O
have	O
unit	O
variance	O
in	O
each	O
iteration	O
.	O
also	O
,	O
without	O
loss	O
of	O
generality	O
,	O
the	O
condition	O
is	O
imposed	O
.	O
the	O
algorithm	O
minimises	O
equation	O
(	O
4.9	O
)	O
through	O
a	O
series	O
of	O
single-function	O
minimisations	O
involving	O
smoothed	O
estimates	O
of	O
bivariate	O
 * * 	O
predictors	O
m	O
2	O
to	O
zero	O
functions	O
,	O
which	O
trivially	O
minimise	O
the	O
squared	O
error	O
criterion	O
,	O
m	O
2	O
 * + 	O
mu	O
&	O
n	O
,	O
minimising	O
(	O
4.9	O
)	O
with	O
 + * 	O
conditional	O
expectations	O
.	O
for	O
a	O
given	O
set	O
of	O
functionsb	O
''	O
lb	O
5	O
yields	O
a	O
newm	O
2	O
respect	O
tom	O
2	O
örq	O
a5	O
b-am2	O
1s	O
:	O
&	O
m	O
#	O
âonpe2	O
m	O
2	O
5c	O
&	O
6ä-	O
.	O
next5	O
in	O
turn	O
with	O
givenm	O
2	O
is	O
minimised	O
for	O
eachbt	O
!	O
5	O
and	O
5	O
@	O
yx	O
u	O
&	O
wv	O
withu	O
!	O
yielding	O
the	O
solution	O
b-a	O
\	O
ö	O
[	O
z	O
!	O
b	O
]	O
ânbpb2	O
:	O
&	O
¬b	O
5	O
'	O
&	O
m	O
2	O
a	O
\	O
.	O
decrease5	O
 * * 	O
5	O
assume	O
values	O
on	O
the	O
real	O
line	O
but	O
their	O
arguments	O
may	O
assume	O
''	O
-b	O
5t	O
''	O
5t	O
''	O
lb	O
m	O
2	O
ba	O
.	O
ace	O
then	O
ﬁnds	O
the	O
transformations	O
that	O
make	O
the	O
relationship	O
ofm	O
245	O
to	O
theb	O
5	O
as	O
linear	O
as	O
possible	O
.	O
÷	O
order	O
regression	O
spline	O
functionä	O
one	O
predictor	O
variable	O
,	O
	O
b	O
*2	O
.	O
an	O
approximating	O
,	O
obtained	O
by	O
dividing	O
the	O
range	O
of	O
values	O
into	O
/	O
(	O
disjoint	O
regions	O
separated	O
by	O
÷	O
degree	O
polynomial	O
in	O
called	O
“	O
knots	O
”	O
.	O
the	O
approximation	O
takes	O
the	O
form	O
of	O
a	O
separate	O
,	O
each	O
region	O
,	O
constrained	O
so	O
that	O
the	O
function	O
and	O
its	O
,	O
(	O
derivatives	O
are	O
continuous	O
.	O
each	O
÷	O
degree	O
polynomial	O
is	O
deﬁned	O
by	O
,	O
	O
	O
(	O
parameters	O
so	O
there	O
are	O
a	O
total	O
of2	O
u	O
(	O
+5	O
#	O
2	O
>	O
,	O
c	O
	O
(	O
*5	O
5	O
be	O
low2	O
>	O
,	O
.	O
continuity	O
requirements	O
place	O
,	O
constraints	O
at	O
each	O
knot	O
location	O
making	O
a	O
total	O
of	O
,	O
constraints	O
.	O
4.7.2	O
mars	O
the	O
mars	O
(	O
multivariate	O
adaptive	O
regression	O
spline	O
)	O
procedure	O
(	O
friedman	O
,	O
1991	O
)	O
is	O
based	O
on	O
a	O
generalisation	O
of	O
spline	O
methods	O
for	O
function	O
ﬁtting	O
.	O
consider	O
the	O
case	O
of	O
only	O
is	O
points	O
values	O
on	O
any	O
set	O
so	O
ordered	O
real	O
,	O
ordered	O
and	O
unordered	O
categorical	O
and	O
binary	O
variables	O
can	O
all	O
be	O
incorporated	O
in	O
the	O
same	O
regression	O
equation	O
.	O
for	O
categorical	O
variables	O
,	O
the	O
procedure	O
can	O
be	O
regarded	O
as	O
estimating	O
optimal	O
scores	O
for	O
each	O
of	O
their	O
values	O
.	O
for	O
use	O
in	O
classiﬁcation	B
problems	O
,	O
the	O
response	O
is	O
replaced	O
by	O
a	O
categorical	O
variable	O
parameters	O
to	O
be	O
adjusted	O
to	O
best	O
ﬁt	O
the	O
data	O
.	O
generally	O
the	O
order	O
of	O
the	O
spline	O
is	O
taken	O
to	O
''	O
''	O
5	O
&	O
ö	O
ð	O
ñ	O
ò	O
5	O
	O
n	O
n	O
a	O
]	O
b	O
a	O
2	O
a	O
5	O
ó	O
ô	O
õ	O
5	O
ö	O
ö	O
b	O
&	O
&	O
ö	O
b	O
n	O
&	O
	O
''	O
5	O
5	O
{	O
n	O
a	O
]	O
<	O
t	O
t	O
t	O
ö	O
q	O
{	O
n	O
a	O
]	O
b	O
a	O
2	O
a	O
5	O
<	O
s	O
t	O
t	O
t	O
 	O
ö	O
2	O
 	O
]	O
b	O
!	O
2	O
!	O
5	O
!	O
®	O
!	O
\	O
5	O
	O
n	O
]	O
!	O
b	O
a	O
2	O
a	O
5	O
<	O
^	O
2	O
n	O
2	O
n	O
!	O
2	O
!	O
	O
	O
5	O
	O
	O
,	O
	O
	O
î	O
48	O
modern	O
statistical	B
techniques	O
[	O
ch	O
.	O
4	O
while	O
regression	O
spline	O
ﬁtting	O
can	O
be	O
implemented	O
by	O
directly	O
solving	O
this	O
constrained	O
minimisation	O
problem	O
,	O
it	O
is	O
more	O
usual	O
to	O
convert	O
the	O
problem	O
to	O
an	O
unconstrained	O
optimi-	O
(	O
given	O
the	O
chosen	O
knot	O
locations	O
)	O
and	O
performing	O
a	O
linear	O
least	O
squares	O
ﬁt	O
of	O
the	O
response	O
on	O
this	O
basis	O
function	O
set	O
.	O
in	O
this	O
case	O
the	O
approximation	O
takes	O
the	O
form	O
(	O
4.12	O
)	O
÷	O
order	O
spline	O
functions	O
	O
@	O
ù	O
5	O
@	O
	O
_k`	O
(	O
4.14	O
)	O
are	O
unconstrained	O
and	O
the	O
continu-	O
.	O
one	O
such	O
(	O
4.13	O
)	O
regions	O
and	O
the	O
truncated	O
power	O
	O
%	O
ù	O
_k`	O
functions	O
are	O
deﬁned	O
basis	O
,	O
the	O
“	O
truncated	O
power	O
basis	O
”	O
,	O
is	O
comprised	O
of	O
the	O
functions	O
sation	O
by	O
chosing	O
a	O
set	O
of	O
basis	O
functions	O
that	O
span	O
the	O
space	O
of	O
all	O
,	O
pm7ba	O
5g	O
&	O
!	O
_k`	O
where	O
the	O
values	O
of	O
the	O
expansion	O
coefﬁcientsp	O
ity	O
constraints	O
are	O
intrinsically	O
embodied	O
in	O
the	O
basis	O
functions2a	O
''	O
m2	O
are	O
the	O
knot	O
locations	O
deﬁning	O
the	O
wherek	O
}	O
l7	O
	O
	O
[	O
î	O
}	O
l7	O
}	O
l7	O
5	O
u	O
}	O
l7	O
pm7m2	O
þ	O
(	O
}	O
l7	O
5	O
}	O
l7	O
5	O
the	O
ﬂexibility	O
of	O
the	O
regression	O
spline	O
approach	O
can	O
be	O
enhanced	O
by	O
incorporating	O
an	O
au-	O
tomatic	O
knot	O
selection	O
strategy	O
as	O
part	O
of	O
the	O
data	O
ﬁtting	O
process	O
.	O
a	O
simple	O
and	O
effective	O
strategy	O
for	O
automatically	O
selecting	O
both	O
the	O
number	O
and	O
locations	O
for	O
the	O
knots	O
was	O
de-	O
scribed	O
by	O
smith	O
(	O
1982	O
)	O
,	O
who	O
suggested	O
using	O
the	O
truncated	O
power	O
basis	O
in	O
a	O
numerical	O
minimisation	O
of	O
the	O
least	O
squares	O
criterion	O
(	O
4.15	O
)	O
max	O
can	O
be	O
regarded	O
as	O
the	O
parameters	O
associated	O
with	O
,	O
p	O
}	O
l7	O
5	O
the	O
multivariate	O
adaptive	O
regression	O
spline	O
method	O
(	O
friedman	O
,	O
1991	O
)	O
can	O
be	O
viewed	O
as	O
thereby	O
estimating	O
the	O
global	O
amount	O
of	O
smoothing	O
to	O
be	O
applied	O
as	O
well	O
as	O
estimating	O
the	O
separate	O
relative	O
amount	O
of	O
smoothing	O
to	O
be	O
applied	O
locally	O
at	O
different	O
locations	O
.	O
.	O
adding	O
or	O
deleting	O
a	O
knot	O
is	O
viewed	O
as	O
adding	O
or	O
deleting	O
the	O
corresponding	O
.	O
the	O
strategy	O
involves	O
starting	O
with	O
a	O
very	O
large	O
number	O
of	O
eligible	O
knot	O
;	O
we	O
may	O
choose	O
one	O
at	O
every	O
interior	O
data	O
point	O
,	O
and	O
considering	O
as	O
candidates	O
to	O
be	O
selected	O
through	O
a	O
statistical	B
variable	O
subset	O
selection	O
procedure	O
.	O
this	O
approach	O
to	O
knot	O
selection	O
is	O
both	O
elegant	O
and	O
here	O
the	O
coefﬁcients	O
ak	O
a	O
multiple	O
linear	O
least	O
squares	O
regression	O
of	O
the	O
responseâ	O
on	O
the	O
“	O
variables	O
”	O
	O
	O
and	O
ã2	O
variable2	O
 * * 	O
locationsk	O
}	O
''	O
6	O
}	O
max	O
corresponding	O
variablesã2	O
powerful	O
.	O
it	O
automatically	O
selects	O
the	O
number	O
of	O
knots	O
 + * 	O
and	O
their	O
locations	O
}	O
''	O
w	O
}	O
b	O
*24ks5	O
ofj	O
a	O
multivariate	O
generalisation	O
of	O
this	O
strategy	O
.	O
an	O
approximating	O
spline	O
functionä	O
variables	O
is	O
deﬁned	O
analogously	O
to	O
that	O
for	O
one	O
variable	O
.	O
thej	O
-dimensional	O
space	O
b	O
h24ks5	O
divided	O
into	O
a	O
set	O
of	O
disjoint	O
regions	O
and	O
within	O
each	O
oneä	O
inj	O
variables	O
with	O
the	O
maximum	O
degree	O
of	O
any	O
single	O
variable	O
being	O
,	O
.	O
the	O
approximation	O
(	O
*5	O
-dimensional	O
region	O
the	O
approximating	O
polynomials	O
in	O
seperate	O
regions	O
along	O
the24j	O
24ks5	O
÷	O
orderj	O
-dimensional	O
spline	O
functions	O
.	O
function	O
set	O
that	O
spans	O
the	O
space	O
of	O
all	O
,	O
and	O
its	O
derivatives	O
are	O
constrained	O
to	O
be	O
everywhere	O
continuous	O
.	O
this	O
places	O
constraints	O
on	O
is	O
is	O
taken	O
to	O
be	O
a	O
polynomial	O
is	O
most	O
easily	O
constructed	O
using	O
a	O
basis	O
boundaries	O
.	O
as	O
in	O
the	O
univariate	O
case	O
,	O
	O
ä	O
b	O
	O
2	O
	O
	O
n	O
7	O
]	O
	O
ø	O
7	O
2	O
	O
5	O
7	O
	O
	O
	O
ø	O
7	O
2	O
	O
	O
	O
	O
	O
a	O
	O
	O
a	O
]	O
	O
	O
	O
}	O
7	O
5	O
	O
`	O
	O
_	O
_	O
2	O
	O
	O
	O
	O
&	O
ì	O
	O
2	O
	O
	O
	O
	O
ü	O
n	O
!	O
]	O
z	O
\	O
â	O
!	O
	O
	O
n	O
a	O
]	O
	O
4	O
a	O
	O
a	O
	O
_	O
n	O
7	O
]	O
	O
	O
	O
`	O
]	O
^	O
4	O
	O
	O
7	O
	O
_	O
	O
a	O
	O
	O
	O
	O
}	O
7	O
5	O
	O
`	O
	O
_	O
	O
	O
}	O
7	O
5	O
	O
`	O
''	O
_	O
	O
	O
	O
`	O
	O
_	O
''	O
_	O
â	O
	O
ä	O
b	O
	O
	O
sec	O
.	O
4.7	O
]	O
other	O
recent	O
approaches	O
49	O
mars	O
implements	O
a	O
forward/backward	O
stepwise	O
selection	O
strategy	O
.	O
the	O
forward	B
se-	O
in	O
each	O
in	O
the	O
model	O
.	O
iteration	O
we	O
consider	O
adding	O
two	O
terms	O
to	O
the	O
model	O
lection	O
begins	O
with	O
only	O
the	O
constant	O
basis	O
functiona	O
agam2	O
}	O
l5	O
agam2f	O
}	O
is	O
one	O
of	O
the	O
basis	O
functions	O
already	O
chosen	O
,	O
	O
wherea	O
not	O
represented	O
ina	O
is	O
one	O
of	O
the	O
predictor	O
variables	O
is	O
a	O
knot	O
location	O
on	O
that	O
variable	O
.	O
the	O
two	O
terms	O
of	O
this	O
form	O
,	O
which	O
cause	O
the	O
greatest	O
decrease	O
in	O
the	O
residual	O
sum	O
of	O
squares	O
,	O
are	O
added	O
to	O
the	O
model	O
.	O
the	O
forward	B
selection	O
process	O
continues	O
until	O
a	O
relatively	O
large	O
number	O
of	O
basis	O
functions	O
is	O
included	O
in	O
a	O
deliberate	O
attempt	O
to	O
overﬁt	O
the	O
data	O
.	O
the	O
backward	B
“	O
pruning	B
”	O
procedure	O
,	O
standard	O
stepwise	O
linear	O
regression	O
,	O
is	O
then	O
applied	O
with	O
the	O
basis	O
functions	O
representing	O
the	O
stock	O
of	O
“	O
variables	O
”	O
.	O
the	O
best	O
ﬁtting	O
model	O
is	O
chosen	O
with	O
the	O
ﬁt	O
measured	O
by	O
a	O
cross-validation	O
criterion	O
.	O
a	O
and	O
}	O
2qks5w	O
&	O
v	O
(	O
(	O
4.16	O
)	O
mars	O
is	O
able	O
to	O
incorporate	O
variables	O
of	O
different	O
type	O
;	O
continuous	O
,	O
discrete	O
and	O
categorical	O
.	O
	O
	O
	O
`	O
	O
	O
5	O
`	O
a	O
5	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
c.	O
feng	O
(	O
1	O
)	O
and	O
d.	O
michie	O
(	O
2	O
)	O
(	O
1	O
)	O
the	O
turing	O
institute	O
and	O
(	O
2	O
)	O
university	O
of	O
strathclyde	O
this	O
chapter	O
is	O
arranged	O
in	O
three	O
sections	O
.	O
section	O
5.1	O
introduces	O
the	O
broad	O
ideas	O
underlying	O
the	O
main	O
rule-learning	O
and	O
tree-learning	O
methods	O
.	O
section	O
5.2	O
summarises	O
the	O
speciﬁc	O
characteristics	O
of	O
algorithms	O
used	O
for	O
comparative	O
trials	O
in	O
the	O
statlog	O
project	O
.	O
section	O
5.3	O
looks	O
beyond	O
the	O
limitations	O
of	O
these	O
particular	O
trials	O
to	O
new	O
approaches	O
and	O
emerging	O
principles	O
.	O
5.1	O
rules	O
and	O
trees	O
from	O
data	O
:	O
first	O
principles	O
5.1.1	O
data	O
ﬁt	O
and	O
mental	B
ﬁt	I
of	O
classiﬁers	O
in	O
a	O
1943	O
lecture	O
(	O
for	O
text	O
see	O
carpenter	O
&	O
doran	O
,	O
1986	O
)	O
a.m.turing	O
identiﬁed	O
machine	O
learning	O
(	O
ml	O
)	O
as	O
a	O
precondition	O
for	O
intelligent	O
systems	O
.	O
a	O
more	O
speciﬁc	O
engineering	O
expression	O
of	O
the	O
same	O
idea	O
was	O
given	O
by	O
claude	O
shannon	O
in	O
1953	O
,	O
and	O
that	O
year	O
also	O
saw	O
the	O
ﬁrst	O
computational	O
learning	O
experiments	O
,	O
by	O
christopher	O
strachey	O
(	O
see	O
muggleton	O
,	O
1993	O
)	O
.	O
after	O
steady	O
growth	O
ml	O
has	O
reached	O
practical	O
maturity	O
under	O
two	O
distinct	O
headings	O
:	O
(	O
a	O
)	O
as	O
a	O
means	O
of	O
engineering	O
rule-based	O
software	O
(	O
for	O
example	B
in	O
“	O
expert	O
systems	O
”	O
)	O
from	O
sample	O
cases	O
volunteered	O
interactively	O
and	O
(	O
b	O
)	O
as	O
a	O
method	O
of	O
data	O
analysis	O
whereby	O
rule-	O
structured	O
classiﬁers	O
for	O
predicting	O
the	O
classes	O
of	O
newly	O
sampled	O
cases	O
are	O
obtained	O
from	O
a	O
“	O
training	O
set	O
”	O
of	O
pre-classiﬁed	O
cases	O
.	O
we	O
are	O
here	O
concerned	O
with	O
heading	O
(	O
b	O
)	O
,	O
exempliﬁed	O
by	O
michalski	O
and	O
chilausky	O
’	O
s	O
(	O
1980	O
)	O
landmark	O
use	O
of	O
the	O
aq11	O
algorithm	O
(	O
michalski	O
&	O
larson	O
,	O
1978	O
)	O
to	O
generate	O
automatically	O
a	O
rule-based	O
classiﬁer	B
for	O
crop	O
farmers	O
.	O
rules	O
for	O
classifying	O
soybean	B
diseases	O
were	O
inductively	O
derived	O
from	O
a	O
training	O
set	O
of	O
290	O
records	O
.	O
each	O
comprised	O
a	O
description	O
in	O
the	O
form	O
of	O
35	O
attribute-values	O
,	O
together	O
with	O
a	O
conﬁrmed	O
allocation	O
to	O
one	O
or	O
another	O
of	O
15	O
main	O
soybean	B
diseases	O
.	O
when	O
used	O
to	O
k1n	O
6n5	O
,	O
canada	O
;	O
donald	O
michie	O
,	O
academic	O
research	O
associates	O
,	O
6	O
inveralmond	O
grove	O
,	O
edinburgh	O
eh4	O
6ra	O
,	O
addresses	O
for	O
correspondence	O
:	O
cao	O
feng	O
,	O
department	O
of	O
computer	O
science	O
,	O
university	O
of	O
ottowa	O
,	O
ottowa	O
,	O
u.k.	O
this	O
chapter	O
conﬁnes	O
itself	O
to	O
a	O
subset	O
of	O
machine	O
learning	O
algorithms	O
,	O
i.e	O
.	O
those	O
that	O
output	B
propositional	O
classiﬁers	O
.	O
inductive	O
logic	O
programming	O
(	O
ilp	O
)	O
uses	O
the	O
symbol	O
system	O
of	O
predicate	O
(	O
as	O
opposed	O
to	O
propositional	O
)	O
logic	O
,	O
and	O
is	O
described	O
in	O
chapter	O
12	O
sec	O
.	O
5.1	O
]	O
rules	O
and	O
trees	O
from	O
data	O
:	O
ﬁrst	O
principles	O
51	O
classify	O
340	O
or	O
so	O
new	O
cases	O
,	O
machine-learned	O
rules	O
proved	O
to	O
be	O
markedly	O
more	O
accurate	O
than	O
the	O
best	O
existing	O
rules	O
used	O
by	O
soybean	B
experts	O
.	O
as	O
important	O
as	O
a	O
good	O
ﬁt	O
to	O
the	O
data	O
,	O
is	O
a	O
property	O
that	O
can	O
be	O
termed	O
“	O
mental	B
ﬁt	I
”	O
.	O
as	O
statisticians	O
,	O
breiman	O
and	O
colleagues	O
(	O
1984	O
)	O
see	O
data-derived	O
classiﬁcations	O
as	O
serving	O
“	O
two	O
purposes	O
:	O
(	O
1	O
)	O
to	O
predict	O
the	O
response	O
variable	O
corresponding	O
to	O
future	O
measurement	O
vectors	O
as	O
accurately	O
as	O
possible	O
;	O
(	O
2	O
)	O
to	O
understand	O
the	O
structural	O
relationships	O
between	O
the	O
response	O
and	O
the	O
measured	O
variables.	O
”	O
ml	O
takes	O
purpose	O
(	O
2	O
)	O
one	O
step	O
further	O
.	O
the	O
soybean	B
rules	O
were	O
sufﬁciently	O
meaningful	O
to	O
the	O
plant	O
pathologist	O
associated	O
with	O
the	O
project	O
that	O
he	O
eventually	O
adopted	O
them	O
in	O
place	O
of	O
his	O
own	O
previous	O
reference	O
set	O
.	O
ml	O
requires	O
that	O
classiﬁers	O
should	O
not	O
only	O
classify	O
but	O
should	O
also	O
constitute	O
explicit	O
concepts	O
,	O
that	O
is	O
,	O
expressions	O
in	O
symbolic	O
form	O
meaningful	O
to	O
humans	O
and	O
evaluable	O
in	O
the	O
head	O
.	O
we	O
need	O
to	O
dispose	O
of	O
confusion	O
between	O
the	O
kinds	O
of	O
computer-aided	O
descriptions	O
which	O
form	O
the	O
ml	O
practitioner	O
’	O
s	O
goal	O
and	O
those	O
in	O
view	O
by	O
statisticians	O
.	O
knowledge-	O
compilations	O
,	O
“	O
meaningful	O
to	O
humans	O
and	O
evaluable	O
in	O
the	O
head	O
”	O
,	O
are	O
available	O
in	O
michalski	O
&	O
chilausky	O
’	O
s	O
paper	O
(	O
their	O
appendix	O
2	O
)	O
,	O
and	O
in	O
shapiro	O
&	O
michie	O
(	O
1986	O
,	O
their	O
appendix	O
b	O
)	O
in	O
shapiro	O
(	O
1987	O
,	O
his	O
appendix	O
a	O
)	O
,	O
and	O
in	O
bratko	O
,	O
mozetic	O
&	O
lavrac	O
(	O
1989	O
,	O
their	O
appendix	O
a	O
)	O
,	O
among	O
other	O
sources	O
.	O
a	O
glance	O
at	O
any	O
of	O
these	O
computer-authored	O
constructions	O
will	O
sufﬁce	O
to	O
show	O
their	O
remoteness	O
from	O
the	O
main-stream	O
of	O
statistics	O
and	O
its	O
goals	O
.	O
yet	O
ml	O
practitioners	O
increasingly	O
need	O
to	O
assimilate	O
and	O
use	O
statistical	B
techniques	O
.	O
once	O
they	O
are	O
ready	O
to	O
go	O
it	O
alone	O
,	O
machine	O
learned	O
bodies	O
of	O
knowledge	O
typically	O
need	O
little	O
further	O
human	O
intervention	O
.	O
but	O
a	O
substantial	O
synthesis	O
may	O
require	O
months	O
or	O
years	O
of	O
prior	O
interactive	O
work	O
,	O
ﬁrst	O
to	O
shape	O
and	O
test	O
the	O
overall	O
logic	O
,	O
then	O
to	O
develop	O
suitable	O
sets	O
of	O
attributes	O
and	O
deﬁnitions	O
,	O
and	O
ﬁnally	O
to	O
select	O
or	O
synthesize	O
voluminous	O
data	O
ﬁles	O
as	O
training	O
material	O
.	O
this	O
contrast	O
has	O
engendered	O
confusion	O
as	O
to	O
the	O
role	O
of	O
human	O
interaction	O
.	O
like	O
music	O
teachers	O
,	O
ml	O
engineers	O
abstain	O
from	O
interaction	O
only	O
when	O
their	O
pupil	O
reaches	O
the	O
concert	O
hall	O
.	O
thereafter	O
abstention	O
is	O
total	O
,	O
clearing	O
the	O
way	O
for	O
new	O
forms	O
of	O
interaction	O
intrinsic	O
to	O
the	O
pupil	O
’	O
s	O
delivery	O
of	O
what	O
has	O
been	O
acquired	O
.	O
but	O
during	O
the	O
process	O
of	O
extracting	O
descriptions	O
from	O
data	O
the	O
working	O
method	O
of	O
ml	O
engineers	O
resemble	O
that	O
of	O
any	O
other	O
data	O
analyst	O
,	O
being	O
essentially	O
iterative	O
and	O
interactive	O
.	O
in	O
ml	O
the	O
“	O
knowledge	O
”	O
orientation	O
is	O
so	O
important	O
that	O
data-derived	O
classiﬁers	O
,	O
however	O
accurate	O
,	O
are	O
not	O
ordinarily	O
acceptable	O
in	O
the	O
absence	O
of	O
mental	B
ﬁt	I
.	O
the	O
reader	O
should	O
bear	O
this	O
point	O
in	O
mind	O
when	O
evaluating	O
empirical	O
studies	O
reported	O
elsewhere	O
in	O
this	O
book	O
.	O
statlog	O
’	O
s	O
use	O
of	O
ml	O
algorithms	O
has	O
not	O
always	O
conformed	O
to	O
purpose	O
(	O
2	O
)	O
above	O
.	O
hence	O
the	O
reader	O
is	O
warned	O
that	O
the	O
book	O
’	O
s	O
use	O
of	O
the	O
phrase	O
“	O
machine	O
learning	O
”	O
in	O
such	O
contexts	O
is	O
by	O
courtesy	O
and	O
convenience	O
only	O
.	O
the	O
michalski-chilausky	O
soybean	B
experiment	O
exempliﬁes	O
supervised	O
learning	O
,	O
given	O
:	O
a	O
sample	O
of	O
input-output	O
pairs	O
of	O
an	O
unknown	O
class-membership	O
function	O
,	O
required	O
:	O
a	O
conjectured	O
reconstruction	O
of	O
the	O
function	O
in	O
the	O
form	O
of	O
a	O
rule-based	O
expression	O
human-evaluable	O
over	O
the	O
domain	O
.	O
note	O
that	O
the	O
function	O
’	O
s	O
output-set	O
is	O
unordered	O
(	O
i.e	O
.	O
consisting	O
of	O
categoric	O
rather	O
than	O
numerical	O
values	O
)	O
and	O
its	O
outputs	O
are	O
taken	O
to	O
be	O
names	O
of	O
classes	O
.	O
the	O
derived	O
function-	O
expression	O
is	O
then	O
a	O
classiﬁer	B
.	O
in	O
contrast	O
to	O
the	O
prediction	O
of	O
numerical	O
quantities	O
,	O
this	O
book	O
conﬁnes	O
itself	O
to	O
the	O
classiﬁcation	B
problem	O
and	O
follows	O
a	O
scheme	O
depicted	O
in	O
figure	O
5.1.	O
constructing	O
ml-type	O
expressions	O
from	O
sample	O
data	O
is	O
known	O
as	O
“	O
concept	O
learning	O
”	O
.	O
52	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
[	O
ch	O
.	O
5	O
t	O
r	O
a	O
i	O
n	O
i	O
n	O
g	O
d	O
a	O
t	O
a	O
l	O
e	O
a	O
r	O
n	O
i	O
n	O
g	O
a	O
l	O
g	O
o	O
r	O
i	O
t	O
h	O
m	O
c	O
l	O
a	O
s	O
s	O
i	O
f	O
i	O
c	O
a	O
t	O
i	O
o	O
n	O
r	O
u	O
l	O
e	O
s	O
t	O
e	O
s	O
t	O
i	O
n	O
g	O
d	O
a	O
t	O
a	O
fig	O
.	O
5.1	O
:	O
classiﬁcation	B
process	O
from	O
training	O
to	O
testing	O
.	O
the	O
ﬁrst	O
such	O
learner	O
was	O
described	O
by	O
earl	O
hunt	O
(	O
1962	O
)	O
.	O
this	O
was	O
followed	O
by	O
hunt	O
,	O
marin	O
&	O
stone	O
’	O
s	O
(	O
1966	O
)	O
cls	O
.	O
the	O
acronym	O
stands	O
for	O
“	O
concept	O
learning	O
system	O
”	O
.	O
in	O
ml	O
,	O
the	O
requirement	O
for	O
user-transparency	O
imparts	O
a	O
bias	O
towards	O
logical	O
,	O
in	O
preference	O
to	O
arithmetical	O
,	O
combinations	O
of	O
attributes	O
.	O
connectives	O
such	O
as	O
“	O
and	O
”	O
,	O
“	O
or	O
”	O
,	O
and	O
“	O
if-then	O
”	O
supply	O
the	O
glue	O
for	O
building	O
rule-structured	O
classiﬁers	O
,	O
as	O
in	O
the	O
following	O
englished	O
form	O
of	O
a	O
rule	O
from	O
michalski	O
and	O
chilausky	O
’	O
s	O
soybean	B
study	O
.	O
if	O
then	O
leaf	O
malformation	O
is	O
absent	O
and	O
stem	O
is	O
abnormal	O
and	O
internal	O
discoloration	O
is	O
black	O
diagnosis	O
is	O
charcoal	O
rot	O
example	B
cases	O
(	O
the	O
“	O
training	O
set	O
”	O
or	O
“	O
learning	O
sample	O
”	O
)	O
are	O
represented	O
as	O
vectors	O
of	O
attribute-values	O
paired	O
with	O
class	O
names	O
.	O
the	O
generic	O
problem	O
is	O
to	O
ﬁnd	O
an	O
expression	O
that	O
predicts	O
the	O
classes	O
of	O
new	O
cases	O
(	O
the	O
“	O
test	O
set	O
”	O
)	O
taken	O
at	O
random	O
from	O
the	O
same	O
population	O
.	O
goodness	O
of	O
agreement	O
between	O
the	O
true	O
classes	O
and	O
the	O
classes	O
picked	O
by	O
the	O
classiﬁer	B
is	O
then	O
used	O
to	O
measure	B
accuracy	O
.	O
an	O
underlying	O
assumption	O
is	O
that	O
either	O
training	O
and	O
test	O
sets	O
are	O
randomly	O
sampled	O
from	O
the	O
same	O
data	O
source	O
,	O
or	O
full	O
statistical	B
allowance	O
can	O
be	O
made	O
for	O
departures	O
from	O
such	O
a	O
regime	O
.	O
symbolic	B
learning	I
is	O
used	O
for	O
the	O
computer-based	O
construction	O
of	O
bodies	O
of	O
articulate	O
expertise	O
in	O
domains	O
which	O
lie	O
partly	O
at	O
least	O
beyond	O
the	O
introspective	O
reach	O
of	O
domain	O
experts	O
.	O
thus	O
the	O
above	O
rule	O
was	O
not	O
of	O
human	O
expert	O
authorship	O
,	O
although	O
an	O
expert	O
can	O
assimilate	O
it	O
and	O
pass	O
it	O
on	O
.	O
to	O
ascend	O
an	O
order	O
of	O
magnitude	O
in	O
scale	O
,	O
kardio	O
’	O
s	O
comprehensive	O
treatise	O
on	O
ecg	O
interpretation	O
(	O
bratko	O
et	O
al.	O
,	O
1989	O
)	O
does	O
not	O
contain	O
a	O
single	O
rule	O
of	O
human	O
authorship	O
.	O
above	O
the	O
level	O
of	O
primitive	O
descriptors	O
,	O
every	O
formu-	O
lation	O
was	O
data-derived	O
,	O
and	O
every	O
data	O
item	O
was	O
generated	O
from	O
a	O
computable	O
logic	O
of	O
heart/electrocardiograph	O
interaction	O
.	O
independently	O
constructed	O
statistical	B
diagnosis	O
sys-	O
tems	O
are	O
commercially	O
available	O
in	O
computer-driven	O
ecg	O
kits	O
,	O
and	O
exhibit	O
accuracies	O
in	O
the	O
80	O
%	O
–	O
90	O
%	O
range	O
.	O
here	O
the	O
ml	O
product	O
scores	O
higher	O
,	O
being	O
subject	O
to	O
error	O
only	O
if	O
the	O
initial	O
logical	O
model	O
contained	O
ﬂaws	O
.	O
none	O
have	O
yet	O
come	O
to	O
light	O
.	O
but	O
the	O
difference	O
that	O
illuminates	O
the	O
distinctive	O
nature	O
of	O
symbolic	O
ml	O
concerns	O
mental	B
ﬁt	I
.	O
because	O
of	O
its	O
mode	O
of	O
construction	O
,	O
kardio	O
is	O
able	O
to	O
support	O
its	O
decisions	O
with	O
insight	O
into	O
causes	O
.	O
statistically	O
derived	O
systems	O
do	O
not	O
.	O
however	O
,	O
developments	O
of	O
bayesian	O
treatments	O
ini-	O
sec	O
.	O
5.1	O
]	O
rules	O
and	O
trees	O
from	O
data	O
:	O
ﬁrst	O
principles	O
53	O
tiated	O
by	O
ml-leaning	O
statisticians	O
(	O
see	O
spiegelhalter	O
,	O
1986	O
)	O
and	O
statistically	O
inclined	O
ml	O
theorists	O
(	O
see	O
pearl	O
,	O
1988	O
)	O
may	O
change	O
this	O
.	O
although	O
marching	O
to	O
a	O
different	O
drum	O
,	O
ml	O
people	O
have	O
for	O
some	O
time	O
been	O
seen	O
as	O
a	O
possibly	O
useful	O
source	O
of	O
algorithms	O
for	O
certain	O
data-analyses	O
required	O
in	O
industry	O
.	O
there	O
are	O
two	O
broad	O
circumstances	O
that	O
might	O
favour	O
applicability	O
:	O
1	O
.	O
2.	O
categorical	O
rather	O
than	O
numerical	O
attributes	O
;	O
strong	O
and	O
pervasive	O
conditional	O
dependencies	O
among	O
attributes	O
.	O
as	O
an	O
example	B
of	O
what	O
is	O
meant	O
by	O
a	O
conditional	O
dependency	O
,	O
let	O
us	O
take	O
the	O
classiﬁcation	B
of	O
vertebrates	O
and	O
consider	O
two	O
variables	O
,	O
namely	O
“	O
breeding-ground	O
”	O
(	O
values	O
:	O
sea	O
,	O
fresh-	O
water	O
,	O
land	O
)	O
and	O
“	O
skin-covering	O
”	O
(	O
values	O
:	O
scales	O
,	O
feathers	O
,	O
hair	O
,	O
none	O
)	O
.	O
as	O
a	O
value	O
for	O
the	O
ﬁrst	O
,	O
“	O
sea	O
”	O
votes	O
overwhelmingly	O
for	O
fish	O
.	O
if	O
the	O
second	O
attribute	O
has	O
the	O
value	O
“	O
none	O
”	O
,	O
then	O
on	O
its	O
own	O
this	O
would	O
virtually	O
clinch	O
the	O
case	O
for	O
amphibian	O
.	O
but	O
in	O
combination	O
with	O
“	O
breeding-ground	O
=	O
sea	O
”	O
it	O
switches	O
identiﬁcation	O
decisively	O
to	O
mammal	O
.	O
whales	O
and	O
some	O
other	O
sea	O
mammals	O
now	O
remain	O
the	O
only	O
possibility	O
.	O
“	O
breeding-ground	O
”	O
and	O
“	O
skin-covering	O
”	O
are	O
said	O
to	O
exhibit	O
strong	O
conditional	O
dependency	O
.	O
problems	O
characterised	O
by	O
violent	O
attribute-interactions	O
of	O
this	O
kind	O
can	O
sometimes	O
be	O
important	O
in	O
industry	O
.	O
in	O
predicting	O
automobile	O
accident	O
risks	O
,	O
for	O
example	B
,	O
information	O
that	O
a	O
driver	O
is	O
in	O
the	O
age-	O
group	O
17	O
–	O
23	O
acquires	O
great	O
signiﬁcance	O
if	O
and	O
only	O
if	O
sex	O
=	O
male	O
.	O
to	O
examine	O
the	O
“	O
horses	O
for	O
courses	O
”	O
aspect	O
of	O
comparisons	O
between	O
ml	O
,	O
neural-net	O
and	O
statistical	B
algorithms	O
,	O
a	O
reasonable	O
principle	O
might	O
be	O
to	O
select	O
datasets	O
approximately	O
evenly	O
among	O
four	O
main	O
categories	O
as	O
shown	O
in	O
figure	O
5.2.	O
conditional	O
dependencies	O
strong	O
and	O
pervasive	O
weak	O
or	O
absent	O
attributes	O
all	O
or	O
mainly	O
categorical	O
all	O
or	O
mainly	O
numerical	O
+	O
+	O
(	O
+	O
)	O
(	O
-	O
)	O
key	O
:	O
+	O
(	O
+	O
)	O
(	O
-	O
)	O
ml	O
expected	O
to	O
do	O
well	O
ml	O
expected	O
to	O
do	O
well	O
,	O
marginally	O
ml	O
expected	O
to	O
do	O
poorly	O
,	O
marginally	O
fig	O
.	O
5.2	O
:	O
relative	O
performance	O
of	O
ml	O
algorithms	O
.	O
in	O
statlog	O
,	O
collection	O
of	O
datasets	O
necessarily	O
followed	O
opportunity	O
rather	O
than	O
design	O
,	O
so	O
that	O
for	O
light	O
upon	O
these	O
particular	O
contrasts	O
the	O
reader	O
will	O
ﬁnd	O
much	O
that	O
is	O
suggestive	O
,	O
but	O
less	O
that	O
is	O
clear-cut	O
.	O
attention	O
is	O
,	O
however	O
,	O
called	O
to	O
the	O
appendices	O
which	O
contain	O
additional	O
information	O
for	O
readers	O
interested	O
in	O
following	O
up	O
particular	O
algorithms	O
and	O
datasets	O
for	O
themselves	O
.	O
classiﬁcation	B
learning	O
is	O
characterised	O
by	O
(	O
i	O
)	O
the	O
data-description	O
language	O
,	O
(	O
ii	O
)	O
the	O
language	O
for	O
expressing	O
the	O
classiﬁer	B
,	O
–	O
i.e	O
.	O
as	O
formulae	O
,	O
rules	O
,	O
etc	O
.	O
and	O
(	O
iii	O
)	O
the	O
learning	O
algorithm	O
itself	O
.	O
of	O
these	O
,	O
(	O
i	O
)	O
and	O
(	O
ii	O
)	O
correspond	O
to	O
the	O
“	O
observation	O
language	O
”	O
and	O
54	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
[	O
ch	O
.	O
5	O
“	O
hypothesis	O
language	O
”	O
respectively	O
of	O
section	O
12.2.	O
under	O
(	O
ii	O
)	O
we	O
consider	O
in	O
the	O
present	O
chapter	O
the	O
machine	O
learning	O
of	O
if-then	O
rule-sets	O
and	O
of	O
decision	O
trees	O
.	O
the	O
two	O
kinds	O
of	O
language	O
are	O
interconvertible	O
,	O
and	O
group	O
themselves	O
around	O
two	O
broad	O
inductive	O
inference	O
strategies	O
,	O
namely	O
speciﬁc-to-general	O
and	O
general-to-speciﬁc	O
5.1.2	O
speciﬁc-to-general	O
:	O
a	O
paradigm	O
for	O
rule-learning	O
michalski	O
’	O
s	O
aq11	O
and	O
related	O
algorithms	O
were	O
inspired	O
by	O
methods	O
used	O
by	O
electrical	O
en-	O
gineers	O
for	O
simplifying	O
boolean	O
circuits	O
(	O
see	O
,	O
for	O
example	B
,	O
higonnet	O
&	O
grea	O
,	O
1958	O
)	O
.	O
they	O
exemplify	O
the	O
speciﬁc-to-general	O
,	O
and	O
typically	O
start	O
with	O
a	O
maximally	O
speciﬁc	O
rule	O
for	O
assigning	O
cases	O
to	O
a	O
given	O
class	O
,	O
–	O
for	O
example	B
to	O
the	O
class	O
mammal	O
in	O
a	O
taxonomy	O
of	O
vertebrates	O
.	O
such	O
a	O
“	O
seed	O
”	O
,	O
as	O
the	O
starting	O
rule	O
is	O
called	O
,	O
speciﬁes	O
a	O
value	O
for	O
every	O
member	O
of	O
the	O
set	O
of	O
attributes	O
characterizing	O
the	O
problem	O
,	O
for	O
example	B
rule	O
1.123456789	O
if	O
skin-covering	O
=	O
hair	O
,	O
breathing	O
=	O
lungs	O
,	O
tail	O
=	O
none	O
,	O
can-ﬂy	O
=	O
y	O
,	O
reproduction	O
=	O
viviparous	O
,	O
legs	O
=	O
y	O
,	O
warm-blooded	O
=	O
y	O
,	O
diet	O
=	O
carnivorous	O
,	O
activity	O
=	O
nocturnal	O
then	O
mammal	O
.	O
we	O
now	O
take	O
the	O
reader	O
through	O
the	O
basics	O
of	O
speciﬁc-to-general	O
rule	O
learning	O
.	O
as	O
a	O
mini-	O
malist	O
tutorial	O
exercise	O
we	O
shall	O
build	O
a	O
mammal-recogniser	O
.	O
the	O
initial	O
rule	O
,	O
numbered	O
1.123456789	O
in	O
the	O
above	O
,	O
is	O
so	O
speciﬁc	O
as	O
probably	O
to	O
be	O
capable	O
only	O
of	O
recognising	O
bats	O
.	O
speciﬁcity	O
is	O
relaxed	O
by	O
dropping	O
attributes	O
one	O
at	O
a	O
time	O
,	O
thus	O
:	O
rule	O
1.23456789	O
rule	O
1.13456789	O
rule	O
1.12456789	O
rule	O
1.12356789	O
rule	O
1.12346789	O
if	O
breathing	O
=	O
lungs	O
,	O
tail	O
=	O
none	O
,	O
can-ﬂy	O
=	O
y	O
,	O
reproduction	O
=	O
viviparous	O
,	O
legs	O
=	O
y	O
,	O
warm-blooded	O
=	O
y	O
,	O
diet	O
=	O
carnivorous	O
,	O
ac-	O
tivity	O
=	O
nocturnal	O
then	O
mammal	O
;	O
if	O
skin-covering	O
=	O
hair	O
,	O
tail	O
=	O
none	O
,	O
can-ﬂy	O
=	O
y	O
,	O
reproduction	O
=	O
viviparous	O
,	O
legs	O
=	O
y	O
,	O
warm-blooded	O
=	O
y	O
,	O
diet	O
=	O
carnivorous	O
,	O
activity	O
=	O
nocturnal	O
then	O
mammal	O
;	O
if	O
skin-covering	O
=	O
hair	O
,	O
breathing	O
=	O
lungs	O
,	O
can-ﬂy	O
=	O
y	O
,	O
reproduction	O
=	O
viviparous	O
,	O
legs	O
=	O
y	O
,	O
warm-blooded	O
=	O
y	O
,	O
diet	O
=	O
carnivorous	O
,	O
activity	O
=	O
nocturnal	O
then	O
mammal	O
;	O
if	O
skin-covering	O
=	O
hair	O
,	O
breathing	O
=	O
lungs	O
,	O
tail	O
=	O
none	O
,	O
reproduction	O
=	O
viviparous	O
,	O
legs	O
=	O
y	O
,	O
warm-blooded	O
=	O
y	O
,	O
diet	O
=	O
carnivorous	O
,	O
activity	O
=	O
nocturnal	O
thenmammal	O
;	O
if	O
skin-covering	O
=	O
hair	O
,	O
breathing	O
=	O
lungs	O
,	O
tail	O
=	O
none	O
,	O
can-ﬂy	O
=	O
y	O
,	O
legs	O
=	O
y	O
,	O
warm-blooded	O
=	O
y	O
,	O
diet	O
=	O
carnivorous	O
,	O
activity	O
=	O
nocturnal	O
bf	O
then	O
mammal	O
;	O
and	O
so	O
on	O
for	O
all	O
the	O
ways	O
of	O
dropping	O
a	O
single	O
attribute	O
,	O
followed	O
by	O
all	O
the	O
ways	O
of	O
drop-	O
ping	O
two	O
attributes	O
,	O
three	O
attributes	O
etc	O
.	O
any	O
rule	O
which	O
includes	O
in	O
its	O
cover	O
a	O
“	O
negative	O
example	B
”	O
,	O
i.e	O
.	O
a	O
non-mammal	O
,	O
is	O
incorrect	O
and	O
is	O
discarded	O
during	O
the	O
process	O
.	O
the	O
cycle	O
terminates	O
by	O
saving	O
a	O
set	O
of	O
shortest	O
rules	O
covering	O
only	O
mammals	O
.	O
as	O
a	O
classiﬁer	B
,	O
such	O
a	O
set	O
is	O
guaranteed	O
correct	O
,	O
but	O
can	O
not	O
be	O
guaranteed	O
complete	O
,	O
as	O
we	O
shall	O
see	O
later	O
.	O
sec	O
.	O
5.1	O
]	O
rules	O
and	O
trees	O
from	O
data	O
:	O
ﬁrst	O
principles	O
55	O
in	O
the	O
present	O
case	O
the	O
terminating	O
set	O
has	O
the	O
single-attribute	O
description	O
:	O
rule	O
1.1	O
if	O
skin-covering	O
=	O
hair	O
then	O
mammal	O
;	O
the	O
process	O
now	O
iterates	O
using	O
a	O
new	O
“	O
seed	O
”	O
for	O
each	O
iteration	O
,	O
for	O
example	B
:	O
rule	O
2.123456789	O
if	O
skin-covering	O
=	O
none	O
,	O
breathing	O
=	O
lungs	O
,	O
tail	O
=	O
none	O
,	O
can-ﬂy	O
=	O
n	O
,	O
reproduction	O
=	O
viviparous	O
,	O
legs	O
=	O
n	O
,	O
warm-blooded	O
=	O
y	O
,	O
diet	O
=	O
mixed	O
,	O
activity	O
=	O
diurnal	O
then	O
mammal	O
;	O
leading	O
to	O
the	O
following	O
set	O
of	O
shortest	O
rules	O
:	O
rule	O
2.15	O
rule	O
2.17	O
rule	O
2.67	O
rule	O
2.57	O
if	O
skin-covering	O
=	O
none	O
,	O
reproduction	O
=	O
viviparous	O
then	O
mammal	O
;	O
if	O
skin-covering	O
=	O
none	O
,	O
warm-blooded	O
=	O
y	O
then	O
mammal	O
;	O
if	O
legs	O
=	O
n	O
,	O
warm-blooded	O
=	O
y	O
then	O
mammal	O
;	O
if	O
reproduction	O
=	O
viviparous	O
,	O
warm-blooded	O
=	O
y	O
then	O
mammal	O
;	O
of	O
these	O
,	O
the	O
ﬁrst	O
covers	O
naked	O
mammals	O
.	O
amphibians	O
,	O
although	O
uniformly	O
naked	O
,	O
are	O
oviparous	O
.	O
the	O
second	O
has	O
the	O
same	O
cover	O
,	O
since	O
amphibians	O
are	O
not	O
warm-blooded	O
,	O
and	O
birds	O
,	O
although	O
warm-blooded	O
,	O
are	O
not	O
naked	O
(	O
we	O
assume	O
that	O
classiﬁcation	B
is	O
done	O
on	O
adult	O
forms	O
)	O
.	O
the	O
third	O
covers	O
various	O
naked	O
marine	O
mammals	O
.	O
so	O
far	O
,	O
these	O
rules	O
collectively	O
contribute	O
little	O
information	O
,	O
merely	O
covering	O
a	O
few	O
overlapping	O
pieces	O
of	O
a	O
large	O
patch-	O
work	O
.	O
but	O
the	O
last	O
rule	O
at	O
a	O
stroke	O
covers	O
almost	O
the	O
whole	O
class	O
of	O
mammals	O
.	O
every	O
attempt	O
at	O
further	O
generalisation	O
now	O
encounters	O
negative	O
examples	O
.	O
dropping	O
“	O
warm-blooded	O
”	O
causes	O
the	O
rule	O
to	O
cover	O
viviparous	O
groups	O
of	O
ﬁsh	O
and	O
of	O
reptiles	O
.	O
dropping	O
“	O
viviparous	O
”	O
causes	O
the	O
rule	O
to	O
cover	O
birds	O
,	O
unacceptable	O
in	O
a	O
mammal-recogniser	O
.	O
but	O
it	O
also	O
has	O
the	O
effect	O
of	O
including	O
the	O
egg-laying	O
mammals	O
“	O
monotremes	O
”	O
,	O
consisting	O
of	O
the	O
duck-billed	O
platypus	O
and	O
two	O
species	O
of	O
spiny	O
ant-eaters	O
.	O
rule	O
2.57	O
fails	O
to	O
cover	O
these	O
,	O
and	O
is	O
thus	O
an	O
instance	O
of	O
the	O
earlier-mentioned	O
kind	O
of	O
classiﬁer	B
that	O
can	O
be	O
guaranteed	O
correct	O
,	O
but	O
can	O
not	O
be	O
guaranteed	O
complete	O
.	O
conversion	O
into	O
a	O
complete	O
and	O
correct	O
classiﬁer	B
is	O
not	O
an	O
option	O
for	O
this	O
purely	O
speciﬁc-to-general	O
process	O
,	O
since	O
we	O
have	O
run	O
out	O
of	O
permissible	O
generalisations	O
.	O
the	O
construction	O
of	O
rule	O
2.57	O
has	O
thus	O
stalled	O
in	O
sight	O
of	O
the	O
ﬁnishing	O
line	O
.	O
but	O
linking	O
two	O
or	O
more	O
rules	O
together	O
,	O
each	O
correct	O
but	O
not	O
complete	O
,	O
can	O
effect	O
the	O
desired	O
result	O
.	O
below	O
we	O
combine	O
the	O
rule	O
yielded	O
by	O
the	O
ﬁrst	O
iteration	O
with	O
,	O
in	O
turn	O
,	O
the	O
ﬁrst	O
and	O
the	O
second	O
rule	O
obtained	O
from	O
the	O
second	O
iteration	O
:	O
rule	O
1.1	O
rule	O
2.15	O
rule	O
1.1	O
rule	O
2.17	O
if	O
skin-covering	O
=	O
hair	O
then	O
mammal	O
;	O
if	O
skin-covering	O
=	O
none	O
,	O
reproduction	O
=	O
viviparous	O
then	O
mammal	O
;	O
if	O
skin-covering	O
=	O
hair	O
then	O
mammal	O
;	O
if	O
skin-covering	O
=	O
none	O
,	O
warm-blooded	O
=	O
y	O
then	O
mammal	O
;	O
these	O
can	O
equivalently	O
be	O
written	O
as	O
disjunctive	O
rules	O
:	O
56	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
[	O
ch	O
.	O
5	O
if	O
or	O
then	O
and	O
if	O
or	O
then	O
skin-covering	O
=	O
hair	O
skin-covering	O
=	O
none	O
,	O
reproduction	O
=	O
viviparous	O
mammal	O
;	O
skin-covering	O
=	O
hair	O
skin-covering	O
=	O
none	O
,	O
warm-blooded	O
=	O
y	O
mammal	O
;	O
in	O
rule	O
induction	O
,	O
following	O
michalski	O
,	O
an	O
attribute-test	O
is	O
called	O
a	O
selector	O
,	O
a	O
conjunction	O
of	O
selectors	O
is	O
a	O
complex	O
,	O
and	O
a	O
disjunction	O
of	O
complexes	O
is	O
called	O
a	O
cover	O
.	O
if	O
a	O
rule	O
is	O
true	O
of	O
an	O
example	B
we	O
say	O
that	O
it	O
covers	O
the	O
example	B
.	O
rule	O
learning	O
systems	O
in	O
practical	O
use	O
qualify	O
and	O
elaborate	O
the	O
above	O
simple	O
scheme	O
,	O
including	O
by	O
assigning	O
a	O
prominent	O
role	O
to	O
general-to-speciﬁc	O
processes	O
.	O
in	O
the	O
statlog	O
experiment	O
such	O
algorithms	O
are	O
exempliﬁed	O
by	O
cn2	O
(	O
clarke	O
&	O
niblett	O
,	O
1989	O
)	O
and	O
itrule	O
.	O
both	O
generate	O
decision	O
rules	O
for	O
each	O
class	O
in	O
turn	O
,	O
for	O
each	O
class	O
starting	O
with	O
a	O
universal	O
rule	O
which	O
assigns	O
all	O
examples	O
to	O
the	O
current	O
class	O
.	O
this	O
rule	O
ought	O
to	O
cover	O
at	O
least	O
one	O
of	O
the	O
examples	O
belonging	O
to	O
that	O
class	O
.	O
specialisations	O
are	O
then	O
repeatedly	O
generated	O
and	O
explored	O
until	O
all	O
rules	O
consistent	O
with	O
the	O
data	O
are	O
found	O
.	O
each	O
rule	O
must	O
correctly	O
classify	O
at	O
least	O
a	O
prespeciﬁed	O
percentage	O
of	O
the	O
examples	O
belonging	O
to	O
the	O
current	O
class	O
.	O
as	O
few	O
as	O
possible	O
negative	O
examples	O
,	O
i.e	O
.	O
examples	O
in	O
other	O
classes	O
,	O
should	O
be	O
covered	O
.	O
specialisations	O
are	O
obtained	O
by	O
adding	O
a	O
condition	O
to	O
the	O
left-hand	O
side	O
of	O
the	O
rule	O
.	O
cn2	O
is	O
an	O
extension	O
of	O
michalski	O
’	O
s	O
(	O
1969	O
)	O
algorithm	O
aq	O
with	O
several	O
techniques	O
to	O
is	O
the	O
number	O
classiﬁed	O
incorrectly	O
,	O
and	O
c	O
is	O
the	O
total	O
number	O
of	O
classes	O
.	O
process	O
noise	O
in	O
the	O
data	O
.	O
the	O
main	O
technique	O
for	O
reducing	O
error	O
is	O
to	O
minimise	O
cd	O
;	O
e	O
fdgih	O
g	O
(	O
laplacian	O
function	O
)	O
where	O
k	O
is	O
the	O
number	O
of	O
examples	O
classiﬁed	O
correctly	O
cdjelkenm	O
by	O
a	O
rule	O
,	O
k	O
itrule	O
produces	O
rules	O
of	O
the	O
form	O
“	O
if	O
...	O
then	O
...	O
with	O
probability	O
...	O
”	O
.	O
this	O
algorithm	O
contains	O
probabilistic	O
inference	O
through	O
the	O
j-measure	O
,	O
which	O
evaluates	O
its	O
candidate	O
rules	O
.	O
j-measure	O
is	O
a	O
product	O
of	O
prior	O
probabilities	O
for	O
each	O
class	O
and	O
the	O
cross-entropy	O
of	O
class	O
values	O
conditional	O
on	O
the	O
attribute	O
values	O
.	O
itrule	O
can	O
not	O
deal	O
with	O
continuous	O
numeric	O
values	O
.	O
it	O
needs	O
accurate	O
evaluation	O
of	O
prior	O
and	O
posterior	O
probabilities	O
.	O
so	O
when	O
such	O
information	O
is	O
not	O
present	O
it	O
is	O
prone	O
to	O
misuse	O
.	O
detailed	O
accounts	O
of	O
these	O
and	O
other	O
algorithms	O
are	O
given	O
in	O
section	O
5.2	O
.	O
5.1.3	O
decision	O
trees	O
reformulation	O
of	O
the	O
mammal-recogniser	O
as	O
a	O
completed	O
decision	O
tree	O
would	O
require	O
the	O
implicit	O
“	O
else	O
not-mammal	O
”	O
to	O
be	O
made	O
explicit	O
,	O
as	O
in	O
figure	O
5.3.	O
construction	O
of	O
the	O
complete	O
outline	O
taxonomy	O
as	O
a	O
set	O
of	O
descriptive	O
concepts	O
,	O
whether	O
in	O
rule-structured	O
or	O
tree-structured	O
form	O
,	O
would	O
entail	O
repetition	O
of	O
the	O
induction	O
process	O
for	O
bird	O
,	O
reptile	O
,	O
amphibian	O
and	O
fish	O
.	O
in	O
order	O
to	O
be	O
meaningful	O
to	O
the	O
user	O
(	O
i.e	O
.	O
to	O
satisfy	O
the	O
“	O
mental	B
ﬁt	I
”	O
criterion	O
)	O
it	O
has	O
been	O
found	O
empirically	O
that	O
trees	O
should	O
be	O
as	O
small	O
and	O
as	O
linear	O
as	O
possible	O
.	O
in	O
fully	O
linear	O
trees	O
,	O
such	O
as	O
that	O
of	O
figure	O
5.3	O
,	O
an	O
internal	O
node	O
(	O
i.e	O
.	O
attribute	O
test	O
)	O
can	O
be	O
the	O
parent	O
of	O
at	O
most	O
one	O
internal	O
node	O
.	O
all	O
its	O
other	O
children	O
must	O
be	O
end-node	O
or	O
“	O
leaves	O
”	O
(	O
outcomes	O
)	O
.	O
quantitative	O
measures	O
of	O
linearity	O
are	O
discussed	O
by	O
arbab	O
&	O
michie	O
(	O
1988	O
)	O
,	O
who	O
present	O
an	O
algorithm	O
,	O
rg	O
,	O
for	O
building	O
trees	O
biased	O
towards	O
linearity	O
.	O
they	O
also	O
compare	O
rg	O
with	O
bratko	O
’	O
s	O
(	O
1983	O
)	O
aocdl	O
directed	O
towards	O
the	O
same	O
end	O
.	O
we	O
now	O
consider	O
the	O
general	O
sec	O
.	O
5.1	O
]	O
rules	O
and	O
trees	O
from	O
data	O
:	O
ﬁrst	O
principles	O
57	O
skin-covering	O
?	O
none	O
hair	O
scales	O
feathers	O
mammal	O
not-mammal	O
not-mammal	O
viviparous	O
?	O
no	O
yes	O
not-mammal	O
mammal	O
qu	O
fig	O
.	O
5.3	O
:	O
translation	O
of	O
a	O
mammal-recognising	O
rule	O
(	O
rule	O
2.15	O
,	O
see	O
text	O
)	O
into	O
tree	O
form	O
.	O
the	O
attribute-values	O
that	O
ﬁgured	O
in	O
the	O
rule-sets	O
built	O
earlier	O
are	O
here	O
set	O
larger	O
in	O
bold	O
type	O
.	O
the	O
rest	O
are	O
tagged	O
with	O
not-mammal	O
labels	O
.	O
properties	O
of	O
algorithms	O
that	O
grow	O
trees	O
from	O
data	O
.	O
5.1.4	O
general-to-speciﬁc	O
:	O
top-down	O
induction	O
of	O
trees	O
in	O
common	O
with	O
cn2	O
and	O
itrule	O
but	O
in	O
contrast	O
to	O
the	O
speciﬁc-to-general	O
earlier	O
style	O
of	O
michalski	O
’	O
s	O
aq	O
family	O
of	O
rule	O
learning	O
,	O
decision-tree	O
learning	O
is	O
general-to-speciﬁc	O
.	O
in	O
illustrating	O
with	O
the	O
vertebrate	O
taxonomy	O
example	B
we	O
will	O
assume	O
that	O
the	O
set	O
of	O
nine	O
at-	O
tributes	O
are	O
sufﬁcient	O
to	O
classify	O
without	O
error	O
all	O
vertebrate	O
species	O
into	O
one	O
of	O
mammal	O
,	O
bird	O
,	O
amphibian	O
,	O
reptile	O
,	O
fish	O
.	O
later	O
we	O
will	O
consider	O
elaborations	O
necessary	O
in	O
underspeciﬁed	O
or	O
in	O
inherently	O
“	O
noisy	O
”	O
domains	O
,	O
where	O
methods	O
from	O
statistical	B
data	O
anal-	O
ysis	O
enter	O
the	O
picture	O
.	O
as	O
shown	O
in	O
figure	O
5.4	O
,	O
the	O
starting	O
point	O
is	O
a	O
tree	O
of	O
only	O
one	O
node	O
that	O
allocates	O
all	O
cases	O
in	O
the	O
training	O
set	O
to	O
a	O
single	O
class	O
.	O
in	O
the	O
case	O
that	O
a	O
mammal-recogniser	O
is	O
required	O
,	O
this	O
default	O
class	O
could	O
be	O
not-mammal	O
.	O
the	O
presumption	O
here	O
is	O
that	O
in	O
the	O
population	O
there	O
are	O
more	O
of	O
these	O
than	O
there	O
are	O
mammals	O
.	O
unless	O
all	O
vertebrates	O
in	O
the	O
training	O
set	O
are	O
non-mammals	O
,	O
some	O
of	O
the	O
training	O
set	O
of	O
cases	O
associated	O
with	O
this	O
single	O
node	O
will	O
be	O
correctly	O
classiﬁed	O
and	O
others	O
incorrectly	O
,	O
–	O
in	O
the	O
terminology	O
of	O
breiman	O
and	O
colleagues	O
(	O
1984	O
)	O
,	O
such	O
a	O
node	O
is	O
“	O
impure	B
”	O
.	O
each	O
available	O
attribute	O
is	O
now	O
used	O
on	O
a	O
trial	O
basis	O
to	O
split	O
the	O
set	O
into	O
subsets	O
.	O
whichever	O
split	O
minimises	O
the	O
estimated	O
“	O
impurity	O
”	O
of	O
the	O
subsets	O
which	O
it	O
generates	O
is	O
retained	O
,	O
and	O
the	O
cycle	O
is	O
repeated	O
on	O
each	O
of	O
the	O
augmented	O
tree	O
’	O
s	O
end-nodes	O
.	O
numerical	O
measures	O
of	O
impurity	O
are	O
many	O
and	O
various	O
.	O
they	O
all	O
aim	O
to	O
capture	O
the	O
degree	O
to	O
which	O
expected	O
frequencies	O
of	O
belonging	O
to	O
given	O
classes	O
(	O
possibly	O
estimated	O
,	O
for	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
p	O
p	O
p	O
p	O
p	O
p	O
q	O
q	O
q	O
r	O
r	O
r	O
r	O
r	O
r	O
s	O
s	O
s	O
s	O
t	O
t	O
t	O
q	O
q	O
q	O
s	O
q	O
q	O
58	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
[	O
ch	O
.	O
5	O
example	B
,	O
in	O
the	O
two-class	O
mammal/not-mammal	O
problem	O
of	O
figure	O
5.4	O
asv	O
are	O
affected	O
by	O
knowledge	O
of	O
attribute	O
values	O
.	O
in	O
general	O
the	O
goodness	O
of	O
a	O
split	O
into	O
subsets	O
(	O
for	O
example	B
by	O
skin-covering	O
,	O
by	O
breathing	O
organs	O
,	O
by	O
tail-type	O
,	O
etc	O
.	O
)	O
is	O
the	O
weighted	O
mean	O
decrease	O
in	O
impurity	O
,	O
weights	O
being	O
proportional	O
to	O
the	O
subset	O
sizes	O
.	O
let	O
us	O
see	O
how	O
these	O
ideas	O
work	O
out	O
in	O
a	O
specimen	O
development	O
of	O
a	O
mammal-recognising	O
tree	O
.	O
to	O
facilitate	O
comparison	O
with	O
the	O
speciﬁc-to-general	O
induction	O
shown	O
earlier	O
,	O
the	O
tree	O
is	O
represented	O
in	O
figure	O
5.5	O
as	O
an	O
if-then-else	O
expression	O
.	O
we	O
underline	O
class	O
names	O
that	O
label	O
temporary	O
leaves	O
.	O
these	O
are	O
nodes	O
that	O
need	O
further	O
splitting	O
to	O
remove	O
or	O
diminish	O
impurity	O
.	O
cvwexv	O
''	O
y	O
)	O
)	O
this	O
simple	O
taxonomic	O
example	B
lacks	O
many	O
of	O
the	O
complicating	O
factors	O
encountered	O
in	O
classiﬁcation	B
generally	O
,	O
and	O
lends	O
itself	O
to	O
this	O
simplest	O
form	O
of	O
decision	O
tree	O
learning	O
.	O
complications	O
arise	O
from	O
the	O
use	O
of	O
numerical	O
attributes	O
in	O
addition	O
to	O
categorical	O
,	O
from	O
the	O
occurrence	O
of	O
error	O
,	O
and	O
from	O
the	O
occurrence	O
of	O
unequal	O
misclassiﬁcation	O
costs	O
.	O
error	O
can	O
inhere	O
in	O
the	O
values	O
of	O
attributes	O
or	O
classes	O
(	O
“	O
noise	O
”	O
)	O
,	O
or	O
the	O
domain	O
may	O
be	O
deterministic	O
,	O
yet	O
the	O
supplied	O
set	O
of	O
attributes	O
may	O
not	O
support	O
error-free	O
classiﬁcation	B
.	O
but	O
to	O
round	O
off	O
the	O
taxonomy	O
example	B
,	O
the	O
following	O
from	O
quinlan	O
(	O
1993	O
)	O
gives	O
the	O
simple	O
essence	O
of	O
tree	O
learning	O
:	O
contains	O
no	O
cases	O
:	O
	O
.	O
contains	O
cases	O
that	O
belong	O
to	O
a	O
mixture	O
of	O
classes	O
:	O
the	O
decision	O
tree	O
is	O
again	O
a	O
leaf	O
,	O
but	O
the	O
class	O
to	O
be	O
associated	O
with	O
the	O
leaf	O
.	O
for	O
example	B
,	O
the	O
leaf	O
might	O
be	O
chosen	O
in	O
accordance	O
with	O
some	O
background	O
knowledge	O
of	O
the	O
domain	O
,	O
such	O
as	O
the	O
overall	O
majority	O
class	O
.	O
to	O
construct	O
a	O
decision	O
tree	O
from	O
a	O
setz	O
of	O
training	O
cases	O
,	O
let	O
the	O
classes	O
be	O
denoted	O
{	O
}	O
|e~	O
{	O
~ede~	O
{	O
b	O
.	O
there	O
are	O
three	O
possibilities	O
:	O
contains	O
one	O
or	O
more	O
cases	O
,	O
all	O
belonging	O
to	O
a	O
single	O
class	O
{	O
b	O
;	O
is	O
a	O
leaf	O
identifying	O
class	O
{	O
the	O
decision	O
tree	O
forz	O
must	O
be	O
determined	O
from	O
information	O
other	O
thanz	O
in	O
this	O
situation	O
,	O
the	O
idea	O
is	O
to	O
reﬁnez	O
~eddi~	O
outcomes	O
	O
wherez	O
$	O
	O
contains	O
all	O
the	O
cases	O
inz	O
the	O
decision	O
tree	O
forz	O
leads	O
to	O
the	O
decision	O
tree	O
constructed	O
from	O
the	O
subsetz	O
into	O
subsets	O
of	O
cases	O
that	O
are	O
,	O
or	O
seem	O
to	O
be	O
heading	O
towards	O
,	O
single-class	O
collections	O
of	O
cases	O
.	O
a	O
test	O
is	O
chosen	O
based	O
on	O
a	O
single	O
attribute	O
,	O
that	O
has	O
two	O
or	O
more	O
mutually	O
exclusive	O
,	O
that	O
have	O
outcome	O
oi	O
of	O
the	O
chosen	O
test	O
.	O
consists	O
of	O
a	O
decision	O
node	O
identifying	O
the	O
test	O
and	O
one	O
branch	O
for	O
each	O
possible	O
outcome	O
.	O
the	O
same	O
tree-building	O
machinery	O
is	O
applied	O
recursively	O
to	O
each	O
subset	O
of	O
training	O
cases	O
,	O
so	O
that	O
the	O
ith	O
branch	O
~edd~	O
z	O
$	O
	O
	O
of	O
training	O
cases	O
.	O
.	O
	O
is	O
partitioned	O
into	O
subsetsz	O
note	O
that	O
this	O
schema	O
is	O
general	O
enough	O
to	O
include	O
multi-class	O
trees	O
,	O
raising	O
a	O
tactical	O
problem	O
in	O
approaching	O
the	O
taxonomic	O
material	O
.	O
should	O
we	O
build	O
in	O
turn	O
a	O
set	O
of	O
yes/no	O
recognizers	O
,	O
one	O
for	O
mammals	O
,	O
one	O
for	O
birds	O
,	O
one	O
for	O
reptiles	O
,	O
etc.	O
,	O
and	O
then	O
daisy-chain	O
them	O
into	O
a	O
tree	O
?	O
or	O
should	O
we	O
apply	O
the	O
full	O
multi-class	O
procedure	O
to	O
the	O
data	O
wholesale	O
,	O
risking	O
a	O
disorderly	O
scattering	O
of	O
different	O
class	O
labels	O
along	O
the	O
resulting	O
tree	O
’	O
s	O
perimeter	O
?	O
if	O
the	O
entire	O
tree-building	O
process	O
is	O
automated	O
,	O
as	O
for	O
the	O
later	O
standardised	O
comparisons	O
,	O
the	O
second	O
regime	O
is	O
mandatory	O
.	O
but	O
in	O
interactive	O
decision-tree	O
building	O
there	O
is	O
no	O
generally	O
“	O
correct	O
”	O
answer	O
.	O
the	O
analyst	O
must	O
be	O
guided	O
by	O
context	O
,	O
by	O
user-requirements	O
and	O
by	O
intermediate	O
results	O
.	O
h	O
z	O
z	O
z	O
|	O
~	O
	O
	O
z	O
|	O
~	O
z	O
	O
sec	O
.	O
5.1	O
]	O
rules	O
and	O
trees	O
from	O
data	O
:	O
ﬁrst	O
principles	O
59	O
empty	O
attribute-test	O
not-mammal	O
if	O
no	O
misclassiﬁcations	O
conﬁrm	O
leaf	O
(	O
solid	O
lines	O
)	O
if	O
misclassiﬁcations	O
occur	O
choose	O
an	O
attribute	O
for	O
splitting	O
the	O
set	O
;	O
for	O
each	O
,	O
calculate	O
a	O
purity	B
measure	O
from	O
the	O
tabulations	O
below	O
:	O
empty	O
attribute-test	O
not-mammal	O
and	O
exit	O
skin-covering	O
?	O
feathers	O
none	O
hair	O
scales	O
total	O
number	O
of	O
mammals	O
in	O
set	O
:	O
number	O
of	O
not-mammals	O
:	O
fe	O
yfe	O
sc	O
	O
no	O
	O
ha	O
	O
yno	O
yha	O
ysc	O
breathing	O
?	O
number	O
of	O
mammals	O
in	O
subset	O
number	O
of	O
not-mammals	O
lungs	O
lu	O
ylu	O
tail	O
?	O
gills	O
	O
gi	O
ygi	O
long	O
short	O
none	O
number	O
of	O
mammals	O
in	O
set	O
number	O
of	O
not-mammals	O
lo	O
ylo	O
	O
no	O
yno	O
and	O
so	O
on	O
sh	O
ysh	O
v	O
''	O
y	O
v	O
''	O
y	O
v	O
''	O
y	O
fig	O
.	O
5.4	O
:	O
first	O
stage	O
in	O
growing	O
a	O
decision	O
tree	O
from	O
a	O
training	O
set	O
.	O
the	O
single	O
end-node	O
is	O
a	O
candidate	O
to	O
be	O
a	O
leaf	O
,	O
and	O
is	O
here	O
drawn	O
with	O
broken	O
lines	O
.	O
it	O
classiﬁes	O
all	O
cases	O
to	O
not-mammal	O
.	O
if	O
correctly	O
,	O
the	O
candidate	O
is	O
conﬁrmed	O
as	O
a	O
leaf	O
.	O
otherwise	O
available	O
attribute-applications	O
are	O
tried	O
for	O
their	O
abilities	O
to	O
split	O
the	O
set	O
,	O
saving	O
for	O
incorporation	O
into	O
the	O
tree	O
whichever	O
maximises	O
some	O
chosen	O
purity	B
measure	O
.	O
each	O
saved	O
subset	O
now	O
serves	O
as	O
a	O
candidate	O
for	O
recursive	O
application	O
of	O
the	O
same	O
split-and-test	O
cycle	O
.	O
s	O
s	O
	O
s	O
	O
v	O
	O
v	O
	O
	O
v	O
60	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
[	O
ch	O
.	O
5	O
step	O
1	O
:	O
construct	O
a	O
single-leaf	O
tree	O
rooted	O
in	O
the	O
empty	O
attribute	O
test	O
:	O
if	O
(	O
)	O
then	O
not-mammal	O
step2	O
:	O
if	O
no	O
impure	B
nodes	O
then	O
exit	O
step	O
3	O
:	O
construct	O
from	O
the	O
training	O
set	O
all	O
single-attribute	O
trees	O
and	O
,	O
for	O
each	O
,	O
calculate	O
the	O
weighted	O
mean	O
impurity	O
over	O
its	O
leaves	O
;	O
step	O
4	O
:	O
retain	O
the	O
attribute	O
giving	O
least	O
impurity	O
.	O
assume	O
this	O
to	O
be	O
skin-covering	O
:	O
if	O
(	O
skin-covering	O
=	O
hair	O
)	O
then	O
mammal	O
if	O
(	O
skin-covering	O
=	O
feathers	O
)	O
then	O
not-mammal	O
if	O
(	O
skin-covering	O
=	O
scales	O
)	O
then	O
not-mammal	O
if	O
(	O
skin-covering	O
=	O
none	O
)	O
then	O
not-mammal	O
step	O
5	O
:	O
if	O
no	O
impure	B
nodes	O
then	O
exit	O
otherwise	O
apply	O
steps	O
3	O
,	O
and	O
4	O
and	O
5	O
recursively	O
to	O
each	O
impure	B
node	O
,	O
thus	O
step	O
3	O
:	O
construct	O
from	O
the	O
not-mammal	O
subset	O
of	O
step	O
4	O
all	O
single-attribute	O
trees	O
and	O
,	O
for	O
each	O
,	O
calculate	O
the	O
weighted	O
mean	O
impurity	O
over	O
its	O
leaves	O
;	O
step	O
4	O
:	O
retain	O
the	O
attribute	O
giving	O
least	O
impurity	O
.	O
perfect	O
scores	O
are	O
achieved	O
by	O
“	O
viviparous	O
”	O
and	O
by	O
“	O
warm-blooded	O
”	O
,	O
giving	O
:	O
and	O
if	O
(	O
skin-covering	O
=	O
hair	O
)	O
then	O
mammal	O
if	O
(	O
skin-covering	O
=	O
feathers	O
)	O
then	O
not-mammal	O
if	O
(	O
skin-covering	O
=	O
scales	O
)	O
then	O
not-mammal	O
if	O
(	O
skin-covering	O
=	O
none	O
)	O
then	O
if	O
(	O
reproduction	O
=	O
viviparous	O
)	O
then	O
mammal	O
else	O
not-mammal	O
if	O
(	O
skin-covering	O
=	O
hair	O
)	O
then	O
mammal	O
if	O
(	O
skin-covering	O
=	O
feathers	O
)	O
then	O
not-mammal	O
if	O
(	O
skin-covering	O
=	O
scales	O
)	O
then	O
not-mammal	O
if	O
(	O
skin-covering	O
=	O
none	O
)	O
then	O
if	O
(	O
warm-blooded	O
=	O
y	O
)	O
then	O
mammal	O
else	O
not-mammal	O
step	O
5	O
:	O
exit	O
fig	O
.	O
5.5	O
:	O
illustration	O
,	O
using	O
the	O
mammal	O
problem	O
,	O
of	O
the	O
basic	O
idea	O
of	O
decision-tree	O
induction	O
.	O
sec	O
.	O
5.1	O
]	O
rules	O
and	O
trees	O
from	O
data	O
:	O
ﬁrst	O
principles	O
61	O
either	O
way	O
,	O
the	O
crux	O
is	O
the	O
idea	O
of	O
reﬁning	O
t	O
“	O
into	O
subsets	O
of	O
cases	O
that	O
are	O
,	O
or	O
seem	O
to	O
be	O
heading	O
towards	O
,	O
single-class	O
collections	O
of	O
cases.	O
”	O
this	O
is	O
the	O
same	O
as	O
the	O
earlier	O
described	O
search	O
for	O
purity	B
.	O
departure	O
from	O
purity	B
is	O
used	O
as	O
the	O
“	O
splitting	O
criterion	O
”	O
,	O
i.e	O
.	O
as	O
the	O
basis	O
on	O
which	O
to	O
select	O
an	O
attribute	O
to	O
apply	O
to	O
the	O
members	O
of	O
a	O
less	O
pure	O
node	O
for	O
partitioning	O
it	O
into	O
purer	O
sub-nodes	O
.	O
but	O
how	O
to	O
measure	B
departure	O
from	O
purity	B
?	O
in	O
practice	O
,	O
as	O
noted	O
by	O
breiman	O
et	O
al.	O
,	O
“	O
overall	O
misclassiﬁcation	O
rate	O
is	O
not	O
sensitive	O
to	O
the	O
choice	O
of	O
a	O
splitting	O
rule	O
,	O
as	O
long	O
as	O
it	O
is	O
within	O
a	O
reasonable	O
class	O
of	O
rules.	O
”	O
for	O
a	O
more	O
general	O
consideration	O
of	O
splitting	O
criteria	O
,	O
we	O
ﬁrst	O
introduce	O
the	O
case	O
where	O
total	O
purity	B
of	O
nodes	O
is	O
not	O
attainable	O
:	O
i.e	O
.	O
some	O
or	O
all	O
of	O
the	O
leaves	O
necessarily	O
end	O
up	O
mixed	O
with	O
respect	O
to	O
class	O
membership	O
.	O
in	O
these	O
circumstances	O
the	O
term	O
“	O
noisy	O
data	O
”	O
is	O
often	O
applied	O
.	O
but	O
we	O
must	O
remember	O
that	O
“	O
noise	O
”	O
(	O
i.e	O
.	O
irreducible	O
measurement	O
error	O
)	O
merely	O
characterises	O
one	O
particular	O
form	O
of	O
inadequate	O
information	O
.	O
imagine	O
the	O
multi-class	O
taxonomy	O
problem	O
under	O
the	O
condition	O
that	O
“	O
skin-covering	O
”	O
,	O
“	O
tail	O
”	O
,	O
and	O
“	O
viviparous	O
”	O
are	O
omitted	O
from	O
the	O
attribute	O
set	O
.	O
owls	O
and	O
bats	O
,	O
for	O
example	B
,	O
can	O
not	O
now	O
be	O
discriminated	O
.	O
stopping	O
rules	O
based	O
on	O
complete	O
purity	B
have	O
then	O
to	O
be	O
replaced	O
by	O
something	O
less	O
stringent	O
.	O
5.1.5	O
stopping	O
rules	O
and	O
class	O
probability	O
trees	O
one	O
method	O
,	O
not	O
necessarily	O
recommended	O
,	O
is	O
to	O
stop	O
when	O
the	O
purity	B
measure	O
exceeds	O
some	O
threshold	O
.	O
the	O
trees	O
that	O
result	O
are	O
no	O
longer	O
strictly	O
“	O
decision	O
trees	O
”	O
(	O
although	O
for	O
brevity	O
we	O
continue	O
to	O
use	O
this	O
generic	O
term	O
)	O
,	O
since	O
a	O
leaf	O
is	O
no	O
longer	O
guaranteed	O
to	O
contain	O
a	O
single-class	O
collection	O
,	O
but	O
instead	O
a	O
frequency	O
distribution	O
over	O
classes	O
.	O
such	O
trees	O
are	O
known	O
as	O
“	O
class	O
probability	O
trees	O
”	O
.	O
conversion	O
into	O
classiﬁers	O
requires	O
a	O
separate	O
mapping	O
from	O
distributions	O
to	O
class	O
labels	O
.	O
one	O
popular	O
but	O
simplistic	O
procedure	O
says	O
“	O
pick	O
the	O
candidate	O
with	O
the	O
most	O
votes	O
”	O
.	O
whether	O
or	O
not	O
such	O
a	O
“	O
plurality	O
rule	O
”	O
makes	O
sense	O
depends	O
in	O
each	O
case	O
on	O
(	O
1	O
)	O
the	O
distribution	O
over	O
the	O
classes	O
in	O
the	O
population	O
from	O
which	O
the	O
training	O
set	O
was	O
drawn	O
,	O
i.e	O
.	O
on	O
the	O
priors	O
,	O
and	O
(	O
2	O
)	O
differential	O
misclassiﬁcation	O
costs	O
.	O
consider	O
two	O
errors	O
:	O
classifying	O
the	O
shuttle	O
main	O
engine	O
as	O
“	O
ok	O
to	O
ﬂy	O
”	O
when	O
it	O
is	O
not	O
,	O
and	O
classifying	O
it	O
as	O
“	O
not	O
ok	O
”	O
when	O
it	O
is	O
.	O
obviously	O
the	O
two	O
costs	O
are	O
unequal	O
.	O
use	O
of	O
purity	B
measures	O
for	O
stopping	O
,	O
sometimes	O
called	O
“	O
forward	B
pruning	O
”	O
,	O
has	O
had	O
mixed	O
results	O
.	O
the	O
authors	O
of	O
two	O
of	O
the	O
leading	O
decision	O
tree	O
algorithms	O
,	O
cart	O
(	O
breiman	O
et	O
al.	O
,	O
1984	O
)	O
and	O
c4.5	O
(	O
quinlan	O
1993	O
)	O
,	O
independently	O
arrived	O
at	O
the	O
opposite	O
philosophy	O
,	O
summarised	O
by	O
breiman	O
and	O
colleagues	O
as	O
“	O
prune	O
instead	O
of	O
stopping	O
.	O
grow	O
a	O
tree	O
that	O
is	O
much	O
too	O
large	O
and	O
prune	O
it	O
upward	O
...	O
”	O
this	O
is	O
sometimes	O
called	O
“	O
backward	B
pruning	O
”	O
.	O
these	O
authors	O
’	O
deﬁnition	O
of	O
“	O
much	O
too	O
large	O
”	O
requires	O
that	O
we	O
continue	O
splitting	O
until	O
each	O
terminal	O
node	O
either	O
or	O
or	O
is	O
pure	O
,	O
contains	O
only	O
identical	O
attribute-vectors	O
(	O
in	O
which	O
case	O
splitting	O
is	O
impossible	O
)	O
,	O
has	O
fewer	O
than	O
a	O
pre-speciﬁed	O
number	O
of	O
distinct	O
attribute-vectors	O
.	O
approaches	O
to	O
the	O
backward	B
pruning	O
of	O
these	O
“	O
much	O
too	O
large	O
”	O
trees	O
form	O
the	O
topic	O
of	O
a	O
later	O
section	O
.	O
we	O
ﬁrst	O
return	O
to	O
the	O
concept	O
of	O
a	O
node	O
’	O
s	O
purity	B
in	O
the	O
context	O
of	O
selecting	O
one	O
attribute	O
in	O
preference	O
to	O
another	O
for	O
splitting	O
a	O
given	O
node	O
.	O
5.1.6	O
splitting	O
criteria	O
readers	O
accustomed	O
to	O
working	O
with	O
categorical	O
data	O
will	O
recognise	O
in	O
figure	O
5.4	O
cross-	O
tabulations	O
reminiscent	O
of	O
the	O
“	O
contingency	O
tables	O
”	O
of	O
statistics	O
.	O
for	O
example	B
it	O
only	O
62	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
[	O
ch	O
.	O
5	O
requires	O
completion	O
of	O
the	O
column	O
totals	O
of	O
the	O
second	O
tabulation	O
to	O
create	O
the	O
standard	O
input	B
to	O
a	O
“	O
two-by-two	O
”	O
	O
by	O
applying	O
a	O
|	O
.	O
the	O
hypothesis	O
under	O
test	O
is	O
that	O
the	O
distribution	O
of	O
cases	O
between	O
mammals	O
and	O
not-mammals	O
is	O
independent	O
of	O
the	O
distribution	O
between	O
the	O
two	O
breathing	O
modes	O
.	O
a	O
possible	O
rule	O
says	O
that	O
the	O
smaller	O
the	O
probability	O
obtained	O
test	O
to	O
this	O
hypothesis	O
then	O
the	O
stronger	O
the	O
splitting	O
credentials	O
of	O
the	O
attribute	O
“	O
breathing	O
”	O
.	O
turning	O
to	O
the	O
construction	O
of	O
multi-class	O
trees	O
rather	O
than	O
yes/no	O
concept-recognisers	O
,	O
an	O
adequate	O
number	O
of	O
ﬁshes	O
in	O
the	O
training	O
sample	O
would	O
,	O
under	O
almost	O
any	O
purity	B
criterion	O
,	O
ensure	O
early	O
selection	O
of	O
“	O
breathing	O
”	O
.	O
similarly	O
,	O
given	O
adequate	O
representation	O
of	O
reptiles	O
,	O
“	O
tail=long	O
”	O
would	O
score	O
highly	O
,	O
since	O
lizards	O
and	O
snakes	O
account	O
for	O
95	O
%	O
of	O
living	O
reptiles	O
.	O
the	O
corresponding	O
5	O
x	O
3	O
contingency	O
table	O
would	O
have	O
the	O
form	O
feg	O
long	O
tail	O
?	O
short	O
none	O
totals	O
table	O
5.1	O
:	O
cross-tabulation	O
of	O
classes	O
and	O
“	O
tail	O
”	O
attribute-values	O
given	O
in	O
table	O
5.1.	O
on	O
the	O
hypothesis	O
of	O
no	O
association	O
,	O
the	O
expected	O
numbers	O
in	O
thej	O
,	O
where	O
|	O
|j	O
''	O
	O
longh	O
cells	O
can	O
be	O
got	O
from	O
the	O
marginal	O
totals	O
.	O
thus	O
expected	O
expectedg	O
h	O
expected	O
is	O
distributed	O
as	O
is	O
the	O
total	O
in	O
the	O
training	O
set	O
.	O
thenc	O
observed	O
	O
,	O
with	O
degrees	O
of	O
freedom	O
equal	O
tocb	O
fdg	O
,	O
i.e	O
.	O
8	O
in	O
this	O
case	O
.	O
	O
c	O
}	O
	O
	O
y|	O
|	O
|	O
	O
|i	O
	O
	O
	O
¡	O
	O
|	O
£	O
|i¢	O
	O
¢	O
number	O
in	O
mammal	O
number	O
in	O
bird	O
number	O
in	O
reptile	O
number	O
in	O
amphibian	O
number	O
in	O
fish	O
total	O
suppose	O
,	O
however	O
,	O
that	O
the	O
“	O
tail	O
”	O
variable	O
were	O
not	O
presented	O
in	O
the	O
form	O
of	O
a	O
categorical	O
attribute	O
with	O
three	O
unordered	O
values	O
,	O
but	O
rather	O
as	O
a	O
number	O
,	O
–	O
as	O
the	O
ratio	O
,	O
for	O
example	B
,	O
of	O
the	O
length	O
of	O
the	O
tail	O
to	O
that	O
of	O
the	O
combined	O
body	O
and	O
head	O
.	O
sometimes	O
the	O
ﬁrst	O
step	O
is	O
to	O
apply	O
some	O
form	O
of	O
clustering	O
method	O
or	O
other	O
approximation	O
.	O
but	O
virtually	O
every	O
algorithm	O
then	O
selects	O
,	O
from	O
all	O
the	O
dichotomous	O
segmentations	O
of	O
the	O
numerical	O
scale	O
meaningful	O
for	O
a	O
given	O
node	O
,	O
that	O
segmentation	O
that	O
maximises	O
the	O
chosen	O
purity	B
measure	O
over	O
classes	O
.	O
k	O
	O
	O
long	O
short	O
none	O
with	O
suitable	O
reﬁnements	O
,	O
the	O
chaid	O
decision-tree	O
algorithm	O
(	O
chi-squared	O
automatic	O
interaction	O
detection	O
)	O
uses	O
a	O
splitting	O
criterion	O
such	O
as	O
that	O
illustrated	O
with	O
the	O
foregoing	O
contingency	O
table	O
(	O
kass	O
,	O
1980	O
)	O
.	O
although	O
not	O
included	O
in	O
the	O
present	O
trials	O
,	O
chaid	O
enjoys	O
widespread	O
commercial	O
availability	O
through	O
its	O
inclusion	O
as	O
an	O
optional	O
module	O
in	O
the	O
spss	O
statistical	B
analysis	O
package	O
.	O
other	O
approaches	O
to	O
such	O
tabulations	O
as	O
the	O
above	O
use	O
information	O
theory	O
.	O
we	O
then	O
enquire	O
“	O
what	O
is	O
the	O
expected	O
gain	O
in	O
information	O
about	O
a	O
case	O
’	O
s	O
row-membership	O
from	O
knowledge	O
of	O
its	O
column-membership	O
?	O
”	O
.	O
methods	O
and	O
difﬁculties	O
are	O
discussed	O
by	O
quinlan	O
(	O
1993	O
)	O
.	O
the	O
reader	O
is	O
also	O
referred	O
to	O
the	O
discussion	O
in	O
section	O
7.3.3	O
,	O
with	O
particular	O
reference	O
to	O
“	O
mutual	O
information	O
”	O
.	O
a	O
related	O
,	O
but	O
more	O
direct	O
,	O
criterion	O
applies	O
bayesian	O
probability	O
theory	O
to	O
the	O
weighing	O
of	O
evidence	O
(	O
see	O
good	O
,	O
1950	O
,	O
for	O
the	O
classical	O
treatment	O
)	O
in	O
a	O
sequential	O
testing	O
framework	O
(	O
wald	O
,	O
1947	O
)	O
.	O
logarithmic	O
measure	B
is	O
again	O
used	O
,	O
namely	O
log-odds	O
or	O
“	O
plausibilities	O
”	O
	O
	O
	O
	O
	O
	O
	O
k	O
k	O
|	O
k	O
k	O
k	O
	O
	O
k	O
|	O
	O
k	O
	O
	O
k	O
k	O
k	O
k	O
	O
k	O
k	O
k	O
	O
¢	O
	O
	O
	O
	O
sec	O
.	O
5.1	O
]	O
rules	O
and	O
trees	O
from	O
data	O
:	O
ﬁrst	O
principles	O
63	O
of	O
hypotheses	O
concerning	O
class-membership	O
.	O
the	O
plausibility-shift	O
occasioned	O
by	O
each	O
observation	O
is	O
interpreted	O
as	O
the	O
weight	O
of	O
the	O
evidence	O
contributed	O
by	O
that	O
observation	O
.	O
class-membership	O
we	O
ask	O
:	O
“	O
what	O
expected	O
total	O
weight	O
of	O
evidence	O
,	O
bearing	O
on	O
the	O
hypotheses	O
,	O
is	O
obtainable	O
from	O
knowledge	O
of	O
an	O
attribute	O
’	O
s	O
values	O
over	O
the¤	O
cells	O
?	O
”	O
.	O
preference	O
goes	O
to	O
that	O
attribute	O
contributing	O
the	O
greatest	O
expected	O
total	O
(	O
michie	O
,	O
1990	O
;	O
michie	O
&	O
al	O
attar	O
,	O
1991	O
)	O
.	O
the	O
sequential	O
bayes	O
criterion	O
has	O
the	O
merit	O
,	O
once	O
the	O
tree	O
is	O
grown	O
,	O
of	O
facilitating	O
the	O
recalculation	O
of	O
probability	O
estimates	O
at	O
the	O
leaves	O
in	O
the	O
light	O
of	O
revised	O
knowledge	O
of	O
the	O
priors	O
.	O
in	O
their	O
cart	O
work	O
breiman	O
and	O
colleagues	O
initially	O
used	O
an	O
information-theoretic	O
criterion	O
,	O
but	O
subsequently	O
adopted	O
their	O
“	O
gini	O
”	O
index	O
.	O
for	O
a	O
given	O
node	O
,	O
and	O
classes	O
with	O
g	O
,	O
	O
estimated	O
probabilities¥¦c	O
authors	O
note	O
a	O
number	O
of	O
interesting	O
interpretations	O
of	O
this	O
expression	O
.	O
but	O
they	O
also	O
remark	O
that	O
“	O
...	O
within	O
a	O
wide	O
range	O
of	O
splitting	O
criteria	O
the	O
properties	O
of	O
the	O
ﬁnal	O
tree	O
selected	O
are	O
surprisingly	O
insensitive	O
to	O
the	O
choice	O
of	O
splitting	O
rule	O
.	O
the	O
criterion	O
used	O
to	O
prune	O
or	O
recombine	O
upward	O
is	O
much	O
more	O
important.	O
”	O
©ª¥	O
~ede~¨§	O
,	O
the	O
index	O
can	O
be	O
writtenf	O
g	O
.	O
the	O
c	O
5.1.7	O
getting	O
a	O
“	O
right-sized	O
tree	O
”	O
cart	O
’	O
s	O
,	O
and	O
c4.5	O
’	O
s	O
,	O
pruning	B
starts	O
with	O
growing	O
“	O
a	O
tree	O
that	O
is	O
much	O
too	O
large	O
”	O
.	O
how	O
large	O
is	O
“	O
too	O
large	O
”	O
?	O
as	O
tree-growth	O
continues	O
and	O
end-nodes	O
multiply	O
,	O
the	O
sizes	O
of	O
their	O
associ-	O
ated	O
samples	O
shrink	O
.	O
probability	O
estimates	O
formed	O
from	O
the	O
empirical	O
class-frequencies	O
at	O
the	O
leaves	O
accordingly	O
suffer	O
escalating	O
estimation	O
errors	O
.	O
yet	O
this	O
only	O
says	O
that	O
overgrown	O
trees	O
make	O
unreliable	O
probability	O
estimators	O
.	O
given	O
an	O
unbiased	O
mapping	O
from	O
probability	O
estimates	O
to	O
decisions	O
,	O
why	O
should	O
their	O
performance	O
as	B
classiﬁers	I
suffer	O
?	O
performance	O
is	O
indeed	O
impaired	O
by	O
overﬁtting	O
,	O
typically	O
more	O
severely	O
in	O
tree-learning	O
than	O
in	O
some	O
other	O
multi-variate	O
methods	O
.	O
figure	O
5.6	O
typiﬁes	O
a	O
universally	O
observed	O
axis	O
)	O
.	O
breiman	O
et	O
al.	O
,	O
from	O
whose	O
book	O
the	O
ﬁgure	O
has	O
been	O
taken	O
,	O
describe	O
this	O
relationship	O
as	O
“	O
a	O
fairly	O
rapid	O
initial	O
decrease	O
followed	O
by	O
a	O
long	O
,	O
ﬂat	O
valley	O
and	O
then	O
a	O
gradual	O
increase	O
...	O
”	O
in	O
this	O
long	O
,	O
ﬂat	O
valley	O
,	O
the	O
minimum	O
“	O
is	O
almost	O
constant	O
except	O
for	O
up-down	O
changes	O
relationship	O
between	O
the	O
number	O
of	O
terminal	O
nodes	O
(	O
«	O
-axis	O
)	O
and	O
misclassiﬁcation	O
rates	O
(	O
¬	O
-	O
f	O
se	O
range.	O
”	O
meanwhile	O
the	O
performance	O
of	O
the	O
tree	O
on	O
the	O
training	O
sample	O
well	O
within	O
the­	O
(	O
not	O
shown	O
in	O
the	O
figure	O
)	O
continues	O
to	O
improve	O
,	O
with	O
an	O
increasingly	O
over-optimistic	O
error	O
rate	O
usually	O
referred	O
to	O
as	O
the	O
“	O
resubstitution	O
”	O
error	O
.	O
an	O
important	O
lesson	O
that	O
can	O
be	O
drawn	O
from	O
inspection	O
of	O
the	O
diagram	O
is	O
that	O
large	O
simpliﬁcations	O
of	O
the	O
tree	O
can	O
be	O
purchased	O
at	O
the	O
expense	O
of	O
rather	O
small	O
reductions	O
of	O
estimated	O
accuracy	O
.	O
overﬁtting	O
is	O
the	O
process	O
of	O
inferring	O
more	O
structure	O
from	O
the	O
training	O
sample	O
than	O
is	O
justiﬁed	O
by	O
the	O
population	O
from	O
which	O
it	O
was	O
drawn	O
.	O
quinlan	O
(	O
1993	O
)	O
illustrates	O
the	O
seeming	O
paradox	O
that	O
an	O
overﬁtted	O
tree	O
can	O
be	O
a	O
worse	O
classiﬁer	B
than	O
one	O
that	O
has	O
no	O
information	O
at	O
all	O
beyond	O
the	O
name	O
of	O
the	O
dataset	O
’	O
s	O
most	O
numerous	O
class	O
.	O
this	O
effect	O
is	O
readily	O
seen	O
in	O
the	O
extreme	O
example	B
of	O
random	O
data	O
in	O
which	O
the	O
class	O
of	O
each	O
case	O
is	O
quite	O
unrelated	O
to	O
its	O
attribute	O
values	O
.	O
i	O
constructed	O
an	O
artiﬁcial	O
dataset	O
of	O
this	O
kind	O
with	O
ten	O
attributes	O
,	O
each	O
of	O
which	O
took	O
the	O
value	O
0	O
or	O
1	O
with	O
equal	O
probability	O
.	O
the	O
class	O
was	O
also	O
binary	O
,	O
yes	O
with	O
probability	O
0.25	O
and	O
no	O
with	O
probability	O
0.75.	O
one	O
thousand	O
randomly	O
generated	O
cases	O
were	O
split	O
intp	O
a	O
training	O
set	O
of	O
500	O
and	O
a	O
test	O
set	O
of	O
500.	O
from	O
this	O
data	O
,	O
c4.5	O
’	O
s	O
initial	O
tree-building	O
routine	O
	O
f	O
	O
64	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
[	O
ch	O
.	O
5	O
1	O
0	O
1	O
10	O
20	O
30	O
40	O
50	O
fig	O
.	O
5.6	O
:	O
a	O
typical	O
plot	O
of	O
misclassiﬁcation	O
rate	O
against	O
different	O
levels	O
of	O
growth	O
of	O
a	O
ﬁtted	O
tree	O
.	O
horizontal	O
axis	O
:	O
no	O
.	O
of	O
terminal	O
nodes	O
.	O
vertical	O
axis	O
:	O
misclassiﬁcation	O
rate	O
measured	O
on	O
test	O
data	O
.	O
produces	O
a	O
nonsensical	O
tree	O
of	O
119	O
nodes	O
that	O
has	O
an	O
error	O
rate	O
of	O
more	O
than	O
35	O
%	O
on	O
the	O
test	O
cases	O
...	O
.for	O
the	O
random	O
data	O
above	O
,	O
a	O
tree	O
consisting	O
of	O
just	O
the	O
leaf	O
no	O
would	O
have	O
an	O
expected	O
error	O
rate	O
of	O
25	O
%	O
on	O
unseen	O
cases	O
,	O
yet	O
the	O
elaborate	O
tree	O
is	O
noticeably	O
less	O
accurate	O
.	O
while	O
the	O
complexity	O
comes	O
as	O
no	O
surprise	O
,	O
the	O
increased	O
error	O
attributable	O
to	O
overﬁtting	O
is	O
not	O
intuitively	O
obvious	O
.	O
to	O
explain	O
this	O
,	O
suppose	O
we	O
have	O
a	O
two-class	O
task	O
in	O
which	O
a	O
case	O
’	O
s	O
class	O
is	O
inherently	O
indeterminate	O
,	O
with	O
classiﬁer	B
assigns	O
all	O
such	O
cases	O
to	O
this	O
majority	O
class	O
,	O
its	O
expected	O
error	O
rate	O
is	O
.	O
if	O
,	O
on	O
the	O
other	O
hand	O
,	O
the	O
classiﬁer	B
assigns	O
a	O
case	O
to	O
the	O
majority	O
,	O
its	O
expected	O
	O
±	O
of	O
the	O
cases	O
belonging	O
to	O
the	O
majority	O
class	O
(	O
here	O
no	O
)	O
.	O
if	O
a	O
proportion¥	O
!	O
¯r°	O
clearlyf	O
	O
¥	O
class	O
with	O
probability¥	O
and	O
to	O
the	O
other	O
class	O
with	O
probabilityf	O
other	O
class	O
,	O
¥¤	O
c	O
majority	O
class	O
,	O
c	O
8¥	O
0.5	O
,	O
this	O
is	O
generally	O
greater	O
thanf	O
g	O
,	O
and	O
k¥	O
which	O
comes	O
to³	O
}	O
k¥0	O
;	O
c	O
the	O
probability	O
that	O
a	O
case	O
belonging	O
to	O
the	O
other	O
class	O
is	O
assigned	O
to	O
the	O
is	O
at	O
least	O
,	O
so	O
the	O
second	O
classiﬁer	B
will	O
have	O
a	O
higher	O
error	O
rate	O
.	O
now	O
,	O
the	O
complex	O
decision	O
tree	O
bears	O
a	O
close	O
resemblance	O
to	O
this	O
second	O
type	O
of	O
classiﬁer	B
.	O
the	O
tests	O
are	O
unrelated	O
to	O
class	O
so	O
,	O
like	O
a	O
symbolic	O
pachinko	O
machine	O
,	O
the	O
tree	O
sends	O
each	O
case	O
randomly	O
to	O
one	O
of	O
the	O
leaves	O
.	O
...	O
n¥	O
g	O
.	O
since¥	O
the	O
probability	O
that	O
a	O
case	O
belonging	O
to	O
the	O
majority	O
class	O
is	O
assigned	O
to	O
the	O
error	O
rate	O
is	O
the	O
sum	O
of	O
²¥	O
8¥	O
l¥	O
quinlan	O
points	O
out	O
that	O
the	O
probability	O
of	O
reaching	O
a	O
leaf	O
labelled	O
with	O
class	O
c	O
is	O
the	O
same	O
as	O
the	O
relative	O
frequency	O
of	O
c	O
in	O
the	O
training	O
data	O
,	O
and	O
concludes	O
that	O
the	O
tree	O
’	O
s	O
expected	O
value	O
.	O
error	O
rate	O
for	O
the	O
random	O
data	O
above	O
is³h°	O
given	O
the	O
acknowledged	O
perils	O
of	O
overﬁtting	O
,	O
how	O
should	O
backward	B
pruning	O
be	O
applied	O
to	O
a	O
too-large	O
tree	O
?	O
the	O
methods	O
adopted	O
for	O
cart	O
and	O
c4.5	O
follow	O
different	O
philosophies	O
,	O
and	O
other	O
decision-tree	O
algorithms	O
have	O
adopted	O
their	O
own	O
variants	O
.	O
we	O
have	O
now	O
reached	O
the	O
level	O
of	O
detail	O
appropriate	O
to	O
section	O
5.2	O
,	O
in	O
which	O
speciﬁc	O
features	O
of	O
the	O
various	O
tree	O
and	O
rule	O
learning	O
algorithms	O
,	O
including	O
their	O
methods	O
of	O
pruning	B
,	O
are	O
examined	O
.	O
before	O
proceeding	O
to	O
these	O
candidates	O
for	O
trial	O
,	O
it	O
should	O
be	O
emphasized	O
that	O
their	O
selection	O
was	O
	O
´±	O
or	O
37.5	O
%	O
,	O
quite	O
close	O
to	O
the	O
observed	O
h°	O
®	O
®	O
®	O
®	O
®	O
®	O
®	O
®	O
®	O
®	O
®	O
®	O
®	O
®	O
®	O
®	O
f	O
f	O
g	O
f	O
	O
³	O
±	O
sec	O
.	O
5.2	O
]	O
statlog	O
’	O
s	O
ml	O
algorithms	O
65	O
necessarily	O
to	O
a	O
large	O
extent	O
arbitrary	O
,	O
having	O
more	O
to	O
do	O
with	O
the	O
practical	O
logic	O
of	O
co-	O
ordinating	O
a	O
complex	O
and	O
geographically	O
distributed	O
project	O
than	O
with	O
judgements	O
of	O
merit	O
or	O
importance	O
.	O
apart	O
from	O
the	O
omission	O
of	O
entire	O
categories	O
of	O
ml	O
(	O
as	O
with	O
the	O
genetic	O
and	O
ilp	O
algorithms	O
discussed	O
in	O
chapter	O
12	O
)	O
particular	O
contributions	O
to	O
decision-tree	O
learning	O
should	O
be	O
acknowledged	O
that	O
would	O
otherwise	O
lack	O
mention	O
.	O
first	O
a	O
major	O
historical	O
role	O
,	O
which	O
continues	O
today	O
,	O
belongs	O
to	O
the	O
assistant	O
algorithm	O
developed	O
by	O
ivan	O
bratko	O
’	O
s	O
group	O
in	O
slovenia	O
(	O
cestnik	O
,	O
kononenko	O
and	O
bratko	O
,	O
1987	O
)	O
.	O
assistant	O
introduced	O
many	O
improvements	O
for	O
dealing	O
with	O
missing	O
values	O
,	O
attribute	O
split-	O
ting	O
and	O
pruning	B
,	O
and	O
has	O
also	O
recently	O
incorporated	O
the	O
m-estimate	O
method	O
(	O
cestnik	O
and	O
bratko	O
,	O
1991	O
;	O
see	O
also	O
dzeroski	O
,	O
cesnik	O
and	O
petrovski	O
,	O
1993	O
)	O
of	O
handling	O
prior	O
probability	O
assumptions	O
.	O
second	O
,	O
an	O
important	O
niche	O
is	O
occupied	O
in	O
the	O
commercialsector	O
of	O
ml	O
by	O
the	O
xpertrule	O
family	O
of	O
packages	O
developed	O
by	O
attar	O
software	O
ltd.	O
facilities	O
for	O
large-scale	O
data	O
analysis	O
are	O
integrated	O
with	O
sophisticated	O
support	O
for	O
structured	O
induction	O
(	O
see	O
for	O
example	B
attar	O
,	O
1991	O
)	O
.	O
these	O
and	O
other	O
features	O
make	O
this	O
suite	O
currently	O
the	O
most	O
powerful	O
and	O
versatile	O
facility	O
available	O
for	O
industrial	O
ml	O
.	O
5.2	O
statlog	O
’	O
s	O
ml	O
algorithms	O
5.2.1	O
tree-learning	O
:	O
further	O
features	O
of	O
c4.5	O
the	O
reader	O
should	O
be	O
aware	O
that	O
the	O
two	O
versions	O
of	O
c4.5	O
used	O
in	O
the	O
statlog	O
trials	O
differ	O
in	O
certain	O
respects	O
from	O
the	O
present	O
version	O
which	O
was	O
recently	O
presented	O
in	O
quinlan	O
(	O
1993	O
)	O
.	O
the	O
version	O
on	O
which	O
accounts	O
in	O
section	O
5.1	O
are	O
based	O
is	O
that	O
of	O
the	O
radical	O
upgrade	O
,	O
described	O
in	O
quinlan	O
(	O
1993	O
)	O
.	O
5.2.2	O
newid	O
newid	O
is	O
a	O
similar	O
decision	O
tree	O
algorithm	O
to	O
c4.5	O
.	O
similar	O
to	O
c4.5	O
,	O
newid	O
inputs	O
a	O
set	O
of	O
2.	O
satisﬁes	O
the	O
termination	O
condition	O
,	O
then	O
output	B
the	O
current	O
tree	O
and	O
halt	O
.	O
.	O
(	O
probabilistic	O
)	O
classiﬁcation	B
.	O
unlike	O
c4.5	O
,	O
newid	O
does	O
not	O
perform	O
windowing	O
.	O
thus	O
its	O
core	O
procedure	O
is	O
simpler	O
:	O
	O
and	O
a	O
classm	O
.	O
its	O
output	B
is	O
a	O
decision	O
tree	O
,	O
which	O
performs	O
,	O
a	O
set	O
of	O
attributes¶	O
examplesµ	O
1.	O
set	O
the	O
current	O
examples	O
{	O
toµ	O
if	O
{	O
	O
,	O
determine	O
the	O
value	O
of	O
the	O
evaluation	O
function	O
.	O
with	O
the	O
attribute	O
3.	O
for	O
each	O
attribute¶	O
that	O
has	O
the	O
largest	O
value	O
of	O
this	O
function	O
,	O
divide	O
the	O
set	O
{	O
values	O
.	O
for	O
each	O
such	O
subset	O
of	O
examplesµ	O
}	O
·	O
,	O
recursively	O
re-enter	O
at	O
step	O
(	O
i	O
)	O
withµ	O
set	O
toµ	O
}	O
·	O
.	O
set	O
the	O
subtrees	O
of	O
the	O
current	O
node	O
to	O
be	O
the	O
subtrees	O
thus	O
produced	O
.	O
the	O
termination	O
condition	O
is	O
simpler	O
than	O
c4.5	O
,	O
i.e	O
.	O
it	O
terminates	O
when	O
the	O
node	O
contains	O
all	O
examples	O
in	O
the	O
same	O
class	O
.	O
this	O
simple-minded	O
strategy	O
tries	O
to	O
overﬁt	O
the	O
training	O
data	O
and	O
will	O
produce	O
a	O
complete	O
tree	O
from	O
the	O
training	O
data	O
.	O
newid	O
deals	O
with	O
empty	O
leaf	O
nodes	O
as	O
c4.5	O
does	O
,	O
but	O
it	O
also	O
considers	O
the	O
possibility	O
of	O
clashing	O
examples	O
.	O
if	O
the	O
set	O
of	O
(	O
untested	O
)	O
attributes	O
is	O
empty	O
it	O
labels	O
the	O
leaf	O
node	O
as	O
clash	O
,	O
meaning	O
that	O
it	O
is	O
impossible	O
to	O
distinguish	O
between	O
the	O
examples	O
.	O
in	O
most	O
situations	O
the	O
attribute	O
set	O
will	O
not	O
be	O
empty	O
.	O
so	O
newid	O
discards	O
attributes	O
that	O
have	O
been	O
used	O
,	O
as	O
they	O
can	O
contribute	O
no	O
more	O
information	O
to	O
the	O
tree	O
.	O
into	O
subsets	O
by	O
attribute	O
¶	O
	O
66	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
[	O
ch	O
.	O
5	O
tion	B
of	O
newid	O
is	O
the	O
information	O
gain	O
function¸¹¶bkbcm	O
for	O
classiﬁcation	B
problems	O
,	O
where	O
the	O
class	O
values	O
are	O
categorical	O
,	O
the	O
evaluation	O
func-	O
it	O
does	O
a	O
similar	O
1-level	O
lookahead	O
to	O
determine	O
the	O
best	O
attribute	O
to	O
split	O
on	O
using	O
a	O
greedy	O
search	O
.	O
it	O
also	O
handles	O
numeric	O
attributes	O
in	O
the	O
same	O
way	O
as	O
c4.5	O
does	O
using	O
the	O
attribute	O
subsetting	O
method	O
.	O
numeric	O
class	O
values	O
newid	O
allows	O
numeric	O
class	O
values	O
and	O
can	O
produce	O
a	O
regression	B
tree	I
.	O
for	O
each	O
split	O
,	O
it	O
aims	O
to	O
reduce	O
the	O
spread	O
of	O
class	O
values	O
in	O
the	O
subsets	O
introduced	O
by	O
the	O
split	O
,	O
instead	O
of	O
trying	O
to	O
gain	O
the	O
most	O
information	O
.	O
formally	O
,	O
for	O
each	O
ordered	O
categorical	O
attribute	O
with	O
g	O
.	O
values	O
in	O
the	O
setº.	O
»	O
¼	O
~e½½½~	O
»	O
â¶2ã.¶2km¨cº	O
class	O
of	O
¤¾	O
,	O
it	O
chooses	O
the	O
one	O
that	O
minimises	O
the	O
value	O
of	O
:	O
¼	O
attribute	O
value	O
of	O
for	O
numeric	O
attributes	O
,	O
the	O
attribute	O
subsetting	O
method	O
is	O
used	O
instead	O
.	O
eá|	O
is	O
a	O
cä	O
cä	O
when	O
the	O
class	O
value	O
is	O
numeric	O
,	O
the	O
termination	O
function	O
of	O
the	O
algorithm	O
will	O
also	O
be	O
different	O
.	O
the	O
criterion	O
that	O
all	O
examples	O
share	O
the	O
same	O
class	O
value	O
is	O
no	O
longer	O
appropriate	O
,	O
and	O
the	O
following	O
criterion	O
is	O
used	O
instead	O
:	O
the	O
algorithm	O
terminates	O
at	O
a	O
node	O
gçæèfdh	O
is	O
the	O
standard	O
deviation	O
,	O
µ	O
split	O
into	O
“	O
fractional	O
examples	O
”	O
for	O
each	O
possible	O
value	O
of	O
that	O
attribute	O
.	O
the	O
fractions	O
of	O
the	O
different	O
values	O
sum	O
to	O
1.	O
they	O
are	O
estimated	O
from	O
the	O
numbers	O
of	O
examples	O
of	O
the	O
same	O
class	O
with	O
a	O
known	O
value	O
of	O
that	O
attribute	O
.	O
user-tunable	O
parameter	O
.	O
missing	O
values	O
there	O
are	O
two	O
types	O
of	O
missing	O
values	O
in	O
newid	O
:	O
unknown	O
values	O
and	O
“	O
don	O
’	O
t-care	O
”	O
values	O
.	O
	O
with	O
examplesä	O
when	O
cµ	O
whereå	O
is	O
the	O
original	O
example	B
set	O
,	O
and	O
the	O
constantd	O
during	O
the	O
training	O
phase	O
,	O
if	O
an	O
example	B
of	O
classm	O
has	O
an	O
unknown	O
attribute	O
value	O
,	O
it	O
is	O
consider	O
attribute¶	O
with	O
values¬âdé	O
andkê	O
.	O
there	O
are	O
9	O
examples	O
at	O
the	O
current	O
node	O
in	O
classm	O
with	O
values	O
for¶	O
:	O
6¬¹eé	O
,	O
2kê	O
and	O
1	O
missing	O
(	O
‘	O
?	O
’	O
)	O
.	O
naively	O
,	O
we	O
would	O
split	O
the	O
‘	O
?	O
’	O
in	O
the	O
ratio	O
6	O
to	O
2	O
(	O
i.e	O
.	O
75	O
%	O
¬âdé	O
and	O
25	O
%	O
kê	O
)	O
.	O
however	O
,	O
the	O
laplace	O
criterion	O
gives	O
a	O
better	O
estimate	O
of	O
the	O
expected	O
ratio	O
of¬¹eé	O
tokê	O
using	O
the	O
formula	O
:	O
fegh	O
ã¶¹mìiêdkbc¬¹eé	O
ï.ðiñe	O
ckíeòkó	O
fegh	O
cõe	O
!	O
³	O
is	O
the	O
no	O
.	O
examples	O
in	O
classm	O
with	O
attribute¶	O
ï3ðiñ	O
íî	O
is	O
the	O
total	O
no	O
.	O
examples	O
in	O
classm	O
is	O
the	O
total	O
no	O
.	O
examples	O
in	O
with¶	O
and	O
similarly	O
forë	O
ã¶âmìiêdkbcbkê	O
“	O
don	O
’	O
t-care	O
”	O
s	O
(	O
‘	O
*	O
’	O
)	O
are	O
intended	O
as	O
a	O
short-hand	O
to	O
cover	O
all	O
the	O
possible	O
values	O
of	O
the	O
don	O
’	O
t-care	O
attribute	O
.	O
they	O
are	O
handled	O
in	O
a	O
similar	O
way	O
to	O
unknowns	O
,	O
except	O
the	O
example	B
is	O
simply	O
duplicated	O
,	O
not	O
fractionalised	O
,	O
for	O
each	O
value	O
of	O
the	O
attribute	O
when	O
being	O
inspected	O
.	O
g	O
.	O
this	O
latter	O
laplace	O
estimate	O
is	O
used	O
in	O
newid	O
.	O
ckíî	O
côe	O
¬¹eé	O
kí	O
where	O
~	O
¶	O
	O
	O
f	O
¿	O
à	O
g	O
	O
»	O
	O
¾	O
g	O
å	O
d	O
å	O
g	O
g	O
ë	O
g	O
	O
g	O
	O
g	O
~	O
k	O
	O
k	O
ó	O
sec	O
.	O
5.2	O
]	O
statlog	O
’	O
s	O
ml	O
algorithms	O
67	O
thus	O
,	O
in	O
a	O
similar	O
case	O
with	O
6¬âdé	O
’	O
s	O
,	O
2kê	O
’	O
s	O
and	O
1	O
‘	O
*	O
’	O
,	O
the	O
‘	O
*	O
’	O
example	B
would	O
be	O
considered	O
as	O
2	O
examples	O
,	O
one	O
with	O
value¬âdé	O
and	O
one	O
with	O
valuekê	O
.	O
this	O
duplication	O
only	O
occurs	O
when	O
inspecting	O
the	O
split	O
caused	O
by	O
attribute¶	O
.	O
if	O
a	O
different	O
attributeö	O
the	O
example	B
with¶	O
is	O
being	O
considered	O
,	O
is	O
only	O
considered	O
as	O
1	O
example	B
.	O
note	O
this	O
is	O
an	O
ad	O
hoc	O
method	O
because	O
the	O
duplication	O
of	O
examples	O
may	O
cause	O
the	O
total	O
number	O
of	O
examples	O
at	O
the	O
leaves	O
to	O
add	O
up	O
to	O
more	O
than	O
the	O
total	O
number	O
of	O
examples	O
originally	O
in	O
the	O
training	O
set	O
.	O
ø×	O
and	O
a	O
known	O
value	O
forö	O
when	O
a	O
tree	O
is	O
executed	O
,	O
and	O
the	O
testing	O
example	B
has	O
an	O
unknown	O
value	O
for	O
the	O
attribute	O
being	O
tested	O
on	O
,	O
the	O
example	B
is	O
again	O
split	O
fractionally	O
using	O
the	O
laplace	O
estimate	O
for	O
the	O
ratio	O
–	O
but	O
as	O
the	O
testing	O
example	B
’	O
s	O
class	O
value	O
is	O
unknown	O
,	O
all	O
the	O
training	O
examples	O
at	O
class	O
ratios	O
are	O
the	O
fractional	O
examples	O
arrive	O
.	O
rather	O
than	O
predicting	O
the	O
majority	O
class	O
,	O
a	O
probabilistic	O
classiﬁcation	B
is	O
made	O
,	O
for	O
to	O
split	O
the	O
testing	O
example	B
into	O
.	O
the	O
numbers	O
of	O
training	O
examples	O
at	O
the	O
node	O
are	O
found	O
by	O
back-propagating	O
the	O
example	B
counts	O
recorded	O
at	O
the	O
leaves	O
of	O
the	O
subtree	O
beneath	O
the	O
node	O
back	O
to	O
that	O
node	O
.	O
the	O
class	O
predicted	O
at	O
a	O
node	O
is	O
the	O
majority	O
class	O
there	O
(	O
if	O
a	O
tie	O
with	O
more	O
than	O
one	O
majority	O
class	O
,	O
select	O
the	O
ﬁrst	O
)	O
.	O
the	O
example	B
may	O
thus	O
be	O
classiﬁed	O
,	O
the	O
node	O
(	O
rather	O
than	O
just	O
those	O
of	O
classm	O
)	O
are	O
used	O
to	O
estimate	O
the	O
appropriate	O
fractions	O
|	O
andm	O
	O
,	O
wherem	O
|	O
andë.	O
asm	O
say	O
,	O
ë2|	O
asm	O
	O
are	O
the	O
majority	O
classes	O
at	O
the	O
two	O
leaves	O
where	O
|	O
and	O
25	O
%	O
as	O
|	O
andm	O
	O
classiﬁes	O
an	O
example	B
75	O
%	O
asm	O
example	B
,	O
a	O
leaf	O
with	O
[	O
6	O
,	O
2	O
]	O
for	O
classesm	O
	O
(	O
rather	O
than	O
simply	O
asm	O
|	O
)	O
.	O
for	O
fractional	O
examples	O
,	O
the	O
distributions	O
would	O
be	O
weighted	O
and	O
summed	O
,	O
for	O
example	B
,	O
10	O
%	O
arrives	O
at	O
leaf	O
[	O
6,2	O
]	O
,	O
90	O
%	O
at	O
leaf	O
[	O
1,3	O
]	O
ù	O
	O
.	O
|	O
and	O
66	O
%	O
m	O
[	O
1,3	O
]	O
=	O
[	O
1.5,2.9	O
]	O
,	O
thus	O
the	O
example	B
is	O
34	O
%	O
m	O
[	O
6,2	O
]	O
+	O
90	O
%	O
	O
10	O
%	O
	O
the	O
pruning	B
algorithm	O
works	O
as	O
follows	O
.	O
given	O
a	O
treez	O
examples	O
,	O
a	O
further	O
pruning	B
set	O
of	O
examples	O
,	O
and	O
a	O
threshold	O
valueú	O
lying	O
below	O
internal	O
node	O
,	O
if	O
the	O
subtree	O
ofz	O
of	O
thez	O
for	O
the	O
pruning	B
examples	O
than	O
node	O
sub-tree	O
and	O
make	O
node	O
is	O
set	O
tof	O
a	O
leaf-node	O
)	O
.	O
by	O
default	O
,	O
ú	O
a	O
testing	O
example	B
tested	O
on	O
an	O
attribute	O
with	O
a	O
don	O
’	O
t-care	O
value	O
is	O
simply	O
duplicated	O
for	O
each	O
outgoing	O
branch	O
,	O
i.e	O
.	O
a	O
whole	O
example	B
is	O
sent	O
down	O
every	O
outgoing	O
branch	O
,	O
thus	O
counting	O
it	O
as	O
several	O
examples	O
.	O
tree	O
pruning	B
induced	O
from	O
a	O
set	O
of	O
learning	O
:	O
then	O
for	O
each	O
providesú	O
%	O
better	O
accuracy	O
°	O
%	O
,	O
but	O
one	O
can	O
modify	O
it	O
does	O
(	O
if	O
labelled	O
by	O
the	O
majority	O
class	O
for	O
the	O
learning	O
examples	O
at	O
that	O
node	O
)	O
,	O
then	O
leave	O
the	O
subtree	O
unpruned	O
;	O
otherwise	O
,	O
prune	O
it	O
(	O
i.e	O
.	O
delete	O
the	O
to	O
suit	O
different	O
tasks	O
.	O
apart	O
from	O
the	O
features	O
described	O
above	O
(	O
which	O
are	O
more	O
relevant	O
to	O
the	O
version	O
of	O
newid	O
used	O
for	O
statlog	O
)	O
,	O
newid	O
has	O
a	O
number	O
of	O
other	O
features	O
.	O
newid	O
can	O
have	O
binary	O
splits	O
for	O
each	O
attribute	O
at	O
a	O
node	O
of	O
a	O
tree	O
using	O
the	O
subsetting	O
principle	O
.	O
it	O
can	O
deal	O
with	O
ordered	O
sequential	O
attributes	O
(	O
i.e	O
.	O
attributes	O
whose	O
values	O
are	O
ordered	O
)	O
.	O
newid	O
can	O
also	O
accept	O
a	O
pre-speciﬁed	O
ordering	O
of	O
attributes	O
so	O
the	O
more	O
important	O
ones	O
will	O
be	O
considered	O
ﬁrst	O
,	O
and	O
the	O
user	O
can	O
force	O
newid	O
to	O
choose	O
a	O
particular	O
attribute	O
for	O
splitting	O
at	O
a	O
node	O
.	O
it	O
can	O
also	O
deal	O
with	O
structured	O
attributes	O
.	O
5.2.3	O
is	O
not	O
a	O
single	O
algorithm	O
,	O
it	O
is	O
a	O
knowledge	O
acquisition	O
environment	O
for	O
expert	O
systems	O
which	O
enables	O
its	O
user	O
to	O
build	O
a	O
knowledge	O
base	O
or	O
an	O
expert	O
system	O
from	O
the	O
analysis	O
of	O
examples	O
provided	O
by	O
the	O
human	O
expert	O
.	O
thus	O
it	O
placed	O
considerable	O
emphasis	O
on	O
the	O
m	O
û	O
{	O
	O
û	O
{	O
	O
68	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
[	O
ch	O
.	O
5	O
graphical	O
interface	O
.	O
this	O
interface	O
is	O
consisting	O
of	O
graphical	O
editors	O
,	O
which	O
enable	O
the	O
user	O
to	O
deﬁne	O
the	O
domain	O
,	O
to	O
interactively	O
build	O
the	O
data	O
base	O
,	O
and	O
to	O
go	O
through	O
the	O
hierarchy	O
of	O
classes	O
and	O
the	O
decision	O
tree	O
.	O
dialog	O
and	O
interaction	O
of	O
the	O
system	O
with	O
the	O
user	O
.	O
the	O
user	O
interacts	O
withû	O
	O
via	O
a	O
	O
can	O
be	O
viewed	O
as	O
an	O
extension	O
of	O
a	O
tree	O
induction	O
algorithm	O
that	O
is	O
essentially	O
the	O
same	O
as	O
newid	O
.	O
because	O
of	O
its	O
user	O
interface	O
,	O
it	O
allows	O
a	O
more	O
natural	O
manner	O
of	O
interaction	O
with	O
a	O
domain	O
expert	O
,	O
the	O
validation	O
of	O
the	O
trees	O
produced	O
,	O
and	O
the	O
test	O
of	O
its	O
accuracy	O
and	O
reliability	O
.	O
it	O
also	O
provides	O
a	O
simple	O
,	O
fast	O
and	O
cheap	O
method	O
to	O
update	O
the	O
rule	O
and	O
data	O
bases	O
.	O
it	O
produces	O
,	O
from	O
data	O
and	O
known	O
rules	O
(	O
trees	O
)	O
of	O
the	O
domain	O
,	O
either	O
a	O
decision	O
tree	O
or	O
a	O
set	O
of	O
rules	O
designed	O
to	O
be	O
used	O
by	O
expert	O
system	O
.	O
5.2.4	O
further	O
features	O
of	O
cart	O
cart	O
,	O
classiﬁcation	B
and	O
regression	B
tree	I
,	O
is	O
a	O
binary	O
decision	O
tree	O
algorithm	O
(	O
breiman	O
et	O
al.	O
,	O
1984	O
)	O
,	O
which	O
has	O
exactly	O
two	O
branches	O
at	O
each	O
internal	O
node	O
.	O
we	O
have	O
used	O
two	O
different	O
implementations	O
of	O
cart	O
:	O
the	O
commercial	O
version	O
of	O
cart	O
and	O
indcart	O
,	O
which	O
is	O
part	O
of	O
the	O
ind	O
package	O
(	O
also	O
see	O
naive	O
bayes	O
,	O
section	O
4.5	O
)	O
.	O
indcart	O
differs	O
from	O
cart	O
as	O
described	O
in	O
breiman	O
et	O
al	O
.	O
(	O
1984	O
)	O
in	O
using	O
a	O
different	O
(	O
probably	O
better	O
)	O
way	O
of	O
handling	O
missing	O
values	O
,	O
in	O
not	O
implementing	O
the	O
regression	O
part	O
of	O
cart	O
,	O
and	O
in	O
the	O
different	O
pruning	B
settings	O
.	O
evaluation	O
function	O
for	O
splitting	O
the	O
evaluation	O
function	O
used	O
by	O
cart	O
is	O
different	O
from	O
that	O
in	O
the	O
id3	O
family	O
of	O
algorithms	O
.	O
consider	O
the	O
case	O
of	O
a	O
problem	O
with	O
two	O
classes	O
,	O
and	O
a	O
node	O
has	O
100	O
examples	O
,	O
50	O
from	O
each	O
class	O
,	O
the	O
node	O
has	O
maximum	O
impurity	O
.	O
if	O
a	O
split	O
could	O
be	O
found	O
that	O
split	O
the	O
data	O
into	O
one	O
subgroup	O
of	O
40:5	O
and	O
another	O
of	O
10:45	O
,	O
then	O
intuitively	O
the	O
impurity	O
has	O
been	O
reduced	O
.	O
the	O
impurity	O
would	O
be	O
completely	O
removed	O
if	O
a	O
split	O
could	O
be	O
found	O
that	O
produced	O
sub-groups	O
50:0	O
and	O
0:50.	O
in	O
cart	O
this	O
intuitive	O
idea	O
of	O
impurity	O
is	O
formalised	O
in	O
the	O
gini	O
index	O
for	O
subgroups	O
is	O
summed	O
and	O
the	O
split	O
with	O
the	O
maximum	O
reduction	O
in	O
impurity	O
chosen	O
.	O
for	O
ordered	O
and	O
numeric	O
attributes	O
,	O
cart	O
considers	O
all	O
possible	O
splits	O
in	O
the	O
sequence	O
.	O
the	O
current	O
nodem	O
:	O
bkicm	O
where¥	O
is	O
the	O
probability	O
of	O
class	O
inm	O
.	O
for	O
each	O
possible	O
split	O
the	O
impurity	O
of	O
the	O
f	O
splits	O
.	O
for	O
categorical	O
attributes	O
cart	O
examines	O
fork	O
values	O
of	O
the	O
attribute	O
,	O
there	O
areký	O
all	O
possible	O
binary	O
splits	O
,	O
which	O
is	O
the	O
same	O
as	O
attribute	O
subsetting	O
used	O
for	O
c4.5	O
.	O
fork	O
f	O
splits	O
.	O
at	O
each	O
node	O
cart	O
searches	O
through	O
the	O
ßþ	O
values	O
of	O
the	O
attribute	O
,	O
there	O
are³	O
attributes	O
one	O
by	O
one	O
.	O
for	O
each	O
attribute	O
it	O
ﬁnds	O
the	O
best	O
split	O
.	O
then	O
it	O
compares	O
the	O
best	O
single	O
splits	O
and	O
selects	O
the	O
best	O
attribute	O
of	O
the	O
best	O
splits	O
.	O
minimal	O
cost	B
complexity	I
tree	O
pruning	B
apart	O
from	O
the	O
evaluation	O
function	O
cart	O
’	O
s	O
most	O
crucial	O
difference	O
from	O
the	O
other	O
machine	O
learning	O
algorithms	O
is	O
its	O
sophisticated	O
pruning	B
mechanism	O
.	O
cart	O
treats	O
pruning	B
as	O
a	O
tradeoff	O
between	O
two	O
issues	O
:	O
getting	O
the	O
right	O
size	O
of	O
a	O
tree	O
and	O
getting	O
accurate	O
estimates	O
of	O
the	O
true	O
probabilities	O
of	O
misclassiﬁcation	O
.	O
this	O
process	O
is	O
known	O
as	O
minimal	O
cost-	O
complexity	O
pruning	B
.	O
{	O
û	O
{	O
ü	O
g	O
	O
f	O
	O
à	O
	O
¥	O
	O
	O
	O
|	O
	O
sec	O
.	O
5.2	O
]	O
statlog	O
’	O
s	O
ml	O
algorithms	O
69	O
is	O
:	O
if	O
ú1cz	O
be	O
a	O
decision	O
tree	O
used	O
to	O
as	O
the	O
cost	O
for	O
each	O
leaf	O
,	O
is	O
such	O
that	O
all	O
other	O
subtrees	O
have	O
higher	O
cost	O
complexities	O
or	O
have	O
the	O
same	O
cost	B
complexity	I
it	O
is	O
a	O
two	O
stage	O
method	O
.	O
considering	O
the	O
ﬁrst	O
stage	O
,	O
letz	O
classifyk	O
examples	O
in	O
the	O
training	O
set	O
{	O
be	O
the	O
misclassiﬁed	O
set	O
of	O
size	O
.	O
ifàbcz	O
.	O
letµ	O
is	O
the	O
number	O
of	O
leaves	O
inz	O
for	O
some	O
parameterá	O
the	O
cost	B
complexity	I
ofz	O
e	O
!	O
áäãeàbcbz	O
úâ	O
.	O
if	O
we	O
regardá	O
whereú1cz	O
is	O
the	O
error	O
estimate	O
ofz	O
is	O
a	O
linear	O
combination	O
of	O
its	O
error	O
estimate	O
and	O
a	O
penalty	O
for	O
its	O
complexity	O
.	O
ifá	O
úâ	O
small	O
the	O
penalty	O
for	O
having	O
a	O
large	O
number	O
of	O
leaves	O
is	O
small	O
andz	O
will	O
be	O
large	O
.	O
asá	O
increases	O
,	O
the	O
minimising	O
subtree	O
will	O
decrease	O
in	O
size	O
.	O
now	O
if	O
we	O
convert	O
some	O
subtreeä	O
to	O
a	O
leaf	O
.	O
the	O
new	O
treezâ	O
would	O
misclassifyd	O
more	O
examples	O
but	O
would	O
containàbcä	O
is	O
the	O
same	O
as	O
that	O
ofz	O
fewer	O
leaves	O
.	O
the	O
cost	B
complexity	I
ofzâ	O
feg	O
knãdcàbcä	O
g	O
for	O
any	O
value	O
ofá	O
â	O
which	O
minimisesú	O
it	O
can	O
be	O
shown	O
that	O
there	O
is	O
a	O
unique	O
subtreez	O
cz	O
â	O
as	O
a	O
pruned	O
subtree	O
.	O
and	O
havez	O
|	O
.	O
there	O
is	O
as	O
above	O
.	O
let	O
this	O
tree	O
bez	O
,	O
we	O
can	O
ﬁnd	O
the	O
subtree	O
such	O
thatá	O
forzå	O
|hæ	O
0æ	O
½½	O
,	O
where	O
each	O
subtree	O
is	O
produced	O
by	O
is	O
then	O
a	O
minimising	O
sequence	O
of	O
treesz	O
fromz	O
we	O
examine	O
each	O
pruning	B
upward	O
from	O
the	O
previous	O
subtree	O
.	O
to	O
producez	O
$	O
ç	O
non-leaf	O
subtree	O
ofz	O
and	O
ﬁnd	O
the	O
minimum	O
value	O
ofá	O
that	O
value	O
ofá	O
will	O
be	O
replaced	O
by	O
leaves	O
.	O
the	O
best	O
tree	O
is	O
selected	O
from	O
this	O
series	O
of	O
trees	O
|	O
would	O
be	O
this	O
latter	O
stage	O
selects	O
a	O
single	O
tree	O
based	O
on	O
its	O
reliability	O
,	O
i.e	O
.	O
classiﬁcation	B
error	O
.	O
the	O
problem	O
of	O
pruning	B
is	O
now	O
reduced	O
to	O
ﬁnding	O
which	O
tree	O
in	O
the	O
sequence	O
is	O
the	O
optimally	O
sized	O
one	O
.	O
chosen	O
.	O
however	O
this	O
is	O
not	O
the	O
case	O
and	O
it	O
tends	O
to	O
underestimate	O
the	O
number	O
of	O
errors	O
.	O
a	O
more	O
honest	O
estimate	O
is	O
therefore	O
needed	O
.	O
in	O
cart	O
this	O
is	O
produced	O
by	O
using	O
cross-	O
validation	O
.	O
the	O
idea	O
is	O
that	O
,	O
instead	O
of	O
using	O
one	O
sample	O
(	O
training	O
data	O
)	O
to	O
build	O
a	O
tree	O
and	O
another	O
sample	O
(	O
pruning	B
data	O
)	O
to	O
test	O
the	O
tree	O
,	O
you	O
can	O
form	O
several	O
pseudo-independent	O
samples	O
from	O
the	O
original	O
sample	O
and	O
use	O
these	O
to	O
form	O
a	O
more	O
accurate	O
estimate	O
of	O
the	O
error	O
.	O
the	O
general	O
method	O
is	O
:	O
with	O
the	O
classiﬁcation	B
error	O
not	O
exceeding	O
an	O
expected	O
error	O
rate	O
on	O
some	O
test	O
set	O
,	O
which	O
is	O
done	O
at	O
the	O
second	O
stage	O
.	O
g	O
was	O
unbiased	O
then	O
the	O
largest	O
treez	O
if	O
the	O
error	O
estimateú1cz	O
.	O
the	O
one	O
or	O
more	O
subtrees	O
with	O
~e½½½~	O
.	O
:	O
ä	O
$	O
	O
tok	O
á|	O
3.	O
form	O
the	O
cross-validation	O
error	O
estimate	O
as	O
1.	O
randomly	O
split	O
the	O
original	O
sampleµ	O
intok	O
equal	O
subsamplesä	O
2.	O
for	O
a	O
)	O
build	O
a	O
tree	O
on	O
the	O
training	O
setänxä	O
$	O
	O
;	O
and	O
b	O
)	O
determine	O
the	O
error	O
estimateúç	O
using	O
the	O
pruning	B
setä	O
$	O
	O
.	O
úç	O
cross-validation	O
and	O
cost	B
complexity	I
pruning	O
is	O
combined	O
to	O
select	O
the	O
value	O
ofá	O
the	O
method	O
is	O
to	O
estimate	O
the	O
expected	O
error	O
rates	O
of	O
estimates	O
obtained	O
withzèâ	O
values	O
ofá	O
using	O
cross-validation	O
.	O
from	O
these	O
estimates	O
,	O
it	O
is	O
then	O
possible	O
to	O
estimate	O
an	O
optimal	O
valueábéëêeì	O
ofá	O
for	O
which	O
the	O
estimated	O
true	O
error	O
rate	O
ofzèâ¹íîï	O
for	O
all	O
the	O
data	O
is	O
the	O
.	O
for	O
all	O
ä	O
$	O
	O
g	O
	O
g	O
g	O
~	O
g	O
	O
	O
h	O
k	O
g	O
	O
f	O
á	O
	O
d	O
g	O
	O
	O
â	O
	O
z	O
z	O
|	O
å	O
|	O
	O
f	O
	O
à	O
	O
¼	O
¼	O
¼	O
ä	O
¼	O
70	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
[	O
ch	O
.	O
5	O
is	O
that	O
value	O
of	O
a	O
which	O
minimises	O
the	O
mean	O
the	O
cart	O
methodology	O
therefore	O
involves	O
two	O
quite	O
separate	O
calculations	O
.	O
first	O
the	O
is	O
determined	O
using	O
cross-validation	O
.	O
ten	O
fold	O
cross-validation	O
is	O
recom-	O
.	O
the	O
valueáðéëêdì	O
minimum	O
for	O
all	O
values	O
ofá	O
cross-validation	O
error	O
estimate	O
.	O
oncezèâ¹íîï	O
has	O
been	O
determined	O
,	O
the	O
tree	O
that	O
is	O
ﬁnally	O
suggested	O
for	O
use	O
is	O
that	O
which	O
minimises	O
the	O
cost-complexity	O
usingáðéëêdì	O
and	O
all	O
the	O
data	O
.	O
value	O
ofá	O
éêdì	O
mended	O
.	O
the	O
second	O
step	O
is	O
using	O
this	O
value	O
ofá	O
é	O
andé¨y	O
of	O
a	O
node	O
.	O
if	O
the	O
best	O
split	O
of	O
éyy	O
on	O
the	O
attributes	O
other	O
than¶	O
éëêeì	O
is	O
the	O
splité	O
on	O
the	O
attribute¶	O
,	O
ﬁnd	O
the	O
split	O
that	O
is	O
most	O
similar	O
toé	O
.	O
if	O
an	O
example	B
has	O
the	O
value	O
of¶	O
missing	O
values	O
missing	O
attribute	O
values	O
in	O
the	O
training	O
and	O
test	O
data	O
are	O
dealt	O
with	O
in	O
cart	O
by	O
using	O
surrogate	O
splits	O
.	O
the	O
idea	O
is	O
this	O
:	O
deﬁne	O
a	O
measure	B
of	O
similarity	O
between	O
any	O
two	O
splits	O
missing	O
,	O
decide	O
whether	O
it	O
goes	O
to	O
the	O
left	O
or	O
right	O
sub-tree	O
by	O
using	O
the	O
best	O
surrogate	O
split	O
.	O
if	O
it	O
is	O
missing	O
the	O
variable	O
containing	O
the	O
best	O
surrogate	O
split	O
,	O
then	O
the	O
second	O
best	O
is	O
used	O
,	O
and	O
so	O
on	O
.	O
to	O
grow	O
the	O
ﬁnal	O
tree	O
.	O
5.2.5	O
cal5	O
cal5	O
is	O
especially	O
designed	O
for	O
continuous	O
and	O
ordered	O
discrete	O
valued	O
attributes	O
,	O
though	O
an	O
added	O
sub-algorithm	O
is	O
able	O
to	O
handle	O
unordered	O
discrete	O
valued	O
attributes	O
as	O
well	O
.	O
~d½ó½~	O
is	O
a	O
decision	O
threshold	O
.	O
similar	O
to	O
other	O
decision	O
tree	O
methods	O
,	O
only	O
class	O
areas	O
bounded	O
by	O
hyperplanes	O
parallel	O
to	O
the	O
axes	O
of	O
the	O
feature	O
space	O
are	O
possible	O
.	O
evaluation	O
function	O
for	O
splitting	O
the	O
tree	O
will	O
be	O
constructed	O
sequentially	O
starting	O
with	O
one	O
attribute	O
and	O
branching	O
with	O
other	O
attributes	O
recursively	O
,	O
if	O
no	O
sufﬁcient	O
discrimination	O
of	O
classes	O
can	O
be	O
achieved	O
.	O
that	O
let	O
the	O
examplesµ	O
be	O
sampled	O
from	O
the	O
examples	O
expressed	O
withk	O
attributes	O
.	O
cal5	O
separates	O
the	O
examples	O
from	O
thek	O
dimensions	O
into	O
areas	O
represented	O
by	O
subsetsµñò	O
~d½ó½~	O
g	O
of	O
samples	O
,	O
where	O
the	O
classmjc	O
g	O
exists	O
with	O
a	O
probability	O
µ	O
''	O
cb	O
gýôöõ	O
¥cm	O
whereõ÷æøf	O
is	O
,	O
if	O
at	O
a	O
node	O
no	O
decision	O
for	O
a	O
classm	O
according	O
to	O
the	O
above	O
formula	O
can	O
be	O
made	O
,	O
a	O
let	O
be	O
a	O
certain	O
non-leaf	O
node	O
in	O
the	O
tree	O
construction	O
process	O
.	O
at	O
ﬁrst	O
the	O
attribute	O
with	O
the	O
best	O
local	O
discrimination	O
measure	B
at	O
this	O
node	O
has	O
to	O
be	O
determined	O
.	O
for	O
that	O
two	O
different	O
methods	O
can	O
be	O
used	O
(	O
controlled	O
by	O
an	O
option	O
)	O
:	O
a	O
statistical	B
and	O
an	O
entropy	O
measure	B
,	O
respectively	O
.	O
the	O
statistical	B
approach	O
is	O
working	O
without	O
any	O
knowledge	O
about	O
the	O
result	O
of	O
the	O
desired	O
discretisation	O
.	O
for	O
continuous	O
attributes	O
the	O
quotient	O
(	O
see	O
meyer-br¨otz	O
&	O
sch¨urmann	O
,	O
1970	O
)	O
:	O
branch	O
formed	O
with	O
a	O
new	O
attribute	O
is	O
appended	O
to	O
the	O
tree	O
.	O
if	O
this	O
attribute	O
is	O
continuous	O
,	O
a	O
discretisation	O
,	O
i.e	O
.	O
intervals	O
corresponding	O
to	O
qualitative	O
values	O
has	O
to	O
be	O
used	O
.	O
e©û	O
is	O
the	O
standard	O
deviation	O
of	O
is	O
the	O
mean	O
value	O
of	O
the	O
square	O
of	O
distances	O
between	O
the	O
classes	O
.	O
this	O
measure	B
has	O
to	O
be	O
computed	O
for	O
each	O
is	O
chosen	O
as	O
the	O
best	O
one	O
for	O
splitting	O
at	O
this	O
node	O
.	O
the	O
entropy	O
measure	B
provided	O
as	O
an	O
evaluation	O
function	O
requires	O
an	O
ùdú	O
êyìiyk'ì¨c	O
is	O
a	O
discrimination	O
measure	B
for	O
a	O
single	O
attribute	O
,	O
whereû	O
examples	O
in	O
from	O
the	O
centroid	O
of	O
the	O
attribute	O
value	O
andû	O
attribute	O
.	O
the	O
attribute	O
with	O
the	O
least	O
value	O
ofùdú	O
êyìiyk'ì¨c	O
intermediate	O
discretisation	O
at	O
n	O
for	O
each	O
attribute¶	O
using	O
the	O
splitting	O
procedure	O
described	O
	O
f	O
k	O
	O
f	O
	O
	O
	O
g	O
	O
û	O
	O
û	O
	O
	O
	O
g	O
sec	O
.	O
5.2	O
]	O
statlog	O
’	O
s	O
ml	O
algorithms	O
71	O
ýò	O
.	O
are	O
ordered	O
along	O
the	O
axis	O
of	O
the	O
selected	O
.	O
the	O
formula	O
for	O
computing	O
this	O
conﬁdence	O
interval	O
:	O
can	O
be	O
used	O
to	O
obtain	O
an	O
estimate	O
of	O
the	O
.	O
the	O
hypothesis	O
:	O
of	O
an	O
already	O
existing	O
interval	O
.	O
discretisation	O
well	O
known	O
id3	O
entropy	O
measure	B
(	O
quinlan	O
,	O
1986	O
)	O
.	O
the	O
attribute	O
with	O
the	O
largest	O
value	O
of	O
all	O
is	O
selected	O
and	O
occurs	O
,	O
than	O
the	O
discretisation	O
procedure	O
(	O
see	O
below	O
)	O
leads	O
to	O
a	O
reﬁnement	O
~d½½ó~	O
g	O
of	O
information	O
will	O
be	O
computed	O
for¶	O
k	O
by	O
the	O
below	O
.	O
then	O
the	O
gain¸c	O
¶	O
the	O
gain	O
is	O
chosen	O
as	O
the	O
best	O
one	O
for	O
splitting	O
at	O
that	O
node	O
.	O
note	O
that	O
at	O
each	O
node	O
~edd~	O
¶	O
will	O
be	O
considered	O
again	O
.	O
if¶	O
available	O
attributes¶	O
already	O
in	O
the	O
path	O
to	O
reaching	O
the	O
current	O
node	O
all	O
examples¤üòöµ	O
new	O
attribute¶	O
according	O
to	O
increasing	O
values	O
.	O
intervals	O
,	O
which	O
contain	O
an	O
ordered	O
set	O
of	O
values	O
of	O
the	O
attribute	O
,	O
are	O
formed	O
recursively	O
on	O
the¶	O
-axis	O
collecting	O
examples	O
from	O
left	O
to	O
right	O
until	O
a	O
class	O
decision	O
can	O
be	O
made	O
on	O
a	O
given	O
level	O
of	O
conﬁdenceá	O
	O
the	O
number	O
letý	O
be	O
a	O
current	O
interval	O
containingk	O
examples	O
of	O
different	O
classes	O
andk	O
	O
.	O
thenk	O
of	O
examples	O
belonging	O
to	O
classm	O
g	O
on	O
the	O
current	O
node	O
probability¥¦cm	O
g+ôþõ	O
h1	O
:	O
there	O
exists	O
a	O
classm	O
	O
occurring	O
iný	O
with¥cm	O
	O
occurring	O
iný	O
õ	O
holds	O
on	O
a	O
certain	O
level	O
g+ÿ	O
the	O
inequality¥cm	O
h2	O
:	O
for	O
all	O
classesm	O
ná	O
yields	O
a	O
conﬁdence	O
interval	O
of	O
conﬁdencef	O
(	O
for	O
a	O
givená	O
)	O
.	O
xá	O
long	O
sequence	O
of	O
examples	O
the	O
true	O
value	O
of	O
probability	O
lies	O
within	O
g	O
for¥cm	O
g	O
and	O
in	O
a	O
an	O
estimation	O
on	O
the	O
levelf	O
cm	O
g	O
with	O
probability	O
cm	O
xá	O
³2ábm	O
m	O
³2ák	O
;	O
ex³	O
³2ák	O
;	O
e	O
!	O
³	O
labels	O
for	O
each	O
classm	O
;	O
see	O
unger	O
&	O
wysotski	O
(	O
1981	O
)	O
)	O
.	O
h1	O
:	O
	O
h2	O
:	O
	O
i.e	O
.	O
this	O
hypothesis	O
is	O
true	O
,	O
if	O
for	O
each	O
classm	O
now	O
the	O
following	O
“	O
meta-decision	O
”	O
on	O
the	O
dominance	O
of	O
a	O
class	O
iný	O
can	O
be	O
deﬁned	O
as	O
:	O
if	O
there	O
exists	O
a	O
classm	O
,	O
where	O
h1	O
is	O
true	O
thenm	O
dominates	O
iný	O
.	O
the	O
intervalý	O
if	O
for	O
all	O
classes	O
appearing	O
iný	O
ý	O
.	O
in	O
this	O
case	O
the	O
interval	O
will	O
be	O
closed	O
,	O
too	O
.	O
a	O
new	O
test	O
with	O
another	O
attribute	O
is	O
if	O
neither	O
1.	O
nor	O
2.	O
occurs	O
,	O
the	O
intervalý	O
has	O
to	O
be	O
extended	O
by	O
the	O
next	O
example	B
of	O
the	O
ý	O
a	O
majority	O
decision	O
will	O
be	O
made	O
.	O
is	O
derived	O
from	O
the	O
tchebyschev	O
inequality	O
by	O
supposing	O
a	O
bernoulli	O
distribution	O
of	O
class	O
i.e	O
.	O
h1	O
is	O
true	O
,	O
if	O
the	O
complete	O
conﬁdence	O
interval	O
lies	O
above	O
the	O
predeﬁned	O
threshold	O
,	O
and	O
order	O
of	O
the	O
current	O
attribute	O
.	O
if	O
there	O
are	O
no	O
more	O
examples	O
for	O
a	O
further	O
extension	O
of	O
taking	O
into	O
account	O
this	O
conﬁdence	O
interval	O
the	O
hypotheses	O
h1	O
and	O
h2	O
are	O
tested	O
by	O
:	O
cm	O
cm	O
cm	O
g+ôþõ	O
gñÿ	O
c	O
closed	O
.	O
the	O
corresponding	O
path	O
of	O
the	O
tree	O
is	O
terminated	O
.	O
the	O
threshold	O
.	O
1	O
.	O
2	O
.	O
3.	O
necessary	O
.	O
will	O
be	O
tested	O
against	O
:	O
áðmc	O
~dde	O
the	O
complete	O
conﬁdence	O
interval	O
is	O
less	O
than	O
the	O
hypothesis	O
h2	O
is	O
true	O
,	O
then	O
no	O
class	O
dominates	O
in	O
is	O
	O
~	O
~	O
f	O
|	O
~	O
¶	O
	O
	O
h	O
k	O
¼	O
	O
	O
¼	O
	O
~	O
	O
¼	O
	O
	O
	O
¼	O
	O
f	O
	O
g	O
	O
	O
f	O
	O
	O
f	O
	O
k	O
g	O
e	O
f	O
~	O
	O
õ	O
	O
f	O
g	O
72	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
[	O
ch	O
.	O
5	O
merging	O
is	O
removed	O
,	O
if	O
the	O
inequality	O
:	O
yield	O
the	O
leaf	O
nodes	O
of	O
the	O
decision	O
tree	O
.	O
the	O
same	O
rule	O
is	O
applied	O
for	O
adjacent	O
intervals	O
where	O
no	O
class	O
dominates	O
and	O
which	O
contain	O
identical	O
remaining	O
classes	O
due	O
to	O
the	O
following	O
|	O
with	O
the	O
same	O
class	O
label	O
can	O
be	O
merged	O
.	O
the	O
resultant	O
intervals	O
ü	O
ç	O
is	O
the	O
total	O
number	O
of	O
different	O
class	O
labels	O
occurring	O
iný	O
adjacent	O
intervalsý	O
k	O
elimination	O
procedure	O
.	O
a	O
class	O
within	O
an	O
intervalý	O
gýôfdh	O
is	O
satisﬁed	O
,	O
wherek	O
cm	O
class	O
will	O
be	O
omitted	O
,	O
if	O
its	O
probability	O
iný	O
distribution	O
of	O
all	O
classes	O
occurring	O
iný	O
)	O
.	O
these	O
resultant	O
intervals	O
yield	O
the	O
intermediate	O
all	O
terminated	O
.	O
note	O
that	O
a	O
majority	O
decision	O
is	O
made	O
at	O
a	O
node	O
if	O
,	O
because	O
of	O
a	O
too	O
smallá	O
nodes	O
in	O
the	O
construction	O
of	O
the	O
decision	O
tree	O
,	O
for	O
which	O
further	O
branching	O
will	O
be	O
performed	O
.	O
every	O
intermediate	O
node	O
becomes	O
the	O
start	O
node	O
for	O
a	O
further	O
iteration	O
step	O
repeating	O
the	O
steps	O
from	O
sections	O
5.2.5	O
to	O
5.2.5.	O
the	O
algorithm	O
stops	O
when	O
all	O
intermediate	O
nodes	O
are	O
,	O
(	O
i.e	O
.	O
a	O
is	O
less	O
than	O
the	O
value	O
of	O
an	O
assumed	O
constant	O
no	O
estimation	O
of	O
probability	O
can	O
be	O
done	O
.	O
discrete	O
unordered	O
attributes	O
to	O
distinguish	O
between	O
the	O
different	O
types	O
of	O
attributes	O
the	O
program	O
needs	O
a	O
special	O
input	B
vector	O
.	O
the	O
algorithm	O
for	O
handling	O
unordered	O
discrete	O
valued	O
attributes	O
is	O
similar	O
to	O
that	O
described	O
in	O
sections	O
5.2.5	O
to	O
5.2.5	O
apart	O
from	O
interval	O
construction	O
.	O
instead	O
of	O
intervals	O
discrete	O
points	O
on	O
the	O
axis	O
of	O
the	O
current	O
attribute	O
have	O
to	O
be	O
considered	O
.	O
all	O
examples	O
with	O
the	O
same	O
value	O
of	O
the	O
current	O
discrete	O
attribute	O
are	O
related	O
to	O
one	O
point	O
on	O
the	O
axis	O
.	O
for	O
each	O
point	O
the	O
hypotheses	O
h1	O
and	O
h2	O
will	O
be	O
tested	O
and	O
the	O
corresponding	O
actions	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
performed	O
,	O
respectively	O
.	O
if	O
neither	O
h1	O
nor	O
h2	O
is	O
true	O
,	O
a	O
majority	O
decision	O
will	O
be	O
made	O
.	O
this	O
approach	O
also	O
allows	O
handling	O
mixed	O
(	O
discrete	O
and	O
continuous	O
)	O
valued	O
attributes	O
.	O
probability	O
threshold	O
and	O
conﬁdence	O
as	O
can	O
be	O
seen	O
from	O
the	O
above	O
two	O
parameters	O
affect	O
the	O
tree	O
construction	O
process	O
:	O
the	O
ﬁrst	O
for	O
accept	O
a	O
node	O
and	O
the	O
second	O
is	O
a	O
predeﬁned	O
conﬁdence	O
level	O
the	O
tree	O
is	O
pre-pruned	O
at	O
should	O
depend	O
on	O
the	O
training	O
(	O
or	O
pruning	B
)	O
set	O
and	O
determines	O
the	O
accuracy	O
of	O
the	O
approximation	O
of	O
the	O
class	O
hyperplane	O
,	O
i.e	O
.	O
the	O
admissible	O
error	O
rate	O
.	O
the	O
higher	O
the	O
degree	O
of	O
overlapping	O
of	O
class	O
regions	O
in	O
the	O
feature	O
space	O
the	O
less	O
the	O
threshold	O
has	O
to	O
be	O
for	O
getting	O
a	O
reasonable	O
classiﬁcation	B
result	O
.	O
the	O
accuracy	O
of	O
the	O
approximation	O
and	O
simulta-	O
neously	O
the	O
complexity	O
of	O
the	O
resulting	O
tree	O
can	O
be	O
controlled	O
by	O
the	O
user	O
.	O
in	O
addition	O
to	O
in	O
a	O
class	O
dependent	O
manner	O
,	O
taking	O
into	O
account	O
different	O
costs	O
for	O
misclassiﬁcation	O
of	O
different	O
classes	O
.	O
with	O
other	O
words	O
the	O
inﬂuence	O
of	O
a	O
given	O
cost	O
matrix	O
can	O
be	O
taken	O
into	O
account	O
during	O
training	O
,	O
if	O
the	O
different	O
costs	O
for	O
misclassiﬁcation	O
can	O
be	O
reﬂected	O
by	O
a	O
class	O
dependent	O
threshold	O
vector	O
.	O
one	O
approach	O
has	O
been	O
adopted	O
by	O
cal5	O
:	O
1	O
.	O
2.	O
is	O
a	O
predeﬁned	O
thresholdõ	O
.	O
if	O
the	O
conditional	O
probability	O
of	O
a	O
class	O
exceeds	O
the	O
thresholdõ	O
that	O
node	O
.	O
the	O
choice	O
ofõ	O
therefore	O
by	O
selecting	O
the	O
value	O
ofõ	O
a	O
constantõ	O
g	O
of	O
the	O
cost	O
matrix	O
will	O
be	O
summed	O
up	O
(	O
ä	O
every	O
columnbc	O
the	O
threshold	O
of	O
that	O
class	O
relating	O
to	O
the	O
column	O
,	O
for	O
whichä	O
$	O
	O
has	O
to	O
be	O
chosen	O
by	O
the	O
user	O
like	O
in	O
the	O
case	O
of	O
a	O
constant	O
threshold	O
(	O
õ	O
	O
the	O
other	O
thresholdsõ	O
ó0c	O
cä	O
$	O
	O
~d½½ó~	O
	O
will	O
be	O
computed	O
by	O
the	O
formula	O
ó	O
	O
the	O
algorithm	O
allows	O
to	O
choose	O
the	O
thresholdõ	O
	O
)	O
;	O
ó	O
	O
)	O
;	O
is	O
a	O
maximum	O
(	O
ä	O
ó	O
)	O
~d½½ó~	O
3.	O
ü	O
~	O
ý	O
	O
á	O
	O
f	O
	O
¿	O
¿	O
õ	O
	O
h	O
ä	O
¿	O
g	O
ã	O
õ	O
¿	O
	O
f	O
	O
g	O
	O
sec	O
.	O
5.2	O
]	O
statlog	O
’	O
s	O
ml	O
algorithms	O
73	O
are	O
proportional	O
to	O
their	O
corresponding	O
column	O
sums	O
of	O
the	O
cost	O
matrix	O
,	O
which	O
can	O
be	O
interpreted	O
as	O
a	O
penalty	O
measure	B
for	O
misclassiﬁcation	O
into	O
those	O
classes	O
.	O
for	O
estimating	O
the	O
appropriate	O
class	O
the	O
better	O
the	O
demanded	O
quality	O
of	O
estimation	O
and	O
the	O
worse	O
the	O
ability	O
to	O
separate	O
intervals	O
,	O
since	O
the	O
algorithm	O
is	O
enforced	O
to	O
construct	O
large	O
intervals	O
in	O
order	O
to	O
get	O
sufﬁcient	O
statistics	O
.	O
from	O
experience	O
should	O
be	O
set	O
to	O
one	O
.	O
thus	O
all	O
values	O
of	O
the	O
class	O
dependent	O
thresholds	O
compared	O
with	O
the	O
threshold	O
the	O
conﬁdence	O
levelá	O
probability	O
has	O
an	O
inversely	O
proportional	O
effect	O
.	O
the	O
less	O
the	O
value	O
ofá	O
andõ	O
a	O
suitable	O
approach	O
for	O
the	O
automatically	O
choosing	O
the	O
parametersá	O
available	O
.	O
therefore	O
a	O
program	O
for	O
varying	O
the	O
parameterá	O
between	O
,	O
by	O
default	O
,	O
°	O
ô	O
andõ	O
between	O
,	O
by	O
default	O
,	O
°	O
is	O
used	O
to	O
predeﬁne	O
the	O
best	O
parameter	O
combination	O
,	O
i.e	O
.	O
that	O
which	O
gives	O
the	O
minimum	O
cost	O
(	O
or	O
error	O
rate	O
,	O
respectively	O
)	O
on	O
a	O
test	O
set	O
.	O
however	O
,	O
this	O
procedure	O
may	O
be	O
computationally	O
expensive	O
in	O
relation	O
to	O
the	O
number	O
of	O
attributes	O
and	O
the	O
size	O
of	O
data	O
set	O
.	O
in	O
steps	O
of°	O
	O
and°	O
is	O
not	O
±	O
and	O
	O
5.2.6	O
bayes	O
tree	O
this	O
is	O
a	O
bayesian	O
approach	O
to	O
decision	O
trees	O
that	O
is	O
described	O
by	O
buntine	O
(	O
1992	O
)	O
,	O
and	O
is	O
available	O
in	O
the	O
ind	O
package	O
.	O
it	O
is	O
based	O
on	O
a	O
full	O
bayesian	O
approach	O
:	O
as	O
such	O
it	O
requires	O
the	O
speciﬁcation	O
of	O
prior	O
class	O
probabilities	O
(	O
usually	O
based	O
on	O
empirical	O
class	O
proportions	O
)	O
,	O
and	O
a	O
probability	O
model	O
for	O
the	O
decision	O
tree	O
.	O
a	O
multiplicative	O
probability	O
model	O
for	O
the	O
probability	O
of	O
a	O
tree	O
is	O
adopted	O
.	O
using	O
this	O
form	O
simpliﬁes	O
the	O
problem	O
of	O
computing	O
tree	O
probabilities	O
,	O
and	O
the	O
decision	O
to	O
grow	O
a	O
tree	O
from	O
a	O
particular	O
node	O
may	O
then	O
be	O
based	O
on	O
the	O
increase	O
in	O
probability	O
of	O
the	O
resulting	O
tree	O
,	O
thus	O
using	O
only	O
information	O
local	O
to	O
that	O
node	O
.	O
of	O
all	O
potential	O
splits	O
at	O
that	O
node	O
,	O
that	O
split	O
is	O
chosen	O
which	O
increases	O
the	O
posterior	O
probability	O
of	O
the	O
tree	O
by	O
the	O
greatest	O
amount	O
.	O
post-pruning	O
is	O
done	O
by	O
using	O
the	O
same	O
principle	O
,	O
i.e	O
.	O
choosing	O
the	O
cut	B
that	O
maximises	O
the	O
posterior	O
probability	O
of	O
the	O
resulting	O
tree	O
.	O
of	O
all	O
those	O
tree	O
structures	O
resulting	O
from	O
pruning	B
a	O
node	O
from	O
the	O
given	O
tree	O
,	O
choose	O
that	O
which	O
has	O
maximum	O
posterior	O
probability	O
.	O
an	O
alternative	O
to	O
post-pruning	O
is	O
to	O
smooth	O
class	O
probabilities	O
.	O
as	O
an	O
example	B
is	O
dropped	O
down	O
the	O
tree	O
,	O
it	O
goes	O
through	O
various	O
nodes	O
.	O
the	O
class	O
probabilities	O
of	O
each	O
node	O
visited	O
contribute	O
to	O
the	O
ﬁnal	O
class	O
probabilities	O
(	O
by	O
a	O
weighted	O
sum	O
)	O
,	O
so	O
that	O
the	O
ﬁnal	O
class	O
probabilities	O
inherit	O
probabilities	O
evaluated	O
higher	O
up	O
the	O
tree	O
.	O
this	O
stabilises	O
the	O
class	O
probability	O
estimates	O
(	O
i.e	O
.	O
reduces	O
their	O
variance	O
)	O
at	O
the	O
expense	O
of	O
introducing	O
bias	O
.	O
costs	O
may	O
be	O
included	O
in	O
learning	O
and	O
testing	O
via	O
a	O
utility	O
function	O
for	O
each	O
class	O
(	O
the	O
utility	O
is	O
the	O
negative	O
of	O
the	O
cost	O
for	O
the	O
two-class	O
case	O
)	O
.	O
5.2.7	O
rule-learning	O
algorithms	O
:	O
cn2	O
this	O
algorithm	O
of	O
clark	O
and	O
niblett	O
’	O
s	O
was	O
sketched	O
earlier	O
.	O
it	O
aims	O
to	O
modify	O
the	O
basic	O
aq	O
algorithm	O
of	O
michalski	O
in	O
such	O
a	O
way	O
as	O
to	O
equip	O
it	O
to	O
cope	O
with	O
noise	O
and	O
other	O
complications	O
in	O
the	O
data	O
.	O
in	O
particular	O
during	O
its	O
search	O
for	O
good	O
complexes	O
cn2	O
does	O
not	O
automatically	O
remove	O
from	O
consideration	O
a	O
candidate	O
that	O
is	O
found	O
to	O
include	O
one	O
or	O
more	O
negative	O
example	B
.	O
rather	O
it	O
retains	O
a	O
set	O
of	O
complexes	O
in	O
its	O
search	O
that	O
is	O
evaluated	O
statistically	O
as	O
covering	O
a	O
large	O
number	O
of	O
examples	O
of	O
a	O
given	O
class	O
and	O
few	O
of	O
other	O
classes	O
.	O
moreover	O
,	O
the	O
manner	O
in	O
which	O
the	O
search	O
is	O
conducted	O
is	O
general-to-speciﬁc	O
.	O
each	O
trial	O
specialisation	O
step	O
takes	O
the	O
form	O
of	O
either	O
adding	O
a	O
new	O
conjunctive	O
term	O
or	O
removing	O
a	O
disjunctive	O
one	O
.	O
having	O
found	O
a	O
good	O
complex	O
,	O
the	O
algorithm	O
removes	O
those	O
examples	O
it	O
	O
f	O
°	O
	O
	O
	O
°	O
±	O
is	O
empty	O
then	O
stop	O
and	O
return	O
rule	O
list	O
;	O
and	O
add	O
the	O
rule	O
“	O
if	O
best	O
cpx	O
then	O
is	O
the	O
most	O
common	O
class	O
of	O
examples	O
covered	O
74	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
[	O
ch	O
.	O
5	O
covers	O
from	O
the	O
training	O
set	O
and	O
adds	O
the	O
rule	O
“	O
if	O
<	O
complex	O
>	O
then	O
predict	O
<	O
class	O
>	O
”	O
to	O
the	O
end	O
of	O
the	O
rule	O
list	O
.	O
the	O
process	O
terminates	O
for	O
each	O
given	O
class	O
when	O
no	O
more	O
acceptable	O
complexes	O
can	O
be	O
found	O
.	O
clark	O
&	O
niblett	O
’	O
s	O
(	O
1989	O
)	O
cn2	O
algorithm	O
has	O
the	O
following	O
main	O
features	O
:	O
1	O
)	O
the	O
dependence	O
on	O
speciﬁc	O
training	O
examples	O
during	O
search	O
(	O
a	O
feature	O
of	O
the	O
aq	O
algorithm	O
)	O
is	O
removed	O
;	O
2	O
)	O
it	O
combines	O
the	O
efﬁciency	O
and	O
ability	O
to	O
cope	O
with	O
noisy	O
data	O
of	O
decision-	O
tree	O
learning	O
with	O
the	O
if-then	O
rule	O
form	O
and	O
ﬂexible	O
search	O
strategy	O
of	O
the	O
aq	O
family	O
;	O
3	O
)	O
it	O
contrasts	O
with	O
other	O
approaches	O
to	O
modify	O
aq	O
to	O
handle	O
noise	O
in	O
that	O
the	O
basic	O
aq	O
algorithm	O
itself	O
is	O
generalised	O
rather	O
than	O
“	O
patched	O
”	O
with	O
additional	O
pre-	O
and	O
post-processing	O
techniques	O
;	O
and	O
4	O
)	O
it	O
produces	O
both	O
ordered	O
and	O
unordered	O
rules	O
.	O
and	O
output	B
a	O
set	O
of	O
rules	O
called	O
rule	O
list	O
.	O
the	O
core	O
of	O
cn2	O
is	O
the	O
procedure	O
as	O
follows	O
,	O
but	O
it	O
needs	O
to	O
use	O
a	O
sub-procedure	O
to	O
return	O
the	O
value	O
of	O
best	O
cpx	O
:	O
1.	O
let	O
rule	O
list	O
be	O
the	O
empty	O
list	O
;	O
cn2	O
inputs	O
a	O
set	O
of	O
training	O
examplesµ	O
2.	O
let	O
best	O
cpx	O
be	O
the	O
best	O
complex	O
found	O
fromµ	O
if	O
best	O
cpx	O
orµ	O
4.	O
remove	O
the	O
examples	O
covered	O
by	O
best	O
cpx	O
fromµ	O
class=m	O
”	O
to	O
the	O
end	O
of	O
rule	O
list	O
wherem	O
by	O
best	O
cpx	O
;	O
re-enter	O
at	O
step	O
(	O
2	O
)	O
.	O
3.	O
;	O
this	O
subprocedure	O
is	O
used	O
for	O
producing	O
ordered	O
rules	O
.	O
cn2	O
also	O
produces	O
a	O
set	O
of	O
unordered	O
rules	O
,	O
which	O
uses	O
a	O
slightly	O
different	O
procedure	O
.	O
to	O
produce	O
unordered	O
rules	O
,	O
the	O
above	O
procedure	O
is	O
repeated	O
for	O
each	O
class	O
in	O
turn	O
.	O
in	O
addition	O
,	O
in	O
step	O
4	O
only	O
the	O
positive	O
examples	O
should	O
be	O
removed	O
.	O
the	O
procedure	O
for	O
ﬁnding	O
the	O
best	O
complex	O
is	O
as	O
follows	O
:	O
1.	O
let	O
the	O
set	O
star	O
contain	O
only	O
the	O
empty	O
complex	O
and	O
best	O
cpx	O
be	O
nil	O
;	O
2.	O
let	O
selectors	O
be	O
the	O
set	O
of	O
all	O
possible	O
selectors	O
;	O
if	O
star	O
is	O
empty	O
,	O
then	O
return	O
the	O
current	O
best	O
cpx	O
;	O
3	O
.	O
4.	O
specialise	O
all	O
complexes	O
in	O
star	O
as	O
newstar	O
,	O
which	O
is	O
the	O
setº2	O
«	O
n¬	O
star~	O
y	O
ò	O
«	O
!	O
ò	O
¬âdé¤ö	O
b¸	O
selectors¾	O
and	O
remove	O
all	O
complexes	O
in	O
newstar	O
that	O
are	O
either	O
in	O
star	O
(	O
i.e	O
.	O
the	O
unspe-	O
cialised	O
ones	O
)	O
or	O
are	O
null	O
(	O
i.e.ö	O
¸	O
5.	O
for	O
every	O
complex	O
{	O
in	O
newstar	O
,	O
if	O
{	O
when	O
tested	O
onµ	O
,	O
then	O
replace	O
the	O
current	O
value	O
of	O
best	O
cpx	O
by	O
{	O
	O
;	O
remove	O
criteria	O
when	O
tested	O
onµ	O
all	O
worst	O
complexes	O
from	O
newstar	O
until	O
the	O
size	O
of	O
newstar	O
is	O
below	O
the	O
user-deﬁned	O
maximum	O
;	O
set	O
star	O
to	O
newstar	O
and	O
re-enter	O
at	O
step	O
(	O
3	O
)	O
.	O
is	O
statistically	O
signiﬁcant	O
(	O
in	O
signiﬁcance	O
)	O
and	O
better	O
than	O
(	O
in	O
goodness	O
)	O
best	O
cpx	O
according	O
to	O
user-deﬁned	O
as	O
can	O
be	O
seen	O
from	O
the	O
algorithm	O
,	O
the	O
basic	O
operation	O
of	O
cn2	O
is	O
that	O
of	O
generating	O
a	O
complex	O
(	O
i.e	O
.	O
a	O
conjunct	O
of	O
attribute	O
tests	O
)	O
which	O
covers	O
(	O
i.e	O
.	O
is	O
satisﬁed	O
by	O
)	O
a	O
subset	O
of	O
the	O
training	O
examples	O
.	O
this	O
complex	O
forms	O
the	O
condition	O
part	O
of	O
a	O
production	O
rule	O
“	O
if	O
condition	O
kê	O
)	O
;	O
then	O
class	O
=m	O
”	O
,	O
where	O
class	O
is	O
the	O
most	O
common	O
class	O
in	O
the	O
(	O
training	O
)	O
examples	O
which	O
satisfy	O
the	O
condition	O
.	O
the	O
condition	O
is	O
a	O
conjunction	O
of	O
selectors	O
,	O
each	O
of	O
which	O
represents	O
a	O
test	O
on	O
the	O
values	O
of	O
an	O
attribute	O
such	O
as	O
“	O
weather=wet	O
”	O
.	O
the	O
search	O
proceeds	O
in	O
both	O
aq	O
and	O
cn2	O
by	O
repeatedly	O
specialising	O
candidate	O
complexes	O
until	O
one	O
which	O
covers	O
a	O
large	O
number	O
of	O
examples	O
of	O
a	O
single	O
class	O
and	O
few	O
of	O
other	O
classes	O
is	O
located	O
.	O
details	O
of	O
each	O
search	O
are	O
outlined	O
below	O
.	O
¼	O
	O
	O
	O
	O
sec	O
.	O
5.2	O
]	O
statlog	O
’	O
s	O
ml	O
algorithms	O
75	O
the	O
search	O
for	O
specialisations	O
the	O
cn2	O
algorithm	O
works	O
in	O
an	O
iterative	O
fashion	O
,	O
each	O
iteration	O
searching	O
for	O
a	O
complex	O
amples	O
of	O
the	O
current	O
class	O
are	O
called	O
“	O
positive	O
”	O
examples	O
and	O
the	O
other	O
examples	O
are	O
called	O
“	O
negative	O
”	O
examples	O
.	O
)	O
the	O
complex	O
must	O
be	O
both	O
predictive	O
and	O
reliable	O
,	O
as	O
determined	O
by	O
cn2	O
’	O
s	O
evaluation	O
functions	O
.	O
having	O
found	O
a	O
good	O
complex	O
,	O
those	O
examples	O
it	O
covers	O
are	O
covering	O
a	O
large	O
number	O
of	O
examples	O
of	O
a	O
single	O
classm	O
and	O
few	O
of	O
other	O
classes	O
.	O
(	O
the	O
ex-	O
removed	O
from	O
the	O
training	O
set	O
and	O
the	O
rule	O
“	O
ifÿ	O
complexô	O
then	O
class=m	O
”	O
is	O
added	O
to	O
the	O
end	O
of	O
the	O
rule	O
list	O
.	O
this	O
greedy	O
process	O
iterates	O
until	O
no	O
more	O
satisfactory	O
complexes	O
can	O
be	O
found	O
.	O
to	O
generate	O
a	O
single	O
rule	O
,	O
cn2	O
ﬁrst	O
starts	O
with	O
the	O
most	O
general	O
rule	O
“	O
if	O
true	O
then	O
class=c	O
”	O
(	O
i.e	O
.	O
all	O
examples	O
are	O
class	O
c	O
)	O
,	O
where	O
c	O
is	O
the	O
current	O
class	O
.	O
then	O
cn2	O
searches	O
for	O
complexes	O
by	O
carrying	O
out	O
a	O
general-to-speciﬁc	O
beam	O
search	O
.	O
the	O
extent	O
of	O
the	O
beam	O
search	O
for	O
a	O
complex	O
can	O
be	O
regulated	O
by	O
controlling	O
the	O
width	O
(	O
i.e	O
.	O
number	O
of	O
complexes	O
explored	O
in	O
parallel	O
)	O
of	O
the	O
beam	O
.	O
at	O
each	O
stage	O
in	O
the	O
search	O
,	O
cn2	O
retains	O
a	O
size-limited	O
the	O
set	O
of	O
all	O
possible	O
selectors	O
with	O
the	O
current	O
star	O
,	O
eliminating	O
all	O
the	O
null	O
and	O
unchanged	O
elements	O
in	O
the	O
resulting	O
set	O
of	O
complexes	O
.	O
(	O
a	O
null	O
complex	O
is	O
one	O
that	O
contains	O
a	O
pair	O
of	O
this	O
set	O
,	O
carrying	O
out	O
a	O
beam	O
search	O
of	O
the	O
space	O
of	O
complexes	O
.	O
a	O
complex	O
is	O
specialised	O
by	O
adding	O
a	O
new	O
conjunctive	O
term	O
in	O
one	O
of	O
its	O
selector	O
.	O
each	O
complex	O
can	O
be	O
specialised	O
in	O
several	O
ways	O
,	O
and	O
cn2	O
generates	O
and	O
evaluates	O
all	O
such	O
specialisations	O
.	O
the	O
star	O
is	O
trimmed	O
after	O
completion	O
of	O
this	O
step	O
by	O
removing	O
its	O
lowest	O
ranking	O
elements	O
as	O
measured	O
by	O
an	O
evaluation	O
function	O
that	O
we	O
will	O
describe	O
shortly	O
.	O
set	O
or	O
starä	O
of	O
“	O
complexes	O
explored	O
so	O
far	O
”	O
.	O
the	O
system	O
examines	O
only	O
specialisations	O
of	O
the	O
implementation	O
of	O
the	O
specialisation	O
step	O
in	O
cn2	O
is	O
to	O
repeatedly	O
intersect	O
incompatible	O
selectors	O
,	O
for	O
example	B
,	O
	O
used	O
for	O
further	O
specialisation	O
in	O
theé	O
	O
	O
ì¶ã	O
)	O
.	O
goodness	O
is	O
a	O
measure	B
of	O
the	O
quality	O
of	O
the	O
search	O
heuristics	O
there	O
are	O
two	O
heuristics	O
used	O
in	O
the	O
search	O
for	O
the	O
best	O
complexes	O
and	O
both	O
can	O
be	O
tuned	O
by	O
the	O
user	O
depending	O
on	O
the	O
speciﬁc	O
domain	O
:	O
the	O
signiﬁcance	O
level	O
and	O
the	O
goodness	O
measure	B
.	O
signiﬁcance	O
is	O
an	O
absolute	O
threshold	O
such	O
that	O
any	O
(	O
specialised	O
)	O
complexes	O
below	O
the	O
threshold	O
will	O
not	O
be	O
considered	O
for	O
selecting	O
the	O
best	O
complex	O
(	O
but	O
they	O
are	O
still	O
complexes	O
so	O
it	O
is	O
used	O
to	O
order	O
the	O
complexes	O
that	O
are	O
above	O
the	O
signiﬁcance	O
threshold	O
to	O
select	O
the	O
best	O
complex	O
.	O
several	O
difference	O
functions	O
can	O
be	O
chosen	O
to	O
guide	O
the	O
search	O
for	O
a	O
good	O
rule	O
in	O
the	O
	O
)	O
.	O
cn2/aq	O
system	O
,	O
for	O
example	B
:	O
“	O
number	O
of	O
correctly	O
classiﬁed	O
examples	O
divided	O
by	O
total	O
number	O
covered	O
”	O
.	O
this	O
is	O
the	O
traditional	O
aq	O
evaluation	O
function	O
.	O
entropy	O
,	O
similar	O
to	O
the	O
information	O
gain	O
measure	B
used	O
by	O
id3	O
(	O
quinlan	O
,	O
1986	O
)	O
and	O
other	O
decision	O
tree	O
algorithms	O
.	O
is	O
the	O
number	O
of	O
classes	O
in	O
the	O
problem	O
.	O
g	O
where	O
fdgih	O
the	O
laplacian	O
error	O
estimate	O
:	O
ûmym	O
cbkk0k	O
ã¶¹m	O
¬¶	O
$	O
ck	O
cked	O
is	O
the	O
total	O
number	O
of	O
examples	O
covered	O
by	O
the	O
rule	O
,	O
k	O
	O
''	O
	O
the	O
intersection	O
of	O
set	O
with	O
set	O
examples	O
covered	O
by	O
the	O
rule	O
andd	O
#	O
(	O
*	O
)	O
.	O
for	O
example	B
,	O
using	O
‘	O
+	O
’	O
to	O
abbreviate	O
is	O
the	O
set	O
!	O
$	O
#	O
%	O
'	O
&	O
47	O
&	O
1	O
,	O
.+	O
20	O
&	O
6	O
,	O
.+	O
20+	O
47	O
&	O
1/0+	O
4-	O
)	O
.	O
if	O
we	O
now	O
remove	O
‘	O
’	O
,	O
-	O
,	O
.+	O
/0	O
&	O
1	O
,	O
+	O
20	O
&	O
3/0+	O
intersected	O
with	O
5	O
,	O
	O
&	O
1/0	O
&	O
320	O
&	O
64-	O
)	O
is	O
-	O
,	O
.+	O
/0	O
&	O
1	O
,	O
.+	O
/0+	O
20	O
&	O
1	O
,	O
+	O
4-	O
)	O
/0+	O
4-	O
)	O
.	O
20+	O
47	O
&	O
1/0+	O
20+	O
47	O
&	O
1	O
,	O
.+	O
/0+	O
20	O
&	O
1	O
,	O
.+	O
/0+	O
unchanged	O
elements	O
in	O
this	O
set	O
we	O
obtain	O
5	O
,	O
+	O
ed+	O
20+	O
47	O
&	O
3/0+	O
is	O
the	O
number	O
of	O
positive	O
ú	O
~	O
k	O
í	O
~	O
d	O
g	O
	O
í	O
k	O
í	O
	O
76	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
[	O
ch	O
.	O
5	O
cn2	O
uses	O
one	O
of	O
these	O
criteria	O
according	O
to	O
the	O
user	O
’	O
s	O
choice	O
to	O
order	O
the	O
goodness	O
of	O
rules	O
.	O
to	O
test	O
signiﬁcance	O
,	O
cn2	O
uses	O
the	O
entropy	O
statistic	O
.	O
this	O
is	O
given	O
by	O
:	O
á|	O
|.~dedi~	O
is	O
the	O
observed	O
frequency	O
distribution	O
of	O
examples	O
among	O
is	O
the	O
expected	O
frequency	O
distribution	O
of	O
the	O
same	O
number	O
of	O
examples	O
under	O
the	O
assumption	O
that	O
the	O
complex	O
selects	O
examples	O
	O
logcó¥	O
where	O
the	O
distribution¥	O
|.~e½½½~	O
classes	O
satisfying	O
a	O
given	O
complex	O
andù	O
randomly	O
.	O
this	O
is	O
taken	O
as	O
thew	O
	O
covered	O
examples	O
distributed	O
among	O
classes	O
r¥	O
with	O
the	O
same	O
probability	O
as	O
that	O
of	O
examples	O
in	O
the	O
entire	O
training	O
set	O
.	O
this	O
statistic	O
provides	O
an	O
information-theoretic	O
measure	B
of	O
the	O
(	O
non-commutative	O
)	O
distance	O
between	O
the	O
two	O
distributions	O
.	O
the	O
user	O
provides	O
a	O
threshold	O
of	O
signiﬁcance	O
below	O
which	O
rules	O
are	O
rejected	O
.	O
missing	O
values	O
similar	O
to	O
newid	O
,	O
cn2	O
can	O
deal	O
with	O
unknown	O
or	O
don	O
’	O
t-care	O
values	O
.	O
during	O
rule	O
generation	O
,	O
a	O
similar	O
policy	O
of	O
handling	O
unknowns	O
and	O
don	O
’	O
t-cares	O
is	O
followed	O
:	O
unknowns	O
are	O
split	O
into	O
fractional	O
examples	O
and	O
don	O
’	O
t-cares	O
are	O
duplicated	O
.	O
each	O
rule	O
produced	O
by	O
cn2	O
is	O
associated	O
with	O
a	O
set	O
of	O
counts	O
which	O
corresponds	O
to	O
the	O
number	O
of	O
examples	O
,	O
covered	O
by	O
the	O
rule	O
,	O
belonging	O
to	O
each	O
class	O
.	O
strictly	O
speaking	O
,	O
for	O
the	O
ordered	O
rules	O
the	O
counts	O
attached	O
to	O
rules	O
when	O
writing	O
the	O
rule	O
set	O
should	O
be	O
those	O
encountered	O
during	O
rule	O
generation	O
.	O
however	O
,	O
for	O
unordered	O
rules	O
,	O
the	O
counts	O
to	O
attach	O
are	O
generated	O
after	O
rule	O
generation	O
in	O
a	O
second	O
pass	O
,	O
following	O
the	O
execution	O
policy	O
of	O
splitting	O
an	O
example	B
with	O
unknown	O
attribute	O
value	O
into	O
equal	O
fractions	O
for	O
each	O
value	O
rather	O
than	O
the	O
laplace-estimated	O
fractions	O
used	O
during	O
rule	O
generation	O
.	O
when	O
normally	O
executing	O
unordered	O
rules	O
without	O
unknowns	O
,	O
for	O
each	O
rule	O
which	O
ﬁres	O
the	O
class	O
distribution	O
(	O
i.e	O
.	O
distribution	O
of	O
training	O
examples	O
among	O
classes	O
)	O
attached	O
to	O
the	O
rule	O
is	O
collected	O
.	O
these	O
are	O
then	O
summed	O
.	O
thus	O
a	O
training	O
example	B
satisfying	O
two	O
rules	O
with	O
attached	O
class	O
distributions	O
[	O
8,2	O
]	O
and	O
[	O
0,1	O
]	O
has	O
an	O
expected	O
distribution	O
[	O
8,3	O
]	O
which	O
desired	O
.	O
the	O
built-in	O
rule	O
executer	O
follows	O
the	O
ﬁrst	O
strategy	O
(	O
the	O
example	B
is	O
classed	O
simply	O
:8	O
h	O
$	O
ff	O
	O
if	O
probabilistic	O
classiﬁcation	B
is	O
|	O
being	O
predicted	O
,	O
or	O
m	O
:	O
m	O
hf2f	O
	O
õ	O
results	O
inm	O
|	O
)	O
.	O
with	O
unordered	O
cn2	O
rules	O
,	O
an	O
attribute	O
test	O
whose	O
value	O
is	O
unknown	O
in	O
the	O
training	O
example	B
causes	O
the	O
example	B
to	O
be	O
examined	O
.	O
if	O
the	O
attribute	O
has	O
three	O
values	O
,	O
1/3	O
of	O
the	O
example	B
is	O
deemed	O
to	O
have	O
passed	O
the	O
test	O
and	O
thus	O
the	O
ﬁnal	O
class	O
distribution	O
is	O
weighted	O
by	O
1/3	O
when	O
collected	O
.	O
a	O
similar	O
rule	O
later	O
will	O
again	O
cause	O
1/3	O
of	O
the	O
example	B
to	O
pass	O
the	O
test	O
.	O
a	O
don	O
’	O
t-care	O
value	O
is	O
always	O
deemed	O
to	O
have	O
passed	O
the	O
attribute	O
test	O
in	O
full	O
(	O
i.e	O
.	O
weight	O
1	O
)	O
.	O
the	O
normalisation	O
of	O
the	O
class	O
counts	O
means	O
that	O
an	O
example	B
with	O
a	O
don	O
’	O
t-care	O
can	O
only	O
count	O
as	O
a	O
single	O
example	B
during	O
testing	O
,	O
unlike	O
newid	O
where	O
it	O
may	O
count	O
as	O
representing	O
several	O
examples	O
.	O
with	O
ordered	O
rules	O
,	O
a	O
similar	O
policy	O
is	O
followed	O
,	O
except	O
after	O
a	O
rule	O
has	O
ﬁred	O
absorbing	O
,	O
say	O
,	O
1/3	O
of	O
the	O
testing	O
example	B
,	O
only	O
the	O
remaining	O
2/3s	O
are	O
sent	O
down	O
the	O
remainder	O
of	O
class	O
frequency	O
to	O
be	O
collected	O
,	O
but	O
a	O
second	O
the	O
rule	O
list	O
.	O
the	O
ﬁrst	O
rule	O
will	O
cause	O
1/3	O
	O
similar	O
rule	O
will	O
cause	O
2/3	O
1/3	O
class	O
frequency	O
to	O
be	O
collected	O
.	O
thus	O
the	O
fraction	O
of	O
the	O
example	B
gets	O
less	O
and	O
less	O
as	O
it	O
progresses	O
down	O
the	O
rule	O
list	O
.	O
a	O
don	O
’	O
t-care	O
value	O
always	O
³	O
	O
à	O
	O
¥	O
	O
h	O
ù	O
	O
g	O
~	O
¥	O
	O
ù	O
	O
|	O
	O
	O
	O
m	O
sec	O
.	O
5.2	O
]	O
statlog	O
’	O
s	O
ml	O
algorithms	O
77	O
f	O
evaluations	O
wherek	O
requires³	O
optimal	O
split	O
withk	O
passes	O
the	O
attribute	O
test	O
in	O
full	O
,	O
and	O
thus	O
no	O
fractional	O
example	B
remains	O
to	O
propagate	O
further	O
down	O
the	O
rule	O
list	O
.	O
numeric	O
attributes	O
and	O
rules	O
for	O
numeric	O
attributes	O
,	O
cn2	O
will	O
partition	O
the	O
values	O
into	O
two	O
subsets	O
and	O
test	O
which	O
subset	O
each	O
example	B
belongs	O
to	O
.	O
the	O
drawback	O
with	O
a	O
na¨ıve	O
implementation	O
of	O
this	O
is	O
that	O
it	O
is	O
the	O
number	O
of	O
attribute	O
values	O
.	O
breiman	O
et	O
al	O
.	O
(	O
1984	O
)	O
proved	O
that	O
in	O
the	O
special	O
case	O
where	O
there	O
are	O
two	O
class	O
values	O
it	O
is	O
possible	O
to	O
ﬁnd	O
an	O
f	O
comparisons	O
.	O
in	O
the	O
general	O
case	O
heuristic	O
methods	O
must	O
be	O
used	O
.	O
the	O
aq	O
algorithm	O
produces	O
an	O
unordered	O
set	O
of	O
rules	O
,	O
whereas	O
the	O
version	O
of	O
the	O
cn2	O
algorithm	O
used	O
in	O
statlog	O
produces	O
an	O
ordered	O
list	O
of	O
rules	O
.	O
unordered	O
rules	O
are	O
on	O
the	O
whole	O
more	O
comprehensible	O
,	O
but	O
require	O
also	O
that	O
they	O
are	O
qualiﬁed	O
with	O
some	O
numeric	O
conﬁdence	O
measure	B
to	O
handle	O
any	O
clashes	O
which	O
may	O
occur	O
.	O
with	O
an	O
ordered	O
list	O
of	O
rules	O
,	O
clashes	O
can	O
not	O
occur	O
as	O
each	O
rule	O
in	O
the	O
list	O
is	O
considered	O
to	O
have	O
precedence	O
over	O
all	O
subsequent	O
rules	O
.	O
relation	O
between	O
cn2	O
and	O
aq	O
there	O
are	O
several	O
differences	O
between	O
these	O
two	O
algorithms	O
;	O
however	O
,	O
it	O
is	O
possible	O
to	O
show	O
that	O
strong	O
relationships	O
exist	O
between	O
the	O
two	O
,	O
so	O
much	O
so	O
that	O
simple	O
modiﬁcations	O
of	O
the	O
cn2	O
system	O
can	O
be	O
introduced	O
to	O
enable	O
it	O
to	O
emulate	O
the	O
behaviour	O
of	O
the	O
aq	O
algorithm	O
.	O
see	O
michalski	O
&	O
larson	O
(	O
1978	O
)	O
.	O
aq	O
searches	O
for	O
rules	O
which	O
are	O
completely	O
consistent	O
with	O
the	O
training	O
data	O
,	O
whereas	O
cn2	O
may	O
prematurely	O
halt	O
specialisation	O
of	O
a	O
rule	O
when	O
no	O
further	O
rules	O
above	O
a	O
certain	O
threshold	O
of	O
statistical	B
signiﬁcance	O
can	O
be	O
generated	O
via	O
specialisation	O
.	O
thus	O
,	O
the	O
behaviour	O
of	O
aq	O
in	O
this	O
respect	O
is	O
equivalent	O
to	O
setting	O
the	O
threshold	O
to	O
zero	O
.	O
when	O
generating	O
specialisations	O
of	O
a	O
rule	O
,	O
aq	O
considers	O
only	O
specialisations	O
which	O
exclude	O
a	O
speciﬁc	O
negative	O
example	B
from	O
the	O
coverage	O
of	O
a	O
rule	O
,	O
whereas	O
cn2	O
considers	O
all	O
specialisations	O
.	O
however	O
,	O
specialisations	O
generated	O
by	O
cn2	O
which	O
don	O
’	O
t	O
exclude	O
any	O
negative	O
examples	O
will	O
be	O
rejected	O
,	O
as	O
they	O
do	O
not	O
contribute	O
anything	O
to	O
the	O
predictive	O
accuracy	O
of	O
the	O
rule	O
.	O
thus	O
,	O
the	O
two	O
algorithms	O
search	O
the	O
same	O
space	O
in	O
different	O
ways	O
.	O
whereas	O
published	O
descriptions	O
of	O
aq	O
leave	O
open	O
the	O
choice	O
of	O
evaluation	O
function	O
to	O
use	O
during	O
search	O
,	O
the	O
published	O
norm	O
is	O
that	O
of	O
“	O
number	O
of	O
correctly	O
classiﬁed	O
examples	O
divided	O
by	O
total	O
examples	O
covered	O
”	O
.	O
the	O
original	O
cn2	O
algorithm	O
uses	O
entropy	O
as	O
its	O
evaluation	O
function	O
.	O
to	O
obtain	O
a	O
synthesis	O
of	O
the	O
two	O
systems	O
,	O
the	O
choice	O
of	O
evaluation	O
function	O
can	O
be	O
user-selected	O
during	O
start	O
of	O
the	O
system	O
.	O
aq	O
generates	O
order-independent	O
rules	O
,	O
whereas	O
cn2	O
generates	O
an	O
ordered	O
list	O
of	O
rules	O
.	O
to	O
modify	O
cn2	O
to	O
produce	O
order-independent	O
rules	O
requires	O
a	O
change	O
to	O
the	O
evaluation	O
function	O
,	O
and	O
a	O
change	O
to	O
the	O
way	O
examples	O
are	O
removed	O
from	O
the	O
training	O
set	O
between	O
iterations	O
of	O
the	O
complex-ﬁnding	O
algorithm	O
.	O
the	O
basic	O
searchalgorithm	O
remains	O
unchanged	O
.	O
5.2.8	O
itrule	O
the	O
hypotheses	O
during	O
decision	O
rule	O
construction	O
.	O
its	O
output	B
is	O
a	O
set	O
of	O
probability	O
rules	O
,	O
which	O
are	O
the	O
most	O
informative	O
selected	O
from	O
the	O
possible	O
rules	O
depending	O
on	O
the	O
training	O
data	O
.	O
goodman	O
&	O
smyth	O
’	O
s	O
(	O
1989	O
)	O
itrule	O
algorithm	O
uses	O
a	O
function	O
called	O
the§	O
-measure	O
to	O
rank	O
to	O
build	O
rules	O
.	O
it	O
keeps	O
a	O
ranked	O
list	O
of	O
the9	O
the	O
algorithm	O
iterates	O
through	O
each	O
attribute	O
(	O
including	O
the	O
class	O
attribute	O
)	O
value	O
in	O
turn	O
best	O
rules	O
determined	O
to	O
that	O
point	O
of	O
the	O
	O
	O
[	O
ch	O
.	O
5	O
78	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
th	O
rule	O
is	O
used	O
as	O
the	O
running	O
minimum	O
to	O
determine	O
whether	O
a	O
new	O
rule	O
should	O
be	O
inserted	O
into	O
the	O
rule	O
list	O
.	O
for	O
each	O
attribute	O
value	O
the	O
algorithm	O
must	O
ﬁnd	O
all	O
possible	O
conditions	O
to	O
add	O
to	O
the	O
left	O
hand	O
side	O
of	O
an	O
over-general	O
rule	O
to	O
specialise	O
it	O
.	O
or	O
it	O
may	O
decide	O
to	O
drop	O
a	O
condition	O
to	O
generalise	O
an	O
over-speciﬁc	O
rule	O
.	O
the	O
rules	O
considered	O
are	O
those	O
limited	O
by	O
the	O
algorithm	O
execution	O
(	O
9	O
is	O
the	O
size	O
of	O
the	O
beam	O
search	O
)	O
.	O
the§	O
-measure	O
of	O
the9	O
minimum	O
running§	O
-measure	O
value	O
,	O
which	O
prevents	O
the	O
algorithm	O
from	O
searching	O
a	O
large	O
three	O
points	O
should	O
be	O
noted	O
.	O
first	O
,	O
itrule	O
produces	O
rules	O
for	O
each	O
attribute	O
value	O
.	O
so	O
it	O
can	O
also	O
capture	O
the	O
dependency	O
relationships	O
between	O
attributes	O
,	O
between	O
attributes	O
and	O
classes	O
and	O
between	O
class	O
values	O
.	O
secondly	O
,	O
itrule	O
not	O
only	O
specialises	O
existing	O
rules	O
but	O
also	O
generalises	O
them	O
if	O
the	O
need	O
arises	O
.	O
specialisation	O
is	O
done	O
through	O
adding	O
conditions	O
to	O
the	O
left	O
hand	O
side	O
of	O
the	O
rule	O
and	O
generalisation	O
is	O
done	O
through	O
dropping	O
conditions	O
.	O
finally	O
,	O
itrule	O
only	O
deals	O
with	O
categorical	O
examples	O
so	O
it	O
generally	O
needs	O
to	O
convert	O
numeric	O
attributes	O
and	O
discrete	O
values	O
.	O
rule	O
space	O
.	O
such	O
estimates	O
can	O
be	O
reasonable	O
accurate	O
.	O
the	O
itrule	O
algorithms	O
uses	O
a	O
maximum	O
entropy	O
estimator	O
:	O
evaluation	O
function	O
:	O
the§	O
-measure	O
~dó½½~	O
à¾	O
andö	O
be	O
an	O
attribute	O
with	O
values	O
let¶	O
be	O
an	O
attribute	O
with	O
values	O
in	O
the	O
setº2¶	O
~e½½½~	O
¤¾	O
.	O
the§	O
-measure	O
is	O
a	O
method	O
for	O
calculating	O
the	O
information	O
content	O
inº2ö	O
	O
.	O
it	O
is	O
g	O
of	O
attribute¶	O
given	O
the	O
value	O
of	O
attributeö	O
ýoc¶	O
¥¦c¶	O
¥¦c¶	O
à½kbc	O
c¶	O
á|	O
¥c¶	O
g	O
is	O
the	O
a	O
priori	O
g	O
is	O
the	O
conditional	O
probability	O
of¶	O
¶	O
givenö	O
where¥c¶.	O
¶	O
.	O
these	O
can	O
normally	O
be	O
estimated	O
from	O
the	O
(	O
conditional	O
)	O
relative	O
probability	O
of¶	O
frequency	O
of	O
the	O
value	O
of¶	O
	O
.	O
when	O
the	O
distribution	O
is	O
uniform	B
and	O
the	O
data	O
set	O
is	O
sufﬁcient	O
áexk	O
;	O
e	O
áe	O
ex³	O
andõ	O
where	O
á	O
are	O
parameters	O
of	O
an	O
initial	O
density	O
estimate	O
,	O
k	O
in	O
the	O
data	O
and	O
(	O
conditional	O
)	O
event¶	O
ó0	O
:	O
<	O
;	O
2c¶	O
c¶	O
c¶	O
¥cö	O
dá|	O
g	O
can	O
be	O
interpreted	O
as	O
a	O
measure	B
of	O
the	O
simplicity	O
of	O
the	O
ofö	O
are	O
zero	O
.	O
the	O
ﬁrst	O
term¥cö	O
hypothesis	O
that¶	O
	O
”	O
.	O
cross-entropy	O
is	O
is	O
dependent	O
on	O
the	O
eventö	O
of	O
the	O
variable¶	O
with	O
the	O
condition	O
“	O
¶	O
known	O
to	O
measure	B
the	O
goodness	O
of	O
ﬁt	O
between	O
two	O
distributions	O
;	O
see	O
goodman	O
&	O
smyth	O
(	O
1989	O
)	O
.	O
rule	O
searching	O
strategy	O
itrule	O
performs	O
both	O
generalisation	O
and	O
specialisation	O
.	O
it	O
starts	O
with	O
a	O
model	O
driven	O
strategy	O
much	O
like	O
cn2	O
.	O
but	O
its	O
rules	O
all	O
have	O
probability	O
attached	O
from	O
the	O
beginning	O
.	O
so	O
a	O
universal	O
rule	O
will	O
be	O
is	O
the	O
(	O
conditional	O
)	O
total	O
number	O
of¶	O
.	O
is	O
related	O
toö	O
.	O
the	O
second	O
term§	O
the	O
above	O
is	O
true	O
because	O
it	O
takes	O
into	O
account	O
the	O
fact	O
that	O
the	O
probabilities	O
of	O
other	O
values	O
the	O
average	O
information	O
content	O
is	O
therefore	O
deﬁned	O
as	O
:	O
¥cö	O
c¶	O
	O
and¥¦c¶	O
is	O
the	O
number	O
of	O
the	O
is	O
equal	O
to	O
the	O
cross-entropy	O
	O
¼	O
	O
	O
f	O
	O
¼	O
	O
	O
f	O
¼	O
ö	O
	O
ö	O
	O
	O
ö	O
§	O
¼	O
ö	O
	O
ö	O
	O
g	O
	O
ü	O
à	O
	O
¼	O
ö	O
	O
g	O
	O
¼	O
ö	O
	O
g	O
g	O
~	O
¼	O
ö	O
	O
	O
	O
ö	O
	O
¥	O
	O
f	O
õ	O
e	O
	O
	O
¶	O
	O
§	O
¼	O
ö	O
	O
ö	O
	O
g	O
	O
¿	O
à	O
	O
g	O
§	O
¼	O
ö	O
	O
ö	O
	O
g	O
	O
	O
g	O
§	O
¼	O
ö	O
	O
ö	O
	O
g	O
	O
	O
¼	O
ö	O
	O
ö	O
	O
g	O
	O
ö	O
sec	O
.	O
5.3	O
]	O
beyond	O
the	O
complexity	O
barrier	O
79	O
into	O
the	O
rule	O
list	O
.	O
this	O
process	O
continues	O
until	O
no	O
rule	O
can	O
be	O
produced	O
to	O
cover	O
remaining	O
examples	O
.	O
to	O
it	O
calculates	O
the	O
gçô	O
m¨·	O
it	O
requires	O
.	O
namely	O
the	O
increase	O
in	O
simplicity	O
is	O
sufﬁciently	O
compensated	O
for	O
ëìiêdké	O
then	O
{	O
à¶¹édé	O
with	O
probabilityf	O
ó0	O
:	O
<	O
;	O
ifûàà	O
êdk	O
ó=	O
:	O
<	O
;	O
c¶	O
ó0	O
:	O
<	O
;	O
2c¶	O
to	O
specialise	O
a	O
rule	O
such	O
as	O
one	O
with	O
current§	O
-value§	O
c¶	O
ó=	O
:	O
<	O
;	O
c¶	O
all	O
possible	O
values	O
of§	O
g	O
for	O
attributem	O
.	O
if§	O
then	O
it	O
insert	O
the	O
new	O
rule	O
with	O
specialised	O
conditionm	O
ó0:7	O
;	O
c¶	O
generalise	O
a	O
rule	O
,	O
with	O
the	O
current	O
-measure	O
value§	O
ó0	O
:	O
<	O
;	O
ó0	O
:	O
<	O
;	O
í3c5d	O
?	O
ba	O
î	O
ê	O
>	O
@	O
?	O
gjô	O
?	O
ba=d	O
ê	O
>	O
@	O
?	O
c¶	O
whereá	O
ã.ãyk'ì	O
my·	O
~	O
by	O
the	O
decrease	O
in	O
cross-entropy	O
.	O
c¶	O
m¨·	O
m¨·	O
5.3	O
beyond	O
the	O
complexity	O
barrier	O
all	O
ml	O
designers	O
,	O
whether	O
rule-oriented	O
or	O
tree-oriented	O
agree	O
that	O
,	O
to	O
the	O
extent	O
that	O
the	O
data	O
permits	O
,	O
mental	B
ﬁt	I
is	O
an	O
indispensible	O
hall-mark	O
.	O
thus	O
,	O
discussing	O
requirements	O
of	O
rule	O
learning	O
systems	O
clark	O
&	O
niblett	O
(	O
1989	O
)	O
state	O
that	O
“	O
for	O
the	O
sake	O
of	O
comprehensibility	O
,	O
the	O
induced	O
rules	O
should	O
be	O
as	O
short	O
as	O
possible	O
.	O
however	O
,	O
when	O
noise	O
is	O
present	O
,	O
overﬁtting	O
can	O
lead	O
to	O
long	O
rules	O
.	O
thus	O
,	O
to	O
induce	O
short	O
rules	O
,	O
one	O
must	O
usually	O
relax	O
the	O
requirement	O
that	O
the	O
induced	O
rules	O
be	O
consistent	O
with	O
all	O
the	O
training	O
data.	O
”	O
such	O
measures	O
constitute	O
the	O
analogue	O
of	O
“	O
pruning	B
”	O
of	O
trees	O
.	O
but	O
tree	O
pruning	B
and	O
rule-set	O
simpliﬁcation	O
measures	O
may	O
encounter	O
complexity	O
barriers	O
that	O
limit	O
how	O
much	O
can	O
be	O
done	O
in	O
the	O
direction	O
of	O
mental	B
ﬁt	I
while	O
retaining	O
acceptable	O
accuracy	O
.	O
when	O
this	O
occurs	O
,	O
are	O
there	O
other	O
directions	O
in	O
which	O
descriptive	O
adequacy	O
can	O
still	O
be	O
sought	O
?	O
5.3.1	O
trees	O
into	O
rules	O
a	O
tree	O
that	O
after	O
pruning	B
still	O
remains	O
too	O
big	O
to	O
be	O
comprehensible	O
is	O
a	O
sign	O
that	O
a	O
more	O
powerful	O
description	O
language	O
is	O
required	O
.	O
a	O
modest	O
,	O
but	O
often	O
effective	O
,	O
step	O
starts	O
by	O
recognising	O
that	O
there	O
is	O
no	O
intrinsic	O
difference	O
in	O
expressive	O
power	O
between	O
rulesets	O
and	O
trees	O
,	O
yet	O
rule	O
languages	O
seem	O
to	O
lend	O
themselves	O
more	O
to	O
user-friendliness	O
.	O
a	O
successful	O
exploitation	O
of	O
this	O
idea	O
takes	O
the	O
form	O
of	O
a	O
compressive	O
re-organisation	O
of	O
induced	O
trees	O
into	O
rule-sets	O
.	O
quinlan	O
’	O
s	O
trees-into-rules	O
algorithm	O
(	O
see	O
his	O
1993	O
book	O
for	O
the	O
most	O
recent	O
version	O
)	O
starts	O
with	O
the	O
set	O
formed	O
from	O
a	O
c4.5	O
decision	O
tree	O
by	O
identifying	O
each	O
root-to-leaf	O
path	O
with	O
a	O
rule	O
.	O
each	O
rule	O
is	O
simpliﬁed	O
by	O
successively	O
dropping	O
conditions	O
(	O
attribute-tests	O
)	O
in	O
the	O
speciﬁc-to-general	O
style	O
illustrated	O
at	O
the	O
beginning	O
of	O
this	O
chapter	O
.	O
the	O
difference	O
lies	O
in	O
the	O
sophistication	O
of	O
criteria	O
used	O
for	O
retracting	O
a	O
trial	O
generalisation	O
when	O
it	O
is	O
found	O
to	O
result	O
in	O
inclusion	O
of	O
cases	O
not	O
belonging	O
to	O
the	O
rule	O
’	O
s	O
decision	O
class	O
.	O
in	O
the	O
noise-free	O
taxonomy	O
problem	O
of	O
the	O
earlier	O
tutorial	O
example	B
,	O
a	O
single	O
“	O
false	O
positive	O
”	O
was	O
taken	O
to	O
bar	O
dropping	O
the	O
given	O
condition	O
.	O
as	O
with	O
cn2	O
and	O
some	O
other	O
rule-learners	O
,	O
a	O
statistical	B
criterion	O
is	O
substituted	O
.	O
quinlan	O
’	O
s	O
is	O
based	O
on	O
forming	O
from	O
the	O
training	O
set	O
“	O
pessimistic	O
”	O
estimates	O
of	O
the	O
accuracy	O
that	O
a	O
candidate	O
rule	O
would	O
show	O
on	O
a	O
test	O
set	O
.	O
when	O
speciﬁc-to-general	O
simpliﬁcation	O
has	O
run	O
its	O
course	O
for	O
each	O
class	O
in	O
turn	O
,	O
a	O
ﬁnal	O
scan	O
is	O
made	O
over	O
each	O
ruleset	O
for	O
any	O
that	O
,	O
in	O
the	O
context	O
of	O
the	O
other	O
rules	O
,	O
are	O
not	O
contributing	O
to	O
the	O
ruleset	O
’	O
s	O
accuracy	O
.	O
any	O
such	O
passengers	O
are	O
dropped	O
.	O
the	O
end	O
of	O
this	O
stage	O
leaves	O
as	O
many	O
subsets	O
of	O
if-then	O
rules	O
(	O
“	O
covers	O
”	O
in	O
the	O
earlier	O
terminology	O
)	O
as	O
there	O
are	O
classes	O
,	O
i.e	O
.	O
one	O
subset	O
for	O
each	O
class	O
.	O
these	O
subsets	O
are	O
then	O
ordered	O
prior	O
to	O
use	O
for	O
classifying	O
new	O
cases	O
.	O
the	O
ordering	O
principle	O
ﬁrst	O
applies	O
the	O
subset	O
which	O
on	O
the	O
{	O
	O
ú	O
{	O
	O
¼	O
ö	O
	O
ö	O
	O
g	O
¼	O
ö	O
	O
ö	O
	O
~	O
m	O
	O
¼	O
ö	O
	O
ö	O
	O
~	O
m	O
	O
§	O
¼	O
ö	O
	O
ö	O
	O
g	O
	O
¼	O
ö	O
	O
ö	O
	O
~	O
m	O
	O
g	O
§	O
¼	O
ö	O
	O
ö	O
	O
á	O
§	O
¼	O
ö	O
	O
ö	O
m	O
	O
m	O
·	O
g	O
~	O
	O
á	O
í	O
á	O
á	O
80	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
[	O
ch	O
.	O
5	O
training	O
set	O
gives	O
fewest	O
false	O
positives	O
.	O
the	O
one	O
with	O
most	O
false	O
positives	O
is	O
the	O
last	O
to	O
be	O
applied	O
.	O
by	O
that	O
time	O
some	O
of	O
the	O
false-positive	O
errors	O
that	O
it	O
could	O
have	O
made	O
have	O
been	O
pre-empted	O
by	O
other	O
rule-sets	O
.	O
finally	O
a	O
default	O
class	O
is	O
chosen	O
to	O
which	O
all	O
cases	O
which	O
do	O
not	O
match	O
any	O
rule	O
are	O
to	O
be	O
assigned	O
.	O
this	O
is	O
calculated	O
from	O
the	O
frequency	O
statistics	O
of	O
such	O
left-overs	O
in	O
the	O
training	O
set	O
.	O
whichever	O
class	O
appears	O
most	O
frequently	O
among	O
these	O
left-overs	O
is	O
selected	O
as	O
the	O
default	O
classiﬁcation	B
.	O
rule-structured	O
classiﬁers	O
generated	O
in	O
this	O
way	O
turn	O
out	O
to	O
be	O
smaller	O
,	O
and	O
better	O
in	O
“	O
mental	B
ﬁt	I
”	O
,	O
than	O
the	O
trees	O
from	O
which	O
the	O
process	O
starts	O
.	O
yet	O
accuracy	O
is	O
found	O
to	O
be	O
fully	O
preserved	O
when	O
assayed	O
against	O
test	O
data	O
.	O
a	O
particularly	O
interesting	O
feature	O
of	O
quinlan	O
’	O
s	O
(	O
1993	O
)	O
account	O
,	O
for	O
which	O
space	O
allows	O
no	O
discussion	O
here	O
,	O
is	O
his	O
detailed	O
illustration	O
of	O
the	O
minimum	O
description	O
length	O
(	O
mdl	O
)	O
principle	O
,	O
according	O
to	O
which	O
the	O
storage	O
costs	O
of	O
rulesets	O
and	O
of	O
their	O
exceptions	O
are	O
expressed	O
in	O
a	O
common	O
information-theoretic	O
coinage	O
.	O
this	O
is	O
used	O
to	O
address	O
a	O
simpliﬁcation	O
problem	O
in	O
building	O
rule-sets	O
that	O
is	O
essentially	O
similar	O
to	O
the	O
regulation	O
of	O
the	O
pruning	B
process	O
in	O
decision	O
trees	O
.	O
the	O
trade-off	O
in	O
each	O
case	O
is	O
between	O
complexity	O
and	O
predictive	O
accuracy	O
.	O
5.3.2	O
manufacturing	O
new	O
attributes	O
if	O
a	O
user-friendly	O
description	O
still	O
can	O
not	O
be	O
extracted	O
more	O
radical	O
treatment	O
may	O
be	O
required	O
.	O
the	O
data-description	O
language	O
’	O
s	O
vocabulary	O
may	O
need	O
extending	O
with	O
new	O
com-	O
binations	O
formed	O
from	O
the	O
original	O
primitive	O
attributes	O
.	O
the	O
effects	O
can	O
be	O
striking	O
.	O
consider	O
the	O
problem	O
of	O
classifying	O
as	O
“	O
illegal	O
”	O
or	O
“	O
legal	O
”	O
the	O
chessboard	O
positions	O
formed	O
by	O
randomly	O
placing	O
the	O
three	O
pieces	O
white	O
king	O
,	O
white	O
rook	O
and	O
black	O
king	O
.	O
combinatorially	O
there	O
areô	O
move	O
.	O
approximately	O
two	O
thirds	O
of	O
the	O
positions	O
are	O
then	O
illegal	O
.	O
two	O
or	O
more	O
pieces	O
may	O
have	O
been	O
placed	O
on	O
the	O
same	O
square	O
,	O
or	O
the	O
two	O
kings	O
may	O
be	O
diagonally	O
or	O
directly	O
adjacent	O
.	O
additionally	O
positions	O
in	O
which	O
the	O
black	O
king	O
is	O
in	O
check	O
from	O
the	O
white	O
rook	O
are	O
also	O
illegal	O
(	O
recall	O
that	O
it	O
is	O
white	O
to	O
move	O
)	O
.	O
	O
positions	O
,	O
or	O
262,144.	O
assume	O
that	O
it	O
is	O
white	O
’	O
s	O
turn	O
to	O
a	O
problem	O
is	O
presented	O
for	O
inductive	O
analysis	O
as	O
a	O
training	O
set	O
of	O
n	O
cases	O
sampled	O
randomly	O
from	O
the	O
total	O
space	O
of	O
possible	O
3-piece	O
placements	O
,	O
as	O
shown	O
in	O
table	O
5.2.	O
given	O
sufﬁciently	O
large	O
n	O
,	O
table	O
5.2	O
constitutes	O
what	O
mccarthy	O
and	O
hayes	O
(	O
1969	O
)	O
termed	O
an	O
“	O
epistemologically	O
adequate	O
”	O
representation	O
:	O
it	O
supplies	O
whatever	O
facts	O
are	O
in	O
principle	O
needed	O
to	O
obtain	O
solutions	O
.	O
but	O
for	O
decision-tree	O
learning	O
,	O
the	O
representation	O
is	O
not	O
“	O
heuristically	O
adequate	O
”	O
.	O
michie	O
&	O
bain	O
(	O
1992	O
)	O
applied	O
a	O
state-of-the-art	O
decision-tree	O
learner	O
(	O
xpertrule	O
)	O
of	O
roughly	O
similar	O
power	O
to	O
c4.5	O
,	O
to	O
training	O
sets	O
of	O
700	O
examples	O
.	O
the	O
resulting	O
27-node	O
tree	O
performed	O
on	O
test	O
data	O
with	O
only	O
69	O
%	O
accuracy	O
,	O
not	O
differing	O
signiﬁcantly	O
from	O
that	O
achievable	O
by	O
making	O
the	O
default	O
conjecture	O
“	O
illegal	O
”	O
for	O
every	O
case	O
.	O
the	O
next	O
step	O
was	O
to	O
augment	O
the	O
six	O
attributes	O
with	O
ﬁfteen	O
new	O
ones	O
,	O
manufactured	O
by	O
forming	O
all	O
possible	O
pairwise	O
differences	O
among	O
the	O
original	O
six	O
.	O
with	O
the	O
augmented	O
attribute	O
set	O
,	O
two	O
random	O
partitions	O
of	O
a	O
ﬁle	O
of	O
999	O
cases	O
were	O
made	O
into	O
a	O
training	O
set	O
of	O
698	O
and	O
a	O
test	O
set	O
of	O
301.	O
trees	O
of	O
99	O
%	O
and	O
97	O
%	O
accuracy	O
now	O
resulted	O
,	O
with	O
49	O
nodes	O
and	O
41	O
nodes	O
respectively	O
.	O
for	O
making	O
these	O
very	O
successful	O
constructions	O
the	O
algorithm	O
seized	O
on	O
just	O
six	O
at-	O
tributes	O
,	O
all	O
newly	O
manufactured	O
,	O
namely	O
the	O
three	O
pairwise	O
differences	O
among	O
attributes	O
1	O
,	O
3	O
,	O
and	O
5	O
,	O
and	O
the	O
three	O
among	O
attributes	O
2	O
,	O
4	O
,	O
and	O
6.	O
in	O
this	O
way	O
,	O
even	O
though	O
in	O
a	O
verbose	O
and	O
contorted	O
style	O
,	O
it	O
was	O
able	O
to	O
express	O
in	O
decision-tree	O
language	O
certain	O
key	O
	O
sec	O
.	O
5.3	O
]	O
beyond	O
the	O
complexity	O
barrier	O
81	O
table	O
5.2	O
:	O
the	O
six	O
attributes	O
encode	O
a	O
position	O
according	O
to	O
the	O
scheme	O
:	O
a1	O
=	O
ﬁle	O
(	O
bk	O
)	O
;	O
a2	O
=	O
rank	O
(	O
bk	O
)	O
;	O
a3	O
=	O
ﬁle	O
(	O
wr	O
)	O
;	O
a4	O
=	O
rank	O
(	O
wr	O
)	O
;	O
a5	O
=	O
ﬁle	O
(	O
bk	O
)	O
;	O
a6	O
=	O
rank	O
(	O
bk	O
)	O
.	O
id	O
no	O
.	O
1	O
2	O
3	O
4	O
..	O
..	O
n-1	O
n	O
a1	O
7	O
6	O
2	O
2	O
..	O
..	O
2	O
7	O
a2	O
8	O
5	O
3	O
2	O
..	O
..	O
7	O
1	O
a3	O
1	O
8	O
3	O
5	O
..	O
..	O
5	O
5	O
a4	O
7	O
4	O
5	O
7	O
..	O
..	O
3	O
4	O
a5	O
6	O
6	O
8	O
5	O
..	O
..	O
2	O
3	O
a6	O
8	O
8	O
7	O
1	O
..	O
..	O
3	O
6	O
class	O
yes	O
no	O
no	O
yes	O
.	O
.	O
.	O
.	O
.	O
.	O
yes	O
no	O
sub-descriptions	O
,	O
–	O
such	O
as	O
the	O
crucial	O
same-ﬁle	O
and	O
same-rank	O
relation	O
between	O
white	O
rook	O
and	O
black	O
king	O
.	O
whenever	O
one	O
of	O
these	O
relations	O
holds	O
it	O
is	O
a	O
good	O
bet	O
that	O
the	O
position	O
is	O
illegal	O
.	O
the	O
gain	O
in	O
classiﬁcation	B
accuracy	O
is	O
impressive	O
,	O
yet	O
no	O
amount	O
of	O
added	O
training	O
data	O
could	O
inductively	O
reﬁne	O
the	O
above	O
“	O
excellent	O
bet	O
”	O
into	O
a	O
certainty	O
.	O
the	O
reason	O
again	O
lies	O
with	O
persisting	O
limitations	O
of	O
the	O
description	O
language	O
.	O
to	O
deﬁne	O
the	O
cases	O
where	O
the	O
classiﬁer	B
’	O
s	O
use	O
of	O
sameﬁle	O
(	O
wr	O
,	O
bk	O
)	O
and	O
samerank	O
(	O
wr	O
,	O
bk	O
)	O
lets	O
it	O
down	O
,	O
one	O
needs	O
to	O
say	O
that	O
this	O
happens	O
if	O
and	O
only	O
if	O
the	O
wk	O
is	O
between	O
the	O
wr	O
and	O
bk	O
.	O
decision-tree	O
learning	O
,	O
with	O
attribute-set	O
augmented	O
as	O
described	O
,	O
can	O
patch	O
together	O
subtrees	O
to	O
do	O
duty	O
for	O
sameﬁle	O
and	O
samerank	O
.	O
but	O
an	O
equivalent	O
feat	O
for	O
a	O
sophisticated	O
three-place	O
relation	O
such	O
as	O
“	O
between	O
”	O
is	O
beyond	O
the	O
expressive	O
powers	O
of	O
an	O
attribute-value	O
propositional-	O
level	O
language	O
.	O
moreover	O
,	O
the	O
decision-tree	O
learner	O
’	O
s	O
constructions	O
were	O
described	O
above	O
as	O
“	O
very	O
successful	O
”	O
on	O
purely	O
operational	O
grounds	O
of	O
accuracy	O
relative	O
to	O
the	O
restricted	O
amount	O
of	O
training	O
material	O
,	O
i.e	O
.	O
successful	O
in	O
predictivity	O
.	O
in	O
terms	O
of	O
“	O
descriptivity	O
”	O
the	O
trees	O
,	O
while	O
not	O
as	O
opaque	O
as	O
those	O
obtained	O
with	O
primitive	O
attributes	O
only	O
,	O
were	O
still	O
far	O
from	O
constituting	O
intelligible	O
theories	O
.	O
inherent	O
limits	O
of	O
propositional-level	O
learning	O
5.3.3	O
construction	O
of	O
theories	O
of	O
high	O
descriptivity	O
is	O
the	O
shared	O
goal	O
of	O
human	O
analysts	O
and	O
of	O
ml	O
.	O
yet	O
the	O
propositional	O
level	O
of	O
ml	O
is	O
too	O
weak	O
to	O
fully	O
solve	O
even	O
the	O
problem	O
here	O
illustrated	O
.	O
the	O
same	O
task	O
,	O
however	O
,	O
was	O
proved	O
to	O
be	O
well	O
within	O
the	O
powers	O
(	O
1	O
)	O
of	O
dr.	O
jane	O
mitchell	O
,	O
a	O
gifted	O
and	O
experienced	O
human	O
data	O
analyst	O
on	O
the	O
academic	O
staff	O
of	O
strathclyde	O
university	O
,	O
and	O
(	O
2	O
)	O
of	O
a	O
predicate-logic	O
ml	O
system	O
belonging	O
to	O
the	O
induc-	O
tive	O
logic	O
programming	O
(	O
ilp	O
)	O
family	O
described	O
in	O
chapter	O
12.	O
the	O
two	O
independently	O
obtained	O
theories	O
were	O
complete	O
and	O
correct	O
.	O
one	O
theory-discovery	O
agent	O
was	O
human	O
,	O
namely	O
a	O
member	O
of	O
the	O
academic	O
staff	O
of	O
a	O
university	O
statistics	O
department	O
.	O
the	O
other	O
was	O
an	O
ilp	O
learner	O
based	O
on	O
muggleton	O
&	O
feng	O
’	O
s	O
(	O
1990	O
)	O
golem	O
,	O
with	O
“	O
closed	O
world	O
specialization	O
”	O
enhancements	O
(	O
bain	O
,	O
private	O
communication	O
)	O
.	O
in	O
essentials	O
the	O
two	O
theo-	O
ries	O
closely	O
approximated	O
to	O
the	O
one	O
shown	O
below	O
in	O
the	O
form	O
of	O
four	O
if-then	O
rules	O
.	O
these	O
are	O
here	O
given	O
in	O
english	O
after	O
back-interpretation	O
into	O
chess	O
terms	O
.	O
neither	O
of	O
the	O
learning	O
agents	O
had	O
any	O
knowledge	O
of	O
the	O
meaning	O
of	O
the	O
task	O
,	O
which	O
was	O
simply	O
presented	O
as	O
in	O
table	O
5.2.	O
they	O
did	O
not	O
know	O
that	O
it	O
had	O
anything	O
to	O
do	O
with	O
chess	O
,	O
nor	O
even	O
with	O
objects	O
placed	O
on	O
plane	O
surfaces	O
.	O
the	O
background	O
knowledge	O
given	O
to	O
the	O
ilp	O
learner	O
was	O
similar	O
82	O
machine	O
learning	O
of	O
rules	O
and	O
trees	O
[	O
ch	O
.	O
5	O
in	O
amount	O
to	O
that	O
earlier	O
given	O
to	O
xpertrule	O
in	O
the	O
form	O
of	O
manufactured	O
attributes	O
.	O
1	O
.	O
2	O
.	O
3	O
.	O
4.	O
if	O
wr	O
and	O
bk	O
either	O
occupy	O
same	O
ﬁle	O
and	O
wk	O
is	O
not	O
directly	O
between	O
or	O
if	O
they	O
occupy	O
the	O
same	O
rank	O
and	O
wk	O
is	O
not	O
directly	O
between	O
then	O
the	O
position	O
is	O
illegal	O
;	O
if	O
wk	O
and	O
bk	O
either	O
are	O
vertically	O
adjacent	O
or	O
are	O
horizontally	O
adjacent	O
or	O
are	O
diagonally	O
adjacent	O
then	O
the	O
position	O
is	O
illegal	O
;	O
if	O
any	O
two	O
pieces	O
are	O
on	O
the	O
same	O
square	O
then	O
the	O
position	O
is	O
illegal	O
;	O
otherwise	O
the	O
position	O
is	O
legal	O
.	O
construction	O
of	O
this	O
theory	O
requires	O
certain	O
key	O
sub-concepts	O
,	O
notably	O
of	O
“	O
directly	O
between	O
”	O
.	O
deﬁnitions	O
were	O
invented	O
by	O
the	O
machine	O
learner	O
,	O
using	O
lower-level	O
concepts	O
such	O
as	O
“	O
less-	O
than	O
”	O
,	O
as	O
background	O
knowledge	O
.	O
“	O
directly	O
between	O
”	O
holds	O
among	O
the	O
three	O
co-ordinate	O
pairs	O
if	O
either	O
the	O
ﬁrst	O
co-ordinates	O
are	O
all	O
equal	O
and	O
the	O
second	O
co-ordinates	O
are	O
in	O
ascending	O
or	O
descending	O
progression	O
,	O
or	O
the	O
second	O
co-ordinates	O
are	O
all	O
equal	O
and	O
the	O
ﬁrst	O
co-ordinates	O
show	O
the	O
progression	O
.	O
bain	O
’	O
s	O
ilp	O
package	O
approached	O
the	O
relation	O
piece-wise	O
,	O
via	O
invention	O
of	O
“	O
between-ﬁle	O
”	O
and	O
“	O
between-rank	O
”	O
.	O
the	O
human	O
learner	O
doubtless	O
came	O
ready-equipped	O
with	O
some	O
at	O
least	O
of	O
the	O
concepts	O
that	O
the	O
ml	O
system	O
had	O
to	O
invent	O
.	O
none	O
the	O
less	O
,	O
with	O
unlimited	O
access	O
to	O
training	O
data	O
and	O
the	O
use	O
of	O
standard	O
statistical	B
analysis	O
and	O
tabulation	O
software	O
,	O
the	O
task	O
of	O
theory	O
building	O
still	O
cost	O
two	O
days	O
of	O
systematic	O
work	O
.	O
human	O
learners	O
given	O
hours	O
rather	O
than	O
days	O
constructed	O
only	O
partial	O
theories	O
,	O
falling	O
far	O
short	O
even	O
of	O
operational	O
adequacy	O
(	O
see	O
also	O
muggleton	O
,	O
s.h.	O
,	O
bain	O
,	O
m.	O
,	O
hayes-michie	O
,	O
j.e	O
.	O
and	O
michie	O
,	O
d.	O
(	O
1989	O
)	O
)	O
.	O
bain	O
’	O
s	O
new	O
work	O
has	O
the	O
further	O
interest	O
that	O
learning	O
takes	O
place	O
incrementally	O
,	O
by	O
successive	O
reﬁnement	O
,	O
a	O
style	O
sometimes	O
referred	O
to	O
as	O
“	O
non-monotonic	O
”	O
.	O
generalisations	O
made	O
in	O
the	O
ﬁrst	O
pass	O
through	O
training	O
data	O
yield	O
exceptions	O
when	O
challenged	O
with	O
new	O
data	O
.	O
as	O
exceptions	O
accumulate	O
they	O
are	O
themselves	O
generalised	O
over	O
,	O
to	O
yield	O
sub-theories	O
which	O
qualify	O
the	O
main	O
theory	O
.	O
these	O
reﬁnements	O
are	O
in	O
turn	O
challenged	O
,	O
and	O
so	O
forth	O
to	O
any	O
desired	O
level	O
.	O
the	O
krk	O
illegality	O
problem	O
was	O
originally	O
included	O
in	O
statlog	O
’	O
s	O
datasets	O
.	O
in	O
the	O
interests	O
of	O
industrial	O
relevance	O
,	O
artiﬁcial	O
problems	O
were	O
not	O
retained	O
except	O
for	O
expository	O
purposes	O
.	O
no	O
connection	O
,	O
however	O
,	O
exists	O
between	O
a	O
data-set	O
’	O
s	O
industrial	O
importance	O
and	O
its	O
intrinsic	O
difﬁculty	O
.	O
all	O
of	O
the	O
ml	O
algorithms	O
tested	O
by	O
statlog	O
were	O
of	O
propositional	O
type	O
.	O
if	O
descriptive	O
adequacy	O
is	O
a	O
desideratum	O
,	O
none	O
can	O
begin	O
to	O
solve	O
the	O
krk-illegal	O
problem	O
.	O
it	O
would	O
be	O
a	O
mistake	O
,	O
however	O
,	O
to	O
assume	O
that	O
problems	O
of	O
complex	O
logical	O
structure	O
do	O
not	O
occur	O
in	O
industry	O
.	O
they	O
can	O
be	O
found	O
,	O
for	O
example	B
,	O
in	O
trouble-shooting	O
complex	O
circuitry	O
(	O
pearce	O
,	O
1989	O
)	O
,	O
in	O
inferring	O
biological	O
activity	O
from	O
speciﬁcations	O
of	O
macromolecular	O
structure	O
in	O
the	O
pharmaceutical	O
industry	O
(	O
see	O
last	O
section	O
of	O
chapter	O
12	O
)	O
and	O
in	O
many	O
other	O
large	O
combinatorial	O
domains	O
.	O
as	O
inductive	O
logic	O
programming	O
matures	O
and	O
assimilates	O
techniques	O
from	O
probability	O
and	O
statistics	O
,	O
industrial	O
need	O
seems	O
set	O
to	O
explore	O
these	O
more	O
powerful	O
ml	O
description	O
languages	O
.	O
sec	O
.	O
5.3	O
]	O
beyond	O
the	O
complexity	O
barrier	O
83	O
5.3.4	O
a	O
human-machine	O
compromise	O
:	O
structured	O
induction	O
in	O
industrial	O
practice	O
more	O
mileage	O
can	O
be	O
got	O
from	O
decision-tree	O
and	O
rule	O
learning	O
than	O
the	O
foregoing	O
account	O
might	O
lead	O
one	O
to	O
expect	O
.	O
comparative	O
trials	O
like	O
statlog	O
’	O
s	O
,	O
having	O
a	O
scientiﬁc	O
end	O
in	O
view	O
,	O
necessarily	O
exclude	O
approaches	O
in	O
which	O
the	O
algorithm	O
’	O
s	O
user	O
intervenes	O
interactively	O
to	O
help	O
it	O
.	O
the	O
inability	O
of	O
propositional	O
learning	O
to	O
invent	O
new	O
attributes	O
can	O
be	O
by-passed	O
in	O
practical	O
contexts	O
where	O
human-computer	O
interaction	O
can	O
plug	O
the	O
gap	O
.	O
from	O
this	O
,	O
an	O
approach	O
known	O
as	O
“	O
structured	O
induction	O
”	O
has	O
come	O
to	O
dominate	O
commercial	O
ml	O
.	O
the	O
method	O
,	O
originated	O
by	O
shapiro	O
&	O
niblett	O
(	O
1982	O
)	O
(	O
see	O
also	O
shapiro	O
,	O
1987	O
)	O
assigns	O
the	O
task	O
of	O
attribute-invention	O
to	O
the	O
user	O
,	O
in	O
a	O
manner	O
that	O
partitions	O
the	O
problem	O
into	O
a	O
hierarchy	O
of	O
smaller	O
problems	O
.	O
for	O
each	O
smaller	O
problem	O
a	O
solution	O
tree	O
is	O
separately	O
induced	O
.	O
structured	O
induction	O
is	O
closely	O
related	O
to	O
the	O
software	O
discipline	O
of	O
“	O
structured	O
pro-	O
gramming	O
”	O
.	O
for	O
large	O
problems	O
the	O
industrial	O
stream	O
of	O
ml	O
work	O
will	O
continue	O
to	O
ﬂow	O
along	O
this	O
human-computer	O
channel	O
.	O
it	O
may	O
for	O
some	O
time	O
remain	O
exceptional	O
for	O
prob-	O
lem	O
complexity	O
to	O
force	O
users	O
to	O
look	O
beyond	O
rule-based	O
ml	O
and	O
multivariate	O
statistics	O
.	O
because	O
the	O
statlog	O
ground-rules	O
of	O
comparative	O
trials	O
necessarily	O
barred	O
the	O
user	O
from	O
substantive	O
importation	O
of	O
domain-speciﬁc	O
knowledge	O
,	O
structured	O
induction	O
does	O
not	O
ﬁgure	O
in	O
this	O
book	O
.	O
but	O
industrially	O
oriented	O
readers	O
may	O
ﬁnd	O
advantage	O
in	O
studying	O
cases	O
of	O
the	O
method	O
’	O
s	O
successful	O
ﬁeld	O
use	O
.	O
one	O
such	O
account	O
by	O
leech	O
(	O
1986	O
)	O
concerned	O
process	O
and	O
quality	O
control	O
in	O
uranium	O
reﬁning	O
.	O
a	O
well-conceived	O
application	O
of	O
structured	O
decision-	O
tree	O
induction	O
transformed	O
the	O
plant	O
from	O
unsatisfactory	O
to	O
highly	O
satisfactory	O
operation	O
,	O
and	O
is	O
described	O
in	O
sufﬁcient	O
detail	O
to	O
be	O
used	O
as	O
a	O
working	O
paradigm	O
.	O
similar	O
experience	O
has	O
been	O
reported	O
from	O
other	O
industries	O
(	O
see	O
michie	O
,	O
1991	O
,	O
for	O
review	O
)	O
.	O
6	O
neural	O
networks	O
r.	O
rohwer	O
(	O
1	O
)	O
,	O
m.	O
wynne-jones	O
(	O
1	O
)	O
and	O
f.	O
wysotzki	O
(	O
2	O
)	O
(	O
1	O
)	O
aston	O
university|	O
and	O
(	O
2	O
)	O
fraunhofer-institute	O
6.1	O
introduction	O
the	O
ﬁeld	O
of	O
neural	O
networks	O
has	O
arisen	O
from	O
diverse	O
sources	O
,	O
ranging	O
from	O
the	O
fascination	O
of	O
mankind	O
with	O
understanding	O
and	O
emulating	O
the	O
human	O
brain	O
,	O
to	O
broader	O
issues	O
of	O
copying	O
human	O
abilities	O
such	O
as	O
speech	O
and	O
the	O
use	O
of	O
language	O
,	O
to	O
the	O
practical	O
commercial	O
,	O
scientiﬁc	O
,	O
and	O
engineering	O
disciplines	O
of	O
pattern	O
recognition	O
,	O
modelling	O
,	O
and	O
prediction	O
.	O
for	O
a	O
good	O
introductory	O
text	O
,	O
see	O
hertz	O
et	O
al	O
.	O
(	O
1991	O
)	O
or	O
wasserman	O
(	O
1989	O
)	O
.	O
linear	O
discriminants	O
were	O
introduced	O
by	O
fisher	O
(	O
1936	O
)	O
,	O
as	O
a	O
statistical	B
procedure	O
for	O
classiﬁcation	B
.	O
here	O
the	O
space	O
of	O
attributes	O
can	O
be	O
partitioned	O
by	O
a	O
set	O
of	O
hyperplanes	O
,	O
each	O
deﬁned	O
by	O
a	O
linear	O
combination	O
of	O
the	O
attribute	O
variables	O
.	O
a	O
similar	O
model	O
for	O
logical	O
processing	O
was	O
suggested	O
by	O
mcculloch	O
&	O
pitts	O
(	O
1943	O
)	O
as	O
a	O
possible	O
structure	O
bearing	O
similarities	O
to	O
neurons	O
in	O
the	O
human	O
brain	O
,	O
and	O
they	O
demonstrated	O
that	O
the	O
model	O
could	O
be	O
used	O
to	O
build	O
any	O
ﬁnite	O
logical	O
expression	O
.	O
the	O
mcculloch-pitts	O
neuron	O
(	O
see	O
figure	O
6.1	O
)	O
consists	O
of	O
a	O
weighted	O
sum	O
of	O
its	O
inputs	O
,	O
followed	O
by	O
a	O
non-linear	O
function	O
called	O
the	O
em	O
activation	O
function	O
,	O
originally	O
a	O
threshold	O
function	O
.	O
formally	O
,	O
(	O
6.1	O
)	O
fe	O
¬·	O
ji	O
if	O
	O
°	O
otherwise	O
lk	O
other	O
neuron	O
models	O
are	O
quite	O
widely	O
used	O
,	O
for	O
example	B
in	O
radial	O
basis	O
function	O
networks	O
,	O
which	O
are	O
discussed	O
in	O
detail	O
in	O
section	O
6.2.3.	O
networks	O
of	O
mcculloch-pitts	O
neurons	O
for	O
arbitrary	O
logical	O
expressions	O
were	O
hand-	O
crafted	O
,	O
until	O
the	O
ability	O
to	O
learn	O
by	O
reinforcement	O
of	O
behaviour	O
was	O
developed	O
in	O
hebb	O
’	O
s	O
book	O
‘	O
the	O
organisation	O
of	O
behaviour	O
’	O
(	O
hebb	O
,	O
1949	O
)	O
.	O
it	O
was	O
established	O
that	O
the	O
func-	O
tionality	O
of	O
neural	O
networks	O
was	O
determined	O
by	O
the	O
strengths	O
of	O
the	O
connections	O
between	O
neurons	O
;	O
hebb	O
’	O
s	O
learning	O
rule	O
prescribes	O
that	O
if	O
the	O
network	O
responds	O
in	O
a	O
desirable	O
way	O
to	O
a	O
given	O
input	B
,	O
then	O
the	O
weights	O
should	O
be	O
adjusted	O
to	O
increase	O
the	O
probability	O
of	O
a	O
similar	O
m	O
address	O
for	O
correspondence	O
:	O
dept	O
.	O
of	O
computer	O
science	O
and	O
applied	O
mathematics	O
,	O
aston	O
university	O
,	O
birmingham	O
b4	O
7et	O
,	O
u.k.	O
g	O
h	O
f	O
·	O
	O
«	O
	O
·	O
¯	O
°	O
sec	O
.	O
6.2	O
]	O
introduction	O
85	O
output¬	O
npo	O
input	B
vectorrweight	O
vectors	O
weighted	O
sum	O
fig	O
.	O
6.1	O
:	O
mcculloch	O
and	O
pitts	O
neuron	O
.	O
response	O
to	O
similar	O
inputs	O
in	O
the	O
future	O
.	O
conversely	O
,	O
if	O
the	O
network	O
responds	O
undesirably	O
to	O
an	O
input	B
,	O
the	O
weights	O
should	O
be	O
adjusted	O
to	O
decrease	O
the	O
probability	O
of	O
a	O
similar	O
response	O
.	O
a	O
distinction	O
is	O
often	O
made	O
,	O
in	O
pattern	O
recognition	O
,	O
between	O
supervised	O
and	O
unsupervised	O
learning	O
.	O
the	O
former	O
describes	O
the	O
case	O
where	O
the	O
the	O
training	O
data	O
,	O
measurements	O
on	O
the	O
surroundings	O
,	O
are	O
accompanied	O
by	O
labels	O
indicating	O
the	O
class	O
of	O
event	O
that	O
the	O
measurements	O
represent	O
,	O
or	O
more	O
generally	O
a	O
desired	O
response	O
to	O
the	O
measurements	O
.	O
this	O
is	O
the	O
more	O
usual	O
case	O
in	O
classiﬁcation	B
tasks	O
,	O
such	O
as	O
those	O
forming	O
the	O
empirical	O
basis	O
of	O
this	O
book	O
.	O
the	O
supervised	O
learning	O
networks	O
described	O
later	O
in	O
this	O
chapter	O
are	O
the	O
perceptron	O
and	O
multi	O
layer	O
perceptron	O
(	O
mlp	O
)	O
,	O
the	O
cascade	O
correlation	O
learning	O
architecture	O
,	O
and	O
radial	O
basis	O
function	O
networks	O
.	O
unsupervised	O
learning	O
refers	O
to	O
the	O
case	O
where	O
measurements	O
are	O
not	O
accompanied	O
by	O
class	O
labels	O
.	O
networks	O
exist	O
which	O
can	O
model	O
the	O
structure	O
of	O
samples	O
in	O
the	O
measurement	O
,	O
or	O
attribute	O
space	O
,	O
usually	O
in	O
terms	O
of	O
a	O
probability	O
density	O
function	O
,	O
or	O
by	O
representing	O
the	O
data	O
in	O
terms	O
of	O
cluster	O
centres	O
and	O
widths	O
.	O
such	O
models	O
include	O
gaussian	O
mixture	O
models	O
and	O
kohonen	O
networks	O
.	O
once	O
a	O
model	O
has	O
been	O
made	O
,	O
it	O
can	O
be	O
used	O
as	O
a	O
classiﬁer	B
in	O
one	O
of	O
two	O
ways	O
.	O
the	O
ﬁrst	O
is	O
to	O
determine	O
which	O
class	O
of	O
pattern	O
in	O
the	O
training	O
data	O
each	O
node	O
or	O
neuron	O
in	O
the	O
model	O
responds	O
most	O
strongly	O
to	O
,	O
most	O
frequently	O
.	O
unseen	O
data	O
can	O
then	O
be	O
classiﬁed	O
according	O
to	O
the	O
class	O
label	O
of	O
the	O
neuron	O
with	O
the	O
strongest	O
activation	O
for	O
each	O
pattern	O
.	O
alternatively	O
,	O
the	O
kohonen	O
network	O
or	O
mixture	O
model	O
can	O
be	O
used	O
as	O
the	O
ﬁrst	O
layer	O
of	O
a	O
radial	O
basis	O
function	O
network	O
,	O
with	O
a	O
subsequent	O
layer	O
of	O
weights	O
used	O
to	O
calculate	O
a	O
set	O
of	O
class	O
probabilities	O
.	O
the	O
weights	O
in	O
this	O
layer	O
are	O
calculated	O
by	O
a	O
linear	O
one-shot	O
learning	O
algorithm	O
(	O
see	O
section	O
6.2.3	O
)	O
,	O
giving	O
radial	O
basis	O
functions	O
a	O
speed	O
advantage	O
over	O
non-linear	O
training	O
algorithms	O
such	O
as	O
most	O
of	O
the	O
supervised	O
learning	O
methods	O
.	O
the	O
ﬁrst	O
layer	O
of	O
a	O
radial	O
basis	O
function	O
network	O
can	O
alternatively	O
be	O
initialised	O
by	O
choosing	O
a	O
subset	O
of	O
the	O
training	O
data	O
points	O
to	O
use	O
as	O
centres	O
.	O
s	O
n	O
q	O
·	O
k	O
·	O
·	O
86	O
neural	O
networks	O
[	O
ch	O
.	O
6	O
and	O
targets	O
,	O
typically	O
by	O
minimising	O
the	O
total	O
squared	O
error	O
6.2	O
supervised	O
networks	O
for	O
classification	O
in	O
supervised	O
learning	O
,	O
we	O
have	O
an	O
instance	O
of	O
data	O
,	O
	O
,	O
comprising	O
an	O
attribute	O
vectort	O
	O
.	O
we	O
processt	O
;	O
	O
with	O
a	O
network	O
,	O
to	O
produce	O
an	O
outputv'	O
,	O
which	O
has	O
and	O
a	O
target	O
vectoru	O
the	O
same	O
form	O
as	O
the	O
target	O
vectoru	O
;	O
	O
.	O
the	O
parameters	O
of	O
the	O
networkw	O
cbv'ðlu	O
;	O
	O
it	O
might	O
seem	O
more	O
natural	O
to	O
use	O
a	O
percentage	O
misclassiﬁcation	O
error	O
measure	B
in	O
classi-	O
ﬁcation	O
problems	O
,	O
but	O
the	O
total	O
squared	O
error	O
has	O
helpful	O
smoothness	O
and	O
differentiability	O
properties	O
.	O
although	O
the	O
total	O
squared	O
error	O
was	O
used	O
for	O
training	O
in	O
the	O
statlog	O
trials	O
,	O
percentage	O
misclassiﬁcation	O
in	O
the	O
trained	O
networks	O
was	O
used	O
for	O
evaluation	O
.	O
are	O
modiﬁed	O
to	O
optimise	O
the	O
match	O
between	O
outputs	O
6.2.1	O
perceptrons	O
and	O
multi	O
layer	O
perceptrons	O
the	O
activation	O
of	O
the	O
mcculloch-pitts	O
neuron	O
has	O
been	O
generalised	O
to	O
the	O
form	O
x	O
h	O
byh3z	O
(	O
6.2	O
)	O
.	O
the	O
threshold	O
level	O
,	O
or	O
bias	O
of	O
equation	O
(	O
6.1	O
)	O
has	O
been	O
included	O
in	O
the	O
sum	O
,	O
with	O
the	O
assumption	O
of	O
an	O
extra	O
component	O
in	O
the	O
vector	O
where	O
the	O
activation	O
function	O
,	O
ë	O
	O
can	O
be	O
any	O
non-linear	O
function	O
.	O
the	O
nodes	O
have	O
been	O
divided	O
into	O
an	O
input	B
layerý	O
and	O
an	O
output	B
layer	O
t	O
whose	O
value	O
is	O
ﬁxed	O
at	O
1.	O
rosenblatt	O
studied	O
the	O
capabilities	O
of	O
groups	O
of	O
neurons	O
in	O
a	O
learning	O
suitable	O
weights	O
for	O
classiﬁcation	B
problems	O
(	O
rosenblatt	O
,	O
1962	O
)	O
.	O
whenë	O
single	O
layer	O
,	O
and	O
hence	O
all	O
acting	O
on	O
the	O
same	O
input	B
vectors	O
;	O
this	O
structure	O
was	O
termed	O
the	O
perceptron	O
(	O
rosenblatt	O
,	O
1958	O
)	O
,	O
and	O
rosenblatt	O
proposed	O
the	O
perceptron	O
learning	O
rule	O
for	O
is	O
a	O
hard	O
threshold	O
function	O
(	O
i.e.	O
,	O
discontinuously	O
jumps	O
from	O
a	O
lower	O
to	O
an	O
upper	O
limiting	O
value	O
)	O
,	O
equation	O
(	O
6.2	O
)	O
deﬁnes	O
a	O
non-linear	O
function	O
across	O
a	O
hyperplane	O
in	O
the	O
attribute	O
space	O
;	O
with	O
a	O
threshold	O
activation	O
function	O
the	O
neuron	O
output	B
is	O
simply	O
1	O
on	O
one	O
side	O
of	O
the	O
hyperplane	O
and	O
0	O
on	O
the	O
other	O
.	O
when	O
combined	O
in	O
a	O
perceptron	O
structure	O
,	O
neurons	O
can	O
segment	O
the	O
attribute	O
space	O
into	O
regions	O
,	O
and	O
this	O
forms	O
the	O
basis	O
of	O
the	O
capability	O
of	O
perceptron	O
networks	O
to	O
perform	O
classiﬁcation	B
.	O
minsky	O
and	O
papert	O
pointed	O
out	O
,	O
however	O
,	O
that	O
many	O
real	O
world	O
problems	O
do	O
not	O
fall	O
into	O
this	O
simple	O
framework	O
,	O
citing	O
the	O
exclusive-or	O
problem	O
as	O
the	O
simplest	O
example	B
.	O
here	O
it	O
is	O
necessary	O
to	O
isolate	O
two	O
convex	O
regions	O
,	O
joining	O
them	O
together	O
in	O
a	O
single	O
class	O
.	O
they	O
showed	O
that	O
while	O
this	O
was	O
not	O
possible	O
with	O
a	O
perceptron	O
network	O
,	O
it	O
can	O
be	O
done	O
with	O
a	O
two	O
layer	O
perceptron	O
structure	O
(	O
minsky	O
&	O
papert	O
,	O
1969	O
)	O
.	O
this	O
formed	O
the	O
multi	O
layer	O
perceptron	O
(	O
mlp	O
)	O
which	O
is	O
widely	O
in	O
use	O
today	O
,	O
although	O
the	O
perceptron	O
learning	O
rule	O
(	O
also	O
called	O
the	O
delta	O
rule	O
)	O
could	O
not	O
be	O
generalised	O
to	O
ﬁnd	O
weights	O
for	O
this	O
structure	O
.	O
a	O
learning	O
rule	O
was	O
proposed	O
in	O
1985	O
which	O
allows	O
the	O
multi	O
layer	O
perceptron	O
to	O
learn	O
.	O
this	O
generalised	O
delta	O
rule	O
(	O
section	O
6.2.2	O
)	O
deﬁnes	O
a	O
notion	O
of	O
back-propagation	O
of	O
error	O
derivatives	O
through	O
the	O
network	O
(	O
werbos	O
,	O
1974	O
;	O
hinton	O
et	O
al.	O
,	O
1985	O
and	O
1986	O
)	O
,	O
and	O
enables	O
a	O
large	O
class	O
of	O
models	O
with	O
different	O
connection	O
structures	O
,	O
or	O
architectures	O
to	O
be	O
trained	O
.	O
these	O
publications	O
initiated	O
the	O
recent	O
academic	O
interest	O
in	O
neural	O
networks	O
,	O
and	O
the	O
ﬁeld	O
subsequently	O
came	O
to	O
the	O
attention	O
of	O
industrial	O
users	O
.	O
	O
µ	O
	O
f	O
³	O
à	O
	O
g	O
	O
	O
¬	O
ë	O
à	O
	O
i	O
	O
sec	O
.	O
6.2	O
]	O
supervised	O
networks	O
for	O
classiﬁcation	B
87	O
>	O
@	O
_jd	O
output	B
nodes	O
hidden	B
nodes	O
:	O
output	B
vector	O
(	O
linear	O
or	O
non-linear	O
)	O
>	O
a`\^d	O
weightss	O
>	O
]	O
\^d	O
weightss	O
(	O
usually	O
non-linear	O
)	O
>	O
b\c	O
d	O
input	B
nodes	O
:	O
input	B
vector	O
fig	O
.	O
6.2	O
:	O
mlp	O
structure	O
.	O
6.2.2	O
multi	O
layer	O
perceptron	O
structure	O
and	O
functionality	B
figure	O
6.2	O
shows	O
the	O
structure	O
of	O
a	O
standard	O
two-layer	O
perceptron	O
.	O
the	O
inputs	O
form	O
the	O
input	B
nodes	O
of	O
the	O
network	O
;	O
the	O
outputs	O
are	O
taken	O
from	O
the	O
output	B
nodes	O
.	O
the	O
middle	O
layer	O
of	O
nodes	O
,	O
visible	O
to	O
neither	O
the	O
inputs	O
nor	O
the	O
outputs	O
,	O
is	O
termed	O
the	O
hidden	B
layer	O
,	O
and	O
unlike	O
the	O
input	B
and	O
output	B
layers	O
,	O
its	O
size	O
is	O
not	O
ﬁxed	O
.	O
the	O
hidden	B
layer	O
is	O
generally	O
used	O
to	O
make	O
a	O
bottleneck	O
,	O
forcing	O
the	O
network	O
to	O
make	O
a	O
simple	O
model	O
of	O
the	O
system	O
generating	O
the	O
data	O
,	O
with	O
the	O
ability	O
to	O
generalise	O
to	O
previously	O
unseen	O
patterns	O
.	O
the	O
operation	O
of	O
this	O
network	O
is	O
speciﬁed	O
by	O
	O
<	O
f	O
>	O
b\d	O
>	O
b\c-d	O
>	O
]	O
\^d	O
$	O
d	O
>	O
b\d	O
>	O
a`\^d	O
>	O
h`id	O
d	O
,	O
via	O
the	O
>	O
b\d	O
,	O
in	O
a	O
manner	O
parameterised	O
by	O
the	O
two	O
layers	O
of	O
weightsw	O
>	O
b\c-d	O
this	O
speciﬁes	O
how	O
input	B
pattern	O
vector	O
«	O
is	O
mapped	O
into	O
output	B
pattern	O
vector¬	O
>	O
bj	O
d	O
are	O
typically	O
each	O
set	O
to	O
>	O
a`\d	O
.	O
the	O
univariate	O
functionsë	O
andw	O
hidden	B
pattern	O
vector¬	O
þk	O
tof	O
atl	O
which	O
varies	O
smoothly	O
from°	O
at	O
!	O
l	O
c	O
«	O
ex	O
,	O
as	O
a	O
threshold	O
function	O
would	O
do	O
abruptly	O
.	O
if	O
the	O
number	O
of	O
hidden	B
layer	O
nodes	O
is	O
less	O
than	O
the	O
number	O
of	O
degrees	O
of	O
freedom	O
inherent	O
in	O
the	O
training	O
data	O
,	O
the	O
activations	O
of	O
the	O
hidden	B
nodes	O
tend	O
to	O
form	O
an	O
orthogonal	O
set	O
of	O
variables	O
,	O
either	O
linear	O
or	O
non-linear	O
combinations	O
of	O
the	O
attribute	O
variables	O
,	O
which	O
span	O
as	O
large	O
a	O
subspace	O
of	O
the	O
problem	O
as	O
possible	O
.	O
with	O
a	O
little	O
extra	O
constraint	O
on	O
(	O
6.3	O
)	O
(	O
6.4	O
)	O
r	O
[	O
[	O
¬	O
	O
	O
ë	O
e	O
à	O
	O
i	O
	O
	O
«	O
g	O
¬	O
	O
	O
ë	O
d	O
e	O
à	O
	O
i	O
	O
	O
¬	O
	O
f	O
g	O
>	O
å	O
ë	O
g	O
	O
f	O
f	O
88	O
neural	O
networks	O
[	O
ch	O
.	O
6	O
the	O
network	O
,	O
these	O
internal	O
variables	O
form	O
a	O
linear	O
or	O
non-linear	O
principal	O
component	O
representation	O
of	O
the	O
attribute	O
space	O
.	O
if	O
the	O
data	O
has	O
noise	O
added	O
that	O
is	O
not	O
an	O
inherent	O
part	O
of	O
the	O
generating	O
system	O
,	O
then	O
the	O
principal	O
component	O
network	O
acts	O
as	O
a	O
ﬁlter	O
of	O
the	O
lower-variance	O
noise	O
signal	O
,	O
provided	O
the	O
signal	O
to	O
noise	O
ratio	O
of	O
the	O
data	O
is	O
sufﬁciently	O
high	O
.	O
this	O
property	O
gives	O
mlps	O
the	O
ability	O
to	O
generalise	O
to	O
previously	O
unseen	O
patterns	O
,	O
by	O
modelling	O
only	O
the	O
important	O
underlying	O
structure	O
of	O
the	O
generating	O
system	O
.	O
the	O
hidden	B
nodes	O
can	O
be	O
regarded	O
as	O
detectors	O
of	O
abstract	O
features	O
of	O
the	O
attribute	O
space	O
.	O
universal	O
approximators	O
and	O
universal	O
computers	O
in	O
the	O
multilayer	O
perceptron	O
(	O
mlp	O
)	O
such	O
as	O
the	O
two-layer	O
version	O
in	O
equation	O
(	O
6.3	O
)	O
,	O
the	O
(	O
and	O
the	O
weights	O
)	O
.	O
it	O
can	O
be	O
shown	O
(	O
funahashi	O
,	O
1989	O
)	O
that	O
the	O
two-layer	O
mlp	O
can	O
approximate	O
an	O
arbitrary	O
continuous	O
mapping	O
arbitrarily	O
closely	O
if	O
there	O
is	O
no	O
limit	O
to	O
the	O
number	O
of	O
hidden	B
nodes	O
.	O
in	O
this	O
sense	O
the	O
mlp	O
is	O
a	O
universal	O
function	O
approximator	O
.	O
this	O
theorem	O
does	O
not	O
imply	O
that	O
more	O
complex	O
mlp	O
architectures	O
are	O
pointless	O
;	O
it	O
can	O
be	O
more	O
efﬁcient	O
(	O
in	O
terms	O
of	O
the	O
number	O
of	O
nodes	O
and	O
weights	O
required	O
)	O
to	O
use	O
different	O
numbers	O
of	O
layers	O
for	O
different	O
problems	O
.	O
unfortunately	O
there	O
is	O
a	O
shortage	O
of	O
rigorous	O
principles	O
on	O
which	O
to	O
base	O
a	O
choice	O
of	O
architecture	O
,	O
but	O
many	O
heuristic	O
principles	O
have	O
been	O
invented	O
and	O
explored	O
.	O
prominent	O
among	O
these	O
are	O
symmetry	O
principles	O
(	O
lang	O
et	O
al.	O
,	O
1990	O
;	O
le	O
cun	O
et	O
al.	O
,	O
1989	O
)	O
and	O
constructive	O
algorithms	O
(	O
wynne-jones	O
,	O
1991	O
)	O
.	O
output-layer	O
node	O
valuesv	O
are	O
functions	O
of	O
the	O
input-layer	O
node	O
valuest	O
deﬁne	O
a	O
recurrent	O
network	O
by	O
feeding	O
the	O
outputs	O
back	O
to	O
the	O
inputs	O
.	O
the	O
general	O
form	O
of	O
a	O
recurrent	O
perceptron	O
is	O
the	O
mlp	O
is	O
a	O
feedforward	O
network	O
,	O
meaning	O
that	O
the	O
output	B
vectorv	O
the	O
input	B
vectort	O
nm	O
c1o	O
for	O
some	O
vector	O
functionm	O
given	O
in	O
detail	O
by	O
(	O
6.3	O
)	O
in	O
the	O
2-layer	O
case	O
.	O
it	O
is	O
also	O
possible	O
to	O
feg	O
¬d¨cìe	O
vjcëìðe	O
feg	O
and	O
some	O
parametersw	O
;	O
w	O
cì	O
pm	O
this	O
is	O
a	O
discrete-time	O
model	O
;	O
continuous-time	O
models	O
governed	O
by	O
a	O
differential	O
equation	O
of	O
similar	O
structure	O
are	O
also	O
studied	O
.	O
recurrent	O
networks	O
are	O
universal	O
computers	O
in	O
the	O
sense	O
that	O
given	O
an	O
inﬁnite	O
number	O
of	O
nodes	O
,	O
they	O
can	O
emulate	O
any	O
calculation	O
which	O
can	O
be	O
done	O
on	O
a	O
universal	O
turing	O
machine	O
.	O
(	O
the	O
inﬁnite	O
number	O
of	O
nodes	O
is	O
needed	O
to	O
simulate	O
the	O
inﬁnite	O
turing	O
tape	O
.	O
)	O
this	O
result	O
is	O
easily	O
proved	O
for	O
hard-threshold	O
recurrent	O
perceptrons	O
by	O
sketching	O
a	O
1-node	O
network	O
which	O
performs	O
not-and	O
and	O
another	O
which	O
functions	O
as	O
a	O
flip-flop	O
.	O
these	O
elements	O
are	O
all	O
that	O
are	O
required	O
to	O
build	O
a	O
computer	O
.	O
this	O
chapter	O
focuses	O
on	O
feedforward	O
neural	O
network	O
models	O
because	O
they	O
are	O
simpler	O
to	O
use	O
,	O
better	O
understood	O
,	O
and	O
closely	O
connected	O
with	O
statistical	B
classiﬁcation	O
methods	O
.	O
however	O
recurrent	O
networks	O
attract	O
a	O
great	O
deal	O
of	O
research	O
interest	O
because	O
of	O
their	O
potential	O
to	O
serve	O
as	O
a	O
vehicle	O
for	O
bringing	O
statistical	B
methods	O
to	O
bear	O
on	O
algorithm	O
design	O
(	O
rohwer	O
1991a	O
,	O
1991b	O
,	O
1992	O
;	O
rohwer	O
et	O
al.	O
,	O
1992	O
;	O
shastri	O
&	O
ajjanagadde	O
1993	O
)	O
.	O
;	O
we	O
can	O
say	O
is	O
a	O
function	O
of	O
(	O
6.5	O
)	O
which	O
could	O
be	O
written	O
c1v+cì	O
g	O
;	O
w	O
w	O
v	O
g	O
	O
ë	O
d	O
e	O
à	O
	O
i	O
	O
	O
¬	O
	O
g	O
f	O
g	O
~	O
g	O
sec	O
.	O
6.2	O
]	O
supervised	O
networks	O
for	O
classiﬁcation	B
89	O
°	O
.	O
the	O
parameters	O
which	O
minimise	O
the	O
error	O
measure	B
(	O
6.6	O
)	O
in	O
(	O
6.3	O
)	O
.	O
if	O
c1u	O
to	O
the	O
average	O
target	O
(	O
6.7	O
)	O
usually	O
classiﬁcation	B
problems	O
are	O
represented	O
using	O
one-out-of-n	O
output	B
coding	O
.	O
one	O
rq	O
cb¬	O
°	O
;	O
otherwiseµ	O
training	O
mlps	O
by	O
nonlinear	O
regression	O
in	O
neural	O
network	O
parlance	O
,	O
training	O
is	O
the	O
process	O
of	O
ﬁtting	O
network	O
parameters	O
(	O
its	O
weights	O
)	O
to	O
given	O
data	O
.	O
the	O
training	O
data	O
consists	O
of	O
a	O
set	O
of	O
examples	O
of	O
corresponding	O
inputs	O
and	O
probabilistic	O
interpretation	O
of	O
mlp	O
outputs	O
if	O
there	O
is	O
a	O
one-to-many	O
relationship	O
between	O
the	O
inputs	O
and	O
targets	O
in	O
the	O
training	O
data	O
,	O
then	O
it	O
is	O
not	O
possible	O
for	O
any	O
mapping	O
of	O
the	O
form	O
(	O
6.5	O
)	O
to	O
perform	O
perfectly	O
.	O
it	O
is	O
straightforward	O
any	O
given	O
network	O
might	O
or	O
not	O
be	O
able	O
to	O
approximate	O
this	O
mapping	O
well	O
,	O
but	O
when	O
trained	O
as	O
well	O
as	O
possible	O
it	O
will	O
form	O
its	O
best	O
possible	O
approximation	O
to	O
this	O
mean	O
.	O
many	O
commonly-used	O
error	O
measures	O
in	O
addition	O
to	O
(	O
6.6	O
)	O
share	O
this	O
property	O
(	O
hampshire	O
&	O
pearlmuter	O
,	O
1990	O
)	O
.	O
desired	O
outputs	O
,	O
or	O
“	O
targets	O
”	O
.	O
let	O
the	O
th	O
example	B
be	O
given	O
by	O
inputy	O
	O
and	O
targetq	O
	O
for	O
input	B
dimension	O
	O
for	O
target	O
dimension	O
.	O
usually	O
a	O
least-squares	O
ﬁt	O
is	O
obtained	O
by	O
ﬁnding	O
	O
are	O
the	O
output	B
values	O
obtained	O
by	O
substituting	O
the	O
inputsy	O
where¬	O
	O
for	O
«	O
the	O
ﬁt	O
is	O
perfect	O
,	O
µ	O
to	O
show	O
(	O
bourlard	O
&	O
wellekens	O
,	O
1990	O
)	O
that	O
if	O
a	O
probability	O
densitys	O
data	O
,	O
then	O
the	O
minimum	O
of	O
(	O
6.6	O
)	O
is	O
attained	O
by	O
the	O
map	O
takingt	O
g	O
describes	O
the	O
uus0cbu	O
output	B
node	O
is	O
allocated	O
for	O
each	O
class	O
,	O
and	O
the	O
target	O
vectoru	O
;	O
	O
for	O
example	O
is	O
all°	O
’	O
s	O
except	O
for	O
af	O
on	O
the	O
node	O
indicating	O
the	O
correct	O
class	O
.	O
in	O
this	O
case	O
,	O
the	O
value	O
computed	O
belongs	O
to	O
class	O
.	O
collectively	O
the	O
outputs	O
expresss0cbu	O
by	O
the	O
g	O
.	O
this	O
not	O
only	O
provides	O
	O
th	O
training	O
inputt	O
iss0cbq	O
	O
,	O
iss0c1q	O
for	O
classiﬁcation	B
problems	O
.	O
given	O
that	O
the	O
value¬	O
	O
output	B
by	O
the	O
th	O
target	O
node	O
given	O
the	O
collection	O
of	O
training	O
outputsu	O
g	O
,	O
the	O
probability	O
of	O
the	O
entire	O
feg	O
,	O
sof	O
a1x	O
nv	O
ö¬	O
a1xy	O
s0c1u	O
bw	O
6z	O
rq	O
g	O
logc	O
	O
log¬	O
helpful	O
insight	O
,	O
but	O
also	O
provides	O
a	O
principle	O
with	O
which	O
neural	O
network	O
models	O
can	O
be	O
combined	O
with	O
other	O
probabilistic	O
models	O
(	O
bourlard	O
&	O
wellekens	O
,	O
1990	O
)	O
.	O
therefore	O
the	O
cross-entropy	O
can	O
be	O
used	O
as	O
an	O
error	O
measure	B
instead	O
of	O
a	O
sum	O
of	O
squares	O
(	O
6.6	O
)	O
.	O
it	O
happens	O
that	O
its	O
minimum	O
also	O
lies	O
at	O
the	O
average	O
target	O
(	O
6.7	O
)	O
,	O
so	O
the	O
network	O
outputs	O
can	O
still	O
be	O
interpreted	O
probabilistically	O
,	O
and	O
furthermore	O
the	O
minimisation	O
of	O
cross-	O
entropy	O
is	O
equivalent	O
to	O
maximisation	O
of	O
the	O
likelihood	O
of	O
the	O
training	O
data	O
in	O
classiﬁcation	B
problems.	O
{	O
the	O
cross-entropy	O
(	O
6.9	O
)	O
has	O
this	O
interpretation	O
when	O
an	O
input	B
can	O
simultaneously	O
be	O
a	O
member	O
of	O
any	O
number	O
of	O
classes	O
,	O
and	O
membership	O
of	O
one	O
class	O
provides	O
no	O
information	O
about	O
membership	O
of	O
another	O
.	O
if	O
an	O
input	B
can	O
th	O
target	O
node	O
can	O
be	O
directly	O
interpreted	O
as	O
the	O
probability	O
that	O
the	O
input	B
pattern	O
the	O
probabilistic	O
interpretation	O
of	O
the	O
the	O
output	B
nodes	O
leads	O
to	O
a	O
natural	O
error	O
measure	B
this	O
is	O
the	O
exponential	O
of	O
the	O
cross-entropy	O
,	O
is	O
ö¬	O
èe	O
þ¬	O
(	O
6.8	O
)	O
(	O
6.9	O
)	O
	O
	O
µ	O
	O
|	O
	O
à	O
	O
à	O
	O
	O
	O
	O
g	O
	O
	O
	O
	O
	O
ô	O
¼	O
t	O
t	O
	O
¼	O
t	O
g	O
u	O
¼	O
t	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
°	O
g	O
	O
	O
¬	O
	O
f	O
	O
|	O
þ	O
w	O
µ	O
	O
à	O
	O
à	O
	O
y	O
q	O
	O
	O
c	O
f	O
	O
	O
f	O
	O
	O
g	O
z	O
90	O
neural	O
networks	O
[	O
ch	O
.	O
6	O
g	O
.	O
first	O
order	O
gradient	O
based	O
methods	O
:	O
the	O
probabilistic	O
interpretation	O
of	O
mlp	O
outputs	O
in	O
classiﬁcation	B
problems	O
must	O
be	O
made	O
with	O
some	O
caution	O
.	O
it	O
only	O
applies	O
if	O
the	O
network	O
is	O
trained	O
to	O
its	O
minimum	O
error	O
,	O
and	O
then	O
belongs	O
to	O
a	O
continuous	O
space	O
or	O
a	O
large	O
discrete	O
set	O
,	O
because	O
technically	O
a	O
large	O
or	O
inﬁnite	O
amount	O
of	O
data	O
is	O
required	O
.	O
this	O
problem	O
is	O
intimately	O
related	O
to	O
the	O
overtraining	O
and	O
generalisation	O
issues	O
discussed	O
below	O
.	O
for	O
the	O
theoretical	O
reasons	O
given	O
here	O
,	O
the	O
cross-entropy	O
is	O
the	O
most	O
appropriate	O
error	O
measure	B
for	O
use	O
in	O
classiﬁcation	B
problems	O
,	O
although	O
practical	O
experience	O
suggests	O
it	O
makes	O
little	O
difference	O
.	O
the	O
sum	O
of	O
squares	O
was	O
used	O
in	O
the	O
statlog	O
neural	O
network	O
trials	O
.	O
minimisation	O
methods	O
so	O
as	O
to	O
minimise	O
an	O
error	O
measure	B
such	O
as	O
(	O
6.6	O
)	O
.	O
in	O
the	O
simplest	O
cases	O
the	O
network	O
outputs	O
are	O
linear	O
in	O
the	O
weights	O
,	O
making	O
(	O
6.6	O
)	O
quadratic	O
.	O
then	O
the	O
minimal	O
error	O
can	O
be	O
found	O
by	O
solving	O
a	O
linear	O
system	O
of	O
equations	O
.	O
this	O
special	O
case	O
is	O
discussed	O
in	O
section	O
6.2.3	O
in	O
the	O
context	O
of	O
radial	O
basis	O
function	O
networks	O
,	O
which	O
have	O
this	O
property	O
.	O
in	O
the	O
generic	O
,	O
nonlinear	O
case	O
the	O
minimisation	O
is	O
accomplished	O
using	O
a	O
variant	O
of	O
gradient	O
descent	O
.	O
this	O
,	O
but	O
not	O
only	O
if	O
the	O
training	O
data	O
accurately	O
represents	O
the	O
underlying	O
probability	O
densitys0c1u	O
the	O
latter	O
condition	O
is	O
problematic	O
ift	O
neural	O
network	O
models	O
are	O
trained	O
by	O
adjusting	O
their	O
weight	O
matrix	O
parametersw	O
produces	O
a	O
local	O
minimum	O
,	O
aw	O
necessarily	O
the	O
global	O
minimum	O
ofµ0c1w	O
from	O
which	O
any	O
inﬁnitesimal	O
change	O
increasesµ	O
g	O
.	O
the	O
gradient|	O
;	O
µ0cbw	O
g	O
ofµ0c1w	O
c1w	O
c1w	O
is	O
the	O
vector	O
ﬁeld	O
of	O
derivatives	O
ofµ	O
|	O
;	O
µ0c1w	O
~edd	O
)	O
a	O
linear	O
approximation	O
toµ0c1w	O
(	O
a	O
ﬁeld	O
because	O
the	O
vector	O
depends	O
onw	O
imal	O
vicinity	O
of	O
an	O
arbitrary	O
weight	O
matrixw	O
ãc1wrw	O
µ0cbw	O
µ0c1w	O
e	O
}	O
|	O
;	O
µ0cbw	O
clearly	O
then	O
,	O
at	O
any	O
pointw	O
vector|	O
;	O
µ	O
changes	O
(	O
of	O
a	O
given	O
magnitude	O
)	O
which	O
one	O
could	O
make	O
tow	O
points	O
in	O
the	O
direction	O
of	O
fastest	O
increase	O
ofµ	O
in	O
the	O
direction	O
of	O
!	O
|	O
;	O
µ	O
the	O
most	O
.	O
consequently	O
an	O
adjustment	O
ofw	O
of|	O
;	O
µ	O
increasesµ	O
provides	O
the	O
maximum	O
possible	O
decrease	O
inµ	O
direction	O
for	O
(	O
inﬁnitesimal	O
)	O
descent	O
changes	O
whenw	O
,	O
chosen	O
small	O
enough	O
for~|	O
;	O
µ	O
descent	O
algorithm	O
requires	O
a	O
step	O
size	O
parameter~	O
wwø~|	O
;	O
µ0cbw	O
in	O
practice	O
,	O
trial	O
and	O
error	O
is	O
used	O
to	O
look	O
for	O
the	O
largest	O
step	O
size~	O
which	O
will	O
work	O
.	O
until	O
the	O
errorµ	O
belong	O
to	O
one	O
and	O
only	O
one	O
class	O
,	O
then	O
the	O
simple	O
entropy	O
,	O
obtained	O
by	O
dropping	O
the	O
terms	O
involving1	O
	O
,	O
the	O
problem	O
with	O
this	O
method	O
is	O
that	O
the	O
theorem	O
on	O
maximal	O
descent	O
only	O
applies	O
to	O
inﬁnitesimal	O
adjustments	O
.	O
the	O
gradient	O
changes	O
as	O
well	O
as	O
the	O
error	O
,	O
so	O
the	O
optimal	O
is	O
adjusted	O
.	O
the	O
pure	O
gradient	O
to	O
be	O
effectively	O
inﬁnitesimal	O
so	O
far	O
as	O
obtaining	O
descent	O
is	O
concerned	O
,	O
but	O
otherwise	O
as	O
large	O
as	O
possible	O
,	O
in	O
the	O
interests	O
of	O
speed	O
.	O
the	O
weights	O
are	O
repeatedly	O
adjusted	O
by	O
is	O
given	O
by	O
in	O
the	O
inﬁnites-	O
(	O
6.10	O
)	O
of	O
the	O
parameter	O
space	O
(	O
weight	O
space	O
)	O
of	O
the	O
network	O
,	O
the	O
;	O
i.e.	O
,	O
of	O
all	O
the	O
inﬁnitesimal	O
,	O
a	O
change	O
in	O
the	O
direction	O
compute	O
the	O
gradient	O
and	O
adjust	O
the	O
weights	O
in	O
the	O
opposite	O
direction	O
.	O
.	O
the	O
basic	O
strategy	O
in	O
gradient	O
descent	O
is	O
to	O
should	O
be	O
used	O
.	O
fails	O
to	O
descend	O
.	O
(	O
6.11	O
)	O
with	O
large	O
step	O
sizes	O
,	O
the	O
gradient	O
will	O
tend	O
to	O
change	O
dramatically	O
with	O
each	O
step	O
.	O
a	O
¼	O
t	O
g	O
g	O
	O
c	O
	O
µ	O
g	O
	O
i	O
|	O
~	O
	O
µ	O
g	O
	O
i	O
	O
g	O
g	O
å	O
g	O
	O
å	O
g	O
å	O
g	O
å	O
g	O
w	O
g	O
sec	O
.	O
6.2	O
]	O
supervised	O
networks	O
for	O
classiﬁcation	B
91	O
w	O
old	O
e©á	O
the	O
quadratic	O
approximation	O
,	O
is	O
the	O
matrix	O
with	O
components	O
popular	O
heuristic	O
is	O
to	O
use	O
a	O
moving	O
average	O
of	O
the	O
gradient	O
vector	O
in	O
order	O
ﬁnd	O
a	O
systematic	O
tendency	O
.	O
this	O
is	O
accomplished	O
by	O
adding	O
a	O
momentum	O
term	O
to	O
(	O
6.11	O
)	O
,	O
involving	O
a	O
parameter	O
these	O
methods	O
offer	O
the	O
beneﬁt	O
of	O
simplicity	O
,	O
but	O
their	O
performance	O
depends	O
sensitively	O
(	O
toolenaere	O
,	O
1990	O
)	O
.	O
different	O
values	O
seem	O
to	O
be	O
appropriate	O
for	O
different	O
problems	O
,	O
and	O
for	O
different	O
stages	O
of	O
training	O
in	O
one	O
problem	O
.	O
this	O
circum-	O
stance	O
has	O
given	O
rise	O
to	O
a	O
plethora	O
of	O
heuristics	O
for	O
adaptive	O
variable	O
step	O
size	O
algorithms	O
(	O
toolenaere	O
,	O
1990	O
;	O
silva	O
&	O
almeida	O
,	O
1990	O
;	O
jacobs	O
,	O
1988	O
)	O
.	O
second-order	B
methods	O
the	O
underlying	O
difﬁculty	O
in	O
ﬁrst	O
order	O
gradient	O
based	O
methods	O
is	O
that	O
the	O
linear	O
approxi-	O
,	O
to	O
a	O
stationary	O
point	O
of	O
this	O
quadratic	O
form	O
.	O
this	O
may	O
be	O
a	O
minimum	O
,	O
maximum	O
,	O
or	O
saddle	O
point	O
.	O
if	O
it	O
is	O
a	O
minimum	O
,	O
then	O
a	O
step	O
in	O
that	O
direction	O
seems	O
a	O
good	O
idea	O
;	O
if	O
not	O
,	O
then	O
a	O
positive	O
or	O
negative	O
step	O
(	O
whichever	O
has	O
a	O
negative	O
projection	O
,	O
is	O
at	O
least	O
not	O
unreasonable	O
.	O
therefore	O
a	O
large	O
class	O
of	O
algorithms	O
has	O
been	O
developed	O
involving	O
the	O
conjugate	O
gradient	O
.	O
.	O
is	O
roughly	O
half	O
the	O
square	O
of	O
the	O
number	O
of	O
components	O
,	O
so	O
for	O
large	O
networks	O
involving	O
many	O
weights	O
,	O
such	O
algorithms	O
lead	O
to	O
impractical	O
computer	O
memory	O
requirements	O
.	O
but	O
one	O
algorithm	O
,	O
generally	O
called	O
the	O
conjugate	O
gradient	O
algorithm	O
,	O
or	O
the	O
memoryless	O
conjugate	O
gradient	O
algorithm	O
,	O
does	O
not	O
.	O
this	O
algorithm	O
á	O
f	O
:	O
wwø~|	O
;	O
µ0cbw	O
here	O
w	O
old	O
refers	O
to	O
the	O
most	O
recent	O
weight	O
change	O
.	O
on	O
the	O
parameters~	O
andá	O
mation	O
(	O
6.10	O
)	O
ignores	O
the	O
curvature	O
ofµ0cbw	O
g	O
.	O
this	O
can	O
be	O
redressed	O
by	O
extending	O
(	O
6.10	O
)	O
to	O
w||	O
;	O
µ0cbw	O
wèe	O
µ0c1w	O
e	O
}	O
|	O
;	O
µ0cbw	O
µ0cbw	O
where||	O
;	O
µ	O
3	O
a	O
,	O
called	O
the	O
inverse	O
hessian	O
(	O
or	O
the	O
hessian	O
,	O
depending	O
on	O
conventions	O
)	O
,	O
and	O
å	O
.	O
the	O
change	O
3	O
	O
\^	O
wªw	O
	O
,	O
bringsw	O
||	O
;	O
µ	O
where	O
on	O
the	O
gradient	O
)	O
in	O
the	O
conjugate	O
gradient	O
direction	O
,	O
u|	O
;	O
µ	O
most	O
of	O
these	O
algorithms	O
require	O
explicit	O
computation	O
or	O
estimation	O
of	O
the	O
hessian	O
the	O
number	O
of	O
components	O
of	O
ofw	O
maintains	O
an	O
estimate	O
of	O
the	O
conjugate	O
direction	O
without	O
directly	O
representing	O
searches	O
for	O
the	O
minimum	O
ofµ0cbw	O
g	O
,	O
starting	O
from	O
the	O
most	O
recent	O
estimate	O
of	O
the	O
minimum	O
orthogonal	O
to	O
the	O
gradient	O
,	O
making	O
the	O
variation	O
ofµ0cbw	O
the	O
update	O
rule	O
for	O
the	O
conjugate	O
gradient	O
direction	O
	O
%	O
	O
and	O
searching	O
for	O
the	O
minimum	O
in	O
the	O
direction	O
of	O
the	O
current	O
estimate	O
of	O
the	O
conjugate	O
gradient	O
.	O
linesearch	O
algorithms	O
are	O
comparatively	O
easy	O
because	O
the	O
issue	O
of	O
direction	O
choice	O
reduces	O
to	O
a	O
binary	O
choice	O
.	O
but	O
because	O
the	O
linesearch	O
appears	O
in	O
the	O
inner	O
loop	O
of	O
the	O
conjugate	O
gradient	O
algorithm	O
,	O
efﬁciency	O
is	O
important	O
.	O
considerable	O
effort	O
therefore	O
goes	O
into	O
it	O
,	O
to	O
the	O
extent	O
that	O
the	O
linesearch	O
is	O
typically	O
the	O
most	O
complicated	O
module	O
of	O
a	O
conjugate	O
gradient	O
implementation	O
.	O
numerical	O
round-off	O
problems	O
are	O
another	O
design	O
consideration	O
in	O
linesearch	O
implementations	O
,	O
because	O
the	O
conjugate	O
gradient	O
is	O
often	O
nearly	O
g	O
along	O
the	O
conjugate	O
gradient	O
is	O
(	O
6.12	O
)	O
the	O
conjugate	O
gradient	O
algorithm	O
uses	O
a	O
sequence	O
of	O
linesearches	O
,	O
one-dimensional	O
especially	O
small	O
.	O
where	O
	O
!	O
|	O
;	O
µ	O
!	O
e	O
!	O
á	O
old	O
.	O
	O
g	O
g	O
	O
å	O
g	O
å	O
g	O
ã	O
å	O
g	O
w	O
x	O
w	O
	O
w	O
	O
	O
	O
þ	O
|	O
	O
92	O
neural	O
networks	O
to	O
continue	O
.	O
this	O
network	O
architecture	O
was	O
used	O
in	O
the	O
work	O
reported	O
in	O
this	O
book	O
.	O
an	O
implementation	O
of	O
the	O
conjugate	O
gradient	O
algorithm	O
will	O
have	O
several	O
parameters	O
(	O
this	O
is	O
the	O
polak-ribiere	O
variant	O
;	O
there	O
are	O
others	O
.	O
)	O
somewhat	O
intricate	O
proofs	O
exist	O
which	O
.	O
in	O
practice	O
good	O
performance	O
is	O
often	O
obtained	O
on	O
much	O
more	O
general	O
functions	O
using	O
very	O
imprecise	O
linesearches	O
.	O
it	O
is	O
necessary	O
to	O
augment	O
(	O
6.13	O
)	O
with	O
a	O
rule	O
.	O
but	O
unlike	O
the	O
step	O
size	O
and	O
momentum	O
parameters	O
of	O
the	O
simpler	O
methods	O
,	O
the	O
performance	O
of	O
the	O
conjugate	O
gradient	O
method	O
is	O
relatively	O
insensitive	O
to	O
its	O
parameters	O
if	O
they	O
are	O
set	O
within	O
reasonable	O
ranges	O
.	O
all	O
algorithms	O
are	O
sensitive	O
to	O
process	O
for	O
selecting	O
initial	O
weights	O
,	O
and	O
many	O
other	O
factors	O
which	O
remain	O
to	O
be	O
carefully	O
isolated	O
.	O
gradient	O
calculations	O
in	O
mlps	O
in	O
the	O
case	O
of	O
an	O
mlp	O
neural	O
network	O
model	O
with	O
an	O
error	O
measure	B
such	O
as	O
(	O
6.6	O
)	O
.	O
the	O
calculation	O
is	O
conveniently	O
organised	O
as	O
a	O
back	O
propagation	O
of	O
error	O
(	O
rumelhart	O
et	O
al.	O
,	O
1986	O
;	O
rohwer	O
&	O
renals	O
,	O
1988	O
)	O
.	O
for	O
a	O
network	O
with	O
a	O
single	O
layer	O
of	O
hidden	B
nodes	O
,	O
this	O
calculation	O
proceeds	O
by	O
propagating	O
forward	B
from	O
the	O
input	B
to	O
output	B
layers	O
for	O
each	O
training	O
example	B
,	O
and	O
related	O
to	O
the	O
output	B
errors	O
backwards	O
through	O
a	O
linearised	O
ã-|	O
;	O
µ	O
|	O
;	O
µ	O
!	O
	O
}	O
|	O
;	O
µ	O
oldz	O
|	O
;	O
µ	O
oldã7|	O
;	O
µ	O
old	O
show	O
that	O
ifµ	O
were	O
purely	O
quadratic	O
inw	O
,	O
	O
were	O
initialised	O
to	O
the	O
gradient	O
,	O
and	O
the	O
linesearches	O
were	O
performed	O
exactly	O
,	O
then	O
would	O
converge	O
on	O
the	O
conjugate	O
gradient	O
components	O
ofw	O
andµ	O
would	O
converge	O
on	O
its	O
minimum	O
after	O
as	O
many	O
iterations	O
of	O
(	O
6.12	O
)	O
as	O
there	O
are	O
to|	O
;	O
µ	O
whenever	O
becomes	O
too	O
nearly	O
orthogonal	O
to	O
the	O
gradient	O
for	O
progress	O
to	O
reset	O
controlling	O
the	O
details	O
of	O
the	O
linesearch	O
,	O
and	O
others	O
which	O
deﬁne	O
exactly	O
when	O
to	O
reset	O
to	O
!	O
|	O
;	O
µ	O
it	O
remains	O
to	O
discuss	O
the	O
computation	O
of	O
the	O
gradient|	O
;	O
µ	O
c1w	O
then	O
propagating	O
quantities	O
version	O
of	O
the	O
network	O
.	O
products	O
of	O
s	O
and¬	O
s	O
then	O
give	O
the	O
gradient	O
.	O
in	O
the	O
case	O
of	O
a	O
node	O
output	B
values¬	O
g	O
,	O
a	O
single	O
hidden	B
layerc6	O
g	O
,	O
and	O
an	O
output	B
or	O
target	O
layer	O
network	O
with	O
an	O
input	B
layercý	O
g	O
,	O
the	O
calculation	O
is	O
:	O
>	O
b\c-d	O
>	O
b\^d	O
>	O
]	O
\^d	O
cbz	O
>	O
a`\^d	O
>	O
b\d	O
>	O
h`id	O
>	O
a`d	O
rq	O
>	O
a`\d	O
>	O
b\^d	O
>	O
a`id	O
>	O
a`d	O
>	O
a`id	O
>	O
h`\d	O
>	O
a`d	O
>	O
b\^d	O
·	O
	O
>	O
b\d	O
>	O
b\^d	O
>	O
b\-d	O
y	O
;	O
·	O
	O
is	O
summed	O
over	O
training	O
examples	O
,	O
while	O
the	O
s	O
andd	O
s	O
refer	O
to	O
nodes	O
,	O
and	O
ï£b¤	O
c0x	O
>	O
6j	O
the	O
index	O
·	O
	O
c¬	O
g	O
c	O
«	O
á'	O
¢¡	O
[	O
ch	O
.	O
6	O
(	O
6.13	O
)	O
(	O
6.14	O
)	O
(	O
6.15	O
)	O
á	O
	O
y	O
g	O
¬	O
	O
	O
ë	O
d	O
e	O
à	O
	O
i	O
	O
	O
«	O
	O
f	O
g	O
¬	O
	O
	O
ë	O
d	O
e	O
à	O
	O
i	O
	O
	O
¬	O
	O
f	O
g	O
	O
	O
	O
	O
	O
	O
g	O
	O
	O
	O
à	O
·	O
ë	O
y	O
·	O
	O
i	O
·	O
	O
	O
µ	O
h	O
	O
i	O
	O
·	O
	O
à	O
	O
	O
	O
ë	O
y	O
	O
	O
¬	O
·	O
	O
	O
µ	O
h	O
	O
i	O
	O
·	O
	O
à	O
	O
	O
	O
ë	O
y	O
	O
	O
ë	O
y	O
d	O
	O
	O
	O
«	O
ë	O
	O
	O
	O
	O
	O
y	O
¥	O
z	O
sec	O
.	O
6.2	O
]	O
supervised	O
networks	O
for	O
classiﬁcation	B
93	O
linear	O
output	B
weights	O
non-linear	O
receptive	O
ﬁelds	O
in	O
attribute	O
space	O
fig	O
.	O
6.3	O
:	O
a	O
radial	O
basis	O
function	O
network	O
.	O
online	O
vs.	O
batch	O
(	O
6.6	O
)	O
and	O
the	O
gradient|	O
;	O
µ	O
(	O
6.14	O
,	O
6.15	O
)	O
are	O
a	O
sum	O
over	O
examples	O
.	O
these	O
could	O
be	O
estimated	O
by	O
randomly	O
selecting	O
a	O
subset	O
of	O
examples	O
for	O
inclusion	O
in	O
the	O
sum	O
.	O
in	O
the	O
extreme	O
,	O
a	O
single	O
example	B
might	O
be	O
used	O
for	O
each	O
gradient	O
estimate	O
.	O
this	O
is	O
a	O
stochastic	O
gradient	O
method	O
.	O
if	O
a	O
similar	O
strategy	O
is	O
used	O
without	O
random	O
selection	O
,	O
but	O
with	O
the	O
data	O
taken	O
in	O
the	O
order	O
it	O
comes	O
,	O
the	O
method	O
is	O
an	O
online	O
one	O
.	O
if	O
a	O
sum	O
over	O
all	O
note	O
that	O
both	O
the	O
errorµ	O
training	O
data	O
is	O
performed	O
for	O
each	O
gradient	O
calculation	O
,	O
then	O
the	O
method	O
is	O
a¦h¶ìm-§	O
variety	O
.	O
given	O
function	O
ofw	O
which	O
can	O
be	O
evaluated	O
precisely	O
so	O
that	O
meaningful	O
comparisons	O
can	O
gradient	O
method	O
,	O
because	O
it	O
is	O
built	O
on	O
procedures	O
and	O
theorems	O
which	O
assume	O
thatµ	O
online	O
and	O
stochastic	O
gradient	O
methods	O
offer	O
a	O
considerable	O
speed	O
advantage	O
if	O
the	O
approximation	O
is	O
serviceable	O
.	O
for	O
problems	O
with	O
large	O
amounts	O
of	O
training	O
data	O
they	O
are	O
highly	O
favoured	O
.	O
however	O
,	O
these	O
approximations	O
can	O
not	O
be	O
used	O
directly	O
in	O
the	O
conjugate	O
is	O
a	O
be	O
made	O
at	O
nearby	O
arguments	O
.	O
therefore	O
the	O
stochastic	O
gradient	O
and	O
online	O
methods	O
tend	O
to	O
be	O
used	O
with	O
simple	O
step-size	O
and	O
momentum	O
methods	O
.	O
there	O
is	O
some	O
work	O
on	O
ﬁnding	O
a	O
compromise	O
method	O
(	O
møller	O
,	O
1993	O
)	O
.	O
6.2.3	O
radial	O
basis	O
function	O
networks	O
the	O
radial	O
basis	O
function	O
network	O
consists	O
of	O
a	O
layer	O
of	O
units	O
performing	O
linear	O
or	O
non-linear	O
functions	O
of	O
the	O
attributes	O
,	O
followed	O
by	O
a	O
layer	O
of	O
weighted	O
connections	O
to	O
nodes	O
whose	O
outputs	O
have	O
the	O
same	O
form	O
as	O
the	O
target	O
vectors	O
.	O
it	O
has	O
a	O
structure	O
like	O
an	O
mlp	O
with	O
one	O
hidden	B
layer	O
,	O
except	O
that	O
each	O
node	O
of	O
the	O
the	O
hidden	B
layer	O
computes	O
an	O
arbitrary	O
function	O
of	O
the	O
inputs	O
(	O
with	O
gaussians	O
being	O
the	O
most	O
popular	O
)	O
,	O
and	O
the	O
transfer	O
function	O
of	O
each	O
output	B
node	O
is	O
the	O
trivial	O
identity	O
function	O
.	O
instead	O
of	O
“	O
synaptic	O
strengths	O
”	O
the	O
hidden	B
layer	O
has	O
parameters	O
appropriate	O
for	O
whatever	O
functions	O
are	O
being	O
used	O
;	O
for	O
example	B
,	O
gaussian	O
widths	O
and	O
positions	O
.	O
this	O
network	O
offers	O
a	O
number	O
of	O
advantages	O
over	O
the	O
multi	O
layer	O
perceptron	O
under	O
certain	O
conditions	O
,	O
although	O
the	O
two	O
models	O
are	O
computationally	O
equivalent	O
.	O
these	O
advantages	O
include	O
a	O
linear	O
training	O
rule	O
once	O
the	O
locations	O
in	O
attribute	O
space	O
of	O
the	O
non-linear	O
functions	O
have	O
been	O
determined	O
,	O
and	O
an	O
underlying	O
model	O
involving	O
localised	O
functions	O
in	O
the	O
attribute	O
space	O
,	O
rather	O
than	O
the	O
long-range	O
functions	O
occurring	O
in	O
perceptron-based	O
models	O
.	O
the	O
linear	O
learning	O
rule	O
avoids	O
problems	O
associated	O
with	O
local	O
minima	O
;	O
in	O
particular	O
it	O
provides	O
enhanced	O
ability	O
to	O
make	O
statments	O
about	O
the	O
accuracy	O
of	O
94	O
neural	O
networks	O
[	O
ch	O
.	O
6	O
the	O
probabilistic	O
interpretation	O
of	O
the	O
outputs	O
in	O
section	O
6.2.2.	O
figure	O
6.3	O
shows	O
the	O
structure	O
of	O
a	O
radial	O
basis	O
function	O
;	O
the	O
non-linearities	O
comprise	O
a	O
position	O
in	O
attribute	O
space	O
at	O
which	O
the	O
function	O
is	O
located	O
(	O
often	O
referred	O
to	O
as	O
the	O
function	O
’	O
s	O
centre	O
)	O
,	O
and	O
a	O
non-linear	O
function	O
of	O
the	O
distance	O
of	O
an	O
input	B
point	O
from	O
that	O
centre	O
,	O
which	O
and	O
produce	O
an	O
interpolating	O
function	O
using	O
non-localised	O
functions	O
,	O
they	O
are	O
often	O
found	O
to	O
have	O
better	O
interpolating	O
properties	O
in	O
the	O
region	O
populated	O
by	O
the	O
training	O
data	O
.	O
can	O
be	O
any	O
function	O
at	O
all	O
.	O
common	O
choices	O
include	O
a	O
gaussian	O
response	O
function	O
,	O
expcik	O
«	O
	O
)	O
,	O
as	O
well	O
as	O
non-local	O
functions	O
such	O
as	O
thin	O
plate	O
log¨	O
)	O
and	O
multiquadrics	O
(	O
	O
splines	O
(	O
¨	O
and	O
inverse	O
multiquadrics	O
(	O
	O
	O
)	O
.	O
although	O
it	O
seems	O
counter-intuitive	O
to	O
try	O
e	O
!	O
m	O
exm	O
the	O
radial	O
basis	O
function	O
network	O
approach	O
involves	O
the	O
expansion	O
or	O
pre-processing	O
of	O
input	B
vectors	O
into	O
a	O
high-dimensional	O
space	O
.	O
this	O
attempts	O
to	O
exploit	O
a	O
theorem	O
of	O
cover	O
(	O
1965	O
)	O
which	O
implies	O
that	O
a	O
classiﬁcation	B
problem	O
cast	O
in	O
a	O
high-dimensional	O
space	O
is	O
more	O
likely	O
to	O
be	O
linearly	O
separable	O
than	O
would	O
be	O
the	O
case	O
in	O
a	O
low-dimensional	O
space	O
.	O
training	O
:	O
choosing	O
the	O
centres	O
and	O
non-linearities	O
a	O
number	O
of	O
methods	O
can	O
be	O
used	O
for	O
choosing	O
the	O
centres	O
for	O
a	O
radial	O
basis	O
function	O
network	O
.	O
it	O
is	O
important	O
that	O
the	O
distribution	O
of	O
centres	O
in	O
the	O
attribute	O
space	O
should	O
be	O
similar	O
to	O
,	O
or	O
at	O
least	O
cover	O
the	O
same	O
region	O
as	O
the	O
training	O
data	O
.	O
it	O
is	O
assumed	O
that	O
the	O
training	O
data	O
is	O
representative	O
of	O
the	O
problem	O
,	O
otherwise	O
good	O
performance	O
can	O
not	O
be	O
expected	O
on	O
future	O
unseen	O
patterns	O
.	O
a	O
ﬁrst	O
order	O
technique	O
for	O
choosing	O
centres	O
is	O
to	O
take	O
points	O
on	O
a	O
square	O
grid	O
covering	O
the	O
region	O
of	O
attribute	O
space	O
covered	O
by	O
the	O
training	O
data	O
.	O
alternatively	O
,	O
better	O
performance	O
might	O
be	O
expected	O
if	O
the	O
centres	O
were	O
sampled	O
at	O
random	O
from	O
the	O
training	O
data	O
itself	O
,	O
using	O
some	O
or	O
all	O
samples	O
,	O
since	O
the	O
more	O
densely	O
populated	O
regions	O
of	O
the	O
attribute	O
space	O
would	O
have	O
a	O
higher	O
resolution	O
model	O
than	O
sparser	O
regions	O
.	O
in	O
this	O
case	O
,	O
it	O
is	O
important	O
to	O
ensure	O
that	O
at	O
least	O
one	O
sample	O
from	O
each	O
class	O
is	O
used	O
as	O
a	O
prototype	O
centre	O
.	O
in	O
the	O
experiments	O
in	O
this	O
book	O
,	O
the	O
number	O
of	O
samples	O
required	O
from	O
each	O
class	O
was	O
calculated	O
before	O
sampling	O
,	O
thereby	O
ensuring	O
this	O
condition	O
was	O
met	O
.	O
when	O
centre	O
positions	O
are	O
chosen	O
for	O
radial	O
basis	O
function	O
networks	O
with	O
localised	O
non-linear	O
functions	O
such	O
as	O
gaussian	O
receptive	O
ﬁelds	O
,	O
it	O
is	O
important	O
to	O
calculate	O
suitable	O
variances	O
,	O
or	O
spreads	O
for	O
the	O
functions	O
.	O
this	O
ensures	O
that	O
large	O
regions	O
of	O
space	O
do	O
not	O
occur	O
between	O
centres	O
,	O
where	O
no	O
centres	O
respond	O
to	O
patterns	O
,	O
and	O
conversely	O
,	O
that	O
no	O
pair	O
of	O
centres	O
respond	O
nearly	O
identically	O
to	O
all	O
patterns	O
.	O
this	O
problem	O
is	O
particularly	O
prevalent	O
in	O
high	O
dimensional	O
attribute	O
spaces	O
because	O
volume	O
depends	O
sensitively	O
on	O
radius	O
.	O
for	O
a	O
quantitative	O
discussion	O
of	O
this	O
point	O
,	O
see	O
prager	O
&	O
fallside	O
(	O
1989	O
)	O
.	O
in	O
the	O
experiments	O
reported	O
in	O
this	O
book	O
,	O
the	O
standard	O
deviations	O
of	O
the	O
gaussian	O
functions	O
were	O
set	O
separately	O
for	O
each	O
coordinate	O
direction	O
to	O
the	O
distance	O
to	O
the	O
nearest	O
centre	O
in	O
that	O
direction	O
,	O
multiplied	O
by	O
an	O
arbitrary	O
scaling	O
parameter	O
(	O
set	O
to	O
1.0	O
)	O
.	O
other	O
methods	O
include	O
using	O
a	O
‘	O
principled	O
’	O
clustering	O
technique	O
to	O
position	O
the	O
centres	O
,	O
such	O
as	O
a	O
gaussian	O
mixture	O
model	O
or	O
a	O
kohonen	O
network	O
.	O
these	O
models	O
are	O
discussed	O
in	O
section	O
6.3.	O
training	O
:	O
optimising	O
the	O
weights	O
as	O
mentioned	O
in	O
section	O
6.2.2	O
,	O
radial	O
basis	O
function	O
networks	O
are	O
trained	O
simply	O
by	O
solving	O
a	O
linear	O
system	O
.	O
the	O
same	O
problem	O
arises	O
in	O
ordinary	O
linear	O
regression	O
,	O
the	O
only	O
difference	O
being	O
that	O
the	O
input	B
to	O
the	O
linear	O
system	O
is	O
the	O
output	B
of	O
the	O
hidden	B
layer	O
of	O
the	O
network	O
,	O
not	O
	O
g	O
¨	O
	O
	O
	O
þ	O
	O
	O
¨	O
	O
	O
	O
	O
sec	O
.	O
6.2	O
]	O
supervised	O
networks	O
for	O
classiﬁcation	B
95	O
written	O
out	O
in	O
full	O
is	O
then	O
>	O
b\d	O
·	O
	O
(	O
6.16	O
)	O
(	O
6.17	O
)	O
(	O
6.18	O
)	O
(	O
6.19	O
)	O
(	O
6.20	O
)	O
is	O
the	O
lies	O
where	O
the	O
gradient	O
vanishes	O
:	O
number	O
of	O
radial	O
basis	O
functions	O
.	O
which	O
has	O
its	O
minimum	O
where	O
the	O
derivative	O
be	O
the	O
correlation	O
matrix	O
of	O
the	O
radial	O
basis	O
function	O
outputs	O
,	O
the	O
attribute	O
variables	O
themselves	O
.	O
there	O
are	O
a	O
few	O
subtleties	O
however	O
,	O
which	O
are	O
discussed	O
>	O
]	O
\^d	O
is	O
computed	O
using	O
the	O
weightsi	O
th	O
radial	O
basis	O
function	O
on	O
the	O
th	O
example	B
.	O
the	O
output	B
be	O
the	O
output	B
of	O
thed	O
here	O
.	O
let¬	O
>	O
]	O
\^d	O
of	O
each	O
target	O
node	O
·	O
as	O
·d¬	O
let	O
the	O
desired	O
output	B
for	O
example	O
on	O
target	O
node	O
beq	O
	O
.	O
the	O
error	O
measure	B
(	O
6.6	O
)	O
>	O
]	O
\^d	O
µ0c1w	O
rq	O
·d¬	O
>	O
b\^d	O
>	O
b\^d	O
>	O
b\d	O
ic©	O
·d¬	O
ñb	O
vanishes	O
.	O
letª	O
>	O
b\d	O
the	O
weight	O
matrixw	O
«	O
which	O
minimisesµ	O
>	O
]	O
\^d	O
¬	O
thus	O
,	O
the	O
problem	O
is	O
solved	O
by	O
inverting	O
the	O
square	O
sition	O
(	O
renals	O
&	O
rohwer	O
,	O
1989	O
)	O
and	O
(	O
press	O
et	O
.	O
al.	O
,	O
1988	O
)	O
ifª	O
if	O
the	O
number	O
of	O
training	O
samples	O
is	O
not	O
at	O
least	O
as	O
great	O
as	O
k	O
be	O
the	O
number	O
of	O
training	O
examples	O
.	O
instead	O
of	O
solving	O
the	O
by	O
the	O
derivatives	O
ofµ	O
>	O
]	O
\^d	O
·d¬	O
unlessk	O
>	O
]	O
\^d	O
>	O
b\d	O
,	O
the	O
matrix	O
with	O
elements¬	O
ç	O
ofv	O
1989	O
)	O
v	O
>	O
b\d	O
uv	O
the	O
matrix	O
inversion	O
can	O
be	O
accomplished	O
by	O
standard	O
methods	O
such	O
as	O
lu	O
decompo-	O
is	O
neither	O
singular	O
nor	O
nearly	O
so	O
.	O
this	O
is	O
typically	O
the	O
case	O
,	O
but	O
things	O
can	O
go	O
wrong	O
.	O
if	O
two	O
radial	O
basis	O
function	O
centres	O
are	O
very	O
close	O
together	O
a	O
singular	O
matrix	O
will	O
result	O
,	O
and	O
a	O
singular	O
matrix	O
is	O
guaranteed	O
.	O
there	O
is	O
no	O
practical	O
way	O
to	O
ensure	O
a	O
non-singular	O
correlation	O
matrix	O
.	O
consequently	O
the	O
safest	O
course	O
of	O
action	O
is	O
to	O
use	O
a	O
slightly	O
more	O
computationally	O
expensive	O
singular	O
value	O
decomposition	O
method	O
.	O
such	O
methods	O
provide	O
an	O
approximate	O
inverse	O
by	O
diagonalising	O
the	O
matrix	O
,	O
inverting	O
only	O
the	O
eigenvalues	O
which	O
exceed	O
zero	O
by	O
a	O
parameter-speciﬁed	O
margin	O
,	O
and	O
transforming	O
back	O
to	O
the	O
original	O
coordinates	O
.	O
this	O
provides	O
an	O
optimal	O
minimum-norm	O
approximation	O
to	O
the	O
inverse	O
in	O
the	O
least-mean-squares	O
sense	O
.	O
another	O
approach	O
to	O
the	O
entire	O
problem	O
is	O
possible	O
(	O
broomhead	O
&	O
lowe	O
,	O
1988	O
)	O
.	O
let	O
linear	O
system	O
given	O
(	O
6.18	O
)	O
,	O
this	O
method	O
focuses	O
on	O
the	O
linear	O
system	O
embedded	O
in	O
the	O
,	O
this	O
is	O
a	O
rectangular	O
system	O
.	O
in	O
general	O
an	O
exact	O
solution	O
does	O
not	O
exist	O
,	O
but	O
the	O
optimal	O
solution	O
in	O
the	O
least-squares	O
sense	O
is	O
given	O
by	O
the	O
pseudo-inverse	O
(	O
kohonen	O
,	O
r	O
matrixª	O
,	O
where	O
(	O
6.21	O
)	O
(	O
6.22	O
)	O
error	O
formula	O
(	O
6.17	O
)	O
itself	O
:	O
¬	O
:	O
>	O
b\d	O
u	O
·	O
	O
	O
¬	O
	O
	O
	O
à	O
·	O
i	O
	O
·	O
	O
	O
g	O
	O
	O
	O
à	O
	O
	O
x	O
à	O
·	O
i	O
	O
·	O
	O
	O
	O
z	O
	O
	O
µ	O
	O
ñ	O
	O
à	O
·	O
à	O
	O
i	O
©	O
·	O
	O
¬	O
	O
	O
	O
à	O
	O
q	O
©	O
ú	O
	O
·	O
	O
à	O
	O
¬	O
¬	O
	O
	O
	O
w	O
«	O
	O
·	O
	O
à	O
©	O
à	O
	O
q	O
	O
©	O
	O
y	O
ª	O
þ	O
|	O
z	O
©	O
·	O
à	O
·	O
i	O
	O
·	O
	O
	O
q	O
	O
	O
	O
	O
	O
	O
w	O
«	O
	O
ç	O
96	O
neural	O
networks	O
>	O
b\dc®	O
>	O
b\d	O
>	O
b\^db¯	O
6.2.4	O
­¬	O
u	O
c1u	O
transpose	O
,	O
can	O
be	O
applied	O
to	O
(	O
6.22	O
)	O
to	O
show	O
that	O
the	O
pseudo-inverse	O
method	O
gives	O
the	O
same	O
result	O
as	O
(	O
6.20	O
)	O
:	O
this	O
formula	O
is	O
applied	O
directly	O
.	O
the	O
identityu	O
that	O
an	O
exact	O
expression	O
exists	O
for	O
updating	O
the	O
inverse	O
correlationª	O
improving	O
the	O
generalisation	O
of	O
feed-forward	O
networks	O
the	O
requirement	O
to	O
invert	O
or	O
pseudo-invert	O
a	O
matrix	O
dependent	O
on	O
the	O
entire	O
dataset	O
makes	O
this	O
a	O
batch	O
method	O
.	O
however	O
an	O
online	O
variant	O
is	O
possible	O
,	O
known	O
as	O
kalman	O
filtering	O
(	O
scalero	O
&	O
tepedelenlioglu	O
,	O
1992	O
)	O
.	O
it	O
is	O
based	O
on	O
the	O
somewhat	O
remarkable	O
fact	O
if	O
another	O
example	B
[	O
ch	O
.	O
6	O
,	O
where¬	O
denotes	O
the	O
matrix	O
þ°	O
(	O
6.23	O
)	O
is	O
added	O
to	O
the	O
sum	O
(	O
6.19	O
)	O
,	O
which	O
does	O
not	O
require	O
recomputation	O
of	O
the	O
inverse	O
.	O
constructive	O
algorithms	O
and	O
pruning	B
a	O
number	O
of	O
techniques	O
have	O
emerged	O
recently	O
,	O
which	O
attempt	O
to	O
improve	O
on	O
the	O
perceptron	O
and	O
multilayer	O
perceptron	O
training	O
algorithms	O
by	O
changing	O
the	O
architecture	O
of	O
the	O
networks	O
as	O
training	O
proceeds	O
.	O
these	O
techniques	O
include	O
pruning	B
useless	O
nodes	O
or	O
weights	O
,	O
and	O
constructive	O
algorithms	O
where	O
extra	O
nodes	O
are	O
added	O
as	O
required	O
.	O
the	O
advantages	O
include	O
smaller	O
networks	O
,	O
faster	O
training	O
times	O
on	O
serial	O
computers	O
,	O
and	O
increased	O
generalisation	O
ability	O
,	O
with	O
a	O
consequent	O
immunity	O
to	O
noise	O
.	O
in	O
addition	O
,	O
it	O
is	O
frequently	O
much	O
easier	O
to	O
interpret	O
what	O
the	O
trained	O
network	O
is	O
doing	O
.	O
as	O
was	O
noted	O
earlier	O
,	O
a	O
minimalist	O
network	O
uses	O
its	O
hidden	B
layer	O
to	O
model	O
as	O
much	O
of	O
the	O
problem	O
as	O
possible	O
in	O
the	O
limited	O
number	O
of	O
degrees	O
of	O
freedom	O
available	O
in	O
its	O
hidden	B
layer	O
.	O
with	O
such	O
a	O
network	O
,	O
one	O
can	O
then	O
begin	O
to	O
draw	O
analogies	O
with	O
other	O
pattern	O
classifying	O
techniques	O
such	O
as	O
decision	O
trees	O
and	O
expert	O
systems	O
.	O
to	O
make	O
a	O
network	O
with	O
good	O
generalisation	O
ability	O
,	O
we	O
must	O
determine	O
a	O
suitable	O
number	O
of	O
hidden	B
nodes	O
.	O
if	O
there	O
are	O
too	O
few	O
,	O
the	O
network	O
may	O
not	O
learn	O
at	O
all	O
,	O
while	O
too	O
many	O
hidden	B
nodes	O
lead	O
to	O
over-learning	O
of	O
individual	O
samples	O
at	O
the	O
expense	O
of	O
forming	O
a	O
near	O
optimal	O
model	O
of	O
the	O
data	O
distributions	O
underlying	O
the	O
training	O
data	O
.	O
in	O
this	O
case	O
,	O
previously	O
unseen	O
patterns	O
are	O
labeled	O
according	O
to	O
the	O
nearest	O
neighbour	O
,	O
rather	O
than	O
in	O
accordance	O
with	O
a	O
good	O
model	O
of	O
the	O
problem	O
.	O
an	O
easy	O
to	O
read	O
introduction	O
to	O
the	O
issues	O
invloved	O
in	O
over-training	O
a	O
network	O
can	O
be	O
found	O
in	O
geman	O
(	O
1992	O
)	O
.	O
early	O
constructive	O
algorithms	O
such	O
as	O
upstart	O
(	O
frean	O
,	O
1990a	O
,	O
1990b	O
)	O
and	O
the	O
tiling	O
algorithm	O
(	O
m´ezard	O
&	O
nadal	O
,	O
1989	O
)	O
built	O
multi-layer	O
feed-forward	O
networks	O
of	O
perceptron	O
units	O
(	O
rosenblatt	O
,	O
1958	O
)	O
,	O
which	O
could	O
be	O
applied	O
to	O
problems	O
involving	O
binary	O
input	B
patterns	O
.	O
convergence	O
of	O
such	O
algorithms	O
is	O
guaranteed	O
if	O
the	O
data	O
is	O
linearly	O
separable	O
,	O
and	O
use	O
of	O
the	O
pocket	O
algorithm	O
(	O
gallant	O
,	O
1985	O
)	O
for	O
training	O
allows	O
an	O
approximate	O
solution	O
to	O
be	O
found	O
for	O
non	O
linearly-separable	O
datasets	O
.	O
these	O
networks	O
do	O
not	O
usually	O
include	O
a	O
stopping	O
criterion	O
to	O
halt	O
the	O
creation	O
of	O
new	O
layers	O
or	O
nodes	O
,	O
so	O
every	O
sample	O
in	O
the	O
training	O
data	O
is	O
learned	O
.	O
this	O
has	O
strong	O
repercussions	O
if	O
the	O
training	O
set	O
is	O
incomplete	O
,	O
has	O
noise	O
,	O
or	O
is	O
derived	O
from	O
a	O
classiﬁcation	B
problem	O
where	O
the	O
distributions	O
overlap	O
.	O
later	O
methods	O
apply	O
to	O
more	O
general	O
problems	O
and	O
are	O
suitable	O
for	O
statistical	B
classiﬁca-	O
tion	B
problems	O
(	O
ash	O
,	O
1989	O
;	O
fahlman	O
&	O
lebi`ere	O
,	O
1990	O
;	O
hanson	O
,	O
1990	O
;	O
refenes	O
&	O
vithlani	O
,	O
1991	O
,	O
and	O
wynne-jones	O
,	O
1992	O
,	O
1993	O
)	O
.	O
they	O
often	O
build	O
a	O
single	O
hidden	B
layer	O
,	O
and	O
incorpo-	O
rate	O
stopping	O
criteria	O
which	O
allow	O
them	O
to	O
converge	O
to	O
solutions	O
with	O
good	O
generalisation	O
ç	O
¬	O
u	O
g	O
ç	O
w	O
«	O
	O
u	O
¬	O
v	O
v	O
¬	O
v	O
ç	O
sec	O
.	O
6.2	O
]	O
supervised	O
networks	O
for	O
classiﬁcation	B
97	O
ability	O
for	O
statistical	B
problems	O
.	O
cascade	O
correlation	O
(	O
fahlman	O
&	O
lebi`ere	O
,	O
1990	O
)	O
is	O
an	O
example	B
of	O
such	O
a	O
network	O
algorithm	O
,	O
and	O
is	O
described	O
below	O
.	O
pruning	B
has	O
been	O
carried	O
out	O
on	O
networks	O
in	O
three	O
ways	O
.	O
the	O
ﬁrst	O
is	O
a	O
heuristic	O
approach	O
based	O
on	O
identifying	O
which	O
nodes	O
or	O
weights	O
contribute	O
little	O
to	O
the	O
mapping	O
.	O
after	O
these	O
have	O
been	O
removed	O
,	O
additional	O
training	O
leads	O
to	O
a	O
better	O
network	O
than	O
the	O
original	O
.	O
an	O
alternative	O
technique	O
is	O
to	O
include	O
terms	O
in	O
the	O
error	O
function	O
,	O
so	O
that	O
weights	O
tend	O
to	O
zero	O
under	O
certain	O
circumstances	O
.	O
zero	O
weights	O
can	O
then	O
be	O
removed	O
without	O
degrading	O
the	O
network	O
performance	O
.	O
this	O
approach	O
is	O
the	O
basis	O
of	O
regularisation	O
,	O
discussed	O
in	O
more	O
detail	O
below	O
.	O
finally	O
,	O
if	O
we	O
deﬁne	O
the	O
sensitivity	O
of	O
the	O
global	O
network	O
error	O
to	O
the	O
removal	O
of	O
a	O
weight	O
or	O
node	O
,	O
we	O
can	O
remove	O
the	O
weights	O
or	O
nodes	O
to	O
which	O
the	O
global	O
error	O
is	O
least	O
sensitive	O
.	O
the	O
sensitivity	O
measure	B
does	O
not	O
interfere	O
with	O
training	O
,	O
and	O
involves	O
only	O
a	O
small	O
amount	O
of	O
extra	O
computational	O
effort	O
.	O
a	O
full	O
review	O
of	O
these	O
techniques	O
can	O
be	O
found	O
in	O
wynne-jones	O
(	O
1991	O
)	O
.	O
cascade	O
correlation	O
:	O
a	O
constructive	O
feed-forward	O
network	O
cascade	O
correlation	O
is	O
a	O
paradigm	O
for	O
building	O
a	O
feed-forward	O
network	O
as	O
training	O
proceeds	O
in	O
a	O
supervised	O
mode	O
(	O
fahlman	O
&	O
lebi`ere	O
,	O
1990	O
)	O
.	O
instead	O
of	O
adjusting	O
the	O
weights	O
in	O
a	O
ﬁxed	O
architecture	O
,	O
it	O
begins	O
with	O
a	O
small	O
network	O
,	O
and	O
adds	O
new	O
hidden	B
nodes	O
one	O
by	O
one	O
,	O
creating	O
a	O
multi-layer	O
structure	O
.	O
once	O
a	O
hidden	B
node	O
has	O
been	O
added	O
to	O
a	O
network	O
,	O
its	O
input-side	O
weights	O
are	O
frozen	O
and	O
it	O
becomes	O
a	O
permanent	O
feature-detector	O
in	O
the	O
network	O
,	O
available	O
for	O
output	B
or	O
for	O
creating	O
other	O
,	O
more	O
complex	O
feature	O
detectors	O
in	O
later	O
layers	O
.	O
cascade	O
correlation	O
can	O
offer	O
reduced	O
training	O
time	O
,	O
and	O
it	O
determines	O
the	O
size	O
and	O
topology	O
of	O
networks	O
automatically	O
.	O
cascade	O
correlation	O
combines	O
two	O
ideas	O
:	O
ﬁrst	O
the	O
cascade	O
architecture	O
,	O
in	O
which	O
hidden	B
nodes	O
are	O
added	O
one	O
at	O
a	O
time	O
,	O
each	O
using	O
the	O
outputs	O
of	O
all	O
others	O
in	O
addition	O
to	O
the	O
input	B
nodes	O
,	O
and	O
second	O
the	O
maximisation	O
of	O
the	O
correlation	O
between	O
a	O
new	O
unit	O
’	O
s	O
output	B
and	O
the	O
residual	O
classiﬁcation	B
error	O
of	O
the	O
parent	O
network	O
.	O
each	O
node	O
added	O
to	O
the	O
network	O
may	O
be	O
of	O
any	O
kind	O
.	O
examples	O
include	O
linear	O
nodes	O
which	O
can	O
be	O
trained	O
using	O
linear	O
algorithms	O
,	O
threshold	O
nodes	O
such	O
as	O
single	O
perceptrons	O
where	O
simple	O
learning	O
rules	O
such	O
as	O
the	O
delta	O
rule	O
or	O
the	O
pocket	O
algorithm	O
can	O
be	O
used	O
,	O
or	O
non-linear	O
nodes	O
such	O
as	O
sigmoids	O
or	O
gaussian	O
functions	O
requiring	O
delta	O
rules	O
or	O
more	O
advanced	O
algorithms	O
such	O
as	O
fahlman	O
’	O
s	O
quickprop	O
(	O
fahlman	O
,	O
1988a	O
,	O
1988b	O
)	O
.	O
standard	O
mlp	O
sigmoids	O
were	O
used	O
in	O
the	O
statlog	O
trials	O
.	O
at	O
each	O
stage	O
in	O
training	O
,	O
each	O
node	O
in	O
a	O
pool	O
of	O
candidate	O
nodes	O
is	O
trained	O
on	O
the	O
residual	O
error	O
of	O
the	O
parent	O
network	O
.	O
of	O
these	O
nodes	O
,	O
the	O
one	O
whose	O
output	B
has	O
the	O
greatest	O
correlation	O
with	O
the	O
error	O
of	O
the	O
parent	O
is	O
added	O
permanently	O
to	O
the	O
network	O
.	O
the	O
error	O
,	O
the	O
sum	O
over	O
all	O
output	B
units	O
of	O
the	O
magnitude	O
of	O
,	O
the	O
candidate	O
unit	O
’	O
s	O
value	O
,	O
the	O
correlation	O
(	O
or	O
,	O
more	O
precisely	O
,	O
the	O
covariance	O
)	O
between±	O
function	O
minimised	O
in	O
this	O
scheme	O
isä	O
for	O
example	O
.ä	O
é	O
,	O
the	O
residual	O
error	O
observed	O
at	O
output	B
unitê	O
andµ	O
î	O
c3±	O
cµ	O
î	O
the	O
quantities±	O
é	O
are	O
the	O
values	O
of±	O
andµ	O
é	O
averaged	O
over	O
all	O
patterns	O
.	O
andµ	O
each	O
of	O
the	O
weights	O
coming	O
into	O
the	O
node	O
,	O
i	O
in	O
order	O
to	O
maximiseä	O
	O
.	O
thus	O
:	O
is	O
deﬁned	O
by	O
:	O
,	O
the	O
partial	O
derivative	O
of	O
the	O
error	O
is	O
calculated	O
with	O
respect	O
to	O
ä	O
	O
à	O
é	O
	O
	O
	O
	O
	O
à	O
	O
	O
	O
±	O
g	O
é	O
	O
µ	O
é	O
g	O
	O
	O
	O
	O
	O
98	O
neural	O
networks	O
[	O
ch	O
.	O
6	O
(	O
6.24	O
)	O
µ	O
}	O
é	O
é+	O
édcµýî	O
î	O
is	O
the	O
sign	O
of	O
the	O
correlation	O
between	O
the	O
candidate	O
’	O
s	O
value	O
and	O
the	O
outputê	O
,	O
ë	O
whereå	O
is	O
the	O
derivative	O
for	O
pattern	O
of	O
the	O
candidate	O
unit	O
’	O
s	O
activation	O
function	O
withe	O
respect	O
to	O
the	O
is	O
the	O
input	B
the	O
candidate	O
unit	O
receives	O
for	O
pattern	O
.	O
sum	O
of	O
its	O
inputs	O
,	O
andý	O
the	O
partial	O
derivatives	O
are	O
used	O
to	O
perform	O
gradient	O
ascent	O
to	O
maximiseä	O
.	O
whenä	O
no	O
longer	O
improves	O
in	O
training	O
for	O
any	O
of	O
the	O
candidate	O
nodes	O
,	O
the	O
best	O
candidate	O
is	O
added	O
to	O
the	O
network	O
,	O
and	O
the	O
others	O
are	O
scrapped	O
.	O
in	O
benchmarks	O
on	O
a	O
toy	O
problem	O
involving	O
classiﬁcation	B
of	O
data	O
points	O
forming	O
two	O
interlocked	O
spirals	O
,	O
cascade	O
correlation	O
is	O
reported	O
to	O
be	O
ten	O
to	O
one	O
hundred	O
times	O
faster	O
than	O
conventional	O
back-propagation	O
of	O
error	O
derivatives	O
in	O
a	O
ﬁxed	O
architecture	O
network	O
.	O
empirical	O
tests	O
on	O
a	O
range	O
of	O
real	O
problems	O
(	O
yang	O
&	O
honavar	O
,	O
1991	O
)	O
indicate	O
a	O
speedup	O
of	O
one	O
to	O
two	O
orders	O
of	O
magnitude	O
with	O
minimal	O
degradation	O
of	O
classiﬁcation	B
accuracy	O
.	O
these	O
results	O
were	O
only	O
obtained	O
after	O
many	O
experiments	O
to	O
determine	O
suitable	O
values	O
for	O
the	O
many	O
parameters	O
which	O
need	O
to	O
be	O
set	O
in	O
the	O
cascade	O
correlation	O
implementation	O
.	O
cascade	O
correlation	O
can	O
also	O
be	O
implemented	O
in	O
computers	O
with	O
limited	O
precision	O
(	O
fahlman	O
,	O
1991b	O
)	O
,	O
and	O
in	O
recurrent	O
networks	O
(	O
hoehfeld	O
&	O
fahlman	O
,	O
1991	O
)	O
.	O
bayesian	O
regularisation	O
in	O
recent	O
years	O
the	O
formalism	O
of	O
bayesian	O
probability	O
theory	O
has	O
been	O
applied	O
to	O
the	O
treatment	O
of	O
feedforward	O
neural	O
network	O
models	O
as	O
nonlinear	O
regression	O
problems	O
.	O
this	O
has	O
brought	O
about	O
a	O
greatly	O
improved	O
understanding	O
of	O
the	O
generalisation	O
problem	O
,	O
and	O
some	O
new	O
techniques	O
to	O
improve	O
generalisation	O
.	O
none	O
of	O
these	O
techniques	O
were	O
used	O
in	O
the	O
numerical	O
experiments	O
described	O
in	O
this	O
book	O
,	O
but	O
a	O
short	O
introduction	O
to	O
this	O
subject	O
is	O
provided	O
here	O
.	O
(	O
the	O
latter	O
technique	O
is	O
marginalisation	O
(	O
mackay	O
,	O
1992a	O
)	O
.	O
)	O
a	O
reasonable	O
scenario	O
for	O
a	O
bayesian	O
treatment	O
of	O
feedforward	O
neural	O
networks	O
is	O
to	O
through	O
some	O
network	O
and	O
corrupting	O
the	O
output	B
with	O
noise	O
from	O
a	O
stationary	O
source	O
.	O
the	O
network	O
involved	O
is	O
assumed	O
to	O
have	O
been	O
drawn	O
from	O
a	O
probability	O
in	O
this	O
distribution	O
can	O
presume	O
that	O
each	O
target	O
training	O
data	O
vectoru	O
was	O
produced	O
by	O
running	O
the	O
corresponding	O
input	B
training	O
vectort	O
g	O
,	O
which	O
is	O
to	O
be	O
estimated	O
.	O
the	O
most	O
probablew	O
distributions0c1w	O
be	O
used	O
as	O
the	O
optimal	O
classiﬁer	B
,	O
or	O
a	O
more	O
sophisticated	O
average	O
overs0cbw	O
g	O
can	O
be	O
used	O
.	O
at	O
a	O
particular	O
point	O
;	O
for	O
example	B
,	O
s	O
g	O
would	O
designate	O
this	O
density	O
at	O
the	O
particular	O
pointw	O
ands	O
c1w	O
unsigniﬁcantly	O
has	O
the	O
same	O
name	O
as	O
the	O
label	O
index	O
ofs	O
g	O
whens	O
furthermore	O
adopt	O
the	O
common	O
practice	O
of	O
writings0c1w	O
the	O
notation	O
used	O
here	O
for	O
probability	O
densities	O
is	O
somewhat	O
cavalier	O
.	O
in	O
discussions	O
involving	O
several	O
probability	O
density	O
functions	O
,	O
the	O
notation	O
should	O
distinguish	O
one	O
density	O
function	O
from	O
another	O
,	O
and	O
further	O
notation	O
should	O
be	O
used	O
when	O
such	O
a	O
density	O
is	O
indicated	O
can	O
designate	O
the	O
density	O
function	O
over	O
weights	O
,	O
,	O
which	O
confusingly	O
and	O
.	O
however	O
,	O
a	O
tempting	O
opportunity	O
to	O
choose	O
names	O
which	O
introduce	O
this	O
confusion	O
will	O
arise	O
in	O
almost	O
every	O
instance	O
that	O
a	O
density	O
function	O
is	O
mentioned	O
,	O
so	O
we	O
shall	O
not	O
only	O
succumb	O
to	O
the	O
temptation	O
,	O
but	O
is	O
meant	O
,	O
in	O
order	O
to	O
be	O
concise	O
.	O
technically	O
,	O
this	O
is	O
an	O
appalling	O
case	O
of	O
using	O
a	O
function	O
argument	O
name	O
(	O
which	O
is	O
ordinarily	O
arbitrary	O
)	O
to	O
designate	O
the	O
function	O
.	O
the	O
bayesian	O
analysis	O
is	O
built	O
on	O
a	O
probabilistic	O
interpretation	O
of	O
the	O
error	O
measure	B
used	O
in	O
training	O
.	O
typically	O
,	O
as	O
in	O
equations	O
(	O
6.6	O
)	O
or	O
(	O
6.9	O
)	O
,	O
it	O
is	O
additive	O
over	O
input-output	O
pairs	O
c1w	O
²	O
ä	O
²	O
i	O
	O
	O
à	O
é	O
å	O
g	O
ë	O
y	O
	O
ý	O
	O
î	O
	O
é	O
y	O
	O
	O
î	O
	O
w	O
w	O
w	O
g	O
sec	O
.	O
6.2	O
]	O
supervised	O
networks	O
for	O
classiﬁcation	B
99	O
;	O
i.e	O
.	O
it	O
can	O
be	O
expressed	O
as	O
an	O
integral	O
over	O
all	O
possible	O
target	O
training	O
data	O
sets	O
of	O
the	O
size	O
under	O
consideration	O
.	O
is	O
all	O
the	O
training	O
data	O
,	O
the	O
set	O
of	O
input-output	O
pairs	O
in	O
the	O
,	O
,	O
drawn	O
from	O
a	O
distribution	O
with	O
correspond	O
to	O
different	O
probabilistic	O
interpretations	O
.	O
given	O
this	O
assumption	O
,	O
and	O
the	O
assumption	O
that	O
training	O
data	O
samples	O
are	O
produced	O
independently	O
of	O
each	O
other	O
,	O
c1u0ê	O
;	O
w	O
;	O
w	O
µ0c1u	O
for	O
some	O
function	O
,	O
whereu	O
is	O
composed	O
of	O
all	O
the	O
input	B
datat	O
sum.u	O
,	O
regarded	O
as	O
ﬁxed	O
,	O
and	O
all	O
the	O
target	O
datau	O
-dependent	O
function	O
oft	O
regarded	O
as	O
a	O
noise-corrupted	O
,	O
w	O
g	O
(	O
or	O
technicallys0cbu	O
density	O
functions0c1u	O
assumption	O
thats0c1u	O
g	O
.	O
the	O
bayesian	O
argument	O
requires	O
the	O
alone	O
.	O
thus	O
,	O
different	O
choices	O
ofµ	O
is	O
a	O
function	O
ofµ	O
s0c1u	O
	O
;	O
w	O
s0cº.u	O
|	O
;	O
w	O
s0c1u	O
the	O
relationship	O
betweenµ0c1u	O
g	O
ands	O
c1u	O
;	O
w	O
g	O
can	O
only	O
have	O
the	O
form	O
þµ´	O
;	O
w	O
s0c1u	O
.³	O
for	O
some	O
parameterõ	O
þµ´	O
;	O
w	O
un	O
if	O
in	O
(	O
6.25	O
)	O
is	O
a	O
function	O
only	O
ofvê'	O
(	O
u8ê	O
,	O
as	O
is	O
(	O
6.6	O
)	O
,	O
then³	O
ofw	O
,	O
a	O
result	O
which	O
is	O
useful	O
later	O
.	O
the	O
only	O
common	O
form	O
of	O
which	O
does	O
not	O
have	O
which	O
case	O
(	O
6.9	O
)	O
and	O
(	O
6.8	O
)	O
together	O
justify	O
the	O
assumption	O
thats	O
c1u	O
f	O
,	O
so³	O
f	O
and³	O
;	O
w	O
µ0cbu	O
is	O
still	O
independent	O
ofw	O
g	O
depends	O
only	O
on	O
g	O
and	O
imply	O
for	O
(	O
6.28	O
)	O
thatõ	O
the	O
probability	O
of	O
the	O
weights	O
given	O
the	O
datas0cbw	O
probability	O
of	O
the	O
data	O
given	O
the	O
weightss0c1u	O
g	O
(	O
the	O
likelihood	O
)	O
,	O
but	O
unfortunately	O
the	O
cbw	O
s0c1w	O
can	O
be	O
used	O
to	O
converts	O
g	O
from	O
equation	O
(	O
6.27	O
)	O
,	O
and	O
a	O
prior	O
over	O
the	O
weightss	O
c1w	O
into	O
the	O
desired	O
distribution	O
.	O
the	O
probability	O
of	O
the	O
datas0c1u	O
g	O
,	O
g	O
is	O
given	O
by	O
the	O
normalisation	O
c1w	O
s0c1u	O
	O
there	O
is	O
a	O
further	O
technicality	O
;	O
the	O
integral	O
(	O
6.28	O
)	O
over	O
target	O
data	O
must	O
be	O
with	O
respect	O
to	O
uniform	B
measure	O
,	O
density	O
(	O
6.27	O
)	O
can	O
also	O
be	O
derived	O
from	O
somewhat	O
different	O
assumptions	O
using	O
a	O
maximum-entropy	O
argument	O
(	O
bilbro	O
&	O
van	O
den	O
bout	O
,	O
1992	O
)	O
.	O
it	O
plays	O
a	O
prominent	O
role	O
in	O
thermodynamics	O
,	O
and	O
thermodynamics	O
jargon	O
has	O
drifted	O
into	O
the	O
neural	O
networks	O
liter-	O
ature	O
partly	O
in	O
consequence	O
of	O
the	O
analogies	O
it	O
underlies	O
.	O
c1u	O
s0c1u	O
c1u	O
ws0cbu	O
this	O
form	O
is	O
the	O
cross-entropy	O
(	O
6.9	O
)	O
.	O
but	O
this	O
is	O
normally	O
used	O
in	O
classiﬁcation	B
problems	O
,	O
in	O
additivity	O
argument	O
does	O
not	O
go	O
through	O
for	O
this	O
.	O
instead	O
,	O
bayes	O
’	O
rule	O
(	O
6.25	O
)	O
(	O
6.26	O
)	O
(	O
6.27	O
)	O
(	O
6.28	O
)	O
.	O
(	O
6.29	O
)	O
(	O
6.30	O
)	O
condition	O
as	O
which	O
may	O
not	O
always	O
be	O
reasonable	O
.	O
is	O
the	O
normalisation	O
term	O
turns	O
out	O
to	O
be	O
independent	O
is	O
of	O
greater	O
interest	O
than	O
the	O
u	O
g	O
	O
à	O
ê	O
g	O
¼	O
w	O
¼	O
w	O
~	O
t	O
¼	O
w	O
g	O
|	O
~	O
u	O
	O
¾	O
¼	O
w	O
g	O
	O
g	O
g	O
¼	O
w	O
¼	O
w	O
g	O
	O
f	O
³	O
w	O
	O
	O
y	O
u	O
z	O
w	O
³	O
w	O
	O
t	O
	O
	O
y	O
u	O
z	O
~	O
w	O
¼	O
w	O
	O
w	O
	O
w	O
¼	O
u	O
g	O
¼	O
w	O
¼	O
u	O
g	O
	O
s	O
¼	O
w	O
g	O
s	O
å	O
g	O
g	O
¼	O
w	O
å	O
g	O
	O
t	O
	O
¼	O
w	O
g	O
s	O
å	O
g	O
100	O
neural	O
networks	O
w·	O
.	O
this	O
ensures	O
that	O
the	O
denominator	O
of	O
(	O
6.31	O
)	O
(	O
u	O
;	O
w	O
)	O
e	O
[	O
ch	O
.	O
6	O
g	O
must	O
express	O
the	O
þèâ	O
g	O
.	O
is	O
given	O
by	O
normalisation	O
.	O
assembling	O
all	O
the	O
pieces	O
,	O
the	O
posterior	O
probability	O
of	O
the	O
weights	O
given	O
the	O
data	O
is	O
c1w	O
in	O
this	O
case.s	O
g	O
.	O
c1w	O
;	O
w	O
the	O
bayesian	O
method	O
helps	O
with	O
one	O
of	O
the	O
most	O
troublesome	O
steps	O
in	O
the	O
regularisa-	O
tion	B
approach	O
to	O
obtaining	O
good	O
generalisation	O
,	O
deciding	O
the	O
values	O
of	O
the	O
regularisation	O
is	O
additive	O
over	O
the	O
weights	O
and	O
an	O
indepen-	O
dence	O
assumption	O
like	O
(	O
6.26	O
)	O
is	O
reasonable	O
,	O
so	O
given	O
that	O
the	O
prior	O
depends	O
only	O
on	O
the	O
regularisation	O
term	O
,	O
then	O
it	O
has	O
the	O
form	O
notion	O
that	O
some	O
weight	O
matrices	O
are	O
more	O
reasonable	O
,	O
a	O
priori	O
,	O
than	O
others	O
.	O
as	O
discussed	O
above	O
,	O
this	O
is	O
normally	O
expressed	O
through	O
regularisation	O
terms	O
added	O
to	O
the	O
error	O
measure	B
.	O
for	O
example	B
,	O
the	O
view	O
that	O
large	O
weights	O
are	O
unreasonable	O
might	O
be	O
expressed	O
by	O
adding	O
a	O
bayesian	O
methods	O
inevitably	O
require	O
a	O
prior	O
,	O
s	O
toµ0c1u	O
“	O
weight	O
decay	O
”	O
term	O
of	O
the	O
formáwøã-w	O
typically	O
,	O
the	O
regularisation	O
errorábµ0c1w	O
c1w	O
where³	O
þµ´	O
;	O
w	O
s0c1w	O
þµ´	O
;	O
w·	O
þèâ	O
þâ	O
provided	O
that	O
(	O
6.28	O
)	O
does	O
not	O
depend	O
onw	O
does	O
not	O
depend	O
onw	O
µ0c1w	O
ﬁnds	O
the	O
maximum	O
ofs0c1w	O
,	O
so	O
the	O
usual	O
training	O
process	O
of	O
minimisingµ	O
hyõ	O
expresses	O
the	O
relative	O
importance	O
of	O
smoothing	O
and	O
data-ﬁtting	O
,	O
parameters	O
.	O
the	O
ratioá	O
s0c1u	O
s0cá	O
s0cbu	O
õjg	O
õjg	O
cá	O
if	O
a	O
uniform	B
priors	O
are	O
those	O
which	O
maximise	O
the	O
evidences0cbu	O
õjg	O
cá	O
opposes	O
the	O
goal	O
of	O
maximisings0cbw	O
weightsw	O
g	O
;	O
the	O
regularisation	O
parametersá	O
andõ	O
imising	O
(	O
6.30	O
)	O
s0c1u	O
g	O
,	O
one	O
attempts	O
to	O
ﬁnd	O
a	O
priors	O
w	O
ﬁt	O
the	O
datau	O
well	O
.	O
this	O
objective	O
is	O
not	O
diametrically	O
opposed	O
to	O
the	O
later	O
objective	O
of	O
g	O
under	O
which	O
“	O
usually	O
”	O
networks	O
selecting	O
the	O
best-ﬁttingw	O
.	O
indeed	O
,	O
the	O
distributions	O
is	O
one	O
which	O
is	O
concentrated	O
on	O
a	O
single	O
overﬁtw	O
g	O
which	O
maximises	O
the	O
evidence	O
õjg	O
,	O
which	O
is	O
given	O
by	O
(	O
6.30	O
)	O
,	O
the	O
denom-	O
cbw	O
.	O
this	O
is	O
prevented	O
only	O
if	O
the	O
the	O
distri-	O
bution	O
of	O
weight	O
matrices	O
parameterised	O
by	O
the	O
regularisation	O
parameters	O
does	O
not	O
include	O
such	O
highly	O
concentrated	O
distributions	O
.	O
therefore	O
it	O
remains	O
an	O
art	O
to	O
select	O
reasonable	O
functional	O
forms	O
for	O
the	O
regularisers	O
,	O
but	O
once	O
selected	O
,	O
the	O
determination	O
of	O
the	O
parameters	O
which	O
deserves	O
to	O
be	O
decided	O
in	O
a	O
principled	O
manner	O
.	O
the	O
bayesian	O
evidence	O
formalism	O
provides	O
a	O
principle	O
and	O
an	O
implementation	O
.	O
it	O
can	O
be	O
computationally	O
demanding	O
if	O
used	O
precisely	O
,	O
but	O
there	O
are	O
practicable	O
approximations	O
.	O
the	O
evidence	O
formalism	O
simply	O
assumes	O
a	O
prior	O
distribution	O
over	O
the	O
regularisation	O
inator	O
of	O
(	O
6.29	O
)	O
.	O
note	O
with	O
reference	O
to	O
(	O
6.29	O
)	O
that	O
the	O
goal	O
of	O
maximising	O
the	O
evidence	O
,	O
and	O
the	O
are	O
optimised	O
for	O
opposing	O
purposes	O
.	O
this	O
expresses	O
the	O
bayesian	O
quantiﬁcation	O
this	O
method	O
of	O
setting	O
regularisation	O
parameters	O
does	O
not	O
provide	O
a	O
guarantee	O
against	O
overﬁtting	O
(	O
wolpert	O
,	O
1992	O
)	O
,	O
but	O
it	O
helps	O
.	O
in	O
setting	O
the	O
regularisation	O
parameters	O
by	O
max-	O
(	O
6.31	O
)	O
(	O
6.32	O
)	O
parameters	O
,	O
and	O
sharpens	O
it	O
using	O
bayes	O
’	O
rule	O
:	O
is	O
assumed	O
,	O
then	O
the	O
most	O
likely	O
regularisation	O
parameters	O
of	O
the	O
compromise	O
between	O
data	O
ﬁtting	O
and	O
smoothing	O
.	O
c1w	O
å	O
g	O
å	O
g	O
s	O
å	O
g	O
	O
f	O
³	O
å	O
	O
	O
>	O
w	O
d	O
å	O
¼	O
u	O
g	O
	O
	O
	O
y	O
u	O
z	O
	O
>	O
w	O
d	O
¶	O
	O
w	O
y	O
	O
	O
y	O
u	O
z	O
	O
>	O
d	O
â	O
´	O
g	O
¼	O
u	O
~	O
õ	O
¼	O
u	O
g	O
	O
¼	O
á	O
~	O
s	O
å	O
~	O
g	O
å	O
~	O
¼	O
á	O
~	O
¼	O
u	O
å	O
sec	O
.	O
6.3	O
]	O
unsupervised	O
learning	O
101	O
themselves	O
is	O
a	O
matter	O
of	O
calculation	O
.	O
the	O
art	O
of	O
selecting	O
regularisation	O
functions	O
has	O
become	O
an	O
interesting	O
research	O
area	O
(	O
nowlan	O
&	O
hinton	O
,	O
1992	O
)	O
.	O
the	O
calculation	O
of	O
(	O
6.32	O
)	O
involves	O
an	O
integration	O
which	O
is	O
generally	O
non-trivial	O
,	O
but	O
which	O
can	O
be	O
done	O
easily	O
in	O
a	O
gaussian	O
approximation	O
.	O
typically	O
this	O
is	O
good	O
enough	O
.	O
this	O
requires	O
computation	O
of	O
the	O
second	O
derivatives	O
of	O
the	O
error	O
measure	B
,	O
which	O
is	O
prohibitive	O
for	O
large	O
problems	O
,	O
but	O
in	O
this	O
case	O
a	O
further	O
approximation	O
is	O
possible	O
and	O
often	O
adequate	O
(	O
mackay	O
,	O
1992b	O
)	O
.	O
6.3	O
unsupervised	O
learning	O
interest	O
in	O
unsupervised	O
learning	O
has	O
increased	O
greatly	O
in	O
recent	O
years	O
.	O
it	O
offers	O
the	O
possi-	O
bility	O
of	O
exploring	O
the	O
structure	O
of	O
data	O
without	O
guidance	O
in	O
the	O
form	O
of	O
class	O
information	O
,	O
and	O
can	O
often	O
reveal	O
features	O
not	O
previously	O
expected	O
or	O
known	O
about	O
.	O
these	O
might	O
in-	O
clude	O
the	O
division	O
of	O
data	O
that	O
was	O
previously	O
thought	O
to	O
be	O
a	O
single	O
uniform	B
cluster	O
,	O
into	O
a	O
number	O
of	O
smaller	O
groups	O
,	O
each	O
with	O
separate	O
identiﬁable	O
properties	O
.	O
the	O
clusters	O
found	O
offer	O
a	O
model	O
of	O
the	O
data	O
in	O
terms	O
of	O
cluster	O
centres	O
,	O
sizes	O
and	O
shapes	O
,	O
which	O
can	O
often	O
be	O
described	O
using	O
less	O
information	O
,	O
and	O
in	O
fewer	O
parameters	O
than	O
were	O
required	O
to	O
store	O
the	O
entire	O
training	O
data	O
set	O
.	O
this	O
has	O
obvious	O
advantages	O
for	O
storing	O
,	O
coding	O
,	O
and	O
transmitting	O
stochastically	O
generated	O
data	O
;	O
if	O
its	O
distribution	O
in	O
the	O
attribute	O
space	O
is	O
known	O
,	O
equivalent	O
data	O
can	O
be	O
generated	O
from	O
the	O
model	O
when	O
required	O
.	O
while	O
general	O
,	O
unsupervised	O
learning	O
methods	O
such	O
as	O
boltzmann	O
machines	O
are	O
com-	O
putationally	O
expensive	O
,	O
iterative	O
clustering	O
algorithms	O
such	O
as	O
kohonen	O
networks	O
,	O
k-means	O
clustering	O
and	O
gaussian	O
mixture	O
models	O
offer	O
the	O
same	O
modelling	O
power	O
with	O
greatly	O
re-	O
duced	O
training	O
time	O
.	O
indeed	O
,	O
while	O
class	O
labels	O
are	O
not	O
used	O
to	O
constrain	O
the	O
structure	O
learned	O
by	O
the	O
models	O
,	O
freedom	O
from	O
this	O
constraint	O
coupled	O
with	O
careful	O
initialisation	O
of	O
the	O
models	O
using	O
any	O
prior	O
information	O
available	O
about	O
the	O
data	O
,	O
can	O
yield	O
very	O
quick	O
and	O
effective	O
models	O
.	O
these	O
models	O
,	O
known	O
collectively	O
as	O
vector	O
quantizers	O
,	O
can	O
be	O
used	O
as	O
the	O
non-linear	O
part	O
of	O
supervised	O
learning	O
models	O
.	O
in	O
this	O
case	O
a	O
linear	O
part	O
is	O
added	O
and	O
trained	O
later	O
to	O
implement	O
the	O
mapping	O
from	O
activation	O
in	O
different	O
parts	O
of	O
the	O
model	O
,	O
to	O
probable	O
classes	O
of	O
event	O
generating	O
the	O
data	O
.	O
6.3.1	O
the	O
k-means	O
clustering	O
algorithm	O
the	O
principle	O
of	O
clustering	O
requires	O
a	O
representation	O
of	O
a	O
set	O
of	O
data	O
to	O
be	O
found	O
which	O
offers	O
a	O
model	O
of	O
the	O
distribution	O
of	O
samples	O
in	O
the	O
attribute	O
space	O
.	O
the	O
k-means	O
algorithm	O
(	O
for	O
example	B
,	O
krishnaiah	O
&	O
kanal	O
,	O
1982	O
)	O
achieves	O
this	O
quickly	O
and	O
efﬁciently	O
as	O
a	O
model	O
with	O
a	O
ﬁxed	O
number	O
of	O
cluster	O
centres	O
,	O
determined	O
by	O
the	O
user	O
in	O
advance	O
.	O
the	O
cluster	O
centres	O
are	O
initially	O
chosen	O
from	O
the	O
data	O
,	O
and	O
each	O
centre	O
forms	O
the	O
code	O
vector	O
for	O
the	O
patch	O
of	O
the	O
input	B
space	O
in	O
which	O
all	O
points	O
are	O
closer	O
to	O
that	O
centre	O
than	O
to	O
any	O
other	O
.	O
this	O
division	O
of	O
the	O
space	O
into	O
patches	O
is	O
known	O
as	O
a	O
voronoi	O
tessellation	O
.	O
since	O
the	O
initial	O
allocation	O
of	O
centres	O
may	O
not	O
form	O
a	O
good	O
model	O
of	O
the	O
probability	O
distribution	O
function	O
(	O
pdf	O
)	O
of	O
the	O
input	B
space	O
,	O
there	O
follows	O
a	O
series	O
of	O
iterations	O
where	O
each	O
cluster	O
centre	O
is	O
moved	O
to	O
the	O
mean	O
position	O
of	O
all	O
the	O
training	O
patterns	O
in	O
its	O
tessellation	O
region	O
.	O
a	O
generalised	O
variant	O
of	O
the	O
k-means	O
algorithm	O
is	O
the	O
gaussian	O
mixture	O
model	O
,	O
or	O
adaptive	O
k-means	O
.	O
in	O
this	O
scheme	O
,	O
voronoi	O
tessellations	O
are	O
replaced	O
with	O
soft	O
transitions	O
from	O
one	O
centre	O
’	O
s	O
receptive	O
ﬁeld	O
to	O
another	O
’	O
s	O
.	O
this	O
is	O
achieved	O
by	O
assigning	O
a	O
variance	O
to	O
each	O
centre	O
,	O
thereby	O
deﬁning	O
a	O
gaussian	O
kernel	O
at	O
each	O
centre	O
.	O
these	O
kernels	O
are	O
mixed	O
102	O
neural	O
networks	O
[	O
ch	O
.	O
6	O
7	O
7	O
7	O
7	O
fig	O
.	O
6.4	O
:	O
k-means	O
clustering	O
:	O
within	O
each	O
patch	O
the	O
centre	O
is	O
moved	O
to	O
the	O
mean	O
position	O
of	O
the	O
patterns	O
.	O
together	O
by	O
a	O
set	O
of	O
mixing	O
weights	O
to	O
approximate	O
the	O
pdf	O
of	O
the	O
input	B
data	O
,	O
and	O
an	O
efﬁcient	O
algorithm	O
exists	O
to	O
calculate	O
iteratively	O
a	O
set	O
of	O
mixing	O
weights	O
,	O
centres	O
,	O
and	O
variances	O
for	O
the	O
centres	O
(	O
dubes	O
&	O
jain	O
,	O
1976	O
,	O
and	O
wu	O
&	O
chan	O
,	O
1991	O
)	O
.	O
while	O
the	O
number	O
of	O
centres	O
for	O
these	O
algorithms	O
is	O
ﬁxed	O
in	O
advance	O
in	O
more	O
popular	O
implementations	O
,	O
some	O
techniques	O
are	O
appearing	O
which	O
allow	O
new	O
centres	O
to	O
be	O
added	O
as	O
training	O
proceeds	O
.	O
(	O
wynne-jones	O
,	O
1992	O
and	O
1993	O
)	O
6.3.2	O
kohonen	O
networks	O
and	O
learning	O
vector	O
quantizers	O
kohonen	O
’	O
s	O
network	O
algorithm	O
(	O
kohonen	O
,	O
1984	O
)	O
also	O
provides	O
a	O
voronoi	O
tessellation	O
of	O
the	O
input	B
space	O
into	O
patches	O
with	O
corresponding	O
code	O
vectors	O
.	O
it	O
has	O
the	O
additional	O
feature	O
that	O
the	O
centres	O
are	O
arranged	O
in	O
a	O
low	O
dimensional	O
structure	O
(	O
usually	O
a	O
string	O
,	O
or	O
a	O
square	O
grid	O
)	O
,	O
such	O
that	O
nearby	O
points	O
in	O
the	O
topological	O
structure	O
(	O
the	O
string	O
or	O
grid	O
)	O
map	O
to	O
nearby	O
points	O
in	O
the	O
attribute	O
space	O
.	O
structures	O
of	O
this	O
kind	O
are	O
thought	O
to	O
occur	O
in	O
nature	O
,	O
for	O
example	B
in	O
the	O
mapping	O
from	O
the	O
ear	O
to	O
the	O
auditory	O
cortex	O
,	O
and	O
the	O
retinotopic	O
map	O
from	O
the	O
retina	O
to	O
the	O
visual	O
cortex	O
or	O
optic	O
tectum	O
.	O
in	O
training	O
,	O
the	O
winning	B
node	O
of	O
the	O
network	O
,	O
which	O
is	O
the	O
nearest	O
node	O
in	O
the	O
input	B
space	O
to	O
a	O
given	O
training	O
pattern	O
,	O
moves	O
towards	O
that	O
training	O
pattern	O
,	O
while	O
dragging	O
with	O
its	O
neighbouring	O
nodes	O
in	O
the	O
network	O
topology	O
.	O
this	O
leads	O
to	O
a	O
smooth	O
distribution	O
of	O
the	O
network	O
topology	O
in	O
a	O
non-linear	O
subspace	O
of	O
the	O
training	O
data	O
.	O
vector	O
quantizers	O
that	O
conserve	O
topographic	O
relations	O
between	O
centres	O
are	O
also	O
partic-	O
ularly	O
useful	O
in	O
communications	O
,	O
where	O
noise	O
added	O
to	O
the	O
coded	O
vectors	O
may	O
corrupt	O
the	O
representation	O
a	O
little	O
;	O
the	O
topographic	O
mapping	O
ensures	O
that	O
a	O
small	O
change	O
in	O
code	O
vector	O
is	O
decoded	O
as	O
a	O
small	O
change	O
in	O
attribute	O
space	O
,	O
and	O
hence	O
a	O
small	O
change	O
at	O
the	O
output	B
.	O
these	O
models	O
have	O
been	O
studied	O
extensively	O
,	O
and	O
recently	O
uniﬁed	O
under	O
the	O
framework	O
of	O
bayes	O
’	O
theory	O
(	O
luttrell	O
,	O
1990	O
,	O
1993	O
)	O
.	O
although	O
it	O
is	O
fundamentally	O
an	O
unsupervised	O
learning	O
algorithm	O
,	O
the	O
learning	O
vector	O
quantizer	O
can	O
be	O
used	O
as	O
a	O
supervised	O
vector	O
quantizer	O
,	O
where	O
network	O
nodes	O
have	O
class	O
labels	O
associated	O
with	O
them	O
.	O
the	O
kohonen	O
learning	O
rule	O
is	O
used	O
when	O
the	O
winning	B
node	O
represents	O
the	O
same	O
class	O
as	O
a	O
new	O
training	O
pattern	O
,	O
while	O
a	O
difference	O
in	O
class	O
between	O
sec	O
.	O
6.4	O
]	O
dipol92	O
103	O
the	O
winning	B
node	O
and	O
a	O
training	O
pattern	O
causes	O
the	O
node	O
to	O
move	O
away	O
from	O
the	O
training	O
pattern	O
by	O
the	O
same	O
distance	O
.	O
learning	O
vector	O
quantizers	O
are	O
reported	O
to	O
give	O
excellent	O
performance	O
in	O
studies	O
on	O
statistical	B
and	O
speech	O
data	O
(	O
kohonen	O
et	O
al.	O
,	O
1988	O
)	O
.	O
argmax	O
6.3.3	O
ramnets	O
one	O
of	O
the	O
oldest	O
practical	O
neurally-inspired	O
classiﬁcation	B
algorithms	O
is	O
still	O
one	O
of	O
the	O
best	O
.	O
it	O
is	O
the	O
n-tuple	O
recognition	O
method	O
introduced	O
by	O
bledsoe	O
&	O
browning	O
(	O
1959	O
)	O
and	O
bledsoe	O
(	O
1961	O
)	O
,	O
which	O
later	O
formed	O
the	O
basis	O
of	O
a	O
commercial	O
product	O
known	O
as	O
wisard	O
(	O
aleksander	O
et	O
al.	O
,	O
1984	O
)	O
.	O
the	O
algorithm	O
is	O
simple	O
.	O
the	O
patterns	O
to	O
be	O
classiﬁed	O
are	O
bit	O
these	O
are	O
the	O
n-tuples	O
.	O
the	O
restriction	O
of	O
a	O
pattern	O
to	O
an	O
n-tuple	O
can	O
be	O
regarded	O
as	O
an	O
n-bit	O
number	O
which	O
constitutes	O
a	O
‘	O
feature	O
’	O
of	O
the	O
pattern	O
.	O
a	O
pattern	O
is	O
classiﬁed	O
as	O
belonging	O
to	O
the	O
class	O
for	O
which	O
it	O
has	O
the	O
most	O
features	O
in	O
common	O
with	O
at	O
least	O
1	O
pattern	O
in	O
the	O
training	O
data	O
.	O
strings	O
of	O
a	O
given	O
length	O
.	O
several	O
(	O
let	O
us	O
say	O
)	O
sets	O
ofk	O
bit	O
locations	O
are	O
selected	O
randomly	O
.	O
to	O
be	O
precise	O
,	O
the	O
class	O
assigned	O
to	O
unclassiﬁed	O
patternú	O
xl¸	O
z	O
$	O
z	O
â½xb	O
>	O
a¾¿dbî	O
â½xb	O
>	O
a	O
:	O
d	O
á|¹	O
:	O
¢º7	O
»	O
¼	O
whereà	O
°	O
,	O
¹	O
for¹	O
is	O
the	O
set	O
of	O
training	O
patterns	O
in	O
classm	O
,	O
¹	O
is	O
the	O
kronecker	O
delta	O
(	O
°	O
,	O
c	O
«	O
	O
and	O
0	O
otherwise	O
.	O
)	O
andá	O
if	O
î	O
î	O
feature	O
of	O
patternú	O
:	O
úá	O
x	O
@	O
>	O
ßþ	O
á¦c	O
eá	O
th	O
bit	O
ofú	O
and~.icë	O
th	O
bit	O
of	O
the	O
hereú	O
is	O
the	O
is	O
the	O
classes	O
to	O
distinguish	O
,	O
the	O
system	O
can	O
be	O
implemented	O
as	O
a	O
set	O
of	O
with	O
{	O
th	O
ram	O
allocated	O
to	O
classm	O
íëâ	O
at	O
addressá	O
of	O
the	O
in	O
which	O
the	O
memory	O
content	O
â½x	O
@	O
>	O
a	O
:	O
d	O
:	O
¢º7	O
»	O
¼	O
íëâ	O
â	O
$	O
î	O
is	O
set	O
if	O
any	O
pattern	O
ofà	O
í	O
has	O
featureá	O
thus	O
and	O
unset	O
otherwise	O
.	O
recognition	O
is	O
accomplished	O
by	O
tallying	O
the	O
set	O
bits	O
in	O
the	O
rams	O
of	O
each	O
class	O
at	O
the	O
addresses	O
given	O
by	O
the	O
features	O
of	O
the	O
unclassiﬁed	O
pattern	O
.	O
ramnets	O
are	O
impressive	O
in	O
that	O
they	O
can	O
be	O
trained	O
faster	O
than	O
mlps	O
or	O
radial	O
basis	O
function	O
networks	O
by	O
orders	O
of	O
magnitude	O
,	O
and	O
often	O
provide	O
comparable	O
results	O
.	O
exper-	O
imental	O
comparisons	O
between	O
ramnets	O
and	O
other	O
methods	O
can	O
be	O
found	O
in	O
rohwer	O
&	O
cressy	O
(	O
1989	O
)	O
.	O
{	O
rams	O
,	O
is	O
(	O
6.35	O
)	O
th	O
n-tuple	O
.	O
íëâ	O
is	O
(	O
6.33	O
)	O
c	O
«	O
for	O
th	O
is	O
the	O
(	O
6.34	O
)	O
6.4	O
dipol92	O
this	O
is	O
something	O
of	O
a	O
hybrid	O
algorithm	O
,	O
which	O
has	O
much	O
in	O
common	O
with	O
both	O
logistic	O
discrimination	O
and	O
some	O
of	O
the	O
nonparametric	O
statistical	B
methods	O
.	O
however	O
,	O
for	O
historical	O
reasons	O
it	O
is	O
included	O
here	O
.	O
m	O
à	O
	O
x	O
à	O
í	O
g	O
	O
°	O
æ	O
g	O
	O
f	O
¹	O
ô	O
	O
	O
	O
f	O
	O
	O
c	O
ú	O
g	O
ú	O
g	O
	O
|	O
à	O
å	O
	O
d	O
³	O
	O
	O
g	O
	O
	O
¹	O
x	O
à	O
z	O
104	O
neural	O
networks	O
[	O
ch	O
.	O
6	O
introduction	O
6.4.1	O
dipol92	O
is	O
a	O
learning	O
algorithm	O
which	O
constructs	O
an	O
optimised	O
piecewise	O
linear	O
classiﬁer	B
by	O
a	O
two	O
step	O
procedure	O
.	O
in	O
the	O
ﬁrst	O
step	O
the	O
initial	O
positions	O
of	O
the	O
discriminating	O
hyper-	O
planes	O
are	O
determined	O
by	O
pairwise	O
linear	O
regression	O
.	O
to	O
optimise	O
these	O
positions	O
in	O
relation	O
to	O
the	O
misclassiﬁed	O
patterns	O
an	O
error	O
criterion	O
function	O
is	O
deﬁned	O
.	O
this	O
function	O
is	O
then	O
minimised	O
by	O
a	O
gradient	O
descent	O
procedure	O
for	O
each	O
hyperplane	O
separately	O
.	O
as	O
an	O
option	O
in	O
the	O
case	O
of	O
non–convex	O
classes	O
(	O
e.g	O
.	O
if	O
a	O
class	O
has	O
a	O
multimodal	O
probability	O
distribution	O
)	O
a	O
clustering	O
procedure	O
decomposing	O
the	O
classes	O
into	O
appropriate	O
subclasses	O
can	O
be	O
applied	O
.	O
(	O
in	O
this	O
case	O
dipol92	O
is	O
really	O
a	O
three	O
step	O
procedure	O
.	O
)	O
seen	O
from	O
a	O
more	O
general	O
point	O
of	O
view	O
dipol92	O
is	O
a	O
combination	O
of	O
a	O
statistical	B
part	O
(	O
regression	O
)	O
with	O
a	O
learning	O
procedure	O
typical	O
for	O
artiﬁcial	O
neural	O
nets	O
.	O
compared	O
with	O
most	O
neural	O
net	O
algorithms	O
an	O
advantage	O
of	O
dipol92	O
is	O
the	O
possibility	O
to	O
determine	O
the	O
number	O
and	O
initial	O
positions	O
of	O
the	O
discriminating	O
hyperplanes	O
(	O
corresponding	O
to	O
“	O
neurons	O
”	O
)	O
a	O
priori	O
,	O
i.e	O
.	O
before	O
learning	O
starts	O
.	O
using	O
the	O
clustering	O
procedure	O
this	O
is	O
true	O
even	O
in	O
the	O
case	O
that	O
a	O
class	O
has	O
several	O
distinct	O
subclasses	O
.	O
there	O
are	O
many	O
relations	O
and	O
similarities	O
between	O
statistical	B
and	O
neural	O
net	O
algorithms	O
but	O
a	O
systematic	O
study	O
of	O
these	O
relations	O
is	O
still	O
lacking	O
.	O
another	O
distinguishing	O
feature	O
of	O
dipol92	O
is	O
the	O
introduction	O
of	O
boolean	O
variables	O
(	O
signs	O
of	O
the	O
normals	O
of	O
the	O
discriminating	O
hyperplanes	O
)	O
for	O
the	O
description	O
of	O
class	O
regions	O
on	O
a	O
symbolic	O
level	O
and	O
using	O
them	O
in	O
the	O
decision	O
procedure	O
.	O
this	O
way	O
additional	O
layers	O
of	O
“	O
hidden	B
units	O
”	O
can	O
be	O
avoided	O
.	O
dipol92	O
has	O
some	O
similarity	O
with	O
the	O
madaline-system	O
(	O
widrow	O
,	O
1962	O
)	O
which	O
is	O
also	O
a	O
piecewise	O
linear	O
classiﬁcation	B
procedure	O
.	O
but	O
instead	O
of	O
applying	O
a	O
majority	O
function	O
for	O
class	O
decision	O
on	O
the	O
symbolic	O
level	O
(	O
as	O
in	O
the	O
case	O
of	O
madaline	O
)	O
dipol92	O
uses	O
more	O
general	O
boolean	O
descriptions	O
of	O
class	O
and	O
subclass	O
segments	O
,	O
respectively	O
.	O
this	O
extends	O
the	O
variety	O
of	O
classiﬁcation	B
problems	O
which	O
can	O
be	O
handled	O
considerably	O
.	O
6.4.2	O
pairwise	O
linear	O
regression	O
then	O
then	O
for	O
for	O
is	O
correctly	O
classiﬁed	O
if	O
follows	O
:	O
if	O
if	O
suppose	O
thaty­â	O
ã	O
~dde	O
~	O
g	O
.	O
then	O
linear	O
regression	O
is	O
used	O
is	O
the	O
set	O
of	O
data	O
«	O
c	O
«	O
«	O
ê	O
|	O
andû	O
	O
by	O
deﬁning	O
the	O
dependent	O
variable	O
b	O
as	O
to	O
discriminate	O
between	O
two	O
classesû	O
«	O
'äû	O
«	O
'äû	O
.~	O
:	O
yçæ­ã	O
withåc	O
«	O
letå	O
be	O
the	O
linear	O
regression	O
functionå	O
«	O
'äû	O
åc	O
«	O
then	O
a	O
pattern	O
«	O
gýô	O
åc	O
«	O
«	O
'äû	O
gýÿ	O
misclassiﬁcations	O
are	O
summed	O
up	O
.	O
suppose	O
thatå	O
6.4.3	O
learning	O
procedure	O
the	O
following	O
criterion	O
function	O
is	O
deﬁned	O
.	O
for	O
all	O
misclassiﬁed	O
patterns	O
the	O
squared	O
distances	O
from	O
the	O
corresponding	O
decision	O
hyperplane	O
multiplied	O
by	O
the	O
costs	O
for	O
these	O
°	O
deﬁnes	O
the	O
decision	O
hyperplane	O
for	O
each	O
pair	O
of	O
classes	O
a	O
discriminating	O
regression	O
function	O
can	O
be	O
calculated	O
.	O
å.e	O
de	O
ê	O
	O
|	O
|	O
~	O
ö	O
	O
e	O
f	O
ö	O
	O
	O
f	O
g	O
	O
i	O
i	O
|	O
«	O
|	O
e	O
e	O
i	O
ê	O
«	O
ê	O
	O
°	O
|	O
°	O
	O
	O
sec	O
.	O
6.4	O
]	O
dipol92	O
105	O
algorithm	O
for	O
each	O
decision	O
surface	O
successively	O
.	O
this	O
means	O
that	O
costs	O
are	O
included	O
explicitly	O
in	O
the	O
learning	O
procedure	O
which	O
consists	O
of	O
by	O
a	O
gradient	O
descent	O
|	O
,	O
i.e.	O
,	O
«	O
'äû	O
|	O
andåc	O
«	O
|	O
andû	O
	O
,	O
respectively	O
.	O
then	O
let	O
|	O
be	O
the	O
set	O
of	O
all	O
misclassiﬁed	O
between	O
the	O
classesû	O
	O
,	O
i.e.	O
,	O
«	O
käû	O
	O
andåc	O
«	O
	O
be	O
the	O
set	O
of	O
all	O
misclassiﬁed	O
patterns	O
g+ÿ	O
patterns	O
of	O
classû	O
°	O
,	O
let	O
gçô	O
g	O
be	O
the	O
costs	O
of	O
the	O
misclassiﬁcation	O
°	O
,	O
and	O
letmdc	O
of	O
classû	O
	O
.	O
we	O
then	O
minimise	O
:	O
åc	O
«	O
åc	O
«	O
into	O
the	O
classû	O
of	O
the	O
classû	O
c6å	O
éè	O
éè	O
feg	O
exmdc³	O
mdc	O
minimizing	O
the	O
criterion	O
function	O
with	O
respect	O
toi	O
~ded	O
~	O
·7	O
>	O
1d	O
û·	O
has	O
been	O
partitioned	O
intoù	O
cb	O
vectorsé	O
6.4.4	O
clustering	O
of	O
classes	O
to	O
handle	O
also	O
problems	O
with	O
non–convex	O
(	O
especially	O
non	O
simply–connected	O
class	O
regions	O
)	O
,	O
one	O
can	O
apply	O
a	O
clustering	O
procedure	O
before	O
the	O
linear	O
regression	O
is	O
carried	O
out	O
.	O
for	O
solving	O
the	O
clustering	O
problem	O
a	O
minimum–squared–error	O
algorithm	O
is	O
used	O
.	O
suppose	O
that	O
a	O
class	O
g	O
withd	O
elements	O
and	O
mean	O
·	O
clustersû	O
~edd~	O
then	O
the	O
criterion	O
function	O
	O
given	O
by	O
éè	O
ìë	O
¢è	O
the	O
criterion	O
function§	O
á|	O
«	O
0	O
is	O
calculated	O
.	O
patterns	O
are	O
moved	O
from	O
one	O
cluster	O
to	O
another	O
if	O
such	O
a	O
move	O
will	O
improve	O
.	O
the	O
mean	O
vectors	O
and	O
the	O
criterion	O
function	O
are	O
updated	O
after	O
each	O
pattern	O
move	O
.	O
like	O
hill–climbing	O
algorithms	O
in	O
general	O
,	O
these	O
approaches	O
guarantee	O
local	O
but	O
not	O
global	O
optimisation	B
.	O
different	O
initial	O
partitions	O
and	O
sequences	O
of	O
the	O
training	O
patterns	O
can	O
lead	O
to	O
different	O
solutions	O
.	O
in	O
the	O
case	O
of	O
clustering	O
the	O
number	O
of	O
two–class	O
problems	O
increases	O
correspondingly	O
.	O
we	O
note	O
that	O
by	O
the	O
combination	O
of	O
the	O
clustering	O
algorithm	O
with	O
the	O
regression	O
tech-	O
nique	O
the	O
number	O
and	O
initial	O
positions	O
of	O
discriminating	O
hyperplanes	O
are	O
ﬁxed	O
a	O
priori	O
(	O
i.e	O
.	O
before	O
learning	O
)	O
in	O
a	O
reasonable	O
manner	O
,	O
even	O
in	O
the	O
case	O
that	O
some	O
classes	O
have	O
multimodal	O
distributions	O
(	O
i.e	O
consist	O
of	O
several	O
subclasses	O
)	O
.	O
thus	O
a	O
well	O
known	O
bottleneck	O
of	O
artiﬁcial	O
neural	O
nets	O
can	O
at	O
least	O
be	O
partly	O
avoided	O
.	O
6.4.5	O
description	O
of	O
the	O
classiﬁcation	B
procedure	O
tained	O
in	O
the	O
training	O
set	O
or	O
not	O
)	O
can	O
be	O
classiﬁed	O
,	O
i.e.	O
,	O
the	O
class	O
predicted	O
.	O
for	O
the	O
pairwise	O
|e~ded	O
~	O
g	O
(	O
con-	O
if	O
the	O
discriminating	O
hyperplanes	O
were	O
calculated	O
then	O
any	O
pattern	O
«	O
c	O
«	O
³	O
hyperplaneså	O
¾âñì	O
)	O
.	O
the	O
followingù	O
«	O
–dimensional	O
fdgih	O
	O
are	O
calculated	O
(	O
in	O
the	O
discrimination	O
of	O
theù	O
classesù	O
vector±'·	O
:	O
if	O
the	O
functionå	O
case	O
of	O
clustering	O
the	O
numberù	O
is	O
changed	O
intoù	O
e	O
kíü	O
	O
,	O
then	O
the	O
i-th	O
component±ß·î	O
|	O
and	O
	O
discriminates	O
the	O
classesû	O
is	O
formed	O
for	O
each	O
classû·	O
|	O
,	O
is	O
equal	O
to	O
-1	O
,	O
ifû	O
	O
,	O
and	O
is	O
is	O
equal	O
to	O
1	O
,	O
ifû	O
is	O
deﬁned	O
for	O
each	O
pattern	O
«	O
equal	O
to	O
0	O
in	O
all	O
other	O
cases	O
.	O
on	O
the	O
basis	O
of	O
the	O
discriminant	O
functions	O
a	O
vector	O
function	O
~	O
	O
m	O
g	O
	O
f	O
~	O
³	O
g	O
à	O
¿	O
	O
g	O
	O
ê	O
«	O
ê	O
	O
~	O
à	O
¿	O
	O
g	O
	O
ê	O
«	O
ê	O
	O
å	O
~	O
i	O
|	O
i	O
¿	O
	O
f	O
ù	O
·	O
é	O
	O
	O
f	O
d	O
	O
à	O
¡	O
c	O
£	O
x	O
¥	O
«	O
§	O
c	O
à	O
	O
à	O
¡	O
c	O
£	O
x	O
¥	O
ê	O
é	O
	O
ê	O
	O
	O
«	O
ê	O
«	O
	O
ù	O
c	O
ù	O
	O
û	O
	O
	O
û	O
	O
û	O
é	O
i	O
with	O
the	O
function	O
106	O
neural	O
networks	O
[	O
ch	O
.	O
6	O
ëîí	O
gig	O
c	O
«	O
(	O
g	O
is	O
the	O
set	O
of	O
integers	O
)	O
is	O
deﬁned	O
by	O
:	O
yæ	O
signc6å	O
c	O
«	O
:	O
yïæ	O
for	O
each	O
classû	O
ä·	O
c	O
«	O
c	O
«	O
·î	O
á|	O
is	O
uniquely	O
classiﬁed	O
by	O
the	O
discriminating	O
hyperplaneså	O
~ded	O
~	O
a	O
pattern	O
«	O
cb	O
into	O
the	O
classû·	O
c	O
«	O
(	O
±ß·î	O
f	O
hyperplanes	O
,	O
which	O
discriminate	O
the	O
classû·	O
i.e.	O
,	O
with	O
respect	O
to	O
theù	O
g	O
have	O
the	O
same	O
sign	O
for	O
all±'·.î	O
°	O
)	O
.	O
for	O
all	O
other	O
classes	O
,	O
rð	O
ñð	O
f	O
classes	O
,	O
the	O
pattern	O
«	O
is	O
placed	O
in	O
the	O
halfspace	O
,	O
belonging	O
to	O
classû·	O
	O
and	O
,	O
ä	O
c	O
«	O
	O
andå	O
(	O
±	O
is	O
valid	O
,	O
because	O
at	O
least	O
with	O
respect	O
to	O
the	O
hyperplane	O
,	O
which	O
discriminates	O
class	O
g	O
have	O
not	O
the	O
classû	O
the	O
pattern	O
«	O
is	O
placed	O
in	O
the	O
halfspace	O
of	O
classû	O
c	O
«	O
a	O
pattern	O
«	O
gjÿ	O
max	O
c	O
«	O
	O
max	O
g	O
.	O
if	O
there	O
is	O
only	O
one	O
in	O
this	O
case	O
all	O
classes	O
were	O
determined	O
withä	O
c	O
«	O
c	O
«	O
such	O
class	O
then	O
«	O
will	O
be	O
assigned	O
to	O
this	O
class	O
.	O
if	O
there	O
are	O
several	O
classes	O
letv	O
~dedi~	O
eü¾	O
.	O
for	O
each	O
class	O
	O
all	O
hyperplanes	O
set	O
of	O
the	O
classes	O
with	O
this	O
property	O
,	O
v	O
º3	O
x	O
for	O
each	O
class	O
	O
are	O
selected	O
for	O
which	O
«	O
discriminating	O
the	O
class	O
	O
against	O
all	O
other	O
classes	O
are	O
found	O
.	O
those	O
of	O
the	O
hyperplanes	O
of	O
hyperplanes	O
is	O
misclassiﬁed	O
,	O
i.e.	O
,	O
for	O
each	O
class	O
	O
a	O
set	O
to	O
all	O
these	O
hyperplaneså	O
x	O
are	O
calculated	O
.	O
~ede~	O
is	O
determined	O
for	O
which	O
«	O
of	O
class	O
	O
.	O
the	O
euclidian	O
distance	O
of	O
«	O
á|	O
is	O
not	O
in	O
the	O
halfspace	O
is	O
assigned	O
to	O
that	O
class	O
for	O
which	O
the	O
minimum	O
is	O
not	O
uniquely	O
classiﬁed	O
if	O
min	O
min	O
is	O
reached	O
.	O
from	O
the	O
other	O
from	O
º¢å	O
c	O
«	O
same	O
sign	O
)	O
.	O
if	O
î	O
ò	O
ò	O
ò	O
be	O
the	O
é	O
i	O
º	O
f	O
~	O
°	O
~	O
	O
f	O
¾	O
é	O
i	O
g	O
	O
	O
	O
·	O
ü	O
ä	O
·	O
g	O
	O
ë	O
í	O
à	O
	O
±	O
	O
×	O
é	O
i	O
g	O
	O
	O
	O
f	O
ù	O
«	O
g	O
ä	O
·	O
g	O
	O
ù	O
	O
f	O
~	O
	O
ù	O
	O
å	O
	O
	O
	O
d	O
	O
ÿ	O
ù	O
	O
f	O
·	O
·	O
	O
î	O
	O
ä	O
	O
ù	O
	O
f	O
	O
g	O
ä	O
	O
	O
|	O
å	O
©	O
	O
	O
x	O
	O
|	O
	O
x	O
å	O
©	O
	O
x	O
¾	O
ñ	O
	O
«	O
	O
x	O
è	O
	O
ñ	O
î	O
©	O
å	O
ñ	O
	O
x	O
g	O
7	O
methods	O
for	O
comparison	O
r.	O
j.	O
henery	O
university	O
of	O
strathclyde|	O
7.1	O
estimation	O
of	O
error	O
rates	O
in	O
classification	O
rules	O
in	O
testing	O
the	O
accuracy	O
of	O
a	O
classiﬁcation	B
rule	O
,	O
it	O
is	O
widely	O
known	O
that	O
error	O
rates	O
tend	O
to	O
be	O
biased	O
if	O
they	O
are	O
estimated	O
from	O
the	O
same	O
set	O
of	O
data	O
as	O
that	O
used	O
to	O
construct	O
the	O
rules	O
.	O
at	O
one	O
extreme	O
,	O
if	O
a	O
decision	O
tree	O
for	O
example	B
is	O
allowed	O
to	O
grow	O
without	O
limit	O
to	O
the	O
number	O
of	O
leaves	O
in	O
the	O
tree	O
,	O
it	O
is	O
possible	O
to	O
classify	O
the	O
given	O
data	O
with	O
100	O
%	O
accuracy	O
,	O
in	O
general	O
at	O
the	O
expense	O
of	O
creating	O
a	O
very	O
complex	O
tree-structure	O
.	O
in	O
practice	O
complex	O
structures	O
do	O
not	O
always	O
perform	O
well	O
when	O
tested	O
on	O
unseen	O
data	O
,	O
and	O
this	O
is	O
one	O
case	O
of	O
the	O
general	O
phenomenon	O
of	O
over-ﬁtting	O
data	O
.	O
of	O
course	O
,	O
overﬁtting	O
is	O
of	O
most	O
concern	O
with	O
noisy	O
data	O
,	O
i.e	O
.	O
data	O
in	O
which	O
100	O
%	O
correct	O
classiﬁcation	B
is	O
impossible	O
in	O
principle	O
as	O
there	O
are	O
conﬂicting	O
examples	O
.	O
however	O
,	O
the	O
problem	O
also	O
arises	O
with	O
noise-free	O
datasets	O
,	O
where	O
,	O
in	O
principle	O
,	O
100	O
%	O
correct	O
classiﬁcation	B
is	O
possible	O
.	O
among	O
the	O
statlog	O
datasets	O
,	O
for	O
example	B
,	O
there	O
is	O
one	O
dataset	O
(	O
shuttle	O
)	O
that	O
is	O
probably	O
noise	O
free	O
,	O
and	O
it	O
is	O
possible	O
to	O
classify	O
the	O
given	O
data	O
100	O
%	O
correctly	O
.	O
however	O
,	O
certain	O
classes	O
are	O
represented	O
so	O
infrequently	O
that	O
we	O
can	O
not	O
be	O
sure	O
what	O
the	O
true	O
classiﬁcation	B
procedure	O
should	O
be	O
.	O
as	O
a	O
general	O
rule	O
,	O
we	O
expect	O
that	O
very	O
simple	O
structures	O
should	O
be	O
used	O
for	O
noisy	O
data	O
,	O
and	O
very	O
complex	O
structures	O
only	O
for	O
data	O
that	O
are	O
noise-free	O
.	O
what	O
is	O
clear	O
is	O
that	O
we	O
should	O
adjust	O
the	O
complexity	O
to	O
suit	O
the	O
problem	O
at	O
hand	O
,	O
otherwise	O
the	O
procedure	O
will	O
be	O
biased	O
.	O
for	O
example	B
,	O
most	O
decision	O
tree	O
procedures	O
(	O
such	O
as	O
cart	O
by	O
breiman	O
et	O
al.	O
,	O
1984	O
)	O
restrict	O
the	O
size	O
of	O
the	O
decision	O
tree	O
by	O
pruning	B
,	O
i.e	O
.	O
by	O
cutting	O
out	O
some	O
branches	O
if	O
they	O
do	O
not	O
lead	O
to	O
useful	O
dichotomies	O
of	O
the	O
data	O
.	O
even	O
if	O
some	O
measure	B
of	O
pruning	B
is	O
added	O
to	O
avoid	O
over-ﬁtting	O
the	O
data	O
,	O
the	O
apparent	O
error-rate	O
,	O
estimated	O
by	O
applying	O
the	O
induced	O
rule	O
on	O
the	O
original	O
data	O
,	O
is	O
usually	O
over-optimistic	O
.	O
one	O
way	O
of	O
correcting	O
for	O
this	O
bias	O
is	O
to	O
use	O
two	O
independent	O
samples	O
of	O
data	O
:	O
one	O
to	O
learn	O
the	O
rule	O
and	O
another	O
to	O
test	O
it	O
.	O
a	O
method	O
that	O
is	O
more	O
suitable	O
for	O
intermediate	O
sample	O
sizes	O
(	O
of	O
order	O
1000	O
)	O
is	O
cross-validation	O
,	O
which	O
ﬁrst	O
came	O
to	O
prominence	O
when	O
lachenbruch	O
&	O
mickey	O
(	O
1968	O
)	O
suggested	O
the	O
leave-one-out	O
procedure	O
.	O
a	O
closely	O
related	O
method	O
,	O
which	O
is	O
used	O
for	O
small	O
sample	O
sizes	O
,	O
is	O
the	O
bootstrap	O
m	O
address	O
for	O
correspondence	O
:	O
department	O
of	O
statistics	O
and	O
modelling	O
science	O
,	O
university	O
of	O
strathclyde	O
,	O
glasgow	O
g1	O
1xh	O
,	O
u.k.	O
108	O
methods	O
for	O
comparison	O
[	O
ch	O
.	O
7	O
procedure	O
of	O
efron	O
(	O
1983	O
)	O
.	O
these	O
three	O
methods	O
of	O
estimating	O
error	O
rates	O
are	O
now	O
described	O
brieﬂy	O
.	O
7.1.1	O
train-and-test	O
the	O
essential	O
idea	O
is	O
this	O
:	O
a	O
sample	O
of	O
data	O
(	O
the	O
training	O
data	O
)	O
is	O
given	O
to	O
enable	O
a	O
classiﬁ-	O
cation	O
rule	O
to	O
be	O
set	O
up	O
.	O
what	O
we	O
would	O
like	O
to	O
know	O
is	O
the	O
proportion	O
of	O
errors	O
made	O
by	O
this	O
rule	O
when	O
it	O
is	O
up-and-running	O
,	O
and	O
classifying	O
new	O
observations	O
without	O
the	O
beneﬁt	O
of	O
knowing	O
the	O
true	O
classiﬁcations	O
.	O
to	O
do	O
this	O
,	O
we	O
test	O
the	O
rule	O
on	O
a	O
second	O
independent	O
sample	O
of	O
new	O
observations	O
(	O
the	O
test	O
data	O
)	O
whose	O
true	O
classiﬁcations	O
are	O
known	O
but	O
are	O
not	O
told	O
to	O
the	O
classiﬁer	B
.	O
the	O
predicted	O
and	O
true	O
classiﬁcations	O
on	O
the	O
test	O
data	O
give	O
an	O
unbiased	O
estimate	O
of	O
the	O
error	O
rate	O
of	O
the	O
classiﬁer	B
.	O
to	O
enable	O
this	O
procedure	O
to	O
be	O
carried	O
out	O
from	O
a	O
given	O
set	O
of	O
data	O
,	O
a	O
proportion	O
of	O
the	O
data	O
is	O
selected	O
at	O
random	O
(	O
usually	O
about	O
20-30	O
%	O
)	O
and	O
used	O
as	O
the	O
test	O
data	O
.	O
the	O
classiﬁer	B
is	O
trained	O
on	O
the	O
remaining	O
data	O
,	O
and	O
then	O
tested	O
on	O
the	O
test	O
data	O
.	O
there	O
is	O
a	O
slight	O
loss	O
of	O
efﬁciency	O
here	O
as	O
we	O
do	O
not	O
use	O
the	O
full	O
sample	O
to	O
train	O
the	O
decision	O
rule	O
,	O
but	O
with	O
very	O
large	O
datasets	O
this	O
is	O
not	O
a	O
major	O
problem	O
.	O
we	O
adopted	O
this	O
procedure	O
when	O
the	O
number	O
of	O
examples	O
was	O
much	O
larger	O
than	O
1000	O
(	O
and	O
allowed	O
the	O
use	O
of	O
a	O
test	O
sample	O
of	O
size	O
1000	O
or	O
so	O
)	O
.	O
we	O
often	O
refer	O
to	O
this	O
method	O
as	O
“	O
one-shot	O
”	O
train-and-test	O
.	O
7.1.2	O
cross-validation	O
for	O
moderate-sized	O
samples	O
,	O
the	O
procedure	O
we	O
adopted	O
was	O
cross-validation	O
.	O
in	O
its	O
most	O
subsamples	O
.	O
each	O
fdg	O
subsamples	O
.	O
in	O
this	O
way	O
the	O
error	O
rate	O
is	O
estimated	O
efﬁciently	O
and	O
in	O
an	O
unbiased	O
way	O
.	O
the	O
rule	O
ﬁnally	O
used	O
is	O
calculated	O
from	O
all	O
the	O
data	O
.	O
the	O
leave-one-out	O
method	O
of	O
lachenbruch	O
&	O
mickey	O
equal	O
to	O
the	O
number	O
of	O
examples	O
.	O
stone	O
(	O
1974	O
)	O
describes	O
cross-validation	O
methods	O
for	O
giving	O
unbiased	O
estimates	O
of	O
the	O
error	O
rate	O
.	O
a	O
practical	O
difﬁculty	O
with	O
the	O
use	O
of	O
cross-validation	O
in	O
computer-intensive	O
methods	O
-fold	O
repetition	O
of	O
the	O
learning	O
cycle	O
,	O
which	O
may	O
require	O
elementary	O
form	O
,	O
cross-validation	O
consists	O
of	O
dividing	O
the	O
data	O
into	O
sub-sample	O
is	O
predicted	O
via	O
the	O
classiﬁcation	B
rule	O
constructed	O
from	O
the	O
remainingcø	O
subsamples	O
,	O
and	O
the	O
estimated	O
error	O
rate	O
is	O
the	O
average	O
error	O
rate	O
from	O
these	O
(	O
1968	O
)	O
is	O
of	O
course	O
such	O
as	O
neural	O
networks	O
is	O
the	O
-fold	O
cross-validation	O
with	O
much	O
computational	O
effort	O
.	O
7.1.3	O
bootstrap	O
the	O
more	O
serious	O
objection	O
to	O
cross-validation	O
is	O
that	O
the	O
error	O
estimates	O
it	O
produces	O
are	O
too	O
scattered	O
,	O
so	O
that	O
the	O
conﬁdence	O
intervals	O
for	O
the	O
true	O
error-rate	O
are	O
too	O
wide	O
.	O
the	O
bootstrap	O
procedure	O
gives	O
much	O
narrower	O
conﬁdence	O
limits	O
,	O
but	O
the	O
penalty	O
paid	O
is	O
that	O
the	O
estimated	O
error-rates	O
are	O
optimistic	O
(	O
i.e	O
.	O
are	O
biased	O
downwards	O
)	O
.	O
the	O
trade-off	O
between	O
bias	O
and	O
random	O
error	O
means	O
that	O
,	O
as	O
a	O
general	O
rule	O
,	O
the	O
bootstrap	O
method	O
is	O
preferred	O
when	O
the	O
sample	O
size	O
is	O
small	O
,	O
and	O
cross-validation	O
when	O
the	O
sample	O
size	O
is	O
large	O
.	O
in	O
conducting	O
a	O
comparative	O
trial	O
between	O
methods	O
on	O
the	O
same	O
dataset	O
,	O
the	O
amount	O
of	O
bias	O
is	O
not	O
so	O
important	O
so	O
long	O
as	O
the	O
bias	O
is	O
the	O
same	O
for	O
all	O
methods	O
.	O
since	O
the	O
bootstrap	O
represents	O
the	O
best	O
way	O
to	O
reduce	O
variability	O
,	O
the	O
most	O
effective	O
way	O
to	O
conduct	O
comparisons	O
in	O
small	O
datasets	O
is	O
to	O
use	O
the	O
bootstrap	O
.	O
since	O
it	O
is	O
not	O
so	O
widely	O
used	O
in	O
classiﬁcation	B
trials	O
as	O
perhaps	O
it	O
should	O
be	O
,	O
we	O
give	O
an	O
extended	O
description	O
here	O
,	O
although	O
it	O
must	O
be	O
admitted	O
sec	O
.	O
7.1	O
]	O
estimation	O
of	O
error	O
rates	O
in	O
classiﬁcation	B
rules	O
109	O
that	O
we	O
did	O
not	O
use	O
the	O
bootstrap	O
in	O
any	O
of	O
our	O
trials	O
as	O
we	O
judged	O
that	O
our	O
samples	O
were	O
large	O
enough	O
to	O
use	O
either	O
cross-validation	O
or	O
train-and-test	O
.	O
in	O
statistical	B
terms	O
,	O
the	O
bootstrap	O
is	O
a	O
non-parametric	O
procedure	O
for	O
estimating	O
param-	O
eters	O
generally	O
and	O
error-rates	O
in	O
particular	O
.	O
the	O
basic	O
idea	O
is	O
to	O
re-use	O
the	O
original	O
dataset	O
(	O
of	O
sizek	O
)	O
to	O
obtain	O
new	O
datasets	O
also	O
of	O
sizek	O
by	O
re-sampling	O
with	O
replacement	O
.	O
see	O
efron	O
(	O
1983	O
)	O
for	O
the	O
deﬁnitive	O
introduction	O
to	O
the	O
subject	O
and	O
crawford	O
(	O
1989	O
)	O
for	O
an	O
application	O
to	O
cart	O
.	O
breiman	O
et	O
al	O
.	O
(	O
1984	O
)	O
note	O
that	O
there	O
are	O
practical	O
difﬁculties	O
in	O
applying	O
the	O
bootstrap	O
to	O
decision	O
trees	O
.	O
in	O
the	O
context	O
of	O
classiﬁcation	B
,	O
the	O
bootstrap	O
idea	O
is	O
to	O
replicate	O
the	O
whole	O
classiﬁcation	B
experiment	O
a	O
large	O
number	O
of	O
times	O
and	O
to	O
estimate	O
quantities	O
like	O
bias	O
from	O
these	O
replicate	O
of	O
bootstrap	O
replicate	O
samples	O
are	O
created	O
,	O
each	O
sample	O
being	O
a	O
replicate	O
(	O
randomly	O
is	O
taken	O
from	O
the	O
original	O
sample	O
by	O
sampling	O
with	O
replacement	O
.	O
sampling	O
with	O
replacement	O
means	O
,	O
for	O
37	O
%	O
of	O
data	O
will	O
not	O
appear	O
in	O
the	O
bootstrap	O
sample	O
)	O
.	O
also	O
,	O
some	O
data	O
points	O
will	O
appear	O
more	O
than	O
once	O
in	O
the	O
bootstrap	O
sample	O
.	O
each	O
bootstrap	O
sample	O
is	O
used	O
to	O
construct	O
a	O
classiﬁcation	B
rule	O
which	O
is	O
then	O
used	O
to	O
predict	O
the	O
classes	O
of	O
those	O
original	O
data	O
that	O
were	O
unused	O
in	O
the	O
37	O
%	O
of	O
the	O
original	O
data	O
will	O
be	O
used	O
as	O
test	O
set	O
)	O
.	O
this	O
gives	O
one	O
estimate	O
of	O
the	O
error	O
rate	O
for	O
each	O
bootstrap	O
sample	O
.	O
the	O
average	O
error	O
rates	O
over	O
all	O
bootstrap	O
samples	O
are	O
then	O
combined	O
to	O
give	O
an	O
estimated	O
error	O
rate	O
for	O
the	O
original	O
rule	O
.	O
see	O
efron	O
(	O
1983	O
)	O
and	O
crawford	O
(	O
1989	O
)	O
for	O
details	O
.	O
the	O
main	O
properties	O
of	O
the	O
bootstrap	O
have	O
been	O
summarised	O
by	O
efron	O
(	O
1983	O
)	O
as	O
follows	O
.	O
properties	O
of	O
cross-validation	O
and	O
bootstrap	O
efron	O
(	O
1983	O
)	O
gives	O
the	O
following	O
properties	O
of	O
the	O
bootstrap	O
as	O
an	O
estimator	O
of	O
error-rate	O
.	O
by	O
experiments	O
.	O
thus	O
,	O
to	O
estimate	O
the	O
error	O
rate	O
in	O
small	O
samples	O
(	O
of	O
sizek	O
say	O
)	O
,	O
a	O
large	O
number	O
chosen	O
)	O
of	O
the	O
original	O
sample	O
.	O
that	O
is	O
,	O
a	O
random	O
sample	O
of	O
sizek	O
example	B
,	O
that	O
some	O
data	O
points	O
will	O
be	O
omitted	O
(	O
on	O
average	O
aboutfdh	O
training	O
set	O
(	O
so	O
aboutfdh	O
taking¦	O
very	O
large	O
(	O
efron	O
recommends	O
approximately	O
200	O
)	O
,	O
the	O
statistical	B
variability	O
in	O
the	O
average	O
error	O
rate	O
efron	O
is	O
small	O
,	O
and	O
for	O
small	O
sample	O
sizek	O
will	O
have	O
very	O
much	O
smaller	O
statistical	B
variability	O
than	O
the	O
cross-validation	O
estimate	O
.	O
,	O
this	O
means	O
that	O
the	O
bootstrap	O
the	O
bootstrap	O
and	O
cross-validation	O
estimates	O
are	O
generally	O
close	O
for	O
large	O
sample	O
sizes	O
,	O
and	O
the	O
ratio	O
between	O
the	O
two	O
estimates	O
approaches	O
unity	O
as	O
the	O
sample	O
size	O
tends	O
to	O
inﬁnity	O
.	O
the	O
bootstrap	O
and	O
cross-validation	O
methods	O
tend	O
to	O
be	O
closer	O
for	O
smoother	O
cost-	O
functions	O
than	O
the	O
0-1	O
loss-function	O
implicit	O
in	O
the	O
error	O
rates	O
discussed	O
above	O
.	O
however	O
the	O
bootstrap	O
may	O
be	O
biased	O
,	O
even	O
for	O
large	O
samples	O
.	O
the	O
effective	O
sample	O
size	O
is	O
determined	O
by	O
the	O
number	O
in	O
the	O
smallest	O
classiﬁcation	B
group	O
.	O
efron	O
(	O
1983	O
)	O
quotes	O
a	O
medical	O
example	B
with	O
n	O
=	O
155	O
cases	O
,	O
but	O
primary	O
interest	O
centres	O
on	O
the	O
33	O
patients	O
that	O
died	O
.	O
the	O
effective	O
sample	O
size	O
here	O
is	O
33.	O
for	O
large	O
samples	O
,	O
group-wise	O
cross-validation	O
may	O
give	O
better	O
results	O
than	O
the	O
leave-	O
one-out	O
method	O
,	O
although	O
this	O
conclusion	O
seems	O
doubtful	O
.	O
7.1.4	O
optimisation	B
of	O
parameters	O
frequently	O
it	O
is	O
desirable	O
to	O
tune	O
some	O
parameter	O
to	O
get	O
the	O
best	O
performance	O
from	O
an	O
algorithm	O
:	O
examples	O
might	O
be	O
the	O
amount	O
of	O
pruning	B
in	O
a	O
decision	O
tree	O
or	O
the	O
number	O
of	O
hidden	B
nodes	O
in	O
the	O
multilayer	O
perceptron	O
.	O
when	O
the	O
objective	O
is	O
to	O
minimise	O
the	O
error-rate	O
of	O
the	O
tree	O
or	O
perceptron	O
,	O
the	O
training	O
data	O
might	O
be	O
divided	O
into	O
two	O
parts	O
:	O
one	O
to	O
build	O
the	O
tree	O
or	O
perceptron	O
,	O
and	O
the	O
other	O
to	O
measure	B
the	O
error	O
rate	O
.	O
a	O
plot	O
of	O
error-rate	O
against	O
the	O
¦	O
	O
	O
	O
	O
110	O
methods	O
for	O
comparison	O
[	O
ch	O
.	O
7	O
parameter	O
will	O
indicate	O
what	O
the	O
best	O
choice	O
of	O
parameter	O
should	O
be	O
.	O
however	O
,	O
the	O
error	O
rate	O
corresponding	O
to	O
this	O
choice	O
of	O
parameter	O
is	O
a	O
biased	O
estimate	O
of	O
the	O
error	O
rate	O
of	O
the	O
classiﬁcation	B
rule	O
when	O
tested	O
on	O
unseen	O
data	O
.	O
when	O
it	O
is	O
necessary	O
to	O
optimise	O
a	O
parameter	O
in	O
this	O
way	O
,	O
we	O
recommend	O
a	O
three-stage	O
process	O
for	O
very	O
large	O
datasets	O
:	O
(	O
i	O
)	O
hold	O
back	O
20	O
%	O
as	O
a	O
test	O
sample	O
;	O
(	O
ii	O
)	O
of	O
the	O
remainder	O
,	O
divide	O
into	O
two	O
,	O
with	O
one	O
set	O
used	O
for	O
building	O
the	O
rule	O
and	O
the	O
other	O
for	O
choosing	O
the	O
parameter	O
;	O
(	O
iii	O
)	O
use	O
the	O
chosen	O
parameter	O
to	O
build	O
a	O
rule	O
for	O
the	O
complete	O
training	O
sample	O
(	O
containing	O
80	O
%	O
of	O
the	O
original	O
data	O
)	O
and	O
test	O
this	O
rule	O
on	O
the	O
test	O
sample	O
.	O
thus	O
,	O
for	O
example	B
,	O
watkins	O
(	O
1987	O
)	O
gives	O
a	O
description	O
of	O
cross-validation	O
in	O
the	O
context	O
of	O
testing	O
decision-tree	O
classiﬁcation	B
algorithms	O
,	O
and	O
uses	O
cross-validation	O
as	O
a	O
means	O
of	O
se-	O
lecting	O
better	O
decision	O
trees	O
.	O
similarly	O
,	O
in	O
this	O
book	O
,	O
cross-validation	O
was	O
used	O
by	O
backprop	O
in	O
ﬁnding	O
the	O
optimal	O
number	O
of	O
nodes	O
in	O
the	O
hidden	B
layer	O
,	O
following	O
the	O
procedure	O
outlined	O
above	O
.	O
this	O
was	O
done	O
also	O
for	O
the	O
trials	O
involving	O
cascade	O
.	O
however	O
,	O
cross-validation	O
runs	O
involve	O
a	O
greatly	O
increased	O
amount	O
of	O
computational	O
labour	O
,	O
increasing	O
the	O
learning	O
time	O
ä	O
fold	O
,	O
and	O
this	O
problem	O
is	O
particularly	O
serious	O
for	O
neural	O
networks	O
.	O
in	O
statlog	O
,	O
most	O
procedures	O
had	O
a	O
tuning	O
parameter	O
that	O
can	O
be	O
set	O
to	O
a	O
default	O
value	O
,	O
and	O
where	O
this	O
was	O
possible	O
the	O
default	O
parameters	O
were	O
used	O
.	O
this	O
was	O
the	O
case	O
,	O
for	O
example	B
,	O
with	O
the	O
decision	O
trees	O
:	O
generally	O
no	O
attempt	O
was	O
made	O
to	O
ﬁnd	O
the	O
optimal	O
amount	O
of	O
pruning	B
,	O
and	O
accuracy	O
and	O
“	O
mental	B
ﬁt	I
”	O
(	O
see	O
chapter	O
5	O
)	O
is	O
thereby	O
sacriﬁced	O
for	O
the	O
sake	O
of	O
speed	O
in	O
the	O
learning	O
process	O
.	O
7.2	O
organisation	O
of	O
comparative	O
trials	O
we	O
describe	O
in	O
this	O
section	O
what	O
we	O
consider	O
to	O
be	O
the	O
ideal	O
setup	O
for	O
comparing	O
classi-	O
ﬁcation	O
procedures	O
.	O
it	O
not	O
easy	O
to	O
compare	O
very	O
different	O
algorithms	O
on	O
a	O
large	O
number	O
of	O
datasets	O
,	O
and	O
in	O
practice	O
some	O
compromises	O
have	O
to	O
be	O
made	O
.	O
we	O
will	O
not	O
detail	O
the	O
compromises	O
that	O
we	O
made	O
in	O
our	O
own	O
trials	O
,	O
but	O
attempt	O
to	O
set	O
out	O
the	O
ideals	O
that	O
we	O
tried	O
to	O
follow	O
,	O
and	O
give	O
a	O
brief	O
description	O
of	O
the	O
unix-based	O
procedures	O
that	O
we	O
adopted	O
.	O
if	O
a	O
potential	O
trialist	O
wishes	O
to	O
perform	O
another	O
set	O
of	O
trials	O
,	O
is	O
able	O
to	O
cast	O
the	O
relevant	O
algorithms	O
into	O
the	O
form	O
that	O
we	O
detail	O
here	O
,	O
and	O
moreover	O
is	O
able	O
to	O
work	O
within	O
a	O
unix	O
environment	O
,	O
then	O
we	O
can	O
recommend	O
that	O
he	O
uses	O
our	O
test	O
procedures	O
.	O
this	O
will	O
guarantee	O
comparability	O
with	O
the	O
majority	O
of	O
our	O
own	O
results	O
.	O
in	O
the	O
following	O
list	O
of	O
desiderata	O
,	O
we	O
use	O
the	O
notation	O
ﬁle1	O
,	O
ﬁle2	O
,	O
...	O
to	O
denote	O
arbitrary	O
ﬁles	O
that	O
either	O
provide	O
data	O
or	O
receive	O
output	B
from	O
the	O
system	O
.	O
throughout	O
we	O
assume	O
that	O
ﬁles	O
used	O
for	O
training/testing	O
are	O
representative	O
of	O
the	O
population	O
and	O
are	O
statistically	O
similar	O
to	O
each	O
other	O
.	O
1.	O
training	O
phase	O
.	O
the	O
most	O
elementary	O
functionality	B
required	O
of	O
any	O
learning	O
algorithm	O
,	O
is	O
to	O
be	O
able	O
to	O
take	O
data	O
from	O
one	O
ﬁle	O
ﬁle1	O
(	O
by	O
assumption	O
ﬁle1	O
contains	O
known	O
classes	O
)	O
and	O
create	O
the	O
rules	O
.	O
(	O
optionally	O
)	O
the	O
resulting	O
rules	O
(	O
or	O
parameters	O
deﬁning	O
the	O
rule	O
)	O
may	O
be	O
saved	O
to	O
another	O
ﬁle	O
ﬁle3	O
;	O
(	O
optionally	O
)	O
a	O
cost	O
matrix	O
(	O
in	O
ﬁle2	O
say	O
)	O
can	O
be	O
read	O
in	O
and	O
used	O
in	O
building	O
the	O
rules	O
2.	O
testing	O
phase	O
.	O
the	O
algorithm	O
can	O
read	O
in	O
the	O
rules	O
and	O
classify	O
unseen	O
data	O
,	O
in	O
the	O
following	O
sequence	O
:	O
sec	O
.	O
7.2	O
]	O
organisation	O
of	O
comparative	O
trials	O
111	O
read	O
in	O
the	O
rules	O
or	O
parameters	O
from	O
the	O
training	O
phase	O
(	O
either	O
passed	O
on	O
directly	O
from	O
the	O
training	O
phase	O
if	O
that	O
immediately	O
precedes	O
the	O
testing	O
phase	O
or	O
read	O
from	O
the	O
ﬁle	O
ﬁle3	O
)	O
read	O
in	O
a	O
set	O
of	O
unseen	O
data	O
from	O
a	O
ﬁle	O
ﬁle4	O
with	O
true	O
classiﬁcations	O
that	O
are	O
hidden	B
from	O
the	O
classiﬁer	B
(	O
optionally	O
)	O
read	O
in	O
a	O
cost	O
matrix	O
from	O
a	O
ﬁle	O
ﬁle5	O
(	O
normally	O
ﬁle5	O
=	O
ﬁle2	O
)	O
and	O
use	O
this	O
cost	O
matrix	O
in	O
the	O
classiﬁcation	B
procedure	O
(	O
optionally	O
)	O
output	B
the	O
classiﬁcations	O
to	O
a	O
ﬁle	O
ﬁle6	O
if	O
true	O
classiﬁcations	O
were	O
provided	O
in	O
the	O
test	O
ﬁle	O
ﬁle4	O
,	O
output	B
to	O
ﬁle	O
ﬁle7	O
a	O
confusion	O
matrix	O
whose	O
rows	O
represent	O
the	O
true	O
classiﬁcations	O
and	O
whose	O
columns	O
represent	O
the	O
classiﬁcations	O
made	O
by	O
the	O
algorithm	O
the	O
two	O
steps	O
above	O
constitute	O
the	O
most	O
basic	O
element	O
of	O
a	O
comparative	O
trial	O
,	O
and	O
we	O
describe	O
this	O
basic	O
element	O
as	O
a	O
simple	O
train-and-test	O
(	O
tt	O
)	O
procedure	O
.	O
all	O
algorithms	O
used	O
in	O
our	O
trials	O
were	O
able	O
to	O
perform	O
the	O
train-and-test	O
procedure	O
.	O
7.2.1	O
cross-validation	O
to	O
follow	O
the	O
cross-validation	O
procedure	O
,	O
it	O
is	O
necessary	O
to	O
build	O
an	O
outer	O
loop	O
of	O
control	O
procedures	O
that	O
divide	O
up	O
the	O
original	O
ﬁle	O
into	O
its	O
component	O
parts	O
and	O
successively	O
use	O
each	O
part	O
as	O
test	O
ﬁle	O
and	O
the	O
remaining	O
part	O
as	O
training	O
ﬁle	O
.	O
of	O
course	O
,	O
the	O
cross-validation	O
procedure	O
results	O
in	O
a	O
succession	O
of	O
mini-confusion	O
matrices	O
,	O
and	O
these	O
must	O
be	O
combined	O
to	O
give	O
the	O
overall	O
confusion	O
matrix	O
.	O
all	O
this	O
can	O
be	O
done	O
within	O
the	O
evaluation	O
assistant	O
shell	O
provided	O
the	O
classiﬁcation	B
procedure	O
is	O
capable	O
of	O
the	O
simple	O
train-and-test	O
steps	O
above	O
.	O
some	O
more	O
sophisticated	O
algorithms	O
may	O
have	O
a	O
cross-validation	O
procedure	O
built	O
in	O
,	O
of	O
course	O
,	O
and	O
if	O
so	O
this	O
is	O
a	O
distinct	O
advantage	O
.	O
7.2.2	O
bootstrap	O
the	O
use	O
of	O
the	O
bootstrap	O
procedure	O
makes	O
it	O
imperative	O
that	O
combining	O
of	O
results	O
,	O
ﬁles	O
etc	O
.	O
is	O
done	O
automatically	O
.	O
once	O
again	O
,	O
if	O
an	O
algorithm	O
is	O
capable	O
of	O
simple	O
train-and-test	O
,	O
it	O
can	O
be	O
embedded	O
in	O
a	O
bootstrap	O
loop	O
using	O
evaluation	O
assistant	O
(	O
although	O
perhaps	O
we	O
should	O
admit	O
that	O
we	O
never	O
used	O
the	O
bootstrap	O
in	O
any	O
of	O
the	O
datasets	O
reported	O
in	O
this	O
book	O
)	O
.	O
7.2.3	O
evaluation	O
assistant	O
evaluation	O
assistant	O
is	O
a	O
tool	O
that	O
facilitates	O
the	O
testing	O
of	O
learning	O
algorithms	O
on	O
given	O
datasets	O
and	O
provides	O
standardised	O
performance	O
measures	O
.	O
in	O
particular	O
,	O
it	O
standardises	O
timings	O
of	O
the	O
various	O
phases	O
,	O
such	O
as	O
training	O
and	O
testing	O
.	O
it	O
also	O
provides	O
statistics	O
describing	O
the	O
trial	O
(	O
mean	O
error	O
rates	O
,	O
total	O
confusion	O
matrices	O
,	O
etc	O
.	O
etc.	O
)	O
.	O
it	O
can	O
be	O
obtained	O
from	O
j.	O
gama	O
of	O
the	O
university	O
of	O
porto	O
.	O
for	O
details	O
of	O
this	O
,	O
and	O
other	O
publicly	O
available	O
software	O
and	O
datasets	O
,	O
see	O
appendices	O
a	O
and	O
b.	O
two	O
versions	O
of	O
evaluation	O
assistant	O
exist	O
:	O
-	O
command	O
version	O
(	O
eac	O
)	O
-	O
interactive	O
version	O
(	O
eai	O
)	O
the	O
command	O
version	O
of	O
evaluation	O
assistant	O
(	O
eac	O
)	O
consists	O
of	O
a	O
set	O
of	O
basic	O
commands	O
that	O
enable	O
the	O
user	O
to	O
test	O
learning	O
algorithms	O
.	O
this	O
version	O
is	O
implemented	O
as	O
a	O
set	O
of	O
c-shell	O
scripts	O
and	O
c	O
programs	O
.	O
the	O
interactive	O
version	O
of	O
evaluation	O
assistant	O
(	O
eai	O
)	O
provides	O
an	O
interactive	O
interface	O
that	O
enables	O
the	O
user	O
to	O
set	O
up	O
the	O
basic	O
parameters	O
for	O
testing	O
.	O
it	O
is	O
implemented	O
in	O
c	O
and	O
112	O
methods	O
for	O
comparison	O
[	O
ch	O
.	O
7	O
the	O
interactive	O
interface	O
exploits	O
x	O
windows	O
.	O
this	O
version	O
generates	O
a	O
customised	O
version	O
of	O
some	O
eac	O
scripts	O
which	O
can	O
be	O
examined	O
and	O
modiﬁed	O
before	O
execution	O
.	O
both	O
versions	O
run	O
on	O
a	O
sun	O
sparcstation	O
and	O
other	O
compatible	O
workstations	O
.	O
7.3	O
characterisation	O
of	O
datasets	O
an	O
important	O
objective	O
is	O
to	O
investigate	O
why	O
certain	O
algorithms	O
do	O
well	O
on	O
some	O
datasets	O
and	O
not	O
so	O
well	O
on	O
others	O
.	O
this	O
section	O
describes	O
measures	O
of	O
datasets	O
which	O
may	O
help	O
to	O
explain	O
our	O
ﬁndings	O
.	O
these	O
measures	O
are	O
of	O
three	O
types	O
:	O
(	O
i	O
)	O
very	O
simple	O
measures	O
such	O
as	O
the	O
number	O
of	O
examples	O
;	O
(	O
ii	O
)	O
statistically	O
based	O
,	O
such	O
as	O
the	O
skewness	O
of	O
the	O
attributes	O
;	O
and	O
(	O
iii	O
)	O
information	O
theoretic	O
,	O
such	O
as	O
the	O
information	O
gain	O
of	O
attributes	O
.	O
we	O
discuss	O
information	O
theoretic	O
measures	O
in	O
section	O
7.3.3.	O
there	O
is	O
a	O
need	O
for	O
a	O
measure	B
which	O
indicates	O
when	O
decision	O
trees	O
will	O
do	O
well	O
.	O
bearing	O
in	O
mind	O
the	O
success	O
of	O
decision	O
trees	O
in	O
image	B
segmentation	I
problems	O
,	O
it	O
seems	O
that	O
some	O
measure	B
of	O
multimodality	O
might	O
be	O
useful	O
in	O
this	O
connection	O
.	O
some	O
algorithms	O
have	O
built	O
in	O
measures	O
which	O
are	O
given	O
as	O
part	O
of	O
the	O
output	B
.	O
for	O
example	B
,	O
castle	O
measures	O
the	O
kullback-leibler	O
information	O
in	O
a	O
dataset	O
.	O
such	O
measures	O
are	O
useful	O
in	O
establishing	O
the	O
validity	O
of	O
speciﬁc	O
assumptions	O
underlying	O
the	O
algorithm	O
and	O
,	O
although	O
they	O
do	O
not	O
always	O
suggest	O
what	O
to	O
do	O
if	O
the	O
assumptions	O
do	O
not	O
hold	O
,	O
at	O
least	O
they	O
give	O
an	O
indication	O
of	O
internal	O
consistency	O
.	O
the	O
measures	O
should	O
continue	O
to	O
be	O
elaborated	O
and	O
reﬁned	O
in	O
the	O
light	O
of	O
experience	O
.	O
7.3.1	O
simple	O
measures	O
the	O
following	O
descriptors	O
of	O
the	O
datasets	O
give	O
very	O
simple	O
measures	O
of	O
the	O
complexity	O
or	O
size	O
of	O
the	O
problem	O
.	O
of	O
course	O
,	O
these	O
measures	O
might	O
advantageously	O
be	O
combined	O
to	O
give	O
other	O
measures	O
more	O
appropriate	O
for	O
speciﬁc	O
tasks	O
,	O
for	O
example	B
by	O
taking	O
products	O
,	O
ratios	O
or	O
logarithms	O
.	O
numerical	O
(	O
either	O
continuous	O
or	O
ordered	O
)	O
attributes	O
.	O
7.3.2	O
statistical	B
measures	O
the	O
following	O
measures	O
are	O
designed	O
principally	O
to	O
explain	O
the	O
performance	O
of	O
statistical	B
algorithms	O
,	O
but	O
are	O
likely	O
to	O
be	O
more	O
generally	O
applicable	O
.	O
often	O
they	O
are	O
much	O
inﬂuenced	O
by	O
the	O
simple	O
measures	O
above	O
.	O
for	O
example	B
,	O
the	O
skewness	O
measure	B
often	O
reﬂects	O
the	O
the	O
total	O
number	O
of	O
attributes	O
in	O
the	O
data	O
as	O
used	O
in	O
the	O
trials	O
.	O
where	O
categorical	O
attributes	O
were	O
originally	O
present	O
,	O
these	O
were	O
converted	O
to	O
binary	O
indicator	O
variables	O
.	O
this	O
is	O
the	O
total	O
number	O
of	O
observations	O
in	O
the	O
whole	O
dataset	O
.	O
in	O
some	O
respects	O
,	O
it	O
might	O
seem	O
more	O
sensible	O
to	O
count	O
only	O
the	O
observations	O
in	O
the	O
training	O
data	O
,	O
but	O
this	O
is	O
generally	O
a	O
large	O
fraction	O
of	O
the	O
total	O
number	O
in	O
any	O
case	O
.	O
number	O
of	O
observations	O
,	O
	O
number	O
of	O
attributes	O
,	O
¥	O
number	O
of	O
classes	O
,	O
ù	O
coded	O
as	O
indicator	O
variables	O
)	O
.	O
by	O
deﬁnition	O
,	O
the	O
remaining¥n	O
bin.att	O
attributes	O
are	O
the	O
total	O
number	O
of	O
classes	O
represented	O
in	O
the	O
entire	O
dataset	O
.	O
number	O
of	O
binary	O
attributes	O
,	O
bin.att	O
the	O
total	O
number	O
of	O
number	O
of	O
attributes	O
that	O
are	O
binary	O
(	O
including	O
categorical	O
attributes	O
sec	O
.	O
7.3	O
]	O
characterisation	O
of	O
datasets	O
113	O
number	O
of	O
binary	O
attributes	O
,	O
and	O
if	O
this	O
is	O
so	O
,	O
the	O
skewness	O
and	O
kurtosis	O
are	O
directly	O
related	O
to	O
each	O
other	O
.	O
however	O
,	O
the	O
statistical	B
measures	O
in	O
this	O
section	O
are	O
generally	O
deﬁned	O
only	O
for	O
continuous	O
attributes	O
.	O
although	O
it	O
is	O
possible	O
to	O
extend	O
their	O
deﬁnitions	O
to	O
include	O
discrete	O
and	O
even	O
categorical	O
attributes	O
,	O
the	O
most	O
natural	O
measures	O
for	O
such	O
data	O
are	O
the	O
information	O
theoretic	O
measures	O
discussed	O
in	O
section	O
7.3.3.	O
test	O
statistic	O
for	O
homogeneity	O
of	O
covariances	O
the	O
covariance	O
matrices	O
are	O
fundamental	O
in	O
the	O
theory	O
of	O
linear	O
and	O
quadratic	O
discrimination	O
detailed	O
in	O
sections	O
3.2	O
and	O
3.3	O
,	O
and	O
the	O
key	O
in	O
understanding	O
when	O
to	O
apply	O
one	O
and	O
not	O
the	O
other	O
lies	O
in	O
the	O
homogeneity	O
or	O
otherwise	O
of	O
the	O
covariances	O
.	O
one	O
measure	B
of	O
the	O
lack	O
of	O
homogeneity	O
of	O
covariances	O
is	O
the	O
geometric	O
mean	O
ratio	O
of	O
standard	O
deviations	O
of	O
the	O
populations	O
of	O
individual	O
classes	O
to	O
the	O
standard	O
deviations	O
of	O
the	O
sample	O
,	O
and	O
is	O
given	O
by	O
th	O
sample	O
covariance	O
matrix	O
and	O
the	O
k	O
de	O
ê	O
>	O
ê.ç	O
where	O
á|	O
are	O
the	O
unbiased	O
estimators	O
of	O
thei	O
expressed	O
as	O
the	O
geometric	O
mean	O
ratio	O
of	O
standard	O
deviations	O
of	O
the	O
individual	O
populations	O
to	O
the	O
pooled	O
standard	O
deviations	O
,	O
via	O
the	O
expression	O
have	O
a	O
common	O
covariance	O
structure	O
,	O
i.e	O
.	O
to	O
the	O
hypothesis	O
;	O
å	O
ã¶ìiê	O
(	O
see	O
below	O
)	O
.	O
this	O
quantity	O
is	O
related	O
to	O
a	O
test	O
of	O
the	O
hypothesis	O
that	O
all	O
populations	O
äbû	O
which	O
can	O
be	O
tested	O
via	O
box	O
’	O
sv	O
feg	O
log¼	O
cbk	O
e8y¥h	O
ùö	O
³¨¥	O
fdg	O
ô¹có¥he	O
d	O
@	O
×	O
d	O
@	O
>	O
andä	O
	O
andä	O
pooled	O
covariance	O
matrix	O
respectively	O
.	O
this	O
statistic	O
has	O
an	O
asymptotic	O
distribution	O
:	O
and	O
the	O
approximation	O
is	O
good	O
if	O
eachk'	O
exceeds	O
20	O
,	O
and	O
ifù	O
and¥	O
are	O
both	O
much	O
smaller	O
than	O
everyk'	O
.	O
in	O
datasets	O
reported	O
in	O
this	O
volume	O
these	O
criteria	O
are	O
not	O
always	O
met	O
,	O
but	O
thev	O
statistic	O
can	O
still	O
be	O
computed	O
,	O
and	O
used	O
as	O
a	O
characteristic	O
of	O
the	O
data	O
.	O
thev	O
statistic	O
can	O
be	O
re-	O
expõ	O
äbû	O
ã¶ìiê	O
theäbû	O
ã¶ìiê	O
in	O
every	O
dataset	O
that	O
we	O
looked	O
at	O
thev	O
statistic	O
is	O
signiﬁcantly	O
different	O
from	O
zero	O
,	O
in	O
which	O
case	O
theäbû	O
ã¶.ìiê	O
the	O
set	O
of	O
correlationsø.	O
	O
between	O
all	O
pairs	O
of	O
attributes	O
give	O
some	O
indication	O
of	O
the	O
as	O
follows	O
.	O
the	O
correlationsø	O
	O
between	O
all	O
pairs	O
of	O
attributes	O
are	O
calculated	O
for	O
each	O
class	O
ø	O
which	O
is	O
a	O
measure	B
of	O
interdependence	O
and	O
over	O
all	O
classes	O
giving	O
the	O
measure	B
corr.abs	O
is	O
strictly	O
greater	O
than	O
unity	O
if	O
the	O
covariances	O
differ	O
,	O
and	O
is	O
equal	O
to	O
unity	O
if	O
and	O
only	O
if	O
the	O
m-statistic	O
is	O
zero	O
,	O
i.e	O
.	O
all	O
individual	O
covariance	O
matrices	O
are	O
equal	O
to	O
the	O
pooled	O
covariance	O
matrix	O
.	O
interdependence	O
of	O
the	O
attributes	O
,	O
and	O
a	O
measure	B
of	O
that	O
interdependence	O
may	O
be	O
calculated	O
mean	O
absolute	O
correlation	O
coefﬁcient	O
,	O
corr.abs	O
is	O
signiﬁcantly	O
greater	O
than	O
unity	O
.	O
separately	O
.	O
the	O
absolute	O
values	O
of	O
these	O
correlations	O
are	O
averaged	O
over	O
all	O
pairs	O
of	O
attributes	O
between	O
attributes	O
.	O
:	O
ó	O
test	O
statistic	O
:	O
fdgõ	O
á|	O
ck'	O
feg	O
|	O
	O
ó	O
	O
	O
	O
ó	O
ë	O
v	O
	O
ô	O
ë	O
à	O
	O
	O
	O
ä	O
þ	O
|	O
	O
ä	O
¼	O
~	O
ô	O
	O
f	O
	O
	O
f	O
c	O
ù	O
	O
à	O
f	O
k	O
	O
	O
f	O
	O
f	O
~	O
	O
|	O
ë	O
þ	O
|	O
	O
	O
v	O
¥	O
	O
ë	O
	O
ö	O
	O
114	O
methods	O
for	O
comparison	O
[	O
ch	O
.	O
7	O
if	O
corr.abs	O
is	O
near	O
unity	O
,	O
there	O
is	O
much	O
redundant	O
information	O
in	O
the	O
attributes	O
and	O
some	O
procedures	O
,	O
such	O
as	O
logistic	O
discriminants	O
,	O
may	O
have	O
technical	B
problems	O
associated	O
with	O
this	O
.	O
also	O
,	O
castle	O
,	O
for	O
example	B
,	O
may	O
be	O
misled	O
substantially	O
by	O
ﬁtting	O
relationships	O
to	O
the	O
attributes	O
,	O
instead	O
of	O
concentrating	O
on	O
getting	O
right	O
the	O
relationship	O
between	O
the	O
classes	O
and	O
the	O
attributes	O
.	O
canonical	O
discriminant	O
correlations	O
classes	O
form	O
some	O
kind	O
of	O
sequence	O
,	O
so	O
that	O
the	O
population	O
means	O
are	O
strung	O
out	O
along	O
some	O
assume	O
that	O
,	O
in¥	O
dimensional	O
space	O
,	O
the	O
sample	O
points	O
from	O
one	O
class	O
form	O
clusters	O
of	O
roughly	O
elliptical	O
shape	O
around	O
its	O
population	O
mean	O
.	O
in	O
general	O
,	O
if	O
there	O
areù	O
classes	O
,	O
theù	O
f	O
dimensional	O
subspace	O
.	O
on	O
the	O
other	O
hand	O
,	O
it	O
happens	O
frequently	O
that	O
the	O
means	O
lie	O
in	O
aù	O
f	O
.	O
the	O
simplest	O
case	O
of	O
all	O
occurs	O
curve	O
that	O
lies	O
ind'	O
dimensional	O
space	O
,	O
whered	O
f	O
and	O
the	O
population	O
means	O
lie	O
along	O
a	O
straight	O
line	O
.	O
canonical	O
discriminants	O
whend	O
are	O
a	O
way	O
of	O
systematically	O
projecting	O
the	O
mean	O
vectors	O
in	O
an	O
optimal	O
way	O
to	O
maximise	O
the	O
ratio	O
of	O
between-mean	O
distances	O
to	O
within-cluster	O
distances	O
,	O
successive	O
discriminants	O
being	O
orthogonal	O
to	O
earlier	O
discriminants	O
.	O
thus	O
the	O
ﬁrst	O
canonical	O
discriminant	O
gives	O
the	O
best	O
single	O
linear	O
combination	O
of	O
attributes	O
that	O
discriminates	O
between	O
the	O
populations	O
.	O
the	O
second	O
canonical	O
discriminant	O
is	O
the	O
best	O
single	O
linear	O
combination	O
orthogonal	O
to	O
the	O
ﬁrst	O
,	O
and	O
so	O
on	O
.	O
the	O
success	O
of	O
these	O
discriminants	O
is	O
measured	O
by	O
the	O
canonical	O
correlations	O
.	O
if	O
the	O
canonical	O
discriminant	O
matrix	O
divided	O
by	O
the	O
sum	O
of	O
all	O
the	O
eigenvalues	O
represents	O
the	O
this	O
gives	O
a	O
measure	B
of	O
collinearity	O
of	O
the	O
class	O
means	O
.	O
when	O
the	O
classes	O
form	O
an	O
ordered	O
sequence	O
,	O
for	O
example	B
soil	O
types	O
might	O
be	O
ordered	O
by	O
wetness	O
,	O
the	O
class	O
means	O
typically	O
proportion	O
of	O
total	O
variation	O
explained	O
by	O
ﬁrst	O
k	O
(	O
=1,2,3,4	O
)	O
canonical	O
discriminants	O
this	O
is	O
based	O
on	O
the	O
idea	O
of	O
describing	O
how	O
the	O
means	O
for	O
the	O
various	O
populations	O
differ	O
in	O
attribute	O
space	O
.	O
each	O
class	O
(	O
population	O
)	O
mean	O
deﬁnes	O
a	O
point	O
in	O
attribute	O
space	O
,	O
and	O
,	O
at	O
its	O
simplest	O
,	O
we	O
wish	O
to	O
know	O
if	O
there	O
is	O
some	O
simple	O
relationship	O
between	O
these	O
class	O
the	O
ﬁrst	O
canonical	O
correlation	O
is	O
close	O
to	O
unity	O
,	O
theù	O
means	O
lie	O
along	O
a	O
straight	O
line	O
nearly	O
.	O
f	O
th	O
canonical	O
correlation	O
is	O
near	O
zero	O
,	O
the	O
means	O
lie	O
ind'	O
dimensional	O
space	O
.	O
if	O
thede	O
means	O
,	O
for	O
example	B
,	O
if	O
they	O
lie	O
along	O
a	O
straight	O
line	O
.	O
the	O
sum	O
of	O
the	O
ﬁrstd	O
eigenvalues	O
of	O
variation	O
here	O
is	O
trc3ó	O
“	O
proportion	O
of	O
total	O
variation	O
”	O
explained	O
by	O
the	O
ﬁrstd	O
canonical	O
discriminants	O
.	O
the	O
total	O
g	O
.	O
we	O
calculate	O
,	O
as	O
fractk	O
,	O
the	O
values	O
of	O
c6ù	O
e	O
}	O
ù	O
c3ù	O
ed	O
gh	O
lie	O
along	O
a	O
curve	O
in	O
low	O
dimensional	O
space	O
.	O
theù	O
’	O
s	O
are	O
the	O
squares	O
of	O
the	O
canonical	O
correlations	O
.	O
the	O
signiﬁcance	O
of	O
theù	O
’	O
s	O
can	O
be	O
judged	O
from	O
the	O
	O
statistics	O
produced	O
by	O
“	O
manova	O
”	O
.	O
this	O
representation	O
of	O
linear	O
discrimination	O
,	O
which	O
is	O
due	O
to	O
fisher	O
(	O
1936	O
)	O
,	O
is	O
discussed	O
also	O
in	O
section	O
3.2.	O
departure	O
from	O
normality	O
the	O
assumption	O
of	O
multivariate	O
normality	O
underlies	O
much	O
of	O
classical	O
discrimination	O
pro-	O
cedures	O
.	O
but	O
the	O
effects	O
of	O
departures	O
from	O
normality	O
on	O
the	O
methods	O
are	O
not	O
easily	O
or	O
clearly	O
understood	O
.	O
moreover	O
,	O
in	O
analysing	O
multiresponse	O
data	O
,	O
it	O
is	O
not	O
known	O
how	O
ro-	O
bust	O
classical	O
procedures	O
are	O
to	O
departures	O
from	O
multivariate	O
normality	O
.	O
most	O
studies	O
on	O
robustness	O
depend	O
on	O
simulation	O
studies	O
.	O
thus	O
,	O
it	O
is	O
useful	O
to	O
have	O
measures	O
for	O
verifying	O
the	O
reasonableness	O
of	O
assuming	O
normality	O
for	O
a	O
given	O
dataset	O
.	O
if	O
available	O
,	O
such	O
a	O
measure	B
would	O
be	O
helpful	O
in	O
guiding	O
the	O
subsequent	O
analysis	O
of	O
the	O
data	O
to	O
make	O
it	O
more	O
normally	O
distributed	O
,	O
or	O
suggesting	O
the	O
most	O
appropriate	O
discrimination	O
method	O
.	O
andrews	O
et	O
al	O
.	O
eù	O
for	O
dd	O
e	O
}	O
ù	O
	O
ÿ	O
ù	O
	O
	O
|	O
e	O
·	O
|	O
	O
e	O
ê	O
g	O
d	O
	O
f	O
~	O
³	O
~	O
8	O
~	O
	O
sec	O
.	O
7.3	O
]	O
characterisation	O
of	O
datasets	O
115	O
(	O
1973	O
)	O
,	O
whose	O
excellent	O
presentation	O
we	O
follow	O
in	O
this	O
section	O
,	O
discuss	O
a	O
variety	O
of	O
methods	O
for	O
assessing	O
normality	O
.	O
with	O
multiresponse	O
data	O
,	O
the	O
possibilities	O
for	O
departure	O
from	O
joint	O
normality	O
are	O
many	O
and	O
varied	O
.	O
one	O
implication	O
of	O
this	O
is	O
the	O
need	O
for	O
a	O
variety	O
of	O
techniques	O
with	O
differing	O
sensitivities	O
to	O
the	O
different	O
types	O
of	O
departure	O
and	O
to	O
the	O
effects	O
that	O
such	O
departures	O
have	O
on	O
the	O
subsequent	O
analysis	O
.	O
of	O
great	O
importance	O
here	O
is	O
the	O
degree	O
of	O
commitment	O
one	O
wishes	O
to	O
make	O
to	O
the	O
coordinate	O
system	O
for	O
the	O
multiresponse	O
observations	O
.	O
at	O
one	O
extreme	O
is	O
the	O
situation	O
where	O
the	O
interest	O
is	O
completely	O
conﬁned	O
to	O
the	O
observed	O
coordinates	O
.	O
in	O
this	O
case	O
,	O
the	O
marginal	O
distributions	O
of	O
each	O
of	O
the	O
observed	O
variables	O
and	O
conditional	O
distributions	O
of	O
certain	O
of	O
these	O
given	O
certain	O
others	O
would	O
be	O
the	O
objects	O
of	O
interest	O
.	O
at	O
the	O
other	O
extreme	O
,	O
the	O
class	O
of	O
all	O
nonsingular	O
linear	O
transformations	O
of	O
the	O
variables	O
would	O
be	O
of	O
interest	O
.	O
one	O
possibility	O
is	O
to	O
look	O
at	O
all	O
possible	O
linear	O
combinations	O
of	O
the	O
variables	O
and	O
ﬁnd	O
the	O
maximum	O
departure	O
from	O
univariate	O
normality	O
in	O
these	O
combinations	O
(	O
machado	O
,	O
1983	O
)	O
.	O
mardia	O
et	O
al	O
.	O
(	O
1979	O
)	O
give	O
multivariate	O
measures	O
of	O
skewness	O
and	O
kurtosis	O
that	O
are	O
invariant	O
to	O
afﬁne	O
transformations	O
of	O
the	O
data	O
:	O
critical	O
values	O
of	O
these	O
statistics	O
for	O
small	O
samples	O
are	O
given	O
in	O
mardia	O
(	O
1974	O
)	O
.	O
these	O
measures	O
are	O
difﬁcult	O
to	O
compare	O
across	O
datasets	O
with	O
differing	O
dimensionality	O
.	O
they	O
also	O
have	O
the	O
disadvantage	O
that	O
they	O
do	O
not	O
reduce	O
to	O
the	O
usual	O
univariate	O
statistics	O
when	O
the	O
attributes	O
are	O
independent	O
.	O
our	O
approach	O
is	O
to	O
concentrate	O
on	O
the	O
original	O
coordinates	O
by	O
looking	O
at	O
their	O
marginal	O
distributions	O
.	O
moreover	O
,	O
the	O
emphasis	O
here	O
is	O
on	O
a	O
measure	B
of	O
non-normality	O
,	O
rather	O
than	O
on	O
a	O
test	O
that	O
tells	O
us	O
how	O
statistically	O
signiﬁcant	O
is	O
the	O
departure	O
from	O
normality	O
.	O
see	O
ozturk	O
&	O
romeu	O
(	O
1992	O
)	O
for	O
a	O
review	O
of	O
methods	O
for	O
testing	O
multivariate	O
normality	O
.	O
univariate	O
skewness	O
and	O
kurtosis	O
measure	B
is	O
deﬁned	O
via	O
the	O
ratio	O
of	O
the	O
fourth	O
moment	O
about	O
the	O
mean	O
to	O
the	O
fourth	O
power	O
of	O
the	O
standard	O
deviation	O
:	O
is	O
generally	O
known	O
as	O
the	O
kurtosis	O
of	O
the	O
distribution	O
.	O
however	O
,	O
we	O
will	O
itself	O
as	O
the	O
measure	B
of	O
kurtosis	O
:	O
since	O
we	O
only	O
use	O
this	O
measure	B
relative	O
to	O
other	O
measurements	O
of	O
the	O
same	O
quantity	O
within	O
this	O
book	O
,	O
this	O
slight	O
abuse	O
of	O
the	O
term	O
kurtosis	O
the	O
usual	O
measure	B
of	O
univariate	O
skewness	O
(	O
kendall	O
et	O
al.	O
,	O
1983	O
)	O
isô	O
|	O
,	O
which	O
is	O
the	O
ratio	O
of	O
|ñô	O
	O
another	O
although	O
,	O
for	O
test	O
purposes	O
,	O
it	O
is	O
usual	O
to	O
quote	O
the	O
square	O
of	O
this	O
quantity	O
:	O
õ	O
k	O
the	O
quantityõ	O
refer	O
toõ	O
8	O
,	O
and	O
h	O
|	O
°	O
andõ	O
may	O
be	O
tolerated	O
.	O
for	O
the	O
normal	O
distribution	O
,	O
the	O
measures	O
areõ	O
	O
byôß|	O
g	O
.	O
as	O
a	O
single	O
ô'|	O
in	O
populationû	O
denote	O
the	O
skewness	O
statistic	O
for	O
attribute	O
c	O
g	O
,	O
averaged	O
over	O
all	O
attributes	O
and	O
over	O
all	O
populations	O
.	O
this	O
gives	O
the	O
measure	B
cb	O
¶¹ö¨é	O
.	O
for	O
a	O
normal	O
population	O
,	O
éed	O
édd	O
¶¹ö¨é	O
¶âöyé	O
are	O
zero	O
and	O
2	O
respectively	O
.	O
similarly	O
,	O
we	O
ﬁnd	O
variables	O
,	O
the	O
theoretical	O
values	O
ofédd	O
we	O
will	O
say	O
that	O
the	O
skewness	O
is	O
zero	O
and	O
the	O
kurtosis	O
is	O
3	O
,	O
although	O
the	O
usual	O
deﬁnition	O
of	O
kurtosis	O
gives	O
a	O
value	O
of	O
zero	O
for	O
a	O
normal	O
distribution	O
.	O
mean	O
skewness	O
and	O
kurtosis	O
k	O
lk	O
µ0cby	O
µ0cby	O
ú8	O
the	O
mean	O
cubed	O
deviation	O
from	O
the	O
mean	O
to	O
the	O
cube	O
of	O
the	O
standard	O
deviation	O
measure	B
of	O
skewness	O
for	O
the	O
whole	O
dataset	O
,	O
we	O
quote	O
the	O
mean	O
of	O
the	O
absolute	O
value	O
of	O
is	O
zero	O
:	O
for	O
uniform	B
and	O
exponential	O
ô	O
|	O
	O
g	O
	O
h	O
å	O
	O
	O
|	O
õ	O
g	O
h	O
å	O
	O
	O
	O
~	O
	O
~	O
	O
i	O
i	O
i	O
116	O
methods	O
for	O
comparison	O
[	O
ch	O
.	O
7	O
cb	O
the	O
mean	O
of	O
the	O
univariate	O
standardised	O
fourth	O
momentõ	O
and	O
populations	O
.	O
this	O
gives	O
the	O
measureõ	O
	O
.	O
for	O
a	O
normal	O
population	O
,	O
õ	O
the	O
corresponding	O
ﬁgures	O
for	O
uniform	B
and	O
exponential	O
variables	O
are	O
1.8	O
and	O
9	O
,	O
respectively	O
.	O
univariate	O
skewness	O
and	O
kurtosis	O
of	O
correlated	O
attributes	O
the	O
univariate	O
measures	O
above	O
have	O
very	O
large	O
variances	O
if	O
the	O
attributes	O
are	O
highly	O
corre-	O
lated	O
.	O
it	O
may	O
therefore	O
be	O
desirable	O
to	O
transform	O
to	O
uncorrelated	O
variables	O
before	O
ﬁnding	O
the	O
univariate	O
skewness	O
and	O
kurtosis	O
measures	O
.	O
this	O
may	O
be	O
achieved	O
via	O
the	O
symmetric	O
inverse	O
square-root	O
of	O
the	O
covariance	O
matrix	O
.	O
the	O
corresponding	O
kurtosis	O
and	O
skewness	O
8	O
exactly	O
,	O
and	O
g	O
,	O
averaged	O
over	O
all	O
attributes	O
bk	O
»	O
say	O
)	O
may	O
be	O
more	O
reliable	O
for	O
correlated	O
attributes	O
.	O
by	O
construction	O
,	O
these	O
measures	O
reduce	O
to	O
the	O
univariate	O
values	O
if	O
the	O
attributes	O
are	O
uncorre-	O
lated	O
.	O
although	O
they	O
were	O
calculated	O
for	O
all	O
the	O
datasets	O
,	O
these	O
particular	O
measures	O
are	O
not	O
quoted	O
in	O
the	O
tables	O
,	O
as	O
they	O
are	O
usually	O
similar	O
to	O
the	O
univariate	O
statistics	O
.	O
bk	O
»	O
andédd	O
measure	B
(	O
d	O
	O
ãdì	O
information	O
theoretic	O
measures	O
7.3.3	O
for	O
the	O
most	O
part	O
,	O
the	O
statistical	B
measures	O
above	O
were	O
based	O
on	O
the	O
assumption	O
of	O
continuous	O
attributes	O
.	O
the	O
measures	O
we	O
discuss	O
now	O
are	O
motivated	O
by	O
information	O
theory	O
and	O
are	O
most	O
appropriate	O
for	O
discrete	O
(	O
and	O
indeed	O
categorical	O
)	O
attributes	O
,	O
although	O
they	O
are	O
able	O
to	O
deal	O
with	O
continuous	O
attributes	O
also	O
.	O
for	O
this	O
reason	O
,	O
these	O
measures	O
are	O
very	O
much	O
used	O
by	O
the	O
machine	O
learning	O
community	O
,	O
and	O
are	O
often	O
used	O
as	O
a	O
basis	O
for	O
splitting	O
criteria	O
when	O
building	O
decision	O
trees	O
.	O
they	O
correspond	O
to	O
the	O
deviance	O
statistics	O
that	O
arise	O
in	O
the	O
analysis	O
of	O
contingency	O
tables	O
(	O
mccullagh	O
&	O
nelder	O
,	O
1989	O
)	O
.	O
for	O
a	O
basic	O
introduction	O
to	O
the	O
subject	O
of	O
information	O
theory	O
,	O
see	O
,	O
for	O
example	B
,	O
jones	O
(	O
1979	O
)	O
.	O
entropy	O
of	O
attributes	O
,	O
entropy	O
is	O
a	O
measure	B
of	O
randomness	O
in	O
a	O
random	O
variable	O
.	O
in	O
general	O
terms	O
the	O
entropy	O
continuous	O
variable	O
with	O
given	O
variance	O
.	O
maximal	O
entropy	O
is	O
attained	O
for	O
normal	O
takes	O
on	O
the	O
i	O
’	O
th	O
value	O
.	O
conventionally	O
,	O
logarithms	O
are	O
to	O
base	O
2	O
,	O
and	O
entropy	O
is	O
then	O
said	O
to	O
be	O
measured	O
in	O
units	O
called	O
``	O
bits	O
''	O
(	O
binary	O
information	O
units	O
)	O
.	O
in	O
what	O
follows	O
,	O
all	O
logarithms	O
are	O
to	O
base	O
2.	O
the	O
special	O
cases	O
to	O
remember	O
are	O
:	O
equal	O
probabilities	O
(	O
uniform	B
distribution	O
)	O
.	O
the	O
entropy	O
of	O
a	O
discrete	O
random	O
variable	O
,	O
the	O
maximal	O
.	O
	O
c1y	O
	O
cby	O
g	O
of	O
a	O
discrete	O
random	O
variable	O
x	O
is	O
deﬁned	O
as	O
the	O
sum	O
	O
c1y	O
	O
log	O
is	O
the	O
probability	O
thaty	O
whereù	O
	O
are	O
equal	O
.	O
if	O
there	O
ared	O
possible	O
values	O
fory	O
is	O
maximal	O
when	O
allù	O
entropy	O
is	O
logd	O
variables	O
,	O
and	O
this	O
maximal	O
entropy	O
is°	O
the	O
attributes	O
we	O
take	O
the	O
c1y	O
	O
c1y	O
logc³.ü	O
g	O
averaged	O
over	O
all	O
attributesy	O
c1y	O
|3~d½½ó~	O
ê	O
:	O
	O
±	O
.	O
in	O
the	O
context	O
of	O
classiﬁcation	B
schemes	O
,	O
the	O
point	O
to	O
note	O
is	O
that	O
an	O
attribute	O
that	O
does	O
not	O
vary	O
at	O
all	O
,	O
and	O
therefore	O
has	O
zero	O
entropy	O
,	O
contains	O
no	O
information	O
for	O
discriminating	O
between	O
classes	O
.	O
the	O
entropy	O
of	O
a	O
collection	O
of	O
attributes	O
is	O
not	O
simply	O
related	O
to	O
the	O
individual	O
entropies	O
,	O
but	O
,	O
as	O
a	O
basic	O
measure	B
,	O
we	O
can	O
average	O
the	O
entropy	O
over	O
all	O
the	O
attributes	O
and	O
take	O
this	O
as	O
a	O
global	O
measure	B
of	O
entropy	O
of	O
the	O
attributes	O
collectively	O
.	O
thus	O
,	O
as	O
a	O
measure	B
of	O
entropy	O
of	O
	O
~	O
	O
ú	O
i	O
û	O
g	O
g	O
	O
	O
à	O
	O
ù	O
ù	O
	O
	O
å	O
	O
g	O
y	O
û	O
g	O
	O
¥	O
þ	O
|	O
à	O
	O
	O
	O
g	O
examples	O
,	O
this	O
means	O
choosing	O
sec	O
.	O
7.3	O
]	O
characterisation	O
of	O
datasets	O
117	O
this	O
measure	B
is	O
strictly	O
appropriate	O
only	O
for	O
independent	O
attributes	O
.	O
the	O
deﬁnition	O
of	O
entropy	O
for	O
continuous	O
distributions	O
is	O
analogous	O
to	O
the	O
discrete	O
case	O
,	O
with	O
an	O
integral	O
replacing	O
the	O
summation	O
term	O
.	O
this	O
deﬁnition	O
is	O
no	O
use	O
for	O
empirical	O
data	O
,	O
however	O
,	O
unless	O
some	O
very	O
drastic	O
assumptions	O
are	O
made	O
(	O
for	O
example	B
assuming	O
that	O
the	O
data	O
have	O
a	O
normal	O
distribution	O
)	O
,	O
and	O
we	O
are	O
forced	O
to	O
apply	O
the	O
discrete	O
deﬁnition	O
to	O
all	O
empirical	O
data	O
.	O
for	O
the	O
measures	O
deﬁned	O
below	O
,	O
we	O
discretised	O
all	O
numerical	O
data	O
into	O
equal-length	O
intervals	O
.	O
the	O
number	O
of	O
intervals	O
was	O
chosen	O
so	O
that	O
there	O
was	O
a	O
fair	O
expectation	O
that	O
there	O
would	O
be	O
about	O
ten	O
observations	O
per	O
cell	O
in	O
the	O
two-way	O
table	O
of	O
in	O
many	O
of	O
our	O
datasets	O
,	O
some	O
classes	O
have	O
very	O
low	O
probabilities	O
of	O
occurrence	O
,	O
and	O
,	O
for	O
practical	O
purposes	O
,	O
the	O
very	O
infrequent	O
classes	O
play	O
little	O
part	O
in	O
the	O
assessment	O
of	O
classiﬁcation	B
schemes	O
.	O
it	O
is	O
therefore	O
inappropriate	O
merely	O
to	O
count	O
the	O
number	O
of	O
classes	O
intervals	O
.	O
a	O
more	O
reﬁned	O
procedure	O
would	O
have	O
the	O
number	O
and	O
width	O
of	O
intervals	O
varying	O
from	O
attribute	O
to	O
attribute	O
,	O
and	O
from	O
dataset	O
to	O
dataset	O
.	O
unless	O
the	O
data	O
are	O
very	O
extensive	O
,	O
the	O
estimated	O
entropies	O
,	O
even	O
for	O
discrete	O
variables	O
,	O
are	O
likely	O
to	O
be	O
severely	O
biased	O
.	O
blyth	O
(	O
1958	O
)	O
discusses	O
methods	O
of	O
reducing	O
the	O
bias	O
.	O
ù	O
cells	O
in	O
a	O
two-way	O
table	O
of	O
attribute	O
(	O
with	O
discrete	O
attribute	O
by	O
class	O
.	O
as	O
there	O
are	O
the	O
simplest	O
,	O
but	O
not	O
the	O
best	O
,	O
procedure	O
is	O
to	O
divide	O
the	O
range	O
of	O
the	O
attribute	O
into	O
equal	O
levels	O
)	O
by	O
class	O
(	O
withù	O
classes	O
)	O
,	O
and	O
there	O
are	O
°	O
.	O
entropy	O
of	O
classes	O
,	O
	O
and	O
use	O
this	O
as	O
a	O
measure	B
of	O
complexity	O
.	O
an	O
alternative	O
is	O
to	O
use	O
the	O
entropy	O
c	O
g	O
of	O
the	O
	O
c	O
	O
logü	O
whereü	O
$	O
	O
is	O
the	O
prior	O
probability	O
for	O
classûç	O
.	O
entropy	O
is	O
related	O
to	O
the	O
average	O
length	O
of	O
a	O
for	O
example	B
)	O
.	O
since	O
class	O
is	O
essentially	O
discrete	O
,	O
the	O
class	O
entropy	O
c	O
when	O
the	O
classes	O
are	O
equally	O
likely	O
,	O
so	O
that	O
c	O
g	O
has	O
maximal	O
value	O
\	O
>	O
@	O
ýd	O
as	O
an	O
effective	O
of	O
classes	O
.	O
a	O
useful	O
way	O
of	O
looking	O
at	O
the	O
entropy	O
c	O
is	O
at	O
most	O
logù	O
,	O
whereù	O
is	O
to	O
regard³	O
joint	O
entropy	O
of	O
class	O
and	O
attribute	O
,	O
	O
c	O
the	O
joint	O
entropy	O
c	O
andy	O
{	O
1~	O
g	O
of	O
two	O
variables	O
{	O
	O
and	O
the	O
-th	O
value	O
of	O
attributey	O
	O
denotes	O
the	O
joint	O
{	O
h~	O
g	O
.	O
if¥	O
combined	O
system	O
of	O
variables	O
,	O
i.e	O
.	O
the	O
pair	O
of	O
variablesc	O
probability	O
of	O
observing	O
classû	O
	O
c	O
{	O
1~	O
log¥¹	O
¥¹	O
andy	O
{	O
h~	O
vc	O
g	O
of	O
two	O
variables	O
{	O
{	O
h~	O
the	O
mutual	O
informationvc	O
the	O
joint	O
probability	O
of	O
observing	O
classûç	O
and	O
the	O
-th	O
value	O
of	O
attributey	O
	O
denotes	O
{	O
h~	O
g	O
is	O
zero	O
.	O
if¥¹	O
there	O
is	O
no	O
shared	O
information	O
,	O
and	O
the	O
mutual	O
informationvc	O
probability	O
of	O
classû	O
isü	O
$	O
	O
,	O
and	O
if	O
the	O
marginal	O
probability	O
of	O
attributey	O
taking	O
on	O
its	O
-th	O
	O
,	O
then	O
the	O
mutual	O
information	O
is	O
deﬁned	O
to	O
be	O
(	O
note	O
that	O
there	O
is	O
no	O
minus	O
sign	O
)	O
:	O
value	O
isù	O
this	O
is	O
a	O
simple	O
extension	O
of	O
the	O
notion	O
of	O
entropy	O
to	O
the	O
combined	O
system	O
of	O
variables	O
.	O
mutual	O
information	O
of	O
class	O
and	O
attribute	O
,	O
is	O
a	O
measure	B
of	O
common	O
infor-	O
mation	O
or	O
entropy	O
shared	O
between	O
the	O
two	O
variables	O
.	O
if	O
the	O
two	O
variables	O
are	O
independent	O
,	O
is	O
a	O
measure	B
of	O
total	O
entropy	O
of	O
the	O
,	O
the	O
joint	O
entropy	O
is	O
number	O
of	O
classes	O
.	O
{	O
1~	O
deﬁned	O
to	O
be	O
:	O
,	O
if	O
the	O
marginal	O
class	O
probability	O
distribution	O
:	O
variable	O
length	O
coding	O
scheme	O
,	O
and	O
there	O
are	O
direct	O
links	O
to	O
decision	O
trees	O
(	O
see	O
jones	O
,	O
1979	O
is	O
the	O
number	O
h	O
	O
ù	O
	O
f	O
c	O
{	O
g	O
{	O
{	O
g	O
	O
	O
à	O
	O
ü	O
	O
{	O
{	O
g	O
{	O
g	O
y	O
g	O
y	O
y	O
	O
y	O
g	O
	O
	O
à	O
	O
	O
	O
	O
	O
û	O
y	O
g	O
y	O
y	O
118	O
methods	O
for	O
comparison	O
[	O
ch	O
.	O
7	O
equivalent	O
deﬁnitions	O
are	O
:	O
{	O
h~	O
{	O
h~	O
since	O
there	O
are	O
many	O
attributes	O
,	O
we	O
have	O
tabulated	O
an	O
average	O
of	O
the	O
mutual	O
information	O
deﬁned	O
formally	O
by	O
the	O
equation	O
in	O
which	O
it	O
appears	O
above	O
,	O
but	O
it	O
has	O
a	O
distinct	O
meaning	O
,	O
namely	O
,	O
the	O
entropy	O
(	O
i.e	O
.	O
randomness	O
or	O
noise	O
)	O
of	O
the	O
class	O
variable	O
that	O
is	O
not	O
removed	O
by	O
is	O
zero	O
,	O
and	O
this	O
occurs	O
when	O
class	O
and	O
attribute	O
are	O
independent	O
.	O
the	O
maximum	O
mutual	O
information	O
is	O
zero	O
.	O
suppose	O
,	O
for	O
example	B
,	O
that	O
is	O
zero	O
.	O
this	O
would	O
mean	O
that	O
the	O
value	O
of	O
class	O
is	O
ﬁxed	O
(	O
non-random	O
)	O
once	O
the	O
,	O
in	O
contains	O
all	O
the	O
information	O
needed	O
to	O
specify	O
the	O
class	O
.	O
the	O
ü	O
{	O
1~	O
logc	O
vc	O
¥o	O
e	O
cby	O
þ	O
c	O
{	O
1~	O
	O
}	O
	O
c	O
vc	O
{	O
1~	O
	O
}	O
	O
cby	O
c1y	O
vc	O
{	O
1~	O
the	O
conditional	O
entropy	O
c	O
vc	O
g	O
,	O
for	O
example	B
,	O
which	O
we	O
have	O
not	O
yet	O
deﬁned	O
,	O
may	O
be	O
knowing	O
the	O
value	O
of	O
the	O
attribute	O
x.	O
minimum	O
mutual	O
informationvc	O
c1y	O
g	O
occurs	O
when	O
one	O
of	O
c	O
g	O
or	O
	O
c	O
{	O
h~	O
vc	O
value	O
ofy	O
is	O
then	O
completely	O
predictable	O
from	O
the	O
attributey	O
the	O
sense	O
that	O
attributey	O
is	O
known	O
.	O
class	O
{	O
{	O
h~	O
g	O
are	O
g+æ	O
minc6	O
c	O
	O
cby	O
corresponding	O
limits	O
ofvc	O
{	O
h~	O
gg	O
vc	O
taken	O
over	O
all	O
attributesy	O
|.~ede~	O
{	O
h~	O
yh	O
ê	O
:	O
vc	O
{	O
h~	O
{	O
1~	O
vc	O
vc	O
the	O
information	O
required	O
to	O
specify	O
the	O
class	O
is	O
c	O
be	O
completely	O
successful	O
unless	O
it	O
provides	O
at	O
least	O
c	O
g	O
,	O
and	O
no	O
classiﬁcation	B
scheme	O
can	O
g	O
bits	O
of	O
useful	O
information	O
.	O
g	O
of	O
all	O
attributes	O
together	O
(	O
herey	O
vector	O
of	O
attributesc1y	O
{	O
h~	O
that	O
the	O
useful	O
information	O
vc	O
|3~d½ó½~	O
g	O
)	O
is	O
greater	O
than	O
the	O
sum	O
of	O
the	O
individual	O
informations	O
{	O
1~	O
½½½	O
{	O
h~	O
g	O
.	O
however	O
,	O
in	O
the	O
simplest	O
(	O
but	O
most	O
unrealistic	O
)	O
case	O
that	O
all	O
e	O
vc	O
vc	O
yê	O
{	O
h~	O
½ó½	O
{	O
1~	O
{	O
1~	O
vc	O
e	O
!	O
vc	O
vc	O
g	O
and	O
the	O
average	O
mutual	O
informationû	O
the	O
ratio	O
between	O
the	O
class	O
entropy	O
c	O
{	O
h~	O
g	O
.	O
	O
c	O
en.attr	O
	O
{	O
h~	O
v÷c	O
of	O
course	O
,	O
we	O
might	O
do	O
better	O
by	O
taking	O
the	O
attributes	O
with	O
highest	O
mutual	O
information	O
,	O
but	O
,	O
in	O
any	O
case	O
,	O
the	O
assumption	O
of	O
independent	O
useful	O
bits	O
of	O
information	O
is	O
very	O
dubious	O
in	O
any	O
case	O
,	O
so	O
this	O
simple	O
measure	B
is	O
probably	O
quite	O
sufﬁcient	O
:	O
this	O
information	O
is	O
to	O
come	O
from	O
the	O
attributes	O
taken	O
together	O
,	O
and	O
it	O
is	O
quite	O
possible	O
stands	O
for	O
the	O
this	O
average	O
mutual	O
information	O
gives	O
a	O
measure	B
of	O
how	O
much	O
useful	O
information	O
about	O
classes	O
is	O
provided	O
by	O
the	O
average	O
attribute	O
.	O
mutual	O
information	O
may	O
be	O
used	O
as	O
a	O
splitting	O
criterion	O
in	O
decision	O
tree	O
algorithms	O
,	O
and	O
is	O
preferable	O
to	O
the	O
gain	O
ratio	O
criterion	O
of	O
c4.5	O
(	O
pagallo	O
&	O
haussler	O
,	O
1990	O
)	O
.	O
equivalent	O
number	O
of	O
attributes	O
,	O
en.attr	O
attributes	O
are	O
independent	O
,	O
we	O
would	O
have	O
in	O
this	O
case	O
the	O
attributes	O
contribute	O
independent	O
bits	O
of	O
useful	O
information	O
for	O
classiﬁcation	B
purposes	O
,	O
and	O
we	O
can	O
count	O
up	O
how	O
many	O
attributes	O
would	O
be	O
required	O
,	O
on	O
average	O
,	O
by	O
taking	O
y	O
g	O
	O
à	O
	O
	O
	O
¥	O
	O
	O
ù	O
	O
g	O
	O
y	O
g	O
	O
	O
c	O
{	O
g	O
g	O
y	O
g	O
y	O
g	O
	O
	O
c	O
{	O
g	O
{	O
¼	O
y	O
g	O
y	O
g	O
	O
	O
g	O
¼	O
{	O
g	O
{	O
¼	O
y	O
y	O
g	O
y	O
{	O
¼	O
y	O
¼	O
{	O
g	O
{	O
¼	O
y	O
g	O
y	O
°	O
æ	O
y	O
{	O
g	O
~	O
	O
y	O
g	O
y	O
û	O
y	O
g	O
	O
¥	O
þ	O
|	O
à	O
	O
g	O
{	O
{	O
y	O
y	O
ê	O
y	O
|	O
g	O
e	O
y	O
ê	O
y	O
g	O
	O
y	O
|	O
g	O
e	O
g	O
{	O
v	O
c	O
y	O
{	O
g	O
û	O
y	O
g	O
sec	O
.	O
7.3	O
]	O
characterisation	O
of	O
datasets	O
119	O
{	O
1~	O
values	O
of	O
the	O
ratio	O
noisiness	O
of	O
attributes	O
,	O
ns.ratio	O
if	O
the	O
useful	O
information	O
is	O
only	O
a	O
small	O
fraction	O
of	O
the	O
total	O
information	O
,	O
we	O
may	O
say	O
that	O
there	O
is	O
a	O
large	O
amount	O
of	O
noise	O
.	O
thus	O
,	O
takeû	O
about	O
class	O
,	O
andû	O
	O
cby	O
g	O
as	O
a	O
measure	B
of	O
useful	O
information	O
vc	O
g	O
as	O
a	O
measure	B
as	O
non-useful	O
information	O
.	O
then	O
large	O
{	O
h~	O
ns.ratio	O
	O
imply	O
a	O
dataset	O
that	O
contains	O
much	O
irrelevant	O
information	O
(	O
noise	O
)	O
.	O
such	O
datasets	O
could	O
be	O
condensed	O
considerably	O
without	O
affecting	O
the	O
performance	O
of	O
the	O
classiﬁer	B
,	O
for	O
example	B
by	O
removing	O
irrelevant	O
attributes	O
,	O
by	O
reducing	O
the	O
number	O
of	O
discrete	O
levels	O
used	O
to	O
specify	O
the	O
attributes	O
,	O
or	O
perhaps	O
by	O
merging	O
qualitative	O
factors	O
.	O
the	O
notation	O
ns.ratio	O
denotes	O
the	O
noise-signal-ratio	O
.	O
note	O
that	O
this	O
is	O
the	O
reciprocal	O
of	O
the	O
more	O
usual	O
signal-noise-ratio	O
(	O
snr	O
)	O
.	O
irrelevant	O
attributes	O
vc	O
vc	O
	O
cby	O
{	O
h~	O
{	O
1~	O
attribute	O
.	O
this	O
context	O
,	O
interpreting	O
the	O
mutual	O
information	O
as	O
a	O
deviance	O
statistic	O
would	O
be	O
useful	O
,	O
and	O
we	O
can	O
give	O
a	O
lower	O
bound	O
to	O
statistically	O
signiﬁcant	O
values	O
for	O
mutual	O
information	O
.	O
and	O
class	O
are	O
,	O
in	O
fact	O
,	O
statistically	O
independent	O
,	O
and	O
suppose	O
is	O
large	O
,	O
then	O
it	O
is	O
approximately	O
equal	O
to	O
the	O
chi-	O
square	O
statistic	O
for	O
testing	O
the	O
independence	O
of	O
attribute	O
and	O
class	O
(	O
for	O
example	B
agresti	O
,	O
g	O
between	O
class	O
and	O
attributeyh	O
can	O
be	O
used	O
to	O
judge	O
yh	O
if	O
attributeyh	O
could	O
,	O
of	O
itself	O
,	O
contribute	O
usefully	O
to	O
a	O
classiﬁcation	B
scheme	O
.	O
attributes	O
{	O
h~	O
the	O
mutual	O
informationvc	O
y1	O
{	O
1~	O
g	O
would	O
not	O
,	O
by	O
themselves	O
,	O
be	O
useful	O
predictors	O
of	O
class	O
.	O
in	O
with	O
small	O
values	O
ofvc	O
suppose	O
that	O
attributey	O
has	O
distinct	O
levels	O
.	O
assuming	O
further	O
that	O
the	O
sample	O
size	O
thaty	O
{	O
h~	O
is	O
well	O
known	O
that	O
the	O
deviance	O
statistic³	O
vc	O
d	O
@	O
>	O
d	O
distribution	O
,	O
and	O
order	O
of	O
{	O
h~	O
g	O
has	O
an	O
approximate	O
1990	O
)	O
.	O
therefore³	O
vc	O
is	O
the	O
number	O
of	O
examples	O
,	O
and	O
fegh	O
,	O
whereù	O
the	O
hypothesis	O
testing	O
sense	O
)	O
if	O
its	O
value	O
exceedsc	O
of	O
classes	O
,	O
	O
in	O
our	O
measures	O
,	O
	O
continuous	O
attributes	O
we	O
chose	O
hf	O
number	O
of	O
levels	O
for	O
so-called	O
continuous	O
attributes	O
was	O
less	O
than	O
d	O
distribution	O
as	O
twice	O
the	O
number	O
of	O
degrees	O
of	O
freedom	O
,	O
dß	O
>	O
h	O
$	O
f	O
ù	O
.	O
if	O
we	O
adopt	O
.	O
with	O
our	O
chosen	O
value	O
of	O
,	O
this	O
is	O
of	O
orderfdhf	O
a	O
critical	O
level	O
for	O
the	O
fdg	O
fegih	O
as³¹c	O
magnitude	O
calculations	O
indicate	O
that	O
the	O
mutual	O
information	O
contributes	O
signiﬁcantly	O
(	O
in	O
is	O
the	O
number	O
is	O
the	O
number	O
of	O
discrete	O
levels	O
for	O
the	O
is	O
the	O
number	O
of	O
levels	O
for	O
integer	O
or	O
binary	O
attributes	O
,	O
and	O
for	O
(	O
so	O
that	O
,	O
on	O
average	O
,	O
there	O
were	O
about	O
10	O
observations	O
per	O
cell	O
in	O
the	O
two-way	O
table	O
of	O
attribute	O
by	O
class	O
)	O
,	O
but	O
occasionally	O
the	O
we	O
have	O
not	O
quoted	O
any	O
measure	B
of	O
this	O
form	O
,	O
as	O
almost	O
all	O
attributes	O
are	O
relevant	O
in	O
this	O
sense	O
(	O
and	O
this	O
measure	B
would	O
have	O
little	O
information	O
content	O
!	O
)	O
.	O
in	O
any	O
case	O
,	O
an	O
equivalent	O
measure	B
would	O
be	O
the	O
difference	O
between	O
the	O
actual	O
number	O
of	O
attributes	O
and	O
the	O
value	O
of	O
en.attr	O
.	O
correlated	O
normal	O
attributes	O
when	O
attributes	O
are	O
correlated	O
,	O
the	O
calculation	O
of	O
information	O
measures	O
becomes	O
much	O
more	O
difﬁcult	O
,	O
so	O
difﬁcult	O
,	O
in	O
fact	O
,	O
that	O
we	O
have	O
avoided	O
it	O
altogether	O
.	O
the	O
above	O
univariate	O
for	O
the	O
sake	O
of	O
argument	O
,	O
we	O
obtain	O
an	O
approximate	O
critical	O
level	O
for	O
the	O
mutual	O
information	O
for	O
continuous	O
attributes	O
.	O
fdg	O
y	O
g	O
	O
û	O
y	O
û	O
g	O
	O
û	O
v	O
c	O
y	O
g	O
û	O
y	O
g	O
	O
y	O
g	O
	O
y	O
	O
>	O
ë	O
þ	O
|	O
	O
þ	O
|	O
	O
	O
c	O
ù	O
	O
	O
	O
	O
°	O
ù	O
	O
	O
°	O
	O
>	O
ë	O
þ	O
|	O
	O
þ	O
|	O
ù	O
	O
c	O
	O
	O
³	O
	O
°	O
120	O
methods	O
for	O
comparison	O
[	O
ch	O
.	O
7	O
measures	O
take	O
no	O
account	O
of	O
any	O
lack	O
of	O
independence	O
,	O
and	O
are	O
therefore	O
very	O
crude	O
approx-	O
imations	O
to	O
reality	O
.	O
there	O
are	O
,	O
however	O
,	O
some	O
simple	O
results	O
concerning	O
the	O
multivariate	O
normal	O
distribution	O
,	O
for	O
which	O
the	O
entropy	O
is	O
logc³.ü	O
	O
±	O
¼	O
is	O
the	O
determinant	O
of	O
the	O
covariance	O
matrix	O
of	O
the	O
variables	O
.	O
similar	O
results	O
hold	O
for	O
mutual	O
information	O
,	O
and	O
there	O
are	O
then	O
links	O
with	O
the	O
statistical	B
measures	O
elaborated	O
in	O
section	O
7.3.2.	O
unfortunately	O
,	O
even	O
if	O
such	O
measures	O
were	O
used	O
for	O
our	O
datasets	O
,	O
most	O
datasets	O
are	O
so	O
far	O
from	O
normality	O
that	O
the	O
interpretation	O
of	O
the	O
resulting	O
measures	O
would	O
be	O
very	O
questionable	O
.	O
where¼	O
7.4	O
pre-processing	O
usually	O
there	O
is	O
no	O
control	O
over	O
the	O
form	O
or	O
content	O
of	O
the	O
vast	O
majority	O
of	O
datasets	O
.	O
generally	O
,	O
they	O
are	O
already	O
converted	O
from	O
whatever	O
raw	O
data	O
was	O
available	O
into	O
some	O
“	O
suitable	O
”	O
format	O
,	O
and	O
there	O
is	O
no	O
way	O
of	O
knowing	O
if	O
the	O
manner	O
in	O
which	O
this	O
was	O
done	O
was	O
consistent	O
,	O
or	O
perhaps	O
chosen	O
to	O
ﬁt	O
in	O
with	O
some	O
pre-conceived	O
type	O
of	O
analysis	O
.	O
in	O
some	O
datasets	O
,	O
it	O
is	O
very	O
clear	O
that	O
some	O
very	O
drastic	O
form	O
of	O
pre-processing	O
has	O
already	O
been	O
done	O
–	O
see	O
section	O
9.5.4	O
,	O
for	O
example	B
.	O
7.4.1	O
missing	O
values	O
	O
)	O
some	O
algorithms	O
(	O
e.g	O
.	O
naive	O
bayes	O
,	O
cart	O
,	O
cn2	O
,	O
bayes	O
tree	O
,	O
newid	O
,	O
c4.5	O
,	O
cal5	O
,	O
û	O
can	O
deal	O
with	O
missing	O
values	O
,	O
whereas	O
others	O
require	O
that	O
the	O
missing	O
values	O
be	O
replaced	O
.	O
the	O
procedure	O
discrim	O
was	O
not	O
able	O
to	O
handle	O
missing	O
values	O
,	O
although	O
this	O
can	O
be	O
done	O
in	O
principle	O
for	O
linear	O
discrimination	O
for	O
certain	O
types	O
of	O
missing	O
value	O
.	O
in	O
order	O
to	O
get	O
comparable	O
results	O
we	O
settled	O
on	O
a	O
general	O
policy	O
of	O
replacing	O
all	O
missing	O
values	O
.	O
where	O
an	O
attribute	O
value	O
was	O
missing	O
it	O
was	O
replaced	O
by	O
the	O
global	O
mean	O
or	O
median	O
for	O
that	O
attribute	O
.	O
if	O
the	O
class	O
value	O
was	O
missing	O
,	O
the	O
whole	O
observation	O
was	O
omitted	O
.	O
usually	O
,	O
the	O
proportion	O
of	O
cases	O
with	O
missing	O
information	O
was	O
very	O
low	O
.	O
as	O
a	O
separate	O
exercise	O
it	O
would	O
be	O
of	O
interest	O
to	O
learn	O
how	O
much	O
information	O
is	O
lost	O
(	O
or	O
gained	O
)	O
in	O
such	O
a	O
strategy	O
by	O
those	O
algorithms	O
that	O
can	O
handle	O
missing	O
values	O
.	O
unfortunately	O
,	O
there	O
are	O
various	O
ways	O
in	O
which	O
missing	O
values	O
might	O
arise	O
,	O
and	O
their	O
treatment	O
is	O
quite	O
different	O
.	O
for	O
example	B
,	O
a	O
clinician	O
may	O
normally	O
use	O
the	O
results	O
of	O
a	O
blood-test	O
in	O
making	O
a	O
diagnosis	O
.	O
if	O
the	O
blood-test	O
is	O
not	O
carried	O
out	O
,	O
perhaps	O
because	O
of	O
faulty	O
equipment	O
,	O
the	O
blood-test	O
measurements	O
are	O
missing	O
for	O
that	O
specimen	O
.	O
a	O
situation	O
that	O
may	O
appear	O
similar	O
,	O
results	O
from	O
doing	O
measurements	O
on	O
a	O
subset	O
of	O
the	O
population	O
,	O
for	O
example	B
only	O
doing	O
pregnancy	O
tests	O
on	O
women	O
,	O
where	O
the	O
test	O
is	O
not	O
relevant	O
for	O
men	O
(	O
and	O
so	O
is	O
missing	O
for	O
men	O
)	O
.	O
in	O
the	O
ﬁrst	O
case	O
,	O
the	O
measurements	O
are	O
missing	O
at	O
random	O
,	O
and	O
in	O
the	O
second	O
the	O
measurements	O
are	O
structured	O
,	O
or	O
hierarchical	O
.	O
although	O
the	O
treatment	O
of	O
these	O
two	O
cases	O
should	O
be	O
radically	O
different	O
,	O
the	O
necessary	O
information	O
is	O
often	O
lacking	O
.	O
in	O
at	O
least	O
one	O
dataset	O
(	O
technical	B
)	O
,	O
it	O
would	O
appear	O
that	O
this	O
problem	O
arises	O
in	O
a	O
very	O
extreme	O
manner	O
,	O
as	O
it	O
would	O
seem	O
that	O
missing	O
values	O
are	O
coded	O
as	O
zero	O
,	O
and	O
that	O
a	O
large	O
majority	O
of	O
observations	O
is	O
zero	O
.	O
7.4.2	O
feature	O
selection	O
and	O
extraction	O
some	O
datasets	O
are	O
so	O
large	O
that	O
many	O
algorithms	O
have	O
problems	O
just	O
entering	O
the	O
data	O
,	O
and	O
the	O
sheer	O
size	O
of	O
the	O
dataset	O
has	O
to	O
be	O
reduced	O
.	O
in	O
this	O
case	O
,	O
to	O
achieve	O
uniformity	O
,	O
a	O
data	O
°	O
¼	O
ó	O
¼	O
g	O
ó	O
{	O
sec	O
.	O
7.4	O
]	O
pre-processing	O
121	O
reduction	O
process	O
was	O
performed	O
in	O
advance	O
of	O
the	O
trials	O
.	O
again	O
it	O
is	O
of	O
interest	O
to	O
note	O
which	O
algorithms	O
can	O
cope	O
with	O
the	O
very	O
large	O
datasets	O
.	O
there	O
are	O
several	O
ways	O
in	O
which	O
data	O
reduction	O
can	O
take	O
place	O
.	O
for	O
example	B
,	O
the	O
karhunen-loeve	O
transformation	O
can	O
be	O
used	O
with	O
very	O
little	O
loss	O
of	O
information	O
–	O
see	O
section	O
9.6.1	O
for	O
an	O
example	B
.	O
another	O
way	O
of	O
reducing	O
the	O
number	O
of	O
variables	O
is	O
by	O
a	O
stepwise	O
procedure	O
in	O
a	O
linear	O
discriminant	O
procedure	O
,	O
for	O
example	B
.	O
this	O
was	O
tried	O
on	O
the	O
“	O
cut50	O
”	O
dataset	O
,	O
in	O
which	O
a	O
version	O
“	O
cut20	O
”	O
with	O
number	O
of	O
attributes	O
reduced	O
from	O
50	O
to	O
20	O
was	O
also	O
considered	O
.	O
results	O
for	O
both	O
these	O
versions	O
are	O
presented	O
,	O
and	O
make	O
for	O
an	O
interesting	O
paired	O
comparison	O
:	O
see	O
the	O
section	O
on	O
paired	O
comparisons	O
for	O
the	O
cut20	O
dataset	O
in	O
section	O
10.2.2.	O
in	O
some	O
datasets	O
,	O
particularly	O
image	B
segmentation	I
,	O
extra	O
relevant	O
information	O
can	O
be	O
included	O
.	O
for	O
example	B
,	O
we	O
can	O
use	O
the	O
prior	O
knowledge	O
that	O
examples	O
which	O
are	O
“	O
neigh-	O
bours	O
”	O
are	O
likely	O
to	O
have	O
the	O
same	O
class	O
.	O
a	O
dataset	O
of	O
this	O
type	O
is	O
considered	O
in	O
section	O
9.6.5	O
in	O
which	O
a	O
satellite	B
image	I
uses	O
the	O
fact	O
that	O
attributes	O
of	O
neighbouring	O
pixels	O
can	O
give	O
useful	O
information	O
in	O
classifying	O
the	O
given	O
pixel	O
.	O
especially	O
in	O
an	O
exploratory	O
study	O
,	O
practitioners	O
often	O
combine	O
attributes	O
in	O
an	O
attempt	O
to	O
increase	O
the	O
descriptive	O
power	O
of	O
the	O
resulting	O
decision	O
tree/rules	O
etc	O
.	O
for	O
example	B
,	O
it	O
that	O
is	O
important	O
rather	O
might	O
be	O
conjectured	O
that	O
it	O
is	O
the	O
sum	O
of	O
two	O
attributes	O
«	O
than	O
each	O
attribute	O
separately	O
.	O
alternatively	O
,	O
some	O
ratios	O
are	O
included	O
such	O
as	O
«	O
in	O
our	O
trials	O
we	O
did	O
not	O
introduce	O
any	O
such	O
combinations	O
.	O
on	O
the	O
other	O
hand	O
,	O
there	O
existed	O
already	O
some	O
linear	O
combinations	O
of	O
attributes	O
in	O
some	O
of	O
the	O
datasets	O
that	O
we	O
looked	O
at	O
.	O
we	O
took	O
the	O
view	O
that	O
these	O
combinations	O
were	O
included	O
because	O
the	O
dataset	O
provider	O
thought	O
that	O
these	O
particular	O
combinations	O
were	O
potentially	O
useful	O
.	O
although	O
capable	O
of	O
running	O
on	O
attributes	O
with	O
linear	O
dependencies	O
,	O
some	O
of	O
the	O
statistical	B
procedures	O
prefer	O
attributes	O
that	O
are	O
linearly	O
independent	O
,	O
so	O
when	O
it	O
came	O
to	O
running	O
lda	O
(	O
discrim	O
)	O
,	O
qda	O
(	O
quadisc	O
)	O
and	O
logistic	O
discrimination	O
(	O
logdisc	O
)	O
we	O
excluded	O
attributes	O
that	O
were	O
linear	O
combinations	O
of	O
others	O
.	O
this	O
was	O
the	O
case	O
for	O
the	O
belgian	O
power	O
data	O
which	O
is	O
described	O
in	O
section	O
9.5.5.	O
although	O
,	O
in	O
principle	O
,	O
the	O
performance	O
of	O
linear	O
discriminant	O
procedures	O
is	O
not	O
affected	O
by	O
the	O
presence	O
of	O
linear	O
combinations	O
of	O
attributes	O
,	O
in	O
practice	O
the	O
resulting	O
singularities	O
are	O
best	O
avoided	O
for	O
numerical	O
reasons	O
.	O
g	O
.	O
c	O
«	O
e	O
«	O
as	O
the	O
performance	O
of	O
statistical	B
procedures	O
is	O
directly	O
related	O
to	O
the	O
statistical	B
properties	O
of	O
the	O
attributes	O
,	O
it	O
is	O
generally	O
advisable	O
to	O
transform	O
the	O
attributes	O
so	O
that	O
their	O
marginal	O
distributions	O
are	O
as	O
near	O
normal	O
as	O
possible	O
.	O
each	O
attribute	O
is	O
considered	O
in	O
turn	O
,	O
and	O
some	O
transformation	O
,	O
usually	O
from	O
the	O
power-law	O
family	O
,	O
is	O
made	O
on	O
the	O
attribute	O
.	O
most	O
frequently	O
,	O
this	O
is	O
done	O
by	O
taking	O
the	O
square-root	O
,	O
logarithm	O
or	O
reciprocal	O
transform	O
.	O
these	O
transforms	O
may	O
help	O
the	O
statistical	B
procedures	O
:	O
in	O
theory	O
,	O
at	O
least	O
,	O
they	O
have	O
no	O
effect	O
on	O
non-parametric	O
procedures	O
,	O
such	O
as	O
the	O
decision	O
trees	O
,	O
or	O
naive	O
bayes	O
.	O
7.4.3	O
large	O
number	O
of	O
categories	O
we	O
describe	O
now	O
the	O
problems	O
that	O
arise	O
for	O
decision	O
trees	O
and	O
statistical	B
algorithms	O
alike	O
when	O
an	O
attribute	O
has	O
a	O
large	O
number	O
of	O
categories	O
.	O
firstly	O
,	O
in	O
building	O
a	O
decision	O
tree	O
,	O
a	O
potential	O
split	O
of	O
a	O
categorical	O
attribute	O
is	O
based	O
on	O
some	O
partitioning	O
of	O
the	O
categories	O
,	O
one	O
partition	O
going	O
down	O
one	O
side	O
of	O
the	O
split	O
and	O
the	O
remainder	O
down	O
the	O
other	O
.	O
the	O
number	O
of	O
is	O
much	O
larger	O
than	O
ten	O
,	O
there	O
is	O
an	O
enormous	O
computational	O
load	O
,	O
and	O
the	O
tree	O
takes	O
a	O
very	O
long	O
time	O
to	O
train	O
.	O
however	O
,	O
there	O
is	O
a	O
computational	O
shortcut	O
that	O
applies	O
potential	O
splits	O
is³à	O
where	O
l	O
is	O
the	O
number	O
of	O
different	O
categories	O
(	O
levels	O
)	O
of	O
the	O
attribute	O
.	O
clearly	O
,	O
ifá	O
|	O
e	O
«	O
	O
|	O
h	O
|	O
	O
122	O
methods	O
for	O
comparison	O
[	O
ch	O
.	O
7	O
to	O
two-class	O
problems	O
(	O
see	O
clark	O
&	O
pregibon	O
,	O
1992	O
for	O
example	B
)	O
.	O
the	O
shortcut	O
method	O
is	O
not	O
implemented	O
in	O
all	O
statlog	O
decision-tree	O
methods	O
.	O
with	O
the	O
statistical	B
algorithms	O
,	O
a	O
speciﬁcation	O
of	O
the	O
attribute	O
.	O
categorical	O
attribute	O
withá	O
categories	O
(	O
levels	O
)	O
needsál	O
f	O
binary	O
variables	O
for	O
a	O
complete	O
as	O
a	O
trivial	O
example	B
,	O
with	O
two	O
numerical	O
attributesy	O
andq	O
rq	O
probably	O
see	O
exactly	O
the	O
same	O
predictive	O
value	O
in	O
the	O
pair	O
of	O
attributes	O
(	O
yreâq	O
the	O
original	O
pair	O
(	O
y	O
now	O
it	O
is	O
a	O
fact	O
that	O
decision	O
trees	O
behave	O
differently	O
for	O
categorical	O
and	O
numerical	O
data	O
.	O
two	O
datasets	O
may	O
be	O
logically	O
equivalent	O
,	O
yet	O
give	O
rise	O
to	O
different	O
decision	O
trees	O
.	O
,	O
statistical	B
algorithms	O
would	O
)	O
as	O
in	O
)	O
,	O
yet	O
the	O
decision	O
trees	O
would	O
be	O
different	O
,	O
as	O
the	O
decision	O
boundaries	O
would	O
now	O
be	O
at	O
an	O
angle	O
of	O
45	O
degrees	O
.	O
when	O
categorical	O
attributes	O
are	O
replaced	O
by	O
binary	O
variables	O
the	O
decision	O
trees	O
will	O
be	O
very	O
different	O
,	O
as	O
most	O
decision	O
tree	O
procedures	O
look	O
at	O
all	O
possible	O
subsets	O
of	O
attribute	O
values	O
when	O
considering	O
potential	O
splits	O
.	O
there	O
is	O
the	O
additional	O
,	O
although	O
perhaps	O
not	O
so	O
important	O
,	O
point	O
that	O
the	O
interpretation	O
of	O
the	O
tree	O
is	O
rendered	O
more	O
difﬁcult	O
.	O
,	O
q	O
,	O
y	O
it	O
is	O
therefore	O
of	O
interest	O
to	O
note	O
where	O
decision	O
tree	O
procedures	O
get	O
almost	O
the	O
same	O
accuracies	O
on	O
an	O
original	O
categorical	O
dataset	O
and	O
the	O
processed	O
binary	O
data	O
.	O
newid	O
,	O
as	O
run	O
by	O
isoft	O
for	O
example	B
,	O
obtained	O
an	O
accuracy	O
of	O
90.05	O
%	O
on	O
the	O
processed	O
dna	O
data	O
and	O
90.80	O
%	O
on	O
the	O
original	O
dna	O
data	O
(	O
with	O
categorical	O
attributes	O
)	O
.	O
these	O
accuracies	O
are	O
probably	O
within	O
what	O
could	O
be	O
called	O
experimental	O
error	O
,	O
so	O
it	O
seems	O
that	O
newid	O
does	O
about	O
as	O
well	O
on	O
either	O
form	O
of	O
the	O
dna	O
dataset	O
.	O
in	O
such	O
circumstances	O
,	O
we	O
have	O
taken	O
the	O
view	O
that	O
for	O
comparative	O
purposes	O
it	O
is	O
better	O
that	O
all	O
algorithms	O
are	O
run	O
on	O
exactly	O
the	O
same	O
preprocessed	O
form	O
.	O
this	O
way	O
we	O
avoid	O
differences	O
in	O
preprocessing	B
when	O
comparing	O
performance	O
.	O
when	O
faced	O
with	O
a	O
new	O
application	O
,	O
it	O
will	O
pay	O
to	O
consider	O
very	O
carefully	O
what	O
form	O
of	O
preprocessing	B
should	O
be	O
done	O
.	O
this	O
is	O
just	O
as	O
true	O
for	O
statistical	B
algorithms	O
as	O
for	O
neural	O
nets	O
or	O
machine	O
learning	O
.	O
7.4.4	O
bias	O
in	O
class	O
proportions	O
first	O
,	O
some	O
general	O
remarks	O
on	O
potential	O
bias	O
in	O
credit	O
datasets	O
.	O
we	O
do	O
not	O
know	O
the	O
way	O
in	O
which	O
the	O
credit	O
datasets	O
were	O
collected	O
,	O
but	O
it	O
is	O
very	O
probable	O
that	O
they	O
were	O
biased	O
in	O
the	O
following	O
way	O
.	O
most	O
credit	O
companies	O
are	O
very	O
unwilling	O
to	O
give	O
credit	O
to	O
all	O
applicants	O
.	O
as	O
a	O
result	O
,	O
data	O
will	O
be	O
gathered	O
for	O
only	O
those	O
customers	O
who	O
were	O
given	O
credit	O
.	O
if	O
the	O
credit	O
approval	O
process	O
is	O
any	O
good	O
at	O
all	O
,	O
the	O
proportion	O
of	O
bad	O
risks	O
among	O
all	O
applicants	O
will	O
be	O
signiﬁcantly	O
higher	O
than	O
in	O
the	O
given	O
dataset	O
.	O
it	O
is	O
very	O
likely	O
also	O
,	O
that	O
the	O
proﬁles	O
of	O
creditors	O
and	O
non-creditors	O
are	O
very	O
different	O
,	O
so	O
rules	O
deduced	O
from	O
the	O
creditors	O
will	O
have	O
much	O
less	O
relevance	O
to	O
the	O
target	O
population	O
(	O
of	O
all	O
applicants	O
)	O
.	O
when	O
the	O
numbers	O
of	O
good	O
and	O
bad	O
risk	O
examples	O
are	O
widely	O
different	O
,	O
and	O
one	O
would	O
expect	O
that	O
the	O
bad	O
risk	O
examples	O
would	O
be	O
relatively	O
infrequent	O
in	O
a	O
well	O
managed	O
lending	O
concern	O
,	O
it	O
becomes	O
rather	O
awkward	O
to	O
include	O
all	O
the	O
data	O
in	O
the	O
training	O
of	O
a	O
classiﬁcation	B
procedure	O
.	O
on	O
the	O
one	O
hand	O
,	O
if	O
we	O
are	O
to	O
preserve	O
the	O
true	O
class	O
proportions	O
in	O
the	O
training	O
sample	O
,	O
the	O
total	O
number	O
of	O
examples	O
may	O
have	O
to	O
be	O
extremely	O
large	O
in	O
order	O
to	O
guarantee	O
sufﬁcient	O
bad	O
risk	O
examples	O
for	O
a	O
reliable	O
rule	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
we	O
follow	O
the	O
common	O
practice	O
in	O
such	O
cases	O
and	O
take	O
as	O
many	O
bad	O
risk	O
examples	O
as	O
possible	O
,	O
together	O
with	O
a	O
matching	O
number	O
of	O
good	O
risk	O
examples	O
,	O
we	O
are	O
constructing	O
a	O
classiﬁcation	B
rule	O
with	O
its	O
boundaries	O
in	O
the	O
wrong	O
places	O
.	O
the	O
common	O
practice	O
is	O
to	O
make	O
an	O
adjustment	O
to	O
the	O
boundaries	O
to	O
take	O
account	O
of	O
the	O
true	O
class	O
proportions	O
.	O
in	O
the	O
case	O
of	O
two	O
classes	O
,	O
such	O
sec	O
.	O
7.4	O
]	O
pre-processing	O
123	O
an	O
adjustment	O
is	O
equivalent	O
to	O
allocating	O
different	O
misclassiﬁcation	O
costs	O
(	O
see	O
sections	O
2.6	O
and	O
10.2.1	O
)	O
.	O
for	O
example	B
,	O
if	O
the	O
true	O
bad	O
risk	O
proportion	O
is	O
5	O
%	O
,	O
and	O
a	O
rule	O
is	O
trained	O
on	O
an	O
artiﬁcial	O
sample	O
with	O
equal	O
numbers	O
of	O
good	O
and	O
bad	O
risks	O
,	O
the	O
recommendation	O
would	O
be	O
to	O
classify	O
as	O
bad	O
risk	O
only	O
those	O
examples	O
whose	O
assessed	O
posterior	O
odds	O
of	O
being	O
bad	O
risk	O
were	O
19	O
to	O
1	O
(	O
95	O
%	O
to	O
5	O
%	O
)	O
.	O
this	O
is	O
equivalent	O
to	O
learning	O
on	O
the	O
artiﬁcial	O
sample	O
,	O
with	O
the	O
cost	O
of	O
misclassifying	O
bad	O
risks	O
as	O
19	O
times	O
that	O
of	O
misclassifying	O
good	O
risk	O
examples	O
.	O
for	O
such	O
a	O
procedure	O
to	O
work	O
,	O
it	O
is	O
necessary	O
that	O
a	O
classiﬁcation	B
procedure	O
returns	O
class	O
probabilities	O
as	O
its	O
output	B
,	O
and	O
the	O
user	O
can	O
then	O
allocate	O
according	O
to	O
his	O
prior	O
probabilities	O
(	O
or	O
according	O
to	O
misclassiﬁcation	O
costs	O
)	O
.	O
many	O
decision	O
trees	O
,	O
cart	O
and	O
bayes	O
tree	O
for	O
example	B
,	O
now	O
output	B
class	O
probabilities	O
rather	O
than	O
classes	O
.	O
but	O
the	O
majority	O
of	O
decision	O
trees	O
in	O
this	O
project	O
do	O
not	O
do	O
so	O
.	O
and	O
,	O
in	O
any	O
case	O
,	O
it	O
is	O
by	O
no	O
means	O
true	O
that	O
this	O
artiﬁcial	O
procedure	O
is	O
,	O
in	O
fact	O
,	O
a	O
proper	O
procedure	O
at	O
all	O
.	O
consider	O
again	O
the	O
case	O
where	O
bad	O
risks	O
form	O
5	O
%	O
of	O
the	O
population	O
,	O
and	O
suppose	O
that	O
we	O
are	O
given	O
a	O
single	O
normally	O
distributed	O
variable	O
(	O
say	O
“	O
bank	O
balance	O
”	O
)	O
on	O
which	O
to	O
classify	O
.	O
for	O
simplicity	O
,	O
suppose	O
also	O
that	O
good	O
and	O
bad	O
risk	O
customers	O
differ	O
only	O
in	O
their	O
mean	O
bank	O
balance	O
.	O
when	O
trained	O
on	O
an	O
artiﬁcial	O
sample	O
with	O
equal	O
good	O
and	O
bad	O
risks	O
,	O
a	O
decision	O
tree	O
method	O
would	O
,	O
correctly	O
,	O
divide	O
the	O
population	O
into	O
two	O
regions	O
above	O
and	O
below	O
the	O
midpoint	O
between	O
the	O
two	O
mean	O
bank	O
balances	O
.	O
in	O
the	O
artiﬁcial	O
sample	O
there	O
will	O
be	O
a	O
proportion	O
,	O
¥	O
say	O
,	O
of	O
good	O
examples	O
above	O
this	O
boundary	O
and	O
,	O
approximately	O
,	O
¥	O
bad	O
examples	O
below	O
the	O
boundary	O
.	O
so	O
,	O
for	O
example	B
,	O
probabilities	O
as¥	O
if	O
a	O
potential	O
customer	O
has	O
bank	O
balance	O
above	O
this	O
boundary	O
,	O
we	O
can	O
assess	O
the	O
class	O
for	O
bad	O
.	O
no	O
matter	O
what	O
adjustment	O
is	O
made	O
for	O
the	O
true	O
prior	O
odds	O
of	O
being	O
bad	O
risk	O
,	O
it	O
is	O
clear	O
that	O
the	O
allocation	O
rule	O
can	O
only	O
take	O
one	O
of	O
two	O
forms	O
:	O
either	O
allocate	O
everyone	O
to	O
being	O
good	O
(	O
or	O
bad	O
)	O
;	O
or	O
allocate	O
good	O
or	O
bad	O
according	O
as	O
bank	O
balance	O
is	O
above	O
or	O
below	O
the	O
established	O
boundary	O
.	O
in	O
the	O
situation	O
we	O
have	O
described	O
,	O
however	O
,	O
it	O
is	O
clear	O
that	O
it	O
is	O
the	O
boundary	O
that	O
should	O
move	O
,	O
rather	O
than	O
adjust	O
the	O
probabilities	O
.	O
the	O
way	O
to	O
modify	O
the	O
procedure	O
is	O
to	O
overgrow	O
the	O
tree	O
and	O
then	O
to	O
take	O
the	O
costs	O
and/or	O
priors	O
into	O
account	O
when	O
pruning	B
.	O
see	O
michie	O
&	O
attar	O
(	O
1991	O
)	O
for	O
further	O
details	O
.	O
for	O
being	O
good	O
andf	O
l¥	O
7.4.5	O
hierarchical	O
attributes	O
it	O
often	O
happens	O
that	O
information	O
is	O
relevant	O
only	O
to	O
some	O
of	O
the	O
examples	O
.	O
for	O
example	B
,	O
certain	O
questions	O
in	O
a	O
population	O
census	O
may	O
apply	O
only	O
to	O
the	O
householder	O
,	O
or	O
certain	O
medical	O
conditions	O
apply	O
to	O
females	O
.	O
there	O
is	O
then	O
a	O
hierarchy	O
of	O
attributes	O
:	O
primary	O
variables	O
refer	O
to	O
all	O
members	O
(	O
sex	O
is	O
a	O
primary	O
attribute	O
)	O
;	O
secondary	O
attributes	O
are	O
only	O
relevant	O
when	O
the	O
appropriate	O
primary	O
attribute	O
is	O
applicable	O
(	O
pregnant	O
is	O
secondary	O
to	O
sex	O
=	O
female	O
)	O
;	O
tertiary	O
variables	O
are	O
relevant	O
when	O
a	O
secondary	O
variable	O
applies	O
(	O
duration	O
of	O
pregnancy	O
is	O
tertiary	O
to	O
pregnant	O
=	O
true	O
)	O
;	O
and	O
so	O
on	O
.	O
note	O
that	O
testing	O
all	O
members	O
of	O
a	O
population	O
for	O
characteristics	O
of	O
pregnancy	O
is	O
not	O
only	O
pointless	O
but	O
wasteful	O
.	O
decision	O
tree	O
methods	O
are	O
readily	O
adapted	O
to	O
deal	O
with	O
such	O
hierarchical	O
datasets	O
,	O
and	O
the	O
algorithm	O
	O
has	O
been	O
so	O
designed	O
.	O
and	O
not	O
for	O
others	O
.	O
obviouslyû	O
the	O
machine	O
fault	O
dataset	O
(	O
see	O
section	O
9.5.7	O
)	O
,	O
which	O
was	O
created	O
by	O
isoft	O
,	O
is	O
an	O
example	B
of	O
a	O
hierarchical	O
dataset	O
,	O
with	O
some	O
attributes	O
being	O
present	O
for	O
one	O
subclass	O
of	O
examples	O
the	O
viewpoint	O
of	O
the	O
other	O
algorithms	O
,	O
the	O
dataset	O
is	O
unreadable	O
,	O
as	O
it	O
has	O
a	O
variable	O
number	O
of	O
attributes	O
.	O
therefore	O
,	O
an	O
alternative	O
version	O
needs	O
to	O
be	O
prepared	O
.	O
of	O
course	O
,	O
the	O
ﬂat	O
	O
can	O
deal	O
with	O
this	O
dataset	O
in	O
its	O
original	O
form	O
,	O
but	O
,	O
from	O
û	O
{	O
{	O
124	O
methods	O
for	O
comparison	O
[	O
ch	O
.	O
7	O
form	O
has	O
lost	O
some	O
of	O
the	O
information	O
that	O
was	O
available	O
in	O
the	O
hierarchical	O
structure	O
of	O
the	O
data	O
.	O
the	O
fact	O
thatû	O
	O
does	O
best	O
on	O
this	O
dataset	O
when	O
it	O
uses	O
this	O
hierarchical	O
information	O
suggests	O
that	O
the	O
hierarchical	O
structure	O
is	O
related	O
to	O
the	O
decision	O
class	O
.	O
coding	O
of	O
hierarchical	O
attributes	O
hierarchical	O
attributes	O
can	O
be	O
coded	O
into	O
ﬂat	O
format	O
without	O
difﬁculty	O
,	O
in	O
that	O
a	O
one-to-one	O
correspondence	O
can	O
be	O
set	O
up	O
between	O
the	O
hierarchically	O
structured	O
data	O
and	O
the	O
ﬂat	O
format	O
.	O
we	O
illustrate	O
the	O
procedure	O
for	O
an	O
artiﬁcial	O
example	B
.	O
consider	O
the	O
primary	O
attribute	O
sex	O
.	O
when	O
sex	O
takes	O
the	O
value	O
“	O
male	O
”	O
,	O
the	O
value	O
of	O
attribute	O
baldness	O
is	O
recorded	O
as	O
one	O
of	O
(	O
yes	O
no	O
)	O
,	O
but	O
when	O
sex	O
takes	O
the	O
value	O
“	O
female	O
”	O
the	O
attribute	O
baldness	O
is	O
simply	O
“	O
not	O
applicable	O
”	O
.	O
one	O
way	O
of	O
coding	O
this	O
information	O
in	O
ﬂat	O
format	O
is	O
to	O
give	O
two	O
attributes	O
,	O
with	O
«	O
value	O
of	O
«	O
possible	O
values	O
for	O
«	O
0	O
)	O
and	O
(	O
0	O
0	O
)	O
.	O
in	O
this	O
formulation	O
,	O
the	O
primary	O
variable	O
is	O
explicitly	O
available	O
through	O
the	O
is	O
equal	O
to	O
0	O
,	O
it	O
is	O
not	O
clear	O
whether	O
this	O
means	O
“	O
not	O
bald	O
”	O
or	O
“	O
not	O
applicable	O
”	O
.	O
strictly	O
,	O
there	O
are	O
three	O
|	O
denoting	O
sex	O
and	O
«	O
|	O
,	O
but	O
there	O
is	O
the	O
difﬁculty	O
,	O
here	O
not	O
too	O
serious	O
,	O
that	O
when	O
«	O
	O
baldness	O
.	O
the	O
three	O
possible	O
triples	O
of	O
values	O
are	O
(	O
1	O
1	O
)	O
,	O
(	O
1	O
	O
:	O
“	O
bald	O
”	O
,	O
“	O
not	O
bald	O
”	O
and	O
“	O
not	O
applicable	O
”	O
,	O
the	O
ﬁrst	O
two	O
possibilities	O
applying	O
only	O
to	O
males	O
.	O
this	O
gives	O
a	O
second	O
formulation	O
,	O
in	O
which	O
the	O
two	O
attributes	O
are	O
lumped	O
together	O
into	O
a	O
single	O
attribute	O
,	O
whose	O
possible	O
values	O
represent	O
the	O
possible	O
states	O
of	O
the	O
system	O
.	O
in	O
the	O
example	B
,	O
the	O
possible	O
states	O
are	O
“	O
bald	O
male	O
”	O
,	O
“	O
not	O
bald	O
male	O
”	O
and	O
“	O
female	O
”	O
.	O
of	O
course	O
,	O
none	O
of	O
the	O
above	O
codings	O
enables	O
ordinary	O
classiﬁers	O
to	O
make	O
use	O
of	O
the	O
hierarchical	O
structure	O
:	O
they	O
are	O
designed	O
merely	O
to	O
represent	O
the	O
information	O
in	O
ﬂat	O
form	O
with	O
the	O
same	O
number	O
of	O
attributes	O
per	O
example	B
.	O
breiman	O
et	O
al	O
.	O
(	O
1984	O
)	O
indicate	O
how	O
hierarchical	O
attributes	O
may	O
be	O
programmed	O
into	O
a	O
tree-building	O
procedure	O
.	O
a	O
logical	O
ﬂag	O
indicates	O
if	O
a	O
test	O
on	O
an	O
attribute	O
is	O
permissible	O
,	O
and	O
for	O
a	O
secondary	O
attribute	O
this	O
ﬂag	O
is	O
set	O
to	O
“	O
true	O
”	O
only	O
when	O
the	O
corresponding	O
primary	O
attribute	O
has	O
already	O
been	O
tested	O
.	O
7.4.6	O
collection	O
of	O
datasets	O
for	O
the	O
most	O
part	O
,	O
when	O
data	O
are	O
gathered	O
,	O
there	O
is	O
an	O
implicit	O
understanding	O
that	O
the	O
data	O
will	O
be	O
analysed	O
by	O
a	O
certain	O
procedure	O
,	O
and	O
the	O
data-gatherer	O
usually	O
sets	O
down	O
the	O
data	O
in	O
a	O
format	O
that	O
is	O
acceptable	O
to	O
that	O
procedure	O
.	O
for	O
example	B
,	O
if	O
linear	O
discriminants	O
are	O
to	O
be	O
used	O
,	O
it	O
is	O
inappropriate	O
to	O
include	O
linear	O
combinations	O
of	O
existing	O
attributes	O
,	O
yet	O
the	O
judicious	O
use	O
of	O
sums	O
or	O
differences	O
can	O
make	O
all	O
the	O
difference	O
to	O
a	O
decision	O
tree	O
procedure	O
.	O
in	O
other	O
cases	O
,	O
the	O
data	O
may	O
have	O
some	O
additional	O
structure	O
that	O
can	O
not	O
be	O
incorporated	O
in	O
the	O
given	O
procedure	O
,	O
and	O
this	O
structure	O
must	O
be	O
removed	O
,	O
or	O
ignored	O
in	O
some	O
way	O
.	O
7.4.7	O
preprocessing	B
strategy	O
in	O
statlog	O
the	O
general	O
strategy	O
with	O
datasets	O
was	O
to	O
circulate	O
the	O
datasets	O
exactly	O
as	O
received	O
,	O
and	O
11	O
datasets	O
were	O
sent	O
out	O
in	O
exactly	O
the	O
same	O
format	O
as	O
they	O
came	O
in	O
.	O
for	O
these	O
11	O
datasets	O
,	O
the	O
only	O
processing	O
was	O
to	O
permute	O
the	O
order	O
of	O
the	O
examples	O
.	O
in	O
four	O
datasets	O
substantial	O
preprocessing	B
was	O
necessary	O
,	O
and	O
in	O
three	O
of	O
these	O
datasets	O
it	O
is	O
possible	O
that	O
the	O
resulting	O
dataset	O
has	O
lost	O
some	O
vital	O
information	O
,	O
or	O
has	O
been	O
biased	O
in	O
some	O
way	O
.	O
for	O
example	B
,	O
the	O
credit	B
management	I
dataset	O
was	O
processed	O
to	O
make	O
the	O
class	O
proportions	O
representative	O
.	O
another	O
source	O
of	O
potential	O
bias	O
is	O
the	O
way	O
in	O
which	O
categorical	O
attributes	O
are	O
treated	O
—	O
a	O
problem	O
that	O
is	O
most	O
acute	O
in	O
the	O
dna	O
dataset	O
.	O
{	O
	O
8	O
review	O
of	O
previous	O
empirical	O
comparisons	O
r.	O
j.	O
henery	O
university	O
of	O
strathclyde|	O
8.1	O
introduction	O
it	O
is	O
very	O
difﬁcult	O
to	O
make	O
sense	O
of	O
the	O
multitude	O
of	O
empirical	O
comparisons	O
that	O
have	O
been	O
made	O
.	O
so	O
often	O
,	O
the	O
results	O
are	O
apparently	O
in	O
direct	O
contradiction	O
,	O
with	O
one	O
author	O
claiming	O
that	O
decision	O
trees	O
are	O
superior	O
to	O
neural	O
nets	O
,	O
and	O
another	O
making	O
the	O
opposite	O
claim	O
.	O
even	O
allowing	O
for	O
differences	O
in	O
the	O
types	O
of	O
data	O
,	O
it	O
is	O
almost	O
impossible	O
to	O
reconcile	O
the	O
various	O
claims	O
that	O
are	O
made	O
for	O
this	O
or	O
that	O
algorithm	O
as	O
being	O
faster	O
,	O
or	O
more	O
accurate	O
,	O
or	O
easier	O
,	O
than	O
some	O
other	O
algorithm	O
.	O
there	O
are	O
no	O
agreed	O
objective	O
criteria	O
by	O
which	O
to	O
judge	O
algorithms	O
,	O
and	O
in	O
any	O
case	O
subjective	O
criteria	O
,	O
such	O
as	O
how	O
easy	O
an	O
algorithm	O
is	O
to	O
program	O
or	O
run	O
,	O
are	O
also	O
very	O
important	O
when	O
a	O
potential	O
user	O
makes	O
his	O
choice	O
from	O
the	O
many	O
methods	O
available	O
.	O
nor	O
is	O
it	O
much	O
help	O
to	O
say	O
to	O
the	O
potential	O
user	O
that	O
a	O
particular	O
neural	O
network	O
,	O
say	O
,	O
is	O
better	O
for	O
a	O
particular	O
dataset	O
.	O
nor	O
are	O
the	O
labels	O
neural	O
network	O
and	O
machine	O
learning	O
particularly	O
helpful	O
either	O
,	O
as	O
there	O
are	O
different	O
types	O
of	O
algorithms	O
within	O
these	O
categories	O
.	O
what	O
is	O
required	O
is	O
some	O
way	O
of	O
categorising	O
the	O
datasets	O
into	O
types	O
,	O
with	O
a	O
statement	O
that	O
for	O
such-and-such	O
a	O
type	O
of	O
dataset	O
,	O
such-and-such	O
a	O
type	O
of	O
algorithm	O
is	O
likely	O
to	O
do	O
well	O
.	O
the	O
situation	O
is	O
made	O
more	O
difﬁcult	O
because	O
rapid	O
advances	O
are	O
being	O
made	O
in	O
all	O
three	O
areas	O
:	O
machine	O
learning	O
,	O
neural	O
networks	O
and	O
statistics	O
.	O
so	O
many	O
comparisons	O
are	O
made	O
between	O
,	O
say	O
,	O
a	O
state-of-the-art	O
neural	O
network	O
and	O
an	O
outmoded	O
machine	O
learning	O
procedure	O
like	O
id3	O
.	O
8.2	O
basic	O
toolbox	O
of	O
algorithms	O
before	O
discussing	O
the	O
various	O
studies	O
,	O
let	O
us	O
make	O
tentative	O
proposals	O
for	O
candidates	O
in	O
future	O
comparative	O
trials	O
,	O
i.e	O
.	O
let	O
us	O
say	O
what	O
,	O
in	O
our	O
opinion	O
,	O
form	O
the	O
basis	O
of	O
a	O
toolbox	O
of	O
good	O
classiﬁcation	B
procedures	O
.	O
in	O
doing	O
so	O
,	O
we	O
are	O
implicitly	O
making	O
a	O
criticism	O
of	O
any	O
comparative	O
studies	O
that	O
do	O
not	O
include	O
these	O
basic	O
algorithms	O
,	O
or	O
something	O
like	O
them	O
.	O
most	O
are	O
available	O
as	O
public	O
domain	O
software	O
.	O
any	O
that	O
are	O
not	O
can	O
be	O
made	O
available	O
m	O
address	O
for	O
correspondence	O
:	O
department	O
of	O
statistics	O
and	O
modelling	O
science	O
,	O
university	O
of	O
strathclyde	O
,	O
glasgow	O
g1	O
1xh	O
,	O
u.k.	O
126	O
review	O
of	O
empirical	O
comparisons	O
[	O
ch	O
.	O
8	O
from	O
the	O
database	O
of	O
algorithms	O
administered	O
from	O
porto	O
(	O
see	O
appendix	O
b	O
)	O
.	O
so	O
there	O
is	O
no	O
excuse	O
for	O
not	O
including	O
them	O
in	O
future	O
studies	O
!	O
1.	O
we	O
should	O
probably	O
always	O
include	O
the	O
linear	O
discriminant	O
rule	O
,	O
as	O
it	O
is	O
sometimes	O
best	O
,	O
but	O
for	O
the	O
other	O
good	O
reason	O
that	O
is	O
a	O
standard	O
algorithm	O
,	O
and	O
the	O
most	O
widely	O
available	O
of	O
all	O
procedures	O
.	O
winner	O
(	O
although	O
if	O
there	O
are	O
scaling	O
problems	O
it	O
was	O
sometimes	O
outright	O
loser	O
too	O
!	O
)	O
3	O
.	O
2.	O
on	O
the	O
basis	O
of	O
our	O
results	O
,	O
thed	O
-nearest	O
neighbour	O
method	O
was	O
often	O
the	O
outright	O
so	O
it	O
would	O
seem	O
sensible	O
to	O
included	O
-nearest	O
neighbour	O
in	O
any	O
comparative	O
studies	O
.	O
although	O
the	O
generally	O
good	O
performance	O
ofd	O
-nearest	O
neighbour	O
is	O
well	O
known	O
,	O
it	O
is	O
in	O
many	O
cases	O
whered	O
-nearest	O
neighbour	O
did	O
badly	O
,	O
the	O
decision-tree	O
methods	O
did	O
surprising	O
how	O
few	O
past	O
studies	O
have	O
involved	O
this	O
procedure	O
,	O
especially	O
as	O
it	O
is	O
so	O
easy	O
to	O
program	O
.	O
relatively	O
well	O
,	O
for	O
example	B
in	O
the	O
(	O
non-cost-matrix	O
)	O
credit	O
datasets	O
.	O
so	O
some	O
kind	O
of	O
decision	O
tree	O
should	O
be	O
included	O
.	O
4.	O
yet	O
again	O
,	O
some	O
of	O
the	O
newer	O
statistical	B
procedures	O
got	O
very	O
good	O
results	O
when	O
all	O
other	O
methods	O
were	O
struggling	O
.	O
so	O
we	O
would	O
also	O
recommend	O
the	O
inclusion	O
of	O
,	O
say	O
,	O
smart	O
as	O
a	O
modern	O
statistical	B
procedure	O
.	O
5.	O
representing	O
neural	O
networks	O
,	O
we	O
would	O
probably	O
choose	O
lvq	O
and/or	O
radial	O
basis	O
functions	O
,	O
as	O
these	O
seem	O
to	O
have	O
a	O
distinct	O
edge	O
over	O
the	O
version	O
of	O
backpropagation	O
that	O
we	O
used	O
.	O
however	O
,	O
as	O
the	O
performance	O
of	O
lvq	O
seems	O
to	O
mirror	O
that	O
of	O
k-nn	O
rather	O
closely	O
,	O
we	O
would	O
recommend	O
inclusion	O
of	O
rbf	O
rather	O
than	O
lvq	O
if	O
k-nn	O
is	O
already	O
included	O
.	O
any	O
comparative	O
study	O
that	O
does	O
not	O
include	O
the	O
majority	O
of	O
these	O
algorithms	O
is	O
clearly	O
not	O
aiming	O
to	O
be	O
complete	O
.	O
also	O
,	O
any	O
comparative	O
study	O
that	O
looks	O
at	O
only	O
two	O
procedures	O
can	O
not	O
give	O
reliable	O
indicators	O
of	O
performance	O
,	O
as	O
our	O
results	O
show	O
.	O
8.3	O
difficulties	O
in	O
previous	O
studies	O
bearing	O
in	O
mind	O
our	O
choice	O
of	O
potential	O
candidates	O
for	O
comparative	O
studies	O
,	O
it	O
will	O
quickly	O
become	O
obvious	O
that	O
most	O
previous	O
studies	O
suffer	O
from	O
the	O
major	O
disadvantage	O
that	O
their	O
choice	O
of	O
algorithms	O
is	O
too	O
narrow	O
.	O
there	O
are	O
many	O
other	O
sources	O
of	O
difﬁculty	O
,	O
and	O
before	O
giving	O
detailed	O
consideration	O
of	O
past	O
empirical	O
studies	O
,	O
we	O
list	O
the	O
pitfalls	O
that	O
await	O
anyone	O
carrying	O
out	O
comparative	O
studies	O
.	O
of	O
course	O
,	O
our	O
own	O
study	O
was	O
not	O
entirely	O
free	O
from	O
them	O
either	O
.	O
the	O
choice	O
of	O
algorithms	O
is	O
too	O
narrow	O
;	O
in	O
many	O
cases	O
,	O
the	O
authors	O
have	O
developed	O
their	O
own	O
pet	O
algorithm	O
,	O
and	O
are	O
expert	O
in	O
their	O
own	O
ﬁeld	O
,	O
but	O
they	O
are	O
not	O
so	O
expert	O
in	O
other	O
methods	O
,	O
resulting	O
in	O
a	O
natural	O
bias	O
against	O
other	O
methods	O
;	O
the	O
chosen	O
algorithms	O
may	O
not	O
represent	O
the	O
state	O
of	O
the	O
art	O
;	O
the	O
datasets	O
are	O
usually	O
small	O
or	O
simulated	O
,	O
and	O
so	O
not	O
representative	O
of	O
real-life	O
applications	O
;	O
there	O
is	O
a	O
substantial	O
bias	O
in	O
the	O
choice	O
of	O
dataset	O
,	O
in	O
simulations	O
especially	O
,	O
giving	O
a	O
substantial	O
bias	O
in	O
favour	O
of	O
certain	O
algorithms	O
;	O
often	O
the	O
choice	O
of	O
criteria	O
is	O
biased	O
in	O
favour	O
of	O
one	O
type	O
of	O
algorithm	O
,	O
sometimes	O
even	O
using	O
unrealistic	O
cost	O
criteria	O
.	O
sec	O
.	O
8.6	O
]	O
previous	O
empirical	O
comparisons	O
127	O
especially	O
across	O
comparative	O
studies	O
,	O
there	O
may	O
be	O
problems	O
due	O
to	O
differences	O
in	O
the	O
way	O
the	O
data	O
were	O
pre-processed	O
,	O
for	O
example	B
by	O
removing	O
or	O
replacing	O
missing	O
values	O
,	O
or	O
transforming	O
categorical	O
to	O
numerical	O
attributes	O
.	O
the	O
class	O
deﬁnitions	O
may	O
be	O
more	O
suited	O
to	O
some	O
algorithms	O
than	O
others	O
.	O
also	O
,	O
the	O
class	O
proportions	O
in	O
the	O
training	O
set	O
may	O
well	O
differ	O
substantially	O
from	O
the	O
population	O
values	O
-	O
often	O
deliberately	O
so	O
.	O
some	O
comparative	O
studies	O
used	O
variant	O
,	O
but	O
not	O
identical	O
,	O
datasets	O
and	O
algorithms	O
.	O
we	O
have	O
attempted	O
to	O
minimise	O
the	O
above	O
problems	O
in	O
our	O
own	O
study	O
,	O
for	O
example	B
,	O
by	O
adopt-	O
ing	O
a	O
uniform	B
policy	O
for	O
missing	O
values	O
and	O
a	O
uniform	B
manner	O
of	O
dealing	O
with	O
categorical	O
variables	O
in	O
some	O
,	O
but	O
not	O
all	O
,	O
of	O
the	O
datasets	O
.	O
8.4	O
previous	O
empirical	O
comparisons	O
while	O
it	O
is	O
easy	O
to	O
criticise	O
past	O
studies	O
on	O
the	O
above	O
grounds	O
,	O
nonetheless	O
many	O
useful	O
comparative	O
studies	O
have	O
been	O
carried	O
out	O
.	O
what	O
they	O
may	O
lack	O
in	O
generality	O
,	O
they	O
may	O
gain	O
in	O
speciﬁcs	O
,	O
the	O
conclusion	O
being	O
that	O
,	O
for	O
at	O
least	O
one	O
dataset	O
,	O
algorithm	O
a	O
is	O
superior	O
(	O
faster	O
or	O
more	O
accurate	O
...	O
)	O
than	O
algorithm	O
b.	O
other	O
studies	O
may	O
also	O
investigate	O
other	O
aspects	O
more	O
fully	O
than	O
we	O
did	O
here	O
,	O
for	O
example	B
,	O
by	O
studying	O
learning	B
curves	I
,	O
i.e	O
.	O
the	O
amount	O
of	O
data	O
that	O
must	O
be	O
presented	O
to	O
an	O
algorithm	O
before	O
it	O
learns	O
something	O
useful	O
.	O
in	O
studying	O
particular	O
characteristics	O
of	O
algorithms	O
,	O
the	O
role	O
of	O
simulations	O
is	O
crucial	O
,	O
as	O
it	O
enables	O
controlled	O
departures	O
from	O
assumptions	O
,	O
giving	O
a	O
measure	B
of	O
robustness	O
etc..	O
(	O
although	O
we	O
have	O
used	O
some	O
simulated	O
data	O
in	O
our	O
study	O
,	O
namely	O
the	O
belgian	O
datasets	O
,	O
this	O
was	O
done	O
because	O
we	O
believed	O
that	O
the	O
simulations	O
were	O
very	O
close	O
to	O
the	O
real-world	O
problem	O
under	O
study	O
,	O
and	O
it	O
was	O
hoped	O
that	O
our	O
trials	O
would	O
help	O
in	O
understanding	O
this	O
particular	O
problem	O
.	O
)	O
here	O
we	O
will	O
not	O
discuss	O
the	O
very	O
many	O
studies	O
that	O
concentrate	O
on	O
just	O
one	O
procedure	O
or	O
set	O
of	O
cognate	O
procedures	O
:	O
rather	O
we	O
will	O
look	O
at	O
cross-disciplinary	O
studies	O
comparing	O
algorithms	O
with	O
widely	O
differing	O
capabilities	O
.	O
among	O
the	O
former	O
however	O
,	O
we	O
may	O
mention	O
comparisons	O
of	O
symbolic	O
(	O
ml	O
)	O
procedures	O
in	O
clark	O
&	O
boswell	O
(	O
1991	O
)	O
,	O
sammut	O
(	O
1988	O
)	O
,	O
quinlan	O
et	O
al	O
.	O
(	O
1986	O
)	O
and	O
aha	O
(	O
1992	O
)	O
;	O
statistical	B
procedures	O
in	O
cherkaoui	O
&	O
cleroux	O
(	O
1991	O
)	O
,	O
titterington	O
et	O
al	O
.	O
(	O
1981	O
)	O
and	O
remme	O
et	O
al	O
.	O
(	O
1980	O
)	O
,	O
and	O
neural	O
networks	O
in	O
huang	O
et	O
al	O
.	O
(	O
1991	O
)	O
,	O
fahlman	O
(	O
1991a	O
)	O
,	O
xu	O
et	O
al	O
.	O
(	O
1991	O
)	O
and	O
ersoy	O
&	O
hong	O
(	O
1991	O
)	O
.	O
several	O
studies	O
use	O
simulated	O
data	O
to	O
explore	O
various	O
aspects	O
of	O
performance	O
under	O
controlled	O
conditions	O
,	O
for	O
example	B
,	O
cherkaoui	O
&	O
cleroux	O
(	O
1991	O
)	O
and	O
remme	O
et	O
al	O
.	O
(	O
1980	O
)	O
.	O
8.5	O
individual	O
results	O
particular	O
methods	O
may	O
do	O
well	O
in	O
some	O
speciﬁc	O
domains	O
and	O
for	O
some	O
performance	O
well	O
in	O
recognising	O
handwritten	O
characters	O
(	O
aha	O
,	O
1992	O
)	O
and	O
(	O
kressel	O
,	O
1991	O
)	O
but	O
not	O
as	O
well	O
on	O
the	O
sonar-target	O
task	O
(	O
gorman	O
&	O
sejnowski	O
,	O
1988	O
)	O
.	O
measures	O
,	O
but	O
not	O
in	O
all	O
applications	O
.	O
for	O
example	B
,	O
d	O
-nearest	O
neighbour	O
performed	O
very	O
8.6	O
machine	O
learning	O
vs.	O
neural	O
network	O
with	O
the	O
recent	O
surge	O
in	O
interest	O
in	O
both	O
machine	O
learning	O
and	O
neural	O
networks	O
,	O
there	O
are	O
many	O
recent	O
studies	O
comparing	O
algorithms	O
from	O
these	O
two	O
areas	O
.	O
commonly	O
,	O
such	O
studies	O
do	O
not	O
include	O
any	O
statistical	B
algorithms	O
:	O
for	O
example	B
fisher	O
&	O
mckusick	O
(	O
1989	O
)	O
and	O
shavlik	O
et	O
al	O
.	O
(	O
1989	O
)	O
and	O
shavlik	O
et	O
al	O
.	O
(	O
1989	O
)	O
used	O
a	O
relatively	O
old	O
symbolic	O
algorithm	O
128	O
review	O
of	O
empirical	O
comparisons	O
[	O
ch	O
.	O
8	O
id3	O
,	O
which	O
has	O
been	O
repeatedly	O
shown	O
to	O
be	O
less	O
effective	O
than	O
its	O
successors	O
(	O
newid	O
and	O
c4.5	O
in	O
this	O
book	O
)	O
.	O
kirkwood	O
et	O
al	O
.	O
(	O
1989	O
)	O
found	O
that	O
a	O
symbolic	O
algorithm	O
,	O
id3	O
,	O
performed	O
better	O
than	O
discriminant	O
analysis	O
for	O
classifying	O
the	O
gait	O
cycle	O
of	O
artiﬁcial	O
limbs	O
.	O
tsaptsinos	O
et	O
al	O
.	O
(	O
1990	O
)	O
also	O
found	O
that	O
id3	O
was	O
more	O
preferable	O
on	O
an	O
engineering	O
control	O
problem	O
than	O
two	O
neural	O
network	O
algorithms	O
.	O
however	O
,	O
on	O
different	O
tasks	O
other	O
researchers	O
found	O
that	O
a	O
higher	O
order	O
neural	O
network	O
(	O
honn	O
)	O
performed	O
better	O
than	O
id3	O
(	O
spivoska	O
&	O
reid	O
,	O
1990	O
)	O
and	O
back-propagation	O
did	O
better	O
than	O
cart	O
(	O
atlas	O
et	O
al.	O
,	O
1991	O
)	O
.	O
gorman	O
&	O
sejnowski	O
(	O
1988	O
)	O
reported	O
that	O
back-propagation	O
outperformed	O
nearest	O
neighbour	O
for	O
classifying	O
sonar	O
targets	O
,	O
whereas	O
some	O
bayes	O
algorithms	O
were	O
shown	O
to	O
be	O
better	O
on	O
other	O
tasks	O
(	O
shadmehr	O
&	O
d	O
’	O
argenio	O
,	O
1990	O
)	O
.	O
more	O
extensive	O
comparisons	O
have	O
also	O
been	O
carried	O
out	O
between	O
neural	O
network	O
and	O
symbolic	O
methods	O
.	O
however	O
,	O
the	O
results	O
of	O
these	O
studies	O
were	O
inconclusive	O
.	O
for	O
example	B
,	O
whereas	O
weiss	O
&	O
kulikowski	O
(	O
1991	O
)	O
and	O
weiss	O
&	O
kapouleas	O
(	O
1989	O
)	O
reported	O
that	O
back-	O
propagation	O
performed	O
worse	O
than	O
symbolic	O
methods	O
(	O
i.e	O
.	O
cart	O
and	O
pvm	O
)	O
,	O
fisher	O
&	O
mckusick	O
(	O
1989	O
)	O
and	O
shavlik	O
et	O
al	O
.	O
(	O
1989	O
)	O
indicated	O
that	O
back-propagation	O
did	O
as	O
well	O
or	O
better	O
than	O
id3	O
.	O
since	O
these	O
are	O
the	O
most	O
extensive	O
comparisons	O
to	O
date	O
,	O
we	O
describe	O
their	O
ﬁndings	O
brieﬂy	O
and	O
detail	O
their	O
limitations	O
in	O
the	O
following	O
two	O
paragraphs	O
.	O
first	O
,	O
fisher	O
&	O
mckusick	O
(	O
1989	O
)	O
compared	O
the	O
accuracy	O
and	O
learning	O
speed	O
(	O
i.e	O
.	O
the	O
number	O
of	O
example	B
presentations	O
required	O
to	O
achieve	O
asymptotic	O
accuracy	O
)	O
of	O
id3	O
and	O
back-	O
propagation	O
.	O
this	O
study	O
is	O
restricted	O
in	O
the	O
selection	O
of	O
algorithms	O
,	O
evaluation	O
measures	O
,	O
and	O
data	O
sets	O
.	O
whereas	O
id3	O
can	O
not	O
tolerate	O
noise	O
,	O
several	O
descendants	O
of	O
id3	O
can	O
tolerate	O
noise	O
more	O
effectively	O
(	O
for	O
example	B
,	O
quinlan	O
,	O
1987b	O
)	O
,	O
which	O
would	O
improve	O
their	O
performance	O
on	O
many	O
noisy	O
data	O
sets	O
.	O
furthermore	O
,	O
their	O
measure	B
of	O
speed	O
,	O
which	O
simply	O
counted	O
the	O
number	O
of	O
example	B
presentations	O
until	O
asymptotic	O
accuracy	O
was	O
attained	O
,	O
unfairly	O
favours	O
id3	O
.	O
whereas	O
the	O
training	O
examples	O
need	O
be	O
given	O
to	O
id3	O
only	O
once	O
,	O
they	O
were	O
repeatedly	O
presented	O
to	O
back-propagation	O
to	O
attain	O
asymptotic	O
accuracies	O
.	O
however	O
,	O
their	O
measure	B
ignored	O
that	O
back-propagation	O
’	O
s	O
cost	O
per	O
example	B
presentation	O
is	O
much	O
lower	O
than	O
id3	O
’	O
s	O
.	O
this	O
measure	B
of	O
speed	O
was	O
later	O
addressed	O
in	O
fisher	O
et	O
al	O
.	O
(	O
1989	O
)	O
,	O
where	O
they	O
deﬁned	O
speed	O
as	O
the	O
product	O
of	O
total	O
example	B
presentations	O
and	O
the	O
cost	O
per	O
presentation	O
.	O
finally	O
,	O
the	O
only	O
data	O
set	O
with	O
industrial	O
ramiﬁcations	O
used	O
in	O
fisher	O
&	O
mckusick	O
(	O
1989	O
)	O
is	O
the	O
garvan	O
institute	O
’	O
s	O
thyroid	O
disease	O
data	O
set	O
.	O
we	O
advocate	O
using	O
more	O
such	O
data	O
sets	O
.	O
second	O
,	O
mooney	O
et	O
al	O
.	O
(	O
1989	O
)	O
and	O
shavlik	O
et	O
al	O
.	O
(	O
1991	O
)	O
compared	O
similar	O
algorithms	O
on	O
a	O
larger	O
collection	B
of	I
data	I
sets	O
.	O
there	O
were	O
only	O
three	O
algorithms	O
involved	O
(	O
i.e	O
.	O
id3	O
,	O
perceptron	O
and	O
back-propagation	O
)	O
.	O
although	O
it	O
is	O
useful	O
to	O
compare	O
the	O
relative	O
perfor-	O
mance	O
of	O
a	O
few	O
algorithms	O
,	O
the	O
symbolic	B
learning	I
and	O
neural	O
network	O
ﬁelds	O
are	O
rapidly	O
developing	O
;	O
there	O
are	O
many	O
newer	O
algorithms	O
that	O
can	O
also	O
solve	O
classiﬁcation	B
tasks	O
(	O
for	O
example	B
,	O
cn2	O
(	O
clark	O
&	O
boswell	O
,	O
1991	O
)	O
,	O
c4.5	O
(	O
quinlan	O
,	O
1987b	O
)	O
,	O
and	O
radial	O
basis	O
networks	O
(	O
poggio	O
&	O
girosi	O
,	O
1990	O
)	O
.	O
many	O
of	O
these	O
can	O
outperform	O
the	O
algorithms	O
selected	O
here	O
.	O
thus	O
,	O
they	O
should	O
also	O
be	O
included	O
in	O
a	O
broader	O
evaluation	O
.	O
in	O
both	O
fisher	O
&	O
mckusick	O
(	O
1989	O
)	O
,	O
mooney	O
et	O
al	O
.	O
(	O
1989	O
)	O
and	O
shavlik	O
et	O
al	O
.	O
(	O
1991	O
)	O
,	O
data	O
sets	O
were	O
separated	O
into	O
a	O
collection	O
of	O
training	O
and	O
test	O
sets	O
.	O
after	O
each	O
system	O
processed	O
a	O
training	O
set	O
its	O
performance	O
,	O
in	O
terms	O
of	O
error	O
rate	O
and	O
training	O
time	O
,	O
was	O
measured	O
on	O
the	O
corresponding	O
test	O
set	O
.	O
the	O
ﬁnal	O
error	O
rate	O
was	O
the	O
geometric	O
means	O
of	O
separate	O
tests	O
.	O
mooney	O
et	O
al	O
.	O
(	O
1989	O
)	O
and	O
shavlik	O
et	O
al	O
.	O
(	O
1991	O
)	O
measured	O
speed	O
differently	O
from	O
fisher	O
et	O
al	O
.	O
(	O
1989	O
)	O
;	O
they	O
used	O
the	O
length	O
of	O
sec	O
.	O
8.8	O
]	O
studies	O
involving	O
ml	O
,	O
k-nn	O
and	O
statistics	O
129	O
training	O
.	O
in	O
both	O
measures	O
,	O
mooney	O
et	O
al	O
.	O
(	O
1989	O
)	O
and	O
shavlik	O
et	O
al	O
.	O
(	O
1991	O
)	O
and	O
fisher	O
et	O
al	O
.	O
(	O
1990	O
)	O
found	O
that	O
back-propagation	O
was	O
signiﬁcantly	O
slower	O
than	O
id3	O
.	O
other	O
signiﬁcant	O
characteristics	O
are	O
:	O
1	O
)	O
they	O
varied	O
the	O
number	O
of	O
training	O
examples	O
and	O
studied	O
the	O
effect	O
on	O
the	O
performance	O
that	O
this	O
will	O
have	O
;	O
and	O
2	O
)	O
they	O
degenerated	O
data	O
in	O
several	O
ways	O
and	O
investigated	O
the	O
sensitivity	O
of	O
the	O
algorithms	O
to	O
the	O
quality	O
of	O
data	O
.	O
8.7	O
studies	O
involving	O
ml	O
,	O
k-nn	O
and	O
statistics	O
thrun	O
,	O
mitchell	O
,	O
and	O
cheng	O
(	O
1991	O
)	O
conducted	O
a	O
co-ordinated	O
comparison	O
study	O
of	O
many	O
algorithms	O
on	O
the	O
monk	O
’	O
s	O
problem	O
.	O
this	O
problem	O
features	O
432	O
simulated	O
robots	O
classiﬁed	O
into	O
two	O
classes	O
using	O
six	O
attributes	O
.	O
although	O
some	O
algorithms	O
outperformed	O
others	O
,	O
there	O
was	O
no	O
apparent	O
analysis	O
of	O
the	O
results	O
.	O
this	O
study	O
is	O
of	O
limited	O
practical	O
interest	O
as	O
it	O
involved	O
simulated	O
data	O
,	O
and	O
,	O
even	O
less	O
realistically	O
,	O
was	O
capable	O
of	O
error-free	O
classiﬁcation	B
.	O
other	O
small-scale	O
comparisons	O
include	O
huang	O
&	O
lippmann	O
(	O
1987	O
)	O
,	O
bonelli	O
&	O
parodi	O
(	O
1991	O
)	O
and	O
sethi	O
&	O
otten	O
(	O
1990	O
)	O
,	O
who	O
all	O
concluded	O
that	O
the	O
various	O
neural	O
networks	O
performed	O
similarly	O
to	O
,	O
or	O
slightly	O
better	O
than	O
,	O
symbolic	O
and	O
statistical	B
algorithms	O
.	O
weiss	O
&	O
kapouleas	O
(	O
1989	O
)	O
involved	O
a	O
few	O
(	O
linear	O
)	O
discriminants	O
and	O
ignored	O
much	O
of	O
the	O
new	O
development	O
in	O
modern	O
statistical	B
classiﬁcation	O
methods	O
.	O
ripley	O
(	O
1993	O
)	O
compared	O
a	O
diverse	O
set	O
of	O
statistical	B
methods	O
,	O
neural	O
networks	O
,	O
and	O
a	O
decision	O
tree	O
classiﬁer	B
on	O
the	O
tsetse	O
ﬂy	O
data	O
.	O
this	O
is	O
a	O
restricted	O
comparison	O
because	O
it	O
has	O
only	O
one	O
data	O
set	O
and	O
includes	O
only	O
one	O
symbolic	O
algorithm	O
.	O
however	O
,	O
some	O
ﬁndings	O
are	O
nevertheless	O
interesting	O
.	O
in	O
accuracy	O
,	O
the	O
results	O
favoured	O
nearest	O
neighbour	O
,	O
the	O
decision	O
tree	O
algorithm	O
,	O
back-propagation	O
and	O
projection	O
pursuit	O
.	O
the	O
decision	O
tree	O
algorithm	O
rapidly	O
produced	O
most	O
interpretable	O
results	O
.	O
more	O
importantly	O
,	O
ripley	O
(	O
1993	O
)	O
also	O
described	O
the	O
“	O
degree	O
of	O
frustration	O
”	O
in	O
getting	O
some	O
algorithms	O
to	O
produce	O
the	O
eventual	O
results	O
(	O
whereas	O
others	O
,	O
for	O
example	B
,	O
fisher	O
&	O
mckusick	O
(	O
1989	O
)	O
and	O
shavlik	O
et	O
al	O
.	O
(	O
1991	O
)	O
did	O
not	O
)	O
.	O
the	O
neural	O
networks	O
were	O
bad	O
in	O
this	O
respect	O
:	O
they	O
were	O
very	O
sensitive	O
to	O
various	O
system	O
settings	O
(	O
for	O
example	B
,	O
hidden	B
units	O
and	O
the	O
stopping	O
criterion	O
)	O
and	O
they	O
generally	O
converged	O
to	O
the	O
ﬁnal	O
accuracies	O
slowly	O
.	O
of	O
course	O
,	O
the	O
inclusion	O
of	O
statistical	B
algorithms	O
does	O
not	O
,	O
of	O
itself	O
,	O
make	O
the	O
com-	O
parisons	O
valid	O
.	O
for	O
example	B
,	O
statisticians	O
would	O
be	O
wary	O
of	O
applying	O
a	O
bayes	O
algorithm	O
to	O
the	O
four	O
problems	O
involved	O
in	O
weiss	O
&	O
kapouleas	O
(	O
1989	O
)	O
because	O
of	O
the	O
lack	O
of	O
basic	O
information	O
regarding	O
the	O
prior	O
and	O
posterior	O
probabilities	O
in	O
the	O
data	O
.	O
this	O
same	O
criticism	O
could	O
be	O
applied	O
to	O
many	O
,	O
if	O
not	O
most	O
,	O
of	O
the	O
datasets	O
in	O
common	O
use	O
.	O
the	O
class	O
pro-	O
portions	O
are	O
clearly	O
unrealistic	O
,	O
and	O
as	O
a	O
result	O
it	O
is	O
difﬁcult	O
to	O
learn	O
the	O
appropriate	O
rule	O
.	O
machine	O
learning	O
algorithms	O
in	O
particular	O
are	O
generally	O
not	O
adaptable	O
to	O
changes	O
in	O
class	O
proportions	O
,	O
although	O
it	O
would	O
be	O
straightforward	O
to	O
implement	O
this	O
.	O
8.8	O
some	O
empirical	O
studies	O
relating	O
to	O
credit	O
risk	O
as	O
this	O
is	O
an	O
important	O
application	O
of	O
machine	O
learning	O
methods	O
,	O
we	O
take	O
some	O
time	O
to	O
mention	O
some	O
previous	O
empirical	O
studies	O
concerning	O
credit	O
datasets	O
.	O
8.8.1	O
traditional	O
and	O
statistical	B
approaches	O
an	O
empirical	O
study	O
of	O
a	O
point	O
awarding	O
approach	O
to	O
credit	O
scoring	O
is	O
made	O
by	O
h¨aussler	O
(	O
1979	O
,	O
1981a	O
,	O
1981b	O
)	O
.	O
fahrmeir	O
et	O
al	O
.	O
(	O
1984	O
)	O
compare	O
the	O
results	O
of	O
a	O
point	O
awarding	O
approach	O
with	O
the	O
results	O
obtained	O
by	O
the	O
linear	O
discriminant	O
.	O
in	O
von	O
stein	O
&	O
ziegler	O
(	O
1984	O
)	O
the	O
authors	O
use	O
thed	O
-nearest	O
neighbour	O
approach	O
to	O
analyse	O
the	O
problem	O
of	O
prognosis	O
and	O
130	O
review	O
of	O
empirical	O
comparisons	O
[	O
ch	O
.	O
8	O
surveillance	O
of	O
corporate	O
credit	O
risk	O
.	O
linear	O
discriminant	O
is	O
applied	O
by	O
bretzger	O
(	O
1991	O
)	O
to	O
early	O
risk	O
recognition	O
in	O
disposition	O
credits	O
.	O
in	O
a	O
comprehensive	O
study	O
of	O
corporate	O
credit	O
granting	O
reported	O
in	O
srinivisan	O
&	O
kim	O
(	O
1987	O
)	O
,	O
the	O
authors	O
evaluate	O
various	O
approaches	O
including	O
parametric	O
,	O
nonparametric	O
and	O
judgemental	O
classiﬁcation	B
procedures	O
.	O
within	O
the	O
nonparametric	O
approaches	O
they	O
use	O
a	O
“	O
recursive	O
partitioning	O
”	O
method	O
based	O
on	O
the	O
decision	O
tree	O
concept	O
.	O
their	O
results	O
show	O
that	O
this	O
“	O
recursive	O
partitioning	O
”	O
approach	O
performs	O
better	O
than	O
the	O
others	O
.	O
8.8.2	O
machine	O
learning	O
and	O
neural	O
networks	O
several	O
empirical	O
studies	O
deal	O
with	O
credit-scoring	O
problem	O
using	O
machine	O
learning	O
and	O
neural	O
networks	O
.	O
the	O
cart	O
method	O
(	O
breiman	O
et	O
al.	O
,	O
1984	O
)	O
is	O
used	O
by	O
hofmann	O
(	O
1990	O
)	O
to	O
analyse	O
consumer	O
credit	O
granting	O
.	O
hofmann	O
concludes	O
that	O
cart	O
has	O
major	O
advantages	O
over	O
discriminant	O
analysis	O
and	O
emphasises	O
the	O
ability	O
of	O
cart	O
to	O
deal	O
with	O
mixed	O
datasets	O
containing	O
both	O
qualitative	O
and	O
quantitative	O
attributes	O
.	O
carter	O
&	O
catlett	O
(	O
1987	O
)	O
use	O
machine	O
learning	O
in	O
assessing	O
credit	O
card	O
applications	O
.	O
besides	O
decision	O
trees	O
they	O
also	O
apply	O
probability	O
trees	O
(	O
that	O
produce	O
probability	O
values	O
to	O
the	O
ﬁnal	O
nodes	O
of	O
the	O
tree	O
)	O
.	O
this	O
means	O
that	O
the	O
algorithm	O
is	O
able	O
decide	O
for	O
a	O
good	O
or	O
bad	O
credit	O
risk	O
with	O
a	O
certain	O
probability	O
attached	O
as	O
well	O
as	O
incorporating	O
costs	O
.	O
one	O
example	B
of	O
the	O
application	O
of	O
neural	O
networks	O
to	O
solving	O
the	O
credit	O
scoring	O
problem	O
is	O
reported	O
in	O
schumann	O
et	O
al	O
.	O
(	O
1992	O
)	O
.	O
michie	O
(	O
1989	O
)	O
reports	O
a	O
case	O
where	O
the	O
aim	O
of	O
the	O
credit-granting	O
procedure	O
was	O
to	O
keep	O
the	O
bad	O
debt	O
rate	O
among	O
those	O
granted	O
credit	O
down	O
to	O
9	O
%	O
.	O
while	O
some	O
procedures	O
accepted	O
only	O
20	O
%	O
of	O
applications	O
,	O
the	O
ml	O
procedure	O
was	O
able	O
to	O
double	O
the	O
proportion	O
of	O
acceptances	O
while	O
keeping	O
the	O
bad-debt	O
rate	O
within	O
bounds	O
.	O
ml	O
procedures	O
almost	O
always	O
output	B
a	O
yes-no	O
decision	O
,	O
and	O
this	O
may	O
be	O
inconvenient	O
in	O
situations	O
where	O
costs	O
may	O
vary	O
from	O
applicant	O
to	O
applicant	O
.	O
in	O
some	O
situations	O
,	O
the	O
bad-debt	O
risk	O
could	O
be	O
allowed	O
to	O
rise	O
to	O
say	O
18	O
%	O
,	O
but	O
it	O
would	O
be	O
necessary	O
to	O
re-train	O
the	O
decision	O
tree	O
,	O
using	O
a	O
different	O
pruning	B
parameter	O
.	O
9	O
dataset	O
descriptions	O
and	O
results	O
various	O
statlog	O
partners	O
see	O
appendix	O
c	O
for	O
a	O
full	O
list|	O
9.1	O
introduction	O
we	O
group	O
the	O
dataset	O
results	O
according	O
to	O
domain	O
type	O
,	O
although	O
this	O
distinction	O
is	O
perhaps	O
arbitrary	O
at	O
times	O
.	O
there	O
are	O
three	O
credit	O
datasets	O
,	O
of	O
which	O
two	O
follow	O
in	O
the	O
next	O
section	O
;	O
the	O
third	O
dataset	O
(	O
german	O
credit	O
)	O
involved	O
a	O
cost	O
matrix	O
,	O
and	O
so	O
is	O
included	O
in	O
section	O
9.4	O
with	O
other	O
cost	O
matrix	O
datasets	O
.	O
several	O
of	O
the	O
datasets	O
involve	O
image	O
data	O
of	O
one	O
form	O
or	O
another	O
.	O
in	O
some	O
cases	O
we	O
are	O
attempting	O
to	O
classify	O
each	O
pixel	O
,	O
and	O
thus	O
segment	O
the	O
image	O
,	O
and	O
in	O
other	O
cases	O
,	O
we	O
need	O
to	O
classify	O
the	O
whole	O
image	O
as	O
an	O
object	O
.	O
similarly	O
the	O
data	O
may	O
be	O
of	O
raw	O
pixel	O
form	O
,	O
or	O
else	O
processed	O
data	O
.	O
these	O
datasets	O
are	O
given	O
in	O
section	O
9.3.	O
the	O
remainder	O
of	O
the	O
datasets	O
are	O
harder	O
to	O
group	O
and	O
are	O
contained	O
in	O
section	O
9.5.	O
see	O
the	O
appendices	O
for	O
general	O
availability	O
of	O
datasets	O
,	O
algorithms	O
and	O
related	O
software	O
.	O
the	O
tables	O
contain	O
information	O
on	O
time	O
,	O
memory	O
and	O
error	O
rates	O
for	O
the	O
training	O
and	O
test	O
sets	O
.	O
the	O
time	O
has	O
been	O
standardised	O
for	O
a	O
sun	O
ipc	O
workstation	O
(	O
quoted	O
at	O
11.1	O
specs	O
)	O
,	O
and	O
for	O
the	O
cross-validation	O
studies	O
the	O
quoted	O
times	O
are	O
the	O
average	O
for	O
each	O
cycle	O
.	O
the	O
unit	O
of	O
memory	O
is	O
the	O
maximum	O
number	O
of	O
pages	O
used	O
during	O
run	O
time	O
.	O
this	O
quantity	O
is	O
obtained	O
from	O
the	O
set	O
time	O
unix	O
command	O
and	O
includes	O
the	O
program	O
requirements	O
as	O
well	O
as	O
data	O
and	O
rules	O
stored	O
during	O
execution	O
.	O
ideally	O
,	O
we	O
would	O
like	O
to	O
decompose	O
this	O
quantity	O
into	O
memory	O
required	O
by	O
the	O
program	O
itself	O
,	O
and	O
the	O
amount	O
during	O
the	O
training	O
,	O
and	O
testing	O
phase	O
,	O
but	O
this	O
was	O
not	O
possible	O
.	O
a	O
page	O
is	O
currently	O
4096	O
bytes	O
,	O
but	O
the	O
quoted	O
ﬁgures	O
are	O
considered	O
to	O
be	O
very	O
crude	O
.	O
indeed	O
,	O
both	O
time	O
and	O
memory	O
measurements	O
should	O
be	O
treated	O
with	O
great	O
caution	O
,	O
and	O
only	O
taken	O
as	O
a	O
rough	O
indication	O
of	O
the	O
truth	O
.	O
in	O
all	O
tables	O
we	O
quote	O
the	O
error	O
rate	O
for	O
the	O
“	O
default	O
”	O
rule	O
,	O
in	O
which	O
each	O
observation	O
is	O
allocated	O
to	O
the	O
most	O
common	O
class	O
.	O
in	O
addition	O
there	O
is	O
a	O
“	O
rank	O
”	O
column	O
which	O
orders	O
the	O
algorithms	O
on	O
the	O
basis	O
of	O
the	O
error	O
rate	O
for	O
the	O
test	O
data	O
.	O
note	O
,	O
however	O
,	O
that	O
this	O
is	O
not	O
the	O
only	O
measure	B
on	O
which	O
they	O
could	O
be	O
ranked	O
,	O
and	O
many	O
practitioners	O
will	O
place	O
great	O
importance	O
on	O
time	O
,	O
memory	O
,	O
or	O
interpretability	O
of	O
the	O
algorithm	O
’	O
s	O
“	O
classifying	O
rule	O
”	O
.	O
we	O
use	O
the	O
notation	O
‘	O
*	O
’	O
for	O
missing	O
(	O
or	O
not	O
applicable	O
)	O
information	O
,	O
and	O
‘	O
fd	O
’	O
to	O
indicate	O
that	O
m	O
address	O
for	O
correspondence	O
:	O
charles	O
taylor	O
,	O
department	O
of	O
statistics	O
,	O
university	O
of	O
leeds	O
,	O
leeds	O
ls2	O
9jt	O
,	O
u.k.	O
132	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
an	O
algorithm	O
failed	O
on	O
that	O
dataset	O
.	O
we	O
tried	O
to	O
determine	O
reasons	O
for	O
failure	O
,	O
but	O
with	O
little	O
success	O
.	O
in	O
most	O
cases	O
it	O
was	O
a	O
“	O
segmentation	O
violation	O
”	O
probably	O
indicating	O
a	O
lack	O
of	O
memory	O
.	O
in	O
section	O
9.6	O
,	O
we	O
present	O
both	O
the	O
statistical	B
and	O
information-based	O
measures	O
for	O
all	O
of	O
the	O
datasets	O
,	O
and	O
give	O
an	O
interpreation	O
for	O
a	O
few	O
of	O
the	O
datasets	O
.	O
9.2	O
credit	O
datasets	O
9.2.1	O
credit	B
management	I
(	O
cred.man	O
)	O
this	O
dataset	O
was	O
donated	O
to	O
the	O
project	O
by	O
a	O
major	O
british	O
engineering	O
company	O
,	O
and	O
comes	O
from	O
the	O
general	O
area	O
of	O
credit	B
management	I
,	O
that	O
is	O
to	O
say	O
,	O
assessing	O
methods	O
for	O
pursuing	O
debt	O
recovery	O
.	O
credit	O
scoring	O
(	O
cs	O
)	O
is	O
one	O
way	O
of	O
giving	O
an	O
objective	O
score	O
indicative	O
of	O
credit	O
risk	O
:	O
it	O
aims	O
to	O
give	O
a	O
numerical	O
score	O
,	O
usually	O
containing	O
components	O
from	O
various	O
factors	O
indicative	O
of	O
risk	O
,	O
by	O
which	O
an	O
objective	O
measure	B
of	O
credit	O
risk	O
can	O
be	O
obtained	O
.	O
the	O
aim	O
of	O
a	O
credit	O
scoring	O
system	O
is	O
to	O
assess	O
the	O
risk	O
associated	O
with	O
each	O
application	O
for	O
credit	O
.	O
being	O
able	O
to	O
assess	O
the	O
risk	O
enables	O
the	O
bank	O
to	O
improve	O
their	O
pricing	O
,	O
marketing	O
and	O
debt	O
recovery	O
procedures	O
.	O
inability	O
to	O
assess	O
the	O
risk	O
can	O
result	O
in	O
lost	O
business	O
.	O
it	O
is	O
also	O
important	O
to	O
assess	O
the	O
determinants	O
of	O
the	O
risk	O
:	O
lawrence	O
&	O
smith	O
(	O
1992	O
)	O
state	O
that	O
payment	O
history	O
is	O
the	O
overwhelming	O
factor	O
in	O
predicting	O
the	O
likelihood	O
of	O
default	O
in	O
mobile	O
home	O
credit	O
cases	O
.	O
risk	O
assessment	O
may	O
inﬂuence	O
the	O
severity	O
with	O
which	O
bad	O
debts	O
are	O
pursued	O
.	O
although	O
it	O
might	O
be	O
thought	O
that	O
the	O
proper	O
end	O
product	O
in	O
this	O
application	O
should	O
be	O
a	O
risk	O
factor	O
or	O
probability	O
assessment	O
rather	O
than	O
a	O
yes-no	O
decision	O
,	O
the	O
dataset	O
was	O
supplied	O
with	O
pre-allocated	O
classes	O
.	O
the	O
aim	O
in	O
this	O
dataset	O
was	O
therefore	O
to	O
classify	O
customers	O
(	O
by	O
simple	O
train-and-test	O
)	O
into	O
one	O
of	O
the	O
two	O
given	O
classes	O
.	O
the	O
classes	O
can	O
be	O
interpreted	O
as	O
the	O
method	O
by	O
which	O
debts	O
will	O
be	O
retrieved	O
,	O
but	O
,	O
for	O
the	O
sake	O
of	O
brevity	O
,	O
we	O
refer	O
to	O
classes	O
as	O
“	O
good	O
”	O
and	O
“	O
bad	O
”	O
risk	O
.	O
table	O
9.1	O
:	O
previously	O
obtained	O
results	O
for	O
the	O
original	O
credit	B
management	I
data	O
,	O
with	O
equal	O
class	O
proportions	O
(	O
*	O
supplied	O
by	O
the	O
turing	O
institute	O
,	O
**	O
supplied	O
by	O
the	O
dataset	O
providers	O
)	O
.	O
algorithm	O
newid*	O
cn2*	O
neural	O
net**	O
error	O
rate	O
0.05	O
0.06	O
0.06	O
the	O
original	O
dataset	O
had	O
20	O
000	O
examples	O
of	O
each	O
class	O
.	O
to	O
make	O
this	O
more	O
repre-	O
sentative	O
of	O
the	O
population	O
as	O
a	O
whole	O
(	O
where	O
approximately	O
5	O
%	O
of	O
credit	O
applicants	O
were	O
assessed	O
–	O
by	O
a	O
human	O
–	O
as	O
bad	O
risk	O
)	O
,	O
the	O
dataset	O
used	O
in	O
the	O
project	O
had	O
20	O
000	O
examples	O
with	O
1000	O
of	O
these	O
being	O
class	O
1	O
(	O
bad	O
credit	O
risk	O
)	O
and	O
19	O
000	O
class	O
2	O
(	O
good	O
credit	O
risk	O
)	O
.	O
as	O
is	O
common	O
when	O
the	O
(	O
true	O
)	O
proportion	O
of	O
bad	O
credits	O
is	O
very	O
small	O
,	O
the	O
default	O
rule	O
(	O
to	O
grant	O
credit	O
to	O
all	O
applicants	O
)	O
achieves	O
a	O
small	O
error	O
rate	O
(	O
which	O
is	O
clearly	O
5	O
%	O
in	O
this	O
case	O
)	O
.	O
in	O
such	O
circumstances	O
the	O
credit-granting	O
company	O
may	O
well	O
adopt	O
the	O
default	O
strategy	O
for	O
the	O
sake	O
of	O
good	O
customer	O
relations	O
–see	O
lawrence	O
&	O
smith	O
(	O
1992	O
)	O
.	O
however	O
,	O
most	O
decision	O
tree	O
algorithms	O
do	O
worse	O
than	O
the	O
default	O
if	O
they	O
are	O
allowed	O
to	O
train	O
on	O
the	O
given	O
data	O
which	O
is	O
strongly	O
biased	O
towards	O
bad	O
credits	O
(	O
typically	O
decision	O
tree	O
algorithms	O
have	O
an	O
error	O
rate	O
of	O
around	O
6	O
%	O
error	O
rate	O
)	O
.	O
this	O
problem	O
disappears	O
if	O
the	O
training	O
set	O
has	O
the	O
proper	O
class	O
pro-	O
portions	O
.	O
for	O
example	B
,	O
a	O
version	O
of	O
cart	O
(	O
the	O
splus	O
module	O
tree	O
(	O
)	O
)	O
obtained	O
an	O
error	O
rate	O
sec	O
.	O
9.2	O
]	O
credit	O
data	O
133	O
table	O
9.2	O
:	O
results	O
for	O
the	O
credit	B
management	I
dataset	O
(	O
2	O
classes	O
,	O
7	O
attributes	O
,	O
(	O
train	O
,	O
test	O
)	O
=	O
(	O
15	O
000	O
,	O
5	O
000	O
)	O
observations	O
)	O
.	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
max	O
.	O
algorithm	O
storage	O
68	O
71	O
889	O
412	O
220	O
108	O
48	O
fd	O
1656	O
104	O
7250	O
1368	O
956	O
2100	O
620	O
377	O
167	O
715	O
218	O
148	O
253	O
476	O
*	O
baytree	O
naivebay	O
cn2	O
c4.5	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
default	O
time	O
(	O
sec	O
.	O
)	O
train	O
32.2	O
67.2	O
165.6	O
27930.0	O
22069.7	O
124187.0	O
370.1	O
fd	O
423.1	O
3035.0	O
5418.0	O
53.1	O
24.3	O
2638.0	O
171.0	O
4470.0	O
553.0	O
*	O
2340.0	O
5950.0	O
435.0	O
2127.0	O
*	O
test	O
3.8	O
12.5	O
14.2	O
5.4	O
*	O
968.0	O
81.4	O
fd	O
415.7	O
2.0	O
3607.0	O
3.3	O
2.8	O
9.5	O
158.0	O
1.9	O
7.2	O
*	O
57.8	O
3.0	O
26.0	O
52.9	O
*	O
error	O
rate	O
test	O
0.033	O
0.050	O
0.030	O
0.020	O
0.031	O
0.088	O
0.047	O
fd	O
0.025	O
0.033	O
0.030	O
0.028	O
0.043	O
0.032	O
0.022	O
0.046	O
0.023	O
0.043	O
0.020	O
0.023	O
0.031	O
0.040	O
0.047	O
train	O
0.031	O
0.051	O
0.031	O
0.021	O
0.033	O
0.028	O
0.051	O
fd	O
0.010	O
0.000	O
0.000	O
0.002	O
0.041	O
0.000	O
0.014	O
0.041	O
0.018	O
0.037	O
0.020	O
0.020	O
0.033	O
0.024	O
0.051	O
rank	O
13	O
21	O
8	O
1	O
10	O
22	O
19	O
6	O
13	O
8	O
7	O
16	O
12	O
3	O
18	O
4	O
16	O
1	O
4	O
10	O
15	O
19	O
of	O
5.8	O
%	O
on	O
the	O
supplied	O
data	O
but	O
only	O
2.35	O
%	O
on	O
the	O
dataset	O
with	O
proper	O
class	O
proportions	O
,	O
whereas	O
linear	O
discriminants	O
obtained	O
an	O
error	O
rate	O
of	O
5.4	O
%	O
on	O
the	O
supplied	O
data	O
and	O
2.35	O
%	O
on	O
the	O
modiﬁed	O
proportions	O
.	O
(	O
the	O
supplier	O
of	O
the	O
credit	B
management	I
dataset	O
quotes	O
error	O
rates	O
for	O
neural	O
nets	O
and	O
decision	O
trees	O
of	O
around	O
5–6	O
%	O
also	O
when	O
trained	O
on	O
the	O
50-50	O
dataset	O
)	O
.	O
note	O
that	O
the	O
effective	O
bias	O
is	O
in	O
favour	O
of	O
the	O
non-statistical	O
algorithms	O
here	O
,	O
as	O
statistical	O
algorithms	O
can	O
cope	O
,	O
to	O
a	O
greater	O
or	O
lesser	O
extent	O
,	O
with	O
prior	O
class	O
proportions	O
that	O
differ	O
from	O
the	O
training	O
proportions	O
.	O
in	O
this	O
dataset	O
the	O
classes	O
were	O
chosen	O
by	O
an	O
expert	O
on	O
the	O
basis	O
of	O
the	O
given	O
attributes	O
(	O
see	O
below	O
)	O
and	O
it	O
is	O
hoped	O
to	O
replace	O
the	O
expert	O
by	O
an	O
algorithm	O
rule	O
in	O
the	O
future	O
.	O
all	O
attribute	O
values	O
are	O
numeric	O
.	O
the	O
dataset	O
providers	O
supplied	O
the	O
performance	O
ﬁgures	O
for	O
algorithms	O
which	O
have	O
been	O
applied	O
to	O
the	O
data	O
drawn	O
from	O
the	O
same	O
source.note	O
that	O
the	O
ﬁgures	O
given	O
in	O
table	O
9.1	O
were	O
achieved	O
using	O
the	O
original	O
dataset	O
with	O
equal	O
numbers	O
of	O
examples	O
of	O
both	O
classes	O
.	O
the	O
best	O
results	O
(	O
in	O
terms	O
of	O
error	O
rate	O
)	O
were	O
achieved	O
by	O
smart	O
,	O
dipol92	O
and	O
the	O
tree	O
algorithms	O
c4.5	O
and	O
cal5	O
.	O
smart	O
is	O
very	O
time	O
consuming	O
to	O
run	O
:	O
however	O
,	O
with	O
credit	O
type	O
datasets	O
small	O
improvements	O
in	O
accuracy	O
can	O
save	O
vast	O
amounts	O
of	O
money	O
so	O
û	O
{	O
	O
134	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
this	O
has	O
to	O
be	O
considered	O
if	O
sacriﬁcing	O
accuracy	O
for	O
time	O
.	O
k-nn	O
did	O
badly	O
due	O
to	O
irrelevant	O
attributes	O
;	O
with	O
a	O
variable	O
selection	O
procedure	O
,	O
it	O
obtained	O
an	O
error	O
rate	O
of	O
3.1	O
%	O
.	O
castle	O
,	O
kohonen	O
,	O
itrule	O
and	O
quadisc	O
perform	O
poorly	O
(	O
the	O
result	O
for	O
quadisc	O
equalling	O
the	O
default	O
rule	O
)	O
.	O
castle	O
uses	O
only	O
attribute	O
7	O
to	O
generate	O
the	O
rule	O
,	O
concluding	O
that	O
this	O
is	O
the	O
only	O
relevant	O
attribute	O
for	O
the	O
classiﬁcation	B
.	O
kohonen	O
works	O
best	O
for	O
datasets	O
with	O
equal	O
class	O
distributions	O
which	O
is	O
not	O
the	O
case	O
for	O
the	O
dataset	O
as	O
preprocessed	O
here	O
.	O
at	O
the	O
cost	O
of	O
signiﬁcantly	O
increasing	O
the	O
cpu	O
time	O
,	O
the	O
performance	O
might	O
be	O
improved	O
by	O
using	O
a	O
larger	O
kohonen	O
net	O
.	O
this	O
dataset	O
.	O
andû	O
the	O
best	O
result	O
for	O
the	O
decision	O
tree	O
algorithms	O
was	O
obtained	O
by	O
c4.5	O
which	O
used	O
the	O
smallest	O
tree	O
with	O
62	O
nodes	O
.	O
cal5	O
used	O
125	O
nodes	O
and	O
achieved	O
a	O
similar	O
error	O
rate	O
;	O
newid	O
	O
used	O
448	O
and	O
415	O
nodes	O
,	O
respectively	O
,	O
which	O
suggests	O
that	O
they	O
over	O
trained	O
on	O
9.2.2	O
australian	O
credit	O
(	O
cr.aust	O
)	O
table	O
9.3	O
:	O
results	O
for	O
the	O
australian	O
credit	O
dataset	O
(	O
2	O
classes	O
,	O
14	O
attributes	O
,	O
690	O
observa-	O
tions	O
,	O
10-fold	O
cross-validation	O
)	O
.	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
max	O
.	O
algorithm	O
storage	O
366	O
353	O
329	O
762	O
102	O
758	O
62	O
149	O
668	O
28	O
404	O
524	O
420	O
215	O
62	O
124	O
128	O
fd	O
52	O
147	O
231	O
81	O
*	O
baytree	O
naivebay	O
cn2	O
c4.5	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
default	O
time	O
(	O
sec	O
.	O
)	O
train	O
31.8	O
30.5	O
21.0	O
246.0	O
876.9	O
3.0	O
46.8	O
68.4	O
34.2	O
15.2	O
400.0	O
7.2	O
3.7	O
42.0	O
6.0	O
173.6	O
24.0	O
fd	O
55.6	O
1369.8	O
12.2	O
260.8	O
*	O
test	O
6.7	O
7.2	O
18.0	O
0.2	O
*	O
7.0	O
5.3	O
1.6	O
32.7	O
0.3	O
14.0	O
0.4	O
0.4	O
3.0	O
1.0	O
0.6	O
2.2	O
fd	O
2.0	O
0.0	O
2.4	O
7.2	O
*	O
error	O
rate	O
test	O
0.141	O
0.207	O
0.141	O
0.158	O
0.201	O
0.181	O
0.148	O
0.145	O
0.152	O
0.181	O
0.181	O
0.171	O
0.151	O
0.204	O
0.155	O
0.137	O
0.131	O
fd	O
0.141	O
0.154	O
0.145	O
0.197	O
0.440	O
train	O
0.139	O
0.185	O
0.125	O
0.090	O
0.194	O
0.000	O
0.144	O
0.145	O
0.081	O
0.000	O
0.000	O
0.000	O
0.136	O
0.001	O
0.099	O
0.162	O
0.132	O
fd	O
0.139	O
0.087	O
0.107	O
0.065	O
0.440	O
rank	O
3	O
21	O
3	O
13	O
19	O
15	O
8	O
6	O
10	O
15	O
15	O
14	O
9	O
20	O
12	O
2	O
1	O
3	O
11	O
6	O
18	O
22	O
the	O
aim	O
is	O
to	O
devise	O
a	O
rule	O
for	O
assessing	O
applications	O
for	O
credit	O
cards	O
.	O
the	O
dataset	O
has	O
been	O
studied	O
before	O
(	O
quinlan	O
,	O
1987a	O
,	O
1993	O
)	O
.	O
interpretation	O
of	O
the	O
results	O
is	O
made	O
difﬁcult	O
because	O
the	O
attributes	O
and	O
classes	O
have	O
been	O
coded	O
to	O
preserve	O
conﬁdentiality	O
,	O
however	O
{	O
û	O
{	O
	O
sec	O
.	O
9.3	O
]	O
image	O
data	O
:	O
object	O
recognition	O
135	O
examples	O
of	O
likely	O
attributes	O
are	O
given	O
for	O
another	O
credit	O
data	O
set	O
in	O
section	O
9.4.3.	O
for	O
our	O
purposes	O
,	O
we	O
replaced	O
the	O
missing	O
values	O
by	O
the	O
overall	O
medians	O
or	O
means	O
(	O
5	O
%	O
of	O
the	O
examples	O
had	O
some	O
missing	O
information	O
)	O
.	O
due	O
to	O
the	O
conﬁdentiality	O
of	O
the	O
classes	O
,	O
it	O
was	O
not	O
possible	O
to	O
assess	O
the	O
relative	O
costs	O
of	O
errors	O
nor	O
to	O
assess	O
the	O
prior	O
odds	O
of	O
good	O
to	O
bad	O
customers	O
.	O
we	O
decided	O
therefore	O
to	O
use	O
the	O
default	O
cost	O
matrix	O
.	O
the	O
use	O
of	O
the	O
default	O
cost	O
matrix	O
is	O
not	O
realistic	O
.	O
in	O
practice	O
it	O
is	O
generally	O
found	O
that	O
it	O
is	O
very	O
difﬁcult	O
to	O
beat	O
the	O
simple	O
rule	O
:	O
“	O
give	O
credit	O
if	O
(	O
and	O
only	O
if	O
)	O
the	O
applicant	O
has	O
a	O
bank	O
account	O
”	O
.	O
we	O
do	O
not	O
know	O
,	O
with	O
this	O
dataset	O
,	O
what	O
success	O
this	O
default	O
rule	O
would	O
have	O
.	O
the	O
results	O
were	O
obtained	O
by	O
10-fold	O
cross	O
validation	O
.	O
the	O
best	O
result	O
here	O
was	O
obtained	O
by	O
cal5	O
,	O
which	O
used	O
only	O
an	O
average	O
of	O
less	O
than	O
6	O
	O
and	O
newid	O
used	O
around	O
70	O
nodes	O
and	O
achieved	O
higher	O
error	O
rates	O
,	O
which	O
suggests	O
that	O
pruning	B
is	O
necessary	O
.	O
nodes	O
in	O
its	O
decision	O
tree	O
.	O
by	O
contrastû	O
9.3	O
image	O
datasets	O
9.3.1	O
handwritten	O
digits	O
(	O
dig44	O
)	O
this	O
dataset	O
consists	O
of	O
18	O
000	O
examples	O
of	O
the	O
digits	O
0	O
to	O
9	O
gathered	O
from	O
postcodes	O
on	O
letters	O
in	O
germany	O
.	O
the	O
handwritten	O
examples	O
were	O
digitised	O
onto	O
images	O
with	O
16	O
	O
16	O
pixels	O
and	O
256	O
grey	O
levels	O
.	O
they	O
were	O
read	O
by	O
one	O
of	O
the	O
automatic	O
address	O
readers	O
built	O
by	O
a	O
german	O
company	O
.	O
these	O
were	O
initially	O
scaled	O
for	O
height	O
and	O
width	O
but	O
not	O
“	O
thinned	O
”	O
or	O
rotated	O
in	O
a	O
standard	O
manner	O
.	O
an	O
example	B
of	O
each	O
digit	O
is	O
given	O
in	O
figure	O
9.1.	O
fig	O
.	O
9.1	O
:	O
hand-written	B
digits	I
from	O
german	O
postcodes	O
(	O
16	O
x	O
16	O
pixels	O
)	O
.	O
the	O
dataset	O
was	O
divided	O
into	O
a	O
training	O
set	O
with	O
900	O
examples	O
per	O
digit	O
and	O
a	O
test	O
set	O
with	O
900	O
examples	O
per	O
digit	O
.	O
due	O
to	O
lack	O
of	O
memory	O
,	O
very	O
few	O
algorithms	O
could	O
cope	O
with	O
the	O
full	O
dataset	O
.	O
in	O
order	O
to	O
get	O
comparable	O
results	O
we	O
used	O
a	O
version	O
with	O
16	O
attributes	O
prepared	O
by	O
averaging	O
over	O
4	O
4	O
neighbourhoods	O
in	O
the	O
original	O
images	O
.	O
for	O
the	O
k-nn	O
classiﬁer	B
this	O
averaging	O
resulted	O
in	O
an	O
increase	O
of	O
the	O
error	O
rate	O
from	O
2.0	O
%	O
to	O
4.7	O
%	O
,	O
whereas	O
for	O
discrim	O
the	O
error	O
rate	O
increased	O
from	O
7.4	O
%	O
to	O
11.4	O
%	O
.	O
backprop	O
could	O
also	O
cope	O
with	O
all	O
256	O
attributes	O
but	O
when	O
presented	O
with	O
all	O
9000	O
examples	O
in	O
the	O
training	O
set	O
took	O
an	O
excessively	O
long	O
time	O
to	O
train	O
(	O
over	O
two	O
cpu	O
days	O
)	O
.	O
the	O
fact	O
that	O
k-nn	O
and	O
lvq	O
do	O
quite	O
well	O
is	O
probably	O
explained	O
by	O
the	O
fact	O
that	O
they	O
make	O
the	O
fewest	O
restrictive	O
assumptions	O
about	O
the	O
data	O
.	O
discriminant	O
analysis	O
,	O
on	O
the	O
other	O
hand	O
,	O
assumes	O
that	O
the	O
data	O
follows	O
a	O
multi-variate	O
normal	O
distribution	O
with	O
the	O
attributes	O
obeying	O
a	O
common	O
covariance	O
matrix	O
and	O
can	O
model	O
only	O
linear	O
aspects	O
of	O
the	O
data	O
.	O
the	O
fact	O
that	O
quadisc	O
,	O
using	O
a	O
reduced	O
version	O
of	O
the	O
dataset	O
,	O
does	O
better	O
than	O
discrim	O
,	O
using	O
either	O
the	O
full	O
version	O
or	O
reduced	O
version	O
,	O
shows	O
the	O
advantage	O
of	O
being	O
able	O
to	O
model	O
non-linearity	O
.	O
castle	O
approximates	O
the	O
data	O
by	O
a	O
polytree	O
and	O
this	O
assumption	O
is	O
too	O
restrictive	O
in	O
this	O
case	O
.	O
naive	O
bayes	O
assumes	O
the	O
attributes	O
are	O
conditionally	O
independent	O
.	O
that	O
naive	O
bayes	O
does	O
so	O
badly	O
is	O
explained	O
by	O
the	O
fact	O
that	O
the	O
attributes	O
are	O
clearly	O
not	O
conditionally	O
independent	O
,	O
since	O
neighbouring	O
pixels	O
are	O
likely	O
to	O
have	O
similar	O
grey	O
levels	O
.	O
it	O
is	O
surprising	O
that	O
cascade	O
does	O
better	O
than	O
backprop	O
,	O
and	O
this	O
may	O
be	O
attributed	O
to	O
the	O
{	O
136	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
9000	O
)	O
observations	O
)	O
.	O
table	O
9.4	O
:	O
results	O
for	O
the	O
4	O
4	O
digit	O
dataset	O
(	O
10	O
classes	O
,	O
16	O
attributes	O
,	O
(	O
train	O
,	O
test	O
)	O
=	O
(	O
9000	O
,	O
time	O
(	O
sec	O
.	O
)	O
train	O
65.3	O
194.4	O
5110.2	O
19490.6	O
1624.0	O
2230.7	O
252.6	O
251.6	O
3614.5	O
500.7	O
10596.0	O
1117.0	O
42.7	O
3325.9	O
778.1	O
1800.1	O
571.0	O
67176.0	O
191.2	O
28910.0	O
1400.0	O
1342.6	O
19171.0	O
*	O
test	O
30.2	O
152.0	O
138.2	O
33.0	O
7041.0	O
2039.2	O
4096.8	O
40.8	O
50.6	O
112.5	O
22415.0	O
59.8	O
61.8	O
119.9	O
60.6	O
9000	O
55.2	O
2075.1	O
43.6	O
110.0	O
250.0	O
123.0	O
1.0	O
*	O
train	O
0.111	O
0.052	O
0.079	O
0.096	O
0.066	O
0.016	O
0.180	O
0.180	O
0.011	O
0.080	O
error	O
rate	O
test	O
0.114	O
0.054	O
0.086	O
0.104	O
0.068	O
0.047	O
0.170	O
0.160	O
0.154	O
0.150	O
0.155	O
0.140	O
0.233	O
0.134	O
0.149	O
0.222	O
0.220	O
0.075	O
0.072	O
0.080	O
0.083	O
0.061	O
0.065	O
0.900	O
0.118	O
0.051	O
0.065	O
0.072	O
0.080	O
0.040	O
0.064	O
0.900	O
0.015	O
0.220	O
0.000	O
0.041	O
*	O
*	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
max	O
.	O
algorithm	O
storage	O
252	O
324	O
1369	O
337	O
393	O
497	O
116	O
240	O
884	O
532	O
770	O
186	O
129	O
1926	O
248	O
504	O
1159	O
646	O
110	O
884	O
268	O
249	O
2442	O
*	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
cascade	O
default	O
baytree	O
naivebay	O
cn2	O
c4.5	O
rank	O
12	O
2	O
10	O
11	O
5	O
1	O
20	O
19	O
17	O
16	O
18	O
14	O
23	O
13	O
15	O
22	O
21	O
7	O
6	O
8	O
9	O
3	O
4	O
24	O
backprop	O
procedure	O
being	O
trapped	O
in	O
a	O
local	O
minimum	O
or	O
to	O
having	O
insufﬁcient	O
time	O
to	O
train	O
.	O
either	O
way	O
,	O
backprop	O
should	O
really	O
do	O
better	O
here	O
,	O
and	O
one	O
suggestion	O
would	O
be	O
to	O
start	O
the	O
backprop	O
procedure	O
with	O
the	O
parameters	O
found	O
from	O
cascade	O
.	O
in	O
this	O
project	O
we	O
ran	O
all	O
algorithms	O
independently	O
,	O
without	O
reference	O
to	O
others	O
,	O
and	O
we	O
did	O
not	O
try	O
to	O
hybridise	O
or	O
run	O
procedures	O
in	O
tandem	O
,	O
although	O
there	O
is	O
no	O
doubt	O
that	O
there	O
would	O
be	O
great	O
beneﬁt	O
from	O
pooling	O
the	O
results	O
.	O
the	O
above	O
dataset	O
is	O
close	O
to	O
“	O
raw	O
”	O
pixel	O
data	O
.	O
a	O
minimum	O
of	O
processing	O
has	O
been	O
carried	O
out	O
,	O
and	O
the	O
results	O
could	O
almost	O
certainly	O
be	O
improved	O
upon	O
using	O
deformable	O
templates	O
or	O
some	O
other	O
statistical	B
pattern	O
recognition	O
technique	O
.	O
note	O
,	O
however	O
,	O
that	O
comparison	O
of	O
performance	O
across	O
handwritten	O
digit	O
datasets	O
should	O
not	O
be	O
made	O
,	O
since	O
they	O
vary	O
widely	O
in	O
quality	O
.	O
in	O
this	O
dataset	O
only	O
zeroes	O
and	O
sevens	O
with	O
strokes	O
are	O
used	O
,	O
and	O
there	O
are	O
a	O
few	O
intentional	O
“	O
mistakes	O
”	O
,	O
for	O
example	B
a	O
digitised	O
“	O
!	O
”	O
is	O
classiﬁed	O
as	O
a	O
1	O
,	O
and	O
the	O
capital	O
“	O
b	O
”	O
is	O
classed	O
as	O
an	O
8.	O
the	O
original	O
256	O
attribute	O
dataset	O
has	O
been	O
analysed	O
by	O
kressel	O
(	O
1991	O
)	O
using	O
(	O
i	O
)	O
a	O
multilayer	O
perceptron	O
with	O
one	O
hidden	B
layer	O
and	O
(	O
ii	O
)	O
linear	O
discriminants	O
with	O
selected	O
û	O
{	O
	O
sec	O
.	O
9.3	O
]	O
image	O
data	O
:	O
object	O
recognition	O
137	O
quadratic	O
terms	O
.	O
both	O
methods	O
achieved	O
about	O
2	O
%	O
error	O
rates	O
on	O
the	O
test	O
set	O
.	O
(	O
2.24	O
%	O
for	O
the	O
linear/quadratic	O
classiﬁer	B
and	O
1.91	O
%	O
errors	O
for	O
the	O
mlp	O
)	O
.	O
hidden	B
layer	O
.	O
9.3.2	O
karhunen-loeve	O
digits	O
(	O
kl	O
)	O
table	O
9.5	O
:	O
results	O
for	O
the	O
kl	O
digits	O
dataset	O
(	O
10	O
classes	O
,	O
40	O
attributes	O
,	O
(	O
train	O
,	O
test	O
)	O
=	O
(	O
9000	O
,	O
9000	O
)	O
observations	O
)	O
.	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
max	O
.	O
algorithm	O
storage	O
306	O
1467	O
1874	O
517	O
500	O
500	O
779	O
fd	O
341	O
1462	O
1444	O
289	O
1453	O
732	O
310	O
1821	O
1739	O
fd	O
221	O
1288	O
268	O
368	O
2540	O
*	O
baytree	O
naivebay	O
cn2	O
c4.5	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
cascade	O
default	O
time	O
(	O
sec	O
.	O
)	O
train	O
87.1	O
1990.2	O
31918.3	O
174965.8	O
23239.9	O
0.0	O
4535.4	O
fd	O
3508.0	O
779.0	O
15155.0	O
1100.4	O
64.9	O
2902.1	O
1437.0	O
*	O
3053.4	O
fd	O
462.8	O
129600.0	O
1700.0	O
1692.1	O
10728.0	O
*	O
test	O
53.9	O
1647.8	O
194.4	O
57.7	O
23279.3	O
6706.4	O
56052.7	O
fd	O
46.9	O
109.0	O
937.0	O
53.0	O
76.0	O
99.7	O
35.5	O
8175.0	O
64.3	O
fd	O
80.0	O
4.0	O
580.0	O
158.1	O
1.0	O
*	O
train	O
0.070	O
0.016	O
0.032	O
0.043	O
0.000	O
0.000	O
0.126	O
fd	O
0.003	O
0.000	O
0.000	O
0.006	O
0.205	O
0.036	O
0.050	O
error	O
rate	O
test	O
0.075	O
0.025	O
0.051	O
0.057	O
0.024	O
0.020	O
0.135	O
fd	O
0.170	O
0.162	O
0.168	O
0.163	O
0.223	O
0.180	O
0.180	O
0.216	O
0.270	O
fd	O
0.039	O
0.049	O
0.055	O
0.026	O
0.075	O
0.900	O
*	O
0.128	O
fd	O
0.030	O
0.041	O
0.048	O
0.011	O
0.063	O
0.900	O
rank	O
10	O
3	O
7	O
9	O
2	O
1	O
12	O
16	O
13	O
15	O
14	O
20	O
17	O
17	O
19	O
21	O
5	O
6	O
8	O
4	O
10	O
22	O
using	O
the	O
ﬁrst	O
40	O
principal	O
components	O
.	O
it	O
is	O
interesting	O
that	O
,	O
with	O
the	O
exception	O
of	O
cascade	O
correlation	O
,	O
the	O
order	O
of	O
performance	O
of	O
the	O
algorithms	O
is	O
virtually	O
unchanged	O
(	O
see	O
table	O
9.5	O
)	O
and	O
that	O
the	O
error	O
rates	O
are	O
now	O
very	O
similar	O
to	O
those	O
obtained	O
(	O
where	O
available	O
)	O
using	O
an	O
alternative	O
data	O
reduction	O
technique	O
(	O
to	O
the	O
4	O
4	O
averaging	O
above	O
)	O
was	O
carried	O
out	O
the	O
original	O
16	O
16	O
pixels	O
.	O
the	O
results	O
for	O
the	O
digits	O
dataset	O
and	O
the	O
kl	O
digits	O
dataset	O
are	O
very	O
similar	O
so	O
are	O
treated	O
together	O
.	O
most	O
algorithms	O
perform	O
a	O
few	O
percent	O
better	O
on	O
the	O
kl	O
digits	O
dataset	O
.	O
the	O
kl	O
digits	O
dataset	O
is	O
the	O
closest	O
to	O
being	O
normal	O
.	O
this	O
could	O
be	O
predicted	O
beforehand	O
,	O
as	O
it	O
is	O
a	O
linear	O
transformation	O
of	O
the	O
attributes	O
that	O
,	O
by	O
the	O
central	O
limit	O
theorem	O
,	O
would	O
be	O
closer	O
to	O
normal	O
than	O
the	O
original	O
.	O
because	O
there	O
are	O
very	O
many	O
attributes	O
in	O
each	O
linear	O
combination	O
,	O
the	O
kl	O
digits	O
dataset	O
is	O
very	O
close	O
to	O
normal	O
(	O
skewness	O
=	O
0.1802	O
,	O
kurtosis	O
=	O
2.9200	O
)	O
as	O
against	O
the	O
exact	O
normal	O
values	O
of	O
(	O
skewness	O
=	O
0	O
,	O
kurtosis	O
=	O
3.0	O
)	O
.	O
û	O
{	O
	O
138	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
in	O
both	O
digits	O
datasets	O
dataset	O
k-nn	O
comes	O
top	O
and	O
rbf	O
and	O
“	O
alloc80	O
”	O
also	O
do	O
fairly	O
well	O
–	O
in	O
fact	O
alloc80	O
failed	O
and	O
an	O
equivalent	O
kernel	O
method	O
,	O
with	O
smoothing	O
parameter	O
asymptotically	O
chosen	O
,	O
was	O
used	O
.	O
these	O
three	O
algorithms	O
are	O
all	O
closely	O
related	O
.	O
kohonen	O
also	O
does	O
well	O
in	O
the	O
digits	O
dataset	O
(	O
but	O
for	O
some	O
reason	O
failed	O
on	O
kl	O
digits	O
)	O
;	O
kohonen	O
has	O
some	O
similarities	O
with	O
k-nn	O
type	O
algorithms	O
.	O
the	O
success	O
of	O
such	O
algorithms	O
suggests	O
that	O
the	O
attributes	O
are	O
equally	O
scaled	O
and	O
equally	O
important	O
.	O
quadisc	O
also	O
does	O
well	O
,	O
coming	O
second	O
in	O
both	O
datasets	O
.	O
the	O
kl	O
version	O
of	O
digits	O
appears	O
to	O
be	O
well	O
suited	O
to	O
quadisc	O
:	O
there	O
is	O
a	O
substantial	O
difference	O
in	O
variances	O
(	O
sd	O
ratio	O
=	O
1.9657	O
)	O
,	O
while	O
at	O
the	O
same	O
time	O
the	O
distributions	O
are	O
not	O
too	O
far	O
from	O
multivariate	O
normality	O
with	O
kurtosis	O
of	O
order	O
3.	O
backprop	O
and	O
lvq	O
do	O
quite	O
well	O
on	O
the	O
	O
digits	O
dataset	O
,	O
bearing	O
out	O
the	O
oft-	O
repeated	O
claim	O
in	O
the	O
neural	O
net	O
literature	O
that	O
neural	O
networks	O
are	O
very	O
well	O
suited	O
to	O
pattern	O
recognition	O
problems	O
(	O
e.g	O
.	O
hecht-nelson	O
,	O
1989	O
)	O
.	O
the	O
decision	O
tree	O
algorithms	O
do	O
not	O
do	O
very	O
well	O
on	O
these	O
digits	O
datasets	O
.	O
the	O
tree	O
sizes	O
are	O
typically	O
in	O
the	O
region	O
of	O
700–1000	O
nodes	O
.	O
9.3.3	O
vehicle	O
silhouettes	O
(	O
vehicle	O
)	O
fig	O
.	O
9.2	O
:	O
vehicle	O
silhouettes	O
prior	O
to	O
high	O
level	O
feature	O
extraction	O
.	O
these	O
are	O
clockwise	O
from	O
top	O
left	O
:	O
double	O
decker	O
bus	O
,	O
opel	O
manta	O
400	O
,	O
saab	O
9000	O
and	O
chevrolet	O
van	O
.	O
a	O
problem	O
in	O
object	O
recognition	O
is	O
to	O
ﬁnd	O
a	O
method	O
of	O
distinguishing	O
3d	O
objects	O
within	O
a	O
2d	O
image	O
by	O
application	O
of	O
an	O
ensemble	O
of	O
shape	O
feature	O
extractors	O
to	O
the	O
2d	O
silhouettes	O
of	O
the	O
objects	O
.	O
this	O
data	O
was	O
originally	O
gathered	O
at	O
the	O
turing	O
institute	O
in	O
1986-87	O
by	O
j.p.	O
siebert	O
.	O
four	O
“	O
corgi	O
”	O
model	O
vehicles	O
were	O
used	O
for	O
the	O
experiment	O
:	O
a	O
double	O
decker	O
bus	O
,	O
chevrolet	O
van	O
,	O
saab	O
9000	O
and	O
an	O
opel	O
manta	O
400.	O
this	O
particular	O
combination	O
of	O
vehicles	O
was	O
chosen	O
with	O
the	O
expectation	O
that	O
the	O
bus	O
,	O
van	O
and	O
either	O
one	O
of	O
the	O
cars	O
would	O
be	O
readily	O
distinguishable	O
,	O
but	O
it	O
would	O
be	O
more	O
difﬁcult	O
to	O
distinguish	O
between	O
the	O
cars	O
.	O
the	O
vehicles	O
were	O
rotated	O
and	O
a	O
number	O
of	O
image	O
silhouettes	O
were	O
obtained	O
from	O
a	O
variety	O
of	O
orientations	O
and	O
angles	O
.	O
all	O
images	O
were	O
captured	O
with	O
a	O
spatial	O
resolution	O
of	O
128	O
	O
sec	O
.	O
9.3	O
]	O
image	O
data	O
:	O
object	O
recognition	O
139	O
128	O
pixels	O
quantised	O
to	O
64	O
grey	O
levels	O
.	O
these	O
images	O
were	O
cleaned	O
up	O
,	O
binarised	O
and	O
subsequently	O
processed	O
to	O
produce	O
18	O
variables	O
intended	O
to	O
characterise	O
shape	O
.	O
for	O
example	B
,	O
circularity	O
,	O
radius	O
ratio	O
,	O
compact-	O
ness	O
,	O
scaled	O
variance	O
along	O
major	O
and	O
minor	O
axes	O
,	O
etc	O
.	O
a	O
total	O
of	O
946	O
examples	O
were	O
obtained	O
but	O
100	O
were	O
retained	O
in	O
case	O
of	O
dispute	O
,	O
so	O
the	O
trials	O
reported	O
here	O
used	O
only	O
846	O
examples	O
and	O
the	O
algorithms	O
were	O
run	O
using	O
9-fold	O
cross-validation	O
to	O
obtain	O
error	O
rates	O
,	O
given	O
in	O
table	O
9.6	O
table	O
9.6	O
:	O
results	O
for	O
the	O
vehicle	O
dataset	O
(	O
4	O
classes	O
,	O
18	O
attributes	O
,	O
846	O
observations	O
,	O
9-fold	O
cross-validation	O
)	O
.	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
max	O
.	O
algorithm	O
storage	O
231	O
593	O
685	O
105	O
227	O
104	O
80	O
158	O
296	O
*	O
776	O
71	O
56	O
*	O
*	O
307	O
171	O
1441	O
64	O
186	O
716	O
77	O
238	O
*	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
cascade	O
default	O
baytree	O
naivebay	O
cn2	O
c4.5	O
time	O
(	O
sec	O
.	O
)	O
train	O
16.3	O
250.9	O
757.9	O
2502.5	O
30.0	O
163.8	O
13.1	O
24.4	O
113.3	O
18.0	O
3135.0	O
27.1	O
5.4	O
100.0	O
174.0	O
985.3	O
23.3	O
5962.0	O
150.6	O
14411.2	O
1735.9	O
229.1	O
289.0	O
*	O
test	O
3.0	O
28.6	O
8.3	O
0.7	O
10.0	O
22.7	O
1.8	O
0.8	O
0.4	O
1.0	O
121.0	O
0.5	O
0.6	O
1.0	O
2.0	O
*	O
0.5	O
50.4	O
8.2	O
3.7	O
11.8	O
2.8	O
1.0	O
*	O
train	O
0.202	O
0.085	O
0.167	O
0.062	O
0.000	O
0.000	O
0.545	O
0.284	O
0.047	O
0.030	O
error	O
rate	O
test	O
0.216	O
0.150	O
0.192	O
0.217	O
0.173	O
0.275	O
0.505	O
0.235	O
0.298	O
0.298	O
0.296	O
0.271	O
0.558	O
0.314	O
0.266	O
0.324	O
0.279	O
0.340	O
0.151	O
0.207	O
0.307	O
0.287	O
0.280	O
0.750	O
0.068	O
0.115	O
0.079	O
0.168	O
0.098	O
0.171	O
0.263	O
0.750	O
0.079	O
0.519	O
0.018	O
0.065	O
*	O
*	O
rank	O
6	O
1	O
4	O
7	O
3	O
11	O
22	O
8	O
16	O
16	O
15	O
10	O
23	O
19	O
9	O
20	O
12	O
21	O
2	O
5	O
18	O
14	O
13	O
24	O
one	O
would	O
expect	O
this	O
dataset	O
to	O
be	O
non-linear	O
since	O
the	O
attributes	O
depend	O
on	O
the	O
angle	O
at	O
which	O
the	O
vehicle	O
is	O
viewed	O
.	O
therefore	O
they	O
are	O
likely	O
to	O
have	O
a	O
sinusoidal	O
dependence	O
,	O
although	O
this	O
dependence	O
was	O
masked	O
by	O
issuing	O
the	O
dataset	O
in	O
permuted	O
order	O
.	O
quadisc	O
does	O
very	O
well	O
,	O
and	O
this	O
is	O
due	O
to	O
the	O
highly	O
non-linear	O
behaviour	O
of	O
this	O
data	O
.	O
one	O
would	O
have	O
expected	O
the	O
backprop	O
algorithm	O
to	O
perform	O
well	O
on	O
this	O
dataset	O
since	O
,	O
it	O
is	O
claimed	O
,	O
backprop	O
can	O
successfully	O
model	O
the	O
non-linear	O
aspects	O
of	O
a	O
dataset	O
.	O
however	O
,	O
backprop	O
is	O
not	O
straightforward	O
to	O
run	O
.	O
unlike	O
discriminant	O
analysis	O
,	O
which	O
requires	O
no	O
choice	O
of	O
free	O
parameters	O
,	O
backprop	O
requires	O
essentially	O
two	O
free	O
parameters	O
-	O
the	O
number	O
of	O
hidden	B
û	O
{	O
	O
140	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
nodes	O
and	O
the	O
training	O
time	O
.	O
neither	O
of	O
these	O
is	O
straightforward	O
to	O
decide	O
.	O
this	O
ﬁgure	O
for	O
backprop	O
was	O
obtained	O
using	O
5	O
hidden	B
nodes	O
and	O
a	O
training	O
time	O
of	O
four	O
hours	O
for	O
the	O
training	O
time	O
in	O
each	O
of	O
the	O
nine	O
cycles	O
of	O
cross-validation	O
.	O
however	O
,	O
one	O
can	O
say	O
that	O
the	O
sheer	O
effort	O
and	O
time	O
taken	O
to	O
optimise	O
the	O
performance	O
for	O
backprop	O
is	O
a	O
major	O
disadvantage	O
compared	O
to	O
quadisc	O
which	O
can	O
achieve	O
a	O
much	O
better	O
result	O
with	O
a	O
lot	O
less	O
effort	O
.	O
dipol92	O
does	O
nearly	O
as	O
well	O
as	O
quadisc	O
.	O
as	O
compared	O
with	O
backprop	O
it	O
performs	O
better	O
and	O
is	O
quicker	O
to	O
run	O
.	O
it	O
determines	O
the	O
number	O
of	O
nodes	O
(	O
hyperplanes	O
,	O
neurons	O
)	O
and	O
the	O
initial	O
weights	O
by	O
a	O
reasonable	O
procedure	O
at	O
the	O
beginning	O
and	O
doesn	O
’	O
t	O
use	O
an	O
additional	O
layer	O
of	O
hidden	B
units	O
but	O
instead	O
a	O
symbolic	O
level	O
.	O
the	O
poor	O
performance	O
of	O
castle	O
is	O
explained	O
by	O
the	O
fact	O
that	O
the	O
attributes	O
are	O
highly	O
correlated	O
.	O
in	O
consequence	O
the	O
relationship	O
between	O
class	O
and	O
attributes	O
is	O
not	O
built	O
strongly	O
into	O
the	O
polytree	O
.	O
the	O
same	O
explanation	O
accounts	O
for	O
the	O
poor	O
performance	O
of	O
naive	O
bayes	O
.	O
k-nn	O
,	O
which	O
performed	O
so	O
well	O
on	O
the	O
raw	O
digits	O
dataset	O
,	O
does	O
not	O
do	O
so	O
well	O
here	O
.	O
this	O
is	O
probably	O
because	O
in	O
the	O
case	O
of	O
the	O
digits	O
the	O
attributes	O
were	O
all	O
commensurate	O
and	O
carried	O
equal	O
weight	O
.	O
in	O
the	O
vehicle	O
dataset	O
the	O
attributes	O
all	O
have	O
different	O
meanings	O
and	O
it	O
is	O
not	O
clear	O
how	O
to	O
build	O
an	O
appropriate	O
distance	O
measure	B
.	O
the	O
attributes	O
for	O
the	O
vehicle	O
dataset	O
,	O
unlike	O
the	O
other	O
image	O
analysis	O
,	O
were	O
generated	O
using	O
image	O
analysis	O
tools	O
and	O
were	O
not	O
simply	O
based	O
on	O
brightness	O
levels	O
.	O
this	O
suggests	O
that	O
the	O
attributes	O
are	O
less	O
likely	O
to	O
be	O
equally	O
scaled	O
and	O
equally	O
important	O
.	O
this	O
is	O
conﬁrmed	O
by	O
the	O
lower	O
performances	O
of	O
k-nn	O
,	O
lvq	O
and	O
radial	O
basis	O
functions	O
,	O
which	O
treat	O
all	O
attributes	O
equally	O
and	O
have	O
a	O
built	O
in	O
mechanism	O
for	O
normalising	O
,	O
which	O
is	O
often	O
not	O
optimal	O
.	O
alloc80	O
did	O
not	O
perform	O
well	O
here	O
,	O
and	O
so	O
an	O
alternative	O
kernel	O
method	O
was	O
used	O
which	O
allowed	O
for	O
correlations	O
between	O
the	O
attributes	O
,	O
and	O
this	O
appeared	O
to	O
be	O
more	O
robust	O
than	O
the	O
other	O
three	O
algorithms	O
although	O
it	O
still	O
fails	O
to	O
learn	O
the	O
difference	O
between	O
the	O
cars	O
.	O
the	O
original	O
siebert	O
(	O
1987	O
)	O
paper	O
showed	O
machine	O
learning	O
performing	O
better	O
	O
and	O
cal5	O
were	O
116	O
and	O
156	O
nodes	O
,	O
respectively	O
.	O
than	O
k-nn	O
,	O
but	O
there	O
is	O
not	O
much	O
support	O
for	O
this	O
in	O
our	O
results	O
.	O
the	O
tree	O
sizes	O
forû	O
the	O
high	O
value	O
of	O
fract2	O
=	O
0.8189	O
(	O
see	O
table	O
9.30	O
)	O
might	O
indicate	O
that	O
linear	O
discrimina-	O
tion	B
could	O
be	O
based	O
on	O
just	O
two	O
discriminants	O
.	O
this	O
may	O
relate	O
to	O
the	O
fact	O
that	O
the	O
two	O
cars	O
are	O
not	O
easily	O
distinguishable	O
,	O
so	O
might	O
be	O
treated	O
as	O
one	O
(	O
reducing	O
dimensionality	O
of	O
the	O
mean	O
vectors	O
to	O
3d	O
)	O
.	O
however	O
,	O
although	O
the	O
fraction	O
of	O
discriminating	O
power	O
for	O
the	O
third	O
discriminant	O
is	O
low	O
(	O
1	O
-	O
0.8189	O
)	O
,	O
it	O
is	O
still	O
statistically	O
signiﬁcant	O
,	O
so	O
can	O
not	O
be	O
discarded	O
without	O
a	O
small	O
loss	O
of	O
discrimination	O
.	O
9.3.4	O
letter	B
recognition	I
(	O
letter	O
)	O
the	O
dataset	O
was	O
constructed	O
by	O
david	O
j.	O
slate	O
,	O
odesta	O
corporation	O
,	O
evanston	O
,	O
il	O
60201.	O
the	O
objective	O
here	O
is	O
to	O
classify	O
each	O
of	O
a	O
large	O
number	O
of	O
black	O
and	O
white	O
rectangular	O
pixel	O
displays	O
as	O
one	O
of	O
the	O
26	O
capital	O
letters	O
of	O
the	O
english	O
alphabet	O
.	O
(	O
one-shot	O
train	O
and	O
test	O
was	O
used	O
for	O
the	O
classiﬁcation	B
.	O
)	O
the	O
character	O
images	O
produced	O
were	O
based	O
on	O
20	O
different	O
fonts	O
and	O
each	O
letter	O
within	O
these	O
fonts	O
was	O
randomly	O
distorted	O
to	O
produce	O
a	O
ﬁle	O
of	O
20	O
000	O
unique	O
images	O
.	O
for	O
each	O
image	O
,	O
16	O
numerical	O
attributes	O
were	O
calculated	O
using	O
edge	O
counts	O
and	O
measures	O
of	O
statistical	B
moments	O
which	O
were	O
scaled	O
and	O
discretised	O
into	O
a	O
range	O
of	O
integer	O
values	O
from	O
0	O
to	O
15.	O
perfect	O
classiﬁcation	B
performance	O
is	O
unlikely	O
to	O
be	O
possible	O
with	O
this	O
dataset	O
.	O
one	O
of	O
the	O
fonts	O
used	O
,	O
gothic	O
roman	O
,	O
appears	O
very	O
different	O
from	O
the	O
others	O
.	O
{	O
sec	O
.	O
9.3	O
]	O
image	O
data	O
:	O
object	O
recognition	O
141	O
table	O
9.7	O
:	O
results	O
for	O
the	O
letters	O
dataset	O
(	O
26	O
classes	O
,	O
16	O
attributes	O
,	O
(	O
train	O
,	O
test	O
)	O
=	O
(	O
15	O
000	O
,	O
5000	O
)	O
observations	O
)	O
.	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
max	O
.	O
algorithm	O
storage	O
78	O
80	O
316	O
881	O
758	O
200	O
1577	O
fd	O
3600	O
376	O
2033	O
2516	O
1464	O
*	O
1042	O
593	O
1554	O
1204	O
189	O
154	O
418	O
377	O
*	O
baytree	O
naivebay	O
cn2	O
c4.5	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
default	O
time	O
(	O
sec	O
.	O
)	O
train	O
325.6	O
3736.2	O
5061.6	O
400919.0	O
39574.7	O
14.8	O
9455.3	O
fd	O
1098.2	O
1056.0	O
2529.0	O
275.5	O
74.6	O
40458.3	O
309.0	O
22325.4	O
1033.4	O
*	O
1303.4	O
277445.0	O
*	O
1487.4	O
*	O
test	O
84.0	O
1222.7	O
38.7	O
184.0	O
*	O
2135.4	O
2933.4	O
fd	O
1020.2	O
2.0	O
92.0	O
7.1	O
17.9	O
52.2	O
292.0	O
69.1	O
8.2	O
*	O
79.5	O
22.0	O
*	O
47.8	O
*	O
error	O
rate	O
test	O
0.302	O
0.113	O
0.234	O
0.295	O
0.064	O
0.068	O
0.245	O
fd	O
0.130	O
0.128	O
0.245	O
0.124	O
0.529	O
0.115	O
0.132	O
0.594	O
0.253	O
0.252	O
0.176	O
0.327	O
0.233	O
0.079	O
0.960	O
train	O
0.297	O
0.101	O
0.234	O
0.287	O
0.065	O
0.000	O
0.237	O
fd	O
0.010	O
0.000	O
0.000	O
0.015	O
0.516	O
0.021	O
0.042	O
0.585	O
0.158	O
0.218	O
0.167	O
0.323	O
0.220	O
0.057	O
0.955	O
rank	O
18	O
4	O
12	O
17	O
1	O
2	O
13	O
8	O
7	O
13	O
6	O
20	O
5	O
9	O
21	O
16	O
15	O
10	O
19	O
11	O
3	O
22	O
quadisc	O
is	O
the	O
best	O
of	O
the	O
classical	O
statistical	B
algorithms	O
on	O
this	O
dataset	O
.	O
this	O
is	O
perhaps	O
not	O
surprising	O
since	O
the	O
measures	O
data	O
gives	O
some	O
support	O
to	O
the	O
assumptions	O
underlying	O
the	O
method	O
.	O
discrim	O
does	O
not	O
perform	O
well	O
although	O
the	O
logistic	O
version	O
is	O
a	O
signiﬁcant	O
improvement	O
.	O
smart	O
is	O
used	O
here	O
with	O
a	O
22	O
term	O
model	O
and	O
its	O
poor	O
performance	O
is	O
surprising	O
.	O
a	O
number	O
of	O
the	O
attributes	O
are	O
non–linear	O
combinations	O
of	O
some	O
others	O
and	O
smart	O
might	O
have	O
been	O
expected	O
to	O
model	O
this	O
well	O
.	O
alloc80	O
achieves	O
the	O
best	O
performance	O
of	O
all	O
with	O
k-nn	O
close	O
behind	O
.	O
in	O
this	O
dataset	O
all	O
the	O
attributes	O
are	O
pre–scaled	O
and	O
all	O
appear	O
to	O
be	O
important	O
so	O
good	O
performance	O
from	O
k-nn	O
is	O
to	O
be	O
expected	O
.	O
castle	O
constructs	O
a	O
polytree	O
with	O
only	O
one	O
attribute	O
contributing	O
to	O
the	O
classiﬁcation	B
which	O
is	O
too	O
restrictive	O
with	O
this	O
dataset	O
.	O
naive	O
bayes	O
assumes	O
conditional	O
independence	O
and	O
this	O
is	O
certainly	O
not	O
satisﬁed	O
for	O
a	O
number	O
of	O
the	O
attributes	O
.	O
newid	O
andû	O
on	O
3000	O
examples	O
drawn	O
from	O
the	O
full	O
training	O
set	O
and	O
that	O
in	O
part	O
explains	O
their	O
rather	O
uninspiring	O
performance	O
.	O
newid	O
builds	O
a	O
huge	O
tree	O
containing	O
over	O
1760	O
nodes	O
while	O
the	O
tree	O
is	O
about	O
half	O
the	O
size	O
.	O
this	O
difference	O
probably	O
explains	O
some	O
of	O
the	O
difference	O
in	O
their	O
respective	O
results	O
.	O
cal5	O
and	O
c4.5	O
also	O
build	O
complex	O
trees	O
while	O
cn2	O
generates	O
448	O
rules	O
in	O
order	O
to	O
classify	O
the	O
training	O
set	O
.	O
itrule	O
is	O
the	O
poorest	O
algorithm	O
on	O
this	O
dataset	O
.	O
	O
were	O
only	O
trained	O
û	O
{	O
	O
{	O
û	O
{	O
	O
142	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
generally	O
we	O
would	O
not	O
expect	O
itrule	O
to	O
perform	O
well	O
on	O
datasets	O
where	O
many	O
of	O
the	O
attributes	O
contributed	O
to	O
the	O
classiﬁcation	B
as	O
it	O
is	O
severely	O
constrained	O
in	O
the	O
complexity	O
of	O
the	O
rules	O
it	O
can	O
construct	O
.	O
of	O
the	O
neural	O
network	O
algorithms	O
,	O
kohonen	O
and	O
lvq	O
would	O
be	O
expected	O
to	O
perform	O
well	O
for	O
the	O
same	O
reasons	O
as	O
k-nn	O
.	O
seen	O
in	O
that	O
light	O
,	O
the	O
kohonen	O
result	O
is	O
a	O
little	O
disappointing	O
.	O
in	O
a	O
previous	O
study	O
frey	O
&	O
slate	O
(	O
1991	O
)	O
investigated	O
the	O
use	O
of	O
an	O
adaptive	O
classiﬁer	B
system	O
and	O
achieved	O
a	O
best	O
error	O
rate	O
of	O
just	O
under	O
20	O
%	O
.	O
rank	O
3	O
1	O
8	O
7	O
18	O
5	O
15	O
9.3.5	O
chromosomes	B
(	O
chrom	O
)	O
table	O
9.8	O
:	O
results	O
for	O
the	O
chromosome	O
dataset	O
(	O
24	O
classes	O
,	O
16	O
attributes	O
,	O
(	O
train	O
,	O
test	O
)	O
=	O
(	O
20	O
000	O
,	O
20	O
000	O
)	O
observations	O
)	O
.	O
max	O
.	O
algorithm	O
storage	O
1586	O
1809	O
1925	O
1164	O
1325	O
1097	O
279	O
fd	O
3768	O
1283	O
1444	O
2840	O
1812	O
1415	O
589	O
637	O
1071	O
1605	O
213	O
fd	O
471	O
373	O
*	O
time	O
(	O
sec	O
.	O
)	O
train	O
830.0	O
1986.3	O
20392.8	O
307515.4	O
184435.0	O
20.1	O
230.2	O
fd	O
2860.3	O
552.0	O
1998.0	O
1369.5	O
107.8	O
9192.6	O
1055.3	O
34348.0	O
564.5	O
*	O
961.8	O
fd	O
*	O
1065.5	O
*	O
test	O
357.0	O
1607.0	O
291.4	O
92.9	O
*	O
14140.6	O
96.2	O
fd	O
2763.8	O
17.0	O
138.0	O
29.7	O
61.0	O
131.9	O
*	O
30.0	O
31.5	O
*	O
258.2	O
fd	O
*	O
*	O
*	O
error	O
rate	O
test	O
0.107	O
0.084	O
0.131	O
0.128	O
0.253	O
0.123	O
0.178	O
fd	O
0.173	O
0.176	O
0.234	O
0.164	O
0.324	O
0.150	O
0.175	O
0.697	O
0.244	O
0.174	O
0.091	O
fd	O
0.129	O
0.121	O
0.956	O
train	O
0.073	O
0.046	O
0.079	O
0.082	O
0.192	O
0.000	O
0.129	O
fd	O
0.007	O
0.000	O
0.000	O
0.034	O
0.260	O
0.010	O
0.038	O
0.681	O
0.142	O
0.109	O
0.049	O
fd	O
0.087	O
0.067	O
0.956	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
baytree	O
naivebay	O
cn2	O
c4.5	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
default	O
11	O
14	O
16	O
10	O
19	O
9	O
13	O
20	O
17	O
12	O
2	O
6	O
4	O
21	O
this	O
data	O
was	O
obtained	O
via	O
the	O
mrc	O
human	O
genetics	O
unit	O
,	O
edinburgh	O
from	O
the	O
routine	O
amniotic	O
2668	O
cell	O
data	O
set	O
(	O
courtesy	O
c.	O
lundsteen	O
,	O
righospitalet	O
,	O
copenhagen	O
)	O
.	O
in	O
our	O
trials	O
we	O
used	O
only	O
16	O
features	O
(	O
and	O
40	O
000	O
examples	O
)	O
which	O
are	O
a	O
subset	O
of	O
a	O
larger	O
database	O
which	O
has	O
30	O
features	O
and	O
nearly	O
80	O
000	O
examples	O
.	O
the	O
subset	O
was	O
selected	O
to	O
reduce	O
the	O
scale	O
of	O
the	O
problem	O
,	O
and	O
selecting	O
the	O
features	O
deﬁned	O
as	O
level	O
1	O
(	O
measured	O
directly	O
from	O
the	O
chromosome	O
image	O
)	O
and	O
level	O
2	O
(	O
measures	O
requiring	O
the	O
axis	O
,	O
e.g	O
.	O
length	O
,	O
to	O
be	O
speciﬁed	O
)	O
.	O
we	O
omitted	O
observations	O
with	O
an	O
“	O
unknown	O
”	O
class	O
as	O
well	O
as	O
features	O
with	O
level	O
3	O
(	O
requiring	O
both	O
axis	O
and	O
proﬁle	O
and	O
knowledge	O
of	O
the	O
chromosome	O
polarity	O
)	O
û	O
{	O
	O
sec	O
.	O
9.3	O
]	O
image	O
data	O
:	O
segmentation	O
143	O
and	O
level	O
4	O
(	O
requiring	O
both	O
the	O
axis	O
and	O
both	O
the	O
polarity	O
and	O
the	O
centrometre	O
location	O
)	O
.	O
classiﬁcation	B
was	O
done	O
using	O
one-shot	O
train-and-test	O
.	O
the	O
result	O
for	O
alloc80	O
is	O
very	O
poor	O
,	O
and	O
the	O
reason	O
for	O
this	O
is	O
not	O
clear	O
.	O
an	O
alternative	O
kernel	O
classiﬁer	B
(	O
using	O
a	O
cauchy	O
kernel	O
,	O
to	O
avoid	O
numerical	O
difﬁculties	O
)	O
gave	O
an	O
error	O
rate	O
of	O
10.67	O
%	O
which	O
is	O
much	O
better	O
.	O
although	O
quadratic	O
discriminants	O
do	O
best	O
here	O
,	O
there	O
is	O
reason	O
to	O
believe	O
that	O
its	O
error	O
rate	O
is	O
perhaps	O
not	O
optimal	O
as	O
there	O
is	O
clear	O
evidence	O
of	O
non-normality	O
in	O
the	O
distribution	O
of	O
the	O
attributes	O
.	O
the	O
best	O
of	O
decision	O
tree	O
results	O
is	O
obtained	O
by	O
cn2	O
which	O
has	O
301	O
rules	O
.	O
c4.5	O
and	O
by	O
contrast	O
newid	O
has	O
2967	O
terminal	O
nodes	O
,	O
but	O
does	O
about	O
as	O
well	O
as	O
c4.5	O
.	O
	O
have	O
856	O
and	O
626	O
terminal	O
nodes	O
,	O
respectively	O
,	O
and	O
yet	O
obtain	O
very	O
differnt	O
error	O
rates	O
.	O
further	O
details	O
of	O
this	O
dataset	O
can	O
be	O
found	O
in	O
piper	O
&	O
granum	O
(	O
1989	O
)	O
who	O
have	O
done	O
extensive	O
experiments	O
on	O
selection	O
and	O
measurement	O
of	O
variables	O
.	O
for	O
the	O
dataset	O
which	O
resembled	O
the	O
one	O
above	O
most	O
closely	O
,	O
they	O
achieved	O
an	O
error	O
rate	O
of	O
9.2	O
%	O
.	O
9.3.6	O
landsat	O
satellite	B
image	I
(	O
satim	O
)	O
the	O
original	O
landsat	O
data	O
for	O
this	O
database	O
was	O
generated	O
from	O
data	O
purchased	O
from	O
nasa	O
by	O
the	O
australian	O
centre	O
for	O
remote	O
sensing	O
,	O
and	O
used	O
for	O
research	O
at	O
the	O
university	O
of	O
new	O
south	O
wales	O
.	O
the	O
sample	O
database	O
was	O
generated	O
taking	O
a	O
small	O
section	O
(	O
82	O
rows	O
and	O
100	O
columns	O
)	O
from	O
the	O
original	O
data	O
.	O
the	O
classiﬁcation	B
for	O
each	O
pixel	O
was	O
performed	O
on	O
the	O
basis	O
of	O
an	O
actual	O
site	O
visit	O
by	O
ms.	O
karen	O
hall	O
,	O
when	O
working	O
for	O
professor	O
john	O
a.	O
richards	O
,	O
at	O
the	O
centre	O
for	O
remote	O
sensing	O
.	O
the	O
database	O
is	O
a	O
(	O
tiny	O
)	O
sub-area	O
of	O
a	O
scene	O
,	O
consisting	O
of	O
82	O
100	O
pixels	O
,	O
each	O
pixel	O
covering	O
an	O
area	O
on	O
the	O
ground	O
of	O
approximately	O
80*80	O
metres	O
.	O
the	O
information	O
given	O
for	O
each	O
pixel	O
consists	O
of	O
the	O
class	O
value	O
and	O
the	O
intensities	O
in	O
four	O
spectral	O
bands	O
(	O
from	O
the	O
green	O
,	O
red	O
,	O
and	O
infra-red	O
regions	O
of	O
the	O
spectrum	O
)	O
.	O
the	O
original	O
data	O
are	O
presented	O
graphically	O
in	O
figure	O
9.3.	O
the	O
ﬁrst	O
four	O
plots	O
(	O
top	O
row	O
and	O
bottom	O
left	O
)	O
show	O
the	O
intensities	O
in	O
four	O
spectral	O
bands	O
:	O
spectral	O
bands	O
1	O
and	O
2	O
are	O
in	O
the	O
green	O
and	O
red	O
regions	O
of	O
the	O
visible	O
spectrum	O
,	O
while	O
spectral	O
bands	O
3	O
and	O
4	O
are	O
in	O
the	O
infra-red	O
(	O
darkest	O
shadings	O
represent	O
greatest	O
intensity	O
)	O
.	O
the	O
middle	O
bottom	O
diagram	O
shows	O
the	O
land	O
use	O
,	O
with	O
shadings	O
representing	O
the	O
seven	O
original	O
classes	O
in	O
the	O
order	O
:	O
red	O
soil	O
,	O
cotton	O
crop	O
,	O
vegetation	O
stubble	O
,	O
mixture	O
(	O
all	O
types	O
present	O
)	O
,	O
grey	O
soil	O
,	O
damp	O
grey	O
soil	O
and	O
very	O
damp	O
grey	O
soil	O
,	O
with	O
red	O
as	O
lightest	O
and	O
very	O
damp	O
grey	O
as	O
darkest	O
shading	O
.	O
also	O
shown	O
(	O
bottom	O
right	O
)	O
are	O
the	O
classes	O
as	O
predicted	O
by	O
linear	O
discriminants	O
.	O
note	O
that	O
the	O
most	O
accurate	O
predictions	O
are	O
for	O
cotton	O
crop	O
(	O
rectangular	O
region	O
bottom	O
left	O
of	O
picture	O
)	O
,	O
and	O
that	O
the	O
predicted	O
boundary	O
damp-vary	O
damp	O
grey	O
soil	O
(	O
l-shape	O
top	O
left	O
of	O
picture	O
)	O
is	O
not	O
well	O
positioned	O
.	O
so	O
that	O
information	O
from	O
the	O
neighbourhood	O
of	O
a	O
pixel	O
might	O
contribute	O
to	O
the	O
classiﬁ-	O
cation	O
of	O
that	O
pixel	O
,	O
the	O
spectra	O
of	O
the	O
eight	O
neighbours	O
of	O
a	O
pixel	O
were	O
included	O
as	O
attributes	O
together	O
with	O
the	O
four	O
spectra	O
of	O
that	O
pixel	O
.	O
each	O
line	O
of	O
data	O
corresponds	O
to	O
a	O
3	O
3	O
square	O
neighbourhood	O
of	O
pixels	O
completely	O
contained	O
within	O
the	O
82	O
100	O
sub-area	O
.	O
thus	O
each	O
line	O
contains	O
the	O
four	O
spectral	O
bands	O
of	O
each	O
of	O
the	O
9	O
pixels	O
in	O
the	O
3	O
3	O
neighbourhood	O
and	O
the	O
class	O
of	O
the	O
central	O
pixel	O
which	O
was	O
one	O
of	O
:	O
red	O
soil	O
,	O
cotton	O
crop	O
,	O
grey	O
soil	O
,	O
damp	O
grey	O
soil	O
,	O
soil	O
with	O
vegetation	O
stubble	O
,	O
very	O
damp	O
grey	O
soil	O
.	O
the	O
“	O
mixed-pixels	O
”	O
,	O
of	O
which	O
there	O
were	O
8.6	O
%	O
,	O
were	O
removed	O
for	O
our	O
purposes	O
,	O
so	O
that	O
there	O
are	O
only	O
six	O
classes	O
in	O
this	O
dataset	O
.	O
the	O
examples	O
were	O
randomised	O
and	O
certain	O
lines	O
were	O
deleted	O
so	O
that	O
simple	O
recon-	O
struction	O
of	O
the	O
original	O
image	O
was	O
not	O
possible	O
.	O
the	O
data	O
were	O
divided	O
into	O
a	O
train	O
set	O
and	O
û	O
{	O
144	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
spectral	O
band	O
1	O
spectral	O
band	O
2	O
spectral	O
band	O
3	O
spectral	O
band	O
4	O
land	O
use	O
(	O
actual	O
)	O
land	O
use	O
(	O
predicted	O
)	O
fig	O
.	O
9.3	O
:	O
satellite	B
image	I
dataset	O
.	O
spectral	O
band	O
intensities	O
as	O
seen	O
from	O
a	O
satellite	O
for	O
a	O
small	O
(	O
8.2*6.6	O
km	O
)	O
region	O
of	O
australia	O
.	O
also	O
given	O
are	O
the	O
actual	O
land	O
use	O
as	O
determined	O
by	O
on-site	O
visit	O
and	O
the	O
estimated	O
classes	O
as	O
given	O
by	O
linear	O
discriminants	O
.	O
a	O
test	O
set	O
with	O
4435	O
examples	O
in	O
the	O
train	O
set	O
and	O
2000	O
in	O
the	O
test	O
set	O
and	O
the	O
error	O
rates	O
are	O
given	O
in	O
table	O
9.9.	O
in	O
the	O
satellite	B
image	I
dataset	O
k-nn	O
performs	O
best	O
.	O
not	O
surprisingly	O
,	O
radial	O
basis	O
func-	O
tions	O
,	O
lvq	O
and	O
“	O
alloc80	O
”	O
also	O
do	O
fairly	O
well	O
as	O
these	O
three	O
algorithms	O
are	O
closely	O
related	O
.	O
[	O
in	O
fact	O
,	O
alloc80	O
failed	O
on	O
this	O
dataset	O
,	O
so	O
an	O
equivalent	O
method	O
,	O
using	O
an	O
asymptotically	O
chosen	O
bandwidth	O
,	O
was	O
used	O
.	O
]	O
their	O
success	O
suggests	O
that	O
all	O
the	O
attributes	O
are	O
equally	O
scaled	O
and	O
equally	O
important	O
.	O
there	O
appears	O
to	O
be	O
little	O
to	O
choose	O
between	O
any	O
of	O
the	O
other	O
algorithms	O
,	O
except	O
that	O
naive	O
bayes	O
does	O
badly	O
(	O
and	O
its	O
close	O
relative	O
castle	O
also	O
does	O
relatively	O
badly	O
)	O
.	O
the	O
decision	O
tree	O
algorithms	O
perform	O
at	O
about	O
the	O
same	O
level	O
,	O
with	O
cart	O
giving	O
the	O
	O
used	O
trees	O
with	O
156	O
and	O
116	O
nodes	O
,	O
respectively	O
,	O
which	O
suggests	O
more	O
pruning	B
is	O
desired	O
for	O
these	O
algorithms	O
.	O
best	O
result	O
using	O
66	O
nodes	O
.	O
cal5	O
andû	O
this	O
dataset	O
has	O
the	O
highest	O
correlation	O
between	O
attributes	O
(	O
corr.abs	O
=	O
0.5977	O
)	O
.	O
this	O
may	O
partly	O
explain	O
the	O
failure	O
of	O
naive	O
bayes	O
(	O
assumes	O
attributes	O
are	O
conditionally	O
independent	O
)	O
,	O
and	O
castle	O
(	O
confused	O
if	O
several	O
attributes	O
contain	O
equal	O
amounts	O
of	O
information	O
)	O
.	O
note	O
that	O
only	O
three	O
canonical	O
discriminants	O
are	O
sufﬁcient	O
to	O
separate	O
all	O
six	O
class	O
means	O
(	O
fract3	O
{	O
sec	O
.	O
9.3	O
]	O
image	O
data	O
:	O
segmentation	O
145	O
table	O
9.9	O
:	O
results	O
for	O
the	O
satellite	B
image	I
dataset	O
(	O
6	O
classes	O
,	O
36	O
attributes	O
,	O
(	O
train	O
,	O
test	O
)	O
=	O
(	O
4435	O
,	O
2000	O
)	O
observations	O
)	O
.	O
algorithm	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
baytree	O
naivebay	O
cn2	O
c4.5	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
cascade	O
default	O
maximum	O
storage	O
254	O
364	O
1205	O
244	O
244	O
180	O
*	O
253	O
819	O
1800	O
*	O
161	O
133	O
682	O
1150	O
fd	O
412	O
*	O
293	O
469	O
195	O
227	O
1210	O
*	O
time	O
(	O
sec	O
.	O
)	O
train	O
67.8	O
157.0	O
4414.1	O
27376.2	O
63840.2	O
2104.9	O
75.0	O
329.9	O
2109.2	O
226.0	O
8244.0	O
247.8	O
75.1	O
1664.0	O
434.0	O
fd	O
764.0	O
12627.0	O
764.3	O
72494.5	O
564.2	O
1273.2	O
7180.0	O
*	O
test	O
11.9	O
52.9	O
41.2	O
10.8	O
28756.5	O
944.1	O
80.0	O
14.2	O
9.2	O
53.0	O
17403.0	O
10.2	O
16.5	O
35.8	O
1.0	O
fd	O
7.2	O
129.0	O
110.7	O
52.6	O
74.1	O
44.2	O
1.0	O
*	O
*	O
train	O
0.149	O
0.106	O
0.119	O
0.123	O
0.036	O
0.089	O
0.186	O
0.079	O
0.023	O
0.067	O
error	O
rate	O
test	O
0.171	O
0.155	O
0.163	O
0.159	O
0.132	O
0.094	O
0.194	O
0.138	O
0.138	O
0.150	O
0.157	O
0.147	O
0.287	O
0.150	O
0.150	O
fd	O
0.151	O
0.179	O
0.111	O
0.139	O
0.121	O
0.105	O
0.163	O
0.769	O
0.020	O
0.308	O
0.010	O
0.040	O
fd	O
0.125	O
0.101	O
0.051	O
0.112	O
0.111	O
0.048	O
0.112	O
0.758	O
rank	O
19	O
14	O
17	O
16	O
5	O
1	O
21	O
6	O
6	O
10	O
15	O
9	O
22	O
10	O
10	O
13	O
20	O
3	O
8	O
4	O
2	O
17	O
23	O
=	O
0.9691	O
)	O
.	O
this	O
may	O
be	O
interpreted	O
as	O
evidence	O
of	O
seriation	O
,	O
with	O
the	O
three	O
classes	O
“	O
grey	O
soil	O
”	O
,	O
“	O
damp	O
grey	O
soil	O
”	O
and	O
“	O
very	O
damp	O
grey	O
soil	O
”	O
forming	O
a	O
continuum	O
.	O
equally	O
,	O
this	O
result	O
can	O
be	O
interpreted	O
as	O
indicating	O
that	O
the	O
original	O
four	O
attributes	O
may	O
be	O
successfully	O
reduced	O
to	O
three	O
with	O
no	O
loss	O
of	O
information	O
.	O
here	O
“	O
information	O
”	O
should	O
be	O
interpreted	O
as	O
mean	O
square	O
distance	O
between	O
classes	O
,	O
or	O
equivalently	O
,	O
as	O
the	O
entropy	O
of	O
a	O
normal	O
distribution	O
.	O
the	O
examples	O
were	O
created	O
using	O
a	O
3	O
3	O
neighbourhood	O
so	O
it	O
is	O
no	O
surprise	O
that	O
there	O
is	O
a	O
very	O
large	O
correlation	O
amongst	O
the	O
36	O
variables	O
.	O
the	O
results	O
from	O
castle	O
suggest	O
that	O
only	O
three	O
of	O
the	O
variables	O
for	O
the	O
centre	O
pixel	O
are	O
necessary	O
to	O
classify	O
the	O
observation	O
.	O
however	O
,	O
other	O
algorithms	O
found	O
a	O
signiﬁcant	O
improvement	O
when	O
information	O
from	O
the	O
neighbouring	O
pixels	O
was	O
used	O
.	O
image	B
segmentation	I
(	O
segm	O
)	O
9.3.7	O
the	O
instances	O
were	O
drawn	O
randomly	O
from	O
a	O
database	O
of	O
7	O
outdoor	O
colour	O
images	O
.	O
these	O
were	O
hand	O
segmented	O
to	O
create	O
a	O
classiﬁcation	B
for	O
every	O
pixel	O
as	O
one	O
of	O
brickface	O
,	O
sky	O
,	O
region	O
,	O
for	O
example	B
summary	O
measures	O
of	O
contrast	O
in	O
the	O
vertical	O
and	O
horizontal	O
directions	O
.	O
foliage	O
,	O
cement	O
,	O
window	O
,	O
path	O
,	O
grass	O
.	O
there	O
were	O
19	O
attributes	O
appropriate	O
for	O
each	O
3	O
3	O
û	O
{	O
	O
146	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
table	O
9.10	O
:	O
results	O
for	O
the	O
image	B
segmentation	I
dataset	O
(	O
7	O
classes	O
,	O
11	O
attributes	O
,	O
2310	O
observations	O
,	O
10-fold	O
cross-validation	O
)	O
.	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
max	O
.	O
algorithm	O
storage	O
365	O
395	O
535	O
144	O
124	O
171	O
142	O
175	O
744	O
*	O
7830	O
676	O
564	O
174	O
57	O
139	O
373	O
233	O
91	O
148	O
381	O
123	O
*	O
baytree	O
naivebay	O
cn2	O
c4.5	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
default	O
time	O
(	O
sec	O
.	O
)	O
train	O
73.6	O
49.7	O
301.8	O
13883.9	O
15274.3	O
5.0	O
465.4	O
79.0	O
1410.5	O
386.0	O
18173.0	O
677.3	O
516.4	O
114.2	O
142.0	O
545.7	O
247.1	O
11333.2	O
503.0	O
88467.2	O
65.0	O
368.2	O
*	O
test	O
6.6	O
15.5	O
8.4	O
0.5	O
*	O
28.0	O
38.3	O
2.3	O
1325.1	O
2.0	O
479.0	O
26.9	O
29.0	O
2.7	O
1.3	O
19.9	O
13.7	O
8.5	O
25.0	O
0.4	O
11.0	O
6.4	O
*	O
error	O
rate	O
test	O
0.116	O
0.157	O
0.109	O
0.052	O
0.030	O
0.077	O
0.112	O
0.040	O
0.045	O
0.034	O
0.031	O
0.033	O
0.265	O
0.043	O
0.040	O
0.455	O
0.062	O
0.067	O
0.039	O
0.054	O
0.069	O
0.046	O
0.760	O
train	O
0.112	O
0.155	O
0.098	O
0.039	O
0.033	O
0.000	O
0.108	O
0.005	O
0.012	O
0.000	O
0.000	O
0.000	O
0.260	O
0.003	O
0.013	O
0.445	O
0.042	O
0.046	O
0.021	O
0.028	O
0.047	O
0.019	O
0.760	O
rank	O
19	O
20	O
17	O
11	O
1	O
16	O
18	O
6	O
9	O
4	O
2	O
3	O
21	O
8	O
6	O
22	O
13	O
14	O
5	O
12	O
15	O
10	O
23	O
average	O
error	O
rates	O
were	O
obtained	O
via	O
10-fold	O
cross-validation	O
,	O
and	O
are	O
given	O
in	O
table	O
9.10	O
.	O
	O
did	O
very	O
well	O
here	O
and	O
used	O
an	O
average	O
of	O
52	O
nodes	O
in	O
its	O
decision	O
trees	O
.	O
it	O
is	O
interesting	O
here	O
that	O
alloc80	O
does	O
so	O
much	O
better	O
than	O
k-nn	O
.	O
the	O
reason	O
for	O
this	O
is	O
that	O
alloc80	O
has	O
a	O
variable	O
selection	O
option	O
which	O
was	O
initially	O
run	O
on	O
the	O
data	O
,	O
and	O
only	O
5	O
of	O
the	O
original	O
attributes	O
were	O
ﬁnally	O
used	O
.	O
when	O
14	O
variables	O
were	O
used	O
the	O
error	O
rate	O
increased	O
to	O
21	O
%	O
.	O
indeed	O
a	O
similar	O
attribute	O
selection	O
procedure	O
increased	O
the	O
performance	O
of	O
k-nn	O
to	O
a	O
very	O
similar	O
error	O
rate	O
.	O
this	O
discrepancy	O
raises	O
the	O
whole	O
issue	O
of	O
pre-	O
processing	O
the	O
data	O
before	O
algorithms	O
are	O
run	O
,	O
and	O
the	O
substantial	O
difference	O
this	O
can	O
make	O
.	O
it	O
is	O
clear	O
that	O
there	O
will	O
still	O
be	O
a	O
place	O
for	O
intelligent	O
analysis	O
alongside	O
any	O
black-box	O
techniques	O
for	O
quite	O
some	O
time	O
!	O
9.3.8	O
cut	B
this	O
dataset	O
was	O
supplied	O
by	O
a	O
statlog	O
partner	O
for	O
whom	O
it	O
is	O
commercially	O
conﬁdential	O
.	O
the	O
dataset	O
was	O
constructed	O
during	O
an	O
investigation	O
into	O
the	O
problem	O
of	O
segmenting	O
indi-	O
vidual	O
characters	O
from	O
joined	O
written	O
text	O
.	O
figure	O
9.4	O
shows	O
an	O
example	B
of	O
the	O
word	O
“	O
eins	O
”	O
(	O
german	O
for	O
one	O
)	O
.	O
each	O
example	B
consists	O
of	O
a	O
number	O
of	O
measurements	O
made	O
on	O
the	O
text	O
relative	O
to	O
a	O
potential	O
cut	B
point	O
along	O
with	O
a	O
decision	O
on	O
whether	O
to	O
cut	B
the	O
text	O
at	O
that	O
û	O
{	O
	O
û	O
{	O
sec	O
.	O
9.3	O
]	O
image	O
data	O
:	O
segmentation	O
147	O
fig	O
.	O
9.4	O
:	O
the	O
german	O
word	O
“	O
eins	O
”	O
with	O
an	O
indication	O
of	O
where	O
it	O
should	O
be	O
cut	B
to	O
separate	O
the	O
individual	O
letters	O
.	O
point	O
or	O
not	O
.	O
as	O
supplied	O
,	O
the	O
dataset	O
contained	O
examples	O
with	O
50	O
real	O
valued	O
attributes	O
.	O
in	O
an	O
attempt	O
to	O
assess	O
the	O
performance	O
of	O
algorithms	O
relative	O
to	O
the	O
dimensionality	O
of	O
the	O
problem	O
,	O
a	O
second	O
dataset	O
was	O
constructed	O
from	O
the	O
original	O
using	O
the	O
“	O
best	O
”	O
20	O
attributes	O
selected	O
by	O
stepwise	O
regression	O
on	O
the	O
whole	O
dataset	O
.	O
this	O
was	O
the	O
only	O
processing	O
carried	O
out	O
on	O
this	O
dataset	O
.	O
the	O
original	O
and	O
reduced	O
datasets	O
were	O
tested	O
.	O
in	O
both	O
cases	O
training	O
sets	O
of	O
11220	O
examples	O
and	O
test	O
sets	O
of	O
7480	O
were	O
used	O
in	O
a	O
single	O
train-and-test	O
procedure	O
to	O
assess	O
accuracy	O
.	O
although	O
individual	O
results	O
differ	O
between	O
the	O
datasets	O
,	O
the	O
ranking	O
of	O
methods	O
is	O
broadly	O
the	O
same	O
and	O
so	O
we	O
shall	O
consider	O
all	O
the	O
results	O
together	O
.	O
the	O
default	O
rule	O
in	O
both	O
cases	O
would	O
give	O
an	O
error	O
rate	O
of	O
around	O
6	O
%	O
but	O
since	O
kohonen	O
,	O
the	O
only	O
unsupervised	O
method	O
in	O
the	O
project	O
,	O
achieves	O
an	O
error	O
rate	O
of	O
5	O
%	O
for	O
both	O
datasets	O
it	O
seems	O
reasonable	O
to	O
choose	O
this	O
value	O
as	O
our	O
performance	O
threshold	O
.	O
this	O
is	O
a	O
dataset	O
on	O
which	O
k–nearest	O
neighbour	O
might	O
be	O
expected	O
to	O
do	O
well	O
;	O
all	O
attributes	O
are	O
continuous	O
with	O
little	O
correlation	O
,	O
and	O
this	O
proves	O
to	O
be	O
the	O
case	O
.	O
indeed	O
,	O
with	O
a	O
variable	O
selection	O
option	O
k-nn	O
obtained	O
an	O
error	O
rate	O
of	O
only	O
2.5	O
%	O
.	O
conversely	O
,	O
the	O
fact	O
that	O
k-nn	O
does	O
well	O
indicates	O
that	O
many	O
variables	O
contribute	O
to	O
the	O
classiﬁcation	B
.	O
alloc80	O
approaches	O
k-nn	O
performance	O
by	O
undersmoothing	O
leading	O
to	O
overﬁtting	O
on	O
the	O
training	O
set	O
.	O
while	O
this	O
may	O
prove	O
to	O
be	O
an	O
effective	O
strategy	O
with	O
large	O
and	O
representative	O
training	O
sets	O
,	O
it	O
is	O
not	O
recommended	O
in	O
general	O
.	O
quadisc	O
,	O
castle	O
and	O
naive	O
bayes	O
perform	O
poorly	O
on	O
both	O
datasets	O
because	O
,	O
in	O
each	O
case	O
,	O
assumptions	O
underlying	O
the	O
method	O
do	O
not	O
match	O
the	O
data	O
.	O
quadisc	O
assumes	O
multi–variate	O
normality	O
and	O
unequal	O
covariance	O
matrices	O
and	O
neither	O
of	O
these	O
assumptions	O
is	O
supported	O
by	O
the	O
data	O
measures	O
.	O
castle	O
achieves	O
default	O
perfor-	O
mance	O
using	O
only	O
one	O
variable	O
,	O
in	O
line	O
with	O
the	O
assumption	O
implicit	O
in	O
the	O
method	O
that	O
only	O
a	O
small	O
number	O
of	O
variables	O
will	O
determine	O
the	O
class	O
.	O
naive	O
bayes	O
assumes	O
conditional	O
independence	O
amongst	O
the	O
attributes	O
and	O
this	O
is	O
unlikely	O
to	O
hold	O
for	O
a	O
dataset	O
of	O
this	O
type	O
.	O
machine	O
learning	O
algorithms	O
generally	O
perform	O
well	O
although	O
with	O
wide	O
variation	O
in	O
tree	O
sizes	O
.	O
baytree	O
and	O
indcart	O
achieve	O
low	O
error	O
rates	O
at	O
the	O
expense	O
of	O
building	O
trees	O
containing	O
more	O
than	O
3000	O
nodes	O
.	O
c4.5	O
performs	O
almost	O
as	O
well	O
,	O
though	O
building	O
a	O
tree	O
containing	O
159	O
terminal	O
nodes	O
.	O
cal5	O
produces	O
a	O
very	O
parsimonious	O
tree	O
,	O
containing	O
only	O
26	O
nodes	O
for	O
the	O
cut20	O
dataset	O
,	O
which	O
is	O
very	O
easy	O
to	O
understand.û	O
	O
and	O
newid	O
build	O
trees	O
{	O
148	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
table	O
9.11	O
:	O
comparative	O
results	O
for	O
the	O
cut20	O
dataset	O
(	O
2	O
classes	O
,	O
20	O
attributes	O
,	O
(	O
train	O
,	O
test	O
)	O
=	O
(	O
11	O
220	O
,	O
7480	O
)	O
observations	O
)	O
.	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
max	O
.	O
algorithm	O
storage	O
71	O
75	O
1547	O
743	O
302	O
190	O
175	O
fd	O
1884	O
1166	O
915	O
1676	O
1352	O
9740	O
2436	O
630	O
188	O
1046	O
379	O
144	O
901	O
291	O
*	O
baytree	O
naivebay	O
cn2	O
c4.5	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
default	O
time	O
(	O
sec	O
.	O
)	O
train	O
115.5	O
394.8	O
587.0	O
21100.5	O
32552.2	O
54810.7	O
1006.0	O
fd	O
*	O
1445.0	O
917.0	O
145.3	O
83.6	O
5390.0	O
293.0	O
11011.0	O
455.5	O
*	O
506.0	O
88532.0	O
6041.0	O
1379.0	O
*	O
test	O
22.7	O
214.2	O
101.2	O
21.8	O
*	O
6052.0	O
368.5	O
fd	O
*	O
3.0	O
48.0	O
25.9	O
27.6	O
470.0	O
28.0	O
50.9	O
23.4	O
*	O
36.1	O
7.0	O
400.0	O
86.9	O
*	O
error	O
rate	O
test	O
0.050	O
0.088	O
0.046	O
0.047	O
0.037	O
0.036	O
0.061	O
fd	O
0.040	O
0.039	O
0.063	O
0.034	O
0.077	O
0.042	O
0.036	O
0.082	O
0.045	O
0.050	O
0.045	O
0.043	O
0.044	O
0.041	O
0.061	O
train	O
0.052	O
0.090	O
0.046	O
0.047	O
0.033	O
0.031	O
0.060	O
fd	O
0.002	O
0.000	O
0.000	O
0.002	O
0.074	O
0.000	O
0.010	O
0.083	O
0.043	O
0.046	O
0.043	O
0.037	O
0.042	O
0.029	O
0.059	O
rank	O
15	O
22	O
13	O
14	O
4	O
2	O
17	O
6	O
5	O
19	O
1	O
20	O
8	O
2	O
21	O
11	O
15	O
11	O
9	O
10	O
7	O
17	O
with	O
38	O
and	O
339	O
nodes	O
,	O
respectively	O
.	O
itrule	O
,	O
like	O
castle	O
,	O
can	O
not	O
deal	O
with	O
continuous	O
attributes	O
directly	O
and	O
also	O
discretises	O
such	O
variables	O
before	O
processing	O
.	O
the	O
major	O
reason	O
for	O
poor	O
performance	O
,	O
though	O
,	O
is	O
that	O
tests	O
were	O
restricted	O
to	O
conjunctions	O
of	O
up	O
to	O
two	O
attributes	O
.	O
cn2	O
,	O
which	O
tested	O
conjunctions	O
of	O
up	O
to	O
5	O
attributes	O
,	O
achieved	O
a	O
much	O
better	O
error	O
rate	O
.	O
subsample	O
.	O
	O
could	O
not	O
handle	O
the	O
full	O
dataset	O
and	O
the	O
results	O
reported	O
are	O
for	O
a	O
10	O
%	O
it	O
is	O
interesting	O
that	O
almost	O
all	O
algorithms	O
achieve	O
a	O
better	O
result	O
on	O
cut50	O
than	O
cut20	O
.	O
this	O
suggests	O
that	O
the	O
attributes	O
excluded	O
from	O
the	O
reduced	O
dataset	O
contain	O
signiﬁcant	O
discriminatory	O
power	O
.	O
cal5	O
achieves	O
its	O
better	O
performance	O
by	O
building	O
a	O
tree	O
ﬁve	O
times	O
larger	O
than	O
that	O
for	O
cut20	O
.	O
newid	O
andû	O
and	O
28	O
nodes	O
)	O
and	O
classify	O
more	O
accurately	O
with	O
them	O
.	O
c4.5	O
uses	O
a	O
tree	O
with	O
142	O
nodes	O
with	O
a	O
slight	O
improvement	O
in	O
accuracy	O
.	O
similarly	O
cn2	O
discovers	O
a	O
smaller	O
set	O
of	O
rules	O
for	O
cut50	O
which	O
deliver	O
improved	O
performance	O
.	O
this	O
general	O
improvement	O
in	O
performance	O
underlines	O
the	O
observation	O
that	O
what	O
is	O
“	O
best	O
”	O
or	O
“	O
optimal	O
”	O
in	O
linear	O
regression	O
terms	O
may	O
not	O
be	O
“	O
best	O
”	O
for	O
other	O
algorithms	O
.	O
	O
both	O
build	O
signiﬁcantly	O
smaller	O
trees	O
(	O
196	O
û	O
{	O
	O
û	O
{	O
{	O
sec	O
.	O
9.4	O
]	O
cost	O
datasets	O
149	O
table	O
9.12	O
:	O
results	O
for	O
the	O
cut50	O
dataset	O
(	O
2	O
classes	O
,	O
50	O
attributes	O
,	O
(	O
train	O
,	O
test	O
)	O
=	O
(	O
11	O
220	O
,	O
7480	O
)	O
observations	O
)	O
.	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
max	O
.	O
algorithm	O
storage	O
73	O
77	O
1579	O
779	O
574	O
356	O
765	O
fd	O
3172	O
1166	O
1812	O
2964	O
2680	O
*	O
*	O
642	O
508	O
*	O
884	O
146	O
649	O
476	O
*	O
baytree	O
naivebay	O
cn2	O
c4.5	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
default	O
time	O
(	O
sec	O
.	O
)	O
train	O
449.2	O
2230.7	O
1990.4	O
63182.0	O
32552.2	O
62553.6	O
7777.6	O
fd	O
2301.4	O
1565.0	O
1850.0	O
324.0	O
219.4	O
28600.0	O
711.0	O
61287.5	O
1131.9	O
*	O
1242.5	O
18448.0	O
6393.0	O
2991.2	O
*	O
test	O
52.5	O
1244.2	O
227.0	O
50.4	O
*	O
6924.0	O
1094.8	O
fd	O
2265.4	O
2.0	O
47.0	O
65.4	O
69.9	O
501.0	O
31.0	O
*	O
58.7	O
*	O
96.9	O
12.0	O
1024.0	O
205.0	O
*	O
train	O
0.052	O
0.092	O
0.038	O
0.035	O
0.030	O
0.025	O
0.060	O
fd	O
0.004	O
0.000	O
0.000	O
0.001	O
0.106	O
0.000	O
0.008	O
error	O
rate	O
test	O
0.050	O
0.097	O
0.037	O
0.039	O
0.034	O
0.027	O
0.061	O
fd	O
0.037	O
0.038	O
0.054	O
0.035	O
0.112	O
0.030	O
0.035	O
0.084	O
0.037	O
0.050	O
0.036	O
0.041	O
0.038	O
0.040	O
0.061	O
*	O
0.030	O
0.046	O
0.031	O
0.041	O
0.036	O
0.024	O
0.059	O
rank	O
15	O
21	O
7	O
12	O
3	O
1	O
18	O
7	O
10	O
18	O
4	O
22	O
2	O
4	O
20	O
7	O
15	O
6	O
14	O
10	O
13	O
17	O
9.4	O
datasets	O
with	O
costs	O
the	O
following	O
three	O
datasets	O
were	O
all	O
tackled	O
using	O
cross-validation	O
.	O
the	O
“	O
error	O
rates	O
”	O
that	O
have	O
been	O
used	O
as	O
a	O
measure	B
of	O
performance	O
are	O
now	O
replaced	O
by	O
average	O
costs	O
per	O
observation	O
(	O
averaged	O
over	O
all	O
cycles	O
in	O
cross-validation	O
)	O
.	O
the	O
average	O
cost	O
is	O
obtained	O
for	O
all	O
algorithms	O
by	O
multiplying	O
the	O
confusion	O
matrix	O
by	O
the	O
cost	O
matrix	O
,	O
summing	O
the	O
entries	O
,	O
and	O
dividing	O
by	O
the	O
number	O
of	O
observations	O
.	O
in	O
the	O
case	O
of	O
a	O
cost	O
matrix	O
in	O
which	O
all	O
errors	O
have	O
unit	O
cost	O
–	O
normally	O
referred	O
to	O
as	O
“	O
no	O
cost	O
matrix	O
”	O
–	O
this	O
measure	B
of	O
average	O
cost	O
is	O
the	O
same	O
as	O
the	O
error	O
rates	O
quoted	O
previously	O
.	O
note	O
that	O
some	O
algorithms	O
did	O
not	O
implement	O
the	O
cost	O
matrix	O
,	O
although	O
in	O
principle	O
this	O
would	O
be	O
straightforward	O
.	O
however	O
,	O
we	O
still	O
include	O
all	O
of	O
the	O
algorithms	O
in	O
the	O
tables	O
,	O
partly	O
for	O
completeness	O
but	O
primarily	O
to	O
show	O
the	O
effect	O
of	O
ignoring	O
the	O
cost	O
matrix	O
.	O
in	O
general	O
,	O
those	O
algorithms	O
which	O
do	O
worse	O
than	O
the	O
default	O
rule	O
are	O
those	O
which	O
do	O
not	O
incorporate	O
costs	O
into	O
the	O
decision	O
making	O
process	O
.	O
9.4.1	O
head	B
injury	I
(	O
head	O
)	O
the	O
data	O
set	O
is	O
a	O
series	O
of	O
1000	O
patients	O
with	O
severe	O
head	B
injury	I
collected	O
prospectively	O
by	O
neurosurgeons	O
between	O
1968	O
and	O
1976.	O
this	O
head	B
injury	I
study	O
was	O
initiated	O
in	O
the	O
institute	O
û	O
{	O
	O
150	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
of	O
neurological	O
sciences	O
,	O
glasgow	O
.	O
after	O
4	O
years	O
2	O
netherlands	O
centres	O
(	O
rotterdam	O
and	O
groningen	O
)	O
joined	O
the	O
study	O
,	O
and	O
late	O
data	O
came	O
also	O
from	O
los	O
angeles	O
.	O
the	O
details	O
of	O
the	O
data	O
collection	O
are	O
given	O
in	O
jennet	O
et	O
al	O
.	O
(	O
1979	O
)	O
.	O
the	O
original	O
purpose	O
of	O
the	O
head	B
injury	I
study	O
was	O
to	O
investigate	O
the	O
feasibility	O
of	O
pre-	O
dicting	O
the	O
degree	O
of	O
recovery	O
which	O
individual	O
patients	O
would	O
attain	O
,	O
using	O
data	O
collected	O
shortly	O
after	O
injury	O
.	O
severely	O
head	O
injured	O
patients	O
require	O
intensive	O
and	O
expensive	O
treat-	O
ment	O
;	O
even	O
with	O
such	O
care	O
almost	O
half	O
of	O
them	O
die	O
and	O
some	O
survivors	O
remain	O
seriously	O
disabled	O
for	O
life	O
.	O
clinicians	O
are	O
concerned	O
to	O
recognise	O
which	O
patients	O
have	O
potential	O
for	O
recovery	O
,	O
so	O
as	O
to	O
concentrate	O
their	O
endeavours	O
on	O
them	O
.	O
outcome	O
was	O
categorised	O
accord-	O
ing	O
to	O
the	O
glasgow	O
outcome	O
scale	O
,	O
but	O
the	O
ﬁve	O
categories	O
described	O
therein	O
were	O
reduced	O
to	O
three	O
for	O
the	O
purpose	O
of	O
prediction	O
.	O
these	O
were	O
:	O
d/v	O
dead	O
or	O
vegetative	O
;	O
sev	O
severe	O
disability	O
;	O
m/g	O
moderate	O
disability	O
or	O
good	O
recovery	O
.	O
table	O
9.13	O
gives	O
the	O
different	O
cost	O
of	O
various	O
possible	O
misclassiﬁcations	O
.	O
table	O
9.13	O
:	O
misclassiﬁcation	O
costs	O
for	O
the	O
head	B
injury	I
dataset	I
.	O
the	O
column	O
represents	O
the	O
predicted	O
class	O
,	O
and	O
the	O
row	O
the	O
true	O
class	O
.	O
d/v	O
0	O
10	O
750	O
sev	O
m/g	O
75	O
10	O
90	O
0	O
100	O
0	O
d/v	O
sev	O
m/g	O
the	O
dataset	O
had	O
a	O
very	O
large	O
number	O
of	O
missing	O
values	O
for	O
patients	O
(	O
about	O
40	O
%	O
)	O
and	O
these	O
were	O
replaced	O
with	O
the	O
median	O
value	O
for	O
the	O
appropriate	O
class	O
.	O
this	O
makes	O
our	O
version	O
of	O
the	O
data	O
considerably	O
easier	O
for	O
classiﬁcation	B
than	O
the	O
original	O
data	O
,	O
and	O
has	O
the	O
merit	O
that	O
all	O
procedures	O
can	O
be	O
applied	O
to	O
the	O
same	O
dataset	O
,	O
but	O
has	O
the	O
disadvantage	O
that	O
the	O
resulting	O
rules	O
are	O
unrealistic	O
in	O
that	O
this	O
replacement	O
strategy	O
is	O
not	O
possible	O
for	O
real	O
data	O
of	O
unknown	O
class	O
.	O
nine	O
fold	O
cross-validation	O
was	O
used	O
to	O
estimate	O
the	O
average	O
misclassiﬁcation	O
cost	O
.	O
the	O
predictive	O
variables	O
are	O
age	O
and	O
various	O
indicators	O
of	O
the	O
brain	O
damage	O
,	O
as	O
reﬂected	O
in	O
brain	O
dysfunction	O
.	O
these	O
are	O
listed	O
below	O
.	O
indicators	O
of	O
brain	O
dysfunction	O
can	O
vary	O
considerably	O
during	O
the	O
few	O
days	O
after	O
injury	O
.	O
measurements	O
were	O
therefore	O
taken	O
frequently	O
,	O
and	O
for	O
each	O
indicant	O
the	O
best	O
and	O
worst	O
states	O
during	O
each	O
of	O
a	O
number	O
of	O
successive	O
time	O
periods	O
were	O
recorded	O
.	O
the	O
data	O
supplied	O
were	O
based	O
on	O
the	O
best	O
state	O
during	O
the	O
ﬁrst	O
24	O
hours	O
after	O
the	O
onset	O
of	O
coma	O
.	O
the	O
emv	O
score	O
in	O
the	O
table	O
is	O
known	O
in	O
the	O
medical	O
literature	O
as	O
the	O
glasgow	O
coma	O
scale	O
.	O
$	O
~	O
°	O
$	O
~eddi~	O
ô°ç	O
~	O
´	O
°2e	O
the	O
sum	O
of	O
e	O
,	O
m	O
and	O
v	O
scores	O
,	O
i.e	O
.	O
emv	O
score	O
,	O
i.e	O
.	O
age	O
,	O
grouped	O
into	O
decades°	O
}	O
	O
eye	O
opening	O
in	O
response	O
to	O
stimulation	O
(	O
e	O
)	O
motor	O
response	O
of	O
best	O
limb	O
in	O
response	O
to	O
stimulation	O
(	O
m	O
)	O
verbal	O
response	O
to	O
stimulation	O
(	O
v	O
)	O
motor	O
response	O
pattern	O
.	O
an	O
overall	O
summary	O
of	O
the	O
motor	O
responses	O
in	O
all	O
four	O
limbs	O
change	O
in	O
neurological	O
function	O
over	O
the	O
ﬁrst	O
24	O
hours	O
f	O
f	O
ô	O
sec	O
.	O
9.4	O
]	O
cost	O
datasets	O
151	O
eye	O
indicant	O
.	O
a	O
summary	O
of	O
sem	O
,	O
ocs	O
and	O
ovs	O
,	O
i.e	O
.	O
spontaneous	O
eye	O
movements	O
(	O
sem	O
)	O
oculocephalics	O
(	O
ocs	O
)	O
oculovestibulars	O
(	O
ovs	O
)	O
pupil	O
reaction	O
to	O
light	O
table	O
9.14	O
:	O
results	O
for	O
the	O
head	B
injury	I
dataset	I
(	O
3	O
classes	O
,	O
6	O
attributes	O
,	O
900	O
observations	O
,	O
9-fold	O
cross-validation	O
)	O
.	O
algorithms	O
in	O
italics	O
have	O
not	O
incorporated	O
costs	O
.	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
max	O
.	O
algorithm	O
storage	O
200	O
642	O
1981	O
81	O
191	O
144	O
82	O
154	O
88	O
38	O
400	O
73	O
52	O
149	O
339	O
97	O
51	O
90	O
41	O
518	O
150	O
82	O
271	O
*	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
cascade	O
default	O
baytree	O
naivebay	O
cn2	O
ac	O
c4.5¢	O
time	O
(	O
sec	O
.	O
)	O
train	O
12.6	O
36.6	O
736.4	O
572.2	O
1.4	O
9.0	O
2.6	O
17.6	O
5.5	O
9.0	O
624.0	O
2.5	O
2.9	O
24.3	O
5.0	O
6.5	O
3.0	O
1772.0	O
10.0	O
312.5	O
17.4	O
190.7	O
181.0	O
*	O
test	O
3.1	O
32.0	O
7.3	O
3.5	O
38.3	O
11.2	O
2.0	O
0.8	O
0.4	O
3.0	O
28.0	O
0.3	O
0.3	O
3.0	O
0.2	O
*	O
0.2	O
3.0	O
1.0	O
31.9	O
5.1	O
1.2	O
1.0	O
*	O
average	O
costs	O
test	O
train	O
19.89	O
19.76	O
20.06	O
17.83	O
16.60	O
17.96	O
21.81	O
13.59	O
31.90	O
18.9	O
35.30	O
9.20	O
18.87	O
20.87	O
20.38	O
19.84	O
25.52	O
25.76	O
53.64	O
18.91	O
17.88	O
56.87	O
22.69	O
10.94	O
23.95	O
23.68	O
53.55	O
14.36	O
59.82	O
82.60	O
37.61	O
33.26	O
70.70	O
26.52	O
21.53	O
63.10	O
46.58	O
19.46	O
44.10	O
32.54	O
35.6	O
25.31	O
18.23	O
53.37	O
29.30	O
15.25	O
44.10	O
*	O
rank	O
3	O
4	O
1	O
8	O
13	O
15	O
6	O
5	O
11	O
20	O
21	O
9	O
10	O
19	O
24	O
16	O
14	O
23	O
12	O
7	O
22	O
18	O
2	O
17	O
smart	O
and	O
dipol92	O
are	O
the	O
only	O
algorithms	O
that	O
as	O
standard	O
can	O
utilise	O
costs	O
directly	O
in	O
the	O
training	O
phase	O
(	O
we	O
used	O
in	O
our	O
results	O
a	O
modiﬁed	O
version	O
of	O
backprop	O
that	O
could	O
utilise	O
costs	O
,	O
but	O
this	O
is	O
very	O
experimental	O
)	O
.	O
however	O
,	O
although	O
these	O
two	O
algorithms	O
do	O
reasonably	O
well	O
,	O
they	O
are	O
not	O
the	O
best	O
.	O
logistic	O
regression	O
does	O
very	O
well	O
and	O
so	O
do	O
discrim	O
and	O
quadisc	O
cart	O
,	O
indcart	O
,	O
bayes	O
tree	O
and	O
cal5	O
are	O
the	O
only	O
decision	O
trees	O
that	O
used	O
a	O
cost	O
matrix	O
here	O
,	O
and	O
hence	O
the	O
others	O
have	O
performed	O
worse	O
than	O
the	O
default	O
rule	O
.	O
cart	O
and	O
nodes	O
.	O
however	O
,	O
using	O
error	O
rate	O
as	O
a	O
criterion	O
we	O
can	O
not	O
judge	O
whether	O
these	O
algorithms	O
cal5	O
both	O
had	O
trees	O
of	O
around	O
5-7	O
nodes	O
,	O
whereasû	O
	O
and	O
newid	O
both	O
had	O
around	O
240	O
{	O
152	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
were	O
under-pruning	O
,	O
since	O
no	O
cost	O
matrix	O
was	O
used	O
in	O
the	O
classiﬁer	B
.	O
but	O
,	O
for	O
interpretability	O
,	O
the	O
smaller	O
trees	O
are	O
preferred	O
.	O
titterington	O
et	O
al	O
.	O
(	O
1981	O
)	O
compared	O
several	O
discrimination	O
procedures	O
on	O
this	O
data	O
.	O
our	O
dataset	O
differs	O
by	O
replacing	O
all	O
missing	O
values	O
with	O
the	O
class	O
median	O
and	O
so	O
the	O
results	O
are	O
not	O
directly	O
comparable	O
.	O
9.4.2	O
heart	B
disease	I
(	O
heart	O
)	O
table	O
9.15	O
:	O
results	O
for	O
the	O
heart	B
disease	I
dataset	O
(	O
2	O
classes	O
,	O
13	O
attributes	O
,	O
270	O
observations	O
,	O
9-fold	O
cross-validation	O
)	O
.	O
algorithms	O
in	O
italics	O
have	O
not	O
incorporated	O
costs	O
.	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
max	O
.	O
algorithm	O
storage	O
223	O
322	O
494	O
88	O
95	O
88	O
93	O
142	O
65	O
21	O
209	O
63	O
50	O
125	O
93	O
102	O
51	O
36	O
53	O
299	O
154	O
54	O
122	O
*	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
cascade	O
default	O
baytree	O
naivebay	O
cn2	O
ac	O
c4.5ã	O
time	O
(	O
sec	O
.	O
)	O
test	O
train	O
1.8	O
7.7	O
9.2	O
18.2	O
79.9	O
4.2	O
0.1	O
350.0	O
5.2	O
31.2	O
1.0	O
0.0	O
20.0	O
3.4	O
0.8	O
4.1	O
0.1	O
8.4	O
3.0	O
9.0	O
243.0	O
7.0	O
0.3	O
2.7	O
1.0	O
1.5	O
4.7	O
19.2	O
29.4	O
0.8	O
*	O
5.1	O
0.8	O
2.3	O
1.9	O
227.1	O
0.3	O
18.0	O
128.2	O
12.9	O
3.7	O
20.4	O
1.0	O
76.6	O
1.0	O
78.3	O
*	O
*	O
average	O
costs	O
test	O
train	O
0.393	O
0.315	O
0.422	O
0.274	O
0.271	O
0.396	O
0.478	O
0.264	O
0.407	O
0.394	O
0.478	O
0.000	O
0.374	O
0.441	O
0.452	O
0.463	O
0.630	O
0.261	O
0.844	O
0.000	O
0.000	O
0.744	O
0.526	O
0.111	O
0.374	O
0.351	O
0.767	O
0.206	O
0.439	O
0.781	O
0.515	O
0.444	O
0.693	O
0.507	O
0.574	O
0.781	O
0.600	O
0.467	O
0.560	O
0.330	O
0.429	O
0.429	O
0.381	O
0.303	O
0.140	O
0.207	O
0.560	O
*	O
rank	O
2	O
5	O
3	O
10	O
4	O
10	O
6	O
8	O
18	O
24	O
20	O
14	O
1	O
21	O
22	O
13	O
7	O
19	O
12	O
16	O
22	O
17	O
9	O
15	O
this	O
database	O
comes	O
from	O
the	O
cleveland	O
clinic	O
foundation	O
and	O
was	O
supplied	O
by	O
robert	O
detrano	O
,	O
m.d.	O
,	O
ph.d.	O
of	O
the	O
v.a	O
.	O
medical	O
center	O
,	O
long	O
beach	O
,	O
ca	O
.	O
it	O
is	O
part	O
of	O
the	O
collection	O
of	O
databases	O
at	O
the	O
university	O
of	O
california	O
,	O
irvine	O
collated	O
by	O
david	O
aha	O
.	O
the	O
purpose	O
of	O
the	O
dataset	O
is	O
to	O
predict	O
the	O
presence	O
or	O
absence	O
of	O
heart	B
disease	I
given	O
the	O
results	O
of	O
various	O
medical	O
tests	O
carried	O
out	O
on	O
a	O
patient	O
.	O
this	O
database	O
contains	O
13	O
attributes	O
,	O
which	O
have	O
been	O
extracted	O
from	O
a	O
larger	O
set	O
of	O
75.	O
the	O
database	O
originally	O
contained	O
303	O
examples	O
but	O
6	O
of	O
these	O
contained	O
missing	O
class	O
values	O
and	O
so	O
were	O
discarded	O
leaving	O
297	O
.	O
27	O
of	O
these	O
were	O
retained	O
in	O
case	O
of	O
dispute	O
,	O
leaving	O
a	O
ﬁnal	O
total	O
of	O
270.	O
there	O
are	O
two	O
classes	O
:	O
presenceand	O
absence	O
(	O
of	O
heart-disease	O
)	O
.	O
this	O
is	O
a	O
reduction	O
of	O
the	O
number	O
of	O
classes	O
sec	O
.	O
9.4	O
]	O
cost	O
datasets	O
153	O
in	O
the	O
original	O
dataset	O
in	O
which	O
there	O
were	O
four	O
different	O
degrees	O
of	O
heart-disease	O
.	O
table	O
9.16	O
gives	O
the	O
different	O
costs	O
of	O
the	O
possible	O
misclassiﬁcations	O
.	O
nine	O
fold	O
cross-validation	O
was	O
used	O
to	O
estimate	O
the	O
average	O
misclassiﬁcation	O
cost	O
.	O
naive	O
bayes	O
performed	O
best	O
on	O
the	O
heart	O
dataset	O
.	O
this	O
may	O
reﬂect	O
the	O
careful	O
selection	O
of	O
attributes	O
by	O
the	O
doctors	O
.	O
of	O
the	O
decision	O
trees	O
,	O
cart	O
and	O
cal5	O
performed	O
the	O
best	O
.	O
cal5	O
tuned	O
the	O
pruning	B
parameter	O
,	O
and	O
not	O
take	O
the	O
cost	O
matrix	O
into	O
account	O
,	O
so	O
the	O
prefered	O
pruning	B
is	O
still	O
an	O
open	O
question	O
.	O
used	O
an	O
average	O
of	O
8	O
nodes	O
in	O
the	O
trees	O
,	O
whereasû	O
this	O
data	O
has	O
been	O
studied	O
in	O
the	O
literature	O
before	O
,	O
but	O
without	O
taking	O
any	O
cost	O
matrix	O
into	O
account	O
and	O
so	O
the	O
results	O
are	O
not	O
comparable	O
with	O
those	O
obtained	O
here	O
.	O
table	O
9.16	O
:	O
misclassiﬁcation	O
costs	O
for	O
the	O
heart	B
disease	I
dataset	O
.	O
the	O
columns	O
represent	O
the	O
predicted	O
class	O
,	O
and	O
the	O
rows	O
the	O
true	O
class	O
.	O
	O
used	O
45	O
nodes	O
.	O
however	O
,	O
û	O
	O
did	O
absent	O
present	O
absent	O
present	O
0	O
5	O
1	O
0	O
9.4.3	O
german	O
credit	O
(	O
cr.ger	O
)	O
table	O
9.17	O
:	O
cost	O
matrix	O
for	O
the	O
german	O
credit	O
dataset	O
.	O
the	O
columns	O
are	O
the	O
predicted	O
class	O
and	O
the	O
rows	O
the	O
true	O
class	O
.	O
good	O
0	O
5	O
bad	O
1	O
0	O
good	O
bad	O
the	O
original	O
dataset	O
(	O
provided	O
by	O
professor	O
dr.	O
hans	O
hofmann	O
,	O
universit¨at	O
hamburg	O
)	O
contained	O
some	O
categorical/symbolic	O
attributes	O
.	O
for	O
algorithms	O
that	O
required	O
numerical	O
attributes	O
,	O
a	O
version	O
was	O
produced	O
with	O
several	O
indicator	O
variables	O
added	O
.	O
the	O
attributes	O
that	O
were	O
ordered	O
categorical	O
were	O
coded	O
as	O
integer	O
.	O
this	O
preprocessed	O
dataset	O
had	O
24	O
numerical	O
attributes	O
and	O
10-fold	O
cross-validation	O
was	O
used	O
for	O
the	O
classiﬁcation	B
,	O
and	O
for	O
uniformity	O
all	O
algorithms	O
used	O
this	O
preprocessed	O
version	O
.	O
it	O
is	O
of	O
interest	O
that	O
newid	O
did	O
the	O
trials	O
with	O
both	O
the	O
preprocessed	O
version	O
and	O
the	O
original	O
data	O
,	O
and	O
obtained	O
nearly	O
identical	O
error	O
rates	O
(	O
32.8	O
%	O
and	O
31.3	O
%	O
)	O
but	O
rather	O
different	O
tree	O
sizes	O
(	O
179	O
and	O
306	O
nodes	O
)	O
.	O
the	O
attributes	O
of	O
the	O
original	O
dataset	O
include	O
:	O
status	O
of	O
existing	O
current	O
account	O
,	O
duration	O
of	O
current	O
account	O
,	O
credit	O
history	O
,	O
reason	O
for	O
loan	O
request	O
(	O
e.g	O
.	O
new	O
car	O
,	O
furniture	O
)	O
,	O
credit	O
amount	O
,	O
savings	O
account/bonds	O
,	O
length	O
of	O
employment	O
,	O
installment	O
rate	O
in	O
percentage	O
of	O
disposable	O
income	O
,	O
marital	O
status	O
and	O
sex	O
,	O
length	O
of	O
time	O
at	O
presentresidence	O
,	O
age	O
and	O
job	O
.	O
{	O
{	O
154	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
results	O
are	O
given	O
in	O
table	O
9.18.	O
the	O
providers	O
of	O
this	O
dataset	O
suggest	O
the	O
cost	O
matrix	O
of	O
table	O
9.17.	O
it	O
is	O
interesting	O
that	O
only	O
10	O
algorithms	O
do	O
better	O
than	O
the	O
default	O
.	O
the	O
results	O
clearly	O
demonstrate	O
that	O
some	O
decision	O
tree	O
algorithms	O
are	O
at	O
a	O
disadvantage	O
when	O
costs	O
are	O
taken	O
into	O
account	O
.	O
that	O
it	O
is	O
possible	O
to	O
include	O
costs	O
into	O
decision	O
trees	O
,	O
is	O
demonstrated	O
by	O
the	O
good	O
results	O
of	O
cal5	O
and	O
cart	O
(	O
breiman	O
et	O
al.	O
,	O
1984	O
)	O
.	O
cal5	O
achieved	O
a	O
good	O
result	O
with	O
an	O
average	O
of	O
only	O
2	O
nodes	O
which	O
would	O
lead	O
to	O
very	O
transparent	O
rules	O
.	O
of	O
those	O
algorithms	O
that	O
did	O
not	O
include	O
costs	O
,	O
c4.5	O
used	O
a	O
tree	O
with	O
49	O
nodes	O
(	O
with	O
an	O
error	O
rates	O
of	O
29.4	O
%	O
and	O
32.8	O
%	O
respectively	O
)	O
.	O
table	O
9.18	O
:	O
results	O
for	O
the	O
german	O
credit	O
dataset	O
(	O
2	O
classes	O
,	O
24	O
attributes	O
,	O
1000	O
observa-	O
tions	O
,	O
10-fold	O
cross-validation	O
)	O
.	O
algorithms	O
in	O
italics	O
have	O
not	O
incorporated	O
costs	O
.	O
	O
and	O
newid	O
used	O
an	O
average	O
of	O
over	O
300	O
nodes	O
(	O
with	O
error	O
rate	O
of	O
27.3	O
%	O
)	O
,	O
whereasû	O
algorithm	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
ac	O
baytree	O
naivebay	O
cn2	O
c4.5	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
default	O
maximum	O
storage	O
556	O
534	O
391	O
935	O
103	O
286	O
93	O
95	O
668	O
118	O
771	O
79	O
460	O
320	O
82	O
69	O
167	O
152	O
53	O
148	O
215	O
97	O
*	O
time	O
(	O
sec	O
.	O
)	O
train	O
50.1	O
53.6	O
56.0	O
6522.9	O
9123.3	O
2.4	O
109.9	O
114.0	O
337.5	O
12.8	O
9668.0	O
7.4	O
26.0	O
116.8	O
13.7	O
32.5	O
19.5	O
5897.2	O
77.8	O
2258.5	O
24.5	O
322.7	O
*	O
test	O
7.3	O
8.2	O
6.7	O
*	O
*	O
9.0	O
9.5	O
1.1	O
248.0	O
15.2	O
232.0	O
0.4	O
5.3	O
3.1	O
1.0	O
3.0	O
1.9	O
5.3	O
5.0	O
0.0	O
3.4	O
4.7	O
*	O
average	O
costs	O
test	O
train	O
0.509	O
0.535	O
0.619	O
0.431	O
0.538	O
0.499	O
0.601	O
0.389	O
0.597	O
0.584	O
0.694	O
0.000	O
0.583	O
0.582	O
0.613	O
0.581	O
0.069	O
0.761	O
0.925	O
0.000	O
0.878	O
0.000	O
0.778	O
0.126	O
0.600	O
0.703	O
0.856	O
0.000	O
0.985	O
0.640	O
0.879	O
0.603	O
1.160	O
0.599	O
0.772	O
0.971	O
0.963	O
0.700	O
0.600	O
0.689	O
0.574	O
0.446	O
0.848	O
0.229	O
0.700	O
*	O
rank	O
1	O
9	O
2	O
6	O
4	O
10	O
3	O
8	O
14	O
19	O
17	O
15	O
12	O
16	O
22	O
18	O
7	O
23	O
5	O
13	O
21	O
20	O
11	O
9.5	O
other	O
datasets	O
this	O
section	O
contains	O
rather	O
a	O
“	O
mixed	O
bag	O
”	O
of	O
datasets	O
,	O
mostly	O
of	O
an	O
industrial	O
application	O
.	O
9.5.1	O
shuttle	B
control	I
(	O
shuttle	O
)	O
the	O
dataset	O
was	O
provided	O
by	O
jason	O
catlett	O
who	O
was	O
then	O
at	O
the	O
basser	O
department	O
of	O
computer	O
science	O
,	O
university	O
of	O
sydney	O
,	O
n.s.w.	O
,	O
australia	O
.	O
the	O
data	O
originated	O
from	O
nasa	O
and	O
concern	O
the	O
position	O
of	O
radiators	O
within	O
the	O
space	O
shuttle	O
.	O
the	O
problem	O
{	O
sec	O
.	O
9.5	O
]	O
miscellaneous	O
data	O
155	O
appears	O
to	O
be	O
noise-free	O
in	O
the	O
sense	O
that	O
arbitrarily	O
small	O
error	O
rates	O
are	O
possible	O
given	O
sufﬁcient	O
data	O
.	O
-	O
-	O
+	O
+	O
-	O
--	O
-	O
-	O
--	O
--	O
-	O
--	O
-	O
-	O
-	O
-	O
-	O
--	O
--	O
-	O
--	O
-	O
--	O
-	O
--	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
--	O
--	O
-	O
-	O
-	O
-	O
--	O
-	O
--	O
-	O
--	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
--	O
-	O
-	O
-	O
+	O
--	O
-	O
--	O
--	O
+	O
--	O
--	O
-	O
+-	O
+-	O
++	O
++	O
++	O
+	O
++	O
++	O
++	O
++++	O
++	O
++++++	O
+	O
+	O
+++	O
+	O
+	O
+	O
+	O
+	O
+	O
+	O
+	O
+	O
+	O
+	O
+	O
+	O
++	O
+	O
+++	O
+	O
+	O
+	O
+	O
+++	O
+	O
+	O
+	O
+++	O
++++	O
+	O
+	O
++	O
+++	O
+	O
+++	O
+++	O
++++	O
++	O
++	O
++	O
+	O
++	O
++	O
++++++	O
+++	O
+	O
+	O
+	O
+	O
|x1	O
<	O
54.5	O
x1	O
<	O
52.5	O
x9	O
<	O
3	O
rad_flow	O
x9	O
<	O
3	O
rad_flow	O
high	O
0	O
0	O
2	O
0	O
0	O
1	O
0	O
+	O
9	O
x	O
0	O
0	O
1	O
-	O
0	O
0	O
2	O
-	O
0	O
0	O
3	O
-	O
30å	O
+	O
40æ	O
50	O
x1ä	O
60	O
rad_flow	O
high	O
fig	O
.	O
9.5	O
:	O
shuttle	O
data	O
:	O
attributes	O
1	O
and	O
9	O
for	O
the	O
two	O
classes	O
rad	O
flow	O
and	O
high	O
only	O
.	O
the	O
symbols	O
“	O
+	O
”	O
and	O
“	O
-	O
”	O
denote	O
the	O
state	O
rad	O
flow	O
and	O
high	O
respectively	O
.	O
the	O
40	O
856	O
examples	O
are	O
classiﬁed	O
correctly	O
by	O
the	O
decision	O
tree	O
in	O
the	O
right	O
diagram	O
.	O
the	O
data	O
was	O
divided	O
into	O
a	O
train	O
set	O
and	O
a	O
test	O
set	O
with	O
43500	O
examples	O
in	O
the	O
train	O
set	O
and	O
14500	O
in	O
the	O
test	O
set	O
.	O
a	O
single	O
train-and-test	O
was	O
used	O
to	O
calculate	O
the	O
accuracy	O
.	O
with	O
samples	O
of	O
this	O
size	O
,	O
it	O
should	O
be	O
possible	O
to	O
obtain	O
an	O
accuracy	O
of	O
99	O
-	O
99.9	O
%	O
.	O
approximately	O
80	O
%	O
of	O
the	O
data	O
belong	O
to	O
class	O
1.	O
at	O
the	O
other	O
extreme	O
,	O
there	O
are	O
only	O
6	O
examples	O
of	O
class	O
6	O
in	O
the	O
learning	O
set	O
.	O
the	O
shuttle	O
dataset	O
also	O
departs	O
widely	O
from	O
typical	O
distribution	O
assumptions	O
.	O
the	O
attributes	O
are	O
numerical	O
and	O
appear	O
to	O
exhibit	O
multimodality	O
(	O
we	O
do	O
not	O
have	O
a	O
good	O
statistical	B
test	O
to	O
measure	B
this	O
)	O
.	O
some	O
feeling	O
for	O
this	O
dataset	O
can	O
be	O
gained	O
by	O
looking	O
at	O
figure	O
9.5.	O
it	O
shows	O
that	O
a	O
rectangular	O
box	O
(	O
with	O
sides	O
parallel	O
to	O
the	O
axes	O
)	O
may	O
be	O
drawn	O
to	O
enclose	O
all	O
examples	O
in	O
the	O
class	O
“	O
high	O
”	O
,	O
although	O
the	O
lower	O
boundary	O
of	O
this	O
box	O
(	O
x9	O
less	O
than	O
3	O
)	O
is	O
so	O
close	O
to	O
examples	O
of	O
class	O
“	O
rad	O
flow	O
”	O
that	O
this	O
particular	O
boundary	O
can	O
not	O
be	O
clearly	O
marked	O
to	O
the	O
scale	O
of	O
figure	O
9.5.	O
in	O
the	O
whole	O
dataset	O
,	O
the	O
data	O
seem	O
to	O
consist	O
of	O
isolated	O
islands	O
or	O
clusters	O
of	O
points	O
,	O
each	O
of	O
which	O
is	O
pure	O
(	O
belongs	O
to	O
only	O
one	O
class	O
)	O
,	O
with	O
one	O
class	O
comprising	O
several	O
such	O
islands	O
.	O
however	O
,	O
neighbouring	O
islands	O
may	O
be	O
very	O
close	O
and	O
yet	O
come	O
from	O
different	O
populations	O
.	O
the	O
boundaries	O
of	O
the	O
islands	O
seem	O
to	O
be	O
parallel	O
with	O
the	O
coordinate	O
axes	O
.	O
if	O
this	O
picture	O
is	O
correct	O
,	O
and	O
the	O
present	O
data	O
do	O
not	O
contradict	O
it	O
,	O
as	O
it	O
is	O
possible	O
to	O
classify	O
the	O
combined	O
dataset	O
with	O
100	O
%	O
accuracy	O
using	O
a	O
decision	O
tree	O
,	O
then	O
it	O
is	O
of	O
interest	O
to	O
ask	O
which	O
of	O
our	O
algorithms	O
are	O
guaranteed	O
to	O
arrive	O
at	O
the	O
correct	O
classiﬁcation	B
given	O
an	O
arbitrarily	O
large	O
learning	O
dataset	O
.	O
in	O
the	O
following	O
,	O
we	O
ignore	O
practical	O
matters	O
such	O
as	O
training	O
times	O
,	O
storage	O
requirements	O
etc.	O
,	O
and	O
concentrate	O
on	O
the	O
limiting	O
behaviour	O
for	O
an	O
inﬁnitely	O
large	O
training	O
set	O
.	O
ç	O
ç	O
ç	O
ç	O
156	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
table	O
9.19	O
:	O
results	O
for	O
the	O
shuttle	O
dataset	O
with	O
error	O
rates	O
are	O
in	O
%	O
(	O
7	O
classes	O
,	O
9	O
attributes	O
,	O
(	O
train	O
,	O
test	O
)	O
=	O
(	O
43	O
500	O
,	O
14	O
500	O
)	O
observations	O
)	O
.	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
max	O
.	O
algorithm	O
storage	O
1957	O
1583	O
1481	O
636	O
636	O
636	O
77	O
176	O
329	O
1535	O
200	O
368	O
225	O
1432	O
3400	O
665	O
372	O
fd	O
674	O
144	O
249	O
650	O
*	O
baytree	O
naivebay	O
cn2	O
c4.5è	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
default	O
time	O
(	O
sec	O
.	O
)	O
train	O
507.8	O
708.6	O
6945.5	O
110009.8	O
55215.0	O
32531.3	O
461.3	O
79.0	O
1151.9	O
6180.0	O
2553.0	O
240.0	O
1029.5	O
11160.0	O
13742.4	O
91969.7	O
313.4	O
fd	O
2068.0	O
5174.0	O
*	O
2813.3	O
*	O
test	O
102.3	O
176.6	O
106.2	O
93.2	O
18333.0	O
10482.0	O
149.7	O
2.3	O
16.2	O
*	O
2271.0	O
16.8	O
22.4	O
*	O
11.1	O
*	O
10.3	O
fd	O
176.2	O
21.0	O
*	O
83.8	O
*	O
%	O
error	O
train	O
4.98	O
6.35	O
3.94	O
0.61	O
0.95	O
0.39	O
3.70	O
0.04	O
0.04	O
0.00	O
0.00	O
0.00	O
4.60	O
0.00	O
0.04	O
*	O
0.03	O
fd	O
0.44	O
4.50	O
1.60	O
0.40	O
21.59	O
test	O
rank	O
4.83	O
20	O
21	O
6.72	O
18	O
3.83	O
14	O
0.59	O
0.83	O
15	O
11	O
0.44	O
17	O
3.80	O
5	O
0.08	O
6	O
0.09	O
0.01	O
1	O
8	O
0.32	O
2	O
0.02	O
19	O
4.50	O
0.03	O
3	O
7	O
0.10	O
9	O
0.41	O
0.03	O
3	O
fd	O
0.48	O
0.43	O
1.40	O
0.44	O
20.84	O
13	O
10	O
16	O
11	O
22	O
procedures	O
which	O
might	O
therefore	O
be	O
expected	O
to	O
ﬁnd	O
a	O
perfect	O
rule	O
for	O
this	O
dataset	O
would	O
seem	O
to	O
be	O
:	O
k-nn	O
,	O
backprop	O
and	O
alloc80	O
.	O
alloc80	O
failed	O
here	O
,	O
and	O
the	O
result	O
obtained	O
by	O
another	O
kernel	O
method	O
(	O
using	O
a	O
sphered	O
transformation	O
of	O
the	O
data	O
)	O
was	O
far	O
from	O
perfect	O
.	O
rbf	O
should	O
also	O
be	O
capable	O
of	O
perfect	O
accuracy	O
,	O
but	O
some	O
changes	O
would	O
be	O
required	O
in	O
the	O
particular	O
implementation	O
used	O
in	O
the	O
project	O
(	O
to	O
avoid	O
singularities	O
)	O
.	O
using	O
a	O
variable	O
selection	O
method	O
(	O
selecting	O
6	O
of	O
the	O
attributes	O
)	O
k-nn	O
achieved	O
an	O
error	O
rate	O
of	O
0.055	O
%	O
.	O
decision	O
trees	O
will	O
also	O
ﬁnd	O
the	O
perfect	O
rule	O
provided	O
that	O
the	O
pruning	B
parameter	O
is	O
properly	O
set	O
,	O
but	O
may	O
not	O
do	O
so	O
under	O
all	O
circumstances	O
as	O
it	O
is	O
occasionally	O
necessary	O
to	O
override	O
the	O
splitting	O
criterion	O
(	O
breiman	O
et	O
al.	O
,	O
1984	O
)	O
.	O
although	O
a	O
machine	O
learning	O
procedure	O
may	O
ﬁnd	O
a	O
decision	O
tree	O
which	O
classiﬁes	O
perfectly	O
,	O
it	O
may	O
not	O
ﬁnd	O
the	O
simplest	O
representation	O
.	O
the	O
tree	O
of	O
figure	O
9.5	O
,	O
which	O
was	O
produced	O
by	O
the	O
splus	O
procedure	O
tree	O
(	O
)	O
,	O
gets	O
100	O
%	O
accuracy	O
with	O
ﬁve	O
terminal	O
nodes	O
,	O
whereas	O
it	O
is	O
easy	O
to	O
construct	O
an	O
equivalent	O
tree	O
with	O
only	O
three	O
terminal	O
nodes	O
(	O
see	O
that	O
the	O
same	O
structure	O
occurs	O
in	O
both	O
halves	O
of	O
the	O
tree	O
in	O
figure	O
9.5	O
)	O
.	O
it	O
is	O
possible	O
to	O
classify	O
the	O
full	O
58	O
000	O
examples	O
with	O
only	O
19	O
errors	O
using	O
a	O
linear	O
decision	O
tree	O
with	O
nine	O
terminal	O
nodes	O
.	O
since	O
there	O
are	O
seven	O
classes	O
,	O
this	O
û	O
{	O
	O
sec	O
.	O
9.5	O
]	O
miscellaneous	O
data	O
157	O
is	O
a	O
remarkably	O
simple	O
tree	O
.	O
this	O
suggests	O
that	O
the	O
data	O
have	O
been	O
generated	O
by	O
a	O
process	O
that	O
is	O
governed	O
by	O
a	O
linear	O
decision	O
tree	O
,	O
that	O
is	O
,	O
a	O
decision	O
tree	O
in	O
which	O
tests	O
are	O
applied	O
sequentially	O
,	O
the	O
result	O
of	O
each	O
test	O
being	O
to	O
allocate	O
one	O
section	O
of	O
the	O
data	O
to	O
one	O
class	O
and	O
to	O
apply	O
subsequent	O
tests	O
to	O
the	O
remaining	O
section	O
.	O
as	O
there	O
are	O
very	O
few	O
examples	O
of	O
class	O
6	O
in	O
the	O
whole	O
58	O
000	O
dataset	O
,	O
it	O
would	O
require	O
enormous	O
amounts	O
of	O
data	O
to	O
construct	O
reliable	O
classiﬁers	O
for	O
class	O
6.	O
the	O
actual	O
trees	O
produced	O
by	O
the	O
algorithms	O
are	O
rather	O
small	O
,	O
as	O
expected	O
:	O
û	O
9.5.2	O
diabetes	B
(	O
diab	O
)	O
	O
has	O
13	O
nodes	O
,	O
and	O
both	O
cal5	O
and	O
cart	O
have	O
21	O
nodes	O
.	O
this	O
dataset	O
was	O
originally	O
donated	O
by	O
vincent	O
sigillito	O
,	O
applied	O
physics	O
laboratory	O
,	O
johns	O
hopkins	O
university	O
,	O
laurel	O
,	O
md	O
20707	O
and	O
was	O
constructed	O
by	O
constrained	O
selection	O
from	O
a	O
larger	O
database	O
held	O
by	O
the	O
national	O
institute	O
of	O
diabetes	B
and	O
digestive	O
and	O
kidney	O
diseases	O
.	O
it	O
is	O
publicly	O
available	O
from	O
the	O
machine	O
learning	O
database	O
at	O
uci	O
(	O
see	O
appendix	O
a	O
)	O
.	O
all	O
patients	O
represented	O
in	O
this	O
dataset	O
are	O
females	O
at	O
least	O
21	O
years	O
old	O
of	O
pima	O
indian	O
heritage	O
living	O
near	O
phoenix	O
,	O
arizona	O
,	O
usa	O
.	O
the	O
problem	O
posed	O
here	O
is	O
to	O
predict	O
whether	O
a	O
patient	O
would	O
test	O
positive	O
for	O
diabetes	B
according	O
to	O
world	O
health	O
organization	O
criteria	O
(	O
i.e	O
.	O
if	O
the	O
patients	O
’	O
2	O
hour	O
post–load	O
plasma	O
glucose	O
is	O
at	O
least	O
200	O
mg/dl	O
.	O
)	O
given	O
a	O
number	O
of	O
physiological	O
measurements	O
and	O
medical	O
test	O
results	O
.	O
the	O
attribute	O
details	O
are	O
given	O
below	O
:	O
number	O
of	O
times	O
pregnant	O
plasma	O
glucose	O
concentration	O
in	O
an	O
oral	O
glucose	O
tolerance	O
test	O
diastolic	O
blood	O
pressure	O
(	O
mm/hg	O
)	O
triceps	O
skin	O
fold	O
thickness	O
(	O
mm	O
)	O
2-hour	O
serum	O
insulin	O
(	O
mu	O
u/ml	O
)	O
body	O
mass	O
index	O
(	O
kg/m	O
)	O
diabetes	B
pedigree	O
function	O
age	O
(	O
years	O
)	O
this	O
is	O
a	O
two	O
class	O
problem	O
with	O
class	O
value	O
1	O
being	O
interpreted	O
as	O
“	O
tested	O
positive	O
for	O
diabetes	B
”	O
.	O
there	O
are	O
500	O
examples	O
of	O
class	O
1	O
and	O
268	O
of	O
class	O
2.	O
twelve–fold	O
cross	O
validation	O
was	O
used	O
to	O
estimate	O
prediction	O
accuracy	O
.	O
the	O
dataset	O
is	O
rather	O
difﬁcult	O
to	O
classify	O
.	O
the	O
so-called	O
“	O
class	O
”	O
value	O
is	O
really	O
a	O
binarised	O
form	O
of	O
another	O
attribute	O
which	O
is	O
itself	O
highly	O
indicative	O
of	O
certain	O
types	O
of	O
diabetes	B
but	O
does	O
not	O
have	O
a	O
one–to–one	O
correspondence	O
with	O
the	O
medical	O
condition	O
of	O
being	O
diabetic	O
.	O
no	O
algorithm	O
performs	O
exceptionally	O
well	O
,	O
although	O
alloc80	O
and	O
k-nn	O
seem	O
to	O
be	O
the	O
poorest	O
.	O
automatic	O
smoothing	O
parameter	O
selection	O
in	O
alloc80	O
can	O
make	O
poor	O
choices	O
for	O
datasets	O
with	O
discrete	O
valued	O
attributes	O
and	O
k-nn	O
can	O
have	O
problems	O
scaling	O
such	O
datasets	O
.	O
overall	O
though	O
,	O
it	O
seems	O
reasonable	O
to	O
conclude	O
that	O
the	O
attributes	O
do	O
not	O
predict	O
the	O
class	O
well	O
.	O
cal5	O
uses	O
only	O
8	O
nodes	O
in	O
its	O
decision	O
tree	O
,	O
whereas	O
newid	O
,	O
which	O
performs	O
less	O
well	O
,	O
has	O
119	O
nodes.û	O
cn2	O
generates	O
52	O
rules	O
,	O
although	O
there	O
is	O
not	O
very	O
much	O
difference	O
in	O
the	O
error	O
rates	O
here	O
.	O
this	O
dataset	O
has	O
been	O
studied	O
by	O
smith	O
et	O
al	O
.	O
(	O
1988	O
)	O
using	O
the	O
adap	O
algorithm	O
.	O
using	O
576	O
examples	O
as	O
a	O
training	O
set	O
,	O
adap	O
achieved	O
an	O
error	O
rate	O
of	O
.24	O
on	O
the	O
remaining	O
192	O
instances	O
.	O
	O
and	O
c4.5	O
have	O
116	O
and	O
32	O
nodes	O
,	O
repectively	O
and	O
{	O
{	O
158	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
table	O
9.20	O
:	O
results	O
for	O
the	O
diabetes	B
dataset	O
(	O
2	O
classes	O
,	O
8	O
attributes	O
,	O
768	O
observations	O
,	O
12-fold	O
cross-validation	O
)	O
.	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
max	O
.	O
algorithm	O
storage	O
338	O
327	O
311	O
780	O
152	O
226	O
82	O
144	O
596	O
87	O
373	O
68	O
431	O
190	O
61	O
60	O
137	O
62	O
52	O
147	O
179	O
69	O
*	O
baytree	O
naivebay	O
cn2	O
c4.5	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
default	O
time	O
(	O
sec	O
.	O
)	O
train	O
27.4	O
24.4	O
30.8	O
3762.0	O
1374.1	O
1.0	O
35.3	O
29.6	O
215.6	O
9.6	O
4377.0	O
10.4	O
25.0	O
38.4	O
11.5	O
31.2	O
236.7	O
1966.4	O
35.8	O
7171.0	O
4.8	O
139.5	O
*	O
test	O
6.5	O
6.6	O
6.6	O
*	O
*	O
2.0	O
4.7	O
0.8	O
209.4	O
10.2	O
241.0	O
0.3	O
7.2	O
2.8	O
0.9	O
1.5	O
0.1	O
2.5	O
0.8	O
0.1	O
0.1	O
1.2	O
*	O
error	O
rate	O
test	O
0.225	O
0.262	O
0.223	O
0.232	O
0.301	O
0.324	O
0.258	O
0.255	O
0.271	O
0.289	O
0.276	O
0.271	O
0.262	O
0.289	O
0.270	O
0.245	O
0.250	O
0.273	O
0.224	O
0.248	O
0.243	O
0.272	O
0.350	O
train	O
0.220	O
0.237	O
0.219	O
0.177	O
0.288	O
0.000	O
0.260	O
0.227	O
0.079	O
0.000	O
0.000	O
0.008	O
0.239	O
0.010	O
0.131	O
0.223	O
0.232	O
0.134	O
0.220	O
0.198	O
0.218	O
0.101	O
0.350	O
rank	O
3	O
11	O
1	O
4	O
21	O
22	O
10	O
9	O
14	O
19	O
18	O
14	O
11	O
19	O
13	O
6	O
8	O
17	O
2	O
7	O
5	O
16	O
23	O
9.5.3	O
dna	O
this	O
classiﬁcation	B
problem	O
is	O
drawn	O
from	O
the	O
ﬁeld	O
of	O
molecular	O
biology	O
.	O
splice	O
junctions	O
are	O
points	O
on	O
a	O
dna	O
sequence	O
at	O
which	O
“	O
superﬂuous	O
”	O
dna	O
is	O
removed	O
during	O
protein	O
creation	O
.	O
the	O
problem	O
posed	O
here	O
is	O
to	O
recognise	O
,	O
given	O
a	O
sequence	O
of	O
dna	O
,	O
the	O
boundaries	O
between	O
exons	O
(	O
the	O
parts	O
of	O
the	O
dna	O
sequence	O
retained	O
after	O
splicing	O
)	O
and	O
introns	O
(	O
the	O
parts	O
of	O
the	O
dna	O
that	O
are	O
spliced	O
out	O
)	O
.	O
the	O
dataset	O
used	O
in	O
the	O
project	O
is	O
a	O
processed	O
version	O
of	O
the	O
irvine	O
primate	O
splice-junction	O
database	O
.	O
each	O
of	O
the	O
3186	O
examples	O
in	O
the	O
database	O
consists	O
of	O
a	O
window	O
of	O
60	O
nucleotides	O
,	O
each	O
represented	O
by	O
one	O
of	O
four	O
symbolic	O
values	O
(	O
a	O
,	O
c	O
,	O
g	O
,	O
t	O
)	O
,	O
and	O
the	O
classiﬁcation	B
of	O
the	O
middle	O
point	O
in	O
the	O
window	O
as	O
one	O
of	O
;	O
intron–extron	O
boundary	O
,	O
extron–intron	O
boundary	O
or	O
neither	O
of	O
these	O
.	O
processing	O
involved	O
the	O
removal	O
of	O
a	O
small	O
number	O
of	O
ambiguous	O
examples	O
(	O
4	O
)	O
,	O
conversion	O
of	O
the	O
original	O
60	O
symbolic	O
attributes	O
to	O
180	O
or	O
240	O
binary	O
attributes	O
and	O
the	O
conversion	O
of	O
symbolic	O
class	O
labels	O
to	O
numeric	O
labels	O
(	O
see	O
section	O
7.4.3	O
)	O
.	O
the	O
training	O
set	O
of	O
2000	O
was	O
chosen	O
randomly	O
from	O
the	O
dataset	O
and	O
the	O
remaining	O
1186	O
examples	O
were	O
used	O
as	O
the	O
test	O
set	O
.	O
this	O
is	O
basically	O
a	O
partitioning	O
problem	O
and	O
so	O
we	O
might	O
expect	O
,	O
in	O
advance	O
,	O
that	O
decision	O
tree	O
algorithms	O
should	O
do	O
well	O
.	O
the	O
classes	O
in	O
this	O
problem	O
have	O
a	O
heirarchical	O
û	O
{	O
	O
sec	O
.	O
9.5	O
]	O
miscellaneous	O
data	O
159	O
table	O
9.21	O
:	O
results	O
for	O
the	O
dna	O
dataset	O
(	O
3	O
classes	O
,	O
60/180/240	O
attributes	O
,	O
(	O
train	O
,	O
test	O
)	O
=	O
(	O
2000	O
,	O
1186	O
)	O
observations	O
)	O
.	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
max	O
.	O
algorithm	O
storage	O
215	O
262	O
1661	O
247	O
188	O
247	O
86	O
283	O
729	O
729	O
9385	O
727	O
727	O
10732	O
1280	O
282	O
755	O
2592	O
518	O
161	O
1129	O
fd	O
*	O
baytree	O
naivebay	O
cn2	O
c4.5	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
default	O
time	O
(	O
sec	O
.	O
)	O
train	O
928.5	O
1581.1	O
5057.4	O
79676.0	O
14393.5	O
2427.5	O
396.7	O
615.0	O
523.0	O
698.4	O
12378.0	O
81.7	O
51.8	O
869.0	O
9.0	O
2211.6	O
1616.0	O
*	O
213.4	O
4094.0	O
*	O
fd	O
*	O
test	O
31.1	O
808.6	O
76.2	O
16.0	O
*	O
882.0	O
225.0	O
8.6	O
515.8	O
1.0	O
87.0	O
10.5	O
14.8	O
74.0	O
2.0	O
5.9	O
7.5	O
*	O
10.1	O
9.0	O
*	O
fd	O
*	O
error	O
rate	O
test	O
0.059	O
0.059	O
0.061	O
0.115	O
0.057	O
0.146	O
0.072	O
0.085	O
0.073	O
0.100	O
0.100	O
0.095	O
0.068	O
0.095	O
0.076	O
0.135	O
0.131	O
0.339	O
0.048	O
0.088	O
0.041	O
fd	O
0.492	O
train	O
0.034	O
0.000	O
0.008	O
0.034	O
0.063	O
0.000	O
0.061	O
0.075	O
0.040	O
0.000	O
0.000	O
0.001	O
0.052	O
0.002	O
0.040	O
0.131	O
0.104	O
0.104	O
0.007	O
0.014	O
0.015	O
fd	O
0.475	O
rank	O
4	O
4	O
6	O
17	O
3	O
20	O
8	O
11	O
9	O
15	O
15	O
13	O
7	O
13	O
10	O
19	O
18	O
21	O
2	O
12	O
1	O
22	O
structure	O
;	O
the	O
primary	O
decision	O
is	O
whether	O
the	O
centre	O
point	O
in	O
the	O
window	O
is	O
a	O
splice–	O
junction	O
or	O
not	O
.	O
if	O
it	O
is	O
a	O
splice–junction	O
then	O
the	O
secondary	O
classiﬁcation	B
is	O
as	O
to	O
its	O
type	O
;	O
intron–extron	O
or	O
extron–intron	O
.	O
unfortunately	O
comparisons	O
between	O
algorithms	O
are	O
more	O
difﬁcult	O
than	O
usual	O
with	O
this	O
dataset	O
as	O
a	O
number	O
of	O
methods	O
were	O
tested	O
with	O
a	O
restricted	O
number	O
of	O
attributes	O
;	O
some	O
were	O
tested	O
with	O
attribute	O
values	O
converted	O
to	O
180	O
binary	O
values	O
,	O
and	O
some	O
to	O
240	O
binary	O
values	O
.	O
castle	O
and	O
cart	O
only	O
used	O
the	O
middle	O
90	O
binary	O
variables	O
.	O
newid	O
,	O
cn2	O
and	O
c4.5	O
used	O
the	O
original	O
60	O
categorical	O
variables	O
and	O
k-nn	O
,	O
kohonen	O
,	O
lvq	O
,	O
backprop	O
and	O
rbf	O
used	O
the	O
one–of–four	O
coding	O
.	O
the	O
classical	O
statistical	B
algorithms	O
perform	O
reasonable	O
well	O
achieving	O
roughly	O
6	O
%	O
error	O
rate	O
.	O
k-nn	O
is	O
probably	O
hampered	O
by	O
the	O
large	O
number	O
of	O
binary	O
attributes	O
,	O
but	O
naive	O
bayes	O
does	O
rather	O
well	O
helped	O
by	O
the	O
fact	O
that	O
the	O
attributes	O
are	O
independent	O
.	O
surprisingly	O
,	O
machine	O
learning	O
algorithms	O
do	O
not	O
outperform	O
classical	O
statistical	B
al-	O
gorithms	O
on	O
this	O
problem	O
.	O
castle	O
and	O
cart	O
were	O
at	O
a	O
disadvantage	O
using	O
a	O
smaller	O
window	O
although	O
performing	O
reasonably	O
.	O
indcart	O
used	O
180	O
attributes	O
and	O
improved	O
on	O
the	O
cart	O
error	O
rate	O
by	O
around	O
1	O
%	O
.	O
itrule	O
and	O
cal5	O
are	O
the	O
poorest	O
performers	O
in	O
this	O
û	O
{	O
	O
160	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
itrule	O
,	O
using	O
only	O
uni–variate	O
and	O
bi–variate	O
tests	O
,	O
is	O
too	O
restricted	O
and	O
cal5	O
is	O
group	O
.	O
probably	O
confused	O
by	O
the	O
large	O
number	O
of	O
attributes	O
.	O
of	O
the	O
neural	O
network	O
algorithms	O
,	O
kohonen	O
performs	O
very	O
poorly	O
not	O
helped	O
by	O
unequal	O
class	O
proportions	O
in	O
the	O
dataset	O
.	O
dipol92	O
constructs	O
an	O
effective	O
set	O
of	O
piecewise	O
linear	O
decision	O
boundaries	O
but	O
overall	O
,	O
rbf	O
is	O
the	O
most	O
accurate	O
algorithm	O
using	O
720	O
centres	O
.	O
it	O
is	O
rather	O
worrying	O
here	O
,	O
that	O
lvq	O
claimed	O
an	O
error	O
rate	O
of	O
0	O
,	O
and	O
this	O
result	O
was	O
unchanged	O
when	O
the	O
test	O
data	O
had	O
the	O
classes	O
permuted	O
.	O
no	O
reason	O
could	O
be	O
found	O
for	O
this	O
phenomenon	O
–	O
presumably	O
it	O
was	O
caused	O
by	O
the	O
excessive	O
number	O
of	O
attributes	O
–	O
but	O
that	O
the	O
algorithm	O
should	O
“	O
lie	O
”	O
with	O
no	O
explanation	O
or	O
warning	O
is	O
still	O
a	O
mystery	O
.	O
this	O
problem	O
did	O
not	O
occur	O
with	O
any	O
other	O
dataset	O
.	O
in	O
order	O
to	O
assess	O
the	O
importance	O
of	O
the	O
window	O
size	O
in	O
this	O
problem	O
,	O
we	O
can	O
examine	O
in	O
a	O
little	O
more	O
detail	O
the	O
performance	O
of	O
one	O
of	O
the	O
machine	O
learning	O
algorithms	O
.	O
cn2	O
classiﬁed	O
the	O
training	O
set	O
using	O
113	O
rules	O
involving	O
tests	O
on	O
from	O
2	O
to	O
6	O
attributes	O
and	O
misclassifying	O
4	O
examples	O
.	O
table	O
9.22	O
shows	O
how	O
frequently	O
attributes	O
in	O
different	O
ranges	O
appeared	O
in	O
those	O
113	O
rules	O
.	O
from	O
the	O
table	O
it	O
appears	O
that	O
a	O
window	O
of	O
size	O
20	O
contains	O
the	O
table	O
9.22	O
:	O
frequency	O
of	O
occurrence	O
of	O
attributes	O
in	O
rules	O
generated	O
by	O
cn2	O
for	O
the	O
dna	O
training	O
set	O
.	O
class	O
1	O
class	O
2	O
class	O
3	O
total	O
1–10	O
17	O
17	O
6	O
40	O
11–20	O
21–30	O
31–40	O
41–50	O
51–60	O
10	O
28	O
8	O
46	O
12	O
78	O
57	O
147	O
59	O
21	O
55	O
135	O
7	O
13	O
4	O
24	O
2	O
11	O
3	O
16	O
most	O
important	O
variables	O
.	O
attributes	O
just	O
after	O
the	O
middle	O
of	O
the	O
window	O
are	O
most	O
important	O
in	O
determining	O
class	O
1	O
and	O
those	O
just	O
before	O
the	O
middle	O
are	O
most	O
important	O
in	O
determining	O
class	O
2.	O
for	O
class	O
3	O
,	O
variables	O
close	O
to	O
the	O
middle	O
on	O
either	O
side	O
are	O
equally	O
important	O
.	O
overall	O
though	O
,	O
variables	O
throughout	O
the	O
60	O
attribute	O
window	O
do	O
seem	O
to	O
contribute	O
.	O
the	O
question	O
of	O
how	O
many	O
attributes	O
to	O
use	O
in	O
the	O
window	O
is	O
vitally	O
important	O
for	O
procedures	O
that	O
include	O
many	O
parameters	O
-	O
quadisc	O
gets	O
much	O
better	O
results	O
(	O
error	O
rate	O
of	O
3.6	O
%	O
on	O
the	O
test	O
set	O
)	O
if	O
it	O
is	O
restricted	O
to	O
the	O
middle	O
20	O
categorical	O
attributes	O
.	O
it	O
is	O
therefore	O
of	O
interest	O
to	O
note	O
that	O
decision	O
tree	O
procedures	O
get	O
almost	O
the	O
same	O
accuracies	O
on	O
the	O
original	O
categorical	O
data	O
and	O
the	O
processed	O
binary	O
data	O
.	O
newid	O
,	O
obtained	O
an	O
error	O
rate	O
of	O
9.95	O
%	O
on	O
the	O
preprocessed	O
data	O
(	O
180	O
variables	O
)	O
and	O
9.20	O
%	O
on	O
the	O
original	O
data	O
(	O
with	O
categorical	O
attributes	O
)	O
.	O
these	O
accuracies	O
are	O
probably	O
within	O
what	O
could	O
be	O
called	O
experimental	O
error	O
,	O
so	O
it	O
seems	O
that	O
newid	O
does	O
about	O
as	O
well	O
on	O
either	O
form	O
of	O
the	O
dataset	O
.	O
there	O
is	O
a	O
little	O
more	O
to	O
the	O
story	O
however	O
,	O
as	O
the	O
university	O
of	O
wisconsin	O
ran	O
several	O
algorithms	O
on	O
this	O
dataset	O
.	O
in	O
table	O
9.23	O
we	O
quote	O
their	O
results	O
alongside	O
ours	O
for	O
nearest	O
neighbour	O
.	O
in	O
this	O
problem	O
,	O
id3	O
and	O
newid	O
are	O
probably	O
equivalent	O
,	O
and	O
the	O
slight	O
discrepancies	O
in	O
error	O
rates	O
achieved	O
by	O
id3	O
at	O
wisconsin	O
(	O
10.5	O
%	O
)	O
compared	O
to	O
newid	O
(	O
9.95	O
%	O
)	O
in	O
this	O
study	O
are	O
attributable	O
to	O
the	O
different	O
random	O
samples	O
used	O
.	O
this	O
can	O
not	O
be	O
the	O
explanation	O
for	O
the	O
differences	O
between	O
the	O
two	O
nearest	O
neighbour	O
results	O
:	O
there	O
appears	O
to	O
be	O
an	O
irreconcilable	O
difference	O
,	O
perhaps	O
due	O
to	O
preprocessing	B
,	O
perhaps	O
due	O
to	O
“	O
distance	O
”	O
being	O
measured	O
in	O
a	O
conditional	O
(	O
class	O
dependent	O
)	O
manner	O
.	O
certainly	O
,	O
the	O
kohonen	O
algorithm	O
used	O
here	O
encountered	O
a	O
problem	O
when	O
deﬁning	O
dis-	O
sec	O
.	O
9.5	O
]	O
miscellaneous	O
data	O
161	O
cû	O
cbû	O
~ü	O
~	O
{	O
{	O
h~	O
ü	O
tances	O
in	O
the	O
attribute	O
space	O
.	O
when	O
using	O
the	O
coding	O
of	O
180	O
attributes	O
,	O
the	O
euclidean	O
distances	O
between	O
pairs	O
were	O
not	O
the	O
same	O
(	O
the	O
squared	O
distances	O
were	O
2.0	O
for	O
pairs	O
g	O
but	O
only	O
1.0	O
for	O
the	O
pairs	O
involvingz	O
therefore	O
kohonen	O
needs	O
the	O
coding	O
of	O
240	O
attributes	O
.	O
this	O
coding	O
was	O
also	O
adopted	O
by	O
other	O
algorithms	O
using	O
distance	O
measures	O
(	O
k-nn	O
,	O
lvq	O
)	O
.	O
table	O
9.23	O
:	O
dna	O
dataset	O
error	O
rates	O
for	O
each	O
of	O
the	O
three	O
classes	O
:	O
splice–junction	O
is	O
intron–	O
extron	O
(	O
ie	O
)	O
,	O
extron–intron	O
(	O
ei	O
)	O
or	O
neither	O
.	O
all	O
trials	O
except	O
the	O
last	O
were	O
carried	O
out	O
by	O
the	O
university	O
of	O
wisconsin	O
,	O
sometimes	O
with	O
local	O
implementations	O
of	O
published	O
algorithms	O
,	O
using	O
ten-fold	O
cross-validation	O
on	O
1000	O
examples	O
randomly	O
selected	O
from	O
the	O
complete	O
set	O
of	O
3190.	O
the	O
last	O
trial	O
was	O
conducted	O
with	O
a	O
training	O
set	O
of	O
2000	O
examples	O
and	O
a	O
test	O
set	O
of	O
1186	O
examples	O
.	O
ü	O
;	O
~	O
{	O
h~	O
g	O
)	O
.	O
cû	O
:	O
algorithm	O
kbann	O
backprop	O
pebls	O
perceptron	O
id3	O
cobweb	O
n	O
neighbour	O
(	O
wisconsin	O
)	O
n	O
neighbour	O
(	O
leeds	O
)	O
neither	O
4.62	O
5.29	O
6.86	O
3.99	O
8.84	O
11.80	O
31.11	O
0.50	O
ei	O
7.56	O
5.74	O
8.18	O
16.32	O
10.58	O
15.04	O
11.65	O
25.74	O
ie	O
8.47	O
10.75	O
7.55	O
17.41	O
13.99	O
9.46	O
9.09	O
36.79	O
overall	O
6.28	O
6.69	O
7.36	O
10.31	O
10.50	O
12.08	O
20.94	O
14.60	O
9.5.4	O
technical	B
(	O
tech	O
)	O
table	O
9.24	O
:	O
the	O
four	O
most	O
common	O
classes	O
in	O
the	O
technical	B
data	O
,	O
classiﬁed	O
by	O
the	O
value	O
of	O
attribute	O
x52	O
.	O
very	O
little	O
is	O
known	O
about	O
this	O
dataset	O
as	O
the	O
nature	O
of	O
the	O
problem	O
domain	O
is	O
secret	O
.	O
it	O
is	O
of	O
commercial	O
interest	O
to	O
daimler-benz	O
ag	O
,	O
germany	O
.	O
the	O
dataset	O
shows	O
indications	O
of	O
some	O
sort	O
of	O
preprocessing	B
,	O
probably	O
by	O
some	O
decision-tree	O
type	O
process	O
,	O
before	O
it	O
was	O
received	O
.	O
to	O
give	O
only	O
one	O
instance	O
,	O
consider	O
only	O
the	O
four	O
most	O
common	O
classes	O
ô	O
+0.085	O
è0ê	O
)	O
,	O
and	O
consider	O
only	O
one	O
attribute	O
(	O
x52	O
)	O
.	O
by	O
simply	O
tabulating	O
the	O
values	O
of	O
attribute	O
x52	O
it	O
becomes	O
obvious	O
that	O
the	O
classiﬁcations	O
are	O
being	O
made	O
according	O
to	O
symmetrically	O
placed	O
boundaries	O
on	O
x52	O
,	O
speciﬁcally	O
the	O
two	O
boundaries	O
at	O
-0.055	O
and	O
+0.055	O
,	O
and	O
also	O
the	O
boundaries	O
at	O
-0.085	O
and	O
+0.085	O
.	O
these	O
boundaries	O
divide	O
the	O
range	O
of	O
x52	O
into	O
ﬁve	O
regions	O
,	O
and	O
if	O
we	O
look	O
at	O
the	O
classes	O
contained	O
in	O
these	O
regions	O
we	O
get	O
the	O
frequency	O
table	O
in	O
table	O
9.24.	O
the	O
symmetric	O
nature	O
of	O
the	O
boundaries	O
suggests	O
strongly	O
that	O
the	O
classes	O
have	O
been	O
deﬁned	O
by	O
their	O
attributes	O
,	O
and	O
that	O
the	O
class	O
deﬁnitions	O
are	O
only	O
concerned	O
with	O
inequalities	O
on	O
the	O
attributes	O
.	O
needless	O
to	O
say	O
,	O
such	O
a	O
system	O
is	O
perfectly	O
suited	O
to	O
decision	O
trees	O
,	O
and	O
we	O
may	O
remark	O
,	O
in	O
passing	O
,	O
that	O
the	O
above	O
table	O
was	O
discovered	O
ãé	O
èè	O
.~	O
(	O
û	O
range	O
of	O
x52	O
-0.085	O
-0.085	O
,	O
-0.055	O
-0.055	O
,	O
+0.055	O
+0.055	O
,	O
+0.085	O
ãé	O
0	O
260	O
0	O
0	O
0	O
èè	O
0	O
0	O
0	O
1036	O
0	O
è0ê	O
180	O
0	O
0	O
0	O
392	O
1	O
0	O
324	O
0	O
0	O
g	O
~	O
g	O
~	O
c	O
~	O
z	O
g	O
~	O
c	O
z	O
g	O
~	O
c	O
z	O
û	O
û	O
è	O
	O
û	O
û	O
ÿ	O
~	O
û	O
è	O
û	O
~	O
û	O
162	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
by	O
a	O
decision	O
tree	O
when	O
applied	O
to	O
the	O
reduced	O
technical	B
dataset	O
with	O
all	O
56	O
attributes	O
but	O
with	O
only	O
the	O
four	O
most	O
common	O
classes	O
(	O
in	O
other	O
words	O
,	O
the	O
decision	O
tree	O
could	O
classify	O
the	O
reduced	O
dataset	O
with	O
1	O
error	O
in	O
2193	O
examples	O
using	O
only	O
one	O
attribute	O
)	O
.	O
table	O
9.25	O
:	O
results	O
for	O
the	O
technical	B
dataset	O
(	O
91	O
classes	O
,	O
56	O
attributes	O
,	O
(	O
train	O
,	O
test	O
)	O
=	O
(	O
4500	O
,	O
2580	O
)	O
observations	O
)	O
.	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
max	O
.	O
algorithm	O
storage	O
365	O
334	O
354	O
524	O
fd	O
213	O
fd	O
fd	O
3328	O
592	O
7400	O
1096	O
656	O
*	O
2876	O
fd	O
842	O
640	O
941	O
fd	O
510	O
559	O
*	O
baytree	O
naivebay	O
cn2	O
c4.5	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
default	O
time	O
(	O
sec	O
.	O
)	O
train	O
421.3	O
19567.8	O
18961.2	O
21563.7	O
fd	O
5129.9	O
fd	O
fd	O
1418.6	O
527.1	O
5028.0	O
175.5	O
169.2	O
3980.0	O
384.0	O
fd	O
2422.1	O
*	O
7226.0	O
fd	O
1264.0	O
2443.2	O
*	O
test	O
200.8	O
11011.6	O
195.9	O
56.8	O
fd	O
2457.0	O
fd	O
fd	O
1423.3	O
12.5	O
273.0	O
9.8	O
81.6	O
465.0	O
96.0	O
fd	O
7.1	O
*	O
1235.0	O
fd	O
323.0	O
87.3	O
*	O
error	O
rate	O
test	O
0.391	O
0.495	O
0.401	O
0.366	O
fd	O
0.204	O
fd	O
fd	O
0.095	O
0.090	O
0.102	O
0.174	O
0.354	O
0.123	O
0.120	O
fd	O
0.183	O
0.357	O
0.192	O
fd	O
0.324	O
0.261	O
0.777	O
train	O
0.368	O
0.405	O
0.350	O
0.356	O
fd	O
0.007	O
fd	O
fd	O
0.007	O
0.000	O
0.006	O
0.019	O
0.323	O
0.048	O
0.050	O
fd	O
0.110	O
0.326	O
0.080	O
fd	O
0.304	O
0.196	O
0.770	O
rank	O
15	O
17	O
16	O
14	O
9	O
2	O
1	O
3	O
6	O
12	O
5	O
4	O
7	O
13	O
8	O
11	O
10	O
18	O
the	O
dataset	O
consists	O
of	O
7080	O
examples	O
with	O
56	O
attributes	O
and	O
91	O
classes	O
.	O
the	O
attributes	O
are	O
all	O
believed	O
to	O
be	O
real	O
:	O
however	O
,	O
the	O
majority	O
of	O
attribute	O
values	O
are	O
zero	O
.	O
this	O
may	O
be	O
the	O
numerical	O
value	O
“	O
0	O
”	O
or	O
more	O
likely	O
“	O
not	O
relevant	O
”	O
,	O
“	O
not	O
measured	O
”	O
or	O
“	O
not	O
applicable	O
”	O
.	O
one-shot	O
train	O
and	O
test	O
was	O
used	O
to	O
calculate	O
the	O
accuracy	O
.	O
the	O
results	O
for	O
this	O
dataset	O
seem	O
quite	O
poor	O
although	O
all	O
are	O
signiﬁcantly	O
better	O
than	O
the	O
default	O
error	O
rate	O
of	O
0.777.	O
several	O
algorithms	O
failed	O
to	O
run	O
on	O
the	O
dataset	O
as	O
they	O
could	O
not	O
cope	O
with	O
the	O
large	O
number	O
of	O
classes	O
.	O
the	O
decision	O
tree	O
algorithms	O
indcart	O
,	O
preprocessing	B
which	O
made	O
the	O
dataset	O
more	O
suited	O
to	O
decision	O
trees	O
algorithms	O
.	O
however	O
,	O
the	O
output	B
produced	O
by	O
the	O
tree	O
algorithms	O
is	O
(	O
not	O
surprisingly	O
)	O
difﬁcult	O
to	O
interpret	O
–	O
newid	O
	O
gave	O
the	O
best	O
results	O
in	O
terms	O
of	O
error	O
rates	O
.	O
this	O
reﬂects	O
the	O
nature	O
of	O
the	O
	O
has	O
newid	O
andû	O
has	O
a	O
tree	O
with	O
590	O
terminal	O
nodes	O
,	O
c4.5	O
has	O
258	O
nodes	O
,	O
cal5	O
has	O
507	O
nodes	O
andû	O
589	O
nodes	O
.	O
statistical	B
algorithms	O
gave	O
much	O
poorer	O
results	O
with	O
quadisc	O
giving	O
the	O
highest	O
error	O
rate	O
of	O
all	O
.	O
they	O
appear	O
to	O
over-train	O
slightly	O
as	O
a	O
result	O
of	O
too	O
many	O
parameters	O
.	O
û	O
{	O
	O
{	O
{	O
sec	O
.	O
9.5	O
]	O
miscellaneous	O
data	O
163	O
9.5.5	O
belgian	O
power	O
(	O
belg	O
)	O
table	O
9.26	O
:	O
results	O
for	O
the	O
belgian	O
power	O
i	O
(	O
2	O
classes	O
,	O
28	O
attributes	O
,	O
(	O
train	O
,	O
test	O
)	O
=	O
(	O
1250	O
,	O
1250	O
)	O
observations	O
)	O
.	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
max	O
.	O
algorithm	O
storage	O
588	O
592	O
465	O
98	O
125	O
86	O
279	O
170	O
293	O
846	O
222	O
289	O
276	O
345	O
77	O
293	O
62	O
216	O
49	O
146	O
*	O
115	O
391	O
*	O
baytree	O
naivebay	O
cn2	O
c4.5	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
cascade	O
default	O
time	O
(	O
sec	O
.	O
)	O
train	O
73.8	O
85.2	O
130.4	O
7804.1	O
3676.2	O
1.0	O
230.2	O
135.1	O
86.5	O
142.0	O
1442.0	O
24.7	O
17.4	O
272.2	O
66.0	O
1906.2	O
13.9	O
7380.6	O
43.0	O
478.0	O
121.4	O
977.7	O
806.0	O
*	O
test	O
27.8	O
40.5	O
27.1	O
15.6	O
*	O
137.0	O
96.2	O
8.5	O
85.4	O
1.0	O
79.0	O
6.7	O
7.6	O
16.9	O
11.6	O
41.1	O
7.2	O
54.9	O
11.9	O
2.0	O
29.3	O
32.0	O
1.0	O
*	O
error	O
rate	O
test	O
0.025	O
0.052	O
0.007	O
0.006	O
0.044	O
0.059	O
0.047	O
0.034	O
0.034	O
0.027	O
0.034	O
0.030	O
0.062	O
0.032	O
0.040	O
0.065	O
0.029	O
0.056	O
0.018	O
0.017	O
0.034	O
0.054	O
0.019	O
0.362	O
train	O
0.022	O
0.036	O
0.002	O
0.003	O
0.026	O
0.000	O
0.029	O
0.009	O
0.007	O
0.017	O
0.000	O
0.000	O
0.046	O
0.000	O
0.010	O
0.043	O
0.025	O
0.026	O
0.015	O
0.011	O
0.021	O
0.002	O
0.005	O
0.363	O
rank	O
6	O
18	O
2	O
1	O
16	O
21	O
17	O
11	O
11	O
7	O
11	O
9	O
22	O
10	O
15	O
23	O
8	O
20	O
4	O
3	O
11	O
19	O
5	O
24	O
the	O
object	O
of	O
this	O
dataset	O
is	O
to	O
ﬁnd	O
a	O
fast	O
and	O
reliable	O
indicator	O
of	O
instability	O
in	O
large	O
scale	O
power	O
systems	O
.	O
the	O
dataset	O
is	O
conﬁdential	O
to	O
statlog	O
and	O
belongs	O
to	O
t.	O
van	O
cutsem	O
and	O
l.	O
wehenkel	O
,	O
university	O
of	O
li`ege	O
,	O
institut	O
monteﬁore	O
,	O
sart-tilman	O
,	O
b-4000	O
li`ege	O
,	O
belgium	O
.	O
the	O
emergency	O
control	O
of	O
voltage	O
stability	O
is	O
still	O
in	O
its	O
infancy	O
but	O
one	O
important	O
aspect	O
of	O
this	O
control	O
is	O
the	O
early	O
detection	O
of	O
critical	O
states	O
in	O
order	O
to	O
reliably	O
trigger	O
automatic	O
corrective	O
actions	O
.	O
this	O
dataset	O
has	O
been	O
constructed	O
by	O
simulating	O
up	O
to	O
ﬁve	O
minutes	O
of	O
the	O
system	O
behaviour	O
.	O
basically	O
,	O
a	O
case	O
is	O
labelled	O
stable	O
if	O
all	O
voltages	O
controlled	O
by	O
on-load	O
tap	O
changers	O
are	O
successfully	O
brought	O
back	O
to	O
their	O
set-point	O
values	O
.	O
otherwise	O
,	O
the	O
system	O
becomes	O
unstable	O
.	O
there	O
are	O
2500	O
examples	O
of	O
stable	O
and	O
unstable	O
states	O
each	O
with	O
28	O
attributes	O
which	O
in-	O
volve	O
measurements	O
of	O
voltage	O
magnitudes	O
,	O
active	O
and	O
reactive	O
power	O
ﬂows	O
and	O
injections	O
.	O
statistical	B
algorithms	O
can	O
not	O
be	O
run	O
on	O
datasets	O
which	O
have	O
linearly	O
dependent	O
attributes	O
and	O
there	O
are	O
7	O
such	O
attributes	O
(	O
x18	O
,	O
x19	O
,	O
x20	O
,	O
x21	O
,	O
x23	O
,	O
x27	O
,	O
x28	O
)	O
in	O
the	O
belgian	O
power	O
dataset	O
.	O
these	O
have	O
to	O
be	O
removed	O
when	O
running	O
the	O
classical	O
statistical	B
algorithms	O
.	O
no	O
other	O
form	O
of	O
pre-processing	O
was	O
done	O
to	O
this	O
dataset	O
.	O
train	O
and	O
test	O
sets	O
have	O
1250	O
û	O
{	O
	O
164	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
kohonen	O
map	O
-	O
belgian	O
power	O
data	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
1	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
1	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
1	O
2ë	O
2ë	O
2ë	O
2ë	O
1	O
1	O
2ë	O
2ë	O
2ë	O
2ë	O
1	O
1	O
2ë	O
2ë	O
1	O
1	O
1	O
1	O
2ë	O
1	O
1	O
1	O
1	O
1	O
2ë	O
1	O
1	O
1	O
1	O
1	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
1	O
1	O
1	O
2ë	O
1	O
2ë	O
1	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
1	O
1	O
1	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
1	O
1	O
1	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
1	O
1	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
1	O
1	O
1	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
2ë	O
1	O
2ë	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
fig	O
.	O
9.6	O
:	O
kohonen	O
map	O
of	O
the	O
belgian	O
power	O
data	O
,	O
showing	O
potential	O
clustering	O
.	O
both	O
classes	O
1	O
and	O
2	O
appear	O
to	O
have	O
two	O
distinct	O
clusters	O
.	O
examples	O
each	O
and	O
single	O
train-and-test	O
is	O
used	O
for	O
the	O
classiﬁcation	B
.	O
the	O
statistical	B
algorithms	O
smart	O
and	O
logdisc	O
produced	O
results	O
which	O
are	O
signiﬁcantly	O
better	O
than	O
the	O
other	O
algorithms	O
tested	O
on	O
this	O
dataset	O
.	O
logdisc	O
is	O
approximately	O
50	O
times	O
quicker	O
at	O
training	O
than	O
smart	O
and	O
still	O
produced	O
an	O
error	O
rate	O
of	O
less	O
than	O
1	O
%	O
.	O
dipol92	O
also	O
gives	O
a	O
fairly	O
low	O
error	O
rate	O
and	O
is	O
not	O
time	O
consuming	O
to	O
run	O
.	O
k-nn	O
was	O
confused	O
by	O
irrelevant	O
attributes	O
,	O
and	O
a	O
variable	O
selection	O
option	O
reduced	O
the	O
error	O
rate	O
to	O
3.4	O
%	O
.	O
the	O
kohonen	O
map	O
of	O
this	O
data	O
may	O
help	O
to	O
understand	O
this	O
dataset	O
.	O
the	O
clustering	O
apparent	O
in	O
fig	O
.	O
9.5.5	O
shows	O
,	O
for	O
example	B
,	O
that	O
there	O
may	O
be	O
two	O
distinct	O
types	O
of	O
“	O
stable	O
state	O
”	O
(	O
denoted	O
by	O
2	O
)	O
.	O
the	O
decision	O
trees	O
did	O
not	O
do	O
so	O
well	O
here	O
.	O
it	O
is	O
interesting	O
that	O
the	O
smallest	O
tree	O
was	O
produced	O
by	O
cal5	O
,	O
with	O
9	O
nodes	O
,	O
and	O
the	O
largest	O
tree	O
was	O
produced	O
by	O
newid	O
with	O
129	O
nodes	O
,	O
and	O
yet	O
the	O
error	O
rates	O
are	O
very	O
similar	O
at	O
2.9	O
%	O
and	O
2.7	O
%	O
,	O
respectively	O
.	O
information	O
about	O
class	O
clusters	O
can	O
be	O
incorporated	O
directly	O
into	O
the	O
dipol92	O
model	O
and	O
helps	O
to	O
produce	O
more	O
accurate	O
results	O
.	O
there	O
is	O
a	O
more	O
technical	B
description	O
of	O
this	O
dataset	O
in	O
van	O
cutsem	O
et	O
al	O
.	O
(	O
1991	O
)	O
.	O
9.5.6	O
belgian	O
power	O
ii	O
(	O
belgii	O
)	O
this	O
dataset	O
is	O
drawn	O
from	O
a	O
larger	O
simulation	O
than	O
the	O
one	O
which	O
produced	O
the	O
belgian	O
power	O
dataset	O
.	O
the	O
objective	O
remains	O
to	O
ﬁnd	O
a	O
fast	O
and	O
reliable	O
indicator	O
of	O
instability	O
in	O
large	O
scale	O
power	O
systems	O
.	O
this	O
dataset	O
is	O
also	O
conﬁdential	O
and	O
belongs	O
to	O
the	O
university	O
sec	O
.	O
9.5	O
]	O
miscellaneous	O
data	O
165	O
table	O
9.27	O
:	O
results	O
for	O
the	O
belgian	O
power	O
ii	O
dataset	O
(	O
2	O
classes	O
,	O
57	O
attributes	O
,	O
(	O
train	O
,	O
test	O
)	O
=	O
(	O
2000	O
,	O
1000	O
)	O
observations	O
)	O
.	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
max	O
.	O
algorithm	O
storage	O
75	O
75	O
1087	O
882	O
185	O
129	O
80	O
232	O
1036	O
624	O
3707	O
968	O
852	O
4708	O
1404	O
291	O
103	O
585	O
154	O
148	O
*	O
194	O
*	O
baytree	O
naivebay	O
cn2	O
c4.5	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
default	O
time	O
(	O
sec	O
.	O
)	O
train	O
107.5	O
516.8	O
336.0	O
11421.3	O
6238.4	O
408.5	O
9.5	O
467.9	O
349.5	O
131.0	O
3864.0	O
83.7	O
54.9	O
967.0	O
184.0	O
9024.1	O
62.1	O
*	O
95.4	O
4315.0	O
*	O
1704.0	O
*	O
test	O
9.3	O
211.8	O
43.6	O
3.1	O
*	O
103.4	O
4.3	O
11.8	O
335.2	O
0.5	O
92.0	O
11.8	O
12.5	O
28.0	O
18.0	O
17.9	O
9.8	O
*	O
13.1	O
1.0	O
*	O
50.8	O
*	O
error	O
rate	O
test	O
0.041	O
0.035	O
0.028	O
0.013	O
0.045	O
0.052	O
0.064	O
0.022	O
0.014	O
0.017	O
0.019	O
0.014	O
0.089	O
0.025	O
0.018	O
0.081	O
0.026	O
0.084	O
0.026	O
0.022	O
0.035	O
0.065	O
0.070	O
train	O
0.048	O
0.015	O
0.031	O
0.010	O
0.057	O
0.000	O
0.062	O
0.022	O
0.004	O
0.000	O
0.000	O
0.000	O
0.087	O
0.000	O
0.008	O
0.080	O
0.037	O
0.061	O
0.030	O
0.021	O
0.037	O
0.018	O
0.076	O
rank	O
15	O
13	O
12	O
1	O
16	O
17	O
18	O
7	O
2	O
4	O
6	O
2	O
23	O
9	O
5	O
21	O
10	O
22	O
10	O
7	O
13	O
19	O
20	O
of	O
li`ege	O
and	O
electricit`e	O
de	O
france	O
.	O
the	O
training	O
set	O
consists	O
of	O
2000	O
examples	O
with	O
57	O
attributes	O
.	O
the	O
test	O
set	O
contains	O
1000	O
examples	O
and	O
there	O
are	O
two	O
classes	O
.	O
no	O
pre-processing	O
was	O
done	O
and	O
one-shot	O
train-and-test	O
was	O
used	O
to	O
calculate	O
the	O
accuracy	O
.	O
as	O
for	O
the	O
previous	O
belgian	O
power	O
dataset	O
,	O
smart	O
comes	O
out	O
top	O
in	O
terms	O
of	O
test	O
error	O
rate	O
(	O
although	O
it	O
takes	O
far	O
longer	O
to	O
run	O
than	O
the	O
other	O
algorithms	O
considered	O
here	O
)	O
.	O
logdisc	O
hasn	O
’	O
t	O
done	O
so	O
well	O
on	O
this	O
larger	O
dataset	O
.	O
k-nn	O
was	O
again	O
confused	O
by	O
irrelevant	O
attributes	O
,	O
and	O
a	O
variable	O
selection	O
option	O
reduced	O
the	O
error	O
rate	O
to	O
2.2	O
%	O
.	O
the	O
machine	O
there	O
is	O
a	O
detailed	O
description	O
of	O
this	O
dataset	O
and	O
related	O
results	O
in	O
wehenkel	O
et	O
al	O
.	O
(	O
1993	O
)	O
.	O
9.5.7	O
machine	B
faults	I
(	O
faults	O
)	O
due	O
to	O
the	O
conﬁdential	O
nature	O
of	O
the	O
problem	O
,	O
very	O
little	O
is	O
known	O
about	O
this	O
dataset	O
.	O
it	O
was	O
donated	O
to	O
the	O
project	O
by	O
the	O
software	O
company	O
isoft	O
,	O
chemin	O
de	O
moulon	O
,	O
f-91190	O
learning	O
algorithms	O
indcart	O
,	O
newid	O
,	O
û	O
results	O
.	O
the	O
tree	O
sizes	O
here	O
were	O
more	O
similar	O
withû	O
and	O
newid	O
using	O
37	O
nodes	O
.	O
naive	O
bayes	O
is	O
worst	O
and	O
along	O
with	O
kohonen	O
and	O
itrule	O
give	O
poorer	O
results	O
than	O
the	O
default	O
rule	O
for	O
the	O
test	O
set	O
error	O
rate	O
(	O
0.074	O
)	O
.	O
	O
,	O
baytree	O
and	O
c4.5	O
give	O
consistently	O
good	O
	O
using	O
36	O
nodes	O
,	O
c4.5	O
25	O
nodes	O
,	O
û	O
{	O
	O
{	O
{	O
166	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
table	O
9.28	O
:	O
results	O
for	O
the	O
machine	B
faults	I
dataset	O
(	O
3	O
classes	O
,	O
45	O
attributes	O
,	O
570	O
observa-	O
tions	O
,	O
10-fold	O
cross-validation	O
)	O
.	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
max	O
.	O
algorithm	O
storage	O
457	O
299	O
406	O
105	O
129	O
87	O
176	O
164	O
672	O
*	O
826	O
596	O
484	O
1600	O
700	O
75	O
197	O
188	O
52	O
147	O
332	O
72	O
*	O
baytree	O
naivebay	O
cn2	O
c4.5	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
default	O
time	O
(	O
sec	O
.	O
)	O
train	O
51.1	O
46.0	O
67.6	O
13521.0	O
802.4	O
260.7	O
350.3	O
90.6	O
36.7	O
*	O
265.0	O
8.6	O
3.3	O
69.2	O
6.3	O
42.1	O
472.8	O
*	O
54.0	O
3724.6	O
58.6	O
90.6	O
*	O
test	O
6.8	O
8.4	O
6.2	O
*	O
*	O
5.2	O
17.3	O
0.9	O
37.2	O
*	O
9.0	O
1.8	O
0.4	O
7.8	O
1.7	O
1.8	O
1.2	O
*	O
10.0	O
0.0	O
12.0	O
2.3	O
*	O
error	O
rate	O
test	O
0.204	O
0.293	O
0.221	O
0.339	O
0.339	O
0.375	O
0.318	O
0.318	O
0.335	O
0.304	O
0.174	O
0.283	O
0.274	O
0.354	O
0.305	O
0.330	O
0.297	O
0.472	O
0.191	O
0.228	O
0.320	O
0.444	O
0.610	O
train	O
0.140	O
0.107	O
0.122	O
0.101	O
0.341	O
0.376	O
0.254	O
0.244	O
0.156	O
0.000	O
0.000	O
0.003	O
0.232	O
0.000	O
0.125	O
0.331	O
0.231	O
0.193	O
0.120	O
0.028	O
0.102	O
0.019	O
0.610	O
rank	O
3	O
8	O
4	O
17	O
17	O
20	O
12	O
12	O
16	O
10	O
1	O
7	O
6	O
19	O
11	O
15	O
9	O
22	O
2	O
5	O
14	O
21	O
23	O
gif	O
sur	O
yvette	O
,	O
france	O
.	O
the	O
only	O
information	O
known	O
about	O
the	O
dataset	O
is	O
that	O
it	O
involves	O
the	O
ﬁnancial	O
aspect	O
of	O
mechanical	O
maintenance	O
and	O
repair	O
.	O
the	O
aim	O
is	O
to	O
evaluate	O
the	O
cost	O
of	O
repairing	O
damaged	O
entities	O
.	O
the	O
original	O
dataset	O
had	O
multiple	O
attribute	O
values	O
and	O
a	O
few	O
errors	O
.	O
this	O
was	O
processed	O
to	O
split	O
the	O
15	O
attributes	O
into	O
45.	O
the	O
original	O
train	O
and	O
test	O
sets	O
supplied	O
by	O
isoft	O
were	O
concatenated	O
and	O
the	O
examples	O
permuted	O
randomly	O
to	O
form	O
a	O
dataset	O
with	O
570	O
examples	O
.	O
the	O
pre-processing	O
of	O
hierarchical	O
data	O
is	O
discussed	O
further	O
in	O
section	O
7.4.5.	O
there	O
are	O
45	O
numerical	O
attributes	O
and	O
3	O
classes	O
and	O
classiﬁcation	B
was	O
done	O
using	O
10-fold	O
cross-validation	O
.	O
this	O
is	O
the	O
only	O
hierarchical	O
dataset	O
studied	O
here	O
.	O
compared	O
with	O
the	O
other	O
algorithms	O
,	O
trials	O
were	O
done	O
on	O
the	O
original	O
dataset	O
whereas	O
the	O
other	O
algorithms	O
on	O
the	O
project	O
used	O
a	O
transformed	O
dataset	O
because	O
they	O
can	O
not	O
handle	O
dataset	O
was	O
preprocessed	O
in	O
order	O
that	O
other	O
algorithms	O
could	O
handle	O
the	O
dataset	O
.	O
this	O
preprocessing	B
was	O
done	O
without	O
loss	O
of	O
information	O
on	O
the	O
attributes	O
,	O
but	O
the	O
hierarchy	O
between	O
attributes	O
was	O
destroyed	O
.	O
the	O
dataset	O
of	O
this	O
application	O
has	O
been	O
designed	O
to	O
run	O
	O
gives	O
the	O
best	O
error	O
rate	O
.	O
theû	O
	O
.	O
in	O
other	O
words	O
,	O
this	O
datasets	O
expressed	O
in	O
the	O
knowledge	O
representation	O
language	O
ofû	O
	O
,	O
thus	O
all	O
the	O
knowledge	O
entered	O
has	O
been	O
used	O
by	O
the	O
program	O
.	O
this	O
explains	O
(	O
in	O
withû	O
	O
and	O
underlines	O
the	O
importance	O
of	O
structuring	O
the	O
knowledge	O
part	O
)	O
the	O
performance	O
ofû	O
û	O
{	O
	O
û	O
{	O
{	O
	O
{	O
{	O
{	O
sec	O
.	O
9.5	O
]	O
miscellaneous	O
data	O
167	O
for	O
an	O
application	O
.	O
although	O
this	O
result	O
is	O
of	O
interest	O
,	O
it	O
was	O
not	O
strictly	O
a	O
fair	O
comparison	O
,	O
	O
used	O
domain-speciﬁc	O
knowledge	O
which	O
the	O
other	O
algorithms	O
did	O
not	O
(	O
and	O
for	O
the	O
most	O
part	O
,	O
could	O
not	O
)	O
.	O
in	O
addition	O
,	O
it	O
should	O
be	O
pointed	O
out	O
that	O
the	O
cross-validation	O
involved	O
a	O
different	O
splitting	O
method	O
that	O
preserved	O
the	O
class	O
proportions	O
,	O
so	O
this	O
will	O
also	O
bias	O
the	O
result	O
somewhat	O
.	O
the	O
size	O
of	O
the	O
tree	O
produced	O
by	O
is	O
340	O
nodes	O
,	O
whereas	O
cal5	O
and	O
newid	O
used	O
trees	O
with	O
33	O
nodes	O
and	O
111	O
nodes	O
,	O
sinceû	O
procedure	O
used	O
withû	O
respectively	O
.	O
kohonen	O
gives	O
the	O
poorest	O
result	O
which	O
is	O
surprising	O
as	O
this	O
neural	O
net	O
algorithm	O
should	O
do	O
better	O
on	O
datasets	O
with	O
nearly	O
equal	O
class	O
numbers	O
.	O
it	O
is	O
interesting	O
to	O
compare	O
this	O
with	O
the	O
results	O
for	O
k-nn	O
.	O
the	O
algorithm	O
should	O
work	O
well	O
on	O
all	O
datasets	O
on	O
which	O
any	O
algorithm	O
similar	O
to	O
the	O
nearest–neighbour	O
algorithm	O
(	O
or	O
a	O
classical	O
cluster	O
analysis	O
)	O
works	O
well	O
.	O
the	O
fact	O
the	O
k-nn	O
performs	O
badly	O
on	O
this	O
dataset	O
suggests	O
that	O
kohonen	O
will	O
too	O
.	O
9.5.8	O
tsetse	B
ﬂy	I
distribution	I
(	O
tsetse	O
)	O
zimbabwe	O
tsetse	O
fly	O
distribution	O
e	O
d	O
u	O
t	O
t	O
a	O
l	O
i	O
6	O
1	O
-	O
8	O
1	O
-	O
0	O
2	O
-	O
2	O
2	O
-	O
+++	O
+++	O
+	O
--	O
-	O
--	O
-	O
-	O
+++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
+++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
+++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
+++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
+++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
+++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
+++++++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
++++++++++++++++++++++++++	O
++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
+++++++++++++++++++++++	O
++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
+++	O
+++++++++++++++++++++++	O
+++++++++++++++++++	O
--	O
-	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
+++++++++++++++++++++++++++++++	O
+++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
++++++++	O
++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
++++++++++++++++++++++++++++++	O
+++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
+++++++++++++++++++++++++++++++	O
++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
+++++++	O
++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
+++++++	O
+++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
+++++++	O
+++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
++++++++++++++++++++++++++++++	O
+++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
+++++++++++++++++++++++++++++++	O
+++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
+++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
+++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
++++++++++++++++++++++++++++++++++++	O
+++++++	O
--	O
--	O
--	O
-	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
+++++++++++++++++++++++++++++++++++++++	O
++++++++	O
+++++	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
+++++++++++++++++	O
+++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
+++++++++++++++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
++++++++++++++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
++++++++++++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
+++++++++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
+++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
+++++++++++++	O
++++++	O
++++++	O
++++++	O
++++++	O
++++++	O
++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
+++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
+++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
+++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
+++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
+++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
+++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
+++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
+++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
++++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
+++++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
+++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
+++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
++++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
+++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
++++++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
++++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
+++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
++++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
++++++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
++++++++++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
++++++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
+++++++++++++++++	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
-	O
+++	O
--	O
-	O
+++++++++	O
+	O
26	O
28	O
30	O
longitude	O
32	O
fig	O
.	O
9.7	O
:	O
tsetse	O
map	O
:	O
the	O
symbols	O
“	O
+	O
”	O
and	O
“	O
-	O
”	O
denote	O
the	O
presence	O
and	O
absence	O
of	O
tsetse	O
ﬂies	O
respectively	O
.	O
tsetse	O
ﬂies	O
are	O
one	O
of	O
the	O
most	O
prevalent	O
insect	O
hosts	O
spreading	O
disease	O
(	O
namely	O
tripanoso-	O
miasis	O
)	O
from	O
cattle	O
to	O
humans	O
in	O
africa	O
.	O
in	O
order	O
to	O
limit	O
the	O
spread	O
of	O
disease	O
it	O
is	O
of	O
interest	O
to	O
predict	O
the	O
distribution	O
of	O
ﬂies	O
and	O
types	O
of	O
environment	O
to	O
which	O
they	O
are	O
best	O
suited	O
.	O
the	O
tsetse	O
dataset	O
contains	O
interpolated	O
data	O
contributed	O
by	O
csiro	O
division	O
of	O
for-	O
restry	O
,	O
australia	O
(	O
booth	O
et	O
al.	O
,	O
1990	O
)	O
and	O
was	O
donated	O
by	O
trevor	O
h.	O
booth	O
,	O
po	O
box	O
4008	O
,	O
{	O
{	O
	O
û	O
{	O
	O
168	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
queen	O
victoria	O
terrace	O
,	O
canberra	O
,	O
act	O
2600	O
,	O
australia	O
.	O
tsetse	O
ﬁles	O
were	O
eradicated	O
from	O
most	O
of	O
zimbabwe	O
but	O
a	O
map	O
of	O
presence/absence	O
was	O
constructed	O
before	O
any	O
eradication	O
programme	O
and	O
this	O
provides	O
the	O
classiﬁed	O
examples	O
.	O
for	O
a	O
total	O
of	O
4999	O
squares	O
of	O
side	O
7km	O
,	O
data	O
has	O
been	O
collected	O
from	O
maps	O
,	O
climatic	O
databases	O
and	O
remotely	O
sensed	O
information	O
.	O
the	O
main	O
interest	O
is	O
in	O
the	O
environmental	O
conditions	O
under	O
which	O
the	O
tsetse	O
ﬂy	O
thrives	O
and	O
the	O
dataset	O
used	O
here	O
consisted	O
of	O
14	O
attributes	O
related	O
to	O
this	O
(	O
shown	O
below	O
)	O
.	O
the	O
2	O
classes	O
are	O
presence	O
or	O
absence	O
of	O
ﬂies	O
and	O
the	O
classiﬁcation	B
was	O
done	O
using	O
one-shot	O
train-and-test	O
.	O
the	O
training	O
set	O
had	O
3500	O
examples	O
and	O
the	O
test	O
set	O
had	O
1499.	O
both	O
had	O
roughly	O
equal	O
numbers	O
in	O
both	O
classes	O
.	O
all	O
attribute	O
values	O
are	O
numeric	O
and	O
indicated	O
below	O
.	O
the	O
original	O
data	O
had	O
measure-	O
ments	O
of	O
latitude	O
and	O
longitude	O
as	O
attributes	O
which	O
were	O
used	O
to	O
construct	O
the	O
map	O
.	O
these	O
attributes	O
were	O
dropped	O
as	O
the	O
purpose	O
is	O
to	O
identify	O
the	O
environmental	O
conditions	O
suitable	O
for	O
ﬂies	O
.	O
elevation	O
annual	O
average	O
nvdi	O
vegetation	O
index	O
nvdi	O
vegetation	O
index	O
for	O
february	O
nvdi	O
vegetation	O
index	O
for	O
september	O
max	O
-	O
min	O
nvdi	O
index	O
annual	O
evaporation	O
annual	O
rainfall	O
max	O
of	O
monthly	O
mean	O
temperature	O
maxima	O
max	O
of	O
monthly	O
mean	O
temperature	O
mean	O
of	O
monthly	O
means	O
min	O
of	O
monthly	O
means	O
minima	O
min	O
of	O
monthly	O
means	O
max	O
of	O
monthly	O
mean	O
temperature	O
maxima	O
-	O
min	O
of	O
monthly	O
means	O
minima	O
the	O
machine	O
learning	O
algorithms	O
produce	O
the	O
best	O
(	O
cn2	O
)	O
and	O
worst	O
(	O
itrule	O
)	O
results	O
for	O
	O
all	O
give	O
rise	O
to	O
number	O
of	O
months	O
with	O
temperatureÿ	O
15.3	O
degrees	O
this	O
dataset	O
.	O
the	O
decision	O
tree	O
algorithms	O
c4.5	O
,	O
cart	O
,	O
newid	O
andû	O
fairly	O
accurate	O
classiﬁcation	B
rules	O
.	O
the	O
modern	O
statistical	B
algorithms	O
,	O
smart	O
,	O
alloc80	O
and	O
k-nn	O
do	O
signiﬁcantly	O
better	O
than	O
the	O
classical	O
statistical	B
algorithms	O
(	O
discrim	O
,	O
quadisc	O
and	O
logdisc	O
)	O
.	O
with	O
a	O
variable	O
selection	O
procedure	O
k-nn	O
obtains	O
an	O
error	O
rate	O
of	O
3.8	O
%	O
,	O
again	O
indicating	O
some	O
unhelpful	O
attributes	O
.	O
similar	O
work	O
has	O
been	O
done	O
on	O
this	O
dataset	O
by	O
booth	O
et	O
al	O
.	O
(	O
1990	O
)	O
and	O
ripley	O
(	O
1993	O
)	O
the	O
dataset	O
used	O
by	O
ripley	O
was	O
slightly	O
different	O
in	O
that	O
the	O
attributes	O
were	O
normalised	O
to	O
be	O
in	O
the	O
range	O
[	O
0,1	O
]	O
over	O
the	O
whole	O
dataset	O
.	O
also	O
,	O
the	O
train	O
and	O
test	O
sets	O
used	O
in	O
the	O
classiﬁcation	B
were	O
both	O
samples	O
of	O
size	O
500	O
taken	O
from	O
the	O
full	O
dataset	O
,	O
which	O
explains	O
the	O
less	O
accurate	O
results	O
achieved	O
.	O
for	O
example	B
,	O
linear	O
discriminants	O
had	O
an	O
error	O
rate	O
of	O
13.8	O
%	O
,	O
an	O
algorithm	O
similar	O
to	O
smart	O
had	O
10.2	O
%	O
,	O
1-nearest	O
neighbour	O
had	O
8.4	O
%	O
and	O
backprop	O
had	O
8.4	O
%	O
.	O
the	O
best	O
results	O
for	O
lvq	O
was	O
9	O
%	O
and	O
for	O
tree	O
algorithms	O
an	O
error	O
rate	O
of	O
10	O
%	O
was	O
reduced	O
to	O
9.6	O
%	O
on	O
pruning	B
.	O
however	O
,	O
the	O
conclusions	O
of	O
both	O
studies	O
agree	O
.	O
the	O
nearest	O
neighbour	O
and	O
lvq	O
algorithms	O
work	O
well	O
(	O
although	O
they	O
provide	O
no	O
explanation	O
of	O
the	O
structure	O
in	O
the	O
dataset	O
)	O
.	O
{	O
sec	O
.	O
9.6	O
]	O
measures	O
169	O
table	O
9.29	O
:	O
results	O
for	O
the	O
tsetse	O
dataset	O
(	O
2	O
classes	O
,	O
14	O
attributes	O
,	O
(	O
train	O
,	O
test	O
)	O
=	O
(	O
3500	O
,	O
1499	O
)	O
observations	O
)	O
.	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
max	O
.	O
algorithm	O
storage	O
69	O
73	O
599	O
179	O
138	O
99	O
233	O
182	O
1071	O
207	O
2365	O
979	O
811	O
6104	O
840	O
199	O
123	O
*	O
131	O
144	O
1239	O
141	O
*	O
baytree	O
naivebay	O
cn2	O
c4.5	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
default	O
time	O
(	O
sec	O
.	O
)	O
train	O
25.8	O
58.5	O
139.7	O
7638.0	O
1944.7	O
3898.8	O
458.0	O
63.5	O
*	O
49.0	O
2236.0	O
21.9	O
13.5	O
468.0	O
32.0	O
761.4	O
49.6	O
*	O
406.1	O
1196.0	O
*	O
536.5	O
*	O
test	O
3.6	O
19.7	O
21.9	O
4.0	O
*	O
276.0	O
172.3	O
3.8	O
*	O
1.0	O
173.0	O
2.6	O
2.7	O
21.0	O
4.0	O
3.4	O
2.4	O
*	O
53.3	O
2.0	O
*	O
14.0	O
*	O
error	O
rate	O
test	O
0.122	O
0.098	O
0.117	O
0.047	O
0.057	O
0.057	O
0.137	O
0.041	O
0.039	O
0.040	O
0.047	O
0.037	O
0.120	O
0.036	O
0.049	O
0.228	O
0.055	O
0.075	O
0.053	O
0.065	O
0.052	O
0.065	O
0.488	O
train	O
0.120	O
0.092	O
0.116	O
0.042	O
0.053	O
0.053	O
0.141	O
0.006	O
0.009	O
0.000	O
0.000	O
0.001	O
0.128	O
0.000	O
0.015	O
0.233	O
0.041	O
0.055	O
0.043	O
0.059	O
0.043	O
0.039	O
0.492	O
rank	O
20	O
17	O
18	O
6	O
12	O
12	O
21	O
5	O
3	O
4	O
6	O
2	O
19	O
1	O
8	O
22	O
11	O
16	O
10	O
14	O
9	O
14	O
23	O
the	O
results	O
ofû	O
that	O
the	O
tree-based	O
methods	O
provide	O
a	O
very	O
good	O
and	O
interpretable	O
ﬁt	O
can	O
be	O
seen	O
from	O
	O
,	O
cart	O
,	O
cal5	O
and	O
newid	O
.	O
similar	O
error	O
rates	O
were	O
obtained	O
forû	O
(	O
which	O
used	O
128	O
nodes	O
)	O
,	O
c4.5	O
(	O
which	O
used	O
92	O
nodes	O
)	O
and	O
newid	O
(	O
which	O
used	O
130	O
nodes	O
)	O
.	O
however	O
,	O
cal5	O
used	O
only	O
72	O
nodes	O
,	O
and	O
achieved	O
a	O
slightly	O
higher	O
error	O
rate	O
,	O
which	O
possibly	O
suggests	O
over-pruning	O
.	O
castle	O
has	O
a	O
high	O
error	O
rate	O
compared	O
with	O
the	O
other	O
algorithms	O
–	O
it	O
appears	O
to	O
use	O
only	O
one	O
attribute	O
to	O
construct	O
the	O
classiﬁcation	B
rule	O
.	O
the	O
mlp	O
result	O
(	O
backprop	O
)	O
is	O
directly	O
comparable	O
with	O
the	O
result	O
achieved	O
by	O
ripley	O
(	O
attribute	O
values	O
were	O
normalised	O
)	O
and	O
gave	O
a	O
slightly	O
better	O
result	O
(	O
error	O
rate	O
1.9	O
%	O
lower	O
)	O
.	O
however	O
,	O
the	O
overall	O
conclusion	O
is	O
the	O
same	O
in	O
that	O
mlps	O
did	O
about	O
the	O
same	O
as	O
lvq	O
and	O
nearest-neighbour	O
,	O
both	O
of	O
which	O
are	O
much	O
simpler	O
to	O
use	O
.	O
9.6	O
statistical	B
and	O
information	O
measures	O
we	O
give	O
,	O
in	O
tables	O
9.30	O
and	O
9.31	O
,	O
the	O
statistical	B
and	O
information	O
measures	O
as	O
described	O
in	O
section	O
7.3.2	O
and	O
7.3.3	O
for	O
all	O
of	O
the	O
datasets	O
.	O
as	O
the	O
calculation	O
of	O
the	O
measures	O
involved	O
substantial	O
computations	O
,	O
some	O
of	O
the	O
measures	O
were	O
calculated	O
for	O
reduced	O
datasets	O
.	O
for	O
example	B
,	O
the	O
measures	O
for	O
kl-digits	O
are	O
based	O
on	O
the	O
training	O
examples	O
only	O
.	O
the	O
following	O
notes	O
are	O
made	O
for	O
a	O
few	O
of	O
the	O
datasets	O
only	O
and	O
are	O
not	O
meant	O
to	O
be	O
û	O
{	O
	O
{	O
{	O
	O
170	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
comprehensive	O
.	O
rather	O
,	O
some	O
instructive	O
points	O
are	O
chosen	O
for	O
illustrating	O
the	O
important	O
ideas	O
contained	O
in	O
the	O
measures	O
.	O
9.6.1	O
kl-digits	O
dataset	O
the	O
dataset	O
that	O
looks	O
closest	O
to	O
being	O
normal	O
is	O
the	O
karhunen-loeve	O
version	O
of	O
digits	O
.	O
this	O
could	O
be	O
predicted	O
beforehand	O
,	O
as	O
it	O
is	O
a	O
linear	O
transformation	O
of	O
the	O
attributes	O
that	O
,	O
by	O
the	O
central	O
limit	O
theorem	O
,	O
would	O
be	O
closer	O
to	O
normal	O
than	O
the	O
original	O
.	O
because	O
there	O
are	O
very	O
many	O
attributes	O
in	O
each	O
linear	O
combination	O
,	O
the	O
kl-digits	O
dataset	O
is	O
very	O
close	O
to	O
normal	O
with	O
skewness	O
=	O
0.1802	O
,	O
and	O
kurtosis	O
=	O
2.92	O
,	O
as	O
against	O
the	O
exact	O
normal	O
values	O
of	O
skewness	O
=	O
0	O
and	O
kurtosis	O
=	O
3.0.	O
rather	O
interestingly	O
,	O
the	O
multivariate	O
kurtosis	O
statisticd	O
for	O
kl	O
digits	O
show	O
a	O
very	O
marked	O
departure	O
from	O
multivariate	O
normality	O
(	O
3.743	O
)	O
,	O
despite	O
the	O
fact	O
that	O
the	O
univariate	O
statistics	O
are	O
close	O
to	O
normal	O
(	O
e.g	O
.	O
kurtosis	O
=	O
2.920	O
)	O
.	O
this	O
is	O
not	O
too	O
surprising	O
:	O
it	O
is	O
possible	O
to	O
take	O
a	O
linear	O
transform	O
from	O
karhunen-loeve	O
space	O
back	O
to	O
the	O
original	O
highly	O
non-normal	O
dataset	O
.	O
this	O
shows	O
the	O
practical	O
desirability	O
of	O
using	O
a	O
multivariate	O
version	O
of	O
kurtosis	O
.	O
k	O
»	O
ãdì	O
the	O
kl	O
version	O
of	O
digits	O
appears	O
to	O
be	O
well	O
suited	O
to	O
quadratic	O
discriminants	O
:	O
there	O
is	O
a	O
substantial	O
difference	O
in	O
variances	O
(	O
sd	O
ratio	O
=	O
1.9657	O
)	O
,	O
while	O
at	O
the	O
same	O
time	O
the	O
distributions	O
are	O
not	O
too	O
far	O
from	O
multivariate	O
normality	O
with	O
kurtosis	O
of	O
order	O
3.	O
also	O
,	O
and	O
more	O
importantly	O
,	O
there	O
are	O
sufﬁcient	O
examples	O
that	O
the	O
many	O
parameters	O
of	O
the	O
quadratic	O
discriminants	O
can	O
be	O
estimated	O
fairly	O
accurately	O
.	O
9.6.2	O
vehicle	O
silhouettes	O
in	O
the	O
vehicle	O
dataset	O
,	O
the	O
high	O
value	O
of	O
fract2	O
=	O
0.9139	O
might	O
indicate	O
that	O
discrimination	O
could	O
be	O
based	O
on	O
just	O
two	O
discriminants	O
.	O
this	O
may	O
relate	O
to	O
the	O
fact	O
that	O
the	O
two	O
cars	O
are	O
not	O
easily	O
distinguishable	O
,	O
so	O
might	O
be	O
treated	O
as	O
one	O
(	O
reducing	O
dimensionality	O
of	O
the	O
mean	O
vectors	O
to	O
3d	O
)	O
.	O
however	O
,	O
although	O
the	O
fraction	O
of	O
discriminating	O
power	O
for	O
the	O
third	O
discriminant	O
is	O
low	O
(	O
1	O
-	O
0.9139	O
)	O
,	O
it	O
is	O
still	O
statistically	O
signiﬁcant	O
,	O
so	O
can	O
not	O
be	O
discarded	O
without	O
a	O
small	O
loss	O
of	O
discrimination	O
.	O
this	O
dataset	O
also	O
illustrates	O
that	O
using	O
mean	O
statistics	O
may	O
mask	O
signiﬁcant	O
differences	O
in	O
behaviour	O
between	O
classes	O
.	O
for	O
example	B
,	O
in	O
the	O
vehicle	O
dataset	O
,	O
for	O
some	O
of	O
the	O
populations	O
(	O
vehicle	O
types	O
1	O
and	O
2	O
)	O
,	O
mardia	O
’	O
s	O
kurtosis	O
statistic	O
is	O
not	O
signiﬁcant	O
.	O
however	O
,	O
for	O
both	O
vehicle	O
types	O
1	O
and	O
2	O
,	O
the	O
univariate	O
statistics	O
are	O
very	O
signiﬁcantly	O
low	O
,	O
indicating	O
marked	O
departure	O
from	O
normality	O
.	O
mardia	O
’	O
s	O
statistic	O
does	O
not	O
pick	O
this	O
up	O
,	O
partly	O
because	O
the	O
also	O
the	O
kl	O
version	O
appears	O
to	O
have	O
a	O
greater	O
difference	O
in	O
variances	O
(	O
sd	O
ratio	O
=1.9657	O
)	O
than	O
the	O
raw	O
digit	O
data	O
(	O
sd	O
ratio	O
=	O
1.5673	O
)	O
.	O
this	O
is	O
an	O
artefact	O
:	O
the	O
digits	O
data	O
used	O
here	O
had	O
several	O
attributes	O
with	O
zero	O
variances	O
in	O
some	O
classes	O
,	O
giving	O
rise	O
to	O
an	O
inﬁnite	O
value	O
for	O
sd	O
ratio	O
.	O
is	O
got	O
by	O
summing	O
over	O
a	O
set	O
of	O
	O
pixels	O
.	O
the	O
original	O
digits	O
data	O
,	O
with	O
256	O
attributes	O
,	O
the	O
total	O
of	O
the	O
individual	O
mutual	O
informations	O
for	O
the	O
kl	O
dataset	O
is	O
40	O
	O
0.2029	O
=	O
namely	O
16	O
0.5049	O
=	O
8.078.	O
these	O
datasets	O
are	O
ultimately	O
derived	O
from	O
the	O
same	O
dataset	O
,	O
so	O
it	O
is	O
no	O
surprise	O
that	O
these	O
totals	O
are	O
rather	O
close	O
.	O
however	O
,	O
most	O
algorithms	O
found	O
the	O
kl	O
attributes	O
more	O
informative	O
about	O
class	O
(	O
and	O
so	O
obtained	O
reduced	O
error	O
rates	O
)	O
.	O
8.116	O
,	O
and	O
this	O
ﬁgure	O
can	O
be	O
compared	O
with	O
the	O
corresponding	O
total	O
for	O
the	O
4x4	O
digit	O
dataset	O
,	O
ú	O
	O
sec	O
.	O
9.6	O
]	O
measures	O
171	O
1.0000	O
1.0000	O
1.9701	O
12.5538	O
0.9912	O
2.3012	O
0.1130	O
20	O
000	O
7	O
2	O
0	O
0	O
1.0975	O
0.1146	O
0.6109	O
dig44	O
18	O
000	O
16	O
10	O
0	O
0	O
1.5673	O
0.2119	O
0.8929	O
0.8902	O
0.2031	O
0.4049	O
0.8562	O
5.1256	O
3.3219	O
6.5452	O
0.5049	O
cred.man	O
cr.aust	O
690	O
14	O
2	O
4	O
0	O
1.2623	O
0.1024	O
0.7713	O
table	O
9.30	O
:	O
table	O
of	O
measures	O
for	O
datasets	O
kl	O
18	O
000	O
40	O
10	O
0	O
0	O
1.9657	O
0.1093	O
0.9207	O
0.9056	O
0.1720	O
0.3385	O
0.1802	O
2.9200	O
3.3219	O
5.5903	O
0.2029	O
segm	O
2310	O
11	O
7	O
0	O
0	O
4.0014	O
0.1425	O
0.9760	O
0.9623	O
0.3098	O
0.6110	O
2.9580	O
24.4813	O
2.8072	O
3.0787	O
0.6672	O
6.1012	O
93.1399	O
0.2893	O
2.7416	O
0.0495	O
letter	O
20	O
000	O
16	O
26	O
0	O
0	O
1.8795	O
0.2577	O
0.8896	O
0.8489	O
0.1680	O
0.3210	O
0.5698	O
3.5385	O
4.6996	O
3.0940	O
0.5189	O
chrom	O
40	O
000	O
16	O
24	O
0	O
0	O
1.3218	O
0.1885	O
0.9884	O
0.9191	O
0.1505	O
0.2807	O
0.4200	O
4.4024	O
4.5603	O
5.6411	O
1.3149	O
satim	O
6435	O
36	O
6	O
0	O
0	O
1.2970	O
0.5977	O
0.9366	O
0.9332	O
0.3586	O
0.7146	O
0.7316	O
4.1737	O
2.4734	O
5.5759	O
0.9443	O
vehicle	O
846	O
18	O
4	O
0	O
0	O
1.5392	O
0.4828	O
0.8420	O
0.8189	O
0.4696	O
0.9139	O
0.8282	O
5.1800	O
1.9979	O
4.2472	O
0.3538	O
cut	B
18	O
700	O
20	O
2	O
0	O
0	O
1.0320	O
0.2178	O
0.5500	O
1.0000	O
0.9012	O
3.5214	O
0.3256	O
4.6908	O
0.0292	O
n	O
p	O
k	O
bin.att	O
cost	O
sd	O
corr.abs	O
cancor1	O
cancor2	O
fract1	O
fract2	O
skewness	O
kurtosis	O
	O
c	O
	O
c1y	O
vc	O
{	O
1~	O
n	O
p	O
k	O
bin.att	O
cost	O
sd	O
corr.abs	O
cancor1	O
cancor2	O
fract1	O
fract2	O
skewness	O
kurtosis	O
	O
c	O
	O
cby	O
vc	O
{	O
h~	O
{	O
g	O
û	O
g	O
û	O
y	O
g	O
{	O
g	O
û	O
g	O
û	O
y	O
g	O
172	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
n	O
p	O
k	O
bin.att	O
cost	O
sd	O
corr.abs	O
cancor1	O
cancor2	O
fract1	O
fract2	O
skewness	O
kurtosis	O
	O
c	O
äc1y	O
v÷c	O
{	O
h~	O
table	O
9.31	O
:	O
table	O
of	O
measures	O
for	O
datasets	O
head	O
900	O
6	O
3	O
1	O
1	O
1.1231	O
0.1217	O
0.7176	O
0.1057	O
0.9787	O
1.0000	O
1.0071	O
5.0408	O
1.3574	O
1.9786	O
0.1929	O
cr.ger	O
1000	O
24	O
2	O
9	O
1	O
1.0369	O
0.0848	O
0.5044	O
heart	O
270	O
13	O
2	O
5	O
1	O
1.0612	O
0.1236	O
0.7384	O
1.0000	O
1.0000	O
1.6986	O
7.7943	O
0.8813	O
1.5031	O
0.0187	O
0.9560	O
3.6494	O
0.9902	O
1.6386	O
0.0876	O
shuttle	O
58	O
000	O
9	O
7	O
0	O
0	O
1.6067	O
0.3558	O
0.9668	O
0.6968	O
0.6252	O
0.9499	O
4.4371	O
160.3108	O
0.9653	O
3.4271	O
0.3348	O
diab	O
768	O
8	O
2	O
0	O
0	O
1.0377	O
0.1439	O
0.5507	O
1.0000	O
1.0586	O
5.8270	O
0.9331	O
4.5301	O
0.1120	O
dna	O
3186	O
180	O
3	O
180	O
0	O
1.5407	O
0.0456	O
0.8729	O
0.8300	O
0.5252	O
1.0000	O
2.5582	O
29.5674	O
1.4725	O
0.8072	O
0.0218	O
tech	O
7078	O
56	O
91	O
0	O
0	O
2.2442	O
0.9165	O
0.6818	O
0.5575	O
0.866	O
6.7156	O
108.2963	O
4.8787	O
0.3672	O
0.1815	O
belg	O
2500	O
28	O
2	O
0	O
0	O
1.5124	O
0.3503	O
0.8869	O
belgii	O
3000	O
57	O
2	O
0	O
0	O
1.0638	O
0.1216	O
0.5286	O
1.0000	O
1.0000	O
0.4334	O
2.6581	O
0.9453	O
5.4853	O
0.3172	O
1.1180	O
6.7738	O
0.3879	O
3.8300	O
0.0421	O
faults	O
570	O
45	O
3	O
43	O
0	O
1.1910	O
0.0751	O
0.8842	O
0.3002	O
0.8966	O
1.000	O
1.8972	O
6.9866	O
1.5749	O
0.8717	O
0.0366	O
tsetse	O
4999	O
14	O
2	O
0	O
0	O
1.1316	O
0.3676	O
0.7792	O
1.0000	O
0.6483	O
4.3322	O
0.9998	O
3.8755	O
0.2850	O
n	O
p	O
k	O
bin.att	O
cost	O
sd	O
corr.abs	O
cancor1	O
cancor2	O
fract1	O
fract2	O
skewness	O
kurtosis	O
	O
c	O
	O
c1y	O
vc	O
{	O
1~	O
{	O
g	O
û	O
g	O
û	O
y	O
g	O
{	O
g	O
û	O
g	O
û	O
y	O
g	O
sec	O
.	O
9.6	O
]	O
measures	O
173	O
number	O
of	O
attributes	O
is	O
fairly	O
large	O
in	O
relation	O
to	O
the	O
number	O
of	O
examples	O
per	O
class	O
,	O
and	O
partly	O
because	O
mardia	O
’	O
s	O
statistic	O
is	O
less	O
efﬁcient	O
than	O
the	O
univariate	O
statistics	O
.	O
9.6.3	O
head	B
injury	I
among	O
the	O
datasets	O
with	O
more	O
than	O
two	O
classes	O
,	O
the	O
clearest	O
evidence	O
of	O
collinearity	O
is	O
in	O
the	O
head	B
injury	I
dataset	I
.	O
here	O
the	O
second	O
canonical	O
correlation	O
is	O
not	O
statistically	O
different	O
from	O
zero	O
,	O
with	O
a	O
critical	O
level	O
ofá	O
=	O
0.074.	O
it	O
appears	O
that	O
a	O
single	O
linear	O
discriminant	O
is	O
sufﬁcient	O
to	O
discriminate	O
between	O
the	O
classes	O
(	O
more	O
precisely	O
:	O
a	O
second	O
linear	O
discriminant	O
does	O
not	O
improve	O
discrimination	O
)	O
.	O
therefore	O
the	O
head	B
injury	I
dataset	I
is	O
very	O
close	O
to	O
linearity	O
.	O
this	O
may	O
also	O
be	O
observed	O
from	O
the	O
value	O
of	O
fract1	O
=	O
0.979	O
,	O
implying	O
that	O
the	O
three	O
class	O
means	O
lie	O
close	O
to	O
a	O
straight	O
line	O
.	O
in	O
turn	O
,	O
this	O
suggests	O
that	O
the	O
class	O
values	O
reﬂect	O
some	O
underlying	O
continuum	O
of	O
severity	O
,	O
so	O
this	O
is	O
not	O
a	O
true	O
discrimination	O
problem	O
.	O
note	O
the	O
similarity	O
with	O
fisher	O
’	O
s	O
original	O
use	O
of	O
discrimination	O
as	O
a	O
means	O
of	O
ordering	O
populations	O
.	O
perhaps	O
this	O
dataset	O
would	O
best	O
be	O
dealt	O
with	O
by	O
a	O
pure	O
regression	O
technique	O
,	O
either	O
linear	O
or	O
logistic	O
.	O
if	O
so	O
,	O
manova	O
gives	O
the	O
best	O
set	O
of	O
scores	O
for	O
the	O
three	O
categories	O
of	O
injury	O
as	O
(	O
0.681	O
,	O
-0.105	O
,	O
-0.725	O
)	O
,	O
indicating	O
that	O
the	O
middle	O
group	O
is	O
slightly	O
nearer	O
to	O
category	O
3	O
than	O
1	O
,	O
but	O
not	O
signiﬁcantly	O
nearer	O
.	O
it	O
appears	O
that	O
there	O
is	O
not	O
much	O
difference	O
between	O
the	O
covariance	O
matrices	O
for	O
the	O
three	O
populations	O
in	O
the	O
head	O
dataset	O
(	O
sd	O
ratio	O
=	O
1.1231	O
)	O
,	O
so	O
the	O
procedure	O
quadratic	O
discrimination	O
is	O
not	O
expected	O
to	O
do	O
much	O
better	O
than	O
linear	O
discrimination	O
(	O
and	O
will	O
probably	O
do	O
worse	O
as	O
it	O
uses	O
many	O
more	O
parameters	O
)	O
.	O
9.6.4	O
heart	B
disease	I
the	O
leading	O
correlation	O
coefﬁcient	O
cancor1	O
=	O
0.7384	O
in	O
the	O
heart	O
dataset	O
is	O
not	O
very	O
high	O
that	O
gives	O
a	O
measure	B
of	O
predictability	O
)	O
.	O
therefore	O
the	O
discriminating	O
power	O
of	O
the	O
linear	O
discriminant	O
is	O
only	O
moderate	O
.	O
this	O
ties	O
up	O
with	O
the	O
moderate	O
success	O
of	O
linear	O
discriminants	O
for	O
this	O
dataset	O
(	O
cost	O
for	O
the	O
training	O
data	O
of	O
0.32	O
)	O
.	O
(	O
bear	O
in	O
mind	O
that	O
it	O
is	O
correlation	O
9.6.5	O
satellite	B
image	I
dataset	O
the	O
satellite	B
image	I
data	O
is	O
the	O
only	O
dataset	O
for	O
which	O
there	O
appears	O
to	O
be	O
very	O
large	O
correlations	O
between	O
the	O
attributes	O
(	O
corr.abs	O
=	O
0.5977	O
)	O
,	O
although	O
there	O
may	O
be	O
some	O
large	O
correlations	O
in	O
the	O
vehicle	O
dataset	O
(	O
but	O
not	O
too	O
many	O
presumably	O
)	O
since	O
here	O
corr.abs	O
=	O
0.4828.	O
note	O
that	O
only	O
three	O
linear	O
discriminants	O
are	O
sufﬁcient	O
to	O
separate	O
all	O
six	O
class	O
means	O
(	O
fract3	O
=	O
0.9691	O
)	O
.	O
this	O
may	O
be	O
interpreted	O
as	O
evidence	O
of	O
seriation	O
,	O
with	O
the	O
three	O
classes	O
“	O
grey	O
soil	O
”	O
,	O
“	O
damp	O
grey	O
soil	O
”	O
and	O
“	O
very	O
damp	O
grey	O
soil	O
”	O
forming	O
a	O
continuum	O
.	O
equally	O
,	O
this	O
result	O
can	O
be	O
interpreted	O
as	O
indicating	O
that	O
the	O
original	O
36	O
attributes	O
may	O
be	O
successfully	O
reduced	O
to	O
three	O
with	O
no	O
loss	O
of	O
information	O
.	O
here	O
“	O
information	O
”	O
should	O
be	O
interpreted	O
as	O
mean	O
square	O
distance	O
between	O
classes	O
,	O
or	O
equivalently	O
,	O
as	O
the	O
entropy	O
of	O
a	O
normal	O
distribution	O
.	O
is	O
0.965	O
and	O
this	O
ﬁgure	O
gives	O
an	O
effective	O
number	O
of	O
classes	O
of	O
1.952	O
,	O
which	O
is	O
approximately	O
2.	O
this	O
can	O
be	O
interpreted	O
as	O
follows	O
.	O
although	O
9.6.6	O
shuttle	B
control	I
the	O
class	O
entropy	O
c	O
\^	O
>	O
@	O
ýjd	O
{	O
g	O
³	O
	O
174	O
dataset	O
descriptions	O
and	O
results	O
[	O
ch	O
.	O
9	O
there	O
are	O
six	O
classes	O
in	O
the	O
shuttle	O
dataset	O
,	O
some	O
class	O
probabilities	O
are	O
very	O
low	O
indeed	O
:	O
so	O
low	O
,	O
in	O
fact	O
,	O
that	O
the	O
complexity	O
of	O
the	O
classiﬁcation	B
problem	O
is	O
on	O
a	O
par	O
with	O
a	O
two-class	O
problem	O
.	O
9.6.7	O
technical	B
although	O
all	O
attributes	O
are	O
nominally	O
continuous	O
,	O
there	O
are	O
very	O
many	O
zeroes	O
,	O
so	O
many	O
that	O
we	O
can	O
regard	O
some	O
of	O
the	O
attributes	O
as	O
nearly	O
constant	O
(	O
and	O
equal	O
to	O
zero	O
)	O
.	O
this	O
is	O
shown	O
0.379	O
,	O
which	O
is	O
substantially	O
less	O
than	O
one	O
bit	O
.	O
by	O
the	O
average	O
attribute	O
entropyû	O
	O
cby	O
the	O
average	O
mutual	O
informationû	O
{	O
h~	O
carried	O
by	O
each	O
attribute	O
,	O
so	O
that	O
,	O
although	O
the	O
attributes	O
contain	O
little	O
information	O
content	O
,	O
this	O
information	O
contains	O
relatively	O
little	O
noise	O
.	O
	O
0.185	O
and	O
this	O
is	O
about	O
half	O
of	O
the	O
information	O
9.6.8	O
belgian	O
power	O
ii	O
the	O
belgian	O
power	O
ii	O
dataset	O
is	O
a	O
prime	O
candidate	O
for	O
data	O
compression	O
as	O
the	O
ratio	O
of	O
noise	O
to	O
useful	O
information	O
is	O
very	O
high	O
(	O
ns.ratio	O
=	O
137.9	O
)	O
.	O
substantial	O
reduction	O
in	O
the	O
size	O
of	O
the	O
dataset	O
is	O
possible	O
without	O
affecting	O
the	O
accuracy	O
of	O
any	O
classiﬁcation	B
procedure	O
.	O
this	O
does	O
not	O
mean	O
that	O
the	O
dataset	O
is	O
“	O
noisy	O
”	O
in	O
the	O
sense	O
of	O
not	O
allowing	O
good	O
prediction	O
.	O
the	O
better	O
algorithms	O
achieve	O
an	O
error	O
rate	O
of	O
less	O
than	O
2	O
%	O
on	O
the	O
existing	O
dataset	O
,	O
and	O
would	O
achieve	O
the	O
same	O
error	O
rate	O
on	O
the	O
condensed	O
dataset	O
.	O
this	O
is	O
particularly	O
true	O
for	O
the	O
decision	O
trees	O
:	O
typically	O
they	O
use	O
only	O
a	O
small	O
number	O
of	O
attributes	O
.	O
g	O
	O
v	O
c	O
y	O
g	O
10	O
analysis	O
of	O
results	O
p.	O
b.	O
brazdil	O
(	O
1	O
)	O
and	O
r.	O
j.	O
henery	O
(	O
2	O
)	O
(	O
1	O
)	O
university	O
of	O
porto|	O
and	O
(	O
2	O
)	O
university	O
of	O
strathclyde	O
10.1	O
introduction	O
we	O
analyse	O
the	O
results	O
of	O
the	O
trials	O
in	O
this	O
chapter	O
using	O
several	O
methods	O
:	O
the	O
section	O
on	O
results	O
by	O
subject	O
areas	O
shows	O
that	O
neural	O
network	O
and	O
statistical	B
methods	O
do	O
better	O
in	O
some	O
areas	O
and	O
machine	O
learning	O
procedures	O
in	O
others	O
.	O
the	O
idea	O
is	O
to	O
give	O
some	O
indication	O
of	O
the	O
subject	O
areas	O
where	O
certain	O
methods	O
do	O
best	O
.	O
the	O
various	O
methods	O
.	O
both	O
algorithms	O
and	O
datasets	O
using	O
the	O
performance	O
(	O
error-rates	O
)	O
of	O
every	O
combination	O
multidimensional	O
scaling	O
is	O
a	O
method	O
that	O
can	O
be	O
used	O
to	O
point	O
out	O
similarities	O
in	O
algorithm	O
dataset	O
as	O
a	O
basis	O
.	O
the	O
aim	O
here	O
is	O
to	O
understand	O
the	O
relationship	O
between	O
we	O
also	O
describe	O
a	O
simple-minded	O
attempt	O
at	O
exploring	O
the	O
relationship	O
between	O
pruning	B
and	O
accuracy	O
of	O
decision	O
trees	O
.	O
a	O
principal	O
aim	O
of	O
statlog	O
was	O
to	O
relate	O
performance	O
of	O
algorithms	O
(	O
usually	O
interpreted	O
as	O
accuracy	O
or	O
error-rate	O
)	O
to	O
characteristics	O
or	O
measures	O
of	O
datasets	O
.	O
here	O
the	O
aim	O
is	O
to	O
give	O
objective	O
measures	O
describing	O
a	O
dataset	O
and	O
to	O
predict	O
how	O
well	O
any	O
given	O
algorithm	O
will	O
perform	O
on	O
that	O
dataset	O
.	O
we	O
discuss	O
several	O
ways	O
in	O
which	O
this	O
might	O
be	O
done	O
.	O
this	O
includes	O
an	O
empirical	O
study	O
of	O
performance	O
related	O
to	O
statistical	B
and	O
information-theoretic	O
measures	O
of	O
the	O
datasets	O
.	O
in	O
particular	O
,	O
one	O
of	O
the	O
learning	O
algorithms	O
under	O
study	O
(	O
c4.5	O
)	O
is	O
used	O
in	O
an	O
ingenious	O
attempt	O
to	O
predict	O
performance	O
of	O
all	O
algorithms	O
(	O
including	O
c4.5	O
!	O
)	O
from	O
the	O
measures	O
on	O
a	O
given	O
dataset	O
.	O
the	O
performance	O
of	O
an	O
algorithm	O
may	O
be	O
predicted	O
by	O
the	O
performance	O
of	O
similar	O
algorithms	O
.	O
if	O
results	O
are	O
already	O
available	O
for	O
a	O
few	O
yardstick	O
methods	O
,	O
the	O
hope	O
is	O
that	O
the	O
performance	O
of	O
other	O
methods	O
can	O
be	O
predicted	O
from	O
the	O
yardstick	O
results	O
.	O
in	O
presenting	O
these	O
analyses	O
,	O
we	O
aim	O
to	O
give	O
many	O
different	O
views	O
of	O
the	O
results	O
so	O
that	O
a	O
reasonably	O
complete	O
(	O
although	O
perhaps	O
not	O
always	O
coherent	O
)	O
picture	O
can	O
be	O
presented	O
of	O
a	O
very	O
complex	O
problem	O
,	O
namely	O
,	O
the	O
problem	O
of	O
explaining	O
why	O
some	O
algorithms	O
do	O
better	O
m	O
address	O
for	O
correspondence	O
:	O
laboratory	O
of	O
ai	O
and	O
computer	O
science	O
(	O
liacc	O
)	O
,	O
university	O
of	O
porto	O
,	O
r.	O
campo	O
alegre	O
823	O
,	O
4100	O
porto	O
,	O
portugal	O
176	O
analysis	O
of	O
results	O
[	O
ch	O
.	O
10	O
on	O
some	O
datasets	O
and	O
not	O
so	O
well	O
on	O
others	O
.	O
these	O
differing	O
analyses	O
may	O
give	O
conﬂicting	O
and	O
perhaps	O
irreconcilable	O
conclusions	O
.	O
however	O
,	O
we	O
are	O
not	O
yet	O
at	O
the	O
stage	O
where	O
we	O
can	O
say	O
that	O
this	O
or	O
that	O
analysis	O
is	O
the	O
ﬁnal	O
and	O
only	O
word	O
on	O
the	O
subject	O
,	O
so	O
we	O
present	O
all	O
the	O
facts	O
in	O
the	O
hope	O
that	O
the	O
reader	O
will	O
be	O
able	O
to	O
judge	O
what	O
is	O
most	O
relevant	O
to	O
the	O
particular	O
application	O
at	O
hand	O
.	O
10.2	O
results	O
by	O
subject	O
areas	O
to	O
begin	O
with	O
,	O
the	O
results	O
of	O
the	O
trials	O
will	O
be	O
discussed	O
in	O
subject	O
areas	O
.	O
this	O
is	O
partly	O
because	O
this	O
makes	O
for	O
easier	O
description	O
and	O
interpretation	O
,	O
but	O
,	O
more	O
importantly	O
,	O
because	O
the	O
performance	O
of	O
the	O
various	O
algorithms	O
is	O
much	O
inﬂuenced	O
by	O
the	O
particular	O
application	O
.	O
several	O
datasets	O
are	O
closely	O
related	O
,	O
and	O
it	O
is	O
easier	O
to	O
spot	O
differences	O
when	O
comparisons	O
are	O
made	O
within	O
the	O
same	O
dataset	O
type	O
.	O
so	O
we	O
will	O
discuss	O
the	O
results	O
under	O
four	O
headings	O
:	O
datasets	O
involving	O
costs	O
credit	O
risk	O
datasets	O
image	O
related	O
datasets	O
others	O
of	O
course	O
,	O
these	O
headings	O
are	O
not	O
necessarily	O
disjoint	O
:	O
one	O
of	O
our	O
datasets	O
(	O
german	O
credit	O
)	O
was	O
a	O
credit	O
dataset	O
involving	O
costs	O
.	O
the	O
feature	O
dominating	O
performance	O
of	O
algorithms	O
is	O
costs	O
,	O
so	O
the	O
german	O
credit	O
dataset	O
is	O
listed	O
under	O
the	O
cost	O
datasets	O
.	O
we	O
do	O
not	O
attempt	O
to	O
give	O
any	O
absolute	O
assessment	O
of	O
accuracies	O
,	O
or	O
average	O
costs	O
.	O
but	O
we	O
have	O
listed	O
the	O
algorithms	O
in	O
each	O
heading	O
by	O
their	O
average	O
ranking	O
within	O
this	O
heading	O
.	O
algorithms	O
at	O
the	O
top	O
of	O
the	O
table	O
do	O
well	O
,	O
on	O
average	O
,	O
and	O
algorithms	O
at	O
the	O
bottom	O
do	O
badly	O
.	O
to	O
illustrate	O
how	O
the	O
ranking	O
was	O
calculated	O
,	O
consider	O
the	O
two	O
(	O
no-cost	O
)	O
credit	O
datasets	O
.	O
because	O
,	O
for	O
example	B
,	O
cal5	O
is	O
ranked	O
1st	O
in	O
the	O
australian.credit	O
and	O
4th	O
in	O
the	O
credit	B
management	I
dataset	O
,	O
cal5	O
has	O
a	O
total	O
rank	O
of	O
5	O
,	O
which	O
is	O
the	O
smallest	O
total	O
of	O
all	O
,	O
and	O
cal5	O
is	O
therefore	O
top	O
of	O
the	O
listing	O
in	O
the	O
credit	O
datasets	O
.	O
similarly	O
,	O
dipol92	O
has	O
a	O
total	O
rank	O
of	O
7	O
,	O
and	O
so	O
is	O
2nd	O
in	O
the	O
list	O
.	O
of	O
course	O
,	O
other	O
considerations	O
,	O
such	O
as	O
memory	O
storage	O
,	O
time	O
to	O
learn	O
etc.	O
,	O
must	O
not	O
be	O
forgotten	O
.	O
in	O
this	O
chapter	O
,	O
we	O
take	O
only	O
error-rate	O
or	O
average	O
cost	O
into	O
account	O
.	O
10.2.1	O
credit	O
datasets	O
we	O
have	O
results	O
for	O
two	O
credit	O
datasets	O
.	O
in	O
two	O
of	O
these	O
,	O
the	O
problem	O
is	O
to	O
predict	O
the	O
creditworthiness	O
of	O
applicants	O
for	O
credit	O
,	O
but	O
they	O
are	O
all	O
either	O
coded	O
or	O
conﬁdential	O
to	O
a	O
greater	O
or	O
lesser	O
extent	O
.	O
so	O
,	O
for	O
example	B
,	O
we	O
do	O
not	O
know	O
the	O
exact	O
deﬁnition	O
of	O
“	O
uncreditworthy	O
”	O
or	O
“	O
bad	O
risk	O
”	O
.	O
possible	O
deﬁnitions	O
are	O
(	O
i	O
)	O
“	O
more	O
than	O
one	O
month	O
late	O
with	O
the	O
ﬁrst	O
payment	O
”	O
;	O
(	O
ii	O
)	O
“	O
more	O
than	O
two	O
months	O
late	O
with	O
the	O
ﬁrst	O
payment	O
”	O
;	O
or	O
even	O
(	O
iii	O
)	O
“	O
the	O
(	O
human	O
)	O
credit	O
manager	O
has	O
already	O
refused	O
credit	O
to	O
this	O
person	O
”	O
.	O
credit	B
management	I
.	O
credit	B
management	I
data	O
from	O
the	O
uk	O
(	O
conﬁdential	O
)	O
.	O
german	O
.	O
credit	O
risk	O
data	O
from	O
germany	O
.	O
australian	O
.	O
credit	O
risk	O
data	O
from	O
(	O
quinlan	O
,	O
1993	O
)	O
it	O
may	O
be	O
that	O
these	O
classiﬁcations	O
are	O
deﬁned	O
by	O
a	O
human	O
:	O
if	O
so	O
,	O
then	O
the	O
aim	O
of	O
the	O
decision	O
rule	O
is	O
to	O
devise	O
a	O
procedure	O
that	O
mimics	O
the	O
human	O
decision	O
process	O
as	O
closely	O
as	O
possible	O
.	O
machine	O
learning	O
procedures	O
are	O
very	O
good	O
at	O
this	O
,	O
and	O
this	O
probably	O
reﬂects	O
a	O
natural	O
sec	O
.	O
10.2	O
]	O
results	O
by	O
subject	O
areas	O
177	O
tendency	O
for	O
human	O
decisions	O
to	O
be	O
made	O
in	O
a	O
sequential	O
manner	O
.	O
it	O
is	O
then	O
correspondingly	O
easy	O
for	O
a	O
human	O
to	O
understand	O
the	O
decision	O
tree	O
methods	O
as	O
this	O
best	O
reﬂects	O
the	O
human	O
decision	O
process	O
.	O
costs	O
of	O
misclassiﬁcation	O
in	O
two	O
of	O
our	O
credit	O
datasets	O
,	O
we	O
were	O
unable	O
to	O
assess	O
either	O
the	O
prior	O
odds	O
of	O
good-bad	O
or	O
the	O
relative	O
costs	O
of	O
making	O
the	O
wrong	O
decisions	O
.	O
however	O
,	O
in	O
the	O
german	O
credit	O
data	O
,	O
we	O
were	O
given	O
an	O
independent	O
assessment	O
that	O
the	O
relative	O
cost	O
of	O
granting	O
credit	O
to	O
a	O
bad	O
risk	O
customer	O
was	O
5	O
times	O
that	O
of	O
turning	O
down	O
an	O
application	O
from	O
a	O
good	O
risk	O
customer	O
,	O
or	O
is	O
the	O
cost	O
of	O
misclassifying	O
a	O
bad	O
credit	O
risk	O
as	O
good	O
and	O
is	O
the	O
cost	O
of	O
misclassifying	O
a	O
good	O
credit	O
risk	O
as	O
bad	O
.	O
(	O
implicitly	O
,	O
we	O
assume	O
that	O
the	O
proportions	O
of	O
good-bad	O
risks	O
in	O
the	O
training	O
sample	O
reﬂect	O
those	O
in	O
the	O
population	O
)	O
.	O
also	O
,	O
in	O
the	O
credit	B
management	I
dataset	O
,	O
it	O
was	O
explicitly	O
stated	O
by	O
the	O
dataset	O
provider	O
that	O
errors	O
of	O
either	O
type	O
were	O
equally	O
important	O
-	O
a	O
statement	O
that	O
we	O
interpreted	O
to	O
mean	O
that	O
the	O
cost-ratio	O
was	O
unity	O
.	O
±	O
,	O
wherem3cö	O
mdcb¸	O
mdcb¸	O
mdcö	O
gih	O
on	O
the	O
other	O
hand	O
,	O
the	O
deﬁnition	O
of	O
“	O
bad	O
”	O
risk	O
may	O
be	O
deﬁned	O
by	O
the	O
lateness	O
of	O
payments	O
,	O
or	O
non-payment	O
.	O
the	O
task	O
here	O
is	O
to	O
assess	O
the	O
degree	O
of	O
risk	O
.	O
most	O
datasets	O
of	O
this	O
nature	O
lose	O
much	O
useful	O
information	O
by	O
binarising	O
some	O
measure	B
of	O
badness	O
.	O
for	O
example	B
,	O
a	O
customer	O
may	O
be	O
classed	O
as	O
a	O
“	O
bad	O
”	O
risk	O
if	O
the	O
ﬁrst	O
repayment	O
is	O
more	O
than	O
one	O
month	O
late	O
,	O
whereas	O
a	O
more	O
natural	O
approach	O
would	O
be	O
to	O
predict	O
the	O
number	O
of	O
months	O
before	O
the	O
ﬁrst	O
payment	O
is	O
made	O
.	O
the	O
statlog	O
versions	O
of	O
machine	O
learning	O
methods	O
were	O
not	O
generally	O
well	O
adapted	O
to	O
prediction	O
problems	O
however	O
.	O
apart	O
from	O
anything	O
else	O
,	O
prediction	O
problems	O
involve	O
some	O
cost	O
function	O
(	O
usually	O
but	O
not	O
necessarily	O
quadratic	O
)	O
:	O
the	O
important	O
point	O
is	O
that	O
some	O
errors	O
are	O
more	O
serious	O
than	O
others	O
.	O
generally	O
in	O
credit	O
risk	O
assessment	O
,	O
the	O
cost	O
of	O
misclassiﬁcation	O
is	O
a	O
vital	O
element	O
.	O
the	O
classiﬁcation	B
of	O
a	O
bad	O
credit	O
risk	O
as	O
good	O
usually	O
costs	O
more	O
than	O
classiﬁcation	B
of	O
a	O
good	O
credit	O
risk	O
as	O
bad	O
.	O
unfortunately	O
,	O
credit	O
institutes	O
can	O
not	O
give	O
precise	O
estimates	O
of	O
the	O
cost	O
of	O
misclassiﬁcation	O
.	O
on	O
the	O
other	O
hand	O
,	O
many	O
of	O
the	O
algorithms	O
in	O
this	O
study	O
can	O
not	O
use	O
a	O
cost	O
matrix	O
in	O
performing	O
the	O
classiﬁcation	B
task	O
,	O
although	O
there	O
have	O
recently	O
been	O
some	O
attempts	O
to	O
consider	O
misclassiﬁcation	O
costs	O
in	O
learning	O
algorithms	O
such	O
newid	O
and	O
c4.5	O
(	O
see	O
knoll	O
,	O
1993	O
)	O
.	O
if	O
we	O
were	O
to	O
judge	O
learning	O
algorithms	O
solely	O
on	O
the	O
basis	O
of	O
average	O
misclassiﬁcation	O
cost	O
,	O
this	O
would	O
penalise	O
the	O
ml	O
algorithms	O
.	O
in	O
some	O
of	O
the	O
datasets	O
therefore	O
,	O
we	O
used	O
the	O
average	O
error	O
rate	O
instead	O
:	O
this	O
is	O
equivalent	O
to	O
average	O
misclassiﬁcation	O
cost	O
in	O
a	O
very	O
special	O
case	O
as	O
we	O
will	O
now	O
show	O
.	O
is	O
misclassiﬁcation	O
as	O
:	O
the	O
error	O
rates	O
in	O
the	O
classiﬁcation	B
of	O
bad	O
and	O
good	O
risks	O
,	O
respectively	O
.	O
denoting	O
the	O
prior	O
is	O
the	O
cost	O
of	O
misclassifying	O
a	O
bad	O
credit	O
risk	O
as	O
good	O
andmdcb¸	O
recall	O
thatmdcö	O
g	O
andµ0c¸	O
g	O
are	O
the	O
cost	O
of	O
misclassifying	O
a	O
good	O
credit	O
risk	O
as	O
bad	O
.	O
suppose	O
also	O
thatµ0cö	O
probabilities	O
of	O
good	O
and	O
bad	O
risks	O
byü	O
'	O
;	O
andü	O
?	O
,	O
we	O
can	O
calculate	O
the	O
expected	O
cost	O
of	O
ü	O
?	O
µ0cö	O
m3cö	O
g	O
and	O
as	O
mentioned	O
above	O
,	O
in	O
practice	O
it	O
is	O
very	O
difﬁcult	O
to	O
ﬁnd	O
out	O
the	O
values	O
ofmdcö	O
g	O
(	O
see	O
for	O
example	B
srinivisan	O
&	O
sim	O
,	O
1987	O
)	O
.	O
because	O
of	O
this	O
,	O
it	O
is	O
often	O
assumed	O
that	O
ü	O
'	O
;	O
mdcb¸	O
ü	O
?	O
mdcö	O
mdc¸	O
ü	O
'	O
;	O
.µ0c¸	O
exmdcb¸	O
(	O
10.1	O
)	O
(	O
10.2	O
)	O
~	O
ö	O
~	O
¸	O
g	O
	O
~	O
¸	O
g	O
~	O
ö	O
g	O
~	O
¸	O
g	O
~	O
ö	O
g	O
9	O
	O
~	O
¸	O
g	O
g	O
~	O
ö	O
g	O
g	O
~	O
¸	O
~	O
ö	O
~	O
¸	O
g	O
~	O
ö	O
g	O
	O
178	O
analysis	O
of	O
results	O
[	O
ch	O
.	O
10	O
using	O
assumption	O
(	O
10.2	O
)	O
,	O
one	O
can	O
get	O
the	O
expected	O
misclassiﬁcation	O
cost	O
k	O
from	O
equation	O
(	O
10.1	O
)	O
(	O
10.3	O
)	O
is	O
the	O
same	O
for	O
all	O
algorithms	O
,	O
so	O
one	O
can	O
use	O
the	O
ü	O
?	O
y	O
cö	O
e	O
!	O
µ0cb¸	O
m3cö	O
total	O
error	O
rateì	O
in	O
equation	O
(	O
10.3	O
)	O
the	O
factorm3cö	O
exµ	O
µ0cö	O
c¸	O
ü	O
?	O
as	O
an	O
equivalent	O
evaluation	O
criterion	O
when	O
comparing	O
the	O
performance	O
of	O
algorithms	O
.	O
results	O
and	O
conclusions	O
table	O
10.1	O
:	O
error	O
rates	O
for	O
credit	O
datasets	O
ordered	O
by	O
their	O
average	O
rank	O
over	O
the	O
datasets	O
.	O
credit	O
cal5	O
dipol92	O
logdisc	O
smart	O
c4.5	O
indcart	O
bprop	O
discrim	O
rbf	O
baytree	O
itrule	O
ac2	O
k-nn	O
naivebay	O
castle	O
alloc80	O
cart	O
newid	O
cn2	O
lvq	O
kohonen	O
quadisc	O
default	O
cr.aus	O
cr.man	O
0.023	O
0.131	O
0.020	O
0.141	O
0.141	O
0.030	O
0.020	O
0.158	O
0.022	O
0.155	O
0.025	O
0.152	O
0.154	O
0.023	O
0.033	O
0.141	O
0.031	O
0.145	O
0.028	O
0.171	O
0.137	O
0.046	O
0.030	O
0.181	O
0.031	O
0.181	O
0.043	O
0.151	O
0.148	O
0.047	O
0.201	O
0.031	O
0.145	O
0.181	O
0.204	O
0.197	O
0.033	O
0.032	O
0.040	O
0.043	O
0.050	O
0.050	O
0.207	O
0.440	O
the	O
table	O
of	O
error	O
rates	O
for	O
the	O
credit	O
datasets	O
is	O
given	O
in	O
table	O
10.1.	O
in	O
reading	O
this	O
table	O
,	O
the	O
reader	O
should	O
beware	O
that	O
:	O
not	O
much	O
can	O
be	O
inferred	O
from	O
only	O
two	O
cases	O
re	O
the	O
suitability	O
of	O
this	O
or	O
that	O
algorithm	O
for	O
credit	O
datasets	O
generally	O
;	O
in	O
real	O
credit	O
applications	O
,	O
differential	O
misclassiﬁcation	O
costs	O
tend	O
to	O
loom	O
large	O
,	O
if	O
not	O
explicitly	O
then	O
by	O
implication	O
.	O
it	O
is	O
noteworthy	O
that	O
three	O
of	O
the	O
top	O
six	O
algorithms	O
are	O
decision	O
trees	O
(	O
cal5	O
,	O
c4.5	O
and	O
indcart	O
)	O
,	O
while	O
the	O
algorithm	O
in	O
second	O
place	O
(	O
dipol92	O
)	O
is	O
akin	O
to	O
a	O
neural	O
network	O
.	O
we	O
may	O
conclude	O
that	O
decision	O
trees	O
do	O
reasonably	O
well	O
on	O
credit	O
datasets	O
.	O
this	O
conclusion	O
9	O
	O
~	O
¸	O
g	O
µ	O
g	O
g	O
	O
~	O
¸	O
g	O
ì	O
	O
g	O
g	O
sec	O
.	O
10.2	O
]	O
results	O
by	O
subject	O
areas	O
179	O
would	O
probably	O
be	O
strengthened	O
if	O
we	O
had	O
persuaded	O
cart	O
to	O
run	O
on	O
the	O
credit	B
management	I
dataset	O
,	O
as	O
it	O
is	O
likely	O
that	O
the	O
error	O
rate	O
for	O
cart	O
would	O
be	O
fairly	O
similar	O
to	O
indcart	O
’	O
s	O
value	O
,	O
and	O
then	O
cart	O
would	O
come	O
above	O
indcart	O
in	O
this	O
table	O
.	O
however	O
,	O
where	O
values	O
were	O
missing	O
,	O
as	O
is	O
the	O
case	O
with	O
cart	O
,	O
the	O
result	O
was	O
assumed	O
to	O
be	O
the	O
default	O
value	O
-	O
an	O
admittedly	O
very	O
conservative	O
procedure	O
,	O
so	O
cart	O
appears	O
low	O
down	O
in	O
table	O
10.1.	O
by	O
itself	O
,	O
the	O
conclusion	O
that	O
decision	O
trees	O
do	O
well	O
on	O
credit	O
datasets	O
,	O
while	O
giving	O
some	O
practical	O
guidance	O
on	O
a	O
speciﬁc	O
application	O
area	O
,	O
does	O
not	O
explain	O
why	O
decision	O
trees	O
should	O
be	O
successful	O
here	O
.	O
a	O
likely	O
explanation	O
is	O
that	O
both	O
datasets	O
are	O
partitioning	O
datasets	O
.	O
this	O
is	O
known	O
to	O
be	O
true	O
for	O
the	O
credit	B
management	I
dataset	O
where	O
a	O
human	O
classiﬁed	O
the	O
data	O
on	O
the	O
basis	O
of	O
the	O
attributes	O
.	O
we	O
suspect	O
that	O
it	O
holds	O
for	O
the	O
other	O
credit	O
dataset	O
also	O
,	O
in	O
view	O
of	O
the	O
following	O
facts	O
:	O
(	O
i	O
)	O
they	O
are	O
both	O
credit	O
datasets	O
;	O
(	O
ii	O
)	O
they	O
are	O
near	O
each	O
other	O
in	O
the	O
multidimensional	O
scaling	O
representation	O
of	O
all	O
datasets	O
;	O
and	O
(	O
iii	O
)	O
they	O
are	O
similar	O
in	O
terms	O
of	O
number	O
of	O
attributes	O
,	O
number	O
of	O
classes	O
,	O
presence	O
of	O
categorical	O
attributes	O
etc	O
.	O
part	O
of	O
the	O
reason	O
for	O
their	O
success	O
in	O
this	O
subject	O
area	O
is	O
undoubtedly	O
that	O
decision	O
tree	O
methods	O
can	O
cope	O
more	O
naturally	O
with	O
a	O
large	O
number	O
of	O
binary	O
or	O
categorical	O
attributes	O
(	O
provided	O
the	O
number	O
of	O
categories	O
is	O
small	O
)	O
.	O
they	O
also	O
incorporate	O
interaction	O
terms	O
as	O
a	O
matter	O
of	O
course	O
.	O
and	O
,	O
perhaps	O
more	O
signiﬁcantly	O
,	O
they	O
mirror	O
the	O
human	O
decision	O
process	O
.	O
10.2.2	O
image	O
datasets	O
image	O
classiﬁcation	O
problems	O
occur	O
in	O
a	O
wide	O
variety	O
of	O
contexts	O
.	O
in	O
some	O
applications	O
,	O
the	O
entire	O
image	O
(	O
or	O
an	O
object	O
in	O
the	O
image	O
)	O
must	O
be	O
classiﬁed	O
,	O
whereas	O
in	O
other	O
cases	O
the	O
classiﬁcation	B
proceeds	O
on	O
a	O
pixel-by-pixel	O
basis	O
(	O
possibly	O
with	O
extra	O
spatial	O
information	O
)	O
.	O
one	O
of	O
the	O
ﬁrst	O
problems	O
to	O
be	O
tackled	O
was	O
of	O
landsat	O
data	O
,	O
where	O
switzer	O
(	O
1980	O
,	O
1983	O
)	O
considered	O
classiﬁcation	B
of	O
each	O
pixel	O
in	O
a	O
spatial	O
context	O
.	O
a	O
similar	O
dataset	O
was	O
used	O
in	O
our	O
trials	O
,	O
whereby	O
the	O
attributes	O
(	O
but	O
not	O
the	O
class	O
)	O
of	O
neighbouring	O
pixels	O
was	O
used	O
to	O
aid	O
the	O
classiﬁcation	B
(	O
section	O
9.3.6	O
)	O
.	O
a	O
further	O
image	B
segmentation	I
problem	O
,	O
of	O
classifying	O
each	O
pixel	O
is	O
considered	O
in	O
section	O
9.3.7.	O
an	O
alternative	O
problem	O
is	O
to	O
classify	O
the	O
entire	O
image	O
into	O
one	O
of	O
several	O
classes	O
.	O
an	O
example	B
of	O
this	O
is	O
object	O
recognition	O
,	O
for	O
example	B
classifying	O
a	O
hand-written	O
character	O
(	O
section	O
9.3.1	O
)	O
,	O
or	O
a	O
remotely	O
sensed	O
vehicle	O
(	O
section	O
9.3.3	O
)	O
.	O
another	O
example	B
in	O
our	O
trials	O
is	O
the	O
classiﬁcation	B
of	O
chromosomes	B
(	O
section	O
9.3.5	O
)	O
,	O
based	O
on	O
a	O
number	O
of	O
features	O
extracted	O
from	O
an	O
image	O
.	O
there	O
are	O
different	O
“	O
levels	O
”	O
of	O
image	O
data	O
.	O
at	O
the	O
simplest	O
level	O
we	O
can	O
consider	O
the	O
grey	O
values	O
at	O
each	O
pixel	O
as	O
the	O
set	O
of	O
variables	O
to	O
classify	O
each	O
pixel	O
,	O
or	O
the	O
whole	O
image	O
.	O
our	O
trials	O
suggest	O
that	O
the	O
latter	O
are	O
not	O
likely	O
to	O
work	O
unless	O
the	O
image	O
is	O
rather	O
small	O
;	O
for	O
ô	O
;	O
	O
example	B
classifying	O
a	O
hand-written	O
number	O
on	O
the	O
basis	O
off	O
most	O
of	O
our	O
algorithms	O
.	O
the	O
pixel	O
data	O
can	O
be	O
further	O
processed	O
to	O
yield	O
a	O
sharper	O
image	O
,	O
or	O
other	O
information	O
which	O
is	O
still	O
pixel-based	O
,	O
for	O
example	B
a	O
gradient	O
ﬁlter	O
can	O
be	O
used	O
to	O
extract	O
edges	O
.	O
a	O
more	O
promising	O
approach	O
to	O
classify	O
images	O
is	O
to	O
extract	O
and	O
select	O
appropriate	O
features	O
and	O
the	O
vehicle	O
silhouette	O
(	O
section	O
9.3.3	O
)	O
and	O
chromosome	O
(	O
section	O
9.3.5	O
)	O
datasets	O
are	O
of	O
this	O
type	O
.	O
the	O
issue	O
of	O
extracting	O
the	O
right	O
features	O
is	O
a	O
harder	O
problem	O
.	O
the	O
temptation	O
is	O
to	O
measure	B
everything	O
which	O
may	O
be	O
useful	O
but	O
additional	O
information	O
which	O
is	O
not	O
relevant	O
may	O
spoil	O
the	O
performance	O
of	O
a	O
classiﬁer	B
.	O
for	O
example	B
,	O
the	O
nearest	O
neighbour	O
method	O
typically	O
treats	O
all	O
variables	O
with	O
equal	O
weight	O
,	O
and	O
if	O
some	O
are	O
of	O
no	O
value	O
then	O
very	O
poor	O
results	O
can	O
occur	O
.	O
other	O
algorithms	O
are	O
more	O
robust	O
to	O
this	O
pitfall	O
.	O
ô	O
grey	O
levels	O
defeated	O
for	O
presentation	O
purposes	O
we	O
will	O
categorise	O
each	O
of	O
the	O
nine	O
image	O
datasets	O
as	O
being	O
f	O
180	O
analysis	O
of	O
results	O
[	O
ch	O
.	O
10	O
one	O
of	O
segmentation	O
or	O
object	O
recognition	O
,	O
and	O
we	O
give	O
the	O
results	O
of	O
the	O
two	O
types	O
separately	O
.	O
results	O
and	O
conclusions	O
:	O
object	O
recognition	O
table	O
10.2	O
:	O
error	O
rates	O
for	O
object	O
recognition	O
datasets	O
.	O
algorithms	O
are	O
listed	O
in	O
order	O
of	O
their	O
average	O
ranking	O
over	O
the	O
ﬁve	O
datasets	O
.	O
algorithms	O
near	O
the	O
top	O
tend	O
to	O
do	O
well	O
at	O
object	O
recognition	O
.	O
object	O
quadisc	O
k-nn	O
dipol92	O
lvq	O
alloc80	O
logdiscr	O
discrim	O
smart	O
rbf	O
baytree	O
backprop	O
cn2	O
c4.5	O
newid	O
indcart	O
cascade	O
kohonen	O
castle	O
cal5	O
cart	O
itrule	O
naivebay	O
default	O
kl	O
0.025	O
0.020	O
0.039	O
0.026	O
0.024	O
0.051	O
0.075	O
0.057	O
0.055	O
0.163	O
0.049	O
0.180	O
0.180	O
0.162	O
0.170	O
0.075	O
0.168	O
0.135	O
0.270	O
0.216	O
0.223	O
0.900	O
0.084	O
0.123	O
0.091	O
0.121	O
0.253	O
0.131	O
0.107	O
0.128	O
0.129	O
0.164	O
digits	O
vehic	O
chrom	O
letter	O
0.113	O
0.054	O
0.047	O
0.070	O
0.176	O
0.072	O
0.079	O
0.061	O
0.064	O
0.068	O
0.086	O
0.234	O
0.302	O
0.114	O
0.295	O
0.104	O
0.233	O
0.083	O
0.140	O
0.124	O
0.327	O
0.080	O
0.115	O
0.134	O
0.132	O
0.149	O
0.150	O
0.128	O
0.154	O
0.130	O
0.065	O
0.155	O
0.075	O
0.170	O
0.220	O
0.160	O
0.222	O
0.233	O
0.900	O
0.150	O
0.275	O
0.151	O
0.287	O
0.173	O
0.192	O
0.216	O
0.217	O
0.307	O
0.271	O
0.207	O
0.314	O
0.266	O
0.298	O
0.298	O
0.280	O
0.296	O
0.340	O
0.505	O
0.279	O
0.235	O
0.324	O
0.558	O
0.750	O
0.245	O
0.252	O
0.245	O
0.253	O
0.150	O
0.175	O
0.176	O
0.173	O
0.234	O
0.174	O
0.178	O
0.244	O
0.697	O
0.324	O
0.960	O
0.594	O
0.529	O
0.960	O
table	O
10.2	O
gives	O
the	O
error-rates	O
for	O
the	O
ﬁve	O
object	O
recognition	O
datasets	O
.	O
it	O
is	O
believed	O
that	O
this	O
group	O
contains	O
pure	O
discrimination	O
datasets	O
(	O
digit	O
,	O
vehicle	O
and	O
letter	B
recognition	I
)	O
.	O
on	O
these	O
datasets	O
,	O
standard	O
statistical	B
procedures	O
and	O
neural	O
networks	O
do	O
well	O
overall	O
.	O
it	O
would	O
be	O
wrong	O
to	O
draw	O
general	O
conclusions	O
from	O
only	O
ﬁve	O
datasets	O
but	O
we	O
can	O
make	O
the	O
following	O
points	O
.	O
the	O
proponents	O
of	O
backpropagation	O
claim	O
that	O
it	O
has	O
a	O
special	O
ability	O
to	O
model	O
non-linear	O
behaviour	O
.	O
some	O
of	O
these	O
datasets	O
have	O
signiﬁcant	O
non-linearity	O
and	O
it	O
is	O
true	O
that	O
backpropagation	O
does	O
well	O
.	O
however	O
,	O
in	O
the	O
case	O
of	O
the	O
digits	O
it	O
performs	O
only	O
marginally	O
better	O
than	O
quadratic	O
discriminants	O
,	O
which	O
can	O
also	O
model	O
non-linear	O
behaviour	O
,	O
and	O
in	O
the	O
case	O
of	O
the	O
vehicles	O
it	O
performs	O
signiﬁcantly	O
worse	O
.	O
when	O
one	O
considers	O
the	O
large	O
amount	O
of	O
extra	O
effort	O
required	O
to	O
optimise	O
and	O
train	O
backpropagation	O
one	O
must	O
ask	O
whether	O
it	O
really	O
offers	O
an	O
advantage	O
over	O
more	O
traditional	O
algorithms	O
.	O
ripley	O
(	O
1993	O
)	O
also	O
raises	O
some	O
important	O
points	O
on	O
the	O
use	O
and	O
claims	O
of	O
neural	O
net	O
methods	O
.	O
û	O
{	O
	O
sec	O
.	O
10.2	O
]	O
results	O
by	O
subject	O
areas	O
181	O
castle	O
performs	O
poorly	O
but	O
this	O
is	O
probably	O
because	O
it	O
is	O
not	O
primarily	O
designed	O
for	O
discrimination	O
.	O
its	O
main	O
advantage	O
is	O
that	O
it	O
gives	O
an	O
easily	O
comprehensible	O
picture	O
of	O
the	O
structure	O
of	O
the	O
data	O
.	O
it	O
indicates	O
which	O
variables	O
inﬂuence	O
one	O
another	O
most	O
strongly	O
and	O
can	O
identify	O
which	O
subset	O
of	O
attributes	O
are	O
the	O
most	O
strongly	O
connected	O
to	O
the	O
decision	O
class	O
.	O
however	O
,	O
it	O
ignores	O
weak	O
connections	O
and	O
this	O
is	O
the	O
reason	O
for	O
its	O
poor	O
performance	O
,	O
in	O
that	O
weak	O
connections	O
may	O
still	O
have	O
an	O
inﬂuence	O
on	O
the	O
ﬁnal	O
decision	O
class	O
.	O
smart	O
and	O
linear	O
discriminants	O
perform	O
similarly	O
on	O
these	O
datasets	O
.	O
both	O
of	O
these	O
work	O
with	O
linear	O
combinations	O
of	O
the	O
attributes	O
,	O
although	O
smart	O
is	O
more	O
general	O
in	O
that	O
it	O
takes	O
non-linear	O
functions	O
of	O
these	O
combinations	O
.	O
however	O
,	O
quadratic	O
discriminants	O
performs	O
rather	O
better	O
which	O
suggests	O
that	O
a	O
better	O
way	O
to	O
model	O
non-linearity	O
would	O
be	O
to	O
input	B
selected	O
quadratic	O
combinations	O
of	O
attributes	O
to	O
linear	O
discriminants	O
.	O
the	O
nearest	O
neighbour	O
algorithm	O
does	O
well	O
if	O
all	O
the	O
variables	O
are	O
useful	O
in	O
classiﬁcation	B
and	O
if	O
there	O
are	O
no	O
problems	O
in	O
choosing	O
the	O
right	O
scaling	O
.	O
raw	O
pixel	O
data	O
such	O
as	O
the	O
satellite	O
data	O
and	O
the	O
hand-written	B
digits	I
satisfy	O
these	O
criteria	O
.	O
if	O
some	O
of	O
the	O
variables	O
are	O
misleading	O
or	O
unhelpful	O
then	O
a	O
variable	O
selection	O
procedure	O
should	O
precede	O
classiﬁcation	B
.	O
the	O
algorithm	O
used	O
here	O
was	O
not	O
efﬁcient	O
in	O
cpu	O
time	O
,	O
since	O
no	O
condensing	O
was	O
used	O
.	O
results	O
from	O
ripley	O
(	O
1993	O
)	O
indicate	O
that	O
condensing	O
does	O
not	O
greatly	O
affect	O
the	O
classiﬁcation	B
performance	O
.	O
paired	O
comparison	O
on	O
digits	O
data	O
:	O
kl	O
and	O
the	O
4x4	O
digits	O
data	O
represent	O
different	O
preprocessed	O
versions	O
of	O
one	O
and	O
the	O
same	O
original	O
dataset	O
.	O
not	O
unexpectedly	O
,	O
there	O
is	O
a	O
high	O
correlation	O
between	O
the	O
error-rates	O
(	O
0.944	O
with	O
two	O
missing	O
values	O
:	O
cart	O
and	O
kohonen	O
on	O
kl	O
)	O
.	O
of	O
much	O
more	O
interest	O
is	O
the	O
fact	O
that	O
the	O
statistical	B
and	O
neural	O
net	O
procedures	O
perform	O
much	O
better	O
on	O
the	O
kl	O
version	O
than	O
on	O
the	O
4x4	O
version	O
.	O
on	O
the	O
other	O
hand	O
,	O
machine	O
learning	O
methods	O
perform	O
rather	O
poorly	O
on	O
the	O
4x4	O
version	O
and	O
do	O
even	O
worse	O
on	O
the	O
kl	O
version	O
.	O
it	O
is	O
rather	O
difﬁcult	O
to	O
account	O
for	O
this	O
phenomenon	O
.	O
ml	O
methods	O
,	O
by	O
their	O
nature	O
,	O
do	O
not	O
seem	O
to	O
cope	O
with	O
situations	O
where	O
the	O
information	O
is	O
spread	O
over	O
a	O
large	O
number	O
of	O
variables	O
.	O
by	O
construction	O
,	O
the	O
karhunen-loeve	O
dataset	O
deliberately	O
creates	O
variables	O
that	O
are	O
linear	O
combinations	O
of	O
the	O
original	O
pixel	O
gray	O
levels	O
,	O
with	O
the	O
ﬁrst	O
variable	O
containing	O
“	O
most	O
”	O
information	O
,	O
the	O
second	O
variable	O
containing	O
the	O
maximum	O
information	O
orthogonal	O
to	O
the	O
ﬁrst	O
,	O
etc..	O
from	O
one	O
point	O
of	O
view	O
therefore	O
,	O
the	O
ﬁrst	O
16	O
kl	O
attributes	O
contain	O
more	O
information	O
than	O
the	O
complete	O
set	O
of	O
16	O
attributes	O
in	O
the	O
4x4	O
digit	O
dataset	O
(	O
as	O
the	O
latter	O
is	O
a	O
particular	O
set	O
of	O
linear	O
combinations	O
of	O
the	O
original	O
data	O
)	O
,	O
and	O
the	O
improvement	O
in	O
error	O
rates	O
of	O
the	O
statistical	B
procedures	O
is	O
consistent	O
with	O
this	O
interpretation	O
.	O
results	O
and	O
conclusions	O
:	O
segmentation	O
table	O
10.3	O
gives	O
the	O
error	O
rates	O
for	O
the	O
four	O
segmentation	O
problems	O
.	O
machine	O
learning	O
procedures	O
do	O
fairly	O
well	O
in	O
segmentation	O
datasets	O
,	O
and	O
traditional	O
statistical	B
methods	O
do	O
very	O
badly	O
.	O
the	O
probable	O
explanation	O
is	O
that	O
these	O
datasets	O
originate	O
as	O
partitioning	O
problems	O
.	O
paired	O
comparison	O
of	O
cut20	O
and	O
cut50	O
:	O
the	O
dataset	O
cut20	O
consists	O
of	O
the	O
ﬁrst	O
20	O
attributes	O
in	O
the	O
cut50	O
dataset	O
ordered	O
by	O
importance	O
in	O
a	O
stepwise	O
regression	O
procedure	O
.	O
one	O
would	O
therefore	O
expect	O
,	O
and	O
generally	O
one	O
observes	O
,	O
that	O
performance	O
deteriorates	O
when	O
the	O
number	O
of	O
attributes	O
is	O
decreased	O
(	O
so	O
that	O
the	O
information	O
content	O
is	O
decreased	O
)	O
.	O
one	O
exception	O
to	O
this	O
rule	O
is	O
quadratic	O
discrimination	O
which	O
does	O
badly	O
in	O
the	O
cut20	O
dataset	O
and	O
even	O
worse	O
in	O
the	O
cut50	O
data	O
.	O
this	O
is	O
the	O
converse	O
of	O
the	O
paired	O
comparison	O
in	O
the	O
digits	O
182	O
analysis	O
of	O
results	O
[	O
ch	O
.	O
10	O
table	O
10.3	O
:	O
error	O
rates	O
for	O
segmentation	O
datasets	O
.	O
algorithms	O
are	O
listed	O
in	O
order	O
of	O
their	O
average	O
ranking	O
over	O
the	O
four	O
datasets	O
.	O
algorithms	O
near	O
the	O
top	O
tend	O
to	O
do	O
well	O
in	O
image	B
segmentation	I
problems	O
.	O
segment	O
alloc80	O
baytree	O
k-nn	O
dipol92	O
c4.5	O
newid	O
cn2	O
indcart	O
lvq	O
rbf	O
backprop	O
cal5	O
smart	O
logdisc	O
cart	O
kohonen	O
discrim	O
castle	O
quadisc	O
default	O
naivebay	O
itrule	O
cascade	O
satim	O
segm	O
cut20	O
cut50	O
0.132	O
0.034	O
0.035	O
0.147	O
0.027	O
0.094	O
0.036	O
0.111	O
0.150	O
0.035	O
0.038	O
0.150	O
0.030	O
0.150	O
0.037	O
0.138	O
0.040	O
0.105	O
0.121	O
0.038	O
0.041	O
0.139	O
0.037	O
0.151	O
0.039	O
0.159	O
0.157	O
0.054	O
0.163	O
0.037	O
0.138	O
0.179	O
0.171	O
0.194	O
0.155	O
0.760	O
0.287	O
0.030	O
0.033	O
0.077	O
0.039	O
0.040	O
0.034	O
0.043	O
0.045	O
0.046	O
0.069	O
0.054	O
0.062	O
0.052	O
0.031	O
0.109	O
0.040	O
0.067	O
0.116	O
0.112	O
0.157	O
0.857	O
0.265	O
0.455	O
0.050	O
0.050	O
0.061	O
0.097	O
0.060	O
0.112	O
0.084	O
0.037	O
0.034	O
0.036	O
0.045	O
0.036	O
0.039	O
0.042	O
0.040	O
0.041	O
0.044	O
0.043	O
0.045	O
0.047	O
0.063	O
0.046	O
0.050	O
0.050	O
0.061	O
0.088	O
0.060	O
0.077	O
0.082	O
0.163	O
dataset	O
:	O
it	O
appears	O
that	O
algorithms	O
that	O
are	O
already	O
doing	O
badly	O
on	O
the	O
most	O
informative	O
set	O
of	O
attributes	O
do	O
even	O
worse	O
when	O
the	O
less	O
informative	O
attributes	O
are	O
added	O
.	O
similarly	O
,	O
machine	O
learning	O
methods	O
do	O
better	O
on	O
the	O
cut50	O
dataset	O
,	O
but	O
there	O
is	O
a	O
surprise	O
:	O
they	O
use	O
smaller	O
decision	O
trees	O
to	O
achieve	O
greater	O
accuracy	O
.	O
this	O
must	O
mean	O
that	O
some	O
of	O
the	O
“	O
less	O
signiﬁcant	O
”	O
attributes	O
contribute	O
to	O
the	O
discrimination	O
by	O
means	O
of	O
interactions	O
(	O
or	O
non-linearities	O
)	O
.	O
here	O
the	O
phrase	O
“	O
less	O
signiﬁcant	O
”	O
is	O
used	O
in	O
a	O
technical	B
sense	O
,	O
referring	O
to	O
the	O
least	O
informative	O
attributes	O
in	O
linear	O
discriminants	O
.	O
clearly	O
attributes	O
that	O
have	O
little	O
information	O
for	O
linear	O
discriminants	O
may	O
have	O
considerable	O
value	O
for	O
other	O
procedures	O
that	O
are	O
capable	O
of	O
incorporating	O
interactions	O
and	O
non-linearities	O
directly	O
.	O
k-nn	O
is	O
best	O
for	O
images	O
perhaps	O
the	O
most	O
striking	O
result	O
in	O
the	O
images	O
datasets	O
is	O
the	O
performance	O
of	O
k-nearest	O
neighbour	O
,	O
with	O
four	O
outright	O
top	O
places	O
and	O
two	O
runners-up	O
.	O
it	O
would	O
seem	O
that	O
,	O
in	O
terms	O
of	O
error-rate	O
,	O
best	O
results	O
in	O
image	O
data	O
are	O
obtained	O
by	O
k-nearest	O
neighbour	O
.	O
û	O
{	O
	O
sec	O
.	O
10.2	O
]	O
10.2.3	O
datasets	O
with	O
costs	O
results	O
by	O
subject	O
areas	O
183	O
there	O
are	O
two	O
medical	O
datasets	O
and	O
one	O
credit	O
dataset	O
in	O
this	O
section	O
.	O
these	O
are	O
illustrative	O
of	O
the	O
application	O
areas	O
where	O
costs	O
are	O
important	O
.	O
there	O
are	O
two	O
ways	O
in	O
which	O
algorithms	O
can	O
incorporate	O
costs	O
into	O
a	O
decision	O
rule	O
:	O
at	O
the	O
learning	O
stage	O
or	O
during	O
the	O
test	O
stage	O
.	O
most	O
statistical	B
procedures	O
are	O
based	O
on	O
estimates	O
of	O
probabilities	O
,	O
and	O
incorporate	O
costs	O
only	O
at	O
the	O
ﬁnal	O
test	O
stage	O
(	O
in	O
evaluating	O
the	O
expected	O
cost	O
of	O
misclassiﬁcation	O
)	O
.	O
however	O
,	O
some	O
procedures	O
can	O
incorporate	O
costs	O
into	O
the	O
learning	O
stage	O
.	O
one	O
simple	O
way	O
to	O
do	O
this	O
might	O
be	O
to	O
give	O
extra	O
weight	O
to	O
observations	O
from	O
classes	O
with	O
high	O
costs	O
of	O
misclassiﬁcation	O
.	O
results	O
and	O
conclusions	O
table	O
10.4	O
:	O
average	O
costs	O
for	O
datasets	O
with	O
cost	O
matrices	O
.	O
algorithms	O
are	O
listed	O
in	O
order	O
of	O
their	O
average	O
ranking	O
over	O
the	O
three	O
datasets	O
.	O
algorithms	O
near	O
the	O
bottom	O
can	O
not	O
cope	O
with	O
costs	O
.	O
costs	O
discrim	O
logdisc	O
castle	O
quadisc	O
alloc80	O
cart	O
naivebay	O
smart	O
cal5	O
dipol92	O
k-nn	O
cascade	O
backprop	O
baytree	O
indcart	O
default	O
itrule	O
lvq	O
cn2	O
newid	O
kohonen	O
rbf	O
c4.5	O
head	O
19.890	O
17.960	O
20.870	O
20.060	O
31.900	O
20.380	O
23.950	O
21.810	O
33.260	O
26.520	O
35.300	O
19.500	O
21.530	O
22.690	O
25.520	O
44.100	O
37.610	O
46.580	O
53.550	O
56.870	O
53.640	O
63.100	O
82.600	O
heart	O
cr.ger	O
0.535	O
0.393	O
0.538	O
0.396	O
0.441	O
0.583	O
0.619	O
0.422	O
0.584	O
0.407	O
0.613	O
0.452	O
0.374	O
0.703	O
0.601	O
0.478	O
0.603	O
0.444	O
0.599	O
0.507	O
0.478	O
0.694	O
0.467	O
0.574	O
0.526	O
0.630	O
0.560	O
0.515	O
0.600	O
0.767	O
0.744	O
0.844	O
0.693	O
0.781	O
0.781	O
0.772	O
0.778	O
0.761	O
0.700	O
0.879	O
0.963	O
0.856	O
0.878	O
0.925	O
1.160	O
0.971	O
0.985	O
the	O
average	O
costs	O
of	O
the	O
various	O
algorithms	O
are	O
given	O
in	O
table	O
10.4.	O
there	O
are	O
some	O
surprises	O
in	O
this	O
table	O
,	O
particularly	O
relating	O
to	O
the	O
default	O
procedure	O
and	O
the	O
performance	O
of	O
most	O
machine	O
learning	O
and	O
some	O
of	O
the	O
neural	O
network	O
procedures	O
.	O
overall	O
,	O
it	O
would	O
seem	O
that	O
the	O
ml	O
procedures	O
do	O
worse	O
than	O
the	O
default	O
(	O
of	O
granting	O
credit	O
to	O
everyone	O
,	O
or	O
declaring	O
everyone	O
to	O
be	O
seriously	O
ill	O
)	O
.	O
û	O
{	O
	O
184	O
analysis	O
of	O
results	O
[	O
ch	O
.	O
10	O
10.2.4	O
other	O
datasets	O
table	O
10.5	O
:	O
error	O
rates	O
for	O
remaining	O
datasets	O
.	O
the	O
shuttle	O
error	O
rates	O
are	O
in	O
%	O
.	O
algorithms	O
are	O
listed	O
in	O
order	O
of	O
their	O
average	O
ranking	O
over	O
the	O
eight	O
datasets	O
.	O
most	O
of	O
the	O
problems	O
in	O
the	O
table	O
are	O
partitioning	O
problems	O
,	O
so	O
it	O
is	O
fairly	O
safe	O
to	O
say	O
that	O
algorithms	O
near	O
the	O
top	O
of	O
the	O
table	O
are	O
most	O
suited	O
to	O
partitioning	O
problems	O
.	O
others	O
dipol92	O
baytree	O
newid	O
indcart	O
c4.5	O
cal5	O
smart	O
logdisc	O
cn2	O
cart	O
backprop	O
rbf	O
discrim	O
quadisc	O
alloc80	O
naivebay	O
castle	O
k-nn	O
itrule	O
lvq	O
kohonen	O
default	O
belg	O
newbel	O
.026	O
.018	O
.014	O
.030	O
.027	O
.017	O
.014	O
.034	O
.019	O
.034	O
.018	O
.040	O
.026	O
.029	O
.006	O
.013	O
.028	O
.007	O
.025	O
.032	O
.022	O
.034	O
.017	O
.022	O
.035	O
.034	O
.041	O
.025	O
.035	O
.052	O
.044	O
.045	O
.089	O
.062	O
.064	O
.047	O
.052	O
.059	O
.065	O
.081	O
.065	O
.054	O
.084	O
.056	O
.362	O
.074	O
tset	O
diab	O
dna	O
faults	O
.191	O
.053	O
.283	O
.037	O
.040	O
.304	O
.335	O
.039	O
.174	O
.047	O
.305	O
.049	O
.297	O
.055	O
.047	O
.339	O
.221	O
.117	O
.354	O
.036	O
.318	O
.041	O
.065	O
.228	O
.320	O
.052	O
.204	O
.122	O
.293	O
.098	O
.057	O
.339	O
.274	O
.120	O
.318	O
.137	O
.375	O
.057	O
.228	O
.330	O
.444	O
.065	O
.472	O
.075	O
.490	O
.610	O
.224	O
.271	O
.289	O
.271	O
.276	O
.270	O
.250	O
.232	O
.223	O
.289	O
.255	O
.248	O
.243	O
.225	O
.262	O
.301	O
.262	O
.258	O
.324	O
.245	O
.272	O
.273	O
.350	O
.048	O
.095	O
.100	O
.073	O
.100	O
.076	O
.131	O
.141	O
.061	O
.095	O
.085	O
.088	O
.041	O
.059	O
.059	O
.057	O
.068	O
.072	O
.155	O
.135	O
.339	O
.480	O
shutt	O
.480	O
.020	O
.010	O
.090	O
.320	O
.100	O
.030	O
.590	O
3.830	O
.030	O
.080	O
.430	O
1.400	O
4.830	O
6.720	O
.830	O
4.500	O
3.800	O
.440	O
.410	O
.440	O
21.400	O
tech	O
.192	O
.174	O
.090	O
.095	O
.102	O
.120	O
.183	O
.366	O
.401	O
.123	O
.324	O
.391	O
.495	O
.354	O
.204	O
.261	O
.357	O
.770	O
of	O
the	O
remaining	O
datasets	O
,	O
at	O
least	O
two	O
(	O
shuttle	O
and	O
technical	B
)	O
are	O
pure	O
partitioning	O
problems	O
,	O
with	O
boundaries	O
characteristically	O
parallel	O
to	O
the	O
attribute	O
axes	O
,	O
a	O
fact	O
that	O
can	O
be	O
judged	O
from	O
plots	O
of	O
the	O
attributes	O
.	O
two	O
are	O
simulated	O
datasets	O
(	O
belgian	O
and	O
belgian	O
power	O
ii	O
)	O
,	O
and	O
can	O
be	O
described	O
as	O
somewhere	O
between	O
prediction	O
and	O
partitioning	O
.	O
the	O
aim	O
of	O
the	O
tsetse	O
dataset	O
can	O
be	O
precisely	O
stated	O
as	O
partitioning	O
a	O
map	O
into	O
two	O
regions	O
,	O
so	O
as	O
to	O
reproduce	O
a	O
given	O
partitioning	O
as	O
closely	O
as	O
possible	O
.	O
the	O
tsetse	O
dataset	O
is	O
also	O
artiﬁcial	O
insofar	O
as	O
some	O
of	O
the	O
attributes	O
have	O
been	O
manufactured	O
(	O
by	O
an	O
interpolation	O
from	O
a	O
small	O
amount	O
of	O
information	O
)	O
.	O
the	O
diabetes	B
dataset	O
is	O
a	O
prediction	O
problem	O
.	O
the	O
nature	O
of	O
the	O
other	O
datasets	O
(	O
dna	O
,	O
machine	B
faults	I
)	O
,	O
i.e	O
.	O
whether	O
we	O
are	O
dealing	O
with	O
partitioning	O
,	O
prediction	O
or	O
discrimination	O
,	O
is	O
not	O
known	O
precisely	O
.	O
results	O
and	O
conclusions	O
table	O
10.5	O
gives	O
the	O
error-rates	O
for	O
these	O
eight	O
datasets	O
.	O
it	O
is	O
perhaps	O
inappropriate	O
to	O
draw	O
general	O
conclusions	O
from	O
such	O
a	O
mixed	O
bag	O
of	O
datasets	O
.	O
however	O
,	O
it	O
would	O
appear	O
,	O
from	O
the	O
performance	O
of	O
the	O
algorithms	O
,	O
that	O
the	O
datasets	O
are	O
best	O
dealt	O
with	O
by	O
machine	O
learning	O
û	O
{	O
	O
sec	O
.	O
10.3	O
]	O
top	O
ﬁve	O
algorithms	O
185	O
or	O
neural	O
network	O
procedures	O
.	O
how	O
much	O
relevance	O
this	O
has	O
to	O
practical	O
problems	O
is	O
debatable	O
however	O
,	O
as	O
two	O
are	O
simulated	O
and	O
two	O
are	O
pure	O
partitioning	O
datasets	O
.	O
10.3	O
top	O
five	O
algorithms	O
in	O
table	O
10.6	O
we	O
present	O
the	O
algorithms	O
that	O
came	O
out	O
top	O
for	O
each	O
of	O
the	O
22	O
datasets	O
.	O
only	O
the	O
top	O
ﬁve	O
algorithms	O
are	O
quoted	O
.	O
the	O
table	O
is	O
quoted	O
for	O
reference	O
only	O
,	O
so	O
that	O
readers	O
can	O
see	O
which	O
algorithms	O
do	O
well	O
on	O
a	O
particular	O
dataset	O
.	O
the	O
algorithms	O
that	O
make	O
the	O
top	O
ﬁve	O
most	O
frequently	O
are	O
dipol92	O
(	O
12	O
times	O
)	O
,	O
alloc80	O
(	O
11	O
)	O
,	O
discrim	O
(	O
9	O
)	O
,	O
logdiscr	O
and	O
quadisc	O
(	O
8	O
)	O
,	O
but	O
not	O
too	O
much	O
should	O
be	O
made	O
of	O
these	O
ﬁgures	O
as	O
they	O
depend	O
very	O
much	O
on	O
the	O
mix	O
of	O
problems	O
used	O
.	O
table	O
10.6	O
:	O
top	O
ﬁve	O
algorithms	O
for	O
all	O
datasets	O
.	O
first	O
fourth	O
second	O
third	O
alloc80	O
quadisc	O
quadisc	O
lvq	O
dipol92	O
cascade	O
discrim	O
logdiscr	O
dataset	O
kl	O
dig44	O
satim	O
vehic	O
head	O
heart	O
belg	O
segm	O
diab	O
cr.ger	O
chrom	O
cr.aus	O
shutt	O
dna	O
tech	O
newbel	O
isoft	O
tset	O
cut20	O
cut50	O
cr.man	O
letter	O
k-nn	O
k-nn	O
k-nn	O
quadisc	O
logdiscr	O
naivebay	O
smart	O
alloc80	O
ac2	O
logdiscr	O
discrim	O
quadisc	O
cal5	O
newid	O
rbf	O
newid	O
smart	O
ac2	O
cn2	O
baytree	O
k-nn	O
smart	O
alloc80	O
dipol92	O
logdiscr	O
dipol92	O
itrule	O
baytree	O
dipol92	O
indcart	O
indcart	O
dipol92	O
baytree	O
k-nn	O
cn2	O
dipol92	O
k-nn	O
fifth	O
dipol92	O
alloc80	O
alloc80	O
bprop	O
cart	O
lvq	O
cascade	O
rbf	O
lvq	O
dipol92	O
alloc80	O
logdiscr	O
quadisc	O
discrim	O
logdiscr	O
alloc80	O
quadisc	O
discrim	O
dipol92	O
bprop	O
dipol92	O
newid	O
baytree	O
smart	O
discrim	O
rbf	O
castle	O
alloc80	O
dipol92	O
lvq	O
discrim	O
logdiscr	O
discrim	O
cn2	O
cal5	O
alloc80	O
discrim	O
ac2	O
baytree	O
discrim	O
indcart	O
c4.5	O
alloc80	O
baytree	O
c4.5	O
lvq	O
k-nn	O
dipol92	O
cart	O
quadisc	O
cn2	O
c4.5	O
c4.5	O
newid	O
bprop	O
logdiscr	O
cart	O
newid	O
alloc80	O
newid	O
c4.5	O
bprop	O
cn2	O
cal5	O
quadisc	O
table	O
10.7	O
gives	O
the	O
same	O
information	O
as	O
table	O
10.6	O
,	O
but	O
here	O
it	O
is	O
the	O
type	O
of	O
algorithm	O
(	O
statistical	B
,	O
machine	O
learning	O
or	O
neural	O
net	O
)	O
that	O
is	O
quoted	O
.	O
in	O
the	O
head	B
injury	I
dataset	I
,	O
the	O
top	O
ﬁve	O
algorithms	O
are	O
all	O
statistical	B
,	O
whereas	O
the	O
top	O
ﬁve	O
are	O
all	O
machine	O
learning	O
for	O
the	O
shuttle	O
and	O
technical	B
datasets	O
.	O
between	O
these	O
two	O
extremes	O
,	O
there	O
is	O
a	O
variety	O
.	O
table	O
10.8	O
orders	O
the	O
datasets	O
by	O
the	O
number	O
of	O
machine	O
learning	O
,	O
statistical	B
or	O
neural	O
network	O
algorithms	O
that	O
are	O
in	O
the	O
top	O
ﬁve	O
.	O
from	O
inspection	O
of	O
the	O
frequencies	O
in	O
table	O
10.8	O
,	O
it	O
appears	O
that	O
neural	O
networks	O
and	O
statistical	B
procedures	O
do	O
well	O
on	O
the	O
same	O
kind	O
of	O
datasets	O
.	O
in	O
other	O
words	O
,	O
neural	O
nets	O
tend	O
to	O
do	O
well	O
when	O
statistical	B
procedures	O
do	O
well	O
and	O
vice	O
versa	O
.	O
as	O
an	O
objective	O
measure	B
of	O
this	O
tendency	O
,	O
a	O
correspondence	O
analysis	O
can	O
be	O
used	O
.	O
correspondence	O
analysis	O
attempts	O
186	O
analysis	O
of	O
results	O
[	O
ch	O
.	O
10	O
table	O
10.7	O
:	O
top	O
ﬁve	O
algorithms	O
for	O
all	O
datasets	O
,	O
by	O
type	O
:	O
machine	O
learning	O
(	O
ml	O
)	O
;	O
statistics	O
(	O
stat	O
)	O
;	O
and	O
neural	O
net	O
(	O
nn	O
)	O
.	O
dataset	O
kl	O
dig44	O
satim	O
vehic	O
head	O
heart	O
belg	O
segm	O
diab	O
cr.ger	O
chrom	O
cr.aus	O
shutt	O
dna	O
tech	O
newbel	O
isoft	O
tset	O
cut20	O
cut50	O
cr.man	O
letter	O
first	O
stat	O
stat	O
stat	O
stat	O
stat	O
stat	O
stat	O
stat	O
stat	O
stat	O
stat	O
ml	O
ml	O
nn	O
ml	O
stat	O
ml	O
ml	O
ml	O
stat	O
stat	O
stat	O
second	O
third	O
stat	O
nn	O
nn	O
stat	O
stat	O
stat	O
nn	O
ml	O
stat	O
stat	O
stat	O
stat	O
ml	O
stat	O
ml	O
ml	O
stat	O
ml	O
ml	O
stat	O
ml	O
nn	O
stat	O
stat	O
nn	O
nn	O
nn	O
stat	O
stat	O
ml	O
nn	O
stat	O
nn	O
ml	O
ml	O
nn	O
ml	O
ml	O
nn	O
ml	O
stat	O
ml	O
nn	O
stat	O
fourth	O
nn	O
nn	O
nn	O
stat	O
stat	O
stat	O
nn	O
ml	O
stat	O
stat	O
nn	O
stat	O
ml	O
stat	O
ml	O
ml	O
stat	O
ml	O
stat	O
ml	O
ml	O
stat	O
fifth	O
nn	O
stat	O
stat	O
nn	O
ml	O
stat	O
stat	O
nn	O
nn	O
nn	O
stat	O
nn	O
ml	O
stat	O
ml	O
ml	O
nn	O
ml	O
ml	O
ml	O
nn	O
ml	O
to	O
give	O
scores	O
to	O
the	O
rows	O
(	O
here	O
datasets	O
)	O
and	O
columns	O
(	O
here	O
procedure	O
types	O
)	O
of	O
an	O
array	O
with	O
positive	O
entries	O
in	O
such	O
a	O
way	O
that	O
the	O
scores	O
are	O
mutually	O
consistent	O
and	O
maximally	O
correlated	O
.	O
for	O
a	O
description	O
of	O
correspondence	O
analysis	O
,	O
see	O
hill	O
(	O
1982	O
)	O
and	O
mardia	O
et	O
al	O
.	O
(	O
1979	O
)	O
.	O
it	O
turns	O
out	O
that	O
the	O
optimal	O
scores	O
for	O
columns	O
2	O
and	O
3	O
(	O
neural	O
net	O
and	O
statistical	B
procedures	O
)	O
are	O
virtually	O
identical	O
,	O
but	O
these	O
are	O
quite	O
different	O
from	O
the	O
score	O
of	O
column	O
1	O
(	O
the	O
ml	O
procedures	O
)	O
.	O
it	O
would	O
appear	O
therefore	O
that	O
neural	O
nets	O
are	O
more	O
similar	O
to	O
statistical	B
procedures	O
than	O
to	O
ml	O
.	O
in	O
passing	O
we	O
may	O
note	O
that	O
the	O
optimal	O
scores	O
that	O
are	O
given	O
to	O
the	O
datasets	O
may	O
be	O
used	O
to	O
give	O
an	O
ordering	O
to	O
the	O
datasets	O
,	O
and	O
this	O
ordering	O
can	O
be	O
understood	O
as	O
a	O
measure	B
of	O
how	O
suited	O
the	O
dataset	O
is	O
to	O
ml	O
procedures	O
.	O
if	O
the	O
same	O
scores	O
are	O
allocated	O
to	O
neural	O
net	O
and	O
statistical	B
procedures	O
,	O
the	O
corresponding	O
ordering	O
of	O
the	O
datasets	O
is	O
exactly	O
that	O
given	O
in	O
the	O
table	O
,	O
with	O
datasets	O
at	O
the	O
bottom	O
being	O
more	O
of	O
type	O
ml	O
.	O
10.3.1	O
dominators	O
it	O
is	O
interesting	O
to	O
note	O
that	O
some	O
algorithms	O
always	O
do	O
better	O
than	O
the	O
default	O
(	O
among	O
the	O
datasets	O
we	O
have	O
looked	O
at	O
)	O
.	O
there	O
are	O
nine	O
such	O
:	O
discrim	O
,	O
logdisc	O
,	O
smart	O
,	O
k-nn	O
,	O
alloc80	O
,	O
cart	O
,	O
cal5	O
,	O
dipol92	O
and	O
cascade	O
.	O
these	O
algorithms	O
“	O
dominate	O
”	O
the	O
default	O
strategy	O
.	O
also	O
,	O
in	O
the	O
seven	O
datasets	O
on	O
which	O
cascade	O
was	O
run	O
,	O
itrule	O
is	O
dominated	O
by	O
cascade	O
.	O
the	O
only	O
other	O
case	O
of	O
an	O
algorithm	O
being	O
dominated	O
by	O
others	O
is	O
kohonen	O
:	O
it	O
sec	O
.	O
10.4	O
]	O
multidimensional	O
scaling	O
187	O
table	O
10.8	O
:	O
datasets	O
ordered	O
by	O
algorithm	O
type	O
.	O
datasets	O
at	O
the	O
top	O
are	O
most	O
suited	O
to	O
statistical	B
and	O
neural	O
net	O
procedures	O
:	O
datasets	O
at	O
the	O
bottom	O
most	O
suited	O
to	O
machine	O
learning	O
.	O
dataset	O
ml	O
nn	O
stat	O
heart	O
5	O
4	O
cr.ger	O
3	O
kl	O
3	O
dig44	O
vehic	O
3	O
3	O
belg	O
3	O
diab	O
3	O
chrom	O
3	O
dna	O
satim	O
2	O
3	O
head	O
3	O
letter	O
2	O
isoft	O
cr.aus	O
2	O
1	O
cr.man	O
2	O
cut20	O
2	O
cut50	O
segm	O
1	O
1	O
newbel	O
0	O
shutt	O
0	O
tech	O
tset	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
2	O
2	O
3	O
3	O
3	O
4	O
5	O
5	O
5	O
0	O
1	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
3	O
1	O
1	O
2	O
1	O
2	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
is	O
dominated	O
by	O
dipol92	O
,	O
cascade	O
and	O
lvq	O
.	O
these	O
comparisons	O
do	O
not	O
include	O
datasets	O
where	O
results	O
is	O
missing	O
(	O
na	O
)	O
,	O
so	O
we	O
should	O
really	O
say	O
:	O
“	O
where	O
results	O
are	O
available	O
,	O
kohonen	O
is	O
always	O
worse	O
than	O
dipol92	O
and	O
lvq	O
”	O
.	O
since	O
we	O
only	O
have	O
results	O
for	O
7	O
cascade	O
trials	O
,	O
the	O
comparison	O
cascade-kohonen	O
is	O
rather	O
meaningless	O
.	O
10.4	O
multidimensional	O
scaling	O
it	O
would	O
be	O
possible	O
to	O
combine	O
the	O
results	O
of	O
all	O
the	O
trials	O
to	O
rank	O
the	O
algorithms	O
by	O
overall	O
success	O
rate	O
or	O
average	O
success	O
rate	O
,	O
but	O
not	O
without	O
some	O
rather	O
arbitrary	O
assumptions	O
to	O
equate	O
error	O
rates	O
with	O
costs	O
.	O
we	O
do	O
not	O
attempt	O
to	O
give	O
such	O
an	O
ordering	O
,	O
as	O
we	O
believe	O
that	O
this	O
is	O
not	O
proﬁtable	O
.	O
we	O
prefer	O
to	O
give	O
a	O
more	O
objective	O
approach	O
based	O
on	O
multidimensional	O
scaling	O
(	O
an	O
equivalent	O
procedure	O
would	O
be	O
correspondence	O
analysis	O
)	O
.	O
in	O
so	O
doing	O
,	O
the	O
aim	O
is	O
to	O
demonstrate	O
the	O
close	O
relationships	O
between	O
the	O
algorithms	O
,	O
and	O
,	O
at	O
the	O
same	O
time	O
,	O
the	O
close	O
similarities	O
between	O
many	O
of	O
the	O
datasets	O
.	O
multidimensional	O
scaling	O
has	O
no	O
background	O
theory	O
:	O
it	O
is	O
an	O
exploratory	O
tool	O
for	O
suggesting	O
relationships	O
in	O
data	O
rather	O
than	O
testing	O
pre-chosen	O
hypotheses	O
.	O
there	O
is	O
no	O
agreed	O
criterion	O
which	O
tells	O
us	O
if	O
the	O
scaling	O
is	O
successful	O
,	O
although	O
there	O
are	O
generally	O
accepted	O
guidelines	O
.	O
188	O
analysis	O
of	O
results	O
[	O
ch	O
.	O
10	O
10.4.1	O
scaling	O
of	O
algorithms	O
to	O
apply	O
multidimensional	O
scaling	O
,	O
the	O
ﬁrst	O
problem	O
,	O
paradoxically	O
,	O
is	O
to	O
scale	O
the	O
variables	O
.	O
the	O
idea	O
is	O
to	O
scale	O
the	O
error-rates	O
and	O
average	O
costs	O
in	O
such	O
a	O
way	O
that	O
each	O
dataset	O
carries	O
equal	O
weight	O
.	O
this	O
is	O
not	O
easy	O
.	O
in	O
each	O
dataset	O
,	O
we	O
rescaled	O
so	O
that	O
the	O
error-rate	O
(	O
or	O
average	O
cost	O
)	O
had	O
a	O
minimum	O
of	O
zero	O
and	O
a	O
maximum	O
of	O
unity	O
.	O
such	O
a	O
rescaling	O
is	O
arbitrary	O
,	O
and	O
can	O
only	O
be	O
justiﬁed	O
a	O
posteriori	O
,	O
insofar	O
as	O
the	O
results	O
conﬁrm	O
known	O
relationships	O
.	O
once	O
the	O
initial	O
scaling	O
has	O
been	O
done	O
,	O
distances	O
between	O
all	O
pairs	O
of	O
algorithms	O
must	O
be	O
computed	O
.	O
distance	O
was	O
taken	O
to	O
be	O
the	O
euclidean	O
distance	O
between	O
the	O
rescaled	O
error-rates	O
on	O
the	O
22	O
datasets	O
.	O
this	O
results	O
in	O
a	O
distance	O
matrix	O
representing	O
distances	O
between	O
all	O
pairs	O
of	O
algorithms	O
in	O
23-dimensional	O
space	O
.	O
the	O
distance	O
matrix	O
can	O
then	O
be	O
decomposed	O
,	O
by	O
an	O
orthogonal	O
decomposition	O
,	O
into	O
distances	O
in	O
a	O
reduced	O
space	O
.	O
most	O
conveniently	O
,	O
the	O
dimensions	O
of	O
the	O
reduced	O
space	O
are	O
chosen	O
to	O
be	O
two	O
,	O
so	O
that	O
the	O
algorithms	O
can	O
be	O
represented	O
as	O
points	O
in	O
a	O
2-dimensional	O
plot	O
.	O
this	O
plot	O
is	O
given	O
in	O
figure	O
10.1.	O
multidimensional	O
scaling	O
of	O
23	O
algorithms	O
(	O
22	O
datasets	O
)	O
cascade	O
logdiscr	O
discrim	O
quadisc	O
dipol92	O
smart	O
alloc80	O
bprop	O
knn	O
cart	O
baytree	O
lvq	O
rbf	O
indcart	O
kohonen	O
castle	O
0	O
.	O
1	O
e	O
t	O
i	O
a	O
n	O
d	O
r	O
o	O
o	O
c	O
g	O
n	O
i	O
l	O
a	O
c	O
s	O
d	O
n	O
o	O
c	O
e	O
s	O
.	O
5	O
0	O
0	O
0	O
.	O
.	O
5	O
0	O
-	O
cn2	O
newid	O
.	O
0	O
1	O
-	O
c4.5	O
cal5	O
ac2	O
naivebay	O
itrule	O
-0.5	O
0.0	O
0.5	O
1.0	O
1.5	O
first	O
scaling	O
coordinate	O
fig	O
.	O
10.1	O
:	O
multidimensional	O
scaling	O
representation	O
of	O
algorithms	O
in	O
the	O
22-dimensional	O
space	O
(	O
each	O
dimension	O
is	O
an	O
error	O
rate	O
or	O
average	O
cost	O
measured	O
on	O
a	O
given	O
dataset	O
)	O
.	O
points	O
near	O
to	O
each	O
other	O
in	O
this	O
2-d	O
plot	O
are	O
not	O
necessarily	O
close	O
in	O
22-d.	O
whether	O
the	O
2-dimensional	O
plot	O
is	O
a	O
good	O
picture	O
of	O
22-dimensional	O
space	O
can	O
be	O
judged	O
from	O
a	O
comparison	O
of	O
the	O
set	O
of	O
distances	O
in	O
2-d	O
compared	O
to	O
the	O
set	O
of	O
distances	O
in	O
22-d.	O
one	O
simple	O
way	O
to	O
measure	B
the	O
goodness	O
of	O
the	O
representation	O
is	O
to	O
compare	O
the	O
total	O
squared	O
distances	O
.	O
letîï	O
be	O
the	O
total	O
of	O
the	O
squared	O
distances	O
taken	O
over	O
all	O
pairs	O
of	O
points	O
in	O
the	O
2-dimensional	O
plot	O
,	O
and	O
letîñïï	O
be	O
the	O
total	O
squared	O
distances	O
over	O
all	O
pairs	O
of	O
points	O
in	O
22-dimensions	O
.	O
the	O
“	O
stress	O
”	O
ð	O
is	O
deﬁned	O
to	O
beñòþîï7ó¢îï=ï	O
.	O
for	O
figure	O
10.1	O
the	O
“	O
stress	O
”	O
ﬁgure	O
is	O
0.266.	O
considering	O
the	O
number	O
of	O
initial	O
dimensions	O
is	O
very	O
high	O
,	O
this	O
is	O
a	O
reasonably	O
small	O
“	O
stress	O
”	O
,	O
although	O
we	O
should	O
say	O
that	O
,	O
conventionally	O
,	O
the	O
“	O
stress	O
”	O
is	O
í	O
í	O
sec	O
.	O
10.4	O
]	O
multidimensional	O
scaling	O
189	O
said	O
to	O
be	O
small	O
when	O
less	O
than	O
0.05.	O
with	O
a	O
3-dimensional	O
representation	O
,	O
the	O
stress	O
factor	O
would	O
be	O
0.089	O
,	O
indicating	O
that	O
it	O
would	O
be	O
more	O
sensible	O
to	O
think	O
of	O
algorithms	O
differing	O
in	O
at	O
least	O
3-dimensions	O
.	O
a	O
three-dimensional	O
representation	O
would	O
raise	O
the	O
prospect	O
of	O
representing	O
all	O
results	O
in	O
terms	O
of	O
three	O
scaling	O
coordinates	O
which	O
might	O
be	O
interpretable	O
as	O
error-rates	O
of	O
three	O
(	O
perhaps	O
notional	O
)	O
algorithms	O
.	O
because	O
the	O
stress	O
ﬁgure	O
is	O
low	O
relative	O
to	O
the	O
number	O
of	O
dimensions	O
,	O
points	O
near	O
each	O
other	O
in	O
figure	O
10.1	O
probably	O
represent	O
algorithms	O
that	O
are	O
similar	O
in	O
performance	O
.	O
for	O
example	B
,	O
the	O
machine	O
learning	O
methods	O
cn2	O
,	O
newid	O
and	O
indcart	O
are	O
very	O
close	O
to	O
each	O
other	O
,	O
and	O
in	O
general	O
,	O
all	O
the	O
machine	O
learning	O
procedures	O
are	O
close	O
in	O
figure	O
10.1.	O
before	O
jumping	O
to	O
the	O
conclusion	O
that	O
they	O
are	O
indeed	O
similar	O
,	O
it	O
is	O
as	O
well	O
to	O
check	O
the	O
tables	O
of	O
results	O
(	O
although	O
the	O
stress	O
is	O
low	O
,	O
it	O
is	O
not	O
zero	O
so	O
the	O
distances	O
in	O
figure	O
10.1	O
are	O
approximate	O
only	O
)	O
.	O
looking	O
at	O
the	O
individual	O
tables	O
,	O
the	O
reader	O
should	O
see	O
that	O
,	O
for	O
example	B
,	O
cn2	O
,	O
newid	O
and	O
indcart	O
tend	O
to	O
come	O
at	O
about	O
the	O
same	O
place	O
in	O
every	O
table	O
apart	O
from	O
a	O
few	O
exceptions	O
.	O
so	O
strong	O
is	O
this	O
similarity	O
,	O
that	O
one	O
is	O
tempted	O
to	O
say	O
that	O
marked	O
deviations	O
from	O
this	O
general	O
pattern	O
should	O
be	O
regarded	O
with	O
suspicion	O
and	O
should	O
be	O
double	O
checked	O
.	O
10.4.2	O
hierarchical	O
clustering	O
of	O
algorithms	O
hierarchical	O
clustering	O
-	O
23	O
algorithms	O
(	O
22	O
datasets	O
)	O
i	O
c	O
s	O
d	O
a	O
u	O
q	O
l	O
e	O
u	O
r	O
t	O
i	O
e	O
l	O
t	O
s	O
a	O
c	O
y	O
a	O
b	O
e	O
v	O
a	O
n	O
i	O
0	O
.	O
2	O
5	O
.	O
1	O
0	O
.	O
1	O
5	O
.	O
0	O
q	O
v	O
l	O
n	O
e	O
n	O
o	O
h	O
o	O
k	O
n	O
n	O
k	O
0	O
8	O
c	O
o	O
l	O
l	O
a	O
f	O
b	O
r	O
5	O
.	O
4	O
c	O
2	O
c	O
a	O
m	O
i	O
r	O
c	O
s	O
d	O
i	O
i	O
r	O
c	O
s	O
d	O
g	O
o	O
l	O
t	O
r	O
a	O
m	O
s	O
e	O
d	O
a	O
c	O
s	O
a	O
c	O
t	O
r	O
a	O
c	O
5	O
l	O
a	O
c	O
p	O
o	O
r	O
p	O
b	O
2	O
9	O
l	O
o	O
p	O
d	O
i	O
e	O
e	O
r	O
t	O
y	O
a	O
b	O
t	O
r	O
a	O
c	O
d	O
n	O
i	O
2	O
n	O
c	O
i	O
d	O
w	O
e	O
n	O
fig	O
.	O
10.2	O
:	O
hierarchical	O
clustering	O
of	O
algorithms	O
using	O
standardised	O
error	O
rates	O
and	O
costs	O
.	O
there	O
is	O
another	O
way	O
to	O
look	O
at	O
relationships	O
between	O
the	O
algorithms	O
based	O
on	O
the	O
set	O
of	O
paired	O
distances	O
,	O
namely	O
by	O
a	O
hierarchical	O
clustering	O
of	O
the	O
algorithms	O
.	O
the	O
resulting	O
figure	O
10.2	O
does	O
indeed	O
capture	O
known	O
similarities	O
(	O
linear	O
and	O
logistic	O
discriminants	O
are	O
very	O
close	O
)	O
,	O
and	O
is	O
very	O
suggestive	O
of	O
other	O
relationships	O
.	O
it	O
is	O
to	O
be	O
expected	O
that	O
some	O
of	O
the	O
similarities	O
picked	O
up	O
by	O
the	O
clustering	O
procedure	O
190	O
analysis	O
of	O
results	O
[	O
ch	O
.	O
10	O
will	O
be	O
accidental	O
.	O
in	O
any	O
case	O
,	O
algorithms	O
should	O
not	O
be	O
declared	O
as	O
similar	O
on	O
the	O
basis	O
of	O
empirical	O
evidence	O
alone	O
,	O
and	O
true	O
understanding	O
of	O
the	O
relationships	O
will	O
follow	O
only	O
when	O
theoretical	O
grounds	O
are	O
found	O
for	O
similarities	O
in	O
behaviour	O
.	O
finally	O
,	O
we	O
should	O
say	O
something	O
about	O
some	O
dissimilarities	O
.	O
there	O
are	O
some	O
surprising	O
“	O
errors	O
”	O
in	O
the	O
clusterings	O
of	O
figure	O
10.2.	O
for	O
example	B
,	O
cart	O
and	O
indcart	O
are	O
attached	O
to	O
slightly	O
different	O
clusterings	O
.	O
this	O
is	O
a	O
major	O
surprise	O
,	O
and	O
we	O
do	O
have	O
ideas	O
on	O
why	O
this	O
is	O
indeed	O
true	O
,	O
but	O
,	O
nonetheless	O
,	O
cart	O
and	O
indcart	O
were	O
grouped	O
together	O
in	O
tables	O
10.1-10.5	O
to	O
facilitate	O
comparisons	O
between	O
the	O
two	O
.	O
10.4.3	O
scaling	O
of	O
datasets	O
the	O
same	O
set	O
of	O
re-scaled	O
error	O
rates	O
may	O
be	O
used	O
to	O
give	O
a	O
2-dimensional	O
plot	O
of	O
datasets	O
.	O
from	O
a	O
formal	O
point	O
of	O
view	O
,	O
the	O
multidimensional	O
scaling	O
procedure	O
is	O
applied	O
to	O
the	O
transpose	O
of	O
the	O
matrix	O
of	O
re-scaled	O
error	O
rates	O
.	O
the	O
default	O
algorithm	O
was	O
excluded	O
from	O
this	O
exercise	O
as	O
distances	O
from	O
this	O
to	O
the	O
other	O
algorithms	O
were	O
going	O
to	O
dominate	O
the	O
picture	O
.	O
multidimensional	O
scaling	O
of	O
22	O
datasets	O
(	O
23	O
algorithms	O
)	O
e	O
t	O
i	O
a	O
n	O
d	O
r	O
o	O
o	O
c	O
g	O
n	O
i	O
l	O
a	O
c	O
s	O
d	O
n	O
o	O
c	O
e	O
s	O
0	O
1	O
.	O
.	O
5	O
0	O
0	O
0	O
.	O
.	O
5	O
0	O
-	O
.	O
0	O
1	O
-	O
cr.aus	O
ml	O
belg	O
stat	O
isoft	O
ml	O
diab	O
stat	O
cr.ger	O
stat	O
head	O
stat	O
heart	O
stat	O
cr.man	O
nn	O
newbel	O
stat	O
dna	O
nn	O
vehic	O
stat	O
satim	O
stat	O
chrom	O
stat	O
tech	O
ml	O
shutt	O
ml	O
øcut50	O
÷cut20	O
segm	O
stat	O
ml	O
stat	O
tset	O
ml	O
letter	O
stat	O
-1.0	O
dig44	O
stat	O
kl	O
stat	O
-0.5	O
first	O
scaling	O
coordinate	O
0.0	O
0.5	O
1.0	O
fig	O
.	O
10.3	O
:	O
multidimensional	O
scaling	O
representation	O
of	O
the	O
datasets	O
in	O
23-dimensional	O
space	O
(	O
each	O
dimension	O
is	O
an	O
error	O
rate	O
and	O
cost	O
achieved	O
by	O
a	O
particular	O
algorithms	O
)	O
.	O
the	O
symbols	O
ml	O
,	O
nn	O
and	O
stat	O
below	O
each	O
dataset	O
indicate	O
which	O
type	O
of	O
algorithm	O
achieved	O
the	O
lowest	O
error-rate	O
or	O
cost	O
on	O
that	O
dataset	O
.	O
datasets	O
near	O
to	O
each	O
other	O
in	O
this	O
2-d	O
plot	O
are	O
not	O
necessarily	O
close	O
in	O
23-d.	O
figure	O
10.3	O
is	O
a	O
multidimensional	O
scaling	O
representation	O
of	O
the	O
error	O
rates	O
and	O
costs	O
given	O
in	O
tables	O
10.1-10.5.	O
each	O
dataset	O
in	O
tables	O
10.1-10.5	O
is	O
described	O
by	O
a	O
point	O
in	O
23-dimensional	O
space	O
,	O
the	O
coordinates	O
of	O
which	O
are	O
the	O
(	O
scaled	O
)	O
error	O
rates	O
or	O
costs	O
of	O
the	O
various	O
algorithms	O
.	O
to	O
help	O
visualise	O
the	O
relationships	O
between	O
the	O
points	O
(	O
datasets	O
)	O
,	O
they	O
have	O
been	O
projected	O
down	O
to	O
2-dimensions	O
in	O
such	O
a	O
way	O
as	O
to	O
preserve	O
their	O
mutual	O
ô	O
õ	O
ô	O
ö	O
ö	O
ö	O
ô	O
÷	O
ø	O
ö	O
sec	O
.	O
10.4	O
]	O
multidimensional	O
scaling	O
191	O
distances	O
as	O
much	O
as	O
possible	O
.	O
this	O
projection	O
is	O
fairly	O
successful	O
as	O
the	O
“	O
stress	O
”	O
factor	O
is	O
only	O
0.149	O
(	O
a	O
value	O
of	O
0.01	O
is	O
regarded	O
as	O
excellent	O
,	O
a	O
value	O
of	O
0.05	O
is	O
good	O
)	O
.	O
again	O
,	O
a	O
3-	O
dimensional	O
representation	O
might	O
be	O
more	O
“	O
acceptable	O
”	O
with	O
a	O
stress	O
factor	O
of	O
0.063.	O
such	O
a	O
3-d	O
representation	O
could	O
be	O
interpreted	O
as	O
saying	O
that	O
datasets	O
differ	O
in	O
three	O
essentially	O
orthogonal	O
ways	O
,	O
and	O
is	O
suggestive	O
of	O
a	O
description	O
of	O
datasets	O
using	O
just	O
three	O
measures	O
.	O
this	O
idea	O
is	O
explored	O
further	O
in	O
the	O
next	O
subsection	O
.	O
several	O
interesting	O
similarities	O
are	O
obvious	O
from	O
figure	O
10.3.	O
the	O
costs	O
datasets	O
are	O
close	O
to	O
each	O
other	O
,	O
as	O
are	O
the	O
two	O
types	O
of	O
image	O
datasets	O
.	O
in	O
addition	O
,	O
the	O
credit	O
datasets	O
are	O
all	O
at	O
the	O
top	O
of	O
the	O
diagram	O
(	O
except	O
for	O
the	O
german	O
credit	O
data	O
which	O
involves	O
costs	O
)	O
.	O
the	O
two	O
pathologically	O
partitioned	O
datasets	O
shuttle	O
and	O
technical	B
are	O
together	O
at	O
the	O
extreme	O
top	O
right	O
of	O
the	O
diagram	O
.	O
in	O
view	O
of	O
these	O
similarities	O
,	O
it	O
is	O
tempting	O
to	O
classify	O
datasets	O
of	O
unknown	O
origin	O
by	O
their	O
proximities	O
to	O
other	O
datasets	O
of	O
known	O
provenance	O
.	O
for	O
example	B
,	O
the	O
diabetes	B
dataset	O
is	O
somewhere	O
between	O
a	O
partitioning	O
type	O
dataset	O
(	O
cf	O
.	O
credit	O
data	O
)	O
and	O
a	O
prediction	O
type	O
dataset	O
(	O
cf	O
.	O
head	B
injury	I
)	O
.	O
interpretation	O
of	O
scaling	O
coordinates	O
the	O
plotting	O
coordinates	O
for	O
the	O
2-dimensional	O
description	O
of	O
datasets	O
in	O
figure	O
10.3	O
are	O
derived	O
by	O
orthogonal	O
transformation	O
of	O
the	O
original	O
error	O
rates/costs	O
.	O
these	O
coordinates	O
clearly	O
represent	O
distinctive	O
features	O
of	O
the	O
datasets	O
as	O
similar	O
datasets	O
are	O
grouped	O
together	O
in	O
the	O
diagram	O
.	O
this	O
suggests	O
either	O
that	O
the	O
scaling	O
coordinates	O
might	O
be	O
used	O
as	O
charac-	O
teristics	O
of	O
the	O
datasets	O
,	O
or	O
,	O
equivalently	O
,	O
might	O
be	O
related	O
to	O
characteristics	O
of	O
the	O
datasets	O
.	O
this	O
suggests	O
that	O
we	O
look	O
at	O
these	O
coordinates	O
and	O
try	O
to	O
relate	O
them	O
to	O
the	O
dataset	O
measures	O
that	O
we	O
deﬁned	O
in	O
chapter	O
7.	O
for	O
example	B
,	O
it	O
turns	O
out	O
that	O
the	O
ﬁrst	O
scaling	O
coordinate	O
is	O
positively	O
correlated	O
with	O
the	O
number	O
of	O
examples	O
in	O
the	O
dataset	O
.	O
in	O
figure	O
10.3	O
,	O
this	O
means	O
that	O
there	O
is	O
a	O
tendency	O
for	O
the	O
larger	O
datasets	O
to	O
lie	O
to	O
the	O
right	O
of	O
the	O
diagram	O
.	O
the	O
second	O
is	O
the	O
number	O
of	O
classes	O
.	O
this	O
implies	O
that	O
a	O
dataset	O
with	O
small	O
kurtosis	O
and	O
large	O
number	O
of	O
classes	O
will	O
tend	O
to	O
lie	O
in	O
the	O
bottom	O
half	O
of	O
figure	O
10.3.	O
however	O
,	O
the	O
correlations	O
are	O
quite	O
weak	O
,	O
and	O
in	O
any	O
case	O
only	O
relate	O
to	O
a	O
subspace	O
of	O
two	O
dimensions	O
with	O
a	O
“	O
stress	O
”	O
of	O
0.149	O
,	O
so	O
we	O
can	O
not	O
say	O
that	O
these	O
measures	O
capture	O
the	O
essential	O
differences	O
between	O
datasets	O
.	O
scaling	O
coordinate	O
is	O
correlated	O
with	O
the	O
curious	O
ratio	O
kurtosisó¢ù	O
,	O
whereù	O
that	O
particular	O
dataset	O
.	O
for	O
example	B
,	O
the	O
algorithmúüû	O
ï	O
(	O
of	O
type	O
ml	O
)	O
comes	O
out	O
top	O
on	O
the	O
10.4.4	O
best	O
algorithms	O
for	O
datasets	O
in	O
figure	O
10.3	O
,	O
each	O
dataset	O
has	O
been	O
labelled	O
by	O
the	O
type	O
of	O
algorithm	O
that	O
does	O
best	O
on	O
faults	O
dataset	O
,	O
so	O
the	O
dataset	O
faults	O
has	O
the	O
label	O
ml	O
attached	O
.	O
inspecting	O
figure	O
10.3	O
,	O
a	O
very	O
clear	O
pattern	O
emerges	O
.	O
machine	O
learning	O
procedures	O
generally	O
do	O
best	O
on	O
datasets	O
at	O
the	O
top	O
or	O
at	O
the	O
extreme	O
right	O
of	O
the	O
diagram	O
.	O
statistical	B
and	O
neural	O
network	O
procedures	O
do	O
best	O
on	O
datasets	O
in	O
the	O
lower	O
half	O
and	O
to	O
the	O
left	O
of	O
the	O
diagram	O
.	O
of	O
course	O
,	O
this	O
pattern	O
is	O
very	O
closely	O
related	O
to	O
the	O
fact	O
that	O
datasets	O
from	O
particular	O
application	O
areas	O
are	O
clustered	O
together	O
.	O
in	O
the	O
spirit	O
of	O
correspondence	O
analysis	O
,	O
it	O
would	O
be	O
possible	O
to	O
use	O
the	O
scaling	O
coor-	O
dinates	O
of	O
datasets	O
or	O
algorithms	O
to	O
come	O
up	O
with	O
a	O
mutually	O
consistent	O
set	O
of	O
coordinates	O
that	O
express	O
the	O
relationships	O
between	O
datasets	O
and	O
algorithms	O
.	O
this	O
can	O
be	O
done	O
,	O
but	O
there	O
are	O
too	O
many	O
missing	O
values	O
in	O
the	O
tables	O
for	O
the	O
usual	O
version	O
of	O
correspondence	O
analysis	O
(	O
no	O
missing	O
values	O
allowed	O
)	O
.	O
192	O
analysis	O
of	O
results	O
10.4.5	O
clustering	O
of	O
datasets	O
[	O
ch	O
.	O
10	O
starting	O
from	O
the	O
distances	O
in	O
23-dimensions	O
,	O
a	O
standard	O
clustering	O
algorithm	O
(	O
using	O
the	O
furthest	O
neighbour	O
option	O
)	O
gives	O
the	O
clustering	O
of	O
figure	O
10.4.	O
hierarchical	O
clustering	O
-	O
22	O
datasets	O
(	O
based	O
on	O
23	O
algorithms	O
)	O
5	O
.	O
2	O
0	O
.	O
2	O
5	O
.	O
1	O
0	O
.	O
1	O
5	O
.	O
0	O
a	O
n	O
d	O
t	O
t	O
u	O
h	O
s	O
h	O
c	O
e	O
t	O
m	O
o	O
r	O
h	O
c	O
t	O
r	O
a	O
e	O
dh	O
a	O
e	O
h	O
r	O
e	O
g	O
.	O
r	O
c	O
m	O
i	O
t	O
a	O
s	O
r	O
e	O
t	O
t	O
e	O
l	O
t	O
e	O
s	O
t	O
m	O
g	O
e	O
s	O
0	O
2	O
t	O
u	O
c	O
0	O
5	O
t	O
u	O
c	O
i	O
c	O
h	O
e	O
v	O
b	O
a	O
d	O
i	O
s	O
u	O
a	O
.	O
r	O
c	O
l	O
g	O
e	O
b	O
t	O
f	O
o	O
s	O
i	O
l	O
e	O
b	O
w	O
e	O
n	O
n	O
a	O
m	O
.	O
r	O
c	O
l	O
k	O
4	O
4	O
g	O
d	O
i	O
fig	O
.	O
10.4	O
:	O
hierarchical	O
clustering	O
of	O
datasets	O
based	O
on	O
standardised	O
error	O
rates	O
and	O
costs	O
.	O
10.5	O
performance	O
related	O
to	O
measures	O
:	O
theoretical	O
there	O
are	O
very	O
few	O
theoretical	O
indicators	O
for	O
algorithm	O
accuracy	O
.	O
what	O
little	O
there	O
are	O
,	O
make	O
speciﬁc	O
distributional	O
assumptions	O
,	O
and	O
the	O
only	O
question	O
is	O
whether	O
these	O
speciﬁc	O
assumptions	O
are	O
valid	O
.	O
in	O
such	O
cases	O
,	O
it	O
is	O
possible	O
to	O
build	O
checks	O
into	O
the	O
algorithm	O
that	O
give	O
an	O
indication	O
if	O
the	O
assumptions	O
are	O
valid	O
.	O
10.5.1	O
normal	O
distributions	O
the	O
statistical	B
measures	O
were	O
deﬁned	O
in	O
section	O
7.3.2	O
with	O
a	O
view	O
to	O
monitoring	O
the	O
success	O
of	O
the	O
two	O
discriminant	O
procedures	O
that	O
are	O
associated	O
with	O
the	O
normal	O
distribution	O
,	O
namely	O
linear	O
and	O
quadratic	O
discriminants	O
.	O
within	O
the	O
class	O
of	O
normal	O
distributions	O
,	O
the	O
measureýî	O
þ¢ÿ 	O
provides	O
a	O
guide	O
as	O
to	O
the	O
relative	O
suitability	O
of	O
linear	O
and	O
quadratic	O
if	O
sample	O
sizes	O
are	O
so	O
large	O
that	O
covariance	O
matrices	O
can	O
be	O
accurately	O
discrimination	O
.	O
measured	O
,	O
it	O
would	O
be	O
legitimate	O
to	O
use	O
the	O
quadratic	O
version	O
exclusively	O
,	O
as	O
it	O
reduces	O
to	O
the	O
linear	O
rule	O
in	O
the	O
special	O
case	O
of	O
equality	O
of	O
covariances	O
.	O
practically	O
speaking	O
,	O
the	O
advice	O
must	O
be	O
reversed	O
:	O
use	O
linear	O
discriminants	O
unless	O
the	O
sample	O
size	O
is	O
very	O
large	O
,	O
the	O
distribution	O
is	O
known	O
to	O
be	O
nearly	O
normal	O
and	O
the	O
covariances	O
are	O
very	O
different	O
.	O
so	O
we	O
consider	O
now	O
when	O
to	O
use	O
quadratic	O
discriminants	O
.	O
it	O
should	O
be	O
noted	O
that	O
this	O
advice	O
is	O
absolute	O
in	O
the	O
sense	O
that	O
it	O
is	O
based	O
only	O
on	O
measures	O
related	O
to	O
the	O
dataset	O
.	O
sec	O
.	O
10.5	O
]	O
performance	O
related	O
to	O
measures	O
:	O
theoretical	O
193	O
10.5.2	O
absolute	O
performance	O
:	O
quadratic	O
discriminants	O
in	O
theory	O
,	O
quadratic	O
discrimination	O
is	O
the	O
best	O
procedure	O
to	O
use	O
when	O
the	O
data	O
are	O
normally	O
distributed	O
,	O
especially	O
so	O
if	O
the	O
covariances	O
differ	O
.	O
because	O
it	O
makes	O
very	O
speciﬁc	O
distribu-	O
tional	O
assumptions	O
,	O
and	O
so	O
is	O
very	O
efﬁcient	O
for	O
normal	O
distributions	O
,	O
it	O
is	O
inadvisable	O
to	O
use	O
quadratic	O
discrimination	O
for	O
non-normal	O
distributions	O
(	O
a	O
common	O
situation	O
with	O
parametric	O
procedures	O
-	O
they	O
are	O
not	O
robust	O
to	O
departures	O
from	O
the	O
assumptions	O
)	O
,	O
and	O
,	O
because	O
it	O
uses	O
many	O
more	O
parameters	O
,	O
it	O
is	O
also	O
not	O
advisable	O
to	O
use	O
quadratic	O
discrimination	O
when	O
the	O
sample	O
sizes	O
are	O
small	O
.	O
we	O
will	O
now	O
relate	O
these	O
facts	O
to	O
our	O
measures	O
for	O
the	O
datasets	O
.	O
tributed	O
,	O
dataset	O
with	O
widely	O
differing	O
covariance	O
matrices	O
.	O
the	O
ideal	O
dataset	O
for	O
quadratic	O
discrimination	O
would	O
be	O
a	O
very	O
large	O
,	O
normally	O
dis-	O
in	O
terms	O
of	O
the	O
measures	O
,	O
kurtosis	O
=	O
3	O
,	O
and	O
sd	O
ratio	O
much	O
greater	O
than	O
unity	O
.	O
skewness	O
=	O
0	O
,	O
ï	O
skewness	O
=	O
kurtosis	O
=	O
2.92	O
(	O
and	O
this	O
is	O
near	O
3	O
)	O
,	O
and	O
,	O
most	O
importantly	O
,	O
sd	O
ratio	O
=	O
1.97	O
(	O
and	O
this	O
is	O
much	O
greater	O
than	O
unity	O
)	O
.	O
this	O
dataset	O
is	O
nearest	O
ideal	O
,	O
so	O
it	O
is	O
predictable	O
that	O
quadratic	O
discrimination	O
will	O
achieve	O
the	O
lowest	O
error	O
rate	O
.	O
in	O
fact	O
,	O
quadratic	O
discriminants	O
achieve	O
an	O
error	O
rate	O
of	O
2.5	O
%	O
,	O
and	O
this	O
is	O
only	O
bettered	O
by	O
k-nn	O
with	O
an	O
error	O
rate	O
of	O
2.0	O
%	O
and	O
by	O
alloc80	O
with	O
an	O
error	O
rate	O
of	O
2.4	O
%	O
.	O
ideally	O
we	O
want	O
the	O
most	O
normal	O
dataset	O
in	O
our	O
study	O
is	O
the	O
kl	O
digits	O
dataset	O
,	O
as	O
	O
0.18	O
(	O
and	O
this	O
is	O
small	O
)	O
,	O
ï	O
skewness	O
=	O
4.4	O
(	O
very	O
large	O
)	O
)	O
,	O
	O
kurtosis	O
=	O
160.3	O
(	O
nowhere	O
near	O
3	O
)	O
,	O
and	O
,	O
to	O
make	O
matters	O
worse	O
,	O
the	O
sd	O
ratio	O
=	O
1.12	O
(	O
and	O
this	O
is	O
not	O
much	O
greater	O
than	O
unity	O
)	O
.	O
therefore	O
,	O
we	O
can	O
predict	O
that	O
this	O
is	O
the	O
least	O
appropriate	O
dataset	O
for	O
quadratic	O
discrimination	O
,	O
and	O
it	O
is	O
no	O
surprise	O
that	O
quadratic	O
discriminants	O
achieve	O
an	O
error	O
rate	O
of	O
6.72	O
%	O
,	O
which	O
is	O
worst	O
of	O
all	O
our	O
results	O
for	O
the	O
shuttle	O
dataset	O
.	O
the	O
decision	O
tree	O
methods	O
get	O
error	O
rates	O
smaller	O
than	O
this	O
by	O
a	O
factor	O
of	O
100	O
!	O
at	O
the	O
other	O
extreme	O
,	O
the	O
least	O
normal	O
dataset	O
is	O
probably	O
the	O
shuttle	O
dataset	O
,	O
with	O
the	O
important	O
proviso	O
should	O
always	O
be	O
borne	O
in	O
mind	O
that	O
there	O
must	O
be	O
enough	O
data	O
to	O
estimate	O
all	O
parameters	O
accurately	O
.	O
10.5.3	O
relative	O
performance	O
:	O
logdisc	O
vs.	O
dipol92	O
another	O
fruitful	O
way	O
of	O
looking	O
at	O
the	O
behaviour	O
of	O
algorithms	O
is	O
by	O
making	O
paired	O
compar-	O
isons	O
between	O
closely	O
related	O
algorithms	O
.	O
this	O
extremely	O
useful	O
device	O
is	O
best	O
illustrated	O
by	O
comparing	O
logistic	O
discrimination	O
(	O
logdisc	O
)	O
and	O
dipol92	O
.	O
from	O
their	O
construction	O
,	O
we	O
can	O
see	O
that	O
dipol92	O
and	O
logistic	O
discrimination	O
have	O
exactly	O
the	O
same	O
formal	O
decision	O
procedure	O
in	O
one	O
special	O
case	O
,	O
namely	O
the	O
case	O
of	O
two-class	O
problems	O
in	O
which	O
there	O
is	O
no	O
clustering	O
(	O
i.e	O
.	O
both	O
classes	O
are	O
“	O
pure	O
”	O
)	O
.	O
where	O
the	O
two	O
differ	O
then	O
,	O
will	O
be	O
in	O
multi-class	O
problems	O
(	O
such	O
as	O
the	O
digits	O
or	O
letters	O
datasets	O
)	O
or	O
in	O
two-class	O
problems	O
in	O
which	O
the	O
classes	O
are	O
impure	B
(	O
such	O
as	O
the	O
belgian	O
power	O
dataset	O
)	O
.	O
with	O
this	O
in	O
mind	O
,	O
it	O
is	O
of	O
interest	O
to	O
compare	O
the	O
performance	O
of	O
dipol92	O
when	O
it	O
does	O
not	O
use	O
clustering	O
with	O
the	O
performance	O
of	O
logistic	O
discrimination	O
,	O
as	O
is	O
done	O
in	O
table	O
10.9.	O
the	O
accuracies/average	O
costs	O
quoted	O
for	O
logistic	O
discrimination	O
are	O
those	O
in	O
the	O
main	O
tables	O
of	O
chapter	O
9.	O
those	O
quoted	O
for	O
dipol92	O
are	O
for	O
the	O
no-clustering	O
version	O
of	O
dipol	O
,	O
and	O
so	O
are	O
different	O
,	O
in	O
general	O
,	O
from	O
those	O
in	O
the	O
main	O
tables	O
.	O
either	O
in	O
table	O
10.9	O
or	O
in	O
the	O
main	O
tables	O
,	O
it	O
is	O
clear	O
that	O
sometimes	O
one	O
procedure	O
is	O
better	O
and	O
sometimes	O
the	O
other	O
.	O
from	O
what	O
is	O
known	O
about	O
the	O
algorithms	O
,	O
however	O
,	O
we	O
should	O
look	O
at	O
the	O
two-class	O
problems	O
separately	O
,	O
and	O
,	O
if	O
this	O
is	O
done	O
,	O
a	O
pattern	O
emerges	O
.	O
indeed	O
from	O
table	O
10.9	O
,	O
it	O
	O
	O
ï	O
194	O
analysis	O
of	O
results	O
[	O
ch	O
.	O
10	O
table	O
10.9	O
:	O
logistic	O
discriminants	O
vs.	O
dipol92	O
with	O
no	O
clustering	O
.	O
dataset	O
belgian	O
chromosome	O
credit	O
aus	O
credit	O
ger	O
credit	O
man	O
cut20	O
cut50	O
dna	O
diabetes	B
digit44	O
faults	O
kl	O
digit	O
letter	O
new.belg	O
sat	O
.	O
image	B
segmentation	I
shuttle	O
technical	B
tsetse	O
vehicle	O
logdisc	O
dipol92	O
(	O
no	O
clustering	O
)	O
no	O
.	O
classes	O
(	O
q	O
)	O
0.0072	O
0.1310	O
0.1406	O
0.5380	O
0.0300	O
0.0460	O
0.0370	O
0.0610	O
0.2230	O
0.0860	O
0.2210	O
0.0510	O
0.2340	O
0.0280	O
0.1630	O
0.1090	O
0.0380	O
0.4010	O
0.1170	O
0.1910	O
0.0184	O
0.0917	O
0.1406	O
0.5440	O
0.0292	O
0.0480	O
0.0490	O
0.0490	O
0.2380	O
0.0700	O
0.2000	O
0.0400	O
0.1770	O
0.0380	O
0.1480	O
0.0510	O
0.0530	O
0.3530	O
0.1210	O
0.2070	O
2	O
24	O
2	O
2	O
2	O
2	O
2	O
3	O
2	O
10	O
3	O
10	O
26	O
2	O
6	O
7	O
7	O
91	O
2	O
4	O
seems	O
that	O
generally	O
logdisc	O
is	O
better	O
than	O
dipol92	O
for	O
two-class	O
problems	O
.	O
knowing	O
this	O
,	O
we	O
can	O
look	O
back	O
at	O
the	O
main	O
tables	O
and	O
come	O
to	O
the	O
following	O
conclusions	O
about	O
the	O
relative	O
performance	O
of	O
logdisc	O
and	O
dipol92	O
.	O
rules	O
comparing	O
logdisc	O
to	O
dipol92	O
we	O
can	O
summarise	O
our	O
conclusions	O
viz-a-viz	O
logistic	O
and	O
dipol	O
by	O
the	O
following	O
rules	O
,	O
which	O
amount	O
to	O
saying	O
that	O
dipol92	O
is	O
usually	O
better	O
than	O
logdisc	O
except	O
for	O
the	O
cases	O
stated	O
.	O
if	O
number	O
of	O
examples	O
is	O
small	O
,	O
–	O
or	O
if	O
cost	O
matrix	O
involved	O
,	O
–	O
or	O
if	O
number	O
of	O
classes	O
=	O
2	O
and	O
if	O
no	O
distinct	O
clusters	O
within	O
classes	O
then	O
logdisc	O
is	O
better	O
than	O
dipol92	O
else	O
dipol92	O
is	O
better	O
than	O
logdisc	O
10.5.4	O
pruning	B
of	O
decision	O
trees	O
this	O
section	O
looks	O
at	O
a	O
small	O
subset	O
of	O
the	O
trials	O
relating	O
to	O
decision	O
tree	O
methods	O
.	O
the	O
speciﬁc	O
aim	O
is	O
to	O
illustrate	O
how	O
error	O
rate	O
(	O
or	O
cost	O
)	O
is	O
related	O
to	O
the	O
complexity	O
(	O
number	O
of	O
nodes	O
)	O
of	O
the	O
decision	O
tree	O
.	O
there	O
is	O
no	O
obvious	O
way	O
of	O
telling	O
if	O
the	O
error-rate	O
of	O
a	O
decision	O
tree	O
is	O
near	O
optimal	O
,	O
indeed	O
the	O
whole	O
question	O
of	O
what	O
is	O
to	O
be	O
optimised	O
is	O
a	O
very	O
open	O
one	O
.	O
in	O
practice	O
a	O
	O
	O
	O
	O
sec	O
.	O
10.5	O
]	O
performance	O
related	O
to	O
measures	O
:	O
theoretical	O
195	O
hypothetical	O
error	O
rates	O
for	O
three	O
algorithms	O
algor_3	O
algor_2	O
algor_1	O
dataset	O
i	O
algor_1	O
algor_3	O
algor_2	O
dataset	O
ii	O
r	O
o	O
r	O
r	O
e	O
0	O
0	O
.	O
1	O
0	O
5	O
.	O
0	O
0	O
1	O
.	O
0	O
5	O
0	O
0	O
.	O
100	O
500	O
1000	O
5000	O
10000	O
50000	O
nodes	O
fig	O
.	O
10.5	O
:	O
hypothetical	O
dependence	O
of	O
error	O
rates	O
on	O
number	O
of	O
end	O
nodes	O
(	O
and	O
so	O
on	O
pruning	B
)	O
for	O
three	O
algorithms	O
on	O
two	O
datasets	O
.	O
balance	O
must	O
be	O
struck	O
between	O
conﬂicting	O
criteria	O
.	O
one	O
way	O
of	O
achieving	O
a	O
balance	O
is	O
the	O
use	O
of	O
cost-complexity	O
as	O
a	O
criterion	O
,	O
as	O
is	O
done	O
by	O
breiman	O
et	O
al	O
.	O
(	O
1984	O
)	O
.	O
this	O
balances	O
complexity	O
of	O
the	O
tree	O
against	O
the	O
error	O
rate	O
,	O
and	O
is	O
used	O
in	O
their	O
cart	O
procedure	O
as	O
a	O
criterion	O
for	O
pruning	B
the	O
decision	O
tree	O
.	O
all	O
the	O
decision	O
trees	O
in	O
this	O
project	O
incorporate	O
some	O
kind	O
of	O
pruning	B
,	O
and	O
the	O
extent	O
of	O
pruning	B
is	O
controlled	O
by	O
a	O
parameter	O
.	O
generally	O
,	O
a	O
tree	O
that	O
is	O
overpruned	O
has	O
too	O
high	O
an	O
error	O
rate	O
because	O
the	O
decision	O
tree	O
does	O
not	O
represent	O
the	O
full	O
structure	O
of	O
the	O
dataset	O
,	O
and	O
the	O
tree	O
is	O
biased	O
.	O
on	O
the	O
other	O
hand	O
,	O
a	O
tree	O
that	O
is	O
not	O
pruned	O
has	O
too	O
much	O
random	O
variation	O
in	O
the	O
allocation	O
of	O
examples	O
.	O
in	O
between	O
these	O
two	O
extremes	O
,	O
there	O
is	O
usually	O
an	O
optimal	O
amount	O
of	O
pruning	B
.	O
if	O
an	O
investigator	O
is	O
prepared	O
to	O
spend	O
some	O
time	O
trying	O
different	O
values	O
of	O
this	O
pruning	B
parameter	O
,	O
and	O
the	O
error-rate	O
is	O
tested	O
against	O
an	O
independent	O
test	O
set	O
,	O
the	O
optimal	O
amount	O
of	O
pruning	B
can	O
be	O
found	O
by	O
plotting	O
the	O
error	O
rate	O
against	O
the	O
pruning	B
parameter	O
.	O
equivalently	O
,	O
the	O
error-rate	O
may	O
be	O
plotted	O
against	O
the	O
number	O
of	O
end	O
nodes	O
.	O
usually	O
,	O
the	O
error	O
rate	O
drops	O
quite	O
quickly	O
to	O
its	O
minimum	O
value	O
as	O
the	O
number	O
of	O
nodes	O
increases	O
,	O
increasing	O
slowly	O
as	O
the	O
nodes	O
increase	O
beyond	O
the	O
optimal	O
value	O
.	O
the	O
number	O
of	O
end	O
nodes	O
is	O
an	O
important	O
measure	B
of	O
the	O
complexity	O
of	O
a	O
decision	O
tree	O
.	O
if	O
the	O
decision	O
tree	O
achieves	O
something	O
near	O
the	O
optimal	O
error-rate	O
,	O
the	O
number	O
of	O
end	O
nodes	O
is	O
also	O
measure	B
of	O
the	O
complexity	O
of	O
the	O
dataset	O
.	O
although	O
it	O
is	O
not	O
to	O
be	O
expected	O
that	O
all	O
decision	O
trees	O
will	O
achieve	O
their	O
optimal	O
error-rates	O
with	O
the	O
same	O
number	O
of	O
end-nodes	O
,	O
it	O
seems	O
reasonable	O
that	O
most	O
decision	O
trees	O
will	O
achieve	O
their	O
optimal	O
error-rates	O
when	O
the	O
number	O
of	O
end-nodes	O
matches	O
the	O
complexity	O
of	O
the	O
dataset	O
.	O
considerations	O
like	O
these	O
lead	O
us	O
to	O
expect	O
that	O
the	O
error-rates	O
of	O
different	O
algorithms	O
	O
	O
	O
	O
196	O
analysis	O
of	O
results	O
[	O
ch	O
.	O
10	O
on	O
the	O
same	O
dataset	O
will	O
behave	O
as	O
sketched	O
in	O
figure	O
10.5.	O
to	O
achieve	O
some	O
kind	O
of	O
comparability	O
between	O
datasets	O
,	O
all	O
the	O
curves	O
for	O
one	O
dataset	O
can	O
be	O
moved	O
horizontally	O
and	O
vertically	O
on	O
the	O
logarithmic	O
scale	O
.	O
this	O
amounts	O
to	O
rescaling	O
all	O
the	O
results	O
on	O
that	O
dataset	O
so	O
that	O
the	O
global	O
minimum	O
error	O
rate	O
is	O
unity	O
and	O
the	O
number	O
of	O
nodes	O
at	O
the	O
global	O
minimum	O
is	O
unity	O
.	O
when	O
no	O
attempt	O
is	O
made	O
to	O
optimise	O
the	O
amount	O
of	O
pruning	B
,	O
we	O
resort	O
to	O
the	O
following	O
plausible	O
argument	O
to	O
compare	O
algorithms	O
.	O
consider	O
,	O
for	O
example	B
,	O
the	O
cut20	O
dataset	O
.	O
four	O
algorithms	O
were	O
tested	O
,	O
with	O
very	O
widely	O
differing	O
error	O
rates	O
and	O
nodes	O
,	O
as	O
shown	O
in	O
table	O
10.10.	O
as	O
the	O
lowest	O
error	O
rate	O
is	O
achieved	O
by	O
c4.5	O
,	O
make	O
everything	O
relative	O
to	O
c4.5	O
,	O
so	O
that	O
the	O
relative	O
numberó	O
opt	O
of	O
nodes	O
and	O
relative	O
error	O
rates	O
$	O
ó	O
opt	O
are	O
given	O
in	O
table	O
10.10	O
these	O
standardised	O
results	O
for	O
the	O
cut20	O
dataset	O
are	O
plotted	O
in	O
figure	O
10.6	O
,	O
along	O
table	O
10.10	O
:	O
error	O
rates	O
and	O
number	O
of	O
end	O
nodes	O
for	O
four	O
decision	O
trees	O
on	O
the	O
cut20	O
dataset	O
.	O
note	O
that	O
c4.5	O
achieves	O
the	O
lowest	O
error	O
rate	O
,	O
so	O
we	O
speculate	O
that	O
the	O
optimal	O
number	O
of	O
end	O
nodes	O
for	O
decision	O
trees	O
is	O
about	O
159.	O
algorithm	O
no	O
.	O
end	O
nodes	O
error	O
rate	O
0.063	O
0.045	O
0.036	O
0.039	O
cal5	O
c4.5	O
newid	O
38	O
14	O
159	O
339	O
ú	O
%	O
û	O
cal5	O
c4.5	O
newid	O
that	O
are	O
not	O
near	O
this	O
“	O
optimal	O
”	O
point	O
.	O
note	O
that	O
cal5	O
appears	O
most	O
frequently	O
in	O
the	O
left	O
of	O
the	O
figure	O
10.6	O
(	O
where	O
it	O
has	O
less	O
1.750	O
1.250	O
1.000	O
1.083	O
table	O
10.11	O
:	O
error	O
rates	O
and	O
number	O
of	O
end	O
nodes	O
for	O
four	O
algorithms	O
relative	O
to	O
the	O
values	O
for	O
c4.5	O
.	O
with	O
standardised	O
results	O
from	O
15	O
other	O
datasets	O
for	O
which	O
we	O
had	O
the	O
relevant	O
information	O
,	O
with	O
the	O
name	O
of	O
the	O
algorithm	O
as	O
label	O
.	O
of	O
course	O
,	O
each	O
dataset	O
will	O
give	O
rise	O
to	O
at	O
least	O
0.239	O
0.088	O
1.000	O
2.132	O
algorithm	O
ñó	O
	O
ú	O
%	O
û	O
ñ	O
and	O
$	O
ó	O
	O
	O
	O
$	O
ó	O
	O
ñ	O
,	O
but	O
we	O
are	O
here	O
concerned	O
with	O
the	O
results	O
ï	O
appear	O
most	O
frequently	O
in	O
the	O
ï	O
are	O
biased	O
ï	O
often	O
use	O
very	O
one	O
point	O
withñó	O
	O
nodes	O
than	O
the	O
“	O
best	O
”	O
algorithm	O
)	O
and	O
both	O
newid	O
andúüû	O
deliberately	O
to	O
obtain	O
trees	O
with	O
simple	O
structure	O
)	O
,	O
whereas	O
newid	O
andúüû	O
has	O
struck	O
the	O
right	O
balance	O
,	O
but	O
it	O
does	O
seem	O
clear	O
that	O
newid	O
andú	O
%	O
û	O
right	O
of	O
the	O
diagram	O
(	O
where	O
they	O
have	O
too	O
many	O
nodes	O
)	O
.	O
it	O
would	O
also	O
appear	O
that	O
c4.5	O
is	O
most	O
likely	O
to	O
use	O
the	O
“	O
best	O
”	O
number	O
of	O
nodes	O
-	O
and	O
this	O
is	O
very	O
indirect	O
evidence	O
that	O
the	O
amount	O
of	O
pruning	B
used	O
by	O
c4.5	O
is	O
correct	O
on	O
average	O
,	O
although	O
this	O
conclusion	O
is	O
based	O
on	O
a	O
small	O
number	O
of	O
datasets	O
.	O
one	O
would	O
expect	O
that	O
a	O
well-trained	O
procedure	O
should	O
attain	O
the	O
optimal	O
number	O
of	O
nodes	O
on	O
average	O
,	O
but	O
it	O
is	O
clear	O
that	O
cal5	O
is	O
biased	O
towards	O
small	O
numbers	O
(	O
this	O
may	O
be	O
done	O
towards	O
more	O
complex	O
trees	O
.	O
in	O
the	O
absence	O
of	O
information	O
on	O
the	O
relative	O
weights	O
to	O
be	O
attached	O
to	O
complexity	O
(	O
number	O
of	O
nodes	O
)	O
or	O
cost	O
(	O
error	O
rate	O
)	O
,	O
we	O
can	O
not	O
say	O
whether	O
cal5	O
ï	O
ï	O
sec	O
.	O
10.6	O
]	O
rule	O
based	O
advice	O
on	O
algorithm	O
application	O
197	O
excess	O
error	O
rate	O
vs.	O
excess	O
no	O
.	O
nodes	O
newid	O
cal5	O
cal5	O
cal5	O
ac2	O
ac2	O
newid	O
cal5	O
cal5	O
cal5	O
cal5	O
ac2	O
ac2	O
cal5	O
c4.5	O
newid	O
c4.5	O
ac2	O
cal5	O
c4.5	O
newid	O
ac2	O
c4.5	O
ac2	O
newid	O
newid	O
ac2	O
newid	O
ac2	O
ac2	O
ac2	O
ac2	O
newid	O
newid	O
cal5	O
ac2	O
c4.5newid	O
cal5	O
cal5	O
newidac2	O
cal5	O
cal5	O
newid	O
c4.5	O
c4.5	O
c4.5	O
newid	O
c4.5	O
newid	O
cal5	O
newidac2	O
cal5	O
c4.5	O
c4.5	O
newid	O
ac2	O
newid	O
2	O
o	O
i	O
t	O
a	O
r	O
.	O
r	O
o	O
r	O
r	O
e	O
1	O
0.1	O
0.5	O
1.0	O
5.0	O
node.ratio	O
10.0	O
50.0	O
100.0	O
fig	O
.	O
10.6	O
:	O
error	O
rate	O
and	O
number	O
of	O
nodes	O
for	O
16	O
datasets	O
.	O
results	O
for	O
each	O
dataset	O
are	O
scaled	O
separately	O
so	O
that	O
the	O
algorithm	O
with	O
lowest	O
error	O
rate	O
on	O
that	O
dataset	O
has	O
unit	O
error	O
rate	O
and	O
unit	O
number	O
of	O
nodes	O
.	O
complex	O
structures	O
with	O
no	O
compensation	O
in	O
reduced	O
error	O
rate	O
.	O
10.6	O
rule	O
based	O
advice	O
on	O
algorithm	O
application	O
many	O
different	O
statistical	B
and	O
machine	O
learning	O
algorithms	O
have	O
been	O
developed	O
in	O
the	O
past	O
.	O
if	O
we	O
are	O
interested	O
in	O
applying	O
these	O
algorithms	O
to	O
concrete	O
tasks	O
we	O
have	O
to	O
consider	O
which	O
learning	O
algorithm	O
is	O
best	O
suited	O
for	O
which	O
problem	O
.	O
a	O
satisfactory	O
answer	O
requires	O
a	O
certain	O
know-how	O
of	O
this	O
area	O
,	O
which	O
can	O
be	O
acquired	O
only	O
with	O
experience	O
.	O
we	O
consider	O
here	O
if	O
machine	O
learning	O
techniques	O
themselves	O
can	O
be	O
useful	O
in	O
organizing	O
this	O
knowledge	O
,	O
speciﬁcally	O
the	O
knowledge	O
embedded	O
in	O
the	O
empirical	O
results	O
of	O
the	O
statlog	O
trials	O
.	O
the	O
aim	O
is	O
to	O
relate	O
the	O
performance	O
of	O
algorithms	O
to	O
the	O
characteristics	O
of	O
the	O
datasets	O
using	O
only	O
the	O
empirical	O
data	O
.	O
the	O
process	O
of	O
generating	O
a	O
set	O
of	O
rules	O
capable	O
of	O
relating	O
these	O
two	O
concepts	O
is	O
referred	O
to	O
as	O
meta-level	O
learning	O
.	O
10.6.1	O
objectives	B
it	O
appears	O
that	O
datasets	O
can	O
be	O
characterised	O
using	O
certain	O
features	O
such	O
as	O
number	O
of	O
attributes	O
,	O
their	O
types	O
,	O
amount	O
of	O
unknown	O
values	O
or	O
other	O
statistical	B
parameters	O
.	O
it	O
is	O
reasonable	O
to	O
try	O
to	O
match	O
the	O
features	O
of	O
datasets	O
with	O
our	O
past	O
knowledge	O
concerning	O
the	O
algorithms	O
.	O
if	O
we	O
select	O
the	O
algorithm	O
that	O
most	O
closely	O
matches	O
the	O
features	O
of	O
the	O
dataset	O
,	O
then	O
we	O
increase	O
the	O
chances	O
of	O
obtaining	O
useful	O
results	O
.	O
the	O
advantage	O
is	O
that	O
not	O
all	O
algorithms	O
need	O
to	O
be	O
tried	O
out	O
.	O
those	O
algorithms	O
that	O
do	O
not	O
match	O
the	O
data	O
can	O
be	O
excluded	O
,	O
and	O
so	O
,	O
a	O
great	O
deal	O
of	O
effort	O
can	O
be	O
saved	O
.	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
198	O
analysis	O
of	O
results	O
[	O
ch	O
.	O
10	O
in	O
order	O
to	O
achieve	O
this	O
aim	O
,	O
we	O
need	O
to	O
determine	O
which	O
dataset	O
features	O
are	O
relevant	O
.	O
after	O
that	O
,	O
various	O
instances	O
of	O
learning	O
tasks	O
can	O
be	O
examined	O
with	O
the	O
aim	O
of	O
formu-	O
lating	O
a	O
“	O
theory	O
”	O
concerning	O
the	O
applicability	O
of	O
different	O
machine	O
learning	O
and	O
statistical	B
algorithms	O
.	O
the	O
knowledge	O
concerning	O
which	O
algorithm	O
is	O
applicable	O
can	O
be	O
summarised	O
in	O
the	O
form	O
of	O
rules	O
stating	O
that	O
if	O
the	O
given	O
dataset	O
has	O
certain	O
characteristics	O
then	O
learning	O
a	O
particular	O
algorithm	O
may	O
be	O
applicable	O
.	O
each	O
rule	O
can	O
,	O
in	O
addition	O
,	O
be	O
qualiﬁed	O
using	O
a	O
certain	O
measure	B
indicating	O
how	O
reliable	O
the	O
rule	O
is	O
.	O
rules	O
like	O
this	O
can	O
be	O
constructed	O
manually	O
,	O
or	O
with	O
the	O
help	O
of	O
machine	O
learning	O
methods	O
on	O
the	O
basis	O
of	O
past	O
cases	O
.	O
in	O
this	O
section	O
we	O
are	O
concerned	O
with	O
this	O
latter	O
method	O
.	O
the	O
process	O
of	O
constructing	O
the	O
rules	O
represents	O
a	O
kind	O
of	O
meta-level	O
learning	O
.	O
as	O
the	O
number	O
of	O
tests	O
was	O
generally	O
limited	O
,	O
few	O
people	O
have	O
attempted	O
to	O
automate	O
the	O
formulation	O
of	O
a	O
theory	O
concerning	O
the	O
applicability	O
of	O
different	O
algorithms	O
.	O
one	O
exception	O
was	O
the	O
work	O
of	O
aha	O
(	O
1992	O
)	O
who	O
represented	O
this	O
knowledge	O
using	O
the	O
following	O
rule	O
schemas	O
:	O
	O
!	O
#	O
''	O
	O
$	O
%	O
	O
,	O
	O
!	O
mlon5	O
''	O
lon	O
lon	O
e	O
%	O
	O
,	O
''	O
3	O
$	O
.6/.14	O
.102	O
%	O
.1054	O
%	O
6	O
)	O
(	O
70	O
%	O
'	O
&	O
)	O
(	O
*+	O
%	O
	O
,	O
#	O
-/	O
.	O
``	O
	O
$	O
''	O
	O
$	O
%	O
cb/	O
%	O
.6	O
,	O
d	O
(	O
	O
,	O
/	O
&	O
5.7b	O
&	O
fe6	O
)	O
(	O
g5h	O
)	O
(	O
67a	O
e5b1	O
%	O
.6	O
,	O
d	O
(	O
	O
,	O
/	O
&	O
#	O
.7b	O
&	O
/e6	O
)	O
(	O
%	O
6	O
)	O
%	O
	O
,	O
j47	O
%	O
(	O
,5i/6	O
)	O
%	O
6/.1	O
(	O
	O
,	O
d	O
(	O
	O
,	O
/	O
&	O
p	O
(	O
	O
,	O
d0	O
.	O
>	O
,	O
j47	O
%	O
70'q	O
#	O
r7s7rdtvu7wx	O
aij	O
%	O
70yid	O
%	O
6z4b2.1070\	O
[	O
5	O
]	O
^	O
;	O
	O
]	O
dtvu7wx	O
i/6/e	O
''	O
7	O
''	O
6	O
)	O
%	O
b/	O
%	O
*/.	O
>	O
,	O
6	O
)	O
(	O
_7`	O
%	O
70a	O
[	O
cbd	O
;	O
	O
]	O
dt	O
:	O
c	O
[	O
2	O
[	O
c8f	O
(	O
747098	O
)	O
:	O
<	O
;	O
7	O
;	O
7	O
;	O
=8	O
>	O
,	O
?	O
``	O
	O
$	O
gchfk	O
one	O
example	B
of	O
such	O
a	O
rule	O
schema	O
is	O
:	O
where	O
ib1	O
ghg	O
c4	O
means	O
that	O
algorithm	O
ib1	O
is	O
predicted	O
to	O
have	O
signiﬁcantly	O
higher	O
accuracies	O
than	O
algorithm	O
c4	O
.	O
our	O
approach	O
differs	O
from	O
aha	O
’	O
s	O
in	O
several	O
respects	O
.	O
the	O
main	O
difference	O
is	O
that	O
we	O
are	O
not	O
concerned	O
with	O
just	O
a	O
comparison	O
between	O
two	O
algorithms	O
,	O
but	O
rather	O
a	O
group	O
of	O
them	O
.	O
our	O
aim	O
is	O
to	O
obtain	O
rules	O
which	O
would	O
indicate	O
when	O
a	O
particular	O
algorithm	O
works	O
better	O
than	O
the	O
rest	O
.	O
a	O
number	O
of	O
interesting	O
relationships	O
have	O
emerged	O
.	O
however	O
,	O
in	O
order	O
to	O
have	O
reliable	O
results	O
,	O
we	O
would	O
need	O
quite	O
an	O
extensive	O
set	O
of	O
test	O
results	O
,	O
certainly	O
much	O
more	O
than	O
the	O
22	O
datasets	O
considered	O
in	O
this	O
book	O
.	O
as	O
part	O
of	O
the	O
overall	O
aim	O
of	O
matching	O
features	O
of	O
datasets	O
with	O
our	O
past	O
knowledge	O
of	O
algorithms	O
,	O
we	O
need	O
to	O
determine	O
which	O
dataset	O
features	O
are	O
relevant	O
.	O
this	O
is	O
not	O
known	O
a	O
priori	O
,	O
so	O
,	O
for	O
exploratory	O
purposes	O
,	O
we	O
used	O
the	O
reduced	O
set	O
of	O
measures	O
given	O
in	O
table	O
10.12.	O
this	O
includes	O
certain	O
simple	O
measures	O
,	O
such	O
as	O
number	O
of	O
examples	O
,	O
attributes	O
and	O
classes	O
,	O
and	O
more	O
complex	O
statistical	B
and	O
information-based	O
measures	O
.	O
some	O
measures	O
represent	O
derived	O
quantities	O
and	O
include	O
,	O
for	O
example	B
,	O
measures	O
that	O
are	O
ratios	O
of	O
other	O
measures	O
.	O
these	O
and	O
other	O
measures	O
are	O
given	O
in	O
sections	O
7.3.1	O
–	O
7.3.3	O
.	O
10.6.2	O
using	O
test	O
results	O
in	O
metalevel	O
learning	O
here	O
we	O
have	O
used	O
all	O
of	O
the	O
available	O
results	O
-	O
as	O
listed	O
in	O
chapter	O
9.	O
the	O
results	O
for	O
each	O
dataset	O
were	O
analysed	O
with	O
the	O
objective	O
of	O
determining	O
which	O
algorithms	O
achieved	O
low	O
error	O
rates	O
(	O
or	O
costs	O
)	O
.	O
all	O
algorithms	O
with	O
low	O
error	O
rates	O
were	O
considered	O
applicable	O
to	O
''	O
$	O
''	O
''	O
@	O
$	O
''	O
!	O
''	O
``	O
''	O
e	O
''	O
''	O
.	O
''	O
@	O
$	O
sec	O
.	O
10.6	O
]	O
rule	O
based	O
advice	O
on	O
algorithm	O
application	O
199	O
table	O
10.12	O
:	O
measures	O
used	O
in	O
metalevel	O
learning	O
.	O
deﬁnition	O
number	O
of	O
examples	O
number	O
of	O
attributes	O
number	O
of	O
classes	O
number	O
of	O
binary	O
attributes	O
cost	O
matrix	O
indicator	O
standard	O
deviation	O
ratio	O
(	O
geometric	O
mean	O
)	O
mean	O
absolute	O
correlation	O
of	O
attributes	O
entropy	O
(	O
complexity	O
)	O
of	O
class	O
mean	O
entropy	O
(	O
complexity	O
)	O
of	O
attributes	O
first	O
canonical	O
correlation	O
(	O
7.3.2	O
)	O
fraction	O
separability	O
due	O
to	O
cancor1	O
skewness	O
-	O
mean	O
ofi	O
kurtosis	O
-	O
mean	O
ofi	O
òymonp/i7ó	O
!	O
ð	O
)	O
p	O
	O
jlk	O
	O
jqkòrmon=sfi7ó	O
(	O
ð	O
)	O
s	O
equivalent	O
number	O
of	O
attributest	O
j6û	O
{	O
xkvn	O
j3û	O
nó	O
j3û	O
xykznynó	O
j6û	O
xykvn	O
noise-signal	O
ratioj	O
mean	O
mutual	O
information	O
of	O
class	O
and	O
attributes	O
jlkzn	O
measure	B
simple	O
n	O
p	O
q	O
bin.att	O
cost	O
statistical	O
sd	O
corr.abs	O
cancor1	O
fract1	O
skewness	O
kurtosis	O
information	O
theory	O
j3û	O
n	O
jlkvn	O
j3û	O
xykzn	O
en.attr	O
ns.ratio	O
this	O
dataset	O
.	O
the	O
other	O
algorithms	O
were	O
considered	O
inapplicable	O
.	O
this	O
categorisation	O
of	O
the	O
test	O
results	O
can	O
be	O
seen	O
as	O
a	O
preparatory	O
step	O
for	O
the	O
metalevel	O
learning	O
task	O
.	O
of	O
course	O
,	O
the	O
categorisation	O
will	O
permit	O
us	O
also	O
to	O
make	O
prediction	O
regarding	O
which	O
algorithms	O
are	O
applicable	O
on	O
a	O
new	O
dataset	O
.	O
of	O
course	O
,	O
the	O
question	O
of	O
whether	O
the	O
error	O
rate	O
is	O
high	O
or	O
low	O
is	O
rather	O
relative	O
.	O
the	O
error	O
rate	O
of	O
15	O
%	O
may	O
be	O
excellent	O
in	O
some	O
domains	O
,	O
while	O
5	O
%	O
may	O
be	O
bad	O
in	O
others	O
.	O
this	O
problem	O
is	O
resolved	O
using	O
a	O
method	O
similar	O
to	O
subset	O
selection	O
in	O
statistics	O
.	O
first	O
,	O
the	O
best	O
algorithm	O
is	O
identiﬁed	O
according	O
to	O
the	O
error	O
rates	O
.	O
then	O
an	O
acceptable	O
margin	O
of	O
tolerance	O
is	O
calculated	O
.	O
all	O
algorithms	O
whose	O
error	O
rates	O
fall	O
within	O
this	O
margin	O
are	O
considered	O
applicable	O
,	O
while	O
the	O
others	O
are	O
labelled	O
as	O
inapplicable	O
.	O
the	O
level	O
of	O
tolerance	O
can	O
reasonably	O
be	O
deﬁned	O
in	O
terms	O
of	O
the	O
standard	O
deviation	O
of	O
the	O
error	O
rate	O
,	O
but	O
since	O
each	O
algorithm	O
achieves	O
a	O
different	O
error	O
rate	O
,	O
the	O
appropriate	O
standard	O
deviation	O
will	O
vary	O
across	O
algorithms	O
.	O
~	O
}	O
.	O
then	O
the	O
standard	O
deviation	O
is	O
deﬁned	O
by	O
to	O
keep	O
things	O
simple	O
,	O
we	O
will	O
quote	O
the	O
standard	O
deviations	O
for	O
the	O
error	O
rate	O
of	O
the	O
“	O
best	O
”	O
algorithm	O
,	O
i.e	O
.	O
that	O
which	O
achieves	O
the	O
lowest	O
error	O
rate	O
.	O
denote	O
the	O
lowest	O
error	O
rate	O
by	O
|	O
	O
|	O
{	O
j=ñcò'	O
|hnîóhx	O
where	O
fall	O
within	O
the	O
intervalj	O
{	O
|	O
xo	O
|35/	O
n	O
are	O
considered	O
applicable	O
.	O
of	O
course	O
we	O
still	O
need	O
to	O
choose	O
a	O
value	O
for	O
which	O
determines	O
the	O
size	O
of	O
the	O
interval	O
.	O
this	O
affects	O
the	O
value	O
of	O
conﬁdence	O
that	O
the	O
truly	O
best	O
algorithm	O
appears	O
in	O
the	O
group	O
considered	O
.	O
the	O
larger	O
the	O
is	O
the	O
number	O
of	O
examples	O
in	O
the	O
test	O
set	O
.	O
then	O
all	O
algorithms	O
whose	O
error	O
rates	O
,	O
the	O
higher	O
the	O
conﬁdence	O
that	O
the	O
best	O
algorithm	O
will	O
be	O
in	O
this	O
interval	O
.	O
t	O
u	O
t	O
u	O
w	O
u	O
w	O
u	O
t	O
ò	O
u	O
w	O
u	O
w	O
	O
w	O
w	O
200	O
analysis	O
of	O
results	O
[	O
ch	O
.	O
10	O
for	O
example	B
,	O
let	O
us	O
consider	O
the	O
tests	O
on	O
the	O
segmentation	O
dataset	O
consisting	O
of	O
2310	O
which	O
is	O
0.35	O
%	O
.	O
in	O
this	O
example	B
,	O
we	O
can	O
say	O
with	O
high	O
conﬁdence	O
that	O
the	O
best	O
algorithms	O
the	O
interval	O
is	O
relatively	O
examples	O
.	O
the	O
best	O
algorithm	O
appears	O
to	O
be	O
alloc80	O
with	O
the	O
error	O
rate	O
of	O
3	O
%	O
j	O
|z	O
1	O
	O
n	O
.	O
then	O
1	O
	O
are	O
in	O
the	O
group	O
with	O
error	O
rates	O
between	O
3	O
%	O
and	O
	O
ï	O
,	O
bayestree	O
)	O
apart	O
from	O
	O
%	O
x	O
small	O
of	O
/	O
	O
/	O
	O
j=ñò	O
~	O
}	O
/	O
	O
%	O
.	O
if	O
pñ	O
/	O
	O
%	O
g	O
and	O
includes	O
only	O
two	O
other	O
algorithms	O
(	O
úüû	O
alloc80	O
.	O
all	O
the	O
algorithms	O
that	O
lie	O
in	O
this	O
interval	O
can	O
be	O
considered	O
applicable	O
to	O
this	O
dataset	O
,	O
and	O
the	O
others	O
inapplicable	O
.	O
if	O
we	O
enlarge	O
the	O
margin	O
,	O
by	O
considering	O
larger	O
values	O
,	O
we	O
get	O
a	O
more	O
relaxed	O
notion	O
of	O
applicability	O
(	O
see	O
table	O
10.13	O
)	O
.	O
table	O
10.13	O
:	O
classiﬁed	O
test	O
results	O
on	O
image	B
segmentation	I
dataset	O
for	O
k=16	O
.	O
nîó	O
margin	O
0.030	O
margin	O
for	O
k=0	O
algorithm	O
error	O
class	O
alloc80	O
.030	O
appl	O
.031	O
appl	O
bayestree	O
.033	O
appl	O
úüû	O
newid	O
.034	O
appl	O
0.0335	O
margin	O
for	O
k=1	O
0.037	O
margin	O
for	O
k=2	O
.040	O
appl	O
c4.5	O
cart	O
.040	O
appl	O
dipol92	O
.040	O
appl	O
.043	O
appl	O
cn2	O
indcart	O
.045	O
appl	O
lvq	O
.046	O
appl	O
smart	O
.052	O
appl	O
backprop	O
.054	O
appl	O
cal5	O
kohonen	O
rbf	O
k-nn	O
.062	O
appl	O
.067	O
appl	O
.069	O
appl	O
.077	O
appl	O
0.044	O
margin	O
for	O
k=4	O
0.058	O
margin	O
for	O
k=8	O
0.086	O
margin	O
for	O
k=16	O
logdisc	O
.109	O
non-appl	O
castle	O
.112	O
non-appl	O
discrim	O
.116	O
non-appl	O
.157	O
non-appl	O
quadisc	O
bayes	O
.265	O
non-appl	O
.455	O
non-appl	O
itrule	O
default	O
.900	O
non-appl	O
the	O
decision	O
as	O
to	O
where	O
to	O
draw	O
the	O
line	O
(	O
by	O
choosing	O
a	O
value	O
for	O
)	O
is	O
,	O
of	O
course	O
,	O
rather	O
subjective	O
.	O
in	O
this	O
work	O
we	O
had	O
to	O
consider	O
an	O
additional	O
constraint	O
related	O
to	O
the	O
purpose	O
we	O
had	O
in	O
mind	O
.	O
as	O
our	O
objective	O
is	O
to	O
generate	O
rules	O
concerning	O
applicability	O
of	O
w	O
	O
ñ	O
	O
ï	O
sec	O
.	O
10.6	O
]	O
rule	O
based	O
advice	O
on	O
algorithm	O
application	O
201	O
algorithms	O
we	O
have	O
opted	O
for	O
the	O
more	O
relaxed	O
scheme	O
of	O
appplicability	O
(	O
k	O
=	O
8	O
or	O
16	O
)	O
,	O
so	O
as	O
to	O
have	O
enough	O
examples	O
in	O
each	O
class	O
(	O
appl	O
,	O
non-appl	O
)	O
.	O
some	O
of	O
the	O
tests	O
results	O
analysed	O
are	O
not	O
characterised	O
using	O
error	O
rates	O
,	O
but	O
rather	O
costs	O
.	O
consequently	O
the	O
notion	O
of	O
error	O
margin	O
discussed	O
earlier	O
has	O
to	O
be	O
adapted	O
to	O
costs	O
.	O
the	O
standard	O
error	O
of	O
the	O
mean	O
cost	O
can	O
be	O
calculated	O
from	O
the	O
confusion	O
matrices	O
(	O
obtained	O
by	O
testing	O
)	O
,	O
and	O
the	O
cost	O
matrix	O
.	O
the	O
values	O
obtained	O
for	O
the	O
leading	O
algorithm	O
in	O
the	O
three	O
relevant	O
datasets	O
were	O
:	O
dataset	O
german	O
credit	O
heart	O
disease	O
head	B
injury	I
algorithm	O
discrim	O
discrim	O
logdisc	O
mean	O
cost	O
0.525	O
0.415	O
18.644	O
standard	O
error	O
of	O
mean	O
0.0327	O
0.0688	O
1.3523	O
in	O
the	O
experiments	O
reported	O
later	O
the	O
error	O
margin	O
was	O
simply	O
set	O
to	O
the	O
values	O
0.0327	O
,	O
0.0688	O
and	O
1.3523	O
respectively	O
,	O
irrespective	O
of	O
the	O
algorithm	O
used	O
.	O
joining	O
data	O
relative	O
to	O
one	O
algorithm	O
the	O
problem	O
of	O
learning	O
was	O
divided	O
into	O
several	O
phases	O
.	O
in	O
each	O
phase	O
all	O
the	O
test	O
results	O
relative	O
to	O
just	O
one	O
particular	O
algorithm	O
(	O
for	O
example	B
,	O
cart	O
)	O
were	O
joined	O
,	O
while	O
all	O
the	O
other	O
results	O
(	O
relative	O
to	O
other	O
algorithms	O
)	O
were	O
temporarily	O
ignored	O
.	O
the	O
purpose	O
of	O
this	O
strategy	O
was	O
to	O
simplify	O
the	O
class	O
structure	O
.	O
for	O
each	O
algorithm	O
we	O
would	O
have	O
just	O
two	O
classes	O
(	O
appl	O
and	O
non-appl	O
)	O
.	O
this	O
strategy	O
worked	O
better	O
than	O
the	O
obvious	O
solution	O
that	O
included	O
all	O
available	O
data	O
for	O
training	O
.	O
for	O
example	B
,	O
when	O
considering	O
the	O
cart	O
algorithm	O
and	O
a	O
margin	O
ofv	O
ñ	O
we	O
get	O
the	O
scheme	O
illustrated	O
in	O
figure	O
10.7.	O
the	O
classiﬁed	O
test	O
cart-non-appl	O
,	O
cart-non-appl	O
,	O
cart-non-appl	O
,	O
cart-non-appl	O
,	O
cart-non-appl	O
,	O
cart-non-appl	O
,	O
cart-non-appl	O
,	O
cart-non-appl	O
,	O
kl	O
dig44	O
chrom	O
shut	O
tech	O
cut	B
cr.man	O
letter	O
cart-appl	O
,	O
cart-appl	O
,	O
cart-appl	O
,	O
cart-appl	O
,	O
cart-appl	O
,	O
cart-appl	O
,	O
cart-appl	O
,	O
cart-appl	O
,	O
cart-appl	O
,	O
cart-appl	O
,	O
cart-appl	O
,	O
cart-appl	O
,	O
cart-appl	O
,	O
satim	O
vehic	O
head	O
heart	O
belg	O
segm	O
diab	O
cr.ger	O
cr.aust	O
dna	O
belgii	O
faults	O
tsetse	O
fig	O
.	O
10.7	O
:	O
classiﬁed	O
test	O
results	O
relative	O
to	O
one	O
particular	O
algorithm	O
(	O
cart	O
)	O
.	O
results	O
are	O
then	O
modiﬁed	O
as	O
follows	O
.	O
the	O
dataset	O
name	O
is	O
simply	O
substituted	O
by	O
a	O
vector	O
containing	O
the	O
corresponding	O
dataset	O
characteristics	O
.	O
values	O
which	O
are	O
not	O
available	O
or	O
missing	O
are	O
simply	O
represented	O
by	O
“	O
?	O
”	O
.	O
this	O
extended	O
dataset	O
is	O
then	O
used	O
in	O
the	O
meta-level	O
learning	O
.	O
choice	O
of	O
algorithm	O
for	O
learning	O
a	O
question	O
arises	O
as	O
to	O
which	O
algorithm	O
we	O
should	O
use	O
in	O
the	O
process	O
of	O
meta-level	O
learn-	O
ing	O
.	O
we	O
have	O
decided	O
to	O
use	O
c4.5	O
for	O
the	O
following	O
reasons	O
.	O
first	O
,	O
as	O
our	O
results	O
have	O
fig	O
.	O
10.8	O
:	O
decision	O
tree	O
generated	O
by	O
c4.5	O
relative	O
to	O
cart	O
.	O
the	O
right	O
hand	O
side	O
of	O
each	O
leaf	O
are	O
either	O
of	O
the	O
form	O
(	O
n	O
)	O
or	O
(	O
n/e	O
)	O
,	O
where	O
n	O
represents	O
the	O
total	O
number	O
of	O
examples	O
satisfying	O
the	O
conditions	O
of	O
the	O
associated	O
branch	O
,	O
and	O
e	O
the	O
number	O
of	O
examples	O
of	O
other	O
classes	O
that	O
have	O
been	O
erroneously	O
covered	O
.	O
if	O
the	O
data	O
contains	O
unknown	O
values	O
,	O
the	O
numbers	O
n	O
and	O
e	O
may	O
be	O
fractional	O
.	O
it	O
has	O
been	O
argued	O
that	O
rules	O
are	O
more	O
legible	O
than	O
trees	O
.	O
the	O
decision	O
tree	O
shown	O
earlier	O
can	O
be	O
transformed	O
into	O
a	O
rule	O
form	O
using	O
a	O
very	O
simple	O
process	O
,	O
where	O
each	O
branch	O
of	O
a	O
tree	O
is	O
simply	O
transcribed	O
as	O
a	O
rule	O
.	O
the	O
applicability	O
of	O
cart	O
can	O
thus	O
be	O
characterised	O
using	O
the	O
rules	O
in	O
figure	O
10.9.	O
cart-appl	O
cart-non-appl	O
cart-non-appl	O
	O
n	O
6435	O
,	O
skewg	O
0.57	O
	O
ng	O
6435	O
	O
n	O
6435	O
,	O
skew	O
0.57	O
202	O
analysis	O
of	O
results	O
[	O
ch	O
.	O
10	O
demonstrated	O
,	O
this	O
algorithm	O
achieves	O
quite	O
good	O
results	O
overall	O
.	O
secondly	O
,	O
the	O
decision	O
tree	O
generated	O
by	O
c4.5	O
can	O
be	O
inspected	O
and	O
analysed	O
.	O
this	O
is	O
not	O
the	O
case	O
with	O
some	O
statistical	B
and	O
neural	O
learning	O
algorithms	O
.	O
so	O
,	O
for	O
example	B
,	O
when	O
c4.5	O
has	O
been	O
supplied	O
with	O
the	O
partial	O
test	O
results	O
relative	O
to	O
cart	O
algorithm	O
,	O
it	O
generated	O
the	O
decision	O
tree	O
in	O
figure	O
10.8.	O
the	O
ﬁgures	O
that	O
appear	O
on	O
[	O
5f/s7	O
]	O
wfe	O
>	O
,	O
+ui2i+b	O
bd	O
;	O
dt	O
q75f/s7	O
]	O
d	O
	O
>	O
	O
%	O
pq2cd	O
;	O
	O
]	O
7rw/e	O
>	O
,	O
+ui7i+b	O
	O
>	O
	O
%	O
	O
[	O
pd	O
;	O
	O
]	O
7rui7i+b	O
l	O
;	O
dt	O
;	O
dt	O
fig	O
.	O
10.9	O
:	O
rules	O
generated	O
by	O
c4.5	O
relative	O
to	O
cart	O
.	O
quinlan	O
(	O
1993	O
)	O
has	O
argued	O
that	O
rules	O
obtained	O
from	O
decision	O
trees	O
can	O
be	O
improved	O
upon	O
in	O
various	O
ways	O
.	O
for	O
example	B
,	O
it	O
is	O
possible	O
to	O
eliminate	O
conditions	O
that	O
are	O
irrelevant	O
,	O
or	O
even	O
drop	O
entire	O
rules	O
that	O
are	O
irrelevant	O
or	O
incorrect	O
.	O
in	O
addition	O
it	O
is	O
possible	O
to	O
reorder	O
the	O
rules	O
according	O
to	O
certain	O
criteria	O
and	O
introduce	O
a	O
default	O
rule	O
to	O
cover	O
the	O
cases	O
that	O
have	O
not	O
been	O
covered	O
.	O
the	O
program	O
c4.5	O
includes	O
a	O
command	O
that	O
permits	O
the	O
user	O
to	O
transform	O
a	O
decision	O
tree	O
into	O
a	O
such	O
a	O
rule	O
set	O
.	O
the	O
rules	O
produced	O
by	O
the	O
system	O
are	O
characterised	O
using	O
(	O
pessimistic	O
)	O
error	O
rate	O
estimates	O
.	O
as	O
is	O
shown	O
in	O
the	O
next	O
section	O
,	O
error	O
rate	O
(	O
or	O
its	O
estimate	O
)	O
is	O
not	O
an	O
ideal	O
measure	B
,	O
however	O
.	O
this	O
is	O
particularly	O
evident	O
when	O
dealing	O
with	O
continuous	O
classes	O
.	O
this	O
problem	O
has	O
motivated	O
us	O
to	O
undertake	O
a	O
separate	O
evaluation	O
of	O
all	O
candidate	O
rules	O
and	O
characterise	O
them	O
using	O
a	O
new	O
measure	B
.	O
the	O
aim	O
is	O
to	O
identify	O
those	O
rules	O
that	O
appear	O
to	O
be	O
most	O
informative	O
.	O
10.6.3	O
characterizing	O
predictive	O
power	O
the	O
rules	O
concerning	O
applicability	O
of	O
a	O
particular	O
algorithm	O
were	O
generated	O
on	O
the	O
basis	O
of	O
only	O
about	O
22	O
examples	O
(	O
each	O
case	O
represents	O
the	O
results	O
of	O
particular	O
test	O
on	O
a	O
particular	O
dataset	O
)	O
.	O
of	O
these	O
,	O
only	O
a	O
part	O
represented	O
“	O
positive	O
examples	O
”	O
,	O
corresponding	O
to	O
the	O
datasets	O
on	O
which	O
the	O
particular	O
algorithm	O
performed	O
well	O
.	O
this	O
is	O
rather	O
a	O
modest	O
number	O
.	O
also	O
,	O
the	O
set	O
of	O
dataset	O
descriptors	O
used	O
may	O
not	O
be	O
optimal	O
.	O
we	O
could	O
thus	O
expect	O
that	O
the	O
rules	O
generated	O
capture	O
a	O
mixture	O
of	O
relevant	O
and	O
fortuitous	O
regularities	O
.	O
w	O
l	O
w	O
	O
	O
l	O
:	O
	O
sec	O
.	O
10.6	O
]	O
rule	O
based	O
advice	O
on	O
algorithm	O
application	O
203	O
in	O
order	O
to	O
strengthen	O
our	O
conﬁdence	O
in	O
the	O
results	O
we	O
have	O
decided	O
to	O
evaluate	O
the	O
rules	O
generated	O
.	O
our	O
aim	O
was	O
to	O
determine	O
whether	O
the	O
rules	O
could	O
actually	O
be	O
used	O
to	O
make	O
useful	O
predictions	O
concerning	O
its	O
applicability	O
.	O
we	O
have	O
adopted	O
a	O
leave-one-out	O
procedure	O
and	O
applied	O
it	O
to	O
datasets	O
,	O
such	O
as	O
the	O
one	O
shown	O
in	O
table	O
10.13.	O
following	O
this	O
procedure	O
,	O
we	O
used	O
all	O
but	O
one	O
items	O
in	O
training	O
,	O
while	O
the	O
remaining	O
item	O
was	O
used	O
for	O
testing	O
.	O
of	O
course	O
,	O
the	O
set	O
of	O
rules	O
generated	O
in	O
each	O
pass	O
could	O
be	O
slightly	O
different	O
,	O
but	O
the	O
form	O
of	O
the	O
rules	O
was	O
not	O
our	O
primary	O
interest	O
here	O
.	O
we	O
were	O
interested	O
to	O
verify	O
how	O
successful	O
the	O
rules	O
were	O
in	O
predicting	O
the	O
applicability	O
(	O
or	O
non-applicability	O
)	O
of	O
the	O
algorithm	O
.	O
let	O
us	O
analyse	O
an	O
example	B
.	O
consider	O
,	O
for	O
example	B
,	O
the	O
problem	O
of	O
predicting	O
the	O
applicability	O
of	O
cart	O
.	O
this	O
can	O
be	O
characterised	O
using	O
confusion	O
matrices	O
,	O
such	O
as	O
the	O
ones	O
shown	O
in	O
figure	O
10.10	O
,	O
showing	O
results	O
relative	O
to	O
the	O
error	O
margin	O
k=16	O
.	O
note	O
that	O
an	O
extra	O
(	O
simulated	O
)	O
dataset	O
has	O
been	O
used	O
in	O
the	O
following	O
calculations	O
and	O
tables	O
,	O
which	O
is	O
why	O
the	O
sum	O
is	O
now	O
22.	O
appl	O
non-appl	O
appl	O
11	O
1	O
non-appl	O
2	O
8	O
fig	O
.	O
10.10	O
:	O
evaluation	O
of	O
the	O
meta-rules	O
concerning	O
applicability	O
of	O
cart	O
.	O
the	O
rows	O
represent	O
the	O
true	O
class	O
,	O
and	O
the	O
columns	O
the	O
predicted	O
class	O
.	O
the	O
confusion	O
matrix	O
shows	O
that	O
the	O
rules	O
generated	O
were	O
capable	O
of	O
correctly	O
predicting	O
the	O
applicability	O
of	O
cart	O
on	O
an	O
unseen	O
dataset	O
in	O
11	O
cases	O
.	O
incorrect	O
prediction	O
was	O
made	O
only	O
in	O
1	O
case	O
.	O
similarly	O
,	O
if	O
we	O
consider	O
non-applicability	O
,	O
we	O
see	O
that	O
correct	O
prediction	O
is	O
made	O
in	O
8	O
cases	O
,	O
and	O
incorrect	O
one	O
in	O
2.	O
this	O
gives	O
a	O
rather	O
good	O
overall	O
success	O
rate	O
of	O
86	O
%	O
.	O
we	O
notice	O
that	O
success	O
rate	O
is	O
not	O
an	O
ideal	O
measure	B
,	O
however	O
.	O
as	O
the	O
margin	O
of	O
larger	O
)	O
,	O
more	O
cases	O
will	O
get	O
classiﬁed	O
as	O
applicable	O
.	O
if	O
we	O
consider	O
an	O
extreme	O
case	O
,	O
when	O
the	O
margin	O
covers	O
all	O
algorithms	O
,	O
we	O
will	O
get	O
an	O
apparent	O
success	O
rate	O
of	O
100	O
%	O
.	O
of	O
course	O
we	O
are	O
not	O
interested	O
in	O
such	O
a	O
useless	O
procedure	O
!	O
this	O
apparent	O
paradox	O
can	O
be	O
resolved	O
by	O
adopting	O
the	O
measure	B
called	O
information	O
score	O
(	O
is	O
)	O
(	O
kononenko	O
&	O
bratko	O
,	O
1991	O
)	O
in	O
the	O
evaluation	O
.	O
this	O
measure	B
takes	O
into	O
account	O
prior	O
probabilities	O
.	O
the	O
information	O
score	O
associated	O
with	O
a	O
deﬁnite	O
positive	O
classiﬁcation	B
represents	O
the	O
prior	O
probability	O
of	O
class	O
c.	O
the	O
information	O
scores	O
can	O
be	O
used	O
to	O
weigh	O
all	O
classiﬁer	B
answers	O
.	O
in	O
our	O
case	O
we	O
have	O
two	O
classes	O
appl	O
and	O
non-appl	O
.	O
the	O
weights	O
can	O
be	O
represented	O
conveniently	O
in	O
the	O
form	O
of	O
an	O
information	O
score	O
matrix	O
as	O
shown	O
in	O
figure	O
10.11.	O
applicability	O
is	O
extended	O
(	O
by	O
making	O
log	O
j6û	O
n	O
,	O
where	O
j3û	O
n	O
is	O
deﬁned	O
asò	O
log	O
j	O
appln	O
logjñcòaj	O
applnn	O
fig	O
.	O
10.11	O
:	O
information	O
score	O
matrix	O
.	O
the	O
rows	O
represent	O
the	O
true	O
class	O
,	O
and	O
the	O
columns	O
the	O
predicted	O
class	O
.	O
non-	O
appl	O
logj=ñò9	O
j	O
non-applnn	O
log	O
j	O
non-	O
appln	O
appl	O
appl	O
non-appl	O
the	O
information	O
scores	O
can	O
be	O
used	O
to	O
calculate	O
the	O
total	O
information	O
provided	O
by	O
a	O
rule	O
ò	O
ò	O
ò	O
ò	O
204	O
analysis	O
of	O
results	O
[	O
ch	O
.	O
10	O
on	O
the	O
given	O
dataset	O
.	O
this	O
can	O
be	O
done	O
simply	O
by	O
multiplying	O
each	O
element	O
of	O
the	O
confusion	O
matrix	O
by	O
the	O
corresponding	O
element	O
of	O
the	O
information	O
score	O
matrix	O
.	O
quencies	O
.	O
if	O
we	O
consider	O
the	O
frequency	O
of	O
appl	O
and	O
non-appl	O
for	O
all	O
algorithms	O
(	O
irrespective	O
of	O
the	O
algorithm	O
in	O
question	O
)	O
,	O
we	O
get	O
a	O
kind	O
of	O
absolute	O
reference	O
point	O
.	O
this	O
enables	O
us	O
to	O
make	O
comparisons	O
right	O
across	O
different	O
algorithms	O
.	O
the	O
quantities	O
j	O
appln	O
and	O
j	O
non-appln	O
can	O
be	O
estimated	O
from	O
the	O
appropriate	O
fre-	O
for	O
example	B
,	O
for	O
the	O
value	O
of	O
log	O
j	O
appln	O
we	O
consider	O
a	O
dataset	O
consisting	O
of	O
506	O
cases	O
(	O
23	O
algorithms	O
	O
the	O
information	O
associated	O
withò	O
¡2j	O
appln	O
µñ	O
.	O
similarly	O
,	O
the	O
isò	O
	O
value	O
ofò	O
logjñ££éó	O
isò	O
n	O
<	O
logj	O
non-appln	O
ñ	O
)	O
,	O
the	O
examples	O
of	O
applicable	O
cases	O
are	O
relatively	O
common	O
.	O
of	O
applicability	O
of	O
	O
#	O
consequently	O
,	O
the	O
information	O
concerning	O
applicability	O
has	O
a	O
somewhat	O
smaller	O
weight	O
(	O
.721	O
)	O
than	O
the	O
information	O
concerning	O
non-applicability	O
(	O
1.346	O
)	O
.	O
if	O
we	O
multiply	O
the	O
elements	O
of	O
the	O
confusion	O
matrix	O
for	O
cart	O
by	O
the	O
corresponding	O
we	O
notice	O
that	O
due	O
to	O
the	O
distribution	O
of	O
this	O
data	O
(	O
given	O
by	O
a	O
relatively	O
large	O
margin	O
22	O
datasets	O
)	O
.	O
as	O
it	O
happens	O
307	O
cases	O
fall	O
into	O
the	O
class	O
appl	O
.	O
elements	O
of	O
the	O
information	O
score	O
matrix	O
we	O
get	O
the	O
matrix	O
shown	O
in	O
figure	O
10.12	O
.	O
¢	O
logj	O
¤	O
	O
.	O
no	O
/	O
¢	O
	O
appl	O
non-appl	O
appl	O
7.93	O
0.72	O
non-appl	O
2.69	O
10.77	O
fig	O
.	O
10.12	O
:	O
adjusted	O
confusion	O
matrix	O
for	O
cart	O
.	O
the	O
rows	O
represent	O
the	O
true	O
class	O
,	O
and	O
the	O
columns	O
the	O
predicted	O
class	O
.	O
this	O
matrix	O
is	O
in	O
a	O
way	O
similar	O
to	O
the	O
confusion	O
matrix	O
shown	O
earlier	O
with	O
the	O
exception	O
that	O
the	O
error	O
counts	O
have	O
been	O
weighted	O
by	O
the	O
appropriate	O
information	O
scores	O
.	O
to	O
obtain	O
an	O
estimate	O
of	O
the	O
average	O
information	O
relative	O
to	O
one	O
case	O
,	O
we	O
need	O
to	O
divide	O
all	O
elements	O
by	O
the	O
number	O
of	O
cases	O
considered	O
(	O
i.e	O
.	O
22	O
)	O
.	O
this	O
way	O
we	O
get	O
the	O
scaled	O
matrix	O
in	O
figure	O
10.13.	O
appl	O
non-appl	O
appl	O
0.360	O
0.033	O
non-appl	O
0.122	O
0.489	O
bits	O
.	O
¤2¥	O
/	O
/	O
	O
¢	O
bits	O
.	O
1	O
fig	O
.	O
10.13	O
:	O
rescaled	O
adjusted	O
confusion	O
matrix	O
for	O
cart	O
.	O
this	O
information	O
provided	O
by	O
the	O
classiﬁcation	B
of	O
appl	O
is1	O
	O
the	O
information	O
provided	O
by	O
classiﬁcation	B
of	O
non-appl	O
is	O
similarly1	O
this	O
information	O
obtained	O
in	O
the	O
manner	O
described	O
can	O
be	O
compared	O
to	O
the	O
information	O
provided	O
by	O
a	O
default	O
rule	O
.	O
this	O
can	O
be	O
calculated	O
simply	O
as	O
follows	O
.	O
first	O
we	O
need	O
to	O
decide	O
whether	O
the	O
algorithm	O
should	O
be	O
applicable	O
or	O
non-applicable	O
by	O
default	O
.	O
this	O
is	O
quite	O
simple	O
.	O
we	O
just	O
look	O
for	O
the	O
classiﬁcation	B
which	O
provides	O
us	O
with	O
the	O
highest	O
information	O
.	O
/	O
	O
ñ	O
/	O
¢	O
ñn0ó	O
is	O
because	O
the	O
information	O
associated	O
with	O
this	O
default	O
isjñ	O
1	O
ñ	O
which	O
is	O
greater	O
than	O
the	O
information	O
associated	O
with	O
the	O
converse	O
rule	O
(	O
i.e	O
.	O
that	O
if	O
we	O
consider	O
the	O
previous	O
example	B
,	O
the	O
class	O
appl	O
is	O
the	O
correct	O
default	O
for	O
cart	O
.	O
this	O
ò'£	O
	O
µñ	O
/	O
¢	O
ó	O
ñ	O
	O
	O
ò	O
	O
£	O
ò	O
	O
	O
¢	O
	O
	O
ñ	O
	O
sec	O
.	O
10.6	O
]	O
cart	O
is	O
non-appl	O
)	O
.	O
rule	O
based	O
advice	O
on	O
algorithm	O
application	O
205	O
how	O
can	O
we	O
decide	O
whether	O
the	O
rules	O
involved	O
in	O
classiﬁcation	B
are	O
actually	O
useful	O
?	O
this	O
is	O
quite	O
straightforward	O
.	O
a	O
rule	O
can	O
be	O
considered	O
useful	O
if	O
it	O
provides	O
us	O
with	O
more	O
information	O
than	O
the	O
default	O
.	O
if	O
we	O
come	O
back	O
to	O
our	O
example	B
,	O
we	O
see	O
that	O
the	O
classiﬁcation	B
for	O
appl	O
provides	O
us	O
with	O
.327	O
bits	O
,	O
while	O
the	O
default	O
classiﬁcation	B
provides	O
only	O
.131	O
bits	O
.	O
this	O
indicates	O
that	O
the	O
rules	O
used	O
in	O
the	O
classiﬁcation	B
are	O
more	O
informative	O
than	O
the	O
default	O
.	O
in	O
consequence	O
,	O
the	O
actual	O
rule	O
should	O
be	O
kept	O
and	O
the	O
default	O
rule	O
discarded	O
.	O
10.6.4	O
rules	O
generated	O
in	O
metalevel	O
learning	O
figure	O
10.14	O
contains	O
some	O
rules	O
generated	O
using	O
the	O
method	O
described	O
.	O
as	O
we	O
have	O
not	O
used	O
a	O
uniform	B
notion	O
of	O
applicability	O
throughout	O
,	O
each	O
rule	O
is	O
qualiﬁed	O
by	O
additional	O
represents	O
the	O
concept	O
of	O
applicability	O
derived	O
on	O
the	O
each	O
rule	O
also	O
shows	O
the	O
information	O
score	O
.	O
this	O
parameter	O
gives	O
an	O
estimate	O
of	O
the	O
usefulness	O
of	O
each	O
rule	O
.	O
the	O
rules	O
presented	O
could	O
be	O
supplemented	O
by	O
another	O
set	O
generated	O
on	O
the	O
basis	O
of	O
the	O
worst	O
error	O
rate	O
(	O
i.e	O
.	O
the	O
error	O
rate	O
associated	O
with	O
the	O
choice	O
of	O
most	O
information	O
.	O
the	O
symbol	O
appl¦o§	O
basis	O
of	O
the	O
best	O
error	O
rate	O
.	O
in	O
case	O
of	O
appl¦¨©	O
the	O
interval	O
of	O
applicability	O
isj	O
best	O
error	O
rate	O
,	O
best	O
error	O
rate	O
+	O
16	O
std	O
’	O
sn	O
and	O
the	O
interval	O
of	O
non-applicability	O
isj	O
best	O
error	O
rate	O
+	O
16	O
std	O
’	O
s	O
,	O
1n	O
.	O
the	O
interval	O
of	O
applicability	O
isj	O
best	O
error	O
common	O
class	O
or	O
worse	O
)	O
.	O
in	O
the	O
case	O
of	O
applªy	O
«	O
rate	O
,	O
default	O
error	O
rate	O
-	O
8	O
std	O
’	O
sn	O
and	O
the	O
interval	O
of	O
non-applicability	O
isj	O
default	O
error	O
rate	O
-	O
8	O
std	O
’	O
s	O
,	O
1n	O
.	O
recognised	O
(	O
they	O
do	O
not	O
have	O
any	O
conditions	O
on	O
the	O
right	O
hand	O
side	O
of	O
“	O
	O
minimally	O
useful	O
(	O
with	O
information	O
scoreg	O
a	O
few	O
more	O
rules	O
which	O
are	O
a	O
bit	O
less	O
informative	O
(	O
with	O
inf	O
.	O
scores	O
down	O
to	O
cart	O
are	O
also	O
shown	O
,	O
as	O
these	O
were	O
discussed	O
earlier	O
.	O
in	O
the	O
implemented	O
system	O
we	O
use	O
each	O
rule	O
included	O
shows	O
also	O
the	O
normalised	O
information	O
score	O
.	O
this	O
parameter	O
gives	O
an	O
estimate	O
of	O
the	O
usefulness	O
of	O
each	O
rule	O
.	O
only	O
those	O
rules	O
that	O
could	O
be	O
considered	O
	O
)	O
have	O
been	O
included	O
here	O
.	O
all	O
rules	O
for	O
the	O
set	O
of	O
rules	O
generated	O
includes	O
a	O
number	O
of	O
“	O
default	O
rules	O
”	O
which	O
can	O
be	O
easily	O
”	O
)	O
.	O
	O
)	O
.	O
discussion	O
the	O
problem	O
of	O
learning	O
rules	O
for	O
all	O
algorithms	O
simultaneously	O
is	O
formidable	O
.	O
we	O
want	O
to	O
obtain	O
a	O
sufﬁcient	O
number	O
rules	O
to	O
qualify	O
each	O
algorithm	O
.	O
to	O
limit	O
the	O
complexity	O
of	O
the	O
problem	O
we	O
have	O
considered	O
one	O
algorithm	O
at	O
a	O
time	O
.	O
this	O
facilitated	O
the	O
construction	O
of	O
rules	O
.	O
considering	O
that	O
the	O
problem	O
is	O
difﬁcult	O
,	O
what	O
conﬁdence	O
can	O
we	O
have	O
that	O
the	O
rules	O
generated	O
are	O
minimally	O
sensible	O
?	O
one	O
possibility	O
is	O
to	O
try	O
to	O
evaluate	O
the	O
rules	O
,	O
by	O
checking	O
whether	O
they	O
are	O
capable	O
of	O
giving	O
useful	O
predictions	O
.	O
this	O
is	O
what	O
we	O
have	O
done	O
in	O
one	O
of	O
the	O
earlier	O
sections	O
.	O
note	O
that	O
measuring	O
simply	O
the	O
success	O
rate	O
has	O
the	O
disadvantage	O
that	O
it	O
does	O
not	O
distinguish	O
between	O
predictions	O
that	O
are	O
easy	O
to	O
make	O
,	O
and	O
those	O
that	O
are	O
more	O
difﬁcult	O
.	O
this	O
is	O
why	O
we	O
have	O
evaluated	O
the	O
rules	O
by	O
examining	O
how	O
informative	O
they	O
are	O
in	O
general	O
.	O
for	O
example	B
,	O
if	O
we	O
examine	O
the	O
rules	O
for	O
the	O
applicability	O
of	O
cart	O
we	O
observe	O
that	O
the	O
rules	O
provide	O
us	O
with	O
useful	O
information	O
if	O
invoked	O
.	O
these	O
measures	O
indicate	O
that	O
the	O
rules	O
generated	O
can	O
indeed	O
provide	O
us	O
with	O
useful	O
information	O
.	O
instead	O
of	O
evaluating	O
rules	O
in	O
the	O
way	O
shown	O
,	O
we	O
could	O
present	O
them	O
to	O
some	O
expert	O
to	O
see	O
if	O
he	O
would	O
ﬁnd	O
them	O
minimally	O
sensible	O
.	O
on	O
a	O
quick	O
glance	O
the	O
condition	O
“	O
n	O
6435	O
”	O
ñ	O
206	O
analysis	O
of	O
results	O
[	O
ch	O
.	O
10	O
statistical	B
algorithms	O
:	O
decision	O
tree	O
and	O
rule	O
algorithms	O
:	O
	O
n	O
4999	O
,	O
kurtosisg	O
2.92	O
	O
n	O
6435	O
,	O
skewg	O
0.57	O
	O
ng	O
6435	O
	O
k	O
7	O
	O
ng	O
768	O
	O
ng	O
1000	O
	O
n	O
1000	O
	O
ng	O
1000	O
kg	O
4	O
	O
n	O
1000	O
	O
n	O
3186	O
kg	O
4	O
c4.5-appl¦¬©	O
newid-appl¦¬©	O
ï	O
-non-appl¦y	O
«	O
ú	O
%	O
û	O
cart-appl¦y	O
«	O
cart-appl¦¬©	O
cart-non-appl¦¬©	O
indcart-appl¦¬©	O
cal5-appl¦¨©	O
cn2-appl¦¨©	O
itrule-non-appl¦y	O
«	O
itrule-non-appl¦¬©	O
discrim-appl¦	O
«	O
discrim-non-appl¦	O
«	O
discrim-non-appl¦¨©	O
quadisc-appl¦y	O
«	O
logdisc-appl¦	O
«	O
logdisc-non-appl¦¨©	O
alloc80-appl¦y	O
«	O
alloc80-appl¦¬©	O
k-nn-appl¦¬©	O
bayes-non-appl¦y	O
«	O
bayes-non-appl¦¬=©	O
baytree-appl¦¬=©	O
k	O
7	O
baytree-non-appl¦¬=©	O
kg	O
7	O
castle-non-appl¦y	O
«	O
	O
ng	O
768	O
,	O
cost	O
0	O
castle-non-appl¦¬=©­	O
bin.att	O
0	O
dipol92-appl¦y	O
«	O
dipol92-appl¦¬©	O
rbf-non-appl¦	O
«	O
lvq-appl¦¬©	O
backprop-appl¦	O
«	O
kohonen-non-appl¦y	O
«	O
cascade-non-appl¦	O
«	O
cascade-non-appl¦¨©	O
neural	O
network	O
algorithms	O
:	O
	O
n	O
3000	O
fig	O
.	O
10.14	O
:	O
some	O
rules	O
generated	O
in	O
meta-level	O
learning	O
.	O
inf	O
.	O
score	O
.477	O
.609	O
.447	O
.186	O
.328	O
.367	O
.384	O
.524	O
.702	O
.549	O
.917	O
.247	O
.452	O
.367	O
.309	O
.495	O
.367	O
.406	O
.797	O
.766	O
.418	O
.705	O
.557	O
.305	O
.420	O
.734	O
.341	O
.544	O
.401	O
.498	O
.495	O
.641	O
.706	O
.866	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
sec	O
.	O
10.6	O
]	O
rule	O
based	O
advice	O
on	O
algorithm	O
application	O
207	O
is	O
a	O
bit	O
puzzling	O
.	O
why	O
should	O
cart	O
perform	O
reasonably	O
well	O
,	O
if	O
the	O
number	O
of	O
examples	O
is	O
less	O
than	O
this	O
number	O
?	O
is	O
necessary	O
to	O
note	O
that	O
the	O
condition	O
“	O
n	O
obviously	O
,	O
as	O
the	O
rules	O
were	O
generated	O
on	O
the	O
basis	O
of	O
a	O
relatively	O
small	O
number	O
of	O
examples	O
,	O
the	O
rules	O
could	O
contain	O
some	O
fortuitous	O
features	O
.	O
of	O
course	O
,	O
unless	O
we	O
have	O
more	O
data	O
available	O
it	O
is	O
difﬁcult	O
to	O
point	O
out	O
which	O
features	O
are	O
or	O
are	O
not	O
relevant	O
.	O
however	O
,	O
it	O
6435	O
”	O
is	O
not	O
an	O
absolute	O
one	O
.	O
rules	O
should	O
not	O
be	O
simply	O
interpreted	O
as	O
-	O
“	O
the	O
algorithm	O
performs	O
well	O
if	O
such	O
and	O
such	O
condition	O
is	O
satisﬁed	O
”	O
.	O
the	O
correct	O
interpretation	O
is	O
something	O
like	O
-	O
“	O
the	O
algorithm	O
is	O
likely	O
to	O
compete	O
well	O
under	O
the	O
conditions	O
stated	O
,	O
provided	O
no	O
other	O
more	O
informative	O
rule	O
applies	O
”	O
.	O
this	O
view	O
helps	O
also	O
to	O
understand	O
better	O
the	O
rule	O
for	O
discrim	O
algorithm	O
generated	O
by	O
the	O
system	O
.	O
	O
®¯	O
discrim-appl	O
the	O
condition	O
“	O
n	O
1000	O
”	O
does	O
not	O
express	O
all	O
the	O
conditions	O
of	O
applicability	O
of	O
algorithm	O
discrim	O
,	O
and	O
could	O
appear	O
rather	O
strange	O
.	O
however	O
,	O
the	O
condition	O
does	O
make	O
sense	O
.	O
some	O
algorithms	O
have	O
a	O
faster	O
learning	O
rate	O
than	O
others	O
.	O
these	O
algorithms	O
compete	O
well	O
with	O
others	O
,	O
provided	O
the	O
number	O
of	O
examples	O
is	O
small	O
.	O
the	O
fast	O
learning	O
algorithms	O
may	O
however	O
be	O
overtaken	O
by	O
others	O
later	O
.	O
experiments	O
with	O
learning	B
curves	I
on	O
the	O
satellite	B
image	I
dataset	O
show	O
that	O
the	O
discrim	O
algorithm	O
is	O
among	O
the	O
ﬁrst	O
six	O
algorithms	O
in	O
terms	O
of	O
error	O
rate	O
as	O
long	O
as	O
the	O
number	O
of	O
examples	O
is	O
relatively	O
small	O
(	O
100	O
,	O
200	O
etc.	O
)	O
.	O
this	O
algorithm	O
seems	O
to	O
pick	O
up	O
quickly	O
what	O
is	O
relevant	O
and	O
so	O
we	O
could	O
say	O
,	O
it	O
competes	O
well	O
under	O
these	O
conditions	O
.	O
when	O
the	O
number	O
of	O
examples	O
is	O
larger	O
,	O
however	O
,	O
discrim	O
is	O
overtaken	O
by	O
other	O
algorithms	O
.	O
with	O
the	O
full	O
training	O
set	O
of	O
6400	O
examples	O
discrim	O
is	O
in	O
19th	O
place	O
in	O
the	O
ranking	O
.	O
this	O
is	O
consistent	O
with	O
the	O
rule	O
generated	O
by	O
our	O
system	O
.	O
the	O
condition	O
generated	O
by	O
the	O
system	O
is	O
not	O
so	O
puzzling	O
as	O
it	O
seems	O
at	O
ﬁrst	O
glance	O
!	O
there	O
is	O
of	O
course	O
a	O
well	O
recognised	O
problem	O
that	O
should	O
be	O
tackled	O
.	O
many	O
conditions	O
contain	O
numeric	O
tests	O
which	O
are	O
either	O
true	O
or	O
false	O
.	O
it	O
does	O
not	O
make	O
sense	O
to	O
consider	O
the	O
discrim	O
algorithm	O
applicable	O
if	O
the	O
number	O
of	O
examples	O
is	O
less	O
than	O
1000	O
,	O
and	O
inapplicable	O
,	O
if	O
this	O
number	O
is	O
just	O
a	O
bit	O
more	O
.	O
a	O
more	O
ﬂexible	O
approach	O
is	O
needed	O
(	O
for	O
example	B
using	O
ﬂexible	O
matching	O
)	O
.	O
10.6.5	O
application	O
assistant	O
rules	O
generated	O
in	O
the	O
way	O
described	O
permit	O
us	O
to	O
give	O
recommendations	O
as	O
to	O
which	O
classiﬁcation	B
algorithm	O
could	O
be	O
used	O
with	O
a	O
given	O
dataset	O
.	O
this	O
is	O
done	O
with	O
the	O
help	O
of	O
a	O
kind	O
of	O
expert	O
system	O
called	O
an	O
application	O
assistant	O
(	O
aplas	O
)	O
.	O
this	O
system	O
contains	O
a	O
knowledge	O
base	O
which	O
is	O
interpreted	O
by	O
an	O
interpreter	O
.	O
the	O
knowledge	O
base	O
contains	O
all	O
the	O
rules	O
shown	O
in	O
the	O
previous	O
section	O
.	O
the	O
interpreter	O
is	O
quite	O
standard	O
,	O
but	O
uses	O
a	O
particular	O
method	O
for	O
resolution	O
of	O
conﬂicts	O
.	O
we	O
notice	O
that	O
the	O
knowledge	O
base	O
may	O
contain	O
potentially	O
conﬂicting	O
rules	O
.	O
in	O
general	O
several	O
rules	O
may	O
apply	O
,	O
some	O
of	O
which	O
may	O
recommend	O
the	O
use	O
of	O
a	O
particular	O
algorithm	O
while	O
others	O
may	O
be	O
against	O
it	O
.	O
some	O
people	O
believe	O
that	O
knowledge	O
bases	O
should	O
always	O
be	O
cleaned	O
up	O
so	O
that	O
such	O
situations	O
would	O
not	O
arise	O
.	O
this	O
would	O
amount	O
to	O
obliterating	O
certain	O
potentially	O
useful	O
information	O
and	O
so	O
we	O
prefer	O
to	O
deal	O
with	O
the	O
problem	O
in	O
the	O
following	O
way	O
.	O
for	O
every	O
algorithm	O
we	O
consider	O
all	O
the	O
rules	O
satisfying	O
the	O
conditions	O
and	O
sum	O
all	O
the	O
information	O
scores	O
.	O
the	O
information	O
scores	O
associated	O
with	O
the	O
recommendation	O
to	O
apply	O
ñ	O
208	O
analysis	O
of	O
results	O
[	O
ch	O
.	O
10	O
an	O
algorithm	O
are	O
taken	O
with	O
a	O
positive	O
sign	O
,	O
the	O
others	O
with	O
a	O
negative	O
one	O
.	O
for	O
example	B
,	O
if	O
we	O
get	O
a	O
recommendation	O
to	O
apply	O
an	O
algorithm	O
with	O
an	O
indication	O
that	O
this	O
is	O
apparently	O
0.5	O
bits	O
worth	O
,	O
and	O
if	O
we	O
also	O
get	O
an	O
opposite	O
recommendation	O
(	O
i.e	O
.	O
not	O
to	O
apply	O
this	O
algorithm	O
)	O
with	O
an	O
indication	O
that	O
this	O
is	O
0.2	O
bits	O
worth	O
,	O
we	O
will	O
go	O
ahead	O
with	O
the	O
recommendation	O
,	O
but	O
decrease	O
the	O
information	O
score	O
accordingly	O
(	O
i.e	O
.	O
to	O
0.3	O
bits	O
)	O
.	O
the	O
output	B
of	O
this	O
phase	O
is	O
a	O
list	O
of	O
algorithms	O
accompanied	O
by	O
their	O
associated	O
overall	O
information	O
scores	O
.	O
a	O
positive	O
score	O
can	O
be	O
interpreted	O
as	O
an	O
argument	O
to	O
apply	O
the	O
algorithm	O
.	O
a	O
negative	O
score	O
can	O
be	O
interpreted	O
as	O
an	O
argument	O
against	O
the	O
application	O
of	O
the	O
algorithm	O
.	O
moreover	O
,	O
the	O
higher	O
the	O
score	O
,	O
the	O
more	O
informative	O
is	O
the	O
recommendation	O
in	O
general	O
.	O
the	O
information	O
score	O
can	O
be	O
then	O
considered	O
as	O
a	O
strength	O
of	O
the	O
recommendation	O
.	O
the	O
recommendations	O
given	O
are	O
of	O
course	O
not	O
perfect	O
.	O
they	O
do	O
not	O
guarantee	O
that	O
the	O
ﬁrst	O
algorithm	O
in	O
the	O
recommendation	O
ordering	O
will	O
have	O
the	O
best	O
performance	O
in	O
reality	O
.	O
however	O
,	O
our	O
results	O
demonstrate	O
that	O
the	O
algorithms	O
accompanied	O
by	O
a	O
strong	O
recommendation	O
do	O
perform	O
quite	O
well	O
in	O
general	O
.	O
the	O
opposite	O
is	O
also	O
true	O
.	O
the	O
algorithms	O
that	O
have	O
not	O
been	O
recommended	O
have	O
a	O
poorer	O
performance	O
in	O
general	O
.	O
in	O
other	O
words	O
,	O
we	O
observe	O
that	O
there	O
is	O
a	O
reasonable	O
degree	O
of	O
correlation	O
between	O
the	O
recommendation	O
and	O
the	O
actual	O
test	O
results	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
10.15	O
which	O
shows	O
the	O
recommendations	O
generated	O
for	O
one	O
particular	O
dataset	O
(	O
letters	O
)	O
.	O
)	O
%	O
(	O
t	O
e	O
a	O
r	O
s	O
s	O
e	O
c	O
c	O
u	O
s	O
5	O
9	O
0	O
9	O
5	O
8	O
0	O
8	O
5	O
7	O
0	O
7	O
5	O
6	O
k-nn	O
lvq	O
alloc80	O
baytree	O
quadisc	O
indcartc4.5	O
cn2	O
newid	O
dipol92	O
castle	O
kohonen	O
logdisc	O
rbf	O
ac2	O
cal5	O
discrim	O
smart	O
backprop	O
-0.6	O
-0.3	O
0	O
0.3	O
0.6	O
information	O
score	O
(	O
bits	O
)	O
fig	O
.	O
10.15	O
:	O
recommendations	O
of	O
the	O
application	O
assistant	O
for	O
the	O
letters	O
dataset	O
.	O
the	O
recommendations	O
were	O
generated	O
on	O
the	O
basis	O
of	O
a	O
rules	O
set	O
similar	O
to	O
the	O
one	O
shown	O
in	O
figure	O
10.14	O
(	O
the	O
rule	O
set	O
included	O
just	O
a	O
few	O
more	O
rules	O
with	O
lower	O
information	O
scores	O
)	O
.	O
the	O
top	O
part	O
shows	O
the	O
algorithms	O
with	O
high	O
success	O
rates	O
.	O
the	O
algorithms	O
on	O
the	O
right	O
are	O
accompanied	O
by	O
a	O
strong	O
recommendation	O
concerning	O
applicability	O
.	O
we	O
notice	O
that	O
sec	O
.	O
10.6	O
]	O
rule	O
based	O
advice	O
on	O
algorithm	O
application	O
209	O
several	O
algorithms	O
with	O
high	O
success	O
rates	O
apear	O
there	O
.	O
the	O
algorithm	O
that	O
is	O
accompanied	O
by	O
the	O
strongest	O
reccomendation	O
for	O
this	O
dataset	O
is	O
alloc80	O
(	O
information	O
score	O
=	O
0.663	O
bits	O
)	O
.	O
this	O
algorithm	O
has	O
also	O
the	O
highest	O
success	O
rate	O
of	O
93.6	O
%	O
.	O
the	O
second	O
place	O
in	O
the	O
ordering	O
of	O
algorithms	O
recommended	O
is	O
k-nn	O
shared	O
by	O
k-nn	O
and	O
dipol92	O
.	O
we	O
note	O
that	O
k-nn	O
is	O
a	O
very	O
good	O
choice	O
,	O
while	O
dipol92	O
is	O
not	O
too	O
bad	O
either	O
.	O
the	O
correlation	O
between	O
the	O
information	O
score	O
and	O
success	O
rate	O
could	O
,	O
of	O
course	O
,	O
be	O
better	O
.	O
the	O
algorithm	O
castle	O
is	O
given	O
somewhat	O
too	O
much	O
weight	O
,	O
while	O
baytree	O
which	O
is	O
near	O
the	O
top	O
is	O
somewhat	O
undervalued	O
.	O
the	O
correlation	O
could	O
be	O
improved	O
,	O
in	O
the	O
ﬁrst	O
place	O
,	O
by	O
obtaining	O
more	O
test	O
results	O
.	O
the	O
results	O
could	O
also	O
be	O
improved	O
by	O
incorporating	O
a	O
better	O
method	O
for	O
combining	O
rules	O
and	O
the	O
corresponding	O
information	O
scores	O
.	O
it	O
would	O
be	O
beneﬁcial	O
to	O
consider	O
also	O
other	O
potentially	O
useful	O
sets	O
of	O
rules	O
,	O
including	O
the	O
ones	O
generated	O
on	O
the	O
basis	O
of	O
other	O
values	O
of	O
k	O
,	O
or	O
even	O
different	O
categorisation	O
schemes	O
.	O
for	O
example	B
,	O
all	O
algorithms	O
with	O
a	O
performance	O
near	O
the	O
default	O
rule	O
could	O
be	O
considered	O
non-applicable	O
,	O
while	O
all	O
others	O
could	O
be	O
classiﬁed	O
as	O
applicable	O
.	O
despite	O
the	O
fact	O
that	O
there	O
is	O
room	O
for	O
possible	O
improvements	O
,	O
the	O
application	O
assistant	O
seems	O
to	O
produce	O
promising	O
results	O
.	O
the	O
user	O
can	O
get	O
a	O
recommendation	O
as	O
to	O
which	O
algorithm	O
could	O
be	O
used	O
with	O
a	O
new	O
dataset	O
.	O
although	O
the	O
recommendation	O
is	O
not	O
guaranteed	O
always	O
to	O
give	O
the	O
best	O
possible	O
advice	O
,	O
it	O
narrows	O
down	O
the	O
user	O
’	O
s	O
choice	O
.	O
10.6.6	O
criticism	O
of	O
metalevel	O
learning	O
approach	O
before	O
accepting	O
any	O
rules	O
,	O
generated	O
by	O
c4.5	O
or	O
otherwise	O
,	O
it	O
is	O
wise	O
to	O
check	O
them	O
against	O
known	O
theoretical	O
and	O
empirical	O
facts	O
.	O
the	O
rules	O
generated	O
in	O
metalevel	O
learning	O
could	O
contain	O
spurious	O
rules	O
with	O
no	O
foundation	O
in	O
theory	O
.	O
if	O
the	O
rule-based	O
approach	O
has	O
shortcomings	O
,	O
how	O
should	O
we	O
proceed	O
?	O
would	O
it	O
be	O
better	O
to	O
use	O
another	O
classiﬁcation	B
scheme	O
in	O
place	O
of	O
the	O
metalevel	O
learning	O
approach	O
using	O
c4.5	O
?	O
as	O
there	O
are	O
insufﬁcient	O
data	O
to	O
construct	O
the	O
rules	O
,	O
the	O
answer	O
is	O
probably	O
to	O
use	O
an	O
interactive	O
method	O
,	O
capable	O
of	O
incorporating	O
prior	O
expert	O
knowledge	O
(	O
background	O
knowledge	O
)	O
.	O
as	O
one	O
simple	O
example	B
,	O
if	O
it	O
is	O
known	O
that	O
an	O
algorithm	O
can	O
handle	O
cost	O
matrices	O
,	O
this	O
could	O
simply	O
be	O
provided	O
to	O
is	O
the	O
system	O
.	O
as	O
another	O
example	B
,	O
the	O
knowledge	O
that	O
the	O
behaviour	O
of	O
newid	O
andú	O
%	O
û	O
ï	O
could	O
then	O
be	O
likely	O
to	O
be	O
similar	O
could	O
also	O
be	O
useful	O
to	O
the	O
system	O
.	O
the	O
rules	O
forúüû	O
constructed	O
from	O
the	O
rule	O
for	O
newid	O
,	O
by	O
adding	O
suitable	O
conditions	O
concerning	O
,	O
for	O
example	B
the	O
hierarchical	O
structure	O
of	O
the	O
attributes	O
.	O
also	O
,	O
some	O
algorithms	O
have	O
inbuilt	O
checks	O
on	O
applicability	O
,	O
such	O
as	O
linear	O
or	O
quadratic	O
discriminants	O
,	O
and	O
these	O
should	O
be	O
incorporated	O
into	O
the	O
learnt	O
rules	O
.	O
10.6.7	O
criticism	O
of	O
measures	O
some	O
of	O
the	O
statistical	B
measures	O
are	O
in	O
fact	O
more	O
complex	O
in	O
structure	O
than	O
the	O
learning	O
the	O
rules	O
.	O
for	O
example	B
,	O
the	O
programming	O
effort	O
in	O
calculating	O
sd	O
ratio	O
is	O
greater	O
than	O
that	O
in	O
establishing	O
the	O
linear	O
discriminant	O
rule	O
.	O
indeed	O
,	O
to	O
ﬁnd	O
sd	O
ratio	O
requires	O
virtually	O
all	O
the	O
quantities	O
needed	O
in	O
ﬁnding	O
the	O
quadratic	O
discriminant	O
.	O
this	O
poses	O
the	O
question	O
:	O
if	O
it	O
is	O
easier	O
to	O
run	O
,	O
say	O
linear	O
discriminants	O
and	O
newid	O
,	O
why	O
not	O
run	O
them	O
and	O
use	O
the	O
performance	O
of	O
these	O
procedures	O
as	O
yardsticks	O
by	O
which	O
to	O
judge	O
the	O
performance	O
of	O
other	O
algorithms	O
?	O
the	O
similarities	O
evident	O
in	O
the	O
empirical	O
results	O
strongly	O
suggest	O
that	O
the	O
best	O
predictor	O
for	O
logistic	O
regression	O
is	O
linear	O
discriminants	O
(	O
with	O
logistic	O
regression	O
doing	O
that	O
is	O
very	O
similar	O
to	O
newid	O
(	O
if	O
there	O
is	O
no	O
hierarchy	O
)	O
,	O
and	O
little	O
better	O
on	O
average	O
)	O
,	O
andúüû	O
ï	O
ï	O
210	O
analysis	O
of	O
results	O
[	O
ch	O
.	O
10	O
so	O
on	O
.	O
this	O
idea	O
can	O
be	O
formalised	O
as	O
we	O
indicate	O
in	O
the	O
next	O
section	O
.	O
10.7	O
prediction	O
of	O
performance	O
what	O
is	O
required	O
is	O
a	O
few	O
simple	O
yardstick	O
methods	O
,	O
readily	O
available	O
(	O
preferably	O
in	O
the	O
public	O
domain	O
)	O
,	O
that	O
can	O
be	O
run	O
quickly	O
on	O
the	O
given	O
dataset	O
.	O
we	O
also	O
need	O
a	O
set	O
of	O
rules	O
that	O
will	O
predict	O
the	O
performance	O
of	O
all	O
other	O
algorithms	O
from	O
the	O
yardstick	O
results	O
.	O
as	O
a	O
ﬁrst	O
suggestion	O
,	O
consider	O
discrim	O
,	O
indcart	O
and	O
k-nn	O
.	O
they	O
contain	O
a	O
statistical	B
,	O
a	O
decision-	O
tree	O
and	O
a	O
non-parametric	O
method	O
,	O
so	O
represent	O
the	O
main	O
strands	O
.	O
the	O
question	O
is	O
this	O
:	O
can	O
they	O
represent	O
the	O
full	O
range	O
of	O
algorithms	O
?	O
in	O
the	O
terminology	O
of	O
multidimensional	O
scaling	O
:	O
do	O
they	O
span	O
the	O
reduced	O
space	O
in	O
which	O
most	O
algorithm	O
results	O
reside	O
?	O
the	O
multi-	O
dimensional	O
scaling	O
diagram	O
in	O
figure	O
10.1	O
suggests	O
that	O
a	O
three-	O
or	O
even	O
two-dimensional	O
subspace	O
is	O
sufﬁcient	O
to	O
represent	O
all	O
results	O
.	O
to	O
give	O
a	O
few	O
examples	O
.	O
let	O
discrim	O
,	O
k-nn	O
and	O
indcart	O
represent	O
the	O
error	O
rates	O
achieved	O
by	O
the	O
respective	O
methods	O
.	O
to	O
predict	O
the	O
accuracy	O
of	O
logdisc	O
from	O
these	O
three	O
reference	O
ﬁgures	O
,	O
we	O
can	O
use	O
a	O
multiple	O
regression	O
of	O
logdisc	O
on	O
the	O
three	O
variables	O
discrim	O
,	O
k-nn	O
and	O
indcart	O
(	O
with	O
no	O
intercept	O
term	O
)	O
.	O
after	O
dropping	O
non-signiﬁcant	O
terms	O
from	O
the	O
regression	O
,	O
this	O
produces	O
the	O
formula	O
:	O
,	O
with	O
a	O
squared	O
correlation	O
coefﬁcient	O
of	O
0.921.	O
see	O
table	O
10.14	O
for	O
a	O
summary	O
of	O
the	O
regression	O
formulae	O
for	O
all	O
the	O
algorithms	O
(	O
excepting	O
discrim	O
,	O
k-nn	O
and	O
indcart	O
naturally	O
)	O
.	O
°d¡2±²³´	O
1	O
¢	O
table	O
10.14	O
:	O
predictors	O
for	O
error-rates	O
based	O
on	O
discrim	O
,	O
k-nn	O
and	O
indcart	O
.	O
£¢î	O
²³þqµ	O
1.34	O
0.79	O
0.54	O
algorithm	O
discrim	O
k-nn	O
indcart	O
r-square	O
quadisc	O
logdisc	O
smart	O
alloc80	O
castle	O
cart	O
newid	O
0.80	O
1.12	O
0.640	O
0.921	O
0.450	O
0.846	O
0.874	O
0.860	O
0.840	O
0.723	O
0.897	O
0.840	O
0.862	O
0.752	O
0.601	O
0.709	O
0.672	O
0.533	O
0.679	O
0.684	O
0.534	O
0.821	O
n	O
trials	O
22	O
22	O
22	O
21	O
21	O
15	O
22	O
22	O
22	O
22	O
22	O
22	O
20	O
22	O
18	O
22	O
20	O
6	O
22	O
21	O
0.56	O
0.74	O
1.23	O
1.12	O
0.89	O
0.79	O
1.01	O
1.17	O
0.97	O
0.53	O
0.88	O
úüû	O
baytree	O
naivebay	O
cn2	O
c4.5	O
itrule	O
cal5	O
kohonen	O
dipol92	O
bprop	O
cascade	O
rbf	O
lvq	O
1.43	O
1.87	O
0.29	O
0.78	O
0.29	O
1.24	O
1.05	O
the	O
discrim	O
coefﬁcient	O
of	O
0.79	O
in	O
the	O
logdisc	O
example	B
shows	O
that	O
logdisc	O
is	O
generally	O
about	O
21	O
%	O
more	O
accurate	O
than	O
discrim	O
,	O
and	O
that	O
the	O
performance	O
of	O
the	O
other	O
two	O
reference	O
methods	O
does	O
not	O
seem	O
to	O
help	O
in	O
the	O
prediction	O
.	O
with	O
an	O
r-squared	O
value	O
of	O
0.921	O
,	O
we	O
can	O
ï	O
sec	O
.	O
10.7	O
]	O
prediction	O
of	O
performance	O
211	O
be	O
quite	O
conﬁdent	O
that	O
logdisc	O
does	O
better	O
than	O
discrim	O
.	O
this	O
result	O
should	O
be	O
qualiﬁed	O
with	O
information	O
on	O
the	O
number	O
of	O
attributes	O
,	O
normality	O
of	O
variables	O
,	O
etc	O
,	O
–	O
and	O
these	O
are	O
quantities	O
that	O
can	O
be	O
measured	O
.	O
in	O
the	O
context	O
of	O
deciding	O
if	O
further	O
trials	O
on	O
additional	O
algorithms	O
are	O
necessary	O
,	O
take	O
the	O
example	B
of	O
the	O
shuttle	O
dataset	O
,	O
and	O
consider	O
what	O
action	O
to	O
recommend	O
after	O
discovering	O
discrim	O
=	O
4.83	O
%	O
,	O
indcart	O
=	O
0.09	O
%	O
and	O
k-nn	O
=	O
0.44	O
%	O
.	O
it	O
does	O
not	O
look	O
as	O
if	O
the	O
error	O
rates	O
of	O
either	O
logdisc	O
or	O
smart	O
will	O
get	O
anywhere	O
near	O
indcart	O
’	O
s	O
value	O
,	O
and	O
the	O
best	O
prospect	O
of	O
improvement	O
lies	O
in	O
the	O
decision	O
tree	O
methods	O
.	O
consider	O
dipol92	O
now	O
.	O
there	O
appears	O
to	O
be	O
no	O
really	O
good	O
predictor	O
,	O
as	O
the	O
r-squared	O
value	O
is	O
relatively	O
small	O
(	O
0.533	O
)	O
.	O
this	O
means	O
that	O
dipol92	O
is	O
doing	O
something	O
outside	O
the	O
scope	O
of	O
the	O
three	O
reference	O
algorithms	O
.	O
the	O
best	O
single	O
predictor	O
is	O
:	O
dipol92	O
=	O
0.29	O
discrim	O
,	O
apparently	O
indicating	O
that	O
dipol92	O
is	O
usually	O
much	O
better	O
than	O
discrim	O
(	O
although	O
not	O
so	O
much	O
better	O
that	O
it	O
would	O
challenge	O
indcart	O
’	O
s	O
good	O
value	O
for	O
the	O
shuttle	O
dataset	O
)	O
.	O
this	O
formula	O
just	O
can	O
not	O
be	O
true	O
in	O
general	O
however	O
:	O
all	O
we	O
can	O
say	O
is	O
that	O
,	O
for	O
datasets	O
around	O
the	O
size	O
in	O
statlog	O
,	O
dipol92	O
has	O
error	O
rates	O
about	O
one	O
third	O
that	O
of	O
discrim	O
,	O
but	O
considerable	O
ﬂuctuation	O
round	O
this	O
value	O
is	O
possible	O
.	O
if	O
we	O
have	O
available	O
the	O
three	O
reference	O
results	O
,	O
the	O
formula	O
would	O
suggest	O
that	O
dipol92	O
should	O
be	O
tried	O
unless	O
either	O
k-nn	O
or	O
cart	O
gets	O
an	O
accuracy	O
much	O
lower	O
than	O
a	O
third	O
of	O
discrim	O
.	O
knowing	O
the	O
structure	O
of	O
dipol92	O
we	O
can	O
predict	O
a	O
good	O
deal	O
more	O
however	O
.	O
when	O
there	O
are	O
just	O
two	O
classes	O
(	O
and	O
9	O
of	O
our	O
datasets	O
were	O
2-class	O
problems	O
)	O
,	O
and	O
if	O
dipol92	O
does	O
not	O
use	O
clustering	O
,	O
dipol92	O
is	O
very	O
similar	O
indeed	O
to	O
logistic	O
regression	O
(	O
they	O
optimise	O
on	O
slightly	O
different	O
criteria	O
)	O
.	O
so	O
the	O
best	O
predictor	O
for	O
dipol92	O
in	O
2-class	O
problems	O
with	O
no	O
obvious	O
clustering	O
will	O
be	O
logdisc	O
.	O
at	O
the	O
other	O
extreme	O
,	O
if	O
many	O
clusters	O
are	O
used	O
in	O
the	O
initial	O
stages	O
of	O
dipol92	O
,	O
then	O
the	O
performance	O
is	O
bound	O
to	O
approach	O
that	O
of	O
,	O
say	O
,	O
radial	O
basis	O
functions	O
or	O
lvq	O
.	O
also	O
,	O
while	O
on	O
the	O
subject	O
of	O
giving	O
explanations	O
for	O
differences	O
in	O
behaviour	O
,	O
consider	O
the	O
performance	O
of	O
alloc80	O
compared	O
to	O
k-nn	O
.	O
from	O
table	O
10.14	O
it	O
is	O
clear	O
that	O
al-	O
loc80	O
usually	O
outperforms	O
k-nn	O
.	O
the	O
reason	O
is	O
probably	O
due	O
to	O
the	O
mechanism	O
within	O
alloc80	O
whereby	O
irrelevant	O
attributes	O
are	O
dropped	O
,	O
or	O
perhaps	O
because	O
a	O
surrogate	O
was	O
substituted	O
for	O
alloc80	O
when	O
it	O
performed	O
badly	O
.	O
if	O
such	O
strategies	O
were	O
instituted	O
for	O
k-nn	O
,	O
it	O
is	O
probable	O
that	O
their	O
performances	O
would	O
be	O
even	O
closer	O
.	O
finally	O
,	O
we	O
should	O
warn	O
that	O
such	O
rules	O
should	O
be	O
treated	O
with	O
great	O
caution	O
,	O
as	O
we	O
have	O
already	O
suggested	O
in	O
connection	O
with	O
the	O
rules	O
derived	O
by	O
c4.5	O
.	O
it	O
is	O
especially	O
dangerous	O
to	O
draw	O
conclusions	O
from	O
incomplete	O
data	O
,	O
as	O
with	O
cart	O
for	O
example	B
,	O
for	O
the	O
reason	O
that	O
a	O
“	O
not	O
available	O
”	O
result	O
is	O
very	O
likely	O
associated	O
with	O
factors	O
leading	O
to	O
high	O
error	O
rates	O
,	O
such	O
as	O
inability	O
to	O
cope	O
with	O
large	O
numbers	O
of	O
categories	O
,	O
or	O
large	O
amounts	O
of	O
data	O
.	O
empirical	O
rules	O
such	O
as	O
those	O
we	O
have	O
put	O
forward	B
should	O
be	O
reﬁned	O
by	O
the	O
inclusion	O
of	O
other	O
factors	O
in	O
the	O
regression	O
,	O
these	O
other	O
factors	O
being	O
directly	O
related	O
to	O
known	O
properties	O
of	O
the	O
algorithm	O
.	O
for	O
example	B
,	O
to	O
predict	O
quadisc	O
,	O
a	O
term	O
involving	O
the	O
measures	O
sd	O
ratio	O
would	O
be	O
required	O
(	O
if	O
that	O
is	O
not	O
too	O
circular	O
an	O
argument	O
)	O
.	O
10.7.1	O
ml	O
on	O
ml	O
vs.	O
regression	O
two	O
methods	O
have	O
been	O
given	O
above	O
for	O
predicting	O
the	O
performance	O
of	O
algorithms	O
,	O
based	O
respectively	O
on	O
rule-based	O
advice	O
using	O
dataset	O
measures	O
(	O
ml	O
on	O
ml	O
)	O
and	O
comparison	O
with	O
reference	O
algorithms	O
(	O
regression	O
)	O
.	O
it	O
is	O
difﬁcult	O
to	O
compare	O
directly	O
the	O
success	O
rates	O
of	O
the	O
respective	O
predictions	O
,	O
as	O
the	O
former	O
is	O
stated	O
in	O
terms	O
of	O
proportion	O
of	O
correct	O
predictions	O
212	O
analysis	O
of	O
results	O
[	O
ch	O
.	O
10	O
formula	O
which	O
is	O
based	O
on	O
the	O
assumption	O
of	O
equal	O
numbers	O
of	O
non-appl	O
and	O
appl	O
:	O
and	O
the	O
latter	O
in	O
terms	O
of	O
squared	O
correlation	O
.	O
we	O
now	O
give	O
a	O
simple	O
method	O
of	O
comparing	O
from	O
the	O
predictability	O
of	O
performance	O
from	O
the	O
two	O
techniques	O
.	O
the	O
r-squared	O
value|	O
regression	O
and	O
the	O
c4.5	O
generated	O
rule	O
error	O
rate	O
¶	O
can	O
be	O
compared	O
by	O
the	O
following	O
	O
when	O
the	O
error	O
rate	O
is	O
0.5	O
,	O
as	O
pure	O
as	O
it	O
should	O
,	O
this	O
formula	O
gives	O
a	O
correlation	O
of|z	O
guesswork	O
would	O
get	O
half	O
the	O
cases	O
correct	O
.	O
to	O
give	O
an	O
example	B
in	O
using	O
this	O
formula	O
,	O
the	O
cart	O
rules	O
(	O
k=16	O
)	O
had	O
3	O
errors	O
in	O
22	O
,	O
with	O
an	O
error	O
rate	O
of	O
¶	O
·j=ñcò'¶on	O
1	O
	O
zjñcòa	O
	O
/	O
and	O
an	O
approximate	O
r-square	O
value	O
of	O
this	O
is	O
somewhat	O
less	O
than	O
the	O
value	O
(	O
|	O
of	O
this	O
section	O
.	O
1	O
£	O
/	O
¥	O
	O
)	O
obtained	O
using	O
the	O
regression	O
techniques	O
ï	O
|	O
ï	O
ï	O
	O
ñ	O
	O
	O
¤	O
|	O
ï	O
ñ	O
	O
	O
¤	O
n	O
ï	O
	O
ï	O
	O
11	O
conclusions	O
of	O
leeds	O
d.	O
michie	O
(	O
1	O
)	O
,	O
d.	O
j.	O
spiegelhalter	O
(	O
2	O
)	O
and	O
c.	O
c.taylor	O
(	O
3	O
)	O
(	O
1	O
)	O
university	O
of	O
strathclyde	O
,	O
(	O
2	O
)	O
mrc	O
biostatistics	O
unit	O
,	O
cambridge	O
and	O
(	O
3	O
)	O
university	O
11.1	O
introduction	O
in	O
this	O
chapter	O
we	O
try	O
to	O
draw	O
together	O
the	O
evidence	O
of	O
the	O
comparative	O
trials	O
and	O
subsequent	O
analyses	O
,	O
comment	O
on	O
the	O
experiences	O
of	O
the	O
users	O
of	O
the	O
algorithms	O
,	O
and	O
suggest	O
topics	O
and	O
areas	O
which	O
need	O
further	O
work	O
.	O
we	O
begin	O
with	O
some	O
comments	O
on	O
each	O
of	O
the	O
methods	O
.	O
it	O
should	O
be	O
noted	O
here	O
that	O
our	O
comments	O
are	O
often	O
directed	O
towards	O
a	O
speciﬁc	O
implementation	O
of	O
a	O
method	O
rather	O
than	O
the	O
method	O
per	O
se	O
.	O
in	O
some	O
instances	O
the	O
slowness	O
or	O
otherwise	O
poor	O
performance	O
of	O
an	O
algorithm	O
is	O
due	O
at	O
least	O
in	O
part	O
to	O
the	O
lack	O
of	O
sophistication	O
of	O
the	O
program	O
.	O
in	O
addition	O
to	O
the	O
potential	O
weakness	O
of	O
the	O
programmer	O
,	O
there	O
is	O
the	O
potential	O
reported	O
on	O
previous	O
chapters	O
were	O
based	O
on	O
a	O
version	O
programmed	O
in	O
lisp	O
.	O
a	O
version	O
is	O
now	O
available	O
in	O
the	O
c	O
language	O
which	O
cuts	O
the	O
cpu	O
time	O
by	O
a	O
factor	O
of	O
10.	O
in	O
terms	O
of	O
error	O
rates	O
,	O
observed	O
differences	O
in	O
goodness	O
of	O
result	O
can	O
arise	O
from	O
inexperience	O
of	O
the	O
user	O
.	O
to	O
give	O
an	O
example	B
,	O
the	O
trials	O
ofúüû	O
different	O
suitabilities	O
of	O
the	O
basic	O
methods	O
for	O
given	O
datasets	O
different	O
sophistications	O
of	O
default	O
procedures	O
for	O
parameter	O
settings	O
different	O
sophistication	O
of	O
the	O
program	O
user	O
in	O
selection	O
of	O
options	O
and	O
tuning	O
of	O
parameters	O
occurrence	O
and	O
effectiveness	O
of	O
pre-processing	O
of	O
the	O
data	O
by	O
the	O
user	O
1	O
.	O
2	O
.	O
3	O
.	O
4.	O
the	O
stronger	O
a	O
program	O
in	O
respect	O
of	O
2	O
,	O
then	O
the	O
better	O
buffered	O
against	O
shortcomings	O
in	O
3.	O
alternatively	O
,	O
if	O
there	O
are	O
no	O
options	O
to	O
select	O
or	O
parameters	O
to	O
tune	O
,	O
then	O
item	O
3	O
is	O
not	O
important	O
.	O
we	O
give	O
a	O
general	O
view	O
of	O
the	O
ease-of-use	O
and	O
the	O
suitable	O
applications	O
of	O
the	O
algorithms	O
.	O
some	O
of	O
the	O
properties	O
are	O
subject	O
to	O
different	O
interpretations	O
.	O
for	O
example	B
,	O
in	O
general	O
a	O
decision	O
tree	O
is	O
considered	O
to	O
be	O
less	O
easy	O
to	O
understand	O
than	O
decision	O
rules	O
.	O
however	O
,	O
both	O
are	O
much	O
easier	O
to	O
understand	O
than	O
a	O
regression	O
formula	O
which	O
contains	O
only	O
coefﬁcients	O
,	O
and	O
some	O
algorithms	O
do	O
not	O
give	O
any	O
easily	O
summarised	O
rule	O
at	O
all	O
(	O
for	O
example	B
,	O
k-nn	O
)	O
.	O
¸	O
address	O
for	O
correspondence	O
:	O
department	O
of	O
statistics	O
,	O
university	O
of	O
leeds	O
,	O
leeds	O
ls2	O
9jt	O
,	O
u.k.	O
ï	O
214	O
conclusions	O
[	O
ch	O
.	O
11	O
the	O
remaining	O
sections	O
discuss	O
more	O
general	O
issues	O
that	O
have	O
been	O
raised	O
in	O
the	O
trials	O
,	O
such	O
as	O
time	O
and	O
memory	O
requirements	O
,	O
the	O
use	O
of	O
cost	O
matrices	O
and	O
general	O
warnings	O
on	O
the	O
interpretation	O
of	O
our	O
results	O
.	O
11.1.1	O
user	O
’	O
s	O
guide	O
to	O
programs	O
here	O
we	O
tabulate	O
some	O
measures	O
to	O
summarise	O
each	O
algorithm	O
.	O
some	O
are	O
subjective	O
quantities	O
based	O
on	O
the	O
user	O
’	O
s	O
perception	O
of	O
the	O
programs	O
used	O
in	O
statlog	O
,	O
and	O
may	O
not	O
hold	O
for	O
other	O
implementations	O
of	O
the	O
method	O
.	O
for	O
example	B
,	O
many	O
of	O
the	O
classical	O
statistical	B
algorithms	O
can	O
handle	O
missing	O
values	O
,	O
whereas	O
those	O
used	O
in	O
this	O
project	O
could	O
not	O
.	O
this	O
would	O
necessitate	O
a	O
“	O
front-end	O
”	O
to	O
replace	O
missing	O
values	O
before	O
running	O
the	O
algorithm	O
.	O
similarly	O
,	O
all	O
of	O
these	O
programs	O
should	O
be	O
able	O
to	O
incorporate	O
costs	O
into	O
their	O
classiﬁcation	B
procedure	O
,	O
yet	O
some	O
of	O
them	O
have	O
not	O
.	O
in	O
table	O
11.1	O
we	O
give	O
information	O
on	O
various	O
basic	O
capabilities	O
of	O
each	O
algorithm	O
.	O
11.2	O
statistical	B
algorithms	O
11.2.1	O
discriminants	O
it	O
can	O
fairly	O
be	O
said	O
that	O
the	O
performance	O
of	O
linear	O
and	O
quadratic	O
discriminants	O
was	O
exactly	O
as	O
might	O
be	O
predicted	O
on	O
the	O
basis	O
of	O
theory	O
.	O
when	O
there	O
was	O
sufﬁcient	O
data	O
,	O
and	O
the	O
class	O
covariance	O
matrices	O
quite	O
dissimilar	O
then	O
quadratic	O
discriminant	O
did	O
better	O
,	O
although	O
at	O
the	O
expense	O
of	O
some	O
computational	O
costs	O
.	O
several	O
practical	O
problems	O
remain	O
however	O
:	O
1.	O
the	O
problem	O
of	O
deleting	O
attributes	O
if	O
they	O
do	O
not	O
contribute	O
usefully	O
to	O
the	O
discrimination	O
between	O
classes	O
(	O
see	O
mclachlan	O
,	O
1992	O
)	O
the	O
desirability	O
of	O
transforming	O
the	O
data	O
;	O
and	O
the	O
possibility	O
of	O
including	O
some	O
quadratic	O
terms	O
in	O
the	O
linear	O
discriminant	O
as	O
a	O
compromise	O
between	O
pure	O
linear	O
and	O
quadratic	O
discrimination	O
.	O
much	O
work	O
needs	O
to	O
be	O
done	O
in	O
this	O
area	O
.	O
2.	O
we	O
found	O
that	O
there	O
was	O
little	O
practical	O
difference	O
in	O
the	O
performance	O
of	O
ordinary	O
and	O
logistic	O
discrimination	O
.	O
this	O
has	O
been	O
observed	O
before	O
-	O
fienberg	O
(	O
1980	O
)	O
quotes	O
an	O
example	B
where	O
the	O
superiority	O
of	O
logistic	O
regression	O
over	O
discriminant	O
analysis	O
is	O
“	O
slight	O
”	O
-	O
and	O
is	O
related	O
to	O
the	O
well-known	O
fact	O
that	O
different	O
link	O
functions	O
in	O
generalised	O
linear	O
models	O
often	O
ﬁt	O
empirical	O
data	O
equally	O
well	O
,	O
especially	O
in	O
the	O
region	O
near	O
classiﬁcation	B
boundaries	O
where	O
the	O
curvature	O
of	O
the	O
probability	O
surface	O
may	O
be	O
negligible	O
.	O
mclachlan	O
(	O
1992	O
)	O
quotes	O
several	O
empirical	O
studies	O
in	O
which	O
the	O
allocation	O
performance	O
of	O
logistic	O
regression	O
was	O
very	O
similar	O
to	O
that	O
of	O
linear	O
discriminants	O
.	O
in	O
view	O
of	O
the	O
much	O
greater	O
computational	O
burden	O
required	O
,	O
the	O
advice	O
must	O
be	O
to	O
use	O
linear	O
or	O
quadratic	O
discriminants	O
for	O
large	O
datasets	O
.	O
the	O
situation	O
may	O
well	O
be	O
different	O
for	O
small	O
datasets	O
.	O
11.2.2	O
alloc80	O
this	O
algorithm	O
was	O
never	O
intended	O
for	O
the	O
size	O
of	O
datasets	O
considered	O
in	O
this	O
book	O
,	O
and	O
it	O
often	O
failed	O
on	O
the	O
larger	O
datasets	O
–	O
with	O
no	O
adequate	O
diagnostics	O
.	O
it	O
can	O
accept	O
attribute	O
data	O
with	O
both	O
numeric	O
and	O
logical	O
values	O
and	O
in	O
this	O
respect	O
appears	O
superior	O
to	O
the	O
other	O
statistical	B
algorithms	O
.	O
the	O
cross-validation	O
methods	O
for	O
parameter	O
selection	O
are	O
too	O
cumbersome	O
for	O
these	O
larger	O
datasets	O
,	O
although	O
in	O
principle	O
they	O
should	O
work	O
.	O
an	O
outstanding	O
problem	O
here	O
is	O
to	O
choose	O
good	O
smoothing	O
parameters	O
-	O
this	O
program	O
uses	O
a	O
sec	O
.	O
11.2	O
]	O
statistical	B
algorithms	O
215	O
table	O
11.1	O
:	O
users	O
guide	O
to	O
the	O
classiﬁcation	B
algorithms	O
.	O
algorithm	O
mv	O
cost	O
interp	O
.	O
compreh	O
.	O
params	O
user-fr	O
.	O
data	O
discrim	O
quadisc	O
logdisc	O
smart	O
alloc80	O
k-nn	O
castle	O
cart	O
indcart	O
newid	O
úüû	O
baytree	O
naivebay	O
cn2	O
c4.5	O
itrule	O
cal5	O
kohonen	O
dipol92	O
backprop	O
rbf	O
lvq	O
cascade	O
n	O
n	O
n	O
n	O
n	O
n	O
n	O
y	O
y	O
y	O
y	O
y	O
y	O
y	O
y	O
n	O
y	O
n	O
n	O
n	O
n	O
n	O
n	O
t	O
t	O
t	O
lt	O
lt	O
t	O
t	O
t	O
t	O
n	O
n	O
t	O
t	O
n	O
n	O
n	O
lt	O
n	O
lt	O
t	O
n	O
n	O
t	O
3	O
2	O
3	O
1	O
1	O
1	O
3	O
5	O
5	O
5	O
5	O
4	O
3	O
5	O
5	O
3	O
5	O
1	O
2	O
1	O
1	O
1	O
1	O
4	O
3	O
4	O
2	O
2	O
5	O
3	O
4	O
4	O
4	O
4	O
4	O
4	O
4	O
4	O
4	O
4	O
1	O
3	O
3	O
1	O
1	O
3	O
4	O
3	O
4	O
1	O
2	O
2	O
3	O
5	O
5	O
4	O
4	O
5	O
4	O
4	O
4	O
4	O
5	O
1	O
2	O
3	O
1	O
1	O
2	O
y	O
y	O
y	O
n	O
n	O
n	O
y	O
y	O
y	O
y	O
y	O
n	O
y	O
y	O
y	O
n	O
y	O
n	O
n	O
n	O
n	O
n	O
n	O
n	O
n	O
n	O
nc	O
nc	O
n	O
nc	O
nc	O
nc	O
nc	O
nch	O
nc	O
n	O
nc	O
nc	O
nc	O
nc	O
n	O
nc	O
n	O
n	O
n	O
n	O
key	O
:	O
mv	O
cost	O
interp	O
.	O
compreh	O
.	O
params	O
user-fr	O
.	O
data	O
whether	O
the	O
program	O
accepts	O
missing	O
values	O
whether	O
the	O
program	O
has	O
a	O
built-in	O
facility	O
to	O
deal	O
with	O
a	O
cost	O
matrix	O
at	O
learning	O
(	O
l	O
)	O
,	O
testing	O
(	O
t	O
)	O
or	O
not	O
at	O
all	O
(	O
n	O
)	O
whether	O
the	O
program	O
gives	O
an	O
easily	O
understood	O
classiﬁer	B
(	O
5	O
=	O
very	O
easy	O
to	O
interpret	O
)	O
whether	O
the	O
principle	O
of	O
the	O
method	O
is	O
easily	O
understood	O
(	O
5	O
=	O
very	O
easy	O
to	O
grasp	O
)	O
whether	O
the	O
program	O
has	O
good	O
user-guidelines	O
or	O
automatic	O
selection	O
of	O
important	O
parameters	O
.	O
whether	O
the	O
program	O
is	O
user-friendly	O
type	O
of	O
data	O
allowed	O
in	O
the	O
attributes	O
(	O
n	O
=	O
numerical	O
,	O
c	O
=	O
categorical	O
,	O
h=	O
hierarchical	O
)	O
.	O
however	O
,	O
note	O
that	O
categorical	O
data	O
can	O
always	O
be	O
transormed	O
to	O
numerical	O
.	O
ï	O
216	O
conclusions	O
[	O
ch	O
.	O
11	O
multiplicative	O
kernel	O
,	O
which	O
may	O
be	O
rather	O
inﬂexible	O
if	O
some	O
of	O
the	O
attributes	O
are	O
highly	O
correlated	O
.	O
fukunaga	O
(	O
1990	O
)	O
suggests	O
a	O
“	O
pre-whitening	O
”	O
of	O
the	O
data	O
which	O
is	O
equivalent	O
to	O
using	O
a	O
multivariate	O
normal	O
kernel	O
with	O
parameters	O
estimated	O
from	O
the	O
sample	O
covariance	O
matrix	O
.	O
this	O
method	O
has	O
shown	O
promise	O
in	O
some	O
of	O
the	O
datasets	O
here	O
,	O
although	O
it	O
is	O
not	O
very	O
robust	O
,	O
and	O
of	O
course	O
still	O
needs	O
smoothing	O
parameter	O
choices	O
.	O
alloc80	O
has	O
a	O
slightly	O
lower	O
error-rate	O
than	O
k-nn	O
,	O
and	O
uses	O
marginally	O
less	O
storage	O
,	O
but	O
takes	O
about	O
twice	O
as	O
long	O
in	O
training	O
and	O
testing	O
(	O
and	O
k-nn	O
is	O
already	O
a	O
very	O
slow	O
this	O
may	O
not	O
generally	O
be	O
true	O
.	O
is	O
a	O
special	O
case	O
of	O
the	O
kernel	O
method	O
so	O
it	O
should	O
be	O
expected	O
to	O
do	O
better	O
.	O
algorithm	O
)	O
.	O
however	O
,	O
since	O
k-nn	O
was	O
always	O
set	O
to¹	O
indeed	O
,	O
	O
11.2.3	O
nearest	O
neighbour	O
although	O
this	O
method	O
did	O
very	O
well	O
on	O
the	O
whole	O
,	O
as	O
expected	O
it	O
was	O
slowest	O
of	O
all	O
for	O
the	O
very	O
large	O
datasets	O
.	O
however	O
,	O
it	O
is	O
known	O
(	O
hart	O
,	O
1968	O
)	O
that	O
substantial	O
time	O
saving	O
can	O
be	O
effected	O
,	O
at	O
the	O
expense	O
of	O
some	O
slight	O
loss	O
of	O
accuracy	O
,	O
by	O
using	O
a	O
condensed	O
version	O
of	O
the	O
training	O
data	O
.	O
an	O
area	O
that	O
requires	O
further	O
study	O
is	O
in	O
fast	O
data-based	O
methods	O
for	O
choosing	O
appropriate	O
distance	O
measures	O
,	O
variable	O
selection	O
and	O
the	O
appropriate	O
number	O
of	O
neighbours	O
.	O
the	O
program	O
in	O
these	O
trials	O
normally	O
used	O
just	O
the	O
nearest	O
neighbour	O
which	O
is	O
certainly	O
not	O
optimal	O
.	O
a	O
simulation	O
study	O
on	O
this	O
problem	O
was	O
carried	O
out	O
by	O
enas	O
&	O
choi	O
(	O
1986	O
)	O
.	O
it	O
is	O
clear	O
from	O
many	O
of	O
the	O
results	O
that	O
substantial	O
improved	O
accuracy	O
can	O
be	O
obtained	O
with	O
careful	O
choice	O
of	O
variables	O
,	O
but	O
the	O
current	O
implementation	O
is	O
much	O
too	O
slow	O
.	O
indeed	O
,	O
lvq	O
has	O
about	O
the	O
same	O
error-rate	O
as	O
k-nn	O
,	O
but	O
is	O
about	O
6	O
times	O
faster	O
,	O
and	O
uses	O
about	O
25	O
%	O
less	O
storage	O
.	O
where	O
scaling	O
of	O
attributes	O
is	O
not	O
important	O
,	O
such	O
as	O
in	O
object	O
recognition	O
datasets	O
,	O
k-nearest	O
neighbour	O
is	O
ﬁrst	O
in	O
the	O
trials	O
.	O
yet	O
the	O
explanatory	O
power	O
of	O
k-nearest	O
neighbour	O
might	O
be	O
said	O
to	O
be	O
very	O
small	O
.	O
11.2.4	O
smart	O
smart	O
is	O
both	O
a	O
classiﬁcation	B
and	O
regression	O
type	O
algorithm	O
which	O
is	O
most	O
easily	O
used	O
in	O
batch	O
mode	O
.	O
it	O
is	O
a	O
very	O
slow	O
algorithm	O
to	O
train	O
,	O
but	O
quite	O
quick	O
in	O
the	O
classiﬁcation	B
stage	O
.	O
the	O
output	B
is	O
virtually	O
incomprehensible	O
to	O
a	O
non-statistician	O
,	O
but	O
a	O
graphical	O
front-	O
end	O
could	O
be	O
written	O
to	O
improve	O
the	O
interpretability	O
.	O
see	O
the	O
example	B
in	O
section	O
4.4.1.	O
in	O
addition	O
there	O
are	O
some	O
difﬁculties	O
in	O
choosing	O
the	O
number	O
of	O
terms	O
to	O
include	O
in	O
the	O
model	O
.	O
this	O
is	O
a	O
similar	O
problem	O
to	O
choosing	O
the	O
smoothing	O
parameter	O
in	O
kernel	O
methods	O
,	O
or	O
the	O
number	O
of	O
neighbours	O
in	O
a	O
nearest	O
neighbour	O
classiﬁer	B
.	O
a	O
major	O
advantage	O
smart	O
has	O
over	O
most	O
of	O
the	O
other	O
algorithms	O
is	O
that	O
it	O
accepts	O
a	O
cost	O
matrix	O
in	O
its	O
training	O
as	O
well	O
as	O
in	O
its	O
testing	O
phase	O
which	O
,	O
in	O
principle	O
,	O
ought	O
to	O
make	O
it	O
much	O
more	O
suited	O
for	O
tackling	O
problems	O
where	O
costs	O
are	O
important	O
.	O
11.2.5	O
naive	O
bayes	O
naive	O
bayes	O
can	O
easily	O
handle	O
unknown	O
or	O
missing	O
values	O
.	O
the	O
main	O
drawback	O
of	O
the	O
algorithm	O
is	O
perhaps	O
its	O
“	O
na¨ıvety	O
”	O
,	O
i.e	O
.	O
it	O
uses	O
directly	O
bayes	O
theorem	O
to	O
classify	O
examples	O
.	O
in	O
addition	O
,	O
for	O
those	O
not	O
ﬂuent	O
with	O
the	O
statistical	B
background	O
there	O
is	O
generally	O
little	O
indication	O
of	O
why	O
it	O
has	O
classiﬁed	O
some	O
examples	O
in	O
one	O
class	O
or	O
the	O
other	O
.	O
theory	O
indicates	O
,	O
and	O
our	O
experience	O
conﬁrms	O
,	O
that	O
naive	O
bayes	O
does	O
best	O
if	O
the	O
attributes	O
are	O
conditionally	O
independent	O
given	O
the	O
class	O
.	O
this	O
seems	O
to	O
hold	O
true	O
for	O
many	O
ñ	O
ñ	O
sec	O
.	O
11.3	O
]	O
decision	O
trees	O
217	O
medical	O
datasets	O
.	O
one	O
reason	O
for	O
this	O
might	O
be	O
that	O
doctors	O
gather	O
as	O
many	O
different	O
(	O
“	O
independent	O
”	O
)	O
bits	O
of	O
relevant	O
information	O
as	O
possible	O
,	O
but	O
they	O
do	O
not	O
include	O
two	O
attributes	O
where	O
one	O
would	O
do	O
.	O
for	O
example	B
,	O
it	O
could	O
be	O
that	O
only	O
one	O
measure	B
of	O
high	O
blood	O
pressure	O
(	O
say	O
diastolic	O
)	O
would	O
be	O
quoted	O
although	O
two	O
(	O
diastolic	O
and	O
systolic	O
)	O
would	O
be	O
available	O
.	O
11.2.6	O
castle	O
in	O
essence	O
castle	O
is	O
a	O
full	O
bayesian	O
modelling	O
algorithm	O
,	O
i.e	O
.	O
it	O
builds	O
a	O
comprehensive	O
probabilistic	O
model	O
of	O
the	O
events	O
(	O
in	O
this	O
case	O
attributes	O
)	O
of	O
the	O
empirical	O
data	O
.	O
it	O
can	O
be	O
used	O
to	O
infer	O
the	O
probability	O
of	O
attributes	O
as	O
well	O
as	O
classes	O
given	O
the	O
values	O
of	O
other	O
attributes	O
.	O
the	O
main	O
reason	O
for	O
using	O
castle	O
is	O
that	O
the	O
polytree	O
models	O
the	O
whole	O
structure	O
of	O
the	O
data	O
,	O
and	O
no	O
special	O
role	O
is	O
given	O
to	O
the	O
variable	O
being	O
predicted	O
,	O
viz	O
.	O
the	O
class	O
of	O
the	O
object	O
.	O
however	O
instructive	O
this	O
may	O
be	O
,	O
it	O
is	O
not	O
the	O
principal	O
task	O
in	O
the	O
above	O
trials	O
(	O
which	O
is	O
to	O
produce	O
a	O
classiﬁcation	B
procedure	O
)	O
.	O
so	O
maybe	O
there	O
should	O
be	O
an	O
option	O
in	O
castle	O
to	O
produce	O
a	O
polytree	O
which	O
classiﬁes	O
rather	O
than	O
ﬁts	O
all	O
the	O
variables	O
.	O
to	O
emphasise	O
the	O
point	O
,	O
it	O
is	O
easy	O
to	O
deﬂect	O
the	O
polytree	O
algorithm	O
by	O
making	O
it	O
ﬁt	O
irrelevant	O
bits	O
of	O
the	O
tree	O
(	O
that	O
are	O
strongly	O
related	O
to	O
each	O
other	O
but	O
are	O
irrelevant	O
to	O
classiﬁcation	B
)	O
.	O
castle	O
can	O
normally	O
be	O
used	O
in	O
both	O
interactive	O
and	O
batch	O
modes	O
.	O
it	O
accepts	O
any	O
data	O
described	O
in	O
probabilities	O
and	O
events	O
,	O
including	O
descriptions	O
of	O
attributes-and-class	O
pairs	O
of	O
data	O
such	O
as	O
that	O
used	O
here	O
.	O
however	O
,	O
all	O
attributes	O
and	O
classes	O
must	O
be	O
discretised	O
to	O
categorical	O
or	O
logical	O
data	O
.	O
the	O
results	O
of	O
castle	O
are	O
in	O
the	O
form	O
of	O
a	O
(	O
bayesian	O
)	O
polytree	O
that	O
provides	O
a	O
graphical	O
explanation	O
of	O
the	O
probabilistic	O
relationships	O
between	O
attributes	O
and	O
classes	O
.	O
thus	O
it	O
is	O
better	O
in	O
term	O
of	O
comprehensibility	O
compared	O
to	O
some	O
of	O
the	O
other	O
statistical	B
algorithms	O
in	O
its	O
explanation	O
of	O
the	O
probabilistic	O
relationships	O
between	O
attributes	O
and	O
classes	O
.	O
the	O
performance	O
of	O
castle	O
should	O
be	O
related	O
to	O
how	O
“	O
tree-like	O
”	O
the	O
dataset	O
is	O
.	O
a	O
major	O
criticism	O
of	O
castle	O
is	O
that	O
there	O
is	O
no	O
internal	O
measure	B
that	O
tells	O
us	O
how	O
closely	O
the	O
empirical	O
data	O
are	O
ﬁtted	O
by	O
the	O
chosen	O
polytree	O
.	O
we	O
recommend	O
that	O
any	O
future	O
implemen-	O
tation	O
of	O
castle	O
incorporates	O
such	O
a	O
“	O
polytree	O
”	O
measure	B
.	O
it	O
should	O
be	O
straightforward	O
to	O
build	O
a	O
goodness-of-ﬁt	O
measure	B
into	O
castle	O
based	O
on	O
a	O
standard	O
test	O
.	O
as	O
a	O
classiﬁer	B
,	O
castle	O
did	O
best	O
in	O
the	O
credit	O
datasets	O
where	O
,	O
generally	O
,	O
only	O
a	O
few	O
attributes	O
are	O
important	O
,	O
but	O
its	O
most	O
useful	O
feature	O
is	O
the	O
ability	O
to	O
produce	O
simple	O
models	O
of	O
the	O
data	O
.	O
unfortunately	O
,	O
simple	O
models	O
ﬁtted	O
only	O
a	O
few	O
of	O
our	O
datasets	O
.	O
11.3	O
decision	O
trees	O
there	O
is	O
a	O
confusing	O
diversity	O
of	O
decision	O
tree	O
algorithms	O
,	O
but	O
they	O
all	O
seem	O
to	O
perform	O
at	O
about	O
the	O
same	O
level	O
.	O
five	O
of	O
the	O
decision	O
trees	O
(	O
ú	O
%	O
û	O
considered	O
in	O
this	O
book	O
are	O
similar	O
in	O
structure	O
to	O
the	O
original	O
id3	O
algorithm	O
,	O
with	O
partitions	O
being	O
made	O
by	O
splitting	O
on	O
an	O
attribute	O
,	O
and	O
with	O
an	O
entropy	O
measure	B
for	O
the	O
splits	O
.	O
there	O
are	O
no	O
indications	O
that	O
this	O
or	O
that	O
splitting	O
criterion	O
is	O
best	O
,	O
but	O
the	O
case	O
for	O
using	O
some	O
kind	O
of	O
pruning	B
is	O
overwhelming	O
,	O
although	O
,	O
again	O
,	O
our	O
results	O
are	O
too	O
limited	O
to	O
say	O
exactly	O
how	O
much	O
pruning	B
to	O
use	O
.	O
it	O
was	O
hoped	O
to	O
relate	O
the	O
performance	O
of	O
a	O
decision	O
tree	O
to	O
some	O
measures	O
of	O
complexity	O
and	O
pruning	B
,	O
speciﬁcally	O
the	O
average	O
depth	O
of	O
the	O
tree	O
and	O
the	O
number	O
of	O
terminal	O
nodes	O
(	O
leaves	O
)	O
.	O
in	O
a	O
sense	O
cart	O
’	O
s	O
cost-complexity	O
pruning	B
automates	O
this	O
.	O
cal5	O
has	O
generally	O
much	O
fewer	O
nodes	O
,	O
so	O
gives	O
a	O
simpler	O
tree	O
.	O
many	O
more	O
nodes	O
,	O
and	O
occasionally	O
scores	O
a	O
success	O
because	O
of	O
that	O
.	O
the	O
fact	O
that	O
all	O
ï	O
,	O
newid	O
,	O
cal5	O
,	O
c4.5	O
,	O
indcart	O
)	O
ï	O
generally	O
has	O
ú	O
%	O
û	O
218	O
conclusions	O
[	O
ch	O
.	O
11	O
the	O
decision	O
trees	O
perform	O
at	O
the	O
same	O
accuracy	O
with	O
such	O
different	O
pruning	B
procedures	O
suggests	O
that	O
much	O
work	O
needs	O
to	O
be	O
done	O
on	O
the	O
question	O
of	O
how	O
many	O
nodes	O
to	O
use	O
.	O
on	O
the	O
basis	O
of	O
our	O
trials	O
on	O
the	O
tsetse	O
ﬂy	O
data	O
and	O
the	O
segmentation	O
data	O
,	O
we	O
speculate	O
that	O
decision	O
tree	O
methods	O
will	O
work	O
well	O
compared	O
to	O
classical	O
statistical	B
methods	O
when	O
the	O
data	O
are	O
multimodal	O
.	O
their	O
success	O
in	O
the	O
shuttle	O
and	O
technical	B
datasets	O
is	O
due	O
to	O
the	O
special	O
structure	O
of	O
these	O
datasets	O
.	O
in	O
the	O
case	O
of	O
the	O
technical	B
dataset	O
observations	O
were	O
partly	O
pre-classiﬁed	O
by	O
the	O
use	O
of	O
a	O
decision	O
tree	O
,	O
and	O
in	O
the	O
shuttle	O
dataset	O
we	O
believe	O
that	O
this	O
may	O
also	O
be	O
so	O
,	O
although	O
we	O
have	O
been	O
unable	O
to	O
obtain	O
conﬁrmation	O
from	O
the	O
data	O
provider	O
.	O
among	O
the	O
decision	O
trees	O
,	O
indcart	O
,	O
cart	O
and	O
cal5	O
method	O
emerge	O
as	O
superior	O
to	O
others	O
because	O
they	O
incorporate	O
costs	O
into	O
decisions	O
.	O
both	O
cart	O
and	O
indcart	O
can	O
deal	O
with	O
categorical	O
variables	O
,	O
and	O
cart	O
has	O
an	O
important	O
additional	O
feature	O
in	O
that	O
it	O
has	O
a	O
systematic	O
method	O
for	O
dealing	O
with	O
missing	O
values	O
.	O
however	O
,	O
for	O
the	O
larger	O
datasets	O
the	O
commercial	O
package	O
cart	O
often	O
failed	O
where	O
the	O
indcart	O
implementation	O
did	O
not	O
.	O
in	O
common	O
with	O
all	O
other	O
decision	O
trees	O
,	O
cart	O
,	O
indcart	O
and	O
cal5	O
have	O
the	O
advantage	O
of	O
being	O
distribution	O
free	O
.	O
incorporating	O
prior	O
knowledge	O
about	O
the	O
dataset	O
,	O
in	O
particular	O
certain	O
forms	O
of	O
hierarchical	O
structure	O
;	O
see	O
chapter	O
12.	O
we	O
looked	O
at	O
one	O
dataset	O
that	O
was	O
hierarchical	O
in	O
nature	O
,	O
in	O
11.3.1	O
ú	O
%	O
û	O
newid	O
andúüû	O
classiﬁers	O
is	O
very	O
close	O
.	O
the	O
main	O
reason	O
for	O
choosingúüû	O
of	O
theú	O
%	O
û	O
whichúüû	O
ï	O
and	O
newid	O
ï	O
are	O
direct	O
descendants	O
of	O
id3	O
,	O
and	O
,	O
empirically	O
,	O
their	O
performance	O
as	O
ï	O
would	O
be	O
to	O
use	O
other	O
aspects	O
ï	O
package	O
,	O
for	O
example	B
,	O
the	O
interactive	O
graphical	O
package	O
and	O
the	O
possibility	O
of	O
ï	O
showed	O
considerable	O
advantage	O
over	O
other	O
methods	O
-	O
see	O
section	O
9.5.7.	O
newid	O
is	O
based	O
on	O
ross	O
quinlan	O
’	O
s	O
original	O
id3	O
program	O
which	O
generates	O
decision	O
trees	O
from	O
examples	O
.	O
it	O
is	O
similar	O
to	O
cn2	O
in	O
its	O
interface	O
and	O
command	O
system	O
.	O
similar	O
to	O
cn2	O
,	O
newid	O
can	O
be	O
used	O
in	O
both	O
interactive	O
and	O
batch	O
mode	O
.	O
the	O
interactive	O
mode	O
is	O
its	O
native	O
mode	O
;	O
and	O
to	O
run	O
in	O
batch	O
mode	O
users	O
need	O
to	O
write	O
a	O
unix	O
shell	O
script	O
as	O
for	O
cn2	O
.	O
newid	O
accepts	O
attribute-value	O
data	O
sets	O
with	O
both	O
logical	O
and	O
numeric	O
data	O
.	O
newid	O
has	O
a	O
post-pruning	O
facility	O
that	O
is	O
used	O
to	O
deal	O
with	O
noise	O
.	O
it	O
can	O
also	O
deal	O
with	O
unknown	O
values	O
.	O
newid	O
outputs	O
a	O
confusion	O
matrix	O
.	O
but	O
this	O
confusion	O
matrix	O
must	O
be	O
used	O
with	O
care	O
because	O
the	O
matrix	O
has	O
an	O
extra	O
row	O
and	O
column	O
for	O
unclassiﬁed	O
examples	O
–	O
some	O
examples	O
are	O
not	O
classiﬁed	O
by	O
the	O
decision	O
tree	O
.	O
it	O
does	O
not	O
accept	O
or	O
incorporate	O
a	O
cost	O
matrix	O
.	O
different	O
from	O
the	O
usual	O
format	O
-	O
mainly	O
due	O
to	O
the	O
need	O
to	O
express	O
hierarchical	O
attributes	O
when	O
there	O
are	O
such	O
.	O
but	O
for	O
non-hierarchical	O
data	O
,	O
there	O
is	O
very	O
limited	O
requirement	O
for	O
data	O
is	O
an	O
extension	O
to	O
id3	O
style	O
of	O
decision	O
tree	O
classiﬁers	O
to	O
learn	O
structures	O
from	O
a	O
predeﬁned	O
hierarchy	O
of	O
attributes	O
.	O
similarly	O
to	O
id3	O
it	O
uses	O
an	O
attribute-value	O
based	O
format	O
for	O
examples	O
with	O
both	O
logical	O
and	O
numeric	O
data	O
.	O
because	O
of	O
its	O
hierarchical	O
representation	O
it	O
can	O
also	O
encode	O
some	O
relations	O
between	O
attribute	O
values	O
.	O
it	O
can	O
be	O
run	O
in	O
interactive	O
mode	O
úüû	O
and	O
data	O
can	O
be	O
edited	O
visually	O
under	O
its	O
user	O
interface.úüû	O
conversion.ú	O
%	O
û	O
user	O
interacts	O
withúüû	O
ï	O
uses	O
an	O
internal	O
format	O
that	O
is	O
ï	O
can	O
deal	O
with	O
unknown	O
values	O
in	O
examples	O
,	O
and	O
multi-valued	O
attributes	O
.	O
ï	O
via	O
a	O
graphical	O
interface	O
.	O
this	O
interface	O
consists	O
of	O
graphical	O
it	O
is	O
also	O
able	O
to	O
deal	O
with	O
knowledge	O
concerning	O
the	O
studied	O
domain	O
,	O
but	O
with	O
the	O
exception	O
of	O
the	O
machine	B
faults	I
dataset	O
,	O
this	O
aspect	O
was	O
deliberately	O
not	O
studied	O
in	O
this	O
book	O
.	O
the	O
ï	O
sec	O
.	O
11.3	O
]	O
decision	O
trees	O
219	O
editors	O
,	O
which	O
enable	O
the	O
user	O
to	O
deﬁne	O
the	O
knowledge	O
of	O
the	O
domain	O
,	O
to	O
interactively	O
build	O
the	O
example	B
base	O
and	O
to	O
go	O
through	O
the	O
hierarchy	O
of	O
classes	O
and	O
the	O
decision	O
tree	O
.	O
ï	O
produces	O
decision	O
trees	O
which	O
can	O
be	O
very	O
large	O
compared	O
to	O
the	O
other	O
decision	O
úüû	O
tree	O
algorithms	O
.	O
the	O
trials	O
reported	O
here	O
suggest	O
thatúüû	O
is	O
relatively	O
slow	O
.	O
this	O
older	O
version	O
used	O
common	O
lisp	O
and	O
has	O
now	O
been	O
superseded	O
by	O
a	O
c	O
version	O
,	O
resulting	O
in	O
a	O
much	O
faster	O
program	O
.	O
11.3.2	O
c4.5	O
c4.5	O
is	O
the	O
direct	O
descendent	O
of	O
id3	O
.	O
it	O
is	O
run	O
in	O
batch	O
mode	O
for	O
training	O
with	O
attribute-value	O
data	O
input	B
.	O
for	O
testing	O
,	O
both	O
interactive	O
and	O
batch	O
modes	O
are	O
available	O
.	O
both	O
logical	O
and	O
numeric	O
values	O
can	O
be	O
used	O
in	O
the	O
attributes	O
;	O
it	O
needs	O
a	O
declaration	O
for	O
the	O
types	O
and	O
range	O
of	O
attributes	O
,	O
and	O
such	O
information	O
needs	O
to	O
be	O
placed	O
in	O
a	O
separate	O
ﬁle	O
.	O
c4.5	O
is	O
very	O
easy	O
to	O
set	O
up	O
and	O
run	O
.	O
in	O
fact	O
it	O
is	O
only	O
a	O
set	O
of	O
unix	O
commands	O
,	O
which	O
should	O
be	O
familiar	O
to	O
all	O
unix	O
users	O
.	O
there	O
are	O
very	O
few	O
parameters	O
.	O
apart	O
from	O
the	O
pruning	B
criterion	O
no	O
major	O
parameter	O
adjustment	O
is	O
needed	O
for	O
most	O
applications	O
-	O
in	O
the	O
trials	O
reported	O
here	O
,	O
the	O
windowing	O
facility	O
was	O
disabled	O
.	O
c4.5	O
produces	O
a	O
confusion	O
matrix	O
from	O
classiﬁcation	B
results	O
.	O
however	O
,	O
it	O
does	O
not	O
incorporate	O
a	O
cost	O
matrix	O
.	O
c4.5	O
allows	O
the	O
users	O
to	O
adjust	O
the	O
degree	O
of	O
the	O
tracing	O
information	O
displayed	O
while	O
the	O
algorithm	O
is	O
running	O
.	O
this	O
facility	O
can	O
satisfy	O
both	O
the	O
users	O
who	O
do	O
not	O
need	O
to	O
know	O
the	O
internal	O
operations	O
of	O
the	O
algorithm	O
and	O
the	O
users	O
who	O
need	O
to	O
monitor	O
the	O
intermidate	O
steps	O
of	O
tree	O
construction	O
.	O
note	O
that	O
c4.5	O
has	O
a	O
rule-generating	O
module	O
,	O
which	O
often	O
improves	O
the	O
error	O
rate	O
and	O
almost	O
invariably	O
the	O
user-transparancy	O
,	O
but	O
this	O
was	O
not	O
used	O
in	O
the	O
comparative	O
trials	O
reported	O
in	O
chapter	O
9	O
.	O
11.3.3	O
cart	O
and	O
indcart	O
cart	O
and	O
indcart	O
are	O
decision	O
tree	O
algorithms	O
based	O
on	O
the	O
work	O
of	O
breiman	O
et	O
al	O
.	O
(	O
1984	O
)	O
.	O
the	O
statlog	O
version	O
of	O
cart	O
is	O
the	O
commercial	O
derivative	O
of	O
the	O
original	O
algorithm	O
developed	O
at	O
caltech	O
.	O
both	O
are	O
classiﬁcation	B
and	O
regression	O
algorithms	O
but	O
they	O
treat	O
regression	O
and	O
unknown	O
values	O
in	O
the	O
data	O
somewhat	O
differently	O
.	O
in	O
both	O
systems	O
there	O
are	O
very	O
few	O
parameters	O
to	O
adjust	O
for	O
new	O
tasks	O
.	O
the	O
noise	O
handling	O
mechanism	O
of	O
the	O
two	O
algorithms	O
are	O
very	O
similar	O
.	O
both	O
can	O
also	O
deal	O
with	O
unknown	O
values	O
,	O
though	O
in	O
different	O
ways	O
.	O
the	O
algorithms	O
both	O
output	B
a	O
decision	O
tree	O
and	O
a	O
confusion	O
matrix	O
as	O
output	O
.	O
but	O
only	O
cart	O
incorporates	O
costs	O
(	O
and	O
it	O
does	O
so	O
in	O
both	O
training	O
and	O
test	O
phases	O
)	O
.	O
note	O
that	O
cart	O
failed	O
to	O
run	O
in	O
many	O
of	O
trials	O
involving	O
very	O
large	O
datasets	O
.	O
11.3.4	O
cal5	O
cal5	O
is	O
a	O
numeric	O
value	O
decision	O
tree	O
classiﬁer	B
using	O
statistical	B
methods	O
.	O
thus	O
discrete	O
values	O
have	O
to	O
be	O
changed	O
into	O
numeric	O
ones	O
.	O
cal5	O
is	O
very	O
easy	O
to	O
set	O
up	O
and	O
run	O
.	O
it	O
has	O
a	O
number	O
of	O
menus	O
to	O
guide	O
the	O
user	O
to	O
complete	O
operations	O
.	O
however	O
,	O
there	O
are	O
a	O
number	O
of	O
parameters	O
,	O
and	O
for	O
novice	O
users	O
the	O
meanings	O
of	O
these	O
parameters	O
are	O
not	O
very	O
easy	O
to	O
understand	O
.	O
the	O
results	O
from	O
different	O
parameter	O
settings	O
can	O
be	O
very	O
different	O
,	O
but	O
tuning	O
of	O
parameters	O
is	O
implemented	O
in	O
a	O
semi-automatic	O
manner	O
.	O
the	O
decision	O
trees	O
produced	O
by	O
cal5	O
are	O
usually	O
quite	O
small	O
and	O
are	O
reasonably	O
easy	O
to	O
understand	O
compared	O
to	O
algorithms	O
such	O
as	O
c4.5	O
when	O
used	O
with	O
default	O
settings	O
of	O
ï	O
220	O
conclusions	O
[	O
ch	O
.	O
11	O
pruning	B
parameters	O
.	O
occasionally	O
,	O
from	O
the	O
point	O
of	O
view	O
of	O
minimising	O
error	O
rates	O
,	O
the	O
tree	O
is	O
over-pruned	O
,	O
though	O
of	O
course	O
the	O
rules	O
are	O
then	O
more	O
transparent	O
.	O
cal5	O
produces	O
a	O
confusion	O
matrix	O
and	O
incorporates	O
a	O
cost	O
matrix	O
.	O
11.3.5	O
bayes	O
tree	O
our	O
trials	O
conﬁrm	O
the	O
results	O
reported	O
in	O
buntine	O
(	O
1992	O
)	O
:	O
bayes	O
trees	O
are	O
generally	O
slower	O
in	O
learning	O
and	O
testing	O
,	O
but	O
perform	O
at	O
around	O
the	O
same	O
accuracy	O
as	O
,	O
say	O
,	O
c4.5	O
or	O
newid	O
.	O
however	O
,	O
it	O
is	O
not	O
so	O
similar	O
to	O
these	O
two	O
algorithms	O
as	O
one	O
might	O
expect	O
,	O
sometimes	O
being	O
substantially	O
better	O
(	O
in	O
the	O
cost	O
datasets	O
)	O
,	O
sometimes	O
marginally	O
better	O
(	O
in	O
the	O
segmented	O
image	O
datasets	O
)	O
and	O
sometimes	O
noticeably	O
worse	O
.	O
bayes	O
tree	O
also	O
did	O
surprisingly	O
badly	O
,	O
for	O
a	O
decision	O
tree	O
,	O
on	O
the	O
technical	B
dataset	O
.	O
this	O
is	O
probably	O
due	O
to	O
the	O
relatively	O
small	O
sample	O
sizes	O
for	O
a	O
large	O
number	O
of	O
the	O
classes	O
.	O
samples	O
with	O
very	O
small	O
a	O
priori	O
probabilities	O
are	O
allocated	O
to	O
the	O
most	O
frequent	O
classes	O
,	O
as	O
the	O
dataset	O
is	O
not	O
large	O
enough	O
for	O
the	O
a	O
priori	O
probabilities	O
to	O
be	O
adapted	O
by	O
the	O
empirical	O
probabilities	O
.	O
apart	O
from	O
the	O
technical	B
dataset	O
,	O
bayes	O
trees	O
probably	O
do	O
well	O
as	O
a	O
result	O
of	O
the	O
explicit	O
mechanism	O
for	O
“	O
pruning	B
”	O
via	O
smoothing	O
class	O
probabilities	O
,	O
and	O
their	O
success	O
gives	O
empirical	O
justiﬁcation	O
for	O
the	O
at-ﬁrst-sight-artiﬁcial	O
model	O
of	O
tree	O
probabilities	O
.	O
11.4	O
rule-based	O
methods	O
11.4.1	O
cn2	O
the	O
rule-based	O
algorithm	O
cn2	O
also	O
belongs	O
to	O
the	O
general	O
class	O
of	O
recursive	O
partitioning	O
algorithms	O
.	O
of	O
the	O
two	O
possible	O
variants	O
,	O
“	O
ordered	O
”	O
and	O
“	O
unordered	O
”	O
rules	O
,	O
it	O
appears	O
that	O
“	O
unordered	O
”	O
rules	O
give	O
best	O
results	O
,	O
and	O
then	O
the	O
performance	O
is	O
practically	O
indistinguishable	O
from	O
the	O
decision	O
trees	O
,	O
while	O
at	O
the	O
same	O
time	O
offering	O
gains	O
in	O
“	O
mental	B
ﬁt	I
”	O
over	O
decision	O
trees	O
.	O
however	O
,	O
cn2	O
performed	O
badly	O
on	O
the	O
datasets	O
involving	O
costs	O
,	O
although	O
this	O
should	O
not	O
be	O
difﬁcult	O
to	O
ﬁx	O
.	O
as	O
a	O
decision	O
tree	O
may	O
be	O
expressed	O
in	O
the	O
form	O
of	O
rules	O
(	O
and	O
vice-	O
versa	O
)	O
,	O
there	O
appears	O
to	O
be	O
no	O
practical	O
reason	O
for	O
choosing	O
rule-based	O
methods	O
except	O
when	O
the	O
complexity	O
of	O
the	O
data-domain	O
demands	O
some	O
simplifying	O
change	O
of	O
representation	O
.	O
this	O
is	O
not	O
an	O
aspect	O
with	O
which	O
this	O
book	O
has	O
been	O
concerned	O
.	O
cn2	O
can	O
be	O
used	O
in	O
both	O
interactive	O
and	O
batch	O
mode	O
.	O
the	O
interactive	O
mode	O
is	O
its	O
native	O
mode	O
;	O
and	O
to	O
run	O
in	O
batch	O
mode	O
users	O
need	O
to	O
write	O
a	O
unix	O
shell	O
script	O
that	O
gives	O
the	O
algorithm	O
a	O
sequence	O
of	O
instructions	O
to	O
run	O
.	O
the	O
slight	O
deviation	O
from	O
the	O
other	O
algorithms	O
is	O
that	O
it	O
needs	O
a	O
set	O
of	O
declarations	O
that	O
deﬁnes	O
the	O
types	O
and	O
range	O
of	O
attribute-values	O
for	O
each	O
attribute	O
.	O
in	O
general	O
there	O
is	O
very	O
little	O
effort	O
needed	O
for	O
data	O
conversion	O
.	O
cn2	O
is	O
very	O
easy	O
to	O
set	O
up	O
and	O
run	O
.	O
in	O
interactive	O
mode	O
,	O
the	O
operations	O
are	O
completely	O
menu	O
driven	O
.	O
after	O
some	O
familiarity	O
it	O
would	O
be	O
very	O
easy	O
to	O
write	O
a	O
unix	O
shell	O
script	O
to	O
run	O
the	O
algorithm	O
in	O
batch	O
mode	O
.	O
there	O
are	O
a	O
few	O
parameters	O
that	O
the	O
users	O
will	O
have	O
to	O
choose	O
.	O
however	O
,	O
there	O
is	O
only	O
one	O
parameter	O
–	O
rule	O
types	O
–	O
which	O
may	O
have	O
signiﬁcant	O
effect	O
on	O
the	O
training	O
results	O
for	O
most	O
applications	O
.	O
11.4.2	O
itrule	O
strictly	O
speaking	O
,	O
itrule	O
is	O
not	O
a	O
classiﬁcation	B
type	O
algorithm	O
,	O
and	O
was	O
not	O
designed	O
for	O
large	O
datasets	O
,	O
or	O
for	O
problems	O
with	O
many	O
classes	O
.	O
it	O
is	O
an	O
exploratory	O
tool	O
,	O
and	O
is	O
best	O
regarded	O
as	O
a	O
way	O
of	O
extracting	O
isolated	O
interesting	O
facts	O
(	O
or	O
rules	O
)	O
from	O
a	O
dataset	O
.	O
the	O
facts	O
(	O
rules	O
)	O
are	O
not	O
meant	O
to	O
cover	O
all	O
examples	O
.	O
we	O
may	O
say	O
that	O
itrule	O
does	O
not	O
look	O
for	O
sec	O
.	O
11.5	O
]	O
neural	O
networks	O
221	O
the	O
“	O
best	O
set	O
of	O
rules	O
”	O
for	O
classiﬁcation	B
(	O
or	O
for	O
any	O
other	O
purpose	O
)	O
.	O
rather	O
it	O
looks	O
for	O
a	O
set	O
of	O
“	O
best	O
rules	O
”	O
,	O
each	O
rule	O
being	O
very	O
simple	O
in	O
form	O
(	O
usually	O
restricted	O
to	O
conjunctions	O
of	O
two	O
conditions	O
)	O
,	O
with	O
the	O
rules	O
being	O
selected	O
as	O
having	O
high	O
information	O
content	O
(	O
in	O
the	O
sense	O
of	O
having	O
highº	O
-measure	O
)	O
.	O
within	O
these	O
limitations	O
,	O
and	O
also	O
with	O
the	O
limitation	O
of	O
discretised	O
variates	O
,	O
the	O
search	O
for	O
the	O
rules	O
is	O
exhaustive	O
and	O
therefore	O
time-consuming	O
.	O
therefore	O
the	O
number	O
of	O
rules	O
found	O
is	O
usually	O
limited	O
to	O
some	O
“	O
small	O
”	O
number	O
,	O
which	O
can	O
be	O
as	O
high	O
as	O
5000	O
or	O
more	O
however	O
.	O
for	O
use	O
in	O
classiﬁcation	B
problems	O
,	O
if	O
the	O
preset	O
rules	O
have	O
been	O
exhausted	O
,	O
a	O
default	O
rule	O
must	O
be	O
applied	O
,	O
and	O
it	O
is	O
probable	O
that	O
most	O
errors	O
are	O
committed	O
at	O
this	O
stage	O
.	O
in	O
some	O
datasets	O
,	O
itrule	O
may	O
generate	O
contradictory	O
rules	O
(	O
i.e	O
.	O
rules	O
with	O
identical	O
condition	O
parts	O
but	O
different	O
conclusions	O
)	O
,	O
and	O
this	O
may	O
also	O
contribute	O
to	O
a	O
high	O
error-rate	O
.	O
this	O
last	O
fact	O
is	O
connected	O
with	O
the	O
asymmetric	O
nature	O
of	O
theº	O
-measure	O
compared	O
to	O
the	O
usual	O
entropy	O
measure	B
.	O
the	O
algorithm	O
does	O
not	O
incorporate	O
a	O
cost	O
matrix	O
facility	O
,	O
but	O
it	O
would	O
appear	O
a	O
relatively	O
simple	O
task	O
to	O
incorporate	O
costs	O
as	O
all	O
rules	O
are	O
associated	O
with	O
a	O
probability	O
measure	B
.	O
(	O
in	O
multi-class	O
problems	O
approximate	O
costs	O
would	O
need	O
to	O
be	O
used	O
,	O
because	O
each	O
probability	O
measure	B
refers	O
to	O
the	O
odds	O
of	O
observing	O
a	O
class	O
or	O
not	O
)	O
.	O
11.5	O
neural	O
networks	O
with	O
care	O
,	O
neural	O
networks	O
perform	O
very	O
well	O
as	O
measured	O
by	O
error	O
rate	O
.	O
they	O
seem	O
to	O
provide	O
either	O
the	O
best	O
or	O
near	O
to	O
best	O
predictive	O
performance	O
in	O
nearly	O
all	O
cases	O
–	O
the	O
notable	O
exceptions	O
are	O
the	O
datasets	O
with	O
cost	O
matrices	O
.	O
in	O
terms	O
of	O
computational	O
burden	O
,	O
and	O
the	O
level	O
of	O
expertise	O
required	O
,	O
they	O
are	O
much	O
more	O
complex	O
than	O
,	O
say	O
,	O
the	O
machine	O
learning	O
procedures	O
.	O
and	O
there	O
are	O
still	O
several	O
unsolved	O
problems	O
,	O
most	O
notably	O
the	O
problems	O
of	O
how	O
to	O
incorporate	O
costs	O
into	O
the	O
learning	O
phase	O
and	O
the	O
optimal	O
choice	O
of	O
architecture	O
.	O
one	O
major	O
weakness	O
of	O
neural	O
nets	O
is	O
the	O
lack	O
of	O
diagnostic	O
help	O
.	O
if	O
something	O
goes	O
wrong	O
,	O
it	O
is	O
difﬁcult	O
to	O
pinpoint	O
the	O
difﬁculty	O
from	O
the	O
mass	O
of	O
inter-related	O
weights	O
and	O
connectivities	O
in	O
the	O
net	O
.	O
because	O
the	O
result	O
of	O
learning	O
is	O
a	O
completed	O
network	O
with	O
layers	O
and	O
nodes	O
linked	O
together	O
with	O
nonlinear	O
functions	O
whose	O
relationship	O
can	O
not	O
easily	O
be	O
described	O
in	O
qualitative	O
terms	O
,	O
neural	O
networks	O
are	O
generally	O
difﬁcult	O
to	O
understand	O
.	O
these	O
algorithms	O
are	O
usually	O
very	O
demanding	O
on	O
the	O
part	O
of	O
the	O
user	O
.	O
he	O
will	O
have	O
to	O
be	O
responsible	O
for	O
setting	O
up	O
the	O
initial	O
weights	O
of	O
the	O
network	O
,	O
selecting	O
the	O
correct	O
number	O
of	O
hidden	B
layers	O
and	O
the	O
number	O
of	O
nodes	O
at	O
each	O
layer	O
.	O
adjusting	O
these	O
parameters	O
of	O
learning	O
is	O
often	O
a	O
laborious	O
task	O
.	O
in	O
addition	O
some	O
of	O
these	O
algorithms	O
are	O
computationally	O
inefﬁcient	O
.	O
a	O
notable	O
exception	O
here	O
is	O
lvq	O
which	O
is	O
relatively	O
easy	O
to	O
set	O
up	O
and	O
fast	O
to	O
run	O
.	O
11.5.1	O
backprop	O
this	O
software	O
package	O
contains	O
programs	O
which	O
implement	O
mult-layer	O
perceptrons	O
and	O
radial	O
basis	O
functions	O
,	O
as	O
well	O
as	O
several	O
neural	O
network	O
models	O
which	O
are	O
not	O
discussed	O
here	O
,	O
including	O
recurrent	O
networks	O
.	O
it	O
is	O
reasonably	O
versatile	O
and	O
ﬂexible	O
in	O
that	O
it	O
can	O
be	O
used	O
to	O
train	O
a	O
variety	O
of	O
networks	O
with	O
a	O
variety	O
of	O
methods	O
using	O
a	O
variety	O
of	O
training	O
data	O
formats	O
.	O
however	O
its	O
functionality	B
is	O
not	O
embellished	O
with	O
a	O
friendly	O
user	O
interface	O
,	O
so	O
its	O
users	O
need	O
at	O
least	O
a	O
cursory	O
familiarity	O
with	O
unix	O
and	O
neural	O
networks	O
,	O
and	O
a	O
signiﬁcant	O
block	O
of	O
time	O
to	O
peruse	O
the	O
documentation	O
and	O
work	O
through	O
the	O
demonstrations	O
.	O
the	O
package	O
is	O
also	O
modular	O
,	O
and	O
extensible	O
by	O
anyone	O
willing	O
to	O
write	O
source	O
code	O
for	O
222	O
conclusions	O
[	O
ch	O
.	O
11	O
new	O
modules	O
,	O
based	O
on	O
existing	O
templates	O
and	O
“	O
hooks	O
”	O
.	O
one	O
of	O
the	O
fundamental	O
modules	O
provides	O
routines	O
for	O
manipulating	O
matrices	O
,	O
submatrices	O
,	O
and	O
linked	O
lists	O
of	O
submatrices	O
.	O
it	O
includes	O
a	O
set	O
of	O
macros	O
written	O
for	O
the	O
unix	O
utility	O
m4	O
which	O
allows	O
complicated	O
array-handling	O
routines	O
to	O
be	O
written	O
using	O
relatively	O
simple	O
m4	O
source	O
code	O
,	O
which	O
in	O
turn	O
is	O
translated	O
into	O
c	O
source	O
by	O
m4	O
.	O
all	O
memory	O
management	O
is	O
handled	O
dynamically	O
.	O
there	O
are	O
several	O
neural	O
network	O
modules	O
,	O
written	O
as	O
applications	O
to	O
the	O
minimisation	O
module	O
.	O
these	O
include	O
a	O
special	O
purpose	O
3-layer	O
mlp	O
,	O
a	O
fully-connected	O
recurrent	O
mlp	O
,	O
a	O
fully-connected	O
recurrent	O
mlp	O
with	O
an	O
unusual	O
training	O
algorithm	O
(	O
silva	O
&	O
almeida	O
,	O
1990	O
)	O
,	O
and	O
general	O
mlp	O
with	O
architecture	O
speciﬁed	O
at	O
runtime	O
.	O
there	O
is	O
also	O
an	O
rbf	O
network	O
which	O
shares	O
the	O
i/o	O
routines	O
but	O
does	O
not	O
use	O
the	O
minimiser	O
.	O
there	O
is	O
a	O
general	O
feeling	O
,	O
especially	O
among	O
statisticians	O
,	O
that	O
the	O
multilayer	O
perceptron	O
is	O
just	O
a	O
highly-parameterised	O
form	O
of	O
non-linear	O
regression	O
.	O
this	O
is	O
not	O
our	O
experience	O
.	O
in	O
practice	O
,	O
the	O
backprop	O
procedure	O
lies	O
somewhere	O
between	O
a	O
regression	O
technique	O
and	O
a	O
decision	O
tree	O
,	O
sometimes	O
being	O
closer	O
to	O
one	O
and	O
sometimes	O
closer	O
to	O
the	O
other	O
.	O
as	O
a	O
result	O
,	O
we	O
can	O
not	O
make	O
general	O
statements	O
about	O
the	O
nature	O
of	O
the	O
decision	O
surfaces	O
,	O
but	O
it	O
would	O
seem	O
that	O
they	O
are	O
not	O
in	O
any	O
sense	O
“	O
local	O
”	O
(	O
otherwise	O
there	O
would	O
be	O
a	O
greater	O
similarity	O
with	O
k-nn	O
)	O
.	O
generally	O
,	O
the	O
absence	O
of	O
diagnostic	O
information	O
and	O
the	O
inability	O
to	O
interpret	O
the	O
output	B
is	O
a	O
great	O
disadvantage	O
.	O
11.5.2	O
kohonen	O
and	O
lvq	O
kohonen	O
’	O
s	O
net	O
is	O
an	O
implementation	O
of	O
the	O
self-organising	O
feature	O
mapping	O
algorithm	O
based	O
on	O
the	O
work	O
of	O
kohonen	O
(	O
1989	O
)	O
.	O
kohonen	O
nets	O
have	O
an	O
inherent	O
parallel	O
feature	O
in	O
the	O
evaluation	O
of	O
links	O
between	O
“	O
neurons	O
”	O
.	O
so	O
this	O
program	O
is	O
implemented	O
,	O
by	O
luebeck	O
university	O
of	O
germany	O
,	O
on	O
a	O
transputer	O
with	O
an	O
ibm	O
pc	O
as	O
the	O
front-end	O
for	O
user	O
interaction	O
.	O
this	O
special	O
hardware	O
requirement	O
thus	O
differs	O
from	O
the	O
norm	O
and	O
makes	O
comparison	O
of	O
memory	O
and	O
cpu	O
time	O
rather	O
difﬁcult	O
.	O
kohonen	O
nets	O
are	O
more	O
general	O
than	O
a	O
number	O
of	O
other	O
neural	O
net	O
algorithms	O
such	O
as	O
backpropagation	O
.	O
in	O
a	O
sense	O
,	O
it	O
is	O
a	O
modelling	O
tool	O
that	O
can	O
be	O
used	O
to	O
model	O
the	O
behaviour	O
of	O
a	O
system	O
with	O
its	O
input	B
and	O
output	B
variables	O
(	O
attributes	O
)	O
all	O
modelled	O
as	O
linked	O
neuronal	O
.	O
in	O
this	O
respect	O
,	O
it	O
is	O
very	O
similar	O
to	O
the	O
statistical	B
algorithm	O
castle	O
–	O
both	O
can	O
be	O
used	O
in	O
wider	O
areas	O
of	O
applications	O
including	O
classiﬁcation	B
and	O
regression	O
.	O
in	O
this	O
book	O
,	O
however	O
,	O
we	O
are	O
primarily	O
concerned	O
with	O
classiﬁcation	B
.	O
the	O
network	O
can	O
accept	O
attribute-value	O
data	O
with	O
numeric	O
values	O
only	O
.	O
this	O
makes	O
it	O
necessary	O
to	O
convert	O
logical	O
or	O
categorical	O
attributes	O
into	O
numeric	O
data	O
.	O
in	O
use	O
there	O
are	O
very	O
few	O
indications	O
as	O
to	O
how	O
many	O
nodes	O
the	O
system	O
should	O
have	O
and	O
how	O
many	O
times	O
the	O
examples	O
should	O
be	O
repeatedly	O
fed	O
to	O
the	O
system	O
for	O
training	O
.	O
all	O
such	O
parameters	O
can	O
only	O
be	O
decided	O
on	O
a	O
trial-and-error	O
basis	O
.	O
kohonen	O
does	O
not	O
accept	O
unknown	O
values	O
so	O
data	O
sets	O
must	O
have	O
their	O
missing	O
attribute-values	O
replaced	O
by	O
estimated	O
values	O
through	O
some	O
statistical	B
methods	O
.	O
similar	O
to	O
all	O
neural	O
networks	O
,	O
the	O
output	B
of	O
the	O
kohonen	O
net	O
normally	O
gives	O
very	O
little	O
insight	O
to	O
users	O
as	O
to	O
why	O
the	O
conclusions	O
have	O
been	O
derived	O
from	O
the	O
given	O
input	B
data	O
.	O
the	O
weights	O
on	O
the	O
links	O
of	O
the	O
nodes	O
in	O
the	O
net	O
are	O
not	O
generally	O
easy	O
to	O
explain	O
from	O
a	O
viewpoint	O
of	O
human	O
understanding	O
.	O
lvq	O
is	O
also	O
based	O
on	O
a	O
kohonen	O
net	O
and	O
the	O
essential	O
difference	O
between	O
these	O
two	O
programs	O
is	O
that	O
lvq	O
uses	O
supervised	O
training	O
,	O
so	O
it	O
should	O
be	O
no	O
surprise	O
that	O
in	O
all	O
the	O
trials	O
(	O
with	O
the	O
exception	O
of	O
the	O
dna	O
dataset	O
)	O
the	O
results	O
of	O
lvq	O
are	O
better	O
than	O
those	O
of	O
sec	O
.	O
11.6	O
]	O
memory	O
and	O
time	O
223	O
kohonen	O
.	O
so	O
,	O
the	O
use	O
of	O
kohonen	O
should	O
be	O
limited	O
to	O
clustering	O
or	O
unsupervised	O
learning	O
,	O
and	O
lvq	O
should	O
always	O
be	O
preferred	O
for	O
standard	O
classiﬁcation	B
tasks	O
.	O
unfortunately	O
,	O
lvq	O
has	O
at	O
least	O
one	O
“	O
bug	O
”	O
that	O
may	O
give	O
seriously	O
misleading	O
results	O
,	O
so	O
the	O
output	B
should	O
be	O
checked	O
carefully	O
(	O
beware	O
reported	O
error	O
rates	O
of	O
zero	O
!	O
)	O
.	O
11.5.3	O
radial	O
basis	O
function	O
neural	O
network	O
the	O
radial	O
basis	O
function	O
neural	O
network	O
(	O
rbf	O
for	O
short	O
)	O
is	O
similar	O
to	O
other	O
neural	O
net	O
algorithms	O
.	O
but	O
it	O
uses	O
a	O
different	O
error	O
estimation	O
and	O
gradient	O
descent	O
function	O
–	O
i.e	O
.	O
the	O
radial	O
basis	O
function	O
.	O
similar	O
to	O
other	O
neural	O
net	O
algorithms	O
the	O
results	O
produced	O
by	O
rbf	O
are	O
very	O
difﬁcult	O
to	O
understand	O
.	O
rbf	O
uses	O
a	O
cross-validation	O
technique	O
to	O
handle	O
the	O
noise	O
.	O
as	O
the	O
algorithm	O
trains	O
it	O
continually	O
tests	O
on	O
a	O
small	O
set	O
called	O
the	O
“	O
cross-validation	O
set	O
”	O
.	O
when	O
the	O
error	O
on	O
this	O
set	O
starts	O
to	O
increase	O
it	O
stops	O
training	O
.	O
thus	O
it	O
can	O
automatically	O
decide	O
when	O
to	O
stop	O
training	O
,	O
which	O
is	O
a	O
major	O
advantage	O
of	O
this	O
algorithm	O
compared	O
to	O
other	O
neural	O
net	O
algorithms	O
.	O
however	O
it	O
can	O
not	O
cope	O
with	O
unknown	O
values	O
.	O
the	O
algorithm	O
is	O
fairly	O
well	O
implemented	O
so	O
it	O
is	O
relatively	O
easy	O
to	O
use	O
compared	O
to	O
many	O
neural	O
network	O
algorithms	O
.	O
because	O
it	O
only	O
has	O
one	O
parameter	O
to	O
adjust	O
for	O
each	O
new	O
application	O
–	O
the	O
number	O
of	O
centres	O
of	O
the	O
radial	O
basis	O
function	O
–	O
it	O
is	O
fairly	O
easy	O
to	O
use	O
.	O
11.5.4	O
dipol92	O
this	O
algorithm	O
has	O
been	O
included	O
as	O
a	O
neural	O
network	O
,	O
and	O
is	O
perhaps	O
closest	O
to	O
mada-	O
line	O
,	O
but	O
in	O
fact	O
it	O
is	O
rather	O
a	O
hybrid	O
,	O
and	O
could	O
also	O
have	O
been	O
classiﬁed	O
as	O
a	O
“	O
non-	O
parametric	O
”	O
statistical	B
algorithm	O
.	O
it	O
uses	O
methods	O
related	O
to	O
logistic	O
regression	O
in	O
the	O
ﬁrst	O
stage	O
,	O
except	O
that	O
it	O
sets	O
up	O
a	O
discriminating	O
hyperplane	O
between	O
all	O
pairs	O
of	O
classes	O
,	O
and	O
then	O
minimises	O
an	O
error	O
function	O
by	O
gradient	O
descent	O
.	O
in	O
addition	O
,	O
an	O
optional	O
clustering	O
procedure	O
allows	O
a	O
class	O
to	O
be	O
treated	O
as	O
several	O
subclasses	O
.	O
this	O
is	O
a	O
new	O
algorithm	O
and	O
the	O
results	O
are	O
very	O
encouraging	O
.	O
although	O
it	O
never	O
quite	O
comes	O
ﬁrst	O
in	O
any	O
one	O
trial	O
,	O
it	O
is	O
very	O
often	O
second	O
best	O
,	O
and	O
its	O
overall	O
performance	O
is	O
excellent	O
.	O
it	O
would	O
be	O
useful	O
to	O
quantify	O
how	O
much	O
the	O
success	O
of	O
dipol92	O
is	O
due	O
to	O
the	O
multi-way	O
hyperplane	O
treatment	O
,	O
and	O
how	O
much	O
is	O
due	O
to	O
the	O
initial	O
clustering	O
,	O
and	O
it	O
would	O
also	O
be	O
useful	O
to	O
automate	O
the	O
selection	O
of	O
clusters	O
(	O
at	O
present	O
the	O
number	O
of	O
subclasses	O
is	O
a	O
user-deﬁned	O
parameter	O
)	O
.	O
it	O
is	O
easy	O
to	O
use	O
,	O
and	O
is	O
intermediate	O
between	O
linear	O
discriminants	O
and	O
multilayer	O
perceptron	O
in	O
ease	O
of	O
interpretation	O
.	O
it	O
strengthens	O
the	O
case	O
for	O
other	O
hybrid	O
algorithms	O
to	O
be	O
explored	O
.	O
11.6	O
memory	O
and	O
time	O
so	O
far	O
we	O
have	O
said	O
very	O
little	O
about	O
either	O
memory	O
requirements	O
or	O
cpu	O
time	O
to	O
train	O
and	O
test	O
on	O
each	O
dataset	O
.	O
on	O
reason	O
for	O
this	O
is	O
that	O
these	O
can	O
vary	O
considerably	O
from	O
one	O
implementation	O
to	O
another	O
.	O
we	O
can	O
,	O
however	O
,	O
make	O
a	O
few	O
comments	O
.	O
11.6.1	O
memory	O
in	O
most	O
of	O
these	O
large	O
datasets	O
,	O
memory	O
was	O
not	O
a	O
problem	O
.	O
the	O
exception	O
to	O
this	O
was	O
the	O
full	O
version	O
of	O
the	O
hand-written	O
digit	O
dataset	O
–	O
see	O
section	O
9.3.1.	O
this	O
dataset	O
had	O
256	O
variables	O
and	O
10,000	O
examples	O
and	O
most	O
algorithms	O
(	O
running	O
on	O
an	O
8	O
mb	O
machine	O
)	O
could	O
224	O
conclusions	O
[	O
ch	O
.	O
11	O
not	O
handle	O
it	O
.	O
however	O
,	O
such	O
problems	O
are	O
likely	O
to	O
be	O
rare	O
in	O
most	O
applications	O
.	O
a	O
problem	O
with	O
the	O
interpretation	O
of	O
these	O
ﬁgures	O
is	O
that	O
they	O
were	O
obtained	O
from	O
the	O
unix	O
command	O
set	O
time	O
=	O
(	O
0	O
``	O
%	O
u	O
%	O
s	O
%	O
m	O
''	O
)	O
and	O
,	O
for	O
a	O
simple	O
fortran	O
program	O
for	O
example	B
,	O
the	O
output	B
is	O
directly	O
related	O
to	O
the	O
dimension	O
declarations	O
.	O
so	O
an	O
edited	O
version	O
could	O
be	O
cut	B
to	O
ﬁt	O
the	O
given	O
dataset	O
and	O
produce	O
a	O
“	O
smaller	O
memory	O
requirement	O
”	O
.	O
a	O
more	O
sensible	O
way	O
to	O
quantify	O
memory	O
would	O
be	O
in	O
terms	O
of	O
the	O
size	O
of	O
the	O
data	O
.	O
for	O
example	B
,	O
in	O
the	O
sas	O
manual	O
(	O
1985	O
)	O
it	O
states	O
that	O
the	O
memory	O
required	O
for	O
nearest	O
neighbour	O
isñ	O
for	O
most	O
situations	O
,	O
of	O
order¼	O
.	O
if	O
similar	O
results	O
could	O
be	O
stated	O
for	O
all	O
our	O
algorithms	O
this	O
would	O
make	O
comparisons	O
much	O
more	O
transparent	O
,	O
and	O
also	O
enable	O
predictions	O
for	O
new	O
datasets	O
.	O
as	O
far	O
as	O
our	O
results	O
are	O
concerned	O
,	O
it	O
is	O
clear	O
that	O
the	O
main	O
difference	O
in	O
memory	O
requirements	O
will	O
depend	O
on	O
whether	O
the	O
algorithm	O
has	O
to	O
store	O
all	O
the	O
data	O
or	O
can	O
process	O
it	O
in	O
pieces	O
.	O
the	O
theory	O
should	O
determine	O
this	O
as	O
well	O
as	O
the	O
numbers	O
,	O
but	O
it	O
is	O
clear	O
that	O
linear	O
and	O
quadratic	O
discriminant	O
classiﬁers	O
are	O
the	O
most	O
efﬁcient	O
here	O
.	O
¥¬	O
»	O
pñnc¼j	O
ñn	O
,	O
i.e	O
.	O
¨	O
»	O
¤	O
»	O
11.6.2	O
time	O
again	O
,	O
the	O
results	O
here	O
are	O
rather	O
confusing	O
.	O
the	O
times	O
do	O
not	O
always	O
measure	B
the	O
same	O
thing	O
,	O
for	O
example	B
if	O
there	O
are	O
parameters	O
to	O
select	O
there	O
are	O
two	O
options	O
.	O
userú	O
may	O
decide	O
½	O
may	O
decide	O
to	O
choose	O
the	O
parameters	O
by	O
cross-validation	O
and	O
reduce	O
the	O
error	O
rate	O
at	O
the	O
to	O
just	O
plug	O
in	O
the	O
parameter	O
(	O
s	O
)	O
and	O
suffer	O
a	O
slight	O
loss	O
in	O
accuracy	O
of	O
the	O
classiﬁer	B
.	O
user	O
expense	O
of	O
a	O
vastly	O
inﬂated	O
training	O
time	O
.	O
it	O
is	O
clear	O
then	O
,	O
that	O
more	O
explanation	O
is	O
required	O
and	O
a	O
more	O
thorough	O
investigation	O
to	O
determine	O
selection	O
of	O
parameters	O
and	O
the	O
trade-off	O
between	O
time	O
and	O
error	O
rate	O
in	O
individual	O
circumstances	O
.	O
there	O
are	O
other	O
anomalies	O
:	O
for	O
example	B
,	O
smart	O
often	O
quotes	O
the	O
smallest	O
time	O
to	O
test	O
,	O
and	O
the	O
amount	O
of	O
computation	O
required	O
is	O
a	O
superset	O
of	O
that	O
required	O
for	O
discrim	O
,	O
which	O
usually	O
takes	O
longer	O
.	O
so	O
it	O
appears	O
that	O
the	O
interpretation	O
of	O
results	O
will	O
again	O
be	O
inﬂuenced	O
by	O
the	O
implementation	O
.	O
it	O
is	O
of	O
interest	O
that	O
smart	O
has	O
the	O
largest	O
ratio	O
of	O
training	O
to	O
testing	O
time	O
in	O
nearly	O
all	O
of	O
the	O
datasets	O
.	O
as	O
with	O
memory	O
requirements	O
,	O
a	O
statement	O
that	O
time	O
is	O
proportional	O
to	O
some	O
function	O
of	O
the	O
data	O
size	O
would	O
be	O
preferred	O
.	O
for	O
example	B
,	O
the	O
sas	O
manual	O
quotes	O
the	O
time	O
is	O
the	O
number	O
of	O
observations	O
in	O
the	O
training	O
data	O
.	O
the	O
above	O
warnings	O
should	O
make	O
us	O
cautious	O
in	O
drawing	O
conclusions	O
,	O
in	O
that	O
some	O
algorithms	O
may	O
not	O
require	O
parameter	O
selection	O
.	O
however	O
,	O
if	O
we	O
sum	O
the	O
training	O
and	O
testing	O
times	O
,	O
we	O
can	O
say	O
generally	O
that	O
for	O
the	O
nearest	O
neighbour	O
classiﬁer	B
to	O
test	O
as	O
proportional	O
to ¼	O
»	O
where 	O
indcart	O
takes	O
longer	O
than	O
cart	O
among	O
the	O
statistical	B
algorithms	O
,	O
the	O
“	O
nonparametric	O
”	O
ones	O
take	O
longer	O
,	O
especially	O
k-nn	O
,	O
smart	O
and	O
alloc80	O
among	O
the	O
decision	O
tree	O
algorithmsúüû	O
ï	O
and	O
itrule	O
take	O
longer	O
among	O
the	O
neural	O
net	O
algorithms	O
,	O
dipol92	O
is	O
probably	O
the	O
quickest	O
11.7	O
general	O
issues	O
11.7.1	O
cost	O
matrices	O
if	O
a	O
cost	O
matrix	O
is	O
involved	O
,	O
be	O
warned	O
that	O
only	O
cart	O
,	O
cal5	O
,	O
the	O
statistical	B
procedures	O
and	O
some	O
of	O
the	O
neural	O
nets	O
take	O
costs	O
into	O
account	O
at	O
all	O
.	O
even	O
then	O
,	O
with	O
the	O
exception	O
of	O
	O
j	O
»	O
	O
»	O
	O
	O
	O
	O
sec	O
.	O
11.7	O
]	O
general	O
issues	O
225	O
dipol92	O
and	O
smart	O
,	O
they	O
do	O
not	O
use	O
costs	O
as	O
part	O
of	O
the	O
learning	O
process	O
.	O
of	O
those	O
algorithms	O
which	O
do	O
not	O
incorporate	O
costs	O
,	O
many	O
output	B
a	O
measure	B
which	O
can	O
be	O
interpreted	O
as	O
a	O
probability	O
,	O
and	O
costs	O
could	O
therefore	O
be	O
incorporated	O
.	O
this	O
book	O
has	O
only	O
considered	O
three	O
datasets	O
which	O
include	O
costs	O
partly	O
for	O
the	O
very	O
reason	O
that	O
some	O
of	O
the	O
decision	O
tree	O
programs	O
can	O
not	O
cope	O
with	O
them	O
.	O
there	O
is	O
a	O
clear	O
need	O
to	O
have	O
the	O
option	O
of	O
incorporating	O
a	O
cost	O
matrix	O
into	O
all	O
classiﬁcation	B
algorithms	O
,	O
and	O
in	O
principle	O
this	O
should	O
be	O
a	O
simple	O
matter	O
.	O
11.7.2	O
interpretation	O
of	O
error	O
rates	O
the	O
previous	O
chapter	O
has	O
already	O
analysed	O
the	O
results	O
from	O
the	O
trials	O
and	O
some	O
sort	O
of	O
a	O
pattern	O
is	O
emerging	O
.	O
it	O
is	O
hoped	O
that	O
one	O
day	O
we	O
can	O
ﬁnd	O
a	O
set	O
of	O
measures	O
which	O
can	O
be	O
obtained	O
from	O
the	O
data	O
,	O
and	O
then	O
using	O
these	O
measures	O
alone	O
we	O
can	O
predict	O
with	O
some	O
degree	O
of	O
conﬁdence	O
which	O
algorithms	O
or	O
methods	O
will	O
perform	O
the	O
best	O
.	O
there	O
is	O
some	O
theory	O
here	O
,	O
for	O
example	B
,	O
the	O
similarity	O
of	O
within-class	O
covariance	O
matrices	O
will	O
determine	O
the	O
relative	O
performance	O
of	O
linear	O
and	O
quadratic	O
discriminant	O
functions	O
and	O
also	O
the	O
performance	O
of	O
these	O
relative	O
to	O
decision	O
tree	O
methods	O
(	O
qualitative	O
conditional	O
dependencies	O
will	O
favour	O
trees	O
)	O
.	O
however	O
,	O
from	O
an	O
empirical	O
perspective	O
there	O
is	O
still	O
some	O
way	O
to	O
go	O
,	O
both	O
from	O
the	O
point	O
of	O
view	O
of	O
determining	O
which	O
measures	O
are	O
important	O
,	O
and	O
how	O
best	O
to	O
make	O
the	O
prediction	O
.	O
the	O
attempts	O
of	O
the	O
previous	O
chapter	O
show	O
how	O
this	O
may	O
be	O
done	O
,	O
although	O
more	O
datasets	O
are	O
required	O
before	O
conﬁdence	O
can	O
be	O
attached	O
to	O
the	O
conclusions	O
.	O
the	O
request	O
for	O
more	O
datasets	O
raises	O
another	O
issue	O
:	O
what	O
kind	O
of	O
datasets	O
?	O
it	O
is	O
clear	O
that	O
we	O
could	O
obtain	O
very	O
biased	O
results	O
if	O
we	O
limit	O
our	O
view	O
to	O
certain	O
types	O
,	O
and	O
the	O
question	O
of	O
what	O
is	O
representative	O
is	O
certainly	O
unanswered	O
.	O
section	O
2.1.3	O
outlines	O
a	O
number	O
of	O
different	O
dataset	O
types	O
,	O
and	O
is	O
likely	O
that	O
this	O
consideration	O
will	O
play	O
the	O
most	O
important	O
rˆole	O
in	O
determining	O
the	O
choice	O
of	O
algorithm	O
.	O
the	O
comparison	O
of	O
algorithms	O
here	O
is	O
almost	O
entirely	O
of	O
a	O
“	O
black-box	O
”	O
nature	O
.	O
so	O
the	O
recommendations	O
as	O
they	O
stand	O
are	O
really	O
only	O
applicable	O
to	O
the	O
“	O
na¨ıve	O
”	O
user	O
.	O
in	O
the	O
hands	O
of	O
an	O
expert	O
the	O
performance	O
of	O
an	O
algorithm	O
can	O
be	O
radically	O
different	O
,	O
and	O
of	O
course	O
there	O
is	O
always	O
the	O
possibility	O
of	O
transforming	O
or	O
otherwise	O
preprocessing	B
the	O
data	O
.	O
these	O
considerations	O
will	O
often	O
outweigh	O
any	O
choice	O
of	O
algorithm	O
.	O
11.7.3	O
structuring	O
the	O
results	O
much	O
of	O
the	O
analysis	O
in	O
the	O
previous	O
chapter	O
depends	O
on	O
the	O
scaling	O
of	O
the	O
results	O
.	O
it	O
is	O
clear	O
that	O
to	O
combine	O
results	O
across	O
many	O
datasets	O
,	O
care	O
will	O
need	O
to	O
be	O
taken	O
that	O
they	O
are	O
treated	O
equally	O
.	O
in	O
sections	O
10.4	O
and	O
10.7	O
the	O
scaling	O
was	O
taken	O
so	O
that	O
the	O
error	O
rates	O
(	O
or	O
costs	O
)	O
for	O
each	O
dataset	O
were	O
mapped	O
to	O
the	O
interval¾	O
x	O
<	O
ñ¿	O
,	O
whereas	O
in	O
section	O
10.6	O
the	O
scaling	O
was	O
done	O
using	O
an	O
estimated	O
standard	O
error	O
for	O
the	O
error	O
rates	O
(	O
or	O
costs	O
)	O
.	O
the	O
different	O
approaches	O
makes	O
the	O
interpretation	O
of	O
the	O
comparison	O
in	O
section	O
10.7.1	O
rather	O
difﬁcult	O
.	O
the	O
pattern	O
which	O
emerges	O
from	O
the	O
multidimensional	O
scaling	O
and	O
associated	O
hierar-	O
chical	O
clustering	O
of	O
the	O
algorithms	O
is	O
very	O
encouraging	O
.	O
it	O
is	O
clear	O
that	O
there	O
is	O
a	O
strong	O
similarity	O
between	O
the	O
construction	O
and	O
the	O
performance	O
of	O
the	O
algorithms	O
.	O
the	O
hierarchi-	O
cal	O
clustering	O
of	O
the	O
datasets	O
is	O
not	O
so	O
convincing	O
.	O
however	O
,	O
the	O
overall	O
picture	O
in	O
figure	O
10.3	O
conﬁrms	O
the	O
breakdown	O
of	O
analysis	O
by	O
subject	O
area	O
(	O
see	O
section	O
10.2	O
)	O
in	O
that	O
convex	O
hulls	O
which	O
do	O
not	O
overlap	O
can	O
be	O
drawn	O
around	O
the	O
datasets	O
of	O
the	O
speciﬁc	O
subject	O
areas	O
.	O
	O
226	O
conclusions	O
[	O
ch	O
.	O
11	O
an	O
outlier	O
here	O
is	O
the	O
tsetse	O
ﬂy	O
data	O
-	O
which	O
could	O
also	O
easily	O
been	O
placed	O
in	O
the	O
category	O
of	O
“	O
image	O
datasets	O
:	O
segmentation	O
”	O
,	O
since	O
the	O
data	O
are	O
of	O
a	O
spatial	O
nature	O
,	O
although	O
it	O
is	O
not	O
a	O
standard	O
image	O
!	O
the	O
analysis	O
of	O
section	O
10.6	O
is	O
a	O
promising	O
one	O
,	O
though	O
there	O
is	O
not	O
enough	O
data	O
to	O
make	O
strong	O
conclusions	O
or	O
to	O
take	O
the	O
rules	O
too	O
seriously	O
.	O
however	O
,	O
it	O
might	O
be	O
better	O
to	O
predict	O
performance	O
on	O
a	O
continuous	O
scale	O
rather	O
than	O
the	O
current	O
approach	O
which	O
discretises	O
the	O
algorithms	O
into	O
“	O
applicable	O
”	O
and	O
“	O
non-applicable	O
”	O
.	O
indeed	O
,	O
the	O
choice	O
of	O
(	O
see	O
section	O
10.6.3	O
)	O
is	O
very	O
much	O
larger	O
than	O
the	O
more	O
commonly	O
used	O
2	O
or	O
3	O
standard	O
errors	O
in	O
hypothesis	O
testing	O
.	O
that	O
the	O
error	O
rate	O
for	O
alloc80	O
could	O
be	O
predicted	O
by	O
taking	O
0.8	O
the	O
attempts	O
to	O
predict	O
performance	O
using	O
the	O
performance	O
of	O
“	O
benchmark	O
”	O
algorithms	O
(	O
see	O
section	O
10.7	O
)	O
is	O
highly	O
dependent	O
on	O
the	O
choice	O
of	O
datasets	O
used	O
.	O
also	O
,	O
it	O
needs	O
to	O
be	O
remembered	O
that	O
the	O
coefﬁcients	O
reported	O
in	O
table	O
10.14	O
are	O
not	O
absolute	O
.	O
they	O
are	O
again	O
based	O
on	O
a	O
transformation	O
of	O
all	O
the	O
results	O
to	O
the	O
unit	O
interval	O
.	O
so	O
for	O
example	B
,	O
the	O
result	O
the	O
error	O
rate	O
for	O
k-nn	O
takes	O
into	O
account	O
the	O
error	O
rates	O
for	O
all	O
of	O
the	O
other	O
algorithms	O
.	O
if	O
we	O
only	O
consider	O
this	O
pair	O
(	O
k-nn	O
and	O
alloc80	O
)	O
then	O
we	O
get	O
a	O
coefﬁcient	O
of	O
0.9	O
but	O
this	O
is	O
still	O
inﬂuenced	O
by	O
one	O
or	O
two	O
observations	O
.	O
an	O
alternative	O
is	O
to	O
consider	O
the	O
average	O
percentage	O
improvement	O
of	O
alloc80	O
,	O
which	O
is	O
6.4	O
%	O
,	O
but	O
none	O
of	O
these	O
possibilities	O
takes	O
account	O
of	O
the	O
different	O
sample	O
sizes	O
.	O
¥	O
or	O
ñ	O
11.7.4	O
removal	O
of	O
irrelevant	O
attributes	O
there	O
are	O
many	O
examples	O
where	O
the	O
performance	O
of	O
algorithms	O
may	O
be	O
improved	O
by	O
removing	O
irrelevant	O
attributes	O
.	O
a	O
speciﬁc	O
example	B
is	O
the	O
dna	O
dataset	O
,	O
where	O
the	O
middle	O
20	O
of	O
the	O
60	O
nominal	O
attributes	O
are	O
by	O
far	O
the	O
most	O
relevant	O
.	O
if	O
a	O
decision	O
tree	O
,	O
for	O
example	B
,	O
is	O
presented	O
with	O
this	O
middle	O
section	O
of	O
the	O
data	O
,	O
it	O
performs	O
much	O
better	O
.	O
the	O
same	O
is	O
true	O
of	O
quadratic	O
discriminants	O
,	O
and	O
,	O
this	O
is	O
a	O
very	O
general	O
problem	O
with	O
black-box	O
procedures	O
.	O
there	O
are	O
ways	O
of	O
removing	O
variables	O
in	O
linear	O
discriminants	O
,	O
for	O
example	B
,	O
but	O
these	O
did	O
not	O
have	O
much	O
effect	O
on	O
accuracy	O
,	O
and	O
this	O
variable	O
selection	O
method	O
does	O
not	O
extend	O
to	O
other	O
algorithms	O
.	O
11.7.5	O
diagnostics	O
and	O
plotting	O
very	O
few	O
procedures	O
contain	O
internal	O
consistency	O
checks	O
.	O
even	O
where	O
they	O
are	O
available	O
in	O
principle	O
,	O
they	O
have	O
not	O
been	O
programmed	O
into	O
our	O
implementations	O
.	O
for	O
example	B
,	O
quadratic	O
discrimination	O
relies	O
on	O
multivariatenormality	O
,	O
and	O
there	O
are	O
tests	O
for	O
this	O
,	O
but	O
they	O
are	O
programmed	O
separately	O
.	O
similarly	O
castle	O
should	O
be	O
able	O
to	O
check	O
if	O
the	O
assumption	O
of	O
polytree	O
structure	O
is	O
a	O
reasonable	O
one	O
,	O
but	O
this	O
is	O
not	O
programmed	O
in	O
.	O
the	O
user	O
must	O
then	O
rely	O
on	O
other	O
ways	O
of	O
doing	O
such	O
checks	O
.	O
an	O
important	O
,	O
but	O
very	O
much	O
underused	O
,	O
method	O
is	O
simply	O
to	O
plot	O
selected	O
portions	O
of	O
the	O
data	O
,	O
for	O
example	B
pairs	O
of	O
coordinates	O
with	O
the	O
classes	O
as	O
labels	O
.	O
this	O
often	O
gives	O
very	O
important	O
insights	O
into	O
the	O
data	O
.	O
the	O
manova	O
procedure	O
,	O
multidimensional	O
scaling	O
,	O
principal	O
components	O
and	O
projection	O
pursuit	O
(	O
smart	O
)	O
all	O
give	O
useful	O
ways	O
in	O
which	O
multidimensional	O
data	O
can	O
be	O
plotted	O
in	O
two	O
dimensions	O
.	O
11.7.6	O
exploratory	O
data	O
if	O
the	O
object	O
of	O
the	O
exercise	O
is	O
to	O
explore	O
the	O
process	O
underlying	O
the	O
classiﬁcations	O
them-	O
selves	O
,	O
for	O
example	B
by	O
ﬁnding	O
out	O
which	O
variables	O
are	O
important	O
or	O
by	O
gaining	O
an	O
insight	O
sec	O
.	O
11.7	O
]	O
general	O
issues	O
227	O
into	O
the	O
structure	O
of	O
the	O
classiﬁcation	B
process	O
,	O
then	O
neural	O
nets	O
,	O
k-nearest	O
neighbour	O
and	O
alloc80	O
are	O
unlikely	O
to	O
be	O
much	O
use	O
.	O
no	O
matter	O
what	O
procedure	O
is	O
actually	O
used	O
,	O
it	O
is	O
often	O
best	O
to	O
prune	O
radically	O
,	O
by	O
keeping	O
only	O
two	O
or	O
three	O
signiﬁcant	O
terms	O
in	O
a	O
regression	O
,	O
or	O
by	O
using	O
trees	O
of	O
depth	O
two	O
,	O
or	O
using	O
only	O
a	O
small	O
number	O
of	O
rules	O
,	O
in	O
the	O
hope	O
that	O
the	O
important	O
structure	O
is	O
retained	O
.	O
less	O
important	O
structures	O
can	O
be	O
added	O
on	O
later	O
as	O
greater	O
accuracy	O
is	O
required	O
.	O
it	O
should	O
also	O
be	O
borne	O
in	O
mind	O
that	O
in	O
exploratory	O
work	O
it	O
is	O
common	O
to	O
include	O
anything	O
at	O
all	O
that	O
might	O
conceivably	O
be	O
relevant	O
,	O
and	O
that	O
often	O
the	O
ﬁrst	O
task	O
is	O
to	O
weed	O
out	O
the	O
irrelevant	O
information	O
before	O
the	O
task	O
of	O
exploring	O
structure	O
can	O
begin	O
.	O
11.7.7	O
special	O
features	O
if	O
a	O
particular	O
application	O
has	O
some	O
special	O
features	O
such	O
as	O
missing	O
values	O
,	O
hierarchical	O
structure	O
in	O
the	O
attributes	O
,	O
ordered	O
classes	O
,	O
presence	O
of	O
known	O
subgroups	O
within	O
classes	O
(	O
hierarchy	O
of	O
classes	O
)	O
,	O
etc	O
.	O
etc.	O
,	O
this	O
extra	O
structure	O
can	O
be	O
used	O
in	O
the	O
classiﬁcation	B
process	O
to	O
improve	O
performance	O
and	O
to	O
improve	O
understanding	O
.	O
also	O
,	O
it	O
is	O
crucial	O
to	O
understand	O
if	O
the	O
class	O
values	O
are	O
in	O
any	O
sense	O
random	O
variables	O
,	O
or	O
outcomes	O
of	O
a	O
chance	O
experiment	O
,	O
as	O
this	O
alters	O
radically	O
the	O
approach	O
that	O
should	O
be	O
adopted	O
.	O
the	O
procrustean	O
approach	O
of	O
forcing	O
all	O
datasets	O
into	O
a	O
common	O
format	O
,	O
as	O
we	O
have	O
done	O
in	O
the	O
trials	O
of	O
this	O
book	O
for	O
comparative	O
purposes	O
,	O
is	O
not	O
recommended	O
in	O
general	O
.	O
the	O
general	O
rule	O
is	O
to	O
use	O
all	O
the	O
available	O
external	O
information	O
,	O
and	O
not	O
to	O
throw	O
it	O
away	O
.	O
11.7.8	O
from	O
classiﬁcation	B
to	O
knowledge	O
organisation	O
and	O
synthesis	O
in	O
chapter	O
5	O
it	O
was	O
stressed	O
that	O
machine	O
learning	O
classiﬁers	O
should	O
possess	O
a	O
mental	B
ﬁt	I
to	O
the	O
data	O
,	O
so	O
that	O
the	O
learned	O
concepts	O
are	O
meaningful	O
to	O
and	O
evaluable	O
by	O
humans	O
.	O
on	O
this	O
criterion	O
,	O
the	O
neural	O
net	O
algorithms	O
are	O
relatively	O
opaque	O
,	O
whereas	O
most	O
of	O
the	O
statistical	B
methods	O
which	O
do	O
not	O
have	O
mental	B
ﬁt	I
can	O
at	O
least	O
determine	O
which	O
of	O
the	O
attributes	O
are	O
important	O
.	O
however	O
,	O
the	O
speciﬁc	O
black-box	O
use	O
of	O
methods	O
would	O
(	O
hopefully	O
!	O
)	O
never	O
take	O
place	O
,	O
and	O
it	O
is	O
worth	O
looking	O
forwards	O
more	O
speculatively	O
to	O
ai	O
uses	O
of	O
classiﬁcation	B
methods	O
.	O
for	O
example	B
,	O
kardio	O
’	O
s	O
comprehensive	O
treatise	O
on	O
ecg	O
interpretation	O
(	O
bratko	O
et	O
al.	O
,	O
1989	O
)	O
does	O
not	O
contain	O
a	O
single	O
rule	O
of	O
human	O
authorship	O
.	O
seen	O
in	O
this	O
light	O
,	O
it	O
becomes	O
clear	O
that	O
classiﬁcation	B
and	O
discrimination	O
are	O
not	O
narrow	O
ﬁelds	O
within	O
statistics	O
or	O
machine	O
learning	O
,	O
but	O
that	O
the	O
art	O
of	O
classiﬁcation	B
can	O
generate	O
substantial	O
contributions	O
to	O
organise	O
(	O
and	O
improve	O
)	O
human	O
knowledge	O
,	O
–	O
even	O
,	O
as	O
in	O
kardio	O
,	O
to	O
manufacture	O
new	O
knowledge	O
.	O
another	O
context	O
in	O
which	O
knowledge	O
derived	O
from	O
humans	O
and	O
data	O
is	O
synthesised	O
is	O
in	O
the	O
area	O
of	O
bayesian	O
expert	O
systems	O
(	O
spiegelhalter	O
et	O
al.	O
,	O
1993	O
)	O
,	O
in	O
which	O
subjective	O
judgments	O
of	O
model	O
structure	O
and	O
conditional	O
probabilities	O
are	O
formally	O
combined	O
with	O
likelihoods	O
derived	O
from	O
data	O
by	O
bayes	O
theorem	O
:	O
this	O
provides	O
a	O
way	O
for	O
a	O
system	O
to	O
smoothly	O
adapt	O
a	O
model	O
from	O
being	O
initially	O
expert-based	O
towards	O
one	O
derived	O
from	O
data	O
.	O
however	O
,	O
this	O
representation	O
of	O
knowledge	O
by	O
causal	O
nets	O
is	O
necessarily	O
rather	O
restricted	O
because	O
it	O
does	O
demand	O
an	O
exhaustive	O
speciﬁcation	O
of	O
the	O
full	O
joint	O
distribution	O
.	O
however	O
,	O
such	O
systems	O
form	O
a	O
complete	O
model	O
of	O
a	O
process	O
and	O
are	O
intended	O
for	O
more	O
than	O
sim-	O
ply	O
classiﬁcation	B
.	O
indeed	O
,	O
they	O
provide	O
a	O
uniﬁed	O
structure	O
for	O
many	O
complex	O
stochastic	O
problems	O
,	O
with	O
connections	O
to	O
image	O
processing	O
,	O
dynamic	O
modelling	O
and	O
so	O
on	O
.	O
12	O
knowledge	O
representation	O
claude	O
sammut	O
university	O
of	O
new	O
south	O
wales	O
12.1	O
introduction	O
in	O
1956	O
,	O
bruner	O
,	O
goodnow	O
and	O
austin	O
published	O
their	O
book	O
a	O
study	O
of	O
thinking	O
,	O
which	O
became	O
a	O
landmark	O
in	O
psychology	O
and	O
would	O
later	O
have	O
a	O
major	O
impact	O
on	O
machine	O
learn-	O
ing	O
.	O
the	O
experiments	O
reported	O
by	O
bruner	O
,	O
goodnow	O
and	O
austin	O
were	O
directed	O
towards	O
understanding	O
a	O
human	O
’	O
s	O
ability	O
to	O
categorise	O
and	O
how	O
categories	O
are	O
learned	O
.	O
we	O
begin	O
with	O
what	O
seems	O
a	O
paradox	O
.	O
the	O
world	O
of	O
experience	O
of	O
any	O
normal	O
man	O
is	O
composed	O
of	O
a	O
tremendous	O
array	O
of	O
discriminably	O
different	O
objects	O
,	O
events	O
,	O
people	O
,	O
impressions	O
...	O
but	O
were	O
we	O
to	O
utilise	O
fully	O
our	O
capacity	O
for	O
registering	O
the	O
differences	O
in	O
things	O
and	O
to	O
respond	O
to	O
each	O
event	O
encountered	O
as	O
unique	O
,	O
we	O
would	O
soon	O
be	O
overwhelmed	O
by	O
the	O
complexity	O
of	O
our	O
environment	O
...	O
the	O
resolution	O
of	O
this	O
seeming	O
paradox	O
...	O
is	O
achieved	O
by	O
man	O
’	O
s	O
capacity	O
to	O
categorise	O
.	O
to	O
categorise	O
is	O
to	O
render	O
discriminably	O
different	O
things	O
equivalent	O
,	O
to	O
group	O
objects	O
and	O
events	O
and	O
people	O
around	O
us	O
into	O
classes	O
...	O
the	O
process	O
of	O
categorizing	O
involves	O
...	O
an	O
act	O
of	O
invention	O
...	O
if	O
we	O
have	O
learned	O
the	O
class	O
“	O
house	O
”	O
as	O
a	O
concept	O
,	O
new	O
exemplars	O
can	O
be	O
readily	O
recognised	O
.	O
the	O
category	O
becomes	O
a	O
tool	O
for	O
further	O
use	O
.	O
the	O
learning	O
and	O
utilisation	O
of	O
categories	O
represents	O
one	O
of	O
the	O
most	O
elementary	O
and	O
general	O
forms	O
of	O
cognition	O
by	O
which	O
man	O
adjusts	O
to	O
his	O
environment	O
.	O
the	O
ﬁrst	O
question	O
that	O
they	O
had	O
to	O
deal	O
with	O
was	O
that	O
of	O
representation	O
:	O
what	O
is	O
a	O
con-	O
cept	O
?	O
they	O
assumed	O
that	O
objects	O
and	O
events	O
could	O
be	O
described	O
by	O
a	O
set	O
of	O
attributes	O
and	O
were	O
concerned	O
with	O
how	O
inferences	O
could	O
be	O
drawn	O
from	O
attributes	O
to	O
class	O
membership	O
.	O
categories	O
were	O
considered	O
to	O
be	O
of	O
three	O
types	O
:	O
conjunctive	O
,	O
disjunctive	O
and	O
relational	O
.	O
...	O
when	O
one	O
learns	O
to	O
categorise	O
a	O
subset	O
of	O
events	O
in	O
a	O
certain	O
way	O
,	O
one	O
is	O
doing	O
more	O
than	O
simply	O
learning	O
to	O
recognise	O
instances	O
encountered	O
.	O
one	O
is	O
also	O
learning	O
a	O
rule	O
that	O
may	O
be	O
applied	O
to	O
new	O
instances	O
.	O
the	O
concept	O
or	O
category	O
is	O
basically	O
,	O
this	O
“	O
rule	O
of	O
grouping	O
”	O
and	O
it	O
is	O
such	O
rules	O
that	O
one	O
constructs	O
in	O
forming	O
and	O
attaining	O
concepts	O
.	O
¸	O
address	O
for	O
correspondence	O
:	O
school	O
of	O
computer	O
science	O
and	O
engineering	O
,	O
artiﬁcialintelligence	O
laboratory	O
,	O
university	O
of	O
new	O
south	O
wales	O
,	O
po	O
box	O
1	O
,	O
kensigton	O
,	O
nsw	O
2033	O
,	O
australia	O
sec	O
.	O
12.2	O
]	O
learning	O
,	O
measurement	O
and	O
representation	O
229	O
the	O
notion	O
of	O
a	O
rule	O
as	O
an	O
abstract	O
representation	O
of	O
a	O
concept	O
in	O
the	O
human	O
mind	O
came	O
to	O
be	O
questioned	O
by	O
psychologists	O
and	O
there	O
is	O
still	O
no	O
good	O
theory	O
to	O
explain	O
how	O
we	O
store	O
concepts	O
.	O
however	O
,	O
the	O
same	O
questions	O
about	O
the	O
nature	O
of	O
representation	O
arise	O
in	O
machine	O
learning	O
,	O
for	O
the	O
choice	O
of	O
representation	O
heavily	O
determines	O
the	O
nature	O
of	O
a	O
learning	O
algorithm	O
.	O
thus	O
,	O
one	O
critical	O
point	O
of	O
comparison	O
among	O
machine	O
learning	O
algorithms	O
is	O
the	O
method	O
of	O
knowledge	O
representation	O
employed	O
.	O
in	O
this	O
chapter	O
we	O
will	O
discuss	O
various	O
methods	O
of	O
representation	O
and	O
compare	O
them	O
according	O
to	O
their	O
power	O
to	O
express	O
complex	O
concepts	O
and	O
the	O
effects	O
of	O
representation	O
on	O
the	O
time	O
and	O
space	O
costs	O
of	O
learning	O
.	O
,	O
and	O
some	O
input	B
,	O
à	O
12.2	O
learning	O
,	O
measurement	O
and	O
representation	O
a	O
learning	O
program	O
is	O
one	O
that	O
is	O
capable	O
of	O
improving	O
its	O
performance	O
through	O
experience	O
.	O
,	O
a	O
normal	O
program	O
would	O
yield	O
the	O
same	O
result	O
given	O
a	O
program	O
,	O
	O
	O
jà+n	O
âá	O
after	O
every	O
application	O
.	O
however	O
,	O
a	O
learning	O
program	O
can	O
alter	O
its	O
initial	O
state	O
ùnã	O
pá	O
.	O
so	O
that	O
its	O
performance	O
is	O
modiﬁed	O
with	O
each	O
application	O
.	O
thus	O
,	O
we	O
can	O
sayjài	O
,	O
given	O
the	O
initial	O
state	O
,	O
ù	O
.	O
the	O
that	O
is	O
,	O
á	O
goal	O
of	O
learning	O
is	O
to	O
construct	O
a	O
new	O
initial	O
,	O
ùä	O
,	O
so	O
that	O
the	O
program	O
alters	O
its	O
behaviour	O
to	O
give	O
a	O
more	O
accurate	O
or	O
quicker	O
result	O
.	O
thus	O
,	O
one	O
way	O
of	O
thinking	O
about	O
what	O
a	O
learning	O
program	O
does	O
is	O
that	O
it	O
builds	O
an	O
increasingly	O
accurate	O
approximation	O
to	O
a	O
mapping	O
from	O
input	B
to	O
output	B
.	O
is	O
the	O
result	O
of	O
applying	O
program	O
to	O
input	B
,	O
à	O
the	O
most	O
common	O
learning	O
task	O
is	O
that	O
of	O
acquiring	O
a	O
function	O
which	O
maps	O
objects	O
,	O
that	O
share	O
common	O
properties	O
,	O
to	O
the	O
same	O
class	O
value	O
.	O
this	O
is	O
the	O
categorisation	O
problem	O
to	O
which	O
bruner	O
,	O
goodnow	O
and	O
austin	O
referred	O
and	O
much	O
of	O
our	O
discussion	O
will	O
be	O
concerned	O
with	O
categorisation	O
.	O
learning	O
experience	O
may	O
be	O
in	O
the	O
form	O
of	O
examples	O
from	O
a	O
trainer	O
or	O
the	O
results	O
of	O
trial	O
and	O
error	O
.	O
in	O
either	O
case	O
,	O
the	O
program	O
must	O
be	O
able	O
to	O
represent	O
its	O
observations	O
of	O
the	O
world	O
,	O
and	O
it	O
must	O
also	O
be	O
able	O
to	O
represent	O
hypotheses	O
about	O
the	O
patterns	O
it	O
may	O
ﬁnd	O
in	O
those	O
observations	O
.	O
thus	O
,	O
we	O
will	O
often	O
refer	O
to	O
the	O
observation	O
language	O
and	O
the	O
hypothesis	O
language	O
.	O
the	O
observation	O
language	O
describes	O
the	O
inputs	O
and	O
outputs	O
of	O
the	O
program	O
and	O
the	O
hypothesis	O
language	O
describes	O
the	O
internal	O
state	O
of	O
the	O
learning	O
program	O
,	O
which	O
corresponds	O
to	O
its	O
theory	O
of	O
the	O
concepts	O
or	O
patterns	O
that	O
exist	O
in	O
the	O
data	O
.	O
the	O
input	B
to	O
a	O
learning	O
program	O
consists	O
of	O
descriptions	O
of	O
objects	O
from	O
the	O
universe	O
and	O
,	O
in	O
the	O
case	O
of	O
supervised	O
learning	O
,	O
an	O
output	B
value	O
associated	O
with	O
the	O
example	B
.	O
the	O
universe	O
can	O
be	O
an	O
abstract	O
one	O
,	O
such	O
as	O
the	O
set	O
of	O
all	O
natural	O
numbers	O
,	O
or	O
the	O
universe	O
may	O
be	O
a	O
subset	O
of	O
the	O
real-world	O
.	O
no	O
matter	O
which	O
method	O
of	O
representation	O
we	O
choose	O
,	O
descriptions	O
of	O
objects	O
in	O
the	O
real	O
world	O
must	O
ultimately	O
rely	O
on	O
measurements	O
of	O
some	O
properties	O
of	O
those	O
objects	O
.	O
these	O
may	O
be	O
physical	O
properties	O
such	O
as	O
size	O
,	O
weight	O
,	O
colour	O
,	O
etc	O
or	O
they	O
may	O
be	O
deﬁned	O
for	O
objects	O
,	O
for	O
example	B
the	O
length	O
of	O
time	O
a	O
person	O
has	O
been	O
employed	O
for	O
the	O
purpose	O
of	O
approving	O
a	O
loan	O
.	O
the	O
accuracy	O
and	O
reliability	O
of	O
a	O
learned	O
concept	O
depends	O
heavily	O
on	O
the	O
accuracy	O
and	O
reliability	O
of	O
the	O
measurements	O
.	O
a	O
program	O
is	O
limited	O
in	O
the	O
concepts	O
that	O
it	O
can	O
learn	O
by	O
the	O
representational	O
capabilities	O
of	O
both	O
observation	O
and	O
hypothesis	O
languages	O
.	O
for	O
example	B
,	O
if	O
an	O
attribute/value	O
list	O
is	O
used	O
to	O
represent	O
examples	O
for	O
an	O
induction	O
program	O
,	O
the	O
measurement	O
of	O
certain	O
attributes	O
and	O
not	O
others	O
clearly	O
places	O
bounds	O
on	O
the	O
kinds	O
of	O
patterns	O
that	O
the	O
learner	O
can	O
ﬁnd	O
.	O
the	O
learner	O
is	O
said	O
to	O
be	O
biased	O
by	O
its	O
observation	O
language	O
.	O
the	O
hypothesis	O
language	O
also	O
places	O
230	O
knowledge	O
representation	O
[	O
ch	O
.	O
12	O
constraints	O
on	O
what	O
may	O
and	O
may	O
not	O
be	O
learned	O
.	O
for	O
example	B
,	O
in	O
the	O
language	O
of	O
attributes	O
and	O
values	O
,	O
relationships	O
between	O
objects	O
are	O
difﬁcult	O
to	O
represent	O
.	O
whereas	O
,	O
a	O
more	O
expressive	O
language	O
,	O
such	O
as	O
ﬁrst	O
order	O
logic	O
,	O
can	O
easily	O
be	O
used	O
to	O
describe	O
relationships	O
.	O
unfortunately	O
,	O
representational	O
power	O
comes	O
at	O
a	O
price	O
.	O
learning	O
can	O
be	O
viewed	O
as	O
a	O
search	O
through	O
the	O
space	O
of	O
all	O
sentences	O
in	O
a	O
language	O
for	O
a	O
sentence	O
that	O
best	O
describes	O
the	O
data	O
.	O
the	O
richer	O
the	O
language	O
,	O
the	O
larger	O
the	O
search	O
space	O
.	O
when	O
the	O
search	O
space	O
is	O
small	O
,	O
it	O
is	O
possible	O
to	O
use	O
“	O
brute	O
force	O
”	O
search	O
methods	O
.	O
if	O
the	O
search	O
space	O
is	O
very	O
large	O
,	O
additional	O
knowledge	O
is	O
required	O
to	O
reduce	O
the	O
search	O
.	O
we	O
will	O
divide	O
our	O
attention	O
among	O
three	O
different	O
classes	O
of	O
machine	O
learning	O
algo-	O
rithms	O
that	O
use	O
distinctly	O
different	O
approaches	O
to	O
the	O
problem	O
of	O
representation	O
:	O
instance-based	B
learning	O
algorithms	O
learn	O
concepts	O
by	O
storing	O
prototypic	O
instances	O
of	O
the	O
concept	O
and	O
do	O
not	O
construct	O
abstract	O
representations	O
at	O
all	O
.	O
function	B
approximation	I
algorithms	O
include	O
connectionist	O
and	O
statistics	O
methods	O
.	O
these	O
algorithms	O
are	O
most	O
closely	O
related	O
to	O
traditional	O
mathematical	O
notions	O
of	O
approxima-	O
tion	B
and	O
interpolation	O
and	O
represent	O
concepts	O
as	O
mathematical	O
formulae	O
.	O
symbolic	B
learning	I
algorithms	O
learn	O
concepts	O
by	O
constructing	O
a	O
symbolic	O
which	O
de-	O
scribes	O
a	O
class	O
of	O
objects	O
.	O
we	O
will	O
consider	O
algorithms	O
that	O
work	O
with	O
representations	O
equivalent	O
to	O
propositional	O
logic	O
and	O
ﬁrst-order	O
logic	O
.	O
12.3	O
prototypes	O
the	O
simplest	O
form	O
of	O
learning	O
is	O
memorisation	O
.	O
when	O
an	O
object	O
is	O
observed	O
or	O
the	O
solution	O
to	O
a	O
problem	O
is	O
found	O
,	O
it	O
is	O
stored	O
in	O
memory	O
for	O
future	O
use	O
.	O
memory	O
can	O
be	O
thought	O
of	O
as	O
a	O
look	O
up	O
table	O
.	O
when	O
a	O
new	O
problem	O
is	O
encountered	O
,	O
memory	O
is	O
searched	O
to	O
ﬁnd	O
if	O
the	O
same	O
problem	O
has	O
been	O
solved	O
before	O
.	O
if	O
an	O
exact	O
match	O
for	O
the	O
search	O
is	O
required	O
,	O
learning	O
is	O
slow	O
and	O
consumes	O
very	O
large	O
amounts	O
of	O
memory	O
.	O
however	O
,	O
approximate	O
matching	O
allows	O
a	O
degree	O
of	O
generalisation	O
that	O
both	O
speeds	O
learning	O
and	O
saves	O
memory	O
.	O
for	O
example	B
,	O
if	O
we	O
are	O
shown	O
an	O
object	O
and	O
we	O
want	O
to	O
know	O
if	O
it	O
is	O
a	O
chair	O
,	O
then	O
we	O
compare	O
the	O
description	O
of	O
this	O
new	O
object	O
with	O
descriptions	O
of	O
“	O
typical	O
”	O
chairs	O
that	O
we	O
have	O
encountered	O
before	O
.	O
if	O
the	O
description	O
of	O
the	O
new	O
object	O
is	O
“	O
close	O
”	O
to	O
the	O
description	O
of	O
one	O
of	O
the	O
stored	O
instances	O
then	O
we	O
may	O
call	O
it	O
a	O
chair	O
.	O
obviously	O
,	O
we	O
must	O
deﬁned	O
what	O
we	O
mean	O
by	O
“	O
typical	O
”	O
and	O
“	O
close	O
”	O
.	O
to	O
better	O
understand	O
the	O
issues	O
involved	O
in	O
learning	O
prototypes	O
,	O
we	O
will	O
brieﬂy	O
de-	O
scribe	O
three	O
experiments	O
in	O
instance-based	B
learning	O
(	O
ibl	O
)	O
by	O
aha	O
,	O
kibler	O
&	O
albert	O
(	O
1991	O
)	O
.	O
ibl	O
learns	O
to	O
classify	O
objects	O
by	O
being	O
shown	O
examples	O
of	O
objects	O
,	O
described	O
by	O
an	O
at-	O
tribute/value	O
list	O
,	O
along	O
with	O
the	O
class	O
to	O
which	O
each	O
example	B
belongs	O
.	O
12.3.1	O
experiment	O
1	O
in	O
the	O
ﬁrst	O
experiment	O
(	O
ib1	O
)	O
,	O
to	O
learn	O
a	O
concept	O
simply	O
required	O
the	O
program	O
to	O
store	O
every	O
example	B
.	O
when	O
an	O
unclassiﬁed	O
object	O
was	O
presented	O
for	O
classiﬁcation	B
by	O
the	O
program	O
,	O
it	O
used	O
a	O
simple	O
euclidean	O
distance	O
measure	B
to	O
determine	O
the	O
nearest	O
neighbour	O
of	O
the	O
object	O
and	O
the	O
class	O
given	O
to	O
it	O
was	O
the	O
class	O
of	O
the	O
neighbour	O
.	O
this	O
simple	O
scheme	O
works	O
well	O
,	O
and	O
is	O
tolerant	O
to	O
some	O
noise	O
in	O
the	O
data	O
.	O
its	O
major	O
disadvantage	O
is	O
that	O
it	O
requires	O
a	O
large	O
amount	O
of	O
storage	O
capacity	O
.	O
sec	O
.	O
12.3	O
]	O
12.3.2	O
experiment	O
2	O
prototypes	O
231	O
the	O
second	O
experiment	O
(	O
ib2	O
)	O
attempted	O
to	O
improve	O
the	O
space	O
performance	O
of	O
ib1	O
.	O
in	O
this	O
case	O
,	O
when	O
new	O
instances	O
of	O
classes	O
were	O
presented	O
to	O
the	O
program	O
,	O
the	O
program	O
attempted	O
to	O
classify	O
them	O
.	O
instances	O
that	O
were	O
correctly	O
classiﬁed	O
were	O
ignored	O
and	O
only	O
incorrectly	O
classiﬁed	O
instances	O
were	O
stored	O
to	O
become	O
part	O
of	O
the	O
concept	O
.	O
while	O
this	O
scheme	O
reduced	O
storage	O
dramatically	O
,	O
it	O
was	O
less	O
noise-tolerant	O
than	O
the	O
ﬁrst	O
.	O
12.3.3	O
experiment	O
3	O
the	O
third	O
experiment	O
(	O
ib3	O
)	O
used	O
a	O
more	O
sophisticated	O
method	O
for	O
evaluating	O
instances	O
to	O
decide	O
if	O
they	O
should	O
be	O
kept	O
or	O
not	O
.	O
ib3	O
is	O
similar	O
to	O
ib2	O
with	O
the	O
following	O
additions	O
.	O
ib3	O
maintains	O
a	O
record	O
of	O
the	O
number	O
of	O
correct	O
and	O
incorrect	O
classiﬁcation	B
attempts	O
for	O
each	O
saved	O
instance	O
.	O
this	O
record	O
summarised	O
an	O
instance	O
’	O
s	O
classiﬁcation	B
performance	O
.	O
ib3	O
uses	O
a	O
signiﬁcance	O
test	O
to	O
determine	O
which	O
instances	O
are	O
good	O
classiﬁers	O
and	O
which	O
ones	O
are	O
believed	O
to	O
be	O
noisy	O
.	O
the	O
latter	O
are	O
discarded	O
from	O
the	O
concept	O
description	O
.	O
this	O
method	O
strengthens	O
noise	O
tolerance	O
,	O
while	O
keeping	O
storage	O
requirements	O
down	O
.	O
12.3.4	O
discussion	O
-	O
+	O
-	O
+	O
-	O
-	O
-	O
-	O
+	O
-	O
+	O
+	O
+	O
+	O
+	O
+	O
+	O
+	O
-	O
-	O
+	O
-	O
-	O
-	O
-	O
+	O
+	O
-	O
+	O
-	O
-	O
-	O
-	O
+	O
+	O
-	O
fig	O
.	O
12.1	O
:	O
the	O
extension	O
of	O
an	O
ibl	O
concept	O
is	O
shown	O
in	O
solid	O
lines	O
.	O
the	O
dashed	O
lines	O
represent	O
the	O
target	O
concept	O
.	O
a	O
sample	O
of	O
positive	O
and	O
negative	O
examples	O
is	O
shown	O
.	O
adapted	O
from	O
aha	O
,	O
kibler	O
and	O
albert	O
(	O
1991	O
)	O
.	O
[	O
ch	O
.	O
12	O
232	O
knowledge	O
representation	O
ib1	O
is	O
strongly	O
related	O
to	O
the	O
-nearest	O
neighbour	O
methods	O
described	O
in	O
section	O
4.3.	O
here	O
is	O
1.	O
the	O
main	O
contribution	O
of	O
aha	O
,	O
kibler	O
and	O
albert	O
(	O
1991	O
)	O
is	O
the	O
attempt	O
to	O
achieve	O
satisfactory	O
accuracy	O
while	O
using	O
less	O
storage	O
.	O
the	O
algorithms	O
presented	O
in	O
chapter	O
4	O
assumed	O
that	O
all	O
training	O
data	O
are	O
available	O
.	O
whereas	O
ib2	O
and	O
ib3	O
examine	O
methods	O
for	O
“	O
forgetting	O
”	O
instances	O
that	O
do	O
not	O
improve	O
classiﬁcation	B
accuracy	O
.	O
figure	O
12.1	O
shows	O
the	O
boundaries	O
of	O
an	O
imaginary	O
concept	O
in	O
a	O
two	O
dimensions	O
space	O
.	O
the	O
dashed	O
lines	O
represent	O
the	O
boundaries	O
of	O
the	O
target	O
concept	O
.	O
the	O
learning	O
procedure	O
attempts	O
to	O
approximate	O
these	O
boundaries	O
by	O
nearest	O
neighbour	O
matches	O
.	O
note	O
that	O
the	O
boundaries	O
deﬁned	O
by	O
the	O
matching	O
procedure	O
are	O
quite	O
irregular	O
.	O
this	O
can	O
have	O
its	O
advantages	O
when	O
the	O
target	O
concept	O
does	O
not	O
have	O
a	O
regular	O
shape	O
.	O
learning	O
by	O
remembering	O
typical	O
examples	O
of	O
a	O
concept	O
has	O
several	O
other	O
advantages	O
.	O
if	O
an	O
efﬁcient	O
indexing	O
mechanism	O
can	O
be	O
devised	O
to	O
ﬁnd	O
near	O
matches	O
,	O
this	O
representation	O
can	O
be	O
very	O
fast	O
as	O
a	O
classiﬁer	B
since	O
it	O
reduces	O
to	O
a	O
table	O
look	O
up	O
.	O
it	O
does	O
not	O
require	O
any	O
sophisticated	O
reasoning	O
system	O
and	O
is	O
very	O
ﬂexible	O
.	O
as	O
we	O
shall	O
see	O
later	O
,	O
representations	O
that	O
rely	O
on	O
abstractions	O
of	O
concepts	O
can	O
run	O
into	O
trouble	O
with	O
what	O
appear	O
to	O
be	O
simple	O
concepts	O
.	O
for	O
example	B
,	O
an	O
abstract	O
representation	O
of	O
a	O
chair	O
may	O
consist	O
of	O
a	O
description	O
of	O
the	O
number	O
legs	O
,	O
the	O
height	O
,	O
etc	O
.	O
however	O
,	O
exceptions	O
abound	O
since	O
anything	O
that	O
can	O
be	O
sat	O
on	O
can	O
be	O
thought	O
of	O
as	O
a	O
chair	O
.	O
thus	O
,	O
abstractions	O
must	O
often	O
be	O
augmented	O
by	O
lists	O
of	O
exceptions	O
.	O
instance-based	B
representation	O
does	O
not	O
suffer	O
from	O
this	O
problem	O
since	O
it	O
only	O
consists	O
exceptions	O
and	O
is	O
designed	O
to	O
handle	O
them	O
efﬁciently	O
.	O
one	O
of	O
the	O
major	O
disadvantages	O
of	O
this	O
style	O
of	O
representation	O
is	O
that	O
it	O
is	O
necessary	O
to	O
deﬁne	O
a	O
similarity	O
metric	O
for	O
objects	O
in	O
the	O
universe	O
.	O
this	O
can	O
often	O
be	O
difﬁcult	O
to	O
do	O
when	O
the	O
objects	O
are	O
quite	O
complex	O
.	O
another	O
disadvantage	O
is	O
that	O
the	O
representation	O
is	O
not	O
human	O
readable	O
.	O
in	O
the	O
previ-	O
ous	O
section	O
we	O
made	O
the	O
distinction	O
between	O
an	O
language	O
of	O
observation	O
and	O
a	O
hypothesis	O
language	O
.	O
when	O
learning	O
using	O
prototypes	O
,	O
the	O
language	O
of	O
observation	O
may	O
be	O
an	O
at-	O
tribute/value	O
representation	O
.	O
the	O
hypothesis	O
language	O
is	O
simply	O
a	O
set	O
of	O
attribute/value	O
or	O
feature	O
vectors	O
,	O
representing	O
the	O
prototypes	O
.	O
while	O
examples	O
are	O
often	O
a	O
useful	O
means	O
of	O
communicating	O
ideas	O
,	O
a	O
very	O
large	O
set	O
of	O
examples	O
can	O
easily	O
swamp	O
the	O
reader	O
with	O
unnecessary	O
detail	O
and	O
fails	O
to	O
emphasis	O
important	O
features	O
of	O
a	O
class	O
.	O
thus	O
a	O
collection	O
of	O
typical	O
instances	O
may	O
not	O
convey	O
much	O
insight	O
into	O
the	O
concept	O
that	O
has	O
been	O
learned	O
.	O
12.4	O
function	B
approximation	I
as	O
we	O
saw	O
in	O
chapters	O
3	O
,	O
4	O
and	O
6	O
,	O
statistical	B
and	O
connectionist	O
approaches	O
to	O
machine	O
learning	O
are	O
related	O
to	O
function	B
approximation	I
methods	O
in	O
mathematics	O
.	O
for	O
the	O
purposes	O
of	O
illustration	O
let	O
us	O
assume	O
that	O
the	O
learning	O
task	O
is	O
one	O
of	O
classiﬁcation	B
.	O
that	O
is	O
,	O
we	O
wish	O
to	O
ﬁnd	O
ways	O
of	O
grouping	O
objects	O
in	O
a	O
universe	O
.	O
in	O
figure	O
12.2	O
we	O
have	O
a	O
universe	O
of	O
objects	O
that	O
belong	O
to	O
either	O
of	O
two	O
classes	O
“	O
+	O
”	O
or	O
“	O
-	O
”	O
.	O
by	O
function	B
approximation	I
,	O
we	O
describe	O
a	O
surface	O
that	O
separates	O
the	O
objects	O
into	O
different	O
regions	O
.	O
the	O
simplest	O
function	O
is	O
that	O
of	O
a	O
line	O
and	O
linear	O
regression	O
methods	O
and	O
perceptrons	O
are	O
used	O
to	O
ﬁnd	O
linear	O
discriminant	O
functions	O
.	O
section	O
6.1	O
described	O
the	O
perceptron	O
pattern	O
classiﬁer	B
.	O
given	O
a	O
binary	O
input	B
vector	O
,	O
x	O
,	O
a	O
weight	O
vector	O
,	O
w	O
,	O
and	O
a	O
threshold	O
value	O
,	O
	O
å1æ5ç	O
,	O
if	O
,	O
g\	O
	O
æ	O
à	O
æ	O
sec	O
.	O
12.4	O
]	O
function	B
approximation	I
233	O
+	O
+	O
+	O
+	O
+	O
+	O
–	O
–	O
–	O
–	O
–	O
–	O
–	O
–	O
fig	O
.	O
12.2	O
:	O
a	O
linear	O
discrimination	O
between	O
two	O
classes	O
.	O
then	O
the	O
output	B
is	O
1	O
,	O
indicating	O
membership	O
of	O
a	O
class	O
,	O
otherwise	O
it	O
is	O
0	O
,	O
indicating	O
exclusion	O
from	O
the	O
class	O
.	O
clearly	O
,	O
wè	O
xò	O
	O
describes	O
a	O
hyperplane	O
and	O
the	O
goal	O
of	O
perceptron	O
learning	O
is	O
to	O
ﬁnd	O
a	O
weight	O
vector	O
,	O
w	O
,	O
that	O
results	O
in	O
correct	O
classiﬁcation	B
for	O
all	O
training	O
examples	O
.	O
the	O
perceptron	O
is	O
an	O
example	B
of	O
a	O
linear	O
threshold	O
unit	O
(	O
ltu	O
)	O
.	O
a	O
single	O
ltu	O
can	O
only	O
recognise	O
one	O
kind	O
of	O
pattern	O
,	O
provided	O
that	O
the	O
input	B
space	O
is	O
linearly	O
separable	O
.	O
if	O
we	O
wish	O
to	O
recognise	O
more	O
than	O
one	O
pattern	O
,	O
several	O
ltu	O
’	O
s	O
can	O
be	O
combined	O
.	O
in	O
this	O
case	O
,	O
instead	O
of	O
having	O
a	O
vector	O
of	O
weights	O
,	O
we	O
have	O
an	O
array	O
.	O
the	O
output	B
will	O
now	O
be	O
a	O
vector	O
:	O
ëêmì	O
ëêíjqîìon	O
where	O
each	O
element	O
of	O
u	O
indicates	O
membership	O
of	O
a	O
class	O
and	O
each	O
row	O
in	O
w	O
is	O
the	O
set	O
of	O
weights	O
for	O
one	O
ltu	O
.	O
this	O
architecture	O
is	O
called	O
a	O
pattern	O
associator	O
.	O
ltu	O
’	O
s	O
can	O
only	O
produce	O
linear	O
discriminant	O
functions	O
and	O
consequently	O
,	O
they	O
are	O
limited	O
in	O
the	O
kinds	O
of	O
classes	O
that	O
can	O
be	O
learned	O
.	O
however	O
,	O
it	O
was	O
found	O
that	O
by	O
cascading	O
pattern	O
associators	O
,	O
it	O
is	O
possible	O
to	O
approximate	O
decision	O
surfaces	O
that	O
are	O
of	O
a	O
higher	O
order	O
than	O
simple	O
hyperplanes	O
.	O
in	O
cascaded	O
system	O
,	O
the	O
outputs	O
of	O
one	O
pattern	O
associator	O
are	O
fed	O
into	O
the	O
inputs	O
of	O
another	O
,	O
thus	O
:	O
to	O
facilitate	O
learning	O
,	O
a	O
further	O
modiﬁcation	O
must	O
be	O
made	O
.	O
rather	O
than	O
using	O
a	O
simple	O
threshold	O
,	O
as	O
in	O
the	O
perceptron	O
,	O
multi-layer	O
networks	O
usually	O
use	O
a	O
non-linear	O
threshold	O
such	O
as	O
a	O
sigmoid	O
function	O
.	O
like	O
perceptron	O
learning	O
,	O
back-propagation	O
attempts	O
to	O
reduce	O
the	O
errors	O
between	O
the	O
output	B
of	O
the	O
network	O
and	O
the	O
desired	O
result	O
.	O
despite	O
the	O
non-linear	O
threshold	O
,	O
multi-layer	O
networks	O
can	O
still	O
be	O
thought	O
of	O
as	O
describing	O
a	O
complex	O
collection	O
of	O
hyperplanes	O
that	O
approximate	O
the	O
required	O
decision	O
surface	O
.	O
é	O
é	O
234	O
knowledge	O
representation	O
[	O
ch	O
.	O
12	O
x	O
fig	O
.	O
12.3	O
:	O
a	O
pole	O
balancer	O
.	O
12.4.1	O
discussion	O
function	B
approximation	I
methods	O
can	O
often	O
produce	O
quite	O
accurate	O
classiﬁers	O
because	O
they	O
are	O
capable	O
of	O
constructing	O
complex	O
decision	O
surfaces	O
.	O
the	O
observation	O
language	O
for	O
algorithms	O
of	O
this	O
class	O
is	O
usually	O
a	O
vector	O
of	O
numbers	O
.	O
often	O
preprocessing	B
will	O
convert	O
raw	O
data	O
into	O
a	O
suitable	O
form	O
.	O
for	O
example	B
,	O
pomerleau	O
(	O
1989	O
)	O
accepts	O
raw	O
data	O
from	O
a	O
camera	O
mounted	O
on	O
a	O
moving	O
vehicle	O
and	O
selects	O
portions	O
of	O
the	O
image	O
to	O
process	O
for	O
input	B
to	O
a	O
neural	O
net	O
that	O
learns	O
how	O
to	O
steer	O
the	O
vehicle	O
.	O
the	O
knowledge	O
acquired	O
by	O
such	O
a	O
system	O
is	O
stored	O
as	O
weights	O
in	O
a	O
matrix	O
.	O
therefore	O
,	O
the	O
hypothesis	O
language	O
is	O
usually	O
an	O
array	O
of	O
real	O
numbers	O
.	O
thus	O
,	O
the	O
results	O
of	O
learning	O
are	O
not	O
easily	O
available	O
for	O
inspection	O
by	O
a	O
human	O
reader	O
.	O
moreover	O
,	O
the	O
design	O
of	O
a	O
network	O
usually	O
requires	O
informed	O
guesswork	O
on	O
the	O
part	O
of	O
the	O
user	O
in	O
order	O
to	O
obtain	O
satisfactory	O
results	O
.	O
although	O
some	O
effort	O
has	O
been	O
devoted	O
to	O
extracting	O
meaning	O
from	O
networks	O
,	O
the	O
still	O
communicate	O
little	O
about	O
the	O
data	O
.	O
connectionist	O
learning	O
algorithms	O
are	O
still	O
computationally	O
expensive	O
.	O
a	O
critical	O
factor	O
in	O
their	O
speed	O
is	O
the	O
encoding	O
of	O
the	O
inputs	O
to	O
the	O
network	O
.	O
this	O
is	O
also	O
critical	O
to	O
genetic	B
algorithms	I
and	O
we	O
will	O
illustrate	O
that	O
problem	O
in	O
the	O
next	O
section	O
.	O
12.5	O
genetic	B
algorithms	I
genetic	O
algorithms	O
(	O
holland	O
,	O
1975	O
)	O
perform	O
a	O
search	O
for	O
the	O
solution	O
to	O
a	O
problem	O
by	O
generating	O
candidate	O
solutions	O
from	O
the	O
space	O
of	O
all	O
solutions	O
and	O
testing	O
the	O
performance	O
of	O
the	O
candidates	O
.	O
the	O
search	O
method	O
is	O
based	O
on	O
ideas	O
from	O
genetics	O
and	O
the	O
size	O
of	O
the	O
search	O
space	O
is	O
determined	O
by	O
the	O
representation	O
of	O
the	O
domain	O
.	O
an	O
understanding	O
of	O
genetic	B
algorithms	I
will	O
be	O
aided	O
by	O
an	O
example	B
.	O
a	O
very	O
common	O
problem	O
in	O
adaptive	O
control	O
is	O
learning	O
to	O
balance	O
a	O
pole	O
that	O
is	O
hinged	O
on	O
a	O
cart	O
that	O
can	O
move	O
in	O
one	O
dimension	O
along	O
a	O
track	O
of	O
ﬁxed	O
length	O
,	O
as	O
show	O
in	O
figure	O
12.3.	O
the	O
control	O
must	O
use	O
“	O
bang-bang	O
”	O
control	O
,	O
that	O
is	O
,	O
a	O
force	O
of	O
ﬁxed	O
magnitude	O
can	O
be	O
applied	O
to	O
push	O
the	O
cart	O
to	O
the	O
left	O
or	O
right	O
.	O
before	O
we	O
can	O
begin	O
to	O
learn	O
how	O
to	O
control	O
this	O
system	O
,	O
it	O
is	O
necessary	O
to	O
represent	O
it	O
somehow	O
.	O
we	O
will	O
use	O
the	O
boxes	O
method	O
that	O
was	O
devised	O
by	O
michie	O
&	O
chambers	O
q	O
sec	O
.	O
12.5	O
]	O
genetic	B
algorithms	I
235	O
x	O
.	O
x.	O
q	O
q	O
.	O
fig	O
.	O
12.4	O
:	O
discretisation	O
of	O
pole	O
balancer	O
state	O
space	O
.	O
,	O
and	O
its	O
velocity	O
.	O
rather	O
than	O
treat	O
the	O
four	O
variables	O
as	O
continuous	O
values	O
,	O
michie	O
and	O
chambers	O
chose	O
to	O
discretise	O
each	O
dimension	O
of	O
the	O
state	O
space	O
.	O
one	O
possible	O
discretisation	O
is	O
shown	O
in	O
figure	O
12.4	O
.	O
(	O
1968	O
)	O
.	O
the	O
measurements	O
taken	O
of	O
the	O
physical	O
system	O
are	O
the	O
angle	O
of	O
the	O
pole	O
,	O
ï	O
,	O
and	O
its	O
angular	O
velocity	O
and	O
the	O
position	O
of	O
the	O
cart	O
,	O
à	O
this	O
discretisation	O
results	O
in	O
ñ	O
“	O
boxes	O
”	O
that	O
partition	O
the	O
state	O
space	O
.	O
¹h	O
162	O
boxes	O
,	O
there	O
are	O
each	O
box	O
has	O
associated	O
with	O
it	O
an	O
action	O
setting	O
which	O
tells	O
the	O
controller	O
that	O
when	O
the	O
system	O
is	O
in	O
that	O
part	O
of	O
the	O
state	O
space	O
,	O
the	O
controller	O
should	O
apply	O
that	O
action	O
,	O
which	O
is	O
a	O
push	O
to	O
the	O
left	O
or	O
a	O
push	O
to	O
the	O
right	O
.	O
since	O
there	O
is	O
a	O
simple	O
binary	O
choice	O
and	O
there	O
are	O
©ï	O
possible	O
control	O
strategies	O
for	O
the	O
pole	O
balancer	O
.	O
the	O
simplest	O
kind	O
of	O
learning	O
in	O
this	O
case	O
,	O
is	O
to	O
exhaustively	O
search	O
for	O
the	O
right	O
combination	O
.	O
however	O
,	O
this	O
is	O
clearly	O
impractical	O
given	O
the	O
size	O
of	O
the	O
search	O
space	O
.	O
instead	O
,	O
we	O
can	O
invoke	O
a	O
genetic	O
search	O
strategy	O
that	O
will	O
reduce	O
the	O
amount	O
of	O
search	O
considerably	O
.	O
in	O
genetic	O
learning	O
,	O
we	O
assume	O
that	O
there	O
is	O
a	O
population	O
of	O
individuals	O
,	O
each	O
one	O
of	O
which	O
,	O
represents	O
a	O
candidate	O
problem	O
solver	O
for	O
a	O
given	O
task	O
.	O
like	O
evolution	O
,	O
genetic	B
algorithms	I
test	O
each	O
individual	O
from	O
the	O
population	O
and	O
only	O
the	O
ﬁttest	O
survive	O
to	O
reproduce	O
for	O
the	O
next	O
generation	O
.	O
the	O
algorithm	O
creates	O
new	O
generations	O
until	O
at	O
least	O
one	O
individual	O
is	O
found	O
that	O
can	O
solve	O
the	O
problem	O
adequately	O
.	O
each	O
problem	O
solver	O
is	O
a	O
chromosome	O
.	O
a	O
position	O
,	O
or	O
set	O
of	O
positions	O
in	O
a	O
chromosome	O
is	O
called	O
a	O
gene	O
.	O
the	O
possible	O
values	O
(	O
from	O
a	O
ﬁxed	O
set	O
of	O
symbols	O
)	O
of	O
a	O
gene	O
are	O
known	O
as	O
alleles	O
.	O
in	O
most	O
genetic	O
algorithm	O
implementations	O
the	O
set	O
of	O
symbols	O
isð	O
chromosome	O
lengths	O
are	O
ﬁxed	O
.	O
most	O
implementations	O
also	O
use	O
ﬁxed	O
population	O
sizes	O
.	O
the	O
most	O
critical	O
problem	O
in	O
applying	O
a	O
genetic	O
algorithm	O
is	O
in	O
ﬁnding	O
a	O
suitable	O
encoding	O
of	O
the	O
examples	O
in	O
the	O
problem	O
domain	O
to	O
a	O
chromosome	O
.	O
a	O
good	O
choice	O
of	O
representation	O
will	O
make	O
the	O
search	O
easy	O
by	O
limiting	O
the	O
search	O
space	O
,	O
a	O
poor	O
choice	O
will	O
result	O
in	O
a	O
large	O
search	O
space	O
.	O
for	O
our	O
pole	B
balancing	I
example	O
,	O
we	O
will	O
use	O
a	O
very	O
simple	O
encoding	O
.	O
a	O
chromosome	O
is	O
a	O
string	O
of	O
162	O
boxes	O
.	O
each	O
box	O
,	O
or	O
gene	O
,	O
can	O
take	O
values	O
:	O
0	O
(	O
meaning	O
push	O
left	O
)	O
or	O
1	O
(	O
meaning	O
push	O
right	O
)	O
.	O
choosing	O
the	O
size	O
of	O
the	O
population	O
can	O
be	O
tricky	O
since	O
a	O
small	O
population	O
size	O
provides	O
an	O
insufﬁcient	O
sample	O
size	O
over	O
the	O
space	O
of	O
solutions	O
for	O
a	O
problem	O
and	O
large	O
population	O
requires	O
a	O
lot	O
of	O
evaluation	O
and	O
will	O
be	O
slow	O
.	O
in	O
this	O
example	B
,	O
50	O
is	O
a	O
suitable	O
population	O
size	O
.	O
x7ññ	O
and	O
	O
	O
	O
	O
236	O
knowledge	O
representation	O
[	O
ch	O
.	O
12	O
each	O
iteration	O
in	O
a	O
genetic	O
algorithm	O
is	O
called	O
a	O
generation	O
.	O
each	O
chromosome	O
in	O
a	O
population	O
is	O
used	O
to	O
solve	O
a	O
problem	O
.	O
its	O
performance	O
is	O
evaluated	O
and	O
the	O
chromosome	O
is	O
given	O
some	O
rating	O
of	O
ﬁtness	O
.	O
the	O
population	O
is	O
also	O
given	O
an	O
overall	O
ﬁtness	O
rating	O
based	O
on	O
the	O
performance	O
of	O
its	O
members	O
.	O
the	O
ﬁtness	O
value	O
indicates	O
how	O
close	O
a	O
chromosome	O
or	O
population	O
is	O
to	O
the	O
required	O
solution	O
.	O
for	O
pole	B
balancing	I
,	O
the	O
ﬁtness	O
value	O
of	O
a	O
chromosome	O
may	O
be	O
the	O
number	O
of	O
time	O
steps	O
that	O
the	O
chromosome	O
is	O
able	O
to	O
keep	O
the	O
pole	O
balanced	O
for	O
.	O
new	O
sets	O
of	O
chromosomes	B
are	O
produced	O
from	O
one	O
generation	O
to	O
the	O
next	O
.	O
reproduction	O
takes	O
place	O
when	O
selected	O
chromosomes	B
from	O
one	O
generation	O
are	O
recombined	O
with	O
others	O
to	O
form	O
chromosomes	B
for	O
the	O
next	O
generation	O
.	O
the	O
new	O
ones	O
are	O
called	O
offspring	O
.	O
selection	O
of	O
chromosomes	B
for	O
reproduction	O
is	O
based	O
on	O
their	O
ﬁtness	O
values	O
.	O
the	O
average	O
ﬁtness	O
of	O
population	O
may	O
also	O
be	O
calculated	O
at	O
end	O
of	O
each	O
generation	O
.	O
for	O
pole	B
balancing	I
,	O
individuals	O
whose	O
ﬁtness	O
is	O
below	O
average	O
are	O
replaced	O
by	O
reproduction	O
of	O
above	O
average	O
chromosomes	B
.	O
the	O
strategy	O
must	O
be	O
modiﬁed	O
if	O
two	O
few	O
or	O
two	O
many	O
chromosomes	B
survive	O
.	O
for	O
example	B
,	O
at	O
least	O
10	O
%	O
and	O
at	O
most	O
60	O
%	O
must	O
survive	O
.	O
operators	O
that	O
recombine	O
the	O
selected	O
chromosomes	B
are	O
called	O
genetic	O
operators	O
.	O
two	O
common	O
operators	O
are	O
crossover	O
and	O
mutation	O
.	O
crossover	O
exchanges	O
portions	O
of	O
a	O
pair	O
of	O
chromosomesat	O
a	O
randomly	O
chosen	O
point	O
called	O
the	O
crossover	O
point	O
.	O
some	O
implementations	O
and	O
have	O
more	O
than	O
one	O
crossover	O
point	O
.	O
for	O
example	B
,	O
if	O
there	O
are	O
two	O
chromosomes	B
,	O
k	O
:	O
and	O
the	O
crossover	O
point	O
is	O
4	O
,	O
the	O
resulting	O
offspring	O
are	O
:	O
offspring	O
produced	O
by	O
crossover	O
can	O
not	O
contain	O
information	O
that	O
is	O
not	O
already	O
in	O
the	O
population	O
,	O
so	O
an	O
additional	O
operator	O
,	O
mutation	O
,	O
is	O
required	O
.	O
mutation	O
generates	O
an	O
offspring	O
by	O
randomly	O
changing	O
the	O
values	O
of	O
genes	O
at	O
one	O
or	O
more	O
gene	O
positions	O
of	O
a	O
selected	O
chromosome	O
.	O
for	O
example	B
,	O
if	O
the	O
following	O
chromosome	O
,	O
	O
h	O
ñ¢ñéñ	O
ñ¢ñ¢ñ	O
ï	O
ñ¢ñ	O
ñ¢ñ	O
ñ	O
!	O
ñ	O
	O
ñéñ	O
	O
	O
kó	O
	O
	O
ñéñ	O
	O
is	O
mutated	O
at	O
positions	O
2	O
,	O
4	O
and	O
9	O
,	O
then	O
the	O
resulting	O
offspring	O
is	O
:	O
the	O
number	O
of	O
offspring	O
produced	O
for	O
each	O
new	O
generation	O
depends	O
on	O
how	O
members	O
are	O
introduced	O
so	O
as	O
to	O
maintain	O
a	O
ﬁxed	O
population	O
size	O
.	O
in	O
a	O
pure	O
replacement	O
strategy	O
,	O
the	O
whole	O
population	O
is	O
replaced	O
by	O
a	O
new	O
one	O
.	O
in	O
an	O
elitist	O
strategy	O
,	O
a	O
proportion	O
of	O
the	O
population	O
survives	O
to	O
the	O
next	O
generation	O
.	O
in	O
pole	B
balancing	I
,	O
all	O
offspring	O
are	O
created	O
by	O
crossover	O
(	O
except	O
when	O
more	O
the	O
60	O
%	O
will	O
survive	O
for	O
more	O
than	O
three	O
generations	O
when	O
the	O
rate	O
is	O
reduced	O
to	O
only	O
0.75	O
being	O
produced	O
by	O
crossover	O
)	O
.	O
mutation	O
is	O
a	O
background	O
operator	O
which	O
helps	O
to	O
sustain	O
exploration	O
.	O
each	O
offspring	O
produced	O
by	O
crossover	O
has	O
a	O
probability	O
of	O
0.01	O
of	O
being	O
mutated	O
before	O
it	O
enters	O
the	O
population	O
.	O
if	O
more	O
then	O
60	O
%	O
will	O
survive	O
,	O
the	O
mutation	O
rate	O
is	O
increased	O
to	O
0.25.	O
the	O
number	O
of	O
offspring	O
an	O
individual	O
can	O
produce	O
by	O
crossover	O
is	O
proportional	O
to	O
its	O
ﬁtness	O
:	O
	O
 ¼o×²²ùø¿ÿ7	O
ûúd×	O
új	O
@	O
ÿ	O
>	O
 ¼	O
 ¼o×²²	O
ú	O
)	O
µýü¨×5þ	O
	O
³¨þ1	O
±¢þ×¬¼	O
ò	O
ñ	O
ñ	O
	O
ñ	O
	O
ò	O
	O
ñ	O
ñ	O
	O
ô	O
ñ	O
ñ	O
	O
ô	O
ñ	O
	O
õ	O
ñ	O
ñ	O
	O
ñ	O
	O
ô	O
ñ	O
	O
ñ	O
	O
ö	O
»	O
	O
»	O
ö	O
ö	O
sec	O
.	O
12.6	O
]	O
propositional	O
learning	O
systems	O
237	O
v_small	O
small	O
medium	O
class2	O
class2	O
class2	O
class2	O
class3	O
class3	O
class3	O
large	O
class1	O
class1	O
v_large	O
red	O
orange	O
yellow	O
green	O
blue	O
violet	O
fig	O
.	O
12.5	O
:	O
discrimination	O
on	O
attributes	O
and	O
values	O
.	O
where	O
the	O
number	O
of	O
children	O
is	O
the	O
total	O
number	O
of	O
individuals	O
to	O
be	O
replaced	O
.	O
mates	O
are	O
chosen	O
at	O
random	O
among	O
the	O
survivors	O
.	O
the	O
pole	B
balancing	I
experiments	O
described	O
above	O
,	O
were	O
conducted	O
by	O
odetayo	O
(	O
1988	O
)	O
.	O
this	O
may	O
not	O
be	O
the	O
only	O
way	O
of	O
encoding	O
the	O
problem	O
for	O
a	O
genetic	O
algorithm	O
and	O
so	O
other	O
solutions	O
may	O
be	O
possible	O
.	O
however	O
,	O
this	O
requires	O
effort	O
on	O
the	O
part	O
of	O
the	O
user	O
to	O
devise	O
a	O
clever	O
encoding	O
.	O
12.6	O
propositional	O
learning	O
systems	O
rather	O
than	O
searching	O
for	O
discriminant	O
functions	O
,	O
symbolic	B
learning	I
systems	O
ﬁnd	O
expres-	O
sions	O
equivalent	O
to	O
sentences	O
in	O
some	O
form	O
of	O
logic	O
.	O
for	O
example	B
,	O
we	O
may	O
distinguish	O
objects	O
according	O
to	O
two	O
attributes	O
:	O
size	O
and	O
colour	O
.	O
we	O
may	O
say	O
that	O
an	O
object	O
belongs	O
to	O
class	O
3	O
if	O
its	O
colour	O
is	O
red	O
and	O
its	O
size	O
is	O
very	O
small	O
to	O
medium	O
.	O
following	O
the	O
notation	O
of	O
michalski	O
(	O
1983	O
)	O
,	O
the	O
classes	O
in	O
figure	O
12.5	O
may	O
be	O
written	O
as	O
:	O
³¨	O
@	O
ÿ7²²¢ñ	O
³¨	O
@	O
ÿ7²²	O
³¨	O
@	O
ÿ7²²	O
²¬qß7×à	O
c	O
@	O
ÿéþ¡2×ãáý³¬	O
úkþ	O
âvðþ×±	O
)	O
xã7þ¢ÿ¼d¡2×ñ	O
ÿ7	O
=xµ×±lú+µñ	O
á³¨	O
ú'þ	O
âzð7þ¢ÿ¼d¡7×xyá2×	O
	O
²¬qß7×	O
âvð²¬µ	O
	O
	O
²¨µÿ7	O
µý×±lú	O
)	O
µýñ	O
á³¨	O
ú'þ	O
#	O
ü¨	O
äúj×	O
²¬qß7×	O
âvðø	O
note	O
that	O
this	O
kind	O
of	O
description	O
partitions	O
the	O
universe	O
into	O
blocks	O
,	O
unlike	O
the	O
function	B
approximation	I
methods	O
that	O
ﬁnd	O
smooth	O
surfaces	O
to	O
discriminate	O
classes	O
.	O
interestingly	O
,	O
one	O
of	O
the	O
popular	O
early	O
machine	O
learning	O
algorithms	O
,	O
aq	O
(	O
michalski	O
,	O
1973	O
)	O
,	O
had	O
its	O
origins	O
in	O
switching	O
theory	O
.	O
one	O
of	O
the	O
concerns	O
of	O
switching	O
theory	O
is	O
to	O
ﬁnd	O
ways	O
of	O
minimising	O
logic	O
circuits	O
,	O
that	O
is	O
,	O
simplifying	O
the	O
truth	O
table	O
description	O
of	O
the	O
function	O
of	O
a	O
circuit	O
to	O
a	O
simple	O
expression	O
in	O
boolean	O
logic	O
.	O
many	O
of	O
the	O
algorithms	O
in	O
switching	O
theory	O
take	O
tables	O
like	O
figure	O
12.5	O
and	O
search	O
for	O
the	O
best	O
way	O
of	O
covering	O
all	O
of	O
the	O
entries	O
in	O
the	O
table	O
.	O
aq	O
,	O
uses	O
a	O
covering	O
algorithm	O
,	O
to	O
build	O
its	O
concept	O
description	O
:	O
4e*+	O
%	O
6ó5å7æ	O
6	O
)	O
%	O
ij	O
%	O
	O
.	O
e	O
>	O
,	O
d	O
%	O
çi+e/07	O
(	O
07	O
%	O
>	O
b1	O
%	O
74	O
(	O
*+	O
%	O
c	O
%	O
è/.	O
>	O
g2i+b1	O
%	O
o	O
?	O
é	O
%	O
''	O
c	O
''	O
	O
$	O
.7b7bp4e	O
,	O
+k	O
>	O
`7	O
,	O
d4	O
6`j4	O
4e	O
,	O
d0	O
%	O
507	O
%	O
''	O
	O
$	O
4	O
>	O
e*	O
)	O
%	O
6p	O
%	O
ë.	O
>	O
,	O
/-c	O
,	O
+e3	O
,	O
d	O
%	O
	O
&	O
f	O
.	O
(	O
*	O
)	O
%	O
c	O
%	O
èi/6	O
)	O
%	O
70207	O
(	O
e	O
>	O
,	O
d0	O
(	O
*	O
)	O
%	O
5	O
%	O
èf.	O
>	O
g7i+b1	O
%	O
\	O
(	O
	O
,	O
#	O
ê/	O
	O
	O
ç	O
ñ	O
	O
	O
''	O
''	O
''	O
''	O
''	O
e	O
!	O
''	O
.	O
''	O
``	O
238	O
knowledge	O
representation	O
[	O
ch	O
.	O
12	O
f	O
...	O
..	O
v	O
1	O
''	O
	O
$	O
''	O
ãí2í	O
''	O
	O
$	O
e7e107	O
%	O
%	O
ìë7ëq_d	O
%	O
70	O
%	O
èi/6+	O
%	O
70707	O
(	O
e	O
,	O
?	O
îèd	O
?	O
6/eg	O
%	O
54e	O
>	O
,	O
d42	O
%	O
i	O
.-2-pè	O
#	O
.105.3	O
,	O
d	O
%	O
c-+	O
(	O
70k	O
>	O
`7	O
,	O
j4	O
(	O
*+	O
%	O
c	O
%	O
è/.	O
>	O
g2i+b1	O
%	O
70c4	O
>	O
e*	O
)	O
%	O
6	O
)	O
%	O
-ë_facè	O
6	O
)	O
%	O
g+e*	O
)	O
%	O
\.7b7b3i+e/07	O
(	O
''	O
	O
$	O
!	O
7	O
''	O
%	O
6	O
)	O
%	O
\.6+	O
%	O
a,1e\i1e107	O
(	O
(	O
*	O
)	O
%	O
c	O
%	O
è/.g7i+b1	O
%	O
70\b1	O
%	O
(	O
b	O
fig	O
.	O
12.6	O
:	O
decision	O
tree	O
learning	O
.	O
!	O
#	O
''	O
	O
$	O
`7	O
,	O
v	O
n	O
(	O
70507	O
%	O
the	O
“	O
best	O
”	O
expression	O
is	O
usually	O
some	O
compromise	O
between	O
the	O
desire	O
to	O
cover	O
as	O
many	O
positive	O
examples	O
as	O
possible	O
and	O
the	O
desire	O
to	O
have	O
as	O
compact	O
and	O
readable	O
a	O
representation	O
as	O
possible	O
.	O
in	O
designing	O
aq	O
,	O
michalski	O
was	O
particularly	O
concerned	O
with	O
the	O
expressiveness	O
of	O
the	O
concept	O
description	O
language	O
.	O
a	O
drawback	O
of	O
the	O
aq	O
learning	O
algorithm	O
is	O
that	O
it	O
does	O
not	O
use	O
statistical	B
information	O
,	O
present	O
in	O
the	O
training	O
sample	O
,	O
to	O
guide	O
induction	O
.	O
however	O
,	O
decision	O
tree	O
learning	O
algo-	O
rithms	O
(	O
quinlan	O
,	O
1993	O
)	O
do	O
.	O
the	O
basic	O
method	O
of	O
building	O
a	O
decision	O
tree	O
is	O
summarised	O
in	O
figure	O
12.6.	O
an	O
simple	O
attribute/value	O
representation	O
is	O
used	O
and	O
so	O
,	O
like	O
aq	O
,	O
decision	O
trees	O
are	O
incapable	O
of	O
representing	O
relational	O
information	O
.	O
they	O
are	O
,	O
however	O
,	O
very	O
quick	O
and	O
easy	O
to	O
build	O
.	O
.	O
f	O
and	O
create	O
a	O
decision	O
node	O
.	O
the	O
algorithm	O
operates	O
over	O
a	O
set	O
of	O
training	O
instances	O
,	O
û	O
if	O
all	O
instances	O
inû	O
,	O
create	O
a	O
node	O
are	O
in	O
class	O
partition	O
the	O
traning	O
instances	O
inû	O
into	O
subsets	O
according	O
to	O
the	O
valuesø	O
apply	O
the	O
algorithm	O
recursively	O
to	O
each	O
if	O
the	O
subsets	O
ofû	O
.	O
æ	O
of	O
f.	O
and	O
stop	O
.	O
otherwise	O
select	O
a	O
feature	O
,	O
4	O
$	O
!	O
''	O
``	O
e	O
''	O
''	O
''	O
''	O
	O
	O
	O
	O
sec	O
.	O
12.6	O
]	O
propositional	O
learning	O
systems	O
239	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
+	O
+	O
+	O
-	O
-	O
-	O
-	O
-	O
+	O
+	O
+	O
+	O
+	O
+	O
+	O
+	O
+	O
+	O
+	O
+	O
+	O
+	O
fig	O
.	O
12.7	O
:	O
the	O
dashed	O
line	O
shows	O
the	O
real	O
division	O
of	O
objects	O
in	O
the	O
universe	O
.	O
the	O
solid	O
lines	O
show	O
a	O
decision	O
tree	O
approximation	O
.	O
decision	O
tree	O
learning	O
algorithms	O
can	O
be	O
seen	O
as	O
methods	O
for	O
partitioning	O
the	O
universe	O
into	O
successively	O
smaller	O
rectangles	O
with	O
the	O
goal	O
that	O
each	O
rectangle	O
only	O
contains	O
objects	O
of	O
one	O
class	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
12.7	O
.	O
12.6.1	O
discussion	O
michalski	O
has	O
always	O
argued	O
in	O
favour	O
of	O
rule-based	O
representations	O
over	O
tree	O
structured	O
representations	O
,	O
on	O
the	O
grounds	O
of	O
readability	O
.	O
when	O
the	O
domain	O
is	O
complex	O
,	O
decision	O
trees	O
can	O
become	O
very	O
“	O
bushy	O
”	O
and	O
difﬁcult	O
to	O
understand	O
,	O
whereas	O
rules	O
tend	O
to	O
be	O
modular	O
and	O
can	O
be	O
read	O
in	O
isolation	O
of	O
the	O
rest	O
of	O
the	O
knowledge-base	O
constructed	O
by	O
induction	O
.	O
on	O
the	O
other	O
hand	O
,	O
decision	O
trees	O
induction	O
programs	O
are	O
usually	O
very	O
fast	O
.	O
a	O
compromise	O
is	O
to	O
use	O
decision	O
tree	O
induction	O
to	O
build	O
an	O
initial	O
tree	O
and	O
then	O
derive	O
rules	O
from	O
the	O
tree	O
thus	O
transforming	O
an	O
efﬁcient	O
but	O
opaque	O
representation	O
into	O
a	O
transparent	O
one	O
(	O
quinlan	O
,	O
1987b	O
)	O
.	O
it	O
is	O
instructive	O
to	O
compare	O
the	O
shapes	O
that	O
are	O
produced	O
by	O
various	O
learning	O
systems	O
when	O
they	O
partition	O
the	O
universe	O
.	O
figure	O
12.7	O
demonstrates	O
one	O
weakness	O
of	O
decision	O
tree	O
and	O
other	O
symbolic	O
classiﬁcation	O
.	O
since	O
they	O
approximate	O
partitions	O
with	O
rectangles	O
(	O
if	O
the	O
universe	O
is	O
2-dimensional	O
)	O
there	O
is	O
an	O
inherent	O
inaccuracy	O
when	O
dealing	O
with	O
domains	O
with	O
continuous	O
attributes	O
.	O
function	B
approximation	I
methods	O
and	O
ibl	O
may	O
be	O
able	O
to	O
attain	O
higher	O
accuracy	O
,	O
but	O
at	O
the	O
expense	O
of	O
transparency	O
of	O
the	O
resulting	O
theory	O
.	O
it	O
is	O
more	O
difﬁcult	O
to	O
make	O
general	O
comments	O
about	O
genetic	B
algorithms	I
since	O
the	O
encoding	O
method	O
240	O
knowledge	O
representation	O
[	O
ch	O
.	O
12	O
class2	O
class1	O
fig	O
.	O
12.8	O
:	O
generalisation	O
as	O
set	O
covering	O
.	O
will	O
affect	O
both	O
accuracy	O
and	O
readability	O
.	O
as	O
we	O
have	O
seen	O
,	O
useful	O
insights	O
into	O
induction	O
can	O
be	O
gained	O
by	O
visualising	O
it	O
as	O
searching	O
for	O
a	O
cover	O
of	O
objects	O
in	O
the	O
universe	O
.	O
unfortunately	O
,	O
there	O
are	O
limits	O
to	O
this	O
geometric	O
interpretation	O
of	O
learning	O
.	O
if	O
we	O
wish	O
to	O
learn	O
concepts	O
that	O
describe	O
complex	O
objects	O
and	O
relationships	O
between	O
the	O
objects	O
,	O
it	O
becomes	O
very	O
difﬁcult	O
to	O
visualise	O
the	O
universe	O
.	O
for	O
this	O
reason	O
,	O
it	O
is	O
often	O
useful	O
to	O
rely	O
on	O
reasoning	O
about	O
the	O
concept	O
description	O
language	O
.	O
as	O
we	O
saw	O
,	O
the	O
cover	O
in	O
figure	O
12.5	O
can	O
be	O
expressed	O
as	O
clauses	O
in	O
propositional	O
logic	O
.	O
we	O
can	O
establish	O
a	O
correspondence	O
between	O
sentences	O
in	O
the	O
concept	O
description	O
language	O
(	O
the	O
hypothesis	O
language	O
)	O
and	O
a	O
diagrammatic	O
representation	O
of	O
the	O
concept	O
.	O
more	O
importantly	O
,	O
we	O
can	O
create	O
a	O
correspondence	O
between	O
generalisation	O
and	O
specialisation	O
operations	O
on	O
the	O
sets	O
of	O
objects	O
and	O
generalisation	O
and	O
specialisation	O
operations	O
on	O
the	O
sentences	O
of	O
the	O
language	O
.	O
for	O
example	B
,	O
figure	O
12.8	O
shows	O
two	O
sets	O
,	O
labelled	O
class	O
1	O
and	O
class	O
2.	O
it	O
is	O
clear	O
that	O
class	O
1	O
is	O
a	O
generalisation	O
of	O
class	O
2	O
since	O
it	O
includes	O
a	O
larger	O
number	O
of	O
objects	O
in	O
the	O
universe	O
.	O
we	O
also	O
call	O
class	O
2	O
a	O
specialisation	O
of	O
class	O
1.	O
by	O
convention	O
,	O
we	O
say	O
the	O
description	O
of	O
class	O
1	O
is	O
a	O
generalisation	O
of	O
the	O
description	O
of	O
class	O
2.	O
thus	O
,	O
(	O
12.1	O
)	O
is	O
a	O
generalisation	O
of	O
³¨	O
@	O
ÿ7²²¢ñ	O
{	O
®²¬qß7×à	O
c	O
@	O
ÿéþ¡2×	O
³¨	O
@	O
ÿ7²²ï®²¬qß7×à	O
c	O
@	O
ÿéþ¡2×ãáý³¬	O
úkþ	O
þ×±	O
(	O
12.2	O
)	O
once	O
we	O
have	O
established	O
the	O
correspondence	O
between	O
sets	O
of	O
objects	O
and	O
their	O
descriptions	O
,	O
it	O
is	O
often	O
convenient	O
to	O
forget	O
about	O
the	O
objects	O
and	O
only	O
consider	O
that	O
we	O
are	O
working	O
with	O
expressions	O
in	O
a	O
language	O
.	O
the	O
reason	O
is	O
simple	O
.	O
beyond	O
a	O
certain	O
point	O
of	O
complexity	O
,	O
it	O
is	O
not	O
possible	O
to	O
visualise	O
sets	O
,	O
but	O
it	O
is	O
relatively	O
easy	O
to	O
apply	O
simple	O
transformations	O
on	O
sentences	O
in	O
a	O
formal	O
language	O
.	O
for	O
example	B
,	O
clause	O
(	O
12.2	O
)	O
can	O
be	O
generalised	O
very	O
easily	O
to	O
clause	O
(	O
12.1	O
)	O
by	O
dropping	O
one	O
of	O
the	O
conditions	O
.	O
sec	O
.	O
12.7	O
]	O
relations	O
and	O
background	O
knowledge	O
241	O
in	O
the	O
next	O
section	O
we	O
will	O
look	O
at	O
learning	O
algorithms	O
that	O
deal	O
with	O
relational	O
infor-	O
mation	O
.	O
in	O
this	O
case	O
,	O
the	O
emphasis	O
on	O
language	O
is	O
essential	O
since	O
geometric	O
interpretations	O
no	O
longer	O
provide	O
us	O
with	O
any	O
real	O
insight	O
into	O
the	O
operation	O
of	O
these	O
algorithms	O
.	O
12.7	O
relations	O
and	O
background	O
knowledge	O
inductions	O
systems	O
,	O
as	O
we	O
have	O
seen	O
so	O
far	O
,	O
might	O
be	O
described	O
as	O
“	O
what	O
you	O
see	O
is	O
what	O
you	O
get	O
”	O
.	O
that	O
is	O
,	O
the	O
output	B
class	O
descriptions	O
use	O
the	O
same	O
vocabulary	O
as	O
the	O
input	B
examples	O
.	O
however	O
,	O
we	O
will	O
see	O
in	O
this	O
section	O
,	O
that	O
it	O
is	O
often	O
useful	O
to	O
incorporate	O
background	O
knowledge	O
into	O
learning	O
.	O
we	O
use	O
a	O
simple	O
example	B
from	O
banerji	O
(	O
1980	O
)	O
to	O
the	O
use	O
of	O
background	O
knowledge	O
.	O
there	O
is	O
a	O
language	O
for	O
describing	O
instances	O
of	O
a	O
concept	O
and	O
another	O
for	O
describing	O
concepts	O
.	O
suppose	O
we	O
wish	O
to	O
represent	O
the	O
binary	O
number	O
,	O
10	O
,	O
by	O
a	O
left-recursive	O
binary	O
tree	O
of	O
digits	O
“	O
0	O
”	O
and	O
“	O
1	O
”	O
:	O
“	O
head	O
”	O
and	O
“	O
tail	O
”	O
are	O
the	O
names	O
of	O
attributes	O
.	O
their	O
values	O
follow	O
the	O
colon	O
.	O
the	O
concepts	O
of	O
binary	O
digit	O
and	O
binary	O
number	O
are	O
deﬁned	O
as	O
:	O
%	O
.-ã	O
àâv±q¡	O
 ôó	O
àîâý¼dú	O
)	O
µ	O
%	O
>	O
.-ãv	O
:	O
dñ	O
.1	O
(	O
bdï	O
,	O
j	O
(	O
bòdñ	O
.1	O
(	O
b^ýò	O
	O
õ	O
à¹	O
à¹	O
j	O
 =ÿ	O
jà	O
)	O
nãâz±l¡ öáþ+×7ÿ7±/jà+n	O
c¼d	O
n	O
j	O
 =ÿ	O
jà	O
)	O
nãâz±l¡ öáþ+×7ÿ7±/jà+n^â¼dú+µn	O
thus	O
,	O
an	O
object	O
belongs	O
to	O
a	O
particular	O
class	O
or	O
concept	O
if	O
it	O
satisﬁes	O
the	O
logical	O
expression	O
in	O
the	O
body	O
of	O
the	O
description	O
.	O
predicates	O
in	O
the	O
expression	O
may	O
test	O
the	O
membership	O
of	O
an	O
object	O
in	O
a	O
previously	O
learned	O
concept	O
.	O
banerji	O
always	O
emphasised	O
the	O
importance	O
of	O
a	O
description	O
language	O
that	O
could	O
“	O
grow	O
”	O
.	O
that	O
is	O
,	O
its	O
descriptive	O
power	O
should	O
increase	O
as	O
new	O
concepts	O
are	O
learned	O
.	O
this	O
can	O
clearly	O
be	O
seen	O
in	O
the	O
example	B
above	O
.	O
having	O
learned	O
to	O
describe	O
binary	O
digits	O
,	O
the	O
concept	O
of	O
digit	O
becomes	O
available	O
for	O
use	O
in	O
the	O
description	O
of	O
more	O
complex	O
concepts	O
such	O
as	O
binary	O
number	O
.	O
and	O
an	O
instance	O
:	O
extensibility	O
is	O
a	O
natural	O
and	O
easily	O
implemented	O
feature	O
of	O
horn-clause	O
logic	O
.	O
in	O
addition	O
,	O
a	O
description	O
in	O
horn-clause	O
logic	O
is	O
a	O
logic	O
program	O
and	O
can	O
be	O
executed	O
.	O
for	O
example	B
,	O
to	O
recognise	O
an	O
object	O
,	O
a	O
horn	O
clause	O
can	O
be	O
interpreted	O
in	O
a	O
forward	B
chaining	O
manner	O
.	O
suppose	O
we	O
have	O
a	O
set	O
of	O
clauses	O
:	O
clause	O
(	O
12.3	O
)	O
recognises	O
the	O
ﬁrst	O
two	O
terms	O
in	O
expression	O
(	O
12.5	O
)	O
reducing	O
it	O
to	O
û÷	O
^oøá^îï	O
ûïv	O
ï¬øáïï-û	O
ïï	O
ï¬	O
îï	O
áý	O
á	O
á	O
ã	O
ï¨áïïöáû	O
clause	O
(	O
12.4	O
)	O
reduces	O
this	O
toûï	O
.	O
that	O
is	O
,	O
clauses	O
(	O
12.3	O
)	O
and	O
(	O
12.4	O
)	O
recognise	O
expression	O
(	O
12.5	O
)	O
as	O
the	O
description	O
of	O
an	O
instance	O
of	O
conceptûï	O
.	O
when	O
clauses	O
are	O
executed	O
in	O
a	O
backward	B
chaining	O
manner	O
,	O
they	O
can	O
either	O
verify	O
that	O
the	O
input	B
object	O
belongs	O
to	O
a	O
concept	O
or	O
produce	O
instances	O
of	O
concepts	O
.	O
in	O
other	O
words	O
,	O
(	O
12.3	O
)	O
(	O
12.4	O
)	O
(	O
12.5	O
)	O
ð	O
$	O
ð	O
$	O
''	O
''	O
ñ	O
ó	O
õ	O
	O
242	O
knowledge	O
representation	O
[	O
ch	O
.	O
12	O
larger	O
(	O
hammer	O
,	O
feather	O
)	O
.	O
denser	O
(	O
hammer	O
,	O
feather	O
)	O
.	O
heavier	O
(	O
a	O
,	O
b	O
)	O
:	O
–	O
denser	O
(	O
a	O
,	O
b	O
)	O
,	O
larger	O
(	O
a	O
,	O
b	O
)	O
.	O
:	O
–	O
heavier	O
(	O
hammer	O
,	O
feather	O
)	O
.	O
heavier	O
(	O
a	O
,	O
b	O
)	O
:	O
–	O
denser	O
(	O
a	O
,	O
b	O
)	O
,	O
larger	O
(	O
a	O
,	O
b	O
)	O
.	O
:	O
–	O
heavier	O
(	O
hammer	O
,	O
feather	O
)	O
.	O
denser	O
(	O
hammer	O
,	O
feather	O
)	O
.	O
larger	O
(	O
hammer	O
,	O
feather	O
)	O
.	O
:	O
–	O
denser	O
(	O
hammer	O
,	O
feather	O
)	O
,	O
larger	O
(	O
hammer	O
,	O
feather	O
)	O
.	O
:	O
–	O
larger	O
(	O
hammer	O
,	O
feather	O
)	O
.	O
fig	O
.	O
12.9	O
:	O
a	O
resolution	O
proof	O
tree	O
from	O
muggleton	O
&	O
feng	O
(	O
1990	O
)	O
.	O
we	O
attempt	O
to	O
prove	O
an	O
assertion	O
is	O
true	O
with	O
respect	O
to	O
a	O
background	O
theory	O
.	O
resolution	O
(	O
robinson	O
,	O
1965	O
)	O
provides	O
an	O
efﬁcient	O
means	O
of	O
deriving	O
a	O
solution	O
to	O
a	O
problem	O
,	O
giving	O
a	O
set	O
of	O
axioms	O
which	O
deﬁne	O
the	O
task	O
environment	O
.	O
the	O
algorithm	O
takes	O
two	O
terms	O
and	O
resolves	O
them	O
into	O
a	O
most	O
general	O
uniﬁer	O
,	O
as	O
illustrated	O
in	O
figure	O
12.9	O
by	O
the	O
execution	O
of	O
a	O
simple	O
prolog	O
program	O
.	O
the	O
box	O
in	O
the	O
ﬁgure	O
contains	O
clauses	O
that	O
make	O
up	O
the	O
theory	O
,	O
or	O
knowledge	O
base	O
,	O
and	O
the	O
question	O
to	O
be	O
answered	O
,	O
namely	O
,	O
“	O
is	O
it	O
true	O
that	O
a	O
hammer	O
is	O
heavier	O
than	O
a	O
feather	O
”	O
?	O
a	O
resolution	O
proof	O
is	O
a	O
proof	O
by	O
refutation	O
.	O
that	O
is	O
,	O
answer	O
the	O
question	O
,	O
we	O
assume	O
that	O
it	O
is	O
false	O
and	O
then	O
see	O
if	O
the	O
addition	O
,	O
to	O
the	O
theory	O
,	O
of	O
this	O
negative	O
statement	O
results	O
in	O
a	O
contradiction	O
.	O
the	O
literals	O
on	O
the	O
left	O
hand	O
side	O
of	O
a	O
prolog	O
clause	O
are	O
positive	O
.	O
those	O
on	O
the	O
left	O
hand	O
side	O
are	O
negative	O
.	O
the	O
proof	O
procedure	O
looks	O
for	O
complimentary	O
literals	O
in	O
two	O
clauses	O
,	O
n	O
and	O
×	O
<	O
ÿ	O
>	O
 ãþ	O
)	O
×-þ	O
.	O
i.e	O
.	O
literals	O
of	O
opposite	O
sign	O
that	O
unify	O
.	O
in	O
the	O
example	B
in	O
figure	O
12.9	O
,	O
þ	O
)	O
×7ÿø×5þ7j1ú	O
x	O
þ	O
)	O
×	O
<	O
ÿø7×-þ7jþkÿµµ×-þx	O
×	O
<	O
ÿ	O
>	O
 ãþ	O
)	O
×-þn	O
unify	O
to	O
create	O
the	O
ﬁrst	O
resolvent	O
:	O
±7×¨¼o²×-þ7jþkÿµîµý×5þx	O
×7ÿ ãþ	O
)	O
×5þn¨xoþ+×7ÿø×-þ7jþkÿµµý×7þx	O
×	O
<	O
ÿ	O
>	O
 ãþ	O
)	O
×-þn	O
a	O
side	O
effect	O
of	O
uniﬁcation	O
is	O
to	O
create	O
variable	O
substitutionsú	O
%	O
óþ'ÿµîµý×5þx	O
by	O
continued	O
application	O
of	O
resolution	O
,	O
we	O
can	O
eventually	O
derive	O
the	O
empty	O
clause	O
,	O
which	O
indicates	O
a	O
contradiction	O
.	O
plotkin	O
’	O
s	O
(	O
1970	O
)	O
work	O
“	O
originated	O
with	O
a	O
suggestion	O
of	O
r.j.	O
popplestone	O
that	O
since	O
uniﬁcation	O
is	O
useful	O
in	O
automatic	O
deduction	O
by	O
the	O
resolution	O
method	O
,	O
its	O
dual	O
might	O
prove	O
helpful	O
for	O
induction	O
.	O
the	O
dual	O
of	O
the	O
most	O
general	O
uniﬁer	O
of	O
two	O
literals	O
is	O
called	O
the	O
least	O
general	O
generalisation	O
”	O
.	O
at	O
about	O
the	O
same	O
time	O
that	O
plotkin	O
took	O
up	O
this	O
idea	O
,	O
j.c.	O
reynolds	O
was	O
also	O
developing	O
the	O
use	O
of	O
least	O
general	O
generalisations	O
.	O
reynolds	O
(	O
1970	O
)	O
also	O
recognised	O
the	O
connection	O
between	O
deductive	O
theorem	O
proving	O
and	O
inductive	B
learning	I
:	O
½	O
ö	O
ö	O
ö	O
½	O
ó	O
ö	O
sec	O
.	O
12.7	O
]	O
relations	O
and	O
background	O
knowledge	O
243	O
robinson	O
’	O
s	O
uniﬁcation	O
algorithm	O
allows	O
the	O
computation	O
of	O
the	O
greatest	O
common	O
instance	O
of	O
any	O
ﬁnite	O
set	O
of	O
uniﬁable	O
atomic	O
formulas	O
.	O
this	O
suggests	O
the	O
existence	O
of	O
a	O
dual	O
operation	O
of	O
“	O
least	O
common	O
generalisation	O
”	O
.	O
it	O
turns	O
out	O
that	O
such	O
an	O
operation	O
exists	O
and	O
can	O
be	O
computed	O
by	O
a	O
simple	O
algorithm	O
.	O
such	O
that	O
(	O
12.6	O
)	O
(	O
12.7	O
)	O
(	O
12.8	O
)	O
.	O
the	O
least	O
general	O
generalisation	O
of	O
background	O
information	O
which	O
may	O
assist	O
generalisation	O
.	O
suppose	O
we	O
are	O
given	O
two	O
instances	O
of	O
a	O
concept	O
cuddly	O
pet	O
,	O
buntine	O
(	O
1988	O
)	O
pointed	O
out	O
that	O
simple	O
subsumption	O
is	O
unable	O
to	O
take	O
advantage	O
of	O
if	O
there	O
is	O
a	O
substitutionð	O
the	O
method	O
of	O
least	O
general	O
generalisations	O
is	O
based	O
on	O
subsumption	O
.	O
a	O
clauseû	O
sub-	O
sumes	O
,	O
or	O
is	O
more	O
general	O
than	O
,	O
another	O
clauseûï	O
ïù	O
jq¡/j3ÿ7n¨xÿ2n	O
ÿ¼o±	O
jl¡/jü¬n¨xoü¨n	O
²	O
jl¡/jqkvn¬xkvn	O
under	O
the	O
substitution	O
ðéÿ½ókvñ	O
,	O
(	O
12.8	O
)	O
is	O
equivalent	O
to	O
(	O
12.6	O
)	O
,	O
and	O
under	O
the	O
substitu-	O
tionðü-ókvñ	O
,	O
(	O
12.8	O
)	O
is	O
equivalent	O
to	O
(	O
12.7	O
)	O
.	O
therefore	O
,	O
the	O
least	O
general	O
generalisation	O
of	O
jq¡/j3ÿ7n¨xÿ2n	O
and	O
»	O
is	O
»	O
jl¡/jlkzn¬xkvn	O
and	O
results	O
in	O
the	O
inverse	O
substitutionð	O
>	O
kúóð¢ÿ	O
)	O
xoü¨ññ	O
.	O
jl¡/jü¬n¨xoü¨n	O
ö	O
)	O
ö	O
äú	O
³oúd±2±7	O
ûá	O
× ¨jqkvnú	O
ö	O
)	O
ö	O
³oúd±2±7	O
ûá	O
äú	O
× ¨jqkvnú	O
±7¡/jqkvn	O
× ¨jqkvnú	O
³-ÿ	O
>	O
 ¨jqkvn	O
× ¨jqkvnú	O
³oúd±2±7	O
ûá	O
× ¨jqkvnû	O
äú	O
³oúd±2±7	O
ûá	O
äú	O
× ¨jqkvnû	O
á1jqkrn^áý±7¡1jqkvn	O
á1jqkrn^áý³5ÿ ¨jlkvn	O
á1jqkrn	O
á1jqkrn^á	O
since	O
unmatched	O
literals	O
are	O
dropped	O
from	O
the	O
clause	O
.	O
however	O
,	O
given	O
the	O
background	O
knowledge	O
,	O
we	O
can	O
see	O
that	O
this	O
is	O
an	O
over-generalisation	O
.	O
a	O
better	O
one	O
is	O
:	O
the	O
moral	O
being	O
that	O
a	O
generalisation	O
should	O
only	O
be	O
done	O
when	O
the	O
relevant	O
background	O
knowledge	O
suggests	O
it	O
.	O
so	O
,	O
observing	O
(	O
12.9	O
)	O
,	O
use	O
clause	O
(	O
12.11	O
)	O
as	O
a	O
rewrite	O
rule	O
to	O
produce	O
a	O
generalisation	O
which	O
is	O
clause	O
(	O
12.12	O
)	O
.	O
which	O
also	O
subsumes	O
clause	O
(	O
12.10	O
)	O
.	O
buntine	O
drew	O
on	O
earlier	O
work	O
by	O
sammut	O
(	O
sammut	O
&	O
banerji	O
,	O
1986	O
)	O
in	O
constructing	O
his	O
generalised	O
subsumption	O
.	O
muggleton	O
&	O
buntine	O
(	O
1998	O
)	O
took	O
this	O
approach	O
a	O
step	O
further	O
and	O
realised	O
that	O
through	O
the	O
application	O
of	O
a	O
few	O
simple	O
rules	O
,	O
they	O
could	O
invert	O
resolution	O
as	O
plotkin	O
and	O
reynolds	O
had	O
wished	O
.	O
here	O
are	O
two	O
of	O
the	O
rewrite	O
rules	O
in	O
propositional	O
form	O
:	O
given	O
a	O
set	O
of	O
clauses	O
,	O
the	O
body	O
of	O
one	O
of	O
which	O
is	O
completely	O
contained	O
in	O
the	O
bodies	O
according	O
to	O
subsumption	O
,	O
the	O
least	O
general	O
generalisation	O
of	O
(	O
12.4	O
)	O
and	O
(	O
12.5	O
)	O
is	O
:	O
(	O
12.9	O
)	O
(	O
12.10	O
)	O
(	O
12.11	O
)	O
(	O
12.12	O
)	O
suppose	O
we	O
also	O
know	O
the	O
following	O
:	O
ö	O
)	O
ö	O
ö	O
)	O
ö	O
×o ¨jlkvn	O
of	O
the	O
others	O
,	O
such	O
as	O
:	O
ú'á	O
ú'á	O
áû'áîzá	O
áû	O
û	O
û	O
	O
ð	O
»	O
»	O
»	O
»	O
»	O
ö	O
»	O
ö	O
»	O
»	O
»	O
ö	O
»	O
ö	O
»	O
k	O
	O
½	O
ò	O
	O
½	O
244	O
knowledge	O
representation	O
[	O
ch	O
.	O
12	O
the	O
absorption	O
operation	O
results	O
in	O
:	O
intra-construction	O
takes	O
a	O
group	O
of	O
rules	O
all	O
having	O
the	O
same	O
head	O
,	O
such	O
as	O
:	O
and	O
replaces	O
them	O
with	O
:	O
áîzáý	O
áû	O
ú'á	O
áû\áîzá	O
áî	O
#	O
áü	O
ú'á	O
áîzá	O
ûcá	O
ú'áü	O
these	O
two	O
operations	O
can	O
be	O
interpreted	O
in	O
terms	O
of	O
the	O
proof	O
tree	O
shown	O
in	O
figure	O
12.9.	O
resolution	O
accepts	O
two	O
clauses	O
and	O
applies	O
uniﬁcation	O
to	O
ﬁnd	O
the	O
maximal	O
common	O
uniﬁer	O
.	O
in	O
the	O
diagram	O
,	O
two	O
clauses	O
at	O
the	O
top	O
of	O
a	O
“	O
v	O
”	O
are	O
resolved	O
to	O
produce	O
the	O
resolvent	O
at	O
the	O
apex	O
of	O
the	O
“	O
v	O
”	O
.	O
absorption	O
accepts	O
the	O
resolvent	O
and	O
one	O
of	O
the	O
other	O
two	O
clauses	O
to	O
produce	O
the	O
third	O
.	O
thus	O
,	O
it	O
inverts	O
the	O
resolution	O
step	O
.	O
intra-construction	O
automatically	O
creates	O
a	O
new	O
term	O
in	O
its	O
attempt	O
to	O
simplify	O
descrip-	O
tions	O
.	O
this	O
is	O
an	O
essential	O
feature	O
of	O
inverse	O
resolution	O
since	O
there	O
may	O
be	O
terms	O
in	O
a	O
theory	O
that	O
are	O
not	O
explicitly	O
shown	O
in	O
an	O
example	B
and	O
may	O
have	O
to	O
be	O
invented	O
by	O
the	O
learning	O
program	O
.	O
12.7.1	O
discussion	O
these	O
methods	O
and	O
others	O
(	O
muggleton	O
&	O
feng	O
,	O
1990	O
;	O
quinlan	O
,	O
1990	O
)	O
have	O
made	O
relational	O
learning	O
quite	O
efﬁcient	O
.	O
because	O
the	O
language	O
of	O
horn-clause	O
logic	O
is	O
more	O
expressive	O
than	O
the	O
other	O
concept	O
description	O
languages	O
we	O
have	O
seen	O
,	O
it	O
is	O
now	O
possible	O
to	O
learn	O
far	O
more	O
complex	O
concepts	O
than	O
was	O
previously	O
possible	O
.	O
a	O
particularly	O
important	O
application	O
of	O
this	O
style	O
of	O
learning	O
is	O
knowledge	O
discovery	O
.	O
there	O
are	O
now	O
vast	O
databases	O
accumulating	O
information	O
on	O
the	O
genetic	O
structure	O
of	O
human	O
beings	O
,	O
aircraft	O
accidents	O
,	O
company	O
invento-	O
ries	O
,	O
pharmaceuticals	O
and	O
countless	O
more	O
.	O
powerful	O
induction	O
programs	O
that	O
use	O
expressive	O
languages	O
may	O
be	O
a	O
vital	O
aid	O
in	O
discovering	O
useful	O
patterns	O
in	O
all	O
these	O
data	O
.	O
for	O
example	B
,	O
the	O
realities	O
of	O
drug	O
design	O
require	O
descriptive	O
powers	O
that	O
encompass	O
stereo-spatial	O
and	O
other	O
long-range	O
relations	O
between	O
different	O
parts	O
of	O
a	O
molecule	O
,	O
and	O
can	O
generate	O
,	O
in	O
effect	O
,	O
new	O
theories	O
.	O
the	O
pharmaceutical	O
industry	O
spends	O
over	O
$	O
250	O
million	O
for	O
each	O
new	O
drug	O
released	O
onto	O
the	O
market	O
.	O
the	O
greater	O
part	O
of	O
this	O
expenditure	O
reﬂects	O
today	O
’	O
s	O
unavoidably	O
“	O
scatter-gun	O
”	O
synthesis	O
of	O
compounds	O
which	O
might	O
possess	O
biological	O
activity	O
.	O
even	O
a	O
limited	O
capability	O
to	O
construct	O
predictive	O
theories	O
from	O
data	O
promises	O
high	O
returns	O
.	O
the	O
relational	O
program	O
golem	O
was	O
applied	O
to	O
the	O
drug	O
design	O
problem	O
of	O
modelling	O
structure-activity	O
relations	O
(	O
king	O
et	O
al.	O
,	O
1992	O
)	O
.	O
training	O
data	O
for	O
the	O
program	O
was	O
44	O
trimethoprim	O
analogues	O
and	O
their	O
observed	O
inhibition	O
of	O
e.	O
coli	O
dihydrofolate	O
reductase	O
.	O
a	O
further	O
11	O
compounds	O
were	O
used	O
as	O
unseen	O
test	O
data	O
.	O
golem	O
obtained	O
rules	O
that	O
were	O
statistically	O
more	O
accurate	O
on	O
the	O
training	O
data	O
and	O
also	O
better	O
on	O
the	O
test	O
data	O
than	O
a	O
han-	O
sch	O
linear	O
regression	O
model	O
.	O
importantly	O
,	O
relational	O
learning	O
yields	O
understandable	O
rules	O
k	O
	O
ò	O
ò	O
	O
½	O
k	O
	O
½	O
k	O
	O
½	O
k	O
	O
½	O
õ	O
õ	O
	O
õ	O
	O
sec	O
.	O
12.8	O
]	O
conclusion	O
245	O
that	O
characterise	O
the	O
stereochemistry	O
of	O
the	O
interaction	O
of	O
trimethoprim	O
with	O
dihydrofolate	O
reductase	O
observed	O
crystallographically	O
.	O
in	O
this	O
domain	O
,	O
relational	O
learning	O
thus	O
offers	O
a	O
new	O
approach	O
which	O
complements	O
other	O
methods	O
,	O
directing	O
the	O
time-consuming	O
process	O
of	O
the	O
design	O
of	O
potent	O
pharmacological	O
agents	O
from	O
a	O
lead	O
compound	O
,	O
–	O
variants	O
of	O
which	O
need	O
to	O
be	O
characterised	O
for	O
likely	O
biological	O
activity	O
before	O
committing	O
resources	O
to	O
their	O
synthesis	O
.	O
12.8	O
conclusions	O
we	O
have	O
now	O
completed	O
a	O
rapid	O
tour	O
of	O
a	O
variety	O
of	O
learning	O
algorithms	O
and	O
seen	O
how	O
the	O
method	O
of	O
representing	O
knowledge	O
is	O
crucial	O
in	O
the	O
following	O
ways	O
:	O
knowledge	O
representation	O
determines	O
the	O
concepts	O
that	O
an	O
algorithm	O
can	O
and	O
can	O
not	O
learn	O
.	O
knowledge	O
representation	O
affects	O
the	O
speed	O
of	O
learning	O
.	O
some	O
representations	O
lend	O
themselves	O
to	O
more	O
efﬁcient	O
implementation	O
than	O
others	O
.	O
also	O
,	O
the	O
more	O
expressive	O
the	O
language	O
,	O
the	O
larger	O
is	O
the	O
search	O
space	O
.	O
knowledge	O
representation	O
determines	O
the	O
readability	O
of	O
the	O
concept	O
description	O
.	O
a	O
representation	O
that	O
is	O
opaque	O
to	O
the	O
user	O
may	O
allow	O
the	O
program	O
to	O
learn	O
,	O
but	O
a	O
repre-	O
sentation	O
that	O
is	O
transparent	O
also	O
allows	O
the	O
user	O
to	O
learn	O
.	O
thus	O
,	O
when	O
approaching	O
a	O
machine	O
learning	O
problem	O
,	O
the	O
choice	O
of	O
knowledge	O
represen-	O
tation	O
formalism	O
is	O
just	O
as	O
important	O
as	O
the	O
choice	O
of	O
learning	O
algorithm	O
.	O
	O
	O
	O
13	O
learning	O
to	O
control	O
dynamic	O
systems	O
tanja	O
urbanˇciˇc	O
(	O
1	O
)	O
and	O
ivan	O
bratko	O
(	O
1,2	O
)	O
(	O
1	O
)	O
joˇzef	O
stefan	O
institute	O
and	O
(	O
2	O
)	O
and	O
university	O
of	O
ljubljana	O
13.1	O
introduction	O
the	O
emphasis	O
in	O
controller	B
design	I
has	O
shifted	O
from	O
the	O
precision	O
requirements	O
towards	O
the	O
following	O
objectives	B
(	O
leitch	O
&	O
francis	O
,	O
1986	O
;	O
enterline	O
,	O
1988	O
;	O
verbruggen	O
and	O
˚astr˝om	O
,	O
1989	O
;	O
˚astr˝om	O
,	O
1991	O
;	O
sammut	O
&	O
michie	O
,	O
1991	O
;	O
airtc92	O
,	O
1992	O
)	O
:	O
control	O
without	O
complete	O
prior	O
knowledge	O
(	O
to	O
extend	O
the	O
range	O
of	O
automatic	O
control	O
applications	O
)	O
,	O
reliability	O
,	O
robustness	O
and	O
adaptivity	O
(	O
to	O
provide	O
successful	O
performance	O
in	O
the	O
real-	O
world	O
environment	O
)	O
,	O
transparency	O
of	O
solutions	O
(	O
to	O
enable	O
understanding	O
and	O
veriﬁcation	O
)	O
,	O
generality	O
(	O
to	O
facilitate	O
the	O
transfer	O
of	O
solutions	O
to	O
similar	O
problems	O
)	O
,	O
realisation	O
of	O
speciﬁed	O
characteristics	O
of	O
system	O
response	O
(	O
to	O
please	O
customers	O
)	O
.	O
these	O
problems	O
are	O
tackled	O
in	O
different	O
ways	O
,	O
for	O
example	B
by	O
using	O
expert	O
systems	O
(	O
dvo-	O
rak	O
,	O
1987	O
)	O
,	O
neural	O
networks	O
(	O
miller	O
et	O
al.	O
,	O
1990	O
;	O
hunt	O
et	O
al.	O
,	O
1992	O
)	O
,	O
fuzzy	O
control	O
(	O
lee	O
,	O
1990	O
)	O
and	O
genetic	B
algorithms	I
(	O
renders	O
&	O
nordvik	O
,	O
1992	O
)	O
.	O
however	O
,	O
in	O
the	O
absence	O
of	O
a	O
complete	O
review	O
and	O
comparative	O
evaluations	O
,	O
the	O
decision	O
about	O
how	O
to	O
solve	O
a	O
problem	O
at	O
hand	O
remains	O
a	O
difﬁcult	O
task	O
and	O
is	O
often	O
taken	O
ad	O
hoc	O
.	O
leitch	O
(	O
1992	O
)	O
has	O
introduced	O
a	O
step	O
towards	O
a	O
systematisation	O
that	O
could	O
provide	O
some	O
guidelines	O
.	O
however	O
,	O
most	O
of	O
the	O
approaches	O
provide	O
only	O
partial	O
fulﬁlment	O
of	O
the	O
objectives	B
stated	O
above	O
.	O
taking	O
into	O
ac-	O
count	O
also	O
increasing	O
complexity	O
of	O
modern	O
systems	O
together	O
with	O
real-time	O
requirements	O
,	O
one	O
must	O
agree	O
with	O
schoppers	O
(	O
1991	O
)	O
,	O
that	O
designing	O
control	O
means	O
looking	O
for	O
a	O
suitable	O
compromise	O
.	O
it	O
should	O
be	O
tailored	O
to	O
the	O
particular	O
problem	O
speciﬁcations	O
,	O
since	O
some	O
objectives	B
are	O
normally	O
achieved	O
at	O
the	O
cost	O
of	O
some	O
others	O
.	O
another	O
important	O
research	O
theme	O
is	O
concerned	O
with	O
the	O
replication	O
of	O
human	O
operators	O
’	O
subconscious	O
skill	O
.	O
experienced	O
operators	O
manage	O
to	O
control	O
systems	O
that	O
are	O
extremely	O
difﬁcult	O
to	O
be	O
modelled	O
and	O
controlled	O
by	O
classical	O
methods	O
.	O
therefore	O
,	O
a	O
“	O
natural	O
”	O
choice	O
would	O
be	O
to	O
mimic	O
such	O
skilful	O
operators	O
.	O
one	O
way	O
of	O
doing	O
this	O
is	O
by	O
modelling	O
the	O
¸	O
address	O
for	O
correspondence	O
:	O
joˇzef	O
stefan	O
institute	O
,	O
univerza	O
v	O
lubljani	O
,	O
61111	O
ljubljana	O
,	O
slovenia	O
	O
	O
	O
	O
	O
sec	O
.	O
13.1	O
]	O
introduction	O
247	O
operator	O
’	O
s	O
strategy	O
in	O
the	O
form	O
of	O
rules	O
.	O
the	O
main	O
problem	O
is	O
how	O
to	O
establish	O
the	O
appropriate	O
set	O
of	O
rules	O
:	O
while	O
gaining	O
skill	O
,	O
people	O
often	O
lose	O
their	O
awareness	O
of	O
what	O
they	O
are	O
actually	O
doing	O
.	O
their	O
knowledge	O
is	O
implicit	O
,	O
meaning	O
that	O
it	O
can	O
be	O
demonstrated	O
and	O
observed	O
,	O
but	O
hardly	O
ever	O
described	O
explicitly	O
in	O
a	O
way	O
needed	O
for	O
the	O
direct	O
transfer	O
into	O
an	O
automatic	O
controller	O
.	O
although	O
the	O
problem	O
is	O
general	O
,	O
it	O
is	O
particularly	O
tough	O
in	O
the	O
case	O
of	O
control	O
of	O
fast	O
dynamic	O
systems	O
where	O
subconscious	O
actions	O
are	O
more	O
or	O
less	O
the	O
prevailing	O
form	O
of	O
performance	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
dynamic	O
system	O
learning	O
system	O
control	O
rule	O
partial	O
knowledge	O
dynamic	O
system	O
learning	O
system	O
control	O
rule	O
(	O
c	O
)	O
operator	O
dynamic	O
system	O
learning	O
system	O
control	O
rule	O
fig	O
.	O
13.1	O
:	O
three	O
modes	O
of	O
learning	O
to	O
control	O
a	O
dynamic	O
system	O
:	O
(	O
b	O
)	O
exploiting	O
partial	O
knowledge	O
,	O
(	O
c	O
)	O
extracting	O
human	O
operator	O
’	O
s	O
skill	O
.	O
(	O
a	O
)	O
learning	O
from	O
scratch	O
,	O
the	O
aim	O
of	O
this	O
chapter	O
is	O
to	O
show	O
how	O
the	O
methods	O
of	O
machine	O
learning	O
can	O
help	O
in	O
the	O
construction	O
of	O
controllers	O
and	O
in	O
bridging	O
the	O
gap	O
between	O
the	O
subcognitive	O
skill	O
and	O
its	O
machine	O
implementation	O
.	O
first	O
successful	O
attempts	O
in	O
learning	O
control	O
treated	O
the	O
controlled	O
system	O
as	O
a	O
black	O
box	O
(	O
for	O
example	B
michie	O
&	O
chambers	O
,	O
1968	O
)	O
,	O
and	O
a	O
program	O
learnt	O
to	O
control	O
it	O
by	O
trials	O
.	O
due	O
to	O
the	O
black	O
box	O
assumption	O
,	O
initial	O
control	O
decisions	O
are	O
practically	O
random	O
,	O
resulting	O
in	O
very	O
poor	O
performance	O
in	O
the	O
ﬁrst	O
experiments	O
.	O
on	O
the	O
basis	O
of	O
experimental	O
evidence	O
,	O
control	O
decisions	O
are	O
evaluated	O
and	O
possibly	O
changed	O
.	O
learning	O
takes	O
place	O
until	O
a	O
certain	O
success	O
criterion	O
is	O
met	O
.	O
later	O
on	O
,	O
this	O
basic	O
idea	O
was	O
implemented	O
in	O
different	O
ways	O
,	O
ranging	O
from	O
neural	O
networks	O
(	O
for	O
example	B
barto	O
ý	O
þ	O
ÿ	O
 	O
	O
	O
	O
ý	O
þ	O
ÿ	O
 	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
248	O
learning	O
to	O
control	O
dynamic	O
systems	O
[	O
ch	O
.	O
13	O
et	O
al.	O
,	O
1983	O
;	O
anderson	O
,	O
1987	O
)	O
to	O
genetic	B
algorithms	I
(	O
for	O
example	B
odetayo	O
&	O
mcgregor	O
,	O
1989	O
)	O
.	O
recently	O
,	O
the	O
research	O
concentrated	O
on	O
removing	O
the	O
deﬁciencies	O
inherent	O
to	O
these	O
methods	O
,	O
like	O
the	O
obscurity	O
and	O
unreliability	O
of	O
the	O
learned	O
control	O
rules	O
(	O
bain	O
,	O
1990	O
;	O
sammut	O
&	O
michie	O
,	O
1991	O
;	O
sammut	O
&	O
cribb	O
,	O
1990	O
)	O
and	O
time-consuming	O
experimentation	O
(	O
sammut	O
,	O
1994	O
)	O
while	O
still	O
presuming	O
no	O
prior	O
knowledge	O
.	O
until	O
recently	O
,	O
this	O
kind	O
of	O
learning	O
control	O
has	O
remained	O
predominant	O
.	O
however	O
,	O
some	O
of	O
the	O
mentioned	O
deﬁciences	O
are	O
closely	O
related	O
to	O
the	O
black	O
box	O
assumption	O
,	O
which	O
is	O
hardly	O
ever	O
necessary	O
in	O
such	O
a	O
strict	O
form	O
.	O
therefore	O
,	O
the	O
latest	O
attempts	O
take	O
advantage	O
of	O
the	O
existing	O
knowledge	O
,	O
being	O
explicit	O
and	O
formulated	O
at	O
the	O
symbolic	O
level	O
(	O
for	O
example	B
urbanˇciˇc	O
&	O
bratko	O
,	O
1992	O
;	O
bratko	O
,	O
1993	O
;	O
varˇsek	O
et	O
al.	O
,	O
1993	O
)	O
,	O
or	O
implicit	O
and	O
observable	O
just	O
as	O
operator	O
’	O
s	O
skill	O
(	O
michie	O
et	O
al.	O
,	O
1990	O
;	O
sammut	O
et	O
al.	O
,	O
1992	O
;	O
camacho	O
&	O
michie	O
,	O
1992	O
;	O
michie	O
&	O
camacho	O
,	O
1994	O
)	O
.	O
the	O
structure	O
of	O
the	O
chapter	O
follows	O
this	O
introductory	O
discussion	O
.	O
we	O
consider	O
three	O
modes	O
of	O
learning	O
to	O
control	O
a	O
system	O
.	O
the	O
three	O
modes	O
,	O
illustrated	O
in	O
figure	O
13.1	O
,	O
are	O
:	O
(	O
a	O
)	O
the	O
learning	O
system	O
learns	O
to	O
control	O
a	O
dynamic	O
system	O
by	O
trial	O
and	O
error	O
,	O
without	O
any	O
prior	O
knowledge	O
about	O
the	O
system	O
to	O
be	O
controlled	O
(	O
learning	O
from	O
scratch	O
)	O
.	O
(	O
b	O
)	O
as	O
in	O
(	O
a	O
)	O
,	O
but	O
the	O
learning	O
system	O
exploits	O
some	O
partial	O
explicit	O
knowledge	O
about	O
the	O
dynamic	O
system	O
.	O
(	O
c	O
)	O
the	O
learning	O
system	O
observes	O
a	O
human	O
operator	O
and	O
learns	O
to	O
replicate	O
the	O
operator	O
’	O
s	O
skill	O
.	O
experiments	O
in	O
learning	O
to	O
control	O
are	O
popularly	O
carried	O
out	O
using	O
the	O
task	O
of	O
controlling	O
the	O
pole-and-cart	O
system	O
.	O
in	O
section	O
13.2	O
we	O
therefore	O
describe	O
this	O
experimental	O
domain	O
.	O
sections	O
13.3	O
and	O
13.4	O
describe	O
two	O
approaches	O
to	O
learning	O
from	O
scratch	O
:	O
boxes	O
and	O
genetic	O
learning	O
.	O
in	O
section	O
13.5	O
the	O
learning	O
system	O
exploits	O
partial	O
explicit	O
knowledge	O
.	O
in	O
section	O
13.6	O
the	O
learning	O
system	O
exploits	O
the	O
operator	O
’	O
s	O
skill	O
.	O
13.2	O
experimental	O
domain	O
the	O
main	O
ideas	O
presented	O
in	O
this	O
chapter	O
will	O
be	O
illustrated	O
by	O
using	O
the	O
pole	B
balancing	I
problem	O
(	O
anderson	O
&	O
miller	O
,	O
1990	O
)	O
as	O
a	O
case	O
study	O
.	O
so	O
let	O
us	O
start	O
with	O
a	O
description	O
of	O
this	O
control	O
task	O
which	O
has	O
often	O
been	O
chosen	O
to	O
demonstrate	O
both	O
classical	O
and	O
nonconventional	O
control	O
techniques	O
.	O
besides	O
being	O
an	O
attractive	O
benchmark	O
,	O
it	O
also	O
bears	O
similarities	O
with	O
tasks	O
of	O
signiﬁcant	O
practical	O
importance	O
such	O
as	O
two-legged	O
walking	O
,	O
and	O
satellite	O
attitude	O
control	O
(	O
sammut	O
&	O
michie	O
,	O
1991	O
)	O
.	O
the	O
system	O
consists	O
of	O
a	O
rigid	O
pole	O
and	O
a	O
cart	O
.	O
the	O
cart	O
can	O
move	O
left	O
and	O
right	O
on	O
a	O
bounded	O
track	O
.	O
the	O
pole	O
is	O
hinged	O
to	O
the	O
top	O
of	O
the	O
cart	O
so	O
that	O
it	O
can	O
swing	O
in	O
the	O
vertical	O
plane	O
.	O
in	O
the	O
ai	O
literature	O
,	O
the	O
task	O
is	O
usually	O
just	O
to	O
prevent	O
the	O
pole	O
from	O
falling	O
and	O
to	O
keep	O
the	O
cart	O
position	O
within	O
the	O
speciﬁed	O
limits	O
,	O
while	O
the	O
control	O
regime	O
is	O
that	O
of	O
bang-bang	O
.	O
the	O
control	O
force	O
has	O
a	O
ﬁxed	O
magnitude	O
and	O
all	O
the	O
controller	O
can	O
do	O
is	O
to	O
change	O
the	O
force	O
direction	O
in	O
regular	O
time	O
intervals	O
.	O
classical	O
methods	O
(	O
for	O
example	B
kwakernaak	O
&	O
sivan	O
,	O
1972	O
)	O
can	O
be	O
applied	O
to	O
con-	O
trolling	O
the	O
system	O
under	O
several	O
assumptions	O
,	O
including	O
complete	O
knowledge	O
about	O
the	O
system	O
,	O
that	O
is	O
a	O
differential	O
equations	O
model	O
up	O
to	O
numerical	O
values	O
of	O
its	O
parameters	O
.	O
alternative	O
approaches	O
tend	O
to	O
weaken	O
these	O
assumptions	O
by	O
constructing	O
control	O
rules	O
in	O
two	O
essentially	O
different	O
ways	O
:	O
by	O
learning	O
from	O
experience	O
,	O
and	O
by	O
qualitative	O
reasoning	O
.	O
the	O
ﬁrst	O
one	O
will	O
be	O
presented	O
in	O
more	O
detail	O
later	O
in	O
this	O
chapter	O
.	O
the	O
second	O
one	O
will	O
be	O
described	O
here	O
only	O
up	O
to	O
the	O
level	O
needed	O
for	O
comparison	O
and	O
understanding	O
,	O
giving	O
a	O
general	O
idea	O
about	O
two	O
solutions	O
of	O
this	O
kind	O
:	O
sec	O
.	O
13.2	O
]	O
experimental	O
domain	O
249	O
	O
	O
critical	O
	O
	O
critical	O
	O
critical	O
	O
critical	O
left	O
left	O
left	O
left	O
	O
critical	O
	O
critical	O
critical	O
critical	O
right	O
right	O
right	O
right	O
fig	O
.	O
13.2	O
:	O
makaroviˇc	O
’	O
s	O
rule	O
for	O
pole	B
balancing	I
.	O
a	O
solution	O
,	O
distinguished	O
by	O
its	O
simplicity	O
,	O
was	O
derived	O
by	O
makaroviˇc	O
(	O
1988	O
)	O
(	O
see	O
figure	O
13.2	O
)	O
.	O
rules	O
of	O
the	O
same	O
tree	O
structure	O
,	O
but	O
with	O
the	O
state	O
variables	O
ordered	O
in	O
different	O
ways	O
,	O
were	O
experimentally	O
studied	O
by	O
dˇzeroski	O
(	O
1989	O
)	O
.	O
he	O
showed	O
that	O
no	O
less	O
than	O
seven	O
permutations	O
of	O
state	O
variables	O
yielded	O
successful	O
control	O
rules	O
.	O
we	O
is	O
a	O
permutation	O
of	O
the	O
variables	O
,	O
determining	O
their	O
top-down	O
order	O
.	O
another	O
solution	O
was	O
inferred	O
by	O
bratko	O
(	O
1991	O
)	O
from	O
a	O
very	O
simple	O
qualitative	O
model	O
of	O
the	O
inverted	O
pendulum	O
system	O
.	O
the	O
derived	O
control	O
rule	O
is	O
described	O
by	O
the	O
following	O
denote	O
such	O
rules	O
as	O
,	O
where	O
relations	O
:	O
	O
ref	O
goal	O
ref	O
	O
goal	O
	O
goal	O
ref	O
and	O
where	O
values	O
required	O
for	O
successful	O
control	O
,	O
and	O
#	O
	O
	O
	O
goal	O
	O
!	O
	O
$	O
#	O
&	O
%	O
ref	O
denote	O
reference	O
values	O
to	O
be	O
reached	O
,	O
	O
ref	O
(	O
13.1	O
)	O
(	O
13.2	O
)	O
goal	O
and	O
	O
(	O
13.3	O
)	O
(	O
13.4	O
)	O
	O
goal	O
denote	O
goal	O
denotes	O
a	O
monotonically	O
increasing	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
''	O
	O
'	O
	O
	O
%	O
'	O
250	O
learning	O
to	O
control	O
dynamic	O
systems	O
[	O
ch	O
.	O
13	O
be	O
simpliﬁed	O
and	O
normalised	O
,	O
resulting	O
in	O
function	O
passing	O
through	O
the	O
point	O
)	O
(	O
+*	O
,	O
(	O
-	O
.	O
when	O
the	O
system	O
is	O
to	O
be	O
controlled	O
under	O
the	O
bang-bang	O
regime	O
,	O
control	O
action	O
.	O
is	O
determined	O
by	O
the	O
sign	O
of	O
force	O
''	O
.	O
6587	O
assuming	O
(	O
without	O
loss	O
of	O
generality	O
,	O
equations	O
(	O
13.1	O
)	O
–	O
(	O
13.4	O
)	O
can	O
	O
;	O
9	O
9	O
*	O
?	O
>	O
where/=	O
if	O
''	O
(	O
and	O
ref	O
:9	O
sign	O
	O
<	O
*	O
*a	O
@	O
b	O
@	O
a	O
@	O
,	O
*	O
,	O
c	O
are	O
numerical	O
parameters	O
.	O
both	O
makaroviˇc	O
’	O
s	O
and	O
bratko	O
’	O
s	O
rule	O
successfully	O
control	O
the	O
inverted	O
pendulum	O
,	O
provided	O
the	O
appropriate	O
values	O
of	O
the	O
numerical	O
parameters	O
are	O
chosen	O
.	O
moreover	O
,	O
there	O
exists	O
a	O
set	O
of	O
parameter	O
values	O
that	O
makes	O
bratko	O
’	O
s	O
rule	O
equivalent	O
to	O
the	O
bang-bang	O
variant	O
of	O
a	O
classical	O
control	O
rule	O
using	O
the	O
sign	O
of	O
pole-placement	O
controller	O
output	O
(	O
dˇzeroski	O
,	O
1989	O
)	O
.	O
then	O
.	O
0/2143	O
else	O
.	O
:	O
(	O
13.5	O
)	O
13.3	O
learning	O
to	O
control	O
from	O
scratch	O
:	O
boxes	O
in	O
learning	O
approaches	O
,	O
trials	O
are	O
performed	O
in	O
order	O
to	O
gain	O
experimental	O
evidence	O
about	O
different	O
control	O
decisions	O
.	O
a	O
trial	O
starts	O
with	O
the	O
system	O
positioned	O
in	O
an	O
initial	O
state	O
chosen	O
from	O
a	O
speciﬁed	O
region	O
,	O
and	O
lasts	O
until	O
failure	O
occurs	O
or	O
successful	O
control	O
is	O
performed	O
for	O
a	O
prescribed	O
maximal	O
period	O
of	O
time	O
.	O
failure	O
occurs	O
when	O
the	O
cart	O
position	O
or	O
pole	O
inclination	O
exceeds	O
the	O
given	O
boundaries	O
.	O
the	O
duration	O
of	O
a	O
trial	O
is	O
called	O
survival	O
time	O
.	O
learning	O
is	O
carried	O
out	O
by	O
performing	O
trials	O
repeatedly	O
until	O
a	O
certain	O
success	O
criterion	O
is	O
met	O
.	O
typically	O
,	O
this	O
criterion	O
requires	O
successful	O
control	O
within	O
a	O
trial	O
to	O
exceed	O
a	O
prescribed	O
period	O
of	O
time	O
.	O
initial	O
control	O
decisions	O
are	O
usually	O
random	O
.	O
on	O
the	O
basis	O
of	O
experimental	O
evidence	O
,	O
they	O
are	O
evaluated	O
and	O
possibly	O
changed	O
,	O
thus	O
improving	O
control	O
quality	O
.	O
this	O
basic	O
idea	O
has	O
been	O
implemented	O
in	O
many	O
different	O
ways	O
,	O
for	O
example	B
in	O
boxes	O
(	O
michie	O
&	O
chambers	O
,	O
1968	O
)	O
,	O
adaptive	O
critic	O
reinforcement	O
method	O
(	O
barto	O
et	O
al.	O
,	O
1983	O
)	O
,	O
cart	O
(	O
connell	O
&	O
utgoff	O
,	O
1987	O
)	O
,	O
multilayer	O
connectionist	O
approach	O
(	O
anderson	O
,	O
1987	O
)	O
and	O
many	O
others	O
.	O
geva	O
and	O
sitte	O
(	O
1993a	O
)	O
provide	O
an	O
exhaustive	O
review	O
.	O
here	O
,	O
two	O
methods	O
will	O
be	O
described	O
in	O
more	O
detail	O
:	O
boxes	O
(	O
michie	O
&	O
chambers	O
,	O
1968	O
)	O
and	O
genetic	O
learning	O
of	O
control	O
(	O
varˇsek	O
et	O
al.	O
,	O
1993	O
)	O
.	O
the	O
choice	O
of	O
methods	O
presented	O
here	O
is	O
subjective	O
.	O
it	O
was	O
guided	O
by	O
our	O
aim	O
to	O
describe	O
recent	O
efforts	O
in	O
changing	O
or	O
upgrading	O
the	O
original	O
ideas	O
.	O
we	O
chose	O
boxes	O
because	O
it	O
introduced	O
a	O
learning	O
scheme	O
that	O
was	O
inspirational	O
to	O
much	O
further	O
work	O
.	O
13.3.1	O
boxes	O
the	O
boxes	O
program	O
(	O
michie	O
&	O
chambers	O
,	O
1968	O
)	O
learns	O
a	O
state-action	O
table	O
,	O
i.e	O
.	O
a	O
set	O
of	O
rules	O
that	O
specify	O
action	O
to	O
be	O
applied	O
to	O
the	O
system	O
in	O
a	O
given	O
state	O
.	O
of	O
course	O
this	O
would	O
not	O
be	O
possible	O
for	O
the	O
original	O
,	O
inﬁnite	O
state	O
space	O
.	O
therefore	O
,	O
the	O
state	O
space	O
is	O
divided	O
into	O
“	O
boxes	O
”	O
.	O
a	O
box	O
is	O
deﬁned	O
as	O
the	O
cartesian	O
product	O
of	O
the	O
values	O
of	O
the	O
system	O
variables	O
,	O
where	O
all	O
the	O
values	O
belong	O
to	O
an	O
interval	O
from	O
a	O
predeﬁned	O
partition	O
.	O
a	O
typical	O
,	O
,	O
giving	O
162	O
boxes	O
.	O
all	O
the	O
points	O
within	O
a	O
box	O
are	O
mapped	O
to	O
the	O
same	O
control	O
decision	O
.	O
during	O
one	O
trial	O
,	O
the	O
state-action	O
table	O
is	O
ﬁxed	O
.	O
when	O
a	O
failure	O
is	O
detected	O
a	O
trial	O
ends	O
.	O
decisions	O
are	O
evaluated	O
with	O
respect	O
to	O
the	O
accumulated	O
numeric	O
information	O
:	O
partition	O
of	O
the	O
four	O
dimensional	O
state	O
space	O
into	O
boxes	O
distinguish	O
3	O
values	O
of	O
6	O
of	O
and	O
3	O
of	O
	O
,	O
3	O
of	O
	O
	O
(	O
	O
''	O
	O
/	O
	O
/	O
	O
	O
/	O
!	O
	O
	O
	O
	O
sec	O
.	O
13.3	O
]	O
learning	O
to	O
control	O
from	O
scratch	O
:	O
boxes	O
251	O
how	O
many	O
times	O
the	O
system	O
entered	O
a	O
particular	O
state	O
,	O
how	O
successful	O
it	O
was	O
after	O
particular	O
decisions	O
,	O
etc	O
.	O
the	O
following	O
information	O
is	O
accumulated	O
for	O
each	O
box	O
:	O
:	O
“	O
left	O
life	O
”	O
,	O
weighted	O
sum	O
of	O
survival	O
times	O
after	O
left	O
decision	O
was	O
taken	O
in	O
this	O
state	O
:	O
“	O
right	O
life	O
”	O
,	O
the	O
same	O
for	O
the	O
right	O
decision	O
,	O
:	O
“	O
left	O
usage	O
”	O
,	O
weighted	O
sum	O
of	O
the	O
number	O
of	O
left	O
decisions	O
taken	O
in	O
this	O
state	O
during	O
previous	O
trials	O
,	O
during	O
previous	O
trials	O
,	O
ded	O
fgd	O
dih	O
f	O
:	O
h	O
trial	O
.	O
j2k	O
the	O
program	O
chooses	O
the	O
action	O
with	O
the	O
higher	O
estimate	O
to	O
be	O
applied	O
in	O
the	O
box	O
during	O
the	O
performance	O
of	O
boxes	O
is	O
generally	O
described	O
by	O
the	O
number	O
of	O
trials	O
needed	O
for	O
ﬁrst	O
achieving	O
10	O
000	O
step	O
survival	O
.	O
figures	O
vary	O
considerably	O
from	O
paper	O
to	O
paper	O
and	O
are	O
between	O
84	O
(	O
geva	O
&	O
sitte	O
,	O
1993b	O
)	O
and	O
557	O
(	O
sammut	O
,	O
1994	O
)	O
.	O
although	O
interesting	O
,	O
these	O
ﬁgures	O
are	O
not	O
sufﬁcient	O
to	O
validate	O
the	O
learning	O
results	O
.	O
reliability	O
,	O
robustness	O
and	O
characteristics	O
of	O
the	O
controller	O
performance	O
are	O
important	O
as	O
well	O
and	O
are	O
discussed	O
in	O
many	O
papers	O
devoted	O
to	O
boxes	O
.	O
:	O
times	O
(	O
i.e	O
.	O
steps	O
)	O
at	O
which	O
the	O
system	O
enters	O
this	O
state	O
during	O
the	O
current	O
after	O
a	O
trial	O
the	O
program	O
updates	O
these	O
ﬁgures	O
.	O
for	O
the	O
states	O
in	O
which	O
decision	O
“	O
left	O
”	O
was	O
taken	O
,	O
the	O
new	O
values	O
are	O
:	O
j2y	O
=xw	O
9^	O
]	O
:	O
“	O
right	O
usage	O
”	O
,	O
the	O
same	O
for	O
right	O
decisions	O
,	O
where	O
the	O
meaning	O
of	O
the	O
parameters	O
is	O
as	O
follows	O
:	O
:	O
number	O
of	O
entries	O
into	O
the	O
state	O
during	O
the	O
run	O
,	O
dlder-sut	O
dzh\r-sut	O
fgder	O
sut	O
f	O
:	O
h\r	O
sut	O
*a	O
@	O
b	O
@	O
a	O
@	O
,	O
*	O
dldnmpo	O
)	O
q	O
dzh	O
[	O
mpo	O
)	O
q	O
fgdnm+o_q	O
mpo	O
)	O
q	O
f	O
:	O
h	O
sut	O
:	O
constant	O
that	O
weighs	O
recent	O
experience	O
relative	O
to	O
earlier	O
experience	O
for	O
the	O
whole	O
system	O
,	O
b	O
(	O
“	O
global	O
life	O
”	O
)	O
andb	O
mpo_q	O
der	O
sut	O
each	O
trial	O
:	O
b	O
m+o_q	O
h\r4sut	O
ofbg	O
ced	O
dih	O
ci	O
f	O
:	O
h	O
=kjml	O
analogous	O
updates	O
are	O
made	O
for	O
the	O
states	O
with	O
decision	O
“	O
right	O
”	O
.	O
is	O
constant	O
that	O
weighs	O
global	O
experience	O
relative	O
to	O
local	O
experience	O
.	O
deh	O
dlh	O
	O
)	O
b	O
	O
)	O
b	O
:	O
ﬁnishing	O
time	O
of	O
the	O
trial	O
.	O
dld	O
fgd	O
wheret	O
the	O
next	O
trial	O
.	O
these	O
values	O
are	O
used	O
for	O
a	O
numeric	O
evaluation	O
of	O
the	O
success	O
for	O
both	O
actions	O
.	O
the	O
estimates	O
are	O
computed	O
after	O
a	O
trial	O
for	O
each	O
qualitative	O
state	O
:	O
suta`	O
	O
,	O
(	O
“	O
global	O
usage	O
”	O
)	O
are	O
computed	O
after	O
j	O
	O
*	O
j	O
	O
	O
9	O
k	O
v	O
	O
	O
j	O
=	O
	O
	O
	O
	O
]	O
j	O
y	O
d	O
h	O
d	O
	O
b	O
9	O
j	O
y	O
b	O
h	O
	O
b	O
9	O
	O
9	O
t	O
b	O
h	O
	O
9	O
t	O
g	O
	O
9	O
t	O
b	O
h	O
	O
9	O
t	O
252	O
learning	O
to	O
control	O
dynamic	O
systems	O
[	O
ch	O
.	O
13	O
variablesded	O
(	O
left	O
action	O
lifetime	O
)	O
,	O
dih	O
13.3.2	O
reﬁnements	O
of	O
boxes	O
sammut	O
(	O
1994	O
)	O
describes	O
some	O
recent	O
reﬁnements	O
of	O
the	O
basic	O
michie-chambers	O
learning	O
scheme	O
.	O
the	O
central	O
mechanism	O
of	O
learning	O
in	O
boxes	O
is	O
the	O
decision	O
rule	O
based	O
on	O
the	O
“	O
experience	O
”	O
of	O
each	O
box	O
.	O
the	O
experience	O
for	O
each	O
individual	O
box	O
is	O
accumulated	O
in	O
the	O
(	O
same	O
for	O
the	O
right	O
action	O
)	O
.	O
the	O
michie-chambers	O
rule	O
determines	O
the	O
decision	O
between	O
left	O
and	O
right	O
action	O
depending	O
on	O
these	O
variables	O
.	O
the	O
rule	O
is	O
designed	O
so	O
that	O
it	O
combines	O
two	O
,	O
possibly	O
conﬂicting	O
interests	O
:	O
exploitation	O
and	O
exploration	O
.	O
the	O
ﬁrst	O
is	O
to	O
perform	O
the	O
action	O
that	O
in	O
the	O
past	O
produced	O
the	O
best	O
results	O
(	O
that	O
is	O
maximum	O
lifetime	O
)	O
,	O
and	O
the	O
second	O
is	O
to	O
explore	O
the	O
alternatives	O
.	O
the	O
alternatives	O
may	O
in	O
the	O
future	O
turn	O
out	O
in	O
fact	O
to	O
be	O
superior	O
to	O
what	O
appears	O
to	O
be	O
best	O
at	O
present	O
.	O
(	O
left	O
action	O
usage	O
)	O
,	O
fgd	O
andf	O
:	O
h	O
the	O
original	O
michie-chambers	O
formulas	O
ﬁnd	O
a	O
particular	O
compromise	O
between	O
these	O
two	O
interests	O
.	O
the	O
compromise	O
can	O
be	O
adjusted	O
by	O
varying	O
the	O
parameters	O
in	O
the	O
formulas	O
.	O
sammut	O
(	O
1994	O
)	O
describes	O
a	O
series	O
of	O
modiﬁcations	O
of	O
the	O
original	O
michie-chambers	O
rule	O
.	O
the	O
following	O
elegant	O
rule	O
(	O
named	O
after	O
law	O
&	O
sammut	O
)	O
experimentally	O
performed	O
the	O
best	O
in	O
terms	O
of	O
learning	O
rate	O
and	O
stability	O
of	O
learning	O
:	O
f\dlh-f	O
:	O
h	O
$	O
n	O
dedeh	O
dih	O
$	O
n	O
if	O
an	O
action	O
has	O
not	O
been	O
tested	O
then	O
choose	O
that	O
action	O
is	O
a	O
user	O
deﬁned	O
parameter	O
that	O
adjusts	O
the	O
relative	O
importance	O
of	O
exploitation	O
and	O
is	O
1.	O
this	O
corresponds	O
to	O
pure	O
exploitation	O
,	O
the	O
system	O
’	O
s	O
mentality	O
changes	O
towards	O
experimentalist	O
.	O
then	O
the	O
system	O
is	O
willing	O
to	O
experiment	O
with	O
actions	O
that	O
from	O
past	O
experience	O
look	O
inferior	O
.	O
else	O
ifdedeh-dzh	O
$	O
n	O
else	O
iff\dlh-f	O
:	O
h	O
$	O
n	O
exploration	O
.	O
the	O
lowest	O
reasonable	O
value	O
fort	O
without	O
any	O
desire	O
to	O
explore	O
the	O
untested	O
.	O
by	O
increasingt	O
a	O
suitable	O
compromise	O
fort	O
pole-and-cart	O
problem	O
,	O
it	O
was	O
experimentally	O
found	O
thatt	O
rate	O
is	O
relatively	O
stable	O
for	O
values	O
oft	O
whereas	O
the	O
law	O
&	O
sammut	O
rule	O
needed	O
75	O
trials	O
(	O
witht	O
@	O
po	O
)	O
.	O
in	O
trying	O
to	O
test	O
the	O
stability	O
of	O
the	O
law	O
&	O
sammut	O
rule	O
,	O
it	O
was	O
found	O
thatt	O
was	O
slightly	O
,	O
but	O
not	O
signiﬁcantly	O
,	O
is	O
needed	O
for	O
overall	O
good	O
performance	O
.	O
for	O
the	O
classical	O
is	O
optimal	O
.	O
the	O
learning	O
between	O
1.4	O
and	O
1.8	O
,	O
and	O
it	O
degrades	O
rapidly	O
when	O
decreases	O
below	O
1.4	O
or	O
increases	O
above	O
1.8.	O
the	O
following	O
improvement	O
of	O
the	O
law	O
&	O
sammut	O
rule	O
with	O
respect	O
to	O
the	O
michie	O
&	O
chambers	O
rule	O
was	O
reported	O
:	O
on	O
the	O
average	O
over	O
20	O
experiments	O
,	O
the	O
original	O
boxes	O
needed	O
557	O
trials	O
to	O
learn	O
to	O
control	O
the	O
system	O
,	O
sensitive	O
to	O
small	O
changes	O
in	O
the	O
learning	O
problem	O
,	O
such	O
as	O
changing	O
the	O
number	O
of	O
boxes	O
from	O
162	O
to	O
225	O
,	O
or	O
introducing	O
asymmetry	O
in	O
the	O
force	O
(	O
left	O
push	O
twice	O
the	O
right	O
push	O
)	O
.	O
else	O
choose	O
an	O
action	O
at	O
random	O
then	O
choose	O
left	O
then	O
choose	O
right	O
geva	O
and	O
sitte	O
(	O
1993a	O
)	O
carried	O
out	O
exhaustive	O
experiments	O
concerning	O
the	O
same	O
topic	O
.	O
with	O
the	O
appropriate	O
parameter	O
setting	O
the	O
boxes	O
method	O
performed	O
as	O
well	O
as	O
the	O
adaptive	O
critic	O
reinforcement	O
learning	O
(	O
barto	O
et	O
al.	O
,	O
1983	O
)	O
.	O
they	O
got	O
an	O
average	O
of	O
52	O
trials	O
out	O
of	O
1000	O
learning	O
experiments	O
(	O
standard	O
deviation	O
was	O
32	O
)	O
.	O
13.4	O
learning	O
to	O
control	O
from	O
scratch	O
:	O
genetic	O
learning	O
genetic	B
algorithms	I
(	O
gas	O
)	O
are	O
loosely	O
based	O
on	O
darwinian	O
principles	O
of	O
evolution	O
:	O
repro-	O
duction	O
,	O
genetic	O
recombination	O
,	O
and	O
the	O
“	O
survival	O
of	O
the	O
ﬁttest	O
”	O
(	O
holland	O
,	O
1975	O
;	O
goldberg	O
,	O
1989	O
)	O
.	O
they	O
maintain	O
a	O
set	O
of	O
candidate	O
solutions	O
called	O
a	O
population	O
.	O
candidate	O
solutions	O
	O
	O
t	O
	O
@	O
o	O
t	O
	O
sec	O
.	O
13.4	O
]	O
learning	O
to	O
control	O
from	O
scratch	O
:	O
genetic	O
learning	O
253	O
are	O
usually	O
represented	O
as	O
binary	O
coded	O
strings	O
of	O
ﬁxed	O
length	O
.	O
the	O
initial	O
population	O
is	O
generated	O
at	O
random	O
.	O
what	O
happens	O
during	O
cycles	O
called	O
generations	O
is	O
as	O
follows	O
.	O
each	O
member	O
of	O
the	O
population	O
is	O
evaluated	O
using	O
a	O
ﬁtness	O
function	O
.	O
after	O
that	O
,	O
the	O
population	O
undergoes	O
reproduction	O
.	O
parents	O
are	O
chosen	O
stochastically	O
,	O
but	O
strings	O
with	O
a	O
higher	O
value	O
of	O
ﬁtness	O
function	O
have	O
higher	O
probability	O
of	O
contributing	O
an	O
offspring	O
.	O
genetic	O
operators	O
,	O
such	O
as	O
crossover	O
and	O
mutation	O
,	O
are	O
applied	O
to	O
parents	O
to	O
produce	O
offspring	O
.	O
a	O
subset	O
of	O
the	O
population	O
is	O
replaced	O
by	O
the	O
offspring	O
,	O
and	O
the	O
process	O
continues	O
on	O
this	O
new	O
genera-	O
tion	B
.	O
through	O
recombination	O
and	O
selection	O
,	O
the	O
evolution	O
converges	O
to	O
highly	O
ﬁt	O
population	O
members	O
representing	O
near-optimal	O
solutions	O
to	O
the	O
considered	O
problem	O
.	O
when	O
controllers	O
are	O
to	O
be	O
built	O
without	O
having	O
an	O
accurate	O
mathematical	O
model	O
of	O
the	O
system	O
to	O
be	O
controlled	O
,	O
two	O
problems	O
arise	O
:	O
ﬁrst	O
,	O
how	O
to	O
establish	O
the	O
structure	O
of	O
the	O
controller	O
,	O
and	O
second	O
,	O
how	O
to	O
choose	O
numerical	O
values	O
for	O
the	O
controller	O
parameters	O
.	O
in	O
the	O
following	O
,	O
we	O
present	O
a	O
three-stage	O
framework	O
proposed	O
by	O
varˇsek	O
et	O
al	O
.	O
(	O
cq	O
1993	O
ieee	O
)	O
.	O
first	O
,	O
control	O
rules	O
,	O
represented	O
as	O
tables	O
,	O
are	O
obtained	O
without	O
prior	O
knowledge	O
about	O
the	O
system	O
to	O
be	O
controlled	O
.	O
next	O
,	O
if-then	O
rules	O
are	O
synthesized	O
by	O
structuring	O
information	O
encoded	O
in	O
the	O
tables	O
,	O
yielding	O
comprehensible	O
control	O
knowledge	O
.	O
this	O
control	O
knowledge	O
has	O
adequate	O
structure	O
,	O
but	O
it	O
may	O
be	O
non-operational	O
because	O
of	O
inadequate	O
settings	O
of	O
its	O
numerical	O
parameters	O
.	O
control	O
knowledge	O
is	O
ﬁnally	O
made	O
operational	O
by	O
ﬁne-tuning	O
numerical	O
parameters	O
that	O
are	O
part	O
of	O
this	O
knowledge	O
.	O
the	O
same	O
ﬁne-tuning	O
mechanism	O
can	O
also	O
be	O
applied	O
when	O
available	O
partial	O
domain	B
knowledge	I
sufﬁces	O
to	O
determine	O
the	O
structure	O
of	O
a	O
control	O
rule	O
in	O
advance	O
.	O
in	O
this	O
approach	O
,	O
the	O
control	O
learning	O
process	O
is	O
considered	O
to	O
be	O
an	O
instance	O
of	O
a	O
combinatorial	O
optimisation	B
problem	O
.	O
in	O
contrast	O
to	O
the	O
previously	O
described	O
learning	O
approach	O
in	O
boxes	O
,	O
where	O
the	O
goal	O
is	O
to	O
maximise	O
survival	O
time	O
,	O
here	O
the	O
goal	O
is	O
to	O
maximise	O
survival	O
time	O
,	O
and	O
,	O
simultaneously	O
,	O
to	O
minimise	O
the	O
discrepancy	O
between	O
the	O
desired	O
and	O
actual	O
system	O
behaviour	O
.	O
this	O
criterion	O
is	O
embodied	O
in	O
a	O
cost	O
function	O
,	O
called	O
the	O
raw	O
ﬁtness	O
function	O
,	O
used	O
to	O
evaluate	O
candidate	O
control	O
rules	O
during	O
the	O
learning	O
process	O
.	O
|g	O
}	O
4	O
}	O
{	O
z	O
y	O
max	O
|g	O
}	O
4	O
}	O
max9	O
	O
	O
mw	O
is	O
calculated	O
as	O
follows	O
:	O
raw	O
ﬁtnessrtsvup	O
(	O
*	O
|g	O
}	O
4	O
}	O
|g	O
}	O
4	O
}	O
@	O
b	O
@	O
a	O
@	O
	O
max*	O
	O
	O
|	O
}	O
4	O
}	O
is	O
the	O
normalised	O
survival	O
time	O
,	O
z	O
is	O
the	O
normalised	O
error	O
,	O
]	O
wherex	O
trials	O
performed	O
to	O
evaluate	O
a	O
candidate	O
solution	O
,	O
y	O
is	O
the	O
survival	O
time	O
in	O
the	O
-th	O
trial	O
,	O
y	O
max	O
is	O
the	O
maximal	O
duration	O
of	O
a	O
trial	O
,	O
and|	O
}	O
4	O
}	O
is	O
the	O
cumulative	O
error	O
of	O
the	O
-th	O
trial	O
.	O
after	O
completing	O
the	O
learning	O
process	O
,	O
solutions	O
were	O
thoroughly	O
evaluated	O
by	O
per-	O
forming	O
100	O
trials	O
with	O
maximal	O
duration	O
of	O
a	O
trial	O
set	O
to	O
1	O
000	O
000	O
steps	O
,	O
corresponding	O
to	O
over	O
5.5	O
hours	O
of	O
simulated	O
time	O
.	O
note	O
that	O
the	O
maximal	O
duration	O
of	O
a	O
trial	O
most	O
frequently	O
found	O
in	O
the	O
ai	O
literature	O
is	O
200	O
seconds	O
.	O
4	O
=xw	O
is	O
the	O
number	O
of	O
r	O
	O
x	O
y	O
	O
	O
x	O
y	O
	O
]	O
k	O
v	O
~	O
w	O
	O
y	O
~	O
z	O
	O
]	O
k	O
v	O
~	O
w	O
	O
~	O
y	O
~	O
~	O
	O
v	O
	O
=	O
	O
	O
	O
=	O
	O
]	O
*	O
y	O
~	O
~	O
254	O
learning	O
to	O
control	O
dynamic	O
systems	O
[	O
ch	O
.	O
13	O
phase	O
1	O
:	O
obtaining	O
control	O
without	O
prior	O
knowledge	O
and	O
	O
during	O
this	O
phase	O
,	O
boxes-like	O
decision	O
rules	O
were	O
learned	O
.	O
for	O
each	O
of	O
the	O
pole-cart	O
variables	O
,	O
	O
and/e143	O
.	O
each	O
decision	O
rule	O
is	O
then	O
represented	O
as	O
a	O
four-dimensional	O
array	O
,	O
where	O
each	O
,	O
the	O
domain	O
is	O
partitioned	O
into	O
three	O
labelled	O
intervals587	O
entry	O
represents	O
a	O
control	O
action	O
.	O
in	O
addition	O
,	O
two	O
partitioning	O
thresholds	O
are	O
required	O
for	O
each	O
system	O
variable	O
.	O
candidate	O
solutions	O
,	O
comprising	O
a	O
decision	O
rule	O
along	O
with	O
the	O
corresponding	O
thresholds	O
,	O
are	O
represented	O
as	O
binary	O
strings	O
.	O
to	O
calculate	O
a	O
ﬁtness	O
value	O
for	O
each	O
individual	O
,	O
25	O
trials	O
were	O
carried	O
out	O
with	O
the	O
maximal	O
duration	O
of	O
a	O
trial	O
set	O
to	O
5000	O
steps	O
.	O
populations	O
of	O
size	O
100	O
were	O
observed	O
for	O
60	O
generations	O
.	O
the	O
experiment	O
was	O
repeated	O
ten	O
times	O
.	O
on	O
average	O
,	O
after	O
about	O
30	O
generations	O
,	O
individualsrepresenting	O
rules	O
better	O
than	O
makaroviˇc	O
’	O
s	O
phase	O
2	O
:	O
inducing	O
rule	O
structure	O
discovered	O
.	O
	O
rule	O
were	O
action	O
(	O
i.e	O
.	O
positive	O
or	O
negative	O
control	O
force	O
)	O
.	O
the	O
obtained	O
rules	O
are	O
very	O
close	O
in	O
form	O
to	O
makaroviˇc	O
’	O
s	O
rule	O
.	O
from	O
the	O
rules	O
shown	O
to	O
automatically	O
synthesize	O
comprehensible	O
rules	O
obtained	O
during	O
phase	O
1	O
,	O
an	O
inductive	B
learning	I
technique	O
was	O
employed	O
.	O
a	O
derivative	O
of	O
the	O
cn2	O
algorithm	O
(	O
clark	O
&	O
niblett	O
,	O
1988	O
)	O
,	O
named	O
ginesys	O
pc	O
(	O
karaliˇc	O
&	O
gams	O
,	O
1989	O
)	O
,	O
was	O
used	O
to	O
compress	O
the	O
ga-induced	O
boxes-like	O
rules	O
into	O
the	O
if-then	O
form	O
.	O
the	O
learning	O
domain	O
for	O
the	O
compression	O
phase	O
was	O
described	O
in	O
terms	O
of	O
four	O
attributes	O
and	O
the	O
class	O
.	O
the	O
attribute	O
values	O
were	O
interval	O
,	O
and	O
the	O
class	O
represented	O
the	O
corresponding	O
labels	O
for	O
the	O
pole-cart	O
variables	O
by	O
dˇzeroski	O
(	O
1989	O
)	O
to	O
successfully	O
control	O
the	O
pole-cart	O
system	O
,	O
rules	O
	O
	O
and	O
	O
	O
,	O
	O
were	O
discovered	O
automatically	O
.	O
the	O
performance	O
of	O
the	O
compressed	O
rules	O
decreased	O
with	O
respect	O
to	O
the	O
original	O
ga-induced	O
boxes-like	O
rules	O
due	O
to	O
inaccurate	O
interpretation	O
of	O
the	O
interval	O
labels	O
.	O
as	O
in	O
the	O
case	O
of	O
table	O
13.1	O
,	O
the	O
100	O
%	O
failure	O
rate	O
of	O
the	O
compressed	O
rule	O
indicates	O
that	O
this	O
rule	O
was	O
never	O
able	O
to	O
balance	O
the	O
system	O
for	O
1	O
000	O
000	O
steps	O
.	O
since	O
the	O
deﬁning	O
thresholds	O
were	O
learned	O
during	O
phase	O
1	O
to	O
perform	O
well	O
with	O
the	O
original	O
ga-induced	O
rules	O
,	O
these	O
thresholds	O
should	O
be	O
adapted	O
to	O
suit	O
the	O
new	O
compressed	O
rules	O
.	O
	O
,	O
and	O
phase	O
3	O
:	O
fine-tuning	O
by	O
optimizing	O
control	O
performance	O
in	O
phase	O
3	O
,	O
the	O
interpretation	O
of	O
symbolic	O
values	O
,	O
i.e	O
.	O
and	O
left	O
unchanged	O
throughout	O
the	O
optimisation	B
process	O
.	O
interval	O
labels	O
,	O
appearing	O
in	O
the	O
found	O
in	O
phase	O
2	O
was	O
adjusted	O
to	O
maximise	O
the	O
control	O
quality	O
.	O
for	O
this	O
purpose	O
,	O
a	O
ga	O
was	O
employed	O
again	O
.	O
this	O
time	O
,	O
each	O
chromosome	O
qualitative	O
rule	O
represented	O
four	O
binary	O
coded	O
thresholds	O
while	O
the	O
rule	O
structure	O
was	O
set	O
to	O
to	O
calculate	O
a	O
ﬁtness	O
value	O
for	O
each	O
individual	O
,	O
only	O
15	O
trials	O
were	O
carried	O
out	O
with	O
maximal	O
duration	O
of	O
a	O
trial	O
set	O
to	O
2000	O
steps	O
.	O
populations	O
of	O
size	O
50	O
were	O
evolved	O
for	O
30	O
generations	O
.	O
after	O
30	O
generations	O
,	O
individuals	O
representing	O
rules	O
better	O
than	O
those	O
obtained	O
during	O
phase	O
1	O
were	O
generated	O
.	O
through	O
the	O
extensive	O
evaluation	O
,	O
the	O
ﬁne-tuned	O
rules	O
were	O
shown	O
reliable	O
(	O
see	O
results	O
in	O
table	O
13.1	O
)	O
.	O
additional	O
experiments	O
were	O
carried	O
out	O
.	O
the	O
robustness	O
of	O
learning	O
“	O
from	O
scratch	O
”	O
was	O
13.4.1	O
robustness	O
and	O
adaptation	O
tested	O
by	O
performing	O
the	O
experiment	O
twice	O
:	O
ﬁrst	O
,	O
with	O
force	O
''	O
s	O
(	O
n*	O
(	O
n	O
,	O
and	O
*	O
	O
*	O
	O
	O
7	O
}	O
1	O
	O
	O
*	O
	O
*	O
	O
*	O
*	O
	O
*	O
	O
	O
*	O
	O
	O
*	O
*	O
	O
	O
	O
*	O
	O
*	O
*	O
	O
	O
*	O
	O
	O
*	O
	O
*	O
	O
*	O
	O
	O
*	O
*	O
	O
	O
	O
*	O
	O
	O
*	O
*	O
	O
	O
9	O
sec	O
.	O
13.5	O
]	O
exploiting	O
partial	O
explicit	O
knowledge	O
table	O
13.1	O
:	O
(	O
cq	O
1993	O
ieee	O
)	O
control	O
performance	O
of	O
ga-induced	O
boxes-like	O
rule	O
,	O
com-	O
pressed	O
rule	O
	O
,	O
and	O
the	O
original	O
makaroviˇc	O
’	O
s	O
rule	O
	O
,	O
ﬁne-tuned	O
rule	O
failures	O
avg	O
.	O
survival	O
	O
.	O
255	O
rule	O
[	O
%	O
]	O
time	O
[	O
steps	O
]	O
ga-based	O
compressed	O
fine-tuned	O
makaroviˇc	O
’	O
s	O
4	O
100	O
0	O
0	O
978	O
149	O
9	O
290	O
1	O
000	O
000	O
1	O
000	O
000	O
fitness	O
0.9442	O
0.0072	O
0.9630	O
0.8857	O
performing	O
two	O
further	O
ﬁne-tuning	O
experiments	O
.	O
second	O
,	O
with	O
asymmetrical	O
force	O
''	O
	O
n	O
.	O
the	O
possibility	O
of	O
adaptation	O
of	O
s	O
(	O
n*	O
	O
obtained	O
for	O
symmetrical	O
force	O
''	O
s	O
the	O
qualitative	O
rule	O
n	O
(	O
n	O
and	O
''	O
the	O
new	O
conditions	O
,	O
''	O
	O
n	O
,	O
was	O
examined	O
by	O
(	O
n*	O
(	O
n*	O
s	O
table	O
13.2	O
:	O
(	O
cq	O
1993	O
ieee	O
)	O
control	O
performance	O
of	O
ga-induced	O
boxes-like	O
rules	O
for	O
$	O
	O
n	O
,	O
and	O
rule	O
(	O
n*	O
s	O
	O
ﬁne-tuned	O
for	O
	O
n	O
.	O
s	O
(	O
n*	O
table	O
13.2	O
shows	O
the	O
performance	O
of	O
four	O
rules	O
obtained	O
in	O
these	O
experiments	O
.	O
it	O
can	O
be	O
seen	O
that	O
gas	O
can	O
successfully	O
learn	O
to	O
control	O
the	O
pole-cart	O
system	O
also	O
under	O
modiﬁed	O
conditions	O
.	O
s	O
(	O
n	O
and	O
''	O
(	O
n	O
and	O
''	O
failures	O
avg	O
.	O
survival	O
(	O
n*	O
(	O
n*	O
s	O
s	O
n*	O
to	O
rule	O
[	O
%	O
]	O
time	O
[	O
steps	O
]	O
ga+10–10	O
ga+10–5	O
tuned+10–10	O
tuned+10–5	O
0	O
44	O
0	O
0	O
1	O
000	O
000	O
665	O
772	O
1	O
000	O
000	O
1	O
000	O
000	O
fitness	O
0.9222	O
0.5572	O
0.9505	O
0.9637	O
to	O
summarise	O
,	O
successful	O
and	O
comprehensible	O
control	O
rules	O
were	O
synthesized	O
automat-	O
ically	O
in	O
three	O
phases	O
.	O
here	O
,	O
a	O
remark	O
should	O
be	O
made	O
about	O
the	O
number	O
of	O
performed	O
trials	O
.	O
in	O
this	O
research	O
,	O
it	O
was	O
very	O
high	O
due	O
to	O
the	O
following	O
reasons	O
.	O
first	O
,	O
the	O
emphasis	O
was	O
put	O
on	O
the	O
reliability	O
of	O
learned	O
rules	O
and	O
this	O
,	O
of	O
course	O
,	O
demands	O
much	O
more	O
experimentation	O
in	O
order	O
to	O
ensure	O
good	O
performance	O
on	O
a	O
wide	O
range	O
of	O
initial	O
states	O
.	O
in	O
our	O
recent	O
ex-	O
periments	O
with	O
a	O
more	O
narrow	O
range	O
of	O
initial	O
states	O
the	O
number	O
of	O
trials	O
was	O
considerably	O
reduced	O
without	O
affecting	O
the	O
reliability	O
.	O
second	O
,	O
the	O
performance	O
of	O
the	O
rules	O
after	O
the	O
ﬁrst	O
phase	O
was	O
practically	O
the	O
same	O
as	O
that	O
of	O
the	O
rules	O
after	O
the	O
third	O
phase	O
.	O
maybe	O
the	O
same	O
controller	O
structure	O
could	O
be	O
obtained	O
in	O
the	O
second	O
phase	O
from	O
less	O
perfect	O
rules	O
.	O
however	O
,	O
it	O
is	O
difﬁcult	O
to	O
know	O
when	O
the	O
learned	O
evidence	O
sufﬁces	O
.	O
to	O
conclude	O
,	O
the	O
exhaustiveness	O
of	O
these	O
experiments	O
was	O
conciously	O
accepted	O
by	O
the	O
authors	O
in	O
order	O
to	O
show	O
that	O
100	O
%	O
reliable	O
rules	O
can	O
be	O
learned	O
from	O
scratch	O
.	O
13.5	O
exploiting	O
partial	O
explicit	O
knowledge	O
13.5.1	O
boxes	O
with	O
partial	O
knowledge	O
to	O
see	O
how	O
adding	O
domain	B
knowledge	I
affects	O
speed	O
and	O
results	O
of	O
learning	O
,	O
three	O
series	O
of	O
experiments	O
were	O
done	O
by	O
urbanˇciˇc	O
&	O
bratko	O
(	O
1992	O
)	O
.	O
the	O
following	O
variants	O
of	O
learning	O
control	O
rules	O
with	O
program	O
boxes	O
were	O
explored	O
:	O
	O
*	O
	O
	O
*	O
*	O
	O
	O
*	O
	O
	O
*	O
*	O
	O
	O
	O
*	O
	O
*	O
	O
*	O
9	O
	O
*	O
	O
	O
*	O
*	O
	O
9	O
9	O
9	O
''	O
9	O
9	O
	O
*	O
	O
	O
*	O
*	O
	O
''	O
9	O
9	O
256	O
learning	O
to	O
control	O
dynamic	O
systems	O
[	O
ch	O
.	O
13	O
a.	O
without	O
domain	B
knowledge	I
,	O
b.	O
with	O
partial	O
domain	B
knowledge	I
,	O
considered	O
as	O
deﬁnitely	O
correct	O
,	O
and	O
c.	O
with	O
partial	O
initial	O
domain	B
knowledge	I
,	O
allowed	O
to	O
be	O
changed	O
during	O
learning	O
.	O
the	O
following	O
rule	O
served	O
as	O
partial	O
domain	B
knowledge	I
:	O
if	O
	O
	O
else	O
if	O
	O
	O
d	O
then	O
action	O
right	O
	O
d	O
then	O
action	O
left	O
	O
	O
although	O
the	O
rule	O
alone	O
is	O
not	O
effective	O
at	O
all	O
(	O
average	O
survival	O
was	O
30	O
steps	O
)	O
,	O
it	O
con-	O
siderably	O
decreased	O
the	O
number	O
of	O
trials	O
needed	O
for	O
achieving	O
10	O
000	O
survival	O
time	O
steps	O
(	O
table	O
13.3	O
)	O
.	O
at	O
the	O
same	O
time	O
,	O
the	O
reliability	O
(	O
i.e	O
.	O
the	O
percentage	O
of	O
trials	O
with	O
the	O
learned	O
state-action	O
table	O
,	O
surviving	O
more	O
than	O
10	O
000	O
simulation	O
steps	O
)	O
increased	O
from	O
16.5	O
%	O
to	O
50	O
%	O
.	O
more	O
detailed	O
description	O
of	O
the	O
experiments	O
is	O
available	O
in	O
urbanˇciˇc	O
&	O
bratko	O
(	O
1992	O
)	O
.	O
table	O
13.3	O
:	O
experimental	O
results	O
showing	O
the	O
inﬂuence	O
of	O
partial	O
knowledge	O
.	O
version	O
length	O
of	O
learning	O
av	O
.	O
reliability	O
av	O
.	O
survival	O
[	O
av	O
.	O
num	O
.	O
of	O
trials	O
]	O
a.	O
b.	O
c.	O
427	O
50	O
197	O
[	O
ratio	O
]	O
3/20	O
10/20	O
4/20	O
[	O
steps	O
]	O
4894	O
7069	O
4679	O
13.5.2	O
exploiting	O
domain	B
knowledge	I
in	O
genetic	O
learning	O
of	O
control	O
domain	B
knowledge	I
can	O
be	O
exploited	O
to	O
bypass	O
the	O
costly	O
process	O
of	O
learning	O
a	O
control	O
rule	O
from	O
scratch	O
.	O
instead	O
of	O
searching	O
for	O
both	O
the	O
structure	O
of	O
a	O
rule	O
and	O
the	O
values	O
of	O
numerical	O
parameters	O
required	O
by	O
the	O
rule	O
,	O
we	O
can	O
start	O
with	O
a	O
known	O
rule	O
structure	O
derived	O
by	O
bratko	O
(	O
1991	O
)	O
from	O
a	O
qualitative	O
model	O
of	O
pole	O
and	O
cart	O
.	O
then	O
we	O
employ	O
a	O
ga	O
to	O
tune	O
the	O
parameters/	O
	O
and/	O
!	O
appearing	O
in	O
the	O
rule	O
.	O
to	O
calculate	O
a	O
ﬁtness	O
value	O
of	O
an	O
individual	O
,	O
25	O
trials	O
were	O
carried	O
out	O
with	O
maximal	O
duration	O
of	O
a	O
trial	O
set	O
to	O
2000	O
steps	O
,	O
corresponding	O
to	O
40	O
seconds	O
of	O
simulated	O
time	O
.	O
populations	O
of	O
size	O
30	O
were	O
evolved	O
for	O
50	O
generations	O
.	O
the	O
ga	O
was	O
run	O
10	O
times	O
.	O
in	O
all	O
the	O
runs	O
,	O
the	O
parameter	O
settings	O
,	O
that	O
ensured	O
maximal	O
survival	O
of	O
the	O
system	O
for	O
all	O
25	O
initial	O
states	O
,	O
were	O
found	O
.	O
table	O
13.4	O
gives	O
the	O
best	O
three	O
obtained	O
parameter	O
settings	O
along	O
with	O
their	O
ﬁtness	O
values	O
.	O
(	O
n*	O
the	O
parameter	O
tuning	O
and	O
evaluation	O
procedures	O
were	O
repeated	O
identically	O
for	O
two	O
modiﬁed	O
versions	O
of	O
the	O
pole-cart	O
system	O
,	O
one	O
being	O
controlled	O
with	O
symmetrical	O
force	O
sv	O
problems	O
were	O
found	O
no	O
harder	O
for	O
the	O
ga	O
than	O
the	O
''	O
(	O
n	O
,	O
and	O
the	O
other	O
with	O
asymmetrical	O
force	O
''	O
s	O
n*	O
it	O
can	O
be	O
noted	O
that	O
in	O
this	O
case	O
,	O
the	O
genetic	O
algorithm	O
is	O
applied	O
just	O
to	O
tune	O
a	O
controller	O
with	O
known	O
structure	O
.	O
in	O
a	O
similar	O
way	O
,	O
other	O
types	O
of	O
controllers	O
can	O
be	O
tuned	O
,	O
for	O
example	B
the	O
classical	O
pid	O
controller	O
(	O
urbanˇciˇc	O
et	O
al.	O
,	O
1992	O
)	O
.	O
s	O
(	O
n*	O
n	O
case	O
.	O
$	O
	O
n	O
.	O
the	O
13.6	O
exploiting	O
operator	O
’	O
s	O
skill	O
13.6.1	O
learning	O
to	O
pilot	O
a	O
plane	O
sammut	O
et	O
al	O
.	O
(	O
1992	O
)	O
and	O
michie	O
&	O
sammut	O
(	O
1993	O
)	O
describe	O
experiments	O
in	O
extracting	O
,	O
by	O
machine	O
learning	O
,	O
the	O
pilot	O
’	O
s	O
subcognitive	O
component	O
of	O
the	O
skill	O
of	O
ﬂying	O
a	O
plane	O
.	O
in	O
	O
	O
	O
i	O
=	O
g	O
=	O
	O
	O
i	O
=	O
g	O
=	O
	O
*	O
/	O
''	O
9	O
9	O
9	O
sec	O
.	O
13.6	O
]	O
table	O
13.4	O
:	O
(	O
cq	O
1993	O
ieee	O
)	O
control	O
performance	O
of	O
bratko	O
’	O
s	O
control	O
rule	O
(	O
a	O
)	O
with	O
parameter	O
values	O
found	O
by	O
a	O
ga	O
,	O
and	O
(	O
b	O
)	O
with	O
parameter	O
values	O
that	O
make	O
the	O
rule	O
equivalent	O
to	O
the	O
bang-bang	O
variant	O
of	O
the	O
classical	O
control	O
rule	O
.	O
exploiting	O
operator	O
’	O
s	O
skill	O
257	O
parameters	O
failures	O
avg	O
.	O
survival	O
0.45	O
0.30	O
0.25	O
0.60	O
0.45	O
0.40	O
22.40	O
19.00	O
13.65	O
parameters	O
(	O
a	O
)	O
(	O
b	O
)	O
[	O
%	O
]	O
0	O
0	O
0	O
time	O
[	O
steps	O
]	O
1,000,000	O
1,000,000	O
1,000,000	O
fitness	O
0.9980	O
0.9977	O
0.9968	O
failures	O
avg	O
.	O
survival	O
[	O
%	O
]	O
0	O
time	O
[	O
steps	O
]	O
1,000,000	O
fitness	O
0.9781	O
/2	O
/	O
!	O
3.91	O
0.147	O
0.319	O
/	O
[	O
	O
these	O
experiments	O
,	O
a	O
simulator	O
of	O
the	O
cessna	O
airplane	O
was	O
used	O
.	O
human	O
pilots	O
were	O
asked	O
to	O
ﬂy	O
the	O
simulated	O
plane	O
according	O
to	O
a	O
well	O
deﬁned	O
ﬂight	O
plan	O
.	O
this	O
plan	O
consisted	O
of	O
seven	O
stages	O
including	O
manouevres	O
like	O
:	O
take	O
off	O
,	O
ﬂying	O
to	O
a	O
speciﬁed	O
point	O
,	O
turning	O
,	O
lining	O
up	O
with	O
the	O
runway	O
,	O
descending	O
to	O
the	O
runway	O
and	O
landing	O
.	O
the	O
pilots	O
’	O
control	O
actions	O
during	O
ﬂight	O
were	O
recorded	O
as	O
“	O
events	O
”	O
.	O
each	O
event	O
record	O
consisted	O
of	O
the	O
plane	O
’	O
s	O
state	O
variables	O
and	O
the	O
control	O
action	O
.	O
the	O
values	O
of	O
state	O
variables	O
belonging	O
to	O
an	O
event	O
were	O
actually	O
taken	O
a	O
little	O
earlier	O
than	O
the	O
pilot	O
’	O
s	O
action	O
.	O
the	O
reason	O
for	O
this	O
was	O
that	O
the	O
action	O
was	O
assumed	O
to	O
be	O
the	O
pilot	O
’	O
s	O
response	O
,	O
with	O
some	O
delay	O
,	O
to	O
the	O
current	O
state	O
of	O
the	O
plane	O
variables	O
.	O
sammut	O
et	O
al	O
.	O
(	O
1992	O
)	O
stated	O
that	O
it	O
remains	O
debatable	O
what	O
a	O
really	O
appropriate	O
delay	O
is	O
between	O
the	O
state	O
of	O
the	O
plane	O
variables	O
and	O
control	O
action	O
invoked	O
by	O
that	O
state	O
:	O
...	O
the	O
action	O
was	O
performed	O
some	O
time	O
later	O
in	O
response	O
to	O
the	O
stimulus	O
.	O
but	O
how	O
do	O
we	O
know	O
what	O
the	O
stimulus	O
was	O
?	O
unfortunately	O
there	O
is	O
no	O
way	O
of	O
knowing	O
.	O
the	O
plane	O
’	O
s	O
state	O
variables	O
included	O
elevation	O
,	O
elevation	O
speed	O
,	O
azimuth	O
,	O
azimuth	O
speed	O
,	O
airspeed	O
etc	O
.	O
the	O
possible	O
control	O
actions	O
affected	O
four	O
control	O
variables	O
:	O
rollers	O
,	O
elevator	O
,	O
thrust	O
and	O
ﬂaps	O
.	O
the	O
problem	O
was	O
decomposed	O
into	O
four	O
induction	O
problems	O
,	O
one	O
for	O
each	O
of	O
the	O
four	O
control	O
variables	O
.	O
these	O
four	O
learning	O
problems	O
were	O
assumed	O
independent	O
.	O
the	O
control	O
rules	O
were	O
induced	O
by	O
the	O
c4.5	O
induction	O
program	O
(	O
quinlan	O
,	O
1987a	O
)	O
.	O
the	O
total	O
data	O
set	O
consisted	O
of	O
90	O
000	O
events	O
collected	O
from	O
three	O
pilots	O
and	O
30	O
ﬂights	O
by	O
each	O
pilot	O
.	O
the	O
data	O
was	O
segmented	O
into	O
seven	O
stages	O
of	O
the	O
complete	O
ﬂight	O
plan	O
and	O
separate	O
rules	O
were	O
induced	O
for	O
each	O
stage	O
.	O
separate	O
control	O
rules	O
were	O
induced	O
for	O
each	O
of	O
the	O
three	O
pilots	O
.	O
it	O
was	O
decided	O
that	O
it	O
was	O
best	O
not	O
to	O
mix	O
the	O
data	O
corresponding	O
to	O
different	O
individuals	O
because	O
different	O
pilots	O
carry	O
out	O
their	O
manouevres	O
in	O
different	O
styles	O
.	O
there	O
was	O
a	O
technical	B
difﬁculty	O
in	O
using	O
c4.5	O
in	O
that	O
it	O
requires	O
discrete	O
class	O
values	O
whereas	O
in	O
the	O
ﬂight	O
problem	O
the	O
control	O
variables	O
are	O
mostly	O
continuous	O
.	O
the	O
continuous	O
ranges	O
therefore	O
had	O
to	O
be	O
converted	O
to	O
discrete	O
classes	O
by	O
segmentation	O
into	O
intervals	O
.	O
this	O
segmentation	O
was	O
done	O
manually	O
.	O
a	O
more	O
natural	O
learning	O
tool	O
for	O
this	O
induction	O
task	O
would	O
therefore	O
be	O
one	O
that	O
allows	O
continuous	O
class	O
,	O
such	O
as	O
the	O
techniques	O
of	O
learning	O
regression	O
trees	O
implemented	O
in	O
the	O
programs	O
cart	O
(	O
breiman	O
et	O
al.	O
,	O
1984	O
)	O
and	O
retis	O
(	O
karaliˇc	O
,	O
1992	O
)	O
.	O
sammut	O
et	O
al	O
.	O
(	O
1992	O
)	O
state	O
that	O
control	O
rules	O
for	O
a	O
complete	O
ﬂight	O
were	O
successfully	O
synthesized	O
resulting	O
in	O
an	O
inductively	O
constructed	O
autopilot	O
.	O
this	O
autopilot	O
ﬂies	O
the	O
cessna	O
/	O
	O
/	O
	O
/	O
!	O
258	O
learning	O
to	O
control	O
dynamic	O
systems	O
[	O
ch	O
.	O
13	O
in	O
a	O
manner	O
very	O
similar	O
to	O
that	O
of	O
the	O
human	O
pilot	O
whose	O
data	O
was	O
used	O
to	O
construct	O
the	O
rules	O
.	O
in	O
some	O
cases	O
the	O
autopilot	O
ﬂies	O
more	O
smoothly	O
than	O
the	O
pilot	O
.	O
we	O
have	O
observed	O
a	O
‘	O
clean-up	O
’	O
effect	O
noted	O
in	O
michie	O
,	O
bain	O
and	O
hayes-michie	O
(	O
1990	O
)	O
.	O
the	O
ﬂight	O
log	O
of	O
any	O
trainer	O
will	O
contain	O
many	O
spurious	O
actions	O
due	O
to	O
human	O
inconsistency	O
and	O
corrections	O
required	O
as	O
a	O
result	O
of	O
inattention	O
.	O
it	O
appears	O
that	O
effects	O
of	O
these	O
examples	O
are	O
pruned	O
away	O
by	O
c4.5	O
,	O
leaving	O
a	O
control	O
rule	O
which	O
ﬂies	O
very	O
smoothly	O
.	O
it	O
is	O
interesting	O
to	O
note	O
the	O
comments	O
of	O
sammut	O
et	O
al	O
.	O
(	O
1992	O
)	O
regarding	O
the	O
contents	O
of	O
the	O
induced	O
rules	O
:	O
one	O
of	O
the	O
limitations	O
we	O
have	O
encountered	O
with	O
existing	O
learning	O
algorithms	O
is	O
that	O
they	O
can	O
only	O
use	O
the	O
primitive	O
attributes	O
supplied	O
in	O
the	O
data	O
.	O
this	O
results	O
in	O
control	O
rules	O
that	O
can	O
not	O
be	O
understood	O
by	O
a	O
human	O
expert	O
.	O
the	O
rules	O
constructed	O
by	O
c4.5	O
are	O
purely	O
reactive	O
.	O
they	O
make	O
decisions	O
on	O
the	O
basis	O
of	O
the	O
values	O
in	O
a	O
single	O
step	O
of	O
simulation	O
.	O
the	O
induction	O
program	O
has	O
no	O
concept	O
of	O
time	O
and	O
causality	O
.	O
in	O
connection	O
with	O
this	O
,	O
some	O
strange	O
rules	O
can	O
turn	O
up	O
.	O
13.6.2	O
learning	O
to	O
control	O
container	B
cranes	I
the	O
world	O
market	O
requires	O
container	B
cranes	I
with	O
as	O
high	O
capacity	O
as	O
possible	O
.	O
one	O
way	O
to	O
meet	O
this	O
requirement	O
is	O
to	O
build	O
bigger	O
and	O
faster	O
cranes	O
;	O
however	O
,	O
this	O
approach	O
is	O
limited	O
by	O
construction	O
problems	O
as	O
well	O
as	O
by	O
unpleasant	O
feelings	O
drivers	O
have	O
when	O
moving	O
with	O
high	O
speeds	O
and	O
accelerations	O
.	O
the	O
other	O
solution	O
is	O
to	O
make	O
the	O
best	O
of	O
the	O
cranes	O
of	O
“	O
reasonable	O
”	O
size	O
,	O
meaning	O
in	O
the	O
ﬁrst	O
place	O
the	O
optimisation	B
of	O
the	O
working	O
cycle	O
and	O
efﬁcient	O
swing	O
damping	O
.	O
it	O
is	O
known	O
that	O
experienced	O
crane	O
drivers	O
can	O
perform	O
very	O
quickly	O
as	O
long	O
as	O
everything	O
goes	O
as	O
expected	O
,	O
while	O
each	O
subsequent	O
correction	O
considerably	O
affects	O
the	O
time	O
needed	O
for	O
accomplishing	O
the	O
task	O
.	O
also	O
,	O
it	O
is	O
very	O
difﬁcult	O
to	O
drive	O
for	O
hours	O
and	O
hours	O
with	O
the	O
same	O
attention	O
,	O
not	O
to	O
mention	O
the	O
years	O
of	O
training	O
needed	O
to	O
gain	O
required	O
skill	O
.	O
consequently	O
,	O
interest	O
for	O
cooperation	O
has	O
been	O
reported	O
by	O
chief	O
designer	O
of	O
metalna	O
machine	O
builders	O
,	O
steel	O
fabricators	O
and	O
erectors	O
,	O
maribor	O
,	O
which	O
is	O
known	O
world-wide	O
for	O
its	O
large-scale	O
container	B
cranes	I
.	O
they	O
are	O
aware	O
of	O
insufﬁciency	O
of	O
classical	O
automatic	O
controllers	O
(	O
for	O
example	B
sakawa	O
&	O
shinido	O
,	O
1982	O
)	O
,	O
which	O
can	O
be	O
easily	O
disturbed	O
in	O
the	O
presence	O
of	O
wind	O
or	O
other	O
unpredictable	O
factors	O
.	O
this	O
explains	O
their	O
interest	O
in	O
what	O
can	O
be	O
offered	O
by	O
alternative	O
methods	O
.	O
impressive	O
results	O
have	O
been	O
obtained	O
by	O
predictive	O
fuzzy	O
control	O
(	O
see	O
yasunobu	O
&	O
hasegawa	O
,	O
1986	O
)	O
.	O
their	O
method	O
involves	O
steps	O
such	O
as	O
describing	O
human	O
operator	O
strate-	O
gies	O
,	O
deﬁning	O
the	O
meaning	O
of	O
linguistic	O
performance	O
indices	O
,	O
deﬁning	O
the	O
models	O
for	O
predicting	O
operation	O
results	O
,	O
and	O
converting	O
the	O
linguistic	O
human	O
operator	O
strategies	O
into	O
predictive	O
fuzzy	O
control	O
rules	O
.	O
in	O
general	O
,	O
these	O
tasks	O
can	O
be	O
very	O
time	O
consuming	O
,	O
so	O
our	O
focus	O
of	O
attention	O
was	O
on	O
the	O
automated	O
synthesis	O
of	O
control	O
rules	O
directly	O
from	O
the	O
recorded	O
performance	O
of	O
well-	O
trained	O
operators	O
.	O
in	O
this	O
idea	O
,	O
we	O
are	O
following	O
the	O
work	O
of	O
michie	O
et	O
al	O
.	O
(	O
1990	O
)	O
,	O
sammut	O
et	O
al	O
.	O
(	O
1992	O
)	O
and	O
michie	O
&	O
camacho	O
(	O
1994	O
)	O
who	O
conﬁrmed	O
the	O
ﬁndings	O
of	O
sammut	O
et	O
(	O
1992	O
)	O
using	O
the	O
acm	O
public-domain	O
simulation	O
of	O
an	O
f-16	O
combat	O
plane	O
.	O
when	O
al	O
.	O
trying	O
to	O
solve	O
the	O
crane	O
control	O
problem	O
in	O
a	O
manner	O
similar	O
to	O
their	O
autopilot	O
construction	O
,	O
sec	O
.	O
13.6	O
]	O
exploiting	O
operator	O
’	O
s	O
skill	O
259	O
we	O
encountered	O
some	O
difﬁculties	O
which	O
are	O
to	O
be	O
investigated	O
more	O
systematically	O
if	O
the	O
method	O
is	O
to	O
become	O
general	O
.	O
to	O
transport	O
a	O
container	O
from	O
shore	O
to	O
a	O
target	O
position	O
on	O
a	O
ship	O
,	O
two	O
operations	O
are	O
to	O
be	O
performed	O
:	O
positioning	O
of	O
the	O
trolley	O
,	O
bringing	O
it	O
above	O
the	O
target	O
load	O
position	O
,	O
and	O
rope	O
operation	O
,	O
bringing	O
the	O
load	O
to	O
the	O
desired	O
height	O
.	O
the	O
performance	O
requirements	O
are	O
as	O
follows	O
:	O
basic	O
safety	O
:	O
obstacles	O
must	O
be	O
avoided	O
,	O
swinging	O
must	O
be	O
kept	O
within	O
prescribed	O
limits	O
;	O
stop-gap	O
accuracy	O
:	O
the	O
gap	O
between	O
the	O
ﬁnal	O
load	O
position	O
and	O
the	O
target	O
position	O
must	O
be	O
within	O
prescribed	O
limits	O
;	O
high	O
capacity	O
:	O
time	O
needed	O
for	O
transportation	O
is	O
to	O
be	O
minimised	O
.	O
the	O
last	O
requirement	O
forces	O
the	O
two	O
operations	O
to	O
be	O
performed	O
simultaneously	O
.	O
the	O
task	O
parameters	O
specifying	O
stop-gap	O
accuracy	O
,	O
swinging	O
limits	O
and	O
capacity	O
are	O
given	O
by	O
the	O
customer	O
and	O
vary	O
from	O
case	O
to	O
case	O
.	O
is	O
speciﬁed	O
by	O
six	O
variables	O
:	O
instead	O
of	O
a	O
real	O
crane	O
,	O
a	O
simulator	O
was	O
used	O
in	O
our	O
experiments	O
.	O
the	O
state	O
of	O
the	O
system	O
trolley	O
position	O
and	O
its	O
velocity	O
,	O
and	O
	O
;	O
rope	O
inclination	O
angle	O
and	O
its	O
angular	O
velocity	O
,	O
	O
and	O
	O
rope	O
length	O
and	O
the	O
length	O
velocity	O
,	O
	O
and	O
	O
.	O
forces	O
are	O
applied	O
:	O
``	O
n	O
direction	O
of	O
the	O
rope	O
.	O
(	O
so	O
''	O
dk	O
is	O
applied	O
to	O
the	O
trolley	O
in	O
the	O
horizontal	O
direction	O
,	O
and	O
''	O
143	O
time	O
is	O
measured	O
in	O
steps	O
.	O
at	O
each	O
step	O
,	O
the	O
state	O
of	O
the	O
system	O
is	O
measured	O
and	O
two	O
control	O
in	O
the	O
is	O
the	O
force	O
in	O
the	O
vertical	O
direction	O
.	O
)	O
the	O
next	O
state	O
is	O
computed	O
using	O
runge-kutta	O
numerical	O
simulation	O
of	O
fourth	O
order	O
,	O
taking	O
into	O
account	O
the	O
dynamic	O
equations	O
of	O
the	O
system	O
.	O
parameters	O
of	O
the	O
system	O
(	O
lengths	O
,	O
heights	O
,	O
masses	O
etc	O
.	O
)	O
are	O
the	O
same	O
as	O
those	O
of	O
the	O
real	O
container	B
cranes	I
in	O
port	O
of	O
koper	O
.	O
simulation	O
runs	O
on	O
ibm	O
pc	O
compatible	O
computers	O
and	O
is	O
real-time	O
for	O
386	O
(	O
33	O
mhz	O
or	O
faster	O
)	O
with	O
a	O
mathematical	O
co-processor	O
.	O
;	O
when	O
experimenting	O
with	O
the	O
simulator	O
,	O
one	O
can	O
choose	O
input	B
mode	O
“	O
record	O
”	O
,	O
“	O
play	O
”	O
or	O
“	O
auto	O
”	O
,	O
output	B
mode	O
“	O
picture	O
”	O
or	O
“	O
instruments	O
”	O
.	O
where	O
one	O
strike	O
at	O
the	O
step	O
.	O
similarly	O
,	O
arrows	O
and	O
in	O
the	O
“	O
record	O
”	O
mode	O
,	O
the	O
values	O
of	O
the	O
current	O
control	O
forces	O
are	O
read	O
from	O
the	O
keyboard	O
,	O
for	O
a	O
certain	O
predeﬁned	O
or	O
means	O
a	O
decrease	O
or	O
increase	O
of	O
''	O
n	O
indicate	O
the	O
change	O
of	O
''	O
d	O
.	O
a	O
ﬁle	O
containing	O
all	O
control	O
actions	O
together	O
with	O
the	O
corresponding	O
times	O
and	O
system	O
states	O
is	O
recorded	O
.	O
in	O
the	O
“	O
play	O
”	O
mode	O
,	O
recorded	O
experiments	O
can	O
be	O
viewed	O
again	O
,	O
using	O
the	O
recorded	O
ﬁles	O
as	O
input	O
.	O
when	O
“	O
auto	O
”	O
mode	O
is	O
chosen	O
,	O
the	O
current	O
values	O
of	O
control	O
forces	O
are	O
determined	O
by	O
a	O
procedure	O
representing	O
an	O
automatic	O
controller	O
.	O
the	O
choice	O
of	O
the	O
output	B
mode	O
enables	O
the	O
graphical	O
representation	O
of	O
the	O
scene	O
(	O
“	O
pic-	O
ture	O
”	O
)	O
or	O
the	O
variant	O
where	O
the	O
six	O
state	O
variables	O
and	O
the	O
force	O
values	O
are	O
presented	O
as	O
columns	O
with	O
dynamically	O
changing	O
height	O
,	O
imitating	O
measuring	O
instruments	O
.	O
six	O
students	O
volunteered	O
in	O
an	O
experiment	O
where	O
they	O
were	O
asked	O
to	O
learn	O
to	O
control	O
the	O
crane	O
simulator	O
simply	O
by	O
playing	O
with	O
the	O
simulator	O
and	O
trying	O
various	O
control	O
strategies	O
.	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
d	O
	O
	O
	O
	O
	O
260	O
learning	O
to	O
control	O
dynamic	O
systems	O
[	O
ch	O
.	O
13	O
they	O
were	O
given	O
just	O
the	O
“	O
instrument	O
”	O
version	O
;	O
in	O
fact	O
,	O
they	O
didn	O
’	O
t	O
know	O
which	O
dynamic	O
system	O
underlied	O
the	O
simulator	O
.	O
in	O
spite	O
of	O
that	O
,	O
they	O
succeeded	O
to	O
learn	O
the	O
task	O
,	O
although	O
the	O
differences	O
in	O
time	O
needed	O
for	O
this	O
as	O
well	O
as	O
the	O
quality	O
of	O
control	O
were	O
remarkable	O
.	O
to	O
learn	O
to	O
control	O
the	O
crane	O
reasonably	O
well	O
,	O
it	O
took	O
a	O
subject	O
between	O
about	O
25	O
and	O
200	O
trials	O
.	O
this	O
amounts	O
to	O
about	O
1	O
to	O
10	O
hours	O
of	O
real	O
time	O
spent	O
with	O
the	O
simulator	O
.	O
our	O
aim	O
was	O
to	O
build	O
automatic	O
controllers	O
from	O
human	O
operators	O
’	O
traces	O
.	O
we	O
applied	O
retis	O
-	O
a	O
program	O
for	O
regression	B
tree	I
construction	O
(	O
karaliˇc	O
,	O
1992	O
)	O
to	O
the	O
recorded	O
data	O
.	O
the	O
ﬁrst	O
problem	O
to	O
solve	O
was	O
how	O
to	O
choose	O
an	O
appropriate	O
set	O
of	O
learning	O
examples	O
out	O
of	O
this	O
enormous	O
set	O
of	O
recorded	O
data	O
.	O
after	O
some	O
initial	O
experiments	O
we	O
found	O
,	O
as	O
in	O
sammut	O
et	O
al	O
.	O
(	O
1992	O
)	O
,	O
that	O
it	O
was	O
beneﬁcial	O
to	O
use	O
different	O
trials	O
performed	O
by	O
the	O
same	O
student	O
,	O
since	O
it	O
was	O
practically	O
impossible	O
to	O
ﬁnd	O
trials	O
perfect	O
in	O
all	O
aspects	O
even	O
among	O
the	O
successful	O
cases	O
.	O
in	O
the	O
preparation	O
of	O
learning	O
data	O
,	O
performance	O
was	O
sampled	O
each	O
0.1	O
second	O
.	O
the	O
actions	O
were	O
related	O
to	O
the	O
states	O
with	O
delay	O
which	O
was	O
also	O
0.1	O
second	O
.	O
the	O
performance	O
of	O
the	O
best	O
autodriver	O
induced	O
in	O
these	O
initial	O
experiments	O
can	O
be	O
seen	O
in	O
figure	O
13.3.	O
it	O
resulted	O
from	O
798	O
learning	O
examples	O
for	O
''	O
	O
and	O
1017	O
examples	O
for	O
''	O
d	O
.	O
the	O
control	O
strategy	O
it	O
uses	O
is	O
rather	O
conservative	O
,	O
minimising	O
the	O
swinging	O
,	O
but	O
at	O
the	O
cost	O
of	O
time	O
.	O
in	O
further	O
experiments	O
,	O
we	O
will	O
try	O
to	O
build	O
an	O
autodriver	O
which	O
will	O
successfully	O
cope	O
with	O
load	O
swinging	O
,	O
resulting	O
in	O
faster	O
and	O
more	O
robust	O
performance	O
.	O
diffx	O
2.5diffl	O
0	O
20	O
40	O
time/s	O
60	O
80	O
0	O
6	O
0	O
4	O
0	O
2	O
0	O
g	O
e	O
d	O
/	O
m	O
/	O
e	O
c	O
n	O
a	O
t	O
s	O
d	O
i	O
n	O
k	O
/	O
e	O
c	O
r	O
o	O
f	O
l	O
o	O
r	O
t	O
n	O
o	O
c	O
5	O
0	O
5	O
-	O
0	O
20	O
40	O
time/s	O
60	O
fx	O
10fy	O
80	O
fig	O
.	O
13.3	O
:	O
the	O
crane	O
simulator	O
response	O
to	O
the	O
control	O
actions	O
of	O
the	O
autodriver	O
.	O
these	O
experiments	O
indicate	O
that	O
further	O
work	O
is	O
needed	O
regarding	O
the	O
following	O
ques-	O
tions	O
:	O
what	O
is	O
the	O
actual	O
delay	O
between	O
the	O
system	O
’	O
s	O
state	O
and	O
the	O
operator	O
’	O
s	O
action	O
;	O
robust-	O
ness	O
of	O
induced	O
rules	O
with	O
respect	O
to	O
initial	O
states	O
;	O
comprehensibility	O
of	O
induced	O
control	O
rules	O
;	O
inducing	O
higher	O
level	O
conceptual	O
description	O
of	O
control	O
strategies	O
.	O
	O
¡	O
	O
	O
conclusions	O
261	O
13.7	O
conclusions	O
in	O
this	O
chapter	O
we	O
have	O
treated	O
the	O
problem	O
of	O
controlling	O
a	O
dynamic	O
system	O
mainly	O
as	O
a	O
classiﬁcation	B
problem	O
.	O
we	O
introduced	O
three	O
modes	O
of	O
learning	O
to	O
control	O
,	O
depending	O
on	O
the	O
information	O
available	O
to	O
the	O
learner	O
.	O
this	O
information	O
included	O
in	O
addition	O
to	O
the	O
usual	O
examples	O
of	O
the	O
behaviour	O
of	O
the	O
controlled	O
system	O
,	O
also	O
explicit	O
symbolic	O
knowledge	O
about	O
the	O
controlled	O
system	O
,	O
and	O
example	B
actions	O
performed	O
by	O
a	O
skilled	O
human	O
operator	O
.	O
one	O
point	O
that	O
the	O
described	O
experiments	O
emphasise	O
is	O
the	O
importance	O
of	O
(	O
possibly	O
incomplete	O
)	O
partial	O
knowledge	O
about	O
the	O
controlled	O
system	O
.	O
methods	O
described	O
in	O
this	O
chapter	O
enable	O
natural	O
use	O
of	O
partial	O
symbolic	O
knowledge	O
.	O
although	O
incomplete	O
,	O
this	O
knowledge	O
may	O
drastically	O
constrain	O
the	O
search	O
for	O
control	O
rules	O
,	O
thereby	O
eliminating	O
in	O
advance	O
large	O
numbers	O
of	O
totally	O
unreasonable	O
rules	O
.	O
our	O
choice	O
of	O
the	O
approaches	O
to	O
learning	O
to	O
control	O
in	O
this	O
chapter	O
was	O
subjective	O
.	O
among	O
a	O
large	O
number	O
of	O
known	O
approaches	O
,	O
we	O
chose	O
for	O
more	O
detailed	O
presentation	O
those	O
that	O
:	O
ﬁrst	O
,	O
we	O
had	O
personal	O
experimental	O
experience	O
with	O
,	O
and	O
second	O
,	O
that	O
enable	O
the	O
use	O
of	O
(	O
possibly	O
partial	O
)	O
symbolic	O
prior	O
knowledge	O
.	O
in	O
all	O
the	O
approaches	O
described	O
,	O
there	O
was	O
an	O
aspiration	O
to	O
generate	O
comprehensible	O
control	O
rules	O
,	O
sometimes	O
at	O
the	O
cost	O
of	O
an	O
additional	O
learning	O
stage	O
.	O
an	O
interesting	O
theme	O
,	O
also	O
described	O
,	O
is	O
“	O
behavioural	B
cloning	I
”	O
where	O
a	O
human	O
’	O
s	O
be-	O
havioural	O
skill	O
is	O
cloned	O
by	O
a	O
learned	O
rule	O
.	O
behavioural	B
cloning	I
is	O
interesting	O
both	O
from	O
the	O
practical	O
and	O
the	O
research	O
points	O
of	O
view	O
.	O
much	O
further	O
work	O
is	O
needed	O
before	O
behavioural	B
cloning	I
may	O
become	O
routinely	O
applicable	O
in	O
practice	O
.	O
g9	O
^/	O
[	O
	O
g9	O
/2	O
behavioural	B
cloning	I
is	O
essentially	O
the	O
regression	O
of	O
the	O
operator	O
’	O
s	O
decision	O
function	O
from	O
examples	O
of	O
his/her	O
decisions	O
.	O
it	O
is	O
relevant	O
in	O
this	O
respect	O
to	O
notice	O
a	O
similarity	O
between	O
this	O
and	O
traditional	O
top-down	O
derivation	O
of	O
control	O
from	O
a	O
detailed	O
model	O
of	O
the	O
system	O
to	O
be	O
controlled	O
.	O
this	O
similarity	O
is	O
illustrated	O
by	O
the	O
fact	O
that	O
such	O
a	O
top-down	O
approach	O
for	O
the	O
pole-and-cart	O
system	O
gives	O
the	O
known	O
linear	O
control	O
rule	O
''	O
which	O
looks	O
just	O
like	O
regression	O
equation	O
.	O
as	O
stated	O
in	O
the	O
introduction	O
,	O
there	O
are	O
several	O
criteria	O
for	O
,	O
and	O
goals	O
of	O
,	O
learning	O
to	O
control	O
,	O
and	O
several	O
assumptions	O
regarding	O
the	O
problem	O
.	O
as	O
shown	O
by	O
the	O
experience	O
with	O
various	O
learning	O
approaches	O
,	O
it	O
is	O
important	O
to	O
clarify	O
very	O
precisely	O
what	O
these	O
goals	O
and	O
assumptions	O
really	O
are	O
in	O
the	O
present	O
problem	O
.	O
correct	O
statement	O
of	O
these	O
may	O
considerably	O
affect	O
the	O
efﬁciency	O
of	O
the	O
learning	O
process	O
.	O
for	O
example	B
,	O
it	O
is	O
important	O
to	O
consider	O
whether	O
some	O
(	O
partial	O
)	O
symbolic	O
knowledge	O
exists	O
about	O
the	O
domain	O
,	O
and	O
not	O
to	O
assume	O
automatically	O
that	O
it	O
is	O
necessary	O
,	O
or	O
best	O
,	O
to	O
learn	O
everything	O
from	O
scratch	O
.	O
in	O
some	O
approaches	O
reviewed	O
,	O
such	O
incomplete	O
prior	O
knowledge	O
could	O
also	O
result	O
from	O
a	O
previous	O
stage	O
of	O
learning	O
when	O
another	O
learning	O
technique	O
was	O
employed	O
.	O
acknowledgements	O
:	O
this	O
work	O
was	O
supported	O
by	O
the	O
ministry	O
of	O
science	O
and	O
technology	O
of	O
slovenia	O
.	O
the	O
authors	O
would	O
like	O
to	O
thank	O
donald	O
michie	O
for	O
comments	O
and	O
suggestions	O
.	O
u9	O
/	O
!	O
/	O
	O
	O
	O
appendices	O
a	O
dataset	O
availability	O
the	O
“	O
public	O
domain	O
”	O
datasets	O
are	O
listed	O
below	O
with	O
an	O
anonymous	O
ftp	O
address	O
.	O
if	O
you	O
do	O
not	O
have	O
access	O
to	O
these	O
,	O
then	O
you	O
can	O
obtain	O
the	O
datasets	O
on	O
diskette	O
from	O
dr.	O
p.	O
b.	O
brazdil	O
,	O
university	O
of	O
porto	O
,	O
laboratory	O
of	O
ai	O
and	O
computer	O
science	O
,	O
r.	O
campo	O
alegre	O
823	O
,	O
4100	O
porto	O
,	O
potugal	O
.	O
the	O
main	O
source	O
of	O
datasets	O
is	O
ics.uci.edu	O
(	O
128.195.1.1	O
)	O
-	O
the	O
uci	O
repository	O
of	O
machine	O
learning	O
databases	O
and	O
domain	O
theories	O
which	O
is	O
managed	O
by	O
d.	O
w.	O
aha	O
.	O
the	O
following	O
datasets	O
(	O
amongst	O
many	O
others	O
)	O
are	O
in	O
pub/machine-learning-	O
databases	O
australian	O
credit	O
(	O
credit-screening/crx.data	O
statlog/australian	O
)	O
diabetes	B
(	O
pima-indian-diabetes	O
)	O
dna	O
(	O
molecular-biology/splice-junction-gene-sequences	O
)	O
heart	B
disease	I
(	O
heart-disease/	O
statlog/heart	O
)	O
letter	B
recognition	I
image	O
segmentation	O
(	O
statlog/segment	O
)	O
shuttle	B
control	I
(	O
statlog/shuttle	O
)	O
landsat	O
satellite	B
image	I
(	O
statlog/satimage	O
)	O
vehicle	B
recognition	I
(	O
statlog/vehicle	O
)	O
the	O
datasets	O
were	O
often	O
processed	O
,	O
and	O
the	O
processed	O
form	O
can	O
be	O
found	O
in	O
the	O
stat-	O
log	O
subdirectory	O
where	O
mentioned	O
above	O
.	O
in	O
addition	O
,	O
the	O
processed	O
datasets	O
(	O
as	O
used	O
in	O
this	O
book	O
)	O
can	O
also	O
be	O
obtained	O
from	O
ftp.strath.ac.uk	O
(	O
130.159.248.24	O
)	O
in	O
directory	O
stams/statlog	O
.	O
these	O
datasets	O
are	O
australian	O
,	O
diabetes	B
,	O
dna	O
,	O
german	O
,	O
heart	O
,	O
letter	O
,	O
satimage	O
,	O
segment	O
,	O
shuttle	O
,	O
shuttle	O
,	O
and	O
there	O
are	O
associated	O
.doc	O
ﬁles	O
as	O
well	O
as	O
a	O
split	O
into	O
train	O
and	O
test	O
set	O
(	O
as	O
used	O
in	O
the	O
statlog	O
project	O
)	O
for	O
the	O
larger	O
datasets	O
.	O
b	O
software	O
sources	O
and	O
details	O
many	O
of	O
the	O
classical	O
statistical	B
algorithms	O
are	O
available	O
in	O
standard	O
statistical	B
packages	O
.	O
here	O
we	O
list	O
some	O
public	O
domain	O
versions	O
and	O
sources	O
,	O
and	O
some	O
commercial	O
packages	O
.	O
if	O
a	O
simple	O
rule	O
has	O
been	O
adopted	O
for	O
parameter	O
selection	O
,	O
then	O
we	O
have	O
also	O
described	O
this	O
.	O
b.	O
software	O
sources	O
and	O
details	O
263	O
alloc80	O
is	O
a	O
fortran	O
program	O
available	O
from	O
j.	O
hermans	O
,	O
dept	O
.	O
of	O
medical	O
statistics	O
,	O
niels	O
bohrweg	O
1	O
,	O
2333	O
ca	O
leiden	O
,	O
university	O
of	O
leiden	O
,	O
the	O
netherlands	O
.	O
smart	O
is	O
a	O
collection	O
of	O
fortran	O
subroutines	O
developed	O
by	O
j.	O
h.	O
friedman	O
,	O
dept	O
.	O
of	O
statistics	O
,	O
sequoia	O
hall	O
,	O
stanford	O
university	O
,	O
stanford	O
,	O
ca	O
94305	O
,	O
usa	O
.	O
castle	O
can	O
be	O
obtained	O
from	O
r.	O
molina	O
,	O
dept	O
of	O
computer	O
science	O
and	O
a.i.	O
,	O
faculty	O
of	O
science	O
,	O
university	O
of	O
granada	O
.	O
18071-granada	O
,	O
spain	O
.	O
indcart	O
,	O
bayes	O
tree	O
and	O
naive	O
bayes	O
.	O
are	O
part	O
of	O
the	O
ind	O
package	O
which	O
is	O
available	O
from	O
w.	O
buntine	O
,	O
nasa	O
ames	O
research	O
center	O
,	O
ms	O
269-2	O
,	O
moffett	O
field	O
,	O
ca	O
94035-1000	O
,	O
usa	O
.	O
(	O
email	O
:	O
¢¤£+¥	O
¦+§©¨p£+ª©	O
«	O
2ª	O
¬­®¥	O
£e¯­x	O
«	O
2¥	O
¬°¥l­±+ª	O
²	O
)	O
dipol92	O
and	O
cal5	O
is	O
available	O
from	O
f.	O
wysotzki	O
,	O
fraunhofer-institute	O
,	O
kurstrasse	O
33	O
,	O
d-19117	O
berlin	O
,	O
germany	O
.	O
for	O
dipol92	O
the	O
number	O
of	O
clusters	O
has	O
to	O
be	O
ﬁxed	O
by	O
the	O
user	O
with	O
some	O
systematic	O
experimentation	O
.	O
all	O
other	O
parameters	O
are	O
determined	O
by	O
the	O
algorithm	O
.	O
for	O
cal5	O
the	O
conﬁdence	O
level	O
for	O
estimation	O
and	O
the	O
threshold	O
for	O
tree	O
pruning	B
were	O
optimised	O
either	O
by	O
hand	O
or	O
a	O
special	O
c	O
-	O
shell	O
.	O
an	O
entropy	O
measure	B
to	O
choose	O
the	O
best	O
discrimination	O
attribute	O
at	O
each	O
current	O
node	O
was	O
used	O
.	O
logistic	O
discriminants	O
,	O
quadratic	O
discrminants	O
and	O
logistic	O
discriminants	O
are	O
for-	O
tran	O
programs	O
available	O
from	O
r.	O
j.	O
henery	O
,	O
department	O
of	O
statistics	O
and	O
modelling	O
sci-	O
ence	O
,	O
university	O
of	O
strathclyde	O
,	O
glasgow	O
g1	O
1xh	O
,	O
uk	O
.	O
there	O
are	O
also	O
available	O
by	O
anony-	O
mous	O
ftp	O
from	O
ftp.strath.ac.uk	O
(	O
130.159.248.24	O
)	O
in	O
directory	O
stams/statlog/programs	O
.	O
is	O
available	O
from	O
h.	O
perdrix	O
,	O
isoft	O
,	O
chemin	O
de	O
moulon	O
,	O
91190	O
gif	O
sur	O
yvette	O
,	O
france	O
.	O
the	O
user	O
must	O
choose	O
between	O
4	O
evaluation	O
functions	O
:	O
.g³	O
¶eº	O
´4µ·¶	O
¸	O
[	O
¹º	O
ª4	O
«	O
·±+¥	O
«	O
+	O
»	O
+ª	O
£-¼2¥	O
¶2º	O
½µ·¶	O
¸	O
[	O
¹	O
±+¥	O
«	O
£+¥	O
¾µ	O
ª	O
»	O
à	O
¥¼	O
¥	O
¬b¿+£	O
¥©	O
«	O
[	O
¯	O
æ+µ	O
¶eº	O
ª©	O
«	O
±+¥	O
«	O
+	O
»	O
+ª	O
£	O
¼	O
¥	O
«	O
ç¼	O
¥©	O
«	O
¶	O
¸ä¹	O
á2è	O
¹	O
¬éà	O
¹âá	O
[	O
¹4¶	O
¥	O
¬b¿+£	O
¿ä¬	O
£i­	O
¹¹	O
¹^¶	O
«	O
·ã2¥-£	O
¥©¨	O
¶eº4¶eº	O
«	O
+±	O
¹âá	O
ª©	O
«	O
[	O
¬	O
£+ª	O
à	O
¿ä¯	O
¦å+¥©	O
«	O
¥	O
£+¥	O
¬	O
¶·¶-¸	O
[	O
¹^¸	O
[	O
º¤¹	O
¦ª	O
»	O
£+¥	O
£e¯	O
ª¥	O
¯¯©ª©¿	O
«	O
in	O
the	O
reported	O
results	O
,	O
the	O
fourth	O
option	O
was	O
chosen	O
.	O
backprop	O
,	O
cascade	O
correlation	O
and	O
radial	O
basis	O
function	O
are	O
fortran	O
programs	O
available	O
from	O
r.	O
rohwer	O
,	O
department	O
of	O
computer	O
science	O
and	O
applied	O
mathematics	O
,	O
aston	O
university	O
,	O
birmingham	O
b4	O
7et	O
,	O
uk	O
.	O
the	O
inputs	O
for	O
all	O
datasets	O
were	O
normalised	O
to	O
zero	O
mean	O
and	O
unit	O
variance	O
.	O
the	O
outputs	O
problem	O
was	O
represented	O
as	O
an	O
n-dimensional	O
vector	O
with	O
all	O
components	O
equal	O
to	O
0	O
except	O
the	O
multilayer	O
perceptron	O
simulations	O
were	O
done	O
with	O
autonet	O
on	O
sun	O
unix	O
worksta-	O
tions	O
.	O
autonet	O
is	O
commercial	O
software	O
available	O
from	O
paul	O
gregory	O
,	O
recognition	O
research	O
,	O
140	O
church	O
lane	O
,	O
marple	O
,	O
stockport	O
,	O
sk6	O
7la	O
,	O
uk	O
,	O
(	O
+44/0	O
)	O
61	O
449-8628.	O
were	O
converted	O
to	O
a	O
1-of-n	O
representation	O
;	O
ie.	O
,	O
the	O
>	O
th	O
class	O
of	O
an	O
n-class	O
classiﬁcation	B
the	O
>	O
th	O
,	O
which	O
is	O
1	O
.	O
¸	O
[	O
º	O
áä¹	O
àà	O
¿¼	O
£ª	O
»	O
îeº	O
àà	O
«	O
ï¥©	O
«	O
+àª©¿	O
¶eº	O
¸+¶	O
&	O
º	O
òe¹º	O
¥¤ë	O
10	O
runs	O
were	O
made	O
,	O
with	O
10which	O
of	O
the	O
10	O
runs	O
was	O
best	O
.	O
random	O
number	O
seed	O
for	O
each	O
run	O
was	O
=	O
run	O
number	O
(	O
1..10	O
)	O
.	O
having	O
picked	O
the	O
best	O
net	O
by	O
cross	O
validation	O
within	O
the	O
training	O
«	O
&	O
ë¤¥	O
¦	O
º°¶	O
¿	O
«	O
ã¤¿	O
¶2º	O
ª©	O
«	O
ó	O
¬-¥	O
£e¬íì	O
¥+¯	O
µ	O
&	O
´	O
ª©	O
«	O
[	O
¬ñðt¬	O
the	O
settings	O
were	O
:	O
±	O
¼2ª	O
ài­	O
¶eº	O
²+¥	O
¶eº	O
	O
º	O
«	O
º	O
ª	O
¹	O
¹	O
º	O
¬	O
¶	O
¢	O
º	O
«	O
¶	O
¶	O
º	O
º	O
¹	O
º	O
º	O
«	O
¶	O
¸	O
ª	O
¯	O
¶	O
¹	O
»	O
º	O
«	O
¹	O
à	O
á	O
¦	O
¹	O
ê	O
¹	O
¹	O
´	O
­	O
¹	O
¶	O
¶	O
º	O
º	O
±	O
«	O
º	O
­	O
264	O
learning	O
to	O
control	O
dynamic	O
systems	O
set	O
,	O
these	O
nets	O
were	O
then	O
used	O
for	O
supplying	O
the	O
performance	O
ﬁgures	O
on	O
the	O
whole	O
training	O
set	O
and	O
on	O
the	O
test	O
set	O
.	O
the	O
ﬁgures	O
averaged	O
for	O
cross	O
validation	O
performance	O
measures	O
were	O
also	O
for	O
the	O
best	O
nets	O
found	O
during	O
local	O
cross-validation	O
within	O
the	O
individual	O
training	O
sets	O
.	O
training	O
proceeds	O
in	O
four	O
stages	O
,	O
with	O
different	O
stages	O
using	O
different	O
subsets	O
of	O
the	O
training	O
data	O
,	O
larger	O
each	O
time	O
.	O
training	O
proceeds	O
until	O
no	O
improvement	O
in	O
error	O
is	O
achieved	O
for	O
a	O
run	O
of	O
updates	O
.	O
the	O
rrnn	O
simulator	O
provided	O
the	O
radial	O
basis	O
function	O
code	O
.	O
this	O
is	O
freely	O
available	O
at	O
the	O
time	O
of	O
writing	O
by	O
anonymous	O
ftp	O
from	O
uk.ac.aston.cs	O
(	O
134.151.52.106	O
)	O
.	O
this	O
package	O
also	O
contains	O
mlp	O
code	O
using	O
the	O
conjugate	O
gradient	O
algorithm	O
,	O
as	O
does	O
autonet	O
,	O
and	O
several	O
other	O
algorithms	O
.	O
reports	O
on	O
benchmark	O
excercises	O
are	O
available	O
for	O
some	O
of	O
these	O
mlp	O
programs	O
in	O
rohwer	O
(	O
1991c	O
)	O
.	O
the	O
centres	O
for	O
the	O
radial	O
basis	O
functions	O
were	O
selected	O
randomly	O
from	O
the	O
training	O
data	O
,	O
except	O
that	O
centres	O
were	O
allocated	O
to	O
each	O
class	O
in	O
proportion	O
to	O
the	O
number	O
of	O
representatives	O
of	O
that	O
class	O
in	O
the	O
dataset	O
,	O
with	O
at	O
least	O
one	O
centre	O
provided	O
to	O
each	O
class	O
in	O
any	O
case	O
.	O
each	O
gaussian	O
radius	O
was	O
set	O
to	O
the	O
distance	O
to	O
the	O
nearest	O
neighboring	O
centre	O
.	O
the	O
linear	O
system	O
was	O
solved	O
by	O
singular	O
value	O
decomposition	O
.	O
for	O
the	O
small	O
datasets	O
the	O
number	O
of	O
centres	O
and	O
thier	O
locations	O
were	O
selected	O
by	O
training	O
with	O
various	O
numbers	O
of	O
centres	O
,	O
using	O
20	O
different	O
random	O
number	O
seeds	O
for	O
each	O
number	O
,	O
and	O
evaluating	O
with	O
a	O
cross	O
validation	O
set	O
withheld	O
from	O
the	O
training	O
data	O
,	O
precisely	O
as	O
was	O
done	O
for	O
the	O
mlps	O
.	O
for	O
the	O
large	O
datasets	O
,	O
time	O
constraints	O
were	O
met	O
by	O
compromising	O
rigour	O
,	O
in	O
that	O
the	O
test	O
set	O
was	O
used	O
for	O
the	O
cross-validation	O
set	O
.	O
results	O
for	O
these	O
sets	O
should	O
therefore	O
be	O
viewed	O
with	O
some	O
caution	O
.	O
this	O
was	O
the	O
case	O
for	O
all	O
data	O
sets	O
,	O
until	O
those	O
for	O
which	O
cross-validation	O
was	O
explicitly	O
required	O
(	O
australian	O
,	O
diabetes	B
,	O
german	O
,	O
isoft	O
,	O
segment	O
)	O
were	O
repeated	O
with	O
cross-validation	O
to	O
select	O
the	O
number	O
of	O
centres	O
carried	O
out	O
within	O
the	O
training	O
set	O
only	O
.	O
the	O
rough	O
guideline	O
followed	O
for	O
deciding	O
on	O
numbers	O
of	O
centres	O
to	O
try	O
is	O
that	O
the	O
number	O
should	O
be	O
about	O
100	O
times	O
the	O
dimension	O
of	O
the	O
input	B
space	O
,	O
unless	O
that	O
would	O
be	O
more	O
than	O
10	O
%	O
of	O
the	O
size	O
of	O
the	O
dataset	O
.	O
lvq	O
is	O
available	O
from	O
the	O
laboratory	O
of	O
computer	O
science	O
and	O
information	O
science	O
,	O
helsinki	O
university	O
of	O
technology	O
,	O
rakentajanaukio	O
2	O
c	O
,	O
sf	O
-02150	O
espoo	O
,	O
finland	O
.	O
it	O
can	O
also	O
be	O
obtained	O
by	O
anonymous	O
ftp	O
from	O
cochlea.hut.ﬁ	O
(	O
130.233.168.48	O
)	O
.	O
cart	O
is	O
a	O
licensed	O
product	O
of	O
california	O
statistical	B
software	O
inc.	O
,	O
961	O
yorkshire	O
court	O
,	O
lafayette	O
,	O
ca	O
94549	O
,	O
usa	O
.	O
c4.5	O
is	O
availbale	O
from	O
j.r.	O
quinlan	O
,	O
dept	O
.	O
of	O
computer	O
science	O
,	O
madsen	O
building	O
f09	O
,	O
university	O
of	O
sydney	O
,	O
new	O
south	O
wales	O
,	O
new	O
south	O
wales	O
.	O
the	O
parameters	O
used	O
were	O
the	O
defaults	O
.	O
the	O
heuristic	O
was	O
information	O
gain	O
.	O
¥4	O
«	O
+àà	O
º4¶	O
¬éà+¥	O
¹4¶	O
¥+¬	O
¬2×	O
òeº	O
àöõ	O
«	O
2¥	O
«	O
+±ôð	O
«	O
+àpª	O
¢	O
¶	O
¸	O
º°¶	O
á	O
[	O
º	O
ªë-àiðùà	O
£+ª	O
¹4¶¤¶eº	O
¬a¿	O
ª·	O
»	O
+ª	O
£e¯	O
«	O
+±	O
º4¶	O
üe¹	O
¦	O
»	O
+ª	O
£à	O
ª	O
¬	O
¬bã	O
ë¥	O
¦	O
«	O
p±	O
áä¹	O
¹¹	O
ª©	O
«	O
£ª	O
»	O
à	O
¿¼	O
£	O
¿	O
«	O
«	O
+±	O
¯-ª4	O
«	O
+	O
»	O
«	O
[	O
¯	O
¬ð	O
ëlð	O
à·	O
»	O
+ª-£	O
{	O
¬	O
´4ú	O
û	O
¶eº	O
ª©	O
«	O
ýð	O
¬-¥	O
»	O
+¥©¿2ë	O
«	O
+	O
»	O
+ª-£	O
¼2¥	O
´4ú	O
´4ú	O
û	O
¶¶	O
however	O
,	O
since	O
the	O
statlog	O
project	O
was	O
completed	O
,	O
there	O
is	O
a	O
more	O
recent	O
version	O
of	O
c4.5	O
,	O
so	O
the	O
results	O
contained	O
in	O
this	O
book	O
may	O
not	O
be	O
exactly	O
reproducable	O
.	O
º	O
¹	O
á	O
ë	O
¹	O
á	O
¿	O
¶	O
à	O
º	O
á	O
ë	O
¹	O
¸	O
¿	O
ë	O
¹	O
º	O
±	O
¶	O
ø	O
á	O
¥	O
ë	O
¦	O
£	O
¹	O
¬	O
¸	O
¹	O
¶	O
õ	O
×	O
ê	O
¹	O
á	O
¬	O
£	O
á	O
º	O
º	O
º	O
ú	O
ê	O
¹	O
¯	O
º	O
¬	O
º	O
¶	O
£	O
ø	O
º	O
º	O
à	O
¹	O
¹	O
ë	O
¹	O
²	O
¹	O
c.	O
contributors	O
265	O
þá	O
þä	O
for	O
cn2	O
:	O
¶eº	O
¹¤¹	O
ª©	O
«	O
newid	O
and	O
cn2	O
are	O
available	O
from	O
robin	O
boswell	O
and	O
tim	O
niblett	O
,	O
respectively	O
at	O
the	O
turing	O
institute	O
,	O
george	O
house	O
,	O
36	O
north	O
hanover	O
street	O
,	O
glasgow	O
g1	O
2ad	O
,	O
uk	O
.	O
for	O
newid	O
:	O
´°ú	O
úúà	O
¶	O
¸	O
¹v¶e¹	O
ª¤ë	O
àßið	O
ª4	O
«	O
¥	O
£	O
¥©	O
«	O
[	O
¯	O
£	O
¼	O
«	O
2¥	O
´4ú	O
úúâ	O
ûãà	O
ãp£	O
¿	O
«	O
ªë-à·	O
»	O
+ª	O
£	O
«	O
p±ßið	O
¹¹	O
¶eº	O
ã+£	O
ª	O
£-¼2¥	O
¬bã2ë¤¥	O
¦i­	O
«	O
p±ßið	O
à	O
ª	O
»	O
¹¶	O
ðçæ	O
£à	O
¦-ã	O
×âå	O
¿2ë	O
à·£	O
¿	O
ë	O
áé	O
º°êe¹	O
å	O
&	O
¬	O
×éè	O
ðçë	O
¹4¶	O
¼¿¼í¯-ë¥	O
¬¤¬^	O
»	O
pª	O
£e¯	O
«	O
p±ið	O
;	O
è	O
×âå+¥	O
ì	O
¶e¹	O
¶eº	O
¶eº	O
×âî££+ª-£	O
ðuï+¥4ã2ë¥	O
¯	O
¥©	O
«	O
	O
»	O
¿	O
«	O
ä¯	O
¼2¥	O
¹é¶	O
¸	O
¸äº	O
¬-ð©¿2¥-£	O
ª¤ë	O
àöõb¯	O
¯-¥4	O
«	O
[	O
¯	O
±	O
«	O
ë	O
[	O
×éè	O
£ª	O
»	O
£-¿2ë	O
¼¿¼ª	O
£¤à	O
¼2¥	O
ì	O
¬ð	O
áä¹	O
´°úú	O
«	O
¿¼	O
£ª	O
»	O
£	O
¿2ë	O
¬ð	O
k-nn	O
is	O
still	O
under	O
development	O
.	O
for	O
all	O
datasets	O
,	O
except	O
the	O
satellite	B
image	I
dataset	O
,	O
ù	O
although	O
the	O
two	O
“	O
belgian	O
power	O
”	O
datasets	O
were	O
run	O
with	O
the	O
above	O
parameters	O
set	O
to	O
(	O
3,5000	O
)	O
and	O
3,2000	O
)	O
.	O
kohonen	O
was	O
written	O
by	O
j.	O
paul	O
,	O
dhamstr	O
.	O
20	O
,	O
w-5948	O
schmallenberg	O
,	O
germany	O
for	O
a	O
pc	O
with	O
an	O
attached	O
transputer	O
board	O
.	O
itrule	O
is	O
available	O
from	O
prof.	O
r.	O
goodman	O
,	O
california	O
institute	O
of	O
technology	O
,	O
electrical	O
engineering	O
,	O
mail	O
code	O
116-81	O
,	O
pasadena	O
,	O
ca	O
91125	O
,	O
usa	O
.	O
for	O
most	O
of	O
the	O
datasets	O
the	O
parameters	O
used	O
were	O
:	O
distance	O
was	O
scaled	O
in	O
a	O
class	O
dependent	O
manner	O
,	O
using	O
the	O
standard	O
deviation	O
.	O
further	O
details	O
can	O
be	O
obtained	O
from	O
c.	O
c.	O
taylor	O
,	O
department	O
of	O
statistics	O
,	O
university	O
of	O
leeds	O
,	O
leeds	O
ls2	O
9jt	O
,	O
uk	O
.	O
à8×ñ	O
»	O
+ª	O
£¥	O
¯¯	O
¶eº	O
«	O
p±·£	O
¿2ë	O
¬ð	O
´4ú	O
úú	O
.	O
c	O
contributors	O
this	O
volume	O
is	O
based	O
on	O
the	O
statlog	O
project	O
,	O
which	O
involved	O
many	O
workers	O
at	O
over	O
13	O
institutions	O
.	O
in	O
this	O
list	O
we	O
aim	O
to	O
include	O
those	O
who	O
contributed	O
to	O
the	O
project	O
and	O
the	O
institutions	O
at	O
which	O
they	O
were	O
primarily	O
based	O
at	O
that	O
time	O
.	O
g.	O
nakhaeizadeh	O
,	O
j.	O
graf	O
,	O
a.	O
merkel	O
,	O
h.	O
keller	O
,	O
laue	O
,	O
h.	O
ulrich	O
,	O
g.	O
kressel	O
,	O
daimler-	O
benz	O
ag	O
r.j.	O
henery	O
,	O
d.	O
michie	O
,	O
j.m.o	O
.	O
mitchell	O
,	O
a.i	O
.	O
sutherland	O
,	O
r.	O
king	O
,	O
s.	O
haque	O
,	O
c.	O
kay	O
,	O
d.	O
young	O
,	O
w.	O
buntine	O
,	O
b.	O
d.	O
ripley	O
,	O
university	O
of	O
strathclyde	O
s.h	O
.	O
muggleton	O
,	O
c.	O
feng	O
,	O
t.	O
niblett	O
,	O
r.	O
boswell	O
,	O
turing	O
institute	O
h.	O
perdrix	O
,	O
t.	O
brunet	O
,	O
t.	O
marre	O
,	O
j-j	O
cannat	O
,	O
isoft	O
j.	O
stender	O
,	O
p.	O
ristau	O
,	O
d.	O
picu	O
,	O
i.	O
chorbadjiev	O
,	O
c.	O
kennedy	O
,	O
g.	O
ruedke	O
,	O
f.	O
boehme	O
,	O
s.	O
schulze-kremer	O
,	O
brainware	O
gmbh	O
p.b	O
.	O
brazdil	O
,	O
j.	O
gama	O
,	O
l.	O
torgo	O
,	O
university	O
of	O
porto	O
r.	O
molina	O
,	O
n.	O
p´erez	O
de	O
la	O
blanca	O
,	O
s.	O
acid	O
,	O
l.	O
m.	O
de	O
campos	O
,	O
gonzalez	O
,	O
university	O
of	O
granada	O
f.	O
wysotzki	O
,	O
w.	O
mueller	O
,	O
der	O
,	O
buhlau	O
,	O
schmelowski	O
,	O
funke	O
,	O
villman	O
,	O
h.	O
herzheim	O
,	O
b.	O
schulmeister	O
,	O
fraunhofer	O
institute	O
þ	O
ü	O
º	O
º	O
£	O
¹	O
¬	O
¸	O
­	O
¸	O
£	O
¹	O
¬	O
¸	O
¶	O
£	O
º	O
­	O
¶	O
¶	O
£	O
º	O
«	O
º	O
´	O
¹	O
¹	O
£	O
¹	O
¹	O
¬	O
½	O
¾	O
º	O
º	O
æ	O
¹	O
¬	O
º	O
º	O
º	O
»	O
º	O
£	O
¹	O
¬	O
¸	O
¹	O
¹	O
ã	O
¹	O
­	O
º	O
¹	O
¹	O
½	O
¹	O
266	O
learning	O
to	O
control	O
dynamic	O
systems	O
t.	O
bueckle	O
,	O
c.	O
ziegler	O
,	O
m.	O
surauer	O
,	O
messerschmitt	O
boelkow	O
blohm	O
j.	O
paul	O
,	O
p.	O
von	O
goldammer	O
,	O
univeristy	O
of	O
l¨ubeck	O
c.c	O
.	O
taylor	O
,	O
x.	O
feng	O
,	O
university	O
of	O
leeds	O
r.	O
rohwer	O
,	O
m.	O
wynne-jones	O
,	O
aston	O
university	O
d.	O
spiegelhalter	O
,	O
mrc	O
,	O
cambridge	O
references	O
acid	O
,	O
s.	O
,	O
campos	O
,	O
l.	O
m.	O
d.	O
,	O
gonz´alez	O
,	O
a.	O
,	O
molina	O
,	O
r.	O
,	O
and	O
p´erez	O
de	O
la	O
blanca	O
,	O
n.	O
(	O
1991a	O
)	O
.	O
bayesian	O
learning	O
algorithms	O
in	O
castle	O
.	O
report	O
no	O
.	O
91-4-2	O
,	O
university	O
of	O
granada	O
.	O
acid	O
,	O
s.	O
,	O
campos	O
,	O
l.	O
m.	O
d.	O
,	O
gonz´alez	O
,	O
a.	O
,	O
molina	O
,	O
r.	O
,	O
and	O
p´erez	O
de	O
la	O
blanca	O
,	O
n.	O
(	O
1991b	O
)	O
.	O
castle	O
:	O
causal	O
structures	O
from	O
inductive	B
learning	I
.	O
release	O
2.0.	O
report	O
no	O
.	O
91-4-3	O
,	O
university	O
of	O
granada	O
.	O
agresti	O
,	O
a	O
.	O
(	O
1990	O
)	O
.	O
categorical	O
data	O
analysis	O
.	O
wiley	O
,	O
new	O
york	O
.	O
aha	O
,	O
d.	O
(	O
1992	O
)	O
.	O
generalising	O
case	O
studies	O
:	O
a	O
case	O
study	O
.	O
in	O
9th	O
int	O
.	O
conf	O
.	O
on	O
machine	O
learning	O
,	O
pages	O
1–10	O
,	O
san	O
mateo	O
,	O
cal	O
.	O
morgan	O
kaufmann	O
.	O
aha	O
,	O
d.	O
w.	O
,	O
kibler	O
,	O
d.	O
,	O
and	O
albert	O
,	O
m.	O
k.	O
(	O
1991	O
)	O
.	O
instance-based	B
learning	O
algorithms	O
.	O
machine	O
learning	O
,	O
6	O
(	O
1	O
)	O
:37–66	O
.	O
airtc92	O
(	O
1992	O
)	O
.	O
preprints	O
of	O
the	O
1992	O
ifac/ifip/imacs	O
international	O
symposium	O
on	O
artiﬁcial	O
intelligence	O
in	O
real-time	O
control	O
.	O
delft	O
,	O
the	O
netherlands	O
,	O
750	O
pages	O
.	O
aitchison	O
,	O
j.	O
and	O
aitken	O
,	O
c.	O
g.	O
g.	O
(	O
1976	O
)	O
.	O
multivariate	O
binary	O
discrimination	O
by	O
the	O
kernel	O
method	O
.	O
biometrika	O
,	O
63:413–420	O
.	O
al-attar	O
,	O
a	O
.	O
(	O
1991	O
)	O
.	O
structured	O
decision	O
tasks	O
methodology	O
.	O
attar	O
software	O
ltd.	O
,	O
new-	O
lands	O
house	O
,	O
newlands	O
road	O
,	O
leigh	O
,	O
lancs	O
.	O
aleksander	O
,	O
i.	O
,	O
thomas	O
,	O
w.	O
v.	O
,	O
and	O
bowden	O
,	O
p.	O
a	O
.	O
(	O
1984	O
)	O
.	O
wisard	O
:	O
a	O
radical	O
step	O
forward	B
in	O
image	O
recognition	O
.	O
sensor	O
review	O
,	O
4:120–124	O
.	O
anderson	O
,	O
c.	O
w.	O
(	O
1987	O
)	O
.	O
strategy	O
learning	O
with	O
multilayer	O
connectionist	O
representations	O
.	O
in	O
lengley	O
,	O
p.	O
,	O
editor	O
,	O
proceedings	O
of	O
the	O
4th	O
international	O
workshop	O
on	O
machine	O
learning	O
,	O
pages	O
103–114	O
.	O
morgan	O
kaufmann	O
.	O
anderson	O
,	O
c.	O
w.	O
and	O
miller	O
,	O
w.	O
t.	O
(	O
1990	O
)	O
.	O
challenging	O
control	O
problems	O
.	O
in	O
miller	O
,	O
w.	O
t.	O
,	O
sutton	O
,	O
r.	O
s.	O
,	O
and	O
werbos	O
,	O
p.	O
j.	O
,	O
editors	O
,	O
neural	O
networks	O
for	O
control	O
,	O
pages	O
475–510	O
.	O
the	O
mit	O
press	O
.	O
anderson	O
,	O
j.	O
a	O
.	O
(	O
1984	O
)	O
.	O
regression	O
and	O
ordered	O
categorical	O
variables	O
.	O
j.	O
r.	O
statist	O
.	O
soc	O
.	O
b	O
,	O
46:1–30	O
.	O
anderson	O
,	O
t.	O
w.	O
(	O
1958	O
)	O
.	O
an	O
introduction	O
to	O
multivariate	O
statistical	B
analysis	O
.	O
john	O
wiley	O
,	O
new	O
york	O
.	O
268	O
references	O
andrews	O
,	O
d.	O
f.	O
and	O
herzberg	O
,	O
a.	O
m.	O
(	O
1985	O
)	O
.	O
data	O
:	O
a	O
collection	O
of	O
problems	O
from	O
many	O
ﬁelds	O
for	O
the	O
student	O
and	O
research	O
worker	O
.	O
springer-verlag	O
,	O
new	O
york	O
.	O
arbab	O
,	O
b.	O
and	O
michie	O
,	O
d.	O
(	O
1988	O
)	O
.	O
generating	O
expert	O
rules	O
from	O
examples	O
in	O
prolog	O
.	O
in	O
hayes	O
,	O
j.	O
e.	O
,	O
michie	O
,	O
d.	O
,	O
and	O
richards	O
,	O
j.	O
,	O
editors	O
,	O
machine	O
intelligence	O
11	O
,	O
pages	O
289	O
–	O
304.	O
oxford	O
university	O
press	O
,	O
oxford	O
.	O
ash	O
,	O
t.	O
(	O
1989	O
)	O
.	O
dynamic	O
node	O
creation	O
in	O
back-propagation	O
networks	O
.	O
ics	O
report	O
8901	O
,	O
institute	O
of	O
cognitive	O
science	O
,	O
university	O
of	O
california	O
,	O
san	O
diego	O
,	O
la	O
jolla	O
,	O
california	O
92093	O
,	O
usa	O
.	O
˚astr˝om	O
,	O
k.	O
j	O
.	O
(	O
1991	O
)	O
.	O
intelligent	O
control	O
.	O
in	O
proceedings	O
of	O
the	O
ecc	O
91	O
european	O
control	O
conference	O
,	O
pages	O
2328–2339	O
.	O
grenoble	O
.	O
atlas	O
,	O
l.	O
,	O
connor	O
,	O
j.	O
,	O
and	O
park	O
,	O
d.	O
et	O
al..	O
(	O
1991	O
)	O
.	O
a	O
performance	O
comparison	O
of	O
trained	O
multi-layer	O
perceptrons	O
and	O
trained	O
classiﬁcation	B
trees	O
.	O
in	O
systems	O
,	O
man	O
and	O
cybernetics	O
:	O
proceedings	O
of	O
the	O
1989	O
ieee	O
international	O
conference	O
,	O
pages	O
915–920	O
,	O
cambridge	O
,	O
ma	O
.	O
hyatt	O
regency	O
.	O
bain	O
,	O
m.	O
(	O
1990	O
)	O
.	O
machine-learned	O
rule-based	O
control	O
.	O
in	O
grimble	O
,	O
m.	O
,	O
mcghee	O
,	O
j.	O
,	O
and	O
mowforth	O
,	O
p.	O
,	O
editors	O
,	O
knowledge-based	O
systems	O
in	O
industrial	O
control	O
,	O
pages	O
222–244	O
,	O
stevenage	O
.	O
peter	O
peregrinus	O
.	O
bain	O
,	O
m.	O
(	O
private	O
communication	O
)	O
.	O
learning	O
logical	O
exceptions	O
in	O
chess	O
.	O
phd	O
thesis	O
,	O
university	O
of	O
strathclyde	O
,	O
glasgow	O
.	O
banerji	O
,	O
r.	O
b	O
.	O
(	O
1980	O
)	O
.	O
artiﬁcial	O
intelligence	O
:	O
a	O
theoretical	O
approach	O
.	O
north	O
holland	O
,	O
new	O
york	O
.	O
barto	O
,	O
a.	O
g.	O
,	O
sutton	O
,	O
r.	O
s.	O
,	O
and	O
anderson	O
,	O
c.	O
w.	O
(	O
1983	O
)	O
.	O
neuronlike	O
adaptive	O
elements	O
that	O
can	O
solve	O
difﬁcult	O
learning	O
control	O
problems	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
and	O
cybernetics	O
,	O
smc-13	O
(	O
5	O
)	O
:834–846	O
.	O
begg	O
,	O
c.	O
b.	O
and	O
gray	O
,	O
r.	O
(	O
1984	O
)	O
.	O
calculation	O
of	O
polychotomous	O
logistic	O
regression	O
param-	O
eters	O
using	O
individualized	O
regressions	O
.	O
biometrika	O
,	O
71:11–18	O
.	O
bilbro	O
,	O
g.	O
and	O
den	O
bout	O
,	O
d.	O
v.	O
(	O
1992	O
)	O
.	O
maximum	O
entropy	O
and	O
learning	O
theory	O
.	O
neural	O
computation	O
,	O
4:839–853	O
.	O
bledsoe	O
,	O
w.	O
w.	O
(	O
1961	O
)	O
.	O
further	O
results	O
on	O
the	O
n-tuple	O
pattern	O
recognition	O
method	O
.	O
ire	O
trans	O
.	O
comp.	O
,	O
ec-10:96.	O
bledsoe	O
,	O
w.	O
w.	O
and	O
browning	O
,	O
i	O
.	O
(	O
1959	O
)	O
.	O
pattern	O
recognition	O
and	O
reading	O
by	O
machine	O
.	O
in	O
proceedings	O
of	O
the	O
eastern	O
joint	O
computer	O
conference	O
,	O
pages	O
232–255	O
,	O
boston	O
.	O
blyth	O
,	O
c.	O
r.	O
(	O
1959	O
)	O
.	O
note	O
on	O
estimating	O
information	O
.	O
annals	O
of	O
math	O
.	O
stats.	O
,	O
30:71–79	O
.	O
bonelli	O
,	O
p.	O
and	O
parodi	O
,	O
a	O
.	O
(	O
1991	O
)	O
.	O
an	O
efﬁcient	O
classiﬁer	B
system	O
and	O
its	O
experimental	O
comparisons	O
with	O
two	O
representative	O
learning	O
methods	O
on	O
three	O
medical	O
domains	O
.	O
in	O
icga-91	O
:	O
genetic	B
algorithms	I
:	O
proceedings	O
of	O
the	O
fourth	O
international	O
conference	O
,	O
pages	O
288–295	O
,	O
san	O
mateo	O
,	O
ca	O
.	O
morgan	O
kaufmann	O
.	O
booth	O
,	O
t.	O
h.	O
,	O
stein	O
,	O
j.	O
a.	O
,	O
hutchinson	O
,	O
m.	O
f.	O
,	O
and	O
nix	O
,	O
h.	O
a	O
.	O
(	O
1990	O
)	O
.	O
identifying	O
areas	O
within	O
a	O
country	O
climatically	O
suitable	O
for	O
particular	O
tree	O
species	O
:	O
an	O
example	B
using	O
zimbabwe	O
.	O
international	O
tree	O
crops	O
journal	O
,	O
6:1–16	O
.	O
bourlard	O
,	O
h.	O
and	O
wellekens	O
,	O
c.	O
j	O
.	O
(	O
1990	O
)	O
.	O
links	O
between	O
markov	O
models	O
and	O
multilayer	O
perceptrons	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
12:1–12	O
.	O
bratko	O
,	O
i	O
.	O
(	O
1983	O
)	O
.	O
generating	O
human-understandable	O
decision	O
rules	O
.	O
technical	B
report	O
,	O
ljubljana	O
university	O
,	O
slovenia	O
.	O
references	O
269	O
bratko	O
,	O
i	O
.	O
(	O
1991	O
)	O
.	O
qualitative	O
modelling	O
:	O
learning	O
and	O
control	O
.	O
in	O
proceedings	O
of	O
the	O
6th	O
czechoslovak	O
conference	O
on	O
artiﬁcial	O
intelligence	O
.	O
prague	O
.	O
bratko	O
,	O
i	O
.	O
(	O
1993	O
)	O
.	O
qualitative	O
reasoning	O
about	O
control	O
.	O
in	O
proceedings	O
of	O
the	O
etfa	O
’	O
93	O
conference	O
.	O
cairns	O
,	O
australia	O
.	O
bratko	O
,	O
i.	O
mozetic	O
,	O
i.	O
and	O
lavrac	O
,	O
l.	O
(	O
1989	O
)	O
.	O
kardio	O
:	O
a	O
study	O
in	O
deep	O
and	O
qualitative	O
knowledge	O
for	O
expert	O
systems	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
,	O
and	O
london	O
.	O
breiman	O
,	O
l.	O
and	O
friedman	O
,	O
j.	O
h.	O
(	O
1985	O
)	O
.	O
estimating	O
optimal	O
transformations	O
for	O
multi-	O
ple	O
regression	O
and	O
correlation	O
(	O
with	O
discussion	O
)	O
.	O
journal	O
of	O
the	O
american	O
statistical	B
association	O
(	O
jasa	O
)	O
,	O
80	O
no	O
.	O
391:580–619	O
.	O
breiman	O
,	O
l.	O
,	O
friedman	O
,	O
j.	O
h.	O
,	O
olshen	O
,	O
r.	O
a.	O
,	O
and	O
stone	O
,	O
c.	O
j	O
.	O
(	O
1984	O
)	O
.	O
classiﬁcation	B
and	O
regression	O
trees	O
.	O
wadsworth	O
and	O
brooks	O
,	O
monterey	O
,	O
ca	O
.	O
breiman	O
,	O
l.	O
,	O
meisel	O
,	O
w.	O
,	O
and	O
purcell	O
,	O
e.	O
(	O
1977	O
)	O
.	O
variable	O
kernel	O
estimates	O
of	O
multivariate	O
densities	O
.	O
technometrics	O
,	O
19:135–144	O
.	O
bretzger	O
,	O
t.	O
m.	O
(	O
1991	O
)	O
.	O
die	O
anwendung	O
statistischer	O
verfahren	O
zur	O
risikofruherkennung	O
bei	O
dispositionskrediten	O
.	O
phd	O
thesis	O
,	O
universitat	O
hohenheim	O
.	O
broomhead	O
,	O
d.	O
s.	O
and	O
lowe	O
,	O
d.	O
(	O
1988	O
)	O
.	O
multi-variable	O
functional	O
interpolation	O
and	O
adaptive	O
networks	O
.	O
complex	O
systems	O
,	O
2:321–355	O
.	O
bruner	O
,	O
j.	O
s.	O
,	O
goodnow	O
,	O
j.	O
j.	O
,	O
and	O
austin	O
,	O
g.	O
a	O
.	O
(	O
1956	O
)	O
.	O
a	O
study	O
of	O
thinking	O
.	O
wiley	O
,	O
new	O
york	O
.	O
buntine	O
,	O
w.	O
ind	O
package	O
of	O
machine	O
learning	O
algorithms	O
ind	O
1.0.	O
technical	B
report	O
ms	O
244-17	O
,	O
research	O
institute	O
for	O
advanced	O
computer	O
science	O
,	O
nasa	O
ames	O
research	O
center	O
,	O
moffett	O
field	O
,	O
ca	O
94035.	O
buntine	O
,	O
w.	O
(	O
1988	O
)	O
.	O
generalized	O
subsumption	O
and	O
its	O
applications	O
to	O
induction	O
and	O
redun-	O
dancy	O
.	O
artiﬁcial	O
intelligence	O
,	O
36:149–176	O
.	O
buntine	O
,	O
w.	O
(	O
1992	O
)	O
.	O
learning	O
classiﬁcation	O
trees	O
.	O
statistics	O
and	O
computing	O
,	O
2:63–73	O
.	O
camacho	O
,	O
r.	O
and	O
michie	O
,	O
d.	O
(	O
1992	O
)	O
.	O
an	O
engineering	O
model	O
of	O
subcognition	O
.	O
in	O
proceedings	O
of	O
the	O
issek	O
workshop	O
1992.	O
bled	O
,	O
slovenia	O
.	O
carpenter	O
,	O
b.	O
e.	O
and	O
doran	O
,	O
r.	O
w.	O
,	O
editors	O
(	O
1986	O
)	O
.	O
a.	O
m.	O
turing	O
’	O
s	O
ace	O
report	O
and	O
other	O
papers	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
carter	O
,	O
c.	O
and	O
catlett	O
,	O
j	O
.	O
(	O
1987	O
)	O
.	O
assessing	O
credit	O
card	O
applications	O
using	O
machine	O
learning	O
.	O
ieee	O
expert	O
:	O
intelligent	O
systems	O
and	O
their	O
applications	O
,	O
2:71–79	O
.	O
cestnik	O
,	O
b.	O
and	O
bratko	O
,	O
i	O
.	O
(	O
1991	O
)	O
.	O
on	O
estimating	O
probabilities	O
in	O
tree	O
pruning	B
.	O
in	O
ewsl	O
‘	O
91	O
,	O
porto	O
,	O
portugal	O
,	O
1991	O
,	O
berlin	O
.	O
springer-verlag	O
.	O
cestnik	O
,	O
b.	O
,	O
kononenko	O
,	O
i.	O
,	O
and	O
bratko	O
,	O
i	O
.	O
(	O
1987	O
)	O
.	O
assistant	O
86	O
:	O
a	O
knowledge-elicitation	O
tool	O
for	O
sophisticated	O
users	O
.	O
in	O
progress	O
in	O
machine	O
learning	O
:	O
proceedings	O
of	O
ewsl-87	O
,	O
pages	O
31–45	O
,	O
bled	O
,	O
yugoslavia	O
.	O
sigma	O
press	O
.	O
cherkaoui	O
,	O
o.	O
and	O
cleroux	O
,	O
r.	O
(	O
1991	O
)	O
.	O
comparative	O
study	O
of	O
six	O
discriminant	O
analysis	O
procedures	O
for	O
mixtures	O
of	O
variables	O
.	O
in	O
proceedings	O
of	O
interface	O
conference	O
1991.	O
morgan	O
kaufmann	O
.	O
clark	O
,	O
l.	O
a.	O
and	O
pregibon	O
,	O
d.	O
(	O
1992	O
)	O
.	O
tree-based	O
models	O
.	O
in	O
chambers	O
,	O
j.	O
and	O
hastie	O
,	O
t.	O
,	O
editors	O
,	O
statistical	B
models	O
in	O
s.	O
wadsworth	O
&	O
brooks	O
,	O
paciﬁc	O
grove	O
,	O
california	O
.	O
clark	O
,	O
p.	O
and	O
boswell	O
,	O
r.	O
(	O
1991	O
)	O
.	O
rule	O
induction	O
with	O
cn2	O
:	O
some	O
recent	O
improvements	O
.	O
in	O
ewsl	O
’	O
91	O
,	O
porto	O
,	O
portugal	O
,	O
1991	O
,	O
pages	O
151–163	O
,	O
berlin	O
.	O
springer-verlag	O
.	O
270	O
references	O
clark	O
,	O
p.	O
and	O
niblett	O
,	O
t.	O
(	O
1988	O
)	O
.	O
the	O
cn2	O
induction	O
algorithm	O
.	O
machine	O
learning	O
,	O
3:261–	O
283.	O
clarke	O
,	O
w.	O
r.	O
,	O
lachenbruch	O
,	O
p.	O
a.	O
,	O
and	O
brofﬁtt	O
,	O
b	O
.	O
(	O
1979	O
)	O
.	O
how	O
nonnormality	O
affects	O
the	O
quadratic	O
discriminant	O
function	O
.	O
comm	O
.	O
statist	O
.	O
—	O
theory	O
meth.	O
,	O
it-16:41–46	O
.	O
connell	O
,	O
m.	O
e.	O
and	O
utgoff	O
,	O
p.	O
e.	O
(	O
1987	O
)	O
.	O
learning	O
to	O
control	O
a	O
dynamic	O
physical	O
system	O
.	O
in	O
proceedings	O
of	O
the	O
6th	O
national	O
conference	O
on	O
artiﬁcial	O
intelligence	O
,	O
pages	O
456–459	O
.	O
morgan	O
kaufmann	O
.	O
cooper	O
,	O
g.	O
f.	O
(	O
1984	O
)	O
.	O
nestro	O
:	O
a	O
computer-based	O
medical	O
diagnostic	O
that	O
integrates	O
causal	O
and	O
probabilistic	O
knowledge	O
.	O
report	O
no	O
.	O
4	O
,	O
hpp-84-48	O
,	O
stanford	O
university	O
,	O
stanford	O
california	O
.	O
cooper	O
,	O
g.	O
f.	O
and	O
herkovsits	O
,	O
e.	O
(	O
1991	O
)	O
.	O
a	O
bayesian	O
method	O
for	O
the	O
induction	O
of	O
proba-	O
bilistic	O
networks	O
from	O
data	O
.	O
technical	B
report	O
ksl-91-02	O
,	O
stanford	O
university	O
.	O
cover	O
,	O
t.	O
m.	O
(	O
1965	O
)	O
.	O
geometrical	O
and	O
statistical	B
properties	O
of	O
systems	O
of	O
linear	O
inequalities	O
with	O
applications	O
in	O
pattern	O
recognition	O
.	O
ieee	O
transactions	O
on	O
electronic	O
computers	O
,	O
14:326–334	O
.	O
cox	O
,	O
d.	O
r.	O
(	O
1966	O
)	O
.	O
some	O
procedures	O
associated	O
with	O
the	O
logistic	O
qualitative	O
response	O
curve	O
.	O
in	O
david	O
,	O
f.	O
n.	O
,	O
editor	O
,	O
research	O
papers	O
on	O
statistics	O
:	O
festschrift	O
for	O
j.	O
neyman	O
,	O
pages	O
55–77	O
.	O
john	O
wiley	O
,	O
new	O
york	O
.	O
crawford	O
,	O
s.	O
l.	O
(	O
1989	O
)	O
.	O
extensions	O
to	O
the	O
cart	O
algorithm	O
.	O
int	O
.	O
j.	O
man-machine	O
studies	O
,	O
31:197–217	O
.	O
cutsem	O
van	O
,	O
t.	O
,	O
wehenkel	O
,	O
l.	O
,	O
pavella	O
,	O
m.	O
,	O
heilbronn	O
,	O
b.	O
,	O
and	O
goubin	O
,	O
m.	O
(	O
1991	O
)	O
.	O
decision	O
trees	O
for	O
detecting	O
emergency	O
voltage	O
conditions	O
.	O
in	O
second	O
international	O
workshop	O
on	O
bulk	O
power	O
system	O
voltage	O
phenomena	O
-	O
voltage	O
stability	O
and	O
security	O
,	O
pages	O
229	O
–	O
240	O
,	O
usa	O
.	O
mchenry	O
.	O
davies	O
,	O
e.	O
r.	O
(	O
1988	O
)	O
.	O
training	O
sets	O
and	O
a	O
priori	O
probabilities	O
with	O
the	O
nearest	O
neighbour	O
method	O
of	O
pattern	O
recognition	O
.	O
pattern	O
recognition	O
letters	O
,	O
8:11–13	O
.	O
day	O
,	O
n.	O
e.	O
and	O
kerridge	O
,	O
d.	O
f.	O
(	O
1967	O
)	O
.	O
a	O
general	O
maximum	O
likelihood	O
discriminant	O
.	O
biometrics	O
,	O
23:313–324	O
.	O
devijver	O
,	O
p.	O
a.	O
and	O
kittler	O
,	O
j.	O
v.	O
(	O
1982	O
)	O
.	O
pattern	O
recognition	O
.	O
a	O
statistical	B
approach	O
.	O
prentice	O
hall	O
,	O
englewood	O
cliffs	O
.	O
djeroski	O
,	O
s.	O
,	O
cestnik	O
,	O
b.	O
,	O
and	O
petrovski	O
,	O
i	O
.	O
(	O
1983	O
)	O
.	O
using	O
the	O
m-estimate	O
in	O
rule	O
induction	O
.	O
j.	O
computing	O
and	O
inf	O
.	O
technology	O
,	O
1:37	O
–46	O
.	O
dubes	O
,	O
r.	O
and	O
jain	O
,	O
a.	O
k.	O
(	O
1976	O
)	O
.	O
clustering	O
techniques	O
:	O
the	O
user	O
’	O
s	O
dilemma	O
.	O
pattern	O
recognition	O
,	O
8:247–260	O
.	O
duin	O
,	O
r.	O
p.	O
w.	O
(	O
1976	O
)	O
.	O
on	O
the	O
choice	O
of	O
smoothing	O
parameters	O
for	O
parzen	O
estimators	O
of	O
probability	O
density	O
functions	O
.	O
ieee	O
transactions	O
on	O
computers	O
,	O
c-25:1175–1179	O
.	O
dvorak	O
,	O
d.	O
l.	O
(	O
1987	O
)	O
.	O
expert	O
systems	O
for	O
monitoring	O
and	O
control	O
.	O
technical	B
report	O
technical	B
report	O
ai87-55	O
,	O
artiﬁcial	O
intelligence	O
laboratory	O
,	O
the	O
university	O
of	O
texas	O
at	O
austin	O
.	O
dˇzeroski	O
,	O
s.	O
(	O
1989	O
)	O
.	O
control	O
of	O
inverted	O
pendulum	O
.	O
b.sc	O
.	O
thesis	O
,	O
faculty	O
of	O
electrical	O
engineering	O
and	O
computer	O
science	O
,	O
university	O
of	O
ljubljana	O
(	O
in	O
slovenian	O
)	O
.	O
efron	O
,	O
b	O
.	O
(	O
1983	O
)	O
.	O
estimating	O
the	O
error	O
rate	O
of	O
a	O
prediction	O
rule	O
:	O
improvements	O
on	O
cross-	O
validation	O
.	O
j.	O
amer	O
.	O
stat	O
.	O
ass.	O
,	O
78:316–331	O
.	O
references	O
271	O
enas	O
,	O
g.	O
g.	O
and	O
choi	O
,	O
s.	O
c.	O
(	O
1986	O
)	O
.	O
choice	O
of	O
the	O
smoothing	O
parameter	O
and	O
efﬁciency	O
of	O
nearest	O
neighbour	O
classiﬁcation	B
.	O
comput	O
.	O
math	O
.	O
applic.	O
,	O
12a:235–244	O
.	O
enterline	O
,	O
l.	O
l.	O
(	O
1988	O
)	O
.	O
strategic	O
requirements	O
for	O
total	O
facility	O
automation	O
.	O
control	O
engi-	O
the	O
neering	O
,	O
2:9–12	O
.	O
ersoy	O
,	O
o.	O
k.	O
and	O
hong	O
,	O
d.	O
(	O
1991	O
)	O
.	O
parallel	O
,	O
self-organizing	O
,	O
hierarchical	O
neural	O
networks	O
for	O
vision	O
and	O
systems	O
control	O
.	O
in	O
kaynak	O
,	O
o.	O
,	O
editor	O
,	O
intelligent	O
motion	O
control	O
:	O
proceedings	O
of	O
the	O
ieee	O
international	O
workshop	O
,	O
new	O
york	O
.	O
ieee	O
.	O
fahlman	O
,	O
s.	O
e.	O
(	O
1988a	O
)	O
.	O
an	O
empirical	O
study	O
of	O
learning	O
speed	O
in	O
back-propagation	O
.	O
tech-	O
nical	O
report	O
cmu-cs-88-162	O
,	O
carnegie	O
mellon	O
university	O
,	O
usa	O
.	O
fahlman	O
,	O
s.	O
e.	O
(	O
1988b	O
)	O
.	O
faster	O
learning	O
variation	O
on	O
back-propagation	O
:	O
an	O
empirical	O
study	O
.	O
in	O
proccedings	O
of	O
the	O
1988	O
connectionist	O
models	O
summer	O
school	O
.	O
morgan	O
kaufmann	O
.	O
fahlman	O
,	O
s.	O
e.	O
(	O
1991a	O
)	O
.	O
the	O
cascade-correlation	O
learning	O
algorithm	O
on	O
the	O
monk	O
’	O
s	O
prob-	O
lems	O
.	O
in	O
thrun	O
,	O
s.	O
,	O
bala	O
,	O
j.	O
,	O
bloedorn	O
,	O
e.	O
,	O
and	O
bratko	O
,	O
i.	O
,	O
editors	O
,	O
the	O
monk	O
’	O
s	O
problems	O
-	O
a	O
performance	O
comparison	O
of	O
different	O
learning	O
algorithms	O
,	O
pages	O
107–112	O
.	O
carnegie	O
mellon	O
university	O
,	O
computer	O
science	O
department	O
.	O
fahlman	O
,	O
s.	O
e.	O
(	O
1991b	O
)	O
.	O
the	O
recurrent	O
cascade-correlation	O
architecture	O
.	O
technical	B
report	O
cmu-cs-91-100	O
,	O
carnegie	O
mellon	O
university	O
.	O
fahlman	O
,	O
s.	O
e.	O
and	O
lebi`ere	O
,	O
c.	O
(	O
1990	O
)	O
.	O
the	O
cascade	O
correlation	O
learning	O
architecture	O
.	O
in	O
tourzetsky	O
,	O
d.	O
s.	O
,	O
editor	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
2	O
,	O
pages	O
524–532	O
.	O
morgan	O
kaufmann	O
.	O
fahrmeir	O
,	O
l.	O
,	O
haussler	O
,	O
w.	O
,	O
and	O
tutz	O
,	O
g.	O
(	O
1984	O
)	O
.	O
diskriminanzanalyse	O
.	O
in	O
fahrmeir	O
,	O
l.	O
and	O
hamerle	O
,	O
a.	O
,	O
editors	O
,	O
multivariate	O
statistische	O
verfahren	O
.	O
verlag	O
de	O
gruyter	O
,	O
berlin	O
.	O
fienberg	O
,	O
s.	O
(	O
1980	O
)	O
.	O
the	O
analysis	O
of	O
cross-classiﬁed	O
categorical	O
data	O
.	O
mit	O
press	O
,	O
cam-	O
bridge	O
,	O
mass	O
.	O
fisher	O
,	O
d.	O
h.	O
and	O
mckusick	O
,	O
k.	O
b	O
.	O
(	O
1989a	O
)	O
.	O
an	O
empirical	O
comparison	O
of	O
id3	O
and	O
back-	O
propagation	O
(	O
vol	O
1	O
)	O
.	O
in	O
ijcai	O
89	O
,	O
pages	O
788–793	O
,	O
san	O
mateo	O
,	O
ca	O
.	O
morgan	O
kaufmann	O
.	O
fisher	O
,	O
d.	O
h.	O
and	O
mckusick	O
,	O
k.	O
b.	O
et	O
al..	O
(	O
1989b	O
)	O
.	O
processing	O
issues	O
in	O
comparisons	O
of	O
symbolic	O
and	O
connectionist	O
learning	O
systems	O
.	O
in	O
spatz	O
,	O
b.	O
,	O
editor	O
,	O
proceedings	O
of	O
the	O
sixth	O
international	O
workshop	O
on	O
machine	O
learning	O
,	O
cornell	O
university	O
,	O
ithaca	O
,	O
new	O
york	O
,	O
pages	O
169–173	O
,	O
san	O
mateo	O
,	O
ca	O
.	O
morgan	O
kaufmann	O
.	O
fisher	O
,	O
r.	O
a	O
.	O
(	O
1936	O
)	O
.	O
the	O
use	O
of	O
multiple	O
measurements	O
in	O
taxonomic	O
problems	O
.	O
annals	O
of	O
eugenics	O
,	O
7:179–188	O
.	O
fisher	O
,	O
r.	O
a	O
.	O
(	O
1938	O
)	O
.	O
the	O
statistical	B
utilisation	O
of	O
multiple	O
measurements	O
.	O
ann	O
.	O
eugen.	O
,	O
8:376–386	O
.	O
fix	O
,	O
e.	O
and	O
hodges	O
,	O
j.	O
l.	O
(	O
1951	O
)	O
.	O
discriminatory	O
analysis	O
,	O
nonparametric	O
estimation	O
:	O
consistency	O
properties	O
.	O
report	O
no	O
.	O
4	O
,	O
project	O
no	O
.	O
21-49-004	O
,	O
uasf	O
school	O
of	O
aviation	O
medicine	O
,	O
randolph	O
field	O
,	O
texas	O
.	O
frean	O
,	O
m.	O
(	O
1990a	O
)	O
.	O
short	O
paths	O
and	O
small	O
nets	O
:	O
optimizing	O
neural	O
computation	O
.	O
phd	O
thesis	O
,	O
university	O
of	O
edinburgh	O
,	O
uk	O
.	O
frean	O
,	O
m.	O
(	O
1990b	O
)	O
.	O
the	O
upstart	O
algorithm	O
:	O
a	O
method	O
for	O
constructing	O
and	O
training	O
feed-	O
forward	B
neural	O
networks	O
.	O
neural	O
computation	O
,	O
2:198–209	O
.	O
frey	O
,	O
p.	O
w.	O
and	O
slate	O
,	O
d.	O
j	O
.	O
(	O
1991	O
)	O
.	O
letter	B
recognition	I
using	O
holland-style	O
adaptive	O
classiﬁers	O
.	O
machine	O
learning	O
,	O
6	O
.	O
272	O
references	O
friedman	O
,	O
j	O
.	O
(	O
1989	O
)	O
.	O
regularized	O
discriminant	O
analysis	O
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
84:165–	O
175.	O
friedman	O
,	O
j.	O
h.	O
(	O
1984	O
)	O
.	O
smart	O
user	O
’	O
s	O
guide	O
.	O
tech	O
.	O
report	O
1.	O
,	O
laboratory	O
of	O
computational	O
statistics	O
,	O
department	O
of	O
statistics	O
,	O
stanford	O
university	O
.	O
friedman	O
,	O
j.	O
h.	O
(	O
1991	O
)	O
.	O
multivariate	O
adaptive	O
regression	O
splines	O
(	O
with	O
discussion	O
)	O
.	O
annals	O
of	O
statistics	O
,	O
19:1–141	O
.	O
friedman	O
,	O
j.	O
h.	O
and	O
stuetzle	O
,	O
w.	O
(	O
1981	O
)	O
.	O
projection	O
pursuit	O
regression	O
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
76:817–823	O
.	O
fukunaga	O
,	O
k.	O
(	O
1990	O
)	O
.	O
introduction	O
to	O
statistical	B
pattern	O
recognition	O
.	O
academic	O
press	O
,	O
2nd	O
edition	O
.	O
fukunaga	O
,	O
k.	O
and	O
narendra	O
,	O
p.	O
m.	O
(	O
1975	O
)	O
.	O
a	O
branch	O
and	O
bound	O
algorithm	O
for	O
computing	O
nearest	O
neighbours	O
.	O
ieee	O
trans	O
.	O
comput.	O
,	O
c-25:917–922	O
.	O
networks	O
.	O
neural	O
networks	O
,	O
2:183.	O
funahashi	O
,	O
k.	O
(	O
1989	O
)	O
.	O
on	O
the	O
approximate	O
realization	O
of	O
continuous	O
mappings	O
by	O
neural	O
fung	O
,	O
r.	O
m.	O
and	O
crawford	O
,	O
s.	O
l.	O
(	O
1991	O
)	O
.	O
constructor	O
:	O
a	O
system	O
for	O
the	O
induction	O
of	O
probabilistic	O
models	O
.	O
in	O
proceedings	O
eighth	O
of	O
the	O
conference	O
on	O
artiﬁcial	O
intelligence	O
,	O
pages	O
762–769	O
,	O
boston	O
,	O
massachussetts	O
.	O
gallant	O
,	O
s.	O
i	O
.	O
(	O
1985	O
)	O
.	O
the	O
pocket	O
algorithm	O
for	O
perceptron	O
learning	O
.	O
technical	B
report	O
sg-85-20	O
,	O
northeastern	O
university	O
college	O
of	O
computer	O
science	O
,	O
usa	O
.	O
gates	O
,	O
g.	O
w.	O
(	O
1972	O
)	O
.	O
the	O
reduced	O
nearest	O
neighbour	O
rule	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
it-18:431.	O
geman	O
,	O
s.	O
(	O
1992	O
)	O
.	O
neural	O
networks	O
and	O
the	O
bias/variance	O
dilemma	O
.	O
neural	O
computation	O
,	O
4:1–58	O
.	O
geva	O
,	O
s.	O
and	O
sitte	O
,	O
j	O
.	O
(	O
1993a	O
)	O
.	O
boxes	O
revisited	O
.	O
in	O
proceedings	O
of	O
the	O
icann	O
’	O
93	O
.	O
amster-	O
dam	O
.	O
geva	O
,	O
s.	O
and	O
sitte	O
,	O
j	O
.	O
(	O
1993b	O
)	O
.	O
the	O
cart-pole	O
experiment	O
as	O
a	O
benchmark	O
for	O
trainable	O
controllers	O
.	O
submitted	O
to	O
ieee	O
control	O
systems	O
magazine	O
.	O
gilbert	O
,	O
e.	O
s.	O
(	O
1969	O
)	O
.	O
the	O
effect	O
of	O
unequal	O
variance	O
covariance	O
matrices	O
on	O
ﬁsher	O
’	O
s	O
linear	O
discriminant	O
function	O
.	O
biometrics	O
,	O
25:505–515	O
.	O
glymour	O
,	O
c.	O
,	O
scheines	O
,	O
r.	O
,	O
spirtes	O
,	O
p.	O
,	O
and	O
kelley	O
,	O
k.	O
(	O
1987	O
)	O
.	O
discovering	O
causal	O
structures	O
statistics	O
and	O
search	O
.	O
academic	O
press	O
,	O
new	O
york	O
.	O
goldberg	O
,	O
d.	O
e.	O
(	O
1989	O
)	O
.	O
genetic	B
algorithms	I
in	O
search	O
,	O
optimization	O
and	O
machine	O
learning	O
.	O
addison-wesley	O
.	O
good	O
,	O
i.	O
j	O
.	O
(	O
1950	O
)	O
.	O
probability	O
and	O
the	O
weighing	O
of	O
evidence	O
.	O
grifﬁn	O
,	O
london	O
.	O
goodman	O
,	O
r.	O
m.	O
and	O
smyth	O
,	O
p.	O
(	O
1989	O
)	O
.	O
the	O
induction	O
of	O
probabilistic	O
rule	O
sets	O
-	O
the	O
itrule	O
algorithm	O
.	O
in	O
spatz	O
,	O
b.	O
,	O
editor	O
,	O
proceedings	O
of	O
the	O
sixth	O
international	O
workshop	O
on	O
machine	O
learning.	O
,	O
pages	O
129	O
–	O
132	O
,	O
san	O
mateo	O
,	O
ca	O
.	O
morgan	O
kaufmann	O
.	O
gorman	O
,	O
r.	O
p.	O
and	O
sejnowski	O
,	O
t.	O
j	O
.	O
(	O
1988	O
)	O
.	O
analysis	O
of	O
hidden	B
units	O
in	O
a	O
layered	O
network	O
trained	O
to	O
classify	O
sonar	O
targets	O
.	O
neural	O
networks	O
,	O
1	O
(	O
part	O
1	O
)	O
:75–89	O
.	O
habbema	O
,	O
j.	O
d.	O
f.	O
,	O
hermans	O
,	O
j.	O
,	O
and	O
van	O
der	O
burght	O
,	O
a.	O
t.	O
(	O
1974	O
)	O
.	O
cases	O
of	O
doubt	O
in	O
allocation	O
problems	O
.	O
biometrika	O
,	O
61:313–324	O
.	O
hampshire	O
,	O
j.	O
and	O
pearlmuter	O
,	O
b	O
.	O
(	O
1990	O
)	O
.	O
equivalence	O
proofs	O
for	O
the	O
multilayer	O
perceptron	O
classiﬁer	B
and	O
the	O
bayes	O
discriminant	O
function	O
.	O
in	O
proceedings	O
of	O
the	O
1988	O
connectionist	O
models	O
summer	O
school	O
,	O
san	O
mateo	O
ca	O
.	O
morgan	O
kaufmann	O
.	O
	O
references	O
273	O
hand	O
,	O
d.	O
j	O
.	O
(	O
1981	O
)	O
.	O
discrimination	O
and	O
classiﬁcation	B
.	O
john	O
wiley	O
,	O
chichester	O
.	O
hand	O
,	O
d.	O
j.	O
and	O
batchelor	O
,	O
b.	O
g.	O
(	O
1978	O
)	O
.	O
an	O
edited	O
nearest	O
neighbour	O
rule	O
.	O
information	O
sciences	O
,	O
14:171–180	O
.	O
hanson	O
,	O
s.	O
j	O
.	O
(	O
1990	O
)	O
.	O
meiosis	O
networks	O
.	O
in	O
tourzetsky	O
,	O
d.	O
s.	O
,	O
editor	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
2	O
,	O
pages	O
533–541	O
.	O
morgan	O
kaufmann	O
.	O
hart	O
,	O
p.	O
e.	O
(	O
1968	O
)	O
.	O
the	O
condensed	O
nearest	O
neighbour	O
rule	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
it-14:515–516	O
.	O
h¨aussler	O
,	O
w.	O
m.	O
(	O
1979	O
)	O
.	O
empirische	O
ergebnisse	O
zur	O
diskriminationsverfahren	O
bei	O
kred-	O
itscoringsystemen	O
.	O
zeitschrift	O
fur	O
operations	O
research	O
.	O
serie	O
b	O
,	O
23:191–210	O
.	O
h¨aussler	O
,	O
w.	O
m.	O
(	O
1981a	O
)	O
.	O
methoden	O
der	O
punktebewertung	O
fur	O
kreditscoringsysteme	O
.	O
zeitschrift	O
fur	O
operations	O
research	O
,	O
25:79–94	O
.	O
h¨aussler	O
,	O
w.	O
m.	O
(	O
1981b	O
)	O
.	O
punktebewertung	O
bei	O
kreditscoringsystemen	O
.	O
knapp	O
,	O
frankfurt	O
.	O
hebb	O
,	O
d.	O
o	O
.	O
(	O
1949	O
)	O
.	O
the	O
organisation	O
of	O
behaviour	O
.	O
john	O
wiley	O
and	O
sons	O
.	O
hecht-nielsen	O
,	O
r.	O
(	O
1989	O
)	O
.	O
neurocomputing	O
.	O
addison-wesley	O
,	O
reading	O
,	O
mass	O
.	O
herkovsits	O
,	O
e.	O
and	O
cooper	O
,	O
g.	O
f.	O
(	O
1990	O
)	O
.	O
kutat´o	O
:	O
an	O
entropy-driven	O
system	O
for	O
the	O
con-	O
struction	O
of	O
probabilistic	O
expert	O
systems	O
from	O
databases	O
.	O
report	O
ksl-90-22	O
,	O
stanford	O
university	O
.	O
hermans	O
,	O
j.	O
,	O
habbema	O
,	O
j.	O
d.	O
f.	O
,	O
kasanmoentalib	O
,	O
t.	O
k.	O
d.	O
,	O
and	O
raatgeven	O
,	O
j.	O
w.	O
(	O
1982	O
)	O
.	O
manual	O
for	O
the	O
alloc80	O
discriminant	O
analysis	O
program	O
.	O
leiden	O
,	O
the	O
netherlands	O
.	O
hertz	O
,	O
j.	O
,	O
krogh	O
,	O
a.	O
,	O
and	O
palmer	O
,	O
r.	O
(	O
1991	O
)	O
.	O
introduction	O
to	O
the	O
theory	O
of	O
neural	O
compu-	O
tation	O
.	O
addison-wesley	O
.	O
higonnet	O
,	O
r.	O
a.	O
and	O
grea	O
,	O
r.	O
a	O
.	O
(	O
1958	O
)	O
.	O
logical	O
design	O
of	O
electrical	O
circuits	O
.	O
mcgraw-hill	O
book	O
co.	O
ltd.	O
hill	O
,	O
m.	O
(	O
1982	O
)	O
.	O
correspondence	O
analysis	O
.	O
in	O
encyclopaedia	O
of	O
statistical	B
sciences	O
,	O
vol-	O
ume	O
2	O
,	O
pages	O
204–210	O
.	O
wiley	O
,	O
new	O
york	O
.	O
hinton	O
,	O
g.	O
e.	O
,	O
rumelhart	O
,	O
d.	O
e.	O
,	O
and	O
williams	O
,	O
r.	O
j	O
.	O
(	O
1985	O
)	O
.	O
learning	O
internal	O
repre-	O
sentations	O
by	O
back-propagating	O
errors	O
.	O
in	O
rumelhart	O
,	O
d.	O
e.	O
,	O
mcclelland	O
,	O
j.	O
l.	O
,	O
and	O
the	O
pdp	O
research	O
group	O
,	O
editors	O
,	O
parallel	O
distributed	O
processing	O
:	O
explorations	O
in	O
the	O
microstructure	O
of	O
cognition	O
,	O
volume	O
1	O
,	O
chapter	O
8.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
hinton	O
,	O
g.	O
e.	O
,	O
rumelhart	O
,	O
d.	O
e.	O
,	O
and	O
williams	O
,	O
r.	O
j	O
.	O
(	O
1986	O
)	O
.	O
learning	O
representatinos	O
by	O
back-propagating	O
errors	O
.	O
nature	O
,	O
323:533–536	O
.	O
hoehfeld	O
,	O
m.	O
and	O
fahlman	O
,	O
s.	O
e.	O
(	O
1991	O
)	O
.	O
learning	O
with	O
limited	O
precision	O
using	O
the	O
cascade-	O
correlation	O
algorithm	O
.	O
technical	B
report	O
cmu-cs-91-130	O
,	O
carnegie	O
mellon	O
university	O
.	O
hofmann	O
,	O
h.	O
j	O
.	O
(	O
1990	O
)	O
.	O
die	O
anwendung	O
des	O
cart-verfahrens	O
zur	O
statistischen	O
bonitatsanalyse	O
von	O
konsumentenkrediten	O
.	O
zeitschrift	O
fur	O
betriebswirtschaft	O
,	O
60:941–962	O
.	O
højsgaard	O
,	O
s.	O
,	O
skjøth	O
,	O
f.	O
,	O
and	O
thiesson	O
,	O
b.	O
user	O
’	O
s	O
guide	O
to	O
bifrost	O
.	O
aalborg	O
university	O
,	O
aalborg	O
,	O
denmark	O
.	O
holland	O
,	O
j.	O
h.	O
(	O
1975a	O
)	O
.	O
adaptation	O
in	O
natural	O
and	O
artiﬁcial	O
systems	O
.	O
university	O
of	O
michigan	O
press	O
,	O
ann	O
arbor	O
,	O
michigan	O
.	O
holland	O
,	O
j.	O
h.	O
(	O
1975b	O
)	O
.	O
adaptation	O
in	O
natural	O
and	O
artiﬁcial	O
systems	O
.	O
university	O
of	O
michi-	O
gan	O
press	O
,	O
ann	O
arbor	O
,	O
mi	O
.	O
274	O
references	O
huang	O
,	O
h.	O
h.	O
,	O
zhang	O
,	O
c.	O
,	O
lee	O
,	O
s.	O
,	O
and	O
wang	O
,	O
h.	O
p.	O
(	O
1991	O
)	O
.	O
implementation	O
and	O
comparison	O
of	O
neural	O
network	O
learning	O
paradigms	O
:	O
back	O
propagation	O
,	O
simulated	O
annealing	O
and	O
tabu	O
search	O
.	O
in	O
dagli	O
,	O
c.	O
,	O
kumara	O
,	O
s.	O
,	O
and	O
shin	O
,	O
y.	O
c.	O
,	O
editors	O
,	O
intelligent	O
engineering	O
systems	O
through	O
artiﬁcial	O
neural	O
networks	O
:	O
proceedings	O
of	O
the	O
artiﬁcial	O
neural	O
networks	O
in	O
engineering	O
conference	O
,	O
new	O
york	O
.	O
american	O
society	O
of	O
mechanical	O
engineers	O
.	O
huang	O
,	O
w.	O
y.	O
and	O
lippmann	O
,	O
r.	O
p.	O
(	O
1987	O
)	O
.	O
comparisons	O
between	O
neural	O
net	O
and	O
conven-	O
tional	O
classiﬁers	O
.	O
in	O
proceedings	O
of	O
the	O
ieee	O
ﬁrst	O
international	O
conference	O
on	O
neural	O
networks	O
,	O
pages	O
485–494	O
,	O
piscataway	O
,	O
nj	O
.	O
ieee	O
.	O
hunt	O
,	O
e.	O
b	O
.	O
(	O
1962	O
)	O
.	O
concept	O
learning	O
:	O
an	O
information	O
processing	O
problem	O
.	O
wiley	O
.	O
hunt	O
,	O
e.	O
b.	O
,	O
martin	O
,	O
j.	O
,	O
and	O
stone	O
,	O
p.	O
i	O
.	O
(	O
1966	O
)	O
.	O
experiemnts	O
in	O
induction	O
.	O
academic	O
press	O
,	O
new	O
york	O
.	O
hunt	O
,	O
k.	O
j.	O
,	O
sbarbaro	O
,	O
d.	O
,	O
˙zbikovski	O
,	O
r.	O
,	O
and	O
gawthrop	O
,	O
p.	O
j	O
.	O
(	O
1992	O
)	O
.	O
neural	O
networks	O
for	O
control	O
systems	O
–	O
a	O
survey	O
.	O
automatica	O
,	O
28	O
(	O
6	O
)	O
:1083–1112	O
.	O
jacobs	O
,	O
r.	O
(	O
1988	O
)	O
.	O
increased	O
rates	O
of	O
convergence	O
through	O
learning	O
rate	O
adaptation	O
.	O
neural	O
networks	O
,	O
1:295–307	O
.	O
jennet	O
,	O
b.	O
,	O
teasdale	O
,	O
g.	O
,	O
braakman	O
,	O
r.	O
,	O
minderhoud	O
,	O
j.	O
,	O
heiden	O
,	O
j.	O
,	O
and	O
kurzi	O
,	O
t.	O
(	O
1979	O
)	O
.	O
prognosis	O
of	O
patients	O
with	O
severe	O
head	B
injury	I
.	O
neurosurgery	O
,	O
4:283	O
–	O
288.	O
jones	O
,	O
d.	O
s.	O
(	O
1979	O
)	O
.	O
elementary	O
information	O
theory	O
.	O
clarendon	O
press	O
,	O
oxford	O
.	O
karaliˇc	O
,	O
a	O
.	O
(	O
1992	O
)	O
.	O
employing	O
linear	O
regression	O
in	O
regression	B
tree	I
leaves	O
.	O
in	O
proceedings	O
of	O
the	O
10th	O
european	O
conference	O
on	O
artiﬁcial	O
intelligence	O
,	O
pages	O
440–441	O
.	O
wiley	O
&	O
sons	O
.	O
wien	O
,	O
austria	O
.	O
karaliˇc	O
,	O
a.	O
and	O
gams	O
,	O
m.	O
(	O
1989	O
)	O
.	O
implementation	O
of	O
the	O
gynesis	O
pc	O
inductive	B
learning	I
system	O
.	O
in	O
proceedings	O
of	O
the	O
33rd	O
etan	O
conference	O
,	O
pages	O
xiii.83–90	O
.	O
novi	O
sad	O
,	O
(	O
in	O
slovenian	O
)	O
.	O
kass	O
,	O
g.	O
v.	O
(	O
1980	O
)	O
.	O
an	O
exploratory	O
technique	O
for	O
investigating	O
large	O
quantities	O
of	O
categorical	O
data	O
.	O
appl	O
.	O
statist.	O
,	O
29:119	O
–	O
127.	O
kendall	O
,	O
m.	O
g.	O
,	O
stuart	O
,	O
a.	O
,	O
and	O
ord	O
,	O
j.	O
k.	O
(	O
1983	O
)	O
.	O
the	O
advanced	O
theory	O
of	O
statistics	O
,	O
vol	O
3	O
,	O
design	O
and	O
analysis	O
and	O
time	O
series	O
.	O
chapter	O
44.	O
grifﬁn	O
,	O
london	O
,	O
fourth	O
edition	O
.	O
king	O
,	O
r.	O
d.	O
,	O
lewis	O
,	O
r.	O
a.	O
,	O
muggleton	O
,	O
s.	O
h.	O
,	O
and	O
sternberg	O
,	O
m.	O
j.	O
e.	O
(	O
1992	O
)	O
.	O
drug	O
design	O
by	O
machine	O
learning	O
:	O
the	O
use	O
of	O
inductive	O
logic	O
programming	O
to	O
model	O
the	O
structure-activity	O
relationship	O
of	O
trimethoprim	O
analogues	O
binding	O
to	O
dihydrofolate	O
reductase	O
.	O
proceedings	O
of	O
the	O
national	O
academy	O
science	O
,	O
89.	O
kirkwood	O
,	O
c.	O
,	O
andrews	O
,	O
b.	O
,	O
and	O
mowforth	O
,	O
p.	O
(	O
1989	O
)	O
.	O
automatic	O
detection	O
of	O
gait	O
events	O
:	O
a	O
case	O
study	O
using	O
inductive	B
learning	I
techniques	O
.	O
journal	O
of	O
biomedical	O
engineering	O
,	O
11	O
(	O
23	O
)	O
:511–516	O
.	O
knoll	O
,	O
u	O
.	O
(	O
1993	O
)	O
.	O
kostenoptimiertes	O
prunen	O
in	O
entscheidungsbaumen	O
.	O
daimler-benz	O
,	O
forschung	O
und	O
technik	O
,	O
ulm	O
.	O
kohonen	O
,	O
t.	O
(	O
1984	O
)	O
.	O
self-organization	O
and	O
associative	O
memory	O
.	O
springer	O
verlag	O
,	O
berlin	O
.	O
kohonen	O
,	O
t.	O
(	O
1989	O
)	O
.	O
self-organization	O
and	O
associative	O
memory	O
.	O
springer-verlag	O
,	O
berlin	O
,	O
3rd	O
edition	O
.	O
kohonen	O
,	O
t.	O
,	O
barna	O
,	O
g.	O
,	O
and	O
chrisley	O
,	O
r.	O
(	O
1988	O
)	O
.	O
statistical	B
pattern	O
recognition	O
with	O
neural	O
networks	O
:	O
benchmarking	O
studies	O
.	O
in	O
ieee	O
international	O
conference	O
on	O
neural	O
networks	O
,	O
volume	O
1	O
,	O
pages	O
61–68	O
,	O
new	O
york	O
.	O
(	O
san	O
diego	O
1988	O
)	O
,	O
ieee	O
.	O
references	O
275	O
kononenko	O
,	O
i.	O
and	O
bratko	O
,	O
i	O
.	O
(	O
1991	O
)	O
.	O
information-based	O
evaluation	O
criterion	O
for	O
classiﬁer	B
’	O
s	O
performance	O
.	O
machine	O
learning	O
,	O
6.	O
kressel	O
,	O
u	O
.	O
(	O
1991	O
)	O
.	O
the	O
impact	O
of	O
the	O
learning	O
set	O
size	O
in	O
handwritten	O
digits	O
recognition	O
.	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
artiﬁcal	O
neural	O
networks	O
,	O
helsinki	O
,	O
finland	O
.	O
krishnaiah	O
,	O
p.	O
and	O
kanal	O
,	O
l.	O
,	O
editors	O
(	O
1982	O
)	O
.	O
classiﬁcation	B
,	O
pattern	O
recognition	O
,	O
and	O
reduction	O
of	O
dimensionality	O
,	O
volume	O
2	O
of	O
handbook	O
of	O
statistics	O
.	O
north	O
holland	O
,	O
am-	O
sterdam	O
.	O
kwakernaak	O
,	O
h.	O
and	O
sivan	O
,	O
r.	O
(	O
1972	O
)	O
.	O
linear	O
optimal	O
control	O
systems	O
.	O
john	O
wiley	O
.	O
lachenbruch	O
,	O
p.	O
and	O
mickey	O
,	O
r.	O
(	O
1968	O
)	O
.	O
estimation	O
of	O
error	O
rates	O
in	O
discriminant	O
analysis	O
.	O
technometrics	O
,	O
10:1–11	O
.	O
lachenbruch	O
,	O
p.	O
a.	O
and	O
mickey	O
,	O
m.	O
r.	O
(	O
1975	O
)	O
.	O
discriminant	O
analysis	O
.	O
hafner	O
press	O
,	O
new	O
york	O
.	O
lang	O
,	O
k.	O
j.	O
,	O
waibel	O
,	O
a.	O
h.	O
,	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
1990	O
)	O
.	O
a	O
time-delay	O
neural	O
network	O
architecture	O
for	O
isolated	O
word	O
recognition	O
.	O
neural	O
networks	O
,	O
3:23–44	O
.	O
lauritzen	O
,	O
s.	O
l.	O
and	O
spiegelhalter	O
,	O
d.	O
j	O
.	O
(	O
1988	O
)	O
.	O
local	O
computations	O
with	O
probabilities	O
on	O
graphical	O
structures	O
and	O
their	O
application	O
to	O
expert	O
systems	O
(	O
with	O
discussion	O
)	O
.	O
j.	O
royal	O
statist	O
.	O
soc.	O
,	O
series	O
b	O
,	O
50:157–224	O
.	O
lawrence	O
,	O
e.	O
c.	O
and	O
smith	O
,	O
l.	O
d.	O
(	O
1992	O
)	O
.	O
an	O
analysis	O
of	O
default	O
risk	O
in	O
mobile	O
home	O
credit	O
.	O
j.	O
banking	O
and	O
finance	O
,	O
16:299–312	O
.	O
le	O
cun	O
,	O
y.	O
,	O
boser	O
,	O
b.	O
,	O
denker	O
,	O
j.	O
s.	O
,	O
henderson	O
,	O
d.	O
,	O
howard	O
,	O
r.	O
e.	O
,	O
hubbard	O
,	O
w.	O
,	O
and	O
d.	O
,	O
j.	O
l.	O
(	O
1989	O
)	O
.	O
backpropagation	O
applied	O
to	O
handwritten	O
zip	O
code	O
recognition	O
.	O
neural	O
computation	O
,	O
1:541–551	O
.	O
lee	O
,	O
c.	O
(	O
1990	O
)	O
.	O
fuzzy	O
logic	O
in	O
control	O
systems	O
:	O
fuzzy	O
logic	O
controller	O
–	O
part	O
1	O
,	O
part	O
2.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
and	O
cybernetics	O
,	O
20	O
(	O
2	O
)	O
:404–435	O
.	O
leech	O
,	O
w.	O
j	O
.	O
(	O
1986	O
)	O
.	O
a	O
rule	O
based	O
process	O
control	O
method	O
with	O
feedback	O
.	O
advances	O
in	O
instrumentation	O
,	O
41.	O
leitch	O
,	O
r.	O
(	O
1992	O
)	O
.	O
knowledge	O
based	O
control	O
:	O
selecting	O
the	O
right	O
tool	O
for	O
the	O
job	O
.	O
in	O
preprints	O
of	O
the	O
1992	O
ifac/ifip/imacs	O
international	O
symposium	O
on	O
artiﬁcial	O
intelligence	O
in	O
real-time	O
control	O
,	O
pages	O
26–33	O
.	O
delft	O
,	O
the	O
netherlands	O
.	O
leitch	O
,	O
r.	O
r.	O
and	O
francis	O
,	O
j.	O
c.	O
(	O
1986	O
)	O
.	O
towards	O
intelligent	O
control	O
systems	O
.	O
in	O
mamdani	O
,	O
a.	O
and	O
efstathiou	O
,	O
j.	O
,	O
editors	O
,	O
expert	O
systems	O
and	O
optimisation	B
in	O
process	O
control	O
,	O
pages	O
62–73	O
,	O
aldershot	O
,	O
england	O
.	O
gower	O
technical	B
press	O
.	O
luttrell	O
,	O
s.	O
p.	O
(	O
1990	O
)	O
.	O
derivation	O
of	O
a	O
class	O
of	O
training	O
algorithms	O
.	O
ieee	O
transactions	O
on	O
neural	O
networks	O
,	O
1	O
(	O
2	O
)	O
:229–232	O
.	O
luttrell	O
,	O
s.	O
p.	O
(	O
1993	O
)	O
.	O
a	O
bayesian	O
analysis	O
of	O
vector	O
quantization	O
algorithms	O
.	O
submitted	O
to	O
neural	O
computation	O
.	O
machado	O
,	O
s.	O
g.	O
(	O
1983	O
)	O
.	O
two	O
statistics	O
for	O
testing	O
for	O
multivariate	O
normality	O
.	O
biometrika	O
,	O
70:713–718	O
.	O
mackay	O
,	O
d.	O
(	O
1992a	O
)	O
.	O
the	O
evidence	O
framework	O
applied	O
to	O
classiﬁcation	B
networks	O
.	O
neural	O
computation	O
,	O
4:720–736	O
.	O
mackay	O
,	O
d.	O
(	O
1992b	O
)	O
.	O
a	O
practical	O
bayesian	O
framework	O
for	O
backpropagation	O
networks	O
.	O
neural	O
computation	O
,	O
4:448–472	O
.	O
276	O
references	O
makaroviˇc	O
,	O
a	O
.	O
(	O
1988	O
)	O
.	O
a	O
qualitative	O
way	O
of	O
solving	O
the	O
pole	B
balancing	I
problem	O
.	O
technical	B
report	O
memorandum	O
inf-88-44	O
,	O
university	O
of	O
twente	O
.	O
also	O
in	O
:	O
machine	O
intelligence	O
12	O
,	O
j.hayes	O
,	O
d.michie	O
,	O
e.tyugu	O
(	O
eds	O
.	O
)	O
,	O
oxford	O
university	O
press	O
,	O
pp	O
.	O
241–258	O
.	O
mardia	O
,	O
k.	O
v.	O
(	O
1974	O
)	O
.	O
applications	O
of	O
some	O
measures	O
of	O
multivariate	O
skewness	O
and	O
kurtosis	O
in	O
testing	O
normality	O
and	O
robustness	O
studies	O
.	O
sankhya	O
b	O
,	O
36:115–128	O
.	O
mardia	O
,	O
k.	O
v.	O
,	O
kent	O
,	O
j.	O
t.	O
,	O
and	O
bibby	O
,	O
j.	O
m.	O
(	O
1979	O
)	O
.	O
multivariate	O
analysis	O
.	O
academic	O
press	O
,	O
london	O
.	O
marks	O
,	O
s.	O
and	O
dunn	O
,	O
o.	O
j	O
.	O
(	O
1974	O
)	O
.	O
discriminant	O
functions	O
when	O
covariance	O
matrices	O
are	O
unequal	O
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
69:555–559	O
.	O
mccarthy	O
,	O
j.	O
and	O
hayes	O
,	O
p.	O
j	O
.	O
(	O
1969	O
)	O
.	O
some	O
philosophical	O
problems	O
from	O
the	O
standpoint	O
of	O
artiﬁcial	O
intelligence	O
.	O
in	O
meltzer	O
,	O
b.	O
and	O
michie	O
,	O
d.	O
,	O
editors	O
,	O
machine	O
intelligence	O
4	O
,	O
pages	O
463	O
–	O
502.	O
eup	O
,	O
edinburgh	O
.	O
mccullagh	O
,	O
p.	O
and	O
nelder	O
,	O
j.	O
a	O
.	O
(	O
1989	O
)	O
.	O
generalized	O
linear	O
models	O
.	O
chapman	O
and	O
hall	O
,	O
london	O
,	O
2nd	O
edition	O
.	O
mcculloch	O
,	O
w.	O
s.	O
and	O
pitts	O
,	O
w.	O
(	O
1943	O
)	O
.	O
a	O
logical	O
calculus	O
of	O
the	O
ideas	O
immanent	O
in	O
nervous	O
activity	O
forms	O
.	O
bulletin	O
of	O
methematical	O
biophysics	O
,	O
9:127–147	O
.	O
mclachlan	O
,	O
g.	O
j	O
.	O
(	O
1992	O
)	O
.	O
discriminant	O
analysis	O
and	O
statistical	B
pattern	O
recognition	O
.	O
john	O
wiley	O
,	O
new	O
york	O
.	O
meyer-br¨otz	O
,	O
g.	O
and	O
sch¨urmann	O
,	O
j	O
.	O
(	O
1970	O
)	O
.	O
methoden	O
der	O
automatischen	O
zeichenerken-	O
nung	O
.	O
akademie-verlag	O
,	O
berlin	O
.	O
m´ezard	O
,	O
m.	O
and	O
nadal	O
,	O
j.	O
p.	O
(	O
1989	O
)	O
.	O
learning	O
in	O
feed-forward	O
layered	O
networks	O
:	O
the	O
tiling	O
algorithm	O
.	O
journal	O
of	O
physics	O
a	O
:	O
mathematics	O
,	O
general	O
,	O
22:2191–2203	O
.	O
m.g	O
.	O
kendall	O
,	O
a.	O
s.	O
and	O
ord	O
,	O
j	O
.	O
(	O
1983	O
)	O
.	O
the	O
advanced	O
theory	O
of	O
statistics	O
,	O
vol	O
1	O
,	O
distri-	O
bution	O
theory	O
.	O
grifﬁn	O
,	O
london	O
,	O
fourth	O
edition	O
.	O
michalski	O
,	O
r.	O
s.	O
(	O
1969	O
)	O
.	O
on	O
the	O
quasi-minimal	O
solution	O
of	O
the	O
general	O
covering	O
problem	O
.	O
in	O
proc	O
.	O
of	O
the	O
fifth	O
internat	O
.	O
symp	O
.	O
on	O
inform	O
.	O
processing	O
,	O
pages	O
125	O
–	O
128	O
,	O
bled	O
,	O
slovenia	O
.	O
michalski	O
,	O
r.	O
s.	O
(	O
1973	O
)	O
.	O
discovering	O
classiﬁcation	B
rules	O
using	O
variable	O
valued	O
logic	O
system	O
vl1	O
.	O
in	O
third	O
international	O
joint	O
conference	O
on	O
artiﬁcial	O
intelligence	O
,	O
pages	O
162–172	O
.	O
michalski	O
,	O
r.	O
s.	O
(	O
1983	O
)	O
.	O
a	O
theory	O
and	O
methodology	O
of	O
inductive	B
learning	I
.	O
in	O
r.	O
s.	O
michalski	O
,	O
j.	O
g.	O
c.	O
and	O
mitchell	O
,	O
t.	O
m.	O
,	O
editors	O
,	O
machine	O
learning	O
:	O
an	O
artiﬁcial	O
intelligence	O
approach	O
.	O
tioga	O
,	O
palo	O
alto	O
.	O
michalski	O
,	O
r.	O
s.	O
and	O
chilauski	O
,	O
r.	O
l.	O
(	O
1980	O
)	O
.	O
knowledge	O
acquisition	O
by	O
encoding	O
ex-	O
pert	O
rules	O
versus	O
computer	O
induction	O
from	O
examples	O
:	O
a	O
case	O
study	O
involving	O
soybean	B
pathology	O
.	O
int	O
.	O
j.	O
man-machine	O
studies	O
,	O
12:63	O
–	O
87.	O
michalski	O
,	O
r.	O
s.	O
and	O
larson	O
,	O
j.	O
b	O
.	O
(	O
1978	O
)	O
.	O
selection	O
of	O
the	O
most	O
representative	O
training	O
examples	O
and	O
incremental	O
generation	O
of	O
vl1	O
hypothesis	O
:	O
the	O
underlying	O
methodology	O
and	O
the	O
description	O
of	O
programs	O
esel	O
and	O
aq11	O
.	O
technical	B
report	O
877	O
,	O
dept	O
.	O
of	O
computer	O
sciencence	O
,	O
u.	O
of	O
illinois	O
,	O
urbana	O
.	O
michie	O
,	O
d.	O
(	O
1989	O
)	O
.	O
problems	O
of	O
computer-aided	O
concept	O
formation	O
.	O
in	O
quinlan	O
,	O
j.	O
r.	O
,	O
editor	O
,	O
applications	O
of	O
expert	O
systems	O
,	O
volume	O
2	O
,	O
pages	O
310	O
–	O
333.	O
addison-wesley	O
,	O
london	O
.	O
michie	O
,	O
d.	O
(	O
1990	O
)	O
.	O
personal	O
models	O
of	O
rationality	O
.	O
j.	O
statist	O
.	O
planning	O
and	O
inference	O
,	O
25:381	O
–	O
399.	O
references	O
277	O
michie	O
,	O
d.	O
(	O
1991	O
)	O
.	O
methodologies	O
from	O
machine	O
learning	O
in	O
data	O
analysis	O
and	O
software	O
.	O
computer	O
journal	O
,	O
34:559	O
–	O
565.	O
michie	O
,	O
d.	O
and	O
al	O
attar	O
,	O
a	O
.	O
(	O
1991	O
)	O
.	O
use	O
of	O
sequential	O
bayes	O
with	O
class	O
probability	O
trees	O
.	O
in	O
hayes	O
,	O
j.	O
,	O
michie	O
,	O
d.	O
,	O
and	O
tyugu	O
,	O
e.	O
,	O
editors	O
,	O
machine	O
intelligence	O
12	O
,	O
pages	O
187–202	O
.	O
oxford	O
university	O
press	O
.	O
michie	O
,	O
d.	O
and	O
bain	O
,	O
m.	O
(	O
1992	O
)	O
.	O
machine	O
acquisition	O
of	O
concepts	O
from	O
sample	O
data	O
.	O
in	O
kopec	O
,	O
d.	O
and	O
thompson	O
,	O
r.	O
b.	O
,	O
editors	O
,	O
artiﬁcial	O
intelligence	O
and	O
intelligent	O
tutoring	O
systems	O
,	O
pages	O
5	O
–	O
23.	O
ellis	O
horwood	O
ltd.	O
,	O
chichester	O
.	O
michie	O
,	O
d.	O
,	O
bain	O
,	O
m.	O
,	O
and	O
hayes-michie	O
,	O
j	O
.	O
(	O
1990	O
)	O
.	O
cognitive	O
models	O
from	O
subcognitive	O
skills	O
.	O
in	O
grimble	O
,	O
m.	O
,	O
mcghee	O
,	O
j.	O
,	O
and	O
mowforth	O
,	O
p.	O
,	O
editors	O
,	O
knowledge-based	O
systems	O
in	O
industrial	O
control	O
,	O
pages	O
71–90	O
,	O
stevenage	O
.	O
peter	O
peregrinus	O
.	O
michie	O
,	O
d.	O
and	O
camacho	O
,	O
r.	O
(	O
1994	O
)	O
.	O
building	O
symbolic	O
representations	O
of	O
intuitive	O
real-	O
time	O
skills	O
from	O
performance	O
data	O
.	O
to	O
appear	O
in	O
machine	O
intelligence	O
and	O
inductive	B
learning	I
,	O
vol	O
.	O
1	O
(	O
eds	O
.	O
furukawa	O
,	O
k.	O
and	O
muggleton	O
,	O
s.	O
h.	O
,	O
new	O
series	O
of	O
machine	O
intelligence	O
,	O
ed	O
.	O
in	O
chief	O
d.	O
michie	O
)	O
,	O
oxford	O
:	O
oxford	O
university	O
press	O
.	O
michie	O
,	O
d.	O
and	O
chambers	O
,	O
r.	O
a	O
.	O
(	O
1968a	O
)	O
.	O
boxes	O
:	O
an	O
experiment	O
in	O
adaptive	O
control	O
.	O
in	O
dale	O
,	O
e.	O
and	O
michie	O
,	O
d.	O
,	O
editors	O
,	O
machine	O
intelligence	O
2.	O
oliver	O
and	O
boyd	O
,	O
edinburgh	O
.	O
michie	O
,	O
d.	O
and	O
chambers	O
,	O
r.	O
a	O
.	O
(	O
1968b	O
)	O
.	O
boxes	O
:	O
an	O
experiment	O
in	O
adaptive	O
control	O
.	O
in	O
dale	O
,	O
e.	O
and	O
michie	O
,	O
d.	O
,	O
editors	O
,	O
machine	O
intelligence	O
2	O
,	O
pages	O
137–152	O
.	O
edinburgh	O
university	O
press	O
.	O
michie	O
,	O
d.	O
and	O
sammut	O
,	O
c.	O
(	O
1993	O
)	O
.	O
machine	O
learning	O
from	O
real-time	O
input-output	O
be-	O
haviour	O
.	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
design	O
to	O
manufacture	O
in	O
modern	O
industry	O
,	O
pages	O
363–369	O
.	O
miller	O
,	O
w.	O
t.	O
,	O
sutton	O
,	O
r.	O
s.	O
,	O
and	O
werbos	O
,	O
p.	O
j.	O
,	O
editors	O
(	O
1990	O
)	O
.	O
neural	O
networks	O
for	O
control	O
.	O
the	O
mit	O
press	O
.	O
minsky	O
,	O
m.	O
c.	O
and	O
papert	O
,	O
s.	O
(	O
1969	O
)	O
.	O
perceptrons	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
,	O
usa	O
.	O
møller	O
,	O
m.	O
(	O
1993	O
)	O
.	O
a	O
scaled	O
conjugate	O
gradient	O
algorithm	O
for	O
fast	O
supervised	O
learning	O
.	O
neural	O
networks	O
,	O
4:525–534	O
.	O
mooney	O
,	O
r.	O
,	O
shavlik	O
,	O
j.	O
,	O
towell	O
,	O
g.	O
,	O
and	O
gove	O
,	O
a	O
.	O
(	O
1989	O
)	O
.	O
an	O
experimental	O
comparison	O
of	O
symbolic	O
and	O
connectionist	O
learning	O
algorithms	O
(	O
vol	O
1	O
)	O
.	O
in	O
ijcai	O
89	O
:	O
proceedings	O
of	O
the	O
eleventh	O
international	O
joint	O
conference	O
on	O
artiﬁcial	O
intelligence	O
,	O
detroit	O
,	O
mi	O
,	O
pages	O
775–780	O
,	O
san	O
mateo	O
,	O
ca	O
.	O
morgan	O
kaufmann	O
for	O
international	O
joint	O
conferences	O
on	O
artiﬁcial	O
intelligence	O
.	O
muggleton	O
,	O
s.	O
h.	O
(	O
1993	O
)	O
.	O
logic	O
and	O
learning	O
:	O
turing	O
’	O
s	O
legacy	O
.	O
in	O
muggleton	O
,	O
s.	O
h.	O
and	O
michie	O
,	O
d.	O
furukaw	O
,	O
k.	O
,	O
editors	O
,	O
machine	O
intelligence	O
13.	O
oxford	O
university	O
press	O
,	O
oxford	O
.	O
muggleton	O
,	O
s.	O
h.	O
,	O
bain	O
,	O
m.	O
,	O
hayes-michie	O
,	O
j.	O
e.	O
,	O
and	O
michie	O
,	O
d.	O
(	O
1989	O
)	O
.	O
an	O
experimental	O
comparison	O
of	O
learning	O
formalisms	O
.	O
in	O
sixth	O
internat	O
.	O
workshop	O
on	O
mach	O
.	O
learning	O
,	O
pages	O
113	O
–	O
118	O
,	O
san	O
mateo	O
,	O
ca	O
.	O
morgan	O
kaufmann	O
.	O
muggleton	O
,	O
s.	O
h.	O
and	O
buntine	O
,	O
w.	O
(	O
1988	O
)	O
.	O
machine	O
invention	O
of	O
ﬁrst-order	O
predicates	O
by	O
inverting	O
resolution	O
.	O
in	O
r.	O
s.	O
michalski	O
,	O
t.	O
m.	O
m.	O
and	O
carbonell	O
,	O
j.	O
g.	O
,	O
editors	O
,	O
proceedings	O
of	O
the	O
fifth	O
international	O
machine	O
.	O
learning	O
conference	O
,	O
pages	O
339–352	O
.	O
morgan	O
kaufmann	O
,	O
,	O
ann	O
arbor	O
,	O
michigan	O
.	O
278	O
references	O
muggleton	O
,	O
s.	O
h.	O
and	O
feng	O
,	O
c.	O
(	O
1990	O
)	O
.	O
efﬁcient	O
induction	O
of	O
logic	O
programs	O
.	O
in	O
first	O
in-	O
ternational	O
conference	O
on	O
algorithmic	O
learning	O
theory	O
,	O
pages	O
369–381	O
,	O
tokyo	O
,	O
japan	O
.	O
japanese	O
society	O
for	O
artiﬁcial	O
intellligence	O
.	O
neapolitan	O
,	O
e.	O
(	O
1990	O
)	O
.	O
probabilistic	O
reasoning	O
in	O
expert	O
systems	O
.	O
john	O
wiley	O
.	O
nowlan	O
,	O
s.	O
and	O
hinton	O
,	O
g.	O
(	O
1992	O
)	O
.	O
simplifying	O
neural	O
networks	O
by	O
soft	O
weight-sharing	O
.	O
neural	O
computation	O
,	O
4:473–493	O
.	O
odetayo	O
,	O
m.	O
o	O
.	O
(	O
1988	O
)	O
.	O
balancing	O
a	O
pole-cart	O
system	O
using	O
genetic	B
algorithms	I
.	O
master	O
’	O
s	O
thesis	O
,	O
department	O
of	O
computer	O
science	O
,	O
university	O
of	O
strathclyde	O
.	O
odetayo	O
,	O
m.	O
o.	O
and	O
mcgregor	O
,	O
d.	O
r.	O
(	O
1989	O
)	O
.	O
genetic	O
algorithm	O
for	O
inducing	O
control	O
rules	O
for	O
a	O
dynamic	O
system	O
.	O
in	O
proceedings	O
of	O
the	O
3rd	O
international	O
conference	O
on	O
genetic	B
algorithms	I
,	O
pages	O
177–182	O
.	O
morgan	O
kaufmann	O
.	O
ozturk	O
,	O
a.	O
and	O
romeu	O
,	O
j.	O
l.	O
(	O
1992	O
)	O
.	O
a	O
new	O
method	O
for	O
assessing	O
multivariate	O
normality	O
with	O
graphical	O
applications	O
.	O
communications	O
in	O
statistics	O
-	O
simulation	O
,	O
21.	O
pearce	O
,	O
d.	O
(	O
1989	O
)	O
.	O
the	O
induction	O
of	O
fault	O
diagnosis	O
systems	O
from	O
qualitative	O
models	O
.	O
in	O
proc	O
.	O
seventh	O
nat	O
.	O
conf	O
.	O
on	O
art	O
.	O
intell	O
.	O
(	O
aaai-88	O
)	O
,	O
pages	O
353	O
–	O
357	O
,	O
st.	O
paul	O
,	O
minnesota	O
.	O
pearl	O
,	O
j	O
.	O
(	O
1988	O
)	O
.	O
probabilistic	O
reasoning	O
in	O
intelligent	O
systems	O
:	O
networks	O
of	O
plausible	O
inference	O
.	O
morgan	O
kaufmann	O
,	O
san	O
mateo	O
.	O
piper	O
,	O
j.	O
and	O
granum	O
,	O
e.	O
(	O
1989	O
)	O
.	O
on	O
fully	O
automatic	O
feature	O
measurement	O
for	O
banded	O
chromosome	O
classiﬁcation	B
.	O
cytometry	O
,	O
10:242–255	O
.	O
plotkin	O
,	O
g.	O
d.	O
(	O
1970	O
)	O
.	O
a	O
note	O
on	O
inductive	O
generalization	O
.	O
in	O
meltzer	O
,	O
b.	O
and	O
michie	O
,	O
d.	O
,	O
editors	O
,	O
machine	O
intelligence	O
5	O
,	O
pages	O
153–163	O
.	O
edinburgh	O
university	O
press	O
.	O
poggio	O
,	O
t.	O
and	O
girosi	O
,	O
f.	O
(	O
1990	O
)	O
.	O
networks	O
for	O
approximation	O
and	O
learning	O
.	O
proceedings	O
of	O
the	O
ieee	O
,	O
78:1481–1497	O
.	O
pomerleau	O
,	O
d.	O
a	O
.	O
(	O
1989	O
)	O
.	O
alvinn	O
:	O
an	O
autonomous	O
land	O
vehicle	O
in	O
a	O
neural	O
network	O
.	O
in	O
touretzky	O
,	O
d.	O
s.	O
,	O
editor	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
.	O
morgan	O
kaufmann	O
publishers	O
,	O
san	O
mateo	O
,	O
ca	O
.	O
prager	O
,	O
r.	O
w.	O
and	O
fallside	O
,	O
f.	O
(	O
1989	O
)	O
.	O
the	O
modiﬁed	O
kanerva	O
model	O
for	O
automatic	O
speech	O
recognition	O
.	O
computer	O
speech	O
and	O
language	O
,	O
3:61–82	O
.	O
press	O
,	O
w.	O
h.	O
,	O
flannery	O
,	O
b.	O
p.	O
,	O
teukolsky	O
,	O
s.	O
a.	O
,	O
and	O
vettering	O
,	O
w.	O
t.	O
(	O
1988	O
)	O
.	O
numerical	O
recipes	O
in	O
c	O
:	O
the	O
art	O
of	O
scientiﬁc	O
computing	O
.	O
cambridge	O
university	O
press	O
,	O
cambridge	O
.	O
quinlan	O
,	O
j.	O
r.	O
(	O
1986	O
)	O
.	O
induction	O
of	O
decision	O
trees	O
.	O
machine	O
learning	O
,	O
1:81–106	O
.	O
quinlan	O
,	O
j.	O
r.	O
(	O
1987a	O
)	O
.	O
generating	O
production	O
rules	O
from	O
decision	O
trees	O
.	O
in	O
international	O
joint	O
conference	O
on	O
artiﬁcial	O
intelligence	O
,	O
pages	O
304–307	O
,	O
milan	O
.	O
quinlan	O
,	O
j.	O
r.	O
(	O
1987b	O
)	O
.	O
generating	O
production	O
rules	O
from	O
decision	O
trees	O
.	O
in	O
proceedings	O
of	O
the	O
tenth	O
international	O
joint	O
conference	O
on	O
artiﬁcial	O
intelligence	O
,	O
pages	O
304–307	O
.	O
morgan	O
kaufmann	O
,	O
san	O
mateo	O
,	O
ca	O
.	O
quinlan	O
,	O
j.	O
r.	O
(	O
1987c	O
)	O
.	O
simplifying	O
decision	O
trees	O
.	O
int	O
j	O
man-machine	O
studies	O
,	O
27:221–234	O
.	O
quinlan	O
,	O
j.	O
r.	O
(	O
1990	O
)	O
.	O
learning	O
logical	O
deﬁnitions	O
from	O
relations	O
.	O
machine	O
learning	O
,	O
5:239–266	O
.	O
quinlan	O
,	O
j.	O
r.	O
(	O
1993	O
)	O
.	O
c4.5	O
:	O
programs	O
for	O
machine	O
learning	O
.	O
morgan	O
kaufmann	O
,	O
san	O
mateo	O
,	O
ca	O
.	O
quinlan	O
,	O
j.	O
r.	O
,	O
compton	O
,	O
p.	O
j.	O
,	O
horn	O
,	O
k.	O
a.	O
,	O
and	O
lazarus	O
,	O
l.	O
(	O
1986	O
)	O
.	O
inductive	O
knowledge	O
acquisition	O
:	O
a	O
case	O
study	O
.	O
in	O
proceedings	O
of	O
the	O
second	O
australian	O
conference	O
on	O
references	O
279	O
applications	O
of	O
expert	O
systems	O
,	O
pages	O
83–204	O
,	O
sydney	O
.	O
new	O
south	O
wales	O
institute	O
of	O
technology	O
.	O
reaven	O
,	O
g.	O
m.	O
and	O
miller	O
,	O
r.	O
g.	O
(	O
1979	O
)	O
.	O
an	O
attempt	O
to	O
deﬁne	O
the	O
nature	O
of	O
chemical	O
diabetes	B
using	O
a	O
multidimensional	O
analysis	O
.	O
diabetologia	O
,	O
16:17–24	O
.	O
refenes	O
,	O
a.	O
n.	O
and	O
vithlani	O
,	O
s.	O
(	O
1991	O
)	O
.	O
constructive	O
learning	O
by	O
specialisation	O
.	O
in	O
proceed-	O
ings	O
of	O
the	O
international	O
conference	O
on	O
artiﬁcial	O
neural	O
networks	O
,	O
helsinki	O
,	O
finland	O
.	O
remme	O
,	O
j.	O
,	O
habbema	O
,	O
j.	O
d.	O
f.	O
,	O
and	O
hermans	O
,	O
j	O
.	O
(	O
1980	O
)	O
.	O
a	O
simulative	O
comparison	O
of	O
linear	O
,	O
quadratic	O
and	O
kernel	O
discrimination	O
.	O
j.	O
statist	O
.	O
comput	O
.	O
simul.	O
,	O
11:87–106	O
.	O
renals	O
,	O
s.	O
and	O
rohwer	O
,	O
r.	O
(	O
1989	O
)	O
.	O
phoneme	O
classiﬁcation	B
experiments	O
using	O
radial	O
basis	O
functions	O
.	O
in	O
proceedings	O
of	O
the	O
international	O
joint	O
conference	O
on	O
neural	O
networks	O
,	O
volume	O
i	O
,	O
pages	O
461–468	O
,	O
washington	O
dc	O
.	O
renders	O
,	O
j.	O
m.	O
and	O
nordvik	O
,	O
j.	O
p.	O
(	O
1992	O
)	O
.	O
genetic	B
algorithms	I
for	O
process	O
control	O
:	O
a	O
survey	O
.	O
in	O
preprints	O
of	O
the	O
1992	O
ifac/ifip/imacs	O
international	O
symposium	O
on	O
artiﬁcial	O
intelligence	O
in	O
real-time	O
control	O
,	O
pages	O
579–584	O
.	O
delft	O
,	O
the	O
netherlands	O
.	O
reynolds	O
,	O
j.	O
c.	O
(	O
1970	O
)	O
.	O
transformational	O
systems	O
and	O
the	O
algebraic	O
structure	O
of	O
atomic	O
formulas	O
.	O
in	O
meltzer	O
,	O
b.	O
and	O
michie	O
,	O
d.	O
,	O
editors	O
,	O
machine	O
intelligence	O
5	O
,	O
pages	O
153–	O
163.	O
edinburgh	O
university	O
press	O
.	O
ripley	O
,	O
b	O
.	O
(	O
1993	O
)	O
.	O
statistical	B
aspects	O
of	O
neural	O
networks	O
.	O
in	O
barndorff-nielsen	O
,	O
o.	O
,	O
cox	O
,	O
d.	O
,	O
jensen	O
,	O
j.	O
,	O
and	O
kendall	O
,	O
w.	O
,	O
editors	O
,	O
chaos	O
and	O
networks	O
-	O
statistical	B
and	O
probabilistic	O
aspects	O
.	O
chapman	O
and	O
hall	O
.	O
robinson	O
,	O
j.	O
a	O
.	O
(	O
1965	O
)	O
.	O
a	O
machine	O
oriented	O
logic	O
based	O
on	O
the	O
resolution	O
principle	O
.	O
journal	O
of	O
the	O
acm	O
,	O
12	O
(	O
1	O
)	O
:23–41	O
.	O
rohwer	O
,	O
r.	O
(	O
1991a	O
)	O
.	O
description	O
and	O
training	O
of	O
neural	O
network	O
dynamics	O
.	O
in	O
pasemann	O
,	O
f.	O
and	O
doebner	O
,	O
h.	O
,	O
editors	O
,	O
neurodynamics	O
,	O
proceedings	O
of	O
the	O
9th	O
summer	O
workshop	O
,	O
clausthal	O
,	O
germany	O
.	O
world	O
scientiﬁc	O
.	O
rohwer	O
,	O
r.	O
(	O
1991b	O
)	O
.	O
neural	O
networks	O
for	O
time-varying	O
data	O
.	O
in	O
murtagh	O
,	O
f.	O
,	O
editor	O
,	O
neural	O
networks	O
for	O
statistical	B
and	O
economic	O
data	O
,	O
pages	O
59–70	O
.	O
statistical	B
ofﬁce	O
of	O
the	O
european	O
communities	O
,	O
luxembourg	O
.	O
rohwer	O
,	O
r.	O
(	O
1991c	O
)	O
.	O
time	O
trials	O
on	O
second-order	B
and	O
variable-learning-rate	O
algorithms	O
.	O
in	O
lippmann	O
,	O
r.	O
,	O
moody	O
,	O
j.	O
,	O
and	O
touretzky	O
,	O
d.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
volume	O
3	O
,	O
pages	O
977–983	O
,	O
san	O
mateo	O
ca	O
.	O
morgan	O
kaufmann	O
.	O
rohwer	O
,	O
r.	O
(	O
1992	O
)	O
.	O
a	O
representation	O
of	O
representation	O
applied	O
to	O
a	O
discussion	O
of	O
vari-	O
able	O
binding	O
.	O
technical	B
report	O
,	O
dept	O
.	O
of	O
computer	O
science	O
and	O
applied	O
maths.	O
,	O
aston	O
university	O
.	O
rohwer	O
,	O
r.	O
and	O
cressy	O
,	O
d.	O
(	O
1989	O
)	O
.	O
phoneme	O
classiﬁcation	B
by	O
boolean	O
networks	O
.	O
in	O
pro-	O
ceedings	O
of	O
the	O
european	O
conference	O
on	O
speech	O
communication	O
and	O
technology	O
,	O
pages	O
557–560	O
,	O
paris	O
.	O
rohwer	O
,	O
r.	O
,	O
grant	O
,	O
b.	O
,	O
and	O
limb	O
,	O
p.	O
r.	O
(	O
1992	O
)	O
.	O
towards	O
a	O
connectionist	O
reasoning	O
system	O
.	O
british	O
telecom	O
technology	O
journal	O
,	O
10:103–109	O
.	O
rohwer	O
,	O
r.	O
and	O
renals	O
,	O
s.	O
(	O
1988	O
)	O
.	O
training	O
recurrent	O
networks	O
.	O
in	O
personnaz	O
,	O
l.	O
and	O
dreyfus	O
,	O
g.	O
,	O
editors	O
,	O
neural	O
networks	O
from	O
models	O
to	O
applications	O
,	O
pages	O
207–216	O
.	O
i.	O
d.	O
s.	O
e.	O
t.	O
,	O
paris	O
.	O
rosenblatt	O
,	O
f.	O
(	O
1958	O
)	O
.	O
psychological	O
review	O
,	O
65:368–408	O
.	O
rosenblatt	O
,	O
f.	O
(	O
1962	O
)	O
.	O
principles	O
of	O
neurodynamics	O
.	O
spartan	O
books	O
,	O
new	O
york	O
.	O
280	O
references	O
rumelhart	O
,	O
d.	O
e.	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
and	O
j.	O
,	O
w.	O
r.	O
(	O
1986	O
)	O
.	O
learning	O
internal	O
representations	O
by	O
error	O
propagation	O
.	O
in	O
rumelhart	O
,	O
d.	O
e.	O
and	O
mcclelland	O
,	O
j.	O
l.	O
,	O
editors	O
,	O
parallel	O
distributed	O
processing	O
,	O
volume	O
1	O
,	O
pages	O
318–362	O
.	O
mit	O
press	O
,	O
cambridge	O
ma	O
.	O
sakawa	O
,	O
y.	O
and	O
shinido	O
,	O
y	O
.	O
(	O
1982	O
)	O
.	O
optimal	O
control	O
of	O
container	O
crane	O
.	O
automatica	O
,	O
18	O
(	O
3	O
)	O
:257–266	O
.	O
sammut	O
,	O
c.	O
(	O
1988	O
)	O
.	O
experimental	O
results	O
from	O
an	O
evaluation	O
of	O
algorithms	O
that	O
learn	O
to	O
control	O
dynamic	O
systems	O
.	O
in	O
laird	O
,	O
j.	O
,	O
editor	O
,	O
proceedings	O
of	O
the	O
ﬁfth	O
international	O
conference	O
on	O
machine	O
learning	O
.	O
ann	O
arbor	O
,	O
michigan	O
,	O
pages	O
437–443	O
,	O
san	O
mateo	O
,	O
ca	O
.	O
morgan	O
kaufmann	O
.	O
sammut	O
,	O
c.	O
(	O
1994	O
)	O
.	O
recent	O
progress	O
with	O
boxes	O
.	O
to	O
appear	O
in	O
machine	O
intelligence	O
and	O
inductive	B
learning	I
,	O
vol	O
.	O
1	O
(	O
eds	O
.	O
furukawa	O
,	O
k.	O
and	O
muggleton	O
,	O
s.	O
h.	O
,	O
new	O
series	O
of	O
machine	O
intelligence	O
,	O
ed	O
.	O
in	O
chief	O
d.	O
michie	O
)	O
,	O
oxford	O
:	O
oxford	O
university	O
press	O
.	O
sammut	O
,	O
c.	O
and	O
cribb	O
,	O
j	O
.	O
(	O
1990	O
)	O
.	O
is	O
learning	O
rate	O
a	O
good	O
performance	O
criterion	O
of	O
learning	O
?	O
in	O
proceedings	O
of	O
the	O
seventh	O
international	O
machine	O
learning	O
conference	O
,	O
pages	O
170–	O
178	O
,	O
austin	O
,	O
texas	O
.	O
morgan	O
kaufmann	O
.	O
sammut	O
,	O
c.	O
,	O
hurst	O
,	O
s.	O
,	O
kedzier	O
,	O
d.	O
,	O
and	O
michie	O
,	O
d.	O
(	O
1992	O
)	O
.	O
learning	O
to	O
ﬂy	O
.	O
in	O
sleeman	O
,	O
d.	O
and	O
edwards	O
,	O
p.	O
,	O
editors	O
,	O
proceedings	O
of	O
the	O
ninth	O
international	O
workshop	O
on	O
machine	O
learning	O
,	O
pages	O
385–393	O
.	O
morgan	O
kaufmann	O
.	O
sammut	O
,	O
c.	O
and	O
michie	O
,	O
d.	O
(	O
1991	O
)	O
.	O
controlling	O
a	O
“	O
black	O
box	O
”	O
simulation	O
of	O
a	O
space	O
craft	O
.	O
ai	O
magazine	O
,	O
12	O
(	O
1	O
)	O
:56–63	O
.	O
sammut	O
,	O
c.	O
a.	O
and	O
banerji	O
,	O
r.	O
b	O
.	O
(	O
1986	O
)	O
.	O
learning	O
concepts	O
by	O
asking	O
questions	O
.	O
in	O
r.	O
s.	O
michalski	O
,	O
j.	O
c.	O
and	O
mitchell	O
,	O
t.	O
,	O
editors	O
,	O
machine	O
learning	O
:	O
an	O
artiﬁcial	O
intelli-	O
gence	O
approach	O
,	O
vol	O
2	O
,	O
pages	O
167–192	O
.	O
morgan	O
kaufmann	O
,	O
los	O
altos	O
,	O
california	O
.	O
sas	O
(	O
1985	O
)	O
.	O
statistical	B
analysis	O
system	O
.	O
sas	O
institute	O
inc.	O
,	O
cary	O
,	O
nc	O
,	O
version	O
5	O
edition	O
.	O
scalero	O
,	O
r.	O
and	O
tepedelenlioglu	O
,	O
n.	O
(	O
1992	O
)	O
.	O
a	O
fast	O
new	O
algorithm	O
for	O
training	O
feedforward	O
neural	O
networks	O
.	O
ieee	O
transactions	O
on	O
signal	O
processing	O
,	O
40:202–210	O
.	O
schalkoff	O
,	O
r.	O
j	O
.	O
(	O
1992	O
)	O
.	O
pattern	O
recognotion	O
:	O
statistical	B
,	O
structural	O
and	O
neural	O
ap-	O
proaches	O
.	O
wiley	O
,	O
singapore	O
.	O
schoppers	O
,	O
m.	O
(	O
1991	O
)	O
.	O
real-time	O
knowledge-based	O
control	O
systems	O
.	O
communications	O
of	O
the	O
acm	O
,	O
34	O
(	O
8	O
)	O
:27–30	O
.	O
schumann	O
,	O
m.	O
,	O
lehrbach	O
,	O
t.	O
,	O
and	O
bahrs	O
,	O
p.	O
(	O
1992	O
)	O
.	O
versuche	O
zur	O
kreditwurdigkeitsprognose	O
mit	O
kunstlichen	O
neuronalen	O
netzen	O
.	O
universitat	O
gottingen	O
.	O
scott	O
,	O
d.	O
w.	O
(	O
1992	O
)	O
.	O
multivariate	O
density	O
estimation	O
:	O
theory	O
,	O
practice	O
,	O
and	O
visualization	O
.	O
john	O
wiley	O
,	O
new	O
york	O
.	O
sethi	O
,	O
i.	O
k.	O
and	O
otten	O
,	O
m.	O
(	O
1990	O
)	O
.	O
comparison	O
between	O
entropy	O
net	O
and	O
decision	O
tree	O
classiﬁers	O
.	O
in	O
ijcnn-90	O
:	O
proceedings	O
of	O
the	O
international	O
joint	O
conference	O
on	O
neural	O
networks	O
,	O
pages	O
63–68	O
,	O
ann	O
arbor	O
,	O
mi	O
.	O
ieee	O
neural	O
networks	O
council	O
.	O
shadmehr	O
,	O
r.	O
and	O
d	O
’	O
argenio	O
,	O
z	O
.	O
(	O
1990	O
)	O
.	O
a	O
comparison	O
of	O
a	O
neural	O
network	O
based	O
esti-	O
mator	O
and	O
two	O
statistical	B
estimators	O
in	O
a	O
sparse	O
and	O
noisy	O
environment	O
.	O
in	O
ijcnn-90	O
:	O
proceedings	O
of	O
the	O
international	O
joint	O
conference	O
on	O
neural	O
networks	O
,	O
pages	O
289–292	O
,	O
ann	O
arbor	O
,	O
mi	O
.	O
ieee	O
neural	O
networks	O
council	O
.	O
shapiro	O
,	O
a.	O
d.	O
(	O
1987	O
)	O
.	O
structured	O
induction	O
in	O
expert	O
systems	O
.	O
addison	O
wesley	O
,	O
london	O
.	O
references	O
281	O
shapiro	O
,	O
a.	O
d.	O
and	O
michie	O
,	O
d.	O
(	O
1986	O
)	O
.	O
a	O
self-commenting	O
facility	O
for	O
inductively	O
syn-	O
thesized	O
end-game	O
expertise	O
.	O
in	O
beal	O
,	O
d.	O
f.	O
,	O
editor	O
,	O
advances	O
in	O
computer	O
chess	O
5.	O
pergamon	O
,	O
oxford	O
.	O
shapiro	O
,	O
a.	O
d.	O
and	O
niblett	O
,	O
t.	O
(	O
1982	O
)	O
.	O
automatic	O
induction	O
of	O
classiﬁcation	B
rules	O
for	O
a	O
chess	O
endgame	O
.	O
in	O
clarke	O
,	O
m.	O
r.	O
b.	O
,	O
editor	O
,	O
advances	O
in	O
computer	O
chess	O
3.	O
pergamon	O
,	O
oxford	O
.	O
shastri	O
,	O
l.	O
and	O
ajjangadde	O
,	O
v.	O
from	O
simple	O
associations	O
to	O
systematic	O
reasoning	O
:	O
a	O
connectionist	O
representation	O
of	O
rules	O
,	O
variables	O
,	O
and	O
dynamic	O
bindings	O
using	O
temporal	O
synchrony	O
.	O
behavioral	O
and	O
brain	O
sciences	O
.	O
to	O
appear	O
.	O
shavlik	O
,	O
j.	O
,	O
mooney	O
,	O
r.	O
,	O
and	O
towell	O
,	O
g.	O
(	O
1991	O
)	O
.	O
symbolic	O
and	O
neural	O
learning	O
algorithms	O
:	O
an	O
experimental	O
comparison	O
.	O
machine	O
learning	O
,	O
6:111–143	O
.	O
siebert	O
,	O
j.	O
p.	O
(	O
1987	O
)	O
.	O
vehicle	B
recognition	I
using	O
rule	O
based	O
methods	O
.	O
tirm-87-018	O
,	O
turing	O
institute	O
.	O
silva	O
,	O
f.	O
m.	O
and	O
almeida	O
,	O
l.	O
b	O
.	O
(	O
1990	O
)	O
.	O
acceleration	O
techniques	O
for	O
the	O
backpropagation	O
algorithm	O
.	O
in	O
almeida	O
,	O
l.	O
b.	O
and	O
wellekens	O
,	O
c.	O
j.	O
,	O
editors	O
,	O
lecture	O
notes	O
in	O
computer	O
science	O
412	O
,	O
neural	O
networks	O
,	O
pages	O
110–119	O
.	O
springer-verlag	O
,	O
berlin	O
.	O
silverman	O
,	O
b.	O
w.	O
(	O
1986	O
)	O
.	O
density	O
estimation	O
for	O
statistics	O
and	O
data	O
analysis	O
.	O
chapman	O
and	O
hall	O
,	O
london	O
.	O
smith	O
,	O
j.	O
w.	O
,	O
everhart	O
,	O
j.	O
e.	O
,	O
dickson	O
,	O
w.	O
c.	O
,	O
knowler	O
,	O
w.	O
c.	O
,	O
and	O
johannes	O
,	O
r.	O
s.	O
(	O
1988	O
)	O
.	O
using	O
the	O
adap	O
learning	O
algorithm	O
to	O
forecast	O
the	O
onset	O
of	O
diabetes	B
mellitus	O
.	O
in	O
proceedings	O
of	O
the	O
symposium	O
on	O
computer	O
applications	O
and	O
medical	O
care	O
,	O
pages	O
261–265	O
.	O
ieee	O
computer	O
society	O
press	O
.	O
smith	O
,	O
p.	O
l.	O
(	O
1982	O
)	O
.	O
curve	O
ﬁtting	O
and	O
modeling	O
with	O
splines	O
using	O
statistical	B
variable	O
selec-	O
tion	B
techniques	O
.	O
technical	B
report	O
nasa	O
166034	O
,	O
langley	O
research	O
center	O
,	O
hampton	O
,	O
va.	O
snedecor	O
,	O
w.	O
and	O
cochran	O
,	O
w.	O
g.	O
(	O
1980	O
)	O
.	O
statistical	B
methods	O
(	O
7th	O
edition	O
)	O
.	O
iowa	O
state	O
university	O
press	O
,	O
iowa	O
,	O
u.s.a.	O
spiegelhalter	O
,	O
d.	O
j.	O
,	O
dawid	O
,	O
a.	O
p.	O
,	O
lauritzen	O
,	O
s.	O
l.	O
,	O
and	O
cowell	O
,	O
r.	O
g.	O
(	O
1993	O
)	O
.	O
bayesian	O
analysis	O
in	O
expert	O
systems	O
.	O
statistical	B
science	O
,	O
8:219–247	O
.	O
spikovska	O
,	O
l.	O
and	O
reid	O
,	O
m.	O
b	O
.	O
(	O
1990	O
)	O
.	O
an	O
empirical	O
comparison	O
of	O
id3	O
and	O
honns	O
for	O
distortion	O
invariant	O
object	O
recognition	O
.	O
in	O
tai-90	O
:	O
tools	O
for	O
artiﬁcial	O
intelligence	O
:	O
pro-	O
ceedings	O
of	O
the	O
2nd	O
international	O
ieee	O
conference	O
,	O
los	O
alamitos	O
,	O
ca	O
.	O
ieee	O
computer	O
society	O
press	O
.	O
spirtes	O
,	O
p.	O
,	O
scheines	O
,	O
r.	O
,	O
glymour	O
,	O
c.	O
,	O
and	O
meek	O
,	O
c.	O
(	O
1992	O
)	O
.	O
tetradii	O
,	O
tools	O
for	O
discovery	O
.	O
srinivisan	O
,	O
v.	O
and	O
kim	O
,	O
y.	O
h.	O
(	O
1987	O
)	O
.	O
credit	O
granting	O
:	O
a	O
comparative	O
analysis	O
of	O
classiﬁ-	O
cation	O
procedures	O
.	O
the	O
journal	O
of	O
finance	O
,	O
42:665–681	O
.	O
statsci	O
(	O
1991	O
)	O
.	O
s-plus	O
user	O
’	O
s	O
manual	O
.	O
technical	B
report	O
,	O
statsci	O
europe	O
,	O
oxford	O
.	O
u.k.	O
stein	O
von	O
,	O
j.	O
h.	O
and	O
ziegler	O
,	O
w.	O
(	O
1984	O
)	O
.	O
the	O
prognosis	O
and	O
surveillance	O
of	O
risks	O
from	O
commercial	O
credit	O
borrowers	O
.	O
journal	O
of	O
banking	O
and	O
finance	O
,	O
8:249–268	O
.	O
stone	O
,	O
m.	O
(	O
1974	O
)	O
.	O
cross-validatory	O
choice	O
and	O
assessment	O
of	O
statistical	B
predictions	O
.	O
j.	O
roy	O
.	O
statist	O
.	O
soc.	O
,	O
36:111–147	O
(	O
including	O
discussion	O
)	O
.	O
switzer	O
,	O
p.	O
(	O
1980	O
)	O
.	O
extensions	O
of	O
linear	O
discriminant	O
analysis	O
for	O
statistical	B
classiﬁcation	O
of	O
remotely	O
sensed	O
satellite	O
imagery	O
.	O
j.	O
int	O
.	O
assoc	O
.	O
for	O
mathematical	O
geology	O
,	O
23:367–376	O
.	O
282	O
references	O
switzer	O
,	O
p.	O
(	O
1983	O
)	O
.	O
some	O
spatial	O
statistics	O
for	O
the	O
interpretation	O
of	O
satellite	O
data	O
.	O
bull	O
.	O
int	O
.	O
stat	O
.	O
inst.	O
,	O
50:962–971	O
.	O
thrun	O
,	O
s.	O
b.	O
,	O
mitchell	O
,	O
t.	O
,	O
and	O
cheng	O
,	O
j	O
.	O
(	O
1991	O
)	O
.	O
the	O
monk	O
’	O
s	O
comparison	O
of	O
learning	O
algorithms	O
-	O
introduction	O
and	O
survey	O
.	O
in	O
thrun	O
,	O
s.	O
,	O
bala	O
,	O
j.	O
,	O
bloedorn	O
,	O
e.	O
,	O
and	O
bratko	O
,	O
i.	O
,	O
editors	O
,	O
the	O
monk	O
’	O
s	O
problems	O
-	O
a	O
performance	O
comparison	O
of	O
different	O
learning	O
algorithms	O
,	O
pages	O
1–6	O
.	O
carnegie	O
mellon	O
university	O
,	O
computer	O
science	O
department	O
.	O
titterington	O
,	O
d.	O
m.	O
,	O
murray	O
,	O
g.	O
d.	O
,	O
murray	O
,	O
l.	O
s.	O
,	O
spiegelhalter	O
,	O
d.	O
j.	O
,	O
skene	O
,	O
a.	O
m.	O
,	O
habbema	O
,	O
j.	O
d.	O
f.	O
,	O
and	O
gelpke	O
,	O
g.	O
j	O
.	O
(	O
1981	O
)	O
.	O
comparison	O
of	O
discrimination	O
techniques	O
applied	O
to	O
a	O
complex	O
data	O
set	O
of	O
head	O
injured	O
patients	O
(	O
with	O
discussion	O
)	O
.	O
j.	O
royal	O
statist	O
.	O
soc	O
.	O
a	O
,	O
144:145–175	O
.	O
nearest	O
neighbour	O
method	O
:	O
the	O
inﬂuence	O
of	O
data	O
transformations	O
and	O
metrics	O
.	O
chemometrics	O
intell	O
.	O
labor	O
.	O
syst.	O
,	O
6:213–220	O
.	O
todeschini	O
,	O
r.	O
(	O
1989	O
)	O
.	O
toolenaere	O
,	O
t.	O
(	O
1990	O
)	O
.	O
supersab	O
:	O
fast	O
adaptive	O
back	O
propagation	O
with	O
good	O
scaling	O
prop-	O
erties	O
.	O
neural	O
networks	O
,	O
3:561–574	O
.	O
tsaptsinos	O
,	O
d.	O
,	O
mirzai	O
,	O
a.	O
,	O
and	O
jervis	O
,	O
b	O
.	O
(	O
1990	O
)	O
.	O
comparison	O
of	O
machine	O
learning	O
paradigms	O
in	O
a	O
classiﬁcation	B
task	O
.	O
in	O
rzevski	O
,	O
g.	O
,	O
editor	O
,	O
applications	O
of	O
artiﬁcial	O
intelligence	O
in	O
engineering	O
v	O
:	O
proceedings	O
of	O
the	O
ﬁfth	O
international	O
conference	O
,	O
berlin	O
.	O
springer-verlag	O
.	O
turing	O
,	O
a.	O
m.	O
(	O
1986	O
)	O
.	O
lecture	O
to	O
the	O
london	O
mathematical	O
society	O
on	O
20	O
february	O
1947.	O
in	O
carpenter	O
,	O
b.	O
e.	O
and	O
doran	O
,	O
r.	O
w.	O
,	O
editors	O
,	O
a.	O
m.	O
turing	O
’	O
s	O
ace	O
report	O
and	O
other	O
papers	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
unger	O
,	O
s.	O
and	O
wysotzki	O
,	O
f.	O
(	O
1981	O
)	O
.	O
lernf¨ahige	O
klassiﬁzierungssysteme	O
.	O
akademie-verlag	O
,	O
berlin	O
.	O
urbanˇciˇc	O
,	O
t.	O
and	O
bratko	O
,	O
i	O
.	O
(	O
1992	O
)	O
.	O
knowledge	O
acquisition	O
for	O
dynamic	O
system	O
control	O
.	O
in	O
souˇcek	O
,	O
b.	O
,	O
editor	O
,	O
dynamic	O
,	O
genetic	O
,	O
and	O
chaotic	O
programming	O
,	O
pages	O
65–83	O
.	O
wiley	O
&	O
sons	O
.	O
urbanˇciˇc	O
,	O
t.	O
,	O
juriˇci´c	O
,	O
d.	O
,	O
filipiˇc	O
,	O
b.	O
,	O
and	O
bratko	O
,	O
i	O
.	O
(	O
1992	O
)	O
.	O
automated	O
synthesis	O
of	O
control	O
for	O
non-linear	O
dynamic	O
systems	O
.	O
in	O
preprints	O
of	O
the	O
1992	O
ifac/ifip/imacs	O
international	O
symposium	O
on	O
artiﬁcial	O
intelligence	O
in	O
real-time	O
control	O
,	O
pages	O
605–610	O
.	O
delft	O
,	O
the	O
netherlands	O
.	O
varˇsek	O
,	O
a.	O
,	O
urbanˇciˇc	O
,	O
t.	O
,	O
and	O
filipiˇc	O
,	O
b	O
.	O
(	O
1993	O
)	O
.	O
genetic	B
algorithms	I
in	O
controller	B
design	I
and	O
tuning	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
and	O
cybernetics	O
.	O
verbruggen	O
,	O
h.	O
b.	O
and	O
˚astr˝om	O
,	O
k.	O
j	O
.	O
(	O
1989	O
)	O
.	O
artiﬁcial	O
intelligence	O
and	O
feedback	O
control	O
.	O
in	O
proceedings	O
of	O
the	O
second	O
ifac	O
workshop	O
on	O
artiﬁcial	O
intelligence	O
in	O
real-time	O
control	O
,	O
pages	O
115–125	O
.	O
shenyang	O
,	O
prc	O
.	O
wald	O
,	O
a	O
.	O
(	O
1947	O
)	O
.	O
sequential	O
analysis	O
.	O
chapman	O
&	O
hall	O
,	O
london	O
.	O
wasserman	O
,	O
p.	O
d.	O
(	O
1989	O
)	O
.	O
neural	O
computing	O
,	O
theory	O
and	O
practice	O
.	O
van	O
nostrand	O
reinhold	O
.	O
watkins	O
,	O
c.	O
j.	O
c.	O
h.	O
(	O
1987	O
)	O
.	O
combining	O
cross-validation	O
and	O
search	O
.	O
in	O
bratko	O
,	O
i.	O
and	O
lavrac	O
,	O
n.	O
,	O
editors	O
,	O
progress	O
in	O
machine	O
learning	O
,	O
pages	O
79–87	O
,	O
wimslow	O
.	O
sigma	O
books	O
.	O
wehenkel	O
,	O
l.	O
,	O
pavella	O
,	O
m.	O
,	O
euxibie	O
,	O
e.	O
,	O
and	O
heilbronn	O
,	O
b	O
.	O
(	O
1993	O
)	O
.	O
decision	O
tree	O
based	O
transient	O
stability	O
assessment	O
-	O
a	O
case	O
study	O
.	O
volume	O
proceedings	O
of	O
ieee/pes	O
1993	O
winter	O
meeting	O
,	O
columbus	O
,	O
oh	O
,	O
jan/feb	O
.	O
5.	O
,	O
pages	O
paper	O
#	O
93	O
wm	O
235–2	O
pwrs	O
.	O
references	O
283	O
weiss	O
,	O
s.	O
m.	O
and	O
kapouleas	O
,	O
i	O
.	O
(	O
1989	O
)	O
.	O
an	O
empirical	O
comparison	O
of	O
pattern	O
recognition	O
,	O
neural	O
nets	O
and	O
machine	O
learning	O
classiﬁcation	B
methods	O
(	O
vol	O
1	O
)	O
.	O
in	O
ijcai	O
89	O
:	O
proceedings	O
of	O
the	O
eleventh	O
international	O
joint	O
conference	O
on	O
artiﬁcial	O
intelligence	O
,	O
detroit	O
,	O
mi	O
,	O
pages	O
781–787	O
,	O
san	O
mateo	O
,	O
ca	O
.	O
morgan	O
kaufmann	O
.	O
weiss	O
,	O
s.	O
m.	O
and	O
kulikowski	O
,	O
c.	O
a	O
.	O
(	O
1991	O
)	O
.	O
computer	O
systems	O
that	O
learn	O
:	O
classiﬁcation	B
and	O
prediction	O
methods	O
from	O
statistics	O
,	O
neural	O
networks	O
,	O
machine	O
learning	O
and	O
expert	O
systems	O
.	O
morgan	O
kaufmann	O
,	O
san	O
mateo	O
,	O
ca	O
.	O
werbos	O
,	O
p.	O
(	O
1975	O
)	O
.	O
beyond	O
regression	O
:	O
new	O
tools	O
for	O
prediction	O
and	O
analysis	O
in	O
the	O
behavioural	O
sciences	O
.	O
phd	O
thesis	O
,	O
harvard	O
university	O
.	O
also	O
printed	O
as	O
a	O
report	O
of	O
the	O
harvard	O
/	O
mit	O
cambridge	O
project	O
.	O
whittaker	O
,	O
j	O
.	O
(	O
1990	O
)	O
.	O
graphical	O
models	O
in	O
applied	O
multivariate	O
analysis	O
.	O
john	O
wiley	O
,	O
chichester	O
.	O
widrow	O
,	O
b	O
.	O
(	O
1962	O
)	O
.	O
generalization	O
and	O
information	O
in	O
networks	O
of	O
adaline	O
neurons	O
.	O
in	O
yovits	O
,	O
j.	O
and	O
goldstein	O
,	O
editors	O
,	O
self-organizing	O
systems	O
,	O
washington	O
.	O
spartan	O
books	O
.	O
wolpert	O
,	O
d.	O
h.	O
(	O
1992	O
)	O
.	O
a	O
rigorous	O
investigation	O
of	O
“	O
evidence	O
”	O
and	O
“	O
occam	O
factors	O
”	O
in	O
bayesian	O
reasoning	O
.	O
technical	B
report	O
,	O
the	O
sante	O
fe	O
institute	O
,	O
1660	O
old	O
pecos	O
trail	O
,	O
suite	O
a	O
,	O
sante	O
fe	O
,	O
nm	O
,	O
87501	O
,	O
usa	O
.	O
wu	O
,	O
j.	O
x.	O
and	O
chan	O
,	O
c.	O
(	O
1991	O
)	O
.	O
a	O
three	O
layer	O
adaptive	O
network	O
for	O
pattern	O
density	O
estimation	O
and	O
classiﬁcation	B
.	O
international	O
journal	O
of	O
neural	O
systems	O
,	O
2	O
(	O
3	O
)	O
:211–220	O
.	O
wynne-jones	O
,	O
m.	O
(	O
1991	O
)	O
.	O
constructive	O
algorithms	O
and	O
pruning	B
:	O
improving	O
the	O
multi	O
layer	O
perceptron	O
.	O
in	O
proceedings	O
of	O
imacs	O
’	O
91	O
,	O
the	O
13th	O
world	O
congress	O
on	O
computation	O
and	O
applied	O
mathematics	O
,	O
dublin	O
,	O
volume	O
2	O
,	O
pages	O
747–750	O
.	O
wynne-jones	O
,	O
m.	O
(	O
1992	O
)	O
.	O
node	O
splitting	O
:	O
a	O
constructive	O
algorithm	O
for	O
feed-forard	O
neural	O
networks	O
.	O
in	O
moody	O
,	O
j.	O
e.	O
,	O
hanson	O
,	O
s.	O
j.	O
,	O
and	O
lippmann	O
,	O
r.	O
p.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
4	O
,	O
pages	O
1072–1079	O
.	O
morgan	O
kaufmann	O
.	O
wynne-jones	O
,	O
m.	O
(	O
1993	O
)	O
.	O
node	O
splitting	O
:	O
a	O
constructive	O
algorithm	O
for	O
feed-forward	O
neural	O
networks	O
.	O
neural	O
computing	O
and	O
applications	O
,	O
1	O
(	O
1	O
)	O
:17–22	O
.	O
xu	O
,	O
l.	O
,	O
krzyzak	O
,	O
a.	O
,	O
and	O
oja	O
,	O
e.	O
(	O
1991	O
)	O
.	O
neural	O
nets	O
for	O
dual	O
subspace	O
pattern	O
recognition	O
method	O
.	O
international	O
journal	O
of	O
neural	O
systems	O
,	O
2	O
(	O
3	O
)	O
:169–184	O
.	O
yang	O
,	O
j.	O
and	O
honavar	O
,	O
v.	O
(	O
1991	O
)	O
.	O
experiments	O
with	O
the	O
cascade-correlation	O
algorithm	O
.	O
technical	B
report	O
91-16	O
,	O
department	O
of	O
computer	O
science	O
,	O
iowa	O
state	O
university	O
,	O
ames	O
,	O
ia	O
50011-1040	O
,	O
usa	O
.	O
yasunobu	O
,	O
s.	O
and	O
hasegawa	O
,	O
t.	O
(	O
1986	O
)	O
.	O
evaluation	O
of	O
an	O
automatic	O
container	O
crane	O
opera-	O
tion	B
system	O
based	O
on	O
predictive	O
fuzzy	O
control	O
.	O
control-theory	O
and	O
advanced	O
technology	O
,	O
2	O
(	O
3	O
)	O
:419–432	O
.	O