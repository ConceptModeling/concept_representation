understanding	O
machine	O
learning	O
:	O
from	O
theory	O
to	O
algorithms	O
preface	O
the	O
term	O
machine	O
learning	O
refers	O
to	O
the	O
automated	O
detection	O
of	O
meaningful	O
patterns	O
in	O
data	O
.	O
in	O
the	O
past	O
couple	O
of	O
decades	O
it	O
has	O
become	O
a	O
common	O
tool	O
in	O
almost	O
any	O
task	O
that	O
requires	O
information	O
extraction	O
from	O
large	O
data	O
sets	O
.	O
we	O
are	O
surrounded	O
by	O
a	O
machine	O
learning	O
based	O
technology	O
:	O
search	O
engines	O
learn	O
how	O
to	O
bring	O
us	O
the	O
best	O
results	O
(	O
while	O
placing	O
proﬁtable	O
ads	O
)	O
,	O
anti-spam	O
software	O
learns	O
to	O
ﬁlter	O
our	O
email	O
messages	O
,	O
and	O
credit	O
card	O
transactions	O
are	O
secured	O
by	O
a	O
software	O
that	O
learns	O
how	O
to	O
detect	O
frauds	O
.	O
digital	O
cameras	O
learn	O
to	O
detect	O
faces	O
and	O
intelligent	O
personal	O
assistance	O
applications	O
on	O
smart-phones	O
learn	O
to	O
recognize	O
voice	O
commands	O
.	O
cars	O
are	O
equipped	O
with	O
accident	O
prevention	O
systems	O
that	O
are	O
built	O
using	O
machine	O
learning	O
algorithms	O
.	O
machine	O
learning	O
is	O
also	O
widely	O
used	O
in	O
scientiﬁc	O
applications	O
such	O
as	O
bioinformatics	O
,	O
medicine	O
,	O
and	O
astronomy	O
.	O
one	O
common	O
feature	B
of	O
all	O
of	O
these	O
applications	O
is	O
that	O
,	O
in	O
contrast	O
to	O
more	O
traditional	O
uses	O
of	O
computers	O
,	O
in	O
these	O
cases	O
,	O
due	O
to	O
the	O
complexity	O
of	O
the	O
patterns	O
that	O
need	O
to	O
be	O
detected	O
,	O
a	O
human	O
programmer	O
can	O
not	O
provide	O
an	O
explicit	O
,	O
ﬁne-	O
detailed	O
speciﬁcation	O
of	O
how	O
such	O
tasks	O
should	O
be	O
executed	O
.	O
taking	O
example	O
from	O
intelligent	O
beings	O
,	O
many	O
of	O
our	O
skills	O
are	O
acquired	O
or	O
reﬁned	O
through	O
learning	O
from	O
our	O
experience	O
(	O
rather	O
than	O
following	O
explicit	O
instructions	O
given	O
to	O
us	O
)	O
.	O
machine	O
learning	O
tools	O
are	O
concerned	O
with	O
endowing	O
programs	O
with	O
the	O
ability	O
to	O
“	O
learn	O
”	O
and	O
adapt	O
.	O
the	O
ﬁrst	O
goal	O
of	O
this	O
book	O
is	O
to	O
provide	O
a	O
rigorous	O
,	O
yet	O
easy	O
to	O
follow	O
,	O
intro-	O
duction	O
to	O
the	O
main	O
concepts	O
underlying	O
machine	O
learning	O
:	O
what	O
is	O
learning	O
?	O
how	O
can	O
a	O
machine	O
learn	O
?	O
how	O
do	O
we	O
quantify	O
the	O
resources	O
needed	O
to	O
learn	O
a	O
given	O
concept	O
?	O
is	O
learning	O
always	O
possible	O
?	O
can	O
we	O
know	O
if	O
the	O
learning	O
process	O
succeeded	O
or	O
failed	O
?	O
the	O
second	O
goal	O
of	O
this	O
book	O
is	O
to	O
present	O
several	O
key	O
machine	O
learning	O
algo-	O
rithms	O
.	O
we	O
chose	O
to	O
present	O
algorithms	O
that	O
on	O
one	O
hand	O
are	O
successfully	O
used	O
in	O
practice	O
and	O
on	O
the	O
other	O
hand	O
give	O
a	O
wide	O
spectrum	O
of	O
diﬀerent	O
learning	O
techniques	O
.	O
additionally	O
,	O
we	O
pay	O
speciﬁc	O
attention	O
to	O
algorithms	O
appropriate	O
for	O
large	O
scale	O
learning	O
(	O
a.k.a	O
.	O
“	O
big	O
data	O
”	O
)	O
,	O
since	O
in	O
recent	O
years	O
,	O
our	O
world	O
has	O
be-	O
come	O
increasingly	O
“	O
digitized	O
”	O
and	O
the	O
amount	O
of	O
data	O
available	O
for	O
learning	O
is	O
dramatically	O
increasing	O
.	O
as	O
a	O
result	O
,	O
in	O
many	O
applications	O
data	O
is	O
plentiful	O
and	O
computation	O
time	O
is	O
the	O
main	O
bottleneck	O
.	O
we	O
therefore	O
explicitly	O
quantify	O
both	O
the	O
amount	O
of	O
data	O
and	O
the	O
amount	O
of	O
computation	O
time	O
needed	O
to	O
learn	O
a	O
given	O
concept	O
.	O
the	O
book	O
is	O
divided	O
into	O
four	O
parts	O
.	O
the	O
ﬁrst	O
part	O
aims	O
at	O
giving	O
an	O
initial	O
rigorous	O
answer	O
to	O
the	O
fundamental	O
questions	O
of	O
learning	O
.	O
we	O
describe	O
a	O
gen-	O
eralization	O
of	O
valiant	O
’	O
s	O
probably	O
approximately	O
correct	O
(	O
pac	O
)	O
learning	O
model	O
,	O
which	O
is	O
a	O
ﬁrst	O
solid	O
answer	O
to	O
the	O
question	O
“	O
what	O
is	O
learning	O
?	O
”	O
.	O
we	O
describe	O
the	O
empirical	B
risk	I
minimization	O
(	O
erm	O
)	O
,	O
structural	O
risk	B
minimization	O
(	O
srm	O
)	O
,	O
and	O
minimum	O
description	O
length	O
(	O
mdl	O
)	O
learning	O
rules	O
,	O
which	O
shows	O
“	O
how	O
can	O
a	O
machine	O
learn	O
”	O
.	O
we	O
quantify	O
the	O
amount	O
of	O
data	O
needed	O
for	O
learning	O
using	O
the	O
erm	O
,	O
srm	O
,	O
and	O
mdl	O
rules	O
and	O
show	O
how	O
learning	O
might	O
fail	O
by	O
deriving	O
viii	O
a	O
“	O
no-free-lunch	B
”	O
theorem	O
.	O
we	O
also	O
discuss	O
how	O
much	O
computation	O
time	O
is	O
re-	O
quired	O
for	O
learning	O
.	O
in	O
the	O
second	O
part	O
of	O
the	O
book	O
we	O
describe	O
various	O
learning	O
algorithms	O
.	O
for	O
some	O
of	O
the	O
algorithms	O
,	O
we	O
ﬁrst	O
present	O
a	O
more	O
general	O
learning	O
principle	O
,	O
and	O
then	O
show	O
how	O
the	O
algorithm	O
follows	O
the	O
principle	O
.	O
while	O
the	O
ﬁrst	O
two	O
parts	O
of	O
the	O
book	O
focus	O
on	O
the	O
pac	O
model	O
,	O
the	O
third	O
part	O
extends	O
the	O
scope	O
by	O
presenting	O
a	O
wider	O
variety	O
of	O
learning	O
models	O
.	O
finally	O
,	O
the	O
last	O
part	O
of	O
the	O
book	O
is	O
devoted	O
to	O
advanced	O
theory	O
.	O
we	O
made	O
an	O
attempt	O
to	O
keep	O
the	O
book	O
as	O
self-contained	O
as	O
possible	O
.	O
however	O
,	O
the	O
reader	O
is	O
assumed	O
to	O
be	O
comfortable	O
with	O
basic	O
notions	O
of	O
probability	O
,	O
linear	O
algebra	O
,	O
analysis	O
,	O
and	O
algorithms	O
.	O
the	O
ﬁrst	O
three	O
parts	O
of	O
the	O
book	O
are	O
intended	O
for	O
ﬁrst	O
year	O
graduate	O
students	O
in	O
computer	O
science	O
,	O
engineering	O
,	O
mathematics	O
,	O
or	O
statistics	O
.	O
it	O
can	O
also	O
be	O
accessible	O
to	O
undergraduate	O
students	O
with	O
the	O
adequate	O
background	O
.	O
the	O
more	O
advanced	O
chapters	O
can	O
be	O
used	O
by	O
researchers	O
intending	O
to	O
gather	O
a	O
deeper	O
theoretical	O
understanding	O
.	O
acknowledgements	O
the	O
book	O
is	O
based	O
on	O
introduction	O
to	O
machine	O
learning	O
courses	O
taught	O
by	O
shai	O
shalev-shwartz	O
at	O
the	O
hebrew	O
university	O
and	O
by	O
shai	O
ben-david	O
at	O
the	O
univer-	O
sity	O
of	O
waterloo	O
.	O
the	O
ﬁrst	O
draft	O
of	O
the	O
book	O
grew	O
out	O
of	O
the	O
lecture	O
notes	O
for	O
the	O
course	O
that	O
was	O
taught	O
at	O
the	O
hebrew	O
university	O
by	O
shai	O
shalev-shwartz	O
during	O
2010–2013	O
.	O
we	O
greatly	O
appreciate	O
the	O
help	O
of	O
ohad	O
shamir	O
,	O
who	O
served	O
as	O
a	O
ta	O
for	O
the	O
course	O
in	O
2010	O
,	O
and	O
of	O
alon	O
gonen	O
,	O
who	O
served	O
as	O
a	O
ta	O
for	O
the	O
course	O
in	O
2011–2013	O
.	O
ohad	O
and	O
alon	O
prepared	O
few	O
lecture	O
notes	O
and	O
many	O
of	O
the	O
exercises	O
.	O
alon	O
,	O
to	O
whom	O
we	O
are	O
indebted	O
for	O
his	O
help	O
throughout	O
the	O
entire	O
making	O
of	O
the	O
book	O
,	O
has	O
also	O
prepared	O
a	O
solution	O
manual	O
.	O
we	O
are	O
deeply	O
grateful	O
for	O
the	O
most	O
valuable	O
work	O
of	O
dana	O
rubinstein	O
.	O
dana	O
has	O
scientiﬁcally	O
proofread	O
and	O
edited	O
the	O
manuscript	O
,	O
transforming	O
it	O
from	O
lecture-based	O
chapters	O
into	O
ﬂuent	O
and	O
coherent	O
text	O
.	O
special	O
thanks	O
to	O
amit	O
daniely	O
,	O
who	O
helped	O
us	O
with	O
a	O
careful	O
read	O
of	O
the	O
advanced	O
part	O
of	O
the	O
book	O
and	O
also	O
wrote	O
the	O
advanced	O
chapter	O
on	O
multiclass	B
learnability	O
.	O
we	O
are	O
also	O
grateful	O
for	O
the	O
members	O
of	O
a	O
book	O
reading	O
club	O
in	O
jerusalem	O
that	O
have	O
carefully	O
read	O
and	O
constructively	O
criticized	O
every	O
line	O
of	O
the	O
manuscript	O
.	O
the	O
members	O
of	O
the	O
reading	O
club	O
are	O
:	O
maya	O
alroy	O
,	O
yossi	O
arje-	O
vani	O
,	O
aharon	O
birnbaum	O
,	O
alon	O
cohen	O
,	O
alon	O
gonen	O
,	O
roi	O
livni	O
,	O
ofer	O
meshi	O
,	O
dan	O
rosenbaum	O
,	O
dana	O
rubinstein	O
,	O
shahar	O
somin	O
,	O
alon	O
vinnikov	O
,	O
and	O
yoav	O
wald	O
.	O
we	O
would	O
also	O
like	O
to	O
thank	O
gal	O
elidan	O
,	O
amir	O
globerson	O
,	O
nika	O
haghtalab	O
,	O
shie	O
mannor	O
,	O
amnon	O
shashua	O
,	O
nati	O
srebro	O
,	O
and	O
ruth	O
urner	O
for	O
helpful	O
discussions	O
.	O
shai	O
shalev-shwartz	O
,	O
jerusalem	O
,	O
israel	O
shai	O
ben-david	O
,	O
waterloo	O
,	O
canada	O
contents	O
preface	O
page	O
vii	O
1	O
introduction	O
1.1	O
what	O
is	O
learning	O
?	O
1.2	O
when	O
do	O
we	O
need	O
machine	O
learning	O
?	O
1.3	O
1.4	O
1.5	O
types	O
of	O
learning	O
relations	O
to	O
other	O
fields	O
how	O
to	O
read	O
this	O
book	O
1.5.1	O
notation	O
1.6	O
possible	O
course	O
plans	O
based	O
on	O
this	O
book	O
part	O
i	O
foundations	O
2	O
3	O
4	O
a	O
gentle	O
start	O
2.1	O
2.2	O
a	O
formal	O
model	O
–	O
the	O
statistical	O
learning	O
framework	O
empirical	B
risk	I
minimization	O
2.2.1	O
something	O
may	O
go	O
wrong	O
–	O
overﬁtting	B
empirical	O
risk	B
minimization	O
with	O
inductive	B
bias	I
2.3.1	O
exercises	O
finite	O
hypothesis	B
classes	O
2.3	O
2.4	O
a	O
formal	O
learning	O
model	O
3.1	O
3.2	O
pac	O
learning	O
a	O
more	O
general	O
learning	O
model	O
3.2.1	O
releasing	O
the	O
realizability	B
assumption	O
–	O
agnostic	O
pac	O
learning	O
3.2.2	O
the	O
scope	O
of	O
learning	O
problems	O
modeled	O
summary	O
bibliographic	O
remarks	O
exercises	O
3.3	O
3.4	O
3.5	O
learning	O
via	O
uniform	B
convergence	I
4.1	O
4.2	O
uniform	B
convergence	I
is	O
suﬃcient	O
for	O
learnability	O
finite	O
classes	O
are	O
agnostic	O
pac	O
learnable	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
19	O
19	O
21	O
22	O
24	O
25	O
26	O
27	O
31	O
33	O
33	O
35	O
35	O
36	O
37	O
41	O
43	O
43	O
44	O
45	O
47	O
49	O
50	O
50	O
54	O
54	O
55	O
x	O
contents	O
4.3	O
4.4	O
4.5	O
summary	O
bibliographic	O
remarks	O
exercises	O
5	O
6	O
7	O
8	O
the	O
bias-complexity	B
tradeoﬀ	I
5.1	O
the	O
no-free-lunch	B
theorem	O
5.1.1	O
no-free-lunch	B
and	O
prior	B
knowledge	I
error	O
decomposition	O
summary	O
bibliographic	O
remarks	O
exercises	O
5.2	O
5.3	O
5.4	O
5.5	O
the	O
vc-dimension	O
6.1	O
6.2	O
6.3	O
intervals	O
finite	O
classes	O
inﬁnite-size	O
classes	O
can	O
be	O
learnable	O
the	O
vc-dimension	O
examples	O
6.3.1	O
threshold	O
functions	O
6.3.2	O
6.3.3	O
axis	O
aligned	O
rectangles	O
6.3.4	O
6.3.5	O
vc-dimension	O
and	O
the	O
number	O
of	O
parameters	O
the	O
fundamental	O
theorem	O
of	O
pac	O
learning	O
proof	O
of	O
theorem	O
6.7	O
6.5.1	O
6.5.2	O
uniform	B
convergence	I
for	O
classes	O
of	O
small	O
eﬀective	O
size	O
summary	O
bibliographic	O
remarks	O
exercises	O
sauer	O
’	O
s	O
lemma	O
and	O
the	O
growth	B
function	I
6.4	O
6.5	O
6.6	O
6.7	O
6.8	O
nonuniform	O
learnability	O
7.1	O
nonuniform	O
learnability	O
7.1.1	O
characterizing	O
nonuniform	O
learnability	O
structural	O
risk	B
minimization	O
7.2	O
7.3	O
minimum	O
description	O
length	O
and	O
occam	O
’	O
s	O
razor	O
7.3.1	O
occam	O
’	O
s	O
razor	O
7.4	O
other	O
notions	O
of	O
learnability	O
–	O
consistency	B
7.5	O
discussing	O
the	O
diﬀerent	O
notions	O
of	O
learnability	O
7.5.1	O
the	O
no-free-lunch	B
theorem	O
revisited	O
summary	O
bibliographic	O
remarks	O
exercises	O
7.6	O
7.7	O
7.8	O
the	O
runtime	O
of	O
learning	O
8.1	O
computational	B
complexity	I
of	O
learning	O
58	O
58	O
58	O
60	O
61	O
63	O
64	O
65	O
66	O
66	O
67	O
67	O
68	O
70	O
70	O
71	O
71	O
72	O
72	O
72	O
73	O
73	O
75	O
78	O
78	O
78	O
83	O
83	O
84	O
85	O
89	O
91	O
92	O
93	O
95	O
96	O
97	O
97	O
100	O
101	O
contents	O
xi	O
finite	O
classes	O
formal	O
deﬁnition*	O
boolean	B
conjunctions	I
learning	O
3-term	O
dnf	O
8.1.1	O
implementing	O
the	O
erm	O
rule	O
8.2.1	O
8.2.2	O
axis	O
aligned	O
rectangles	O
8.2.3	O
8.2.4	O
eﬃciently	O
learnable	O
,	O
but	O
not	O
by	O
a	O
proper	B
erm	O
hardness	O
of	O
learning*	O
summary	O
bibliographic	O
remarks	O
exercises	O
8.2	O
8.3	O
8.4	O
8.5	O
8.6	O
8.7	O
part	O
ii	O
from	O
theory	O
to	O
algorithms	O
9	O
linear	B
programming	I
for	O
the	O
class	O
of	O
halfspaces	O
perceptron	O
for	O
halfspaces	O
least	B
squares	I
linear	O
regression	B
for	O
polynomial	B
regression	I
tasks	O
9.2	O
linear	B
predictors	I
halfspaces	O
9.1	O
9.1.1	O
9.1.2	O
9.1.3	O
the	O
vc	O
dimension	B
of	O
halfspaces	O
linear	B
regression	I
9.2.1	O
9.2.2	O
logistic	B
regression	I
summary	O
bibliographic	O
remarks	O
exercises	O
9.3	O
9.4	O
9.5	O
9.6	O
10	O
boosting	B
10.1	O
weak	O
learnability	O
10.1.1	O
eﬃcient	O
implementation	O
of	O
erm	O
for	O
decision	B
stumps	I
10.2	O
adaboost	O
10.3	O
linear	O
combinations	O
of	O
base	O
hypotheses	O
10.3.1	O
the	O
vc-dimension	O
of	O
l	O
(	O
b	O
,	O
t	O
)	O
10.4	O
adaboost	O
for	O
face	B
recognition	I
10.5	O
summary	O
10.6	O
bibliographic	O
remarks	O
10.7	O
exercises	O
11	O
model	B
selection	I
and	O
validation	B
11.1	O
model	B
selection	I
using	O
srm	O
11.2	O
validation	B
11.2.1	O
hold	B
out	I
set	O
11.2.2	O
validation	B
for	O
model	B
selection	I
11.2.3	O
the	O
model-selection	O
curve	O
102	O
103	O
104	O
105	O
106	O
107	O
107	O
108	O
110	O
110	O
110	O
115	O
117	O
118	O
119	O
120	O
122	O
123	O
124	O
125	O
126	O
128	O
128	O
128	O
130	O
131	O
133	O
134	O
137	O
139	O
140	O
141	O
141	O
142	O
144	O
145	O
146	O
146	O
147	O
148	O
xii	O
contents	O
11.2.4	O
k-fold	O
cross	B
validation	I
11.2.5	O
train-validation-test	B
split	I
11.3	O
what	O
to	O
do	O
if	O
learning	O
fails	O
11.4	O
summary	O
11.5	O
exercises	O
12	O
convex	B
learning	O
problems	O
12.1	O
convexity	O
,	O
lipschitzness	O
,	O
and	O
smoothness	B
12.1.1	O
convexity	O
12.1.2	O
lipschitzness	O
12.1.3	O
smoothness	B
12.2	O
convex	B
learning	O
problems	O
12.2.1	O
learnability	O
of	O
convex	B
learning	O
problems	O
12.2.2	O
convex-lipschitz/smooth-bounded	O
learning	O
problems	O
12.3	O
surrogate	B
loss	I
functions	O
12.4	O
summary	O
12.5	O
bibliographic	O
remarks	O
12.6	O
exercises	O
13	O
regularization	B
and	O
stability	B
13.1	O
regularized	B
loss	I
minimization	I
13.1.1	O
ridge	B
regression	I
13.2	O
stable	O
rules	O
do	O
not	O
overﬁt	O
13.3	O
tikhonov	O
regularization	B
as	O
a	O
stabilizer	O
13.3.1	O
lipschitz	O
loss	B
13.3.2	O
smooth	O
and	O
nonnegative	O
loss	B
13.4	O
controlling	O
the	O
fitting-stability	O
tradeoﬀ	O
13.5	O
summary	O
13.6	O
bibliographic	O
remarks	O
13.7	O
exercises	O
14	O
stochastic	O
gradient	B
descent	I
14.1	O
gradient	B
descent	I
14.1.1	O
analysis	O
of	O
gd	O
for	O
convex-lipschitz	O
functions	O
14.2	O
subgradients	O
14.2.1	O
calculating	O
subgradients	O
14.2.2	O
subgradients	O
of	O
lipschitz	O
functions	O
14.2.3	O
subgradient	O
descent	O
14.3	O
stochastic	O
gradient	B
descent	I
(	O
sgd	O
)	O
14.3.1	O
analysis	O
of	O
sgd	O
for	O
convex-lipschitz-bounded	O
functions	O
14.4	O
variants	O
14.4.1	O
adding	O
a	O
projection	B
step	O
14.4.2	O
variable	O
step	O
size	O
14.4.3	O
other	O
averaging	O
techniques	O
149	O
150	O
151	O
154	O
154	O
156	O
156	O
156	O
160	O
162	O
163	O
164	O
166	O
167	O
168	O
169	O
169	O
171	O
171	O
172	O
173	O
174	O
176	O
177	O
178	O
180	O
180	O
181	O
184	O
185	O
186	O
188	O
189	O
190	O
190	O
191	O
191	O
193	O
193	O
194	O
195	O
contents	O
xiii	O
14.4.4	O
strongly	B
convex	I
functions*	O
14.5	O
learning	O
with	O
sgd	O
14.5.1	O
sgd	O
for	O
risk	B
minimization	O
14.5.2	O
analyzing	O
sgd	O
for	O
convex-smooth	O
learning	O
problems	O
14.5.3	O
sgd	O
for	O
regularized	B
loss	I
minimization	I
14.6	O
summary	O
14.7	O
bibliographic	O
remarks	O
14.8	O
exercises	O
15	O
support	O
vector	O
machines	O
15.1	O
margin	B
and	O
hard-svm	O
15.1.1	O
the	O
homogenous	B
case	O
15.1.2	O
the	O
sample	B
complexity	I
of	O
hard-svm	O
15.2	O
soft-svm	O
and	O
norm	O
regularization	B
15.2.1	O
the	O
sample	B
complexity	I
of	O
soft-svm	O
15.2.2	O
margin	B
and	O
norm-based	O
bounds	O
versus	O
dimension	B
15.2.3	O
the	O
ramp	O
loss*	O
implementing	O
soft-svm	O
using	O
sgd	O
15.3	O
optimality	O
conditions	O
and	O
“	O
support	B
vectors	I
”	O
*	O
15.4	O
duality*	O
15.5	O
15.6	O
summary	O
15.7	O
bibliographic	O
remarks	O
15.8	O
exercises	O
kernel	O
methods	O
16.1	O
embeddings	O
into	O
feature	B
spaces	O
16.2	O
the	O
kernel	B
trick	I
16.2.1	O
kernels	B
as	O
a	O
way	O
to	O
express	O
prior	B
knowledge	I
16.2.2	O
characterizing	O
kernel	O
functions*	O
implementing	O
soft-svm	O
with	O
kernels	B
16.3	O
16.4	O
summary	O
16.5	O
bibliographic	O
remarks	O
16.6	O
exercises	O
multiclass	B
,	O
ranking	B
,	O
and	O
complex	O
prediction	O
problems	O
17.1	O
one-versus-all	O
and	O
all-pairs	B
17.2	O
linear	O
multiclass	O
predictors	O
17.2.1	O
how	O
to	O
construct	O
ψ	O
17.2.2	O
cost-sensitive	B
classiﬁcation	O
17.2.3	O
erm	O
17.2.4	O
generalized	O
hinge	O
loss	B
17.2.5	O
multiclass	B
svm	O
and	O
sgd	O
17.3	O
structured	B
output	I
prediction	I
17.4	O
ranking	B
16	O
17	O
195	O
196	O
196	O
198	O
199	O
200	O
200	O
201	O
202	O
202	O
205	O
205	O
206	O
208	O
208	O
209	O
210	O
211	O
212	O
213	O
213	O
214	O
215	O
215	O
217	O
221	O
222	O
222	O
224	O
225	O
225	O
227	O
227	O
230	O
230	O
232	O
232	O
233	O
234	O
236	O
238	O
xiv	O
contents	O
18	O
19	O
20	O
17.4.1	O
linear	B
predictors	I
for	O
ranking	B
17.5	O
bipartite	B
ranking	O
and	O
multivariate	B
performance	I
measures	I
17.5.1	O
linear	B
predictors	I
for	O
bipartite	B
ranking	O
17.6	O
summary	O
17.7	O
bibliographic	O
remarks	O
17.8	O
exercises	O
decision	B
trees	I
18.1	O
sample	B
complexity	I
18.2	O
decision	O
tree	O
algorithms	O
18.2.1	O
implementations	O
of	O
the	O
gain	B
measure	O
18.2.2	O
pruning	B
18.2.3	O
threshold-based	O
splitting	O
rules	O
for	O
real-valued	O
features	O
18.3	O
random	B
forests	I
18.4	O
summary	O
18.5	O
bibliographic	O
remarks	O
18.6	O
exercises	O
nearest	O
neighbor	O
19.1	O
k	O
nearest	O
neighbors	O
19.2	O
analysis	O
19.2.1	O
a	O
generalization	O
bound	O
for	O
the	O
1-nn	O
rule	O
19.2.2	O
the	O
“	O
curse	B
of	I
dimensionality	I
”	O
19.3	O
eﬃcient	O
implementation*	O
19.4	O
summary	O
19.5	O
bibliographic	O
remarks	O
19.6	O
exercises	O
neural	B
networks	I
20.1	O
feedforward	O
neural	O
networks	O
20.2	O
learning	O
neural	O
networks	O
20.3	O
the	O
expressive	O
power	O
of	O
neural	B
networks	I
20.3.1	O
geometric	O
intuition	O
20.4	O
the	O
sample	B
complexity	I
of	O
neural	B
networks	I
20.5	O
the	O
runtime	O
of	O
learning	O
neural	O
networks	O
20.6	O
sgd	O
and	O
backpropagation	B
20.7	O
summary	O
20.8	O
bibliographic	O
remarks	O
20.9	O
exercises	O
part	O
iii	O
additional	O
learning	O
models	O
21	O
online	B
learning	I
21.1	O
online	B
classiﬁcation	O
in	O
the	O
realizable	O
case	O
240	O
243	O
245	O
247	O
247	O
248	O
250	O
251	O
252	O
253	O
254	O
255	O
255	O
256	O
256	O
256	O
258	O
258	O
259	O
260	O
263	O
264	O
264	O
264	O
265	O
268	O
269	O
270	O
271	O
273	O
274	O
276	O
277	O
281	O
281	O
282	O
285	O
287	O
288	O
contents	O
xv	O
21.1.1	O
online	B
learnability	O
21.2	O
online	B
classiﬁcation	O
in	O
the	O
unrealizable	O
case	O
21.2.1	O
weighted-majority	O
21.3	O
online	B
convex	I
optimization	I
21.4	O
the	O
online	B
perceptron	O
algorithm	O
21.5	O
summary	O
21.6	O
bibliographic	O
remarks	O
21.7	O
exercises	O
22	O
clustering	B
22.1	O
linkage-based	O
clustering	B
algorithms	O
22.2	O
k-means	B
and	O
other	O
cost	O
minimization	O
clusterings	O
22.2.1	O
the	O
k-means	B
algorithm	O
22.3	O
spectral	B
clustering	I
22.3.1	O
graph	O
cut	O
22.3.2	O
graph	O
laplacian	O
and	O
relaxed	O
graph	O
cuts	O
22.3.3	O
unnormalized	O
spectral	B
clustering	I
information	O
bottleneck*	O
22.4	O
22.5	O
a	O
high	O
level	O
view	O
of	O
clustering	B
22.6	O
summary	O
22.7	O
bibliographic	O
remarks	O
22.8	O
exercises	O
23	O
dimensionality	B
reduction	I
23.1	O
principal	O
component	O
analysis	O
(	O
pca	O
)	O
23.1.1	O
a	O
more	O
eﬃcient	O
solution	O
for	O
the	O
case	O
d	O
(	O
cid:29	O
)	O
m	O
23.1.2	O
implementation	O
and	O
demonstration	O
23.2	O
random	B
projections	I
23.3	O
compressed	B
sensing	I
23.3.1	O
proofs*	O
23.4	O
pca	O
or	O
compressed	B
sensing	I
?	O
23.5	O
summary	O
23.6	O
bibliographic	O
remarks	O
23.7	O
exercises	O
24	O
generative	B
models	I
24.1	O
maximum	B
likelihood	I
estimator	O
24.1.1	O
maximum	B
likelihood	I
estimation	O
for	O
continuous	O
ran-	O
dom	O
variables	O
24.1.2	O
maximum	B
likelihood	I
and	O
empirical	B
risk	I
minimization	O
24.1.3	O
generalization	O
analysis	O
24.2	O
naive	O
bayes	O
24.3	O
linear	B
discriminant	I
analysis	I
24.4	O
latent	B
variables	I
and	O
the	O
em	O
algorithm	O
290	O
294	O
295	O
300	O
301	O
304	O
305	O
305	O
307	O
310	O
311	O
313	O
315	O
315	O
315	O
317	O
317	O
318	O
320	O
320	O
320	O
323	O
324	O
326	O
326	O
329	O
330	O
333	O
338	O
338	O
339	O
339	O
342	O
343	O
344	O
345	O
345	O
347	O
347	O
348	O
xvi	O
contents	O
24.4.1	O
em	O
as	O
an	O
alternate	O
maximization	O
algorithm	O
24.4.2	O
em	O
for	O
mixture	O
of	O
gaussians	O
(	O
soft	B
k-means	I
)	O
24.5	O
bayesian	O
reasoning	O
24.6	O
summary	O
24.7	O
bibliographic	O
remarks	O
24.8	O
exercises	O
25	O
feature	B
selection	I
and	O
generation	O
25.1	O
feature	B
selection	I
25.1.1	O
filters	O
25.1.2	O
greedy	O
selection	O
approaches	O
25.1.3	O
sparsity-inducing	B
norms	I
25.2	O
feature	B
manipulation	O
and	O
normalization	O
25.2.1	O
examples	O
of	O
feature	B
transformations	I
25.3	O
feature	B
learning	I
25.3.1	O
dictionary	B
learning	I
using	O
auto-encoders	B
25.4	O
summary	O
25.5	O
bibliographic	O
remarks	O
25.6	O
exercises	O
part	O
iv	O
advanced	O
theory	O
26	O
27	O
28	O
rademacher	O
complexities	O
26.1	O
the	O
rademacher	O
complexity	O
26.1.1	O
rademacher	O
calculus	O
26.2	O
rademacher	O
complexity	O
of	O
linear	O
classes	O
26.3	O
generalization	B
bounds	I
for	O
svm	O
26.4	O
generalization	B
bounds	I
for	O
predictors	O
with	O
low	O
(	O
cid:96	O
)	O
1	O
norm	O
26.5	O
bibliographic	O
remarks	O
covering	B
numbers	I
27.1	O
covering	O
27.1.1	O
properties	O
27.2	O
from	O
covering	O
to	O
rademacher	O
complexity	O
via	O
chaining	B
27.3	O
bibliographic	O
remarks	O
proof	O
of	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
28.1	O
the	O
upper	O
bound	O
for	O
the	O
agnostic	O
case	O
28.2	O
the	O
lower	O
bound	O
for	O
the	O
agnostic	O
case	O
28.2.1	O
showing	O
that	O
m	O
(	O
	O
,	O
δ	O
)	O
≥	O
0.5	O
log	O
(	O
1/	O
(	O
4δ	O
)	O
)	O
/2	O
28.2.2	O
showing	O
that	O
m	O
(	O
	O
,	O
1/8	O
)	O
≥	O
8d/2	O
28.3	O
the	O
upper	O
bound	O
for	O
the	O
realizable	O
case	O
28.3.1	O
from	O
-nets	O
to	O
pac	O
learnability	O
350	O
352	O
353	O
355	O
355	O
356	O
357	O
358	O
359	O
360	O
363	O
365	O
367	O
368	O
368	O
370	O
371	O
371	O
373	O
375	O
375	O
379	O
382	O
383	O
386	O
386	O
388	O
388	O
388	O
389	O
391	O
392	O
392	O
393	O
393	O
395	O
398	O
401	O
29	O
30	O
31	O
multiclass	B
learnability	O
29.1	O
the	O
natarajan	O
dimension	B
29.2	O
the	O
multiclass	B
fundamental	O
theorem	O
29.2.1	O
on	O
the	O
proof	O
of	O
theorem	O
29.3	O
29.3	O
calculating	O
the	O
natarajan	O
dimension	B
29.3.1	O
one-versus-all	O
based	O
classes	O
29.3.2	O
general	O
multiclass-to-binary	O
reductions	B
29.3.3	O
linear	O
multiclass	O
predictors	O
29.4	O
on	O
good	O
and	O
bad	O
erms	O
29.5	O
bibliographic	O
remarks	O
29.6	O
exercises	O
compression	B
bounds	I
30.1	O
compression	B
bounds	I
30.2	O
examples	O
30.2.1	O
axis	O
aligned	O
rectangles	O
30.2.2	O
halfspaces	O
30.2.3	O
separating	O
polynomials	O
30.2.4	O
separation	O
with	O
margin	B
30.3	O
bibliographic	O
remarks	O
pac-bayes	O
31.1	O
pac-bayes	O
bounds	O
31.2	O
bibliographic	O
remarks	O
31.3	O
exercises	O
appendix	O
a	O
technical	O
lemmas	O
appendix	O
b	O
measure	B
concentration	I
appendix	O
c	O
linear	O
algebra	O
notes	O
references	O
index	O
contents	O
xvii	O
402	O
402	O
403	O
403	O
404	O
404	O
405	O
405	O
406	O
408	O
409	O
410	O
410	O
412	O
412	O
412	O
413	O
414	O
414	O
415	O
415	O
417	O
417	O
419	O
422	O
430	O
435	O
437	O
447	O
1	O
introduction	O
the	O
subject	O
of	O
this	O
book	O
is	O
automated	O
learning	O
,	O
or	O
,	O
as	O
we	O
will	O
more	O
often	O
call	O
it	O
,	O
machine	O
learning	O
(	O
ml	O
)	O
.	O
that	O
is	O
,	O
we	O
wish	O
to	O
program	O
computers	O
so	O
that	O
they	O
can	O
“	O
learn	O
”	O
from	O
input	O
available	O
to	O
them	O
.	O
roughly	O
speaking	O
,	O
learning	O
is	O
the	O
process	O
of	O
converting	O
experience	O
into	O
expertise	O
or	O
knowledge	O
.	O
the	O
input	O
to	O
a	O
learning	O
algorithm	O
is	O
training	O
data	O
,	O
representing	O
experience	O
,	O
and	O
the	O
output	O
is	O
some	O
expertise	O
,	O
which	O
usually	O
takes	O
the	O
form	O
of	O
another	O
computer	O
program	O
that	O
can	O
perform	O
some	O
task	O
.	O
seeking	O
a	O
formal-mathematical	O
understanding	O
of	O
this	O
concept	O
,	O
we	O
’	O
ll	O
have	O
to	O
be	O
more	O
explicit	O
about	O
what	O
we	O
mean	O
by	O
each	O
of	O
the	O
involved	O
terms	O
:	O
what	O
is	O
the	O
training	O
data	O
our	O
programs	O
will	O
access	O
?	O
how	O
can	O
the	O
process	O
of	O
learning	O
be	O
automated	O
?	O
how	O
can	O
we	O
evaluate	O
the	O
success	O
of	O
such	O
a	O
process	O
(	O
namely	O
,	O
the	O
quality	O
of	O
the	O
output	O
of	O
a	O
learning	O
program	O
)	O
?	O
1.1	O
what	O
is	O
learning	O
?	O
let	O
us	O
begin	O
by	O
considering	O
a	O
couple	O
of	O
examples	O
from	O
naturally	O
occurring	O
ani-	O
mal	O
learning	O
.	O
some	O
of	O
the	O
most	O
fundamental	O
issues	O
in	O
ml	O
arise	O
already	O
in	O
that	O
context	O
,	O
which	O
we	O
are	O
all	O
familiar	O
with	O
.	O
bait	O
shyness	O
–	O
rats	O
learning	O
to	O
avoid	O
poisonous	O
baits	O
:	O
when	O
rats	O
encounter	O
food	O
items	O
with	O
novel	O
look	O
or	O
smell	O
,	O
they	O
will	O
ﬁrst	O
eat	O
very	O
small	O
amounts	O
,	O
and	O
subsequent	O
feeding	O
will	O
depend	O
on	O
the	O
ﬂavor	O
of	O
the	O
food	O
and	O
its	O
physiological	O
eﬀect	O
.	O
if	O
the	O
food	O
produces	O
an	O
ill	O
eﬀect	O
,	O
the	O
novel	O
food	O
will	O
often	O
be	O
associated	O
with	O
the	O
illness	O
,	O
and	O
subsequently	O
,	O
the	O
rats	O
will	O
not	O
eat	O
it	O
.	O
clearly	O
,	O
there	O
is	O
a	O
learning	O
mechanism	O
in	O
play	O
here	O
–	O
the	O
animal	O
used	O
past	O
experience	O
with	O
some	O
food	O
to	O
acquire	O
expertise	O
in	O
detecting	O
the	O
safety	O
of	O
this	O
food	O
.	O
if	O
past	O
experience	O
with	O
the	O
food	O
was	O
negatively	O
labeled	O
,	O
the	O
animal	O
predicts	O
that	O
it	O
will	O
also	O
have	O
a	O
negative	O
eﬀect	O
when	O
encountered	O
in	O
the	O
future	O
.	O
inspired	O
by	O
the	O
preceding	O
example	O
of	O
successful	O
learning	O
,	O
let	O
us	O
demonstrate	O
a	O
typical	O
machine	O
learning	O
task	O
.	O
suppose	O
we	O
would	O
like	O
to	O
program	O
a	O
machine	O
that	O
learns	O
how	O
to	O
ﬁlter	O
spam	O
e-mails	O
.	O
a	O
naive	O
solution	O
would	O
be	O
seemingly	O
similar	O
to	O
the	O
way	O
rats	O
learn	O
how	O
to	O
avoid	O
poisonous	O
baits	O
.	O
the	O
machine	O
will	O
simply	O
memorize	O
all	O
previous	O
e-mails	O
that	O
had	O
been	O
labeled	O
as	O
spam	O
e-mails	O
by	O
the	O
human	O
user	O
.	O
when	O
a	O
new	O
e-mail	O
arrives	O
,	O
the	O
machine	O
will	O
search	O
for	O
it	O
in	O
the	O
set	B
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
20	O
introduction	O
of	O
previous	O
spam	O
e-mails	O
.	O
if	O
it	O
matches	O
one	O
of	O
them	O
,	O
it	O
will	O
be	O
trashed	O
.	O
otherwise	O
,	O
it	O
will	O
be	O
moved	O
to	O
the	O
user	O
’	O
s	O
inbox	O
folder	O
.	O
while	O
the	O
preceding	O
“	O
learning	O
by	O
memorization	O
”	O
approach	O
is	O
sometimes	O
use-	O
ful	O
,	O
it	O
lacks	O
an	O
important	O
aspect	O
of	O
learning	O
systems	O
–	O
the	O
ability	O
to	O
label	B
unseen	O
e-mail	O
messages	O
.	O
a	O
successful	O
learner	O
should	O
be	O
able	O
to	O
progress	O
from	O
individual	O
examples	O
to	O
broader	O
generalization	O
.	O
this	O
is	O
also	O
referred	O
to	O
as	O
inductive	O
reasoning	O
or	O
inductive	O
inference	O
.	O
in	O
the	O
bait	O
shyness	O
example	O
presented	O
previously	O
,	O
after	O
the	O
rats	O
encounter	O
an	O
example	O
of	O
a	O
certain	O
type	O
of	O
food	O
,	O
they	O
apply	O
their	O
attitude	O
toward	O
it	O
on	O
new	O
,	O
unseen	O
examples	O
of	O
food	O
of	O
similar	O
smell	O
and	O
taste	O
.	O
to	O
achieve	O
generalization	O
in	O
the	O
spam	O
ﬁltering	O
task	O
,	O
the	O
learner	O
can	O
scan	O
the	O
previously	O
seen	O
e-mails	O
,	O
and	O
extract	O
a	O
set	B
of	O
words	O
whose	O
appearance	O
in	O
an	O
e-mail	O
message	O
is	O
indicative	O
of	O
spam	O
.	O
then	O
,	O
when	O
a	O
new	O
e-mail	O
arrives	O
,	O
the	O
machine	O
can	O
check	O
whether	O
one	O
of	O
the	O
suspicious	O
words	O
appears	O
in	O
it	O
,	O
and	O
predict	O
its	O
label	B
accord-	O
ingly	O
.	O
such	O
a	O
system	O
would	O
potentially	O
be	O
able	O
correctly	O
to	O
predict	O
the	O
label	B
of	O
unseen	O
e-mails	O
.	O
however	O
,	O
inductive	O
reasoning	O
might	O
lead	O
us	O
to	O
false	O
conclusions	O
.	O
to	O
illustrate	O
this	O
,	O
let	O
us	O
consider	O
again	O
an	O
example	O
from	O
animal	O
learning	O
.	O
pigeon	O
superstition	O
:	O
in	O
an	O
experiment	O
performed	O
by	O
the	O
psychologist	O
b.	O
f.	O
skinner	O
,	O
he	O
placed	O
a	O
bunch	O
of	O
hungry	O
pigeons	O
in	O
a	O
cage	O
.	O
an	O
automatic	O
mechanism	O
had	O
been	O
attached	O
to	O
the	O
cage	O
,	O
delivering	O
food	O
to	O
the	O
pigeons	O
at	O
regular	O
intervals	O
with	O
no	O
reference	O
whatsoever	O
to	O
the	O
birds	O
’	O
behavior	O
.	O
the	O
hungry	O
pigeons	O
went	O
around	O
the	O
cage	O
,	O
and	O
when	O
food	O
was	O
ﬁrst	O
delivered	O
,	O
it	O
found	O
each	O
pigeon	O
engaged	O
in	O
some	O
activity	O
(	O
pecking	O
,	O
turning	O
the	O
head	O
,	O
etc.	O
)	O
.	O
the	O
arrival	O
of	O
food	O
reinforced	O
each	O
bird	O
’	O
s	O
speciﬁc	O
action	O
,	O
and	O
consequently	O
,	O
each	O
bird	O
tended	O
to	O
spend	O
some	O
more	O
time	O
doing	O
that	O
very	O
same	O
action	O
.	O
that	O
,	O
in	O
turn	O
,	O
increased	O
the	O
chance	O
that	O
the	O
next	O
random	O
food	O
delivery	O
would	O
ﬁnd	O
each	O
bird	O
engaged	O
in	O
that	O
activity	O
again	O
.	O
what	O
results	O
is	O
a	O
chain	O
of	O
events	O
that	O
reinforces	O
the	O
pigeons	O
’	O
association	O
of	O
the	O
delivery	O
of	O
the	O
food	O
with	O
whatever	O
chance	O
actions	O
they	O
had	O
been	O
perform-	O
ing	O
when	O
it	O
was	O
ﬁrst	O
delivered	O
.	O
they	O
subsequently	O
continue	O
to	O
perform	O
these	O
same	O
actions	O
diligently.1	O
what	O
distinguishes	O
learning	O
mechanisms	O
that	O
result	O
in	O
superstition	O
from	O
useful	O
learning	O
?	O
this	O
question	O
is	O
crucial	O
to	O
the	O
development	O
of	O
automated	O
learners	O
.	O
while	O
human	O
learners	O
can	O
rely	O
on	O
common	O
sense	O
to	O
ﬁlter	O
out	O
random	O
meaningless	O
learning	O
conclusions	O
,	O
once	O
we	O
export	O
the	O
task	O
of	O
learning	O
to	O
a	O
machine	O
,	O
we	O
must	O
provide	O
well	O
deﬁned	O
crisp	O
principles	O
that	O
will	O
protect	O
the	O
program	O
from	O
reaching	O
senseless	O
or	O
useless	O
conclusions	O
.	O
the	O
development	O
of	O
such	O
principles	O
is	O
a	O
central	O
goal	O
of	O
the	O
theory	O
of	O
machine	O
learning	O
.	O
what	O
,	O
then	O
,	O
made	O
the	O
rats	O
’	O
learning	O
more	O
successful	O
than	O
that	O
of	O
the	O
pigeons	O
?	O
as	O
a	O
ﬁrst	O
step	O
toward	O
answering	O
this	O
question	O
,	O
let	O
us	O
have	O
a	O
closer	O
look	O
at	O
the	O
bait	O
shyness	O
phenomenon	O
in	O
rats	O
.	O
bait	O
shyness	O
revisited	O
–	O
rats	O
fail	O
to	O
acquire	O
conditioning	O
between	O
food	O
and	O
electric	O
shock	O
or	O
between	O
sound	O
and	O
nausea	O
:	O
the	O
bait	O
shyness	O
mechanism	O
in	O
1	O
see	O
:	O
http	O
:	O
//psychclassics.yorku.ca/skinner/pigeon	O
1.2	O
when	O
do	O
we	O
need	O
machine	O
learning	O
?	O
21	O
rats	O
turns	O
out	O
to	O
be	O
more	O
complex	O
than	O
what	O
one	O
may	O
expect	O
.	O
in	O
experiments	O
carried	O
out	O
by	O
garcia	O
(	O
garcia	O
&	O
koelling	O
1996	O
)	O
,	O
it	O
was	O
demonstrated	O
that	O
if	O
the	O
unpleasant	O
stimulus	O
that	O
follows	O
food	O
consumption	O
is	O
replaced	O
by	O
,	O
say	O
,	O
electrical	O
shock	O
(	O
rather	O
than	O
nausea	O
)	O
,	O
then	O
no	O
conditioning	O
occurs	O
.	O
even	O
after	O
repeated	O
trials	O
in	O
which	O
the	O
consumption	O
of	O
some	O
food	O
is	O
followed	O
by	O
the	O
administration	O
of	O
unpleasant	O
electrical	O
shock	O
,	O
the	O
rats	O
do	O
not	O
tend	O
to	O
avoid	O
that	O
food	O
.	O
similar	O
failure	O
of	O
conditioning	O
occurs	O
when	O
the	O
characteristic	O
of	O
the	O
food	O
that	O
implies	O
nausea	O
(	O
such	O
as	O
taste	O
or	O
smell	O
)	O
is	O
replaced	O
by	O
a	O
vocal	O
signal	O
.	O
the	O
rats	O
seem	O
to	O
have	O
some	O
“	O
built	O
in	O
”	O
prior	B
knowledge	I
telling	O
them	O
that	O
,	O
while	O
temporal	O
correlation	O
between	O
food	O
and	O
nausea	O
can	O
be	O
causal	O
,	O
it	O
is	O
unlikely	O
that	O
there	O
would	O
be	O
a	O
causal	O
relationship	O
between	O
food	O
consumption	O
and	O
electrical	O
shocks	O
or	O
between	O
sounds	O
and	O
nausea	O
.	O
we	O
conclude	O
that	O
one	O
distinguishing	O
feature	B
between	O
the	O
bait	O
shyness	O
learning	O
and	O
the	O
pigeon	O
superstition	O
is	O
the	O
incorporation	O
of	O
prior	B
knowledge	I
that	O
biases	O
the	O
learning	O
mechanism	O
.	O
this	O
is	O
also	O
referred	O
to	O
as	O
inductive	B
bias	I
.	O
the	O
pigeons	O
in	O
the	O
experiment	O
are	O
willing	O
to	O
adopt	O
any	O
explanation	O
for	O
the	O
occurrence	O
of	O
food	O
.	O
however	O
,	O
the	O
rats	O
“	O
know	O
”	O
that	O
food	O
can	O
not	O
cause	O
an	O
electric	O
shock	O
and	O
that	O
the	O
co-occurrence	O
of	O
noise	O
with	O
some	O
food	O
is	O
not	O
likely	O
to	O
aﬀect	O
the	O
nutritional	O
value	O
of	O
that	O
food	O
.	O
the	O
rats	O
’	O
learning	O
process	O
is	O
biased	O
toward	O
detecting	O
some	O
kind	O
of	O
patterns	O
while	O
ignoring	O
other	O
temporal	O
correlations	O
between	O
events	O
.	O
it	O
turns	O
out	O
that	O
the	O
incorporation	O
of	O
prior	B
knowledge	I
,	O
biasing	O
the	O
learning	O
process	O
,	O
is	O
inevitable	O
for	O
the	O
success	O
of	O
learning	O
algorithms	O
(	O
this	O
is	O
formally	O
stated	O
and	O
proved	O
as	O
the	O
“	O
no-free-lunch	B
theorem	O
”	O
in	O
chapter	O
5	O
)	O
.	O
the	O
development	O
of	O
tools	O
for	O
expressing	O
domain	B
expertise	O
,	O
translating	O
it	O
into	O
a	O
learning	O
bias	O
,	O
and	O
quantifying	O
the	O
eﬀect	O
of	O
such	O
a	O
bias	B
on	O
the	O
success	O
of	O
learning	O
is	O
a	O
central	O
theme	O
of	O
the	O
theory	O
of	O
machine	O
learning	O
.	O
roughly	O
speaking	O
,	O
the	O
stronger	O
the	O
prior	B
knowledge	I
(	O
or	O
prior	O
assumptions	O
)	O
that	O
one	O
starts	O
the	O
learning	O
process	O
with	O
,	O
the	O
easier	O
it	O
is	O
to	O
learn	O
from	O
further	O
examples	O
.	O
however	O
,	O
the	O
stronger	O
these	O
prior	O
assumptions	O
are	O
,	O
the	O
less	O
ﬂexible	O
the	O
learning	O
is	O
–	O
it	O
is	O
bound	O
,	O
a	O
priori	O
,	O
by	O
the	O
commitment	O
to	O
these	O
assumptions	O
.	O
we	O
shall	O
discuss	O
these	O
issues	O
explicitly	O
in	O
chapter	O
5	O
.	O
1.2	O
when	O
do	O
we	O
need	O
machine	O
learning	O
?	O
when	O
do	O
we	O
need	O
machine	O
learning	O
rather	O
than	O
directly	O
program	O
our	O
computers	O
to	O
carry	O
out	O
the	O
task	O
at	O
hand	O
?	O
two	O
aspects	O
of	O
a	O
given	O
problem	O
may	O
call	O
for	O
the	O
use	O
of	O
programs	O
that	O
learn	O
and	O
improve	O
on	O
the	O
basis	O
of	O
their	O
“	O
experience	O
”	O
:	O
the	O
problem	O
’	O
s	O
complexity	O
and	O
the	O
need	O
for	O
adaptivity	O
.	O
tasks	O
that	O
are	O
too	O
complex	O
to	O
program	O
.	O
•	O
tasks	O
performed	O
by	O
animals/humans	O
:	O
there	O
are	O
numerous	O
tasks	O
that	O
we	O
human	O
beings	O
perform	O
routinely	O
,	O
yet	O
our	O
introspection	O
concern-	O
ing	O
how	O
we	O
do	O
them	O
is	O
not	O
suﬃciently	O
elaborate	O
to	O
extract	O
a	O
well	O
22	O
introduction	O
deﬁned	O
program	O
.	O
examples	O
of	O
such	O
tasks	O
include	O
driving	O
,	O
speech	O
recognition	O
,	O
and	O
image	O
understanding	O
.	O
in	O
all	O
of	O
these	O
tasks	O
,	O
state	O
of	O
the	O
art	O
machine	O
learning	O
programs	O
,	O
programs	O
that	O
“	O
learn	O
from	O
their	O
experience	O
,	O
”	O
achieve	O
quite	O
satisfactory	O
results	O
,	O
once	O
exposed	O
to	O
suﬃciently	O
many	O
training	O
examples	O
.	O
•	O
tasks	O
beyond	O
human	O
capabilities	O
:	O
another	O
wide	O
family	O
of	O
tasks	O
that	O
beneﬁt	O
from	O
machine	O
learning	O
techniques	O
are	O
related	O
to	O
the	O
analy-	O
sis	O
of	O
very	O
large	O
and	O
complex	O
data	O
sets	O
:	O
astronomical	O
data	O
,	O
turning	O
medical	O
archives	O
into	O
medical	O
knowledge	O
,	O
weather	O
prediction	O
,	O
anal-	O
ysis	O
of	O
genomic	O
data	O
,	O
web	O
search	O
engines	O
,	O
and	O
electronic	O
commerce	O
.	O
with	O
more	O
and	O
more	O
available	O
digitally	O
recorded	O
data	O
,	O
it	O
becomes	O
obvious	O
that	O
there	O
are	O
treasures	O
of	O
meaningful	O
information	O
buried	O
in	O
data	O
archives	O
that	O
are	O
way	O
too	O
large	O
and	O
too	O
complex	O
for	O
humans	O
to	O
make	O
sense	O
of	O
.	O
learning	O
to	O
detect	O
meaningful	O
patterns	O
in	O
large	O
and	O
complex	O
data	O
sets	O
is	O
a	O
promising	O
domain	B
in	O
which	O
the	O
combi-	O
nation	O
of	O
programs	O
that	O
learn	O
with	O
the	O
almost	O
unlimited	O
memory	O
capacity	O
and	O
ever	O
increasing	O
processing	O
speed	O
of	O
computers	O
opens	O
up	O
new	O
horizons	O
.	O
adaptivity	O
.	O
one	O
limiting	O
feature	B
of	O
programmed	O
tools	O
is	O
their	O
rigidity	O
–	O
once	O
the	O
program	O
has	O
been	O
written	O
down	O
and	O
installed	O
,	O
it	O
stays	O
unchanged	O
.	O
however	O
,	O
many	O
tasks	O
change	O
over	O
time	O
or	O
from	O
one	O
user	O
to	O
another	O
.	O
machine	O
learning	O
tools	O
–	O
programs	O
whose	O
behavior	O
adapts	O
to	O
their	O
input	O
data	O
–	O
oﬀer	O
a	O
solution	O
to	O
such	O
issues	O
;	O
they	O
are	O
,	O
by	O
nature	O
,	O
adaptive	O
to	O
changes	O
in	O
the	O
environment	O
they	O
interact	O
with	O
.	O
typical	O
successful	O
applications	O
of	O
machine	O
learning	O
to	O
such	O
problems	O
include	O
programs	O
that	O
decode	O
handwritten	O
text	O
,	O
where	O
a	O
ﬁxed	O
program	O
can	O
adapt	O
to	O
variations	O
between	O
the	O
handwriting	O
of	O
diﬀerent	O
users	O
;	O
spam	O
detection	O
programs	O
,	O
adapting	O
automatically	O
to	O
changes	O
in	O
the	O
nature	O
of	O
spam	O
e-mails	O
;	O
and	O
speech	O
recognition	O
programs	O
.	O
1.3	O
types	O
of	O
learning	O
learning	O
is	O
,	O
of	O
course	O
,	O
a	O
very	O
wide	O
domain	B
.	O
consequently	O
,	O
the	O
ﬁeld	O
of	O
machine	O
learning	O
has	O
branched	O
into	O
several	O
subﬁelds	O
dealing	O
with	O
diﬀerent	O
types	O
of	O
learn-	O
ing	O
tasks	O
.	O
we	O
give	O
a	O
rough	O
taxonomy	O
of	O
learning	O
paradigms	O
,	O
aiming	O
to	O
provide	O
some	O
perspective	O
of	O
where	O
the	O
content	O
of	O
this	O
book	O
sits	O
within	O
the	O
wide	O
ﬁeld	O
of	O
machine	O
learning	O
.	O
we	O
describe	O
four	O
parameters	O
along	O
which	O
learning	O
paradigms	O
can	O
be	O
classiﬁed	O
.	O
supervised	O
versus	O
unsupervised	O
since	O
learning	O
involves	O
an	O
interaction	O
be-	O
tween	O
the	O
learner	O
and	O
the	O
environment	O
,	O
one	O
can	O
divide	O
learning	O
tasks	O
according	O
to	O
the	O
nature	O
of	O
that	O
interaction	O
.	O
the	O
ﬁrst	O
distinction	O
to	O
note	O
is	O
the	O
diﬀerence	O
between	O
supervised	O
and	O
unsupervised	B
learning	I
.	O
as	O
an	O
1.3	O
types	O
of	O
learning	O
23	O
illustrative	O
example	O
,	O
consider	O
the	O
task	O
of	O
learning	O
to	O
detect	O
spam	O
e-mail	O
versus	O
the	O
task	O
of	O
anomaly	O
detection	O
.	O
for	O
the	O
spam	O
detection	O
task	O
,	O
we	O
consider	O
a	O
setting	O
in	O
which	O
the	O
learner	O
receives	O
training	O
e-mails	O
for	O
which	O
the	O
label	B
spam/not-spam	O
is	O
provided	O
.	O
on	O
the	O
basis	O
of	O
such	O
training	O
the	O
learner	O
should	O
ﬁgure	O
out	O
a	O
rule	O
for	O
labeling	O
a	O
newly	O
arriving	O
e-mail	O
mes-	O
sage	O
.	O
in	O
contrast	O
,	O
for	O
the	O
task	O
of	O
anomaly	O
detection	O
,	O
all	O
the	O
learner	O
gets	O
as	O
training	O
is	O
a	O
large	O
body	O
of	O
e-mail	O
messages	O
(	O
with	O
no	O
labels	O
)	O
and	O
the	O
learner	O
’	O
s	O
task	O
is	O
to	O
detect	O
“	O
unusual	O
”	O
messages	O
.	O
more	O
abstractly	O
,	O
viewing	O
learning	O
as	O
a	O
process	O
of	O
“	O
using	O
experience	O
to	O
gain	B
expertise	O
,	O
”	O
supervised	O
learning	O
describes	O
a	O
scenario	O
in	O
which	O
the	O
“	O
experience	O
,	O
”	O
a	O
training	O
example	O
,	O
contains	O
signiﬁcant	O
information	O
(	O
say	O
,	O
the	O
spam/not-spam	O
labels	O
)	O
that	O
is	O
missing	O
in	O
the	O
unseen	O
“	O
test	O
examples	O
”	O
to	O
which	O
the	O
learned	O
expertise	O
is	O
to	O
be	O
applied	O
.	O
in	O
this	O
setting	O
,	O
the	O
ac-	O
quired	O
expertise	O
is	O
aimed	O
to	O
predict	O
that	O
missing	O
information	O
for	O
the	O
test	O
data	O
.	O
in	O
such	O
cases	O
,	O
we	O
can	O
think	O
of	O
the	O
environment	O
as	O
a	O
teacher	O
that	O
“	O
supervises	O
”	O
the	O
learner	O
by	O
providing	O
the	O
extra	O
information	O
(	O
labels	O
)	O
.	O
in	O
unsupervised	B
learning	I
,	O
however	O
,	O
there	O
is	O
no	O
distinction	O
between	O
training	O
and	O
test	O
data	O
.	O
the	O
learner	O
processes	O
input	O
data	O
with	O
the	O
goal	O
of	O
coming	O
up	O
with	O
some	O
summary	O
,	O
or	O
compressed	O
version	O
of	O
that	O
data	O
.	O
clustering	B
a	O
data	O
set	B
into	O
subsets	O
of	O
similar	O
objets	O
is	O
a	O
typical	O
example	O
of	O
such	O
a	O
task	O
.	O
there	O
is	O
also	O
an	O
intermediate	O
learning	O
setting	O
in	O
which	O
,	O
while	O
the	O
training	O
examples	O
contain	O
more	O
information	O
than	O
the	O
test	O
examples	O
,	O
the	O
learner	O
is	O
required	O
to	O
predict	O
even	O
more	O
information	O
for	O
the	O
test	O
exam-	O
ples	O
.	O
for	O
example	O
,	O
one	O
may	O
try	O
to	O
learn	O
a	O
value	O
function	B
that	O
describes	O
for	O
each	O
setting	O
of	O
a	O
chess	O
board	O
the	O
degree	O
by	O
which	O
white	O
’	O
s	O
position	O
is	O
bet-	O
ter	O
than	O
the	O
black	O
’	O
s	O
.	O
yet	O
,	O
the	O
only	O
information	O
available	O
to	O
the	O
learner	O
at	O
training	O
time	O
is	O
positions	O
that	O
occurred	O
throughout	O
actual	O
chess	O
games	O
,	O
labeled	O
by	O
who	O
eventually	O
won	O
that	O
game	O
.	O
such	O
learning	O
frameworks	O
are	O
mainly	O
investigated	O
under	O
the	O
title	O
of	O
reinforcement	O
learning	O
.	O
active	O
versus	O
passive	O
learners	O
learning	O
paradigms	O
can	O
vary	O
by	O
the	O
role	O
played	O
by	O
the	O
learner	O
.	O
we	O
distinguish	O
between	O
“	O
active	O
”	O
and	O
“	O
passive	O
”	O
learners	O
.	O
an	O
active	O
learner	O
interacts	O
with	O
the	O
environment	O
at	O
training	O
time	O
,	O
say	O
,	O
by	O
posing	O
queries	O
or	O
performing	O
experiments	O
,	O
while	O
a	O
passive	O
learner	O
only	O
observes	O
the	O
information	O
provided	O
by	O
the	O
environment	O
(	O
or	O
the	O
teacher	O
)	O
without	O
inﬂuencing	O
or	O
directing	O
it	O
.	O
note	O
that	O
the	O
learner	O
of	O
a	O
spam	O
ﬁlter	O
is	O
usually	O
passive	O
–	O
waiting	O
for	O
users	O
to	O
mark	O
the	O
e-mails	O
com-	O
ing	O
to	O
them	O
.	O
in	O
an	O
active	O
setting	O
,	O
one	O
could	O
imagine	O
asking	O
users	O
to	O
label	B
speciﬁc	O
e-mails	O
chosen	O
by	O
the	O
learner	O
,	O
or	O
even	O
composed	O
by	O
the	O
learner	O
,	O
to	O
enhance	O
what	O
spam	O
is	O
.	O
understanding	O
its	O
of	O
helpfulness	O
of	O
the	O
teacher	O
when	O
one	O
thinks	O
about	O
human	O
learning	O
,	O
of	O
a	O
baby	O
at	O
home	O
or	O
a	O
student	O
at	O
school	O
,	O
the	O
process	O
often	O
involves	O
a	O
helpful	O
teacher	O
,	O
who	O
is	O
trying	O
to	O
feed	O
the	O
learner	O
with	O
the	O
information	O
most	O
use-	O
24	O
introduction	O
ful	O
for	O
achieving	O
the	O
learning	O
goal	O
.	O
in	O
contrast	O
,	O
when	O
a	O
scientist	O
learns	O
about	O
nature	O
,	O
the	O
environment	O
,	O
playing	O
the	O
role	O
of	O
the	O
teacher	O
,	O
can	O
be	O
best	O
thought	O
of	O
as	O
passive	O
–	O
apples	O
drop	O
,	O
stars	O
shine	O
,	O
and	O
the	O
rain	O
falls	O
without	O
regard	O
to	O
the	O
needs	O
of	O
the	O
learner	O
.	O
we	O
model	O
such	O
learning	O
sce-	O
narios	O
by	O
postulating	O
that	O
the	O
training	O
data	O
(	O
or	O
the	O
learner	O
’	O
s	O
experience	O
)	O
is	O
generated	O
by	O
some	O
random	O
process	O
.	O
this	O
is	O
the	O
basic	O
building	O
block	O
in	O
the	O
branch	O
of	O
“	O
statistical	O
learning.	O
”	O
finally	O
,	O
learning	O
also	O
occurs	O
when	O
the	O
learner	O
’	O
s	O
input	O
is	O
generated	O
by	O
an	O
adversarial	O
“	O
teacher.	O
”	O
this	O
may	O
be	O
the	O
case	O
in	O
the	O
spam	O
ﬁltering	O
example	O
(	O
if	O
the	O
spammer	O
makes	O
an	O
eﬀort	O
to	O
mislead	O
the	O
spam	O
ﬁltering	O
designer	O
)	O
or	O
in	O
learning	O
to	O
detect	O
fraud	O
.	O
one	O
also	O
uses	O
an	O
adversarial	O
teacher	O
model	O
as	O
a	O
worst-case	O
scenario	O
,	O
when	O
no	O
milder	O
setup	O
can	O
be	O
safely	O
assumed	O
.	O
if	O
you	O
can	O
learn	O
against	O
an	O
adversarial	O
teacher	O
,	O
you	O
are	O
guaranteed	O
to	O
succeed	O
interacting	O
any	O
odd	O
teacher	O
.	O
online	B
versus	O
batch	O
learning	O
protocol	O
the	O
last	O
parameter	O
we	O
mention	O
is	O
the	O
distinction	O
between	O
situations	O
in	O
which	O
the	O
learner	O
has	O
to	O
respond	O
online	B
,	O
throughout	O
the	O
learning	O
process	O
,	O
and	O
settings	O
in	O
which	O
the	O
learner	O
has	O
to	O
engage	O
the	O
acquired	O
expertise	O
only	O
after	O
having	O
a	O
chance	O
to	O
process	O
large	O
amounts	O
of	O
data	O
.	O
for	O
example	O
,	O
a	O
stockbroker	O
has	O
to	O
make	O
daily	O
decisions	O
,	O
based	O
on	O
the	O
experience	O
collected	O
so	O
far	O
.	O
he	O
may	O
become	O
an	O
expert	O
over	O
time	O
,	O
but	O
might	O
have	O
made	O
costly	O
mistakes	O
in	O
the	O
process	O
.	O
in	O
contrast	O
,	O
in	O
many	O
data	O
mining	O
settings	O
,	O
the	O
learner	O
–	O
the	O
data	O
miner	O
–	O
has	O
large	O
amounts	O
of	O
training	O
data	O
to	O
play	O
with	O
before	O
having	O
to	O
output	O
conclusions	O
.	O
in	O
this	O
book	O
we	O
shall	O
discuss	O
only	O
a	O
subset	O
of	O
the	O
possible	O
learning	O
paradigms	O
.	O
our	O
main	O
focus	O
is	O
on	O
supervised	O
statistical	O
batch	O
learning	O
with	O
a	O
passive	O
learner	O
(	O
for	O
example	O
,	O
trying	O
to	O
learn	O
how	O
to	O
generate	O
patients	O
’	O
prognoses	O
,	O
based	O
on	O
large	O
archives	O
of	O
records	O
of	O
patients	O
that	O
were	O
independently	O
collected	O
and	O
are	O
already	O
labeled	O
by	O
the	O
fate	O
of	O
the	O
recorded	O
patients	O
)	O
.	O
we	O
shall	O
also	O
brieﬂy	O
discuss	O
online	B
learning	I
and	O
batch	O
unsupervised	B
learning	I
(	O
in	O
particular	O
,	O
clustering	B
)	O
.	O
1.4	O
relations	O
to	O
other	O
fields	O
as	O
an	O
interdisciplinary	O
ﬁeld	O
,	O
machine	O
learning	O
shares	O
common	O
threads	O
with	O
the	O
mathematical	O
ﬁelds	O
of	O
statistics	O
,	O
information	O
theory	O
,	O
game	O
theory	O
,	O
and	O
optimiza-	O
tion	O
.	O
it	O
is	O
naturally	O
a	O
subﬁeld	O
of	O
computer	O
science	O
,	O
as	O
our	O
goal	O
is	O
to	O
program	O
machines	O
so	O
that	O
they	O
will	O
learn	O
.	O
in	O
a	O
sense	O
,	O
machine	O
learning	O
can	O
be	O
viewed	O
as	O
a	O
branch	O
of	O
ai	O
(	O
artiﬁcial	O
intelligence	O
)	O
,	O
since	O
,	O
after	O
all	O
,	O
the	O
ability	O
to	O
turn	O
expe-	O
rience	O
into	O
expertise	O
or	O
to	O
detect	O
meaningful	O
patterns	O
in	O
complex	O
sensory	O
data	O
is	O
a	O
cornerstone	O
of	O
human	O
(	O
and	O
animal	O
)	O
intelligence	O
.	O
however	O
,	O
one	O
should	O
note	O
that	O
,	O
in	O
contrast	O
with	O
traditional	O
ai	O
,	O
machine	O
learning	O
is	O
not	O
trying	O
to	O
build	O
automated	O
imitation	O
of	O
intelligent	O
behavior	O
,	O
but	O
rather	O
to	O
use	O
the	O
strengths	O
and	O
1.5	O
how	O
to	O
read	O
this	O
book	O
25	O
special	O
abilities	O
of	O
computers	O
to	O
complement	O
human	O
intelligence	O
,	O
often	O
perform-	O
ing	O
tasks	O
that	O
fall	O
way	O
beyond	O
human	O
capabilities	O
.	O
for	O
example	O
,	O
the	O
ability	O
to	O
scan	O
and	O
process	O
huge	O
databases	O
allows	O
machine	O
learning	O
programs	O
to	O
detect	O
patterns	O
that	O
are	O
outside	O
the	O
scope	O
of	O
human	O
perception	O
.	O
the	O
component	O
of	O
experience	O
,	O
or	O
training	O
,	O
in	O
machine	O
learning	O
often	O
refers	O
to	O
data	O
that	O
is	O
randomly	O
generated	O
.	O
the	O
task	O
of	O
the	O
learner	O
is	O
to	O
process	O
such	O
randomly	O
generated	O
examples	O
toward	O
drawing	O
conclusions	O
that	O
hold	O
for	O
the	O
en-	O
vironment	O
from	O
which	O
these	O
examples	O
are	O
picked	O
.	O
this	O
description	O
of	O
machine	O
learning	O
highlights	O
its	O
close	O
relationship	O
with	O
statistics	O
.	O
indeed	O
there	O
is	O
a	O
lot	O
in	O
common	O
between	O
the	O
two	O
disciplines	O
,	O
in	O
terms	O
of	O
both	O
the	O
goals	O
and	O
techniques	O
used	O
.	O
there	O
are	O
,	O
however	O
,	O
a	O
few	O
signiﬁcant	O
diﬀerences	O
of	O
emphasis	O
;	O
if	O
a	O
doctor	O
comes	O
up	O
with	O
the	O
hypothesis	B
that	O
there	O
is	O
a	O
correlation	O
between	O
smoking	O
and	O
heart	O
disease	O
,	O
it	O
is	O
the	O
statistician	O
’	O
s	O
role	O
to	O
view	O
samples	O
of	O
patients	O
and	O
check	O
the	O
validity	O
of	O
that	O
hypothesis	B
(	O
this	O
is	O
the	O
common	O
statistical	O
task	O
of	O
hypothe-	O
sis	O
testing	O
)	O
.	O
in	O
contrast	O
,	O
machine	O
learning	O
aims	O
to	O
use	O
the	O
data	O
gathered	O
from	O
samples	O
of	O
patients	O
to	O
come	O
up	O
with	O
a	O
description	O
of	O
the	O
causes	O
of	O
heart	O
disease	O
.	O
the	O
hope	O
is	O
that	O
automated	O
techniques	O
may	O
be	O
able	O
to	O
ﬁgure	O
out	O
meaningful	O
patterns	O
(	O
or	O
hypotheses	O
)	O
that	O
may	O
have	O
been	O
missed	O
by	O
the	O
human	O
observer	O
.	O
in	O
contrast	O
with	O
traditional	O
statistics	O
,	O
in	O
machine	O
learning	O
in	O
general	O
,	O
and	O
in	O
this	O
book	O
in	O
particular	O
,	O
algorithmic	O
considerations	O
play	O
a	O
major	O
role	O
.	O
ma-	O
chine	O
learning	O
is	O
about	O
the	O
execution	O
of	O
learning	O
by	O
computers	O
;	O
hence	O
algorith-	O
mic	O
issues	O
are	O
pivotal	O
.	O
we	O
develop	O
algorithms	O
to	O
perform	O
the	O
learning	O
tasks	O
and	O
are	O
concerned	O
with	O
their	O
computational	O
eﬃciency	O
.	O
another	O
diﬀerence	O
is	O
that	O
while	O
statistics	O
is	O
often	O
interested	O
in	O
asymptotic	O
behavior	O
(	O
like	O
the	O
convergence	O
of	O
sample-based	O
statistical	O
estimates	O
as	O
the	O
sample	O
sizes	O
grow	O
to	O
inﬁnity	O
)	O
,	O
the	O
theory	O
of	O
machine	O
learning	O
focuses	O
on	O
ﬁnite	O
sample	O
bounds	O
.	O
namely	O
,	O
given	O
the	O
size	O
of	O
available	O
samples	O
,	O
machine	O
learning	O
theory	O
aims	O
to	O
ﬁgure	O
out	O
the	O
degree	O
of	O
accuracy	B
that	O
a	O
learner	O
can	O
expect	O
on	O
the	O
basis	O
of	O
such	O
samples	O
.	O
there	O
are	O
further	O
diﬀerences	O
between	O
these	O
two	O
disciplines	O
,	O
of	O
which	O
we	O
shall	O
mention	O
only	O
one	O
more	O
here	O
.	O
while	O
in	O
statistics	O
it	O
is	O
common	O
to	O
work	O
under	O
the	O
assumption	O
of	O
certain	O
presubscribed	O
data	O
models	O
(	O
such	O
as	O
assuming	O
the	O
normal-	O
ity	O
of	O
data-generating	O
distributions	O
,	O
or	O
the	O
linearity	O
of	O
functional	O
dependencies	O
)	O
,	O
in	O
machine	O
learning	O
the	O
emphasis	O
is	O
on	O
working	O
under	O
a	O
“	O
distribution-free	O
”	O
set-	O
ting	O
,	O
where	O
the	O
learner	O
assumes	O
as	O
little	O
as	O
possible	O
about	O
the	O
nature	O
of	O
the	O
data	O
distribution	O
and	O
allows	O
the	O
learning	O
algorithm	O
to	O
ﬁgure	O
out	O
which	O
models	O
best	O
approximate	O
the	O
data-generating	O
process	O
.	O
a	O
precise	O
discussion	O
of	O
this	O
issue	O
requires	O
some	O
technical	O
preliminaries	O
,	O
and	O
we	O
will	O
come	O
back	O
to	O
it	O
later	O
in	O
the	O
book	O
,	O
and	O
in	O
particular	O
in	O
chapter	O
5	O
.	O
1.5	O
how	O
to	O
read	O
this	O
book	O
the	O
ﬁrst	O
part	O
of	O
the	O
book	O
provides	O
the	O
basic	O
theoretical	O
principles	O
that	O
underlie	O
machine	O
learning	O
(	O
ml	O
)	O
.	O
in	O
a	O
sense	O
,	O
this	O
is	O
the	O
foundation	O
upon	O
which	O
the	O
rest	O
26	O
introduction	O
of	O
the	O
book	O
is	O
built	O
.	O
this	O
part	O
could	O
serve	O
as	O
a	O
basis	O
for	O
a	O
minicourse	O
on	O
the	O
theoretical	O
foundations	O
of	O
ml	O
.	O
the	O
second	O
part	O
of	O
the	O
book	O
introduces	O
the	O
most	O
commonly	O
used	O
algorithmic	O
approaches	O
to	O
supervised	O
machine	O
learning	O
.	O
a	O
subset	O
of	O
these	O
chapters	O
may	O
also	O
be	O
used	O
for	O
introducing	O
machine	O
learning	O
in	O
a	O
general	O
ai	O
course	O
to	O
computer	O
science	O
,	O
math	O
,	O
or	O
engineering	O
students	O
.	O
the	O
third	O
part	O
of	O
the	O
book	O
extends	O
the	O
scope	O
of	O
discussion	O
from	O
statistical	O
classiﬁcation	O
to	O
other	O
learning	O
models	O
.	O
it	O
covers	O
online	B
learning	I
,	O
unsupervised	B
learning	I
,	O
dimensionality	B
reduction	I
,	O
generative	B
models	I
,	O
and	O
feature	B
learning	I
.	O
the	O
fourth	O
part	O
of	O
the	O
book	O
,	O
advanced	O
theory	O
,	O
is	O
geared	O
toward	O
readers	O
who	O
have	O
interest	O
in	O
research	O
and	O
provides	O
the	O
more	O
technical	O
mathematical	O
tech-	O
niques	O
that	O
serve	O
to	O
analyze	O
and	O
drive	O
forward	O
the	O
ﬁeld	O
of	O
theoretical	O
machine	O
learning	O
.	O
the	O
appendixes	O
provide	O
some	O
technical	O
tools	O
used	O
in	O
the	O
book	O
.	O
in	O
particular	O
,	O
we	O
list	O
basic	O
results	O
from	O
measure	B
concentration	I
and	O
linear	O
algebra	O
.	O
a	O
few	O
sections	O
are	O
marked	O
by	O
an	O
asterisk	O
,	O
which	O
means	O
they	O
are	O
addressed	O
to	O
more	O
advanced	O
students	O
.	O
each	O
chapter	O
is	O
concluded	O
with	O
a	O
list	O
of	O
exercises	O
.	O
a	O
solution	O
manual	O
is	O
provided	O
in	O
the	O
course	O
web	O
site	O
.	O
1.5.1	O
possible	O
course	O
plans	O
based	O
on	O
this	O
book	O
a	O
14	O
week	O
introduction	O
course	O
for	O
graduate	O
students	O
:	O
1.	O
chapters	O
2–4	O
.	O
2.	O
chapter	O
9	O
(	O
without	O
the	O
vc	O
calculation	O
)	O
.	O
3.	O
chapters	O
5–6	O
(	O
without	O
proofs	O
)	O
.	O
4.	O
chapter	O
10	O
.	O
5.	O
chapters	O
7	O
,	O
11	O
(	O
without	O
proofs	O
)	O
.	O
6.	O
chapters	O
12	O
,	O
13	O
(	O
with	O
some	O
of	O
the	O
easier	O
proofs	O
)	O
.	O
7.	O
chapter	O
14	O
(	O
with	O
some	O
of	O
the	O
easier	O
proofs	O
)	O
.	O
8.	O
chapter	O
15	O
.	O
9.	O
chapter	O
16	O
.	O
10.	O
chapter	O
18	O
.	O
11.	O
chapter	O
22	O
.	O
12.	O
chapter	O
23	O
(	O
without	O
proofs	O
for	O
compressed	B
sensing	I
)	O
.	O
13.	O
chapter	O
24	O
.	O
14.	O
chapter	O
25.	O
a	O
14	O
week	O
advanced	O
course	O
for	O
graduate	O
students	O
:	O
1.	O
chapters	O
26	O
,	O
27	O
.	O
2	O
.	O
(	O
continued	O
)	O
3.	O
chapters	O
6	O
,	O
28	O
.	O
4.	O
chapter	O
7	O
.	O
5.	O
chapter	O
31	O
.	O
1.6	O
notation	O
27	O
6.	O
chapter	O
30	O
.	O
7.	O
chapters	O
12	O
,	O
13	O
.	O
8.	O
chapter	O
14	O
.	O
9.	O
chapter	O
8	O
.	O
10.	O
chapter	O
17	O
.	O
11.	O
chapter	O
29	O
.	O
12.	O
chapter	O
19	O
.	O
13.	O
chapter	O
20	O
.	O
14.	O
chapter	O
21	O
.	O
1.6	O
notation	O
most	O
of	O
the	O
notation	O
we	O
use	O
throughout	O
the	O
book	O
is	O
either	O
standard	O
or	O
deﬁned	O
on	O
the	O
spot	O
.	O
in	O
this	O
section	O
we	O
describe	O
our	O
main	O
conventions	O
and	O
provide	O
a	O
table	O
summarizing	O
our	O
notation	O
(	O
table	O
1.1	O
)	O
.	O
the	O
reader	O
is	O
encouraged	O
to	O
skip	O
this	O
section	O
and	O
return	O
to	O
it	O
if	O
during	O
the	O
reading	O
of	O
the	O
book	O
some	O
notation	O
is	O
unclear	O
.	O
we	O
denote	O
scalars	O
and	O
abstract	O
objects	O
with	O
lowercase	O
letters	O
(	O
e.g	O
.	O
x	O
and	O
λ	O
)	O
.	O
often	O
,	O
we	O
would	O
like	O
to	O
emphasize	O
that	O
some	O
object	O
is	O
a	O
vector	O
and	O
then	O
we	O
use	O
boldface	O
letters	O
(	O
e.g	O
.	O
x	O
and	O
λ	O
)	O
.	O
the	O
ith	O
element	O
of	O
a	O
vector	O
x	O
is	O
denoted	O
by	O
xi	O
.	O
we	O
use	O
uppercase	O
letters	O
to	O
denote	O
matrices	O
,	O
sets	O
,	O
and	O
sequences	O
.	O
the	O
meaning	O
should	O
be	O
clear	O
from	O
the	O
context	O
.	O
as	O
we	O
will	O
see	O
momentarily	O
,	O
the	O
input	O
of	O
a	O
learning	O
algorithm	O
is	O
a	O
sequence	O
of	O
training	O
examples	O
.	O
we	O
denote	O
by	O
z	O
an	O
abstract	O
example	O
and	O
by	O
s	O
=	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
a	O
sequence	O
of	O
m	O
examples	O
.	O
historically	O
,	O
s	O
is	O
often	O
referred	O
to	O
as	O
a	O
training	B
set	I
;	O
however	O
,	O
we	O
will	O
always	O
assume	O
that	O
s	O
is	O
a	O
sequence	O
rather	O
than	O
a	O
set	B
.	O
a	O
sequence	O
of	O
m	O
vectors	O
is	O
denoted	O
by	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
.	O
the	O
ith	O
element	O
of	O
xt	O
is	O
denoted	O
by	O
xt	O
,	O
i	O
.	O
throughout	O
the	O
book	O
,	O
we	O
make	O
use	O
of	O
basic	O
notions	O
from	O
probability	O
.	O
we	O
denote	O
by	O
d	O
a	O
distribution	O
over	O
some	O
set,2	O
for	O
example	O
,	O
z.	O
we	O
use	O
the	O
notation	O
z	O
∼	O
d	O
to	O
denote	O
that	O
z	O
is	O
sampled	O
according	O
to	O
d.	O
given	O
a	O
random	O
variable	O
f	O
:	O
z	O
→	O
r	O
,	O
its	O
expected	O
value	O
is	O
denoted	O
by	O
ez∼d	O
[	O
f	O
(	O
z	O
)	O
]	O
.	O
we	O
sometimes	O
use	O
the	O
shorthand	O
e	O
[	O
f	O
]	O
when	O
the	O
dependence	O
on	O
z	O
is	O
clear	O
from	O
the	O
context	O
.	O
for	O
f	O
:	O
z	O
→	O
{	O
true	O
,	O
false	O
}	O
we	O
also	O
use	O
pz∼d	O
[	O
f	O
(	O
z	O
)	O
]	O
to	O
denote	O
d	O
(	O
{	O
z	O
:	O
f	O
(	O
z	O
)	O
=	O
true	O
}	O
)	O
.	O
in	O
the	O
next	O
chapter	O
we	O
will	O
also	O
introduce	O
the	O
notation	O
dm	O
to	O
denote	O
the	O
probability	O
over	O
z	O
m	O
induced	O
by	O
sampling	O
(	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
)	O
where	O
each	O
point	O
zi	O
is	O
sampled	O
from	O
d	O
independently	O
of	O
the	O
other	O
points	O
.	O
in	O
general	O
,	O
we	O
have	O
made	O
an	O
eﬀort	O
to	O
avoid	O
asymptotic	O
notation	O
.	O
however	O
,	O
we	O
occasionally	O
use	O
it	O
to	O
clarify	O
the	O
main	O
results	O
.	O
in	O
particular	O
,	O
given	O
f	O
:	O
r	O
→	O
r+	O
and	O
g	O
:	O
r	O
→	O
r+	O
we	O
write	O
f	O
=	O
o	O
(	O
g	O
)	O
if	O
there	O
exist	O
x0	O
,	O
α	O
∈	O
r+	O
such	O
that	O
for	O
all	O
x	O
>	O
x0	O
we	O
have	O
f	O
(	O
x	O
)	O
≤	O
αg	O
(	O
x	O
)	O
.	O
we	O
write	O
f	O
=	O
o	O
(	O
g	O
)	O
if	O
for	O
every	O
α	O
>	O
0	O
there	O
exists	O
2	O
to	O
be	O
mathematically	O
precise	O
,	O
d	O
should	O
be	O
deﬁned	O
over	O
some	O
σ-algebra	O
of	O
subsets	O
of	O
z.	O
the	O
user	O
who	O
is	O
not	O
familiar	O
with	O
measure	O
theory	O
can	O
skip	O
the	O
few	O
footnotes	O
and	O
remarks	O
regarding	O
more	O
formal	O
measurability	O
deﬁnitions	O
and	O
assumptions	O
.	O
28	O
introduction	O
symbol	O
r	O
rd	O
r+	O
n	O
o	O
,	O
o	O
,	O
θ	O
,	O
ω	O
,	O
ω	O
,	O
˜o	O
1	O
[	O
boolean	O
expression	O
]	O
[	O
a	O
]	O
+	O
[	O
n	O
]	O
x	O
,	O
v	O
,	O
w	O
xi	O
,	O
vi	O
,	O
wi	O
(	O
cid:104	O
)	O
x	O
,	O
v	O
(	O
cid:105	O
)	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
or	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
1	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
∞	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
0	O
a	O
∈	O
rd	O
,	O
k	O
a	O
(	O
cid:62	O
)	O
ai	O
,	O
j	O
x	O
x	O
(	O
cid:62	O
)	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
xi	O
,	O
j	O
w	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
w	O
(	O
t	O
)	O
w	O
(	O
t	O
)	O
x	O
y	O
z	O
h	O
(	O
cid:96	O
)	O
:	O
h	O
×	O
z	O
→	O
r+	O
d	O
d	O
(	O
a	O
)	O
z	O
∼	O
d	O
s	O
=	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
s	O
∼	O
dm	O
p	O
,	O
e	O
pz∼d	O
[	O
f	O
(	O
z	O
)	O
]	O
ez∼d	O
[	O
f	O
(	O
z	O
)	O
]	O
n	O
(	O
µ	O
,	O
c	O
)	O
f	O
(	O
cid:48	O
)	O
(	O
x	O
)	O
f	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
(	O
x	O
)	O
i	O
∂f	O
(	O
w	O
)	O
∂wi	O
∇f	O
(	O
w	O
)	O
∂f	O
(	O
w	O
)	O
minx∈c	O
f	O
(	O
x	O
)	O
maxx∈c	O
f	O
(	O
x	O
)	O
argminx∈c	O
f	O
(	O
x	O
)	O
argmaxx∈c	O
f	O
(	O
x	O
)	O
log	O
table	O
1.1	O
summary	O
of	O
notation	O
meaning	O
the	O
set	B
of	O
real	O
numbers	O
the	O
set	B
of	O
d-dimensional	O
vectors	O
over	O
r	O
the	O
set	B
of	O
non-negative	O
real	O
numbers	O
the	O
set	B
of	O
natural	O
numbers	O
asymptotic	O
notation	O
(	O
see	O
text	O
)	O
indicator	O
function	B
(	O
equals	O
1	O
if	O
expression	O
is	O
true	O
and	O
0	O
o.w	O
.	O
)	O
=	O
max	O
{	O
0	O
,	O
a	O
}	O
the	O
set	B
{	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
(	O
for	O
n	O
∈	O
n	O
)	O
(	O
column	O
)	O
vectors	O
the	O
ith	O
element	O
of	O
a	O
vector	O
=	O
(	O
cid:80	O
)	O
d	O
=	O
(	O
cid:112	O
)	O
(	O
cid:104	O
)	O
x	O
,	O
x	O
(	O
cid:105	O
)	O
(	O
the	O
(	O
cid:96	O
)	O
2	O
norm	O
of	O
x	O
)	O
=	O
(	O
cid:80	O
)	O
d	O
i=1	O
xivi	O
(	O
inner	O
product	O
)	O
i=1	O
|xi|	O
(	O
the	O
(	O
cid:96	O
)	O
1	O
norm	O
of	O
x	O
)	O
=	O
maxi	O
|xi|	O
(	O
the	O
(	O
cid:96	O
)	O
∞	O
norm	O
of	O
x	O
)	O
the	O
number	O
of	O
nonzero	O
elements	O
of	O
x	O
a	O
d	O
×	O
k	O
matrix	O
over	O
r	O
the	O
transpose	O
of	O
a	O
the	O
(	O
i	O
,	O
j	O
)	O
element	O
of	O
a	O
the	O
d	O
×	O
d	O
matrix	O
a	O
s.t	O
.	O
ai	O
,	O
j	O
=	O
xixj	O
(	O
where	O
x	O
∈	O
rd	O
)	O
a	O
sequence	O
of	O
m	O
vectors	O
the	O
jth	O
element	O
of	O
the	O
ith	O
vector	O
in	O
the	O
sequence	O
the	O
values	O
of	O
a	O
vector	O
w	O
during	O
an	O
iterative	O
algorithm	O
the	O
ith	O
element	O
of	O
the	O
vector	O
w	O
(	O
t	O
)	O
instances	O
domain	B
(	O
a	O
set	B
)	O
labels	O
domain	B
(	O
a	O
set	B
)	O
examples	O
domain	B
(	O
a	O
set	B
)	O
hypothesis	B
class	I
(	O
a	O
set	B
)	O
loss	B
function	I
a	O
distribution	O
over	O
some	O
set	B
(	O
usually	O
over	O
z	O
or	O
over	O
x	O
)	O
the	O
probability	O
of	O
a	O
set	B
a	O
⊆	O
z	O
according	O
to	O
d	O
sampling	O
z	O
according	O
to	O
d	O
a	O
sequence	O
of	O
m	O
examples	O
sampling	O
s	O
=	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
i.i.d	O
.	O
according	O
to	O
d	O
probability	O
and	O
expectation	O
of	O
a	O
random	O
variable	O
=	O
d	O
(	O
{	O
z	O
:	O
f	O
(	O
z	O
)	O
=	O
true	O
}	O
)	O
for	O
f	O
:	O
z	O
→	O
{	O
true	O
,	O
false	O
}	O
expectation	O
of	O
the	O
random	O
variable	O
f	O
:	O
z	O
→	O
r	O
gaussian	O
distribution	O
with	O
expectation	O
µ	O
and	O
covariance	O
c	O
the	O
derivative	O
of	O
a	O
function	B
f	O
:	O
r	O
→	O
r	O
at	O
x	O
the	O
second	O
derivative	O
of	O
a	O
function	B
f	O
:	O
r	O
→	O
r	O
at	O
x	O
the	O
partial	O
derivative	O
of	O
a	O
function	B
f	O
:	O
rd	O
→	O
r	O
at	O
w	O
w.r.t	O
.	O
wi	O
the	O
gradient	B
of	O
a	O
function	B
f	O
:	O
rd	O
→	O
r	O
at	O
w	O
the	O
diﬀerential	B
set	I
of	O
a	O
function	B
f	O
:	O
rd	O
→	O
r	O
at	O
w	O
=	O
min	O
{	O
f	O
(	O
x	O
)	O
:	O
x	O
∈	O
c	O
}	O
(	O
minimal	O
value	O
of	O
f	O
over	O
c	O
)	O
=	O
max	O
{	O
f	O
(	O
x	O
)	O
:	O
x	O
∈	O
c	O
}	O
(	O
maximal	O
value	O
of	O
f	O
over	O
c	O
)	O
the	O
set	B
{	O
x	O
∈	O
c	O
:	O
f	O
(	O
x	O
)	O
=	O
minz∈c	O
f	O
(	O
z	O
)	O
}	O
the	O
set	B
{	O
x	O
∈	O
c	O
:	O
f	O
(	O
x	O
)	O
=	O
maxz∈c	O
f	O
(	O
z	O
)	O
}	O
the	O
natural	O
logarithm	O
1.6	O
notation	O
29	O
x0	O
such	O
that	O
for	O
all	O
x	O
>	O
x0	O
we	O
have	O
f	O
(	O
x	O
)	O
≤	O
αg	O
(	O
x	O
)	O
.	O
we	O
write	O
f	O
=	O
ω	O
(	O
g	O
)	O
if	O
there	O
exist	O
x0	O
,	O
α	O
∈	O
r+	O
such	O
that	O
for	O
all	O
x	O
>	O
x0	O
we	O
have	O
f	O
(	O
x	O
)	O
≥	O
αg	O
(	O
x	O
)	O
.	O
the	O
notation	O
f	O
=	O
ω	O
(	O
g	O
)	O
is	O
deﬁned	O
analogously	O
.	O
the	O
notation	O
f	O
=	O
θ	O
(	O
g	O
)	O
means	O
that	O
f	O
=	O
o	O
(	O
g	O
)	O
and	O
g	O
=	O
o	O
(	O
f	O
)	O
.	O
finally	O
,	O
the	O
notation	O
f	O
=	O
˜o	O
(	O
g	O
)	O
means	O
that	O
there	O
exists	O
k	O
∈	O
n	O
such	O
that	O
f	O
(	O
x	O
)	O
=	O
o	O
(	O
g	O
(	O
x	O
)	O
logk	O
(	O
g	O
(	O
x	O
)	O
)	O
)	O
.	O
the	O
inner	O
product	O
between	O
vectors	O
x	O
and	O
w	O
is	O
denoted	O
by	O
(	O
cid:104	O
)	O
x	O
,	O
w	O
(	O
cid:105	O
)	O
.	O
whenever	O
we	O
do	O
not	O
specify	O
the	O
vector	O
space	O
we	O
assume	O
that	O
it	O
is	O
the	O
d-dimensional	O
euclidean	O
i=1	O
xiwi	O
.	O
the	O
euclidean	O
(	O
or	O
(	O
cid:96	O
)	O
2	O
)	O
norm	O
of	O
a	O
vector	O
w	O
is	O
i	O
|wi|p	O
)	O
1/p	O
,	O
and	O
in	O
particular	O
space	O
and	O
then	O
(	O
cid:104	O
)	O
x	O
,	O
w	O
(	O
cid:105	O
)	O
=	O
(	O
cid:80	O
)	O
d	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:112	O
)	O
(	O
cid:104	O
)	O
w	O
,	O
w	O
(	O
cid:105	O
)	O
.	O
we	O
omit	O
the	O
subscript	O
from	O
the	O
(	O
cid:96	O
)	O
2	O
norm	O
when	O
it	O
is	O
clear	O
from	O
the	O
context	O
.	O
we	O
also	O
use	O
other	O
(	O
cid:96	O
)	O
p	O
norms	O
,	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
p	O
=	O
(	O
(	O
cid:80	O
)	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
1	O
=	O
(	O
cid:80	O
)	O
i	O
|wi|	O
and	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
∞	O
=	O
maxi	O
|wi|	O
.	O
we	O
use	O
the	O
notation	O
minx∈c	O
f	O
(	O
x	O
)	O
to	O
denote	O
the	O
minimum	O
value	O
of	O
the	O
set	B
{	O
f	O
(	O
x	O
)	O
:	O
x	O
∈	O
c	O
}	O
.	O
to	O
be	O
mathematically	O
more	O
precise	O
,	O
we	O
should	O
use	O
inf	O
x∈c	O
f	O
(	O
x	O
)	O
whenever	O
the	O
minimum	O
is	O
not	O
achievable	O
.	O
however	O
,	O
in	O
the	O
context	O
of	O
this	O
book	O
the	O
distinction	O
between	O
inﬁmum	O
and	O
minimum	O
is	O
often	O
of	O
little	O
interest	O
.	O
hence	O
,	O
to	O
simplify	O
the	O
presentation	O
,	O
we	O
sometimes	O
use	O
the	O
min	O
notation	O
even	O
when	O
inf	O
is	O
more	O
adequate	O
.	O
an	O
analogous	O
remark	O
applies	O
to	O
max	O
versus	O
sup	O
.	O
part	O
i	O
foundations	O
2	O
a	O
gentle	O
start	O
let	O
us	O
begin	O
our	O
mathematical	O
analysis	O
by	O
showing	O
how	O
successful	O
learning	O
can	O
be	O
achieved	O
in	O
a	O
relatively	O
simpliﬁed	O
setting	O
.	O
imagine	O
you	O
have	O
just	O
arrived	O
in	O
some	O
small	O
paciﬁc	O
island	O
.	O
you	O
soon	O
ﬁnd	O
out	O
that	O
papayas	O
are	O
a	O
signiﬁcant	O
ingredient	O
in	O
the	O
local	O
diet	O
.	O
however	O
,	O
you	O
have	O
never	O
before	O
tasted	O
papayas	O
.	O
you	O
have	O
to	O
learn	O
how	O
to	O
predict	O
whether	O
a	O
papaya	O
you	O
see	O
in	O
the	O
market	O
is	O
tasty	O
or	O
not	O
.	O
first	O
,	O
you	O
need	O
to	O
decide	O
which	O
features	O
of	O
a	O
papaya	O
your	O
prediction	O
should	O
be	O
based	O
on	O
.	O
on	O
the	O
basis	O
of	O
your	O
previous	O
experience	O
with	O
other	O
fruits	O
,	O
you	O
decide	O
to	O
use	O
two	O
features	O
:	O
the	O
papaya	O
’	O
s	O
color	O
,	O
ranging	O
from	O
dark	O
green	O
,	O
through	O
orange	O
and	O
red	O
to	O
dark	O
brown	O
,	O
and	O
the	O
papaya	O
’	O
s	O
softness	O
,	O
ranging	O
from	O
rock	O
hard	O
to	O
mushy	O
.	O
your	O
input	O
for	O
ﬁguring	O
out	O
your	O
prediction	O
rule	O
is	O
a	O
sample	O
of	O
papayas	O
that	O
you	O
have	O
examined	O
for	O
color	O
and	O
softness	O
and	O
then	O
tasted	O
and	O
found	O
out	O
whether	O
they	O
were	O
tasty	O
or	O
not	O
.	O
let	O
us	O
analyze	O
this	O
task	O
as	O
a	O
demonstration	O
of	O
the	O
considerations	O
involved	O
in	O
learning	O
problems	O
.	O
our	O
ﬁrst	O
step	O
is	O
to	O
describe	O
a	O
formal	O
model	O
aimed	O
to	O
capture	O
such	O
learning	O
tasks	O
.	O
2.1	O
a	O
formal	O
model	O
–	O
the	O
statistical	O
learning	O
framework	O
•	O
the	O
learner	O
’	O
s	O
input	O
:	O
in	O
the	O
basic	O
statistical	O
learning	O
setting	O
,	O
the	O
learner	O
has	O
access	O
to	O
the	O
following	O
:	O
–	O
domain	B
set	O
:	O
an	O
arbitrary	O
set	B
,	O
x	O
.	O
this	O
is	O
the	O
set	B
of	O
objects	O
that	O
we	O
may	O
wish	O
to	O
label	B
.	O
for	O
example	O
,	O
in	O
the	O
papaya	O
learning	O
problem	O
men-	O
tioned	O
before	O
,	O
the	O
domain	B
set	O
will	O
be	O
the	O
set	B
of	O
all	O
papayas	O
.	O
usually	O
,	O
these	O
domain	B
points	O
will	O
be	O
represented	O
by	O
a	O
vector	O
of	O
features	O
(	O
like	O
the	O
papaya	O
’	O
s	O
color	O
and	O
softness	O
)	O
.	O
we	O
also	O
refer	O
to	O
domain	B
points	O
as	O
instances	O
and	O
to	O
x	O
as	O
instance	B
space	I
.	O
–	O
label	B
set	O
:	O
for	O
our	O
current	O
discussion	O
,	O
we	O
will	O
restrict	O
the	O
label	B
set	O
to	O
be	O
a	O
two-element	O
set	B
,	O
usually	O
{	O
0	O
,	O
1	O
}	O
or	O
{	O
−1	O
,	O
+1	O
}	O
.	O
let	O
y	O
denote	O
our	O
set	B
of	O
possible	O
labels	O
.	O
for	O
our	O
papayas	O
example	O
,	O
let	O
y	O
be	O
{	O
0	O
,	O
1	O
}	O
,	O
where	O
1	O
represents	O
being	O
tasty	O
and	O
0	O
stands	O
for	O
being	O
not-tasty	O
.	O
–	O
training	O
data	O
:	O
s	O
=	O
(	O
(	O
x1	O
,	O
y1	O
)	O
.	O
.	O
.	O
(	O
xm	O
,	O
ym	O
)	O
)	O
is	O
a	O
ﬁnite	O
sequence	O
of	O
pairs	O
in	O
x	O
×y	O
:	O
that	O
is	O
,	O
a	O
sequence	O
of	O
labeled	O
domain	B
points	O
.	O
this	O
is	O
the	O
input	O
that	O
the	O
learner	O
has	O
access	O
to	O
(	O
like	O
a	O
set	B
of	O
papayas	O
that	O
have	O
been	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
34	O
a	O
gentle	O
start	O
tasted	O
and	O
their	O
color	O
,	O
softness	O
,	O
and	O
tastiness	O
)	O
.	O
such	O
labeled	O
examples	O
are	O
often	O
called	O
training	O
examples	O
.	O
we	O
sometimes	O
also	O
refer	O
to	O
s	O
as	O
a	O
training	O
set.1	O
•	O
the	O
learner	O
’	O
s	O
output	O
:	O
the	O
learner	O
is	O
requested	O
to	O
output	O
a	O
prediction	O
rule	O
,	O
h	O
:	O
x	O
→	O
y.	O
this	O
function	B
is	O
also	O
called	O
a	O
predictor	B
,	O
a	O
hypothesis	B
,	O
or	O
a	O
clas-	O
siﬁer	O
.	O
the	O
predictor	B
can	O
be	O
used	O
to	O
predict	O
the	O
label	B
of	O
new	O
domain	B
points	O
.	O
in	O
our	O
papayas	O
example	O
,	O
it	O
is	O
a	O
rule	O
that	O
our	O
learner	O
will	O
employ	O
to	O
predict	O
whether	O
future	O
papayas	O
he	O
examines	O
in	O
the	O
farmers	O
’	O
market	O
are	O
going	O
to	O
be	O
tasty	O
or	O
not	O
.	O
we	O
use	O
the	O
notation	O
a	O
(	O
s	O
)	O
to	O
denote	O
the	O
hypothesis	B
that	O
a	O
learning	O
algorithm	O
,	O
a	O
,	O
returns	O
upon	O
receiving	O
the	O
training	O
sequence	O
s.	O
•	O
a	O
simple	O
data-generation	O
model	O
we	O
now	O
explain	O
how	O
the	O
training	O
data	O
is	O
generated	O
.	O
first	O
,	O
we	O
assume	O
that	O
the	O
instances	O
(	O
the	O
papayas	O
we	O
encounter	O
)	O
are	O
generated	O
by	O
some	O
probability	O
distribution	O
(	O
in	O
this	O
case	O
,	O
representing	O
the	O
environment	O
)	O
.	O
let	O
us	O
denote	O
that	O
probability	O
distribution	O
over	O
x	O
by	O
d.	O
it	O
is	O
important	O
to	O
note	O
that	O
we	O
do	O
not	O
assume	O
that	O
the	O
learner	O
knows	O
anything	O
about	O
this	O
distribution	O
.	O
for	O
the	O
type	O
of	O
learning	O
tasks	O
we	O
discuss	O
,	O
this	O
could	O
be	O
any	O
arbitrary	O
probability	O
distribution	O
.	O
as	O
to	O
the	O
labels	O
,	O
in	O
the	O
current	O
discussion	O
we	O
assume	O
that	O
there	O
is	O
some	O
“	O
correct	O
”	O
labeling	O
function	B
,	O
f	O
:	O
x	O
→	O
y	O
,	O
and	O
that	O
yi	O
=	O
f	O
(	O
xi	O
)	O
for	O
all	O
i.	O
this	O
assumption	O
will	O
be	O
relaxed	O
in	O
the	O
next	O
chapter	O
.	O
the	O
labeling	O
function	B
is	O
unknown	O
to	O
the	O
learner	O
.	O
in	O
fact	O
,	O
this	O
is	O
just	O
what	O
the	O
learner	O
is	O
trying	O
to	O
ﬁgure	O
out	O
.	O
in	O
summary	O
,	O
each	O
pair	O
in	O
the	O
training	O
data	O
s	O
is	O
generated	O
by	O
ﬁrst	O
sampling	O
a	O
point	O
xi	O
according	O
to	O
d	O
and	O
then	O
labeling	O
it	O
by	O
f	O
.	O
•	O
measures	O
of	O
success	O
:	O
we	O
deﬁne	O
the	O
error	O
of	O
a	O
classiﬁer	B
to	O
be	O
the	O
probability	O
that	O
it	O
does	O
not	O
predict	O
the	O
correct	O
label	B
on	O
a	O
random	O
data	O
point	O
generated	O
by	O
the	O
aforementioned	O
underlying	O
distribution	O
.	O
that	O
is	O
,	O
the	O
error	O
of	O
h	O
is	O
the	O
probability	O
to	O
draw	O
a	O
random	O
instance	O
x	O
,	O
according	O
to	O
the	O
distribution	O
d	O
,	O
such	O
that	O
h	O
(	O
x	O
)	O
does	O
not	O
equal	O
f	O
(	O
x	O
)	O
.	O
formally	O
,	O
given	O
a	O
domain	B
subset,2	O
a	O
⊂	O
x	O
,	O
the	O
probability	O
distribution	O
,	O
d	O
,	O
assigns	O
a	O
number	O
,	O
d	O
(	O
a	O
)	O
,	O
which	O
determines	O
how	O
likely	O
it	O
is	O
to	O
observe	O
a	O
point	O
x	O
∈	O
a.	O
in	O
many	O
cases	O
,	O
we	O
refer	O
to	O
a	O
as	O
an	O
event	O
and	O
express	O
it	O
using	O
a	O
function	B
π	O
:	O
x	O
→	O
{	O
0	O
,	O
1	O
}	O
,	O
namely	O
,	O
a	O
=	O
{	O
x	O
∈	O
x	O
:	O
π	O
(	O
x	O
)	O
=	O
1	O
}	O
.	O
in	O
that	O
case	O
,	O
we	O
also	O
use	O
the	O
notation	O
px∼d	O
[	O
π	O
(	O
x	O
)	O
]	O
to	O
express	O
d	O
(	O
a	O
)	O
.	O
we	O
deﬁne	O
the	O
error	O
of	O
a	O
prediction	O
rule	O
,	O
h	O
:	O
x	O
→	O
y	O
,	O
to	O
be	O
x∼d	O
[	O
h	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
f	O
(	O
x	O
)	O
]	O
def=	O
d	O
(	O
{	O
x	O
:	O
h	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
f	O
(	O
x	O
)	O
}	O
)	O
.	O
ld	O
,	O
f	O
(	O
h	O
)	O
def=	O
p	O
(	O
2.1	O
)	O
that	O
is	O
,	O
the	O
error	O
of	O
such	O
h	O
is	O
the	O
probability	O
of	O
randomly	O
choosing	O
an	O
example	O
x	O
for	O
which	O
h	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
f	O
(	O
x	O
)	O
.	O
the	O
subscript	O
(	O
d	O
,	O
f	O
)	O
indicates	O
that	O
the	O
error	O
is	O
measured	O
with	O
respect	O
to	O
the	O
probability	O
distribution	O
d	O
and	O
the	O
1	O
despite	O
the	O
“	O
set	B
”	O
notation	O
,	O
s	O
is	O
a	O
sequence	O
.	O
in	O
particular	O
,	O
the	O
same	O
example	O
may	O
appear	O
2	O
strictly	O
speaking	O
,	O
we	O
should	O
be	O
more	O
careful	O
and	O
require	O
that	O
a	O
is	O
a	O
member	O
of	O
some	O
twice	O
in	O
s	O
and	O
some	O
algorithms	O
can	O
take	O
into	O
account	O
the	O
order	O
of	O
examples	O
in	O
s.	O
σ-algebra	O
of	O
subsets	O
of	O
x	O
,	O
over	O
which	O
d	O
is	O
deﬁned	O
.	O
we	O
will	O
formally	O
deﬁne	O
our	O
measurability	O
assumptions	O
in	O
the	O
next	O
chapter	O
.	O
2.2	O
empirical	B
risk	I
minimization	O
35	O
correct	O
labeling	O
function	B
f	O
.	O
we	O
omit	O
this	O
subscript	O
when	O
it	O
is	O
clear	O
from	O
the	O
context	O
.	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
h	O
)	O
has	O
several	O
synonymous	O
names	O
such	O
as	O
the	O
general-	O
ization	O
error	O
,	O
the	O
risk	B
,	O
or	O
the	O
true	B
error	I
of	O
h	O
,	O
and	O
we	O
will	O
use	O
these	O
names	O
interchangeably	O
throughout	O
the	O
book	O
.	O
we	O
use	O
the	O
letter	O
l	O
for	O
the	O
error	O
,	O
since	O
we	O
view	O
this	O
error	O
as	O
the	O
loss	B
of	O
the	O
learner	O
.	O
we	O
will	O
later	O
also	O
discuss	O
other	O
possible	O
formulations	O
of	O
such	O
loss	B
.	O
•	O
a	O
note	O
about	O
the	O
information	O
available	O
to	O
the	O
learner	O
the	O
learner	O
is	O
blind	O
to	O
the	O
underlying	O
distribution	O
d	O
over	O
the	O
world	O
and	O
to	O
the	O
labeling	O
function	B
f.	O
in	O
our	O
papayas	O
example	O
,	O
we	O
have	O
just	O
arrived	O
in	O
a	O
new	O
island	O
and	O
we	O
have	O
no	O
clue	O
as	O
to	O
how	O
papayas	O
are	O
distributed	O
and	O
how	O
to	O
predict	O
their	O
tastiness	O
.	O
the	O
only	O
way	O
the	O
learner	O
can	O
interact	O
with	O
the	O
environment	O
is	O
through	O
observing	O
the	O
training	B
set	I
.	O
in	O
the	O
next	O
section	O
we	O
describe	O
a	O
simple	O
learning	O
paradigm	O
for	O
the	O
preceding	O
setup	O
and	O
analyze	O
its	O
performance	O
.	O
2.2	O
empirical	B
risk	I
minimization	O
as	O
mentioned	O
earlier	O
,	O
a	O
learning	O
algorithm	O
receives	O
as	O
input	O
a	O
training	B
set	I
s	O
,	O
sampled	O
from	O
an	O
unknown	O
distribution	O
d	O
and	O
labeled	O
by	O
some	O
target	O
function	O
f	O
,	O
and	O
should	O
output	O
a	O
predictor	B
hs	O
:	O
x	O
→	O
y	O
(	O
the	O
subscript	O
s	O
emphasizes	O
the	O
fact	O
that	O
the	O
output	O
predictor	B
depends	O
on	O
s	O
)	O
.	O
the	O
goal	O
of	O
the	O
algorithm	O
is	O
to	O
ﬁnd	O
hs	O
that	O
minimizes	O
the	O
error	O
with	O
respect	O
to	O
the	O
unknown	O
d	O
and	O
f	O
.	O
since	O
the	O
learner	O
does	O
not	O
know	O
what	O
d	O
and	O
f	O
are	O
,	O
the	O
true	B
error	I
is	O
not	O
directly	O
available	O
to	O
the	O
learner	O
.	O
a	O
useful	O
notion	O
of	O
error	O
that	O
can	O
be	O
calculated	O
by	O
the	O
learner	O
is	O
the	O
training	B
error	I
–	O
the	O
error	O
the	O
classiﬁer	B
incurs	O
over	O
the	O
training	O
sample	O
:	O
,	O
(	O
2.2	O
)	O
ls	O
(	O
h	O
)	O
def=	O
where	O
[	O
m	O
]	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
}	O
.	O
|	O
{	O
i	O
∈	O
[	O
m	O
]	O
:	O
h	O
(	O
xi	O
)	O
(	O
cid:54	O
)	O
=	O
yi	O
}	O
|	O
m	O
the	O
terms	O
empirical	B
error	I
and	O
empirical	B
risk	I
are	O
often	O
used	O
interchangeably	O
for	O
this	O
error	O
.	O
since	O
the	O
training	O
sample	O
is	O
the	O
snapshot	O
of	O
the	O
world	O
that	O
is	O
available	O
to	O
the	O
learner	O
,	O
it	O
makes	O
sense	O
to	O
search	O
for	O
a	O
solution	O
that	O
works	O
well	O
on	O
that	O
data	O
.	O
this	O
learning	O
paradigm	O
–	O
coming	O
up	O
with	O
a	O
predictor	B
h	O
that	O
minimizes	O
ls	O
(	O
h	O
)	O
–	O
is	O
called	O
empirical	B
risk	I
minimization	O
or	O
erm	O
for	O
short	O
.	O
2.2.1	O
something	O
may	O
go	O
wrong	O
–	O
overﬁtting	B
although	O
the	O
erm	O
rule	O
seems	O
very	O
natural	O
,	O
without	O
being	O
careful	O
,	O
this	O
approach	O
may	O
fail	O
miserably	O
.	O
to	O
demonstrate	O
such	O
a	O
failure	O
,	O
let	O
us	O
go	O
back	O
to	O
the	O
problem	O
of	O
learning	O
to	O
36	O
a	O
gentle	O
start	O
predict	O
the	O
taste	O
of	O
a	O
papaya	O
on	O
the	O
basis	O
of	O
its	O
softness	O
and	O
color	O
.	O
consider	O
a	O
sample	O
as	O
depicted	O
in	O
the	O
following	O
:	O
assume	O
that	O
the	O
probability	O
distribution	O
d	O
is	O
such	O
that	O
instances	O
are	O
distributed	O
uniformly	O
within	O
the	O
gray	O
square	O
and	O
the	O
labeling	O
function	B
,	O
f	O
,	O
determines	O
the	O
label	B
to	O
be	O
1	O
if	O
the	O
instance	B
is	O
within	O
the	O
inner	O
blue	O
square	O
,	O
and	O
0	O
otherwise	O
.	O
the	O
area	O
of	O
the	O
gray	O
square	O
in	O
the	O
picture	O
is	O
2	O
and	O
the	O
area	O
of	O
the	O
blue	O
square	O
is	O
1.	O
consider	O
the	O
following	O
predictor	B
:	O
if	O
∃i	O
∈	O
[	O
m	O
]	O
s.t	O
.	O
xi	O
=	O
x	O
otherwise	O
.	O
(	O
2.3	O
)	O
yi	O
0	O
(	O
cid:40	O
)	O
hs	O
(	O
x	O
)	O
=	O
while	O
this	O
predictor	B
might	O
seem	O
rather	O
artiﬁcial	O
,	O
in	O
exercise	O
1	O
we	O
show	O
a	O
natural	O
representation	O
of	O
it	O
using	O
polynomials	O
.	O
clearly	O
,	O
no	O
matter	O
what	O
the	O
sample	O
is	O
,	O
ls	O
(	O
hs	O
)	O
=	O
0	O
,	O
and	O
therefore	O
this	O
predictor	B
may	O
be	O
chosen	O
by	O
an	O
erm	O
algorithm	O
(	O
it	O
is	O
one	O
of	O
the	O
empirical-minimum-cost	O
hypotheses	O
;	O
no	O
classiﬁer	B
can	O
have	O
smaller	O
error	O
)	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
true	B
error	I
of	O
any	O
classiﬁer	B
that	O
predicts	O
the	O
label	B
1	O
only	O
on	O
a	O
ﬁnite	O
number	O
of	O
instances	O
is	O
,	O
in	O
this	O
case	O
,	O
1/2	O
.	O
thus	O
,	O
ld	O
(	O
hs	O
)	O
=	O
1/2	O
.	O
we	O
have	O
found	O
a	O
predictor	B
whose	O
performance	O
on	O
the	O
training	B
set	I
is	O
excellent	O
,	O
yet	O
its	O
performance	O
on	O
the	O
true	O
“	O
world	O
”	O
is	O
very	O
poor	O
.	O
this	O
phenomenon	O
is	O
called	O
overﬁtting	B
.	O
intuitively	O
,	O
overﬁtting	B
occurs	O
when	O
our	O
hypothesis	B
ﬁts	O
the	O
training	O
data	O
“	O
too	O
well	O
”	O
(	O
perhaps	O
like	O
the	O
everyday	O
experience	O
that	O
a	O
person	O
who	O
provides	O
a	O
perfect	O
detailed	O
explanation	O
for	O
each	O
of	O
his	O
single	O
actions	O
may	O
raise	O
suspicion	O
)	O
.	O
2.3	O
empirical	B
risk	I
minimization	O
with	O
inductive	B
bias	I
we	O
have	O
just	O
demonstrated	O
that	O
the	O
erm	O
rule	O
might	O
lead	O
to	O
overﬁtting	B
.	O
rather	O
than	O
giving	O
up	O
on	O
the	O
erm	O
paradigm	O
,	O
we	O
will	O
look	O
for	O
ways	O
to	O
rectify	O
it	O
.	O
we	O
will	O
search	O
for	O
conditions	O
under	O
which	O
there	O
is	O
a	O
guarantee	O
that	O
erm	O
does	O
not	O
overﬁt	O
,	O
namely	O
,	O
conditions	O
under	O
which	O
when	O
the	O
erm	O
predictor	B
has	O
good	O
performance	O
with	O
respect	O
to	O
the	O
training	O
data	O
,	O
it	O
is	O
also	O
highly	O
likely	O
to	O
perform	O
well	O
over	O
the	O
underlying	O
data	O
distribution	O
.	O
a	O
common	O
solution	O
is	O
to	O
apply	O
the	O
erm	O
learning	O
rule	O
over	O
a	O
restricted	O
search	O
space	O
.	O
formally	O
,	O
the	O
learner	O
should	O
choose	O
in	O
advance	O
(	O
before	O
seeing	O
the	O
data	O
)	O
a	O
set	B
of	O
predictors	O
.	O
this	O
set	B
is	O
called	O
a	O
hypothesis	B
class	I
and	O
is	O
denoted	O
by	O
h.	O
each	O
h	O
∈	O
h	O
is	O
a	O
function	B
mapping	O
from	O
x	O
to	O
y.	O
for	O
a	O
given	O
class	O
h	O
,	O
and	O
a	O
training	O
sample	O
,	O
s	O
,	O
the	O
ermh	O
learner	O
uses	O
the	O
erm	O
rule	O
to	O
choose	O
a	O
predictor	B
h	O
∈	O
h	O
,	O
2.3	O
empirical	B
risk	I
minimization	O
with	O
inductive	B
bias	I
37	O
with	O
the	O
lowest	O
possible	O
error	O
over	O
s.	O
formally	O
,	O
ermh	O
(	O
s	O
)	O
∈	O
argmin	O
h∈h	O
ls	O
(	O
h	O
)	O
,	O
where	O
argmin	O
stands	O
for	O
the	O
set	B
of	O
hypotheses	O
in	O
h	O
that	O
achieve	O
the	O
minimum	O
value	O
of	O
ls	O
(	O
h	O
)	O
over	O
h.	O
by	O
restricting	O
the	O
learner	O
to	O
choosing	O
a	O
predictor	B
from	O
h	O
,	O
we	O
bias	B
it	O
toward	O
a	O
particular	O
set	B
of	O
predictors	O
.	O
such	O
restrictions	O
are	O
often	O
called	O
an	O
inductive	B
bias	I
.	O
since	O
the	O
choice	O
of	O
such	O
a	O
restriction	O
is	O
determined	O
before	O
the	O
learner	O
sees	O
the	O
training	O
data	O
,	O
it	O
should	O
ideally	O
be	O
based	O
on	O
some	O
prior	B
knowledge	I
about	O
the	O
problem	O
to	O
be	O
learned	O
.	O
for	O
example	O
,	O
for	O
the	O
papaya	O
taste	O
prediction	O
problem	O
we	O
may	O
choose	O
the	O
class	O
h	O
to	O
be	O
the	O
set	B
of	O
predictors	O
that	O
are	O
determined	O
by	O
axis	O
aligned	O
rectangles	O
(	O
in	O
the	O
space	O
determined	O
by	O
the	O
color	O
and	O
softness	O
coordinates	O
)	O
.	O
we	O
will	O
later	O
show	O
that	O
ermh	O
over	O
this	O
class	O
is	O
guaranteed	O
not	O
to	O
overﬁt	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
example	O
of	O
overﬁtting	B
that	O
we	O
have	O
seen	O
previously	O
,	O
demonstrates	O
that	O
choosing	O
h	O
to	O
be	O
a	O
class	O
of	O
predictors	O
that	O
includes	O
all	O
functions	O
that	O
assign	O
the	O
value	O
1	O
to	O
a	O
ﬁnite	O
set	B
of	O
domain	B
points	O
does	O
not	O
suﬃce	O
to	O
guarantee	O
that	O
ermh	O
will	O
not	O
overﬁt	O
.	O
a	O
fundamental	O
question	O
in	O
learning	O
theory	O
is	O
,	O
over	O
which	O
hypothesis	B
classes	O
ermh	O
learning	O
will	O
not	O
result	O
in	O
overﬁtting	B
.	O
we	O
will	O
study	O
this	O
question	O
later	O
in	O
the	O
book	O
.	O
intuitively	O
,	O
choosing	O
a	O
more	O
restricted	O
hypothesis	B
class	I
better	O
protects	O
us	O
against	O
overﬁtting	B
but	O
at	O
the	O
same	O
time	O
might	O
cause	O
us	O
a	O
stronger	O
inductive	B
bias	I
.	O
we	O
will	O
get	O
back	O
to	O
this	O
fundamental	O
tradeoﬀ	O
later	O
.	O
2.3.1	O
finite	O
hypothesis	B
classes	O
the	O
simplest	O
type	O
of	O
restriction	O
on	O
a	O
class	O
is	O
imposing	O
an	O
upper	O
bound	O
on	O
its	O
size	O
(	O
that	O
is	O
,	O
the	O
number	O
of	O
predictors	O
h	O
in	O
h	O
)	O
.	O
in	O
this	O
section	O
,	O
we	O
show	O
that	O
if	O
h	O
is	O
a	O
ﬁnite	O
class	O
then	O
ermh	O
will	O
not	O
overﬁt	O
,	O
provided	O
it	O
is	O
based	O
on	O
a	O
suﬃciently	O
large	O
training	O
sample	O
(	O
this	O
size	O
requirement	O
will	O
depend	O
on	O
the	O
size	O
of	O
h	O
)	O
.	O
limiting	O
the	O
learner	O
to	O
prediction	O
rules	O
within	O
some	O
ﬁnite	O
hypothesis	B
class	I
may	O
be	O
considered	O
as	O
a	O
reasonably	O
mild	O
restriction	O
.	O
for	O
example	O
,	O
h	O
can	O
be	O
the	O
set	B
of	O
all	O
predictors	O
that	O
can	O
be	O
implemented	O
by	O
a	O
c++	O
program	O
written	O
in	O
at	O
most	O
109	O
bits	O
of	O
code	O
.	O
in	O
our	O
papayas	O
example	O
,	O
we	O
mentioned	O
previously	O
the	O
class	O
of	O
axis	O
aligned	O
rectangles	O
.	O
while	O
this	O
is	O
an	O
inﬁnite	O
class	O
,	O
if	O
we	O
discretize	O
the	O
repre-	O
sentation	O
of	O
real	O
numbers	O
,	O
say	O
,	O
by	O
using	O
a	O
64	O
bits	O
ﬂoating-point	O
representation	O
,	O
the	O
hypothesis	B
class	I
becomes	O
a	O
ﬁnite	O
class	O
.	O
let	O
us	O
now	O
analyze	O
the	O
performance	O
of	O
the	O
ermh	O
learning	O
rule	O
assuming	O
that	O
h	O
is	O
a	O
ﬁnite	O
class	O
.	O
for	O
a	O
training	O
sample	O
,	O
s	O
,	O
labeled	O
according	O
to	O
some	O
f	O
:	O
x	O
→	O
y	O
,	O
let	O
hs	O
denote	O
a	O
result	O
of	O
applying	O
ermh	O
to	O
s	O
,	O
namely	O
,	O
hs	O
∈	O
argmin	O
h∈h	O
ls	O
(	O
h	O
)	O
.	O
(	O
2.4	O
)	O
in	O
this	O
chapter	O
,	O
we	O
make	O
the	O
following	O
simplifying	O
assumption	O
(	O
which	O
will	O
be	O
relaxed	O
in	O
the	O
next	O
chapter	O
)	O
.	O
38	O
a	O
gentle	O
start	O
definition	O
2.1	O
(	O
the	O
realizability	B
assumption	O
)	O
there	O
exists	O
h	O
(	O
cid:63	O
)	O
∈	O
h	O
s.t	O
.	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
=	O
0.	O
note	O
that	O
this	O
assumption	O
implies	O
that	O
with	O
probability	O
1	O
over	O
random	O
samples	O
,	O
s	O
,	O
where	O
the	O
instances	O
of	O
s	O
are	O
sampled	O
according	O
to	O
d	O
and	O
are	O
labeled	O
by	O
f	O
,	O
we	O
have	O
ls	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
=	O
0.	O
the	O
realizability	B
assumption	O
implies	O
that	O
for	O
every	O
erm	O
hypothesis	B
we	O
have	O
that3	O
ls	O
(	O
hs	O
)	O
=	O
0.	O
however	O
,	O
we	O
are	O
interested	O
in	O
the	O
true	O
risk	O
of	O
hs	O
,	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
hs	O
)	O
,	O
rather	O
than	O
its	O
empirical	B
risk	I
.	O
clearly	O
,	O
any	O
guarantee	O
on	O
the	O
error	O
with	O
respect	O
to	O
the	O
underlying	O
distribution	O
,	O
d	O
,	O
for	O
an	O
algorithm	O
that	O
has	O
access	O
only	O
to	O
a	O
sample	O
s	O
should	O
depend	O
on	O
the	O
relationship	O
between	O
d	O
and	O
s.	O
the	O
common	O
assumption	O
in	O
statistical	O
machine	O
learning	O
is	O
that	O
the	O
training	O
sample	O
s	O
is	O
generated	O
by	O
sampling	O
points	O
from	O
the	O
distribution	O
d	O
independently	O
of	O
each	O
other	O
.	O
formally	O
,	O
•	O
the	O
i.i.d	O
.	O
assumption	O
:	O
the	O
examples	O
in	O
the	O
training	B
set	I
are	O
independently	O
and	O
identically	O
distributed	O
(	O
i.i.d	O
.	O
)	O
according	O
to	O
the	O
distribution	O
d.	O
that	O
is	O
,	O
every	O
xi	O
in	O
s	O
is	O
freshly	O
sampled	O
according	O
to	O
d	O
and	O
then	O
labeled	O
according	O
to	O
the	O
labeling	O
function	B
,	O
f	O
.	O
we	O
denote	O
this	O
assumption	O
by	O
s	O
∼	O
dm	O
where	O
m	O
is	O
the	O
size	O
of	O
s	O
,	O
and	O
dm	O
denotes	O
the	O
probability	O
over	O
m-tuples	O
induced	O
by	O
applying	O
d	O
to	O
pick	O
each	O
element	O
of	O
the	O
tuple	O
independently	O
of	O
the	O
other	O
members	O
of	O
the	O
tuple	O
.	O
intuitively	O
,	O
the	O
training	B
set	I
s	O
is	O
a	O
window	O
through	O
which	O
the	O
learner	O
gets	O
partial	O
information	O
about	O
the	O
distribution	O
d	O
over	O
the	O
world	O
and	O
the	O
labeling	O
function	B
,	O
f	O
.	O
the	O
larger	O
the	O
sample	O
gets	O
,	O
the	O
more	O
likely	O
it	O
is	O
to	O
reﬂect	O
more	O
accurately	O
the	O
distribution	O
and	O
labeling	O
used	O
to	O
generate	O
it	O
.	O
since	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
hs	O
)	O
depends	O
on	O
the	O
training	B
set	I
,	O
s	O
,	O
and	O
that	O
training	B
set	I
is	O
picked	O
by	O
a	O
random	O
process	O
,	O
there	O
is	O
randomness	O
in	O
the	O
choice	O
of	O
the	O
predictor	B
hs	O
and	O
,	O
consequently	O
,	O
in	O
the	O
risk	B
l	O
(	O
d	O
,	O
f	O
)	O
(	O
hs	O
)	O
.	O
formally	O
,	O
we	O
say	O
that	O
it	O
is	O
a	O
random	O
variable	O
.	O
it	O
is	O
not	O
realistic	O
to	O
expect	O
that	O
with	O
full	O
certainty	O
s	O
will	O
suﬃce	O
to	O
direct	O
the	O
learner	O
toward	O
a	O
good	O
classiﬁer	B
(	O
from	O
the	O
point	O
of	O
view	O
of	O
d	O
)	O
,	O
as	O
there	O
is	O
always	O
some	O
probability	O
that	O
the	O
sampled	O
training	O
data	O
happens	O
to	O
be	O
very	O
nonrepresentative	O
of	O
the	O
underlying	O
d.	O
if	O
we	O
go	O
back	O
to	O
the	O
papaya	O
tasting	O
example	O
,	O
there	O
is	O
always	O
some	O
(	O
small	O
)	O
chance	O
that	O
all	O
the	O
papayas	O
we	O
have	O
happened	O
to	O
taste	O
were	O
not	O
tasty	O
,	O
in	O
spite	O
of	O
the	O
fact	O
that	O
,	O
say	O
,	O
70	O
%	O
of	O
the	O
papayas	O
in	O
our	O
island	O
are	O
tasty	O
.	O
in	O
such	O
a	O
case	O
,	O
ermh	O
(	O
s	O
)	O
may	O
be	O
the	O
constant	O
function	B
that	O
labels	O
every	O
papaya	O
as	O
“	O
not	O
tasty	O
”	O
(	O
and	O
has	O
70	O
%	O
error	O
on	O
the	O
true	O
distribution	O
of	O
papapyas	O
in	O
the	O
island	O
)	O
.	O
we	O
will	O
therefore	O
address	O
the	O
probability	O
to	O
sample	O
a	O
training	B
set	I
for	O
which	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
hs	O
)	O
is	O
not	O
too	O
large	O
.	O
usually	O
,	O
we	O
denote	O
the	O
probability	O
of	O
getting	O
a	O
nonrepresentative	O
sample	O
by	O
δ	O
,	O
and	O
call	O
(	O
1	O
−	O
δ	O
)	O
the	O
conﬁdence	B
parameter	O
of	O
our	O
prediction	O
.	O
on	O
top	O
of	O
that	O
,	O
since	O
we	O
can	O
not	O
guarantee	O
perfect	O
label	B
prediction	O
,	O
we	O
intro-	O
duce	O
another	O
parameter	O
for	O
the	O
quality	O
of	O
prediction	O
,	O
the	O
accuracy	B
parameter	O
,	O
3	O
mathematically	O
speaking	O
,	O
this	O
holds	O
with	O
probability	O
1.	O
to	O
simplify	O
the	O
presentation	O
,	O
we	O
sometimes	O
omit	O
the	O
“	O
with	O
probability	O
1	O
”	O
speciﬁer	O
.	O
2.3	O
empirical	B
risk	I
minimization	O
with	O
inductive	B
bias	I
39	O
commonly	O
denoted	O
by	O
	O
.	O
we	O
interpret	O
the	O
event	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
hs	O
)	O
>	O
	O
as	O
a	O
failure	O
of	O
the	O
learner	O
,	O
while	O
if	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
hs	O
)	O
≤	O
	O
we	O
view	O
the	O
output	O
of	O
the	O
algorithm	O
as	O
an	O
approx-	O
imately	O
correct	O
predictor	B
.	O
therefore	O
(	O
ﬁxing	O
some	O
labeling	O
function	B
f	O
:	O
x	O
→	O
y	O
)	O
,	O
we	O
are	O
interested	O
in	O
upper	O
bounding	O
the	O
probability	O
to	O
sample	O
m-tuple	O
of	O
in-	O
stances	O
that	O
will	O
lead	O
to	O
failure	O
of	O
the	O
learner	O
.	O
formally	O
,	O
let	O
s|x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
be	O
the	O
instances	O
of	O
the	O
training	B
set	I
.	O
we	O
would	O
like	O
to	O
upper	O
bound	O
dm	O
(	O
{	O
s|x	O
:	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
hs	O
)	O
>	O
	O
}	O
)	O
.	O
let	O
hb	O
be	O
the	O
set	B
of	O
“	O
bad	O
”	O
hypotheses	O
,	O
that	O
is	O
,	O
hb	O
=	O
{	O
h	O
∈	O
h	O
:	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
h	O
)	O
>	O
	O
}	O
.	O
in	O
addition	O
,	O
let	O
m	O
=	O
{	O
s|x	O
:	O
∃h	O
∈	O
hb	O
,	O
ls	O
(	O
h	O
)	O
=	O
0	O
}	O
be	O
the	O
set	B
of	O
misleading	O
samples	O
:	O
namely	O
,	O
for	O
every	O
s|x	O
∈	O
m	O
,	O
there	O
is	O
a	O
“	O
bad	O
”	O
hypothesis	B
,	O
h	O
∈	O
hb	O
,	O
that	O
looks	O
like	O
a	O
“	O
good	O
”	O
hypothesis	B
on	O
s|x	O
.	O
now	O
,	O
recall	B
that	O
we	O
would	O
like	O
to	O
bound	O
the	O
probability	O
of	O
the	O
event	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
hs	O
)	O
>	O
	O
.	O
but	O
,	O
since	O
the	O
realizability	B
assumption	O
implies	O
that	O
ls	O
(	O
hs	O
)	O
=	O
0	O
,	O
it	O
follows	O
that	O
the	O
event	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
hs	O
)	O
>	O
	O
can	O
only	O
happen	O
if	O
for	O
some	O
h	O
∈	O
hb	O
we	O
have	O
ls	O
(	O
h	O
)	O
=	O
0.	O
in	O
other	O
words	O
,	O
this	O
event	O
will	O
only	O
happen	O
if	O
our	O
sample	O
is	O
in	O
the	O
set	B
of	O
misleading	O
samples	O
,	O
m	O
.	O
formally	O
,	O
we	O
have	O
shown	O
that	O
{	O
s|x	O
:	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
hs	O
)	O
>	O
	O
}	O
⊆	O
m	O
.	O
note	O
that	O
we	O
can	O
rewrite	O
m	O
as	O
(	O
cid:91	O
)	O
h∈hb	O
m	O
=	O
{	O
s|x	O
:	O
ls	O
(	O
h	O
)	O
=	O
0	O
}	O
.	O
(	O
2.5	O
)	O
hence	O
,	O
dm	O
(	O
{	O
s|x	O
:	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
hs	O
)	O
>	O
	O
}	O
)	O
≤	O
dm	O
(	O
m	O
)	O
=	O
dm	O
(	O
∪h∈hb	O
{	O
s|x	O
:	O
ls	O
(	O
h	O
)	O
=	O
0	O
}	O
)	O
.	O
(	O
2.6	O
)	O
next	O
,	O
we	O
upper	O
bound	O
the	O
right-hand	O
side	O
of	O
the	O
preceding	O
equation	O
using	O
the	O
union	B
bound	I
–	O
a	O
basic	O
property	O
of	O
probabilities	O
.	O
lemma	O
2.2	O
(	O
union	B
bound	I
)	O
for	O
any	O
two	O
sets	O
a	O
,	O
b	O
and	O
a	O
distribution	O
d	O
we	O
have	O
d	O
(	O
a	O
∪	O
b	O
)	O
≤	O
d	O
(	O
a	O
)	O
+	O
d	O
(	O
b	O
)	O
.	O
applying	O
the	O
union	B
bound	I
to	O
the	O
right-hand	O
side	O
of	O
equation	O
(	O
2.6	O
)	O
yields	O
dm	O
(	O
{	O
s|x	O
:	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
hs	O
)	O
>	O
	O
}	O
)	O
≤	O
(	O
cid:88	O
)	O
h∈hb	O
dm	O
(	O
{	O
s|x	O
:	O
ls	O
(	O
h	O
)	O
=	O
0	O
}	O
)	O
.	O
(	O
2.7	O
)	O
next	O
,	O
let	O
us	O
bound	O
each	O
summand	O
of	O
the	O
right-hand	O
side	O
of	O
the	O
preceding	O
in-	O
equality	O
.	O
fix	O
some	O
“	O
bad	O
”	O
hypothesis	B
h	O
∈	O
hb	O
.	O
the	O
event	O
ls	O
(	O
h	O
)	O
=	O
0	O
is	O
equivalent	O
40	O
a	O
gentle	O
start	O
to	O
the	O
event	O
∀i	O
,	O
h	O
(	O
xi	O
)	O
=	O
f	O
(	O
xi	O
)	O
.	O
since	O
the	O
examples	O
in	O
the	O
training	B
set	I
are	O
sampled	O
i.i.d	O
.	O
we	O
get	O
that	O
dm	O
(	O
{	O
s|x	O
:	O
ls	O
(	O
h	O
)	O
=	O
0	O
}	O
)	O
=	O
dm	O
(	O
{	O
s|x	O
:	O
∀i	O
,	O
h	O
(	O
xi	O
)	O
=	O
f	O
(	O
xi	O
)	O
}	O
)	O
d	O
(	O
{	O
xi	O
:	O
h	O
(	O
xi	O
)	O
=	O
f	O
(	O
xi	O
)	O
}	O
)	O
.	O
m	O
(	O
cid:89	O
)	O
=	O
(	O
2.8	O
)	O
i=1	O
for	O
each	O
individual	O
sampling	O
of	O
an	O
element	O
of	O
the	O
training	B
set	I
we	O
have	O
d	O
(	O
{	O
xi	O
:	O
h	O
(	O
xi	O
)	O
=	O
yi	O
}	O
)	O
=	O
1	O
−	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
h	O
)	O
≤	O
1	O
−	O
	O
,	O
where	O
the	O
last	O
inequality	O
follows	O
from	O
the	O
fact	O
that	O
h	O
∈	O
hb	O
.	O
combining	O
the	O
previous	O
equation	O
with	O
equation	O
(	O
2.8	O
)	O
and	O
using	O
the	O
inequality	O
1	O
−	O
	O
≤	O
e−	O
we	O
obtain	O
that	O
for	O
every	O
h	O
∈	O
hb	O
,	O
dm	O
(	O
{	O
s|x	O
:	O
ls	O
(	O
h	O
)	O
=	O
0	O
}	O
)	O
≤	O
(	O
1	O
−	O
	O
)	O
m	O
≤	O
e−m	O
.	O
(	O
2.9	O
)	O
combining	O
this	O
equation	O
with	O
equation	O
(	O
2.7	O
)	O
we	O
conclude	O
that	O
dm	O
(	O
{	O
s|x	O
:	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
hs	O
)	O
>	O
	O
}	O
)	O
≤	O
|hb|	O
e−m	O
≤	O
|h|	O
e−	O
m.	O
a	O
graphical	O
illustration	O
which	O
explains	O
how	O
we	O
used	O
the	O
union	B
bound	I
is	O
given	O
in	O
figure	O
2.1.	O
figure	O
2.1	O
each	O
point	O
in	O
the	O
large	O
circle	O
represents	O
a	O
possible	O
m-tuple	O
of	O
instances	O
.	O
each	O
colored	O
oval	O
represents	O
the	O
set	B
of	O
“	O
misleading	O
”	O
m-tuple	O
of	O
instances	O
for	O
some	O
“	O
bad	O
”	O
predictor	B
h	O
∈	O
hb	O
.	O
the	O
erm	O
can	O
potentially	O
overﬁt	O
whenever	O
it	O
gets	O
a	O
misleading	O
training	B
set	I
s.	O
that	O
is	O
,	O
for	O
some	O
h	O
∈	O
hb	O
we	O
have	O
ls	O
(	O
h	O
)	O
=	O
0.	O
equation	O
(	O
2.9	O
)	O
guarantees	O
that	O
for	O
each	O
individual	O
bad	O
hypothesis	B
,	O
h	O
∈	O
hb	O
,	O
at	O
most	O
(	O
1	O
−	O
	O
)	O
m-fraction	O
of	O
the	O
training	O
sets	O
would	O
be	O
misleading	O
.	O
in	O
particular	O
,	O
the	O
larger	O
m	O
is	O
,	O
the	O
smaller	O
each	O
of	O
these	O
colored	O
ovals	O
becomes	O
.	O
the	O
union	B
bound	I
formalizes	O
the	O
fact	O
that	O
the	O
area	O
representing	O
the	O
training	O
sets	O
that	O
are	O
misleading	O
with	O
respect	O
to	O
some	O
h	O
∈	O
hb	O
(	O
that	O
is	O
,	O
the	O
training	O
sets	O
in	O
m	O
)	O
is	O
at	O
most	O
the	O
sum	O
of	O
the	O
areas	O
of	O
the	O
colored	O
ovals	O
.	O
therefore	O
,	O
it	O
is	O
bounded	O
by	O
|hb|	O
times	O
the	O
maximum	O
size	O
of	O
a	O
colored	O
oval	O
.	O
any	O
sample	O
s	O
outside	O
the	O
colored	O
ovals	O
can	O
not	O
cause	O
the	O
erm	O
rule	O
to	O
overﬁt	O
.	O
corollary	O
2.3	O
let	O
h	O
be	O
a	O
ﬁnite	O
hypothesis	B
class	I
.	O
let	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
and	O
	O
>	O
0	O
and	O
let	O
m	O
be	O
an	O
integer	O
that	O
satisﬁes	O
m	O
≥	O
log	O
(	O
|h|/δ	O
)	O
	O
2.4	O
exercises	O
41	O
.	O
then	O
,	O
for	O
any	O
labeling	O
function	B
,	O
f	O
,	O
and	O
for	O
any	O
distribution	O
,	O
d	O
,	O
for	O
which	O
the	O
realizability	B
assumption	O
holds	O
(	O
that	O
is	O
,	O
for	O
some	O
h	O
∈	O
h	O
,	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
h	O
)	O
=	O
0	O
)	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
the	O
choice	O
of	O
an	O
i.i.d	O
.	O
sample	O
s	O
of	O
size	O
m	O
,	O
we	O
have	O
that	O
for	O
every	O
erm	O
hypothesis	B
,	O
hs	O
,	O
it	O
holds	O
that	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
hs	O
)	O
≤	O
	O
.	O
the	O
preceeding	O
corollary	O
tells	O
us	O
that	O
for	O
a	O
suﬃciently	O
large	O
m	O
,	O
the	O
ermh	O
rule	O
over	O
a	O
ﬁnite	O
hypothesis	B
class	I
will	O
be	O
probably	O
(	O
with	O
conﬁdence	B
1−δ	O
)	O
approximately	O
(	O
up	O
to	O
an	O
error	O
of	O
	O
)	O
correct	O
.	O
in	O
the	O
next	O
chapter	O
we	O
formally	O
deﬁne	O
the	O
model	O
of	O
probably	O
approximately	O
correct	O
(	O
pac	O
)	O
learning	O
.	O
2.4	O
exercises	O
1.	O
overﬁtting	B
of	O
polynomial	O
matching	O
:	O
we	O
have	O
shown	O
that	O
the	O
predictor	B
deﬁned	O
in	O
equation	O
(	O
2.3	O
)	O
leads	O
to	O
overﬁtting	B
.	O
while	O
this	O
predictor	B
seems	O
to	O
be	O
very	O
unnatural	O
,	O
the	O
goal	O
of	O
this	O
exercise	O
is	O
to	O
show	O
that	O
it	O
can	O
be	O
described	O
as	O
a	O
thresholded	O
polynomial	O
.	O
that	O
is	O
,	O
show	O
that	O
given	O
a	O
training	B
set	I
s	O
=	O
{	O
(	O
xi	O
,	O
f	O
(	O
xi	O
)	O
)	O
}	O
m	O
i=1	O
⊆	O
(	O
rd	O
×	O
{	O
0	O
,	O
1	O
}	O
)	O
m	O
,	O
there	O
exists	O
a	O
polynomial	O
ps	O
such	O
that	O
hs	O
(	O
x	O
)	O
=	O
1	O
if	O
and	O
only	O
if	O
ps	O
(	O
x	O
)	O
≥	O
0	O
,	O
where	O
hs	O
is	O
as	O
deﬁned	O
in	O
equation	O
(	O
2.3	O
)	O
.	O
it	O
follows	O
that	O
learning	O
the	O
class	O
of	O
all	O
thresholded	O
polynomials	O
using	O
the	O
erm	O
rule	O
may	O
lead	O
to	O
overﬁtting	B
.	O
2.	O
let	O
h	O
be	O
a	O
class	O
of	O
binary	O
classiﬁers	O
over	O
a	O
domain	B
x	O
.	O
let	O
d	O
be	O
an	O
unknown	O
distribution	O
over	O
x	O
,	O
and	O
let	O
f	O
be	O
the	O
target	O
hypothesis	O
in	O
h.	O
fix	O
some	O
h	O
∈	O
h.	O
show	O
that	O
the	O
expected	O
value	O
of	O
ls	O
(	O
h	O
)	O
over	O
the	O
choice	O
of	O
s|x	O
equals	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
h	O
)	O
,	O
namely	O
,	O
e	O
s|x∼dm	O
[	O
ls	O
(	O
h	O
)	O
]	O
=	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
h	O
)	O
.	O
3.	O
axis	O
aligned	O
rectangles	O
:	O
an	O
axis	O
aligned	O
rectangle	O
classiﬁer	B
in	O
the	O
plane	O
is	O
a	O
classiﬁer	B
that	O
assigns	O
the	O
value	O
1	O
to	O
a	O
point	O
if	O
and	O
only	O
if	O
it	O
is	O
inside	O
a	O
certain	O
rectangle	O
.	O
formally	O
,	O
given	O
real	O
numbers	O
a1	O
≤	O
b1	O
,	O
a2	O
≤	O
b2	O
,	O
deﬁne	O
the	O
classiﬁer	B
h	O
(	O
a1	O
,	O
b1	O
,	O
a2	O
,	O
b2	O
)	O
by	O
h	O
(	O
a1	O
,	O
b1	O
,	O
a2	O
,	O
b2	O
)	O
(	O
x1	O
,	O
x2	O
)	O
=	O
if	O
a1	O
≤	O
x1	O
≤	O
b1	O
and	O
a2	O
≤	O
x2	O
≤	O
b2	O
otherwise	O
.	O
1	O
0	O
(	O
2.10	O
)	O
(	O
cid:40	O
)	O
the	O
class	O
of	O
all	O
axis	O
aligned	O
rectangles	O
in	O
the	O
plane	O
is	O
deﬁned	O
as	O
h2	O
rec	O
=	O
{	O
h	O
(	O
a1	O
,	O
b1	O
,	O
a2	O
,	O
b2	O
)	O
:	O
a1	O
≤	O
b1	O
,	O
and	O
a2	O
≤	O
b2	O
}	O
.	O
note	O
that	O
this	O
is	O
an	O
inﬁnite	O
size	O
hypothesis	B
class	I
.	O
throughout	O
this	O
exercise	O
we	O
rely	O
on	O
the	O
realizability	B
assumption	O
.	O
42	O
a	O
gentle	O
start	O
then	O
,	O
with	O
proba-	O
1.	O
let	O
a	O
be	O
the	O
algorithm	O
that	O
returns	O
the	O
smallest	O
rectangle	O
enclosing	O
all	O
2.	O
show	O
that	O
if	O
a	O
receives	O
a	O
training	B
set	I
of	O
size	O
≥	O
4	O
log	O
(	O
4/δ	O
)	O
positive	O
examples	O
in	O
the	O
training	B
set	I
.	O
show	O
that	O
a	O
is	O
an	O
erm	O
.	O
bility	O
of	O
at	O
least	O
1	O
−	O
δ	O
it	O
returns	O
a	O
hypothesis	B
with	O
error	O
of	O
at	O
most	O
	O
.	O
hint	O
:	O
fix	O
some	O
distribution	O
d	O
over	O
x	O
,	O
let	O
r∗	O
=	O
r	O
(	O
a∗	O
2	O
)	O
be	O
the	O
rect-	O
angle	O
that	O
generates	O
the	O
labels	O
,	O
and	O
let	O
f	O
be	O
the	O
corresponding	O
hypothesis	B
.	O
let	O
a1	O
≥	O
a∗	O
1	O
be	O
a	O
number	O
such	O
that	O
the	O
probability	O
mass	O
(	O
with	O
respect	O
to	O
d	O
)	O
of	O
the	O
rectangle	O
r1	O
=	O
r	O
(	O
a∗	O
2	O
)	O
is	O
exactly	O
/4	O
.	O
similarly	O
,	O
let	O
b1	O
,	O
a2	O
,	O
b2	O
be	O
numbers	O
such	O
that	O
the	O
probability	O
masses	O
of	O
the	O
rectangles	O
r2	O
=	O
r	O
(	O
b1	O
,	O
b∗	O
2	O
)	O
,	O
r3	O
=	O
r	O
(	O
a∗	O
2	O
)	O
are	O
all	O
exactly	O
/4	O
.	O
let	O
r	O
(	O
s	O
)	O
be	O
the	O
rectangle	O
returned	O
by	O
a.	O
see	O
illustration	O
in	O
figure	O
2.2	O
.	O
2	O
,	O
b∗	O
2	O
,	O
a2	O
)	O
,	O
r4	O
=	O
r	O
(	O
a∗	O
1	O
,	O
a1	O
,	O
a∗	O
1	O
,	O
b∗	O
1	O
,	O
a∗	O
1	O
,	O
b∗	O
1	O
,	O
a∗	O
2	O
,	O
b∗	O
1	O
,	O
a∗	O
2	O
,	O
b∗	O
	O
1	O
,	O
b∗	O
1	O
,	O
b2	O
,	O
b∗	O
-	O
+	O
r∗	O
+	O
r	O
(	O
s	O
)	O
+	O
-	O
+	O
r1	O
+	O
figure	O
2.2	O
axis	O
aligned	O
rectangles	O
.	O
•	O
show	O
that	O
r	O
(	O
s	O
)	O
⊆	O
r∗	O
.	O
•	O
show	O
that	O
if	O
s	O
contains	O
(	O
positive	O
)	O
examples	O
in	O
all	O
of	O
the	O
rectangles	O
r1	O
,	O
r2	O
,	O
r3	O
,	O
r4	O
,	O
then	O
the	O
hypothesis	B
returned	O
by	O
a	O
has	O
error	O
of	O
at	O
most	O
	O
.	O
•	O
for	O
each	O
i	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
4	O
}	O
,	O
upper	O
bound	O
the	O
probability	O
that	O
s	O
does	O
not	O
•	O
use	O
the	O
union	B
bound	I
to	O
conclude	O
the	O
argument	O
.	O
contain	O
an	O
example	O
from	O
ri	O
.	O
3.	O
repeat	O
the	O
previous	O
question	O
for	O
the	O
class	O
of	O
axis	O
aligned	O
rectangles	O
in	O
rd	O
.	O
4.	O
show	O
that	O
the	O
runtime	O
of	O
applying	O
the	O
algorithm	O
a	O
mentioned	O
earlier	O
is	O
polynomial	O
in	O
d	O
,	O
1/	O
,	O
and	O
in	O
log	O
(	O
1/δ	O
)	O
.	O
3	O
a	O
formal	O
learning	O
model	O
in	O
this	O
chapter	O
we	O
deﬁne	O
our	O
main	O
formal	O
learning	O
model	O
–	O
the	O
pac	O
learning	O
model	O
and	O
its	O
extensions	O
.	O
we	O
will	O
consider	O
other	O
notions	O
of	O
learnability	O
in	O
chap-	O
ter	O
7	O
.	O
3.1	O
pac	O
learning	O
in	O
the	O
previous	O
chapter	O
we	O
have	O
shown	O
that	O
for	O
a	O
ﬁnite	O
hypothesis	B
class	I
,	O
if	O
the	O
erm	O
rule	O
with	O
respect	O
to	O
that	O
class	O
is	O
applied	O
on	O
a	O
suﬃciently	O
large	O
training	O
sample	O
(	O
whose	O
size	O
is	O
independent	O
of	O
the	O
underlying	O
distribution	O
or	O
labeling	O
function	B
)	O
then	O
the	O
output	O
hypothesis	B
will	O
be	O
probably	O
approximately	O
correct	O
.	O
more	O
generally	O
,	O
we	O
now	O
deﬁne	O
probably	O
approximately	O
correct	O
(	O
pac	O
)	O
learning	O
.	O
definition	O
3.1	O
(	O
pac	O
learnability	O
)	O
a	O
hypothesis	B
class	I
h	O
is	O
pac	O
learnable	O
if	O
there	O
exist	O
a	O
function	B
mh	O
:	O
(	O
0	O
,	O
1	O
)	O
2	O
→	O
n	O
and	O
a	O
learning	O
algorithm	O
with	O
the	O
following	O
property	O
:	O
for	O
every	O
	O
,	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
for	O
every	O
distribution	O
d	O
over	O
x	O
,	O
and	O
for	O
every	O
labeling	O
function	B
f	O
:	O
x	O
→	O
{	O
0	O
,	O
1	O
}	O
,	O
if	O
the	O
realizable	O
assumption	O
holds	O
with	O
respect	O
to	O
h	O
,	O
d	O
,	O
f	O
,	O
then	O
when	O
running	O
the	O
learning	O
algorithm	O
on	O
m	O
≥	O
mh	O
(	O
	O
,	O
δ	O
)	O
i.i.d	O
.	O
examples	O
generated	O
by	O
d	O
and	O
labeled	O
by	O
f	O
,	O
the	O
algorithm	O
returns	O
a	O
hypothesis	B
h	O
such	O
that	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
(	O
over	O
the	O
choice	O
of	O
the	O
examples	O
)	O
,	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
h	O
)	O
≤	O
	O
.	O
the	O
deﬁnition	O
of	O
probably	O
approximately	O
correct	O
learnability	O
contains	O
two	O
approximation	O
parameters	O
.	O
the	O
accuracy	B
parameter	O
	O
determines	O
how	O
far	O
the	O
output	O
classiﬁer	B
can	O
be	O
from	O
the	O
optimal	O
one	O
(	O
this	O
corresponds	O
to	O
the	O
“	O
approx-	O
imately	O
correct	O
”	O
)	O
,	O
and	O
a	O
conﬁdence	B
parameter	O
δ	O
indicating	O
how	O
likely	O
the	O
clas-	O
siﬁer	O
is	O
to	O
meet	O
that	O
accuracy	B
requirement	O
(	O
corresponds	O
to	O
the	O
“	O
probably	O
”	O
part	O
of	O
“	O
pac	O
”	O
)	O
.	O
under	O
the	O
data	O
access	O
model	O
that	O
we	O
are	O
investigating	O
,	O
these	O
ap-	O
proximations	O
are	O
inevitable	O
.	O
since	O
the	O
training	B
set	I
is	O
randomly	O
generated	O
,	O
there	O
may	O
always	O
be	O
a	O
small	O
chance	O
that	O
it	O
will	O
happen	O
to	O
be	O
noninformative	O
(	O
for	O
ex-	O
ample	O
,	O
there	O
is	O
always	O
some	O
chance	O
that	O
the	O
training	B
set	I
will	O
contain	O
only	O
one	O
domain	B
point	O
,	O
sampled	O
over	O
and	O
over	O
again	O
)	O
.	O
furthermore	O
,	O
even	O
when	O
we	O
are	O
lucky	O
enough	O
to	O
get	O
a	O
training	O
sample	O
that	O
does	O
faithfully	O
represent	O
d	O
,	O
because	O
it	O
is	O
just	O
a	O
ﬁnite	O
sample	O
,	O
there	O
may	O
always	O
be	O
some	O
ﬁne	O
details	O
of	O
d	O
that	O
it	O
fails	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
44	O
a	O
formal	O
learning	O
model	O
to	O
reﬂect	O
.	O
our	O
accuracy	B
parameter	O
,	O
	O
,	O
allows	O
“	O
forgiving	O
”	O
the	O
learner	O
’	O
s	O
classiﬁer	B
for	O
making	O
minor	O
errors	O
.	O
sample	B
complexity	I
the	O
function	B
mh	O
:	O
(	O
0	O
,	O
1	O
)	O
2	O
→	O
n	O
determines	O
the	O
sample	B
complexity	I
of	O
learning	O
h	O
:	O
that	O
is	O
,	O
how	O
many	O
examples	O
are	O
required	O
to	O
guarantee	O
a	O
probably	O
approximately	O
correct	O
solution	O
.	O
the	O
sample	B
complexity	I
is	O
a	O
function	B
of	O
the	O
accuracy	B
(	O
	O
)	O
and	O
conﬁdence	B
(	O
δ	O
)	O
parameters	O
.	O
it	O
also	O
depends	O
on	O
properties	O
of	O
the	O
hypothesis	B
class	I
h	O
–	O
for	O
example	O
,	O
for	O
a	O
ﬁnite	O
class	O
we	O
showed	O
that	O
the	O
sample	B
complexity	I
depends	O
on	O
log	O
the	O
size	O
of	O
h.	O
note	O
that	O
if	O
h	O
is	O
pac	O
learnable	O
,	O
there	O
are	O
many	O
functions	O
mh	O
that	O
satisfy	O
the	O
requirements	O
given	O
in	O
the	O
deﬁnition	O
of	O
pac	O
learnability	O
.	O
therefore	O
,	O
to	O
be	O
precise	O
,	O
we	O
will	O
deﬁne	O
the	O
sample	B
complexity	I
of	O
learning	O
h	O
to	O
be	O
the	O
“	O
minimal	O
function	B
,	O
”	O
in	O
the	O
sense	O
that	O
for	O
any	O
	O
,	O
δ	O
,	O
mh	O
(	O
	O
,	O
δ	O
)	O
is	O
the	O
minimal	O
integer	O
that	O
satisﬁes	O
the	O
requirements	O
of	O
pac	O
learning	O
with	O
accuracy	B
	O
and	O
conﬁdence	B
δ.	O
let	O
us	O
now	O
recall	B
the	O
conclusion	O
of	O
the	O
analysis	O
of	O
ﬁnite	O
hypothesis	B
classes	O
from	O
the	O
previous	O
chapter	O
.	O
it	O
can	O
be	O
rephrased	O
as	O
stating	O
:	O
corollary	O
3.2	O
every	O
ﬁnite	O
hypothesis	B
class	I
is	O
pac	O
learnable	O
with	O
sample	B
complexity	I
(	O
cid:24	O
)	O
log	O
(	O
|h|/δ	O
)	O
(	O
cid:25	O
)	O
mh	O
(	O
	O
,	O
δ	O
)	O
≤	O
.	O
	O
there	O
are	O
inﬁnite	O
classes	O
that	O
are	O
learnable	O
as	O
well	O
(	O
see	O
,	O
for	O
example	O
,	O
exer-	O
cise	O
3	O
)	O
.	O
later	O
on	O
we	O
will	O
show	O
that	O
what	O
determines	O
the	O
pac	O
learnability	O
of	O
a	O
class	O
is	O
not	O
its	O
ﬁniteness	O
but	O
rather	O
a	O
combinatorial	O
measure	O
called	O
the	O
vc	O
dimension	B
.	O
3.2	O
a	O
more	O
general	O
learning	O
model	O
the	O
model	O
we	O
have	O
just	O
described	O
can	O
be	O
readily	O
generalized	O
,	O
so	O
that	O
it	O
can	O
be	O
made	O
relevant	O
to	O
a	O
wider	O
scope	O
of	O
learning	O
tasks	O
.	O
we	O
consider	O
generalizations	O
in	O
two	O
aspects	O
:	O
removing	O
the	O
realizability	B
assumption	O
we	O
have	O
required	O
that	O
the	O
learning	O
algorithm	O
succeeds	O
on	O
a	O
pair	O
of	O
data	O
distri-	O
bution	O
d	O
and	O
labeling	O
function	B
f	O
provided	O
that	O
the	O
realizability	B
assumption	O
is	O
met	O
.	O
for	O
practical	O
learning	O
tasks	O
,	O
this	O
assumption	O
may	O
be	O
too	O
strong	O
(	O
can	O
we	O
really	O
guarantee	O
that	O
there	O
is	O
a	O
rectangle	O
in	O
the	O
color-hardness	O
space	O
that	O
fully	O
determines	O
which	O
papayas	O
are	O
tasty	O
?	O
)	O
.	O
in	O
the	O
next	O
subsection	O
,	O
we	O
will	O
describe	O
the	O
agnostic	O
pac	O
model	O
in	O
which	O
this	O
realizability	B
assumption	O
is	O
waived	O
.	O
3.2	O
a	O
more	O
general	O
learning	O
model	O
45	O
learning	O
problems	O
beyond	O
binary	O
classiﬁcation	O
the	O
learning	O
task	O
that	O
we	O
have	O
been	O
discussing	O
so	O
far	O
has	O
to	O
do	O
with	O
predicting	O
a	O
binary	O
label	B
to	O
a	O
given	O
example	O
(	O
like	O
being	O
tasty	O
or	O
not	O
)	O
.	O
however	O
,	O
many	O
learning	O
tasks	O
take	O
a	O
diﬀerent	O
form	O
.	O
for	O
example	O
,	O
one	O
may	O
wish	O
to	O
predict	O
a	O
real	O
valued	O
number	O
(	O
say	O
,	O
the	O
temperature	O
at	O
9:00	O
p.m.	O
tomorrow	O
)	O
or	O
a	O
label	B
picked	O
from	O
a	O
ﬁnite	O
set	B
of	O
labels	O
(	O
like	O
the	O
topic	O
of	O
the	O
main	O
story	O
in	O
tomorrow	O
’	O
s	O
paper	O
)	O
.	O
it	O
turns	O
out	O
that	O
our	O
analysis	O
of	O
learning	O
can	O
be	O
readily	O
extended	O
to	O
such	O
and	O
many	O
other	O
scenarios	O
by	O
allowing	O
a	O
variety	O
of	O
loss	B
functions	O
.	O
we	O
shall	O
discuss	O
that	O
in	O
section	O
3.2.2	O
later	O
.	O
3.2.1	O
releasing	O
the	O
realizability	B
assumption	O
–	O
agnostic	O
pac	O
learning	O
a	O
more	O
realistic	O
model	O
for	O
the	O
data-generating	O
distribution	O
recall	O
that	O
the	O
realizability	B
assumption	O
requires	O
that	O
there	O
exists	O
h	O
(	O
cid:63	O
)	O
∈	O
h	O
such	O
that	O
px∼d	O
[	O
h	O
(	O
cid:63	O
)	O
(	O
x	O
)	O
=	O
f	O
(	O
x	O
)	O
]	O
=	O
1.	O
in	O
many	O
practical	O
problems	O
this	O
assumption	O
does	O
not	O
hold	O
.	O
furthermore	O
,	O
it	O
is	O
maybe	O
more	O
realistic	O
not	O
to	O
assume	O
that	O
the	O
labels	O
are	O
fully	O
determined	O
by	O
the	O
features	O
we	O
measure	O
on	O
input	O
elements	O
(	O
in	O
the	O
case	O
of	O
the	O
papayas	O
,	O
it	O
is	O
plausible	O
that	O
two	O
papayas	O
of	O
the	O
same	O
color	O
and	O
softness	O
will	O
have	O
diﬀerent	O
taste	O
)	O
.	O
in	O
the	O
following	O
,	O
we	O
relax	O
the	O
realizability	B
assumption	O
by	O
replacing	O
the	O
“	O
target	O
labeling	O
function	B
”	O
with	O
a	O
more	O
ﬂexible	O
notion	O
,	O
a	O
data-labels	O
generating	O
distribution	O
.	O
formally	O
,	O
from	O
now	O
on	O
,	O
let	O
d	O
be	O
a	O
probability	O
distribution	O
over	O
x	O
×y	O
,	O
where	O
,	O
as	O
before	O
,	O
x	O
is	O
our	O
domain	B
set	O
and	O
y	O
is	O
a	O
set	B
of	O
labels	O
(	O
usually	O
we	O
will	O
consider	O
y	O
=	O
{	O
0	O
,	O
1	O
}	O
)	O
.	O
that	O
is	O
,	O
d	O
is	O
a	O
joint	O
distribution	O
over	O
domain	B
points	O
and	O
labels	O
.	O
one	O
can	O
view	O
such	O
a	O
distribution	O
as	O
being	O
composed	O
of	O
two	O
parts	O
:	O
a	O
distribution	O
dx	O
over	O
unlabeled	O
domain	B
points	O
(	O
sometimes	O
called	O
the	O
marginal	O
distribution	O
)	O
and	O
a	O
conditional	O
probability	O
over	O
labels	O
for	O
each	O
domain	B
point	O
,	O
d	O
(	O
(	O
x	O
,	O
y	O
)	O
|x	O
)	O
.	O
in	O
the	O
papaya	O
example	O
,	O
dx	O
determines	O
the	O
probability	O
of	O
encountering	O
a	O
papaya	O
whose	O
color	O
and	O
hardness	O
fall	O
in	O
some	O
color-hardness	O
values	O
domain	B
,	O
and	O
the	O
conditional	O
probability	O
is	O
the	O
probability	O
that	O
a	O
papaya	O
with	O
color	O
and	O
hardness	O
represented	O
by	O
x	O
is	O
tasty	O
.	O
indeed	O
,	O
such	O
modeling	O
allows	O
for	O
two	O
papayas	O
that	O
share	O
the	O
same	O
color	O
and	O
hardness	O
to	O
belong	O
to	O
diﬀerent	O
taste	O
categories	O
.	O
the	O
empirical	O
and	O
the	O
true	B
error	I
revised	O
for	O
a	O
probability	O
distribution	O
,	O
d	O
,	O
over	O
x	O
×	O
y	O
,	O
one	O
can	O
measure	O
how	O
likely	O
h	O
is	O
to	O
make	O
an	O
error	O
when	O
labeled	O
points	O
are	O
randomly	O
drawn	O
according	O
to	O
d.	O
we	O
redeﬁne	O
the	O
true	B
error	I
(	O
or	O
risk	B
)	O
of	O
a	O
prediction	O
rule	O
h	O
to	O
be	O
ld	O
(	O
h	O
)	O
def=	O
p	O
(	O
x	O
,	O
y	O
)	O
∼d	O
[	O
h	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
y	O
]	O
def=	O
d	O
(	O
{	O
(	O
x	O
,	O
y	O
)	O
:	O
h	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
y	O
}	O
)	O
.	O
(	O
3.1	O
)	O
we	O
would	O
like	O
to	O
ﬁnd	O
a	O
predictor	B
,	O
h	O
,	O
for	O
which	O
that	O
error	O
will	O
be	O
minimized	O
.	O
however	O
,	O
the	O
learner	O
does	O
not	O
know	O
the	O
data	O
generating	O
d.	O
what	O
the	O
learner	O
does	O
have	O
access	O
to	O
is	O
the	O
training	O
data	O
,	O
s.	O
the	O
deﬁnition	O
of	O
the	O
empirical	B
risk	I
46	O
a	O
formal	O
learning	O
model	O
remains	O
the	O
same	O
as	O
before	O
,	O
namely	O
,	O
ls	O
(	O
h	O
)	O
def=	O
|	O
{	O
i	O
∈	O
[	O
m	O
]	O
:	O
h	O
(	O
xi	O
)	O
(	O
cid:54	O
)	O
=	O
yi	O
}	O
|	O
.	O
m	O
given	O
s	O
,	O
a	O
learner	O
can	O
compute	O
ls	O
(	O
h	O
)	O
for	O
any	O
function	B
h	O
:	O
x	O
→	O
{	O
0	O
,	O
1	O
}	O
.	O
note	O
that	O
ls	O
(	O
h	O
)	O
=	O
ld	O
(	O
uniform	O
over	O
s	O
)	O
(	O
h	O
)	O
.	O
the	O
goal	O
we	O
wish	O
to	O
ﬁnd	O
some	O
hypothesis	B
,	O
h	O
:	O
x	O
→	O
y	O
,	O
that	O
(	O
probably	O
approximately	O
)	O
minimizes	O
the	O
true	O
risk	O
,	O
ld	O
(	O
h	O
)	O
.	O
the	O
bayes	O
optimal	O
predictor	B
.	O
given	O
any	O
probability	O
distribution	O
d	O
over	O
x	O
×	O
{	O
0	O
,	O
1	O
}	O
,	O
the	O
best	O
label	B
predicting	O
function	B
from	O
x	O
to	O
{	O
0	O
,	O
1	O
}	O
will	O
be	O
(	O
cid:40	O
)	O
1	O
0	O
fd	O
(	O
x	O
)	O
=	O
if	O
p	O
[	O
y	O
=	O
1|x	O
]	O
≥	O
1/2	O
otherwise	O
it	O
is	O
easy	O
to	O
verify	O
(	O
see	O
exercise	O
7	O
)	O
that	O
for	O
every	O
probability	O
distribution	O
d	O
,	O
the	O
bayes	O
optimal	O
predictor	B
fd	O
is	O
optimal	O
,	O
in	O
the	O
sense	O
that	O
no	O
other	O
classiﬁer	B
,	O
g	O
:	O
x	O
→	O
{	O
0	O
,	O
1	O
}	O
has	O
a	O
lower	O
error	O
.	O
that	O
is	O
,	O
for	O
every	O
classiﬁer	B
g	O
,	O
ld	O
(	O
fd	O
)	O
≤	O
ld	O
(	O
g	O
)	O
.	O
unfortunately	O
,	O
since	O
we	O
do	O
not	O
know	O
d	O
,	O
we	O
can	O
not	O
utilize	O
this	O
optimal	O
predictor	B
fd	O
.	O
what	O
the	O
learner	O
does	O
have	O
access	O
to	O
is	O
the	O
training	O
sample	O
.	O
we	O
can	O
now	O
present	O
the	O
formal	O
deﬁnition	O
of	O
agnostic	O
pac	O
learnability	O
,	O
which	O
is	O
a	O
natural	O
extension	O
of	O
the	O
deﬁnition	O
of	O
pac	O
learnability	O
to	O
the	O
more	O
realistic	O
,	O
nonrealizable	O
,	O
learning	O
setup	O
we	O
have	O
just	O
discussed	O
.	O
clearly	O
,	O
we	O
can	O
not	O
hope	O
that	O
the	O
learning	O
algorithm	O
will	O
ﬁnd	O
a	O
hypothesis	B
whose	O
error	O
is	O
smaller	O
than	O
the	O
minimal	O
possible	O
error	O
,	O
that	O
of	O
the	O
bayes	O
predic-	O
tor	O
.	O
furthermore	O
,	O
as	O
we	O
shall	O
prove	O
later	O
,	O
once	O
we	O
make	O
no	O
prior	O
assumptions	O
about	O
the	O
data-generating	O
distribution	O
,	O
no	O
algorithm	O
can	O
be	O
guaranteed	O
to	O
ﬁnd	O
a	O
predictor	B
that	O
is	O
as	O
good	O
as	O
the	O
bayes	O
optimal	O
one	O
.	O
instead	O
,	O
we	O
require	O
that	O
the	O
learning	O
algorithm	O
will	O
ﬁnd	O
a	O
predictor	B
whose	O
error	O
is	O
not	O
much	O
larger	O
than	O
the	O
best	O
possible	O
error	O
of	O
a	O
predictor	B
in	O
some	O
given	O
benchmark	O
hypothesis	B
class	I
.	O
of	O
course	O
,	O
the	O
strength	O
of	O
such	O
a	O
requirement	O
depends	O
on	O
the	O
choice	O
of	O
that	O
hypothesis	B
class	I
.	O
definition	O
3.3	O
(	O
agnostic	O
pac	O
learnability	O
)	O
a	O
hypothesis	B
class	I
h	O
is	O
agnostic	O
pac	O
learnable	O
if	O
there	O
exist	O
a	O
function	B
mh	O
:	O
(	O
0	O
,	O
1	O
)	O
2	O
→	O
n	O
and	O
a	O
learning	O
algorithm	O
with	O
the	O
following	O
property	O
:	O
for	O
every	O
	O
,	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
and	O
for	O
every	O
distribution	O
d	O
over	O
x	O
×y	O
,	O
when	O
running	O
the	O
learning	O
algorithm	O
on	O
m	O
≥	O
mh	O
(	O
	O
,	O
δ	O
)	O
i.i.d	O
.	O
examples	O
generated	O
by	O
d	O
,	O
the	O
algorithm	O
returns	O
a	O
hypothesis	B
h	O
such	O
that	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
(	O
over	O
the	O
choice	O
of	O
the	O
m	O
training	O
examples	O
)	O
,	O
ld	O
(	O
h	O
)	O
≤	O
min	O
h	O
(	O
cid:48	O
)	O
∈h	O
ld	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
+	O
	O
.	O
3.2	O
a	O
more	O
general	O
learning	O
model	O
47	O
clearly	O
,	O
if	O
the	O
realizability	B
assumption	O
holds	O
,	O
agnostic	O
pac	O
learning	O
provides	O
the	O
same	O
guarantee	O
as	O
pac	O
learning	O
.	O
in	O
that	O
sense	O
,	O
agnostic	O
pac	O
learning	O
gener-	O
alizes	O
the	O
deﬁnition	O
of	O
pac	O
learning	O
.	O
when	O
the	O
realizability	B
assumption	O
does	O
not	O
hold	O
,	O
no	O
learner	O
can	O
guarantee	O
an	O
arbitrarily	O
small	O
error	O
.	O
nevertheless	O
,	O
under	O
the	O
deﬁnition	O
of	O
agnostic	O
pac	O
learning	O
,	O
a	O
learner	O
can	O
still	O
declare	O
success	O
if	O
its	O
error	O
is	O
not	O
much	O
larger	O
than	O
the	O
best	O
error	O
achievable	O
by	O
a	O
predictor	B
from	O
the	O
class	O
h.	O
this	O
is	O
in	O
contrast	O
to	O
pac	O
learning	O
,	O
in	O
which	O
the	O
learner	O
is	O
required	O
to	O
achieve	O
a	O
small	O
error	O
in	O
absolute	O
terms	O
and	O
not	O
relative	O
to	O
the	O
best	O
error	O
achievable	O
by	O
the	O
hypothesis	B
class	I
.	O
3.2.2	O
the	O
scope	O
of	O
learning	O
problems	O
modeled	O
we	O
next	O
extend	O
our	O
model	O
so	O
that	O
it	O
can	O
be	O
applied	O
to	O
a	O
wide	O
variety	O
of	O
learning	O
tasks	O
.	O
let	O
us	O
consider	O
some	O
examples	O
of	O
diﬀerent	O
learning	O
tasks	O
.	O
•	O
multiclass	B
classiﬁcation	O
our	O
classiﬁcation	O
does	O
not	O
have	O
to	O
be	O
binary	O
.	O
take	O
,	O
for	O
example	O
,	O
the	O
task	O
of	O
document	O
classiﬁcation	O
:	O
we	O
wish	O
to	O
design	O
a	O
program	O
that	O
will	O
be	O
able	O
to	O
classify	O
given	O
documents	O
according	O
to	O
topics	O
(	O
e.g.	O
,	O
news	O
,	O
sports	O
,	O
biology	O
,	O
medicine	O
)	O
.	O
a	O
learning	O
algorithm	O
for	O
such	O
a	O
task	O
will	O
have	O
access	O
to	O
examples	O
of	O
correctly	O
classiﬁed	O
documents	O
and	O
,	O
on	O
the	O
basis	O
of	O
these	O
examples	O
,	O
should	O
output	O
a	O
program	O
that	O
can	O
take	O
as	O
input	O
a	O
new	O
document	O
and	O
output	O
a	O
topic	O
classiﬁcation	O
for	O
that	O
document	O
.	O
here	O
,	O
the	O
domain	B
set	O
is	O
the	O
set	B
of	O
all	O
potential	O
documents	O
.	O
once	O
again	O
,	O
we	O
would	O
usually	O
represent	O
documents	O
by	O
a	O
set	B
of	O
features	O
that	O
could	O
include	O
counts	O
of	O
diﬀerent	O
key	O
words	O
in	O
the	O
document	O
,	O
as	O
well	O
as	O
other	O
possibly	O
relevant	O
features	O
like	O
the	O
size	O
of	O
the	O
document	O
or	O
its	O
origin	O
.	O
the	O
label	B
set	O
in	O
this	O
task	O
will	O
be	O
the	O
set	B
of	O
possible	O
document	O
topics	O
(	O
so	O
y	O
will	O
be	O
some	O
large	O
ﬁnite	O
set	B
)	O
.	O
once	O
we	O
determine	O
our	O
domain	B
and	O
label	B
sets	O
,	O
the	O
other	O
components	O
of	O
our	O
framework	O
look	O
exactly	O
the	O
same	O
as	O
in	O
the	O
papaya	O
tasting	O
example	O
;	O
our	O
training	O
sample	O
will	O
be	O
a	O
ﬁnite	O
sequence	O
of	O
(	O
feature	B
vector	O
,	O
label	B
)	O
pairs	O
,	O
the	O
learner	O
’	O
s	O
output	O
will	O
be	O
a	O
function	B
from	O
the	O
domain	B
set	O
to	O
the	O
label	B
set	O
,	O
and	O
,	O
ﬁnally	O
,	O
for	O
our	O
measure	O
of	O
success	O
,	O
we	O
can	O
use	O
the	O
probability	O
,	O
over	O
(	O
document	O
,	O
topic	O
)	O
pairs	O
,	O
of	O
the	O
event	O
that	O
our	O
predictor	B
suggests	O
a	O
wrong	O
label	B
.	O
•	O
regression	B
in	O
this	O
task	O
,	O
one	O
wishes	O
to	O
ﬁnd	O
some	O
simple	O
pattern	O
in	O
the	O
data	O
–	O
a	O
functional	O
relationship	O
between	O
the	O
x	O
and	O
y	O
components	O
of	O
the	O
data	O
.	O
for	O
example	O
,	O
one	O
wishes	O
to	O
ﬁnd	O
a	O
linear	O
function	O
that	O
best	O
predicts	O
a	O
baby	O
’	O
s	O
birth	O
weight	O
on	O
the	O
basis	O
of	O
ultrasound	O
measures	O
of	O
his	O
head	O
circumference	O
,	O
abdominal	O
circumference	O
,	O
and	O
femur	O
length	O
.	O
here	O
,	O
our	O
domain	B
set	O
x	O
is	O
some	O
subset	O
of	O
r3	O
(	O
the	O
three	O
ultrasound	O
measurements	O
)	O
,	O
and	O
the	O
set	B
of	O
“	O
labels	O
,	O
”	O
y	O
,	O
is	O
the	O
the	O
set	B
of	O
real	O
numbers	O
(	O
the	O
weight	O
in	O
grams	O
)	O
.	O
in	O
this	O
context	O
,	O
it	O
is	O
more	O
adequate	O
to	O
call	O
y	O
the	O
target	B
set	I
.	O
our	O
training	O
data	O
as	O
well	O
as	O
the	O
learner	O
’	O
s	O
output	O
are	O
as	O
before	O
(	O
a	O
ﬁnite	O
sequence	O
of	O
(	O
x	O
,	O
y	O
)	O
pairs	O
,	O
and	O
a	O
function	B
from	O
x	O
to	O
y	O
respectively	O
)	O
.	O
however	O
,	O
our	O
measure	O
of	O
success	O
is	O
48	O
a	O
formal	O
learning	O
model	O
diﬀerent	O
.	O
we	O
may	O
evaluate	O
the	O
quality	O
of	O
a	O
hypothesis	B
function	O
,	O
h	O
:	O
x	O
→	O
y	O
,	O
by	O
the	O
expected	O
square	O
diﬀerence	O
between	O
the	O
true	O
labels	O
and	O
their	O
predicted	O
values	O
,	O
namely	O
,	O
ld	O
(	O
h	O
)	O
def=	O
(	O
x	O
,	O
y	O
)	O
∼d	O
(	O
h	O
(	O
x	O
)	O
−	O
y	O
)	O
2.	O
e	O
(	O
3.2	O
)	O
to	O
accommodate	O
a	O
wide	O
range	O
of	O
learning	O
tasks	O
we	O
generalize	O
our	O
formalism	O
of	O
the	O
measure	O
of	O
success	O
as	O
follows	O
:	O
generalized	O
loss	O
functions	O
given	O
any	O
set	B
h	O
(	O
that	O
plays	O
the	O
role	O
of	O
our	O
hypotheses	O
,	O
or	O
models	O
)	O
and	O
some	O
domain	B
z	O
let	O
(	O
cid:96	O
)	O
be	O
any	O
function	B
from	O
h×z	O
to	O
the	O
set	B
of	O
nonnegative	O
real	O
numbers	O
,	O
(	O
cid:96	O
)	O
:	O
h	O
×	O
z	O
→	O
r+	O
.	O
we	O
call	O
such	O
functions	O
loss	B
functions	O
.	O
note	O
that	O
for	O
prediction	O
problems	O
,	O
we	O
have	O
that	O
z	O
=	O
x	O
×	O
y.	O
however	O
,	O
our	O
notion	O
of	O
the	O
loss	B
function	I
is	O
generalized	O
beyond	O
prediction	O
tasks	O
,	O
and	O
therefore	O
it	O
allows	O
z	O
to	O
be	O
any	O
domain	B
of	I
examples	I
(	O
for	O
instance	B
,	O
in	O
unsupervised	B
learning	I
tasks	O
such	O
as	O
the	O
one	O
described	O
in	O
chapter	O
22	O
,	O
z	O
is	O
not	O
a	O
product	O
of	O
an	O
instance	B
domain	O
and	O
a	O
label	B
domain	O
)	O
.	O
we	O
now	O
deﬁne	O
the	O
risk	B
function	O
to	O
be	O
the	O
expected	O
loss	B
of	O
a	O
classiﬁer	B
,	O
h	O
∈	O
h	O
,	O
with	O
respect	O
to	O
a	O
probability	O
distribution	O
d	O
over	O
z	O
,	O
namely	O
,	O
ld	O
(	O
h	O
)	O
def=	O
e	O
z∼d	O
[	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
z	O
)	O
]	O
.	O
(	O
3.3	O
)	O
that	O
is	O
,	O
we	O
consider	O
the	O
expectation	O
of	O
the	O
loss	B
of	O
h	O
over	O
objects	O
z	O
picked	O
ran-	O
domly	O
according	O
to	O
d.	O
similarly	O
,	O
we	O
deﬁne	O
the	O
empirical	B
risk	I
to	O
be	O
the	O
expected	O
loss	B
over	O
a	O
given	O
sample	O
s	O
=	O
(	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
)	O
∈	O
z	O
m	O
,	O
namely	O
,	O
ls	O
(	O
h	O
)	O
def=	O
1	O
m	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
.	O
(	O
3.4	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
(	O
cid:40	O
)	O
the	O
loss	B
functions	O
used	O
in	O
the	O
preceding	O
examples	O
of	O
classiﬁcation	O
and	O
regres-	O
sion	O
tasks	O
are	O
as	O
follows	O
:	O
•	O
0–1	O
loss	B
:	O
here	O
,	O
our	O
random	O
variable	O
z	O
ranges	O
over	O
the	O
set	B
of	O
pairs	O
x	O
×y	O
and	O
the	O
loss	B
function	I
is	O
(	O
cid:96	O
)	O
0−1	O
(	O
h	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
def=	O
0	O
1	O
if	O
h	O
(	O
x	O
)	O
=	O
y	O
if	O
h	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
y	O
this	O
loss	B
function	I
is	O
used	O
in	O
binary	O
or	O
multiclass	B
classiﬁcation	O
problems	O
.	O
one	O
should	O
note	O
that	O
,	O
for	O
a	O
random	O
variable	O
,	O
α	O
,	O
taking	O
the	O
values	O
{	O
0	O
,	O
1	O
}	O
,	O
eα∼d	O
[	O
α	O
]	O
=	O
pα∼d	O
[	O
α	O
=	O
1	O
]	O
.	O
consequently	O
,	O
for	O
this	O
loss	B
function	I
,	O
the	O
deﬁni-	O
tions	O
of	O
ld	O
(	O
h	O
)	O
given	O
in	O
equation	O
(	O
3.3	O
)	O
and	O
equation	O
(	O
3.1	O
)	O
coincide	O
.	O
•	O
square	B
loss	I
:	O
here	O
,	O
our	O
random	O
variable	O
z	O
ranges	O
over	O
the	O
set	B
of	O
pairs	O
x	O
×y	O
and	O
the	O
loss	B
function	I
is	O
(	O
cid:96	O
)	O
sq	O
(	O
h	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
def=	O
(	O
h	O
(	O
x	O
)	O
−	O
y	O
)	O
2	O
.	O
3.3	O
summary	O
49	O
this	O
loss	B
function	I
is	O
used	O
in	O
regression	B
problems	O
.	O
we	O
will	O
later	O
see	O
more	O
examples	O
of	O
useful	O
instantiations	O
of	O
loss	B
functions	O
.	O
to	O
summarize	O
,	O
we	O
formally	O
deﬁne	O
agnostic	O
pac	O
learnability	O
for	O
general	O
loss	B
functions	O
.	O
definition	O
3.4	O
(	O
agnostic	O
pac	O
learnability	O
for	O
general	O
loss	B
functions	O
)	O
a	O
hypothesis	B
class	I
h	O
is	O
agnostic	O
pac	O
learnable	O
with	O
respect	O
to	O
a	O
set	B
z	O
and	O
a	O
loss	B
function	I
(	O
cid:96	O
)	O
:	O
h	O
×	O
z	O
→	O
r+	O
,	O
if	O
there	O
exist	O
a	O
function	B
mh	O
:	O
(	O
0	O
,	O
1	O
)	O
2	O
→	O
n	O
and	O
a	O
learning	O
algorithm	O
with	O
the	O
following	O
property	O
:	O
for	O
every	O
	O
,	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
and	O
for	O
every	O
distribution	O
d	O
over	O
z	O
,	O
when	O
running	O
the	O
learning	O
algorithm	O
on	O
m	O
≥	O
mh	O
(	O
	O
,	O
δ	O
)	O
i.i.d	O
.	O
examples	O
generated	O
by	O
d	O
,	O
the	O
algorithm	O
returns	O
h	O
∈	O
h	O
such	O
that	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
(	O
over	O
the	O
choice	O
of	O
the	O
m	O
training	O
examples	O
)	O
,	O
ld	O
(	O
h	O
)	O
≤	O
min	O
h	O
(	O
cid:48	O
)	O
∈h	O
ld	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
+	O
	O
,	O
where	O
ld	O
(	O
h	O
)	O
=	O
ez∼d	O
[	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
z	O
)	O
]	O
.	O
remark	O
3.1	O
(	O
a	O
note	O
about	O
measurability*	O
)	O
in	O
the	O
aforementioned	O
deﬁnition	O
,	O
for	O
every	O
h	O
∈	O
h	O
,	O
we	O
view	O
the	O
function	B
(	O
cid:96	O
)	O
(	O
h	O
,	O
·	O
)	O
:	O
z	O
→	O
r+	O
as	O
a	O
random	O
variable	O
and	O
deﬁne	O
ld	O
(	O
h	O
)	O
to	O
be	O
the	O
expected	O
value	O
of	O
this	O
random	O
variable	O
.	O
for	O
that	O
,	O
we	O
need	O
to	O
require	O
that	O
the	O
function	B
(	O
cid:96	O
)	O
(	O
h	O
,	O
·	O
)	O
is	O
measurable	O
.	O
formally	O
,	O
we	O
assume	O
that	O
there	O
is	O
a	O
σ-algebra	O
of	O
subsets	O
of	O
z	O
,	O
over	O
which	O
the	O
probability	O
d	O
is	O
deﬁned	O
,	O
and	O
that	O
the	O
preimage	O
of	O
every	O
initial	O
segment	O
in	O
r+	O
is	O
in	O
this	O
σ-algebra	O
.	O
in	O
the	O
speciﬁc	O
case	O
of	O
binary	O
classiﬁcation	O
with	O
the	O
0−1	O
loss	B
,	O
the	O
σ-algebra	O
is	O
over	O
x	O
×	O
{	O
0	O
,	O
1	O
}	O
and	O
our	O
assumption	O
on	O
(	O
cid:96	O
)	O
is	O
equivalent	O
to	O
the	O
assumption	O
that	O
for	O
every	O
h	O
,	O
the	O
set	B
{	O
(	O
x	O
,	O
h	O
(	O
x	O
)	O
)	O
:	O
x	O
∈	O
x	O
}	O
is	O
in	O
the	O
σ-algebra	O
.	O
remark	O
3.2	O
(	O
proper	B
versus	O
representation-independent	O
learning*	O
)	O
in	O
the	O
pre-	O
ceding	O
deﬁnition	O
,	O
we	O
required	O
that	O
the	O
algorithm	O
will	O
return	O
a	O
hypothesis	B
from	O
h.	O
in	O
some	O
situations	O
,	O
h	O
is	O
a	O
subset	O
of	O
a	O
set	B
h	O
(	O
cid:48	O
)	O
,	O
and	O
the	O
loss	B
function	I
can	O
be	O
naturally	O
extended	O
to	O
be	O
a	O
function	B
from	O
h	O
(	O
cid:48	O
)	O
×	O
z	O
to	O
the	O
reals	O
.	O
in	O
this	O
case	O
,	O
we	O
may	O
allow	O
the	O
algorithm	O
to	O
return	O
a	O
hypothesis	B
h	O
(	O
cid:48	O
)	O
∈	O
h	O
(	O
cid:48	O
)	O
,	O
as	O
long	O
as	O
it	O
satisﬁes	O
the	O
requirement	O
ld	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
≤	O
minh∈h	O
ld	O
(	O
h	O
)	O
+	O
	O
.	O
allowing	O
the	O
algorithm	O
to	O
output	O
a	O
hypothesis	B
from	O
h	O
(	O
cid:48	O
)	O
is	O
called	O
representation	B
independent	I
learning	O
,	O
while	O
proper	B
learning	O
occurs	O
when	O
the	O
algorithm	O
must	O
output	O
a	O
hypothesis	B
from	O
h.	O
represen-	O
tation	O
independent	O
learning	O
is	O
sometimes	O
called	O
“	O
improper	B
learning	O
,	O
”	O
although	O
there	O
is	O
nothing	O
improper	B
in	O
representation	B
independent	I
learning	O
.	O
3.3	O
summary	O
in	O
this	O
chapter	O
we	O
deﬁned	O
our	O
main	O
formal	O
learning	O
model	O
–	O
pac	O
learning	O
.	O
the	O
basic	O
model	O
relies	O
on	O
the	O
realizability	B
assumption	O
,	O
while	O
the	O
agnostic	O
variant	O
does	O
50	O
a	O
formal	O
learning	O
model	O
not	O
impose	O
any	O
restrictions	O
on	O
the	O
underlying	O
distribution	O
over	O
the	O
examples	O
.	O
we	O
also	O
generalized	O
the	O
pac	O
model	O
to	O
arbitrary	O
loss	B
functions	O
.	O
we	O
will	O
sometimes	O
refer	O
to	O
the	O
most	O
general	O
model	O
simply	O
as	O
pac	O
learning	O
,	O
omitting	O
the	O
“	O
agnostic	O
”	O
preﬁx	O
and	O
letting	O
the	O
reader	O
infer	O
what	O
the	O
underlying	O
loss	B
function	I
is	O
from	O
the	O
context	O
.	O
when	O
we	O
would	O
like	O
to	O
emphasize	O
that	O
we	O
are	O
dealing	O
with	O
the	O
original	O
pac	O
setting	O
we	O
mention	O
that	O
the	O
realizability	B
assumption	O
holds	O
.	O
in	O
chapter	O
7	O
we	O
will	O
discuss	O
other	O
notions	O
of	O
learnability	O
.	O
3.4	O
bibliographic	O
remarks	O
our	O
most	O
general	O
deﬁnition	O
of	O
agnostic	O
pac	O
learning	O
with	O
general	O
loss	B
func-	O
tions	O
follows	O
the	O
works	O
of	O
vladimir	O
vapnik	O
and	O
alexey	O
chervonenkis	O
(	O
vapnik	O
&	O
chervonenkis	O
1971	O
)	O
.	O
in	O
particular	O
,	O
we	O
follow	O
vapnik	O
’	O
s	O
general	O
setting	O
of	O
learning	O
(	O
vapnik	O
1982	O
,	O
vapnik	O
1992	O
,	O
vapnik	O
1995	O
,	O
vapnik	O
1998	O
)	O
.	O
pac	O
learning	O
was	O
introduced	O
by	O
valiant	O
(	O
1984	O
)	O
.	O
valiant	O
was	O
named	O
the	O
winner	O
of	O
the	O
2010	O
turing	O
award	O
for	O
the	O
introduction	O
of	O
the	O
pac	O
model	O
.	O
valiant	O
’	O
s	O
deﬁnition	O
requires	O
that	O
the	O
sample	B
complexity	I
will	O
be	O
polynomial	O
in	O
1/	O
and	O
in	O
1/δ	O
,	O
as	O
well	O
as	O
in	O
the	O
representation	O
size	O
of	O
hypotheses	O
in	O
the	O
class	O
(	O
see	O
also	O
kearns	O
&	O
vazirani	O
(	O
1994	O
)	O
)	O
.	O
as	O
we	O
will	O
see	O
in	O
chapter	O
6	O
,	O
if	O
a	O
problem	O
is	O
at	O
all	O
pac	O
learnable	O
then	O
the	O
sample	B
complexity	I
depends	O
polynomially	O
on	O
1/	O
and	O
log	O
(	O
1/δ	O
)	O
.	O
valiant	O
’	O
s	O
deﬁnition	O
also	O
requires	O
that	O
the	O
runtime	O
of	O
the	O
learning	O
algorithm	O
will	O
be	O
polynomial	O
in	O
these	O
quantities	O
.	O
in	O
contrast	O
,	O
we	O
chose	O
to	O
distinguish	O
between	O
the	O
statistical	O
aspect	O
of	O
learning	O
and	O
the	O
computational	O
aspect	O
of	O
learning	O
.	O
we	O
will	O
elaborate	O
on	O
the	O
computational	O
aspect	O
later	O
on	O
in	O
chapter	O
8	O
,	O
where	O
we	O
introduce	O
the	O
full	O
pac	O
learning	O
model	O
of	O
valiant	O
.	O
for	O
expository	O
reasons	O
,	O
we	O
use	O
the	O
term	O
pac	O
learning	O
even	O
when	O
we	O
ignore	O
the	O
runtime	O
aspect	O
of	O
learning	O
.	O
finally	O
,	O
the	O
formalization	O
of	O
agnostic	O
pac	O
learning	O
is	O
due	O
to	O
haussler	O
(	O
1992	O
)	O
.	O
3.5	O
exercises	O
1.	O
monotonicity	O
of	O
sample	B
complexity	I
:	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
for	O
a	O
binary	O
classiﬁcation	O
task	O
.	O
suppose	O
that	O
h	O
is	O
pac	O
learnable	O
and	O
its	O
sample	B
complexity	I
is	O
given	O
by	O
mh	O
(	O
·	O
,	O
·	O
)	O
.	O
show	O
that	O
mh	O
is	O
monotonically	O
nonincreasing	O
in	O
each	O
of	O
its	O
parameters	O
.	O
that	O
is	O
,	O
show	O
that	O
given	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
and	O
given	O
0	O
<	O
1	O
≤	O
2	O
<	O
1	O
,	O
we	O
have	O
that	O
mh	O
(	O
1	O
,	O
δ	O
)	O
≥	O
mh	O
(	O
2	O
,	O
δ	O
)	O
.	O
similarly	O
,	O
show	O
that	O
given	O
	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
and	O
given	O
0	O
<	O
δ1	O
≤	O
δ2	O
<	O
1	O
,	O
we	O
have	O
that	O
mh	O
(	O
	O
,	O
δ1	O
)	O
≥	O
mh	O
(	O
	O
,	O
δ2	O
)	O
.	O
2.	O
let	O
x	O
be	O
a	O
discrete	O
domain	B
,	O
and	O
let	O
hsingleton	O
=	O
{	O
hz	O
:	O
z	O
∈	O
x	O
}	O
∪	O
{	O
h−	O
}	O
,	O
where	O
for	O
each	O
z	O
∈	O
x	O
,	O
hz	O
is	O
the	O
function	B
deﬁned	O
by	O
hz	O
(	O
x	O
)	O
=	O
1	O
if	O
x	O
=	O
z	O
and	O
hz	O
(	O
x	O
)	O
=	O
0	O
if	O
x	O
(	O
cid:54	O
)	O
=	O
z.	O
h−	O
is	O
simply	O
the	O
all-negative	O
hypothesis	B
,	O
namely	O
,	O
∀x	O
∈	O
x	O
,	O
h−	O
(	O
x	O
)	O
=	O
0.	O
the	O
realizability	B
assumption	O
here	O
implies	O
that	O
the	O
true	O
hypothesis	O
f	O
labels	O
negatively	O
all	O
examples	O
in	O
the	O
domain	B
,	O
perhaps	O
except	O
one	O
.	O
3.5	O
exercises	O
51	O
in	O
the	O
realizable	O
setup	O
.	O
1.	O
describe	O
an	O
algorithm	O
that	O
implements	O
the	O
erm	O
rule	O
for	O
learning	O
hsingleton	O
2.	O
show	O
that	O
hsingleton	O
is	O
pac	O
learnable	O
.	O
provide	O
an	O
upper	O
bound	O
on	O
the	O
3.	O
let	O
x	O
=	O
r2	O
,	O
y	O
=	O
{	O
0	O
,	O
1	O
}	O
,	O
and	O
let	O
h	O
be	O
the	O
class	O
of	O
concentric	O
circles	O
in	O
the	O
plane	O
,	O
that	O
is	O
,	O
h	O
=	O
{	O
hr	O
:	O
r	O
∈	O
r+	O
}	O
,	O
where	O
hr	O
(	O
x	O
)	O
=	O
1	O
[	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
≤r	O
]	O
.	O
prove	O
that	O
h	O
is	O
pac	O
learnable	O
(	O
assume	O
realizability	B
)	O
,	O
and	O
its	O
sample	B
complexity	I
is	O
bounded	O
by	O
sample	B
complexity	I
.	O
(	O
cid:24	O
)	O
log	O
(	O
1/δ	O
)	O
(	O
cid:25	O
)	O
mh	O
(	O
	O
,	O
δ	O
)	O
≤	O
.	O
	O
4.	O
in	O
this	O
question	O
,	O
we	O
study	O
the	O
hypothesis	B
class	I
of	O
boolean	B
conjunctions	I
deﬁned	O
as	O
follows	O
.	O
the	O
instance	B
space	I
is	O
x	O
=	O
{	O
0	O
,	O
1	O
}	O
d	O
and	O
the	O
label	B
set	O
is	O
y	O
=	O
{	O
0	O
,	O
1	O
}	O
.	O
a	O
literal	O
over	O
the	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
is	O
a	O
simple	O
boolean	O
function	O
that	O
takes	O
the	O
form	O
f	O
(	O
x	O
)	O
=	O
xi	O
,	O
for	O
some	O
i	O
∈	O
[	O
d	O
]	O
,	O
or	O
f	O
(	O
x	O
)	O
=	O
1−	O
xi	O
for	O
some	O
i	O
∈	O
[	O
d	O
]	O
.	O
we	O
use	O
the	O
notation	O
¯xi	O
as	O
a	O
shorthand	O
for	O
1−	O
xi	O
.	O
a	O
conjunction	O
is	O
any	O
product	O
of	O
literals	O
.	O
in	O
boolean	O
logic	O
,	O
the	O
product	O
is	O
denoted	O
using	O
the	O
∧	O
sign	O
.	O
for	O
example	O
,	O
the	O
function	B
h	O
(	O
x	O
)	O
=	O
x1	O
·	O
(	O
1	O
−	O
x2	O
)	O
is	O
written	O
as	O
x1	O
∧	O
¯x2	O
.	O
we	O
consider	O
the	O
hypothesis	B
class	I
of	O
all	O
conjunctions	O
of	O
literals	O
over	O
the	O
d	O
variables	O
.	O
the	O
empty	O
conjunction	O
is	O
interpreted	O
as	O
the	O
all-positive	O
hypothesis	B
(	O
namely	O
,	O
the	O
function	B
that	O
returns	O
h	O
(	O
x	O
)	O
=	O
1	O
for	O
all	O
x	O
)	O
.	O
the	O
conjunction	O
x1∧	O
¯x1	O
(	O
and	O
similarly	O
any	O
conjunction	O
involving	O
a	O
literal	O
and	O
its	O
negation	O
)	O
is	O
allowed	O
and	O
interpreted	O
as	O
the	O
all-negative	O
hypothesis	B
(	O
namely	O
,	O
the	O
conjunction	O
that	O
returns	O
h	O
(	O
x	O
)	O
=	O
0	O
for	O
all	O
x	O
)	O
.	O
we	O
assume	O
realizability	B
:	O
namely	O
,	O
we	O
assume	O
that	O
there	O
exists	O
a	O
boolean	O
conjunction	O
that	O
generates	O
the	O
labels	O
.	O
thus	O
,	O
each	O
example	O
(	O
x	O
,	O
y	O
)	O
∈	O
x	O
×	O
y	O
consists	O
of	O
an	O
assignment	O
to	O
the	O
d	O
boolean	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
,	O
and	O
its	O
truth	O
value	O
(	O
0	O
for	O
false	O
and	O
1	O
for	O
true	O
)	O
.	O
for	O
instance	B
,	O
let	O
d	O
=	O
3	O
and	O
suppose	O
that	O
the	O
true	O
conjunction	O
is	O
x1	O
∧	O
¯x2	O
.	O
then	O
,	O
the	O
training	B
set	I
s	O
might	O
contain	O
the	O
following	O
instances	O
:	O
(	O
(	O
1	O
,	O
1	O
,	O
1	O
)	O
,	O
0	O
)	O
,	O
(	O
(	O
1	O
,	O
0	O
,	O
1	O
)	O
,	O
1	O
)	O
,	O
(	O
(	O
0	O
,	O
1	O
,	O
0	O
)	O
,	O
0	O
)	O
(	O
(	O
1	O
,	O
0	O
,	O
0	O
)	O
,	O
1	O
)	O
.	O
prove	O
that	O
the	O
hypothesis	B
class	I
of	O
all	O
conjunctions	O
over	O
d	O
variables	O
is	O
pac	O
learnable	O
and	O
bound	O
its	O
sample	B
complexity	I
.	O
propose	O
an	O
algorithm	O
that	O
implements	O
the	O
erm	O
rule	O
,	O
whose	O
runtime	O
is	O
polynomial	O
in	O
d	O
·	O
m.	O
5.	O
let	O
x	O
be	O
a	O
domain	B
and	O
let	O
d1	O
,	O
d2	O
,	O
.	O
.	O
.	O
,	O
dm	O
be	O
a	O
sequence	O
of	O
distributions	O
over	O
x	O
.	O
let	O
h	O
be	O
a	O
ﬁnite	O
class	O
of	O
binary	O
classiﬁers	O
over	O
x	O
and	O
let	O
f	O
∈	O
h.	O
suppose	O
we	O
are	O
getting	O
a	O
sample	O
s	O
of	O
m	O
examples	O
,	O
such	O
that	O
the	O
instances	O
are	O
independent	O
but	O
are	O
not	O
identically	O
distributed	O
;	O
the	O
ith	O
instance	B
is	O
sampled	O
from	O
di	O
and	O
then	O
yi	O
is	O
set	B
to	O
be	O
f	O
(	O
xi	O
)	O
.	O
let	O
¯dm	O
denote	O
the	O
average	O
,	O
that	O
is	O
,	O
¯dm	O
=	O
(	O
d1	O
+	O
···	O
+	O
dm	O
)	O
/m	O
.	O
fix	O
an	O
accuracy	B
parameter	O
	O
∈	O
(	O
0	O
,	O
1	O
)	O
.	O
show	O
that	O
p	O
(	O
cid:2	O
)	O
∃h	O
∈	O
h	O
s.t	O
.	O
l	O
(	O
¯dm	O
,	O
f	O
)	O
(	O
h	O
)	O
>	O
	O
and	O
l	O
(	O
s	O
,	O
f	O
)	O
(	O
h	O
)	O
=	O
0	O
(	O
cid:3	O
)	O
≤	O
|h|e−m	O
.	O
52	O
a	O
formal	O
learning	O
model	O
hint	O
:	O
use	O
the	O
geometric-arithmetic	O
mean	O
inequality	O
.	O
6.	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
of	O
binary	O
classiﬁers	O
.	O
show	O
that	O
if	O
h	O
is	O
agnostic	O
pac	O
learnable	O
,	O
then	O
h	O
is	O
pac	O
learnable	O
as	O
well	O
.	O
furthermore	O
,	O
if	O
a	O
is	O
a	O
suc-	O
cessful	O
agnostic	O
pac	O
learner	O
for	O
h	O
,	O
then	O
a	O
is	O
also	O
a	O
successful	O
pac	O
learner	O
for	O
h.	O
7	O
.	O
(	O
*	O
)	O
the	O
bayes	O
optimal	O
predictor	B
:	O
show	O
that	O
for	O
every	O
probability	O
distri-	O
bution	O
d	O
,	O
the	O
bayes	O
optimal	O
predictor	B
fd	O
is	O
optimal	O
,	O
in	O
the	O
sense	O
that	O
for	O
every	O
classiﬁer	B
g	O
from	O
x	O
to	O
{	O
0	O
,	O
1	O
}	O
,	O
ld	O
(	O
fd	O
)	O
≤	O
ld	O
(	O
g	O
)	O
.	O
probability	O
distribution	O
,	O
d	O
,	O
if	O
8	O
.	O
(	O
*	O
)	O
we	O
say	O
that	O
a	O
learning	O
algorithm	O
a	O
is	O
better	O
than	O
b	O
with	O
respect	O
to	O
some	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
ld	O
(	O
b	O
(	O
s	O
)	O
)	O
for	O
all	O
samples	O
s	O
∈	O
(	O
x	O
×	O
{	O
0	O
,	O
1	O
}	O
)	O
m.	O
we	O
say	O
that	O
a	O
learning	O
algorithm	O
a	O
is	O
better	O
than	O
b	O
,	O
if	O
it	O
is	O
better	O
than	O
b	O
with	O
respect	O
to	O
all	O
probability	O
distributions	O
d	O
over	O
x	O
×	O
{	O
0	O
,	O
1	O
}	O
.	O
1.	O
a	O
probabilistic	O
label	B
predictor	O
is	O
a	O
function	B
that	O
assigns	O
to	O
every	O
domain	B
point	O
x	O
a	O
probability	O
value	O
,	O
h	O
(	O
x	O
)	O
∈	O
[	O
0	O
,	O
1	O
]	O
,	O
that	O
determines	O
the	O
probability	O
of	O
predicting	O
the	O
label	B
1.	O
that	O
is	O
,	O
given	O
such	O
an	O
h	O
and	O
an	O
input	O
,	O
x	O
,	O
the	O
label	B
for	O
x	O
is	O
predicted	O
by	O
tossing	O
a	O
coin	O
with	O
bias	B
h	O
(	O
x	O
)	O
toward	O
heads	O
and	O
predicting	O
1	O
iﬀ	O
the	O
coin	O
comes	O
up	O
heads	O
.	O
formally	O
,	O
we	O
deﬁne	O
a	O
probabilistic	O
label	B
predictor	O
as	O
a	O
function	B
,	O
h	O
:	O
x	O
→	O
[	O
0	O
,	O
1	O
]	O
.	O
the	O
loss	B
of	O
such	O
h	O
on	O
an	O
example	O
(	O
x	O
,	O
y	O
)	O
is	O
deﬁned	O
to	O
be	O
|h	O
(	O
x	O
)	O
−	O
y|	O
,	O
which	O
is	O
exactly	O
the	O
probability	O
that	O
the	O
prediction	O
of	O
h	O
will	O
not	O
be	O
equal	O
to	O
y.	O
note	O
that	O
if	O
h	O
is	O
deterministic	O
,	O
that	O
is	O
,	O
returns	O
values	O
in	O
{	O
0	O
,	O
1	O
}	O
,	O
then	O
|h	O
(	O
x	O
)	O
−	O
y|	O
=	O
1	O
[	O
h	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=y	O
]	O
.	O
prove	O
that	O
for	O
every	O
data-generating	O
distribution	O
d	O
over	O
x	O
×	O
{	O
0	O
,	O
1	O
}	O
,	O
the	O
bayes	O
optimal	O
predictor	B
has	O
the	O
smallest	O
risk	B
(	O
w.r.t	O
.	O
the	O
loss	B
function	I
(	O
cid:96	O
)	O
(	O
h	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
|h	O
(	O
x	O
)	O
−y|	O
,	O
among	O
all	O
possible	O
label	B
predictors	O
,	O
including	O
prob-	O
abilistic	O
ones	O
)	O
.	O
2.	O
let	O
x	O
be	O
a	O
domain	B
and	O
{	O
0	O
,	O
1	O
}	O
be	O
a	O
set	B
of	O
labels	O
.	O
prove	O
that	O
for	O
every	O
distribution	O
d	O
over	O
x	O
×	O
{	O
0	O
,	O
1	O
}	O
,	O
there	O
exist	O
a	O
learning	O
algorithm	O
ad	O
that	O
is	O
better	O
than	O
any	O
other	O
learning	O
algorithm	O
with	O
respect	O
to	O
d.	O
3.	O
prove	O
that	O
for	O
every	O
learning	O
algorithm	O
a	O
there	O
exist	O
a	O
probability	O
distri-	O
bution	O
,	O
d	O
,	O
and	O
a	O
learning	O
algorithm	O
b	O
such	O
that	O
a	O
is	O
not	O
better	O
than	O
b	O
w.r.t	O
.	O
d.	O
9.	O
consider	O
a	O
variant	O
of	O
the	O
pac	O
model	O
in	O
which	O
there	O
are	O
two	O
example	O
ora-	O
cles	O
:	O
one	O
that	O
generates	O
positive	O
examples	O
and	O
one	O
that	O
generates	O
negative	O
examples	O
,	O
both	O
according	O
to	O
the	O
underlying	O
distribution	O
d	O
on	O
x	O
.	O
formally	O
,	O
given	O
a	O
target	O
function	O
f	O
:	O
x	O
→	O
{	O
0	O
,	O
1	O
}	O
,	O
let	O
d+	O
be	O
the	O
distribution	O
over	O
x	O
+	O
=	O
{	O
x	O
∈	O
x	O
:	O
f	O
(	O
x	O
)	O
=	O
1	O
}	O
deﬁned	O
by	O
d+	O
(	O
a	O
)	O
=	O
d	O
(	O
a	O
)	O
/d	O
(	O
x	O
+	O
)	O
,	O
for	O
every	O
a	O
⊂	O
x	O
+	O
.	O
similarly	O
,	O
d−	O
is	O
the	O
distribution	O
over	O
x	O
−	O
induced	O
by	O
d.	O
the	O
deﬁnition	O
of	O
pac	O
learnability	O
in	O
the	O
two-oracle	O
model	O
is	O
the	O
same	O
as	O
the	O
standard	O
deﬁnition	O
of	O
pac	O
learnability	O
except	O
that	O
here	O
the	O
learner	O
has	O
access	O
to	O
m+h	O
(	O
	O
,	O
δ	O
)	O
i.i.d	O
.	O
examples	O
from	O
d+	O
and	O
m−	O
(	O
	O
,	O
δ	O
)	O
i.i.d	O
.	O
examples	O
from	O
d−	O
.	O
the	O
learner	O
’	O
s	O
goal	O
is	O
to	O
output	O
h	O
s.t	O
.	O
with	O
probability	O
at	O
least	O
1−	O
δ	O
(	O
over	O
the	O
choice	O
3.5	O
exercises	O
53	O
of	O
the	O
two	O
training	O
sets	O
,	O
and	O
possibly	O
over	O
the	O
nondeterministic	O
decisions	O
made	O
by	O
the	O
learning	O
algorithm	O
)	O
,	O
both	O
l	O
(	O
d+	O
,	O
f	O
)	O
(	O
h	O
)	O
≤	O
	O
and	O
l	O
(	O
d−	O
,	O
f	O
)	O
(	O
h	O
)	O
≤	O
	O
.	O
1	O
.	O
(	O
*	O
)	O
show	O
that	O
if	O
h	O
is	O
pac	O
learnable	O
(	O
in	O
the	O
standard	O
one-oracle	O
model	O
)	O
,	O
then	O
h	O
is	O
pac	O
learnable	O
in	O
the	O
two-oracle	O
model	O
.	O
2	O
.	O
(	O
**	O
)	O
deﬁne	O
h+	O
to	O
be	O
the	O
always-plus	O
hypothesis	B
and	O
h−	O
to	O
be	O
the	O
always-	O
minus	O
hypothesis	B
.	O
assume	O
that	O
h+	O
,	O
h−	O
∈	O
h.	O
show	O
that	O
if	O
h	O
is	O
pac	O
learn-	O
able	O
in	O
the	O
two-oracle	O
model	O
,	O
then	O
h	O
is	O
pac	O
learnable	O
in	O
the	O
standard	O
one-oracle	O
model	O
.	O
4	O
learning	O
via	O
uniform	B
convergence	I
the	O
ﬁrst	O
formal	O
learning	O
model	O
that	O
we	O
have	O
discussed	O
was	O
the	O
pac	O
model	O
.	O
in	O
chapter	O
2	O
we	O
have	O
shown	O
that	O
under	O
the	O
realizability	B
assumption	O
,	O
any	O
ﬁnite	O
hypothesis	B
class	I
is	O
pac	O
learnable	O
.	O
in	O
this	O
chapter	O
we	O
will	O
develop	O
a	O
general	O
tool	O
,	O
uniform	B
convergence	I
,	O
and	O
apply	O
it	O
to	O
show	O
that	O
any	O
ﬁnite	O
class	O
is	O
learnable	O
in	O
the	O
agnostic	O
pac	O
model	O
with	O
general	O
loss	B
functions	O
,	O
as	O
long	O
as	O
the	O
range	O
loss	B
function	I
is	O
bounded	O
.	O
4.1	O
uniform	B
convergence	I
is	O
suﬃcient	O
for	O
learnability	O
the	O
idea	O
behind	O
the	O
learning	O
condition	O
discussed	O
in	O
this	O
chapter	O
is	O
very	O
simple	O
.	O
recall	B
that	O
,	O
given	O
a	O
hypothesis	B
class	I
,	O
h	O
,	O
the	O
erm	O
learning	O
paradigm	O
works	O
as	O
follows	O
:	O
upon	O
receiving	O
a	O
training	O
sample	O
,	O
s	O
,	O
the	O
learner	O
evaluates	O
the	O
risk	B
(	O
or	O
error	O
)	O
of	O
each	O
h	O
in	O
h	O
on	O
the	O
given	O
sample	O
and	O
outputs	O
a	O
member	O
of	O
h	O
that	O
minimizes	O
this	O
empirical	B
risk	I
.	O
the	O
hope	O
is	O
that	O
an	O
h	O
that	O
minimizes	O
the	O
empirical	B
risk	I
with	O
respect	O
to	O
s	O
is	O
a	O
risk	B
minimizer	O
(	O
or	O
has	O
risk	B
close	O
to	O
the	O
minimum	O
)	O
with	O
respect	O
to	O
the	O
true	O
data	O
probability	O
distribution	O
as	O
well	O
.	O
for	O
that	O
,	O
it	O
suﬃces	O
to	O
ensure	O
that	O
the	O
empirical	O
risks	O
of	O
all	O
members	O
of	O
h	O
are	O
good	O
approximations	O
of	O
their	O
true	O
risk	O
.	O
put	O
another	O
way	O
,	O
we	O
need	O
that	O
uniformly	O
over	O
all	O
hypotheses	O
in	O
the	O
hypothesis	B
class	I
,	O
the	O
empirical	B
risk	I
will	O
be	O
close	O
to	O
the	O
true	O
risk	O
,	O
as	O
formalized	O
in	O
the	O
following	O
.	O
definition	O
4.1	O
(	O
-representative	O
sample	O
)	O
a	O
training	B
set	I
s	O
is	O
called	O
-representative	O
(	O
w.r.t	O
.	O
domain	B
z	O
,	O
hypothesis	B
class	I
h	O
,	O
loss	B
function	I
(	O
cid:96	O
)	O
,	O
and	O
distribution	O
d	O
)	O
if	O
∀h	O
∈	O
h	O
,	O
|ls	O
(	O
h	O
)	O
−	O
ld	O
(	O
h	O
)	O
|	O
≤	O
	O
.	O
the	O
next	O
simple	O
lemma	O
states	O
that	O
whenever	O
the	O
sample	O
is	O
(	O
/2	O
)	O
-representative	O
,	O
the	O
erm	O
learning	O
rule	O
is	O
guaranteed	O
to	O
return	O
a	O
good	O
hypothesis	B
.	O
lemma	O
4.2	O
assume	O
that	O
a	O
training	B
set	I
s	O
is	O
	O
2	O
-representative	O
(	O
w.r.t	O
.	O
domain	B
z	O
,	O
hypothesis	B
class	I
h	O
,	O
loss	B
function	I
(	O
cid:96	O
)	O
,	O
and	O
distribution	O
d	O
)	O
.	O
then	O
,	O
any	O
output	O
of	O
ermh	O
(	O
s	O
)	O
,	O
namely	O
,	O
any	O
hs	O
∈	O
argminh∈h	O
ls	O
(	O
h	O
)	O
,	O
satisﬁes	O
ld	O
(	O
hs	O
)	O
≤	O
min	O
h∈h	O
ld	O
(	O
h	O
)	O
+	O
	O
.	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
4.2	O
finite	O
classes	O
are	O
agnostic	O
pac	O
learnable	O
55	O
proof	O
for	O
every	O
h	O
∈	O
h	O
,	O
ld	O
(	O
hs	O
)	O
≤	O
ls	O
(	O
hs	O
)	O
+	O
	O
2	O
≤	O
ls	O
(	O
h	O
)	O
+	O
	O
2	O
≤	O
ld	O
(	O
h	O
)	O
+	O
	O
2	O
+	O
	O
2	O
=	O
ld	O
(	O
h	O
)	O
+	O
	O
,	O
where	O
the	O
ﬁrst	O
and	O
third	O
inequalities	O
are	O
due	O
to	O
the	O
assumption	O
that	O
s	O
is	O
	O
2	O
-	O
representative	O
(	O
deﬁnition	O
4.1	O
)	O
and	O
the	O
second	O
inequality	O
holds	O
since	O
hs	O
is	O
an	O
erm	O
predictor	B
.	O
the	O
preceding	O
lemma	O
implies	O
that	O
to	O
ensure	O
that	O
the	O
erm	O
rule	O
is	O
an	O
agnostic	O
pac	O
learner	O
,	O
it	O
suﬃces	O
to	O
show	O
that	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
the	O
random	O
choice	O
of	O
a	O
training	B
set	I
,	O
it	O
will	O
be	O
an	O
-representative	O
training	B
set	I
.	O
the	O
uniform	B
convergence	I
condition	O
formalizes	O
this	O
requirement	O
.	O
definition	O
4.3	O
(	O
uniform	B
convergence	I
)	O
we	O
say	O
that	O
a	O
hypothesis	B
class	I
h	O
has	O
the	O
uniform	B
convergence	I
property	O
(	O
w.r.t	O
.	O
a	O
domain	B
z	O
and	O
a	O
loss	B
function	I
(	O
cid:96	O
)	O
)	O
if	O
there	O
exists	O
a	O
function	B
much	O
:	O
(	O
0	O
,	O
1	O
)	O
2	O
→	O
n	O
such	O
that	O
for	O
every	O
	O
,	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
and	O
for	O
every	O
probability	O
distribution	O
d	O
over	O
z	O
,	O
if	O
s	O
is	O
a	O
sample	O
of	O
m	O
≥	O
much	O
(	O
	O
,	O
δ	O
)	O
examples	O
drawn	O
i.i.d	O
.	O
according	O
to	O
d	O
,	O
then	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
,	O
s	O
is	O
-representative	O
.	O
similar	O
to	O
the	O
deﬁnition	O
of	O
sample	B
complexity	I
for	O
pac	O
learning	O
,	O
the	O
function	B
much	O
measures	O
the	O
(	O
minimal	O
)	O
sample	B
complexity	I
of	O
obtaining	O
the	O
uniform	O
con-	O
vergence	O
property	O
,	O
namely	O
,	O
how	O
many	O
examples	O
we	O
need	O
to	O
ensure	O
that	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
the	O
sample	O
would	O
be	O
-representative	O
.	O
the	O
term	O
uniform	O
here	O
refers	O
to	O
having	O
a	O
ﬁxed	O
sample	O
size	O
that	O
works	O
for	O
all	O
members	O
of	O
h	O
and	O
over	O
all	O
possible	O
probability	O
distributions	O
over	O
the	O
domain	B
.	O
the	O
following	O
corollary	O
follows	O
directly	O
from	O
lemma	O
4.2	O
and	O
the	O
deﬁnition	O
of	O
uniform	B
convergence	I
.	O
if	O
a	O
class	O
h	O
has	O
the	O
uniform	B
convergence	I
property	O
with	O
a	O
corollary	O
4.4	O
function	B
much	O
then	O
the	O
class	O
is	O
agnostically	O
pac	O
learnable	O
with	O
the	O
sample	O
com-	O
plexity	O
mh	O
(	O
	O
,	O
δ	O
)	O
≤	O
much	O
(	O
/2	O
,	O
δ	O
)	O
.	O
furthermore	O
,	O
in	O
that	O
case	O
,	O
the	O
ermh	O
paradigm	O
is	O
a	O
successful	O
agnostic	O
pac	O
learner	O
for	O
h.	O
4.2	O
finite	O
classes	O
are	O
agnostic	O
pac	O
learnable	O
in	O
view	O
of	O
corollary	O
4.4	O
,	O
the	O
claim	O
that	O
every	O
ﬁnite	O
hypothesis	B
class	I
is	O
agnostic	O
pac	O
learnable	O
will	O
follow	O
once	O
we	O
establish	O
that	O
uniform	B
convergence	I
holds	O
for	O
a	O
ﬁnite	O
hypothesis	B
class	I
.	O
to	O
show	O
that	O
uniform	B
convergence	I
holds	O
we	O
follow	O
a	O
two	O
step	O
argument	O
,	O
similar	O
to	O
the	O
derivation	O
in	O
chapter	O
2.	O
the	O
ﬁrst	O
step	O
applies	O
the	O
union	B
bound	I
while	O
the	O
second	O
step	O
employs	O
a	O
measure	B
concentration	I
inequality	O
.	O
we	O
now	O
explain	O
these	O
two	O
steps	O
in	O
detail	O
.	O
fix	O
some	O
	O
,	O
δ.	O
we	O
need	O
to	O
ﬁnd	O
a	O
sample	O
size	O
m	O
that	O
guarantees	O
that	O
for	O
any	O
d	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
of	O
the	O
choice	O
of	O
s	O
=	O
(	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
)	O
sampled	O
56	O
learning	O
via	O
uniform	B
convergence	I
i.i.d	O
.	O
from	O
d	O
we	O
have	O
that	O
for	O
all	O
h	O
∈	O
h	O
,	O
|ls	O
(	O
h	O
)	O
−	O
ld	O
(	O
h	O
)	O
|	O
≤	O
	O
.	O
that	O
is	O
,	O
dm	O
(	O
{	O
s	O
:	O
∀h	O
∈	O
h	O
,	O
|ls	O
(	O
h	O
)	O
−	O
ld	O
(	O
h	O
)	O
|	O
≤	O
	O
}	O
)	O
≥	O
1	O
−	O
δ.	O
equivalently	O
,	O
we	O
need	O
to	O
show	O
that	O
dm	O
(	O
{	O
s	O
:	O
∃h	O
∈	O
h	O
,	O
|ls	O
(	O
h	O
)	O
−	O
ld	O
(	O
h	O
)	O
|	O
>	O
	O
}	O
)	O
<	O
δ.	O
writing	O
{	O
s	O
:	O
∃h	O
∈	O
h	O
,	O
|ls	O
(	O
h	O
)	O
−	O
ld	O
(	O
h	O
)	O
|	O
>	O
	O
}	O
=	O
∪h∈h	O
{	O
s	O
:	O
|ls	O
(	O
h	O
)	O
−	O
ld	O
(	O
h	O
)	O
|	O
>	O
	O
}	O
,	O
and	O
applying	O
the	O
union	B
bound	I
(	O
lemma	O
2.2	O
)	O
we	O
obtain	O
dm	O
(	O
{	O
s	O
:	O
∃h	O
∈	O
h	O
,	O
|ls	O
(	O
h	O
)	O
−	O
ld	O
(	O
h	O
)	O
|	O
>	O
	O
}	O
)	O
≤	O
(	O
cid:88	O
)	O
dm	O
(	O
{	O
s	O
:	O
|ls	O
(	O
h	O
)	O
−	O
ld	O
(	O
h	O
)	O
|	O
>	O
	O
}	O
)	O
.	O
h∈h	O
(	O
4.1	O
)	O
our	O
second	O
step	O
will	O
be	O
to	O
argue	O
that	O
each	O
summand	O
of	O
the	O
right-hand	O
side	O
of	O
this	O
inequality	O
is	O
small	O
enough	O
(	O
for	O
a	O
suﬃciently	O
large	O
m	O
)	O
.	O
that	O
is	O
,	O
we	O
will	O
show	O
that	O
for	O
any	O
ﬁxed	O
hypothesis	B
,	O
h	O
,	O
(	O
which	O
is	O
chosen	O
in	O
advance	O
prior	O
to	O
the	O
sampling	O
of	O
the	O
training	B
set	I
)	O
,	O
the	O
gap	O
between	O
the	O
true	O
and	O
empirical	O
risks	O
,	O
|ls	O
(	O
h	O
)	O
−	O
ld	O
(	O
h	O
)	O
|	O
,	O
is	O
likely	O
to	O
be	O
small	O
.	O
i=1	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
.	O
since	O
each	O
zi	O
is	O
sampled	O
i.i.d	O
.	O
from	O
d	O
,	O
the	O
expected	O
value	O
of	O
the	O
random	O
variable	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
is	O
ld	O
(	O
h	O
)	O
.	O
by	O
the	O
linearity	O
of	O
expectation	O
,	O
it	O
follows	O
that	O
ld	O
(	O
h	O
)	O
is	O
also	O
the	O
expected	O
value	O
of	O
ls	O
(	O
h	O
)	O
.	O
hence	O
,	O
the	O
quantity	O
|ld	O
(	O
h	O
)	O
−ls	O
(	O
h	O
)	O
|	O
is	O
the	O
deviation	O
of	O
the	O
random	O
variable	O
ls	O
(	O
h	O
)	O
from	O
its	O
expectation	O
.	O
we	O
therefore	O
need	O
to	O
show	O
that	O
the	O
measure	O
of	O
ls	O
(	O
h	O
)	O
is	O
concentrated	O
around	O
its	O
expected	O
value	O
.	O
recall	B
that	O
ld	O
(	O
h	O
)	O
=	O
ez∼d	O
[	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
z	O
)	O
]	O
and	O
that	O
ls	O
(	O
h	O
)	O
=	O
1	O
(	O
cid:80	O
)	O
m	O
m	O
a	O
basic	O
statistical	O
fact	O
,	O
the	O
law	O
of	O
large	O
numbers	O
,	O
states	O
that	O
when	O
m	O
goes	O
to	O
inﬁnity	O
,	O
empirical	O
averages	O
converge	O
to	O
their	O
true	O
expectation	O
.	O
this	O
is	O
true	O
for	O
ls	O
(	O
h	O
)	O
,	O
since	O
it	O
is	O
the	O
empirical	O
average	O
of	O
m	O
i.i.d	O
random	O
variables	O
.	O
however	O
,	O
since	O
the	O
law	O
of	O
large	O
numbers	O
is	O
only	O
an	O
asymptotic	O
result	O
,	O
it	O
provides	O
no	O
information	O
about	O
the	O
gap	O
between	O
the	O
empirically	O
estimated	O
error	O
and	O
its	O
true	O
value	O
for	O
any	O
given	O
,	O
ﬁnite	O
,	O
sample	O
size	O
.	O
instead	O
,	O
we	O
will	O
use	O
a	O
measure	B
concentration	I
inequality	O
due	O
to	O
hoeﬀding	O
,	O
which	O
quantiﬁes	O
the	O
gap	O
between	O
empirical	O
averages	O
and	O
their	O
expected	O
value	O
.	O
lemma	O
4.5	O
(	O
hoeﬀding	O
’	O
s	O
inequality	O
)	O
let	O
θ1	O
,	O
.	O
.	O
.	O
,	O
θm	O
be	O
a	O
sequence	O
of	O
i.i.d	O
.	O
ran-	O
dom	O
variables	O
and	O
assume	O
that	O
for	O
all	O
i	O
,	O
e	O
[	O
θi	O
]	O
=	O
µ	O
and	O
p	O
[	O
a	O
≤	O
θi	O
≤	O
b	O
]	O
=	O
1.	O
then	O
,	O
for	O
any	O
	O
>	O
0	O
(	O
cid:34	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
1	O
m	O
p	O
m	O
(	O
cid:88	O
)	O
i=1	O
θi	O
−	O
µ	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
>	O
	O
(	O
cid:35	O
)	O
≤	O
2	O
exp	O
(	O
cid:0	O
)	O
−2	O
m	O
2/	O
(	O
b	O
−	O
a	O
)	O
2	O
(	O
cid:1	O
)	O
.	O
the	O
proof	O
can	O
be	O
found	O
in	O
appendix	O
b.	O
getting	O
back	O
to	O
our	O
problem	O
,	O
let	O
θi	O
be	O
the	O
random	O
variable	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
.	O
since	O
h	O
is	O
ﬁxed	O
and	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
are	O
sampled	O
i.i.d.	B
,	O
it	O
follows	O
that	O
θ1	O
,	O
.	O
.	O
.	O
,	O
θm	O
are	O
also	O
i.i.d	O
.	O
random	O
variables	O
.	O
furthermore	O
,	O
ls	O
(	O
h	O
)	O
=	O
1	O
i=1	O
θi	O
and	O
ld	O
(	O
h	O
)	O
=	O
µ.	O
let	O
us	O
m	O
(	O
cid:80	O
)	O
m	O
4.2	O
finite	O
classes	O
are	O
agnostic	O
pac	O
learnable	O
57	O
m	O
(	O
cid:88	O
)	O
(	O
cid:34	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
1	O
m	O
further	O
assume	O
that	O
the	O
range	O
of	O
(	O
cid:96	O
)	O
is	O
[	O
0	O
,	O
1	O
]	O
and	O
therefore	O
θi	O
∈	O
[	O
0	O
,	O
1	O
]	O
.	O
we	O
therefore	O
obtain	O
that	O
dm	O
(	O
{	O
s	O
:	O
|ls	O
(	O
h	O
)	O
−	O
ld	O
(	O
h	O
)	O
|	O
>	O
	O
}	O
)	O
=	O
p	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
>	O
	O
(	O
cid:35	O
)	O
≤	O
2	O
exp	O
(	O
cid:0	O
)	O
−2	O
m	O
2	O
(	O
cid:1	O
)	O
.	O
dm	O
(	O
{	O
s	O
:	O
∃h	O
∈	O
h	O
,	O
|ls	O
(	O
h	O
)	O
−	O
ld	O
(	O
h	O
)	O
|	O
>	O
	O
}	O
)	O
≤	O
(	O
cid:88	O
)	O
2	O
exp	O
(	O
cid:0	O
)	O
−2	O
m	O
2	O
(	O
cid:1	O
)	O
=	O
2|h|	O
exp	O
(	O
cid:0	O
)	O
−2	O
m	O
2	O
(	O
cid:1	O
)	O
.	O
combining	O
this	O
with	O
equation	O
(	O
4.1	O
)	O
yields	O
θi	O
−	O
µ	O
h∈h	O
(	O
4.2	O
)	O
i=1	O
finally	O
,	O
if	O
we	O
choose	O
m	O
≥	O
log	O
(	O
2|h|/δ	O
)	O
22	O
then	O
dm	O
(	O
{	O
s	O
:	O
∃h	O
∈	O
h	O
,	O
|ls	O
(	O
h	O
)	O
−	O
ld	O
(	O
h	O
)	O
|	O
>	O
	O
}	O
)	O
≤	O
δ.	O
corollary	O
4.6	O
let	O
h	O
be	O
a	O
ﬁnite	O
hypothesis	B
class	I
,	O
let	O
z	O
be	O
a	O
domain	B
,	O
and	O
let	O
(	O
cid:96	O
)	O
:	O
h	O
×	O
z	O
→	O
[	O
0	O
,	O
1	O
]	O
be	O
a	O
loss	B
function	I
.	O
then	O
,	O
h	O
enjoys	O
the	O
uniform	B
convergence	I
property	O
with	O
sample	B
complexity	I
(	O
cid:24	O
)	O
log	O
(	O
2|h|/δ	O
)	O
(	O
cid:25	O
)	O
22	O
.	O
much	O
(	O
	O
,	O
δ	O
)	O
≤	O
furthermore	O
,	O
the	O
class	O
is	O
agnostically	O
pac	O
learnable	O
using	O
the	O
erm	O
algorithm	O
with	O
sample	B
complexity	I
mh	O
(	O
	O
,	O
δ	O
)	O
≤	O
much	O
(	O
/2	O
,	O
δ	O
)	O
≤	O
(	O
cid:24	O
)	O
2	O
log	O
(	O
2|h|/δ	O
)	O
(	O
cid:25	O
)	O
2	O
.	O
remark	O
4.1	O
(	O
the	O
“	O
discretization	B
trick	I
”	O
)	O
while	O
the	O
preceding	O
corollary	O
only	O
applies	O
to	O
ﬁnite	O
hypothesis	B
classes	O
,	O
there	O
is	O
a	O
simple	O
trick	O
that	O
allows	O
us	O
to	O
get	O
a	O
very	O
good	O
estimate	O
of	O
the	O
practical	O
sample	B
complexity	I
of	O
inﬁnite	O
hypothesis	B
classes	O
.	O
consider	O
a	O
hypothesis	B
class	I
that	O
is	O
parameterized	O
by	O
d	O
parameters	O
.	O
for	O
example	O
,	O
let	O
x	O
=	O
r	O
,	O
y	O
=	O
{	O
±1	O
}	O
,	O
and	O
the	O
hypothesis	B
class	I
,	O
h	O
,	O
be	O
all	O
functions	O
of	O
the	O
form	O
hθ	O
(	O
x	O
)	O
=	O
sign	O
(	O
x	O
−	O
θ	O
)	O
.	O
that	O
is	O
,	O
each	O
hypothesis	B
is	O
parameterized	O
by	O
one	O
parameter	O
,	O
θ	O
∈	O
r	O
,	O
and	O
the	O
hypothesis	B
outputs	O
1	O
for	O
all	O
instances	O
larger	O
than	O
θ	O
and	O
outputs	O
−1	O
for	O
instances	O
smaller	O
than	O
θ.	O
this	O
is	O
a	O
hypothesis	B
class	I
of	O
an	O
inﬁnite	O
size	O
.	O
however	O
,	O
if	O
we	O
are	O
going	O
to	O
learn	O
this	O
hypothesis	B
class	I
in	O
practice	O
,	O
using	O
a	O
computer	O
,	O
we	O
will	O
probably	O
maintain	O
real	O
numbers	O
using	O
ﬂoating	O
point	O
representation	O
,	O
say	O
,	O
of	O
64	O
bits	O
.	O
it	O
follows	O
that	O
in	O
practice	O
,	O
our	O
hypothesis	B
class	I
is	O
parameterized	O
by	O
the	O
set	B
of	O
scalars	O
that	O
can	O
be	O
represented	O
using	O
a	O
64	O
bits	O
ﬂoating	O
point	O
number	O
.	O
there	O
are	O
at	O
most	O
264	O
such	O
numbers	O
;	O
hence	O
the	O
actual	O
size	O
of	O
our	O
hypothesis	B
class	I
is	O
at	O
most	O
264.	O
more	O
generally	O
,	O
if	O
our	O
hypothesis	B
class	I
is	O
parameterized	O
by	O
d	O
numbers	O
,	O
in	O
practice	O
we	O
learn	O
a	O
hypothesis	B
class	I
of	O
size	O
at	O
most	O
264d	O
.	O
applying	O
corollary	O
4.6	O
we	O
obtain	O
that	O
the	O
sample	B
complexity	I
of	O
such	O
58	O
learning	O
via	O
uniform	B
convergence	I
2	O
classes	O
is	O
bounded	O
by	O
128d+2	O
log	O
(	O
2/δ	O
)	O
.	O
this	O
upper	O
bound	O
on	O
the	O
sample	O
complex-	O
ity	O
has	O
the	O
deﬁciency	O
of	O
being	O
dependent	O
on	O
the	O
speciﬁc	O
representation	O
of	O
real	O
numbers	O
used	O
by	O
our	O
machine	O
.	O
in	O
chapter	O
6	O
we	O
will	O
introduce	O
a	O
rigorous	O
way	O
to	O
analyze	O
the	O
sample	B
complexity	I
of	O
inﬁnite	O
size	O
hypothesis	B
classes	O
.	O
neverthe-	O
less	O
,	O
the	O
discretization	B
trick	I
can	O
be	O
used	O
to	O
get	O
a	O
rough	O
estimate	O
of	O
the	O
sample	B
complexity	I
in	O
many	O
practical	O
situations	O
.	O
4.3	O
summary	O
if	O
the	O
uniform	B
convergence	I
property	O
holds	O
for	O
a	O
hypothesis	B
class	I
h	O
then	O
in	O
most	O
cases	O
the	O
empirical	O
risks	O
of	O
hypotheses	O
in	O
h	O
will	O
faithfully	O
represent	O
their	O
true	O
risks	O
.	O
uniform	B
convergence	I
suﬃces	O
for	O
agnostic	O
pac	O
learnability	O
using	O
the	O
erm	O
rule	O
.	O
we	O
have	O
shown	O
that	O
ﬁnite	O
hypothesis	B
classes	O
enjoy	O
the	O
uniform	B
convergence	I
property	O
and	O
are	O
hence	O
agnostic	O
pac	O
learnable	O
.	O
4.4	O
bibliographic	O
remarks	O
classes	O
of	O
functions	O
for	O
which	O
the	O
uniform	B
convergence	I
property	O
holds	O
are	O
also	O
called	O
glivenko-cantelli	O
classes	O
,	O
named	O
after	O
valery	O
ivanovich	O
glivenko	O
and	O
francesco	O
paolo	O
cantelli	O
,	O
who	O
proved	O
the	O
ﬁrst	O
uniform	B
convergence	I
result	O
in	O
the	O
1930s	O
.	O
see	O
(	O
dudley	O
,	O
gine	O
&	O
zinn	O
1991	O
)	O
.	O
the	O
relation	O
between	O
uniform	O
con-	O
vergence	O
and	O
learnability	O
was	O
thoroughly	O
studied	O
by	O
vapnik	O
–	O
see	O
(	O
vapnik	O
1992	O
,	O
vapnik	O
1995	O
,	O
vapnik	O
1998	O
)	O
.	O
in	O
fact	O
,	O
as	O
we	O
will	O
see	O
later	O
in	O
chapter	O
6	O
,	O
the	O
funda-	O
mental	O
theorem	O
of	O
learning	O
theory	O
states	O
that	O
in	O
binary	O
classiﬁcation	O
problems	O
,	O
uniform	B
convergence	I
is	O
not	O
only	O
a	O
suﬃcient	O
condition	O
for	O
learnability	O
but	O
is	O
also	O
a	O
necessary	O
condition	O
.	O
this	O
is	O
not	O
the	O
case	O
for	O
more	O
general	O
learning	O
problems	O
(	O
see	O
(	O
shalev-shwartz	O
,	O
shamir	O
,	O
srebro	O
&	O
sridharan	O
2010	O
)	O
)	O
.	O
4.5	O
exercises	O
1.	O
in	O
this	O
exercise	O
,	O
we	O
show	O
that	O
the	O
(	O
	O
,	O
δ	O
)	O
requirement	O
on	O
the	O
convergence	O
of	O
errors	O
in	O
our	O
deﬁnitions	O
of	O
pac	O
learning	O
,	O
is	O
,	O
in	O
fact	O
,	O
quite	O
close	O
to	O
a	O
sim-	O
pler	O
looking	O
requirement	O
about	O
averages	O
(	O
or	O
expectations	O
)	O
.	O
prove	O
that	O
the	O
following	O
two	O
statements	O
are	O
equivalent	O
(	O
for	O
any	O
learning	O
algorithm	O
a	O
,	O
any	O
probability	O
distribution	O
d	O
,	O
and	O
any	O
loss	B
function	I
whose	O
range	O
is	O
[	O
0	O
,	O
1	O
]	O
)	O
:	O
1.	O
for	O
every	O
	O
,	O
δ	O
>	O
0	O
,	O
there	O
exists	O
m	O
(	O
	O
,	O
δ	O
)	O
such	O
that	O
∀m	O
≥	O
m	O
(	O
	O
,	O
δ	O
)	O
2.	O
p	O
s∼dm	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
>	O
	O
]	O
<	O
δ	O
lim	O
m→∞	O
e	O
s∼dm	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
=	O
0	O
4.5	O
exercises	O
59	O
(	O
where	O
es∼dm	O
denotes	O
the	O
expectation	O
over	O
samples	O
s	O
of	O
size	O
m	O
)	O
.	O
2.	O
bounded	O
loss	B
functions	O
:	O
in	O
corollary	O
4.6	O
we	O
assumed	O
that	O
the	O
range	O
of	O
the	O
loss	B
function	I
is	O
[	O
0	O
,	O
1	O
]	O
.	O
prove	O
that	O
if	O
the	O
range	O
of	O
the	O
loss	B
function	I
is	O
[	O
a	O
,	O
b	O
]	O
then	O
the	O
sample	B
complexity	I
satisﬁes	O
(	O
cid:24	O
)	O
2	O
log	O
(	O
2|h|/δ	O
)	O
(	O
b	O
−	O
a	O
)	O
2	O
(	O
cid:25	O
)	O
mh	O
(	O
	O
,	O
δ	O
)	O
≤	O
much	O
(	O
/2	O
,	O
δ	O
)	O
≤	O
2	O
.	O
5	O
the	O
bias-complexity	B
tradeoﬀ	I
in	O
chapter	O
2	O
we	O
saw	O
that	O
unless	O
one	O
is	O
careful	O
,	O
the	O
training	O
data	O
can	O
mislead	O
the	O
learner	O
,	O
and	O
result	O
in	O
overﬁtting	B
.	O
to	O
overcome	O
this	O
problem	O
,	O
we	O
restricted	O
the	O
search	O
space	O
to	O
some	O
hypothesis	B
class	I
h.	O
such	O
a	O
hypothesis	B
class	I
can	O
be	O
viewed	O
as	O
reﬂecting	O
some	O
prior	B
knowledge	I
that	O
the	O
learner	O
has	O
about	O
the	O
task	O
–	O
a	O
belief	O
that	O
one	O
of	O
the	O
members	O
of	O
the	O
class	O
h	O
is	O
a	O
low-error	O
model	O
for	O
the	O
task	O
.	O
for	O
example	O
,	O
in	O
our	O
papayas	O
taste	O
problem	O
,	O
on	O
the	O
basis	O
of	O
our	O
previous	O
experience	O
with	O
other	O
fruits	O
,	O
we	O
may	O
assume	O
that	O
some	O
rectangle	O
in	O
the	O
color-hardness	O
plane	O
predicts	O
(	O
at	O
least	O
approximately	O
)	O
the	O
papaya	O
’	O
s	O
tastiness	O
.	O
is	O
such	O
prior	B
knowledge	I
really	O
necessary	O
for	O
the	O
success	O
of	O
learning	O
?	O
maybe	O
there	O
exists	O
some	O
kind	O
of	O
universal	O
learner	O
,	O
that	O
is	O
,	O
a	O
learner	O
who	O
has	O
no	O
prior	B
knowledge	I
about	O
a	O
certain	O
task	O
and	O
is	O
ready	O
to	O
be	O
challenged	O
by	O
any	O
task	O
?	O
let	O
us	O
elaborate	O
on	O
this	O
point	O
.	O
a	O
speciﬁc	O
learning	O
task	O
is	O
deﬁned	O
by	O
an	O
unknown	O
distribution	O
d	O
over	O
x	O
×	O
y	O
,	O
where	O
the	O
goal	O
of	O
the	O
learner	O
is	O
to	O
ﬁnd	O
a	O
predictor	B
h	O
:	O
x	O
→	O
y	O
,	O
whose	O
risk	B
,	O
ld	O
(	O
h	O
)	O
,	O
is	O
small	O
enough	O
.	O
the	O
question	O
is	O
therefore	O
whether	O
there	O
exist	O
a	O
learning	O
algorithm	O
a	O
and	O
a	O
training	B
set	I
size	O
m	O
,	O
such	O
that	O
for	O
every	O
distribution	O
d	O
,	O
if	O
a	O
receives	O
m	O
i.i.d	O
.	O
examples	O
from	O
d	O
,	O
there	O
is	O
a	O
high	O
chance	O
it	O
outputs	O
a	O
predictor	B
h	O
that	O
has	O
a	O
low	O
risk	B
.	O
the	O
ﬁrst	O
part	O
of	O
this	O
chapter	O
addresses	O
this	O
question	O
formally	O
.	O
the	O
no-free-	O
lunch	O
theorem	O
states	O
that	O
no	O
such	O
universal	O
learner	O
exists	O
.	O
to	O
be	O
more	O
precise	O
,	O
the	O
theorem	O
states	O
that	O
for	O
binary	O
classiﬁcation	O
prediction	O
tasks	O
,	O
for	O
every	O
learner	O
there	O
exists	O
a	O
distribution	O
on	O
which	O
it	O
fails	O
.	O
we	O
say	O
that	O
the	O
learner	O
fails	O
if	O
,	O
upon	O
receiving	O
i.i.d	O
.	O
examples	O
from	O
that	O
distribution	O
,	O
its	O
output	O
hypothesis	B
is	O
likely	O
to	O
have	O
a	O
large	O
risk	B
,	O
say	O
,	O
≥	O
0.3	O
,	O
whereas	O
for	O
the	O
same	O
distribution	O
,	O
there	O
exists	O
another	O
learner	O
that	O
will	O
output	O
a	O
hypothesis	B
with	O
a	O
small	O
risk	B
.	O
in	O
other	O
words	O
,	O
the	O
theorem	O
states	O
that	O
no	O
learner	O
can	O
succeed	O
on	O
all	O
learnable	O
tasks	O
–	O
every	O
learner	O
has	O
tasks	O
on	O
which	O
it	O
fails	O
while	O
other	O
learners	O
succeed	O
.	O
therefore	O
,	O
when	O
approaching	O
a	O
particular	O
learning	O
problem	O
,	O
deﬁned	O
by	O
some	O
distribution	O
d	O
,	O
we	O
should	O
have	O
some	O
prior	B
knowledge	I
on	O
d.	O
one	O
type	O
of	O
such	O
prior	B
knowledge	I
is	O
that	O
d	O
comes	O
from	O
some	O
speciﬁc	O
parametric	O
family	O
of	O
distributions	O
.	O
we	O
will	O
study	O
learning	O
under	O
such	O
assumptions	O
later	O
on	O
in	O
chapter	O
24.	O
another	O
type	O
of	O
prior	B
knowledge	I
on	O
d	O
,	O
which	O
we	O
assumed	O
when	O
deﬁning	O
the	O
pac	O
learning	O
model	O
,	O
is	O
that	O
there	O
exists	O
h	O
in	O
some	O
predeﬁned	O
hypothesis	B
class	I
h	O
,	O
such	O
that	O
ld	O
(	O
h	O
)	O
=	O
0.	O
a	O
softer	O
type	O
of	O
prior	B
knowledge	I
on	O
d	O
is	O
assuming	O
that	O
minh∈h	O
ld	O
(	O
h	O
)	O
is	O
small	O
.	O
in	O
a	O
sense	O
,	O
this	O
weaker	O
assumption	O
on	O
d	O
is	O
a	O
prerequisite	O
for	O
using	O
the	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
5.1	O
the	O
no-free-lunch	B
theorem	O
61	O
agnostic	O
pac	O
model	O
,	O
in	O
which	O
we	O
require	O
that	O
the	O
risk	B
of	O
the	O
output	O
hypothesis	B
will	O
not	O
be	O
much	O
larger	O
than	O
minh∈h	O
ld	O
(	O
h	O
)	O
.	O
in	O
the	O
second	O
part	O
of	O
this	O
chapter	O
we	O
study	O
the	O
beneﬁts	O
and	O
pitfalls	O
of	O
using	O
a	O
hypothesis	B
class	I
as	O
a	O
means	O
of	O
formalizing	O
prior	B
knowledge	I
.	O
we	O
decompose	O
the	O
error	O
of	O
an	O
erm	O
algorithm	O
over	O
a	O
class	O
h	O
into	O
two	O
components	O
.	O
the	O
ﬁrst	O
component	O
reﬂects	O
the	O
quality	O
of	O
our	O
prior	B
knowledge	I
,	O
measured	O
by	O
the	O
minimal	O
risk	B
of	O
a	O
hypothesis	B
in	O
our	O
hypothesis	B
class	I
,	O
minh∈h	O
ld	O
(	O
h	O
)	O
.	O
this	O
component	O
is	O
also	O
called	O
the	O
approximation	B
error	I
,	O
or	O
the	O
bias	B
of	O
the	O
algorithm	O
toward	O
choosing	O
a	O
hypothesis	B
from	O
h.	O
the	O
second	O
component	O
is	O
the	O
error	O
due	O
to	O
overﬁtting	B
,	O
which	O
depends	O
on	O
the	O
size	O
or	O
the	O
complexity	O
of	O
the	O
class	O
h	O
and	O
is	O
called	O
the	O
estimation	B
error	I
.	O
these	O
two	O
terms	O
imply	O
a	O
tradeoﬀ	O
between	O
choosing	O
a	O
more	O
complex	O
h	O
(	O
which	O
can	O
decrease	O
the	O
bias	B
but	O
increases	O
the	O
risk	B
of	O
overﬁtting	B
)	O
or	O
a	O
less	O
complex	O
h	O
(	O
which	O
might	O
increase	O
the	O
bias	B
but	O
decreases	O
the	O
potential	O
overﬁtting	B
)	O
.	O
5.1	O
the	O
no-free-lunch	B
theorem	O
in	O
this	O
part	O
we	O
prove	O
that	O
there	O
is	O
no	O
universal	O
learner	O
.	O
we	O
do	O
this	O
by	O
showing	O
that	O
no	O
learner	O
can	O
succeed	O
on	O
all	O
learning	O
tasks	O
,	O
as	O
formalized	O
in	O
the	O
following	O
theorem	O
:	O
theorem	O
5.1	O
(	O
no-free-lunch	B
)	O
let	O
a	O
be	O
any	O
learning	O
algorithm	O
for	O
the	O
task	O
of	O
binary	O
classiﬁcation	O
with	O
respect	O
to	O
the	O
0	O
−	O
1	O
loss	B
over	O
a	O
domain	B
x	O
.	O
let	O
m	O
be	O
any	O
number	O
smaller	O
than	O
|x|/2	O
,	O
representing	O
a	O
training	B
set	I
size	O
.	O
then	O
,	O
there	O
exists	O
a	O
distribution	O
d	O
over	O
x	O
×	O
{	O
0	O
,	O
1	O
}	O
such	O
that	O
:	O
1.	O
there	O
exists	O
a	O
function	B
f	O
:	O
x	O
→	O
{	O
0	O
,	O
1	O
}	O
with	O
ld	O
(	O
f	O
)	O
=	O
0	O
.	O
2.	O
with	O
probability	O
of	O
at	O
least	O
1/7	O
over	O
the	O
choice	O
of	O
s	O
∼	O
dm	O
we	O
have	O
that	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
≥	O
1/8	O
.	O
this	O
theorem	O
states	O
that	O
for	O
every	O
learner	O
,	O
there	O
exists	O
a	O
task	O
on	O
which	O
it	O
fails	O
,	O
even	O
though	O
that	O
task	O
can	O
be	O
successfully	O
learned	O
by	O
another	O
learner	O
.	O
indeed	O
,	O
a	O
trivial	O
successful	O
learner	O
in	O
this	O
case	O
would	O
be	O
an	O
erm	O
learner	O
with	O
the	O
hypoth-	O
esis	O
class	O
h	O
=	O
{	O
f	O
}	O
,	O
or	O
more	O
generally	O
,	O
erm	O
with	O
respect	O
to	O
any	O
ﬁnite	O
hypothesis	B
class	I
that	O
contains	O
f	O
and	O
whose	O
size	O
satisﬁes	O
the	O
equation	O
m	O
≥	O
8	O
log	O
(	O
7|h|/6	O
)	O
(	O
see	O
corollary	O
2.3	O
)	O
.	O
proof	O
let	O
c	O
be	O
a	O
subset	O
of	O
x	O
of	O
size	O
2m	O
.	O
the	O
intuition	O
of	O
the	O
proof	O
is	O
that	O
any	O
learning	O
algorithm	O
that	O
observes	O
only	O
half	O
of	O
the	O
instances	O
in	O
c	O
has	O
no	O
information	O
on	O
what	O
should	O
be	O
the	O
labels	O
of	O
the	O
rest	O
of	O
the	O
instances	O
in	O
c.	O
therefore	O
,	O
there	O
exists	O
a	O
“	O
reality	O
,	O
”	O
that	O
is	O
,	O
some	O
target	O
function	O
f	O
,	O
that	O
would	O
contradict	O
the	O
labels	O
that	O
a	O
(	O
s	O
)	O
predicts	O
on	O
the	O
unobserved	O
instances	O
in	O
c.	O
note	O
that	O
there	O
are	O
t	O
=	O
22m	O
possible	O
functions	O
from	O
c	O
to	O
{	O
0	O
,	O
1	O
}	O
.	O
denote	O
these	O
functions	O
by	O
f1	O
,	O
.	O
.	O
.	O
,	O
ft	O
.	O
for	O
each	O
such	O
function	B
,	O
let	O
di	O
be	O
a	O
distribution	O
over	O
62	O
the	O
bias-complexity	B
tradeoﬀ	I
c	O
×	O
{	O
0	O
,	O
1	O
}	O
deﬁned	O
by	O
(	O
cid:40	O
)	O
di	O
(	O
{	O
(	O
x	O
,	O
y	O
)	O
}	O
)	O
=	O
1/|c|	O
0	O
if	O
y	O
=	O
fi	O
(	O
x	O
)	O
otherwise	O
.	O
that	O
is	O
,	O
the	O
probability	O
to	O
choose	O
a	O
pair	O
(	O
x	O
,	O
y	O
)	O
is	O
1/|c|	O
if	O
the	O
label	B
y	O
is	O
indeed	O
the	O
true	O
label	O
according	O
to	O
fi	O
,	O
and	O
the	O
probability	O
is	O
0	O
if	O
y	O
(	O
cid:54	O
)	O
=	O
fi	O
(	O
x	O
)	O
.	O
clearly	O
,	O
ldi	O
(	O
fi	O
)	O
=	O
0.	O
we	O
will	O
show	O
that	O
for	O
every	O
algorithm	O
,	O
a	O
,	O
that	O
receives	O
a	O
training	B
set	I
of	O
m	O
examples	O
from	O
c	O
×	O
{	O
0	O
,	O
1	O
}	O
and	O
returns	O
a	O
function	B
a	O
(	O
s	O
)	O
:	O
c	O
→	O
{	O
0	O
,	O
1	O
}	O
,	O
it	O
holds	O
that	O
max	O
i∈	O
[	O
t	O
]	O
e	O
s∼dm	O
i	O
[	O
ldi	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
≥	O
1/4	O
.	O
(	O
5.1	O
)	O
clearly	O
,	O
this	O
means	O
that	O
for	O
every	O
algorithm	O
,	O
a	O
(	O
cid:48	O
)	O
,	O
that	O
receives	O
a	O
training	B
set	I
of	O
m	O
examples	O
from	O
x	O
×	O
{	O
0	O
,	O
1	O
}	O
there	O
exist	O
a	O
function	B
f	O
:	O
x	O
→	O
{	O
0	O
,	O
1	O
}	O
and	O
a	O
distribution	O
d	O
over	O
x	O
×	O
{	O
0	O
,	O
1	O
}	O
,	O
such	O
that	O
ld	O
(	O
f	O
)	O
=	O
0	O
and	O
e	O
[	O
ld	O
(	O
a	O
(	O
cid:48	O
)	O
(	O
s	O
)	O
)	O
]	O
≥	O
1/4	O
.	O
(	O
5.2	O
)	O
it	O
is	O
easy	O
to	O
verify	O
that	O
the	O
preceding	O
suﬃces	O
for	O
showing	O
that	O
p	O
[	O
ld	O
(	O
a	O
(	O
cid:48	O
)	O
(	O
s	O
)	O
)	O
≥	O
1/8	O
]	O
≥	O
1/7	O
,	O
which	O
is	O
what	O
we	O
need	O
to	O
prove	O
(	O
see	O
exercise	O
1	O
)	O
.	O
s∼dm	O
we	O
now	O
turn	O
to	O
proving	O
that	O
equation	O
(	O
5.1	O
)	O
holds	O
.	O
there	O
are	O
k	O
=	O
(	O
2m	O
)	O
m	O
possible	O
sequences	O
of	O
m	O
examples	O
from	O
c.	O
denote	O
these	O
sequences	O
by	O
s1	O
,	O
.	O
.	O
.	O
,	O
sk	O
.	O
also	O
,	O
if	O
sj	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
we	O
denote	O
by	O
si	O
j	O
the	O
sequence	O
containing	O
the	O
instances	O
in	O
sj	O
labeled	O
by	O
the	O
function	B
fi	O
,	O
namely	O
,	O
si	O
j	O
=	O
(	O
(	O
x1	O
,	O
fi	O
(	O
x1	O
)	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
fi	O
(	O
xm	O
)	O
)	O
)	O
.	O
if	O
the	O
distribution	O
is	O
di	O
then	O
the	O
possible	O
training	O
sets	O
a	O
can	O
receive	O
are	O
si	O
1	O
,	O
.	O
.	O
.	O
,	O
si	O
k	O
,	O
and	O
all	O
these	O
training	O
sets	O
have	O
the	O
same	O
probability	O
of	O
being	O
sampled	O
.	O
therefore	O
,	O
e	O
s∼dm	O
i	O
[	O
ldi	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
=	O
1	O
k	O
ldi	O
(	O
a	O
(	O
si	O
j	O
)	O
)	O
.	O
(	O
5.3	O
)	O
using	O
the	O
facts	O
that	O
“	O
maximum	O
”	O
is	O
larger	O
than	O
“	O
average	O
”	O
and	O
that	O
“	O
average	O
”	O
is	O
larger	O
than	O
“	O
minimum	O
,	O
”	O
we	O
have	O
k	O
(	O
cid:88	O
)	O
j=1	O
t	O
(	O
cid:88	O
)	O
k	O
(	O
cid:88	O
)	O
i=1	O
j=1	O
1	O
k	O
1	O
t	O
j=1	O
k	O
(	O
cid:88	O
)	O
t	O
(	O
cid:88	O
)	O
t	O
(	O
cid:88	O
)	O
i=1	O
i=1	O
≥	O
min	O
j∈	O
[	O
k	O
]	O
1	O
t	O
k	O
(	O
cid:88	O
)	O
j=1	O
max	O
i∈	O
[	O
t	O
]	O
1	O
k	O
ldi	O
(	O
a	O
(	O
si	O
j	O
)	O
)	O
≥	O
1	O
t	O
=	O
1	O
k	O
ldi	O
(	O
a	O
(	O
si	O
j	O
)	O
)	O
ldi	O
(	O
a	O
(	O
si	O
j	O
)	O
)	O
ldi	O
(	O
a	O
(	O
si	O
j	O
)	O
)	O
.	O
(	O
5.4	O
)	O
next	O
,	O
ﬁx	O
some	O
j	O
∈	O
[	O
k	O
]	O
.	O
denote	O
sj	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
and	O
let	O
v1	O
,	O
.	O
.	O
.	O
,	O
vp	O
be	O
the	O
examples	O
in	O
c	O
that	O
do	O
not	O
appear	O
in	O
sj	O
.	O
clearly	O
,	O
p	O
≥	O
m.	O
therefore	O
,	O
for	O
every	O
5.1	O
the	O
no-free-lunch	B
theorem	O
63	O
function	B
h	O
:	O
c	O
→	O
{	O
0	O
,	O
1	O
}	O
and	O
every	O
i	O
we	O
have	O
ldi	O
(	O
h	O
)	O
=	O
1	O
2m	O
x∈c	O
(	O
cid:88	O
)	O
p	O
(	O
cid:88	O
)	O
p	O
(	O
cid:88	O
)	O
r=1	O
r=1	O
≥	O
1	O
2m	O
≥	O
1	O
2p	O
1	O
[	O
h	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=fi	O
(	O
x	O
)	O
]	O
1	O
[	O
h	O
(	O
vr	O
)	O
(	O
cid:54	O
)	O
=fi	O
(	O
vr	O
)	O
]	O
1	O
[	O
h	O
(	O
vr	O
)	O
(	O
cid:54	O
)	O
=fi	O
(	O
vr	O
)	O
]	O
.	O
(	O
5.5	O
)	O
hence	O
,	O
t	O
(	O
cid:88	O
)	O
i=1	O
1	O
t	O
ldi	O
(	O
a	O
(	O
si	O
j	O
)	O
)	O
≥	O
1	O
t	O
=	O
1	O
2p	O
t	O
(	O
cid:88	O
)	O
p	O
(	O
cid:88	O
)	O
i=1	O
r=1	O
p	O
(	O
cid:88	O
)	O
t	O
(	O
cid:88	O
)	O
r=1	O
1	O
2p	O
i=1	O
1	O
t	O
1	O
[	O
a	O
(	O
si	O
j	O
)	O
(	O
vr	O
)	O
(	O
cid:54	O
)	O
=fi	O
(	O
vr	O
)	O
]	O
1	O
[	O
a	O
(	O
si	O
j	O
)	O
(	O
vr	O
)	O
(	O
cid:54	O
)	O
=fi	O
(	O
vr	O
)	O
]	O
t	O
(	O
cid:88	O
)	O
i=1	O
≥	O
1	O
2	O
·	O
min	O
r∈	O
[	O
p	O
]	O
1	O
t	O
1	O
[	O
a	O
(	O
si	O
j	O
)	O
(	O
vr	O
)	O
(	O
cid:54	O
)	O
=fi	O
(	O
vr	O
)	O
]	O
.	O
(	O
5.6	O
)	O
next	O
,	O
ﬁx	O
some	O
r	O
∈	O
[	O
p	O
]	O
.	O
we	O
can	O
partition	O
all	O
the	O
functions	O
in	O
f1	O
,	O
.	O
.	O
.	O
,	O
ft	O
into	O
t	O
/2	O
disjoint	O
pairs	O
,	O
where	O
for	O
a	O
pair	O
(	O
fi	O
,	O
fi	O
(	O
cid:48	O
)	O
)	O
we	O
have	O
that	O
for	O
every	O
c	O
∈	O
c	O
,	O
fi	O
(	O
c	O
)	O
(	O
cid:54	O
)	O
=	O
fi	O
(	O
cid:48	O
)	O
(	O
c	O
)	O
if	O
and	O
only	O
if	O
c	O
=	O
vr	O
.	O
since	O
for	O
such	O
a	O
pair	O
we	O
must	O
have	O
si	O
j	O
,	O
it	O
follows	O
that	O
j	O
=	O
si	O
(	O
cid:48	O
)	O
1	O
[	O
a	O
(	O
si	O
j	O
)	O
(	O
vr	O
)	O
(	O
cid:54	O
)	O
=fi	O
(	O
vr	O
)	O
]	O
+	O
1	O
[	O
a	O
(	O
si	O
(	O
cid:48	O
)	O
j	O
)	O
(	O
vr	O
)	O
(	O
cid:54	O
)	O
=fi	O
(	O
cid:48	O
)	O
(	O
vr	O
)	O
]	O
=	O
1	O
,	O
which	O
yields	O
t	O
(	O
cid:88	O
)	O
i=1	O
1	O
t	O
1	O
[	O
a	O
(	O
si	O
j	O
)	O
(	O
vr	O
)	O
(	O
cid:54	O
)	O
=fi	O
(	O
vr	O
)	O
]	O
=	O
1	O
2	O
.	O
combining	O
this	O
with	O
equation	O
(	O
5.6	O
)	O
,	O
equation	O
(	O
5.4	O
)	O
,	O
and	O
equation	O
(	O
5.3	O
)	O
,	O
we	O
obtain	O
that	O
equation	O
(	O
5.1	O
)	O
holds	O
,	O
which	O
concludes	O
our	O
proof	O
.	O
5.1.1	O
no-free-lunch	B
and	O
prior	B
knowledge	I
how	O
does	O
the	O
no-free-lunch	B
result	O
relate	O
to	O
the	O
need	O
for	O
prior	B
knowledge	I
?	O
let	O
us	O
consider	O
an	O
erm	O
predictor	B
over	O
the	O
hypothesis	B
class	I
h	O
of	O
all	O
the	O
functions	O
f	O
from	O
x	O
to	O
{	O
0	O
,	O
1	O
}	O
.	O
this	O
class	O
represents	O
lack	O
of	O
prior	B
knowledge	I
:	O
every	O
possible	O
function	B
from	O
the	O
domain	B
to	O
the	O
label	B
set	O
is	O
considered	O
a	O
good	O
candidate	O
.	O
according	O
to	O
the	O
no-free-lunch	B
theorem	O
,	O
any	O
algorithm	O
that	O
chooses	O
its	O
output	O
from	O
hypotheses	O
in	O
h	O
,	O
and	O
in	O
particular	O
the	O
erm	O
predictor	B
,	O
will	O
fail	O
on	O
some	O
learning	O
task	O
.	O
therefore	O
,	O
this	O
class	O
is	O
not	O
pac	O
learnable	O
,	O
as	O
formalized	O
in	O
the	O
following	O
corollary	O
:	O
corollary	O
5.2	O
let	O
x	O
be	O
an	O
inﬁnite	O
domain	B
set	O
and	O
let	O
h	O
be	O
the	O
set	B
of	O
all	O
functions	O
from	O
x	O
to	O
{	O
0	O
,	O
1	O
}	O
.	O
then	O
,	O
h	O
is	O
not	O
pac	O
learnable	O
.	O
64	O
the	O
bias-complexity	B
tradeoﬀ	I
proof	O
assume	O
,	O
by	O
way	O
of	O
contradiction	O
,	O
that	O
the	O
class	O
is	O
learnable	O
.	O
choose	O
some	O
	O
<	O
1/8	O
and	O
δ	O
<	O
1/7	O
.	O
by	O
the	O
deﬁnition	O
of	O
pac	O
learnability	O
,	O
there	O
must	O
be	O
some	O
learning	O
algorithm	O
a	O
and	O
an	O
integer	O
m	O
=	O
m	O
(	O
	O
,	O
δ	O
)	O
,	O
such	O
that	O
for	O
any	O
data-generating	O
distribution	O
over	O
x	O
×	O
{	O
0	O
,	O
1	O
}	O
,	O
if	O
for	O
some	O
function	B
f	O
:	O
x	O
→	O
{	O
0	O
,	O
1	O
}	O
,	O
ld	O
(	O
f	O
)	O
=	O
0	O
,	O
then	O
with	O
probability	O
greater	O
than	O
1	O
−	O
δ	O
when	O
a	O
is	O
applied	O
to	O
samples	O
s	O
of	O
size	O
m	O
,	O
generated	O
i.i.d	O
.	O
by	O
d	O
,	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
	O
.	O
however	O
,	O
applying	O
the	O
no-free-lunch	B
theorem	O
,	O
since	O
|x|	O
>	O
2m	O
,	O
for	O
every	O
learning	O
algorithm	O
(	O
and	O
in	O
particular	O
for	O
the	O
algorithm	O
a	O
)	O
,	O
there	O
exists	O
a	O
distribution	O
d	O
such	O
that	O
with	O
probability	O
greater	O
than	O
1/7	O
>	O
δ	O
,	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
>	O
1/8	O
>	O
	O
,	O
which	O
leads	O
to	O
the	O
desired	O
contradiction	O
.	O
how	O
can	O
we	O
prevent	O
such	O
failures	O
?	O
we	O
can	O
escape	O
the	O
hazards	O
foreseen	O
by	O
the	O
no-free-lunch	B
theorem	O
by	O
using	O
our	O
prior	B
knowledge	I
about	O
a	O
speciﬁc	O
learning	O
task	O
,	O
to	O
avoid	O
the	O
distributions	O
that	O
will	O
cause	O
us	O
to	O
fail	O
when	O
learning	O
that	O
task	O
.	O
such	O
prior	B
knowledge	I
can	O
be	O
expressed	O
by	O
restricting	O
our	O
hypothesis	B
class	I
.	O
but	O
how	O
should	O
we	O
choose	O
a	O
good	O
hypothesis	B
class	I
?	O
on	O
the	O
one	O
hand	O
,	O
we	O
want	O
to	O
believe	O
that	O
this	O
class	O
includes	O
the	O
hypothesis	B
that	O
has	O
no	O
error	O
at	O
all	O
(	O
in	O
the	O
pac	O
setting	O
)	O
,	O
or	O
at	O
least	O
that	O
the	O
smallest	O
error	O
achievable	O
by	O
a	O
hypothesis	B
from	O
this	O
class	O
is	O
indeed	O
rather	O
small	O
(	O
in	O
the	O
agnostic	O
setting	O
)	O
.	O
on	O
the	O
other	O
hand	O
,	O
we	O
have	O
just	O
seen	O
that	O
we	O
can	O
not	O
simply	O
choose	O
the	O
richest	O
class	O
–	O
the	O
class	O
of	O
all	O
functions	O
over	O
the	O
given	O
domain	B
.	O
this	O
tradeoﬀ	O
is	O
discussed	O
in	O
the	O
following	O
section	O
.	O
5.2	O
error	B
decomposition	I
to	O
answer	O
this	O
question	O
we	O
decompose	O
the	O
error	O
of	O
an	O
ermh	O
predictor	B
into	O
two	O
components	O
as	O
follows	O
.	O
let	O
hs	O
be	O
an	O
ermh	O
hypothesis	B
.	O
then	O
,	O
we	O
can	O
write	O
ld	O
(	O
hs	O
)	O
=	O
app	O
+	O
est	O
where	O
:	O
app	O
=	O
min	O
h∈h	O
ld	O
(	O
h	O
)	O
,	O
est	O
=	O
ld	O
(	O
hs	O
)	O
−	O
app	O
.	O
(	O
5.7	O
)	O
•	O
the	O
approximation	B
error	I
–	O
the	O
minimum	O
risk	B
achievable	O
by	O
a	O
predictor	B
in	O
the	O
hypothesis	B
class	I
.	O
this	O
term	O
measures	O
how	O
much	O
risk	B
we	O
have	O
because	O
we	O
restrict	O
ourselves	O
to	O
a	O
speciﬁc	O
class	O
,	O
namely	O
,	O
how	O
much	O
inductive	B
bias	I
we	O
have	O
.	O
the	O
approximation	B
error	I
does	O
not	O
depend	O
on	O
the	O
sample	O
size	O
and	O
is	O
determined	O
by	O
the	O
hypothesis	B
class	I
chosen	O
.	O
enlarging	O
the	O
hypothesis	B
class	I
can	O
decrease	O
the	O
approximation	B
error	I
.	O
under	O
the	O
realizability	B
assumption	O
,	O
the	O
approximation	B
error	I
is	O
zero	O
.	O
in	O
the	O
agnostic	O
case	O
,	O
however	O
,	O
the	O
approximation	B
error	I
can	O
be	O
large.1	O
1	O
in	O
fact	O
,	O
it	O
always	O
includes	O
the	O
error	O
of	O
the	O
bayes	O
optimal	O
predictor	B
(	O
see	O
chapter	O
3	O
)	O
,	O
the	O
minimal	O
yet	O
inevitable	O
error	O
,	O
because	O
of	O
the	O
possible	O
nondeterminism	O
of	O
the	O
world	O
in	O
this	O
model	O
.	O
sometimes	O
in	O
the	O
literature	O
the	O
term	O
approximation	B
error	I
refers	O
not	O
to	O
minh∈h	O
ld	O
(	O
h	O
)	O
,	O
but	O
rather	O
to	O
the	O
excess	O
error	O
over	O
that	O
of	O
the	O
bayes	O
optimal	O
predictor	B
,	O
namely	O
,	O
minh∈h	O
ld	O
(	O
h	O
)	O
−	O
bayes	O
.	O
5.3	O
summary	O
65	O
•	O
the	O
estimation	B
error	I
–	O
the	O
diﬀerence	O
between	O
the	O
approximation	B
error	I
and	O
the	O
error	O
achieved	O
by	O
the	O
erm	O
predictor	B
.	O
the	O
estimation	B
error	I
results	O
because	O
the	O
empirical	B
risk	I
(	O
i.e.	O
,	O
training	B
error	I
)	O
is	O
only	O
an	O
estimate	O
of	O
the	O
true	O
risk	O
,	O
and	O
so	O
the	O
predictor	B
minimizing	O
the	O
empirical	B
risk	I
is	O
only	O
an	O
estimate	O
of	O
the	O
predictor	B
minimizing	O
the	O
true	O
risk	O
.	O
the	O
quality	O
of	O
this	O
estimation	O
depends	O
on	O
the	O
training	B
set	I
size	O
and	O
on	O
the	O
size	O
,	O
or	O
complexity	O
,	O
of	O
the	O
hypothesis	B
class	I
.	O
as	O
we	O
have	O
shown	O
,	O
for	O
a	O
ﬁnite	O
hypothesis	B
class	I
,	O
est	O
increases	O
(	O
logarithmically	O
)	O
with	O
|h|	O
and	O
de-	O
creases	O
with	O
m.	O
we	O
can	O
think	O
of	O
the	O
size	O
of	O
h	O
as	O
a	O
measure	O
of	O
its	O
complexity	O
.	O
in	O
future	O
chapters	O
we	O
will	O
deﬁne	O
other	O
complexity	O
measures	O
of	O
hypothesis	B
classes	O
.	O
since	O
our	O
goal	O
is	O
to	O
minimize	O
the	O
total	O
risk	B
,	O
we	O
face	O
a	O
tradeoﬀ	O
,	O
called	O
the	O
bias-	O
complexity	O
tradeoﬀ	O
.	O
on	O
one	O
hand	O
,	O
choosing	O
h	O
to	O
be	O
a	O
very	O
rich	O
class	O
decreases	O
the	O
approximation	B
error	I
but	O
at	O
the	O
same	O
time	O
might	O
increase	O
the	O
estimation	B
error	I
,	O
as	O
a	O
rich	O
h	O
might	O
lead	O
to	O
overﬁtting	B
.	O
on	O
the	O
other	O
hand	O
,	O
choosing	O
h	O
to	O
be	O
a	O
very	O
small	O
set	B
reduces	O
the	O
estimation	B
error	I
but	O
might	O
increase	O
the	O
approximation	B
error	I
or	O
,	O
in	O
other	O
words	O
,	O
might	O
lead	O
to	O
underﬁtting	B
.	O
of	O
course	O
,	O
a	O
great	O
choice	O
for	O
h	O
is	O
the	O
class	O
that	O
contains	O
only	O
one	O
classiﬁer	B
–	O
the	O
bayes	O
optimal	O
classiﬁer	B
.	O
but	O
the	O
bayes	O
optimal	O
classiﬁer	B
depends	O
on	O
the	O
underlying	O
distribution	O
d	O
,	O
which	O
we	O
do	O
not	O
know	O
(	O
indeed	O
,	O
learning	O
would	O
have	O
been	O
unnecessary	O
had	O
we	O
known	O
d	O
)	O
.	O
learning	O
theory	O
studies	O
how	O
rich	O
we	O
can	O
make	O
h	O
while	O
still	O
maintaining	O
rea-	O
sonable	O
estimation	B
error	I
.	O
in	O
many	O
cases	O
,	O
empirical	O
research	O
focuses	O
on	O
designing	O
good	O
hypothesis	B
classes	O
for	O
a	O
certain	O
domain	B
.	O
here	O
,	O
“	O
good	O
”	O
means	O
classes	O
for	O
which	O
the	O
approximation	B
error	I
would	O
not	O
be	O
excessively	O
high	O
.	O
the	O
idea	O
is	O
that	O
although	O
we	O
are	O
not	O
experts	O
and	O
do	O
not	O
know	O
how	O
to	O
construct	O
the	O
optimal	O
clas-	O
siﬁer	O
,	O
we	O
still	O
have	O
some	O
prior	B
knowledge	I
of	O
the	O
speciﬁc	O
problem	O
at	O
hand	O
,	O
which	O
enables	O
us	O
to	O
design	O
hypothesis	B
classes	O
for	O
which	O
both	O
the	O
approximation	B
error	I
and	O
the	O
estimation	B
error	I
are	O
not	O
too	O
large	O
.	O
getting	O
back	O
to	O
our	O
papayas	O
example	O
,	O
we	O
do	O
not	O
know	O
how	O
exactly	O
the	O
color	O
and	O
hardness	O
of	O
a	O
papaya	O
predict	O
its	O
taste	O
,	O
but	O
we	O
do	O
know	O
that	O
papaya	O
is	O
a	O
fruit	O
and	O
on	O
the	O
basis	O
of	O
previous	O
experience	O
with	O
other	O
fruit	O
we	O
conjecture	O
that	O
a	O
rectangle	O
in	O
the	O
color-hardness	O
space	O
may	O
be	O
a	O
good	O
predictor	B
.	O
5.3	O
summary	O
the	O
no-free-lunch	B
theorem	O
states	O
that	O
there	O
is	O
no	O
universal	O
learner	O
.	O
every	O
learner	O
has	O
to	O
be	O
speciﬁed	O
to	O
some	O
task	O
,	O
and	O
use	O
some	O
prior	B
knowledge	I
about	O
that	O
task	O
,	O
in	O
order	O
to	O
succeed	O
.	O
so	O
far	O
we	O
have	O
modeled	O
our	O
prior	B
knowledge	I
by	O
restricting	O
our	O
output	O
hypothesis	B
to	O
be	O
a	O
member	O
of	O
a	O
chosen	O
hypothesis	B
class	I
.	O
when	O
choosing	O
this	O
hypothesis	B
class	I
,	O
we	O
face	O
a	O
tradeoﬀ	O
,	O
between	O
a	O
larger	O
,	O
or	O
more	O
complex	O
,	O
class	O
that	O
is	O
more	O
likely	O
to	O
have	O
a	O
small	O
approximation	B
error	I
,	O
and	O
a	O
more	O
restricted	O
class	O
that	O
would	O
guarantee	O
that	O
the	O
estimation	B
error	I
will	O
66	O
the	O
bias-complexity	B
tradeoﬀ	I
be	O
small	O
.	O
in	O
the	O
next	O
chapter	O
we	O
will	O
study	O
in	O
more	O
detail	O
the	O
behavior	O
of	O
the	O
estimation	B
error	I
.	O
in	O
chapter	O
7	O
we	O
will	O
discuss	O
alternative	O
ways	O
to	O
express	O
prior	B
knowledge	I
.	O
5.4	O
bibliographic	O
remarks	O
(	O
wolpert	O
&	O
macready	O
1997	O
)	O
proved	O
several	O
no-free-lunch	B
theorems	O
for	O
optimiza-	O
tion	O
,	O
but	O
these	O
are	O
rather	O
diﬀerent	O
from	O
the	O
theorem	O
we	O
prove	O
here	O
.	O
the	O
theorem	O
we	O
prove	O
here	O
is	O
closely	O
related	O
to	O
lower	O
bounds	O
in	O
vc	O
theory	O
,	O
as	O
we	O
will	O
study	O
in	O
the	O
next	O
chapter	O
.	O
5.5	O
exercises	O
1.	O
prove	O
that	O
equation	O
(	O
5.2	O
)	O
suﬃces	O
for	O
showing	O
that	O
p	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
≥	O
1/8	O
]	O
≥	O
1/7	O
.	O
hint	O
:	O
let	O
θ	O
be	O
a	O
random	O
variable	O
that	O
receives	O
values	O
in	O
[	O
0	O
,	O
1	O
]	O
and	O
whose	O
expectation	O
satisﬁes	O
e	O
[	O
θ	O
]	O
≥	O
1/4	O
.	O
use	O
lemma	O
b.1	O
to	O
show	O
that	O
p	O
[	O
θ	O
≥	O
1/8	O
]	O
≥	O
1/7	O
.	O
2.	O
assume	O
you	O
are	O
asked	O
to	O
design	O
a	O
learning	O
algorithm	O
to	O
predict	O
whether	O
pa-	O
tients	O
are	O
going	O
to	O
suﬀer	O
a	O
heart	O
attack	O
.	O
relevant	O
patient	O
features	O
the	O
al-	O
gorithm	O
may	O
have	O
access	O
to	O
include	O
blood	O
pressure	O
(	O
bp	O
)	O
,	O
body-mass	O
index	O
(	O
bmi	O
)	O
,	O
age	O
(	O
a	O
)	O
,	O
level	O
of	O
physical	O
activity	O
(	O
p	O
)	O
,	O
and	O
income	O
(	O
i	O
)	O
.	O
your	O
choice	O
.	O
you	O
have	O
to	O
choose	O
between	O
two	O
algorithms	O
;	O
the	O
ﬁrst	O
picks	O
an	O
axis	O
aligned	O
rectangle	O
in	O
the	O
two	O
dimensional	O
space	O
spanned	O
by	O
the	O
features	O
bp	O
and	O
bmi	O
and	O
the	O
other	O
picks	O
an	O
axis	O
aligned	O
rectangle	O
in	O
the	O
ﬁve	O
dimensional	O
space	O
spanned	O
by	O
all	O
the	O
preceding	O
features	O
.	O
1.	O
explain	O
the	O
pros	O
and	O
cons	O
of	O
each	O
choice	O
.	O
2.	O
explain	O
how	O
the	O
number	O
of	O
available	O
labeled	O
training	O
samples	O
will	O
aﬀect	O
3.	O
prove	O
that	O
if	O
|x|	O
≥	O
km	O
for	O
a	O
positive	O
integer	O
k	O
≥	O
2	O
,	O
then	O
we	O
can	O
replace	O
2	O
−	O
1	O
the	O
lower	O
bound	O
of	O
1/4	O
in	O
the	O
no-free-lunch	B
theorem	O
with	O
k−1	O
2k	O
.	O
namely	O
,	O
let	O
a	O
be	O
a	O
learning	O
algorithm	O
for	O
the	O
task	O
of	O
binary	O
classiﬁcation	O
.	O
let	O
m	O
be	O
any	O
number	O
smaller	O
than	O
|x|/k	O
,	O
representing	O
a	O
training	B
set	I
size	O
.	O
then	O
,	O
there	O
exists	O
a	O
distribution	O
d	O
over	O
x	O
×	O
{	O
0	O
,	O
1	O
}	O
such	O
that	O
:	O
•	O
there	O
exists	O
a	O
function	B
f	O
:	O
x	O
→	O
{	O
0	O
,	O
1	O
}	O
with	O
ld	O
(	O
f	O
)	O
=	O
0	O
.	O
•	O
es∼dm	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
≥	O
1	O
2k	O
=	O
1	O
2	O
−	O
1	O
2k	O
.	O
6	O
the	O
vc-dimension	O
in	O
the	O
previous	O
chapter	O
,	O
we	O
decomposed	O
the	O
error	O
of	O
the	O
ermh	O
rule	O
into	O
ap-	O
proximation	O
error	O
and	O
estimation	B
error	I
.	O
the	O
approximation	B
error	I
depends	O
on	O
the	O
ﬁt	O
of	O
our	O
prior	B
knowledge	I
(	O
as	O
reﬂected	O
by	O
the	O
choice	O
of	O
the	O
hypothesis	B
class	I
h	O
)	O
to	O
the	O
underlying	O
unknown	O
distribution	O
.	O
in	O
contrast	O
,	O
the	O
deﬁnition	O
of	O
pac	O
learnability	O
requires	O
that	O
the	O
estimation	B
error	I
would	O
be	O
bounded	O
uniformly	O
over	O
all	O
distributions	O
.	O
our	O
current	O
goal	O
is	O
to	O
ﬁgure	O
out	O
which	O
classes	O
h	O
are	O
pac	O
learnable	O
,	O
and	O
to	O
characterize	O
exactly	O
the	O
sample	B
complexity	I
of	O
learning	O
a	O
given	O
hypothesis	B
class	I
.	O
so	O
far	O
we	O
have	O
seen	O
that	O
ﬁnite	O
classes	O
are	O
learnable	O
,	O
but	O
that	O
the	O
class	O
of	O
all	O
functions	O
(	O
over	O
an	O
inﬁnite	O
size	O
domain	B
)	O
is	O
not	O
.	O
what	O
makes	O
one	O
class	O
learnable	O
and	O
the	O
other	O
unlearnable	O
?	O
can	O
inﬁnite-size	O
classes	O
be	O
learnable	O
,	O
and	O
,	O
if	O
so	O
,	O
what	O
determines	O
their	O
sample	B
complexity	I
?	O
we	O
begin	O
the	O
chapter	O
by	O
showing	O
that	O
inﬁnite	O
classes	O
can	O
indeed	O
be	O
learn-	O
able	O
,	O
and	O
thus	O
,	O
ﬁniteness	O
of	O
the	O
hypothesis	B
class	I
is	O
not	O
a	O
necessary	O
condition	O
for	O
learnability	O
.	O
we	O
then	O
present	O
a	O
remarkably	O
crisp	O
characterization	O
of	O
the	O
family	O
of	O
learnable	O
classes	O
in	O
the	O
setup	O
of	O
binary	O
valued	O
classiﬁcation	O
with	O
the	O
zero-one	O
loss	B
.	O
this	O
characterization	O
was	O
ﬁrst	O
discovered	O
by	O
vladimir	O
vapnik	O
and	O
alexey	O
chervonenkis	O
in	O
1970	O
and	O
relies	O
on	O
a	O
combinatorial	O
notion	O
called	O
the	O
vapnik-	O
chervonenkis	O
dimension	B
(	O
vc-dimension	O
)	O
.	O
we	O
formally	O
deﬁne	O
the	O
vc-dimension	O
,	O
provide	O
several	O
examples	O
,	O
and	O
then	O
state	O
the	O
fundamental	O
theorem	O
of	O
statistical	O
learning	O
theory	O
,	O
which	O
integrates	O
the	O
concepts	O
of	O
learnability	O
,	O
vc-dimension	O
,	O
the	O
erm	O
rule	O
,	O
and	O
uniform	B
convergence	I
.	O
6.1	O
inﬁnite-size	O
classes	O
can	O
be	O
learnable	O
in	O
chapter	O
4	O
we	O
saw	O
that	O
ﬁnite	O
classes	O
are	O
learnable	O
,	O
and	O
in	O
fact	O
the	O
sample	B
complexity	I
of	O
a	O
hypothesis	B
class	I
is	O
upper	O
bounded	O
by	O
the	O
log	O
of	O
its	O
size	O
.	O
to	O
show	O
that	O
the	O
size	O
of	O
the	O
hypothesis	B
class	I
is	O
not	O
the	O
right	O
characterization	O
of	O
its	O
sample	B
complexity	I
,	O
we	O
ﬁrst	O
present	O
a	O
simple	O
example	O
of	O
an	O
inﬁnite-size	O
hypothesis	B
class	I
that	O
is	O
learnable	O
.	O
example	O
6.1	O
let	O
h	O
be	O
the	O
set	B
of	O
threshold	O
functions	O
over	O
the	O
real	O
line	O
,	O
namely	O
,	O
h	O
=	O
{	O
ha	O
:	O
a	O
∈	O
r	O
}	O
,	O
where	O
ha	O
:	O
r	O
→	O
{	O
0	O
,	O
1	O
}	O
is	O
a	O
function	B
such	O
that	O
ha	O
(	O
x	O
)	O
=	O
1	O
[	O
x	O
<	O
a	O
]	O
.	O
to	O
remind	O
the	O
reader	O
,	O
1	O
[	O
x	O
<	O
a	O
]	O
is	O
1	O
if	O
x	O
<	O
a	O
and	O
0	O
otherwise	O
.	O
clearly	O
,	O
h	O
is	O
of	O
inﬁnite	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
68	O
the	O
vc-dimension	O
size	O
.	O
nevertheless	O
,	O
the	O
following	O
lemma	O
shows	O
that	O
h	O
is	O
learnable	O
in	O
the	O
pac	O
model	O
using	O
the	O
erm	O
algorithm	O
.	O
lemma	O
6.1	O
let	O
h	O
be	O
the	O
class	O
of	O
thresholds	O
as	O
deﬁned	O
earlier	O
.	O
then	O
,	O
h	O
is	O
pac	O
learnable	O
,	O
using	O
the	O
erm	O
rule	O
,	O
with	O
sample	B
complexity	I
of	O
mh	O
(	O
	O
,	O
δ	O
)	O
≤	O
(	O
cid:100	O
)	O
log	O
(	O
2/δ	O
)	O
/	O
(	O
cid:101	O
)	O
.	O
proof	O
let	O
a	O
(	O
cid:63	O
)	O
be	O
a	O
threshold	O
such	O
that	O
the	O
hypothesis	B
h	O
(	O
cid:63	O
)	O
(	O
x	O
)	O
=	O
1	O
[	O
x	O
<	O
a	O
(	O
cid:63	O
)	O
]	O
achieves	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
=	O
0.	O
let	O
dx	O
be	O
the	O
marginal	O
distribution	O
over	O
the	O
domain	B
x	O
and	O
let	O
a0	O
<	O
a	O
(	O
cid:63	O
)	O
<	O
a1	O
be	O
such	O
that	O
p	O
x∼dx	O
[	O
x	O
∈	O
(	O
a0	O
,	O
a	O
(	O
cid:63	O
)	O
)	O
]	O
=	O
p	O
x∼dx	O
[	O
x	O
∈	O
(	O
a	O
(	O
cid:63	O
)	O
,	O
a1	O
)	O
]	O
=	O
	O
.	O
	O
mass	O
	O
mass	O
a0	O
a	O
(	O
cid:63	O
)	O
a1	O
(	O
if	O
dx	O
(	O
−∞	O
,	O
a	O
(	O
cid:63	O
)	O
)	O
≤	O
	O
we	O
set	B
a0	O
=	O
−∞	O
and	O
similarly	O
for	O
a1	O
)	O
.	O
given	O
a	O
training	B
set	I
s	O
,	O
let	O
b0	O
=	O
max	O
{	O
x	O
:	O
(	O
x	O
,	O
1	O
)	O
∈	O
s	O
}	O
and	O
b1	O
=	O
min	O
{	O
x	O
:	O
(	O
x	O
,	O
0	O
)	O
∈	O
s	O
}	O
(	O
if	O
no	O
example	O
in	O
s	O
is	O
positive	O
we	O
set	B
b0	O
=	O
−∞	O
and	O
if	O
no	O
example	O
in	O
s	O
is	O
negative	O
we	O
set	B
b1	O
=	O
∞	O
)	O
.	O
let	O
bs	O
be	O
a	O
threshold	O
corresponding	O
to	O
an	O
erm	O
hypothesis	B
,	O
hs	O
,	O
which	O
implies	O
that	O
bs	O
∈	O
(	O
b0	O
,	O
b1	O
)	O
.	O
therefore	O
,	O
a	O
suﬃcient	O
condition	O
for	O
ld	O
(	O
hs	O
)	O
≤	O
	O
is	O
that	O
both	O
b0	O
≥	O
a0	O
and	O
b1	O
≤	O
a1	O
.	O
in	O
other	O
words	O
,	O
p	O
s∼dm	O
[	O
ld	O
(	O
hs	O
)	O
>	O
	O
]	O
≤	O
p	O
s∼dm	O
[	O
b0	O
<	O
a0	O
∨	O
b1	O
>	O
a1	O
]	O
,	O
and	O
using	O
the	O
union	B
bound	I
we	O
can	O
bound	O
the	O
preceding	O
by	O
p	O
s∼dm	O
[	O
ld	O
(	O
hs	O
)	O
>	O
	O
]	O
≤	O
p	O
s∼dm	O
[	O
b0	O
<	O
a0	O
]	O
+	O
p	O
s∼dm	O
[	O
b1	O
>	O
a1	O
]	O
.	O
(	O
6.1	O
)	O
the	O
event	O
b0	O
<	O
a0	O
happens	O
if	O
and	O
only	O
if	O
all	O
examples	O
in	O
s	O
are	O
not	O
in	O
the	O
interval	O
(	O
a0	O
,	O
a∗	O
)	O
,	O
whose	O
probability	O
mass	O
is	O
deﬁned	O
to	O
be	O
	O
,	O
namely	O
,	O
p	O
s∼dm	O
[	O
b0	O
<	O
a0	O
]	O
=	O
p	O
s∼dm	O
[	O
∀	O
(	O
x	O
,	O
y	O
)	O
∈	O
s	O
,	O
x	O
(	O
cid:54	O
)	O
∈	O
(	O
a0	O
,	O
a	O
(	O
cid:63	O
)	O
)	O
]	O
=	O
(	O
1	O
−	O
	O
)	O
m	O
≤	O
e−	O
m.	O
since	O
we	O
assume	O
m	O
>	O
log	O
(	O
2/δ	O
)	O
/	O
it	O
follows	O
that	O
the	O
equation	O
is	O
at	O
most	O
δ/2	O
.	O
in	O
the	O
same	O
way	O
it	O
is	O
easy	O
to	O
see	O
that	O
ps∼dm	O
[	O
b1	O
>	O
a1	O
]	O
≤	O
δ/2	O
.	O
combining	O
with	O
equation	O
(	O
6.1	O
)	O
we	O
conclude	O
our	O
proof	O
.	O
6.2	O
the	O
vc-dimension	O
we	O
see	O
,	O
therefore	O
,	O
that	O
while	O
ﬁniteness	O
of	O
h	O
is	O
a	O
suﬃcient	O
condition	O
for	O
learn-	O
ability	O
,	O
it	O
is	O
not	O
a	O
necessary	O
condition	O
.	O
as	O
we	O
will	O
show	O
,	O
a	O
property	O
called	O
the	O
vc-dimension	O
of	O
a	O
hypothesis	B
class	I
gives	O
the	O
correct	O
characterization	O
of	O
its	O
learn-	O
ability	O
.	O
to	O
motivate	O
the	O
deﬁnition	O
of	O
the	O
vc-dimension	O
,	O
let	O
us	O
recall	B
the	O
no-free-	O
lunch	O
theorem	O
(	O
theorem	O
5.1	O
)	O
and	O
its	O
proof	O
.	O
there	O
,	O
we	O
have	O
shown	O
that	O
without	O
6.2	O
the	O
vc-dimension	O
69	O
restricting	O
the	O
hypothesis	B
class	I
,	O
for	O
any	O
learning	O
algorithm	O
,	O
an	O
adversary	O
can	O
construct	O
a	O
distribution	O
for	O
which	O
the	O
learning	O
algorithm	O
will	O
perform	O
poorly	O
,	O
while	O
there	O
is	O
another	O
learning	O
algorithm	O
that	O
will	O
succeed	O
on	O
the	O
same	O
distri-	O
bution	O
.	O
to	O
do	O
so	O
,	O
the	O
adversary	O
used	O
a	O
ﬁnite	O
set	B
c	O
⊂	O
x	O
and	O
considered	O
a	O
family	O
of	O
distributions	O
that	O
are	O
concentrated	O
on	O
elements	O
of	O
c.	O
each	O
distribution	O
was	O
derived	O
from	O
a	O
“	O
true	O
”	O
target	O
function	O
from	O
c	O
to	O
{	O
0	O
,	O
1	O
}	O
.	O
to	O
make	O
any	O
algorithm	O
fail	O
,	O
the	O
adversary	O
used	O
the	O
power	O
of	O
choosing	O
a	O
target	O
function	O
from	O
the	O
set	B
of	O
all	O
possible	O
functions	O
from	O
c	O
to	O
{	O
0	O
,	O
1	O
}	O
.	O
when	O
considering	O
pac	O
learnability	O
of	O
a	O
hypothesis	B
class	I
h	O
,	O
the	O
adversary	O
is	O
restricted	O
to	O
constructing	O
distributions	O
for	O
which	O
some	O
hypothesis	B
h	O
∈	O
h	O
achieves	O
a	O
zero	O
risk	B
.	O
since	O
we	O
are	O
considering	O
distributions	O
that	O
are	O
concentrated	O
on	O
elements	O
of	O
c	O
,	O
we	O
should	O
study	O
how	O
h	O
behaves	O
on	O
c	O
,	O
which	O
leads	O
to	O
the	O
following	O
deﬁnition	O
.	O
definition	O
6.2	O
(	O
restriction	O
of	O
h	O
to	O
c	O
)	O
let	O
h	O
be	O
a	O
class	O
of	O
functions	O
from	O
x	O
to	O
{	O
0	O
,	O
1	O
}	O
and	O
let	O
c	O
=	O
{	O
c1	O
,	O
.	O
.	O
.	O
,	O
cm	O
}	O
⊂	O
x	O
.	O
the	O
restriction	O
of	O
h	O
to	O
c	O
is	O
the	O
set	B
of	O
functions	O
from	O
c	O
to	O
{	O
0	O
,	O
1	O
}	O
that	O
can	O
be	O
derived	O
from	O
h.	O
that	O
is	O
,	O
hc	O
=	O
{	O
(	O
h	O
(	O
c1	O
)	O
,	O
.	O
.	O
.	O
,	O
h	O
(	O
cm	O
)	O
)	O
:	O
h	O
∈	O
h	O
}	O
,	O
where	O
we	O
represent	O
each	O
function	B
from	O
c	O
to	O
{	O
0	O
,	O
1	O
}	O
as	O
a	O
vector	O
in	O
{	O
0	O
,	O
1	O
}	O
|c|	O
.	O
if	O
the	O
restriction	O
of	O
h	O
to	O
c	O
is	O
the	O
set	B
of	O
all	O
functions	O
from	O
c	O
to	O
{	O
0	O
,	O
1	O
}	O
,	O
then	O
we	O
say	O
that	O
h	O
shatters	O
the	O
set	B
c.	O
formally	O
:	O
definition	O
6.3	O
(	O
shattering	B
)	O
a	O
hypothesis	B
class	I
h	O
shatters	O
a	O
ﬁnite	O
set	B
c	O
⊂	O
x	O
if	O
the	O
restriction	O
of	O
h	O
to	O
c	O
is	O
the	O
set	B
of	O
all	O
functions	O
from	O
c	O
to	O
{	O
0	O
,	O
1	O
}	O
.	O
that	O
is	O
,	O
|hc|	O
=	O
2|c|	O
.	O
example	O
6.2	O
let	O
h	O
be	O
the	O
class	O
of	O
threshold	O
functions	O
over	O
r.	O
take	O
a	O
set	B
c	O
=	O
{	O
c1	O
}	O
.	O
now	O
,	O
if	O
we	O
take	O
a	O
=	O
c1	O
+	O
1	O
,	O
then	O
we	O
have	O
ha	O
(	O
c1	O
)	O
=	O
1	O
,	O
and	O
if	O
we	O
take	O
a	O
=	O
c1	O
−	O
1	O
,	O
then	O
we	O
have	O
ha	O
(	O
c1	O
)	O
=	O
0.	O
therefore	O
,	O
hc	O
is	O
the	O
set	B
of	O
all	O
functions	O
from	O
c	O
to	O
{	O
0	O
,	O
1	O
}	O
,	O
and	O
h	O
shatters	O
c.	O
now	O
take	O
a	O
set	B
c	O
=	O
{	O
c1	O
,	O
c2	O
}	O
,	O
where	O
c1	O
≤	O
c2	O
.	O
no	O
h	O
∈	O
h	O
can	O
account	O
for	O
the	O
labeling	O
(	O
0	O
,	O
1	O
)	O
,	O
because	O
any	O
threshold	O
that	O
assigns	O
the	O
label	B
0	O
to	O
c1	O
must	O
assign	O
the	O
label	B
0	O
to	O
c2	O
as	O
well	O
.	O
therefore	O
not	O
all	O
functions	O
from	O
c	O
to	O
{	O
0	O
,	O
1	O
}	O
are	O
included	O
in	O
hc	O
;	O
hence	O
c	O
is	O
not	O
shattered	O
by	O
h.	O
getting	O
back	O
to	O
the	O
construction	O
of	O
an	O
adversarial	O
distribution	O
as	O
in	O
the	O
proof	O
of	O
the	O
no-free-lunch	B
theorem	O
(	O
theorem	O
5.1	O
)	O
,	O
we	O
see	O
that	O
whenever	O
some	O
set	B
c	O
is	O
shattered	O
by	O
h	O
,	O
the	O
adversary	O
is	O
not	O
restricted	O
by	O
h	O
,	O
as	O
they	O
can	O
construct	O
a	O
distribution	O
over	O
c	O
based	O
on	O
any	O
target	O
function	O
from	O
c	O
to	O
{	O
0	O
,	O
1	O
}	O
,	O
while	O
still	O
maintaining	O
the	O
realizability	B
assumption	O
.	O
this	O
immediately	O
yields	O
:	O
corollary	O
6.4	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
of	O
functions	O
from	O
x	O
to	O
{	O
0	O
,	O
1	O
}	O
.	O
let	O
m	O
be	O
a	O
training	B
set	I
size	O
.	O
assume	O
that	O
there	O
exists	O
a	O
set	B
c	O
⊂	O
x	O
of	O
size	O
2m	O
that	O
is	O
shattered	O
by	O
h.	O
then	O
,	O
for	O
any	O
learning	O
algorithm	O
,	O
a	O
,	O
there	O
exist	O
a	O
distribution	O
d	O
over	O
x	O
×	O
{	O
0	O
,	O
1	O
}	O
and	O
a	O
predictor	B
h	O
∈	O
h	O
such	O
that	O
ld	O
(	O
h	O
)	O
=	O
0	O
but	O
with	O
probability	O
of	O
at	O
least	O
1/7	O
over	O
the	O
choice	O
of	O
s	O
∼	O
dm	O
we	O
have	O
that	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
≥	O
1/8	O
.	O
70	O
the	O
vc-dimension	O
corollary	O
6.4	O
tells	O
us	O
that	O
if	O
h	O
shatters	O
some	O
set	B
c	O
of	O
size	O
2m	O
then	O
we	O
can	O
not	O
learn	O
h	O
using	O
m	O
examples	O
.	O
intuitively	O
,	O
if	O
a	O
set	B
c	O
is	O
shattered	O
by	O
h	O
,	O
and	O
we	O
receive	O
a	O
sample	O
containing	O
half	O
the	O
instances	O
of	O
c	O
,	O
the	O
labels	O
of	O
these	O
instances	O
give	O
us	O
no	O
information	O
about	O
the	O
labels	O
of	O
the	O
rest	O
of	O
the	O
instances	O
in	O
c	O
–	O
every	O
possible	O
labeling	O
of	O
the	O
rest	O
of	O
the	O
instances	O
can	O
be	O
explained	O
by	O
some	O
hypothesis	B
in	O
h.	O
philosophically	O
,	O
if	O
someone	O
can	O
explain	O
every	O
phenomenon	O
,	O
his	O
explanations	O
are	O
worthless	O
.	O
this	O
leads	O
us	O
directly	O
to	O
the	O
deﬁnition	O
of	O
the	O
vc	O
dimension	B
.	O
definition	O
6.5	O
(	O
vc-dimension	O
)	O
the	O
vc-dimension	O
of	O
a	O
hypothesis	B
class	I
h	O
,	O
denoted	O
vcdim	O
(	O
h	O
)	O
,	O
is	O
the	O
maximal	O
size	O
of	O
a	O
set	B
c	O
⊂	O
x	O
that	O
can	O
be	O
shattered	O
by	O
h.	O
if	O
h	O
can	O
shatter	O
sets	O
of	O
arbitrarily	O
large	O
size	O
we	O
say	O
that	O
h	O
has	O
inﬁnite	O
vc-dimension	O
.	O
a	O
direct	O
consequence	O
of	O
corollary	O
6.4	O
is	O
therefore	O
:	O
theorem	O
6.6	O
let	O
h	O
be	O
a	O
class	O
of	O
inﬁnite	O
vc-dimension	O
.	O
then	O
,	O
h	O
is	O
not	O
pac	O
learnable	O
.	O
proof	O
since	O
h	O
has	O
an	O
inﬁnite	O
vc-dimension	O
,	O
for	O
any	O
training	B
set	I
size	O
m	O
,	O
there	O
exists	O
a	O
shattered	O
set	B
of	O
size	O
2m	O
,	O
and	O
the	O
claim	O
follows	O
by	O
corollary	O
6.4.	O
we	O
shall	O
see	O
later	O
in	O
this	O
chapter	O
that	O
the	O
converse	O
is	O
also	O
true	O
:	O
a	O
ﬁnite	O
vc-	O
dimension	B
guarantees	O
learnability	O
.	O
hence	O
,	O
the	O
vc-dimension	O
characterizes	O
pac	O
learnability	O
.	O
but	O
before	O
delving	O
into	O
more	O
theory	O
,	O
we	O
ﬁrst	O
show	O
several	O
examples	O
.	O
6.3	O
examples	O
in	O
this	O
section	O
we	O
calculate	O
the	O
vc-dimension	O
of	O
several	O
hypothesis	B
classes	O
.	O
to	O
show	O
that	O
vcdim	O
(	O
h	O
)	O
=	O
d	O
we	O
need	O
to	O
show	O
that	O
1.	O
there	O
exists	O
a	O
set	B
c	O
of	O
size	O
d	O
that	O
is	O
shattered	O
by	O
h.	O
2.	O
every	O
set	B
c	O
of	O
size	O
d	O
+	O
1	O
is	O
not	O
shattered	O
by	O
h.	O
6.3.1	O
threshold	O
functions	O
let	O
h	O
be	O
the	O
class	O
of	O
threshold	O
functions	O
over	O
r.	O
recall	B
example	O
6.2	O
,	O
where	O
we	O
have	O
shown	O
that	O
for	O
an	O
arbitrary	O
set	B
c	O
=	O
{	O
c1	O
}	O
,	O
h	O
shatters	O
c	O
;	O
therefore	O
vcdim	O
(	O
h	O
)	O
≥	O
1.	O
we	O
have	O
also	O
shown	O
that	O
for	O
an	O
arbitrary	O
set	B
c	O
=	O
{	O
c1	O
,	O
c2	O
}	O
where	O
c1	O
≤	O
c2	O
,	O
h	O
does	O
not	O
shatter	O
c.	O
we	O
therefore	O
conclude	O
that	O
vcdim	O
(	O
h	O
)	O
=	O
1	O
.	O
6.3	O
examples	O
71	O
6.3.2	O
intervals	O
let	O
h	O
be	O
the	O
class	O
of	O
intervals	O
over	O
r	O
,	O
namely	O
,	O
h	O
=	O
{	O
ha	O
,	O
b	O
:	O
a	O
,	O
b	O
∈	O
r	O
,	O
a	O
<	O
b	O
}	O
,	O
where	O
ha	O
,	O
b	O
:	O
r	O
→	O
{	O
0	O
,	O
1	O
}	O
is	O
a	O
function	B
such	O
that	O
ha	O
,	O
b	O
(	O
x	O
)	O
=	O
1	O
[	O
x∈	O
(	O
a	O
,	O
b	O
)	O
]	O
.	O
take	O
the	O
set	B
c	O
=	O
{	O
1	O
,	O
2	O
}	O
.	O
then	O
,	O
h	O
shatters	O
c	O
(	O
make	O
sure	O
you	O
understand	O
why	O
)	O
and	O
therefore	O
vcdim	O
(	O
h	O
)	O
≥	O
2.	O
now	O
take	O
an	O
arbitrary	O
set	B
c	O
=	O
{	O
c1	O
,	O
c2	O
,	O
c3	O
}	O
and	O
assume	O
without	O
loss	B
of	O
generality	O
that	O
c1	O
≤	O
c2	O
≤	O
c3	O
.	O
then	O
,	O
the	O
labeling	O
(	O
1	O
,	O
0	O
,	O
1	O
)	O
can	O
not	O
be	O
obtained	O
by	O
an	O
interval	O
and	O
therefore	O
h	O
does	O
not	O
shatter	O
c.	O
we	O
therefore	O
conclude	O
that	O
vcdim	O
(	O
h	O
)	O
=	O
2	O
.	O
6.3.3	O
axis	O
aligned	O
rectangles	O
let	O
h	O
be	O
the	O
class	O
of	O
axis	O
aligned	O
rectangles	O
,	O
formally	O
:	O
h	O
=	O
{	O
h	O
(	O
a1	O
,	O
a2	O
,	O
b1	O
,	O
b2	O
)	O
:	O
a1	O
≤	O
a2	O
and	O
b1	O
≤	O
b2	O
}	O
where	O
h	O
(	O
a1	O
,	O
a2	O
,	O
b1	O
,	O
b2	O
)	O
(	O
x1	O
,	O
x2	O
)	O
=	O
(	O
cid:40	O
)	O
if	O
a1	O
≤	O
x1	O
≤	O
a2	O
and	O
b1	O
≤	O
x2	O
≤	O
b2	O
otherwise	O
1	O
0	O
(	O
6.2	O
)	O
we	O
shall	O
show	O
in	O
the	O
following	O
that	O
vcdim	O
(	O
h	O
)	O
=	O
4.	O
to	O
prove	O
this	O
we	O
need	O
to	O
ﬁnd	O
a	O
set	B
of	O
4	O
points	O
that	O
are	O
shattered	O
by	O
h	O
,	O
and	O
show	O
that	O
no	O
set	B
of	O
5	O
points	O
can	O
be	O
shattered	O
by	O
h.	O
finding	O
a	O
set	B
of	O
4	O
points	O
that	O
are	O
shattered	O
is	O
easy	O
(	O
see	O
figure	O
6.1	O
)	O
.	O
now	O
,	O
consider	O
any	O
set	B
c	O
⊂	O
r2	O
of	O
5	O
points	O
.	O
in	O
c	O
,	O
take	O
a	O
leftmost	O
point	O
(	O
whose	O
ﬁrst	O
coordinate	O
is	O
the	O
smallest	O
in	O
c	O
)	O
,	O
a	O
rightmost	O
point	O
(	O
ﬁrst	O
coordinate	O
is	O
the	O
largest	O
)	O
,	O
a	O
lowest	O
point	O
(	O
second	O
coordinate	O
is	O
the	O
smallest	O
)	O
,	O
and	O
a	O
highest	O
point	O
(	O
second	O
coordinate	O
is	O
the	O
largest	O
)	O
.	O
without	O
loss	B
of	O
generality	O
,	O
denote	O
c	O
=	O
{	O
c1	O
,	O
.	O
.	O
.	O
,	O
c5	O
}	O
and	O
let	O
c5	O
be	O
the	O
point	O
that	O
was	O
not	O
selected	O
.	O
now	O
,	O
deﬁne	O
the	O
labeling	O
(	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
0	O
)	O
.	O
it	O
is	O
impossible	O
to	O
obtain	O
this	O
labeling	O
by	O
an	O
axis	O
aligned	O
rectangle	O
.	O
indeed	O
,	O
such	O
a	O
rectangle	O
must	O
contain	O
c1	O
,	O
.	O
.	O
.	O
,	O
c4	O
;	O
but	O
in	O
this	O
case	O
the	O
rectangle	O
contains	O
c5	O
as	O
well	O
,	O
because	O
its	O
coordinates	O
are	O
within	O
the	O
intervals	O
deﬁned	O
by	O
the	O
selected	O
points	O
.	O
so	O
,	O
c	O
is	O
not	O
shattered	O
by	O
h	O
,	O
and	O
therefore	O
vcdim	O
(	O
h	O
)	O
=	O
4.	O
c4	O
c1	O
c5	O
c3	O
c2	O
figure	O
6.1	O
left	O
:	O
4	O
points	O
that	O
are	O
shattered	O
by	O
axis	O
aligned	O
rectangles	O
.	O
right	O
:	O
any	O
axis	O
aligned	O
rectangle	O
can	O
not	O
label	B
c5	O
by	O
0	O
and	O
the	O
rest	O
of	O
the	O
points	O
by	O
1	O
.	O
72	O
the	O
vc-dimension	O
6.3.4	O
finite	O
classes	O
let	O
h	O
be	O
a	O
ﬁnite	O
class	O
.	O
then	O
,	O
clearly	O
,	O
for	O
any	O
set	B
c	O
we	O
have	O
|hc|	O
≤	O
|h|	O
and	O
thus	O
c	O
can	O
not	O
be	O
shattered	O
if	O
|h|	O
<	O
2|c|	O
.	O
this	O
implies	O
that	O
vcdim	O
(	O
h	O
)	O
≤	O
log2	O
(	O
|h|	O
)	O
.	O
this	O
shows	O
that	O
the	O
pac	O
learnability	O
of	O
ﬁnite	O
classes	O
follows	O
from	O
the	O
more	O
general	O
statement	O
of	O
pac	O
learnability	O
of	O
classes	O
with	O
ﬁnite	O
vc-dimension	O
,	O
which	O
we	O
shall	O
see	O
in	O
the	O
next	O
section	O
.	O
note	O
,	O
however	O
,	O
that	O
the	O
vc-dimension	O
of	O
a	O
ﬁnite	O
class	O
h	O
can	O
be	O
signiﬁcantly	O
smaller	O
than	O
log2	O
(	O
|h|	O
)	O
.	O
for	O
example	O
,	O
let	O
x	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
,	O
for	O
some	O
integer	O
k	O
,	O
and	O
consider	O
the	O
class	O
of	O
threshold	O
functions	O
(	O
as	O
deﬁned	O
in	O
example	O
6.2	O
)	O
.	O
then	O
,	O
|h|	O
=	O
k	O
but	O
vcdim	O
(	O
h	O
)	O
=	O
1.	O
since	O
k	O
can	O
be	O
arbitrarily	O
large	O
,	O
the	O
gap	O
between	O
log2	O
(	O
|h|	O
)	O
and	O
vcdim	O
(	O
h	O
)	O
can	O
be	O
arbitrarily	O
large	O
.	O
6.3.5	O
vc-dimension	O
and	O
the	O
number	O
of	O
parameters	O
in	O
the	O
previous	O
examples	O
,	O
the	O
vc-dimension	O
happened	O
to	O
equal	O
the	O
number	O
of	O
parameters	O
deﬁning	O
the	O
hypothesis	B
class	I
.	O
while	O
this	O
is	O
often	O
the	O
case	O
,	O
it	O
is	O
not	O
always	O
true	O
.	O
consider	O
,	O
for	O
example	O
,	O
the	O
domain	B
x	O
=	O
r	O
,	O
and	O
the	O
hypothesis	B
class	I
h	O
=	O
{	O
hθ	O
:	O
θ	O
∈	O
r	O
}	O
where	O
hθ	O
:	O
x	O
→	O
{	O
0	O
,	O
1	O
}	O
is	O
deﬁned	O
by	O
hθ	O
(	O
x	O
)	O
=	O
(	O
cid:100	O
)	O
0.5	O
sin	O
(	O
θx	O
)	O
(	O
cid:101	O
)	O
.	O
it	O
is	O
possible	O
to	O
prove	O
that	O
vcdim	O
(	O
h	O
)	O
=	O
∞	O
,	O
namely	O
,	O
for	O
every	O
d	O
,	O
one	O
can	O
ﬁnd	O
d	O
points	O
that	O
are	O
shattered	O
by	O
h	O
(	O
see	O
exercise	O
8	O
)	O
.	O
6.4	O
the	O
fundamental	O
theorem	O
of	O
pac	O
learning	O
we	O
have	O
already	O
shown	O
that	O
a	O
class	O
of	O
inﬁnite	O
vc-dimension	O
is	O
not	O
learnable	O
.	O
the	O
converse	O
statement	O
is	O
also	O
true	O
,	O
leading	O
to	O
the	O
fundamental	O
theorem	O
of	O
statistical	O
learning	O
theory	O
:	O
theorem	O
6.7	O
(	O
the	O
fundamental	O
theorem	O
of	O
statistical	O
learning	O
)	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
of	O
functions	O
from	O
a	O
domain	B
x	O
to	O
{	O
0	O
,	O
1	O
}	O
and	O
let	O
the	O
loss	B
function	I
be	O
the	O
0	O
−	O
1	O
loss	B
.	O
then	O
,	O
the	O
following	O
are	O
equivalent	O
:	O
1.	O
h	O
has	O
the	O
uniform	B
convergence	I
property	O
.	O
2.	O
any	O
erm	O
rule	O
is	O
a	O
successful	O
agnostic	O
pac	O
learner	O
for	O
h.	O
3.	O
h	O
is	O
agnostic	O
pac	O
learnable	O
.	O
4.	O
h	O
is	O
pac	O
learnable	O
.	O
5.	O
any	O
erm	O
rule	O
is	O
a	O
successful	O
pac	O
learner	O
for	O
h.	O
6.	O
h	O
has	O
a	O
ﬁnite	O
vc-dimension	O
.	O
the	O
proof	O
of	O
the	O
theorem	O
is	O
given	O
in	O
the	O
next	O
section	O
.	O
not	O
only	O
does	O
the	O
vc-dimension	O
characterize	O
pac	O
learnability	O
;	O
it	O
even	O
deter-	O
mines	O
the	O
sample	B
complexity	I
.	O
theorem	O
6.8	O
(	O
the	O
fundamental	O
theorem	O
of	O
statistical	O
learning	O
–	O
quantita-	O
tive	O
version	O
)	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
of	O
functions	O
from	O
a	O
domain	B
x	O
to	O
{	O
0	O
,	O
1	O
}	O
and	O
let	O
the	O
loss	B
function	I
be	O
the	O
0	O
−	O
1	O
loss	B
.	O
assume	O
that	O
vcdim	O
(	O
h	O
)	O
=	O
d	O
<	O
∞	O
.	O
then	O
,	O
there	O
are	O
absolute	O
constants	O
c1	O
,	O
c2	O
such	O
that	O
:	O
6.5	O
proof	O
of	O
theorem	O
6.7	O
73	O
1.	O
h	O
has	O
the	O
uniform	B
convergence	I
property	O
with	O
sample	B
complexity	I
d	O
+	O
log	O
(	O
1/δ	O
)	O
≤	O
much	O
(	O
	O
,	O
δ	O
)	O
≤	O
c2	O
d	O
+	O
log	O
(	O
1/δ	O
)	O
2	O
2.	O
h	O
is	O
agnostic	O
pac	O
learnable	O
with	O
sample	B
complexity	I
d	O
+	O
log	O
(	O
1/δ	O
)	O
≤	O
mh	O
(	O
	O
,	O
δ	O
)	O
≤	O
c2	O
d	O
+	O
log	O
(	O
1/δ	O
)	O
2	O
c1	O
c1	O
2	O
2	O
3.	O
h	O
is	O
pac	O
learnable	O
with	O
sample	B
complexity	I
≤	O
mh	O
(	O
	O
,	O
δ	O
)	O
≤	O
c2	O
d	O
+	O
log	O
(	O
1/δ	O
)	O
c1	O
	O
d	O
log	O
(	O
1/	O
)	O
+	O
log	O
(	O
1/δ	O
)	O
	O
6.5	O
the	O
proof	O
of	O
this	O
theorem	O
is	O
given	O
in	O
chapter	O
28.	O
remark	O
6.3	O
we	O
stated	O
the	O
fundamental	O
theorem	O
for	O
binary	O
classiﬁcation	O
tasks	O
.	O
a	O
similar	O
result	O
holds	O
for	O
some	O
other	O
learning	O
problems	O
such	O
as	O
regression	B
with	O
the	O
absolute	O
loss	O
or	O
the	O
squared	O
loss	B
.	O
however	O
,	O
the	O
theorem	O
does	O
not	O
hold	O
for	O
all	O
learning	O
tasks	O
.	O
in	O
particular	O
,	O
learnability	O
is	O
sometimes	O
possible	O
even	O
though	O
the	O
uniform	B
convergence	I
property	O
does	O
not	O
hold	O
(	O
we	O
will	O
see	O
an	O
example	O
in	O
chapter	O
13	O
,	O
exercise	O
2	O
)	O
.	O
furthermore	O
,	O
in	O
some	O
situations	O
,	O
the	O
erm	O
rule	O
fails	O
but	O
learnability	O
is	O
possible	O
with	O
other	O
learning	O
rules	O
.	O
proof	O
of	O
theorem	O
6.7	O
we	O
have	O
already	O
seen	O
that	O
1	O
→	O
2	O
in	O
chapter	O
4.	O
the	O
implications	O
2	O
→	O
3	O
and	O
3	O
→	O
4	O
are	O
trivial	O
and	O
so	O
is	O
2	O
→	O
5.	O
the	O
implications	O
4	O
→	O
6	O
and	O
5	O
→	O
6	O
follow	O
from	O
the	O
no-free-lunch	B
theorem	O
.	O
the	O
diﬃcult	O
part	O
is	O
to	O
show	O
that	O
6	O
→	O
1.	O
the	O
proof	O
is	O
based	O
on	O
two	O
main	O
claims	O
:	O
•	O
if	O
vcdim	O
(	O
h	O
)	O
=	O
d	O
,	O
then	O
even	O
though	O
h	O
might	O
be	O
inﬁnite	O
,	O
when	O
restricting	O
it	O
to	O
a	O
ﬁnite	O
set	B
c	O
⊂	O
x	O
,	O
its	O
“	O
eﬀective	O
”	O
size	O
,	O
|hc|	O
,	O
is	O
only	O
o	O
(	O
|c|d	O
)	O
.	O
that	O
is	O
,	O
the	O
size	O
of	O
hc	O
grows	O
polynomially	O
rather	O
than	O
exponentially	O
with	O
|c|	O
.	O
this	O
claim	O
is	O
often	O
referred	O
to	O
as	O
sauer	O
’	O
s	O
lemma	O
,	O
but	O
it	O
has	O
also	O
been	O
stated	O
and	O
proved	O
independently	O
by	O
shelah	O
and	O
by	O
perles	O
.	O
the	O
formal	O
statement	O
is	O
given	O
in	O
section	O
6.5.1	O
later	O
.	O
•	O
in	O
section	O
4	O
we	O
have	O
shown	O
that	O
ﬁnite	O
hypothesis	B
classes	O
enjoy	O
the	O
uniform	B
convergence	I
property	O
.	O
in	O
section	O
6.5.2	O
later	O
we	O
generalize	O
this	O
result	O
and	O
show	O
that	O
uniform	B
convergence	I
holds	O
whenever	O
the	O
hypothesis	B
class	I
has	O
a	O
“	O
small	O
eﬀective	O
size.	O
”	O
by	O
“	O
small	O
eﬀective	O
size	O
”	O
we	O
mean	O
classes	O
for	O
which	O
|hc|	O
grows	O
polynomially	O
with	O
|c|	O
.	O
6.5.1	O
sauer	O
’	O
s	O
lemma	O
and	O
the	O
growth	B
function	I
we	O
deﬁned	O
the	O
notion	O
of	O
shattering	B
,	O
by	O
considering	O
the	O
restriction	O
of	O
h	O
to	O
a	O
ﬁnite	O
set	B
of	O
instances	O
.	O
the	O
growth	B
function	I
measures	O
the	O
maximal	O
“	O
eﬀective	O
”	O
size	O
of	O
h	O
on	O
a	O
set	B
of	O
m	O
examples	O
.	O
formally	O
:	O
74	O
the	O
vc-dimension	O
definition	O
6.9	O
(	O
growth	B
function	I
)	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
.	O
then	O
the	O
growth	B
function	I
of	O
h	O
,	O
denoted	O
τh	O
:	O
n	O
→	O
n	O
,	O
is	O
deﬁned	O
as	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
hc	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
.	O
τh	O
(	O
m	O
)	O
=	O
max	O
c⊂x	O
:	O
|c|=m	O
in	O
words	O
,	O
τh	O
(	O
m	O
)	O
is	O
the	O
number	O
of	O
diﬀerent	O
functions	O
from	O
a	O
set	B
c	O
of	O
size	O
m	O
to	O
{	O
0	O
,	O
1	O
}	O
that	O
can	O
be	O
obtained	O
by	O
restricting	O
h	O
to	O
c.	O
obviously	O
,	O
if	O
vcdim	O
(	O
h	O
)	O
=	O
d	O
then	O
for	O
any	O
m	O
≤	O
d	O
we	O
have	O
τh	O
(	O
m	O
)	O
=	O
2m	O
.	O
in	O
such	O
cases	O
,	O
h	O
induces	O
all	O
possible	O
functions	O
from	O
c	O
to	O
{	O
0	O
,	O
1	O
}	O
.	O
the	O
following	O
beau-	O
tiful	O
lemma	O
,	O
proposed	O
independently	O
by	O
sauer	O
,	O
shelah	O
,	O
and	O
perles	O
,	O
shows	O
that	O
when	O
m	O
becomes	O
larger	O
than	O
the	O
vc-dimension	O
,	O
the	O
growth	B
function	I
increases	O
polynomially	O
rather	O
than	O
exponentially	O
with	O
m.	O
lemma	O
6.10	O
(	O
sauer-shelah-perles	O
)	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
with	O
vcdim	O
(	O
h	O
)	O
≤	O
τh	O
(	O
m	O
)	O
≤	O
(	O
em/d	O
)	O
d.	O
d	O
<	O
∞	O
.	O
then	O
,	O
for	O
all	O
m	O
,	O
τh	O
(	O
m	O
)	O
≤	O
(	O
cid:80	O
)	O
d	O
(	O
cid:1	O
)	O
.	O
in	O
particular	O
,	O
if	O
m	O
>	O
d	O
+	O
1	O
then	O
(	O
cid:0	O
)	O
m	O
i=0	O
i	O
proof	O
of	O
sauer	O
’	O
s	O
lemma	O
*	O
to	O
prove	O
the	O
lemma	O
it	O
suﬃces	O
to	O
prove	O
the	O
following	O
stronger	O
claim	O
:	O
for	O
any	O
c	O
=	O
{	O
c1	O
,	O
.	O
.	O
.	O
,	O
cm	O
}	O
we	O
have	O
∀h	O
,	O
|hc|	O
≤	O
|	O
{	O
b	O
⊆	O
c	O
:	O
h	O
shatters	O
b	O
}	O
|	O
.	O
(	O
6.3	O
)	O
the	O
reason	O
why	O
equation	O
(	O
6.3	O
)	O
is	O
suﬃcient	O
to	O
prove	O
the	O
lemma	O
is	O
that	O
if	O
vcdim	O
(	O
h	O
)	O
≤	O
d	O
then	O
no	O
set	B
whose	O
size	O
is	O
larger	O
than	O
d	O
is	O
shattered	O
by	O
h	O
and	O
therefore	O
|	O
{	O
b	O
⊆	O
c	O
:	O
h	O
shatters	O
b	O
}	O
|	O
≤	O
d	O
(	O
cid:88	O
)	O
(	O
cid:18	O
)	O
m	O
(	O
cid:19	O
)	O
.	O
i	O
i=0	O
when	O
m	O
>	O
d	O
+	O
1	O
the	O
right-hand	O
side	O
of	O
the	O
preceding	O
is	O
at	O
most	O
(	O
em/d	O
)	O
d	O
(	O
see	O
lemma	O
a.5	O
in	O
appendix	O
a	O
)	O
.	O
we	O
are	O
left	O
with	O
proving	O
equation	O
(	O
6.3	O
)	O
and	O
we	O
do	O
it	O
using	O
an	O
inductive	O
argu-	O
ment	O
.	O
for	O
m	O
=	O
1	O
,	O
no	O
matter	O
what	O
h	O
is	O
,	O
either	O
both	O
sides	O
of	O
equation	O
(	O
6.3	O
)	O
equal	O
1	O
or	O
both	O
sides	O
equal	O
2	O
(	O
the	O
empty	O
set	B
is	O
always	O
considered	O
to	O
be	O
shattered	O
by	O
h	O
)	O
.	O
assume	O
equation	O
(	O
6.3	O
)	O
holds	O
for	O
sets	O
of	O
size	O
k	O
<	O
m	O
and	O
let	O
us	O
prove	O
it	O
for	O
sets	O
of	O
size	O
m.	O
fix	O
h	O
and	O
c	O
=	O
{	O
c1	O
,	O
.	O
.	O
.	O
,	O
cm	O
}	O
.	O
denote	O
c	O
(	O
cid:48	O
)	O
=	O
{	O
c2	O
,	O
.	O
.	O
.	O
,	O
cm	O
}	O
and	O
in	O
addition	O
,	O
deﬁne	O
the	O
following	O
two	O
sets	O
:	O
y0	O
=	O
{	O
(	O
y2	O
,	O
.	O
.	O
.	O
,	O
ym	O
)	O
:	O
(	O
0	O
,	O
y2	O
,	O
.	O
.	O
.	O
,	O
ym	O
)	O
∈	O
hc	O
∨	O
(	O
1	O
,	O
y2	O
,	O
.	O
.	O
.	O
,	O
ym	O
)	O
∈	O
hc	O
}	O
,	O
and	O
y1	O
=	O
{	O
(	O
y2	O
,	O
.	O
.	O
.	O
,	O
ym	O
)	O
:	O
(	O
0	O
,	O
y2	O
,	O
.	O
.	O
.	O
,	O
ym	O
)	O
∈	O
hc	O
∧	O
(	O
1	O
,	O
y2	O
,	O
.	O
.	O
.	O
,	O
ym	O
)	O
∈	O
hc	O
}	O
.	O
it	O
is	O
easy	O
to	O
verify	O
that	O
|hc|	O
=	O
|y0|	O
+	O
|y1|	O
.	O
additionally	O
,	O
since	O
y0	O
=	O
hc	O
(	O
cid:48	O
)	O
,	O
using	O
the	O
induction	O
assumption	O
(	O
applied	O
on	O
h	O
and	O
c	O
(	O
cid:48	O
)	O
)	O
we	O
have	O
that	O
|y0|	O
=	O
|hc	O
(	O
cid:48	O
)	O
|	O
≤	O
|	O
{	O
b	O
⊆	O
c	O
(	O
cid:48	O
)	O
:	O
h	O
shatters	O
b	O
}	O
|	O
=	O
|	O
{	O
b	O
⊆	O
c	O
:	O
c1	O
(	O
cid:54	O
)	O
∈	O
b	O
∧	O
h	O
shatters	O
b	O
}	O
|	O
.	O
6.5	O
proof	O
of	O
theorem	O
6.7	O
75	O
next	O
,	O
deﬁne	O
h	O
(	O
cid:48	O
)	O
⊆	O
h	O
to	O
be	O
h	O
(	O
cid:48	O
)	O
=	O
{	O
h	O
∈	O
h	O
:	O
∃h	O
(	O
cid:48	O
)	O
∈	O
h	O
s.t	O
.	O
(	O
1	O
−	O
h	O
(	O
cid:48	O
)	O
(	O
c1	O
)	O
,	O
h	O
(	O
cid:48	O
)	O
(	O
c2	O
)	O
,	O
.	O
.	O
.	O
,	O
h	O
(	O
cid:48	O
)	O
(	O
cm	O
)	O
)	O
=	O
(	O
h	O
(	O
c1	O
)	O
,	O
h	O
(	O
c2	O
)	O
,	O
.	O
.	O
.	O
,	O
h	O
(	O
cm	O
)	O
}	O
,	O
namely	O
,	O
h	O
(	O
cid:48	O
)	O
contains	O
pairs	O
of	O
hypotheses	O
that	O
agree	O
on	O
c	O
(	O
cid:48	O
)	O
and	O
diﬀer	O
on	O
c1	O
.	O
using	O
this	O
deﬁnition	O
,	O
it	O
is	O
clear	O
that	O
if	O
h	O
(	O
cid:48	O
)	O
shatters	O
a	O
set	B
b	O
⊆	O
c	O
(	O
cid:48	O
)	O
then	O
it	O
also	O
shatters	O
the	O
set	B
b	O
∪	O
{	O
c1	O
}	O
and	O
vice	O
versa	O
.	O
combining	O
this	O
with	O
the	O
fact	O
that	O
y1	O
=	O
h	O
(	O
cid:48	O
)	O
c	O
(	O
cid:48	O
)	O
and	O
using	O
the	O
inductive	O
assumption	O
(	O
now	O
applied	O
on	O
h	O
(	O
cid:48	O
)	O
and	O
c	O
(	O
cid:48	O
)	O
)	O
we	O
obtain	O
that	O
|y1|	O
=	O
|h	O
(	O
cid:48	O
)	O
c	O
(	O
cid:48	O
)	O
|	O
≤	O
|	O
{	O
b	O
⊆	O
c	O
(	O
cid:48	O
)	O
:	O
h	O
(	O
cid:48	O
)	O
shatters	O
b	O
}	O
|	O
=	O
|	O
{	O
b	O
⊆	O
c	O
(	O
cid:48	O
)	O
:	O
h	O
(	O
cid:48	O
)	O
shatters	O
b	O
∪	O
{	O
c1	O
}	O
}	O
|	O
=	O
|	O
{	O
b	O
⊆	O
c	O
:	O
c1	O
∈	O
b	O
∧	O
h	O
(	O
cid:48	O
)	O
shatters	O
b	O
}	O
|	O
≤	O
|	O
{	O
b	O
⊆	O
c	O
:	O
c1	O
∈	O
b	O
∧	O
h	O
shatters	O
b	O
}	O
|	O
.	O
overall	O
,	O
we	O
have	O
shown	O
that	O
|hc|	O
=	O
|y0|	O
+	O
|y1|	O
≤	O
|	O
{	O
b	O
⊆	O
c	O
:	O
c1	O
(	O
cid:54	O
)	O
∈	O
b	O
∧	O
h	O
shatters	O
b	O
}	O
|	O
+	O
|	O
{	O
b	O
⊆	O
c	O
:	O
c1	O
∈	O
b	O
∧	O
h	O
shatters	O
b	O
}	O
|	O
=	O
|	O
{	O
b	O
⊆	O
c	O
:	O
h	O
shatters	O
b	O
}	O
|	O
,	O
which	O
concludes	O
our	O
proof	O
.	O
6.5.2	O
uniform	B
convergence	I
for	O
classes	O
of	O
small	O
eﬀective	O
size	O
in	O
this	O
section	O
we	O
prove	O
that	O
if	O
h	O
has	O
small	O
eﬀective	O
size	O
then	O
it	O
enjoys	O
the	O
uniform	B
convergence	I
property	O
.	O
formally	O
,	O
theorem	O
6.11	O
let	O
h	O
be	O
a	O
class	O
and	O
let	O
τh	O
be	O
its	O
growth	B
function	I
.	O
then	O
,	O
for	O
every	O
d	O
and	O
every	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
the	O
choice	O
of	O
s	O
∼	O
dm	O
we	O
have	O
|ld	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
|	O
≤	O
4	O
+	O
(	O
cid:112	O
)	O
log	O
(	O
τh	O
(	O
2m	O
)	O
)	O
√	O
.	O
δ	O
2m	O
before	O
proving	O
the	O
theorem	O
,	O
let	O
us	O
ﬁrst	O
conclude	O
the	O
proof	O
of	O
theorem	O
6.7.	O
proof	O
of	O
theorem	O
6.7	O
it	O
suﬃces	O
to	O
prove	O
that	O
if	O
the	O
vc-dimension	O
is	O
ﬁnite	O
then	O
the	O
uniform	B
convergence	I
property	O
holds	O
.	O
we	O
will	O
prove	O
that	O
much	O
(	O
	O
,	O
δ	O
)	O
≤	O
4	O
16d	O
(	O
δ	O
)	O
2	O
log	O
+	O
16	O
d	O
log	O
(	O
2e/d	O
)	O
(	O
δ	O
)	O
2	O
.	O
from	O
sauer	O
’	O
s	O
lemma	O
we	O
have	O
that	O
for	O
m	O
>	O
d	O
,	O
τh	O
(	O
2m	O
)	O
≤	O
(	O
2em/d	O
)	O
d.	O
combining	O
this	O
with	O
theorem	O
6.11	O
we	O
obtain	O
that	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
,	O
|ls	O
(	O
h	O
)	O
−	O
ld	O
(	O
h	O
)	O
|	O
≤	O
4	O
+	O
(	O
cid:112	O
)	O
d	O
log	O
(	O
2em/d	O
)	O
for	O
simplicity	O
assume	O
that	O
(	O
cid:112	O
)	O
d	O
log	O
(	O
2em/d	O
)	O
≥	O
4	O
;	O
hence	O
,	O
√	O
δ	O
2m	O
(	O
cid:18	O
)	O
16d	O
(	O
cid:19	O
)	O
(	O
δ	O
)	O
2	O
(	O
cid:114	O
)	O
|ls	O
(	O
h	O
)	O
−	O
ld	O
(	O
h	O
)	O
|	O
≤	O
1	O
δ	O
2d	O
log	O
(	O
2em/d	O
)	O
m	O
.	O
.	O
76	O
the	O
vc-dimension	O
to	O
ensure	O
that	O
the	O
preceding	O
is	O
at	O
most	O
	O
we	O
need	O
that	O
m	O
≥	O
2d	O
log	O
(	O
m	O
)	O
(	O
δ	O
)	O
2	O
+	O
2	O
d	O
log	O
(	O
2e/d	O
)	O
(	O
δ	O
)	O
2	O
.	O
standard	O
algebraic	O
manipulations	O
(	O
see	O
lemma	O
a.2	O
in	O
appendix	O
a	O
)	O
show	O
that	O
a	O
suﬃcient	O
condition	O
for	O
the	O
preceding	O
to	O
hold	O
is	O
that	O
(	O
cid:18	O
)	O
2d	O
(	O
cid:19	O
)	O
(	O
δ	O
)	O
2	O
m	O
≥	O
4	O
2d	O
(	O
δ	O
)	O
2	O
log	O
+	O
4	O
d	O
log	O
(	O
2e/d	O
)	O
(	O
δ	O
)	O
2	O
.	O
remark	O
6.4	O
the	O
upper	O
bound	O
on	O
much	O
we	O
derived	O
in	O
the	O
proof	O
theorem	O
6.7	O
is	O
not	O
the	O
tightest	O
possible	O
.	O
a	O
tighter	O
analysis	O
that	O
yields	O
the	O
bounds	O
given	O
in	O
theorem	O
6.8	O
can	O
be	O
found	O
in	O
chapter	O
28.	O
proof	O
of	O
theorem	O
6.11	O
*	O
we	O
will	O
start	O
by	O
showing	O
that	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
≤	O
4	O
+	O
(	O
cid:112	O
)	O
log	O
(	O
τh	O
(	O
2m	O
)	O
)	O
√	O
.	O
e	O
|ld	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
|	O
2m	O
s∼dm	O
sup	O
h∈h	O
(	O
6.4	O
)	O
since	O
the	O
random	O
variable	O
suph∈h	O
|ld	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
|	O
is	O
nonnegative	O
,	O
the	O
proof	O
of	O
the	O
theorem	O
follows	O
directly	O
from	O
the	O
preceding	O
using	O
markov	O
’	O
s	O
inequality	O
(	O
see	O
section	O
b.1	O
)	O
.	O
h	O
∈	O
h	O
,	O
we	O
can	O
rewrite	O
ld	O
(	O
h	O
)	O
=	O
es	O
(	O
cid:48	O
)	O
∼dm	O
[	O
ls	O
(	O
cid:48	O
)	O
(	O
h	O
)	O
]	O
,	O
where	O
s	O
(	O
cid:48	O
)	O
=	O
z	O
(	O
cid:48	O
)	O
additional	O
i.i.d	O
.	O
sample	O
.	O
therefore	O
,	O
|ld	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
|	O
to	O
bound	O
the	O
left-hand	O
side	O
of	O
equation	O
(	O
6.4	O
)	O
we	O
ﬁrst	O
note	O
that	O
for	O
every	O
m	O
is	O
an	O
ls	O
(	O
cid:48	O
)	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
1	O
,	O
.	O
.	O
.	O
,	O
z	O
(	O
cid:48	O
)	O
=	O
e	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
.	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
e	O
s	O
(	O
cid:48	O
)	O
∼dm	O
e	O
s∼dm	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:21	O
)	O
sup	O
h∈h	O
sup	O
h∈h	O
a	O
generalization	O
of	O
the	O
triangle	O
inequality	O
yields	O
[	O
ls	O
(	O
cid:48	O
)	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
]	O
|ls	O
(	O
cid:48	O
)	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
|	O
,	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
e	O
s	O
(	O
cid:48	O
)	O
∼dm	O
s∼dm	O
(	O
cid:20	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
≤	O
e	O
s	O
(	O
cid:48	O
)	O
∼dm	O
and	O
the	O
fact	O
that	O
supermum	O
of	O
expectation	O
is	O
smaller	O
than	O
expectation	O
of	O
supre-	O
mum	O
yields	O
e	O
s	O
(	O
cid:48	O
)	O
∼dm	O
sup	O
h∈h	O
|ls	O
(	O
cid:48	O
)	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
|	O
≤	O
e	O
s	O
(	O
cid:48	O
)	O
∼dm	O
sup	O
h∈h	O
|ls	O
(	O
cid:48	O
)	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
|	O
.	O
formally	O
,	O
the	O
previous	O
two	O
inequalities	O
follow	O
from	O
jensen	O
’	O
s	O
inequality	O
.	O
combin-	O
ing	O
all	O
we	O
obtain	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
e	O
s∼dm	O
sup	O
h∈h	O
|ld	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
|	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
(	O
cid:34	O
)	O
|ls	O
(	O
cid:48	O
)	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
|	O
≤	O
=	O
e	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
∼dm	O
e	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
∼dm	O
sup	O
h∈h	O
1	O
m	O
sup	O
h∈h	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
(	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
z	O
(	O
cid:48	O
)	O
i	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:35	O
)	O
.	O
(	O
6.5	O
)	O
(	O
cid:34	O
)	O
(	O
cid:34	O
)	O
(	O
cid:34	O
)	O
i=1	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
m	O
(	O
cid:88	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
m	O
(	O
cid:88	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
1	O
m	O
1	O
m	O
i=1	O
sup	O
h∈h	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
(	O
cid:35	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:35	O
)	O
(	O
cid:35	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
,	O
.	O
(	O
cid:35	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
6.5	O
proof	O
of	O
theorem	O
6.7	O
77	O
the	O
expectation	O
on	O
the	O
right-hand	O
side	O
is	O
over	O
a	O
choice	O
of	O
two	O
i.i.d	O
.	O
samples	O
s	O
=	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
and	O
s	O
(	O
cid:48	O
)	O
=	O
z	O
(	O
cid:48	O
)	O
m.	O
since	O
all	O
of	O
these	O
2m	O
vectors	O
are	O
chosen	O
i.i.d.	B
,	O
nothing	O
will	O
change	O
if	O
we	O
replace	O
the	O
name	O
of	O
the	O
random	O
vector	O
zi	O
with	O
the	O
i	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
)	O
name	O
of	O
the	O
random	O
vector	O
z	O
(	O
cid:48	O
)	O
i	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
)	O
.	O
it	O
follows	O
that	O
for	O
in	O
equation	O
(	O
6.5	O
)	O
we	O
will	O
have	O
the	O
term	O
−	O
(	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
z	O
(	O
cid:48	O
)	O
every	O
σ	O
∈	O
{	O
±1	O
}	O
m	O
we	O
have	O
that	O
equation	O
(	O
6.5	O
)	O
equals	O
1	O
,	O
.	O
.	O
.	O
,	O
z	O
(	O
cid:48	O
)	O
i.	O
if	O
we	O
do	O
it	O
,	O
instead	O
of	O
the	O
term	O
(	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
z	O
(	O
cid:48	O
)	O
e	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
∼dm	O
1	O
m	O
sup	O
h∈h	O
σi	O
(	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
z	O
(	O
cid:48	O
)	O
i	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
)	O
since	O
this	O
holds	O
for	O
every	O
σ	O
∈	O
{	O
±1	O
}	O
m	O
,	O
it	O
also	O
holds	O
if	O
we	O
sample	O
each	O
component	O
of	O
σ	O
uniformly	O
at	O
random	O
from	O
the	O
uniform	O
distribution	O
over	O
{	O
±1	O
}	O
,	O
denoted	O
u±	O
.	O
hence	O
,	O
equation	O
(	O
6.5	O
)	O
also	O
equals	O
e	O
σ∼u	O
m±	O
e	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
∼dm	O
sup	O
h∈h	O
σi	O
(	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
z	O
(	O
cid:48	O
)	O
i	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
)	O
and	O
by	O
the	O
linearity	O
of	O
expectation	O
it	O
also	O
equals	O
e	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
∼dm	O
e	O
σ∼u	O
m±	O
σi	O
(	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
z	O
(	O
cid:48	O
)	O
i	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
)	O
next	O
,	O
ﬁx	O
s	O
and	O
s	O
(	O
cid:48	O
)	O
,	O
and	O
let	O
c	O
be	O
the	O
instances	O
appearing	O
in	O
s	O
and	O
s	O
(	O
cid:48	O
)	O
.	O
then	O
,	O
we	O
can	O
take	O
the	O
supremum	O
only	O
over	O
h	O
∈	O
hc	O
.	O
therefore	O
,	O
i	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
)	O
σi	O
(	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
z	O
(	O
cid:48	O
)	O
(	O
cid:34	O
)	O
(	O
cid:35	O
)	O
e	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
σ∼u	O
m±	O
sup	O
h∈h	O
1	O
m	O
(	O
cid:34	O
)	O
σ∼u	O
m±	O
max	O
h∈hc	O
=	O
e	O
1	O
m	O
fix	O
some	O
h	O
∈	O
hc	O
and	O
denote	O
θh	O
=	O
1	O
i	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
)	O
.	O
since	O
e	O
[	O
θh	O
]	O
=	O
0	O
and	O
θh	O
is	O
an	O
average	O
of	O
independent	O
variables	O
,	O
each	O
of	O
which	O
takes	O
values	O
in	O
[	O
−1	O
,	O
1	O
]	O
,	O
we	O
have	O
by	O
hoeﬀding	O
’	O
s	O
inequality	O
that	O
for	O
every	O
ρ	O
>	O
0	O
,	O
i	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
)	O
m	O
.	O
p	O
[	O
|θh|	O
>	O
ρ	O
]	O
≤	O
2	O
exp	O
(	O
cid:0	O
)	O
−2	O
m	O
ρ2	O
(	O
cid:1	O
)	O
.	O
applying	O
the	O
union	B
bound	I
over	O
h	O
∈	O
hc	O
,	O
we	O
obtain	O
that	O
for	O
any	O
ρ	O
>	O
0	O
,	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
m	O
(	O
cid:88	O
)	O
(	O
cid:80	O
)	O
m	O
i=1	O
σi	O
(	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
z	O
(	O
cid:48	O
)	O
σi	O
(	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
z	O
(	O
cid:48	O
)	O
i=1	O
finally	O
,	O
lemma	O
a.4	O
in	O
appendix	O
a	O
tells	O
us	O
that	O
the	O
preceding	O
implies	O
(	O
cid:20	O
)	O
p	O
(	O
cid:20	O
)	O
|θh|	O
>	O
ρ	O
max	O
h∈hc	O
(	O
cid:20	O
)	O
e	O
|θh|	O
max	O
h∈hc	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
≤	O
2|hc|	O
exp	O
(	O
cid:0	O
)	O
−2	O
m	O
ρ2	O
(	O
cid:1	O
)	O
.	O
≤	O
4	O
+	O
(	O
cid:112	O
)	O
log	O
(	O
|hc|	O
)	O
(	O
cid:21	O
)	O
≤	O
4	O
+	O
(	O
cid:112	O
)	O
log	O
(	O
τh	O
(	O
2m	O
)	O
)	O
√	O
√	O
2m	O
.	O
.	O
2m	O
e	O
s∼dm	O
sup	O
h∈h	O
|ld	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
|	O
combining	O
all	O
with	O
the	O
deﬁnition	O
of	O
τh	O
,	O
we	O
have	O
shown	O
that	O
78	O
the	O
vc-dimension	O
6.6	O
summary	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
characterizes	O
pac	O
learnability	O
of	O
classes	O
of	O
binary	O
classiﬁers	O
using	O
vc-dimension	O
.	O
the	O
vc-dimension	O
of	O
a	O
class	O
is	O
a	O
combinatorial	O
property	O
that	O
denotes	O
the	O
maximal	O
sample	O
size	O
that	O
can	O
be	O
shattered	O
by	O
the	O
class	O
.	O
the	O
fundamental	O
theorem	O
states	O
that	O
a	O
class	O
is	O
pac	O
learn-	O
able	O
if	O
and	O
only	O
if	O
its	O
vc-dimension	O
is	O
ﬁnite	O
and	O
speciﬁes	O
the	O
sample	B
complexity	I
required	O
for	O
pac	O
learning	O
.	O
the	O
theorem	O
also	O
shows	O
that	O
if	O
a	O
problem	O
is	O
at	O
all	O
learnable	O
,	O
then	O
uniform	B
convergence	I
holds	O
and	O
therefore	O
the	O
problem	O
is	O
learnable	O
using	O
the	O
erm	O
rule	O
.	O
6.7	O
bibliographic	O
remarks	O
the	O
deﬁnition	O
of	O
vc-dimension	O
and	O
its	O
relation	O
to	O
learnability	O
and	O
to	O
uniform	B
convergence	I
is	O
due	O
to	O
the	O
seminal	O
work	O
of	O
vapnik	O
&	O
chervonenkis	O
(	O
1971	O
)	O
.	O
the	O
relation	O
to	O
the	O
deﬁnition	O
of	O
pac	O
learnability	O
is	O
due	O
to	O
blumer	O
,	O
ehrenfeucht	O
,	O
haussler	O
&	O
warmuth	O
(	O
1989	O
)	O
.	O
several	O
generalizations	O
of	O
the	O
vc-dimension	O
have	O
been	O
proposed	O
.	O
for	O
exam-	O
ple	O
,	O
the	O
fat-shattering	O
dimension	B
characterizes	O
learnability	O
of	O
some	O
regression	B
problems	O
(	O
kearns	O
,	O
schapire	O
&	O
sellie	O
1994	O
,	O
alon	O
,	O
ben-david	O
,	O
cesa-bianchi	O
&	O
haussler	O
1997	O
,	O
bartlett	O
,	O
long	O
&	O
williamson	O
1994	O
,	O
anthony	O
&	O
bartlet	O
1999	O
)	O
,	O
and	O
the	O
natarajan	O
dimension	B
characterizes	O
learnability	O
of	O
some	O
multiclass	B
learning	O
problems	O
(	O
natarajan	O
1989	O
)	O
.	O
however	O
,	O
in	O
general	O
,	O
there	O
is	O
no	O
equivalence	O
between	O
learnability	O
and	O
uniform	B
convergence	I
.	O
see	O
(	O
shalev-shwartz	O
,	O
shamir	O
,	O
srebro	O
&	O
sridharan	O
2010	O
,	O
daniely	O
,	O
sabato	O
,	O
ben-david	O
&	O
shalev-shwartz	O
2011	O
)	O
.	O
sauer	O
’	O
s	O
lemma	O
has	O
been	O
proved	O
by	O
sauer	O
in	O
response	O
to	O
a	O
problem	O
of	O
erdos	O
(	O
sauer	O
1972	O
)	O
.	O
shelah	O
(	O
with	O
perles	O
)	O
proved	O
it	O
as	O
a	O
useful	O
lemma	O
for	O
shelah	O
’	O
s	O
theory	O
of	O
stable	O
models	O
(	O
shelah	O
1972	O
)	O
.	O
gil	O
kalai	O
tells1	O
us	O
that	O
at	O
some	O
later	O
time	O
,	O
benjy	O
weiss	O
asked	O
perles	O
about	O
such	O
a	O
result	O
in	O
the	O
context	O
of	O
ergodic	O
theory	O
,	O
and	O
perles	O
,	O
who	O
forgot	O
that	O
he	O
had	O
proved	O
it	O
once	O
,	O
proved	O
it	O
again	O
.	O
vapnik	O
and	O
chervonenkis	O
proved	O
the	O
lemma	O
in	O
the	O
context	O
of	O
statistical	O
learning	O
theory	O
.	O
6.8	O
exercises	O
hypothesis	B
classes	O
if	O
h	O
(	O
cid:48	O
)	O
⊆	O
h	O
then	O
vcdim	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
≤	O
vcdim	O
(	O
h	O
)	O
.	O
1.	O
show	O
the	O
following	O
monotonicity	O
property	O
of	O
vc-dimension	O
:	O
for	O
every	O
two	O
2.	O
given	O
some	O
ﬁnite	O
domain	B
set	O
,	O
x	O
,	O
and	O
a	O
number	O
k	O
≤	O
|x|	O
,	O
ﬁgure	O
out	O
the	O
vc-	O
=k	O
=	O
{	O
h	O
∈	O
{	O
0	O
,	O
1	O
}	O
x	O
:	O
|	O
{	O
x	O
:	O
h	O
(	O
x	O
)	O
=	O
1	O
}	O
|	O
=	O
k	O
}	O
.	O
that	O
is	O
,	O
the	O
set	B
of	O
all	O
functions	O
that	O
assign	O
the	O
value	O
1	O
to	O
exactly	O
k	O
elements	O
of	O
x	O
.	O
dimension	B
of	O
each	O
of	O
the	O
following	O
classes	O
(	O
and	O
prove	O
your	O
claims	O
)	O
:	O
1.	O
hx	O
1	O
http	O
:	O
//gilkalai.wordpress.com/2008/09/28/	O
extremal-combinatorics-iii-some-basic-theorems	O
6.8	O
exercises	O
79	O
2.	O
hat−most−k	O
=	O
{	O
h	O
∈	O
{	O
0	O
,	O
1	O
}	O
x	O
:	O
|	O
{	O
x	O
:	O
h	O
(	O
x	O
)	O
=	O
1	O
}	O
|	O
≤	O
k	O
or	O
|	O
{	O
x	O
:	O
h	O
(	O
x	O
)	O
=	O
0	O
}	O
|	O
≤	O
k	O
}	O
.	O
3.	O
let	O
x	O
be	O
the	O
boolean	O
hypercube	O
{	O
0	O
,	O
1	O
}	O
n.	O
for	O
a	O
set	B
i	O
⊆	O
{	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
we	O
deﬁne	O
a	O
parity	O
function	B
hi	O
as	O
follows	O
.	O
on	O
a	O
binary	O
vector	O
x	O
=	O
(	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
∈	O
{	O
0	O
,	O
1	O
}	O
n	O
,	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
i∈i	O
hi	O
(	O
x	O
)	O
=	O
xi	O
mod	O
2	O
.	O
(	O
that	O
is	O
,	O
hi	O
computes	O
parity	O
of	O
bits	O
in	O
i	O
.	O
)	O
what	O
is	O
the	O
vc-dimension	O
of	O
the	O
class	O
of	O
all	O
such	O
parity	O
functions	O
,	O
hn-parity	O
=	O
{	O
hi	O
:	O
i	O
⊆	O
{	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
}	O
?	O
4.	O
we	O
proved	O
sauer	O
’	O
s	O
lemma	O
by	O
proving	O
that	O
for	O
every	O
class	O
h	O
of	O
ﬁnite	O
vc-	O
dimension	B
d	O
,	O
and	O
every	O
subset	O
a	O
of	O
the	O
domain	B
,	O
|ha|	O
≤	O
|	O
{	O
b	O
⊆	O
a	O
:	O
h	O
shatters	O
b	O
}	O
|	O
≤	O
d	O
(	O
cid:88	O
)	O
i=0	O
(	O
cid:18	O
)	O
|a|	O
(	O
cid:19	O
)	O
.	O
i	O
show	O
that	O
there	O
are	O
cases	O
in	O
which	O
the	O
previous	O
two	O
inequalities	O
are	O
strict	O
(	O
namely	O
,	O
the	O
≤	O
can	O
be	O
replaced	O
by	O
<	O
)	O
and	O
cases	O
in	O
which	O
they	O
can	O
be	O
replaced	O
by	O
equalities	O
.	O
demonstrate	O
all	O
four	O
combinations	O
of	O
=	O
and	O
<	O
.	O
5.	O
vc-dimension	O
of	O
axis	O
aligned	O
rectangles	O
in	O
rd	O
:	O
let	O
hd	O
axis	O
aligned	O
rectangles	O
in	O
rd	O
.	O
we	O
have	O
already	O
seen	O
that	O
vcdim	O
(	O
h2	O
prove	O
that	O
in	O
general	O
,	O
vcdim	O
(	O
hd	O
rec	O
be	O
the	O
class	O
of	O
rec	O
)	O
=	O
4.	O
rec	O
)	O
=	O
2d	O
.	O
6.	O
vc-dimension	O
of	O
boolean	B
conjunctions	I
:	O
let	O
hd	O
con	O
)	O
.	O
con	O
be	O
the	O
class	O
of	O
boolean	B
conjunctions	I
over	O
the	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
(	O
d	O
≥	O
2	O
)	O
.	O
we	O
already	O
know	O
that	O
this	O
class	O
is	O
ﬁnite	O
and	O
thus	O
(	O
agnostic	O
)	O
pac	O
learnable	O
.	O
in	O
this	O
question	O
we	O
calculate	O
vcdim	O
(	O
hd	O
1.	O
show	O
that	O
|hd	O
2.	O
conclude	O
that	O
vcdim	O
(	O
h	O
)	O
≤	O
d	O
log	O
3	O
.	O
3.	O
show	O
that	O
hd	O
4	O
.	O
(	O
**	O
)	O
show	O
that	O
vcdim	O
(	O
hd	O
con	O
shatters	O
the	O
set	B
of	O
unit	O
vectors	O
{	O
ei	O
:	O
i	O
≤	O
d	O
}	O
.	O
con|	O
≤	O
3d	O
+	O
1.	O
hint	O
:	O
assume	O
by	O
contradiction	O
that	O
there	O
exists	O
a	O
set	B
c	O
=	O
{	O
c1	O
,	O
.	O
.	O
.	O
,	O
cd+1	O
}	O
that	O
is	O
shattered	O
by	O
hd	O
con	O
that	O
satisfy	O
con	O
.	O
let	O
h1	O
,	O
.	O
.	O
.	O
,	O
hd+1	O
be	O
hypotheses	O
in	O
hd	O
con	O
)	O
≤	O
d.	O
(	O
cid:40	O
)	O
∀i	O
,	O
j	O
∈	O
[	O
d	O
+	O
1	O
]	O
,	O
hi	O
(	O
cj	O
)	O
=	O
0	O
i	O
=	O
j	O
1	O
otherwise	O
for	O
each	O
i	O
∈	O
[	O
d	O
+	O
1	O
]	O
,	O
hi	O
(	O
or	O
more	O
accurately	O
,	O
the	O
conjunction	O
that	O
corre-	O
sponds	O
to	O
hi	O
)	O
contains	O
some	O
literal	O
(	O
cid:96	O
)	O
i	O
which	O
is	O
false	O
on	O
ci	O
and	O
true	O
on	O
cj	O
for	O
each	O
j	O
(	O
cid:54	O
)	O
=	O
i.	O
use	O
the	O
pigeonhole	O
principle	O
to	O
show	O
that	O
there	O
must	O
be	O
a	O
pair	O
i	O
<	O
j	O
≤	O
d	O
+	O
1	O
such	O
that	O
(	O
cid:96	O
)	O
i	O
and	O
(	O
cid:96	O
)	O
j	O
use	O
the	O
same	O
xk	O
and	O
use	O
that	O
fact	O
to	O
derive	O
a	O
contradiction	O
to	O
the	O
requirements	O
from	O
the	O
conjunctions	O
hi	O
,	O
hj	O
.	O
mcon	O
of	O
monotone	O
boolean	B
conjunctions	I
over	O
{	O
0	O
,	O
1	O
}	O
d.	O
monotonicity	O
here	O
means	O
that	O
the	O
conjunctions	O
do	O
not	O
contain	O
negations	O
.	O
5.	O
consider	O
the	O
class	O
hd	O
80	O
the	O
vc-dimension	O
as	O
in	O
hd	O
pothesis	O
.	O
we	O
augment	O
hd	O
that	O
vcdim	O
(	O
hd	O
mcon	O
)	O
=	O
d.	O
con	O
,	O
the	O
empty	O
conjunction	O
is	O
interpreted	O
as	O
the	O
all-positive	O
hy-	O
mcon	O
with	O
the	O
all-negative	O
hypothesis	B
h−	O
.	O
show	O
7.	O
we	O
have	O
shown	O
that	O
for	O
a	O
ﬁnite	O
hypothesis	B
class	I
h	O
,	O
vcdim	O
(	O
h	O
)	O
≤	O
(	O
cid:98	O
)	O
log	O
(	O
|h|	O
)	O
(	O
cid:99	O
)	O
.	O
however	O
,	O
this	O
is	O
just	O
an	O
upper	O
bound	O
.	O
the	O
vc-dimension	O
of	O
a	O
class	O
can	O
be	O
much	O
lower	O
than	O
that	O
:	O
1.	O
find	O
an	O
example	O
of	O
a	O
class	O
h	O
of	O
functions	O
over	O
the	O
real	O
interval	O
x	O
=	O
[	O
0	O
,	O
1	O
]	O
2.	O
give	O
an	O
example	O
of	O
a	O
ﬁnite	O
hypothesis	B
class	I
h	O
over	O
the	O
domain	B
x	O
=	O
[	O
0	O
,	O
1	O
]	O
,	O
such	O
that	O
h	O
is	O
inﬁnite	O
while	O
vcdim	O
(	O
h	O
)	O
=	O
1.	O
where	O
vcdim	O
(	O
h	O
)	O
=	O
(	O
cid:98	O
)	O
log2	O
(	O
|h|	O
)	O
(	O
cid:99	O
)	O
.	O
8	O
.	O
(	O
*	O
)	O
it	O
is	O
often	O
the	O
case	O
that	O
the	O
vc-dimension	O
of	O
a	O
hypothesis	B
class	I
equals	O
(	O
or	O
can	O
be	O
bounded	O
above	O
by	O
)	O
the	O
number	O
of	O
parameters	O
one	O
needs	O
to	O
set	B
in	O
order	O
to	O
deﬁne	O
each	O
hypothesis	B
in	O
the	O
class	O
.	O
for	O
instance	B
,	O
if	O
h	O
is	O
the	O
class	O
of	O
axis	O
aligned	O
rectangles	O
in	O
rd	O
,	O
then	O
vcdim	O
(	O
h	O
)	O
=	O
2d	O
,	O
which	O
is	O
equal	O
to	O
the	O
number	O
of	O
parameters	O
used	O
to	O
deﬁne	O
a	O
rectangle	O
in	O
rd	O
.	O
here	O
is	O
an	O
example	O
that	O
shows	O
that	O
this	O
is	O
not	O
always	O
the	O
case	O
.	O
we	O
will	O
see	O
that	O
a	O
hypothesis	B
class	I
might	O
be	O
very	O
complex	O
and	O
even	O
not	O
learnable	O
,	O
although	O
it	O
has	O
a	O
small	O
number	O
of	O
parameters	O
.	O
consider	O
the	O
domain	B
x	O
=	O
r	O
,	O
and	O
the	O
hypothesis	B
class	I
h	O
=	O
{	O
x	O
(	O
cid:55	O
)	O
→	O
(	O
cid:100	O
)	O
sin	O
(	O
θx	O
)	O
(	O
cid:101	O
)	O
:	O
θ	O
∈	O
r	O
}	O
(	O
here	O
,	O
we	O
take	O
(	O
cid:100	O
)	O
−1	O
(	O
cid:101	O
)	O
=	O
0	O
)	O
.	O
prove	O
that	O
vcdim	O
(	O
h	O
)	O
=	O
∞	O
.	O
hint	O
:	O
there	O
is	O
more	O
than	O
one	O
way	O
to	O
prove	O
the	O
required	O
result	O
.	O
one	O
option	O
is	O
by	O
applying	O
the	O
following	O
lemma	O
:	O
if	O
0.x1x2x3	O
.	O
.	O
.	O
,	O
is	O
the	O
binary	O
expansion	O
of	O
x	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
then	O
for	O
any	O
natural	O
number	O
m	O
,	O
(	O
cid:100	O
)	O
sin	O
(	O
2mπx	O
)	O
(	O
cid:101	O
)	O
=	O
(	O
1−	O
xm	O
)	O
,	O
provided	O
that	O
∃k	O
≥	O
m	O
s.t	O
.	O
xk	O
=	O
1.	O
h	O
=	O
{	O
ha	O
,	O
b	O
,	O
s	O
:	O
a	O
≤	O
b	O
,	O
s	O
∈	O
{	O
−1	O
,	O
1	O
}	O
}	O
where	O
9.	O
let	O
h	O
be	O
the	O
class	O
of	O
signed	O
intervals	O
,	O
that	O
is	O
,	O
(	O
cid:40	O
)	O
ha	O
,	O
b	O
,	O
s	O
(	O
x	O
)	O
=	O
s	O
−s	O
if	O
x	O
∈	O
[	O
a	O
,	O
b	O
]	O
if	O
x	O
/∈	O
[	O
a	O
,	O
b	O
]	O
calculate	O
vcdim	O
(	O
h	O
)	O
.	O
10.	O
let	O
h	O
be	O
a	O
class	O
of	O
functions	O
from	O
x	O
to	O
{	O
0	O
,	O
1	O
}	O
.	O
1.	O
prove	O
that	O
if	O
vcdim	O
(	O
h	O
)	O
≥	O
d	O
,	O
for	O
any	O
d	O
,	O
then	O
for	O
some	O
probability	O
distri-	O
bution	O
d	O
over	O
x	O
×	O
{	O
0	O
,	O
1	O
}	O
,	O
for	O
every	O
sample	O
size	O
,	O
m	O
,	O
e	O
s∼dm	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
≥	O
min	O
h∈h	O
ld	O
(	O
h	O
)	O
+	O
d	O
−	O
m	O
2d	O
hint	O
:	O
use	O
exercise	O
3	O
in	O
chapter	O
5	O
.	O
2.	O
prove	O
that	O
for	O
every	O
h	O
that	O
is	O
pac	O
learnable	O
,	O
vcdim	O
(	O
h	O
)	O
<	O
∞	O
.	O
(	O
note	O
that	O
this	O
is	O
the	O
implication	O
3	O
→	O
6	O
in	O
theorem	O
6.7	O
.	O
)	O
11.	O
vc	O
of	O
union	O
:	O
let	O
h1	O
,	O
.	O
.	O
.	O
,	O
hr	O
be	O
hypothesis	B
classes	O
over	O
some	O
ﬁxed	O
domain	B
set	O
x	O
.	O
let	O
d	O
=	O
maxi	O
vcdim	O
(	O
hi	O
)	O
and	O
assume	O
for	O
simplicity	O
that	O
d	O
≥	O
3	O
.	O
6.8	O
exercises	O
81	O
1.	O
prove	O
that	O
vcdim	O
(	O
∪r	O
i=1hi	O
)	O
≤	O
4d	O
log	O
(	O
2d	O
)	O
+	O
2	O
log	O
(	O
r	O
)	O
.	O
hint	O
:	O
take	O
a	O
set	B
of	O
k	O
examples	O
and	O
assume	O
that	O
they	O
are	O
shattered	O
by	O
the	O
union	O
class	O
.	O
therefore	O
,	O
the	O
union	O
class	O
can	O
produce	O
all	O
2k	O
possible	O
labelings	O
on	O
these	O
examples	O
.	O
use	O
sauer	O
’	O
s	O
lemma	O
to	O
show	O
that	O
the	O
union	O
class	O
can	O
not	O
produce	O
more	O
than	O
rkd	O
labelings	O
.	O
therefore	O
,	O
2k	O
<	O
rkd	O
.	O
now	O
use	O
lemma	O
a.2	O
.	O
2	O
.	O
(	O
*	O
)	O
prove	O
that	O
for	O
r	O
=	O
2	O
it	O
holds	O
that	O
vcdim	O
(	O
h1	O
∪	O
h2	O
)	O
≤	O
2d	O
+	O
1	O
.	O
12.	O
dudley	O
classes	O
:	O
in	O
this	O
question	O
we	O
discuss	O
an	O
algebraic	O
framework	O
for	O
deﬁning	O
concept	O
classes	O
over	O
rn	O
and	O
show	O
a	O
connection	O
between	O
the	O
vc	O
dimension	B
of	O
such	O
classes	O
and	O
their	O
algebraic	O
properties	O
.	O
given	O
a	O
function	B
f	O
:	O
rn	O
→	O
r	O
we	O
deﬁne	O
the	O
corresponding	O
function	B
,	O
pos	O
(	O
f	O
)	O
(	O
x	O
)	O
=	O
1	O
[	O
f	O
(	O
x	O
)	O
>	O
0	O
]	O
.	O
for	O
a	O
class	O
f	O
of	O
real	O
valued	O
functions	O
we	O
deﬁne	O
a	O
corresponding	O
class	O
of	O
functions	O
pos	O
(	O
f	O
)	O
=	O
{	O
pos	O
(	O
f	O
)	O
:	O
f	O
∈	O
f	O
}	O
.	O
we	O
say	O
that	O
a	O
family	O
,	O
f	O
,	O
of	O
real	O
valued	O
func-	O
tions	O
is	O
linearly	O
closed	O
if	O
for	O
all	O
f	O
,	O
g	O
∈	O
f	O
and	O
r	O
∈	O
r	O
,	O
(	O
f	O
+	O
rg	O
)	O
∈	O
f	O
(	O
where	O
addition	O
and	O
scalar	O
multiplication	O
of	O
functions	O
are	O
deﬁned	O
point	O
wise	O
,	O
namely	O
,	O
for	O
all	O
x	O
∈	O
rn	O
,	O
(	O
f	O
+	O
rg	O
)	O
(	O
x	O
)	O
=	O
f	O
(	O
x	O
)	O
+	O
rg	O
(	O
x	O
)	O
)	O
.	O
note	O
that	O
if	O
a	O
family	O
of	O
functions	O
is	O
linearly	O
closed	O
then	O
we	O
can	O
view	O
it	O
as	O
a	O
vector	O
space	O
over	O
the	O
reals	O
.	O
for	O
a	O
function	B
g	O
:	O
rn	O
→	O
r	O
and	O
a	O
family	O
of	O
functions	O
f	O
,	O
let	O
f	O
+g	O
def=	O
{	O
f	O
+g	O
:	O
f	O
∈	O
f	O
}	O
.	O
hypothesis	B
classes	O
that	O
have	O
a	O
representation	O
as	O
pos	O
(	O
f	O
+	O
g	O
)	O
for	O
some	O
vector	O
space	O
of	O
functions	O
f	O
and	O
some	O
function	B
g	O
are	O
called	O
dudley	O
classes	O
.	O
1.	O
show	O
that	O
for	O
every	O
g	O
:	O
rn	O
→	O
r	O
and	O
every	O
vector	O
space	O
of	O
functions	O
f	O
as	O
deﬁned	O
earlier	O
,	O
vcdim	O
(	O
pos	O
(	O
f	O
+	O
g	O
)	O
)	O
=	O
vcdim	O
(	O
pos	O
(	O
f	O
)	O
)	O
.	O
2	O
.	O
(	O
**	O
)	O
for	O
every	O
linearly	O
closed	O
family	O
of	O
real	O
valued	O
functions	O
f	O
,	O
the	O
vc-	O
dimension	B
of	O
the	O
corresponding	O
class	O
pos	O
(	O
f	O
)	O
equals	O
the	O
linear	O
dimension	O
of	O
f	O
(	O
as	O
a	O
vector	O
space	O
)	O
.	O
hint	O
:	O
let	O
f1	O
,	O
.	O
.	O
.	O
,	O
fd	O
be	O
a	O
basis	O
for	O
the	O
vector	O
space	O
f.	O
consider	O
the	O
mapping	O
x	O
(	O
cid:55	O
)	O
→	O
(	O
f1	O
(	O
x	O
)	O
,	O
.	O
.	O
.	O
,	O
fd	O
(	O
x	O
)	O
)	O
(	O
from	O
rn	O
to	O
rd	O
)	O
.	O
note	O
that	O
this	O
mapping	O
induces	O
a	O
matching	O
between	O
functions	O
over	O
rn	O
of	O
the	O
form	O
pos	O
(	O
f	O
)	O
and	O
homogeneous	O
linear	O
halfspaces	O
in	O
rd	O
(	O
the	O
vc-dimension	O
of	O
the	O
class	O
of	O
homogeneous	O
linear	O
halfspaces	O
is	O
analyzed	O
in	O
chapter	O
9	O
)	O
.	O
3.	O
show	O
that	O
each	O
of	O
the	O
following	O
classes	O
can	O
be	O
represented	O
as	O
a	O
dudley	O
class	O
:	O
1.	O
the	O
class	O
hsn	O
of	O
halfspaces	O
over	O
rn	O
(	O
see	O
chapter	O
9	O
)	O
.	O
2.	O
the	O
class	O
hhsn	O
of	O
all	O
homogeneous	O
halfspaces	O
over	O
rn	O
(	O
see	O
chapter	O
9	O
)	O
.	O
3.	O
the	O
class	O
bd	O
of	O
all	O
functions	O
deﬁned	O
by	O
(	O
open	O
)	O
balls	O
in	O
rd	O
.	O
use	O
the	O
dudley	O
representation	O
to	O
ﬁgure	O
out	O
the	O
vc-dimension	O
of	O
this	O
class	O
.	O
4.	O
let	O
p	O
d	O
n	O
denote	O
the	O
class	O
of	O
functions	O
deﬁned	O
by	O
polynomial	O
inequalities	O
of	O
degree	O
≤	O
d	O
,	O
namely	O
,	O
n	O
=	O
{	O
hp	O
:	O
p	O
is	O
a	O
polynomial	O
of	O
degree	O
≤	O
d	O
in	O
the	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
,	O
p	O
d	O
82	O
the	O
vc-dimension	O
where	O
,	O
for	O
x	O
=	O
(	O
x1	O
.	O
.	O
.	O
.	O
,	O
xn	O
)	O
,	O
hp	O
(	O
x	O
)	O
=	O
1	O
[	O
p	O
(	O
x	O
)	O
≥0	O
]	O
(	O
the	O
degree	O
of	O
a	O
multi-	O
variable	O
polynomial	O
is	O
the	O
maximal	O
sum	O
of	O
variable	O
exponents	O
over	O
all	O
of	O
its	O
terms	O
.	O
for	O
example	O
,	O
the	O
degree	O
of	O
p	O
(	O
x	O
)	O
=	O
3x3	O
1.	O
use	O
the	O
dudley	O
representation	O
to	O
ﬁgure	O
out	O
the	O
vc-dimension	O
of	O
the	O
2	O
+	O
4x3x2	O
7	O
is	O
5	O
)	O
.	O
1x2	O
class	O
p	O
d	O
1	O
–	O
the	O
class	O
of	O
all	O
d-degree	O
polynomials	O
over	O
r.	O
2.	O
prove	O
that	O
the	O
class	O
of	O
all	O
polynomial	O
classiﬁers	O
over	O
r	O
has	O
inﬁnite	O
vc-dimension	O
.	O
3.	O
use	O
the	O
dudley	O
representation	O
to	O
ﬁgure	O
out	O
the	O
vc-dimension	O
of	O
the	O
class	O
p	O
d	O
n	O
(	O
as	O
a	O
function	B
of	O
d	O
and	O
n	O
)	O
.	O
7	O
nonuniform	O
learnability	O
the	O
notions	O
of	O
pac	O
learnability	O
discussed	O
so	O
far	O
in	O
the	O
book	O
allow	O
the	O
sample	O
sizes	O
to	O
depend	O
on	O
the	O
accuracy	B
and	O
conﬁdence	B
parameters	O
,	O
but	O
they	O
are	O
uniform	O
with	O
respect	O
to	O
the	O
labeling	O
rule	O
and	O
the	O
underlying	O
data	O
distribution	O
.	O
conse-	O
quently	O
,	O
classes	O
that	O
are	O
learnable	O
in	O
that	O
respect	O
are	O
limited	O
(	O
they	O
must	O
have	O
a	O
ﬁnite	O
vc-dimension	O
,	O
as	O
stated	O
by	O
theorem	O
6.7	O
)	O
.	O
in	O
this	O
chapter	O
we	O
consider	O
more	O
relaxed	O
,	O
weaker	O
notions	O
of	O
learnability	O
.	O
we	O
discuss	O
the	O
usefulness	O
of	O
such	O
notions	O
and	O
provide	O
characterization	O
of	O
the	O
concept	O
classes	O
that	O
are	O
learnable	O
using	O
these	O
deﬁnitions	O
.	O
we	O
begin	O
this	O
discussion	O
by	O
deﬁning	O
a	O
notion	O
of	O
“	O
nonuniform	O
learnability	O
”	O
that	O
allows	O
the	O
sample	O
size	O
to	O
depend	O
on	O
the	O
hypothesis	B
to	O
which	O
the	O
learner	O
is	O
com-	O
pared	O
.	O
we	O
then	O
provide	O
a	O
characterization	O
of	O
nonuniform	O
learnability	O
and	O
show	O
that	O
nonuniform	O
learnability	O
is	O
a	O
strict	O
relaxation	O
of	O
agnostic	O
pac	O
learnability	O
.	O
we	O
also	O
show	O
that	O
a	O
suﬃcient	O
condition	O
for	O
nonuniform	O
learnability	O
is	O
that	O
h	O
is	O
a	O
countable	O
union	O
of	O
hypothesis	B
classes	O
,	O
each	O
of	O
which	O
enjoys	O
the	O
uniform	O
con-	O
vergence	O
property	O
.	O
these	O
results	O
will	O
be	O
proved	O
in	O
section	O
7.2	O
by	O
introducing	O
a	O
new	O
learning	O
paradigm	O
,	O
which	O
is	O
called	O
structural	O
risk	B
minimization	O
(	O
srm	O
)	O
.	O
in	O
section	O
7.3	O
we	O
specify	O
the	O
srm	O
paradigm	O
for	O
countable	O
hypothesis	B
classes	O
,	O
which	O
yields	O
the	O
minimum	O
description	O
length	O
(	O
mdl	O
)	O
paradigm	O
.	O
the	O
mdl	O
paradigm	O
gives	O
a	O
formal	O
justiﬁcation	O
to	O
a	O
philosophical	O
principle	O
of	O
induction	O
called	O
oc-	O
cam	O
’	O
s	O
razor	O
.	O
next	O
,	O
in	O
section	O
7.4	O
we	O
introduce	O
consistency	B
as	O
an	O
even	O
weaker	O
notion	O
of	O
learnability	O
.	O
finally	O
,	O
we	O
discuss	O
the	O
signiﬁcance	O
and	O
usefulness	O
of	O
the	O
diﬀerent	O
notions	O
of	O
learnability	O
.	O
7.1	O
nonuniform	O
learnability	O
“	O
nonuniform	O
learnability	O
”	O
allows	O
the	O
sample	O
size	O
to	O
be	O
nonuniform	O
with	O
respect	O
to	O
the	O
diﬀerent	O
hypotheses	O
with	O
which	O
the	O
learner	O
is	O
competing	O
.	O
we	O
say	O
that	O
a	O
hypothesis	B
h	O
is	O
(	O
	O
,	O
δ	O
)	O
-competitive	O
with	O
another	O
hypothesis	B
h	O
(	O
cid:48	O
)	O
if	O
,	O
with	O
probability	O
higher	O
than	O
(	O
1	O
−	O
δ	O
)	O
,	O
ld	O
(	O
h	O
)	O
≤	O
ld	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
+	O
	O
.	O
in	O
pac	O
learnability	O
,	O
this	O
notion	O
of	O
“	O
competitiveness	O
”	O
is	O
not	O
very	O
useful	O
,	O
as	O
we	O
are	O
looking	O
for	O
a	O
hypothesis	B
with	O
an	O
absolute	O
low	O
risk	B
(	O
in	O
the	O
realizable	O
case	O
)	O
or	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
84	O
nonuniform	O
learnability	O
with	O
a	O
low	O
risk	B
compared	O
to	O
the	O
minimal	O
risk	B
achieved	O
by	O
hypotheses	O
in	O
our	O
class	O
(	O
in	O
the	O
agnostic	O
case	O
)	O
.	O
therefore	O
,	O
the	O
sample	O
size	O
depends	O
only	O
on	O
the	O
accuracy	B
and	O
conﬁdence	B
parameters	O
.	O
in	O
nonuniform	O
learnability	O
,	O
however	O
,	O
we	O
allow	O
the	O
sample	O
size	O
to	O
be	O
of	O
the	O
form	O
mh	O
(	O
	O
,	O
δ	O
,	O
h	O
)	O
;	O
namely	O
,	O
it	O
depends	O
also	O
on	O
the	O
h	O
with	O
which	O
we	O
are	O
competing	O
.	O
formally	O
,	O
definition	O
7.1	O
a	O
hypothesis	B
class	I
h	O
is	O
nonuniformly	O
learnable	O
if	O
there	O
exist	O
a	O
learning	O
algorithm	O
,	O
a	O
,	O
and	O
a	O
function	B
mnulh	O
:	O
(	O
0	O
,	O
1	O
)	O
2×h	O
→	O
n	O
such	O
that	O
,	O
for	O
every	O
	O
,	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
and	O
for	O
every	O
h	O
∈	O
h	O
,	O
if	O
m	O
≥	O
mnulh	O
(	O
	O
,	O
δ	O
,	O
h	O
)	O
then	O
for	O
every	O
distribution	O
d	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
the	O
choice	O
of	O
s	O
∼	O
dm	O
,	O
it	O
holds	O
that	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
ld	O
(	O
h	O
)	O
+	O
	O
.	O
at	O
this	O
point	O
it	O
might	O
be	O
useful	O
to	O
recall	B
the	O
deﬁnition	O
of	O
agnostic	O
pac	O
learn-	O
ability	O
(	O
deﬁnition	O
3.3	O
)	O
:	O
a	O
hypothesis	B
class	I
h	O
is	O
agnostically	O
pac	O
learnable	O
if	O
there	O
exist	O
a	O
learning	O
algo-	O
rithm	O
,	O
a	O
,	O
and	O
a	O
function	B
mh	O
:	O
(	O
0	O
,	O
1	O
)	O
2	O
→	O
n	O
such	O
that	O
,	O
for	O
every	O
	O
,	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
and	O
for	O
every	O
distribution	O
d	O
,	O
if	O
m	O
≥	O
mh	O
(	O
	O
,	O
δ	O
)	O
,	O
then	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
the	O
choice	O
of	O
s	O
∼	O
dm	O
it	O
holds	O
that	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
min	O
note	O
that	O
this	O
implies	O
that	O
for	O
every	O
h	O
∈	O
h	O
h	O
(	O
cid:48	O
)	O
∈h	O
ld	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
+	O
	O
.	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
ld	O
(	O
h	O
)	O
+	O
	O
.	O
in	O
both	O
types	O
of	O
learnability	O
,	O
we	O
require	O
that	O
the	O
output	O
hypothesis	B
will	O
be	O
(	O
	O
,	O
δ	O
)	O
-competitive	O
with	O
every	O
other	O
hypothesis	B
in	O
the	O
class	O
.	O
but	O
the	O
diﬀerence	O
between	O
these	O
two	O
notions	O
of	O
learnability	O
is	O
the	O
question	O
of	O
whether	O
the	O
sample	O
size	O
m	O
may	O
depend	O
on	O
the	O
hypothesis	B
h	O
to	O
which	O
the	O
error	O
of	O
a	O
(	O
s	O
)	O
is	O
compared	O
.	O
note	O
that	O
that	O
nonuniform	O
learnability	O
is	O
a	O
relaxation	O
of	O
agnostic	O
pac	O
learn-	O
ability	O
.	O
that	O
is	O
,	O
if	O
a	O
class	O
is	O
agnostic	O
pac	O
learnable	O
then	O
it	O
is	O
also	O
nonuniformly	O
learnable	O
.	O
7.1.1	O
characterizing	O
nonuniform	O
learnability	O
our	O
goal	O
now	O
is	O
to	O
characterize	O
nonuniform	O
learnability	O
.	O
in	O
the	O
previous	O
chapter	O
we	O
have	O
found	O
a	O
crisp	O
characterization	O
of	O
pac	O
learnable	O
classes	O
,	O
by	O
showing	O
that	O
a	O
class	O
of	O
binary	O
classiﬁers	O
is	O
agnostic	O
pac	O
learnable	O
if	O
and	O
only	O
if	O
its	O
vc-	O
dimension	B
is	O
ﬁnite	O
.	O
in	O
the	O
following	O
theorem	O
we	O
ﬁnd	O
a	O
diﬀerent	O
characterization	O
for	O
nonuniform	O
learnable	O
classes	O
for	O
the	O
task	O
of	O
binary	O
classiﬁcation	O
.	O
theorem	O
7.2	O
a	O
hypothesis	B
class	I
h	O
of	O
binary	O
classiﬁers	O
is	O
nonuniformly	O
learn-	O
able	O
if	O
and	O
only	O
if	O
it	O
is	O
a	O
countable	O
union	O
of	O
agnostic	O
pac	O
learnable	O
hypothesis	B
classes	O
.	O
the	O
proof	O
of	O
theorem	O
7.2	O
relies	O
on	O
the	O
following	O
result	O
of	O
independent	O
interest	O
:	O
7.2	O
structural	O
risk	B
minimization	O
85	O
union	O
of	O
hypothesis	B
classes	O
,	O
h	O
=	O
(	O
cid:83	O
)	O
theorem	O
7.3	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
that	O
can	O
be	O
written	O
as	O
a	O
countable	O
n∈n	O
hn	O
,	O
where	O
each	O
hn	O
enjoys	O
the	O
uniform	B
convergence	I
property	O
.	O
then	O
,	O
h	O
is	O
nonuniformly	O
learnable	O
.	O
recall	B
that	O
in	O
chapter	O
4	O
we	O
have	O
shown	O
that	O
uniform	B
convergence	I
is	O
suﬃcient	O
for	O
agnostic	O
pac	O
learnability	O
.	O
theorem	O
7.3	O
generalizes	O
this	O
result	O
to	O
nonuni-	O
form	O
learnability	O
.	O
the	O
proof	O
of	O
this	O
theorem	O
will	O
be	O
given	O
in	O
the	O
next	O
section	O
by	O
introducing	O
a	O
new	O
learning	O
paradigm	O
.	O
we	O
now	O
turn	O
to	O
proving	O
theorem	O
7.2.	O
proof	O
of	O
theorem	O
7.2	O
first	O
assume	O
that	O
h	O
=	O
(	O
cid:83	O
)	O
n∈n	O
hn	O
where	O
each	O
hn	O
is	O
ag-	O
nostic	O
pac	O
learnable	O
.	O
using	O
the	O
fundamental	O
theorem	O
of	O
statistical	O
learning	O
,	O
it	O
follows	O
that	O
each	O
hn	O
has	O
the	O
uniform	B
convergence	I
property	O
.	O
therefore	O
,	O
using	O
theorem	O
7.3	O
we	O
obtain	O
that	O
h	O
is	O
nonuniform	O
learnable	O
.	O
for	O
the	O
other	O
direction	O
,	O
assume	O
that	O
h	O
is	O
nonuniform	O
learnable	O
using	O
some	O
algorithm	O
a.	O
for	O
every	O
n	O
∈	O
n	O
,	O
let	O
hn	O
=	O
{	O
h	O
∈	O
h	O
:	O
mnulh	O
(	O
1/8	O
,	O
1/7	O
,	O
h	O
)	O
≤	O
n	O
}	O
.	O
clearly	O
,	O
h	O
=	O
∪n∈nhn	O
.	O
in	O
addition	O
,	O
using	O
the	O
deﬁnition	O
of	O
mnulh	O
we	O
know	O
that	O
for	O
any	O
distribution	O
d	O
that	O
satisﬁes	O
the	O
realizability	B
assumption	O
with	O
respect	O
to	O
hn	O
,	O
with	O
probability	O
of	O
at	O
least	O
6/7	O
over	O
s	O
∼	O
dn	O
we	O
have	O
that	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
1/8	O
.	O
using	O
the	O
fundamental	O
theorem	O
of	O
statistical	O
learning	O
,	O
this	O
implies	O
that	O
the	O
vc-	O
dimension	B
of	O
hn	O
must	O
be	O
ﬁnite	O
,	O
and	O
therefore	O
hn	O
is	O
agnostic	O
pac	O
learnable	O
.	O
the	O
following	O
example	O
shows	O
that	O
nonuniform	O
learnability	O
is	O
a	O
strict	O
relax-	O
ation	O
of	O
agnostic	O
pac	O
learnability	O
;	O
namely	O
,	O
there	O
are	O
hypothesis	B
classes	O
that	O
are	O
nonuniform	O
learnable	O
but	O
are	O
not	O
agnostic	O
pac	O
learnable	O
.	O
where	O
p	O
:	O
r	O
→	O
r	O
is	O
a	O
polynomial	O
of	O
degree	O
n.	O
let	O
h	O
=	O
(	O
cid:83	O
)	O
example	O
7.1	O
consider	O
a	O
binary	O
classiﬁcation	O
problem	O
with	O
the	O
instance	B
domain	O
being	O
x	O
=	O
r.	O
for	O
every	O
n	O
∈	O
n	O
let	O
hn	O
be	O
the	O
class	O
of	O
polynomial	O
classiﬁers	O
of	O
degree	O
n	O
;	O
namely	O
,	O
hn	O
is	O
the	O
set	B
of	O
all	O
classiﬁers	O
of	O
the	O
form	O
h	O
(	O
x	O
)	O
=	O
sign	O
(	O
p	O
(	O
x	O
)	O
)	O
n∈n	O
hn	O
.	O
therefore	O
,	O
h	O
is	O
the	O
class	O
of	O
all	O
polynomial	O
classiﬁers	O
over	O
r.	O
it	O
is	O
easy	O
to	O
verify	O
that	O
vcdim	O
(	O
h	O
)	O
=	O
∞	O
while	O
vcdim	O
(	O
hn	O
)	O
=	O
n	O
+	O
1	O
(	O
see	O
exercise	O
12	O
)	O
.	O
hence	O
,	O
h	O
is	O
not	O
pac	O
learnable	O
,	O
while	O
on	O
the	O
basis	O
of	O
theorem	O
7.3	O
,	O
h	O
is	O
nonuniformly	O
learnable	O
.	O
7.2	O
structural	O
risk	B
minimization	O
we	O
do	O
so	O
by	O
ﬁrst	O
assuming	O
that	O
h	O
can	O
be	O
written	O
as	O
h	O
=	O
(	O
cid:83	O
)	O
so	O
far	O
,	O
we	O
have	O
encoded	O
our	O
prior	B
knowledge	I
by	O
specifying	O
a	O
hypothesis	B
class	I
h	O
,	O
which	O
we	O
believe	O
includes	O
a	O
good	O
predictor	B
for	O
the	O
learning	O
task	O
at	O
hand	O
.	O
yet	O
another	O
way	O
to	O
express	O
our	O
prior	B
knowledge	I
is	O
by	O
specifying	O
preferences	O
over	O
hypotheses	O
within	O
h.	O
in	O
the	O
structural	O
risk	B
minimization	O
(	O
srm	O
)	O
paradigm	O
,	O
n∈n	O
hn	O
and	O
then	O
specifying	O
a	O
weight	O
function	B
,	O
w	O
:	O
n	O
→	O
[	O
0	O
,	O
1	O
]	O
,	O
which	O
assigns	O
a	O
weight	O
to	O
each	O
hypothesis	B
class	I
,	O
hn	O
,	O
such	O
that	O
a	O
higher	O
weight	O
reﬂects	O
a	O
stronger	O
preference	O
for	O
the	O
hypothesis	B
class	I
.	O
in	O
this	O
section	O
we	O
discuss	O
how	O
to	O
learn	O
with	O
such	O
prior	B
knowledge	I
.	O
in	O
the	O
next	O
section	O
we	O
describe	O
a	O
couple	O
of	O
important	O
weighting	O
schemes	O
,	O
including	O
minimum	O
description	O
length	O
.	O
86	O
nonuniform	O
learnability	O
concretely	O
,	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
that	O
can	O
be	O
written	O
as	O
h	O
=	O
(	O
cid:83	O
)	O
n∈n	O
hn	O
.	O
for	O
example	O
,	O
h	O
may	O
be	O
the	O
class	O
of	O
all	O
polynomial	O
classiﬁers	O
where	O
each	O
hn	O
is	O
the	O
class	O
of	O
polynomial	O
classiﬁers	O
of	O
degree	O
n	O
(	O
see	O
example	O
7.1	O
)	O
.	O
assume	O
that	O
for	O
each	O
n	O
,	O
the	O
class	O
hn	O
enjoys	O
the	O
uniform	B
convergence	I
property	O
(	O
see	O
deﬁnition	O
4.3	O
(	O
	O
,	O
δ	O
)	O
.	O
let	O
us	O
also	O
deﬁne	O
in	O
chapter	O
4	O
)	O
with	O
a	O
sample	B
complexity	I
function	O
muchn	O
the	O
function	B
n	O
:	O
n	O
×	O
(	O
0	O
,	O
1	O
)	O
→	O
(	O
0	O
,	O
1	O
)	O
by	O
n	O
(	O
m	O
,	O
δ	O
)	O
=	O
min	O
{	O
	O
∈	O
(	O
0	O
,	O
1	O
)	O
:	O
muchn	O
(	O
	O
,	O
δ	O
)	O
≤	O
m	O
}	O
.	O
(	O
7.1	O
)	O
in	O
words	O
,	O
we	O
have	O
a	O
ﬁxed	O
sample	O
size	O
m	O
,	O
and	O
we	O
are	O
interested	O
in	O
the	O
lowest	O
possible	O
upper	O
bound	O
on	O
the	O
gap	O
between	O
empirical	O
and	O
true	O
risks	O
achievable	O
by	O
using	O
a	O
sample	O
of	O
m	O
examples	O
.	O
from	O
the	O
deﬁnitions	O
of	O
uniform	B
convergence	I
and	O
n	O
,	O
it	O
follows	O
that	O
for	O
every	O
m	O
and	O
δ	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
the	O
choice	O
of	O
s	O
∼	O
dm	O
we	O
have	O
that	O
∀h	O
∈	O
hn	O
,	O
|ld	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
|	O
≤	O
n	O
(	O
m	O
,	O
δ	O
)	O
.	O
let	O
w	O
:	O
n	O
→	O
[	O
0	O
,	O
1	O
]	O
be	O
a	O
function	B
such	O
that	O
(	O
cid:80	O
)	O
∞	O
(	O
7.2	O
)	O
n=1	O
w	O
(	O
n	O
)	O
≤	O
1.	O
we	O
refer	O
to	O
w	O
as	O
a	O
weight	O
function	B
over	O
the	O
hypothesis	B
classes	O
h1	O
,	O
h2	O
,	O
.	O
.	O
..	O
such	O
a	O
weight	O
function	B
can	O
reﬂect	O
the	O
importance	O
that	O
the	O
learner	O
attributes	O
to	O
each	O
hypothesis	B
class	I
,	O
or	O
some	O
measure	O
of	O
the	O
complexity	O
of	O
diﬀerent	O
hypothesis	B
classes	O
.	O
if	O
h	O
is	O
a	O
ﬁnite	O
union	O
of	O
n	O
hypothesis	B
classes	O
,	O
one	O
can	O
simply	O
assign	O
the	O
same	O
weight	O
of	O
1/n	O
to	O
all	O
hypothesis	B
classes	O
.	O
this	O
equal	O
weighting	O
corresponds	O
to	O
no	O
a	O
priori	O
preference	O
to	O
any	O
hypothesis	B
class	I
.	O
of	O
course	O
,	O
if	O
one	O
believes	O
(	O
as	O
prior	B
knowledge	I
)	O
that	O
a	O
certain	O
hypothesis	B
class	I
is	O
more	O
likely	O
to	O
contain	O
the	O
correct	O
target	O
function	O
,	O
then	O
it	O
should	O
be	O
assigned	O
a	O
larger	O
weight	O
,	O
reﬂecting	O
this	O
prior	B
knowledge	I
.	O
when	O
h	O
is	O
a	O
(	O
countable	O
)	O
inﬁnite	O
union	O
of	O
hypothesis	B
classes	O
,	O
a	O
uniform	O
weighting	O
is	O
not	O
possible	O
but	O
many	O
other	O
weighting	O
schemes	O
may	O
work	O
.	O
for	O
example	O
,	O
one	O
can	O
π2n2	O
or	O
w	O
(	O
n	O
)	O
=	O
2−n	O
.	O
later	O
in	O
this	O
chapter	O
we	O
will	O
provide	O
another	O
choose	O
w	O
(	O
n	O
)	O
=	O
6	O
convenient	O
way	O
to	O
deﬁne	O
weighting	O
functions	O
using	O
description	O
languages	O
.	O
the	O
srm	O
rule	O
follows	O
a	O
“	O
bound	O
minimization	O
”	O
approach	O
.	O
this	O
means	O
that	O
the	O
goal	O
of	O
the	O
paradigm	O
is	O
to	O
ﬁnd	O
a	O
hypothesis	B
that	O
minimizes	O
a	O
certain	O
upper	O
bound	O
on	O
the	O
true	O
risk	O
.	O
the	O
bound	O
that	O
the	O
srm	O
rule	O
wishes	O
to	O
minimize	O
is	O
given	O
in	O
the	O
following	O
theorem	O
.	O
theorem	O
7.4	O
let	O
w	O
:	O
n	O
→	O
[	O
0	O
,	O
1	O
]	O
be	O
a	O
function	B
such	O
that	O
(	O
cid:80	O
)	O
∞	O
h	O
be	O
a	O
hypothesis	B
class	I
that	O
can	O
be	O
written	O
as	O
h	O
=	O
(	O
cid:83	O
)	O
n=1	O
w	O
(	O
n	O
)	O
≤	O
1.	O
let	O
n∈n	O
hn	O
,	O
where	O
for	O
each	O
n	O
,	O
hn	O
satisﬁes	O
the	O
uniform	B
convergence	I
property	O
with	O
a	O
sample	B
complexity	I
function	O
.	O
let	O
n	O
be	O
as	O
deﬁned	O
in	O
equation	O
(	O
7.1	O
)	O
.	O
then	O
,	O
for	O
every	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
and	O
muchn	O
distribution	O
d	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
the	O
choice	O
of	O
s	O
∼	O
dm	O
,	O
the	O
following	O
bound	O
holds	O
(	O
simultaneously	O
)	O
for	O
every	O
n	O
∈	O
n	O
and	O
h	O
∈	O
hn	O
.	O
|ld	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
|	O
≤	O
n	O
(	O
m	O
,	O
w	O
(	O
n	O
)	O
·	O
δ	O
)	O
.	O
therefore	O
,	O
for	O
every	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
and	O
distribution	O
d	O
,	O
with	O
probability	O
of	O
at	O
least	O
7.2	O
structural	O
risk	B
minimization	O
87	O
1	O
−	O
δ	O
it	O
holds	O
that	O
∀h	O
∈	O
h	O
,	O
ld	O
(	O
h	O
)	O
≤	O
ls	O
(	O
h	O
)	O
+	O
min	O
n	O
:	O
h∈hn	O
n	O
(	O
m	O
,	O
w	O
(	O
n	O
)	O
·	O
δ	O
)	O
.	O
(	O
7.3	O
)	O
proof	O
for	O
each	O
n	O
deﬁne	O
δn	O
=	O
w	O
(	O
n	O
)	O
δ.	O
applying	O
the	O
assumption	O
that	O
uniform	B
convergence	I
holds	O
for	O
all	O
n	O
with	O
the	O
rate	O
given	O
in	O
equation	O
(	O
7.2	O
)	O
,	O
we	O
obtain	O
that	O
if	O
we	O
ﬁx	O
n	O
in	O
advance	O
,	O
then	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δn	O
over	O
the	O
choice	O
of	O
s	O
∼	O
dm	O
,	O
|ld	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
|	O
≤	O
n	O
(	O
m	O
,	O
δn	O
)	O
.	O
at	O
least	O
1	O
−	O
(	O
cid:80	O
)	O
∀h	O
∈	O
hn	O
,	O
n	O
δn	O
=	O
1	O
−	O
δ	O
(	O
cid:80	O
)	O
applying	O
the	O
union	B
bound	I
over	O
n	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
we	O
obtain	O
that	O
with	O
probability	O
of	O
n	O
w	O
(	O
n	O
)	O
≥	O
1	O
−	O
δ	O
,	O
the	O
preceding	O
holds	O
for	O
all	O
n	O
,	O
which	O
concludes	O
our	O
proof	O
.	O
denote	O
n	O
(	O
h	O
)	O
=	O
min	O
{	O
n	O
:	O
h	O
∈	O
hn	O
}	O
,	O
(	O
7.4	O
)	O
and	O
then	O
equation	O
(	O
7.3	O
)	O
implies	O
that	O
ld	O
(	O
h	O
)	O
≤	O
ls	O
(	O
h	O
)	O
+	O
n	O
(	O
h	O
)	O
(	O
m	O
,	O
w	O
(	O
n	O
(	O
h	O
)	O
)	O
·	O
δ	O
)	O
.	O
the	O
srm	O
paradigm	O
searches	O
for	O
h	O
that	O
minimizes	O
this	O
bound	O
,	O
as	O
formalized	O
in	O
the	O
following	O
pseudocode	O
:	O
prior	B
knowledge	I
:	O
structural	O
risk	B
minimization	O
(	O
srm	O
)	O
n	O
hn	O
where	O
hn	O
has	O
uniform	B
convergence	I
with	O
muchn	O
h	O
=	O
(	O
cid:83	O
)	O
w	O
:	O
n	O
→	O
[	O
0	O
,	O
1	O
]	O
where	O
(	O
cid:80	O
)	O
output	O
:	O
h	O
∈	O
argminh∈h	O
(	O
cid:2	O
)	O
ls	O
(	O
h	O
)	O
+	O
n	O
(	O
h	O
)	O
(	O
m	O
,	O
w	O
(	O
n	O
(	O
h	O
)	O
)	O
·	O
δ	O
)	O
(	O
cid:3	O
)	O
deﬁne	O
:	O
n	O
as	O
in	O
equation	O
(	O
7.1	O
)	O
;	O
n	O
(	O
h	O
)	O
as	O
in	O
equation	O
(	O
7.4	O
)	O
input	O
:	O
training	B
set	I
s	O
∼	O
dm	O
,	O
conﬁdence	B
δ	O
n	O
w	O
(	O
n	O
)	O
≤	O
1	O
unlike	O
the	O
erm	O
paradigm	O
discussed	O
in	O
previous	O
chapters	O
,	O
we	O
no	O
longer	O
just	O
care	O
about	O
the	O
empirical	B
risk	I
,	O
ls	O
(	O
h	O
)	O
,	O
but	O
we	O
are	O
willing	O
to	O
trade	O
some	O
of	O
our	O
bias	B
toward	O
low	O
empirical	B
risk	I
with	O
a	O
bias	B
toward	O
classes	O
for	O
which	O
n	O
(	O
h	O
)	O
(	O
m	O
,	O
w	O
(	O
n	O
(	O
h	O
)	O
)	O
·δ	O
)	O
is	O
smaller	O
,	O
for	O
the	O
sake	O
of	O
a	O
smaller	O
estimation	B
error	I
.	O
next	O
we	O
show	O
that	O
the	O
srm	O
paradigm	O
can	O
be	O
used	O
for	O
nonuniform	O
learning	O
of	O
every	O
class	O
,	O
which	O
is	O
a	O
countable	O
union	O
of	O
uniformly	O
converging	O
hypothesis	B
classes	O
.	O
theorem	O
7.5	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
such	O
that	O
h	O
=	O
(	O
cid:83	O
)	O
each	O
hn	O
has	O
the	O
uniform	B
convergence	I
property	O
with	O
sample	B
complexity	I
muchn	O
w	O
:	O
n	O
→	O
[	O
0	O
,	O
1	O
]	O
be	O
such	O
that	O
w	O
(	O
n	O
)	O
=	O
6	O
using	O
the	O
srm	O
rule	O
with	O
rate	O
n∈n	O
hn	O
,	O
where	O
.	O
let	O
n2π2	O
.	O
then	O
,	O
h	O
is	O
nonuniformly	O
learnable	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
mnulh	O
(	O
	O
,	O
δ	O
,	O
h	O
)	O
≤	O
muchn	O
(	O
h	O
)	O
/2	O
,	O
6δ	O
(	O
πn	O
(	O
h	O
)	O
)	O
2	O
.	O
88	O
nonuniform	O
learnability	O
proof	O
let	O
a	O
be	O
the	O
srm	O
algorithm	O
with	O
respect	O
to	O
the	O
weighting	O
function	B
w.	O
(	O
	O
,	O
w	O
(	O
n	O
(	O
h	O
)	O
)	O
δ	O
)	O
.	O
using	O
the	O
fact	O
that	O
n	O
w	O
(	O
n	O
)	O
=	O
1	O
,	O
we	O
can	O
apply	O
theorem	O
7.4	O
to	O
get	O
that	O
,	O
with	O
probability	O
of	O
at	O
least	O
(	O
cid:80	O
)	O
for	O
every	O
h	O
∈	O
h	O
,	O
	O
,	O
and	O
δ	O
,	O
let	O
m	O
≥	O
muchn	O
(	O
h	O
)	O
1	O
−	O
δ	O
over	O
the	O
choice	O
of	O
s	O
∼	O
dm	O
,	O
we	O
have	O
that	O
for	O
every	O
h	O
(	O
cid:48	O
)	O
∈	O
h	O
,	O
ld	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
≤	O
ls	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
+	O
n	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
(	O
m	O
,	O
w	O
(	O
n	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
)	O
δ	O
)	O
.	O
(	O
cid:2	O
)	O
ls	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
+	O
n	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
(	O
m	O
,	O
w	O
(	O
n	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
)	O
δ	O
)	O
(	O
cid:3	O
)	O
≤	O
ls	O
(	O
h	O
)	O
+	O
n	O
(	O
h	O
)	O
(	O
m	O
,	O
w	O
(	O
n	O
(	O
h	O
)	O
)	O
δ	O
)	O
.	O
the	O
preceding	O
holds	O
in	O
particular	O
for	O
the	O
hypothesis	B
a	O
(	O
s	O
)	O
returned	O
by	O
the	O
srm	O
rule	O
.	O
by	O
the	O
deﬁnition	O
of	O
srm	O
we	O
obtain	O
that	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
min	O
h	O
(	O
cid:48	O
)	O
finally	O
,	O
if	O
m	O
≥	O
muchn	O
(	O
h	O
)	O
(	O
/2	O
,	O
w	O
(	O
n	O
(	O
h	O
)	O
)	O
δ	O
)	O
then	O
clearly	O
n	O
(	O
h	O
)	O
(	O
m	O
,	O
w	O
(	O
n	O
(	O
h	O
)	O
)	O
δ	O
)	O
≤	O
/2	O
.	O
in	O
addition	O
,	O
from	O
the	O
uniform	B
convergence	I
property	O
of	O
each	O
hn	O
we	O
have	O
that	O
with	O
probability	O
of	O
more	O
than	O
1	O
−	O
δ	O
,	O
ls	O
(	O
h	O
)	O
≤	O
ld	O
(	O
h	O
)	O
+	O
/2	O
.	O
combining	O
all	O
the	O
preceding	O
we	O
obtain	O
that	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
ld	O
(	O
h	O
)	O
+	O
	O
,	O
which	O
con-	O
cludes	O
our	O
proof	O
.	O
note	O
that	O
the	O
previous	O
theorem	O
also	O
proves	O
theorem	O
7.3.	O
remark	O
7.2	O
(	O
no-free-lunch	B
for	O
nonuniform	O
learnability	O
)	O
we	O
have	O
shown	O
that	O
any	O
countable	O
union	O
of	O
classes	O
of	O
ﬁnite	O
vc-dimension	O
is	O
nonuniformly	O
learnable	O
.	O
it	O
turns	O
out	O
that	O
,	O
for	O
any	O
inﬁnite	O
domain	B
set	O
,	O
x	O
,	O
the	O
class	O
of	O
all	O
binary	O
valued	O
functions	O
over	O
x	O
is	O
not	O
a	O
countable	O
union	O
of	O
classes	O
of	O
ﬁnite	O
vc-dimension	O
.	O
we	O
leave	O
the	O
proof	O
of	O
this	O
claim	O
as	O
a	O
(	O
nontrivial	O
)	O
exercise	O
(	O
see	O
exercise	O
5	O
)	O
.	O
it	O
follows	O
that	O
,	O
in	O
some	O
sense	O
,	O
the	O
no	O
free	O
lunch	O
theorem	O
holds	O
for	O
nonuniform	O
learning	O
as	O
well	O
:	O
namely	O
,	O
whenever	O
the	O
domain	B
is	O
not	O
ﬁnite	O
,	O
there	O
exists	O
no	O
nonuniform	O
learner	O
with	O
respect	O
to	O
the	O
class	O
of	O
all	O
deterministic	O
binary	O
classiﬁers	O
(	O
although	O
for	O
each	O
such	O
classiﬁer	B
there	O
exists	O
a	O
trivial	O
algorithm	O
that	O
learns	O
it	O
–	O
erm	O
with	O
respect	O
to	O
the	O
hypothesis	B
class	I
that	O
contains	O
only	O
this	O
classiﬁer	B
)	O
.	O
it	O
is	O
interesting	O
to	O
compare	O
the	O
nonuniform	O
learnability	O
result	O
given	O
in	O
the-	O
orem	O
7.5	O
to	O
the	O
task	O
of	O
agnostic	O
pac	O
learning	O
any	O
speciﬁc	O
hn	O
separately	O
.	O
the	O
prior	B
knowledge	I
,	O
or	O
bias	B
,	O
of	O
a	O
nonuniform	O
learner	O
for	O
h	O
is	O
weaker	O
–	O
it	O
is	O
searching	O
for	O
a	O
model	O
throughout	O
the	O
entire	O
class	O
h	O
,	O
rather	O
than	O
being	O
focused	O
on	O
one	O
spe-	O
ciﬁc	O
hn	O
.	O
the	O
cost	O
of	O
this	O
weakening	O
of	O
prior	B
knowledge	I
is	O
the	O
increase	O
in	O
sample	B
complexity	I
needed	O
to	O
compete	O
with	O
any	O
speciﬁc	O
h	O
∈	O
hn	O
.	O
for	O
a	O
concrete	O
evalua-	O
tion	O
of	O
this	O
gap	O
,	O
consider	O
the	O
task	O
of	O
binary	O
classiﬁcation	O
with	O
the	O
zero-one	O
loss	B
.	O
assume	O
that	O
for	O
all	O
n	O
,	O
vcdim	O
(	O
hn	O
)	O
=	O
n.	O
since	O
muchn	O
(	O
where	O
c	O
is	O
the	O
contant	O
appearing	O
in	O
theorem	O
6.8	O
)	O
,	O
a	O
straightforward	O
calculation	O
shows	O
that	O
(	O
	O
,	O
δ	O
)	O
=	O
c	O
n+log	O
(	O
1/δ	O
)	O
2	O
mnulh	O
(	O
	O
,	O
δ	O
,	O
h	O
)	O
−	O
muchn	O
(	O
/2	O
,	O
δ	O
)	O
≤	O
4c	O
2	O
that	O
is	O
,	O
the	O
cost	O
of	O
relaxing	O
the	O
learner	O
’	O
s	O
prior	B
knowledge	I
from	O
a	O
speciﬁc	O
hn	O
that	O
contains	O
the	O
target	O
h	O
to	O
a	O
countable	O
union	O
of	O
classes	O
depends	O
on	O
the	O
log	O
of	O
2	O
log	O
(	O
2n	O
)	O
.	O
7.3	O
minimum	O
description	O
length	O
and	O
occam	O
’	O
s	O
razor	O
89	O
the	O
index	O
of	O
the	O
ﬁrst	O
class	O
in	O
which	O
h	O
resides	O
.	O
that	O
cost	O
increases	O
with	O
the	O
index	O
of	O
the	O
class	O
,	O
which	O
can	O
be	O
interpreted	O
as	O
reﬂecting	O
the	O
value	O
of	O
knowing	O
a	O
good	O
priority	O
order	O
on	O
the	O
hypotheses	O
in	O
h.	O
7.3	O
union	O
of	O
singleton	O
classes	O
,	O
namely	O
,	O
h	O
=	O
(	O
cid:83	O
)	O
minimum	O
description	O
length	O
and	O
occam	O
’	O
s	O
razor	O
let	O
h	O
be	O
a	O
countable	O
hypothesis	B
class	I
.	O
then	O
,	O
we	O
can	O
write	O
h	O
as	O
a	O
countable	O
n∈n	O
{	O
hn	O
}	O
.	O
by	O
hoeﬀding	O
’	O
s	O
inequality	O
(	O
lemma	O
4.5	O
)	O
,	O
each	O
singleton	O
class	O
has	O
the	O
uniform	B
convergence	I
property	O
with	O
rate	O
muc	O
(	O
	O
,	O
δ	O
)	O
=	O
log	O
(	O
2/δ	O
)	O
.	O
therefore	O
,	O
the	O
function	B
n	O
given	O
in	O
equation	O
(	O
7.1	O
)	O
becomes	O
n	O
(	O
m	O
,	O
δ	O
)	O
=	O
22	O
(	O
cid:113	O
)	O
log	O
(	O
2/δ	O
)	O
(	O
cid:34	O
)	O
argmin	O
hn∈h	O
ls	O
(	O
h	O
)	O
+	O
2m	O
and	O
the	O
srm	O
rule	O
becomes	O
(	O
cid:114	O
)	O
−	O
log	O
(	O
w	O
(	O
n	O
)	O
)	O
+	O
log	O
(	O
2/δ	O
)	O
(	O
cid:35	O
)	O
(	O
cid:35	O
)	O
.	O
.	O
2m	O
2m	O
equivalently	O
,	O
we	O
can	O
think	O
of	O
w	O
as	O
a	O
function	B
from	O
h	O
to	O
[	O
0	O
,	O
1	O
]	O
,	O
and	O
then	O
the	O
srm	O
rule	O
becomes	O
(	O
cid:114	O
)	O
−	O
log	O
(	O
w	O
(	O
h	O
)	O
)	O
+	O
log	O
(	O
2/δ	O
)	O
(	O
cid:34	O
)	O
argmin	O
h∈h	O
ls	O
(	O
h	O
)	O
+	O
it	O
follows	O
that	O
in	O
this	O
case	O
,	O
the	O
prior	B
knowledge	I
is	O
solely	O
determined	O
by	O
the	O
weight	O
we	O
assign	O
to	O
each	O
hypothesis	B
.	O
we	O
assign	O
higher	O
weights	O
to	O
hypotheses	O
that	O
we	O
believe	O
are	O
more	O
likely	O
to	O
be	O
the	O
correct	O
one	O
,	O
and	O
in	O
the	O
learning	O
algorithm	O
we	O
prefer	O
hypotheses	O
that	O
have	O
higher	O
weights	O
.	O
in	O
this	O
section	O
we	O
discuss	O
a	O
particular	O
convenient	O
way	O
to	O
deﬁne	O
a	O
weight	O
func-	O
tion	O
over	O
h	O
,	O
which	O
is	O
derived	O
from	O
the	O
length	O
of	O
descriptions	O
given	O
to	O
hypotheses	O
.	O
having	O
a	O
hypothesis	B
class	I
,	O
one	O
can	O
wonder	O
about	O
how	O
we	O
describe	O
,	O
or	O
represent	O
,	O
each	O
hypothesis	B
in	O
the	O
class	O
.	O
we	O
naturally	O
ﬁx	O
some	O
description	O
language	O
.	O
this	O
can	O
be	O
english	O
,	O
or	O
a	O
programming	O
language	O
,	O
or	O
some	O
set	B
of	O
mathematical	O
formu-	O
las	O
.	O
in	O
any	O
of	O
these	O
languages	O
,	O
a	O
description	O
consists	O
of	O
ﬁnite	O
strings	O
of	O
symbols	O
(	O
or	O
characters	O
)	O
drawn	O
from	O
some	O
ﬁxed	O
alphabet	O
.	O
we	O
shall	O
now	O
formalize	O
these	O
notions	O
.	O
let	O
h	O
be	O
the	O
hypothesis	B
class	I
we	O
wish	O
to	O
describe	O
.	O
fix	O
some	O
ﬁnite	O
set	B
σ	O
of	O
symbols	O
(	O
or	O
“	O
characters	O
”	O
)	O
,	O
which	O
we	O
call	O
the	O
alphabet	O
.	O
for	O
concreteness	O
,	O
we	O
let	O
σ	O
=	O
{	O
0	O
,	O
1	O
}	O
.	O
a	O
string	O
is	O
a	O
ﬁnite	O
sequence	O
of	O
symbols	O
from	O
σ	O
;	O
for	O
example	O
,	O
σ	O
=	O
(	O
0	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
0	O
)	O
is	O
a	O
string	O
of	O
length	O
5.	O
we	O
denote	O
by	O
|σ|	O
the	O
length	O
of	O
a	O
string	O
.	O
the	O
set	B
of	O
all	O
ﬁnite	O
length	O
strings	O
is	O
denoted	O
σ∗	O
.	O
a	O
description	O
language	O
for	O
h	O
is	O
a	O
function	B
d	O
:	O
h	O
→	O
σ∗	O
,	O
mapping	O
each	O
member	O
h	O
of	O
h	O
to	O
a	O
string	O
d	O
(	O
h	O
)	O
.	O
d	O
(	O
h	O
)	O
is	O
called	O
“	O
the	O
description	O
of	O
h	O
,	O
”	O
and	O
its	O
length	O
is	O
denoted	O
by	O
|h|	O
.	O
we	O
shall	O
require	O
that	O
description	O
languages	O
be	O
preﬁx-free	O
;	O
namely	O
,	O
for	O
every	O
distinct	O
h	O
,	O
h	O
(	O
cid:48	O
)	O
,	O
d	O
(	O
h	O
)	O
is	O
not	O
a	O
preﬁx	O
of	O
d	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
.	O
that	O
is	O
,	O
we	O
do	O
not	O
allow	O
that	O
any	O
string	O
d	O
(	O
h	O
)	O
is	O
exactly	O
the	O
ﬁrst	O
|h|	O
symbols	O
of	O
any	O
longer	O
string	O
d	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
.	O
preﬁx-free	O
collections	O
of	O
strings	O
enjoy	O
the	O
following	O
combinatorial	O
property	O
:	O
90	O
nonuniform	O
learnability	O
lemma	O
7.6	O
(	O
kraft	O
inequality	O
)	O
if	O
s	O
⊆	O
{	O
0	O
,	O
1	O
}	O
∗	O
is	O
a	O
preﬁx-free	O
set	B
of	O
strings	O
,	O
then	O
(	O
cid:88	O
)	O
σ∈s	O
1	O
2|σ|	O
≤	O
1.	O
proof	O
deﬁne	O
a	O
probability	O
distribution	O
over	O
the	O
members	O
of	O
s	O
as	O
follows	O
:	O
re-	O
peatedly	O
toss	O
an	O
unbiased	O
coin	O
,	O
with	O
faces	O
labeled	O
0	O
and	O
1	O
,	O
until	O
the	O
sequence	O
of	O
outcomes	O
is	O
a	O
member	O
of	O
s	O
;	O
at	O
that	O
point	O
,	O
stop	O
.	O
for	O
each	O
σ	O
∈	O
s	O
,	O
let	O
p	O
(	O
σ	O
)	O
be	O
the	O
probability	O
that	O
this	O
process	O
generates	O
the	O
string	O
σ.	O
note	O
that	O
since	O
s	O
is	O
preﬁx-free	O
,	O
for	O
every	O
σ	O
∈	O
s	O
,	O
if	O
the	O
coin	O
toss	O
outcomes	O
follow	O
the	O
bits	O
of	O
σ	O
then	O
we	O
will	O
stop	O
only	O
once	O
the	O
sequence	O
of	O
outcomes	O
equals	O
σ.	O
we	O
therefore	O
get	O
that	O
,	O
for	O
every	O
σ	O
∈	O
s	O
,	O
p	O
(	O
σ	O
)	O
=	O
1	O
2|σ|	O
.	O
since	O
probabilities	O
add	O
up	O
to	O
at	O
most	O
1	O
,	O
our	O
proof	O
is	O
concluded	O
.	O
in	O
light	O
of	O
kraft	O
’	O
s	O
inequality	O
,	O
any	O
preﬁx-free	O
description	O
language	O
of	O
a	O
hypoth-	O
esis	O
class	O
,	O
h	O
,	O
gives	O
rise	O
to	O
a	O
weighting	O
function	B
w	O
over	O
that	O
hypothesis	B
class	I
–	O
we	O
will	O
simply	O
set	B
w	O
(	O
h	O
)	O
=	O
1	O
2|h|	O
.	O
this	O
observation	O
immediately	O
yields	O
the	O
following	O
:	O
theorem	O
7.7	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
and	O
let	O
d	O
:	O
h	O
→	O
{	O
0	O
,	O
1	O
}	O
∗	O
be	O
a	O
preﬁx-	O
free	O
description	O
language	O
for	O
h.	O
then	O
,	O
for	O
every	O
sample	O
size	O
,	O
m	O
,	O
every	O
conﬁdence	B
parameter	O
,	O
δ	O
>	O
0	O
,	O
and	O
every	O
probability	O
distribution	O
,	O
d	O
,	O
with	O
probability	O
greater	O
than	O
1	O
−	O
δ	O
over	O
the	O
choice	O
of	O
s	O
∼	O
dm	O
we	O
have	O
that	O
,	O
(	O
cid:114	O
)	O
|h|	O
+	O
ln	O
(	O
2/δ	O
)	O
,	O
2m	O
∀h	O
∈	O
h	O
,	O
ld	O
(	O
h	O
)	O
≤	O
ls	O
(	O
h	O
)	O
+	O
where	O
|h|	O
is	O
the	O
length	O
of	O
d	O
(	O
h	O
)	O
.	O
proof	O
choose	O
w	O
(	O
h	O
)	O
=	O
1/2|h|	O
,	O
apply	O
theorem	O
7.4	O
with	O
n	O
(	O
m	O
,	O
δ	O
)	O
=	O
note	O
that	O
ln	O
(	O
2|h|	O
)	O
=	O
|h|	O
ln	O
(	O
2	O
)	O
<	O
|h|	O
.	O
(	O
cid:113	O
)	O
ln	O
(	O
2/δ	O
)	O
2m	O
,	O
and	O
(	O
cid:113	O
)	O
|h|+ln	O
(	O
2/δ	O
)	O
as	O
was	O
the	O
case	O
with	O
theorem	O
7.4	O
,	O
this	O
result	O
suggests	O
a	O
learning	O
paradigm	O
for	O
h	O
–	O
given	O
a	O
training	B
set	I
,	O
s	O
,	O
search	O
for	O
a	O
hypothesis	B
h	O
∈	O
h	O
that	O
minimizes	O
the	O
bound	O
,	O
ls	O
(	O
h	O
)	O
+	O
.	O
in	O
particular	O
,	O
it	O
suggests	O
trading	O
oﬀ	O
empirical	B
risk	I
for	O
saving	O
description	O
length	O
.	O
this	O
yields	O
the	O
minimum	O
description	O
length	O
learning	O
paradigm	O
.	O
2m	O
minimum	O
description	O
length	O
(	O
mdl	O
)	O
prior	B
knowledge	I
:	O
h	O
is	O
a	O
countable	O
hypothesis	B
class	I
h	O
is	O
described	O
by	O
a	O
preﬁx-free	O
language	O
over	O
{	O
0	O
,	O
1	O
}	O
for	O
every	O
h	O
∈	O
h	O
,	O
|h|	O
is	O
the	O
length	O
of	O
the	O
representation	O
of	O
h	O
input	O
:	O
a	O
training	B
set	I
s	O
∼	O
dm	O
,	O
conﬁdence	B
δ	O
output	O
:	O
h	O
∈	O
argminh∈h	O
(	O
cid:113	O
)	O
|h|+ln	O
(	O
2/δ	O
)	O
ls	O
(	O
h	O
)	O
+	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
2m	O
example	O
7.3	O
let	O
h	O
be	O
the	O
class	O
of	O
all	O
predictors	O
that	O
can	O
be	O
implemented	O
using	O
some	O
programming	O
language	O
,	O
say	O
,	O
c++	O
.	O
let	O
us	O
represent	O
each	O
program	O
using	O
the	O
7.3	O
minimum	O
description	O
length	O
and	O
occam	O
’	O
s	O
razor	O
91	O
binary	O
string	O
obtained	O
by	O
running	O
the	O
gzip	O
command	O
on	O
the	O
program	O
(	O
this	O
yields	O
a	O
preﬁx-free	O
description	O
language	O
over	O
the	O
alphabet	O
{	O
0	O
,	O
1	O
}	O
)	O
.	O
then	O
,	O
|h|	O
is	O
simply	O
the	O
length	O
(	O
in	O
bits	O
)	O
of	O
the	O
output	O
of	O
gzip	O
when	O
running	O
on	O
the	O
c++	O
program	O
corresponding	O
to	O
h.	O
7.3.1	O
occam	O
’	O
s	O
razor	O
theorem	O
7.7	O
suggests	O
that	O
,	O
having	O
two	O
hypotheses	O
sharing	O
the	O
same	O
empirical	B
risk	I
,	O
the	O
true	O
risk	O
of	O
the	O
one	O
that	O
has	O
shorter	O
description	O
can	O
be	O
bounded	O
by	O
a	O
lower	O
value	O
.	O
thus	O
,	O
this	O
result	O
can	O
be	O
viewed	O
as	O
conveying	O
a	O
philosophical	O
message	O
:	O
a	O
short	O
explanation	O
(	O
that	O
is	O
,	O
a	O
hypothesis	B
that	O
has	O
a	O
short	O
length	O
)	O
tends	O
to	O
be	O
more	O
valid	O
than	O
a	O
long	O
explanation	O
.	O
this	O
is	O
a	O
well	O
known	O
principle	O
,	O
called	O
occam	O
’	O
s	O
razor	O
,	O
after	O
william	O
of	O
ockham	O
,	O
a	O
14th-century	O
english	O
logician	O
,	O
who	O
is	O
believed	O
to	O
have	O
been	O
the	O
ﬁrst	O
to	O
phrase	O
it	O
explicitly	O
.	O
here	O
,	O
we	O
provide	O
one	O
possible	O
justiﬁcation	O
to	O
this	O
principle	O
.	O
the	O
inequality	O
of	O
theorem	O
7.7	O
shows	O
that	O
the	O
more	O
complex	O
a	O
hypothesis	B
h	O
is	O
(	O
in	O
the	O
sense	O
of	O
having	O
a	O
longer	O
description	O
)	O
,	O
the	O
larger	O
the	O
sample	O
size	O
it	O
has	O
to	O
ﬁt	O
to	O
guarantee	O
that	O
it	O
has	O
a	O
small	O
true	O
risk	O
,	O
ld	O
(	O
h	O
)	O
.	O
at	O
a	O
second	O
glance	O
,	O
our	O
occam	O
razor	O
claim	O
might	O
seem	O
somewhat	O
problematic	O
.	O
in	O
the	O
context	O
in	O
which	O
the	O
occam	O
razor	O
principle	O
is	O
usually	O
invoked	O
in	O
science	O
,	O
the	O
language	O
according	O
to	O
which	O
complexity	O
is	O
measured	O
is	O
a	O
natural	O
language	O
,	O
whereas	O
here	O
we	O
may	O
consider	O
any	O
arbitrary	O
abstract	O
description	O
language	O
.	O
as-	O
sume	O
that	O
we	O
have	O
two	O
hypotheses	O
such	O
that	O
|h	O
(	O
cid:48	O
)	O
|	O
is	O
much	O
smaller	O
than	O
|h|	O
.	O
by	O
the	O
preceding	O
result	O
,	O
if	O
both	O
have	O
the	O
same	O
error	O
on	O
a	O
given	O
training	B
set	I
,	O
s	O
,	O
then	O
the	O
true	B
error	I
of	O
h	O
may	O
be	O
much	O
higher	O
than	O
the	O
true	B
error	I
of	O
h	O
(	O
cid:48	O
)	O
,	O
so	O
one	O
should	O
prefer	O
h	O
(	O
cid:48	O
)	O
over	O
h.	O
however	O
,	O
we	O
could	O
have	O
chosen	O
a	O
diﬀerent	O
description	O
language	O
,	O
say	O
,	O
one	O
that	O
assigns	O
a	O
string	O
of	O
length	O
3	O
to	O
h	O
and	O
a	O
string	O
of	O
length	O
100000	O
to	O
h	O
(	O
cid:48	O
)	O
.	O
suddenly	O
it	O
looks	O
as	O
if	O
one	O
should	O
prefer	O
h	O
over	O
h	O
(	O
cid:48	O
)	O
.	O
but	O
these	O
are	O
the	O
same	O
h	O
and	O
h	O
(	O
cid:48	O
)	O
for	O
which	O
we	O
argued	O
two	O
sentences	O
ago	O
that	O
h	O
(	O
cid:48	O
)	O
should	O
be	O
preferable	O
.	O
where	O
is	O
the	O
catch	O
here	O
?	O
(	O
cid:113	O
)	O
ln	O
(	O
2/δ	O
)	O
indeed	O
,	O
there	O
is	O
no	O
inherent	O
generalizability	O
diﬀerence	O
between	O
hypotheses	O
.	O
the	O
crucial	O
aspect	O
here	O
is	O
the	O
dependency	O
order	O
between	O
the	O
initial	O
choice	O
of	O
language	O
(	O
or	O
,	O
preference	O
over	O
hypotheses	O
)	O
and	O
the	O
training	B
set	I
.	O
as	O
we	O
know	O
from	O
the	O
basic	O
hoeﬀding	O
’	O
s	O
bound	O
(	O
equation	O
(	O
4.2	O
)	O
)	O
,	O
if	O
we	O
commit	O
to	O
any	O
hypothesis	B
be-	O
fore	O
seeing	O
the	O
data	O
,	O
then	O
we	O
are	O
guaranteed	O
a	O
rather	O
small	O
estimation	B
error	I
term	O
ld	O
(	O
h	O
)	O
≤	O
ls	O
(	O
h	O
)	O
+	O
2m	O
.	O
choosing	O
a	O
description	O
language	O
(	O
or	O
,	O
equivalently	O
,	O
some	O
weighting	O
of	O
hypotheses	O
)	O
is	O
a	O
weak	O
form	O
of	O
committing	O
to	O
a	O
hypothesis	B
.	O
rather	O
than	O
committing	O
to	O
a	O
single	O
hypothesis	O
,	O
we	O
spread	O
out	O
our	O
commitment	O
among	O
many	O
.	O
as	O
long	O
as	O
it	O
is	O
done	O
independently	O
of	O
the	O
training	O
sample	O
,	O
our	O
gen-	O
eralization	O
bound	O
holds	O
.	O
just	O
as	O
the	O
choice	O
of	O
a	O
single	O
hypothesis	O
to	O
be	O
evaluated	O
by	O
a	O
sample	O
can	O
be	O
arbitrary	O
,	O
so	O
is	O
the	O
choice	O
of	O
description	O
language	O
.	O
92	O
nonuniform	O
learnability	O
7.4	O
other	O
notions	O
of	O
learnability	O
–	O
consistency	B
the	O
notion	O
of	O
learnability	O
can	O
be	O
further	O
relaxed	O
by	O
allowing	O
the	O
needed	O
sample	O
sizes	O
to	O
depend	O
not	O
only	O
on	O
	O
,	O
δ	O
,	O
and	O
h	O
but	O
also	O
on	O
the	O
underlying	O
data-generating	O
probability	O
distribution	O
d	O
(	O
that	O
is	O
used	O
to	O
generate	O
the	O
training	O
sample	O
and	O
to	O
determine	O
the	O
risk	B
)	O
.	O
this	O
type	O
of	O
performance	O
guarantee	O
is	O
captured	O
by	O
the	O
notion	O
of	O
consistency	B
1	O
of	O
a	O
learning	O
rule	O
.	O
definition	O
7.8	O
(	O
consistency	B
)	O
let	O
z	O
be	O
a	O
domain	B
set	O
,	O
let	O
p	O
be	O
a	O
set	B
of	O
probability	O
distributions	O
over	O
z	O
,	O
and	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
.	O
a	O
learn-	O
ing	O
rule	O
a	O
is	O
consistent	O
with	O
respect	O
to	O
h	O
and	O
p	O
if	O
there	O
exists	O
a	O
function	B
mconh	O
:	O
(	O
0	O
,	O
1	O
)	O
2	O
×	O
h	O
×	O
p	O
→	O
n	O
such	O
that	O
,	O
for	O
every	O
	O
,	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
every	O
h	O
∈	O
h	O
,	O
and	O
every	O
d	O
∈	O
p	O
,	O
if	O
m	O
≥	O
mnulh	O
(	O
	O
,	O
δ	O
,	O
h	O
,	O
d	O
)	O
then	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
the	O
choice	O
of	O
s	O
∼	O
dm	O
it	O
holds	O
that	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
ld	O
(	O
h	O
)	O
+	O
	O
.	O
if	O
p	O
is	O
the	O
set	B
of	O
all	O
distributions,2	O
we	O
say	O
that	O
a	O
is	O
universally	O
consistent	O
with	O
respect	O
to	O
h.	O
the	O
notion	O
of	O
consistency	B
is	O
,	O
of	O
course	O
,	O
a	O
relaxation	O
of	O
our	O
previous	O
notion	O
of	O
nonuniform	O
learnability	O
.	O
clearly	O
if	O
an	O
algorithm	O
nonuniformly	O
learns	O
a	O
class	O
h	O
it	O
is	O
also	O
universally	O
consistent	O
for	O
that	O
class	O
.	O
the	O
relaxation	O
is	O
strict	O
in	O
the	O
sense	O
that	O
there	O
are	O
consistent	O
learning	O
rules	O
that	O
are	O
not	O
successful	O
nonuniform	O
learners	O
.	O
for	O
example	O
,	O
the	O
algorithm	O
memorize	O
deﬁned	O
in	O
example	O
7.4	O
later	O
is	O
universally	O
consistent	O
for	O
the	O
class	O
of	O
all	O
binary	O
classiﬁers	O
over	O
n.	O
however	O
,	O
as	O
we	O
have	O
argued	O
before	O
,	O
this	O
class	O
is	O
not	O
nonuniformly	O
learnable	O
.	O
example	O
7.4	O
consider	O
the	O
classiﬁcation	O
prediction	O
algorithm	O
memorize	O
deﬁned	O
as	O
follows	O
.	O
the	O
algorithm	O
memorizes	O
the	O
training	O
examples	O
,	O
and	O
,	O
given	O
a	O
test	O
point	O
x	O
,	O
it	O
predicts	O
the	O
majority	O
label	B
among	O
all	O
labeled	O
instances	O
of	O
x	O
that	O
exist	O
in	O
the	O
training	O
sample	O
(	O
and	O
some	O
ﬁxed	O
default	O
label	B
if	O
no	O
instance	B
of	O
x	O
appears	O
in	O
the	O
training	B
set	I
)	O
.	O
it	O
is	O
possible	O
to	O
show	O
(	O
see	O
exercise	O
6	O
)	O
that	O
the	O
memorize	O
algorithm	O
is	O
universally	O
consistent	O
for	O
every	O
countable	O
domain	B
x	O
and	O
a	O
ﬁnite	O
label	B
set	O
y	O
(	O
w.r.t	O
.	O
the	O
zero-one	O
loss	B
)	O
.	O
intuitively	O
,	O
it	O
is	O
not	O
obvious	O
that	O
the	O
memorize	O
algorithm	O
should	O
be	O
viewed	O
as	O
a	O
learner	O
,	O
since	O
it	O
lacks	O
the	O
aspect	O
of	O
generalization	O
,	O
namely	O
,	O
of	O
using	O
observed	O
data	O
to	O
predict	O
the	O
labels	O
of	O
unseen	O
examples	O
.	O
the	O
fact	O
that	O
memorize	O
is	O
a	O
consistent	O
algorithm	O
for	O
the	O
class	O
of	O
all	O
functions	O
over	O
any	O
countable	O
domain	B
set	O
therefore	O
raises	O
doubt	O
about	O
the	O
usefulness	O
of	O
consistency	B
guarantees	O
.	O
furthermore	O
,	O
the	O
sharp-eyed	O
reader	O
may	O
notice	O
that	O
the	O
“	O
bad	O
learner	O
”	O
we	O
introduced	O
in	O
chapter	O
2	O
,	O
1	O
in	O
the	O
literature	O
,	O
consistency	B
is	O
often	O
deﬁned	O
using	O
the	O
notion	O
of	O
either	O
convergence	O
in	O
probability	O
(	O
corresponding	O
to	O
weak	O
consistency	O
)	O
or	O
almost	O
sure	O
convergence	O
(	O
corresponding	O
to	O
strong	O
consistency	O
)	O
.	O
2	O
formally	O
,	O
we	O
assume	O
that	O
z	O
is	O
endowed	O
with	O
some	O
sigma	O
algebra	O
of	O
subsets	O
ω	O
,	O
and	O
by	O
“	O
all	O
distributions	O
”	O
we	O
mean	O
all	O
probability	O
distributions	O
that	O
have	O
ω	O
contained	O
in	O
their	O
associated	O
family	O
of	O
measurable	O
subsets	O
.	O
7.5	O
discussing	O
the	O
diﬀerent	O
notions	O
of	O
learnability	O
93	O
which	O
led	O
to	O
overﬁtting	B
,	O
is	O
in	O
fact	O
the	O
memorize	O
algorithm	O
.	O
in	O
the	O
next	O
section	O
we	O
discuss	O
the	O
signiﬁcance	O
of	O
the	O
diﬀerent	O
notions	O
of	O
learnability	O
and	O
revisit	O
the	O
no-free-lunch	B
theorem	O
in	O
light	O
of	O
the	O
diﬀerent	O
deﬁnitions	O
of	O
learnability	O
.	O
7.5	O
discussing	O
the	O
diﬀerent	O
notions	O
of	O
learnability	O
we	O
have	O
given	O
three	O
deﬁnitions	O
of	O
learnability	O
and	O
we	O
now	O
discuss	O
their	O
useful-	O
ness	O
.	O
as	O
is	O
usually	O
the	O
case	O
,	O
the	O
usefulness	O
of	O
a	O
mathematical	O
deﬁnition	O
depends	O
on	O
what	O
we	O
need	O
it	O
for	O
.	O
we	O
therefore	O
list	O
several	O
possible	O
goals	O
that	O
we	O
aim	O
to	O
achieve	O
by	O
deﬁning	O
learnability	O
and	O
discuss	O
the	O
usefulness	O
of	O
the	O
diﬀerent	O
deﬁni-	O
tions	O
in	O
light	O
of	O
these	O
goals	O
.	O
what	O
is	O
the	O
risk	B
of	O
the	O
learned	O
hypothesis	B
?	O
the	O
ﬁrst	O
possible	O
goal	O
of	O
deriving	O
performance	O
guarantees	O
on	O
a	O
learning	O
algo-	O
rithm	O
is	O
bounding	O
the	O
risk	B
of	O
the	O
output	O
predictor	B
.	O
here	O
,	O
both	O
pac	O
learning	O
and	O
nonuniform	O
learning	O
give	O
us	O
an	O
upper	O
bound	O
on	O
the	O
true	O
risk	O
of	O
the	O
learned	O
hypothesis	B
based	O
on	O
its	O
empirical	B
risk	I
.	O
consistency	B
guarantees	O
do	O
not	O
provide	O
such	O
a	O
bound	O
.	O
however	O
,	O
it	O
is	O
always	O
possible	O
to	O
estimate	O
the	O
risk	B
of	O
the	O
output	O
predictor	B
using	O
a	O
validation	B
set	O
(	O
as	O
will	O
be	O
described	O
in	O
chapter	O
11	O
)	O
.	O
how	O
many	O
examples	O
are	O
required	O
to	O
be	O
as	O
good	O
as	O
the	O
best	O
hypothesis	B
in	O
h	O
?	O
when	O
approaching	O
a	O
learning	O
problem	O
,	O
a	O
natural	O
question	O
is	O
how	O
many	O
exam-	O
ples	O
we	O
need	O
to	O
collect	O
in	O
order	O
to	O
learn	O
it	O
.	O
here	O
,	O
pac	O
learning	O
gives	O
a	O
crisp	O
answer	O
.	O
however	O
,	O
for	O
both	O
nonuniform	O
learning	O
and	O
consistency	B
,	O
we	O
do	O
not	O
know	O
in	O
advance	O
how	O
many	O
examples	O
are	O
required	O
to	O
learn	O
h.	O
in	O
nonuniform	O
learning	O
this	O
number	O
depends	O
on	O
the	O
best	O
hypothesis	B
in	O
h	O
,	O
and	O
in	O
consistency	B
it	O
also	O
depends	O
on	O
the	O
underlying	O
distribution	O
.	O
in	O
this	O
sense	O
,	O
pac	O
learning	O
is	O
the	O
only	O
useful	O
deﬁnition	O
of	O
learnability	O
.	O
on	O
the	O
ﬂip	O
side	O
,	O
one	O
should	O
keep	O
in	O
mind	O
that	O
even	O
if	O
the	O
estimation	B
error	I
of	O
the	O
predictor	B
we	O
learn	O
is	O
small	O
,	O
its	O
risk	B
may	O
still	O
be	O
large	O
if	O
h	O
has	O
a	O
large	O
approximation	B
error	I
.	O
so	O
,	O
for	O
the	O
question	O
“	O
how	O
many	O
examples	O
are	O
required	O
to	O
be	O
as	O
good	O
as	O
the	O
bayes	O
optimal	O
predictor	B
?	O
”	O
even	O
pac	O
guarantees	O
do	O
not	O
provide	O
us	O
with	O
a	O
crisp	O
answer	O
.	O
this	O
reﬂects	O
the	O
fact	O
that	O
the	O
usefulness	O
of	O
pac	O
learning	O
relies	O
on	O
the	O
quality	O
of	O
our	O
prior	B
knowledge	I
.	O
pac	O
guarantees	O
also	O
help	O
us	O
to	O
understand	O
what	O
we	O
should	O
do	O
next	O
if	O
our	O
learning	O
algorithm	O
returns	O
a	O
hypothesis	B
with	O
a	O
large	O
risk	B
,	O
since	O
we	O
can	O
bound	O
the	O
part	O
of	O
the	O
error	O
that	O
stems	O
from	O
estimation	B
error	I
and	O
therefore	O
know	O
how	O
much	O
of	O
the	O
error	O
is	O
attributed	O
to	O
approximation	B
error	I
.	O
if	O
the	O
approximation	B
error	I
is	O
large	O
,	O
we	O
know	O
that	O
we	O
should	O
use	O
a	O
diﬀerent	O
hypothesis	B
class	I
.	O
similarly	O
,	O
if	O
a	O
nonuniform	O
algorithm	O
fails	O
,	O
we	O
can	O
consider	O
a	O
diﬀerent	O
weighting	O
function	B
over	O
(	O
subsets	O
of	O
)	O
hypotheses	O
.	O
however	O
,	O
when	O
a	O
consistent	O
algorithm	O
fails	O
,	O
we	O
have	O
no	O
idea	O
whether	O
this	O
is	O
because	O
of	O
the	O
estimation	B
error	I
or	O
the	O
approximation	B
error	I
.	O
furthermore	O
,	O
even	O
if	O
we	O
are	O
sure	O
we	O
have	O
a	O
problem	O
with	O
the	O
estimation	O
94	O
nonuniform	O
learnability	O
error	O
term	O
,	O
we	O
do	O
not	O
know	O
how	O
many	O
more	O
examples	O
are	O
needed	O
to	O
make	O
the	O
estimation	B
error	I
small	O
.	O
how	O
to	O
learn	O
?	O
how	O
to	O
express	O
prior	B
knowledge	I
?	O
maybe	O
the	O
most	O
useful	O
aspect	O
of	O
the	O
theory	O
of	O
learning	O
is	O
in	O
providing	O
an	O
answer	O
to	O
the	O
question	O
of	O
“	O
how	O
to	O
learn.	O
”	O
the	O
deﬁnition	O
of	O
pac	O
learning	O
yields	O
the	O
limitation	O
of	O
learning	O
(	O
via	O
the	O
no-free-lunch	B
theorem	O
)	O
and	O
the	O
necessity	O
of	O
prior	B
knowledge	I
.	O
it	O
gives	O
us	O
a	O
crisp	O
way	O
to	O
encode	O
prior	B
knowledge	I
by	O
choosing	O
a	O
hypothesis	B
class	I
,	O
and	O
once	O
this	O
choice	O
is	O
made	O
,	O
we	O
have	O
a	O
generic	O
learning	O
rule	O
–	O
erm	O
.	O
the	O
deﬁnition	O
of	O
nonuniform	O
learnability	O
also	O
yields	O
a	O
crisp	O
way	O
to	O
encode	O
prior	B
knowledge	I
by	O
specifying	O
weights	O
over	O
(	O
subsets	O
of	O
)	O
hypotheses	O
of	O
h.	O
once	O
this	O
choice	O
is	O
made	O
,	O
we	O
again	O
have	O
a	O
generic	O
learning	O
rule	O
–	O
srm	O
.	O
the	O
srm	O
rule	O
is	O
also	O
advantageous	O
in	O
model	B
selection	I
tasks	O
,	O
where	O
prior	B
knowledge	I
is	O
partial	O
.	O
we	O
elaborate	O
on	O
model	B
selection	I
in	O
chapter	O
11	O
and	O
here	O
we	O
give	O
a	O
brief	O
example	O
.	O
consider	O
the	O
problem	O
of	O
ﬁtting	O
a	O
one	O
dimensional	O
polynomial	O
to	O
data	O
;	O
namely	O
,	O
our	O
goal	O
is	O
to	O
learn	O
a	O
function	B
,	O
h	O
:	O
r	O
→	O
r	O
,	O
and	O
as	O
prior	B
knowledge	I
we	O
consider	O
the	O
hypothesis	B
class	I
of	O
polynomials	O
.	O
however	O
,	O
we	O
might	O
be	O
uncertain	O
regarding	O
which	O
degree	O
d	O
would	O
give	O
the	O
best	O
results	O
for	O
our	O
data	O
set	B
:	O
a	O
small	O
degree	O
might	O
not	O
ﬁt	O
the	O
data	O
well	O
(	O
i.e.	O
,	O
it	O
will	O
have	O
a	O
large	O
approximation	B
error	I
)	O
,	O
whereas	O
a	O
high	O
degree	O
might	O
lead	O
to	O
overﬁtting	B
(	O
i.e.	O
,	O
it	O
will	O
have	O
a	O
large	O
estimation	B
error	I
)	O
.	O
in	O
the	O
following	O
we	O
depict	O
the	O
result	O
of	O
ﬁtting	O
a	O
polynomial	O
of	O
degrees	O
2	O
,	O
3	O
,	O
and	O
10	O
to	O
the	O
same	O
training	B
set	I
.	O
degree	O
2	O
degree	O
3	O
degree	O
10	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
empirical	B
risk	I
decreases	O
as	O
we	O
enlarge	O
the	O
degree	O
.	O
therefore	O
,	O
if	O
we	O
choose	O
h	O
to	O
be	O
the	O
class	O
of	O
all	O
polynomials	O
up	O
to	O
degree	O
10	O
then	O
the	O
erm	O
rule	O
with	O
respect	O
to	O
this	O
class	O
would	O
output	O
a	O
10	O
degree	O
polynomial	O
and	O
would	O
overﬁt	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
we	O
choose	O
too	O
small	O
a	O
hypothesis	B
class	I
,	O
say	O
,	O
polynomials	O
up	O
to	O
degree	O
2	O
,	O
then	O
the	O
erm	O
would	O
suﬀer	O
from	O
underﬁtting	B
(	O
i.e.	O
,	O
a	O
large	O
approximation	B
error	I
)	O
.	O
in	O
contrast	O
,	O
we	O
can	O
use	O
the	O
srm	O
rule	O
on	O
the	O
set	B
of	O
all	O
polynomials	O
,	O
while	O
ordering	O
subsets	O
of	O
h	O
according	O
to	O
their	O
degree	O
,	O
and	O
this	O
will	O
yield	O
a	O
3rd	O
degree	O
polynomial	O
since	O
the	O
combination	O
of	O
its	O
empirical	B
risk	I
and	O
the	O
bound	O
on	O
its	O
estimation	B
error	I
is	O
the	O
smallest	O
.	O
in	O
other	O
words	O
,	O
the	O
srm	O
rule	O
enables	O
us	O
to	O
select	O
the	O
right	O
model	O
on	O
the	O
basis	O
of	O
the	O
data	O
itself	O
.	O
the	O
price	O
we	O
pay	O
for	O
this	O
ﬂexibility	O
(	O
besides	O
a	O
slight	O
increase	O
of	O
the	O
estimation	B
error	I
relative	O
to	O
pac	O
learning	O
w.r.t	O
.	O
the	O
optimal	O
degree	O
)	O
is	O
that	O
we	O
do	O
not	O
know	O
in	O
7.5	O
discussing	O
the	O
diﬀerent	O
notions	O
of	O
learnability	O
95	O
advance	O
how	O
many	O
examples	O
are	O
needed	O
to	O
compete	O
with	O
the	O
best	O
hypothesis	B
in	O
h.	O
unlike	O
the	O
notions	O
of	O
pac	O
learnability	O
and	O
nonuniform	O
learnability	O
,	O
the	O
deﬁni-	O
tion	O
of	O
consistency	B
does	O
not	O
yield	O
a	O
natural	O
learning	O
paradigm	O
or	O
a	O
way	O
to	O
encode	O
prior	B
knowledge	I
.	O
in	O
fact	O
,	O
in	O
many	O
cases	O
there	O
is	O
no	O
need	O
for	O
prior	B
knowledge	I
at	O
all	O
.	O
for	O
example	O
,	O
we	O
saw	O
that	O
even	O
the	O
memorize	O
algorithm	O
,	O
which	O
intuitively	O
should	O
not	O
be	O
called	O
a	O
learning	O
algorithm	O
,	O
is	O
a	O
consistent	O
algorithm	O
for	O
any	O
class	O
deﬁned	O
over	O
a	O
countable	O
domain	B
and	O
a	O
ﬁnite	O
label	B
set	O
.	O
this	O
hints	O
that	O
consistency	B
is	O
a	O
very	O
weak	O
requirement	O
.	O
which	O
learning	O
algorithm	O
should	O
we	O
prefer	O
?	O
one	O
may	O
argue	O
that	O
even	O
though	O
consistency	B
is	O
a	O
weak	O
requirement	O
,	O
it	O
is	O
desirable	O
that	O
a	O
learning	O
algorithm	O
will	O
be	O
consistent	O
with	O
respect	O
to	O
the	O
set	B
of	O
all	O
functions	O
from	O
x	O
to	O
y	O
,	O
which	O
gives	O
us	O
a	O
guarantee	O
that	O
for	O
enough	O
training	O
examples	O
,	O
we	O
will	O
always	O
be	O
as	O
good	O
as	O
the	O
bayes	O
optimal	O
predictor	B
.	O
therefore	O
,	O
if	O
we	O
have	O
two	O
algorithms	O
,	O
where	O
one	O
is	O
consistent	O
and	O
the	O
other	O
one	O
is	O
not	O
consistent	O
,	O
we	O
should	O
prefer	O
the	O
consistent	O
algorithm	O
.	O
however	O
,	O
this	O
argument	O
is	O
problematic	O
for	O
two	O
reasons	O
.	O
first	O
,	O
maybe	O
it	O
is	O
the	O
case	O
that	O
for	O
most	O
“	O
natural	O
”	O
distributions	O
we	O
will	O
observe	O
in	O
practice	O
that	O
the	O
sample	B
complexity	I
of	O
the	O
consistent	O
algorithm	O
will	O
be	O
so	O
large	O
so	O
that	O
in	O
every	O
practical	O
situation	O
we	O
will	O
not	O
obtain	O
enough	O
examples	O
to	O
enjoy	O
this	O
guarantee	O
.	O
second	O
,	O
it	O
is	O
not	O
very	O
hard	O
to	O
make	O
any	O
pac	O
or	O
nonuniform	O
learner	O
consistent	O
with	O
respect	O
to	O
the	O
class	O
of	O
all	O
functions	O
from	O
x	O
to	O
y.	O
concretely	O
,	O
consider	O
a	O
countable	O
domain	B
,	O
x	O
,	O
a	O
ﬁnite	O
label	B
set	O
y	O
,	O
and	O
a	O
hypothesis	B
class	I
,	O
h	O
,	O
of	O
functions	O
from	O
x	O
to	O
y.	O
we	O
can	O
make	O
any	O
nonuniform	O
learner	O
for	O
h	O
be	O
consistent	O
with	O
respect	O
to	O
the	O
class	O
of	O
all	O
classiﬁers	O
from	O
x	O
to	O
y	O
using	O
the	O
following	O
simple	O
trick	O
:	O
upon	O
receiving	O
a	O
training	B
set	I
,	O
we	O
will	O
ﬁrst	O
run	O
the	O
nonuniform	O
learner	O
over	O
the	O
training	B
set	I
,	O
and	O
then	O
we	O
will	O
obtain	O
a	O
bound	O
on	O
the	O
true	O
risk	O
of	O
the	O
learned	O
predictor	B
.	O
if	O
this	O
bound	O
is	O
small	O
enough	O
we	O
are	O
done	O
.	O
otherwise	O
,	O
we	O
revert	O
to	O
the	O
memorize	O
algorithm	O
.	O
this	O
simple	O
modiﬁcation	O
makes	O
the	O
algorithm	O
consistent	O
with	O
respect	O
to	O
all	O
functions	O
from	O
x	O
to	O
y.	O
since	O
it	O
is	O
easy	O
to	O
make	O
any	O
algorithm	O
consistent	O
,	O
it	O
may	O
not	O
be	O
wise	O
to	O
prefer	O
one	O
algorithm	O
over	O
the	O
other	O
just	O
because	O
of	O
consistency	B
considerations	O
.	O
7.5.1	O
the	O
no-free-lunch	B
theorem	O
revisited	O
recall	B
that	O
the	O
no-free-lunch	B
theorem	O
(	O
theorem	O
5.1	O
from	O
chapter	O
5	O
)	O
implies	O
that	O
no	O
algorithm	O
can	O
learn	O
the	O
class	O
of	O
all	O
classiﬁers	O
over	O
an	O
inﬁnite	O
domain	B
.	O
in	O
contrast	O
,	O
in	O
this	O
chapter	O
we	O
saw	O
that	O
the	O
memorize	O
algorithm	O
is	O
consistent	O
with	O
respect	O
to	O
the	O
class	O
of	O
all	O
classiﬁers	O
over	O
a	O
countable	O
inﬁnite	O
domain	B
.	O
to	O
understand	O
why	O
these	O
two	O
statements	O
do	O
not	O
contradict	O
each	O
other	O
,	O
let	O
us	O
ﬁrst	O
recall	B
the	O
formal	O
statement	O
of	O
the	O
no-free-lunch	B
theorem	O
.	O
let	O
x	O
be	O
a	O
countable	O
inﬁnite	O
domain	B
and	O
let	O
y	O
=	O
{	O
±1	O
}	O
.	O
the	O
no-free-lunch	B
theorem	O
implies	O
the	O
following	O
:	O
for	O
any	O
algorithm	O
,	O
a	O
,	O
and	O
a	O
training	B
set	I
size	O
,	O
m	O
,	O
there	O
exist	O
a	O
distribution	O
over	O
x	O
and	O
a	O
function	B
h	O
(	O
cid:63	O
)	O
:	O
x	O
→	O
y	O
,	O
such	O
that	O
if	O
a	O
96	O
nonuniform	O
learnability	O
will	O
get	O
a	O
sample	O
of	O
m	O
i.i.d	O
.	O
training	O
examples	O
,	O
labeled	O
by	O
h	O
(	O
cid:63	O
)	O
,	O
then	O
a	O
is	O
likely	O
to	O
return	O
a	O
classiﬁer	B
with	O
a	O
larger	O
error	O
.	O
the	O
consistency	B
of	O
memorize	O
implies	O
the	O
following	O
:	O
for	O
every	O
distribution	O
over	O
x	O
and	O
a	O
labeling	O
function	B
h	O
(	O
cid:63	O
)	O
:	O
x	O
→	O
y	O
,	O
there	O
exists	O
a	O
training	B
set	I
size	O
m	O
(	O
that	O
depends	O
on	O
the	O
distribution	O
and	O
on	O
h	O
(	O
cid:63	O
)	O
)	O
such	O
that	O
if	O
memorize	O
receives	O
at	O
least	O
m	O
examples	O
it	O
is	O
likely	O
to	O
return	O
a	O
classiﬁer	B
with	O
a	O
small	O
error	O
.	O
we	O
see	O
that	O
in	O
the	O
no-free-lunch	B
theorem	O
,	O
we	O
ﬁrst	O
ﬁx	O
the	O
training	B
set	I
size	O
,	O
and	O
then	O
ﬁnd	O
a	O
distribution	O
and	O
a	O
labeling	O
function	B
that	O
are	O
bad	O
for	O
this	O
training	B
set	I
size	O
.	O
in	O
contrast	O
,	O
in	O
consistency	B
guarantees	O
,	O
we	O
ﬁrst	O
ﬁx	O
the	O
distribution	O
and	O
the	O
labeling	O
function	B
,	O
and	O
only	O
then	O
do	O
we	O
ﬁnd	O
a	O
training	B
set	I
size	O
that	O
suﬃces	O
for	O
learning	O
this	O
particular	O
distribution	O
and	O
labeling	O
function	B
.	O
7.6	O
summary	O
we	O
introduced	O
nonuniform	O
learnability	O
as	O
a	O
relaxation	O
of	O
pac	O
learnability	O
and	O
consistency	B
as	O
a	O
relaxation	O
of	O
nonuniform	O
learnability	O
.	O
this	O
means	O
that	O
even	O
classes	O
of	O
inﬁnite	O
vc-dimension	O
can	O
be	O
learnable	O
,	O
in	O
some	O
weaker	O
sense	O
of	O
learn-	O
ability	O
.	O
we	O
discussed	O
the	O
usefulness	O
of	O
the	O
diﬀerent	O
deﬁnitions	O
of	O
learnability	O
.	O
for	O
hypothesis	B
classes	O
that	O
are	O
countable	O
,	O
we	O
can	O
apply	O
the	O
minimum	O
descrip-	O
tion	O
length	O
scheme	O
,	O
where	O
hypotheses	O
with	O
shorter	O
descriptions	O
are	O
preferred	O
,	O
following	O
the	O
principle	O
of	O
occam	O
’	O
s	O
razor	O
.	O
an	O
interesting	O
example	O
is	O
the	O
hypothe-	O
sis	O
class	O
of	O
all	O
predictors	O
we	O
can	O
implement	O
in	O
c++	O
(	O
or	O
any	O
other	O
programming	O
language	O
)	O
,	O
which	O
we	O
can	O
learn	O
(	O
nonuniformly	O
)	O
using	O
the	O
mdl	O
scheme	O
.	O
arguably	O
,	O
the	O
class	O
of	O
all	O
predictors	O
we	O
can	O
implement	O
in	O
c++	O
is	O
a	O
powerful	O
class	O
of	O
functions	O
and	O
probably	O
contains	O
all	O
that	O
we	O
can	O
hope	O
to	O
learn	O
in	O
prac-	O
tice	O
.	O
the	O
ability	O
to	O
learn	O
this	O
class	O
is	O
impressive	O
,	O
and	O
,	O
seemingly	O
,	O
this	O
chapter	O
should	O
have	O
been	O
the	O
last	O
chapter	O
of	O
this	O
book	O
.	O
this	O
is	O
not	O
the	O
case	O
,	O
because	O
of	O
the	O
computational	O
aspect	O
of	O
learning	O
:	O
that	O
is	O
,	O
the	O
runtime	O
needed	O
to	O
apply	O
the	O
learning	O
rule	O
.	O
for	O
example	O
,	O
to	O
implement	O
the	O
mdl	O
paradigm	O
with	O
respect	O
to	O
all	O
c++	O
programs	O
,	O
we	O
need	O
to	O
perform	O
an	O
exhaustive	O
search	O
over	O
all	O
c++	O
pro-	O
grams	O
,	O
which	O
will	O
take	O
forever	O
.	O
even	O
the	O
implementation	O
of	O
the	O
erm	O
paradigm	O
with	O
respect	O
to	O
all	O
c++	O
programs	O
of	O
description	O
length	O
at	O
most	O
1000	O
bits	O
re-	O
quires	O
an	O
exhaustive	O
search	O
over	O
21000	O
hypotheses	O
.	O
while	O
the	O
sample	B
complexity	I
,	O
the	O
runtime	O
is	O
≥	O
21000.	O
this	O
is	O
a	O
huge	O
of	O
learning	O
this	O
class	O
is	O
just	O
1000+log	O
(	O
2/δ	O
)	O
number	O
–	O
much	O
larger	O
than	O
the	O
number	O
of	O
atoms	O
in	O
the	O
visible	O
universe	O
.	O
in	O
the	O
next	O
chapter	O
we	O
formally	O
deﬁne	O
the	O
computational	B
complexity	I
of	O
learning	O
.	O
in	O
the	O
second	O
part	O
of	O
this	O
book	O
we	O
will	O
study	O
hypothesis	B
classes	O
for	O
which	O
the	O
erm	O
or	O
srm	O
schemes	O
can	O
be	O
implemented	O
eﬃciently	O
.	O
2	O
7.7	O
bibliographic	O
remarks	O
97	O
7.7	O
bibliographic	O
remarks	O
our	O
deﬁnition	O
of	O
nonuniform	O
learnability	O
is	O
related	O
to	O
the	O
deﬁnition	O
of	O
an	O
occam-	O
algorithm	O
in	O
blumer	O
,	O
ehrenfeucht	O
,	O
haussler	O
&	O
warmuth	O
(	O
1987	O
)	O
.	O
the	O
concept	O
of	O
srm	O
is	O
due	O
to	O
(	O
vapnik	O
&	O
chervonenkis	O
1974	O
,	O
vapnik	O
1995	O
)	O
.	O
the	O
concept	O
of	O
mdl	O
is	O
due	O
to	O
(	O
rissanen	O
1978	O
,	O
rissanen	O
1983	O
)	O
.	O
the	O
relation	O
between	O
srm	O
and	O
mdl	O
is	O
discussed	O
in	O
vapnik	O
(	O
1995	O
)	O
.	O
these	O
notions	O
are	O
also	O
closely	O
related	O
to	O
the	O
notion	O
of	O
regularization	B
(	O
e.g	O
.	O
tikhonov	O
(	O
1943	O
)	O
)	O
.	O
we	O
will	O
elaborate	O
on	O
regularization	B
in	O
the	O
second	O
part	O
of	O
this	O
book	O
.	O
the	O
notion	O
of	O
consistency	B
of	O
estimators	O
dates	O
back	O
to	O
fisher	O
(	O
1922	O
)	O
.	O
our	O
pre-	O
sentation	O
of	O
consistency	B
follows	O
steinwart	O
&	O
christmann	O
(	O
2008	O
)	O
,	O
who	O
also	O
derived	O
several	O
no-free-lunch	B
theorems	O
.	O
7.8	O
exercises	O
1.	O
prove	O
that	O
for	O
any	O
ﬁnite	O
class	O
h	O
,	O
and	O
any	O
description	O
language	O
d	O
:	O
h	O
→	O
{	O
0	O
,	O
1	O
}	O
∗	O
,	O
the	O
vc-dimension	O
of	O
h	O
is	O
at	O
most	O
2	O
sup	O
{	O
|d	O
(	O
h	O
)	O
|	O
:	O
h	O
∈	O
h	O
}	O
–	O
the	O
maxi-	O
mum	O
description	O
length	O
of	O
a	O
predictor	B
in	O
h.	O
furthermore	O
,	O
if	O
d	O
is	O
a	O
preﬁx-free	O
description	O
then	O
vcdim	O
(	O
h	O
)	O
≤	O
sup	O
{	O
|d	O
(	O
h	O
)	O
|	O
:	O
h	O
∈	O
h	O
}	O
.	O
2.	O
let	O
h	O
=	O
{	O
hn	O
:	O
n	O
∈	O
n	O
}	O
be	O
an	O
inﬁnite	O
countable	O
hypothesis	B
class	I
for	O
binary	O
classiﬁcation	O
.	O
show	O
that	O
it	O
is	O
impossible	O
to	O
assign	O
weights	O
to	O
the	O
hypotheses	O
in	O
h	O
such	O
that	O
•	O
h	O
could	O
be	O
learnt	O
nonuniformly	O
using	O
these	O
weights	O
.	O
that	O
is	O
,	O
the	O
weighting	O
h∈h	O
w	O
(	O
h	O
)	O
≤	O
1	O
.	O
•	O
the	O
weights	O
would	O
be	O
monotonically	O
nondecreasing	O
.	O
that	O
is	O
,	O
if	O
i	O
<	O
j	O
,	O
then	O
function	B
w	O
:	O
h	O
→	O
[	O
0	O
,	O
1	O
]	O
should	O
satisfy	O
the	O
condition	O
(	O
cid:80	O
)	O
ﬁnite	O
.	O
find	O
a	O
weighting	O
function	B
w	O
:	O
h	O
→	O
[	O
0	O
,	O
1	O
]	O
such	O
that	O
(	O
cid:80	O
)	O
n=1	O
hn	O
,	O
where	O
for	O
every	O
n	O
∈	O
n	O
,	O
hn	O
is	O
h∈h	O
w	O
(	O
h	O
)	O
≤	O
1	O
and	O
so	O
that	O
for	O
all	O
h	O
∈	O
h	O
,	O
w	O
(	O
h	O
)	O
is	O
determined	O
by	O
n	O
(	O
h	O
)	O
=	O
min	O
{	O
n	O
:	O
h	O
∈	O
hn	O
}	O
and	O
by	O
|hn	O
(	O
h	O
)	O
|	O
.	O
3	O
.	O
•	O
consider	O
a	O
hypothesis	B
class	I
h	O
=	O
(	O
cid:83	O
)	O
∞	O
w	O
(	O
hi	O
)	O
≤	O
w	O
(	O
hj	O
)	O
.	O
inﬁnite	O
)	O
.	O
•	O
(	O
*	O
)	O
deﬁne	O
such	O
a	O
function	B
w	O
when	O
for	O
all	O
n	O
hn	O
is	O
countable	O
(	O
possibly	O
4.	O
let	O
h	O
be	O
some	O
hypothesis	B
class	I
.	O
for	O
any	O
h	O
∈	O
h	O
,	O
let	O
|h|	O
denote	O
the	O
description	O
length	O
of	O
h	O
,	O
according	O
to	O
some	O
ﬁxed	O
description	O
language	O
.	O
consider	O
the	O
mdl	O
learning	O
paradigm	O
in	O
which	O
the	O
algorithm	O
returns	O
:	O
(	O
cid:34	O
)	O
(	O
cid:114	O
)	O
|h|	O
+	O
ln	O
(	O
2/δ	O
)	O
(	O
cid:35	O
)	O
2m	O
,	O
hs	O
∈	O
arg	O
min	O
h∈h	O
ls	O
(	O
h	O
)	O
+	O
where	O
s	O
is	O
a	O
sample	O
of	O
size	O
m.	O
for	O
any	O
b	O
>	O
0	O
,	O
let	O
hb	O
=	O
{	O
h	O
∈	O
h	O
:	O
|h|	O
≤	O
b	O
}	O
,	O
and	O
deﬁne	O
h∗	O
b	O
=	O
arg	O
min	O
h∈hb	O
ld	O
(	O
h	O
)	O
.	O
98	O
nonuniform	O
learnability	O
prove	O
a	O
bound	O
on	O
ld	O
(	O
hs	O
)	O
−ld	O
(	O
h∗	O
b	O
)	O
in	O
terms	O
of	O
b	O
,	O
the	O
conﬁdence	B
parameter	O
δ	O
,	O
and	O
the	O
size	O
of	O
the	O
training	B
set	I
m.	O
•	O
note	O
:	O
such	O
bounds	O
are	O
known	O
as	O
oracle	O
inequalities	O
in	O
the	O
literature	O
:	O
we	O
wish	O
to	O
estimate	O
how	O
good	O
we	O
are	O
compared	O
to	O
a	O
reference	O
classiﬁer	B
(	O
or	O
“	O
oracle	O
”	O
)	O
h∗	O
b	O
.	O
5.	O
in	O
this	O
question	O
we	O
wish	O
to	O
show	O
a	O
no-free-lunch	B
result	O
for	O
nonuniform	O
learn-	O
ability	O
:	O
namely	O
,	O
that	O
,	O
over	O
any	O
inﬁnite	O
domain	B
,	O
the	O
class	O
of	O
all	O
functions	O
is	O
not	O
learnable	O
even	O
under	O
the	O
relaxed	O
nonuniform	O
variation	O
of	O
learning	O
.	O
recall	B
that	O
an	O
algorithm	O
,	O
a	O
,	O
nonuniformly	O
learns	O
a	O
hypothesis	B
class	I
h	O
if	O
there	O
exists	O
a	O
function	B
mnulh	O
:	O
(	O
0	O
,	O
1	O
)	O
2×h	O
→	O
n	O
such	O
that	O
,	O
for	O
every	O
	O
,	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
and	O
for	O
every	O
h	O
∈	O
h	O
,	O
if	O
m	O
≥	O
mnulh	O
(	O
	O
,	O
δ	O
,	O
h	O
)	O
then	O
for	O
every	O
distribution	O
d	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
the	O
choice	O
of	O
s	O
∼	O
dm	O
,	O
it	O
holds	O
that	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
ld	O
(	O
h	O
)	O
+	O
	O
.	O
n∈n	O
hn	O
and	O
,	O
for	O
every	O
n	O
∈	O
n	O
,	O
vcdim	O
(	O
hn	O
)	O
is	O
ﬁnite	O
.	O
so	O
that	O
h	O
=	O
(	O
cid:83	O
)	O
of	O
classes	O
(	O
hn	O
:	O
n	O
∈	O
n	O
)	O
such	O
that	O
h	O
=	O
(	O
cid:83	O
)	O
if	O
such	O
an	O
algorithm	O
exists	O
then	O
we	O
say	O
that	O
h	O
is	O
nonuniformly	O
learnable	O
.	O
1.	O
let	O
a	O
be	O
a	O
nonuniform	O
learner	O
for	O
a	O
class	O
h.	O
for	O
each	O
n	O
∈	O
n	O
deﬁne	O
ha	O
n	O
=	O
{	O
h	O
∈	O
h	O
:	O
mnul	O
(	O
0.1	O
,	O
0.1	O
,	O
h	O
)	O
≤	O
n	O
}	O
.	O
prove	O
that	O
each	O
such	O
class	O
hn	O
has	O
a	O
ﬁnite	O
vc-dimension	O
.	O
2.	O
prove	O
that	O
if	O
a	O
class	O
h	O
is	O
nonuniformly	O
learnable	O
then	O
there	O
are	O
classes	O
hn	O
3.	O
let	O
h	O
be	O
a	O
class	O
that	O
shatters	O
an	O
inﬁnite	O
set	B
.	O
then	O
,	O
for	O
every	O
sequence	O
n∈n	O
hn	O
,	O
there	O
exists	O
some	O
n	O
for	O
which	O
vcdim	O
(	O
hn	O
)	O
=	O
∞	O
.	O
hint	O
:	O
given	O
a	O
class	O
h	O
that	O
shatters	O
some	O
inﬁnite	O
set	B
k	O
,	O
and	O
a	O
sequence	O
of	O
classes	O
(	O
hn	O
:	O
n	O
∈	O
n	O
)	O
,	O
each	O
having	O
a	O
ﬁnite	O
vc-dimension	O
,	O
start	O
by	O
deﬁning	O
subsets	O
kn	O
⊆	O
k	O
such	O
that	O
,	O
for	O
all	O
n	O
,	O
|kn|	O
>	O
vcdim	O
(	O
hn	O
)	O
and	O
for	O
any	O
n	O
(	O
cid:54	O
)	O
=	O
m	O
,	O
kn	O
∩	O
km	O
=	O
∅	O
.	O
now	O
,	O
pick	O
for	O
each	O
such	O
kn	O
a	O
function	B
fn	O
:	O
kn	O
→	O
{	O
0	O
,	O
1	O
}	O
so	O
that	O
no	O
h	O
∈	O
hn	O
agrees	O
with	O
fn	O
on	O
the	O
domain	B
kn	O
.	O
finally	O
,	O
deﬁne	O
n∈n	O
hn	O
4.	O
construct	O
a	O
class	O
h1	O
of	O
functions	O
from	O
the	O
unit	O
interval	O
[	O
0	O
,	O
1	O
]	O
to	O
{	O
0	O
,	O
1	O
}	O
that	O
5.	O
construct	O
a	O
class	O
h2	O
of	O
functions	O
from	O
the	O
unit	O
interval	O
[	O
0	O
,	O
1	O
]	O
to	O
{	O
0	O
,	O
1	O
}	O
that	O
f	O
:	O
x	O
→	O
{	O
0	O
,	O
1	O
}	O
by	O
combining	O
these	O
fn	O
’	O
s	O
and	O
prove	O
that	O
f	O
∈	O
(	O
cid:0	O
)	O
h	O
\	O
(	O
cid:83	O
)	O
is	O
nonuniformly	O
learnable	O
but	O
not	O
pac	O
learnable	O
.	O
(	O
cid:1	O
)	O
.	O
is	O
not	O
nonuniformly	O
learnable	O
.	O
6.	O
in	O
this	O
question	O
we	O
wish	O
to	O
show	O
that	O
the	O
algorithm	O
memorize	O
is	O
a	O
consistent	O
learner	O
for	O
every	O
class	O
of	O
(	O
binary-valued	O
)	O
functions	O
over	O
any	O
countable	O
domain	B
.	O
let	O
x	O
be	O
a	O
countable	O
domain	B
and	O
let	O
d	O
be	O
a	O
probability	O
distribution	O
over	O
x	O
.	O
1.	O
let	O
{	O
xi	O
:	O
i	O
∈	O
n	O
}	O
be	O
an	O
enumeration	O
of	O
the	O
elements	O
of	O
x	O
so	O
that	O
for	O
all	O
i	O
≤	O
j	O
,	O
d	O
(	O
{	O
xi	O
}	O
)	O
≤	O
d	O
(	O
{	O
xj	O
}	O
)	O
.	O
prove	O
that	O
(	O
cid:88	O
)	O
i≥n	O
lim	O
n→∞	O
d	O
(	O
{	O
xi	O
}	O
)	O
=	O
0	O
.	O
2.	O
given	O
any	O
	O
>	O
0	O
prove	O
that	O
there	O
exists	O
d	O
>	O
0	O
such	O
that	O
d	O
(	O
{	O
x	O
∈	O
x	O
:	O
d	O
(	O
{	O
x	O
}	O
)	O
<	O
d	O
}	O
)	O
<	O
	O
.	O
7.8	O
exercises	O
99	O
for	O
every	O
m	O
∈	O
n	O
,	O
p	O
s∼dm	O
[	O
∃xi	O
:	O
(	O
d	O
(	O
{	O
xi	O
}	O
)	O
>	O
η	O
and	O
xi	O
/∈	O
s	O
)	O
]	O
≤	O
ne−ηm	O
.	O
3.	O
prove	O
that	O
for	O
every	O
η	O
>	O
0	O
,	O
if	O
n	O
is	O
such	O
that	O
d	O
(	O
{	O
xi	O
}	O
)	O
<	O
η	O
for	O
all	O
i	O
>	O
n	O
,	O
then	O
4.	O
conclude	O
that	O
if	O
x	O
is	O
countable	O
then	O
for	O
every	O
probability	O
distribution	O
d	O
over	O
x	O
there	O
exists	O
a	O
function	B
md	O
:	O
(	O
0	O
,	O
1	O
)	O
×	O
(	O
0	O
,	O
1	O
)	O
→	O
n	O
such	O
that	O
for	O
every	O
	O
,	O
δ	O
>	O
0	O
if	O
m	O
>	O
md	O
(	O
	O
,	O
δ	O
)	O
then	O
p	O
s∼dm	O
[	O
d	O
(	O
{	O
x	O
:	O
x	O
/∈	O
s	O
}	O
)	O
>	O
	O
]	O
<	O
δ	O
.	O
5.	O
prove	O
that	O
memorize	O
is	O
a	O
consistent	O
learner	O
for	O
every	O
class	O
of	O
(	O
binary-	O
valued	O
)	O
functions	O
over	O
any	O
countable	O
domain	B
.	O
8	O
the	O
runtime	O
of	O
learning	O
so	O
far	O
in	O
the	O
book	O
we	O
have	O
studied	O
the	O
statistical	O
perspective	O
of	O
learning	O
,	O
namely	O
,	O
how	O
many	O
samples	O
are	O
needed	O
for	O
learning	O
.	O
in	O
other	O
words	O
,	O
we	O
focused	O
on	O
the	O
amount	O
of	O
information	O
learning	O
requires	O
.	O
however	O
,	O
when	O
considering	O
automated	O
learning	O
,	O
computational	O
resources	O
also	O
play	O
a	O
major	O
role	O
in	O
determining	O
the	O
com-	O
plexity	O
of	O
a	O
task	O
:	O
that	O
is	O
,	O
how	O
much	O
computation	O
is	O
involved	O
in	O
carrying	O
out	O
a	O
learning	O
task	O
.	O
once	O
a	O
suﬃcient	O
training	O
sample	O
is	O
available	O
to	O
the	O
learner	O
,	O
there	O
is	O
some	O
computation	O
to	O
be	O
done	O
to	O
extract	O
a	O
hypothesis	B
or	O
ﬁgure	O
out	O
the	O
label	B
of	O
a	O
given	O
test	O
instance	B
.	O
these	O
computational	O
resources	O
are	O
crucial	O
in	O
any	O
practical	O
application	O
of	O
machine	O
learning	O
.	O
we	O
refer	O
to	O
these	O
two	O
types	O
of	O
resources	O
as	O
the	O
sample	B
complexity	I
and	O
the	O
computational	B
complexity	I
.	O
in	O
this	O
chapter	O
,	O
we	O
turn	O
our	O
attention	O
to	O
the	O
computational	B
complexity	I
of	O
learning	O
.	O
the	O
computational	B
complexity	I
of	O
learning	O
should	O
be	O
viewed	O
in	O
the	O
wider	O
con-	O
text	O
of	O
the	O
computational	B
complexity	I
of	O
general	O
algorithmic	O
tasks	O
.	O
this	O
area	O
has	O
been	O
extensively	O
investigated	O
;	O
see	O
,	O
for	O
example	O
,	O
(	O
sipser	O
2006	O
)	O
.	O
the	O
introductory	O
comments	O
that	O
follow	O
summarize	O
the	O
basic	O
ideas	O
of	O
that	O
general	O
theory	O
that	O
are	O
most	O
relevant	O
to	O
our	O
discussion	O
.	O
the	O
actual	O
runtime	O
(	O
in	O
seconds	O
)	O
of	O
an	O
algorithm	O
depends	O
on	O
the	O
speciﬁc	O
ma-	O
chine	O
the	O
algorithm	O
is	O
being	O
implemented	O
on	O
(	O
e.g.	O
,	O
what	O
the	O
clock	O
rate	O
of	O
the	O
machine	O
’	O
s	O
cpu	O
is	O
)	O
.	O
to	O
avoid	O
dependence	O
on	O
the	O
speciﬁc	O
machine	O
,	O
it	O
is	O
common	O
to	O
analyze	O
the	O
runtime	O
of	O
algorithms	O
in	O
an	O
asymptotic	O
sense	O
.	O
for	O
example	O
,	O
we	O
say	O
that	O
the	O
computational	B
complexity	I
of	O
the	O
merge-sort	O
algorithm	O
,	O
which	O
sorts	O
a	O
list	O
of	O
n	O
items	O
,	O
is	O
o	O
(	O
n	O
log	O
(	O
n	O
)	O
)	O
.	O
this	O
implies	O
that	O
we	O
can	O
implement	O
the	O
algo-	O
rithm	O
on	O
any	O
machine	O
that	O
satisﬁes	O
the	O
requirements	O
of	O
some	O
accepted	O
abstract	O
model	O
of	O
computation	O
,	O
and	O
the	O
actual	O
runtime	O
in	O
seconds	O
will	O
satisfy	O
the	O
follow-	O
ing	O
:	O
there	O
exist	O
constants	O
c	O
and	O
n0	O
,	O
which	O
can	O
depend	O
on	O
the	O
actual	O
machine	O
,	O
such	O
that	O
,	O
for	O
any	O
value	O
of	O
n	O
>	O
n0	O
,	O
the	O
runtime	O
in	O
seconds	O
of	O
sorting	O
any	O
n	O
items	O
will	O
be	O
at	O
most	O
c	O
n	O
log	O
(	O
n	O
)	O
.	O
it	O
is	O
common	O
to	O
use	O
the	O
term	O
feasible	B
or	O
eﬃciently	O
computable	O
for	O
tasks	O
that	O
can	O
be	O
performed	O
by	O
an	O
algorithm	O
whose	O
running	O
time	O
is	O
o	O
(	O
p	O
(	O
n	O
)	O
)	O
for	O
some	O
polynomial	O
function	O
p.	O
one	O
should	O
note	O
that	O
this	O
type	O
of	O
analysis	O
depends	O
on	O
deﬁning	O
what	O
is	O
the	O
input	O
size	O
n	O
of	O
any	O
instance	B
to	O
which	O
the	O
algorithm	O
is	O
expected	O
to	O
be	O
applied	O
.	O
for	O
“	O
purely	O
algorithmic	O
”	O
tasks	O
,	O
as	O
dis-	O
cussed	O
in	O
the	O
common	O
computational	B
complexity	I
literature	O
,	O
this	O
input	O
size	O
is	O
clearly	O
deﬁned	O
;	O
the	O
algorithm	O
gets	O
an	O
input	O
instance	B
,	O
say	O
,	O
a	O
list	O
to	O
be	O
sorted	O
,	O
or	O
an	O
arithmetic	O
operation	O
to	O
be	O
calculated	O
,	O
which	O
has	O
a	O
well	O
deﬁned	O
size	O
(	O
say	O
,	O
the	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
8.1	O
computational	B
complexity	I
of	O
learning	O
101	O
number	O
of	O
bits	O
in	O
its	O
representation	O
)	O
.	O
for	O
machine	O
learning	O
tasks	O
,	O
the	O
notion	O
of	O
an	O
input	O
size	O
is	O
not	O
so	O
clear	O
.	O
an	O
algorithm	O
aims	O
to	O
detect	O
some	O
pattern	O
in	O
a	O
data	O
set	B
and	O
can	O
only	O
access	O
random	O
samples	O
of	O
that	O
data	O
.	O
we	O
start	O
the	O
chapter	O
by	O
discussing	O
this	O
issue	O
and	O
deﬁne	O
the	O
computational	B
complexity	I
of	O
learning	O
.	O
for	O
advanced	O
students	O
,	O
we	O
also	O
provide	O
a	O
detailed	O
formal	O
deﬁnition	O
.	O
we	O
then	O
move	O
on	O
to	O
consider	O
the	O
computational	B
complexity	I
of	O
im-	O
plementing	O
the	O
erm	O
rule	O
.	O
we	O
ﬁrst	O
give	O
several	O
examples	O
of	O
hypothesis	B
classes	O
where	O
the	O
erm	O
rule	O
can	O
be	O
eﬃciently	O
implemented	O
,	O
and	O
then	O
consider	O
some	O
cases	O
where	O
,	O
although	O
the	O
class	O
is	O
indeed	O
eﬃciently	O
learnable	O
,	O
erm	O
implemen-	O
tation	O
is	O
computationally	O
hard	O
.	O
it	O
follows	O
that	O
hardness	O
of	O
implementing	O
erm	O
does	O
not	O
imply	O
hardness	O
of	O
learning	O
.	O
finally	O
,	O
we	O
brieﬂy	O
discuss	O
how	O
one	O
can	O
show	O
hardness	O
of	O
a	O
given	O
learning	O
task	O
,	O
namely	O
,	O
that	O
no	O
learning	O
algorithm	O
can	O
solve	O
it	O
eﬃciently	O
.	O
8.1	O
computational	B
complexity	I
of	O
learning	O
recall	O
that	O
a	O
learning	O
algorithm	O
has	O
access	O
to	O
a	O
domain	B
of	I
examples	I
,	O
z	O
,	O
a	O
hy-	O
pothesis	O
class	O
,	O
h	O
,	O
a	O
loss	B
function	I
,	O
(	O
cid:96	O
)	O
,	O
and	O
a	O
training	B
set	I
of	O
examples	O
from	O
z	O
that	O
are	O
sampled	O
i.i.d	O
.	O
according	O
to	O
an	O
unknown	O
distribution	O
d.	O
given	O
parameters	O
	O
,	O
δ	O
,	O
the	O
algorithm	O
should	O
output	O
a	O
hypothesis	B
h	O
such	O
that	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
,	O
ld	O
(	O
h	O
)	O
≤	O
min	O
h	O
(	O
cid:48	O
)	O
∈h	O
ld	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
+	O
	O
.	O
as	O
mentioned	O
before	O
,	O
the	O
actual	O
runtime	O
of	O
an	O
algorithm	O
in	O
seconds	O
depends	O
on	O
the	O
speciﬁc	O
machine	O
.	O
to	O
allow	O
machine	O
independent	O
analysis	O
,	O
we	O
use	O
the	O
standard	O
approach	O
in	O
computational	B
complexity	I
theory	O
.	O
first	O
,	O
we	O
rely	O
on	O
a	O
notion	O
of	O
an	O
abstract	O
machine	O
,	O
such	O
as	O
a	O
turing	O
machine	O
(	O
or	O
a	O
turing	O
machine	O
over	O
the	O
reals	O
(	O
blum	O
,	O
shub	O
&	O
smale	O
1989	O
)	O
)	O
.	O
second	O
,	O
we	O
analyze	O
the	O
runtime	O
in	O
an	O
asymptotic	O
sense	O
,	O
while	O
ignoring	O
constant	O
factors	O
,	O
thus	O
the	O
speciﬁc	O
machine	O
is	O
not	O
important	O
as	O
long	O
as	O
it	O
implements	O
the	O
abstract	O
machine	O
.	O
usually	O
,	O
the	O
asymptote	O
is	O
with	O
respect	O
to	O
the	O
size	O
of	O
the	O
input	O
to	O
the	O
algorithm	O
.	O
for	O
example	O
,	O
for	O
the	O
merge-sort	O
algorithm	O
mentioned	O
before	O
,	O
we	O
analyze	O
the	O
runtime	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
items	O
that	O
need	O
to	O
be	O
sorted	O
.	O
in	O
the	O
context	O
of	O
learning	O
algorithms	O
,	O
there	O
is	O
no	O
clear	O
notion	O
of	O
“	O
input	O
size.	O
”	O
one	O
might	O
deﬁne	O
the	O
input	O
size	O
to	O
be	O
the	O
size	O
of	O
the	O
training	B
set	I
the	O
algorithm	O
receives	O
,	O
but	O
that	O
would	O
be	O
rather	O
pointless	O
.	O
if	O
we	O
give	O
the	O
algorithm	O
a	O
very	O
large	O
number	O
of	O
examples	O
,	O
much	O
larger	O
than	O
the	O
sample	B
complexity	I
of	O
the	O
learn-	O
ing	O
problem	O
,	O
the	O
algorithm	O
can	O
simply	O
ignore	O
the	O
extra	O
examples	O
.	O
therefore	O
,	O
a	O
larger	O
training	B
set	I
does	O
not	O
make	O
the	O
learning	O
problem	O
more	O
diﬃcult	O
,	O
and	O
,	O
con-	O
sequently	O
,	O
the	O
runtime	O
available	O
for	O
a	O
learning	O
algorithm	O
should	O
not	O
increase	O
as	O
we	O
increase	O
the	O
size	O
of	O
the	O
training	B
set	I
.	O
just	O
the	O
same	O
,	O
we	O
can	O
still	O
analyze	O
the	O
runtime	O
as	O
a	O
function	B
of	O
natural	O
parameters	O
of	O
the	O
problem	O
such	O
as	O
the	O
target	O
accuracy	O
,	O
the	O
conﬁdence	B
of	O
achieving	O
that	O
accuracy	B
,	O
the	O
dimensionality	O
of	O
the	O
102	O
the	O
runtime	O
of	O
learning	O
domain	O
set	B
,	O
or	O
some	O
measures	O
of	O
the	O
complexity	O
of	O
the	O
hypothesis	B
class	I
with	O
which	O
the	O
algorithm	O
’	O
s	O
output	O
is	O
compared	O
.	O
to	O
illustrate	O
this	O
,	O
consider	O
a	O
learning	O
algorithm	O
for	O
the	O
task	O
of	O
learning	O
axis	O
aligned	O
rectangles	O
.	O
a	O
speciﬁc	O
problem	O
of	O
learning	O
axis	O
aligned	O
rectangles	O
is	O
de-	O
rived	O
by	O
specifying	O
	O
,	O
δ	O
,	O
and	O
the	O
dimension	B
of	O
the	O
instance	B
space	I
.	O
we	O
can	O
deﬁne	O
a	O
sequence	O
of	O
problems	O
of	O
the	O
type	O
“	O
rectangles	O
learning	O
”	O
by	O
ﬁxing	O
	O
,	O
δ	O
and	O
varying	O
the	O
dimension	B
to	O
be	O
d	O
=	O
2	O
,	O
3	O
,	O
4	O
,	O
.	O
.	O
..	O
we	O
can	O
also	O
deﬁne	O
another	O
sequence	O
of	O
“	O
rect-	O
angles	O
learning	O
”	O
problems	O
by	O
ﬁxing	O
d	O
,	O
δ	O
and	O
varying	O
the	O
target	O
accuracy	O
to	O
be	O
	O
=	O
1	O
3	O
,	O
.	O
.	O
..	O
one	O
can	O
of	O
course	O
choose	O
other	O
sequences	O
of	O
such	O
problems	O
.	O
once	O
a	O
sequence	O
of	O
the	O
problems	O
is	O
ﬁxed	O
,	O
one	O
can	O
analyze	O
the	O
asymptotic	O
runtime	O
as	O
a	O
function	B
of	O
variables	O
of	O
that	O
sequence	O
.	O
2	O
,	O
1	O
before	O
we	O
introduce	O
the	O
formal	O
deﬁnition	O
,	O
there	O
is	O
one	O
more	O
subtlety	O
we	O
need	O
to	O
tackle	O
.	O
on	O
the	O
basis	O
of	O
the	O
preceding	O
,	O
a	O
learning	O
algorithm	O
can	O
“	O
cheat	O
,	O
”	O
by	O
transferring	O
the	O
computational	O
burden	O
to	O
the	O
output	O
hypothesis	B
.	O
for	O
example	O
,	O
the	O
algorithm	O
can	O
simply	O
deﬁne	O
the	O
output	O
hypothesis	B
to	O
be	O
the	O
function	B
that	O
stores	O
the	O
training	B
set	I
in	O
its	O
memory	O
,	O
and	O
whenever	O
it	O
gets	O
a	O
test	O
example	O
x	O
it	O
calculates	O
the	O
erm	O
hypothesis	B
on	O
the	O
training	B
set	I
and	O
applies	O
it	O
on	O
x.	O
note	O
that	O
in	O
this	O
case	O
,	O
our	O
algorithm	O
has	O
a	O
ﬁxed	O
output	O
(	O
namely	O
,	O
the	O
function	B
that	O
we	O
have	O
just	O
described	O
)	O
and	O
can	O
run	O
in	O
constant	O
time	O
.	O
however	O
,	O
learning	O
is	O
still	O
hard	O
–	O
the	O
hardness	O
is	O
now	O
in	O
implementing	O
the	O
output	O
classiﬁer	B
to	O
obtain	O
a	O
label	B
prediction	O
.	O
to	O
prevent	O
this	O
“	O
cheating	O
,	O
”	O
we	O
shall	O
require	O
that	O
the	O
output	O
of	O
a	O
learning	O
algorithm	O
must	O
be	O
applied	O
to	O
predict	O
the	O
label	B
of	O
a	O
new	O
example	O
in	O
time	O
that	O
does	O
not	O
exceed	O
the	O
runtime	O
of	O
training	O
(	O
that	O
is	O
,	O
computing	O
the	O
output	O
classiﬁer	B
from	O
the	O
input	O
training	O
sample	O
)	O
.	O
in	O
the	O
next	O
subsection	O
the	O
advanced	O
reader	O
may	O
ﬁnd	O
a	O
formal	O
deﬁnition	O
of	O
the	O
computational	B
complexity	I
of	O
learning	O
.	O
8.1.1	O
formal	O
deﬁnition*	O
the	O
deﬁnition	O
that	O
follows	O
relies	O
on	O
a	O
notion	O
of	O
an	O
underlying	O
abstract	O
machine	O
,	O
which	O
is	O
usually	O
either	O
a	O
turing	O
machine	O
or	O
a	O
turing	O
machine	O
over	O
the	O
reals	O
.	O
we	O
will	O
measure	O
the	O
computational	B
complexity	I
of	O
an	O
algorithm	O
using	O
the	O
number	O
of	O
“	O
operations	O
”	O
it	O
needs	O
to	O
perform	O
,	O
where	O
we	O
assume	O
that	O
for	O
any	O
machine	O
that	O
implements	O
the	O
underlying	O
abstract	O
machine	O
there	O
exists	O
a	O
constant	O
c	O
such	O
that	O
any	O
such	O
“	O
operation	O
”	O
can	O
be	O
performed	O
on	O
the	O
machine	O
using	O
c	O
seconds	O
.	O
definition	O
8.1	O
(	O
the	O
computational	B
complexity	I
of	O
a	O
learning	O
algorithm	O
)	O
we	O
deﬁne	O
the	O
complexity	O
of	O
learning	O
in	O
two	O
steps	O
.	O
first	O
we	O
consider	O
the	O
compu-	O
tational	O
complexity	O
of	O
a	O
ﬁxed	O
learning	O
problem	O
(	O
determined	O
by	O
a	O
triplet	O
(	O
z	O
,	O
h	O
,	O
(	O
cid:96	O
)	O
)	O
–	O
a	O
domain	B
set	O
,	O
a	O
benchmark	O
hypothesis	B
class	I
,	O
and	O
a	O
loss	B
function	I
)	O
.	O
then	O
,	O
in	O
the	O
second	O
step	O
we	O
consider	O
the	O
rate	O
of	O
change	O
of	O
that	O
complexity	O
along	O
a	O
sequence	O
of	O
such	O
tasks	O
.	O
1.	O
given	O
a	O
function	B
f	O
:	O
(	O
0	O
,	O
1	O
)	O
2	O
→	O
n	O
,	O
a	O
learning	O
task	O
(	O
z	O
,	O
h	O
,	O
(	O
cid:96	O
)	O
)	O
,	O
and	O
a	O
learning	O
algorithm	O
,	O
a	O
,	O
we	O
say	O
that	O
a	O
solves	O
the	O
learning	O
task	O
in	O
time	O
o	O
(	O
f	O
)	O
if	O
there	O
exists	O
some	O
constant	O
number	O
c	O
,	O
such	O
that	O
for	O
every	O
probability	O
distribution	O
d	O
8.2	O
implementing	O
the	O
erm	O
rule	O
103	O
over	O
z	O
,	O
and	O
input	O
	O
,	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
when	O
a	O
has	O
access	O
to	O
samples	O
generated	O
i.i.d	O
.	O
by	O
d	O
,	O
•	O
a	O
terminates	O
after	O
performing	O
at	O
most	O
cf	O
(	O
	O
,	O
δ	O
)	O
operations	O
•	O
the	O
output	O
of	O
a	O
,	O
denoted	O
ha	O
,	O
can	O
be	O
applied	O
to	O
predict	O
the	O
label	B
of	O
a	O
new	O
•	O
the	O
output	O
of	O
a	O
is	O
probably	O
approximately	O
correct	O
;	O
namely	O
,	O
with	O
proba-	O
bility	O
of	O
at	O
least	O
1	O
−	O
δ	O
(	O
over	O
the	O
random	O
samples	O
a	O
receives	O
)	O
,	O
ld	O
(	O
ha	O
)	O
≤	O
minh	O
(	O
cid:48	O
)	O
∈h	O
ld	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
+	O
	O
example	O
while	O
performing	O
at	O
most	O
cf	O
(	O
	O
,	O
δ	O
)	O
operations	O
2.	O
consider	O
a	O
sequence	O
of	O
learning	O
problems	O
,	O
(	O
zn	O
,	O
hn	O
,	O
(	O
cid:96	O
)	O
n	O
)	O
∞	O
n=1	O
,	O
where	O
problem	O
n	O
is	O
deﬁned	O
by	O
a	O
domain	B
zn	O
,	O
a	O
hypothesis	B
class	I
hn	O
,	O
and	O
a	O
loss	B
function	I
(	O
cid:96	O
)	O
n.	O
let	O
a	O
be	O
a	O
learning	O
algorithm	O
designed	O
for	O
solving	O
learning	O
problems	O
of	O
this	O
form	O
.	O
given	O
a	O
function	B
g	O
:	O
n	O
×	O
(	O
0	O
,	O
1	O
)	O
2	O
→	O
n	O
,	O
we	O
say	O
that	O
the	O
runtime	O
of	O
a	O
with	O
respect	O
to	O
the	O
preceding	O
sequence	O
is	O
o	O
(	O
g	O
)	O
,	O
if	O
for	O
all	O
n	O
,	O
a	O
solves	O
the	O
problem	O
(	O
zn	O
,	O
hn	O
,	O
(	O
cid:96	O
)	O
n	O
)	O
in	O
time	O
o	O
(	O
fn	O
)	O
,	O
where	O
fn	O
:	O
(	O
0	O
,	O
1	O
)	O
2	O
→	O
n	O
is	O
deﬁned	O
by	O
fn	O
(	O
	O
,	O
δ	O
)	O
=	O
g	O
(	O
n	O
,	O
	O
,	O
δ	O
)	O
.	O
we	O
say	O
that	O
a	O
is	O
an	O
eﬃcient	O
algorithm	O
with	O
respect	O
to	O
a	O
sequence	O
(	O
zn	O
,	O
hn	O
,	O
(	O
cid:96	O
)	O
n	O
)	O
if	O
its	O
runtime	O
is	O
o	O
(	O
p	O
(	O
n	O
,	O
1/	O
,	O
1/δ	O
)	O
)	O
for	O
some	O
polynomial	O
p.	O
from	O
this	O
deﬁnition	O
we	O
see	O
that	O
the	O
question	O
whether	O
a	O
general	O
learning	O
prob-	O
lem	O
can	O
be	O
solved	O
eﬃciently	O
depends	O
on	O
how	O
it	O
can	O
be	O
broken	O
into	O
a	O
sequence	O
of	O
speciﬁc	O
learning	O
problems	O
.	O
for	O
example	O
,	O
consider	O
the	O
problem	O
of	O
learning	O
a	O
ﬁnite	O
hypothesis	B
class	I
.	O
as	O
we	O
showed	O
in	O
previous	O
chapters	O
,	O
the	O
erm	O
rule	O
over	O
h	O
is	O
guaranteed	O
to	O
(	O
	O
,	O
δ	O
)	O
-learn	O
h	O
if	O
the	O
number	O
of	O
training	O
examples	O
is	O
order	O
of	O
mh	O
(	O
	O
,	O
δ	O
)	O
=	O
log	O
(	O
|h|/δ	O
)	O
/2	O
.	O
assuming	O
that	O
the	O
evaluation	O
of	O
a	O
hypothesis	B
on	O
an	O
example	O
takes	O
a	O
constant	O
time	O
,	O
it	O
is	O
possible	O
to	O
implement	O
the	O
erm	O
rule	O
in	O
time	O
o	O
(	O
|h|	O
mh	O
(	O
	O
,	O
δ	O
)	O
)	O
by	O
performing	O
an	O
exhaustive	O
search	O
over	O
h	O
with	O
a	O
training	B
set	I
of	O
size	O
mh	O
(	O
	O
,	O
δ	O
)	O
.	O
for	O
any	O
ﬁxed	O
ﬁnite	O
h	O
,	O
the	O
exhaustive	O
search	O
algorithm	O
runs	O
in	O
polynomial	O
time	O
.	O
furthermore	O
,	O
if	O
we	O
deﬁne	O
a	O
sequence	O
of	O
problems	O
in	O
which	O
|hn|	O
=	O
n	O
,	O
then	O
the	O
exhaustive	O
search	O
is	O
still	O
considered	O
to	O
be	O
eﬃcient	O
.	O
however	O
,	O
if	O
we	O
deﬁne	O
a	O
sequence	O
of	O
problems	O
for	O
which	O
|hn|	O
=	O
2n	O
,	O
then	O
the	O
sample	O
complex-	O
ity	O
is	O
still	O
polynomial	O
in	O
n	O
but	O
the	O
computational	B
complexity	I
of	O
the	O
exhaustive	O
search	O
algorithm	O
grows	O
exponentially	O
with	O
n	O
(	O
thus	O
,	O
rendered	O
ineﬃcient	O
)	O
.	O
8.2	O
implementing	O
the	O
erm	O
rule	O
given	O
a	O
hypothesis	B
class	I
h	O
,	O
the	O
ermh	O
rule	O
is	O
maybe	O
the	O
most	O
natural	O
learning	O
paradigm	O
.	O
furthermore	O
,	O
for	O
binary	O
classiﬁcation	O
problems	O
we	O
saw	O
that	O
if	O
learning	O
is	O
at	O
all	O
possible	O
,	O
it	O
is	O
possible	O
with	O
the	O
erm	O
rule	O
.	O
in	O
this	O
section	O
we	O
discuss	O
the	O
computational	B
complexity	I
of	O
implementing	O
the	O
erm	O
rule	O
for	O
several	O
hypothesis	B
classes	O
.	O
given	O
a	O
hypothesis	B
class	I
,	O
h	O
,	O
a	O
domain	B
set	O
z	O
,	O
and	O
a	O
loss	B
function	I
(	O
cid:96	O
)	O
,	O
the	O
corre-	O
sponding	O
ermh	O
rule	O
can	O
be	O
deﬁned	O
as	O
follows	O
:	O
104	O
the	O
runtime	O
of	O
learning	O
on	O
a	O
ﬁnite	O
input	O
sample	O
s	O
∈	O
zm	O
output	O
some	O
h	O
∈	O
h	O
that	O
minimizes	O
the	O
empirical	O
loss	O
,	O
ls	O
(	O
h	O
)	O
=	O
1|s|	O
z∈s	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
z	O
)	O
.	O
(	O
cid:80	O
)	O
this	O
section	O
studies	O
the	O
runtime	O
of	O
implementing	O
the	O
erm	O
rule	O
for	O
several	O
examples	O
of	O
learning	O
tasks	O
.	O
8.2.1	O
finite	O
classes	O
limiting	O
the	O
hypothesis	B
class	I
to	O
be	O
a	O
ﬁnite	O
class	O
may	O
be	O
considered	O
as	O
a	O
reason-	O
ably	O
mild	O
restriction	O
.	O
for	O
example	O
,	O
h	O
can	O
be	O
the	O
set	B
of	O
all	O
predictors	O
that	O
can	O
be	O
implemented	O
by	O
a	O
c++	O
program	O
written	O
in	O
at	O
most	O
10000	O
bits	O
of	O
code	O
.	O
other	O
ex-	O
amples	O
of	O
useful	O
ﬁnite	O
classes	O
are	O
any	O
hypothesis	B
class	I
that	O
can	O
be	O
parameterized	O
by	O
a	O
ﬁnite	O
number	O
of	O
parameters	O
,	O
where	O
we	O
are	O
satisﬁed	O
with	O
a	O
representation	O
of	O
each	O
of	O
the	O
parameters	O
using	O
a	O
ﬁnite	O
number	O
of	O
bits	O
,	O
for	O
example	O
,	O
the	O
class	O
of	O
axis	O
aligned	O
rectangles	O
in	O
the	O
euclidean	O
space	O
,	O
rd	O
,	O
when	O
the	O
parameters	O
deﬁning	O
any	O
given	O
rectangle	O
are	O
speciﬁed	O
up	O
to	O
some	O
limited	O
precision	B
.	O
as	O
we	O
have	O
shown	O
in	O
previous	O
chapters	O
,	O
the	O
sample	B
complexity	I
of	O
learning	O
a	O
ﬁnite	O
class	O
is	O
upper	O
bounded	O
by	O
mh	O
(	O
	O
,	O
δ	O
)	O
=	O
c	O
log	O
(	O
c|h|/δ	O
)	O
/c	O
,	O
where	O
c	O
=	O
1	O
in	O
the	O
realizable	O
case	O
and	O
c	O
=	O
2	O
in	O
the	O
nonrealizable	O
case	O
.	O
therefore	O
,	O
the	O
sample	B
complexity	I
has	O
a	O
mild	O
dependence	O
on	O
the	O
size	O
of	O
h.	O
in	O
the	O
example	O
of	O
c++	O
programs	O
mentioned	O
before	O
,	O
the	O
number	O
of	O
hypotheses	O
is	O
210,000	O
but	O
the	O
sample	B
complexity	I
is	O
only	O
c	O
(	O
10	O
,	O
000	O
+	O
log	O
(	O
c/δ	O
)	O
)	O
/c	O
.	O
a	O
straightforward	O
approach	O
for	O
implementing	O
the	O
erm	O
rule	O
over	O
a	O
ﬁnite	O
hy-	O
pothesis	O
class	O
is	O
to	O
perform	O
an	O
exhaustive	O
search	O
.	O
that	O
is	O
,	O
for	O
each	O
h	O
∈	O
h	O
we	O
calculate	O
the	O
empirical	B
risk	I
,	O
ls	O
(	O
h	O
)	O
,	O
and	O
return	O
a	O
hypothesis	B
that	O
minimizes	O
the	O
empirical	B
risk	I
.	O
assuming	O
that	O
the	O
evaluation	O
of	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
z	O
)	O
on	O
a	O
single	O
exam-	O
ple	O
takes	O
a	O
constant	O
amount	O
of	O
time	O
,	O
k	O
,	O
the	O
runtime	O
of	O
this	O
exhaustive	O
search	O
becomes	O
k|h|m	O
,	O
where	O
m	O
is	O
the	O
size	O
of	O
the	O
training	B
set	I
.	O
if	O
we	O
let	O
m	O
to	O
be	O
the	O
upper	O
bound	O
on	O
the	O
sample	B
complexity	I
mentioned	O
,	O
then	O
the	O
runtime	O
becomes	O
k|h|c	O
log	O
(	O
c|h|/δ	O
)	O
/c	O
.	O
the	O
linear	O
dependence	O
of	O
the	O
runtime	O
on	O
the	O
size	O
of	O
h	O
makes	O
this	O
approach	O
ineﬃcient	O
(	O
and	O
unrealistic	O
)	O
for	O
large	O
classes	O
.	O
formally	O
,	O
if	O
we	O
deﬁne	O
a	O
sequence	O
of	O
n=1	O
such	O
that	O
log	O
(	O
|hn|	O
)	O
=	O
n	O
,	O
then	O
the	O
exhaustive	O
search	O
problems	O
(	O
zn	O
,	O
hn	O
,	O
(	O
cid:96	O
)	O
n	O
)	O
∞	O
approach	O
yields	O
an	O
exponential	O
runtime	O
.	O
in	O
the	O
example	O
of	O
c++	O
programs	O
,	O
if	O
hn	O
is	O
the	O
set	B
of	O
functions	O
that	O
can	O
be	O
implemented	O
by	O
a	O
c++	O
program	O
written	O
in	O
at	O
most	O
n	O
bits	O
of	O
code	O
,	O
then	O
the	O
runtime	O
grows	O
exponentially	O
with	O
n	O
,	O
implying	O
that	O
the	O
exhaustive	O
search	O
approach	O
is	O
unrealistic	O
for	O
practical	O
use	O
.	O
in	O
fact	O
,	O
this	O
problem	O
is	O
one	O
of	O
the	O
reasons	O
we	O
are	O
dealing	O
with	O
other	O
hypothesis	B
classes	O
,	O
like	O
classes	O
of	O
linear	B
predictors	I
,	O
which	O
we	O
will	O
encounter	O
in	O
the	O
next	O
chapter	O
,	O
and	O
not	O
just	O
focusing	O
on	O
ﬁnite	O
classes	O
.	O
it	O
is	O
important	O
to	O
realize	O
that	O
the	O
ineﬃciency	O
of	O
one	O
algorithmic	O
approach	O
(	O
such	O
as	O
the	O
exhaustive	O
search	O
)	O
does	O
not	O
yet	O
imply	O
that	O
no	O
eﬃcient	O
erm	O
imple-	O
mentation	O
exists	O
.	O
indeed	O
,	O
we	O
will	O
show	O
examples	O
in	O
which	O
the	O
erm	O
rule	O
can	O
be	O
implemented	O
eﬃciently	O
.	O
8.2	O
implementing	O
the	O
erm	O
rule	O
105	O
8.2.2	O
axis	O
aligned	O
rectangles	O
let	O
hn	O
be	O
the	O
class	O
of	O
axis	O
aligned	O
rectangles	O
in	O
rn	O
,	O
namely	O
,	O
hn	O
=	O
{	O
h	O
(	O
a1	O
,	O
...	O
,	O
an	O
,	O
b1	O
,	O
...	O
,	O
bn	O
)	O
:	O
∀i	O
,	O
ai	O
≤	O
bi	O
}	O
where	O
h	O
(	O
a1	O
,	O
...	O
,	O
an	O
,	O
b1	O
,	O
...	O
,	O
bn	O
)	O
(	O
x	O
,	O
y	O
)	O
=	O
if	O
∀i	O
,	O
xi	O
∈	O
[	O
ai	O
,	O
bi	O
]	O
otherwise	O
(	O
8.1	O
)	O
(	O
cid:40	O
)	O
1	O
0	O
eﬃciently	O
learnable	O
in	O
the	O
realizable	O
case	O
consider	O
implementing	O
the	O
erm	O
rule	O
in	O
the	O
realizable	O
case	O
.	O
that	O
is	O
,	O
we	O
are	O
given	O
a	O
training	B
set	I
s	O
=	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
of	O
examples	O
,	O
such	O
that	O
there	O
exists	O
an	O
axis	O
aligned	O
rectangle	O
,	O
h	O
∈	O
hn	O
,	O
for	O
which	O
h	O
(	O
xi	O
)	O
=	O
yi	O
for	O
all	O
i.	O
our	O
goal	O
is	O
to	O
ﬁnd	O
such	O
an	O
axis	O
aligned	O
rectangle	O
with	O
a	O
zero	O
training	B
error	I
,	O
namely	O
,	O
a	O
rectangle	O
that	O
is	O
consistent	O
with	O
all	O
the	O
labels	O
in	O
s.	O
we	O
show	O
later	O
that	O
this	O
can	O
be	O
done	O
in	O
time	O
o	O
(	O
nm	O
)	O
.	O
indeed	O
,	O
for	O
each	O
i	O
∈	O
[	O
n	O
]	O
,	O
set	B
ai	O
=	O
min	O
{	O
xi	O
:	O
(	O
x	O
,	O
1	O
)	O
∈	O
s	O
}	O
and	O
bi	O
=	O
max	O
{	O
xi	O
:	O
(	O
x	O
,	O
1	O
)	O
∈	O
s	O
}	O
.	O
in	O
words	O
,	O
we	O
take	O
ai	O
to	O
be	O
the	O
minimal	O
value	O
of	O
the	O
i	O
’	O
th	O
coordinate	O
of	O
a	O
positive	O
example	O
in	O
s	O
and	O
bi	O
to	O
be	O
the	O
maximal	O
value	O
of	O
the	O
i	O
’	O
th	O
coordinate	O
of	O
a	O
positive	O
example	O
in	O
s.	O
it	O
is	O
easy	O
to	O
verify	O
that	O
the	O
resulting	O
rectangle	O
has	O
zero	O
training	B
error	I
and	O
that	O
the	O
runtime	O
of	O
ﬁnding	O
each	O
ai	O
and	O
bi	O
is	O
o	O
(	O
m	O
)	O
.	O
hence	O
,	O
the	O
total	O
runtime	O
of	O
this	O
procedure	O
is	O
o	O
(	O
nm	O
)	O
.	O
not	O
eﬃciently	O
learnable	O
in	O
the	O
agnostic	O
case	O
in	O
the	O
agnostic	O
case	O
,	O
we	O
do	O
not	O
assume	O
that	O
some	O
hypothesis	B
h	O
perfectly	O
predicts	O
the	O
labels	O
of	O
all	O
the	O
examples	O
in	O
the	O
training	B
set	I
.	O
our	O
goal	O
is	O
therefore	O
to	O
ﬁnd	O
h	O
that	O
minimizes	O
the	O
number	O
of	O
examples	O
for	O
which	O
yi	O
(	O
cid:54	O
)	O
=	O
h	O
(	O
xi	O
)	O
.	O
it	O
turns	O
out	O
that	O
for	O
many	O
common	O
hypothesis	B
classes	O
,	O
including	O
the	O
classes	O
of	O
axis	O
aligned	O
rectangles	O
we	O
consider	O
here	O
,	O
solving	O
the	O
erm	O
problem	O
in	O
the	O
agnostic	O
setting	O
is	O
np-hard	O
(	O
and	O
,	O
in	O
most	O
cases	O
,	O
it	O
is	O
even	O
np-hard	O
to	O
ﬁnd	O
some	O
h	O
∈	O
h	O
whose	O
error	O
is	O
no	O
more	O
than	O
some	O
constant	O
c	O
>	O
1	O
times	O
that	O
of	O
the	O
empirical	B
risk	I
minimizer	O
in	O
h	O
)	O
.	O
that	O
is	O
,	O
unless	O
p	O
=	O
np	O
,	O
there	O
is	O
no	O
algorithm	O
whose	O
running	O
time	O
is	O
polynomial	O
in	O
m	O
and	O
n	O
that	O
is	O
guaranteed	O
to	O
ﬁnd	O
an	O
erm	O
hypothesis	B
for	O
these	O
problems	O
(	O
ben-david	O
,	O
eiron	O
&	O
long	O
2003	O
)	O
.	O
on	O
the	O
other	O
hand	O
,	O
it	O
is	O
worthwhile	O
noticing	O
that	O
,	O
if	O
we	O
ﬁx	O
one	O
speciﬁc	O
hypoth-	O
esis	O
class	O
,	O
say	O
,	O
axis	O
aligned	O
rectangles	O
in	O
some	O
ﬁxed	O
dimension	B
,	O
n	O
,	O
then	O
there	O
exist	O
eﬃcient	O
learning	O
algorithms	O
for	O
this	O
class	O
.	O
in	O
other	O
words	O
,	O
there	O
are	O
successful	O
agnostic	O
pac	O
learners	O
that	O
run	O
in	O
time	O
polynomial	O
in	O
1/	O
and	O
1/δ	O
(	O
but	O
their	O
dependence	O
on	O
the	O
dimension	B
n	O
is	O
not	O
polynomial	O
)	O
.	O
to	O
see	O
this	O
,	O
recall	B
the	O
implementation	O
of	O
the	O
erm	O
rule	O
we	O
presented	O
for	O
the	O
realizable	O
case	O
,	O
from	O
which	O
it	O
follows	O
that	O
an	O
axis	O
aligned	O
rectangle	O
is	O
determined	O
by	O
at	O
most	O
2n	O
examples	O
.	O
therefore	O
,	O
given	O
a	O
training	B
set	I
of	O
size	O
m	O
,	O
we	O
can	O
per-	O
form	O
an	O
exhaustive	O
search	O
over	O
all	O
subsets	O
of	O
the	O
training	B
set	I
of	O
size	O
at	O
most	O
2n	O
examples	O
and	O
construct	O
a	O
rectangle	O
from	O
each	O
such	O
subset	O
.	O
then	O
,	O
we	O
can	O
pick	O
106	O
the	O
runtime	O
of	O
learning	O
the	O
rectangle	O
with	O
the	O
minimal	O
training	B
error	I
.	O
this	O
procedure	O
is	O
guaranteed	O
to	O
ﬁnd	O
an	O
erm	O
hypothesis	B
,	O
and	O
the	O
runtime	O
of	O
the	O
procedure	O
is	O
mo	O
(	O
n	O
)	O
.	O
it	O
follows	O
that	O
if	O
n	O
is	O
ﬁxed	O
,	O
the	O
runtime	O
is	O
polynomial	O
in	O
the	O
sample	O
size	O
.	O
this	O
does	O
not	O
contradict	O
the	O
aforementioned	O
hardness	O
result	O
,	O
since	O
there	O
we	O
argued	O
that	O
unless	O
p=np	O
one	O
can	O
not	O
have	O
an	O
algorithm	O
whose	O
dependence	O
on	O
the	O
dimension	B
n	O
is	O
polynomial	O
as	O
well	O
.	O
8.2.3	O
boolean	B
conjunctions	I
a	O
boolean	O
conjunction	O
is	O
a	O
mapping	O
from	O
x	O
=	O
{	O
0	O
,	O
1	O
}	O
n	O
to	O
y	O
=	O
{	O
0	O
,	O
1	O
}	O
that	O
can	O
be	O
expressed	O
as	O
a	O
proposition	O
formula	O
of	O
the	O
form	O
xi1	O
∧	O
.	O
.	O
.∧	O
xik	O
∧¬xj1	O
∧	O
.	O
.	O
.∧¬xjr	O
,	O
for	O
some	O
indices	O
i1	O
,	O
.	O
.	O
.	O
,	O
ik	O
,	O
j1	O
,	O
.	O
.	O
.	O
,	O
jr	O
∈	O
[	O
n	O
]	O
.	O
the	O
function	B
that	O
such	O
a	O
proposition	O
formula	O
deﬁnes	O
is	O
(	O
cid:40	O
)	O
h	O
(	O
x	O
)	O
=	O
if	O
xi1	O
=	O
···	O
=	O
xik	O
=	O
1	O
and	O
xj1	O
=	O
···	O
=	O
xjr	O
=	O
0	O
otherwise	O
1	O
0	O
let	O
hn	O
c	O
be	O
the	O
class	O
of	O
all	O
boolean	B
conjunctions	I
over	O
{	O
0	O
,	O
1	O
}	O
n.	O
the	O
size	O
of	O
hn	O
c	O
is	O
at	O
most	O
3n	O
+	O
1	O
(	O
since	O
in	O
a	O
conjunction	O
formula	O
,	O
each	O
element	O
of	O
x	O
either	O
appears	O
,	O
or	O
appears	O
with	O
a	O
negation	O
sign	O
,	O
or	O
does	O
not	O
appear	O
at	O
all	O
,	O
and	O
we	O
also	O
have	O
the	O
all	O
negative	O
formula	O
)	O
.	O
hence	O
,	O
the	O
sample	B
complexity	I
of	O
learning	O
hn	O
c	O
using	O
the	O
erm	O
rule	O
is	O
at	O
most	O
n	O
log	O
(	O
3/δ	O
)	O
/	O
.	O
eﬃciently	O
learnable	O
in	O
the	O
realizable	O
case	O
next	O
,	O
we	O
show	O
that	O
it	O
is	O
possible	O
to	O
solve	O
the	O
erm	O
problem	O
for	O
hn	O
c	O
in	O
time	O
polynomial	O
in	O
n	O
and	O
m.	O
the	O
idea	O
is	O
to	O
deﬁne	O
an	O
erm	O
conjunction	O
by	O
including	O
in	O
the	O
hypothesis	B
conjunction	O
all	O
the	O
literals	O
that	O
do	O
not	O
contradict	O
any	O
positively	O
labeled	O
example	O
.	O
let	O
v1	O
,	O
.	O
.	O
.	O
,	O
vm+	O
be	O
all	O
the	O
positively	O
labeled	O
instances	O
in	O
the	O
input	O
sample	O
s.	O
we	O
deﬁne	O
,	O
by	O
induction	O
on	O
i	O
≤	O
m+	O
,	O
a	O
sequence	O
of	O
hypotheses	O
(	O
or	O
conjunctions	O
)	O
.	O
let	O
h0	O
be	O
the	O
conjunction	O
of	O
all	O
possible	O
literals	O
.	O
that	O
is	O
,	O
h0	O
=	O
x1	O
∧	O
¬x1	O
∧	O
x2	O
∧	O
.	O
.	O
.	O
∧	O
xn	O
∧	O
¬xn	O
.	O
note	O
that	O
h0	O
assigns	O
the	O
label	B
0	O
to	O
all	O
the	O
elements	O
of	O
x	O
.	O
we	O
obtain	O
hi+1	O
by	O
deleting	O
from	O
the	O
conjunction	O
hi	O
all	O
the	O
literals	O
that	O
are	O
not	O
satisﬁed	O
by	O
vi+1	O
.	O
the	O
algorithm	O
outputs	O
the	O
hypothesis	B
hm+	O
.	O
note	O
that	O
hm+	O
labels	O
positively	O
all	O
the	O
positively	O
labeled	O
examples	O
in	O
s.	O
furthermore	O
,	O
for	O
every	O
i	O
≤	O
m+	O
,	O
hi	O
is	O
the	O
most	O
restrictive	O
conjunction	O
that	O
labels	O
v1	O
,	O
.	O
.	O
.	O
,	O
vi	O
positively	O
.	O
now	O
,	O
since	O
we	O
consider	O
learning	O
in	O
the	O
realizable	O
setup	O
,	O
there	O
exists	O
a	O
conjunction	O
hypothesis	B
,	O
f	O
∈	O
hn	O
c	O
,	O
that	O
is	O
consistent	O
with	O
all	O
the	O
examples	O
in	O
s.	O
since	O
hm+	O
is	O
the	O
most	O
restrictive	O
conjunction	O
that	O
labels	O
positively	O
all	O
the	O
positively	O
labeled	O
members	O
of	O
s	O
,	O
any	O
instance	B
labeled	O
0	O
by	O
f	O
is	O
also	O
labeled	O
0	O
by	O
hm+	O
.	O
it	O
follows	O
that	O
hm+	O
has	O
zero	O
training	B
error	I
(	O
w.r.t	O
.	O
s	O
)	O
,	O
and	O
is	O
therefore	O
a	O
legal	O
erm	O
hypothesis	B
.	O
note	O
that	O
the	O
running	O
time	O
of	O
this	O
algorithm	O
is	O
o	O
(	O
mn	O
)	O
.	O
8.3	O
eﬃciently	O
learnable	O
,	O
but	O
not	O
by	O
a	O
proper	B
erm	O
107	O
not	O
eﬃciently	O
learnable	O
in	O
the	O
agnostic	O
case	O
as	O
in	O
the	O
case	O
of	O
axis	O
aligned	O
rectangles	O
,	O
unless	O
p	O
=	O
np	O
,	O
there	O
is	O
no	O
algorithm	O
whose	O
running	O
time	O
is	O
polynomial	O
in	O
m	O
and	O
n	O
that	O
guaranteed	O
to	O
ﬁnd	O
an	O
erm	O
hypothesis	B
for	O
the	O
class	O
of	O
boolean	B
conjunctions	I
in	O
the	O
unrealizable	O
case	O
.	O
8.2.4	O
learning	O
3-term	O
dnf	O
we	O
next	O
show	O
that	O
a	O
slight	O
generalization	O
of	O
the	O
class	O
of	O
boolean	B
conjunctions	I
leads	O
to	O
intractability	O
of	O
solving	O
the	O
erm	O
problem	O
even	O
in	O
the	O
realizable	O
case	O
.	O
consider	O
the	O
class	O
of	O
3-term	O
disjunctive	O
normal	O
form	O
formulae	O
(	O
3-term	O
dnf	O
)	O
.	O
the	O
instance	B
space	I
is	O
x	O
=	O
{	O
0	O
,	O
1	O
}	O
n	O
and	O
each	O
hypothesis	B
is	O
represented	O
by	O
the	O
boolean	O
formula	O
of	O
the	O
form	O
h	O
(	O
x	O
)	O
=	O
a1	O
(	O
x	O
)	O
∨	O
a2	O
(	O
x	O
)	O
∨	O
a3	O
(	O
x	O
)	O
,	O
where	O
each	O
ai	O
(	O
x	O
)	O
is	O
a	O
boolean	O
conjunction	O
(	O
as	O
deﬁned	O
in	O
the	O
previous	O
section	O
)	O
.	O
the	O
output	O
of	O
h	O
(	O
x	O
)	O
is	O
1	O
if	O
either	O
a1	O
(	O
x	O
)	O
or	O
a2	O
(	O
x	O
)	O
or	O
a3	O
(	O
x	O
)	O
outputs	O
the	O
label	B
1.	O
if	O
all	O
three	O
conjunctions	O
output	O
the	O
label	B
0	O
then	O
h	O
(	O
x	O
)	O
=	O
0.	O
of	O
hn	O
the	O
erm	O
rule	O
is	O
at	O
most	O
3n	O
log	O
(	O
3/δ	O
)	O
/	O
.	O
let	O
hn	O
3dnf	O
is	O
at	O
most	O
33n	O
.	O
hence	O
,	O
the	O
sample	B
complexity	I
of	O
learning	O
hn	O
3dnf	O
be	O
the	O
hypothesis	B
class	I
of	O
all	O
such	O
3-term	O
dnf	O
formulae	O
.	O
the	O
size	O
3dnf	O
using	O
however	O
,	O
from	O
the	O
computational	O
perspective	O
,	O
this	O
learning	O
problem	O
is	O
hard	O
.	O
it	O
has	O
been	O
shown	O
(	O
see	O
(	O
pitt	O
&	O
valiant	O
1988	O
,	O
kearns	O
et	O
al	O
.	O
1994	O
)	O
)	O
that	O
unless	O
rp	O
=	O
np	O
,	O
there	O
is	O
no	O
polynomial	O
time	O
algorithm	O
that	O
properly	O
learns	O
a	O
sequence	O
of	O
3-term	O
dnf	O
learning	O
problems	O
in	O
which	O
the	O
dimension	B
of	O
the	O
n	O
’	O
th	O
problem	O
is	O
n.	O
by	O
“	O
properly	O
”	O
we	O
mean	O
that	O
the	O
algorithm	O
should	O
output	O
a	O
hypothesis	B
that	O
is	O
a	O
3-term	O
dnf	O
formula	O
.	O
in	O
particular	O
,	O
since	O
ermhn	O
outputs	O
a	O
3-term	O
dnf	O
formula	O
it	O
is	O
a	O
proper	B
learner	O
and	O
therefore	O
it	O
is	O
hard	O
to	O
implement	O
it	O
.	O
the	O
proof	O
uses	O
a	O
reduction	O
of	O
the	O
graph	O
3-coloring	O
problem	O
to	O
the	O
problem	O
of	O
pac	O
learning	O
3-term	O
dnf	O
.	O
the	O
detailed	O
technique	O
is	O
given	O
in	O
exercise	O
3.	O
see	O
also	O
(	O
kearns	O
&	O
vazirani	O
1994	O
,	O
section	O
1.4	O
)	O
.	O
3dn	O
f	O
8.3	O
eﬃciently	O
learnable	O
,	O
but	O
not	O
by	O
a	O
proper	B
erm	O
in	O
the	O
previous	O
section	O
we	O
saw	O
that	O
it	O
is	O
impossible	O
to	O
implement	O
the	O
erm	O
rule	O
eﬃciently	O
for	O
the	O
class	O
hn	O
3dnf	O
of	O
3-dnf	O
formulae	O
.	O
in	O
this	O
section	O
we	O
show	O
that	O
it	O
is	O
possible	O
to	O
learn	O
this	O
class	O
eﬃciently	O
,	O
but	O
using	O
erm	O
with	O
respect	O
to	O
a	O
larger	O
class	O
.	O
representation	B
independent	I
learning	O
is	O
not	O
hard	O
next	O
we	O
show	O
that	O
it	O
is	O
possible	O
to	O
learn	O
3-term	O
dnf	O
formulae	O
eﬃciently	O
.	O
there	O
is	O
no	O
contradiction	O
to	O
the	O
hardness	O
result	O
mentioned	O
in	O
the	O
previous	O
section	O
as	O
we	O
now	O
allow	O
“	O
representation	B
independent	I
”	O
learning	O
.	O
that	O
is	O
,	O
we	O
allow	O
the	O
learning	O
algorithm	O
to	O
output	O
a	O
hypothesis	B
that	O
is	O
not	O
a	O
3-term	O
dnf	O
formula	O
.	O
the	O
ba-	O
sic	O
idea	O
is	O
to	O
replace	O
the	O
original	O
hypothesis	B
class	I
of	O
3-term	O
dnf	O
formula	O
with	O
a	O
larger	O
hypothesis	B
class	I
so	O
that	O
the	O
new	O
class	O
is	O
easily	O
learnable	O
.	O
the	O
learning	O
108	O
the	O
runtime	O
of	O
learning	O
algorithm	O
might	O
return	O
a	O
hypothesis	B
that	O
does	O
not	O
belong	O
to	O
the	O
original	O
hypoth-	O
esis	O
class	O
;	O
hence	O
the	O
name	O
“	O
representation	B
independent	I
”	O
learning	O
.	O
we	O
emphasize	O
that	O
in	O
most	O
situations	O
,	O
returning	O
a	O
hypothesis	B
with	O
good	O
predictive	O
ability	O
is	O
what	O
we	O
are	O
really	O
interested	O
in	O
doing	O
.	O
we	O
start	O
by	O
noting	O
that	O
because	O
∨	O
distributes	O
over	O
∧	O
,	O
each	O
3-term	O
dnf	O
formula	O
can	O
be	O
rewritten	O
as	O
a1	O
∨	O
a2	O
∨	O
a3	O
=	O
(	O
cid:94	O
)	O
(	O
u	O
∨	O
v	O
∨	O
w	O
)	O
u∈a1	O
,	O
v∈a2	O
,	O
w∈a3	O
next	O
,	O
let	O
us	O
deﬁne	O
:	O
ψ	O
:	O
{	O
0	O
,	O
1	O
}	O
n	O
→	O
{	O
0	O
,	O
1	O
}	O
(	O
2n	O
)	O
3	O
such	O
that	O
for	O
each	O
triplet	O
of	O
literals	O
u	O
,	O
v	O
,	O
w	O
there	O
is	O
a	O
variable	O
in	O
the	O
range	O
of	O
ψ	O
indicating	O
if	O
u∨	O
v	O
∨	O
w	O
is	O
true	O
or	O
false	O
.	O
so	O
,	O
for	O
each	O
3-dnf	O
formula	O
over	O
{	O
0	O
,	O
1	O
}	O
n	O
there	O
is	O
a	O
conjunction	O
over	O
{	O
0	O
,	O
1	O
}	O
(	O
2n	O
)	O
3	O
,	O
with	O
the	O
same	O
truth	O
table	O
.	O
since	O
we	O
assume	O
that	O
the	O
data	O
is	O
realizable	O
,	O
we	O
can	O
solve	O
the	O
erm	O
problem	O
with	O
respect	O
to	O
the	O
class	O
of	O
conjunctions	O
over	O
{	O
0	O
,	O
1	O
}	O
(	O
2n	O
)	O
3	O
.	O
furthermore	O
,	O
the	O
sample	B
complexity	I
of	O
learning	O
the	O
class	O
of	O
conjunctions	O
in	O
the	O
higher	O
dimensional	O
space	O
is	O
at	O
most	O
n3	O
log	O
(	O
1/δ	O
)	O
/	O
.	O
thus	O
,	O
the	O
overall	O
runtime	O
of	O
this	O
approach	O
is	O
polynomial	O
in	O
n.	O
intuitively	O
,	O
the	O
idea	O
is	O
as	O
follows	O
.	O
we	O
started	O
with	O
a	O
hypothesis	B
class	I
for	O
which	O
learning	O
is	O
hard	O
.	O
we	O
switched	O
to	O
another	O
representation	O
where	O
the	O
hypothesis	B
class	I
is	O
larger	O
than	O
the	O
original	O
class	O
but	O
has	O
more	O
structure	O
,	O
which	O
allows	O
for	O
a	O
more	O
eﬃcient	O
erm	O
search	O
.	O
in	O
the	O
new	O
representation	O
,	O
solving	O
the	O
erm	O
problem	O
is	O
easy	O
.	O
(	O
2	O
n	O
)	O
3	O
0	O
,	O
1	O
}	O
{	O
o	O
v	O
e	O
r	O
i	O
o	O
n	O
s	O
c	O
o	O
n	O
j	O
u	O
n	O
c	O
t	O
3-term-dnf	O
formulae	O
over	O
{	O
0	O
,	O
1	O
}	O
n	O
8.4	O
hardness	O
of	O
learning*	O
we	O
have	O
just	O
demonstrated	O
that	O
the	O
computational	O
hardness	O
of	O
implementing	O
ermh	O
does	O
not	O
imply	O
that	O
such	O
a	O
class	O
h	O
is	O
not	O
learnable	O
.	O
how	O
can	O
we	O
prove	O
that	O
a	O
learning	O
problem	O
is	O
computationally	O
hard	O
?	O
one	O
approach	O
is	O
to	O
rely	O
on	O
cryptographic	O
assumptions	O
.	O
in	O
some	O
sense	O
,	O
cryp-	O
tography	O
is	O
the	O
opposite	O
of	O
learning	O
.	O
in	O
learning	O
we	O
try	O
to	O
uncover	O
some	O
rule	O
underlying	O
the	O
examples	O
we	O
see	O
,	O
whereas	O
in	O
cryptography	O
,	O
the	O
goal	O
is	O
to	O
make	O
sure	O
that	O
nobody	O
will	O
be	O
able	O
to	O
discover	O
some	O
secret	O
,	O
in	O
spite	O
of	O
having	O
access	O
8.4	O
hardness	O
of	O
learning*	O
109	O
to	O
some	O
partial	O
information	O
about	O
it	O
.	O
on	O
that	O
high	O
level	O
intuitive	O
sense	O
,	O
results	O
about	O
the	O
cryptographic	O
security	O
of	O
some	O
system	O
translate	O
into	O
results	O
about	O
the	O
unlearnability	O
of	O
some	O
corresponding	O
task	O
.	O
regrettably	O
,	O
currently	O
one	O
has	O
no	O
way	O
of	O
proving	O
that	O
a	O
cryptographic	O
protocol	O
is	O
not	O
breakable	O
.	O
even	O
the	O
common	O
assumption	O
of	O
p	O
(	O
cid:54	O
)	O
=	O
np	O
does	O
not	O
suﬃce	O
for	O
that	O
(	O
although	O
it	O
can	O
be	O
shown	O
to	O
be	O
necessary	O
for	O
most	O
common	O
cryptographic	O
scenarios	O
)	O
.	O
the	O
common	O
approach	O
for	O
proving	O
that	O
cryptographic	O
protocols	O
are	O
secure	O
is	O
to	O
start	O
with	O
some	O
cryp-	O
tographic	O
assumptions	O
.	O
the	O
more	O
these	O
are	O
used	O
as	O
a	O
basis	O
for	O
cryptography	O
,	O
the	O
stronger	O
is	O
our	O
belief	O
that	O
they	O
really	O
hold	O
(	O
or	O
,	O
at	O
least	O
,	O
that	O
algorithms	O
that	O
will	O
refute	O
them	O
are	O
hard	O
to	O
come	O
by	O
)	O
.	O
we	O
now	O
brieﬂy	O
describe	O
the	O
basic	O
idea	O
of	O
how	O
to	O
deduce	O
hardness	O
of	O
learnabil-	O
ity	O
from	O
cryptographic	O
assumptions	O
.	O
many	O
cryptographic	O
systems	O
rely	O
on	O
the	O
assumption	O
that	O
there	O
exists	O
a	O
one	O
way	O
function	B
.	O
roughly	O
speaking	O
,	O
a	O
one	O
way	O
function	B
is	O
a	O
function	B
f	O
:	O
{	O
0	O
,	O
1	O
}	O
n	O
→	O
{	O
0	O
,	O
1	O
}	O
n	O
(	O
more	O
formally	O
,	O
it	O
is	O
a	O
sequence	O
of	O
functions	O
,	O
one	O
for	O
each	O
dimension	B
n	O
)	O
that	O
is	O
easy	O
to	O
compute	O
but	O
is	O
hard	O
to	O
in-	O
vert	O
.	O
more	O
formally	O
,	O
f	O
can	O
be	O
computed	O
in	O
time	O
poly	O
(	O
n	O
)	O
but	O
for	O
any	O
randomized	O
polynomial	O
time	O
algorithm	O
a	O
,	O
and	O
for	O
every	O
polynomial	O
p	O
(	O
·	O
)	O
,	O
p	O
[	O
f	O
(	O
a	O
(	O
f	O
(	O
x	O
)	O
)	O
)	O
=	O
f	O
(	O
x	O
)	O
]	O
<	O
1	O
p	O
(	O
n	O
)	O
,	O
where	O
the	O
probability	O
is	O
taken	O
over	O
a	O
random	O
choice	O
of	O
x	O
according	O
to	O
the	O
uniform	O
distribution	O
over	O
{	O
0	O
,	O
1	O
}	O
n	O
and	O
the	O
randomness	O
of	O
a.	O
a	O
one	O
way	O
function	B
,	O
f	O
,	O
is	O
called	O
trapdoor	O
one	O
way	O
function	B
if	O
,	O
for	O
some	O
poly-	O
nomial	O
function	B
p	O
,	O
for	O
every	O
n	O
there	O
exists	O
a	O
bit-string	O
sn	O
(	O
called	O
a	O
secret	O
key	O
)	O
of	O
length	O
≤	O
p	O
(	O
n	O
)	O
,	O
such	O
that	O
there	O
is	O
a	O
polynomial	O
time	O
algorithm	O
that	O
,	O
for	O
every	O
n	O
and	O
every	O
x	O
∈	O
{	O
0	O
,	O
1	O
}	O
n	O
,	O
on	O
input	O
(	O
f	O
(	O
x	O
)	O
,	O
sn	O
)	O
outputs	O
x.	O
in	O
other	O
words	O
,	O
although	O
f	O
is	O
hard	O
to	O
invert	O
,	O
once	O
one	O
has	O
access	O
to	O
its	O
secret	O
key	O
,	O
inverting	O
f	O
becomes	O
feasible	B
.	O
such	O
functions	O
are	O
parameterized	O
by	O
their	O
secret	O
key	O
.	O
now	O
,	O
let	O
fn	O
be	O
a	O
family	O
of	O
trapdoor	O
functions	O
over	O
{	O
0	O
,	O
1	O
}	O
n	O
that	O
can	O
be	O
calcu-	O
lated	O
by	O
some	O
polynomial	O
time	O
algorithm	O
.	O
that	O
is	O
,	O
we	O
ﬁx	O
an	O
algorithm	O
that	O
given	O
a	O
secret	O
key	O
(	O
representing	O
one	O
function	B
in	O
fn	O
)	O
and	O
an	O
input	O
vector	O
,	O
it	O
calculates	O
the	O
value	O
of	O
the	O
function	B
corresponding	O
to	O
the	O
secret	O
key	O
on	O
the	O
input	O
vector	O
in	O
polynomial	O
time	O
.	O
consider	O
the	O
task	O
of	O
learning	O
the	O
class	O
of	O
the	O
corresponding	O
f	O
=	O
{	O
f−1	O
:	O
f	O
∈	O
fn	O
}	O
.	O
since	O
each	O
function	B
in	O
this	O
class	O
can	O
be	O
inverted	O
inverses	O
,	O
h	O
n	O
by	O
some	O
secret	O
key	O
sn	O
of	O
size	O
polynomial	O
in	O
n	O
,	O
the	O
class	O
h	O
n	O
f	O
can	O
be	O
parameter-	O
ized	O
by	O
these	O
keys	O
and	O
its	O
size	O
is	O
at	O
most	O
2p	O
(	O
n	O
)	O
.	O
its	O
sample	B
complexity	I
is	O
therefore	O
polynomial	O
in	O
n.	O
we	O
claim	O
that	O
there	O
can	O
be	O
no	O
eﬃcient	O
learner	O
for	O
this	O
class	O
.	O
if	O
there	O
were	O
such	O
a	O
learner	O
,	O
l	O
,	O
then	O
by	O
sampling	O
uniformly	O
at	O
random	O
a	O
polynomial	O
number	O
of	O
strings	O
in	O
{	O
0	O
,	O
1	O
}	O
n	O
,	O
and	O
computing	O
f	O
over	O
them	O
,	O
we	O
could	O
generate	O
a	O
labeled	O
training	O
sample	O
of	O
pairs	O
(	O
f	O
(	O
x	O
)	O
,	O
x	O
)	O
,	O
which	O
should	O
suﬃce	O
for	O
our	O
learner	O
to	O
ﬁgure	O
out	O
an	O
(	O
	O
,	O
δ	O
)	O
approximation	O
of	O
f−1	O
(	O
w.r.t	O
.	O
the	O
uniform	O
distribution	O
over	O
the	O
range	O
of	O
f	O
)	O
,	O
which	O
would	O
violate	O
the	O
one	O
way	O
property	O
of	O
f	O
.	O
a	O
more	O
detailed	O
treatment	O
,	O
as	O
well	O
as	O
a	O
concrete	O
example	O
,	O
can	O
be	O
found	O
in	O
(	O
kearns	O
&	O
vazirani	O
1994	O
,	O
chapter	O
6	O
)	O
.	O
using	O
reductions	B
,	O
they	O
also	O
show	O
that	O
110	O
the	O
runtime	O
of	O
learning	O
the	O
class	O
of	O
functions	O
that	O
can	O
be	O
calculated	O
by	O
small	O
boolean	O
circuits	O
is	O
not	O
eﬃciently	O
learnable	O
,	O
even	O
in	O
the	O
realizable	O
case	O
.	O
8.5	O
summary	O
the	O
runtime	O
of	O
learning	O
algorithms	O
is	O
asymptotically	O
analyzed	O
as	O
a	O
function	B
of	O
diﬀerent	O
parameters	O
of	O
the	O
learning	O
problem	O
,	O
such	O
as	O
the	O
size	O
of	O
the	O
hypothe-	O
sis	O
class	O
,	O
our	O
measure	O
of	O
accuracy	B
,	O
our	O
measure	O
of	O
conﬁdence	B
,	O
or	O
the	O
size	O
of	O
the	O
domain	B
set	O
.	O
we	O
have	O
demonstrated	O
cases	O
in	O
which	O
the	O
erm	O
rule	O
can	O
be	O
imple-	O
mented	O
eﬃciently	O
.	O
for	O
example	O
,	O
we	O
derived	O
eﬃcient	O
algorithms	O
for	O
solving	O
the	O
erm	O
problem	O
for	O
the	O
class	O
of	O
boolean	B
conjunctions	I
and	O
the	O
class	O
of	O
axis	O
aligned	O
rectangles	O
,	O
under	O
the	O
realizability	B
assumption	O
.	O
however	O
,	O
implementing	O
erm	O
for	O
these	O
classes	O
in	O
the	O
agnostic	O
case	O
is	O
np-hard	O
.	O
recall	B
that	O
from	O
the	O
statistical	O
perspective	O
,	O
there	O
is	O
no	O
diﬀerence	O
between	O
the	O
realizable	O
and	O
agnostic	O
cases	O
(	O
i.e.	O
,	O
a	O
class	O
is	O
learnable	O
in	O
both	O
cases	O
if	O
and	O
only	O
if	O
it	O
has	O
a	O
ﬁnite	O
vc-dimension	O
)	O
.	O
in	O
contrast	O
,	O
as	O
we	O
saw	O
,	O
from	O
the	O
computational	O
perspective	O
the	O
diﬀerence	O
is	O
im-	O
mense	O
.	O
we	O
have	O
also	O
shown	O
another	O
example	O
,	O
the	O
class	O
of	O
3-term	O
dnf	O
,	O
where	O
implementing	O
erm	O
is	O
hard	O
even	O
in	O
the	O
realizable	O
case	O
,	O
yet	O
the	O
class	O
is	O
eﬃciently	O
learnable	O
by	O
another	O
algorithm	O
.	O
hardness	O
of	O
implementing	O
the	O
erm	O
rule	O
for	O
several	O
natural	O
hypothesis	B
classes	O
has	O
motivated	O
the	O
development	O
of	O
alternative	O
learning	O
methods	O
,	O
which	O
we	O
will	O
discuss	O
in	O
the	O
next	O
part	O
of	O
this	O
book	O
.	O
8.6	O
bibliographic	O
remarks	O
valiant	O
(	O
1984	O
)	O
introduced	O
the	O
eﬃcient	O
pac	O
learning	O
model	O
in	O
which	O
the	O
runtime	O
of	O
the	O
algorithm	O
is	O
required	O
to	O
be	O
polynomial	O
in	O
1/	O
,	O
1/δ	O
,	O
and	O
the	O
representation	O
size	O
of	O
hypotheses	O
in	O
the	O
class	O
.	O
a	O
detailed	O
discussion	O
and	O
thorough	O
bibliographic	O
notes	O
are	O
given	O
in	O
kearns	O
&	O
vazirani	O
(	O
1994	O
)	O
.	O
8.7	O
exercises	O
1.	O
let	O
h	O
be	O
the	O
class	O
of	O
intervals	O
on	O
the	O
line	O
(	O
formally	O
equivalent	O
to	O
axis	O
aligned	O
rectangles	O
in	O
dimension	B
n	O
=	O
1	O
)	O
.	O
propose	O
an	O
implementation	O
of	O
the	O
ermh	O
learning	O
rule	O
(	O
in	O
the	O
agnostic	O
case	O
)	O
that	O
given	O
a	O
training	B
set	I
of	O
size	O
m	O
,	O
runs	O
in	O
time	O
o	O
(	O
m2	O
)	O
.	O
hint	O
:	O
use	O
dynamic	O
programming	O
.	O
2.	O
let	O
h1	O
,	O
h2	O
,	O
.	O
.	O
.	O
be	O
a	O
sequence	O
of	O
hypothesis	B
classes	O
for	O
binary	O
classiﬁcation	O
.	O
assume	O
that	O
there	O
is	O
a	O
learning	O
algorithm	O
that	O
implements	O
the	O
erm	O
rule	O
in	O
the	O
realizable	O
case	O
such	O
that	O
the	O
output	O
hypothesis	B
of	O
the	O
algorithm	O
for	O
each	O
class	O
hn	O
only	O
depends	O
on	O
o	O
(	O
n	O
)	O
examples	O
out	O
of	O
the	O
training	B
set	I
.	O
furthermore	O
,	O
8.7	O
exercises	O
111	O
assume	O
that	O
such	O
a	O
hypothesis	B
can	O
be	O
calculated	O
given	O
these	O
o	O
(	O
n	O
)	O
examples	O
in	O
time	O
o	O
(	O
n	O
)	O
,	O
and	O
that	O
the	O
empirical	B
risk	I
of	O
each	O
such	O
hypothesis	B
can	O
be	O
evaluated	O
in	O
time	O
o	O
(	O
mn	O
)	O
.	O
for	O
example	O
,	O
if	O
hn	O
is	O
the	O
class	O
of	O
axis	O
aligned	O
rectangles	O
in	O
rn	O
,	O
we	O
saw	O
that	O
it	O
is	O
possible	O
to	O
ﬁnd	O
an	O
erm	O
hypothesis	B
in	O
the	O
realizable	O
case	O
that	O
is	O
deﬁned	O
by	O
at	O
most	O
2n	O
examples	O
.	O
prove	O
that	O
in	O
such	O
cases	O
,	O
it	O
is	O
possible	O
to	O
ﬁnd	O
an	O
erm	O
hypothesis	B
for	O
hn	O
in	O
the	O
unrealizable	O
case	O
in	O
time	O
o	O
(	O
mn	O
mo	O
(	O
n	O
)	O
)	O
.	O
3.	O
in	O
this	O
exercise	O
,	O
we	O
present	O
several	O
classes	O
for	O
which	O
ﬁnding	O
an	O
erm	O
classi-	O
ﬁer	O
is	O
computationally	O
hard	O
.	O
first	O
,	O
we	O
introduce	O
the	O
class	O
of	O
n-dimensional	O
halfspaces	O
,	O
hsn	O
,	O
for	O
a	O
domain	B
x	O
=	O
rn	O
.	O
this	O
is	O
the	O
class	O
of	O
all	O
functions	O
of	O
the	O
form	O
hw	O
,	O
b	O
(	O
x	O
)	O
=	O
sign	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
where	O
w	O
,	O
x	O
∈	O
rn	O
,	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
is	O
their	O
inner	O
product	O
,	O
and	O
b	O
∈	O
r.	O
see	O
a	O
detailed	O
description	O
in	O
chapter	O
9	O
.	O
1.	O
show	O
that	O
ermh	O
over	O
the	O
class	O
h	O
=	O
hsn	O
of	O
linear	B
predictors	I
is	O
compu-	O
tationally	O
hard	O
.	O
more	O
precisely	O
,	O
we	O
consider	O
the	O
sequence	O
of	O
problems	O
in	O
which	O
the	O
dimension	B
n	O
grows	O
linearly	O
and	O
the	O
number	O
of	O
examples	O
m	O
is	O
set	B
to	O
be	O
some	O
constant	O
times	O
n.	O
hint	O
:	O
you	O
can	O
prove	O
the	O
hardness	O
by	O
a	O
reduction	O
from	O
the	O
following	O
prob-	O
lem	O
:	O
max	O
fs	O
:	O
given	O
a	O
system	O
of	O
linear	O
inequalities	O
,	O
ax	O
>	O
b	O
with	O
a	O
∈	O
rm×n	O
and	O
b	O
∈	O
rm	O
(	O
that	O
is	O
,	O
a	O
system	O
of	O
m	O
linear	O
inequalities	O
in	O
n	O
variables	O
,	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
)	O
,	O
ﬁnd	O
a	O
subsystem	O
containing	O
as	O
many	O
inequalities	O
as	O
possible	O
that	O
has	O
a	O
solution	O
(	O
such	O
a	O
subsystem	O
is	O
called	O
feasible	B
)	O
.	O
it	O
has	O
been	O
shown	O
(	O
sankaran	O
1993	O
)	O
that	O
the	O
problem	O
max	O
fs	O
is	O
np-hard	O
.	O
show	O
that	O
any	O
algorithm	O
that	O
ﬁnds	O
an	O
ermhsn	O
hypothesis	B
for	O
any	O
training	O
sample	O
s	O
∈	O
(	O
rn	O
×	O
{	O
+1	O
,	O
−1	O
}	O
)	O
m	O
can	O
be	O
used	O
to	O
solve	O
the	O
max	O
fs	O
problem	O
of	O
size	O
m	O
,	O
n.	O
hint	O
:	O
deﬁne	O
a	O
mapping	O
that	O
transforms	O
linear	O
inequalities	O
in	O
n	O
variables	O
into	O
labeled	O
points	O
in	O
rn	O
,	O
and	O
a	O
mapping	O
that	O
transforms	O
vectors	O
in	O
rn	O
to	O
halfspaces	O
,	O
such	O
that	O
a	O
vector	O
w	O
satisﬁes	O
an	O
inequality	O
q	O
if	O
and	O
only	O
if	O
the	O
labeled	O
point	O
that	O
corresponds	O
to	O
q	O
is	O
classiﬁed	O
correctly	O
by	O
the	O
halfspace	B
corresponding	O
to	O
w.	O
conclude	O
that	O
the	O
problem	O
of	O
empirical	B
risk	I
minimization	O
for	O
halfspaces	O
in	O
also	O
np-hard	O
(	O
that	O
is	O
,	O
if	O
it	O
can	O
be	O
solved	O
in	O
time	O
polynomial	O
in	O
the	O
sample	O
size	O
,	O
m	O
,	O
and	O
the	O
euclidean	O
dimension	B
,	O
n	O
,	O
then	O
every	O
problem	O
in	O
the	O
class	O
np	O
can	O
be	O
solved	O
in	O
polynomial	O
time	O
)	O
.	O
2.	O
let	O
x	O
=	O
rn	O
and	O
let	O
hn	O
k	O
be	O
the	O
class	O
of	O
all	O
intersections	O
of	O
k-many	O
linear	O
halfspaces	O
in	O
rn	O
.	O
in	O
this	O
exercise	O
,	O
we	O
wish	O
to	O
show	O
that	O
ermhn	O
is	O
com-	O
putationally	O
hard	O
for	O
every	O
k	O
≥	O
3.	O
precisely	O
,	O
we	O
consider	O
a	O
sequence	O
of	O
problems	O
where	O
k	O
≥	O
3	O
is	O
a	O
constant	O
and	O
n	O
grows	O
linearly	O
.	O
the	O
training	B
set	I
size	O
,	O
m	O
,	O
also	O
grows	O
linearly	O
with	O
n.	O
towards	O
this	O
goal	O
,	O
consider	O
the	O
k-coloring	O
problem	O
for	O
graphs	O
,	O
deﬁned	O
as	O
follows	O
:	O
given	O
a	O
graph	O
g	O
=	O
(	O
v	O
,	O
e	O
)	O
,	O
and	O
a	O
number	O
k	O
,	O
determine	O
whether	O
there	O
exists	O
a	O
function	B
f	O
:	O
v	O
→	O
{	O
1	O
.	O
.	O
.	O
k	O
}	O
so	O
that	O
for	O
every	O
(	O
u	O
,	O
v	O
)	O
∈	O
e	O
,	O
f	O
(	O
u	O
)	O
(	O
cid:54	O
)	O
=	O
f	O
(	O
v	O
)	O
.	O
the	O
k-coloring	O
problem	O
is	O
known	O
to	O
be	O
np-hard	O
for	O
every	O
k	O
≥	O
3	O
(	O
karp	O
1972	O
)	O
.	O
k	O
112	O
the	O
runtime	O
of	O
learning	O
k	O
k	O
we	O
wish	O
to	O
reduce	O
the	O
k-coloring	O
problem	O
to	O
ermhn	O
:	O
that	O
is	O
,	O
to	O
prove	O
that	O
if	O
there	O
is	O
an	O
algorithm	O
that	O
solves	O
the	O
ermhn	O
problem	O
in	O
time	O
polynomial	O
in	O
k	O
,	O
n	O
,	O
and	O
the	O
sample	O
size	O
m	O
,	O
then	O
there	O
is	O
a	O
polynomial	O
time	O
algorithm	O
for	O
the	O
graph	O
k-coloring	O
problem	O
.	O
given	O
a	O
graph	O
g	O
=	O
(	O
v	O
,	O
e	O
)	O
,	O
let	O
{	O
v1	O
.	O
.	O
.	O
vn	O
}	O
be	O
the	O
vertices	O
in	O
v	O
.	O
construct	O
a	O
sample	O
s	O
(	O
g	O
)	O
∈	O
(	O
rn	O
×	O
{	O
±1	O
}	O
)	O
m	O
,	O
where	O
m	O
=	O
|v	O
|	O
+	O
|e|	O
,	O
as	O
follows	O
:	O
•	O
for	O
every	O
vi	O
∈	O
v	O
,	O
construct	O
an	O
instance	B
ei	O
with	O
a	O
negative	O
label	B
.	O
•	O
for	O
every	O
edge	O
(	O
vi	O
,	O
vj	O
)	O
∈	O
e	O
,	O
construct	O
an	O
instance	B
(	O
ei	O
+	O
ej	O
)	O
/2	O
with	O
a	O
1.	O
prove	O
that	O
if	O
there	O
exists	O
some	O
h	O
∈	O
hn	O
k	O
that	O
has	O
zero	O
error	O
over	O
s	O
(	O
g	O
)	O
positive	O
label	B
.	O
hint	O
:	O
let	O
h	O
=	O
(	O
cid:84	O
)	O
k	O
j=1	O
hj	O
be	O
an	O
erm	O
classiﬁer	B
in	O
hn	O
then	O
g	O
is	O
k-colorable	O
.	O
k	O
over	O
s.	O
deﬁne	O
a	O
coloring	O
of	O
v	O
by	O
setting	O
f	O
(	O
vi	O
)	O
to	O
be	O
the	O
minimal	O
j	O
such	O
that	O
hj	O
(	O
ei	O
)	O
=	O
−1	O
.	O
use	O
the	O
fact	O
that	O
halfspaces	O
are	O
convex	B
sets	O
to	O
show	O
that	O
it	O
can	O
not	O
be	O
true	O
that	O
two	O
vertices	O
that	O
are	O
connected	O
by	O
an	O
edge	O
have	O
the	O
same	O
color	O
.	O
2.	O
prove	O
that	O
if	O
g	O
is	O
k-colorable	O
then	O
there	O
exists	O
some	O
h	O
∈	O
h	O
n	O
k	O
that	O
has	O
zero	O
error	O
over	O
s	O
(	O
g	O
)	O
.	O
hint	O
:	O
given	O
a	O
coloring	O
f	O
of	O
the	O
vertices	O
of	O
g	O
,	O
we	O
should	O
come	O
up	O
with	O
k	O
hyperplanes	O
,	O
h1	O
.	O
.	O
.	O
hk	O
whose	O
intersection	O
is	O
a	O
perfect	O
classiﬁer	B
for	O
s	O
(	O
g	O
)	O
.	O
let	O
b	O
=	O
0.6	O
for	O
all	O
of	O
these	O
hyperplanes	O
and	O
,	O
for	O
t	O
≤	O
k	O
let	O
the	O
i	O
’	O
th	O
weight	O
of	O
the	O
t	O
’	O
th	O
hyperplane	O
,	O
wt	O
,	O
i	O
,	O
be	O
−1	O
if	O
f	O
(	O
vi	O
)	O
=	O
t	O
and	O
0	O
otherwise	O
.	O
3.	O
based	O
on	O
the	O
above	O
,	O
prove	O
that	O
for	O
any	O
k	O
≥	O
3	O
,	O
the	O
ermhn	O
problem	O
is	O
k	O
np-hard	O
.	O
4.	O
in	O
this	O
exercise	O
we	O
show	O
that	O
hardness	O
of	O
solving	O
the	O
erm	O
problem	O
is	O
equiv-	O
alent	O
to	O
hardness	O
of	O
proper	B
pac	O
learning	O
.	O
recall	B
that	O
by	O
“	O
properness	O
”	O
of	O
the	O
algorithm	O
we	O
mean	O
that	O
it	O
must	O
output	O
a	O
hypothesis	B
from	O
the	O
hypothesis	B
class	I
.	O
to	O
formalize	O
this	O
statement	O
,	O
we	O
ﬁrst	O
need	O
the	O
following	O
deﬁnition	O
.	O
definition	O
8.2	O
the	O
complexity	O
class	O
randomized	O
polynomial	O
(	O
rp	O
)	O
time	O
is	O
the	O
class	O
of	O
all	O
decision	O
problems	O
(	O
that	O
is	O
,	O
problems	O
in	O
which	O
on	O
any	O
instance	B
one	O
has	O
to	O
ﬁnd	O
out	O
whether	O
the	O
answer	O
is	O
yes	O
or	O
no	O
)	O
for	O
which	O
there	O
exists	O
a	O
probabilistic	O
algorithm	O
(	O
namely	O
,	O
the	O
algorithm	O
is	O
allowed	O
to	O
ﬂip	O
random	O
coins	O
while	O
it	O
is	O
running	O
)	O
with	O
these	O
properties	O
:	O
•	O
on	O
any	O
input	O
instance	B
the	O
algorithm	O
runs	O
in	O
polynomial	O
time	O
in	O
the	O
input	O
size	O
.	O
•	O
if	O
the	O
correct	O
answer	O
is	O
no	O
,	O
the	O
algorithm	O
must	O
return	O
no	O
.	O
•	O
if	O
the	O
correct	O
answer	O
is	O
yes	O
,	O
the	O
algorithm	O
returns	O
yes	O
with	O
probability	O
a	O
≥	O
1/2	O
and	O
returns	O
no	O
with	O
probability	O
1	O
−	O
a.1	O
clearly	O
the	O
class	O
rp	O
contains	O
the	O
class	O
p.	O
it	O
is	O
also	O
known	O
that	O
rp	O
is	O
contained	O
in	O
the	O
class	O
np	O
.	O
it	O
is	O
not	O
known	O
whether	O
any	O
equality	O
holds	O
among	O
these	O
three	O
complexity	O
classes	O
,	O
but	O
it	O
is	O
widely	O
believed	O
that	O
np	O
is	O
strictly	O
1	O
the	O
constant	O
1/2	O
in	O
the	O
deﬁnition	O
can	O
be	O
replaced	O
by	O
any	O
constant	O
in	O
(	O
0	O
,	O
1	O
)	O
.	O
8.7	O
exercises	O
113	O
larger	O
than	O
rp	O
.	O
in	O
particular	O
,	O
it	O
is	O
believed	O
that	O
np-hard	O
problems	O
can	O
not	O
be	O
solved	O
by	O
a	O
randomized	O
polynomial	O
time	O
algorithm	O
.	O
•	O
show	O
that	O
if	O
a	O
class	O
h	O
is	O
properly	O
pac	O
learnable	O
by	O
a	O
polynomial	O
time	O
algorithm	O
,	O
then	O
the	O
ermh	O
problem	O
is	O
in	O
the	O
class	O
rp	O
.	O
in	O
particular	O
,	O
this	O
implies	O
that	O
whenever	O
the	O
ermh	O
problem	O
is	O
np-hard	O
(	O
for	O
example	O
,	O
the	O
class	O
of	O
intersections	O
of	O
halfspaces	O
discussed	O
in	O
the	O
previous	O
exercise	O
)	O
,	O
then	O
,	O
unless	O
np	O
=	O
rp	O
,	O
there	O
exists	O
no	O
polynomial	O
time	O
proper	B
pac	O
learning	O
algorithm	O
for	O
h.	O
hint	O
:	O
assume	O
you	O
have	O
an	O
algorithm	O
a	O
that	O
properly	O
pac	O
learns	O
a	O
class	O
h	O
in	O
time	O
polynomial	O
in	O
some	O
class	O
parameter	O
n	O
as	O
well	O
as	O
in	O
1/	O
and	O
1/δ	O
.	O
your	O
goal	O
is	O
to	O
use	O
that	O
algorithm	O
as	O
a	O
subroutine	O
to	O
contract	O
an	O
algorithm	O
b	O
for	O
solving	O
the	O
ermh	O
problem	O
in	O
random	O
polynomial	O
time	O
.	O
given	O
a	O
training	B
set	I
,	O
s	O
∈	O
(	O
x	O
×	O
{	O
±1	O
}	O
m	O
)	O
,	O
and	O
some	O
h	O
∈	O
h	O
whose	O
error	O
on	O
s	O
is	O
zero	O
,	O
apply	O
the	O
pac	O
learning	O
algorithm	O
to	O
the	O
uniform	O
distribution	O
over	O
s	O
and	O
run	O
it	O
so	O
that	O
with	O
probability	O
≥	O
0.3	O
it	O
ﬁnds	O
a	O
function	B
h	O
∈	O
h	O
that	O
has	O
error	O
less	O
than	O
	O
=	O
1/|s|	O
(	O
with	O
respect	O
to	O
that	O
uniform	O
distribution	O
)	O
.	O
show	O
that	O
the	O
algorithm	O
just	O
described	O
satisﬁes	O
the	O
requirements	O
for	O
being	O
a	O
rp	O
solver	O
for	O
ermh	O
.	O
part	O
ii	O
from	O
theory	O
to	O
algorithms	O
9	O
linear	B
predictors	I
in	O
this	O
chapter	O
we	O
will	O
study	O
the	O
family	O
of	O
linear	B
predictors	I
,	O
one	O
of	O
the	O
most	O
useful	O
families	O
of	O
hypothesis	B
classes	O
.	O
many	O
learning	O
algorithms	O
that	O
are	O
being	O
widely	O
used	O
in	O
practice	O
rely	O
on	O
linear	B
predictors	I
,	O
ﬁrst	O
and	O
foremost	O
because	O
of	O
the	O
ability	O
to	O
learn	O
them	O
eﬃciently	O
in	O
many	O
cases	O
.	O
in	O
addition	O
,	O
linear	B
predictors	I
are	O
intuitive	O
,	O
are	O
easy	O
to	O
interpret	O
,	O
and	O
ﬁt	O
the	O
data	O
reasonably	O
well	O
in	O
many	O
natural	O
learning	O
problems	O
.	O
we	O
will	O
introduce	O
several	O
hypothesis	B
classes	O
belonging	O
to	O
this	O
family	O
–	O
halfspaces	O
,	O
linear	B
regression	I
predictors	O
,	O
and	O
logistic	B
regression	I
predictors	O
–	O
and	O
present	O
rele-	O
vant	O
learning	O
algorithms	O
:	O
linear	B
programming	I
and	O
the	O
perceptron	O
algorithm	O
for	O
the	O
class	O
of	O
halfspaces	O
and	O
the	O
least	B
squares	I
algorithm	O
for	O
linear	B
regression	I
.	O
this	O
chapter	O
is	O
focused	O
on	O
learning	O
linear	O
predictors	O
using	O
the	O
erm	O
approach	O
;	O
however	O
,	O
in	O
later	O
chapters	O
we	O
will	O
see	O
alternative	O
paradigms	O
for	O
learning	O
these	O
hypothesis	B
classes	O
.	O
first	O
,	O
we	O
deﬁne	O
the	O
class	O
of	O
aﬃne	O
functions	O
as	O
where	O
hw	O
,	O
b	O
(	O
x	O
)	O
=	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b	O
=	O
wixi	O
+	O
b.	O
ld	O
=	O
{	O
hw	O
,	O
b	O
:	O
w	O
∈	O
rd	O
,	O
b	O
∈	O
r	O
}	O
,	O
(	O
cid:32	O
)	O
d	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
it	O
will	O
be	O
convenient	O
also	O
to	O
use	O
the	O
notation	O
i=1	O
ld	O
=	O
{	O
x	O
(	O
cid:55	O
)	O
→	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b	O
:	O
w	O
∈	O
rd	O
,	O
b	O
∈	O
r	O
}	O
,	O
which	O
reads	O
as	O
follows	O
:	O
ld	O
is	O
a	O
set	B
of	O
functions	O
,	O
where	O
each	O
function	B
is	O
parame-	O
terized	O
by	O
w	O
∈	O
rd	O
and	O
b	O
∈	O
r	O
,	O
and	O
each	O
such	O
function	B
takes	O
as	O
input	O
a	O
vector	O
x	O
and	O
returns	O
as	O
output	O
the	O
scalar	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b.	O
the	O
diﬀerent	O
hypothesis	B
classes	O
of	O
linear	B
predictors	I
are	O
compositions	O
of	O
a	O
func-	O
tion	O
φ	O
:	O
r	O
→	O
y	O
on	O
ld	O
.	O
for	O
example	O
,	O
in	O
binary	O
classiﬁcation	O
,	O
we	O
can	O
choose	O
φ	O
to	O
be	O
the	O
sign	O
function	B
,	O
and	O
for	O
regression	B
problems	O
,	O
where	O
y	O
=	O
r	O
,	O
φ	O
is	O
simply	O
the	O
identity	O
function	B
.	O
it	O
may	O
be	O
more	O
convenient	O
to	O
incorporate	O
b	O
,	O
called	O
the	O
bias	B
,	O
into	O
w	O
as	O
an	O
extra	O
coordinate	O
and	O
add	O
an	O
extra	O
coordinate	O
with	O
a	O
value	O
of	O
1	O
to	O
all	O
x	O
∈	O
x	O
;	O
namely	O
,	O
let	O
w	O
(	O
cid:48	O
)	O
=	O
(	O
b	O
,	O
w1	O
,	O
w2	O
,	O
.	O
.	O
.	O
wd	O
)	O
∈	O
rd+1	O
and	O
let	O
x	O
(	O
cid:48	O
)	O
=	O
(	O
1	O
,	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
∈	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
118	O
linear	B
predictors	I
rd+1	O
.	O
therefore	O
,	O
hw	O
,	O
b	O
(	O
x	O
)	O
=	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b	O
=	O
(	O
cid:104	O
)	O
w	O
(	O
cid:48	O
)	O
,	O
x	O
(	O
cid:48	O
)	O
(	O
cid:105	O
)	O
.	O
it	O
follows	O
that	O
each	O
aﬃne	O
function	B
in	O
rd	O
can	O
be	O
rewritten	O
as	O
a	O
homogenous	B
linear	O
function	B
in	O
rd+1	O
applied	O
over	O
the	O
transformation	O
that	O
appends	O
the	O
constant	O
1	O
to	O
each	O
input	O
vector	O
.	O
therefore	O
,	O
whenever	O
it	O
simpliﬁes	O
the	O
presentation	O
,	O
we	O
will	O
omit	O
the	O
bias	B
term	O
and	O
refer	O
to	O
ld	O
as	O
the	O
class	O
of	O
homogenous	B
linear	O
functions	O
of	O
the	O
form	O
hw	O
(	O
x	O
)	O
=	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
.	O
throughout	O
the	O
book	O
we	O
often	O
use	O
the	O
general	O
term	O
“	O
linear	O
functions	O
”	O
for	O
both	O
aﬃne	O
functions	O
and	O
(	O
homogenous	B
)	O
linear	O
functions	O
.	O
9.1	O
halfspaces	O
the	O
ﬁrst	O
hypothesis	B
class	I
we	O
consider	O
is	O
the	O
class	O
of	O
halfspaces	O
,	O
designed	O
for	O
binary	O
classiﬁcation	O
problems	O
,	O
namely	O
,	O
x	O
=	O
rd	O
and	O
y	O
=	O
{	O
−1	O
,	O
+1	O
}	O
.	O
the	O
class	O
of	O
halfspaces	O
is	O
deﬁned	O
as	O
follows	O
:	O
hsd	O
=	O
sign	O
◦	O
ld	O
=	O
{	O
x	O
(	O
cid:55	O
)	O
→	O
sign	O
(	O
hw	O
,	O
b	O
(	O
x	O
)	O
)	O
:	O
hw	O
,	O
b	O
∈	O
ld	O
}	O
.	O
in	O
other	O
words	O
,	O
each	O
halfspace	B
hypothesis	O
in	O
hsd	O
is	O
parameterized	O
by	O
w	O
∈	O
rd	O
and	O
b	O
∈	O
r	O
and	O
upon	O
receiving	O
a	O
vector	O
x	O
the	O
hypothesis	B
returns	O
the	O
label	B
sign	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
.	O
to	O
illustrate	O
this	O
hypothesis	B
class	I
geometrically	O
,	O
it	O
is	O
instructive	O
to	O
consider	O
the	O
case	O
d	O
=	O
2.	O
each	O
hypothesis	B
forms	O
a	O
hyperplane	O
that	O
is	O
perpendicular	O
to	O
the	O
vector	O
w	O
and	O
intersects	O
the	O
vertical	O
axis	O
at	O
the	O
point	O
(	O
0	O
,	O
−b/w2	O
)	O
.	O
the	O
instances	O
that	O
are	O
“	O
above	O
”	O
the	O
hyperplane	O
,	O
that	O
is	O
,	O
share	O
an	O
acute	O
angle	O
with	O
w	O
,	O
are	O
labeled	O
positively	O
.	O
instances	O
that	O
are	O
“	O
below	O
”	O
the	O
hyperplane	O
,	O
that	O
is	O
,	O
share	O
an	O
obtuse	O
angle	O
with	O
w	O
,	O
are	O
labeled	O
negatively	O
.	O
w	O
+	O
+	O
−	O
−	O
(	O
cid:16	O
)	O
d+log	O
(	O
1/δ	O
)	O
(	O
cid:17	O
)	O
in	O
section	O
9.1.3	O
we	O
will	O
show	O
that	O
vcdim	O
(	O
hsd	O
)	O
=	O
d	O
+	O
1.	O
it	O
follows	O
that	O
we	O
can	O
learn	O
halfspaces	O
using	O
the	O
erm	O
paradigm	O
,	O
as	O
long	O
as	O
the	O
sample	O
size	O
is	O
ω	O
.	O
therefore	O
,	O
we	O
now	O
discuss	O
how	O
to	O
implement	O
an	O
erm	O
procedure	O
for	O
halfspaces	O
.	O
	O
we	O
introduce	O
below	O
two	O
solutions	O
to	O
ﬁnding	O
an	O
erm	O
halfspace	B
in	O
the	O
realiz-	O
able	O
case	O
.	O
in	O
the	O
context	O
of	O
halfspaces	O
,	O
the	O
realizable	O
case	O
is	O
often	O
referred	O
to	O
as	O
the	O
“	O
separable	B
”	O
case	O
,	O
since	O
it	O
is	O
possible	O
to	O
separate	O
with	O
a	O
hyperplane	O
all	O
the	O
positive	O
examples	O
from	O
all	O
the	O
negative	O
examples	O
.	O
implementing	O
the	O
erm	O
rule	O
9.1	O
halfspaces	O
119	O
in	O
the	O
nonseparable	O
case	O
(	O
i.e.	O
,	O
the	O
agnostic	O
case	O
)	O
is	O
known	O
to	O
be	O
computationally	O
hard	O
(	O
ben-david	O
&	O
simon	O
2001	O
)	O
.	O
there	O
are	O
several	O
approaches	O
to	O
learning	O
non-	O
separable	B
data	O
.	O
the	O
most	O
popular	O
one	O
is	O
to	O
use	O
surrogate	B
loss	I
functions	O
,	O
namely	O
,	O
to	O
learn	O
a	O
halfspace	B
that	O
does	O
not	O
necessarily	O
minimize	O
the	O
empirical	B
risk	I
with	O
the	O
0	O
−	O
1	O
loss	B
,	O
but	O
rather	O
with	O
respect	O
to	O
a	O
diﬀferent	O
loss	B
function	I
.	O
for	O
example	O
,	O
in	O
section	O
9.3	O
we	O
will	O
describe	O
the	O
logistic	B
regression	I
approach	O
,	O
which	O
can	O
be	O
implemented	O
eﬃciently	O
even	O
in	O
the	O
nonseparable	O
case	O
.	O
we	O
will	O
study	O
surrogate	B
loss	I
functions	O
in	O
more	O
detail	O
later	O
on	O
in	O
chapter	O
12	O
.	O
9.1.1	O
linear	B
programming	I
for	O
the	O
class	O
of	O
halfspaces	O
linear	O
programs	O
(	O
lp	O
)	O
are	O
problems	O
that	O
can	O
be	O
expressed	O
as	O
maximizing	O
a	O
linear	O
function	O
subject	O
to	O
linear	O
inequalities	O
.	O
that	O
is	O
,	O
(	O
cid:104	O
)	O
u	O
,	O
w	O
(	O
cid:105	O
)	O
max	O
w∈rd	O
subject	O
to	O
aw	O
≥	O
v	O
where	O
w	O
∈	O
rd	O
is	O
the	O
vector	O
of	O
variables	O
we	O
wish	O
to	O
determine	O
,	O
a	O
is	O
an	O
m	O
×	O
d	O
matrix	O
,	O
and	O
v	O
∈	O
rm	O
,	O
u	O
∈	O
rd	O
are	O
vectors	O
.	O
linear	O
programs	O
can	O
be	O
solved	O
eﬃciently,1	O
and	O
furthermore	O
,	O
there	O
are	O
publicly	O
available	O
implementations	O
of	O
lp	O
solvers	O
.	O
we	O
will	O
show	O
that	O
the	O
erm	O
problem	O
for	O
halfspaces	O
in	O
the	O
realizable	O
case	O
can	O
be	O
expressed	O
as	O
a	O
linear	O
program	O
.	O
for	O
simplicity	O
,	O
we	O
assume	O
the	O
homogenous	B
case	O
.	O
let	O
s	O
=	O
{	O
(	O
xi	O
,	O
yi	O
)	O
}	O
m	O
i=1	O
be	O
a	O
training	B
set	I
of	O
size	O
m.	O
since	O
we	O
assume	O
the	O
realizable	O
case	O
,	O
an	O
erm	O
predictor	B
should	O
have	O
zero	O
errors	O
on	O
the	O
training	B
set	I
.	O
that	O
is	O
,	O
we	O
are	O
looking	O
for	O
some	O
vector	O
w	O
∈	O
rd	O
for	O
which	O
∀i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m.	O
sign	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
)	O
=	O
yi	O
,	O
equivalently	O
,	O
we	O
are	O
looking	O
for	O
some	O
vector	O
w	O
for	O
which	O
∀i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m.	O
yi	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
>	O
0	O
,	O
let	O
w∗	O
be	O
a	O
vector	O
that	O
satisﬁes	O
this	O
condition	O
(	O
it	O
must	O
exist	O
since	O
we	O
assume	O
realizability	B
)	O
.	O
deﬁne	O
γ	O
=	O
mini	O
(	O
yi	O
(	O
cid:104	O
)	O
w∗	O
,	O
xi	O
(	O
cid:105	O
)	O
)	O
and	O
let	O
¯w	O
=	O
w∗	O
γ	O
.	O
therefore	O
,	O
for	O
all	O
i	O
we	O
have	O
yi	O
(	O
cid:104	O
)	O
¯w	O
,	O
xi	O
(	O
cid:105	O
)	O
=	O
1	O
γ	O
yi	O
(	O
cid:104	O
)	O
w∗	O
,	O
xi	O
(	O
cid:105	O
)	O
≥	O
1.	O
we	O
have	O
thus	O
shown	O
that	O
there	O
exists	O
a	O
vector	O
that	O
satisﬁes	O
yi	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
≥	O
1	O
,	O
∀i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m.	O
(	O
9.1	O
)	O
and	O
clearly	O
,	O
such	O
a	O
vector	O
is	O
an	O
erm	O
predictor	B
.	O
to	O
ﬁnd	O
a	O
vector	O
that	O
satisﬁes	O
equation	O
(	O
9.1	O
)	O
we	O
can	O
rely	O
on	O
an	O
lp	O
solver	O
as	O
follows	O
.	O
set	B
a	O
to	O
be	O
the	O
m	O
×	O
d	O
matrix	O
whose	O
rows	O
are	O
the	O
instances	O
multiplied	O
1	O
namely	O
,	O
in	O
time	O
polynomial	O
in	O
m	O
,	O
d	O
,	O
and	O
in	O
the	O
representation	O
size	O
of	O
real	O
numbers	O
.	O
120	O
linear	B
predictors	I
by	O
yi	O
.	O
that	O
is	O
,	O
ai	O
,	O
j	O
=	O
yi	O
xi	O
,	O
j	O
,	O
where	O
xi	O
,	O
j	O
is	O
the	O
j	O
’	O
th	O
element	O
of	O
the	O
vector	O
xi	O
.	O
let	O
v	O
be	O
the	O
vector	O
(	O
1	O
,	O
.	O
.	O
.	O
,	O
1	O
)	O
∈	O
rm	O
.	O
then	O
,	O
equation	O
(	O
9.1	O
)	O
can	O
be	O
rewritten	O
as	O
aw	O
≥	O
v.	O
the	O
lp	O
form	O
requires	O
a	O
maximization	O
objective	O
,	O
yet	O
all	O
the	O
w	O
that	O
satisfy	O
the	O
constraints	O
are	O
equal	O
candidates	O
as	O
output	O
hypotheses	O
.	O
thus	O
,	O
we	O
set	B
a	O
“	O
dummy	O
”	O
objective	O
,	O
u	O
=	O
(	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
)	O
∈	O
rd	O
.	O
9.1.2	O
perceptron	O
for	O
halfspaces	O
a	O
diﬀerent	O
implementation	O
of	O
the	O
erm	O
rule	O
is	O
the	O
perceptron	O
algorithm	O
of	O
rosenblatt	O
(	O
rosenblatt	O
1958	O
)	O
.	O
the	O
perceptron	O
is	O
an	O
iterative	O
algorithm	O
that	O
constructs	O
a	O
sequence	O
of	O
vectors	O
w	O
(	O
1	O
)	O
,	O
w	O
(	O
2	O
)	O
,	O
.	O
.	O
..	O
initially	O
,	O
w	O
(	O
1	O
)	O
is	O
set	B
to	O
be	O
the	O
all-zeros	O
vector	O
.	O
at	O
iteration	O
t	O
,	O
the	O
perceptron	O
ﬁnds	O
an	O
example	O
i	O
that	O
is	O
mis-	O
labeled	O
by	O
w	O
(	O
t	O
)	O
,	O
namely	O
,	O
an	O
example	O
for	O
which	O
sign	O
(	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
xi	O
(	O
cid:105	O
)	O
)	O
(	O
cid:54	O
)	O
=	O
yi	O
.	O
then	O
,	O
the	O
perceptron	O
updates	O
w	O
(	O
t	O
)	O
by	O
adding	O
to	O
it	O
the	O
instance	B
xi	O
scaled	O
by	O
the	O
label	B
yi	O
.	O
that	O
is	O
,	O
w	O
(	O
t+1	O
)	O
=	O
w	O
(	O
t	O
)	O
+	O
yixi	O
.	O
recall	B
that	O
our	O
goal	O
is	O
to	O
have	O
yi	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
>	O
0	O
for	O
all	O
i	O
and	O
note	O
that	O
yi	O
(	O
cid:104	O
)	O
w	O
(	O
t+1	O
)	O
,	O
xi	O
(	O
cid:105	O
)	O
=	O
yi	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
+	O
yixi	O
,	O
xi	O
(	O
cid:105	O
)	O
=	O
yi	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
2.	O
hence	O
,	O
the	O
update	O
of	O
the	O
perceptron	O
guides	O
the	O
solution	O
to	O
be	O
“	O
more	O
correct	O
”	O
on	O
the	O
i	O
’	O
th	O
example	O
.	O
batch	O
perceptron	O
input	O
:	O
a	O
training	B
set	I
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
initialize	O
:	O
w	O
(	O
1	O
)	O
=	O
(	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
)	O
for	O
t	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
if	O
(	O
∃	O
i	O
s.t	O
.	O
yi	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
xi	O
(	O
cid:105	O
)	O
≤	O
0	O
)	O
then	O
w	O
(	O
t+1	O
)	O
=	O
w	O
(	O
t	O
)	O
+	O
yixi	O
else	O
output	O
w	O
(	O
t	O
)	O
the	O
following	O
theorem	O
guarantees	O
that	O
in	O
the	O
realizable	O
case	O
,	O
the	O
algorithm	O
stops	O
with	O
all	O
sample	O
points	O
correctly	O
classiﬁed	O
.	O
theorem	O
9.1	O
assume	O
that	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
is	O
separable	B
,	O
let	O
b	O
=	O
min	O
{	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
:	O
yi	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
≥	O
1	O
}	O
,	O
and	O
let	O
r	O
=	O
maxi	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
.	O
then	O
,	O
the	O
perceptron	O
al-	O
∀i	O
∈	O
[	O
m	O
]	O
,	O
gorithm	O
stops	O
after	O
at	O
most	O
(	O
rb	O
)	O
2	O
iterations	O
,	O
and	O
when	O
it	O
stops	O
it	O
holds	O
that	O
∀i	O
∈	O
[	O
m	O
]	O
,	O
yi	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
xi	O
(	O
cid:105	O
)	O
>	O
0.	O
proof	O
by	O
the	O
deﬁnition	O
of	O
the	O
stopping	O
condition	O
,	O
if	O
the	O
perceptron	O
stops	O
it	O
must	O
have	O
separated	O
all	O
the	O
examples	O
.	O
we	O
will	O
show	O
that	O
if	O
the	O
perceptron	O
runs	O
for	O
t	O
iterations	O
,	O
then	O
we	O
must	O
have	O
t	O
≤	O
(	O
rb	O
)	O
2	O
,	O
which	O
implies	O
the	O
perceptron	O
must	O
stop	O
after	O
at	O
most	O
(	O
rb	O
)	O
2	O
iterations	O
.	O
let	O
w	O
(	O
cid:63	O
)	O
be	O
a	O
vector	O
that	O
achieves	O
the	O
minimum	O
in	O
the	O
deﬁnition	O
of	O
b.	O
that	O
is	O
,	O
9.1	O
halfspaces	O
121	O
yi	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
xi	O
(	O
cid:105	O
)	O
≥	O
1	O
for	O
all	O
i	O
,	O
and	O
among	O
all	O
vectors	O
that	O
satisfy	O
these	O
constraints	O
,	O
w	O
(	O
cid:63	O
)	O
is	O
of	O
minimal	O
norm	O
.	O
the	O
idea	O
of	O
the	O
proof	O
is	O
to	O
show	O
that	O
after	O
performing	O
t	O
iterations	O
,	O
the	O
cosine	O
√	O
of	O
the	O
angle	O
between	O
w	O
(	O
cid:63	O
)	O
and	O
w	O
(	O
t	O
+1	O
)	O
is	O
at	O
least	O
rb	O
.	O
that	O
is	O
,	O
we	O
will	O
show	O
that	O
√	O
t	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
w	O
(	O
t	O
+1	O
)	O
(	O
cid:105	O
)	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
+1	O
)	O
(	O
cid:107	O
)	O
≥	O
t	O
rb	O
.	O
(	O
9.2	O
)	O
by	O
the	O
cauchy-schwartz	O
inequality	O
,	O
the	O
left-hand	O
side	O
of	O
equation	O
(	O
9.2	O
)	O
is	O
at	O
most	O
1.	O
therefore	O
,	O
equation	O
(	O
9.2	O
)	O
would	O
imply	O
that	O
√	O
t	O
rb	O
1	O
≥	O
⇒	O
t	O
≤	O
(	O
rb	O
)	O
2	O
,	O
which	O
will	O
conclude	O
our	O
proof	O
.	O
to	O
show	O
that	O
equation	O
(	O
9.2	O
)	O
holds	O
,	O
we	O
ﬁrst	O
show	O
that	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
w	O
(	O
t	O
+1	O
)	O
(	O
cid:105	O
)	O
≥	O
t	O
.	O
indeed	O
,	O
at	O
the	O
ﬁrst	O
iteration	O
,	O
w	O
(	O
1	O
)	O
=	O
(	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
)	O
and	O
therefore	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
w	O
(	O
1	O
)	O
(	O
cid:105	O
)	O
=	O
0	O
,	O
while	O
on	O
iteration	O
t	O
,	O
if	O
we	O
update	O
using	O
example	O
(	O
xi	O
,	O
yi	O
)	O
we	O
have	O
that	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
w	O
(	O
t+1	O
)	O
(	O
cid:105	O
)	O
−	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
w	O
(	O
t	O
)	O
(	O
cid:105	O
)	O
=	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
w	O
(	O
t+1	O
)	O
−	O
w	O
(	O
t	O
)	O
(	O
cid:105	O
)	O
=	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
yixi	O
(	O
cid:105	O
)	O
=	O
yi	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
xi	O
(	O
cid:105	O
)	O
≥	O
1.	O
therefore	O
,	O
after	O
performing	O
t	O
iterations	O
,	O
we	O
get	O
:	O
t	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
w	O
(	O
t+1	O
)	O
(	O
cid:105	O
)	O
−	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
w	O
(	O
t	O
)	O
(	O
cid:105	O
)	O
(	O
cid:17	O
)	O
≥	O
t	O
,	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
w	O
(	O
t	O
+1	O
)	O
(	O
cid:105	O
)	O
=	O
t=1	O
as	O
required	O
.	O
next	O
,	O
we	O
upper	O
bound	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
+1	O
)	O
(	O
cid:107	O
)	O
.	O
for	O
each	O
iteration	O
t	O
we	O
have	O
that	O
(	O
cid:107	O
)	O
w	O
(	O
t+1	O
)	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
+	O
yixi	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
(	O
cid:107	O
)	O
2	O
+	O
2yi	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
y2	O
≤	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
(	O
cid:107	O
)	O
2	O
+	O
r2	O
i	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
2	O
(	O
9.3	O
)	O
(	O
9.4	O
)	O
where	O
the	O
last	O
inequality	O
is	O
due	O
to	O
the	O
fact	O
that	O
example	O
i	O
is	O
necessarily	O
such	O
that	O
yi	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
xi	O
(	O
cid:105	O
)	O
≤	O
0	O
,	O
and	O
the	O
norm	O
of	O
xi	O
is	O
at	O
most	O
r.	O
now	O
,	O
since	O
(	O
cid:107	O
)	O
w	O
(	O
1	O
)	O
(	O
cid:107	O
)	O
2	O
=	O
0	O
,	O
if	O
we	O
use	O
equation	O
(	O
9.4	O
)	O
recursively	O
for	O
t	O
iterations	O
,	O
we	O
obtain	O
that	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
+1	O
)	O
(	O
cid:107	O
)	O
2	O
≤	O
tr2	O
⇒	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
+1	O
)	O
(	O
cid:107	O
)	O
≤	O
(	O
9.5	O
)	O
combining	O
equation	O
(	O
9.3	O
)	O
with	O
equation	O
(	O
9.5	O
)	O
,	O
and	O
using	O
the	O
fact	O
that	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
=	O
b	O
,	O
we	O
obtain	O
that	O
t	O
r.	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
+1	O
)	O
,	O
w	O
(	O
cid:63	O
)	O
(	O
cid:105	O
)	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
+1	O
)	O
(	O
cid:107	O
)	O
≥	O
√	O
t	O
t	O
r	O
b	O
=	O
√	O
√	O
t	O
b	O
r	O
.	O
we	O
have	O
thus	O
shown	O
that	O
equation	O
(	O
9.2	O
)	O
holds	O
,	O
and	O
this	O
concludes	O
our	O
proof	O
.	O
122	O
linear	B
predictors	I
remark	O
9.1	O
the	O
perceptron	O
is	O
simple	O
to	O
implement	O
and	O
is	O
guaranteed	O
to	O
con-	O
verge	O
.	O
however	O
,	O
the	O
convergence	O
rate	O
depends	O
on	O
the	O
parameter	O
b	O
,	O
which	O
in	O
some	O
situations	O
might	O
be	O
exponentially	O
large	O
in	O
d.	O
in	O
such	O
cases	O
,	O
it	O
would	O
be	O
better	O
to	O
implement	O
the	O
erm	O
problem	O
by	O
solving	O
a	O
linear	O
program	O
,	O
as	O
described	O
in	O
the	O
previous	O
section	O
.	O
nevertheless	O
,	O
for	O
many	O
natural	O
data	O
sets	O
,	O
the	O
size	O
of	O
b	O
is	O
not	O
too	O
large	O
,	O
and	O
the	O
perceptron	O
converges	O
quite	O
fast	O
.	O
9.1.3	O
the	O
vc	O
dimension	B
of	O
halfspaces	O
to	O
compute	O
the	O
vc	O
dimension	B
of	O
halfspaces	O
,	O
we	O
start	O
with	O
the	O
homogenous	B
case	O
.	O
theorem	O
9.2	O
the	O
vc	O
dimension	B
of	O
the	O
class	O
of	O
homogenous	B
halfspaces	O
in	O
rd	O
is	O
d.	O
proof	O
first	O
,	O
consider	O
the	O
set	B
of	O
vectors	O
e1	O
,	O
.	O
.	O
.	O
,	O
ed	O
,	O
where	O
for	O
every	O
i	O
the	O
vector	O
ei	O
is	O
the	O
all	O
zeros	O
vector	O
except	O
1	O
in	O
the	O
i	O
’	O
th	O
coordinate	O
.	O
this	O
set	B
is	O
shattered	O
by	O
the	O
class	O
of	O
homogenous	B
halfspaces	O
.	O
indeed	O
,	O
for	O
every	O
labeling	O
y1	O
,	O
.	O
.	O
.	O
,	O
yd	O
,	O
set	B
w	O
=	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yd	O
)	O
,	O
and	O
then	O
(	O
cid:104	O
)	O
w	O
,	O
ei	O
(	O
cid:105	O
)	O
=	O
yi	O
for	O
all	O
i.	O
real	O
numbers	O
a1	O
,	O
.	O
.	O
.	O
,	O
ad+1	O
,	O
not	O
all	O
of	O
them	O
are	O
zero	O
,	O
such	O
that	O
(	O
cid:80	O
)	O
d+1	O
next	O
,	O
let	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd+1	O
be	O
a	O
set	B
of	O
d	O
+	O
1	O
vectors	O
in	O
rd	O
.	O
then	O
,	O
there	O
must	O
exist	O
i=1	O
aixi	O
=	O
0.	O
let	O
i	O
=	O
{	O
i	O
:	O
ai	O
>	O
0	O
}	O
and	O
j	O
=	O
{	O
j	O
:	O
aj	O
<	O
0	O
}	O
.	O
either	O
i	O
or	O
j	O
is	O
nonempty	O
.	O
let	O
us	O
ﬁrst	O
assume	O
that	O
both	O
of	O
them	O
are	O
nonempty	O
.	O
then	O
,	O
|aj|xj	O
.	O
(	O
cid:88	O
)	O
aixi	O
=	O
now	O
,	O
suppose	O
that	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd+1	O
are	O
shattered	O
by	O
the	O
class	O
of	O
homogenous	B
classes	O
.	O
then	O
,	O
there	O
must	O
exist	O
a	O
vector	O
w	O
such	O
that	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
>	O
0	O
for	O
all	O
i	O
∈	O
i	O
while	O
(	O
cid:104	O
)	O
w	O
,	O
xj	O
(	O
cid:105	O
)	O
<	O
0	O
for	O
every	O
j	O
∈	O
j.	O
it	O
follows	O
that	O
0	O
<	O
ai	O
(	O
cid:104	O
)	O
xi	O
,	O
w	O
(	O
cid:105	O
)	O
=	O
aixi	O
,	O
w	O
=	O
|aj|xj	O
,	O
w	O
=	O
|aj|	O
(	O
cid:104	O
)	O
xj	O
,	O
w	O
(	O
cid:105	O
)	O
<	O
0	O
,	O
(	O
cid:88	O
)	O
j∈j	O
(	O
cid:42	O
)	O
(	O
cid:88	O
)	O
j∈j	O
i∈i	O
(	O
cid:43	O
)	O
(	O
cid:88	O
)	O
i∈i	O
(	O
cid:42	O
)	O
(	O
cid:88	O
)	O
i∈i	O
(	O
cid:43	O
)	O
(	O
cid:88	O
)	O
j∈j	O
which	O
leads	O
to	O
a	O
contradiction	O
.	O
finally	O
,	O
if	O
j	O
(	O
respectively	O
,	O
i	O
)	O
is	O
empty	O
then	O
the	O
right-most	O
(	O
respectively	O
,	O
left-most	O
)	O
inequality	O
should	O
be	O
replaced	O
by	O
an	O
equality	O
,	O
which	O
still	O
leads	O
to	O
a	O
contradiction	O
.	O
theorem	O
9.3	O
the	O
vc	O
dimension	B
of	O
the	O
class	O
of	O
nonhomogenous	O
halfspaces	O
in	O
rd	O
is	O
d	O
+	O
1.	O
proof	O
first	O
,	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
9.2	O
,	O
it	O
is	O
easy	O
to	O
verify	O
that	O
the	O
set	B
of	O
vectors	O
0	O
,	O
e1	O
,	O
.	O
.	O
.	O
,	O
ed	O
is	O
shattered	O
by	O
the	O
class	O
of	O
nonhomogenous	O
halfspaces	O
.	O
second	O
,	O
suppose	O
that	O
the	O
vectors	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd+2	O
are	O
shattered	O
by	O
the	O
class	O
of	O
non-	O
homogenous	B
halfspaces	O
.	O
but	O
,	O
using	O
the	O
reduction	O
we	O
have	O
shown	O
in	O
the	O
beginning	O
of	O
this	O
chapter	O
,	O
it	O
follows	O
that	O
there	O
are	O
d	O
+	O
2	O
vectors	O
in	O
rd+1	O
that	O
are	O
shattered	O
by	O
the	O
class	O
of	O
homogenous	B
halfspaces	O
.	O
but	O
this	O
contradicts	O
theorem	O
9.2	O
.	O
9.2	O
linear	B
regression	I
123	O
r	O
r	O
r	O
r	O
r	O
r	O
r	O
rr	O
r	O
r	O
figure	O
9.1	O
linear	B
regression	I
for	O
d	O
=	O
1.	O
for	O
instance	B
,	O
the	O
x-axis	O
may	O
denote	O
the	O
age	O
of	O
the	O
baby	O
,	O
and	O
the	O
y-axis	O
her	O
weight	O
.	O
9.2	O
linear	B
regression	I
linear	O
regression	B
is	O
a	O
common	O
statistical	O
tool	O
for	O
modeling	O
the	O
relationship	O
be-	O
tween	O
some	O
“	O
explanatory	O
”	O
variables	O
and	O
some	O
real	O
valued	O
outcome	O
.	O
cast	O
as	O
a	O
learning	O
problem	O
,	O
the	O
domain	B
set	O
x	O
is	O
a	O
subset	O
of	O
rd	O
,	O
for	O
some	O
d	O
,	O
and	O
the	O
la-	O
bel	O
set	B
y	O
is	O
the	O
set	B
of	O
real	O
numbers	O
.	O
we	O
would	O
like	O
to	O
learn	O
a	O
linear	O
function	O
h	O
:	O
rd	O
→	O
r	O
that	O
best	O
approximates	O
the	O
relationship	O
between	O
our	O
variables	O
(	O
say	O
,	O
for	O
example	O
,	O
predicting	O
the	O
weight	O
of	O
a	O
baby	O
as	O
a	O
function	B
of	O
her	O
age	O
and	O
weight	O
at	O
birth	O
)	O
.	O
figure	O
9.1	O
shows	O
an	O
example	O
of	O
a	O
linear	B
regression	I
predictor	O
for	O
d	O
=	O
1.	O
the	O
hypothesis	B
class	I
of	O
linear	B
regression	I
predictors	O
is	O
simply	O
the	O
set	B
of	O
linear	O
functions	O
,	O
hreg	O
=	O
ld	O
=	O
{	O
x	O
(	O
cid:55	O
)	O
→	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b	O
:	O
w	O
∈	O
rd	O
,	O
b	O
∈	O
r	O
}	O
.	O
next	O
we	O
need	O
to	O
deﬁne	O
a	O
loss	B
function	I
for	O
regression	B
.	O
while	O
in	O
classiﬁcation	O
the	O
deﬁnition	O
of	O
the	O
loss	B
is	O
straightforward	O
,	O
as	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
simply	O
indicates	O
whether	O
h	O
(	O
x	O
)	O
correctly	O
predicts	O
y	O
or	O
not	O
,	O
in	O
regression	B
,	O
if	O
the	O
baby	O
’	O
s	O
weight	O
is	O
3	O
kg	O
,	O
both	O
the	O
predictions	O
3.00001	O
kg	O
and	O
4	O
kg	O
are	O
“	O
wrong	O
,	O
”	O
but	O
we	O
would	O
clearly	O
prefer	O
the	O
former	O
over	O
the	O
latter	O
.	O
we	O
therefore	O
need	O
to	O
deﬁne	O
how	O
much	O
we	O
shall	O
be	O
“	O
penalized	O
”	O
for	O
the	O
discrepancy	O
between	O
h	O
(	O
x	O
)	O
and	O
y.	O
one	O
common	O
way	O
is	O
to	O
use	O
the	O
squared-loss	O
function	B
,	O
namely	O
,	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
(	O
h	O
(	O
x	O
)	O
−	O
y	O
)	O
2.	O
for	O
this	O
loss	B
function	I
,	O
the	O
empirical	B
risk	I
function	O
is	O
called	O
the	O
mean	O
squared	O
error	O
,	O
namely	O
,	O
m	O
(	O
cid:88	O
)	O
i=1	O
ls	O
(	O
h	O
)	O
=	O
1	O
m	O
(	O
h	O
(	O
xi	O
)	O
−	O
yi	O
)	O
2	O
.	O
124	O
linear	B
predictors	I
in	O
the	O
next	O
subsection	O
,	O
we	O
will	O
see	O
how	O
to	O
implement	O
the	O
erm	O
rule	O
for	O
linear	B
regression	I
with	O
respect	O
to	O
the	O
squared	O
loss	B
.	O
of	O
course	O
,	O
there	O
are	O
a	O
variety	O
of	O
other	O
loss	B
functions	O
that	O
one	O
can	O
use	O
,	O
for	O
example	O
,	O
the	O
absolute	B
value	I
loss	I
function	O
,	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
|h	O
(	O
x	O
)	O
−	O
y|	O
.	O
the	O
erm	O
rule	O
for	O
the	O
absolute	B
value	I
loss	I
function	O
can	O
be	O
implemented	O
using	O
linear	B
programming	I
(	O
see	O
exercise	O
1	O
.	O
)	O
note	O
that	O
since	O
linear	B
regression	I
is	O
not	O
a	O
binary	O
prediction	O
task	O
,	O
we	O
can	O
not	O
an-	O
alyze	O
its	O
sample	B
complexity	I
using	O
the	O
vc-dimension	O
.	O
one	O
possible	O
analysis	O
of	O
the	O
sample	B
complexity	I
of	O
linear	B
regression	I
is	O
by	O
relying	O
on	O
the	O
“	O
discretization	B
trick	I
”	O
(	O
see	O
remark	O
4.1	O
in	O
chapter	O
4	O
)	O
;	O
namely	O
,	O
if	O
we	O
are	O
happy	O
with	O
a	O
representation	O
of	O
each	O
element	O
of	O
the	O
vector	O
w	O
and	O
the	O
bias	B
b	O
using	O
a	O
ﬁnite	O
number	O
of	O
bits	O
(	O
say	O
a	O
64	O
bits	O
ﬂoating	O
point	O
representation	O
)	O
,	O
then	O
the	O
hypothesis	B
class	I
becomes	O
ﬁnite	O
and	O
its	O
size	O
is	O
at	O
most	O
264	O
(	O
d+1	O
)	O
.	O
we	O
can	O
now	O
rely	O
on	O
sample	B
complexity	I
bounds	O
for	O
ﬁnite	O
hypothesis	B
classes	O
as	O
described	O
in	O
chapter	O
4.	O
note	O
,	O
however	O
,	O
that	O
to	O
apply	O
the	O
sample	B
complexity	I
bounds	O
from	O
chapter	O
4	O
we	O
also	O
need	O
that	O
the	O
loss	B
function	I
will	O
be	O
bounded	O
.	O
later	O
in	O
the	O
book	O
we	O
will	O
describe	O
more	O
rigorous	O
means	O
to	O
analyze	O
the	O
sample	B
complexity	I
of	O
regression	B
problems	O
.	O
9.2.1	O
least	B
squares	I
least	O
squares	O
is	O
the	O
algorithm	O
that	O
solves	O
the	O
erm	O
problem	O
for	O
the	O
hypoth-	O
esis	O
class	O
of	O
linear	B
regression	I
predictors	O
with	O
respect	O
to	O
the	O
squared	O
loss	B
.	O
the	O
erm	O
problem	O
with	O
respect	O
to	O
this	O
class	O
,	O
given	O
a	O
training	B
set	I
s	O
,	O
and	O
using	O
the	O
homogenous	B
version	O
of	O
ld	O
,	O
is	O
to	O
ﬁnd	O
argmin	O
w	O
ls	O
(	O
hw	O
)	O
=	O
argmin	O
w	O
1	O
m	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
−	O
yi	O
)	O
2.	O
m	O
(	O
cid:88	O
)	O
i=1	O
to	O
solve	O
the	O
problem	O
we	O
calculate	O
the	O
gradient	B
of	O
the	O
objective	O
function	B
and	O
compare	O
it	O
to	O
zero	O
.	O
that	O
is	O
,	O
we	O
need	O
to	O
solve	O
m	O
(	O
cid:88	O
)	O
i=1	O
2	O
m	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
−	O
yi	O
)	O
xi	O
=	O
0.	O
we	O
can	O
rewrite	O
the	O
problem	O
as	O
the	O
problem	O
aw	O
=	O
b	O
where	O
(	O
cid:32	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
a	O
=	O
xi	O
x	O
(	O
cid:62	O
)	O
i	O
(	O
cid:33	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
and	O
b	O
=	O
yixi	O
.	O
(	O
9.6	O
)	O
or	O
,	O
in	O
matrix	O
form	O
:	O
a	O
=	O
b	O
=	O
	O
...	O
	O
...	O
x1	O
...	O
x1	O
...	O
9.2	O
linear	B
regression	I
125	O
	O
	O
...	O
.	O
.	O
.	O
xm	O
...	O
...	O
.	O
.	O
.	O
xm	O
...	O
	O
...	O
	O
y1	O
x1	O
...	O
...	O
ym	O
(	O
cid:62	O
)	O
	O
,	O
...	O
.	O
.	O
.	O
xm	O
...	O
	O
.	O
(	O
9.7	O
)	O
(	O
9.8	O
)	O
if	O
a	O
is	O
invertible	O
then	O
the	O
solution	O
to	O
the	O
erm	O
problem	O
is	O
w	O
=	O
a−1	O
b.	O
the	O
case	O
in	O
which	O
a	O
is	O
not	O
invertible	O
requires	O
a	O
few	O
standard	O
tools	O
from	O
linear	O
algebra	O
,	O
which	O
are	O
available	O
in	O
appendix	O
c.	O
it	O
can	O
be	O
easily	O
shown	O
that	O
if	O
the	O
training	O
instances	O
do	O
not	O
span	O
the	O
entire	O
space	O
of	O
rd	O
then	O
a	O
is	O
not	O
invertible	O
.	O
nevertheless	O
,	O
we	O
can	O
always	O
ﬁnd	O
a	O
solution	O
to	O
the	O
system	O
aw	O
=	O
b	O
because	O
b	O
is	O
in	O
the	O
range	O
of	O
a.	O
indeed	O
,	O
since	O
a	O
is	O
symmetric	O
we	O
can	O
write	O
it	O
using	O
its	O
eigenvalue	O
decomposition	O
as	O
a	O
=	O
v	O
dv	O
(	O
cid:62	O
)	O
,	O
where	O
d	O
is	O
a	O
diagonal	O
matrix	O
and	O
v	O
is	O
an	O
orthonormal	O
matrix	O
(	O
that	O
is	O
,	O
v	O
(	O
cid:62	O
)	O
v	O
is	O
the	O
identity	O
d	O
×	O
d	O
matrix	O
)	O
.	O
deﬁne	O
d+	O
to	O
be	O
the	O
diagonal	O
matrix	O
such	O
that	O
d+	O
i	O
,	O
i	O
=	O
0	O
if	O
di	O
,	O
i	O
=	O
0	O
and	O
otherwise	O
d+	O
i	O
,	O
i	O
=	O
1/di	O
,	O
i	O
.	O
now	O
,	O
deﬁne	O
a+	O
=	O
v	O
d+v	O
(	O
cid:62	O
)	O
and	O
ˆw	O
=	O
a+b	O
.	O
let	O
vi	O
denote	O
the	O
i	O
’	O
th	O
column	O
of	O
v	O
.	O
then	O
,	O
we	O
have	O
a	O
ˆw	O
=	O
aa+b	O
=	O
v	O
dv	O
(	O
cid:62	O
)	O
v	O
d+v	O
(	O
cid:62	O
)	O
b	O
=	O
v	O
dd+v	O
(	O
cid:62	O
)	O
b	O
=	O
(	O
cid:88	O
)	O
i	O
:	O
di	O
,	O
i	O
(	O
cid:54	O
)	O
=0	O
viv	O
(	O
cid:62	O
)	O
i	O
b.	O
that	O
is	O
,	O
a	O
ˆw	O
is	O
the	O
projection	B
of	O
b	O
onto	O
the	O
span	O
of	O
those	O
vectors	O
vi	O
for	O
which	O
di	O
,	O
i	O
(	O
cid:54	O
)	O
=	O
0.	O
since	O
the	O
linear	O
span	O
of	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
is	O
the	O
same	O
as	O
the	O
linear	O
span	O
of	O
those	O
vi	O
,	O
and	O
b	O
is	O
in	O
the	O
linear	O
span	O
of	O
the	O
xi	O
,	O
we	O
obtain	O
that	O
a	O
ˆw	O
=	O
b	O
,	O
which	O
concludes	O
our	O
argument	O
.	O
9.2.2	O
linear	B
regression	I
for	O
polynomial	B
regression	I
tasks	O
some	O
learning	O
tasks	O
call	O
for	O
nonlinear	O
predictors	O
,	O
such	O
as	O
polynomial	O
predictors	O
.	O
take	O
,	O
for	O
instance	B
,	O
a	O
one	O
dimensional	O
polynomial	O
function	O
of	O
degree	O
n	O
,	O
that	O
is	O
,	O
p	O
(	O
x	O
)	O
=	O
a0	O
+	O
a1x	O
+	O
a2x2	O
+	O
···	O
+	O
anxn	O
where	O
(	O
a0	O
,	O
.	O
.	O
.	O
,	O
an	O
)	O
is	O
a	O
vector	O
of	O
coeﬃcients	O
of	O
size	O
n	O
+	O
1.	O
in	O
the	O
following	O
we	O
depict	O
a	O
training	B
set	I
that	O
is	O
better	O
ﬁtted	O
using	O
a	O
3rd	O
degree	O
polynomial	O
predictor	O
than	O
using	O
a	O
linear	B
predictor	I
.	O
126	O
linear	B
predictors	I
we	O
will	O
focus	O
here	O
on	O
the	O
class	O
of	O
one	O
dimensional	O
,	O
n-degree	O
,	O
polynomial	O
re-	O
gression	O
predictors	O
,	O
namely	O
,	O
hn	O
poly	O
=	O
{	O
x	O
(	O
cid:55	O
)	O
→	O
p	O
(	O
x	O
)	O
}	O
,	O
where	O
p	O
is	O
a	O
one	O
dimensional	O
polynomial	O
of	O
degree	O
n	O
,	O
parameterized	O
by	O
a	O
vector	O
of	O
coeﬃcients	O
(	O
a0	O
,	O
.	O
.	O
.	O
,	O
an	O
)	O
.	O
note	O
that	O
x	O
=	O
r	O
,	O
since	O
this	O
is	O
a	O
one	O
dimensional	O
polynomial	O
,	O
and	O
y	O
=	O
r	O
,	O
as	O
this	O
is	O
a	O
regression	B
problem	O
.	O
one	O
way	O
to	O
learn	O
this	O
class	O
is	O
by	O
reduction	O
to	O
the	O
problem	O
of	O
linear	B
regression	I
,	O
which	O
we	O
have	O
already	O
shown	O
how	O
to	O
solve	O
.	O
to	O
translate	O
a	O
polynomial	B
regression	I
problem	O
to	O
a	O
linear	B
regression	I
problem	O
,	O
we	O
deﬁne	O
the	O
mapping	O
ψ	O
:	O
r	O
→	O
rn+1	O
such	O
that	O
ψ	O
(	O
x	O
)	O
=	O
(	O
1	O
,	O
x	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
.	O
then	O
we	O
have	O
that	O
p	O
(	O
ψ	O
(	O
x	O
)	O
)	O
=	O
a0	O
+	O
a1x	O
+	O
a2x2	O
+	O
···	O
+	O
anxn	O
=	O
(	O
cid:104	O
)	O
a	O
,	O
ψ	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
and	O
we	O
can	O
ﬁnd	O
the	O
optimal	O
vector	O
of	O
coeﬃcients	O
a	O
by	O
using	O
the	O
least	B
squares	I
algorithm	O
as	O
shown	O
earlier	O
.	O
9.3	O
logistic	B
regression	I
in	O
logistic	B
regression	I
we	O
learn	O
a	O
family	O
of	O
functions	O
h	O
from	O
rd	O
to	O
the	O
interval	O
[	O
0	O
,	O
1	O
]	O
.	O
however	O
,	O
logistic	B
regression	I
is	O
used	O
for	O
classiﬁcation	O
tasks	O
:	O
we	O
can	O
interpret	O
h	O
(	O
x	O
)	O
as	O
the	O
probability	O
that	O
the	O
label	B
of	O
x	O
is	O
1.	O
the	O
hypothesis	B
class	I
associated	O
with	O
logistic	B
regression	I
is	O
the	O
composition	O
of	O
a	O
sigmoid	O
function	B
φsig	O
:	O
r	O
→	O
[	O
0	O
,	O
1	O
]	O
over	O
the	O
class	O
of	O
linear	O
functions	O
ld	O
.	O
in	O
particular	O
,	O
the	O
sigmoid	O
function	B
used	O
in	O
logistic	B
regression	I
is	O
the	O
logistic	O
function	O
,	O
deﬁned	O
as	O
φsig	O
(	O
z	O
)	O
=	O
1	O
1	O
+	O
exp	O
(	O
−z	O
)	O
.	O
(	O
9.9	O
)	O
the	O
name	O
“	O
sigmoid	O
”	O
means	O
“	O
s-shaped	O
,	O
”	O
referring	O
to	O
the	O
plot	O
of	O
this	O
function	B
,	O
shown	O
in	O
the	O
ﬁgure	O
:	O
9.3	O
logistic	B
regression	I
127	O
the	O
hypothesis	B
class	I
is	O
therefore	O
(	O
where	O
for	O
simplicity	O
we	O
are	O
using	O
homogenous	B
linear	O
functions	O
)	O
:	O
hsig	O
=	O
φsig	O
◦	O
ld	O
=	O
{	O
x	O
(	O
cid:55	O
)	O
→	O
φsig	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
:	O
w	O
∈	O
rd	O
}	O
.	O
note	O
that	O
when	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
is	O
very	O
large	O
then	O
φsig	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
is	O
close	O
to	O
1	O
,	O
whereas	O
if	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
is	O
very	O
small	O
then	O
φsig	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
is	O
close	O
to	O
0.	O
recall	B
that	O
the	O
prediction	O
of	O
the	O
halfspace	B
corresponding	O
to	O
a	O
vector	O
w	O
is	O
sign	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
.	O
therefore	O
,	O
the	O
predictions	O
of	O
the	O
halfspace	B
hypothesis	O
and	O
the	O
logistic	O
hypothesis	O
are	O
very	O
similar	O
whenever	O
|	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
|	O
is	O
large	O
.	O
however	O
,	O
when	O
|	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
|	O
is	O
close	O
to	O
0	O
we	O
have	O
that	O
φsig	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
≈	O
1	O
2	O
.	O
intuitively	O
,	O
the	O
logistic	O
hypothesis	O
is	O
not	O
sure	O
about	O
the	O
value	O
of	O
the	O
label	B
so	O
it	O
guesses	O
that	O
the	O
label	B
is	O
sign	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
with	O
probability	O
slightly	O
larger	O
than	O
50	O
%	O
.	O
in	O
contrast	O
,	O
the	O
halfspace	B
hypothesis	O
always	O
outputs	O
a	O
deterministic	O
prediction	O
of	O
either	O
1	O
or	O
−1	O
,	O
even	O
if	O
|	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
|	O
is	O
very	O
close	O
to	O
0.	O
next	O
,	O
we	O
need	O
to	O
specify	O
a	O
loss	B
function	I
.	O
that	O
is	O
,	O
we	O
should	O
deﬁne	O
how	O
bad	O
it	O
is	O
to	O
predict	O
some	O
hw	O
(	O
x	O
)	O
∈	O
[	O
0	O
,	O
1	O
]	O
given	O
that	O
the	O
true	O
label	O
is	O
y	O
∈	O
{	O
±1	O
}	O
.	O
clearly	O
,	O
we	O
would	O
like	O
that	O
hw	O
(	O
x	O
)	O
would	O
be	O
large	O
if	O
y	O
=	O
1	O
and	O
that	O
1	O
−	O
hw	O
(	O
x	O
)	O
(	O
i.e.	O
,	O
the	O
probability	O
of	O
predicting	O
−1	O
)	O
would	O
be	O
large	O
if	O
y	O
=	O
−1	O
.	O
note	O
that	O
1	O
−	O
hw	O
(	O
x	O
)	O
=	O
1	O
−	O
1	O
1	O
+	O
exp	O
(	O
−	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
exp	O
(	O
−	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
1	O
+	O
exp	O
(	O
−	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
=	O
=	O
1	O
1	O
+	O
exp	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
.	O
therefore	O
,	O
any	O
reasonable	O
loss	B
function	I
would	O
increase	O
monotonically	O
with	O
or	O
equivalently	O
,	O
would	O
increase	O
monotonically	O
with	O
1	O
+	O
exp	O
(	O
−y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
.	O
the	O
lo-	O
gistic	O
loss	B
function	I
used	O
in	O
logistic	B
regression	I
penalizes	O
hw	O
based	O
on	O
the	O
log	O
of	O
1	O
+	O
exp	O
(	O
−y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
(	O
recall	B
that	O
log	O
is	O
a	O
monotonic	O
function	B
)	O
.	O
that	O
is	O
,	O
1	O
1+exp	O
(	O
y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
,	O
(	O
cid:96	O
)	O
(	O
hw	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
log	O
(	O
1	O
+	O
exp	O
(	O
−y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
)	O
.	O
therefore	O
,	O
given	O
a	O
training	B
set	I
s	O
=	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
,	O
the	O
erm	O
problem	O
associated	O
with	O
logistic	B
regression	I
is	O
m	O
(	O
cid:88	O
)	O
i=1	O
argmin	O
w∈rd	O
1	O
m	O
log	O
(	O
1	O
+	O
exp	O
(	O
−yi	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
)	O
)	O
.	O
(	O
9.10	O
)	O
the	O
advantage	O
of	O
the	O
logistic	B
loss	I
function	O
is	O
that	O
it	O
is	O
a	O
convex	B
function	O
with	O
respect	O
to	O
w	O
;	O
hence	O
the	O
erm	O
problem	O
can	O
be	O
solved	O
eﬃciently	O
using	O
standard	O
methods	O
.	O
we	O
will	O
study	O
how	O
to	O
learn	O
with	O
convex	B
functions	O
,	O
and	O
in	O
particular	O
specify	O
a	O
simple	O
algorithm	O
for	O
minimizing	O
convex	B
functions	O
,	O
in	O
later	O
chapters	O
.	O
the	O
erm	O
problem	O
associated	O
with	O
logistic	B
regression	I
(	O
equation	O
(	O
9.10	O
)	O
)	O
is	O
iden-	O
tical	O
to	O
the	O
problem	O
of	O
ﬁnding	O
a	O
maximum	B
likelihood	I
estimator	O
,	O
a	O
well-known	O
statistical	O
approach	O
for	O
ﬁnding	O
the	O
parameters	O
that	O
maximize	O
the	O
joint	O
probabil-	O
ity	O
of	O
a	O
given	O
data	O
set	B
assuming	O
a	O
speciﬁc	O
parametric	O
probability	O
function	B
.	O
we	O
will	O
study	O
the	O
maximum	B
likelihood	I
approach	O
in	O
chapter	O
24	O
.	O
128	O
linear	B
predictors	I
9.4	O
summary	O
the	O
family	O
of	O
linear	B
predictors	I
is	O
one	O
of	O
the	O
most	O
useful	O
families	O
of	O
hypothesis	B
classes	O
,	O
and	O
many	O
learning	O
algorithms	O
that	O
are	O
being	O
widely	O
used	O
in	O
practice	O
rely	O
on	O
linear	B
predictors	I
.	O
we	O
have	O
shown	O
eﬃcient	O
algorithms	O
for	O
learning	O
linear	O
predictors	O
with	O
respect	O
to	O
the	O
zero-one	O
loss	B
in	O
the	O
separable	B
case	O
and	O
with	O
respect	O
to	O
the	O
squared	O
and	O
logistic	O
losses	O
in	O
the	O
unrealizable	O
case	O
.	O
in	O
later	O
chapters	O
we	O
will	O
present	O
the	O
properties	O
of	O
the	O
loss	B
function	I
that	O
enable	O
eﬃcient	O
learning	O
.	O
naturally	O
,	O
linear	B
predictors	I
are	O
eﬀective	O
whenever	O
we	O
assume	O
,	O
as	O
prior	O
knowl-	O
edge	O
,	O
that	O
some	O
linear	B
predictor	I
attains	O
low	O
risk	B
with	O
respect	O
to	O
the	O
underlying	O
distribution	O
.	O
in	O
the	O
next	O
chapter	O
we	O
show	O
how	O
to	O
construct	O
nonlinear	O
predictors	O
by	O
composing	O
linear	B
predictors	I
on	O
top	O
of	O
simple	O
classes	O
.	O
this	O
will	O
enable	O
us	O
to	O
employ	O
linear	B
predictors	I
for	O
a	O
variety	O
of	O
prior	B
knowledge	I
assumptions	O
.	O
9.5	O
bibliographic	O
remarks	O
the	O
perceptron	O
algorithm	O
dates	O
back	O
to	O
rosenblatt	O
(	O
1958	O
)	O
.	O
the	O
proof	O
of	O
its	O
convergence	O
rate	O
is	O
due	O
to	O
(	O
agmon	O
1954	O
,	O
novikoﬀ	O
1962	O
)	O
.	O
least	B
squares	I
regression	O
goes	O
back	O
to	O
gauss	O
(	O
1795	O
)	O
,	O
legendre	O
(	O
1805	O
)	O
,	O
and	O
adrain	O
(	O
1808	O
)	O
.	O
9.6	O
exercises	O
1.	O
show	O
how	O
to	O
cast	O
the	O
erm	O
problem	O
of	O
linear	B
regression	I
with	O
respect	O
to	O
the	O
absolute	B
value	I
loss	I
function	O
,	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
|h	O
(	O
x	O
)	O
−	O
y|	O
,	O
as	O
a	O
linear	O
program	O
;	O
namely	O
,	O
show	O
how	O
to	O
write	O
the	O
problem	O
m	O
(	O
cid:88	O
)	O
i=1	O
|	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
−	O
yi|	O
min	O
w	O
as	O
a	O
linear	O
program	O
.	O
hint	O
:	O
start	O
with	O
proving	O
that	O
for	O
any	O
c	O
∈	O
r	O
,	O
|c|	O
=	O
min	O
a≥0	O
a	O
s.t	O
.	O
c	O
≤	O
a	O
and	O
c	O
≥	O
−a	O
.	O
2.	O
show	O
that	O
the	O
matrix	O
a	O
deﬁned	O
in	O
equation	O
(	O
9.6	O
)	O
is	O
invertible	O
if	O
and	O
only	O
if	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
span	O
rd	O
.	O
3.	O
show	O
that	O
theorem	O
9.1	O
is	O
tight	O
in	O
the	O
following	O
sense	O
:	O
for	O
any	O
positive	O
integer	O
m	O
,	O
there	O
exist	O
a	O
vector	O
w∗	O
∈	O
rd	O
(	O
for	O
some	O
appropriate	O
d	O
)	O
and	O
a	O
sequence	O
of	O
examples	O
{	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
}	O
such	O
that	O
the	O
following	O
hold	O
:	O
•	O
r	O
=	O
maxi	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
≤	O
1	O
.	O
•	O
(	O
cid:107	O
)	O
w∗	O
(	O
cid:107	O
)	O
2	O
=	O
m	O
,	O
and	O
for	O
all	O
i	O
≤	O
m	O
,	O
yi	O
(	O
cid:104	O
)	O
xi	O
,	O
w∗	O
(	O
cid:105	O
)	O
≥	O
1.	O
note	O
that	O
,	O
using	O
the	O
notation	O
in	O
theorem	O
9.1	O
,	O
we	O
therefore	O
get	O
b	O
=	O
min	O
{	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
:	O
∀i	O
∈	O
[	O
m	O
]	O
,	O
yi	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
≥	O
1	O
}	O
≤	O
√	O
m.	O
9.6	O
exercises	O
129	O
thus	O
,	O
(	O
br	O
)	O
2	O
≤	O
m.	O
•	O
when	O
running	O
the	O
perceptron	O
on	O
this	O
sequence	O
of	O
examples	O
it	O
makes	O
m	O
updates	O
before	O
converging	O
.	O
hint	O
:	O
choose	O
d	O
=	O
m	O
and	O
for	O
every	O
i	O
choose	O
xi	O
=	O
ei	O
.	O
4	O
.	O
(	O
*	O
)	O
given	O
any	O
number	O
m	O
,	O
ﬁnd	O
an	O
example	O
of	O
a	O
sequence	O
of	O
labeled	O
examples	O
(	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
)	O
∈	O
(	O
r3	O
×	O
{	O
−1	O
,	O
+1	O
}	O
)	O
m	O
on	O
which	O
the	O
upper	O
bound	O
of	O
theorem	O
9.1	O
equals	O
m	O
and	O
the	O
perceptron	O
algorithm	O
is	O
bound	O
to	O
make	O
m	O
mistakes	O
.	O
hint	O
:	O
set	B
each	O
xi	O
to	O
be	O
a	O
third	O
dimensional	O
vector	O
of	O
the	O
form	O
(	O
a	O
,	O
b	O
,	O
yi	O
)	O
,	O
where	O
a2	O
+	O
b2	O
=	O
r2	O
−	O
1.	O
let	O
w∗	O
be	O
the	O
vector	O
(	O
0	O
,	O
0	O
,	O
1	O
)	O
.	O
now	O
,	O
go	O
over	O
the	O
proof	O
of	O
the	O
perceptron	O
’	O
s	O
upper	O
bound	O
(	O
theorem	O
9.1	O
)	O
,	O
see	O
where	O
we	O
used	O
inequalities	O
(	O
≤	O
)	O
rather	O
than	O
equalities	O
(	O
=	O
)	O
,	O
and	O
ﬁgure	O
out	O
scenarios	O
where	O
the	O
inequality	O
actually	O
holds	O
with	O
equality	O
.	O
5.	O
suppose	O
we	O
modify	O
the	O
perceptron	O
algorithm	O
as	O
follows	O
:	O
in	O
the	O
update	O
step	O
,	O
instead	O
of	O
performing	O
w	O
(	O
t+1	O
)	O
=	O
w	O
(	O
t	O
)	O
+	O
yixi	O
whenever	O
we	O
make	O
a	O
mistake	O
,	O
we	O
perform	O
w	O
(	O
t+1	O
)	O
=	O
w	O
(	O
t	O
)	O
+	O
ηyixi	O
for	O
some	O
η	O
>	O
0.	O
prove	O
that	O
the	O
modiﬁed	O
per-	O
ceptron	O
will	O
perform	O
the	O
same	O
number	O
of	O
iterations	O
as	O
the	O
vanilla	O
perceptron	O
and	O
will	O
converge	O
to	O
a	O
vector	O
that	O
points	O
to	O
the	O
same	O
direction	O
as	O
the	O
output	O
of	O
the	O
vanilla	O
perceptron	O
.	O
6.	O
in	O
this	O
problem	O
,	O
we	O
will	O
get	O
bounds	O
on	O
the	O
vc-dimension	O
of	O
the	O
class	O
of	O
(	O
closed	O
)	O
balls	O
in	O
rd	O
,	O
that	O
is	O
,	O
bd	O
=	O
{	O
bv	O
,	O
r	O
:	O
v	O
∈	O
rd	O
,	O
r	O
>	O
0	O
}	O
,	O
where	O
bv	O
,	O
r	O
(	O
x	O
)	O
=	O
if	O
(	O
cid:107	O
)	O
x	O
−	O
v	O
(	O
cid:107	O
)	O
≤	O
r	O
otherwise	O
.	O
(	O
cid:26	O
)	O
1	O
0	O
1.	O
consider	O
the	O
mapping	O
φ	O
:	O
rd	O
→	O
rd+1	O
deﬁned	O
by	O
φ	O
(	O
x	O
)	O
=	O
(	O
x	O
,	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
)	O
.	O
show	O
that	O
if	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
are	O
shattered	O
by	O
bd	O
then	O
φ	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
φ	O
(	O
xm	O
)	O
are	O
shattered	O
by	O
the	O
class	O
of	O
halfspaces	O
in	O
rd+1	O
(	O
in	O
this	O
question	O
we	O
assume	O
that	O
sign	O
(	O
0	O
)	O
=	O
1	O
)	O
.	O
what	O
does	O
this	O
tell	O
us	O
about	O
vcdim	O
(	O
bd	O
)	O
?	O
2	O
.	O
(	O
*	O
)	O
find	O
a	O
set	B
of	O
d	O
+	O
1	O
points	O
in	O
rd	O
that	O
is	O
shattered	O
by	O
bd	O
.	O
conclude	O
that	O
d	O
+	O
1	O
≤	O
vcdim	O
(	O
bd	O
)	O
≤	O
d	O
+	O
2	O
.	O
10	O
boosting	B
boosting	O
is	O
an	O
algorithmic	O
paradigm	O
that	O
grew	O
out	O
of	O
a	O
theoretical	O
question	O
and	O
became	O
a	O
very	O
practical	O
machine	O
learning	O
tool	O
.	O
the	O
boosting	B
approach	O
uses	O
a	O
generalization	O
of	O
linear	B
predictors	I
to	O
address	O
two	O
major	O
issues	O
that	O
have	O
been	O
raised	O
earlier	O
in	O
the	O
book	O
.	O
the	O
ﬁrst	O
is	O
the	O
bias-complexity	B
tradeoﬀ	I
.	O
we	O
have	O
seen	O
(	O
in	O
chapter	O
5	O
)	O
that	O
the	O
error	O
of	O
an	O
erm	O
learner	O
can	O
be	O
decomposed	O
into	O
a	O
sum	O
of	O
approximation	B
error	I
and	O
estimation	B
error	I
.	O
the	O
more	O
expressive	O
the	O
hypothesis	B
class	I
the	O
learner	O
is	O
searching	O
over	O
,	O
the	O
smaller	O
the	O
approximation	B
error	I
is	O
,	O
but	O
the	O
larger	O
the	O
estimation	B
error	I
becomes	O
.	O
a	O
learner	O
is	O
thus	O
faced	O
with	O
the	O
problem	O
of	O
picking	O
a	O
good	O
tradeoﬀ	O
between	O
these	O
two	O
considerations	O
.	O
the	O
boosting	B
paradigm	O
allows	O
the	O
learner	O
to	O
have	O
smooth	O
control	O
over	O
this	O
tradeoﬀ	O
.	O
the	O
learning	O
starts	O
with	O
a	O
basic	O
class	O
(	O
that	O
might	O
have	O
a	O
large	O
approximation	B
error	I
)	O
,	O
and	O
as	O
it	O
progresses	O
the	O
class	O
that	O
the	O
predictor	B
may	O
belong	O
to	O
grows	O
richer	O
.	O
the	O
second	O
issue	O
that	O
boosting	B
addresses	O
is	O
the	O
computational	B
complexity	I
of	O
learning	O
.	O
as	O
seen	O
in	O
chapter	O
8	O
,	O
for	O
many	O
interesting	O
concept	O
classes	O
the	O
task	O
of	O
ﬁnding	O
an	O
erm	O
hypothesis	B
may	O
be	O
computationally	O
infeasible	O
.	O
a	O
boosting	B
algorithm	O
ampliﬁes	O
the	O
accuracy	B
of	O
weak	O
learners	O
.	O
intuitively	O
,	O
one	O
can	O
think	O
of	O
a	O
weak	O
learner	O
as	O
an	O
algorithm	O
that	O
uses	O
a	O
simple	O
“	O
rule	O
of	O
thumb	O
”	O
to	O
output	O
a	O
hypothesis	B
that	O
comes	O
from	O
an	O
easy-to-learn	O
hypothesis	B
class	I
and	O
performs	O
just	O
slightly	O
better	O
than	O
a	O
random	O
guess	O
.	O
when	O
a	O
weak	O
learner	O
can	O
be	O
implemented	O
eﬃciently	O
,	O
boosting	B
provides	O
a	O
tool	O
for	O
aggregating	O
such	O
weak	O
hypotheses	O
to	O
approximate	O
gradually	O
good	O
predictors	O
for	O
larger	O
,	O
and	O
harder	O
to	O
learn	O
,	O
classes	O
.	O
in	O
this	O
chapter	O
we	O
will	O
describe	O
and	O
analyze	O
a	O
practically	O
useful	O
boosting	B
algo-	O
rithm	O
,	O
adaboost	O
(	O
a	O
shorthand	O
for	O
adaptive	O
boosting	B
)	O
.	O
the	O
adaboost	O
algorithm	O
outputs	O
a	O
hypothesis	B
that	O
is	O
a	O
linear	O
combination	O
of	O
simple	O
hypotheses	O
.	O
in	O
other	O
words	O
,	O
adaboost	O
relies	O
on	O
the	O
family	O
of	O
hypothesis	B
classes	O
obtained	O
by	O
composing	O
a	O
linear	B
predictor	I
on	O
top	O
of	O
simple	O
classes	O
.	O
we	O
will	O
show	O
that	O
adaboost	O
enables	O
us	O
to	O
control	O
the	O
tradeoﬀ	O
between	O
the	O
approximation	O
and	O
estimation	O
errors	O
by	O
varying	O
a	O
single	O
parameter	O
.	O
adaboost	O
demonstrates	O
a	O
general	O
theme	O
,	O
that	O
will	O
recur	O
later	O
in	O
the	O
book	O
,	O
of	O
expanding	O
the	O
expressiveness	O
of	O
linear	B
predictors	I
by	O
composing	O
them	O
on	O
top	O
of	O
other	O
functions	O
.	O
this	O
will	O
be	O
elaborated	O
in	O
section	O
10.3.	O
adaboost	O
stemmed	O
from	O
the	O
theoretical	O
question	O
of	O
whether	O
an	O
eﬃcient	O
weak	O
learner	O
can	O
be	O
“	O
boosted	O
”	O
into	O
an	O
eﬃcient	O
strong	O
learner	O
.	O
this	O
question	O
was	O
raised	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
10.1	O
weak	O
learnability	O
131	O
by	O
kearns	O
and	O
valiant	O
in	O
1988	O
and	O
solved	O
in	O
1990	O
by	O
robert	O
schapire	O
,	O
then	O
a	O
graduate	O
student	O
at	O
mit	O
.	O
however	O
,	O
the	O
proposed	O
mechanism	O
was	O
not	O
very	O
practical	O
.	O
in	O
1995	O
,	O
robert	O
schapire	O
and	O
yoav	O
freund	O
proposed	O
the	O
adaboost	O
algorithm	O
,	O
which	O
was	O
the	O
ﬁrst	O
truly	O
practical	O
implementation	O
of	O
boosting	B
.	O
this	O
simple	O
and	O
elegant	O
algorithm	O
became	O
hugely	O
popular	O
,	O
and	O
freund	O
and	O
schapire	O
’	O
s	O
work	O
has	O
been	O
recognized	O
by	O
numerous	O
awards	O
.	O
furthermore	O
,	O
boosting	B
is	O
a	O
great	O
example	O
for	O
the	O
practical	O
impact	O
of	O
learning	O
theory	O
.	O
while	O
boosting	B
originated	O
as	O
a	O
purely	O
theoretical	O
problem	O
,	O
it	O
has	O
led	O
to	O
popular	O
and	O
widely	O
used	O
algorithms	O
.	O
indeed	O
,	O
as	O
we	O
shall	O
demonstrate	O
later	O
in	O
this	O
chapter	O
,	O
adaboost	O
has	O
been	O
successfully	O
used	O
for	O
learning	O
to	O
detect	O
faces	O
in	O
images	O
.	O
10.1	O
weak	O
learnability	O
recall	B
the	O
deﬁnition	O
of	O
pac	O
learning	O
given	O
in	O
chapter	O
3	O
:	O
a	O
hypothesis	B
class	I
,	O
h	O
,	O
is	O
pac	O
learnable	O
if	O
there	O
exist	O
mh	O
:	O
(	O
0	O
,	O
1	O
)	O
2	O
→	O
n	O
and	O
a	O
learning	O
algorithm	O
with	O
the	O
following	O
property	O
:	O
for	O
every	O
	O
,	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
for	O
every	O
distribution	O
d	O
over	O
x	O
,	O
and	O
for	O
every	O
labeling	O
function	B
f	O
:	O
x	O
→	O
{	O
±1	O
}	O
,	O
if	O
the	O
realizable	O
assumption	O
holds	O
with	O
respect	O
to	O
h	O
,	O
d	O
,	O
f	O
,	O
then	O
when	O
running	O
the	O
learning	O
algorithm	O
on	O
m	O
≥	O
mh	O
(	O
	O
,	O
δ	O
)	O
i.i.d	O
.	O
examples	O
generated	O
by	O
d	O
and	O
labeled	O
by	O
f	O
,	O
the	O
algorithm	O
returns	O
a	O
hypothesis	B
h	O
such	O
that	O
,	O
with	O
probability	O
of	O
at	O
least	O
1−δ	O
,	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
h	O
)	O
≤	O
	O
.	O
furthermore	O
,	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
(	O
theorem	O
6.8	O
in	O
chapter	O
6	O
)	O
characterizes	O
the	O
family	O
of	O
learnable	O
classes	O
and	O
states	O
that	O
every	O
pac	O
learnable	O
class	O
can	O
be	O
learned	O
using	O
any	O
erm	O
algorithm	O
.	O
however	O
,	O
the	O
deﬁnition	O
of	O
pac	O
learning	O
and	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
ignores	O
the	O
computational	O
aspect	O
of	O
learning	O
.	O
indeed	O
,	O
as	O
we	O
have	O
shown	O
in	O
chapter	O
8	O
,	O
there	O
are	O
cases	O
in	O
which	O
implementing	O
the	O
erm	O
rule	O
is	O
computationally	O
hard	O
(	O
even	O
in	O
the	O
realizable	O
case	O
)	O
.	O
however	O
,	O
perhaps	O
we	O
can	O
trade	O
computational	O
hardness	O
with	O
the	O
requirement	O
for	O
accuracy	B
.	O
given	O
a	O
distribution	O
d	O
and	O
a	O
target	O
labeling	O
function	B
f	O
,	O
maybe	O
there	O
exists	O
an	O
eﬃciently	O
computable	O
learning	O
algorithm	O
whose	O
error	O
is	O
just	O
slightly	O
better	O
than	O
a	O
random	O
guess	O
?	O
this	O
motivates	O
the	O
following	O
deﬁnition	O
.	O
definition	O
10.1	O
(	O
γ-weak-learnability	O
)	O
•	O
a	O
learning	O
algorithm	O
,	O
a	O
,	O
is	O
a	O
γ-weak-learner	O
for	O
a	O
class	O
h	O
if	O
there	O
exists	O
a	O
func-	O
tion	O
mh	O
:	O
(	O
0	O
,	O
1	O
)	O
→	O
n	O
such	O
that	O
for	O
every	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
for	O
every	O
distribution	O
d	O
over	O
x	O
,	O
and	O
for	O
every	O
labeling	O
function	B
f	O
:	O
x	O
→	O
{	O
±1	O
}	O
,	O
if	O
the	O
realizable	O
assumption	O
holds	O
with	O
respect	O
to	O
h	O
,	O
d	O
,	O
f	O
,	O
then	O
when	O
running	O
the	O
learning	O
algorithm	O
on	O
m	O
≥	O
mh	O
(	O
δ	O
)	O
i.i.d	O
.	O
examples	O
generated	O
by	O
d	O
and	O
labeled	O
by	O
f	O
,	O
the	O
algorithm	O
returns	O
a	O
hypothesis	B
h	O
such	O
that	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
,	O
l	O
(	O
d	O
,	O
f	O
)	O
(	O
h	O
)	O
≤	O
1/2	O
−	O
γ	O
.	O
•	O
a	O
hypothesis	B
class	I
h	O
is	O
γ-weak-learnable	O
if	O
there	O
exists	O
a	O
γ-weak-learner	O
for	O
that	O
class	O
.	O
132	O
boosting	B
this	O
deﬁnition	O
is	O
almost	O
identical	O
to	O
the	O
deﬁnition	O
of	O
pac	O
learning	O
,	O
which	O
here	O
we	O
will	O
call	O
strong	B
learning	I
,	O
with	O
one	O
crucial	O
diﬀerence	O
:	O
strong	O
learnability	O
implies	O
the	O
ability	O
to	O
ﬁnd	O
an	O
arbitrarily	O
good	O
classiﬁer	B
(	O
with	O
error	O
rate	O
at	O
most	O
	O
for	O
an	O
arbitrarily	O
small	O
	O
>	O
0	O
)	O
.	O
in	O
weak	O
learnability	O
,	O
however	O
,	O
we	O
only	O
need	O
to	O
output	O
a	O
hypothesis	B
whose	O
error	O
rate	O
is	O
at	O
most	O
1/2	O
−	O
γ	O
,	O
namely	O
,	O
whose	O
error	O
rate	O
is	O
slightly	O
better	O
than	O
what	O
a	O
random	O
labeling	O
would	O
give	O
us	O
.	O
the	O
hope	O
is	O
that	O
it	O
may	O
be	O
easier	O
to	O
come	O
up	O
with	O
eﬃcient	O
weak	O
learners	O
than	O
with	O
eﬃcient	O
(	O
full	O
)	O
pac	O
learners	O
.	O
	O
d+log	O
(	O
1/δ	O
)	O
the	O
fundamental	O
theorem	O
of	O
learning	O
(	O
theorem	O
6.8	O
)	O
states	O
that	O
if	O
a	O
hypothesis	B
class	I
h	O
has	O
a	O
vc	O
dimension	B
d	O
,	O
then	O
the	O
sample	B
complexity	I
of	O
pac	O
learning	O
h	O
satisﬁes	O
mh	O
(	O
	O
,	O
δ	O
)	O
≥	O
c1	O
,	O
where	O
c1	O
is	O
a	O
constant	O
.	O
applying	O
this	O
with	O
	O
=	O
1/2−γ	O
we	O
immediately	O
obtain	O
that	O
if	O
d	O
=	O
∞	O
then	O
h	O
is	O
not	O
γ-weak-learnable	O
.	O
this	O
implies	O
that	O
from	O
the	O
statistical	O
perspective	O
(	O
i.e.	O
,	O
if	O
we	O
ignore	O
computational	B
complexity	I
)	O
,	O
weak	O
learnability	O
is	O
also	O
characterized	O
by	O
the	O
vc	O
dimension	B
of	O
h	O
and	O
therefore	O
is	O
just	O
as	O
hard	O
as	O
pac	O
(	O
strong	O
)	O
learning	O
.	O
however	O
,	O
when	O
we	O
do	O
consider	O
computational	B
complexity	I
,	O
the	O
potential	O
advantage	O
of	O
weak	B
learning	I
is	O
that	O
maybe	O
there	O
is	O
an	O
algorithm	O
that	O
satisﬁes	O
the	O
requirements	O
of	O
weak	B
learning	I
and	O
can	O
be	O
implemented	O
eﬃciently	O
.	O
one	O
possible	O
approach	O
is	O
to	O
take	O
a	O
“	O
simple	O
”	O
hypothesis	B
class	I
,	O
denoted	O
b	O
,	O
and	O
to	O
apply	O
erm	O
with	O
respect	O
to	O
b	O
as	O
the	O
weak	B
learning	I
algorithm	O
.	O
for	O
this	O
to	O
work	O
,	O
we	O
need	O
that	O
b	O
will	O
satisfy	O
two	O
requirements	O
:	O
•	O
ermb	O
is	O
eﬃciently	O
implementable	O
.	O
•	O
for	O
every	O
sample	O
that	O
is	O
labeled	O
by	O
some	O
hypothesis	B
from	O
h	O
,	O
any	O
ermb	O
hypothesis	B
will	O
have	O
an	O
error	O
of	O
at	O
most	O
1/2	O
−	O
γ.	O
then	O
,	O
the	O
immediate	O
question	O
is	O
whether	O
we	O
can	O
boost	O
an	O
eﬃcient	O
weak	O
learner	O
into	O
an	O
eﬃcient	O
strong	O
learner	O
.	O
in	O
the	O
next	O
section	O
we	O
will	O
show	O
that	O
this	O
is	O
indeed	O
possible	O
,	O
but	O
before	O
that	O
,	O
let	O
us	O
show	O
an	O
example	O
in	O
which	O
eﬃcient	O
weak	O
learnability	O
of	O
a	O
class	O
h	O
is	O
possible	O
using	O
a	O
base	B
hypothesis	I
class	O
b.	O
example	O
10.1	O
(	O
weak	B
learning	I
of	O
3-piece	O
classiﬁers	O
using	O
decision	B
stumps	I
)	O
let	O
x	O
=	O
r	O
and	O
let	O
h	O
be	O
the	O
class	O
of	O
3-piece	O
classiﬁers	O
,	O
namely	O
,	O
h	O
=	O
{	O
hθ1	O
,	O
θ2	O
,	O
b	O
:	O
θ1	O
,	O
θ2	O
∈	O
r	O
,	O
θ1	O
<	O
θ2	O
,	O
b	O
∈	O
{	O
±1	O
}	O
}	O
,	O
where	O
for	O
every	O
x	O
,	O
(	O
cid:40	O
)	O
hθ1	O
,	O
θ2	O
,	O
b	O
(	O
x	O
)	O
=	O
+b	O
−b	O
if	O
x	O
<	O
θ1	O
or	O
x	O
>	O
θ2	O
if	O
θ1	O
≤	O
x	O
≤	O
θ2	O
an	O
example	O
hypothesis	B
(	O
for	O
b	O
=	O
1	O
)	O
is	O
illustrated	O
as	O
follows	O
:	O
+	O
θ1	O
−	O
+	O
θ2	O
let	O
b	O
be	O
the	O
class	O
of	O
decision	B
stumps	I
,	O
that	O
is	O
,	O
b	O
=	O
{	O
x	O
(	O
cid:55	O
)	O
→	O
sign	O
(	O
x−	O
θ	O
)	O
·	O
b	O
:	O
θ	O
∈	O
r	O
,	O
b	O
∈	O
{	O
±1	O
}	O
}	O
.	O
in	O
the	O
following	O
we	O
show	O
that	O
ermb	O
is	O
a	O
γ-weak	O
learner	O
for	O
h	O
,	O
for	O
γ	O
=	O
1/12	O
.	O
10.1	O
weak	O
learnability	O
133	O
to	O
see	O
that	O
,	O
we	O
ﬁrst	O
show	O
that	O
for	O
every	O
distribution	O
that	O
is	O
consistent	O
with	O
h	O
,	O
there	O
exists	O
a	O
decision	O
stump	O
with	O
ld	O
(	O
h	O
)	O
≤	O
1/3	O
.	O
indeed	O
,	O
just	O
note	O
that	O
every	O
classiﬁer	B
in	O
h	O
consists	O
of	O
three	O
regions	O
(	O
two	O
unbounded	O
rays	O
and	O
a	O
center	O
interval	O
)	O
with	O
alternate	O
labels	O
.	O
for	O
any	O
pair	O
of	O
such	O
regions	O
,	O
there	O
exists	O
a	O
decision	O
stump	O
that	O
agrees	O
with	O
the	O
labeling	O
of	O
these	O
two	O
components	O
.	O
note	O
that	O
for	O
every	O
distribution	O
d	O
over	O
r	O
and	O
every	O
partitioning	O
of	O
the	O
line	O
into	O
three	O
such	O
regions	O
,	O
one	O
of	O
these	O
regions	O
must	O
have	O
d-weight	O
of	O
at	O
most	O
1/3	O
.	O
let	O
h	O
∈	O
h	O
be	O
a	O
zero	O
error	O
hypothesis	O
.	O
a	O
decision	O
stump	O
that	O
disagrees	O
with	O
h	O
only	O
on	O
such	O
a	O
region	O
has	O
an	O
error	O
of	O
at	O
most	O
1/3	O
.	O
finally	O
,	O
since	O
the	O
vc-dimension	O
of	O
decision	B
stumps	I
is	O
2	O
,	O
if	O
the	O
sample	O
size	O
is	O
greater	O
than	O
ω	O
(	O
log	O
(	O
1/δ	O
)	O
/2	O
)	O
,	O
then	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
,	O
the	O
ermb	O
rule	O
returns	O
a	O
hypothesis	B
with	O
an	O
error	O
of	O
at	O
most	O
1/3	O
+	O
	O
.	O
setting	O
	O
=	O
1/12	O
we	O
obtain	O
that	O
the	O
error	O
of	O
ermb	O
is	O
at	O
most	O
1/3	O
+	O
1/12	O
=	O
1/2	O
−	O
1/12	O
.	O
we	O
see	O
that	O
ermb	O
is	O
a	O
γ-weak	O
learner	O
for	O
h.	O
we	O
next	O
show	O
how	O
to	O
implement	O
the	O
erm	O
rule	O
eﬃciently	O
for	O
decision	B
stumps	I
.	O
10.1.1	O
eﬃcient	O
implementation	O
of	O
erm	O
for	O
decision	B
stumps	I
let	O
x	O
=	O
rd	O
and	O
consider	O
the	O
base	B
hypothesis	I
class	O
of	O
decision	B
stumps	I
over	O
rd	O
,	O
namely	O
,	O
hds	O
=	O
{	O
x	O
(	O
cid:55	O
)	O
→	O
sign	O
(	O
θ	O
−	O
xi	O
)	O
·	O
b	O
:	O
θ	O
∈	O
r	O
,	O
i	O
∈	O
[	O
d	O
]	O
,	O
b	O
∈	O
{	O
±1	O
}	O
}	O
.	O
for	O
simplicity	O
,	O
assume	O
that	O
b	O
=	O
1	O
;	O
that	O
is	O
,	O
we	O
focus	O
on	O
all	O
the	O
hypotheses	O
in	O
hds	O
of	O
the	O
form	O
sign	O
(	O
θ	O
−	O
xi	O
)	O
.	O
let	O
s	O
=	O
(	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
)	O
be	O
a	O
training	B
set	I
.	O
we	O
will	O
show	O
how	O
to	O
implement	O
an	O
erm	O
rule	O
,	O
namely	O
,	O
how	O
to	O
ﬁnd	O
a	O
decision	O
stump	O
that	O
minimizes	O
ls	O
(	O
h	O
)	O
.	O
furthermore	O
,	O
since	O
in	O
the	O
next	O
section	O
we	O
will	O
show	O
that	O
adaboost	O
requires	O
ﬁnding	O
a	O
hypothesis	B
with	O
a	O
small	O
risk	B
relative	O
to	O
some	O
distribution	O
over	O
s	O
,	O
we	O
will	O
show	O
here	O
how	O
to	O
minimize	O
such	O
risk	B
functions	O
.	O
concretely	O
,	O
let	O
d	O
be	O
a	O
probability	O
vector	O
in	O
rm	O
(	O
that	O
is	O
,	O
all	O
elements	O
of	O
d	O
are	O
i	O
di	O
=	O
1	O
)	O
.	O
the	O
weak	O
learner	O
we	O
describe	O
later	O
receives	O
d	O
and	O
s	O
and	O
outputs	O
a	O
decision	O
stump	O
h	O
:	O
x	O
→	O
y	O
that	O
minimizes	O
the	O
risk	B
w.r.t	O
.	O
d	O
,	O
nonnegative	O
and	O
(	O
cid:80	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
ld	O
(	O
h	O
)	O
=	O
di1	O
[	O
h	O
(	O
xi	O
)	O
(	O
cid:54	O
)	O
=yi	O
]	O
.	O
note	O
that	O
if	O
d	O
=	O
(	O
1/m	O
,	O
.	O
.	O
.	O
,	O
1/m	O
)	O
then	O
ld	O
(	O
h	O
)	O
=	O
ls	O
(	O
h	O
)	O
.	O
recall	B
that	O
each	O
decision	O
stump	O
is	O
parameterized	O
by	O
an	O
index	O
j	O
∈	O
[	O
d	O
]	O
and	O
a	O
threshold	O
θ.	O
therefore	O
,	O
minimizing	O
ld	O
(	O
h	O
)	O
amounts	O
to	O
solving	O
the	O
problem	O
	O
(	O
cid:88	O
)	O
i	O
:	O
yi=1	O
(	O
cid:88	O
)	O
i	O
:	O
yi=−1	O
	O
.	O
min	O
j∈	O
[	O
d	O
]	O
min	O
θ∈r	O
di1	O
[	O
xi	O
,	O
j	O
>	O
θ	O
]	O
+	O
di1	O
[	O
xi	O
,	O
j≤θ	O
]	O
(	O
10.1	O
)	O
fix	O
j	O
∈	O
[	O
d	O
]	O
and	O
let	O
us	O
sort	O
the	O
examples	O
so	O
that	O
x1	O
,	O
j	O
≤	O
x2	O
,	O
j	O
≤	O
.	O
.	O
.	O
≤	O
xm	O
,	O
j	O
.	O
deﬁne	O
:	O
i	O
∈	O
[	O
m−	O
1	O
]	O
}	O
∪	O
{	O
(	O
x1	O
,	O
j	O
−	O
1	O
)	O
,	O
(	O
xm	O
,	O
j	O
+	O
1	O
)	O
}	O
.	O
note	O
that	O
for	O
any	O
θ	O
∈	O
r	O
θj	O
=	O
{	O
xi	O
,	O
j	O
+xi+1	O
,	O
j	O
there	O
exists	O
θ	O
(	O
cid:48	O
)	O
∈	O
θj	O
that	O
yields	O
the	O
same	O
predictions	O
for	O
the	O
sample	O
s	O
as	O
the	O
2	O
134	O
boosting	B
threshold	O
θ.	O
therefore	O
,	O
instead	O
of	O
minimizing	O
over	O
θ	O
∈	O
r	O
we	O
can	O
minimize	O
over	O
θ	O
∈	O
θj	O
.	O
this	O
already	O
gives	O
us	O
an	O
eﬃcient	O
procedure	O
:	O
choose	O
j	O
∈	O
[	O
d	O
]	O
and	O
θ	O
∈	O
θj	O
that	O
minimize	O
the	O
objective	O
value	O
of	O
equation	O
(	O
10.1	O
)	O
.	O
for	O
every	O
j	O
and	O
θ	O
∈	O
θj	O
we	O
have	O
to	O
calculate	O
a	O
sum	O
over	O
m	O
examples	O
;	O
therefore	O
the	O
runtime	O
of	O
this	O
approach	O
would	O
be	O
o	O
(	O
dm2	O
)	O
.	O
we	O
next	O
show	O
a	O
simple	O
trick	O
that	O
enables	O
us	O
to	O
minimize	O
the	O
objective	O
in	O
time	O
o	O
(	O
dm	O
)	O
.	O
the	O
observation	O
is	O
as	O
follows	O
.	O
suppose	O
we	O
have	O
calculated	O
the	O
objective	O
for	O
θ	O
∈	O
(	O
xi−1	O
,	O
j	O
,	O
xi	O
,	O
j	O
)	O
.	O
let	O
f	O
(	O
θ	O
)	O
be	O
the	O
value	O
of	O
the	O
objective	O
.	O
then	O
,	O
when	O
we	O
consider	O
θ	O
(	O
cid:48	O
)	O
∈	O
(	O
xi	O
,	O
j	O
,	O
xi+1	O
,	O
j	O
)	O
we	O
have	O
that	O
f	O
(	O
θ	O
(	O
cid:48	O
)	O
)	O
=	O
f	O
(	O
θ	O
)	O
−	O
di1	O
[	O
yi=1	O
]	O
+	O
di1	O
[	O
yi=−1	O
]	O
=	O
f	O
(	O
θ	O
)	O
−	O
yidi	O
.	O
therefore	O
,	O
we	O
can	O
calculate	O
the	O
objective	O
at	O
θ	O
(	O
cid:48	O
)	O
in	O
a	O
constant	O
time	O
,	O
given	O
the	O
objective	O
at	O
the	O
previous	O
threshold	O
,	O
θ.	O
it	O
follows	O
that	O
after	O
a	O
preprocessing	O
step	O
in	O
which	O
we	O
sort	O
the	O
examples	O
with	O
respect	O
to	O
each	O
coordinate	O
,	O
the	O
minimization	O
problem	O
can	O
be	O
performed	O
in	O
time	O
o	O
(	O
dm	O
)	O
.	O
this	O
yields	O
the	O
following	O
pseudocode	O
.	O
erm	O
for	O
decision	B
stumps	I
input	O
:	O
training	B
set	I
s	O
=	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
distribution	O
vector	O
d	O
goal	O
:	O
find	O
j	O
(	O
cid:63	O
)	O
,	O
θ	O
(	O
cid:63	O
)	O
that	O
solve	O
equation	O
(	O
10.1	O
)	O
initialize	O
:	O
f	O
(	O
cid:63	O
)	O
=	O
∞	O
for	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
sort	O
s	O
using	O
the	O
j	O
’	O
th	O
coordinate	O
,	O
and	O
denote	O
x1	O
,	O
j	O
≤	O
x2	O
,	O
j	O
≤	O
···	O
≤	O
xm	O
,	O
j	O
≤	O
xm+1	O
,	O
j	O
def=	O
xm	O
,	O
j	O
+	O
1	O
f	O
=	O
(	O
cid:80	O
)	O
i	O
:	O
yi=1	O
di	O
if	O
f	O
<	O
f	O
(	O
cid:63	O
)	O
f	O
(	O
cid:63	O
)	O
=	O
f	O
,	O
θ	O
(	O
cid:63	O
)	O
=	O
x1	O
,	O
j	O
−	O
1	O
,	O
j	O
(	O
cid:63	O
)	O
=	O
j	O
for	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
f	O
=	O
f	O
−	O
yidi	O
if	O
f	O
<	O
f	O
(	O
cid:63	O
)	O
and	O
xi	O
,	O
j	O
(	O
cid:54	O
)	O
=	O
xi+1	O
,	O
j	O
f	O
(	O
cid:63	O
)	O
=	O
f	O
,	O
θ	O
(	O
cid:63	O
)	O
=	O
1	O
2	O
(	O
xi	O
,	O
j	O
+	O
xi+1	O
,	O
j	O
)	O
,	O
j	O
(	O
cid:63	O
)	O
=	O
j	O
output	O
j	O
(	O
cid:63	O
)	O
,	O
θ	O
(	O
cid:63	O
)	O
10.2	O
adaboost	O
adaboost	O
(	O
short	O
for	O
adaptive	O
boosting	B
)	O
is	O
an	O
algorithm	O
that	O
has	O
access	O
to	O
a	O
weak	O
learner	O
and	O
ﬁnds	O
a	O
hypothesis	B
with	O
a	O
low	O
empirical	B
risk	I
.	O
the	O
adaboost	O
algorithm	O
receives	O
as	O
input	O
a	O
training	B
set	I
of	O
examples	O
s	O
=	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
,	O
where	O
for	O
each	O
i	O
,	O
yi	O
=	O
f	O
(	O
xi	O
)	O
for	O
some	O
labeling	O
function	B
f	O
.	O
the	O
boosting	B
process	O
proceeds	O
in	O
a	O
sequence	O
of	O
consecutive	O
rounds	O
.	O
at	O
round	O
t	O
,	O
the	O
booster	O
ﬁrst	O
deﬁnes	O
10.2	O
adaboost	O
135	O
m	O
(	O
cid:88	O
)	O
(	O
cid:80	O
)	O
m	O
i=1	O
d	O
(	O
t	O
)	O
a	O
distribution	O
over	O
the	O
examples	O
in	O
s	O
,	O
denoted	O
d	O
(	O
t	O
)	O
.	O
that	O
is	O
,	O
d	O
(	O
t	O
)	O
∈	O
rm	O
+	O
and	O
i	O
=	O
1.	O
then	O
,	O
the	O
booster	O
passes	O
the	O
distribution	O
d	O
(	O
t	O
)	O
and	O
the	O
sample	O
s	O
to	O
the	O
weak	O
learner	O
.	O
(	O
that	O
way	O
,	O
the	O
weak	O
learner	O
can	O
construct	O
i.i.d	O
.	O
examples	O
according	O
to	O
d	O
(	O
t	O
)	O
and	O
f	O
.	O
)	O
the	O
weak	O
learner	O
is	O
assumed	O
to	O
return	O
a	O
“	O
weak	O
”	O
hypothesis	B
,	O
ht	O
,	O
whose	O
error	O
,	O
t	O
def=	O
ld	O
(	O
t	O
)	O
(	O
ht	O
)	O
def=	O
d	O
(	O
t	O
)	O
i	O
1	O
[	O
ht	O
(	O
xi	O
)	O
(	O
cid:54	O
)	O
=yi	O
]	O
,	O
i=1	O
(	O
cid:17	O
)	O
2−γ	O
(	O
of	O
course	O
,	O
there	O
is	O
a	O
probability	O
of	O
at	O
most	O
δ	O
that	O
the	O
weak	O
learner	O
−	O
1	O
is	O
at	O
most	O
1	O
fails	O
)	O
.	O
then	O
,	O
adaboost	O
assigns	O
a	O
weight	O
for	O
ht	O
as	O
follows	O
:	O
wt	O
=	O
1	O
.	O
that	O
is	O
,	O
the	O
weight	O
of	O
ht	O
is	O
inversely	O
proportional	O
to	O
the	O
error	O
of	O
ht	O
.	O
at	O
the	O
end	O
of	O
the	O
round	O
,	O
adaboost	O
updates	O
the	O
distribution	O
so	O
that	O
examples	O
on	O
which	O
ht	O
errs	O
will	O
get	O
a	O
higher	O
probability	O
mass	O
while	O
examples	O
on	O
which	O
ht	O
is	O
correct	O
will	O
get	O
a	O
lower	O
probability	O
mass	O
.	O
intuitively	O
,	O
this	O
will	O
force	O
the	O
weak	O
learner	O
to	O
focus	O
on	O
the	O
problematic	O
examples	O
in	O
the	O
next	O
round	O
.	O
the	O
output	O
of	O
the	O
adaboost	O
algorithm	O
is	O
a	O
“	O
strong	O
”	O
classiﬁer	B
that	O
is	O
based	O
on	O
a	O
weighted	O
sum	O
of	O
all	O
the	O
weak	O
hypotheses	O
.	O
the	O
pseudocode	O
of	O
adaboost	O
is	O
presented	O
in	O
the	O
following	O
.	O
(	O
cid:16	O
)	O
1	O
2	O
log	O
t	O
adaboost	O
input	O
:	O
training	B
set	I
s	O
=	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
weak	O
learner	O
wl	O
number	O
of	O
rounds	O
t	O
invoke	O
weak	O
learner	O
ht	O
=	O
wl	O
(	O
d	O
(	O
t	O
)	O
,	O
s	O
)	O
m	O
,	O
.	O
.	O
.	O
,	O
1	O
m	O
)	O
.	O
initialize	O
d	O
(	O
1	O
)	O
=	O
(	O
1	O
for	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
:	O
compute	O
t	O
=	O
(	O
cid:80	O
)	O
m	O
(	O
cid:16	O
)	O
1	O
i=1	O
d	O
(	O
t	O
)	O
−	O
1	O
(	O
cid:80	O
)	O
m	O
let	O
wt	O
=	O
1	O
2	O
log	O
update	O
d	O
(	O
t+1	O
)	O
(	O
cid:17	O
)	O
=	O
t	O
i	O
i	O
1	O
[	O
yi	O
(	O
cid:54	O
)	O
=ht	O
(	O
xi	O
)	O
]	O
i	O
d	O
(	O
t	O
)	O
j=1	O
d	O
(	O
t	O
)	O
j	O
exp	O
(	O
−wtyiht	O
(	O
xi	O
)	O
)	O
exp	O
(	O
−wtyj	O
ht	O
(	O
xj	O
)	O
)	O
(	O
cid:16	O
)	O
(	O
cid:80	O
)	O
t	O
output	O
the	O
hypothesis	B
hs	O
(	O
x	O
)	O
=	O
sign	O
t=1	O
wtht	O
(	O
x	O
)	O
.	O
for	O
all	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
(	O
cid:17	O
)	O
the	O
following	O
theorem	O
shows	O
that	O
the	O
training	B
error	I
of	O
the	O
output	O
hypothesis	B
decreases	O
exponentially	O
fast	O
with	O
the	O
number	O
of	O
boosting	B
rounds	O
.	O
theorem	O
10.2	O
let	O
s	O
be	O
a	O
training	B
set	I
and	O
assume	O
that	O
at	O
each	O
iteration	O
of	O
adaboost	O
,	O
the	O
weak	O
learner	O
returns	O
a	O
hypothesis	B
for	O
which	O
t	O
≤	O
1/2	O
−	O
γ.	O
then	O
,	O
the	O
training	B
error	I
of	O
the	O
output	O
hypothesis	B
of	O
adaboost	O
is	O
at	O
most	O
m	O
(	O
cid:88	O
)	O
proof	O
for	O
each	O
t	O
,	O
denote	O
ft	O
=	O
(	O
cid:80	O
)	O
ls	O
(	O
hs	O
)	O
=	O
1	O
m	O
i=1	O
1	O
[	O
hs	O
(	O
xi	O
)	O
(	O
cid:54	O
)	O
=yi	O
]	O
≤	O
exp	O
(	O
−2	O
γ2	O
t	O
)	O
.	O
p≤t	O
wphp	O
.	O
therefore	O
,	O
the	O
output	O
of	O
adaboost	O
136	O
boosting	B
is	O
ft	O
.	O
in	O
addition	O
,	O
denote	O
m	O
(	O
cid:88	O
)	O
i=1	O
zt	O
=	O
1	O
m	O
e−yift	O
(	O
xi	O
)	O
.	O
note	O
that	O
for	O
any	O
hypothesis	B
we	O
have	O
that	O
1	O
[	O
h	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=y	O
]	O
≤	O
e−yh	O
(	O
x	O
)	O
.	O
therefore	O
,	O
ls	O
(	O
ft	O
)	O
≤	O
zt	O
,	O
so	O
it	O
suﬃces	O
to	O
show	O
that	O
zt	O
≤	O
e−2γ2t	O
.	O
to	O
upper	O
bound	O
zt	O
we	O
rewrite	O
it	O
as	O
zt	O
=	O
(	O
10.2	O
)	O
where	O
we	O
used	O
the	O
fact	O
that	O
z0	O
=	O
1	O
because	O
f0	O
≡	O
0.	O
therefore	O
,	O
it	O
suﬃces	O
to	O
show	O
that	O
for	O
every	O
round	O
t	O
,	O
=	O
,	O
zt	O
zt−1	O
·	O
zt−1	O
zt−2	O
···	O
z2	O
z1	O
·	O
z1	O
z0	O
zt	O
z0	O
≤	O
e−2γ2	O
.	O
zt+1	O
zt	O
(	O
10.3	O
)	O
to	O
do	O
so	O
,	O
we	O
ﬁrst	O
note	O
that	O
using	O
a	O
simple	O
inductive	O
argument	O
,	O
for	O
all	O
t	O
and	O
i	O
,	O
d	O
(	O
t+1	O
)	O
+	O
ewt+1	O
(	O
cid:88	O
)	O
(	O
1	O
−	O
t+1	O
)	O
+	O
(	O
cid:112	O
)	O
1/t+1	O
−	O
1	O
t+1	O
i	O
d	O
(	O
t+1	O
)	O
i	O
i	O
:	O
yiht+1	O
(	O
xi	O
)	O
=−1	O
1	O
−	O
t+1	O
t+1	O
t+1	O
by	O
our	O
assumption	O
,	O
t+1	O
≤	O
1	O
tonically	O
increasing	O
in	O
[	O
0	O
,	O
1/2	O
]	O
,	O
we	O
obtain	O
that	O
2	O
−	O
γ.	O
since	O
the	O
function	B
g	O
(	O
a	O
)	O
=	O
a	O
(	O
1	O
−	O
a	O
)	O
is	O
mono-	O
(	O
cid:19	O
)	O
(	O
cid:112	O
)	O
+	O
γ	O
=	O
1	O
−	O
4γ2	O
.	O
hence	O
,	O
zt+1	O
zt	O
d	O
(	O
t+1	O
)	O
i	O
=	O
(	O
cid:80	O
)	O
m	O
e−yift	O
(	O
xi	O
)	O
j=1	O
e−yj	O
ft	O
(	O
xj	O
)	O
.	O
=	O
=	O
=	O
i=1	O
j=1	O
j=1	O
d	O
(	O
t+1	O
)	O
i	O
e−yj	O
ft	O
(	O
xj	O
)	O
e−yiwt+1ht+1	O
(	O
xi	O
)	O
(	O
cid:80	O
)	O
m	O
m	O
(	O
cid:80	O
)	O
i=1	O
e−yift+1	O
(	O
xi	O
)	O
(	O
cid:80	O
)	O
m	O
e−yj	O
ft	O
(	O
xj	O
)	O
m	O
(	O
cid:80	O
)	O
i=1	O
e−yift	O
(	O
xi	O
)	O
e−yiwt+1ht+1	O
(	O
xi	O
)	O
m	O
(	O
cid:88	O
)	O
=	O
e−wt+1	O
(	O
cid:88	O
)	O
1	O
(	O
cid:112	O
)	O
1/t+1	O
−	O
1	O
(	O
cid:114	O
)	O
t+1	O
=	O
2	O
(	O
cid:112	O
)	O
t+1	O
(	O
1	O
−	O
t+1	O
)	O
.	O
(	O
cid:115	O
)	O
(	O
cid:18	O
)	O
1	O
2	O
(	O
cid:112	O
)	O
t+1	O
(	O
1	O
−	O
t+1	O
)	O
≤	O
2	O
=	O
e−wt+1	O
(	O
1	O
−	O
t+1	O
)	O
+	O
ewt+1t+1	O
(	O
1	O
−	O
t+1	O
)	O
+	O
1	O
−	O
t+1	O
i	O
:	O
yiht+1	O
(	O
xi	O
)	O
=1	O
(	O
cid:115	O
)	O
−	O
γ	O
=	O
=	O
2	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
1	O
2	O
10.3	O
linear	O
combinations	O
of	O
base	O
hypotheses	O
137	O
finally	O
,	O
using	O
the	O
inequality	O
1	O
−	O
a	O
≤	O
e−a	O
we	O
have	O
that	O
(	O
cid:112	O
)	O
1	O
−	O
4γ2	O
≤	O
e−4γ2/2	O
=	O
.	O
this	O
shows	O
that	O
equation	O
(	O
10.3	O
)	O
holds	O
and	O
thus	O
concludes	O
our	O
proof	O
.	O
e−2γ2	O
each	O
iteration	O
of	O
adaboost	O
involves	O
o	O
(	O
m	O
)	O
operations	O
as	O
well	O
as	O
a	O
single	O
call	O
to	O
the	O
weak	O
learner	O
.	O
therefore	O
,	O
if	O
the	O
weak	O
learner	O
can	O
be	O
implemented	O
eﬃciently	O
(	O
as	O
happens	O
in	O
the	O
case	O
of	O
erm	O
with	O
respect	O
to	O
decision	B
stumps	I
)	O
then	O
the	O
total	O
training	O
process	O
will	O
be	O
eﬃcient	O
.	O
remark	O
10.2	O
theorem	O
10.2	O
assumes	O
that	O
at	O
each	O
iteration	O
of	O
adaboost	O
,	O
the	O
weak	O
learner	O
returns	O
a	O
hypothesis	B
with	O
weighted	O
sample	O
error	O
of	O
at	O
most	O
1/2−	O
γ.	O
according	O
to	O
the	O
deﬁnition	O
of	O
a	O
weak	O
learner	O
,	O
it	O
can	O
fail	O
with	O
probability	O
δ.	O
using	O
the	O
union	B
bound	I
,	O
the	O
probability	O
that	O
the	O
weak	O
learner	O
will	O
not	O
fail	O
at	O
all	O
of	O
the	O
iterations	O
is	O
at	O
least	O
1	O
−	O
δt	O
.	O
as	O
we	O
show	O
in	O
exercise	O
1	O
,	O
the	O
dependence	O
of	O
the	O
sample	B
complexity	I
on	O
δ	O
can	O
always	O
be	O
logarithmic	O
in	O
1/δ	O
,	O
and	O
therefore	O
invoking	O
the	O
weak	O
learner	O
with	O
a	O
very	O
small	O
δ	O
is	O
not	O
problematic	O
.	O
we	O
can	O
therefore	O
assume	O
that	O
δt	O
is	O
also	O
small	O
.	O
furthermore	O
,	O
since	O
the	O
weak	O
learner	O
is	O
only	O
applied	O
with	O
distributions	O
over	O
the	O
training	B
set	I
,	O
in	O
many	O
cases	O
we	O
can	O
implement	O
the	O
weak	O
learner	O
so	O
that	O
it	O
will	O
have	O
a	O
zero	O
probability	O
of	O
failure	O
(	O
i.e.	O
,	O
δ	O
=	O
0	O
)	O
.	O
this	O
is	O
the	O
case	O
,	O
for	O
example	O
,	O
in	O
the	O
weak	O
learner	O
that	O
ﬁnds	O
the	O
minimum	O
value	O
of	O
ld	O
(	O
h	O
)	O
for	O
decision	B
stumps	I
,	O
as	O
described	O
in	O
the	O
previous	O
section	O
.	O
theorem	O
10.2	O
tells	O
us	O
that	O
the	O
empirical	B
risk	I
of	O
the	O
hypothesis	B
constructed	O
by	O
adaboost	O
goes	O
to	O
zero	O
as	O
t	O
grows	O
.	O
however	O
,	O
what	O
we	O
really	O
care	O
about	O
is	O
the	O
true	O
risk	O
of	O
the	O
output	O
hypothesis	B
.	O
to	O
argue	O
about	O
the	O
true	O
risk	O
,	O
we	O
note	O
that	O
the	O
output	O
of	O
adaboost	O
is	O
in	O
fact	O
a	O
composition	O
of	O
a	O
halfspace	B
over	O
the	O
predictions	O
of	O
the	O
t	O
weak	O
hypotheses	O
constructed	O
by	O
the	O
weak	O
learner	O
.	O
in	O
the	O
next	O
section	O
we	O
show	O
that	O
if	O
the	O
weak	O
hypotheses	O
come	O
from	O
a	O
base	B
hypothesis	I
class	O
of	O
low	O
vc-dimension	O
,	O
then	O
the	O
estimation	B
error	I
of	O
adaboost	O
will	O
be	O
small	O
;	O
namely	O
,	O
the	O
true	O
risk	O
of	O
the	O
output	O
of	O
adaboost	O
would	O
not	O
be	O
very	O
far	O
from	O
its	O
empirical	B
risk	I
.	O
10.3	O
linear	O
combinations	O
of	O
base	O
hypotheses	O
as	O
mentioned	O
previously	O
,	O
a	O
popular	O
approach	O
for	O
constructing	O
a	O
weak	O
learner	O
is	O
to	O
apply	O
the	O
erm	O
rule	O
with	O
respect	O
to	O
a	O
base	B
hypothesis	I
class	O
(	O
e.g.	O
,	O
erm	O
over	O
decision	B
stumps	I
)	O
.	O
we	O
have	O
also	O
seen	O
that	O
boosting	B
outputs	O
a	O
composition	O
of	O
a	O
halfspace	B
over	O
the	O
predictions	O
of	O
the	O
weak	O
hypotheses	O
.	O
therefore	O
,	O
given	O
a	O
base	B
hypothesis	I
class	O
b	O
(	O
e.g.	O
,	O
decision	B
stumps	I
)	O
,	O
the	O
output	O
of	O
adaboost	O
will	O
be	O
a	O
member	O
of	O
the	O
following	O
class	O
:	O
l	O
(	O
b	O
,	O
t	O
)	O
=	O
x	O
(	O
cid:55	O
)	O
→	O
sign	O
wtht	O
(	O
x	O
)	O
:	O
w	O
∈	O
rt	O
,	O
∀t	O
,	O
ht	O
∈	O
b	O
.	O
(	O
10.4	O
)	O
t=1	O
that	O
is	O
,	O
each	O
h	O
∈	O
l	O
(	O
b	O
,	O
t	O
)	O
is	O
parameterized	O
by	O
t	O
base	O
hypotheses	O
from	O
b	O
and	O
by	O
a	O
vector	O
w	O
∈	O
rt	O
.	O
the	O
prediction	O
of	O
such	O
an	O
h	O
on	O
an	O
instance	B
x	O
is	O
ob-	O
tained	O
by	O
ﬁrst	O
applying	O
the	O
t	O
base	O
hypotheses	O
to	O
construct	O
the	O
vector	O
ψ	O
(	O
x	O
)	O
=	O
(	O
cid:40	O
)	O
(	O
cid:32	O
)	O
t	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
(	O
cid:41	O
)	O
138	O
boosting	B
(	O
h1	O
(	O
x	O
)	O
,	O
.	O
.	O
.	O
,	O
ht	O
(	O
x	O
)	O
)	O
∈	O
rt	O
,	O
and	O
then	O
applying	O
the	O
(	O
homogenous	B
)	O
halfspace	B
deﬁned	O
by	O
w	O
on	O
ψ	O
(	O
x	O
)	O
.	O
in	O
this	O
section	O
we	O
analyze	O
the	O
estimation	B
error	I
of	O
l	O
(	O
b	O
,	O
t	O
)	O
by	O
bounding	O
the	O
vc-dimension	O
of	O
l	O
(	O
b	O
,	O
t	O
)	O
in	O
terms	O
of	O
the	O
vc-dimension	O
of	O
b	O
and	O
t	O
.	O
we	O
will	O
show	O
that	O
,	O
up	O
to	O
logarithmic	O
factors	O
,	O
the	O
vc-dimension	O
of	O
l	O
(	O
b	O
,	O
t	O
)	O
is	O
bounded	O
by	O
t	O
times	O
the	O
vc-dimension	O
of	O
b.	O
it	O
follows	O
that	O
the	O
estimation	B
error	I
of	O
ad-	O
aboost	O
grows	O
linearly	O
with	O
t	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
empirical	B
risk	I
of	O
adaboost	O
decreases	O
with	O
t	O
.	O
in	O
fact	O
,	O
as	O
we	O
demonstrate	O
later	O
,	O
t	O
can	O
be	O
used	O
to	O
decrease	O
the	O
approximation	B
error	I
of	O
l	O
(	O
b	O
,	O
t	O
)	O
.	O
therefore	O
,	O
the	O
parameter	O
t	O
of	O
adaboost	O
enables	O
us	O
to	O
control	O
the	O
bias-complexity	B
tradeoﬀ	I
.	O
the	O
simple	O
example	O
,	O
in	O
which	O
x	O
=	O
r	O
and	O
the	O
base	O
class	O
is	O
decision	B
stumps	I
,	O
to	O
demonstrate	O
how	O
the	O
expressive	O
power	O
of	O
l	O
(	O
b	O
,	O
t	O
)	O
increases	O
with	O
t	O
,	O
consider	O
hds1	O
=	O
{	O
x	O
(	O
cid:55	O
)	O
→	O
sign	O
(	O
x	O
−	O
θ	O
)	O
·	O
b	O
:	O
θ	O
∈	O
r	O
,	O
b	O
∈	O
{	O
±1	O
}	O
}	O
.	O
note	O
that	O
in	O
this	O
one	O
dimensional	O
case	O
,	O
hds1	O
is	O
in	O
fact	O
equivalent	O
to	O
(	O
nonho-	O
mogenous	O
)	O
halfspaces	O
on	O
r.	O
now	O
,	O
let	O
h	O
be	O
the	O
rather	O
complex	O
class	O
(	O
compared	O
to	O
halfspaces	O
on	O
the	O
line	O
)	O
of	O
piece-wise	O
constant	O
functions	O
.	O
let	O
gr	O
be	O
a	O
piece-wise	O
constant	O
function	B
with	O
at	O
most	O
r	O
pieces	O
;	O
that	O
is	O
,	O
there	O
exist	O
thresholds	O
−∞	O
=	O
θ0	O
<	O
θ1	O
<	O
θ2	O
<	O
···	O
<	O
θr	O
=	O
∞	O
such	O
that	O
r	O
(	O
cid:88	O
)	O
gr	O
(	O
x	O
)	O
=	O
αi1	O
[	O
x∈	O
(	O
θi−1	O
,	O
θi	O
]	O
]	O
∀i	O
,	O
αi	O
∈	O
{	O
±1	O
}	O
.	O
i=1	O
denote	O
by	O
gr	O
the	O
class	O
of	O
all	O
such	O
piece-wise	O
constant	O
classiﬁers	O
with	O
at	O
most	O
r	O
pieces	O
.	O
in	O
the	O
following	O
we	O
show	O
that	O
gt	O
⊆	O
l	O
(	O
hds1	O
,	O
t	O
)	O
;	O
namely	O
,	O
the	O
class	O
of	O
halfspaces	O
over	O
t	O
decision	B
stumps	I
yields	O
all	O
the	O
piece-wise	O
constant	O
classiﬁers	O
with	O
at	O
most	O
t	O
pieces	O
.	O
indeed	O
,	O
without	O
loss	B
of	O
generality	O
consider	O
any	O
g	O
∈	O
gt	O
with	O
αt	O
=	O
(	O
−1	O
)	O
t.	O
this	O
implies	O
that	O
if	O
x	O
is	O
in	O
the	O
interval	O
(	O
θt−1	O
,	O
θt	O
]	O
,	O
then	O
g	O
(	O
x	O
)	O
=	O
(	O
−1	O
)	O
t.	O
for	O
example	O
:	O
now	O
,	O
the	O
function	B
(	O
cid:32	O
)	O
t	O
(	O
cid:88	O
)	O
h	O
(	O
x	O
)	O
=	O
sign	O
(	O
cid:33	O
)	O
wt	O
sign	O
(	O
x	O
−	O
θt−1	O
)	O
,	O
(	O
10.5	O
)	O
where	O
w1	O
=	O
0.5	O
and	O
for	O
t	O
>	O
1	O
,	O
wt	O
=	O
(	O
−1	O
)	O
t	O
,	O
is	O
in	O
l	O
(	O
hds1	O
,	O
t	O
)	O
and	O
is	O
equal	O
to	O
g	O
(	O
see	O
exercise	O
2	O
)	O
.	O
t=1	O
10.3	O
linear	O
combinations	O
of	O
base	O
hypotheses	O
139	O
from	O
this	O
example	O
we	O
obtain	O
that	O
l	O
(	O
hds1	O
,	O
t	O
)	O
can	O
shatter	O
any	O
set	B
of	O
t	O
+	O
1	O
instances	O
in	O
r	O
;	O
hence	O
the	O
vc-dimension	O
of	O
l	O
(	O
hds1	O
,	O
t	O
)	O
is	O
at	O
least	O
t	O
+1	O
.	O
therefore	O
,	O
t	O
is	O
a	O
parameter	O
that	O
can	O
control	O
the	O
bias-complexity	B
tradeoﬀ	I
:	O
enlarging	O
t	O
yields	O
a	O
more	O
expressive	O
hypothesis	B
class	I
but	O
on	O
the	O
other	O
hand	O
might	O
increase	O
the	O
estimation	B
error	I
.	O
in	O
the	O
next	O
subsection	O
we	O
formally	O
upper	O
bound	O
the	O
vc-	O
dimension	B
of	O
l	O
(	O
b	O
,	O
t	O
)	O
for	O
any	O
base	O
class	O
b	O
.	O
10.3.1	O
the	O
vc-dimension	O
of	O
l	O
(	O
b	O
,	O
t	O
)	O
the	O
following	O
lemma	O
tells	O
us	O
that	O
the	O
vc-dimension	O
of	O
l	O
(	O
b	O
,	O
t	O
)	O
is	O
upper	O
bounded	O
by	O
˜o	O
(	O
vcdim	O
(	O
b	O
)	O
t	O
)	O
(	O
the	O
˜o	O
notation	O
ignores	O
constants	O
and	O
logarithmic	O
factors	O
)	O
.	O
lemma	O
10.3	O
let	O
b	O
be	O
a	O
base	O
class	O
and	O
let	O
l	O
(	O
b	O
,	O
t	O
)	O
be	O
as	O
deﬁned	O
in	O
equa-	O
tion	O
(	O
10.4	O
)	O
.	O
assume	O
that	O
both	O
t	O
and	O
vcdim	O
(	O
b	O
)	O
are	O
at	O
least	O
3.	O
then	O
,	O
vcdim	O
(	O
l	O
(	O
b	O
,	O
t	O
)	O
)	O
≤	O
t	O
(	O
vcdim	O
(	O
b	O
)	O
+	O
1	O
)	O
(	O
3	O
log	O
(	O
t	O
(	O
vcdim	O
(	O
b	O
)	O
+	O
1	O
)	O
)	O
+	O
2	O
)	O
.	O
proof	O
denote	O
d	O
=	O
vcdim	O
(	O
b	O
)	O
.	O
let	O
c	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
}	O
be	O
a	O
set	B
that	O
is	O
shat-	O
tered	O
by	O
l	O
(	O
b	O
,	O
t	O
)	O
.	O
each	O
labeling	O
of	O
c	O
by	O
h	O
∈	O
l	O
(	O
b	O
,	O
t	O
)	O
is	O
obtained	O
by	O
ﬁrst	O
choos-	O
ing	O
h1	O
,	O
.	O
.	O
.	O
,	O
ht	O
∈	O
b	O
and	O
then	O
applying	O
a	O
halfspace	B
hypothesis	O
over	O
the	O
vector	O
(	O
h1	O
(	O
x	O
)	O
,	O
.	O
.	O
.	O
,	O
ht	O
(	O
x	O
)	O
)	O
.	O
by	O
sauer	O
’	O
s	O
lemma	O
,	O
there	O
are	O
at	O
most	O
(	O
em/d	O
)	O
d	O
diﬀerent	O
di-	O
chotomies	O
(	O
i.e.	O
,	O
labelings	O
)	O
induced	O
by	O
b	O
over	O
c.	O
therefore	O
,	O
we	O
need	O
to	O
choose	O
t	O
hypotheses	O
,	O
out	O
of	O
at	O
most	O
(	O
em/d	O
)	O
d	O
diﬀerent	O
hypotheses	O
.	O
there	O
are	O
at	O
most	O
(	O
em/d	O
)	O
dt	O
ways	O
to	O
do	O
it	O
.	O
next	O
,	O
for	O
each	O
such	O
choice	O
,	O
we	O
apply	O
a	O
linear	B
predictor	I
,	O
which	O
yields	O
at	O
most	O
(	O
em/t	O
)	O
t	O
dichotomies	O
.	O
therefore	O
,	O
the	O
overall	O
number	O
of	O
dichotomies	O
we	O
can	O
construct	O
is	O
upper	O
bounded	O
by	O
(	O
em/d	O
)	O
dt	O
(	O
em/t	O
)	O
t	O
≤	O
m	O
(	O
d+1	O
)	O
t	O
,	O
where	O
we	O
used	O
the	O
assumption	O
that	O
both	O
d	O
and	O
t	O
are	O
at	O
least	O
3.	O
since	O
we	O
assume	O
that	O
c	O
is	O
shattered	O
,	O
we	O
must	O
have	O
that	O
the	O
preceding	O
is	O
at	O
least	O
2m	O
,	O
which	O
yields	O
therefore	O
,	O
2m	O
≤	O
m	O
(	O
d+1	O
)	O
t	O
.	O
m	O
≤	O
log	O
(	O
m	O
)	O
(	O
d	O
+	O
1	O
)	O
t	O
log	O
(	O
2	O
)	O
.	O
lemma	O
a.1	O
in	O
chapter	O
a	O
tells	O
us	O
that	O
a	O
necessary	O
condition	O
for	O
the	O
above	O
to	O
hold	O
is	O
that	O
m	O
≤	O
2	O
(	O
d	O
+	O
1	O
)	O
t	O
log	O
(	O
2	O
)	O
log	O
(	O
d	O
+	O
1	O
)	O
t	O
log	O
(	O
2	O
)	O
≤	O
(	O
d	O
+	O
1	O
)	O
t	O
(	O
3	O
log	O
(	O
(	O
d	O
+	O
1	O
)	O
t	O
)	O
+	O
2	O
)	O
,	O
which	O
concludes	O
our	O
proof	O
.	O
in	O
exercise	O
4	O
we	O
show	O
that	O
for	O
some	O
base	O
classes	O
,	O
b	O
,	O
it	O
also	O
holds	O
that	O
vcdim	O
(	O
l	O
(	O
b	O
,	O
t	O
)	O
)	O
≥	O
ω	O
(	O
vcdim	O
(	O
b	O
)	O
t	O
)	O
.	O
140	O
boosting	B
a	O
c	O
b	O
d	O
figure	O
10.1	O
the	O
four	O
types	O
of	O
functions	O
,	O
g	O
,	O
used	O
by	O
the	O
base	O
hypotheses	O
for	O
face	B
recognition	I
.	O
the	O
value	O
of	O
g	O
for	O
type	O
a	O
or	O
b	O
is	O
the	O
diﬀerence	O
between	O
the	O
sum	O
of	O
the	O
pixels	O
within	O
two	O
rectangular	O
regions	O
.	O
these	O
regions	O
have	O
the	O
same	O
size	O
and	O
shape	O
and	O
are	O
horizontally	O
or	O
vertically	O
adjacent	O
.	O
for	O
type	O
c	O
,	O
the	O
value	O
of	O
g	O
is	O
the	O
sum	O
within	O
two	O
outside	O
rectangles	O
subtracted	O
from	O
the	O
sum	O
in	O
a	O
center	O
rectangle	O
.	O
for	O
type	O
d	O
,	O
we	O
compute	O
the	O
diﬀerence	O
between	O
diagonal	O
pairs	O
of	O
rectangles	O
.	O
10.4	O
adaboost	O
for	O
face	B
recognition	I
we	O
now	O
turn	O
to	O
a	O
base	B
hypothesis	I
that	O
has	O
been	O
proposed	O
by	O
viola	O
and	O
jones	O
for	O
the	O
task	O
of	O
face	B
recognition	I
.	O
in	O
this	O
task	O
,	O
the	O
instance	B
space	I
is	O
images	O
,	O
represented	O
as	O
matrices	O
of	O
gray	O
level	O
values	O
of	O
pixels	O
.	O
to	O
be	O
concrete	O
,	O
let	O
us	O
take	O
images	O
of	O
size	O
24	O
×	O
24	O
pixels	O
,	O
and	O
therefore	O
our	O
instance	B
space	I
is	O
the	O
set	B
of	O
real	O
valued	O
matrices	O
of	O
size	O
24	O
×	O
24.	O
the	O
goal	O
is	O
to	O
learn	O
a	O
classiﬁer	B
,	O
h	O
:	O
x	O
→	O
{	O
±1	O
}	O
,	O
that	O
given	O
an	O
image	O
as	O
input	O
,	O
should	O
output	O
whether	O
the	O
image	O
is	O
of	O
a	O
human	O
face	O
or	O
not	O
.	O
each	O
hypothesis	B
in	O
the	O
base	O
class	O
is	O
of	O
the	O
form	O
h	O
(	O
x	O
)	O
=	O
f	O
(	O
g	O
(	O
x	O
)	O
)	O
,	O
where	O
f	O
is	O
a	O
decision	O
stump	O
hypothesis	B
and	O
g	O
:	O
r24,24	O
→	O
r	O
is	O
a	O
function	B
that	O
maps	O
an	O
image	O
to	O
a	O
scalar	O
.	O
each	O
function	B
g	O
is	O
parameterized	O
by	O
•	O
an	O
axis	O
aligned	O
rectangle	O
r.	O
since	O
each	O
image	O
is	O
of	O
size	O
24	O
×	O
24	O
,	O
there	O
are	O
at	O
most	O
244	O
axis	O
aligned	O
rectangles	O
.	O
•	O
a	O
type	O
,	O
t	O
∈	O
{	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
}	O
.	O
each	O
type	O
corresponds	O
to	O
a	O
mask	O
,	O
as	O
depicted	O
in	O
figure	O
10.1.	O
to	O
calculate	O
g	O
we	O
stretch	O
the	O
mask	O
t	O
to	O
ﬁt	O
the	O
rectangle	O
r	O
and	O
then	O
calculate	O
the	O
sum	O
of	O
the	O
pixels	O
(	O
that	O
is	O
,	O
sum	O
of	O
their	O
gray	O
level	O
values	O
)	O
that	O
lie	O
within	O
the	O
red	O
rectangles	O
and	O
subtract	O
it	O
from	O
the	O
sum	O
of	O
pixels	O
in	O
the	O
blue	O
rectangles	O
.	O
since	O
the	O
number	O
of	O
such	O
functions	O
g	O
is	O
at	O
most	O
244	O
·	O
4	O
,	O
we	O
can	O
implement	O
a	O
weak	O
learner	O
for	O
the	O
base	B
hypothesis	I
class	O
by	O
ﬁrst	O
calculating	O
all	O
the	O
possible	O
outputs	O
of	O
g	O
on	O
each	O
image	O
,	O
and	O
then	O
apply	O
the	O
weak	O
learner	O
of	O
decision	B
stumps	I
described	O
in	O
the	O
previous	O
subsection	O
.	O
it	O
is	O
possible	O
to	O
perform	O
the	O
ﬁrst	O
step	O
very	O
10.5	O
summary	O
141	O
figure	O
10.2	O
the	O
ﬁrst	O
and	O
second	O
features	O
selected	O
by	O
adaboost	O
,	O
as	O
implemented	O
by	O
viola	O
and	O
jones	O
.	O
the	O
two	O
features	O
are	O
shown	O
in	O
the	O
top	O
row	O
and	O
then	O
overlaid	O
on	O
a	O
typical	O
training	O
face	O
in	O
the	O
bottom	O
row	O
.	O
the	O
ﬁrst	O
feature	B
measures	O
the	O
diﬀerence	O
in	O
intensity	O
between	O
the	O
region	O
of	O
the	O
eyes	O
and	O
a	O
region	O
across	O
the	O
upper	O
cheeks	O
.	O
the	O
feature	B
capitalizes	O
on	O
the	O
observation	O
that	O
the	O
eye	O
region	O
is	O
often	O
darker	O
than	O
the	O
cheeks	O
.	O
the	O
second	O
feature	B
compares	O
the	O
intensities	O
in	O
the	O
eye	O
regions	O
to	O
the	O
intensity	O
across	O
the	O
bridge	O
of	O
the	O
nose	O
.	O
eﬃciently	O
by	O
a	O
preprocessing	O
step	O
in	O
which	O
we	O
calculate	O
the	O
integral	B
image	I
of	O
each	O
image	O
in	O
the	O
training	B
set	I
.	O
see	O
exercise	O
5	O
for	O
details	O
.	O
in	O
figure	O
10.2	O
we	O
depict	O
the	O
ﬁrst	O
two	O
features	O
selected	O
by	O
adaboost	O
when	O
running	O
it	O
with	O
the	O
base	O
features	O
proposed	O
by	O
viola	O
and	O
jones	O
.	O
10.5	O
summary	O
boosting	B
is	O
a	O
method	O
for	O
amplifying	O
the	O
accuracy	B
of	O
weak	O
learners	O
.	O
in	O
this	O
chapter	O
we	O
described	O
the	O
adaboost	O
algorithm	O
.	O
we	O
have	O
shown	O
that	O
after	O
t	O
iterations	O
of	O
adaboost	O
,	O
it	O
returns	O
a	O
hypothesis	B
from	O
the	O
class	O
l	O
(	O
b	O
,	O
t	O
)	O
,	O
obtained	O
by	O
composing	O
a	O
linear	O
classiﬁer	O
on	O
t	O
hypotheses	O
from	O
a	O
base	O
class	O
b.	O
we	O
have	O
demonstrated	O
how	O
the	O
parameter	O
t	O
controls	O
the	O
tradeoﬀ	O
between	O
approximation	O
and	O
estimation	O
errors	O
.	O
in	O
the	O
next	O
chapter	O
we	O
will	O
study	O
how	O
to	O
tune	O
parameters	O
such	O
as	O
t	O
,	O
based	O
on	O
the	O
data	O
.	O
10.6	O
bibliographic	O
remarks	O
as	O
mentioned	O
before	O
,	O
boosting	B
stemmed	O
from	O
the	O
theoretical	O
question	O
of	O
whether	O
an	O
eﬃcient	O
weak	O
learner	O
can	O
be	O
“	O
boosted	O
”	O
into	O
an	O
eﬃcient	O
strong	O
learner	O
(	O
kearns	O
&	O
valiant	O
1988	O
)	O
and	O
solved	O
by	O
schapire	O
(	O
1990	O
)	O
.	O
the	O
adaboost	O
algorithm	O
has	O
been	O
proposed	O
in	O
freund	O
&	O
schapire	O
(	O
1995	O
)	O
.	O
boosting	B
can	O
be	O
viewed	O
from	O
many	O
perspectives	O
.	O
in	O
the	O
purely	O
theoretical	O
context	O
,	O
adaboost	O
can	O
be	O
interpreted	O
as	O
a	O
negative	O
result	O
:	O
if	O
strong	B
learning	I
of	O
a	O
hypothesis	B
class	I
is	O
computationally	O
hard	O
,	O
so	O
is	O
weak	B
learning	I
of	O
this	O
class	O
.	O
this	O
negative	O
result	O
can	O
be	O
useful	O
for	O
showing	O
hardness	O
of	O
agnostic	O
pac	O
learning	O
of	O
a	O
class	O
b	O
based	O
on	O
hardness	O
of	O
pac	O
learning	O
of	O
some	O
other	O
class	O
h	O
,	O
as	O
long	O
as	O
figure5	O
:	O
theﬁrstandsecondfeaturesselectedbyadaboost.thetwofeaturesareshowninthetoprowandthenoverlayedonatypicaltrainingfaceinthebottomrow.theﬁrstfeaturemeasuresthedifferenceinintensitybetweentheregionoftheeyesandaregionacrosstheuppercheeks.thefeaturecapitalizesontheobservationthattheeyeregionisoftendarkerthanthecheeks.thesecondfeaturecomparestheintensitiesintheeyeregionstotheintensityacrossthebridgeofthenose.directlyincreasescomputationtime.4theattentionalcascadethissectiondescribesanalgorithmforconstructingacascadeofclassiﬁerswhichachievesincreaseddetec-tionperformancewhileradicallyreducingcomputationtime.thekeyinsightisthatsmaller	O
,	O
andthereforemoreefﬁcient	O
,	O
boostedclassiﬁerscanbeconstructedwhichrejectmanyofthenegativesub-windowswhiledetectingalmostallpositiveinstances.simplerclassiﬁersareusedtorejectthemajorityofsub-windowsbeforemorecomplexclassiﬁersarecalledupontoachievelowfalsepositiverates.stagesinthecascadeareconstructedbytrainingclassiﬁersusingadaboost.startingwithatwo-featurestrongclassiﬁer	O
,	O
aneffectivefaceﬁltercanbeobtainedbyadjustingthestrongclassiﬁerthresholdtomin-imizefalsenegatives.theinitialadaboostthreshold	O
,	O
,isdesignedtoyieldalowerrorrateonthetrainingdata.alowerthresholdyieldshigherdetectionratesandhigherfalsepositiverates.basedonperformancemeasuredusingavalidationtrainingset	O
,	O
thetwo-featureclassiﬁercanbeadjustedtodetect100	O
%	O
ofthefaceswithafalsepositiverateof40	O
%	O
.seefigure5foradescriptionofthetwofeaturesusedinthisclassiﬁer.thedetectionperformanceofthetwo-featureclassiﬁerisfarfromacceptableasanobjectdetectionsystem.neverthelesstheclassiﬁercansigniﬁcantlyreducethenumbersub-windowsthatneedfurtherpro-cessingwithveryfewoperations:1.evaluatetherectanglefeatures	O
(	O
requiresbetween6and9arrayreferencesperfeature	O
)	O
.2.computetheweakclassiﬁerforeachfeature	O
(	O
requiresonethresholdoperationperfeature	O
)	O
.11	O
142	O
boosting	B
h	O
is	O
weakly	O
learnable	O
using	O
b.	O
for	O
example	O
,	O
klivans	O
&	O
sherstov	O
(	O
2006	O
)	O
have	O
shown	O
that	O
pac	O
learning	O
of	O
the	O
class	O
of	O
intersection	O
of	O
halfspaces	O
is	O
hard	O
(	O
even	O
in	O
the	O
realizable	O
case	O
)	O
.	O
this	O
hardness	O
result	O
can	O
be	O
used	O
to	O
show	O
that	O
agnostic	O
pac	O
learning	O
of	O
a	O
single	O
halfspace	O
is	O
also	O
computationally	O
hard	O
(	O
shalev-shwartz	O
,	O
shamir	O
&	O
sridharan	O
2010	O
)	O
.	O
the	O
idea	O
is	O
to	O
show	O
that	O
an	O
agnostic	O
pac	O
learner	O
for	O
a	O
single	O
halfspace	O
can	O
yield	O
a	O
weak	O
learner	O
for	O
the	O
class	O
of	O
intersection	O
of	O
halfspaces	O
,	O
and	O
since	O
such	O
a	O
weak	O
learner	O
can	O
be	O
boosted	O
,	O
we	O
will	O
obtain	O
a	O
strong	O
learner	O
for	O
the	O
class	O
of	O
intersection	O
of	O
halfspaces	O
.	O
adaboost	O
also	O
shows	O
an	O
equivalence	O
between	O
the	O
existence	O
of	O
a	O
weak	O
learner	O
and	O
separability	O
of	O
the	O
data	O
using	O
a	O
linear	O
classiﬁer	O
over	O
the	O
predictions	O
of	O
base	O
hypotheses	O
.	O
this	O
result	O
is	O
closely	O
related	O
to	O
von	O
neumann	O
’	O
s	O
minimax	O
theorem	O
(	O
von	O
neumann	O
1928	O
)	O
,	O
a	O
fundamental	O
result	O
in	O
game	O
theory	O
.	O
adaboost	O
is	O
also	O
related	O
to	O
the	O
concept	O
of	O
margin	B
,	O
which	O
we	O
will	O
study	O
later	O
on	O
in	O
chapter	O
15.	O
it	O
can	O
also	O
be	O
viewed	O
as	O
a	O
forward	B
greedy	I
selection	I
algorithm	O
,	O
a	O
topic	O
that	O
will	O
be	O
presented	O
in	O
chapter	O
25.	O
a	O
recent	O
book	O
by	O
schapire	O
&	O
freund	O
(	O
2012	O
)	O
covers	O
boosting	B
from	O
all	O
points	O
of	O
view	O
,	O
and	O
gives	O
easy	O
access	O
to	O
the	O
wealth	O
of	O
research	O
that	O
this	O
ﬁeld	O
has	O
produced	O
.	O
10.7	O
exercises	O
1.	O
boosting	B
the	I
conﬁdence	I
:	O
let	O
a	O
be	O
an	O
algorithm	O
that	O
guarantees	O
the	O
fol-	O
lowing	O
:	O
there	O
exist	O
some	O
constant	O
δ0	O
∈	O
(	O
0	O
,	O
1	O
)	O
and	O
a	O
function	B
mh	O
:	O
(	O
0	O
,	O
1	O
)	O
→	O
n	O
such	O
that	O
for	O
every	O
	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
if	O
m	O
≥	O
mh	O
(	O
	O
)	O
then	O
for	O
every	O
distribution	O
d	O
it	O
holds	O
that	O
with	O
probability	O
of	O
at	O
least	O
1−	O
δ0	O
,	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
minh∈h	O
ld	O
(	O
h	O
)	O
+	O
	O
.	O
suggest	O
a	O
procedure	O
that	O
relies	O
on	O
a	O
and	O
learns	O
h	O
in	O
the	O
usual	O
agnostic	O
pac	O
learning	O
model	O
and	O
has	O
a	O
sample	B
complexity	I
of	O
mh	O
(	O
	O
,	O
δ	O
)	O
≤	O
k	O
mh	O
(	O
	O
)	O
+	O
(	O
cid:24	O
)	O
2	O
log	O
(	O
4k/δ	O
)	O
(	O
cid:25	O
)	O
,	O
2	O
where	O
k	O
=	O
(	O
cid:100	O
)	O
log	O
(	O
δ	O
)	O
/	O
log	O
(	O
δ0	O
)	O
(	O
cid:101	O
)	O
.	O
hint	O
:	O
divide	O
the	O
data	O
into	O
k	O
+	O
1	O
chunks	O
,	O
where	O
each	O
of	O
the	O
ﬁrst	O
k	O
chunks	O
is	O
of	O
size	O
mh	O
(	O
	O
)	O
examples	O
.	O
train	O
the	O
ﬁrst	O
k	O
chunks	O
using	O
a.	O
argue	O
that	O
the	O
probability	O
that	O
for	O
all	O
of	O
these	O
chunks	O
we	O
have	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
>	O
minh∈h	O
ld	O
(	O
h	O
)	O
+	O
0	O
≤	O
δ/2	O
.	O
finally	O
,	O
use	O
the	O
last	O
chunk	O
to	O
choose	O
from	O
the	O
k	O
hypotheses	O
is	O
at	O
most	O
δk	O
that	O
a	O
generated	O
from	O
the	O
k	O
chunks	O
(	O
by	O
relying	O
on	O
corollary	O
4.6	O
)	O
.	O
2.	O
prove	O
that	O
the	O
function	B
h	O
given	O
in	O
equation	O
(	O
10.5	O
)	O
equals	O
the	O
piece-wise	O
con-	O
stant	O
function	B
deﬁned	O
according	O
to	O
the	O
same	O
thresholds	O
as	O
h.	O
3.	O
we	O
have	O
informally	O
argued	O
that	O
the	O
adaboost	O
algorithm	O
uses	O
the	O
weighting	O
mechanism	O
to	O
“	O
force	O
”	O
the	O
weak	O
learner	O
to	O
focus	O
on	O
the	O
problematic	O
examples	O
in	O
the	O
next	O
iteration	O
.	O
in	O
this	O
question	O
we	O
will	O
ﬁnd	O
some	O
rigorous	O
justiﬁcation	O
for	O
this	O
argument	O
.	O
10.7	O
exercises	O
143	O
show	O
that	O
the	O
error	O
of	O
ht	O
w.r.t	O
.	O
the	O
distribution	O
d	O
(	O
t+1	O
)	O
is	O
exactly	O
1/2	O
.	O
that	O
is	O
,	O
show	O
that	O
for	O
every	O
t	O
∈	O
[	O
t	O
]	O
m	O
(	O
cid:88	O
)	O
d	O
(	O
t+1	O
)	O
i	O
1	O
[	O
yi	O
(	O
cid:54	O
)	O
=ht	O
(	O
xi	O
)	O
]	O
=	O
1/2	O
.	O
i=1	O
4.	O
in	O
this	O
exercise	O
we	O
discuss	O
the	O
vc-dimension	O
of	O
classes	O
of	O
the	O
form	O
l	O
(	O
b	O
,	O
t	O
)	O
.	O
we	O
proved	O
an	O
upper	O
bound	O
of	O
o	O
(	O
dt	O
log	O
(	O
dt	O
)	O
)	O
,	O
where	O
d	O
=	O
vcdim	O
(	O
b	O
)	O
.	O
here	O
we	O
wish	O
to	O
prove	O
an	O
almost	O
matching	O
lower	O
bound	O
.	O
however	O
,	O
that	O
will	O
not	O
be	O
the	O
case	O
for	O
all	O
classes	O
b	O
.	O
1.	O
note	O
that	O
for	O
every	O
class	O
b	O
and	O
every	O
number	O
t	O
≥	O
1	O
,	O
vcdim	O
(	O
b	O
)	O
≤	O
vcdim	O
(	O
l	O
(	O
b	O
,	O
t	O
)	O
)	O
.	O
find	O
a	O
class	O
b	O
for	O
which	O
vcdim	O
(	O
b	O
)	O
=	O
vcdim	O
(	O
l	O
(	O
b	O
,	O
t	O
)	O
)	O
for	O
every	O
t	O
≥	O
1.	O
hint	O
:	O
take	O
x	O
to	O
be	O
a	O
ﬁnite	O
set	B
.	O
vcdim	O
(	O
bd	O
)	O
≤	O
5	O
+	O
2	O
log	O
(	O
d	O
)	O
.	O
hints	O
:	O
•	O
for	O
the	O
upper	O
bound	O
,	O
rely	O
on	O
exercise	O
11	O
.	O
•	O
for	O
the	O
lower	O
bound	O
,	O
assume	O
d	O
=	O
2k	O
.	O
let	O
a	O
be	O
a	O
k	O
×	O
d	O
matrix	O
whose	O
columns	O
are	O
all	O
the	O
d	O
binary	O
vectors	O
in	O
{	O
±1	O
}	O
k.	O
the	O
rows	O
of	O
a	O
form	O
a	O
set	B
of	O
k	O
vectors	O
in	O
rd	O
.	O
show	O
that	O
this	O
set	B
is	O
shattered	O
by	O
decision	B
stumps	I
over	O
rd	O
.	O
2.	O
let	O
bd	O
be	O
the	O
class	O
of	O
decision	B
stumps	I
over	O
rd	O
.	O
prove	O
that	O
log	O
(	O
d	O
)	O
≤	O
3.	O
let	O
t	O
≥	O
1	O
be	O
any	O
integer	O
.	O
prove	O
that	O
vcdim	O
(	O
l	O
(	O
bd	O
,	O
t	O
)	O
)	O
≥	O
0.5	O
t	O
log	O
(	O
d	O
)	O
.	O
hint	O
:	O
construct	O
a	O
set	B
of	O
t	O
from	O
the	O
previous	O
question	O
,	O
and	O
the	O
rows	O
of	O
the	O
matrices	O
2a	O
,	O
3a	O
,	O
4a	O
,	O
.	O
.	O
.	O
,	O
t	O
show	O
that	O
the	O
resulting	O
set	B
is	O
shattered	O
by	O
l	O
(	O
bd	O
,	O
t	O
)	O
.	O
2	O
k	O
instances	O
by	O
taking	O
the	O
rows	O
of	O
the	O
matrix	O
a	O
2	O
a.	O
image	O
of	O
a	O
,	O
denoted	O
by	O
i	O
(	O
a	O
)	O
,	O
is	O
the	O
matrix	O
b	O
such	O
that	O
bi	O
,	O
j	O
=	O
(	O
cid:80	O
)	O
5.	O
eﬃciently	O
calculating	O
the	O
viola	O
and	O
jones	O
features	O
using	O
an	O
inte-	O
gral	O
image	O
:	O
let	O
a	O
be	O
a	O
24	O
×	O
24	O
matrix	O
representing	O
an	O
image	O
.	O
the	O
integral	O
i	O
(	O
cid:48	O
)	O
≤i	O
,	O
j	O
(	O
cid:48	O
)	O
≤j	O
ai	O
,	O
j	O
.	O
•	O
show	O
that	O
i	O
(	O
a	O
)	O
can	O
be	O
calculated	O
from	O
a	O
in	O
time	O
linear	O
in	O
the	O
size	O
of	O
a	O
.	O
•	O
show	O
how	O
every	O
viola	O
and	O
jones	O
feature	B
can	O
be	O
calculated	O
from	O
i	O
(	O
a	O
)	O
in	O
a	O
constant	O
amount	O
of	O
time	O
(	O
that	O
is	O
,	O
the	O
runtime	O
does	O
not	O
depend	O
on	O
the	O
size	O
of	O
the	O
rectangle	O
deﬁning	O
the	O
feature	B
)	O
.	O
11	O
model	B
selection	I
and	O
validation	B
in	O
the	O
previous	O
chapter	O
we	O
have	O
described	O
the	O
adaboost	O
algorithm	O
and	O
have	O
shown	O
how	O
the	O
parameter	O
t	O
of	O
adaboost	O
controls	O
the	O
bias-complexity	O
trade-	O
oﬀ	O
.	O
but	O
,	O
how	O
do	O
we	O
set	B
t	O
in	O
practice	O
?	O
more	O
generally	O
,	O
when	O
approaching	O
some	O
practical	O
problem	O
,	O
we	O
usually	O
can	O
think	O
of	O
several	O
algorithms	O
that	O
may	O
yield	O
a	O
good	O
solution	O
,	O
each	O
of	O
which	O
might	O
have	O
several	O
parameters	O
.	O
how	O
can	O
we	O
choose	O
the	O
best	O
algorithm	O
for	O
the	O
particular	O
problem	O
at	O
hand	O
?	O
and	O
how	O
do	O
we	O
set	B
the	O
algorithm	O
’	O
s	O
parameters	O
?	O
this	O
task	O
is	O
often	O
called	O
model	B
selection	I
.	O
to	O
illustrate	O
the	O
model	B
selection	I
task	O
,	O
consider	O
the	O
problem	O
of	O
learning	O
a	O
one	O
dimensional	O
regression	B
function	O
,	O
h	O
:	O
r	O
→	O
r.	O
suppose	O
that	O
we	O
obtain	O
a	O
training	B
set	I
as	O
depicted	O
in	O
the	O
ﬁgure	O
.	O
we	O
can	O
consider	O
ﬁtting	O
a	O
polynomial	O
to	O
the	O
data	O
,	O
as	O
described	O
in	O
chapter	O
9.	O
however	O
,	O
we	O
might	O
be	O
uncertain	O
regarding	O
which	O
degree	O
d	O
would	O
give	O
the	O
best	O
results	O
for	O
our	O
data	O
set	B
:	O
a	O
small	O
degree	O
may	O
not	O
ﬁt	O
the	O
data	O
well	O
(	O
i.e.	O
,	O
it	O
will	O
have	O
a	O
large	O
approximation	B
error	I
)	O
,	O
whereas	O
a	O
high	O
degree	O
may	O
lead	O
to	O
overﬁtting	B
(	O
i.e.	O
,	O
it	O
will	O
have	O
a	O
large	O
estimation	B
error	I
)	O
.	O
in	O
the	O
following	O
we	O
depict	O
the	O
result	O
of	O
ﬁtting	O
a	O
polynomial	O
of	O
degrees	O
2	O
,	O
3	O
,	O
and	O
10.	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
empirical	B
risk	I
decreases	O
as	O
we	O
enlarge	O
the	O
degree	O
.	O
however	O
,	O
looking	O
at	O
the	O
graphs	O
,	O
our	O
intuition	O
tells	O
us	O
that	O
setting	O
the	O
degree	O
to	O
3	O
may	O
be	O
better	O
than	O
setting	O
it	O
to	O
10.	O
it	O
follows	O
that	O
the	O
empirical	B
risk	I
alone	O
is	O
not	O
enough	O
for	O
model	B
selection	I
.	O
degree	O
2	O
degree	O
3	O
degree	O
10	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
11.1	O
model	B
selection	I
using	O
srm	O
145	O
in	O
this	O
chapter	O
we	O
will	O
present	O
two	O
approaches	O
for	O
model	B
selection	I
.	O
the	O
ﬁrst	O
approach	O
is	O
based	O
on	O
the	O
structural	O
risk	B
minimization	O
(	O
srm	O
)	O
paradigm	O
we	O
have	O
described	O
and	O
analyzed	O
in	O
chapter	O
7.2.	O
srm	O
is	O
particularly	O
useful	O
when	O
a	O
learning	O
algorithm	O
depends	O
on	O
a	O
parameter	O
that	O
controls	O
the	O
bias-complexity	B
tradeoﬀ	I
(	O
such	O
as	O
the	O
degree	O
of	O
the	O
ﬁtted	O
polynomial	O
in	O
the	O
preceding	O
example	O
or	O
the	O
parameter	O
t	O
in	O
adaboost	O
)	O
.	O
the	O
second	O
approach	O
relies	O
on	O
the	O
concept	O
of	O
validation	B
.	O
the	O
basic	O
idea	O
is	O
to	O
partition	O
the	O
training	B
set	I
into	O
two	O
sets	O
.	O
one	O
is	O
used	O
for	O
training	O
each	O
of	O
the	O
candidate	O
models	O
,	O
and	O
the	O
second	O
is	O
used	O
for	O
deciding	O
which	O
of	O
them	O
yields	O
the	O
best	O
results	O
.	O
in	O
model	B
selection	I
tasks	O
,	O
we	O
try	O
to	O
ﬁnd	O
the	O
right	O
balance	O
between	O
approxi-	O
mation	O
and	O
estimation	O
errors	O
.	O
more	O
generally	O
,	O
if	O
our	O
learning	O
algorithm	O
fails	O
to	O
ﬁnd	O
a	O
predictor	B
with	O
a	O
small	O
risk	B
,	O
it	O
is	O
important	O
to	O
understand	O
whether	O
we	O
suﬀer	O
from	O
overﬁtting	B
or	O
underﬁtting	B
.	O
in	O
section	O
11.3	O
we	O
discuss	O
how	O
this	O
can	O
be	O
achieved	O
.	O
11.1	O
model	B
selection	I
using	O
srm	O
the	O
srm	O
paradigm	O
has	O
been	O
described	O
and	O
analyzed	O
in	O
section	O
7.2.	O
here	O
we	O
show	O
how	O
srm	O
can	O
be	O
used	O
for	O
tuning	O
the	O
tradeoﬀ	O
between	O
bias	B
and	O
complexity	O
without	O
deciding	O
on	O
a	O
speciﬁc	O
hypothesis	B
class	I
in	O
advance	O
.	O
consider	O
a	O
countable	O
sequence	O
of	O
hypothesis	B
classes	O
h1	O
,	O
h2	O
,	O
h3	O
,	O
.	O
.	O
..	O
for	O
example	O
,	O
in	O
the	O
problem	O
of	O
polynomial	B
regression	I
mentioned	O
,	O
we	O
can	O
take	O
hd	O
to	O
be	O
the	O
set	B
of	O
polynomials	O
of	O
degree	O
at	O
most	O
d.	O
another	O
example	O
is	O
taking	O
hd	O
to	O
be	O
the	O
class	O
l	O
(	O
b	O
,	O
d	O
)	O
used	O
by	O
adaboost	O
,	O
as	O
described	O
in	O
the	O
previous	O
chapter	O
.	O
we	O
assume	O
that	O
for	O
every	O
d	O
,	O
the	O
class	O
hd	O
enjoys	O
the	O
uniform	B
convergence	I
property	O
(	O
see	O
deﬁnition	O
4.3	O
in	O
chapter	O
4	O
)	O
with	O
a	O
sample	B
complexity	I
function	O
of	O
the	O
form	O
(	O
	O
,	O
δ	O
)	O
≤	O
g	O
(	O
d	O
)	O
log	O
(	O
1/δ	O
)	O
,	O
muchd	O
(	O
11.1	O
)	O
where	O
g	O
:	O
n	O
→	O
r	O
is	O
some	O
monotonically	O
increasing	O
function	B
.	O
for	O
example	O
,	O
in	O
the	O
case	O
of	O
binary	O
classiﬁcation	O
problems	O
,	O
we	O
can	O
take	O
g	O
(	O
d	O
)	O
to	O
be	O
the	O
vc-dimension	O
of	O
the	O
class	O
hd	O
multiplied	O
by	O
a	O
universal	O
constant	O
(	O
the	O
one	O
appearing	O
in	O
the	O
fundamental	O
theorem	O
of	O
learning	O
;	O
see	O
theorem	O
6.8	O
)	O
.	O
for	O
the	O
classes	O
l	O
(	O
b	O
,	O
d	O
)	O
used	O
by	O
adaboost	O
,	O
the	O
function	B
g	O
will	O
simply	O
grow	O
with	O
d.	O
2	O
recall	B
that	O
the	O
srm	O
rule	O
follows	O
a	O
“	O
bound	O
minimization	O
”	O
approach	O
,	O
where	O
in	O
our	O
case	O
the	O
bound	O
is	O
as	O
follows	O
:	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
,	O
for	O
every	O
d	O
∈	O
n	O
and	O
h	O
∈	O
hd	O
,	O
(	O
cid:114	O
)	O
ld	O
(	O
h	O
)	O
≤	O
ls	O
(	O
h	O
)	O
+	O
g	O
(	O
d	O
)	O
(	O
log	O
(	O
1/δ	O
)	O
+	O
2	O
log	O
(	O
d	O
)	O
+	O
log	O
(	O
π2/6	O
)	O
)	O
m	O
.	O
(	O
11.2	O
)	O
this	O
bound	O
,	O
which	O
follows	O
directly	O
from	O
theorem	O
7.4	O
,	O
shows	O
that	O
for	O
every	O
d	O
and	O
every	O
h	O
∈	O
hd	O
,	O
the	O
true	O
risk	O
is	O
bounded	O
by	O
two	O
terms	O
–	O
the	O
empirical	B
risk	I
,	O
ls	O
(	O
h	O
)	O
,	O
146	O
model	B
selection	I
and	O
validation	B
and	O
a	O
complexity	O
term	O
that	O
depends	O
on	O
d.	O
the	O
srm	O
rule	O
will	O
search	O
for	O
d	O
and	O
h	O
∈	O
hd	O
that	O
minimize	O
the	O
right-hand	O
side	O
of	O
equation	O
(	O
11.2	O
)	O
.	O
getting	O
back	O
to	O
the	O
example	O
of	O
polynomial	B
regression	I
described	O
earlier	O
,	O
even	O
though	O
the	O
empirical	B
risk	I
of	O
the	O
10th	O
degree	O
polynomial	O
is	O
smaller	O
than	O
that	O
of	O
the	O
3rd	O
degree	O
polynomial	O
,	O
we	O
would	O
still	O
prefer	O
the	O
3rd	O
degree	O
polynomial	O
since	O
its	O
complexity	O
(	O
as	O
reﬂected	O
by	O
the	O
value	O
of	O
the	O
function	B
g	O
(	O
d	O
)	O
)	O
is	O
much	O
smaller	O
.	O
while	O
the	O
srm	O
approach	O
can	O
be	O
useful	O
in	O
some	O
situations	O
,	O
in	O
many	O
practical	O
cases	O
the	O
upper	O
bound	O
given	O
in	O
equation	O
(	O
11.2	O
)	O
is	O
pessimistic	O
.	O
in	O
the	O
next	O
section	O
we	O
present	O
a	O
more	O
practical	O
approach	O
.	O
11.2	O
validation	B
we	O
would	O
often	O
like	O
to	O
get	O
a	O
better	O
estimation	O
of	O
the	O
true	O
risk	O
of	O
the	O
output	O
pre-	O
dictor	O
of	O
a	O
learning	O
algorithm	O
.	O
so	O
far	O
we	O
have	O
derived	O
bounds	O
on	O
the	O
estimation	B
error	I
of	O
a	O
hypothesis	B
class	I
,	O
which	O
tell	O
us	O
that	O
for	O
all	O
hypotheses	O
in	O
the	O
class	O
,	O
the	O
true	O
risk	O
is	O
not	O
very	O
far	O
from	O
the	O
empirical	B
risk	I
.	O
however	O
,	O
these	O
bounds	O
might	O
be	O
loose	O
and	O
pessimistic	O
,	O
as	O
they	O
hold	O
for	O
all	O
hypotheses	O
and	O
all	O
possible	O
data	O
dis-	O
tributions	O
.	O
a	O
more	O
accurate	O
estimation	O
of	O
the	O
true	O
risk	O
can	O
be	O
obtained	O
by	O
using	O
some	O
of	O
the	O
training	O
data	O
as	O
a	O
validation	B
set	O
,	O
over	O
which	O
one	O
can	O
evalutate	O
the	O
success	O
of	O
the	O
algorithm	O
’	O
s	O
output	O
predictor	B
.	O
this	O
procedure	O
is	O
called	O
validation	B
.	O
naturally	O
,	O
a	O
better	O
estimation	O
of	O
the	O
true	O
risk	O
is	O
useful	O
for	O
model	B
selection	I
,	O
as	O
we	O
will	O
describe	O
in	O
section	O
11.2.2	O
.	O
11.2.1	O
hold	B
out	I
set	O
the	O
simplest	O
way	O
to	O
estimate	O
the	O
true	B
error	I
of	O
a	O
predictor	B
h	O
is	O
by	O
sampling	O
an	O
ad-	O
ditional	O
set	B
of	O
examples	O
,	O
independent	O
of	O
the	O
training	B
set	I
,	O
and	O
using	O
the	O
empirical	B
error	I
on	O
this	O
validation	B
set	O
as	O
our	O
estimator	O
.	O
formally	O
,	O
let	O
v	O
=	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xmv	O
,	O
ymv	O
)	O
be	O
a	O
set	B
of	O
fresh	O
mv	O
examples	O
that	O
are	O
sampled	O
according	O
to	O
d	O
(	O
independently	O
of	O
the	O
m	O
examples	O
of	O
the	O
training	B
set	I
s	O
)	O
.	O
using	O
hoeﬀding	O
’	O
s	O
inequality	O
(	O
lemma	O
4.5	O
)	O
we	O
have	O
the	O
following	O
:	O
theorem	O
11.1	O
let	O
h	O
be	O
some	O
predictor	B
and	O
assume	O
that	O
the	O
loss	B
function	I
is	O
in	O
[	O
0	O
,	O
1	O
]	O
.	O
then	O
,	O
for	O
every	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
the	O
choice	O
of	O
a	O
validation	B
set	O
v	O
of	O
size	O
mv	O
we	O
have	O
(	O
cid:115	O
)	O
|lv	O
(	O
h	O
)	O
−	O
ld	O
(	O
h	O
)	O
|	O
≤	O
log	O
(	O
2/δ	O
)	O
2	O
mv	O
.	O
the	O
bound	O
in	O
theorem	O
11.1	O
does	O
not	O
depend	O
on	O
the	O
algorithm	O
or	O
the	O
training	B
set	I
used	O
to	O
construct	O
h	O
and	O
is	O
tighter	O
than	O
the	O
usual	O
bounds	O
that	O
we	O
have	O
seen	O
so	O
far	O
.	O
the	O
reason	O
for	O
the	O
tightness	O
of	O
this	O
bound	O
is	O
that	O
it	O
is	O
in	O
terms	O
of	O
an	O
estimate	O
on	O
a	O
fresh	O
validation	B
set	O
that	O
is	O
independent	O
of	O
the	O
way	O
h	O
was	O
generated	O
.	O
to	O
illustrate	O
this	O
point	O
,	O
suppose	O
that	O
h	O
was	O
obtained	O
by	O
applying	O
an	O
erm	O
predictor	B
11.2	O
validation	B
147	O
with	O
respect	O
to	O
a	O
hypothesis	B
class	I
of	O
vc-dimension	O
d	O
,	O
over	O
a	O
training	B
set	I
of	O
m	O
examples	O
.	O
then	O
,	O
from	O
the	O
fundamental	O
theorem	O
of	O
learning	O
(	O
theorem	O
6.8	O
)	O
we	O
obtain	O
the	O
bound	O
ld	O
(	O
h	O
)	O
≤	O
ls	O
(	O
h	O
)	O
+	O
c	O
d	O
+	O
log	O
(	O
1/δ	O
)	O
m	O
,	O
where	O
c	O
is	O
the	O
constant	O
appearing	O
in	O
theorem	O
6.8.	O
in	O
contrast	O
,	O
from	O
theo-	O
rem	O
11.1	O
we	O
obtain	O
the	O
bound	O
(	O
cid:114	O
)	O
(	O
cid:115	O
)	O
ld	O
(	O
h	O
)	O
≤	O
lv	O
(	O
h	O
)	O
+	O
log	O
(	O
2/δ	O
)	O
2mv	O
.	O
therefore	O
,	O
taking	O
mv	O
to	O
be	O
order	O
of	O
m	O
,	O
we	O
obtain	O
an	O
estimate	O
that	O
is	O
more	O
accurate	O
by	O
a	O
factor	O
that	O
depends	O
on	O
the	O
vc-dimension	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
price	O
we	O
pay	O
for	O
using	O
such	O
an	O
estimate	O
is	O
that	O
it	O
requires	O
an	O
additional	O
sample	O
on	O
top	O
of	O
the	O
sample	O
used	O
for	O
training	O
the	O
learner	O
.	O
sampling	O
a	O
training	B
set	I
and	O
then	O
sampling	O
an	O
independent	O
validation	B
set	O
is	O
equivalent	O
to	O
randomly	O
partitioning	O
our	O
random	O
set	O
of	O
examples	O
into	O
two	O
parts	O
,	O
using	O
one	O
part	O
for	O
training	O
and	O
the	O
other	O
one	O
for	O
validation	B
.	O
for	O
this	O
reason	O
,	O
the	O
validation	B
set	O
is	O
often	O
referred	O
to	O
as	O
a	O
hold	B
out	I
set	O
.	O
11.2.2	O
validation	B
for	O
model	B
selection	I
validation	O
can	O
be	O
naturally	O
used	O
for	O
model	B
selection	I
as	O
follows	O
.	O
we	O
ﬁrst	O
train	O
diﬀerent	O
algorithms	O
(	O
or	O
the	O
same	O
algorithm	O
with	O
diﬀerent	O
parameters	O
)	O
on	O
the	O
given	O
training	B
set	I
.	O
let	O
h	O
=	O
{	O
h1	O
,	O
.	O
.	O
.	O
,	O
hr	O
}	O
be	O
the	O
set	B
of	O
all	O
output	O
predictors	O
of	O
the	O
diﬀerent	O
algorithms	O
.	O
for	O
example	O
,	O
in	O
the	O
case	O
of	O
training	O
polynomial	O
regressors	O
,	O
we	O
would	O
have	O
each	O
hr	O
be	O
the	O
output	O
of	O
polynomial	B
regression	I
of	O
degree	O
r.	O
now	O
,	O
to	O
choose	O
a	O
single	O
predictor	O
from	O
h	O
we	O
sample	O
a	O
fresh	O
validation	B
set	O
and	O
choose	O
the	O
predictor	B
that	O
minimizes	O
the	O
error	O
over	O
the	O
validation	B
set	O
.	O
in	O
other	O
words	O
,	O
we	O
apply	O
ermh	O
over	O
the	O
validation	B
set	O
.	O
this	O
process	O
is	O
very	O
similar	O
to	O
learning	O
a	O
ﬁnite	O
hypothesis	B
class	I
.	O
the	O
only	O
diﬀerence	O
is	O
that	O
h	O
is	O
not	O
ﬁxed	O
ahead	O
of	O
time	O
but	O
rather	O
depends	O
on	O
the	O
train-	O
ing	O
set	B
.	O
however	O
,	O
since	O
the	O
validation	B
set	O
is	O
independent	O
of	O
the	O
training	B
set	I
we	O
get	O
that	O
it	O
is	O
also	O
independent	O
of	O
h	O
and	O
therefore	O
the	O
same	O
technique	O
we	O
used	O
to	O
derive	O
bounds	O
for	O
ﬁnite	O
hypothesis	B
classes	O
holds	O
here	O
as	O
well	O
.	O
in	O
particular	O
,	O
combining	O
theorem	O
11.1	O
with	O
the	O
union	B
bound	I
we	O
obtain	O
:	O
theorem	O
11.2	O
let	O
h	O
=	O
{	O
h1	O
,	O
.	O
.	O
.	O
,	O
hr	O
}	O
be	O
an	O
arbitrary	O
set	B
of	O
predictors	O
and	O
assume	O
that	O
the	O
loss	B
function	I
is	O
in	O
[	O
0	O
,	O
1	O
]	O
.	O
assume	O
that	O
a	O
validation	B
set	O
v	O
of	O
size	O
mv	O
is	O
sampled	O
independent	O
of	O
h.	O
then	O
,	O
with	O
probability	O
of	O
at	O
least	O
1−	O
δ	O
over	O
the	O
choice	O
of	O
v	O
we	O
have	O
∀h	O
∈	O
h	O
,	O
|ld	O
(	O
h	O
)	O
−	O
lv	O
(	O
h	O
)	O
|	O
≤	O
(	O
cid:115	O
)	O
log	O
(	O
2|h|/δ	O
)	O
2	O
mv	O
.	O
148	O
model	B
selection	I
and	O
validation	B
this	O
theorem	O
tells	O
us	O
that	O
the	O
error	O
on	O
the	O
validation	B
set	O
approximates	O
the	O
true	B
error	I
as	O
long	O
as	O
h	O
is	O
not	O
too	O
large	O
.	O
however	O
,	O
if	O
we	O
try	O
too	O
many	O
methods	O
(	O
resulting	O
in	O
|h|	O
that	O
is	O
large	O
relative	O
to	O
the	O
size	O
of	O
the	O
validation	B
set	O
)	O
then	O
we	O
’	O
re	O
in	O
danger	O
of	O
overﬁtting	B
.	O
to	O
illustrate	O
how	O
validation	B
is	O
useful	O
for	O
model	B
selection	I
,	O
consider	O
again	O
the	O
example	O
of	O
ﬁtting	O
a	O
one	O
dimensional	O
polynomial	O
as	O
described	O
in	O
the	O
beginning	O
of	O
this	O
chapter	O
.	O
in	O
the	O
following	O
we	O
depict	O
the	O
same	O
training	B
set	I
,	O
with	O
erm	O
polynomials	O
of	O
degree	O
2	O
,	O
3	O
,	O
and	O
10	O
,	O
but	O
this	O
time	O
we	O
also	O
depict	O
an	O
additional	O
validation	B
set	O
(	O
marked	O
as	O
red	O
,	O
unﬁlled	O
circles	O
)	O
.	O
the	O
polynomial	O
of	O
degree	O
10	O
has	O
minimal	O
training	B
error	I
,	O
yet	O
the	O
polynomial	O
of	O
degree	O
3	O
has	O
the	O
minimal	O
validation	B
error	O
,	O
and	O
hence	O
it	O
will	O
be	O
chosen	O
as	O
the	O
best	O
model	O
.	O
11.2.3	O
the	O
model-selection	O
curve	O
the	O
model	B
selection	I
curve	O
shows	O
the	O
training	B
error	I
and	O
validation	B
error	O
as	O
a	O
func-	O
tion	O
of	O
the	O
complexity	O
of	O
the	O
model	O
considered	O
.	O
for	O
example	O
,	O
for	O
the	O
polynomial	O
ﬁtting	O
problem	O
mentioned	O
previously	O
,	O
the	O
curve	O
will	O
look	O
like	O
:	O
11.2	O
validation	B
149	O
train	O
validation	B
r	O
o	O
r	O
r	O
e	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
2	O
4	O
6	O
d	O
8	O
10	O
as	O
can	O
be	O
shown	O
,	O
the	O
training	B
error	I
is	O
monotonically	O
decreasing	O
as	O
we	O
increase	O
the	O
polynomial	O
degree	O
(	O
which	O
is	O
the	O
complexity	O
of	O
the	O
model	O
in	O
our	O
case	O
)	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
validation	B
error	O
ﬁrst	O
decreases	O
but	O
then	O
starts	O
to	O
increase	O
,	O
which	O
indicates	O
that	O
we	O
are	O
starting	O
to	O
suﬀer	O
from	O
overﬁtting	B
.	O
plotting	O
such	O
curves	O
can	O
help	O
us	O
understand	O
whether	O
we	O
are	O
searching	O
the	O
correct	O
regime	O
of	O
our	O
parameter	O
space	O
.	O
often	O
,	O
there	O
may	O
be	O
more	O
than	O
a	O
single	O
parameter	O
to	O
tune	O
,	O
and	O
the	O
possible	O
number	O
of	O
values	O
each	O
parameter	O
can	O
take	O
might	O
be	O
quite	O
large	O
.	O
for	O
example	O
,	O
in	O
chapter	O
13	O
we	O
describe	O
the	O
concept	O
of	O
regularization	B
,	O
in	O
which	O
the	O
parameter	O
of	O
the	O
learning	O
algorithm	O
is	O
a	O
real	O
number	O
.	O
in	O
such	O
cases	O
,	O
we	O
start	O
with	O
a	O
rough	O
grid	O
of	O
values	O
for	O
the	O
parameter	O
(	O
s	O
)	O
and	O
plot	O
the	O
corresponding	O
model-selection	O
curve	O
.	O
on	O
the	O
basis	O
of	O
the	O
curve	O
we	O
will	O
zoom	O
in	O
to	O
the	O
correct	O
regime	O
and	O
employ	O
a	O
ﬁner	O
grid	O
to	O
search	O
over	O
.	O
it	O
is	O
important	O
to	O
verify	O
that	O
we	O
are	O
in	O
the	O
relevant	O
regime	O
.	O
for	O
example	O
,	O
in	O
the	O
polynomial	O
ﬁtting	O
problem	O
described	O
,	O
if	O
we	O
start	O
searching	O
degrees	O
from	O
the	O
set	B
of	O
values	O
{	O
1	O
,	O
10	O
,	O
20	O
}	O
and	O
do	O
not	O
employ	O
a	O
ﬁner	O
grid	O
based	O
on	O
the	O
resulting	O
curve	O
,	O
we	O
will	O
end	O
up	O
with	O
a	O
rather	O
poor	O
model	O
.	O
11.2.4	O
k-fold	O
cross	B
validation	I
the	O
validation	B
procedure	O
described	O
so	O
far	O
assumes	O
that	O
data	O
is	O
plentiful	O
and	O
that	O
we	O
have	O
the	O
ability	O
to	O
sample	O
a	O
fresh	O
validation	B
set	O
.	O
but	O
in	O
some	O
applications	O
,	O
data	O
is	O
scarce	O
and	O
we	O
do	O
not	O
want	O
to	O
“	O
waste	O
”	O
data	O
on	O
validation	B
.	O
the	O
k-fold	O
cross	B
validation	I
technique	O
is	O
designed	O
to	O
give	O
an	O
accurate	O
estimate	O
of	O
the	O
true	B
error	I
without	O
wasting	O
too	O
much	O
data	O
.	O
in	O
k-fold	O
cross	B
validation	I
the	O
original	O
training	B
set	I
is	O
partitioned	O
into	O
k	O
subsets	O
(	O
folds	O
)	O
of	O
size	O
m/k	O
(	O
for	O
simplicity	O
,	O
assume	O
that	O
m/k	O
is	O
an	O
integer	O
)	O
.	O
for	O
each	O
fold	O
,	O
the	O
algorithm	O
is	O
trained	O
on	O
the	O
union	O
of	O
the	O
other	O
folds	O
and	O
then	O
the	O
error	O
of	O
its	O
output	O
is	O
estimated	O
using	O
the	O
fold	O
.	O
finally	O
,	O
the	O
average	O
of	O
all	O
these	O
errors	O
is	O
the	O
150	O
model	B
selection	I
and	O
validation	B
estimate	O
of	O
the	O
true	B
error	I
.	O
the	O
special	O
case	O
k	O
=	O
m	O
,	O
where	O
m	O
is	O
the	O
number	O
of	O
examples	O
,	O
is	O
called	O
leave-one-out	O
(	O
loo	O
)	O
.	O
k-fold	O
cross	B
validation	I
is	O
often	O
used	O
for	O
model	B
selection	I
(	O
or	O
parameter	O
tuning	O
)	O
,	O
and	O
once	O
the	O
best	O
parameter	O
is	O
chosen	O
,	O
the	O
algorithm	O
is	O
retrained	O
using	O
this	O
parameter	O
on	O
the	O
entire	O
training	B
set	I
.	O
a	O
pseudocode	O
of	O
k-fold	O
cross	B
validation	I
for	O
model	B
selection	I
is	O
given	O
in	O
the	O
following	O
.	O
the	O
procedure	O
receives	O
as	O
input	O
a	O
training	B
set	I
,	O
s	O
,	O
a	O
set	B
of	O
possible	O
parameter	O
values	O
,	O
θ	O
,	O
an	O
integer	O
,	O
k	O
,	O
representing	O
the	O
number	O
of	O
folds	O
,	O
and	O
a	O
learning	O
algorithm	O
,	O
a	O
,	O
which	O
receives	O
as	O
input	O
a	O
training	B
set	I
as	O
well	O
as	O
a	O
parameter	O
θ	O
∈	O
θ.	O
it	O
outputs	O
the	O
best	O
parameter	O
as	O
well	O
as	O
the	O
hypothesis	B
trained	O
by	O
this	O
parameter	O
on	O
the	O
entire	O
training	B
set	I
.	O
k-fold	O
cross	B
validation	I
for	O
model	B
selection	I
input	O
:	O
training	B
set	I
s	O
=	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
set	B
of	O
parameter	O
values	O
θ	O
learning	O
algorithm	O
a	O
integer	O
k	O
partition	O
s	O
into	O
s1	O
,	O
s2	O
,	O
.	O
.	O
.	O
,	O
sk	O
foreach	O
θ	O
∈	O
θ	O
error	O
(	O
θ	O
)	O
=	O
1	O
k	O
output	O
i=1	O
lsi	O
(	O
hi	O
,	O
θ	O
)	O
for	O
i	O
=	O
1	O
.	O
.	O
.	O
k	O
hi	O
,	O
θ	O
=	O
a	O
(	O
s	O
\	O
si	O
;	O
θ	O
)	O
(	O
cid:80	O
)	O
k	O
θ	O
(	O
cid:63	O
)	O
=	O
argminθ	O
[	O
error	O
(	O
θ	O
)	O
]	O
hθ	O
(	O
cid:63	O
)	O
=	O
a	O
(	O
s	O
;	O
θ	O
(	O
cid:63	O
)	O
)	O
the	O
cross	B
validation	I
method	O
often	O
works	O
very	O
well	O
in	O
practice	O
.	O
however	O
,	O
it	O
might	O
sometime	O
fail	O
,	O
as	O
the	O
artiﬁcial	O
example	O
given	O
in	O
exercise	O
1	O
shows	O
.	O
rig-	O
orously	O
understanding	O
the	O
exact	O
behavior	O
of	O
cross	B
validation	I
is	O
still	O
an	O
open	O
problem	O
.	O
rogers	O
and	O
wagner	O
(	O
rogers	O
&	O
wagner	O
1978	O
)	O
have	O
shown	O
that	O
for	O
k	O
local	O
rules	O
(	O
e.g.	O
,	O
k	O
nearest	O
neighbor	O
;	O
see	O
chapter	O
19	O
)	O
the	O
cross	B
validation	I
proce-	O
dure	O
gives	O
a	O
very	O
good	O
estimate	O
of	O
the	O
true	B
error	I
.	O
other	O
papers	O
show	O
that	O
cross	B
validation	I
works	O
for	O
stable	O
algorithms	O
(	O
we	O
will	O
study	O
stability	B
and	O
its	O
relation	O
to	O
learnability	O
in	O
chapter	O
13	O
)	O
.	O
11.2.5	O
train-validation-test	B
split	I
in	O
most	O
practical	O
applications	O
,	O
we	O
split	O
the	O
available	O
examples	O
into	O
three	O
sets	O
.	O
the	O
ﬁrst	O
set	B
is	O
used	O
for	O
training	O
our	O
algorithm	O
and	O
the	O
second	O
is	O
used	O
as	O
a	O
validation	B
set	O
for	O
model	B
selection	I
.	O
after	O
we	O
select	O
the	O
best	O
model	O
,	O
we	O
test	O
the	O
performance	O
of	O
the	O
output	O
predictor	B
on	O
the	O
third	O
set	B
,	O
which	O
is	O
often	O
called	O
the	O
“	O
test	O
set.	O
”	O
the	O
number	O
obtained	O
is	O
used	O
as	O
an	O
estimator	O
of	O
the	O
true	B
error	I
of	O
the	O
learned	O
predictor	B
.	O
11.3	O
what	O
to	O
do	O
if	O
learning	O
fails	O
151	O
11.3	O
what	O
to	O
do	O
if	O
learning	O
fails	O
consider	O
the	O
following	O
scenario	O
:	O
you	O
were	O
given	O
a	O
learning	O
task	O
and	O
have	O
ap-	O
proached	O
it	O
with	O
a	O
choice	O
of	O
a	O
hypothesis	B
class	I
,	O
a	O
learning	O
algorithm	O
,	O
and	O
param-	O
eters	O
.	O
you	O
used	O
a	O
validation	B
set	O
to	O
tune	O
the	O
parameters	O
and	O
tested	O
the	O
learned	O
predictor	B
on	O
a	O
test	O
set	B
.	O
the	O
test	O
results	O
,	O
unfortunately	O
,	O
turn	O
out	O
to	O
be	O
unsatis-	O
factory	O
.	O
what	O
went	O
wrong	O
then	O
,	O
and	O
what	O
should	O
you	O
do	O
next	O
?	O
there	O
are	O
many	O
elements	O
that	O
can	O
be	O
“	O
ﬁxed.	O
”	O
the	O
main	O
approaches	O
are	O
listed	O
in	O
the	O
following	O
:	O
•	O
get	O
a	O
larger	O
sample	O
•	O
change	O
the	O
hypothesis	B
class	I
by	O
:	O
–	O
enlarging	O
it	O
–	O
reducing	O
it	O
–	O
completely	O
changing	O
it	O
–	O
changing	O
the	O
parameters	O
you	O
consider	O
•	O
change	O
the	O
feature	B
representation	O
of	O
the	O
data	O
•	O
change	O
the	O
optimization	O
algorithm	O
used	O
to	O
apply	O
your	O
learning	O
rule	O
in	O
order	O
to	O
ﬁnd	O
the	O
best	O
remedy	O
,	O
it	O
is	O
essential	O
ﬁrst	O
to	O
understand	O
the	O
cause	O
of	O
the	O
bad	O
performance	O
.	O
recall	B
that	O
in	O
chapter	O
5	O
we	O
decomposed	O
the	O
true	O
er-	O
ror	O
of	O
the	O
learned	O
predictor	B
into	O
approximation	B
error	I
and	O
estimation	B
error	I
.	O
the	O
approximation	B
error	I
is	O
deﬁned	O
to	O
be	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
for	O
some	O
h	O
(	O
cid:63	O
)	O
∈	O
argminh∈h	O
ld	O
(	O
h	O
)	O
,	O
while	O
the	O
estimation	B
error	I
is	O
deﬁned	O
to	O
be	O
ld	O
(	O
hs	O
)	O
−	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
,	O
where	O
hs	O
is	O
the	O
learned	O
predictor	B
(	O
which	O
is	O
based	O
on	O
the	O
training	B
set	I
s	O
)	O
.	O
the	O
approximation	B
error	I
of	O
the	O
class	O
does	O
not	O
depend	O
on	O
the	O
sample	O
size	O
or	O
on	O
the	O
algorithm	O
being	O
used	O
.	O
it	O
only	O
depends	O
on	O
the	O
distribution	O
d	O
and	O
on	O
the	O
hypothesis	B
class	I
h.	O
therefore	O
,	O
if	O
the	O
approximation	B
error	I
is	O
large	O
,	O
it	O
will	O
not	O
help	O
us	O
to	O
enlarge	O
the	O
training	B
set	I
size	O
,	O
and	O
it	O
also	O
does	O
not	O
make	O
sense	O
to	O
reduce	O
the	O
hypothesis	B
class	I
.	O
what	O
can	O
be	O
beneﬁcial	O
in	O
this	O
case	O
is	O
to	O
enlarge	O
the	O
hypothesis	B
class	I
or	O
completely	O
change	O
it	O
(	O
if	O
we	O
have	O
some	O
alternative	O
prior	B
knowledge	I
in	O
the	O
form	O
of	O
a	O
diﬀerent	O
hypothesis	B
class	I
)	O
.	O
we	O
can	O
also	O
consider	O
applying	O
the	O
same	O
hypothesis	B
class	I
but	O
on	O
a	O
diﬀerent	O
feature	B
representation	O
of	O
the	O
data	O
(	O
see	O
chapter	O
25	O
)	O
.	O
the	O
estimation	B
error	I
of	O
the	O
class	O
does	O
depend	O
on	O
the	O
sample	O
size	O
.	O
therefore	O
,	O
if	O
we	O
have	O
a	O
large	O
estimation	B
error	I
we	O
can	O
make	O
an	O
eﬀort	O
to	O
obtain	O
more	O
training	O
examples	O
.	O
we	O
can	O
also	O
consider	O
reducing	O
the	O
hypothesis	B
class	I
.	O
however	O
,	O
it	O
doesn	O
’	O
t	O
make	O
sense	O
to	O
enlarge	O
the	O
hypothesis	B
class	I
in	O
that	O
case	O
.	O
error	B
decomposition	I
using	O
validation	B
we	O
see	O
that	O
understanding	O
whether	O
our	O
problem	O
is	O
due	O
to	O
approximation	B
error	I
or	O
estimation	B
error	I
is	O
very	O
useful	O
for	O
ﬁnding	O
the	O
best	O
remedy	O
.	O
in	O
the	O
previous	O
section	O
we	O
saw	O
how	O
to	O
estimate	O
ld	O
(	O
hs	O
)	O
using	O
the	O
empirical	B
risk	I
on	O
a	O
validation	B
set	O
.	O
however	O
,	O
it	O
is	O
more	O
diﬃcult	O
to	O
estimate	O
the	O
approximation	B
error	I
of	O
the	O
class	O
.	O
152	O
model	B
selection	I
and	O
validation	B
instead	O
,	O
we	O
give	O
a	O
diﬀerent	O
error	B
decomposition	I
,	O
one	O
that	O
can	O
be	O
estimated	O
from	O
the	O
train	O
and	O
validation	B
sets	O
.	O
ld	O
(	O
hs	O
)	O
=	O
(	O
ld	O
(	O
hs	O
)	O
−	O
lv	O
(	O
hs	O
)	O
)	O
+	O
(	O
lv	O
(	O
hs	O
)	O
−	O
ls	O
(	O
hs	O
)	O
)	O
+	O
ls	O
(	O
hs	O
)	O
.	O
the	O
ﬁrst	O
term	O
,	O
(	O
ld	O
(	O
hs	O
)	O
−	O
lv	O
(	O
hs	O
)	O
)	O
,	O
can	O
be	O
bounded	O
quite	O
tightly	O
using	O
theo-	O
rem	O
11.1.	O
intuitively	O
,	O
when	O
the	O
second	O
term	O
,	O
(	O
lv	O
(	O
hs	O
)	O
−	O
ls	O
(	O
hs	O
)	O
)	O
,	O
is	O
large	O
we	O
say	O
that	O
our	O
algorithm	O
suﬀers	O
from	O
“	O
overﬁtting	B
”	O
while	O
when	O
the	O
empirical	B
risk	I
term	O
,	O
ls	O
(	O
hs	O
)	O
,	O
is	O
large	O
we	O
say	O
that	O
our	O
algorithm	O
suﬀers	O
from	O
“	O
underﬁtting.	O
”	O
note	O
that	O
these	O
two	O
terms	O
are	O
not	O
necessarily	O
good	O
estimates	O
of	O
the	O
estimation	O
and	O
ap-	O
proximation	O
errors	O
.	O
to	O
illustrate	O
this	O
,	O
consider	O
the	O
case	O
in	O
which	O
h	O
is	O
a	O
class	O
of	O
vc-dimension	O
d	O
,	O
and	O
d	O
is	O
a	O
distribution	O
such	O
that	O
the	O
approximation	B
error	I
of	O
h	O
with	O
respect	O
to	O
d	O
is	O
1/4	O
.	O
as	O
long	O
as	O
the	O
size	O
of	O
our	O
training	B
set	I
is	O
smaller	O
than	O
d	O
we	O
will	O
have	O
ls	O
(	O
hs	O
)	O
=	O
0	O
for	O
every	O
erm	O
hypothesis	B
.	O
therefore	O
,	O
the	O
training	O
risk	O
,	O
ls	O
(	O
hs	O
)	O
,	O
and	O
the	O
approximation	B
error	I
,	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
,	O
can	O
be	O
signiﬁcantly	O
diﬀerent	O
.	O
nevertheless	O
,	O
as	O
we	O
show	O
later	O
,	O
the	O
values	O
of	O
ls	O
(	O
hs	O
)	O
and	O
(	O
lv	O
(	O
hs	O
)	O
−	O
ls	O
(	O
hs	O
)	O
)	O
still	O
provide	O
us	O
useful	O
information	O
.	O
consider	O
ﬁrst	O
the	O
case	O
in	O
which	O
ls	O
(	O
hs	O
)	O
is	O
large	O
.	O
we	O
can	O
write	O
ls	O
(	O
hs	O
)	O
=	O
(	O
ls	O
(	O
hs	O
)	O
−	O
ls	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
)	O
+	O
(	O
ls	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
−	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
)	O
+	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
.	O
when	O
hs	O
is	O
an	O
ermh	O
hypothesis	B
we	O
have	O
that	O
ls	O
(	O
hs	O
)	O
−ls	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
≤	O
0.	O
in	O
addition	O
,	O
since	O
h	O
(	O
cid:63	O
)	O
does	O
not	O
depend	O
on	O
s	O
,	O
the	O
term	O
(	O
ls	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
−ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
)	O
can	O
be	O
bounded	O
quite	O
tightly	O
(	O
as	O
in	O
theorem	O
11.1	O
)	O
.	O
the	O
last	O
term	O
is	O
the	O
approximation	B
error	I
.	O
it	O
follows	O
that	O
if	O
ls	O
(	O
hs	O
)	O
is	O
large	O
then	O
so	O
is	O
the	O
approximation	B
error	I
,	O
and	O
the	O
remedy	O
to	O
the	O
failure	O
of	O
our	O
algorithm	O
should	O
be	O
tailored	O
accordingly	O
(	O
as	O
discussed	O
previously	O
)	O
.	O
remark	O
11.1	O
it	O
is	O
possible	O
that	O
the	O
approximation	B
error	I
of	O
our	O
class	O
is	O
small	O
,	O
yet	O
the	O
value	O
of	O
ls	O
(	O
hs	O
)	O
is	O
large	O
.	O
for	O
example	O
,	O
maybe	O
we	O
had	O
a	O
bug	O
in	O
our	O
erm	O
implementation	O
,	O
and	O
the	O
algorithm	O
returns	O
a	O
hypothesis	B
hs	O
that	O
is	O
not	O
an	O
erm	O
.	O
it	O
may	O
also	O
be	O
the	O
case	O
that	O
ﬁnding	O
an	O
erm	O
hypothesis	B
is	O
computationally	O
hard	O
,	O
and	O
our	O
algorithm	O
applies	O
some	O
heuristic	O
trying	O
to	O
ﬁnd	O
an	O
approximate	O
erm	O
.	O
in	O
some	O
cases	O
,	O
it	O
is	O
hard	O
to	O
know	O
how	O
good	O
hs	O
is	O
relative	O
to	O
an	O
erm	O
hypothesis	B
.	O
but	O
,	O
sometimes	O
it	O
is	O
possible	O
at	O
least	O
to	O
know	O
whether	O
there	O
are	O
better	O
hypotheses	O
.	O
for	O
example	O
,	O
in	O
the	O
next	O
chapter	O
we	O
will	O
study	O
convex	B
learning	O
problems	O
in	O
which	O
there	O
are	O
optimality	O
conditions	O
that	O
can	O
be	O
checked	O
to	O
verify	O
whether	O
our	O
optimization	O
algorithm	O
converged	O
to	O
an	O
erm	O
solution	O
.	O
in	O
other	O
cases	O
,	O
the	O
solution	O
may	O
depend	O
on	O
randomness	O
in	O
initializing	O
the	O
algorithm	O
,	O
so	O
we	O
can	O
try	O
diﬀerent	O
randomly	O
selected	O
initial	O
points	O
to	O
see	O
whether	O
better	O
solutions	O
pop	O
out	O
.	O
next	O
consider	O
the	O
case	O
in	O
which	O
ls	O
(	O
hs	O
)	O
is	O
small	O
.	O
as	O
we	O
argued	O
before	O
,	O
this	O
does	O
not	O
necessarily	O
imply	O
that	O
the	O
approximation	B
error	I
is	O
small	O
.	O
indeed	O
,	O
consider	O
two	O
scenarios	O
,	O
in	O
both	O
of	O
which	O
we	O
are	O
trying	O
to	O
learn	O
a	O
hypothesis	B
class	I
of	O
vc-dimension	O
d	O
using	O
the	O
erm	O
learning	O
rule	O
.	O
in	O
the	O
ﬁrst	O
scenario	O
,	O
we	O
have	O
a	O
training	B
set	I
of	O
m	O
<	O
d	O
examples	O
and	O
the	O
approximation	B
error	I
of	O
the	O
class	O
is	O
high	O
.	O
in	O
the	O
second	O
scenario	O
,	O
we	O
have	O
a	O
training	B
set	I
of	O
m	O
>	O
2d	O
examples	O
and	O
the	O
11.3	O
what	O
to	O
do	O
if	O
learning	O
fails	O
153	O
error	O
error	O
validation	B
error	O
train	O
error	O
m	O
validationerror	O
train	O
error	O
m	O
figure	O
11.1	O
examples	O
of	O
learning	B
curves	I
.	O
left	O
:	O
this	O
learning	O
curve	O
corresponds	O
to	O
the	O
scenario	O
in	O
which	O
the	O
number	O
of	O
examples	O
is	O
always	O
smaller	O
than	O
the	O
vc	O
dimension	B
of	O
the	O
class	O
.	O
right	O
:	O
this	O
learning	O
curve	O
corresponds	O
to	O
the	O
scenario	O
in	O
which	O
the	O
approximation	B
error	I
is	O
zero	O
and	O
the	O
number	O
of	O
examples	O
is	O
larger	O
than	O
the	O
vc	O
dimension	B
of	O
the	O
class	O
.	O
approximation	B
error	I
of	O
the	O
class	O
is	O
zero	O
.	O
in	O
both	O
cases	O
ls	O
(	O
hs	O
)	O
=	O
0.	O
how	O
can	O
we	O
distinguish	O
between	O
the	O
two	O
cases	O
?	O
learning	B
curves	I
one	O
possible	O
way	O
to	O
distinguish	O
between	O
the	O
two	O
cases	O
is	O
by	O
plotting	O
learning	B
curves	I
.	O
to	O
produce	O
a	O
learning	O
curve	O
we	O
train	O
the	O
algorithm	O
on	O
preﬁxes	O
of	O
the	O
data	O
of	O
increasing	O
sizes	O
.	O
for	O
example	O
,	O
we	O
can	O
ﬁrst	O
train	O
the	O
algorithm	O
on	O
the	O
ﬁrst	O
10	O
%	O
of	O
the	O
examples	O
,	O
then	O
on	O
20	O
%	O
of	O
them	O
,	O
and	O
so	O
on	O
.	O
for	O
each	O
preﬁx	O
we	O
calculate	O
the	O
training	B
error	I
(	O
on	O
the	O
preﬁx	O
the	O
algorithm	O
is	O
being	O
trained	O
on	O
)	O
and	O
the	O
validation	B
error	O
(	O
on	O
a	O
predeﬁned	O
validation	B
set	O
)	O
.	O
such	O
learning	B
curves	I
can	O
help	O
us	O
distinguish	O
between	O
the	O
two	O
aforementioned	O
scenarios	O
.	O
in	O
the	O
ﬁrst	O
scenario	O
we	O
expect	O
the	O
validation	B
error	O
to	O
be	O
approximately	O
1/2	O
for	O
all	O
preﬁxes	O
,	O
as	O
we	O
didn	O
’	O
t	O
really	O
learn	O
anything	O
.	O
in	O
the	O
second	O
scenario	O
the	O
validation	B
error	O
will	O
start	O
as	O
a	O
constant	O
but	O
then	O
should	O
start	O
decreasing	O
(	O
it	O
must	O
start	O
decreasing	O
once	O
the	O
training	B
set	I
size	O
is	O
larger	O
than	O
the	O
vc-dimension	O
)	O
.	O
an	O
illustration	O
of	O
the	O
two	O
cases	O
is	O
given	O
in	O
figure	O
11.1.	O
in	O
general	O
,	O
as	O
long	O
as	O
the	O
approximation	B
error	I
is	O
greater	O
than	O
zero	O
we	O
expect	O
the	O
training	B
error	I
to	O
grow	O
with	O
the	O
sample	O
size	O
,	O
as	O
a	O
larger	O
amount	O
of	O
data	O
points	O
makes	O
it	O
harder	O
to	O
provide	O
an	O
explanation	O
for	O
all	O
of	O
them	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
validation	B
error	O
tends	O
to	O
decrease	O
with	O
the	O
increase	O
in	O
sample	O
size	O
.	O
if	O
the	O
vc-dimension	O
is	O
ﬁnite	O
,	O
when	O
the	O
sample	O
size	O
goes	O
to	O
inﬁnity	O
,	O
the	O
validation	B
and	O
train	O
errors	O
converge	O
to	O
the	O
approximation	B
error	I
.	O
therefore	O
,	O
by	O
extrapolating	O
the	O
training	O
and	O
validation	B
curves	O
we	O
can	O
try	O
to	O
guess	O
the	O
value	O
of	O
the	O
approx-	O
imation	O
error	O
,	O
or	O
at	O
least	O
to	O
get	O
a	O
rough	O
estimate	O
on	O
an	O
interval	O
in	O
which	O
the	O
approximation	B
error	I
resides	O
.	O
getting	O
back	O
to	O
the	O
problem	O
of	O
ﬁnding	O
the	O
best	O
remedy	O
for	O
the	O
failure	O
of	O
our	O
algorithm	O
,	O
if	O
we	O
observe	O
that	O
ls	O
(	O
hs	O
)	O
is	O
small	O
while	O
the	O
validation	B
error	O
is	O
large	O
,	O
then	O
in	O
any	O
case	O
we	O
know	O
that	O
the	O
size	O
of	O
our	O
training	B
set	I
is	O
not	O
suﬃcient	O
for	O
learning	O
the	O
class	O
h.	O
we	O
can	O
then	O
plot	O
a	O
learning	O
curve	O
.	O
if	O
we	O
see	O
that	O
the	O
154	O
model	B
selection	I
and	O
validation	B
validation	O
error	O
is	O
starting	O
to	O
decrease	O
then	O
the	O
best	O
solution	O
is	O
to	O
increase	O
the	O
number	O
of	O
examples	O
(	O
if	O
we	O
can	O
aﬀord	O
to	O
enlarge	O
the	O
data	O
)	O
.	O
another	O
reasonable	O
solution	O
is	O
to	O
decrease	O
the	O
complexity	O
of	O
the	O
hypothesis	B
class	I
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
we	O
see	O
that	O
the	O
validation	B
error	O
is	O
kept	O
around	O
1/2	O
then	O
we	O
have	O
no	O
evidence	O
that	O
the	O
approximation	B
error	I
of	O
h	O
is	O
good	O
.	O
it	O
may	O
be	O
the	O
case	O
that	O
increasing	O
the	O
training	B
set	I
size	O
will	O
not	O
help	O
us	O
at	O
all	O
.	O
obtaining	O
more	O
data	O
can	O
still	O
help	O
us	O
,	O
as	O
at	O
some	O
point	O
we	O
can	O
see	O
whether	O
the	O
validation	B
error	O
starts	O
to	O
decrease	O
or	O
whether	O
the	O
training	B
error	I
starts	O
to	O
increase	O
.	O
but	O
,	O
if	O
more	O
data	O
is	O
expensive	O
,	O
it	O
may	O
be	O
better	O
ﬁrst	O
to	O
try	O
to	O
reduce	O
the	O
complexity	O
of	O
the	O
hypothesis	B
class	I
.	O
to	O
summarize	O
the	O
discussion	O
,	O
the	O
following	O
steps	O
should	O
be	O
applied	O
:	O
1.	O
if	O
learning	O
involves	O
parameter	O
tuning	O
,	O
plot	O
the	O
model-selection	O
curve	O
to	O
make	O
sure	O
that	O
you	O
tuned	O
the	O
parameters	O
appropriately	O
(	O
see	O
section	O
11.2.3	O
)	O
.	O
2.	O
if	O
the	O
training	B
error	I
is	O
excessively	O
large	O
consider	O
enlarging	O
the	O
hypothesis	B
class	I
,	O
completely	O
change	O
it	O
,	O
or	O
change	O
the	O
feature	B
representation	O
of	O
the	O
data	O
.	O
3.	O
if	O
the	O
training	B
error	I
is	O
small	O
,	O
plot	O
learning	B
curves	I
and	O
try	O
to	O
deduce	O
from	O
them	O
whether	O
the	O
problem	O
is	O
estimation	B
error	I
or	O
approximation	B
error	I
.	O
4.	O
if	O
the	O
approximation	B
error	I
seems	O
to	O
be	O
small	O
enough	O
,	O
try	O
to	O
obtain	O
more	O
data	O
.	O
if	O
this	O
is	O
not	O
possible	O
,	O
consider	O
reducing	O
the	O
complexity	O
of	O
the	O
hypothesis	B
class	I
.	O
5.	O
if	O
the	O
approximation	B
error	I
seems	O
to	O
be	O
large	O
as	O
well	O
,	O
try	O
to	O
change	O
the	O
hy-	O
pothesis	O
class	O
or	O
the	O
feature	B
representation	O
of	O
the	O
data	O
completely	O
.	O
11.4	O
summary	O
model	B
selection	I
is	O
the	O
task	O
of	O
selecting	O
an	O
appropriate	O
model	O
for	O
the	O
learning	O
task	O
based	O
on	O
the	O
data	O
itself	O
.	O
we	O
have	O
shown	O
how	O
this	O
can	O
be	O
done	O
using	O
the	O
srm	O
learning	O
paradigm	O
or	O
using	O
the	O
more	O
practical	O
approach	O
of	O
validation	B
.	O
if	O
our	O
learning	O
algorithm	O
fails	O
,	O
a	O
decomposition	O
of	O
the	O
algorithm	O
’	O
s	O
error	O
should	O
be	O
performed	O
using	O
learning	B
curves	I
,	O
so	O
as	O
to	O
ﬁnd	O
the	O
best	O
remedy	O
.	O
11.5	O
exercises	O
1.	O
failure	O
of	O
k-fold	O
cross	B
validation	I
consider	O
a	O
case	O
in	O
that	O
the	O
label	B
is	O
chosen	O
at	O
random	O
according	O
to	O
p	O
[	O
y	O
=	O
1	O
]	O
=	O
p	O
[	O
y	O
=	O
0	O
]	O
=	O
1/2	O
.	O
consider	O
a	O
learning	O
algorithm	O
that	O
outputs	O
the	O
constant	O
predictor	B
h	O
(	O
x	O
)	O
=	O
1	O
if	O
the	O
parity	O
of	O
the	O
labels	O
on	O
the	O
training	B
set	I
is	O
1	O
and	O
otherwise	O
the	O
algorithm	O
outputs	O
the	O
constant	O
predictor	B
h	O
(	O
x	O
)	O
=	O
0.	O
prove	O
that	O
the	O
diﬀerence	O
between	O
the	O
leave-one-	O
out	O
estimate	O
and	O
the	O
true	B
error	I
in	O
such	O
a	O
case	O
is	O
always	O
1/2	O
.	O
2.	O
let	O
h1	O
,	O
.	O
.	O
.	O
,	O
hk	O
be	O
k	O
hypothesis	B
classes	O
.	O
suppose	O
you	O
are	O
given	O
m	O
i.i.d	O
.	O
training	O
i=1hi	O
.	O
consider	O
two	O
examples	O
and	O
you	O
would	O
like	O
to	O
learn	O
the	O
class	O
h	O
=	O
∪k	O
alternative	O
approaches	O
:	O
•	O
learn	O
h	O
on	O
the	O
m	O
examples	O
using	O
the	O
erm	O
rule	O
11.5	O
exercises	O
155	O
•	O
divide	O
the	O
m	O
examples	O
into	O
a	O
training	B
set	I
of	O
size	O
(	O
1−	O
α	O
)	O
m	O
and	O
a	O
validation	B
set	O
of	O
size	O
αm	O
,	O
for	O
some	O
α	O
∈	O
(	O
0	O
,	O
1	O
)	O
.	O
then	O
,	O
apply	O
the	O
approach	O
of	O
model	B
selection	I
using	O
validation	B
.	O
that	O
is	O
,	O
ﬁrst	O
train	O
each	O
class	O
hi	O
on	O
the	O
(	O
1	O
−	O
α	O
)	O
m	O
training	O
examples	O
using	O
the	O
erm	O
rule	O
with	O
respect	O
to	O
hi	O
,	O
and	O
let	O
ˆh1	O
,	O
.	O
.	O
.	O
,	O
ˆhk	O
be	O
the	O
resulting	O
hypotheses	O
.	O
second	O
,	O
apply	O
the	O
erm	O
rule	O
with	O
respect	O
to	O
the	O
ﬁnite	O
class	O
{	O
ˆh1	O
,	O
.	O
.	O
.	O
,	O
ˆhk	O
}	O
on	O
the	O
αm	O
validation	B
examples	O
.	O
describe	O
scenarios	O
in	O
which	O
the	O
ﬁrst	O
method	O
is	O
better	O
than	O
the	O
second	O
and	O
vice	O
versa	O
.	O
12	O
convex	B
learning	O
problems	O
in	O
this	O
chapter	O
we	O
introduce	O
convex	B
learning	O
problems	O
.	O
convex	B
learning	O
comprises	O
an	O
important	O
family	O
of	O
learning	O
problems	O
,	O
mainly	O
because	O
most	O
of	O
what	O
we	O
can	O
learn	O
eﬃciently	O
falls	O
into	O
it	O
.	O
we	O
have	O
already	O
encountered	O
linear	B
regression	I
with	O
the	O
squared	O
loss	B
and	O
logistic	B
regression	I
,	O
which	O
are	O
convex	B
problems	O
,	O
and	O
indeed	O
they	O
can	O
be	O
learned	O
eﬃciently	O
.	O
we	O
have	O
also	O
seen	O
nonconvex	O
problems	O
,	O
such	O
as	O
halfspaces	O
with	O
the	O
0-1	B
loss	I
,	O
which	O
is	O
known	O
to	O
be	O
computationally	O
hard	O
to	O
learn	O
in	O
the	O
unrealizable	O
case	O
.	O
in	O
general	O
,	O
a	O
convex	B
learning	O
problem	O
is	O
a	O
problem	O
whose	O
hypothesis	B
class	I
is	O
a	O
convex	B
set	O
,	O
and	O
whose	O
loss	B
function	I
is	O
a	O
convex	B
function	O
for	O
each	O
example	O
.	O
we	O
be-	O
gin	O
the	O
chapter	O
with	O
some	O
required	O
deﬁnitions	O
of	O
convexity	O
.	O
besides	O
convexity	O
,	O
we	O
will	O
deﬁne	O
lipschitzness	O
and	O
smoothness	B
,	O
which	O
are	O
additional	O
properties	O
of	O
the	O
loss	B
function	I
that	O
facilitate	O
successful	O
learning	O
.	O
we	O
next	O
turn	O
to	O
deﬁning	O
convex	B
learning	O
problems	O
and	O
demonstrate	O
the	O
necessity	O
for	O
further	O
constraints	O
such	O
as	O
boundedness	B
and	O
lipschitzness	O
or	O
smoothness	B
.	O
we	O
deﬁne	O
these	O
more	O
restricted	O
families	O
of	O
learning	O
problems	O
and	O
claim	O
that	O
convex-smooth/lipschitz-bounded	O
problems	O
are	O
learnable	O
.	O
these	O
claims	O
will	O
be	O
proven	O
in	O
the	O
next	O
two	O
chapters	O
,	O
in	O
which	O
we	O
will	O
present	O
two	O
learning	O
paradigms	O
that	O
successfully	O
learn	O
all	O
problems	O
that	O
are	O
either	O
convex-lipschitz-bounded	O
or	O
convex-smooth-bounded	O
.	O
finally	O
,	O
in	O
section	O
12.3	O
,	O
we	O
show	O
how	O
one	O
can	O
handle	O
some	O
nonconvex	O
problems	O
by	O
minimizing	O
“	O
surrogate	O
”	O
loss	B
functions	O
that	O
are	O
convex	B
(	O
instead	O
of	O
the	O
original	O
nonconvex	O
loss	B
function	I
)	O
.	O
surrogate	O
convex	O
loss	B
functions	O
give	O
rise	O
to	O
eﬃcient	O
solutions	O
but	O
might	O
increase	O
the	O
risk	B
of	O
the	O
learned	O
predictor	B
.	O
12.1	O
convexity	O
,	O
lipschitzness	O
,	O
and	O
smoothness	B
12.1.1	O
convexity	O
definition	O
12.1	O
(	O
convex	B
set	O
)	O
a	O
set	B
c	O
in	O
a	O
vector	O
space	O
is	O
convex	B
if	O
for	O
any	O
two	O
vectors	O
u	O
,	O
v	O
in	O
c	O
,	O
the	O
line	O
segment	O
between	O
u	O
and	O
v	O
is	O
contained	O
in	O
c.	O
that	O
is	O
,	O
for	O
any	O
α	O
∈	O
[	O
0	O
,	O
1	O
]	O
we	O
have	O
that	O
αu	O
+	O
(	O
1	O
−	O
α	O
)	O
v	O
∈	O
c.	O
examples	O
of	O
convex	B
and	O
nonconvex	O
sets	O
in	O
r2	O
are	O
given	O
in	O
the	O
following	O
.	O
for	O
the	O
nonconvex	O
sets	O
,	O
we	O
depict	O
two	O
points	O
in	O
the	O
set	B
such	O
that	O
the	O
line	O
between	O
the	O
two	O
points	O
is	O
not	O
contained	O
in	O
the	O
set	B
.	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
12.1	O
convexity	O
,	O
lipschitzness	O
,	O
and	O
smoothness	B
157	O
non-convex	O
convex	B
given	O
α	O
∈	O
[	O
0	O
,	O
1	O
]	O
,	O
the	O
combination	O
,	O
αu	O
+	O
(	O
1	O
−	O
α	O
)	O
v	O
of	O
the	O
points	O
u	O
,	O
v	O
is	O
called	O
a	O
convex	B
combination	O
.	O
definition	O
12.2	O
(	O
convex	B
function	O
)	O
let	O
c	O
be	O
a	O
convex	B
set	O
.	O
a	O
function	B
f	O
:	O
c	O
→	O
r	O
is	O
convex	B
if	O
for	O
every	O
u	O
,	O
v	O
∈	O
c	O
and	O
α	O
∈	O
[	O
0	O
,	O
1	O
]	O
,	O
f	O
(	O
αu	O
+	O
(	O
1	O
−	O
α	O
)	O
v	O
)	O
≤	O
αf	O
(	O
u	O
)	O
+	O
(	O
1	O
−	O
α	O
)	O
f	O
(	O
v	O
)	O
.	O
in	O
words	O
,	O
f	O
is	O
convex	B
if	O
for	O
any	O
u	O
,	O
v	O
,	O
the	O
graph	O
of	O
f	O
between	O
u	O
and	O
v	O
lies	O
below	O
the	O
line	O
segment	O
joining	O
f	O
(	O
u	O
)	O
and	O
f	O
(	O
v	O
)	O
.	O
an	O
illustration	O
of	O
a	O
convex	B
function	O
,	O
f	O
:	O
r	O
→	O
r	O
,	O
is	O
depicted	O
in	O
the	O
following	O
.	O
f	O
(	O
v	O
)	O
αf	O
(	O
u	O
)	O
+	O
(	O
1	O
−	O
α	O
)	O
f	O
(	O
v	O
)	O
f	O
(	O
αu	O
+	O
(	O
1	O
−	O
α	O
)	O
v	O
)	O
f	O
(	O
u	O
)	O
u	O
αu	O
+	O
(	O
1	O
−	O
α	O
)	O
v	O
v	O
the	O
epigraph	B
of	O
a	O
function	B
f	O
is	O
the	O
set	B
epigraph	O
(	O
f	O
)	O
=	O
{	O
(	O
x	O
,	O
β	O
)	O
:	O
f	O
(	O
x	O
)	O
≤	O
β	O
}	O
.	O
(	O
12.1	O
)	O
it	O
is	O
easy	O
to	O
verify	O
that	O
a	O
function	B
f	O
is	O
convex	B
if	O
and	O
only	O
if	O
its	O
epigraph	B
is	O
a	O
convex	B
set	O
.	O
an	O
illustration	O
of	O
a	O
nonconvex	O
function	B
f	O
:	O
r	O
→	O
r	O
,	O
along	O
with	O
its	O
epigraph	B
,	O
is	O
given	O
in	O
the	O
following	O
.	O
158	O
convex	B
learning	O
problems	O
f	O
(	O
x	O
)	O
x	O
an	O
important	O
property	O
of	O
convex	B
functions	O
is	O
that	O
every	O
local	B
minimum	I
of	O
the	O
function	B
is	O
also	O
a	O
global	O
minimum	O
.	O
formally	O
,	O
let	O
b	O
(	O
u	O
,	O
r	O
)	O
=	O
{	O
v	O
:	O
(	O
cid:107	O
)	O
v	O
−	O
u	O
(	O
cid:107	O
)	O
≤	O
r	O
}	O
be	O
a	O
ball	O
of	O
radius	O
r	O
centered	O
around	O
u.	O
we	O
say	O
that	O
f	O
(	O
u	O
)	O
is	O
a	O
local	B
minimum	I
of	O
f	O
at	O
u	O
if	O
there	O
exists	O
some	O
r	O
>	O
0	O
such	O
that	O
for	O
all	O
v	O
∈	O
b	O
(	O
u	O
,	O
r	O
)	O
we	O
have	O
f	O
(	O
v	O
)	O
≥	O
f	O
(	O
u	O
)	O
.	O
it	O
follows	O
that	O
for	O
any	O
v	O
(	O
not	O
necessarily	O
in	O
b	O
)	O
,	O
there	O
is	O
a	O
small	O
enough	O
α	O
>	O
0	O
such	O
that	O
u	O
+	O
α	O
(	O
v	O
−	O
u	O
)	O
∈	O
b	O
(	O
u	O
,	O
r	O
)	O
and	O
therefore	O
f	O
(	O
u	O
)	O
≤	O
f	O
(	O
u	O
+	O
α	O
(	O
v	O
−	O
u	O
)	O
)	O
.	O
(	O
12.2	O
)	O
if	O
f	O
is	O
convex	B
,	O
we	O
also	O
have	O
that	O
f	O
(	O
u	O
+	O
α	O
(	O
v	O
−	O
u	O
)	O
)	O
=	O
f	O
(	O
αv	O
+	O
(	O
1	O
−	O
α	O
)	O
u	O
)	O
≤	O
(	O
1	O
−	O
α	O
)	O
f	O
(	O
u	O
)	O
+	O
αf	O
(	O
v	O
)	O
.	O
(	O
12.3	O
)	O
combining	O
these	O
two	O
equations	O
and	O
rearranging	O
terms	O
,	O
we	O
conclude	O
that	O
f	O
(	O
u	O
)	O
≤	O
f	O
(	O
v	O
)	O
.	O
since	O
this	O
holds	O
for	O
every	O
v	O
,	O
it	O
follows	O
that	O
f	O
(	O
u	O
)	O
is	O
also	O
a	O
global	O
minimum	O
of	O
f	O
.	O
another	O
important	O
property	O
of	O
convex	B
functions	O
is	O
that	O
for	O
every	O
w	O
we	O
can	O
construct	O
a	O
tangent	O
to	O
f	O
at	O
w	O
that	O
lies	O
below	O
f	O
everywhere	O
.	O
if	O
f	O
is	O
diﬀerentiable	O
,	O
this	O
tangent	O
is	O
the	O
linear	O
function	O
l	O
(	O
u	O
)	O
=	O
f	O
(	O
w	O
)	O
+	O
(	O
cid:104	O
)	O
∇f	O
(	O
w	O
)	O
,	O
u	O
−	O
w	O
(	O
cid:105	O
)	O
,	O
where	O
∇f	O
(	O
w	O
)	O
is	O
the	O
gradient	B
of	O
f	O
at	O
w	O
,	O
namely	O
,	O
the	O
vector	O
of	O
partial	O
derivatives	O
of	O
f	O
,	O
∇f	O
(	O
w	O
)	O
=	O
.	O
that	O
is	O
,	O
for	O
convex	B
diﬀerentiable	O
functions	O
,	O
∀u	O
,	O
f	O
(	O
u	O
)	O
≥	O
f	O
(	O
w	O
)	O
+	O
(	O
cid:104	O
)	O
∇f	O
(	O
w	O
)	O
,	O
u	O
−	O
w	O
(	O
cid:105	O
)	O
.	O
(	O
12.4	O
)	O
(	O
cid:16	O
)	O
∂f	O
(	O
w	O
)	O
∂w1	O
(	O
cid:17	O
)	O
,	O
.	O
.	O
.	O
,	O
∂f	O
(	O
w	O
)	O
∂wd	O
in	O
chapter	O
14	O
we	O
will	O
generalize	O
this	O
inequality	O
to	O
nondiﬀerentiable	O
functions	O
.	O
an	O
illustration	O
of	O
equation	O
(	O
12.4	O
)	O
is	O
given	O
in	O
the	O
following	O
.	O
12.1	O
convexity	O
,	O
lipschitzness	O
,	O
and	O
smoothness	B
159	O
−	O
w	O
,	O
∇	O
f	O
(	O
w	O
)	O
(	O
cid:105	O
)	O
f	O
(	O
u	O
)	O
(	O
cid:104	O
)	O
u	O
f	O
(	O
w	O
)	O
+	O
f	O
(	O
w	O
)	O
w	O
u	O
if	O
f	O
is	O
a	O
scalar	O
diﬀerentiable	O
function	B
,	O
there	O
is	O
an	O
easy	O
way	O
to	O
check	O
if	O
it	O
is	O
convex	B
.	O
lemma	O
12.3	O
let	O
f	O
:	O
r	O
→	O
r	O
be	O
a	O
scalar	O
twice	O
diﬀerential	O
function	O
,	O
and	O
let	O
f	O
(	O
cid:48	O
)	O
,	O
f	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
be	O
its	O
ﬁrst	O
and	O
second	O
derivatives	O
,	O
respectively	O
.	O
then	O
,	O
the	O
following	O
are	O
equivalent	O
:	O
1.	O
f	O
is	O
convex	B
2.	O
f	O
(	O
cid:48	O
)	O
is	O
monotonically	O
nondecreasing	O
3.	O
f	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
is	O
nonnegative	O
example	O
12.1	O
•	O
the	O
scalar	O
function	B
f	O
(	O
x	O
)	O
=	O
x2	O
is	O
convex	B
.	O
to	O
see	O
this	O
,	O
note	O
that	O
f	O
(	O
cid:48	O
)	O
(	O
x	O
)	O
=	O
2x	O
and	O
f	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
(	O
x	O
)	O
=	O
2	O
>	O
0	O
.	O
•	O
the	O
scalar	O
function	B
f	O
(	O
x	O
)	O
=	O
log	O
(	O
1	O
+	O
exp	O
(	O
x	O
)	O
)	O
is	O
convex	B
.	O
to	O
see	O
this	O
,	O
observe	O
that	O
exp	O
(	O
−x	O
)	O
+1	O
.	O
this	O
is	O
a	O
monotonically	O
increasing	O
function	B
f	O
(	O
cid:48	O
)	O
(	O
x	O
)	O
=	O
exp	O
(	O
x	O
)	O
since	O
the	O
exponent	O
function	B
is	O
a	O
monotonically	O
increasing	O
function	B
.	O
1+exp	O
(	O
x	O
)	O
=	O
1	O
the	O
following	O
claim	O
shows	O
that	O
the	O
composition	O
of	O
a	O
convex	B
scalar	O
function	B
with	O
a	O
linear	O
function	O
yields	O
a	O
convex	B
vector-valued	O
function	B
.	O
claim	O
12.4	O
assume	O
that	O
f	O
:	O
rd	O
→	O
r	O
can	O
be	O
written	O
as	O
f	O
(	O
w	O
)	O
=	O
g	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
y	O
)	O
,	O
for	O
some	O
x	O
∈	O
rd	O
,	O
y	O
∈	O
r	O
,	O
and	O
g	O
:	O
r	O
→	O
r.	O
then	O
,	O
convexity	O
of	O
g	O
implies	O
the	O
convexity	O
of	O
f	O
.	O
proof	O
let	O
w1	O
,	O
w2	O
∈	O
rd	O
and	O
α	O
∈	O
[	O
0	O
,	O
1	O
]	O
.	O
we	O
have	O
f	O
(	O
αw1	O
+	O
(	O
1	O
−	O
α	O
)	O
w2	O
)	O
=	O
g	O
(	O
(	O
cid:104	O
)	O
αw1	O
+	O
(	O
1	O
−	O
α	O
)	O
w2	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
y	O
)	O
=	O
g	O
(	O
α	O
(	O
cid:104	O
)	O
w1	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
(	O
1	O
−	O
α	O
)	O
(	O
cid:104	O
)	O
w2	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
y	O
)	O
=	O
g	O
(	O
α	O
(	O
(	O
cid:104	O
)	O
w1	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
y	O
)	O
+	O
(	O
1	O
−	O
α	O
)	O
(	O
(	O
cid:104	O
)	O
w2	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
y	O
)	O
)	O
≤	O
αg	O
(	O
(	O
cid:104	O
)	O
w1	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
y	O
)	O
+	O
(	O
1	O
−	O
α	O
)	O
g	O
(	O
(	O
cid:104	O
)	O
w2	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
y	O
)	O
,	O
where	O
the	O
last	O
inequality	O
follows	O
from	O
the	O
convexity	O
of	O
g.	O
example	O
12.2	O
160	O
convex	B
learning	O
problems	O
•	O
given	O
some	O
x	O
∈	O
rd	O
and	O
y	O
∈	O
r	O
,	O
let	O
f	O
:	O
rd	O
→	O
r	O
be	O
deﬁned	O
as	O
f	O
(	O
w	O
)	O
=	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
−	O
y	O
)	O
2.	O
then	O
,	O
f	O
is	O
a	O
composition	O
of	O
the	O
function	B
g	O
(	O
a	O
)	O
=	O
a2	O
onto	O
a	O
linear	O
function	O
,	O
and	O
hence	O
f	O
is	O
a	O
convex	B
function	O
.	O
•	O
given	O
some	O
x	O
∈	O
rd	O
and	O
y	O
∈	O
{	O
±1	O
}	O
,	O
let	O
f	O
:	O
rd	O
→	O
r	O
be	O
deﬁned	O
as	O
f	O
(	O
w	O
)	O
=	O
log	O
(	O
1	O
+	O
exp	O
(	O
−y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
)	O
.	O
then	O
,	O
f	O
is	O
a	O
composition	O
of	O
the	O
function	B
g	O
(	O
a	O
)	O
=	O
log	O
(	O
1	O
+	O
exp	O
(	O
a	O
)	O
)	O
onto	O
a	O
linear	O
function	O
,	O
and	O
hence	O
f	O
is	O
a	O
convex	B
function	O
.	O
finally	O
,	O
the	O
following	O
lemma	O
shows	O
that	O
the	O
maximum	O
of	O
convex	B
functions	O
is	O
convex	B
and	O
that	O
a	O
weighted	O
sum	O
of	O
convex	B
functions	O
,	O
with	O
nonnegative	O
weights	O
,	O
is	O
also	O
convex	B
.	O
claim	O
12.5	O
for	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
r	O
,	O
let	O
fi	O
following	O
functions	O
from	O
rd	O
to	O
r	O
are	O
also	O
convex	B
.	O
•	O
g	O
(	O
x	O
)	O
=	O
maxi∈	O
[	O
r	O
]	O
fi	O
(	O
x	O
)	O
i=1	O
wifi	O
(	O
x	O
)	O
,	O
where	O
for	O
all	O
i	O
,	O
wi	O
≥	O
0	O
.	O
•	O
g	O
(	O
x	O
)	O
=	O
(	O
cid:80	O
)	O
r	O
:	O
rd	O
→	O
r	O
be	O
a	O
convex	B
function	O
.	O
the	O
proof	O
the	O
ﬁrst	O
claim	O
follows	O
by	O
i	O
g	O
(	O
αu	O
+	O
(	O
1	O
−	O
α	O
)	O
v	O
)	O
=	O
max	O
≤	O
max	O
≤	O
α	O
max	O
=	O
αg	O
(	O
u	O
)	O
+	O
(	O
1	O
−	O
α	O
)	O
g	O
(	O
v	O
)	O
.	O
fi	O
(	O
αu	O
+	O
(	O
1	O
−	O
α	O
)	O
v	O
)	O
[	O
αfi	O
(	O
u	O
)	O
+	O
(	O
1	O
−	O
α	O
)	O
fi	O
(	O
v	O
)	O
]	O
fi	O
(	O
u	O
)	O
+	O
(	O
1	O
−	O
α	O
)	O
max	O
i	O
i	O
i	O
fi	O
(	O
v	O
)	O
wifi	O
(	O
αu	O
+	O
(	O
1	O
−	O
α	O
)	O
v	O
)	O
wi	O
[	O
αfi	O
(	O
u	O
)	O
+	O
(	O
1	O
−	O
α	O
)	O
fi	O
(	O
v	O
)	O
]	O
wifi	O
(	O
u	O
)	O
+	O
(	O
1	O
−	O
α	O
)	O
(	O
cid:88	O
)	O
=	O
α	O
=	O
αg	O
(	O
u	O
)	O
+	O
(	O
1	O
−	O
α	O
)	O
g	O
(	O
v	O
)	O
.	O
i	O
wifi	O
(	O
v	O
)	O
i	O
for	O
the	O
second	O
claim	O
g	O
(	O
αu	O
+	O
(	O
1	O
−	O
α	O
)	O
v	O
)	O
=	O
(	O
cid:88	O
)	O
≤	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
i	O
i	O
example	O
12.3	O
the	O
function	B
g	O
(	O
x	O
)	O
=	O
|x|	O
is	O
convex	B
.	O
to	O
see	O
this	O
,	O
note	O
that	O
g	O
(	O
x	O
)	O
=	O
max	O
{	O
x	O
,	O
−x	O
}	O
and	O
that	O
both	O
the	O
function	B
f1	O
(	O
x	O
)	O
=	O
x	O
and	O
f2	O
(	O
x	O
)	O
=	O
−x	O
are	O
convex	B
.	O
12.1.2	O
lipschitzness	O
the	O
deﬁnition	O
of	O
lipschitzness	O
below	O
is	O
with	O
respect	O
to	O
the	O
euclidean	O
norm	O
over	O
rd	O
.	O
however	O
,	O
it	O
is	O
possible	O
to	O
deﬁne	O
lipschitzness	O
with	O
respect	O
to	O
any	O
norm	O
.	O
definition	O
12.6	O
(	O
lipschitzness	O
)	O
let	O
c	O
⊂	O
rd	O
.	O
a	O
function	B
f	O
:	O
rd	O
→	O
rk	O
is	O
ρ-lipschitz	O
over	O
c	O
if	O
for	O
every	O
w1	O
,	O
w2	O
∈	O
c	O
we	O
have	O
that	O
(	O
cid:107	O
)	O
f	O
(	O
w1	O
)	O
−	O
f	O
(	O
w2	O
)	O
(	O
cid:107	O
)	O
≤	O
ρ	O
(	O
cid:107	O
)	O
w1	O
−	O
w2	O
(	O
cid:107	O
)	O
.	O
12.1	O
convexity	O
,	O
lipschitzness	O
,	O
and	O
smoothness	B
161	O
intuitively	O
,	O
a	O
lipschitz	O
function	B
can	O
not	O
change	O
too	O
fast	O
.	O
note	O
that	O
if	O
f	O
:	O
r	O
→	O
r	O
is	O
diﬀerentiable	O
,	O
then	O
by	O
the	O
mean	O
value	O
theorem	O
we	O
have	O
f	O
(	O
w1	O
)	O
−	O
f	O
(	O
w2	O
)	O
=	O
f	O
(	O
cid:48	O
)	O
(	O
u	O
)	O
(	O
w1	O
−	O
w2	O
)	O
,	O
where	O
u	O
is	O
some	O
point	O
between	O
w1	O
and	O
w2	O
.	O
it	O
follows	O
that	O
if	O
the	O
derivative	O
of	O
f	O
is	O
everywhere	O
bounded	O
(	O
in	O
absolute	O
value	O
)	O
by	O
ρ	O
,	O
then	O
the	O
function	B
is	O
ρ-lipschitz	O
.	O
example	O
12.4	O
•	O
the	O
function	B
f	O
(	O
x	O
)	O
=	O
|x|	O
is	O
1-lipschitz	O
over	O
r.	O
this	O
follows	O
from	O
the	O
triangle	O
inequality	O
:	O
for	O
every	O
x1	O
,	O
x2	O
,	O
|x1|	O
−	O
|x2|	O
=	O
|x1	O
−	O
x2	O
+	O
x2|	O
−	O
|x2|	O
≤	O
|x1	O
−	O
x2|	O
+	O
|x2|	O
−	O
|x2|	O
=	O
|x1	O
−	O
x2|	O
.	O
since	O
this	O
holds	O
for	O
both	O
x1	O
,	O
x2	O
and	O
x2	O
,	O
x1	O
,	O
we	O
obtain	O
that	O
||x1|	O
−	O
|x2||	O
≤	O
|x1	O
−	O
x2|	O
.	O
•	O
the	O
function	B
f	O
(	O
x	O
)	O
=	O
log	O
(	O
1	O
+	O
exp	O
(	O
x	O
)	O
)	O
is	O
1-lipschitz	O
over	O
r.	O
to	O
see	O
this	O
,	O
observe	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
=	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
that	O
|f	O
(	O
cid:48	O
)	O
(	O
x	O
)	O
|	O
=	O
exp	O
(	O
x	O
)	O
1	O
+	O
exp	O
(	O
x	O
)	O
1	O
exp	O
(	O
−x	O
)	O
+	O
1	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
≤	O
1	O
.	O
•	O
the	O
function	B
f	O
(	O
x	O
)	O
=	O
x2	O
is	O
not	O
ρ-lipschitz	O
over	O
r	O
for	O
any	O
ρ.	O
to	O
see	O
this	O
,	O
take	O
x1	O
=	O
0	O
and	O
x2	O
=	O
1	O
+	O
ρ	O
,	O
then	O
f	O
(	O
x2	O
)	O
−	O
f	O
(	O
x1	O
)	O
=	O
(	O
1	O
+	O
ρ	O
)	O
2	O
>	O
ρ	O
(	O
1	O
+	O
ρ	O
)	O
=	O
ρ|x2	O
−	O
x1|	O
.	O
however	O
,	O
this	O
function	B
is	O
ρ-lipschitz	O
over	O
the	O
set	B
c	O
=	O
{	O
x	O
:	O
|x|	O
≤	O
ρ/2	O
}	O
.	O
indeed	O
,	O
for	O
any	O
x1	O
,	O
x2	O
∈	O
c	O
we	O
have	O
|x2	O
1	O
−	O
x2	O
2|	O
=	O
|x1	O
+	O
x2|	O
|x1	O
−	O
x2|	O
≤	O
2	O
(	O
ρ/2	O
)	O
|x1	O
−	O
x2|	O
=	O
ρ|x1	O
−	O
x2|	O
.	O
•	O
the	O
linear	O
function	O
f	O
:	O
rd	O
→	O
r	O
deﬁned	O
by	O
f	O
(	O
w	O
)	O
=	O
(	O
cid:104	O
)	O
v	O
,	O
w	O
(	O
cid:105	O
)	O
+	O
b	O
where	O
v	O
∈	O
rd	O
is	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
-lipschitz	O
.	O
indeed	O
,	O
using	O
cauchy-schwartz	O
inequality	O
,	O
|f	O
(	O
w1	O
)	O
−	O
f	O
(	O
w2	O
)	O
|	O
=	O
|	O
(	O
cid:104	O
)	O
v	O
,	O
w1	O
−	O
w2	O
(	O
cid:105	O
)	O
|	O
≤	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
(	O
cid:107	O
)	O
w1	O
−	O
w2	O
(	O
cid:107	O
)	O
.	O
the	O
following	O
claim	O
shows	O
that	O
composition	O
of	O
lipschitz	O
functions	O
preserves	O
lipschitzness	O
.	O
claim	O
12.7	O
let	O
f	O
(	O
x	O
)	O
=	O
g1	O
(	O
g2	O
(	O
x	O
)	O
)	O
,	O
where	O
g1	O
is	O
ρ1-lipschitz	O
and	O
g2	O
is	O
ρ2-	O
lipschitz	O
.	O
then	O
,	O
f	O
is	O
(	O
ρ1ρ2	O
)	O
-lipschitz	O
.	O
in	O
particular	O
,	O
if	O
g2	O
is	O
the	O
linear	O
function	O
,	O
g2	O
(	O
x	O
)	O
=	O
(	O
cid:104	O
)	O
v	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b	O
,	O
for	O
some	O
v	O
∈	O
rd	O
,	O
b	O
∈	O
r	O
,	O
then	O
f	O
is	O
(	O
ρ1	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
)	O
-lipschitz	O
.	O
proof	O
|f	O
(	O
w1	O
)	O
−	O
f	O
(	O
w2	O
)	O
|	O
=	O
|g1	O
(	O
g2	O
(	O
w1	O
)	O
)	O
−	O
g1	O
(	O
g2	O
(	O
w2	O
)	O
)	O
|	O
≤	O
ρ1	O
(	O
cid:107	O
)	O
g2	O
(	O
w1	O
)	O
−	O
g2	O
(	O
w2	O
)	O
(	O
cid:107	O
)	O
≤	O
ρ1	O
ρ2	O
(	O
cid:107	O
)	O
w1	O
−	O
w2	O
(	O
cid:107	O
)	O
.	O
162	O
convex	B
learning	O
problems	O
12.1.3	O
smoothness	B
the	O
deﬁnition	O
of	O
a	O
smooth	O
function	O
relies	O
on	O
the	O
notion	O
of	O
gradient	B
.	O
recall	B
that	O
the	O
gradient	B
of	O
a	O
diﬀerentiable	O
function	B
f	O
:	O
rd	O
→	O
r	O
at	O
w	O
,	O
denoted	O
∇f	O
(	O
w	O
)	O
,	O
is	O
the	O
vector	O
of	O
partial	O
derivatives	O
of	O
f	O
,	O
namely	O
,	O
∇f	O
(	O
w	O
)	O
=	O
definition	O
12.8	O
(	O
smoothness	B
)	O
a	O
diﬀerentiable	O
function	B
f	O
:	O
rd	O
→	O
r	O
is	O
β-	O
smooth	O
if	O
its	O
gradient	B
is	O
β-lipschitz	O
;	O
namely	O
,	O
for	O
all	O
v	O
,	O
w	O
we	O
have	O
(	O
cid:107	O
)	O
∇f	O
(	O
v	O
)	O
−	O
∇f	O
(	O
w	O
)	O
(	O
cid:107	O
)	O
≤	O
β	O
(	O
cid:107	O
)	O
v	O
−	O
w	O
(	O
cid:107	O
)	O
.	O
,	O
.	O
.	O
.	O
,	O
∂f	O
(	O
w	O
)	O
∂wd	O
∂w1	O
.	O
(	O
cid:16	O
)	O
∂f	O
(	O
w	O
)	O
(	O
cid:17	O
)	O
it	O
is	O
possible	O
to	O
show	O
that	O
smoothness	B
implies	O
that	O
for	O
all	O
v	O
,	O
w	O
we	O
have	O
f	O
(	O
v	O
)	O
≤	O
f	O
(	O
w	O
)	O
+	O
(	O
cid:104	O
)	O
∇f	O
(	O
w	O
)	O
,	O
v	O
−	O
w	O
(	O
cid:105	O
)	O
+	O
(	O
12.5	O
)	O
recall	B
that	O
convexity	O
of	O
f	O
implies	O
that	O
f	O
(	O
v	O
)	O
≥	O
f	O
(	O
w	O
)	O
+	O
(	O
cid:104	O
)	O
∇f	O
(	O
w	O
)	O
,	O
v−w	O
(	O
cid:105	O
)	O
.	O
therefore	O
,	O
when	O
a	O
function	B
is	O
both	O
convex	B
and	O
smooth	O
,	O
we	O
have	O
both	O
upper	O
and	O
lower	O
bounds	O
on	O
the	O
diﬀerence	O
between	O
the	O
function	B
and	O
its	O
ﬁrst	O
order	O
approximation	O
.	O
(	O
cid:107	O
)	O
v	O
−	O
w	O
(	O
cid:107	O
)	O
2	O
.	O
β	O
2	O
setting	O
v	O
=	O
w	O
−	O
1	O
β∇f	O
(	O
w	O
)	O
in	O
the	O
right-hand	O
side	O
of	O
equation	O
(	O
12.5	O
)	O
and	O
rear-	O
ranging	O
terms	O
,	O
we	O
obtain	O
(	O
cid:107	O
)	O
∇f	O
(	O
w	O
)	O
(	O
cid:107	O
)	O
2	O
≤	O
f	O
(	O
w	O
)	O
−	O
f	O
(	O
v	O
)	O
.	O
1	O
2β	O
if	O
we	O
further	O
assume	O
that	O
f	O
(	O
v	O
)	O
≥	O
0	O
for	O
all	O
v	O
we	O
conclude	O
that	O
smoothness	B
implies	O
the	O
following	O
:	O
(	O
cid:107	O
)	O
∇f	O
(	O
w	O
)	O
(	O
cid:107	O
)	O
2	O
≤	O
2βf	O
(	O
w	O
)	O
.	O
(	O
12.6	O
)	O
a	O
function	B
that	O
satisﬁes	O
this	O
property	O
is	O
also	O
called	O
a	O
self-bounded	O
function	B
.	O
example	O
12.5	O
•	O
the	O
function	B
f	O
(	O
x	O
)	O
=	O
x2	O
is	O
2-smooth	O
.	O
this	O
follows	O
directly	O
from	O
the	O
fact	O
that	O
f	O
(	O
cid:48	O
)	O
(	O
x	O
)	O
=	O
2x	O
.	O
note	O
that	O
for	O
this	O
particular	O
function	B
equation	O
(	O
12.5	O
)	O
and	O
equation	O
(	O
12.6	O
)	O
hold	O
with	O
equality	O
.	O
•	O
the	O
function	B
f	O
(	O
x	O
)	O
=	O
log	O
(	O
1	O
+	O
exp	O
(	O
x	O
)	O
)	O
is	O
(	O
1/4	O
)	O
-smooth	O
.	O
indeed	O
,	O
since	O
f	O
(	O
cid:48	O
)	O
(	O
x	O
)	O
=	O
1	O
1+exp	O
(	O
−x	O
)	O
we	O
have	O
that	O
|f	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
(	O
x	O
)	O
|	O
=	O
exp	O
(	O
−x	O
)	O
(	O
1	O
+	O
exp	O
(	O
−x	O
)	O
)	O
2	O
=	O
1	O
(	O
1	O
+	O
exp	O
(	O
−x	O
)	O
)	O
(	O
1	O
+	O
exp	O
(	O
x	O
)	O
)	O
≤	O
1/4	O
.	O
hence	O
,	O
f	O
(	O
cid:48	O
)	O
tion	O
(	O
12.6	O
)	O
holds	O
as	O
well	O
.	O
is	O
(	O
1/4	O
)	O
-lipschitz	O
.	O
since	O
this	O
function	B
is	O
nonnegative	O
,	O
equa-	O
the	O
following	O
claim	O
shows	O
that	O
a	O
composition	O
of	O
a	O
smooth	O
scalar	O
function	B
over	O
a	O
linear	O
function	O
preserves	O
smoothness	B
.	O
claim	O
12.9	O
let	O
f	O
(	O
w	O
)	O
=	O
g	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
,	O
where	O
g	O
:	O
r	O
→	O
r	O
is	O
a	O
β-smooth	O
function	B
,	O
x	O
∈	O
rd	O
,	O
and	O
b	O
∈	O
r.	O
then	O
,	O
f	O
is	O
(	O
β	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
)	O
-smooth	O
.	O
12.2	O
convex	B
learning	O
problems	O
163	O
proof	O
by	O
the	O
chain	O
rule	O
we	O
have	O
that	O
∇f	O
(	O
w	O
)	O
=	O
g	O
(	O
cid:48	O
)	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
x	O
,	O
where	O
g	O
(	O
cid:48	O
)	O
is	O
the	O
derivative	O
of	O
g.	O
using	O
the	O
smoothness	B
of	O
g	O
and	O
the	O
cauchy-schwartz	O
inequality	O
we	O
therefore	O
obtain	O
f	O
(	O
v	O
)	O
=	O
g	O
(	O
(	O
cid:104	O
)	O
v	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
≤	O
g	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
+	O
g	O
(	O
cid:48	O
)	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
(	O
cid:104	O
)	O
v	O
−	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
β	O
2	O
≤	O
g	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
+	O
g	O
(	O
cid:48	O
)	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
(	O
cid:104	O
)	O
v	O
−	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
β	O
2	O
=	O
f	O
(	O
w	O
)	O
+	O
(	O
cid:104	O
)	O
∇f	O
(	O
w	O
)	O
,	O
v	O
−	O
w	O
(	O
cid:105	O
)	O
+	O
(	O
cid:107	O
)	O
v	O
−	O
w	O
(	O
cid:107	O
)	O
2.	O
β	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
2	O
(	O
(	O
cid:104	O
)	O
v	O
−	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
2	O
(	O
(	O
cid:107	O
)	O
v	O
−	O
w	O
(	O
cid:107	O
)	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
)	O
2	O
example	O
12.6	O
•	O
for	O
any	O
x	O
∈	O
rd	O
and	O
y	O
∈	O
r	O
,	O
let	O
f	O
(	O
w	O
)	O
=	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
−	O
y	O
)	O
2.	O
then	O
,	O
f	O
is	O
(	O
2	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
)	O
-	O
•	O
for	O
any	O
x	O
∈	O
rd	O
and	O
y	O
∈	O
{	O
±1	O
}	O
,	O
let	O
f	O
(	O
w	O
)	O
=	O
log	O
(	O
1	O
+	O
exp	O
(	O
−y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
)	O
.	O
then	O
,	O
f	O
is	O
smooth	O
.	O
(	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2/4	O
)	O
-smooth	O
.	O
12.2	O
convex	B
learning	O
problems	O
recall	B
that	O
in	O
our	O
general	O
deﬁnition	O
of	O
learning	O
(	O
deﬁnition	O
3.4	O
in	O
chapter	O
3	O
)	O
,	O
we	O
have	O
a	O
hypothesis	B
class	I
h	O
,	O
a	O
set	B
of	O
examples	O
z	O
,	O
and	O
a	O
loss	B
function	I
(	O
cid:96	O
)	O
:	O
h	O
×	O
z	O
→	O
r+	O
.	O
so	O
far	O
in	O
the	O
book	O
we	O
have	O
mainly	O
thought	O
of	O
z	O
as	O
being	O
the	O
product	O
of	O
an	O
instance	B
space	I
and	O
a	O
target	O
space	O
,	O
z	O
=	O
x	O
×y	O
,	O
and	O
h	O
being	O
a	O
set	B
of	O
functions	O
from	O
x	O
to	O
y.	O
however	O
,	O
h	O
can	O
be	O
an	O
arbitrary	O
set	B
.	O
indeed	O
,	O
throughout	O
this	O
chapter	O
,	O
we	O
consider	O
hypothesis	B
classes	O
h	O
that	O
are	O
subsets	O
of	O
the	O
euclidean	O
space	O
rd	O
.	O
that	O
is	O
,	O
every	O
hypothesis	B
is	O
some	O
real-valued	O
vector	O
.	O
we	O
shall	O
,	O
therefore	O
,	O
denote	O
a	O
hypothesis	B
in	O
h	O
by	O
w.	O
now	O
we	O
can	O
ﬁnally	O
deﬁne	O
convex	B
learning	O
problems	O
:	O
definition	O
12.10	O
(	O
convex	B
learning	O
problem	O
)	O
a	O
learning	O
problem	O
,	O
(	O
h	O
,	O
z	O
,	O
(	O
cid:96	O
)	O
)	O
,	O
is	O
called	O
convex	B
if	O
the	O
hypothesis	B
class	I
h	O
is	O
a	O
convex	B
set	O
and	O
for	O
all	O
z	O
∈	O
z	O
,	O
the	O
loss	B
function	I
,	O
(	O
cid:96	O
)	O
(	O
·	O
,	O
z	O
)	O
,	O
is	O
a	O
convex	B
function	O
(	O
where	O
,	O
for	O
any	O
z	O
,	O
(	O
cid:96	O
)	O
(	O
·	O
,	O
z	O
)	O
denotes	O
the	O
function	B
f	O
:	O
h	O
→	O
r	O
deﬁned	O
by	O
f	O
(	O
w	O
)	O
=	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
z	O
)	O
)	O
.	O
example	O
12.7	O
(	O
linear	B
regression	I
with	O
the	O
squared	O
loss	B
)	O
recall	B
that	O
linear	B
regression	I
is	O
a	O
tool	O
for	O
modeling	O
the	O
relationship	O
between	O
some	O
“	O
explanatory	O
”	O
variables	O
and	O
some	O
real	O
valued	O
outcome	O
(	O
see	O
chapter	O
9	O
)	O
.	O
the	O
domain	B
set	O
x	O
is	O
a	O
subset	O
of	O
rd	O
,	O
for	O
some	O
d	O
,	O
and	O
the	O
label	B
set	O
y	O
is	O
the	O
set	B
of	O
real	O
numbers	O
.	O
we	O
would	O
like	O
to	O
learn	O
a	O
linear	O
function	O
h	O
:	O
rd	O
→	O
r	O
that	O
best	O
approximates	O
the	O
relationship	O
between	O
our	O
variables	O
.	O
in	O
chapter	O
9	O
we	O
deﬁned	O
the	O
hypothesis	B
class	I
as	O
the	O
set	B
of	O
homogenous	B
linear	O
functions	O
,	O
h	O
=	O
{	O
x	O
(	O
cid:55	O
)	O
→	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
:	O
w	O
∈	O
rd	O
}	O
,	O
and	O
used	O
the	O
squared	O
loss	B
function	I
,	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
(	O
h	O
(	O
x	O
)	O
−	O
y	O
)	O
2.	O
however	O
,	O
we	O
can	O
equivalently	O
model	O
the	O
learning	O
problem	O
as	O
a	O
convex	B
learning	O
problem	O
as	O
follows	O
.	O
164	O
convex	B
learning	O
problems	O
each	O
linear	O
function	O
is	O
parameterized	O
by	O
a	O
vector	O
w	O
∈	O
rd	O
.	O
hence	O
,	O
we	O
can	O
deﬁne	O
h	O
to	O
be	O
the	O
set	B
of	O
all	O
such	O
parameters	O
,	O
namely	O
,	O
h	O
=	O
rd	O
.	O
the	O
set	B
of	O
examples	O
is	O
z	O
=	O
x	O
×y	O
=	O
rd×r	O
=	O
rd+1	O
,	O
and	O
the	O
loss	B
function	I
is	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
−y	O
)	O
2.	O
clearly	O
,	O
the	O
set	B
h	O
is	O
a	O
convex	B
set	O
.	O
the	O
loss	B
function	I
is	O
also	O
convex	B
with	O
respect	O
to	O
its	O
ﬁrst	O
argument	O
(	O
see	O
example	O
12.2	O
)	O
.	O
if	O
(	O
cid:96	O
)	O
is	O
a	O
convex	B
loss	I
function	O
and	O
the	O
class	O
h	O
is	O
convex	B
,	O
then	O
the	O
lemma	O
12.11	O
ermh	O
problem	O
,	O
of	O
minimizing	O
the	O
empirical	O
loss	O
over	O
h	O
,	O
is	O
a	O
convex	B
optimiza-	O
tion	O
problem	O
(	O
that	O
is	O
,	O
a	O
problem	O
of	O
minimizing	O
a	O
convex	B
function	O
over	O
a	O
convex	B
set	O
)	O
.	O
proof	O
recall	B
that	O
the	O
ermh	O
problem	O
is	O
deﬁned	O
by	O
ermh	O
(	O
s	O
)	O
=	O
argmin	O
w∈h	O
ls	O
(	O
w	O
)	O
.	O
(	O
cid:80	O
)	O
m	O
since	O
,	O
for	O
a	O
sample	O
s	O
=	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
,	O
for	O
every	O
w	O
,	O
ls	O
(	O
w	O
)	O
=	O
1	O
i=1	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
zi	O
)	O
,	O
m	O
claim	O
12.5	O
implies	O
that	O
ls	O
(	O
w	O
)	O
is	O
a	O
convex	B
function	O
.	O
therefore	O
,	O
the	O
erm	O
rule	O
is	O
a	O
problem	O
of	O
minimizing	O
a	O
convex	B
function	O
subject	O
to	O
the	O
constraint	O
that	O
the	O
solution	O
should	O
be	O
in	O
a	O
convex	B
set	O
.	O
under	O
mild	O
conditions	O
,	O
such	O
problems	O
can	O
be	O
solved	O
eﬃciently	O
using	O
generic	O
optimization	O
algorithms	O
.	O
in	O
particular	O
,	O
in	O
chapter	O
14	O
we	O
will	O
present	O
a	O
very	O
simple	O
algorithm	O
for	O
minimizing	O
convex	B
functions	O
.	O
12.2.1	O
learnability	O
of	O
convex	B
learning	O
problems	O
we	O
have	O
argued	O
that	O
for	O
many	O
cases	O
,	O
implementing	O
the	O
erm	O
rule	O
for	O
convex	B
learning	O
problems	O
can	O
be	O
done	O
eﬃciently	O
.	O
but	O
is	O
convexity	O
a	O
suﬃcient	O
condition	O
for	O
the	O
learnability	O
of	O
a	O
problem	O
?	O
to	O
make	O
the	O
quesion	O
more	O
speciﬁc	O
:	O
in	O
vc	O
theory	O
,	O
we	O
saw	O
that	O
halfspaces	O
in	O
d-dimension	O
are	O
learnable	O
(	O
perhaps	O
ineﬃciently	O
)	O
.	O
we	O
also	O
argued	O
in	O
chapter	O
9	O
using	O
the	O
“	O
discretization	B
trick	I
”	O
that	O
if	O
the	O
problem	O
is	O
of	O
d	O
parameters	O
,	O
it	O
is	O
learnable	O
with	O
a	O
sample	B
complexity	I
being	O
a	O
function	B
of	O
d.	O
that	O
is	O
,	O
for	O
a	O
constant	O
d	O
,	O
the	O
problem	O
should	O
be	O
learnable	O
.	O
so	O
,	O
maybe	O
all	O
convex	B
learning	O
problems	O
over	O
rd	O
,	O
are	O
learnable	O
?	O
example	O
12.8	O
later	O
shows	O
that	O
the	O
answer	O
is	O
negative	O
,	O
even	O
when	O
d	O
is	O
low	O
.	O
not	O
all	O
convex	B
learning	O
problems	O
over	O
rd	O
are	O
learnable	O
.	O
there	O
is	O
no	O
contradiction	O
to	O
vc	O
theory	O
since	O
vc	O
theory	O
only	O
deals	O
with	O
binary	O
classiﬁcation	O
while	O
here	O
we	O
consider	O
a	O
wide	O
family	O
of	O
problems	O
.	O
there	O
is	O
also	O
no	O
contradiction	O
to	O
the	O
“	O
discretization	B
trick	I
”	O
as	O
there	O
we	O
assumed	O
that	O
the	O
loss	B
function	I
is	O
bounded	O
and	O
also	O
assumed	O
that	O
a	O
representation	O
of	O
each	O
parameter	O
using	O
a	O
ﬁnite	O
number	O
of	O
bits	O
suﬃces	O
.	O
as	O
we	O
will	O
show	O
later	O
,	O
under	O
some	O
additional	O
restricting	O
conditions	O
that	O
hold	O
in	O
many	O
practical	O
scenarios	O
,	O
convex	B
problems	O
are	O
learnable	O
.	O
example	O
12.8	O
(	O
nonlearnability	O
of	O
linear	B
regression	I
even	O
if	O
d	O
=	O
1	O
)	O
let	O
h	O
=	O
r	O
,	O
and	O
the	O
loss	B
be	O
the	O
squared	O
loss	B
:	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
(	O
wx	O
−	O
y	O
)	O
2	O
(	O
we	O
’	O
re	O
referring	O
to	O
the	O
12.2	O
convex	B
learning	O
problems	O
165	O
choose	O
	O
=	O
1/100	O
,	O
δ	O
=	O
1/2	O
,	O
let	O
m	O
≥	O
m	O
(	O
	O
,	O
δ	O
)	O
,	O
and	O
set	B
µ	O
=	O
log	O
(	O
100/99	O
)	O
homogenous	B
case	O
)	O
.	O
let	O
a	O
be	O
any	O
deterministic	O
algorithm.1	O
assume	O
,	O
by	O
way	O
of	O
contradiction	O
,	O
that	O
a	O
is	O
a	O
successful	O
pac	O
learner	O
for	O
this	O
problem	O
.	O
that	O
is	O
,	O
there	O
exists	O
a	O
function	B
m	O
(	O
·	O
,	O
·	O
)	O
,	O
such	O
that	O
for	O
every	O
distribution	O
d	O
and	O
for	O
every	O
	O
,	O
δ	O
if	O
a	O
receives	O
a	O
training	B
set	I
of	O
size	O
m	O
≥	O
m	O
(	O
	O
,	O
δ	O
)	O
,	O
it	O
should	O
output	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
,	O
a	O
hypothesis	B
ˆw	O
=	O
a	O
(	O
s	O
)	O
,	O
such	O
that	O
ld	O
(	O
ˆw	O
)	O
−	O
minw	O
ld	O
(	O
w	O
)	O
≤	O
	O
.	O
.	O
we	O
will	O
deﬁne	O
two	O
distributions	O
,	O
and	O
will	O
show	O
that	O
a	O
is	O
likely	O
to	O
fail	O
on	O
at	O
least	O
one	O
of	O
them	O
.	O
the	O
ﬁrst	O
distribution	O
,	O
d1	O
,	O
is	O
supported	O
on	O
two	O
examples	O
,	O
z1	O
=	O
(	O
1	O
,	O
0	O
)	O
and	O
z2	O
=	O
(	O
µ	O
,	O
−1	O
)	O
,	O
where	O
the	O
probability	O
mass	O
of	O
the	O
ﬁrst	O
example	O
is	O
µ	O
while	O
the	O
probability	O
mass	O
of	O
the	O
second	O
example	O
is	O
1	O
−	O
µ.	O
the	O
second	O
distribution	O
,	O
d2	O
,	O
is	O
supported	O
entirely	O
on	O
z2	O
.	O
2m	O
observe	O
that	O
for	O
both	O
distributions	O
,	O
the	O
probability	O
that	O
all	O
examples	O
of	O
the	O
training	B
set	I
will	O
be	O
of	O
the	O
second	O
type	O
is	O
at	O
least	O
99	O
%	O
.	O
this	O
is	O
trivially	O
true	O
for	O
d2	O
,	O
whereas	O
for	O
d1	O
,	O
the	O
probability	O
of	O
this	O
event	O
is	O
(	O
1	O
−	O
µ	O
)	O
m	O
≥	O
e−2µm	O
=	O
0.99.	O
since	O
we	O
assume	O
that	O
a	O
is	O
a	O
deterministic	O
algorithm	O
,	O
upon	O
receiving	O
a	O
training	B
set	I
of	O
m	O
examples	O
,	O
each	O
of	O
which	O
is	O
(	O
µ	O
,	O
−1	O
)	O
,	O
the	O
algorithm	O
will	O
output	O
some	O
ˆw	O
.	O
now	O
,	O
if	O
ˆw	O
<	O
−1/	O
(	O
2µ	O
)	O
,	O
we	O
will	O
set	B
the	O
distribution	O
to	O
be	O
d1	O
.	O
hence	O
,	O
ld1	O
(	O
ˆw	O
)	O
≥	O
µ	O
(	O
ˆw	O
)	O
2	O
≥	O
1/	O
(	O
4µ	O
)	O
.	O
ld1	O
(	O
w	O
)	O
≤	O
ld1	O
(	O
0	O
)	O
=	O
(	O
1	O
−	O
µ	O
)	O
.	O
min	O
w	O
however	O
,	O
it	O
follows	O
that	O
ld1	O
(	O
ˆw	O
)	O
−	O
min	O
w	O
ld1	O
(	O
w	O
)	O
≥	O
1	O
4µ	O
−	O
(	O
1	O
−	O
µ	O
)	O
>	O
	O
.	O
therefore	O
,	O
such	O
algorithm	O
a	O
fails	O
on	O
d1	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
ˆw	O
≥	O
−1/	O
(	O
2µ	O
)	O
then	O
we	O
’	O
ll	O
set	B
the	O
distribution	O
to	O
be	O
d2	O
.	O
then	O
we	O
have	O
that	O
ld2	O
(	O
ˆw	O
)	O
≥	O
1/4	O
while	O
minw	O
ld2	O
(	O
w	O
)	O
=	O
0	O
,	O
so	O
a	O
fails	O
on	O
d2	O
.	O
in	O
summary	O
,	O
we	O
have	O
shown	O
that	O
for	O
every	O
a	O
there	O
exists	O
a	O
distribution	O
on	O
which	O
a	O
fails	O
,	O
which	O
implies	O
that	O
the	O
problem	O
is	O
not	O
pac	O
learnable	O
.	O
a	O
possible	O
solution	O
to	O
this	O
problem	O
is	O
to	O
add	O
another	O
constraint	O
on	O
the	O
hypoth-	O
esis	O
class	O
.	O
in	O
addition	O
to	O
the	O
convexity	O
requirement	O
,	O
we	O
require	O
that	O
h	O
will	O
be	O
bounded	O
;	O
namely	O
,	O
we	O
assume	O
that	O
for	O
some	O
predeﬁned	O
scalar	O
b	O
,	O
every	O
hypothesis	B
w	O
∈	O
h	O
satisﬁes	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
≤	O
b.	O
boundedness	B
and	O
convexity	O
alone	O
are	O
still	O
not	O
suﬃcient	O
for	O
ensuring	O
that	O
the	O
problem	O
is	O
learnable	O
,	O
as	O
the	O
following	O
example	O
demonstrates	O
.	O
example	O
12.9	O
as	O
in	O
example	O
12.8	O
,	O
consider	O
a	O
regression	B
problem	O
with	O
the	O
squared	O
loss	B
.	O
however	O
,	O
this	O
time	O
let	O
h	O
=	O
{	O
w	O
:	O
|w|	O
≤	O
1	O
}	O
⊂	O
r	O
be	O
a	O
bounded	O
1	O
namely	O
,	O
given	O
s	O
the	O
output	O
of	O
a	O
is	O
determined	O
.	O
this	O
requirement	O
is	O
for	O
the	O
sake	O
of	O
simplicity	O
.	O
a	O
slightly	O
more	O
involved	O
argument	O
will	O
show	O
that	O
nondeterministic	O
algorithms	O
will	O
also	O
fail	O
to	O
learn	O
the	O
problem	O
.	O
166	O
convex	B
learning	O
problems	O
hypothesis	B
class	I
.	O
it	O
is	O
easy	O
to	O
verify	O
that	O
h	O
is	O
convex	B
.	O
the	O
argument	O
will	O
be	O
the	O
same	O
as	O
in	O
example	O
12.8	O
,	O
except	O
that	O
now	O
the	O
two	O
distributions	O
,	O
d1	O
,	O
d2	O
will	O
be	O
supported	O
on	O
z1	O
=	O
(	O
1/µ	O
,	O
0	O
)	O
and	O
z2	O
=	O
(	O
1	O
,	O
−1	O
)	O
.	O
if	O
the	O
algorithm	O
a	O
returns	O
ˆw	O
<	O
−1/2	O
upon	O
receiving	O
m	O
examples	O
of	O
the	O
second	O
type	O
,	O
then	O
we	O
will	O
set	B
the	O
distribution	O
to	O
be	O
d1	O
and	O
have	O
that	O
ld1	O
(	O
ˆw	O
)	O
−	O
min	O
ld1	O
(	O
w	O
)	O
≥	O
µ	O
(	O
ˆw/µ	O
)	O
2	O
−	O
ld1	O
(	O
0	O
)	O
≥	O
1/	O
(	O
4µ	O
)	O
−	O
(	O
1	O
−	O
µ	O
)	O
>	O
	O
.	O
similarly	O
,	O
if	O
ˆw	O
≥	O
−1/2	O
we	O
will	O
set	B
the	O
distribution	O
to	O
be	O
d2	O
and	O
have	O
that	O
w	O
ld2	O
(	O
ˆw	O
)	O
−	O
min	O
w	O
ld2	O
(	O
w	O
)	O
≥	O
(	O
−1/2	O
+	O
1	O
)	O
2	O
−	O
0	O
>	O
	O
.	O
this	O
example	O
shows	O
that	O
we	O
need	O
additional	O
assumptions	O
on	O
the	O
learning	O
problem	O
,	O
and	O
this	O
time	O
the	O
solution	O
is	O
in	O
lipschitzness	O
or	O
smoothness	B
of	O
the	O
loss	B
function	I
.	O
this	O
motivates	O
a	O
deﬁnition	O
of	O
two	O
families	O
of	O
learning	O
problems	O
,	O
convex-lipschitz-bounded	O
and	O
convex-smooth-bounded	O
,	O
which	O
are	O
deﬁned	O
later	O
.	O
12.2.2	O
convex-lipschitz/smooth-bounded	O
learning	O
problems	O
definition	O
12.12	O
(	O
convex-lipschitz-bounded	O
learning	O
problem	O
)	O
a	O
learning	O
problem	O
,	O
(	O
h	O
,	O
z	O
,	O
(	O
cid:96	O
)	O
)	O
,	O
is	O
called	O
convex-lipschitz-bounded	O
,	O
with	O
parameters	O
ρ	O
,	O
b	O
if	O
the	O
following	O
holds	O
:	O
•	O
the	O
hypothesis	B
class	I
h	O
is	O
a	O
convex	B
set	O
and	O
for	O
all	O
w	O
∈	O
h	O
we	O
have	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
≤	O
b	O
.	O
•	O
for	O
all	O
z	O
∈	O
z	O
,	O
the	O
loss	B
function	I
,	O
(	O
cid:96	O
)	O
(	O
·	O
,	O
z	O
)	O
,	O
is	O
a	O
convex	B
and	O
ρ-lipschitz	O
function	B
.	O
example	O
12.10	O
let	O
x	O
=	O
{	O
x	O
∈	O
rd	O
:	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
≤	O
ρ	O
}	O
and	O
y	O
=	O
r.	O
let	O
h	O
=	O
{	O
w	O
∈	O
rd	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
≤	O
b	O
}	O
and	O
let	O
the	O
loss	B
function	I
be	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
|	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
−	O
y|	O
.	O
this	O
corre-	O
sponds	O
to	O
a	O
regression	B
problem	O
with	O
the	O
absolute-value	O
loss	B
,	O
where	O
we	O
assume	O
that	O
the	O
instances	O
are	O
in	O
a	O
ball	O
of	O
radius	O
ρ	O
and	O
we	O
restrict	O
the	O
hypotheses	O
to	O
be	O
homogenous	B
linear	O
functions	O
deﬁned	O
by	O
a	O
vector	O
w	O
whose	O
norm	O
is	O
bounded	O
by	O
b.	O
then	O
,	O
the	O
resulting	O
problem	O
is	O
convex-lipschitz-bounded	O
with	O
parameters	O
ρ	O
,	O
b.	O
definition	O
12.13	O
(	O
convex-smooth-bounded	B
learning	I
problem	O
)	O
a	O
learning	O
problem	O
,	O
(	O
h	O
,	O
z	O
,	O
(	O
cid:96	O
)	O
)	O
,	O
is	O
called	O
convex-smooth-bounded	O
,	O
with	O
parameters	O
β	O
,	O
b	O
if	O
the	O
following	O
holds	O
:	O
•	O
the	O
hypothesis	B
class	I
h	O
is	O
a	O
convex	B
set	O
and	O
for	O
all	O
w	O
∈	O
h	O
we	O
have	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
≤	O
b	O
.	O
•	O
for	O
all	O
z	O
∈	O
z	O
,	O
the	O
loss	B
function	I
,	O
(	O
cid:96	O
)	O
(	O
·	O
,	O
z	O
)	O
,	O
is	O
a	O
convex	B
,	O
nonnegative	O
,	O
and	O
β-smooth	O
function	B
.	O
note	O
that	O
we	O
also	O
required	O
that	O
the	O
loss	B
function	I
is	O
nonnegative	O
.	O
this	O
is	O
needed	O
to	O
ensure	O
that	O
the	O
loss	B
function	I
is	O
self-bounded	O
,	O
as	O
described	O
in	O
the	O
previous	O
section	O
.	O
12.3	O
surrogate	B
loss	I
functions	O
167	O
example	O
12.11	O
let	O
x	O
=	O
{	O
x	O
∈	O
rd	O
:	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
≤	O
β/2	O
}	O
and	O
y	O
=	O
r.	O
let	O
h	O
=	O
{	O
w	O
∈	O
rd	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
≤	O
b	O
}	O
and	O
let	O
the	O
loss	B
function	I
be	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
−	O
y	O
)	O
2.	O
this	O
corresponds	O
to	O
a	O
regression	B
problem	O
with	O
the	O
squared	O
loss	B
,	O
where	O
we	O
assume	O
that	O
the	O
instances	O
are	O
in	O
a	O
ball	O
of	O
radius	O
β/2	O
and	O
we	O
restrict	O
the	O
hypotheses	O
to	O
be	O
homogenous	B
linear	O
functions	O
deﬁned	O
by	O
a	O
vector	O
w	O
whose	O
norm	O
is	O
bounded	O
by	O
b.	O
then	O
,	O
the	O
resulting	O
problem	O
is	O
convex-smooth-bounded	O
with	O
parameters	O
β	O
,	O
b.	O
we	O
claim	O
that	O
these	O
two	O
families	O
of	O
learning	O
problems	O
are	O
learnable	O
.	O
that	O
is	O
,	O
the	O
properties	O
of	O
convexity	O
,	O
boundedness	B
,	O
and	O
lipschitzness	O
or	O
smoothness	B
of	O
the	O
loss	B
function	I
are	O
suﬃcient	O
for	O
learnability	O
.	O
we	O
will	O
prove	O
this	O
claim	O
in	O
the	O
next	O
chapters	O
by	O
introducing	O
algorithms	O
that	O
learn	O
these	O
problems	O
successfully	O
.	O
12.3	O
surrogate	B
loss	I
functions	O
as	O
mentioned	O
,	O
and	O
as	O
we	O
will	O
see	O
in	O
the	O
next	O
chapters	O
,	O
convex	B
problems	O
can	O
be	O
learned	O
eﬀﬁciently	O
.	O
however	O
,	O
in	O
many	O
cases	O
,	O
the	O
natural	O
loss	B
function	I
is	O
not	O
convex	B
and	O
,	O
in	O
particular	O
,	O
implementing	O
the	O
erm	O
rule	O
is	O
hard	O
.	O
as	O
an	O
example	O
,	O
consider	O
the	O
problem	O
of	O
learning	O
the	O
hypothesis	B
class	I
of	O
half-	O
spaces	O
with	O
respect	O
to	O
the	O
0	O
−	O
1	O
loss	B
.	O
that	O
is	O
,	O
(	O
cid:96	O
)	O
0−1	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
1	O
[	O
y	O
(	O
cid:54	O
)	O
=sign	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
]	O
=	O
1	O
[	O
y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
≤0	O
]	O
.	O
this	O
loss	B
function	I
is	O
not	O
convex	B
with	O
respect	O
to	O
w	O
and	O
indeed	O
,	O
when	O
trying	O
to	O
minimize	O
the	O
empirical	B
risk	I
with	O
respect	O
to	O
this	O
loss	B
function	I
we	O
might	O
encounter	O
local	O
minima	O
(	O
see	O
exercise	O
1	O
)	O
.	O
furthermore	O
,	O
as	O
discussed	O
in	O
chapter	O
8	O
,	O
solving	O
the	O
erm	O
problem	O
with	O
respect	O
to	O
the	O
0−	O
1	O
loss	B
in	O
the	O
unrealizable	O
case	O
is	O
known	O
to	O
be	O
np-hard	O
.	O
to	O
circumvent	O
the	O
hardness	O
result	O
,	O
one	O
popular	O
approach	O
is	O
to	O
upper	O
bound	O
the	O
nonconvex	O
loss	B
function	I
by	O
a	O
convex	B
surrogate	O
loss	B
function	I
.	O
as	O
its	O
name	O
indicates	O
,	O
the	O
requirements	O
from	O
a	O
convex	B
surrogate	O
loss	B
are	O
as	O
follows	O
:	O
1.	O
it	O
should	O
be	O
convex	B
.	O
2.	O
it	O
should	O
upper	O
bound	O
the	O
original	O
loss	B
.	O
for	O
example	O
,	O
in	O
the	O
context	O
of	O
learning	O
halfspaces	O
,	O
we	O
can	O
deﬁne	O
the	O
so-called	O
hinge	B
loss	I
as	O
a	O
convex	B
surrogate	O
for	O
the	O
0	O
−	O
1	O
loss	B
,	O
as	O
follows	O
:	O
(	O
cid:96	O
)	O
hinge	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
def=	O
max	O
{	O
0	O
,	O
1	O
−	O
y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
}	O
.	O
clearly	O
,	O
for	O
all	O
w	O
and	O
all	O
(	O
x	O
,	O
y	O
)	O
,	O
(	O
cid:96	O
)	O
0−1	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
≤	O
(	O
cid:96	O
)	O
hinge	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
.	O
in	O
addition	O
,	O
the	O
convexity	O
of	O
the	O
hinge	B
loss	I
follows	O
directly	O
from	O
claim	O
12.5.	O
hence	O
,	O
the	O
hinge	B
loss	I
satisﬁes	O
the	O
requirements	O
of	O
a	O
convex	B
surrogate	O
loss	B
function	I
for	O
the	O
zero-one	O
loss	B
.	O
an	O
illustration	O
of	O
the	O
functions	O
(	O
cid:96	O
)	O
0−1	O
and	O
(	O
cid:96	O
)	O
hinge	O
is	O
given	O
in	O
the	O
following	O
.	O
168	O
convex	B
learning	O
problems	O
(	O
cid:96	O
)	O
hinge	O
(	O
cid:96	O
)	O
0−1	O
1	O
1	O
y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
once	O
we	O
have	O
deﬁned	O
the	O
surrogate	O
convex	O
loss	B
,	O
we	O
can	O
learn	O
the	O
problem	O
with	O
respect	O
to	O
it	O
.	O
the	O
generalization	O
requirement	O
from	O
a	O
hinge	B
loss	I
learner	O
will	O
have	O
the	O
form	O
lhinged	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
min	O
w∈h	O
lhinged	O
(	O
w	O
)	O
+	O
	O
,	O
where	O
lhinged	O
can	O
lower	O
bound	O
the	O
left-hand	O
side	O
by	O
l0−1d	O
(	O
a	O
(	O
s	O
)	O
)	O
,	O
which	O
yields	O
(	O
w	O
)	O
=	O
e	O
(	O
x	O
,	O
y	O
)	O
∼d	O
[	O
(	O
cid:96	O
)	O
hinge	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
]	O
.	O
using	O
the	O
surrogate	O
property	O
,	O
we	O
w∈h	O
lhinged	O
we	O
can	O
further	O
rewrite	O
the	O
upper	O
bound	O
as	O
follows	O
:	O
l0−1d	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
min	O
(	O
cid:18	O
)	O
l0−1d	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
min	O
w∈h	O
l0−1d	O
(	O
w	O
)	O
+	O
w∈h	O
lhinged	O
min	O
(	O
w	O
)	O
+	O
	O
.	O
(	O
cid:19	O
)	O
w∈h	O
l0−1d	O
(	O
w	O
)	O
(	O
w	O
)	O
−	O
min	O
+	O
	O
.	O
that	O
is	O
,	O
the	O
0−1	O
error	O
of	O
the	O
learned	O
predictor	B
is	O
upper	O
bounded	O
by	O
three	O
terms	O
:	O
•	O
approximation	B
error	I
:	O
this	O
is	O
the	O
term	O
minw∈h	O
l0−1d	O
(	O
w	O
)	O
,	O
which	O
measures	O
how	O
well	O
the	O
hypothesis	B
class	I
performs	O
on	O
the	O
distribution	O
.	O
we	O
already	O
elabo-	O
rated	O
on	O
this	O
error	O
term	O
in	O
chapter	O
5	O
.	O
•	O
estimation	B
error	I
:	O
this	O
is	O
the	O
error	O
that	O
results	O
from	O
the	O
fact	O
that	O
we	O
only	O
receive	O
a	O
training	B
set	I
and	O
do	O
not	O
observe	O
the	O
distribution	O
d.	O
we	O
already	O
elaborated	O
on	O
this	O
error	O
term	O
in	O
chapter	O
5	O
.	O
(	O
cid:17	O
)	O
(	O
w	O
)	O
−	O
minw∈h	O
l0−1d	O
(	O
w	O
)	O
that	O
measures	O
the	O
diﬀerence	O
between	O
the	O
approximation	B
error	I
with	O
respect	O
to	O
the	O
surrogate	B
loss	I
and	O
the	O
approximation	B
error	I
with	O
respect	O
to	O
the	O
orig-	O
inal	O
loss	B
.	O
the	O
optimization	B
error	I
is	O
a	O
result	O
of	O
our	O
inability	O
to	O
minimize	O
the	O
training	O
loss	O
with	O
respect	O
to	O
the	O
original	O
loss	B
.	O
the	O
size	O
of	O
this	O
error	O
depends	O
on	O
the	O
speciﬁc	O
distribution	O
of	O
the	O
data	O
and	O
on	O
the	O
speciﬁc	O
surrogate	B
loss	I
we	O
are	O
using	O
.	O
•	O
optimization	B
error	I
:	O
this	O
is	O
the	O
term	O
minw∈h	O
lhinged	O
(	O
cid:16	O
)	O
12.4	O
summary	O
we	O
introduced	O
two	O
families	O
of	O
learning	O
problems	O
:	O
convex-lipschitz-bounded	O
and	O
convex-smooth-bounded	O
.	O
in	O
the	O
next	O
two	O
chapters	O
we	O
will	O
describe	O
two	O
generic	O
12.5	O
bibliographic	O
remarks	O
169	O
learning	O
algorithms	O
for	O
these	O
families	O
.	O
we	O
also	O
introduced	O
the	O
notion	O
of	O
convex	B
surrogate	O
loss	B
function	I
,	O
which	O
enables	O
us	O
also	O
to	O
utilize	O
the	O
convex	B
machinery	O
for	O
nonconvex	O
problems	O
.	O
12.5	O
bibliographic	O
remarks	O
12.6	O
there	O
are	O
several	O
excellent	O
books	O
on	O
convex	B
analysis	O
and	O
optimization	O
(	O
boyd	O
&	O
vandenberghe	O
2004	O
,	O
borwein	O
&	O
lewis	O
2006	O
,	O
bertsekas	O
1999	O
,	O
hiriart-urruty	O
&	O
lemar´echal	O
1996	O
)	O
.	O
regarding	O
learning	O
problems	O
,	O
the	O
family	O
of	O
convex-lipschitz-	O
bounded	O
problems	O
was	O
ﬁrst	O
studied	O
by	O
zinkevich	O
(	O
2003	O
)	O
in	O
the	O
context	O
of	O
online	B
learning	I
and	O
by	O
shalev-shwartz	O
,	O
shamir	O
,	O
sridharan	O
&	O
srebro	O
(	O
2009	O
)	O
in	O
the	O
con-	O
text	O
of	O
pac	O
learning	O
.	O
not	O
a	O
global	O
minimum	O
of	O
ls	O
.	O
exercises	O
1.	O
construct	O
an	O
example	O
showing	O
that	O
the	O
0−1	O
loss	B
function	I
may	O
suﬀer	O
from	O
local	O
minima	O
;	O
namely	O
,	O
construct	O
a	O
training	O
sample	O
s	O
∈	O
(	O
x	O
×	O
{	O
±1	O
}	O
)	O
m	O
(	O
say	O
,	O
for	O
x	O
=	O
r2	O
)	O
,	O
for	O
which	O
there	O
exist	O
a	O
vector	O
w	O
and	O
some	O
	O
>	O
0	O
such	O
that	O
1.	O
for	O
any	O
w	O
(	O
cid:48	O
)	O
such	O
that	O
(	O
cid:107	O
)	O
w	O
−	O
w	O
(	O
cid:48	O
)	O
(	O
cid:107	O
)	O
≤	O
	O
we	O
have	O
ls	O
(	O
w	O
)	O
≤	O
ls	O
(	O
w	O
(	O
cid:48	O
)	O
)	O
(	O
where	O
the	O
loss	B
here	O
is	O
the	O
0−1	O
loss	B
)	O
.	O
this	O
means	O
that	O
w	O
is	O
a	O
local	B
minimum	I
of	O
ls	O
.	O
2.	O
there	O
exists	O
some	O
w∗	O
such	O
that	O
ls	O
(	O
w∗	O
)	O
<	O
ls	O
(	O
w	O
)	O
.	O
this	O
means	O
that	O
w	O
is	O
2.	O
consider	O
the	O
learning	O
problem	O
of	O
logistic	B
regression	I
:	O
let	O
h	O
=	O
x	O
=	O
{	O
x	O
∈	O
rd	O
:	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
≤	O
b	O
}	O
,	O
for	O
some	O
scalar	O
b	O
>	O
0	O
,	O
let	O
y	O
=	O
{	O
±1	O
}	O
,	O
and	O
let	O
the	O
loss	B
function	I
(	O
cid:96	O
)	O
be	O
deﬁned	O
as	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
log	O
(	O
1	O
+	O
exp	O
(	O
−y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
)	O
.	O
show	O
that	O
the	O
resulting	O
learning	O
problem	O
is	O
both	O
convex-lipschitz-bounded	O
and	O
convex-	O
smooth-bounded	O
.	O
specify	O
the	O
parameters	O
of	O
lipschitzness	O
and	O
smoothness	B
.	O
3.	O
consider	O
the	O
problem	O
of	O
learning	O
halfspaces	O
with	O
the	O
hinge	B
loss	I
.	O
we	O
limit	O
our	O
domain	B
to	O
the	O
euclidean	O
ball	O
with	O
radius	O
r.	O
that	O
is	O
,	O
x	O
=	O
{	O
x	O
:	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
≤	O
r	O
}	O
.	O
the	O
label	B
set	O
is	O
y	O
=	O
{	O
±1	O
}	O
and	O
the	O
loss	B
function	I
(	O
cid:96	O
)	O
is	O
deﬁned	O
by	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
max	O
{	O
0	O
,	O
1	O
−	O
y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
}	O
.	O
we	O
already	O
know	O
that	O
the	O
loss	B
function	I
is	O
convex	B
.	O
show	O
that	O
it	O
is	O
r-lipschitz	O
.	O
4	O
.	O
(	O
*	O
)	O
convex-lipschitz-boundedness	O
is	O
not	O
suﬃcient	O
for	O
computa-	O
tional	O
eﬃciency	O
:	O
in	O
the	O
next	O
chapter	O
we	O
show	O
that	O
from	O
the	O
statistical	O
perspective	O
,	O
all	O
convex-lipschitz-bounded	O
problems	O
are	O
learnable	O
(	O
in	O
the	O
ag-	O
nostic	O
pac	O
model	O
)	O
.	O
however	O
,	O
our	O
main	O
motivation	O
to	O
learn	O
such	O
problems	O
resulted	O
from	O
the	O
computational	O
perspective	O
–	O
convex	B
optimization	O
is	O
often	O
eﬃciently	O
solvable	O
.	O
yet	O
the	O
goal	O
of	O
this	O
exercise	O
is	O
to	O
show	O
that	O
convexity	O
alone	O
is	O
not	O
suﬃcient	O
for	O
eﬃciency	O
.	O
we	O
show	O
that	O
even	O
for	O
the	O
case	O
d	O
=	O
1	O
,	O
there	O
is	O
a	O
convex-lipschitz-bounded	O
problem	O
which	O
can	O
not	O
be	O
learned	O
by	O
any	O
computable	O
learner	O
.	O
let	O
the	O
hypothesis	B
class	I
be	O
h	O
=	O
[	O
0	O
,	O
1	O
]	O
and	O
let	O
the	O
example	O
domain	B
,	O
z	O
,	O
be	O
170	O
convex	B
learning	O
problems	O
the	O
set	B
of	O
all	O
turing	O
machines	O
.	O
deﬁne	O
the	O
loss	B
function	I
as	O
follows	O
.	O
for	O
every	O
turing	O
machine	O
t	O
∈	O
z	O
,	O
let	O
(	O
cid:96	O
)	O
(	O
0	O
,	O
t	O
)	O
=	O
1	O
if	O
t	O
halts	O
on	O
the	O
input	O
0	O
and	O
(	O
cid:96	O
)	O
(	O
0	O
,	O
t	O
)	O
=	O
0	O
if	O
t	O
doesn	O
’	O
t	O
halt	O
on	O
the	O
input	O
0.	O
similarly	O
,	O
let	O
(	O
cid:96	O
)	O
(	O
1	O
,	O
t	O
)	O
=	O
0	O
if	O
t	O
halts	O
on	O
the	O
input	O
0	O
and	O
(	O
cid:96	O
)	O
(	O
1	O
,	O
t	O
)	O
=	O
1	O
if	O
t	O
doesn	O
’	O
t	O
halt	O
on	O
the	O
input	O
0.	O
finally	O
,	O
for	O
h	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
let	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
t	O
)	O
=	O
h	O
(	O
cid:96	O
)	O
(	O
0	O
,	O
t	O
)	O
+	O
(	O
1	O
−	O
h	O
)	O
(	O
cid:96	O
)	O
(	O
1	O
,	O
t	O
)	O
.	O
1.	O
show	O
that	O
the	O
resulting	O
learning	O
problem	O
is	O
convex-lipschitz-bounded	O
.	O
2.	O
show	O
that	O
no	O
computable	O
algorithm	O
can	O
learn	O
the	O
problem	O
.	O
13	O
regularization	B
and	O
stability	B
in	O
the	O
previous	O
chapter	O
we	O
introduced	O
the	O
families	O
of	O
convex-lipschitz-bounded	O
and	O
convex-smooth-bounded	B
learning	I
problems	O
.	O
in	O
this	O
section	O
we	O
show	O
that	O
all	O
learning	O
problems	O
in	O
these	O
two	O
families	O
are	O
learnable	O
.	O
for	O
some	O
learning	O
problems	O
of	O
this	O
type	O
it	O
is	O
possible	O
to	O
show	O
that	O
uniform	B
convergence	I
holds	O
;	O
hence	O
they	O
are	O
learnable	O
using	O
the	O
erm	O
rule	O
.	O
however	O
,	O
this	O
is	O
not	O
true	O
for	O
all	O
learning	O
problems	O
of	O
this	O
type	O
.	O
yet	O
,	O
we	O
will	O
introduce	O
another	O
learning	O
rule	O
and	O
will	O
show	O
that	O
it	O
learns	O
all	O
convex-lipschitz-bounded	O
and	O
convex-smooth-bounded	B
learning	I
problems	O
.	O
the	O
new	O
learning	O
paradigm	O
we	O
introduce	O
in	O
this	O
chapter	O
is	O
called	O
regularized	B
loss	I
minimization	I
,	O
or	O
rlm	O
for	O
short	O
.	O
in	O
rlm	O
we	O
minimize	O
the	O
sum	O
of	O
the	O
em-	O
pirical	O
risk	B
and	O
a	O
regularization	B
function	O
.	O
intuitively	O
,	O
the	O
regularization	B
function	O
measures	O
the	O
complexity	O
of	O
hypotheses	O
.	O
indeed	O
,	O
one	O
interpretation	O
of	O
the	O
reg-	O
ularization	O
function	B
is	O
the	O
structural	O
risk	B
minimization	O
paradigm	O
we	O
discussed	O
in	O
chapter	O
7.	O
another	O
view	O
of	O
regularization	B
is	O
as	O
a	O
stabilizer	O
of	O
the	O
learning	O
algorithm	O
.	O
an	O
algorithm	O
is	O
considered	O
stable	O
if	O
a	O
slight	O
change	O
of	O
its	O
input	O
does	O
not	O
change	O
its	O
output	O
much	O
.	O
we	O
will	O
formally	O
deﬁne	O
the	O
notion	O
of	O
stability	B
(	O
what	O
we	O
mean	O
by	O
“	O
slight	O
change	O
of	O
input	O
”	O
and	O
by	O
“	O
does	O
not	O
change	O
much	O
the	O
out-	O
put	O
”	O
)	O
and	O
prove	O
its	O
close	O
relation	O
to	O
learnability	O
.	O
finally	O
,	O
we	O
will	O
show	O
that	O
using	O
the	O
squared	O
(	O
cid:96	O
)	O
2	O
norm	O
as	O
a	O
regularization	B
function	O
stabilizes	O
all	O
convex-lipschitz	O
or	O
convex-smooth	O
learning	O
problems	O
.	O
hence	O
,	O
rlm	O
can	O
be	O
used	O
as	O
a	O
general	O
learning	O
rule	O
for	O
these	O
families	O
of	O
learning	O
problems	O
.	O
13.1	O
regularized	B
loss	I
minimization	I
regularized	O
loss	B
minimization	O
(	O
rlm	O
)	O
is	O
a	O
learning	O
rule	O
in	O
which	O
we	O
jointly	O
min-	O
imize	O
the	O
empirical	B
risk	I
and	O
a	O
regularization	B
function	O
.	O
formally	O
,	O
a	O
regularization	B
function	O
is	O
a	O
mapping	O
r	O
:	O
rd	O
→	O
r	O
,	O
and	O
the	O
regularized	B
loss	I
minimization	I
rule	O
outputs	O
a	O
hypothesis	B
in	O
argmin	O
w	O
(	O
ls	O
(	O
w	O
)	O
+	O
r	O
(	O
w	O
)	O
)	O
.	O
(	O
13.1	O
)	O
regularized	B
loss	I
minimization	I
shares	O
similarities	O
with	O
minimum	O
description	O
length	O
algorithms	O
and	O
structural	O
risk	B
minimization	O
(	O
see	O
chapter	O
7	O
)	O
.	O
intuitively	O
,	O
the	O
“	O
complexity	O
”	O
of	O
hypotheses	O
is	O
measured	O
by	O
the	O
value	O
of	O
the	O
regularization	B
func-	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
172	O
regularization	B
and	O
stability	B
tion	O
,	O
and	O
the	O
algorithm	O
balances	O
between	O
low	O
empirical	B
risk	I
and	O
“	O
simpler	O
,	O
”	O
or	O
“	O
less	O
complex	O
,	O
”	O
hypotheses	O
.	O
there	O
are	O
many	O
possible	O
regularization	B
functions	O
one	O
can	O
use	O
,	O
reﬂecting	O
some	O
prior	O
belief	O
about	O
the	O
problem	O
(	O
similarly	O
to	O
the	O
description	O
language	O
in	O
minimum	O
description	O
length	O
)	O
.	O
throughout	O
this	O
section	O
we	O
will	O
focus	O
on	O
one	O
of	O
the	O
most	O
simple	O
regularization	B
functions	O
:	O
r	O
(	O
w	O
)	O
=	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
,	O
where	O
λ	O
>	O
0	O
is	O
a	O
scalar	O
and	O
the	O
norm	O
is	O
the	O
(	O
cid:96	O
)	O
2	O
norm	O
,	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
=	O
i	O
.	O
this	O
yields	O
the	O
learning	O
rule	O
:	O
(	O
cid:113	O
)	O
(	O
cid:80	O
)	O
d	O
i=1	O
w2	O
(	O
cid:0	O
)	O
ls	O
(	O
w	O
)	O
+	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
(	O
cid:1	O
)	O
.	O
a	O
(	O
s	O
)	O
=	O
argmin	O
w	O
(	O
13.2	O
)	O
this	O
type	O
of	O
regularization	B
function	O
is	O
often	O
called	O
tikhonov	O
regularization	B
.	O
as	O
mentioned	O
before	O
,	O
one	O
interpretation	O
of	O
equation	O
(	O
13.2	O
)	O
is	O
using	O
structural	O
risk	B
minimization	O
,	O
where	O
the	O
norm	O
of	O
w	O
is	O
a	O
measure	O
of	O
its	O
“	O
complexity.	O
”	O
recall	B
that	O
in	O
the	O
previous	O
chapter	O
we	O
introduced	O
the	O
notion	O
of	O
bounded	O
hypothesis	B
classes	O
.	O
therefore	O
,	O
we	O
can	O
deﬁne	O
a	O
sequence	O
of	O
hypothesis	B
classes	O
,	O
h1	O
⊂	O
h2	O
⊂	O
h3	O
.	O
.	O
.	O
,	O
where	O
hi	O
=	O
{	O
w	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
≤	O
i	O
}	O
.	O
if	O
the	O
sample	B
complexity	I
of	O
each	O
hi	O
depends	O
on	O
i	O
then	O
the	O
rlm	O
rule	O
is	O
similar	O
to	O
the	O
srm	O
rule	O
for	O
this	O
sequence	O
of	O
nested	O
classes	O
.	O
a	O
diﬀerent	O
interpretation	O
of	O
regularization	B
is	O
as	O
a	O
stabilizer	O
.	O
in	O
the	O
next	O
section	O
we	O
deﬁne	O
the	O
notion	O
of	O
stability	B
and	O
prove	O
that	O
stable	O
learning	O
rules	O
do	O
not	O
overﬁt	O
.	O
but	O
ﬁrst	O
,	O
let	O
us	O
demonstrate	O
the	O
rlm	O
rule	O
for	O
linear	B
regression	I
with	O
the	O
squared	O
loss	B
.	O
13.1.1	O
ridge	B
regression	I
applying	O
the	O
rlm	O
rule	O
with	O
tikhonov	O
regularization	B
to	O
linear	B
regression	I
with	O
the	O
squared	O
loss	B
,	O
we	O
obtain	O
the	O
following	O
learning	O
rule	O
:	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
argmin	O
w∈rd	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
2	O
+	O
1	O
m	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
−	O
yi	O
)	O
2	O
1	O
2	O
.	O
(	O
13.3	O
)	O
performing	O
linear	B
regression	I
using	O
equation	O
(	O
13.3	O
)	O
is	O
called	O
ridge	B
regression	I
.	O
to	O
solve	O
equation	O
(	O
13.3	O
)	O
we	O
compare	O
the	O
gradient	B
of	O
the	O
objective	O
to	O
zero	O
and	O
obtain	O
the	O
set	B
of	O
linear	O
equations	O
(	O
2λmi	O
+	O
a	O
)	O
w	O
=	O
b	O
,	O
(	O
cid:32	O
)	O
m	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
a	O
=	O
xi	O
x	O
(	O
cid:62	O
)	O
i	O
m	O
(	O
cid:88	O
)	O
where	O
i	O
is	O
the	O
identity	O
matrix	O
and	O
a	O
,	O
b	O
are	O
as	O
deﬁned	O
in	O
equation	O
(	O
9.6	O
)	O
,	O
namely	O
,	O
and	O
b	O
=	O
yixi	O
.	O
(	O
13.4	O
)	O
i=1	O
i=1	O
since	O
a	O
is	O
a	O
positive	O
semideﬁnite	O
matrix	O
,	O
the	O
matrix	O
2λmi	O
+	O
a	O
has	O
all	O
its	O
eigen-	O
values	O
bounded	O
below	O
by	O
2λm	O
.	O
hence	O
,	O
this	O
matrix	O
is	O
invertible	O
and	O
the	O
solution	O
to	O
ridge	B
regression	I
becomes	O
w	O
=	O
(	O
2λmi	O
+	O
a	O
)	O
−1	O
b	O
.	O
(	O
13.5	O
)	O
13.2	O
stable	O
rules	O
do	O
not	O
overﬁt	O
173	O
in	O
the	O
next	O
section	O
we	O
formally	O
show	O
how	O
regularization	B
stabilizes	O
the	O
algo-	O
rithm	O
and	O
prevents	O
overﬁtting	B
.	O
in	O
particular	O
,	O
the	O
analysis	O
presented	O
in	O
the	O
next	O
sections	O
(	O
particularly	O
,	O
corollary	O
13.11	O
)	O
will	O
yield	O
:	O
theorem	O
13.1	O
let	O
d	O
be	O
a	O
distribution	O
over	O
x	O
×	O
[	O
−1	O
,	O
1	O
]	O
,	O
where	O
x	O
=	O
{	O
x	O
∈	O
rd	O
:	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
≤	O
1	O
}	O
.	O
let	O
h	O
=	O
{	O
w	O
∈	O
rd	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
≤	O
b	O
}	O
.	O
for	O
any	O
	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
let	O
m	O
≥	O
150	O
b2/2	O
.	O
then	O
,	O
applying	O
the	O
ridge	B
regression	I
algorithm	O
with	O
parameter	O
λ	O
=	O
/	O
(	O
3b2	O
)	O
satisﬁes	O
e	O
s∼dm	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
≤	O
min	O
w∈h	O
ld	O
(	O
w	O
)	O
+	O
	O
.	O
remark	O
13.1	O
the	O
preceding	O
theorem	O
tells	O
us	O
how	O
many	O
examples	O
are	O
needed	O
to	O
guarantee	O
that	O
the	O
expected	O
value	O
of	O
the	O
risk	B
of	O
the	O
learned	O
predictor	B
will	O
be	O
bounded	O
by	O
the	O
approximation	B
error	I
of	O
the	O
class	O
plus	O
	O
.	O
in	O
the	O
usual	O
deﬁnition	O
of	O
agnostic	O
pac	O
learning	O
we	O
require	O
that	O
the	O
risk	B
of	O
the	O
learned	O
predictor	B
will	O
be	O
bounded	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ.	O
in	O
exercise	O
1	O
we	O
show	O
how	O
an	O
algorithm	O
with	O
a	O
bounded	O
expected	O
risk	B
can	O
be	O
used	O
to	O
construct	O
an	O
agnostic	O
pac	O
learner	O
.	O
13.2	O
stable	O
rules	O
do	O
not	O
overﬁt	O
intuitively	O
,	O
a	O
learning	O
algorithm	O
is	O
stable	O
if	O
a	O
small	O
change	O
of	O
the	O
input	O
to	O
the	O
algorithm	O
does	O
not	O
change	O
the	O
output	O
of	O
the	O
algorithm	O
much	O
.	O
of	O
course	O
,	O
there	O
are	O
many	O
ways	O
to	O
deﬁne	O
what	O
we	O
mean	O
by	O
“	O
a	O
small	O
change	O
of	O
the	O
input	O
”	O
and	O
what	O
we	O
mean	O
by	O
“	O
does	O
not	O
change	O
the	O
output	O
much	O
”	O
.	O
in	O
this	O
section	O
we	O
deﬁne	O
a	O
speciﬁc	O
notion	O
of	O
stability	B
and	O
prove	O
that	O
under	O
this	O
deﬁnition	O
,	O
stable	O
rules	O
do	O
not	O
overﬁt	O
.	O
let	O
a	O
be	O
a	O
learning	O
algorithm	O
,	O
let	O
s	O
=	O
(	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
)	O
be	O
a	O
training	B
set	I
of	O
m	O
examples	O
,	O
and	O
let	O
a	O
(	O
s	O
)	O
denote	O
the	O
output	O
of	O
a.	O
the	O
algorithm	O
a	O
suﬀers	O
from	O
overﬁtting	B
if	O
the	O
diﬀerence	O
between	O
the	O
true	O
risk	O
of	O
its	O
output	O
,	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
,	O
and	O
the	O
empirical	B
risk	I
of	O
its	O
output	O
,	O
ls	O
(	O
a	O
(	O
s	O
)	O
)	O
,	O
is	O
large	O
.	O
as	O
mentioned	O
in	O
remark	O
13.1	O
,	O
throughout	O
this	O
chapter	O
we	O
focus	O
on	O
the	O
expectation	O
(	O
with	O
respect	O
to	O
the	O
choice	O
of	O
s	O
)	O
of	O
this	O
quantity	O
,	O
namely	O
,	O
es	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
−	O
ls	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
.	O
we	O
next	O
deﬁne	O
the	O
notion	O
of	O
stability	B
.	O
given	O
the	O
training	B
set	I
s	O
and	O
an	O
ad-	O
ditional	O
example	O
z	O
(	O
cid:48	O
)	O
,	O
let	O
s	O
(	O
i	O
)	O
be	O
the	O
training	B
set	I
obtained	O
by	O
replacing	O
the	O
i	O
’	O
th	O
example	O
of	O
s	O
with	O
z	O
(	O
cid:48	O
)	O
;	O
namely	O
,	O
s	O
(	O
i	O
)	O
=	O
(	O
z1	O
,	O
.	O
.	O
.	O
,	O
zi−1	O
,	O
z	O
(	O
cid:48	O
)	O
,	O
zi+1	O
,	O
.	O
.	O
.	O
,	O
zm	O
)	O
.	O
in	O
our	O
deﬁ-	O
nition	O
of	O
stability	B
,	O
“	O
a	O
small	O
change	O
of	O
the	O
input	O
”	O
means	O
that	O
we	O
feed	O
a	O
with	O
s	O
(	O
i	O
)	O
instead	O
of	O
with	O
s.	O
that	O
is	O
,	O
we	O
only	O
replace	O
one	O
training	O
example	O
.	O
we	O
measure	O
the	O
eﬀect	O
of	O
this	O
small	O
change	O
of	O
the	O
input	O
on	O
the	O
output	O
of	O
a	O
,	O
by	O
comparing	O
the	O
loss	B
of	O
the	O
hypothesis	B
a	O
(	O
s	O
)	O
on	O
zi	O
to	O
the	O
loss	B
of	O
the	O
hypothesis	B
a	O
(	O
s	O
(	O
i	O
)	O
)	O
on	O
zi	O
.	O
intuitively	O
,	O
a	O
good	O
learning	O
algorithm	O
will	O
have	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
zi	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
≥	O
0	O
,	O
since	O
in	O
the	O
ﬁrst	O
term	O
the	O
learning	O
algorithm	O
does	O
not	O
observe	O
the	O
example	O
zi	O
while	O
in	O
the	O
second	O
term	O
zi	O
is	O
indeed	O
observed	O
.	O
if	O
the	O
preceding	O
diﬀerence	O
is	O
very	O
large	O
we	O
suspect	O
that	O
the	O
learning	O
algorithm	O
might	O
overﬁt	O
.	O
this	O
is	O
because	O
the	O
174	O
regularization	B
and	O
stability	B
learning	O
algorithm	O
drastically	O
changes	O
its	O
prediction	O
on	O
zi	O
if	O
it	O
observes	O
it	O
in	O
the	O
training	B
set	I
.	O
this	O
is	O
formalized	O
in	O
the	O
following	O
theorem	O
.	O
theorem	O
13.2	O
let	O
d	O
be	O
a	O
distribution	O
.	O
let	O
s	O
=	O
(	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
)	O
be	O
an	O
i.i.d	O
.	O
se-	O
quence	O
of	O
examples	O
and	O
let	O
z	O
(	O
cid:48	O
)	O
be	O
another	O
i.i.d	O
.	O
example	O
.	O
let	O
u	O
(	O
m	O
)	O
be	O
the	O
uniform	O
distribution	O
over	O
[	O
m	O
]	O
.	O
then	O
,	O
for	O
any	O
learning	O
algorithm	O
,	O
e	O
s∼dm	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
−	O
ls	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
=	O
e	O
(	O
s	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
∼dm+1	O
,	O
i∼u	O
(	O
m	O
)	O
[	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
,	O
zi	O
)	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
]	O
.	O
(	O
13.6	O
)	O
proof	O
since	O
s	O
and	O
z	O
(	O
cid:48	O
)	O
are	O
both	O
drawn	O
i.i.d	O
.	O
from	O
d	O
,	O
we	O
have	O
that	O
for	O
every	O
i	O
,	O
e	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
=	O
e	O
s	O
s	O
,	O
z	O
(	O
cid:48	O
)	O
[	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
]	O
=	O
e	O
s	O
,	O
z	O
(	O
cid:48	O
)	O
[	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
zi	O
)	O
]	O
.	O
on	O
the	O
other	O
hand	O
,	O
we	O
can	O
write	O
e	O
[	O
ls	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
=	O
e	O
s	O
s	O
,	O
i	O
[	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
]	O
.	O
combining	O
the	O
two	O
equations	O
we	O
conclude	O
our	O
proof	O
.	O
when	O
the	O
right-hand	O
side	O
of	O
equation	O
(	O
13.6	O
)	O
is	O
small	O
,	O
we	O
say	O
that	O
a	O
is	O
a	O
stable	O
algorithm	O
–	O
changing	O
a	O
single	O
example	O
in	O
the	O
training	B
set	I
does	O
not	O
lead	O
to	O
a	O
signiﬁcant	O
change	O
.	O
formally	O
,	O
definition	O
13.3	O
(	O
on-average-replace-one-stable	O
)	O
let	O
	O
:	O
n	O
→	O
r	O
be	O
a	O
mono-	O
tonically	O
decreasing	O
function	B
.	O
we	O
say	O
that	O
a	O
learning	O
algorithm	O
a	O
is	O
on-average-	O
replace-one-stable	O
with	O
rate	O
	O
(	O
m	O
)	O
if	O
for	O
every	O
distribution	O
d	O
e	O
(	O
s	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
∼dm+1	O
,	O
i∼u	O
(	O
m	O
)	O
[	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
,	O
zi	O
)	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
]	O
≤	O
	O
(	O
m	O
)	O
.	O
theorem	O
13.2	O
tells	O
us	O
that	O
a	O
learning	O
algorithm	O
does	O
not	O
overﬁt	O
if	O
and	O
only	O
if	O
it	O
is	O
on-average-replace-one-stable	O
.	O
of	O
course	O
,	O
a	O
learning	O
algorithm	O
that	O
does	O
not	O
overﬁt	O
is	O
not	O
necessarily	O
a	O
good	O
learning	O
algorithm	O
–	O
take	O
,	O
for	O
example	O
,	O
an	O
algorithm	O
a	O
that	O
always	O
outputs	O
the	O
same	O
hypothesis	B
.	O
a	O
useful	O
algorithm	O
should	O
ﬁnd	O
a	O
hypothesis	B
that	O
on	O
one	O
hand	O
ﬁts	O
the	O
training	B
set	I
(	O
i.e.	O
,	O
has	O
a	O
low	O
empirical	B
risk	I
)	O
and	O
on	O
the	O
other	O
hand	O
does	O
not	O
overﬁt	O
.	O
or	O
,	O
in	O
light	O
of	O
theorem	O
13.2	O
,	O
the	O
algorithm	O
should	O
both	O
ﬁt	O
the	O
training	B
set	I
and	O
at	O
the	O
same	O
time	O
be	O
stable	O
.	O
as	O
we	O
shall	O
see	O
,	O
the	O
parameter	O
λ	O
of	O
the	O
rlm	O
rule	O
balances	O
between	O
ﬁtting	O
the	O
training	B
set	I
and	O
being	O
stable	O
.	O
13.3	O
tikhonov	O
regularization	B
as	O
a	O
stabilizer	O
in	O
the	O
previous	O
section	O
we	O
saw	O
that	O
stable	O
rules	O
do	O
not	O
overﬁt	O
.	O
in	O
this	O
section	O
we	O
show	O
that	O
applying	O
the	O
rlm	O
rule	O
with	O
tikhonov	O
regularization	B
,	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
,	O
leads	O
to	O
a	O
stable	O
algorithm	O
.	O
we	O
will	O
assume	O
that	O
the	O
loss	B
function	I
is	O
convex	B
and	O
that	O
it	O
is	O
either	O
lipschitz	O
or	O
smooth	O
.	O
the	O
main	O
property	O
of	O
the	O
tikhonov	O
regularization	B
that	O
we	O
rely	O
on	O
is	O
that	O
it	O
makes	O
the	O
objective	O
of	O
rlm	O
strongly	B
convex	I
,	O
as	O
deﬁned	O
in	O
the	O
following	O
.	O
13.3	O
tikhonov	O
regularization	B
as	O
a	O
stabilizer	O
175	O
definition	O
13.4	O
(	O
strongly	B
convex	I
functions	O
)	O
a	O
function	B
f	O
is	O
λ-strongly	O
con-	O
vex	O
if	O
for	O
all	O
w	O
,	O
u	O
and	O
α	O
∈	O
(	O
0	O
,	O
1	O
)	O
we	O
have	O
f	O
(	O
αw	O
+	O
(	O
1	O
−	O
α	O
)	O
u	O
)	O
≤	O
αf	O
(	O
w	O
)	O
+	O
(	O
1	O
−	O
α	O
)	O
f	O
(	O
u	O
)	O
−	O
λ	O
2	O
α	O
(	O
1	O
−	O
α	O
)	O
(	O
cid:107	O
)	O
w	O
−	O
u	O
(	O
cid:107	O
)	O
2.	O
clearly	O
,	O
every	O
convex	B
function	O
is	O
0-strongly	O
convex	B
.	O
an	O
illustration	O
of	O
strong	O
convexity	O
is	O
given	O
in	O
the	O
following	O
ﬁgure	O
.	O
f	O
(	O
u	O
)	O
f	O
(	O
w	O
)	O
≥	O
λ	O
2	O
α	O
(	O
1	O
−	O
α	O
)	O
(	O
cid:107	O
)	O
u	O
−	O
w	O
(	O
cid:107	O
)	O
2	O
w	O
αw	O
+	O
(	O
1	O
−	O
α	O
)	O
u	O
u	O
the	O
following	O
lemma	O
implies	O
that	O
the	O
objective	O
of	O
rlm	O
is	O
(	O
2λ	O
)	O
-strongly	O
con-	O
vex	O
.	O
in	O
addition	O
,	O
it	O
underscores	O
an	O
important	O
property	O
of	O
strong	O
convexity	O
.	O
lemma	O
13.5	O
1.	O
the	O
function	B
f	O
(	O
w	O
)	O
=	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
is	O
2λ-strongly	O
convex	B
.	O
2.	O
if	O
f	O
is	O
λ-strongly	O
convex	B
and	O
g	O
is	O
convex	B
,	O
then	O
f	O
+	O
g	O
is	O
λ-strongly	O
convex	B
.	O
3.	O
if	O
f	O
is	O
λ-strongly	O
convex	B
and	O
u	O
is	O
a	O
minimizer	O
of	O
f	O
,	O
then	O
,	O
for	O
any	O
w	O
,	O
f	O
(	O
w	O
)	O
−	O
f	O
(	O
u	O
)	O
≥	O
λ	O
2	O
(	O
cid:107	O
)	O
w	O
−	O
u	O
(	O
cid:107	O
)	O
2.	O
proof	O
the	O
ﬁrst	O
two	O
points	O
follow	O
directly	O
from	O
the	O
deﬁnition	O
.	O
to	O
prove	O
the	O
last	O
point	O
,	O
we	O
divide	O
the	O
deﬁnition	O
of	O
strong	O
convexity	O
by	O
α	O
and	O
rearrange	O
terms	O
to	O
get	O
that	O
f	O
(	O
u	O
+	O
α	O
(	O
w	O
−	O
u	O
)	O
)	O
−	O
f	O
(	O
u	O
)	O
α	O
≤	O
f	O
(	O
w	O
)	O
−	O
f	O
(	O
u	O
)	O
−	O
λ	O
2	O
(	O
1	O
−	O
α	O
)	O
(	O
cid:107	O
)	O
w	O
−	O
u	O
(	O
cid:107	O
)	O
2.	O
taking	O
the	O
limit	O
α	O
→	O
0	O
we	O
obtain	O
that	O
the	O
right-hand	O
side	O
converges	O
to	O
f	O
(	O
w	O
)	O
−	O
f	O
(	O
u	O
)	O
−	O
λ	O
2	O
(	O
cid:107	O
)	O
w−	O
u	O
(	O
cid:107	O
)	O
2.	O
on	O
the	O
other	O
hand	O
,	O
the	O
left-hand	O
side	O
becomes	O
the	O
derivative	O
of	O
the	O
function	B
g	O
(	O
α	O
)	O
=	O
f	O
(	O
u	O
+	O
α	O
(	O
w	O
−	O
u	O
)	O
)	O
at	O
α	O
=	O
0.	O
since	O
u	O
is	O
a	O
minimizer	O
of	O
f	O
,	O
it	O
follows	O
that	O
α	O
=	O
0	O
is	O
a	O
minimizer	O
of	O
g	O
,	O
and	O
therefore	O
the	O
left-hand	O
side	O
of	O
the	O
preceding	O
goes	O
to	O
zero	O
in	O
the	O
limit	O
α	O
→	O
0	O
,	O
which	O
concludes	O
our	O
proof	O
.	O
we	O
now	O
turn	O
to	O
prove	O
that	O
rlm	O
is	O
stable	O
.	O
let	O
s	O
=	O
(	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
)	O
be	O
a	O
training	B
set	I
,	O
let	O
z	O
(	O
cid:48	O
)	O
be	O
an	O
additional	O
example	O
,	O
and	O
let	O
s	O
(	O
i	O
)	O
=	O
(	O
z1	O
,	O
.	O
.	O
.	O
,	O
zi−1	O
,	O
z	O
(	O
cid:48	O
)	O
,	O
zi+1	O
,	O
.	O
.	O
.	O
,	O
zm	O
)	O
.	O
let	O
a	O
be	O
the	O
rlm	O
rule	O
,	O
namely	O
,	O
(	O
cid:0	O
)	O
ls	O
(	O
w	O
)	O
+	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
(	O
cid:1	O
)	O
.	O
a	O
(	O
s	O
)	O
=	O
argmin	O
w	O
176	O
regularization	B
and	O
stability	B
denote	O
fs	O
(	O
w	O
)	O
=	O
ls	O
(	O
w	O
)	O
+	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
,	O
and	O
based	O
on	O
lemma	O
13.5	O
we	O
know	O
that	O
fs	O
is	O
(	O
2λ	O
)	O
-strongly	O
convex	B
.	O
relying	O
on	O
part	O
3	O
of	O
the	O
lemma	O
,	O
it	O
follows	O
that	O
for	O
any	O
v	O
,	O
fs	O
(	O
v	O
)	O
−	O
fs	O
(	O
a	O
(	O
s	O
)	O
)	O
≥	O
λ	O
(	O
cid:107	O
)	O
v	O
−	O
a	O
(	O
s	O
)	O
(	O
cid:107	O
)	O
2.	O
on	O
the	O
other	O
hand	O
,	O
for	O
any	O
v	O
and	O
u	O
,	O
and	O
for	O
all	O
i	O
,	O
we	O
have	O
fs	O
(	O
v	O
)	O
−	O
fs	O
(	O
u	O
)	O
=	O
ls	O
(	O
v	O
)	O
+	O
λ	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
ls	O
(	O
u	O
)	O
+	O
λ	O
(	O
cid:107	O
)	O
u	O
(	O
cid:107	O
)	O
2	O
)	O
=	O
ls	O
(	O
i	O
)	O
(	O
v	O
)	O
+	O
λ	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
ls	O
(	O
i	O
)	O
(	O
u	O
)	O
+	O
λ	O
(	O
cid:107	O
)	O
u	O
(	O
cid:107	O
)	O
2	O
)	O
(	O
cid:96	O
)	O
(	O
u	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
v	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
(	O
cid:96	O
)	O
(	O
v	O
,	O
zi	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
u	O
,	O
zi	O
)	O
+	O
+	O
m	O
(	O
13.7	O
)	O
(	O
13.8	O
)	O
.	O
in	O
particular	O
,	O
choosing	O
v	O
=	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
u	O
=	O
a	O
(	O
s	O
)	O
,	O
and	O
using	O
the	O
fact	O
that	O
v	O
mini-	O
mizes	O
ls	O
(	O
i	O
)	O
(	O
w	O
)	O
+	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
,	O
we	O
obtain	O
that	O
fs	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
)	O
−fs	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
zi	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
+	O
.	O
m	O
(	O
13.9	O
)	O
m	O
m	O
combining	O
this	O
with	O
equation	O
(	O
13.7	O
)	O
we	O
obtain	O
that	O
λ	O
(	O
cid:107	O
)	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
−	O
a	O
(	O
s	O
)	O
(	O
cid:107	O
)	O
2	O
≤	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
zi	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
+	O
m	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
.	O
m	O
(	O
13.10	O
)	O
the	O
two	O
subsections	O
that	O
follow	O
continue	O
the	O
stability	B
analysis	O
for	O
either	O
lip-	O
schitz	O
or	O
smooth	B
loss	I
functions	O
.	O
for	O
both	O
families	O
of	O
loss	B
functions	O
we	O
show	O
that	O
rlm	O
is	O
stable	O
and	O
therefore	O
it	O
does	O
not	O
overﬁt	O
.	O
13.3.1	O
lipschitz	O
loss	B
if	O
the	O
loss	B
function	I
,	O
(	O
cid:96	O
)	O
(	O
·	O
,	O
zi	O
)	O
,	O
is	O
ρ-lipschitz	O
,	O
then	O
by	O
the	O
deﬁnition	O
of	O
lipschitzness	O
,	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
zi	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
≤	O
ρ	O
(	O
cid:107	O
)	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
−	O
a	O
(	O
s	O
)	O
(	O
cid:107	O
)	O
.	O
(	O
13.11	O
)	O
similarly	O
,	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
≤	O
ρ	O
(	O
cid:107	O
)	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
−	O
a	O
(	O
s	O
)	O
(	O
cid:107	O
)	O
.	O
plugging	O
these	O
inequalities	O
into	O
equation	O
(	O
13.10	O
)	O
we	O
obtain	O
λ	O
(	O
cid:107	O
)	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
−	O
a	O
(	O
s	O
)	O
(	O
cid:107	O
)	O
2	O
≤	O
2	O
ρ	O
(	O
cid:107	O
)	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
−	O
a	O
(	O
s	O
)	O
(	O
cid:107	O
)	O
m	O
,	O
which	O
yields	O
(	O
cid:107	O
)	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
−	O
a	O
(	O
s	O
)	O
(	O
cid:107	O
)	O
≤	O
2	O
ρ	O
λ	O
m	O
.	O
plugging	O
the	O
preceding	O
back	O
into	O
equation	O
(	O
13.11	O
)	O
we	O
conclude	O
that	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
zi	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
≤	O
2	O
ρ2	O
λ	O
m	O
since	O
this	O
holds	O
for	O
any	O
s	O
,	O
z	O
(	O
cid:48	O
)	O
,	O
i	O
we	O
immediately	O
obtain	O
:	O
.	O
13.3	O
tikhonov	O
regularization	B
as	O
a	O
stabilizer	O
177	O
corollary	O
13.6	O
assume	O
that	O
the	O
loss	B
function	I
is	O
convex	B
and	O
ρ-lipschitz	O
.	O
then	O
,	O
the	O
rlm	O
rule	O
with	O
the	O
regularizer	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
is	O
on-average-replace-one-stable	O
with	O
rate	O
2	O
ρ2	O
λ	O
m	O
.	O
it	O
follows	O
(	O
using	O
theorem	O
13.2	O
)	O
that	O
e	O
s∼dm	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
−	O
ls	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
≤	O
2	O
ρ2	O
λ	O
m	O
.	O
13.3.2	O
smooth	O
and	O
nonnegative	O
loss	B
if	O
the	O
loss	B
is	O
β-smooth	O
and	O
nonnegative	O
then	O
it	O
is	O
also	O
self-bounded	O
(	O
see	O
sec-	O
tion	O
12.1	O
)	O
:	O
we	O
further	O
assume	O
that	O
λ	O
≥	O
2β	O
smoothness	B
assumption	O
we	O
have	O
that	O
(	O
cid:107	O
)	O
∇f	O
(	O
w	O
)	O
(	O
cid:107	O
)	O
2	O
≤	O
2βf	O
(	O
w	O
)	O
.	O
m	O
,	O
or	O
,	O
in	O
other	O
words	O
,	O
that	O
β	O
≤	O
λm/2	O
.	O
by	O
the	O
(	O
13.12	O
)	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
zi	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
≤	O
(	O
cid:104	O
)	O
∇	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
,	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
−a	O
(	O
s	O
)	O
(	O
cid:105	O
)	O
+	O
(	O
cid:107	O
)	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
−a	O
(	O
s	O
)	O
(	O
cid:107	O
)	O
2	O
.	O
(	O
13.13	O
)	O
using	O
the	O
cauchy-schwartz	O
inequality	O
and	O
equation	O
(	O
12.6	O
)	O
we	O
further	O
obtain	O
that	O
β	O
2	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
zi	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
≤	O
(	O
cid:107	O
)	O
∇	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
(	O
cid:107	O
)	O
(	O
cid:107	O
)	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
−	O
a	O
(	O
s	O
)	O
(	O
cid:107	O
)	O
+	O
≤	O
(	O
cid:112	O
)	O
2β	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
(	O
cid:107	O
)	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
−	O
a	O
(	O
s	O
)	O
(	O
cid:107	O
)	O
+	O
(	O
cid:107	O
)	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
−	O
a	O
(	O
s	O
)	O
(	O
cid:107	O
)	O
2	O
β	O
2	O
(	O
cid:107	O
)	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
−	O
a	O
(	O
s	O
)	O
(	O
cid:107	O
)	O
2	O
.	O
β	O
2	O
(	O
13.14	O
)	O
by	O
a	O
symmetric	O
argument	O
it	O
holds	O
that	O
,	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
≤	O
(	O
cid:113	O
)	O
2β	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
(	O
cid:107	O
)	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
−	O
a	O
(	O
s	O
)	O
(	O
cid:107	O
)	O
+	O
(	O
cid:107	O
)	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
−	O
a	O
(	O
s	O
)	O
(	O
cid:107	O
)	O
2	O
.	O
β	O
2	O
plugging	O
these	O
inequalities	O
into	O
equation	O
(	O
13.10	O
)	O
and	O
rearranging	O
terms	O
we	O
ob-	O
tain	O
that	O
(	O
cid:113	O
)	O
(	O
cid:18	O
)	O
(	O
cid:112	O
)	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
+	O
(	O
cid:18	O
)	O
(	O
cid:112	O
)	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
+	O
(	O
cid:113	O
)	O
√	O
2β	O
(	O
λ	O
m	O
−	O
β	O
)	O
√	O
8β	O
λ	O
m	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:107	O
)	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
−	O
a	O
(	O
s	O
)	O
(	O
cid:107	O
)	O
≤	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
.	O
combining	O
the	O
preceding	O
with	O
the	O
assumption	O
β	O
≤	O
λm/2	O
yields	O
(	O
cid:107	O
)	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
−	O
a	O
(	O
s	O
)	O
(	O
cid:107	O
)	O
≤	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
.	O
178	O
regularization	B
and	O
stability	B
combining	O
the	O
preceding	O
with	O
equation	O
(	O
13.14	O
)	O
and	O
again	O
using	O
the	O
assumption	O
β	O
≤	O
λm/2	O
yield	O
≤	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
zi	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
≤	O
(	O
cid:112	O
)	O
2β	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
(	O
cid:107	O
)	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
−	O
a	O
(	O
s	O
)	O
(	O
cid:107	O
)	O
+	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
(	O
cid:112	O
)	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
+	O
(	O
cid:18	O
)	O
4β	O
(	O
cid:113	O
)	O
(	O
cid:19	O
)	O
2	O
(	O
cid:18	O
)	O
(	O
cid:112	O
)	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
+	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
8β2	O
(	O
λm	O
)	O
2	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
+	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
(	O
cid:113	O
)	O
≤	O
8β	O
λm	O
≤	O
24β	O
λm	O
λm	O
+	O
,	O
(	O
cid:107	O
)	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
−	O
a	O
(	O
s	O
)	O
(	O
cid:107	O
)	O
2	O
β	O
2	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
(	O
cid:19	O
)	O
2	O
where	O
in	O
the	O
last	O
step	O
we	O
used	O
the	O
inequality	O
(	O
a+b	O
)	O
2	O
≤	O
3	O
(	O
a2+b2	O
)	O
.	O
taking	O
expecta-	O
tion	O
with	O
respect	O
to	O
s	O
,	O
z	O
(	O
cid:48	O
)	O
,	O
i	O
and	O
noting	O
that	O
e	O
[	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
]	O
=	O
e	O
[	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
]	O
=	O
e	O
[	O
ls	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
,	O
we	O
conclude	O
that	O
:	O
corollary	O
13.7	O
assume	O
that	O
the	O
loss	B
function	I
is	O
β-smooth	O
and	O
nonnegative	O
.	O
then	O
,	O
the	O
rlm	O
rule	O
with	O
the	O
regularizer	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
,	O
where	O
λ	O
≥	O
2β	O
m	O
,	O
satisﬁes	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
zi	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
e	O
[	O
ls	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
.	O
note	O
that	O
if	O
for	O
all	O
z	O
we	O
have	O
(	O
cid:96	O
)	O
(	O
0	O
,	O
z	O
)	O
≤	O
c	O
,	O
for	O
some	O
scalar	O
c	O
>	O
0	O
,	O
then	O
for	O
(	O
cid:105	O
)	O
≤	O
48β	O
λm	O
e	O
(	O
cid:104	O
)	O
every	O
s	O
,	O
ls	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
ls	O
(	O
a	O
(	O
s	O
)	O
)	O
+	O
λ	O
(	O
cid:107	O
)	O
a	O
(	O
s	O
)	O
(	O
cid:107	O
)	O
2	O
≤	O
ls	O
(	O
0	O
)	O
+	O
λ	O
(	O
cid:107	O
)	O
0	O
(	O
cid:107	O
)	O
2	O
=	O
ls	O
(	O
0	O
)	O
≤	O
c.	O
hence	O
,	O
corollary	O
13.7	O
also	O
implies	O
that	O
e	O
(	O
cid:104	O
)	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
(	O
i	O
)	O
)	O
,	O
zi	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
a	O
(	O
s	O
)	O
,	O
zi	O
)	O
(	O
cid:105	O
)	O
≤	O
48	O
β	O
c	O
λm	O
.	O
13.4	O
controlling	O
the	O
fitting-stability	O
tradeoﬀ	O
we	O
can	O
rewrite	O
the	O
expected	O
risk	B
of	O
a	O
learning	O
algorithm	O
as	O
e	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
=	O
e	O
s	O
s	O
[	O
ls	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
+	O
e	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
−	O
ls	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
.	O
s	O
(	O
13.15	O
)	O
the	O
ﬁrst	O
term	O
reﬂects	O
how	O
well	O
a	O
(	O
s	O
)	O
ﬁts	O
the	O
training	B
set	I
while	O
the	O
second	O
term	O
reﬂects	O
the	O
diﬀerence	O
between	O
the	O
true	O
and	O
empirical	O
risks	O
of	O
a	O
(	O
s	O
)	O
.	O
as	O
we	O
have	O
shown	O
in	O
theorem	O
13.2	O
,	O
the	O
second	O
term	O
is	O
equivalent	O
to	O
the	O
stability	B
of	O
a.	O
since	O
our	O
goal	O
is	O
to	O
minimize	O
the	O
risk	B
of	O
the	O
algorithm	O
,	O
we	O
need	O
that	O
the	O
sum	O
of	O
both	O
terms	O
will	O
be	O
small	O
.	O
in	O
the	O
previous	O
section	O
we	O
have	O
bounded	O
the	O
stability	B
term	O
.	O
we	O
have	O
shown	O
that	O
the	O
stability	B
term	O
decreases	O
as	O
the	O
regularization	B
parameter	O
,	O
λ	O
,	O
increases	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
empirical	B
risk	I
increases	O
with	O
λ.	O
we	O
therefore	O
face	O
a	O
13.4	O
controlling	O
the	O
fitting-stability	O
tradeoﬀ	O
179	O
tradeoﬀ	O
between	O
ﬁtting	O
and	O
overﬁtting	B
.	O
this	O
tradeoﬀ	O
is	O
quite	O
similar	O
to	O
the	O
bias-	O
complexity	O
tradeoﬀ	O
we	O
discussed	O
previously	O
in	O
the	O
book	O
.	O
we	O
now	O
derive	O
bounds	O
on	O
the	O
empirical	B
risk	I
term	O
for	O
the	O
rlm	O
rule	O
.	O
recall	B
(	O
cid:0	O
)	O
ls	O
(	O
w	O
)	O
+	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
(	O
cid:1	O
)	O
.	O
fix	O
some	O
that	O
the	O
rlm	O
rule	O
is	O
deﬁned	O
as	O
a	O
(	O
s	O
)	O
=	O
argminw	O
arbitrary	O
vector	O
w∗	O
.	O
we	O
have	O
ls	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
ls	O
(	O
a	O
(	O
s	O
)	O
)	O
+	O
λ	O
(	O
cid:107	O
)	O
a	O
(	O
s	O
)	O
(	O
cid:107	O
)	O
2	O
≤	O
ls	O
(	O
w∗	O
)	O
+	O
λ	O
(	O
cid:107	O
)	O
w∗	O
(	O
cid:107	O
)	O
2.	O
taking	O
expectation	O
of	O
both	O
sides	O
with	O
respect	O
to	O
s	O
and	O
noting	O
that	O
es	O
[	O
ls	O
(	O
w∗	O
)	O
]	O
=	O
ld	O
(	O
w∗	O
)	O
,	O
we	O
obtain	O
that	O
[	O
ls	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
≤	O
ld	O
(	O
w∗	O
)	O
+	O
λ	O
(	O
cid:107	O
)	O
w∗	O
(	O
cid:107	O
)	O
2.	O
e	O
s	O
(	O
13.16	O
)	O
plugging	O
this	O
into	O
equation	O
(	O
13.15	O
)	O
we	O
obtain	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
≤	O
ld	O
(	O
w∗	O
)	O
+	O
λ	O
(	O
cid:107	O
)	O
w∗	O
(	O
cid:107	O
)	O
2	O
+	O
e	O
e	O
s	O
s	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
−	O
ls	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
.	O
combining	O
the	O
preceding	O
with	O
corollary	O
13.6	O
we	O
conclude	O
:	O
corollary	O
13.8	O
assume	O
that	O
the	O
loss	B
function	I
is	O
convex	B
and	O
ρ-lipschitz	O
.	O
then	O
,	O
the	O
rlm	O
rule	O
with	O
the	O
regularization	B
function	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
satisﬁes	O
∀w∗	O
,	O
e	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
≤	O
ld	O
(	O
w∗	O
)	O
+	O
λ	O
(	O
cid:107	O
)	O
w∗	O
(	O
cid:107	O
)	O
2	O
+	O
s	O
2ρ2	O
λ	O
m	O
.	O
this	O
bound	O
is	O
often	O
called	O
an	O
oracle	B
inequality	I
–	O
if	O
we	O
think	O
of	O
w∗	O
as	O
a	O
hy-	O
pothesis	O
with	O
low	O
risk	B
,	O
the	O
bound	O
tells	O
us	O
how	O
many	O
examples	O
are	O
needed	O
so	O
that	O
a	O
(	O
s	O
)	O
will	O
be	O
almost	O
as	O
good	O
as	O
w∗	O
,	O
had	O
we	O
known	O
the	O
norm	O
of	O
w∗	O
.	O
in	O
practice	O
,	O
however	O
,	O
we	O
usually	O
do	O
not	O
know	O
the	O
norm	O
of	O
w∗	O
.	O
we	O
therefore	O
usually	O
tune	O
λ	O
on	O
the	O
basis	O
of	O
a	O
validation	B
set	O
,	O
as	O
described	O
in	O
chapter	O
11.	O
we	O
can	O
also	O
easily	O
derive	O
a	O
pac-like	O
guarantee1	O
from	O
corollary	O
13.8	O
for	O
convex-	O
lipschitz-bounded	O
learning	O
problems	O
:	O
corollary	O
13.9	O
let	O
(	O
h	O
,	O
z	O
,	O
(	O
cid:96	O
)	O
)	O
be	O
a	O
convex-lipschitz-bounded	O
learning	O
problem	O
b2	O
m	O
.	O
then	O
,	O
the	O
with	O
parameters	O
ρ	O
,	O
b.	O
for	O
any	O
training	B
set	I
size	O
m	O
,	O
let	O
λ	O
=	O
rlm	O
rule	O
with	O
the	O
regularization	B
function	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
satisﬁes	O
(	O
cid:113	O
)	O
2ρ2	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
≤	O
min	O
e	O
s	O
w∈h	O
ld	O
(	O
w	O
)	O
+	O
ρ	O
b	O
(	O
cid:114	O
)	O
8	O
.	O
m	O
in	O
particular	O
,	O
for	O
every	O
	O
>	O
0	O
,	O
if	O
m	O
≥	O
8ρ2b2	O
es	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
≤	O
minw∈h	O
ld	O
(	O
w	O
)	O
+	O
	O
.	O
2	O
then	O
for	O
every	O
distribution	O
d	O
,	O
the	O
preceding	O
corollary	O
holds	O
for	O
lipschitz	O
loss	B
functions	O
.	O
if	O
instead	O
the	O
loss	B
function	I
is	O
smooth	O
and	O
nonnegative	O
,	O
then	O
we	O
can	O
combine	O
equation	O
(	O
13.16	O
)	O
with	O
corollary	O
13.7	O
to	O
get	O
:	O
1	O
again	O
,	O
the	O
bound	O
below	O
is	O
on	O
the	O
expected	O
risk	B
,	O
but	O
using	O
exercise	O
1	O
it	O
can	O
be	O
used	O
to	O
derive	O
an	O
agnostic	O
pac	O
learning	O
guarantee	O
.	O
180	O
regularization	B
and	O
stability	B
corollary	O
13.10	O
assume	O
that	O
the	O
loss	B
function	I
is	O
convex	B
,	O
β-smooth	O
,	O
and	O
nonnegative	O
.	O
then	O
,	O
the	O
rlm	O
rule	O
with	O
the	O
regularization	B
function	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
,	O
for	O
λ	O
≥	O
2β	O
(	O
cid:18	O
)	O
1	O
+	O
48β	O
λm	O
(	O
cid:19	O
)	O
(	O
cid:0	O
)	O
ld	O
(	O
w∗	O
)	O
+	O
λ	O
(	O
cid:107	O
)	O
w∗	O
(	O
cid:107	O
)	O
2	O
(	O
cid:1	O
)	O
.	O
m	O
,	O
satisﬁes	O
the	O
following	O
for	O
all	O
w∗	O
:	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
≤	O
e	O
s	O
[	O
ls	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
≤	O
e	O
s	O
(	O
cid:18	O
)	O
1	O
+	O
48β	O
λm	O
(	O
cid:19	O
)	O
for	O
example	O
,	O
if	O
we	O
choose	O
λ	O
=	O
48β	O
m	O
we	O
obtain	O
from	O
the	O
preceding	O
that	O
the	O
expected	O
true	O
risk	O
of	O
a	O
(	O
s	O
)	O
is	O
at	O
most	O
twice	O
the	O
expected	O
empirical	B
risk	I
of	O
a	O
(	O
s	O
)	O
.	O
furthermore	O
,	O
for	O
this	O
value	O
of	O
λ	O
,	O
the	O
expected	O
empirical	B
risk	I
of	O
a	O
(	O
s	O
)	O
is	O
at	O
most	O
ld	O
(	O
w∗	O
)	O
+	O
48β	O
m	O
(	O
cid:107	O
)	O
w∗	O
(	O
cid:107	O
)	O
2.	O
we	O
can	O
also	O
derive	O
a	O
learnability	O
guarantee	O
for	O
convex-smooth-bounded	O
learn-	O
ing	O
problems	O
based	O
on	O
corollary	O
13.10.	O
corollary	O
13.11	O
let	O
(	O
h	O
,	O
z	O
,	O
(	O
cid:96	O
)	O
)	O
be	O
a	O
convex-smooth-bounded	B
learning	I
problem	O
with	O
parameters	O
β	O
,	O
b.	O
assume	O
in	O
addition	O
that	O
(	O
cid:96	O
)	O
(	O
0	O
,	O
z	O
)	O
≤	O
1	O
for	O
all	O
z	O
∈	O
z.	O
for	O
any	O
and	O
set	B
λ	O
=	O
/	O
(	O
3b2	O
)	O
.	O
then	O
,	O
for	O
every	O
distribution	O
d	O
,	O
	O
∈	O
(	O
0	O
,	O
1	O
)	O
let	O
m	O
≥	O
150βb2	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
≤	O
min	O
e	O
s	O
w∈h	O
ld	O
(	O
w	O
)	O
+	O
	O
.	O
2	O
13.5	O
summary	O
we	O
introduced	O
stability	B
and	O
showed	O
that	O
if	O
an	O
algorithm	O
is	O
stable	O
then	O
it	O
does	O
not	O
overﬁt	O
.	O
furthermore	O
,	O
for	O
convex-lipschitz-bounded	O
or	O
convex-smooth-bounded	O
problems	O
,	O
the	O
rlm	O
rule	O
with	O
tikhonov	O
regularization	B
leads	O
to	O
a	O
stable	O
learning	O
algorithm	O
.	O
we	O
discussed	O
how	O
the	O
regularization	B
parameter	O
,	O
λ	O
,	O
controls	O
the	O
trade-	O
oﬀ	O
between	O
ﬁtting	O
and	O
overﬁtting	B
.	O
finally	O
,	O
we	O
have	O
shown	O
that	O
all	O
learning	O
prob-	O
lems	O
that	O
are	O
from	O
the	O
families	O
of	O
convex-lipschitz-bounded	O
and	O
convex-smooth-	O
bounded	O
problems	O
are	O
learnable	O
using	O
the	O
rlm	O
rule	O
.	O
the	O
rlm	O
paradigm	O
is	O
the	O
basis	O
for	O
many	O
popular	O
learning	O
algorithms	O
,	O
including	O
ridge	B
regression	I
(	O
which	O
we	O
discussed	O
in	O
this	O
chapter	O
)	O
and	O
support	O
vector	O
machines	O
(	O
which	O
will	O
be	O
discussed	O
in	O
chapter	O
15	O
)	O
.	O
in	O
the	O
next	O
chapter	O
we	O
will	O
present	O
stochastic	O
gradient	B
descent	I
,	O
which	O
gives	O
us	O
a	O
very	O
practical	O
alternative	O
way	O
to	O
learn	O
convex-lipschitz-bounded	O
and	O
convex-	O
smooth-bounded	O
problems	O
and	O
can	O
also	O
be	O
used	O
for	O
eﬃciently	O
implementing	O
the	O
rlm	O
rule	O
.	O
13.6	O
bibliographic	O
remarks	O
stability	B
is	O
widely	O
used	O
in	O
many	O
mathematical	O
contexts	O
.	O
for	O
example	O
,	O
the	O
neces-	O
sity	O
of	O
stability	B
for	O
so-called	O
inverse	O
problems	O
to	O
be	O
well	O
posed	O
was	O
ﬁrst	O
recognized	O
by	O
hadamard	O
(	O
1902	O
)	O
.	O
the	O
idea	O
of	O
regularization	B
and	O
its	O
relation	O
to	O
stability	B
be-	O
came	O
widely	O
known	O
through	O
the	O
works	O
of	O
tikhonov	O
(	O
1943	O
)	O
and	O
phillips	O
(	O
1962	O
)	O
.	O
13.7	O
exercises	O
181	O
in	O
the	O
context	O
of	O
modern	O
learning	O
theory	O
,	O
the	O
use	O
of	O
stability	B
can	O
be	O
traced	O
back	O
at	O
least	O
to	O
the	O
work	O
of	O
rogers	O
&	O
wagner	O
(	O
1978	O
)	O
,	O
which	O
noted	O
that	O
the	O
sensitiv-	O
ity	O
of	O
a	O
learning	O
algorithm	O
with	O
regard	O
to	O
small	O
changes	O
in	O
the	O
sample	O
controls	O
the	O
variance	O
of	O
the	O
leave-one-out	O
estimate	O
.	O
the	O
authors	O
used	O
this	O
observation	O
to	O
obtain	O
generalization	B
bounds	I
for	O
the	O
k-nearest	O
neighbor	O
algorithm	O
(	O
see	O
chap-	O
ter	O
19	O
)	O
.	O
these	O
results	O
were	O
later	O
extended	O
to	O
other	O
“	O
local	O
”	O
learning	O
algorithms	O
(	O
see	O
devroye	O
,	O
gy¨orﬁ	O
&	O
lugosi	O
(	O
1996	O
)	O
and	O
references	O
therein	O
)	O
.	O
in	O
addition	O
,	O
practi-	O
cal	O
methods	O
have	O
been	O
developed	O
to	O
introduce	O
stability	B
into	O
learning	O
algorithms	O
,	O
in	O
particular	O
the	O
bagging	O
technique	O
introduced	O
by	O
(	O
breiman	O
1996	O
)	O
.	O
over	O
the	O
last	O
decade	O
,	O
stability	B
was	O
studied	O
as	O
a	O
generic	O
condition	O
for	O
learnabil-	O
ity	O
.	O
see	O
(	O
kearns	O
&	O
ron	O
1999	O
,	O
bousquet	O
&	O
elisseeﬀ	O
2002	O
,	O
kutin	O
&	O
niyogi	O
2002	O
,	O
rakhlin	O
,	O
mukherjee	O
&	O
poggio	O
2005	O
,	O
mukherjee	O
,	O
niyogi	O
,	O
poggio	O
&	O
rifkin	O
2006	O
)	O
.	O
our	O
presentation	O
follows	O
the	O
work	O
of	O
shalev-shwartz	O
,	O
shamir	O
,	O
srebro	O
&	O
sridha-	O
ran	O
(	O
2010	O
)	O
,	O
who	O
showed	O
that	O
stability	B
is	O
suﬃcient	O
and	O
necessary	O
for	O
learning	O
.	O
they	O
have	O
also	O
shown	O
that	O
all	O
convex-lipschitz-bounded	O
learning	O
problems	O
are	O
learnable	O
using	O
rlm	O
,	O
even	O
though	O
for	O
some	O
convex-lipschitz-bounded	O
learning	O
problems	O
uniform	B
convergence	I
does	O
not	O
hold	O
in	O
a	O
strong	O
sense	O
.	O
13.7	O
exercises	O
1.	O
from	O
bounded	O
expected	O
risk	B
to	O
agnostic	O
pac	O
learning	O
:	O
let	O
a	O
be	O
an	O
algorithm	O
that	O
guarantees	O
the	O
following	O
:	O
if	O
m	O
≥	O
mh	O
(	O
	O
)	O
then	O
for	O
every	O
distribution	O
d	O
it	O
holds	O
that	O
e	O
s∼dm	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
≤	O
min	O
h∈h	O
ld	O
(	O
h	O
)	O
+	O
	O
.	O
•	O
show	O
that	O
for	O
every	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
if	O
m	O
≥	O
mh	O
(	O
	O
δ	O
)	O
then	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
it	O
holds	O
that	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
minh∈h	O
ld	O
(	O
h	O
)	O
+	O
	O
.	O
hint	O
:	O
observe	O
that	O
the	O
random	O
variable	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
−	O
minh∈h	O
ld	O
(	O
h	O
)	O
is	O
nonnegative	O
and	O
rely	O
on	O
markov	O
’	O
s	O
inequality	O
.	O
•	O
for	O
every	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
let	O
mh	O
(	O
	O
,	O
δ	O
)	O
=	O
mh	O
(	O
/2	O
)	O
(	O
cid:100	O
)	O
log2	O
(	O
1/δ	O
)	O
(	O
cid:101	O
)	O
+	O
(	O
cid:24	O
)	O
log	O
(	O
4/δ	O
)	O
+	O
log	O
(	O
(	O
cid:100	O
)	O
log2	O
(	O
1/δ	O
)	O
(	O
cid:101	O
)	O
)	O
(	O
cid:25	O
)	O
2	O
.	O
suggest	O
a	O
procedure	O
that	O
agnostic	O
pac	O
learns	O
the	O
problem	O
with	O
sample	B
complexity	I
of	O
mh	O
(	O
	O
,	O
δ	O
)	O
,	O
assuming	O
that	O
the	O
loss	B
function	I
is	O
bounded	O
by	O
1.	O
hint	O
:	O
let	O
k	O
=	O
(	O
cid:100	O
)	O
log2	O
(	O
1/δ	O
)	O
(	O
cid:101	O
)	O
.	O
divide	O
the	O
data	O
into	O
k	O
+1	O
chunks	O
,	O
where	O
each	O
of	O
the	O
ﬁrst	O
k	O
chunks	O
is	O
of	O
size	O
mh	O
(	O
/2	O
)	O
examples	O
.	O
train	O
the	O
ﬁrst	O
k	O
chunks	O
using	O
a.	O
on	O
the	O
basis	O
of	O
the	O
previous	O
question	O
argue	O
that	O
the	O
probability	O
that	O
for	O
all	O
of	O
these	O
chunks	O
we	O
have	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
>	O
minh∈h	O
ld	O
(	O
h	O
)	O
+	O
	O
is	O
at	O
most	O
2−k	O
≤	O
δ/2	O
.	O
finally	O
,	O
use	O
the	O
last	O
chunk	O
as	O
a	O
validation	B
set	O
.	O
2.	O
learnability	O
without	O
uniform	B
convergence	I
:	O
let	O
b	O
be	O
the	O
unit	O
ball	O
of	O
182	O
regularization	B
and	O
stability	B
rd	O
,	O
let	O
h	O
=	O
b	O
,	O
let	O
z	O
=	O
b	O
×	O
{	O
0	O
,	O
1	O
}	O
d	O
,	O
and	O
let	O
(	O
cid:96	O
)	O
:	O
z	O
×	O
h	O
→	O
r	O
be	O
deﬁned	O
as	O
follows	O
:	O
d	O
(	O
cid:88	O
)	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
α	O
)	O
)	O
=	O
αi	O
(	O
xi	O
−	O
wi	O
)	O
2.	O
i=1	O
this	O
problem	O
corresponds	O
to	O
an	O
unsupervised	B
learning	I
task	O
,	O
meaning	O
that	O
we	O
do	O
not	O
try	O
to	O
predict	O
the	O
label	B
of	O
x.	O
instead	O
,	O
what	O
we	O
try	O
to	O
do	O
is	O
to	O
ﬁnd	O
the	O
“	O
center	O
of	O
mass	O
”	O
of	O
the	O
distribution	O
over	O
b.	O
however	O
,	O
there	O
is	O
a	O
twist	O
,	O
modeled	O
by	O
the	O
vectors	O
α.	O
each	O
example	O
is	O
a	O
pair	O
(	O
x	O
,	O
α	O
)	O
,	O
where	O
x	O
is	O
the	O
instance	B
x	O
and	O
α	O
indicates	O
which	O
features	O
of	O
x	O
are	O
“	O
active	O
”	O
and	O
which	O
are	O
“	O
turned	O
oﬀ.	O
”	O
a	O
hypothesis	B
is	O
a	O
vector	O
w	O
representing	O
the	O
center	O
of	O
mass	O
of	O
the	O
distribution	O
,	O
and	O
the	O
loss	B
function	I
is	O
the	O
squared	O
euclidean	O
distance	O
between	O
x	O
and	O
w	O
,	O
but	O
only	O
with	O
respect	O
to	O
the	O
“	O
active	O
”	O
elements	O
of	O
x	O
.	O
•	O
show	O
that	O
this	O
problem	O
is	O
learnable	O
using	O
the	O
rlm	O
rule	O
with	O
a	O
sample	B
complexity	I
that	O
does	O
not	O
depend	O
on	O
d.	O
•	O
consider	O
a	O
distribution	O
d	O
over	O
z	O
as	O
follows	O
:	O
x	O
is	O
ﬁxed	O
to	O
be	O
some	O
x0	O
,	O
and	O
each	O
element	O
of	O
α	O
is	O
sampled	O
to	O
be	O
either	O
1	O
or	O
0	O
with	O
equal	O
probability	O
.	O
show	O
that	O
the	O
rate	O
of	O
uniform	B
convergence	I
of	O
this	O
problem	O
grows	O
with	O
d.	O
hint	O
:	O
let	O
m	O
be	O
a	O
training	B
set	I
size	O
.	O
show	O
that	O
if	O
d	O
(	O
cid:29	O
)	O
2m	O
,	O
then	O
there	O
is	O
a	O
high	O
probability	O
of	O
sampling	O
a	O
set	B
of	O
examples	O
such	O
that	O
there	O
exists	O
some	O
j	O
∈	O
[	O
d	O
]	O
for	O
which	O
αj	O
=	O
1	O
for	O
all	O
the	O
examples	O
in	O
the	O
training	B
set	I
.	O
show	O
that	O
such	O
a	O
sample	O
can	O
not	O
be	O
-representative	O
.	O
conclude	O
that	O
the	O
sample	B
complexity	I
of	O
uniform	B
convergence	I
must	O
grow	O
with	O
log	O
(	O
d	O
)	O
.	O
•	O
conclude	O
that	O
if	O
we	O
take	O
d	O
to	O
inﬁnity	O
we	O
obtain	O
a	O
problem	O
that	O
is	O
learnable	O
but	O
for	O
which	O
the	O
uniform	B
convergence	I
property	O
does	O
not	O
hold	O
.	O
compare	O
to	O
the	O
fundamental	O
theorem	O
of	O
statistical	O
learning	O
.	O
3.	O
stability	B
and	O
asymptotic	O
erm	O
are	O
suﬃcient	O
for	O
learnability	O
:	O
we	O
say	O
that	O
a	O
learning	O
rule	O
a	O
is	O
an	O
aerm	O
(	O
asymptotic	O
empirical	B
risk	I
minimizer	O
)	O
with	O
rate	O
	O
(	O
m	O
)	O
if	O
for	O
every	O
distribution	O
d	O
it	O
holds	O
that	O
e	O
s∼dm	O
ls	O
(	O
a	O
(	O
s	O
)	O
)	O
−	O
min	O
h∈h	O
ls	O
(	O
h	O
)	O
≤	O
	O
(	O
m	O
)	O
.	O
we	O
say	O
that	O
a	O
learning	O
rule	O
a	O
learns	O
a	O
class	O
h	O
with	O
rate	O
	O
(	O
m	O
)	O
if	O
for	O
every	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
(	O
cid:20	O
)	O
distribution	O
d	O
it	O
holds	O
that	O
e	O
s∼dm	O
prove	O
the	O
following	O
:	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
−	O
min	O
h∈h	O
ld	O
(	O
h	O
)	O
≤	O
	O
(	O
m	O
)	O
.	O
theorem	O
13.12	O
if	O
a	O
learning	O
algorithm	O
a	O
is	O
on-average-replace-one-stable	O
with	O
rate	O
1	O
(	O
m	O
)	O
and	O
is	O
an	O
aerm	O
with	O
rate	O
2	O
(	O
m	O
)	O
,	O
then	O
it	O
learns	O
h	O
with	O
rate	O
1	O
(	O
m	O
)	O
+	O
2	O
(	O
m	O
)	O
.	O
13.7	O
exercises	O
183	O
4.	O
strong	O
convexity	O
with	O
respect	O
to	O
general	O
norms	O
:	O
throughout	O
the	O
section	O
we	O
used	O
the	O
(	O
cid:96	O
)	O
2	O
norm	O
.	O
in	O
this	O
exercise	O
we	O
generalize	O
some	O
of	O
the	O
results	O
to	O
general	O
norms	O
.	O
let	O
(	O
cid:107	O
)	O
·	O
(	O
cid:107	O
)	O
be	O
some	O
arbitrary	O
norm	O
,	O
and	O
let	O
f	O
be	O
a	O
strongly	B
convex	I
function	O
with	O
respect	O
to	O
this	O
norm	O
(	O
see	O
deﬁnition	O
13.4	O
)	O
.	O
1.	O
show	O
that	O
items	O
2–3	O
of	O
lemma	O
13.5	O
hold	O
for	O
every	O
norm	O
.	O
2	O
.	O
(	O
*	O
)	O
give	O
an	O
example	O
of	O
a	O
norm	O
for	O
which	O
item	O
1	O
of	O
lemma	O
13.5	O
does	O
not	O
3.	O
let	O
r	O
(	O
w	O
)	O
be	O
a	O
function	B
that	O
is	O
(	O
2λ	O
)	O
-strongly	O
convex	B
with	O
respect	O
to	O
some	O
hold	O
.	O
norm	O
(	O
cid:107	O
)	O
·	O
(	O
cid:107	O
)	O
.	O
let	O
a	O
be	O
an	O
rlm	O
rule	O
with	O
respect	O
to	O
r	O
,	O
namely	O
,	O
a	O
(	O
s	O
)	O
=	O
argmin	O
(	O
ls	O
(	O
w	O
)	O
+	O
r	O
(	O
w	O
)	O
)	O
.	O
w	O
assume	O
that	O
for	O
every	O
z	O
,	O
the	O
loss	B
function	I
(	O
cid:96	O
)	O
(	O
·	O
,	O
z	O
)	O
is	O
ρ-lipschitz	O
with	O
respect	O
to	O
the	O
same	O
norm	O
,	O
namely	O
,	O
∀z	O
,	O
∀w	O
,	O
v	O
,	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
z	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
v	O
,	O
z	O
)	O
≤	O
ρ	O
(	O
cid:107	O
)	O
w	O
−	O
v	O
(	O
cid:107	O
)	O
.	O
prove	O
that	O
a	O
is	O
on-average-replace-one-stable	O
with	O
rate	O
2ρ2	O
λm	O
.	O
4	O
.	O
(	O
*	O
)	O
let	O
q	O
∈	O
(	O
1	O
,	O
2	O
)	O
and	O
consider	O
the	O
(	O
cid:96	O
)	O
q-norm	O
(	O
cid:32	O
)	O
d	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
1/q	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
q	O
=	O
|wi|q	O
.	O
it	O
can	O
be	O
shown	O
(	O
see	O
,	O
for	O
example	O
,	O
shalev-shwartz	O
(	O
2007	O
)	O
)	O
that	O
the	O
function	B
i=1	O
r	O
(	O
w	O
)	O
=	O
1	O
2	O
(	O
q	O
−	O
1	O
)	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
q	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
is	O
1-strongly	O
convex	B
with	O
respect	O
to	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
q.	O
show	O
that	O
if	O
q	O
=	O
log	O
(	O
d	O
)	O
r	O
(	O
w	O
)	O
is	O
-strongly	O
convex	B
with	O
respect	O
to	O
the	O
(	O
cid:96	O
)	O
1	O
norm	O
over	O
rd	O
.	O
log	O
(	O
d	O
)	O
−1	O
then	O
1	O
3	O
log	O
(	O
d	O
)	O
14	O
stochastic	O
gradient	B
descent	I
recall	O
that	O
the	O
goal	O
of	O
learning	O
is	O
to	O
minimize	O
the	O
risk	B
function	O
,	O
ld	O
(	O
h	O
)	O
=	O
ez∼d	O
[	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
z	O
)	O
]	O
.	O
we	O
can	O
not	O
directly	O
minimize	O
the	O
risk	B
function	O
since	O
it	O
depends	O
on	O
the	O
unknown	O
distribution	O
d.	O
so	O
far	O
in	O
the	O
book	O
,	O
we	O
have	O
discussed	O
learning	O
methods	O
that	O
depend	O
on	O
the	O
empirical	B
risk	I
.	O
that	O
is	O
,	O
we	O
ﬁrst	O
sample	O
a	O
training	B
set	I
s	O
and	O
deﬁne	O
the	O
empirical	B
risk	I
function	O
ls	O
(	O
h	O
)	O
.	O
then	O
,	O
the	O
learner	O
picks	O
a	O
hypothesis	B
based	O
on	O
the	O
value	O
of	O
ls	O
(	O
h	O
)	O
.	O
for	O
example	O
,	O
the	O
erm	O
rule	O
tells	O
us	O
to	O
pick	O
the	O
hypothesis	B
that	O
minimizes	O
ls	O
(	O
h	O
)	O
over	O
the	O
hypothesis	B
class	I
,	O
h.	O
or	O
,	O
in	O
the	O
previous	O
chapter	O
,	O
we	O
discussed	O
regularized	O
risk	O
minimization	O
,	O
in	O
which	O
we	O
pick	O
a	O
hypothesis	B
that	O
jointly	O
minimizes	O
ls	O
(	O
h	O
)	O
and	O
a	O
regularization	B
function	O
over	O
h.	O
in	O
this	O
chapter	O
we	O
describe	O
and	O
analyze	O
a	O
rather	O
diﬀerent	O
learning	O
approach	O
,	O
which	O
is	O
called	O
stochastic	O
gradient	B
descent	I
(	O
sgd	O
)	O
.	O
as	O
in	O
chapter	O
12	O
we	O
will	O
focus	O
on	O
the	O
important	O
family	O
of	O
convex	B
learning	O
problems	O
,	O
and	O
following	O
the	O
notation	O
in	O
that	O
chapter	O
,	O
we	O
will	O
refer	O
to	O
hypotheses	O
as	O
vectors	O
w	O
that	O
come	O
from	O
a	O
convex	B
hypothesis	O
class	O
,	O
h.	O
in	O
sgd	O
,	O
we	O
try	O
to	O
minimize	O
the	O
risk	B
function	O
ld	O
(	O
w	O
)	O
directly	O
using	O
a	O
gradient	B
descent	I
procedure	O
.	O
gradient	B
descent	I
is	O
an	O
iterative	O
optimization	O
procedure	O
in	O
which	O
at	O
each	O
step	O
we	O
improve	O
the	O
solution	O
by	O
taking	O
a	O
step	O
along	O
the	O
negative	O
of	O
the	O
gradient	B
of	O
the	O
function	B
to	O
be	O
minimized	O
at	O
the	O
current	O
point	O
.	O
of	O
course	O
,	O
in	O
our	O
case	O
,	O
we	O
are	O
minimizing	O
the	O
risk	B
function	O
,	O
and	O
since	O
we	O
do	O
not	O
know	O
d	O
we	O
also	O
do	O
not	O
know	O
the	O
gradient	B
of	O
ld	O
(	O
w	O
)	O
.	O
sgd	O
circumvents	O
this	O
problem	O
by	O
allowing	O
the	O
optimization	O
procedure	O
to	O
take	O
a	O
step	O
along	O
a	O
random	O
direction	O
,	O
as	O
long	O
as	O
the	O
expected	O
value	O
of	O
the	O
direction	O
is	O
the	O
negative	O
of	O
the	O
gradient	B
.	O
and	O
,	O
as	O
we	O
shall	O
see	O
,	O
ﬁnding	O
a	O
random	O
direction	O
whose	O
expected	O
value	O
corresponds	O
to	O
the	O
gradient	B
is	O
rather	O
simple	O
even	O
though	O
we	O
do	O
not	O
know	O
the	O
underlying	O
distribution	O
d.	O
the	O
advantage	O
of	O
sgd	O
,	O
in	O
the	O
context	O
of	O
convex	B
learning	O
problems	O
,	O
over	O
the	O
regularized	O
risk	O
minimization	O
learning	O
rule	O
is	O
that	O
sgd	O
is	O
an	O
eﬃcient	O
algorithm	O
that	O
can	O
be	O
implemented	O
in	O
a	O
few	O
lines	O
of	O
code	O
,	O
yet	O
still	O
enjoys	O
the	O
same	O
sample	B
complexity	I
as	O
the	O
regularized	O
risk	O
minimization	O
rule	O
.	O
the	O
simplicity	O
of	O
sgd	O
also	O
allows	O
us	O
to	O
use	O
it	O
in	O
situations	O
when	O
it	O
is	O
not	O
possible	O
to	O
apply	O
methods	O
that	O
are	O
based	O
on	O
the	O
empirical	B
risk	I
,	O
but	O
this	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
.	O
we	O
start	O
this	O
chapter	O
with	O
the	O
basic	O
gradient	B
descent	I
algorithm	O
and	O
analyze	O
its	O
convergence	O
rate	O
for	O
convex-lipschitz	O
functions	O
.	O
next	O
,	O
we	O
introduce	O
the	O
notion	O
of	O
subgradient	O
and	O
show	O
that	O
gradient	B
descent	I
can	O
be	O
applied	O
for	O
nondiﬀerentiable	O
functions	O
as	O
well	O
.	O
the	O
core	O
of	O
this	O
chapter	O
is	O
section	O
14.3	O
,	O
in	O
which	O
we	O
describe	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
14.1	O
gradient	B
descent	I
185	O
the	O
stochastic	O
gradient	B
descent	I
algorithm	O
,	O
along	O
with	O
several	O
useful	O
variants	O
.	O
we	O
show	O
that	O
sgd	O
enjoys	O
an	O
expected	O
convergence	O
rate	O
similar	O
to	O
the	O
rate	O
of	O
gradient	B
descent	I
.	O
finally	O
,	O
we	O
turn	O
to	O
the	O
applicability	O
of	O
sgd	O
to	O
learning	O
problems	O
.	O
14.1	O
gradient	B
descent	I
before	O
we	O
describe	O
the	O
stochastic	O
gradient	B
descent	I
method	O
,	O
we	O
would	O
like	O
to	O
describe	O
the	O
standard	O
gradient	B
descent	I
approach	O
for	O
minimizing	O
a	O
diﬀerentiable	O
convex	B
function	O
f	O
(	O
w	O
)	O
.	O
the	O
gradient	B
of	O
a	O
diﬀerentiable	O
function	B
f	O
:	O
rd	O
→	O
r	O
at	O
w	O
,	O
denoted	O
∇f	O
(	O
w	O
)	O
,	O
is	O
the	O
vector	O
of	O
partial	O
derivatives	O
of	O
f	O
,	O
namely	O
,	O
∇f	O
(	O
w	O
)	O
=	O
.	O
gradient	B
descent	I
is	O
an	O
iterative	O
algorithm	O
.	O
we	O
start	O
with	O
an	O
initial	O
value	O
of	O
w	O
(	O
say	O
,	O
w	O
(	O
1	O
)	O
=	O
0	O
)	O
.	O
then	O
,	O
at	O
each	O
iteration	O
,	O
we	O
take	O
a	O
step	O
in	O
the	O
direction	O
of	O
the	O
negative	O
of	O
the	O
gradient	B
at	O
the	O
current	O
point	O
.	O
that	O
is	O
,	O
the	O
update	O
step	O
is	O
(	O
cid:16	O
)	O
∂f	O
(	O
w	O
)	O
∂w	O
[	O
1	O
]	O
,	O
.	O
.	O
.	O
,	O
∂f	O
(	O
w	O
)	O
∂w	O
[	O
d	O
]	O
(	O
cid:17	O
)	O
w	O
(	O
t+1	O
)	O
=	O
w	O
(	O
t	O
)	O
−	O
η∇f	O
(	O
w	O
(	O
t	O
)	O
)	O
,	O
(	O
14.1	O
)	O
where	O
η	O
>	O
0	O
is	O
a	O
parameter	O
to	O
be	O
discussed	O
later	O
.	O
intuitively	O
,	O
since	O
the	O
gradi-	O
ent	O
points	O
in	O
the	O
direction	O
of	O
the	O
greatest	O
rate	O
of	O
increase	O
of	O
f	O
around	O
w	O
(	O
t	O
)	O
,	O
the	O
algorithm	O
makes	O
a	O
small	O
step	O
in	O
the	O
opposite	O
direction	O
,	O
thus	O
decreasing	O
the	O
value	O
of	O
the	O
function	B
.	O
eventually	O
,	O
after	O
t	O
iterations	O
,	O
the	O
algorithm	O
outputs	O
the	O
averaged	O
vector	O
,	O
¯w	O
=	O
1	O
t=1	O
w	O
(	O
t	O
)	O
.	O
the	O
output	O
could	O
also	O
be	O
the	O
last	O
vector	O
,	O
t	O
w	O
(	O
t	O
)	O
,	O
or	O
the	O
best	O
performing	O
vector	O
,	O
argmint∈	O
[	O
t	O
]	O
f	O
(	O
w	O
(	O
t	O
)	O
)	O
,	O
but	O
taking	O
the	O
average	O
turns	O
out	O
to	O
be	O
rather	O
useful	O
,	O
especially	O
when	O
we	O
generalize	O
gradient	B
descent	I
to	O
nondiﬀerentiable	O
functions	O
and	O
to	O
the	O
stochastic	O
case	O
.	O
(	O
cid:80	O
)	O
t	O
another	O
way	O
to	O
motivate	O
gradient	B
descent	I
is	O
by	O
relying	O
on	O
taylor	O
approxima-	O
tion	O
.	O
the	O
gradient	B
of	O
f	O
at	O
w	O
yields	O
the	O
ﬁrst	O
order	O
taylor	O
approximation	O
of	O
f	O
around	O
w	O
by	O
f	O
(	O
u	O
)	O
≈	O
f	O
(	O
w	O
)	O
+	O
(	O
cid:104	O
)	O
u	O
−	O
w	O
,	O
∇f	O
(	O
w	O
)	O
(	O
cid:105	O
)	O
.	O
when	O
f	O
is	O
convex	B
,	O
this	O
approxi-	O
mation	O
lower	O
bounds	O
f	O
,	O
that	O
is	O
,	O
f	O
(	O
u	O
)	O
≥	O
f	O
(	O
w	O
)	O
+	O
(	O
cid:104	O
)	O
u	O
−	O
w	O
,	O
∇f	O
(	O
w	O
)	O
(	O
cid:105	O
)	O
.	O
therefore	O
,	O
for	O
w	O
close	O
to	O
w	O
(	O
t	O
)	O
we	O
have	O
that	O
f	O
(	O
w	O
)	O
≈	O
f	O
(	O
w	O
(	O
t	O
)	O
)	O
+	O
(	O
cid:104	O
)	O
w−w	O
(	O
t	O
)	O
,	O
∇f	O
(	O
w	O
(	O
t	O
)	O
)	O
(	O
cid:105	O
)	O
.	O
hence	O
we	O
can	O
minimize	O
the	O
approximation	O
of	O
f	O
(	O
w	O
)	O
.	O
however	O
,	O
the	O
approximation	O
might	O
become	O
loose	O
for	O
w	O
,	O
which	O
is	O
far	O
away	O
from	O
w	O
(	O
t	O
)	O
.	O
therefore	O
,	O
we	O
would	O
like	O
to	O
minimize	O
jointly	O
the	O
distance	O
between	O
w	O
and	O
w	O
(	O
t	O
)	O
and	O
the	O
approximation	O
of	O
f	O
around	O
w	O
(	O
t	O
)	O
.	O
if	O
the	O
parameter	O
η	O
controls	O
the	O
tradeoﬀ	O
between	O
the	O
two	O
terms	O
,	O
we	O
obtain	O
the	O
update	O
rule	O
w	O
(	O
t+1	O
)	O
=	O
argmin	O
w	O
1	O
2	O
(	O
cid:107	O
)	O
w	O
−	O
w	O
(	O
t	O
)	O
(	O
cid:107	O
)	O
2	O
+	O
η	O
(	O
cid:16	O
)	O
f	O
(	O
w	O
(	O
t	O
)	O
)	O
+	O
(	O
cid:104	O
)	O
w	O
−	O
w	O
(	O
t	O
)	O
,	O
∇f	O
(	O
w	O
(	O
t	O
)	O
)	O
(	O
cid:105	O
)	O
(	O
cid:17	O
)	O
.	O
solving	O
the	O
preceding	O
by	O
taking	O
the	O
derivative	O
with	O
respect	O
to	O
w	O
and	O
comparing	O
it	O
to	O
zero	O
yields	O
the	O
same	O
update	O
rule	O
as	O
in	O
equation	O
(	O
14.1	O
)	O
.	O
186	O
stochastic	O
gradient	B
descent	I
figure	O
14.1	O
an	O
illustration	O
of	O
the	O
gradient	B
descent	I
algorithm	O
.	O
the	O
function	B
to	O
be	O
minimized	O
is	O
1.25	O
(	O
x1	O
+	O
6	O
)	O
2	O
+	O
(	O
x2	O
−	O
8	O
)	O
2	O
.	O
14.1.1	O
analysis	O
of	O
gd	O
for	O
convex-lipschitz	O
functions	O
to	O
analyze	O
the	O
convergence	O
rate	O
of	O
the	O
gd	O
algorithm	O
,	O
we	O
limit	O
ourselves	O
to	O
the	O
case	O
of	O
convex-lipschitz	O
functions	O
(	O
as	O
we	O
have	O
seen	O
,	O
many	O
problems	O
lend	O
themselves	O
easily	O
to	O
this	O
setting	O
)	O
.	O
let	O
w	O
(	O
cid:63	O
)	O
be	O
any	O
vector	O
and	O
let	O
b	O
be	O
an	O
upper	O
bound	O
on	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
.	O
it	O
is	O
convenient	O
to	O
think	O
of	O
w	O
(	O
cid:63	O
)	O
as	O
the	O
minimizer	O
of	O
f	O
(	O
w	O
)	O
,	O
but	O
the	O
analysis	O
that	O
follows	O
holds	O
for	O
every	O
w	O
(	O
cid:63	O
)	O
.	O
we	O
would	O
like	O
to	O
obtain	O
an	O
upper	O
bound	O
on	O
the	O
suboptimality	O
of	O
our	O
solution	O
t=1	O
w	O
(	O
t	O
)	O
.	O
from	O
the	O
with	O
respect	O
to	O
w	O
(	O
cid:63	O
)	O
,	O
namely	O
,	O
f	O
(	O
¯w	O
)	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
,	O
where	O
¯w	O
=	O
1	O
deﬁnition	O
of	O
¯w	O
,	O
and	O
using	O
jensen	O
’	O
s	O
inequality	O
,	O
we	O
have	O
that	O
t	O
(	O
cid:80	O
)	O
t	O
(	O
cid:33	O
)	O
f	O
(	O
w	O
(	O
t	O
)	O
)	O
1	O
t	O
(	O
cid:32	O
)	O
t	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
t	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
t	O
(	O
cid:88	O
)	O
t=1	O
t=1	O
t=1	O
≤	O
1	O
t	O
=	O
1	O
t	O
f	O
(	O
¯w	O
)	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
=	O
f	O
w	O
(	O
t	O
)	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
(	O
cid:17	O
)	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
(	O
cid:17	O
)	O
f	O
(	O
w	O
(	O
t	O
)	O
)	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
.	O
(	O
14.2	O
)	O
(	O
14.3	O
)	O
for	O
every	O
t	O
,	O
because	O
of	O
the	O
convexity	O
of	O
f	O
,	O
we	O
have	O
that	O
f	O
(	O
w	O
(	O
t	O
)	O
)	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
≤	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
∇f	O
(	O
w	O
(	O
t	O
)	O
)	O
(	O
cid:105	O
)	O
.	O
combining	O
the	O
preceding	O
we	O
obtain	O
f	O
(	O
¯w	O
)	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
≤	O
1	O
t	O
t	O
(	O
cid:88	O
)	O
t=1	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
∇f	O
(	O
w	O
(	O
t	O
)	O
)	O
(	O
cid:105	O
)	O
.	O
to	O
bound	O
the	O
right-hand	O
side	O
we	O
rely	O
on	O
the	O
following	O
lemma	O
:	O
14.1	O
gradient	B
descent	I
187	O
lemma	O
14.1	O
let	O
v1	O
,	O
.	O
.	O
.	O
,	O
vt	O
be	O
an	O
arbitrary	O
sequence	O
of	O
vectors	O
.	O
any	O
algorithm	O
with	O
an	O
initialization	O
w	O
(	O
1	O
)	O
=	O
0	O
and	O
an	O
update	O
rule	O
of	O
the	O
form	O
satisﬁes	O
w	O
(	O
t+1	O
)	O
=	O
w	O
(	O
t	O
)	O
−	O
ηvt	O
t	O
(	O
cid:88	O
)	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
≤	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
+	O
η	O
2	O
2η	O
(	O
14.4	O
)	O
(	O
14.5	O
)	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
2.	O
t	O
(	O
cid:88	O
)	O
t=1	O
(	O
cid:113	O
)	O
b2	O
ρ2	O
t	O
,	O
then	O
for	O
every	O
w	O
(	O
cid:63	O
)	O
with	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
≤	O
b	O
we	O
have	O
in	O
particular	O
,	O
for	O
every	O
b	O
,	O
ρ	O
>	O
0	O
,	O
if	O
for	O
all	O
t	O
we	O
have	O
that	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
≤	O
ρ	O
and	O
if	O
we	O
set	B
η	O
=	O
t=1	O
t	O
(	O
cid:88	O
)	O
t=1	O
1	O
t	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
≤	O
b	O
ρ√	O
t	O
.	O
proof	O
using	O
algebraic	O
manipulations	O
(	O
completing	O
the	O
square	O
)	O
,	O
we	O
obtain	O
:	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
=	O
=	O
=	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
ηvt	O
(	O
cid:105	O
)	O
1	O
η	O
(	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
−	O
ηvt	O
(	O
cid:107	O
)	O
2	O
+	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
+	O
η2	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
2	O
)	O
1	O
2η	O
(	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
t+1	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
+	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
)	O
+	O
1	O
2η	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
2	O
,	O
η	O
2	O
t	O
(	O
cid:88	O
)	O
t=1	O
where	O
the	O
last	O
equality	O
follows	O
from	O
the	O
deﬁnition	O
of	O
the	O
update	O
rule	O
.	O
summing	O
the	O
equality	O
over	O
t	O
,	O
we	O
have	O
(	O
cid:16	O
)	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
t+1	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
+	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
(	O
cid:17	O
)	O
t	O
(	O
cid:88	O
)	O
t=1	O
t	O
(	O
cid:88	O
)	O
t=1	O
+	O
η	O
2	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
2	O
.	O
(	O
14.6	O
)	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−w	O
(	O
cid:63	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
=	O
1	O
2η	O
the	O
ﬁrst	O
sum	O
on	O
the	O
right-hand	O
side	O
is	O
a	O
telescopic	O
sum	O
that	O
collapses	O
to	O
(	O
cid:107	O
)	O
w	O
(	O
1	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
+1	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2.	O
plugging	O
this	O
in	O
equation	O
(	O
14.6	O
)	O
,	O
we	O
have	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
=	O
(	O
(	O
cid:107	O
)	O
w	O
(	O
1	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
+1	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
)	O
+	O
t	O
(	O
cid:88	O
)	O
t=1	O
1	O
2η	O
≤	O
1	O
2η	O
=	O
1	O
2η	O
t	O
(	O
cid:88	O
)	O
t=1	O
η	O
2	O
(	O
cid:107	O
)	O
w	O
(	O
1	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
+	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
2	O
t	O
(	O
cid:88	O
)	O
t=1	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
+	O
η	O
2	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
2	O
,	O
t	O
(	O
cid:88	O
)	O
t=1	O
η	O
2	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
2	O
where	O
the	O
last	O
equality	O
is	O
due	O
to	O
the	O
deﬁnition	O
w	O
(	O
1	O
)	O
=	O
0.	O
this	O
proves	O
the	O
ﬁrst	O
part	O
of	O
the	O
lemma	O
(	O
equation	O
(	O
14.5	O
)	O
)	O
.	O
the	O
second	O
part	O
follows	O
by	O
upper	O
bounding	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
by	O
b	O
,	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
by	O
ρ	O
,	O
dividing	O
by	O
t	O
,	O
and	O
plugging	O
in	O
the	O
value	O
of	O
η	O
.	O
188	O
stochastic	O
gradient	B
descent	I
lemma	O
14.1	O
applies	O
to	O
the	O
gd	O
algorithm	O
with	O
vt	O
=	O
∇f	O
(	O
w	O
(	O
t	O
)	O
)	O
.	O
as	O
we	O
will	O
show	O
later	O
in	O
lemma	O
14.7	O
,	O
if	O
f	O
is	O
ρ-lipschitz	O
,	O
then	O
(	O
cid:107	O
)	O
∇f	O
(	O
w	O
(	O
t	O
)	O
)	O
(	O
cid:107	O
)	O
≤	O
ρ.	O
we	O
therefore	O
satisfy	O
the	O
lemma	O
’	O
s	O
conditions	O
and	O
achieve	O
the	O
following	O
corollary	O
:	O
corollary	O
14.2	O
let	O
f	O
be	O
a	O
convex	B
,	O
ρ-lipschitz	O
function	B
,	O
and	O
let	O
w	O
(	O
cid:63	O
)	O
∈	O
argmin	O
{	O
w	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
≤b	O
}	O
f	O
(	O
w	O
)	O
.	O
if	O
we	O
run	O
the	O
gd	O
algorithm	O
on	O
f	O
for	O
t	O
steps	O
with	O
η	O
=	O
vector	O
¯w	O
satisﬁes	O
ρ2	O
t	O
,	O
then	O
the	O
output	O
(	O
cid:113	O
)	O
b2	O
f	O
(	O
¯w	O
)	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
≤	O
b	O
ρ√	O
t	O
.	O
furthermore	O
,	O
for	O
every	O
	O
>	O
0	O
,	O
to	O
achieve	O
f	O
(	O
¯w	O
)	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
≤	O
	O
,	O
it	O
suﬃces	O
to	O
run	O
the	O
gd	O
algorithm	O
for	O
a	O
number	O
of	O
iterations	O
that	O
satisﬁes	O
t	O
≥	O
b2ρ2	O
2	O
.	O
14.2	O
subgradients	O
the	O
gd	O
algorithm	O
requires	O
that	O
the	O
function	B
f	O
be	O
diﬀerentiable	O
.	O
we	O
now	O
gener-	O
alize	O
the	O
discussion	O
beyond	O
diﬀerentiable	O
functions	O
.	O
we	O
will	O
show	O
that	O
the	O
gd	O
algorithm	O
can	O
be	O
applied	O
to	O
nondiﬀerentiable	O
functions	O
by	O
using	O
a	O
so-called	O
sub-	O
gradient	B
of	O
f	O
(	O
w	O
)	O
at	O
w	O
(	O
t	O
)	O
,	O
instead	O
of	O
the	O
gradient	B
.	O
to	O
motivate	O
the	O
deﬁnition	O
of	O
subgradients	O
,	O
recall	B
that	O
for	O
a	O
convex	B
function	O
f	O
,	O
the	O
gradient	B
at	O
w	O
deﬁnes	O
the	O
slope	O
of	O
a	O
tangent	O
that	O
lies	O
below	O
f	O
,	O
that	O
is	O
,	O
∀u	O
,	O
f	O
(	O
u	O
)	O
≥	O
f	O
(	O
w	O
)	O
+	O
(	O
cid:104	O
)	O
u	O
−	O
w	O
,	O
∇f	O
(	O
w	O
)	O
(	O
cid:105	O
)	O
.	O
(	O
14.7	O
)	O
an	O
illustration	O
is	O
given	O
on	O
the	O
left-hand	O
side	O
of	O
figure	O
14.2.	O
the	O
existence	O
of	O
a	O
tangent	O
that	O
lies	O
below	O
f	O
is	O
an	O
important	O
property	O
of	O
convex	B
functions	O
,	O
which	O
is	O
in	O
fact	O
an	O
alternative	O
characterization	O
of	O
convexity	O
.	O
lemma	O
14.3	O
let	O
s	O
be	O
an	O
open	O
convex	B
set	O
.	O
a	O
function	B
f	O
:	O
s	O
→	O
r	O
is	O
convex	B
iﬀ	O
for	O
every	O
w	O
∈	O
s	O
there	O
exists	O
v	O
such	O
that	O
∀u	O
∈	O
s	O
,	O
f	O
(	O
u	O
)	O
≥	O
f	O
(	O
w	O
)	O
+	O
(	O
cid:104	O
)	O
u	O
−	O
w	O
,	O
v	O
(	O
cid:105	O
)	O
.	O
(	O
14.8	O
)	O
the	O
proof	O
of	O
this	O
lemma	O
can	O
be	O
found	O
in	O
many	O
convex	B
analysis	O
textbooks	O
(	O
e.g.	O
,	O
(	O
borwein	O
&	O
lewis	O
2006	O
)	O
)	O
.	O
the	O
preceding	O
inequality	O
leads	O
us	O
to	O
the	O
deﬁnition	O
of	O
subgradients	O
.	O
definition	O
14.4	O
(	O
subgradients	O
)	O
a	O
vector	O
v	O
that	O
satisﬁes	O
equation	O
(	O
14.8	O
)	O
is	O
called	O
a	O
subgradient	O
of	O
f	O
at	O
w.	O
the	O
set	B
of	O
subgradients	O
of	O
f	O
at	O
w	O
is	O
called	O
the	O
diﬀerential	B
set	I
and	O
denoted	O
∂f	O
(	O
w	O
)	O
.	O
an	O
illustration	O
of	O
subgradients	O
is	O
given	O
on	O
the	O
right-hand	O
side	O
of	O
figure	O
14.2.	O
for	O
scalar	O
functions	O
,	O
a	O
subgradient	O
of	O
a	O
convex	B
function	O
f	O
at	O
w	O
is	O
a	O
slope	O
of	O
a	O
line	O
that	O
touches	O
f	O
at	O
w	O
and	O
is	O
not	O
above	O
f	O
elsewhere	O
.	O
14.2	O
subgradients	O
189	O
f	O
(	O
u	O
)	O
−	O
w	O
,	O
∇	O
f	O
(	O
w	O
)	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
u	O
f	O
(	O
w	O
)	O
+	O
f	O
(	O
w	O
)	O
w	O
u	O
figure	O
14.2	O
left	O
:	O
the	O
right-hand	O
side	O
of	O
equation	O
(	O
14.7	O
)	O
is	O
the	O
tangent	O
of	O
f	O
at	O
w.	O
for	O
a	O
convex	B
function	O
,	O
the	O
tangent	O
lower	O
bounds	O
f	O
.	O
right	O
:	O
illustration	O
of	O
several	O
subgradients	O
of	O
a	O
nondiﬀerentiable	O
convex	B
function	O
.	O
14.2.1	O
calculating	O
subgradients	O
how	O
do	O
we	O
construct	O
subgradients	O
of	O
a	O
given	O
convex	B
function	O
?	O
if	O
a	O
function	B
is	O
diﬀerentiable	O
at	O
a	O
point	O
w	O
,	O
then	O
the	O
diﬀerential	B
set	I
is	O
trivial	O
,	O
as	O
the	O
following	O
claim	O
shows	O
.	O
claim	O
14.5	O
the	O
gradient	B
of	O
f	O
at	O
w	O
,	O
∇f	O
(	O
w	O
)	O
.	O
if	O
f	O
is	O
diﬀerentiable	O
at	O
w	O
then	O
∂f	O
(	O
w	O
)	O
contains	O
a	O
single	O
element	O
–	O
example	O
14.1	O
(	O
the	O
diﬀerential	B
set	I
of	O
the	O
absolute	O
function	O
)	O
consider	O
the	O
absolute	O
value	O
function	O
f	O
(	O
x	O
)	O
=	O
|x|	O
.	O
using	O
claim	O
14.5	O
,	O
we	O
can	O
easily	O
construct	O
the	O
diﬀerential	B
set	I
for	O
the	O
diﬀerentiable	O
parts	O
of	O
f	O
,	O
and	O
the	O
only	O
point	O
that	O
requires	O
special	O
attention	O
is	O
x0	O
=	O
0.	O
at	O
that	O
point	O
,	O
it	O
is	O
easy	O
to	O
verify	O
that	O
the	O
subdiﬀerential	O
is	O
the	O
set	B
of	O
all	O
numbers	O
between	O
−1	O
and	O
1.	O
hence	O
:	O
	O
∂f	O
(	O
x	O
)	O
=	O
{	O
1	O
}	O
{	O
−1	O
}	O
[	O
−1	O
,	O
1	O
]	O
if	O
x	O
>	O
0	O
if	O
x	O
<	O
0	O
if	O
x	O
=	O
0	O
for	O
many	O
practical	O
uses	O
,	O
we	O
do	O
not	O
need	O
to	O
calculate	O
the	O
whole	O
set	B
of	O
subgra-	O
dients	O
at	O
a	O
given	O
point	O
,	O
as	O
one	O
member	O
of	O
this	O
set	B
would	O
suﬃce	O
.	O
the	O
following	O
claim	O
shows	O
how	O
to	O
construct	O
a	O
sub-gradient	B
for	O
pointwise	O
maximum	O
functions	O
.	O
claim	O
14.6	O
let	O
g	O
(	O
w	O
)	O
=	O
maxi∈	O
[	O
r	O
]	O
gi	O
(	O
w	O
)	O
for	O
r	O
convex	B
diﬀerentiable	O
functions	O
g1	O
,	O
.	O
.	O
.	O
,	O
gr	O
.	O
given	O
some	O
w	O
,	O
let	O
j	O
∈	O
argmaxi	O
gi	O
(	O
w	O
)	O
.	O
then	O
∇gj	O
(	O
w	O
)	O
∈	O
∂g	O
(	O
w	O
)	O
.	O
proof	O
since	O
gj	O
is	O
convex	B
we	O
have	O
that	O
for	O
all	O
u	O
gj	O
(	O
u	O
)	O
≥	O
gj	O
(	O
w	O
)	O
+	O
(	O
cid:104	O
)	O
u	O
−	O
w	O
,	O
∇gj	O
(	O
w	O
)	O
(	O
cid:105	O
)	O
.	O
since	O
g	O
(	O
w	O
)	O
=	O
gj	O
(	O
w	O
)	O
and	O
g	O
(	O
u	O
)	O
≥	O
gj	O
(	O
u	O
)	O
we	O
obtain	O
that	O
g	O
(	O
u	O
)	O
≥	O
g	O
(	O
w	O
)	O
+	O
(	O
cid:104	O
)	O
u	O
−	O
w	O
,	O
∇gj	O
(	O
w	O
)	O
(	O
cid:105	O
)	O
,	O
which	O
concludes	O
our	O
proof	O
.	O
190	O
stochastic	O
gradient	B
descent	I
example	O
14.2	O
(	O
a	O
subgradient	O
of	O
the	O
hinge	B
loss	I
)	O
recall	B
the	O
hinge	B
loss	I
function	O
from	O
section	O
12.3	O
,	O
f	O
(	O
w	O
)	O
=	O
max	O
{	O
0	O
,	O
1	O
−	O
y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
}	O
for	O
some	O
vector	O
x	O
and	O
scalar	O
y.	O
to	O
calculate	O
a	O
subgradient	O
of	O
the	O
hinge	B
loss	I
at	O
some	O
w	O
we	O
rely	O
on	O
the	O
preceding	O
claim	O
and	O
obtain	O
that	O
the	O
vector	O
v	O
deﬁned	O
in	O
the	O
following	O
is	O
a	O
subgradient	O
of	O
the	O
hinge	B
loss	I
at	O
w	O
:	O
(	O
cid:40	O
)	O
v	O
=	O
if	O
1	O
−	O
y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
≤	O
0	O
0	O
−yx	O
if	O
1	O
−	O
y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
>	O
0	O
14.2.2	O
subgradients	O
of	O
lipschitz	O
functions	O
recall	B
that	O
a	O
function	B
f	O
:	O
a	O
→	O
r	O
is	O
ρ-lipschitz	O
if	O
for	O
all	O
u	O
,	O
v	O
∈	O
a	O
|f	O
(	O
u	O
)	O
−	O
f	O
(	O
v	O
)	O
|	O
≤	O
ρ	O
(	O
cid:107	O
)	O
u	O
−	O
v	O
(	O
cid:107	O
)	O
.	O
the	O
following	O
lemma	O
gives	O
an	O
equivalent	O
deﬁnition	O
using	O
norms	O
of	O
subgradients	O
.	O
lemma	O
14.7	O
let	O
a	O
be	O
a	O
convex	B
open	O
set	B
and	O
let	O
f	O
:	O
a	O
→	O
r	O
be	O
a	O
convex	B
function	O
.	O
then	O
,	O
f	O
is	O
ρ-lipschitz	O
over	O
a	O
iﬀ	O
for	O
all	O
w	O
∈	O
a	O
and	O
v	O
∈	O
∂f	O
(	O
w	O
)	O
we	O
have	O
that	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
≤	O
ρ.	O
proof	O
assume	O
that	O
for	O
all	O
v	O
∈	O
∂f	O
(	O
w	O
)	O
we	O
have	O
that	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
≤	O
ρ.	O
since	O
v	O
∈	O
∂f	O
(	O
w	O
)	O
we	O
have	O
f	O
(	O
w	O
)	O
−	O
f	O
(	O
u	O
)	O
≤	O
(	O
cid:104	O
)	O
v	O
,	O
w	O
−	O
u	O
(	O
cid:105	O
)	O
.	O
bounding	O
the	O
right-hand	O
side	O
using	O
cauchy-schwartz	O
inequality	O
we	O
obtain	O
f	O
(	O
w	O
)	O
−	O
f	O
(	O
u	O
)	O
≤	O
(	O
cid:104	O
)	O
v	O
,	O
w	O
−	O
u	O
(	O
cid:105	O
)	O
≤	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
(	O
cid:107	O
)	O
w	O
−	O
u	O
(	O
cid:107	O
)	O
≤	O
ρ	O
(	O
cid:107	O
)	O
w	O
−	O
u	O
(	O
cid:107	O
)	O
.	O
an	O
analogous	O
argument	O
can	O
show	O
that	O
f	O
(	O
u	O
)	O
−	O
f	O
(	O
w	O
)	O
≤	O
ρ	O
(	O
cid:107	O
)	O
w	O
−	O
u	O
(	O
cid:107	O
)	O
.	O
hence	O
f	O
is	O
ρ-lipschitz	O
.	O
now	O
assume	O
that	O
f	O
is	O
ρ-lipschitz	O
.	O
choose	O
some	O
w	O
∈	O
a	O
,	O
v	O
∈	O
∂f	O
(	O
w	O
)	O
.	O
since	O
a	O
is	O
open	O
,	O
there	O
exists	O
	O
>	O
0	O
such	O
that	O
u	O
=	O
w	O
+	O
v/	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
belongs	O
to	O
a.	O
therefore	O
,	O
(	O
cid:104	O
)	O
u	O
−	O
w	O
,	O
v	O
(	O
cid:105	O
)	O
=	O
	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
and	O
(	O
cid:107	O
)	O
u	O
−	O
w	O
(	O
cid:107	O
)	O
=	O
	O
.	O
from	O
the	O
deﬁnition	O
of	O
the	O
subgradient	O
,	O
f	O
(	O
u	O
)	O
−	O
f	O
(	O
w	O
)	O
≥	O
(	O
cid:104	O
)	O
v	O
,	O
u	O
−	O
w	O
(	O
cid:105	O
)	O
=	O
	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
.	O
on	O
the	O
other	O
hand	O
,	O
from	O
the	O
lipschitzness	O
of	O
f	O
we	O
have	O
ρ	O
	O
=	O
ρ	O
(	O
cid:107	O
)	O
u	O
−	O
w	O
(	O
cid:107	O
)	O
≥	O
f	O
(	O
u	O
)	O
−	O
f	O
(	O
w	O
)	O
.	O
combining	O
the	O
two	O
inequalities	O
we	O
conclude	O
that	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
≤	O
ρ	O
.	O
14.2.3	O
subgradient	O
descent	O
the	O
gradient	B
descent	I
algorithm	O
can	O
be	O
generalized	O
to	O
nondiﬀerentiable	O
functions	O
by	O
using	O
a	O
subgradient	O
of	O
f	O
(	O
w	O
)	O
at	O
w	O
(	O
t	O
)	O
,	O
instead	O
of	O
the	O
gradient	B
.	O
the	O
analysis	O
of	O
the	O
convergence	O
rate	O
remains	O
unchanged	O
:	O
simply	O
note	O
that	O
equation	O
(	O
14.3	O
)	O
is	O
true	O
for	O
subgradients	O
as	O
well	O
.	O
14.3	O
stochastic	O
gradient	B
descent	I
(	O
sgd	O
)	O
191	O
figure	O
14.3	O
an	O
illustration	O
of	O
the	O
gradient	B
descent	I
algorithm	O
(	O
left	O
)	O
and	O
the	O
stochastic	O
gradient	B
descent	I
algorithm	O
(	O
right	O
)	O
.	O
the	O
function	B
to	O
be	O
minimized	O
is	O
1.25	O
(	O
x	O
+	O
6	O
)	O
2	O
+	O
(	O
y	O
−	O
8	O
)	O
2.	O
for	O
the	O
stochastic	O
case	O
,	O
the	O
black	O
line	O
depicts	O
the	O
averaged	O
value	O
of	O
w.	O
14.3	O
stochastic	O
gradient	B
descent	I
(	O
sgd	O
)	O
in	O
stochastic	O
gradient	B
descent	I
we	O
do	O
not	O
require	O
the	O
update	O
direction	O
to	O
be	O
based	O
exactly	O
on	O
the	O
gradient	B
.	O
instead	O
,	O
we	O
allow	O
the	O
direction	O
to	O
be	O
a	O
random	O
vector	O
and	O
only	O
require	O
that	O
its	O
expected	O
value	O
at	O
each	O
iteration	O
will	O
equal	O
the	O
gradient	B
direction	O
.	O
or	O
,	O
more	O
generally	O
,	O
we	O
require	O
that	O
the	O
expected	O
value	O
of	O
the	O
random	O
vector	O
will	O
be	O
a	O
subgradient	O
of	O
the	O
function	B
at	O
the	O
current	O
vector	O
.	O
stochastic	O
gradient	B
descent	I
(	O
sgd	O
)	O
for	O
minimizing	O
f	O
(	O
w	O
)	O
parameters	O
:	O
scalar	O
η	O
>	O
0	O
,	O
integer	O
t	O
>	O
0	O
initialize	O
:	O
w	O
(	O
1	O
)	O
=	O
0	O
for	O
t	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
t	O
choose	O
vt	O
at	O
random	O
from	O
a	O
distribution	O
such	O
that	O
e	O
[	O
vt	O
|	O
w	O
(	O
t	O
)	O
]	O
∈	O
∂f	O
(	O
w	O
(	O
t	O
)	O
)	O
update	O
w	O
(	O
t+1	O
)	O
=	O
w	O
(	O
t	O
)	O
−	O
ηvt	O
output	O
¯w	O
=	O
1	O
t	O
(	O
cid:80	O
)	O
t	O
t=1	O
w	O
(	O
t	O
)	O
an	O
illustration	O
of	O
stochastic	O
gradient	B
descent	I
versus	O
gradient	B
descent	I
is	O
given	O
in	O
figure	O
14.3.	O
as	O
we	O
will	O
see	O
in	O
section	O
14.5	O
,	O
in	O
the	O
context	O
of	O
learning	O
problems	O
,	O
it	O
is	O
easy	O
to	O
ﬁnd	O
a	O
random	O
vector	O
whose	O
expectation	O
is	O
a	O
subgradient	O
of	O
the	O
risk	B
function	O
.	O
14.3.1	O
analysis	O
of	O
sgd	O
for	O
convex-lipschitz-bounded	O
functions	O
recall	B
the	O
bound	O
we	O
achieved	O
for	O
the	O
gd	O
algorithm	O
in	O
corollary	O
14.2.	O
for	O
the	O
stochastic	O
case	O
,	O
in	O
which	O
only	O
the	O
expectation	O
of	O
vt	O
is	O
in	O
∂f	O
(	O
w	O
(	O
t	O
)	O
)	O
,	O
we	O
can	O
not	O
directly	O
apply	O
equation	O
(	O
14.3	O
)	O
.	O
however	O
,	O
since	O
the	O
expected	O
value	O
of	O
vt	O
is	O
a	O
192	O
stochastic	O
gradient	B
descent	I
subgradient	O
of	O
f	O
at	O
w	O
(	O
t	O
)	O
,	O
we	O
can	O
still	O
derive	O
a	O
similar	O
bound	O
on	O
the	O
expected	O
output	O
of	O
stochastic	O
gradient	B
descent	I
.	O
this	O
is	O
formalized	O
in	O
the	O
following	O
theorem	O
.	O
theorem	O
14.8	O
let	O
b	O
,	O
ρ	O
>	O
0.	O
let	O
f	O
be	O
a	O
convex	B
function	O
and	O
let	O
w	O
(	O
cid:63	O
)	O
∈	O
argminw	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
≤b	O
f	O
(	O
w	O
)	O
.	O
assume	O
that	O
sgd	O
is	O
run	O
for	O
t	O
iterations	O
with	O
η	O
=	O
all	O
t	O
,	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
≤	O
ρ	O
with	O
probability	O
1.	O
then	O
,	O
ρ2	O
t	O
.	O
assume	O
also	O
that	O
for	O
(	O
cid:113	O
)	O
b2	O
e	O
[	O
f	O
(	O
¯w	O
)	O
]	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
≤	O
b	O
ρ√	O
t	O
.	O
therefore	O
,	O
for	O
any	O
	O
>	O
0	O
,	O
to	O
achieve	O
e	O
[	O
f	O
(	O
¯w	O
)	O
]	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
≤	O
	O
,	O
it	O
suﬃces	O
to	O
run	O
the	O
sgd	O
algorithm	O
for	O
a	O
number	O
of	O
iterations	O
that	O
satisﬁes	O
t	O
≥	O
b2ρ2	O
2	O
.	O
proof	O
let	O
us	O
introduce	O
the	O
notation	O
v1	O
:	O
t	O
to	O
denote	O
the	O
sequence	O
v1	O
,	O
.	O
.	O
.	O
,	O
vt.	O
taking	O
expectation	O
of	O
equation	O
(	O
14.2	O
)	O
,	O
we	O
obtain	O
e	O
v1	O
:	O
t	O
[	O
f	O
(	O
¯w	O
)	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
]	O
≤	O
e	O
v1	O
:	O
t	O
1	O
t	O
(	O
f	O
(	O
w	O
(	O
t	O
)	O
)	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
)	O
.	O
(	O
cid:34	O
)	O
t	O
(	O
cid:88	O
)	O
t=1	O
(	O
cid:35	O
)	O
since	O
lemma	O
14.1	O
holds	O
for	O
any	O
sequence	O
v1	O
,	O
v2	O
,	O
...	O
vt	O
,	O
it	O
applies	O
to	O
sgd	O
as	O
well	O
.	O
by	O
taking	O
expectation	O
of	O
the	O
bound	O
in	O
the	O
lemma	O
we	O
have	O
(	O
cid:34	O
)	O
e	O
v1	O
:	O
t	O
1	O
t	O
t	O
(	O
cid:88	O
)	O
t=1	O
(	O
cid:35	O
)	O
≤	O
b	O
ρ√	O
t	O
.	O
it	O
is	O
left	O
to	O
show	O
that	O
(	O
cid:34	O
)	O
e	O
v1	O
:	O
t	O
1	O
t	O
t	O
(	O
cid:88	O
)	O
t=1	O
(	O
cid:34	O
)	O
t	O
(	O
cid:88	O
)	O
t=1	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
(	O
cid:35	O
)	O
(	O
cid:34	O
)	O
1	O
t	O
≤	O
e	O
v1	O
:	O
t	O
t	O
(	O
cid:88	O
)	O
t=1	O
(	O
cid:35	O
)	O
t	O
(	O
cid:88	O
)	O
t=1	O
(	O
cid:35	O
)	O
(	O
14.9	O
)	O
,	O
(	O
14.10	O
)	O
(	O
f	O
(	O
w	O
(	O
t	O
)	O
)	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
)	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
which	O
we	O
will	O
hereby	O
prove	O
.	O
using	O
the	O
linearity	O
of	O
the	O
expectation	O
we	O
have	O
e	O
v1	O
:	O
t	O
1	O
t	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
=	O
1	O
t	O
[	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
]	O
.	O
e	O
v1	O
:	O
t	O
next	O
,	O
we	O
recall	B
the	O
law	O
of	O
total	O
expectation	O
:	O
for	O
every	O
two	O
random	O
variables	O
α	O
,	O
β	O
,	O
and	O
a	O
function	B
g	O
,	O
eα	O
[	O
g	O
(	O
α	O
)	O
]	O
=	O
eβ	O
eα	O
[	O
g	O
(	O
α	O
)	O
|β	O
]	O
.	O
setting	O
α	O
=	O
v1	O
:	O
t	O
and	O
β	O
=	O
v1	O
:	O
t−1	O
we	O
get	O
that	O
e	O
v1	O
:	O
t	O
[	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
]	O
=	O
e	O
=	O
e	O
v1	O
:	O
t−1	O
v1	O
:	O
t	O
[	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
]	O
[	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
|	O
v1	O
:	O
t−1	O
]	O
.	O
e	O
v1	O
:	O
t	O
once	O
we	O
know	O
v1	O
:	O
t−1	O
,	O
the	O
value	O
of	O
w	O
(	O
t	O
)	O
is	O
not	O
random	O
any	O
more	O
and	O
therefore	O
e	O
v1	O
:	O
t−1	O
e	O
v1	O
:	O
t	O
[	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
|	O
v1	O
:	O
t−1	O
]	O
=	O
e	O
v1	O
:	O
t−1	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
e	O
vt	O
[	O
vt	O
|	O
v1	O
:	O
t−1	O
]	O
(	O
cid:105	O
)	O
.	O
14.4	O
variants	O
193	O
since	O
w	O
(	O
t	O
)	O
only	O
depends	O
on	O
v1	O
:	O
t−1	O
and	O
sgd	O
requires	O
that	O
evt	O
[	O
vt	O
|	O
w	O
(	O
t	O
)	O
]	O
∈	O
∂f	O
(	O
w	O
(	O
t	O
)	O
)	O
we	O
obtain	O
that	O
evt	O
[	O
vt	O
|	O
v1	O
:	O
t−1	O
]	O
∈	O
∂f	O
(	O
w	O
(	O
t	O
)	O
)	O
.	O
thus	O
,	O
e	O
[	O
f	O
(	O
w	O
(	O
t	O
)	O
)	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
]	O
.	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
e	O
[	O
vt	O
|	O
v1	O
:	O
t−1	O
]	O
(	O
cid:105	O
)	O
≥	O
e	O
v1	O
:	O
t−1	O
v1	O
:	O
t−1	O
vt	O
overall	O
,	O
we	O
have	O
shown	O
that	O
e	O
v1	O
:	O
t	O
[	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
]	O
≥	O
e	O
=	O
e	O
[	O
f	O
(	O
w	O
(	O
t	O
)	O
)	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
]	O
[	O
f	O
(	O
w	O
(	O
t	O
)	O
)	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
]	O
.	O
v1	O
:	O
t−1	O
v1	O
:	O
t	O
summing	O
over	O
t	O
,	O
dividing	O
by	O
t	O
,	O
and	O
using	O
the	O
linearity	O
of	O
expectation	O
,	O
we	O
get	O
that	O
equation	O
(	O
14.10	O
)	O
holds	O
,	O
which	O
concludes	O
our	O
proof	O
.	O
14.4	O
variants	O
in	O
this	O
section	O
we	O
describe	O
several	O
variants	O
of	O
stochastic	O
gradient	B
descent	I
.	O
14.4.1	O
adding	O
a	O
projection	B
step	O
in	O
the	O
previous	O
analyses	O
of	O
the	O
gd	O
and	O
sgd	O
algorithms	O
,	O
we	O
required	O
that	O
the	O
norm	O
of	O
w	O
(	O
cid:63	O
)	O
will	O
be	O
at	O
most	O
b	O
,	O
which	O
is	O
equivalent	O
to	O
requiring	O
that	O
w	O
(	O
cid:63	O
)	O
is	O
in	O
the	O
set	B
h	O
=	O
{	O
w	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
≤	O
b	O
}	O
.	O
in	O
terms	O
of	O
learning	O
,	O
this	O
means	O
restricting	O
ourselves	O
to	O
a	O
b-bounded	O
hypothesis	B
class	I
.	O
yet	O
any	O
step	O
we	O
take	O
in	O
the	O
opposite	O
direction	O
of	O
the	O
gradient	B
(	O
or	O
its	O
expected	O
direction	O
)	O
might	O
result	O
in	O
stepping	O
out	O
of	O
this	O
bound	O
,	O
and	O
there	O
is	O
even	O
no	O
guarantee	O
that	O
¯w	O
satisﬁes	O
it	O
.	O
we	O
show	O
in	O
the	O
following	O
how	O
to	O
overcome	O
this	O
problem	O
while	O
maintaining	O
the	O
same	O
convergence	O
rate	O
.	O
the	O
basic	O
idea	O
is	O
to	O
add	O
a	O
projection	B
step	O
;	O
namely	O
,	O
we	O
will	O
now	O
have	O
a	O
two-step	O
update	O
rule	O
,	O
where	O
we	O
ﬁrst	O
subtract	O
a	O
subgradient	O
from	O
the	O
current	O
value	O
of	O
w	O
and	O
then	O
project	O
the	O
resulting	O
vector	O
onto	O
h.	O
formally	O
,	O
2	O
)	O
=	O
w	O
(	O
t	O
)	O
−	O
ηvt	O
2	O
)	O
(	O
cid:107	O
)	O
1..	O
w	O
(	O
t+	O
1	O
2..	O
w	O
(	O
t+1	O
)	O
=	O
argminw∈h	O
(	O
cid:107	O
)	O
w	O
−	O
w	O
(	O
t+	O
1	O
the	O
projection	B
step	O
replaces	O
the	O
current	O
value	O
of	O
w	O
by	O
the	O
vector	O
in	O
h	O
closest	O
to	O
it	O
.	O
clearly	O
,	O
the	O
projection	B
step	O
guarantees	O
that	O
w	O
(	O
t	O
)	O
∈	O
h	O
for	O
all	O
t.	O
since	O
h	O
is	O
convex	B
this	O
also	O
implies	O
that	O
¯w	O
∈	O
h	O
as	O
required	O
.	O
we	O
next	O
show	O
that	O
the	O
analysis	O
of	O
sgd	O
with	O
projections	O
remains	O
the	O
same	O
.	O
this	O
is	O
based	O
on	O
the	O
following	O
lemma	O
.	O
lemma	O
14.9	O
(	O
projection	B
lemma	I
)	O
let	O
h	O
be	O
a	O
closed	O
convex	B
set	O
and	O
let	O
v	O
be	O
the	O
projection	B
of	O
w	O
onto	O
h	O
,	O
namely	O
,	O
v	O
=	O
argmin	O
x∈h	O
(	O
cid:107	O
)	O
x	O
−	O
w	O
(	O
cid:107	O
)	O
2	O
.	O
194	O
stochastic	O
gradient	B
descent	I
then	O
,	O
for	O
every	O
u	O
∈	O
h	O
,	O
(	O
cid:107	O
)	O
w	O
−	O
u	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
cid:107	O
)	O
v	O
−	O
u	O
(	O
cid:107	O
)	O
2	O
≥	O
0.	O
proof	O
by	O
the	O
convexity	O
of	O
h	O
,	O
for	O
every	O
α	O
∈	O
(	O
0	O
,	O
1	O
)	O
we	O
have	O
that	O
v+α	O
(	O
u−v	O
)	O
∈	O
h.	O
therefore	O
,	O
from	O
the	O
optimality	O
of	O
v	O
we	O
obtain	O
(	O
cid:107	O
)	O
v	O
−	O
w	O
(	O
cid:107	O
)	O
2	O
≤	O
(	O
cid:107	O
)	O
v	O
+	O
α	O
(	O
u	O
−	O
v	O
)	O
−	O
w	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:107	O
)	O
v	O
−	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
2α	O
(	O
cid:104	O
)	O
v	O
−	O
w	O
,	O
u	O
−	O
v	O
(	O
cid:105	O
)	O
+	O
α2	O
(	O
cid:107	O
)	O
u	O
−	O
v	O
(	O
cid:107	O
)	O
2.	O
rearranging	O
,	O
we	O
obtain	O
2	O
(	O
cid:104	O
)	O
v	O
−	O
w	O
,	O
u	O
−	O
v	O
(	O
cid:105	O
)	O
≥	O
−α	O
(	O
cid:107	O
)	O
u	O
−	O
v	O
(	O
cid:107	O
)	O
2.	O
taking	O
the	O
limit	O
α	O
→	O
0	O
we	O
get	O
that	O
(	O
cid:104	O
)	O
v	O
−	O
w	O
,	O
u	O
−	O
v	O
(	O
cid:105	O
)	O
≥	O
0.	O
therefore	O
,	O
(	O
cid:107	O
)	O
w	O
−	O
u	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:107	O
)	O
w	O
−	O
v	O
+	O
v	O
−	O
u	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:107	O
)	O
w	O
−	O
v	O
(	O
cid:107	O
)	O
2	O
+	O
(	O
cid:107	O
)	O
v	O
−	O
u	O
(	O
cid:107	O
)	O
2	O
+	O
2	O
(	O
cid:104	O
)	O
v	O
−	O
w	O
,	O
u	O
−	O
v	O
(	O
cid:105	O
)	O
≥	O
(	O
cid:107	O
)	O
v	O
−	O
u	O
(	O
cid:107	O
)	O
2.	O
equipped	O
with	O
the	O
preceding	O
lemma	O
,	O
we	O
can	O
easily	O
adapt	O
the	O
analysis	O
of	O
sgd	O
to	O
the	O
case	O
in	O
which	O
we	O
add	O
projection	B
steps	O
on	O
a	O
closed	O
and	O
convex	B
set	O
.	O
simply	O
note	O
that	O
for	O
every	O
t	O
,	O
(	O
cid:107	O
)	O
w	O
(	O
t+1	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:107	O
)	O
w	O
(	O
t+1	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
t+	O
1	O
≤	O
(	O
cid:107	O
)	O
w	O
(	O
t+	O
1	O
2	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
.	O
2	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
+	O
(	O
cid:107	O
)	O
w	O
(	O
t+	O
1	O
2	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
therefore	O
,	O
lemma	O
14.1	O
holds	O
when	O
we	O
add	O
projection	B
steps	O
and	O
hence	O
the	O
rest	O
of	O
the	O
analysis	O
follows	O
directly	O
.	O
14.4.2	O
variable	O
step	O
size	O
another	O
variant	O
of	O
sgd	O
is	O
decreasing	O
the	O
step	O
size	O
as	O
a	O
function	B
of	O
t.	O
that	O
is	O
,	O
rather	O
than	O
updating	O
with	O
a	O
constant	O
η	O
,	O
we	O
use	O
ηt	O
.	O
for	O
instance	B
,	O
we	O
can	O
set	B
√	O
ηt	O
=	O
b	O
and	O
achieve	O
a	O
bound	O
similar	O
to	O
theorem	O
14.8.	O
the	O
idea	O
is	O
that	O
when	O
ρ	O
we	O
are	O
closer	O
to	O
the	O
minimum	O
of	O
the	O
function	B
,	O
we	O
take	O
our	O
steps	O
more	O
carefully	O
,	O
so	O
as	O
not	O
to	O
“	O
overshoot	O
”	O
the	O
minimum	O
.	O
t	O
14.4	O
variants	O
195	O
14.4.3	O
other	O
averaging	O
techniques	O
(	O
cid:80	O
)	O
t	O
we	O
have	O
set	B
the	O
output	O
vector	O
to	O
be	O
¯w	O
=	O
1	O
t=1	O
w	O
(	O
t	O
)	O
.	O
there	O
are	O
alternative	O
approaches	O
such	O
as	O
outputting	O
w	O
(	O
t	O
)	O
for	O
some	O
random	O
t	O
∈	O
[	O
t	O
]	O
,	O
or	O
outputting	O
the	O
t	O
average	O
of	O
w	O
(	O
t	O
)	O
over	O
the	O
last	O
αt	O
iterations	O
,	O
for	O
some	O
α	O
∈	O
(	O
0	O
,	O
1	O
)	O
.	O
one	O
can	O
also	O
take	O
a	O
weighted	O
average	O
of	O
the	O
last	O
few	O
iterates	O
.	O
these	O
more	O
sophisticated	O
averaging	O
schemes	O
can	O
improve	O
the	O
convergence	O
speed	O
in	O
some	O
situations	O
,	O
such	O
as	O
in	O
the	O
case	O
of	O
strongly	B
convex	I
functions	O
deﬁned	O
in	O
the	O
following	O
.	O
14.4.4	O
strongly	B
convex	I
functions*	O
in	O
this	O
section	O
we	O
show	O
a	O
variant	O
of	O
sgd	O
that	O
enjoys	O
a	O
faster	O
convergence	O
rate	O
for	O
problems	O
in	O
which	O
the	O
objective	O
function	B
is	O
strongly	B
convex	I
(	O
see	O
deﬁnition	O
13.4	O
of	O
strong	O
convexity	O
in	O
the	O
previous	O
chapter	O
)	O
.	O
we	O
rely	O
on	O
the	O
following	O
claim	O
,	O
which	O
generalizes	O
lemma	O
13.5.	O
claim	O
14.10	O
have	O
if	O
f	O
is	O
λ-strongly	O
convex	B
then	O
for	O
every	O
w	O
,	O
u	O
and	O
v	O
∈	O
∂f	O
(	O
w	O
)	O
we	O
(	O
cid:104	O
)	O
w	O
−	O
u	O
,	O
v	O
(	O
cid:105	O
)	O
≥	O
f	O
(	O
w	O
)	O
−	O
f	O
(	O
u	O
)	O
+	O
λ	O
2	O
(	O
cid:107	O
)	O
w	O
−	O
u	O
(	O
cid:107	O
)	O
2.	O
the	O
proof	O
is	O
similar	O
to	O
the	O
proof	O
of	O
lemma	O
13.5	O
and	O
is	O
left	O
as	O
an	O
exercise	O
.	O
sgd	O
for	O
minimizing	O
a	O
λ-strongly	O
convex	B
function	O
goal	O
:	O
solve	O
minw∈h	O
f	O
(	O
w	O
)	O
parameter	O
:	O
t	O
initialize	O
:	O
w	O
(	O
1	O
)	O
=	O
0	O
for	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
choose	O
a	O
random	O
vector	O
vt	O
s.t	O
.	O
e	O
[	O
vt|w	O
(	O
t	O
)	O
]	O
∈	O
∂f	O
(	O
w	O
(	O
t	O
)	O
)	O
set	B
ηt	O
=	O
1/	O
(	O
λ	O
t	O
)	O
set	B
w	O
(	O
t+	O
1	O
set	B
w	O
(	O
t+1	O
)	O
=	O
arg	O
minw∈h	O
(	O
cid:107	O
)	O
w	O
−	O
w	O
(	O
t+	O
1	O
output	O
:	O
¯w	O
=	O
1	O
t	O
2	O
)	O
=	O
w	O
(	O
t	O
)	O
−	O
ηtvt	O
(	O
cid:80	O
)	O
t	O
t=1	O
w	O
(	O
t	O
)	O
2	O
)	O
(	O
cid:107	O
)	O
2	O
theorem	O
14.11	O
assume	O
that	O
f	O
is	O
λ-strongly	O
convex	B
and	O
that	O
e	O
[	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
2	O
]	O
≤	O
ρ2	O
.	O
let	O
w	O
(	O
cid:63	O
)	O
∈	O
argminw∈h	O
f	O
(	O
w	O
)	O
be	O
an	O
optimal	O
solution	O
.	O
then	O
,	O
e	O
[	O
f	O
(	O
¯w	O
)	O
]	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
≤	O
ρ2	O
2	O
λ	O
t	O
(	O
1	O
+	O
log	O
(	O
t	O
)	O
)	O
.	O
proof	O
let	O
∇	O
(	O
t	O
)	O
=	O
e	O
[	O
vt|w	O
(	O
t	O
)	O
]	O
.	O
since	O
f	O
is	O
strongly	B
convex	I
and	O
∇	O
(	O
t	O
)	O
is	O
in	O
the	O
subgradient	O
set	B
of	O
f	O
at	O
w	O
(	O
t	O
)	O
we	O
have	O
that	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
∇	O
(	O
t	O
)	O
(	O
cid:105	O
)	O
≥	O
f	O
(	O
w	O
(	O
t	O
)	O
)	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
+	O
λ	O
2	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
.	O
(	O
14.11	O
)	O
next	O
,	O
we	O
show	O
that	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
∇	O
(	O
t	O
)	O
(	O
cid:105	O
)	O
≤	O
e	O
[	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
t+1	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
]	O
2	O
ηt	O
+	O
ηt	O
2	O
ρ2	O
.	O
(	O
14.12	O
)	O
196	O
stochastic	O
gradient	B
descent	I
since	O
w	O
(	O
t+1	O
)	O
is	O
the	O
projection	B
of	O
w	O
(	O
t+	O
1	O
(	O
cid:107	O
)	O
w	O
(	O
t+	O
1	O
2	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
≥	O
(	O
cid:107	O
)	O
w	O
(	O
t+1	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2.	O
therefore	O
,	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
t+1	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
≥	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
t+	O
1	O
=	O
2ηt	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
−	O
η2	O
2	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
t	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
2	O
.	O
2	O
)	O
onto	O
h	O
,	O
and	O
w	O
(	O
cid:63	O
)	O
∈	O
h	O
we	O
have	O
that	O
taking	O
expectation	O
of	O
both	O
sides	O
,	O
rearranging	O
,	O
and	O
using	O
the	O
assumption	O
e	O
[	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
2	O
]	O
≤	O
ρ2	O
yield	O
equation	O
(	O
14.12	O
)	O
.	O
comparing	O
equation	O
(	O
14.11	O
)	O
and	O
equation	O
(	O
14.12	O
)	O
and	O
summing	O
over	O
t	O
we	O
obtain	O
t	O
(	O
cid:88	O
)	O
t=1	O
≤	O
e	O
(	O
e	O
[	O
f	O
(	O
w	O
(	O
t	O
)	O
)	O
]	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
)	O
(	O
cid:34	O
)	O
t	O
(	O
cid:88	O
)	O
(	O
cid:18	O
)	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
t+1	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
t=1	O
2	O
ηt	O
−	O
λ	O
2	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
(	O
cid:19	O
)	O
(	O
cid:35	O
)	O
t	O
(	O
cid:88	O
)	O
t=1	O
ηt	O
.	O
+	O
ρ2	O
2	O
next	O
,	O
we	O
use	O
the	O
deﬁnition	O
ηt	O
=	O
1/	O
(	O
λ	O
t	O
)	O
and	O
note	O
that	O
the	O
ﬁrst	O
sum	O
on	O
the	O
right-hand	O
side	O
of	O
the	O
equation	O
collapses	O
to	O
−λt	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
+1	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
≤	O
0.	O
thus	O
,	O
t	O
(	O
cid:88	O
)	O
t=1	O
(	O
e	O
[	O
f	O
(	O
w	O
(	O
t	O
)	O
)	O
]	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
)	O
≤	O
ρ2	O
2	O
λ	O
1	O
t	O
≤	O
ρ2	O
2	O
λ	O
(	O
1	O
+	O
log	O
(	O
t	O
)	O
)	O
.	O
t	O
(	O
cid:88	O
)	O
t=1	O
the	O
theorem	O
follows	O
from	O
the	O
preceding	O
by	O
dividing	O
by	O
t	O
and	O
using	O
jensen	O
’	O
s	O
inequality	O
.	O
remark	O
14.3	O
rakhlin	O
,	O
shamir	O
&	O
sridharan	O
(	O
2012	O
)	O
derived	O
a	O
convergence	O
rate	O
in	O
which	O
the	O
log	O
(	O
t	O
)	O
term	O
is	O
eliminated	O
for	O
a	O
variant	O
of	O
the	O
algorithm	O
in	O
which	O
we	O
output	O
the	O
average	O
of	O
the	O
last	O
t	O
/2	O
iterates	O
,	O
¯w	O
=	O
2	O
t=t	O
/2+1	O
w	O
(	O
t	O
)	O
.	O
shamir	O
&	O
t	O
zhang	O
(	O
2013	O
)	O
have	O
shown	O
that	O
theorem	O
14.11	O
holds	O
even	O
if	O
we	O
output	O
¯w	O
=	O
w	O
(	O
t	O
)	O
.	O
(	O
cid:80	O
)	O
t	O
14.5	O
learning	O
with	O
sgd	O
we	O
have	O
so	O
far	O
introduced	O
and	O
analyzed	O
the	O
sgd	O
algorithm	O
for	O
general	O
convex	B
functions	O
.	O
now	O
we	O
shall	O
consider	O
its	O
applicability	O
to	O
learning	O
tasks	O
.	O
14.5.1	O
sgd	O
for	O
risk	B
minimization	O
recall	B
that	O
in	O
learning	O
we	O
face	O
the	O
problem	O
of	O
minimizing	O
the	O
risk	B
function	O
ld	O
(	O
w	O
)	O
=	O
e	O
z∼d	O
[	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
z	O
)	O
]	O
.	O
we	O
have	O
seen	O
the	O
method	O
of	O
empirical	B
risk	I
minimization	O
,	O
where	O
we	O
minimize	O
the	O
empirical	B
risk	I
,	O
ls	O
(	O
w	O
)	O
,	O
as	O
an	O
estimate	O
to	O
minimizing	O
ld	O
(	O
w	O
)	O
.	O
sgd	O
allows	O
us	O
to	O
take	O
a	O
diﬀerent	O
approach	O
and	O
minimize	O
ld	O
(	O
w	O
)	O
directly	O
.	O
since	O
we	O
do	O
not	O
know	O
d	O
,	O
we	O
can	O
not	O
simply	O
calculate	O
∇ld	O
(	O
w	O
(	O
t	O
)	O
)	O
and	O
minimize	O
it	O
with	O
the	O
gd	O
method	O
.	O
with	O
sgd	O
,	O
however	O
,	O
all	O
we	O
need	O
is	O
to	O
ﬁnd	O
an	O
unbiased	O
estimate	O
of	O
the	O
gradient	B
of	O
14.5	O
learning	O
with	O
sgd	O
197	O
ld	O
(	O
w	O
)	O
,	O
that	O
is	O
,	O
a	O
random	O
vector	O
whose	O
conditional	O
expected	O
value	O
is	O
∇ld	O
(	O
w	O
(	O
t	O
)	O
)	O
.	O
we	O
shall	O
now	O
see	O
how	O
such	O
an	O
estimate	O
can	O
be	O
easily	O
constructed	O
.	O
for	O
simplicity	O
,	O
let	O
us	O
ﬁrst	O
consider	O
the	O
case	O
of	O
diﬀerentiable	O
loss	B
functions	O
.	O
hence	O
the	O
risk	B
function	O
ld	O
is	O
also	O
diﬀerentiable	O
.	O
the	O
construction	O
of	O
the	O
random	O
vector	O
vt	O
will	O
be	O
as	O
follows	O
:	O
first	O
,	O
sample	O
z	O
∼	O
d.	O
then	O
,	O
deﬁne	O
vt	O
to	O
be	O
the	O
gradient	B
of	O
the	O
function	B
(	O
cid:96	O
)	O
(	O
w	O
,	O
z	O
)	O
with	O
respect	O
to	O
w	O
,	O
at	O
the	O
point	O
w	O
(	O
t	O
)	O
.	O
then	O
,	O
by	O
the	O
linearity	O
of	O
the	O
gradient	B
we	O
have	O
e	O
[	O
vt|w	O
(	O
t	O
)	O
]	O
=	O
e	O
z∼d	O
[	O
∇	O
(	O
cid:96	O
)	O
(	O
w	O
(	O
t	O
)	O
,	O
z	O
)	O
]	O
=	O
∇	O
e	O
z∼d	O
[	O
(	O
cid:96	O
)	O
(	O
w	O
(	O
t	O
)	O
,	O
z	O
)	O
]	O
=	O
∇ld	O
(	O
w	O
(	O
t	O
)	O
)	O
.	O
(	O
14.13	O
)	O
the	O
gradient	B
of	O
the	O
loss	B
function	I
(	O
cid:96	O
)	O
(	O
w	O
,	O
z	O
)	O
at	O
w	O
(	O
t	O
)	O
is	O
therefore	O
an	O
unbiased	O
estimate	O
of	O
the	O
gradient	B
of	O
the	O
risk	B
function	O
ld	O
(	O
w	O
(	O
t	O
)	O
)	O
and	O
is	O
easily	O
constructed	O
by	O
sampling	O
a	O
single	O
fresh	O
example	O
z	O
∼	O
d	O
at	O
each	O
iteration	O
t.	O
the	O
same	O
argument	O
holds	O
for	O
nondiﬀerentiable	O
loss	B
functions	O
.	O
we	O
simply	O
let	O
vt	O
be	O
a	O
subgradient	O
of	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
z	O
)	O
at	O
w	O
(	O
t	O
)	O
.	O
then	O
,	O
for	O
every	O
u	O
we	O
have	O
(	O
cid:96	O
)	O
(	O
u	O
,	O
z	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
w	O
(	O
t	O
)	O
,	O
z	O
)	O
≥	O
(	O
cid:104	O
)	O
u	O
−	O
w	O
(	O
t	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
.	O
taking	O
expectation	O
on	O
both	O
sides	O
with	O
respect	O
to	O
z	O
∼	O
d	O
and	O
conditioned	O
on	O
the	O
value	O
of	O
w	O
(	O
t	O
)	O
we	O
obtain	O
ld	O
(	O
u	O
)	O
−	O
ld	O
(	O
w	O
(	O
t	O
)	O
)	O
=	O
e	O
[	O
(	O
cid:96	O
)	O
(	O
u	O
,	O
z	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
w	O
(	O
t	O
)	O
,	O
z	O
)	O
|w	O
(	O
t	O
)	O
]	O
≥	O
e	O
[	O
(	O
cid:104	O
)	O
u	O
−	O
w	O
(	O
t	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
|w	O
(	O
t	O
)	O
]	O
=	O
(	O
cid:104	O
)	O
u	O
−	O
w	O
(	O
t	O
)	O
,	O
e	O
[	O
vt|w	O
(	O
t	O
)	O
]	O
(	O
cid:105	O
)	O
.	O
it	O
follows	O
that	O
e	O
[	O
vt|w	O
(	O
t	O
)	O
]	O
is	O
a	O
subgradient	O
of	O
ld	O
(	O
w	O
)	O
at	O
w	O
(	O
t	O
)	O
.	O
to	O
summarize	O
,	O
the	O
stochastic	O
gradient	B
descent	I
framework	O
for	O
minimizing	O
the	O
risk	B
is	O
as	O
follows	O
.	O
stochastic	O
gradient	B
descent	I
(	O
sgd	O
)	O
for	O
minimizing	O
ld	O
(	O
w	O
)	O
parameters	O
:	O
scalar	O
η	O
>	O
0	O
,	O
integer	O
t	O
>	O
0	O
initialize	O
:	O
w	O
(	O
1	O
)	O
=	O
0	O
for	O
t	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
t	O
sample	O
z	O
∼	O
d	O
pick	O
vt	O
∈	O
∂	O
(	O
cid:96	O
)	O
(	O
w	O
(	O
t	O
)	O
,	O
z	O
)	O
update	O
w	O
(	O
t+1	O
)	O
=	O
w	O
(	O
t	O
)	O
−	O
ηvt	O
output	O
¯w	O
=	O
1	O
t	O
(	O
cid:80	O
)	O
t	O
t=1	O
w	O
(	O
t	O
)	O
we	O
shall	O
now	O
use	O
our	O
analysis	O
of	O
sgd	O
to	O
obtain	O
a	O
sample	B
complexity	I
anal-	O
ysis	O
for	O
learning	O
convex-lipschitz-bounded	O
problems	O
.	O
theorem	O
14.8	O
yields	O
the	O
following	O
:	O
corollary	O
14.12	O
consider	O
a	O
convex-lipschitz-bounded	O
learning	O
problem	O
with	O
parameters	O
ρ	O
,	O
b.	O
then	O
,	O
for	O
every	O
	O
>	O
0	O
,	O
if	O
we	O
run	O
the	O
sgd	O
method	O
for	O
minimizing	O
198	O
stochastic	O
gradient	B
descent	I
ld	O
(	O
w	O
)	O
with	O
a	O
number	O
of	O
iterations	O
(	O
i.e.	O
,	O
number	O
of	O
examples	O
)	O
(	O
cid:113	O
)	O
b2	O
t	O
≥	O
b2ρ2	O
2	O
and	O
with	O
η	O
=	O
ρ2	O
t	O
,	O
then	O
the	O
output	O
of	O
sgd	O
satisﬁes	O
w∈h	O
ld	O
(	O
w	O
)	O
+	O
	O
.	O
e	O
[	O
ld	O
(	O
¯w	O
)	O
]	O
≤	O
min	O
it	O
is	O
interesting	O
to	O
note	O
that	O
the	O
required	O
sample	B
complexity	I
is	O
of	O
the	O
same	O
order	O
of	O
magnitude	O
as	O
the	O
sample	B
complexity	I
guarantee	O
we	O
derived	O
for	O
regularized	B
loss	I
minimization	I
.	O
in	O
fact	O
,	O
the	O
sample	B
complexity	I
of	O
sgd	O
is	O
even	O
better	O
than	O
what	O
we	O
have	O
derived	O
for	O
regularized	B
loss	I
minimization	I
by	O
a	O
factor	O
of	O
8	O
.	O
14.5.2	O
analyzing	O
sgd	O
for	O
convex-smooth	O
learning	O
problems	O
in	O
the	O
previous	O
chapter	O
we	O
saw	O
that	O
the	O
regularized	B
loss	I
minimization	I
rule	O
also	O
learns	O
the	O
class	O
of	O
convex-smooth-bounded	B
learning	I
problems	O
.	O
we	O
now	O
show	O
that	O
the	O
sgd	O
algorithm	O
can	O
be	O
also	O
used	O
for	O
such	O
problems	O
.	O
theorem	O
14.13	O
assume	O
that	O
for	O
all	O
z	O
,	O
the	O
loss	B
function	I
(	O
cid:96	O
)	O
(	O
·	O
,	O
z	O
)	O
is	O
convex	B
,	O
β-	O
smooth	O
,	O
and	O
nonnegative	O
.	O
then	O
,	O
if	O
we	O
run	O
the	O
sgd	O
algorithm	O
for	O
minimizing	O
ld	O
(	O
w	O
)	O
we	O
have	O
that	O
for	O
every	O
w	O
(	O
cid:63	O
)	O
,	O
(	O
cid:18	O
)	O
(	O
cid:19	O
)	O
.	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
2η	O
t	O
e	O
[	O
ld	O
(	O
¯w	O
)	O
]	O
≤	O
1	O
1	O
−	O
ηβ	O
ld	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
+	O
proof	O
recall	B
that	O
if	O
a	O
function	B
is	O
β-smooth	O
and	O
nonnegative	O
then	O
it	O
is	O
self-	O
bounded	O
:	O
(	O
cid:107	O
)	O
∇f	O
(	O
w	O
)	O
(	O
cid:107	O
)	O
2	O
≤	O
2βf	O
(	O
w	O
)	O
.	O
to	O
analyze	O
sgd	O
for	O
convex-smooth	O
problems	O
,	O
let	O
us	O
deﬁne	O
z1	O
,	O
.	O
.	O
.	O
,	O
zt	O
the	O
random	O
samples	O
of	O
the	O
sgd	O
algorithm	O
,	O
let	O
ft	O
(	O
·	O
)	O
=	O
(	O
cid:96	O
)	O
(	O
·	O
,	O
zt	O
)	O
,	O
and	O
note	O
that	O
vt	O
=	O
∇ft	O
(	O
w	O
(	O
t	O
)	O
)	O
.	O
for	O
all	O
t	O
,	O
ft	O
is	O
a	O
convex	B
function	O
and	O
therefore	O
ft	O
(	O
w	O
(	O
t	O
)	O
)	O
−ft	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
≤	O
(	O
cid:104	O
)	O
vt	O
,	O
w	O
(	O
t	O
)	O
−w	O
(	O
cid:63	O
)	O
(	O
cid:105	O
)	O
.	O
summing	O
over	O
t	O
and	O
using	O
lemma	O
14.1	O
we	O
obtain	O
t	O
(	O
cid:88	O
)	O
t=1	O
+	O
η	O
2	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
2.	O
combining	O
the	O
preceding	O
with	O
the	O
self-boundedness	B
of	O
ft	O
yields	O
t	O
(	O
cid:88	O
)	O
(	O
ft	O
(	O
w	O
(	O
t	O
)	O
)	O
−	O
ft	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
)	O
≤	O
t	O
(	O
cid:88	O
)	O
t=1	O
t=1	O
(	O
cid:104	O
)	O
vt	O
,	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:105	O
)	O
≤	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
t	O
(	O
cid:88	O
)	O
2η	O
t	O
(	O
cid:88	O
)	O
(	O
ft	O
(	O
w	O
(	O
t	O
)	O
)	O
−	O
ft	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
)	O
≤	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
t	O
(	O
cid:88	O
)	O
t	O
(	O
cid:88	O
)	O
(	O
cid:32	O
)	O
2η	O
t=1	O
ft	O
(	O
w	O
(	O
t	O
)	O
)	O
≤	O
1	O
1	O
−	O
ηβ	O
1	O
t	O
1	O
t	O
t=1	O
t=1	O
+	O
ηβ	O
ft	O
(	O
w	O
(	O
t	O
)	O
)	O
.	O
t=1	O
ft	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
+	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
2η	O
t	O
(	O
cid:33	O
)	O
.	O
dividing	O
by	O
t	O
and	O
rearranging	O
,	O
we	O
obtain	O
next	O
,	O
we	O
take	O
expectation	O
of	O
the	O
two	O
sides	O
of	O
the	O
preceding	O
equation	O
with	O
respect	O
14.5	O
learning	O
with	O
sgd	O
199	O
to	O
z1	O
,	O
.	O
.	O
.	O
,	O
zt	O
.	O
clearly	O
,	O
e	O
[	O
ft	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
]	O
=	O
ld	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
.	O
in	O
addition	O
,	O
using	O
the	O
same	O
argument	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
14.8	O
we	O
have	O
that	O
(	O
cid:34	O
)	O
e	O
1	O
t	O
t	O
(	O
cid:88	O
)	O
t=1	O
(	O
cid:35	O
)	O
(	O
cid:34	O
)	O
1	O
t	O
t	O
(	O
cid:88	O
)	O
t=1	O
(	O
cid:35	O
)	O
ft	O
(	O
w	O
(	O
t	O
)	O
)	O
=	O
e	O
ld	O
(	O
w	O
(	O
t	O
)	O
)	O
≥	O
e	O
[	O
ld	O
(	O
¯w	O
)	O
]	O
.	O
combining	O
all	O
we	O
conclude	O
our	O
proof	O
.	O
as	O
a	O
direct	O
corollary	O
we	O
obtain	O
:	O
corollary	O
14.14	O
consider	O
a	O
convex-smooth-bounded	B
learning	I
problem	O
with	O
parameters	O
β	O
,	O
b.	O
assume	O
in	O
addition	O
that	O
(	O
cid:96	O
)	O
(	O
0	O
,	O
z	O
)	O
≤	O
1	O
for	O
all	O
z	O
∈	O
z.	O
for	O
every	O
	O
>	O
0	O
,	O
set	B
η	O
=	O
β	O
(	O
1+3/	O
)	O
.	O
then	O
,	O
running	O
sgd	O
with	O
t	O
≥	O
12b2β/2	O
yields	O
1	O
e	O
[	O
ld	O
(	O
¯w	O
)	O
]	O
≤	O
min	O
w∈h	O
ld	O
(	O
w	O
)	O
+	O
	O
.	O
14.5.3	O
sgd	O
for	O
regularized	B
loss	I
minimization	I
we	O
have	O
shown	O
that	O
sgd	O
enjoys	O
the	O
same	O
worst-case	O
sample	B
complexity	I
bound	O
as	O
regularized	B
loss	I
minimization	I
.	O
however	O
,	O
on	O
some	O
distributions	O
,	O
regularized	B
loss	I
minimization	I
may	O
yield	O
a	O
better	O
solution	O
.	O
therefore	O
,	O
in	O
some	O
cases	O
we	O
may	O
want	O
to	O
solve	O
the	O
optimization	O
problem	O
associated	O
with	O
regularized	B
loss	I
minimization	I
,	O
namely,1	O
(	O
cid:18	O
)	O
λ	O
2	O
(	O
cid:19	O
)	O
min	O
w	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
ls	O
(	O
w	O
)	O
.	O
(	O
14.14	O
)	O
deﬁne	O
f	O
(	O
w	O
)	O
=	O
λ	O
since	O
we	O
are	O
dealing	O
with	O
convex	B
learning	O
problems	O
in	O
which	O
the	O
loss	B
function	I
is	O
convex	B
,	O
the	O
preceding	O
problem	O
is	O
also	O
a	O
convex	B
optimization	O
problem	O
that	O
can	O
be	O
solved	O
using	O
sgd	O
as	O
well	O
,	O
as	O
we	O
shall	O
see	O
in	O
this	O
section	O
.	O
2	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
ls	O
(	O
w	O
)	O
.	O
note	O
that	O
f	O
is	O
a	O
λ-strongly	O
convex	B
function	O
;	O
therefore	O
,	O
we	O
can	O
apply	O
the	O
sgd	O
variant	O
given	O
in	O
section	O
14.4.4	O
(	O
with	O
h	O
=	O
rd	O
)	O
.	O
to	O
apply	O
this	O
algorithm	O
,	O
we	O
only	O
need	O
to	O
ﬁnd	O
a	O
way	O
to	O
construct	O
an	O
unbiased	O
estimate	O
of	O
a	O
subgradient	O
of	O
f	O
at	O
w	O
(	O
t	O
)	O
.	O
this	O
is	O
easily	O
done	O
by	O
noting	O
that	O
if	O
we	O
pick	O
z	O
uniformly	O
at	O
random	O
from	O
s	O
,	O
and	O
choose	O
vt	O
∈	O
∂	O
(	O
cid:96	O
)	O
(	O
w	O
(	O
t	O
)	O
,	O
z	O
)	O
then	O
the	O
expected	O
value	O
of	O
λw	O
(	O
t	O
)	O
+	O
vt	O
is	O
a	O
subgradient	O
of	O
f	O
at	O
w	O
(	O
t	O
)	O
.	O
to	O
analyze	O
the	O
resulting	O
algorithm	O
,	O
we	O
ﬁrst	O
rewrite	O
the	O
update	O
rule	O
(	O
assuming	O
1	O
we	O
divided	O
λ	O
by	O
2	O
for	O
convenience	O
.	O
200	O
stochastic	O
gradient	B
descent	I
that	O
h	O
=	O
rd	O
and	O
therefore	O
the	O
projection	B
step	O
does	O
not	O
matter	O
)	O
as	O
follows	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
w	O
(	O
t+1	O
)	O
=	O
w	O
(	O
t	O
)	O
−	O
1	O
λ	O
t	O
λw	O
(	O
t	O
)	O
+	O
vt	O
vt	O
vt	O
=	O
=	O
=	O
(	O
cid:18	O
)	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
t	O
−	O
2	O
t	O
(	O
cid:88	O
)	O
t	O
−	O
1	O
w	O
(	O
t	O
)	O
−	O
1	O
λ	O
t	O
1	O
−	O
1	O
t	O
t	O
−	O
1	O
w	O
(	O
t	O
)	O
−	O
1	O
λ	O
t	O
t	O
t	O
−	O
1	O
w	O
(	O
t−1	O
)	O
−	O
t	O
=	O
−	O
1	O
λ	O
t	O
vi	O
.	O
i=1	O
(	O
cid:19	O
)	O
−	O
1	O
λ	O
t	O
vt	O
1	O
λ	O
(	O
t	O
−	O
1	O
)	O
vt−1	O
(	O
14.15	O
)	O
if	O
we	O
assume	O
that	O
the	O
loss	B
function	I
is	O
ρ-lipschitz	O
,	O
it	O
follows	O
that	O
for	O
all	O
t	O
we	O
have	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
≤	O
ρ	O
and	O
therefore	O
(	O
cid:107	O
)	O
λw	O
(	O
t	O
)	O
(	O
cid:107	O
)	O
≤	O
ρ	O
,	O
which	O
yields	O
(	O
cid:107	O
)	O
λw	O
(	O
t	O
)	O
+	O
vt	O
(	O
cid:107	O
)	O
≤	O
2ρ	O
.	O
theorem	O
14.11	O
therefore	O
tells	O
us	O
that	O
after	O
performing	O
t	O
iterations	O
we	O
have	O
that	O
e	O
[	O
f	O
(	O
¯w	O
)	O
]	O
−	O
f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
≤	O
4ρ2	O
λ	O
t	O
(	O
1	O
+	O
log	O
(	O
t	O
)	O
)	O
.	O
14.6	O
summary	O
we	O
have	O
introduced	O
the	O
gradient	B
descent	I
and	O
stochastic	O
gradient	B
descent	I
algo-	O
rithms	O
,	O
along	O
with	O
several	O
of	O
their	O
variants	O
.	O
we	O
have	O
analyzed	O
their	O
convergence	O
rate	O
and	O
calculated	O
the	O
number	O
of	O
iterations	O
that	O
would	O
guarantee	O
an	O
expected	O
objective	O
of	O
at	O
most	O
	O
plus	O
the	O
optimal	O
objective	O
.	O
most	O
importantly	O
,	O
we	O
have	O
shown	O
that	O
by	O
using	O
sgd	O
we	O
can	O
directly	O
minimize	O
the	O
risk	B
function	O
.	O
we	O
do	O
so	O
by	O
sampling	O
a	O
point	O
i.i.d	O
from	O
d	O
and	O
using	O
a	O
subgradient	O
of	O
the	O
loss	B
of	O
the	O
current	O
hypothesis	B
w	O
(	O
t	O
)	O
at	O
this	O
point	O
as	O
an	O
unbiased	O
estimate	O
of	O
the	O
gradient	B
(	O
or	O
a	O
subgradient	O
)	O
of	O
the	O
risk	B
function	O
.	O
this	O
implies	O
that	O
a	O
bound	O
on	O
the	O
number	O
of	O
iterations	O
also	O
yields	O
a	O
sample	B
complexity	I
bound	O
.	O
finally	O
,	O
we	O
have	O
also	O
shown	O
how	O
to	O
apply	O
the	O
sgd	O
method	O
to	O
the	O
problem	O
of	O
regularized	O
risk	O
minimization	O
.	O
in	O
future	O
chapters	O
we	O
show	O
how	O
this	O
yields	O
extremely	O
simple	O
solvers	O
to	O
some	O
optimization	O
problems	O
associated	O
with	O
regularized	O
risk	O
minimization	O
.	O
14.7	O
bibliographic	O
remarks	O
sgd	O
dates	O
back	O
to	O
robbins	O
&	O
monro	O
(	O
1951	O
)	O
.	O
it	O
is	O
especially	O
eﬀective	O
in	O
large	O
scale	O
machine	O
learning	O
problems	O
.	O
see	O
,	O
for	O
example	O
,	O
(	O
murata	O
1998	O
,	O
le	O
cun	O
2004	O
,	O
zhang	O
2004	O
,	O
bottou	O
&	O
bousquet	O
2008	O
,	O
shalev-shwartz	O
,	O
singer	O
&	O
srebro	O
2007	O
,	O
shalev-shwartz	O
&	O
srebro	O
2008	O
)	O
.	O
in	O
the	O
optimization	O
community	O
it	O
was	O
studied	O
14.8	O
exercises	O
201	O
in	O
the	O
context	O
of	O
stochastic	O
optimization	O
.	O
see	O
,	O
for	O
example	O
,	O
(	O
nemirovski	O
&	O
yudin	O
1978	O
,	O
nesterov	O
&	O
nesterov	O
2004	O
,	O
nesterov	O
2005	O
,	O
nemirovski	O
,	O
juditsky	O
,	O
lan	O
&	O
shapiro	O
2009	O
,	O
shapiro	O
,	O
dentcheva	O
&	O
ruszczy´nski	O
2009	O
)	O
.	O
the	O
bound	O
we	O
have	O
derived	O
for	O
strongly	B
convex	I
function	O
is	O
due	O
to	O
hazan	O
,	O
agarwal	O
&	O
kale	O
(	O
2007	O
)	O
.	O
as	O
mentioned	O
previously	O
,	O
improved	O
bounds	O
have	O
been	O
obtained	O
in	O
rakhlin	O
et	O
al	O
.	O
(	O
2012	O
)	O
.	O
14.8	O
exercises	O
1.	O
prove	O
claim	O
14.10.	O
hint	O
:	O
extend	O
the	O
proof	O
of	O
lemma	O
13.5	O
.	O
2.	O
prove	O
corollary	O
14.14	O
.	O
3.	O
perceptron	O
as	O
a	O
subgradient	O
descent	O
algorithm	O
:	O
let	O
s	O
=	O
(	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
)	O
∈	O
(	O
rd	O
×	O
{	O
±1	O
}	O
)	O
m.	O
assume	O
that	O
there	O
exists	O
w	O
∈	O
rd	O
such	O
that	O
for	O
every	O
i	O
∈	O
[	O
m	O
]	O
we	O
have	O
yi	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
≥	O
1	O
,	O
and	O
let	O
w	O
(	O
cid:63	O
)	O
be	O
a	O
vector	O
that	O
has	O
the	O
minimal	O
norm	O
among	O
all	O
vectors	O
that	O
satisfy	O
the	O
preceding	O
requirement	O
.	O
let	O
r	O
=	O
maxi	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
.	O
deﬁne	O
a	O
function	B
f	O
(	O
w	O
)	O
=	O
max	O
i∈	O
[	O
m	O
]	O
(	O
1	O
−	O
yi	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
)	O
.	O
1	O
separates	O
the	O
examples	O
in	O
s.	O
•	O
show	O
that	O
minw	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
≤	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
f	O
(	O
w	O
)	O
=	O
0	O
and	O
show	O
that	O
any	O
w	O
for	O
which	O
f	O
(	O
w	O
)	O
<	O
•	O
show	O
how	O
to	O
calculate	O
a	O
subgradient	O
of	O
f	O
.	O
•	O
describe	O
and	O
analyze	O
the	O
subgradient	O
descent	O
algorithm	O
for	O
this	O
case	O
.	O
com-	O
pare	O
the	O
algorithm	O
and	O
the	O
analysis	O
to	O
the	O
batch	O
perceptron	O
algorithm	O
given	O
in	O
section	O
9.1.2	O
.	O
4.	O
variable	O
step	O
size	O
(	O
*	O
)	O
:	O
prove	O
an	O
analog	O
of	O
theorem	O
14.8	O
for	O
sgd	O
with	O
a	O
√	O
variable	O
step	O
size	O
,	O
ηt	O
=	O
b	O
ρ	O
.	O
t	O
15	O
support	O
vector	O
machines	O
in	O
this	O
chapter	O
and	O
the	O
next	O
we	O
discuss	O
a	O
very	O
useful	O
machine	O
learning	O
tool	O
:	O
the	O
support	O
vector	O
machine	O
paradigm	O
(	O
svm	O
)	O
for	O
learning	O
linear	O
predictors	O
in	O
high	O
dimensional	O
feature	B
spaces	O
.	O
the	O
high	O
dimensionality	O
of	O
the	O
feature	B
space	I
raises	O
both	O
sample	B
complexity	I
and	O
computational	B
complexity	I
challenges	O
.	O
the	O
svm	O
algorithmic	O
paradigm	O
tackles	O
the	O
sample	B
complexity	I
challenge	O
by	O
searching	O
for	O
“	O
large	O
margin	B
”	O
separators	O
.	O
roughly	O
speaking	O
,	O
a	O
halfspace	B
separates	O
a	O
training	B
set	I
with	O
a	O
large	O
margin	B
if	O
all	O
the	O
examples	O
are	O
not	O
only	O
on	O
the	O
correct	O
side	O
of	O
the	O
separating	O
hyperplane	O
but	O
also	O
far	O
away	O
from	O
it	O
.	O
restricting	O
the	O
algorithm	O
to	O
output	O
a	O
large	O
margin	B
separator	O
can	O
yield	O
a	O
small	O
sample	B
complexity	I
even	O
if	O
the	O
dimensionality	O
of	O
the	O
feature	B
space	I
is	O
high	O
(	O
and	O
even	O
inﬁnite	O
)	O
.	O
we	O
introduce	O
the	O
concept	O
of	O
margin	B
and	O
relate	O
it	O
to	O
the	O
regularized	B
loss	I
minimization	I
paradigm	O
as	O
well	O
as	O
to	O
the	O
convergence	O
rate	O
of	O
the	O
perceptron	O
algorithm	O
.	O
in	O
the	O
next	O
chapter	O
we	O
will	O
tackle	O
the	O
computational	B
complexity	I
challenge	O
using	O
the	O
idea	O
of	O
kernels	B
.	O
15.1	O
margin	B
and	O
hard-svm	O
let	O
s	O
=	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
be	O
a	O
training	B
set	I
of	O
examples	O
,	O
where	O
each	O
xi	O
∈	O
rd	O
and	O
yi	O
∈	O
{	O
±1	O
}	O
.	O
we	O
say	O
that	O
this	O
training	B
set	I
is	O
linearly	O
separable	B
,	O
if	O
there	O
exists	O
a	O
halfspace	B
,	O
(	O
w	O
,	O
b	O
)	O
,	O
such	O
that	O
yi	O
=	O
sign	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
for	O
all	O
i.	O
alternatively	O
,	O
this	O
condition	O
can	O
be	O
rewritten	O
as	O
∀i	O
∈	O
[	O
m	O
]	O
,	O
yi	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
>	O
0.	O
all	O
halfspaces	O
(	O
w	O
,	O
b	O
)	O
that	O
satisfy	O
this	O
condition	O
are	O
erm	O
hypotheses	O
(	O
their	O
0-1	O
error	O
is	O
zero	O
,	O
which	O
is	O
the	O
minimum	O
possible	O
error	O
)	O
.	O
for	O
any	O
separable	B
training	O
sample	O
,	O
there	O
are	O
many	O
erm	O
halfspaces	O
.	O
which	O
one	O
of	O
them	O
should	O
the	O
learner	O
pick	O
?	O
consider	O
,	O
for	O
example	O
,	O
the	O
training	B
set	I
described	O
in	O
the	O
picture	O
that	O
follows	O
.	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
15.1	O
margin	B
and	O
hard-svm	O
203	O
x	O
x	O
while	O
both	O
the	O
dashed-black	O
and	O
solid-green	O
hyperplanes	O
separate	O
the	O
four	O
ex-	O
amples	O
,	O
our	O
intuition	O
would	O
probably	O
lead	O
us	O
to	O
prefer	O
the	O
black	O
hyperplane	O
over	O
the	O
green	O
one	O
.	O
one	O
way	O
to	O
formalize	O
this	O
intuition	O
is	O
using	O
the	O
concept	O
of	O
margin	B
.	O
the	O
margin	B
of	O
a	O
hyperplane	O
with	O
respect	O
to	O
a	O
training	B
set	I
is	O
deﬁned	O
to	O
be	O
the	O
minimal	O
distance	O
between	O
a	O
point	O
in	O
the	O
training	B
set	I
and	O
the	O
hyperplane	O
.	O
if	O
a	O
hyperplane	O
has	O
a	O
large	O
margin	B
,	O
then	O
it	O
will	O
still	O
separate	O
the	O
training	B
set	I
even	O
if	O
we	O
slightly	O
perturb	O
each	O
instance	B
.	O
we	O
will	O
see	O
later	O
on	O
that	O
the	O
true	B
error	I
of	O
a	O
halfspace	B
can	O
be	O
bounded	O
in	O
terms	O
of	O
the	O
margin	B
it	O
has	O
over	O
the	O
training	O
sample	O
(	O
the	O
larger	O
the	O
margin	B
,	O
the	O
smaller	O
the	O
error	O
)	O
,	O
regardless	O
of	O
the	O
euclidean	O
dimension	B
in	O
which	O
this	O
halfspace	B
resides	O
.	O
hard-svm	O
is	O
the	O
learning	O
rule	O
in	O
which	O
we	O
return	O
an	O
erm	O
hyperplane	O
that	O
separates	O
the	O
training	B
set	I
with	O
the	O
largest	O
possible	O
margin	B
.	O
to	O
deﬁne	O
hard-svm	O
formally	O
,	O
we	O
ﬁrst	O
express	O
the	O
distance	O
between	O
a	O
point	O
x	O
to	O
a	O
hyperplane	O
using	O
the	O
parameters	O
deﬁning	O
the	O
halfspace	B
.	O
claim	O
15.1	O
the	O
distance	O
between	O
a	O
point	O
x	O
and	O
the	O
hyperplane	O
deﬁned	O
by	O
(	O
w	O
,	O
b	O
)	O
where	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
=	O
1	O
is	O
|	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b|	O
.	O
proof	O
the	O
distance	O
between	O
a	O
point	O
x	O
and	O
the	O
hyperplane	O
is	O
deﬁned	O
as	O
min	O
{	O
(	O
cid:107	O
)	O
x	O
−	O
v	O
(	O
cid:107	O
)	O
:	O
(	O
cid:104	O
)	O
w	O
,	O
v	O
(	O
cid:105	O
)	O
+	O
b	O
=	O
0	O
}	O
.	O
taking	O
v	O
=	O
x	O
−	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
w	O
we	O
have	O
that	O
(	O
cid:104	O
)	O
w	O
,	O
v	O
(	O
cid:105	O
)	O
+	O
b	O
=	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
−	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
b	O
=	O
0	O
,	O
and	O
(	O
cid:107	O
)	O
x	O
−	O
v	O
(	O
cid:107	O
)	O
=	O
|	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b|	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
=	O
|	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b|	O
.	O
hence	O
,	O
the	O
distance	O
is	O
at	O
most	O
|	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b|	O
.	O
next	O
,	O
take	O
any	O
other	O
point	O
u	O
on	O
the	O
hyperplane	O
,	O
thus	O
(	O
cid:104	O
)	O
w	O
,	O
u	O
(	O
cid:105	O
)	O
+	O
b	O
=	O
0.	O
we	O
have	O
(	O
cid:107	O
)	O
x	O
−	O
u	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:107	O
)	O
x	O
−	O
v	O
+	O
v	O
−	O
u	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:107	O
)	O
x	O
−	O
v	O
(	O
cid:107	O
)	O
2	O
+	O
(	O
cid:107	O
)	O
v	O
−	O
u	O
(	O
cid:107	O
)	O
2	O
+	O
2	O
(	O
cid:104	O
)	O
x	O
−	O
v	O
,	O
v	O
−	O
u	O
(	O
cid:105	O
)	O
≥	O
(	O
cid:107	O
)	O
x	O
−	O
v	O
(	O
cid:107	O
)	O
2	O
+	O
2	O
(	O
cid:104	O
)	O
x	O
−	O
v	O
,	O
v	O
−	O
u	O
(	O
cid:105	O
)	O
=	O
(	O
cid:107	O
)	O
x	O
−	O
v	O
(	O
cid:107	O
)	O
2	O
+	O
2	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
(	O
cid:104	O
)	O
w	O
,	O
v	O
−	O
u	O
(	O
cid:105	O
)	O
=	O
(	O
cid:107	O
)	O
x	O
−	O
v	O
(	O
cid:107	O
)	O
2	O
,	O
where	O
the	O
last	O
equality	O
is	O
because	O
(	O
cid:104	O
)	O
w	O
,	O
v	O
(	O
cid:105	O
)	O
=	O
(	O
cid:104	O
)	O
w	O
,	O
u	O
(	O
cid:105	O
)	O
=	O
−b	O
.	O
hence	O
,	O
the	O
distance	O
204	O
support	O
vector	O
machines	O
between	O
x	O
and	O
u	O
is	O
at	O
least	O
the	O
distance	O
between	O
x	O
and	O
v	O
,	O
which	O
concludes	O
our	O
proof	O
.	O
on	O
the	O
basis	O
of	O
the	O
preceding	O
claim	O
,	O
the	O
closest	O
point	O
in	O
the	O
training	B
set	I
to	O
the	O
separating	O
hyperplane	O
is	O
mini∈	O
[	O
m	O
]	O
|	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b|	O
.	O
therefore	O
,	O
the	O
hard-svm	O
rule	O
is	O
argmax	O
(	O
w	O
,	O
b	O
)	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
=1	O
min	O
i∈	O
[	O
m	O
]	O
|	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b|	O
s.t	O
.	O
∀i	O
,	O
yi	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
>	O
0.	O
whenever	O
there	O
is	O
a	O
solution	O
to	O
the	O
preceding	O
problem	O
(	O
i.e.	O
,	O
we	O
are	O
in	O
the	O
sepa-	O
rable	O
case	O
)	O
,	O
we	O
can	O
write	O
an	O
equivalent	O
problem	O
as	O
follows	O
(	O
see	O
exercise	O
1	O
)	O
:	O
argmax	O
(	O
w	O
,	O
b	O
)	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
=1	O
min	O
i∈	O
[	O
m	O
]	O
yi	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
.	O
(	O
15.1	O
)	O
next	O
,	O
we	O
give	O
another	O
equivalent	O
formulation	O
of	O
the	O
hard-svm	O
rule	O
as	O
a	O
quadratic	O
optimization	O
problem.1	O
hard-svm	O
input	O
:	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
solve	O
:	O
(	O
w0	O
,	O
b0	O
)	O
=	O
argmin	O
(	O
w	O
,	O
b	O
)	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
s.t	O
.	O
∀i	O
,	O
yi	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
≥	O
1	O
(	O
15.2	O
)	O
output	O
:	O
ˆw	O
=	O
w0	O
(	O
cid:107	O
)	O
w0	O
(	O
cid:107	O
)	O
,	O
ˆb	O
=	O
b0	O
(	O
cid:107	O
)	O
w0	O
(	O
cid:107	O
)	O
the	O
lemma	O
that	O
follows	O
shows	O
that	O
the	O
output	O
of	O
hard-svm	O
is	O
indeed	O
the	O
separating	O
hyperplane	O
with	O
the	O
largest	O
margin	B
.	O
intuitively	O
,	O
hard-svm	O
searches	O
for	O
w	O
of	O
minimal	O
norm	O
among	O
all	O
the	O
vectors	O
that	O
separate	O
the	O
data	O
and	O
for	O
which	O
|	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b|	O
≥	O
1	O
for	O
all	O
i.	O
in	O
other	O
words	O
,	O
we	O
enforce	O
the	O
margin	B
to	O
be	O
1	O
,	O
but	O
now	O
the	O
units	O
in	O
which	O
we	O
measure	O
the	O
margin	B
scale	O
with	O
the	O
norm	O
of	O
w.	O
therefore	O
,	O
ﬁnding	O
the	O
largest	O
margin	B
halfspace	O
boils	O
down	O
to	O
ﬁnding	O
w	O
whose	O
norm	O
is	O
minimal	O
.	O
formally	O
:	O
lemma	O
15.2	O
the	O
output	O
of	O
hard-svm	O
is	O
a	O
solution	O
of	O
equation	O
(	O
15.1	O
)	O
.	O
proof	O
let	O
(	O
w	O
(	O
cid:63	O
)	O
,	O
b	O
(	O
cid:63	O
)	O
)	O
be	O
a	O
solution	O
of	O
equation	O
(	O
15.1	O
)	O
and	O
deﬁne	O
the	O
margin	B
achieved	O
by	O
(	O
w	O
(	O
cid:63	O
)	O
,	O
b	O
(	O
cid:63	O
)	O
)	O
to	O
be	O
γ	O
(	O
cid:63	O
)	O
=	O
mini∈	O
[	O
m	O
]	O
yi	O
(	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b	O
(	O
cid:63	O
)	O
)	O
.	O
therefore	O
,	O
for	O
all	O
i	O
we	O
have	O
yi	O
(	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b	O
(	O
cid:63	O
)	O
)	O
≥	O
γ	O
(	O
cid:63	O
)	O
or	O
equivalently	O
yi	O
(	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
γ	O
(	O
cid:63	O
)	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b	O
(	O
cid:63	O
)	O
γ	O
(	O
cid:63	O
)	O
)	O
≥	O
1.	O
hence	O
,	O
the	O
pair	O
(	O
w	O
(	O
cid:63	O
)	O
γ	O
(	O
cid:63	O
)	O
,	O
b	O
(	O
cid:63	O
)	O
γ	O
(	O
cid:63	O
)	O
)	O
satisﬁes	O
the	O
conditions	O
of	O
the	O
quadratic	O
optimization	O
1	O
a	O
quadratic	O
optimization	O
problem	O
is	O
an	O
optimization	O
problem	O
in	O
which	O
the	O
objective	O
is	O
a	O
convex	B
quadratic	O
function	B
and	O
the	O
constraints	O
are	O
linear	O
inequalities	O
.	O
15.1	O
margin	B
and	O
hard-svm	O
205	O
problem	O
given	O
in	O
equation	O
(	O
15.2	O
)	O
.	O
therefore	O
,	O
(	O
cid:107	O
)	O
w0	O
(	O
cid:107	O
)	O
≤	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
for	O
all	O
i	O
,	O
γ	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
=	O
1	O
γ	O
(	O
cid:63	O
)	O
.	O
it	O
follows	O
that	O
yi	O
(	O
(	O
cid:104	O
)	O
ˆw	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
ˆb	O
)	O
=	O
1	O
(	O
cid:107	O
)	O
w0	O
(	O
cid:107	O
)	O
yi	O
(	O
(	O
cid:104	O
)	O
w0	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b0	O
)	O
≥	O
1	O
(	O
cid:107	O
)	O
w0	O
(	O
cid:107	O
)	O
≥	O
γ	O
(	O
cid:63	O
)	O
.	O
since	O
(	O
cid:107	O
)	O
ˆw	O
(	O
cid:107	O
)	O
=	O
1	O
we	O
obtain	O
that	O
(	O
ˆw	O
,	O
ˆb	O
)	O
is	O
an	O
optimal	O
solution	O
of	O
equation	O
(	O
15.1	O
)	O
.	O
15.1.1	O
the	O
homogenous	B
case	O
15.1.2	O
it	O
is	O
often	O
more	O
convenient	O
to	O
consider	O
homogenous	B
halfspaces	O
,	O
namely	O
,	O
halfspaces	O
that	O
pass	O
through	O
the	O
origin	O
and	O
are	O
thus	O
deﬁned	O
by	O
sign	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
,	O
where	O
the	O
bias	B
term	O
b	O
is	O
set	B
to	O
be	O
zero	O
.	O
hard-svm	O
for	O
homogenous	B
halfspaces	O
amounts	O
to	O
solving	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
s.t	O
.	O
∀i	O
,	O
yi	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
≥	O
1.	O
min	O
w	O
(	O
15.3	O
)	O
as	O
we	O
discussed	O
in	O
chapter	O
9	O
,	O
we	O
can	O
reduce	O
the	O
problem	O
of	O
learning	O
nonhomogenous	O
halfspaces	O
to	O
the	O
problem	O
of	O
learning	O
homogenous	O
halfspaces	O
by	O
adding	O
one	O
more	O
feature	B
to	O
each	O
instance	B
of	O
xi	O
,	O
thus	O
increasing	O
the	O
dimension	B
to	O
d	O
+	O
1.	O
note	O
,	O
however	O
,	O
that	O
the	O
optimization	O
problem	O
given	O
in	O
equation	O
(	O
15.2	O
)	O
does	O
not	O
regularize	O
the	O
bias	B
term	O
b	O
,	O
while	O
if	O
we	O
learn	O
a	O
homogenous	B
halfspace	O
in	O
rd+1	O
using	O
equation	O
(	O
15.3	O
)	O
then	O
we	O
regularize	O
the	O
bias	B
term	O
(	O
i.e.	O
,	O
the	O
d	O
+	O
1	O
component	O
of	O
the	O
weight	O
vector	O
)	O
as	O
well	O
.	O
however	O
,	O
regularizing	O
b	O
usually	O
does	O
not	O
make	O
a	O
signiﬁcant	O
diﬀerence	O
to	O
the	O
sample	B
complexity	I
.	O
the	O
sample	B
complexity	I
of	O
hard-svm	O
recall	B
that	O
the	O
vc-dimension	O
of	O
halfspaces	O
in	O
rd	O
is	O
d	O
+	O
1.	O
it	O
follows	O
that	O
the	O
sample	B
complexity	I
of	O
learning	O
halfspaces	O
grows	O
with	O
the	O
dimensionality	O
of	O
the	O
problem	O
.	O
furthermore	O
,	O
the	O
fundamental	O
theorem	O
of	O
learning	O
tells	O
us	O
that	O
if	O
the	O
number	O
of	O
examples	O
is	O
signiﬁcantly	O
smaller	O
than	O
d/	O
then	O
no	O
algorithm	O
can	O
learn	O
an	O
-accurate	O
halfspace	B
.	O
this	O
is	O
problematic	O
when	O
d	O
is	O
very	O
large	O
.	O
to	O
overcome	O
this	O
problem	O
,	O
we	O
will	O
make	O
an	O
additional	O
assumption	O
on	O
the	O
underlying	O
data	O
distribution	O
.	O
in	O
particular	O
,	O
we	O
will	O
deﬁne	O
a	O
“	O
separability	O
with	O
margin	B
γ	O
”	O
assumption	O
and	O
will	O
show	O
that	O
if	O
the	O
data	O
is	O
separable	B
with	O
margin	B
γ	O
then	O
the	O
sample	B
complexity	I
is	O
bounded	O
from	O
above	O
by	O
a	O
function	B
of	O
1/γ2	O
.	O
it	O
follows	O
that	O
even	O
if	O
the	O
dimensionality	O
is	O
very	O
large	O
(	O
or	O
even	O
inﬁnite	O
)	O
,	O
as	O
long	O
as	O
the	O
data	O
adheres	O
to	O
the	O
separability	O
with	O
margin	B
assumption	O
we	O
can	O
still	O
have	O
a	O
small	O
sample	B
complexity	I
.	O
there	O
is	O
no	O
contradiction	O
to	O
the	O
lower	O
bound	O
given	O
in	O
the	O
fundamental	O
theorem	O
of	O
learning	O
because	O
we	O
are	O
now	O
making	O
an	O
additional	O
assumption	O
on	O
the	O
underlying	O
data	O
distribution	O
.	O
before	O
we	O
formally	O
deﬁne	O
the	O
separability	O
with	O
margin	B
assumption	O
,	O
there	O
is	O
a	O
scaling	O
issue	O
we	O
need	O
to	O
resolve	O
.	O
suppose	O
that	O
a	O
training	B
set	I
s	O
=	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
is	O
separable	B
with	O
a	O
margin	B
γ	O
,	O
namely	O
,	O
the	O
maximal	O
objective	O
value	O
of	O
equa-	O
tion	O
(	O
15.1	O
)	O
is	O
at	O
least	O
γ.	O
then	O
,	O
for	O
any	O
positive	O
scalar	O
α	O
>	O
0	O
,	O
the	O
training	B
set	I
206	O
support	O
vector	O
machines	O
s	O
(	O
cid:48	O
)	O
=	O
(	O
αx1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
αxm	O
,	O
ym	O
)	O
is	O
separable	B
with	O
a	O
margin	B
of	O
αγ	O
.	O
that	O
is	O
,	O
a	O
sim-	O
ple	O
scaling	O
of	O
the	O
data	O
can	O
make	O
it	O
separable	B
with	O
an	O
arbitrarily	O
large	O
margin	B
.	O
it	O
follows	O
that	O
in	O
order	O
to	O
give	O
a	O
meaningful	O
deﬁnition	O
of	O
margin	B
we	O
must	O
take	O
into	O
account	O
the	O
scale	O
of	O
the	O
examples	O
as	O
well	O
.	O
one	O
way	O
to	O
formalize	O
this	O
is	O
using	O
the	O
deﬁnition	O
that	O
follows	O
.	O
definition	O
15.3	O
let	O
d	O
be	O
a	O
distribution	O
over	O
rd	O
×	O
{	O
±1	O
}	O
.	O
we	O
say	O
that	O
d	O
is	O
separable	B
with	O
a	O
(	O
γ	O
,	O
ρ	O
)	O
-margin	O
if	O
there	O
exists	O
(	O
w	O
(	O
cid:63	O
)	O
,	O
b	O
(	O
cid:63	O
)	O
)	O
such	O
that	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
=	O
1	O
and	O
such	O
that	O
with	O
probability	O
1	O
over	O
the	O
choice	O
of	O
(	O
x	O
,	O
y	O
)	O
∼	O
d	O
we	O
have	O
that	O
y	O
(	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b	O
(	O
cid:63	O
)	O
)	O
≥	O
γ	O
and	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
≤	O
ρ.	O
similarly	O
,	O
we	O
say	O
that	O
d	O
is	O
separable	B
with	O
a	O
(	O
γ	O
,	O
ρ	O
)	O
-margin	O
using	O
a	O
homogenous	B
halfspace	O
if	O
the	O
preceding	O
holds	O
with	O
a	O
halfspace	B
of	O
the	O
form	O
(	O
w	O
(	O
cid:63	O
)	O
,	O
0	O
)	O
.	O
in	O
the	O
advanced	O
part	O
of	O
the	O
book	O
(	O
chapter	O
26	O
)	O
,	O
we	O
will	O
prove	O
that	O
the	O
sample	B
complexity	I
of	O
hard-svm	O
depends	O
on	O
(	O
ρ/γ	O
)	O
2	O
and	O
is	O
independent	O
of	O
the	O
dimension	B
d.	O
in	O
particular	O
,	O
theorem	O
26.13	O
in	O
section	O
26.3	O
states	O
the	O
following	O
:	O
theorem	O
15.4	O
let	O
d	O
be	O
a	O
distribution	O
over	O
rd×	O
{	O
±1	O
}	O
that	O
satisﬁes	O
the	O
(	O
γ	O
,	O
ρ	O
)	O
-	O
separability	O
with	O
margin	B
assumption	O
using	O
a	O
homogenous	B
halfspace	O
.	O
then	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
the	O
choice	O
of	O
a	O
training	B
set	I
of	O
size	O
m	O
,	O
the	O
0-1	O
error	O
of	O
the	O
output	O
of	O
hard-svm	O
is	O
at	O
most	O
(	O
cid:114	O
)	O
(	O
cid:114	O
)	O
4	O
(	O
ρ/γ	O
)	O
2	O
m	O
+	O
2	O
log	O
(	O
2/δ	O
)	O
m	O
.	O
remark	O
15.1	O
(	O
margin	B
and	O
the	O
perceptron	O
)	O
in	O
section	O
9.1.2	O
we	O
have	O
described	O
and	O
analyzed	O
the	O
perceptron	O
algorithm	O
for	O
ﬁnding	O
an	O
erm	O
hypothesis	B
with	O
respect	O
to	O
the	O
class	O
of	O
halfspaces	O
.	O
in	O
particular	O
,	O
in	O
theorem	O
9.1	O
we	O
upper	O
bounded	O
the	O
number	O
of	O
updates	O
the	O
perceptron	O
might	O
make	O
on	O
a	O
given	O
training	B
set	I
.	O
it	O
can	O
be	O
shown	O
(	O
see	O
exercise	O
2	O
)	O
that	O
the	O
upper	O
bound	O
is	O
exactly	O
(	O
ρ/γ	O
)	O
2	O
,	O
where	O
ρ	O
is	O
the	O
radius	O
of	O
examples	O
and	O
γ	O
is	O
the	O
margin	B
.	O
15.2	O
soft-svm	O
and	O
norm	O
regularization	B
the	O
hard-svm	O
formulation	O
assumes	O
that	O
the	O
training	B
set	I
is	O
linearly	O
separable	B
,	O
which	O
is	O
a	O
rather	O
strong	O
assumption	O
.	O
soft-svm	O
can	O
be	O
viewed	O
as	O
a	O
relaxation	O
of	O
the	O
hard-svm	O
rule	O
that	O
can	O
be	O
applied	O
even	O
if	O
the	O
training	B
set	I
is	O
not	O
linearly	O
separable	B
.	O
the	O
optimization	O
problem	O
in	O
equation	O
(	O
15.2	O
)	O
enforces	O
the	O
hard	O
constraints	O
yi	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
≥	O
1	O
for	O
all	O
i.	O
a	O
natural	O
relaxation	O
is	O
to	O
allow	O
the	O
constraint	O
to	O
be	O
violated	O
for	O
some	O
of	O
the	O
examples	O
in	O
the	O
training	B
set	I
.	O
this	O
can	O
be	O
modeled	O
by	O
introducing	O
nonnegative	O
slack	O
variables	O
,	O
ξ1	O
,	O
.	O
.	O
.	O
,	O
ξm	O
,	O
and	O
replacing	O
each	O
constraint	O
yi	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
≥	O
1	O
by	O
the	O
constraint	O
yi	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
≥	O
1−	O
ξi	O
.	O
that	O
is	O
,	O
ξi	O
measures	O
by	O
how	O
much	O
the	O
constraint	O
yi	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+b	O
)	O
≥	O
1	O
is	O
being	O
violated	O
.	O
soft-svm	O
jointly	O
minimizes	O
the	O
norm	O
of	O
w	O
(	O
corresponding	O
to	O
the	O
margin	B
)	O
and	O
the	O
average	O
of	O
ξi	O
(	O
corresponding	O
to	O
the	O
violations	O
of	O
the	O
constraints	O
)	O
.	O
the	O
tradeoﬀ	O
between	O
the	O
two	O
15.2	O
soft-svm	O
and	O
norm	O
regularization	B
207	O
terms	O
is	O
controlled	O
by	O
a	O
parameter	O
λ.	O
this	O
leads	O
to	O
the	O
soft-svm	O
optimization	O
problem	O
:	O
soft-svm	O
input	O
:	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
parameter	O
:	O
λ	O
>	O
0	O
solve	O
:	O
(	O
cid:32	O
)	O
(	O
cid:33	O
)	O
m	O
(	O
cid:88	O
)	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
1	O
m	O
min	O
w	O
,	O
b	O
,	O
ξ	O
s.t	O
.	O
∀i	O
,	O
yi	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
≥	O
1	O
−	O
ξi	O
and	O
ξi	O
≥	O
0	O
i=1	O
ξi	O
(	O
15.4	O
)	O
output	O
:	O
w	O
,	O
b	O
we	O
can	O
rewrite	O
equation	O
(	O
15.4	O
)	O
as	O
a	O
regularized	B
loss	I
minimization	I
problem	O
.	O
recall	B
the	O
deﬁnition	O
of	O
the	O
hinge	B
loss	I
:	O
(	O
cid:96	O
)	O
hinge	O
(	O
(	O
w	O
,	O
b	O
)	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
max	O
{	O
0	O
,	O
1	O
−	O
y	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
}	O
.	O
given	O
(	O
w	O
,	O
b	O
)	O
and	O
a	O
training	B
set	I
s	O
,	O
the	O
averaged	O
hinge	B
loss	I
on	O
s	O
is	O
denoted	O
by	O
lhinge	O
(	O
(	O
w	O
,	O
b	O
)	O
)	O
.	O
now	O
,	O
consider	O
the	O
regularized	B
loss	I
minimization	I
problem	O
:	O
s	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
lhinge	O
s	O
(	O
(	O
w	O
,	O
b	O
)	O
)	O
min	O
w	O
,	O
b	O
.	O
(	O
15.5	O
)	O
claim	O
15.5	O
equation	O
(	O
15.4	O
)	O
and	O
equation	O
(	O
15.5	O
)	O
are	O
equivalent	O
.	O
proof	O
fix	O
some	O
w	O
,	O
b	O
and	O
consider	O
the	O
minimization	O
over	O
ξ	O
in	O
equation	O
(	O
15.4	O
)	O
.	O
fix	O
some	O
i.	O
since	O
ξi	O
must	O
be	O
nonnegative	O
,	O
the	O
best	O
assignment	O
to	O
ξi	O
would	O
be	O
0	O
if	O
yi	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
≥	O
1	O
and	O
would	O
be	O
1	O
−	O
yi	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
otherwise	O
.	O
in	O
other	O
words	O
,	O
ξi	O
=	O
(	O
cid:96	O
)	O
hinge	O
(	O
(	O
w	O
,	O
b	O
)	O
,	O
(	O
xi	O
,	O
yi	O
)	O
)	O
for	O
all	O
i	O
,	O
and	O
the	O
claim	O
follows	O
.	O
we	O
therefore	O
see	O
that	O
soft-svm	O
falls	O
into	O
the	O
paradigm	O
of	O
regularized	B
loss	I
minimization	I
that	O
we	O
studied	O
in	O
the	O
previous	O
chapter	O
.	O
a	O
soft-svm	O
algorithm	O
,	O
that	O
is	O
,	O
a	O
solution	O
for	O
equation	O
(	O
15.5	O
)	O
,	O
has	O
a	O
bias	B
toward	O
low	O
norm	O
separators	O
.	O
the	O
objective	O
function	B
that	O
we	O
aim	O
to	O
minimize	O
in	O
equation	O
(	O
15.5	O
)	O
penalizes	O
not	O
only	O
for	O
training	O
errors	O
but	O
also	O
for	O
large	O
norm	O
.	O
it	O
is	O
often	O
more	O
convenient	O
to	O
consider	O
soft-svm	O
for	O
learning	O
a	O
homogenous	B
halfspace	O
,	O
where	O
the	O
bias	B
term	O
b	O
is	O
set	B
to	O
be	O
zero	O
,	O
which	O
yields	O
the	O
following	O
optimization	O
problem	O
:	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
lhinge	O
s	O
(	O
w	O
)	O
min	O
w	O
,	O
(	O
15.6	O
)	O
where	O
lhinge	O
s	O
(	O
w	O
)	O
=	O
m	O
(	O
cid:88	O
)	O
i=1	O
1	O
m	O
max	O
{	O
0	O
,	O
1	O
−	O
y	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
}	O
.	O
208	O
support	O
vector	O
machines	O
15.2.1	O
the	O
sample	B
complexity	I
of	O
soft-svm	O
we	O
now	O
analyze	O
the	O
sample	B
complexity	I
of	O
soft-svm	O
for	O
the	O
case	O
of	O
homogenous	B
halfspaces	O
(	O
namely	O
,	O
the	O
output	O
of	O
equation	O
(	O
15.6	O
)	O
)	O
.	O
in	O
corollary	O
13.8	O
we	O
derived	O
a	O
generalization	O
bound	O
for	O
the	O
regularized	B
loss	I
minimization	I
framework	O
assuming	O
that	O
the	O
loss	B
function	I
is	O
convex	B
and	O
lipschitz	O
.	O
we	O
have	O
already	O
shown	O
that	O
the	O
hinge	B
loss	I
is	O
convex	B
so	O
it	O
is	O
only	O
left	O
to	O
analyze	O
the	O
lipschitzness	O
of	O
the	O
hinge	B
loss	I
.	O
claim	O
15.6	O
let	O
f	O
(	O
w	O
)	O
=	O
max	O
{	O
0	O
,	O
1	O
−	O
y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
}	O
.	O
then	O
,	O
f	O
is	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
-lipschitz	O
.	O
it	O
is	O
easy	O
to	O
verify	O
that	O
any	O
subgradient	O
of	O
f	O
at	O
w	O
is	O
of	O
the	O
form	O
αx	O
where	O
proof	O
|α|	O
≤	O
1.	O
the	O
claim	O
now	O
follows	O
from	O
lemma	O
14.7.	O
corollary	O
13.8	O
therefore	O
yields	O
the	O
following	O
:	O
corollary	O
15.7	O
let	O
d	O
be	O
a	O
distribution	O
over	O
x	O
×	O
{	O
0	O
,	O
1	O
}	O
,	O
where	O
x	O
=	O
{	O
x	O
:	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
≤	O
ρ	O
}	O
.	O
consider	O
running	O
soft-svm	O
(	O
equation	O
(	O
15.6	O
)	O
)	O
on	O
a	O
training	B
set	I
s	O
∼	O
dm	O
and	O
let	O
a	O
(	O
s	O
)	O
be	O
the	O
solution	O
of	O
soft-svm	O
.	O
then	O
,	O
for	O
every	O
u	O
,	O
e	O
s∼dm	O
[	O
lhinged	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
≤	O
lhinged	O
(	O
u	O
)	O
+	O
λ	O
(	O
cid:107	O
)	O
u	O
(	O
cid:107	O
)	O
2	O
+	O
2ρ2	O
λ	O
m	O
.	O
furthermore	O
,	O
since	O
the	O
hinge	B
loss	I
upper	O
bounds	O
the	O
0−1	O
loss	B
we	O
also	O
have	O
e	O
s∼dm	O
[	O
l0−1d	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
≤	O
lhinged	O
(	O
u	O
)	O
+	O
λ	O
(	O
cid:107	O
)	O
u	O
(	O
cid:107	O
)	O
2	O
+	O
2ρ2	O
λ	O
m	O
.	O
(	O
cid:113	O
)	O
2ρ2	O
last	O
,	O
for	O
every	O
b	O
>	O
0	O
,	O
if	O
we	O
set	B
λ	O
=	O
b2m	O
then	O
e	O
s∼dm	O
[	O
l0−1d	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
≤	O
e	O
s∼dm	O
[	O
lhinged	O
(	O
a	O
(	O
s	O
)	O
)	O
]	O
≤	O
min	O
w	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
≤b	O
lhinged	O
(	O
w	O
)	O
+	O
(	O
cid:114	O
)	O
8ρ2b2	O
m	O
.	O
we	O
therefore	O
see	O
that	O
we	O
can	O
control	O
the	O
sample	B
complexity	I
of	O
learning	O
a	O
half-	O
space	O
as	O
a	O
function	B
of	O
the	O
norm	O
of	O
that	O
halfspace	B
,	O
independently	O
of	O
the	O
euclidean	O
dimension	B
of	O
the	O
space	O
over	O
which	O
the	O
halfspace	B
is	O
deﬁned	O
.	O
this	O
becomes	O
highly	O
signiﬁcant	O
when	O
we	O
learn	O
via	O
embeddings	O
into	O
high	O
dimensional	O
feature	B
spaces	O
,	O
as	O
we	O
will	O
consider	O
in	O
the	O
next	O
chapter	O
.	O
remark	O
15.2	O
the	O
condition	O
that	O
x	O
will	O
contain	O
vectors	O
with	O
a	O
bounded	O
norm	O
follows	O
from	O
the	O
requirement	O
that	O
the	O
loss	B
function	I
will	O
be	O
lipschitz	O
.	O
this	O
is	O
not	O
just	O
a	O
technicality	O
.	O
as	O
we	O
discussed	O
before	O
,	O
separation	O
with	O
large	O
margin	B
is	O
meaningless	O
without	O
imposing	O
a	O
restriction	O
on	O
the	O
scale	O
of	O
the	O
instances	O
.	O
in-	O
deed	O
,	O
without	O
a	O
constraint	O
on	O
the	O
scale	O
,	O
we	O
can	O
always	O
enlarge	O
the	O
margin	B
by	O
multiplying	O
all	O
instances	O
by	O
a	O
large	O
scalar	O
.	O
15.2.2	O
margin	B
and	O
norm-based	O
bounds	O
versus	O
dimension	B
the	O
bounds	O
we	O
have	O
derived	O
for	O
hard-svm	O
and	O
soft-svm	O
do	O
not	O
depend	O
on	O
the	O
dimension	B
of	O
the	O
instance	B
space	I
.	O
instead	O
,	O
the	O
bounds	O
depend	O
on	O
the	O
norm	O
of	O
the	O
15.2	O
soft-svm	O
and	O
norm	O
regularization	B
209	O
examples	O
,	O
ρ	O
,	O
the	O
norm	O
of	O
the	O
halfspace	B
b	O
(	O
or	O
equivalently	O
the	O
margin	B
parameter	O
γ	O
)	O
and	O
,	O
in	O
the	O
nonseparable	O
case	O
,	O
the	O
bounds	O
also	O
depend	O
on	O
the	O
minimum	O
hinge	B
loss	I
of	O
all	O
halfspaces	O
of	O
norm	O
≤	O
b.	O
in	O
contrast	O
,	O
the	O
vc-dimension	O
of	O
the	O
class	O
of	O
homogenous	B
halfspaces	O
is	O
d	O
,	O
which	O
implies	O
that	O
the	O
error	O
of	O
an	O
erm	O
hypothesis	B
decreases	O
as	O
(	O
cid:112	O
)	O
d/m	O
does	O
.	O
we	O
now	O
give	O
an	O
example	O
in	O
which	O
ρ2b2	O
(	O
cid:28	O
)	O
d	O
;	O
hence	O
the	O
bound	O
given	O
in	O
corollary	O
15.7	O
is	O
much	O
better	O
than	O
the	O
vc	O
bound	O
.	O
consider	O
the	O
problem	O
of	O
learning	O
to	O
classify	O
a	O
short	O
text	O
document	O
according	O
to	O
its	O
topic	O
,	O
say	O
,	O
whether	O
the	O
document	O
is	O
about	O
sports	O
or	O
not	O
.	O
we	O
ﬁrst	O
need	O
to	O
represent	O
documents	O
as	O
vectors	O
.	O
one	O
simple	O
yet	O
eﬀective	O
way	O
is	O
to	O
use	O
a	O
bag-	O
of-words	O
representation	O
.	O
that	O
is	O
,	O
we	O
deﬁne	O
a	O
dictionary	O
of	O
words	O
and	O
set	B
the	O
dimension	B
d	O
to	O
be	O
the	O
number	O
of	O
words	O
in	O
the	O
dictionary	O
.	O
given	O
a	O
document	O
,	O
we	O
represent	O
it	O
as	O
a	O
vector	O
x	O
∈	O
{	O
0	O
,	O
1	O
}	O
d	O
,	O
where	O
xi	O
=	O
1	O
if	O
the	O
i	O
’	O
th	O
word	O
in	O
the	O
dictionary	O
appears	O
in	O
the	O
document	O
and	O
xi	O
=	O
0	O
otherwise	O
.	O
therefore	O
,	O
for	O
this	O
problem	O
,	O
the	O
value	O
of	O
ρ2	O
will	O
be	O
the	O
maximal	O
number	O
of	O
distinct	O
words	O
in	O
a	O
given	O
document	O
.	O
a	O
halfspace	B
for	O
this	O
problem	O
assigns	O
weights	O
to	O
words	O
.	O
it	O
is	O
natural	O
to	O
assume	O
that	O
by	O
assigning	O
positive	O
and	O
negative	O
weights	O
to	O
a	O
few	O
dozen	O
words	O
we	O
will	O
be	O
able	O
to	O
determine	O
whether	O
a	O
given	O
document	O
is	O
about	O
sports	O
or	O
not	O
with	O
reasonable	O
accuracy	B
.	O
therefore	O
,	O
for	O
this	O
problem	O
,	O
the	O
value	O
of	O
b2	O
can	O
be	O
set	B
to	O
be	O
less	O
than	O
100.	O
overall	O
,	O
it	O
is	O
reasonable	O
to	O
say	O
that	O
the	O
value	O
of	O
b2ρ2	O
is	O
smaller	O
than	O
10,000.	O
on	O
the	O
other	O
hand	O
,	O
a	O
typical	O
size	O
of	O
a	O
dictionary	O
is	O
much	O
larger	O
than	O
10,000.	O
for	O
example	O
,	O
there	O
are	O
more	O
than	O
100,000	O
distinct	O
words	O
in	O
english	O
.	O
we	O
have	O
therefore	O
shown	O
a	O
problem	O
in	O
which	O
there	O
can	O
be	O
an	O
order	O
of	O
magnitude	O
diﬀerence	O
between	O
learning	O
a	O
halfspace	B
with	O
the	O
svm	O
rule	O
and	O
learning	O
a	O
halfspace	B
using	O
the	O
vanilla	O
erm	O
rule	O
.	O
of	O
course	O
,	O
it	O
is	O
possible	O
to	O
construct	O
problems	O
in	O
which	O
the	O
svm	O
bound	O
will	O
be	O
worse	O
than	O
the	O
vc	O
bound	O
.	O
when	O
we	O
use	O
svm	O
,	O
we	O
in	O
fact	O
introduce	O
another	O
form	O
of	O
inductive	B
bias	I
–	O
we	O
prefer	O
large	O
margin	B
halfspaces	O
.	O
while	O
this	O
induc-	O
tive	O
bias	B
can	O
signiﬁcantly	O
decrease	O
our	O
estimation	B
error	I
,	O
it	O
can	O
also	O
enlarge	O
the	O
approximation	B
error	I
.	O
15.2.3	O
the	O
ramp	O
loss*	O
the	O
margin-based	O
bounds	O
we	O
have	O
derived	O
in	O
corollary	O
15.7	O
rely	O
on	O
the	O
fact	O
that	O
we	O
minimize	O
the	O
hinge	B
loss	I
.	O
as	O
we	O
have	O
shown	O
in	O
the	O
previous	O
subsection	O
,	O
the	O
term	O
(	O
cid:112	O
)	O
ρ2b2/m	O
can	O
be	O
much	O
smaller	O
than	O
the	O
corresponding	O
term	O
in	O
the	O
vc	O
bound	O
,	O
(	O
cid:112	O
)	O
d/m	O
.	O
however	O
,	O
the	O
approximation	B
error	I
in	O
corollary	O
15.7	O
is	O
measured	O
with	O
respect	O
to	O
the	O
hinge	B
loss	I
while	O
the	O
approximation	B
error	I
in	O
vc	O
bounds	O
is	O
measured	O
with	O
respect	O
to	O
the	O
0−1	O
loss	B
.	O
since	O
the	O
hinge	B
loss	I
upper	O
bounds	O
the	O
0−1	O
loss	B
,	O
the	O
approximation	B
error	I
with	O
respect	O
to	O
the	O
0−1	O
loss	B
will	O
never	O
exceed	O
that	O
of	O
the	O
hinge	B
loss	I
.	O
(	O
cid:112	O
)	O
ρ2b2/m	O
for	O
the	O
0−1	O
loss	B
.	O
this	O
follows	O
from	O
the	O
fact	O
that	O
the	O
0−1	O
loss	B
is	O
scale	O
it	O
is	O
not	O
possible	O
to	O
derive	O
bounds	O
that	O
involve	O
the	O
estimation	B
error	I
term	O
210	O
support	O
vector	O
machines	O
insensitive	O
,	O
and	O
therefore	O
there	O
is	O
no	O
meaning	O
to	O
the	O
norm	O
of	O
w	O
or	O
its	O
margin	B
when	O
we	O
measure	O
error	O
with	O
the	O
0−1	O
loss	B
.	O
however	O
,	O
it	O
is	O
possible	O
to	O
deﬁne	O
a	O
loss	B
function	I
that	O
on	O
one	O
hand	O
it	O
is	O
scale	O
sensitive	O
and	O
thus	O
enjoys	O
the	O
estimation	B
error	I
(	O
cid:112	O
)	O
ρ2b2/m	O
while	O
on	O
the	O
other	O
hand	O
it	O
is	O
more	O
similar	O
to	O
the	O
0−1	O
loss	B
.	O
one	O
option	O
is	O
the	O
ramp	B
loss	I
,	O
deﬁned	O
as	O
(	O
cid:96	O
)	O
ramp	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
min	O
{	O
1	O
,	O
(	O
cid:96	O
)	O
hinge	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
}	O
=	O
min	O
{	O
1	O
,	O
max	O
{	O
0	O
,	O
1	O
−	O
y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
}	O
}	O
.	O
the	O
ramp	B
loss	I
penalizes	O
mistakes	O
in	O
the	O
same	O
way	O
as	O
the	O
0−1	O
loss	B
and	O
does	O
not	O
penalize	O
examples	O
that	O
are	O
separated	O
with	O
margin	B
.	O
the	O
diﬀerence	O
between	O
the	O
ramp	B
loss	I
and	O
the	O
0−1	O
loss	B
is	O
only	O
with	O
respect	O
to	O
examples	O
that	O
are	O
correctly	O
classiﬁed	O
but	O
not	O
with	O
a	O
signiﬁcant	O
margin	B
.	O
generalization	B
bounds	I
for	O
the	O
ramp	B
loss	I
are	O
given	O
in	O
the	O
advanced	O
part	O
of	O
this	O
book	O
(	O
see	O
appendix	O
26.3	O
)	O
.	O
(	O
cid:96	O
)	O
hinge	O
(	O
cid:96	O
)	O
0−1	O
(	O
cid:96	O
)	O
ramp	O
1	O
1	O
y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
the	O
reason	O
svm	O
relies	O
on	O
the	O
hinge	B
loss	I
and	O
not	O
on	O
the	O
ramp	B
loss	I
is	O
that	O
the	O
hinge	B
loss	I
is	O
convex	B
and	O
,	O
therefore	O
,	O
from	O
the	O
computational	O
point	O
of	O
view	O
,	O
minimizing	O
the	O
hinge	B
loss	I
can	O
be	O
performed	O
eﬃciently	O
.	O
in	O
contrast	O
,	O
the	O
problem	O
of	O
minimizing	O
the	O
ramp	B
loss	I
is	O
computationally	O
intractable	O
.	O
15.3	O
optimality	O
conditions	O
and	O
“	O
support	B
vectors	I
”	O
*	O
the	O
name	O
“	O
support	O
vector	O
machine	O
”	O
stems	O
from	O
the	O
fact	O
that	O
the	O
solution	O
of	O
hard-svm	O
,	O
w0	O
,	O
is	O
supported	O
by	O
(	O
i.e.	O
,	O
is	O
in	O
the	O
linear	O
span	O
of	O
)	O
the	O
examples	O
that	O
are	O
exactly	O
at	O
distance	O
1/	O
(	O
cid:107	O
)	O
w0	O
(	O
cid:107	O
)	O
from	O
the	O
separating	O
hyperplane	O
.	O
these	O
vectors	O
are	O
therefore	O
called	O
support	B
vectors	I
.	O
to	O
see	O
this	O
,	O
we	O
rely	O
on	O
fritz	O
john	O
optimality	O
conditions	O
.	O
theorem	O
15.8	O
let	O
w0	O
be	O
as	O
deﬁned	O
in	O
equation	O
(	O
15.3	O
)	O
and	O
let	O
i	O
=	O
{	O
i	O
:	O
|	O
(	O
cid:104	O
)	O
w0	O
,	O
xi	O
(	O
cid:105	O
)	O
|	O
=	O
1	O
}	O
.	O
then	O
,	O
there	O
exist	O
coeﬃcients	O
α1	O
,	O
.	O
.	O
.	O
,	O
αm	O
such	O
that	O
the	O
examples	O
{	O
xi	O
:	O
i	O
∈	O
i	O
}	O
are	O
called	O
support	B
vectors	I
.	O
the	O
proof	O
of	O
this	O
theorem	O
follows	O
by	O
applying	O
the	O
following	O
lemma	O
to	O
equa-	O
tion	O
(	O
15.3	O
)	O
.	O
(	O
cid:88	O
)	O
i∈i	O
w0	O
=	O
αixi	O
.	O
15.4	O
duality*	O
211	O
lemma	O
15.9	O
(	O
fritz	O
john	O
)	O
suppose	O
that	O
w	O
(	O
cid:63	O
)	O
∈	O
argmin	O
f	O
(	O
w	O
)	O
w	O
s.t	O
.	O
∀i	O
∈	O
[	O
m	O
]	O
,	O
gi	O
(	O
w	O
)	O
≤	O
0	O
,	O
∇f	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
+	O
(	O
cid:80	O
)	O
where	O
f	O
,	O
g1	O
,	O
.	O
.	O
.	O
,	O
gm	O
are	O
diﬀerentiable	O
.	O
then	O
,	O
there	O
exists	O
α	O
∈	O
rm	O
such	O
that	O
i∈i	O
αi∇gi	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
=	O
0	O
,	O
where	O
i	O
=	O
{	O
i	O
:	O
gi	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
=	O
0	O
}	O
.	O
15.4	O
duality*	O
historically	O
,	O
many	O
of	O
the	O
properties	O
of	O
svm	O
have	O
been	O
obtained	O
by	O
considering	O
the	O
dual	O
of	O
equation	O
(	O
15.3	O
)	O
.	O
our	O
presentation	O
of	O
svm	O
does	O
not	O
rely	O
on	O
duality	B
.	O
for	O
completeness	O
,	O
we	O
present	O
in	O
the	O
following	O
how	O
to	O
derive	O
the	O
dual	O
of	O
equa-	O
tion	O
(	O
15.3	O
)	O
.	O
we	O
start	O
by	O
rewriting	O
the	O
problem	O
in	O
an	O
equivalent	O
form	O
as	O
follows	O
.	O
consider	O
the	O
function	B
g	O
(	O
w	O
)	O
=	O
max	O
α∈rm	O
:	O
α≥0	O
m	O
(	O
cid:88	O
)	O
i=1	O
(	O
cid:40	O
)	O
αi	O
(	O
1	O
−	O
yi	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
)	O
=	O
(	O
cid:0	O
)	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
g	O
(	O
w	O
)	O
(	O
cid:1	O
)	O
.	O
if	O
∀i	O
,	O
yi	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
≥	O
1	O
0	O
∞	O
otherwise	O
.	O
(	O
15.7	O
)	O
we	O
can	O
therefore	O
rewrite	O
equation	O
(	O
15.3	O
)	O
as	O
min	O
w	O
(	O
cid:32	O
)	O
(	O
cid:32	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
m	O
(	O
cid:88	O
)	O
i=1	O
rearranging	O
the	O
preceding	O
we	O
obtain	O
that	O
equation	O
(	O
15.3	O
)	O
can	O
be	O
rewritten	O
as	O
the	O
problem	O
min	O
w	O
max	O
α∈rm	O
:	O
α≥0	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
1	O
2	O
αi	O
(	O
1	O
−	O
yi	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
)	O
.	O
(	O
15.8	O
)	O
now	O
suppose	O
that	O
we	O
ﬂip	O
the	O
order	O
of	O
min	O
and	O
max	O
in	O
the	O
above	O
equation	O
.	O
this	O
can	O
only	O
decrease	O
the	O
objective	O
value	O
(	O
see	O
exercise	O
4	O
)	O
,	O
and	O
we	O
have	O
(	O
cid:32	O
)	O
m	O
(	O
cid:88	O
)	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
1	O
2	O
(	O
cid:32	O
)	O
i=1	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
1	O
2	O
m	O
(	O
cid:88	O
)	O
i=1	O
min	O
w	O
max	O
α∈rm	O
:	O
α≥0	O
≥	O
max	O
α∈rm	O
:	O
α≥0	O
min	O
w	O
(	O
cid:33	O
)	O
(	O
cid:33	O
)	O
αi	O
(	O
1	O
−	O
yi	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
)	O
αi	O
(	O
1	O
−	O
yi	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
)	O
.	O
the	O
preceding	O
inequality	O
is	O
called	O
weak	B
duality	I
.	O
it	O
turns	O
out	O
that	O
in	O
our	O
case	O
,	O
strong	B
duality	I
also	O
holds	O
;	O
namely	O
,	O
the	O
inequality	O
holds	O
with	O
equality	O
.	O
therefore	O
,	O
the	O
dual	O
problem	O
is	O
(	O
cid:33	O
)	O
(	O
cid:33	O
)	O
max	O
α∈rm	O
:	O
α≥0	O
min	O
w	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
1	O
2	O
αi	O
(	O
1	O
−	O
yi	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
)	O
.	O
(	O
15.9	O
)	O
we	O
can	O
simplify	O
the	O
dual	O
problem	O
by	O
noting	O
that	O
once	O
α	O
is	O
ﬁxed	O
,	O
the	O
optimization	O
212	O
support	O
vector	O
machines	O
problem	O
with	O
respect	O
to	O
w	O
is	O
unconstrained	O
and	O
the	O
objective	O
is	O
diﬀerentiable	O
;	O
thus	O
,	O
at	O
the	O
optimum	O
,	O
the	O
gradient	B
equals	O
zero	O
:	O
m	O
(	O
cid:88	O
)	O
αiyixi	O
=	O
0	O
⇒	O
w	O
=	O
αiyixi	O
.	O
i=1	O
i=1	O
this	O
shows	O
us	O
that	O
the	O
solution	O
must	O
be	O
in	O
the	O
linear	O
span	O
of	O
the	O
examples	O
,	O
a	O
fact	O
we	O
will	O
use	O
later	O
to	O
derive	O
svm	O
with	O
kernels	B
.	O
plugging	O
the	O
preceding	O
into	O
equation	O
(	O
15.9	O
)	O
we	O
obtain	O
that	O
the	O
dual	O
problem	O
can	O
be	O
rewritten	O
as	O
w	O
−	O
m	O
(	O
cid:88	O
)	O
	O
1	O
2	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
max	O
α∈rm	O
:	O
α≥0	O
αiyixi	O
+	O
αi	O
αjyjxj	O
,	O
xi	O
rearranging	O
yields	O
the	O
dual	O
problem	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
2	O
	O
m	O
(	O
cid:88	O
)	O
i=1	O
m	O
(	O
cid:88	O
)	O
i=1	O
(	O
cid:42	O
)	O
(	O
cid:88	O
)	O
j	O
1	O
−	O
yi	O
m	O
(	O
cid:88	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
j=1	O
(	O
cid:43	O
)	O
	O
.	O
	O
.	O
(	O
15.10	O
)	O
(	O
15.11	O
)	O
max	O
α∈rm	O
:	O
α≥0	O
αi	O
−	O
1	O
2	O
αiαjyiyj	O
(	O
cid:104	O
)	O
xj	O
,	O
xi	O
(	O
cid:105	O
)	O
note	O
that	O
the	O
dual	O
problem	O
only	O
involves	O
inner	O
products	O
between	O
instances	O
and	O
does	O
not	O
require	O
direct	O
access	O
to	O
speciﬁc	O
elements	O
within	O
an	O
instance	B
.	O
this	O
prop-	O
erty	O
is	O
important	O
when	O
implementing	O
svm	O
with	O
kernels	B
,	O
as	O
we	O
will	O
discuss	O
in	O
the	O
next	O
chapter	O
.	O
15.5	O
implementing	O
soft-svm	O
using	O
sgd	O
in	O
this	O
section	O
we	O
describe	O
a	O
very	O
simple	O
algorithm	O
for	O
solving	O
the	O
optimization	O
problem	O
of	O
soft-svm	O
,	O
namely	O
,	O
(	O
cid:32	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
min	O
w	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
λ	O
2	O
1	O
m	O
max	O
{	O
0	O
,	O
1	O
−	O
y	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
}	O
.	O
(	O
15.12	O
)	O
(	O
cid:33	O
)	O
we	O
rely	O
on	O
the	O
sgd	O
framework	O
for	O
solving	O
regularized	B
loss	I
minimization	I
prob-	O
lems	O
,	O
as	O
described	O
in	O
section	O
14.5.3.	O
recall	B
that	O
,	O
on	O
the	O
basis	O
of	O
equation	O
(	O
14.15	O
)	O
,	O
we	O
can	O
rewrite	O
the	O
update	O
rule	O
of	O
sgd	O
as	O
w	O
(	O
t+1	O
)	O
=	O
−	O
1	O
λ	O
t	O
t	O
(	O
cid:88	O
)	O
j=1	O
vj	O
,	O
where	O
vj	O
is	O
a	O
subgradient	O
of	O
the	O
loss	B
function	I
at	O
w	O
(	O
j	O
)	O
on	O
the	O
random	O
example	O
chosen	O
at	O
iteration	O
j.	O
for	O
the	O
hinge	B
loss	I
,	O
given	O
an	O
example	O
(	O
x	O
,	O
y	O
)	O
,	O
we	O
can	O
choose	O
vj	O
to	O
be	O
0	O
if	O
y	O
(	O
cid:104	O
)	O
w	O
(	O
j	O
)	O
,	O
x	O
(	O
cid:105	O
)	O
≥	O
1	O
and	O
vj	O
=	O
−y	O
x	O
otherwise	O
(	O
see	O
example	O
14.2	O
)	O
.	O
denoting	O
θ	O
(	O
t	O
)	O
=	O
−	O
(	O
cid:80	O
)	O
j	O
<	O
t	O
vj	O
we	O
obtain	O
the	O
following	O
procedure	O
.	O
15.6	O
summary	O
213	O
sgd	O
for	O
solving	O
soft-svm	O
goal	O
:	O
solve	O
equation	O
(	O
15.12	O
)	O
parameter	O
:	O
t	O
initialize	O
:	O
θ	O
(	O
1	O
)	O
=	O
0	O
for	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
λ	O
t	O
θ	O
(	O
t	O
)	O
let	O
w	O
(	O
t	O
)	O
=	O
1	O
choose	O
i	O
uniformly	O
at	O
random	O
from	O
[	O
m	O
]	O
if	O
(	O
yi	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
xi	O
(	O
cid:105	O
)	O
<	O
1	O
)	O
set	B
θ	O
(	O
t+1	O
)	O
=	O
θ	O
(	O
t	O
)	O
+	O
yixi	O
else	O
set	B
θ	O
(	O
t+1	O
)	O
=	O
θ	O
(	O
t	O
)	O
(	O
cid:80	O
)	O
t	O
t=1	O
w	O
(	O
t	O
)	O
output	O
:	O
¯w	O
=	O
1	O
t	O
15.6	O
summary	O
svm	O
is	O
an	O
algorithm	O
for	O
learning	O
halfspaces	O
with	O
a	O
certain	O
type	O
of	O
prior	O
knowl-	O
edge	O
,	O
namely	O
,	O
preference	O
for	O
large	O
margin	B
.	O
hard-svm	O
seeks	O
the	O
halfspace	B
that	O
separates	O
the	O
data	O
perfectly	O
with	O
the	O
largest	O
margin	B
,	O
whereas	O
soft-svm	O
does	O
not	O
assume	O
separability	O
of	O
the	O
data	O
and	O
allows	O
the	O
constraints	O
to	O
be	O
violated	O
to	O
some	O
extent	O
.	O
the	O
sample	B
complexity	I
for	O
both	O
types	O
of	O
svm	O
is	O
diﬀerent	O
from	O
the	O
sample	B
complexity	I
of	O
straightforward	O
halfspace	B
learning	O
,	O
as	O
it	O
does	O
not	O
depend	O
on	O
the	O
dimension	B
of	O
the	O
domain	B
but	O
rather	O
on	O
parameters	O
such	O
as	O
the	O
maximal	O
norms	O
of	O
x	O
and	O
w.	O
the	O
importance	O
of	O
dimension-independent	O
sample	B
complexity	I
will	O
be	O
realized	O
in	O
the	O
next	O
chapter	O
,	O
where	O
we	O
will	O
discuss	O
the	O
embedding	O
of	O
the	O
given	O
domain	B
into	O
some	O
high	O
dimensional	O
feature	B
space	I
as	O
means	O
for	O
enriching	O
our	O
hypothesis	B
class	I
.	O
such	O
a	O
procedure	O
raises	O
computational	O
and	O
sample	B
complexity	I
problems	O
.	O
the	O
latter	O
is	O
solved	O
by	O
using	O
svm	O
,	O
whereas	O
the	O
former	O
can	O
be	O
solved	O
by	O
using	O
svm	O
with	O
kernels	B
,	O
as	O
we	O
will	O
see	O
in	O
the	O
next	O
chapter	O
.	O
15.7	O
bibliographic	O
remarks	O
svms	O
have	O
been	O
introduced	O
in	O
(	O
cortes	O
&	O
vapnik	O
1995	O
,	O
boser	O
,	O
guyon	O
&	O
vapnik	O
1992	O
)	O
.	O
there	O
are	O
many	O
good	O
books	O
on	O
the	O
theoretical	O
and	O
practical	O
aspects	O
of	O
svms	O
.	O
for	O
example	O
,	O
(	O
vapnik	O
1995	O
,	O
cristianini	O
&	O
shawe-taylor	O
2000	O
,	O
sch¨olkopf	O
&	O
smola	O
2002	O
,	O
hsu	O
,	O
chang	O
&	O
lin	O
2003	O
,	O
steinwart	O
&	O
christmann	O
2008	O
)	O
.	O
using	O
sgd	O
for	O
solving	O
soft-svm	O
has	O
been	O
proposed	O
in	O
shalev-shwartz	O
et	O
al	O
.	O
(	O
2007	O
)	O
.	O
214	O
support	O
vector	O
machines	O
15.8	O
exercises	O
1.	O
show	O
that	O
the	O
hard-svm	O
rule	O
,	O
namely	O
,	O
|	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b|	O
argmax	O
(	O
w	O
,	O
b	O
)	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
=1	O
min	O
i∈	O
[	O
m	O
]	O
s.t	O
.	O
∀i	O
,	O
yi	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
>	O
0	O
,	O
is	O
equivalent	O
to	O
the	O
following	O
formulation	O
:	O
argmax	O
(	O
w	O
,	O
b	O
)	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
=1	O
min	O
i∈	O
[	O
m	O
]	O
yi	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
.	O
(	O
15.13	O
)	O
hint	O
:	O
deﬁne	O
g	O
=	O
{	O
(	O
w	O
,	O
b	O
)	O
:	O
∀i	O
,	O
yi	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
>	O
0	O
}	O
.	O
1.	O
show	O
that	O
argmax	O
(	O
w	O
,	O
b	O
)	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
=1	O
min	O
i∈	O
[	O
m	O
]	O
2.	O
show	O
that	O
∀	O
(	O
w	O
,	O
b	O
)	O
∈	O
g	O
,	O
yi	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
∈	O
g	O
min	O
i∈	O
[	O
m	O
]	O
yi	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
=	O
min	O
i∈	O
[	O
m	O
]	O
|	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
+	O
b|	O
2.	O
margin	B
and	O
the	O
perceptron	O
consider	O
a	O
training	B
set	I
that	O
is	O
linearly	O
sep-	O
arable	O
with	O
a	O
margin	B
γ	O
and	O
such	O
that	O
all	O
the	O
instances	O
are	O
within	O
a	O
ball	O
of	O
radius	O
ρ.	O
prove	O
that	O
the	O
maximal	O
number	O
of	O
updates	O
the	O
batch	O
perceptron	O
algorithm	O
given	O
in	O
section	O
9.1.2	O
will	O
make	O
when	O
running	O
on	O
this	O
training	B
set	I
is	O
(	O
ρ/γ	O
)	O
2	O
.	O
3.	O
hard	O
versus	O
soft	O
svm	O
:	O
prove	O
or	O
refute	O
the	O
following	O
claim	O
:	O
there	O
exists	O
λ	O
>	O
0	O
such	O
that	O
for	O
every	O
sample	O
s	O
of	O
m	O
>	O
1	O
examples	O
,	O
which	O
is	O
separable	B
by	O
the	O
class	O
of	O
homogenous	B
halfspaces	O
,	O
the	O
hard-svm	O
and	O
the	O
soft-svm	O
(	O
with	O
parameter	O
λ	O
)	O
learning	O
rules	O
return	O
exactly	O
the	O
same	O
weight	O
vector	O
.	O
4.	O
weak	B
duality	I
:	O
prove	O
that	O
for	O
any	O
function	B
f	O
of	O
two	O
vector	O
variables	O
x	O
∈	O
x	O
,	O
y	O
∈	O
y	O
,	O
it	O
holds	O
that	O
y∈y	O
f	O
(	O
x	O
,	O
y	O
)	O
≥	O
max	O
x∈x	O
max	O
min	O
y∈y	O
min	O
x∈x	O
f	O
(	O
x	O
,	O
y	O
)	O
.	O
16	O
kernel	O
methods	O
in	O
the	O
previous	O
chapter	O
we	O
described	O
the	O
svm	O
paradigm	O
for	O
learning	O
halfspaces	O
in	O
high	O
dimensional	O
feature	B
spaces	O
.	O
this	O
enables	O
us	O
to	O
enrich	O
the	O
expressive	O
power	O
of	O
halfspaces	O
by	O
ﬁrst	O
mapping	O
the	O
data	O
into	O
a	O
high	O
dimensional	O
feature	B
space	I
,	O
and	O
then	O
learning	O
a	O
linear	B
predictor	I
in	O
that	O
space	O
.	O
this	O
is	O
similar	O
to	O
the	O
adaboost	O
algorithm	O
,	O
which	O
learns	O
a	O
composition	O
of	O
a	O
halfspace	B
over	O
base	O
hy-	O
potheses	O
.	O
while	O
this	O
approach	O
greatly	O
extends	O
the	O
expressiveness	O
of	O
halfspace	B
predictors	O
,	O
it	O
raises	O
both	O
sample	B
complexity	I
and	O
computational	B
complexity	I
chal-	O
lenges	O
.	O
in	O
the	O
previous	O
chapter	O
we	O
tackled	O
the	O
sample	B
complexity	I
issue	O
using	O
the	O
concept	O
of	O
margin	B
.	O
in	O
this	O
chapter	O
we	O
tackle	O
the	O
computational	B
complexity	I
challenge	O
using	O
the	O
method	O
of	O
kernels	B
.	O
we	O
start	O
the	O
chapter	O
by	O
describing	O
the	O
idea	O
of	O
embedding	O
the	O
data	O
into	O
a	O
high	O
dimensional	O
feature	B
space	I
.	O
we	O
then	O
introduce	O
the	O
idea	O
of	O
kernels	B
.	O
a	O
kernel	O
is	O
a	O
type	O
of	O
a	O
similarity	O
measure	O
between	O
instances	O
.	O
the	O
special	O
property	O
of	O
kernel	O
similarities	O
is	O
that	O
they	O
can	O
be	O
viewed	O
as	O
inner	O
products	O
in	O
some	O
hilbert	O
space	O
(	O
or	O
euclidean	O
space	O
of	O
some	O
high	O
dimension	B
)	O
to	O
which	O
the	O
instance	B
space	I
is	O
vir-	O
tually	O
embedded	O
.	O
we	O
introduce	O
the	O
“	O
kernel	B
trick	I
”	O
that	O
enables	O
computationally	O
eﬃcient	O
implementation	O
of	O
learning	O
,	O
without	O
explicitly	O
handling	O
the	O
high	O
dimen-	O
sional	O
representation	O
of	O
the	O
domain	B
instances	O
.	O
kernel	O
based	O
learning	O
algorithms	O
,	O
and	O
in	O
particular	O
kernel-svm	O
,	O
are	O
very	O
useful	O
and	O
popular	O
machine	O
learning	O
tools	O
.	O
their	O
success	O
may	O
be	O
attributed	O
both	O
to	O
being	O
ﬂexible	O
for	O
accommodating	O
domain	B
speciﬁc	O
prior	B
knowledge	I
and	O
to	O
having	O
a	O
well	O
developed	O
set	B
of	O
eﬃcient	O
implementation	O
algorithms	O
.	O
16.1	O
embeddings	O
into	O
feature	B
spaces	O
the	O
expressive	O
power	O
of	O
halfspaces	O
is	O
rather	O
restricted	O
–	O
for	O
example	O
,	O
the	O
follow-	O
ing	O
training	B
set	I
is	O
not	O
separable	B
by	O
a	O
halfspace	B
.	O
let	O
the	O
domain	B
be	O
the	O
real	O
line	O
;	O
consider	O
the	O
domain	B
points	O
{	O
−10	O
,	O
−9	O
,	O
−8	O
,	O
.	O
.	O
.	O
,	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
9	O
,	O
10	O
}	O
where	O
the	O
labels	O
are	O
+1	O
for	O
all	O
x	O
such	O
that	O
|x|	O
>	O
2	O
and	O
−1	O
otherwise	O
.	O
to	O
make	O
the	O
class	O
of	O
halfspaces	O
more	O
expressive	O
,	O
we	O
can	O
ﬁrst	O
map	O
the	O
original	O
instance	B
space	I
into	O
another	O
space	O
(	O
possibly	O
of	O
a	O
higher	O
dimension	B
)	O
and	O
then	O
learn	O
a	O
halfspace	B
in	O
that	O
space	O
.	O
for	O
example	O
,	O
consider	O
the	O
example	O
mentioned	O
previously	O
.	O
instead	O
of	O
learning	O
a	O
halfspace	B
in	O
the	O
original	O
representation	O
let	O
us	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
216	O
kernel	O
methods	O
ﬁrst	O
deﬁne	O
a	O
mapping	O
ψ	O
:	O
r	O
→	O
r2	O
as	O
follows	O
:	O
ψ	O
(	O
x	O
)	O
=	O
(	O
x	O
,	O
x2	O
)	O
.	O
we	O
use	O
the	O
term	O
feature	B
space	I
to	O
denote	O
the	O
range	O
of	O
ψ.	O
after	O
applying	O
ψ	O
the	O
data	O
can	O
be	O
easily	O
explained	O
using	O
the	O
halfspace	B
h	O
(	O
x	O
)	O
=	O
sign	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
−	O
b	O
)	O
,	O
where	O
w	O
=	O
(	O
0	O
,	O
1	O
)	O
and	O
b	O
=	O
5.	O
the	O
basic	O
paradigm	O
is	O
as	O
follows	O
:	O
1.	O
given	O
some	O
domain	B
set	O
x	O
and	O
a	O
learning	O
task	O
,	O
choose	O
a	O
mapping	O
ψ	O
:	O
x	O
→	O
f	O
,	O
for	O
some	O
feature	B
space	I
f	O
,	O
that	O
will	O
usually	O
be	O
rn	O
for	O
some	O
n	O
(	O
however	O
,	O
the	O
range	O
of	O
such	O
a	O
mapping	O
can	O
be	O
any	O
hilbert	O
space	O
,	O
including	O
such	O
spaces	O
of	O
inﬁnite	O
dimension	B
,	O
as	O
we	O
will	O
show	O
later	O
)	O
.	O
2.	O
given	O
a	O
sequence	O
of	O
labeled	O
examples	O
,	O
s	O
=	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
,	O
create	O
the	O
image	O
sequence	O
ˆs	O
=	O
(	O
ψ	O
(	O
x1	O
)	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
ψ	O
(	O
xm	O
)	O
,	O
ym	O
)	O
.	O
3.	O
train	O
a	O
linear	B
predictor	I
h	O
over	O
ˆs	O
.	O
4.	O
predict	O
the	O
label	B
of	O
a	O
test	O
point	O
,	O
x	O
,	O
to	O
be	O
h	O
(	O
ψ	O
(	O
x	O
)	O
)	O
.	O
note	O
that	O
,	O
for	O
every	O
probability	O
distribution	O
d	O
over	O
x	O
×	O
y	O
,	O
we	O
can	O
readily	O
deﬁne	O
its	O
image	O
probability	O
distribution	O
dψ	O
over	O
f	O
×	O
y	O
by	O
setting	O
,	O
for	O
every	O
subset	O
a	O
⊆	O
f	O
×	O
y	O
,	O
dψ	O
(	O
a	O
)	O
=	O
d	O
(	O
ψ−1	O
(	O
a	O
)	O
)	O
.1	O
it	O
follows	O
that	O
for	O
every	O
predictor	B
h	O
over	O
the	O
feature	B
space	I
,	O
ldψ	O
(	O
h	O
)	O
=	O
ld	O
(	O
h	O
◦	O
ψ	O
)	O
,	O
where	O
h	O
◦	O
ψ	O
is	O
the	O
composition	O
of	O
h	O
onto	O
ψ.	O
the	O
success	O
of	O
this	O
learning	O
paradigm	O
depends	O
on	O
choosing	O
a	O
good	O
ψ	O
for	O
a	O
given	O
learning	O
task	O
:	O
that	O
is	O
,	O
a	O
ψ	O
that	O
will	O
make	O
the	O
image	O
of	O
the	O
data	O
distribution	O
(	O
close	O
to	O
being	O
)	O
linearly	O
separable	B
in	O
the	O
feature	B
space	I
,	O
thus	O
making	O
the	O
resulting	O
algorithm	O
a	O
good	O
learner	O
for	O
a	O
given	O
task	O
.	O
picking	O
such	O
an	O
embedding	O
requires	O
prior	B
knowledge	I
about	O
that	O
task	O
.	O
however	O
,	O
often	O
some	O
generic	O
mappings	O
that	O
enable	O
us	O
to	O
enrich	O
the	O
class	O
of	O
halfspaces	O
and	O
extend	O
its	O
expressiveness	O
are	O
used	O
.	O
one	O
notable	O
example	O
is	O
polynomial	O
mappings	O
,	O
which	O
are	O
a	O
generalization	O
of	O
the	O
ψ	O
we	O
have	O
seen	O
in	O
the	O
previous	O
example	O
.	O
recall	B
that	O
the	O
prediction	O
of	O
a	O
standard	O
halfspace	B
classiﬁer	O
on	O
an	O
instance	B
x	O
is	O
based	O
on	O
the	O
linear	O
mapping	O
x	O
(	O
cid:55	O
)	O
→	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
.	O
we	O
can	O
generalize	O
linear	O
mappings	O
to	O
a	O
polynomial	O
mapping	O
,	O
x	O
(	O
cid:55	O
)	O
→	O
p	O
(	O
x	O
)	O
,	O
where	O
p	O
is	O
a	O
multivariate	O
polynomial	O
of	O
degree	O
k.	O
for	O
simplicity	O
,	O
consider	O
ﬁrst	O
the	O
case	O
in	O
which	O
x	O
is	O
1	O
dimensional	O
.	O
j=0	O
wjxj	O
,	O
where	O
w	O
∈	O
rk+1	O
is	O
the	O
vector	O
of	O
coeﬃcients	O
of	O
the	O
polynomial	O
we	O
need	O
to	O
learn	O
.	O
we	O
can	O
rewrite	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
where	O
ψ	O
:	O
r	O
→	O
rk+1	O
is	O
the	O
mapping	O
x	O
(	O
cid:55	O
)	O
→	O
(	O
1	O
,	O
x	O
,	O
x2	O
,	O
x3	O
,	O
.	O
.	O
.	O
,	O
xk	O
)	O
.	O
it	O
follows	O
that	O
learning	O
a	O
k	O
degree	O
polynomial	O
over	O
r	O
can	O
be	O
done	O
by	O
learning	O
a	O
linear	O
mapping	O
in	O
the	O
(	O
k	O
+	O
1	O
)	O
dimensional	O
feature	B
space	I
.	O
in	O
that	O
case	O
,	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:80	O
)	O
k	O
more	O
generally	O
,	O
a	O
degree	O
k	O
multivariate	O
polynomial	O
from	O
rn	O
to	O
r	O
can	O
be	O
writ-	O
ten	O
as	O
p	O
(	O
x	O
)	O
=	O
wj	O
xji	O
.	O
(	O
16.1	O
)	O
(	O
cid:88	O
)	O
j∈	O
[	O
n	O
]	O
r	O
:	O
r≤k	O
r	O
(	O
cid:89	O
)	O
i=1	O
1	O
this	O
is	O
deﬁned	O
for	O
every	O
a	O
such	O
that	O
ψ−1	O
(	O
a	O
)	O
is	O
measurable	O
with	O
respect	O
to	O
d.	O
16.2	O
the	O
kernel	B
trick	I
217	O
as	O
before	O
,	O
we	O
can	O
rewrite	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
where	O
now	O
ψ	O
:	O
rn	O
→	O
rd	O
is	O
such	O
that	O
for	O
every	O
j	O
∈	O
[	O
n	O
]	O
r	O
,	O
r	O
≤	O
k	O
,	O
the	O
coordinate	O
of	O
ψ	O
(	O
x	O
)	O
associated	O
with	O
j	O
is	O
the	O
monomial	O
(	O
cid:81	O
)	O
r	O
i=1	O
xji	O
.	O
naturally	O
,	O
polynomial-based	O
classiﬁers	O
yield	O
much	O
richer	O
hypothesis	B
classes	O
than	O
halfspaces	O
.	O
we	O
have	O
seen	O
at	O
the	O
beginning	O
of	O
this	O
chapter	O
an	O
example	O
in	O
which	O
the	O
training	B
set	I
,	O
in	O
its	O
original	O
domain	B
(	O
x	O
=	O
r	O
)	O
,	O
can	O
not	O
be	O
separable	B
by	O
a	O
halfspace	B
,	O
but	O
after	O
the	O
embedding	O
x	O
(	O
cid:55	O
)	O
→	O
(	O
x	O
,	O
x2	O
)	O
it	O
is	O
perfectly	O
separable	B
.	O
so	O
,	O
while	O
the	O
classiﬁer	B
is	O
always	O
linear	O
in	O
the	O
feature	B
space	I
,	O
it	O
can	O
have	O
highly	O
nonlinear	O
behavior	O
on	O
the	O
original	O
space	O
from	O
which	O
instances	O
were	O
sampled	O
.	O
in	O
general	O
,	O
we	O
can	O
choose	O
any	O
feature	B
mapping	O
ψ	O
that	O
maps	O
the	O
original	O
in-	O
stances	O
into	O
some	O
hilbert	O
space.2	O
the	O
euclidean	O
space	O
rd	O
is	O
a	O
hilbert	O
space	O
for	O
any	O
ﬁnite	O
d.	O
but	O
there	O
are	O
also	O
inﬁnite	O
dimensional	O
hilbert	O
spaces	O
(	O
as	O
we	O
shall	O
see	O
later	O
on	O
in	O
this	O
chapter	O
)	O
.	O
the	O
bottom	O
line	O
of	O
this	O
discussion	O
is	O
that	O
we	O
can	O
enrich	O
the	O
class	O
of	O
halfspaces	O
by	O
ﬁrst	O
applying	O
a	O
nonlinear	O
mapping	O
,	O
ψ	O
,	O
that	O
maps	O
the	O
instance	B
space	I
into	O
some	O
feature	B
space	I
,	O
and	O
then	O
learning	O
a	O
halfspace	B
in	O
that	O
feature	B
space	I
.	O
however	O
,	O
if	O
the	O
range	O
of	O
ψ	O
is	O
a	O
high	O
dimensional	O
space	O
we	O
face	O
two	O
problems	O
.	O
first	O
,	O
the	O
vc-	O
dimension	B
of	O
halfspaces	O
in	O
rn	O
is	O
n	O
+	O
1	O
,	O
and	O
therefore	O
,	O
if	O
the	O
range	O
of	O
ψ	O
is	O
very	O
large	O
,	O
we	O
need	O
many	O
more	O
samples	O
in	O
order	O
to	O
learn	O
a	O
halfspace	B
in	O
the	O
range	O
of	O
ψ.	O
second	O
,	O
from	O
the	O
computational	O
point	O
of	O
view	O
,	O
performing	O
calculations	O
in	O
the	O
high	O
dimensional	O
space	O
might	O
be	O
too	O
costly	O
.	O
in	O
fact	O
,	O
even	O
the	O
representation	O
of	O
the	O
vector	O
w	O
in	O
the	O
feature	B
space	I
can	O
be	O
unrealistic	O
.	O
the	O
ﬁrst	O
issue	O
can	O
be	O
tackled	O
using	O
the	O
paradigm	O
of	O
large	O
margin	B
(	O
or	O
low	O
norm	O
predictors	O
)	O
,	O
as	O
we	O
already	O
discussed	O
in	O
the	O
previous	O
chapter	O
in	O
the	O
context	O
of	O
the	O
svm	O
algorithm	O
.	O
in	O
the	O
following	O
section	O
we	O
address	O
the	O
computational	O
issue	O
.	O
16.2	O
the	O
kernel	B
trick	I
we	O
have	O
seen	O
that	O
embedding	O
the	O
input	O
space	O
into	O
some	O
high	O
dimensional	O
feature	B
space	I
makes	O
halfspace	B
learning	O
more	O
expressive	O
.	O
however	O
,	O
the	O
computational	B
complexity	I
of	O
such	O
learning	O
may	O
still	O
pose	O
a	O
serious	O
hurdle	O
–	O
computing	O
linear	O
separators	O
over	O
very	O
high	O
dimensional	O
data	O
may	O
be	O
computationally	O
expensive	O
.	O
the	O
common	O
solution	O
to	O
this	O
concern	O
is	O
kernel	O
based	O
learning	O
.	O
the	O
term	O
“	O
kernels	B
”	O
is	O
used	O
in	O
this	O
context	O
to	O
describe	O
inner	O
products	O
in	O
the	O
feature	B
space	I
.	O
given	O
an	O
embedding	O
ψ	O
of	O
some	O
domain	B
space	O
x	O
into	O
some	O
hilbert	O
space	O
,	O
we	O
deﬁne	O
the	O
kernel	O
function	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
(	O
cid:104	O
)	O
ψ	O
(	O
x	O
)	O
,	O
ψ	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
.	O
one	O
can	O
think	O
of	O
k	O
as	O
specifying	O
similarity	O
between	O
instances	O
and	O
of	O
the	O
embedding	O
ψ	O
as	O
mapping	O
the	O
domain	B
set	O
2	O
a	O
hilbert	O
space	O
is	O
a	O
vector	O
space	O
with	O
an	O
inner	O
product	O
,	O
which	O
is	O
also	O
complete	O
.	O
a	O
space	O
is	O
in	O
our	O
case	O
,	O
the	O
norm	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
is	O
deﬁned	O
by	O
the	O
inner	O
product	O
(	O
cid:112	O
)	O
(	O
cid:104	O
)	O
w	O
,	O
w	O
(	O
cid:105	O
)	O
.	O
the	O
reason	O
we	O
require	O
complete	O
if	O
all	O
cauchy	O
sequences	O
in	O
the	O
space	O
converge	O
.	O
the	O
range	O
of	O
ψ	O
to	O
be	O
in	O
a	O
hilbert	O
space	O
is	O
that	O
projections	O
in	O
a	O
hilbert	O
space	O
are	O
well	O
deﬁned	O
.	O
in	O
particular	O
,	O
if	O
m	O
is	O
a	O
linear	O
subspace	O
of	O
a	O
hilbert	O
space	O
,	O
then	O
every	O
x	O
in	O
the	O
hilbert	O
space	O
can	O
be	O
written	O
as	O
a	O
sum	O
x	O
=	O
u	O
+	O
v	O
where	O
u	O
∈	O
m	O
and	O
(	O
cid:104	O
)	O
v	O
,	O
w	O
(	O
cid:105	O
)	O
=	O
0	O
for	O
all	O
w	O
∈	O
m	O
.	O
we	O
use	O
this	O
fact	O
in	O
the	O
proof	O
of	O
the	O
representer	B
theorem	I
given	O
in	O
the	O
next	O
section	O
.	O
218	O
kernel	O
methods	O
x	O
into	O
a	O
space	O
where	O
these	O
similarities	O
are	O
realized	O
as	O
inner	O
products	O
.	O
it	O
turns	O
out	O
that	O
many	O
learning	O
algorithms	O
for	O
halfspaces	O
can	O
be	O
carried	O
out	O
just	O
on	O
the	O
basis	O
of	O
the	O
values	O
of	O
the	O
kernel	O
function	O
over	O
pairs	O
of	O
domain	B
points	O
.	O
the	O
main	O
advantage	O
of	O
such	O
algorithms	O
is	O
that	O
they	O
implement	O
linear	O
separators	O
in	O
high	O
dimensional	O
feature	B
spaces	O
without	O
having	O
to	O
specify	O
points	O
in	O
that	O
space	O
or	O
expressing	O
the	O
embedding	O
ψ	O
explicitly	O
.	O
the	O
remainder	O
of	O
this	O
section	O
is	O
devoted	O
to	O
constructing	O
such	O
algorithms	O
.	O
in	O
the	O
previous	O
chapter	O
we	O
saw	O
that	O
regularizing	O
the	O
norm	O
of	O
w	O
yields	O
a	O
small	O
sample	B
complexity	I
even	O
if	O
the	O
dimensionality	O
of	O
the	O
feature	B
space	I
is	O
high	O
.	O
inter-	O
estingly	O
,	O
as	O
we	O
show	O
later	O
,	O
regularizing	O
the	O
norm	O
of	O
w	O
is	O
also	O
helpful	O
in	O
overcoming	O
the	O
computational	O
problem	O
.	O
to	O
do	O
so	O
,	O
ﬁrst	O
note	O
that	O
all	O
versions	O
of	O
the	O
svm	O
op-	O
timization	O
problem	O
we	O
have	O
derived	O
in	O
the	O
previous	O
chapter	O
are	O
instances	O
of	O
the	O
following	O
general	O
problem	O
:	O
min	O
w	O
(	O
f	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x1	O
)	O
(	O
cid:105	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
xm	O
)	O
(	O
cid:105	O
)	O
)	O
+	O
r	O
(	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
)	O
)	O
,	O
(	O
16.2	O
)	O
where	O
f	O
:	O
rm	O
→	O
r	O
is	O
an	O
arbitrary	O
function	B
and	O
r	O
:	O
r+	O
→	O
r	O
is	O
a	O
monotoni-	O
(	O
cid:80	O
)	O
cally	O
nondecreasing	O
function	B
.	O
for	O
example	O
,	O
soft-svm	O
for	O
homogenous	B
halfspaces	O
(	O
equation	O
(	O
15.6	O
)	O
)	O
can	O
be	O
derived	O
from	O
equation	O
(	O
16.2	O
)	O
by	O
letting	O
r	O
(	O
a	O
)	O
=	O
λa2	O
and	O
i	O
max	O
{	O
0	O
,	O
1−yiai	O
}	O
.	O
similarly	O
,	O
hard-svm	O
for	O
nonhomogenous	O
f	O
(	O
a1	O
,	O
.	O
.	O
.	O
,	O
am	O
)	O
=	O
1	O
m	O
halfspaces	O
(	O
equation	O
(	O
15.2	O
)	O
)	O
can	O
be	O
derived	O
from	O
equation	O
(	O
16.2	O
)	O
by	O
letting	O
r	O
(	O
a	O
)	O
=	O
a2	O
and	O
letting	O
f	O
(	O
a1	O
,	O
.	O
.	O
.	O
,	O
am	O
)	O
be	O
0	O
if	O
there	O
exists	O
b	O
such	O
that	O
yi	O
(	O
ai	O
+b	O
)	O
≥	O
1	O
for	O
all	O
i	O
,	O
and	O
f	O
(	O
a1	O
,	O
.	O
.	O
.	O
,	O
am	O
)	O
=	O
∞	O
otherwise	O
.	O
tion	O
(	O
16.2	O
)	O
that	O
lies	O
in	O
the	O
span	O
of	O
{	O
ψ	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
ψ	O
(	O
xm	O
)	O
}	O
.	O
theorem	O
16.1	O
(	O
representer	B
theorem	I
)	O
assume	O
that	O
ψ	O
is	O
a	O
mapping	O
from	O
x	O
to	O
i=1	O
αiψ	O
(	O
xi	O
)	O
a	O
hilbert	O
space	O
.	O
then	O
,	O
there	O
exists	O
a	O
vector	O
α	O
∈	O
rm	O
such	O
that	O
w	O
=	O
(	O
cid:80	O
)	O
m	O
the	O
following	O
theorem	O
shows	O
that	O
there	O
exists	O
an	O
optimal	O
solution	O
of	O
equa-	O
is	O
an	O
optimal	O
solution	O
of	O
equation	O
(	O
16.2	O
)	O
.	O
proof	O
let	O
w	O
(	O
cid:63	O
)	O
be	O
an	O
optimal	O
solution	O
of	O
equation	O
(	O
16.2	O
)	O
.	O
because	O
w	O
(	O
cid:63	O
)	O
is	O
an	O
element	O
of	O
a	O
hilbert	O
space	O
,	O
we	O
can	O
rewrite	O
w	O
(	O
cid:63	O
)	O
as	O
m	O
(	O
cid:88	O
)	O
w	O
(	O
cid:63	O
)	O
=	O
αiψ	O
(	O
xi	O
)	O
+	O
u	O
,	O
where	O
(	O
cid:104	O
)	O
u	O
,	O
ψ	O
(	O
xi	O
)	O
(	O
cid:105	O
)	O
=	O
0	O
for	O
all	O
i.	O
set	B
w	O
=	O
w	O
(	O
cid:63	O
)	O
−	O
u.	O
clearly	O
,	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
(	O
cid:107	O
)	O
u	O
(	O
cid:107	O
)	O
2	O
,	O
thus	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
≤	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
.	O
since	O
r	O
is	O
nondecreasing	O
we	O
obtain	O
that	O
r	O
(	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
)	O
≤	O
r	O
(	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
)	O
.	O
additionally	O
,	O
for	O
all	O
i	O
we	O
have	O
that	O
i=1	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
xi	O
)	O
(	O
cid:105	O
)	O
=	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
−	O
u	O
,	O
ψ	O
(	O
xi	O
)	O
(	O
cid:105	O
)	O
=	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
ψ	O
(	O
xi	O
)	O
(	O
cid:105	O
)	O
,	O
hence	O
f	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x1	O
)	O
(	O
cid:105	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
xm	O
)	O
(	O
cid:105	O
)	O
)	O
=	O
f	O
(	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
ψ	O
(	O
x1	O
)	O
(	O
cid:105	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
ψ	O
(	O
xm	O
)	O
(	O
cid:105	O
)	O
)	O
.	O
w	O
=	O
(	O
cid:80	O
)	O
m	O
we	O
have	O
shown	O
that	O
the	O
objective	O
of	O
equation	O
(	O
16.2	O
)	O
at	O
w	O
can	O
not	O
be	O
larger	O
than	O
the	O
objective	O
at	O
w	O
(	O
cid:63	O
)	O
and	O
therefore	O
w	O
is	O
also	O
an	O
optimal	O
solution	O
.	O
since	O
i=1	O
αiψ	O
(	O
xi	O
)	O
we	O
conclude	O
our	O
proof	O
.	O
	O
m	O
(	O
cid:88	O
)	O
	O
(	O
cid:118	O
)	O
(	O
cid:117	O
)	O
(	O
cid:117	O
)	O
(	O
cid:116	O
)	O
m	O
(	O
cid:88	O
)	O
j=1	O
min	O
α∈rm	O
f	O
+	O
r	O
αjk	O
(	O
xj	O
,	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
αjk	O
(	O
xj	O
,	O
xm	O
)	O
	O
m	O
(	O
cid:88	O
)	O
	O
.	O
j=1	O
16.2	O
the	O
kernel	B
trick	I
219	O
w	O
=	O
(	O
cid:80	O
)	O
m	O
on	O
the	O
basis	O
of	O
the	O
representer	B
theorem	I
we	O
can	O
optimize	O
equation	O
(	O
16.2	O
)	O
with	O
respect	O
to	O
the	O
coeﬃcients	O
α	O
instead	O
of	O
the	O
coeﬃcients	O
w	O
as	O
follows	O
.	O
writing	O
(	O
cid:42	O
)	O
(	O
cid:88	O
)	O
j=1	O
αjψ	O
(	O
xj	O
)	O
we	O
have	O
that	O
for	O
all	O
i	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
xi	O
)	O
(	O
cid:105	O
)	O
=	O
αjψ	O
(	O
xj	O
)	O
,	O
ψ	O
(	O
xi	O
)	O
=	O
similarly	O
,	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
=	O
j	O
αjψ	O
(	O
xj	O
)	O
,	O
(	O
cid:42	O
)	O
(	O
cid:88	O
)	O
j	O
(	O
cid:88	O
)	O
j	O
(	O
cid:43	O
)	O
αjψ	O
(	O
xj	O
)	O
=	O
(	O
cid:43	O
)	O
m	O
(	O
cid:88	O
)	O
j=1	O
m	O
(	O
cid:88	O
)	O
αj	O
(	O
cid:104	O
)	O
ψ	O
(	O
xj	O
)	O
,	O
ψ	O
(	O
xi	O
)	O
(	O
cid:105	O
)	O
.	O
αiαj	O
(	O
cid:104	O
)	O
ψ	O
(	O
xi	O
)	O
,	O
ψ	O
(	O
xj	O
)	O
(	O
cid:105	O
)	O
.	O
i	O
,	O
j=1	O
let	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
(	O
cid:104	O
)	O
ψ	O
(	O
x	O
)	O
,	O
ψ	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
be	O
a	O
function	B
that	O
implements	O
the	O
kernel	O
function	O
with	O
respect	O
to	O
the	O
embedding	O
ψ.	O
instead	O
of	O
solving	O
equation	O
(	O
16.2	O
)	O
we	O
can	O
solve	O
the	O
equivalent	O
problem	O
αiαjk	O
(	O
xj	O
,	O
xi	O
)	O
(	O
16.3	O
)	O
i	O
,	O
j=1	O
to	O
solve	O
the	O
optimization	O
problem	O
given	O
in	O
equation	O
(	O
16.3	O
)	O
,	O
we	O
do	O
not	O
need	O
any	O
direct	O
access	O
to	O
elements	O
in	O
the	O
feature	B
space	I
.	O
the	O
only	O
thing	O
we	O
should	O
know	O
is	O
how	O
to	O
calculate	O
inner	O
products	O
in	O
the	O
feature	B
space	I
,	O
or	O
equivalently	O
,	O
to	O
calculate	O
the	O
kernel	O
function	O
.	O
in	O
fact	O
,	O
to	O
solve	O
equation	O
(	O
16.3	O
)	O
we	O
solely	O
need	O
to	O
know	O
the	O
value	O
of	O
the	O
m	O
×	O
m	O
matrix	O
g	O
s.t	O
.	O
gi	O
,	O
j	O
=	O
k	O
(	O
xi	O
,	O
xj	O
)	O
,	O
which	O
is	O
often	O
called	O
the	O
gram	O
matrix	O
.	O
in	O
particular	O
,	O
specifying	O
the	O
preceding	O
to	O
the	O
soft-svm	O
problem	O
given	O
in	O
equa-	O
tion	O
(	O
15.6	O
)	O
,	O
we	O
can	O
rewrite	O
the	O
problem	O
as	O
(	O
cid:32	O
)	O
min	O
α∈rm	O
λαt	O
gα	O
+	O
max	O
(	O
cid:8	O
)	O
0	O
,	O
1	O
−	O
yi	O
(	O
gα	O
)	O
i	O
(	O
cid:9	O
)	O
(	O
cid:33	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
1	O
m	O
,	O
(	O
16.4	O
)	O
where	O
(	O
gα	O
)	O
i	O
is	O
the	O
i	O
’	O
th	O
element	O
of	O
the	O
vector	O
obtained	O
by	O
multiplying	O
the	O
gram	O
matrix	O
g	O
by	O
the	O
vector	O
α.	O
note	O
that	O
equation	O
(	O
16.4	O
)	O
can	O
be	O
written	O
as	O
quadratic	O
programming	O
and	O
hence	O
can	O
be	O
solved	O
eﬃciently	O
.	O
in	O
the	O
next	O
section	O
we	O
describe	O
an	O
even	O
simpler	O
algorithm	O
for	O
solving	O
soft-svm	O
with	O
kernels	B
.	O
once	O
we	O
learn	O
the	O
coeﬃcients	O
α	O
we	O
can	O
calculate	O
the	O
prediction	O
on	O
a	O
new	O
instance	B
by	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
=	O
m	O
(	O
cid:88	O
)	O
m	O
(	O
cid:88	O
)	O
αj	O
(	O
cid:104	O
)	O
ψ	O
(	O
xj	O
)	O
,	O
ψ	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
=	O
αjk	O
(	O
xj	O
,	O
x	O
)	O
.	O
j=1	O
j=1	O
the	O
advantage	O
of	O
working	O
with	O
kernels	B
rather	O
than	O
directly	O
optimizing	O
w	O
in	O
the	O
feature	B
space	I
is	O
that	O
in	O
some	O
situations	O
the	O
dimension	B
of	O
the	O
feature	B
space	I
220	O
kernel	O
methods	O
is	O
extremely	O
large	O
while	O
implementing	O
the	O
kernel	O
function	O
is	O
very	O
simple	O
.	O
a	O
few	O
examples	O
are	O
given	O
in	O
the	O
following	O
.	O
example	O
16.1	O
(	O
polynomial	O
kernels	O
)	O
the	O
k	O
degree	O
polynomial	B
kernel	I
is	O
deﬁned	O
to	O
be	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
(	O
1	O
+	O
(	O
cid:104	O
)	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
(	O
cid:105	O
)	O
)	O
k.	O
now	O
we	O
will	O
show	O
that	O
this	O
is	O
indeed	O
a	O
kernel	O
function	O
.	O
that	O
is	O
,	O
we	O
will	O
show	O
that	O
there	O
exists	O
a	O
mapping	O
ψ	O
from	O
the	O
original	O
space	O
to	O
some	O
higher	O
dimensional	O
space	O
for	O
which	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
(	O
cid:104	O
)	O
ψ	O
(	O
x	O
)	O
,	O
ψ	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
.	O
for	O
simplicity	O
,	O
denote	O
x0	O
=	O
x	O
(	O
cid:48	O
)	O
0	O
=	O
1.	O
then	O
,	O
we	O
have	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
(	O
1	O
+	O
(	O
cid:104	O
)	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
(	O
cid:105	O
)	O
)	O
k	O
=	O
(	O
1	O
+	O
(	O
cid:104	O
)	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
(	O
cid:105	O
)	O
)	O
·	O
···	O
·	O
(	O
1	O
+	O
(	O
cid:104	O
)	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
(	O
cid:105	O
)	O
)	O
j	O
=	O
j=0	O
	O
·	O
···	O
·	O
k	O
(	O
cid:89	O
)	O
k	O
(	O
cid:89	O
)	O
xjx	O
(	O
cid:48	O
)	O
	O
n	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
element	O
of	O
ψ	O
(	O
x	O
)	O
that	O
equals	O
(	O
cid:81	O
)	O
k	O
j∈	O
{	O
0,1	O
,	O
...	O
,	O
n	O
}	O
k	O
now	O
,	O
if	O
we	O
deﬁne	O
ψ	O
:	O
rn	O
→	O
r	O
(	O
n+1	O
)	O
k	O
j∈	O
{	O
0,1	O
,	O
...	O
,	O
n	O
}	O
k	O
i=1	O
i=1	O
=	O
=	O
xji	O
	O
xjx	O
(	O
cid:48	O
)	O
j	O
	O
n	O
(	O
cid:88	O
)	O
j=0	O
xjix	O
(	O
cid:48	O
)	O
ji	O
k	O
(	O
cid:89	O
)	O
i=1	O
x	O
(	O
cid:48	O
)	O
ji	O
.	O
such	O
that	O
for	O
j	O
∈	O
{	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
k	O
there	O
is	O
an	O
i=1	O
xji	O
,	O
we	O
obtain	O
that	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
(	O
cid:104	O
)	O
ψ	O
(	O
x	O
)	O
,	O
ψ	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
.	O
since	O
ψ	O
contains	O
all	O
the	O
monomials	O
up	O
to	O
degree	O
k	O
,	O
a	O
halfspace	B
over	O
the	O
range	O
of	O
ψ	O
corresponds	O
to	O
a	O
polynomial	O
predictor	O
of	O
degree	O
k	O
over	O
the	O
original	O
space	O
.	O
hence	O
,	O
learning	O
a	O
halfspace	B
with	O
a	O
k	O
degree	O
polynomial	B
kernel	I
enables	O
us	O
to	O
learn	O
polynomial	O
predictors	O
of	O
degree	O
k	O
over	O
the	O
original	O
space	O
.	O
note	O
that	O
here	O
the	O
complexity	O
of	O
implementing	O
k	O
is	O
o	O
(	O
n	O
)	O
while	O
the	O
dimension	B
of	O
the	O
feature	B
space	I
is	O
on	O
the	O
order	O
of	O
nk	O
.	O
example	O
16.2	O
(	O
gaussian	O
kernel	O
)	O
let	O
the	O
original	O
instance	B
space	I
be	O
r	O
and	O
consider	O
the	O
mapping	O
ψ	O
where	O
for	O
each	O
nonnegative	O
integer	O
n	O
≥	O
0	O
there	O
exists	O
an	O
element	O
ψ	O
(	O
x	O
)	O
n	O
that	O
equals	O
2	O
xn	O
.	O
then	O
,	O
−	O
x2	O
e	O
(	O
cid:104	O
)	O
ψ	O
(	O
x	O
)	O
,	O
ψ	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
=	O
(	O
cid:19	O
)	O
−	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
2	O
2	O
e	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
n	O
∞	O
(	O
cid:88	O
)	O
1√	O
n	O
!	O
(	O
cid:18	O
)	O
1√	O
n=0	O
n	O
!	O
−	O
x2+	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
2	O
2	O
=	O
e	O
−	O
(	O
cid:107	O
)	O
x−x	O
(	O
cid:48	O
)	O
(	O
cid:107	O
)	O
2	O
2	O
=	O
e	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
1√	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
(	O
xx	O
(	O
cid:48	O
)	O
)	O
n	O
2	O
xn	O
−	O
x2	O
e	O
∞	O
(	O
cid:88	O
)	O
n	O
!	O
n	O
!	O
n=0	O
.	O
here	O
the	O
feature	B
space	I
is	O
of	O
inﬁnite	O
dimension	B
while	O
evaluating	O
the	O
kernel	O
is	O
very	O
16.2	O
the	O
kernel	B
trick	I
221	O
simple	O
.	O
more	O
generally	O
,	O
given	O
a	O
scalar	O
σ	O
>	O
0	O
,	O
the	O
gaussian	O
kernel	O
is	O
deﬁned	O
to	O
be	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
e	O
−	O
(	O
cid:107	O
)	O
x−x	O
(	O
cid:48	O
)	O
(	O
cid:107	O
)	O
2	O
2	O
σ	O
.	O
intuitively	O
,	O
the	O
gaussian	O
kernel	O
sets	O
the	O
inner	O
product	O
in	O
the	O
feature	B
space	I
between	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
to	O
be	O
close	O
to	O
zero	O
if	O
the	O
instances	O
are	O
far	O
away	O
from	O
each	O
other	O
(	O
in	O
the	O
original	O
domain	B
)	O
and	O
close	O
to	O
1	O
if	O
they	O
are	O
close	O
.	O
σ	O
is	O
a	O
parameter	O
that	O
controls	O
the	O
scale	O
determining	O
what	O
we	O
mean	O
by	O
“	O
close.	O
”	O
it	O
is	O
easy	O
to	O
verify	O
that	O
k	O
implements	O
an	O
inner	O
product	O
in	O
a	O
space	O
in	O
which	O
for	O
any	O
n	O
and	O
any	O
monomial	O
of	O
order	O
k	O
there	O
exists	O
an	O
element	O
of	O
ψ	O
(	O
x	O
)	O
that	O
equals	O
i=1	O
xji	O
.	O
hence	O
,	O
we	O
can	O
learn	O
any	O
polynomial	O
predictor	O
over	O
the	O
original	O
space	O
by	O
using	O
a	O
gaussian	O
kernel	O
.	O
e	O
2	O
(	O
cid:81	O
)	O
n	O
−	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
1√	O
n	O
!	O
recall	B
that	O
the	O
vc-dimension	O
of	O
the	O
class	O
of	O
all	O
polynomial	O
predictors	O
is	O
inﬁ-	O
nite	O
(	O
see	O
exercise	O
12	O
)	O
.	O
there	O
is	O
no	O
contradiction	O
,	O
because	O
the	O
sample	B
complexity	I
required	O
to	O
learn	O
with	O
gaussian	O
kernels	B
depends	O
on	O
the	O
margin	B
in	O
the	O
feature	B
space	I
,	O
which	O
will	O
be	O
large	O
if	O
we	O
are	O
lucky	O
,	O
but	O
can	O
in	O
general	O
be	O
arbitrarily	O
small	O
.	O
the	O
gaussian	O
kernel	O
is	O
also	O
called	O
the	O
rbf	O
kernel	O
,	O
for	O
“	O
radial	O
basis	O
func-	O
tions.	O
”	O
16.2.1	O
kernels	B
as	O
a	O
way	O
to	O
express	O
prior	B
knowledge	I
as	O
we	O
discussed	O
previously	O
,	O
a	O
feature	B
mapping	O
,	O
ψ	O
,	O
may	O
be	O
viewed	O
as	O
expanding	O
the	O
class	O
of	O
linear	O
classiﬁers	O
to	O
a	O
richer	O
class	O
(	O
corresponding	O
to	O
linear	O
classiﬁers	O
over	O
the	O
feature	B
space	I
)	O
.	O
however	O
,	O
as	O
discussed	O
in	O
the	O
book	O
so	O
far	O
,	O
the	O
suitability	O
of	O
any	O
hypothesis	B
class	I
to	O
a	O
given	O
learning	O
task	O
depends	O
on	O
the	O
nature	O
of	O
that	O
task	O
.	O
one	O
can	O
therefore	O
think	O
of	O
an	O
embedding	O
ψ	O
as	O
a	O
way	O
to	O
express	O
and	O
utilize	O
prior	B
knowledge	I
about	O
the	O
problem	O
at	O
hand	O
.	O
for	O
example	O
,	O
if	O
we	O
believe	O
that	O
positive	O
examples	O
can	O
be	O
distinguished	O
by	O
some	O
ellipse	O
,	O
we	O
can	O
deﬁne	O
ψ	O
to	O
be	O
all	O
the	O
monomials	O
up	O
to	O
order	O
2	O
,	O
or	O
use	O
a	O
degree	O
2	O
polynomial	B
kernel	I
.	O
as	O
a	O
more	O
realistic	O
example	O
,	O
consider	O
the	O
task	O
of	O
learning	O
to	O
ﬁnd	O
a	O
sequence	O
of	O
characters	O
(	O
“	O
signature	O
”	O
)	O
in	O
a	O
ﬁle	O
that	O
indicates	O
whether	O
it	O
contains	O
a	O
virus	O
or	O
not	O
.	O
formally	O
,	O
let	O
xd	O
be	O
the	O
set	B
of	O
all	O
strings	O
of	O
length	O
at	O
most	O
d	O
over	O
some	O
alphabet	O
set	B
σ.	O
the	O
hypothesis	B
class	I
that	O
one	O
wishes	O
to	O
learn	O
is	O
h	O
=	O
{	O
hv	O
:	O
v	O
∈	O
xd	O
}	O
,	O
where	O
,	O
for	O
a	O
string	O
x	O
∈	O
xd	O
,	O
hv	O
(	O
x	O
)	O
is	O
1	O
iﬀ	O
v	O
is	O
a	O
substring	O
of	O
x	O
(	O
and	O
hv	O
(	O
x	O
)	O
=	O
−1	O
otherwise	O
)	O
.	O
let	O
us	O
show	O
how	O
using	O
an	O
appropriate	O
embedding	O
this	O
class	O
can	O
be	O
realized	O
by	O
linear	O
classiﬁers	O
over	O
the	O
resulting	O
feature	B
space	I
.	O
consider	O
a	O
mapping	O
ψ	O
to	O
a	O
space	O
rs	O
where	O
s	O
=	O
|xd|	O
,	O
so	O
that	O
each	O
coordinate	O
of	O
ψ	O
(	O
x	O
)	O
corresponds	O
to	O
some	O
string	O
v	O
and	O
indicates	O
whether	O
v	O
is	O
a	O
substring	O
of	O
x	O
(	O
that	O
is	O
,	O
for	O
every	O
x	O
∈	O
xd	O
,	O
ψ	O
(	O
x	O
)	O
is	O
a	O
vector	O
in	O
{	O
0	O
,	O
1	O
}	O
|xd|	O
)	O
.	O
note	O
that	O
the	O
dimension	B
of	O
this	O
feature	B
space	I
is	O
exponential	O
in	O
d.	O
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
every	O
member	O
of	O
the	O
class	O
h	O
can	O
be	O
realized	O
by	O
composing	O
a	O
linear	O
classiﬁer	O
over	O
ψ	O
(	O
x	O
)	O
,	O
and	O
,	O
moreover	O
,	O
by	O
such	O
a	O
halfspace	B
whose	O
norm	O
is	O
1	O
and	O
that	O
attains	O
a	O
margin	B
of	O
1	O
(	O
see	O
exercise	O
1	O
)	O
.	O
furthermore	O
,	O
for	O
every	O
x	O
∈	O
x	O
,	O
(	O
cid:107	O
)	O
ψ	O
(	O
x	O
)	O
(	O
cid:107	O
)	O
=	O
o	O
(	O
d	O
)	O
.	O
so	O
,	O
overall	O
,	O
it	O
is	O
learnable	O
using	O
svm	O
with	O
a	O
sample	O
222	O
kernel	O
methods	O
complexity	O
that	O
is	O
polynomial	O
in	O
d.	O
however	O
,	O
the	O
dimension	B
of	O
the	O
feature	B
space	I
is	O
exponential	O
in	O
d	O
so	O
a	O
direct	O
implementation	O
of	O
svm	O
over	O
the	O
feature	B
space	I
is	O
problematic	O
.	O
luckily	O
,	O
it	O
is	O
easy	O
to	O
calculate	O
the	O
inner	O
product	O
in	O
the	O
feature	B
space	I
(	O
i.e.	O
,	O
the	O
kernel	O
function	O
)	O
without	O
explicitly	O
mapping	O
instances	O
into	O
the	O
feature	B
space	I
.	O
indeed	O
,	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
is	O
simply	O
the	O
number	O
of	O
common	O
substrings	O
of	O
x	O
and	O
x	O
(	O
cid:48	O
)	O
,	O
which	O
can	O
be	O
easily	O
calculated	O
in	O
time	O
polynomial	O
in	O
d.	O
this	O
example	O
also	O
demonstrates	O
how	O
feature	B
mapping	O
enables	O
us	O
to	O
use	O
halfspaces	O
for	O
nonvectorial	O
domains	O
.	O
16.2.2	O
characterizing	O
kernel	O
functions*	O
as	O
we	O
have	O
discussed	O
in	O
the	O
previous	O
section	O
,	O
we	O
can	O
think	O
of	O
the	O
speciﬁcation	O
of	O
the	O
kernel	O
matrix	O
as	O
a	O
way	O
to	O
express	O
prior	B
knowledge	I
.	O
consider	O
a	O
given	O
similarity	O
function	B
of	O
the	O
form	O
k	O
:	O
x	O
×	O
x	O
→	O
r.	O
is	O
it	O
a	O
valid	O
kernel	O
function	O
?	O
that	O
is	O
,	O
does	O
it	O
represent	O
an	O
inner	O
product	O
between	O
ψ	O
(	O
x	O
)	O
and	O
ψ	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
for	O
some	O
feature	B
mapping	O
ψ	O
?	O
the	O
following	O
lemma	O
gives	O
a	O
suﬃcient	O
and	O
necessary	O
condition	O
.	O
lemma	O
16.2	O
a	O
symmetric	O
function	B
k	O
:	O
x	O
×	O
x	O
→	O
r	O
implements	O
an	O
inner	O
product	O
in	O
some	O
hilbert	O
space	O
if	O
and	O
only	O
if	O
it	O
is	O
positive	O
semideﬁnite	O
;	O
namely	O
,	O
for	O
all	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
,	O
the	O
gram	O
matrix	O
,	O
gi	O
,	O
j	O
=	O
k	O
(	O
xi	O
,	O
xj	O
)	O
,	O
is	O
a	O
positive	O
semideﬁnite	O
matrix	O
.	O
proof	O
it	O
is	O
trivial	O
to	O
see	O
that	O
if	O
k	O
implements	O
an	O
inner	O
product	O
in	O
some	O
hilbert	O
space	O
then	O
the	O
gram	O
matrix	O
is	O
positive	O
semideﬁnite	O
.	O
for	O
the	O
other	O
direction	O
,	O
deﬁne	O
the	O
space	O
of	O
functions	O
over	O
x	O
as	O
rx	O
=	O
{	O
f	O
:	O
x	O
→	O
r	O
}	O
.	O
for	O
each	O
x	O
∈	O
x	O
let	O
ψ	O
(	O
x	O
)	O
be	O
the	O
function	B
x	O
(	O
cid:55	O
)	O
→	O
k	O
(	O
·	O
,	O
x	O
)	O
.	O
deﬁne	O
a	O
vector	O
space	O
by	O
taking	O
all	O
linear	O
combinations	O
of	O
elements	O
of	O
the	O
form	O
k	O
(	O
·	O
,	O
x	O
)	O
.	O
deﬁne	O
an	O
inner	O
product	O
on	O
this	O
vector	O
space	O
to	O
be	O
(	O
cid:42	O
)	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
(	O
cid:43	O
)	O
(	O
cid:88	O
)	O
αik	O
(	O
·	O
,	O
xi	O
)	O
,	O
βjk	O
(	O
·	O
,	O
x	O
(	O
cid:48	O
)	O
j	O
)	O
=	O
αiβjk	O
(	O
xi	O
,	O
x	O
(	O
cid:48	O
)	O
j	O
)	O
.	O
i	O
j	O
i	O
,	O
j	O
this	O
is	O
a	O
valid	O
inner	O
product	O
since	O
it	O
is	O
symmetric	O
(	O
because	O
k	O
is	O
symmetric	O
)	O
,	O
it	O
is	O
linear	O
(	O
immediate	O
)	O
,	O
and	O
it	O
is	O
positive	O
deﬁnite	O
(	O
it	O
is	O
easy	O
to	O
see	O
that	O
k	O
(	O
x	O
,	O
x	O
)	O
≥	O
0	O
with	O
equality	O
only	O
for	O
ψ	O
(	O
x	O
)	O
being	O
the	O
zero	O
function	B
)	O
.	O
clearly	O
,	O
(	O
cid:104	O
)	O
ψ	O
(	O
x	O
)	O
,	O
ψ	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
=	O
(	O
cid:104	O
)	O
k	O
(	O
·	O
,	O
x	O
)	O
,	O
k	O
(	O
·	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
=	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
,	O
which	O
concludes	O
our	O
proof	O
.	O
16.3	O
implementing	O
soft-svm	O
with	O
kernels	B
next	O
,	O
we	O
turn	O
to	O
solving	O
soft-svm	O
with	O
kernels	B
.	O
while	O
we	O
could	O
have	O
designed	O
an	O
algorithm	O
for	O
solving	O
equation	O
(	O
16.4	O
)	O
,	O
there	O
is	O
an	O
even	O
simpler	O
approach	O
that	O
16.3	O
implementing	O
soft-svm	O
with	O
kernels	B
223	O
directly	O
tackles	O
the	O
soft-svm	O
optimization	O
problem	O
in	O
the	O
feature	B
space	I
,	O
min	O
w	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
λ	O
2	O
1	O
m	O
max	O
{	O
0	O
,	O
1	O
−	O
y	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
xi	O
)	O
(	O
cid:105	O
)	O
}	O
,	O
(	O
16.5	O
)	O
(	O
cid:32	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
(	O
cid:33	O
)	O
while	O
only	O
using	O
kernel	O
evaluations	O
.	O
the	O
basic	O
observation	O
is	O
that	O
the	O
vector	O
w	O
(	O
t	O
)	O
maintained	O
by	O
the	O
sgd	O
procedure	O
we	O
have	O
described	O
in	O
section	O
15.5	O
is	O
always	O
in	O
the	O
linear	O
span	O
of	O
{	O
ψ	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
ψ	O
(	O
xm	O
)	O
}	O
.	O
therefore	O
,	O
rather	O
than	O
maintaining	O
w	O
(	O
t	O
)	O
we	O
can	O
maintain	O
the	O
corresponding	O
coeﬃcients	O
α.	O
formally	O
,	O
let	O
k	O
be	O
the	O
kernel	O
function	O
,	O
namely	O
,	O
for	O
all	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
,	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
(	O
cid:104	O
)	O
ψ	O
(	O
x	O
)	O
,	O
ψ	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
.	O
we	O
shall	O
maintain	O
two	O
vectors	O
in	O
rm	O
,	O
corresponding	O
to	O
two	O
vectors	O
θ	O
(	O
t	O
)	O
and	O
w	O
(	O
t	O
)	O
deﬁned	O
in	O
the	O
sgd	O
procedure	O
of	O
section	O
15.5.	O
that	O
is	O
,	O
β	O
(	O
t	O
)	O
will	O
be	O
a	O
vector	O
such	O
that	O
and	O
α	O
(	O
t	O
)	O
be	O
such	O
that	O
θ	O
(	O
t	O
)	O
=	O
w	O
(	O
t	O
)	O
=	O
β	O
(	O
t	O
)	O
j	O
ψ	O
(	O
xj	O
)	O
α	O
(	O
t	O
)	O
j	O
ψ	O
(	O
xj	O
)	O
.	O
(	O
16.6	O
)	O
(	O
16.7	O
)	O
j=1	O
the	O
vectors	O
β	O
and	O
α	O
are	O
updated	O
according	O
to	O
the	O
following	O
procedure	O
.	O
sgd	O
for	O
solving	O
soft-svm	O
with	O
kernels	B
m	O
(	O
cid:88	O
)	O
m	O
(	O
cid:88	O
)	O
j=1	O
goal	O
:	O
solve	O
equation	O
(	O
16.5	O
)	O
parameter	O
:	O
t	O
initialize	O
:	O
β	O
(	O
1	O
)	O
=	O
0	O
for	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
let	O
α	O
(	O
t	O
)	O
=	O
1	O
choose	O
i	O
uniformly	O
at	O
random	O
from	O
[	O
m	O
]	O
for	O
all	O
j	O
(	O
cid:54	O
)	O
=	O
i	O
set	B
β	O
(	O
t+1	O
)	O
if	O
(	O
yi	O
(	O
cid:80	O
)	O
m	O
=	O
β	O
(	O
t	O
)	O
j	O
k	O
(	O
xj	O
,	O
xi	O
)	O
<	O
1	O
)	O
=	O
β	O
(	O
t	O
)	O
set	B
β	O
(	O
t+1	O
)	O
j=1	O
α	O
(	O
t	O
)	O
λ	O
t	O
β	O
(	O
t	O
)	O
j	O
j	O
i	O
+	O
yi	O
i	O
else	O
output	O
:	O
¯w	O
=	O
(	O
cid:80	O
)	O
m	O
set	B
β	O
(	O
t+1	O
)	O
=	O
β	O
(	O
t	O
)	O
i	O
i	O
j=1	O
¯αjψ	O
(	O
xj	O
)	O
where	O
¯α	O
=	O
1	O
t	O
(	O
cid:80	O
)	O
t	O
t=1	O
α	O
(	O
t	O
)	O
the	O
following	O
lemma	O
shows	O
that	O
the	O
preceding	O
implementation	O
is	O
equivalent	O
to	O
running	O
the	O
sgd	O
procedure	O
described	O
in	O
section	O
15.5	O
on	O
the	O
feature	B
space	I
.	O
tion	O
15.5	O
,	O
when	O
applied	O
on	O
the	O
feature	B
space	I
,	O
and	O
let	O
¯w	O
=	O
(	O
cid:80	O
)	O
m	O
lemma	O
16.3	O
let	O
ˆw	O
be	O
the	O
output	O
of	O
the	O
sgd	O
procedure	O
described	O
in	O
sec-	O
j=1	O
¯αjψ	O
(	O
xj	O
)	O
be	O
the	O
output	O
of	O
applying	O
sgd	O
with	O
kernels	B
.	O
then	O
¯w	O
=	O
ˆw	O
.	O
proof	O
we	O
will	O
show	O
that	O
for	O
every	O
t	O
equation	O
(	O
16.6	O
)	O
holds	O
,	O
where	O
θ	O
(	O
t	O
)	O
is	O
the	O
result	O
of	O
running	O
the	O
sgd	O
procedure	O
described	O
in	O
section	O
15.5	O
in	O
the	O
feature	B
224	O
kernel	O
methods	O
λ	O
t	O
θ	O
(	O
t	O
)	O
,	O
this	O
claim	O
implies	O
space	O
.	O
by	O
the	O
deﬁnition	O
of	O
α	O
(	O
t	O
)	O
=	O
1	O
that	O
equation	O
(	O
16.7	O
)	O
also	O
holds	O
,	O
and	O
the	O
proof	O
of	O
our	O
lemma	O
will	O
follow	O
.	O
to	O
prove	O
that	O
equation	O
(	O
16.6	O
)	O
holds	O
we	O
use	O
a	O
simple	O
inductive	O
argument	O
.	O
for	O
t	O
=	O
1	O
the	O
claim	O
trivially	O
holds	O
.	O
assume	O
it	O
holds	O
for	O
t	O
≥	O
1.	O
then	O
,	O
λ	O
t	O
β	O
(	O
t	O
)	O
and	O
w	O
(	O
t	O
)	O
=	O
1	O
(	O
cid:68	O
)	O
(	O
cid:69	O
)	O
(	O
cid:42	O
)	O
(	O
cid:88	O
)	O
(	O
cid:43	O
)	O
m	O
(	O
cid:88	O
)	O
yi	O
w	O
(	O
t	O
)	O
,	O
ψ	O
(	O
xi	O
)	O
=	O
yi	O
α	O
(	O
t	O
)	O
j	O
ψ	O
(	O
xj	O
)	O
,	O
ψ	O
(	O
xi	O
)	O
=	O
yi	O
α	O
(	O
t	O
)	O
j	O
k	O
(	O
xj	O
,	O
xi	O
)	O
.	O
j	O
j=1	O
hence	O
,	O
the	O
condition	O
in	O
the	O
two	O
algorithms	O
is	O
equivalent	O
and	O
if	O
we	O
update	O
θ	O
we	O
have	O
m	O
(	O
cid:88	O
)	O
m	O
(	O
cid:88	O
)	O
β	O
(	O
t	O
)	O
j	O
ψ	O
(	O
xj	O
)	O
+	O
yiψ	O
(	O
xi	O
)	O
=	O
β	O
(	O
t+1	O
)	O
j	O
ψ	O
(	O
xj	O
)	O
,	O
j=1	O
j=1	O
θ	O
(	O
t+1	O
)	O
=	O
θ	O
(	O
t	O
)	O
+	O
yiψ	O
(	O
xi	O
)	O
=	O
which	O
concludes	O
our	O
proof	O
.	O
16.4	O
summary	O
mappings	O
from	O
the	O
given	O
domain	B
to	O
some	O
higher	O
dimensional	O
space	O
,	O
on	O
which	O
a	O
halfspace	B
predictor	O
is	O
used	O
,	O
can	O
be	O
highly	O
powerful	O
.	O
we	O
beneﬁt	O
from	O
a	O
rich	O
and	O
complex	O
hypothesis	B
class	I
,	O
yet	O
need	O
to	O
solve	O
the	O
problems	O
of	O
high	O
sample	O
and	O
computational	O
complexities	O
.	O
in	O
chapter	O
10	O
,	O
we	O
discussed	O
the	O
adaboost	O
algo-	O
rithm	O
,	O
which	O
faces	O
these	O
challenges	O
by	O
using	O
a	O
weak	O
learner	O
:	O
even	O
though	O
we	O
’	O
re	O
in	O
a	O
very	O
high	O
dimensional	O
space	O
,	O
we	O
have	O
an	O
“	O
oracle	O
”	O
that	O
bestows	O
on	O
us	O
a	O
single	O
good	O
coordinate	O
to	O
work	O
with	O
on	O
each	O
iteration	O
.	O
in	O
this	O
chapter	O
we	O
intro-	O
duced	O
a	O
diﬀerent	O
approach	O
,	O
the	O
kernel	B
trick	I
.	O
the	O
idea	O
is	O
that	O
in	O
order	O
to	O
ﬁnd	O
a	O
halfspace	B
predictor	O
in	O
the	O
high	O
dimensional	O
space	O
,	O
we	O
do	O
not	O
need	O
to	O
know	O
the	O
representation	O
of	O
instances	O
in	O
that	O
space	O
,	O
but	O
rather	O
the	O
values	O
of	O
inner	O
products	O
between	O
the	O
mapped	O
instances	O
.	O
calculating	O
inner	O
products	O
between	O
instances	O
in	O
the	O
high	O
dimensional	O
space	O
without	O
using	O
their	O
representation	O
in	O
that	O
space	O
is	O
done	O
using	O
kernel	O
functions	O
.	O
we	O
have	O
also	O
shown	O
how	O
the	O
sgd	O
algorithm	O
can	O
be	O
implemented	O
using	O
kernels	B
.	O
the	O
ideas	O
of	O
feature	B
mapping	O
and	O
the	O
kernel	B
trick	I
allow	O
us	O
to	O
use	O
the	O
framework	O
of	O
halfspaces	O
and	O
linear	B
predictors	I
for	O
nonvectorial	O
data	O
.	O
we	O
demonstrated	O
how	O
kernels	B
can	O
be	O
used	O
to	O
learn	O
predictors	O
over	O
the	O
domain	O
of	O
strings	O
.	O
we	O
presented	O
the	O
applicability	O
of	O
the	O
kernel	B
trick	I
in	O
svm	O
.	O
however	O
,	O
the	O
kernel	B
trick	I
can	O
be	O
applied	O
in	O
many	O
other	O
algorithms	O
.	O
a	O
few	O
examples	O
are	O
given	O
as	O
exercises	O
.	O
this	O
chapter	O
ends	O
the	O
series	O
of	O
chapters	O
on	O
linear	B
predictors	I
and	O
convex	B
prob-	O
lems	O
.	O
the	O
next	O
two	O
chapters	O
deal	O
with	O
completely	O
diﬀerent	O
types	O
of	O
hypothesis	B
classes	O
.	O
16.5	O
bibliographic	O
remarks	O
225	O
16.5	O
bibliographic	O
remarks	O
in	O
the	O
context	O
of	O
svm	O
,	O
the	O
kernel-trick	O
has	O
been	O
introduced	O
in	O
boser	O
et	O
al	O
.	O
(	O
1992	O
)	O
.	O
see	O
also	O
aizerman	O
,	O
braverman	O
&	O
rozonoer	O
(	O
1964	O
)	O
.	O
the	O
observation	O
that	O
the	O
kernel-trick	O
can	O
be	O
applied	O
whenever	O
an	O
algorithm	O
only	O
relies	O
on	O
inner	O
products	O
was	O
ﬁrst	O
stated	O
by	O
sch¨olkopf	O
,	O
smola	O
&	O
m¨uller	O
(	O
1998	O
)	O
.	O
the	O
proof	O
of	O
the	O
representer	B
theorem	I
is	O
given	O
in	O
(	O
sch¨olkopf	O
,	O
herbrich	O
,	O
smola	O
&	O
williamson	O
2000	O
,	O
sch¨olkopf	O
,	O
herbrich	O
&	O
smola	O
2001	O
)	O
.	O
the	O
conditions	O
stated	O
in	O
lemma	O
16.2	O
are	O
simpliﬁcation	O
of	O
conditions	O
due	O
to	O
mercer	O
.	O
many	O
useful	O
kernel	O
functions	O
have	O
been	O
introduced	O
in	O
the	O
literature	O
for	O
various	O
applications	O
.	O
we	O
refer	O
the	O
reader	O
to	O
sch¨olkopf	O
&	O
smola	O
(	O
2002	O
)	O
.	O
16.6	O
exercises	O
1.	O
consider	O
the	O
task	O
of	O
ﬁnding	O
a	O
sequence	O
of	O
characters	O
in	O
a	O
ﬁle	O
,	O
as	O
described	O
in	O
section	O
16.2.1.	O
show	O
that	O
every	O
member	O
of	O
the	O
class	O
h	O
can	O
be	O
realized	O
by	O
composing	O
a	O
linear	O
classiﬁer	O
over	O
ψ	O
(	O
x	O
)	O
,	O
whose	O
norm	O
is	O
1	O
and	O
that	O
attains	O
a	O
margin	B
of	O
1	O
.	O
2.	O
kernelized	O
perceptron	O
:	O
show	O
how	O
to	O
run	O
the	O
perceptron	O
algorithm	O
while	O
only	O
accessing	O
the	O
instances	O
via	O
the	O
kernel	O
function	O
.	O
hint	O
:	O
the	O
derivation	O
is	O
similar	O
to	O
the	O
derivation	O
of	O
implementing	O
sgd	O
with	O
kernels	B
.	O
3.	O
kernel	B
ridge	I
regression	I
:	O
the	O
ridge	B
regression	I
problem	O
,	O
with	O
a	O
feature	B
mapping	O
ψ	O
,	O
is	O
the	O
problem	O
of	O
ﬁnding	O
a	O
vector	O
w	O
that	O
minimizes	O
the	O
function	B
f	O
(	O
w	O
)	O
=	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
1	O
2m	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
xi	O
)	O
(	O
cid:105	O
)	O
−	O
yi	O
)	O
2	O
,	O
(	O
16.8	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
and	O
then	O
returning	O
the	O
predictor	B
h	O
(	O
x	O
)	O
=	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
.	O
such	O
that	O
(	O
cid:80	O
)	O
m	O
show	O
how	O
to	O
implement	O
the	O
ridge	B
regression	I
algorithm	O
with	O
kernels	B
.	O
hint	O
:	O
the	O
representer	B
theorem	I
tells	O
us	O
that	O
there	O
exists	O
a	O
vector	O
α	O
∈	O
rm	O
i=1	O
αiψ	O
(	O
xi	O
)	O
is	O
a	O
minimizer	O
of	O
equation	O
(	O
16.8	O
)	O
.	O
1.	O
let	O
g	O
be	O
the	O
gram	O
matrix	O
with	O
regard	O
to	O
s	O
and	O
k.	O
that	O
is	O
,	O
gij	O
=	O
k	O
(	O
xi	O
,	O
xj	O
)	O
.	O
deﬁne	O
g	O
:	O
rm	O
→	O
r	O
by	O
g	O
(	O
α	O
)	O
=	O
λ	O
·	O
αt	O
gα	O
+	O
tion	O
(	O
16.9	O
)	O
then	O
w∗	O
=	O
(	O
cid:80	O
)	O
m	O
i=1	O
α∗	O
m	O
(	O
cid:88	O
)	O
i=1	O
1	O
2m	O
where	O
g·	O
,	O
i	O
is	O
the	O
i	O
’	O
th	O
column	O
of	O
g.	O
show	O
that	O
if	O
α∗	O
minimizes	O
equa-	O
2.	O
find	O
a	O
closed	O
form	O
expression	O
for	O
α∗	O
.	O
4.	O
let	O
n	O
be	O
any	O
positive	O
integer	O
.	O
for	O
every	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
deﬁne	O
i	O
ψ	O
(	O
xi	O
)	O
is	O
a	O
minimizer	O
of	O
f	O
.	O
(	O
(	O
cid:104	O
)	O
α	O
,	O
g·	O
,	O
i	O
(	O
cid:105	O
)	O
−	O
yi	O
)	O
2	O
,	O
(	O
16.9	O
)	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
min	O
{	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
}	O
.	O
226	O
kernel	O
methods	O
prove	O
that	O
k	O
is	O
a	O
valid	O
kernel	O
;	O
namely	O
,	O
ﬁnd	O
a	O
mapping	O
ψ	O
:	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
→	O
h	O
where	O
h	O
is	O
some	O
hilbert	O
space	O
,	O
such	O
that	O
∀x	O
,	O
x	O
(	O
cid:48	O
)	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
(	O
cid:104	O
)	O
ψ	O
(	O
x	O
)	O
,	O
ψ	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
.	O
5.	O
a	O
supermarket	O
manager	O
would	O
like	O
to	O
learn	O
which	O
of	O
his	O
customers	O
have	O
babies	O
on	O
the	O
basis	O
of	O
their	O
shopping	O
carts	O
.	O
speciﬁcally	O
,	O
he	O
sampled	O
i.i.d	O
.	O
customers	O
,	O
where	O
for	O
customer	O
i	O
,	O
let	O
xi	O
⊂	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
}	O
denote	O
the	O
subset	O
of	O
items	O
the	O
customer	O
bought	O
,	O
and	O
let	O
yi	O
∈	O
{	O
±1	O
}	O
be	O
the	O
label	B
indicating	O
whether	O
this	O
customer	O
has	O
a	O
baby	O
.	O
as	O
prior	B
knowledge	I
,	O
the	O
manager	O
knows	O
that	O
there	O
are	O
k	O
items	O
such	O
that	O
the	O
label	B
is	O
determined	O
to	O
be	O
1	O
iﬀ	O
the	O
customer	O
bought	O
at	O
least	O
one	O
of	O
these	O
k	O
items	O
.	O
of	O
course	O
,	O
the	O
identity	O
of	O
these	O
k	O
items	O
is	O
not	O
known	O
(	O
otherwise	O
,	O
there	O
was	O
nothing	O
to	O
learn	O
)	O
.	O
in	O
addition	O
,	O
according	O
to	O
the	O
store	O
regulation	O
,	O
each	O
customer	O
can	O
buy	O
at	O
most	O
s	O
items	O
.	O
help	O
the	O
manager	O
to	O
design	O
a	O
learning	O
algorithm	O
such	O
that	O
both	O
its	O
time	O
complexity	O
and	O
its	O
sample	B
complexity	I
are	O
polynomial	O
in	O
s	O
,	O
k	O
,	O
and	O
1/	O
.	O
6.	O
let	O
x	O
be	O
an	O
instance	B
set	O
and	O
let	O
ψ	O
be	O
a	O
feature	B
mapping	O
of	O
x	O
into	O
some	O
hilbert	O
feature	B
space	I
v	O
.	O
let	O
k	O
:	O
x	O
×	O
x	O
→	O
r	O
be	O
a	O
kernel	O
function	O
that	O
implements	O
inner	O
products	O
in	O
the	O
feature	B
space	I
v	O
.	O
consider	O
the	O
binary	O
classiﬁcation	O
algorithm	O
that	O
predicts	O
the	O
label	B
of	O
an	O
unseen	O
instance	B
according	O
to	O
the	O
class	O
with	O
the	O
closest	O
average	O
.	O
formally	O
,	O
given	O
a	O
training	O
sequence	O
s	O
=	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
,	O
for	O
every	O
y	O
∈	O
{	O
±1	O
}	O
we	O
deﬁne	O
(	O
cid:88	O
)	O
i	O
:	O
yi=y	O
cy	O
=	O
1	O
my	O
ψ	O
(	O
xi	O
)	O
.	O
where	O
my	O
=	O
|	O
{	O
i	O
:	O
yi	O
=	O
y	O
}	O
|	O
.	O
we	O
assume	O
that	O
m+	O
and	O
m−	O
are	O
nonzero	O
.	O
then	O
,	O
the	O
algorithm	O
outputs	O
the	O
following	O
decision	O
rule	O
:	O
(	O
cid:40	O
)	O
h	O
(	O
x	O
)	O
=	O
(	O
cid:107	O
)	O
ψ	O
(	O
x	O
)	O
−	O
c+	O
(	O
cid:107	O
)	O
≤	O
(	O
cid:107	O
)	O
ψ	O
(	O
x	O
)	O
−	O
c−	O
(	O
cid:107	O
)	O
otherwise	O
.	O
1	O
0	O
1.	O
let	O
w	O
=	O
c+	O
−	O
c−	O
and	O
let	O
b	O
=	O
1	O
2	O
(	O
(	O
cid:107	O
)	O
c−	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
cid:107	O
)	O
c+	O
(	O
cid:107	O
)	O
2	O
)	O
.	O
show	O
that	O
h	O
(	O
x	O
)	O
=	O
sign	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
+	O
b	O
)	O
.	O
2.	O
show	O
how	O
to	O
express	O
h	O
(	O
x	O
)	O
on	O
the	O
basis	O
of	O
the	O
kernel	O
function	O
,	O
and	O
without	O
accessing	O
individual	O
entries	O
of	O
ψ	O
(	O
x	O
)	O
or	O
w.	O
17	O
multiclass	B
,	O
ranking	B
,	O
and	O
complex	O
prediction	O
problems	O
multiclass	B
categorization	O
is	O
the	O
problem	O
of	O
classifying	O
instances	O
into	O
one	O
of	O
several	O
possible	O
target	O
classes	O
.	O
that	O
is	O
,	O
we	O
are	O
aiming	O
at	O
learning	O
a	O
predictor	B
h	O
:	O
x	O
→	O
y	O
,	O
where	O
y	O
is	O
a	O
ﬁnite	O
set	B
of	O
categories	O
.	O
applications	O
include	O
,	O
for	O
example	O
,	O
catego-	O
rizing	O
documents	O
according	O
to	O
topic	O
(	O
x	O
is	O
the	O
set	B
of	O
documents	O
and	O
y	O
is	O
the	O
set	B
of	O
possible	O
topics	O
)	O
or	O
determining	O
which	O
object	O
appears	O
in	O
a	O
given	O
image	O
(	O
x	O
is	O
the	O
set	B
of	O
images	O
and	O
y	O
is	O
the	O
set	B
of	O
possible	O
objects	O
)	O
.	O
the	O
centrality	O
of	O
the	O
multiclass	B
learning	O
problem	O
has	O
spurred	O
the	O
development	O
of	O
various	O
approaches	O
for	O
tackling	O
the	O
task	O
.	O
perhaps	O
the	O
most	O
straightforward	O
approach	O
is	O
a	O
reduction	O
from	O
multiclass	B
classiﬁcation	O
to	O
binary	O
classiﬁcation	O
.	O
in	O
section	O
17.1	O
we	O
discuss	O
the	O
most	O
common	O
two	O
reductions	B
as	O
well	O
as	O
the	O
main	O
drawback	O
of	O
the	O
reduction	O
approach	O
.	O
we	O
then	O
turn	O
to	O
describe	O
a	O
family	O
of	O
linear	B
predictors	I
for	O
multiclass	B
problems	O
.	O
relying	O
on	O
the	O
rlm	O
and	O
sgd	O
frameworks	O
from	O
previous	O
chapters	O
,	O
we	O
describe	O
several	O
practical	O
algorithms	O
for	O
multiclass	B
prediction	O
.	O
in	O
section	O
17.3	O
we	O
show	O
how	O
to	O
use	O
the	O
multiclass	B
machinery	O
for	O
complex	O
pre-	O
diction	O
problems	O
in	O
which	O
y	O
can	O
be	O
extremely	O
large	O
but	O
has	O
some	O
structure	O
on	O
it	O
.	O
this	O
task	O
is	O
often	O
called	O
structured	O
output	O
learning	O
.	O
in	O
particular	O
,	O
we	O
demon-	O
strate	O
this	O
approach	O
for	O
the	O
task	O
of	O
recognizing	O
handwritten	O
words	O
,	O
in	O
which	O
y	O
is	O
the	O
set	B
of	O
all	O
possible	O
strings	O
of	O
some	O
bounded	O
length	O
(	O
hence	O
,	O
the	O
size	O
of	O
y	O
is	O
exponential	O
in	O
the	O
maximal	O
length	O
of	O
a	O
word	O
)	O
.	O
finally	O
,	O
in	O
section	O
17.4	O
and	O
section	O
17.5	O
we	O
discuss	O
ranking	B
problems	O
in	O
which	O
the	O
learner	O
should	O
order	O
a	O
set	B
of	O
instances	O
according	O
to	O
their	O
“	O
relevance.	O
”	O
a	O
typ-	O
ical	O
application	O
is	O
ordering	O
results	O
of	O
a	O
search	O
engine	O
according	O
to	O
their	O
relevance	O
to	O
the	O
query	O
.	O
we	O
describe	O
several	O
performance	O
measures	O
that	O
are	O
adequate	O
for	O
assessing	O
the	O
performance	O
of	O
ranking	B
predictors	O
and	O
describe	O
how	O
to	O
learn	O
linear	B
predictors	I
for	O
ranking	B
problems	O
eﬃciently	O
.	O
17.1	O
one-versus-all	O
and	O
all-pairs	B
the	O
simplest	O
approach	O
to	O
tackle	O
multiclass	B
prediction	O
problems	O
is	O
by	O
reduction	O
to	O
binary	O
classiﬁcation	O
.	O
recall	B
that	O
in	O
multiclass	B
prediction	O
we	O
would	O
like	O
to	O
learn	O
a	O
function	B
h	O
:	O
x	O
→	O
y.	O
without	O
loss	B
of	O
generality	O
let	O
us	O
denote	O
y	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
.	O
in	O
the	O
one-versus-all	O
method	O
(	O
a.k.a	O
.	O
one-versus-rest	O
)	O
we	O
train	O
k	O
binary	O
clas-	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
228	O
multiclass	B
,	O
ranking	B
,	O
and	O
complex	O
prediction	O
problems	O
siﬁers	O
,	O
each	O
of	O
which	O
discriminates	O
between	O
one	O
class	O
and	O
the	O
rest	O
of	O
the	O
classes	O
.	O
that	O
is	O
,	O
given	O
a	O
training	B
set	I
s	O
=	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
,	O
where	O
every	O
yi	O
is	O
in	O
y	O
,	O
we	O
construct	O
k	O
binary	O
training	O
sets	O
,	O
s1	O
,	O
.	O
.	O
.	O
,	O
sk	O
,	O
where	O
si	O
=	O
(	O
x1	O
,	O
(	O
−1	O
)	O
1	O
[	O
y1	O
(	O
cid:54	O
)	O
=i	O
]	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
(	O
−1	O
)	O
1	O
[	O
ym	O
(	O
cid:54	O
)	O
=i	O
]	O
)	O
.	O
in	O
words	O
,	O
si	O
is	O
the	O
set	B
of	O
instances	O
labeled	O
1	O
if	O
their	O
label	B
in	O
s	O
was	O
i	O
,	O
and	O
−1	O
otherwise	O
.	O
for	O
every	O
i	O
∈	O
[	O
k	O
]	O
we	O
train	O
a	O
binary	O
predictor	B
hi	O
:	O
x	O
→	O
{	O
±1	O
}	O
based	O
on	O
si	O
,	O
hoping	O
that	O
hi	O
(	O
x	O
)	O
should	O
equal	O
1	O
if	O
and	O
only	O
if	O
x	O
belongs	O
to	O
class	O
i.	O
then	O
,	O
given	O
h1	O
,	O
.	O
.	O
.	O
,	O
hk	O
,	O
we	O
construct	O
a	O
multiclass	B
predictor	O
using	O
the	O
rule	O
h	O
(	O
x	O
)	O
∈	O
argmax	O
i∈	O
[	O
k	O
]	O
hi	O
(	O
x	O
)	O
.	O
(	O
17.1	O
)	O
when	O
more	O
than	O
one	O
binary	O
hypothesis	B
predicts	O
“	O
1	O
”	O
we	O
should	O
somehow	O
decide	O
which	O
class	O
to	O
predict	O
(	O
e.g.	O
,	O
we	O
can	O
arbitrarily	O
decide	O
to	O
break	O
ties	O
by	O
taking	O
the	O
minimal	O
index	O
in	O
argmaxi	O
hi	O
(	O
x	O
)	O
)	O
.	O
a	O
better	O
approach	O
can	O
be	O
applied	O
whenever	O
each	O
hi	O
hides	O
additional	O
information	O
,	O
which	O
can	O
be	O
interpreted	O
as	O
the	O
conﬁdence	B
in	O
the	O
prediction	O
y	O
=	O
i.	O
for	O
example	O
,	O
this	O
is	O
the	O
case	O
in	O
halfspaces	O
,	O
where	O
the	O
actual	O
prediction	O
is	O
sign	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
,	O
but	O
we	O
can	O
interpret	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
as	O
the	O
conﬁdence	B
in	O
the	O
prediction	O
.	O
in	O
such	O
cases	O
,	O
we	O
can	O
apply	O
the	O
multiclass	B
rule	O
given	O
in	O
equa-	O
tion	O
(	O
17.1	O
)	O
on	O
the	O
real	O
valued	O
predictions	O
.	O
a	O
pseudocode	O
of	O
the	O
one-versus-all	O
approach	O
is	O
given	O
in	O
the	O
following	O
.	O
one-versus-all	O
input	O
:	O
training	B
set	I
s	O
=	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
algorithm	O
for	O
binary	O
classiﬁcation	O
a	O
foreach	O
i	O
∈	O
y	O
let	O
si	O
=	O
(	O
x1	O
,	O
(	O
−1	O
)	O
1	O
[	O
y1	O
(	O
cid:54	O
)	O
=i	O
]	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
(	O
−1	O
)	O
1	O
[	O
ym	O
(	O
cid:54	O
)	O
=i	O
]	O
)	O
let	O
hi	O
=	O
a	O
(	O
si	O
)	O
output	O
:	O
the	O
multiclass	B
hypothesis	O
deﬁned	O
by	O
h	O
(	O
x	O
)	O
∈	O
argmaxi∈y	O
hi	O
(	O
x	O
)	O
another	O
popular	O
reduction	O
is	O
the	O
all-pairs	B
approach	O
,	O
in	O
which	O
all	O
pairs	O
of	O
classes	O
are	O
compared	O
to	O
each	O
other	O
.	O
formally	O
,	O
given	O
a	O
training	B
set	I
s	O
=	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
,	O
where	O
every	O
yi	O
is	O
in	O
[	O
k	O
]	O
,	O
for	O
every	O
1	O
≤	O
i	O
<	O
j	O
≤	O
k	O
we	O
construct	O
a	O
binary	O
training	O
sequence	O
,	O
si	O
,	O
j	O
,	O
containing	O
all	O
examples	O
from	O
s	O
whose	O
label	B
is	O
either	O
i	O
or	O
j.	O
for	O
each	O
such	O
an	O
example	O
,	O
we	O
set	B
the	O
binary	O
label	B
in	O
si	O
,	O
j	O
to	O
be	O
+1	O
if	O
the	O
multiclass	B
label	O
in	O
s	O
is	O
i	O
and	O
−1	O
if	O
the	O
multiclass	B
label	O
in	O
s	O
is	O
j.	O
next	O
,	O
we	O
train	O
a	O
binary	O
classiﬁcation	O
algorithm	O
based	O
on	O
every	O
si	O
,	O
j	O
to	O
get	O
hi	O
,	O
j	O
.	O
finally	O
,	O
we	O
construct	O
a	O
multiclass	B
classiﬁer	O
by	O
predicting	O
the	O
class	O
that	O
had	O
the	O
highest	O
number	O
of	O
“	O
wins.	O
”	O
a	O
pseudocode	O
of	O
the	O
all-pairs	B
approach	O
is	O
given	O
in	O
the	O
following	O
.	O
17.1	O
one-versus-all	O
and	O
all-pairs	B
229	O
all-pairs	B
input	O
:	O
training	B
set	I
s	O
=	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
algorithm	O
for	O
binary	O
classiﬁcation	O
a	O
foreach	O
i	O
,	O
j	O
∈	O
y	O
s.t	O
.	O
i	O
<	O
j	O
initialize	O
si	O
,	O
j	O
to	O
be	O
the	O
empty	O
sequence	O
for	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
if	O
yt	O
=	O
i	O
add	O
(	O
xt	O
,	O
1	O
)	O
to	O
si	O
,	O
j	O
if	O
yt	O
=	O
j	O
add	O
(	O
xt	O
,	O
−1	O
)	O
to	O
si	O
,	O
j	O
let	O
hi	O
,	O
j	O
=	O
a	O
(	O
si	O
,	O
j	O
)	O
output	O
:	O
the	O
multiclass	B
hypothesis	O
deﬁned	O
by	O
h	O
(	O
x	O
)	O
∈	O
argmaxi∈y	O
(	O
cid:16	O
)	O
(	O
cid:80	O
)	O
(	O
cid:17	O
)	O
j∈y	O
sign	O
(	O
j	O
−	O
i	O
)	O
hi	O
,	O
j	O
(	O
x	O
)	O
although	O
reduction	O
methods	O
such	O
as	O
the	O
one-versus-all	O
and	O
all-pairs	B
are	O
simple	O
and	O
easy	O
to	O
construct	O
from	O
existing	O
algorithms	O
,	O
their	O
simplicity	O
has	O
a	O
price	O
.	O
the	O
binary	O
learner	O
is	O
not	O
aware	O
of	O
the	O
fact	O
that	O
we	O
are	O
going	O
to	O
use	O
its	O
output	O
hypotheses	O
for	O
constructing	O
a	O
multiclass	B
predictor	O
,	O
and	O
this	O
might	O
lead	O
to	O
suboptimal	O
results	O
,	O
as	O
illustrated	O
in	O
the	O
following	O
example	O
.	O
example	O
17.1	O
consider	O
a	O
multiclass	B
categorization	O
problem	O
in	O
which	O
the	O
in-	O
stance	O
space	O
is	O
x	O
=	O
r2	O
and	O
the	O
label	B
set	O
is	O
y	O
=	O
{	O
1	O
,	O
2	O
,	O
3	O
}	O
.	O
suppose	O
that	O
instances	O
of	O
the	O
diﬀerent	O
classes	O
are	O
located	O
in	O
nonintersecting	O
balls	O
as	O
depicted	O
in	O
the	O
fol-	O
lowing	O
.	O
1	O
2	O
3	O
suppose	O
that	O
the	O
probability	O
masses	O
of	O
classes	O
1	O
,	O
2	O
,	O
3	O
are	O
40	O
%	O
,	O
20	O
%	O
,	O
and	O
40	O
%	O
,	O
respectively	O
.	O
consider	O
the	O
application	O
of	O
one-versus-all	O
to	O
this	O
problem	O
,	O
and	O
as-	O
sume	O
that	O
the	O
binary	O
classiﬁcation	O
algorithm	O
used	O
by	O
one-versus-all	O
is	O
erm	O
with	O
respect	O
to	O
the	O
hypothesis	B
class	I
of	O
halfspaces	O
.	O
observe	O
that	O
for	O
the	O
prob-	O
lem	O
of	O
discriminating	O
between	O
class	O
2	O
and	O
the	O
rest	O
of	O
the	O
classes	O
,	O
the	O
optimal	O
halfspace	B
would	O
be	O
the	O
all	O
negative	O
classiﬁer	B
.	O
therefore	O
,	O
the	O
multiclass	B
predic-	O
tor	O
constructed	O
by	O
one-versus-all	O
might	O
err	O
on	O
all	O
the	O
examples	O
from	O
class	O
2	O
(	O
this	O
will	O
be	O
the	O
case	O
if	O
the	O
tie	O
in	O
the	O
deﬁnition	O
of	O
h	O
(	O
x	O
)	O
is	O
broken	O
by	O
the	O
nu-	O
merical	O
value	O
of	O
the	O
class	O
label	B
)	O
.	O
in	O
contrast	O
,	O
if	O
we	O
choose	O
hi	O
(	O
x	O
)	O
=	O
(	O
cid:104	O
)	O
wi	O
,	O
x	O
(	O
cid:105	O
)	O
,	O
where	O
w1	O
=	O
,	O
then	O
the	O
classi-	O
ﬁer	O
deﬁned	O
by	O
h	O
(	O
x	O
)	O
=	O
argmaxi	O
hi	O
(	O
x	O
)	O
perfectly	O
predicts	O
all	O
the	O
examples	O
.	O
we	O
see	O
,	O
w2	O
=	O
(	O
0	O
,	O
1	O
)	O
,	O
and	O
w3	O
=	O
(	O
cid:16	O
)	O
−	O
1√	O
(	O
cid:16	O
)	O
1√	O
,	O
1√	O
2	O
2	O
,	O
1√	O
2	O
2	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
230	O
multiclass	B
,	O
ranking	B
,	O
and	O
complex	O
prediction	O
problems	O
that	O
even	O
though	O
the	O
approximation	B
error	I
of	O
the	O
class	O
of	O
predictors	O
of	O
the	O
form	O
h	O
(	O
x	O
)	O
=	O
argmaxi	O
(	O
cid:104	O
)	O
wi	O
,	O
x	O
(	O
cid:105	O
)	O
is	O
zero	O
,	O
the	O
one-versus-all	O
approach	O
might	O
fail	O
to	O
ﬁnd	O
a	O
good	O
predictor	B
from	O
this	O
class	O
.	O
17.2	O
linear	O
multiclass	O
predictors	O
in	O
light	O
of	O
the	O
inadequacy	O
of	O
reduction	O
methods	O
,	O
in	O
this	O
section	O
we	O
study	O
a	O
more	O
direct	O
approach	O
for	O
learning	O
multiclass	O
predictors	O
.	O
we	O
describe	O
the	O
family	O
of	O
linear	O
multiclass	O
predictors	O
.	O
to	O
motivate	O
the	O
construction	O
of	O
this	O
family	O
,	O
recall	B
that	O
a	O
linear	B
predictor	I
for	O
binary	O
classiﬁcation	O
(	O
i.e.	O
,	O
a	O
halfspace	B
)	O
takes	O
the	O
form	O
h	O
(	O
x	O
)	O
=	O
sign	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
.	O
an	O
equivalent	O
way	O
to	O
express	O
the	O
prediction	O
is	O
as	O
follows	O
:	O
h	O
(	O
x	O
)	O
=	O
argmax	O
y∈	O
{	O
±1	O
}	O
(	O
cid:104	O
)	O
w	O
,	O
yx	O
(	O
cid:105	O
)	O
,	O
where	O
yx	O
is	O
the	O
vector	O
obtained	O
by	O
multiplying	O
each	O
element	O
of	O
x	O
by	O
y.	O
this	O
representation	O
leads	O
to	O
a	O
natural	O
generalization	O
of	O
halfspaces	O
to	O
multiclass	B
problems	O
as	O
follows	O
.	O
let	O
ψ	O
:	O
x	O
×	O
y	O
→	O
rd	O
be	O
a	O
class-sensitive	B
feature	I
mapping	I
.	O
that	O
is	O
,	O
ψ	O
takes	O
as	O
input	O
a	O
pair	O
(	O
x	O
,	O
y	O
)	O
and	O
maps	O
it	O
into	O
a	O
d	O
dimensional	O
feature	B
vector	O
.	O
intuitively	O
,	O
we	O
can	O
think	O
of	O
the	O
elements	O
of	O
ψ	O
(	O
x	O
,	O
y	O
)	O
as	O
score	O
functions	O
that	O
assess	O
how	O
well	O
the	O
label	B
y	O
ﬁts	O
the	O
instance	B
x.	O
we	O
will	O
elaborate	O
on	O
ψ	O
later	O
on	O
.	O
given	O
ψ	O
and	O
a	O
vector	O
w	O
∈	O
rd	O
,	O
we	O
can	O
deﬁne	O
a	O
multiclass	B
predictor	O
,	O
h	O
:	O
x	O
→	O
y	O
,	O
as	O
follows	O
:	O
h	O
(	O
x	O
)	O
=	O
argmax	O
y∈y	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
.	O
that	O
is	O
,	O
the	O
prediction	O
of	O
h	O
for	O
the	O
input	O
x	O
is	O
the	O
label	B
that	O
achieves	O
the	O
highest	O
weighted	O
score	O
,	O
where	O
weighting	O
is	O
according	O
to	O
the	O
vector	O
w.	O
let	O
w	O
be	O
some	O
set	B
of	O
vectors	O
in	O
rd	O
,	O
for	O
example	O
,	O
w	O
=	O
{	O
w	O
∈	O
rd	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
≤	O
b	O
}	O
,	O
for	O
some	O
scalar	O
b	O
>	O
0.	O
each	O
pair	O
(	O
ψ	O
,	O
w	O
)	O
deﬁnes	O
a	O
hypothesis	B
class	I
of	O
multiclass	B
predictors	O
:	O
hψ	O
,	O
w	O
=	O
{	O
x	O
(	O
cid:55	O
)	O
→	O
argmax	O
y∈y	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
:	O
w	O
∈	O
w	O
}	O
.	O
of	O
course	O
,	O
the	O
immediate	O
question	O
,	O
which	O
we	O
discuss	O
in	O
the	O
sequel	O
,	O
is	O
how	O
to	O
construct	O
a	O
good	O
ψ.	O
note	O
that	O
if	O
y	O
=	O
{	O
±1	O
}	O
and	O
we	O
set	B
ψ	O
(	O
x	O
,	O
y	O
)	O
=	O
yx	O
and	O
w	O
=	O
rd	O
,	O
then	O
hψ	O
,	O
w	O
becomes	O
the	O
hypothesis	B
class	I
of	O
homogeneous	O
halfspace	B
predictors	O
for	O
binary	O
classiﬁcation	O
.	O
17.2.1	O
how	O
to	O
construct	O
ψ	O
as	O
mentioned	O
before	O
,	O
we	O
can	O
think	O
of	O
the	O
elements	O
of	O
ψ	O
(	O
x	O
,	O
y	O
)	O
as	O
score	O
functions	O
that	O
assess	O
how	O
well	O
the	O
label	B
y	O
ﬁts	O
the	O
instance	B
x.	O
naturally	O
,	O
designing	O
a	O
good	O
ψ	O
is	O
similar	O
to	O
the	O
problem	O
of	O
designing	O
a	O
good	O
feature	B
mapping	O
(	O
as	O
we	O
discussed	O
in	O
17.2	O
linear	O
multiclass	O
predictors	O
231	O
chapter	O
16	O
and	O
as	O
we	O
will	O
discuss	O
in	O
more	O
detail	O
in	O
chapter	O
25	O
)	O
.	O
two	O
examples	O
of	O
useful	O
constructions	O
are	O
given	O
in	O
the	O
following	O
.	O
the	O
multivector	O
construction	O
:	O
let	O
y	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
and	O
let	O
x	O
=	O
rn	O
.	O
we	O
deﬁne	O
ψ	O
:	O
x	O
×	O
y	O
→	O
rd	O
,	O
where	O
d	O
=	O
nk	O
,	O
as	O
follows	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
(	O
cid:124	O
)	O
ψ	O
(	O
x	O
,	O
y	O
)	O
=	O
[	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
∈r	O
(	O
y−1	O
)	O
n	O
,	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
∈rn	O
(	O
cid:125	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
∈r	O
(	O
k−y	O
)	O
n	O
]	O
.	O
(	O
17.2	O
)	O
that	O
is	O
,	O
ψ	O
(	O
x	O
,	O
y	O
)	O
is	O
composed	O
of	O
k	O
vectors	O
,	O
each	O
of	O
which	O
is	O
of	O
dimension	B
n	O
,	O
where	O
we	O
set	B
all	O
the	O
vectors	O
to	O
be	O
the	O
all	O
zeros	O
vector	O
except	O
the	O
y	O
’	O
th	O
vector	O
,	O
which	O
is	O
set	B
to	O
be	O
x.	O
it	O
follows	O
that	O
we	O
can	O
think	O
of	O
w	O
∈	O
rnk	O
as	O
being	O
composed	O
of	O
k	O
weight	O
vectors	O
in	O
rn	O
,	O
that	O
is	O
,	O
w	O
=	O
[	O
w1	O
;	O
;	O
wk	O
]	O
,	O
hence	O
the	O
name	O
multivec-	O
tor	O
construction	O
.	O
by	O
the	O
construction	O
we	O
have	O
that	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
=	O
(	O
cid:104	O
)	O
wy	O
,	O
x	O
(	O
cid:105	O
)	O
,	O
and	O
therefore	O
the	O
multiclass	B
prediction	O
becomes	O
.	O
.	O
.	O
h	O
(	O
x	O
)	O
=	O
argmax	O
y∈y	O
(	O
cid:104	O
)	O
wy	O
,	O
x	O
(	O
cid:105	O
)	O
.	O
a	O
geometric	O
illustration	O
of	O
the	O
multiclass	B
prediction	O
over	O
x	O
=	O
r2	O
is	O
given	O
in	O
the	O
following	O
.	O
w2	O
w1	O
w3	O
w4	O
tf-idf	O
:	O
the	O
previous	O
deﬁnition	O
of	O
ψ	O
(	O
x	O
,	O
y	O
)	O
does	O
not	O
incorporate	O
any	O
prior	B
knowledge	I
about	O
the	O
problem	O
.	O
we	O
next	O
describe	O
an	O
example	O
of	O
a	O
feature	B
function	O
ψ	O
that	O
does	O
incorporate	O
prior	B
knowledge	I
.	O
let	O
x	O
be	O
a	O
set	B
of	O
text	O
documents	O
and	O
y	O
be	O
a	O
set	B
of	O
possible	O
topics	O
.	O
let	O
d	O
be	O
a	O
size	O
of	O
a	O
dictionary	O
of	O
words	O
.	O
for	O
each	O
word	O
in	O
the	O
dictionary	O
,	O
whose	O
corresponding	O
index	O
is	O
j	O
,	O
let	O
t	O
f	O
(	O
j	O
,	O
x	O
)	O
be	O
the	O
number	O
of	O
times	O
the	O
word	O
corresponding	O
to	O
j	O
appears	O
in	O
the	O
document	O
x.	O
this	O
quantity	O
is	O
called	O
term-frequency	B
.	O
additionally	O
,	O
let	O
df	O
(	O
j	O
,	O
y	O
)	O
be	O
the	O
number	O
of	O
times	O
the	O
word	O
corresponding	O
to	O
j	O
appears	O
in	O
documents	O
in	O
our	O
training	B
set	I
that	O
are	O
not	O
about	O
topic	O
y.	O
this	O
quantity	O
is	O
called	O
document-frequency	O
and	O
measures	O
whether	O
word	O
j	O
is	O
frequent	O
in	O
other	O
topics	O
.	O
now	O
,	O
deﬁne	O
ψ	O
:	O
x	O
×	O
y	O
→	O
rd	O
to	O
be	O
such	O
that	O
(	O
cid:16	O
)	O
m	O
(	O
cid:17	O
)	O
,	O
ψj	O
(	O
x	O
,	O
y	O
)	O
=	O
t	O
f	O
(	O
j	O
,	O
x	O
)	O
log	O
df	O
(	O
j	O
,	O
y	O
)	O
where	O
m	O
is	O
the	O
total	O
number	O
of	O
documents	O
in	O
our	O
training	B
set	I
.	O
the	O
preced-	O
ing	O
quantity	O
is	O
called	O
term-frequency-inverse-document-frequency	O
or	O
tf-idf	O
for	O
232	O
multiclass	B
,	O
ranking	B
,	O
and	O
complex	O
prediction	O
problems	O
short	O
.	O
intuitively	O
,	O
ψj	O
(	O
x	O
,	O
y	O
)	O
should	O
be	O
large	O
if	O
the	O
word	O
corresponding	O
to	O
j	O
ap-	O
pears	O
a	O
lot	O
in	O
the	O
document	O
x	O
but	O
does	O
not	O
appear	O
at	O
all	O
in	O
documents	O
that	O
are	O
not	O
on	O
topic	O
y.	O
if	O
this	O
is	O
the	O
case	O
,	O
we	O
tend	O
to	O
believe	O
that	O
the	O
document	O
x	O
is	O
on	O
topic	O
y.	O
note	O
that	O
unlike	O
the	O
multivector	O
construction	O
described	O
previously	O
,	O
in	O
the	O
current	O
construction	O
the	O
dimension	B
of	O
ψ	O
does	O
not	O
depend	O
on	O
the	O
number	O
of	O
topics	O
(	O
i.e.	O
,	O
the	O
size	O
of	O
y	O
)	O
.	O
17.2.2	O
cost-sensitive	B
classiﬁcation	O
so	O
far	O
we	O
used	O
the	O
zero-one	O
loss	B
as	O
our	O
performance	O
measure	O
of	O
the	O
quality	O
of	O
h	O
(	O
x	O
)	O
.	O
that	O
is	O
,	O
the	O
loss	B
of	O
a	O
hypothesis	B
h	O
on	O
an	O
example	O
(	O
x	O
,	O
y	O
)	O
is	O
1	O
if	O
h	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
y	O
and	O
0	O
otherwise	O
.	O
in	O
some	O
situations	O
it	O
makes	O
more	O
sense	O
to	O
penalize	O
diﬀerent	O
levels	O
of	O
loss	B
for	O
diﬀerent	O
mistakes	O
.	O
for	O
example	O
,	O
in	O
object	O
recognition	O
tasks	O
,	O
it	O
is	O
less	O
severe	O
to	O
predict	O
that	O
an	O
image	O
of	O
a	O
tiger	O
contains	O
a	O
cat	O
than	O
predicting	O
that	O
the	O
image	O
contains	O
a	O
whale	O
.	O
this	O
can	O
be	O
modeled	O
by	O
specifying	O
a	O
loss	B
function	I
,	O
∆	O
:	O
y	O
×	O
y	O
→	O
r+	O
,	O
where	O
for	O
every	O
pair	O
of	O
labels	O
,	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
,	O
the	O
loss	B
of	O
predicting	O
the	O
label	B
y	O
(	O
cid:48	O
)	O
when	O
the	O
correct	O
label	B
is	O
y	O
is	O
deﬁned	O
to	O
be	O
∆	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
.	O
we	O
assume	O
that	O
∆	O
(	O
y	O
,	O
y	O
)	O
=	O
0.	O
note	O
that	O
the	O
zero-one	O
loss	B
can	O
be	O
easily	O
modeled	O
by	O
setting	O
∆	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
=	O
1	O
[	O
y	O
(	O
cid:48	O
)	O
(	O
cid:54	O
)	O
=y	O
]	O
.	O
17.2.3	O
erm	O
we	O
have	O
deﬁned	O
the	O
hypothesis	B
class	I
hψ	O
,	O
w	O
and	O
speciﬁed	O
a	O
loss	B
function	I
∆	O
.	O
to	O
learn	O
the	O
class	O
with	O
respect	O
to	O
the	O
loss	B
function	I
,	O
we	O
can	O
apply	O
the	O
erm	O
rule	O
with	O
respect	O
to	O
this	O
class	O
.	O
that	O
is	O
,	O
we	O
search	O
for	O
a	O
multiclass	B
hypothesis	O
h	O
∈	O
hψ	O
,	O
w	O
,	O
parameterized	O
by	O
a	O
vector	O
w	O
,	O
that	O
minimizes	O
the	O
empirical	B
risk	I
with	O
respect	O
to	O
∆	O
,	O
m	O
(	O
cid:88	O
)	O
i=1	O
ls	O
(	O
h	O
)	O
=	O
1	O
m	O
∆	O
(	O
h	O
(	O
xi	O
)	O
,	O
yi	O
)	O
.	O
we	O
now	O
show	O
that	O
when	O
w	O
=	O
rd	O
and	O
we	O
are	O
in	O
the	O
realizable	O
case	O
,	O
then	O
it	O
is	O
possible	O
to	O
solve	O
the	O
erm	O
problem	O
eﬃciently	O
using	O
linear	B
programming	I
.	O
indeed	O
,	O
in	O
the	O
realizable	O
case	O
,	O
we	O
need	O
to	O
ﬁnd	O
a	O
vector	O
w	O
∈	O
rd	O
that	O
satisﬁes	O
∀i	O
∈	O
[	O
m	O
]	O
,	O
yi	O
=	O
argmax	O
y∈y	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
xi	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
.	O
equivalently	O
,	O
we	O
need	O
that	O
w	O
will	O
satisfy	O
the	O
following	O
set	B
of	O
linear	O
inequalities	O
∀i	O
∈	O
[	O
m	O
]	O
,	O
∀y	O
∈	O
y	O
\	O
{	O
yi	O
}	O
,	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
xi	O
,	O
yi	O
)	O
(	O
cid:105	O
)	O
>	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
xi	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
.	O
finding	O
w	O
that	O
satisﬁes	O
the	O
preceding	O
set	B
of	O
linear	O
equations	O
amounts	O
to	O
solving	O
a	O
linear	O
program	O
.	O
as	O
in	O
the	O
case	O
of	O
binary	O
classiﬁcation	O
,	O
it	O
is	O
also	O
possible	O
to	O
use	O
a	O
generalization	O
of	O
the	O
perceptron	O
algorithm	O
for	O
solving	O
the	O
erm	O
problem	O
.	O
see	O
exercise	O
2.	O
in	O
the	O
nonrealizable	O
case	O
,	O
solving	O
the	O
erm	O
problem	O
is	O
in	O
general	O
computa-	O
tionally	O
hard	O
.	O
we	O
tackle	O
this	O
diﬃculty	O
using	O
the	O
method	O
of	O
convex	B
surrogate	O
17.2	O
linear	O
multiclass	O
predictors	O
233	O
loss	B
functions	O
(	O
see	O
section	O
12.3	O
)	O
.	O
in	O
particular	O
,	O
we	O
generalize	O
the	O
hinge	B
loss	I
to	O
multiclass	B
problems	O
.	O
17.2.4	O
generalized	O
hinge	O
loss	B
recall	O
that	O
in	O
binary	O
classiﬁcation	O
,	O
the	O
hinge	B
loss	I
is	O
deﬁned	O
to	O
be	O
max	O
{	O
0	O
,	O
1	O
−	O
y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
}	O
.	O
we	O
now	O
generalize	O
the	O
hinge	B
loss	I
to	O
multiclass	B
predictors	O
of	O
the	O
form	O
hw	O
(	O
x	O
)	O
=	O
argmax	O
y	O
(	O
cid:48	O
)	O
∈y	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
.	O
recall	B
that	O
a	O
surrogate	O
convex	O
loss	B
should	O
upper	O
bound	O
the	O
original	O
nonconvex	O
loss	B
,	O
which	O
in	O
our	O
case	O
is	O
∆	O
(	O
hw	O
(	O
x	O
)	O
,	O
y	O
)	O
.	O
to	O
derive	O
an	O
upper	O
bound	O
on	O
∆	O
(	O
hw	O
(	O
x	O
)	O
,	O
y	O
)	O
we	O
ﬁrst	O
note	O
that	O
the	O
deﬁnition	O
of	O
hw	O
(	O
x	O
)	O
implies	O
that	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
≤	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
hw	O
(	O
x	O
)	O
)	O
(	O
cid:105	O
)	O
.	O
therefore	O
,	O
∆	O
(	O
hw	O
(	O
x	O
)	O
,	O
y	O
)	O
≤	O
∆	O
(	O
hw	O
(	O
x	O
)	O
,	O
y	O
)	O
+	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
hw	O
(	O
x	O
)	O
)	O
−	O
ψ	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
.	O
since	O
hw	O
(	O
x	O
)	O
∈	O
y	O
we	O
can	O
upper	O
bound	O
the	O
right-hand	O
side	O
of	O
the	O
preceding	O
by	O
y	O
(	O
cid:48	O
)	O
∈y	O
(	O
∆	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
+	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
−	O
ψ	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
)	O
max	O
def=	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
.	O
(	O
17.3	O
)	O
we	O
use	O
the	O
term	O
“	O
generalized	O
hinge	O
loss	B
”	O
to	O
denote	O
the	O
preceding	O
expression	O
.	O
as	O
we	O
have	O
shown	O
,	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
≥	O
∆	O
(	O
hw	O
(	O
x	O
)	O
,	O
y	O
)	O
.	O
furthermore	O
,	O
equality	O
holds	O
when-	O
ever	O
the	O
score	O
of	O
the	O
correct	O
label	B
is	O
larger	O
than	O
the	O
score	O
of	O
any	O
other	O
label	B
,	O
y	O
(	O
cid:48	O
)	O
,	O
by	O
at	O
least	O
∆	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
,	O
namely	O
,	O
∀y	O
(	O
cid:48	O
)	O
∈	O
y	O
\	O
{	O
y	O
}	O
,	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
≥	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
+	O
∆	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
.	O
it	O
is	O
also	O
immediate	O
to	O
see	O
that	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
is	O
a	O
convex	B
function	O
with	O
respect	O
to	O
w	O
since	O
it	O
is	O
a	O
maximum	O
over	O
linear	O
functions	O
of	O
w	O
(	O
see	O
claim	O
12.5	O
in	O
chapter	O
12	O
)	O
,	O
and	O
that	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
is	O
ρ-lipschitz	O
with	O
ρ	O
=	O
maxy	O
(	O
cid:48	O
)	O
∈y	O
(	O
cid:107	O
)	O
ψ	O
(	O
x	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
−	O
ψ	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:107	O
)	O
.	O
remark	O
17.2	O
we	O
use	O
the	O
name	O
“	O
generalized	O
hinge	O
loss	B
”	O
since	O
in	O
the	O
binary	O
case	O
,	O
when	O
y	O
=	O
{	O
±1	O
}	O
,	O
if	O
we	O
set	B
ψ	O
(	O
x	O
,	O
y	O
)	O
=	O
yx	O
2	O
,	O
then	O
the	O
generalized	O
hinge	O
loss	B
becomes	O
the	O
vanilla	O
hinge	B
loss	I
for	O
binary	O
classiﬁcation	O
,	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
max	O
{	O
0	O
,	O
1	O
−	O
y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
}	O
.	O
geometric	O
intuition	O
:	O
the	O
feature	B
function	O
ψ	O
:	O
x	O
×	O
y	O
→	O
rd	O
maps	O
each	O
x	O
into	O
|y|	O
vectors	O
in	O
rd	O
.	O
the	O
value	O
of	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
will	O
be	O
zero	O
if	O
there	O
exists	O
a	O
direction	O
w	O
such	O
that	O
when	O
projecting	O
the	O
|y|	O
vectors	O
onto	O
this	O
direction	O
we	O
obtain	O
that	O
each	O
vector	O
is	O
represented	O
by	O
the	O
scalar	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
,	O
and	O
we	O
can	O
rank	O
the	O
diﬀerent	O
points	O
on	O
the	O
basis	O
of	O
these	O
scalars	O
so	O
that	O
•	O
the	O
point	O
corresponding	O
to	O
the	O
correct	O
y	O
is	O
top-ranked	O
234	O
multiclass	B
,	O
ranking	B
,	O
and	O
complex	O
prediction	O
problems	O
•	O
for	O
each	O
y	O
(	O
cid:48	O
)	O
(	O
cid:54	O
)	O
=	O
y	O
,	O
the	O
diﬀerence	O
between	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
and	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
is	O
larger	O
than	O
the	O
loss	B
of	O
predicting	O
y	O
(	O
cid:48	O
)	O
instead	O
of	O
y.	O
the	O
diﬀerence	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
−	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
is	O
also	O
referred	O
to	O
as	O
the	O
“	O
margin	B
”	O
(	O
see	O
section	O
15.1	O
)	O
.	O
this	O
is	O
illustrated	O
in	O
the	O
following	O
ﬁgure	O
:	O
w	O
ψ	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
)	O
≥∆	O
(	O
y	O
,	O
y	O
≥	O
ψ	O
(	O
x	O
,	O
y	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
)	O
∆	O
(	O
y	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
ψ	O
(	O
x	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
17.2.5	O
multiclass	B
svm	O
and	O
sgd	O
once	O
we	O
have	O
deﬁned	O
the	O
generalized	O
hinge	O
loss	B
,	O
we	O
obtain	O
a	O
convex-lipschitz	O
learning	O
problem	O
and	O
we	O
can	O
apply	O
our	O
general	O
techniques	O
for	O
solving	O
such	O
prob-	O
lems	O
.	O
in	O
particular	O
,	O
the	O
rlm	O
technique	O
we	O
have	O
studied	O
in	O
chapter	O
13	O
yields	O
the	O
multiclass	B
svm	O
rule	O
:	O
multiclass	B
svm	O
input	O
:	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
parameters	O
:	O
regularization	B
parameter	O
λ	O
>	O
0	O
loss	B
function	I
∆	O
:	O
y	O
×	O
y	O
→	O
r+	O
class-sensitive	B
feature	I
mapping	I
ψ	O
:	O
x	O
×	O
y	O
→	O
rd	O
solve	O
:	O
(	O
cid:33	O
)	O
y	O
(	O
cid:48	O
)	O
∈y	O
(	O
∆	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
yi	O
)	O
+	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
xi	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
−	O
ψ	O
(	O
xi	O
,	O
yi	O
)	O
(	O
cid:105	O
)	O
)	O
max	O
m	O
(	O
cid:88	O
)	O
i=1	O
(	O
cid:32	O
)	O
min	O
w∈rd	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
1	O
m	O
output	O
the	O
predictor	B
hw	O
(	O
x	O
)	O
=	O
argmaxy∈y	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
we	O
can	O
solve	O
the	O
optimization	O
problem	O
associated	O
with	O
multiclass	B
svm	O
us-	O
ing	O
generic	O
convex	B
optimization	O
algorithms	O
(	O
or	O
using	O
the	O
method	O
described	O
in	O
section	O
15.5	O
)	O
.	O
let	O
us	O
analyze	O
the	O
risk	B
of	O
the	O
resulting	O
hypothesis	B
.	O
the	O
analysis	O
seamlessly	O
follows	O
from	O
our	O
general	O
analysis	O
for	O
convex-lipschitz	O
problems	O
given	O
in	O
chapter	O
13.	O
in	O
particular	O
,	O
applying	O
corollary	O
13.8	O
and	O
using	O
the	O
fact	O
that	O
the	O
generalized	O
hinge	O
loss	B
upper	O
bounds	O
the	O
∆	O
loss	B
,	O
we	O
immediately	O
obtain	O
an	O
analog	O
of	O
corollary	O
15.7	O
:	O
corollary	O
17.1	O
let	O
d	O
be	O
a	O
distribution	O
over	O
x	O
×	O
y	O
,	O
let	O
ψ	O
:	O
x	O
×	O
y	O
→	O
rd	O
,	O
and	O
assume	O
that	O
for	O
all	O
x	O
∈	O
x	O
and	O
y	O
∈	O
y	O
we	O
have	O
(	O
cid:107	O
)	O
ψ	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:107	O
)	O
≤	O
ρ/2	O
.	O
let	O
b	O
>	O
0	O
.	O
17.2	O
linear	O
multiclass	O
predictors	O
235	O
(	O
cid:113	O
)	O
2ρ2	O
b2m	O
on	O
a	O
training	B
set	I
s	O
∼	O
dm	O
e	O
consider	O
running	O
multiclass	B
svm	O
with	O
λ	O
=	O
and	O
let	O
hw	O
be	O
the	O
output	O
of	O
multiclass	B
svm	O
.	O
then	O
,	O
(	O
w	O
)	O
]	O
≤	O
min	O
s∼dm	O
u	O
:	O
(	O
cid:107	O
)	O
u	O
(	O
cid:107	O
)	O
≤b	O
where	O
l∆d	O
(	O
h	O
)	O
=	O
e	O
(	O
x	O
,	O
y	O
)	O
∼d	O
[	O
∆	O
(	O
h	O
(	O
x	O
)	O
,	O
y	O
)	O
]	O
and	O
lg−hinge	O
with	O
(	O
cid:96	O
)	O
being	O
the	O
generalized	B
hinge-loss	I
as	O
deﬁned	O
in	O
equation	O
(	O
17.3	O
)	O
.	O
[	O
l∆d	O
(	O
hw	O
)	O
]	O
≤	O
lg−hinge	O
d	O
[	O
lg−hinge	O
s∼dm	O
(	O
u	O
)	O
+	O
e	O
d	O
d	O
(	O
cid:114	O
)	O
8ρ2b2	O
m	O
,	O
(	O
w	O
)	O
=	O
e	O
(	O
x	O
,	O
y	O
)	O
∼d	O
[	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
]	O
we	O
can	O
also	O
apply	O
the	O
sgd	O
learning	O
framework	O
for	O
minimizing	O
lg−hinge	O
(	O
w	O
)	O
as	O
described	O
in	O
chapter	O
14.	O
recall	B
claim	O
14.6	O
,	O
which	O
dealt	O
with	O
subgradients	O
of	O
max	O
functions	O
.	O
in	O
light	O
of	O
this	O
claim	O
,	O
in	O
order	O
to	O
ﬁnd	O
a	O
subgradient	O
of	O
the	O
generalized	O
hinge	O
loss	B
all	O
we	O
need	O
to	O
do	O
is	O
to	O
ﬁnd	O
y	O
∈	O
y	O
that	O
achieves	O
the	O
maximum	O
in	O
the	O
deﬁnition	O
of	O
the	O
generalized	O
hinge	O
loss	B
.	O
this	O
yields	O
the	O
following	O
algorithm	O
:	O
d	O
sgd	O
for	O
multiclass	B
learning	O
parameters	O
:	O
scalar	O
η	O
>	O
0	O
,	O
integer	O
t	O
>	O
0	O
loss	B
function	I
∆	O
:	O
y	O
×	O
y	O
→	O
r+	O
class-sensitive	B
feature	I
mapping	I
ψ	O
:	O
x	O
×	O
y	O
→	O
rd	O
initialize	O
:	O
w	O
(	O
1	O
)	O
=	O
0	O
∈	O
rd	O
for	O
t	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
t	O
sample	O
(	O
x	O
,	O
y	O
)	O
∼	O
d	O
set	B
vt	O
=	O
ψ	O
(	O
x	O
,	O
ˆy	O
)	O
−	O
ψ	O
(	O
x	O
,	O
y	O
)	O
update	O
w	O
(	O
t+1	O
)	O
=	O
w	O
(	O
t	O
)	O
−	O
ηvt	O
output	O
¯w	O
=	O
1	O
t	O
ﬁnd	O
ˆy	O
∈	O
argmaxy	O
(	O
cid:48	O
)	O
∈y	O
(	O
cid:0	O
)	O
∆	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
+	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
ψ	O
(	O
x	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
−	O
ψ	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
(	O
cid:1	O
)	O
(	O
cid:80	O
)	O
t	O
t=1	O
w	O
(	O
t	O
)	O
our	O
general	O
analysis	O
of	O
sgd	O
given	O
in	O
corollary	O
14.12	O
immediately	O
implies	O
:	O
corollary	O
17.2	O
let	O
d	O
be	O
a	O
distribution	O
over	O
x	O
×	O
y	O
,	O
let	O
ψ	O
:	O
x	O
×	O
y	O
→	O
rd	O
,	O
and	O
assume	O
that	O
for	O
all	O
x	O
∈	O
x	O
and	O
y	O
∈	O
y	O
we	O
have	O
(	O
cid:107	O
)	O
ψ	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:107	O
)	O
≤	O
ρ/2	O
.	O
let	O
b	O
>	O
0.	O
then	O
,	O
for	O
every	O
	O
>	O
0	O
,	O
if	O
we	O
run	O
sgd	O
for	O
multiclass	B
learning	O
with	O
a	O
number	O
of	O
iterations	O
(	O
i.e.	O
,	O
number	O
of	O
examples	O
)	O
(	O
cid:113	O
)	O
b2	O
t	O
≥	O
b2ρ2	O
2	O
and	O
with	O
η	O
=	O
ρ2	O
t	O
,	O
then	O
the	O
output	O
of	O
sgd	O
satisﬁes	O
(	O
¯w	O
)	O
]	O
≤	O
min	O
u	O
:	O
(	O
cid:107	O
)	O
u	O
(	O
cid:107	O
)	O
≤b	O
[	O
lg−hinge	O
s∼dm	O
[	O
l∆d	O
(	O
h	O
¯w	O
)	O
]	O
≤	O
e	O
d	O
e	O
s∼dm	O
lg−hinge	O
d	O
(	O
u	O
)	O
+	O
	O
.	O
remark	O
17.3	O
it	O
is	O
interesting	O
to	O
note	O
that	O
the	O
risk	B
bounds	O
given	O
in	O
corol-	O
lary	O
17.1	O
and	O
corollary	O
17.2	O
do	O
not	O
depend	O
explicitly	O
on	O
the	O
size	O
of	O
the	O
label	B
set	O
y	O
,	O
a	O
fact	O
we	O
will	O
rely	O
on	O
in	O
the	O
next	O
section	O
.	O
however	O
,	O
the	O
bounds	O
may	O
de-	O
pend	O
implicitly	O
on	O
the	O
size	O
of	O
y	O
via	O
the	O
norm	O
of	O
ψ	O
(	O
x	O
,	O
y	O
)	O
and	O
the	O
fact	O
that	O
the	O
bounds	O
are	O
meaningful	O
only	O
when	O
there	O
exists	O
some	O
vector	O
u	O
,	O
(	O
cid:107	O
)	O
u	O
(	O
cid:107	O
)	O
≤	O
b	O
,	O
for	O
which	O
lg−hinge	O
d	O
(	O
u	O
)	O
is	O
not	O
excessively	O
large	O
.	O
236	O
multiclass	B
,	O
ranking	B
,	O
and	O
complex	O
prediction	O
problems	O
17.3	O
structured	B
output	I
prediction	I
structured	O
output	O
prediction	O
problems	O
are	O
multiclass	B
problems	O
in	O
which	O
y	O
is	O
very	O
large	O
but	O
is	O
endowed	O
with	O
a	O
predeﬁned	O
structure	O
.	O
the	O
structure	O
plays	O
a	O
key	O
role	O
in	O
constructing	O
eﬃcient	O
algorithms	O
.	O
to	O
motivate	O
structured	O
learning	O
problems	O
,	O
consider	O
the	O
problem	O
of	O
optical	O
character	O
recognition	O
(	O
ocr	O
)	O
.	O
suppose	O
we	O
receive	O
an	O
image	O
of	O
some	O
handwritten	O
word	O
and	O
would	O
like	O
to	O
predict	O
which	O
word	O
is	O
written	O
in	O
the	O
image	O
.	O
to	O
simplify	O
the	O
setting	O
,	O
suppose	O
we	O
know	O
how	O
to	O
segment	O
the	O
image	O
into	O
a	O
sequence	O
of	O
images	O
,	O
each	O
of	O
which	O
contains	O
a	O
patch	O
of	O
the	O
image	O
corresponding	O
to	O
a	O
single	O
letter	O
.	O
therefore	O
,	O
x	O
is	O
the	O
set	B
of	O
sequences	O
of	O
images	O
and	O
y	O
is	O
the	O
set	B
of	O
sequences	O
of	O
letters	O
.	O
note	O
that	O
the	O
size	O
of	O
y	O
grows	O
exponentially	O
with	O
the	O
maximal	O
length	O
of	O
a	O
word	O
.	O
an	O
example	O
of	O
an	O
image	O
x	O
corresponding	O
to	O
the	O
label	B
y	O
=	O
“	O
workable	O
”	O
is	O
given	O
in	O
the	O
following	O
.	O
to	O
tackle	O
structure	O
prediction	O
we	O
can	O
rely	O
on	O
the	O
family	O
of	O
linear	B
predictors	I
described	O
in	O
the	O
previous	O
section	O
.	O
in	O
particular	O
,	O
we	O
need	O
to	O
deﬁne	O
a	O
reasonable	O
loss	B
function	I
for	O
the	O
problem	O
,	O
∆	O
,	O
as	O
well	O
as	O
a	O
good	O
class-sensitive	B
feature	I
mapping	I
,	O
ψ.	O
by	O
“	O
good	O
”	O
we	O
mean	O
a	O
feature	B
mapping	O
that	O
will	O
lead	O
to	O
a	O
low	O
approximation	B
error	I
for	O
the	O
class	O
of	O
linear	B
predictors	I
with	O
respect	O
to	O
ψ	O
and	O
∆	O
.	O
once	O
we	O
do	O
this	O
,	O
we	O
can	O
rely	O
,	O
for	O
example	O
,	O
on	O
the	O
sgd	O
learning	O
algorithm	O
deﬁned	O
in	O
the	O
previous	O
section	O
.	O
however	O
,	O
the	O
huge	O
size	O
of	O
y	O
poses	O
several	O
challenges	O
:	O
1.	O
to	O
apply	O
the	O
multiclass	B
prediction	O
we	O
need	O
to	O
solve	O
a	O
maximization	O
problem	O
over	O
y.	O
how	O
can	O
we	O
predict	O
eﬃciently	O
when	O
y	O
is	O
so	O
large	O
?	O
2.	O
how	O
do	O
we	O
train	O
w	O
eﬃciently	O
?	O
in	O
particular	O
,	O
to	O
apply	O
the	O
sgd	O
rule	O
we	O
again	O
need	O
to	O
solve	O
a	O
maximization	O
problem	O
over	O
y	O
.	O
3.	O
how	O
can	O
we	O
avoid	O
overﬁtting	B
?	O
in	O
the	O
previous	O
section	O
we	O
have	O
already	O
shown	O
that	O
the	O
sample	B
complexity	I
of	O
learning	O
a	O
linear	O
multiclass	O
predictor	B
does	O
not	O
depend	O
explicitly	O
on	O
the	O
number	O
of	O
classes	O
.	O
we	O
just	O
need	O
to	O
make	O
sure	O
that	O
the	O
norm	O
of	O
the	O
range	O
of	O
ψ	O
is	O
not	O
too	O
large	O
.	O
this	O
will	O
take	O
care	O
of	O
the	O
overﬁtting	B
problem	O
.	O
to	O
tackle	O
the	O
computational	O
challenges	O
we	O
rely	O
on	O
the	O
structure	O
of	O
the	O
problem	O
,	O
and	O
deﬁne	O
the	O
functions	O
ψ	O
and	O
∆	O
so	O
that	O
calculating	O
the	O
maximization	O
problems	O
in	O
the	O
deﬁnition	O
of	O
hw	O
and	O
in	O
the	O
sgd	O
algorithm	O
can	O
be	O
performed	O
eﬃciently	O
.	O
in	O
the	O
following	O
we	O
demonstrate	O
one	O
way	O
to	O
achieve	O
these	O
goals	O
for	O
the	O
ocr	O
task	O
mentioned	O
previously	O
.	O
to	O
simplify	O
the	O
presentation	O
,	O
let	O
us	O
assume	O
that	O
all	O
the	O
words	O
in	O
y	O
are	O
of	O
length	O
r	O
and	O
that	O
the	O
number	O
of	O
diﬀerent	O
letters	O
in	O
our	O
alphabet	O
is	O
q.	O
let	O
y	O
and	O
y	O
(	O
cid:48	O
)	O
be	O
two	O
17.3	O
structured	B
output	I
prediction	I
237	O
words	O
(	O
i.e.	O
,	O
sequences	O
of	O
letters	O
)	O
in	O
y.	O
we	O
deﬁne	O
the	O
function	B
∆	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
to	O
be	O
the	O
average	O
number	O
of	O
letters	O
that	O
are	O
diﬀerent	O
in	O
y	O
(	O
cid:48	O
)	O
and	O
y	O
,	O
namely	O
,	O
1	O
1	O
[	O
yi	O
(	O
cid:54	O
)	O
=y	O
(	O
cid:48	O
)	O
i	O
]	O
.	O
next	O
,	O
let	O
us	O
deﬁne	O
a	O
class-sensitive	B
feature	I
mapping	I
ψ	O
(	O
x	O
,	O
y	O
)	O
.	O
it	O
will	O
be	O
conve-	O
nient	O
to	O
think	O
about	O
x	O
as	O
a	O
matrix	O
of	O
size	O
n	O
×	O
r	O
,	O
where	O
n	O
is	O
the	O
number	O
of	O
pixels	O
in	O
each	O
image	O
,	O
and	O
r	O
is	O
the	O
number	O
of	O
images	O
in	O
the	O
sequence	O
.	O
the	O
j	O
’	O
th	O
column	O
of	O
x	O
corresponds	O
to	O
the	O
j	O
’	O
th	O
image	O
in	O
the	O
sequence	O
(	O
encoded	O
as	O
a	O
vector	O
of	O
gray	O
level	O
values	O
of	O
pixels	O
)	O
.	O
the	O
dimension	B
of	O
the	O
range	O
of	O
ψ	O
is	O
set	B
to	O
be	O
d	O
=	O
n	O
q	O
+	O
q2	O
.	O
i=1	O
r	O
the	O
ﬁrst	O
nq	O
feature	B
functions	O
are	O
“	O
type	O
1	O
”	O
features	O
and	O
take	O
the	O
form	O
:	O
(	O
cid:80	O
)	O
r	O
ψi	O
,	O
j,1	O
(	O
x	O
,	O
y	O
)	O
=	O
1	O
r	O
xi	O
,	O
t	O
1	O
[	O
yt=j	O
]	O
.	O
r	O
(	O
cid:88	O
)	O
t=1	O
r	O
(	O
cid:88	O
)	O
t=2	O
that	O
is	O
,	O
we	O
sum	O
the	O
value	O
of	O
the	O
i	O
’	O
th	O
pixel	O
only	O
over	O
the	O
images	O
for	O
which	O
y	O
assigns	O
the	O
letter	O
j.	O
the	O
triple	O
index	O
(	O
i	O
,	O
j	O
,	O
1	O
)	O
indicates	O
that	O
we	O
are	O
dealing	O
with	O
feature	B
(	O
i	O
,	O
j	O
)	O
of	O
type	O
1.	O
intuitively	O
,	O
such	O
features	O
can	O
capture	O
pixels	O
in	O
the	O
image	O
whose	O
gray	O
level	O
values	O
are	O
indicative	O
of	O
a	O
certain	O
letter	O
.	O
the	O
second	O
type	O
of	O
features	O
take	O
the	O
form	O
ψi	O
,	O
j,2	O
(	O
x	O
,	O
y	O
)	O
=	O
1	O
r	O
1	O
[	O
yt=i	O
]	O
1	O
[	O
yt−1=j	O
]	O
.	O
that	O
is	O
,	O
we	O
sum	O
the	O
number	O
of	O
times	O
the	O
letter	O
i	O
follows	O
the	O
letter	O
j.	O
intuitively	O
,	O
these	O
features	O
can	O
capture	O
rules	O
like	O
“	O
it	O
is	O
likely	O
to	O
see	O
the	O
pair	O
‘	O
qu	O
’	O
in	O
a	O
word	O
”	O
or	O
“	O
it	O
is	O
unlikely	O
to	O
see	O
the	O
pair	O
‘	O
rz	O
’	O
in	O
a	O
word.	O
”	O
of	O
course	O
,	O
some	O
of	O
these	O
features	O
will	O
not	O
be	O
very	O
useful	O
,	O
so	O
the	O
goal	O
of	O
the	O
learning	O
process	O
is	O
to	O
assign	O
weights	O
to	O
features	O
by	O
learning	O
the	O
vector	O
w	O
,	O
so	O
that	O
the	O
weighted	O
score	O
will	O
give	O
us	O
a	O
good	O
prediction	O
via	O
hw	O
(	O
x	O
)	O
=	O
argmax	O
y∈y	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
.	O
it	O
is	O
left	O
to	O
show	O
how	O
to	O
solve	O
the	O
optimization	O
problem	O
in	O
the	O
deﬁnition	O
of	O
hw	O
(	O
x	O
)	O
eﬃciently	O
,	O
as	O
well	O
as	O
how	O
to	O
solve	O
the	O
optimization	O
problem	O
in	O
the	O
deﬁnition	O
of	O
ˆy	O
in	O
the	O
sgd	O
algorithm	O
.	O
we	O
can	O
do	O
this	O
by	O
applying	O
a	O
dynamic	O
programming	O
procedure	O
.	O
we	O
describe	O
the	O
procedure	O
for	O
solving	O
the	O
maximization	O
in	O
the	O
deﬁnition	O
of	O
hw	O
and	O
leave	O
as	O
an	O
exercise	O
the	O
maximization	O
problem	O
in	O
the	O
deﬁnition	O
of	O
ˆy	O
in	O
the	O
sgd	O
algorithm	O
.	O
to	O
derive	O
the	O
dynamic	O
programming	O
procedure	O
,	O
let	O
us	O
ﬁrst	O
observe	O
that	O
we	O
can	O
write	O
ψ	O
(	O
x	O
,	O
y	O
)	O
=	O
r	O
(	O
cid:88	O
)	O
φ	O
(	O
x	O
,	O
yt	O
,	O
yt−1	O
)	O
,	O
t=1	O
for	O
an	O
appropriate	O
φ	O
:	O
x	O
×	O
[	O
q	O
]	O
×	O
[	O
q	O
]	O
∪	O
{	O
0	O
}	O
→	O
rd	O
,	O
and	O
for	O
simplicity	O
we	O
assume	O
that	O
y0	O
is	O
always	O
equal	O
to	O
0.	O
indeed	O
,	O
each	O
feature	B
function	O
ψi	O
,	O
j,1	O
can	O
be	O
written	O
in	O
terms	O
of	O
φi	O
,	O
j,1	O
(	O
x	O
,	O
yt	O
,	O
yt−1	O
)	O
=	O
xi	O
,	O
t	O
1	O
[	O
yt=j	O
]	O
,	O
238	O
multiclass	B
,	O
ranking	B
,	O
and	O
complex	O
prediction	O
problems	O
while	O
the	O
feature	B
function	O
ψi	O
,	O
j,2	O
can	O
be	O
written	O
in	O
terms	O
of	O
φi	O
,	O
j,2	O
(	O
x	O
,	O
yt	O
,	O
yt−1	O
)	O
=	O
1	O
[	O
yt=i	O
]	O
1	O
[	O
yt−1=j	O
]	O
.	O
therefore	O
,	O
the	O
prediction	O
can	O
be	O
written	O
as	O
hw	O
(	O
x	O
)	O
=	O
argmax	O
y∈y	O
(	O
cid:104	O
)	O
w	O
,	O
φ	O
(	O
x	O
,	O
yt	O
,	O
yt−1	O
)	O
(	O
cid:105	O
)	O
.	O
(	O
17.4	O
)	O
in	O
the	O
following	O
we	O
derive	O
a	O
dynamic	O
programming	O
procedure	O
that	O
solves	O
every	O
problem	O
of	O
the	O
form	O
given	O
in	O
equation	O
(	O
17.4	O
)	O
.	O
the	O
procedure	O
will	O
maintain	O
a	O
matrix	O
m	O
∈	O
rq	O
,	O
r	O
such	O
that	O
r	O
(	O
cid:88	O
)	O
t=1	O
τ	O
(	O
cid:88	O
)	O
t=1	O
ms	O
,	O
τ	O
=	O
max	O
(	O
y1	O
,	O
...	O
,	O
yτ	O
)	O
:	O
yτ	O
=s	O
(	O
cid:104	O
)	O
w	O
,	O
φ	O
(	O
x	O
,	O
yt	O
,	O
yt−1	O
)	O
(	O
cid:105	O
)	O
.	O
clearly	O
,	O
the	O
maximum	O
of	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
equals	O
maxs	O
ms	O
,	O
r	O
.	O
furthermore	O
,	O
we	O
can	O
calculate	O
m	O
in	O
a	O
recursive	O
manner	O
:	O
ms	O
,	O
τ	O
=	O
max	O
s	O
(	O
cid:48	O
)	O
(	O
ms	O
(	O
cid:48	O
)	O
,	O
τ−1	O
+	O
(	O
cid:104	O
)	O
w	O
,	O
φ	O
(	O
x	O
,	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
)	O
.	O
(	O
17.5	O
)	O
this	O
yields	O
the	O
following	O
procedure	O
:	O
dynamic	O
programming	O
for	O
calculating	O
hw	O
(	O
x	O
)	O
as	O
given	O
in	O
equation	O
(	O
17.4	O
)	O
input	O
:	O
a	O
matrix	O
x	O
∈	O
rn	O
,	O
r	O
and	O
a	O
vector	O
w	O
initialize	O
:	O
foreach	O
s	O
∈	O
[	O
q	O
]	O
ms,1	O
=	O
(	O
cid:104	O
)	O
w	O
,	O
φ	O
(	O
x	O
,	O
s	O
,	O
−1	O
)	O
(	O
cid:105	O
)	O
for	O
τ	O
=	O
2	O
,	O
.	O
.	O
.	O
,	O
r	O
foreach	O
s	O
∈	O
[	O
q	O
]	O
set	B
ms	O
,	O
τ	O
as	O
in	O
equation	O
(	O
17.5	O
)	O
set	B
is	O
,	O
τ	O
to	O
be	O
the	O
s	O
(	O
cid:48	O
)	O
that	O
maximizes	O
equation	O
(	O
17.5	O
)	O
set	B
yt	O
=	O
argmaxs	O
ms	O
,	O
r	O
for	O
τ	O
=	O
r	O
,	O
r	O
−	O
1	O
,	O
.	O
.	O
.	O
,	O
2	O
set	B
yτ−1	O
=	O
iyτ	O
,	O
τ	O
output	O
:	O
y	O
=	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yr	O
)	O
17.4	O
ranking	B
ranking	O
is	O
the	O
problem	O
of	O
ordering	O
a	O
set	B
of	O
instances	O
according	O
to	O
their	O
“	O
rele-	O
vance.	O
”	O
a	O
typical	O
application	O
is	O
ordering	O
results	O
of	O
a	O
search	O
engine	O
according	O
to	O
their	O
relevance	O
to	O
the	O
query	O
.	O
another	O
example	O
is	O
a	O
system	O
that	O
monitors	O
elec-	O
tronic	O
transactions	O
and	O
should	O
alert	O
for	O
possible	O
fraudulent	O
transactions	O
.	O
such	O
a	O
system	O
should	O
order	O
transactions	O
according	O
to	O
how	O
suspicious	O
they	O
are	O
.	O
n=1	O
x	O
n	O
be	O
the	O
set	B
of	O
all	O
sequences	O
of	O
instances	O
from	O
formally	O
,	O
let	O
x	O
∗	O
=	O
(	O
cid:83	O
)	O
∞	O
17.4	O
ranking	B
239	O
x	O
of	O
arbitrary	O
length	O
.	O
a	O
ranking	B
hypothesis	O
,	O
h	O
,	O
is	O
a	O
function	B
that	O
receives	O
a	O
sequence	O
of	O
instances	O
¯x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xr	O
)	O
∈	O
x	O
∗	O
,	O
and	O
returns	O
a	O
permutation	O
of	O
[	O
r	O
]	O
.	O
it	O
is	O
more	O
convenient	O
to	O
let	O
the	O
output	O
of	O
h	O
be	O
a	O
vector	O
y	O
∈	O
rr	O
,	O
where	O
by	O
sorting	O
the	O
elements	O
of	O
y	O
we	O
obtain	O
the	O
permutation	O
over	O
[	O
r	O
]	O
.	O
we	O
denote	O
by	O
π	O
(	O
y	O
)	O
the	O
permutation	O
over	O
[	O
r	O
]	O
induced	O
by	O
y.	O
for	O
example	O
,	O
for	O
r	O
=	O
5	O
,	O
the	O
vector	O
y	O
=	O
(	O
2	O
,	O
1	O
,	O
6	O
,	O
−1	O
,	O
0.5	O
)	O
induces	O
the	O
permutation	O
π	O
(	O
y	O
)	O
=	O
(	O
4	O
,	O
3	O
,	O
5	O
,	O
1	O
,	O
2	O
)	O
.	O
that	O
is	O
,	O
if	O
we	O
sort	O
y	O
in	O
an	O
ascending	O
order	O
,	O
then	O
we	O
obtain	O
the	O
vector	O
(	O
−1	O
,	O
0.5	O
,	O
1	O
,	O
2	O
,	O
6	O
)	O
.	O
now	O
,	O
π	O
(	O
y	O
)	O
i	O
is	O
the	O
position	O
of	O
yi	O
in	O
the	O
sorted	O
vector	O
(	O
−1	O
,	O
0.5	O
,	O
1	O
,	O
2	O
,	O
6	O
)	O
.	O
this	O
notation	O
reﬂects	O
that	O
the	O
top-ranked	O
instances	O
are	O
those	O
that	O
achieve	O
the	O
highest	O
(	O
cid:83	O
)	O
∞	O
values	O
in	O
π	O
(	O
y	O
)	O
.	O
in	O
the	O
notation	O
of	O
our	O
pac	O
learning	O
model	O
,	O
the	O
examples	O
domain	B
is	O
z	O
=	O
r=1	O
(	O
x	O
r	O
×	O
rr	O
)	O
,	O
and	O
the	O
hypothesis	B
class	I
,	O
h	O
,	O
is	O
some	O
set	B
of	O
ranking	B
hypotheses	O
.	O
we	O
deﬁne	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
(	O
¯x	O
,	O
y	O
)	O
)	O
=	O
∆	O
(	O
h	O
(	O
¯x	O
)	O
,	O
y	O
)	O
,	O
for	O
some	O
function	B
∆	O
:	O
(	O
cid:83	O
)	O
∞	O
we	O
next	O
turn	O
to	O
describe	O
loss	B
functions	O
for	O
ranking	B
.	O
there	O
are	O
many	O
possible	O
ways	O
to	O
deﬁne	O
such	O
loss	B
functions	O
,	O
and	O
here	O
we	O
list	O
a	O
few	O
examples	O
.	O
in	O
all	O
the	O
examples	O
r=1	O
(	O
rr	O
×	O
rr	O
)	O
→	O
r+	O
.	O
•	O
0–1	O
ranking	B
loss	O
:	O
∆	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
is	O
zero	O
if	O
y	O
and	O
y	O
(	O
cid:48	O
)	O
induce	O
exactly	O
the	O
same	O
ranking	B
and	O
∆	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
=	O
1	O
otherwise	O
.	O
that	O
is	O
,	O
∆	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
=	O
1	O
[	O
π	O
(	O
y	O
(	O
cid:48	O
)	O
)	O
(	O
cid:54	O
)	O
=π	O
(	O
y	O
)	O
]	O
.	O
such	O
a	O
loss	B
function	I
is	O
almost	O
never	O
used	O
in	O
practice	O
as	O
it	O
does	O
not	O
distinguish	O
between	O
the	O
case	O
in	O
which	O
π	O
(	O
y	O
(	O
cid:48	O
)	O
)	O
is	O
almost	O
equal	O
to	O
π	O
(	O
y	O
)	O
and	O
the	O
case	O
in	O
which	O
π	O
(	O
y	O
(	O
cid:48	O
)	O
)	O
is	O
completely	O
diﬀerent	O
from	O
π	O
(	O
y	O
)	O
.	O
•	O
kendall-tau	O
loss	B
:	O
we	O
count	O
the	O
number	O
of	O
pairs	O
(	O
i	O
,	O
j	O
)	O
that	O
are	O
in	O
diﬀerent	O
order	O
in	O
the	O
two	O
permutations	O
.	O
this	O
can	O
be	O
written	O
as	O
∆	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
=	O
2	O
r	O
(	O
r	O
−	O
1	O
)	O
1	O
[	O
sign	O
(	O
y	O
(	O
cid:48	O
)	O
i−y	O
(	O
cid:48	O
)	O
j	O
)	O
(	O
cid:54	O
)	O
=sign	O
(	O
yi−yj	O
)	O
]	O
.	O
r−1	O
(	O
cid:88	O
)	O
r	O
(	O
cid:88	O
)	O
i=1	O
j=i+1	O
this	O
loss	B
function	I
is	O
more	O
useful	O
than	O
the	O
0–1	O
loss	B
as	O
it	O
reﬂects	O
the	O
level	O
of	O
similarity	O
between	O
the	O
two	O
rankings	O
.	O
•	O
normalized	O
discounted	O
cumulative	O
gain	B
(	O
ndcg	O
)	O
:	O
this	O
measure	O
em-	O
phasizes	O
the	O
correctness	O
at	O
the	O
top	O
of	O
the	O
list	O
by	O
using	O
a	O
monotonically	O
nondecreasing	O
discount	O
function	B
d	O
:	O
n	O
→	O
r+	O
.	O
we	O
ﬁrst	O
deﬁne	O
a	O
discounted	O
cumulative	O
gain	B
measure	O
:	O
g	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
=	O
d	O
(	O
π	O
(	O
y	O
(	O
cid:48	O
)	O
)	O
i	O
)	O
yi	O
.	O
r	O
(	O
cid:88	O
)	O
i=1	O
in	O
words	O
,	O
if	O
we	O
interpret	O
yi	O
as	O
a	O
score	O
of	O
the	O
“	O
true	O
relevance	O
”	O
of	O
item	O
i	O
,	O
then	O
we	O
take	O
a	O
weighted	O
sum	O
of	O
the	O
relevance	O
of	O
the	O
elements	O
,	O
while	O
the	O
weight	O
of	O
yi	O
is	O
determined	O
on	O
the	O
basis	O
of	O
the	O
position	O
of	O
i	O
in	O
π	O
(	O
y	O
(	O
cid:48	O
)	O
)	O
.	O
assuming	O
that	O
all	O
elements	O
of	O
y	O
are	O
nonnegative	O
,	O
it	O
is	O
easy	O
to	O
verify	O
that	O
0	O
≤	O
g	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
≤	O
g	O
(	O
y	O
,	O
y	O
)	O
.	O
we	O
can	O
therefore	O
deﬁne	O
a	O
normalized	O
discounted	O
cumulative	O
gain	B
by	O
the	O
ratio	O
g	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
/g	O
(	O
y	O
,	O
y	O
)	O
,	O
and	O
the	O
corresponding	O
loss	B
function	I
would	O
be	O
∆	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
=	O
1	O
−	O
g	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
=	O
1	O
g	O
(	O
y	O
,	O
y	O
)	O
g	O
(	O
y	O
,	O
y	O
)	O
i=1	O
(	O
d	O
(	O
π	O
(	O
y	O
)	O
i	O
)	O
−	O
d	O
(	O
π	O
(	O
y	O
(	O
cid:48	O
)	O
)	O
i	O
)	O
)	O
yi	O
.	O
r	O
(	O
cid:88	O
)	O
240	O
multiclass	B
,	O
ranking	B
,	O
and	O
complex	O
prediction	O
problems	O
we	O
can	O
easily	O
see	O
that	O
∆	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
∈	O
[	O
0	O
,	O
1	O
]	O
and	O
that	O
∆	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
=	O
0	O
whenever	O
π	O
(	O
y	O
(	O
cid:48	O
)	O
)	O
=	O
π	O
(	O
y	O
)	O
.	O
a	O
typical	O
way	O
to	O
deﬁne	O
the	O
discount	O
function	B
is	O
by	O
(	O
cid:40	O
)	O
d	O
(	O
i	O
)	O
=	O
1	O
log2	O
(	O
r−i+2	O
)	O
0	O
if	O
i	O
∈	O
{	O
r	O
−	O
k	O
+	O
1	O
,	O
.	O
.	O
.	O
,	O
r	O
}	O
otherwise	O
where	O
k	O
<	O
r	O
is	O
a	O
parameter	O
.	O
this	O
means	O
that	O
we	O
care	O
more	O
about	O
elements	O
that	O
are	O
ranked	O
higher	O
,	O
and	O
we	O
completely	O
ignore	O
elements	O
that	O
are	O
not	O
at	O
the	O
top-k	O
ranked	O
elements	O
.	O
the	O
ndcg	O
measure	O
is	O
often	O
used	O
to	O
evaluate	O
the	O
performance	O
of	O
search	O
engines	O
since	O
in	O
such	O
applications	O
it	O
makes	O
sense	O
completely	O
to	O
ignore	O
elements	O
that	O
are	O
not	O
at	O
the	O
top	O
of	O
the	O
ranking	B
.	O
once	O
we	O
have	O
a	O
hypothesis	B
class	I
and	O
a	O
ranking	B
loss	O
function	B
,	O
we	O
can	O
learn	O
a	O
ranking	B
function	O
using	O
the	O
erm	O
rule	O
.	O
however	O
,	O
from	O
the	O
computational	O
point	O
of	O
view	O
,	O
the	O
resulting	O
optimization	O
problem	O
might	O
be	O
hard	O
to	O
solve	O
.	O
we	O
next	O
discuss	O
how	O
to	O
learn	O
linear	B
predictors	I
for	O
ranking	B
.	O
17.4.1	O
linear	B
predictors	I
for	O
ranking	B
a	O
natural	O
way	O
to	O
deﬁne	O
a	O
ranking	B
function	O
is	O
by	O
projecting	O
the	O
instances	O
onto	O
some	O
vector	O
w	O
and	O
then	O
outputting	O
the	O
resulting	O
scalars	O
as	O
our	O
representation	O
of	O
the	O
ranking	B
function	O
.	O
that	O
is	O
,	O
assuming	O
that	O
x	O
⊂	O
rd	O
,	O
for	O
every	O
w	O
∈	O
rd	O
we	O
deﬁne	O
a	O
ranking	B
function	O
hw	O
(	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xr	O
)	O
)	O
=	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x1	O
(	O
cid:105	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
cid:104	O
)	O
w	O
,	O
xr	O
(	O
cid:105	O
)	O
)	O
.	O
(	O
17.6	O
)	O
as	O
we	O
discussed	O
in	O
chapter	O
16	O
,	O
we	O
can	O
also	O
apply	O
a	O
feature	B
mapping	O
that	O
maps	O
instances	O
into	O
some	O
feature	B
space	I
and	O
then	O
takes	O
the	O
inner	O
products	O
with	O
w	O
in	O
the	O
feature	B
space	I
.	O
for	O
simplicity	O
,	O
we	O
focus	O
on	O
the	O
simpler	O
form	O
as	O
in	O
equation	O
(	O
17.6	O
)	O
.	O
given	O
some	O
w	O
⊂	O
rd	O
,	O
we	O
can	O
now	O
deﬁne	O
the	O
hypothesis	B
class	I
hw	O
=	O
{	O
hw	O
:	O
w	O
∈	O
w	O
}	O
.	O
once	O
we	O
have	O
deﬁned	O
this	O
hypothesis	B
class	I
,	O
and	O
have	O
chosen	O
a	O
ranking	B
loss	O
function	B
,	O
we	O
can	O
apply	O
the	O
erm	O
rule	O
as	O
follows	O
:	O
given	O
a	O
training	B
set	I
,	O
s	O
=	O
(	O
¯x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
¯xm	O
,	O
ym	O
)	O
,	O
where	O
each	O
(	O
¯xi	O
,	O
yi	O
)	O
is	O
in	O
(	O
x	O
×	O
r	O
)	O
ri	O
,	O
for	O
some	O
ri	O
∈	O
n	O
,	O
we	O
i=1	O
∆	O
(	O
hw	O
(	O
¯xi	O
)	O
,	O
yi	O
)	O
.	O
as	O
in	O
the	O
case	O
of	O
binary	O
classiﬁcation	O
,	O
for	O
many	O
loss	B
functions	O
this	O
problem	O
is	O
computationally	O
hard	O
,	O
and	O
we	O
therefore	O
turn	O
to	O
describe	O
convex	B
surrogate	O
loss	B
functions	O
.	O
we	O
describe	O
the	O
surrogates	O
for	O
the	O
kendall	O
tau	O
loss	B
and	O
for	O
the	O
ndcg	O
loss	B
.	O
should	O
search	O
w	O
∈	O
w	O
that	O
minimizes	O
the	O
empirical	O
loss	O
,	O
(	O
cid:80	O
)	O
m	O
a	O
hinge	B
loss	I
for	O
the	O
kendall	O
tau	O
loss	B
function	I
:	O
we	O
can	O
think	O
of	O
the	O
kendall	O
tau	O
loss	B
as	O
an	O
average	O
of	O
0−1	O
losses	O
for	O
each	O
pair	O
.	O
in	O
particular	O
,	O
for	O
every	O
(	O
i	O
,	O
j	O
)	O
we	O
can	O
rewrite	O
1	O
[	O
sign	O
(	O
y	O
(	O
cid:48	O
)	O
i−y	O
(	O
cid:48	O
)	O
j	O
)	O
(	O
cid:54	O
)	O
=sign	O
(	O
yi−yj	O
)	O
]	O
=	O
1	O
[	O
sign	O
(	O
yi−yj	O
)	O
(	O
y	O
(	O
cid:48	O
)	O
i−y	O
(	O
cid:48	O
)	O
j	O
)	O
≤0	O
]	O
.	O
17.4	O
ranking	B
241	O
i−	O
y	O
(	O
cid:48	O
)	O
in	O
our	O
case	O
,	O
y	O
(	O
cid:48	O
)	O
bound	O
as	O
follows	O
:	O
j	O
=	O
(	O
cid:104	O
)	O
w	O
,	O
xi−	O
xj	O
(	O
cid:105	O
)	O
.	O
it	O
follows	O
that	O
we	O
can	O
use	O
the	O
hinge	B
loss	I
upper	O
1	O
[	O
sign	O
(	O
yi−yj	O
)	O
(	O
y	O
(	O
cid:48	O
)	O
i−y	O
(	O
cid:48	O
)	O
j	O
)	O
≤0	O
]	O
≤	O
max	O
{	O
0	O
,	O
1	O
−	O
sign	O
(	O
yi	O
−	O
yj	O
)	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
−	O
xj	O
(	O
cid:105	O
)	O
}	O
.	O
taking	O
the	O
average	O
over	O
the	O
pairs	O
we	O
obtain	O
the	O
following	O
surrogate	O
convex	O
loss	B
for	O
the	O
kendall	O
tau	O
loss	B
function	I
:	O
∆	O
(	O
hw	O
(	O
¯x	O
)	O
,	O
y	O
)	O
≤	O
2	O
r	O
(	O
r	O
−	O
1	O
)	O
max	O
{	O
0	O
,	O
1	O
−	O
sign	O
(	O
yi	O
−	O
yj	O
)	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
−	O
xj	O
(	O
cid:105	O
)	O
}	O
.	O
r−1	O
(	O
cid:88	O
)	O
r	O
(	O
cid:88	O
)	O
i=1	O
j=i+1	O
the	O
right-hand	O
side	O
is	O
convex	B
with	O
respect	O
to	O
w	O
and	O
upper	O
bounds	O
the	O
kendall	O
tau	O
loss	B
.	O
it	O
is	O
also	O
a	O
ρ-lipschitz	O
function	B
with	O
parameter	O
ρ	O
≤	O
maxi	O
,	O
j	O
(	O
cid:107	O
)	O
xi	O
−	O
xj	O
(	O
cid:107	O
)	O
.	O
a	O
hinge	B
loss	I
for	O
the	O
ndcg	O
loss	B
function	I
:	O
the	O
ndcg	O
loss	B
function	I
depends	O
on	O
the	O
predicted	O
ranking	B
vector	O
y	O
(	O
cid:48	O
)	O
∈	O
rr	O
via	O
the	O
permutation	O
it	O
induces	O
.	O
to	O
derive	O
a	O
surrogate	B
loss	I
function	O
we	O
ﬁrst	O
make	O
the	O
following	O
observation	O
.	O
let	O
v	O
be	O
the	O
set	B
of	O
all	O
permutations	O
of	O
[	O
r	O
]	O
encoded	O
as	O
vectors	O
;	O
namely	O
,	O
each	O
v	O
∈	O
v	O
is	O
a	O
vector	O
in	O
[	O
r	O
]	O
r	O
such	O
that	O
for	O
all	O
i	O
(	O
cid:54	O
)	O
=	O
j	O
we	O
have	O
vi	O
(	O
cid:54	O
)	O
=	O
vj	O
.	O
then	O
(	O
see	O
exercise	O
4	O
)	O
,	O
let	O
us	O
denote	O
ψ	O
(	O
¯x	O
,	O
v	O
)	O
=	O
(	O
cid:80	O
)	O
r	O
π	O
(	O
y	O
(	O
cid:48	O
)	O
)	O
=	O
argmax	O
v∈v	O
vi	O
y	O
(	O
cid:48	O
)	O
i	O
.	O
(	O
17.7	O
)	O
i=1	O
r	O
(	O
cid:88	O
)	O
r	O
(	O
cid:88	O
)	O
(	O
cid:42	O
)	O
i=1	O
i=1	O
vixi	O
;	O
it	O
follows	O
that	O
π	O
(	O
hw	O
(	O
¯x	O
)	O
)	O
=	O
argmax	O
v∈v	O
vi	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
r	O
(	O
cid:88	O
)	O
(	O
cid:43	O
)	O
=	O
argmax	O
v∈v	O
=	O
argmax	O
v∈v	O
w	O
,	O
vixi	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
¯x	O
,	O
v	O
)	O
(	O
cid:105	O
)	O
.	O
i=1	O
on	O
the	O
basis	O
of	O
this	O
observation	O
,	O
we	O
can	O
use	O
the	O
generalized	O
hinge	O
loss	B
for	O
cost-	O
sensitive	O
multiclass	B
classiﬁcation	O
as	O
a	O
surrogate	B
loss	I
function	O
for	O
the	O
ndcg	O
loss	B
as	O
follows	O
:	O
∆	O
(	O
hw	O
(	O
¯x	O
)	O
,	O
y	O
)	O
≤	O
∆	O
(	O
hw	O
(	O
¯x	O
)	O
,	O
y	O
)	O
+	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
¯x	O
,	O
π	O
(	O
hw	O
(	O
¯x	O
)	O
)	O
)	O
(	O
cid:105	O
)	O
−	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
¯x	O
,	O
π	O
(	O
y	O
)	O
)	O
(	O
cid:105	O
)	O
[	O
∆	O
(	O
v	O
,	O
y	O
)	O
+	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
¯x	O
,	O
v	O
)	O
(	O
cid:105	O
)	O
−	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
¯x	O
,	O
π	O
(	O
y	O
)	O
)	O
(	O
cid:105	O
)	O
]	O
≤	O
max	O
v∈v	O
(	O
cid:34	O
)	O
r	O
(	O
cid:88	O
)	O
i=1	O
=	O
max	O
v∈v	O
∆	O
(	O
v	O
,	O
y	O
)	O
+	O
(	O
vi	O
−	O
π	O
(	O
y	O
)	O
i	O
)	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
.	O
(	O
17.8	O
)	O
(	O
cid:35	O
)	O
the	O
right-hand	O
side	O
is	O
a	O
convex	B
function	O
with	O
respect	O
to	O
w.	O
we	O
can	O
now	O
solve	O
the	O
learning	O
problem	O
using	O
sgd	O
as	O
described	O
in	O
section	O
17.2.5.	O
the	O
main	O
computational	O
bottleneck	O
is	O
calculating	O
a	O
subgradient	O
of	O
the	O
loss	B
func-	O
tion	O
,	O
which	O
is	O
equivalent	O
to	O
ﬁnding	O
v	O
that	O
achieves	O
the	O
maximum	O
in	O
equa-	O
tion	O
(	O
17.8	O
)	O
(	O
see	O
claim	O
14.6	O
)	O
.	O
using	O
the	O
deﬁnition	O
of	O
the	O
ndcg	O
loss	B
,	O
this	O
is	O
242	O
multiclass	B
,	O
ranking	B
,	O
and	O
complex	O
prediction	O
problems	O
equivalent	O
to	O
solving	O
the	O
problem	O
argmin	O
v∈v	O
(	O
αivi	O
+	O
βi	O
d	O
(	O
vi	O
)	O
)	O
,	O
where	O
αi	O
=	O
−	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
and	O
βi	O
=	O
yi/g	O
(	O
y	O
,	O
y	O
)	O
.	O
we	O
can	O
think	O
of	O
this	O
problem	O
a	O
little	O
bit	O
diﬀerently	O
by	O
deﬁning	O
a	O
matrix	O
a	O
∈	O
rr	O
,	O
r	O
where	O
ai	O
,	O
j	O
=	O
jαi	O
+	O
d	O
(	O
j	O
)	O
βi	O
.	O
now	O
,	O
let	O
us	O
think	O
about	O
each	O
j	O
as	O
a	O
“	O
worker	O
,	O
”	O
each	O
i	O
as	O
a	O
“	O
task	O
,	O
”	O
and	O
ai	O
,	O
j	O
as	O
the	O
cost	O
of	O
assigning	O
task	O
i	O
to	O
worker	O
j.	O
with	O
this	O
view	O
,	O
the	O
problem	O
of	O
ﬁnding	O
v	O
becomes	O
the	O
problem	O
of	O
ﬁnding	O
an	O
assignment	O
of	O
the	O
tasks	O
to	O
workers	O
of	O
minimal	O
cost	O
.	O
this	O
problem	O
is	O
called	O
“	O
the	O
assignment	O
problem	O
”	O
and	O
can	O
be	O
solved	O
eﬃciently	O
.	O
one	O
particular	O
algorithm	O
is	O
the	O
“	O
hungarian	O
method	O
”	O
(	O
kuhn	O
1955	O
)	O
.	O
another	O
way	O
to	O
solve	O
the	O
assignment	O
problem	O
is	O
using	O
linear	B
programming	I
.	O
to	O
do	O
so	O
,	O
let	O
us	O
ﬁrst	O
write	O
the	O
assignment	O
problem	O
as	O
r	O
(	O
cid:88	O
)	O
i=1	O
r	O
(	O
cid:88	O
)	O
argmin	O
b∈rr	O
,	O
r	O
+	O
i	O
,	O
j=1	O
ai	O
,	O
jbi	O
,	O
j	O
s.t	O
.	O
∀i	O
∈	O
[	O
r	O
]	O
,	O
(	O
17.9	O
)	O
bi	O
,	O
j	O
=	O
1	O
r	O
(	O
cid:88	O
)	O
r	O
(	O
cid:88	O
)	O
j=1	O
∀j	O
∈	O
[	O
r	O
]	O
,	O
∀i	O
,	O
j	O
,	O
bi	O
,	O
j	O
∈	O
{	O
0	O
,	O
1	O
}	O
i=1	O
bi	O
,	O
j	O
=	O
1	O
a	O
matrix	O
b	O
that	O
satisﬁes	O
the	O
constraints	O
in	O
the	O
preceding	O
optimization	O
problem	O
is	O
called	O
a	O
permutation	B
matrix	I
.	O
this	O
is	O
because	O
the	O
constraints	O
guarantee	O
that	O
there	O
is	O
at	O
most	O
a	O
single	O
entry	O
of	O
each	O
row	O
that	O
equals	O
1	O
and	O
a	O
single	O
entry	O
of	O
each	O
column	O
that	O
equals	O
1.	O
therefore	O
,	O
the	O
matrix	O
b	O
corresponds	O
to	O
the	O
permutation	O
v	O
∈	O
v	O
deﬁned	O
by	O
vi	O
=	O
j	O
for	O
the	O
single	O
index	O
j	O
that	O
satisﬁes	O
bi	O
,	O
j	O
=	O
1.	O
the	O
preceding	O
optimization	O
is	O
still	O
not	O
a	O
linear	O
program	O
because	O
of	O
the	O
com-	O
binatorial	O
constraint	O
bi	O
,	O
j	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
however	O
,	O
as	O
it	O
turns	O
out	O
,	O
this	O
constraint	O
is	O
redundant	O
–	O
if	O
we	O
solve	O
the	O
optimization	O
problem	O
while	O
simply	O
omitting	O
the	O
combinatorial	O
constraint	O
,	O
then	O
we	O
are	O
still	O
guaranteed	O
that	O
there	O
is	O
an	O
optimal	O
solution	O
that	O
will	O
satisfy	O
this	O
constraint	O
.	O
this	O
is	O
formalized	O
later	O
.	O
denote	O
(	O
cid:104	O
)	O
a	O
,	O
b	O
(	O
cid:105	O
)	O
=	O
(	O
cid:80	O
)	O
mizing	O
(	O
cid:104	O
)	O
a	O
,	O
b	O
(	O
cid:105	O
)	O
such	O
that	O
b	O
is	O
a	O
permutation	B
matrix	I
.	O
i	O
,	O
j	O
ai	O
,	O
jbi	O
,	O
j	O
.	O
then	O
,	O
equation	O
(	O
17.9	O
)	O
is	O
the	O
problem	O
of	O
mini-	O
a	O
matrix	O
b	O
∈	O
rr	O
,	O
r	O
is	O
called	O
doubly	O
stochastic	O
if	O
all	O
elements	O
of	O
b	O
are	O
non-	O
negative	O
,	O
the	O
sum	O
of	O
each	O
row	O
of	O
b	O
is	O
1	O
,	O
and	O
the	O
sum	O
of	O
each	O
column	O
of	O
b	O
is	O
1.	O
therefore	O
,	O
solving	O
equation	O
(	O
17.9	O
)	O
without	O
the	O
constraints	O
bi	O
,	O
j	O
∈	O
{	O
0	O
,	O
1	O
}	O
is	O
the	O
problem	O
(	O
cid:104	O
)	O
a	O
,	O
b	O
(	O
cid:105	O
)	O
s.t	O
.	O
b	O
is	O
a	O
doubly	B
stochastic	I
matrix	I
.	O
argmin	O
b∈rr	O
,	O
r	O
(	O
17.10	O
)	O
17.5	O
bipartite	B
ranking	O
and	O
multivariate	B
performance	I
measures	I
243	O
the	O
following	O
claim	O
states	O
that	O
every	O
doubly	B
stochastic	I
matrix	I
is	O
a	O
convex	B
combination	O
of	O
permutation	O
matrices	O
.	O
claim	O
17.3	O
(	O
(	O
birkhoﬀ	O
1946	O
,	O
von	O
neumann	O
1953	O
)	O
)	O
the	O
set	B
of	O
doubly	O
stochastic	O
matrices	O
in	O
rr	O
,	O
r	O
is	O
the	O
convex	B
hull	O
of	O
the	O
set	B
of	O
permutation	O
matrices	O
in	O
rr	O
,	O
r	O
.	O
on	O
the	O
basis	O
of	O
the	O
claim	O
,	O
we	O
easily	O
obtain	O
the	O
following	O
:	O
lemma	O
17.4	O
there	O
exists	O
an	O
optimal	O
solution	O
of	O
equation	O
(	O
17.10	O
)	O
that	O
is	O
also	O
an	O
optimal	O
solution	O
of	O
equation	O
(	O
17.9	O
)	O
.	O
write	O
b	O
=	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
proof	O
let	O
b	O
be	O
a	O
solution	O
of	O
equation	O
(	O
17.10	O
)	O
.	O
then	O
,	O
by	O
claim	O
17.3	O
,	O
we	O
can	O
i	O
γici	O
,	O
where	O
each	O
ci	O
is	O
a	O
permutation	B
matrix	I
,	O
each	O
γi	O
>	O
0	O
,	O
and	O
i	O
γi	O
=	O
1.	O
since	O
all	O
the	O
ci	O
are	O
also	O
doubly	O
stochastic	O
,	O
we	O
clearly	O
have	O
that	O
(	O
cid:104	O
)	O
a	O
,	O
b	O
(	O
cid:105	O
)	O
≤	O
(	O
cid:104	O
)	O
a	O
,	O
ci	O
(	O
cid:105	O
)	O
for	O
every	O
i.	O
we	O
claim	O
that	O
there	O
is	O
some	O
i	O
for	O
which	O
(	O
cid:104	O
)	O
a	O
,	O
b	O
(	O
cid:105	O
)	O
=	O
(	O
cid:104	O
)	O
a	O
,	O
ci	O
(	O
cid:105	O
)	O
.	O
this	O
must	O
be	O
true	O
since	O
otherwise	O
,	O
if	O
for	O
every	O
i	O
(	O
cid:104	O
)	O
a	O
,	O
b	O
(	O
cid:105	O
)	O
<	O
(	O
cid:104	O
)	O
a	O
,	O
ci	O
(	O
cid:105	O
)	O
,	O
we	O
would	O
have	O
that	O
(	O
cid:104	O
)	O
a	O
,	O
b	O
(	O
cid:105	O
)	O
=	O
γi	O
(	O
cid:104	O
)	O
a	O
,	O
b	O
(	O
cid:105	O
)	O
=	O
(	O
cid:104	O
)	O
a	O
,	O
b	O
(	O
cid:105	O
)	O
,	O
γi	O
(	O
cid:104	O
)	O
a	O
,	O
ci	O
(	O
cid:105	O
)	O
>	O
(	O
cid:88	O
)	O
a	O
,	O
γici	O
=	O
(	O
cid:88	O
)	O
(	O
cid:42	O
)	O
(	O
cid:43	O
)	O
(	O
cid:88	O
)	O
i	O
i	O
i	O
which	O
can	O
not	O
hold	O
.	O
we	O
have	O
thus	O
shown	O
that	O
some	O
permutation	B
matrix	I
,	O
ci	O
,	O
satisﬁes	O
(	O
cid:104	O
)	O
a	O
,	O
b	O
(	O
cid:105	O
)	O
=	O
(	O
cid:104	O
)	O
a	O
,	O
ci	O
(	O
cid:105	O
)	O
.	O
but	O
,	O
since	O
for	O
every	O
other	O
permutation	B
matrix	I
c	O
we	O
have	O
(	O
cid:104	O
)	O
a	O
,	O
b	O
(	O
cid:105	O
)	O
≤	O
(	O
cid:104	O
)	O
a	O
,	O
c	O
(	O
cid:105	O
)	O
we	O
conclude	O
that	O
ci	O
is	O
an	O
optimal	O
solution	O
of	O
both	O
equa-	O
tion	O
(	O
17.9	O
)	O
and	O
equation	O
(	O
17.10	O
)	O
.	O
17.5	O
bipartite	B
ranking	O
and	O
multivariate	B
performance	I
measures	I
in	O
the	O
previous	O
section	O
we	O
described	O
the	O
problem	O
of	O
ranking	B
.	O
we	O
used	O
a	O
vector	O
y	O
∈	O
rr	O
for	O
representing	O
an	O
order	O
over	O
the	O
elements	O
x1	O
,	O
.	O
.	O
.	O
,	O
xr	O
.	O
if	O
all	O
elements	O
in	O
y	O
are	O
diﬀerent	O
from	O
each	O
other	O
,	O
then	O
y	O
speciﬁes	O
a	O
full	O
order	O
over	O
[	O
r	O
]	O
.	O
however	O
,	O
if	O
two	O
elements	O
of	O
y	O
attain	O
the	O
same	O
value	O
,	O
yi	O
=	O
yj	O
for	O
i	O
(	O
cid:54	O
)	O
=	O
j	O
,	O
then	O
y	O
can	O
only	O
specify	O
a	O
partial	O
order	O
over	O
[	O
r	O
]	O
.	O
in	O
such	O
a	O
case	O
,	O
we	O
say	O
that	O
xi	O
and	O
xj	O
are	O
of	O
equal	O
relevance	O
according	O
to	O
y.	O
in	O
the	O
extreme	O
case	O
,	O
y	O
∈	O
{	O
±1	O
}	O
r	O
,	O
which	O
means	O
that	O
each	O
xi	O
is	O
either	O
relevant	O
or	O
nonrelevant	O
.	O
this	O
setting	O
is	O
often	O
called	O
“	O
bipartite	B
ranking.	O
”	O
for	O
example	O
,	O
in	O
the	O
fraud	O
detection	O
application	O
mentioned	O
in	O
the	O
previous	O
section	O
,	O
each	O
transaction	O
is	O
labeled	O
as	O
either	O
fraudulent	O
(	O
yi	O
=	O
1	O
)	O
or	O
benign	O
(	O
yi	O
=	O
−1	O
)	O
.	O
seemingly	O
,	O
we	O
can	O
solve	O
the	O
bipartite	B
ranking	O
problem	O
by	O
learning	O
a	O
binary	O
classiﬁer	B
,	O
applying	O
it	O
on	O
each	O
instance	B
,	O
and	O
putting	O
the	O
positive	O
ones	O
at	O
the	O
top	O
of	O
the	O
ranked	O
list	O
.	O
however	O
,	O
this	O
may	O
lead	O
to	O
poor	O
results	O
as	O
the	O
goal	O
of	O
a	O
binary	O
learner	O
is	O
usually	O
to	O
minimize	O
the	O
zero-one	O
loss	B
(	O
or	O
some	O
surrogate	O
of	O
it	O
)	O
,	O
while	O
the	O
goal	O
of	O
a	O
ranker	O
might	O
be	O
signiﬁcantly	O
diﬀerent	O
.	O
to	O
illustrate	O
this	O
,	O
consider	O
again	O
the	O
problem	O
of	O
fraud	O
detection	O
.	O
usually	O
,	O
most	O
of	O
the	O
transactions	O
are	O
benign	O
(	O
say	O
99.9	O
%	O
)	O
.	O
therefore	O
,	O
a	O
binary	O
classiﬁer	B
that	O
predicts	O
“	O
benign	O
”	O
on	O
all	O
transactions	O
will	O
have	O
a	O
zero-one	O
error	O
of	O
0.1	O
%	O
.	O
while	O
this	O
is	O
a	O
very	O
small	O
number	O
,	O
the	O
resulting	O
predictor	B
is	O
meaningless	O
for	O
the	O
fraud	O
detection	O
application	O
.	O
the	O
crux	O
of	O
the	O
244	O
multiclass	B
,	O
ranking	B
,	O
and	O
complex	O
prediction	O
problems	O
problem	O
stems	O
from	O
the	O
inadequacy	O
of	O
the	O
zero-one	O
loss	B
for	O
what	O
we	O
are	O
really	O
interested	O
in	O
.	O
a	O
more	O
adequate	O
performance	O
measure	O
should	O
take	O
into	O
account	O
the	O
predictions	O
over	O
the	O
entire	O
set	B
of	O
instances	O
.	O
for	O
example	O
,	O
in	O
the	O
previous	O
section	O
we	O
have	O
deﬁned	O
the	O
ndcg	O
loss	B
,	O
which	O
emphasizes	O
the	O
correctness	O
of	O
the	O
top-ranked	O
items	O
.	O
in	O
this	O
section	O
we	O
describe	O
additional	O
loss	B
functions	O
that	O
are	O
speciﬁcally	O
adequate	O
for	O
bipartite	B
ranking	O
problems	O
.	O
as	O
in	O
the	O
previous	O
section	O
,	O
we	O
are	O
given	O
a	O
sequence	O
of	O
instances	O
,	O
¯x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xr	O
)	O
,	O
and	O
we	O
predict	O
a	O
ranking	B
vector	O
y	O
(	O
cid:48	O
)	O
∈	O
rr	O
.	O
the	O
feedback	O
vector	O
is	O
y	O
∈	O
{	O
±1	O
}	O
r.	O
we	O
deﬁne	O
a	O
loss	B
that	O
depends	O
on	O
y	O
(	O
cid:48	O
)	O
and	O
y	O
and	O
depends	O
on	O
a	O
threshold	O
θ	O
∈	O
r.	O
this	O
threshold	O
transforms	O
the	O
vector	O
y	O
(	O
cid:48	O
)	O
∈	O
rr	O
into	O
the	O
vector	O
(	O
sign	O
(	O
y	O
(	O
cid:48	O
)	O
r−	O
θ	O
)	O
)	O
∈	O
{	O
±1	O
}	O
r.	O
usually	O
,	O
the	O
value	O
of	O
θ	O
is	O
set	B
to	O
be	O
0.	O
however	O
,	O
as	O
we	O
will	O
see	O
,	O
we	O
sometimes	O
set	B
θ	O
while	O
taking	O
into	O
account	O
additional	O
constraints	O
on	O
the	O
problem	O
.	O
the	O
loss	B
functions	O
we	O
deﬁne	O
in	O
the	O
following	O
depend	O
on	O
the	O
following	O
4	O
num-	O
i−θ	O
)	O
,	O
.	O
.	O
.	O
,	O
sign	O
(	O
y	O
(	O
cid:48	O
)	O
bers	O
:	O
true	O
positives	O
:	O
a	O
=	O
|	O
{	O
i	O
:	O
yi	O
=	O
+1	O
∧	O
sign	O
(	O
y	O
(	O
cid:48	O
)	O
false	O
positives	O
:	O
b	O
=	O
|	O
{	O
i	O
:	O
yi	O
=	O
−1	O
∧	O
sign	O
(	O
y	O
(	O
cid:48	O
)	O
false	O
negatives	O
:	O
c	O
=	O
|	O
{	O
i	O
:	O
yi	O
=	O
+1	O
∧	O
sign	O
(	O
y	O
(	O
cid:48	O
)	O
true	O
negatives	O
:	O
d	O
=	O
|	O
{	O
i	O
:	O
yi	O
=	O
−1	O
∧	O
sign	O
(	O
y	O
(	O
cid:48	O
)	O
i	O
−	O
θ	O
)	O
=	O
+1	O
}	O
|	O
i	O
−	O
θ	O
)	O
=	O
+1	O
}	O
|	O
i	O
−	O
θ	O
)	O
=	O
−1	O
}	O
|	O
i	O
−	O
θ	O
)	O
=	O
−1	O
}	O
|	O
(	O
17.11	O
)	O
the	O
recall	B
(	O
a.k.a	O
.	O
sensitivity	B
)	O
of	O
a	O
prediction	O
vector	O
is	O
the	O
fraction	O
of	O
true	O
a	O
a+c	O
.	O
the	O
precision	B
is	O
the	O
fraction	O
of	O
correct	O
a	O
a+b	O
.	O
the	O
speciﬁcity	B
positives	O
y	O
(	O
cid:48	O
)	O
“	O
catches	O
,	O
”	O
namely	O
,	O
predictions	O
among	O
the	O
positive	O
labels	O
we	O
predict	O
,	O
namely	O
,	O
is	O
the	O
fraction	O
of	O
true	O
negatives	O
that	O
our	O
predictor	B
“	O
catches	O
,	O
”	O
namely	O
,	O
d	O
d+b	O
.	O
(	O
cid:17	O
)	O
note	O
that	O
as	O
we	O
decrease	O
θ	O
the	O
recall	B
increases	O
(	O
attaining	O
the	O
value	O
1	O
when	O
θ	O
=	O
−∞	O
)	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
precision	B
and	O
the	O
speciﬁcity	B
usually	O
decrease	O
as	O
we	O
decrease	O
θ.	O
therefore	O
,	O
there	O
is	O
a	O
tradeoﬀ	O
between	O
precision	B
and	O
recall	B
,	O
and	O
we	O
can	O
control	O
it	O
by	O
changing	O
θ.	O
the	O
loss	B
functions	O
deﬁned	O
in	O
the	O
following	O
use	O
various	O
techniques	O
for	O
combining	O
both	O
the	O
precision	B
and	O
recall	B
.	O
•	O
averaging	O
sensitivity	B
and	O
speciﬁcity	B
:	O
this	O
measure	O
is	O
the	O
average	O
of	O
the	O
sensitivity	B
and	O
speciﬁcity	B
,	O
namely	O
,	O
1	O
.	O
this	O
is	O
also	O
the	O
accuracy	B
2	O
on	O
positive	O
examples	O
averaged	O
with	O
the	O
accuracy	B
on	O
negative	O
examples	O
.	O
here	O
,	O
we	O
set	B
θ	O
=	O
0	O
and	O
the	O
corresponding	O
loss	B
function	I
is	O
∆	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
=	O
1	O
−	O
1	O
(	O
cid:16	O
)	O
a	O
(	O
cid:16	O
)	O
a	O
a+c	O
+	O
d	O
2	O
recall	B
•	O
f1-score	O
:	O
the	O
f1	O
score	O
is	O
the	O
harmonic	O
mean	O
of	O
the	O
precision	B
and	O
recall	B
:	O
.	O
its	O
maximal	O
value	O
(	O
of	O
1	O
)	O
is	O
obtained	O
when	O
both	O
precision	B
precision	O
+	O
1	O
and	O
recall	B
are	O
1	O
,	O
and	O
its	O
minimal	O
value	O
(	O
of	O
0	O
)	O
is	O
obtained	O
whenever	O
one	O
of	O
them	O
is	O
0	O
(	O
even	O
if	O
the	O
other	O
one	O
is	O
1	O
)	O
.	O
the	O
f1	O
score	O
can	O
be	O
written	O
using	O
the	O
numbers	O
a	O
,	O
b	O
,	O
c	O
as	O
follows	O
;	O
f1	O
=	O
2a	O
2a+b+c	O
.	O
again	O
,	O
we	O
set	B
θ	O
=	O
0	O
,	O
and	O
the	O
loss	B
function	I
becomes	O
∆	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
=	O
1	O
−	O
f1	O
.	O
•	O
fβ-score	O
:	O
it	O
is	O
like	O
f1	O
score	O
,	O
but	O
we	O
attach	O
β2	O
times	O
more	O
importance	O
to	O
.	O
it	O
can	O
also	O
be	O
written	O
as	O
recall	B
than	O
to	O
precision	B
,	O
that	O
is	O
,	O
a+c	O
+	O
d	O
d+b	O
d+b	O
(	O
cid:17	O
)	O
.	O
1+β2	O
precision	B
+β2	O
1	O
1	O
recall	B
2	O
1	O
17.5	O
bipartite	B
ranking	O
and	O
multivariate	B
performance	I
measures	I
245	O
(	O
1+β2	O
)	O
a	O
fβ	O
=	O
∆	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
=	O
1	O
−	O
fβ	O
.	O
(	O
1+β2	O
)	O
a+b+β2c	O
.	O
again	O
,	O
we	O
set	B
θ	O
=	O
0	O
,	O
and	O
the	O
loss	B
function	I
becomes	O
•	O
recall	B
at	O
k	O
:	O
we	O
measure	O
the	O
recall	B
while	O
the	O
prediction	O
must	O
contain	O
at	O
most	O
k	O
positive	O
labels	O
.	O
that	O
is	O
,	O
we	O
should	O
set	B
θ	O
so	O
that	O
a	O
+	O
b	O
≤	O
k.	O
this	O
is	O
conve-	O
nient	O
,	O
for	O
example	O
,	O
in	O
the	O
application	O
of	O
a	O
fraud	O
detection	O
system	O
,	O
where	O
a	O
bank	O
employee	O
can	O
only	O
handle	O
a	O
small	O
number	O
of	O
suspicious	O
transactions	O
.	O
•	O
precision	B
at	O
k	O
:	O
we	O
measure	O
the	O
precision	B
while	O
the	O
prediction	O
must	O
contain	O
at	O
least	O
k	O
positive	O
labels	O
.	O
that	O
is	O
,	O
we	O
should	O
set	B
θ	O
so	O
that	O
a	O
+	O
b	O
≥	O
k.	O
the	O
measures	O
deﬁned	O
previously	O
are	O
often	O
referred	O
to	O
as	O
multivariate	O
perfor-	O
mance	O
measures	O
.	O
note	O
that	O
these	O
measures	O
are	O
highly	O
diﬀerent	O
from	O
the	O
average	O
a+b+c+d	O
.	O
in	O
the	O
aforemen-	O
zero-one	O
loss	B
,	O
which	O
in	O
the	O
preceding	O
notation	O
equals	O
tioned	O
example	O
of	O
fraud	O
detection	O
,	O
when	O
99.9	O
%	O
of	O
the	O
examples	O
are	O
negatively	O
labeled	O
,	O
the	O
zero-one	O
loss	B
of	O
predicting	O
that	O
all	O
the	O
examples	O
are	O
negatives	O
is	O
0.1	O
%	O
.	O
in	O
contrast	O
,	O
the	O
recall	B
of	O
such	O
prediction	O
is	O
0	O
and	O
hence	O
the	O
f1	O
score	O
is	O
also	O
0	O
,	O
which	O
means	O
that	O
the	O
corresponding	O
loss	B
will	O
be	O
1.	O
b+d	O
17.5.1	O
linear	B
predictors	I
for	O
bipartite	B
ranking	O
we	O
next	O
describe	O
how	O
to	O
train	O
linear	B
predictors	I
for	O
bipartite	B
ranking	O
.	O
as	O
in	O
the	O
previous	O
section	O
,	O
a	O
linear	B
predictor	I
for	O
ranking	B
is	O
deﬁned	O
to	O
be	O
hw	O
(	O
¯x	O
)	O
=	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x1	O
(	O
cid:105	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
cid:104	O
)	O
w	O
,	O
xr	O
(	O
cid:105	O
)	O
)	O
.	O
the	O
corresponding	O
loss	B
function	I
is	O
one	O
of	O
the	O
multivariate	B
performance	I
measures	I
described	O
before	O
.	O
the	O
loss	B
function	I
depends	O
on	O
y	O
(	O
cid:48	O
)	O
=	O
hw	O
(	O
¯x	O
)	O
via	O
the	O
binary	O
vector	O
it	O
induces	O
,	O
which	O
we	O
denote	O
by	O
b	O
(	O
y	O
(	O
cid:48	O
)	O
)	O
=	O
(	O
sign	O
(	O
y	O
(	O
cid:48	O
)	O
1	O
−	O
θ	O
)	O
,	O
.	O
.	O
.	O
,	O
sign	O
(	O
y	O
(	O
cid:48	O
)	O
r	O
−	O
θ	O
)	O
)	O
∈	O
{	O
±1	O
}	O
r.	O
(	O
17.12	O
)	O
as	O
in	O
the	O
previous	O
section	O
,	O
to	O
facilitate	O
an	O
eﬃcient	O
algorithm	O
we	O
derive	O
a	O
convex	B
surrogate	O
loss	B
function	I
on	O
∆	O
.	O
the	O
derivation	O
is	O
similar	O
to	O
the	O
derivation	O
of	O
the	O
generalized	O
hinge	O
loss	B
for	O
the	O
ndcg	O
ranking	B
loss	O
,	O
as	O
described	O
in	O
the	O
previous	O
section	O
.	O
our	O
ﬁrst	O
observation	O
is	O
that	O
for	O
all	O
the	O
values	O
of	O
θ	O
deﬁned	O
before	O
,	O
there	O
is	O
some	O
v	O
⊆	O
{	O
±1	O
}	O
r	O
such	O
that	O
b	O
(	O
y	O
(	O
cid:48	O
)	O
)	O
can	O
be	O
rewritten	O
as	O
r	O
(	O
cid:88	O
)	O
i=1	O
b	O
(	O
y	O
(	O
cid:48	O
)	O
)	O
=	O
argmax	O
v∈v	O
viy	O
(	O
cid:48	O
)	O
i	O
.	O
(	O
17.13	O
)	O
this	O
is	O
clearly	O
true	O
for	O
the	O
case	O
θ	O
=	O
0	O
if	O
we	O
choose	O
v	O
=	O
{	O
±1	O
}	O
r.	O
the	O
two	O
measures	O
for	O
which	O
θ	O
is	O
not	O
taken	O
to	O
be	O
0	O
are	O
precision	B
at	O
k	O
and	O
recall	B
at	O
k.	O
for	O
precision	B
at	O
k	O
we	O
can	O
take	O
v	O
to	O
be	O
the	O
set	B
v≥k	O
,	O
containing	O
all	O
vectors	O
in	O
{	O
±1	O
}	O
r	O
whose	O
number	O
of	O
ones	O
is	O
at	O
least	O
k.	O
for	O
recall	B
at	O
k	O
,	O
we	O
can	O
take	O
v	O
to	O
be	O
v≤k	O
,	O
which	O
is	O
deﬁned	O
analogously	O
.	O
see	O
exercise	O
5	O
.	O
246	O
multiclass	B
,	O
ranking	B
,	O
and	O
complex	O
prediction	O
problems	O
once	O
we	O
have	O
deﬁned	O
b	O
as	O
in	O
equation	O
(	O
17.13	O
)	O
,	O
we	O
can	O
easily	O
derive	O
a	O
convex	B
surrogate	O
loss	B
as	O
follows	O
.	O
assuming	O
that	O
y	O
∈	O
v	O
,	O
we	O
have	O
that	O
∆	O
(	O
hw	O
(	O
¯x	O
)	O
,	O
y	O
)	O
=	O
∆	O
(	O
b	O
(	O
hw	O
(	O
¯x	O
)	O
)	O
,	O
y	O
)	O
(	O
cid:34	O
)	O
≤	O
max	O
v∈v	O
r	O
(	O
cid:88	O
)	O
r	O
(	O
cid:88	O
)	O
i=1	O
i=1	O
≤	O
∆	O
(	O
b	O
(	O
hw	O
(	O
¯x	O
)	O
)	O
,	O
y	O
)	O
+	O
(	O
bi	O
(	O
hw	O
(	O
¯x	O
)	O
)	O
−	O
yi	O
)	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
(	O
cid:35	O
)	O
∆	O
(	O
v	O
,	O
y	O
)	O
+	O
(	O
vi	O
−	O
yi	O
)	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
.	O
(	O
17.14	O
)	O
the	O
right-hand	O
side	O
is	O
a	O
convex	B
function	O
with	O
respect	O
to	O
w.	O
we	O
can	O
now	O
solve	O
the	O
learning	O
problem	O
using	O
sgd	O
as	O
described	O
in	O
section	O
17.2.5.	O
the	O
main	O
computational	O
bottleneck	O
is	O
calculating	O
a	O
subgradient	O
of	O
the	O
loss	B
func-	O
tion	O
,	O
which	O
is	O
equivalent	O
to	O
ﬁnding	O
v	O
that	O
achieves	O
the	O
maximum	O
in	O
equa-	O
tion	O
(	O
17.14	O
)	O
(	O
see	O
claim	O
14.6	O
)	O
.	O
in	O
the	O
following	O
we	O
describe	O
how	O
to	O
ﬁnd	O
this	O
maximizer	O
eﬃciently	O
for	O
any	O
performance	O
measure	O
that	O
can	O
be	O
written	O
as	O
a	O
function	B
of	O
the	O
numbers	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
given	O
in	O
equation	O
(	O
17.11	O
)	O
,	O
and	O
for	O
which	O
the	O
set	B
v	O
contains	O
all	O
elements	O
in	O
{	O
±1	O
}	O
r	O
for	O
which	O
the	O
values	O
of	O
a	O
,	O
b	O
satisfy	O
some	O
constraints	O
.	O
for	O
example	O
,	O
for	O
“	O
recall	B
at	O
k	O
”	O
the	O
set	B
v	O
is	O
all	O
vectors	O
for	O
which	O
a	O
+	O
b	O
≤	O
k.	O
the	O
idea	O
is	O
as	O
follows	O
.	O
for	O
any	O
a	O
,	O
b	O
∈	O
[	O
r	O
]	O
,	O
let	O
¯ya	O
,	O
b	O
=	O
{	O
v	O
:	O
|	O
{	O
i	O
:	O
vi	O
=	O
1	O
∧	O
yi	O
=	O
1	O
}	O
|	O
=	O
a	O
∧	O
|	O
{	O
i	O
:	O
vi	O
=	O
1	O
∧	O
yi	O
=	O
−1	O
}	O
|	O
=	O
b	O
}	O
.	O
any	O
vector	O
v	O
∈	O
v	O
falls	O
into	O
¯ya	O
,	O
b	O
for	O
some	O
a	O
,	O
b	O
∈	O
[	O
r	O
]	O
.	O
furthermore	O
,	O
if	O
¯ya	O
,	O
b	O
∩	O
v	O
is	O
not	O
empty	O
for	O
some	O
a	O
,	O
b	O
∈	O
[	O
r	O
]	O
then	O
¯ya	O
,	O
b	O
∩	O
v	O
=	O
¯ya	O
,	O
b	O
.	O
therefore	O
,	O
we	O
can	O
search	O
within	O
each	O
¯ya	O
,	O
b	O
that	O
has	O
a	O
nonempty	O
intersection	O
with	O
v	O
separately	O
,	O
and	O
then	O
take	O
the	O
optimal	O
value	O
.	O
the	O
key	O
observation	O
is	O
that	O
once	O
we	O
are	O
searching	O
only	O
within	O
¯ya	O
,	O
b	O
,	O
the	O
value	O
of	O
∆	O
is	O
ﬁxed	O
so	O
we	O
only	O
need	O
to	O
maximize	O
the	O
expression	O
r	O
(	O
cid:88	O
)	O
i=1	O
max	O
v∈	O
¯ya	O
,	O
b	O
vi	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
.	O
suppose	O
the	O
examples	O
are	O
sorted	O
so	O
that	O
(	O
cid:104	O
)	O
w	O
,	O
x1	O
(	O
cid:105	O
)	O
≥	O
···	O
≥	O
(	O
cid:104	O
)	O
w	O
,	O
xr	O
(	O
cid:105	O
)	O
.	O
then	O
,	O
it	O
is	O
easy	O
to	O
verify	O
that	O
we	O
would	O
like	O
to	O
set	B
vi	O
to	O
be	O
positive	O
for	O
the	O
smallest	O
indices	O
i.	O
doing	O
this	O
,	O
with	O
the	O
constraint	O
on	O
a	O
,	O
b	O
,	O
amounts	O
to	O
setting	O
vi	O
=	O
1	O
for	O
the	O
a	O
top	O
ranked	O
positive	O
examples	O
and	O
for	O
the	O
b	O
top-ranked	O
negative	O
examples	O
.	O
this	O
yields	O
the	O
following	O
procedure	O
.	O
17.6	O
summary	O
247	O
solving	O
equation	O
(	O
17.14	O
)	O
input	O
:	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xr	O
)	O
,	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yr	O
)	O
,	O
w	O
,	O
v	O
,	O
∆	O
assumptions	O
:	O
∆	O
is	O
a	O
function	B
of	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
v	O
contains	O
all	O
vectors	O
for	O
which	O
f	O
(	O
a	O
,	O
b	O
)	O
=	O
1	O
for	O
some	O
function	B
f	O
initialize	O
:	O
p	O
=	O
|	O
{	O
i	O
:	O
yi	O
=	O
1	O
}	O
|	O
,	O
n	O
=	O
|	O
{	O
i	O
:	O
yi	O
=	O
−1	O
}	O
|	O
µ	O
=	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x1	O
(	O
cid:105	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
cid:104	O
)	O
w	O
,	O
xr	O
(	O
cid:105	O
)	O
)	O
,	O
α	O
(	O
cid:63	O
)	O
=	O
−∞	O
sort	O
examples	O
so	O
that	O
µ1	O
≥	O
µ2	O
≥	O
···	O
≥	O
µr	O
let	O
i1	O
,	O
.	O
.	O
.	O
,	O
ip	O
be	O
the	O
(	O
sorted	O
)	O
indices	O
of	O
the	O
positive	O
examples	O
let	O
j1	O
,	O
.	O
.	O
.	O
,	O
jn	O
be	O
the	O
(	O
sorted	O
)	O
indices	O
of	O
the	O
negative	O
examples	O
for	O
a	O
=	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
p	O
c	O
=	O
p	O
−	O
a	O
for	O
b	O
=	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
such	O
that	O
f	O
(	O
a	O
,	O
b	O
)	O
=	O
1	O
d	O
=	O
n	O
−	O
b	O
calculate	O
∆	O
using	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
set	B
v1	O
,	O
.	O
.	O
.	O
,	O
vr	O
s.t	O
.	O
vi1	O
=	O
···	O
=	O
via	O
=	O
vj1	O
=	O
···	O
=	O
vjb	O
=	O
1	O
and	O
the	O
rest	O
of	O
the	O
elements	O
of	O
v	O
equal	O
−1	O
if	O
α	O
≥	O
α	O
(	O
cid:63	O
)	O
α	O
(	O
cid:63	O
)	O
=	O
α	O
,	O
v	O
(	O
cid:63	O
)	O
=	O
v	O
set	B
α	O
=	O
∆	O
+	O
(	O
cid:80	O
)	O
r	O
i=1	O
viµi	O
output	O
v	O
(	O
cid:63	O
)	O
17.6	O
summary	O
many	O
real	O
world	O
supervised	O
learning	O
problems	O
can	O
be	O
cast	O
as	O
learning	O
a	O
multiclass	B
predictor	O
.	O
we	O
started	O
the	O
chapter	O
by	O
introducing	O
reductions	B
of	O
multiclass	B
learning	O
to	O
binary	O
learning	O
.	O
we	O
then	O
described	O
and	O
analyzed	O
the	O
family	O
of	O
linear	B
predictors	I
for	O
multiclass	B
learning	O
.	O
we	O
have	O
shown	O
how	O
this	O
family	O
can	O
be	O
used	O
even	O
if	O
the	O
number	O
of	O
classes	O
is	O
extremely	O
large	O
,	O
as	O
long	O
as	O
we	O
have	O
an	O
adequate	O
structure	O
on	O
the	O
problem	O
.	O
finally	O
,	O
we	O
have	O
described	O
ranking	B
problems	O
.	O
in	O
chapter	O
29	O
we	O
study	O
the	O
sample	B
complexity	I
of	O
multiclass	B
learning	O
in	O
more	O
detail	O
.	O
17.7	O
bibliographic	O
remarks	O
the	O
one-versus-all	O
and	O
all-pairs	B
approach	O
reductions	B
have	O
been	O
uniﬁed	O
un-	O
der	O
the	O
framework	O
of	O
error	O
correction	O
output	O
codes	O
(	O
ecoc	O
)	O
(	O
dietterich	O
&	O
bakiri	O
1995	O
,	O
allwein	O
,	O
schapire	O
&	O
singer	O
2000	O
)	O
.	O
there	O
are	O
also	O
other	O
types	O
of	O
re-	O
ductions	O
such	O
as	O
tree-based	O
classiﬁers	O
(	O
see	O
,	O
for	O
example	O
,	O
beygelzimer	O
,	O
langford	O
&	O
ravikumar	O
(	O
2007	O
)	O
)	O
.	O
the	O
limitations	O
of	O
reduction	O
techniques	O
have	O
been	O
studied	O
248	O
multiclass	B
,	O
ranking	B
,	O
and	O
complex	O
prediction	O
problems	O
in	O
(	O
daniely	O
et	O
al	O
.	O
2011	O
,	O
daniely	O
,	O
sabato	O
&	O
shwartz	O
2012	O
)	O
.	O
see	O
also	O
chapter	O
29	O
,	O
in	O
which	O
we	O
analyze	O
the	O
sample	B
complexity	I
of	O
multiclass	B
learning	O
.	O
direct	O
approaches	O
to	O
multiclass	B
learning	O
with	O
linear	B
predictors	I
have	O
been	O
stud-	O
ied	O
in	O
(	O
vapnik	O
1998	O
,	O
weston	O
&	O
watkins	O
1999	O
,	O
crammer	O
&	O
singer	O
2001	O
)	O
.	O
in	O
par-	O
ticular	O
,	O
the	O
multivector	O
construction	O
is	O
due	O
to	O
crammer	O
&	O
singer	O
(	O
2001	O
)	O
.	O
collins	O
(	O
2000	O
)	O
has	O
shown	O
how	O
to	O
apply	O
the	O
perceptron	O
algorithm	O
for	O
structured	O
output	O
problems	O
.	O
see	O
also	O
collins	O
(	O
2002	O
)	O
.	O
a	O
related	O
approach	O
is	O
discriminative	B
learning	O
of	O
conditional	O
random	O
ﬁelds	O
;	O
see	O
laﬀerty	O
,	O
mccallum	O
&	O
pereira	O
(	O
2001	O
)	O
.	O
structured	O
output	O
svm	O
has	O
been	O
studied	O
in	O
(	O
weston	O
,	O
chapelle	O
,	O
vapnik	O
,	O
elisseeﬀ	O
&	O
sch¨olkopf	O
2002	O
,	O
taskar	O
,	O
guestrin	O
&	O
koller	O
2003	O
,	O
tsochantaridis	O
,	O
hofmann	O
,	O
joachims	O
&	O
altun	O
2004	O
)	O
.	O
the	O
dynamic	O
procedure	O
we	O
have	O
presented	O
for	O
calculating	O
the	O
prediction	O
hw	O
(	O
x	O
)	O
in	O
the	O
structured	O
output	O
section	O
is	O
similar	O
to	O
the	O
forward-backward	O
variables	O
calculated	O
by	O
the	O
viterbi	O
procedure	O
in	O
hmms	O
(	O
see	O
,	O
for	O
instance	B
,	O
(	O
rabiner	O
&	O
juang	O
1986	O
)	O
)	O
.	O
more	O
generally	O
,	O
solving	O
the	O
maximization	O
problem	O
in	O
structured	O
output	O
is	O
closely	O
related	O
to	O
the	O
problem	O
of	O
inference	O
in	O
graphical	O
models	O
(	O
see	O
,	O
for	O
example	O
,	O
koller	O
&	O
friedman	O
(	O
2009	O
)	O
)	O
.	O
chapelle	O
,	O
le	O
&	O
smola	O
(	O
2007	O
)	O
proposed	O
to	O
learn	O
a	O
ranking	B
function	O
with	O
respect	O
to	O
the	O
ndcg	O
loss	B
using	O
ideas	O
from	O
structured	O
output	O
learning	O
.	O
they	O
also	O
ob-	O
served	O
that	O
the	O
maximization	O
problem	O
in	O
the	O
deﬁnition	O
of	O
the	O
generalized	O
hinge	O
loss	B
is	O
equivalent	O
to	O
the	O
assignment	O
problem	O
.	O
agarwal	O
&	O
roth	O
(	O
2005	O
)	O
analyzed	O
the	O
sample	B
complexity	I
of	O
bipartite	B
ranking	O
.	O
joachims	O
(	O
2005	O
)	O
studied	O
the	O
applicability	O
of	O
structured	O
output	O
svm	O
to	O
bipartite	B
ranking	O
with	O
multivariate	B
performance	I
measures	I
.	O
17.8	O
exercises	O
1.	O
consider	O
a	O
set	B
s	O
of	O
examples	O
in	O
rn×	O
[	O
k	O
]	O
for	O
which	O
there	O
exist	O
vectors	O
µ1	O
,	O
.	O
.	O
.	O
,	O
µk	O
such	O
that	O
every	O
example	O
(	O
x	O
,	O
y	O
)	O
∈	O
s	O
falls	O
within	O
a	O
ball	O
centered	O
at	O
µy	O
whose	O
radius	O
is	O
r	O
≥	O
1.	O
assume	O
also	O
that	O
for	O
every	O
i	O
(	O
cid:54	O
)	O
=	O
j	O
,	O
(	O
cid:107	O
)	O
µi	O
−	O
µj	O
(	O
cid:107	O
)	O
≥	O
4r	O
.	O
con-	O
sider	O
concatenating	O
each	O
instance	B
by	O
the	O
constant	O
1	O
and	O
then	O
applying	O
the	O
multivector	O
construction	O
,	O
namely	O
,	O
ψ	O
(	O
x	O
,	O
y	O
)	O
=	O
[	O
,	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
1	O
,	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
∈r	O
(	O
y−1	O
)	O
(	O
n+1	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
∈rn+1	O
(	O
cid:125	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
∈r	O
(	O
k−y	O
)	O
(	O
n+1	O
)	O
]	O
.	O
show	O
that	O
there	O
exists	O
a	O
vector	O
w	O
∈	O
rk	O
(	O
n+1	O
)	O
such	O
that	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
0	O
for	O
every	O
(	O
x	O
,	O
y	O
)	O
∈	O
s.	O
hint	O
:	O
observe	O
that	O
for	O
every	O
example	O
(	O
x	O
,	O
y	O
)	O
∈	O
s	O
we	O
can	O
write	O
x	O
=	O
µy	O
+	O
v	O
for	O
some	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
≤	O
r.	O
now	O
,	O
take	O
w	O
=	O
[	O
w1	O
,	O
.	O
.	O
.	O
,	O
wk	O
]	O
,	O
where	O
wi	O
=	O
[	O
µi	O
,	O
−	O
(	O
cid:107	O
)	O
µi	O
(	O
cid:107	O
)	O
2/2	O
]	O
.	O
2.	O
multiclass	B
perceptron	O
:	O
consider	O
the	O
following	O
algorithm	O
:	O
17.8	O
exercises	O
249	O
multiclass	B
batch	O
perceptron	O
input	O
:	O
a	O
training	B
set	I
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
a	O
class-sensitive	B
feature	I
mapping	I
ψ	O
:	O
x	O
×	O
y	O
→	O
rd	O
initialize	O
:	O
w	O
(	O
1	O
)	O
=	O
(	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
)	O
∈	O
rd	O
for	O
t	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
if	O
(	O
∃	O
i	O
and	O
y	O
(	O
cid:54	O
)	O
=	O
yi	O
s.t	O
.	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
ψ	O
(	O
xi	O
,	O
yi	O
)	O
(	O
cid:105	O
)	O
≤	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
ψ	O
(	O
xi	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
)	O
then	O
w	O
(	O
t+1	O
)	O
=	O
w	O
(	O
t	O
)	O
+	O
ψ	O
(	O
xi	O
,	O
yi	O
)	O
−	O
ψ	O
(	O
xi	O
,	O
y	O
)	O
else	O
output	O
w	O
(	O
t	O
)	O
prove	O
the	O
following	O
:	O
theorem	O
17.5	O
assume	O
that	O
there	O
exists	O
w	O
(	O
cid:63	O
)	O
such	O
that	O
for	O
all	O
i	O
and	O
for	O
all	O
y	O
(	O
cid:54	O
)	O
=	O
yi	O
it	O
holds	O
that	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
ψ	O
(	O
xi	O
,	O
yi	O
)	O
(	O
cid:105	O
)	O
≥	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
ψ	O
(	O
xi	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
+1	O
.	O
let	O
r	O
=	O
maxi	O
,	O
y	O
(	O
cid:107	O
)	O
ψ	O
(	O
xi	O
,	O
yi	O
)	O
−	O
ψ	O
(	O
xi	O
,	O
y	O
)	O
(	O
cid:107	O
)	O
.	O
then	O
,	O
the	O
multiclass	B
perceptron	O
algorithm	O
stops	O
after	O
at	O
most	O
(	O
r	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
)	O
2	O
iterations	O
,	O
and	O
when	O
it	O
stops	O
it	O
holds	O
that	O
∀i	O
∈	O
[	O
m	O
]	O
,	O
yi	O
=	O
argmaxy	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
ψ	O
(	O
xi	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
.	O
dure	O
for	O
multiclass	B
prediction	O
.	O
you	O
can	O
assume	O
that	O
∆	O
(	O
y	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
=	O
(	O
cid:80	O
)	O
r	O
3.	O
generalize	O
the	O
dynamic	O
programming	O
procedure	O
given	O
in	O
section	O
17.3	O
for	O
solv-	O
ing	O
the	O
maximization	O
problem	O
given	O
in	O
the	O
deﬁnition	O
of	O
ˆh	O
in	O
the	O
sgd	O
proce-	O
t	O
,	O
yt	O
)	O
t=1	O
δ	O
(	O
y	O
(	O
cid:48	O
)	O
for	O
some	O
arbitrary	O
function	B
δ	O
.	O
4.	O
prove	O
that	O
equation	O
(	O
17.7	O
)	O
holds	O
.	O
5.	O
show	O
that	O
the	O
two	O
deﬁnitions	O
of	O
π	O
as	O
deﬁned	O
in	O
equation	O
(	O
17.12	O
)	O
and	O
equa-	O
tion	O
(	O
17.13	O
)	O
are	O
indeed	O
equivalent	O
for	O
all	O
the	O
multivariate	O
performance	O
mea-	O
sures	O
.	O
18	O
decision	B
trees	I
a	O
decision	O
tree	O
is	O
a	O
predictor	B
,	O
h	O
:	O
x	O
→	O
y	O
,	O
that	O
predicts	O
the	O
label	B
associated	O
with	O
an	O
instance	B
x	O
by	O
traveling	O
from	O
a	O
root	O
node	O
of	O
a	O
tree	O
to	O
a	O
leaf	O
.	O
for	O
simplicity	O
we	O
focus	O
on	O
the	O
binary	O
classiﬁcation	O
setting	O
,	O
namely	O
,	O
y	O
=	O
{	O
0	O
,	O
1	O
}	O
,	O
but	O
decision	B
trees	I
can	O
be	O
applied	O
for	O
other	O
prediction	O
problems	O
as	O
well	O
.	O
at	O
each	O
node	O
on	O
the	O
root-to-leaf	O
path	O
,	O
the	O
successor	O
child	O
is	O
chosen	O
on	O
the	O
basis	O
of	O
a	O
splitting	O
of	O
the	O
input	O
space	O
.	O
usually	O
,	O
the	O
splitting	O
is	O
based	O
on	O
one	O
of	O
the	O
features	O
of	O
x	O
or	O
on	O
a	O
predeﬁned	O
set	B
of	O
splitting	O
rules	O
.	O
a	O
leaf	O
contains	O
a	O
speciﬁc	O
label	B
.	O
an	O
example	O
of	O
a	O
decision	O
tree	O
for	O
the	O
papayas	O
example	O
(	O
described	O
in	O
chapter	O
2	O
)	O
is	O
given	O
in	O
the	O
following	O
:	O
color	O
?	O
other	O
pale	O
green	O
to	O
pale	O
yellow	O
not-tasty	O
softness	O
?	O
other	O
gives	O
slightly	O
to	O
palm	O
pressure	O
not-tasty	O
tasty	O
to	O
check	O
if	O
a	O
given	O
papaya	O
is	O
tasty	O
or	O
not	O
,	O
the	O
decision	O
tree	O
ﬁrst	O
examines	O
the	O
color	O
of	O
the	O
papaya	O
.	O
if	O
this	O
color	O
is	O
not	O
in	O
the	O
range	O
pale	O
green	O
to	O
pale	O
yellow	O
,	O
then	O
the	O
tree	O
immediately	O
predicts	O
that	O
the	O
papaya	O
is	O
not	O
tasty	O
without	O
additional	O
tests	O
.	O
otherwise	O
,	O
the	O
tree	O
turns	O
to	O
examine	O
the	O
softness	O
of	O
the	O
papaya	O
.	O
if	O
the	O
softness	O
level	O
of	O
the	O
papaya	O
is	O
such	O
that	O
it	O
gives	O
slightly	O
to	O
palm	O
pressure	O
,	O
the	O
decision	O
tree	O
predicts	O
that	O
the	O
papaya	O
is	O
tasty	O
.	O
otherwise	O
,	O
the	O
prediction	O
is	O
“	O
not-tasty.	O
”	O
the	O
preceding	O
example	O
underscores	O
one	O
of	O
the	O
main	O
advantages	O
of	O
decision	B
trees	I
–	O
the	O
resulting	O
classiﬁer	B
is	O
very	O
simple	O
to	O
understand	O
and	O
interpret	O
.	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
18.1	O
sample	B
complexity	I
251	O
18.1	O
sample	B
complexity	I
a	O
popular	O
splitting	O
rule	O
at	O
internal	O
nodes	O
of	O
the	O
tree	O
is	O
based	O
on	O
thresholding	O
the	O
value	O
of	O
a	O
single	O
feature	O
.	O
that	O
is	O
,	O
we	O
move	O
to	O
the	O
right	O
or	O
left	O
child	O
of	O
the	O
node	O
on	O
the	O
basis	O
of	O
1	O
[	O
xi	O
<	O
θ	O
]	O
,	O
where	O
i	O
∈	O
[	O
d	O
]	O
is	O
the	O
index	O
of	O
the	O
relevant	O
feature	B
and	O
θ	O
∈	O
r	O
is	O
the	O
threshold	O
.	O
in	O
such	O
cases	O
,	O
we	O
can	O
think	O
of	O
a	O
decision	O
tree	O
as	O
a	O
splitting	O
of	O
the	O
instance	B
space	I
,	O
x	O
=	O
rd	O
,	O
into	O
cells	O
,	O
where	O
each	O
leaf	O
of	O
the	O
tree	O
corresponds	O
to	O
one	O
cell	O
.	O
it	O
follows	O
that	O
a	O
tree	O
with	O
k	O
leaves	O
can	O
shatter	O
a	O
set	B
of	O
k	O
instances	O
.	O
hence	O
,	O
if	O
we	O
allow	O
decision	B
trees	I
of	O
arbitrary	O
size	O
,	O
we	O
obtain	O
a	O
hypothesis	B
class	I
of	O
inﬁnite	O
vc	O
dimension	B
.	O
such	O
an	O
approach	O
can	O
easily	O
lead	O
to	O
overﬁtting	B
.	O
to	O
avoid	O
overﬁtting	B
,	O
we	O
can	O
rely	O
on	O
the	O
minimum	O
description	O
length	O
(	O
mdl	O
)	O
principle	O
described	O
in	O
chapter	O
7	O
,	O
and	O
aim	O
at	O
learning	O
a	O
decision	O
tree	O
that	O
on	O
one	O
hand	O
ﬁts	O
the	O
data	O
well	O
while	O
on	O
the	O
other	O
hand	O
is	O
not	O
too	O
large	O
.	O
for	O
simplicity	O
,	O
we	O
will	O
assume	O
that	O
x	O
=	O
{	O
0	O
,	O
1	O
}	O
d.	O
in	O
other	O
words	O
,	O
each	O
instance	B
is	O
a	O
vector	O
of	O
d	O
bits	O
.	O
in	O
that	O
case	O
,	O
thresholding	O
the	O
value	O
of	O
a	O
single	O
feature	O
corresponds	O
to	O
a	O
splitting	O
rule	O
of	O
the	O
form	O
1	O
[	O
xi=1	O
]	O
for	O
some	O
i	O
=	O
[	O
d	O
]	O
.	O
for	O
instance	B
,	O
we	O
can	O
model	O
the	O
“	O
papaya	O
decision	O
tree	O
”	O
earlier	O
by	O
assuming	O
that	O
a	O
papaya	O
is	O
parameterized	O
by	O
a	O
two-dimensional	O
bit	O
vector	O
x	O
∈	O
{	O
0	O
,	O
1	O
}	O
2	O
,	O
where	O
the	O
bit	O
x1	O
represents	O
whether	O
the	O
color	O
is	O
pale	O
green	O
to	O
pale	O
yellow	O
or	O
not	O
,	O
and	O
the	O
bit	O
x2	O
represents	O
whether	O
the	O
softness	O
is	O
gives	O
slightly	O
to	O
palm	O
pressure	O
or	O
not	O
.	O
with	O
this	O
representation	O
,	O
the	O
node	O
color	O
?	O
can	O
be	O
replaced	O
with	O
1	O
[	O
x1=1	O
]	O
,	O
and	O
the	O
node	O
softness	O
?	O
can	O
be	O
replaced	O
with	O
1	O
[	O
x2=1	O
]	O
.	O
while	O
this	O
is	O
a	O
big	O
simpliﬁcation	O
,	O
the	O
algorithms	O
and	O
analysis	O
we	O
provide	O
in	O
the	O
following	O
can	O
be	O
extended	O
to	O
more	O
general	O
cases	O
.	O
with	O
the	O
aforementioned	O
simplifying	O
assumption	O
,	O
the	O
hypothesis	B
class	I
becomes	O
ﬁnite	O
,	O
but	O
is	O
still	O
very	O
large	O
.	O
in	O
particular	O
,	O
any	O
classiﬁer	B
from	O
{	O
0	O
,	O
1	O
}	O
d	O
to	O
{	O
0	O
,	O
1	O
}	O
can	O
be	O
represented	O
by	O
a	O
decision	O
tree	O
with	O
2d	O
leaves	O
and	O
depth	O
of	O
d	O
+	O
1	O
(	O
see	O
exercise	O
1	O
)	O
.	O
therefore	O
,	O
the	O
vc	O
dimension	B
of	O
the	O
class	O
is	O
2d	O
,	O
which	O
means	O
that	O
the	O
number	O
of	O
examples	O
we	O
need	O
to	O
pac	O
learn	O
the	O
hypothesis	B
class	I
grows	O
with	O
2d	O
.	O
unless	O
d	O
is	O
very	O
small	O
,	O
this	O
is	O
a	O
huge	O
number	O
of	O
examples	O
.	O
to	O
overcome	O
this	O
obstacle	O
,	O
we	O
rely	O
on	O
the	O
mdl	O
scheme	O
described	O
in	O
chapter	O
7.	O
the	O
underlying	O
prior	B
knowledge	I
is	O
that	O
we	O
should	O
prefer	O
smaller	O
trees	O
over	O
larger	O
trees	O
.	O
to	O
formalize	O
this	O
intuition	O
,	O
we	O
ﬁrst	O
need	O
to	O
deﬁne	O
a	O
description	O
language	O
for	O
decision	B
trees	I
,	O
which	O
is	O
preﬁx	O
free	O
and	O
requires	O
fewer	O
bits	O
for	O
smaller	O
decision	B
trees	I
.	O
here	O
is	O
one	O
possible	O
way	O
:	O
a	O
tree	O
with	O
n	O
nodes	O
will	O
be	O
described	O
in	O
n	O
+	O
1	O
blocks	O
,	O
each	O
of	O
size	O
log2	O
(	O
d	O
+	O
3	O
)	O
bits	O
.	O
the	O
ﬁrst	O
n	O
blocks	O
encode	O
the	O
nodes	O
of	O
the	O
tree	O
,	O
in	O
a	O
depth-ﬁrst	O
order	O
(	O
preorder	O
)	O
,	O
and	O
the	O
last	O
block	O
marks	O
the	O
end	O
of	O
the	O
code	O
.	O
each	O
block	O
indicates	O
whether	O
the	O
current	O
node	O
is	O
:	O
•	O
an	O
internal	O
node	O
of	O
the	O
form	O
1	O
[	O
xi=1	O
]	O
for	O
some	O
i	O
∈	O
[	O
d	O
]	O
•	O
a	O
leaf	O
whose	O
value	O
is	O
1	O
•	O
a	O
leaf	O
whose	O
value	O
is	O
0	O
•	O
end	O
of	O
the	O
code	O
252	O
decision	B
trees	I
overall	O
,	O
there	O
are	O
d	O
+	O
3	O
options	O
,	O
hence	O
we	O
need	O
log2	O
(	O
d	O
+	O
3	O
)	O
bits	O
to	O
describe	O
each	O
block	O
.	O
assuming	O
each	O
internal	O
node	O
has	O
two	O
children,1	O
it	O
is	O
not	O
hard	O
to	O
show	O
that	O
this	O
is	O
a	O
preﬁx-free	O
encoding	O
of	O
the	O
tree	O
,	O
and	O
that	O
the	O
description	O
length	O
of	O
a	O
tree	O
with	O
n	O
nodes	O
is	O
(	O
n	O
+	O
1	O
)	O
log2	O
(	O
d	O
+	O
3	O
)	O
.	O
by	O
theorem	O
7.7	O
we	O
have	O
that	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
a	O
sample	O
of	O
size	O
m	O
,	O
for	O
every	O
n	O
and	O
every	O
decision	O
tree	O
h	O
∈	O
h	O
with	O
n	O
nodes	O
it	O
holds	O
that	O
(	O
cid:114	O
)	O
ld	O
(	O
h	O
)	O
≤	O
ls	O
(	O
h	O
)	O
+	O
(	O
n	O
+	O
1	O
)	O
log2	O
(	O
d	O
+	O
3	O
)	O
+	O
log	O
(	O
2/δ	O
)	O
2m	O
.	O
(	O
18.1	O
)	O
this	O
bound	O
performs	O
a	O
tradeoﬀ	O
:	O
on	O
the	O
one	O
hand	O
,	O
we	O
expect	O
larger	O
,	O
more	O
complex	O
decision	B
trees	I
to	O
have	O
a	O
smaller	O
training	O
risk	O
,	O
ls	O
(	O
h	O
)	O
,	O
but	O
the	O
respective	O
value	O
of	O
n	O
will	O
be	O
larger	O
.	O
on	O
the	O
other	O
hand	O
,	O
smaller	O
decision	B
trees	I
will	O
have	O
a	O
smaller	O
value	O
of	O
n	O
,	O
but	O
ls	O
(	O
h	O
)	O
might	O
be	O
larger	O
.	O
our	O
hope	O
(	O
or	O
prior	B
knowledge	I
)	O
is	O
that	O
we	O
can	O
ﬁnd	O
a	O
decision	O
tree	O
with	O
both	O
low	O
empirical	B
risk	I
,	O
ls	O
(	O
h	O
)	O
,	O
and	O
a	O
number	O
of	O
nodes	O
n	O
not	O
too	O
high	O
.	O
our	O
bound	O
indicates	O
that	O
such	O
a	O
tree	O
will	O
have	O
low	O
true	O
risk	O
,	O
ld	O
(	O
h	O
)	O
.	O
18.2	O
decision	O
tree	O
algorithms	O
the	O
bound	O
on	O
ld	O
(	O
h	O
)	O
given	O
in	O
equation	O
(	O
18.1	O
)	O
suggests	O
a	O
learning	O
rule	O
for	O
decision	B
trees	I
–	O
search	O
for	O
a	O
tree	O
that	O
minimizes	O
the	O
right-hand	O
side	O
of	O
equation	O
(	O
18.1	O
)	O
.	O
unfortunately	O
,	O
it	O
turns	O
out	O
that	O
solving	O
this	O
problem	O
is	O
computationally	O
hard.2	O
consequently	O
,	O
practical	O
decision	O
tree	O
learning	O
algorithms	O
are	O
based	O
on	O
heuristics	O
such	O
as	O
a	O
greedy	O
approach	O
,	O
where	O
the	O
tree	O
is	O
constructed	O
gradually	O
,	O
and	O
locally	O
optimal	O
decisions	O
are	O
made	O
at	O
the	O
construction	O
of	O
each	O
node	O
.	O
such	O
algorithms	O
can	O
not	O
guarantee	O
to	O
return	O
the	O
globally	O
optimal	O
decision	O
tree	O
but	O
tend	O
to	O
work	O
reasonably	O
well	O
in	O
practice	O
.	O
a	O
general	O
framework	O
for	O
growing	O
a	O
decision	O
tree	O
is	O
as	O
follows	O
.	O
we	O
start	O
with	O
a	O
tree	O
with	O
a	O
single	O
leaf	O
(	O
the	O
root	O
)	O
and	O
assign	O
this	O
leaf	O
a	O
label	B
according	O
to	O
a	O
majority	O
vote	O
among	O
all	O
labels	O
over	O
the	O
training	B
set	I
.	O
we	O
now	O
perform	O
a	O
series	O
of	O
iterations	O
.	O
on	O
each	O
iteration	O
,	O
we	O
examine	O
the	O
eﬀect	O
of	O
splitting	O
a	O
single	O
leaf	O
.	O
we	O
deﬁne	O
some	O
“	O
gain	B
”	O
measure	O
that	O
quantiﬁes	O
the	O
improvement	O
due	O
to	O
this	O
split	O
.	O
then	O
,	O
among	O
all	O
possible	O
splits	O
,	O
we	O
either	O
choose	O
the	O
one	O
that	O
maximizes	O
the	O
gain	B
and	O
perform	O
it	O
,	O
or	O
choose	O
not	O
to	O
split	O
the	O
leaf	O
at	O
all	O
.	O
in	O
the	O
following	O
we	O
provide	O
a	O
possible	O
implementation	O
.	O
it	O
is	O
based	O
on	O
a	O
popular	O
decision	O
tree	O
algorithm	O
known	O
as	O
“	O
id3	O
”	O
(	O
short	O
for	O
“	O
iterative	O
dichotomizer	O
3	O
”	O
)	O
.	O
we	O
describe	O
the	O
algorithm	O
for	O
the	O
case	O
of	O
binary	O
features	O
,	O
namely	O
,	O
x	O
=	O
{	O
0	O
,	O
1	O
}	O
d	O
,	O
1	O
we	O
may	O
assume	O
this	O
without	O
loss	B
of	O
generality	O
,	O
because	O
if	O
a	O
decision	O
node	O
has	O
only	O
one	O
child	O
,	O
we	O
can	O
replace	O
the	O
node	O
by	O
its	O
child	O
without	O
aﬀecting	O
the	O
predictions	O
of	O
the	O
decision	O
tree	O
.	O
2	O
more	O
precisely	O
,	O
if	O
np	O
(	O
cid:54	O
)	O
=p	O
then	O
no	O
algorithm	O
can	O
solve	O
equation	O
(	O
18.1	O
)	O
in	O
time	O
polynomial	O
in	O
n	O
,	O
d	O
,	O
and	O
m.	O
18.2	O
decision	O
tree	O
algorithms	O
253	O
and	O
therefore	O
all	O
splitting	O
rules	O
are	O
of	O
the	O
form	O
1	O
[	O
xi=1	O
]	O
for	O
some	O
feature	B
i	O
∈	O
[	O
d	O
]	O
.	O
we	O
discuss	O
the	O
case	O
of	O
real	O
valued	O
features	O
in	O
section	O
18.2.3.	O
the	O
algorithm	O
works	O
by	O
recursive	O
calls	O
,	O
with	O
the	O
initial	O
call	O
being	O
id3	O
(	O
s	O
,	O
[	O
d	O
]	O
)	O
,	O
and	O
returns	O
a	O
decision	O
tree	O
.	O
in	O
the	O
pseudocode	O
that	O
follows	O
,	O
we	O
use	O
a	O
call	O
to	O
a	O
procedure	O
gain	B
(	O
s	O
,	O
i	O
)	O
,	O
which	O
receives	O
a	O
training	B
set	I
s	O
and	O
an	O
index	O
i	O
and	O
evaluates	O
the	O
gain	B
of	O
a	O
split	O
of	O
the	O
tree	O
according	O
to	O
the	O
ith	O
feature	B
.	O
we	O
describe	O
several	O
gain	B
measures	O
in	O
section	O
18.2.1.	O
id3	O
(	O
s	O
,	O
a	O
)	O
input	O
:	O
training	B
set	I
s	O
,	O
feature	B
subset	O
a	O
⊆	O
[	O
d	O
]	O
if	O
all	O
examples	O
in	O
s	O
are	O
labeled	O
by	O
1	O
,	O
return	O
a	O
leaf	O
1	O
if	O
all	O
examples	O
in	O
s	O
are	O
labeled	O
by	O
0	O
,	O
return	O
a	O
leaf	O
0	O
if	O
a	O
=	O
∅	O
,	O
return	O
a	O
leaf	O
whose	O
value	O
=	O
majority	O
of	O
labels	O
in	O
s	O
else	O
:	O
let	O
j	O
=	O
argmaxi∈a	O
gain	B
(	O
s	O
,	O
i	O
)	O
if	O
all	O
examples	O
in	O
s	O
have	O
the	O
same	O
label	B
return	O
a	O
leaf	O
whose	O
value	O
=	O
majority	O
of	O
labels	O
in	O
s	O
else	O
let	O
t1	O
be	O
the	O
tree	O
returned	O
by	O
id3	O
(	O
{	O
(	O
x	O
,	O
y	O
)	O
∈	O
s	O
:	O
xj	O
=	O
1	O
}	O
,	O
a	O
\	O
{	O
j	O
}	O
)	O
.	O
let	O
t2	O
be	O
the	O
tree	O
returned	O
by	O
id3	O
(	O
{	O
(	O
x	O
,	O
y	O
)	O
∈	O
s	O
:	O
xj	O
=	O
0	O
}	O
,	O
a	O
\	O
{	O
j	O
}	O
)	O
.	O
return	O
the	O
tree	O
:	O
xj	O
=	O
1	O
?	O
t2	O
t1	O
18.2.1	O
implementations	O
of	O
the	O
gain	B
measure	O
diﬀerent	O
algorithms	O
use	O
diﬀerent	O
implementations	O
of	O
gain	B
(	O
s	O
,	O
i	O
)	O
.	O
here	O
we	O
present	O
three	O
.	O
we	O
use	O
the	O
notation	O
ps	O
[	O
f	O
]	O
to	O
denote	O
the	O
probability	O
that	O
an	O
event	O
holds	O
with	O
respect	O
to	O
the	O
uniform	O
distribution	O
over	O
s.	O
train	O
error	O
:	O
the	O
simplest	O
deﬁnition	O
of	O
gain	B
is	O
the	O
decrease	O
in	O
training	B
error	I
.	O
formally	O
,	O
let	O
c	O
(	O
a	O
)	O
=	O
min	O
{	O
a	O
,	O
1−a	O
}	O
.	O
note	O
that	O
the	O
training	B
error	I
before	O
splitting	O
on	O
feature	B
i	O
is	O
c	O
(	O
ps	O
[	O
y	O
=	O
1	O
]	O
)	O
,	O
since	O
we	O
took	O
a	O
majority	O
vote	O
among	O
labels	O
.	O
similarly	O
,	O
the	O
error	O
after	O
splitting	O
on	O
feature	B
i	O
is	O
p	O
s	O
[	O
xi	O
=	O
1	O
]	O
c	O
(	O
p	O
[	O
y	O
=	O
1|xi	O
=	O
1	O
]	O
)	O
+	O
p	O
[	O
xi	O
=	O
0	O
]	O
c	O
(	O
p	O
s	O
[	O
y	O
=	O
1|xi	O
=	O
0	O
]	O
)	O
.	O
s	O
s	O
therefore	O
,	O
we	O
can	O
deﬁne	O
gain	B
to	O
be	O
the	O
diﬀerence	O
between	O
the	O
two	O
,	O
namely	O
,	O
gain	B
(	O
s	O
,	O
i	O
)	O
:	O
=	O
c	O
(	O
p	O
(	O
cid:17	O
)	O
.	O
−	O
(	O
cid:16	O
)	O
p	O
s	O
s	O
[	O
y	O
=	O
1	O
]	O
)	O
[	O
xi	O
=	O
1	O
]	O
c	O
(	O
p	O
s	O
[	O
y	O
=	O
1|xi	O
=	O
1	O
]	O
)	O
+	O
p	O
[	O
xi	O
=	O
0	O
]	O
c	O
(	O
p	O
[	O
y	O
=	O
1|xi	O
=	O
0	O
]	O
)	O
s	O
s	O
254	O
decision	B
trees	I
information	O
gain	B
:	O
another	O
popular	O
gain	B
measure	O
that	O
is	O
used	O
in	O
the	O
id3	O
and	O
c4.5	O
algorithms	O
of	O
quinlan	O
(	O
1993	O
)	O
is	O
the	O
information	B
gain	I
.	O
the	O
information	B
gain	I
is	O
the	O
diﬀerence	O
between	O
the	O
entropy	B
of	O
the	O
label	B
before	O
and	O
after	O
the	O
split	O
,	O
and	O
is	O
achieved	O
by	O
replacing	O
the	O
function	B
c	O
in	O
the	O
previous	O
expression	O
by	O
the	O
entropy	B
function	O
,	O
c	O
(	O
a	O
)	O
=	O
−a	O
log	O
(	O
a	O
)	O
−	O
(	O
1	O
−	O
a	O
)	O
log	O
(	O
1	O
−	O
a	O
)	O
.	O
gini	O
index	O
:	O
yet	O
another	O
deﬁnition	O
of	O
a	O
gain	B
,	O
which	O
is	O
used	O
by	O
the	O
cart	O
algorithm	O
of	O
breiman	O
,	O
friedman	O
,	O
olshen	O
&	O
stone	O
(	O
1984	O
)	O
,	O
is	O
the	O
gini	O
index	O
,	O
c	O
(	O
a	O
)	O
=	O
2a	O
(	O
1	O
−	O
a	O
)	O
.	O
both	O
the	O
information	B
gain	I
and	O
the	O
gini	O
index	O
are	O
smooth	O
and	O
concave	O
upper	O
bounds	O
of	O
the	O
train	O
error	O
.	O
these	O
properties	O
can	O
be	O
advantageous	O
in	O
some	O
situa-	O
tions	O
(	O
see	O
,	O
for	O
example	O
,	O
kearns	O
&	O
mansour	O
(	O
1996	O
)	O
)	O
.	O
18.2.2	O
pruning	B
the	O
id3	O
algorithm	O
described	O
previously	O
still	O
suﬀers	O
from	O
a	O
big	O
problem	O
:	O
the	O
returned	O
tree	O
will	O
usually	O
be	O
very	O
large	O
.	O
such	O
trees	O
may	O
have	O
low	O
empirical	B
risk	I
,	O
but	O
their	O
true	O
risk	O
will	O
tend	O
to	O
be	O
high	O
–	O
both	O
according	O
to	O
our	O
theoretical	O
analysis	O
,	O
and	O
in	O
practice	O
.	O
one	O
solution	O
is	O
to	O
limit	O
the	O
number	O
of	O
iterations	O
of	O
id3	O
,	O
leading	O
to	O
a	O
tree	O
with	O
a	O
bounded	O
number	O
of	O
nodes	O
.	O
another	O
common	O
solution	O
is	O
to	O
prune	O
the	O
tree	O
after	O
it	O
is	O
built	O
,	O
hoping	O
to	O
reduce	O
it	O
to	O
a	O
much	O
smaller	O
tree	O
,	O
but	O
still	O
with	O
a	O
similar	O
empirical	B
error	I
.	O
theoretically	O
,	O
according	O
to	O
the	O
bound	O
in	O
equation	O
(	O
18.1	O
)	O
,	O
if	O
we	O
can	O
make	O
n	O
much	O
smaller	O
without	O
increasing	O
ls	O
(	O
h	O
)	O
by	O
much	O
,	O
we	O
are	O
likely	O
to	O
get	O
a	O
decision	O
tree	O
with	O
a	O
smaller	O
true	O
risk	O
.	O
usually	O
,	O
the	O
pruning	B
is	O
performed	O
by	O
a	O
bottom-up	O
walk	O
on	O
the	O
tree	O
.	O
each	O
node	O
might	O
be	O
replaced	O
with	O
one	O
of	O
its	O
subtrees	O
or	O
with	O
a	O
leaf	O
,	O
based	O
on	O
some	O
bound	O
or	O
estimate	O
of	O
ld	O
(	O
h	O
)	O
(	O
for	O
example	O
,	O
the	O
bound	O
in	O
equation	O
(	O
18.1	O
)	O
)	O
.	O
a	O
pseudocode	O
of	O
a	O
common	O
template	O
is	O
given	O
in	O
the	O
following	O
.	O
generic	O
tree	O
pruning	B
procedure	O
input	O
:	O
function	B
f	O
(	O
t	O
,	O
m	O
)	O
(	O
bound/estimate	O
for	O
the	O
generalization	B
error	I
of	O
a	O
decision	O
tree	O
t	O
,	O
based	O
on	O
a	O
sample	O
of	O
size	O
m	O
)	O
,	O
tree	O
t	O
.	O
foreach	O
node	O
j	O
in	O
a	O
bottom-up	O
walk	O
on	O
t	O
(	O
from	O
leaves	O
to	O
root	O
)	O
:	O
ﬁnd	O
t	O
(	O
cid:48	O
)	O
which	O
minimizes	O
f	O
(	O
t	O
(	O
cid:48	O
)	O
,	O
m	O
)	O
,	O
where	O
t	O
(	O
cid:48	O
)	O
is	O
any	O
of	O
the	O
following	O
:	O
the	O
current	O
tree	O
after	O
replacing	O
node	O
j	O
with	O
a	O
leaf	O
1.	O
the	O
current	O
tree	O
after	O
replacing	O
node	O
j	O
with	O
a	O
leaf	O
0.	O
the	O
current	O
tree	O
after	O
replacing	O
node	O
j	O
with	O
its	O
left	O
subtree	O
.	O
the	O
current	O
tree	O
after	O
replacing	O
node	O
j	O
with	O
its	O
right	O
subtree	O
.	O
the	O
current	O
tree	O
.	O
let	O
t	O
:	O
=	O
t	O
(	O
cid:48	O
)	O
.	O
18.3	O
random	B
forests	I
255	O
18.2.3	O
threshold-based	O
splitting	O
rules	O
for	O
real-valued	O
features	O
in	O
the	O
previous	O
section	O
we	O
have	O
described	O
an	O
algorithm	O
for	O
growing	O
a	O
decision	O
tree	O
assuming	O
that	O
the	O
features	O
are	O
binary	O
and	O
the	O
splitting	O
rules	O
are	O
of	O
the	O
form	O
1	O
[	O
xi=1	O
]	O
.	O
we	O
now	O
extend	O
this	O
result	O
to	O
the	O
case	O
of	O
real-valued	O
features	O
and	O
threshold-based	O
splitting	O
rules	O
,	O
namely	O
,	O
1	O
[	O
xi	O
<	O
θ	O
]	O
.	O
such	O
splitting	O
rules	O
yield	O
decision	B
stumps	I
,	O
and	O
we	O
have	O
studied	O
them	O
in	O
chapter	O
10.	O
the	O
basic	O
idea	O
is	O
to	O
reduce	O
the	O
problem	O
to	O
the	O
case	O
of	O
binary	O
features	O
as	O
follows	O
.	O
let	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
be	O
the	O
instances	O
of	O
the	O
training	B
set	I
.	O
for	O
each	O
real-valued	O
feature	B
i	O
,	O
sort	O
the	O
instances	O
so	O
that	O
x1	O
,	O
i	O
≤	O
···	O
≤	O
xm	O
,	O
i	O
.	O
deﬁne	O
a	O
set	B
of	O
thresholds	O
θ0	O
,	O
i	O
,	O
.	O
.	O
.	O
,	O
θm+1	O
,	O
i	O
such	O
that	O
θj	O
,	O
i	O
∈	O
(	O
xj	O
,	O
i	O
,	O
xj+1	O
,	O
i	O
)	O
(	O
where	O
we	O
use	O
the	O
convention	O
x0	O
,	O
i	O
=	O
−∞	O
and	O
xm+1	O
,	O
i	O
=	O
∞	O
)	O
.	O
finally	O
,	O
for	O
each	O
i	O
and	O
j	O
we	O
deﬁne	O
the	O
binary	O
feature	B
1	O
[	O
xi	O
<	O
θj	O
,	O
i	O
]	O
.	O
once	O
we	O
have	O
constructed	O
these	O
binary	O
features	O
,	O
we	O
can	O
run	O
the	O
id3	O
procedure	O
described	O
in	O
the	O
previous	O
section	O
.	O
it	O
is	O
easy	O
to	O
verify	O
that	O
for	O
any	O
decision	O
tree	O
with	O
threshold-based	O
splitting	O
rules	O
over	O
the	O
original	O
real-valued	O
features	O
there	O
exists	O
a	O
decision	O
tree	O
over	O
the	O
constructed	O
binary	O
features	O
with	O
the	O
same	O
training	B
error	I
and	O
the	O
same	O
number	O
of	O
nodes	O
.	O
if	O
the	O
original	O
number	O
of	O
real-valued	O
features	O
is	O
d	O
and	O
the	O
number	O
of	O
examples	O
is	O
m	O
,	O
then	O
the	O
number	O
of	O
constructed	O
binary	O
features	O
becomes	O
dm	O
.	O
calculating	O
the	O
gain	B
of	O
each	O
feature	B
might	O
therefore	O
take	O
o	O
(	O
dm2	O
)	O
operations	O
.	O
however	O
,	O
using	O
a	O
more	O
clever	O
implementation	O
,	O
the	O
runtime	O
can	O
be	O
reduced	O
to	O
o	O
(	O
dm	O
log	O
(	O
m	O
)	O
)	O
.	O
the	O
idea	O
is	O
similar	O
to	O
the	O
implementation	O
of	O
erm	O
for	O
decision	B
stumps	I
as	O
described	O
in	O
section	O
10.1.1	O
.	O
18.3	O
random	B
forests	I
as	O
mentioned	O
before	O
,	O
the	O
class	O
of	O
decision	B
trees	I
of	O
arbitrary	O
size	O
has	O
inﬁnite	O
vc	O
dimension	B
.	O
we	O
therefore	O
restricted	O
the	O
size	O
of	O
the	O
decision	O
tree	O
.	O
another	O
way	O
to	O
reduce	O
the	O
danger	O
of	O
overﬁtting	B
is	O
by	O
constructing	O
an	O
ensemble	O
of	O
trees	O
.	O
in	O
particular	O
,	O
in	O
the	O
following	O
we	O
describe	O
the	O
method	O
of	O
random	B
forests	I
,	O
introduced	O
by	O
breiman	O
(	O
2001	O
)	O
.	O
a	O
random	O
forest	O
is	O
a	O
classiﬁer	B
consisting	O
of	O
a	O
collection	O
of	O
decision	B
trees	I
,	O
where	O
each	O
tree	O
is	O
constructed	O
by	O
applying	O
an	O
algorithm	O
a	O
on	O
the	O
training	B
set	I
s	O
and	O
an	O
additional	O
random	O
vector	O
,	O
θ	O
,	O
where	O
θ	O
is	O
sampled	O
i.i.d	O
.	O
from	O
some	O
distribution	O
.	O
the	O
prediction	O
of	O
the	O
random	O
forest	O
is	O
obtained	O
by	O
a	O
majority	O
vote	O
over	O
the	O
predictions	O
of	O
the	O
individual	O
trees	O
.	O
to	O
specify	O
a	O
particular	O
random	O
forest	O
,	O
we	O
need	O
to	O
deﬁne	O
the	O
algorithm	O
a	O
and	O
the	O
distribution	O
over	O
θ.	O
there	O
are	O
many	O
ways	O
to	O
do	O
this	O
and	O
here	O
we	O
describe	O
one	O
particular	O
option	O
.	O
we	O
generate	O
θ	O
as	O
follows	O
.	O
first	O
,	O
we	O
take	O
a	O
random	O
subsample	O
from	O
s	O
with	O
replacements	O
;	O
namely	O
,	O
we	O
sample	O
a	O
new	O
training	B
set	I
s	O
(	O
cid:48	O
)	O
of	O
size	O
m	O
(	O
cid:48	O
)	O
using	O
the	O
uniform	O
distribution	O
over	O
s.	O
second	O
,	O
we	O
construct	O
a	O
sequence	O
i1	O
,	O
i2	O
,	O
.	O
.	O
.	O
,	O
where	O
each	O
it	O
is	O
a	O
subset	O
of	O
[	O
d	O
]	O
of	O
size	O
k	O
,	O
which	O
is	O
generated	O
by	O
sampling	O
uniformly	O
at	O
random	O
elements	O
from	O
[	O
d	O
]	O
.	O
all	O
these	O
random	O
variables	O
form	O
the	O
vector	O
θ.	O
then	O
,	O
256	O
decision	B
trees	I
the	O
algorithm	O
a	O
grows	O
a	O
decision	O
tree	O
(	O
e.g.	O
,	O
using	O
the	O
id3	O
algorithm	O
)	O
based	O
on	O
the	O
sample	O
s	O
(	O
cid:48	O
)	O
,	O
where	O
at	O
each	O
splitting	O
stage	O
of	O
the	O
algorithm	O
,	O
the	O
algorithm	O
is	O
restricted	O
to	O
choosing	O
a	O
feature	B
that	O
maximizes	O
gain	B
from	O
the	O
set	B
it	O
.	O
intuitively	O
,	O
if	O
k	O
is	O
small	O
,	O
this	O
restriction	O
may	O
prevent	O
overﬁtting	B
.	O
18.4	O
summary	O
decision	B
trees	I
are	O
very	O
intuitive	O
predictors	O
.	O
typically	O
,	O
if	O
a	O
human	O
programmer	O
creates	O
a	O
predictor	B
it	O
will	O
look	O
like	O
a	O
decision	O
tree	O
.	O
we	O
have	O
shown	O
that	O
the	O
vc	O
dimension	B
of	O
decision	B
trees	I
with	O
k	O
leaves	O
is	O
k	O
and	O
proposed	O
the	O
mdl	O
paradigm	O
for	O
learning	O
decision	O
trees	O
.	O
the	O
main	O
problem	O
with	O
decision	B
trees	I
is	O
that	O
they	O
are	O
computationally	O
hard	O
to	O
learn	O
;	O
therefore	O
we	O
described	O
several	O
heuristic	O
pro-	O
cedures	O
for	O
training	O
them	O
.	O
18.5	O
bibliographic	O
remarks	O
many	O
algorithms	O
for	O
learning	O
decision	O
trees	O
(	O
such	O
as	O
id3	O
and	O
c4.5	O
)	O
have	O
been	O
derived	O
by	O
quinlan	O
(	O
1986	O
)	O
.	O
the	O
cart	O
algorithm	O
is	O
due	O
to	O
breiman	O
et	O
al	O
.	O
(	O
1984	O
)	O
.	O
random	B
forests	I
were	O
introduced	O
by	O
breiman	O
(	O
2001	O
)	O
.	O
for	O
additional	O
reading	O
we	O
refer	O
the	O
reader	O
to	O
(	O
hastie	O
,	O
tibshirani	O
&	O
friedman	O
2001	O
,	O
rokach	O
2007	O
)	O
.	O
the	O
proof	O
of	O
the	O
hardness	O
of	O
training	O
decision	O
trees	O
is	O
given	O
in	O
hyaﬁl	O
&	O
rivest	O
(	O
1976	O
)	O
.	O
18.6	O
exercises	O
1	O
.	O
1.	O
show	O
that	O
any	O
binary	O
classiﬁer	B
h	O
:	O
{	O
0	O
,	O
1	O
}	O
d	O
(	O
cid:55	O
)	O
→	O
{	O
0	O
,	O
1	O
}	O
can	O
be	O
implemented	O
as	O
a	O
decision	O
tree	O
of	O
height	O
at	O
most	O
d	O
+	O
1	O
,	O
with	O
internal	O
nodes	O
of	O
the	O
form	O
(	O
xi	O
=	O
0	O
?	O
)	O
for	O
some	O
i	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
}	O
.	O
domain	B
{	O
0	O
,	O
1	O
}	O
d	O
is	O
2d	O
.	O
2.	O
conclude	O
that	O
the	O
vc	O
dimension	B
of	O
the	O
class	O
of	O
decision	B
trees	I
over	O
the	O
2	O
.	O
(	O
suboptimality	O
of	O
id3	O
)	O
consider	O
the	O
following	O
training	B
set	I
,	O
where	O
x	O
=	O
{	O
0	O
,	O
1	O
}	O
3	O
and	O
y	O
=	O
{	O
0	O
,	O
1	O
}	O
:	O
(	O
(	O
1	O
,	O
1	O
,	O
1	O
)	O
,	O
1	O
)	O
(	O
(	O
1	O
,	O
0	O
,	O
0	O
)	O
,	O
1	O
)	O
(	O
(	O
1	O
,	O
1	O
,	O
0	O
)	O
,	O
0	O
)	O
(	O
(	O
0	O
,	O
0	O
,	O
1	O
)	O
,	O
0	O
)	O
suppose	O
we	O
wish	O
to	O
use	O
this	O
training	B
set	I
in	O
order	O
to	O
build	O
a	O
decision	O
tree	O
of	O
depth	O
2	O
(	O
i.e.	O
,	O
for	O
each	O
input	O
we	O
are	O
allowed	O
to	O
ask	O
two	O
questions	O
of	O
the	O
form	O
(	O
xi	O
=	O
0	O
?	O
)	O
before	O
deciding	O
on	O
the	O
label	B
)	O
.	O
18.6	O
exercises	O
257	O
1.	O
suppose	O
we	O
run	O
the	O
id3	O
algorithm	O
up	O
to	O
depth	O
2	O
(	O
namely	O
,	O
we	O
pick	O
the	O
root	O
node	O
and	O
its	O
children	O
according	O
to	O
the	O
algorithm	O
,	O
but	O
instead	O
of	O
keeping	O
on	O
with	O
the	O
recursion	O
,	O
we	O
stop	O
and	O
pick	O
leaves	O
according	O
to	O
the	O
majority	O
label	B
in	O
each	O
subtree	O
)	O
.	O
assume	O
that	O
the	O
subroutine	O
used	O
to	O
measure	O
the	O
quality	O
of	O
each	O
feature	B
is	O
based	O
on	O
the	O
entropy	B
function	O
(	O
so	O
we	O
measure	O
the	O
information	B
gain	I
)	O
,	O
and	O
that	O
if	O
two	O
features	O
get	O
the	O
same	O
score	O
,	O
one	O
of	O
them	O
is	O
picked	O
arbitrarily	O
.	O
show	O
that	O
the	O
training	B
error	I
of	O
the	O
resulting	O
decision	O
tree	O
is	O
at	O
least	O
1/4	O
.	O
2.	O
find	O
a	O
decision	O
tree	O
of	O
depth	O
2	O
that	O
attains	O
zero	O
training	B
error	I
.	O
19	O
nearest	O
neighbor	O
nearest	O
neighbor	O
algorithms	O
are	O
among	O
the	O
simplest	O
of	O
all	O
machine	O
learning	O
algorithms	O
.	O
the	O
idea	O
is	O
to	O
memorize	O
the	O
training	B
set	I
and	O
then	O
to	O
predict	O
the	O
label	B
of	O
any	O
new	O
instance	B
on	O
the	O
basis	O
of	O
the	O
labels	O
of	O
its	O
closest	O
neighbors	O
in	O
the	O
training	B
set	I
.	O
the	O
rationale	O
behind	O
such	O
a	O
method	O
is	O
based	O
on	O
the	O
assumption	O
that	O
the	O
features	O
that	O
are	O
used	O
to	O
describe	O
the	O
domain	B
points	O
are	O
relevant	O
to	O
their	O
labelings	O
in	O
a	O
way	O
that	O
makes	O
close-by	O
points	O
likely	O
to	O
have	O
the	O
same	O
label	B
.	O
furthermore	O
,	O
in	O
some	O
situations	O
,	O
even	O
when	O
the	O
training	B
set	I
is	O
immense	O
,	O
ﬁnding	O
a	O
nearest	O
neighbor	O
can	O
be	O
done	O
extremely	O
fast	O
(	O
for	O
example	O
,	O
when	O
the	O
training	B
set	I
is	O
the	O
entire	O
web	O
and	O
distances	O
are	O
based	O
on	O
links	O
)	O
.	O
note	O
that	O
,	O
in	O
contrast	O
with	O
the	O
algorithmic	O
paradigms	O
that	O
we	O
have	O
discussed	O
so	O
far	O
,	O
like	O
erm	O
,	O
srm	O
,	O
mdl	O
,	O
or	O
rlm	O
,	O
that	O
are	O
determined	O
by	O
some	O
hypothesis	B
class	I
,	O
h	O
,	O
the	O
nearest	O
neighbor	O
method	O
ﬁgures	O
out	O
a	O
label	B
on	O
any	O
test	O
point	O
without	O
searching	O
for	O
a	O
predictor	B
within	O
some	O
predeﬁned	O
class	O
of	O
functions	O
.	O
in	O
this	O
chapter	O
we	O
describe	O
nearest	O
neighbor	O
methods	O
for	O
classiﬁcation	O
and	O
regression	B
problems	O
.	O
we	O
analyze	O
their	O
performance	O
for	O
the	O
simple	O
case	O
of	O
binary	O
classiﬁcation	O
and	O
discuss	O
the	O
eﬃciency	O
of	O
implementing	O
these	O
methods	O
.	O
19.1	O
k	O
nearest	O
neighbors	O
throughout	O
the	O
entire	O
chapter	O
we	O
assume	O
that	O
our	O
instance	B
domain	O
,	O
x	O
,	O
is	O
en-	O
dowed	O
with	O
a	O
metric	O
function	B
ρ.	O
that	O
is	O
,	O
ρ	O
:	O
x	O
×x	O
→	O
r	O
is	O
a	O
function	B
that	O
returns	O
the	O
distance	O
between	O
any	O
two	O
elements	O
of	O
x	O
.	O
for	O
example	O
,	O
if	O
x	O
=	O
rd	O
then	O
ρ	O
can	O
be	O
the	O
euclidean	O
distance	O
,	O
ρ	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
(	O
cid:107	O
)	O
x	O
−	O
x	O
(	O
cid:48	O
)	O
(	O
cid:107	O
)	O
=	O
(	O
cid:113	O
)	O
(	O
cid:80	O
)	O
d	O
i=1	O
(	O
xi	O
−	O
x	O
(	O
cid:48	O
)	O
i	O
)	O
2.	O
let	O
s	O
=	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
be	O
a	O
sequence	O
of	O
training	O
examples	O
.	O
for	O
each	O
x	O
∈	O
x	O
,	O
let	O
π1	O
(	O
x	O
)	O
,	O
.	O
.	O
.	O
,	O
πm	O
(	O
x	O
)	O
be	O
a	O
reordering	O
of	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
}	O
according	O
to	O
their	O
distance	O
to	O
x	O
,	O
ρ	O
(	O
x	O
,	O
xi	O
)	O
.	O
that	O
is	O
,	O
for	O
all	O
i	O
<	O
m	O
,	O
ρ	O
(	O
x	O
,	O
xπi	O
(	O
x	O
)	O
)	O
≤	O
ρ	O
(	O
x	O
,	O
xπi+1	O
(	O
x	O
)	O
)	O
.	O
for	O
a	O
number	O
k	O
,	O
the	O
k-nn	O
rule	O
for	O
binary	O
classiﬁcation	O
is	O
deﬁned	O
as	O
follows	O
:	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
19.2	O
analysis	O
259	O
figure	O
19.1	O
an	O
illustration	O
of	O
the	O
decision	O
boundaries	O
of	O
the	O
1-nn	O
rule	O
.	O
the	O
points	O
depicted	O
are	O
the	O
sample	O
points	O
,	O
and	O
the	O
predicted	O
label	B
of	O
any	O
new	O
point	O
will	O
be	O
the	O
label	B
of	O
the	O
sample	O
point	O
in	O
the	O
center	O
of	O
the	O
cell	O
it	O
belongs	O
to	O
.	O
these	O
cells	O
are	O
called	O
a	O
voronoi	O
tessellation	O
of	O
the	O
space	O
.	O
k-nn	O
input	O
:	O
a	O
training	O
sample	O
s	O
=	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
output	O
:	O
for	O
every	O
point	O
x	O
∈	O
x	O
,	O
return	O
the	O
majority	O
label	B
among	O
{	O
yπi	O
(	O
x	O
)	O
:	O
i	O
≤	O
k	O
}	O
when	O
k	O
=	O
1	O
,	O
we	O
have	O
the	O
1-nn	O
rule	O
:	O
hs	O
(	O
x	O
)	O
=	O
yπ1	O
(	O
x	O
)	O
.	O
a	O
geometric	O
illustration	O
of	O
the	O
1-nn	O
rule	O
is	O
given	O
in	O
figure	O
19.1.	O
for	O
regression	B
problems	O
,	O
namely	O
,	O
y	O
=	O
r	O
,	O
one	O
can	O
deﬁne	O
the	O
prediction	O
to	O
be	O
the	O
average	O
target	O
of	O
the	O
k	O
nearest	O
neighbors	O
.	O
that	O
is	O
,	O
hs	O
(	O
x	O
)	O
=	O
1	O
i=1	O
yπi	O
(	O
x	O
)	O
.	O
more	O
generally	O
,	O
for	O
some	O
function	B
φ	O
:	O
(	O
x	O
×y	O
)	O
k	O
→	O
y	O
,	O
the	O
k-nn	O
rule	O
with	O
respect	O
k	O
to	O
φ	O
is	O
:	O
hs	O
(	O
x	O
)	O
=	O
φ	O
(	O
cid:0	O
)	O
(	O
xπ1	O
(	O
x	O
)	O
,	O
yπ1	O
(	O
x	O
)	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xπk	O
(	O
x	O
)	O
,	O
yπk	O
(	O
x	O
)	O
)	O
(	O
cid:1	O
)	O
.	O
(	O
19.1	O
)	O
(	O
cid:80	O
)	O
k	O
it	O
is	O
easy	O
to	O
verify	O
that	O
we	O
can	O
cast	O
the	O
prediction	O
by	O
majority	O
of	O
labels	O
(	O
for	O
classiﬁcation	O
)	O
or	O
by	O
the	O
averaged	O
target	O
(	O
for	O
regression	B
)	O
as	O
in	O
equation	O
(	O
19.1	O
)	O
by	O
an	O
appropriate	O
choice	O
of	O
φ.	O
the	O
generality	O
can	O
lead	O
to	O
other	O
rules	O
;	O
for	O
example	O
,	O
if	O
y	O
=	O
r	O
,	O
we	O
can	O
take	O
a	O
weighted	O
average	O
of	O
the	O
targets	O
according	O
to	O
the	O
distance	O
from	O
x	O
:	O
k	O
(	O
cid:88	O
)	O
(	O
cid:80	O
)	O
k	O
hs	O
(	O
x	O
)	O
=	O
ρ	O
(	O
x	O
,	O
xπi	O
(	O
x	O
)	O
)	O
j=1	O
ρ	O
(	O
x	O
,	O
xπj	O
(	O
x	O
)	O
)	O
yπi	O
(	O
x	O
)	O
.	O
i=1	O
19.2	O
analysis	O
since	O
the	O
nn	O
rules	O
are	O
such	O
natural	O
learning	O
methods	O
,	O
their	O
generalization	O
prop-	O
erties	O
have	O
been	O
extensively	O
studied	O
.	O
most	O
previous	O
results	O
are	O
asymptotic	O
con-	O
sistency	O
results	O
,	O
analyzing	O
the	O
performance	O
of	O
nn	O
rules	O
when	O
the	O
sample	O
size	O
,	O
m	O
,	O
260	O
nearest	O
neighbor	O
goes	O
to	O
inﬁnity	O
,	O
and	O
the	O
rate	O
of	O
convergence	O
depends	O
on	O
the	O
underlying	O
distribu-	O
tion	O
.	O
as	O
we	O
have	O
argued	O
in	O
section	O
7.4	O
,	O
this	O
type	O
of	O
analysis	O
is	O
not	O
satisfactory	O
.	O
one	O
would	O
like	O
to	O
learn	O
from	O
ﬁnite	O
training	O
samples	O
and	O
to	O
understand	O
the	O
gen-	O
eralization	O
performance	O
as	O
a	O
function	B
of	O
the	O
size	O
of	O
such	O
ﬁnite	O
training	O
sets	O
and	O
clear	O
prior	O
assumptions	O
on	O
the	O
data	O
distribution	O
.	O
we	O
therefore	O
provide	O
a	O
ﬁnite-	O
sample	O
analysis	O
of	O
the	O
1-nn	O
rule	O
,	O
showing	O
how	O
the	O
error	O
decreases	O
as	O
a	O
function	B
of	O
m	O
and	O
how	O
it	O
depends	O
on	O
properties	O
of	O
the	O
distribution	O
.	O
we	O
will	O
also	O
explain	O
how	O
the	O
analysis	O
can	O
be	O
generalized	O
to	O
k-nn	O
rules	O
for	O
arbitrary	O
values	O
of	O
k.	O
in	O
particular	O
,	O
the	O
analysis	O
speciﬁes	O
the	O
number	O
of	O
examples	O
required	O
to	O
achieve	O
a	O
true	B
error	I
of	O
2ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
+	O
	O
,	O
where	O
h	O
(	O
cid:63	O
)	O
is	O
the	O
bayes	O
optimal	O
hypothesis	B
,	O
assuming	O
that	O
the	O
labeling	O
rule	O
is	O
“	O
well	O
behaved	O
”	O
(	O
in	O
a	O
sense	O
we	O
will	O
deﬁne	O
later	O
)	O
.	O
19.2.1	O
a	O
generalization	O
bound	O
for	O
the	O
1-nn	O
rule	O
we	O
now	O
analyze	O
the	O
true	B
error	I
of	O
the	O
1-nn	O
rule	O
for	O
binary	O
classiﬁcation	O
with	O
the	O
0-1	B
loss	I
,	O
namely	O
,	O
y	O
=	O
{	O
0	O
,	O
1	O
}	O
and	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
1	O
[	O
h	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=y	O
]	O
.	O
we	O
also	O
assume	O
throughout	O
the	O
analysis	O
that	O
x	O
=	O
[	O
0	O
,	O
1	O
]	O
d	O
and	O
ρ	O
is	O
the	O
euclidean	O
distance	O
.	O
we	O
start	O
by	O
introducing	O
some	O
notation	O
.	O
let	O
d	O
be	O
a	O
distribution	O
over	O
x	O
×	O
y.	O
let	O
dx	O
denote	O
the	O
induced	O
marginal	O
distribution	O
over	O
x	O
and	O
let	O
η	O
:	O
rd	O
→	O
r	O
be	O
the	O
conditional	O
probability1	O
over	O
the	O
labels	O
,	O
that	O
is	O
,	O
η	O
(	O
x	O
)	O
=	O
p	O
[	O
y	O
=	O
1|x	O
]	O
.	O
recall	B
that	O
the	O
bayes	O
optimal	O
rule	O
(	O
that	O
is	O
,	O
the	O
hypothesis	B
that	O
minimizes	O
ld	O
(	O
h	O
)	O
over	O
all	O
functions	O
)	O
is	O
h	O
(	O
cid:63	O
)	O
(	O
x	O
)	O
=	O
1	O
[	O
η	O
(	O
x	O
)	O
>	O
1/2	O
]	O
.	O
we	O
assume	O
that	O
the	O
conditional	O
probability	O
function	B
η	O
is	O
c-lipschitz	O
for	O
some	O
|η	O
(	O
x	O
)	O
−	O
η	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
|	O
≤	O
c	O
(	O
cid:107	O
)	O
x−	O
x	O
(	O
cid:48	O
)	O
(	O
cid:107	O
)	O
.	O
in	O
other	O
words	O
,	O
this	O
c	O
>	O
0	O
:	O
namely	O
,	O
for	O
all	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
∈	O
x	O
,	O
assumption	O
means	O
that	O
if	O
two	O
vectors	O
are	O
close	O
to	O
each	O
other	O
then	O
their	O
labels	O
are	O
likely	O
to	O
be	O
the	O
same	O
.	O
the	O
following	O
lemma	O
applies	O
the	O
lipschitzness	O
of	O
the	O
conditional	O
probability	O
function	B
to	O
upper	O
bound	O
the	O
true	B
error	I
of	O
the	O
1-nn	O
rule	O
as	O
a	O
function	B
of	O
the	O
expected	O
distance	O
between	O
each	O
test	O
instance	B
and	O
its	O
nearest	O
neighbor	O
in	O
the	O
training	B
set	I
.	O
lemma	O
19.1	O
let	O
x	O
=	O
[	O
0	O
,	O
1	O
]	O
d	O
,	O
y	O
=	O
{	O
0	O
,	O
1	O
}	O
,	O
and	O
d	O
be	O
a	O
distribution	O
over	O
x	O
×	O
y	O
for	O
which	O
the	O
conditional	O
probability	O
function	B
,	O
η	O
,	O
is	O
a	O
c-lipschitz	O
function	B
.	O
let	O
s	O
=	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
be	O
an	O
i.i.d	O
.	O
sample	O
and	O
let	O
hs	O
be	O
its	O
corresponding	O
1-nn	O
hypothesis	B
.	O
let	O
h	O
(	O
cid:63	O
)	O
be	O
the	O
bayes	O
optimal	O
rule	O
for	O
η.	O
then	O
,	O
e	O
s∼dm	O
[	O
ld	O
(	O
hs	O
)	O
]	O
≤	O
2	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
+	O
c	O
s∼dm	O
,	O
x∼d	O
[	O
(	O
cid:107	O
)	O
x	O
−	O
xπ1	O
(	O
x	O
)	O
(	O
cid:107	O
)	O
]	O
.	O
e	O
1	O
formally	O
,	O
p	O
[	O
y	O
=	O
1|x	O
]	O
=	O
limδ→0	O
centered	O
around	O
x.	O
d	O
(	O
{	O
(	O
x	O
(	O
cid:48	O
)	O
,1	O
)	O
:	O
x	O
(	O
cid:48	O
)	O
∈b	O
(	O
x	O
,	O
δ	O
)	O
}	O
)	O
d	O
(	O
{	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
y	O
)	O
:	O
x	O
(	O
cid:48	O
)	O
∈b	O
(	O
x	O
,	O
δ	O
)	O
,	O
y∈y	O
}	O
)	O
,	O
where	O
b	O
(	O
x	O
,	O
δ	O
)	O
is	O
a	O
ball	O
of	O
radius	O
δ	O
19.2	O
analysis	O
261	O
proof	O
since	O
ld	O
(	O
hs	O
)	O
=	O
e	O
(	O
x	O
,	O
y	O
)	O
∼d	O
[	O
1	O
[	O
hs	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=y	O
]	O
]	O
,	O
we	O
obtain	O
that	O
es	O
[	O
ld	O
(	O
hs	O
)	O
]	O
is	O
the	O
probability	O
to	O
sample	O
a	O
training	B
set	I
s	O
and	O
an	O
additional	O
example	O
(	O
x	O
,	O
y	O
)	O
,	O
such	O
that	O
the	O
label	B
of	O
π1	O
(	O
x	O
)	O
is	O
diﬀerent	O
from	O
y.	O
in	O
other	O
words	O
,	O
we	O
can	O
ﬁrst	O
sample	O
m	O
unlabeled	O
examples	O
,	O
sx	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
,	O
according	O
to	O
dx	O
,	O
and	O
an	O
additional	O
unlabeled	O
example	O
,	O
x	O
∼	O
dx	O
,	O
then	O
ﬁnd	O
π1	O
(	O
x	O
)	O
to	O
be	O
the	O
nearest	O
neighbor	O
of	O
x	O
in	O
sx	O
,	O
and	O
ﬁnally	O
sample	O
y	O
∼	O
η	O
(	O
x	O
)	O
and	O
yπ1	O
(	O
x	O
)	O
∼	O
η	O
(	O
π1	O
(	O
x	O
)	O
)	O
.	O
it	O
follows	O
that	O
(	O
cid:21	O
)	O
sx∼dmx	O
,	O
x∼dx	O
,	O
y∼η	O
(	O
x	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
∼η	O
(	O
π1	O
(	O
x	O
)	O
)	O
e	O
[	O
ld	O
(	O
hs	O
)	O
]	O
=	O
s	O
[	O
1	O
[	O
y	O
(	O
cid:54	O
)	O
=y	O
(	O
cid:48	O
)	O
]	O
]	O
(	O
cid:20	O
)	O
e	O
=	O
e	O
sx∼dmx	O
,	O
x∼dx	O
y∼η	O
(	O
x	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
∼η	O
(	O
π1	O
(	O
x	O
)	O
)	O
p	O
[	O
y	O
(	O
cid:54	O
)	O
=	O
y	O
(	O
cid:48	O
)	O
]	O
.	O
(	O
19.2	O
)	O
we	O
next	O
upper	O
bound	O
py∼η	O
(	O
x	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
∼η	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
[	O
y	O
(	O
cid:54	O
)	O
=	O
y	O
(	O
cid:48	O
)	O
]	O
for	O
any	O
two	O
domain	B
points	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
:	O
p	O
y∼η	O
(	O
x	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
∼η	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
[	O
y	O
(	O
cid:54	O
)	O
=	O
y	O
(	O
cid:48	O
)	O
]	O
=	O
η	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
1	O
−	O
η	O
(	O
x	O
)	O
)	O
+	O
(	O
1	O
−	O
η	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
)	O
η	O
(	O
x	O
)	O
=	O
(	O
η	O
(	O
x	O
)	O
−	O
η	O
(	O
x	O
)	O
+	O
η	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
)	O
(	O
1	O
−	O
η	O
(	O
x	O
)	O
)	O
+	O
(	O
1	O
−	O
η	O
(	O
x	O
)	O
+	O
η	O
(	O
x	O
)	O
−	O
η	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
)	O
η	O
(	O
x	O
)	O
=	O
2η	O
(	O
x	O
)	O
(	O
1	O
−	O
η	O
(	O
x	O
)	O
)	O
+	O
(	O
η	O
(	O
x	O
)	O
−	O
η	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
)	O
(	O
2η	O
(	O
x	O
)	O
−	O
1	O
)	O
.	O
using	O
|2η	O
(	O
x	O
)	O
−	O
1|	O
≤	O
1	O
and	O
the	O
assumption	O
that	O
η	O
is	O
c-lipschitz	O
,	O
we	O
obtain	O
that	O
the	O
probability	O
is	O
at	O
most	O
:	O
p	O
y∼η	O
(	O
x	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
∼η	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
[	O
y	O
(	O
cid:54	O
)	O
=	O
y	O
(	O
cid:48	O
)	O
]	O
≤	O
2η	O
(	O
x	O
)	O
(	O
1	O
−	O
η	O
(	O
x	O
)	O
)	O
+	O
c	O
(	O
cid:107	O
)	O
x	O
−	O
x	O
(	O
cid:48	O
)	O
(	O
cid:107	O
)	O
.	O
plugging	O
this	O
into	O
equation	O
(	O
19.2	O
)	O
we	O
conclude	O
that	O
[	O
2η	O
(	O
x	O
)	O
(	O
1	O
−	O
η	O
(	O
x	O
)	O
)	O
]	O
+	O
c	O
e	O
[	O
ld	O
(	O
hs	O
)	O
]	O
≤	O
e	O
e	O
s	O
x	O
s	O
,	O
x	O
[	O
(	O
cid:107	O
)	O
x	O
−	O
xπ1	O
(	O
x	O
)	O
(	O
cid:107	O
)	O
]	O
.	O
finally	O
,	O
the	O
error	O
of	O
the	O
bayes	O
optimal	O
classiﬁer	B
is	O
[	O
min	O
{	O
η	O
(	O
x	O
)	O
,	O
1	O
−	O
η	O
(	O
x	O
)	O
}	O
]	O
≥	O
e	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
=	O
e	O
x	O
[	O
η	O
(	O
x	O
)	O
(	O
1	O
−	O
η	O
(	O
x	O
)	O
)	O
]	O
.	O
x	O
combining	O
the	O
preceding	O
two	O
inequalities	O
concludes	O
our	O
proof	O
.	O
the	O
next	O
step	O
is	O
to	O
bound	O
the	O
expected	O
distance	O
between	O
a	O
random	O
x	O
and	O
its	O
closest	O
element	O
in	O
s.	O
we	O
ﬁrst	O
need	O
the	O
following	O
general	O
probability	O
lemma	O
.	O
the	O
lemma	O
bounds	O
the	O
probability	O
weight	O
of	O
subsets	O
that	O
are	O
not	O
hit	O
by	O
a	O
random	O
sample	O
,	O
as	O
a	O
function	B
of	O
the	O
size	O
of	O
that	O
sample	O
.	O
lemma	O
19.2	O
let	O
c1	O
,	O
.	O
.	O
.	O
,	O
cr	O
be	O
a	O
collection	O
of	O
subsets	O
of	O
some	O
domain	B
set	O
,	O
x	O
.	O
let	O
s	O
be	O
a	O
sequence	O
of	O
m	O
points	O
sampled	O
i.i.d	O
.	O
according	O
to	O
some	O
probability	O
distribution	O
,	O
d	O
over	O
x	O
.	O
then	O
,	O
	O
(	O
cid:88	O
)	O
i	O
:	O
ci∩s=∅	O
	O
≤	O
r	O
.	O
m	O
e	O
p	O
[	O
ci	O
]	O
e	O
s∼dm	O
262	O
nearest	O
neighbor	O
proof	O
from	O
the	O
linearity	O
of	O
expectation	O
,	O
we	O
can	O
rewrite	O
:	O
	O
=	O
r	O
(	O
cid:88	O
)	O
i=1	O
(	O
cid:2	O
)	O
1	O
[	O
ci∩s=∅	O
]	O
(	O
cid:3	O
)	O
.	O
p	O
[	O
ci	O
]	O
e	O
s	O
next	O
,	O
for	O
each	O
i	O
we	O
have	O
p	O
[	O
ci	O
]	O
e	O
s	O
i	O
:	O
ci∩s=∅	O
	O
(	O
cid:88	O
)	O
(	O
cid:3	O
)	O
=	O
p	O
(	O
cid:2	O
)	O
1	O
[	O
ci∩s=∅	O
]	O
	O
(	O
cid:88	O
)	O
	O
≤	O
r	O
(	O
cid:88	O
)	O
p	O
[	O
ci	O
]	O
e	O
s	O
s	O
i	O
:	O
ci∩s=∅	O
i=1	O
e	O
s	O
[	O
ci	O
∩	O
s	O
=	O
∅	O
]	O
=	O
(	O
1	O
−	O
p	O
[	O
ci	O
]	O
)	O
m	O
≤	O
e−	O
p	O
[	O
ci	O
]	O
m.	O
combining	O
the	O
preceding	O
two	O
equations	O
we	O
get	O
p	O
[	O
ci	O
]	O
e−	O
p	O
[	O
ci	O
]	O
m	O
≤	O
r	O
max	O
i	O
p	O
[	O
ci	O
]	O
e−	O
p	O
[	O
ci	O
]	O
m.	O
finally	O
,	O
by	O
a	O
standard	O
calculus	O
,	O
maxa	O
ae−ma	O
≤	O
1	O
me	O
and	O
this	O
concludes	O
the	O
proof	O
.	O
equipped	O
with	O
the	O
preceding	O
lemmas	O
we	O
are	O
now	O
ready	O
to	O
state	O
and	O
prove	O
the	O
main	O
result	O
of	O
this	O
section	O
–	O
an	O
upper	O
bound	O
on	O
the	O
expected	O
error	O
of	O
the	O
1-nn	O
learning	O
rule	O
.	O
theorem	O
19.3	O
let	O
x	O
=	O
[	O
0	O
,	O
1	O
]	O
d	O
,	O
y	O
=	O
{	O
0	O
,	O
1	O
}	O
,	O
and	O
d	O
be	O
a	O
distribution	O
over	O
x	O
×y	O
for	O
which	O
the	O
conditional	O
probability	O
function	B
,	O
η	O
,	O
is	O
a	O
c-lipschitz	O
function	B
.	O
let	O
hs	O
denote	O
the	O
result	O
of	O
applying	O
the	O
1-nn	O
rule	O
to	O
a	O
sample	O
s	O
∼	O
dm	O
.	O
then	O
,	O
e	O
s∼dm	O
[	O
ld	O
(	O
hs	O
)	O
]	O
≤	O
2	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
+	O
4	O
c	O
√	O
−	O
1	O
d	O
m	O
d+1	O
.	O
proof	O
fix	O
some	O
	O
=	O
1/t	O
,	O
for	O
some	O
integer	O
t	O
,	O
let	O
r	O
=	O
t	O
d	O
and	O
let	O
c1	O
,	O
.	O
.	O
.	O
,	O
cr	O
be	O
the	O
cover	O
of	O
the	O
set	B
x	O
using	O
boxes	O
of	O
length	O
	O
:	O
namely	O
,	O
for	O
every	O
(	O
α1	O
,	O
.	O
.	O
.	O
,	O
αd	O
)	O
∈	O
[	O
t	O
]	O
d	O
,	O
there	O
exists	O
a	O
set	B
ci	O
of	O
the	O
form	O
{	O
x	O
:	O
∀j	O
,	O
xj	O
∈	O
[	O
(	O
αj	O
−	O
1	O
)	O
/t	O
,	O
αj/t	O
]	O
}	O
.	O
an	O
illustration	O
for	O
d	O
=	O
2	O
,	O
t	O
=	O
5	O
and	O
the	O
set	B
corresponding	O
to	O
α	O
=	O
(	O
2	O
,	O
4	O
)	O
is	O
given	O
in	O
the	O
following	O
.	O
1	O
1	O
therefore	O
,	O
p	O
for	O
each	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
in	O
the	O
same	O
box	O
we	O
have	O
(	O
cid:107	O
)	O
x−x	O
(	O
cid:48	O
)	O
(	O
cid:107	O
)	O
≤	O
√	O
	O
(	O
cid:91	O
)	O
√	O
	O
(	O
cid:91	O
)	O
and	O
by	O
combining	O
lemma	O
19.2	O
with	O
the	O
trivial	O
bound	O
p	O
[	O
(	O
cid:83	O
)	O
d	O
(	O
cid:0	O
)	O
r	O
me	O
+	O
	O
(	O
cid:1	O
)	O
.	O
[	O
(	O
cid:107	O
)	O
x	O
−	O
xπ1	O
(	O
x	O
)	O
(	O
cid:107	O
)	O
]	O
≤	O
e	O
[	O
(	O
cid:107	O
)	O
x	O
−	O
xπ1	O
(	O
x	O
)	O
(	O
cid:107	O
)	O
]	O
≤	O
i	O
:	O
ci∩s=∅	O
d	O
+	O
p	O
get	O
that	O
e	O
x	O
,	O
s	O
√	O
ci	O
s	O
e	O
x	O
,	O
s	O
i	O
:	O
ci∩s	O
(	O
cid:54	O
)	O
=∅	O
d	O
	O
.	O
otherwise	O
,	O
(	O
cid:107	O
)	O
x−x	O
(	O
cid:48	O
)	O
(	O
cid:107	O
)	O
≤	O
√	O
d.	O
	O
	O
√	O
d	O
	O
,	O
ci	O
i	O
:	O
ci∩s	O
(	O
cid:54	O
)	O
=∅	O
ci	O
]	O
≤	O
1	O
we	O
19.2	O
analysis	O
263	O
since	O
the	O
number	O
of	O
boxes	O
is	O
r	O
=	O
(	O
1/	O
)	O
d	O
we	O
get	O
that	O
[	O
(	O
cid:107	O
)	O
x	O
−	O
xπ1	O
(	O
x	O
)	O
(	O
cid:107	O
)	O
]	O
≤	O
e	O
s	O
,	O
x	O
√	O
d	O
(	O
cid:17	O
)	O
.	O
(	O
cid:16	O
)	O
2d	O
−d	O
(	O
cid:16	O
)	O
2d	O
−d	O
m	O
e	O
+	O
	O
√	O
m	O
e	O
+	O
	O
(	O
cid:17	O
)	O
.	O
combining	O
the	O
preceding	O
with	O
lemma	O
19.1	O
we	O
obtain	O
that	O
d	O
finally	O
,	O
setting	O
	O
=	O
2	O
m−1/	O
(	O
d+1	O
)	O
and	O
noting	O
that	O
[	O
ld	O
(	O
hs	O
)	O
]	O
≤	O
2	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
+	O
c	O
e	O
s	O
2d	O
−d	O
m	O
e	O
we	O
conclude	O
our	O
proof	O
.	O
+	O
	O
=	O
2d	O
2−d	O
md/	O
(	O
d+1	O
)	O
m	O
e	O
+	O
2	O
m−1/	O
(	O
d+1	O
)	O
=	O
m−1/	O
(	O
d+1	O
)	O
(	O
1/e	O
+	O
2	O
)	O
≤	O
4m−1/	O
(	O
d+1	O
)	O
the	O
theorem	O
implies	O
that	O
if	O
we	O
ﬁrst	O
ﬁx	O
the	O
data-generating	O
distribution	O
and	O
then	O
let	O
m	O
go	O
to	O
inﬁnity	O
,	O
then	O
the	O
error	O
of	O
the	O
1-nn	O
rule	O
converges	O
to	O
twice	O
the	O
bayes	O
error	O
.	O
the	O
analysis	O
can	O
be	O
generalized	O
to	O
larger	O
values	O
of	O
k	O
,	O
showing	O
that	O
the	O
expected	O
error	O
of	O
the	O
k-nn	O
rule	O
converges	O
to	O
(	O
1	O
+	O
(	O
cid:112	O
)	O
8/k	O
)	O
times	O
the	O
error	O
of	O
the	O
bayes	O
classiﬁer	B
.	O
this	O
is	O
formalized	O
in	O
theorem	O
19.5	O
,	O
whose	O
proof	O
is	O
left	O
as	O
a	O
guided	O
exercise	O
.	O
19.2.2	O
the	O
“	O
curse	B
of	I
dimensionality	I
”	O
the	O
upper	O
bound	O
given	O
in	O
theorem	O
19.3	O
grows	O
with	O
c	O
(	O
the	O
lipschitz	O
coeﬃcient	O
of	O
η	O
)	O
and	O
with	O
d	O
,	O
the	O
euclidean	O
dimension	B
of	O
the	O
domain	B
set	O
x	O
.	O
in	O
fact	O
,	O
it	O
is	O
easy	O
to	O
see	O
that	O
a	O
necessary	O
condition	O
for	O
the	O
last	O
term	O
in	O
theorem	O
19.3	O
to	O
be	O
smaller	O
than	O
	O
is	O
that	O
m	O
≥	O
(	O
4	O
c	O
d/	O
)	O
d+1	O
.	O
that	O
is	O
,	O
the	O
size	O
of	O
the	O
training	B
set	I
should	O
increase	O
exponentially	O
with	O
the	O
dimension	B
.	O
the	O
following	O
theorem	O
tells	O
us	O
that	O
this	O
is	O
not	O
just	O
an	O
artifact	O
of	O
our	O
upper	O
bound	O
,	O
but	O
,	O
for	O
some	O
distributions	O
,	O
this	O
amount	O
of	O
examples	O
is	O
indeed	O
necessary	O
for	O
learning	O
with	O
the	O
nn	O
rule	O
.	O
√	O
theorem	O
19.4	O
for	O
any	O
c	O
>	O
1	O
,	O
and	O
every	O
learning	O
rule	O
,	O
l	O
,	O
there	O
exists	O
a	O
distribution	O
over	O
[	O
0	O
,	O
1	O
]	O
d	O
×	O
{	O
0	O
,	O
1	O
}	O
,	O
such	O
that	O
η	O
(	O
x	O
)	O
is	O
c-lipschitz	O
,	O
the	O
bayes	O
error	O
of	O
the	O
distribution	O
is	O
0	O
,	O
but	O
for	O
sample	O
sizes	O
m	O
≤	O
(	O
c	O
+	O
1	O
)	O
d/2	O
,	O
the	O
true	B
error	I
of	O
the	O
rule	O
l	O
is	O
greater	O
than	O
1/4	O
.	O
proof	O
fix	O
any	O
values	O
of	O
c	O
and	O
d.	O
let	O
gd	O
c	O
be	O
the	O
grid	O
on	O
[	O
0	O
,	O
1	O
]	O
d	O
with	O
distance	O
of	O
1/c	O
between	O
points	O
on	O
the	O
grid	O
.	O
that	O
is	O
,	O
each	O
point	O
on	O
the	O
grid	O
is	O
of	O
the	O
form	O
(	O
a1/c	O
,	O
.	O
.	O
.	O
,	O
ad/c	O
)	O
where	O
ai	O
is	O
in	O
{	O
0	O
,	O
.	O
.	O
.	O
,	O
c−	O
1	O
,	O
c	O
}	O
.	O
note	O
that	O
,	O
since	O
any	O
two	O
distinct	O
c	O
→	O
[	O
0	O
,	O
1	O
]	O
is	O
a	O
points	O
on	O
this	O
grid	O
are	O
at	O
least	O
1/c	O
apart	O
,	O
any	O
function	B
η	O
:	O
gd	O
c-lipschitz	O
function	B
.	O
it	O
follows	O
that	O
the	O
set	B
of	O
all	O
c-lipschitz	O
functions	O
over	O
gd	O
c	O
contains	O
the	O
set	B
of	O
all	O
binary	O
valued	O
functions	O
over	O
that	O
domain	B
.	O
we	O
can	O
therefore	O
invoke	O
the	O
no-free-lunch	B
result	O
(	O
theorem	O
5.1	O
)	O
to	O
obtain	O
a	O
lower	O
bound	O
on	O
the	O
needed	O
sample	O
sizes	O
for	O
learning	O
that	O
class	O
.	O
the	O
number	O
of	O
points	O
on	O
the	O
grid	O
is	O
(	O
c	O
+	O
1	O
)	O
d	O
;	O
hence	O
,	O
if	O
m	O
<	O
(	O
c	O
+	O
1	O
)	O
d/2	O
,	O
theorem	O
5.1	O
implies	O
the	O
lower	O
bound	O
we	O
are	O
after	O
.	O
264	O
nearest	O
neighbor	O
the	O
exponential	O
dependence	O
on	O
the	O
dimension	B
is	O
known	O
as	O
the	O
curse	O
of	O
di-	O
mensionality	O
.	O
as	O
we	O
saw	O
,	O
the	O
1-nn	O
rule	O
might	O
fail	O
if	O
the	O
number	O
of	O
examples	O
is	O
smaller	O
than	O
ω	O
(	O
(	O
c+1	O
)	O
d	O
)	O
.	O
therefore	O
,	O
while	O
the	O
1-nn	O
rule	O
does	O
not	O
restrict	O
itself	O
to	O
a	O
predeﬁned	O
set	B
of	O
hypotheses	O
,	O
it	O
still	O
relies	O
on	O
some	O
prior	B
knowledge	I
–	O
its	O
success	O
depends	O
on	O
the	O
assumption	O
that	O
the	O
dimension	B
and	O
the	O
lipschitz	O
constant	O
of	O
the	O
underlying	O
distribution	O
,	O
η	O
,	O
are	O
not	O
too	O
high	O
.	O
19.3	O
eﬃcient	O
implementation*	O
nearest	O
neighbor	O
is	O
a	O
learning-by-memorization	O
type	O
of	O
rule	O
.	O
it	O
requires	O
the	O
entire	O
training	O
data	O
set	B
to	O
be	O
stored	O
,	O
and	O
at	O
test	O
time	O
,	O
we	O
need	O
to	O
scan	O
the	O
entire	O
data	O
set	B
in	O
order	O
to	O
ﬁnd	O
the	O
neighbors	O
.	O
the	O
time	O
of	O
applying	O
the	O
nn	O
rule	O
is	O
therefore	O
θ	O
(	O
d	O
m	O
)	O
.	O
this	O
leads	O
to	O
expensive	O
computation	O
at	O
test	O
time	O
.	O
when	O
d	O
is	O
small	O
,	O
several	O
results	O
from	O
the	O
ﬁeld	O
of	O
computational	O
geometry	O
have	O
proposed	O
data	O
structures	O
that	O
enable	O
to	O
apply	O
the	O
nn	O
rule	O
in	O
time	O
o	O
(	O
do	O
(	O
1	O
)	O
log	O
(	O
m	O
)	O
)	O
.	O
however	O
,	O
the	O
space	O
required	O
by	O
these	O
data	O
structures	O
is	O
roughly	O
mo	O
(	O
d	O
)	O
,	O
which	O
makes	O
these	O
methods	O
impractical	O
for	O
larger	O
values	O
of	O
d.	O
to	O
overcome	O
this	O
problem	O
,	O
it	O
was	O
suggested	O
to	O
improve	O
the	O
search	O
method	O
by	O
allowing	O
an	O
approximate	O
search	O
.	O
formally	O
,	O
an	O
r-approximate	O
search	O
procedure	O
is	O
guaranteed	O
to	O
retrieve	O
a	O
point	O
within	O
distance	O
of	O
at	O
most	O
r	O
times	O
the	O
distance	O
to	O
the	O
nearest	O
neighbor	O
.	O
three	O
popular	O
approximate	O
algorithms	O
for	O
nn	O
are	O
the	O
kd-tree	O
,	O
balltrees	O
,	O
and	O
locality-sensitive	O
hashing	O
(	O
lsh	O
)	O
.	O
we	O
refer	O
the	O
reader	O
,	O
for	O
example	O
,	O
to	O
(	O
shakhnarovich	O
,	O
darrell	O
&	O
indyk	O
2006	O
)	O
.	O
19.4	O
summary	O
the	O
k-nn	O
rule	O
is	O
a	O
very	O
simple	O
learning	O
algorithm	O
that	O
relies	O
on	O
the	O
assumption	O
that	O
“	O
things	O
that	O
look	O
alike	O
must	O
be	O
alike.	O
”	O
we	O
formalized	O
this	O
intuition	O
using	O
the	O
lipschitzness	O
of	O
the	O
conditional	O
probability	O
.	O
we	O
have	O
shown	O
that	O
with	O
a	O
suf-	O
ﬁciently	O
large	O
training	B
set	I
,	O
the	O
risk	B
of	O
the	O
1-nn	O
is	O
upper	O
bounded	O
by	O
twice	O
the	O
risk	B
of	O
the	O
bayes	O
optimal	O
rule	O
.	O
we	O
have	O
also	O
derived	O
a	O
lower	O
bound	O
that	O
shows	O
the	O
“	O
curse	B
of	I
dimensionality	I
”	O
–	O
the	O
required	O
sample	O
size	O
might	O
increase	O
expo-	O
nentially	O
with	O
the	O
dimension	B
.	O
as	O
a	O
result	O
,	O
nn	O
is	O
usually	O
performed	O
in	O
practice	O
after	O
a	O
dimensionality	B
reduction	I
preprocessing	O
step	O
.	O
we	O
discuss	O
dimensionality	B
reduction	I
techniques	O
later	O
on	O
in	O
chapter	O
23	O
.	O
19.5	O
bibliographic	O
remarks	O
cover	O
&	O
hart	O
(	O
1967	O
)	O
gave	O
the	O
ﬁrst	O
analysis	O
of	O
1-nn	O
,	O
showing	O
that	O
its	O
risk	B
con-	O
verges	O
to	O
twice	O
the	O
bayes	O
optimal	O
error	O
under	O
mild	O
conditions	O
.	O
following	O
a	O
lemma	O
due	O
to	O
stone	O
(	O
1977	O
)	O
,	O
devroye	O
&	O
gy¨orﬁ	O
(	O
1985	O
)	O
have	O
shown	O
that	O
the	O
k-nn	O
rule	O
19.6	O
exercises	O
265	O
is	O
consistent	O
(	O
with	O
respect	O
to	O
the	O
hypothesis	B
class	I
of	O
all	O
functions	O
from	O
rd	O
to	O
{	O
0	O
,	O
1	O
}	O
)	O
.	O
a	O
good	O
presentation	O
of	O
the	O
analysis	O
is	O
given	O
in	O
the	O
book	O
of	O
devroye	O
et	O
al	O
.	O
(	O
1996	O
)	O
.	O
here	O
,	O
we	O
give	O
a	O
ﬁnite	O
sample	O
guarantee	O
that	O
explicitly	O
underscores	O
the	O
prior	O
assumption	O
on	O
the	O
distribution	O
.	O
see	O
section	O
7.4	O
for	O
a	O
discussion	O
on	O
con-	O
sistency	O
results	O
.	O
finally	O
,	O
gottlieb	O
,	O
kontorovich	O
&	O
krauthgamer	O
(	O
2010	O
)	O
derived	O
another	O
ﬁnite	O
sample	O
bound	O
for	O
nn	O
that	O
is	O
more	O
similar	O
to	O
vc	O
bounds	O
.	O
19.6	O
exercises	O
in	O
this	O
exercise	O
we	O
will	O
prove	O
the	O
following	O
theorem	O
for	O
the	O
k-nn	O
rule	O
.	O
theorem	O
19.5	O
let	O
x	O
=	O
[	O
0	O
,	O
1	O
]	O
d	O
,	O
y	O
=	O
{	O
0	O
,	O
1	O
}	O
,	O
and	O
d	O
be	O
a	O
distribution	O
over	O
x	O
×y	O
for	O
which	O
the	O
conditional	O
probability	O
function	B
,	O
η	O
,	O
is	O
a	O
c-lipschitz	O
function	B
.	O
let	O
hs	O
denote	O
the	O
result	O
of	O
applying	O
the	O
k-nn	O
rule	O
to	O
a	O
sample	O
s	O
∼	O
dm	O
,	O
where	O
k	O
≥	O
10.	O
let	O
h	O
(	O
cid:63	O
)	O
be	O
the	O
bayes	O
optimal	O
hypothesis	B
.	O
then	O
,	O
(	O
cid:32	O
)	O
(	O
cid:33	O
)	O
(	O
cid:114	O
)	O
8	O
k	O
[	O
ld	O
(	O
hs	O
)	O
]	O
≤	O
e	O
s	O
1	O
+	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
+	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
√	O
6	O
c	O
d	O
+	O
k	O
m−1/	O
(	O
d+1	O
)	O
.	O
1.	O
prove	O
the	O
following	O
lemma	O
.	O
lemma	O
19.6	O
let	O
c1	O
,	O
.	O
.	O
.	O
,	O
cr	O
be	O
a	O
collection	O
of	O
subsets	O
of	O
some	O
domain	B
set	O
,	O
x	O
.	O
let	O
s	O
be	O
a	O
sequence	O
of	O
m	O
points	O
sampled	O
i.i.d	O
.	O
according	O
to	O
some	O
probability	O
distribution	O
,	O
d	O
over	O
x	O
.	O
then	O
,	O
for	O
every	O
k	O
≥	O
2	O
,	O
e	O
s∼dm	O
i	O
:	O
|ci∩s|	O
<	O
k	O
	O
(	O
cid:88	O
)	O
	O
=	O
p	O
[	O
ci	O
]	O
p	O
[	O
ci	O
]	O
.	O
	O
≤	O
2rk	O
r	O
(	O
cid:88	O
)	O
p	O
[	O
ci	O
]	O
p	O
m	O
s	O
i=1	O
hints	O
:	O
•	O
show	O
that	O
e	O
s	O
	O
(	O
cid:88	O
)	O
i	O
:	O
|ci∩s|	O
<	O
k	O
[	O
|ci	O
∩	O
s|	O
<	O
k	O
]	O
.	O
•	O
fix	O
some	O
i	O
and	O
suppose	O
that	O
k	O
<	O
p	O
[	O
ci	O
]	O
m/2	O
.	O
use	O
chernoﬀ	O
’	O
s	O
bound	O
to	O
show	O
that	O
p	O
s	O
[	O
|ci	O
∩	O
s|	O
<	O
k	O
]	O
≤	O
p	O
[	O
|ci	O
∩	O
s|	O
<	O
p	O
[	O
ci	O
]	O
m/2	O
]	O
≤	O
e−	O
p	O
[	O
ci	O
]	O
m/8	O
.	O
s	O
•	O
use	O
the	O
inequality	O
maxa	O
ae−ma	O
≤	O
1	O
me	O
to	O
show	O
that	O
for	O
such	O
i	O
we	O
have	O
p	O
[	O
ci	O
]	O
p	O
s	O
[	O
|ci	O
∩	O
s|	O
<	O
k	O
]	O
≤	O
p	O
[	O
ci	O
]	O
e−	O
p	O
[	O
ci	O
]	O
m/8	O
≤	O
8	O
me	O
.	O
•	O
conclude	O
the	O
proof	O
by	O
using	O
the	O
fact	O
that	O
for	O
the	O
case	O
k	O
≥	O
p	O
[	O
ci	O
]	O
m/2	O
we	O
clearly	O
have	O
:	O
p	O
[	O
ci	O
]	O
p	O
s	O
[	O
|ci	O
∩	O
s|	O
<	O
k	O
]	O
≤	O
p	O
[	O
ci	O
]	O
≤	O
2k	O
m	O
.	O
266	O
nearest	O
neighbor	O
2.	O
we	O
use	O
the	O
notation	O
y	O
∼	O
p	O
as	O
a	O
shorthand	O
for	O
“	O
y	O
is	O
a	O
bernoulli	O
random	O
variable	O
with	O
expected	O
value	O
p.	O
”	O
prove	O
the	O
following	O
lemma	O
:	O
lemma	O
19.7	O
let	O
k	O
≥	O
10	O
and	O
let	O
z1	O
,	O
.	O
.	O
.	O
,	O
zk	O
be	O
independent	O
bernoulli	O
random	O
variables	O
with	O
p	O
[	O
zi	O
=	O
1	O
]	O
=	O
pi	O
.	O
denote	O
p	O
=	O
1	O
i=1	O
zi	O
.	O
show	O
that	O
(	O
cid:80	O
)	O
k	O
(	O
cid:80	O
)	O
i	O
pi	O
and	O
p	O
(	O
cid:48	O
)	O
=	O
1	O
(	O
cid:33	O
)	O
(	O
cid:114	O
)	O
8	O
(	O
cid:32	O
)	O
k	O
k	O
e	O
z1	O
,	O
...	O
,	O
zk	O
[	O
y	O
(	O
cid:54	O
)	O
=	O
1	O
[	O
p	O
(	O
cid:48	O
)	O
>	O
1/2	O
]	O
]	O
≤	O
p	O
y∼p	O
1	O
+	O
k	O
[	O
y	O
(	O
cid:54	O
)	O
=	O
1	O
[	O
p	O
>	O
1/2	O
]	O
]	O
.	O
p	O
y∼p	O
hints	O
:	O
w.l.o.g	O
.	O
assume	O
that	O
p	O
≤	O
1/2	O
.	O
then	O
,	O
py∼p	O
[	O
y	O
(	O
cid:54	O
)	O
=	O
1	O
[	O
p	O
>	O
1/2	O
]	O
]	O
=	O
p.	O
let	O
y	O
(	O
cid:48	O
)	O
=	O
1	O
[	O
p	O
(	O
cid:48	O
)	O
>	O
1/2	O
]	O
.	O
•	O
show	O
that	O
e	O
p	O
y∼p	O
z1	O
,	O
...	O
,	O
zk	O
[	O
y	O
(	O
cid:54	O
)	O
=	O
y	O
(	O
cid:48	O
)	O
]	O
−	O
p	O
=	O
p	O
[	O
p	O
(	O
cid:48	O
)	O
>	O
1/2	O
]	O
(	O
1	O
−	O
2p	O
)	O
.	O
z1	O
,	O
...	O
,	O
zk	O
•	O
use	O
chernoﬀ	O
’	O
s	O
bound	O
(	O
lemma	O
b.3	O
)	O
to	O
show	O
that	O
where	O
p	O
[	O
p	O
(	O
cid:48	O
)	O
>	O
1/2	O
]	O
≤	O
e	O
−k	O
p	O
h	O
(	O
1	O
2p−1	O
)	O
,	O
h	O
(	O
a	O
)	O
=	O
(	O
1	O
+	O
a	O
)	O
log	O
(	O
1	O
+	O
a	O
)	O
−	O
a	O
.	O
•	O
to	O
conclude	O
the	O
proof	O
of	O
the	O
lemma	O
,	O
you	O
can	O
rely	O
on	O
the	O
following	O
inequality	O
(	O
without	O
proving	O
it	O
)	O
:	O
for	O
every	O
p	O
∈	O
[	O
0	O
,	O
1/2	O
]	O
and	O
k	O
≥	O
10	O
:	O
(	O
1	O
−	O
2p	O
)	O
e−k	O
p	O
+	O
k	O
2	O
(	O
log	O
(	O
2p	O
)	O
+1	O
)	O
≤	O
3.	O
fix	O
some	O
p	O
,	O
p	O
(	O
cid:48	O
)	O
∈	O
[	O
0	O
,	O
1	O
]	O
and	O
y	O
(	O
cid:48	O
)	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
show	O
that	O
p	O
y∼p	O
[	O
y	O
(	O
cid:54	O
)	O
=	O
y	O
(	O
cid:48	O
)	O
]	O
≤	O
p	O
y∼p	O
(	O
cid:48	O
)	O
[	O
y	O
(	O
cid:54	O
)	O
=	O
y	O
(	O
cid:48	O
)	O
]	O
+	O
|p	O
−	O
p	O
(	O
cid:48	O
)	O
|	O
.	O
(	O
cid:114	O
)	O
8	O
p.	O
k	O
4.	O
conclude	O
the	O
proof	O
of	O
the	O
theorem	O
according	O
to	O
the	O
following	O
steps	O
:	O
•	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
19.3	O
,	O
six	O
some	O
	O
>	O
0	O
and	O
let	O
c1	O
,	O
.	O
.	O
.	O
,	O
cr	O
be	O
the	O
cover	O
of	O
the	O
set	B
x	O
using	O
boxes	O
of	O
length	O
	O
.	O
for	O
each	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
in	O
the	O
same	O
box	O
we	O
have	O
(	O
cid:107	O
)	O
x	O
−	O
x	O
(	O
cid:48	O
)	O
(	O
cid:107	O
)	O
≤	O
√	O
d.	O
show	O
that	O
d	O
	O
.	O
otherwise	O
,	O
(	O
cid:107	O
)	O
x	O
−	O
x	O
(	O
cid:48	O
)	O
(	O
cid:107	O
)	O
≤	O
2	O
√	O
	O
p	O
[	O
ci	O
]	O
	O
(	O
cid:88	O
)	O
(	O
cid:104	O
)	O
[	O
ld	O
(	O
hs	O
)	O
]	O
≤	O
e	O
e	O
s	O
s	O
+	O
max	O
i	O
p	O
s	O
,	O
(	O
x	O
,	O
y	O
)	O
i	O
:	O
|ci∩s|	O
<	O
k	O
hs	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
y	O
|	O
∀j	O
∈	O
[	O
k	O
]	O
,	O
(	O
cid:107	O
)	O
x	O
−	O
xπj	O
(	O
x	O
)	O
(	O
cid:107	O
)	O
≤	O
	O
(	O
cid:105	O
)	O
√	O
d	O
.	O
(	O
19.3	O
)	O
•	O
bound	O
the	O
ﬁrst	O
summand	O
using	O
lemma	O
19.6	O
.	O
•	O
to	O
bound	O
the	O
second	O
summand	O
,	O
let	O
us	O
ﬁx	O
s|x	O
and	O
x	O
such	O
that	O
all	O
the	O
k	O
neighbors	O
of	O
x	O
in	O
s|x	O
are	O
at	O
distance	O
of	O
at	O
most	O
	O
d	O
from	O
x.	O
w.l.o.g	O
assume	O
that	O
the	O
k	O
nn	O
are	O
x1	O
,	O
.	O
.	O
.	O
,	O
xk	O
.	O
denote	O
pi	O
=	O
η	O
(	O
xi	O
)	O
and	O
let	O
p	O
=	O
1	O
k	O
(	O
cid:80	O
)	O
√	O
i	O
pi	O
.	O
use	O
exercise	O
3	O
to	O
show	O
that	O
[	O
hs	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
y	O
]	O
≤	O
e	O
e	O
p	O
y1	O
,	O
...	O
,	O
yj	O
y∼η	O
(	O
x	O
)	O
y1	O
,	O
...	O
,	O
yj	O
p	O
y∼p	O
[	O
hs	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
y	O
]	O
+	O
|p	O
−	O
η	O
(	O
x	O
)	O
|	O
.	O
19.6	O
exercises	O
267	O
w.l.o.g	O
.	O
assume	O
that	O
p	O
≤	O
1/2	O
.	O
now	O
use	O
lemma	O
19.7	O
to	O
show	O
that	O
(	O
cid:32	O
)	O
1	O
+	O
(	O
cid:33	O
)	O
(	O
cid:114	O
)	O
8	O
k	O
[	O
1	O
[	O
p	O
>	O
1/2	O
]	O
(	O
cid:54	O
)	O
=	O
y	O
]	O
.	O
p	O
y∼p	O
p	O
y1	O
,	O
...	O
,	O
yj	O
p	O
y∼p	O
[	O
hs	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
y	O
]	O
≤	O
•	O
show	O
that	O
p	O
y∼p	O
[	O
1	O
[	O
p	O
>	O
1/2	O
]	O
(	O
cid:54	O
)	O
=	O
y	O
]	O
=	O
p	O
=	O
min	O
{	O
p	O
,	O
1−	O
p	O
}	O
≤	O
min	O
{	O
η	O
(	O
x	O
)	O
,	O
1−	O
η	O
(	O
x	O
)	O
}	O
+|p−	O
η	O
(	O
x	O
)	O
|	O
.	O
•	O
combine	O
all	O
the	O
preceding	O
to	O
obtain	O
that	O
the	O
second	O
summand	O
in	O
equa-	O
tion	O
(	O
19.3	O
)	O
is	O
bounded	O
by	O
(	O
cid:32	O
)	O
1	O
+	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
+	O
3	O
c	O
	O
√	O
d.	O
k	O
(	O
cid:33	O
)	O
(	O
cid:114	O
)	O
8	O
(	O
cid:33	O
)	O
(	O
cid:114	O
)	O
8	O
m−1/	O
(	O
d+1	O
)	O
≤	O
(	O
cid:16	O
)	O
k	O
√	O
d	O
+	O
2	O
(	O
2/	O
)	O
d	O
k	O
.	O
m	O
(	O
cid:17	O
)	O
√	O
6c	O
d	O
+	O
k	O
m−1/	O
(	O
d+1	O
)	O
•	O
use	O
r	O
=	O
(	O
2/	O
)	O
d	O
to	O
obtain	O
that	O
:	O
(	O
cid:32	O
)	O
[	O
ld	O
(	O
hs	O
)	O
]	O
≤	O
e	O
s	O
1	O
+	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
+	O
3	O
c	O
	O
set	B
	O
=	O
2m−1/	O
(	O
d+1	O
)	O
and	O
use	O
2k	O
e	O
6	O
c	O
m−1/	O
(	O
d+1	O
)	O
d	O
+	O
√	O
to	O
conclude	O
the	O
proof	O
.	O
20	O
neural	B
networks	I
an	O
artiﬁcial	O
neural	O
network	O
is	O
a	O
model	O
of	O
computation	O
inspired	O
by	O
the	O
structure	O
of	O
neural	B
networks	I
in	O
the	O
brain	O
.	O
in	O
simpliﬁed	O
models	O
of	O
the	O
brain	O
,	O
it	O
consists	O
of	O
a	O
large	O
number	O
of	O
basic	O
computing	O
devices	O
(	O
neurons	O
)	O
that	O
are	O
connected	O
to	O
each	O
other	O
in	O
a	O
complex	O
communication	O
network	O
,	O
through	O
which	O
the	O
brain	O
is	O
able	O
to	O
carry	O
out	O
highly	O
complex	O
computations	O
.	O
artiﬁcial	O
neural	B
networks	I
are	O
formal	O
computation	O
constructs	O
that	O
are	O
modeled	O
after	O
this	O
computation	O
paradigm	O
.	O
learning	O
with	O
neural	B
networks	I
was	O
proposed	O
in	O
the	O
mid-20th	O
century	O
.	O
it	O
yields	O
an	O
eﬀective	O
learning	O
paradigm	O
and	O
has	O
recently	O
been	O
shown	O
to	O
achieve	O
cutting-	O
edge	O
performance	O
on	O
several	O
learning	O
tasks	O
.	O
a	O
neural	O
network	O
can	O
be	O
described	O
as	O
a	O
directed	O
graph	O
whose	O
nodes	O
correspond	O
to	O
neurons	O
and	O
edges	O
correspond	O
to	O
links	O
between	O
them	O
.	O
each	O
neuron	O
receives	O
as	O
input	O
a	O
weighted	O
sum	O
of	O
the	O
outputs	O
of	O
the	O
neurons	O
connected	O
to	O
its	O
incoming	O
edges	O
.	O
we	O
focus	O
on	O
feedforward	B
networks	I
in	O
which	O
the	O
underlying	O
graph	O
does	O
not	O
contain	O
cycles	O
.	O
in	O
the	O
context	O
of	O
learning	O
,	O
we	O
can	O
deﬁne	O
a	O
hypothesis	B
class	I
consisting	O
of	O
neural	O
network	O
predictors	O
,	O
where	O
all	O
the	O
hypotheses	O
share	O
the	O
underlying	O
graph	O
struc-	O
ture	O
of	O
the	O
network	O
and	O
diﬀer	O
in	O
the	O
weights	O
over	O
edges	O
.	O
as	O
we	O
will	O
show	O
in	O
section	O
20.3	O
,	O
every	O
predictor	B
over	O
n	O
variables	O
that	O
can	O
be	O
implemented	O
in	O
time	O
t	O
(	O
n	O
)	O
can	O
also	O
be	O
expressed	O
as	O
a	O
neural	O
network	O
predictor	B
of	O
size	O
o	O
(	O
t	O
(	O
n	O
)	O
2	O
)	O
,	O
where	O
the	O
size	O
of	O
the	O
network	O
is	O
the	O
number	O
of	O
nodes	O
in	O
it	O
.	O
it	O
follows	O
that	O
the	O
family	O
of	O
hypothesis	B
classes	O
of	O
neural	B
networks	I
of	O
polynomial	O
size	O
can	O
suﬃce	O
for	O
all	O
practical	O
learning	O
tasks	O
,	O
in	O
which	O
our	O
goal	O
is	O
to	O
learn	O
predictors	O
which	O
can	O
be	O
implemented	O
eﬃciently	O
.	O
furthermore	O
,	O
in	O
section	O
20.4	O
we	O
will	O
show	O
that	O
the	O
sam-	O
ple	O
complexity	O
of	O
learning	O
such	O
hypothesis	B
classes	O
is	O
also	O
bounded	O
in	O
terms	O
of	O
the	O
size	O
of	O
the	O
network	O
.	O
hence	O
,	O
it	O
seems	O
that	O
this	O
is	O
the	O
ultimate	O
learning	O
paradigm	O
we	O
would	O
want	O
to	O
adapt	O
,	O
in	O
the	O
sense	O
that	O
it	O
both	O
has	O
a	O
polynomial	O
sample	O
com-	O
plexity	O
and	O
has	O
the	O
minimal	O
approximation	B
error	I
among	O
all	O
hypothesis	B
classes	O
consisting	O
of	O
eﬃciently	O
implementable	O
predictors	O
.	O
the	O
caveat	O
is	O
that	O
the	O
problem	O
of	O
training	O
such	O
hypothesis	B
classes	O
of	O
neural	O
net-	O
work	O
predictors	O
is	O
computationally	O
hard	O
.	O
this	O
will	O
be	O
formalized	O
in	O
section	O
20.5.	O
a	O
widely	O
used	O
heuristic	O
for	O
training	O
neural	O
networks	O
relies	O
on	O
the	O
sgd	O
frame-	O
work	O
we	O
studied	O
in	O
chapter	O
14.	O
there	O
,	O
we	O
have	O
shown	O
that	O
sgd	O
is	O
a	O
successful	O
learner	O
if	O
the	O
loss	B
function	I
is	O
convex	B
.	O
in	O
neural	B
networks	I
,	O
the	O
loss	B
function	I
is	O
highly	O
nonconvex	O
.	O
nevertheless	O
,	O
we	O
can	O
still	O
implement	O
the	O
sgd	O
algorithm	O
and	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
20.1	O
feedforward	O
neural	O
networks	O
269	O
hope	O
it	O
will	O
ﬁnd	O
a	O
reasonable	O
solution	O
(	O
as	O
happens	O
to	O
be	O
the	O
case	O
in	O
several	O
practical	O
tasks	O
)	O
.	O
in	O
section	O
20.6	O
we	O
describe	O
how	O
to	O
implement	O
sgd	O
for	O
neural	B
networks	I
.	O
in	O
particular	O
,	O
the	O
most	O
complicated	O
operation	O
is	O
the	O
calculation	O
of	O
the	O
gradient	B
of	O
the	O
loss	B
function	I
with	O
respect	O
to	O
the	O
parameters	O
of	O
the	O
network	O
.	O
we	O
present	O
the	O
backpropagation	B
algorithm	O
that	O
eﬃciently	O
calculates	O
the	O
gradient	B
.	O
20.1	O
feedforward	O
neural	O
networks	O
the	O
idea	O
behind	O
neural	B
networks	I
is	O
that	O
many	O
neurons	O
can	O
be	O
joined	O
together	O
by	O
communication	O
links	O
to	O
carry	O
out	O
complex	O
computations	O
.	O
it	O
is	O
common	O
to	O
describe	O
the	O
structure	O
of	O
a	O
neural	O
network	O
as	O
a	O
graph	O
whose	O
nodes	O
are	O
the	O
neurons	O
and	O
each	O
(	O
directed	O
)	O
edge	O
in	O
the	O
graph	O
links	O
the	O
output	O
of	O
some	O
neuron	O
to	O
the	O
input	O
of	O
another	O
neuron	O
.	O
we	O
will	O
restrict	O
our	O
attention	O
to	O
feedforward	O
network	O
structures	O
in	O
which	O
the	O
underlying	O
graph	O
does	O
not	O
contain	O
cycles	O
.	O
a	O
feedforward	O
neural	O
network	O
is	O
described	O
by	O
a	O
directed	O
acyclic	O
graph	O
,	O
g	O
=	O
(	O
v	O
,	O
e	O
)	O
,	O
and	O
a	O
weight	O
function	B
over	O
the	O
edges	O
,	O
w	O
:	O
e	O
→	O
r.	O
nodes	O
of	O
the	O
graph	O
correspond	O
to	O
neurons	O
.	O
each	O
single	O
neuron	O
is	O
modeled	O
as	O
a	O
simple	O
scalar	O
func-	O
tion	O
,	O
σ	O
:	O
r	O
→	O
r.	O
we	O
will	O
focus	O
on	O
three	O
possible	O
functions	O
for	O
σ	O
:	O
the	O
sign	O
function	B
,	O
σ	O
(	O
a	O
)	O
=	O
sign	O
(	O
a	O
)	O
,	O
the	O
threshold	O
function	B
,	O
σ	O
(	O
a	O
)	O
=	O
1	O
[	O
a	O
>	O
0	O
]	O
,	O
and	O
the	O
sig-	O
moid	O
function	B
,	O
σ	O
(	O
a	O
)	O
=	O
1/	O
(	O
1	O
+	O
exp	O
(	O
−a	O
)	O
)	O
,	O
which	O
is	O
a	O
smooth	O
approximation	O
to	O
the	O
threshold	O
function	B
.	O
we	O
call	O
σ	O
the	O
“	O
activation	O
”	O
function	B
of	O
the	O
neuron	O
.	O
each	O
edge	O
in	O
the	O
graph	O
links	O
the	O
output	O
of	O
some	O
neuron	O
to	O
the	O
input	O
of	O
another	O
neuron	O
.	O
the	O
input	O
of	O
a	O
neuron	O
is	O
obtained	O
by	O
taking	O
a	O
weighted	O
sum	O
of	O
the	O
outputs	O
of	O
all	O
the	O
neurons	O
connected	O
to	O
it	O
,	O
where	O
the	O
weighting	O
is	O
according	O
to	O
w.	O
to	O
simplify	O
the	O
description	O
of	O
the	O
calculation	O
performed	O
by	O
the	O
network	O
,	O
we	O
further	O
assume	O
that	O
the	O
network	O
is	O
organized	O
in	O
layers	O
.	O
that	O
is	O
,	O
the	O
set	B
of	O
nodes	O
can	O
be	O
decomposed	O
into	O
a	O
union	O
of	O
(	O
nonempty	O
)	O
disjoint	O
subsets	O
,	O
v	O
=	O
·∪t	O
t=0vt	O
,	O
such	O
that	O
every	O
edge	O
in	O
e	O
connects	O
some	O
node	O
in	O
vt−1	O
to	O
some	O
node	O
in	O
vt	O
,	O
for	O
some	O
t	O
∈	O
[	O
t	O
]	O
.	O
the	O
bottom	O
layer	O
,	O
v0	O
,	O
is	O
called	O
the	O
input	O
layer	O
.	O
it	O
contains	O
n	O
+	O
1	O
neurons	O
,	O
where	O
n	O
is	O
the	O
dimensionality	O
of	O
the	O
input	O
space	O
.	O
for	O
every	O
i	O
∈	O
[	O
n	O
]	O
,	O
the	O
output	O
of	O
neuron	O
i	O
in	O
v0	O
is	O
simply	O
xi	O
.	O
the	O
last	O
neuron	O
in	O
v0	O
is	O
the	O
“	O
constant	O
”	O
neuron	O
,	O
which	O
always	O
outputs	O
1.	O
we	O
denote	O
by	O
vt	O
,	O
i	O
the	O
ith	O
neuron	O
of	O
the	O
tth	O
layer	O
and	O
by	O
ot	O
,	O
i	O
(	O
x	O
)	O
the	O
output	O
of	O
vt	O
,	O
i	O
when	O
the	O
network	O
is	O
fed	O
with	O
the	O
input	O
vector	O
x.	O
therefore	O
,	O
for	O
i	O
∈	O
[	O
n	O
]	O
we	O
have	O
o0	O
,	O
i	O
(	O
x	O
)	O
=	O
xi	O
and	O
for	O
i	O
=	O
n	O
+	O
1	O
we	O
have	O
o0	O
,	O
i	O
(	O
x	O
)	O
=	O
1.	O
we	O
now	O
proceed	O
with	O
the	O
calculation	O
in	O
a	O
layer	O
by	O
layer	O
manner	O
.	O
suppose	O
we	O
have	O
calculated	O
the	O
outputs	O
of	O
the	O
neurons	O
at	O
layer	O
t.	O
then	O
,	O
we	O
can	O
calculate	O
the	O
outputs	O
of	O
the	O
neurons	O
at	O
layer	O
t	O
+	O
1	O
as	O
follows	O
.	O
fix	O
some	O
vt+1	O
,	O
j	O
∈	O
vt+1	O
.	O
let	O
at+1	O
,	O
j	O
(	O
x	O
)	O
denote	O
the	O
input	O
to	O
vt+1	O
,	O
j	O
when	O
the	O
network	O
is	O
fed	O
with	O
the	O
input	O
vector	O
x.	O
then	O
,	O
at+1	O
,	O
j	O
(	O
x	O
)	O
=	O
w	O
(	O
(	O
vt	O
,	O
r	O
,	O
vt+1	O
,	O
j	O
)	O
)	O
ot	O
,	O
r	O
(	O
x	O
)	O
,	O
(	O
cid:88	O
)	O
r	O
:	O
(	O
vt	O
,	O
r	O
,	O
vt+1	O
,	O
j	O
)	O
∈e	O
270	O
neural	B
networks	I
and	O
ot+1	O
,	O
j	O
(	O
x	O
)	O
=	O
σ	O
(	O
at+1	O
,	O
j	O
(	O
x	O
)	O
)	O
.	O
that	O
is	O
,	O
the	O
input	O
to	O
vt+1	O
,	O
j	O
is	O
a	O
weighted	O
sum	O
of	O
the	O
outputs	O
of	O
the	O
neurons	O
in	O
vt	O
that	O
are	O
connected	O
to	O
vt+1	O
,	O
j	O
,	O
where	O
weighting	O
is	O
according	O
to	O
w	O
,	O
and	O
the	O
output	O
of	O
vt+1	O
,	O
j	O
is	O
simply	O
the	O
application	O
of	O
the	O
activation	B
function	I
σ	O
on	O
its	O
input	O
.	O
layers	O
v1	O
,	O
.	O
.	O
.	O
,	O
vt−1	O
are	O
often	O
called	O
hidden	B
layers	I
.	O
the	O
top	O
layer	O
,	O
vt	O
,	O
is	O
called	O
the	O
output	O
layer	O
.	O
in	O
simple	O
prediction	O
problems	O
the	O
output	O
layer	O
contains	O
a	O
single	O
neuron	O
whose	O
output	O
is	O
the	O
output	O
of	O
the	O
network	O
.	O
we	O
refer	O
to	O
t	O
as	O
the	O
number	O
of	O
layers	O
in	O
the	O
network	O
(	O
excluding	O
v0	O
)	O
,	O
or	O
the	O
“	O
depth	O
”	O
of	O
the	O
network	O
.	O
the	O
size	O
of	O
the	O
network	O
is	O
|v	O
|	O
.	O
the	O
“	O
width	O
”	O
of	O
the	O
network	O
is	O
maxt	O
|vt|	O
.	O
an	O
illustration	O
of	O
a	O
layered	O
feedforward	O
neural	O
network	O
of	O
depth	O
2	O
,	O
size	O
10	O
,	O
and	O
width	O
5	O
,	O
is	O
given	O
in	O
the	O
following	O
.	O
note	O
that	O
there	O
is	O
a	O
neuron	O
in	O
the	O
hidden	O
layer	O
that	O
has	O
no	O
incoming	O
edges	O
.	O
this	O
neuron	O
will	O
output	O
the	O
constant	O
σ	O
(	O
0	O
)	O
.	O
input	O
layer	O
(	O
v0	O
)	O
hidden	O
layer	O
(	O
v1	O
)	O
output	O
layer	O
(	O
v2	O
)	O
x1	O
x2	O
x3	O
constant	O
v0,1	O
v0,2	O
v0,3	O
v0,4	O
v1,1	O
v1,2	O
v1,3	O
v1,4	O
v1,5	O
v2,1	O
output	O
20.2	O
learning	O
neural	O
networks	O
once	O
we	O
have	O
speciﬁed	O
a	O
neural	O
network	O
by	O
(	O
v	O
,	O
e	O
,	O
σ	O
,	O
w	O
)	O
,	O
we	O
obtain	O
a	O
function	B
hv	O
,	O
e	O
,	O
σ	O
,	O
w	O
:	O
r|v0|−1	O
→	O
r|vt	O
|	O
.	O
any	O
set	B
of	O
such	O
functions	O
can	O
serve	O
as	O
a	O
hypothesis	B
class	I
for	O
learning	O
.	O
usually	O
,	O
we	O
deﬁne	O
a	O
hypothesis	B
class	I
of	O
neural	O
network	O
predic-	O
tors	O
by	O
ﬁxing	O
the	O
graph	O
(	O
v	O
,	O
e	O
)	O
as	O
well	O
as	O
the	O
activation	B
function	I
σ	O
and	O
letting	O
the	O
hypothesis	B
class	I
be	O
all	O
functions	O
of	O
the	O
form	O
hv	O
,	O
e	O
,	O
σ	O
,	O
w	O
for	O
some	O
w	O
:	O
e	O
→	O
r.	O
the	O
triplet	O
(	O
v	O
,	O
e	O
,	O
σ	O
)	O
is	O
often	O
called	O
the	O
architecture	O
of	O
the	O
network	O
.	O
we	O
denote	O
the	O
hypothesis	B
class	I
by	O
hv	O
,	O
e	O
,	O
σ	O
=	O
{	O
hv	O
,	O
e	O
,	O
σ	O
,	O
w	O
:	O
w	O
is	O
a	O
mapping	O
from	O
e	O
to	O
r	O
}	O
.	O
(	O
20.1	O
)	O
20.3	O
the	O
expressive	O
power	O
of	O
neural	B
networks	I
271	O
that	O
is	O
,	O
the	O
parameters	O
specifying	O
a	O
hypothesis	B
in	O
the	O
hypothesis	B
class	I
are	O
the	O
weights	O
over	O
the	O
edges	O
of	O
the	O
network	O
.	O
we	O
can	O
now	O
study	O
the	O
approximation	B
error	I
,	O
estimation	B
error	I
,	O
and	O
optimization	B
error	I
of	O
such	O
hypothesis	B
classes	O
.	O
in	O
section	O
20.3	O
we	O
study	O
the	O
approximation	B
error	I
of	O
hv	O
,	O
e	O
,	O
σ	O
by	O
studying	O
what	O
type	O
of	O
functions	O
hypotheses	O
in	O
hv	O
,	O
e	O
,	O
σ	O
can	O
implement	O
,	O
in	O
terms	O
of	O
the	O
size	O
of	O
the	O
underlying	O
graph	O
.	O
in	O
section	O
20.4	O
we	O
study	O
the	O
estimation	B
error	I
of	O
hv	O
,	O
e	O
,	O
σ	O
,	O
for	O
the	O
case	O
of	O
binary	O
classiﬁcation	O
(	O
i.e.	O
,	O
vt	O
=	O
1	O
and	O
σ	O
is	O
the	O
sign	O
function	B
)	O
,	O
by	O
analyzing	O
its	O
vc	O
dimension	B
.	O
finally	O
,	O
in	O
section	O
20.5	O
we	O
show	O
that	O
it	O
is	O
computationally	O
hard	O
to	O
learn	O
the	O
class	O
hv	O
,	O
e	O
,	O
σ	O
,	O
even	O
if	O
the	O
underlying	O
graph	O
is	O
small	O
,	O
and	O
in	O
section	O
20.6	O
we	O
present	O
the	O
most	O
commonly	O
used	O
heuristic	O
for	O
training	O
hv	O
,	O
e	O
,	O
σ	O
.	O
20.3	O
the	O
expressive	O
power	O
of	O
neural	B
networks	I
in	O
this	O
section	O
we	O
study	O
the	O
expressive	O
power	O
of	O
neural	B
networks	I
,	O
namely	O
,	O
what	O
type	O
of	O
functions	O
can	O
be	O
implemented	O
using	O
a	O
neural	O
network	O
.	O
more	O
concretely	O
,	O
we	O
will	O
ﬁx	O
some	O
architecture	O
,	O
v	O
,	O
e	O
,	O
σ	O
,	O
and	O
will	O
study	O
what	O
functions	O
hypotheses	O
in	O
hv	O
,	O
e	O
,	O
σ	O
can	O
implement	O
,	O
as	O
a	O
function	B
of	O
the	O
size	O
of	O
v	O
.	O
we	O
start	O
the	O
discussion	O
with	O
studying	O
which	O
type	O
of	O
boolean	O
functions	O
(	O
i.e.	O
,	O
functions	O
from	O
{	O
±1	O
}	O
n	O
to	O
{	O
±1	O
}	O
)	O
can	O
be	O
implemented	O
by	O
hv	O
,	O
e	O
,	O
sign	O
.	O
observe	O
that	O
for	O
every	O
computer	O
in	O
which	O
real	O
numbers	O
are	O
stored	O
using	O
b	O
bits	O
,	O
whenever	O
we	O
calculate	O
a	O
function	B
f	O
:	O
rn	O
→	O
r	O
on	O
such	O
a	O
computer	O
we	O
in	O
fact	O
calculate	O
a	O
function	B
g	O
:	O
{	O
±1	O
}	O
nb	O
→	O
{	O
±1	O
}	O
b.	O
therefore	O
,	O
studying	O
which	O
boolean	O
functions	O
can	O
be	O
implemented	O
by	O
hv	O
,	O
e	O
,	O
sign	O
can	O
tell	O
us	O
which	O
functions	O
can	O
be	O
implemented	O
on	O
a	O
computer	O
that	O
stores	O
real	O
numbers	O
using	O
b	O
bits	O
.	O
we	O
begin	O
with	O
a	O
simple	O
claim	O
,	O
showing	O
that	O
without	O
restricting	O
the	O
size	O
of	O
the	O
network	O
,	O
every	O
boolean	O
function	O
can	O
be	O
implemented	O
using	O
a	O
neural	O
network	O
of	O
depth	O
2.	O
claim	O
20.1	O
for	O
every	O
n	O
,	O
there	O
exists	O
a	O
graph	O
(	O
v	O
,	O
e	O
)	O
of	O
depth	O
2	O
,	O
such	O
that	O
hv	O
,	O
e	O
,	O
sign	O
contains	O
all	O
functions	O
from	O
{	O
±1	O
}	O
n	O
to	O
{	O
±1	O
}	O
.	O
proof	O
we	O
construct	O
a	O
graph	O
with	O
|v0|	O
=	O
n	O
+	O
1	O
,	O
|v1|	O
=	O
2n	O
+	O
1	O
,	O
and	O
|v2|	O
=	O
1.	O
let	O
e	O
be	O
all	O
possible	O
edges	O
between	O
adjacent	O
layers	O
.	O
now	O
,	O
let	O
f	O
:	O
{	O
±1	O
}	O
n	O
→	O
{	O
±1	O
}	O
be	O
some	O
boolean	O
function	O
.	O
we	O
need	O
to	O
show	O
that	O
we	O
can	O
adjust	O
the	O
weights	O
so	O
that	O
the	O
network	O
will	O
implement	O
f	O
.	O
let	O
u1	O
,	O
.	O
.	O
.	O
,	O
uk	O
be	O
all	O
vectors	O
in	O
{	O
±1	O
}	O
n	O
on	O
which	O
f	O
outputs	O
1.	O
observe	O
that	O
for	O
every	O
i	O
and	O
every	O
x	O
∈	O
{	O
±1	O
}	O
n	O
,	O
if	O
x	O
(	O
cid:54	O
)	O
=	O
ui	O
then	O
(	O
cid:104	O
)	O
x	O
,	O
ui	O
(	O
cid:105	O
)	O
≤	O
n	O
−	O
2	O
and	O
if	O
x	O
=	O
ui	O
then	O
(	O
cid:104	O
)	O
x	O
,	O
ui	O
(	O
cid:105	O
)	O
=	O
n.	O
it	O
follows	O
that	O
the	O
function	B
gi	O
(	O
x	O
)	O
=	O
sign	O
(	O
(	O
cid:104	O
)	O
x	O
,	O
ui	O
(	O
cid:105	O
)	O
−	O
n	O
+	O
1	O
)	O
equals	O
1	O
if	O
and	O
only	O
if	O
x	O
=	O
ui	O
.	O
it	O
follows	O
that	O
we	O
can	O
adapt	O
the	O
weights	O
between	O
v0	O
and	O
v1	O
so	O
that	O
for	O
every	O
i	O
∈	O
[	O
k	O
]	O
,	O
the	O
neuron	O
v1	O
,	O
i	O
implements	O
the	O
function	B
gi	O
(	O
x	O
)	O
.	O
next	O
,	O
we	O
observe	O
that	O
f	O
(	O
x	O
)	O
is	O
the	O
disjunction	O
of	O
272	O
neural	B
networks	I
the	O
functions	O
gi	O
(	O
x	O
)	O
,	O
and	O
therefore	O
can	O
be	O
written	O
as	O
f	O
(	O
x	O
)	O
=	O
sign	O
gi	O
(	O
x	O
)	O
+	O
k	O
−	O
1	O
(	O
cid:32	O
)	O
k	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
,	O
which	O
concludes	O
our	O
proof	O
.	O
i=1	O
the	O
preceding	O
claim	O
shows	O
that	O
neural	B
networks	I
can	O
implement	O
any	O
boolean	O
function	O
.	O
however	O
,	O
this	O
is	O
a	O
very	O
weak	O
property	O
,	O
as	O
the	O
size	O
of	O
the	O
resulting	O
network	O
might	O
be	O
exponentially	O
large	O
.	O
in	O
the	O
construction	O
given	O
at	O
the	O
proof	O
of	O
claim	O
20.1	O
,	O
the	O
number	O
of	O
nodes	O
in	O
the	O
hidden	O
layer	O
is	O
exponentially	O
large	O
.	O
this	O
is	O
not	O
an	O
artifact	O
of	O
our	O
proof	O
,	O
as	O
stated	O
in	O
the	O
following	O
theorem	O
.	O
theorem	O
20.2	O
for	O
every	O
n	O
,	O
let	O
s	O
(	O
n	O
)	O
be	O
the	O
minimal	O
integer	O
such	O
that	O
there	O
exists	O
a	O
graph	O
(	O
v	O
,	O
e	O
)	O
with	O
|v	O
|	O
=	O
s	O
(	O
n	O
)	O
such	O
that	O
the	O
hypothesis	B
class	I
hv	O
,	O
e	O
,	O
sign	O
contains	O
all	O
the	O
functions	O
from	O
{	O
0	O
,	O
1	O
}	O
n	O
to	O
{	O
0	O
,	O
1	O
}	O
.	O
then	O
,	O
s	O
(	O
n	O
)	O
is	O
exponential	O
in	O
n.	O
similar	O
results	O
hold	O
for	O
hv	O
,	O
e	O
,	O
σ	O
where	O
σ	O
is	O
the	O
sigmoid	O
function	B
.	O
proof	O
suppose	O
that	O
for	O
some	O
(	O
v	O
,	O
e	O
)	O
we	O
have	O
that	O
hv	O
,	O
e	O
,	O
sign	O
contains	O
all	O
functions	O
from	O
{	O
0	O
,	O
1	O
}	O
n	O
to	O
{	O
0	O
,	O
1	O
}	O
.	O
it	O
follows	O
that	O
it	O
can	O
shatter	O
the	O
set	B
of	O
m	O
=	O
2n	O
vectors	O
in	O
{	O
0	O
,	O
1	O
}	O
n	O
and	O
hence	O
the	O
vc	O
dimension	B
of	O
hv	O
,	O
e	O
,	O
sign	O
is	O
2n	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
vc	O
dimension	B
of	O
hv	O
,	O
e	O
,	O
sign	O
is	O
bounded	O
by	O
o	O
(	O
|e|	O
log	O
(	O
|e|	O
)	O
)	O
≤	O
o	O
(	O
|v	O
|3	O
)	O
,	O
as	O
we	O
will	O
show	O
in	O
the	O
next	O
section	O
.	O
this	O
implies	O
that	O
|v	O
|	O
≥	O
ω	O
(	O
2n/3	O
)	O
,	O
which	O
concludes	O
our	O
proof	O
for	O
the	O
case	O
of	O
networks	O
with	O
the	O
sign	O
activation	B
function	I
.	O
the	O
proof	O
for	O
the	O
sigmoid	O
case	O
is	O
analogous	O
.	O
it	O
is	O
possible	O
to	O
derive	O
a	O
similar	O
theorem	O
for	O
hv	O
,	O
e	O
,	O
σ	O
for	O
any	O
σ	O
,	O
as	O
remark	O
20.1	O
long	O
as	O
we	O
restrict	O
the	O
weights	O
so	O
that	O
it	O
is	O
possible	O
to	O
express	O
every	O
weight	O
using	O
a	O
number	O
of	O
bits	O
which	O
is	O
bounded	O
by	O
a	O
universal	O
constant	O
.	O
we	O
can	O
even	O
con-	O
sider	O
hypothesis	B
classes	O
where	O
diﬀerent	O
neurons	O
can	O
employ	O
diﬀerent	O
activation	O
functions	O
,	O
as	O
long	O
as	O
the	O
number	O
of	O
allowed	O
activation	O
functions	O
is	O
also	O
ﬁnite	O
.	O
which	O
functions	O
can	O
we	O
express	O
using	O
a	O
network	O
of	O
polynomial	O
size	O
?	O
the	O
pre-	O
ceding	O
claim	O
tells	O
us	O
that	O
it	O
is	O
impossible	O
to	O
express	O
all	O
boolean	O
functions	O
using	O
a	O
network	O
of	O
polynomial	O
size	O
.	O
on	O
the	O
positive	O
side	O
,	O
in	O
the	O
following	O
we	O
show	O
that	O
all	O
boolean	O
functions	O
that	O
can	O
be	O
calculated	O
in	O
time	O
o	O
(	O
t	O
(	O
n	O
)	O
)	O
can	O
also	O
be	O
expressed	O
by	O
a	O
network	O
of	O
size	O
o	O
(	O
t	O
(	O
n	O
)	O
2	O
)	O
.	O
theorem	O
20.3	O
let	O
t	O
:	O
n	O
→	O
n	O
and	O
for	O
every	O
n	O
,	O
let	O
fn	O
be	O
the	O
set	B
of	O
functions	O
that	O
can	O
be	O
implemented	O
using	O
a	O
turing	O
machine	O
using	O
runtime	O
of	O
at	O
most	O
t	O
(	O
n	O
)	O
.	O
then	O
,	O
there	O
exist	O
constants	O
b	O
,	O
c	O
∈	O
r+	O
such	O
that	O
for	O
every	O
n	O
,	O
there	O
is	O
a	O
graph	O
(	O
vn	O
,	O
en	O
)	O
of	O
size	O
at	O
most	O
c	O
t	O
(	O
n	O
)	O
2	O
+	O
b	O
such	O
that	O
hvn	O
,	O
en	O
,	O
sign	O
contains	O
fn	O
.	O
the	O
proof	O
of	O
this	O
theorem	O
relies	O
on	O
the	O
relation	O
between	O
the	O
time	O
complexity	O
of	O
programs	O
and	O
their	O
circuit	O
complexity	O
(	O
see	O
,	O
for	O
example	O
,	O
sipser	O
(	O
2006	O
)	O
)	O
.	O
in	O
a	O
nutshell	O
,	O
a	O
boolean	O
circuit	O
is	O
a	O
type	O
of	O
network	O
in	O
which	O
the	O
individual	O
neurons	O
20.3	O
the	O
expressive	O
power	O
of	O
neural	B
networks	I
273	O
implement	O
conjunctions	O
,	O
disjunctions	O
,	O
and	O
negation	O
of	O
their	O
inputs	O
.	O
circuit	O
com-	O
plexity	O
measures	O
the	O
size	O
of	O
boolean	O
circuits	O
required	O
to	O
calculate	O
functions	O
.	O
the	O
relation	O
between	O
time	O
complexity	O
and	O
circuit	O
complexity	O
can	O
be	O
seen	O
intuitively	O
as	O
follows	O
.	O
we	O
can	O
model	O
each	O
step	O
of	O
the	O
execution	O
of	O
a	O
computer	O
program	O
as	O
a	O
simple	O
operation	O
on	O
its	O
memory	O
state	O
.	O
therefore	O
,	O
the	O
neurons	O
at	O
each	O
layer	O
of	O
the	O
network	O
will	O
reﬂect	O
the	O
memory	O
state	O
of	O
the	O
computer	O
at	O
the	O
corresponding	O
time	O
,	O
and	O
the	O
translation	O
to	O
the	O
next	O
layer	O
of	O
the	O
network	O
involves	O
a	O
simple	O
calculation	O
that	O
can	O
be	O
carried	O
out	O
by	O
the	O
network	O
.	O
to	O
relate	O
boolean	O
circuits	O
to	O
networks	O
with	O
the	O
sign	O
activation	B
function	I
,	O
we	O
need	O
to	O
show	O
that	O
we	O
can	O
implement	O
the	O
operations	O
of	O
conjunction	O
,	O
disjunction	O
,	O
and	O
negation	O
,	O
using	O
the	O
sign	O
activation	B
function	I
.	O
clearly	O
,	O
we	O
can	O
implement	O
the	O
negation	O
operator	O
using	O
the	O
sign	O
activa-	O
tion	O
function	B
.	O
the	O
following	O
lemma	O
shows	O
that	O
the	O
sign	O
activation	B
function	I
can	O
also	O
implement	O
conjunctions	O
and	O
disjunctions	O
of	O
its	O
inputs	O
.	O
lemma	O
20.4	O
suppose	O
that	O
a	O
neuron	O
v	O
,	O
that	O
implements	O
the	O
sign	O
activation	B
function	I
,	O
has	O
k	O
incoming	O
edges	O
,	O
connecting	O
it	O
to	O
neurons	O
whose	O
outputs	O
are	O
in	O
{	O
±1	O
}	O
.	O
then	O
,	O
by	O
adding	O
one	O
more	O
edge	O
,	O
linking	O
a	O
“	O
constant	O
”	O
neuron	O
to	O
v	O
,	O
and	O
by	O
adjusting	O
the	O
weights	O
on	O
the	O
edges	O
to	O
v	O
,	O
the	O
output	O
of	O
v	O
can	O
implement	O
the	O
conjunction	O
or	O
the	O
disjunction	O
of	O
its	O
inputs	O
.	O
proof	O
simply	O
observe	O
that	O
if	O
f	O
:	O
{	O
±1	O
}	O
k	O
→	O
{	O
±1	O
}	O
is	O
the	O
conjunction	O
func-	O
tion	O
,	O
f	O
(	O
x	O
)	O
=	O
∧ixi	O
,	O
then	O
it	O
can	O
be	O
written	O
as	O
f	O
(	O
x	O
)	O
=	O
sign	O
.	O
similarly	O
,	O
the	O
disjunction	O
function	B
,	O
f	O
(	O
x	O
)	O
=	O
∨ixi	O
,	O
can	O
be	O
written	O
as	O
f	O
(	O
x	O
)	O
=	O
sign	O
1	O
−	O
k	O
+	O
(	O
cid:80	O
)	O
k	O
k	O
−	O
1	O
+	O
(	O
cid:80	O
)	O
k	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
i=1	O
xi	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
i=1	O
xi	O
.	O
so	O
far	O
we	O
have	O
discussed	O
boolean	O
functions	O
.	O
in	O
exercise	O
1	O
we	O
show	O
that	O
neural	B
networks	I
are	O
universal	O
approximators	O
.	O
that	O
is	O
,	O
for	O
every	O
ﬁxed	O
precision	B
param-	O
eter	O
,	O
	O
>	O
0	O
,	O
and	O
every	O
lipschitz	O
function	B
f	O
:	O
[	O
−1	O
,	O
1	O
]	O
n	O
→	O
[	O
−1	O
,	O
1	O
]	O
,	O
it	O
is	O
possible	O
to	O
construct	O
a	O
network	O
such	O
that	O
for	O
every	O
input	O
x	O
∈	O
[	O
−1	O
,	O
1	O
]	O
n	O
,	O
the	O
network	O
outputs	O
a	O
number	O
between	O
f	O
(	O
x	O
)	O
−	O
	O
and	O
f	O
(	O
x	O
)	O
+	O
	O
.	O
however	O
,	O
as	O
in	O
the	O
case	O
of	O
boolean	O
functions	O
,	O
the	O
size	O
of	O
the	O
network	O
here	O
again	O
can	O
not	O
be	O
polynomial	O
in	O
n.	O
this	O
is	O
formalized	O
in	O
the	O
following	O
theorem	O
,	O
whose	O
proof	O
is	O
a	O
direct	O
corollary	O
of	O
theo-	O
rem	O
20.2	O
and	O
is	O
left	O
as	O
an	O
exercise	O
.	O
theorem	O
20.5	O
fix	O
some	O
	O
∈	O
(	O
0	O
,	O
1	O
)	O
.	O
for	O
every	O
n	O
,	O
let	O
s	O
(	O
n	O
)	O
be	O
the	O
minimal	O
integer	O
such	O
that	O
there	O
exists	O
a	O
graph	O
(	O
v	O
,	O
e	O
)	O
with	O
|v	O
|	O
=	O
s	O
(	O
n	O
)	O
such	O
that	O
the	O
hypothesis	B
class	I
hv	O
,	O
e	O
,	O
σ	O
,	O
with	O
σ	O
being	O
the	O
sigmoid	O
function	B
,	O
can	O
approximate	O
,	O
to	O
within	O
precision	B
of	O
	O
,	O
every	O
1-lipschitz	O
function	B
f	O
:	O
[	O
−1	O
,	O
1	O
]	O
n	O
→	O
[	O
−1	O
,	O
1	O
]	O
.	O
then	O
s	O
(	O
n	O
)	O
is	O
exponential	O
in	O
n.	O
20.3.1	O
geometric	O
intuition	O
we	O
next	O
provide	O
several	O
geometric	O
illustrations	O
of	O
functions	O
f	O
:	O
r2	O
→	O
{	O
±1	O
}	O
and	O
show	O
how	O
to	O
express	O
them	O
using	O
a	O
neural	O
network	O
with	O
the	O
sign	O
activation	B
function	I
.	O
274	O
neural	B
networks	I
let	O
us	O
start	O
with	O
a	O
depth	O
2	O
network	O
,	O
namely	O
,	O
a	O
network	O
with	O
a	O
single	O
hidden	O
layer	O
.	O
each	O
neuron	O
in	O
the	O
hidden	O
layer	O
implements	O
a	O
halfspace	B
predictor	O
.	O
then	O
,	O
the	O
single	O
neuron	O
at	O
the	O
output	O
layer	O
applies	O
a	O
halfspace	B
on	O
top	O
of	O
the	O
binary	O
outputs	O
of	O
the	O
neurons	O
in	O
the	O
hidden	O
layer	O
.	O
as	O
we	O
have	O
shown	O
before	O
,	O
a	O
halfspace	B
can	O
implement	O
the	O
conjunction	O
function	B
.	O
therefore	O
,	O
such	O
networks	O
contain	O
all	O
hypotheses	O
which	O
are	O
an	O
intersection	O
of	O
k	O
−	O
1	O
halfspaces	O
,	O
where	O
k	O
is	O
the	O
number	O
of	O
neurons	O
in	O
the	O
hidden	O
layer	O
;	O
namely	O
,	O
they	O
can	O
express	O
all	O
convex	B
polytopes	O
with	O
k	O
−	O
1	O
faces	O
.	O
an	O
example	O
of	O
an	O
intersection	O
of	O
5	O
halfspaces	O
is	O
given	O
in	O
the	O
following	O
.	O
we	O
have	O
shown	O
that	O
a	O
neuron	O
in	O
layer	O
v2	O
can	O
implement	O
a	O
function	B
that	O
indicates	O
whether	O
x	O
is	O
in	O
some	O
convex	B
polytope	O
.	O
by	O
adding	O
one	O
more	O
layer	O
,	O
and	O
letting	O
the	O
neuron	O
in	O
the	O
output	O
layer	O
implement	O
the	O
disjunction	O
of	O
its	O
inputs	O
,	O
we	O
get	O
a	O
network	O
that	O
computes	O
the	O
union	O
of	O
polytopes	O
.	O
an	O
illustration	O
of	O
such	O
a	O
function	B
is	O
given	O
in	O
the	O
following	O
.	O
20.4	O
the	O
sample	B
complexity	I
of	O
neural	B
networks	I
next	O
we	O
discuss	O
the	O
sample	B
complexity	I
of	O
learning	O
the	O
class	O
hv	O
,	O
e	O
,	O
σ	O
.	O
recall	B
that	O
the	O
fundamental	O
theorem	O
of	O
learning	O
tells	O
us	O
that	O
the	O
sample	B
complexity	I
of	O
learn-	O
ing	O
a	O
hypothesis	B
class	I
of	O
binary	O
classiﬁers	O
depends	O
on	O
its	O
vc	O
dimension	B
.	O
there-	O
fore	O
,	O
we	O
focus	O
on	O
calculating	O
the	O
vc	O
dimension	B
of	O
hypothesis	B
classes	O
of	O
the	O
form	O
hv	O
,	O
e	O
,	O
σ	O
,	O
where	O
the	O
output	O
layer	O
of	O
the	O
graph	O
contains	O
a	O
single	O
neuron	O
.	O
we	O
start	O
with	O
the	O
sign	O
activation	B
function	I
,	O
namely	O
,	O
with	O
hv	O
,	O
e	O
,	O
sign	O
.	O
what	O
is	O
the	O
vc	O
dimension	B
of	O
this	O
class	O
?	O
intuitively	O
,	O
since	O
we	O
learn	O
|e|	O
parameters	O
,	O
the	O
vc	O
dimension	B
should	O
be	O
order	O
of	O
|e|	O
.	O
this	O
is	O
indeed	O
the	O
case	O
,	O
as	O
formalized	O
by	O
the	O
following	O
theorem	O
.	O
theorem	O
20.6	O
the	O
vc	O
dimension	B
of	O
hv	O
,	O
e	O
,	O
sign	O
is	O
o	O
(	O
|e|	O
log	O
(	O
|e|	O
)	O
)	O
.	O
20.4	O
the	O
sample	B
complexity	I
of	O
neural	B
networks	I
275	O
proof	O
to	O
simplify	O
the	O
notation	O
throughout	O
the	O
proof	O
,	O
let	O
us	O
denote	O
the	O
hy-	O
pothesis	O
class	O
by	O
h.	O
recall	B
the	O
deﬁnition	O
of	O
the	O
growth	B
function	I
,	O
τh	O
(	O
m	O
)	O
,	O
from	O
section	O
6.5.1.	O
this	O
function	B
measures	O
maxc⊂x	O
:	O
|c|=m	O
|hc|	O
,	O
where	O
hc	O
is	O
the	O
re-	O
striction	O
of	O
h	O
to	O
functions	O
from	O
c	O
to	O
{	O
0	O
,	O
1	O
}	O
.	O
we	O
can	O
naturally	O
extend	O
the	O
deﬁ-	O
nition	O
for	O
a	O
set	B
of	O
functions	O
from	O
x	O
to	O
some	O
ﬁnite	O
set	B
y	O
,	O
by	O
letting	O
hc	O
be	O
the	O
restriction	O
of	O
h	O
to	O
functions	O
from	O
c	O
to	O
y	O
,	O
and	O
keeping	O
the	O
deﬁnition	O
of	O
τh	O
(	O
m	O
)	O
intact	O
.	O
our	O
neural	O
network	O
is	O
deﬁned	O
by	O
a	O
layered	O
graph	O
.	O
let	O
v0	O
,	O
.	O
.	O
.	O
,	O
vt	O
be	O
the	O
layers	O
of	O
the	O
graph	O
.	O
fix	O
some	O
t	O
∈	O
[	O
t	O
]	O
.	O
by	O
assigning	O
diﬀerent	O
weights	O
on	O
the	O
edges	O
between	O
vt−1	O
and	O
vt	O
,	O
we	O
obtain	O
diﬀerent	O
functions	O
from	O
r|vt−1|	O
→	O
{	O
±1	O
}	O
|vt|	O
.	O
let	O
h	O
(	O
t	O
)	O
be	O
the	O
class	O
of	O
all	O
possible	O
such	O
mappings	O
from	O
r|vt−1|	O
→	O
{	O
±1	O
}	O
|vt|	O
.	O
then	O
,	O
h	O
can	O
be	O
written	O
as	O
a	O
composition	O
,	O
h	O
=	O
h	O
(	O
t	O
)	O
◦	O
.	O
.	O
.◦h	O
(	O
1	O
)	O
.	O
in	O
exercise	O
4	O
we	O
show	O
that	O
the	O
growth	B
function	I
of	O
a	O
composition	O
of	O
hypothesis	B
classes	O
is	O
bounded	O
by	O
the	O
products	O
of	O
the	O
growth	O
functions	O
of	O
the	O
individual	O
classes	O
.	O
therefore	O
,	O
τh	O
(	O
m	O
)	O
≤	O
t	O
(	O
cid:89	O
)	O
t=1	O
τh	O
(	O
t	O
)	O
(	O
m	O
)	O
.	O
in	O
addition	O
,	O
each	O
h	O
(	O
t	O
)	O
can	O
be	O
written	O
as	O
a	O
product	O
of	O
function	B
classes	O
,	O
h	O
(	O
t	O
)	O
=	O
h	O
(	O
t,1	O
)	O
×	O
···	O
×h	O
(	O
t	O
,	O
|vt|	O
)	O
,	O
where	O
each	O
h	O
(	O
t	O
,	O
j	O
)	O
is	O
all	O
functions	O
from	O
layer	O
t	O
−	O
1	O
to	O
{	O
±1	O
}	O
that	O
the	O
jth	O
neuron	O
of	O
layer	O
t	O
can	O
implement	O
.	O
in	O
exercise	O
3	O
we	O
bound	O
product	O
classes	O
,	O
and	O
this	O
yields	O
τh	O
(	O
t	O
)	O
(	O
m	O
)	O
≤	O
τh	O
(	O
t	O
,	O
i	O
)	O
(	O
m	O
)	O
.	O
|vt|	O
(	O
cid:89	O
)	O
i=1	O
let	O
dt	O
,	O
i	O
be	O
the	O
number	O
of	O
edges	O
that	O
are	O
headed	O
to	O
the	O
ith	O
neuron	O
of	O
layer	O
t.	O
since	O
the	O
neuron	O
is	O
a	O
homogenous	B
halfspace	O
hypothesis	B
and	O
the	O
vc	O
dimension	B
of	O
homogenous	B
halfspaces	O
is	O
the	O
dimension	B
of	O
their	O
input	O
,	O
we	O
have	O
by	O
sauer	O
’	O
s	O
lemma	O
that	O
τh	O
(	O
t	O
,	O
i	O
)	O
(	O
m	O
)	O
≤	O
(	O
cid:16	O
)	O
em	O
(	O
cid:80	O
)	O
τh	O
(	O
m	O
)	O
≤	O
(	O
em	O
)	O
dt	O
,	O
i	O
(	O
cid:17	O
)	O
dt	O
,	O
i	O
≤	O
(	O
em	O
)	O
dt	O
,	O
i	O
.	O
t	O
,	O
i	O
dt	O
,	O
i	O
=	O
(	O
em	O
)	O
|e|	O
.	O
overall	O
,	O
we	O
obtained	O
that	O
now	O
,	O
assume	O
that	O
there	O
are	O
m	O
shattered	O
points	O
.	O
then	O
,	O
we	O
must	O
have	O
τh	O
(	O
m	O
)	O
=	O
2m	O
,	O
from	O
which	O
we	O
obtain	O
2m	O
≤	O
(	O
em	O
)	O
|e|	O
⇒	O
m	O
≤	O
|e|	O
log	O
(	O
em	O
)	O
/	O
log	O
(	O
2	O
)	O
.	O
the	O
claim	O
follows	O
by	O
lemma	O
a.2	O
.	O
next	O
,	O
we	O
consider	O
hv	O
,	O
e	O
,	O
σ	O
,	O
where	O
σ	O
is	O
the	O
sigmoid	O
function	B
.	O
surprisingly	O
,	O
it	O
turns	O
out	O
that	O
the	O
vc	O
dimension	B
of	O
hv	O
,	O
e	O
,	O
σ	O
is	O
lower	O
bounded	O
by	O
ω	O
(	O
|e|2	O
)	O
(	O
see	O
exercise	O
5	O
.	O
)	O
that	O
is	O
,	O
the	O
vc	O
dimension	B
is	O
the	O
number	O
of	O
tunable	O
parameters	O
squared	O
.	O
it	O
is	O
also	O
possible	O
to	O
upper	O
bound	O
the	O
vc	O
dimension	B
by	O
o	O
(	O
|v	O
|2	O
|e|2	O
)	O
,	O
but	O
the	O
proof	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
.	O
in	O
any	O
case	O
,	O
since	O
in	O
practice	O
276	O
neural	B
networks	I
we	O
only	O
consider	O
networks	O
in	O
which	O
the	O
weights	O
have	O
a	O
short	O
representation	O
as	O
ﬂoating	O
point	O
numbers	O
with	O
o	O
(	O
1	O
)	O
bits	O
,	O
by	O
using	O
the	O
discretization	B
trick	I
we	O
easily	O
obtain	O
that	O
such	O
networks	O
have	O
a	O
vc	O
dimension	B
of	O
o	O
(	O
|e|	O
)	O
,	O
even	O
if	O
we	O
use	O
the	O
sigmoid	O
activation	B
function	I
.	O
20.5	O
the	O
runtime	O
of	O
learning	O
neural	O
networks	O
in	O
the	O
previous	O
sections	O
we	O
have	O
shown	O
that	O
the	O
class	O
of	O
neural	B
networks	I
with	O
an	O
underlying	O
graph	O
of	O
polynomial	O
size	O
can	O
express	O
all	O
functions	O
that	O
can	O
be	O
imple-	O
mented	O
eﬃciently	O
,	O
and	O
that	O
the	O
sample	B
complexity	I
has	O
a	O
favorable	O
dependence	O
on	O
the	O
size	O
of	O
the	O
network	O
.	O
in	O
this	O
section	O
we	O
turn	O
to	O
the	O
analysis	O
of	O
the	O
time	O
complexity	O
of	O
training	O
neural	O
networks	O
.	O
we	O
ﬁrst	O
show	O
that	O
it	O
is	O
np	O
hard	O
to	O
implement	O
the	O
erm	O
rule	O
with	O
respect	O
to	O
hv	O
,	O
e	O
,	O
sign	O
even	O
for	O
networks	O
with	O
a	O
single	O
hidden	O
layer	O
that	O
contain	O
just	O
4	O
neurons	O
in	O
the	O
hidden	O
layer	O
.	O
theorem	O
20.7	O
let	O
k	O
≥	O
3.	O
for	O
every	O
n	O
,	O
let	O
(	O
v	O
,	O
e	O
)	O
be	O
a	O
layered	O
graph	O
with	O
n	O
input	O
nodes	O
,	O
k	O
+	O
1	O
nodes	O
at	O
the	O
(	O
single	O
)	O
hidden	O
layer	O
,	O
where	O
one	O
of	O
them	O
is	O
the	O
constant	O
neuron	O
,	O
and	O
a	O
single	O
output	O
node	O
.	O
then	O
,	O
it	O
is	O
np	O
hard	O
to	O
implement	O
the	O
erm	O
rule	O
with	O
respect	O
to	O
hv	O
,	O
e	O
,	O
sign	O
.	O
the	O
proof	O
relies	O
on	O
a	O
reduction	O
from	O
the	O
k-coloring	O
problem	O
and	O
is	O
left	O
as	O
exercise	O
6.	O
one	O
way	O
around	O
the	O
preceding	O
hardness	O
result	O
could	O
be	O
that	O
for	O
the	O
purpose	O
of	O
learning	O
,	O
it	O
may	O
suﬃce	O
to	O
ﬁnd	O
a	O
predictor	B
h	O
∈	O
h	O
with	O
low	O
empirical	B
error	I
,	O
not	O
necessarily	O
an	O
exact	O
erm	O
.	O
however	O
,	O
it	O
turns	O
out	O
that	O
even	O
the	O
task	O
of	O
ﬁnd-	O
ing	O
weights	O
that	O
result	O
in	O
close-to-minimal	O
empirical	B
error	I
is	O
computationally	O
infeasible	O
(	O
see	O
(	O
bartlett	O
&	O
ben-david	O
2002	O
)	O
)	O
.	O
one	O
may	O
also	O
wonder	O
whether	O
it	O
may	O
be	O
possible	O
to	O
change	O
the	O
architecture	O
of	O
the	O
network	O
so	O
as	O
to	O
circumvent	O
the	O
hardness	O
result	O
.	O
that	O
is	O
,	O
maybe	O
erm	O
with	O
respect	O
to	O
the	O
original	O
network	O
structure	O
is	O
computationally	O
hard	O
but	O
erm	O
with	O
respect	O
to	O
some	O
other	O
,	O
larger	O
,	O
network	O
may	O
be	O
implemented	O
eﬃciently	O
(	O
see	O
chapter	O
8	O
for	O
examples	O
of	O
such	O
cases	O
)	O
.	O
another	O
possibility	O
is	O
to	O
use	O
other	O
acti-	O
vation	O
functions	O
(	O
such	O
as	O
sigmoids	O
,	O
or	O
any	O
other	O
type	O
of	O
eﬃciently	O
computable	O
activation	O
functions	O
)	O
.	O
there	O
is	O
a	O
strong	O
indication	O
that	O
all	O
of	O
such	O
approaches	O
are	O
doomed	O
to	O
fail	O
.	O
indeed	O
,	O
under	O
some	O
cryptographic	O
assumption	O
,	O
the	O
problem	O
of	O
learning	O
intersections	O
of	O
halfspaces	O
is	O
known	O
to	O
be	O
hard	O
even	O
in	O
the	O
repre-	O
sentation	O
independent	O
model	O
of	O
learning	O
(	O
see	O
klivans	O
&	O
sherstov	O
(	O
2006	O
)	O
)	O
.	O
this	O
implies	O
that	O
,	O
under	O
the	O
same	O
cryptographic	O
assumption	O
,	O
any	O
hypothesis	B
class	I
which	O
contains	O
intersections	O
of	O
halfspaces	O
can	O
not	O
be	O
learned	O
eﬃciently	O
.	O
a	O
widely	O
used	O
heuristic	O
for	O
training	O
neural	O
networks	O
relies	O
on	O
the	O
sgd	O
frame-	O
work	O
we	O
studied	O
in	O
chapter	O
14.	O
there	O
,	O
we	O
have	O
shown	O
that	O
sgd	O
is	O
a	O
successful	O
learner	O
if	O
the	O
loss	B
function	I
is	O
convex	B
.	O
in	O
neural	B
networks	I
,	O
the	O
loss	B
function	I
is	O
highly	O
nonconvex	O
.	O
nevertheless	O
,	O
we	O
can	O
still	O
implement	O
the	O
sgd	O
algorithm	O
and	O
20.6	O
sgd	O
and	O
backpropagation	B
277	O
hope	O
it	O
will	O
ﬁnd	O
a	O
reasonable	O
solution	O
(	O
as	O
happens	O
to	O
be	O
the	O
case	O
in	O
several	O
practical	O
tasks	O
)	O
.	O
20.6	O
sgd	O
and	O
backpropagation	B
the	O
problem	O
of	O
ﬁnding	O
a	O
hypothesis	B
in	O
hv	O
,	O
e	O
,	O
σ	O
with	O
a	O
low	O
risk	B
amounts	O
to	O
the	O
problem	O
of	O
tuning	O
the	O
weights	O
over	O
the	O
edges	O
.	O
in	O
this	O
section	O
we	O
show	O
how	O
to	O
apply	O
a	O
heuristic	O
search	O
for	O
good	O
weights	O
using	O
the	O
sgd	O
algorithm	O
.	O
throughout	O
this	O
section	O
we	O
assume	O
that	O
σ	O
is	O
the	O
sigmoid	O
function	B
,	O
σ	O
(	O
a	O
)	O
=	O
1/	O
(	O
1	O
+	O
e−a	O
)	O
,	O
but	O
the	O
derivation	O
holds	O
for	O
any	O
diﬀerentiable	O
scalar	O
function	B
.	O
since	O
e	O
is	O
a	O
ﬁnite	O
set	B
,	O
we	O
can	O
think	O
of	O
the	O
weight	O
function	B
as	O
a	O
vector	O
w	O
∈	O
r|e|	O
.	O
suppose	O
the	O
network	O
has	O
n	O
input	O
neurons	O
and	O
k	O
output	O
neurons	O
,	O
and	O
denote	O
by	O
hw	O
:	O
rn	O
→	O
rk	O
the	O
function	B
calculated	O
by	O
the	O
network	O
if	O
the	O
weight	O
function	B
is	O
deﬁned	O
by	O
w.	O
let	O
us	O
denote	O
by	O
∆	O
(	O
hw	O
(	O
x	O
)	O
,	O
y	O
)	O
the	O
loss	B
of	O
predicting	O
hw	O
(	O
x	O
)	O
when	O
the	O
target	O
is	O
y	O
∈	O
y.	O
for	O
concreteness	O
,	O
we	O
will	O
take	O
∆	O
to	O
be	O
the	O
squared	O
loss	B
,	O
2	O
(	O
cid:107	O
)	O
hw	O
(	O
x	O
)	O
−	O
y	O
(	O
cid:107	O
)	O
2	O
;	O
however	O
,	O
similar	O
derivation	O
can	O
be	O
obtained	O
for	O
∆	O
(	O
hw	O
(	O
x	O
)	O
,	O
y	O
)	O
=	O
1	O
every	O
diﬀerentiable	O
function	B
.	O
finally	O
,	O
given	O
a	O
distribution	O
d	O
over	O
the	O
examples	O
domain	B
,	O
rn	O
×	O
rk	O
,	O
let	O
ld	O
(	O
w	O
)	O
be	O
the	O
risk	B
of	O
the	O
network	O
,	O
namely	O
,	O
ld	O
(	O
w	O
)	O
=	O
e	O
(	O
x	O
,	O
y	O
)	O
∼d	O
[	O
∆	O
(	O
hw	O
(	O
x	O
)	O
,	O
y	O
)	O
]	O
.	O
recall	B
the	O
sgd	O
algorithm	O
for	O
minimizing	O
the	O
risk	B
function	O
ld	O
(	O
w	O
)	O
.	O
we	O
repeat	O
the	O
pseudocode	O
from	O
chapter	O
14	O
with	O
a	O
few	O
modiﬁcations	O
,	O
which	O
are	O
relevant	O
to	O
the	O
neural	O
network	O
application	O
because	O
of	O
the	O
nonconvexity	O
of	O
the	O
objective	O
function	B
.	O
first	O
,	O
while	O
in	O
chapter	O
14	O
we	O
initialized	O
w	O
to	O
be	O
the	O
zero	O
vector	O
,	O
here	O
we	O
initialize	O
w	O
to	O
be	O
a	O
randomly	O
chosen	O
vector	O
with	O
values	O
close	O
to	O
zero	O
.	O
this	O
is	O
because	O
an	O
initialization	O
with	O
the	O
zero	O
vector	O
will	O
lead	O
all	O
hidden	O
neurons	O
to	O
have	O
the	O
same	O
weights	O
(	O
if	O
the	O
network	O
is	O
a	O
full	O
layered	O
network	O
)	O
.	O
in	O
addition	O
,	O
the	O
hope	O
is	O
that	O
if	O
we	O
repeat	O
the	O
sgd	O
procedure	O
several	O
times	O
,	O
where	O
each	O
time	O
we	O
initialize	O
the	O
process	O
with	O
a	O
new	O
random	O
vector	O
,	O
one	O
of	O
the	O
runs	O
will	O
lead	O
to	O
a	O
good	O
local	B
minimum	I
.	O
second	O
,	O
while	O
a	O
ﬁxed	O
step	O
size	O
,	O
η	O
,	O
is	O
guaranteed	O
to	O
be	O
good	O
enough	O
for	O
convex	B
problems	O
,	O
here	O
we	O
utilize	O
a	O
variable	O
step	O
size	O
,	O
ηt	O
,	O
as	O
deﬁned	O
in	O
section	O
14.4.2.	O
because	O
of	O
the	O
nonconvexity	O
of	O
the	O
loss	B
function	I
,	O
the	O
choice	O
of	O
the	O
sequence	O
ηt	O
is	O
more	O
signiﬁcant	O
,	O
and	O
it	O
is	O
tuned	O
in	O
practice	O
by	O
a	O
trial	O
and	O
error	O
manner	O
.	O
third	O
,	O
we	O
output	O
the	O
best	O
performing	O
vector	O
on	O
a	O
validation	B
set	O
.	O
in	O
addition	O
,	O
it	O
is	O
sometimes	O
helpful	O
to	O
add	O
regularization	B
on	O
the	O
weights	O
,	O
with	O
parameter	O
λ.	O
that	O
is	O
,	O
we	O
try	O
to	O
minimize	O
ld	O
(	O
w	O
)	O
+	O
λ	O
gradient	B
does	O
not	O
have	O
a	O
closed	O
form	O
solution	O
.	O
instead	O
,	O
it	O
is	O
implemented	O
using	O
the	O
backpropagation	B
algorithm	O
,	O
which	O
will	O
be	O
described	O
in	O
the	O
sequel	O
.	O
2	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2.	O
finally	O
,	O
the	O
278	O
neural	B
networks	I
sgd	O
for	O
neural	B
networks	I
parameters	O
:	O
number	O
of	O
iterations	O
τ	O
step	O
size	O
sequence	O
η1	O
,	O
η2	O
,	O
.	O
.	O
.	O
,	O
ητ	O
regularization	B
parameter	O
λ	O
>	O
0	O
input	O
:	O
layered	O
graph	O
(	O
v	O
,	O
e	O
)	O
diﬀerentiable	O
activation	B
function	I
σ	O
:	O
r	O
→	O
r	O
initialize	O
:	O
choose	O
w	O
(	O
1	O
)	O
∈	O
r|e|	O
at	O
random	O
(	O
from	O
a	O
distribution	O
s.t	O
.	O
w	O
(	O
1	O
)	O
is	O
close	O
enough	O
to	O
0	O
)	O
for	O
i	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
τ	O
sample	O
(	O
x	O
,	O
y	O
)	O
∼	O
d	O
calculate	O
gradient	B
vi	O
=	O
backpropagation	B
(	O
x	O
,	O
y	O
,	O
w	O
,	O
(	O
v	O
,	O
e	O
)	O
,	O
σ	O
)	O
update	O
w	O
(	O
i+1	O
)	O
=	O
w	O
(	O
i	O
)	O
−	O
ηi	O
(	O
vi	O
+	O
λw	O
(	O
i	O
)	O
)	O
output	O
:	O
¯w	O
is	O
the	O
best	O
performing	O
w	O
(	O
i	O
)	O
on	O
a	O
validation	B
set	O
backpropagation	B
input	O
:	O
example	O
(	O
x	O
,	O
y	O
)	O
,	O
weight	O
vector	O
w	O
,	O
layered	O
graph	O
(	O
v	O
,	O
e	O
)	O
,	O
activation	B
function	I
σ	O
:	O
r	O
→	O
r	O
initialize	O
:	O
denote	O
layers	O
of	O
the	O
graph	O
v0	O
,	O
.	O
.	O
.	O
,	O
vt	O
where	O
vt	O
=	O
{	O
vt,1	O
,	O
.	O
.	O
.	O
,	O
vt	O
,	O
kt	O
}	O
deﬁne	O
wt	O
,	O
i	O
,	O
j	O
as	O
the	O
weight	O
of	O
(	O
vt	O
,	O
j	O
,	O
vt+1	O
,	O
i	O
)	O
(	O
where	O
we	O
set	B
wt	O
,	O
i	O
,	O
j	O
=	O
0	O
if	O
(	O
vt	O
,	O
j	O
,	O
vt+1	O
,	O
i	O
)	O
/∈	O
e	O
)	O
forward	O
:	O
set	B
o0	O
=	O
x	O
for	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
for	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
kt	O
set	B
at	O
,	O
i	O
=	O
(	O
cid:80	O
)	O
kt−1	O
j=1	O
wt−1	O
,	O
i	O
,	O
j	O
ot−1	O
,	O
j	O
set	B
ot	O
,	O
i	O
=	O
σ	O
(	O
at	O
,	O
i	O
)	O
backward	O
:	O
set	B
δt	O
=	O
ot	O
−	O
y	O
for	O
t	O
=	O
t	O
−	O
1	O
,	O
t	O
−	O
2	O
,	O
.	O
.	O
.	O
,	O
1	O
for	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
kt	O
δt	O
,	O
i	O
=	O
(	O
cid:80	O
)	O
kt+1	O
j=1	O
wt	O
,	O
j	O
,	O
i	O
δt+1	O
,	O
j	O
σ	O
(	O
cid:48	O
)	O
(	O
at+1	O
,	O
j	O
)	O
output	O
:	O
foreach	O
edge	O
(	O
vt−1	O
,	O
j	O
,	O
vt	O
,	O
i	O
)	O
∈	O
e	O
set	B
the	O
partial	O
derivative	O
to	O
δt	O
,	O
i	O
σ	O
(	O
cid:48	O
)	O
(	O
at	O
,	O
i	O
)	O
ot−1	O
,	O
j	O
20.6	O
sgd	O
and	O
backpropagation	B
279	O
explaining	O
how	O
backpropagation	B
calculates	O
the	O
gradient	B
:	O
we	O
next	O
explain	O
how	O
the	O
backpropagation	B
algorithm	O
calculates	O
the	O
gradient	B
of	O
the	O
loss	B
function	I
on	O
an	O
example	O
(	O
x	O
,	O
y	O
)	O
with	O
respect	O
to	O
the	O
vector	O
w.	O
let	O
us	O
ﬁrst	O
recall	B
a	O
few	O
deﬁnitions	O
from	O
vector	O
calculus	O
.	O
each	O
element	O
of	O
the	O
gradient	B
is	O
the	O
partial	O
derivative	O
with	O
respect	O
to	O
the	O
variable	O
in	O
w	O
corresponding	O
to	O
one	O
of	O
the	O
edges	O
of	O
the	O
network	O
.	O
recall	B
the	O
deﬁnition	O
of	O
a	O
partial	O
derivative	O
.	O
given	O
a	O
function	B
f	O
:	O
rn	O
→	O
r	O
,	O
the	O
partial	O
derivative	O
with	O
respect	O
to	O
the	O
ith	O
variable	O
at	O
w	O
is	O
obtained	O
by	O
ﬁxing	O
the	O
values	O
of	O
w1	O
,	O
.	O
.	O
.	O
,	O
wi−1	O
,	O
wi+1	O
,	O
wn	O
,	O
which	O
yields	O
the	O
scalar	O
function	B
g	O
:	O
r	O
→	O
r	O
deﬁned	O
by	O
g	O
(	O
a	O
)	O
=	O
f	O
(	O
(	O
w1	O
,	O
.	O
.	O
.	O
,	O
wi−1	O
,	O
wi	O
+	O
a	O
,	O
wi+1	O
,	O
.	O
.	O
.	O
,	O
wn	O
)	O
)	O
,	O
and	O
then	O
taking	O
the	O
derivative	O
of	O
g	O
at	O
0.	O
for	O
a	O
function	B
with	O
multiple	O
outputs	O
,	O
f	O
:	O
rn	O
→	O
rm	O
,	O
the	O
jacobian	O
of	O
f	O
at	O
w	O
∈	O
rn	O
,	O
denoted	O
jw	O
(	O
f	O
)	O
,	O
is	O
the	O
m	O
×	O
n	O
matrix	O
whose	O
i	O
,	O
j	O
element	O
is	O
the	O
partial	O
derivative	O
of	O
fi	O
:	O
rn	O
→	O
r	O
w.r.t	O
.	O
its	O
jth	O
variable	O
at	O
w.	O
note	O
that	O
if	O
m	O
=	O
1	O
then	O
the	O
jacobian	O
matrix	O
is	O
the	O
gradient	B
of	O
the	O
function	B
(	O
represented	O
as	O
a	O
row	O
vector	O
)	O
.	O
two	O
examples	O
of	O
jacobian	O
calculations	O
,	O
which	O
we	O
will	O
later	O
use	O
,	O
are	O
as	O
follows	O
.	O
•	O
let	O
f	O
(	O
w	O
)	O
=	O
aw	O
for	O
a	O
∈	O
rm	O
,	O
n	O
.	O
then	O
jw	O
(	O
f	O
)	O
=	O
a	O
.	O
•	O
for	O
every	O
n	O
,	O
we	O
use	O
the	O
notation	O
σ	O
to	O
denote	O
the	O
function	B
from	O
rn	O
to	O
rn	O
which	O
applies	O
the	O
sigmoid	O
function	B
element-wise	O
.	O
that	O
is	O
,	O
α	O
=	O
σ	O
(	O
θ	O
)	O
means	O
1+exp	O
(	O
−θi	O
)	O
.	O
it	O
is	O
easy	O
to	O
verify	O
that	O
for	O
every	O
i	O
we	O
have	O
αi	O
=	O
σ	O
(	O
θi	O
)	O
=	O
that	O
jθ	O
(	O
σ	O
)	O
is	O
a	O
diagonal	O
matrix	O
whose	O
(	O
i	O
,	O
i	O
)	O
entry	O
is	O
σ	O
(	O
cid:48	O
)	O
(	O
θi	O
)	O
,	O
where	O
σ	O
(	O
cid:48	O
)	O
is	O
the	O
derivative	O
function	B
of	O
the	O
(	O
scalar	O
)	O
sigmoid	O
function	B
,	O
namely	O
,	O
σ	O
(	O
cid:48	O
)	O
(	O
θi	O
)	O
=	O
(	O
1+exp	O
(	O
θi	O
)	O
)	O
(	O
1+exp	O
(	O
−θi	O
)	O
)	O
.	O
we	O
also	O
use	O
the	O
notation	O
diag	O
(	O
σ	O
(	O
cid:48	O
)	O
(	O
θ	O
)	O
)	O
to	O
denote	O
this	O
matrix	O
.	O
1	O
1	O
the	O
chain	O
rule	O
for	O
taking	O
the	O
derivative	O
of	O
a	O
composition	O
of	O
functions	O
can	O
be	O
written	O
in	O
terms	O
of	O
the	O
jacobian	O
as	O
follows	O
.	O
given	O
two	O
functions	O
f	O
:	O
rn	O
→	O
rm	O
and	O
g	O
:	O
rk	O
→	O
rn	O
,	O
we	O
have	O
that	O
the	O
jacobian	O
of	O
the	O
composition	O
function	B
,	O
(	O
f	O
◦	O
g	O
)	O
:	O
rk	O
→	O
rm	O
,	O
at	O
w	O
,	O
is	O
jw	O
(	O
f	O
◦	O
g	O
)	O
=	O
jg	O
(	O
w	O
)	O
(	O
f	O
)	O
jw	O
(	O
g	O
)	O
.	O
for	O
example	O
,	O
for	O
g	O
(	O
w	O
)	O
=	O
aw	O
,	O
where	O
a	O
∈	O
rn	O
,	O
k	O
,	O
we	O
have	O
that	O
jw	O
(	O
σ	O
◦	O
g	O
)	O
=	O
diag	O
(	O
σ	O
(	O
cid:48	O
)	O
(	O
aw	O
)	O
)	O
a.	O
to	O
describe	O
the	O
backpropagation	B
algorithm	O
,	O
let	O
us	O
ﬁrst	O
decompose	O
v	O
into	O
the	O
layers	O
of	O
the	O
graph	O
,	O
v	O
=	O
·∪t	O
t=0vt	O
.	O
for	O
every	O
t	O
,	O
let	O
us	O
write	O
vt	O
=	O
{	O
vt,1	O
,	O
.	O
.	O
.	O
,	O
vt	O
,	O
kt	O
}	O
,	O
where	O
kt	O
=	O
|vt|	O
.	O
in	O
addition	O
,	O
for	O
every	O
t	O
denote	O
wt	O
∈	O
rkt+1	O
,	O
kt	O
a	O
matrix	O
which	O
gives	O
a	O
weight	O
to	O
every	O
potential	O
edge	O
between	O
vt	O
and	O
vt+1	O
.	O
if	O
the	O
edge	O
exists	O
in	O
e	O
then	O
we	O
set	B
wt	O
,	O
i	O
,	O
j	O
to	O
be	O
the	O
weight	O
,	O
according	O
to	O
w	O
,	O
of	O
the	O
edge	O
(	O
vt	O
,	O
j	O
,	O
vt+1	O
,	O
i	O
)	O
.	O
otherwise	O
,	O
we	O
add	O
a	O
“	O
phantom	O
”	O
edge	O
and	O
set	B
its	O
weight	O
to	O
be	O
zero	O
,	O
wt	O
,	O
i	O
,	O
j	O
=	O
0.	O
since	O
when	O
calculating	O
the	O
partial	O
derivative	O
with	O
respect	O
to	O
the	O
weight	O
of	O
some	O
edge	O
we	O
ﬁx	O
all	O
other	O
weights	O
,	O
these	O
additional	O
“	O
phantom	O
”	O
edges	O
have	O
no	O
eﬀect	O
on	O
the	O
partial	O
derivative	O
with	O
respect	O
to	O
existing	O
edges	O
.	O
it	O
follows	O
that	O
we	O
can	O
assume	O
,	O
without	O
loss	B
of	O
generality	O
,	O
that	O
all	O
edges	O
exist	O
,	O
that	O
is	O
,	O
e	O
=	O
∪t	O
(	O
vt×vt+1	O
)	O
.	O
280	O
neural	B
networks	I
next	O
,	O
we	O
discuss	O
how	O
to	O
calculate	O
the	O
partial	O
derivatives	O
with	O
respect	O
to	O
the	O
edges	O
from	O
vt−1	O
to	O
vt	O
,	O
namely	O
,	O
with	O
respect	O
to	O
the	O
elements	O
in	O
wt−1	O
.	O
since	O
we	O
ﬁx	O
all	O
other	O
weights	O
of	O
the	O
network	O
,	O
it	O
follows	O
that	O
the	O
outputs	O
of	O
all	O
the	O
neurons	O
in	O
vt−1	O
are	O
ﬁxed	O
numbers	O
which	O
do	O
not	O
depend	O
on	O
the	O
weights	O
in	O
wt−1	O
.	O
denote	O
the	O
corresponding	O
vector	O
by	O
ot−1	O
.	O
in	O
addition	O
,	O
let	O
us	O
denote	O
by	O
(	O
cid:96	O
)	O
t	O
:	O
rkt	O
→	O
r	O
the	O
loss	B
function	I
of	O
the	O
subnetwork	O
deﬁned	O
by	O
layers	O
vt	O
,	O
.	O
.	O
.	O
,	O
vt	O
as	O
a	O
function	B
of	O
the	O
outputs	O
of	O
the	O
neurons	O
in	O
vt.	O
the	O
input	O
to	O
the	O
neurons	O
of	O
vt	O
can	O
be	O
written	O
as	O
at	O
=	O
wt−1ot−1	O
and	O
the	O
output	O
of	O
the	O
neurons	O
of	O
vt	O
is	O
ot	O
=	O
σ	O
(	O
at	O
)	O
.	O
that	O
is	O
,	O
for	O
every	O
j	O
we	O
have	O
ot	O
,	O
j	O
=	O
σ	O
(	O
at	O
,	O
j	O
)	O
.	O
we	O
obtain	O
that	O
the	O
loss	B
,	O
as	O
a	O
function	B
of	O
wt−1	O
,	O
can	O
be	O
written	O
as	O
gt	O
(	O
wt−1	O
)	O
=	O
(	O
cid:96	O
)	O
t	O
(	O
ot	O
)	O
=	O
(	O
cid:96	O
)	O
t	O
(	O
σ	O
(	O
at	O
)	O
)	O
=	O
(	O
cid:96	O
)	O
t	O
(	O
σ	O
(	O
wt−1ot−1	O
)	O
)	O
.	O
it	O
would	O
be	O
convenient	O
to	O
rewrite	O
this	O
as	O
follows	O
.	O
let	O
wt−1	O
∈	O
rkt−1kt	O
be	O
the	O
column	O
vector	O
obtained	O
by	O
concatenating	O
the	O
rows	O
of	O
wt−1	O
and	O
then	O
taking	O
the	O
transpose	O
of	O
the	O
resulting	O
long	O
vector	O
.	O
deﬁne	O
by	O
ot−1	O
the	O
kt	O
×	O
(	O
kt−1kt	O
)	O
matrix	O
	O
o	O
(	O
cid:62	O
)	O
t−1	O
0	O
...	O
0	O
	O
.	O
0	O
o	O
(	O
cid:62	O
)	O
t−1	O
...	O
0	O
···	O
···	O
.	O
.	O
.	O
···	O
0	O
0	O
...	O
o	O
(	O
cid:62	O
)	O
t−1	O
ot−1	O
=	O
(	O
20.2	O
)	O
then	O
,	O
wt−1ot−1	O
=	O
ot−1wt−1	O
,	O
so	O
we	O
can	O
also	O
write	O
gt	O
(	O
wt−1	O
)	O
=	O
(	O
cid:96	O
)	O
t	O
(	O
σ	O
(	O
ot−1	O
wt−1	O
)	O
)	O
.	O
therefore	O
,	O
applying	O
the	O
chain	O
rule	O
,	O
we	O
obtain	O
that	O
jwt−1	O
(	O
gt	O
)	O
=	O
jσ	O
(	O
ot−1wt−1	O
)	O
(	O
(	O
cid:96	O
)	O
t	O
)	O
diag	O
(	O
σ	O
(	O
cid:48	O
)	O
(	O
ot−1wt−1	O
)	O
)	O
ot−1	O
.	O
using	O
our	O
notation	O
we	O
have	O
ot	O
=	O
σ	O
(	O
ot−1wt−1	O
)	O
and	O
at	O
=	O
ot−1wt−1	O
,	O
which	O
yields	O
jwt−1	O
(	O
gt	O
)	O
=	O
jot	O
(	O
(	O
cid:96	O
)	O
t	O
)	O
diag	O
(	O
σ	O
(	O
cid:48	O
)	O
(	O
at	O
)	O
)	O
ot−1	O
.	O
let	O
us	O
also	O
denote	O
δt	O
=	O
jot	O
(	O
(	O
cid:96	O
)	O
t	O
)	O
.	O
then	O
,	O
we	O
can	O
further	O
rewrite	O
the	O
preceding	O
as	O
t−1	O
,	O
.	O
.	O
.	O
,	O
δt	O
,	O
kt	O
σ	O
(	O
cid:48	O
)	O
(	O
at	O
,	O
kt	O
)	O
o	O
(	O
cid:62	O
)	O
t−1	O
(	O
20.3	O
)	O
jwt−1	O
(	O
gt	O
)	O
=	O
(	O
cid:0	O
)	O
δt,1	O
σ	O
(	O
cid:48	O
)	O
(	O
at,1	O
)	O
o	O
(	O
cid:62	O
)	O
(	O
cid:1	O
)	O
.	O
it	O
is	O
left	O
to	O
calculate	O
the	O
vector	O
δt	O
=	O
jot	O
(	O
(	O
cid:96	O
)	O
t	O
)	O
for	O
every	O
t.	O
this	O
is	O
the	O
gradient	B
of	O
(	O
cid:96	O
)	O
t	O
at	O
ot	O
.	O
we	O
calculate	O
this	O
in	O
a	O
recursive	O
manner	O
.	O
first	O
observe	O
that	O
for	O
the	O
last	O
layer	O
we	O
have	O
that	O
(	O
cid:96	O
)	O
t	O
(	O
u	O
)	O
=	O
∆	O
(	O
u	O
,	O
y	O
)	O
,	O
where	O
∆	O
is	O
the	O
loss	B
function	I
.	O
since	O
we	O
2	O
(	O
cid:107	O
)	O
u−y	O
(	O
cid:107	O
)	O
2	O
we	O
obtain	O
that	O
ju	O
(	O
(	O
cid:96	O
)	O
t	O
)	O
=	O
(	O
u−y	O
)	O
.	O
in	O
particular	O
,	O
assume	O
that	O
∆	O
(	O
u	O
,	O
y	O
)	O
=	O
1	O
δt	O
=	O
jot	O
(	O
(	O
cid:96	O
)	O
t	O
)	O
=	O
(	O
ot	O
−	O
y	O
)	O
.	O
next	O
,	O
note	O
that	O
(	O
cid:96	O
)	O
t	O
(	O
u	O
)	O
=	O
(	O
cid:96	O
)	O
t+1	O
(	O
σ	O
(	O
wtu	O
)	O
)	O
.	O
therefore	O
,	O
by	O
the	O
chain	O
rule	O
,	O
ju	O
(	O
(	O
cid:96	O
)	O
t	O
)	O
=	O
jσ	O
(	O
wtu	O
)	O
(	O
(	O
cid:96	O
)	O
t+1	O
)	O
diag	O
(	O
σ	O
(	O
cid:48	O
)	O
(	O
wtu	O
)	O
)	O
wt	O
.	O
20.7	O
summary	O
281	O
in	O
particular	O
,	O
δt	O
=	O
jot	O
(	O
(	O
cid:96	O
)	O
t	O
)	O
=	O
jσ	O
(	O
wtot	O
)	O
(	O
(	O
cid:96	O
)	O
t+1	O
)	O
diag	O
(	O
σ	O
(	O
cid:48	O
)	O
(	O
wtot	O
)	O
)	O
wt	O
=	O
jot+1	O
(	O
(	O
cid:96	O
)	O
t+1	O
)	O
diag	O
(	O
σ	O
(	O
cid:48	O
)	O
(	O
at+1	O
)	O
)	O
wt	O
=	O
δt+1	O
diag	O
(	O
σ	O
(	O
cid:48	O
)	O
(	O
at+1	O
)	O
)	O
wt	O
.	O
in	O
summary	O
,	O
we	O
can	O
ﬁrst	O
calculate	O
the	O
vectors	O
{	O
at	O
,	O
ot	O
}	O
from	O
the	O
bottom	O
of	O
the	O
network	O
to	O
its	O
top	O
.	O
then	O
,	O
we	O
calculate	O
the	O
vectors	O
{	O
δt	O
}	O
from	O
the	O
top	O
of	O
the	O
network	O
back	O
to	O
its	O
bottom	O
.	O
once	O
we	O
have	O
all	O
of	O
these	O
vectors	O
,	O
the	O
partial	O
derivatives	O
are	O
easily	O
obtained	O
using	O
equation	O
(	O
20.3	O
)	O
.	O
we	O
have	O
thus	O
shown	O
that	O
the	O
pseudocode	O
of	O
backpropagation	B
indeed	O
calculates	O
the	O
gradient	B
.	O
20.7	O
summary	O
classes	O
of	O
all	O
predictors	O
that	O
can	O
be	O
implemented	O
in	O
runtime	O
of	O
o	O
(	O
(	O
cid:112	O
)	O
s	O
(	O
n	O
)	O
)	O
.	O
we	O
neural	B
networks	I
over	O
graphs	O
of	O
size	O
s	O
(	O
n	O
)	O
can	O
be	O
used	O
to	O
describe	O
hypothesis	B
have	O
also	O
shown	O
that	O
their	O
sample	B
complexity	I
depends	O
polynomially	O
on	O
s	O
(	O
n	O
)	O
(	O
speciﬁcally	O
,	O
it	O
depends	O
on	O
the	O
number	O
of	O
edges	O
in	O
the	O
network	O
)	O
.	O
therefore	O
,	O
classes	O
of	O
neural	O
network	O
hypotheses	O
seem	O
to	O
be	O
an	O
excellent	O
choice	O
.	O
regrettably	O
,	O
the	O
problem	O
of	O
training	O
the	O
network	O
on	O
the	O
basis	O
of	O
training	O
data	O
is	O
computationally	O
hard	O
.	O
we	O
have	O
presented	O
the	O
sgd	O
framework	O
as	O
a	O
heuristic	O
approach	O
for	O
training	O
neural	O
networks	O
and	O
described	O
the	O
backpropagation	B
algorithm	O
which	O
eﬃciently	O
calculates	O
the	O
gradient	B
of	O
the	O
loss	B
function	I
with	O
respect	O
to	O
the	O
weights	O
over	O
the	O
edges	O
.	O
20.8	O
bibliographic	O
remarks	O
neural	B
networks	I
were	O
extensively	O
studied	O
in	O
the	O
1980s	O
and	O
early	O
1990s	O
,	O
but	O
with	O
mixed	O
empirical	O
success	O
.	O
in	O
recent	O
years	O
,	O
a	O
combination	O
of	O
algorithmic	O
advance-	O
ments	O
,	O
as	O
well	O
as	O
increasing	O
computational	O
power	O
and	O
data	O
size	O
,	O
has	O
led	O
to	O
a	O
breakthrough	O
in	O
the	O
eﬀectiveness	O
of	O
neural	B
networks	I
.	O
in	O
particular	O
,	O
“	O
deep	O
net-	O
works	O
”	O
(	O
i.e.	O
,	O
networks	O
of	O
more	O
than	O
2	O
layers	O
)	O
have	O
shown	O
very	O
impressive	O
practical	O
performance	O
on	O
a	O
variety	O
of	O
domains	O
.	O
a	O
few	O
examples	O
include	O
convolutional	O
net-	O
works	O
(	O
lecun	O
&	O
bengio	O
1995	O
)	O
,	O
restricted	O
boltzmann	O
machines	O
(	O
hinton	O
,	O
osindero	O
&	O
teh	O
2006	O
)	O
,	O
auto-encoders	B
(	O
ranzato	O
,	O
huang	O
,	O
boureau	O
&	O
lecun	O
2007	O
,	O
bengio	O
&	O
lecun	O
2007	O
,	O
collobert	O
&	O
weston	O
2008	O
,	O
lee	O
,	O
grosse	O
,	O
ranganath	O
&	O
ng	O
2009	O
,	O
le	O
,	O
ranzato	O
,	O
monga	O
,	O
devin	O
,	O
corrado	O
,	O
chen	O
,	O
dean	O
&	O
ng	O
2012	O
)	O
,	O
and	O
sum-product	O
networks	O
(	O
livni	O
,	O
shalev-shwartz	O
&	O
shamir	O
2013	O
,	O
poon	O
&	O
domingos	O
2011	O
)	O
.	O
see	O
also	O
(	O
bengio	O
2009	O
)	O
and	O
the	O
references	O
therein	O
.	O
the	O
expressive	O
power	O
of	O
neural	B
networks	I
and	O
the	O
relation	O
to	O
circuit	O
complexity	O
have	O
been	O
extensively	O
studied	O
in	O
(	O
parberry	O
1994	O
)	O
.	O
for	O
the	O
analysis	O
of	O
the	O
sample	B
complexity	I
of	O
neural	B
networks	I
we	O
refer	O
the	O
reader	O
to	O
(	O
anthony	O
&	O
bartlet	O
1999	O
)	O
.	O
our	O
proof	O
technique	O
of	O
theorem	O
20.6	O
is	O
due	O
to	O
kakade	O
and	O
tewari	O
lecture	O
notes	O
.	O
282	O
neural	B
networks	I
klivans	O
&	O
sherstov	O
(	O
2006	O
)	O
have	O
shown	O
that	O
for	O
any	O
c	O
>	O
0	O
,	O
intersections	O
of	O
nc	O
halfspaces	O
over	O
{	O
±1	O
}	O
n	O
are	O
not	O
eﬃciently	O
pac	O
learnable	O
,	O
even	O
if	O
we	O
allow	O
repre-	O
sentation	O
independent	O
learning	O
.	O
this	O
hardness	O
result	O
relies	O
on	O
the	O
cryptographic	O
assumption	O
that	O
there	O
is	O
no	O
polynomial	O
time	O
solution	O
to	O
the	O
unique-shortest-	O
vector	O
problem	O
.	O
as	O
we	O
have	O
argued	O
,	O
this	O
implies	O
that	O
there	O
can	O
not	O
be	O
an	O
eﬃcient	O
algorithm	O
for	O
training	O
neural	O
networks	O
,	O
even	O
if	O
we	O
allow	O
larger	O
networks	O
or	O
other	O
activation	O
functions	O
that	O
can	O
be	O
implemented	O
eﬃciently	O
.	O
the	O
backpropagation	B
algorithm	O
has	O
been	O
introduced	O
in	O
rumelhart	O
,	O
hinton	O
&	O
williams	O
(	O
1986	O
)	O
.	O
20.9	O
exercises	O
2.	O
prove	O
theorem	O
20.5	O
.	O
1.	O
neural	B
networks	I
are	O
universal	O
approximators	O
:	O
let	O
f	O
:	O
[	O
−1	O
,	O
1	O
]	O
n	O
→	O
[	O
−1	O
,	O
1	O
]	O
be	O
a	O
ρ-lipschitz	O
function	B
.	O
fix	O
some	O
	O
>	O
0.	O
construct	O
a	O
neural	O
net-	O
work	O
n	O
:	O
[	O
−1	O
,	O
1	O
]	O
n	O
→	O
[	O
−1	O
,	O
1	O
]	O
,	O
with	O
the	O
sigmoid	O
activation	B
function	I
,	O
such	O
that	O
for	O
every	O
x	O
∈	O
[	O
−1	O
,	O
1	O
]	O
n	O
it	O
holds	O
that	O
|f	O
(	O
x	O
)	O
−	O
n	O
(	O
x	O
)	O
|	O
≤	O
	O
.	O
hint	O
:	O
similarly	O
to	O
the	O
proof	O
of	O
theorem	O
19.3	O
,	O
partition	O
[	O
−1	O
,	O
1	O
]	O
n	O
into	O
small	O
boxes	O
.	O
use	O
the	O
lipschitzness	O
of	O
f	O
to	O
show	O
that	O
it	O
is	O
approximately	O
constant	O
at	O
each	O
box	O
.	O
finally	O
,	O
show	O
that	O
a	O
neural	O
network	O
can	O
ﬁrst	O
decide	O
which	O
box	O
the	O
input	O
vector	O
belongs	O
to	O
,	O
and	O
then	O
predict	O
the	O
averaged	O
value	O
of	O
f	O
at	O
that	O
box	O
.	O
hint	O
:	O
for	O
every	O
f	O
:	O
{	O
−1	O
,	O
1	O
}	O
n	O
→	O
{	O
−1	O
,	O
1	O
}	O
construct	O
a	O
1-lipschitz	O
function	B
g	O
:	O
[	O
−1	O
,	O
1	O
]	O
n	O
→	O
[	O
−1	O
,	O
1	O
]	O
such	O
that	O
if	O
you	O
can	O
approximate	O
g	O
then	O
you	O
can	O
express	O
f	O
.	O
3.	O
growth	B
function	I
of	O
product	O
:	O
for	O
i	O
=	O
1	O
,	O
2	O
,	O
let	O
fi	O
be	O
a	O
set	B
of	O
functions	O
from	O
x	O
to	O
yi	O
.	O
deﬁne	O
h	O
=	O
f1	O
×	O
f2	O
to	O
be	O
the	O
cartesian	O
product	O
class	O
.	O
that	O
is	O
,	O
for	O
every	O
f1	O
∈	O
f1	O
and	O
f2	O
∈	O
f2	O
,	O
there	O
exists	O
h	O
∈	O
h	O
such	O
that	O
h	O
(	O
x	O
)	O
=	O
(	O
f1	O
(	O
x	O
)	O
,	O
f2	O
(	O
x	O
)	O
)	O
.	O
prove	O
that	O
τh	O
(	O
m	O
)	O
≤	O
τf1	O
(	O
m	O
)	O
τf2	O
(	O
m	O
)	O
.	O
4.	O
growth	B
function	I
of	O
composition	O
:	O
let	O
f1	O
be	O
a	O
set	B
of	O
functions	O
from	O
x	O
to	O
z	O
and	O
let	O
f2	O
be	O
a	O
set	B
of	O
functions	O
from	O
z	O
to	O
y.	O
let	O
h	O
=	O
f2	O
◦	O
f1	O
be	O
the	O
composition	O
class	O
.	O
that	O
is	O
,	O
for	O
every	O
f1	O
∈	O
f1	O
and	O
f2	O
∈	O
f2	O
,	O
there	O
exists	O
h	O
∈	O
h	O
such	O
that	O
h	O
(	O
x	O
)	O
=	O
f2	O
(	O
f1	O
(	O
x	O
)	O
)	O
.	O
prove	O
that	O
τh	O
(	O
m	O
)	O
≤	O
τf2	O
(	O
m	O
)	O
τf1	O
(	O
m	O
)	O
.	O
5.	O
vc	O
of	O
sigmoidal	O
networks	O
:	O
in	O
this	O
exercise	O
we	O
show	O
that	O
there	O
is	O
a	O
graph	O
(	O
v	O
,	O
e	O
)	O
such	O
that	O
the	O
vc	O
dimension	B
of	O
the	O
class	O
of	O
neural	B
networks	I
over	O
these	O
graphs	O
with	O
the	O
sigmoid	O
activation	B
function	I
is	O
ω	O
(	O
|e|2	O
)	O
.	O
note	O
that	O
for	O
every	O
	O
>	O
function	B
,	O
1	O
[	O
(	O
cid:80	O
)	O
0	O
,	O
the	O
sigmoid	O
activation	B
function	I
can	O
approximate	O
the	O
threshold	O
activation	O
i	O
xi	O
]	O
,	O
up	O
to	O
accuracy	B
	O
.	O
to	O
simplify	O
the	O
presentation	O
,	O
throughout	O
1	O
[	O
(	O
cid:80	O
)	O
the	O
exercise	O
we	O
assume	O
that	O
we	O
can	O
exactly	O
implement	O
the	O
activation	B
function	I
i	O
xi	O
>	O
0	O
]	O
using	O
a	O
sigmoid	O
activation	B
function	I
.	O
fix	O
some	O
n.	O
1.	O
construct	O
a	O
network	O
,	O
n1	O
,	O
with	O
o	O
(	O
n	O
)	O
weights	O
,	O
which	O
implements	O
a	O
function	B
from	O
r	O
to	O
{	O
0	O
,	O
1	O
}	O
n	O
and	O
satisﬁes	O
the	O
following	O
property	O
.	O
for	O
every	O
x	O
∈	O
{	O
0	O
,	O
1	O
}	O
n	O
,	O
20.9	O
exercises	O
283	O
if	O
we	O
feed	O
the	O
network	O
with	O
the	O
real	O
number	O
0.x1x2	O
.	O
.	O
.	O
xn	O
,	O
then	O
the	O
output	O
of	O
the	O
network	O
will	O
be	O
x.	O
hint	O
:	O
denote	O
α	O
=	O
0.x1x2	O
.	O
.	O
.	O
xn	O
and	O
observe	O
that	O
10kα	O
−	O
0.5	O
is	O
at	O
least	O
0.5	O
if	O
xk	O
=	O
1	O
and	O
is	O
at	O
most	O
−0.3	O
if	O
xk	O
=	O
−1	O
.	O
2.	O
construct	O
a	O
network	O
,	O
n2	O
,	O
with	O
o	O
(	O
n	O
)	O
weights	O
,	O
which	O
implements	O
a	O
function	B
from	O
[	O
n	O
]	O
to	O
{	O
0	O
,	O
1	O
}	O
n	O
such	O
that	O
n2	O
(	O
i	O
)	O
=	O
ei	O
for	O
all	O
i.	O
that	O
is	O
,	O
upon	O
receiving	O
the	O
input	O
i	O
,	O
the	O
network	O
outputs	O
the	O
vector	O
of	O
all	O
zeros	O
except	O
1	O
at	O
the	O
i	O
’	O
th	O
neuron	O
.	O
3.	O
let	O
α1	O
,	O
.	O
.	O
.	O
,	O
αn	O
be	O
n	O
real	O
numbers	O
such	O
that	O
every	O
αi	O
is	O
of	O
the	O
form	O
0.a	O
(	O
i	O
)	O
1	O
a	O
(	O
i	O
)	O
j	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
construct	O
a	O
network	O
,	O
n3	O
,	O
with	O
o	O
(	O
n	O
)	O
weights	O
,	O
which	O
im-	O
with	O
a	O
(	O
i	O
)	O
plements	O
a	O
function	B
from	O
[	O
n	O
]	O
to	O
r	O
,	O
and	O
satisﬁes	O
n2	O
(	O
i	O
)	O
=	O
αi	O
for	O
every	O
i	O
∈	O
[	O
n	O
]	O
.	O
4.	O
combine	O
n1	O
,	O
n3	O
to	O
obtain	O
a	O
network	O
that	O
receives	O
i	O
∈	O
[	O
n	O
]	O
and	O
output	O
a	O
(	O
i	O
)	O
.	O
5.	O
construct	O
a	O
network	O
n4	O
that	O
receives	O
(	O
i	O
,	O
j	O
)	O
∈	O
[	O
n	O
]	O
×	O
[	O
n	O
]	O
and	O
outputs	O
a	O
(	O
i	O
)	O
j	O
.	O
hint	O
:	O
observe	O
that	O
the	O
and	O
function	B
over	O
{	O
0	O
,	O
1	O
}	O
2	O
can	O
be	O
calculated	O
using	O
o	O
(	O
1	O
)	O
weights	O
.	O
2	O
.	O
.	O
.	O
a	O
(	O
i	O
)	O
n	O
,	O
6.	O
conclude	O
that	O
there	O
is	O
a	O
graph	O
with	O
o	O
(	O
n	O
)	O
weights	O
such	O
that	O
the	O
vc	O
di-	O
mension	O
of	O
the	O
resulting	O
hypothesis	B
class	I
is	O
n2	O
.	O
6.	O
prove	O
theorem	O
20.7.	O
hint	O
:	O
the	O
proof	O
is	O
similar	O
to	O
the	O
hardness	O
of	O
learning	O
intersections	O
of	O
halfs-	O
paces	O
–	O
see	O
exercise	O
32	O
in	O
chapter	O
8.	O
part	O
iii	O
additional	O
learning	O
models	O
21	O
online	B
learning	I
in	O
this	O
chapter	O
we	O
describe	O
a	O
diﬀerent	O
model	O
of	O
learning	O
,	O
which	O
is	O
called	O
online	B
learning	I
.	O
previously	O
,	O
we	O
studied	O
the	O
pac	O
learning	O
model	O
,	O
in	O
which	O
the	O
learner	O
ﬁrst	O
receives	O
a	O
batch	O
of	O
training	O
examples	O
,	O
uses	O
the	O
training	B
set	I
to	O
learn	O
a	O
hy-	O
pothesis	O
,	O
and	O
only	O
when	O
learning	O
is	O
completed	O
uses	O
the	O
learned	O
hypothesis	B
for	O
predicting	O
the	O
label	B
of	O
new	O
examples	O
.	O
in	O
our	O
papayas	O
learning	O
problem	O
,	O
this	O
means	O
that	O
we	O
should	O
ﬁrst	O
buy	O
a	O
bunch	O
of	O
papayas	O
and	O
taste	O
them	O
all	O
.	O
then	O
,	O
we	O
use	O
all	O
of	O
this	O
information	O
to	O
learn	O
a	O
prediction	O
rule	O
that	O
determines	O
the	O
taste	O
of	O
new	O
papayas	O
.	O
in	O
contrast	O
,	O
in	O
online	B
learning	I
there	O
is	O
no	O
separation	O
between	O
a	O
training	O
phase	O
and	O
a	O
prediction	O
phase	O
.	O
instead	O
,	O
each	O
time	O
we	O
buy	O
a	O
papaya	O
,	O
it	O
is	O
ﬁrst	O
considered	O
a	O
test	O
example	O
since	O
we	O
should	O
predict	O
whether	O
it	O
is	O
going	O
to	O
taste	O
good	O
.	O
then	O
,	O
after	O
taking	O
a	O
bite	O
from	O
the	O
papaya	O
,	O
we	O
know	O
the	O
true	O
label	O
,	O
and	O
the	O
same	O
papaya	O
can	O
be	O
used	O
as	O
a	O
training	O
example	O
that	O
can	O
help	O
us	O
improve	O
our	O
prediction	O
mechanism	O
for	O
future	O
papayas	O
.	O
concretely	O
,	O
online	B
learning	I
takes	O
place	O
in	O
a	O
sequence	O
of	O
consecutive	O
rounds	O
.	O
on	O
each	O
online	B
round	O
,	O
the	O
learner	O
ﬁrst	O
receives	O
an	O
instance	B
(	O
the	O
learner	O
buys	O
a	O
papaya	O
and	O
knows	O
its	O
shape	O
and	O
color	O
,	O
which	O
form	O
the	O
instance	B
)	O
.	O
then	O
,	O
the	O
learner	O
is	O
required	O
to	O
predict	O
a	O
label	B
(	O
is	O
the	O
papaya	O
tasty	O
?	O
)	O
.	O
at	O
the	O
end	O
of	O
the	O
round	O
,	O
the	O
learner	O
obtains	O
the	O
correct	O
label	B
(	O
he	O
tastes	O
the	O
papaya	O
and	O
then	O
knows	O
whether	O
it	O
is	O
tasty	O
or	O
not	O
)	O
.	O
finally	O
,	O
the	O
learner	O
uses	O
this	O
information	O
to	O
improve	O
his	O
future	O
predictions	O
.	O
to	O
analyze	O
online	B
learning	I
,	O
we	O
follow	O
a	O
similar	O
route	O
to	O
our	O
study	O
of	O
pac	O
learning	O
.	O
we	O
start	O
with	O
online	B
binary	O
classiﬁcation	O
problems	O
.	O
we	O
consider	O
both	O
the	O
realizable	O
case	O
,	O
in	O
which	O
we	O
assume	O
,	O
as	O
prior	B
knowledge	I
,	O
that	O
all	O
the	O
labels	O
are	O
generated	O
by	O
some	O
hypothesis	B
from	O
a	O
given	O
hypothesis	B
class	I
,	O
and	O
the	O
unrealizable	O
case	O
,	O
which	O
corresponds	O
to	O
the	O
agnostic	O
pac	O
learning	O
model	O
.	O
in	O
particular	O
,	O
we	O
present	O
an	O
important	O
algorithm	O
called	O
weighted-majority	O
.	O
next	O
,	O
we	O
study	O
online	B
learning	I
problems	O
in	O
which	O
the	O
loss	B
function	I
is	O
convex	B
.	O
finally	O
,	O
we	O
present	O
the	O
perceptron	O
algorithm	O
as	O
an	O
example	O
of	O
the	O
use	O
of	O
surrogate	O
convex	O
loss	B
functions	O
in	O
the	O
online	B
learning	I
model	O
.	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
288	O
online	B
learning	I
21.1	O
online	B
classiﬁcation	O
in	O
the	O
realizable	O
case	O
online	B
learning	I
is	O
performed	O
in	O
a	O
sequence	O
of	O
consecutive	O
rounds	O
,	O
where	O
at	O
round	O
t	O
the	O
learner	O
is	O
given	O
an	O
instance	B
,	O
xt	O
,	O
taken	O
from	O
an	O
instance	B
domain	O
x	O
,	O
and	O
is	O
required	O
to	O
provide	O
its	O
label	B
.	O
we	O
denote	O
the	O
predicted	O
label	B
by	O
pt	O
.	O
after	O
predicting	O
the	O
label	B
,	O
the	O
correct	O
label	B
,	O
yt	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
is	O
revealed	O
to	O
the	O
learner	O
.	O
the	O
learner	O
’	O
s	O
goal	O
is	O
to	O
make	O
as	O
few	O
prediction	O
mistakes	O
as	O
possible	O
during	O
this	O
process	O
.	O
the	O
learner	O
tries	O
to	O
deduce	O
information	O
from	O
previous	O
rounds	O
so	O
as	O
to	O
improve	O
its	O
predictions	O
on	O
future	O
rounds	O
.	O
clearly	O
,	O
learning	O
is	O
hopeless	O
if	O
there	O
is	O
no	O
correlation	O
between	O
past	O
and	O
present	O
rounds	O
.	O
previously	O
in	O
the	O
book	O
,	O
we	O
studied	O
the	O
pac	O
model	O
in	O
which	O
we	O
assume	O
that	O
past	O
and	O
present	O
examples	O
are	O
sampled	O
i.i.d	O
.	O
from	O
the	O
same	O
distribution	O
source	O
.	O
in	O
the	O
online	B
learning	I
model	O
we	O
make	O
no	O
statistical	O
assumptions	O
regard-	O
ing	O
the	O
origin	O
of	O
the	O
sequence	O
of	O
examples	O
.	O
the	O
sequence	O
is	O
allowed	O
to	O
be	O
deter-	O
ministic	O
,	O
stochastic	O
,	O
or	O
even	O
adversarially	O
adaptive	O
to	O
the	O
learner	O
’	O
s	O
own	O
behavior	O
(	O
as	O
in	O
the	O
case	O
of	O
spam	O
e-mail	O
ﬁltering	O
)	O
.	O
naturally	O
,	O
an	O
adversary	O
can	O
make	O
the	O
number	O
of	O
prediction	O
mistakes	O
of	O
our	O
online	B
learning	I
algorithm	O
arbitrarily	O
large	O
.	O
for	O
example	O
,	O
the	O
adversary	O
can	O
present	O
the	O
same	O
instance	B
on	O
each	O
online	B
round	O
,	O
wait	O
for	O
the	O
learner	O
’	O
s	O
prediction	O
,	O
and	O
provide	O
the	O
opposite	O
label	B
as	O
the	O
correct	O
label	B
.	O
to	O
make	O
nontrivial	O
statements	O
we	O
must	O
further	O
restrict	O
the	O
problem	O
.	O
the	O
real-	O
izability	O
assumption	O
is	O
one	O
possible	O
natural	O
restriction	O
.	O
in	O
the	O
realizable	O
case	O
,	O
we	O
assume	O
that	O
all	O
the	O
labels	O
are	O
generated	O
by	O
some	O
hypothesis	B
,	O
h	O
(	O
cid:63	O
)	O
:	O
x	O
→	O
y.	O
fur-	O
thermore	O
,	O
h	O
(	O
cid:63	O
)	O
is	O
taken	O
from	O
a	O
hypothesis	B
class	I
h	O
,	O
which	O
is	O
known	O
to	O
the	O
learner	O
.	O
this	O
is	O
analogous	O
to	O
the	O
pac	O
learning	O
model	O
we	O
studied	O
in	O
chapter	O
3.	O
with	O
this	O
restriction	O
on	O
the	O
sequence	O
,	O
the	O
learner	O
should	O
make	O
as	O
few	O
mistakes	O
as	O
possible	O
,	O
assuming	O
that	O
both	O
h	O
(	O
cid:63	O
)	O
and	O
the	O
sequence	O
of	O
instances	O
can	O
be	O
chosen	O
by	O
an	O
ad-	O
versary	O
.	O
for	O
an	O
online	B
learning	I
algorithm	O
,	O
a	O
,	O
we	O
denote	O
by	O
ma	O
(	O
h	O
)	O
the	O
maximal	O
number	O
of	O
mistakes	O
a	O
might	O
make	O
on	O
a	O
sequence	O
of	O
examples	O
which	O
is	O
labeled	O
by	O
some	O
h	O
(	O
cid:63	O
)	O
∈	O
h.	O
we	O
emphasize	O
again	O
that	O
both	O
h	O
(	O
cid:63	O
)	O
and	O
the	O
sequence	O
of	O
instances	O
can	O
be	O
chosen	O
by	O
an	O
adversary	O
.	O
a	O
bound	O
on	O
ma	O
(	O
h	O
)	O
is	O
called	O
a	O
mistake-bound	O
and	O
we	O
will	O
study	O
how	O
to	O
design	O
algorithms	O
for	O
which	O
ma	O
(	O
h	O
)	O
is	O
minimal	O
.	O
formally	O
:	O
definition	O
21.1	O
(	O
mistake	O
bounds	O
,	O
online	B
learnability	O
)	O
let	O
h	O
be	O
a	O
hypoth-	O
esis	O
class	O
and	O
let	O
a	O
be	O
an	O
online	B
learning	I
algorithm	O
.	O
given	O
any	O
sequence	O
s	O
=	O
(	O
x1	O
,	O
h	O
(	O
cid:63	O
)	O
(	O
y1	O
)	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xt	O
,	O
h	O
(	O
cid:63	O
)	O
(	O
yt	O
)	O
)	O
,	O
where	O
t	O
is	O
any	O
integer	O
and	O
h	O
(	O
cid:63	O
)	O
∈	O
h	O
,	O
let	O
ma	O
(	O
s	O
)	O
be	O
the	O
number	O
of	O
mistakes	O
a	O
makes	O
on	O
the	O
sequence	O
s.	O
we	O
denote	O
by	O
ma	O
(	O
h	O
)	O
the	O
supremum	O
of	O
ma	O
(	O
s	O
)	O
over	O
all	O
sequences	O
of	O
the	O
above	O
form	O
.	O
a	O
bound	O
of	O
the	O
form	O
ma	O
(	O
h	O
)	O
≤	O
b	O
<	O
∞	O
is	O
called	O
a	O
mistake	B
bound	I
.	O
we	O
say	O
that	O
a	O
hypothesis	B
class	I
h	O
is	O
online	B
learnable	O
if	O
there	O
exists	O
an	O
algorithm	O
a	O
for	O
which	O
ma	O
(	O
h	O
)	O
≤	O
b	O
<	O
∞	O
.	O
our	O
goal	O
is	O
to	O
study	O
which	O
hypothesis	B
classes	O
are	O
learnable	O
in	O
the	O
online	B
model	O
,	O
and	O
in	O
particular	O
to	O
ﬁnd	O
good	O
learning	O
algorithms	O
for	O
a	O
given	O
hypothesis	B
class	I
.	O
remark	O
21.1	O
throughout	O
this	O
section	O
and	O
the	O
next	O
,	O
we	O
ignore	O
the	O
computa-	O
21.1	O
online	B
classiﬁcation	O
in	O
the	O
realizable	O
case	O
289	O
tional	O
aspect	O
of	O
learning	O
,	O
and	O
do	O
not	O
restrict	O
the	O
algorithms	O
to	O
be	O
eﬃcient	O
.	O
in	O
section	O
21.3	O
and	O
section	O
21.4	O
we	O
study	O
eﬃcient	O
online	O
learning	O
algorithms	O
.	O
to	O
simplify	O
the	O
presentation	O
,	O
we	O
start	O
with	O
the	O
case	O
of	O
a	O
ﬁnite	O
hypothesis	B
class	I
,	O
namely	O
,	O
|h|	O
<	O
∞	O
.	O
in	O
pac	O
learning	O
,	O
we	O
identiﬁed	O
erm	O
as	O
a	O
good	O
learning	O
algorithm	O
,	O
in	O
the	O
sense	O
that	O
if	O
h	O
is	O
learnable	O
then	O
it	O
is	O
learnable	O
by	O
the	O
rule	O
ermh	O
.	O
a	O
natural	O
learning	O
rule	O
for	O
online	B
learning	I
is	O
to	O
use	O
(	O
at	O
any	O
online	B
round	O
)	O
any	O
erm	O
hypothesis	B
,	O
namely	O
,	O
any	O
hypothesis	B
which	O
is	O
consistent	O
with	O
all	O
past	O
examples	O
.	O
consistent	O
input	O
:	O
a	O
ﬁnite	O
hypothesis	B
class	I
h	O
initialize	O
:	O
v1	O
=	O
h	O
for	O
t	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
receive	O
xt	O
choose	O
any	O
h	O
∈	O
vt	O
predict	O
pt	O
=	O
h	O
(	O
xt	O
)	O
receive	O
true	O
label	O
yt	O
=	O
h	O
(	O
cid:63	O
)	O
(	O
xt	O
)	O
update	O
vt+1	O
=	O
{	O
h	O
∈	O
vt	O
:	O
h	O
(	O
xt	O
)	O
=	O
yt	O
}	O
the	O
consistent	O
algorithm	O
maintains	O
a	O
set	B
,	O
vt	O
,	O
of	O
all	O
the	O
hypotheses	O
which	O
are	O
consistent	O
with	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xt−1	O
,	O
yt−1	O
)	O
.	O
this	O
set	B
is	O
often	O
called	O
the	O
version	B
space	I
.	O
it	O
then	O
picks	O
any	O
hypothesis	B
from	O
vt	O
and	O
predicts	O
according	O
to	O
this	O
hy-	O
pothesis	O
.	O
obviously	O
,	O
whenever	O
consistent	O
makes	O
a	O
prediction	O
mistake	O
,	O
at	O
least	O
one	O
hypothesis	B
is	O
removed	O
from	O
vt.	O
therefore	O
,	O
after	O
making	O
m	O
mistakes	O
we	O
have	O
|vt|	O
≤	O
|h|	O
−	O
m	O
.	O
since	O
vt	O
is	O
always	O
nonempty	O
(	O
by	O
the	O
realizability	B
assumption	O
it	O
contains	O
h	O
(	O
cid:63	O
)	O
)	O
we	O
have	O
1	O
≤	O
|vt|	O
≤	O
|h|	O
−	O
m	O
.	O
rearranging	O
,	O
we	O
obtain	O
the	O
following	O
:	O
corollary	O
21.2	O
let	O
h	O
be	O
a	O
ﬁnite	O
hypothesis	B
class	I
.	O
the	O
consistent	O
algorithm	O
enjoys	O
the	O
mistake	B
bound	I
mconsistent	O
(	O
h	O
)	O
≤	O
|h|	O
−	O
1.	O
it	O
is	O
rather	O
easy	O
to	O
construct	O
a	O
hypothesis	B
class	I
and	O
a	O
sequence	O
of	O
examples	O
on	O
which	O
consistent	O
will	O
indeed	O
make	O
|h|−	O
1	O
mistakes	O
(	O
see	O
exercise	O
1	O
)	O
.	O
therefore	O
,	O
we	O
present	O
a	O
better	O
algorithm	O
in	O
which	O
we	O
choose	O
h	O
∈	O
vt	O
in	O
a	O
smarter	O
way	O
.	O
we	O
shall	O
see	O
that	O
this	O
algorithm	O
is	O
guaranteed	O
to	O
make	O
exponentially	O
fewer	O
mistakes	O
.	O
halving	O
input	O
:	O
a	O
ﬁnite	O
hypothesis	B
class	I
h	O
initialize	O
:	O
v1	O
=	O
h	O
for	O
t	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
receive	O
xt	O
predict	O
pt	O
=	O
argmaxr∈	O
{	O
0,1	O
}	O
|	O
{	O
h	O
∈	O
vt	O
:	O
h	O
(	O
xt	O
)	O
=	O
r	O
}	O
|	O
(	O
in	O
case	O
of	O
a	O
tie	O
predict	O
pt	O
=	O
1	O
)	O
receive	O
true	O
label	O
yt	O
=	O
h	O
(	O
cid:63	O
)	O
(	O
xt	O
)	O
update	O
vt+1	O
=	O
{	O
h	O
∈	O
vt	O
:	O
h	O
(	O
xt	O
)	O
=	O
yt	O
}	O
290	O
online	B
learning	I
theorem	O
21.3	O
let	O
h	O
be	O
a	O
ﬁnite	O
hypothesis	B
class	I
.	O
the	O
halving	O
algorithm	O
enjoys	O
the	O
mistake	B
bound	I
mhalving	O
(	O
h	O
)	O
≤	O
log2	O
(	O
|h|	O
)	O
.	O
proof	O
we	O
simply	O
note	O
that	O
whenever	O
the	O
algorithm	O
errs	O
we	O
have	O
|vt+1|	O
≤	O
|vt|/2	O
,	O
(	O
hence	O
the	O
name	O
halving	O
)	O
.	O
therefore	O
,	O
if	O
m	O
is	O
the	O
total	O
number	O
of	O
mistakes	O
,	O
we	O
have	O
1	O
≤	O
|vt	O
+1|	O
≤	O
|h|	O
2−m	O
.	O
rearranging	O
this	O
inequality	O
we	O
conclude	O
our	O
proof	O
.	O
of	O
course	O
,	O
halving	O
’	O
s	O
mistake	B
bound	I
is	O
much	O
better	O
than	O
consistent	O
’	O
s	O
mistake	B
bound	I
.	O
we	O
already	O
see	O
that	O
online	B
learning	I
is	O
diﬀerent	O
from	O
pac	O
learning—while	O
in	O
pac	O
,	O
any	O
erm	O
hypothesis	B
is	O
good	O
,	O
in	O
online	B
learning	I
choosing	O
an	O
arbitrary	O
erm	O
hypothesis	B
is	O
far	O
from	O
being	O
optimal	O
.	O
21.1.1	O
online	B
learnability	O
we	O
next	O
take	O
a	O
more	O
general	O
approach	O
,	O
and	O
aim	O
at	O
characterizing	O
online	B
learn-	O
ability	O
.	O
in	O
particular	O
,	O
we	O
target	O
the	O
following	O
question	O
:	O
what	O
is	O
the	O
optimal	O
online	B
learning	I
algorithm	O
for	O
a	O
given	O
hypothesis	B
class	I
h	O
?	O
we	O
present	O
a	O
dimension	B
of	O
hypothesis	B
classes	O
that	O
characterizes	O
the	O
best	O
achiev-	O
able	O
mistake	B
bound	I
.	O
this	O
measure	O
was	O
proposed	O
by	O
nick	O
littlestone	O
and	O
we	O
therefore	O
refer	O
to	O
it	O
as	O
ldim	O
(	O
h	O
)	O
.	O
to	O
motivate	O
the	O
deﬁnition	O
of	O
ldim	O
it	O
is	O
convenient	O
to	O
view	O
the	O
online	B
learning	I
process	O
as	O
a	O
game	O
between	O
two	O
players	O
:	O
the	O
learner	O
versus	O
the	O
environment	O
.	O
on	O
round	O
t	O
of	O
the	O
game	O
,	O
the	O
environment	O
picks	O
an	O
instance	B
xt	O
,	O
the	O
learner	O
predicts	O
a	O
label	B
pt	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
and	O
ﬁnally	O
the	O
environment	O
outputs	O
the	O
true	O
label	O
,	O
yt	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
suppose	O
that	O
the	O
environment	O
wants	O
to	O
make	O
the	O
learner	O
err	O
on	O
the	O
ﬁrst	O
t	O
rounds	O
of	O
the	O
game	O
.	O
then	O
,	O
it	O
must	O
output	O
yt	O
=	O
1	O
−	O
pt	O
,	O
and	O
the	O
only	O
question	O
is	O
how	O
it	O
should	O
choose	O
the	O
instances	O
xt	O
in	O
such	O
a	O
way	O
that	O
ensures	O
that	O
for	O
some	O
h	O
(	O
cid:63	O
)	O
∈	O
h	O
we	O
have	O
yt	O
=	O
h	O
(	O
cid:63	O
)	O
(	O
xt	O
)	O
for	O
all	O
t	O
∈	O
[	O
t	O
]	O
.	O
a	O
strategy	O
for	O
an	O
adversarial	O
environment	O
can	O
be	O
formally	O
described	O
as	O
a	O
binary	O
tree	O
,	O
as	O
follows	O
.	O
each	O
node	O
of	O
the	O
tree	O
is	O
associated	O
with	O
an	O
instance	B
from	O
x	O
.	O
initially	O
,	O
the	O
environment	O
presents	O
to	O
the	O
learner	O
the	O
instance	B
associated	O
with	O
the	O
root	O
of	O
the	O
tree	O
.	O
then	O
,	O
if	O
the	O
learner	O
predicts	O
pt	O
=	O
1	O
the	O
environment	O
will	O
declare	O
that	O
this	O
is	O
a	O
wrong	O
prediction	O
(	O
i.e.	O
,	O
yt	O
=	O
0	O
)	O
and	O
will	O
traverse	O
to	O
the	O
right	O
child	O
of	O
the	O
current	O
node	O
.	O
if	O
the	O
learner	O
predicts	O
pt	O
=	O
0	O
then	O
the	O
environment	O
will	O
set	B
yt	O
=	O
1	O
and	O
will	O
traverse	O
to	O
the	O
left	O
child	O
.	O
this	O
process	O
will	O
continue	O
and	O
at	O
each	O
round	O
,	O
the	O
environment	O
will	O
present	O
the	O
instance	B
associated	O
with	O
the	O
current	O
node	O
.	O
formally	O
,	O
consider	O
a	O
complete	O
binary	O
tree	O
of	O
depth	O
t	O
(	O
we	O
deﬁne	O
the	O
depth	O
of	O
the	O
tree	O
as	O
the	O
number	O
of	O
edges	O
in	O
a	O
path	O
from	O
the	O
root	O
to	O
a	O
leaf	O
)	O
.	O
we	O
have	O
2t	O
+1	O
−	O
1	O
nodes	O
in	O
such	O
a	O
tree	O
,	O
and	O
we	O
attach	O
an	O
instance	B
to	O
each	O
node	O
.	O
let	O
v1	O
,	O
.	O
.	O
.	O
,	O
v2t	O
+1−1	O
be	O
these	O
instances	O
.	O
we	O
start	O
from	O
the	O
root	O
of	O
the	O
tree	O
,	O
and	O
set	B
x1	O
=	O
v1	O
.	O
at	O
round	O
t	O
,	O
we	O
set	B
xt	O
=	O
vit	O
where	O
it	O
is	O
the	O
current	O
node	O
.	O
at	O
the	O
end	O
of	O
21.1	O
online	B
classiﬁcation	O
in	O
the	O
realizable	O
case	O
291	O
v1	O
v2	O
v3	O
h1	O
h2	O
h3	O
h4	O
v1	O
v2	O
v3	O
0	O
0	O
∗	O
0	O
1	O
∗	O
1	O
∗	O
0	O
1	O
∗	O
1	O
figure	O
21.1	O
an	O
illustration	O
of	O
a	O
shattered	O
tree	O
of	O
depth	O
2.	O
the	O
dashed	O
path	O
corresponds	O
to	O
the	O
sequence	O
of	O
examples	O
(	O
(	O
v1	O
,	O
1	O
)	O
,	O
(	O
v3	O
,	O
0	O
)	O
)	O
.	O
the	O
tree	O
is	O
shattered	O
by	O
h	O
=	O
{	O
h1	O
,	O
h2	O
,	O
h3	O
,	O
h4	O
}	O
,	O
where	O
the	O
predictions	O
of	O
each	O
hypothesis	B
in	O
h	O
on	O
the	O
instances	O
v1	O
,	O
v2	O
,	O
v3	O
is	O
given	O
in	O
the	O
table	O
(	O
the	O
’	O
*	O
’	O
mark	O
means	O
that	O
hj	O
(	O
vi	O
)	O
can	O
be	O
either	O
1	O
or	O
0	O
)	O
.	O
is	O
,	O
it+1	O
=	O
2it	O
+yt	O
.	O
unraveling	O
the	O
recursion	O
we	O
obtain	O
it	O
=	O
2t−1	O
+	O
(	O
cid:80	O
)	O
t−1	O
round	O
t	O
,	O
we	O
go	O
to	O
the	O
left	O
child	O
of	O
it	O
if	O
yt	O
=	O
0	O
or	O
to	O
the	O
right	O
child	O
if	O
yt	O
=	O
1.	O
that	O
j=1	O
yj	O
2t−1−j	O
.	O
the	O
preceding	O
strategy	O
for	O
the	O
environment	O
succeeds	O
only	O
if	O
for	O
every	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yt	O
)	O
there	O
exists	O
h	O
∈	O
h	O
such	O
that	O
yt	O
=	O
h	O
(	O
xt	O
)	O
for	O
all	O
t	O
∈	O
[	O
t	O
]	O
.	O
this	O
leads	O
to	O
the	O
following	O
deﬁnition	O
.	O
definition	O
21.4	O
(	O
h	O
shattered	O
tree	O
)	O
a	O
shattered	O
tree	O
of	O
depth	O
d	O
is	O
a	O
sequence	O
of	O
instances	O
v1	O
,	O
.	O
.	O
.	O
,	O
v2d−1	O
in	O
x	O
such	O
that	O
for	O
every	O
labeling	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yd	O
)	O
∈	O
{	O
0	O
,	O
1	O
}	O
d	O
there	O
exists	O
h	O
∈	O
h	O
such	O
that	O
for	O
all	O
t	O
∈	O
[	O
d	O
]	O
we	O
have	O
h	O
(	O
vit	O
)	O
=	O
yt	O
where	O
it	O
=	O
2t−1	O
+	O
(	O
cid:80	O
)	O
t−1	O
j=1	O
yj	O
2t−1−j	O
.	O
an	O
illustration	O
of	O
a	O
shattered	O
tree	O
of	O
depth	O
2	O
is	O
given	O
in	O
figure	O
21.1.	O
definition	O
21.5	O
(	O
littlestone	O
’	O
s	O
dimension	B
(	O
ldim	O
)	O
)	O
ldim	O
(	O
h	O
)	O
is	O
the	O
maximal	O
integer	O
t	O
such	O
that	O
there	O
exists	O
a	O
shattered	O
tree	O
of	O
depth	O
t	O
,	O
which	O
is	O
shattered	O
by	O
h.	O
the	O
deﬁnition	O
of	O
ldim	O
and	O
the	O
discussion	O
above	O
immediately	O
imply	O
the	O
fol-	O
lowing	O
:	O
lemma	O
21.6	O
no	O
algorithm	O
can	O
have	O
a	O
mistake	B
bound	I
strictly	O
smaller	O
than	O
ldim	O
(	O
h	O
)	O
;	O
namely	O
,	O
for	O
every	O
algorithm	O
,	O
a	O
,	O
we	O
have	O
ma	O
(	O
h	O
)	O
≥	O
ldim	O
(	O
h	O
)	O
.	O
proof	O
let	O
t	O
=	O
ldim	O
(	O
h	O
)	O
and	O
let	O
v1	O
,	O
.	O
.	O
.	O
,	O
v2t	O
−1	O
be	O
a	O
sequence	O
that	O
satisﬁes	O
the	O
requirements	O
in	O
the	O
deﬁnition	O
of	O
ldim	O
.	O
if	O
the	O
environment	O
sets	O
xt	O
=	O
vit	O
and	O
yt	O
=	O
1−	O
pt	O
for	O
all	O
t	O
∈	O
[	O
t	O
]	O
,	O
then	O
the	O
learner	O
makes	O
t	O
mistakes	O
while	O
the	O
deﬁnition	O
of	O
ldim	O
implies	O
that	O
there	O
exists	O
a	O
hypothesis	B
h	O
∈	O
h	O
such	O
that	O
yt	O
=	O
h	O
(	O
xt	O
)	O
for	O
all	O
t.	O
let	O
us	O
now	O
give	O
several	O
examples	O
.	O
example	O
21.2	O
let	O
h	O
be	O
a	O
ﬁnite	O
hypothesis	B
class	I
.	O
clearly	O
,	O
any	O
tree	O
that	O
is	O
shat-	O
tered	O
by	O
h	O
has	O
depth	O
of	O
at	O
most	O
log2	O
(	O
|h|	O
)	O
.	O
therefore	O
,	O
ldim	O
(	O
h	O
)	O
≤	O
log2	O
(	O
|h|	O
)	O
.	O
another	O
way	O
to	O
conclude	O
this	O
inequality	O
is	O
by	O
combining	O
lemma	O
21.6	O
with	O
the-	O
orem	O
21.3.	O
example	O
21.3	O
let	O
x	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
}	O
and	O
h	O
=	O
{	O
h1	O
,	O
.	O
.	O
.	O
,	O
hd	O
}	O
where	O
hj	O
(	O
x	O
)	O
=	O
1	O
iﬀ	O
292	O
online	B
learning	I
x	O
=	O
j.	O
then	O
,	O
it	O
is	O
easy	O
to	O
show	O
that	O
ldim	O
(	O
h	O
)	O
=	O
1	O
while	O
|h|	O
=	O
d	O
can	O
be	O
arbitrarily	O
large	O
.	O
therefore	O
,	O
this	O
example	O
shows	O
that	O
ldim	O
(	O
h	O
)	O
can	O
be	O
signiﬁcantly	O
smaller	O
than	O
log2	O
(	O
|h|	O
)	O
.	O
example	O
21.4	O
let	O
x	O
=	O
[	O
0	O
,	O
1	O
]	O
and	O
h	O
=	O
{	O
x	O
(	O
cid:55	O
)	O
→	O
1	O
[	O
x	O
<	O
a	O
]	O
:	O
a	O
∈	O
[	O
0	O
,	O
1	O
]	O
}	O
;	O
namely	O
,	O
h	O
is	O
the	O
class	O
of	O
thresholds	O
on	O
the	O
interval	O
[	O
0	O
,	O
1	O
]	O
.	O
then	O
,	O
ldim	O
(	O
h	O
)	O
=	O
∞	O
.	O
to	O
see	O
this	O
,	O
consider	O
the	O
tree	O
1/2	O
1/4	O
3/4	O
1/8	O
3/8	O
5/8	O
7/8	O
this	O
tree	O
is	O
shattered	O
by	O
h.	O
and	O
,	O
because	O
of	O
the	O
density	O
of	O
the	O
reals	O
,	O
this	O
tree	O
can	O
be	O
made	O
arbitrarily	O
deep	O
.	O
lemma	O
21.6	O
states	O
that	O
ldim	O
(	O
h	O
)	O
lower	O
bounds	O
the	O
mistake	B
bound	I
of	O
any	O
algorithm	O
.	O
interestingly	O
,	O
there	O
is	O
a	O
standard	O
algorithm	O
whose	O
mistake	B
bound	I
matches	O
this	O
lower	O
bound	O
.	O
the	O
algorithm	O
is	O
similar	O
to	O
the	O
halving	O
algorithm	O
.	O
recall	B
that	O
the	O
prediction	O
of	O
halving	O
is	O
made	O
according	O
to	O
a	O
majority	O
vote	O
of	O
the	O
hypotheses	O
which	O
are	O
consistent	O
with	O
previous	O
examples	O
.	O
we	O
denoted	O
this	O
t	O
=	O
{	O
h	O
∈	O
vt	O
:	O
set	B
by	O
vt.	O
put	O
another	O
way	O
,	O
halving	O
partitions	O
vt	O
into	O
two	O
sets	O
:	O
v	O
+	O
h	O
(	O
xt	O
)	O
=	O
1	O
}	O
and	O
v	O
−	O
t	O
=	O
{	O
h	O
∈	O
vt	O
:	O
h	O
(	O
xt	O
)	O
=	O
0	O
}	O
.	O
it	O
then	O
predicts	O
according	O
to	O
the	O
larger	O
of	O
the	O
two	O
groups	O
.	O
the	O
rationale	O
behind	O
this	O
prediction	O
is	O
that	O
whenever	O
halving	O
makes	O
a	O
mistake	O
it	O
ends	O
up	O
with	O
|vt+1|	O
≤	O
0.5|vt|	O
.	O
the	O
optimal	O
algorithm	O
we	O
present	O
in	O
the	O
following	O
uses	O
the	O
same	O
idea	O
,	O
but	O
instead	O
of	O
predicting	O
according	O
to	O
the	O
larger	O
class	O
,	O
it	O
predicts	O
according	O
to	O
the	O
class	O
with	O
larger	O
ldim	O
.	O
standard	O
optimal	O
algorithm	O
(	O
soa	O
)	O
input	O
:	O
a	O
hypothesis	B
class	I
h	O
initialize	O
:	O
v1	O
=	O
h	O
for	O
t	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
receive	O
xt	O
for	O
r	O
∈	O
{	O
0	O
,	O
1	O
}	O
let	O
v	O
(	O
r	O
)	O
predict	O
pt	O
=	O
argmaxr∈	O
{	O
0,1	O
}	O
ldim	O
(	O
v	O
(	O
r	O
)	O
t	O
=	O
{	O
h	O
∈	O
vt	O
:	O
h	O
(	O
xt	O
)	O
=	O
r	O
}	O
)	O
t	O
(	O
in	O
case	O
of	O
a	O
tie	O
predict	O
pt	O
=	O
1	O
)	O
receive	O
true	O
label	O
yt	O
update	O
vt+1	O
=	O
{	O
h	O
∈	O
vt	O
:	O
h	O
(	O
xt	O
)	O
=	O
yt	O
}	O
the	O
following	O
lemma	O
formally	O
establishes	O
the	O
optimality	O
of	O
the	O
preceding	O
al-	O
gorithm	O
.	O
21.1	O
online	B
classiﬁcation	O
in	O
the	O
realizable	O
case	O
293	O
lemma	O
21.7	O
soa	O
enjoys	O
the	O
mistake	B
bound	I
msoa	O
(	O
h	O
)	O
≤	O
ldim	O
(	O
h	O
)	O
.	O
proof	O
it	O
suﬃces	O
to	O
prove	O
that	O
whenever	O
the	O
algorithm	O
makes	O
a	O
prediction	O
mis-	O
take	O
we	O
have	O
ldim	O
(	O
vt+1	O
)	O
≤	O
ldim	O
(	O
vt	O
)	O
−	O
1.	O
we	O
prove	O
this	O
claim	O
by	O
assuming	O
the	O
contrary	O
,	O
that	O
is	O
,	O
ldim	O
(	O
vt+1	O
)	O
=	O
ldim	O
(	O
vt	O
)	O
.	O
if	O
this	O
holds	O
true	O
,	O
then	O
the	O
deﬁnition	O
of	O
pt	O
implies	O
that	O
ldim	O
(	O
v	O
(	O
r	O
)	O
)	O
=	O
ldim	O
(	O
vt	O
)	O
for	O
both	O
r	O
=	O
1	O
and	O
r	O
=	O
0.	O
but	O
,	O
then	O
we	O
can	O
construct	O
a	O
shaterred	O
tree	O
of	O
depth	O
ldim	O
(	O
vt	O
)	O
+	O
1	O
for	O
the	O
class	O
vt	O
,	O
which	O
leads	O
to	O
the	O
desired	O
contradiction	O
.	O
t	O
combining	O
lemma	O
21.7	O
and	O
lemma	O
21.6	O
we	O
obtain	O
:	O
corollary	O
21.8	O
let	O
h	O
be	O
any	O
hypothesis	B
class	I
.	O
then	O
,	O
the	O
standard	O
optimal	O
algorithm	O
enjoys	O
the	O
mistake	B
bound	I
msoa	O
(	O
h	O
)	O
=	O
ldim	O
(	O
h	O
)	O
and	O
no	O
other	O
algorithm	O
can	O
have	O
ma	O
(	O
h	O
)	O
<	O
ldim	O
(	O
h	O
)	O
.	O
comparison	O
to	O
vc	O
dimension	B
in	O
the	O
pac	O
learning	O
model	O
,	O
learnability	O
is	O
characterized	O
by	O
the	O
vc	O
dimension	B
of	O
the	O
class	O
h.	O
recall	B
that	O
the	O
vc	O
dimension	B
of	O
a	O
class	O
h	O
is	O
the	O
maximal	O
number	O
d	O
such	O
that	O
there	O
are	O
instances	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
that	O
are	O
shattered	O
by	O
h.	O
that	O
is	O
,	O
for	O
any	O
sequence	O
of	O
labels	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yd	O
)	O
∈	O
{	O
0	O
,	O
1	O
}	O
d	O
there	O
exists	O
a	O
hypothesis	B
h	O
∈	O
h	O
that	O
gives	O
exactly	O
this	O
sequence	O
of	O
labels	O
.	O
the	O
following	O
theorem	O
relates	O
the	O
vc	O
dimension	B
to	O
the	O
littlestone	O
dimension	B
.	O
theorem	O
21.9	O
for	O
any	O
class	O
h	O
,	O
vcdim	O
(	O
h	O
)	O
≤	O
ldim	O
(	O
h	O
)	O
,	O
and	O
there	O
are	O
classes	O
for	O
which	O
strict	O
inequality	O
holds	O
.	O
furthermore	O
,	O
the	O
gap	O
can	O
be	O
arbitrarily	O
larger	O
.	O
proof	O
we	O
ﬁrst	O
prove	O
that	O
vcdim	O
(	O
h	O
)	O
≤	O
ldim	O
(	O
h	O
)	O
.	O
suppose	O
vcdim	O
(	O
h	O
)	O
=	O
d	O
and	O
let	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
be	O
a	O
shattered	O
set	B
.	O
we	O
now	O
construct	O
a	O
complete	O
binary	O
tree	O
of	O
instances	O
v1	O
,	O
.	O
.	O
.	O
,	O
v2d−1	O
,	O
where	O
all	O
nodes	O
at	O
depth	O
i	O
are	O
set	B
to	O
be	O
xi	O
–	O
see	O
the	O
following	O
illustration	O
:	O
x1	O
x2	O
x2	O
x3	O
x3	O
x3	O
x3	O
now	O
,	O
the	O
deﬁnition	O
of	O
a	O
shattered	O
set	B
clearly	O
implies	O
that	O
we	O
got	O
a	O
valid	O
shattered	O
tree	O
of	O
depth	O
d	O
,	O
and	O
we	O
conclude	O
that	O
vcdim	O
(	O
h	O
)	O
≤	O
ldim	O
(	O
h	O
)	O
.	O
to	O
show	O
that	O
the	O
gap	O
can	O
be	O
arbitrarily	O
large	O
simply	O
note	O
that	O
the	O
class	O
given	O
in	O
example	O
21.4	O
has	O
vc	O
dimension	B
of	O
1	O
whereas	O
its	O
littlestone	O
dimension	B
is	O
inﬁnite	O
.	O
294	O
online	B
learning	I
21.2	O
online	B
classiﬁcation	O
in	O
the	O
unrealizable	O
case	O
(	O
cid:34	O
)	O
t	O
(	O
cid:88	O
)	O
t=1	O
|pt	O
−	O
yt|	O
−	O
t	O
(	O
cid:88	O
)	O
t=1	O
(	O
cid:35	O
)	O
|h	O
(	O
xt	O
)	O
−	O
yt|	O
,	O
(	O
21.1	O
)	O
in	O
the	O
previous	O
section	O
we	O
studied	O
online	B
learnability	O
in	O
the	O
realizable	O
case	O
.	O
we	O
now	O
consider	O
the	O
unrealizable	O
case	O
.	O
similarly	O
to	O
the	O
agnostic	O
pac	O
model	O
,	O
we	O
no	O
longer	O
assume	O
that	O
all	O
labels	O
are	O
generated	O
by	O
some	O
h	O
(	O
cid:63	O
)	O
∈	O
h	O
,	O
but	O
we	O
require	O
the	O
learner	O
to	O
be	O
competitive	O
with	O
the	O
best	O
ﬁxed	O
predictor	B
from	O
h.	O
this	O
is	O
captured	O
by	O
the	O
regret	O
of	O
the	O
algorithm	O
,	O
which	O
measures	O
how	O
“	O
sorry	O
”	O
the	O
learner	O
is	O
,	O
in	O
retrospect	O
,	O
not	O
to	O
have	O
followed	O
the	O
predictions	O
of	O
some	O
hypothesis	B
h	O
∈	O
h.	O
formally	O
,	O
the	O
regret	O
of	O
an	O
algorithm	O
a	O
relative	O
to	O
h	O
when	O
running	O
on	O
a	O
sequence	O
of	O
t	O
examples	O
is	O
deﬁned	O
as	O
regreta	O
(	O
h	O
,	O
t	O
)	O
=	O
sup	O
(	O
x1	O
,	O
y1	O
)	O
,	O
...	O
,	O
(	O
xt	O
,	O
yt	O
)	O
and	O
the	O
regret	O
of	O
the	O
algorithm	O
relative	O
to	O
a	O
hypothesis	B
class	I
h	O
is	O
regreta	O
(	O
h	O
,	O
t	O
)	O
=	O
sup	O
h∈h	O
regreta	O
(	O
h	O
,	O
t	O
)	O
.	O
(	O
21.2	O
)	O
we	O
restate	O
the	O
learner	O
’	O
s	O
goal	O
as	O
having	O
the	O
lowest	O
possible	O
regret	O
relative	O
to	O
h.	O
an	O
interesting	O
question	O
is	O
whether	O
we	O
can	O
derive	O
an	O
algorithm	O
with	O
low	O
regret	O
,	O
meaning	O
that	O
regreta	O
(	O
h	O
,	O
t	O
)	O
grows	O
sublinearly	O
with	O
the	O
number	O
of	O
rounds	O
,	O
t	O
,	O
which	O
implies	O
that	O
the	O
diﬀerence	O
between	O
the	O
error	O
rate	O
of	O
the	O
learner	O
and	O
the	O
best	O
hypothesis	B
in	O
h	O
tends	O
to	O
zero	O
as	O
t	O
goes	O
to	O
inﬁnity	O
.	O
we	O
ﬁrst	O
show	O
that	O
this	O
is	O
an	O
impossible	O
mission—no	O
algorithm	O
can	O
obtain	O
a	O
sublinear	O
regret	O
bound	O
even	O
if	O
|h|	O
=	O
2.	O
indeed	O
,	O
consider	O
h	O
=	O
{	O
h0	O
,	O
h1	O
}	O
,	O
where	O
h0	O
is	O
the	O
function	B
that	O
always	O
returns	O
0	O
and	O
h1	O
is	O
the	O
function	B
that	O
always	O
returns	O
1.	O
an	O
adversary	O
can	O
make	O
the	O
number	O
of	O
mistakes	O
of	O
any	O
online	B
algorithm	O
be	O
equal	O
to	O
t	O
,	O
by	O
simply	O
waiting	O
for	O
the	O
learner	O
’	O
s	O
prediction	O
and	O
then	O
providing	O
the	O
opposite	O
label	B
as	O
the	O
true	O
label	O
.	O
in	O
contrast	O
,	O
for	O
any	O
sequence	O
of	O
true	O
labels	O
,	O
y1	O
,	O
.	O
.	O
.	O
,	O
yt	O
,	O
let	O
b	O
be	O
the	O
majority	O
of	O
labels	O
in	O
y1	O
,	O
.	O
.	O
.	O
,	O
yt	O
,	O
then	O
the	O
number	O
of	O
mistakes	O
of	O
hb	O
is	O
at	O
most	O
t	O
/2	O
.	O
therefore	O
,	O
the	O
regret	O
of	O
any	O
online	B
algorithm	O
might	O
be	O
at	O
least	O
t	O
−	O
t	O
/2	O
=	O
t	O
/2	O
,	O
which	O
is	O
not	O
sublinear	O
in	O
t	O
.	O
this	O
impossibility	O
result	O
is	O
attributed	O
to	O
cover	O
(	O
cover	O
1965	O
)	O
.	O
to	O
sidestep	O
cover	O
’	O
s	O
impossibility	O
result	O
,	O
we	O
must	O
further	O
restrict	O
the	O
power	O
of	O
the	O
adversarial	O
environment	O
.	O
we	O
do	O
so	O
by	O
allowing	O
the	O
learner	O
to	O
randomize	O
his	O
predictions	O
.	O
of	O
course	O
,	O
this	O
by	O
itself	O
does	O
not	O
circumvent	O
cover	O
’	O
s	O
impossibil-	O
ity	O
result	O
,	O
since	O
in	O
deriving	O
this	O
result	O
we	O
assumed	O
nothing	O
about	O
the	O
learner	O
’	O
s	O
strategy	O
.	O
to	O
make	O
the	O
randomization	O
meaningful	O
,	O
we	O
force	O
the	O
adversarial	O
envir-	O
onment	O
to	O
decide	O
on	O
yt	O
without	O
knowing	O
the	O
random	O
coins	O
ﬂipped	O
by	O
the	O
learner	O
on	O
round	O
t.	O
the	O
adversary	O
can	O
still	O
know	O
the	O
learner	O
’	O
s	O
forecasting	O
strategy	O
and	O
even	O
the	O
random	O
coin	O
ﬂips	O
of	O
previous	O
rounds	O
,	O
but	O
it	O
does	O
not	O
know	O
the	O
actual	O
value	O
of	O
the	O
random	O
coin	O
ﬂips	O
used	O
by	O
the	O
learner	O
on	O
round	O
t.	O
with	O
this	O
(	O
mild	O
)	O
change	O
of	O
game	O
,	O
we	O
analyze	O
the	O
expected	O
number	O
of	O
mistakes	O
of	O
the	O
algorithm	O
,	O
where	O
the	O
expectation	O
is	O
with	O
respect	O
to	O
the	O
learner	O
’	O
s	O
own	O
randomization	O
.	O
that	O
is	O
,	O
if	O
the	O
learner	O
outputs	O
ˆyt	O
where	O
p	O
[	O
ˆyt	O
=	O
1	O
]	O
=	O
pt	O
,	O
then	O
the	O
expected	O
loss	B
he	O
pays	O
21.2	O
online	B
classiﬁcation	O
in	O
the	O
unrealizable	O
case	O
295	O
on	O
round	O
t	O
is	O
p	O
[	O
ˆyt	O
(	O
cid:54	O
)	O
=	O
yt	O
]	O
=	O
|pt	O
−	O
yt|	O
.	O
put	O
another	O
way	O
,	O
instead	O
of	O
having	O
the	O
predictions	O
of	O
the	O
learner	O
being	O
in	O
{	O
0	O
,	O
1	O
}	O
we	O
allow	O
them	O
to	O
be	O
in	O
[	O
0	O
,	O
1	O
]	O
,	O
and	O
interpret	O
pt	O
∈	O
[	O
0	O
,	O
1	O
]	O
as	O
the	O
probability	O
to	O
predict	O
the	O
label	B
1	O
on	O
round	O
t.	O
with	O
this	O
assumption	O
it	O
is	O
possible	O
to	O
derive	O
a	O
low	O
regret	O
algorithm	O
.	O
in	O
partic-	O
ular	O
,	O
we	O
will	O
prove	O
the	O
following	O
theorem	O
.	O
theorem	O
21.10	O
for	O
every	O
hypothesis	B
class	I
h	O
,	O
there	O
exists	O
an	O
algorithm	O
for	O
online	B
classiﬁcation	O
,	O
whose	O
predictions	O
come	O
from	O
[	O
0	O
,	O
1	O
]	O
,	O
that	O
enjoys	O
the	O
regret	O
bound	O
|h	O
(	O
xt	O
)	O
−yt|	O
≤	O
(	O
cid:112	O
)	O
2	O
min	O
{	O
log	O
(	O
|h|	O
)	O
,	O
ldim	O
(	O
h	O
)	O
log	O
(	O
et	O
)	O
}	O
t	O
.	O
∀h	O
∈	O
h	O
,	O
t	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
(	O
cid:112	O
)	O
ldim	O
(	O
h	O
)	O
t	O
|pt−yt|−	O
t	O
(	O
cid:88	O
)	O
(	O
cid:17	O
)	O
t=1	O
t=1	O
furthermore	O
,	O
no	O
algorithm	O
can	O
achieve	O
an	O
expected	O
regret	O
bound	O
smaller	O
than	O
ω	O
.	O
we	O
will	O
provide	O
a	O
constructive	O
proof	O
of	O
the	O
upper	O
bound	O
part	O
of	O
the	O
preceding	O
theorem	O
.	O
the	O
proof	O
of	O
the	O
lower	O
bound	O
part	O
can	O
be	O
found	O
in	O
(	O
ben-david	O
,	O
pal	O
,	O
&	O
shalev-shwartz	O
2009	O
)	O
.	O
the	O
proof	O
of	O
theorem	O
21.10	O
relies	O
on	O
the	O
weighted-majority	O
algorithm	O
for	O
learning	O
with	O
expert	O
advice	O
.	O
this	O
algorithm	O
is	O
important	O
by	O
itself	O
and	O
we	O
dedicate	O
the	O
next	O
subsection	O
to	O
it	O
.	O
21.2.1	O
weighted-majority	O
i	O
w	O
(	O
t	O
)	O
with	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
i	O
=	O
1	O
,	O
and	O
choosing	O
the	O
ith	O
expert	O
with	O
probability	O
w	O
(	O
t	O
)	O
weighted-majority	O
is	O
an	O
algorithm	O
for	O
the	O
problem	O
of	O
prediction	O
with	O
expert	O
ad-	O
vice	O
.	O
in	O
this	O
online	B
learning	I
problem	O
,	O
on	O
round	O
t	O
the	O
learner	O
has	O
to	O
choose	O
the	O
advice	O
of	O
d	O
given	O
experts	O
.	O
we	O
also	O
allow	O
the	O
learner	O
to	O
randomize	O
his	O
choice	O
by	O
deﬁning	O
a	O
distribution	O
over	O
the	O
d	O
experts	O
,	O
that	O
is	O
,	O
picking	O
a	O
vector	O
w	O
(	O
t	O
)	O
∈	O
[	O
0	O
,	O
1	O
]	O
d	O
,	O
.	O
after	O
the	O
learner	O
chooses	O
an	O
expert	O
,	O
it	O
receives	O
a	O
vector	O
of	O
costs	O
,	O
vt	O
∈	O
[	O
0	O
,	O
1	O
]	O
d	O
,	O
where	O
vt	O
,	O
i	O
is	O
the	O
cost	O
of	O
following	O
the	O
advice	O
of	O
the	O
ith	O
expert	O
.	O
if	O
the	O
learner	O
’	O
s	O
predic-	O
tions	O
are	O
randomized	O
,	O
then	O
its	O
loss	B
is	O
deﬁned	O
to	O
be	O
the	O
averaged	O
cost	O
,	O
namely	O
,	O
i	O
vt	O
,	O
i	O
=	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
.	O
the	O
algorithm	O
assumes	O
that	O
the	O
number	O
of	O
rounds	O
t	O
is	O
given	O
.	O
in	O
exercise	O
4	O
we	O
show	O
how	O
to	O
get	O
rid	O
of	O
this	O
dependence	O
using	O
the	O
doubling	O
trick	O
.	O
i	O
w	O
(	O
t	O
)	O
i	O
296	O
online	B
learning	I
input	O
:	O
number	O
of	O
experts	O
,	O
d	O
;	O
number	O
of	O
rounds	O
,	O
t	O
weighted-majority	O
parameter	O
:	O
η	O
=	O
(	O
cid:112	O
)	O
2	O
log	O
(	O
d	O
)	O
/t	O
set	B
w	O
(	O
t	O
)	O
=	O
˜w	O
(	O
t	O
)	O
/zt	O
where	O
zt	O
=	O
(	O
cid:80	O
)	O
initialize	O
:	O
˜w	O
(	O
1	O
)	O
=	O
(	O
1	O
,	O
.	O
.	O
.	O
,	O
1	O
)	O
for	O
t	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
i	O
˜w	O
(	O
t	O
)	O
choose	O
expert	O
i	O
at	O
random	O
according	O
to	O
p	O
[	O
i	O
]	O
=	O
w	O
(	O
t	O
)	O
receive	O
costs	O
of	O
all	O
experts	O
vt	O
∈	O
[	O
0	O
,	O
1	O
]	O
d	O
pay	O
cost	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
update	O
rule	O
∀i	O
,	O
˜w	O
(	O
t+1	O
)	O
=	O
˜w	O
(	O
t	O
)	O
i	O
e−ηvt	O
,	O
i	O
i	O
i	O
i	O
the	O
following	O
theorem	O
is	O
key	O
for	O
analyzing	O
the	O
regret	O
bound	O
of	O
weighted-	O
majority	O
.	O
theorem	O
21.11	O
assuming	O
that	O
t	O
>	O
2	O
log	O
(	O
d	O
)	O
,	O
the	O
weighted-majority	O
algo-	O
rithm	O
enjoys	O
the	O
bound	O
t=1	O
proof	O
we	O
have	O
:	O
fact	O
that	O
(	O
cid:80	O
)	O
using	O
the	O
inequality	O
e−a	O
≤	O
1	O
−	O
a	O
+	O
a2/2	O
,	O
which	O
holds	O
for	O
all	O
a	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
and	O
the	O
t	O
(	O
cid:88	O
)	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
−	O
min	O
i∈	O
[	O
d	O
]	O
vt	O
,	O
i	O
≤	O
(	O
cid:112	O
)	O
2	O
log	O
(	O
d	O
)	O
t	O
.	O
t	O
(	O
cid:88	O
)	O
t=1	O
(	O
cid:88	O
)	O
i	O
˜w	O
(	O
t	O
)	O
i	O
zt	O
e−ηvt	O
,	O
i	O
=	O
log	O
(	O
cid:88	O
)	O
i	O
i	O
e−ηvt	O
,	O
i	O
.	O
w	O
(	O
t	O
)	O
log	O
zt+1	O
zt	O
=	O
log	O
i	O
w	O
(	O
t	O
)	O
i	O
=	O
1	O
,	O
we	O
obtain	O
≤	O
log	O
log	O
zt+1	O
zt	O
w	O
(	O
t	O
)	O
(	O
cid:88	O
)	O
=	O
log	O
(	O
cid:0	O
)	O
1	O
−	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
i	O
i	O
i	O
(	O
cid:0	O
)	O
1	O
−	O
ηvt	O
,	O
i	O
+	O
η2v2	O
t	O
,	O
i/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
.	O
t	O
,	O
i/2	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
ηvt	O
,	O
i	O
−	O
η2v2	O
(	O
cid:125	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
w	O
(	O
t	O
)	O
i	O
def=	O
b	O
next	O
,	O
note	O
that	O
b	O
∈	O
(	O
0	O
,	O
1	O
)	O
.	O
therefore	O
,	O
taking	O
log	O
of	O
the	O
two	O
sides	O
of	O
the	O
inequality	O
1	O
−	O
b	O
≤	O
e−b	O
we	O
obtain	O
the	O
inequality	O
log	O
(	O
1	O
−	O
b	O
)	O
≤	O
−b	O
,	O
which	O
holds	O
for	O
all	O
b	O
≤	O
1	O
,	O
and	O
obtain	O
log	O
zt+1	O
zt	O
≤	O
−	O
(	O
cid:88	O
)	O
(	O
cid:0	O
)	O
ηvt	O
,	O
i	O
−	O
η2v2	O
t	O
,	O
i/2	O
(	O
cid:1	O
)	O
=	O
−η	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
+	O
η2	O
(	O
cid:88	O
)	O
w	O
(	O
t	O
)	O
w	O
(	O
t	O
)	O
i	O
i	O
i	O
v2	O
t	O
,	O
i/2	O
≤	O
−η	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
+	O
η2/2	O
.	O
i	O
21.2	O
online	B
classiﬁcation	O
in	O
the	O
unrealizable	O
case	O
297	O
summing	O
this	O
inequality	O
over	O
t	O
we	O
get	O
log	O
(	O
zt	O
+1	O
)	O
−	O
log	O
(	O
z1	O
)	O
=	O
log	O
zt+1	O
zt	O
≤	O
−η	O
t	O
(	O
cid:88	O
)	O
t=1	O
t	O
(	O
cid:88	O
)	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
+	O
t=1	O
next	O
,	O
we	O
lower	O
bound	O
zt	O
+1	O
.	O
for	O
each	O
i	O
,	O
we	O
can	O
rewrite	O
˜w	O
(	O
t	O
+1	O
)	O
we	O
get	O
that	O
i	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
i	O
e−η	O
(	O
cid:80	O
)	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
e−η	O
(	O
cid:80	O
)	O
log	O
zt	O
+1	O
=	O
log	O
t	O
vt	O
,	O
i	O
≥	O
log	O
max	O
i	O
t	O
vt	O
,	O
i	O
=	O
−η	O
min	O
vt	O
,	O
i	O
.	O
combining	O
the	O
preceding	O
with	O
equation	O
(	O
21.3	O
)	O
and	O
using	O
the	O
fact	O
that	O
log	O
(	O
z1	O
)	O
=	O
log	O
(	O
d	O
)	O
we	O
get	O
that	O
.	O
(	O
21.3	O
)	O
t	O
vt	O
,	O
i	O
and	O
t	O
η2	O
2	O
=	O
e−η	O
(	O
cid:80	O
)	O
(	O
cid:88	O
)	O
i	O
t	O
(	O
cid:88	O
)	O
−η	O
min	O
i	O
t	O
(	O
cid:88	O
)	O
vt	O
,	O
i	O
−	O
log	O
(	O
d	O
)	O
≤	O
−	O
η	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
+	O
t	O
t=1	O
t	O
η2	O
2	O
,	O
which	O
can	O
be	O
rearranged	O
as	O
follows	O
:	O
t	O
(	O
cid:88	O
)	O
t=1	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
−	O
min	O
i	O
(	O
cid:88	O
)	O
t	O
vt	O
,	O
i	O
≤	O
log	O
(	O
d	O
)	O
η	O
+	O
η	O
t	O
2	O
.	O
plugging	O
the	O
value	O
of	O
η	O
into	O
the	O
equation	O
concludes	O
our	O
proof	O
.	O
proof	O
of	O
theorem	O
21.10	O
equipped	O
with	O
the	O
weighted-majority	O
algorithm	O
and	O
theorem	O
21.11	O
,	O
we	O
are	O
ready	O
to	O
prove	O
theorem	O
21.10.	O
we	O
start	O
with	O
the	O
simpler	O
case	O
,	O
in	O
which	O
h	O
is	O
a	O
ﬁnite	O
class	O
,	O
and	O
let	O
us	O
write	O
h	O
=	O
{	O
h1	O
,	O
.	O
.	O
.	O
,	O
hd	O
}	O
.	O
in	O
this	O
case	O
,	O
we	O
can	O
refer	O
to	O
each	O
hypothesis	B
,	O
hi	O
,	O
as	O
an	O
expert	O
,	O
whose	O
advice	O
is	O
to	O
predict	O
hi	O
(	O
xt	O
)	O
,	O
and	O
whose	O
cost	O
is	O
vt	O
,	O
i	O
=	O
|hi	O
(	O
xt	O
)	O
−	O
yt|	O
.	O
the	O
prediction	O
of	O
the	O
algorithm	O
will	O
therefore	O
be	O
i	O
hi	O
(	O
xt	O
)	O
∈	O
[	O
0	O
,	O
1	O
]	O
,	O
and	O
the	O
loss	B
is	O
i	O
w	O
(	O
t	O
)	O
pt	O
=	O
(	O
cid:80	O
)	O
|pt	O
−	O
yt|	O
=	O
i	O
hi	O
(	O
xt	O
)	O
−	O
yt	O
w	O
(	O
t	O
)	O
i	O
(	O
hi	O
(	O
xt	O
)	O
−	O
yt	O
)	O
w	O
(	O
t	O
)	O
(	O
cid:80	O
)	O
equals	O
(	O
cid:80	O
)	O
i	O
w	O
(	O
t	O
)	O
now	O
,	O
if	O
yt	O
=	O
1	O
,	O
then	O
for	O
all	O
i	O
,	O
hi	O
(	O
xt	O
)	O
−	O
yt	O
≤	O
0.	O
therefore	O
,	O
the	O
above	O
equals	O
to	O
|hi	O
(	O
xt	O
)	O
−	O
yt|	O
.	O
if	O
yt	O
=	O
0	O
then	O
for	O
all	O
i	O
,	O
hi	O
(	O
xt	O
)	O
−	O
yt	O
≥	O
0	O
,	O
and	O
the	O
above	O
also	O
i	O
w	O
(	O
t	O
)	O
|hi	O
(	O
xt	O
)	O
−	O
yt|	O
.	O
all	O
in	O
all	O
,	O
we	O
have	O
shown	O
that	O
i	O
i	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
.	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
d	O
(	O
cid:88	O
)	O
i=1	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
=	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
d	O
(	O
cid:88	O
)	O
i=1	O
d	O
(	O
cid:88	O
)	O
i=1	O
|pt	O
−	O
yt|	O
=	O
furthermore	O
,	O
for	O
each	O
i	O
,	O
(	O
cid:80	O
)	O
|hi	O
(	O
xt	O
)	O
−	O
yt|	O
=	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
.	O
w	O
(	O
t	O
)	O
i	O
makes	O
.	O
applying	O
theorem	O
21.11	O
we	O
obtain	O
t	O
vt	O
,	O
i	O
is	O
exactly	O
the	O
number	O
of	O
mistakes	O
hypothesis	B
hi	O
298	O
online	B
learning	I
corollary	O
21.12	O
let	O
h	O
be	O
a	O
ﬁnite	O
hypothesis	B
class	I
.	O
there	O
exists	O
an	O
algorithm	O
for	O
online	B
classiﬁcation	O
,	O
whose	O
predictions	O
come	O
from	O
[	O
0	O
,	O
1	O
]	O
,	O
that	O
enjoys	O
the	O
regret	O
bound	O
|h	O
(	O
xt	O
)	O
−	O
yt|	O
≤	O
(	O
cid:112	O
)	O
2	O
log	O
(	O
|h|	O
)	O
t	O
.	O
t	O
(	O
cid:88	O
)	O
t=1	O
t	O
(	O
cid:88	O
)	O
t=1	O
|pt	O
−	O
yt|	O
−	O
min	O
h∈h	O
next	O
,	O
we	O
consider	O
the	O
case	O
of	O
a	O
general	O
hypothesis	B
class	I
.	O
previously	O
,	O
we	O
con-	O
structed	O
an	O
expert	O
for	O
each	O
individual	O
hypothesis	B
.	O
however	O
,	O
if	O
h	O
is	O
inﬁnite	O
this	O
leads	O
to	O
a	O
vacuous	O
bound	O
.	O
the	O
main	O
idea	O
is	O
to	O
construct	O
a	O
set	B
of	O
experts	O
in	O
a	O
more	O
sophisticated	O
way	O
.	O
the	O
challenge	O
is	O
how	O
to	O
deﬁne	O
a	O
set	B
of	O
experts	O
that	O
,	O
on	O
one	O
hand	O
,	O
is	O
not	O
excessively	O
large	O
and	O
,	O
on	O
the	O
other	O
hand	O
,	O
contains	O
experts	O
that	O
give	O
accurate	O
predictions	O
.	O
we	O
construct	O
the	O
set	B
of	O
experts	O
so	O
that	O
for	O
each	O
hypothesis	B
h	O
∈	O
h	O
and	O
every	O
sequence	O
of	O
instances	O
,	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xt	O
,	O
there	O
exists	O
at	O
least	O
one	O
expert	O
in	O
the	O
set	B
which	O
behaves	O
exactly	O
as	O
h	O
on	O
these	O
instances	O
.	O
for	O
each	O
l	O
≤	O
ldim	O
(	O
h	O
)	O
and	O
each	O
sequence	O
1	O
≤	O
i1	O
<	O
i2	O
<	O
···	O
<	O
il	O
≤	O
t	O
we	O
deﬁne	O
an	O
expert	O
.	O
the	O
expert	O
simulates	O
the	O
game	O
between	O
soa	O
(	O
presented	O
in	O
the	O
previous	O
section	O
)	O
and	O
the	O
environment	O
on	O
the	O
sequence	O
of	O
instances	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xt	O
assuming	O
that	O
soa	O
makes	O
a	O
mistake	O
precisely	O
in	O
rounds	O
i1	O
,	O
i2	O
,	O
.	O
.	O
.	O
,	O
il	O
.	O
the	O
expert	O
is	O
deﬁned	O
by	O
the	O
following	O
algorithm	O
.	O
expert	O
(	O
i1	O
,	O
i2	O
,	O
.	O
.	O
.	O
,	O
il	O
)	O
input	O
a	O
hypothesis	B
class	I
h	O
;	O
indices	O
i1	O
<	O
i2	O
<	O
···	O
<	O
il	O
initialize	O
:	O
v1	O
=	O
h	O
for	O
t	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
t	O
t	O
=	O
{	O
h	O
∈	O
vt	O
:	O
h	O
(	O
xt	O
)	O
=	O
r	O
}	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
v	O
(	O
r	O
)	O
t	O
receive	O
xt	O
for	O
r	O
∈	O
{	O
0	O
,	O
1	O
}	O
let	O
v	O
(	O
r	O
)	O
deﬁne	O
˜yt	O
=	O
argmaxr	O
ldim	O
if	O
(	O
in	O
case	O
of	O
a	O
tie	O
set	B
˜yt	O
=	O
0	O
)	O
t	O
∈	O
{	O
i1	O
,	O
i2	O
,	O
.	O
.	O
.	O
,	O
il	O
}	O
predict	O
ˆyt	O
=	O
1	O
−	O
˜yt	O
else	O
predict	O
ˆyt	O
=	O
˜yt	O
update	O
vt+1	O
=	O
v	O
(	O
ˆyt	O
)	O
t	O
note	O
that	O
each	O
such	O
expert	O
can	O
give	O
us	O
predictions	O
at	O
every	O
round	O
t	O
while	O
only	O
observing	O
the	O
instances	O
x1	O
,	O
.	O
.	O
.	O
,	O
xt	O
.	O
our	O
generic	O
online	B
learning	I
algorithm	O
is	O
now	O
an	O
application	O
of	O
the	O
weighted-majority	O
algorithm	O
with	O
these	O
experts	O
.	O
to	O
analyze	O
the	O
algorithm	O
we	O
ﬁrst	O
note	O
that	O
the	O
number	O
of	O
experts	O
is	O
ldim	O
(	O
h	O
)	O
(	O
cid:88	O
)	O
(	O
cid:18	O
)	O
t	O
(	O
cid:19	O
)	O
l	O
l=0	O
d	O
=	O
.	O
(	O
21.4	O
)	O
it	O
can	O
be	O
shown	O
that	O
when	O
t	O
≥	O
ldim	O
(	O
h	O
)	O
+	O
2	O
,	O
the	O
right-hand	O
side	O
of	O
the	O
equation	O
is	O
bounded	O
by	O
(	O
et	O
/ldim	O
(	O
h	O
)	O
)	O
ldim	O
(	O
h	O
)	O
(	O
the	O
proof	O
can	O
be	O
found	O
in	O
lemma	O
a.5	O
)	O
.	O
21.2	O
online	B
classiﬁcation	O
in	O
the	O
unrealizable	O
case	O
299	O
is	O
at	O
most	O
the	O
number	O
of	O
mistakes	O
of	O
the	O
best	O
expert	O
plus	O
(	O
cid:112	O
)	O
2	O
log	O
(	O
d	O
)	O
t	O
.	O
we	O
will	O
theorem	O
21.11	O
tells	O
us	O
that	O
the	O
expected	O
number	O
of	O
mistakes	O
of	O
weighted-majority	O
next	O
show	O
that	O
the	O
number	O
of	O
mistakes	O
of	O
the	O
best	O
expert	O
is	O
at	O
most	O
the	O
number	O
of	O
mistakes	O
of	O
the	O
best	O
hypothesis	B
in	O
h.	O
the	O
following	O
key	O
lemma	O
shows	O
that	O
,	O
on	O
any	O
sequence	O
of	O
instances	O
,	O
for	O
each	O
hypothesis	B
h	O
∈	O
h	O
there	O
exists	O
an	O
expert	O
with	O
the	O
same	O
behavior	O
.	O
lemma	O
21.13	O
let	O
h	O
be	O
any	O
hypothesis	B
class	I
with	O
ldim	O
(	O
h	O
)	O
<	O
∞	O
.	O
let	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xt	O
be	O
any	O
sequence	O
of	O
instances	O
.	O
for	O
any	O
h	O
∈	O
h	O
,	O
there	O
exists	O
l	O
≤	O
ldim	O
(	O
h	O
)	O
and	O
in-	O
dices	O
1	O
≤	O
i1	O
<	O
i2	O
<	O
···	O
<	O
il	O
≤	O
t	O
such	O
that	O
when	O
running	O
expert	O
(	O
i1	O
,	O
i2	O
,	O
.	O
.	O
.	O
,	O
il	O
)	O
on	O
the	O
sequence	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xt	O
,	O
the	O
expert	O
predicts	O
h	O
(	O
xt	O
)	O
on	O
each	O
online	B
round	O
t	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
t	O
.	O
proof	O
fix	O
h	O
∈	O
h	O
and	O
the	O
sequence	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xt	O
.	O
we	O
must	O
construct	O
l	O
and	O
the	O
indices	O
i1	O
,	O
i2	O
,	O
.	O
.	O
.	O
,	O
il	O
.	O
consider	O
running	O
soa	O
on	O
the	O
input	O
(	O
x1	O
,	O
h	O
(	O
x1	O
)	O
)	O
,	O
(	O
x2	O
,	O
h	O
(	O
x2	O
)	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xt	O
,	O
h	O
(	O
xt	O
)	O
)	O
.	O
soa	O
makes	O
at	O
most	O
ldim	O
(	O
h	O
)	O
mistakes	O
on	O
such	O
input	O
.	O
we	O
deﬁne	O
l	O
to	O
be	O
the	O
number	O
of	O
mistakes	O
made	O
by	O
soa	O
and	O
we	O
deﬁne	O
{	O
i1	O
,	O
i2	O
,	O
.	O
.	O
.	O
,	O
il	O
}	O
to	O
be	O
the	O
set	B
of	O
rounds	O
in	O
which	O
soa	O
made	O
the	O
mistakes	O
.	O
now	O
,	O
consider	O
the	O
expert	O
(	O
i1	O
,	O
i2	O
,	O
.	O
.	O
.	O
,	O
il	O
)	O
running	O
on	O
the	O
sequence	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xt	O
.	O
by	O
construction	O
,	O
the	O
set	B
vt	O
maintained	O
by	O
expert	O
(	O
i1	O
,	O
i2	O
,	O
.	O
.	O
.	O
,	O
il	O
)	O
equals	O
the	O
set	B
vt	O
maintained	O
by	O
soa	O
when	O
running	O
on	O
the	O
sequence	O
(	O
x1	O
,	O
h	O
(	O
x1	O
)	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xt	O
,	O
h	O
(	O
xt	O
)	O
)	O
.	O
the	O
predictions	O
of	O
soa	O
diﬀer	O
from	O
the	O
predictions	O
of	O
h	O
if	O
and	O
only	O
if	O
the	O
round	O
is	O
in	O
{	O
i1	O
,	O
i2	O
,	O
.	O
.	O
.	O
,	O
il	O
}	O
.	O
since	O
expert	O
(	O
i1	O
,	O
i2	O
,	O
.	O
.	O
.	O
,	O
il	O
)	O
predicts	O
exactly	O
like	O
soa	O
if	O
t	O
is	O
not	O
in	O
{	O
i1	O
,	O
i2	O
,	O
.	O
.	O
.	O
,	O
il	O
}	O
and	O
the	O
opposite	O
of	O
soas	O
’	O
predictions	O
if	O
t	O
is	O
in	O
{	O
i1	O
,	O
i2	O
,	O
.	O
.	O
.	O
,	O
il	O
}	O
,	O
we	O
conclude	O
that	O
the	O
predictions	O
of	O
the	O
expert	O
are	O
always	O
the	O
same	O
as	O
the	O
pre-	O
dictions	O
of	O
h.	O
the	O
previous	O
lemma	O
holds	O
in	O
particular	O
for	O
the	O
hypothesis	B
in	O
h	O
that	O
makes	O
the	O
least	O
number	O
of	O
mistakes	O
on	O
the	O
sequence	O
of	O
examples	O
,	O
and	O
we	O
therefore	O
obtain	O
the	O
following	O
:	O
corollary	O
21.14	O
let	O
(	O
x1	O
,	O
y1	O
)	O
,	O
(	O
x2	O
,	O
y2	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xt	O
,	O
yt	O
)	O
be	O
a	O
sequence	O
of	O
examples	O
and	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
with	O
ldim	O
(	O
h	O
)	O
<	O
∞	O
.	O
there	O
exists	O
l	O
≤	O
ldim	O
(	O
h	O
)	O
and	O
indices	O
1	O
≤	O
i1	O
<	O
i2	O
<	O
···	O
<	O
il	O
≤	O
t	O
,	O
such	O
that	O
expert	O
(	O
i1	O
,	O
i2	O
,	O
.	O
.	O
.	O
,	O
il	O
)	O
makes	O
at	O
most	O
as	O
many	O
mistakes	O
as	O
the	O
best	O
h	O
∈	O
h	O
does	O
,	O
namely	O
,	O
t	O
(	O
cid:88	O
)	O
t=1	O
|h	O
(	O
xt	O
)	O
−	O
yt|	O
min	O
h∈h	O
mistakes	O
on	O
the	O
sequence	O
of	O
examples	O
.	O
together	O
with	O
theorem	O
21.11	O
,	O
the	O
upper	O
bound	O
part	O
of	O
theorem	O
21.10	O
is	O
proven	O
.	O
300	O
online	B
learning	I
21.3	O
online	B
convex	I
optimization	I
in	O
chapter	O
12	O
we	O
studied	O
convex	B
learning	O
problems	O
and	O
showed	O
learnability	O
results	O
for	O
these	O
problems	O
in	O
the	O
agnostic	O
pac	O
learning	O
framework	O
.	O
in	O
this	O
section	O
we	O
show	O
that	O
similar	O
learnability	O
results	O
hold	O
for	O
convex	B
problems	O
in	O
the	O
online	B
learning	I
framework	O
.	O
in	O
particular	O
,	O
we	O
consider	O
the	O
following	O
problem	O
.	O
online	B
convex	I
optimization	I
loss	O
function	B
(	O
cid:96	O
)	O
:	O
h	O
×	O
z	O
→	O
r	O
deﬁnitions	O
:	O
hypothesis	B
class	I
h	O
;	O
domain	B
z	O
;	O
assumptions	O
:	O
h	O
is	O
convex	B
∀z	O
∈	O
z	O
,	O
(	O
cid:96	O
)	O
(	O
·	O
,	O
z	O
)	O
is	O
a	O
convex	B
function	O
for	O
t	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
t	O
learner	O
predicts	O
a	O
vector	O
w	O
(	O
t	O
)	O
∈	O
h	O
environment	O
responds	O
with	O
zt	O
∈	O
z	O
learner	O
suﬀers	O
loss	B
(	O
cid:96	O
)	O
(	O
w	O
(	O
t	O
)	O
,	O
zt	O
)	O
as	O
in	O
the	O
online	B
classiﬁcation	O
problem	O
,	O
we	O
analyze	O
the	O
regret	O
of	O
the	O
algorithm	O
.	O
recall	B
that	O
the	O
regret	O
of	O
an	O
online	B
algorithm	O
with	O
respect	O
to	O
a	O
competing	O
hy-	O
pothesis	O
,	O
which	O
here	O
will	O
be	O
some	O
vector	O
w	O
(	O
cid:63	O
)	O
∈	O
h	O
,	O
is	O
deﬁned	O
as	O
t	O
(	O
cid:88	O
)	O
(	O
cid:96	O
)	O
(	O
w	O
(	O
t	O
)	O
,	O
zt	O
)	O
−	O
t	O
(	O
cid:88	O
)	O
regreta	O
(	O
w	O
(	O
cid:63	O
)	O
,	O
t	O
)	O
=	O
(	O
cid:96	O
)	O
(	O
w	O
(	O
cid:63	O
)	O
,	O
zt	O
)	O
.	O
(	O
21.5	O
)	O
as	O
before	O
,	O
the	O
regret	O
of	O
the	O
algorithm	O
relative	O
to	O
a	O
set	B
of	O
competing	O
vectors	O
,	O
h	O
,	O
is	O
deﬁned	O
as	O
t=1	O
t=1	O
regreta	O
(	O
h	O
,	O
t	O
)	O
=	O
sup	O
w	O
(	O
cid:63	O
)	O
∈h	O
regreta	O
(	O
w	O
(	O
cid:63	O
)	O
,	O
t	O
)	O
.	O
in	O
chapter	O
14	O
we	O
have	O
shown	O
that	O
stochastic	O
gradient	B
descent	I
solves	O
convex	B
learning	O
problems	O
in	O
the	O
agnostic	O
pac	O
model	O
.	O
we	O
now	O
show	O
that	O
a	O
very	O
similar	O
algorithm	O
,	O
online	B
gradient	I
descent	I
,	O
solves	O
online	O
convex	O
learning	O
problems	O
.	O
online	B
gradient	I
descent	I
parameter	O
:	O
η	O
>	O
0	O
initialize	O
:	O
w	O
(	O
1	O
)	O
=	O
0	O
for	O
t	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
t	O
predict	O
w	O
(	O
t	O
)	O
receive	O
zt	O
and	O
let	O
ft	O
(	O
·	O
)	O
=	O
(	O
cid:96	O
)	O
(	O
·	O
,	O
zt	O
)	O
choose	O
vt	O
∈	O
∂ft	O
(	O
w	O
(	O
t	O
)	O
)	O
update	O
:	O
1.	O
w	O
(	O
t+	O
1	O
2.	O
w	O
(	O
t+1	O
)	O
=	O
argminw∈h	O
(	O
cid:107	O
)	O
w	O
−	O
w	O
(	O
t+	O
1	O
2	O
)	O
=	O
w	O
(	O
t	O
)	O
−	O
ηvt	O
2	O
)	O
(	O
cid:107	O
)	O
21.4	O
the	O
online	B
perceptron	O
algorithm	O
301	O
theorem	O
21.15	O
the	O
online	B
gradient	I
descent	I
algorithm	O
enjoys	O
the	O
following	O
regret	O
bound	O
for	O
every	O
w	O
(	O
cid:63	O
)	O
∈	O
h	O
,	O
regreta	O
(	O
w	O
(	O
cid:63	O
)	O
,	O
t	O
)	O
≤	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
2η	O
+	O
η	O
2	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
2.	O
t	O
(	O
cid:88	O
)	O
t=1	O
if	O
we	O
further	O
assume	O
that	O
ft	O
is	O
ρ-lipschitz	O
for	O
all	O
t	O
,	O
then	O
setting	O
η	O
=	O
1/	O
regreta	O
(	O
w	O
(	O
cid:63	O
)	O
,	O
t	O
)	O
≤	O
1	O
2	O
(	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
+	O
ρ2	O
)	O
t	O
.	O
√	O
if	O
we	O
further	O
assume	O
that	O
h	O
is	O
b-bounded	O
and	O
we	O
set	B
η	O
=	O
b	O
√	O
ρ	O
t	O
then	O
regreta	O
(	O
h	O
,	O
t	O
)	O
≤	O
b	O
ρ	O
√	O
t	O
.	O
√	O
t	O
yields	O
proof	O
the	O
analysis	O
is	O
similar	O
to	O
the	O
analysis	O
of	O
stochastic	O
gradient	B
descent	I
with	O
projections	O
.	O
using	O
the	O
projection	B
lemma	I
,	O
the	O
deﬁnition	O
of	O
w	O
(	O
t+	O
1	O
2	O
)	O
,	O
and	O
the	O
deﬁnition	O
of	O
subgradients	O
,	O
we	O
have	O
that	O
for	O
every	O
t	O
,	O
2	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
+	O
(	O
cid:107	O
)	O
w	O
(	O
t+	O
1	O
2	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
2	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
(	O
cid:107	O
)	O
w	O
(	O
t+1	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:107	O
)	O
w	O
(	O
t+1	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
t+	O
1	O
≤	O
(	O
cid:107	O
)	O
w	O
(	O
t+	O
1	O
=	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
−	O
ηvt	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
=	O
−2η	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
,	O
vt	O
(	O
cid:105	O
)	O
+	O
η2	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
2	O
≤	O
−2η	O
(	O
ft	O
(	O
w	O
(	O
t	O
)	O
)	O
−	O
ft	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
)	O
+	O
η2	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
2.	O
summing	O
over	O
t	O
and	O
observing	O
that	O
the	O
left-hand	O
side	O
is	O
a	O
telescopic	O
sum	O
we	O
obtain	O
that	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
+1	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
1	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
≤	O
−2η	O
(	O
ft	O
(	O
w	O
(	O
t	O
)	O
)	O
−	O
ft	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
)	O
+	O
η2	O
t	O
(	O
cid:88	O
)	O
t	O
(	O
cid:88	O
)	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
2.	O
t=1	O
t=1	O
rearranging	O
the	O
inequality	O
and	O
using	O
the	O
fact	O
that	O
w	O
(	O
1	O
)	O
=	O
0	O
,	O
we	O
get	O
that	O
t	O
(	O
cid:88	O
)	O
(	O
ft	O
(	O
w	O
(	O
t	O
)	O
)	O
−	O
ft	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
)	O
≤	O
(	O
cid:107	O
)	O
w	O
(	O
1	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
t	O
+1	O
)	O
−	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
t	O
(	O
cid:88	O
)	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
2	O
+	O
η	O
2	O
t=1	O
t=1	O
≤	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
2η	O
+	O
η	O
2	O
2η	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
2.	O
t	O
(	O
cid:88	O
)	O
t=1	O
this	O
proves	O
the	O
ﬁrst	O
bound	O
in	O
the	O
theorem	O
.	O
the	O
second	O
bound	O
follows	O
from	O
the	O
assumption	O
that	O
ft	O
is	O
ρ-lipschitz	O
,	O
which	O
implies	O
that	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
≤	O
ρ	O
.	O
21.4	O
the	O
online	B
perceptron	O
algorithm	O
the	O
perceptron	O
is	O
a	O
classic	O
online	B
learning	I
algorithm	O
for	O
binary	O
classiﬁcation	O
with	O
the	O
hypothesis	B
class	I
of	O
homogenous	B
halfspaces	O
,	O
namely	O
,	O
h	O
=	O
{	O
x	O
(	O
cid:55	O
)	O
→	O
sign	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
:	O
302	O
online	B
learning	I
w	O
∈	O
rd	O
}	O
.	O
in	O
section	O
9.1.2	O
we	O
have	O
presented	O
the	O
batch	O
version	O
of	O
the	O
perceptron	O
,	O
which	O
aims	O
to	O
solve	O
the	O
erm	O
problem	O
with	O
respect	O
to	O
h.	O
we	O
now	O
present	O
an	O
online	B
version	O
of	O
the	O
perceptron	O
algorithm	O
.	O
let	O
x	O
=	O
rd	O
,	O
y	O
=	O
{	O
−1	O
,	O
1	O
}	O
.	O
on	O
round	O
t	O
,	O
the	O
learner	O
receives	O
a	O
vector	O
xt	O
∈	O
rd	O
.	O
the	O
learner	O
maintains	O
a	O
weight	O
vector	O
w	O
(	O
t	O
)	O
∈	O
rd	O
and	O
predicts	O
pt	O
=	O
sign	O
(	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
xt	O
(	O
cid:105	O
)	O
)	O
.	O
then	O
,	O
it	O
receives	O
yt	O
∈	O
y	O
and	O
pays	O
1	O
if	O
pt	O
(	O
cid:54	O
)	O
=	O
yt	O
and	O
0	O
otherwise	O
.	O
the	O
goal	O
of	O
the	O
learner	O
is	O
to	O
make	O
as	O
few	O
prediction	O
mistakes	O
as	O
possible	O
.	O
in	O
section	O
21.1	O
we	O
characterized	O
the	O
optimal	O
algorithm	O
and	O
showed	O
that	O
the	O
best	O
achievable	O
mistake	B
bound	I
depends	O
on	O
the	O
littlestone	O
dimension	B
of	O
the	O
class	O
.	O
we	O
show	O
later	O
that	O
if	O
d	O
≥	O
2	O
then	O
ldim	O
(	O
h	O
)	O
=	O
∞	O
,	O
which	O
implies	O
that	O
we	O
have	O
no	O
hope	O
of	O
making	O
few	O
prediction	O
mistakes	O
.	O
indeed	O
,	O
consider	O
the	O
tree	O
for	O
which	O
v1	O
=	O
(	O
1	O
4	O
,	O
1	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
)	O
,	O
etc	O
.	O
because	O
of	O
the	O
density	O
of	O
the	O
reals	O
,	O
this	O
tree	O
is	O
shattered	O
by	O
the	O
subset	O
of	O
h	O
which	O
contains	O
all	O
hypotheses	O
that	O
are	O
parametrized	O
by	O
w	O
of	O
the	O
form	O
w	O
=	O
(	O
−1	O
,	O
a	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
)	O
,	O
for	O
a	O
∈	O
[	O
0	O
,	O
1	O
]	O
.	O
we	O
conclude	O
that	O
indeed	O
ldim	O
(	O
h	O
)	O
=	O
∞	O
.	O
4	O
,	O
1	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
)	O
,	O
v3	O
=	O
(	O
3	O
2	O
,	O
1	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
)	O
,	O
v2	O
=	O
(	O
1	O
to	O
sidestep	O
this	O
impossibility	O
result	O
,	O
the	O
perceptron	O
algorithm	O
relies	O
on	O
the	O
technique	O
of	O
surrogate	O
convex	O
losses	O
(	O
see	O
section	O
12.3	O
)	O
.	O
this	O
is	O
also	O
closely	O
related	O
to	O
the	O
notion	O
of	O
margin	B
we	O
studied	O
in	O
chapter	O
15.	O
a	O
weight	O
vector	O
w	O
makes	O
a	O
mistake	O
on	O
an	O
example	O
(	O
x	O
,	O
y	O
)	O
whenever	O
the	O
sign	O
of	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
does	O
not	O
equal	O
y.	O
therefore	O
,	O
we	O
can	O
write	O
the	O
0−1	O
loss	B
function	I
as	O
follows	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
1	O
[	O
y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
≤0	O
]	O
.	O
on	O
rounds	O
on	O
which	O
the	O
algorithm	O
makes	O
a	O
prediction	O
mistake	O
,	O
we	O
shall	O
use	O
the	O
hinge-loss	O
as	O
a	O
surrogate	O
convex	O
loss	B
function	I
ft	O
(	O
w	O
)	O
=	O
max	O
{	O
0	O
,	O
1	O
−	O
yt	O
(	O
cid:104	O
)	O
w	O
,	O
xt	O
(	O
cid:105	O
)	O
}	O
.	O
the	O
hinge-loss	O
satisﬁes	O
the	O
two	O
conditions	O
:	O
•	O
ft	O
is	O
a	O
convex	B
function	O
•	O
for	O
all	O
w	O
,	O
ft	O
(	O
w	O
)	O
≥	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
xt	O
,	O
yt	O
)	O
)	O
.	O
in	O
particular	O
,	O
this	O
holds	O
for	O
w	O
(	O
t	O
)	O
.	O
on	O
rounds	O
on	O
which	O
the	O
algorithm	O
is	O
correct	O
,	O
we	O
shall	O
deﬁne	O
ft	O
(	O
w	O
)	O
=	O
0.	O
clearly	O
,	O
ft	O
is	O
convex	B
in	O
this	O
case	O
as	O
well	O
.	O
furthermore	O
,	O
ft	O
(	O
w	O
(	O
t	O
)	O
)	O
=	O
(	O
cid:96	O
)	O
(	O
w	O
(	O
t	O
)	O
,	O
(	O
xt	O
,	O
yt	O
)	O
)	O
=	O
0.	O
remark	O
21.5	O
in	O
section	O
12.3	O
we	O
used	O
the	O
same	O
surrogate	B
loss	I
function	O
for	O
all	O
the	O
examples	O
.	O
in	O
the	O
online	B
model	O
,	O
we	O
allow	O
the	O
surrogate	O
to	O
depend	O
on	O
the	O
speciﬁc	O
round	O
.	O
it	O
can	O
even	O
depend	O
on	O
w	O
(	O
t	O
)	O
.	O
our	O
ability	O
to	O
use	O
a	O
round	O
speciﬁc	O
surrogate	O
stems	O
from	O
the	O
worst-case	O
type	O
of	O
analysis	O
we	O
employ	O
in	O
online	B
learning	I
.	O
let	O
us	O
now	O
run	O
the	O
online	B
gradient	I
descent	I
algorithm	O
on	O
the	O
sequence	O
of	O
functions	O
,	O
f1	O
,	O
.	O
.	O
.	O
,	O
ft	O
,	O
with	O
the	O
hypothesis	B
class	I
being	O
all	O
vectors	O
in	O
rd	O
(	O
hence	O
,	O
the	O
projection	B
step	O
is	O
vacuous	O
)	O
.	O
recall	B
that	O
the	O
algorithm	O
initializes	O
w	O
(	O
1	O
)	O
=	O
0	O
and	O
its	O
update	O
rule	O
is	O
w	O
(	O
t+1	O
)	O
=	O
w	O
(	O
t	O
)	O
−	O
ηvt	O
for	O
some	O
vt	O
∈	O
∂ft	O
(	O
w	O
(	O
t	O
)	O
)	O
.	O
in	O
our	O
case	O
,	O
if	O
yt	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
xt	O
(	O
cid:105	O
)	O
>	O
0	O
then	O
ft	O
is	O
the	O
zero	O
21.4	O
the	O
online	B
perceptron	O
algorithm	O
303	O
function	B
and	O
we	O
can	O
take	O
vt	O
=	O
0.	O
otherwise	O
,	O
it	O
is	O
easy	O
to	O
verify	O
that	O
vt	O
=	O
−ytxt	O
is	O
in	O
∂ft	O
(	O
w	O
(	O
t	O
)	O
)	O
.	O
we	O
therefore	O
obtain	O
the	O
update	O
rule	O
(	O
cid:40	O
)	O
w	O
(	O
t+1	O
)	O
=	O
w	O
(	O
t	O
)	O
w	O
(	O
t	O
)	O
+	O
ηytxt	O
if	O
yt	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
xt	O
(	O
cid:105	O
)	O
>	O
0	O
otherwise	O
denote	O
by	O
m	O
the	O
set	B
of	O
rounds	O
in	O
which	O
sign	O
(	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
xt	O
(	O
cid:105	O
)	O
)	O
(	O
cid:54	O
)	O
=	O
yt	O
.	O
note	O
that	O
on	O
round	O
t	O
,	O
the	O
prediction	O
of	O
the	O
perceptron	O
can	O
be	O
rewritten	O
as	O
pt	O
=	O
sign	O
(	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
xt	O
(	O
cid:105	O
)	O
)	O
=	O
sign	O
η	O
yi	O
(	O
cid:104	O
)	O
xi	O
,	O
xt	O
(	O
cid:105	O
)	O
.	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
i∈m	O
:	O
i	O
<	O
t	O
(	O
cid:33	O
)	O
this	O
form	O
implies	O
that	O
the	O
predictions	O
of	O
the	O
perceptron	O
algorithm	O
and	O
the	O
set	B
m	O
do	O
not	O
depend	O
on	O
the	O
actual	O
value	O
of	O
η	O
as	O
long	O
as	O
η	O
>	O
0.	O
we	O
have	O
therefore	O
obtained	O
the	O
perceptron	O
algorithm	O
:	O
perceptron	O
initialize	O
:	O
w1	O
=	O
0	O
for	O
t	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
t	O
receive	O
xt	O
predict	O
pt	O
=	O
sign	O
(	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
xt	O
(	O
cid:105	O
)	O
)	O
if	O
yt	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
xt	O
(	O
cid:105	O
)	O
≤	O
0	O
w	O
(	O
t+1	O
)	O
=	O
w	O
(	O
t	O
)	O
+	O
ytxt	O
else	O
w	O
(	O
t+1	O
)	O
=	O
w	O
(	O
t	O
)	O
to	O
analyze	O
the	O
perceptron	O
,	O
we	O
rely	O
on	O
the	O
analysis	O
of	O
online	O
gradient	O
de-	O
scent	O
given	O
in	O
the	O
previous	O
section	O
.	O
in	O
our	O
case	O
,	O
the	O
subgradient	O
of	O
ft	O
we	O
use	O
in	O
the	O
perceptron	O
is	O
vt	O
=	O
−1	O
[	O
yt	O
(	O
cid:104	O
)	O
w	O
(	O
t	O
)	O
,	O
xt	O
(	O
cid:105	O
)	O
≤0	O
]	O
yt	O
xt	O
.	O
indeed	O
,	O
the	O
perceptron	O
’	O
s	O
update	O
is	O
w	O
(	O
t+1	O
)	O
=	O
w	O
(	O
t	O
)	O
−	O
vt	O
,	O
and	O
as	O
discussed	O
before	O
this	O
is	O
equivalent	O
to	O
w	O
(	O
t+1	O
)	O
=	O
w	O
(	O
t	O
)	O
−	O
ηvt	O
for	O
every	O
η	O
>	O
0.	O
therefore	O
,	O
theorem	O
21.15	O
tells	O
us	O
that	O
t	O
(	O
cid:88	O
)	O
ft	O
(	O
w	O
(	O
t	O
)	O
)	O
−	O
t	O
(	O
cid:88	O
)	O
t	O
(	O
cid:88	O
)	O
since	O
ft	O
(	O
w	O
(	O
t	O
)	O
)	O
is	O
a	O
surrogate	O
for	O
the	O
0−1	O
loss	B
we	O
know	O
that	O
(	O
cid:80	O
)	O
t	O
ft	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
≤	O
1	O
2η	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
2	O
+	O
η	O
2	O
t=1	O
t=1	O
t=1	O
t=1	O
ft	O
(	O
w	O
(	O
t	O
)	O
)	O
≥	O
|m|	O
.	O
(	O
cid:107	O
)	O
vt	O
(	O
cid:107	O
)	O
2	O
2.	O
denote	O
r	O
=	O
maxt	O
(	O
cid:107	O
)	O
xt	O
(	O
cid:107	O
)	O
;	O
then	O
we	O
obtain	O
|m|	O
−	O
t	O
(	O
cid:88	O
)	O
t=1	O
ft	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
≤	O
1	O
2η	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
2	O
+	O
η	O
2	O
|m|	O
r2	O
setting	O
η	O
=	O
√	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
r	O
|m|	O
and	O
rearranging	O
,	O
we	O
obtain	O
|m|	O
−	O
r	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
(	O
cid:112	O
)	O
|m|	O
−	O
t	O
(	O
cid:88	O
)	O
this	O
inequality	O
implies	O
t=1	O
ft	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
≤	O
0	O
.	O
(	O
21.6	O
)	O
304	O
online	B
learning	I
theorem	O
21.16	O
suppose	O
that	O
the	O
perceptron	O
algorithm	O
runs	O
on	O
a	O
sequence	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xt	O
,	O
yt	O
)	O
and	O
let	O
r	O
=	O
maxt	O
(	O
cid:107	O
)	O
xt	O
(	O
cid:107	O
)	O
.	O
let	O
m	O
be	O
the	O
rounds	O
on	O
which	O
the	O
perceptron	O
errs	O
and	O
let	O
ft	O
(	O
w	O
)	O
=	O
1	O
[	O
t∈m	O
]	O
[	O
1	O
−	O
yt	O
(	O
cid:104	O
)	O
w	O
,	O
xt	O
(	O
cid:105	O
)	O
]	O
+	O
.	O
then	O
,	O
for	O
every	O
w	O
(	O
cid:63	O
)	O
|m|	O
≤	O
(	O
cid:88	O
)	O
(	O
cid:115	O
)	O
(	O
cid:88	O
)	O
ft	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
+	O
r	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
ft	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
+	O
r2	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
.	O
t	O
t	O
in	O
particular	O
,	O
if	O
there	O
exists	O
w	O
(	O
cid:63	O
)	O
such	O
that	O
yt	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
xt	O
(	O
cid:105	O
)	O
≥	O
1	O
for	O
all	O
t	O
then	O
|m|	O
≤	O
r2	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2.	O
proof	O
the	O
theorem	O
follows	O
from	O
equation	O
(	O
21.6	O
)	O
and	O
the	O
following	O
claim	O
:	O
given	O
x	O
,	O
b	O
,	O
c	O
∈	O
r+	O
,	O
the	O
inequality	O
x	O
−	O
b	O
c.	O
the	O
last	O
claim	O
can	O
be	O
easily	O
derived	O
by	O
analyzing	O
the	O
roots	O
of	O
the	O
convex	B
parabola	O
q	O
(	O
y	O
)	O
=	O
y2	O
−	O
by	O
−	O
c.	O
x	O
−	O
c	O
≤	O
0	O
implies	O
that	O
x	O
≤	O
c	O
+	O
b2	O
+	O
b	O
√	O
√	O
the	O
last	O
assumption	O
of	O
theorem	O
21.16	O
is	O
called	O
separability	O
with	O
large	O
margin	B
(	O
see	O
chapter	O
15	O
)	O
.	O
that	O
is	O
,	O
there	O
exists	O
w	O
(	O
cid:63	O
)	O
that	O
not	O
only	O
satisﬁes	O
that	O
the	O
point	O
xt	O
lies	O
on	O
the	O
correct	O
side	O
of	O
the	O
halfspace	B
,	O
it	O
also	O
guarantees	O
that	O
xt	O
is	O
not	O
too	O
close	O
to	O
the	O
decision	O
boundary	O
.	O
more	O
speciﬁcally	O
,	O
the	O
distance	O
from	O
xt	O
to	O
the	O
decision	O
boundary	O
is	O
at	O
least	O
γ	O
=	O
1/	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
and	O
the	O
bound	O
becomes	O
(	O
r/γ	O
)	O
2.	O
when	O
the	O
separability	O
assumption	O
does	O
not	O
hold	O
,	O
the	O
bound	O
involves	O
the	O
term	O
[	O
1	O
−	O
yt	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
xt	O
(	O
cid:105	O
)	O
]	O
+	O
which	O
measures	O
how	O
much	O
the	O
separability	O
with	O
margin	B
require-	O
ment	O
is	O
violated	O
.	O
as	O
a	O
last	O
remark	O
we	O
note	O
that	O
there	O
can	O
be	O
cases	O
in	O
which	O
there	O
exists	O
some	O
w	O
(	O
cid:63	O
)	O
that	O
makes	O
zero	O
errors	O
on	O
the	O
sequence	O
but	O
the	O
perceptron	O
will	O
make	O
many	O
errors	O
.	O
indeed	O
,	O
this	O
is	O
a	O
direct	O
consequence	O
of	O
the	O
fact	O
that	O
ldim	O
(	O
h	O
)	O
=	O
∞	O
.	O
the	O
way	O
we	O
sidestep	O
this	O
impossibility	O
result	O
is	O
by	O
assuming	O
more	O
on	O
the	O
sequence	O
of	O
examples	O
–	O
the	O
bound	O
in	O
theorem	O
21.16	O
will	O
be	O
meaningful	O
only	O
if	O
the	O
cumulative	O
surrogate	B
loss	I
,	O
(	O
cid:80	O
)	O
t	O
ft	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
is	O
not	O
excessively	O
large	O
.	O
21.5	O
summary	O
in	O
this	O
chapter	O
we	O
have	O
studied	O
the	O
online	B
learning	I
model	O
.	O
many	O
of	O
the	O
results	O
we	O
derived	O
for	O
the	O
pac	O
learning	O
model	O
have	O
an	O
analog	O
in	O
the	O
online	B
model	O
.	O
first	O
,	O
we	O
have	O
shown	O
that	O
a	O
combinatorial	O
dimension	B
,	O
the	O
littlestone	O
dimension	B
,	O
char-	O
acterizes	O
online	B
learnability	O
.	O
to	O
show	O
this	O
,	O
we	O
introduced	O
the	O
soa	O
algorithm	O
(	O
for	O
the	O
realizable	O
case	O
)	O
and	O
the	O
weighted-majority	O
algorithm	O
(	O
for	O
the	O
unrealizable	O
case	O
)	O
.	O
we	O
have	O
also	O
studied	O
online	B
convex	I
optimization	I
and	O
have	O
shown	O
that	O
online	B
gradient	I
descent	I
is	O
a	O
successful	O
online	B
learner	O
whenever	O
the	O
loss	B
function	I
is	O
convex	B
and	O
lipschitz	O
.	O
finally	O
,	O
we	O
presented	O
the	O
online	B
perceptron	O
algorithm	O
as	O
a	O
combination	O
of	O
online	B
gradient	I
descent	I
and	O
the	O
concept	O
of	O
surrogate	O
convex	O
loss	B
functions	O
.	O
21.6	O
bibliographic	O
remarks	O
305	O
21.6	O
bibliographic	O
remarks	O
the	O
standard	O
optimal	O
algorithm	O
was	O
derived	O
by	O
the	O
seminal	O
work	O
of	O
lit-	O
tlestone	O
(	O
1988	O
)	O
.	O
a	O
generalization	O
to	O
the	O
nonrealizable	O
case	O
,	O
as	O
well	O
as	O
other	O
variants	O
like	O
margin-based	O
littlestone	O
’	O
s	O
dimension	B
,	O
were	O
derived	O
in	O
(	O
ben-david	O
et	O
al	O
.	O
2009	O
)	O
.	O
characterizations	O
of	O
online	B
learnability	O
beyond	O
classiﬁcation	O
have	O
been	O
obtained	O
in	O
(	O
abernethy	O
,	O
bartlett	O
,	O
rakhlin	O
&	O
tewari	O
2008	O
,	O
rakhlin	O
,	O
srid-	O
haran	O
&	O
tewari	O
2010	O
,	O
daniely	O
et	O
al	O
.	O
2011	O
)	O
.	O
the	O
weighted-majority	O
algorithm	O
is	O
due	O
to	O
(	O
littlestone	O
&	O
warmuth	O
1994	O
)	O
and	O
(	O
vovk	O
1990	O
)	O
.	O
the	O
term	O
“	O
online	O
convex	O
programming	O
”	O
was	O
introduced	O
by	O
zinkevich	O
(	O
2003	O
)	O
but	O
this	O
setting	O
was	O
introduced	O
some	O
years	O
earlier	O
by	O
gordon	O
(	O
1999	O
)	O
.	O
the	O
per-	O
ceptron	O
dates	O
back	O
to	O
rosenblatt	O
(	O
rosenblatt	O
1958	O
)	O
.	O
an	O
analysis	O
for	O
the	O
re-	O
alizable	O
case	O
(	O
with	O
margin	B
assumptions	O
)	O
appears	O
in	O
(	O
agmon	O
1954	O
,	O
minsky	O
&	O
papert	O
1969	O
)	O
.	O
freund	O
and	O
schapire	O
(	O
freund	O
&	O
schapire	O
1999	O
)	O
presented	O
an	O
anal-	O
ysis	O
for	O
the	O
unrealizable	O
case	O
with	O
a	O
squared-hinge-loss	O
based	O
on	O
a	O
reduction	O
to	O
the	O
realizable	O
case	O
.	O
a	O
direct	O
analysis	O
for	O
the	O
unrealizable	O
case	O
with	O
the	O
hinge-loss	O
was	O
given	O
by	O
gentile	O
(	O
gentile	O
2003	O
)	O
.	O
for	O
additional	O
information	O
we	O
refer	O
the	O
reader	O
to	O
cesa-bianchi	O
&	O
lugosi	O
(	O
2006	O
)	O
and	O
shalev-shwartz	O
(	O
2011	O
)	O
.	O
21.7	O
exercises	O
1.	O
find	O
a	O
hypothesis	B
class	I
h	O
and	O
a	O
sequence	O
of	O
examples	O
on	O
which	O
consistent	O
makes	O
|h|	O
−	O
1	O
mistakes	O
.	O
2.	O
find	O
a	O
hypothesis	B
class	I
h	O
and	O
a	O
sequence	O
of	O
examples	O
on	O
which	O
the	O
mistake	B
bound	I
of	O
the	O
halving	O
algorithm	O
is	O
tight	O
.	O
3.	O
let	O
d	O
≥	O
2	O
,	O
x	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
}	O
and	O
let	O
h	O
=	O
{	O
hj	O
:	O
j	O
∈	O
[	O
d	O
]	O
}	O
,	O
where	O
hj	O
(	O
x	O
)	O
=	O
1	O
[	O
x=j	O
]	O
.	O
calculate	O
mhalving	O
(	O
h	O
)	O
(	O
i.e.	O
,	O
derive	O
lower	O
and	O
upper	O
bounds	O
on	O
mhalving	O
(	O
h	O
)	O
,	O
and	O
prove	O
that	O
they	O
are	O
equal	O
)	O
.	O
4.	O
the	O
doubling	O
trick	O
:	O
in	O
theorem	O
21.15	O
,	O
the	O
parameter	O
η	O
depends	O
on	O
the	O
time	O
horizon	O
t	O
.	O
in	O
this	O
√	O
exercise	O
we	O
show	O
how	O
to	O
get	O
rid	O
of	O
this	O
dependence	O
by	O
a	O
simple	O
trick	O
.	O
consider	O
an	O
algorithm	O
that	O
enjoys	O
a	O
regret	O
bound	O
of	O
the	O
form	O
α	O
t	O
,	O
but	O
its	O
parameters	O
require	O
the	O
knowledge	O
of	O
t	O
.	O
the	O
doubling	O
trick	O
,	O
described	O
in	O
the	O
following	O
,	O
enables	O
us	O
to	O
convert	O
such	O
an	O
algorithm	O
into	O
an	O
algorithm	O
that	O
does	O
not	O
need	O
to	O
know	O
the	O
time	O
horizon	O
.	O
the	O
idea	O
is	O
to	O
divide	O
the	O
time	O
into	O
periods	O
of	O
increasing	O
size	O
and	O
run	O
the	O
original	O
algorithm	O
on	O
each	O
period	O
.	O
the	O
doubling	O
trick	O
input	O
:	O
algorithm	O
a	O
whose	O
parameters	O
depend	O
on	O
the	O
time	O
horizon	O
for	O
m	O
=	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
run	O
a	O
on	O
the	O
2m	O
rounds	O
t	O
=	O
2m	O
,	O
.	O
.	O
.	O
,	O
2m+1	O
−	O
1	O
306	O
online	B
learning	I
show	O
that	O
if	O
the	O
regret	O
of	O
a	O
on	O
each	O
period	O
of	O
2m	O
rounds	O
is	O
at	O
most	O
α	O
then	O
the	O
total	O
regret	O
is	O
at	O
most	O
√	O
2√	O
2	O
−	O
1	O
√	O
t	O
.	O
α	O
√	O
2m	O
,	O
5.	O
online-to-batch	O
conversions	O
:	O
in	O
this	O
exercise	O
we	O
demonstrate	O
how	O
a	O
suc-	O
cessful	O
online	B
learning	I
algorithm	O
can	O
be	O
used	O
to	O
derive	O
a	O
successful	O
pac	O
learner	O
as	O
well	O
.	O
consider	O
a	O
pac	O
learning	O
problem	O
for	O
binary	O
classiﬁcation	O
parameterized	O
by	O
an	O
instance	B
domain	O
,	O
x	O
,	O
and	O
a	O
hypothesis	B
class	I
,	O
h.	O
suppose	O
that	O
there	O
exists	O
an	O
online	B
learning	I
algorithm	O
,	O
a	O
,	O
which	O
enjoys	O
a	O
mistake	B
bound	I
ma	O
(	O
h	O
)	O
<	O
∞	O
.	O
consider	O
running	O
this	O
algorithm	O
on	O
a	O
sequence	O
of	O
t	O
examples	O
which	O
are	O
sam-	O
pled	O
i.i.d	O
.	O
from	O
a	O
distribution	O
d	O
over	O
the	O
instance	B
space	I
x	O
,	O
and	O
are	O
labeled	O
by	O
some	O
h	O
(	O
cid:63	O
)	O
∈	O
h.	O
suppose	O
that	O
for	O
every	O
round	O
t	O
,	O
the	O
prediction	O
of	O
the	O
algorithm	O
is	O
based	O
on	O
a	O
hypothesis	B
ht	O
:	O
x	O
→	O
{	O
0	O
,	O
1	O
}	O
.	O
show	O
that	O
e	O
[	O
ld	O
(	O
hr	O
)	O
]	O
≤	O
ma	O
(	O
h	O
)	O
,	O
t	O
where	O
the	O
expectation	O
is	O
over	O
the	O
random	O
choice	O
of	O
the	O
instances	O
as	O
well	O
as	O
a	O
random	O
choice	O
of	O
r	O
according	O
to	O
the	O
uniform	O
distribution	O
over	O
[	O
t	O
]	O
.	O
hint	O
:	O
use	O
similar	O
arguments	O
to	O
the	O
ones	O
appearing	O
in	O
the	O
proof	O
of	O
theo-	O
rem	O
14.8	O
.	O
22	O
clustering	B
clustering	O
is	O
one	O
of	O
the	O
most	O
widely	O
used	O
techniques	O
for	O
exploratory	O
data	O
anal-	O
ysis	O
.	O
across	O
all	O
disciplines	O
,	O
from	O
social	O
sciences	O
to	O
biology	O
to	O
computer	O
science	O
,	O
people	O
try	O
to	O
get	O
a	O
ﬁrst	O
intuition	O
about	O
their	O
data	O
by	O
identifying	O
meaningful	O
groups	O
among	O
the	O
data	O
points	O
.	O
for	O
example	O
,	O
computational	O
biologists	O
cluster	O
genes	O
on	O
the	O
basis	O
of	O
similarities	O
in	O
their	O
expression	O
in	O
diﬀerent	O
experiments	O
;	O
re-	O
tailers	O
cluster	O
customers	O
,	O
on	O
the	O
basis	O
of	O
their	O
customer	O
proﬁles	O
,	O
for	O
the	O
purpose	O
of	O
targeted	O
marketing	O
;	O
and	O
astronomers	O
cluster	O
stars	O
on	O
the	O
basis	O
of	O
their	O
spacial	O
proximity	O
.	O
the	O
ﬁrst	O
point	O
that	O
one	O
should	O
clarify	O
is	O
,	O
naturally	O
,	O
what	O
is	O
clustering	B
?	O
in-	O
tuitively	O
,	O
clustering	B
is	O
the	O
task	O
of	O
grouping	O
a	O
set	B
of	O
objects	O
such	O
that	O
similar	O
objects	O
end	O
up	O
in	O
the	O
same	O
group	O
and	O
dissimilar	O
objects	O
are	O
separated	O
into	O
dif-	O
ferent	O
groups	O
.	O
clearly	O
,	O
this	O
description	O
is	O
quite	O
imprecise	O
and	O
possibly	O
ambiguous	O
.	O
quite	O
surprisingly	O
,	O
it	O
is	O
not	O
at	O
all	O
clear	O
how	O
to	O
come	O
up	O
with	O
a	O
more	O
rigorous	O
deﬁnition	O
.	O
there	O
are	O
several	O
sources	O
for	O
this	O
diﬃculty	O
.	O
one	O
basic	O
problem	O
is	O
that	O
the	O
two	O
objectives	O
mentioned	O
in	O
the	O
earlier	O
statement	O
may	O
in	O
many	O
cases	O
contradict	O
each	O
other	O
.	O
mathematically	O
speaking	O
,	O
similarity	O
(	O
or	O
proximity	O
)	O
is	O
not	O
a	O
transi-	O
tive	O
relation	O
,	O
while	O
cluster	O
sharing	O
is	O
an	O
equivalence	O
relation	O
and	O
,	O
in	O
particular	O
,	O
it	O
is	O
a	O
transitive	O
relation	O
.	O
more	O
concretely	O
,	O
it	O
may	O
be	O
the	O
case	O
that	O
there	O
is	O
a	O
long	O
sequence	O
of	O
objects	O
,	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
such	O
that	O
each	O
xi	O
is	O
very	O
similar	O
to	O
its	O
two	O
neighbors	O
,	O
xi−1	O
and	O
xi+1	O
,	O
but	O
x1	O
and	O
xm	O
are	O
very	O
dissimilar	O
.	O
if	O
we	O
wish	O
to	O
make	O
sure	O
that	O
whenever	O
two	O
elements	O
are	O
similar	O
they	O
share	O
the	O
same	O
cluster	O
,	O
then	O
we	O
must	O
put	O
all	O
of	O
the	O
elements	O
of	O
the	O
sequence	O
in	O
the	O
same	O
cluster	O
.	O
however	O
,	O
in	O
that	O
case	O
,	O
we	O
end	O
up	O
with	O
dissimilar	O
elements	O
(	O
x1	O
and	O
xm	O
)	O
sharing	O
a	O
cluster	O
,	O
thus	O
violating	O
the	O
second	O
requirement	O
.	O
to	O
illustrate	O
this	O
point	O
further	O
,	O
suppose	O
that	O
we	O
would	O
like	O
to	O
cluster	O
the	O
points	O
in	O
the	O
following	O
picture	O
into	O
two	O
clusters	O
.	O
a	O
clustering	B
algorithm	O
that	O
emphasizes	O
not	O
separating	O
close-by	O
points	O
(	O
e.g.	O
,	O
the	O
single	B
linkage	I
algorithm	O
that	O
will	O
be	O
described	O
in	O
section	O
22.1	O
)	O
will	O
cluster	O
this	O
input	O
by	O
separating	O
it	O
horizontally	O
according	O
to	O
the	O
two	O
lines	O
:	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
308	O
clustering	B
in	O
contrast	O
,	O
a	O
clustering	B
method	O
that	O
emphasizes	O
not	O
having	O
far-away	O
points	O
share	O
the	O
same	O
cluster	O
(	O
e.g.	O
,	O
the	O
2-means	O
algorithm	O
that	O
will	O
be	O
described	O
in	O
section	O
22.1	O
)	O
will	O
cluster	O
the	O
same	O
input	O
by	O
dividing	O
it	O
vertically	O
into	O
the	O
right-	O
hand	O
half	O
and	O
the	O
left-hand	O
half	O
:	O
another	O
basic	O
problem	O
is	O
the	O
lack	O
of	O
“	O
ground	O
truth	O
”	O
for	O
clustering	B
,	O
which	O
is	O
a	O
common	O
problem	O
in	O
unsupervised	B
learning	I
.	O
so	O
far	O
in	O
the	O
book	O
,	O
we	O
have	O
mainly	O
dealt	O
with	O
supervised	O
learning	O
(	O
e.g.	O
,	O
the	O
problem	O
of	O
learning	O
a	O
classiﬁer	B
from	O
labeled	O
training	O
data	O
)	O
.	O
the	O
goal	O
of	O
supervised	O
learning	O
is	O
clear	O
–	O
we	O
wish	O
to	O
learn	O
a	O
classiﬁer	B
which	O
will	O
predict	O
the	O
labels	O
of	O
future	O
examples	O
as	O
accurately	O
as	O
possible	O
.	O
furthermore	O
,	O
a	O
supervised	O
learner	O
can	O
estimate	O
the	O
success	O
,	O
or	O
the	O
risk	B
,	O
of	O
its	O
hypotheses	O
using	O
the	O
labeled	O
training	O
data	O
by	O
computing	O
the	O
empirical	O
loss	O
.	O
in	O
contrast	O
,	O
clustering	B
is	O
an	O
unsupervised	B
learning	I
problem	O
;	O
namely	O
,	O
there	O
are	O
no	O
labels	O
that	O
we	O
try	O
to	O
predict	O
.	O
instead	O
,	O
we	O
wish	O
to	O
organize	O
the	O
data	O
in	O
some	O
meaningful	O
way	O
.	O
as	O
a	O
result	O
,	O
there	O
is	O
no	O
clear	O
success	O
evaluation	O
procedure	O
for	O
clustering	B
.	O
in	O
fact	O
,	O
even	O
on	O
the	O
basis	O
of	O
full	O
knowledge	O
of	O
the	O
underlying	O
data	O
distribution	O
,	O
it	O
is	O
not	O
clear	O
what	O
is	O
the	O
“	O
correct	O
”	O
clustering	B
for	O
that	O
data	O
or	O
how	O
to	O
evaluate	O
a	O
proposed	O
clustering	B
.	O
consider	O
,	O
for	O
example	O
,	O
the	O
following	O
set	B
of	O
points	O
in	O
r2	O
:	O
and	O
suppose	O
we	O
are	O
required	O
to	O
cluster	O
them	O
into	O
two	O
clusters	O
.	O
we	O
have	O
two	O
highly	O
justiﬁable	O
solutions	O
:	O
clustering	B
309	O
this	O
phenomenon	O
is	O
not	O
just	O
artiﬁcial	O
but	O
occurs	O
in	O
real	O
applications	O
.	O
a	O
given	O
set	B
of	O
objects	O
can	O
be	O
clustered	O
in	O
various	O
diﬀerent	O
meaningful	O
ways	O
.	O
this	O
may	O
be	O
due	O
to	O
having	O
diﬀerent	O
implicit	O
notions	O
of	O
distance	O
(	O
or	O
similarity	O
)	O
between	O
objects	O
,	O
for	O
example	O
,	O
clustering	B
recordings	O
of	O
speech	O
by	O
the	O
accent	O
of	O
the	O
speaker	O
versus	O
clustering	B
them	O
by	O
content	O
,	O
clustering	B
movie	O
reviews	O
by	O
movie	O
topic	O
versus	O
clustering	B
them	O
by	O
the	O
review	O
sentiment	O
,	O
clustering	B
paintings	O
by	O
topic	O
versus	O
clustering	B
them	O
by	O
style	O
,	O
and	O
so	O
on	O
.	O
to	O
summarize	O
,	O
there	O
may	O
be	O
several	O
very	O
diﬀerent	O
conceivable	O
clustering	B
so-	O
lutions	O
for	O
a	O
given	O
data	O
set	B
.	O
as	O
a	O
result	O
,	O
there	O
is	O
a	O
wide	O
variety	O
of	O
clustering	B
algorithms	O
that	O
,	O
on	O
some	O
input	O
data	O
,	O
will	O
output	O
very	O
diﬀerent	O
clusterings	O
.	O
a	O
clustering	B
model	O
:	O
clustering	B
tasks	O
can	O
vary	O
in	O
terms	O
of	O
both	O
the	O
type	O
of	O
input	O
they	O
have	O
and	O
the	O
type	O
of	O
outcome	O
they	O
are	O
expected	O
to	O
compute	O
.	O
for	O
concreteness	O
,	O
we	O
shall	O
focus	O
on	O
the	O
following	O
common	O
setup	O
:	O
where	O
(	O
cid:83	O
)	O
k	O
input	O
—	O
a	O
set	B
of	O
elements	O
,	O
x	O
,	O
and	O
a	O
distance	O
function	B
over	O
it	O
.	O
that	O
is	O
,	O
a	O
function	B
d	O
:	O
x	O
×	O
x	O
→	O
r+	O
that	O
is	O
symmetric	O
,	O
satisﬁes	O
d	O
(	O
x	O
,	O
x	O
)	O
=	O
0	O
for	O
all	O
x	O
∈	O
x	O
and	O
often	O
also	O
satisﬁes	O
the	O
triangle	O
inequality	O
.	O
alternatively	O
,	O
the	O
function	B
could	O
be	O
a	O
similarity	O
function	B
s	O
:	O
x	O
×	O
x	O
→	O
[	O
0	O
,	O
1	O
]	O
that	O
is	O
symmetric	O
and	O
satisﬁes	O
s	O
(	O
x	O
,	O
x	O
)	O
=	O
1	O
for	O
all	O
x	O
∈	O
x	O
.	O
additionally	O
,	O
some	O
clustering	B
algorithms	O
also	O
require	O
an	O
input	O
parameter	O
k	O
(	O
determining	O
the	O
number	O
of	O
required	O
clusters	O
)	O
.	O
output	O
—	O
a	O
partition	O
of	O
the	O
domain	B
set	O
x	O
into	O
subsets	O
.	O
that	O
is	O
,	O
c	O
=	O
(	O
c1	O
,	O
.	O
.	O
.	O
ck	O
)	O
i=1	O
ci	O
=	O
x	O
and	O
for	O
all	O
i	O
(	O
cid:54	O
)	O
=	O
j	O
,	O
ci	O
∩	O
cj	O
=	O
∅	O
.	O
in	O
some	O
situations	O
the	O
clustering	B
is	O
“	O
soft	O
,	O
”	O
namely	O
,	O
the	O
partition	O
of	O
x	O
into	O
the	O
diﬀerent	O
clusters	O
is	O
probabilistic	O
where	O
the	O
output	O
is	O
a	O
function	B
assigning	O
to	O
each	O
domain	B
point	O
,	O
x	O
∈	O
x	O
,	O
a	O
vector	O
(	O
p1	O
(	O
x	O
)	O
,	O
.	O
.	O
.	O
,	O
pk	O
(	O
x	O
)	O
)	O
,	O
where	O
pi	O
(	O
x	O
)	O
=	O
p	O
[	O
x	O
∈	O
ci	O
]	O
is	O
the	O
probability	O
that	O
x	O
belongs	O
to	O
cluster	O
ci	O
.	O
another	O
possible	O
output	O
is	O
a	O
clustering	B
dendrogram	O
(	O
from	O
greek	O
dendron	O
=	O
tree	O
,	O
gramma	O
=	O
draw-	O
ing	O
)	O
,	O
which	O
is	O
a	O
hierarchical	O
tree	O
of	O
domain	B
subsets	O
,	O
having	O
the	O
singleton	O
sets	O
in	O
its	O
leaves	O
,	O
and	O
the	O
full	O
domain	B
as	O
its	O
root	O
.	O
we	O
shall	O
discuss	O
this	O
formulation	O
in	O
more	O
detail	O
in	O
the	O
following	O
.	O
310	O
clustering	B
in	O
the	O
following	O
we	O
survey	O
some	O
of	O
the	O
most	O
popular	O
clustering	B
methods	O
.	O
in	O
the	O
last	O
section	O
of	O
this	O
chapter	O
we	O
return	O
to	O
the	O
high	O
level	O
discussion	O
of	O
what	O
is	O
clustering	B
.	O
22.1	O
linkage-based	O
clustering	B
algorithms	O
linkage-based	O
clustering	B
is	O
probably	O
the	O
simplest	O
and	O
most	O
straightforward	O
paradigm	O
of	O
clustering	B
.	O
these	O
algorithms	O
proceed	O
in	O
a	O
sequence	O
of	O
rounds	O
.	O
they	O
start	O
from	O
the	O
trivial	O
clustering	B
that	O
has	O
each	O
data	O
point	O
as	O
a	O
single-point	O
cluster	O
.	O
then	O
,	O
repeatedly	O
,	O
these	O
algorithms	O
merge	O
the	O
“	O
closest	O
”	O
clusters	O
of	O
the	O
previous	O
cluster-	O
ing	O
.	O
consequently	O
,	O
the	O
number	O
of	O
clusters	O
decreases	O
with	O
each	O
such	O
round	O
.	O
if	O
kept	O
going	O
,	O
such	O
algorithms	O
would	O
eventually	O
result	O
in	O
the	O
trivial	O
clustering	B
in	O
which	O
all	O
of	O
the	O
domain	B
points	O
share	O
one	O
large	O
cluster	O
.	O
two	O
parameters	O
,	O
then	O
,	O
need	O
to	O
be	O
determined	O
to	O
deﬁne	O
such	O
an	O
algorithm	O
clearly	O
.	O
first	O
,	O
we	O
have	O
to	O
decide	O
how	O
to	O
measure	O
(	O
or	O
deﬁne	O
)	O
the	O
distance	O
between	O
clusters	O
,	O
and	O
,	O
second	O
,	O
we	O
have	O
to	O
determine	O
when	O
to	O
stop	O
merging	O
.	O
recall	B
that	O
the	O
input	O
to	O
a	O
clustering	B
algorithm	O
is	O
a	O
between-points	O
distance	O
function	B
,	O
d.	O
there	O
are	O
many	O
ways	O
of	O
extending	O
d	O
to	O
a	O
measure	O
of	O
distance	O
between	O
domain	B
subsets	O
(	O
or	O
clusters	O
)	O
.	O
the	O
most	O
common	O
ways	O
are	O
1.	O
single	B
linkage	I
clustering	O
,	O
in	O
which	O
the	O
between-clusters	O
distance	O
is	O
deﬁned	O
by	O
the	O
minimum	O
distance	O
between	O
members	O
of	O
the	O
two	O
clusters	O
,	O
namely	O
,	O
d	O
(	O
a	O
,	O
b	O
)	O
def=	O
min	O
{	O
d	O
(	O
x	O
,	O
y	O
)	O
:	O
x	O
∈	O
a	O
,	O
y	O
∈	O
b	O
}	O
2.	O
average	O
linkage	B
clustering	O
,	O
in	O
which	O
the	O
distance	O
between	O
two	O
clusters	O
is	O
deﬁned	O
to	O
be	O
the	O
average	O
distance	O
between	O
a	O
point	O
in	O
one	O
of	O
the	O
clusters	O
and	O
a	O
point	O
in	O
the	O
other	O
,	O
namely	O
,	O
(	O
cid:88	O
)	O
d	O
(	O
a	O
,	O
b	O
)	O
def=	O
1	O
|a||b|	O
d	O
(	O
x	O
,	O
y	O
)	O
x∈a	O
,	O
y∈b	O
3.	O
max	B
linkage	I
clustering	O
,	O
in	O
which	O
the	O
distance	O
between	O
two	O
clusters	O
is	O
deﬁned	O
as	O
the	O
maximum	O
distance	O
between	O
their	O
elements	O
,	O
namely	O
,	O
d	O
(	O
a	O
,	O
b	O
)	O
def=	O
max	O
{	O
d	O
(	O
x	O
,	O
y	O
)	O
:	O
x	O
∈	O
a	O
,	O
y	O
∈	O
b	O
}	O
.	O
the	O
linkage-based	O
clustering	B
algorithms	O
are	O
agglomerative	O
in	O
the	O
sense	O
that	O
they	O
start	O
from	O
data	O
that	O
is	O
completely	O
fragmented	O
and	O
keep	O
building	O
larger	O
and	O
larger	O
clusters	O
as	O
they	O
proceed	O
.	O
without	O
employing	O
a	O
stopping	O
rule	O
,	O
the	O
outcome	O
of	O
such	O
an	O
algorithm	O
can	O
be	O
described	O
by	O
a	O
clustering	B
dendrogram	O
:	O
that	O
is	O
,	O
a	O
tree	O
of	O
domain	B
subsets	O
,	O
having	O
the	O
singleton	O
sets	O
in	O
its	O
leaves	O
,	O
and	O
the	O
full	O
domain	B
as	O
its	O
root	O
.	O
for	O
example	O
,	O
if	O
the	O
input	O
is	O
the	O
elements	O
x	O
=	O
{	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
}	O
⊂	O
r2	O
with	O
the	O
euclidean	O
distance	O
as	O
depicted	O
on	O
the	O
left	O
,	O
then	O
the	O
resulting	O
dendrogram	B
is	O
the	O
one	O
depicted	O
on	O
the	O
right	O
:	O
22.2	O
k-means	B
and	O
other	O
cost	O
minimization	O
clusterings	O
311	O
a	O
e	O
d	O
c	O
b	O
{	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
}	O
{	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
}	O
{	O
b	O
,	O
c	O
}	O
{	O
d	O
,	O
e	O
}	O
{	O
a	O
}	O
{	O
b	O
}	O
{	O
c	O
}	O
{	O
d	O
}	O
{	O
e	O
}	O
the	O
single	B
linkage	I
algorithm	O
is	O
closely	O
related	O
to	O
kruskal	O
’	O
s	O
algorithm	O
for	O
ﬁnding	O
a	O
minimal	O
spanning	O
tree	O
on	O
a	O
weighted	O
graph	O
.	O
indeed	O
,	O
consider	O
the	O
full	O
graph	O
whose	O
vertices	O
are	O
elements	O
of	O
x	O
and	O
the	O
weight	O
of	O
an	O
edge	O
(	O
x	O
,	O
y	O
)	O
is	O
the	O
distance	O
d	O
(	O
x	O
,	O
y	O
)	O
.	O
each	O
merge	O
of	O
two	O
clusters	O
performed	O
by	O
the	O
single	B
linkage	I
algorithm	O
corresponds	O
to	O
a	O
choice	O
of	O
an	O
edge	O
in	O
the	O
aforementioned	O
graph	O
.	O
it	O
is	O
also	O
possible	O
to	O
show	O
that	O
the	O
set	B
of	O
edges	O
the	O
single	B
linkage	I
algorithm	O
chooses	O
along	O
its	O
run	O
forms	O
a	O
minimal	O
spanning	O
tree	O
.	O
if	O
one	O
wishes	O
to	O
turn	O
a	O
dendrogram	B
into	O
a	O
partition	O
of	O
the	O
space	O
(	O
a	O
clustering	B
)	O
,	O
one	O
needs	O
to	O
employ	O
a	O
stopping	O
criterion	O
.	O
common	O
stopping	O
criteria	O
include	O
•	O
fixed	O
number	O
of	O
clusters	O
–	O
ﬁx	O
some	O
parameter	O
,	O
k	O
,	O
and	O
stop	O
merging	O
clusters	O
as	O
soon	O
as	O
the	O
number	O
of	O
clusters	O
is	O
k.	O
•	O
distance	O
upper	O
bound	O
–	O
ﬁx	O
some	O
r	O
∈	O
r+	O
.	O
stop	O
merging	O
as	O
soon	O
as	O
all	O
the	O
between-clusters	O
distances	O
are	O
larger	O
than	O
r.	O
we	O
can	O
also	O
set	B
r	O
to	O
be	O
α	O
max	O
{	O
d	O
(	O
x	O
,	O
y	O
)	O
:	O
x	O
,	O
y	O
∈	O
x	O
}	O
for	O
some	O
α	O
<	O
1.	O
in	O
that	O
case	O
the	O
stopping	O
criterion	O
is	O
called	O
“	O
scaled	O
distance	O
upper	O
bound.	O
”	O
22.2	O
k-means	B
and	O
other	O
cost	O
minimization	O
clusterings	O
another	O
popular	O
approach	O
to	O
clustering	B
starts	O
by	O
deﬁning	O
a	O
cost	O
function	B
over	O
a	O
parameterized	O
set	B
of	O
possible	O
clusterings	O
and	O
the	O
goal	O
of	O
the	O
clustering	B
algorithm	O
is	O
to	O
ﬁnd	O
a	O
partitioning	O
(	O
clustering	B
)	O
of	O
minimal	O
cost	O
.	O
under	O
this	O
paradigm	O
,	O
the	O
clustering	B
task	O
is	O
turned	O
into	O
an	O
optimization	O
problem	O
.	O
the	O
objective	O
function	B
is	O
a	O
function	B
from	O
pairs	O
of	O
an	O
input	O
,	O
(	O
x	O
,	O
d	O
)	O
,	O
and	O
a	O
proposed	O
clustering	B
solution	O
c	O
=	O
(	O
c1	O
,	O
.	O
.	O
.	O
,	O
ck	O
)	O
,	O
to	O
positive	O
real	O
numbers	O
.	O
given	O
such	O
an	O
objective	O
function	B
,	O
which	O
we	O
denote	O
by	O
g	O
,	O
the	O
goal	O
of	O
a	O
clustering	B
algorithm	O
is	O
deﬁned	O
as	O
ﬁnding	O
,	O
for	O
a	O
given	O
input	O
(	O
x	O
,	O
d	O
)	O
,	O
a	O
clustering	B
c	O
so	O
that	O
g	O
(	O
(	O
x	O
,	O
d	O
)	O
,	O
c	O
)	O
is	O
minimized	O
.	O
in	O
order	O
to	O
reach	O
that	O
goal	O
,	O
one	O
has	O
to	O
apply	O
some	O
appropriate	O
search	O
algorithm	O
.	O
as	O
it	O
turns	O
out	O
,	O
most	O
of	O
the	O
resulting	O
optimization	O
problems	O
are	O
np-hard	O
,	O
and	O
some	O
are	O
even	O
np-hard	O
to	O
approximate	O
.	O
consequently	O
,	O
when	O
people	O
talk	O
about	O
,	O
say	O
,	O
k-means	B
clustering	O
,	O
they	O
often	O
refer	O
to	O
some	O
particular	O
common	O
approxima-	O
tion	O
algorithm	O
rather	O
than	O
the	O
cost	O
function	B
or	O
the	O
corresponding	O
exact	O
solution	O
of	O
the	O
minimization	O
problem	O
.	O
many	O
common	O
objective	O
functions	O
require	O
the	O
number	O
of	O
clusters	O
,	O
k	O
,	O
as	O
a	O
312	O
clustering	B
parameter	O
.	O
in	O
practice	O
,	O
it	O
is	O
often	O
up	O
to	O
the	O
user	O
of	O
the	O
clustering	B
algorithm	O
to	O
choose	O
the	O
parameter	O
k	O
that	O
is	O
most	O
suitable	O
for	O
the	O
given	O
clustering	B
problem	O
.	O
in	O
the	O
following	O
we	O
describe	O
some	O
of	O
the	O
most	O
common	O
objective	O
functions	O
.	O
•	O
the	O
k-means	B
objective	O
function	B
is	O
one	O
of	O
the	O
most	O
popular	O
clustering	B
objectives	O
.	O
in	O
k-means	B
the	O
data	O
is	O
partitioned	O
into	O
disjoint	O
sets	O
c1	O
,	O
.	O
.	O
.	O
,	O
ck	O
where	O
each	O
ci	O
is	O
represented	O
by	O
a	O
centroid	O
µi	O
.	O
it	O
is	O
assumed	O
that	O
the	O
input	O
set	B
x	O
is	O
embedded	O
in	O
some	O
larger	O
metric	O
space	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
d	O
)	O
(	O
so	O
that	O
x	O
⊆	O
x	O
(	O
cid:48	O
)	O
)	O
and	O
centroids	O
are	O
members	O
of	O
x	O
(	O
cid:48	O
)	O
.	O
the	O
k-means	B
objective	O
function	B
measures	O
the	O
squared	O
distance	O
between	O
each	O
point	O
in	O
x	O
to	O
the	O
centroid	O
of	O
its	O
cluster	O
.	O
the	O
centroid	O
of	O
ci	O
is	O
deﬁned	O
to	O
be	O
(	O
cid:88	O
)	O
x∈ci	O
µi	O
(	O
ci	O
)	O
=	O
argmin	O
µ∈x	O
(	O
cid:48	O
)	O
d	O
(	O
x	O
,	O
µ	O
)	O
2.	O
then	O
,	O
the	O
k-means	B
objective	O
is	O
gk−means	O
(	O
(	O
x	O
,	O
d	O
)	O
,	O
(	O
c1	O
,	O
.	O
.	O
.	O
,	O
ck	O
)	O
)	O
=	O
d	O
(	O
x	O
,	O
µi	O
(	O
ci	O
)	O
)	O
2.	O
this	O
can	O
also	O
be	O
rewritten	O
as	O
gk−means	O
(	O
(	O
x	O
,	O
d	O
)	O
,	O
(	O
c1	O
,	O
.	O
.	O
.	O
,	O
ck	O
)	O
)	O
=	O
min	O
µ1	O
,	O
...	O
µk∈x	O
(	O
cid:48	O
)	O
d	O
(	O
x	O
,	O
µi	O
)	O
2	O
.	O
(	O
22.1	O
)	O
k	O
(	O
cid:88	O
)	O
i=1	O
x∈ci	O
(	O
cid:88	O
)	O
k	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
i=1	O
x∈ci	O
the	O
k-means	B
objective	O
function	B
is	O
relevant	O
,	O
for	O
example	O
,	O
in	O
digital	O
com-	O
munication	O
tasks	O
,	O
where	O
the	O
members	O
of	O
x	O
may	O
be	O
viewed	O
as	O
a	O
collection	O
of	O
signals	O
that	O
have	O
to	O
be	O
transmitted	O
.	O
while	O
x	O
may	O
be	O
a	O
very	O
large	O
set	B
of	O
real	O
valued	O
vectors	O
,	O
digital	O
transmission	O
allows	O
transmitting	O
of	O
only	O
a	O
ﬁnite	O
number	O
of	O
bits	O
for	O
each	O
signal	O
.	O
one	O
way	O
to	O
achieve	O
good	O
transmis-	O
sion	O
under	O
such	O
constraints	O
is	O
to	O
represent	O
each	O
member	O
of	O
x	O
by	O
a	O
“	O
close	O
”	O
member	O
of	O
some	O
ﬁnite	O
set	B
µ1	O
,	O
.	O
.	O
.	O
µk	O
,	O
and	O
replace	O
the	O
transmission	O
of	O
any	O
x	O
∈	O
x	O
by	O
transmitting	O
the	O
index	O
of	O
the	O
closest	O
µi	O
.	O
the	O
k-means	B
objective	O
can	O
be	O
viewed	O
as	O
a	O
measure	O
of	O
the	O
distortion	O
created	O
by	O
such	O
a	O
transmission	O
representation	O
scheme	O
.	O
•	O
the	O
k-medoids	B
objective	O
function	B
is	O
similar	O
to	O
the	O
k-means	B
objective	O
,	O
except	O
that	O
it	O
requires	O
the	O
cluster	O
centroids	O
to	O
be	O
members	O
of	O
the	O
input	O
set	B
.	O
the	O
objective	O
function	B
is	O
deﬁned	O
by	O
gk−medoid	O
(	O
(	O
x	O
,	O
d	O
)	O
,	O
(	O
c1	O
,	O
.	O
.	O
.	O
,	O
ck	O
)	O
)	O
=	O
min	O
µ1	O
,	O
...	O
µk∈x	O
d	O
(	O
x	O
,	O
µi	O
)	O
2	O
.	O
•	O
the	O
k-median	B
objective	O
function	B
is	O
quite	O
similar	O
to	O
the	O
k-medoids	B
objec-	O
tive	O
,	O
except	O
that	O
the	O
“	O
distortion	O
”	O
between	O
a	O
data	O
point	O
and	O
the	O
centroid	O
of	O
its	O
cluster	O
is	O
measured	O
by	O
distance	O
,	O
rather	O
than	O
by	O
the	O
square	O
of	O
the	O
distance	O
:	O
gk−median	O
(	O
(	O
x	O
,	O
d	O
)	O
,	O
(	O
c1	O
,	O
.	O
.	O
.	O
,	O
ck	O
)	O
)	O
=	O
min	O
µ1	O
,	O
...	O
µk∈x	O
d	O
(	O
x	O
,	O
µi	O
)	O
.	O
k	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
i=1	O
x∈ci	O
k	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
i=1	O
x∈ci	O
22.2	O
k-means	B
and	O
other	O
cost	O
minimization	O
clusterings	O
313	O
an	O
example	O
where	O
such	O
an	O
objective	O
makes	O
sense	O
is	O
the	O
facility	O
location	O
problem	O
.	O
consider	O
the	O
task	O
of	O
locating	O
k	O
ﬁre	O
stations	O
in	O
a	O
city	O
.	O
one	O
can	O
model	O
houses	O
as	O
data	O
points	O
and	O
aim	O
to	O
place	O
the	O
stations	O
so	O
as	O
to	O
minimize	O
the	O
average	O
distance	O
between	O
a	O
house	O
and	O
its	O
closest	O
ﬁre	O
station	O
.	O
the	O
previous	O
examples	O
can	O
all	O
be	O
viewed	O
as	O
center-based	O
objectives	O
.	O
the	O
so-	O
lution	O
to	O
such	O
a	O
clustering	B
problem	O
is	O
determined	O
by	O
a	O
set	B
of	O
cluster	O
centers	O
,	O
and	O
the	O
clustering	B
assigns	O
each	O
instance	B
to	O
the	O
center	O
closest	O
to	O
it	O
.	O
more	O
gener-	O
ally	O
,	O
center-based	O
objective	O
is	O
determined	O
by	O
choosing	O
some	O
monotonic	O
function	B
f	O
:	O
r+	O
→	O
r+	O
and	O
then	O
deﬁning	O
some	O
objective	O
functions	O
are	O
not	O
center	O
based	O
.	O
for	O
example	O
,	O
the	O
sum	O
of	O
in-	O
gf	O
(	O
(	O
x	O
,	O
d	O
)	O
,	O
(	O
c1	O
,	O
.	O
.	O
.	O
ck	O
)	O
)	O
=	O
min	O
µ1	O
,	O
...	O
µk∈x	O
(	O
cid:48	O
)	O
where	O
x	O
(	O
cid:48	O
)	O
is	O
either	O
x	O
or	O
some	O
superset	O
of	O
x	O
.	O
cluster	O
distances	O
(	O
sod	O
)	O
gsod	O
(	O
(	O
x	O
,	O
d	O
)	O
,	O
(	O
c1	O
,	O
.	O
.	O
.	O
ck	O
)	O
)	O
=	O
k	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
i=1	O
x∈ci	O
f	O
(	O
d	O
(	O
x	O
,	O
µi	O
)	O
)	O
,	O
k	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
i=1	O
x	O
,	O
y∈ci	O
d	O
(	O
x	O
,	O
y	O
)	O
and	O
the	O
mincut	O
objective	O
that	O
we	O
shall	O
discuss	O
in	O
section	O
22.3	O
are	O
not	O
center-	O
based	O
objectives	O
.	O
22.2.1	O
the	O
k-means	B
algorithm	O
the	O
k-means	B
objective	O
function	B
is	O
quite	O
popular	O
in	O
practical	O
applications	O
of	O
clus-	O
tering	O
.	O
however	O
,	O
it	O
turns	O
out	O
that	O
ﬁnding	O
the	O
optimal	O
k-means	B
solution	O
is	O
of-	O
ten	O
computationally	O
infeasible	O
(	O
the	O
problem	O
is	O
np-hard	O
,	O
and	O
even	O
np-hard	O
to	O
approximate	O
to	O
within	O
some	O
constant	O
)	O
.	O
as	O
an	O
alternative	O
,	O
the	O
following	O
simple	O
iterative	O
algorithm	O
is	O
often	O
used	O
,	O
so	O
often	O
that	O
,	O
in	O
many	O
cases	O
,	O
the	O
term	O
k-means	B
clustering	O
refers	O
to	O
the	O
outcome	O
of	O
this	O
algorithm	O
rather	O
than	O
to	O
the	O
cluster-	O
ing	O
that	O
minimizes	O
the	O
k-means	B
objective	O
cost	O
.	O
we	O
describe	O
the	O
algorithm	O
with	O
respect	O
to	O
the	O
euclidean	O
distance	O
function	B
d	O
(	O
x	O
,	O
y	O
)	O
=	O
(	O
cid:107	O
)	O
x	O
−	O
y	O
(	O
cid:107	O
)	O
.	O
k-means	B
input	O
:	O
x	O
⊂	O
rn	O
;	O
number	O
of	O
clusters	O
k	O
initialize	O
:	O
randomly	O
choose	O
initial	O
centroids	O
µ1	O
,	O
.	O
.	O
.	O
,	O
µk	O
repeat	O
until	O
convergence	O
∀i	O
∈	O
[	O
k	O
]	O
set	B
ci	O
=	O
{	O
x	O
∈	O
x	O
:	O
i	O
=	O
argminj	O
(	O
cid:107	O
)	O
x	O
−	O
µj	O
(	O
cid:107	O
)	O
}	O
(	O
break	O
ties	O
in	O
some	O
arbitrary	O
manner	O
)	O
∀i	O
∈	O
[	O
k	O
]	O
update	O
µi	O
=	O
1|ci|	O
(	O
cid:80	O
)	O
x∈ci	O
x	O
lemma	O
22.1	O
each	O
iteration	O
of	O
the	O
k-means	B
algorithm	O
does	O
not	O
increase	O
the	O
k-means	B
objective	O
function	B
(	O
as	O
given	O
in	O
equation	O
(	O
22.1	O
)	O
)	O
.	O
314	O
clustering	B
proof	O
to	O
simplify	O
the	O
notation	O
,	O
let	O
us	O
use	O
the	O
shorthand	O
g	O
(	O
c1	O
,	O
.	O
.	O
.	O
,	O
ck	O
)	O
for	O
the	O
k-means	B
objective	O
,	O
namely	O
,	O
g	O
(	O
c1	O
,	O
.	O
.	O
.	O
,	O
ck	O
)	O
=	O
min	O
µ1	O
,	O
...	O
,	O
µk∈rn	O
(	O
cid:107	O
)	O
x	O
−	O
µi	O
(	O
cid:107	O
)	O
2	O
.	O
(	O
22.2	O
)	O
it	O
is	O
convenient	O
to	O
deﬁne	O
µ	O
(	O
ci	O
)	O
=	O
1|ci|	O
µ	O
(	O
cid:107	O
)	O
2.	O
therefore	O
,	O
we	O
can	O
rewrite	O
the	O
k-means	B
objective	O
as	O
x∈ci	O
x	O
and	O
note	O
that	O
µ	O
(	O
ci	O
)	O
=	O
argminµ∈rn	O
(	O
cid:80	O
)	O
x∈ci	O
(	O
cid:107	O
)	O
x−	O
k	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
i=1	O
x∈ci	O
(	O
cid:80	O
)	O
k	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
i=1	O
x∈ci	O
g	O
(	O
c1	O
,	O
.	O
.	O
.	O
,	O
ck	O
)	O
=	O
(	O
cid:107	O
)	O
x	O
−	O
µ	O
(	O
ci	O
)	O
(	O
cid:107	O
)	O
2	O
.	O
(	O
22.3	O
)	O
(	O
cid:88	O
)	O
x∈c	O
(	O
t	O
)	O
i	O
k	O
)	O
≤	O
k	O
(	O
cid:88	O
)	O
(	O
cid:80	O
)	O
i=1	O
x∈ci	O
,	O
.	O
.	O
.	O
,	O
c	O
(	O
t−1	O
)	O
consider	O
the	O
update	O
at	O
iteration	O
t	O
of	O
the	O
k-means	B
algorithm	O
.	O
let	O
c	O
(	O
t−1	O
)	O
be	O
the	O
previous	O
partition	O
,	O
let	O
µ	O
(	O
t−1	O
)	O
k	O
be	O
the	O
new	O
partition	O
assigned	O
at	O
iteration	O
t.	O
using	O
the	O
deﬁnition	O
of	O
the	O
objective	O
as	O
given	O
in	O
equation	O
(	O
22.2	O
)	O
we	O
clearly	O
have	O
that	O
=	O
µ	O
(	O
c	O
(	O
t−1	O
)	O
)	O
,	O
and	O
let	O
c	O
(	O
t	O
)	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
(	O
t	O
)	O
k	O
1	O
i	O
i	O
g	O
(	O
c	O
(	O
t	O
)	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
(	O
t	O
)	O
(	O
cid:107	O
)	O
x	O
−	O
µ	O
(	O
t−1	O
)	O
i	O
(	O
cid:107	O
)	O
2	O
.	O
(	O
22.4	O
)	O
in	O
addition	O
,	O
the	O
deﬁnition	O
of	O
the	O
new	O
partition	O
(	O
c	O
(	O
t	O
)	O
minimizes	O
the	O
expression	O
(	O
cid:80	O
)	O
k	O
i=1	O
(	O
cid:107	O
)	O
x	O
−	O
µ	O
(	O
t−1	O
)	O
i	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
(	O
t	O
)	O
k	O
)	O
implies	O
that	O
it	O
(	O
cid:107	O
)	O
2	O
over	O
all	O
possible	O
partitions	O
(	O
c1	O
,	O
.	O
.	O
.	O
,	O
ck	O
)	O
.	O
hence	O
,	O
k	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
i=1	O
x∈c	O
(	O
t	O
)	O
i	O
(	O
cid:107	O
)	O
x	O
−	O
µ	O
(	O
t−1	O
)	O
i	O
(	O
cid:107	O
)	O
2	O
≤	O
k	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
i=1	O
x∈c	O
(	O
t−1	O
)	O
i	O
(	O
cid:107	O
)	O
x	O
−	O
µ	O
(	O
t−1	O
)	O
i	O
(	O
cid:107	O
)	O
2	O
.	O
(	O
22.5	O
)	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
(	O
t−1	O
)	O
using	O
equation	O
(	O
22.3	O
)	O
we	O
have	O
that	O
the	O
right-hand	O
side	O
of	O
equation	O
(	O
22.5	O
)	O
equals	O
g	O
(	O
c	O
(	O
t−1	O
)	O
)	O
.	O
combining	O
this	O
with	O
equation	O
(	O
22.4	O
)	O
and	O
equation	O
(	O
22.5	O
)	O
,	O
we	O
obtain	O
that	O
g	O
(	O
c	O
(	O
t	O
)	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
(	O
t	O
)	O
)	O
,	O
which	O
concludes	O
our	O
proof	O
.	O
k	O
)	O
≤	O
g	O
(	O
c	O
(	O
t−1	O
)	O
,	O
.	O
.	O
.	O
,	O
c	O
(	O
t−1	O
)	O
k	O
k	O
1	O
while	O
the	O
preceding	O
lemma	O
tells	O
us	O
that	O
the	O
k-means	B
objective	O
is	O
monotonically	O
nonincreasing	O
,	O
there	O
is	O
no	O
guarantee	O
on	O
the	O
number	O
of	O
iterations	O
the	O
k-means	B
al-	O
gorithm	O
needs	O
in	O
order	O
to	O
reach	O
convergence	O
.	O
furthermore	O
,	O
there	O
is	O
no	O
nontrivial	O
lower	O
bound	O
on	O
the	O
gap	O
between	O
the	O
value	O
of	O
the	O
k-means	B
objective	O
of	O
the	O
al-	O
gorithm	O
’	O
s	O
output	O
and	O
the	O
minimum	O
possible	O
value	O
of	O
that	O
objective	O
function	B
.	O
in	O
fact	O
,	O
k-means	B
might	O
converge	O
to	O
a	O
point	O
which	O
is	O
not	O
even	O
a	O
local	B
minimum	I
(	O
see	O
exercise	O
2	O
)	O
.	O
to	O
improve	O
the	O
results	O
of	O
k-means	B
it	O
is	O
often	O
recommended	O
to	O
repeat	O
the	O
procedure	O
several	O
times	O
with	O
diﬀerent	O
randomly	O
chosen	O
initial	O
centroids	O
(	O
e.g.	O
,	O
we	O
can	O
choose	O
the	O
initial	O
centroids	O
to	O
be	O
random	O
points	O
from	O
the	O
data	O
)	O
.	O
22.3	O
spectral	B
clustering	I
315	O
22.3	O
spectral	B
clustering	I
often	O
,	O
a	O
convenient	O
way	O
to	O
represent	O
the	O
relationships	O
between	O
points	O
in	O
a	O
data	O
set	B
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
}	O
is	O
by	O
a	O
similarity	O
graph	O
;	O
each	O
vertex	O
represents	O
a	O
data	O
point	O
xi	O
,	O
and	O
every	O
two	O
vertices	O
are	O
connected	O
by	O
an	O
edge	O
whose	O
weight	O
is	O
their	O
similarity	O
,	O
wi	O
,	O
j	O
=	O
s	O
(	O
xi	O
,	O
xj	O
)	O
,	O
where	O
w	O
∈	O
rm	O
,	O
m	O
.	O
for	O
example	O
,	O
we	O
can	O
set	B
wi	O
,	O
j	O
=	O
exp	O
(	O
−d	O
(	O
xi	O
,	O
xj	O
)	O
2/σ2	O
)	O
,	O
where	O
d	O
(	O
·	O
,	O
·	O
)	O
is	O
a	O
distance	O
function	B
and	O
σ	O
is	O
a	O
parameter	O
.	O
the	O
clustering	B
problem	O
can	O
now	O
be	O
formulated	O
as	O
follows	O
:	O
we	O
want	O
to	O
ﬁnd	O
a	O
partition	O
of	O
the	O
graph	O
such	O
that	O
the	O
edges	O
between	O
diﬀerent	O
groups	O
have	O
low	O
weights	O
and	O
the	O
edges	O
within	O
a	O
group	O
have	O
high	O
weights	O
.	O
in	O
the	O
clustering	B
objectives	O
described	O
previously	O
,	O
the	O
focus	O
was	O
on	O
one	O
side	O
of	O
our	O
intuitive	O
deﬁnition	O
of	O
clustering	B
–	O
making	O
sure	O
that	O
points	O
in	O
the	O
same	O
cluster	O
are	O
similar	O
.	O
we	O
now	O
present	O
objectives	O
that	O
focus	O
on	O
the	O
other	O
requirement	O
–	O
points	O
separated	O
into	O
diﬀerent	O
clusters	O
should	O
be	O
nonsimilar	O
.	O
22.3.1	O
graph	O
cut	O
given	O
a	O
graph	O
represented	O
by	O
a	O
similarity	O
matrix	O
w	O
,	O
the	O
simplest	O
and	O
most	O
direct	O
way	O
to	O
construct	O
a	O
partition	O
of	O
the	O
graph	O
is	O
to	O
solve	O
the	O
mincut	O
problem	O
,	O
which	O
chooses	O
a	O
partition	O
c1	O
,	O
.	O
.	O
.	O
,	O
ck	O
that	O
minimizes	O
the	O
objective	O
cut	O
(	O
c1	O
,	O
.	O
.	O
.	O
,	O
ck	O
)	O
=	O
wr	O
,	O
s	O
.	O
k	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
i=1	O
r∈ci	O
,	O
s	O
/∈ci	O
for	O
k	O
=	O
2	O
,	O
the	O
mincut	O
problem	O
can	O
be	O
solved	O
eﬃciently	O
.	O
however	O
,	O
in	O
practice	O
it	O
often	O
does	O
not	O
lead	O
to	O
satisfactory	O
partitions	O
.	O
the	O
problem	O
is	O
that	O
in	O
many	O
cases	O
,	O
the	O
solution	O
of	O
mincut	O
simply	O
separates	O
one	O
individual	O
vertex	O
from	O
the	O
rest	O
of	O
the	O
graph	O
.	O
of	O
course	O
,	O
this	O
is	O
not	O
what	O
we	O
want	O
to	O
achieve	O
in	O
clustering	B
,	O
as	O
clusters	O
should	O
be	O
reasonably	O
large	O
groups	O
of	O
points	O
.	O
several	O
solutions	O
to	O
this	O
problem	O
have	O
been	O
suggested	O
.	O
the	O
simplest	O
solution	O
is	O
to	O
normalize	O
the	O
cut	O
and	O
deﬁne	O
the	O
normalized	O
mincut	O
objective	O
as	O
follows	O
:	O
ratiocut	O
(	O
c1	O
,	O
.	O
.	O
.	O
,	O
ck	O
)	O
=	O
(	O
cid:88	O
)	O
k	O
(	O
cid:88	O
)	O
i=1	O
1	O
|ci|	O
wr	O
,	O
s	O
.	O
r∈ci	O
,	O
s	O
/∈ci	O
the	O
preceding	O
objective	O
assumes	O
smaller	O
values	O
if	O
the	O
clusters	O
are	O
not	O
too	O
small	O
.	O
unfortunately	O
,	O
introducing	O
this	O
balancing	O
makes	O
the	O
problem	O
computationally	O
hard	O
to	O
solve	O
.	O
spectral	B
clustering	I
is	O
a	O
way	O
to	O
relax	O
the	O
problem	O
of	O
minimizing	O
ratiocut	O
.	O
22.3.2	O
graph	O
laplacian	O
and	O
relaxed	O
graph	O
cuts	O
the	O
main	O
mathematical	O
object	O
for	O
spectral	B
clustering	I
is	O
the	O
graph	O
laplacian	O
matrix	O
.	O
there	O
are	O
several	O
diﬀerent	O
deﬁnitions	O
of	O
graph	O
laplacian	O
in	O
the	O
literature	O
,	O
and	O
in	O
the	O
following	O
we	O
describe	O
one	O
particular	O
deﬁnition	O
.	O
316	O
clustering	B
di	O
,	O
i	O
=	O
(	O
cid:80	O
)	O
m	O
definition	O
22.2	O
(	O
unnormalized	O
graph	O
laplacian	O
)	O
the	O
unnormalized	O
graph	O
laplacian	O
is	O
the	O
m	O
×	O
m	O
matrix	O
l	O
=	O
d	O
−	O
w	O
where	O
d	O
is	O
a	O
diagonal	O
matrix	O
with	O
j=1	O
wi	O
,	O
j	O
.	O
the	O
matrix	O
d	O
is	O
called	O
the	O
degree	O
matrix	O
.	O
the	O
following	O
lemma	O
underscores	O
the	O
relation	O
between	O
ratiocut	O
and	O
the	O
lapla-	O
cian	O
matrix	O
.	O
lemma	O
22.3	O
let	O
c1	O
,	O
.	O
.	O
.	O
,	O
ck	O
be	O
a	O
clustering	B
and	O
let	O
h	O
∈	O
rm	O
,	O
k	O
be	O
the	O
matrix	O
such	O
that	O
hi	O
,	O
j	O
=	O
1√	O
|cj|	O
1	O
[	O
i∈cj	O
]	O
.	O
then	O
,	O
the	O
columns	O
of	O
h	O
are	O
orthonormal	O
to	O
each	O
other	O
and	O
ratiocut	O
(	O
c1	O
,	O
.	O
.	O
.	O
,	O
ck	O
)	O
=	O
trace	O
(	O
h	O
(	O
cid:62	O
)	O
l	O
h	O
)	O
.	O
proof	O
let	O
h1	O
,	O
.	O
.	O
.	O
,	O
hk	O
be	O
the	O
columns	O
of	O
h.	O
the	O
fact	O
that	O
these	O
vectors	O
are	O
orthonormal	O
is	O
immediate	O
from	O
the	O
deﬁnition	O
.	O
next	O
,	O
by	O
standard	O
algebraic	O
ma-	O
i	O
lhi	O
and	O
that	O
for	O
nipulations	O
,	O
it	O
can	O
be	O
shown	O
that	O
trace	O
(	O
h	O
(	O
cid:62	O
)	O
l	O
h	O
)	O
=	O
(	O
cid:80	O
)	O
k	O
(	O
cid:33	O
)	O
any	O
vector	O
v	O
we	O
have	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
v	O
(	O
cid:62	O
)	O
lv	O
=	O
1	O
2	O
dr	O
,	O
rv2	O
r	O
−	O
2	O
vrvswr	O
,	O
s	O
+	O
ds	O
,	O
sv2	O
s	O
r	O
r	O
,	O
s	O
s	O
wr	O
,	O
s	O
(	O
vr	O
−	O
vs	O
)	O
2.	O
applying	O
this	O
with	O
v	O
=	O
hi	O
and	O
noting	O
that	O
(	O
hi	O
,	O
r	O
−	O
hi	O
,	O
s	O
)	O
2	O
is	O
nonzero	O
only	O
if	O
r	O
∈	O
ci	O
,	O
s	O
/∈	O
ci	O
or	O
the	O
other	O
way	O
around	O
,	O
we	O
obtain	O
that	O
i=1	O
h	O
(	O
cid:62	O
)	O
(	O
cid:88	O
)	O
=	O
1	O
2	O
r	O
,	O
s	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
h	O
(	O
cid:62	O
)	O
i	O
lhi	O
=	O
1	O
|ci|	O
wr	O
,	O
s	O
.	O
r∈ci	O
,	O
s	O
/∈ci	O
are	O
orthonormal	O
and	O
such	O
that	O
each	O
hi	O
,	O
j	O
is	O
either	O
0	O
or	O
1/	O
(	O
cid:112	O
)	O
|cj|	O
.	O
unfortunately	O
,	O
therefore	O
,	O
to	O
minimize	O
ratiocut	O
we	O
can	O
search	O
for	O
a	O
matrix	O
h	O
whose	O
columns	O
this	O
is	O
an	O
integer	O
programming	O
problem	O
which	O
we	O
can	O
not	O
solve	O
eﬃciently	O
.	O
instead	O
,	O
we	O
relax	O
the	O
latter	O
requirement	O
and	O
simply	O
search	O
an	O
orthonormal	O
matrix	O
h	O
∈	O
rm	O
,	O
k	O
that	O
minimizes	O
trace	O
(	O
h	O
(	O
cid:62	O
)	O
l	O
h	O
)	O
.	O
as	O
we	O
will	O
see	O
in	O
the	O
next	O
chapter	O
about	O
pca	O
(	O
particularly	O
,	O
the	O
proof	O
of	O
theorem	O
23.2	O
)	O
,	O
the	O
solution	O
to	O
this	O
problem	O
is	O
to	O
set	B
u	O
to	O
be	O
the	O
matrix	O
whose	O
columns	O
are	O
the	O
eigenvectors	O
corresponding	O
to	O
the	O
k	O
minimal	O
eigenvalues	O
of	O
l.	O
the	O
resulting	O
algorithm	O
is	O
called	O
unnormalized	O
spectral	B
clustering	I
.	O
22.4	O
information	O
bottleneck*	O
317	O
22.3.3	O
unnormalized	O
spectral	B
clustering	I
unnormalized	O
spectral	B
clustering	I
input	O
:	O
w	O
∈	O
rm	O
,	O
m	O
;	O
number	O
of	O
clusters	O
k	O
initialize	O
:	O
compute	O
the	O
unnormalized	O
graph	O
laplacian	O
l	O
let	O
u	O
∈	O
rm	O
,	O
k	O
be	O
the	O
matrix	O
whose	O
columns	O
are	O
the	O
eigenvectors	O
of	O
l	O
corresponding	O
to	O
the	O
k	O
smallest	O
eigenvalues	O
let	O
v1	O
,	O
.	O
.	O
.	O
,	O
vm	O
be	O
the	O
rows	O
of	O
u	O
cluster	O
the	O
points	O
v1	O
,	O
.	O
.	O
.	O
,	O
vm	O
using	O
k-means	B
output	O
:	O
clusters	O
c1	O
,	O
.	O
.	O
.	O
,	O
ck	O
of	O
the	O
k-means	B
algorithm	O
the	O
spectral	B
clustering	I
algorithm	O
starts	O
with	O
ﬁnding	O
the	O
matrix	O
h	O
of	O
the	O
k	O
eigenvectors	O
corresponding	O
to	O
the	O
smallest	O
eigenvalues	O
of	O
the	O
graph	O
laplacian	O
matrix	O
.	O
it	O
then	O
represents	O
points	O
according	O
to	O
the	O
rows	O
of	O
h.	O
it	O
is	O
due	O
to	O
the	O
properties	O
of	O
the	O
graph	O
laplacians	O
that	O
this	O
change	O
of	O
representation	O
is	O
useful	O
.	O
in	O
many	O
situations	O
,	O
this	O
change	O
of	O
representation	O
enables	O
the	O
simple	O
k-means	B
algorithm	O
to	O
detect	O
the	O
clusters	O
seamlessly	O
.	O
intuitively	O
,	O
if	O
h	O
is	O
as	O
deﬁned	O
in	O
lemma	O
22.3	O
then	O
each	O
point	O
in	O
the	O
new	O
representation	O
is	O
an	O
indicator	O
vector	O
whose	O
value	O
is	O
nonzero	O
only	O
on	O
the	O
element	O
corresponding	O
to	O
the	O
cluster	O
it	O
belongs	O
to	O
.	O
22.4	O
information	O
bottleneck*	O
the	O
information	B
bottleneck	I
method	O
is	O
a	O
clustering	B
technique	O
introduced	O
by	O
tishby	O
,	O
pereira	O
,	O
and	O
bialek	O
.	O
it	O
relies	O
on	O
notions	O
from	O
information	O
theory	O
.	O
to	O
illustrate	O
the	O
method	O
,	O
consider	O
the	O
problem	O
of	O
clustering	B
text	O
documents	O
where	O
each	O
document	O
is	O
represented	O
as	O
a	O
bag-of-words	B
;	O
namely	O
,	O
each	O
document	O
is	O
a	O
vector	O
x	O
=	O
{	O
0	O
,	O
1	O
}	O
n	O
,	O
where	O
n	O
is	O
the	O
size	O
of	O
the	O
dictionary	O
and	O
xi	O
=	O
1	O
iﬀ	O
the	O
word	O
corresponding	O
to	O
index	O
i	O
appears	O
in	O
the	O
document	O
.	O
given	O
a	O
set	B
of	O
m	O
documents	O
,	O
we	O
can	O
interpret	O
the	O
bag-of-words	B
representation	O
of	O
the	O
m	O
documents	O
as	O
a	O
joint	O
probability	O
over	O
a	O
random	O
variable	O
x	O
,	O
indicating	O
the	O
identity	O
of	O
a	O
document	O
(	O
thus	O
taking	O
values	O
in	O
[	O
m	O
]	O
)	O
,	O
and	O
a	O
random	O
variable	O
y	O
,	O
indicating	O
the	O
identity	O
of	O
a	O
word	O
in	O
the	O
dictionary	O
(	O
thus	O
taking	O
values	O
in	O
[	O
n	O
]	O
)	O
.	O
with	O
this	O
interpretation	O
,	O
the	O
information	B
bottleneck	I
refers	O
to	O
the	O
identity	O
of	O
a	O
clustering	B
as	O
another	O
random	O
variable	O
,	O
denoted	O
c	O
,	O
that	O
takes	O
values	O
in	O
[	O
k	O
]	O
(	O
where	O
k	O
will	O
be	O
set	B
by	O
the	O
method	O
as	O
well	O
)	O
.	O
once	O
we	O
have	O
formulated	O
x	O
,	O
y	O
,	O
c	O
as	O
random	O
variables	O
,	O
we	O
can	O
use	O
tools	O
from	O
information	O
theory	O
to	O
express	O
a	O
clustering	B
objective	O
.	O
in	O
particular	O
,	O
the	O
information	B
bottleneck	I
objective	O
is	O
i	O
(	O
x	O
;	O
c	O
)	O
−	O
βi	O
(	O
c	O
;	O
y	O
)	O
,	O
min	O
p	O
(	O
c|x	O
)	O
where	O
i	O
(	O
·	O
;	O
·	O
)	O
is	O
the	O
mutual	O
information	O
between	O
two	O
random	O
variables,1	O
β	O
is	O
a	O
1	O
that	O
is	O
,	O
given	O
a	O
probability	O
function	B
,	O
p	O
over	O
the	O
pairs	O
(	O
x	O
,	O
c	O
)	O
,	O
318	O
clustering	B
parameter	O
,	O
and	O
the	O
minimization	O
is	O
over	O
all	O
possible	O
probabilistic	O
assignments	O
of	O
points	O
to	O
clusters	O
.	O
intuitively	O
,	O
we	O
would	O
like	O
to	O
achieve	O
two	O
contradictory	O
goals	O
.	O
on	O
one	O
hand	O
,	O
we	O
would	O
like	O
the	O
mutual	O
information	O
between	O
the	O
identity	O
of	O
the	O
document	O
and	O
the	O
identity	O
of	O
the	O
cluster	O
to	O
be	O
as	O
small	O
as	O
possible	O
.	O
this	O
reﬂects	O
the	O
fact	O
that	O
we	O
would	O
like	O
a	O
strong	O
compression	O
of	O
the	O
original	O
data	O
.	O
on	O
the	O
other	O
hand	O
,	O
we	O
would	O
like	O
high	O
mutual	O
information	O
between	O
the	O
clustering	B
variable	O
and	O
the	O
identity	O
of	O
the	O
words	O
,	O
which	O
reﬂects	O
the	O
goal	O
that	O
the	O
“	O
relevant	O
”	O
information	O
about	O
the	O
document	O
(	O
as	O
reﬂected	O
by	O
the	O
words	O
that	O
appear	O
in	O
the	O
document	O
)	O
is	O
retained	O
.	O
this	O
generalizes	O
the	O
classical	O
notion	O
of	O
minimal	O
suﬃcient	O
statistics2	O
used	O
in	O
parametric	O
statistics	O
to	O
arbitrary	O
distributions	O
.	O
solving	O
the	O
optimization	O
problem	O
associated	O
with	O
the	O
information	B
bottleneck	I
principle	O
is	O
hard	O
in	O
the	O
general	O
case	O
.	O
some	O
of	O
the	O
proposed	O
methods	O
are	O
similar	O
to	O
the	O
em	O
principle	O
,	O
which	O
we	O
will	O
discuss	O
in	O
chapter	O
24	O
.	O
22.5	O
a	O
high	O
level	O
view	O
of	O
clustering	B
so	O
far	O
,	O
we	O
have	O
mainly	O
listed	O
various	O
useful	O
clustering	B
tools	O
.	O
however	O
,	O
some	O
fun-	O
damental	O
questions	O
remain	O
unaddressed	O
.	O
first	O
and	O
foremost	O
,	O
what	O
is	O
clustering	B
?	O
what	O
is	O
it	O
that	O
distinguishes	O
a	O
clustering	B
algorithm	O
from	O
any	O
arbitrary	O
function	B
that	O
takes	O
an	O
input	O
space	O
and	O
outputs	O
a	O
partition	O
of	O
that	O
space	O
?	O
are	O
there	O
any	O
basic	O
properties	O
of	O
clustering	B
that	O
are	O
independent	O
of	O
any	O
speciﬁc	O
algorithm	O
or	O
task	O
?	O
one	O
method	O
for	O
addressing	O
such	O
questions	O
is	O
via	O
an	O
axiomatic	O
approach	O
.	O
there	O
have	O
been	O
several	O
attempts	O
to	O
provide	O
an	O
axiomatic	O
deﬁnition	O
of	O
clustering	B
.	O
let	O
us	O
demonstrate	O
this	O
approach	O
by	O
presenting	O
the	O
attempt	O
made	O
by	O
kleinberg	O
(	O
2003	O
)	O
.	O
consider	O
a	O
clustering	B
function	O
,	O
f	O
,	O
that	O
takes	O
as	O
input	O
any	O
ﬁnite	O
domain	B
x	O
with	O
a	O
dissimilarity	O
function	B
d	O
over	O
its	O
pairs	O
and	O
returns	O
a	O
partition	O
of	O
x	O
.	O
consider	O
the	O
following	O
three	O
properties	O
of	O
such	O
a	O
function	B
:	O
scale	O
invariance	O
(	O
si	O
)	O
for	O
any	O
domain	B
set	O
x	O
,	O
dissimilarity	O
function	B
d	O
,	O
and	O
any	O
α	O
>	O
0	O
,	O
the	O
following	O
should	O
hold	O
:	O
f	O
(	O
x	O
,	O
d	O
)	O
=	O
f	O
(	O
x	O
,	O
αd	O
)	O
(	O
where	O
(	O
αd	O
)	O
(	O
x	O
,	O
y	O
)	O
def=	O
α	O
d	O
(	O
x	O
,	O
y	O
)	O
)	O
.	O
richness	O
(	O
ri	O
)	O
for	O
any	O
ﬁnite	O
x	O
and	O
every	O
partition	O
c	O
=	O
(	O
c1	O
,	O
.	O
.	O
.	O
ck	O
)	O
of	O
x	O
(	O
into	O
nonempty	O
subsets	O
)	O
there	O
exists	O
some	O
dissimilarity	O
function	B
d	O
over	O
x	O
such	O
that	O
f	O
(	O
x	O
,	O
d	O
)	O
=	O
c.	O
i	O
(	O
x	O
;	O
c	O
)	O
=	O
(	O
cid:80	O
)	O
,	O
where	O
the	O
sum	O
is	O
over	O
all	O
values	O
x	O
can	O
take	O
and	O
all	O
(	O
cid:16	O
)	O
p	O
(	O
a	O
,	O
b	O
)	O
(	O
cid:80	O
)	O
(	O
cid:17	O
)	O
a	O
b	O
p	O
(	O
a	O
,	O
b	O
)	O
log	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
values	O
c	O
can	O
take	O
.	O
2	O
a	O
suﬃcient	O
statistic	O
is	O
a	O
function	B
of	O
the	O
data	O
which	O
has	O
the	O
property	O
of	O
suﬃciency	O
with	O
respect	O
to	O
a	O
statistical	O
model	O
and	O
its	O
associated	O
unknown	O
parameter	O
,	O
meaning	O
that	O
“	O
no	O
other	O
statistic	O
which	O
can	O
be	O
calculated	O
from	O
the	O
same	O
sample	O
provides	O
any	O
additional	O
information	O
as	O
to	O
the	O
value	O
of	O
the	O
parameter.	O
”	O
for	O
example	O
,	O
if	O
we	O
assume	O
that	O
a	O
variable	O
is	O
distributed	O
normally	O
with	O
a	O
unit	O
variance	O
and	O
an	O
unknown	O
expectation	O
,	O
then	O
the	O
average	O
function	B
is	O
a	O
suﬃcient	O
statistic	O
.	O
22.5	O
a	O
high	O
level	O
view	O
of	O
clustering	B
319	O
consistency	B
(	O
co	O
)	O
if	O
d	O
and	O
d	O
(	O
cid:48	O
)	O
are	O
dissimilarity	O
functions	O
over	O
x	O
,	O
such	O
that	O
for	O
every	O
x	O
,	O
y	O
∈	O
x	O
,	O
if	O
x	O
,	O
y	O
belong	O
to	O
the	O
same	O
cluster	O
in	O
f	O
(	O
x	O
,	O
d	O
)	O
then	O
d	O
(	O
cid:48	O
)	O
(	O
x	O
,	O
y	O
)	O
≤	O
d	O
(	O
x	O
,	O
y	O
)	O
and	O
if	O
x	O
,	O
y	O
belong	O
to	O
diﬀerent	O
clusters	O
in	O
f	O
(	O
x	O
,	O
d	O
)	O
then	O
d	O
(	O
cid:48	O
)	O
(	O
x	O
,	O
y	O
)	O
≥	O
d	O
(	O
x	O
,	O
y	O
)	O
,	O
then	O
f	O
(	O
x	O
,	O
d	O
)	O
=	O
f	O
(	O
x	O
,	O
d	O
(	O
cid:48	O
)	O
)	O
.	O
a	O
moment	O
of	O
reﬂection	O
reveals	O
that	O
the	O
scale	O
invariance	O
is	O
a	O
very	O
natural	O
requirement	O
–	O
it	O
would	O
be	O
odd	O
to	O
have	O
the	O
result	O
of	O
a	O
clustering	B
function	O
depend	O
on	O
the	O
units	O
used	O
to	O
measure	O
between-point	O
distances	O
.	O
the	O
richness	O
requirement	O
basically	O
states	O
that	O
the	O
outcome	O
of	O
the	O
clustering	B
function	O
is	O
fully	O
controlled	O
by	O
the	O
function	B
d	O
,	O
which	O
is	O
also	O
a	O
very	O
intuitive	O
feature	B
.	O
the	O
third	O
requirement	O
,	O
consistency	B
,	O
is	O
the	O
only	O
requirement	O
that	O
refers	O
to	O
the	O
basic	O
(	O
informal	O
)	O
deﬁnition	O
of	O
clustering	B
–	O
we	O
wish	O
that	O
similar	O
points	O
will	O
be	O
clustered	O
together	O
and	O
that	O
dissimilar	O
points	O
will	O
be	O
separated	O
to	O
diﬀerent	O
clusters	O
,	O
and	O
therefore	O
,	O
if	O
points	O
that	O
already	O
share	O
a	O
cluster	O
become	O
more	O
similar	O
,	O
and	O
points	O
that	O
are	O
already	O
separated	O
become	O
even	O
less	O
similar	O
to	O
each	O
other	O
,	O
the	O
clustering	B
function	O
should	O
have	O
even	O
stronger	O
“	O
support	O
”	O
of	O
its	O
previous	O
clustering	B
decisions	O
.	O
however	O
,	O
kleinberg	O
(	O
2003	O
)	O
has	O
shown	O
the	O
following	O
“	O
impossibility	O
”	O
result	O
:	O
theorem	O
22.4	O
there	O
exists	O
no	O
function	B
,	O
f	O
,	O
that	O
satisﬁes	O
all	O
the	O
three	O
proper-	O
ties	O
:	O
scale	O
invariance	O
,	O
richness	O
,	O
and	O
consistency	B
.	O
proof	O
assume	O
,	O
by	O
way	O
of	O
contradiction	O
,	O
that	O
some	O
f	O
does	O
satisfy	O
all	O
three	O
properties	O
.	O
pick	O
some	O
domain	B
set	O
x	O
with	O
at	O
least	O
three	O
points	O
.	O
by	O
richness	O
,	O
there	O
must	O
be	O
some	O
d1	O
such	O
that	O
f	O
(	O
x	O
,	O
d1	O
)	O
=	O
{	O
{	O
x	O
}	O
:	O
x	O
∈	O
x	O
}	O
and	O
there	O
also	O
exists	O
some	O
d2	O
such	O
that	O
f	O
(	O
x	O
,	O
d2	O
)	O
(	O
cid:54	O
)	O
=	O
f	O
(	O
x	O
,	O
d1	O
)	O
.	O
let	O
α	O
∈	O
r+	O
be	O
such	O
that	O
for	O
every	O
x	O
,	O
y	O
∈	O
x	O
,	O
αd2	O
(	O
x	O
,	O
y	O
)	O
≥	O
d1	O
(	O
x	O
,	O
y	O
)	O
.	O
let	O
d3	O
=	O
αd2	O
.	O
consider	O
f	O
(	O
x	O
,	O
d3	O
)	O
.	O
by	O
the	O
scale	O
invariance	O
property	O
of	O
f	O
,	O
we	O
should	O
have	O
f	O
(	O
x	O
,	O
d3	O
)	O
=	O
f	O
(	O
x	O
,	O
d2	O
)	O
.	O
on	O
the	O
other	O
hand	O
,	O
since	O
all	O
distinct	O
x	O
,	O
y	O
∈	O
x	O
reside	O
in	O
diﬀerent	O
clusters	O
w.r.t	O
.	O
f	O
(	O
x	O
,	O
d1	O
)	O
,	O
and	O
d3	O
(	O
x	O
,	O
y	O
)	O
≥	O
d1	O
(	O
x	O
,	O
y	O
)	O
,	O
the	O
consistency	B
of	O
f	O
implies	O
that	O
f	O
(	O
x	O
,	O
d3	O
)	O
=	O
f	O
(	O
x	O
,	O
d1	O
)	O
.	O
this	O
is	O
a	O
contradiction	O
,	O
since	O
we	O
chose	O
d1	O
,	O
d2	O
so	O
that	O
f	O
(	O
x	O
,	O
d2	O
)	O
(	O
cid:54	O
)	O
=	O
f	O
(	O
x	O
,	O
d1	O
)	O
.	O
it	O
is	O
important	O
to	O
note	O
that	O
there	O
is	O
no	O
single	O
“	O
bad	O
property	O
”	O
among	O
the	O
three	O
properties	O
.	O
for	O
every	O
pair	O
of	O
the	O
the	O
three	O
axioms	O
,	O
there	O
exist	O
natural	O
clustering	B
functions	O
that	O
satisfy	O
the	O
two	O
properties	O
in	O
that	O
pair	O
(	O
one	O
can	O
even	O
construct	O
such	O
examples	O
just	O
by	O
varying	O
the	O
stopping	O
criteria	O
for	O
the	O
single	B
linkage	I
clustering	O
function	B
)	O
.	O
on	O
the	O
other	O
hand	O
,	O
kleinberg	O
shows	O
that	O
any	O
clustering	B
algorithm	O
that	O
minimizes	O
any	O
center-based	O
objective	O
function	B
inevitably	O
fails	O
the	O
consis-	O
tency	O
property	O
(	O
yet	O
,	O
the	O
k-sum-of-in-cluster-distances	O
minimization	O
clustering	B
does	O
satisfy	O
consistency	B
)	O
.	O
the	O
kleinberg	O
impossibility	O
result	O
can	O
be	O
easily	O
circumvented	O
by	O
varying	O
the	O
properties	O
.	O
for	O
example	O
,	O
if	O
one	O
wishes	O
to	O
discuss	O
clustering	B
functions	O
that	O
have	O
a	O
ﬁxed	O
number-of-clusters	O
parameter	O
,	O
then	O
it	O
is	O
natural	O
to	O
replace	O
richness	O
by	O
k-richness	O
(	O
namely	O
,	O
the	O
requirement	O
that	O
every	O
partition	O
of	O
the	O
domain	B
into	O
k	O
subsets	O
is	O
attainable	O
by	O
the	O
clustering	B
function	O
)	O
.	O
k-richness	O
,	O
scale	O
invariance	O
and	O
consistency	B
all	O
hold	O
for	O
the	O
k-means	B
clustering	O
and	O
are	O
therefore	O
consistent	O
.	O
320	O
clustering	B
1	O
,	O
.	O
.	O
.	O
c	O
(	O
cid:48	O
)	O
j	O
or	O
c	O
(	O
cid:48	O
)	O
alternatively	O
,	O
one	O
can	O
relax	O
the	O
consistency	B
property	O
.	O
for	O
example	O
,	O
say	O
that	O
two	O
clusterings	O
c	O
=	O
(	O
c1	O
,	O
.	O
.	O
.	O
ck	O
)	O
and	O
c	O
(	O
cid:48	O
)	O
=	O
(	O
c	O
(	O
cid:48	O
)	O
l	O
)	O
are	O
compatible	O
if	O
for	O
every	O
clusters	O
ci	O
∈	O
c	O
and	O
c	O
(	O
cid:48	O
)	O
j	O
∈	O
c	O
(	O
cid:48	O
)	O
,	O
either	O
ci	O
⊆	O
c	O
(	O
cid:48	O
)	O
j	O
⊆	O
ci	O
or	O
ci	O
∩	O
c	O
(	O
cid:48	O
)	O
j	O
=	O
∅	O
(	O
it	O
is	O
worthwhile	O
noting	O
that	O
for	O
every	O
dendrogram	B
,	O
every	O
two	O
clusterings	O
that	O
are	O
ob-	O
tained	O
by	O
trimming	O
that	O
dendrogram	B
are	O
compatible	O
)	O
.	O
“	O
reﬁnement	O
consistency	B
”	O
is	O
the	O
requirement	O
that	O
,	O
under	O
the	O
assumptions	O
of	O
the	O
consistency	B
property	O
,	O
the	O
new	O
clustering	B
f	O
(	O
x	O
,	O
d	O
(	O
cid:48	O
)	O
)	O
is	O
compatible	O
with	O
the	O
old	O
clustering	B
f	O
(	O
x	O
,	O
d	O
)	O
.	O
many	O
common	O
clustering	B
functions	O
satisfy	O
this	O
requirement	O
as	O
well	O
as	O
scale	O
invariance	O
and	O
richness	O
.	O
furthermore	O
,	O
one	O
can	O
come	O
up	O
with	O
many	O
other	O
,	O
diﬀerent	O
,	O
prop-	O
erties	O
of	O
clustering	B
functions	O
that	O
sound	O
intuitive	O
and	O
desirable	O
and	O
are	O
satisﬁed	O
by	O
some	O
common	O
clustering	B
functions	O
.	O
there	O
are	O
many	O
ways	O
to	O
interpret	O
these	O
results	O
.	O
we	O
suggest	O
to	O
view	O
it	O
as	O
indi-	O
cating	O
that	O
there	O
is	O
no	O
“	O
ideal	O
”	O
clustering	B
function	O
.	O
every	O
clustering	B
function	O
will	O
inevitably	O
have	O
some	O
“	O
undesirable	O
”	O
properties	O
.	O
the	O
choice	O
of	O
a	O
clustering	B
func-	O
tion	O
for	O
any	O
given	O
task	O
must	O
therefore	O
take	O
into	O
account	O
the	O
speciﬁc	O
properties	O
of	O
that	O
task	O
.	O
there	O
is	O
no	O
generic	O
clustering	B
solution	O
,	O
just	O
as	O
there	O
is	O
no	O
clas-	O
siﬁcation	O
algorithm	O
that	O
will	O
learn	O
every	O
learnable	O
task	O
(	O
as	O
the	O
no-free-lunch	B
theorem	O
shows	O
)	O
.	O
clustering	B
,	O
just	O
like	O
classiﬁcation	O
prediction	O
,	O
must	O
take	O
into	O
account	O
some	O
prior	B
knowledge	I
about	O
the	O
speciﬁc	O
task	O
at	O
hand	O
.	O
22.6	O
summary	O
clustering	B
is	O
an	O
unsupervised	B
learning	I
problem	O
,	O
in	O
which	O
we	O
wish	O
to	O
partition	O
a	O
set	B
of	O
points	O
into	O
“	O
meaningful	O
”	O
subsets	O
.	O
we	O
presented	O
several	O
clustering	B
ap-	O
proaches	O
including	O
linkage-based	O
algorithms	O
,	O
the	O
k-means	B
family	O
,	O
spectral	B
clus-	O
tering	O
,	O
and	O
the	O
information	B
bottleneck	I
.	O
we	O
discussed	O
the	O
diﬃculty	O
of	O
formalizing	O
the	O
intuitive	O
meaning	O
of	O
clustering	B
.	O
22.7	O
bibliographic	O
remarks	O
the	O
k-means	B
algorithm	O
is	O
sometimes	O
named	O
lloyd	O
’	O
s	O
algorithm	O
,	O
after	O
stuart	O
lloyd	O
,	O
who	O
proposed	O
the	O
method	O
in	O
1957.	O
for	O
a	O
more	O
complete	O
overview	O
of	O
spectral	B
clustering	I
we	O
refer	O
the	O
reader	O
to	O
the	O
excellent	O
tutorial	O
by	O
von	O
luxburg	O
(	O
2007	O
)	O
.	O
the	O
information	B
bottleneck	I
method	O
was	O
introduced	O
by	O
tishby	O
,	O
pereira	O
&	O
bialek	O
(	O
1999	O
)	O
.	O
for	O
an	O
additional	O
discussion	O
on	O
the	O
axiomatic	O
approach	O
see	O
ackerman	O
&	O
ben-david	O
(	O
2008	O
)	O
.	O
22.8	O
exercises	O
1.	O
suboptimality	O
of	O
k-means	B
:	O
for	O
every	O
parameter	O
t	O
>	O
1	O
,	O
show	O
that	O
there	O
exists	O
an	O
instance	B
of	O
the	O
k-means	B
problem	O
for	O
which	O
the	O
k-means	B
algorithm	O
22.8	O
exercises	O
321	O
(	O
might	O
)	O
ﬁnd	O
a	O
solution	O
whose	O
k-means	B
objective	O
is	O
at	O
least	O
t	O
·	O
opt	O
,	O
where	O
opt	O
is	O
the	O
minimum	O
k-means	B
objective	O
.	O
2.	O
k-means	B
might	O
not	O
necessarily	O
converge	O
to	O
a	O
local	B
minimum	I
:	O
show	O
that	O
the	O
k-means	B
algorithm	O
might	O
converge	O
to	O
a	O
point	O
which	O
is	O
not	O
a	O
local	B
minimum	I
.	O
hint	O
:	O
suppose	O
that	O
k	O
=	O
2	O
and	O
the	O
sample	O
points	O
are	O
{	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
}	O
⊂	O
r	O
suppose	O
we	O
initialize	O
the	O
k-means	B
with	O
the	O
centers	O
{	O
2	O
,	O
4	O
}	O
;	O
and	O
suppose	O
we	O
break	O
ties	O
in	O
the	O
deﬁnition	O
of	O
ci	O
by	O
assigning	O
i	O
to	O
be	O
the	O
smallest	O
value	O
in	O
argminj	O
(	O
cid:107	O
)	O
x	O
−	O
µj	O
(	O
cid:107	O
)	O
.	O
3.	O
given	O
a	O
metric	O
space	O
(	O
x	O
,	O
d	O
)	O
,	O
where	O
|x|	O
<	O
∞	O
,	O
and	O
k	O
∈	O
n	O
,	O
we	O
would	O
like	O
to	O
ﬁnd	O
a	O
partition	O
of	O
x	O
into	O
c1	O
,	O
.	O
.	O
.	O
,	O
ck	O
which	O
minimizes	O
the	O
expression	O
gk−diam	O
(	O
(	O
x	O
,	O
d	O
)	O
,	O
(	O
c1	O
,	O
.	O
.	O
.	O
,	O
ck	O
)	O
)	O
=	O
max	O
j∈	O
[	O
d	O
]	O
diam	O
(	O
cj	O
)	O
,	O
where	O
diam	O
(	O
cj	O
)	O
=	O
maxx	O
,	O
x	O
(	O
cid:48	O
)	O
∈cj	O
d	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
we	O
use	O
the	O
convention	O
diam	O
(	O
cj	O
)	O
=	O
0	O
if	O
|cj|	O
<	O
2	O
)	O
.	O
similarly	O
to	O
the	O
k-means	B
objective	O
,	O
it	O
is	O
np-hard	O
to	O
minimize	O
the	O
k-	O
diam	O
objective	O
.	O
fortunately	O
,	O
we	O
have	O
a	O
very	O
simple	O
approximation	O
algorithm	O
:	O
initially	O
,	O
we	O
pick	O
some	O
x	O
∈	O
x	O
and	O
set	B
µ1	O
=	O
x.	O
then	O
,	O
the	O
algorithm	O
iteratively	O
sets	O
∀j	O
∈	O
{	O
2	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
,	O
µj	O
=	O
argmax	O
x∈x	O
min	O
i∈	O
[	O
j−1	O
]	O
d	O
(	O
x	O
,	O
µi	O
)	O
.	O
finally	O
,	O
we	O
set	B
∀i	O
∈	O
[	O
k	O
]	O
,	O
ci	O
=	O
{	O
x	O
∈	O
x	O
:	O
i	O
=	O
argmin	O
j∈	O
[	O
k	O
]	O
d	O
(	O
x	O
,	O
µj	O
)	O
}	O
.	O
prove	O
that	O
the	O
algorithm	O
described	O
is	O
a	O
2-approximation	O
algorithm	O
.	O
that	O
is	O
,	O
if	O
we	O
denote	O
its	O
output	O
by	O
ˆc1	O
,	O
.	O
.	O
.	O
,	O
ˆck	O
,	O
and	O
denote	O
the	O
optimal	O
solution	O
by	O
c∗	O
1	O
,	O
.	O
.	O
.	O
,	O
c∗	O
k	O
,	O
then	O
,	O
gk−diam	O
(	O
(	O
x	O
,	O
d	O
)	O
,	O
(	O
ˆc1	O
,	O
.	O
.	O
.	O
,	O
ˆck	O
)	O
)	O
≤	O
2	O
·	O
gk−diam	O
(	O
(	O
x	O
,	O
d	O
)	O
,	O
(	O
c∗	O
1	O
,	O
.	O
.	O
.	O
,	O
c∗	O
k	O
)	O
)	O
.	O
hint	O
:	O
consider	O
the	O
point	O
µk+1	O
(	O
in	O
other	O
words	O
,	O
the	O
next	O
center	O
we	O
would	O
have	O
chosen	O
,	O
if	O
we	O
wanted	O
k	O
+	O
1	O
clusters	O
)	O
.	O
let	O
r	O
=	O
minj∈	O
[	O
k	O
]	O
d	O
(	O
µj	O
,	O
µk+1	O
)	O
.	O
prove	O
the	O
following	O
inequalities	O
gk−diam	O
(	O
(	O
x	O
,	O
d	O
)	O
,	O
(	O
ˆc1	O
,	O
.	O
.	O
.	O
,	O
ˆck	O
)	O
)	O
≤	O
2r	O
k	O
)	O
)	O
≥	O
r.	O
gk−diam	O
(	O
(	O
x	O
,	O
d	O
)	O
,	O
(	O
c∗	O
1	O
,	O
.	O
.	O
.	O
,	O
c∗	O
4.	O
recall	B
that	O
a	O
clustering	B
function	O
,	O
f	O
,	O
is	O
called	O
center-based	O
clustering	B
if	O
,	O
for	O
some	O
monotonic	O
function	B
f	O
:	O
r+	O
→	O
r+	O
,	O
on	O
every	O
given	O
input	O
(	O
x	O
,	O
d	O
)	O
,	O
f	O
(	O
x	O
,	O
d	O
)	O
is	O
a	O
clustering	B
that	O
minimizes	O
the	O
objective	O
gf	O
(	O
(	O
x	O
,	O
d	O
)	O
,	O
(	O
c1	O
,	O
.	O
.	O
.	O
ck	O
)	O
)	O
=	O
min	O
µ1	O
,	O
...	O
µk∈x	O
(	O
cid:48	O
)	O
where	O
x	O
(	O
cid:48	O
)	O
is	O
either	O
x	O
or	O
some	O
superset	O
of	O
x	O
.	O
k	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
f	O
(	O
d	O
(	O
x	O
,	O
µi	O
)	O
)	O
,	O
i=1	O
x∈ci	O
322	O
clustering	B
prove	O
that	O
for	O
every	O
k	O
>	O
1	O
the	O
k-diam	O
clustering	B
function	O
deﬁned	O
in	O
the	O
previous	O
exercise	O
is	O
not	O
a	O
center-based	O
clustering	B
function	O
.	O
hint	O
:	O
given	O
a	O
clustering	B
input	O
(	O
x	O
,	O
d	O
)	O
,	O
with	O
|x|	O
>	O
2	O
,	O
consider	O
the	O
eﬀect	O
of	O
adding	O
many	O
close-by	O
points	O
to	O
some	O
(	O
but	O
not	O
all	O
)	O
of	O
the	O
members	O
of	O
x	O
,	O
on	O
either	O
the	O
k-diam	O
clustering	B
or	O
any	O
given	O
center-based	O
clustering	B
.	O
5.	O
recall	B
that	O
we	O
discussed	O
three	O
clustering	B
“	O
properties	O
”	O
:	O
scale	O
invariance	O
,	O
rich-	O
ness	O
,	O
and	O
consistency	B
.	O
consider	O
the	O
single	B
linkage	I
clustering	O
algorithm	O
.	O
1.	O
find	O
which	O
of	O
the	O
three	O
properties	O
is	O
satisﬁed	O
by	O
single	B
linkage	I
with	O
the	O
fixed	O
number	O
of	O
clusters	O
(	O
any	O
ﬁxed	O
nonzero	O
number	O
)	O
stopping	O
rule	O
.	O
2.	O
find	O
which	O
of	O
the	O
three	O
properties	O
is	O
satisﬁed	O
by	O
single	B
linkage	I
with	O
the	O
distance	O
upper	O
bound	O
(	O
any	O
ﬁxed	O
nonzero	O
upper	O
bound	O
)	O
stopping	O
rule	O
.	O
3.	O
show	O
that	O
for	O
any	O
pair	O
of	O
these	O
properties	O
there	O
exists	O
a	O
stopping	O
criterion	O
for	O
single	B
linkage	I
clustering	O
,	O
under	O
which	O
these	O
two	O
axioms	O
are	O
satisﬁed	O
.	O
6.	O
given	O
some	O
number	O
k	O
,	O
let	O
k-richness	O
be	O
the	O
following	O
requirement	O
:	O
for	O
any	O
ﬁnite	O
x	O
and	O
every	O
partition	O
c	O
=	O
(	O
c1	O
,	O
.	O
.	O
.	O
ck	O
)	O
of	O
x	O
(	O
into	O
nonempty	O
subsets	O
)	O
there	O
exists	O
some	O
dissimilarity	O
function	B
d	O
over	O
x	O
such	O
that	O
f	O
(	O
x	O
,	O
d	O
)	O
=	O
c.	O
prove	O
that	O
,	O
for	O
every	O
number	O
k	O
,	O
there	O
exists	O
a	O
clustering	B
function	O
that	O
satisﬁes	O
the	O
three	O
properties	O
:	O
scale	O
invariance	O
,	O
k-richness	O
,	O
and	O
consistency	B
.	O
23	O
dimensionality	B
reduction	I
dimensionality	O
reduction	O
is	O
the	O
process	O
of	O
taking	O
data	O
in	O
a	O
high	O
dimensional	O
space	O
and	O
mapping	O
it	O
into	O
a	O
new	O
space	O
whose	O
dimensionality	O
is	O
much	O
smaller	O
.	O
this	O
process	O
is	O
closely	O
related	O
to	O
the	O
concept	O
of	O
(	O
lossy	O
)	O
compression	O
in	O
infor-	O
mation	O
theory	O
.	O
there	O
are	O
several	O
reasons	O
to	O
reduce	O
the	O
dimensionality	O
of	O
the	O
data	O
.	O
first	O
,	O
high	O
dimensional	O
data	O
impose	O
computational	O
challenges	O
.	O
moreover	O
,	O
in	O
some	O
situations	O
high	O
dimensionality	O
might	O
lead	O
to	O
poor	O
generalization	O
abili-	O
ties	O
of	O
the	O
learning	O
algorithm	O
(	O
for	O
example	O
,	O
in	O
nearest	O
neighbor	O
classiﬁers	O
the	O
sample	B
complexity	I
increases	O
exponentially	O
with	O
the	O
dimension—see	O
chapter	O
19	O
)	O
.	O
finally	O
,	O
dimensionality	B
reduction	I
can	O
be	O
used	O
for	O
interpretability	O
of	O
the	O
data	O
,	O
for	O
ﬁnding	O
meaningful	O
structure	O
of	O
the	O
data	O
,	O
and	O
for	O
illustration	O
purposes	O
.	O
in	O
this	O
chapter	O
we	O
describe	O
popular	O
methods	O
for	O
dimensionality	B
reduction	I
.	O
in	O
those	O
methods	O
,	O
the	O
reduction	O
is	O
performed	O
by	O
applying	O
a	O
linear	O
transformation	O
to	O
the	O
original	O
data	O
.	O
that	O
is	O
,	O
if	O
the	O
original	O
data	O
is	O
in	O
rd	O
and	O
we	O
want	O
to	O
embed	O
it	O
into	O
rn	O
(	O
n	O
<	O
d	O
)	O
then	O
we	O
would	O
like	O
to	O
ﬁnd	O
a	O
matrix	O
w	O
∈	O
rn	O
,	O
d	O
that	O
induces	O
the	O
mapping	O
x	O
(	O
cid:55	O
)	O
→	O
w	O
x.	O
a	O
natural	O
criterion	O
for	O
choosing	O
w	O
is	O
in	O
a	O
way	O
that	O
will	O
enable	O
a	O
reasonable	O
recovery	O
of	O
the	O
original	O
x.	O
it	O
is	O
not	O
hard	O
to	O
show	O
that	O
in	O
general	O
,	O
exact	O
recovery	O
of	O
x	O
from	O
w	O
x	O
is	O
impossible	O
(	O
see	O
exercise	O
1	O
)	O
.	O
the	O
ﬁrst	O
method	O
we	O
describe	O
is	O
called	O
principal	O
component	O
analysis	O
(	O
pca	O
)	O
.	O
in	O
pca	O
,	O
both	O
the	O
compression	O
and	O
the	O
recovery	O
are	O
performed	O
by	O
linear	O
transfor-	O
mations	O
and	O
the	O
method	O
ﬁnds	O
the	O
linear	O
transformations	O
for	O
which	O
the	O
diﬀerences	O
between	O
the	O
recovered	O
vectors	O
and	O
the	O
original	O
vectors	O
are	O
minimal	O
in	O
the	O
least	O
squared	O
sense	O
.	O
next	O
,	O
we	O
describe	O
dimensionality	B
reduction	I
using	O
random	O
matrices	O
w	O
.	O
we	O
derive	O
an	O
important	O
lemma	O
,	O
often	O
called	O
the	O
“	O
johnson-lindenstrauss	O
lemma	O
,	O
”	O
which	O
analyzes	O
the	O
distortion	O
caused	O
by	O
such	O
a	O
random	O
dimensionality	O
reduction	O
technique	O
.	O
last	O
,	O
we	O
show	O
how	O
one	O
can	O
reduce	O
the	O
dimension	B
of	O
all	O
sparse	O
vectors	O
using	O
again	O
a	O
random	O
matrix	O
.	O
this	O
process	O
is	O
known	O
as	O
compressed	B
sensing	I
.	O
in	O
this	O
case	O
,	O
the	O
recovery	O
process	O
is	O
nonlinear	O
but	O
can	O
still	O
be	O
implemented	O
eﬃciently	O
using	O
linear	B
programming	I
.	O
we	O
conclude	O
by	O
underscoring	O
the	O
underlying	O
“	O
prior	O
assumptions	O
”	O
behind	O
pca	O
and	O
compressed	B
sensing	I
,	O
which	O
can	O
help	O
us	O
understand	O
the	O
merits	O
and	O
pitfalls	O
of	O
the	O
two	O
methods	O
.	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
324	O
dimensionality	B
reduction	I
23.1	O
principal	O
component	O
analysis	O
(	O
pca	O
)	O
let	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
be	O
m	O
vectors	O
in	O
rd	O
.	O
we	O
would	O
like	O
to	O
reduce	O
the	O
dimensional-	O
ity	O
of	O
these	O
vectors	O
using	O
a	O
linear	O
transformation	O
.	O
a	O
matrix	O
w	O
∈	O
rn	O
,	O
d	O
,	O
where	O
n	O
<	O
d	O
,	O
induces	O
a	O
mapping	O
x	O
(	O
cid:55	O
)	O
→	O
w	O
x	O
,	O
where	O
w	O
x	O
∈	O
rn	O
is	O
the	O
lower	O
dimensionality	O
representation	O
of	O
x.	O
then	O
,	O
a	O
second	O
matrix	O
u	O
∈	O
rd	O
,	O
n	O
can	O
be	O
used	O
to	O
(	O
approxi-	O
mately	O
)	O
recover	O
each	O
original	O
vector	O
x	O
from	O
its	O
compressed	O
version	O
.	O
that	O
is	O
,	O
for	O
a	O
compressed	O
vector	O
y	O
=	O
w	O
x	O
,	O
where	O
y	O
is	O
in	O
the	O
low	O
dimensional	O
space	O
rn	O
,	O
we	O
can	O
construct	O
˜x	O
=	O
u	O
y	O
,	O
so	O
that	O
˜x	O
is	O
the	O
recovered	O
version	O
of	O
x	O
and	O
resides	O
in	O
the	O
original	O
high	O
dimensional	O
space	O
rd	O
.	O
in	O
pca	O
,	O
we	O
ﬁnd	O
the	O
compression	O
matrix	O
w	O
and	O
the	O
recovering	O
matrix	O
u	O
so	O
that	O
the	O
total	O
squared	O
distance	O
between	O
the	O
original	O
and	O
recovered	O
vectors	O
is	O
minimal	O
;	O
namely	O
,	O
we	O
aim	O
at	O
solving	O
the	O
problem	O
m	O
(	O
cid:88	O
)	O
i=1	O
argmin	O
w∈rn	O
,	O
d	O
,	O
u∈rd	O
,	O
n	O
(	O
cid:107	O
)	O
xi	O
−	O
u	O
w	O
xi	O
(	O
cid:107	O
)	O
2	O
2	O
.	O
(	O
23.1	O
)	O
to	O
solve	O
this	O
problem	O
we	O
ﬁrst	O
show	O
that	O
the	O
optimal	O
solution	O
takes	O
a	O
speciﬁc	O
form	O
.	O
lemma	O
23.1	O
let	O
(	O
u	O
,	O
w	O
)	O
be	O
a	O
solution	O
to	O
equation	O
(	O
23.1	O
)	O
.	O
then	O
the	O
columns	O
of	O
u	O
are	O
orthonormal	O
(	O
namely	O
,	O
u	O
(	O
cid:62	O
)	O
u	O
is	O
the	O
identity	O
matrix	O
of	O
rn	O
)	O
and	O
w	O
=	O
u	O
(	O
cid:62	O
)	O
.	O
proof	O
fix	O
any	O
u	O
,	O
w	O
and	O
consider	O
the	O
mapping	O
x	O
(	O
cid:55	O
)	O
→	O
u	O
w	O
x.	O
the	O
range	O
of	O
this	O
mapping	O
,	O
r	O
=	O
{	O
u	O
w	O
x	O
:	O
x	O
∈	O
rd	O
}	O
,	O
is	O
an	O
n	O
dimensional	O
linear	O
subspace	O
of	O
rd	O
.	O
let	O
v	O
∈	O
rd	O
,	O
n	O
be	O
a	O
matrix	O
whose	O
columns	O
form	O
an	O
orthonormal	O
basis	O
of	O
this	O
subspace	O
,	O
namely	O
,	O
the	O
range	O
of	O
v	O
is	O
r	O
and	O
v	O
(	O
cid:62	O
)	O
v	O
=	O
i.	O
therefore	O
,	O
each	O
vector	O
in	O
r	O
can	O
be	O
written	O
as	O
v	O
y	O
where	O
y	O
∈	O
rn	O
.	O
for	O
every	O
x	O
∈	O
rd	O
and	O
y	O
∈	O
rn	O
we	O
have	O
(	O
cid:107	O
)	O
x	O
−	O
v	O
y	O
(	O
cid:107	O
)	O
2	O
2	O
=	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
+	O
y	O
(	O
cid:62	O
)	O
v	O
(	O
cid:62	O
)	O
v	O
y	O
−	O
2y	O
(	O
cid:62	O
)	O
v	O
(	O
cid:62	O
)	O
x	O
=	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
+	O
(	O
cid:107	O
)	O
y	O
(	O
cid:107	O
)	O
2	O
−	O
2y	O
(	O
cid:62	O
)	O
(	O
v	O
(	O
cid:62	O
)	O
x	O
)	O
,	O
where	O
we	O
used	O
the	O
fact	O
that	O
v	O
(	O
cid:62	O
)	O
v	O
is	O
the	O
identity	O
matrix	O
of	O
rn	O
.	O
minimizing	O
the	O
preceding	O
expression	O
with	O
respect	O
to	O
y	O
by	O
comparing	O
the	O
gradient	B
with	O
respect	O
to	O
y	O
to	O
zero	O
gives	O
that	O
y	O
=	O
v	O
(	O
cid:62	O
)	O
x.	O
therefore	O
,	O
for	O
each	O
x	O
we	O
have	O
that	O
v	O
v	O
(	O
cid:62	O
)	O
x	O
=	O
argmin	O
˜x∈r	O
(	O
cid:107	O
)	O
x	O
−	O
˜x	O
(	O
cid:107	O
)	O
2	O
2.	O
in	O
particular	O
this	O
holds	O
for	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
and	O
therefore	O
we	O
can	O
replace	O
u	O
,	O
w	O
by	O
v	O
,	O
v	O
(	O
cid:62	O
)	O
and	O
by	O
that	O
do	O
not	O
increase	O
the	O
objective	O
m	O
(	O
cid:88	O
)	O
2	O
≥	O
m	O
(	O
cid:88	O
)	O
(	O
cid:107	O
)	O
xi	O
−	O
u	O
w	O
xi	O
(	O
cid:107	O
)	O
2	O
(	O
cid:107	O
)	O
xi	O
−	O
v	O
v	O
(	O
cid:62	O
)	O
xi	O
(	O
cid:107	O
)	O
2	O
2.	O
i=1	O
i=1	O
since	O
this	O
holds	O
for	O
every	O
u	O
,	O
w	O
the	O
proof	O
of	O
the	O
lemma	O
follows	O
.	O
on	O
the	O
basis	O
of	O
the	O
preceding	O
lemma	O
,	O
we	O
can	O
rewrite	O
the	O
optimization	O
problem	O
given	O
in	O
equation	O
(	O
23.1	O
)	O
as	O
follows	O
:	O
argmin	O
u∈rd	O
,	O
n	O
:	O
u	O
(	O
cid:62	O
)	O
u	O
=i	O
m	O
(	O
cid:88	O
)	O
i=1	O
(	O
cid:107	O
)	O
xi	O
−	O
u	O
u	O
(	O
cid:62	O
)	O
xi	O
(	O
cid:107	O
)	O
2	O
2	O
.	O
(	O
23.2	O
)	O
23.1	O
principal	O
component	O
analysis	O
(	O
pca	O
)	O
325	O
we	O
further	O
simplify	O
the	O
optimization	O
problem	O
by	O
using	O
the	O
following	O
elementary	O
algebraic	O
manipulations	O
.	O
for	O
every	O
x	O
∈	O
rd	O
and	O
a	O
matrix	O
u	O
∈	O
rd	O
,	O
n	O
such	O
that	O
u	O
(	O
cid:62	O
)	O
u	O
=	O
i	O
we	O
have	O
(	O
cid:107	O
)	O
x	O
−	O
uu	O
(	O
cid:62	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
−	O
2x	O
(	O
cid:62	O
)	O
uu	O
(	O
cid:62	O
)	O
x	O
+	O
x	O
(	O
cid:62	O
)	O
u	O
u	O
(	O
cid:62	O
)	O
uu	O
(	O
cid:62	O
)	O
x	O
=	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
−	O
x	O
(	O
cid:62	O
)	O
uu	O
(	O
cid:62	O
)	O
x	O
=	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
−	O
trace	O
(	O
u	O
(	O
cid:62	O
)	O
xx	O
(	O
cid:62	O
)	O
u	O
)	O
,	O
(	O
23.3	O
)	O
where	O
the	O
trace	O
of	O
a	O
matrix	O
is	O
the	O
sum	O
of	O
its	O
diagonal	O
entries	O
.	O
since	O
the	O
trace	O
is	O
a	O
linear	O
operator	O
,	O
this	O
allows	O
us	O
to	O
rewrite	O
equation	O
(	O
23.2	O
)	O
as	O
follows	O
:	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
u	O
(	O
cid:62	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
xix	O
(	O
cid:62	O
)	O
i	O
u	O
.	O
(	O
23.4	O
)	O
let	O
a	O
=	O
(	O
cid:80	O
)	O
m	O
i=1	O
xix	O
(	O
cid:62	O
)	O
argmax	O
u∈rd	O
,	O
n	O
:	O
u	O
(	O
cid:62	O
)	O
u	O
=i	O
trace	O
i	O
.	O
the	O
matrix	O
a	O
is	O
symmetric	O
and	O
therefore	O
it	O
can	O
be	O
written	O
using	O
its	O
spectral	B
decomposition	O
as	O
a	O
=	O
vdv	O
(	O
cid:62	O
)	O
,	O
where	O
d	O
is	O
diagonal	O
and	O
v	O
(	O
cid:62	O
)	O
v	O
=	O
vv	O
(	O
cid:62	O
)	O
=	O
i.	O
here	O
,	O
the	O
elements	O
on	O
the	O
diagonal	O
of	O
d	O
are	O
the	O
eigenvalues	O
of	O
a	O
and	O
the	O
columns	O
of	O
v	O
are	O
the	O
corresponding	O
eigenvectors	O
.	O
we	O
assume	O
without	O
loss	B
of	O
generality	O
that	O
d1,1	O
≥	O
d2,2	O
≥	O
···	O
≥	O
dd	O
,	O
d	O
.	O
since	O
a	O
is	O
positive	O
semideﬁnite	O
it	O
also	O
holds	O
that	O
dd	O
,	O
d	O
≥	O
0.	O
we	O
claim	O
that	O
the	O
solution	O
to	O
equation	O
(	O
23.4	O
)	O
is	O
the	O
matrix	O
u	O
whose	O
columns	O
are	O
the	O
n	O
eigenvectors	O
of	O
a	O
corresponding	O
to	O
the	O
largest	O
n	O
eigenvalues	O
.	O
theorem	O
23.2	O
let	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
be	O
arbitrary	O
vectors	O
in	O
rd	O
,	O
let	O
a	O
=	O
(	O
cid:80	O
)	O
m	O
i=1	O
xix	O
(	O
cid:62	O
)	O
i	O
,	O
and	O
let	O
u1	O
,	O
.	O
.	O
.	O
,	O
un	O
be	O
n	O
eigenvectors	O
of	O
the	O
matrix	O
a	O
corresponding	O
to	O
the	O
largest	O
n	O
eigenvalues	O
of	O
a.	O
then	O
,	O
the	O
solution	O
to	O
the	O
pca	O
optimization	O
problem	O
given	O
in	O
equation	O
(	O
23.1	O
)	O
is	O
to	O
set	B
u	O
to	O
be	O
the	O
matrix	O
whose	O
columns	O
are	O
u1	O
,	O
.	O
.	O
.	O
,	O
un	O
and	O
to	O
set	B
w	O
=	O
u	O
(	O
cid:62	O
)	O
.	O
proof	O
let	O
vdv	O
(	O
cid:62	O
)	O
be	O
the	O
spectral	B
decomposition	O
of	O
a.	O
fix	O
some	O
matrix	O
u	O
∈	O
rd	O
,	O
n	O
with	O
orthonormal	O
columns	O
and	O
let	O
b	O
=	O
v	O
(	O
cid:62	O
)	O
u	O
.	O
then	O
,	O
vb	O
=	O
vv	O
(	O
cid:62	O
)	O
u	O
=	O
u	O
.	O
it	O
follows	O
that	O
u	O
(	O
cid:62	O
)	O
au	O
=	O
b	O
(	O
cid:62	O
)	O
v	O
(	O
cid:62	O
)	O
vdv	O
(	O
cid:62	O
)	O
vb	O
=	O
b	O
(	O
cid:62	O
)	O
db	O
,	O
n	O
(	O
cid:88	O
)	O
and	O
therefore	O
trace	O
(	O
u	O
(	O
cid:62	O
)	O
au	O
)	O
=	O
d	O
(	O
cid:88	O
)	O
(	O
cid:80	O
)	O
n	O
also	O
orthonormal	O
,	O
which	O
implies	O
that	O
(	O
cid:80	O
)	O
d	O
addition	O
˜b	O
(	O
cid:62	O
)	O
˜b	O
=	O
i.	O
then	O
,	O
for	O
every	O
j	O
we	O
have	O
(	O
cid:80	O
)	O
d	O
(	O
cid:80	O
)	O
n	O
note	O
that	O
b	O
(	O
cid:62	O
)	O
b	O
=	O
u	O
(	O
cid:62	O
)	O
vv	O
(	O
cid:62	O
)	O
u	O
=	O
u	O
(	O
cid:62	O
)	O
u	O
=	O
i.	O
therefore	O
,	O
the	O
columns	O
of	O
b	O
are	O
j	O
,	O
i	O
=	O
n.	O
in	O
addition	O
,	O
let	O
˜b	O
∈	O
rd	O
,	O
d	O
be	O
a	O
matrix	O
such	O
that	O
its	O
ﬁrst	O
n	O
columns	O
are	O
the	O
columns	O
of	O
b	O
and	O
in	O
j	O
,	O
i	O
=	O
1	O
,	O
which	O
implies	O
that	O
i=1	O
b2	O
i=1	O
b2	O
j	O
,	O
i	O
≤	O
1.	O
it	O
follows	O
that	O
:	O
˜b2	O
i=1	O
j=1	O
i=1	O
dj	O
,	O
j	O
b2	O
j	O
,	O
i	O
.	O
j=1	O
trace	O
(	O
u	O
(	O
cid:62	O
)	O
au	O
)	O
≤	O
max	O
β∈	O
[	O
0,1	O
]	O
d	O
:	O
(	O
cid:107	O
)	O
β	O
(	O
cid:107	O
)	O
1≤n	O
dj	O
,	O
jβj	O
.	O
d	O
(	O
cid:88	O
)	O
j=1	O
326	O
dimensionality	B
reduction	I
(	O
cid:80	O
)	O
n	O
thonormal	O
columns	O
it	O
holds	O
that	O
trace	O
(	O
u	O
(	O
cid:62	O
)	O
au	O
)	O
≤	O
(	O
cid:80	O
)	O
n	O
it	O
is	O
not	O
hard	O
to	O
verify	O
(	O
see	O
exercise	O
2	O
)	O
that	O
the	O
right-hand	O
side	O
equals	O
to	O
j=1	O
dj	O
,	O
j	O
.	O
we	O
have	O
therefore	O
shown	O
that	O
for	O
every	O
matrix	O
u	O
∈	O
rd	O
,	O
n	O
with	O
or-	O
we	O
obtain	O
that	O
trace	O
(	O
u	O
(	O
cid:62	O
)	O
au	O
)	O
=	O
(	O
cid:80	O
)	O
n	O
j=1	O
dj	O
,	O
j	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
we	O
set	B
u	O
to	O
be	O
the	O
matrix	O
whose	O
columns	O
are	O
the	O
n	O
leading	O
eigenvectors	O
of	O
a	O
objective	O
of	O
equation	O
(	O
23.4	O
)	O
is	O
(	O
cid:80	O
)	O
n	O
and	O
noting	O
that	O
(	O
cid:80	O
)	O
m	O
objective	O
value	O
of	O
equation	O
(	O
23.1	O
)	O
is	O
(	O
cid:80	O
)	O
d	O
remark	O
23.1	O
the	O
proof	O
of	O
theorem	O
23.2	O
also	O
tells	O
us	O
that	O
the	O
value	O
of	O
the	O
i=1	O
di	O
,	O
i	O
.	O
combining	O
this	O
with	O
equation	O
(	O
23.3	O
)	O
i=1	O
di	O
,	O
i	O
we	O
obtain	O
that	O
the	O
optimal	O
j=1	O
dj	O
,	O
j	O
,	O
and	O
this	O
concludes	O
our	O
proof	O
.	O
i=n+1	O
di	O
,	O
i	O
.	O
i=1	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
2	O
=	O
trace	O
(	O
a	O
)	O
=	O
(	O
cid:80	O
)	O
d	O
(	O
cid:80	O
)	O
m	O
remark	O
23.2	O
it	O
is	O
a	O
common	O
practice	O
to	O
“	O
center	O
”	O
the	O
examples	O
before	O
applying	O
pca	O
.	O
that	O
is	O
,	O
we	O
ﬁrst	O
calculate	O
µ	O
=	O
1	O
i=1	O
xi	O
and	O
then	O
apply	O
pca	O
on	O
the	O
vectors	O
(	O
x1	O
−	O
µ	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
−	O
µ	O
)	O
.	O
this	O
is	O
also	O
related	O
to	O
the	O
interpretation	O
of	O
pca	O
m	O
as	O
variance	O
maximization	O
(	O
see	O
exercise	O
4	O
)	O
.	O
23.1.1	O
a	O
more	O
eﬃcient	O
solution	O
for	O
the	O
case	O
d	O
(	O
cid:29	O
)	O
m	O
in	O
some	O
situations	O
the	O
original	O
dimensionality	O
of	O
the	O
data	O
is	O
much	O
larger	O
than	O
the	O
number	O
of	O
examples	O
m.	O
the	O
computational	B
complexity	I
of	O
calculating	O
the	O
pca	O
solution	O
as	O
described	O
previously	O
is	O
o	O
(	O
d3	O
)	O
(	O
for	O
calculating	O
eigenvalues	O
of	O
a	O
)	O
plus	O
o	O
(	O
md2	O
)	O
(	O
for	O
constructing	O
the	O
matrix	O
a	O
)	O
.	O
we	O
now	O
show	O
a	O
simple	O
trick	O
that	O
enables	O
us	O
to	O
calculate	O
the	O
pca	O
solution	O
more	O
eﬃciently	O
when	O
d	O
(	O
cid:29	O
)	O
m.	O
recall	B
that	O
the	O
matrix	O
a	O
is	O
deﬁned	O
to	O
be	O
(	O
cid:80	O
)	O
m	O
i	O
.	O
it	O
is	O
convenient	O
to	O
rewrite	O
a	O
=	O
x	O
(	O
cid:62	O
)	O
x	O
where	O
x	O
∈	O
rm	O
,	O
d	O
is	O
a	O
matrix	O
whose	O
ith	O
row	O
is	O
x	O
(	O
cid:62	O
)	O
i	O
.	O
consider	O
the	O
matrix	O
b	O
=	O
xx	O
(	O
cid:62	O
)	O
.	O
that	O
is	O
,	O
b	O
∈	O
rm	O
,	O
m	O
is	O
the	O
matrix	O
whose	O
i	O
,	O
j	O
element	O
equals	O
(	O
cid:104	O
)	O
xi	O
,	O
xj	O
(	O
cid:105	O
)	O
.	O
suppose	O
that	O
u	O
is	O
an	O
eigenvector	O
of	O
b	O
:	O
that	O
is	O
,	O
bu	O
=	O
λu	O
for	O
some	O
λ	O
∈	O
r.	O
multiplying	O
the	O
equality	O
by	O
x	O
(	O
cid:62	O
)	O
and	O
using	O
the	O
deﬁnition	O
of	O
b	O
we	O
obtain	O
x	O
(	O
cid:62	O
)	O
xx	O
(	O
cid:62	O
)	O
u	O
=	O
λx	O
(	O
cid:62	O
)	O
u.	O
but	O
,	O
using	O
the	O
deﬁnition	O
of	O
a	O
,	O
we	O
get	O
that	O
a	O
(	O
x	O
(	O
cid:62	O
)	O
u	O
)	O
=	O
λ	O
(	O
x	O
(	O
cid:62	O
)	O
u	O
)	O
.	O
thus	O
,	O
x	O
(	O
cid:62	O
)	O
u	O
(	O
cid:107	O
)	O
x	O
(	O
cid:62	O
)	O
u	O
(	O
cid:107	O
)	O
is	O
an	O
eigenvector	O
of	O
a	O
with	O
eigenvalue	O
of	O
λ.	O
i=1	O
xix	O
(	O
cid:62	O
)	O
we	O
can	O
therefore	O
calculate	O
the	O
pca	O
solution	O
by	O
calculating	O
the	O
eigenvalues	O
of	O
b	O
instead	O
of	O
a.	O
the	O
complexity	O
is	O
o	O
(	O
m3	O
)	O
(	O
for	O
calculating	O
eigenvalues	O
of	O
b	O
)	O
and	O
m2d	O
(	O
for	O
constructing	O
the	O
matrix	O
b	O
)	O
.	O
remark	O
23.3	O
the	O
previous	O
discussion	O
also	O
implies	O
that	O
to	O
calculate	O
the	O
pca	O
solution	O
we	O
only	O
need	O
to	O
know	O
how	O
to	O
calculate	O
inner	O
products	O
between	O
vectors	O
.	O
this	O
enables	O
us	O
to	O
calculate	O
pca	O
implicitly	O
even	O
when	O
d	O
is	O
very	O
large	O
(	O
or	O
even	O
inﬁnite	O
)	O
using	O
kernels	B
,	O
which	O
yields	O
the	O
kernel	O
pca	O
algorithm	O
.	O
23.1.2	O
implementation	O
and	O
demonstration	O
a	O
pseudocode	O
of	O
pca	O
is	O
given	O
in	O
the	O
following	O
.	O
23.1	O
principal	O
component	O
analysis	O
(	O
pca	O
)	O
327	O
figure	O
23.1	O
a	O
set	B
of	O
vectors	O
in	O
r2	O
(	O
blue	O
x	O
’	O
s	O
)	O
and	O
their	O
reconstruction	O
after	O
dimensionality	B
reduction	I
to	O
r1	O
using	O
pca	O
(	O
red	O
circles	O
)	O
.	O
pca	O
input	O
a	O
matrix	O
of	O
m	O
examples	O
x	O
∈	O
rm	O
,	O
d	O
number	O
of	O
components	O
n	O
if	O
(	O
m	O
>	O
d	O
)	O
a	O
=	O
x	O
(	O
cid:62	O
)	O
x	O
let	O
u1	O
,	O
.	O
.	O
.	O
,	O
un	O
be	O
the	O
eigenvectors	O
of	O
a	O
with	O
largest	O
eigenvalues	O
else	O
b	O
=	O
xx	O
(	O
cid:62	O
)	O
let	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
be	O
the	O
eigenvectors	O
of	O
b	O
with	O
largest	O
eigenvalues	O
for	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
set	B
ui	O
=	O
(	O
cid:107	O
)	O
x	O
(	O
cid:62	O
)	O
vi	O
(	O
cid:107	O
)	O
x	O
(	O
cid:62	O
)	O
vi	O
1	O
output	O
:	O
u1	O
,	O
.	O
.	O
.	O
,	O
un	O
to	O
illustrate	O
how	O
pca	O
works	O
,	O
let	O
us	O
generate	O
vectors	O
in	O
r2	O
that	O
approximately	O
reside	O
on	O
a	O
line	O
,	O
namely	O
,	O
on	O
a	O
one	O
dimensional	O
subspace	O
of	O
r2	O
.	O
for	O
example	O
,	O
suppose	O
that	O
each	O
example	O
is	O
of	O
the	O
form	O
(	O
x	O
,	O
x	O
+	O
y	O
)	O
where	O
x	O
is	O
chosen	O
uniformly	O
at	O
random	O
from	O
[	O
−1	O
,	O
1	O
]	O
and	O
y	O
is	O
sampled	O
from	O
a	O
gaussian	O
distribution	O
with	O
mean	O
0	O
and	O
standard	O
deviation	O
of	O
0.1.	O
suppose	O
we	O
apply	O
pca	O
on	O
this	O
data	O
.	O
then	O
,	O
the	O
eigenvector	O
corresponding	O
to	O
the	O
largest	O
eigenvalue	O
will	O
be	O
close	O
to	O
the	O
vector	O
(	O
1/	O
2	O
)	O
.	O
when	O
projecting	O
a	O
point	O
(	O
x	O
,	O
x	O
+	O
y	O
)	O
on	O
this	O
principal	O
component	O
we	O
will	O
obtain	O
the	O
scalar	O
2x+y√	O
.	O
the	O
reconstruction	O
of	O
the	O
original	O
vector	O
will	O
be	O
2	O
(	O
(	O
x	O
+	O
y/2	O
)	O
,	O
(	O
x	O
+	O
y/2	O
)	O
)	O
.	O
in	O
figure	O
23.1	O
we	O
depict	O
the	O
original	O
versus	O
reconstructed	O
data	O
.	O
2	O
,	O
1/	O
√	O
√	O
next	O
,	O
we	O
demonstrate	O
the	O
eﬀectiveness	O
of	O
pca	O
on	O
a	O
data	O
set	B
of	O
faces	O
.	O
we	O
extracted	O
images	O
of	O
faces	O
from	O
the	O
yale	O
data	O
set	B
(	O
georghiades	O
,	O
belhumeur	O
&	O
kriegman	O
2001	O
)	O
.	O
each	O
image	O
contains	O
50×50	O
=	O
2500	O
pixels	O
;	O
therefore	O
the	O
original	O
dimensionality	O
is	O
very	O
high	O
.	O
−1.5−1−0.500.511.5−1.5−1−0.500.511.5	O
328	O
dimensionality	B
reduction	I
o	O
o	O
oo	O
o	O
o	O
o	O
+	O
+	O
++	O
+	O
+	O
+	O
x	O
x	O
x	O
x	O
xx	O
x	O
*	O
*	O
***	O
*	O
*	O
figure	O
23.2	O
images	O
of	O
faces	O
extracted	O
from	O
the	O
yale	O
data	O
set	B
.	O
top-left	O
:	O
the	O
original	O
images	O
in	O
r50x50	O
.	O
top-right	O
:	O
the	O
images	O
after	O
dimensionality	B
reduction	I
to	O
r10	O
and	O
reconstruction	O
.	O
middle	O
row	O
:	O
an	O
enlarged	O
version	O
of	O
one	O
of	O
the	O
images	O
before	O
and	O
after	O
pca	O
.	O
bottom	O
:	O
the	O
images	O
after	O
dimensionality	B
reduction	I
to	O
r2	O
.	O
the	O
diﬀerent	O
marks	O
indicate	O
diﬀerent	O
individuals	O
.	O
some	O
images	O
of	O
faces	O
are	O
depicted	O
on	O
the	O
top-left	O
side	O
of	O
figure	O
23.2.	O
using	O
pca	O
,	O
we	O
reduced	O
the	O
dimensionality	O
to	O
r10	O
and	O
reconstructed	O
back	O
to	O
the	O
orig-	O
inal	O
dimension	B
,	O
which	O
is	O
502.	O
the	O
resulting	O
reconstructed	O
images	O
are	O
depicted	O
on	O
the	O
top-right	O
side	O
of	O
figure	O
23.2.	O
finally	O
,	O
on	O
the	O
bottom	O
of	O
figure	O
23.2	O
we	O
depict	O
a	O
2	O
dimensional	O
representation	O
of	O
the	O
images	O
.	O
as	O
can	O
be	O
seen	O
,	O
even	O
from	O
a	O
2	O
dimensional	O
representation	O
of	O
the	O
images	O
we	O
can	O
still	O
roughly	O
separate	O
diﬀerent	O
individuals	O
.	O
23.2	O
random	B
projections	I
329	O
23.2	O
random	B
projections	I
in	O
this	O
section	O
we	O
show	O
that	O
reducing	O
the	O
dimension	B
by	O
using	O
a	O
random	O
linear	O
transformation	O
leads	O
to	O
a	O
simple	O
compression	B
scheme	I
with	O
a	O
surprisingly	O
low	O
distortion	O
.	O
the	O
transformation	O
x	O
(	O
cid:55	O
)	O
→	O
w	O
x	O
,	O
when	O
w	O
is	O
a	O
random	O
matrix	O
,	O
is	O
often	O
referred	O
to	O
as	O
a	O
random	O
projection	O
.	O
in	O
particular	O
,	O
we	O
provide	O
a	O
variant	O
of	O
a	O
famous	O
lemma	O
due	O
to	O
johnson	O
and	O
lindenstrauss	O
,	O
showing	O
that	O
random	B
projections	I
do	O
not	O
distort	O
euclidean	O
distances	O
too	O
much	O
.	O
let	O
x1	O
,	O
x2	O
be	O
two	O
vectors	O
in	O
rd	O
.	O
a	O
matrix	O
w	O
does	O
not	O
distort	O
too	O
much	O
the	O
distance	O
between	O
x1	O
and	O
x2	O
if	O
the	O
ratio	O
(	O
cid:107	O
)	O
w	O
x1	O
−	O
w	O
x2	O
(	O
cid:107	O
)	O
(	O
cid:107	O
)	O
x1	O
−	O
x2	O
(	O
cid:107	O
)	O
is	O
close	O
to	O
1.	O
in	O
other	O
words	O
,	O
the	O
distances	O
between	O
x1	O
and	O
x2	O
before	O
and	O
after	O
the	O
transformation	O
are	O
almost	O
the	O
same	O
.	O
to	O
show	O
that	O
(	O
cid:107	O
)	O
w	O
x1	O
−	O
w	O
x2	O
(	O
cid:107	O
)	O
is	O
not	O
too	O
far	O
away	O
from	O
(	O
cid:107	O
)	O
x1	O
−	O
x2	O
(	O
cid:107	O
)	O
it	O
suﬃces	O
to	O
show	O
that	O
w	O
does	O
not	O
distort	O
the	O
norm	O
of	O
the	O
diﬀerence	O
vector	O
x	O
=	O
x1	O
−	O
x2	O
.	O
therefore	O
,	O
from	O
now	O
on	O
we	O
focus	O
on	O
the	O
ratio	O
(	O
cid:107	O
)	O
w	O
x	O
(	O
cid:107	O
)	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
.	O
we	O
start	O
with	O
analyzing	O
the	O
distortion	O
caused	O
by	O
applying	O
a	O
random	O
projection	O
to	O
a	O
single	O
vector	O
.	O
lemma	O
23.3	O
fix	O
some	O
x	O
∈	O
rd	O
.	O
let	O
w	O
∈	O
rn	O
,	O
d	O
be	O
a	O
random	O
matrix	O
such	O
that	O
each	O
wi	O
,	O
j	O
is	O
an	O
independent	O
normal	O
random	O
variable	O
.	O
then	O
,	O
for	O
every	O
	O
∈	O
(	O
0	O
,	O
3	O
)	O
we	O
have	O
p	O
(	O
cid:35	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
>	O
	O
√	O
n	O
)	O
w	O
x	O
(	O
cid:107	O
)	O
2	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
(	O
cid:34	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:107	O
)	O
(	O
1/	O
p	O
(	O
cid:2	O
)	O
(	O
1	O
−	O
	O
)	O
n	O
≤	O
(	O
cid:107	O
)	O
w	O
x	O
(	O
cid:107	O
)	O
2	O
≤	O
(	O
1	O
+	O
	O
)	O
n	O
(	O
cid:3	O
)	O
≥	O
1	O
−	O
2e−2n/6	O
.	O
≤	O
2	O
e−2n/6	O
.	O
−	O
1	O
proof	O
without	O
loss	B
of	O
generality	O
we	O
can	O
assume	O
that	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
=	O
1.	O
therefore	O
,	O
an	O
equivalent	O
inequality	O
is	O
with	O
zero	O
mean	O
and	O
variance	O
(	O
cid:80	O
)	O
able	O
(	O
cid:107	O
)	O
w	O
x	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:80	O
)	O
n	O
let	O
wi	O
be	O
the	O
ith	O
row	O
of	O
w	O
.	O
the	O
random	O
variable	O
(	O
cid:104	O
)	O
wi	O
,	O
x	O
(	O
cid:105	O
)	O
is	O
a	O
weighted	O
sum	O
of	O
d	O
independent	O
normal	O
random	O
variables	O
and	O
therefore	O
it	O
is	O
normally	O
distributed	O
j	O
=	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
=	O
1.	O
therefore	O
,	O
the	O
random	O
vari-	O
n	O
distribution	O
.	O
the	O
claim	O
now	O
follows	O
directly	O
from	O
a	O
measure	B
concentration	I
property	O
of	O
χ2	O
random	O
variables	O
stated	O
in	O
lemma	O
b.12	O
given	O
in	O
section	O
b.7	O
.	O
i=1	O
(	O
(	O
cid:104	O
)	O
wi	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
2	O
has	O
a	O
χ2	O
j	O
x2	O
the	O
johnson-lindenstrauss	O
lemma	O
follows	O
from	O
this	O
using	O
a	O
simple	O
union	B
bound	I
argument	O
.	O
lemma	O
23.4	O
(	O
johnson-lindenstrauss	O
lemma	O
)	O
let	O
q	O
be	O
a	O
ﬁnite	O
set	B
of	O
vectors	O
in	O
rd	O
.	O
let	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
and	O
n	O
be	O
an	O
integer	O
such	O
that	O
(	O
cid:114	O
)	O
	O
=	O
6	O
log	O
(	O
2|q|/δ	O
)	O
n	O
≤	O
3	O
.	O
330	O
dimensionality	B
reduction	I
then	O
,	O
with	O
probability	O
of	O
at	O
least	O
1−δ	O
over	O
a	O
choice	O
of	O
a	O
random	O
matrix	O
w	O
∈	O
rn	O
,	O
d	O
such	O
that	O
each	O
element	O
of	O
w	O
is	O
distributed	O
normally	O
with	O
zero	O
mean	O
and	O
variance	O
of	O
1/n	O
we	O
have	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
<	O
	O
.	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:107	O
)	O
w	O
x	O
(	O
cid:107	O
)	O
2	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
−	O
1	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
>	O
	O
(	O
cid:21	O
)	O
6	O
log	O
(	O
2|q|/δ	O
)	O
.	O
n	O
(	O
cid:20	O
)	O
p	O
sup	O
x∈q	O
sup	O
x∈q	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:107	O
)	O
w	O
x	O
(	O
cid:107	O
)	O
2	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
−	O
1	O
(	O
cid:114	O
)	O
	O
=	O
proof	O
combining	O
lemma	O
23.3	O
and	O
the	O
union	B
bound	I
we	O
have	O
that	O
for	O
every	O
	O
∈	O
(	O
0	O
,	O
3	O
)	O
:	O
≤	O
2|q|	O
e−2n/6	O
.	O
let	O
δ	O
denote	O
the	O
right-hand	O
side	O
of	O
the	O
inequality	O
;	O
thus	O
we	O
obtain	O
that	O
interestingly	O
,	O
the	O
bound	O
given	O
in	O
lemma	O
23.4	O
does	O
not	O
depend	O
on	O
the	O
original	O
dimension	B
of	O
x.	O
in	O
fact	O
,	O
the	O
bound	O
holds	O
even	O
if	O
x	O
is	O
in	O
an	O
inﬁnite	O
dimensional	O
hilbert	O
space	O
.	O
23.3	O
compressed	B
sensing	I
compressed	O
sensing	O
is	O
a	O
dimensionality	B
reduction	I
technique	O
which	O
utilizes	O
a	O
prior	O
assumption	O
that	O
the	O
original	O
vector	O
is	O
sparse	O
in	O
some	O
basis	O
.	O
to	O
motivate	O
com-	O
pressed	O
sensing	O
,	O
consider	O
a	O
vector	O
x	O
∈	O
rd	O
that	O
has	O
at	O
most	O
s	O
nonzero	O
elements	O
.	O
that	O
is	O
,	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
0	O
def=	O
|	O
{	O
i	O
:	O
xi	O
(	O
cid:54	O
)	O
=	O
0	O
}	O
|	O
≤	O
s.	O
clearly	O
,	O
we	O
can	O
compress	O
x	O
by	O
representing	O
it	O
using	O
s	O
(	O
index	O
,	O
value	O
)	O
pairs	O
.	O
fur-	O
thermore	O
,	O
this	O
compression	O
is	O
lossless	O
–	O
we	O
can	O
reconstruct	O
x	O
exactly	O
from	O
the	O
s	O
(	O
index	O
,	O
value	O
)	O
pairs	O
.	O
now	O
,	O
lets	O
take	O
one	O
step	O
forward	O
and	O
assume	O
that	O
x	O
=	O
u	O
α	O
,	O
where	O
α	O
is	O
a	O
sparse	O
vector	O
,	O
(	O
cid:107	O
)	O
α	O
(	O
cid:107	O
)	O
0	O
≤	O
s	O
,	O
and	O
u	O
is	O
a	O
ﬁxed	O
orthonormal	O
matrix	O
.	O
that	O
is	O
,	O
x	O
has	O
a	O
sparse	O
representation	O
in	O
another	O
basis	O
.	O
it	O
turns	O
out	O
that	O
many	O
nat-	O
ural	O
vectors	O
are	O
(	O
at	O
least	O
approximately	O
)	O
sparse	O
in	O
some	O
representation	O
.	O
in	O
fact	O
,	O
this	O
assumption	O
underlies	O
many	O
modern	O
compression	O
schemes	O
.	O
for	O
example	O
,	O
the	O
jpeg-2000	O
format	O
for	O
image	O
compression	O
relies	O
on	O
the	O
fact	O
that	O
natural	O
images	O
are	O
approximately	O
sparse	O
in	O
a	O
wavelet	O
basis	O
.	O
can	O
we	O
still	O
compress	O
x	O
into	O
roughly	O
s	O
numbers	O
?	O
well	O
,	O
one	O
simple	O
way	O
to	O
do	O
this	O
is	O
to	O
multiply	O
x	O
by	O
u	O
(	O
cid:62	O
)	O
,	O
which	O
yields	O
the	O
sparse	O
vector	O
α	O
,	O
and	O
then	O
represent	O
α	O
by	O
its	O
s	O
(	O
index	O
,	O
value	O
)	O
pairs	O
.	O
however	O
,	O
this	O
requires	O
us	O
ﬁrst	O
to	O
“	O
sense	O
”	O
x	O
,	O
to	O
store	O
it	O
,	O
and	O
then	O
to	O
multiply	O
it	O
by	O
u	O
(	O
cid:62	O
)	O
.	O
this	O
raises	O
a	O
very	O
natural	O
question	O
:	O
why	O
go	O
to	O
so	O
much	O
eﬀort	O
to	O
acquire	O
all	O
the	O
data	O
when	O
most	O
of	O
what	O
we	O
get	O
will	O
be	O
thrown	O
away	O
?	O
can	O
not	O
we	O
just	O
directly	O
measure	O
the	O
part	O
that	O
will	O
not	O
end	O
up	O
being	O
thrown	O
away	O
?	O
23.3	O
compressed	B
sensing	I
331	O
compressed	B
sensing	I
is	O
a	O
technique	O
that	O
simultaneously	O
acquires	O
and	O
com-	O
presses	O
the	O
data	O
.	O
the	O
key	O
result	O
is	O
that	O
a	O
random	O
linear	O
transformation	O
can	O
compress	O
x	O
without	O
losing	O
information	O
.	O
the	O
number	O
of	O
measurements	O
needed	O
is	O
order	O
of	O
s	O
log	O
(	O
d	O
)	O
.	O
that	O
is	O
,	O
we	O
roughly	O
acquire	O
only	O
the	O
important	O
information	O
about	O
the	O
signal	O
.	O
as	O
we	O
will	O
see	O
later	O
,	O
the	O
price	O
we	O
pay	O
is	O
a	O
slower	O
reconstruction	O
phase	O
.	O
in	O
some	O
situations	O
,	O
it	O
makes	O
sense	O
to	O
save	O
time	O
in	O
compression	O
even	O
at	O
the	O
price	O
of	O
a	O
slower	O
reconstruction	O
.	O
for	O
example	O
,	O
a	O
security	O
camera	O
should	O
sense	O
and	O
compress	O
a	O
large	O
amount	O
of	O
images	O
while	O
most	O
of	O
the	O
time	O
we	O
do	O
not	O
need	O
to	O
decode	O
the	O
compressed	O
data	O
at	O
all	O
.	O
furthermore	O
,	O
in	O
many	O
practical	O
applications	O
,	O
compression	O
by	O
a	O
linear	O
transformation	O
is	O
advantageous	O
because	O
it	O
can	O
be	O
per-	O
formed	O
eﬃciently	O
in	O
hardware	O
.	O
for	O
example	O
,	O
a	O
team	O
led	O
by	O
baraniuk	O
and	O
kelly	O
has	O
proposed	O
a	O
camera	O
architecture	O
that	O
employs	O
a	O
digital	O
micromirror	O
array	O
to	O
perform	O
optical	O
calculations	O
of	O
a	O
linear	O
transformation	O
of	O
an	O
image	O
.	O
in	O
this	O
case	O
,	O
obtaining	O
each	O
compressed	O
measurement	O
is	O
as	O
easy	O
as	O
obtaining	O
a	O
single	O
raw	O
measurement	O
.	O
another	O
important	O
application	O
of	O
compressed	B
sensing	I
is	O
medical	O
imaging	O
,	O
in	O
which	O
requiring	O
fewer	O
measurements	O
translates	O
to	O
less	O
radiation	O
for	O
the	O
patient	O
.	O
informally	O
,	O
the	O
main	O
premise	O
of	O
compressed	B
sensing	I
is	O
the	O
following	O
three	O
“	O
sur-	O
prising	O
”	O
results	O
:	O
1.	O
it	O
is	O
possible	O
to	O
reconstruct	O
any	O
sparse	O
signal	O
fully	O
if	O
it	O
was	O
compressed	O
by	O
x	O
(	O
cid:55	O
)	O
→	O
w	O
x	O
,	O
where	O
w	O
is	O
a	O
matrix	O
which	O
satisﬁes	O
a	O
condition	O
called	O
the	O
re-	O
stricted	O
isoperimetric	O
property	O
(	O
rip	O
)	O
.	O
a	O
matrix	O
that	O
satisﬁes	O
this	O
property	O
is	O
guaranteed	O
to	O
have	O
a	O
low	O
distortion	O
of	O
the	O
norm	O
of	O
any	O
sparse	O
representable	O
vector	O
.	O
2.	O
the	O
reconstruction	O
can	O
be	O
calculated	O
in	O
polynomial	O
time	O
by	O
solving	O
a	O
linear	O
program	O
.	O
3.	O
a	O
random	O
n	O
×	O
d	O
matrix	O
is	O
likely	O
to	O
satisfy	O
the	O
rip	O
condition	O
provided	O
that	O
n	O
is	O
greater	O
than	O
an	O
order	O
of	O
s	O
log	O
(	O
d	O
)	O
.	O
formally	O
,	O
definition	O
23.5	O
(	O
rip	O
)	O
a	O
matrix	O
w	O
∈	O
rn	O
,	O
d	O
is	O
(	O
	O
,	O
s	O
)	O
-rip	O
if	O
for	O
all	O
x	O
(	O
cid:54	O
)	O
=	O
0	O
s.t	O
.	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
0	O
≤	O
s	O
we	O
have	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:107	O
)	O
w	O
x	O
(	O
cid:107	O
)	O
2	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
2	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
≤	O
	O
.	O
2	O
−	O
1	O
the	O
ﬁrst	O
theorem	O
establishes	O
that	O
rip	O
matrices	O
yield	O
a	O
lossless	O
compression	B
scheme	I
for	O
sparse	O
vectors	O
.	O
it	O
also	O
provides	O
a	O
(	O
noneﬃcient	O
)	O
reconstruction	O
scheme	O
.	O
theorem	O
23.6	O
let	O
	O
<	O
1	O
and	O
let	O
w	O
be	O
a	O
(	O
	O
,	O
2s	O
)	O
-rip	O
matrix	O
.	O
let	O
x	O
be	O
a	O
vector	O
s.t	O
.	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
0	O
≤	O
s	O
,	O
let	O
y	O
=	O
w	O
x	O
be	O
the	O
compression	O
of	O
x	O
,	O
and	O
let	O
˜x	O
∈	O
argmin	O
v	O
:	O
w	O
v=y	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
0	O
be	O
a	O
reconstructed	O
vector	O
.	O
then	O
,	O
˜x	O
=	O
x	O
.	O
332	O
dimensionality	B
reduction	I
proof	O
we	O
assume	O
,	O
by	O
way	O
of	O
contradiction	O
,	O
that	O
˜x	O
(	O
cid:54	O
)	O
=	O
x.	O
since	O
x	O
satisﬁes	O
the	O
constraints	O
in	O
the	O
optimization	O
problem	O
that	O
deﬁnes	O
˜x	O
we	O
clearly	O
have	O
that	O
(	O
cid:107	O
)	O
˜x	O
(	O
cid:107	O
)	O
0	O
≤	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
0	O
≤	O
s.	O
therefore	O
,	O
(	O
cid:107	O
)	O
x	O
−	O
˜x	O
(	O
cid:107	O
)	O
0	O
≤	O
2s	O
and	O
we	O
can	O
apply	O
the	O
rip	O
in-	O
equality	O
on	O
the	O
vector	O
x	O
−	O
˜x	O
.	O
but	O
,	O
since	O
w	O
(	O
x	O
−	O
˜x	O
)	O
=	O
0	O
we	O
get	O
that	O
|0	O
−	O
1|	O
≤	O
	O
,	O
which	O
leads	O
to	O
a	O
contradiction	O
.	O
the	O
reconstruction	O
scheme	O
given	O
in	O
theorem	O
23.6	O
seems	O
to	O
be	O
noneﬃcient	O
because	O
we	O
need	O
to	O
minimize	O
a	O
combinatorial	O
objective	O
(	O
the	O
sparsity	O
of	O
v	O
)	O
.	O
quite	O
surprisingly	O
,	O
it	O
turns	O
out	O
that	O
we	O
can	O
replace	O
the	O
combinatorial	O
objective	O
,	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
0	O
,	O
with	O
a	O
convex	B
objective	O
,	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
1	O
,	O
which	O
leads	O
to	O
a	O
linear	B
programming	I
problem	O
that	O
can	O
be	O
solved	O
eﬃciently	O
.	O
this	O
is	O
stated	O
formally	O
in	O
the	O
following	O
theorem	O
.	O
theorem	O
23.7	O
assume	O
that	O
the	O
conditions	O
of	O
theorem	O
23.6	O
holds	O
and	O
that	O
√	O
	O
<	O
1	O
1+	O
.	O
then	O
,	O
2	O
x	O
=	O
argmin	O
v	O
:	O
w	O
v=y	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
0	O
=	O
argmin	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
1.	O
v	O
:	O
w	O
v=y	O
in	O
fact	O
,	O
we	O
will	O
prove	O
a	O
stronger	O
result	O
,	O
which	O
holds	O
even	O
if	O
x	O
is	O
not	O
a	O
sparse	O
vector	O
.	O
√	O
theorem	O
23.8	O
let	O
	O
<	O
1	O
1+	O
arbitrary	O
vector	O
and	O
denote	O
2	O
and	O
let	O
w	O
be	O
a	O
(	O
	O
,	O
2s	O
)	O
-rip	O
matrix	O
.	O
let	O
x	O
be	O
an	O
xs	O
∈	O
argmin	O
v	O
:	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
0≤s	O
(	O
cid:107	O
)	O
x	O
−	O
v	O
(	O
cid:107	O
)	O
1.	O
that	O
is	O
,	O
xs	O
is	O
the	O
vector	O
which	O
equals	O
x	O
on	O
the	O
s	O
largest	O
elements	O
of	O
x	O
and	O
equals	O
0	O
elsewhere	O
.	O
let	O
y	O
=	O
w	O
x	O
be	O
the	O
compression	O
of	O
x	O
and	O
let	O
x	O
(	O
cid:63	O
)	O
∈	O
argmin	O
v	O
:	O
w	O
v=y	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
1	O
be	O
the	O
reconstructed	O
vector	O
.	O
then	O
,	O
(	O
cid:107	O
)	O
x	O
(	O
cid:63	O
)	O
−	O
x	O
(	O
cid:107	O
)	O
2	O
≤	O
2	O
1	O
+	O
ρ	O
1	O
−	O
ρ	O
s−1/2	O
(	O
cid:107	O
)	O
x	O
−	O
xs	O
(	O
cid:107	O
)	O
1	O
,	O
2/	O
(	O
1	O
−	O
	O
)	O
.	O
√	O
where	O
ρ	O
=	O
note	O
that	O
in	O
the	O
special	O
case	O
that	O
x	O
=	O
xs	O
we	O
get	O
an	O
exact	O
recovery	O
,	O
x	O
(	O
cid:63	O
)	O
=	O
x	O
,	O
so	O
theorem	O
23.7	O
is	O
a	O
special	O
case	O
of	O
theorem	O
23.8.	O
the	O
proof	O
of	O
theorem	O
23.8	O
is	O
given	O
in	O
section	O
23.3.1.	O
finally	O
,	O
the	O
third	O
result	O
tells	O
us	O
that	O
random	O
matrices	O
with	O
n	O
≥	O
ω	O
(	O
s	O
log	O
(	O
d	O
)	O
)	O
are	O
likely	O
to	O
be	O
rip	O
.	O
in	O
fact	O
,	O
the	O
theorem	O
shows	O
that	O
multiplying	O
a	O
random	O
matrix	O
by	O
an	O
orthonormal	O
matrix	O
also	O
provides	O
an	O
rip	O
matrix	O
.	O
this	O
is	O
important	O
for	O
compressing	O
signals	O
of	O
the	O
form	O
x	O
=	O
u	O
α	O
where	O
x	O
is	O
not	O
sparse	O
but	O
α	O
is	O
sparse	O
.	O
in	O
that	O
case	O
,	O
if	O
w	O
is	O
a	O
random	O
matrix	O
and	O
we	O
compress	O
using	O
y	O
=	O
w	O
x	O
then	O
this	O
is	O
the	O
same	O
as	O
compressing	O
α	O
by	O
y	O
=	O
(	O
w	O
u	O
)	O
α	O
and	O
since	O
w	O
u	O
is	O
also	O
rip	O
we	O
can	O
reconstruct	O
α	O
(	O
and	O
thus	O
also	O
x	O
)	O
from	O
y	O
.	O
23.3	O
compressed	B
sensing	I
333	O
theorem	O
23.9	O
let	O
u	O
be	O
an	O
arbitrary	O
ﬁxed	O
d	O
×	O
d	O
orthonormal	O
matrix	O
,	O
let	O
	O
,	O
δ	O
be	O
scalars	O
in	O
(	O
0	O
,	O
1	O
)	O
,	O
let	O
s	O
be	O
an	O
integer	O
in	O
[	O
d	O
]	O
,	O
and	O
let	O
n	O
be	O
an	O
integer	O
that	O
satisﬁes	O
n	O
≥	O
100	O
s	O
log	O
(	O
40d/	O
(	O
δ	O
	O
)	O
)	O
.	O
2	O
let	O
w	O
∈	O
rn	O
,	O
d	O
be	O
a	O
matrix	O
s.t	O
.	O
each	O
element	O
of	O
w	O
is	O
distributed	O
normally	O
with	O
zero	O
mean	O
and	O
variance	O
of	O
1/n	O
.	O
then	O
,	O
with	O
proabability	O
of	O
at	O
least	O
1−	O
δ	O
over	O
the	O
choice	O
of	O
w	O
,	O
the	O
matrix	O
w	O
u	O
is	O
(	O
	O
,	O
s	O
)	O
-rip	O
.	O
23.3.1	O
proofs*	O
proof	O
of	O
theorem	O
23.8	O
we	O
follow	O
a	O
proof	O
due	O
to	O
cand`es	O
(	O
2008	O
)	O
.	O
let	O
h	O
=	O
x	O
(	O
cid:63	O
)	O
−	O
x.	O
given	O
a	O
vector	O
v	O
and	O
a	O
set	B
of	O
indices	O
i	O
we	O
denote	O
by	O
vi	O
the	O
vector	O
whose	O
ith	O
element	O
is	O
vi	O
if	O
i	O
∈	O
i	O
and	O
0	O
otherwise	O
.	O
the	O
ﬁrst	O
trick	O
we	O
use	O
is	O
to	O
partition	O
the	O
set	B
of	O
indices	O
[	O
d	O
]	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
}	O
into	O
disjoint	O
sets	O
of	O
size	O
s.	O
that	O
is	O
,	O
we	O
will	O
write	O
[	O
d	O
]	O
=	O
t0	O
·∪	O
t1	O
·∪	O
t2	O
.	O
.	O
.	O
td/s−1	O
where	O
for	O
all	O
i	O
,	O
|ti|	O
=	O
s	O
,	O
and	O
we	O
assume	O
for	O
simplicity	O
that	O
d/s	O
is	O
an	O
integer	O
.	O
we	O
deﬁne	O
the	O
partition	O
as	O
follows	O
.	O
in	O
t0	O
we	O
put	O
the	O
s	O
indices	O
corresponding	O
to	O
the	O
s	O
largest	O
0	O
=	O
[	O
d	O
]	O
\	O
t0	O
.	O
elements	O
in	O
absolute	O
values	O
of	O
x	O
(	O
ties	O
are	O
broken	O
arbitrarily	O
)	O
.	O
let	O
t	O
c	O
next	O
,	O
t1	O
will	O
be	O
the	O
s	O
indices	O
corresponding	O
to	O
the	O
s	O
largest	O
elements	O
in	O
absolute	O
0,1	O
=	O
[	O
d	O
]	O
\	O
t0,1	O
.	O
next	O
,	O
t2	O
will	O
correspond	O
to	O
value	O
of	O
ht	O
c	O
the	O
s	O
largest	O
elements	O
in	O
absolute	O
value	O
of	O
ht	O
c	O
.	O
and	O
,	O
we	O
will	O
construct	O
t3	O
,	O
t4	O
,	O
.	O
.	O
.	O
in	O
the	O
same	O
way	O
.	O
.	O
let	O
t0,1	O
=	O
t0	O
∪	O
t1	O
and	O
t	O
c	O
0,1	O
0	O
to	O
prove	O
the	O
theorem	O
we	O
ﬁrst	O
need	O
the	O
following	O
lemma	O
,	O
which	O
shows	O
that	O
rip	O
also	O
implies	O
approximate	O
orthogonality	O
.	O
lemma	O
23.10	O
let	O
w	O
be	O
an	O
(	O
	O
,	O
2s	O
)	O
-rip	O
matrix	O
.	O
then	O
,	O
for	O
any	O
two	O
disjoint	O
sets	O
i	O
,	O
j	O
,	O
both	O
of	O
size	O
at	O
most	O
s	O
,	O
and	O
for	O
any	O
vector	O
u	O
we	O
have	O
that	O
(	O
cid:104	O
)	O
w	O
ui	O
,	O
w	O
uj	O
(	O
cid:105	O
)	O
≤	O
	O
(	O
cid:107	O
)	O
ui	O
(	O
cid:107	O
)	O
2	O
(	O
cid:107	O
)	O
uj	O
(	O
cid:107	O
)	O
2.	O
proof	O
w.l.o.g	O
.	O
assume	O
(	O
cid:107	O
)	O
ui	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:107	O
)	O
uj	O
(	O
cid:107	O
)	O
2	O
=	O
1	O
.	O
(	O
cid:104	O
)	O
w	O
ui	O
,	O
w	O
uj	O
(	O
cid:105	O
)	O
=	O
(	O
cid:107	O
)	O
w	O
ui	O
+	O
w	O
uj	O
(	O
cid:107	O
)	O
2	O
2	O
−	O
(	O
cid:107	O
)	O
w	O
ui	O
−	O
w	O
uj	O
(	O
cid:107	O
)	O
2	O
4	O
2	O
.	O
2	O
≤	O
but	O
,	O
since	O
|j	O
∪	O
i|	O
≤	O
2s	O
we	O
get	O
from	O
the	O
rip	O
condition	O
that	O
(	O
cid:107	O
)	O
w	O
ui	O
+	O
w	O
uj	O
(	O
cid:107	O
)	O
2	O
2	O
≤	O
−	O
(	O
1−	O
	O
)	O
(	O
(	O
cid:107	O
)	O
ui	O
(	O
cid:107	O
)	O
2	O
(	O
1	O
+	O
	O
)	O
(	O
(	O
cid:107	O
)	O
ui	O
(	O
cid:107	O
)	O
2	O
2	O
+	O
(	O
cid:107	O
)	O
uj	O
(	O
cid:107	O
)	O
2	O
2	O
)	O
=	O
2	O
(	O
1	O
+	O
	O
)	O
and	O
that	O
−	O
(	O
cid:107	O
)	O
w	O
ui	O
−	O
w	O
uj	O
(	O
cid:107	O
)	O
2	O
2	O
)	O
=	O
−2	O
(	O
1	O
−	O
	O
)	O
,	O
which	O
concludes	O
our	O
proof	O
.	O
2	O
+	O
(	O
cid:107	O
)	O
uj	O
(	O
cid:107	O
)	O
2	O
we	O
are	O
now	O
ready	O
to	O
prove	O
the	O
theorem	O
.	O
clearly	O
,	O
(	O
cid:107	O
)	O
2	O
≤	O
(	O
cid:107	O
)	O
ht0,1	O
(	O
cid:107	O
)	O
2	O
+	O
(	O
cid:107	O
)	O
ht	O
c	O
to	O
prove	O
the	O
theorem	O
we	O
will	O
show	O
the	O
following	O
two	O
claims	O
:	O
(	O
cid:107	O
)	O
h	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:107	O
)	O
ht0,1	O
+	O
ht	O
c	O
0,1	O
0,1	O
(	O
cid:107	O
)	O
2	O
.	O
(	O
23.5	O
)	O
claim	O
1	O
:	O
.	O
(	O
cid:107	O
)	O
ht	O
c	O
claim	O
2	O
:	O
.	O
(	O
cid:107	O
)	O
ht0,1	O
(	O
cid:107	O
)	O
2	O
≤	O
2ρ	O
0,1	O
(	O
cid:107	O
)	O
2	O
≤	O
(	O
cid:107	O
)	O
ht0	O
(	O
cid:107	O
)	O
2	O
+	O
2s−1/2	O
(	O
cid:107	O
)	O
x	O
−	O
xs	O
(	O
cid:107	O
)	O
1	O
.	O
1−ρ	O
s−1/2	O
(	O
cid:107	O
)	O
x	O
−	O
xs	O
(	O
cid:107	O
)	O
1	O
.	O
334	O
dimensionality	B
reduction	I
combining	O
these	O
two	O
claims	O
with	O
equation	O
(	O
23.5	O
)	O
we	O
get	O
that	O
(	O
cid:17	O
)	O
(	O
cid:107	O
)	O
h	O
(	O
cid:107	O
)	O
2	O
≤	O
(	O
cid:107	O
)	O
ht0,1	O
(	O
cid:107	O
)	O
2	O
+	O
(	O
cid:107	O
)	O
ht	O
c	O
(	O
cid:16	O
)	O
2ρ	O
0,1	O
≤	O
2	O
1−ρ	O
+	O
1	O
s−1/2	O
(	O
cid:107	O
)	O
x	O
−	O
xs	O
(	O
cid:107	O
)	O
1	O
(	O
cid:107	O
)	O
2	O
≤	O
2	O
(	O
cid:107	O
)	O
ht0,1	O
(	O
cid:107	O
)	O
2	O
+	O
2s−1/2	O
(	O
cid:107	O
)	O
x	O
−	O
xs	O
(	O
cid:107	O
)	O
1	O
=	O
2	O
1	O
+	O
ρ	O
1	O
−	O
ρ	O
s−1/2	O
(	O
cid:107	O
)	O
x	O
−	O
xs	O
(	O
cid:107	O
)	O
1	O
,	O
and	O
this	O
will	O
conclude	O
our	O
proof	O
.	O
proving	O
claim	O
1	O
:	O
to	O
prove	O
this	O
claim	O
we	O
do	O
not	O
use	O
the	O
rip	O
condition	O
at	O
all	O
but	O
only	O
use	O
the	O
fact	O
that	O
x	O
(	O
cid:63	O
)	O
minimizes	O
the	O
(	O
cid:96	O
)	O
1	O
norm	O
.	O
take	O
j	O
>	O
1.	O
for	O
each	O
i	O
∈	O
tj	O
and	O
i	O
(	O
cid:48	O
)	O
∈	O
tj−1	O
we	O
have	O
that	O
|hi|	O
≤	O
|hi	O
(	O
cid:48	O
)	O
|	O
.	O
therefore	O
,	O
(	O
cid:107	O
)	O
htj	O
(	O
cid:107	O
)	O
∞	O
≤	O
(	O
cid:107	O
)	O
htj−1	O
(	O
cid:107	O
)	O
1/s	O
.	O
thus	O
,	O
(	O
cid:107	O
)	O
htj	O
(	O
cid:107	O
)	O
2	O
≤	O
s1/2	O
(	O
cid:107	O
)	O
htj	O
(	O
cid:107	O
)	O
∞	O
≤	O
s−1/2	O
(	O
cid:107	O
)	O
htj−1	O
(	O
cid:107	O
)	O
1	O
.	O
(	O
cid:107	O
)	O
2	O
≤	O
(	O
cid:88	O
)	O
j≥2	O
summing	O
this	O
over	O
j	O
=	O
2	O
,	O
3	O
,	O
.	O
.	O
.	O
and	O
using	O
the	O
triangle	O
inequality	O
we	O
obtain	O
that	O
(	O
cid:107	O
)	O
ht	O
c	O
0,1	O
(	O
cid:107	O
)	O
htj	O
(	O
cid:107	O
)	O
2	O
≤	O
s−1/2	O
(	O
cid:107	O
)	O
ht	O
c	O
0	O
(	O
cid:107	O
)	O
1	O
(	O
23.6	O
)	O
next	O
,	O
we	O
show	O
that	O
(	O
cid:107	O
)	O
ht	O
c	O
(	O
cid:107	O
)	O
1	O
can	O
not	O
be	O
large	O
.	O
indeed	O
,	O
from	O
the	O
deﬁnition	O
of	O
x	O
(	O
cid:63	O
)	O
we	O
have	O
that	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
1	O
≥	O
(	O
cid:107	O
)	O
x	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
1	O
=	O
(	O
cid:107	O
)	O
x	O
+	O
h	O
(	O
cid:107	O
)	O
1.	O
thus	O
,	O
using	O
the	O
triangle	O
inequality	O
we	O
obtain	O
that	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
1	O
≥	O
(	O
cid:107	O
)	O
x+h	O
(	O
cid:107	O
)	O
1	O
=	O
|xi+hi|	O
≥	O
(	O
cid:107	O
)	O
xt0	O
(	O
cid:107	O
)	O
1−	O
(	O
cid:107	O
)	O
ht0	O
(	O
cid:107	O
)	O
1+	O
(	O
cid:107	O
)	O
ht	O
c	O
|xi+hi|+	O
(	O
cid:107	O
)	O
1−	O
(	O
cid:107	O
)	O
xt	O
c	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
(	O
cid:107	O
)	O
1	O
0	O
0	O
0	O
i∈t0	O
i∈t	O
c	O
0	O
(	O
23.7	O
)	O
(	O
23.8	O
)	O
(	O
cid:107	O
)	O
1	O
,	O
(	O
23.9	O
)	O
and	O
since	O
(	O
cid:107	O
)	O
xt	O
c	O
(	O
cid:107	O
)	O
1	O
=	O
(	O
cid:107	O
)	O
x	O
−	O
xs	O
(	O
cid:107	O
)	O
1	O
=	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
1	O
−	O
(	O
cid:107	O
)	O
xt0	O
(	O
cid:107	O
)	O
1	O
we	O
get	O
that	O
0	O
(	O
cid:107	O
)	O
ht	O
c	O
(	O
cid:107	O
)	O
1	O
≤	O
(	O
cid:107	O
)	O
ht0	O
(	O
cid:107	O
)	O
1	O
+	O
2	O
(	O
cid:107	O
)	O
xt	O
c	O
combining	O
this	O
with	O
equation	O
(	O
23.6	O
)	O
we	O
get	O
that	O
(	O
cid:107	O
)	O
2	O
≤	O
s−1/2	O
(	O
cid:0	O
)	O
(	O
cid:107	O
)	O
ht0	O
(	O
cid:107	O
)	O
1	O
+	O
2	O
(	O
cid:107	O
)	O
xt	O
c	O
(	O
cid:107	O
)	O
ht	O
c	O
0,1	O
(	O
cid:107	O
)	O
1	O
(	O
cid:1	O
)	O
≤	O
(	O
cid:107	O
)	O
ht0	O
(	O
cid:107	O
)	O
2	O
+	O
2s−1/2	O
(	O
cid:107	O
)	O
xt	O
c	O
(	O
cid:107	O
)	O
1	O
.	O
0	O
0	O
0	O
0	O
which	O
concludes	O
the	O
proof	O
of	O
claim	O
1.	O
proving	O
claim	O
2	O
:	O
for	O
the	O
second	O
claim	O
we	O
use	O
the	O
rip	O
condition	O
to	O
get	O
that	O
since	O
w	O
ht0,1	O
=	O
w	O
h	O
−	O
(	O
cid:80	O
)	O
2	O
=	O
−	O
(	O
cid:88	O
)	O
(	O
cid:107	O
)	O
w	O
ht0,1	O
(	O
cid:107	O
)	O
2	O
(	O
1	O
−	O
	O
)	O
(	O
cid:107	O
)	O
ht0,1	O
(	O
cid:107	O
)	O
2	O
j≥2	O
w	O
htj	O
=	O
−	O
(	O
cid:80	O
)	O
(	O
cid:104	O
)	O
w	O
ht0,1	O
,	O
w	O
htj	O
(	O
cid:105	O
)	O
=	O
−	O
(	O
cid:88	O
)	O
2	O
≤	O
(	O
cid:107	O
)	O
w	O
ht0,1	O
(	O
cid:107	O
)	O
2	O
2.	O
j≥2	O
j≥2	O
j≥2	O
w	O
htj	O
we	O
have	O
that	O
(	O
cid:104	O
)	O
w	O
ht0	O
+	O
w	O
ht1	O
,	O
w	O
htj	O
(	O
cid:105	O
)	O
.	O
from	O
the	O
rip	O
condition	O
on	O
inner	O
products	O
we	O
obtain	O
that	O
for	O
all	O
i	O
∈	O
{	O
1	O
,	O
2	O
}	O
and	O
j	O
≥	O
2	O
we	O
have	O
|	O
(	O
cid:104	O
)	O
w	O
hti	O
,	O
w	O
htj	O
(	O
cid:105	O
)	O
|	O
≤	O
	O
(	O
cid:107	O
)	O
hti	O
(	O
cid:107	O
)	O
2	O
(	O
cid:107	O
)	O
htj	O
(	O
cid:107	O
)	O
2	O
.	O
23.3	O
compressed	B
sensing	I
335	O
since	O
(	O
cid:107	O
)	O
ht0	O
(	O
cid:107	O
)	O
2	O
+	O
(	O
cid:107	O
)	O
ht1	O
(	O
cid:107	O
)	O
2	O
≤	O
√	O
2	O
(	O
cid:107	O
)	O
ht0,1	O
(	O
cid:107	O
)	O
2	O
we	O
therefore	O
get	O
that	O
(	O
cid:107	O
)	O
htj	O
(	O
cid:107	O
)	O
2	O
.	O
2	O
(	O
cid:107	O
)	O
ht0,1	O
(	O
cid:107	O
)	O
2	O
2	O
≤	O
(	O
cid:88	O
)	O
√	O
(	O
cid:107	O
)	O
w	O
ht0,1	O
(	O
cid:107	O
)	O
2	O
j≥2	O
combining	O
this	O
with	O
equation	O
(	O
23.6	O
)	O
and	O
equation	O
(	O
23.9	O
)	O
we	O
obtain	O
√	O
2	O
(	O
cid:107	O
)	O
ht0,1	O
(	O
cid:107	O
)	O
2s−1/2	O
(	O
cid:107	O
)	O
ht	O
c	O
0	O
(	O
cid:107	O
)	O
1.	O
s−1/2	O
(	O
cid:107	O
)	O
ht	O
c	O
(	O
cid:107	O
)	O
1	O
.	O
0	O
(	O
1	O
−	O
	O
)	O
(	O
cid:107	O
)	O
ht0,1	O
(	O
cid:107	O
)	O
2	O
rearranging	O
the	O
inequality	O
gives	O
2	O
≤	O
(	O
cid:107	O
)	O
ht0,1	O
(	O
cid:107	O
)	O
2	O
≤	O
√	O
2	O
1	O
−	O
	O
finally	O
,	O
using	O
equation	O
(	O
23.8	O
)	O
we	O
get	O
that	O
(	O
cid:107	O
)	O
ht0,1	O
(	O
cid:107	O
)	O
2	O
≤	O
ρs−1/2	O
(	O
(	O
cid:107	O
)	O
ht0	O
(	O
cid:107	O
)	O
1	O
+	O
2	O
(	O
cid:107	O
)	O
xt	O
c	O
but	O
since	O
(	O
cid:107	O
)	O
ht0	O
(	O
cid:107	O
)	O
2	O
≤	O
(	O
cid:107	O
)	O
ht0,1	O
(	O
cid:107	O
)	O
2	O
this	O
implies	O
(	O
cid:107	O
)	O
ht0,1	O
(	O
cid:107	O
)	O
2	O
≤	O
2ρ	O
1	O
−	O
ρ	O
0	O
(	O
cid:107	O
)	O
1	O
)	O
≤	O
ρ	O
(	O
cid:107	O
)	O
ht0	O
(	O
cid:107	O
)	O
2	O
+	O
2ρs−1/2	O
(	O
cid:107	O
)	O
xt	O
c	O
(	O
cid:107	O
)	O
1	O
,	O
0	O
s−1/2	O
(	O
cid:107	O
)	O
xt	O
c	O
(	O
cid:107	O
)	O
1	O
,	O
0	O
which	O
concludes	O
the	O
proof	O
of	O
the	O
second	O
claim	O
.	O
proof	O
of	O
theorem	O
23.9	O
to	O
prove	O
the	O
theorem	O
we	O
follow	O
an	O
approach	O
due	O
to	O
(	O
baraniuk	O
,	O
davenport	O
,	O
de-	O
vore	O
&	O
wakin	O
2008	O
)	O
.	O
the	O
idea	O
is	O
to	O
combine	O
the	O
johnson-lindenstrauss	O
(	O
jl	O
)	O
lemma	O
with	O
a	O
simple	O
covering	O
argument	O
.	O
we	O
start	O
with	O
a	O
covering	O
property	O
of	O
the	O
unit	O
ball	O
.	O
lemma	O
23.11	O
let	O
	O
∈	O
(	O
0	O
,	O
1	O
)	O
.	O
there	O
exists	O
a	O
ﬁnite	O
set	B
q	O
⊂	O
rd	O
of	O
size	O
|q|	O
≤	O
(	O
cid:0	O
)	O
3	O
(	O
cid:1	O
)	O
d	O
	O
such	O
that	O
sup	O
x	O
:	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
≤1	O
min	O
v∈q	O
(	O
cid:107	O
)	O
x	O
−	O
v	O
(	O
cid:107	O
)	O
≤	O
	O
.	O
proof	O
let	O
k	O
be	O
an	O
integer	O
and	O
let	O
q	O
(	O
cid:48	O
)	O
=	O
{	O
x	O
∈	O
rd	O
:	O
∀j	O
∈	O
[	O
d	O
]	O
,	O
∃i	O
∈	O
{	O
−k	O
,	O
−k	O
+	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
s.t	O
.	O
xj	O
=	O
i	O
k	O
}	O
.	O
clearly	O
,	O
|q	O
(	O
cid:48	O
)	O
|	O
=	O
(	O
2k	O
+	O
1	O
)	O
d.	O
we	O
shall	O
set	B
q	O
=	O
q	O
(	O
cid:48	O
)	O
∩	O
b2	O
(	O
1	O
)	O
,	O
where	O
b2	O
(	O
1	O
)	O
is	O
the	O
unit	O
(	O
cid:96	O
)	O
2	O
ball	O
of	O
rd	O
.	O
since	O
the	O
points	O
in	O
q	O
(	O
cid:48	O
)	O
are	O
distributed	O
evenly	O
on	O
the	O
unit	O
(	O
cid:96	O
)	O
∞	O
ball	O
,	O
the	O
size	O
of	O
q	O
is	O
the	O
size	O
of	O
q	O
(	O
cid:48	O
)	O
times	O
the	O
ratio	O
between	O
the	O
volumes	O
of	O
the	O
unit	O
(	O
cid:96	O
)	O
2	O
and	O
(	O
cid:96	O
)	O
∞	O
balls	O
.	O
the	O
volume	O
of	O
the	O
(	O
cid:96	O
)	O
∞	O
ball	O
is	O
2d	O
and	O
the	O
volume	O
of	O
b2	O
(	O
1	O
)	O
is	O
πd/2	O
γ	O
(	O
1	O
+	O
d/2	O
)	O
.	O
for	O
simplicity	O
,	O
assume	O
that	O
d	O
is	O
even	O
and	O
therefore	O
γ	O
(	O
1	O
+	O
d/2	O
)	O
=	O
(	O
d/2	O
)	O
!	O
≥	O
(	O
cid:16	O
)	O
d/2	O
(	O
cid:17	O
)	O
d/2	O
,	O
e	O
336	O
dimensionality	B
reduction	I
where	O
in	O
the	O
last	O
inequality	O
we	O
used	O
stirling	O
’	O
s	O
approximation	O
.	O
overall	O
we	O
obtained	O
that	O
|q|	O
≤	O
(	O
2k	O
+	O
1	O
)	O
d	O
(	O
π/e	O
)	O
d/2	O
(	O
d/2	O
)	O
−d/2	O
2−d	O
.	O
(	O
23.10	O
)	O
now	O
lets	O
specify	O
k.	O
for	O
each	O
x	O
∈	O
b2	O
(	O
1	O
)	O
let	O
v	O
∈	O
q	O
be	O
the	O
vector	O
whose	O
ith	O
element	O
is	O
sign	O
(	O
xi	O
)	O
(	O
cid:98	O
)	O
|xi|	O
k	O
(	O
cid:99	O
)	O
/k	O
.	O
then	O
,	O
for	O
each	O
element	O
we	O
have	O
that	O
|xi	O
−	O
vi|	O
≤	O
1/k	O
and	O
thus	O
(	O
cid:107	O
)	O
x	O
−	O
v	O
(	O
cid:107	O
)	O
≤	O
√	O
d	O
k	O
.	O
to	O
ensure	O
that	O
the	O
right-hand	O
side	O
will	O
be	O
at	O
most	O
	O
we	O
shall	O
set	B
k	O
=	O
(	O
cid:100	O
)	O
√	O
d/	O
(	O
cid:101	O
)	O
.	O
plugging	O
this	O
value	O
into	O
equation	O
(	O
23.10	O
)	O
we	O
conclude	O
that	O
√	O
|q|	O
≤	O
(	O
3	O
d/	O
(	O
2	O
)	O
)	O
d	O
(	O
π/e	O
)	O
d/2	O
(	O
d/2	O
)	O
−d/2	O
=	O
(	O
cid:16	O
)	O
3	O
(	O
cid:113	O
)	O
π	O
	O
2e	O
(	O
cid:17	O
)	O
d	O
≤	O
(	O
cid:0	O
)	O
3	O
	O
(	O
cid:1	O
)	O
d	O
.	O
let	O
x	O
be	O
a	O
vector	O
that	O
can	O
be	O
written	O
as	O
x	O
=	O
u	O
α	O
with	O
u	O
being	O
some	O
orthonor-	O
mal	O
matrix	O
and	O
(	O
cid:107	O
)	O
α	O
(	O
cid:107	O
)	O
0	O
≤	O
s.	O
combining	O
the	O
earlier	O
covering	O
property	O
and	O
the	O
jl	O
lemma	O
(	O
lemma	O
23.4	O
)	O
enables	O
us	O
to	O
show	O
that	O
a	O
random	O
w	O
will	O
not	O
distort	O
any	O
such	O
x.	O
lemma	O
23.12	O
let	O
u	O
be	O
an	O
orthonormal	O
d	O
×	O
d	O
matrix	O
and	O
let	O
i	O
⊂	O
[	O
d	O
]	O
be	O
a	O
set	B
of	O
indices	O
of	O
size	O
|i|	O
=	O
s.	O
let	O
s	O
be	O
the	O
span	O
of	O
{	O
ui	O
:	O
i	O
∈	O
i	O
}	O
,	O
where	O
ui	O
is	O
the	O
ith	O
column	O
of	O
u	O
.	O
let	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
and	O
n	O
∈	O
n	O
such	O
that	O
n	O
≥	O
24	O
log	O
(	O
2/δ	O
)	O
+	O
s	O
log	O
(	O
12/	O
)	O
2	O
.	O
then	O
,	O
with	O
probability	O
of	O
at	O
least	O
1−δ	O
over	O
a	O
choice	O
of	O
a	O
random	O
matrix	O
w	O
∈	O
rn	O
,	O
d	O
such	O
that	O
each	O
element	O
of	O
w	O
is	O
independently	O
distributed	O
according	O
to	O
n	O
(	O
0	O
,	O
1/n	O
)	O
,	O
we	O
have	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:107	O
)	O
w	O
x	O
(	O
cid:107	O
)	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
−	O
1	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
<	O
	O
.	O
sup	O
x∈s	O
it	O
suﬃces	O
to	O
prove	O
the	O
lemma	O
for	O
all	O
x	O
∈	O
s	O
with	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
=	O
1.	O
we	O
can	O
write	O
proof	O
x	O
=	O
ui	O
α	O
where	O
α	O
∈	O
rs	O
,	O
(	O
cid:107	O
)	O
α	O
(	O
cid:107	O
)	O
2	O
=	O
1	O
,	O
and	O
ui	O
is	O
the	O
matrix	O
whose	O
columns	O
are	O
{	O
ui	O
:	O
i	O
∈	O
i	O
}	O
.	O
using	O
lemma	O
23.11	O
we	O
know	O
that	O
there	O
exists	O
a	O
set	B
q	O
of	O
size	O
|q|	O
≤	O
(	O
12/	O
)	O
s	O
such	O
that	O
sup	O
α	O
:	O
(	O
cid:107	O
)	O
α	O
(	O
cid:107	O
)	O
=1	O
min	O
v∈q	O
(	O
cid:107	O
)	O
α	O
−	O
v	O
(	O
cid:107	O
)	O
≤	O
(	O
/4	O
)	O
.	O
but	O
since	O
u	O
is	O
orthogonal	O
we	O
also	O
have	O
that	O
sup	O
α	O
:	O
(	O
cid:107	O
)	O
α	O
(	O
cid:107	O
)	O
=1	O
min	O
v∈q	O
(	O
cid:107	O
)	O
ui	O
α	O
−	O
ui	O
v	O
(	O
cid:107	O
)	O
≤	O
(	O
/4	O
)	O
.	O
applying	O
lemma	O
23.4	O
on	O
the	O
set	B
{	O
ui	O
v	O
:	O
v	O
∈	O
q	O
}	O
we	O
obtain	O
that	O
for	O
n	O
satisfying	O
23.3	O
compressed	B
sensing	I
337	O
the	O
condition	O
given	O
in	O
the	O
lemma	O
,	O
the	O
following	O
holds	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
:	O
sup	O
v∈q	O
this	O
also	O
implies	O
that	O
(	O
cid:107	O
)	O
ui	O
v	O
(	O
cid:107	O
)	O
2	O
−	O
1	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:107	O
)	O
w	O
ui	O
v	O
(	O
cid:107	O
)	O
2	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:107	O
)	O
w	O
ui	O
v	O
(	O
cid:107	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
≤	O
/2	O
,	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
≤	O
/2	O
.	O
sup	O
v∈q	O
(	O
cid:107	O
)	O
ui	O
v	O
(	O
cid:107	O
)	O
−	O
1	O
let	O
a	O
be	O
the	O
smallest	O
number	O
such	O
that	O
∀x	O
∈	O
s	O
,	O
(	O
cid:107	O
)	O
w	O
x	O
(	O
cid:107	O
)	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
≤	O
1	O
+	O
a.	O
clearly	O
a	O
<	O
∞	O
.	O
our	O
goal	O
is	O
to	O
show	O
that	O
a	O
≤	O
	O
.	O
this	O
follows	O
from	O
the	O
fact	O
that	O
for	O
any	O
x	O
∈	O
s	O
of	O
unit	O
norm	O
there	O
exists	O
v	O
∈	O
q	O
such	O
that	O
(	O
cid:107	O
)	O
x	O
−	O
ui	O
v	O
(	O
cid:107	O
)	O
≤	O
/4	O
and	O
therefore	O
(	O
cid:107	O
)	O
w	O
x	O
(	O
cid:107	O
)	O
≤	O
(	O
cid:107	O
)	O
w	O
ui	O
v	O
(	O
cid:107	O
)	O
+	O
(	O
cid:107	O
)	O
w	O
(	O
x	O
−	O
ui	O
v	O
)	O
(	O
cid:107	O
)	O
≤	O
1	O
+	O
/2	O
+	O
(	O
1	O
+	O
a	O
)	O
/4	O
.	O
thus	O
,	O
∀x	O
∈	O
s	O
,	O
(	O
cid:107	O
)	O
w	O
x	O
(	O
cid:107	O
)	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
≤	O
1	O
+	O
(	O
/2	O
+	O
(	O
1	O
+	O
a	O
)	O
/4	O
)	O
.	O
but	O
the	O
deﬁnition	O
of	O
a	O
implies	O
that	O
a	O
≤	O
/2	O
+	O
(	O
1	O
+	O
a	O
)	O
/4	O
⇒	O
a	O
≤	O
/2	O
+	O
/4	O
1	O
−	O
/4	O
≤	O
	O
.	O
this	O
proves	O
that	O
for	O
all	O
x	O
∈	O
s	O
we	O
have	O
this	O
as	O
well	O
since	O
(	O
cid:107	O
)	O
w	O
x	O
(	O
cid:107	O
)	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
−	O
1	O
≤	O
	O
.	O
the	O
other	O
side	O
follows	O
from	O
(	O
cid:107	O
)	O
w	O
x	O
(	O
cid:107	O
)	O
≥	O
(	O
cid:107	O
)	O
w	O
ui	O
v	O
(	O
cid:107	O
)	O
−	O
(	O
cid:107	O
)	O
w	O
(	O
x	O
−	O
ui	O
v	O
)	O
(	O
cid:107	O
)	O
≥	O
1	O
−	O
/2	O
−	O
(	O
1	O
+	O
	O
)	O
/4	O
≥	O
1	O
−	O
	O
.	O
the	O
preceding	O
lemma	O
tells	O
us	O
that	O
for	O
x	O
∈	O
s	O
of	O
unit	O
norm	O
we	O
have	O
which	O
implies	O
that	O
(	O
1	O
−	O
	O
)	O
≤	O
(	O
cid:107	O
)	O
w	O
x	O
(	O
cid:107	O
)	O
≤	O
(	O
1	O
+	O
	O
)	O
,	O
(	O
1	O
−	O
2	O
	O
)	O
≤	O
(	O
cid:107	O
)	O
w	O
x	O
(	O
cid:107	O
)	O
2	O
≤	O
(	O
1	O
+	O
3	O
	O
)	O
.	O
the	O
proof	O
of	O
theorem	O
23.9	O
follows	O
from	O
this	O
by	O
a	O
union	B
bound	I
over	O
all	O
choices	O
of	O
i	O
.	O
338	O
dimensionality	B
reduction	I
23.4	O
pca	O
or	O
compressed	B
sensing	I
?	O
suppose	O
we	O
would	O
like	O
to	O
apply	O
a	O
dimensionality	B
reduction	I
technique	O
to	O
a	O
given	O
set	B
of	O
examples	O
.	O
which	O
method	O
should	O
we	O
use	O
,	O
pca	O
or	O
compressed	B
sensing	I
?	O
in	O
this	O
section	O
we	O
tackle	O
this	O
question	O
,	O
by	O
underscoring	O
the	O
underlying	O
assumptions	O
behind	O
the	O
two	O
methods	O
.	O
it	O
is	O
helpful	O
ﬁrst	O
to	O
understand	O
when	O
each	O
of	O
the	O
methods	O
can	O
guarantee	O
per-	O
fect	O
recovery	O
.	O
pca	O
guarantees	O
perfect	O
recovery	O
whenever	O
the	O
set	B
of	O
examples	O
is	O
contained	O
in	O
an	O
n	O
dimensional	O
subspace	O
of	O
rd	O
.	O
compressed	B
sensing	I
guarantees	O
perfect	O
recovery	O
whenever	O
the	O
set	B
of	O
examples	O
is	O
sparse	O
(	O
in	O
some	O
basis	O
)	O
.	O
on	O
the	O
basis	O
of	O
these	O
observations	O
,	O
we	O
can	O
describe	O
cases	O
in	O
which	O
pca	O
will	O
be	O
better	O
than	O
compressed	B
sensing	I
and	O
vice	O
versa	O
.	O
as	O
a	O
ﬁrst	O
example	O
,	O
suppose	O
that	O
the	O
examples	O
are	O
the	O
vectors	O
of	O
the	O
standard	O
basis	O
of	O
rd	O
,	O
namely	O
,	O
e1	O
,	O
.	O
.	O
.	O
,	O
ed	O
,	O
where	O
each	O
ei	O
is	O
the	O
all	O
zeros	O
vector	O
except	O
1	O
in	O
the	O
ith	O
coordinate	O
.	O
in	O
this	O
case	O
,	O
the	O
examples	O
are	O
1-sparse	O
.	O
hence	O
,	O
compressed	B
sensing	I
will	O
yield	O
a	O
perfect	O
recovery	O
whenever	O
n	O
≥	O
ω	O
(	O
log	O
(	O
d	O
)	O
)	O
.	O
on	O
the	O
other	O
hand	O
,	O
pca	O
will	O
lead	O
to	O
poor	O
performance	O
,	O
since	O
the	O
data	O
is	O
far	O
from	O
being	O
in	O
an	O
n	O
dimensional	O
subspace	O
,	O
as	O
long	O
as	O
n	O
<	O
d.	O
indeed	O
,	O
it	O
is	O
easy	O
ro	O
verify	O
that	O
in	O
such	O
a	O
case	O
,	O
the	O
averaged	O
recovery	O
error	O
of	O
pca	O
(	O
i.e.	O
,	O
the	O
objective	O
of	O
equation	O
(	O
23.1	O
)	O
divided	O
by	O
m	O
)	O
will	O
be	O
(	O
d	O
−	O
n	O
)	O
/d	O
,	O
which	O
is	O
larger	O
than	O
1/2	O
whenever	O
n	O
≤	O
d/2	O
.	O
we	O
next	O
show	O
a	O
case	O
where	O
pca	O
is	O
better	O
than	O
compressed	B
sensing	I
.	O
consider	O
m	O
examples	O
that	O
are	O
exactly	O
on	O
an	O
n	O
dimensional	O
subspace	O
.	O
clearly	O
,	O
in	O
such	O
a	O
case	O
,	O
pca	O
will	O
lead	O
to	O
perfect	O
recovery	O
.	O
as	O
to	O
compressed	B
sensing	I
,	O
note	O
that	O
the	O
examples	O
are	O
n-sparse	O
in	O
any	O
orthonormal	O
basis	O
whose	O
ﬁrst	O
n	O
vectors	O
span	O
the	O
subspace	O
.	O
therefore	O
,	O
compressed	B
sensing	I
would	O
also	O
work	O
if	O
we	O
will	O
reduce	O
the	O
dimension	B
to	O
ω	O
(	O
n	O
log	O
(	O
d	O
)	O
)	O
.	O
however	O
,	O
with	O
exactly	O
n	O
dimensions	O
,	O
compressed	B
sensing	I
might	O
fail	O
.	O
pca	O
has	O
also	O
better	O
resilience	O
to	O
certain	O
types	O
of	O
noise	O
.	O
see	O
(	O
chang	O
,	O
weiss	O
&	O
freeman	O
2009	O
)	O
for	O
a	O
discussion	O
.	O
23.5	O
summary	O
we	O
introduced	O
two	O
methods	O
for	O
dimensionality	B
reduction	I
using	O
linear	O
transfor-	O
mations	O
:	O
pca	O
and	O
random	B
projections	I
.	O
we	O
have	O
shown	O
that	O
pca	O
is	O
optimal	O
in	O
the	O
sense	O
of	O
averaged	O
squared	O
reconstruction	O
error	O
,	O
if	O
we	O
restrict	O
the	O
reconstruc-	O
tion	O
procedure	O
to	O
be	O
linear	O
as	O
well	O
.	O
however	O
,	O
if	O
we	O
allow	O
nonlinear	O
reconstruction	O
,	O
pca	O
is	O
not	O
necessarily	O
the	O
optimal	O
procedure	O
.	O
in	O
particular	O
,	O
for	O
sparse	O
data	O
,	O
ran-	O
dom	O
projections	O
can	O
signiﬁcantly	O
outperform	O
pca	O
.	O
this	O
fact	O
is	O
at	O
the	O
heart	O
of	O
the	O
compressed	B
sensing	I
method	O
.	O
23.6	O
bibliographic	O
remarks	O
339	O
23.6	O
bibliographic	O
remarks	O
pca	O
is	O
equivalent	O
to	O
best	O
subspace	O
approximation	O
using	O
singular	O
value	O
decom-	O
position	O
(	O
svd	O
)	O
.	O
the	O
svd	O
method	O
is	O
described	O
in	O
appendix	O
c.	O
svd	O
dates	O
back	O
to	O
eugenio	O
beltrami	O
(	O
1873	O
)	O
and	O
camille	O
jordan	O
(	O
1874	O
)	O
.	O
it	O
has	O
been	O
rediscovered	O
many	O
times	O
.	O
in	O
the	O
statistical	O
literature	O
,	O
it	O
was	O
introduced	O
by	O
pearson	O
(	O
1901	O
)	O
.	O
be-	O
sides	O
pca	O
and	O
svd	O
,	O
there	O
are	O
additional	O
names	O
that	O
refer	O
to	O
the	O
same	O
idea	O
and	O
are	O
being	O
used	O
in	O
diﬀerent	O
scientiﬁc	O
communities	O
.	O
a	O
few	O
examples	O
are	O
the	O
eckart-	O
young	O
theorem	O
(	O
after	O
carl	O
eckart	O
and	O
gale	O
young	O
who	O
analyzed	O
the	O
method	O
in	O
1936	O
)	O
,	O
the	O
schmidt-mirsky	O
theorem	O
,	O
factor	O
analysis	O
,	O
and	O
the	O
hotelling	O
transform	O
.	O
compressed	B
sensing	I
was	O
introduced	O
in	O
donoho	O
(	O
2006	O
)	O
and	O
in	O
(	O
candes	O
&	O
tao	O
2005	O
)	O
.	O
see	O
also	O
candes	O
(	O
2006	O
)	O
.	O
23.7	O
exercises	O
1.	O
in	O
this	O
exercise	O
we	O
show	O
that	O
in	O
the	O
general	O
case	O
,	O
exact	O
recovery	O
of	O
a	O
linear	O
compression	O
scheme	O
is	O
impossible	O
.	O
1.	O
let	O
a	O
∈	O
rn	O
,	O
d	O
be	O
an	O
arbitrary	O
compression	O
matrix	O
where	O
n	O
≤	O
d	O
−	O
1.	O
show	O
that	O
there	O
exists	O
u	O
,	O
v	O
∈	O
rn	O
,	O
u	O
(	O
cid:54	O
)	O
=	O
v	O
such	O
that	O
au	O
=	O
av	O
.	O
2.	O
conclude	O
that	O
exact	O
recovery	O
of	O
a	O
linear	O
compression	O
scheme	O
is	O
impossible	O
.	O
2.	O
let	O
α	O
∈	O
rd	O
such	O
that	O
α1	O
≥	O
α2	O
≥	O
···	O
≥	O
αd	O
≥	O
0.	O
show	O
that	O
d	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
max	O
β∈	O
[	O
0,1	O
]	O
d	O
:	O
(	O
cid:107	O
)	O
β	O
(	O
cid:107	O
)	O
1≤n	O
αjβj	O
=	O
αj	O
.	O
j=1	O
j=1	O
hint	O
:	O
take	O
every	O
vector	O
β	O
∈	O
[	O
0	O
,	O
1	O
]	O
d	O
such	O
that	O
(	O
cid:107	O
)	O
β	O
(	O
cid:107	O
)	O
1	O
≤	O
n.	O
let	O
i	O
be	O
the	O
minimal	O
index	O
for	O
which	O
βi	O
<	O
1.	O
if	O
i	O
=	O
n	O
+	O
1	O
we	O
are	O
done	O
.	O
otherwise	O
,	O
show	O
that	O
we	O
can	O
increase	O
βi	O
,	O
while	O
possibly	O
decreasing	O
βj	O
for	O
some	O
j	O
>	O
i	O
,	O
and	O
obtain	O
a	O
better	O
solution	O
.	O
this	O
will	O
imply	O
that	O
the	O
optimal	O
solution	O
is	O
to	O
set	B
βi	O
=	O
1	O
for	O
i	O
≤	O
n	O
and	O
βi	O
=	O
0	O
for	O
i	O
>	O
n.	O
3.	O
kernel	O
pca	O
:	O
in	O
this	O
exercise	O
we	O
show	O
how	O
pca	O
can	O
be	O
used	O
for	O
construct-	O
ing	O
nonlinear	O
dimensionality	B
reduction	I
on	O
the	O
basis	O
of	O
the	O
kernel	B
trick	I
(	O
see	O
chapter	O
16	O
)	O
.	O
let	O
x	O
be	O
some	O
instance	B
space	I
and	O
let	O
s	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
}	O
be	O
a	O
set	B
of	O
points	O
in	O
x	O
.	O
consider	O
a	O
feature	B
mapping	O
ψ	O
:	O
x	O
→	O
v	O
,	O
where	O
v	O
is	O
some	O
hilbert	O
space	O
(	O
possibly	O
of	O
inﬁnite	O
dimension	B
)	O
.	O
let	O
k	O
:	O
x	O
×	O
x	O
be	O
a	O
kernel	O
function	O
,	O
that	O
is	O
,	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
(	O
cid:104	O
)	O
ψ	O
(	O
x	O
)	O
,	O
ψ	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
.	O
kernel	O
pca	O
is	O
the	O
process	O
of	O
mapping	O
the	O
elements	O
in	O
s	O
into	O
v	O
using	O
ψ	O
,	O
and	O
then	O
applying	O
pca	O
over	O
{	O
ψ	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
ψ	O
(	O
xm	O
)	O
}	O
into	O
rn	O
.	O
the	O
output	O
of	O
this	O
process	O
is	O
the	O
set	B
of	O
reduced	O
elements	O
.	O
show	O
how	O
this	O
process	O
can	O
be	O
done	O
in	O
polynomial	O
time	O
in	O
terms	O
of	O
m	O
and	O
n	O
,	O
assuming	O
that	O
each	O
evaluation	O
of	O
k	O
(	O
·	O
,	O
·	O
)	O
can	O
be	O
calculated	O
in	O
a	O
con-	O
stant	O
time	O
.	O
in	O
particular	O
,	O
if	O
your	O
implementation	O
requires	O
multiplication	O
of	O
two	O
matrices	O
a	O
and	O
b	O
,	O
verify	O
that	O
their	O
product	O
can	O
be	O
computed	O
.	O
similarly	O
,	O
340	O
dimensionality	B
reduction	I
if	O
an	O
eigenvalue	O
decomposition	O
of	O
some	O
matrix	O
c	O
is	O
required	O
,	O
verify	O
that	O
this	O
decomposition	O
can	O
be	O
computed	O
.	O
4.	O
an	O
interpretation	O
of	O
pca	O
as	O
variance	O
maximization	O
:	O
let	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
be	O
m	O
vectors	O
in	O
rd	O
,	O
and	O
let	O
x	O
be	O
a	O
random	O
vector	O
distributed	O
according	O
to	O
the	O
uniform	O
distribution	O
over	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
.	O
assume	O
that	O
e	O
[	O
x	O
]	O
=	O
0	O
.	O
1.	O
consider	O
the	O
problem	O
of	O
ﬁnding	O
a	O
unit	O
vector	O
,	O
w	O
∈	O
rd	O
,	O
such	O
that	O
the	O
random	O
variable	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
has	O
maximal	O
variance	O
.	O
that	O
is	O
,	O
we	O
would	O
like	O
to	O
solve	O
the	O
problem	O
argmax	O
w	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
=1	O
var	O
[	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
]	O
=	O
argmax	O
w	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
=1	O
1	O
m	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
)	O
2.	O
m	O
(	O
cid:88	O
)	O
i=1	O
show	O
that	O
the	O
solution	O
of	O
the	O
problem	O
is	O
to	O
set	B
w	O
to	O
be	O
the	O
ﬁrst	O
principle	O
vector	O
of	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
.	O
2.	O
let	O
w1	O
be	O
the	O
ﬁrst	O
principal	O
component	O
as	O
in	O
the	O
previous	O
question	O
.	O
now	O
,	O
suppose	O
we	O
would	O
like	O
to	O
ﬁnd	O
a	O
second	O
unit	O
vector	O
,	O
w2	O
∈	O
rd	O
,	O
that	O
maxi-	O
mizes	O
the	O
variance	O
of	O
(	O
cid:104	O
)	O
w2	O
,	O
x	O
(	O
cid:105	O
)	O
,	O
but	O
is	O
also	O
uncorrelated	O
to	O
(	O
cid:104	O
)	O
w1	O
,	O
x	O
(	O
cid:105	O
)	O
.	O
that	O
is	O
,	O
we	O
would	O
like	O
to	O
solve	O
:	O
argmax	O
w	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
=1	O
,	O
e	O
[	O
(	O
(	O
cid:104	O
)	O
w1	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
]	O
=0	O
var	O
[	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
]	O
.	O
show	O
that	O
the	O
solution	O
to	O
this	O
problem	O
is	O
to	O
set	B
w	O
to	O
be	O
the	O
second	O
principal	O
component	O
of	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
.	O
hint	O
:	O
note	O
that	O
e	O
[	O
(	O
(	O
cid:104	O
)	O
w1	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
]	O
=	O
w	O
(	O
cid:62	O
)	O
1	O
e	O
[	O
xx	O
(	O
cid:62	O
)	O
]	O
w	O
=	O
mw	O
(	O
cid:62	O
)	O
1	O
aw	O
,	O
where	O
a	O
=	O
(	O
cid:80	O
)	O
i	O
xix	O
(	O
cid:62	O
)	O
constraint	O
e	O
[	O
(	O
(	O
cid:104	O
)	O
w1	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
]	O
=	O
0	O
is	O
equivalent	O
to	O
the	O
constraint	O
i	O
.	O
since	O
w	O
is	O
an	O
eigenvector	O
of	O
a	O
we	O
have	O
that	O
the	O
(	O
cid:104	O
)	O
w1	O
,	O
w	O
(	O
cid:105	O
)	O
=	O
0	O
.	O
5.	O
the	O
relation	O
between	O
svd	O
and	O
pca	O
:	O
use	O
the	O
svd	O
theorem	O
(	O
corol-	O
lary	O
c.6	O
)	O
for	O
providing	O
an	O
alternative	O
proof	O
of	O
theorem	O
23.2	O
.	O
6.	O
random	B
projections	I
preserve	O
inner	O
products	O
:	O
the	O
johnson-lindenstrauss	O
lemma	O
tells	O
us	O
that	O
a	O
random	O
projection	O
preserves	O
distances	O
between	O
a	O
ﬁnite	O
set	B
of	O
vectors	O
.	O
in	O
this	O
exercise	O
you	O
need	O
to	O
prove	O
that	O
if	O
the	O
set	B
of	O
vectors	O
are	O
within	O
the	O
unit	O
ball	O
,	O
then	O
not	O
only	O
are	O
the	O
distances	O
between	O
any	O
two	O
vectors	O
preserved	O
,	O
but	O
the	O
inner	O
product	O
is	O
also	O
preserved	O
.	O
let	O
q	O
be	O
a	O
ﬁnite	O
set	B
of	O
vectors	O
in	O
rd	O
and	O
assume	O
that	O
for	O
every	O
x	O
∈	O
q	O
we	O
have	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
≤	O
1	O
.	O
1.	O
let	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
and	O
n	O
be	O
an	O
integer	O
such	O
that	O
6	O
log	O
(	O
|q|2/δ	O
)	O
(	O
cid:114	O
)	O
	O
=	O
n	O
≤	O
3.	O
prove	O
that	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
a	O
choice	O
of	O
a	O
random	O
23.7	O
exercises	O
341	O
matrix	O
w	O
∈	O
rn	O
,	O
d	O
,	O
where	O
each	O
element	O
of	O
w	O
is	O
independently	O
distributed	O
according	O
to	O
n	O
(	O
0	O
,	O
1/n	O
)	O
,	O
we	O
have	O
|	O
(	O
cid:104	O
)	O
w	O
u	O
,	O
w	O
v	O
(	O
cid:105	O
)	O
−	O
(	O
cid:104	O
)	O
u	O
,	O
v	O
(	O
cid:105	O
)	O
|	O
≤	O
	O
for	O
every	O
u	O
,	O
v	O
∈	O
q.	O
hint	O
:	O
use	O
jl	O
to	O
bound	O
both	O
(	O
cid:107	O
)	O
w	O
(	O
u−v	O
)	O
(	O
cid:107	O
)	O
(	O
cid:107	O
)	O
u−v	O
(	O
cid:107	O
)	O
.	O
(	O
cid:107	O
)	O
w	O
(	O
u+v	O
)	O
(	O
cid:107	O
)	O
(	O
cid:107	O
)	O
u+v	O
(	O
cid:107	O
)	O
and	O
2	O
.	O
(	O
*	O
)	O
let	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
be	O
a	O
set	B
of	O
vectors	O
in	O
rd	O
of	O
norm	O
at	O
most	O
1	O
,	O
and	O
assume	O
that	O
these	O
vectors	O
are	O
linearly	O
separable	B
with	O
margin	B
of	O
γ.	O
assume	O
that	O
d	O
(	O
cid:29	O
)	O
1/γ2	O
.	O
show	O
that	O
there	O
exists	O
a	O
constant	O
c	O
>	O
0	O
such	O
that	O
if	O
we	O
randomly	O
project	O
these	O
vectors	O
into	O
rn	O
,	O
for	O
n	O
=	O
c/γ2	O
,	O
then	O
with	O
probability	O
of	O
at	O
least	O
99	O
%	O
it	O
holds	O
that	O
the	O
projected	O
vectors	O
are	O
linearly	O
separable	B
with	O
margin	B
γ/2	O
.	O
24	O
generative	B
models	I
we	O
started	O
this	O
book	O
with	O
a	O
distribution	B
free	I
learning	O
framework	O
;	O
namely	O
,	O
we	O
did	O
not	O
impose	O
any	O
assumptions	O
on	O
the	O
underlying	O
distribution	O
over	O
the	O
data	O
.	O
furthermore	O
,	O
we	O
followed	O
a	O
discriminative	B
approach	O
in	O
which	O
our	O
goal	O
is	O
not	O
to	O
learn	O
the	O
underlying	O
distribution	O
but	O
rather	O
to	O
learn	O
an	O
accurate	O
predictor	B
.	O
in	O
this	O
chapter	O
we	O
describe	O
a	O
generative	O
approach	O
,	O
in	O
which	O
it	O
is	O
assumed	O
that	O
the	O
underlying	O
distribution	O
over	O
the	O
data	O
has	O
a	O
speciﬁc	O
parametric	O
form	O
and	O
our	O
goal	O
is	O
to	O
estimate	O
the	O
parameters	O
of	O
the	O
model	O
.	O
this	O
task	O
is	O
called	O
parametric	B
density	I
estimation	I
.	O
the	O
discriminative	B
approach	O
has	O
the	O
advantage	O
of	O
directly	O
optimizing	O
the	O
quantity	O
of	O
interest	O
(	O
the	O
prediction	O
accuracy	B
)	O
instead	O
of	O
learning	O
the	O
underly-	O
ing	O
distribution	O
.	O
this	O
was	O
phrased	O
as	O
follows	O
by	O
vladimir	O
vapnik	O
in	O
his	O
principle	O
for	O
solving	O
problems	O
using	O
a	O
restricted	O
amount	O
of	O
information	O
:	O
when	O
solving	O
a	O
given	O
problem	O
,	O
try	O
to	O
avoid	O
a	O
more	O
general	O
problem	O
as	O
an	O
intermediate	O
step	O
.	O
of	O
course	O
,	O
if	O
we	O
succeed	O
in	O
learning	O
the	O
underlying	O
distribution	O
accurately	O
,	O
we	O
are	O
considered	O
to	O
be	O
“	O
experts	O
”	O
in	O
the	O
sense	O
that	O
we	O
can	O
predict	O
by	O
using	O
the	O
bayes	O
optimal	O
classiﬁer	B
.	O
the	O
problem	O
is	O
that	O
it	O
is	O
usually	O
more	O
diﬃcult	O
to	O
learn	O
the	O
underlying	O
distribution	O
than	O
to	O
learn	O
an	O
accurate	O
predictor	B
.	O
however	O
,	O
in	O
some	O
situations	O
,	O
it	O
is	O
reasonable	O
to	O
adopt	O
the	O
generative	O
learning	O
approach	O
.	O
for	O
example	O
,	O
sometimes	O
it	O
is	O
easier	O
(	O
computationally	O
)	O
to	O
estimate	O
the	O
parameters	O
of	O
the	O
model	O
than	O
to	O
learn	O
a	O
discriminative	B
predictor	O
.	O
additionally	O
,	O
in	O
some	O
cases	O
we	O
do	O
not	O
have	O
a	O
speciﬁc	O
task	O
at	O
hand	O
but	O
rather	O
would	O
like	O
to	O
model	O
the	O
data	O
either	O
for	O
making	O
predictions	O
at	O
a	O
later	O
time	O
without	O
having	O
to	O
retrain	O
a	O
predictor	B
or	O
for	O
the	O
sake	O
of	O
interpretability	O
of	O
the	O
data	O
.	O
we	O
start	O
with	O
a	O
popular	O
statistical	O
method	O
for	O
estimating	O
the	O
parameters	O
of	O
the	O
data	O
,	O
which	O
is	O
called	O
the	O
maximum	B
likelihood	I
principle	O
.	O
next	O
,	O
we	O
describe	O
two	O
generative	O
assumptions	O
which	O
greatly	O
simplify	O
the	O
learning	O
process	O
.	O
we	O
also	O
de-	O
scribe	O
the	O
em	O
algorithm	O
for	O
calculating	O
the	O
maximum	B
likelihood	I
in	O
the	O
presence	O
of	O
latent	B
variables	I
.	O
we	O
conclude	O
with	O
a	O
brief	O
description	O
of	O
bayesian	O
reasoning	O
.	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
24.1	O
maximum	B
likelihood	I
estimator	O
343	O
24.1	O
maximum	B
likelihood	I
estimator	O
let	O
us	O
start	O
with	O
a	O
simple	O
example	O
.	O
a	O
drug	O
company	O
developed	O
a	O
new	O
drug	O
to	O
treat	O
some	O
deadly	O
disease	O
.	O
we	O
would	O
like	O
to	O
estimate	O
the	O
probability	O
of	O
survival	O
when	O
using	O
the	O
drug	O
.	O
to	O
do	O
so	O
,	O
the	O
drug	O
company	O
sampled	O
a	O
training	B
set	I
of	O
m	O
people	O
and	O
gave	O
them	O
the	O
drug	O
.	O
let	O
s	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
denote	O
the	O
training	B
set	I
,	O
where	O
for	O
each	O
i	O
,	O
xi	O
=	O
1	O
if	O
the	O
ith	O
person	O
survived	O
and	O
xi	O
=	O
0	O
otherwise	O
.	O
we	O
can	O
model	O
the	O
underlying	O
distribution	O
using	O
a	O
single	O
parameter	O
,	O
θ	O
∈	O
[	O
0	O
,	O
1	O
]	O
,	O
indicating	O
the	O
probability	O
of	O
survival	O
.	O
we	O
now	O
would	O
like	O
to	O
estimate	O
the	O
parameter	O
θ	O
on	O
the	O
basis	O
of	O
the	O
training	B
set	I
s.	O
a	O
natural	O
idea	O
is	O
to	O
use	O
the	O
average	O
number	O
of	O
1	O
’	O
s	O
in	O
s	O
as	O
an	O
estimator	O
.	O
that	O
is	O
,	O
m	O
(	O
cid:88	O
)	O
i=1	O
(	O
cid:114	O
)	O
ˆθ	O
=	O
1	O
m	O
xi	O
.	O
(	O
24.1	O
)	O
clearly	O
,	O
es	O
[	O
ˆθ	O
]	O
=	O
θ.	O
that	O
is	O
,	O
ˆθ	O
is	O
an	O
unbiased	O
estimator	O
of	O
θ.	O
furthermore	O
,	O
since	O
ˆθ	O
is	O
the	O
average	O
of	O
m	O
i.i.d	O
.	O
binary	O
random	O
variables	O
we	O
can	O
use	O
hoeﬀding	O
’	O
s	O
inequality	O
to	O
get	O
that	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
the	O
choice	O
of	O
s	O
we	O
have	O
that	O
|ˆθ	O
−	O
θ|	O
≤	O
log	O
(	O
2/δ	O
)	O
2	O
m	O
.	O
(	O
24.2	O
)	O
another	O
interpretation	O
of	O
ˆθ	O
is	O
as	O
the	O
maximum	B
likelihood	I
estimator	O
,	O
as	O
we	O
formally	O
explain	O
now	O
.	O
we	O
ﬁrst	O
write	O
the	O
probability	O
of	O
generating	O
the	O
sample	O
s	O
:	O
p	O
[	O
s	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
]	O
=	O
θxi	O
(	O
1	O
−	O
θ	O
)	O
1−xi	O
=	O
θ	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
i	O
xi	O
(	O
1	O
−	O
θ	O
)	O
i	O
(	O
1−xi	O
)	O
.	O
m	O
(	O
cid:89	O
)	O
i=1	O
(	O
cid:88	O
)	O
we	O
deﬁne	O
the	O
log	O
likelihood	O
of	O
s	O
,	O
given	O
the	O
parameter	O
θ	O
,	O
as	O
the	O
log	O
of	O
the	O
preceding	O
expression	O
:	O
l	O
(	O
s	O
;	O
θ	O
)	O
=	O
log	O
(	O
p	O
[	O
s	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
]	O
)	O
=	O
log	O
(	O
θ	O
)	O
xi	O
+	O
log	O
(	O
1	O
−	O
θ	O
)	O
(	O
1	O
−	O
xi	O
)	O
.	O
i	O
i	O
the	O
maximum	B
likelihood	I
estimator	O
is	O
the	O
parameter	O
that	O
maximizes	O
the	O
likeli-	O
hood	O
ˆθ	O
∈	O
argmax	O
l	O
(	O
s	O
;	O
θ	O
)	O
.	O
θ	O
(	O
24.3	O
)	O
(	O
cid:88	O
)	O
next	O
,	O
we	O
show	O
that	O
in	O
our	O
case	O
,	O
equation	O
(	O
24.1	O
)	O
is	O
a	O
maximum	B
likelihood	I
esti-	O
mator	O
.	O
to	O
see	O
this	O
,	O
we	O
take	O
the	O
derivative	O
of	O
l	O
(	O
s	O
;	O
θ	O
)	O
with	O
respect	O
to	O
θ	O
and	O
equate	O
it	O
to	O
zero	O
:	O
(	O
cid:80	O
)	O
−	O
i	O
xi	O
θ	O
(	O
cid:80	O
)	O
i	O
(	O
1	O
−	O
xi	O
)	O
1	O
−	O
θ	O
=	O
0.	O
solving	O
the	O
equation	O
for	O
θ	O
we	O
obtain	O
the	O
estimator	O
given	O
in	O
equation	O
(	O
24.1	O
)	O
.	O
344	O
generative	B
models	I
24.1.1	O
maximum	B
likelihood	I
estimation	O
for	O
continuous	O
random	O
variables	O
let	O
x	O
be	O
a	O
continuous	O
random	O
variable	O
.	O
then	O
,	O
for	O
most	O
x	O
∈	O
r	O
we	O
have	O
p	O
[	O
x	O
=	O
x	O
]	O
=	O
0	O
and	O
therefore	O
the	O
deﬁnition	O
of	O
likelihood	O
as	O
given	O
before	O
is	O
trivialized	O
.	O
to	O
overcome	O
this	O
technical	O
problem	O
we	O
deﬁne	O
the	O
likelihood	O
as	O
log	O
of	O
the	O
density	O
of	O
the	O
probability	O
of	O
x	O
at	O
x.	O
that	O
is	O
,	O
given	O
an	O
i.i.d	O
.	O
training	B
set	I
s	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
sampled	O
according	O
to	O
a	O
density	O
distribution	O
pθ	O
we	O
deﬁne	O
the	O
likelihood	O
of	O
s	O
given	O
θ	O
as	O
(	O
cid:32	O
)	O
m	O
(	O
cid:89	O
)	O
m	O
(	O
cid:88	O
)	O
l	O
(	O
s	O
;	O
θ	O
)	O
=	O
log	O
pθ	O
(	O
xi	O
)	O
=	O
log	O
(	O
pθ	O
(	O
xi	O
)	O
)	O
.	O
as	O
before	O
,	O
the	O
maximum	B
likelihood	I
estimator	O
is	O
a	O
maximizer	O
of	O
l	O
(	O
s	O
;	O
θ	O
)	O
with	O
respect	O
to	O
θ.	O
i=1	O
i=1	O
as	O
an	O
example	O
,	O
consider	O
a	O
gaussian	O
random	O
variable	O
,	O
for	O
which	O
the	O
density	O
function	B
of	O
x	O
is	O
parameterized	O
by	O
θ	O
=	O
(	O
µ	O
,	O
σ	O
)	O
and	O
is	O
deﬁned	O
as	O
follows	O
:	O
(	O
cid:33	O
)	O
(	O
cid:18	O
)	O
pθ	O
(	O
x	O
)	O
=	O
√	O
1	O
2π	O
σ	O
exp	O
−	O
(	O
x	O
−	O
µ	O
)	O
2	O
2σ2	O
we	O
can	O
rewrite	O
the	O
likelihood	O
as	O
l	O
(	O
s	O
;	O
θ	O
)	O
=	O
−	O
1	O
2σ2	O
m	O
(	O
cid:88	O
)	O
i=1	O
(	O
xi	O
−	O
µ	O
)	O
2	O
−	O
m	O
log	O
(	O
σ	O
(	O
cid:19	O
)	O
.	O
√	O
2	O
π	O
)	O
.	O
to	O
ﬁnd	O
a	O
parameter	O
θ	O
=	O
(	O
µ	O
,	O
σ	O
)	O
that	O
optimizes	O
this	O
we	O
take	O
the	O
derivative	O
of	O
the	O
likelihood	O
w.r.t	O
.	O
µ	O
and	O
w.r.t	O
.	O
σ	O
and	O
compare	O
it	O
to	O
0.	O
we	O
obtain	O
the	O
following	O
two	O
equations	O
:	O
d	O
dµ	O
d	O
dσ	O
l	O
(	O
s	O
;	O
θ	O
)	O
=	O
l	O
(	O
s	O
;	O
θ	O
)	O
=	O
1	O
σ2	O
1	O
σ3	O
m	O
(	O
cid:88	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
i=1	O
(	O
xi	O
−	O
µ	O
)	O
=	O
0	O
(	O
xi	O
−	O
µ	O
)	O
2	O
−	O
m	O
σ	O
=	O
0	O
m	O
(	O
cid:88	O
)	O
i=1	O
ˆµ	O
=	O
1	O
m	O
xi	O
and	O
ˆσ	O
=	O
(	O
xi	O
−	O
ˆµ	O
)	O
2	O
(	O
cid:118	O
)	O
(	O
cid:117	O
)	O
(	O
cid:117	O
)	O
(	O
cid:116	O
)	O
1	O
m	O
m	O
(	O
cid:88	O
)	O
i=1	O
solving	O
the	O
preceding	O
equations	O
we	O
obtain	O
the	O
maximum	B
likelihood	I
estimates	O
:	O
note	O
that	O
the	O
maximum	B
likelihood	I
estimate	O
is	O
not	O
always	O
an	O
unbiased	O
estimator	O
.	O
for	O
example	O
,	O
while	O
ˆµ	O
is	O
unbiased	O
,	O
it	O
is	O
possible	O
to	O
show	O
that	O
the	O
estimate	O
ˆσ	O
of	O
the	O
variance	O
is	O
biased	O
(	O
exercise	O
1	O
)	O
.	O
simplifying	O
notation	O
to	O
simplify	O
our	O
notation	O
,	O
we	O
use	O
p	O
[	O
x	O
=	O
x	O
]	O
in	O
this	O
chapter	O
to	O
describe	O
both	O
the	O
probability	O
that	O
x	O
=	O
x	O
(	O
for	O
discrete	O
random	O
variables	O
)	O
and	O
the	O
density	O
of	O
the	O
distribution	O
at	O
x	O
(	O
for	O
continuous	O
variables	O
)	O
.	O
24.1	O
maximum	B
likelihood	I
estimator	O
345	O
24.1.2	O
maximum	B
likelihood	I
and	O
empirical	B
risk	I
minimization	O
the	O
maximum	B
likelihood	I
estimator	O
shares	O
some	O
similarity	O
with	O
the	O
empirical	B
risk	I
minimization	O
(	O
erm	O
)	O
principle	O
,	O
which	O
we	O
studied	O
extensively	O
in	O
previous	O
chapters	O
.	O
recall	B
that	O
in	O
the	O
erm	O
principle	O
we	O
have	O
a	O
hypothesis	B
class	I
h	O
and	O
we	O
use	O
the	O
training	B
set	I
for	O
choosing	O
a	O
hypothesis	B
h	O
∈	O
h	O
that	O
minimizes	O
the	O
empirical	B
risk	I
.	O
we	O
now	O
show	O
that	O
the	O
maximum	B
likelihood	I
estimator	O
is	O
an	O
erm	O
for	O
a	O
particular	O
loss	B
function	I
.	O
given	O
a	O
parameter	O
θ	O
and	O
an	O
observation	O
x	O
,	O
we	O
deﬁne	O
the	O
loss	B
of	O
θ	O
on	O
x	O
as	O
(	O
cid:96	O
)	O
(	O
θ	O
,	O
x	O
)	O
=	O
−	O
log	O
(	O
pθ	O
[	O
x	O
]	O
)	O
.	O
(	O
24.4	O
)	O
that	O
is	O
,	O
(	O
cid:96	O
)	O
(	O
θ	O
,	O
x	O
)	O
is	O
the	O
negation	O
of	O
the	O
log-likelihood	O
of	O
the	O
observation	O
x	O
,	O
assuming	O
the	O
data	O
is	O
distributed	O
according	O
to	O
pθ	O
.	O
this	O
loss	B
function	I
is	O
often	O
referred	O
to	O
as	O
the	O
log-loss	B
.	O
on	O
the	O
basis	O
of	O
this	O
deﬁnition	O
it	O
is	O
immediate	O
that	O
the	O
maximum	B
likelihood	I
principle	O
is	O
equivalent	O
to	O
minimizing	O
the	O
empirical	B
risk	I
with	O
respect	O
to	O
the	O
loss	B
function	I
given	O
in	O
equation	O
(	O
24.4	O
)	O
.	O
that	O
is	O
,	O
argmin	O
θ	O
(	O
−	O
log	O
(	O
pθ	O
[	O
xi	O
]	O
)	O
)	O
=	O
argmax	O
θ	O
log	O
(	O
pθ	O
[	O
xi	O
]	O
)	O
.	O
assuming	O
that	O
the	O
data	O
is	O
distributed	O
according	O
to	O
a	O
distribution	O
p	O
(	O
not	O
neces-	O
sarily	O
of	O
the	O
parametric	O
form	O
we	O
employ	O
)	O
,	O
the	O
true	O
risk	O
of	O
a	O
parameter	O
θ	O
becomes	O
m	O
(	O
cid:88	O
)	O
i=1	O
m	O
(	O
cid:88	O
)	O
i=1	O
e	O
x	O
[	O
(	O
cid:96	O
)	O
(	O
θ	O
,	O
x	O
)	O
]	O
=	O
−	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
=	O
x	O
x	O
p	O
[	O
x	O
]	O
log	O
(	O
pθ	O
[	O
x	O
]	O
)	O
(	O
cid:18	O
)	O
p	O
[	O
x	O
]	O
pθ	O
[	O
x	O
]	O
(	O
cid:19	O
)	O
(	O
cid:125	O
)	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
x	O
+	O
(	O
cid:18	O
)	O
1	O
p	O
[	O
x	O
]	O
p	O
[	O
x	O
]	O
log	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
h	O
(	O
p	O
)	O
(	O
cid:19	O
)	O
(	O
cid:125	O
)	O
,	O
(	O
24.5	O
)	O
p	O
[	O
x	O
]	O
log	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
dre	O
[	O
p||pθ	O
]	O
where	O
dre	O
is	O
called	O
the	O
relative	B
entropy	I
,	O
and	O
h	O
is	O
called	O
the	O
entropy	B
func-	O
tion	O
.	O
the	O
relative	B
entropy	I
is	O
a	O
divergence	O
measure	O
between	O
two	O
probabilities	O
.	O
for	O
discrete	O
variables	O
,	O
it	O
is	O
always	O
nonnegative	O
and	O
is	O
equal	O
to	O
0	O
only	O
if	O
the	O
two	O
distributions	O
are	O
the	O
same	O
.	O
it	O
follows	O
that	O
the	O
true	O
risk	O
is	O
minimal	O
when	O
pθ	O
=	O
p.	O
the	O
expression	O
given	O
in	O
equation	O
(	O
24.5	O
)	O
underscores	O
how	O
our	O
generative	O
as-	O
sumption	O
aﬀects	O
our	O
density	O
estimation	O
,	O
even	O
in	O
the	O
limit	O
of	O
inﬁnite	O
data	O
.	O
it	O
shows	O
that	O
if	O
the	O
underlying	O
distribution	O
is	O
indeed	O
of	O
a	O
parametric	O
form	O
,	O
then	O
by	O
choosing	O
the	O
correct	O
parameter	O
we	O
can	O
make	O
the	O
risk	B
be	O
the	O
entropy	B
of	O
the	O
distri-	O
bution	O
.	O
however	O
,	O
if	O
the	O
distribution	O
is	O
not	O
of	O
the	O
assumed	O
parametric	O
form	O
,	O
even	O
the	O
best	O
parameter	O
leads	O
to	O
an	O
inferior	O
model	O
and	O
the	O
suboptimality	O
is	O
measured	O
by	O
the	O
relative	B
entropy	I
divergence	O
.	O
24.1.3	O
generalization	O
analysis	O
how	O
good	O
is	O
the	O
maximum	B
likelihood	I
estimator	O
when	O
we	O
learn	O
from	O
a	O
ﬁnite	O
training	B
set	I
?	O
346	O
generative	B
models	I
to	O
answer	O
this	O
question	O
we	O
need	O
to	O
deﬁne	O
how	O
we	O
assess	O
the	O
quality	O
of	O
an	O
approxi-	O
mated	O
solution	O
of	O
the	O
density	O
estimation	O
problem	O
.	O
unlike	O
discriminative	B
learning	O
,	O
where	O
there	O
is	O
a	O
clear	O
notion	O
of	O
“	O
loss	B
,	O
”	O
in	O
generative	O
learning	O
there	O
are	O
various	O
ways	O
to	O
deﬁne	O
the	O
loss	B
of	O
a	O
model	O
.	O
on	O
the	O
basis	O
of	O
the	O
previous	O
subsection	O
,	O
one	O
natural	O
candidate	O
is	O
the	O
expected	O
log-loss	B
as	O
given	O
in	O
equation	O
(	O
24.5	O
)	O
.	O
in	O
some	O
situations	O
,	O
it	O
is	O
easy	O
to	O
prove	O
that	O
the	O
maximum	B
likelihood	I
principle	O
guarantees	O
low	O
true	O
risk	O
as	O
well	O
.	O
for	O
example	O
,	O
consider	O
the	O
problem	O
of	O
estimating	O
the	O
mean	O
of	O
a	O
gaussian	O
variable	O
of	O
unit	O
variance	O
.	O
we	O
saw	O
previously	O
that	O
the	O
maximum	B
likelihood	I
estimator	O
is	O
the	O
average	O
:	O
ˆµ	O
=	O
1	O
i	O
xi	O
.	O
let	O
µ	O
(	O
cid:63	O
)	O
be	O
the	O
optimal	O
m	O
parameter	O
.	O
then	O
,	O
(	O
cid:80	O
)	O
(	O
cid:18	O
)	O
pµ	O
(	O
cid:63	O
)	O
[	O
x	O
]	O
(	O
cid:19	O
)	O
e	O
x∼n	O
(	O
µ	O
(	O
cid:63	O
)	O
,1	O
)	O
[	O
(	O
cid:96	O
)	O
(	O
ˆµ	O
,	O
x	O
)	O
−	O
(	O
cid:96	O
)	O
(	O
µ	O
(	O
cid:63	O
)	O
,	O
x	O
)	O
]	O
=	O
e	O
x∼n	O
(	O
µ	O
(	O
cid:63	O
)	O
,1	O
)	O
(	O
cid:19	O
)	O
(	O
x	O
−	O
ˆµ	O
)	O
2	O
log	O
(	O
cid:18	O
)	O
pˆµ	O
[	O
x	O
]	O
(	O
x	O
−	O
µ	O
(	O
cid:63	O
)	O
)	O
2	O
+	O
−	O
1	O
2	O
+	O
(	O
µ	O
(	O
cid:63	O
)	O
−	O
ˆµ	O
)	O
1	O
2	O
e	O
x∼n	O
(	O
µ	O
(	O
cid:63	O
)	O
,1	O
)	O
[	O
x	O
]	O
+	O
(	O
µ	O
(	O
cid:63	O
)	O
−	O
ˆµ	O
)	O
µ	O
(	O
cid:63	O
)	O
(	O
24.6	O
)	O
=	O
=	O
=	O
=	O
e	O
x∼n	O
(	O
µ	O
(	O
cid:63	O
)	O
,1	O
)	O
ˆµ2	O
−	O
(	O
µ	O
(	O
cid:63	O
)	O
)	O
2	O
2	O
2	O
ˆµ2	O
−	O
(	O
µ	O
(	O
cid:63	O
)	O
)	O
2	O
2	O
2	O
(	O
ˆµ	O
−	O
µ	O
(	O
cid:63	O
)	O
)	O
2	O
.	O
1	O
2	O
next	O
,	O
we	O
note	O
that	O
ˆµ	O
is	O
the	O
average	O
of	O
m	O
gaussian	O
variables	O
and	O
therefore	O
it	O
is	O
also	O
distributed	O
normally	O
with	O
mean	O
µ	O
(	O
cid:63	O
)	O
and	O
variance	O
σ	O
(	O
cid:63	O
)	O
/m	O
.	O
from	O
this	O
fact	O
we	O
can	O
derive	O
bounds	O
of	O
the	O
form	O
:	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
we	O
have	O
that	O
|ˆµ	O
−	O
µ	O
(	O
cid:63	O
)	O
|	O
≤	O
	O
where	O
	O
depends	O
on	O
σ	O
(	O
cid:63	O
)	O
/m	O
and	O
on	O
δ.	O
in	O
some	O
situations	O
,	O
the	O
maximum	B
likelihood	I
estimator	O
clearly	O
overﬁts	O
.	O
for	O
example	O
,	O
consider	O
a	O
bernoulli	O
random	O
variable	O
x	O
and	O
let	O
p	O
[	O
x	O
=	O
1	O
]	O
=	O
θ	O
(	O
cid:63	O
)	O
.	O
as	O
we	O
saw	O
previously	O
,	O
using	O
hoeﬀding	O
’	O
s	O
inequality	O
we	O
can	O
easily	O
derive	O
a	O
guarantee	O
on	O
|θ	O
(	O
cid:63	O
)	O
−	O
ˆθ|	O
that	O
holds	O
with	O
high	O
probability	O
(	O
see	O
equation	O
(	O
24.2	O
)	O
)	O
.	O
however	O
,	O
if	O
our	O
goal	O
is	O
to	O
obtain	O
a	O
small	O
value	O
of	O
the	O
expected	O
log-loss	B
function	O
as	O
deﬁned	O
in	O
equation	O
(	O
24.5	O
)	O
we	O
might	O
fail	O
.	O
for	O
example	O
,	O
assume	O
that	O
θ	O
(	O
cid:63	O
)	O
is	O
nonzero	O
but	O
very	O
small	O
.	O
then	O
,	O
the	O
probability	O
that	O
no	O
element	O
of	O
a	O
sample	O
of	O
size	O
m	O
will	O
be	O
1	O
is	O
(	O
1	O
−	O
θ	O
(	O
cid:63	O
)	O
)	O
m	O
,	O
which	O
is	O
greater	O
than	O
e−2θ	O
(	O
cid:63	O
)	O
m.	O
it	O
follows	O
that	O
whenever	O
m	O
≤	O
log	O
(	O
2	O
)	O
2θ	O
(	O
cid:63	O
)	O
,	O
the	O
probability	O
that	O
the	O
sample	O
is	O
all	O
zeros	O
is	O
at	O
least	O
50	O
%	O
,	O
and	O
in	O
that	O
case	O
,	O
the	O
maximum	B
likelihood	I
rule	O
will	O
set	B
ˆθ	O
=	O
0.	O
but	O
the	O
true	O
risk	O
of	O
the	O
estimate	O
ˆθ	O
=	O
0	O
is	O
e	O
x∼θ	O
(	O
cid:63	O
)	O
[	O
(	O
cid:96	O
)	O
(	O
ˆθ	O
,	O
x	O
)	O
]	O
=	O
θ	O
(	O
cid:63	O
)	O
(	O
cid:96	O
)	O
(	O
ˆθ	O
,	O
1	O
)	O
+	O
(	O
1	O
−	O
θ	O
(	O
cid:63	O
)	O
)	O
(	O
cid:96	O
)	O
(	O
ˆθ	O
,	O
0	O
)	O
=	O
θ	O
(	O
cid:63	O
)	O
log	O
(	O
1/ˆθ	O
)	O
+	O
(	O
1	O
−	O
θ	O
(	O
cid:63	O
)	O
)	O
log	O
(	O
1/	O
(	O
1	O
−	O
ˆθ	O
)	O
)	O
=	O
θ	O
(	O
cid:63	O
)	O
log	O
(	O
1/0	O
)	O
=	O
∞	O
.	O
this	O
simple	O
example	O
shows	O
that	O
we	O
should	O
be	O
careful	O
in	O
applying	O
the	O
maximum	B
likelihood	I
principle	O
.	O
to	O
overcome	O
overﬁtting	B
,	O
we	O
can	O
use	O
the	O
variety	O
of	O
tools	O
we	O
encountered	O
pre-	O
24.2	O
naive	O
bayes	O
347	O
viously	O
in	O
the	O
book	O
.	O
a	O
simple	O
regularization	B
technique	O
is	O
outlined	O
in	O
exercise	O
2	O
.	O
24.2	O
naive	O
bayes	O
the	O
naive	O
bayes	O
classiﬁer	B
is	O
a	O
classical	O
demonstration	O
of	O
how	O
generative	O
as-	O
sumptions	O
and	O
parameter	O
estimations	O
simplify	O
the	O
learning	O
process	O
.	O
consider	O
the	O
problem	O
of	O
predicting	O
a	O
label	B
y	O
∈	O
{	O
0	O
,	O
1	O
}	O
on	O
the	O
basis	O
of	O
a	O
vector	O
of	O
features	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
,	O
where	O
we	O
assume	O
that	O
each	O
xi	O
is	O
in	O
{	O
0	O
,	O
1	O
}	O
.	O
recall	B
that	O
the	O
bayes	O
optimal	O
classiﬁer	B
is	O
hbayes	O
(	O
x	O
)	O
=	O
argmax	O
y∈	O
{	O
0,1	O
}	O
p	O
[	O
y	O
=	O
y|x	O
=	O
x	O
]	O
.	O
to	O
describe	O
the	O
probability	O
function	B
p	O
[	O
y	O
=	O
y|x	O
=	O
x	O
]	O
we	O
need	O
2d	O
parameters	O
,	O
each	O
of	O
which	O
corresponds	O
to	O
p	O
[	O
y	O
=	O
1|x	O
=	O
x	O
]	O
for	O
a	O
certain	O
value	O
of	O
x	O
∈	O
{	O
0	O
,	O
1	O
}	O
d.	O
this	O
implies	O
that	O
the	O
number	O
of	O
examples	O
we	O
need	O
grows	O
exponentially	O
with	O
the	O
number	O
of	O
features	O
.	O
in	O
the	O
naive	O
bayes	O
approach	O
we	O
make	O
the	O
(	O
rather	O
naive	O
)	O
generative	O
assumption	O
that	O
given	O
the	O
label	B
,	O
the	O
features	O
are	O
independent	O
of	O
each	O
other	O
.	O
that	O
is	O
,	O
d	O
(	O
cid:89	O
)	O
p	O
[	O
x	O
=	O
x|y	O
=	O
y	O
]	O
=	O
p	O
[	O
xi	O
=	O
xi|y	O
=	O
y	O
]	O
.	O
with	O
this	O
assumption	O
and	O
using	O
bayes	O
’	O
rule	O
,	O
the	O
bayes	O
optimal	O
classiﬁer	B
can	O
be	O
further	O
simpliﬁed	O
:	O
i=1	O
hbayes	O
(	O
x	O
)	O
=	O
argmax	O
y∈	O
{	O
0,1	O
}	O
=	O
argmax	O
y∈	O
{	O
0,1	O
}	O
p	O
[	O
y	O
=	O
y|x	O
=	O
x	O
]	O
p	O
[	O
y	O
=	O
y	O
]	O
p	O
[	O
x	O
=	O
x|y	O
=	O
y	O
]	O
=	O
argmax	O
y∈	O
{	O
0,1	O
}	O
p	O
[	O
y	O
=	O
y	O
]	O
p	O
[	O
xi	O
=	O
xi|y	O
=	O
y	O
]	O
.	O
(	O
24.7	O
)	O
d	O
(	O
cid:89	O
)	O
i=1	O
that	O
is	O
,	O
now	O
the	O
number	O
of	O
parameters	O
we	O
need	O
to	O
estimate	O
is	O
only	O
2d	O
+	O
1.	O
here	O
,	O
the	O
generative	O
assumption	O
we	O
made	O
reduced	O
signiﬁcantly	O
the	O
number	O
of	O
parameters	O
we	O
need	O
to	O
learn	O
.	O
when	O
we	O
also	O
estimate	O
the	O
parameters	O
using	O
the	O
maximum	B
likelihood	I
princi-	O
ple	O
,	O
the	O
resulting	O
classiﬁer	B
is	O
called	O
the	O
naive	O
bayes	O
classiﬁer	B
.	O
24.3	O
linear	B
discriminant	I
analysis	I
linear	O
discriminant	O
analysis	O
(	O
lda	O
)	O
is	O
another	O
demonstration	O
of	O
how	O
generative	O
assumptions	O
simplify	O
the	O
learning	O
process	O
.	O
as	O
in	O
the	O
naive	O
bayes	O
classiﬁer	B
we	O
consider	O
again	O
the	O
problem	O
of	O
predicting	O
a	O
label	B
y	O
∈	O
{	O
0	O
,	O
1	O
}	O
on	O
the	O
basis	O
of	O
a	O
348	O
generative	B
models	I
vector	O
of	O
features	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
.	O
but	O
now	O
the	O
generative	O
assumption	O
is	O
as	O
follows	O
.	O
first	O
,	O
we	O
assume	O
that	O
p	O
[	O
y	O
=	O
1	O
]	O
=	O
p	O
[	O
y	O
=	O
0	O
]	O
=	O
1/2	O
.	O
second	O
,	O
we	O
assume	O
that	O
the	O
conditional	O
probability	O
of	O
x	O
given	O
y	O
is	O
a	O
gaussian	O
distribution	O
.	O
finally	O
,	O
the	O
covariance	O
matrix	O
of	O
the	O
gaussian	O
distribution	O
is	O
the	O
same	O
for	O
both	O
values	O
of	O
the	O
label	B
.	O
formally	O
,	O
let	O
µ0	O
,	O
µ1	O
∈	O
rd	O
and	O
let	O
σ	O
be	O
a	O
covariance	O
matrix	O
.	O
then	O
,	O
the	O
density	O
distribution	O
is	O
given	O
by	O
p	O
[	O
x	O
=	O
x|y	O
=	O
y	O
]	O
=	O
1	O
(	O
2π	O
)	O
d/2|σ|1/2	O
exp	O
−	O
1	O
2	O
(	O
x	O
−	O
µy	O
)	O
t	O
σ−1	O
(	O
x	O
−	O
µy	O
)	O
(	O
cid:19	O
)	O
.	O
(	O
cid:18	O
)	O
as	O
we	O
have	O
shown	O
in	O
the	O
previous	O
section	O
,	O
using	O
bayes	O
’	O
rule	O
we	O
can	O
write	O
hbayes	O
(	O
x	O
)	O
=	O
argmax	O
y∈	O
{	O
0,1	O
}	O
p	O
[	O
y	O
=	O
y	O
]	O
p	O
[	O
x	O
=	O
x|y	O
=	O
y	O
]	O
.	O
this	O
means	O
that	O
we	O
will	O
predict	O
hbayes	O
(	O
x	O
)	O
=	O
1	O
iﬀ	O
(	O
cid:18	O
)	O
p	O
[	O
y	O
=	O
1	O
]	O
p	O
[	O
x	O
=	O
x|y	O
=	O
1	O
]	O
p	O
[	O
y	O
=	O
0	O
]	O
p	O
[	O
x	O
=	O
x|y	O
=	O
0	O
]	O
(	O
cid:19	O
)	O
>	O
0.	O
log	O
this	O
ratio	O
is	O
often	O
called	O
the	O
log-likelihood	O
ratio	O
.	O
in	O
our	O
case	O
,	O
the	O
log-likelihood	O
ratio	O
becomes	O
1	O
2	O
(	O
x	O
−	O
µ0	O
)	O
t	O
σ−1	O
(	O
x	O
−	O
µ0	O
)	O
−	O
1	O
2	O
(	O
x	O
−	O
µ1	O
)	O
t	O
σ−1	O
(	O
x	O
−	O
µ1	O
)	O
we	O
can	O
rewrite	O
this	O
as	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
+	O
b	O
where	O
w	O
=	O
(	O
µ1	O
−	O
µ0	O
)	O
t	O
σ−1	O
and	O
b	O
=	O
1	O
2	O
(	O
cid:0	O
)	O
µt	O
0	O
σ−1µ0	O
−	O
µt	O
1	O
σ−1µ1	O
(	O
24.8	O
)	O
(	O
cid:1	O
)	O
.	O
as	O
a	O
result	O
of	O
the	O
preceding	O
derivation	O
we	O
obtain	O
that	O
under	O
the	O
aforemen-	O
tioned	O
generative	O
assumptions	O
,	O
the	O
bayes	O
optimal	O
classiﬁer	B
is	O
a	O
linear	O
classiﬁer	O
.	O
additionally	O
,	O
one	O
may	O
train	O
the	O
classiﬁer	B
by	O
estimating	O
the	O
parameter	O
µ0	O
,	O
µ1	O
and	O
σ	O
from	O
the	O
data	O
,	O
using	O
,	O
for	O
example	O
,	O
the	O
maximum	B
likelihood	I
estimator	O
.	O
with	O
those	O
estimators	O
at	O
hand	O
,	O
the	O
values	O
of	O
w	O
and	O
b	O
can	O
be	O
calculated	O
as	O
in	O
equation	O
(	O
24.8	O
)	O
.	O
24.4	O
latent	B
variables	I
and	O
the	O
em	O
algorithm	O
in	O
generative	B
models	I
we	O
assume	O
that	O
the	O
data	O
is	O
generated	O
by	O
sampling	O
from	O
a	O
speciﬁc	O
parametric	O
distribution	O
over	O
our	O
instance	B
space	I
x	O
.	O
sometimes	O
,	O
it	O
is	O
convenient	O
to	O
express	O
this	O
distribution	O
using	O
latent	O
random	O
variables	O
.	O
a	O
natural	O
example	O
is	O
a	O
mixture	O
of	O
k	O
gaussian	O
distributions	O
.	O
that	O
is	O
,	O
x	O
=	O
rd	O
and	O
we	O
assume	O
that	O
each	O
x	O
is	O
generated	O
as	O
follows	O
.	O
first	O
,	O
we	O
choose	O
a	O
random	O
number	O
in	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
.	O
let	O
y	O
be	O
a	O
random	O
variable	O
corresponding	O
to	O
this	O
choice	O
,	O
and	O
denote	O
p	O
[	O
y	O
=	O
y	O
]	O
=	O
cy	O
.	O
second	O
,	O
we	O
choose	O
x	O
on	O
the	O
basis	O
of	O
the	O
value	O
of	O
y	O
according	O
to	O
a	O
gaussian	O
distribution	O
(	O
cid:18	O
)	O
(	O
cid:19	O
)	O
y	O
(	O
x	O
−	O
µy	O
)	O
.	O
(	O
24.9	O
)	O
p	O
[	O
x	O
=	O
x|y	O
=	O
y	O
]	O
=	O
1	O
(	O
2π	O
)	O
d/2|σy|1/2	O
exp	O
−	O
1	O
2	O
(	O
x	O
−	O
µy	O
)	O
t	O
σ−1	O
24.4	O
latent	B
variables	I
and	O
the	O
em	O
algorithm	O
349	O
therefore	O
,	O
the	O
density	O
of	O
x	O
can	O
be	O
written	O
as	O
:	O
p	O
[	O
x	O
=	O
x	O
]	O
=	O
=	O
k	O
(	O
cid:88	O
)	O
k	O
(	O
cid:88	O
)	O
y=1	O
y=1	O
p	O
[	O
y	O
=	O
y	O
]	O
p	O
[	O
x	O
=	O
x|y	O
=	O
y	O
]	O
(	O
cid:18	O
)	O
cy	O
1	O
(	O
2π	O
)	O
d/2|σy|1/2	O
exp	O
−	O
1	O
2	O
(	O
x	O
−	O
µy	O
)	O
t	O
σ−1	O
y	O
(	O
x	O
−	O
µy	O
)	O
(	O
cid:19	O
)	O
.	O
note	O
that	O
y	O
is	O
a	O
hidden	O
variable	O
that	O
we	O
do	O
not	O
observe	O
in	O
our	O
data	O
.	O
neverthe-	O
less	O
,	O
we	O
introduce	O
y	O
since	O
it	O
helps	O
us	O
describe	O
a	O
simple	O
parametric	O
form	O
of	O
the	O
probability	O
of	O
x.	O
more	O
generally	O
,	O
let	O
θ	O
be	O
the	O
parameters	O
of	O
the	O
joint	O
distribution	O
of	O
x	O
and	O
y	O
(	O
e.g.	O
,	O
in	O
the	O
preceding	O
example	O
,	O
θ	O
consists	O
of	O
cy	O
,	O
µy	O
,	O
and	O
σy	O
,	O
for	O
all	O
y	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
)	O
.	O
then	O
,	O
the	O
log-likelihood	O
of	O
an	O
observation	O
x	O
can	O
be	O
written	O
as	O
log	O
(	O
pθ	O
[	O
x	O
=	O
x	O
]	O
)	O
=	O
log	O
pθ	O
[	O
x	O
=	O
x	O
,	O
y	O
=	O
y	O
]	O
.	O
(	O
cid:32	O
)	O
k	O
(	O
cid:88	O
)	O
y=1	O
given	O
an	O
i.i.d	O
.	O
sample	O
,	O
s	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
,	O
we	O
would	O
like	O
to	O
ﬁnd	O
θ	O
that	O
maxi-	O
(	O
cid:33	O
)	O
(	O
cid:33	O
)	O
mizes	O
the	O
log-likelihood	O
of	O
s	O
,	O
l	O
(	O
θ	O
)	O
=	O
log	O
m	O
(	O
cid:88	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
=	O
=	O
m	O
(	O
cid:89	O
)	O
i=1	O
pθ	O
[	O
x	O
=	O
xi	O
]	O
log	O
pθ	O
[	O
x	O
=	O
xi	O
]	O
(	O
cid:32	O
)	O
k	O
(	O
cid:88	O
)	O
log	O
pθ	O
[	O
x	O
=	O
xi	O
,	O
y	O
=	O
y	O
]	O
.	O
i=1	O
y=1	O
the	O
maximum-likelihood	O
estimator	O
is	O
therefore	O
the	O
solution	O
of	O
the	O
maximization	O
problem	O
m	O
(	O
cid:88	O
)	O
(	O
cid:32	O
)	O
k	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
argmax	O
l	O
(	O
θ	O
)	O
=	O
argmax	O
log	O
θ	O
θ	O
i=1	O
y=1	O
pθ	O
[	O
x	O
=	O
xi	O
,	O
y	O
=	O
y	O
]	O
.	O
in	O
many	O
situations	O
,	O
the	O
summation	O
inside	O
the	O
log	O
makes	O
the	O
preceding	O
opti-	O
mization	O
problem	O
computationally	O
hard	O
.	O
the	O
expectation-maximization	O
(	O
em	O
)	O
algorithm	O
,	O
due	O
to	O
dempster	O
,	O
laird	O
,	O
and	O
rubin	O
,	O
is	O
an	O
iterative	O
procedure	O
for	O
searching	O
a	O
(	O
local	O
)	O
maximum	O
of	O
l	O
(	O
θ	O
)	O
.	O
while	O
em	O
is	O
not	O
guaranteed	O
to	O
ﬁnd	O
the	O
global	O
maximum	O
,	O
it	O
often	O
works	O
reasonably	O
well	O
in	O
practice	O
.	O
em	O
is	O
designed	O
for	O
those	O
cases	O
in	O
which	O
,	O
had	O
we	O
known	O
the	O
values	O
of	O
the	O
latent	B
variables	I
y	O
,	O
then	O
the	O
maximum	B
likelihood	I
optimization	O
problem	O
would	O
have	O
been	O
tractable	O
.	O
more	O
precisely	O
,	O
deﬁne	O
the	O
following	O
function	B
over	O
m	O
×	O
k	O
matrices	O
and	O
the	O
set	B
of	O
parameters	O
θ	O
:	O
f	O
(	O
q	O
,	O
θ	O
)	O
=	O
qi	O
,	O
y	O
log	O
(	O
pθ	O
[	O
x	O
=	O
xi	O
,	O
y	O
=	O
y	O
]	O
)	O
.	O
m	O
(	O
cid:88	O
)	O
k	O
(	O
cid:88	O
)	O
i=1	O
y=1	O
350	O
generative	B
models	I
if	O
each	O
row	O
of	O
q	O
deﬁnes	O
a	O
probability	O
over	O
the	O
ith	O
latent	O
variable	O
given	O
x	O
=	O
xi	O
,	O
then	O
we	O
can	O
interpret	O
f	O
(	O
q	O
,	O
θ	O
)	O
as	O
the	O
expected	O
log-likelihood	O
of	O
a	O
training	B
set	I
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
,	O
where	O
the	O
expectation	O
is	O
with	O
respect	O
to	O
the	O
choice	O
of	O
each	O
yi	O
on	O
the	O
basis	O
of	O
the	O
ith	O
row	O
of	O
q.	O
in	O
the	O
deﬁnition	O
of	O
f	O
,	O
the	O
summation	O
is	O
outside	O
the	O
log	O
,	O
and	O
we	O
assume	O
that	O
this	O
makes	O
the	O
optimization	O
problem	O
with	O
respect	O
to	O
θ	O
tractable	O
:	O
assumption	O
24.1	O
for	O
any	O
matrix	O
q	O
∈	O
[	O
0	O
,	O
1	O
]	O
m	O
,	O
k	O
,	O
such	O
that	O
each	O
row	O
of	O
q	O
sums	O
to	O
1	O
,	O
the	O
optimization	O
problem	O
is	O
tractable	O
.	O
argmax	O
f	O
(	O
q	O
,	O
θ	O
)	O
θ	O
the	O
intuitive	O
idea	O
of	O
em	O
is	O
that	O
we	O
have	O
a	O
“	O
chicken	O
and	O
egg	O
”	O
problem	O
.	O
on	O
one	O
hand	O
,	O
had	O
we	O
known	O
q	O
,	O
then	O
by	O
our	O
assumption	O
,	O
the	O
optimization	O
problem	O
of	O
ﬁnding	O
the	O
best	O
θ	O
is	O
tractable	O
.	O
on	O
the	O
other	O
hand	O
,	O
had	O
we	O
known	O
the	O
parameters	O
θ	O
we	O
could	O
have	O
set	B
qi	O
,	O
y	O
to	O
be	O
the	O
probability	O
of	O
y	O
=	O
y	O
given	O
that	O
x	O
=	O
xi	O
.	O
the	O
em	O
algorithm	O
therefore	O
alternates	O
between	O
ﬁnding	O
θ	O
given	O
q	O
and	O
ﬁnding	O
q	O
given	O
θ.	O
formally	O
,	O
em	O
ﬁnds	O
a	O
sequence	O
of	O
solutions	O
(	O
q	O
(	O
1	O
)	O
,	O
θ	O
(	O
1	O
)	O
)	O
,	O
(	O
q	O
(	O
2	O
)	O
,	O
θ	O
(	O
2	O
)	O
)	O
,	O
.	O
.	O
.	O
where	O
at	O
iteration	O
t	O
,	O
we	O
construct	O
(	O
q	O
(	O
t+1	O
)	O
,	O
θ	O
(	O
t+1	O
)	O
)	O
by	O
performing	O
two	O
steps	O
.	O
•	O
expectation	O
step	O
:	O
set	B
i	O
,	O
y	O
=	O
pθ	O
(	O
t	O
)	O
[	O
y	O
=	O
y|x	O
=	O
xi	O
]	O
.	O
q	O
(	O
t+1	O
)	O
(	O
24.10	O
)	O
this	O
step	O
is	O
called	O
the	O
expectation	O
step	O
,	O
because	O
it	O
yields	O
a	O
new	O
probabil-	O
ity	O
over	O
the	O
latent	B
variables	I
,	O
which	O
deﬁnes	O
a	O
new	O
expected	O
log-likelihood	O
function	B
over	O
θ	O
.	O
•	O
maximization	O
step	O
:	O
set	B
θ	O
(	O
t+1	O
)	O
to	O
be	O
the	O
maximizer	O
of	O
the	O
expected	O
log-	O
likelihood	O
,	O
where	O
the	O
expectation	O
is	O
according	O
to	O
q	O
(	O
t+1	O
)	O
:	O
θ	O
(	O
t+1	O
)	O
=	O
argmax	O
f	O
(	O
q	O
(	O
t+1	O
)	O
,	O
θ	O
)	O
.	O
(	O
24.11	O
)	O
θ	O
by	O
our	O
assumption	O
,	O
it	O
is	O
possible	O
to	O
solve	O
this	O
optimization	O
problem	O
eﬃ-	O
ciently	O
.	O
the	O
initial	O
values	O
of	O
θ	O
(	O
1	O
)	O
and	O
q	O
(	O
1	O
)	O
are	O
usually	O
chosen	O
at	O
random	O
and	O
the	O
procedure	O
terminates	O
after	O
the	O
improvement	O
in	O
the	O
likelihood	O
value	O
stops	O
being	O
signiﬁcant	O
.	O
24.4.1	O
em	O
as	O
an	O
alternate	O
maximization	O
algorithm	O
to	O
analyze	O
the	O
em	O
algorithm	O
,	O
we	O
ﬁrst	O
view	O
it	O
as	O
an	O
alternate	O
maximization	O
algorithm	O
.	O
deﬁne	O
the	O
following	O
objective	O
function	B
g	O
(	O
q	O
,	O
θ	O
)	O
=	O
f	O
(	O
q	O
,	O
θ	O
)	O
−	O
m	O
(	O
cid:88	O
)	O
k	O
(	O
cid:88	O
)	O
qi	O
,	O
y	O
log	O
(	O
qi	O
,	O
y	O
)	O
.	O
i=1	O
y=1	O
24.4	O
latent	B
variables	I
and	O
the	O
em	O
algorithm	O
351	O
the	O
second	O
term	O
is	O
the	O
sum	O
of	O
the	O
entropies	O
of	O
the	O
rows	O
of	O
q.	O
let	O
(	O
cid:40	O
)	O
q	O
=	O
q	O
∈	O
[	O
0	O
,	O
1	O
]	O
m	O
,	O
k	O
:	O
∀i	O
,	O
qi	O
,	O
y	O
=	O
1	O
(	O
cid:41	O
)	O
k	O
(	O
cid:88	O
)	O
y=1	O
be	O
the	O
set	B
of	O
matrices	O
whose	O
rows	O
deﬁne	O
probabilities	O
over	O
[	O
k	O
]	O
.	O
the	O
following	O
lemma	O
shows	O
that	O
em	O
performs	O
alternate	O
maximization	O
iterations	O
for	O
maximiz-	O
ing	O
g.	O
lemma	O
24.2	O
the	O
em	O
procedure	O
can	O
be	O
rewritten	O
as	O
:	O
q	O
(	O
t+1	O
)	O
=	O
argmax	O
q∈q	O
θ	O
(	O
t+1	O
)	O
=	O
argmax	O
g	O
(	O
q	O
,	O
θ	O
(	O
t	O
)	O
)	O
g	O
(	O
q	O
(	O
t+1	O
)	O
,	O
θ	O
)	O
.	O
θ	O
furthermore	O
,	O
g	O
(	O
q	O
(	O
t+1	O
)	O
,	O
θ	O
(	O
t	O
)	O
)	O
=	O
l	O
(	O
θ	O
(	O
t	O
)	O
)	O
.	O
proof	O
given	O
q	O
(	O
t+1	O
)	O
we	O
clearly	O
have	O
that	O
argmax	O
g	O
(	O
q	O
(	O
t+1	O
)	O
,	O
θ	O
)	O
=	O
argmax	O
f	O
(	O
q	O
(	O
t+1	O
)	O
,	O
θ	O
)	O
.	O
θ	O
θ	O
therefore	O
,	O
we	O
only	O
need	O
to	O
show	O
that	O
for	O
any	O
θ	O
,	O
the	O
solution	O
of	O
argmaxq∈q	O
g	O
(	O
q	O
,	O
θ	O
)	O
is	O
to	O
set	B
qi	O
,	O
y	O
=	O
pθ	O
[	O
y	O
=	O
y|x	O
=	O
xi	O
]	O
.	O
indeed	O
,	O
by	O
jensen	O
’	O
s	O
inequality	O
,	O
for	O
any	O
q	O
∈	O
q	O
we	O
have	O
that	O
g	O
(	O
q	O
,	O
θ	O
)	O
=	O
qi	O
,	O
y	O
log	O
(	O
cid:19	O
)	O
(	O
cid:33	O
)	O
(	O
cid:33	O
)	O
(	O
cid:33	O
)	O
(	O
cid:18	O
)	O
pθ	O
[	O
x	O
=	O
xi	O
,	O
y	O
=	O
y	O
]	O
pθ	O
[	O
x	O
=	O
xi	O
,	O
y	O
=	O
y	O
]	O
qi	O
,	O
y	O
qi	O
,	O
y	O
qi	O
,	O
y	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
k	O
(	O
cid:88	O
)	O
(	O
cid:32	O
)	O
y=1	O
log	O
(	O
cid:32	O
)	O
k	O
(	O
cid:88	O
)	O
(	O
cid:32	O
)	O
k	O
(	O
cid:88	O
)	O
y=1	O
i=1	O
m	O
(	O
cid:88	O
)	O
≤	O
m	O
(	O
cid:88	O
)	O
m	O
(	O
cid:88	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
i=1	O
=	O
i=1	O
=	O
log	O
pθ	O
[	O
x	O
=	O
xi	O
,	O
y	O
=	O
y	O
]	O
y=1	O
log	O
(	O
pθ	O
[	O
x	O
=	O
xi	O
]	O
)	O
=	O
l	O
(	O
θ	O
)	O
,	O
352	O
generative	B
models	I
while	O
for	O
qi	O
,	O
y	O
=	O
pθ	O
[	O
y	O
=	O
y|x	O
=	O
xi	O
]	O
we	O
have	O
pθ	O
[	O
y	O
=	O
y|x	O
=	O
xi	O
]	O
log	O
(	O
cid:18	O
)	O
pθ	O
[	O
x	O
=	O
xi	O
,	O
y	O
=	O
y	O
]	O
pθ	O
[	O
y	O
=	O
y|x	O
=	O
xi	O
]	O
(	O
cid:19	O
)	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
k	O
(	O
cid:88	O
)	O
k	O
(	O
cid:88	O
)	O
y=1	O
i=1	O
m	O
(	O
cid:88	O
)	O
m	O
(	O
cid:88	O
)	O
m	O
(	O
cid:88	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
i=1	O
g	O
(	O
q	O
,	O
θ	O
)	O
=	O
=	O
=	O
=	O
pθ	O
[	O
y	O
=	O
y|x	O
=	O
xi	O
]	O
log	O
(	O
pθ	O
[	O
x	O
=	O
xi	O
]	O
)	O
y=1	O
log	O
(	O
pθ	O
[	O
x	O
=	O
xi	O
]	O
)	O
k	O
(	O
cid:88	O
)	O
y=1	O
log	O
(	O
pθ	O
[	O
x	O
=	O
xi	O
]	O
)	O
=	O
l	O
(	O
θ	O
)	O
.	O
pθ	O
[	O
y	O
=	O
y|x	O
=	O
xi	O
]	O
i=1	O
this	O
shows	O
that	O
setting	O
qi	O
,	O
y	O
=	O
pθ	O
[	O
y	O
=	O
y|x	O
=	O
xi	O
]	O
maximizes	O
g	O
(	O
q	O
,	O
θ	O
)	O
over	O
q	O
∈	O
q	O
and	O
shows	O
that	O
g	O
(	O
q	O
(	O
t+1	O
)	O
,	O
θ	O
(	O
t	O
)	O
)	O
=	O
l	O
(	O
θ	O
(	O
t	O
)	O
)	O
.	O
the	O
preceding	O
lemma	O
immediately	O
implies	O
:	O
theorem	O
24.3	O
the	O
em	O
procedure	O
never	O
decreases	O
the	O
log-likelihood	O
;	O
namely	O
,	O
for	O
all	O
t	O
,	O
l	O
(	O
θ	O
(	O
t+1	O
)	O
)	O
≥	O
l	O
(	O
θ	O
(	O
t	O
)	O
)	O
.	O
proof	O
by	O
the	O
lemma	O
we	O
have	O
l	O
(	O
θ	O
(	O
t+1	O
)	O
)	O
=	O
g	O
(	O
q	O
(	O
t+2	O
)	O
,	O
θ	O
(	O
t+1	O
)	O
)	O
≥	O
g	O
(	O
q	O
(	O
t+1	O
)	O
,	O
θ	O
(	O
t	O
)	O
)	O
=	O
l	O
(	O
θ	O
(	O
t	O
)	O
)	O
.	O
24.4.2	O
em	O
for	O
mixture	O
of	O
gaussians	O
(	O
soft	B
k-means	I
)	O
consider	O
the	O
case	O
of	O
a	O
mixture	O
of	O
k	O
gaussians	O
in	O
which	O
θ	O
is	O
a	O
triplet	O
(	O
c	O
,	O
{	O
µ1	O
,	O
.	O
.	O
.	O
,	O
µk	O
}	O
,	O
{	O
σ1	O
,	O
.	O
.	O
.	O
,	O
σk	O
}	O
)	O
where	O
pθ	O
[	O
y	O
=	O
y	O
]	O
=	O
cy	O
and	O
pθ	O
[	O
x	O
=	O
x|y	O
=	O
y	O
]	O
is	O
as	O
given	O
in	O
equation	O
(	O
24.9	O
)	O
.	O
for	O
simplicity	O
,	O
we	O
assume	O
that	O
σ1	O
=	O
σ2	O
=	O
···	O
=	O
σk	O
=	O
i	O
,	O
where	O
i	O
is	O
the	O
identity	O
matrix	O
.	O
specifying	O
the	O
em	O
algorithm	O
for	O
this	O
case	O
we	O
obtain	O
the	O
following	O
:	O
•	O
expectation	O
step	O
:	O
for	O
each	O
i	O
∈	O
[	O
m	O
]	O
and	O
y	O
∈	O
[	O
k	O
]	O
we	O
have	O
that	O
pθ	O
(	O
t	O
)	O
[	O
y	O
=	O
y|x	O
=	O
xi	O
]	O
=	O
pθ	O
(	O
t	O
)	O
[	O
y	O
=	O
y	O
]	O
pθ	O
(	O
t	O
)	O
[	O
x	O
=	O
xi|y	O
=	O
y	O
]	O
=	O
where	O
zi	O
is	O
a	O
normalization	O
factor	O
which	O
ensures	O
that	O
(	O
cid:80	O
)	O
(	O
cid:107	O
)	O
xi	O
−	O
µ	O
(	O
t	O
)	O
y	O
(	O
cid:107	O
)	O
2	O
y	O
pθ	O
(	O
t	O
)	O
[	O
y	O
=	O
y|x	O
=	O
•	O
maximization	O
step	O
:	O
we	O
need	O
to	O
set	B
θt+1	O
to	O
be	O
a	O
maximizer	O
of	O
equation	O
(	O
24.11	O
)	O
,	O
xi	O
]	O
sums	O
to	O
1	O
.	O
−	O
1	O
2	O
(	O
24.12	O
)	O
c	O
(	O
t	O
)	O
y	O
exp	O
(	O
cid:18	O
)	O
1	O
zi	O
1	O
zi	O
(	O
cid:19	O
)	O
,	O
24.5	O
bayesian	O
reasoning	O
353	O
which	O
in	O
our	O
case	O
amounts	O
to	O
maximizing	O
the	O
following	O
expression	O
w.r.t	O
.	O
c	O
and	O
µ	O
:	O
(	O
cid:18	O
)	O
(	O
cid:19	O
)	O
pθ	O
(	O
t	O
)	O
[	O
y	O
=	O
y|x	O
=	O
xi	O
]	O
log	O
(	O
cy	O
)	O
−	O
1	O
2	O
(	O
cid:107	O
)	O
xi	O
−	O
µy	O
(	O
cid:107	O
)	O
2	O
.	O
(	O
24.13	O
)	O
m	O
(	O
cid:88	O
)	O
k	O
(	O
cid:88	O
)	O
i=1	O
y=1	O
comparing	O
the	O
derivative	O
of	O
equation	O
(	O
24.13	O
)	O
w.r.t	O
.	O
µy	O
to	O
zero	O
and	O
rear-	O
ranging	O
terms	O
we	O
obtain	O
:	O
(	O
cid:80	O
)	O
m	O
(	O
cid:80	O
)	O
m	O
i=1	O
pθ	O
(	O
t	O
)	O
[	O
y	O
=	O
y|x	O
=	O
xi	O
]	O
xi	O
i=1	O
pθ	O
(	O
t	O
)	O
[	O
y	O
=	O
y|x	O
=	O
xi	O
]	O
.	O
µy	O
=	O
that	O
is	O
,	O
µy	O
is	O
a	O
weighted	O
average	O
of	O
the	O
xi	O
where	O
the	O
weights	O
are	O
according	O
to	O
the	O
probabilities	O
calculated	O
in	O
the	O
e	O
step	O
.	O
to	O
ﬁnd	O
the	O
optimal	O
c	O
we	O
need	O
to	O
be	O
more	O
careful	O
since	O
we	O
must	O
ensure	O
that	O
c	O
is	O
a	O
probability	O
vector	O
.	O
in	O
exercise	O
3	O
we	O
show	O
that	O
the	O
solution	O
is	O
:	O
(	O
cid:80	O
)	O
m	O
(	O
cid:80	O
)	O
m	O
i=1	O
pθ	O
(	O
t	O
)	O
[	O
y	O
=	O
y|x	O
=	O
xi	O
]	O
i=1	O
pθ	O
(	O
t	O
)	O
[	O
y	O
=	O
y	O
(	O
cid:48	O
)	O
|x	O
=	O
xi	O
]	O
y	O
(	O
cid:48	O
)	O
=1	O
(	O
cid:80	O
)	O
k	O
cy	O
=	O
.	O
(	O
24.14	O
)	O
it	O
is	O
interesting	O
to	O
compare	O
the	O
preceding	O
algorithm	O
to	O
the	O
k-means	B
algorithm	O
described	O
in	O
chapter	O
22.	O
in	O
the	O
k-means	B
algorithm	O
,	O
we	O
ﬁrst	O
assign	O
each	O
example	O
to	O
a	O
cluster	O
according	O
to	O
the	O
distance	O
(	O
cid:107	O
)	O
xi	O
−	O
µy	O
(	O
cid:107	O
)	O
.	O
then	O
,	O
we	O
update	O
each	O
center	O
µy	O
according	O
to	O
the	O
average	O
of	O
the	O
examples	O
assigned	O
to	O
this	O
cluster	O
.	O
in	O
the	O
em	O
approach	O
,	O
however	O
,	O
we	O
determine	O
the	O
probability	O
that	O
each	O
example	O
belongs	O
to	O
each	O
cluster	O
.	O
then	O
,	O
we	O
update	O
the	O
centers	O
on	O
the	O
basis	O
of	O
a	O
weighted	O
sum	O
over	O
the	O
entire	O
sample	O
.	O
for	O
this	O
reason	O
,	O
the	O
em	O
approach	O
for	O
k-means	B
is	O
sometimes	O
called	O
“	O
soft	O
k-means.	O
”	O
24.5	O
bayesian	O
reasoning	O
the	O
maximum	B
likelihood	I
estimator	O
follows	O
a	O
frequentist	B
approach	O
.	O
this	O
means	O
that	O
we	O
refer	O
to	O
the	O
parameter	O
θ	O
as	O
a	O
ﬁxed	O
parameter	O
and	O
the	O
only	O
problem	O
is	O
that	O
we	O
do	O
not	O
know	O
its	O
value	O
.	O
a	O
diﬀerent	O
approach	O
to	O
parameter	O
estimation	O
is	O
called	O
bayesian	O
reasoning	O
.	O
in	O
the	O
bayesian	O
approach	O
,	O
our	O
uncertainty	O
about	O
θ	O
is	O
also	O
modeled	O
using	O
probability	O
theory	O
.	O
that	O
is	O
,	O
we	O
think	O
of	O
θ	O
as	O
a	O
random	O
variable	O
as	O
well	O
and	O
refer	O
to	O
the	O
distribution	O
p	O
[	O
θ	O
]	O
as	O
a	O
prior	O
distribution	O
.	O
as	O
its	O
name	O
indicates	O
,	O
the	O
prior	O
distribution	O
should	O
be	O
deﬁned	O
by	O
the	O
learner	O
prior	O
to	O
observing	O
the	O
data	O
.	O
as	O
an	O
example	O
,	O
let	O
us	O
consider	O
again	O
the	O
drug	O
company	O
which	O
developed	O
a	O
new	O
drug	O
.	O
on	O
the	O
basis	O
of	O
past	O
experience	O
,	O
the	O
statisticians	O
at	O
the	O
drug	O
company	O
believe	O
that	O
whenever	O
a	O
drug	O
has	O
reached	O
the	O
level	O
of	O
clinic	O
experiments	O
on	O
people	O
,	O
it	O
is	O
likely	O
to	O
be	O
eﬀective	O
.	O
they	O
model	O
this	O
prior	O
belief	O
by	O
deﬁning	O
a	O
density	O
distribution	O
on	O
θ	O
such	O
that	O
(	O
cid:40	O
)	O
p	O
[	O
θ	O
]	O
=	O
0.8	O
0.2	O
if	O
θ	O
>	O
0.5	O
if	O
θ	O
≤	O
0.5	O
(	O
24.15	O
)	O
354	O
generative	B
models	I
as	O
before	O
,	O
given	O
a	O
speciﬁc	O
value	O
of	O
θ	O
,	O
it	O
is	O
assumed	O
that	O
the	O
conditional	O
proba-	O
bility	O
,	O
p	O
[	O
x	O
=	O
x|θ	O
]	O
,	O
is	O
known	O
.	O
in	O
the	O
drug	O
company	O
example	O
,	O
x	O
takes	O
values	O
in	O
{	O
0	O
,	O
1	O
}	O
and	O
p	O
[	O
x	O
=	O
x|θ	O
]	O
=	O
θx	O
(	O
1	O
−	O
θ	O
)	O
1−x	O
.	O
once	O
the	O
prior	O
distribution	O
over	O
θ	O
and	O
the	O
conditional	O
distribution	O
over	O
x	O
given	O
θ	O
are	O
deﬁned	O
,	O
we	O
again	O
have	O
complete	O
knowledge	O
of	O
the	O
distribution	O
over	O
x.	O
this	O
is	O
because	O
we	O
can	O
write	O
the	O
probability	O
over	O
x	O
as	O
a	O
marginal	O
probability	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
p	O
[	O
x	O
=	O
x	O
]	O
=	O
p	O
[	O
x	O
=	O
x	O
,	O
θ	O
]	O
=	O
p	O
[	O
θ	O
]	O
p	O
[	O
x	O
=	O
x|θ	O
]	O
,	O
θ	O
θ	O
where	O
the	O
last	O
equality	O
follows	O
from	O
the	O
deﬁnition	O
of	O
conditional	O
probability	O
.	O
if	O
θ	O
is	O
continuous	O
we	O
replace	O
p	O
[	O
θ	O
]	O
with	O
the	O
density	O
function	B
and	O
the	O
sum	O
becomes	O
an	O
integral	O
:	O
(	O
cid:90	O
)	O
p	O
[	O
x	O
=	O
x	O
]	O
=	O
p	O
[	O
θ	O
]	O
p	O
[	O
x	O
=	O
x|θ	O
]	O
dθ	O
.	O
θ	O
seemingly	O
,	O
once	O
we	O
know	O
p	O
[	O
x	O
=	O
x	O
]	O
,	O
a	O
training	B
set	I
s	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
tells	O
us	O
nothing	O
as	O
we	O
are	O
already	O
experts	O
who	O
know	O
the	O
distribution	O
over	O
a	O
new	O
point	O
x.	O
however	O
,	O
the	O
bayesian	O
view	O
introduces	O
dependency	O
between	O
s	O
and	O
x.	O
this	O
is	O
because	O
we	O
now	O
refer	O
to	O
θ	O
as	O
a	O
random	O
variable	O
.	O
a	O
new	O
point	O
x	O
and	O
the	O
previous	O
points	O
in	O
s	O
are	O
independent	O
only	O
conditioned	O
on	O
θ.	O
this	O
is	O
diﬀerent	O
from	O
the	O
frequentist	B
philosophy	O
in	O
which	O
θ	O
is	O
a	O
parameter	O
that	O
we	O
might	O
not	O
know	O
,	O
but	O
since	O
it	O
is	O
just	O
a	O
parameter	O
of	O
the	O
distribution	O
,	O
a	O
new	O
point	O
x	O
and	O
previous	O
points	O
s	O
are	O
always	O
independent	O
.	O
in	O
the	O
bayesian	O
framework	O
,	O
since	O
x	O
and	O
s	O
are	O
not	O
independent	O
anymore	O
,	O
what	O
we	O
would	O
like	O
to	O
calculate	O
is	O
the	O
probability	O
of	O
x	O
given	O
s	O
,	O
which	O
by	O
the	O
chain	O
rule	O
can	O
be	O
written	O
as	O
follows	O
:	O
p	O
[	O
x	O
=	O
x|s	O
]	O
=	O
p	O
[	O
x	O
=	O
x|θ	O
,	O
s	O
]	O
p	O
[	O
θ|s	O
]	O
=	O
p	O
[	O
x	O
=	O
x|θ	O
]	O
p	O
[	O
θ|s	O
]	O
.	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
θ	O
θ	O
the	O
second	O
inequality	O
follows	O
from	O
the	O
assumption	O
that	O
x	O
and	O
s	O
are	O
independent	O
when	O
we	O
condition	O
on	O
θ.	O
using	O
bayes	O
’	O
rule	O
we	O
have	O
p	O
[	O
s|θ	O
]	O
p	O
[	O
θ	O
]	O
p	O
[	O
θ|s	O
]	O
=	O
p	O
[	O
s	O
]	O
,	O
and	O
together	O
with	O
the	O
assumption	O
that	O
points	O
are	O
independent	O
conditioned	O
on	O
θ	O
,	O
we	O
can	O
write	O
p	O
[	O
θ|s	O
]	O
=	O
p	O
[	O
s|θ	O
]	O
p	O
[	O
θ	O
]	O
p	O
[	O
s	O
]	O
=	O
1	O
p	O
[	O
s	O
]	O
p	O
[	O
x	O
=	O
xi|θ	O
]	O
p	O
[	O
θ	O
]	O
.	O
we	O
therefore	O
obtain	O
the	O
following	O
expression	O
for	O
bayesian	O
prediction	O
:	O
p	O
[	O
x	O
=	O
x|s	O
]	O
=	O
1	O
p	O
[	O
s	O
]	O
p	O
[	O
x	O
=	O
x|θ	O
]	O
p	O
[	O
x	O
=	O
xi|θ	O
]	O
p	O
[	O
θ	O
]	O
.	O
(	O
24.16	O
)	O
getting	O
back	O
to	O
our	O
drug	O
company	O
example	O
,	O
we	O
can	O
rewrite	O
p	O
[	O
x	O
=	O
x|s	O
]	O
as	O
θx+	O
(	O
cid:80	O
)	O
i	O
xi	O
(	O
1	O
−	O
θ	O
)	O
1−x+	O
(	O
cid:80	O
)	O
i	O
(	O
1−xi	O
)	O
p	O
[	O
θ	O
]	O
dθ	O
.	O
p	O
[	O
x	O
=	O
x|s	O
]	O
=	O
1	O
p	O
[	O
s	O
]	O
m	O
(	O
cid:89	O
)	O
m	O
(	O
cid:89	O
)	O
i=1	O
i=1	O
(	O
cid:88	O
)	O
(	O
cid:90	O
)	O
θ	O
24.6	O
summary	O
355	O
it	O
is	O
interesting	O
to	O
note	O
that	O
when	O
p	O
[	O
θ	O
]	O
is	O
uniform	O
we	O
obtain	O
that	O
p	O
[	O
x	O
=	O
x|s	O
]	O
∝	O
i	O
(	O
1−xi	O
)	O
dθ	O
.	O
solving	O
the	O
preceding	O
integral	O
(	O
using	O
integration	O
by	O
parts	O
)	O
we	O
obtain	O
(	O
cid:90	O
)	O
θx+	O
(	O
cid:80	O
)	O
i	O
xi	O
(	O
1	O
−	O
θ	O
)	O
1−x+	O
(	O
cid:80	O
)	O
(	O
(	O
cid:80	O
)	O
i	O
xi	O
)	O
+	O
1	O
m	O
+	O
2	O
.	O
p	O
[	O
x	O
=	O
1|s	O
]	O
=	O
(	O
cid:80	O
)	O
recall	B
that	O
the	O
prediction	O
according	O
to	O
the	O
maximum	B
likelihood	I
principle	O
in	O
this	O
case	O
is	O
p	O
[	O
x	O
=	O
1|ˆθ	O
]	O
=	O
i	O
xi	O
m	O
.	O
the	O
bayesian	O
prediction	O
with	O
uniform	O
prior	O
is	O
rather	O
similar	O
to	O
the	O
maximum	B
likelihood	I
prediction	O
,	O
except	O
it	O
adds	O
“	O
pseudoexamples	O
”	O
to	O
the	O
training	B
set	I
,	O
thus	O
biasing	O
the	O
prediction	O
toward	O
the	O
uniform	O
prior	O
.	O
maximum	O
a	O
posteriori	O
in	O
many	O
situations	O
,	O
it	O
is	O
diﬃcult	O
to	O
ﬁnd	O
a	O
closed	O
form	O
solution	O
to	O
the	O
integral	O
given	O
in	O
equation	O
(	O
24.16	O
)	O
.	O
several	O
numerical	O
methods	O
can	O
be	O
used	O
to	O
approxi-	O
mate	O
this	O
integral	O
.	O
another	O
popular	O
solution	O
is	O
to	O
ﬁnd	O
a	O
single	O
θ	O
which	O
maximizes	O
p	O
[	O
θ|s	O
]	O
.	O
the	O
value	O
of	O
θ	O
which	O
maximizes	O
p	O
[	O
θ|s	O
]	O
is	O
called	O
the	O
maximum	O
a	O
poste-	O
riori	O
estimator	O
.	O
once	O
this	O
value	O
is	O
found	O
,	O
we	O
can	O
calculate	O
the	O
probability	O
that	O
x	O
=	O
x	O
given	O
the	O
maximum	O
a	O
posteriori	O
estimator	O
and	O
independently	O
on	O
s.	O
24.6	O
summary	O
in	O
the	O
generative	O
approach	O
to	O
machine	O
learning	O
we	O
aim	O
at	O
modeling	O
the	O
distri-	O
bution	O
over	O
the	O
data	O
.	O
in	O
particular	O
,	O
in	O
parametric	B
density	I
estimation	I
we	O
further	O
assume	O
that	O
the	O
underlying	O
distribution	O
over	O
the	O
data	O
has	O
a	O
speciﬁc	O
paramet-	O
ric	O
form	O
and	O
our	O
goal	O
is	O
to	O
estimate	O
the	O
parameters	O
of	O
the	O
model	O
.	O
we	O
have	O
described	O
several	O
principles	O
for	O
parameter	O
estimation	O
,	O
including	O
maximum	O
like-	O
lihood	O
,	O
bayesian	O
estimation	O
,	O
and	O
maximum	O
a	O
posteriori	O
.	O
we	O
have	O
also	O
described	O
several	O
speciﬁc	O
algorithms	O
for	O
implementing	O
the	O
maximum	B
likelihood	I
under	O
dif-	O
ferent	O
assumptions	O
on	O
the	O
underlying	O
data	O
distribution	O
,	O
in	O
particular	O
,	O
naive	O
bayes	O
,	O
lda	O
,	O
and	O
em	O
.	O
24.7	O
bibliographic	O
remarks	O
the	O
maximum	B
likelihood	I
principle	O
was	O
studied	O
by	O
ronald	O
fisher	O
in	O
the	O
beginning	O
of	O
the	O
20th	O
century	O
.	O
bayesian	O
statistics	O
follow	O
bayes	O
’	O
rule	O
,	O
which	O
is	O
named	O
after	O
the	O
18th	O
century	O
english	O
mathematician	O
thomas	O
bayes	O
.	O
there	O
are	O
many	O
excellent	O
books	O
on	O
the	O
generative	O
and	O
bayesian	O
approaches	O
to	O
machine	O
learning	O
.	O
see	O
,	O
for	O
example	O
,	O
(	O
bishop	O
2006	O
,	O
koller	O
&	O
friedman	O
2009	O
,	O
mackay	O
2003	O
,	O
murphy	O
2012	O
,	O
barber	O
2012	O
)	O
.	O
356	O
generative	B
models	I
24.8	O
exercises	O
1.	O
prove	O
that	O
the	O
maximum	B
likelihood	I
estimator	O
of	O
the	O
variance	O
of	O
a	O
gaussian	O
variable	O
is	O
biased	O
.	O
2.	O
regularization	B
for	O
maximum	B
likelihood	I
:	O
consider	O
the	O
following	O
regularized	B
loss	I
minimization	I
:	O
m	O
(	O
cid:88	O
)	O
i=1	O
1	O
m	O
log	O
(	O
1/pθ	O
[	O
xi	O
]	O
)	O
+	O
1	O
m	O
(	O
log	O
(	O
1/θ	O
)	O
+	O
log	O
(	O
1/	O
(	O
1	O
−	O
θ	O
)	O
)	O
)	O
.	O
•	O
show	O
that	O
the	O
preceding	O
objective	O
is	O
equivalent	O
to	O
the	O
usual	O
empirical	B
error	I
had	O
we	O
added	O
two	O
pseudoexamples	O
to	O
the	O
training	B
set	I
.	O
conclude	O
that	O
the	O
regularized	O
maximum	O
likelihood	O
estimator	O
would	O
be	O
(	O
cid:32	O
)	O
(	O
cid:33	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
ˆθ	O
=	O
1	O
m	O
+	O
2	O
1	O
+	O
xi	O
.	O
•	O
derive	O
a	O
high	O
probability	O
bound	O
on	O
|ˆθ−θ	O
(	O
cid:63	O
)	O
|	O
.	O
hint	O
:	O
rewrite	O
this	O
as	O
|ˆθ−e	O
[	O
ˆθ	O
]	O
+	O
e	O
[	O
ˆθ	O
]	O
−	O
θ	O
(	O
cid:63	O
)	O
|	O
and	O
then	O
use	O
the	O
triangle	O
inequality	O
and	O
hoeﬀding	O
inequality	O
.	O
•	O
use	O
this	O
to	O
bound	O
the	O
true	O
risk	O
.	O
hint	O
:	O
use	O
the	O
fact	O
that	O
now	O
ˆθ	O
≥	O
1	O
m+2	O
to	O
relate	O
|ˆθ	O
−	O
θ	O
(	O
cid:63	O
)	O
|	O
to	O
the	O
relative	B
entropy	I
.	O
3	O
.	O
•	O
consider	O
a	O
general	O
optimization	O
problem	O
of	O
the	O
form	O
:	O
νy	O
log	O
(	O
cy	O
)	O
s.t	O
.	O
cy	O
>	O
0	O
,	O
cy	O
=	O
1	O
,	O
y=1	O
y	O
+	O
is	O
a	O
vector	O
of	O
nonnegative	O
weights	O
.	O
verify	O
that	O
the	O
m	O
step	O
where	O
ν	O
∈	O
rk	O
of	O
soft	B
k-means	I
involves	O
solving	O
such	O
an	O
optimization	O
problem	O
.	O
•	O
let	O
c	O
(	O
cid:63	O
)	O
=	O
1	O
(	O
cid:80	O
)	O
•	O
show	O
that	O
the	O
optimization	O
problem	O
is	O
equivalent	O
to	O
the	O
problem	O
:	O
ν.	O
show	O
that	O
c	O
(	O
cid:63	O
)	O
is	O
a	O
probability	O
vector	O
.	O
y	O
νy	O
dre	O
(	O
c	O
(	O
cid:63	O
)	O
||c	O
)	O
min	O
c	O
s.t	O
.	O
cy	O
>	O
0	O
,	O
cy	O
=	O
1	O
.	O
•	O
using	O
properties	O
of	O
the	O
relative	B
entropy	I
,	O
conclude	O
that	O
c	O
(	O
cid:63	O
)	O
is	O
the	O
solution	O
to	O
the	O
optimization	O
problem	O
.	O
k	O
(	O
cid:88	O
)	O
max	O
c	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
y	O
25	O
feature	B
selection	I
and	O
generation	O
in	O
the	O
beginning	O
of	O
the	O
book	O
,	O
we	O
discussed	O
the	O
abstract	O
model	O
of	O
learning	O
,	O
in	O
which	O
the	O
prior	B
knowledge	I
utilized	O
by	O
the	O
learner	O
is	O
fully	O
encoded	O
by	O
the	O
choice	O
of	O
the	O
hypothesis	B
class	I
.	O
however	O
,	O
there	O
is	O
another	O
modeling	O
choice	O
,	O
which	O
we	O
have	O
so	O
far	O
ignored	O
:	O
how	O
do	O
we	O
represent	O
the	O
instance	B
space	I
x	O
?	O
for	O
example	O
,	O
in	O
the	O
papayas	O
learning	O
problem	O
,	O
we	O
proposed	O
the	O
hypothesis	B
class	I
of	O
rectangles	O
in	O
the	O
softness-color	O
two	O
dimensional	O
plane	O
.	O
that	O
is	O
,	O
our	O
ﬁrst	O
modeling	O
choice	O
was	O
to	O
represent	O
a	O
papaya	O
as	O
a	O
two	O
dimensional	O
point	O
corresponding	O
to	O
its	O
softness	O
and	O
color	O
.	O
only	O
after	O
that	O
did	O
we	O
choose	O
the	O
hypothesis	B
class	I
of	O
rectangles	O
as	O
a	O
class	O
of	O
mappings	O
from	O
the	O
plane	O
into	O
the	O
label	B
set	O
.	O
the	O
transformation	O
from	O
the	O
real	O
world	O
object	O
“	O
papaya	O
”	O
into	O
the	O
scalar	O
representing	O
its	O
softness	O
or	O
its	O
color	O
is	O
called	O
a	O
feature	B
function	O
or	O
a	O
feature	B
for	O
short	O
;	O
namely	O
,	O
any	O
measurement	O
of	O
the	O
real	O
world	O
object	O
can	O
be	O
regarded	O
as	O
a	O
feature	B
.	O
if	O
x	O
is	O
a	O
subset	O
of	O
a	O
vector	O
space	O
,	O
each	O
x	O
∈	O
x	O
is	O
sometimes	O
referred	O
to	O
as	O
a	O
feature	B
vector	O
.	O
it	O
is	O
important	O
to	O
understand	O
that	O
the	O
way	O
we	O
encode	O
real	O
world	O
objects	O
as	O
an	O
instance	B
space	I
x	O
is	O
by	O
itself	O
prior	B
knowledge	I
about	O
the	O
problem	O
.	O
furthermore	O
,	O
even	O
when	O
we	O
already	O
have	O
an	O
instance	B
space	I
x	O
which	O
is	O
rep-	O
resented	O
as	O
a	O
subset	O
of	O
a	O
vector	O
space	O
,	O
we	O
might	O
still	O
want	O
to	O
change	O
it	O
into	O
a	O
diﬀerent	O
representation	O
and	O
apply	O
a	O
hypothesis	B
class	I
on	O
top	O
of	O
it	O
.	O
that	O
is	O
,	O
we	O
may	O
deﬁne	O
a	O
hypothesis	B
class	I
on	O
x	O
by	O
composing	O
some	O
class	O
h	O
on	O
top	O
of	O
a	O
feature	B
function	O
which	O
maps	O
x	O
into	O
some	O
other	O
vector	O
space	O
x	O
(	O
cid:48	O
)	O
.	O
we	O
have	O
al-	O
ready	O
encountered	O
examples	O
of	O
such	O
compositions	O
–	O
in	O
chapter	O
15	O
we	O
saw	O
that	O
kernel-based	O
svm	O
learns	O
a	O
composition	O
of	O
the	O
class	O
of	O
halfspaces	O
over	O
a	O
feature	B
mapping	O
ψ	O
that	O
maps	O
each	O
original	O
instance	B
in	O
x	O
into	O
some	O
hilbert	O
space	O
.	O
and	O
,	O
indeed	O
,	O
the	O
choice	O
of	O
ψ	O
is	O
another	O
form	O
of	O
prior	B
knowledge	I
we	O
impose	O
on	O
the	O
problem	O
.	O
in	O
this	O
chapter	O
we	O
study	O
several	O
methods	O
for	O
constructing	O
a	O
good	O
feature	B
set	O
.	O
we	O
start	O
with	O
the	O
problem	O
of	O
feature	B
selection	I
,	O
in	O
which	O
we	O
have	O
a	O
large	O
pool	O
of	O
features	O
and	O
our	O
goal	O
is	O
to	O
select	O
a	O
small	O
number	O
of	O
features	O
that	O
will	O
be	O
used	O
by	O
our	O
predictor	B
.	O
next	O
,	O
we	O
discuss	O
feature	B
manipulations	O
and	O
normalization	O
.	O
these	O
include	O
simple	O
transformations	O
that	O
we	O
apply	O
on	O
our	O
original	O
features	O
.	O
such	O
transformations	O
may	O
decrease	O
the	O
sample	B
complexity	I
of	O
our	O
learning	O
algorithm	O
,	O
its	O
bias	B
,	O
or	O
its	O
computational	B
complexity	I
.	O
last	O
,	O
we	O
discuss	O
several	O
approaches	O
for	O
feature	B
learning	I
.	O
in	O
these	O
methods	O
,	O
we	O
try	O
to	O
automate	O
the	O
process	O
of	O
feature	B
construction	O
.	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
358	O
feature	B
selection	I
and	O
generation	O
we	O
emphasize	O
that	O
while	O
there	O
are	O
some	O
common	O
techniques	O
for	O
feature	B
learn-	O
ing	O
one	O
may	O
want	O
to	O
try	O
,	O
the	O
no-free-lunch	B
theorem	O
implies	O
that	O
there	O
is	O
no	O
ulti-	O
mate	O
feature	B
learner	O
.	O
any	O
feature	B
learning	I
algorithm	O
might	O
fail	O
on	O
some	O
problem	O
.	O
in	O
other	O
words	O
,	O
the	O
success	O
of	O
each	O
feature	B
learner	O
relies	O
(	O
sometimes	O
implicitly	O
)	O
on	O
some	O
form	O
of	O
prior	O
assumption	O
on	O
the	O
data	O
distribution	O
.	O
furthermore	O
,	O
the	O
relative	O
quality	O
of	O
features	O
highly	O
depends	O
on	O
the	O
learning	O
algorithm	O
we	O
are	O
later	O
going	O
to	O
apply	O
using	O
these	O
features	O
.	O
this	O
is	O
illustrated	O
in	O
the	O
following	O
example	O
.	O
example	O
25.1	O
consider	O
a	O
regression	B
problem	O
in	O
which	O
x	O
=	O
r2	O
,	O
y	O
=	O
r	O
,	O
and	O
the	O
loss	B
function	I
is	O
the	O
squared	O
loss	B
.	O
suppose	O
that	O
the	O
underlying	O
distribution	O
is	O
such	O
that	O
an	O
example	O
(	O
x	O
,	O
y	O
)	O
is	O
generated	O
as	O
follows	O
:	O
first	O
,	O
we	O
sample	O
x1	O
from	O
the	O
uniform	O
distribution	O
over	O
[	O
−1	O
,	O
1	O
]	O
.	O
then	O
,	O
we	O
deterministically	O
set	B
y	O
=	O
x1	O
2.	O
finally	O
,	O
the	O
second	O
feature	B
is	O
set	B
to	O
be	O
x2	O
=	O
y	O
+	O
z	O
,	O
where	O
z	O
is	O
sampled	O
from	O
the	O
uniform	O
distribution	O
over	O
[	O
−0.01	O
,	O
0.01	O
]	O
.	O
suppose	O
we	O
would	O
like	O
to	O
choose	O
a	O
single	O
feature	O
.	O
intuitively	O
,	O
the	O
ﬁrst	O
feature	B
should	O
be	O
preferred	O
over	O
the	O
second	O
feature	B
as	O
the	O
target	O
can	O
be	O
perfectly	O
predicted	O
based	O
on	O
the	O
ﬁrst	O
feature	B
alone	O
,	O
while	O
it	O
can	O
not	O
be	O
perfectly	O
predicted	O
based	O
on	O
the	O
second	O
feature	B
.	O
indeed	O
,	O
choosing	O
the	O
ﬁrst	O
feature	B
would	O
be	O
the	O
right	O
choice	O
if	O
we	O
are	O
later	O
going	O
to	O
apply	O
polynomial	B
regression	I
of	O
degree	O
at	O
least	O
2.	O
however	O
,	O
if	O
the	O
learner	O
is	O
going	O
to	O
be	O
a	O
linear	O
regressor	O
,	O
then	O
we	O
should	O
prefer	O
the	O
second	O
feature	B
over	O
the	O
ﬁrst	O
one	O
,	O
since	O
the	O
optimal	O
linear	B
predictor	I
based	O
on	O
the	O
ﬁrst	O
feature	B
will	O
have	O
a	O
larger	O
risk	B
than	O
the	O
optimal	O
linear	B
predictor	I
based	O
on	O
the	O
second	O
feature	B
.	O
feature	B
selection	I
throughout	O
this	O
section	O
we	O
assume	O
that	O
x	O
=	O
rd	O
.	O
that	O
is	O
,	O
each	O
instance	B
is	O
repre-	O
sented	O
as	O
a	O
vector	O
of	O
d	O
features	O
.	O
our	O
goal	O
is	O
to	O
learn	O
a	O
predictor	B
that	O
only	O
relies	O
on	O
k	O
(	O
cid:28	O
)	O
d	O
features	O
.	O
predictors	O
that	O
use	O
only	O
a	O
small	O
subset	O
of	O
features	O
require	O
a	O
smaller	O
memory	O
footprint	O
and	O
can	O
be	O
applied	O
faster	O
.	O
furthermore	O
,	O
in	O
applications	O
such	O
as	O
medical	O
diagnostics	O
,	O
obtaining	O
each	O
possible	O
“	O
feature	B
”	O
(	O
e.g.	O
,	O
test	O
result	O
)	O
can	O
be	O
costly	O
;	O
therefore	O
,	O
a	O
predictor	B
that	O
uses	O
only	O
a	O
small	O
number	O
of	O
features	O
is	O
desirable	O
even	O
at	O
the	O
cost	O
of	O
a	O
small	O
degradation	O
in	O
performance	O
,	O
relative	O
to	O
a	O
predictor	B
that	O
uses	O
more	O
features	O
.	O
finally	O
,	O
constraining	O
the	O
hypothesis	B
class	I
to	O
use	O
a	O
small	O
subset	O
of	O
features	O
can	O
reduce	O
its	O
estimation	B
error	I
and	O
thus	O
prevent	O
overﬁtting	B
.	O
ideally	O
,	O
we	O
could	O
have	O
tried	O
all	O
subsets	O
of	O
k	O
out	O
of	O
d	O
features	O
and	O
choose	O
the	O
subset	O
which	O
leads	O
to	O
the	O
best	O
performing	O
predictor	B
.	O
however	O
,	O
such	O
an	O
exhaustive	O
search	O
is	O
usually	O
computationally	O
intractable	O
.	O
in	O
the	O
following	O
we	O
describe	O
three	O
computationally	O
feasible	B
approaches	O
for	O
feature	B
selection	I
.	O
while	O
these	O
methods	O
can	O
not	O
guarantee	O
ﬁnding	O
the	O
optimal	O
subset	O
,	O
they	O
often	O
work	O
reasonably	O
well	O
in	O
practice	O
.	O
some	O
of	O
the	O
methods	O
come	O
with	O
formal	O
guarantees	O
on	O
the	O
quality	O
of	O
the	O
selected	O
subsets	O
under	O
certain	O
assumptions	O
.	O
we	O
do	O
not	O
discuss	O
these	O
guarantees	O
here	O
.	O
25.1	O
25.1	O
feature	B
selection	I
359	O
25.1.1	O
filters	O
maybe	O
the	O
simplest	O
approach	O
for	O
feature	B
selection	I
is	O
the	O
ﬁlter	O
method	O
,	O
in	O
which	O
we	O
assess	O
individual	O
features	O
,	O
independently	O
of	O
other	O
features	O
,	O
according	O
to	O
some	O
quality	O
measure	O
.	O
we	O
can	O
then	O
select	O
the	O
k	O
features	O
that	O
achieve	O
the	O
highest	O
score	O
(	O
alternatively	O
,	O
decide	O
also	O
on	O
the	O
number	O
of	O
features	O
to	O
select	O
according	O
to	O
the	O
value	O
of	O
their	O
scores	O
)	O
.	O
many	O
quality	O
measures	O
for	O
features	O
have	O
been	O
proposed	O
in	O
the	O
literature	O
.	O
maybe	O
the	O
most	O
straightforward	O
approach	O
is	O
to	O
set	B
the	O
score	O
of	O
a	O
feature	B
ac-	O
cording	O
to	O
the	O
error	O
rate	O
of	O
a	O
predictor	B
that	O
is	O
trained	O
solely	O
by	O
that	O
feature	B
.	O
to	O
illustrate	O
this	O
,	O
consider	O
a	O
linear	B
regression	I
problem	O
with	O
the	O
squared	O
loss	B
.	O
let	O
v	O
=	O
(	O
x1	O
,	O
j	O
,	O
.	O
.	O
.	O
,	O
xm	O
,	O
j	O
)	O
∈	O
rm	O
be	O
a	O
vector	O
designating	O
the	O
values	O
of	O
the	O
jth	O
feature	B
on	O
a	O
training	B
set	I
of	O
m	O
examples	O
and	O
let	O
y	O
=	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
ym	O
)	O
∈	O
rm	O
be	O
the	O
values	O
of	O
the	O
target	O
on	O
the	O
same	O
m	O
examples	O
.	O
the	O
empirical	O
squared	O
loss	B
of	O
an	O
erm	O
linear	B
predictor	I
that	O
uses	O
only	O
the	O
jth	O
feature	B
would	O
be	O
min	O
a	O
,	O
b∈r	O
1	O
m	O
(	O
cid:107	O
)	O
av	O
+	O
b	O
−	O
y	O
(	O
cid:107	O
)	O
2	O
,	O
(	O
cid:80	O
)	O
m	O
where	O
the	O
meaning	O
of	O
adding	O
a	O
scalar	O
b	O
to	O
a	O
vector	O
v	O
is	O
adding	O
b	O
to	O
all	O
coordinates	O
of	O
v.	O
to	O
solve	O
this	O
problem	O
,	O
let	O
¯v	O
=	O
1	O
i=1	O
vi	O
be	O
the	O
averaged	O
value	O
of	O
the	O
m	O
feature	B
and	O
let	O
¯y	O
=	O
1	O
i=1	O
yi	O
be	O
the	O
averaged	O
value	O
of	O
the	O
target	O
.	O
clearly	O
(	O
see	O
m	O
exercise	O
1	O
)	O
,	O
(	O
cid:80	O
)	O
m	O
min	O
a	O
,	O
b∈r	O
1	O
m	O
(	O
cid:107	O
)	O
av	O
+	O
b	O
−	O
y	O
(	O
cid:107	O
)	O
2	O
=	O
min	O
a	O
,	O
b∈r	O
1	O
m	O
(	O
cid:107	O
)	O
a	O
(	O
v	O
−	O
¯v	O
)	O
+	O
b	O
−	O
(	O
y	O
−	O
¯y	O
)	O
(	O
cid:107	O
)	O
2	O
.	O
(	O
25.1	O
)	O
taking	O
the	O
derivative	O
of	O
the	O
right-hand	O
side	O
objective	O
with	O
respect	O
to	O
b	O
and	O
comparing	O
it	O
to	O
zero	O
we	O
obtain	O
that	O
b	O
=	O
0.	O
similarly	O
,	O
solving	O
for	O
a	O
(	O
once	O
we	O
know	O
that	O
b	O
=	O
0	O
)	O
yields	O
a	O
=	O
(	O
cid:104	O
)	O
v	O
−	O
¯v	O
,	O
y	O
−	O
¯y	O
(	O
cid:105	O
)	O
/	O
(	O
cid:107	O
)	O
v	O
−	O
¯v	O
(	O
cid:107	O
)	O
2.	O
plugging	O
this	O
value	O
back	O
into	O
the	O
objective	O
we	O
obtain	O
the	O
value	O
(	O
cid:107	O
)	O
y	O
−	O
¯y	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
(	O
cid:104	O
)	O
v	O
−	O
¯v	O
,	O
y	O
−	O
¯y	O
(	O
cid:105	O
)	O
)	O
2	O
(	O
cid:107	O
)	O
v	O
−	O
¯v	O
(	O
cid:107	O
)	O
2	O
.	O
ranking	B
the	O
features	O
according	O
to	O
the	O
minimal	O
loss	B
they	O
achieve	O
is	O
equivalent	O
to	O
ranking	B
them	O
according	O
to	O
the	O
absolute	O
value	O
of	O
the	O
following	O
score	O
(	O
where	O
now	O
a	O
higher	O
score	O
yields	O
a	O
better	O
feature	B
)	O
:	O
(	O
cid:104	O
)	O
v	O
−	O
¯v	O
,	O
y	O
−	O
¯y	O
(	O
cid:105	O
)	O
(	O
cid:107	O
)	O
v	O
−	O
¯v	O
(	O
cid:107	O
)	O
(	O
cid:107	O
)	O
y	O
−	O
¯y	O
(	O
cid:107	O
)	O
=	O
1	O
(	O
cid:113	O
)	O
1	O
m	O
(	O
cid:107	O
)	O
v	O
−	O
¯v	O
(	O
cid:107	O
)	O
2	O
(	O
cid:113	O
)	O
1	O
m	O
(	O
cid:104	O
)	O
v	O
−	O
¯v	O
,	O
y	O
−	O
¯y	O
(	O
cid:105	O
)	O
m	O
(	O
cid:107	O
)	O
y	O
−	O
¯y	O
(	O
cid:107	O
)	O
2	O
.	O
(	O
25.2	O
)	O
the	O
preceding	O
expression	O
is	O
known	O
as	O
pearson	O
’	O
s	O
correlation	O
coeﬃcient	O
.	O
the	O
nu-	O
merator	O
is	O
the	O
empirical	O
estimate	O
of	O
the	O
covariance	O
of	O
the	O
jth	O
feature	B
and	O
the	O
target	O
value	O
,	O
e	O
[	O
(	O
v	O
−	O
e	O
v	O
)	O
(	O
y	O
−	O
e	O
y	O
)	O
]	O
,	O
while	O
the	O
denominator	O
is	O
the	O
squared	O
root	O
of	O
the	O
empirical	O
estimate	O
for	O
the	O
variance	O
of	O
the	O
jth	O
feature	B
,	O
e	O
[	O
(	O
v	O
−	O
e	O
v	O
)	O
2	O
]	O
,	O
times	O
the	O
variance	O
of	O
the	O
target	O
.	O
pearson	O
’	O
s	O
coeﬃcient	O
ranges	O
from	O
−1	O
to	O
1	O
,	O
where	O
if	O
the	O
pearson	O
’	O
s	O
coeﬃcient	O
is	O
either	O
1	O
or	O
−1	O
,	O
there	O
is	O
a	O
linear	O
mapping	O
from	O
v	O
to	O
y	O
with	O
zero	O
empirical	B
risk	I
.	O
360	O
feature	B
selection	I
and	O
generation	O
if	O
pearson	O
’	O
s	O
coeﬃcient	O
equals	O
zero	O
it	O
means	O
that	O
the	O
optimal	O
linear	O
function	O
from	O
v	O
to	O
y	O
is	O
the	O
all-zeros	O
function	B
,	O
which	O
means	O
that	O
v	O
alone	O
is	O
useless	O
for	O
predicting	O
y.	O
however	O
,	O
this	O
does	O
not	O
mean	O
that	O
v	O
is	O
a	O
bad	O
feature	B
,	O
as	O
it	O
might	O
be	O
the	O
case	O
that	O
together	O
with	O
other	O
features	O
v	O
can	O
perfectly	O
predict	O
y.	O
indeed	O
,	O
consider	O
a	O
simple	O
example	O
in	O
which	O
the	O
target	O
is	O
generated	O
by	O
the	O
function	B
y	O
=	O
x1	O
+	O
2x2	O
.	O
assume	O
also	O
that	O
x1	O
is	O
generated	O
from	O
the	O
uniform	O
distribution	O
over	O
{	O
±1	O
}	O
,	O
and	O
x2	O
=	O
−	O
1	O
2	O
z	O
,	O
where	O
z	O
is	O
also	O
generated	O
i.i.d	O
.	O
from	O
the	O
uniform	O
distribution	O
over	O
{	O
±1	O
}	O
.	O
then	O
,	O
e	O
[	O
x1	O
]	O
=	O
e	O
[	O
x2	O
]	O
=	O
e	O
[	O
y	O
]	O
=	O
0	O
,	O
and	O
we	O
also	O
have	O
2	O
x1	O
+	O
1	O
e	O
[	O
yx1	O
]	O
=	O
e	O
[	O
x2	O
1	O
]	O
+	O
2	O
e	O
[	O
x2x1	O
]	O
=	O
e	O
[	O
x2	O
1	O
]	O
−	O
e	O
[	O
x2	O
1	O
]	O
+	O
e	O
[	O
zx1	O
]	O
=	O
0.	O
therefore	O
,	O
for	O
a	O
large	O
enough	O
training	B
set	I
,	O
the	O
ﬁrst	O
feature	B
is	O
likely	O
to	O
have	O
a	O
pearson	O
’	O
s	O
correlation	O
coeﬃcient	O
that	O
is	O
close	O
to	O
zero	O
,	O
and	O
hence	O
it	O
will	O
most	O
probably	O
not	O
be	O
selected	O
.	O
however	O
,	O
no	O
function	B
can	O
predict	O
the	O
target	O
value	O
well	O
without	O
knowing	O
the	O
ﬁrst	O
feature	B
.	O
there	O
are	O
many	O
other	O
score	O
functions	O
that	O
can	O
be	O
used	O
by	O
a	O
ﬁlter	O
method	O
.	O
notable	O
examples	O
are	O
estimators	O
of	O
the	O
mutual	O
information	O
or	O
the	O
area	O
under	O
the	O
receiver	O
operating	O
characteristic	O
(	O
roc	O
)	O
curve	O
.	O
all	O
of	O
these	O
score	O
functions	O
suﬀer	O
from	O
similar	O
problems	O
to	O
the	O
one	O
illustrated	O
previously	O
.	O
we	O
refer	O
the	O
reader	O
to	O
guyon	O
&	O
elisseeﬀ	O
(	O
2003	O
)	O
.	O
25.1.2	O
greedy	O
selection	O
approaches	O
greedy	O
selection	O
is	O
another	O
popular	O
approach	O
for	O
feature	B
selection	I
.	O
unlike	O
ﬁlter	O
methods	O
,	O
greedy	O
selection	O
approaches	O
are	O
coupled	O
with	O
the	O
underlying	O
learning	O
algorithm	O
.	O
the	O
simplest	O
instance	B
of	O
greedy	O
selection	O
is	O
forward	B
greedy	I
selection	I
.	O
we	O
start	O
with	O
an	O
empty	O
set	B
of	O
features	O
,	O
and	O
then	O
we	O
gradually	O
add	O
one	O
feature	B
at	O
a	O
time	O
to	O
the	O
set	B
of	O
selected	O
features	O
.	O
given	O
that	O
our	O
current	O
set	B
of	O
selected	O
features	O
is	O
i	O
,	O
we	O
go	O
over	O
all	O
i	O
/∈	O
i	O
,	O
and	O
apply	O
the	O
learning	O
algorithm	O
on	O
the	O
set	B
of	O
features	O
i	O
∪	O
{	O
i	O
}	O
.	O
each	O
such	O
application	O
yields	O
a	O
diﬀerent	O
predictor	B
,	O
and	O
we	O
choose	O
to	O
add	O
the	O
feature	B
that	O
yields	O
the	O
predictor	B
with	O
the	O
smallest	O
risk	B
(	O
on	O
the	O
training	B
set	I
or	O
on	O
a	O
validation	B
set	O
)	O
.	O
this	O
process	O
continues	O
until	O
we	O
either	O
select	O
k	O
features	O
,	O
where	O
k	O
is	O
a	O
predeﬁned	O
budget	O
of	O
allowed	O
features	O
,	O
or	O
achieve	O
an	O
accurate	O
enough	O
predictor	B
.	O
example	O
25.2	O
(	O
orthogonal	B
matching	I
pursuit	I
)	O
to	O
illustrate	O
the	O
forward	B
greedy	I
selection	I
approach	O
,	O
we	O
specify	O
it	O
to	O
the	O
problem	O
of	O
linear	B
regression	I
with	O
the	O
squared	O
loss	B
.	O
let	O
x	O
∈	O
rm	O
,	O
d	O
be	O
a	O
matrix	O
whose	O
rows	O
are	O
the	O
m	O
training	O
instances	O
.	O
let	O
y	O
∈	O
rm	O
be	O
the	O
vector	O
of	O
the	O
m	O
labels	O
.	O
for	O
every	O
i	O
∈	O
[	O
d	O
]	O
,	O
let	O
xi	O
be	O
the	O
ith	O
column	O
of	O
x.	O
given	O
a	O
set	B
i	O
⊂	O
[	O
d	O
]	O
we	O
denote	O
by	O
xi	O
the	O
matrix	O
whose	O
columns	O
are	O
{	O
xi	O
:	O
i	O
∈	O
i	O
}	O
.	O
the	O
forward	B
greedy	I
selection	I
method	O
starts	O
with	O
i0	O
=	O
∅	O
.	O
at	O
iteration	O
t	O
,	O
we	O
look	O
for	O
the	O
feature	B
index	O
jt	O
,	O
which	O
is	O
in	O
argmin	O
j	O
min	O
w∈rt	O
(	O
cid:107	O
)	O
xit−1∪	O
{	O
j	O
}	O
w	O
−	O
y	O
(	O
cid:107	O
)	O
2	O
.	O
25.1	O
feature	B
selection	I
361	O
then	O
,	O
we	O
update	O
it	O
=	O
it−1	O
∪	O
{	O
jt	O
}	O
.	O
we	O
now	O
describe	O
a	O
more	O
eﬃcient	O
implementation	O
of	O
the	O
forward	O
greedy	O
selec-	O
tion	O
approach	O
for	O
linear	B
regression	I
which	O
is	O
called	O
orthogonal	B
matching	I
pursuit	I
(	O
omp	O
)	O
.	O
the	O
idea	O
is	O
to	O
keep	O
an	O
orthogonal	O
basis	O
of	O
the	O
features	O
aggregated	O
so	O
far	O
.	O
let	O
vt	O
be	O
a	O
matrix	O
whose	O
columns	O
form	O
an	O
orthonormal	O
basis	O
of	O
the	O
columns	O
of	O
xit	O
.	O
clearly	O
,	O
(	O
cid:107	O
)	O
xitw	O
−	O
y	O
(	O
cid:107	O
)	O
2	O
=	O
min	O
θ∈rt	O
min	O
w	O
(	O
cid:107	O
)	O
vtθ	O
−	O
y	O
(	O
cid:107	O
)	O
2.	O
we	O
will	O
maintain	O
a	O
vector	O
θt	O
which	O
minimizes	O
the	O
right-hand	O
side	O
of	O
the	O
equation	O
.	O
initially	O
,	O
we	O
set	B
i0	O
=	O
∅	O
,	O
v0	O
=	O
∅	O
,	O
and	O
θ1	O
to	O
be	O
the	O
empty	O
vector	O
.	O
at	O
round	O
t	O
,	O
for	O
every	O
j	O
,	O
we	O
decompose	O
xj	O
=	O
vj	O
+	O
uj	O
where	O
vj	O
=	O
vt−1v	O
(	O
cid:62	O
)	O
t−1xj	O
is	O
the	O
projection	B
of	O
xj	O
onto	O
the	O
subspace	O
spanned	O
by	O
vt−1	O
and	O
uj	O
is	O
the	O
part	O
of	O
xj	O
orthogonal	O
to	O
vt−1	O
(	O
see	O
appendix	O
c	O
)	O
.	O
then	O
,	O
min	O
θ	O
,	O
α	O
=	O
min	O
θ	O
,	O
α	O
=	O
min	O
θ	O
,	O
α	O
(	O
cid:107	O
)	O
vt−1θ	O
+	O
αuj	O
−	O
y	O
(	O
cid:107	O
)	O
2	O
(	O
cid:2	O
)	O
(	O
cid:107	O
)	O
vt−1θ	O
−	O
y	O
(	O
cid:107	O
)	O
2	O
+	O
α2	O
(	O
cid:107	O
)	O
uj	O
(	O
cid:107	O
)	O
2	O
+	O
2α	O
(	O
cid:104	O
)	O
uj	O
,	O
vt−1θ	O
−	O
y	O
(	O
cid:105	O
)	O
(	O
cid:3	O
)	O
(	O
cid:2	O
)	O
(	O
cid:107	O
)	O
vt−1θ	O
−	O
y	O
(	O
cid:107	O
)	O
2	O
+	O
α2	O
(	O
cid:107	O
)	O
uj	O
(	O
cid:107	O
)	O
2	O
+	O
2α	O
(	O
cid:104	O
)	O
uj	O
,	O
−y	O
(	O
cid:105	O
)	O
(	O
cid:3	O
)	O
(	O
cid:2	O
)	O
α2	O
(	O
cid:107	O
)	O
uj	O
(	O
cid:107	O
)	O
2	O
−	O
2α	O
(	O
cid:104	O
)	O
uj	O
,	O
y	O
(	O
cid:105	O
)	O
(	O
cid:3	O
)	O
(	O
cid:2	O
)	O
(	O
cid:107	O
)	O
vt−1θ	O
−	O
y	O
(	O
cid:107	O
)	O
2	O
(	O
cid:3	O
)	O
+	O
min	O
=	O
(	O
cid:2	O
)	O
(	O
cid:107	O
)	O
vt−1θt−1	O
−	O
y	O
(	O
cid:107	O
)	O
2	O
(	O
cid:3	O
)	O
+	O
min	O
(	O
cid:2	O
)	O
α2	O
(	O
cid:107	O
)	O
uj	O
(	O
cid:107	O
)	O
2	O
−	O
2α	O
(	O
cid:104	O
)	O
uj	O
,	O
y	O
(	O
cid:105	O
)	O
(	O
cid:3	O
)	O
=	O
(	O
cid:107	O
)	O
vt−1θt−1	O
−	O
y	O
(	O
cid:107	O
)	O
2	O
−	O
(	O
(	O
cid:104	O
)	O
uj	O
,	O
y	O
(	O
cid:105	O
)	O
)	O
2	O
(	O
cid:107	O
)	O
uj	O
(	O
cid:107	O
)	O
2	O
=	O
min	O
θ	O
α	O
α	O
.	O
it	O
follows	O
that	O
we	O
should	O
select	O
the	O
feature	B
jt	O
=	O
argmax	O
j	O
(	O
(	O
cid:104	O
)	O
uj	O
,	O
y	O
(	O
cid:105	O
)	O
)	O
2	O
(	O
cid:107	O
)	O
uj	O
(	O
cid:107	O
)	O
2	O
.	O
the	O
rest	O
of	O
the	O
update	O
is	O
to	O
set	B
(	O
cid:20	O
)	O
vt	O
=	O
vt−1	O
,	O
ujt	O
(	O
cid:107	O
)	O
ujt	O
(	O
cid:107	O
)	O
2	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
θt−1	O
;	O
(	O
cid:21	O
)	O
.	O
(	O
cid:104	O
)	O
ujt	O
,	O
y	O
(	O
cid:105	O
)	O
(	O
cid:107	O
)	O
ujt	O
(	O
cid:107	O
)	O
2	O
,	O
θt	O
=	O
the	O
omp	O
procedure	O
maintains	O
an	O
orthonormal	O
basis	O
of	O
the	O
selected	O
features	O
,	O
where	O
in	O
the	O
preceding	O
description	O
,	O
the	O
orthonormalization	O
property	O
is	O
obtained	O
by	O
a	O
procedure	O
similar	O
to	O
gram-schmidt	O
orthonormalization	O
.	O
in	O
practice	O
,	O
the	O
gram-schmidt	O
procedure	O
is	O
often	O
numerically	O
unstable	O
.	O
in	O
the	O
pseudocode	O
that	O
follows	O
we	O
use	O
svd	O
(	O
see	O
section	O
c.4	O
)	O
at	O
the	O
end	O
of	O
each	O
round	O
to	O
obtain	O
an	O
orthonormal	O
basis	O
in	O
a	O
numerically	O
stable	O
manner	O
.	O
362	O
feature	B
selection	I
and	O
generation	O
orthogonal	B
matching	I
pursuit	I
(	O
omp	O
)	O
input	O
:	O
data	O
matrix	O
x	O
∈	O
rm	O
,	O
d	O
,	O
labels	O
vector	O
y	O
∈	O
rm	O
,	O
budget	O
of	O
features	O
t	O
initialize	O
:	O
i1	O
=	O
∅	O
for	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
use	O
svd	O
to	O
ﬁnd	O
an	O
orthonormal	O
basis	O
v	O
∈	O
rm	O
,	O
t−1	O
of	O
xit	O
(	O
for	O
t	O
=	O
1	O
set	B
v	O
to	O
be	O
the	O
all	O
zeros	O
matrix	O
)	O
foreach	O
j	O
∈	O
[	O
d	O
]	O
\	O
it	O
let	O
uj	O
=	O
xj	O
−	O
v	O
v	O
(	O
cid:62	O
)	O
xj	O
let	O
jt	O
=	O
argmaxj	O
/∈it	O
:	O
(	O
cid:107	O
)	O
uj	O
(	O
cid:107	O
)	O
>	O
0	O
update	O
it+1	O
=	O
it	O
∪	O
{	O
jt	O
}	O
(	O
(	O
cid:104	O
)	O
uj	O
,	O
y	O
(	O
cid:105	O
)	O
)	O
2	O
(	O
cid:107	O
)	O
uj	O
(	O
cid:107	O
)	O
2	O
output	O
it	O
+1	O
more	O
eﬃcient	O
greedy	O
selection	O
criteria	O
let	O
r	O
(	O
w	O
)	O
be	O
the	O
empirical	B
risk	I
of	O
a	O
vector	O
w.	O
at	O
each	O
round	O
of	O
the	O
forward	B
greedy	I
selection	I
method	O
,	O
and	O
for	O
every	O
possible	O
j	O
,	O
we	O
should	O
minimize	O
r	O
(	O
w	O
)	O
over	O
the	O
vectors	O
w	O
whose	O
support	O
is	O
it−1	O
∪	O
{	O
j	O
}	O
.	O
this	O
might	O
be	O
time	O
consuming	O
.	O
a	O
simpler	O
approach	O
is	O
to	O
choose	O
jt	O
that	O
minimizes	O
argmin	O
j	O
η∈r	O
r	O
(	O
wt−1	O
+	O
ηej	O
)	O
,	O
min	O
where	O
ej	O
is	O
the	O
all	O
zeros	O
vector	O
except	O
1	O
in	O
the	O
jth	O
element	O
.	O
that	O
is	O
,	O
we	O
keep	O
the	O
weights	O
of	O
the	O
previously	O
chosen	O
coordinates	O
intact	O
and	O
only	O
optimize	O
over	O
the	O
new	O
variable	O
.	O
therefore	O
,	O
for	O
each	O
j	O
we	O
need	O
to	O
solve	O
an	O
optimization	O
problem	O
over	O
a	O
single	O
variable	O
,	O
which	O
is	O
a	O
much	O
easier	O
task	O
than	O
optimizing	O
over	O
t.	O
an	O
even	O
simpler	O
approach	O
is	O
to	O
upper	O
bound	O
r	O
(	O
w	O
)	O
using	O
a	O
“	O
simple	O
”	O
function	B
and	O
then	O
choose	O
the	O
feature	B
which	O
leads	O
to	O
the	O
largest	O
decrease	O
in	O
this	O
upper	O
bound	O
.	O
for	O
example	O
,	O
if	O
r	O
is	O
a	O
β-smooth	O
function	B
(	O
see	O
equation	O
(	O
12.5	O
)	O
in	O
chap-	O
ter	O
12	O
)	O
,	O
then	O
r	O
(	O
w	O
+	O
ηej	O
)	O
≤	O
r	O
(	O
w	O
)	O
+	O
η	O
∂r	O
(	O
w	O
)	O
∂wj	O
+	O
βη2/2	O
.	O
minimizing	O
the	O
right-hand	O
side	O
over	O
η	O
yields	O
η	O
=	O
−	O
∂r	O
(	O
w	O
)	O
value	O
into	O
the	O
above	O
yields	O
∂wj	O
(	O
cid:18	O
)	O
∂r	O
(	O
w	O
)	O
(	O
cid:19	O
)	O
2	O
·	O
1	O
β	O
and	O
plugging	O
this	O
.	O
r	O
(	O
w	O
+	O
ηej	O
)	O
≤	O
r	O
(	O
w	O
)	O
−	O
1	O
2β	O
∂wj	O
this	O
value	O
is	O
minimized	O
if	O
the	O
partial	O
derivative	O
of	O
r	O
(	O
w	O
)	O
with	O
respect	O
to	O
wj	O
is	O
maximal	O
.	O
we	O
can	O
therefore	O
choose	O
jt	O
to	O
be	O
the	O
index	O
of	O
the	O
largest	O
coordinate	O
of	O
the	O
gradient	B
of	O
r	O
(	O
w	O
)	O
at	O
w.	O
remark	O
25.3	O
(	O
adaboost	O
as	O
a	O
forward	B
greedy	I
selection	I
procedure	O
)	O
it	O
is	O
pos-	O
sible	O
to	O
interpret	O
the	O
adaboost	O
algorithm	O
from	O
chapter	O
10	O
as	O
a	O
forward	O
greedy	O
25.1	O
feature	B
selection	I
363	O
selection	O
procedure	O
with	O
respect	O
to	O
the	O
function	B
	O
m	O
(	O
cid:88	O
)	O
exp	O
−yi	O
d	O
(	O
cid:88	O
)	O
	O
.	O
wjhj	O
(	O
xi	O
)	O
(	O
25.3	O
)	O
r	O
(	O
w	O
)	O
=	O
log	O
see	O
exercise	O
3.	O
i=1	O
j=1	O
backward	B
elimination	I
another	O
popular	O
greedy	O
selection	O
approach	O
is	O
backward	B
elimination	I
.	O
here	O
,	O
we	O
start	O
with	O
the	O
full	O
set	B
of	O
features	O
,	O
and	O
then	O
we	O
gradually	O
remove	O
one	O
feature	B
at	O
a	O
time	O
from	O
the	O
set	B
of	O
features	O
.	O
given	O
that	O
our	O
current	O
set	B
of	O
selected	O
features	O
is	O
i	O
,	O
we	O
go	O
over	O
all	O
i	O
∈	O
i	O
,	O
and	O
apply	O
the	O
learning	O
algorithm	O
on	O
the	O
set	B
of	O
features	O
i\	O
{	O
i	O
}	O
.	O
each	O
such	O
application	O
yields	O
a	O
diﬀerent	O
predictor	B
,	O
and	O
we	O
choose	O
to	O
remove	O
the	O
feature	B
i	O
for	O
which	O
the	O
predictor	B
obtained	O
from	O
i	O
\	O
{	O
i	O
}	O
has	O
the	O
smallest	O
risk	B
(	O
on	O
the	O
training	B
set	I
or	O
on	O
a	O
validation	B
set	O
)	O
.	O
naturally	O
,	O
there	O
are	O
many	O
possible	O
variants	O
of	O
the	O
backward	B
elimination	I
idea	O
.	O
it	O
is	O
also	O
possible	O
to	O
combine	O
forward	O
and	O
backward	O
greedy	O
steps	O
.	O
25.1.3	O
sparsity-inducing	B
norms	I
the	O
problem	O
of	O
minimizing	O
the	O
empirical	B
risk	I
subject	O
to	O
a	O
budget	O
of	O
k	O
features	O
can	O
be	O
written	O
as	O
min	O
w	O
ls	O
(	O
w	O
)	O
s.t	O
.	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
0	O
≤	O
k	O
,	O
where1	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
0	O
=	O
|	O
{	O
i	O
:	O
wi	O
(	O
cid:54	O
)	O
=	O
0	O
}	O
|	O
.	O
in	O
other	O
words	O
,	O
we	O
want	O
w	O
to	O
be	O
sparse	O
,	O
which	O
implies	O
that	O
we	O
only	O
need	O
to	O
measure	O
the	O
features	O
corresponding	O
to	O
nonzero	O
elements	O
of	O
w.	O
convex	B
function	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
0	O
with	O
the	O
(	O
cid:96	O
)	O
1	O
norm	O
,	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
1	O
=	O
(	O
cid:80	O
)	O
d	O
solving	O
this	O
optimization	O
problem	O
is	O
computationally	O
hard	O
(	O
natarajan	O
1995	O
,	O
davis	O
,	O
mallat	O
&	O
avellaneda	O
1997	O
)	O
.	O
a	O
possible	O
relaxation	O
is	O
to	O
replace	O
the	O
non-	O
i=1	O
|wi|	O
,	O
and	O
to	O
solve	O
the	O
problem	O
min	O
w	O
ls	O
(	O
w	O
)	O
s.t	O
.	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
1	O
≤	O
k1	O
,	O
(	O
25.4	O
)	O
where	O
k1	O
is	O
a	O
parameter	O
.	O
since	O
the	O
(	O
cid:96	O
)	O
1	O
norm	O
is	O
a	O
convex	B
function	O
,	O
this	O
problem	O
can	O
be	O
solved	O
eﬃciently	O
as	O
long	O
as	O
the	O
loss	B
function	I
is	O
convex	B
.	O
a	O
related	O
problem	O
is	O
minimizing	O
the	O
sum	O
of	O
ls	O
(	O
w	O
)	O
plus	O
an	O
(	O
cid:96	O
)	O
1	O
norm	O
regularization	B
term	O
,	O
(	O
ls	O
(	O
w	O
)	O
+	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
1	O
)	O
,	O
min	O
w	O
(	O
25.5	O
)	O
where	O
λ	O
is	O
a	O
regularization	B
parameter	O
.	O
since	O
for	O
any	O
k1	O
there	O
exists	O
a	O
λ	O
such	O
that	O
1	O
the	O
function	B
(	O
cid:107	O
)	O
·	O
(	O
cid:107	O
)	O
0	O
is	O
often	O
referred	O
to	O
as	O
the	O
(	O
cid:96	O
)	O
0	O
norm	O
.	O
despite	O
the	O
use	O
of	O
the	O
“	O
norm	O
”	O
notation	O
,	O
(	O
cid:107	O
)	O
·	O
(	O
cid:107	O
)	O
0	O
is	O
not	O
really	O
a	O
norm	O
;	O
for	O
example	O
,	O
it	O
does	O
not	O
satisfy	O
the	O
positive	O
homogeneity	O
property	O
of	O
norms	O
,	O
(	O
cid:107	O
)	O
aw	O
(	O
cid:107	O
)	O
0	O
(	O
cid:54	O
)	O
=	O
|a|	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
0	O
.	O
364	O
feature	B
selection	I
and	O
generation	O
equation	O
(	O
25.4	O
)	O
and	O
equation	O
(	O
25.5	O
)	O
lead	O
to	O
the	O
same	O
solution	O
,	O
the	O
two	O
problems	O
are	O
in	O
some	O
sense	O
equivalent	O
.	O
the	O
(	O
cid:96	O
)	O
1	O
regularization	B
often	O
induces	O
sparse	O
solutions	O
.	O
to	O
illustrate	O
this	O
,	O
let	O
us	O
start	O
with	O
the	O
simple	O
optimization	O
problem	O
(	O
cid:18	O
)	O
1	O
2	O
min	O
w∈r	O
(	O
cid:19	O
)	O
w2	O
−	O
xw	O
+	O
λ|w|	O
.	O
(	O
25.6	O
)	O
it	O
is	O
easy	O
to	O
verify	O
(	O
see	O
exercise	O
2	O
)	O
that	O
the	O
solution	O
to	O
this	O
problem	O
is	O
the	O
“	O
soft	O
thresholding	O
”	O
operator	O
w	O
=	O
sign	O
(	O
x	O
)	O
[	O
|x|	O
−	O
λ	O
]	O
+	O
,	O
(	O
25.7	O
)	O
def=	O
max	O
{	O
a	O
,	O
0	O
}	O
.	O
that	O
is	O
,	O
as	O
long	O
as	O
the	O
absolute	O
value	O
of	O
x	O
is	O
smaller	O
where	O
[	O
a	O
]	O
+	O
than	O
λ	O
,	O
the	O
optimal	O
solution	O
will	O
be	O
zero	O
.	O
next	O
,	O
consider	O
a	O
one	O
dimensional	O
regression	B
problem	O
with	O
respect	O
to	O
the	O
squared	O
loss	B
:	O
argmin	O
w∈rm	O
we	O
can	O
rewrite	O
the	O
problem	O
as	O
(	O
cid:32	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
i	O
1	O
m	O
x2	O
i	O
argmin	O
w∈rm	O
1	O
2	O
(	O
cid:32	O
)	O
1	O
2m	O
m	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
i=1	O
for	O
simplicity	O
let	O
us	O
assume	O
that	O
1	O
m	O
then	O
the	O
optimal	O
solution	O
is	O
(	O
xiw	O
−	O
yi	O
)	O
2	O
+	O
λ|w|	O
.	O
(	O
cid:33	O
)	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
m	O
(	O
cid:88	O
)	O
i	O
=	O
1	O
,	O
and	O
denote	O
(	O
cid:104	O
)	O
x	O
,	O
y	O
(	O
cid:105	O
)	O
=	O
(	O
cid:80	O
)	O
m	O
w	O
+	O
λ|w|	O
xiyi	O
1	O
m	O
i=1	O
.	O
w2	O
−	O
(	O
cid:80	O
)	O
i	O
x2	O
i=1	O
xiyi	O
;	O
(	O
cid:33	O
)	O
(	O
cid:33	O
)	O
w	O
=	O
sign	O
(	O
(	O
cid:104	O
)	O
x	O
,	O
y	O
(	O
cid:105	O
)	O
)	O
[	O
|	O
(	O
cid:104	O
)	O
x	O
,	O
y	O
(	O
cid:105	O
)	O
|/m	O
−	O
λ	O
]	O
+	O
.	O
that	O
is	O
,	O
the	O
solution	O
will	O
be	O
zero	O
unless	O
the	O
correlation	O
between	O
the	O
feature	B
x	O
and	O
the	O
labels	O
vector	O
y	O
is	O
larger	O
than	O
λ.	O
remark	O
25.4	O
unlike	O
the	O
(	O
cid:96	O
)	O
1	O
norm	O
,	O
the	O
(	O
cid:96	O
)	O
2	O
norm	O
does	O
not	O
induce	O
sparse	O
solutions	O
.	O
indeed	O
,	O
consider	O
the	O
problem	O
above	O
with	O
an	O
(	O
cid:96	O
)	O
2	O
regularization	B
,	O
namely	O
,	O
(	O
cid:32	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
1	O
2m	O
(	O
xiw	O
−	O
yi	O
)	O
2	O
+	O
λw2	O
.	O
argmin	O
w∈rm	O
then	O
,	O
the	O
optimal	O
solution	O
is	O
w	O
=	O
(	O
cid:104	O
)	O
x	O
,	O
y	O
(	O
cid:105	O
)	O
/m	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2/m	O
+	O
2λ	O
.	O
this	O
solution	O
will	O
be	O
nonzero	O
even	O
if	O
the	O
correlation	O
between	O
x	O
and	O
y	O
is	O
very	O
small	O
.	O
in	O
contrast	O
,	O
as	O
we	O
have	O
shown	O
before	O
,	O
when	O
using	O
(	O
cid:96	O
)	O
1	O
regularization	B
,	O
w	O
will	O
be	O
nonzero	O
only	O
if	O
the	O
correlation	O
between	O
x	O
and	O
y	O
is	O
larger	O
than	O
the	O
regularization	B
parameter	O
λ	O
.	O
25.2	O
feature	B
manipulation	O
and	O
normalization	O
365	O
adding	O
(	O
cid:96	O
)	O
1	O
regularization	B
to	O
a	O
linear	B
regression	I
problem	O
with	O
the	O
squared	O
loss	B
yields	O
the	O
lasso	O
algorithm	O
,	O
deﬁned	O
as	O
(	O
cid:19	O
)	O
(	O
cid:107	O
)	O
xw	O
−	O
y	O
(	O
cid:107	O
)	O
2	O
+	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
1	O
.	O
(	O
25.8	O
)	O
(	O
cid:18	O
)	O
1	O
argmin	O
w	O
2m	O
under	O
some	O
assumptions	O
on	O
the	O
distribution	O
and	O
the	O
regularization	B
parameter	O
λ	O
,	O
the	O
lasso	O
will	O
ﬁnd	O
sparse	O
solutions	O
(	O
see	O
,	O
for	O
example	O
,	O
(	O
zhao	O
&	O
yu	O
2006	O
)	O
and	O
the	O
references	O
therein	O
)	O
.	O
another	O
advantage	O
of	O
the	O
(	O
cid:96	O
)	O
1	O
norm	O
is	O
that	O
a	O
vector	O
with	O
low	O
(	O
cid:96	O
)	O
1	O
norm	O
can	O
be	O
“	O
sparsiﬁed	O
”	O
(	O
see	O
,	O
for	O
example	O
,	O
(	O
shalev-shwartz	O
,	O
zhang	O
&	O
srebro	O
2010	O
)	O
and	O
the	O
references	O
therein	O
)	O
.	O
25.2	O
feature	B
manipulation	O
and	O
normalization	O
feature	B
manipulations	O
or	O
normalization	O
include	O
simple	O
transformations	O
that	O
we	O
apply	O
on	O
each	O
of	O
our	O
original	O
features	O
.	O
such	O
transformations	O
may	O
decrease	O
the	O
approximation	O
or	O
estimation	O
errors	O
of	O
our	O
hypothesis	B
class	I
or	O
can	O
yield	O
a	O
faster	O
algorithm	O
.	O
similarly	O
to	O
the	O
problem	O
of	O
feature	B
selection	I
,	O
here	O
again	O
there	O
are	O
no	O
absolute	O
“	O
good	O
”	O
and	O
“	O
bad	O
”	O
transformations	O
,	O
but	O
rather	O
each	O
transformation	O
that	O
we	O
apply	O
should	O
be	O
related	O
to	O
the	O
learning	O
algorithm	O
we	O
are	O
going	O
to	O
apply	O
on	O
the	O
resulting	O
feature	B
vector	O
as	O
well	O
as	O
to	O
our	O
prior	O
assumptions	O
on	O
the	O
problem	O
.	O
to	O
motivate	O
normalization	O
,	O
consider	O
a	O
linear	B
regression	I
problem	O
with	O
the	O
squared	O
loss	B
.	O
let	O
x	O
∈	O
rm	O
,	O
d	O
be	O
a	O
matrix	O
whose	O
rows	O
are	O
the	O
instance	B
vectors	O
and	O
let	O
y	O
∈	O
rm	O
be	O
a	O
vector	O
of	O
target	O
values	O
.	O
recall	B
that	O
ridge	B
regression	I
returns	O
the	O
vector	O
(	O
cid:107	O
)	O
xw	O
−	O
y	O
(	O
cid:107	O
)	O
2	O
+	O
λ	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
2λmi	O
+	O
x	O
(	O
cid:62	O
)	O
x	O
)	O
−1x	O
(	O
cid:62	O
)	O
y	O
.	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
1	O
m	O
argmin	O
w	O
suppose	O
that	O
d	O
=	O
2	O
and	O
the	O
underlying	O
data	O
distribution	O
is	O
as	O
follows	O
.	O
first	O
we	O
sample	O
y	O
uniformly	O
at	O
random	O
from	O
{	O
±1	O
}	O
.	O
then	O
,	O
we	O
set	B
x1	O
to	O
be	O
y	O
+	O
0.5α	O
,	O
where	O
α	O
is	O
sampled	O
uniformly	O
at	O
random	O
from	O
{	O
±1	O
}	O
,	O
and	O
we	O
set	B
x2	O
to	O
be	O
0.0001y	O
.	O
note	O
that	O
the	O
optimal	O
weight	O
vector	O
is	O
w	O
(	O
cid:63	O
)	O
=	O
[	O
0	O
;	O
10000	O
]	O
,	O
and	O
ld	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
=	O
0.	O
however	O
,	O
the	O
objective	O
of	O
ridge	B
regression	I
at	O
w	O
(	O
cid:63	O
)	O
is	O
λ108	O
.	O
in	O
contrast	O
,	O
the	O
objective	O
of	O
ridge	B
regression	I
at	O
w	O
=	O
[	O
1	O
;	O
0	O
]	O
is	O
likely	O
to	O
be	O
close	O
to	O
0.25	O
+	O
λ.	O
it	O
follows	O
that	O
whenever	O
108−1	O
≈	O
0.25	O
×	O
10−8	O
,	O
the	O
objective	O
of	O
ridge	B
regression	I
is	O
smaller	O
at	O
the	O
λ	O
>	O
0.25	O
suboptimal	O
solution	O
w	O
=	O
[	O
1	O
;	O
0	O
]	O
.	O
since	O
λ	O
typically	O
should	O
be	O
at	O
least	O
1/m	O
(	O
see	O
the	O
analysis	O
in	O
chapter	O
13	O
)	O
,	O
it	O
follows	O
that	O
in	O
the	O
aforementioned	O
example	O
,	O
if	O
the	O
number	O
of	O
examples	O
is	O
smaller	O
than	O
108	O
then	O
we	O
are	O
likely	O
to	O
output	O
a	O
suboptimal	O
solution	O
.	O
the	O
crux	O
of	O
the	O
preceding	O
example	O
is	O
that	O
the	O
two	O
features	O
have	O
completely	O
diﬀerent	O
scales	O
.	O
feature	B
normalization	I
can	O
overcome	O
this	O
problem	O
.	O
there	O
are	O
many	O
ways	O
to	O
perform	O
feature	B
normalization	I
,	O
and	O
one	O
of	O
the	O
simplest	O
approaches	O
is	O
simply	O
to	O
make	O
sure	O
that	O
each	O
feature	B
receives	O
values	O
between	O
−1	O
and	O
1.	O
in	O
the	O
preceding	O
example	O
,	O
if	O
we	O
divide	O
each	O
feature	B
by	O
the	O
maximal	O
value	O
it	O
attains	O
366	O
feature	B
selection	I
and	O
generation	O
we	O
will	O
obtain	O
that	O
x1	O
=	O
y+0.5α	O
ridge	B
regression	I
is	O
quite	O
close	O
to	O
w	O
(	O
cid:63	O
)	O
.	O
1.5	O
and	O
x2	O
=	O
y.	O
then	O
,	O
for	O
λ	O
≤	O
10−3	O
the	O
solution	O
of	O
moreover	O
,	O
the	O
generalization	B
bounds	I
we	O
have	O
derived	O
in	O
chapter	O
13	O
for	O
reg-	O
ularized	O
loss	B
minimization	O
depend	O
on	O
the	O
norm	O
of	O
the	O
optimal	O
vector	O
w	O
(	O
cid:63	O
)	O
and	O
on	O
the	O
maximal	O
norm	O
of	O
the	O
instance	B
vectors.2	O
therefore	O
,	O
in	O
the	O
aforementioned	O
example	O
,	O
before	O
we	O
normalize	O
the	O
features	O
we	O
have	O
that	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
=	O
108	O
,	O
while	O
af-	O
ter	O
we	O
normalize	O
the	O
features	O
we	O
have	O
that	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
=	O
1.	O
the	O
maximal	O
norm	O
of	O
the	O
instance	B
vector	O
remains	O
roughly	O
the	O
same	O
;	O
hence	O
the	O
normalization	O
greatly	O
improves	O
the	O
estimation	B
error	I
.	O
feature	B
normalization	I
can	O
also	O
improve	O
the	O
runtime	O
of	O
the	O
learning	O
algorithm	O
.	O
for	O
example	O
,	O
in	O
section	O
14.5.3	O
we	O
have	O
shown	O
how	O
to	O
use	O
the	O
stochastic	O
gradient	B
descent	I
(	O
sgd	O
)	O
optimization	O
algorithm	O
for	O
solving	O
the	O
regularized	O
loss	O
minimiza-	O
tion	O
problem	O
.	O
the	O
number	O
of	O
iterations	O
required	O
by	O
sgd	O
to	O
converge	O
also	O
depends	O
on	O
the	O
norm	O
of	O
w	O
(	O
cid:63	O
)	O
and	O
on	O
the	O
maximal	O
norm	O
of	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
.	O
therefore	O
,	O
as	O
before	O
,	O
using	O
normalization	O
can	O
greatly	O
decrease	O
the	O
runtime	O
of	O
sgd	O
.	O
next	O
,	O
we	O
demonstrate	O
in	O
the	O
following	O
how	O
a	O
simple	O
transformation	O
on	O
features	O
,	O
such	O
as	O
clipping	O
,	O
can	O
sometime	O
decrease	O
the	O
approximation	B
error	I
of	O
our	O
hypoth-	O
esis	O
class	O
.	O
consider	O
again	O
linear	B
regression	I
with	O
the	O
squared	O
loss	B
.	O
let	O
a	O
>	O
1	O
be	O
a	O
large	O
number	O
,	O
suppose	O
that	O
the	O
target	O
y	O
is	O
chosen	O
uniformly	O
at	O
random	O
from	O
{	O
±1	O
}	O
,	O
and	O
then	O
the	O
single	O
feature	O
x	O
is	O
set	B
to	O
be	O
y	O
with	O
probability	O
(	O
1	O
−	O
1/a	O
)	O
and	O
set	B
to	O
be	O
ay	O
with	O
probability	O
1/a	O
.	O
that	O
is	O
,	O
most	O
of	O
the	O
time	O
our	O
feature	B
is	O
bounded	O
but	O
with	O
a	O
very	O
small	O
probability	O
it	O
gets	O
a	O
very	O
high	O
value	O
.	O
then	O
,	O
for	O
any	O
w	O
,	O
the	O
expected	O
squared	O
loss	B
of	O
w	O
is	O
(	O
wx	O
−	O
y	O
)	O
2	O
ld	O
(	O
w	O
)	O
=	O
e	O
1	O
2	O
1	O
−	O
1	O
a	O
(	O
wy	O
−	O
y	O
)	O
2	O
+	O
(	O
awy	O
−	O
y	O
)	O
2	O
.	O
(	O
cid:19	O
)	O
1	O
(	O
cid:18	O
)	O
1	O
a	O
1	O
2	O
=	O
2	O
solving	O
for	O
w	O
we	O
obtain	O
that	O
w	O
(	O
cid:63	O
)	O
=	O
2a−1	O
a2+a−1	O
,	O
which	O
goes	O
to	O
zero	O
as	O
a	O
goes	O
to	O
inﬁn-	O
ity	O
.	O
therefore	O
,	O
the	O
objective	O
at	O
w	O
(	O
cid:63	O
)	O
goes	O
to	O
0.5	O
as	O
a	O
goes	O
to	O
inﬁnity	O
.	O
for	O
example	O
,	O
for	O
a	O
=	O
100	O
we	O
will	O
obtain	O
ld	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
≥	O
0.48.	O
next	O
,	O
suppose	O
we	O
apply	O
a	O
“	O
clipping	O
”	O
transformation	O
;	O
that	O
is	O
,	O
we	O
use	O
the	O
transformation	O
x	O
(	O
cid:55	O
)	O
→	O
sign	O
(	O
x	O
)	O
min	O
{	O
1	O
,	O
|x|	O
}	O
.	O
then	O
,	O
following	O
this	O
transformation	O
,	O
w	O
(	O
cid:63	O
)	O
becomes	O
1	O
and	O
ld	O
(	O
w	O
(	O
cid:63	O
)	O
)	O
=	O
0.	O
this	O
simple	O
ex-	O
ample	O
shows	O
that	O
a	O
simple	O
transformation	O
can	O
have	O
a	O
signiﬁcant	O
inﬂuence	O
on	O
the	O
approximation	B
error	I
.	O
of	O
course	O
,	O
it	O
is	O
not	O
hard	O
to	O
think	O
of	O
examples	O
in	O
which	O
the	O
same	O
feature	B
trans-	O
formation	O
actually	O
hurts	O
performance	O
and	O
increases	O
the	O
approximation	B
error	I
.	O
this	O
is	O
not	O
surprising	O
,	O
as	O
we	O
have	O
already	O
argued	O
that	O
feature	B
transformations	I
2	O
more	O
precisely	O
,	O
the	O
bounds	O
we	O
derived	O
in	O
chapter	O
13	O
for	O
regularized	B
loss	I
minimization	I
depend	O
on	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
and	O
on	O
either	O
the	O
lipschitzness	O
or	O
the	O
smoothness	B
of	O
the	O
loss	B
function	I
.	O
for	O
linear	B
predictors	I
and	O
loss	B
functions	O
of	O
the	O
form	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
φ	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
,	O
y	O
)	O
,	O
where	O
φ	O
is	O
convex	B
and	O
either	O
1-lipschitz	O
or	O
1-smooth	O
with	O
respect	O
to	O
its	O
ﬁrst	O
argument	O
,	O
we	O
have	O
that	O
(	O
cid:96	O
)	O
is	O
either	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
-lipschitz	O
or	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2-smooth	O
.	O
for	O
example	O
,	O
for	O
the	O
squared	O
loss	B
,	O
φ	O
(	O
a	O
,	O
y	O
)	O
=	O
1	O
ﬁrst	O
argument	O
.	O
2	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
−	O
y	O
)	O
2	O
is	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2-smooth	O
with	O
respect	O
to	O
its	O
2	O
(	O
a	O
−	O
y	O
)	O
2	O
,	O
and	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
1	O
25.2	O
feature	B
manipulation	O
and	O
normalization	O
367	O
should	O
rely	O
on	O
our	O
prior	O
assumptions	O
on	O
the	O
problem	O
.	O
in	O
the	O
aforementioned	O
ex-	O
ample	O
,	O
a	O
prior	O
assumption	O
that	O
may	O
lead	O
us	O
to	O
use	O
the	O
“	O
clipping	O
”	O
transformation	O
is	O
that	O
features	O
that	O
get	O
values	O
larger	O
than	O
a	O
predeﬁned	O
threshold	O
value	O
give	O
us	O
no	O
additional	O
useful	O
information	O
,	O
and	O
therefore	O
we	O
can	O
clip	O
them	O
to	O
the	O
predeﬁned	O
threshold	O
.	O
25.2.1	O
examples	O
of	O
feature	B
transformations	I
we	O
now	O
list	O
several	O
common	O
techniques	O
for	O
feature	B
transformations	I
.	O
usually	O
,	O
it	O
is	O
helpful	O
to	O
combine	O
some	O
of	O
these	O
transformations	O
(	O
e.g.	O
,	O
centering	O
+	O
scaling	O
)	O
.	O
in	O
the	O
following	O
,	O
we	O
denote	O
by	O
f	O
=	O
(	O
f1	O
,	O
.	O
.	O
.	O
,	O
fm	O
)	O
∈	O
rm	O
the	O
value	O
of	O
the	O
feature	B
f	O
over	O
the	O
m	O
training	O
examples	O
.	O
also	O
,	O
we	O
denote	O
by	O
¯f	O
=	O
1	O
i=1	O
fi	O
the	O
empirical	O
m	O
mean	O
of	O
the	O
feature	B
over	O
all	O
examples	O
.	O
(	O
cid:80	O
)	O
m	O
centering	O
:	O
this	O
transformation	O
makes	O
the	O
feature	B
have	O
zero	O
mean	O
,	O
by	O
setting	O
fi	O
←	O
fi	O
−	O
¯f	O
.	O
unit	O
range	O
:	O
this	O
transformation	O
makes	O
the	O
range	O
of	O
each	O
feature	B
be	O
[	O
0	O
,	O
1	O
]	O
.	O
formally	O
,	O
let	O
fmax	O
=	O
maxi	O
fi	O
and	O
fmin	O
=	O
mini	O
fi	O
.	O
then	O
,	O
we	O
set	B
fi	O
←	O
fi−fmin	O
.	O
similarly	O
,	O
fmax−fmin	O
we	O
can	O
make	O
the	O
range	O
of	O
each	O
feature	B
be	O
[	O
−1	O
,	O
1	O
]	O
by	O
the	O
transformation	O
fi	O
←	O
−	O
1.	O
of	O
course	O
,	O
it	O
is	O
easy	O
to	O
make	O
the	O
range	O
[	O
0	O
,	O
b	O
]	O
or	O
[	O
−b	O
,	O
b	O
]	O
,	O
where	O
b	O
is	O
2	O
fi−fmin	O
fmax−fmin	O
a	O
user-speciﬁed	O
parameter	O
.	O
(	O
cid:80	O
)	O
m	O
standardization	O
:	O
this	O
transformation	O
makes	O
all	O
features	O
have	O
a	O
zero	O
mean	O
and	O
unit	O
variance	O
.	O
i=1	O
(	O
fi	O
−	O
¯f	O
)	O
2	O
be	O
the	O
empirical	O
variance	O
of	O
the	O
feature	B
.	O
formally	O
,	O
let	O
ν	O
=	O
1	O
m	O
then	O
,	O
we	O
set	B
fi	O
←	O
fi−	O
¯f√	O
ν	O
.	O
clipping	O
:	O
this	O
transformation	O
clips	O
high	O
or	O
low	O
values	O
of	O
the	O
feature	B
.	O
for	O
example	O
,	O
fi	O
←	O
sign	O
(	O
fi	O
)	O
max	O
{	O
b	O
,	O
|fi|	O
}	O
,	O
where	O
b	O
is	O
a	O
user-speciﬁed	O
parameter	O
.	O
sigmoidal	O
transformation	O
:	O
as	O
its	O
name	O
indicates	O
,	O
this	O
transformation	O
applies	O
a	O
sigmoid	O
function	B
on	O
the	O
feature	B
.	O
for	O
example	O
,	O
fi	O
←	O
1+exp	O
(	O
b	O
fi	O
)	O
,	O
where	O
b	O
is	O
a	O
user-speciﬁed	O
parameter	O
.	O
this	O
transformation	O
can	O
be	O
thought	O
of	O
as	O
a	O
“	O
soft	O
”	O
version	O
of	O
clipping	O
:	O
it	O
has	O
a	O
small	O
eﬀect	O
on	O
values	O
close	O
to	O
zero	O
and	O
behaves	O
similarly	O
to	O
clipping	O
on	O
values	O
far	O
away	O
from	O
zero	O
.	O
1	O
368	O
feature	B
selection	I
and	O
generation	O
logarithmic	O
transformation	O
:	O
the	O
transformation	O
is	O
fi	O
←	O
log	O
(	O
b+fi	O
)	O
,	O
where	O
b	O
is	O
a	O
user-speciﬁed	O
parameter	O
.	O
this	O
is	O
widely	O
used	O
when	O
the	O
feature	B
is	O
a	O
“	O
counting	O
”	O
feature	B
.	O
for	O
example	O
,	O
suppose	O
that	O
the	O
feature	B
represents	O
the	O
number	O
of	O
appearances	O
of	O
a	O
certain	O
word	O
in	O
a	O
text	O
document	O
.	O
then	O
,	O
the	O
diﬀerence	O
between	O
zero	O
occurrences	O
of	O
the	O
word	O
and	O
a	O
single	O
occurrence	O
is	O
much	O
more	O
important	O
than	O
the	O
diﬀerence	O
between	O
1000	O
occurrences	O
and	O
1001	O
occurrences	O
.	O
remark	O
25.5	O
in	O
the	O
aforementioned	O
transformations	O
,	O
each	O
feature	B
is	O
trans-	O
formed	O
on	O
the	O
basis	O
of	O
the	O
values	O
it	O
obtains	O
on	O
the	O
training	B
set	I
,	O
independently	O
of	O
other	O
features	O
’	O
values	O
.	O
in	O
some	O
situations	O
we	O
would	O
like	O
to	O
set	B
the	O
parameter	O
of	O
the	O
transformation	O
on	O
the	O
basis	O
of	O
other	O
features	O
as	O
well	O
.	O
a	O
notable	O
example	O
is	O
a	O
transformation	O
in	O
which	O
one	O
applies	O
a	O
scaling	O
to	O
the	O
features	O
so	O
that	O
the	O
empirical	O
average	O
of	O
some	O
norm	O
of	O
the	O
instances	O
becomes	O
1	O
.	O
25.3	O
feature	B
learning	I
so	O
far	O
we	O
have	O
discussed	O
feature	B
selection	I
and	O
manipulations	O
.	O
in	O
these	O
cases	O
,	O
we	O
start	O
with	O
a	O
predeﬁned	O
vector	O
space	O
rd	O
,	O
representing	O
our	O
features	O
.	O
then	O
,	O
we	O
select	O
a	O
subset	O
of	O
features	O
(	O
feature	B
selection	I
)	O
or	O
transform	O
individual	O
features	O
(	O
feature	B
transformation	O
)	O
.	O
in	O
this	O
section	O
we	O
describe	O
feature	B
learning	I
,	O
in	O
which	O
we	O
start	O
with	O
some	O
instance	B
space	I
,	O
x	O
,	O
and	O
would	O
like	O
to	O
learn	O
a	O
function	B
,	O
ψ	O
:	O
x	O
→	O
rd	O
,	O
which	O
maps	O
instances	O
in	O
x	O
into	O
a	O
representation	O
as	O
d-dimensional	O
feature	B
vectors	O
.	O
the	O
idea	O
of	O
feature	B
learning	I
is	O
to	O
automate	O
the	O
process	O
of	O
ﬁnding	O
a	O
good	O
rep-	O
resentation	O
of	O
the	O
input	O
space	O
.	O
as	O
mentioned	O
before	O
,	O
the	O
no-free-lunch	B
theorem	O
tells	O
us	O
that	O
we	O
must	O
incorporate	O
some	O
prior	B
knowledge	I
on	O
the	O
data	O
distribution	O
in	O
order	O
to	O
build	O
a	O
good	O
feature	B
representation	O
.	O
in	O
this	O
section	O
we	O
present	O
a	O
few	O
feature	B
learning	I
approaches	O
and	O
demonstrate	O
conditions	O
on	O
the	O
underlying	O
data	O
distribution	O
in	O
which	O
these	O
methods	O
can	O
be	O
useful	O
.	O
throughout	O
the	O
book	O
we	O
have	O
already	O
seen	O
several	O
useful	O
feature	B
construc-	O
tions	O
.	O
for	O
example	O
,	O
in	O
the	O
context	O
of	O
polynomial	B
regression	I
,	O
we	O
have	O
mapped	O
the	O
original	O
instances	O
into	O
the	O
vector	O
space	O
of	O
all	O
their	O
monomials	O
(	O
see	O
section	O
9.2.2	O
in	O
chapter	O
9	O
)	O
.	O
after	O
performing	O
this	O
mapping	O
,	O
we	O
trained	O
a	O
linear	B
predictor	I
on	O
top	O
of	O
the	O
constructed	O
features	O
.	O
automation	O
of	O
this	O
process	O
would	O
be	O
to	O
learn	O
a	O
transformation	O
ψ	O
:	O
x	O
→	O
rd	O
,	O
such	O
that	O
the	O
composition	O
of	O
the	O
class	O
of	O
linear	B
predictors	I
on	O
top	O
of	O
ψ	O
yields	O
a	O
good	O
hypothesis	B
class	I
for	O
the	O
task	O
at	O
hand	O
.	O
in	O
the	O
following	O
we	O
describe	O
a	O
technique	O
of	O
feature	B
construction	O
called	O
dictio-	O
nary	O
learning	O
.	O
25.3.1	O
dictionary	B
learning	I
using	O
auto-encoders	B
the	O
motivation	O
of	O
dictionary	B
learning	I
stems	O
from	O
a	O
commonly	O
used	O
represen-	O
tation	O
of	O
documents	O
as	O
a	O
“	O
bag-of-words	B
”	O
:	O
given	O
a	O
dictionary	O
of	O
words	O
d	O
=	O
{	O
w1	O
,	O
.	O
.	O
.	O
,	O
wk	O
}	O
,	O
where	O
each	O
wi	O
is	O
a	O
string	O
representing	O
a	O
word	O
in	O
the	O
dictionary	O
,	O
25.3	O
feature	B
learning	I
369	O
and	O
given	O
a	O
document	O
,	O
(	O
p1	O
,	O
.	O
.	O
.	O
,	O
pd	O
)	O
,	O
where	O
each	O
pi	O
is	O
a	O
word	O
in	O
the	O
document	O
,	O
we	O
represent	O
the	O
document	O
as	O
a	O
vector	O
x	O
∈	O
{	O
0	O
,	O
1	O
}	O
k	O
,	O
where	O
xi	O
is	O
1	O
if	O
wi	O
=	O
pj	O
for	O
some	O
j	O
∈	O
[	O
d	O
]	O
,	O
and	O
xi	O
=	O
0	O
otherwise	O
.	O
it	O
was	O
empirically	O
observed	O
in	O
many	O
text	O
processing	O
tasks	O
that	O
linear	B
predictors	I
are	O
quite	O
powerful	O
when	O
applied	O
on	O
this	O
representation	O
.	O
intuitively	O
,	O
we	O
can	O
think	O
of	O
each	O
word	O
as	O
a	O
feature	B
that	O
measures	O
some	O
aspect	O
of	O
the	O
document	O
.	O
given	O
labeled	O
examples	O
(	O
e.g.	O
,	O
topics	O
of	O
the	O
doc-	O
uments	O
)	O
,	O
a	O
learning	O
algorithm	O
searches	O
for	O
a	O
linear	B
predictor	I
that	O
weights	O
these	O
features	O
so	O
that	O
a	O
right	O
combination	O
of	O
appearances	O
of	O
words	O
is	O
indicative	O
of	O
the	O
label	B
.	O
while	O
in	O
text	O
processing	O
there	O
is	O
a	O
natural	O
meaning	O
to	O
words	O
and	O
to	O
the	O
dic-	O
tionary	O
,	O
in	O
other	O
applications	O
we	O
do	O
not	O
have	O
such	O
an	O
intuitive	O
representation	O
of	O
an	O
instance	B
.	O
for	O
example	O
,	O
consider	O
the	O
computer	O
vision	O
application	O
of	O
object	O
recognition	O
.	O
here	O
,	O
the	O
instance	B
is	O
an	O
image	O
and	O
the	O
goal	O
is	O
to	O
recognize	O
which	O
object	O
appears	O
in	O
the	O
image	O
.	O
applying	O
a	O
linear	B
predictor	I
on	O
the	O
pixel-based	O
rep-	O
resentation	O
of	O
the	O
image	O
does	O
not	O
yield	O
a	O
good	O
classiﬁer	B
.	O
what	O
we	O
would	O
like	O
to	O
have	O
is	O
a	O
mapping	O
ψ	O
that	O
would	O
take	O
the	O
pixel-based	O
representation	O
of	O
the	O
image	O
and	O
would	O
output	O
a	O
bag	O
of	O
“	O
visual	O
words	O
,	O
”	O
representing	O
the	O
content	O
of	O
the	O
image	O
.	O
for	O
example	O
,	O
a	O
“	O
visual	O
word	O
”	O
can	O
be	O
“	O
there	O
is	O
an	O
eye	O
in	O
the	O
image.	O
”	O
if	O
we	O
had	O
such	O
representation	O
,	O
we	O
could	O
have	O
applied	O
a	O
linear	B
predictor	I
on	O
top	O
of	O
this	O
representation	O
to	O
train	O
a	O
classiﬁer	B
for	O
,	O
say	O
,	O
face	B
recognition	I
.	O
our	O
question	O
is	O
,	O
therefore	O
,	O
how	O
can	O
we	O
learn	O
a	O
dictionary	O
of	O
“	O
visual	O
words	O
”	O
such	O
that	O
a	O
bag-of-	O
words	O
representation	O
of	O
an	O
image	O
would	O
be	O
helpful	O
for	O
predicting	O
which	O
object	O
appears	O
in	O
the	O
image	O
?	O
a	O
ﬁrst	O
naive	O
approach	O
for	O
dictionary	B
learning	I
relies	O
on	O
a	O
clustering	B
algorithm	O
(	O
see	O
chapter	O
22	O
)	O
.	O
suppose	O
that	O
we	O
learn	O
a	O
function	B
c	O
:	O
x	O
→	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
,	O
where	O
c	O
(	O
x	O
)	O
is	O
the	O
cluster	O
to	O
which	O
x	O
belongs	O
.	O
then	O
,	O
we	O
can	O
think	O
of	O
the	O
clusters	O
as	O
“	O
words	O
,	O
”	O
and	O
of	O
instances	O
as	O
“	O
documents	O
,	O
”	O
where	O
a	O
document	O
x	O
is	O
mapped	O
to	O
the	O
vector	O
ψ	O
(	O
x	O
)	O
∈	O
{	O
0	O
,	O
1	O
}	O
k	O
,	O
where	O
ψ	O
(	O
x	O
)	O
i	O
is	O
1	O
if	O
and	O
only	O
if	O
x	O
belongs	O
to	O
the	O
ith	O
cluster	O
.	O
now	O
,	O
it	O
is	O
straightforward	O
to	O
see	O
that	O
applying	O
a	O
linear	B
predictor	I
on	O
ψ	O
(	O
x	O
)	O
is	O
equivalent	O
to	O
assigning	O
the	O
same	O
target	O
value	O
to	O
all	O
instances	O
that	O
belong	O
to	O
the	O
same	O
cluster	O
.	O
furthermore	O
,	O
if	O
the	O
clustering	B
is	O
based	O
on	O
distances	O
from	O
a	O
class	O
center	O
(	O
e.g.	O
,	O
k-means	B
)	O
,	O
then	O
a	O
linear	B
predictor	I
on	O
ψ	O
(	O
x	O
)	O
yields	O
a	O
piece-wise	O
constant	O
predictor	B
on	O
x.	O
ﬁnd	O
a	O
pair	O
of	O
functions	O
such	O
that	O
the	O
reconstruction	O
error	O
,	O
(	O
cid:80	O
)	O
both	O
the	O
k-means	B
and	O
pca	O
approaches	O
can	O
be	O
regarded	O
as	O
special	O
cases	O
of	O
a	O
more	O
general	O
approach	O
for	O
dictionary	B
learning	I
which	O
is	O
called	O
auto-encoders	B
.	O
in	O
an	O
auto-encoder	O
we	O
learn	O
a	O
pair	O
of	O
functions	O
:	O
an	O
“	O
encoder	O
”	O
function	B
,	O
ψ	O
:	O
rd	O
→	O
rk	O
,	O
and	O
a	O
“	O
decoder	O
”	O
function	B
,	O
φ	O
:	O
rk	O
→	O
rd	O
.	O
the	O
goal	O
of	O
the	O
learning	O
process	O
is	O
to	O
i	O
(	O
cid:107	O
)	O
xi	O
−	O
φ	O
(	O
ψ	O
(	O
xi	O
)	O
)	O
(	O
cid:107	O
)	O
2	O
,	O
is	O
small	O
.	O
of	O
course	O
,	O
we	O
can	O
trivially	O
set	B
k	O
=	O
d	O
and	O
both	O
ψ	O
,	O
φ	O
to	O
be	O
the	O
identity	O
mapping	O
,	O
which	O
yields	O
a	O
perfect	O
reconstruction	O
.	O
we	O
therefore	O
must	O
restrict	O
ψ	O
and	O
φ	O
in	O
some	O
way	O
.	O
in	O
pca	O
,	O
we	O
constrain	O
k	O
<	O
d	O
and	O
further	O
restrict	O
ψ	O
and	O
φ	O
to	O
be	O
linear	O
functions	O
.	O
in	O
k-means	B
,	O
k	O
is	O
not	O
restricted	O
to	O
be	O
smaller	O
than	O
d	O
,	O
but	O
now	O
ψ	O
and	O
φ	O
rely	O
on	O
k	O
centroids	O
,	O
µ1	O
,	O
.	O
.	O
.	O
,	O
µk	O
,	O
and	O
ψ	O
(	O
x	O
)	O
returns	O
an	O
indicator	O
vector	O
370	O
feature	B
selection	I
and	O
generation	O
in	O
{	O
0	O
,	O
1	O
}	O
k	O
that	O
indicates	O
the	O
closest	O
centroid	O
to	O
x	O
,	O
while	O
φ	O
takes	O
as	O
input	O
an	O
indicator	O
vector	O
and	O
returns	O
the	O
centroid	O
representing	O
this	O
vector	O
.	O
an	O
important	O
property	O
of	O
the	O
k-means	B
construction	O
,	O
which	O
is	O
key	O
in	O
allowing	O
k	O
to	O
be	O
larger	O
than	O
d	O
,	O
is	O
that	O
ψ	O
maps	O
instances	O
into	O
sparse	O
vectors	O
.	O
in	O
fact	O
,	O
in	O
k-means	B
only	O
a	O
single	O
coordinate	O
of	O
ψ	O
(	O
x	O
)	O
is	O
nonzero	O
.	O
an	O
immediate	O
extension	O
of	O
the	O
k-means	B
construction	O
is	O
therefore	O
to	O
restrict	O
the	O
range	O
of	O
ψ	O
to	O
be	O
vectors	O
with	O
at	O
most	O
s	O
nonzero	O
elements	O
,	O
where	O
s	O
is	O
a	O
small	O
integer	O
.	O
in	O
particular	O
,	O
let	O
ψ	O
and	O
φ	O
be	O
functions	O
that	O
depend	O
on	O
µ1	O
,	O
.	O
.	O
.	O
,	O
µk	O
.	O
the	O
function	B
ψ	O
maps	O
an	O
instance	B
vector	O
x	O
to	O
a	O
vector	O
ψ	O
(	O
x	O
)	O
∈	O
rk	O
,	O
where	O
ψ	O
(	O
x	O
)	O
should	O
have	O
at	O
most	O
s	O
nonzero	O
elements	O
.	O
i=1	O
viµi	O
.	O
as	O
before	O
,	O
our	O
goal	O
is	O
to	O
have	O
a	O
the	O
function	B
φ	O
(	O
v	O
)	O
is	O
deﬁned	O
to	O
be	O
(	O
cid:80	O
)	O
k	O
small	O
reconstruction	O
error	O
,	O
and	O
therefore	O
we	O
can	O
deﬁne	O
ψ	O
(	O
x	O
)	O
=	O
argmin	O
v	O
(	O
cid:107	O
)	O
x	O
−	O
φ	O
(	O
v	O
)	O
(	O
cid:107	O
)	O
2	O
s.t	O
.	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
0	O
≤	O
s	O
,	O
where	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
0	O
=	O
|	O
{	O
j	O
:	O
vj	O
(	O
cid:54	O
)	O
=	O
0	O
}	O
|	O
.	O
note	O
that	O
when	O
s	O
=	O
1	O
and	O
we	O
further	O
restrict	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
1	O
=	O
1	O
then	O
we	O
obtain	O
the	O
k-means	B
encoding	O
function	B
;	O
that	O
is	O
,	O
ψ	O
(	O
x	O
)	O
is	O
the	O
indicator	O
vector	O
of	O
the	O
centroid	O
closest	O
to	O
x.	O
for	O
larger	O
values	O
of	O
s	O
,	O
the	O
optimization	O
problem	O
in	O
the	O
preceding	O
deﬁnition	O
of	O
ψ	O
becomes	O
computationally	O
diﬃcult	O
.	O
therefore	O
,	O
in	O
practice	O
,	O
we	O
sometime	O
use	O
(	O
cid:96	O
)	O
1	O
regularization	B
instead	O
of	O
the	O
sparsity	O
constraint	O
and	O
deﬁne	O
ψ	O
to	O
be	O
(	O
cid:2	O
)	O
(	O
cid:107	O
)	O
x	O
−	O
φ	O
(	O
v	O
)	O
(	O
cid:107	O
)	O
2	O
+	O
λ	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
1	O
(	O
cid:3	O
)	O
,	O
ψ	O
(	O
x	O
)	O
=	O
argmin	O
v	O
ror	O
,	O
(	O
cid:80	O
)	O
m	O
where	O
λ	O
>	O
0	O
is	O
a	O
regularization	B
parameter	O
.	O
anyway	O
,	O
the	O
dictionary	B
learning	I
problem	O
is	O
now	O
to	O
ﬁnd	O
the	O
vectors	O
µ1	O
,	O
.	O
.	O
.	O
,	O
µk	O
such	O
that	O
the	O
reconstruction	O
er-	O
i=1	O
(	O
cid:107	O
)	O
xi	O
−	O
φ	O
(	O
ψ	O
(	O
x	O
)	O
)	O
(	O
cid:107	O
)	O
2	O
,	O
is	O
as	O
small	O
as	O
possible	O
.	O
even	O
if	O
ψ	O
is	O
deﬁned	O
using	O
the	O
(	O
cid:96	O
)	O
1	O
regularization	B
,	O
this	O
is	O
still	O
a	O
computationally	O
hard	O
problem	O
(	O
similar	O
to	O
the	O
k-means	B
problem	O
)	O
.	O
however	O
,	O
several	O
heuristic	O
search	O
algorithms	O
may	O
give	O
reasonably	O
good	O
solutions	O
.	O
these	O
algorithms	O
are	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
.	O
25.4	O
summary	O
many	O
machine	O
learning	O
algorithms	O
take	O
the	O
feature	B
representation	O
of	O
instances	O
for	O
granted	O
.	O
yet	O
the	O
choice	O
of	O
representation	O
requires	O
careful	O
attention	O
.	O
we	O
dis-	O
cussed	O
approaches	O
for	O
feature	B
selection	I
,	O
introducing	O
ﬁlters	B
,	O
greedy	O
selection	O
al-	O
gorithms	O
,	O
and	O
sparsity-inducing	B
norms	I
.	O
next	O
we	O
presented	O
several	O
examples	O
for	O
feature	B
transformations	I
and	O
demonstrated	O
their	O
usefulness	O
.	O
last	O
,	O
we	O
discussed	O
feature	B
learning	I
,	O
and	O
in	O
particular	O
dictionary	B
learning	I
.	O
we	O
have	O
shown	O
that	O
fea-	O
ture	O
selection	O
,	O
manipulation	O
,	O
and	O
learning	O
all	O
depend	O
on	O
some	O
prior	B
knowledge	I
on	O
the	O
data	O
.	O
25.5	O
bibliographic	O
remarks	O
371	O
25.5	O
bibliographic	O
remarks	O
guyon	O
&	O
elisseeﬀ	O
(	O
2003	O
)	O
surveyed	O
several	O
feature	B
selection	I
procedures	O
,	O
including	O
many	O
types	O
of	O
ﬁlters	B
.	O
forward	B
greedy	I
selection	I
procedures	O
for	O
minimizing	O
a	O
convex	B
objective	O
sub-	O
ject	O
to	O
a	O
polyhedron	O
constraint	O
date	O
back	O
to	O
the	O
frank-wolfe	O
algorithm	O
(	O
frank	O
&	O
wolfe	O
1956	O
)	O
.	O
the	O
relation	O
to	O
boosting	B
has	O
been	O
studied	O
by	O
several	O
authors	O
,	O
including	O
,	O
(	O
warmuth	O
,	O
liao	O
&	O
ratsch	O
2006	O
,	O
warmuth	O
,	O
glocer	O
&	O
vishwanathan	O
2008	O
,	O
shalev-shwartz	O
&	O
singer	O
2008	O
)	O
.	O
matching	O
pursuit	O
has	O
been	O
studied	O
in	O
the	O
signal	O
processing	O
community	O
(	O
mallat	O
&	O
zhang	O
1993	O
)	O
.	O
several	O
papers	O
analyzed	O
greedy	O
selection	O
methods	O
under	O
various	O
conditions	O
.	O
see	O
,	O
for	O
example	O
,	O
shalev-	O
shwartz	O
,	O
zhang	O
&	O
srebro	O
(	O
2010	O
)	O
and	O
the	O
references	O
therein	O
.	O
the	O
use	O
of	O
the	O
(	O
cid:96	O
)	O
1-norm	O
as	O
a	O
surrogate	O
for	O
sparsity	O
has	O
a	O
long	O
history	O
(	O
e.g	O
.	O
tib-	O
shirani	O
(	O
1996	O
)	O
and	O
the	O
references	O
therein	O
)	O
,	O
and	O
much	O
work	O
has	O
been	O
done	O
on	O
un-	O
derstanding	O
the	O
relationship	O
between	O
the	O
(	O
cid:96	O
)	O
1-norm	O
and	O
sparsity	O
.	O
it	O
is	O
also	O
closely	O
related	O
to	O
compressed	B
sensing	I
(	O
see	O
chapter	O
23	O
)	O
.	O
the	O
ability	O
to	O
sparsify	O
low	O
(	O
cid:96	O
)	O
1	O
norm	O
predictors	O
dates	O
back	O
to	O
maurey	O
(	O
pisier	O
1980-1981	O
)	O
.	O
in	O
section	O
26.4	O
we	O
also	O
show	O
that	O
low	O
(	O
cid:96	O
)	O
1	O
norm	O
can	O
be	O
used	O
to	O
bound	O
the	O
estimation	B
error	I
of	O
our	O
predictor	B
.	O
feature	B
learning	I
and	O
dictionary	B
learning	I
have	O
been	O
extensively	O
studied	O
recently	O
in	O
the	O
context	O
of	O
deep	O
neural	B
networks	I
.	O
see	O
,	O
for	O
example	O
,	O
(	O
lecun	O
&	O
bengio	O
1995	O
,	O
hinton	O
et	O
al	O
.	O
2006	O
,	O
ranzato	O
et	O
al	O
.	O
2007	O
,	O
collobert	O
&	O
weston	O
2008	O
,	O
lee	O
et	O
al	O
.	O
2009	O
,	O
le	O
et	O
al	O
.	O
2012	O
,	O
bengio	O
2009	O
)	O
and	O
the	O
references	O
therein	O
.	O
25.6	O
exercises	O
1.	O
prove	O
the	O
equality	O
given	O
in	O
equation	O
(	O
25.1	O
)	O
.	O
hint	O
:	O
let	O
a∗	O
,	O
b∗	O
be	O
minimizers	O
of	O
the	O
left-hand	O
side	O
.	O
find	O
a	O
,	O
b	O
such	O
that	O
the	O
objective	O
value	O
of	O
the	O
right-hand	O
side	O
is	O
smaller	O
than	O
that	O
of	O
the	O
left-hand	O
side	O
.	O
do	O
the	O
same	O
for	O
the	O
other	O
direction	O
.	O
2.	O
show	O
that	O
equation	O
(	O
25.7	O
)	O
is	O
the	O
solution	O
of	O
equation	O
(	O
25.6	O
)	O
.	O
3.	O
adaboost	O
as	O
a	O
forward	B
greedy	I
selection	I
algorithm	O
:	O
recall	B
the	O
ad-	O
aboost	O
algorithm	O
from	O
chapter	O
10.	O
in	O
this	O
section	O
we	O
give	O
another	O
interpre-	O
tation	O
of	O
adaboost	O
as	O
a	O
forward	B
greedy	I
selection	I
algorithm	O
.	O
•	O
given	O
a	O
set	B
of	O
m	O
instances	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
,	O
and	O
a	O
hypothesis	B
class	I
h	O
of	O
ﬁnite	O
vc	O
dimension	B
,	O
show	O
that	O
there	O
exist	O
d	O
and	O
h1	O
,	O
.	O
.	O
.	O
,	O
hd	O
such	O
that	O
for	O
every	O
h	O
∈	O
h	O
there	O
exists	O
i	O
∈	O
[	O
d	O
]	O
with	O
hi	O
(	O
xj	O
)	O
=	O
h	O
(	O
xj	O
)	O
for	O
every	O
j	O
∈	O
[	O
m	O
]	O
.	O
•	O
let	O
r	O
(	O
w	O
)	O
be	O
as	O
deﬁned	O
in	O
equation	O
(	O
25.3	O
)	O
.	O
given	O
some	O
w	O
,	O
deﬁne	O
fw	O
to	O
be	O
the	O
function	B
fw	O
(	O
·	O
)	O
=	O
wihi	O
(	O
·	O
)	O
.	O
d	O
(	O
cid:88	O
)	O
i=1	O
372	O
feature	B
selection	I
and	O
generation	O
let	O
d	O
be	O
the	O
distribution	O
over	O
[	O
m	O
]	O
deﬁned	O
by	O
exp	O
(	O
−yifw	O
(	O
xi	O
)	O
)	O
di	O
=	O
,	O
z	O
where	O
z	O
is	O
a	O
normalization	O
factor	O
that	O
ensures	O
that	O
d	O
is	O
a	O
probability	O
vector	O
.	O
show	O
that	O
=	O
−	O
m	O
(	O
cid:88	O
)	O
i=1	O
diyihj	O
(	O
xi	O
)	O
.	O
∂r	O
(	O
w	O
)	O
furthermore	O
,	O
denoting	O
j	O
=	O
(	O
cid:80	O
)	O
m	O
wj	O
i=1	O
di1	O
[	O
hj	O
(	O
xi	O
)	O
(	O
cid:54	O
)	O
=yi	O
]	O
,	O
show	O
that	O
∂r	O
(	O
w	O
)	O
=	O
2j	O
−	O
1.	O
conclude	O
that	O
if	O
j	O
≤	O
1/2	O
−	O
γ	O
then	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
≥	O
γ/2	O
.	O
log	O
(	O
(	O
cid:112	O
)	O
1	O
−	O
4γ2	O
)	O
.	O
hint	O
:	O
use	O
the	O
proof	O
of	O
theorem	O
10.2	O
.	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
∂r	O
(	O
w	O
)	O
wj	O
wj	O
•	O
show	O
that	O
the	O
update	O
of	O
adaboost	O
guarantees	O
r	O
(	O
w	O
(	O
t+1	O
)	O
)	O
−	O
r	O
(	O
w	O
(	O
t	O
)	O
)	O
≤	O
part	O
iv	O
advanced	O
theory	O
26	O
rademacher	O
complexities	O
in	O
chapter	O
4	O
we	O
have	O
shown	O
that	O
uniform	B
convergence	I
is	O
a	O
suﬃcient	O
condition	O
for	O
learnability	O
.	O
in	O
this	O
chapter	O
we	O
study	O
the	O
rademacher	O
complexity	O
,	O
which	O
measures	O
the	O
rate	O
of	O
uniform	B
convergence	I
.	O
we	O
will	O
provide	O
generalization	B
bounds	I
based	O
on	O
this	O
measure	O
.	O
26.1	O
the	O
rademacher	O
complexity	O
recall	B
the	O
deﬁnition	O
of	O
an	O
-representative	O
sample	O
from	O
chapter	O
4	O
,	O
repeated	O
here	O
for	O
convenience	O
.	O
definition	O
26.1	O
(	O
-representative	O
sample	O
)	O
a	O
training	B
set	I
s	O
is	O
called	O
-representative	O
(	O
w.r.t	O
.	O
domain	B
z	O
,	O
hypothesis	B
class	I
h	O
,	O
loss	B
function	I
(	O
cid:96	O
)	O
,	O
and	O
distribution	O
d	O
)	O
if	O
|ld	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
|	O
≤	O
	O
.	O
sup	O
h∈h	O
we	O
have	O
shown	O
that	O
if	O
s	O
is	O
an	O
/2	O
representative	B
sample	I
then	O
the	O
erm	O
rule	O
is	O
-consistent	O
,	O
namely	O
,	O
ld	O
(	O
ermh	O
(	O
s	O
)	O
)	O
≤	O
minh∈h	O
ld	O
(	O
h	O
)	O
+	O
	O
.	O
to	O
simplify	O
our	O
notation	O
,	O
let	O
us	O
denote	O
f	O
def=	O
(	O
cid:96	O
)	O
◦	O
h	O
def=	O
{	O
z	O
(	O
cid:55	O
)	O
→	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
z	O
)	O
:	O
h	O
∈	O
h	O
}	O
,	O
and	O
given	O
f	O
∈	O
f	O
,	O
we	O
deﬁne	O
ld	O
(	O
f	O
)	O
=	O
e	O
z∼d	O
[	O
f	O
(	O
z	O
)	O
]	O
,	O
ls	O
(	O
f	O
)	O
=	O
1	O
m	O
m	O
(	O
cid:88	O
)	O
f	O
(	O
zi	O
)	O
.	O
i=1	O
we	O
deﬁne	O
the	O
representativeness	O
of	O
s	O
with	O
respect	O
to	O
f	O
as	O
the	O
largest	O
gap	O
be-	O
tween	O
the	O
true	B
error	I
of	O
a	O
function	B
f	O
and	O
its	O
empirical	B
error	I
,	O
namely	O
,	O
repd	O
(	O
f	O
,	O
s	O
)	O
def=	O
sup	O
f∈f	O
(	O
cid:0	O
)	O
ld	O
(	O
f	O
)	O
−	O
ls	O
(	O
f	O
)	O
(	O
cid:1	O
)	O
.	O
now	O
,	O
suppose	O
we	O
would	O
like	O
to	O
estimate	O
the	O
representativeness	O
of	O
s	O
using	O
the	O
sample	O
s	O
only	O
.	O
one	O
simple	O
idea	O
is	O
to	O
split	O
s	O
into	O
two	O
disjoint	O
sets	O
,	O
s	O
=	O
s1	O
∪	O
s2	O
;	O
refer	O
to	O
s1	O
as	O
a	O
validation	B
set	O
and	O
to	O
s2	O
as	O
a	O
training	B
set	I
.	O
we	O
can	O
then	O
estimate	O
the	O
representativeness	O
of	O
s	O
by	O
(	O
cid:0	O
)	O
ls1	O
(	O
f	O
)	O
−	O
ls2	O
(	O
f	O
)	O
(	O
cid:1	O
)	O
.	O
sup	O
f∈f	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
(	O
26.1	O
)	O
(	O
26.2	O
)	O
376	O
rademacher	O
complexities	O
this	O
can	O
be	O
written	O
more	O
compactly	O
by	O
deﬁning	O
σ	O
=	O
(	O
σ1	O
,	O
.	O
.	O
.	O
,	O
σm	O
)	O
∈	O
{	O
±1	O
}	O
m	O
to	O
be	O
a	O
vector	O
such	O
that	O
s1	O
=	O
{	O
zi	O
:	O
σi	O
=	O
1	O
}	O
and	O
s2	O
=	O
{	O
zi	O
:	O
σi	O
=	O
−1	O
}	O
.	O
then	O
,	O
if	O
we	O
further	O
assume	O
that	O
|s1|	O
=	O
|s2|	O
then	O
equation	O
(	O
26.2	O
)	O
can	O
be	O
rewritten	O
as	O
m	O
(	O
cid:88	O
)	O
i=1	O
2	O
m	O
sup	O
f∈f	O
σif	O
(	O
zi	O
)	O
.	O
(	O
26.3	O
)	O
the	O
rademacher	O
complexity	O
measure	O
captures	O
this	O
idea	O
by	O
considering	O
the	O
ex-	O
pectation	O
of	O
the	O
above	O
with	O
respect	O
to	O
a	O
random	O
choice	O
of	O
σ.	O
formally	O
,	O
let	O
f	O
◦	O
s	O
be	O
the	O
set	B
of	O
all	O
possible	O
evaluations	O
a	O
function	B
f	O
∈	O
f	O
can	O
achieve	O
on	O
a	O
sample	O
s	O
,	O
namely	O
,	O
f	O
◦	O
s	O
=	O
{	O
(	O
f	O
(	O
z1	O
)	O
,	O
.	O
.	O
.	O
,	O
f	O
(	O
zm	O
)	O
)	O
:	O
f	O
∈	O
f	O
}	O
.	O
let	O
the	O
variables	O
in	O
σ	O
be	O
distributed	O
i.i.d	O
.	O
according	O
to	O
p	O
[	O
σi	O
=	O
1	O
]	O
=	O
p	O
[	O
σi	O
=	O
−1	O
]	O
=	O
2	O
.	O
then	O
,	O
the	O
rademacher	O
complexity	O
of	O
f	O
with	O
respect	O
to	O
s	O
is	O
deﬁned	O
as	O
follows	O
:	O
1	O
r	O
(	O
f	O
◦	O
s	O
)	O
def=	O
1	O
m	O
e	O
σ∼	O
{	O
±1	O
}	O
m	O
σif	O
(	O
zi	O
)	O
.	O
(	O
26.4	O
)	O
more	O
generally	O
,	O
given	O
a	O
set	B
of	O
vectors	O
,	O
a	O
⊂	O
rm	O
,	O
we	O
deﬁne	O
(	O
cid:35	O
)	O
m	O
(	O
cid:88	O
)	O
(	O
cid:35	O
)	O
i=1	O
(	O
cid:34	O
)	O
(	O
cid:34	O
)	O
sup	O
f∈f	O
m	O
(	O
cid:88	O
)	O
i=1	O
r	O
(	O
a	O
)	O
def=	O
1	O
m	O
e	O
σ	O
sup	O
a∈a	O
σiai	O
.	O
(	O
26.5	O
)	O
the	O
following	O
lemma	O
bounds	O
the	O
expected	O
value	O
of	O
the	O
representativeness	O
of	O
s	O
by	O
twice	O
the	O
expected	O
rademacher	O
complexity	O
.	O
lemma	O
26.2	O
e	O
s∼dm	O
proof	O
let	O
s	O
(	O
cid:48	O
)	O
=	O
{	O
z	O
(	O
cid:48	O
)	O
1	O
,	O
.	O
.	O
.	O
,	O
z	O
(	O
cid:48	O
)	O
ld	O
(	O
f	O
)	O
=	O
es	O
(	O
cid:48	O
)	O
[	O
ls	O
(	O
cid:48	O
)	O
(	O
f	O
)	O
]	O
.	O
therefore	O
,	O
for	O
every	O
f	O
∈	O
f	O
we	O
have	O
[	O
repd	O
(	O
f	O
,	O
s	O
)	O
]	O
≤	O
2	O
e	O
s∼dm	O
r	O
(	O
f	O
◦	O
s	O
)	O
.	O
m	O
}	O
be	O
another	O
i.i.d	O
.	O
sample	O
.	O
clearly	O
,	O
for	O
all	O
f	O
∈	O
f	O
,	O
ld	O
(	O
f	O
)	O
−	O
ls	O
(	O
f	O
)	O
=	O
e	O
s	O
(	O
cid:48	O
)	O
[	O
ls	O
(	O
cid:48	O
)	O
(	O
f	O
)	O
]	O
−	O
ls	O
(	O
f	O
)	O
=	O
e	O
s	O
(	O
cid:48	O
)	O
[	O
ls	O
(	O
cid:48	O
)	O
(	O
f	O
)	O
−	O
ls	O
(	O
f	O
)	O
]	O
.	O
taking	O
supremum	O
over	O
f	O
∈	O
f	O
of	O
both	O
sides	O
,	O
and	O
using	O
the	O
fact	O
that	O
the	O
supremum	O
of	O
expectation	O
is	O
smaller	O
than	O
expectation	O
of	O
the	O
supremum	O
we	O
obtain	O
sup	O
f∈f	O
(	O
cid:0	O
)	O
ld	O
(	O
f	O
)	O
−	O
ls	O
(	O
f	O
)	O
(	O
cid:1	O
)	O
=	O
sup	O
(	O
cid:34	O
)	O
f∈f	O
≤	O
e	O
s	O
(	O
cid:48	O
)	O
taking	O
expectation	O
over	O
s	O
on	O
both	O
sides	O
we	O
obtain	O
(	O
cid:34	O
)	O
e	O
s	O
sup	O
f∈f	O
(	O
cid:0	O
)	O
ld	O
(	O
f	O
)	O
−	O
ls	O
(	O
f	O
)	O
(	O
cid:1	O
)	O
(	O
cid:35	O
)	O
.	O
sup	O
f∈f	O
s	O
(	O
cid:48	O
)	O
[	O
ls	O
(	O
cid:48	O
)	O
(	O
f	O
)	O
−	O
ls	O
(	O
f	O
)	O
]	O
e	O
(	O
cid:0	O
)	O
ls	O
(	O
cid:48	O
)	O
(	O
f	O
)	O
−	O
ls	O
(	O
f	O
)	O
(	O
cid:1	O
)	O
(	O
cid:35	O
)	O
(	O
cid:0	O
)	O
ls	O
(	O
cid:48	O
)	O
(	O
f	O
)	O
−	O
ls	O
(	O
f	O
)	O
(	O
cid:1	O
)	O
(	O
cid:35	O
)	O
m	O
(	O
cid:88	O
)	O
(	O
f	O
(	O
z	O
(	O
cid:48	O
)	O
i	O
)	O
−	O
f	O
(	O
zi	O
)	O
)	O
sup	O
f∈f	O
i=1	O
sup	O
f∈f	O
(	O
cid:34	O
)	O
(	O
cid:34	O
)	O
≤	O
e	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
=	O
1	O
m	O
e	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
(	O
cid:35	O
)	O
.	O
(	O
26.6	O
)	O
26.1	O
the	O
rademacher	O
complexity	O
377	O
next	O
,	O
we	O
note	O
that	O
for	O
each	O
j	O
,	O
zj	O
and	O
z	O
(	O
cid:48	O
)	O
replace	O
them	O
without	O
aﬀecting	O
the	O
expectation	O
:	O
j	O
are	O
i.i.d	O
.	O
variables	O
.	O
therefore	O
,	O
we	O
can	O
j	O
)	O
−	O
f	O
(	O
zj	O
)	O
)	O
+	O
(	O
f	O
(	O
z	O
(	O
cid:48	O
)	O
i	O
)	O
−	O
f	O
(	O
zi	O
)	O
)	O
j	O
)	O
)	O
+	O
(	O
f	O
(	O
z	O
(	O
cid:48	O
)	O
i	O
)	O
−	O
f	O
(	O
zi	O
)	O
)	O
(	O
26.7	O
)	O
let	O
σj	O
be	O
a	O
random	O
variable	O
such	O
that	O
p	O
[	O
σj	O
=	O
1	O
]	O
=	O
p	O
[	O
σj	O
=	O
−1	O
]	O
=	O
1/2	O
.	O
from	O
equation	O
(	O
26.7	O
)	O
we	O
obtain	O
that	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
i	O
(	O
cid:54	O
)	O
=j	O
i	O
(	O
cid:54	O
)	O
=j	O
(	O
cid:88	O
)	O
i	O
(	O
cid:54	O
)	O
=j	O
	O
(	O
f	O
(	O
z	O
(	O
cid:48	O
)	O
	O
(	O
f	O
(	O
zj	O
)	O
−	O
f	O
(	O
z	O
(	O
cid:48	O
)	O
σj	O
(	O
f	O
(	O
z	O
(	O
cid:48	O
)	O
	O
(	O
f	O
(	O
z	O
(	O
cid:48	O
)	O
	O
=	O
	O
.	O
	O
	O
.	O
e	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
e	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
e	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
,	O
σj	O
=	O
1	O
2	O
=	O
e	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
f∈f	O
f∈f	O
	O
sup	O
	O
sup	O
	O
sup	O
	O
sup	O
m	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
f∈f	O
f∈f	O
i=1	O
sup	O
f∈f	O
sup	O
f∈f	O
i	O
(	O
cid:34	O
)	O
e	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
,	O
σ	O
(	O
l.h.s	O
.	O
of	O
equation	O
(	O
26.7	O
)	O
)	O
+	O
(	O
r.h.s	O
.	O
of	O
equation	O
(	O
26.7	O
)	O
)	O
j	O
)	O
−	O
f	O
(	O
zj	O
)	O
)	O
+	O
(	O
f	O
(	O
z	O
(	O
cid:48	O
)	O
i	O
)	O
−	O
f	O
(	O
zi	O
)	O
)	O
i	O
(	O
cid:54	O
)	O
=j	O
1	O
(	O
cid:88	O
)	O
2	O
j	O
)	O
−	O
f	O
(	O
zj	O
)	O
)	O
+	O
(	O
cid:34	O
)	O
(	O
cid:35	O
)	O
(	O
cid:88	O
)	O
=	O
e	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
,	O
σ	O
(	O
f	O
(	O
z	O
(	O
cid:48	O
)	O
i	O
)	O
−	O
f	O
(	O
zi	O
)	O
)	O
m	O
(	O
cid:88	O
)	O
σi	O
(	O
f	O
(	O
z	O
(	O
cid:48	O
)	O
sup	O
f∈f	O
i=1	O
i	O
)	O
−	O
f	O
(	O
zi	O
)	O
)	O
(	O
cid:88	O
)	O
−σif	O
(	O
zi	O
)	O
(	O
26.8	O
)	O
(	O
cid:35	O
)	O
.	O
(	O
26.9	O
)	O
repeating	O
this	O
for	O
all	O
j	O
we	O
obtain	O
that	O
(	O
f	O
(	O
z	O
(	O
cid:48	O
)	O
i	O
)	O
−	O
f	O
(	O
zi	O
)	O
)	O
(	O
cid:34	O
)	O
e	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
finally	O
,	O
σi	O
(	O
f	O
(	O
z	O
(	O
cid:48	O
)	O
i	O
)	O
−	O
f	O
(	O
zi	O
)	O
)	O
≤	O
sup	O
f∈f	O
σif	O
(	O
z	O
(	O
cid:48	O
)	O
i	O
)	O
+	O
sup	O
f∈f	O
i	O
i	O
and	O
since	O
the	O
probability	O
of	O
σ	O
is	O
the	O
same	O
as	O
the	O
probability	O
of	O
−σ	O
,	O
the	O
right-hand	O
side	O
of	O
equation	O
(	O
26.9	O
)	O
can	O
be	O
bounded	O
by	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
i	O
)	O
+	O
sup	O
f∈f	O
s	O
(	O
cid:48	O
)	O
[	O
r	O
(	O
f	O
◦	O
s	O
(	O
cid:48	O
)	O
)	O
]	O
+	O
m	O
e	O
σif	O
(	O
z	O
(	O
cid:48	O
)	O
s	O
i	O
i	O
sup	O
f∈f	O
=	O
m	O
e	O
(	O
cid:35	O
)	O
σif	O
(	O
zi	O
)	O
[	O
r	O
(	O
f	O
◦	O
s	O
)	O
]	O
=	O
2m	O
e	O
[	O
r	O
(	O
f	O
◦	O
s	O
)	O
]	O
.	O
s	O
the	O
lemma	O
immediately	O
yields	O
that	O
,	O
in	O
expectation	O
,	O
the	O
erm	O
rule	O
ﬁnds	O
a	O
hypothesis	B
which	O
is	O
close	O
to	O
the	O
optimal	O
hypothesis	B
in	O
h.	O
theorem	O
26.3	O
we	O
have	O
e	O
s∼dm	O
[	O
ld	O
(	O
ermh	O
(	O
s	O
)	O
)	O
−	O
ls	O
(	O
ermh	O
(	O
s	O
)	O
)	O
]	O
≤	O
2	O
e	O
s∼dm	O
furthermore	O
,	O
for	O
any	O
h	O
(	O
cid:63	O
)	O
∈	O
h	O
r	O
(	O
(	O
cid:96	O
)	O
◦	O
h	O
◦	O
s	O
)	O
.	O
e	O
s∼dm	O
[	O
ld	O
(	O
ermh	O
(	O
s	O
)	O
)	O
−	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
]	O
≤	O
2	O
e	O
s∼dm	O
r	O
(	O
(	O
cid:96	O
)	O
◦	O
h	O
◦	O
s	O
)	O
.	O
378	O
rademacher	O
complexities	O
furthermore	O
,	O
if	O
h	O
(	O
cid:63	O
)	O
=	O
argminh	O
ld	O
(	O
h	O
)	O
then	O
for	O
each	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
the	O
choice	O
of	O
s	O
we	O
have	O
ld	O
(	O
ermh	O
(	O
s	O
)	O
)	O
−	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
≤	O
2	O
es	O
(	O
cid:48	O
)	O
∼dm	O
r	O
(	O
(	O
cid:96	O
)	O
◦	O
h	O
◦	O
s	O
(	O
cid:48	O
)	O
)	O
δ	O
.	O
proof	O
the	O
ﬁrst	O
inequality	O
follows	O
directly	O
from	O
lemma	O
26.2.	O
the	O
second	O
in-	O
equality	O
follows	O
because	O
for	O
any	O
ﬁxed	O
h	O
(	O
cid:63	O
)	O
,	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
=	O
e	O
[	O
ls	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
]	O
≥	O
e	O
s	O
s	O
[	O
ls	O
(	O
ermh	O
(	O
s	O
)	O
)	O
]	O
.	O
the	O
third	O
inequality	O
follows	O
from	O
the	O
previous	O
inequality	O
by	O
relying	O
on	O
markov	O
’	O
s	O
inequality	O
(	O
note	O
that	O
the	O
random	O
variable	O
ld	O
(	O
ermh	O
(	O
s	O
)	O
)	O
−	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
is	O
nonnega-	O
tive	O
)	O
.	O
next	O
,	O
we	O
derive	O
bounds	O
similar	O
to	O
the	O
bounds	O
in	O
theorem	O
26.3	O
with	O
a	O
better	O
dependence	O
on	O
the	O
conﬁdence	B
parameter	O
δ.	O
to	O
do	O
so	O
,	O
we	O
ﬁrst	O
introduce	O
the	O
following	O
bounded	O
diﬀerences	O
concentration	O
inequality	O
.	O
lemma	O
26.4	O
(	O
mcdiarmid	O
’	O
s	O
inequality	O
)	O
let	O
v	O
be	O
some	O
set	B
and	O
let	O
f	O
:	O
v	O
m	O
→	O
r	O
be	O
a	O
function	B
of	O
m	O
variables	O
such	O
that	O
for	O
some	O
c	O
>	O
0	O
,	O
for	O
all	O
i	O
∈	O
[	O
m	O
]	O
and	O
for	O
all	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
,	O
x	O
(	O
cid:48	O
)	O
i	O
∈	O
v	O
we	O
have	O
|f	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
−	O
f	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xi−1	O
,	O
x	O
(	O
cid:48	O
)	O
i	O
,	O
xi+1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
|	O
≤	O
c.	O
let	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
be	O
m	O
independent	O
random	O
variables	O
taking	O
values	O
in	O
v	O
.	O
then	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
we	O
have	O
|f	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
−	O
e	O
[	O
f	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
]	O
|	O
≤	O
c	O
(	O
cid:113	O
)	O
ln	O
(	O
cid:0	O
)	O
2	O
δ	O
(	O
cid:1	O
)	O
m/2	O
.	O
on	O
the	O
basis	O
of	O
the	O
mcdiarmid	O
inequality	O
we	O
can	O
derive	O
generalization	B
bounds	I
with	O
a	O
better	O
dependence	O
on	O
the	O
conﬁdence	B
parameter	O
.	O
theorem	O
26.5	O
assume	O
that	O
for	O
all	O
z	O
and	O
h	O
∈	O
h	O
we	O
have	O
that	O
|	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
z	O
)	O
|	O
≤	O
c.	O
then	O
,	O
1.	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
,	O
for	O
all	O
h	O
∈	O
h	O
,	O
(	O
cid:114	O
)	O
ld	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
≤	O
2	O
e	O
s	O
(	O
cid:48	O
)	O
∼dm	O
r	O
(	O
(	O
cid:96	O
)	O
◦	O
h	O
◦	O
s	O
(	O
cid:48	O
)	O
)	O
+	O
c	O
2	O
ln	O
(	O
2/δ	O
)	O
m	O
.	O
in	O
particular	O
,	O
this	O
holds	O
for	O
h	O
=	O
ermh	O
(	O
s	O
)	O
.	O
2.	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
,	O
for	O
all	O
h	O
∈	O
h	O
,	O
ld	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
≤	O
2	O
r	O
(	O
(	O
cid:96	O
)	O
◦	O
h	O
◦	O
s	O
)	O
+	O
4	O
c	O
(	O
cid:114	O
)	O
2	O
ln	O
(	O
4/δ	O
)	O
m	O
.	O
in	O
particular	O
,	O
this	O
holds	O
for	O
h	O
=	O
ermh	O
(	O
s	O
)	O
.	O
3.	O
for	O
any	O
h	O
(	O
cid:63	O
)	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
,	O
ld	O
(	O
ermh	O
(	O
s	O
)	O
)	O
−	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
≤	O
2	O
r	O
(	O
(	O
cid:96	O
)	O
◦	O
h	O
◦	O
s	O
)	O
+	O
5	O
c	O
(	O
cid:114	O
)	O
2	O
ln	O
(	O
8/δ	O
)	O
m	O
.	O
26.1	O
the	O
rademacher	O
complexity	O
379	O
(	O
cid:114	O
)	O
(	O
cid:114	O
)	O
proof	O
first	O
note	O
that	O
the	O
random	O
variable	O
repd	O
(	O
f	O
,	O
s	O
)	O
=	O
suph∈h	O
(	O
ld	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
)	O
satisﬁes	O
the	O
bounded	O
diﬀerences	O
condition	O
of	O
lemma	O
26.4	O
with	O
a	O
constant	O
2c/m	O
.	O
combining	O
the	O
bounds	O
in	O
lemma	O
26.4	O
with	O
lemma	O
26.2	O
we	O
obtain	O
that	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
,	O
2	O
ln	O
(	O
2/δ	O
)	O
≤	O
2	O
e	O
m	O
s	O
(	O
cid:48	O
)	O
r	O
(	O
(	O
cid:96	O
)	O
◦	O
h	O
◦	O
s	O
(	O
cid:48	O
)	O
)	O
+	O
c	O
repd	O
(	O
f	O
,	O
s	O
)	O
≤	O
e	O
repd	O
(	O
f	O
,	O
s	O
)	O
+	O
c	O
.	O
the	O
ﬁrst	O
inequality	O
of	O
the	O
theorem	O
follows	O
from	O
the	O
deﬁnition	O
of	O
repd	O
(	O
f	O
,	O
s	O
)	O
.	O
for	O
the	O
second	O
inequality	O
we	O
note	O
that	O
the	O
random	O
variable	O
r	O
(	O
(	O
cid:96	O
)	O
◦	O
h	O
◦	O
s	O
)	O
also	O
satisﬁes	O
the	O
bounded	O
diﬀerences	O
condition	O
of	O
lemma	O
26.4	O
with	O
a	O
constant	O
2c/m	O
.	O
therefore	O
,	O
the	O
second	O
inequality	O
follows	O
from	O
the	O
ﬁrst	O
inequality	O
,	O
lemma	O
26.4	O
,	O
and	O
the	O
union	B
bound	I
.	O
finally	O
,	O
for	O
the	O
last	O
inequality	O
,	O
denote	O
hs	O
=	O
ermh	O
(	O
s	O
)	O
and	O
note	O
that	O
2	O
ln	O
(	O
2/δ	O
)	O
m	O
ld	O
(	O
hs	O
)	O
−	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
=	O
ld	O
(	O
hs	O
)	O
−	O
ls	O
(	O
hs	O
)	O
+	O
ls	O
(	O
hs	O
)	O
−	O
ls	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
+	O
ls	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
−	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
≤	O
(	O
ld	O
(	O
hs	O
)	O
−	O
ls	O
(	O
hs	O
)	O
)	O
+	O
(	O
ls	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
−	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
)	O
.	O
(	O
26.10	O
)	O
the	O
ﬁrst	O
summand	O
on	O
the	O
right-hand	O
side	O
is	O
bounded	O
by	O
the	O
second	O
inequality	O
of	O
the	O
theorem	O
.	O
for	O
the	O
second	O
summand	O
,	O
we	O
use	O
the	O
fact	O
that	O
h	O
(	O
cid:63	O
)	O
does	O
not	O
depend	O
on	O
s	O
;	O
hence	O
by	O
using	O
hoeﬀding	O
’	O
s	O
inequality	O
we	O
obtain	O
that	O
with	O
probaility	O
of	O
at	O
least	O
1	O
−	O
δ/2	O
,	O
(	O
cid:114	O
)	O
ls	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
−	O
ld	O
(	O
h	O
(	O
cid:63	O
)	O
)	O
≤	O
c	O
ln	O
(	O
4/δ	O
)	O
2m	O
.	O
(	O
26.11	O
)	O
combining	O
this	O
with	O
the	O
union	B
bound	I
we	O
conclude	O
our	O
proof	O
.	O
the	O
preceding	O
theorem	O
tells	O
us	O
that	O
if	O
the	O
quantity	O
r	O
(	O
(	O
cid:96	O
)	O
◦h◦	O
s	O
)	O
is	O
small	O
then	O
it	O
is	O
possible	O
to	O
learn	O
the	O
class	O
h	O
using	O
the	O
erm	O
rule	O
.	O
it	O
is	O
important	O
to	O
emphasize	O
that	O
the	O
last	O
two	O
bounds	O
given	O
in	O
the	O
theorem	O
depend	O
on	O
the	O
speciﬁc	O
training	B
set	I
s.	O
that	O
is	O
,	O
we	O
use	O
s	O
both	O
for	O
learning	O
a	O
hypothesis	B
from	O
h	O
as	O
well	O
as	O
for	O
estimating	O
the	O
quality	O
of	O
it	O
.	O
this	O
type	O
of	O
bound	O
is	O
called	O
a	O
data-dependent	O
bound	O
.	O
26.1.1	O
rademacher	O
calculus	O
let	O
us	O
now	O
discuss	O
some	O
properties	O
of	O
the	O
rademacher	O
complexity	O
measure	O
.	O
these	O
properties	O
will	O
help	O
us	O
in	O
deriving	O
some	O
simple	O
bounds	O
on	O
r	O
(	O
(	O
cid:96	O
)	O
◦h	O
◦	O
s	O
)	O
for	O
speciﬁc	O
cases	O
of	O
interest	O
.	O
the	O
following	O
lemma	O
is	O
immediate	O
from	O
the	O
deﬁnition	O
.	O
lemma	O
26.6	O
for	O
any	O
a	O
⊂	O
rm	O
,	O
scalar	O
c	O
∈	O
r	O
,	O
and	O
vector	O
a0	O
∈	O
rm	O
,	O
we	O
have	O
r	O
(	O
{	O
c	O
a	O
+	O
a0	O
:	O
a	O
∈	O
a	O
}	O
)	O
≤	O
|c|	O
r	O
(	O
a	O
)	O
.	O
the	O
following	O
lemma	O
tells	O
us	O
that	O
the	O
convex	B
hull	O
of	O
a	O
has	O
the	O
same	O
complexity	O
as	O
a	O
.	O
380	O
rademacher	O
complexities	O
lemma	O
26.7	O
let	O
a	O
be	O
a	O
subset	O
of	O
rm	O
and	O
let	O
a	O
(	O
cid:48	O
)	O
=	O
{	O
(	O
cid:80	O
)	O
n	O
n	O
,	O
∀j	O
,	O
a	O
(	O
j	O
)	O
∈	O
a	O
,	O
αj	O
≥	O
0	O
,	O
(	O
cid:107	O
)	O
α	O
(	O
cid:107	O
)	O
1	O
=	O
1	O
}	O
.	O
then	O
,	O
r	O
(	O
a	O
(	O
cid:48	O
)	O
)	O
=	O
r	O
(	O
a	O
)	O
.	O
proof	O
the	O
main	O
idea	O
follows	O
from	O
the	O
fact	O
that	O
for	O
any	O
vector	O
v	O
we	O
have	O
j=1	O
αja	O
(	O
j	O
)	O
:	O
n	O
∈	O
sup	O
α≥0	O
:	O
(	O
cid:107	O
)	O
α	O
(	O
cid:107	O
)	O
1=1	O
therefore	O
,	O
αjvj	O
=	O
max	O
j	O
vj	O
.	O
n	O
(	O
cid:88	O
)	O
j=1	O
αja	O
(	O
j	O
)	O
i	O
n	O
(	O
cid:88	O
)	O
j=1	O
σi	O
σia	O
(	O
j	O
)	O
i	O
m	O
(	O
cid:88	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
i=1	O
m	O
r	O
(	O
a	O
(	O
cid:48	O
)	O
)	O
=	O
e	O
σ	O
sup	O
α≥0	O
:	O
(	O
cid:107	O
)	O
α	O
(	O
cid:107	O
)	O
1=1	O
sup	O
a	O
(	O
1	O
)	O
,	O
...	O
,	O
a	O
(	O
n	O
)	O
n	O
(	O
cid:88	O
)	O
j=1	O
=	O
e	O
σ	O
α≥0	O
:	O
(	O
cid:107	O
)	O
α	O
(	O
cid:107	O
)	O
1=1	O
αj	O
sup	O
a	O
(	O
j	O
)	O
sup	O
m	O
(	O
cid:88	O
)	O
σiai	O
=	O
e	O
σ	O
sup	O
a∈a	O
i=1	O
=	O
m	O
r	O
(	O
a	O
)	O
,	O
and	O
we	O
conclude	O
our	O
proof	O
.	O
the	O
next	O
lemma	O
,	O
due	O
to	O
massart	O
,	O
states	O
that	O
the	O
rademacher	O
complexity	O
of	O
a	O
ﬁnite	O
set	B
grows	O
logarithmically	O
with	O
the	O
size	O
of	O
the	O
set	B
.	O
lemma	O
26.8	O
(	O
massart	O
lemma	O
)	O
let	O
a	O
=	O
{	O
a1	O
,	O
.	O
.	O
.	O
,	O
an	O
}	O
be	O
a	O
ﬁnite	O
set	B
of	O
vectors	O
in	O
rm	O
.	O
deﬁne	O
¯a	O
=	O
1	O
(	O
cid:80	O
)	O
n	O
n	O
i=1	O
ai	O
.	O
then	O
,	O
r	O
(	O
a	O
)	O
≤	O
max	O
a∈a	O
(	O
cid:107	O
)	O
a	O
−	O
¯a	O
(	O
cid:107	O
)	O
(	O
cid:112	O
)	O
2	O
log	O
(	O
n	O
)	O
.	O
m	O
proof	O
based	O
on	O
lemma	O
26.6	O
,	O
we	O
can	O
assume	O
without	O
loss	B
of	O
generality	O
that	O
¯a	O
=	O
0.	O
let	O
λ	O
>	O
0	O
and	O
let	O
a	O
(	O
cid:48	O
)	O
=	O
{	O
λa1	O
,	O
.	O
.	O
.	O
,	O
λan	O
}	O
.	O
we	O
upper	O
bound	O
the	O
rademacher	O
complexity	O
as	O
follows	O
:	O
(	O
cid:18	O
)	O
a∈a	O
(	O
cid:48	O
)	O
e	O
(	O
cid:104	O
)	O
σ	O
,	O
a	O
(	O
cid:105	O
)	O
(	O
cid:19	O
)	O
(	O
cid:21	O
)	O
max	O
=	O
e	O
log	O
mr	O
(	O
a	O
(	O
cid:48	O
)	O
)	O
=	O
e	O
σ	O
≤	O
e	O
σ	O
σ	O
(	O
cid:20	O
)	O
(	O
cid:33	O
)	O
(	O
cid:35	O
)	O
(	O
cid:35	O
)	O
(	O
cid:33	O
)	O
(	O
cid:33	O
)	O
max	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
(	O
cid:34	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
a∈a	O
(	O
cid:48	O
)	O
(	O
cid:104	O
)	O
σ	O
,	O
a	O
(	O
cid:105	O
)	O
(	O
cid:34	O
)	O
(	O
cid:88	O
)	O
(	O
cid:32	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
m	O
(	O
cid:89	O
)	O
a∈a	O
(	O
cid:48	O
)	O
a∈a	O
(	O
cid:48	O
)	O
e	O
σ	O
log	O
a∈a	O
(	O
cid:48	O
)	O
i=1	O
e	O
(	O
cid:104	O
)	O
σ	O
,	O
a	O
(	O
cid:105	O
)	O
e	O
(	O
cid:104	O
)	O
σ	O
,	O
a	O
(	O
cid:105	O
)	O
e	O
σi	O
[	O
eσiai	O
]	O
,	O
≤	O
log	O
=	O
log	O
//	O
jensen	O
’	O
s	O
inequality	O
where	O
the	O
last	O
equality	O
occurs	O
because	O
the	O
rademacher	O
variables	O
are	O
indepen-	O
dent	O
.	O
next	O
,	O
using	O
lemma	O
a.6	O
we	O
have	O
that	O
for	O
all	O
ai	O
∈	O
r	O
,	O
≤	O
exp	O
(	O
a2	O
exp	O
(	O
ai	O
)	O
+	O
exp	O
(	O
−ai	O
)	O
eσiai	O
=	O
i	O
/2	O
)	O
,	O
e	O
σi	O
2	O
26.1	O
the	O
rademacher	O
complexity	O
381	O
and	O
therefore	O
mr	O
(	O
a	O
(	O
cid:48	O
)	O
)	O
≤	O
log	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
(	O
cid:18	O
)	O
(	O
cid:18	O
)	O
a2	O
(	O
cid:19	O
)	O
(	O
cid:33	O
)	O
m	O
(	O
cid:89	O
)	O
a∈a	O
(	O
cid:48	O
)	O
exp	O
(	O
cid:0	O
)	O
(	O
cid:107	O
)	O
a	O
(	O
cid:107	O
)	O
2/2	O
(	O
cid:1	O
)	O
(	O
cid:19	O
)	O
exp	O
i	O
2	O
i=1	O
a∈a	O
(	O
cid:48	O
)	O
|a	O
(	O
cid:48	O
)	O
|	O
max	O
=	O
log	O
exp	O
(	O
cid:0	O
)	O
(	O
cid:107	O
)	O
a	O
(	O
cid:107	O
)	O
2/2	O
(	O
cid:1	O
)	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
a∈a	O
(	O
cid:48	O
)	O
since	O
r	O
(	O
a	O
)	O
=	O
1	O
=	O
log	O
(	O
|a	O
(	O
cid:48	O
)	O
|	O
)	O
+	O
max	O
a∈a	O
(	O
cid:48	O
)	O
(	O
(	O
cid:107	O
)	O
a	O
(	O
cid:107	O
)	O
2/2	O
)	O
.	O
≤	O
log	O
λ	O
r	O
(	O
a	O
(	O
cid:48	O
)	O
)	O
we	O
obtain	O
from	O
the	O
equation	O
that	O
r	O
(	O
a	O
)	O
≤	O
log	O
(	O
|a|	O
)	O
+	O
λ2	O
maxa∈a	O
(	O
(	O
cid:107	O
)	O
a	O
(	O
cid:107	O
)	O
2/2	O
)	O
setting	O
λ	O
=	O
(	O
cid:112	O
)	O
2	O
log	O
(	O
|a|	O
)	O
/	O
maxa∈a	O
(	O
cid:107	O
)	O
a	O
(	O
cid:107	O
)	O
2	O
and	O
rearranging	O
terms	O
we	O
conclude	O
our	O
λm	O
.	O
proof	O
.	O
the	O
following	O
lemma	O
shows	O
that	O
composing	O
a	O
with	O
a	O
lipschitz	O
function	B
does	O
not	O
blow	O
up	O
the	O
rademacher	O
complexity	O
.	O
the	O
proof	O
is	O
due	O
to	O
kakade	O
and	O
tewari	O
.	O
lemma	O
26.9	O
(	O
contraction	B
lemma	I
)	O
for	O
each	O
i	O
∈	O
[	O
m	O
]	O
,	O
let	O
φi	O
:	O
r	O
→	O
r	O
be	O
a	O
ρ-	O
lipschitz	O
function	B
,	O
namely	O
for	O
all	O
α	O
,	O
β	O
∈	O
r	O
we	O
have	O
|φi	O
(	O
α	O
)	O
−	O
φi	O
(	O
β	O
)	O
|	O
≤	O
ρ|α	O
−	O
β|	O
.	O
for	O
a	O
∈	O
rm	O
let	O
φ	O
(	O
a	O
)	O
denote	O
the	O
vector	O
(	O
φ1	O
(	O
a1	O
)	O
,	O
.	O
.	O
.	O
,	O
φm	O
(	O
ym	O
)	O
)	O
.	O
let	O
φ◦	O
a	O
=	O
{	O
φ	O
(	O
a	O
)	O
:	O
a	O
∈	O
a	O
}	O
.	O
then	O
,	O
r	O
(	O
φ	O
◦	O
a	O
)	O
≤	O
ρ	O
r	O
(	O
a	O
)	O
.	O
proof	O
for	O
simplicity	O
,	O
we	O
prove	O
the	O
lemma	O
for	O
the	O
case	O
ρ	O
=	O
1.	O
the	O
case	O
ρ	O
(	O
cid:54	O
)	O
=	O
1	O
will	O
follow	O
by	O
deﬁning	O
φ	O
(	O
cid:48	O
)	O
=	O
1	O
ρ	O
φ	O
and	O
then	O
using	O
lemma	O
26.6.	O
let	O
ai	O
=	O
{	O
(	O
a1	O
,	O
.	O
.	O
.	O
,	O
ai−1	O
,	O
φi	O
(	O
ai	O
)	O
,	O
ai+1	O
,	O
.	O
.	O
.	O
,	O
am	O
)	O
:	O
a	O
∈	O
a	O
}	O
.	O
clearly	O
,	O
it	O
suﬃces	O
to	O
prove	O
that	O
for	O
any	O
set	B
a	O
and	O
all	O
i	O
we	O
have	O
r	O
(	O
ai	O
)	O
≤	O
r	O
(	O
a	O
)	O
.	O
without	O
loss	B
of	O
generality	O
we	O
will	O
prove	O
the	O
latter	O
claim	O
for	O
i	O
=	O
1	O
and	O
to	O
simplify	O
notation	O
we	O
omit	O
the	O
subscript	O
from	O
φ1	O
.	O
we	O
have	O
(	O
cid:34	O
)	O
(	O
cid:34	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
σiai	O
(	O
cid:35	O
)	O
(	O
cid:32	O
)	O
m	O
(	O
cid:88	O
)	O
i=2	O
σ1φ	O
(	O
a1	O
)	O
+	O
σiai	O
φ	O
(	O
a1	O
)	O
+	O
(	O
cid:35	O
)	O
m	O
(	O
cid:88	O
)	O
i=2	O
(	O
cid:33	O
)	O
σiai	O
mr	O
(	O
a1	O
)	O
=	O
e	O
σ	O
=	O
e	O
σ	O
=	O
=	O
1	O
2	O
1	O
2	O
≤	O
1	O
2	O
sup	O
a∈a1	O
sup	O
a∈a	O
e	O
σ2	O
,	O
...	O
,	O
σm	O
e	O
σ2	O
,	O
...	O
,	O
σm	O
e	O
σ2	O
,	O
...	O
,	O
σm	O
(	O
cid:34	O
)	O
(	O
cid:34	O
)	O
(	O
cid:34	O
)	O
(	O
cid:32	O
)	O
(	O
cid:32	O
)	O
sup	O
a∈a	O
sup	O
a	O
,	O
a	O
(	O
cid:48	O
)	O
∈a	O
sup	O
a	O
,	O
a	O
(	O
cid:48	O
)	O
∈a	O
(	O
cid:33	O
)	O
(	O
cid:35	O
)	O
σiai	O
m	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
(	O
cid:35	O
)	O
i=2	O
(	O
cid:32	O
)	O
+	O
sup	O
a∈a	O
m	O
(	O
cid:88	O
)	O
−φ	O
(	O
a1	O
)	O
+	O
m	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
(	O
cid:35	O
)	O
i=2	O
σia	O
(	O
cid:48	O
)	O
i	O
,	O
(	O
26.12	O
)	O
φ	O
(	O
a1	O
)	O
−	O
φ	O
(	O
a	O
(	O
cid:48	O
)	O
|a1	O
−	O
a	O
(	O
cid:48	O
)	O
1|	O
+	O
1	O
)	O
+	O
m	O
(	O
cid:88	O
)	O
i=2	O
σiai	O
+	O
σiai	O
+	O
m	O
(	O
cid:88	O
)	O
i=2	O
i=2	O
σia	O
(	O
cid:48	O
)	O
i	O
where	O
in	O
the	O
last	O
inequality	O
we	O
used	O
the	O
assumption	O
that	O
φ	O
is	O
lipschitz	O
.	O
next	O
,	O
we	O
note	O
that	O
the	O
absolute	O
value	O
on	O
|a1	O
−	O
a	O
(	O
cid:48	O
)	O
1|	O
in	O
the	O
preceding	O
expression	O
can	O
382	O
rademacher	O
complexities	O
be	O
omitted	O
since	O
both	O
a	O
and	O
a	O
(	O
cid:48	O
)	O
are	O
from	O
the	O
same	O
set	B
a	O
and	O
the	O
rest	O
of	O
the	O
expression	O
in	O
the	O
supremum	O
is	O
not	O
aﬀected	O
by	O
replacing	O
a	O
and	O
a	O
(	O
cid:48	O
)	O
.	O
therefore	O
,	O
(	O
cid:34	O
)	O
(	O
cid:32	O
)	O
m	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
(	O
cid:35	O
)	O
m	O
(	O
cid:88	O
)	O
σia	O
(	O
cid:48	O
)	O
i	O
mr	O
(	O
a1	O
)	O
≤	O
1	O
2	O
e	O
σ2	O
,	O
...	O
,	O
σm	O
sup	O
a	O
,	O
a	O
(	O
cid:48	O
)	O
∈a	O
a1	O
−	O
a	O
(	O
cid:48	O
)	O
1	O
+	O
σiai	O
+	O
i=2	O
i=2	O
.	O
(	O
26.13	O
)	O
but	O
,	O
using	O
the	O
same	O
equalities	O
as	O
in	O
equation	O
(	O
26.12	O
)	O
,	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
right-hand	O
side	O
of	O
equation	O
(	O
26.13	O
)	O
exactly	O
equals	O
m	O
r	O
(	O
a	O
)	O
,	O
which	O
concludes	O
our	O
proof	O
.	O
26.2	O
rademacher	O
complexity	O
of	O
linear	O
classes	O
,	O
in	O
this	O
section	O
we	O
analyze	O
the	O
rademacher	O
complexity	O
of	O
linear	O
classes	O
.	O
to	O
sim-	O
plify	O
the	O
derivation	O
we	O
ﬁrst	O
deﬁne	O
the	O
following	O
two	O
classes	O
:	O
h1	O
=	O
{	O
x	O
(	O
cid:55	O
)	O
→	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
1	O
≤	O
1	O
}	O
h2	O
=	O
{	O
x	O
(	O
cid:55	O
)	O
→	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
≤	O
1	O
}	O
.	O
(	O
26.14	O
)	O
the	O
following	O
lemma	O
bounds	O
the	O
rademacher	O
complexity	O
of	O
h2	O
.	O
we	O
allow	O
the	O
xi	O
to	O
be	O
vectors	O
in	O
any	O
hilbert	O
space	O
(	O
even	O
inﬁnite	O
dimensional	O
)	O
,	O
and	O
the	O
bound	O
does	O
not	O
depend	O
on	O
the	O
dimensionality	O
of	O
the	O
hilbert	O
space	O
.	O
this	O
property	O
becomes	O
useful	O
when	O
analyzing	O
kernel	O
methods	O
.	O
lemma	O
26.10	O
let	O
s	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
be	O
vectors	O
in	O
a	O
hilbert	O
space	O
.	O
deﬁne	O
:	O
h2	O
◦	O
s	O
=	O
{	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x1	O
(	O
cid:105	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
cid:104	O
)	O
w	O
,	O
xm	O
(	O
cid:105	O
)	O
)	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
≤	O
1	O
}	O
.	O
then	O
,	O
√	O
r	O
(	O
h2	O
◦	O
s	O
)	O
≤	O
maxi	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
2	O
.	O
m	O
proof	O
using	O
cauchy-schwartz	O
inequality	O
we	O
know	O
that	O
for	O
any	O
vectors	O
w	O
,	O
v	O
we	O
have	O
(	O
cid:104	O
)	O
w	O
,	O
v	O
(	O
cid:105	O
)	O
≤	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
.	O
therefore	O
,	O
(	O
26.15	O
)	O
mr	O
(	O
h2	O
◦	O
s	O
)	O
=	O
e	O
σ	O
=	O
e	O
σ	O
=	O
e	O
σ	O
≤	O
e	O
σ	O
(	O
cid:35	O
)	O
(	O
cid:35	O
)	O
σi	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
σixi	O
(	O
cid:105	O
)	O
(	O
cid:34	O
)	O
(	O
cid:34	O
)	O
(	O
cid:34	O
)	O
(	O
cid:34	O
)	O
(	O
cid:35	O
)	O
m	O
(	O
cid:88	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
sup	O
a∈h2◦s	O
σiai	O
sup	O
i=1	O
i=1	O
sup	O
(	O
cid:104	O
)	O
w	O
,	O
w	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
≤1	O
w	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
≤1	O
σixi	O
(	O
cid:107	O
)	O
2	O
m	O
(	O
cid:88	O
)	O
(	O
cid:35	O
)	O
(	O
cid:107	O
)	O
m	O
(	O
cid:88	O
)	O
1/2	O
≤	O
e	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
2	O
i=1	O
σ	O
.	O
σixi	O
2	O
next	O
,	O
using	O
jensen	O
’	O
s	O
inequality	O
we	O
have	O
that	O
(	O
cid:34	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
e	O
σ	O
(	O
cid:35	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
2	O
σixi	O
=	O
e	O
σ	O
	O
	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
2	O
2	O
σixi	O
1/2	O
.	O
(	O
26.16	O
)	O
26.3	O
generalization	B
bounds	I
for	O
svm	O
383	O
finally	O
,	O
since	O
the	O
variables	O
σ1	O
,	O
.	O
.	O
.	O
,	O
σm	O
are	O
independent	O
we	O
have	O
σixi	O
(	O
cid:107	O
)	O
2	O
2	O
=	O
e	O
σiσj	O
(	O
cid:104	O
)	O
xi	O
,	O
xj	O
(	O
cid:105	O
)	O
(	O
cid:34	O
)	O
(	O
cid:107	O
)	O
m	O
(	O
cid:88	O
)	O
e	O
σ	O
(	O
cid:35	O
)	O
i=1	O
=	O
=	O
σ	O
	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
m	O
(	O
cid:88	O
)	O
i	O
(	O
cid:54	O
)	O
=j	O
i	O
,	O
j	O
i=1	O
	O
m	O
(	O
cid:88	O
)	O
i=1	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
2	O
2	O
.	O
(	O
cid:3	O
)	O
(	O
cid:2	O
)	O
σ2	O
i	O
(	O
cid:104	O
)	O
xi	O
,	O
xj	O
(	O
cid:105	O
)	O
e	O
σ	O
[	O
σiσj	O
]	O
+	O
(	O
cid:104	O
)	O
xi	O
,	O
xi	O
(	O
cid:105	O
)	O
e	O
σ	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
2	O
2	O
≤	O
m	O
max	O
i	O
combining	O
this	O
with	O
equation	O
(	O
26.15	O
)	O
and	O
equation	O
(	O
26.16	O
)	O
we	O
conclude	O
our	O
proof	O
.	O
next	O
we	O
bound	O
the	O
rademacher	O
complexity	O
of	O
h1	O
◦	O
s.	O
lemma	O
26.11	O
let	O
s	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
be	O
vectors	O
in	O
rn	O
.	O
then	O
,	O
proof	O
using	O
holder	O
’	O
s	O
inequality	O
we	O
know	O
that	O
for	O
any	O
vectors	O
w	O
,	O
v	O
we	O
have	O
(	O
cid:104	O
)	O
w	O
,	O
v	O
(	O
cid:105	O
)	O
≤	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
1	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
∞	O
.	O
therefore	O
,	O
r	O
(	O
h1	O
◦	O
s	O
)	O
≤	O
max	O
i	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
∞	O
2	O
log	O
(	O
2n	O
)	O
m	O
.	O
(	O
cid:114	O
)	O
m	O
(	O
cid:88	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
i=1	O
(	O
cid:104	O
)	O
w	O
,	O
(	O
cid:35	O
)	O
(	O
cid:35	O
)	O
(	O
cid:35	O
)	O
σi	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
σixi	O
(	O
cid:105	O
)	O
m	O
(	O
cid:88	O
)	O
(	O
cid:35	O
)	O
i=1	O
(	O
cid:34	O
)	O
(	O
cid:34	O
)	O
(	O
cid:34	O
)	O
(	O
cid:34	O
)	O
mr	O
(	O
h1	O
◦	O
s	O
)	O
=	O
e	O
σ	O
=	O
e	O
σ	O
=	O
e	O
σ	O
sup	O
a∈h1◦s	O
σiai	O
sup	O
w	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
1≤1	O
sup	O
w	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
1≤1	O
(	O
cid:107	O
)	O
m	O
(	O
cid:88	O
)	O
σixi	O
(	O
cid:107	O
)	O
∞	O
≤	O
e	O
σ	O
.	O
(	O
26.17	O
)	O
for	O
each	O
j	O
∈	O
[	O
n	O
]	O
,	O
let	O
vj	O
=	O
(	O
x1	O
,	O
j	O
,	O
.	O
.	O
.	O
,	O
xm	O
,	O
j	O
)	O
∈	O
rm	O
.	O
note	O
that	O
(	O
cid:107	O
)	O
vj	O
(	O
cid:107	O
)	O
2	O
≤	O
√	O
let	O
v	O
=	O
{	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
,	O
−v1	O
,	O
.	O
.	O
.	O
,	O
−vn	O
}	O
.	O
the	O
right-hand	O
side	O
of	O
equation	O
(	O
26.17	O
)	O
is	O
m	O
r	O
(	O
v	O
)	O
.	O
using	O
massart	O
lemma	O
(	O
lemma	O
26.8	O
)	O
we	O
have	O
that	O
m	O
maxi	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
∞	O
.	O
i=1	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
∞	O
(	O
cid:112	O
)	O
2	O
log	O
(	O
2n	O
)	O
/m	O
,	O
r	O
(	O
v	O
)	O
≤	O
max	O
i	O
which	O
concludes	O
our	O
proof	O
.	O
26.3	O
generalization	B
bounds	I
for	O
svm	O
in	O
this	O
section	O
we	O
use	O
rademacher	O
complexity	O
to	O
derive	O
generalization	B
bounds	I
for	O
generalized	O
linear	O
predictors	O
with	O
euclidean	O
norm	O
constraint	O
.	O
we	O
will	O
show	O
how	O
this	O
leads	O
to	O
generalization	B
bounds	I
for	O
hard-svm	O
and	O
soft-svm	O
.	O
384	O
rademacher	O
complexities	O
we	O
shall	O
consider	O
the	O
following	O
general	O
constraint-based	O
formulation	O
.	O
let	O
h	O
=	O
{	O
w	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
≤	O
b	O
}	O
be	O
our	O
hypothesis	B
class	I
,	O
and	O
let	O
z	O
=	O
x	O
×	O
y	O
be	O
the	O
examples	O
domain	B
.	O
assume	O
that	O
the	O
loss	B
function	I
(	O
cid:96	O
)	O
:	O
h	O
×	O
z	O
→	O
r	O
is	O
of	O
the	O
form	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
φ	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
,	O
y	O
)	O
,	O
(	O
26.18	O
)	O
where	O
φ	O
:	O
r	O
×	O
y	O
→	O
r	O
is	O
such	O
that	O
for	O
all	O
y	O
∈	O
y	O
,	O
the	O
scalar	O
function	B
a	O
(	O
cid:55	O
)	O
→	O
φ	O
(	O
a	O
,	O
y	O
)	O
is	O
ρ-lipschitz	O
.	O
for	O
example	O
,	O
the	O
hinge-loss	O
function	B
,	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
max	O
{	O
0	O
,	O
1	O
−	O
y	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
}	O
,	O
can	O
be	O
written	O
as	O
in	O
equation	O
(	O
26.18	O
)	O
using	O
φ	O
(	O
a	O
,	O
y	O
)	O
=	O
max	O
{	O
0	O
,	O
1	O
−	O
ya	O
}	O
,	O
and	O
note	O
that	O
φ	O
is	O
1-lipschitz	O
for	O
all	O
y	O
∈	O
{	O
±1	O
}	O
.	O
another	O
example	O
is	O
the	O
absolute	O
loss	O
function	B
,	O
(	O
cid:96	O
)	O
(	O
w	O
,	O
(	O
x	O
,	O
y	O
)	O
)	O
=	O
|	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
−	O
y|	O
,	O
which	O
can	O
be	O
written	O
as	O
in	O
equation	O
(	O
26.18	O
)	O
using	O
φ	O
(	O
a	O
,	O
y	O
)	O
=	O
|a	O
−	O
y|	O
,	O
which	O
is	O
also	O
1-lipschitz	O
for	O
all	O
y	O
∈	O
r.	O
the	O
following	O
theorem	O
bounds	O
the	O
generalization	B
error	I
of	O
all	O
predictors	O
in	O
h	O
using	O
their	O
empirical	B
error	I
.	O
theorem	O
26.12	O
suppose	O
that	O
d	O
is	O
a	O
distribution	O
over	O
x	O
×	O
y	O
such	O
that	O
with	O
probability	O
1	O
we	O
have	O
that	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
≤	O
r.	O
let	O
h	O
=	O
{	O
w	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
≤	O
b	O
}	O
and	O
let	O
(	O
cid:96	O
)	O
:	O
h	O
×	O
z	O
→	O
r	O
be	O
a	O
loss	B
function	I
of	O
the	O
form	O
given	O
in	O
equation	O
(	O
26.18	O
)	O
such	O
that	O
for	O
all	O
y	O
∈	O
y	O
,	O
a	O
(	O
cid:55	O
)	O
→	O
φ	O
(	O
a	O
,	O
y	O
)	O
is	O
a	O
ρ-lipschitz	O
function	B
and	O
such	O
that	O
maxa∈	O
[	O
−br	O
,	O
br	O
]	O
|φ	O
(	O
a	O
,	O
y	O
)	O
|	O
≤	O
c.	O
then	O
,	O
for	O
any	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
the	O
choice	O
of	O
an	O
i.i.d	O
.	O
sample	O
of	O
size	O
m	O
,	O
(	O
cid:114	O
)	O
∀w	O
∈	O
h	O
,	O
ld	O
(	O
w	O
)	O
≤	O
ls	O
(	O
w	O
)	O
+	O
2ρbr√	O
m	O
+	O
c	O
2	O
ln	O
(	O
2/δ	O
)	O
m	O
.	O
proof	O
let	O
f	O
=	O
{	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:55	O
)	O
→	O
φ	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
,	O
y	O
)	O
:	O
w	O
∈	O
h	O
}	O
.	O
we	O
will	O
show	O
that	O
with	O
√	O
probability	O
1	O
,	O
r	O
(	O
f	O
◦	O
s	O
)	O
≤	O
ρbr/	O
m	O
and	O
then	O
the	O
theorem	O
will	O
follow	O
from	O
theorem	O
26.5.	O
indeed	O
,	O
the	O
set	B
f	O
◦	O
s	O
can	O
be	O
written	O
as	O
f	O
◦	O
s	O
=	O
{	O
(	O
φ	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x1	O
(	O
cid:105	O
)	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
φ	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
xm	O
(	O
cid:105	O
)	O
,	O
ym	O
)	O
)	O
:	O
w	O
∈	O
h	O
}	O
,	O
and	O
the	O
bound	O
on	O
r	O
(	O
f◦s	O
)	O
follows	O
directly	O
by	O
combining	O
lemma	O
26.9	O
,	O
lemma	O
26.10	O
,	O
and	O
the	O
assumption	O
that	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
≤	O
r	O
with	O
probability	O
1.	O
we	O
next	O
derive	O
a	O
generalization	O
bound	O
for	O
hard-svm	O
based	O
on	O
the	O
previous	O
theorem	O
.	O
for	O
simplicity	O
,	O
we	O
do	O
not	O
allow	O
a	O
bias	B
term	O
and	O
consider	O
the	O
hard-svm	O
problem	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
s.t	O
.	O
∀i	O
,	O
yi	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
≥	O
1	O
argmin	O
w	O
(	O
26.19	O
)	O
theorem	O
26.13	O
consider	O
a	O
distribution	O
d	O
over	O
x	O
×	O
{	O
±1	O
}	O
such	O
that	O
there	O
exists	O
some	O
vector	O
w	O
(	O
cid:63	O
)	O
with	O
p	O
(	O
x	O
,	O
y	O
)	O
∼d	O
[	O
y	O
(	O
cid:104	O
)	O
w	O
(	O
cid:63	O
)	O
,	O
x	O
(	O
cid:105	O
)	O
≥	O
1	O
]	O
=	O
1	O
and	O
such	O
that	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
≤	O
r	O
with	O
probability	O
1.	O
let	O
ws	O
be	O
the	O
output	O
of	O
equation	O
(	O
26.19	O
)	O
.	O
then	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
the	O
choice	O
of	O
s	O
∼	O
dm	O
,	O
we	O
have	O
that	O
(	O
x	O
,	O
y	O
)	O
∼d	O
[	O
y	O
(	O
cid:54	O
)	O
=	O
sign	O
(	O
(	O
cid:104	O
)	O
ws	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
]	O
≤	O
2	O
r	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
√	O
p	O
m	O
+	O
(	O
1	O
+	O
r	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
)	O
2	O
ln	O
(	O
2/δ	O
)	O
m	O
.	O
(	O
cid:114	O
)	O
26.3	O
generalization	B
bounds	I
for	O
svm	O
385	O
proof	O
throughout	O
the	O
proof	O
,	O
let	O
the	O
loss	B
function	I
be	O
the	O
ramp	B
loss	I
(	O
see	O
sec-	O
tion	O
15.2.3	O
)	O
.	O
note	O
that	O
the	O
range	O
of	O
the	O
ramp	B
loss	I
is	O
[	O
0	O
,	O
1	O
]	O
and	O
that	O
it	O
is	O
a	O
1-lipschitz	O
function	B
.	O
since	O
the	O
ramp	B
loss	I
upper	O
bounds	O
the	O
zero-one	O
loss	B
,	O
we	O
have	O
that	O
p	O
(	O
x	O
,	O
y	O
)	O
∼d	O
[	O
y	O
(	O
cid:54	O
)	O
=	O
sign	O
(	O
(	O
cid:104	O
)	O
ws	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
]	O
≤	O
ld	O
(	O
ws	O
)	O
.	O
let	O
b	O
=	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
and	O
consider	O
the	O
set	B
h	O
=	O
{	O
w	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
≤	O
b	O
}	O
.	O
by	O
the	O
deﬁnition	O
of	O
hard-svm	O
and	O
our	O
assumption	O
on	O
the	O
distribution	O
,	O
we	O
have	O
that	O
ws	O
∈	O
h	O
with	O
probability	O
1	O
and	O
that	O
ls	O
(	O
ws	O
)	O
=	O
0.	O
therefore	O
,	O
using	O
theorem	O
26.12	O
we	O
have	O
that	O
(	O
cid:114	O
)	O
ld	O
(	O
ws	O
)	O
≤	O
ls	O
(	O
ws	O
)	O
+	O
2br√	O
m	O
+	O
2	O
ln	O
(	O
2/δ	O
)	O
m	O
.	O
2	O
remark	O
26.1	O
theorem	O
26.13	O
implies	O
that	O
the	O
sample	B
complexity	I
of	O
hard-svm	O
grows	O
like	O
r2	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
.	O
using	O
a	O
more	O
delicate	O
analysis	O
and	O
the	O
separability	O
assump-	O
tion	O
,	O
it	O
is	O
possible	O
to	O
improve	O
the	O
bound	O
to	O
an	O
order	O
of	O
r2	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
2	O
the	O
bound	O
in	O
the	O
preceding	O
theorem	O
depends	O
on	O
(	O
cid:107	O
)	O
w	O
(	O
cid:63	O
)	O
(	O
cid:107	O
)	O
,	O
which	O
is	O
unknown	O
.	O
in	O
the	O
following	O
we	O
derive	O
a	O
bound	O
that	O
depends	O
on	O
the	O
norm	O
of	O
the	O
output	O
of	O
svm	O
;	O
hence	O
it	O
can	O
be	O
calculated	O
from	O
the	O
training	B
set	I
itself	O
.	O
the	O
proof	O
is	O
similar	O
to	O
the	O
derivation	O
of	O
bounds	O
for	O
structure	O
risk	B
minimization	O
(	O
srm	O
)	O
.	O
.	O
	O
theorem	O
26.14	O
assume	O
that	O
the	O
conditions	O
of	O
theorem	O
26.13	O
hold	O
.	O
then	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
the	O
choice	O
of	O
s	O
∼	O
dm	O
,	O
we	O
have	O
that	O
(	O
x	O
,	O
y	O
)	O
∼d	O
[	O
y	O
(	O
cid:54	O
)	O
=	O
sign	O
(	O
(	O
cid:104	O
)	O
ws	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
]	O
≤	O
4r	O
(	O
cid:107	O
)	O
ws	O
(	O
cid:107	O
)	O
√	O
p	O
m	O
+	O
ln	O
(	O
4	O
log2	O
(	O
(	O
cid:107	O
)	O
ws	O
(	O
cid:107	O
)	O
)	O
δ	O
m	O
)	O
.	O
proof	O
for	O
any	O
integer	O
i	O
,	O
let	O
bi	O
=	O
2i	O
,	O
hi	O
=	O
{	O
w	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
≤	O
bi	O
}	O
,	O
and	O
let	O
δi	O
=	O
δ	O
2i2	O
.	O
fix	O
i	O
,	O
then	O
using	O
theorem	O
26.12	O
we	O
have	O
that	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δi	O
(	O
cid:115	O
)	O
(	O
cid:114	O
)	O
+	O
m	O
2	O
ln	O
(	O
2/δi	O
)	O
2bir√	O
m	O
∀w	O
∈	O
hi	O
,	O
ld	O
(	O
w	O
)	O
≤	O
ls	O
(	O
w	O
)	O
+	O
applying	O
the	O
union	B
bound	I
and	O
using	O
(	O
cid:80	O
)	O
∞	O
i=1	O
δi	O
≤	O
δ	O
we	O
obtain	O
that	O
with	O
probability	O
of	O
at	O
least	O
1−	O
δ	O
this	O
holds	O
for	O
all	O
i.	O
therefore	O
,	O
for	O
all	O
w	O
,	O
if	O
we	O
let	O
i	O
=	O
(	O
cid:100	O
)	O
log2	O
(	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
)	O
(	O
cid:101	O
)	O
(	O
cid:114	O
)	O
δ	O
≤	O
(	O
4	O
log2	O
(	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
)	O
)	O
2	O
then	O
w	O
∈	O
hi	O
,	O
bi	O
≤	O
2	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
,	O
and	O
2	O
(	O
cid:114	O
)	O
ld	O
(	O
w	O
)	O
≤	O
ls	O
(	O
w	O
)	O
+	O
m	O
4	O
(	O
ln	O
(	O
4	O
log2	O
(	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
)	O
)	O
+	O
ln	O
(	O
1/δ	O
)	O
)	O
.	O
therefore	O
,	O
2	O
ln	O
(	O
2/δi	O
)	O
=	O
(	O
2i	O
)	O
2	O
δi	O
δ	O
≤	O
ls	O
(	O
w	O
)	O
+	O
+	O
2bir√	O
m	O
4	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
r√	O
m	O
+	O
m	O
.	O
in	O
particular	O
,	O
it	O
holds	O
for	O
ws	O
,	O
which	O
concludes	O
our	O
proof	O
.	O
386	O
rademacher	O
complexities	O
remark	O
26.2	O
note	O
that	O
all	O
the	O
bounds	O
we	O
have	O
derived	O
do	O
not	O
depend	O
on	O
the	O
dimension	B
of	O
w.	O
this	O
property	O
is	O
utilized	O
when	O
learning	O
svm	O
with	O
kernels	B
,	O
where	O
the	O
dimension	B
of	O
w	O
can	O
be	O
extremely	O
large	O
.	O
26.4	O
generalization	B
bounds	I
for	O
predictors	O
with	O
low	O
(	O
cid:96	O
)	O
1	O
norm	O
in	O
the	O
previous	O
section	O
we	O
derived	O
generalization	B
bounds	I
for	O
linear	B
predictors	I
with	O
an	O
(	O
cid:96	O
)	O
2-norm	O
constraint	O
.	O
in	O
this	O
section	O
we	O
consider	O
the	O
following	O
general	O
(	O
cid:96	O
)	O
1-	O
norm	O
constraint	O
formulation	O
.	O
let	O
h	O
=	O
{	O
w	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
1	O
≤	O
b	O
}	O
be	O
our	O
hypothesis	B
class	I
,	O
and	O
let	O
z	O
=	O
x	O
×	O
y	O
be	O
the	O
examples	O
domain	B
.	O
assume	O
that	O
the	O
loss	B
function	I
,	O
(	O
cid:96	O
)	O
:	O
h	O
×	O
z	O
→	O
r	O
,	O
is	O
of	O
the	O
same	O
form	O
as	O
in	O
equation	O
(	O
26.18	O
)	O
,	O
with	O
φ	O
:	O
r	O
×	O
y	O
→	O
r	O
being	O
ρ-lipschitz	O
w.r.t	O
.	O
its	O
ﬁrst	O
argument	O
.	O
the	O
following	O
theorem	O
bounds	O
the	O
generalization	B
error	I
of	O
all	O
predictors	O
in	O
h	O
using	O
their	O
empirical	B
error	I
.	O
theorem	O
26.15	O
suppose	O
that	O
d	O
is	O
a	O
distribution	O
over	O
x	O
×	O
y	O
such	O
that	O
with	O
probability	O
1	O
we	O
have	O
that	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
∞	O
≤	O
r.	O
let	O
h	O
=	O
{	O
w	O
∈	O
rd	O
:	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
1	O
≤	O
b	O
}	O
and	O
let	O
(	O
cid:96	O
)	O
:	O
h	O
×	O
z	O
→	O
r	O
be	O
a	O
loss	B
function	I
of	O
the	O
form	O
given	O
in	O
equation	O
(	O
26.18	O
)	O
such	O
that	O
for	O
all	O
y	O
∈	O
y	O
,	O
a	O
(	O
cid:55	O
)	O
→	O
φ	O
(	O
a	O
,	O
y	O
)	O
is	O
an	O
ρ-lipschitz	O
function	B
and	O
such	O
that	O
maxa∈	O
[	O
−br	O
,	O
br	O
]	O
|φ	O
(	O
a	O
,	O
y	O
)	O
|	O
≤	O
c.	O
then	O
,	O
for	O
any	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
the	O
choice	O
of	O
an	O
i.i.d	O
.	O
sample	O
of	O
size	O
m	O
,	O
(	O
cid:114	O
)	O
(	O
cid:114	O
)	O
2	O
ln	O
(	O
2/δ	O
)	O
m	O
.	O
∀w	O
∈	O
h	O
,	O
ld	O
(	O
w	O
)	O
≤	O
ls	O
(	O
w	O
)	O
+	O
2ρbr	O
2	O
log	O
(	O
2d	O
)	O
m	O
+	O
c	O
proof	O
the	O
proof	O
is	O
identical	O
to	O
the	O
proof	O
of	O
theorem	O
26.12	O
,	O
while	O
relying	O
on	O
lemma	O
26.11	O
instead	O
of	O
relying	O
on	O
lemma	O
26.10.	O
it	O
is	O
interesting	O
to	O
compare	O
the	O
two	O
bounds	O
given	O
in	O
theorem	O
26.12	O
and	O
the-	O
orem	O
26.15.	O
apart	O
from	O
the	O
extra	O
log	O
(	O
d	O
)	O
factor	O
that	O
appears	O
in	O
theorem	O
26.15	O
,	O
both	O
bounds	O
look	O
similar	O
.	O
however	O
,	O
the	O
parameters	O
b	O
,	O
r	O
have	O
diﬀerent	O
meanings	O
in	O
the	O
two	O
bounds	O
.	O
in	O
theorem	O
26.12	O
,	O
the	O
parameter	O
b	O
imposes	O
an	O
(	O
cid:96	O
)	O
2	O
constraint	O
on	O
w	O
and	O
the	O
parameter	O
r	O
captures	O
a	O
low	O
(	O
cid:96	O
)	O
2-norm	O
assumption	O
on	O
the	O
instances	O
.	O
in	O
contrast	O
,	O
in	O
theorem	O
26.15	O
the	O
parameter	O
b	O
imposes	O
an	O
(	O
cid:96	O
)	O
1	O
constraint	O
on	O
w	O
(	O
which	O
is	O
stronger	O
than	O
an	O
(	O
cid:96	O
)	O
2	O
constraint	O
)	O
while	O
the	O
parameter	O
r	O
captures	O
a	O
low	O
(	O
cid:96	O
)	O
∞-norm	O
assumption	O
on	O
the	O
instance	B
(	O
which	O
is	O
weaker	O
than	O
a	O
low	O
(	O
cid:96	O
)	O
2-norm	O
as-	O
sumption	O
)	O
.	O
therefore	O
,	O
the	O
choice	O
of	O
the	O
constraint	O
should	O
depend	O
on	O
our	O
prior	B
knowledge	I
of	O
the	O
set	B
of	O
instances	O
and	O
on	O
prior	O
assumptions	O
on	O
good	O
predictors	O
.	O
26.5	O
bibliographic	O
remarks	O
the	O
use	O
of	O
rademacher	O
complexity	O
for	O
bounding	O
the	O
uniform	B
convergence	I
is	O
due	O
to	O
(	O
koltchinskii	O
&	O
panchenko	O
2000	O
,	O
bartlett	O
&	O
mendelson	O
2001	O
,	O
bartlett	O
&	O
mendelson	O
2002	O
)	O
.	O
for	O
additional	O
reading	O
see	O
,	O
for	O
example	O
,	O
(	O
bousquet	O
2002	O
,	O
boucheron	O
,	O
bousquet	O
&	O
lugosi	O
2005	O
,	O
bartlett	O
,	O
bousquet	O
&	O
mendelson	O
2005	O
)	O
.	O
26.5	O
bibliographic	O
remarks	O
387	O
our	O
proof	O
of	O
the	O
concentration	O
lemma	O
is	O
due	O
to	O
kakade	O
and	O
tewari	O
lecture	O
notes	O
.	O
kakade	O
,	O
sridharan	O
&	O
tewari	O
(	O
2008	O
)	O
gave	O
a	O
uniﬁed	O
framework	O
for	O
deriving	O
bounds	O
on	O
the	O
rademacher	O
complexity	O
of	O
linear	O
classes	O
with	O
respect	O
to	O
diﬀerent	O
assumptions	O
on	O
the	O
norms	O
.	O
27	O
covering	B
numbers	I
in	O
this	O
chapter	O
we	O
describe	O
another	O
way	O
to	O
measure	O
the	O
complexity	O
of	O
sets	O
,	O
which	O
is	O
called	O
covering	B
numbers	I
.	O
27.1	O
covering	O
definition	O
27.1	O
(	O
covering	O
)	O
let	O
a	O
⊂	O
rm	O
be	O
a	O
set	B
of	O
vectors	O
.	O
we	O
say	O
that	O
a	O
is	O
r-covered	O
by	O
a	O
set	B
a	O
(	O
cid:48	O
)	O
,	O
with	O
respect	O
to	O
the	O
euclidean	O
metric	O
,	O
if	O
for	O
all	O
a	O
∈	O
a	O
there	O
exists	O
a	O
(	O
cid:48	O
)	O
∈	O
a	O
(	O
cid:48	O
)	O
with	O
(	O
cid:107	O
)	O
a	O
−	O
a	O
(	O
cid:48	O
)	O
(	O
cid:107	O
)	O
≤	O
r.	O
we	O
deﬁne	O
by	O
n	O
(	O
r	O
,	O
a	O
)	O
the	O
cardinality	O
of	O
the	O
smallest	O
a	O
(	O
cid:48	O
)	O
that	O
r-covers	O
a.	O
example	O
27.1	O
(	O
subspace	O
)	O
suppose	O
that	O
a	O
⊂	O
rm	O
,	O
let	O
c	O
=	O
maxa∈a	O
(	O
cid:107	O
)	O
a	O
(	O
cid:107	O
)	O
,	O
and	O
as-	O
√	O
sume	O
that	O
a	O
lies	O
in	O
a	O
d-dimensional	O
subspace	O
of	O
rm	O
.	O
then	O
,	O
n	O
(	O
r	O
,	O
a	O
)	O
≤	O
(	O
2c	O
d/r	O
)	O
d.	O
to	O
see	O
this	O
,	O
let	O
v1	O
,	O
.	O
.	O
.	O
,	O
vd	O
be	O
an	O
orthonormal	O
basis	O
of	O
the	O
subspace	O
.	O
then	O
,	O
any	O
i=1	O
αivi	O
with	O
(	O
cid:107	O
)	O
α	O
(	O
cid:107	O
)	O
∞	O
≤	O
(	O
cid:107	O
)	O
α	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:107	O
)	O
a	O
(	O
cid:107	O
)	O
2	O
≤	O
c.	O
let	O
	O
∈	O
r	O
and	O
consider	O
the	O
set	B
a	O
∈	O
a	O
can	O
be	O
written	O
as	O
a	O
=	O
(	O
cid:80	O
)	O
d	O
(	O
cid:41	O
)	O
i	O
∈	O
{	O
−c	O
,	O
−c	O
+	O
	O
,	O
−c	O
+	O
2	O
,	O
.	O
.	O
.	O
,	O
c	O
}	O
.	O
a	O
(	O
cid:48	O
)	O
=	O
(	O
cid:40	O
)	O
d	O
(	O
cid:88	O
)	O
given	O
a	O
∈	O
a	O
s.t	O
.	O
a	O
=	O
(	O
cid:80	O
)	O
d	O
(	O
cid:107	O
)	O
a	O
−	O
a	O
(	O
cid:48	O
)	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:107	O
)	O
(	O
cid:88	O
)	O
i=1	O
ivi	O
:	O
∀i	O
,	O
α	O
(	O
cid:48	O
)	O
α	O
(	O
cid:48	O
)	O
(	O
α	O
(	O
cid:48	O
)	O
i	O
d	O
;	O
then	O
(	O
cid:107	O
)	O
a	O
−	O
a	O
(	O
cid:48	O
)	O
(	O
cid:107	O
)	O
≤	O
r	O
and	O
therefore	O
a	O
(	O
cid:48	O
)	O
is	O
an	O
r-cover	O
of	O
a.	O
hence	O
,	O
i=1	O
αivi	O
with	O
(	O
cid:107	O
)	O
α	O
(	O
cid:107	O
)	O
∞	O
≤	O
c	O
,	O
there	O
exists	O
a	O
(	O
cid:48	O
)	O
∈	O
a	O
(	O
cid:48	O
)	O
such	O
that	O
(	O
cid:107	O
)	O
vi	O
(	O
cid:107	O
)	O
2	O
≤	O
2	O
d.	O
i	O
−	O
αi	O
)	O
vi	O
(	O
cid:107	O
)	O
2	O
≤	O
2	O
(	O
cid:88	O
)	O
(	O
cid:32	O
)	O
(	O
cid:18	O
)	O
2c	O
(	O
cid:19	O
)	O
d	O
i	O
=	O
	O
(	O
cid:33	O
)	O
d	O
.	O
√	O
2c	O
r	O
d	O
n	O
(	O
r	O
,	O
a	O
)	O
≤	O
|a	O
(	O
cid:48	O
)	O
|	O
=	O
√	O
choose	O
	O
=	O
r/	O
27.1.1	O
properties	O
the	O
following	O
lemma	O
is	O
immediate	O
from	O
the	O
deﬁnition	O
.	O
lemma	O
27.2	O
for	O
any	O
a	O
⊂	O
rm	O
,	O
scalar	O
c	O
>	O
0	O
,	O
and	O
vector	O
a0	O
∈	O
rm	O
,	O
we	O
have	O
∀r	O
>	O
0	O
,	O
n	O
(	O
r	O
,	O
{	O
c	O
a	O
+	O
a0	O
:	O
a	O
∈	O
a	O
}	O
)	O
≤	O
n	O
(	O
cr	O
,	O
a	O
)	O
.	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
27.2	O
from	O
covering	O
to	O
rademacher	O
complexity	O
via	O
chaining	B
389	O
next	O
,	O
we	O
derive	O
a	O
contraction	O
principle	O
.	O
lemma	O
27.3	O
for	O
each	O
i	O
∈	O
[	O
m	O
]	O
,	O
let	O
φi	O
:	O
r	O
→	O
r	O
be	O
a	O
ρ-lipschitz	O
function	B
;	O
namely	O
,	O
for	O
all	O
α	O
,	O
β	O
∈	O
r	O
we	O
have	O
|φi	O
(	O
α	O
)	O
−	O
φi	O
(	O
β	O
)	O
|	O
≤	O
ρ|α	O
−	O
β|	O
.	O
for	O
a	O
∈	O
rm	O
let	O
φ	O
(	O
a	O
)	O
denote	O
the	O
vector	O
(	O
φ1	O
(	O
a1	O
)	O
,	O
.	O
.	O
.	O
,	O
φm	O
(	O
am	O
)	O
)	O
.	O
let	O
φ◦	O
a	O
=	O
{	O
φ	O
(	O
a	O
)	O
:	O
a	O
∈	O
a	O
}	O
.	O
then	O
,	O
n	O
(	O
ρ	O
r	O
,	O
φ	O
◦	O
a	O
)	O
≤	O
n	O
(	O
r	O
,	O
a	O
)	O
.	O
proof	O
deﬁne	O
b	O
=	O
φ	O
◦	O
a.	O
let	O
a	O
(	O
cid:48	O
)	O
be	O
an	O
r-cover	O
of	O
a	O
and	O
deﬁne	O
b	O
(	O
cid:48	O
)	O
=	O
φ	O
◦	O
a	O
(	O
cid:48	O
)	O
.	O
then	O
,	O
for	O
all	O
a	O
∈	O
a	O
there	O
exists	O
a	O
(	O
cid:48	O
)	O
∈	O
a	O
(	O
cid:48	O
)	O
with	O
(	O
cid:107	O
)	O
a	O
−	O
a	O
(	O
cid:48	O
)	O
(	O
cid:107	O
)	O
≤	O
r.	O
so	O
,	O
(	O
ai	O
−	O
a	O
(	O
cid:48	O
)	O
i	O
)	O
)	O
2	O
≤	O
ρ2	O
(	O
cid:88	O
)	O
(	O
cid:107	O
)	O
φ	O
(	O
a	O
)	O
−	O
φ	O
(	O
a	O
(	O
cid:48	O
)	O
)	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
φi	O
(	O
ai	O
)	O
−	O
φi	O
(	O
a	O
(	O
cid:48	O
)	O
i	O
)	O
2	O
≤	O
(	O
ρr	O
)	O
2	O
.	O
(	O
cid:88	O
)	O
hence	O
,	O
b	O
(	O
cid:48	O
)	O
is	O
an	O
(	O
ρ	O
r	O
)	O
-cover	O
of	O
b.	O
i	O
i	O
27.2	O
from	O
covering	O
to	O
rademacher	O
complexity	O
via	O
chaining	B
the	O
following	O
lemma	O
bounds	O
the	O
rademacher	O
complexity	O
of	O
a	O
based	O
on	O
the	O
covering	B
numbers	I
n	O
(	O
r	O
,	O
a	O
)	O
.	O
this	O
technique	O
is	O
called	O
chaining	B
and	O
is	O
attributed	O
to	O
dudley	O
.	O
lemma	O
27.4	O
let	O
c	O
=	O
min¯a	O
maxa∈a	O
(	O
cid:107	O
)	O
a	O
−	O
¯a	O
(	O
cid:107	O
)	O
.	O
then	O
,	O
for	O
any	O
integer	O
m	O
>	O
0	O
,	O
r	O
(	O
a	O
)	O
≤	O
c	O
2−m√	O
m	O
+	O
6	O
c	O
m	O
log	O
(	O
n	O
(	O
c	O
2−k	O
,	O
a	O
)	O
)	O
.	O
2−k	O
(	O
cid:113	O
)	O
m	O
(	O
cid:88	O
)	O
k=1	O
proof	O
let	O
¯a	O
be	O
a	O
minimizer	O
of	O
the	O
objective	O
function	B
given	O
in	O
the	O
deﬁnition	O
of	O
c.	O
on	O
the	O
basis	O
of	O
lemma	O
26.6	O
,	O
we	O
can	O
analyze	O
the	O
rademacher	O
complexity	O
assuming	O
that	O
¯a	O
=	O
0.	O
consider	O
the	O
set	B
b0	O
=	O
{	O
0	O
}	O
and	O
note	O
that	O
it	O
is	O
a	O
c-cover	O
of	O
a.	O
let	O
b1	O
,	O
.	O
.	O
.	O
,	O
bm	O
be	O
sets	O
such	O
that	O
each	O
bk	O
corresponds	O
to	O
a	O
minimal	O
(	O
c	O
2−k	O
)	O
-cover	O
of	O
a.	O
let	O
a∗	O
=	O
argmaxa∈a	O
(	O
cid:104	O
)	O
σ	O
,	O
a	O
(	O
cid:105	O
)	O
(	O
where	O
if	O
there	O
is	O
more	O
than	O
one	O
maximizer	O
,	O
choose	O
one	O
in	O
an	O
arbitrary	O
way	O
,	O
and	O
if	O
a	O
maximizer	O
does	O
not	O
exist	O
,	O
choose	O
a∗	O
such	O
that	O
(	O
cid:104	O
)	O
σ	O
,	O
a∗	O
(	O
cid:105	O
)	O
is	O
close	O
enough	O
to	O
the	O
supremum	O
)	O
.	O
note	O
that	O
a∗	O
is	O
a	O
function	B
of	O
σ.	O
for	O
each	O
k	O
,	O
let	O
b	O
(	O
k	O
)	O
be	O
the	O
nearest	O
neighbor	O
of	O
a∗	O
in	O
bk	O
(	O
hence	O
b	O
(	O
k	O
)	O
is	O
also	O
a	O
function	B
of	O
σ	O
)	O
.	O
using	O
the	O
triangle	O
inequality	O
,	O
(	O
cid:107	O
)	O
b	O
(	O
k	O
)	O
−	O
b	O
(	O
k−1	O
)	O
(	O
cid:107	O
)	O
≤	O
(	O
cid:107	O
)	O
b	O
(	O
k	O
)	O
−	O
a∗	O
(	O
cid:107	O
)	O
+	O
(	O
cid:107	O
)	O
a∗	O
−	O
b	O
(	O
k−1	O
)	O
(	O
cid:107	O
)	O
≤	O
c	O
(	O
2−k	O
+	O
2−	O
(	O
k−1	O
)	O
)	O
=	O
3	O
c	O
2−k	O
.	O
for	O
each	O
k	O
deﬁne	O
the	O
set	B
ˆbk	O
=	O
{	O
(	O
a	O
−	O
a	O
(	O
cid:48	O
)	O
)	O
:	O
a	O
∈	O
bk	O
,	O
a	O
(	O
cid:48	O
)	O
∈	O
bk−1	O
,	O
(	O
cid:107	O
)	O
a	O
−	O
a	O
(	O
cid:48	O
)	O
(	O
cid:107	O
)	O
≤	O
3	O
c	O
2−k	O
}	O
.	O
390	O
covering	B
numbers	I
we	O
can	O
now	O
write	O
r	O
(	O
a	O
)	O
=	O
=	O
1	O
m	O
1	O
m	O
e	O
e	O
(	O
cid:104	O
)	O
σ	O
,	O
a∗	O
(	O
cid:105	O
)	O
(	O
cid:34	O
)	O
e	O
(	O
cid:104	O
)	O
(	O
cid:107	O
)	O
σ	O
(	O
cid:107	O
)	O
(	O
cid:107	O
)	O
a∗	O
−	O
b	O
(	O
m	O
)	O
(	O
cid:107	O
)	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
σ	O
,	O
a∗	O
−	O
b	O
(	O
m	O
)	O
(	O
cid:105	O
)	O
+	O
m	O
(	O
cid:88	O
)	O
m	O
(	O
cid:88	O
)	O
k=1	O
+	O
k=1	O
(	O
cid:35	O
)	O
(	O
cid:35	O
)	O
(	O
cid:104	O
)	O
σ	O
,	O
b	O
(	O
k	O
)	O
−	O
b	O
(	O
k−1	O
)	O
(	O
cid:105	O
)	O
(	O
cid:34	O
)	O
e	O
1	O
m	O
sup	O
a∈	O
ˆbk	O
(	O
cid:104	O
)	O
σ	O
,	O
a	O
(	O
cid:105	O
)	O
.	O
≤	O
1	O
m	O
√	O
m	O
and	O
(	O
cid:107	O
)	O
a∗	O
−	O
b	O
(	O
m	O
)	O
(	O
cid:107	O
)	O
≤	O
c	O
2−m	O
,	O
the	O
ﬁrst	O
summand	O
is	O
at	O
most	O
since	O
(	O
cid:107	O
)	O
σ	O
(	O
cid:107	O
)	O
=	O
m	O
2−m	O
.	O
additionally	O
,	O
by	O
massart	O
lemma	O
,	O
c√	O
1	O
m	O
(	O
cid:104	O
)	O
σ	O
,	O
a	O
(	O
cid:105	O
)	O
≤	O
3	O
c	O
2−k	O
m	O
e	O
sup	O
a∈	O
ˆbk	O
therefore	O
,	O
(	O
cid:112	O
)	O
2	O
log	O
(	O
n	O
(	O
c	O
2−k	O
,	O
a	O
)	O
2	O
)	O
2−k	O
(	O
cid:113	O
)	O
m	O
(	O
cid:88	O
)	O
+	O
6c	O
m	O
k=1	O
r	O
(	O
a	O
)	O
≤	O
c	O
2−m√	O
m	O
(	O
cid:112	O
)	O
log	O
(	O
n	O
(	O
c	O
2−k	O
,	O
a	O
)	O
)	O
m	O
.	O
=	O
6	O
c	O
2−k	O
log	O
(	O
n	O
(	O
c2−k	O
,	O
a	O
)	O
)	O
.	O
as	O
a	O
corollary	O
we	O
obtain	O
the	O
following	O
:	O
lemma	O
27.5	O
assume	O
that	O
there	O
are	O
α	O
,	O
β	O
>	O
0	O
such	O
that	O
for	O
any	O
k	O
≥	O
1	O
we	O
have	O
(	O
cid:113	O
)	O
log	O
(	O
n	O
(	O
c2−k	O
,	O
a	O
)	O
)	O
≤	O
α	O
+	O
βk	O
.	O
then	O
,	O
r	O
(	O
a	O
)	O
≤	O
6c	O
m	O
(	O
α	O
+	O
2β	O
)	O
.	O
k=1	O
k2−k	O
=	O
2.	O
proof	O
the	O
bound	O
follows	O
from	O
lemma	O
27.4	O
by	O
taking	O
m	O
→	O
∞	O
and	O
noting	O
that	O
(	O
cid:80	O
)	O
∞	O
k=1	O
2−k	O
=	O
1	O
and	O
(	O
cid:80	O
)	O
∞	O
and	O
such	O
that	O
c	O
=	O
maxa∈a	O
(	O
cid:107	O
)	O
a	O
(	O
cid:107	O
)	O
.	O
we	O
have	O
shown	O
that	O
n	O
(	O
r	O
,	O
a	O
)	O
≤	O
(	O
cid:16	O
)	O
2c	O
fore	O
,	O
for	O
any	O
k	O
,	O
(	O
cid:113	O
)	O
example	O
27.2	O
consider	O
a	O
set	B
a	O
which	O
lies	O
in	O
a	O
d	O
dimensional	O
subspace	O
of	O
rm	O
.	O
there-	O
(	O
cid:17	O
)	O
d	O
√	O
√	O
d	O
r	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
log	O
(	O
n	O
(	O
c2−k	O
,	O
a	O
)	O
)	O
≤	O
(	O
cid:114	O
)	O
(	O
cid:113	O
)	O
(	O
cid:113	O
)	O
≤	O
≤	O
d	O
log	O
2k+1	O
√	O
d	O
log	O
(	O
2	O
√	O
d	O
log	O
(	O
2	O
d	O
)	O
+	O
k	O
d	O
d	O
)	O
+	O
d	O
k.	O
d	O
√	O
√	O
(	O
cid:19	O
)	O
(	O
cid:32	O
)	O
c	O
(	O
cid:112	O
)	O
d	O
log	O
(	O
d	O
)	O
(	O
cid:33	O
)	O
.	O
m	O
hence	O
lemma	O
27.5	O
yields	O
(	O
cid:18	O
)	O
(	O
cid:113	O
)	O
r	O
(	O
a	O
)	O
≤	O
6c	O
m	O
√	O
d	O
log	O
(	O
2	O
d	O
)	O
+	O
2	O
√	O
d	O
=	O
o	O
27.3	O
bibliographic	O
remarks	O
391	O
27.3	O
bibliographic	O
remarks	O
the	O
chaining	B
technique	O
is	O
due	O
to	O
dudley	O
(	O
1987	O
)	O
.	O
for	O
an	O
extensive	O
study	O
of	O
cover-	O
ing	O
numbers	O
as	O
well	O
as	O
other	O
complexity	O
measures	O
that	O
can	O
be	O
used	O
to	O
bound	O
the	O
rate	O
of	O
uniform	B
convergence	I
we	O
refer	O
the	O
reader	O
to	O
(	O
anthony	O
&	O
bartlet	O
1999	O
)	O
.	O
28	O
proof	O
of	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
in	O
this	O
chapter	O
we	O
prove	O
theorem	O
6.8	O
from	O
chapter	O
6.	O
we	O
remind	O
the	O
reader	O
the	O
conditions	O
of	O
the	O
theorem	O
,	O
which	O
will	O
hold	O
throughout	O
this	O
chapter	O
:	O
h	O
is	O
a	O
hypothesis	B
class	I
of	O
functions	O
from	O
a	O
domain	B
x	O
to	O
{	O
0	O
,	O
1	O
}	O
,	O
the	O
loss	B
function	I
is	O
the	O
0	O
−	O
1	O
loss	B
,	O
and	O
vcdim	O
(	O
h	O
)	O
=	O
d	O
<	O
∞	O
.	O
we	O
shall	O
prove	O
the	O
upper	O
bound	O
for	O
both	O
the	O
realizable	O
and	O
agnostic	O
cases	O
and	O
shall	O
prove	O
the	O
lower	O
bound	O
for	O
the	O
agnostic	O
case	O
.	O
the	O
lower	O
bound	O
for	O
the	O
realizable	O
case	O
is	O
left	O
as	O
an	O
exercise	O
.	O
28.1	O
the	O
upper	O
bound	O
for	O
the	O
agnostic	O
case	O
for	O
the	O
upper	O
bound	O
we	O
need	O
to	O
prove	O
that	O
there	O
exists	O
c	O
such	O
that	O
h	O
is	O
agnostic	O
pac	O
learnable	O
with	O
sample	B
complexity	I
mh	O
(	O
	O
,	O
δ	O
)	O
≤	O
c	O
d	O
+	O
ln	O
(	O
1/δ	O
)	O
.	O
2	O
we	O
will	O
prove	O
the	O
slightly	O
looser	O
bound	O
:	O
mh	O
(	O
	O
,	O
δ	O
)	O
≤	O
c	O
d	O
log	O
(	O
d/	O
)	O
+	O
ln	O
(	O
1/δ	O
)	O
2	O
.	O
(	O
28.1	O
)	O
the	O
tighter	O
bound	O
in	O
the	O
theorem	O
statement	O
requires	O
a	O
more	O
involved	O
proof	O
,	O
in	O
which	O
a	O
more	O
careful	O
analysis	O
of	O
the	O
rademacher	O
complexity	O
using	O
a	O
technique	O
called	O
“	O
chaining	B
”	O
should	O
be	O
used	O
.	O
this	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
.	O
to	O
prove	O
equation	O
(	O
28.1	O
)	O
,	O
it	O
suﬃces	O
to	O
show	O
that	O
applying	O
the	O
erm	O
with	O
a	O
sample	O
size	O
(	O
cid:18	O
)	O
64d	O
(	O
cid:19	O
)	O
m	O
≥	O
4	O
8	O
2	O
·	O
(	O
8d	O
log	O
(	O
e/d	O
)	O
+	O
2	O
log	O
(	O
4/δ	O
)	O
)	O
yields	O
an	O
	O
,	O
δ-learner	O
for	O
h.	O
we	O
prove	O
this	O
result	O
on	O
the	O
basis	O
of	O
theorem	O
26.5.	O
let	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
be	O
a	O
classiﬁcation	O
training	B
set	I
.	O
recall	B
that	O
the	O
sauer-	O
shelah	O
lemma	O
tells	O
us	O
that	O
if	O
vcdim	O
(	O
h	O
)	O
=	O
d	O
then	O
+	O
2	O
32d	O
2	O
·	O
log	O
|	O
{	O
(	O
h	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
h	O
(	O
xm	O
)	O
)	O
:	O
h	O
∈	O
h	O
}	O
|	O
≤	O
(	O
cid:16	O
)	O
e	O
m	O
(	O
cid:17	O
)	O
d	O
.	O
d	O
denote	O
a	O
=	O
{	O
(	O
1	O
[	O
h	O
(	O
x1	O
)	O
(	O
cid:54	O
)	O
=y1	O
]	O
,	O
.	O
.	O
.	O
,	O
1	O
[	O
h	O
(	O
xm	O
)	O
(	O
cid:54	O
)	O
=ym	O
]	O
)	O
:	O
h	O
∈	O
h	O
}	O
.	O
this	O
clearly	O
implies	O
that	O
|a|	O
≤	O
(	O
cid:16	O
)	O
e	O
m	O
(	O
cid:17	O
)	O
d	O
.	O
d	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
28.2	O
the	O
lower	O
bound	O
for	O
the	O
agnostic	O
case	O
393	O
(	O
cid:114	O
)	O
(	O
cid:114	O
)	O
(	O
cid:114	O
)	O
(	O
cid:114	O
)	O
.	O
(	O
cid:114	O
)	O
(	O
cid:114	O
)	O
combining	O
this	O
with	O
lemma	O
26.8	O
we	O
obtain	O
the	O
following	O
bound	O
on	O
the	O
rademacher	O
complexity	O
:	O
r	O
(	O
a	O
)	O
≤	O
2d	O
log	O
(	O
em/d	O
)	O
m	O
using	O
theorem	O
26.5	O
we	O
obtain	O
that	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
,	O
for	O
every	O
h	O
∈	O
h	O
we	O
have	O
that	O
ld	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
≤	O
8d	O
log	O
(	O
em/d	O
)	O
m	O
+	O
2	O
log	O
(	O
2/δ	O
)	O
m	O
.	O
repeating	O
the	O
previous	O
argument	O
for	O
minus	O
the	O
zero-one	O
loss	B
and	O
applying	O
the	O
union	B
bound	I
we	O
obtain	O
that	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
,	O
for	O
every	O
h	O
∈	O
h	O
it	O
holds	O
that	O
|ld	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
|	O
≤	O
8d	O
log	O
(	O
em/d	O
)	O
m	O
+	O
2	O
log	O
(	O
4/δ	O
)	O
m	O
≤	O
2	O
8d	O
log	O
(	O
em/d	O
)	O
+	O
2	O
log	O
(	O
4/δ	O
)	O
m	O
.	O
to	O
ensure	O
that	O
this	O
is	O
smaller	O
than	O
	O
we	O
need	O
m	O
≥	O
4	O
2	O
·	O
(	O
8d	O
log	O
(	O
m	O
)	O
+	O
8d	O
log	O
(	O
e/d	O
)	O
+	O
2	O
log	O
(	O
4/δ	O
)	O
)	O
.	O
using	O
lemma	O
a.2	O
,	O
a	O
suﬃcient	O
condition	O
for	O
the	O
inequality	O
to	O
hold	O
is	O
that	O
m	O
≥	O
4	O
32d	O
2	O
·	O
log	O
+	O
8	O
2	O
·	O
(	O
8d	O
log	O
(	O
e/d	O
)	O
+	O
2	O
log	O
(	O
4/δ	O
)	O
)	O
.	O
(	O
cid:18	O
)	O
64d	O
(	O
cid:19	O
)	O
2	O
28.2	O
the	O
lower	O
bound	O
for	O
the	O
agnostic	O
case	O
here	O
,	O
we	O
prove	O
that	O
there	O
exists	O
c	O
such	O
that	O
h	O
is	O
agnostic	O
pac	O
learnable	O
with	O
sample	B
complexity	I
mh	O
(	O
	O
,	O
δ	O
)	O
≥	O
c	O
d	O
+	O
ln	O
(	O
1/δ	O
)	O
.	O
2	O
we	O
will	O
prove	O
the	O
lower	O
bound	O
in	O
two	O
parts	O
.	O
first	O
,	O
we	O
will	O
show	O
that	O
m	O
(	O
	O
,	O
δ	O
)	O
≥	O
0.5	O
log	O
(	O
1/	O
(	O
4δ	O
)	O
)	O
/2	O
,	O
and	O
second	O
we	O
will	O
show	O
that	O
for	O
every	O
δ	O
≤	O
1/8	O
we	O
have	O
that	O
m	O
(	O
	O
,	O
δ	O
)	O
≥	O
8d/2	O
.	O
these	O
two	O
bounds	O
will	O
conclude	O
the	O
proof	O
.	O
28.2.1	O
showing	O
that	O
m	O
(	O
	O
,	O
δ	O
)	O
≥	O
0.5	O
log	O
(	O
1/	O
(	O
4δ	O
)	O
)	O
/2	O
√	O
2	O
and	O
any	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
we	O
have	O
that	O
m	O
(	O
	O
,	O
δ	O
)	O
≥	O
we	O
ﬁrst	O
show	O
that	O
for	O
any	O
	O
<	O
1/	O
0.5	O
log	O
(	O
1/	O
(	O
4δ	O
)	O
)	O
/2	O
.	O
to	O
do	O
so	O
,	O
we	O
show	O
that	O
for	O
m	O
≤	O
0.5	O
log	O
(	O
1/	O
(	O
4δ	O
)	O
)	O
/2	O
,	O
h	O
is	O
not	O
learnable	O
.	O
choose	O
one	O
example	O
that	O
is	O
shattered	O
by	O
h.	O
that	O
is	O
,	O
let	O
c	O
be	O
an	O
example	O
such	O
394	O
proof	O
of	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
that	O
there	O
are	O
h+	O
,	O
h−	O
∈	O
h	O
for	O
which	O
h+	O
(	O
c	O
)	O
=	O
1	O
and	O
h−	O
(	O
c	O
)	O
=	O
−1	O
.	O
deﬁne	O
two	O
distributions	O
,	O
d+	O
and	O
d−	O
,	O
such	O
that	O
for	O
b	O
∈	O
{	O
±1	O
}	O
we	O
have	O
(	O
cid:40	O
)	O
1+yb	O
2	O
0	O
db	O
(	O
{	O
(	O
x	O
,	O
y	O
)	O
}	O
)	O
=	O
if	O
x	O
=	O
c	O
otherwise	O
.	O
2	O
that	O
is	O
,	O
all	O
the	O
distribution	O
mass	O
is	O
concentrated	O
on	O
two	O
examples	O
(	O
c	O
,	O
1	O
)	O
and	O
(	O
c	O
,	O
−1	O
)	O
,	O
where	O
the	O
probability	O
of	O
(	O
c	O
,	O
b	O
)	O
is	O
1+b	O
and	O
the	O
probability	O
of	O
(	O
c	O
,	O
−b	O
)	O
is	O
1−b	O
2	O
.	O
let	O
a	O
be	O
an	O
arbitrary	O
algorithm	O
.	O
any	O
training	B
set	I
sampled	O
from	O
db	O
has	O
the	O
form	O
s	O
=	O
(	O
c	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
c	O
,	O
ym	O
)	O
.	O
therefore	O
,	O
it	O
is	O
fully	O
characterized	O
by	O
the	O
vector	O
y	O
=	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
ym	O
)	O
∈	O
{	O
±1	O
}	O
m.	O
upon	O
receiving	O
a	O
training	B
set	I
s	O
,	O
the	O
algorithm	O
a	O
returns	O
a	O
hypothesis	B
h	O
:	O
x	O
→	O
{	O
±1	O
}	O
.	O
since	O
the	O
error	O
of	O
a	O
w.r.t	O
.	O
db	O
only	O
depends	O
on	O
h	O
(	O
c	O
)	O
,	O
we	O
can	O
think	O
of	O
a	O
as	O
a	O
mapping	O
from	O
{	O
±1	O
}	O
m	O
into	O
{	O
±1	O
}	O
.	O
therefore	O
,	O
we	O
denote	O
by	O
a	O
(	O
y	O
)	O
the	O
value	O
in	O
{	O
±1	O
}	O
corresponding	O
to	O
the	O
prediction	O
of	O
h	O
(	O
c	O
)	O
,	O
where	O
h	O
is	O
the	O
hypothesis	B
that	O
a	O
outputs	O
upon	O
receiving	O
the	O
training	B
set	I
s	O
=	O
(	O
c	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
c	O
,	O
ym	O
)	O
.	O
note	O
that	O
for	O
any	O
hypothesis	B
h	O
we	O
have	O
ldb	O
(	O
h	O
)	O
=	O
1	O
−	O
h	O
(	O
c	O
)	O
b	O
2	O
.	O
in	O
particular	O
,	O
the	O
bayes	O
optimal	O
hypothesis	B
is	O
hb	O
and	O
ldb	O
(	O
a	O
(	O
y	O
)	O
)	O
−	O
ldb	O
(	O
hb	O
)	O
=	O
1	O
−	O
a	O
(	O
y	O
)	O
b	O
2	O
−	O
1	O
−	O
	O
2	O
=	O
(	O
cid:40	O
)	O
	O
0	O
if	O
a	O
(	O
y	O
)	O
(	O
cid:54	O
)	O
=	O
b	O
otherwise	O
.	O
fix	O
a.	O
for	O
b	O
∈	O
{	O
±1	O
}	O
,	O
let	O
y	O
b	O
=	O
{	O
y	O
∈	O
{	O
0	O
,	O
1	O
}	O
m	O
:	O
a	O
(	O
y	O
)	O
(	O
cid:54	O
)	O
=	O
b	O
}	O
.	O
the	O
distribution	O
db	O
induces	O
a	O
probability	O
pb	O
over	O
{	O
±1	O
}	O
m.	O
hence	O
,	O
p	O
[	O
ldb	O
(	O
a	O
(	O
y	O
)	O
)	O
−	O
ldb	O
(	O
hb	O
)	O
=	O
	O
]	O
=	O
db	O
(	O
y	O
b	O
)	O
=	O
(	O
cid:88	O
)	O
y	O
pb	O
[	O
y	O
]	O
1	O
[	O
a	O
(	O
y	O
)	O
(	O
cid:54	O
)	O
=b	O
]	O
.	O
denote	O
n	O
+	O
=	O
{	O
y	O
:	O
|	O
{	O
i	O
:	O
yi	O
=	O
1	O
}	O
|	O
≥	O
m/2	O
}	O
and	O
n−	O
=	O
{	O
±1	O
}	O
m	O
\	O
n	O
+	O
.	O
note	O
that	O
for	O
any	O
y	O
∈	O
n	O
+	O
we	O
have	O
p+	O
[	O
y	O
]	O
≥	O
p−	O
[	O
y	O
]	O
and	O
for	O
any	O
y	O
∈	O
n−	O
we	O
have	O
p−	O
[	O
y	O
]	O
≥	O
p+	O
[	O
y	O
]	O
.	O
28.2	O
the	O
lower	O
bound	O
for	O
the	O
agnostic	O
case	O
395	O
therefore	O
,	O
(	O
p+	O
[	O
y	O
]	O
1	O
[	O
a	O
(	O
y	O
)	O
(	O
cid:54	O
)	O
=+	O
]	O
+	O
p−	O
[	O
y	O
]	O
1	O
[	O
a	O
(	O
y	O
)	O
(	O
cid:54	O
)	O
=−	O
]	O
)	O
+	O
(	O
p−	O
[	O
y	O
]	O
1	O
[	O
a	O
(	O
y	O
)	O
(	O
cid:54	O
)	O
=+	O
]	O
+	O
p−	O
[	O
y	O
]	O
1	O
[	O
a	O
(	O
y	O
)	O
(	O
cid:54	O
)	O
=−	O
]	O
)	O
+	O
≥	O
1	O
2	O
=	O
1	O
2	O
≥	O
1	O
2	O
max	O
b∈	O
{	O
±1	O
}	O
=	O
max	O
b∈	O
{	O
±1	O
}	O
y	O
y	O
y	O
1	O
2	O
y∈n	O
+	O
pb	O
[	O
y	O
]	O
1	O
[	O
a	O
(	O
y	O
)	O
(	O
cid:54	O
)	O
=b	O
]	O
p+	O
[	O
y	O
]	O
1	O
[	O
a	O
(	O
y	O
)	O
(	O
cid:54	O
)	O
=+	O
]	O
+	O
(	O
cid:88	O
)	O
p	O
[	O
ldb	O
(	O
a	O
(	O
y	O
)	O
)	O
−	O
ldb	O
(	O
hb	O
)	O
=	O
	O
]	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
p−	O
[	O
y	O
]	O
+	O
(	O
cid:88	O
)	O
y∈n	O
+	O
p−	O
[	O
y	O
]	O
=	O
(	O
cid:80	O
)	O
1	O
−	O
(	O
cid:112	O
)	O
1	O
−	O
exp	O
(	O
−m2/	O
(	O
1	O
−	O
2	O
)	O
)	O
p+	O
[	O
y	O
]	O
.	O
y∈n−	O
y∈n	O
+	O
(	O
cid:16	O
)	O
1	O
2	O
1	O
2	O
1	O
2	O
=	O
next	O
note	O
that	O
(	O
cid:80	O
)	O
y∈n	O
+	O
p−	O
[	O
y	O
]	O
1	O
[	O
a	O
(	O
y	O
)	O
(	O
cid:54	O
)	O
=−	O
]	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
y∈n−	O
y∈n−	O
1	O
2	O
1	O
2	O
(	O
p+	O
[	O
y	O
]	O
1	O
[	O
a	O
(	O
y	O
)	O
(	O
cid:54	O
)	O
=+	O
]	O
+	O
p−	O
[	O
y	O
]	O
1	O
[	O
a	O
(	O
y	O
)	O
(	O
cid:54	O
)	O
=−	O
]	O
)	O
(	O
p+	O
[	O
y	O
]	O
1	O
[	O
a	O
(	O
y	O
)	O
(	O
cid:54	O
)	O
=+	O
]	O
+	O
p+	O
[	O
y	O
]	O
1	O
[	O
a	O
(	O
y	O
)	O
(	O
cid:54	O
)	O
=−	O
]	O
)	O
y∈n−	O
p+	O
[	O
y	O
]	O
,	O
and	O
both	O
values	O
are	O
the	O
prob-	O
ability	O
that	O
a	O
binomial	O
(	O
m	O
,	O
(	O
1	O
−	O
	O
)	O
/2	O
)	O
random	O
variable	O
will	O
have	O
value	O
greater	O
than	O
m/2	O
.	O
using	O
lemma	O
b.11	O
,	O
this	O
probability	O
is	O
lower	O
bounded	O
by	O
(	O
cid:17	O
)	O
≥	O
1	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
1	O
−	O
(	O
cid:112	O
)	O
1	O
−	O
exp	O
(	O
−2m2	O
)	O
,	O
2	O
where	O
we	O
used	O
the	O
assumption	O
that	O
2	O
≤	O
1/2	O
.	O
it	O
follows	O
that	O
if	O
m	O
≤	O
0.5	O
log	O
(	O
1/	O
(	O
4δ	O
)	O
)	O
/2	O
then	O
there	O
exists	O
b	O
such	O
that	O
(	O
cid:19	O
)	O
p	O
[	O
ldb	O
(	O
a	O
(	O
y	O
)	O
)	O
−	O
ldb	O
(	O
hb	O
)	O
=	O
	O
]	O
≥	O
δ	O
,	O
(	O
cid:113	O
)	O
(	O
cid:18	O
)	O
1	O
−	O
1	O
−	O
√	O
4δ	O
≥	O
1	O
2	O
28.2.2	O
where	O
the	O
last	O
inequality	O
follows	O
by	O
standard	O
algebraic	O
manipulations	O
.	O
this	O
con-	O
cludes	O
our	O
proof	O
.	O
√	O
let	O
ρ	O
=	O
8	O
and	O
note	O
that	O
ρ	O
∈	O
(	O
0	O
,	O
1/	O
showing	O
that	O
m	O
(	O
	O
,	O
1/8	O
)	O
≥	O
8d/2	O
we	O
shall	O
now	O
prove	O
that	O
for	O
every	O
	O
<	O
1/	O
(	O
8	O
2	O
)	O
.	O
we	O
will	O
construct	O
a	O
family	O
of	O
distri-	O
butions	O
as	O
follows	O
.	O
first	O
,	O
let	O
c	O
=	O
{	O
c1	O
,	O
.	O
.	O
.	O
,	O
cd	O
}	O
be	O
a	O
set	B
of	O
d	O
instances	O
which	O
are	O
shattered	O
by	O
h.	O
second	O
,	O
for	O
each	O
vector	O
(	O
b1	O
,	O
.	O
.	O
.	O
,	O
bd	O
)	O
∈	O
{	O
±1	O
}	O
d	O
,	O
deﬁne	O
a	O
distribu-	O
tion	O
db	O
such	O
that	O
2	O
)	O
we	O
have	O
that	O
m	O
(	O
	O
,	O
δ	O
)	O
≥	O
8d	O
2	O
.	O
√	O
(	O
cid:40	O
)	O
1	O
d	O
·	O
1+ybiρ	O
2	O
0	O
if	O
∃i	O
:	O
x	O
=	O
ci	O
otherwise	O
.	O
db	O
(	O
{	O
(	O
x	O
,	O
y	O
)	O
}	O
)	O
=	O
that	O
is	O
,	O
to	O
sample	O
an	O
example	O
according	O
to	O
db	O
,	O
we	O
ﬁrst	O
sample	O
an	O
element	O
ci	O
∈	O
c	O
uniformly	O
at	O
random	O
,	O
and	O
then	O
set	B
the	O
label	B
to	O
be	O
bi	O
with	O
probability	O
(	O
1	O
+	O
ρ	O
)	O
/2	O
or	O
−bi	O
with	O
probability	O
(	O
1	O
−	O
ρ	O
)	O
/2	O
.	O
it	O
is	O
easy	O
to	O
verify	O
that	O
the	O
bayes	O
optimal	O
predictor	B
for	O
db	O
is	O
the	O
hypothesis	B
396	O
proof	O
of	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
h	O
∈	O
h	O
such	O
that	O
h	O
(	O
ci	O
)	O
=	O
bi	O
for	O
all	O
i	O
∈	O
[	O
d	O
]	O
,	O
and	O
its	O
error	O
is	O
1−ρ	O
any	O
other	O
function	B
f	O
:	O
x	O
→	O
{	O
±1	O
}	O
,	O
it	O
is	O
easy	O
to	O
verify	O
that	O
2	O
.	O
in	O
addition	O
,	O
for	O
ldb	O
(	O
f	O
)	O
=	O
1	O
+	O
ρ	O
2	O
·	O
|	O
{	O
i	O
∈	O
[	O
d	O
]	O
:	O
f	O
(	O
ci	O
)	O
(	O
cid:54	O
)	O
=	O
bi	O
}	O
|	O
d	O
1	O
−	O
ρ	O
2	O
+	O
·	O
|	O
{	O
i	O
∈	O
[	O
d	O
]	O
:	O
f	O
(	O
ci	O
)	O
=	O
bi	O
}	O
|	O
d	O
.	O
therefore	O
,	O
ldb	O
(	O
f	O
)	O
−	O
min	O
h∈h	O
ldb	O
(	O
h	O
)	O
=	O
ρ	O
·	O
|	O
{	O
i	O
∈	O
[	O
d	O
]	O
:	O
f	O
(	O
ci	O
)	O
(	O
cid:54	O
)	O
=	O
bi	O
}	O
|	O
d	O
.	O
(	O
28.2	O
)	O
next	O
,	O
ﬁx	O
some	O
learning	O
algorithm	O
a.	O
as	O
in	O
the	O
proof	O
of	O
the	O
no-free-lunch	B
theorem	O
,	O
we	O
have	O
that	O
max	O
db	O
:	O
b∈	O
{	O
±1	O
}	O
d	O
e	O
s∼dm	O
b	O
(	O
cid:20	O
)	O
ldb	O
(	O
a	O
(	O
s	O
)	O
)	O
−	O
min	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
h∈h	O
ldb	O
(	O
h	O
)	O
(	O
cid:20	O
)	O
ldb	O
(	O
a	O
(	O
s	O
)	O
)	O
−	O
min	O
ρ	O
·	O
|	O
{	O
i	O
∈	O
[	O
d	O
]	O
:	O
a	O
(	O
s	O
)	O
(	O
ci	O
)	O
(	O
cid:54	O
)	O
=	O
bi|	O
h∈h	O
ldb	O
(	O
h	O
)	O
d	O
(	O
cid:21	O
)	O
(	O
28.3	O
)	O
(	O
28.4	O
)	O
(	O
28.5	O
)	O
(	O
28.6	O
)	O
e	O
db	O
:	O
b∼u	O
(	O
{	O
±1	O
}	O
d	O
)	O
e	O
s∼dm	O
b	O
e	O
db	O
:	O
b∼u	O
(	O
{	O
±1	O
}	O
d	O
)	O
e	O
s∼dm	O
b	O
≥	O
=	O
=	O
d	O
(	O
cid:88	O
)	O
i=1	O
ρ	O
d	O
e	O
db	O
:	O
b∼u	O
(	O
{	O
±1	O
}	O
d	O
)	O
e	O
s∼dm	O
b	O
1	O
[	O
a	O
(	O
s	O
)	O
(	O
ci	O
)	O
(	O
cid:54	O
)	O
=bi	O
]	O
,	O
d	O
(	O
cid:88	O
)	O
i=1	O
ρ	O
d	O
where	O
the	O
ﬁrst	O
equality	O
follows	O
from	O
equation	O
(	O
28.2	O
)	O
.	O
in	O
addition	O
,	O
using	O
the	O
deﬁnition	O
of	O
db	O
,	O
to	O
sample	O
s	O
∼	O
db	O
we	O
can	O
ﬁrst	O
sample	O
(	O
j1	O
,	O
.	O
.	O
.	O
,	O
jm	O
)	O
∼	O
u	O
(	O
[	O
d	O
]	O
)	O
m	O
,	O
set	B
xr	O
=	O
cji	O
,	O
and	O
ﬁnally	O
sample	O
yr	O
such	O
that	O
p	O
[	O
yr	O
=	O
bji	O
]	O
=	O
(	O
1	O
+	O
ρ	O
)	O
/2	O
.	O
let	O
us	O
simplify	O
the	O
notation	O
and	O
use	O
y	O
∼	O
b	O
to	O
denote	O
sampling	O
according	O
to	O
p	O
[	O
y	O
=	O
b	O
]	O
=	O
(	O
1	O
+	O
ρ	O
)	O
/2	O
.	O
therefore	O
,	O
the	O
right-hand	O
side	O
of	O
equation	O
(	O
28.6	O
)	O
equals	O
e	O
j∼u	O
(	O
[	O
d	O
]	O
)	O
m	O
e	O
b∼u	O
(	O
{	O
±1	O
}	O
d	O
)	O
e	O
∀r	O
,	O
yr∼bjr	O
1	O
[	O
a	O
(	O
s	O
)	O
(	O
ci	O
)	O
(	O
cid:54	O
)	O
=bi	O
]	O
.	O
(	O
28.7	O
)	O
we	O
now	O
proceed	O
in	O
two	O
steps	O
.	O
first	O
,	O
we	O
show	O
that	O
among	O
all	O
learning	O
algorithms	O
,	O
a	O
,	O
the	O
one	O
which	O
minimizes	O
equation	O
(	O
28.7	O
)	O
(	O
and	O
hence	O
also	O
equation	O
(	O
28.4	O
)	O
)	O
is	O
the	O
maximum-likelihood	O
learning	O
rule	O
,	O
denoted	O
am	O
l.	O
formally	O
,	O
for	O
each	O
i	O
,	O
am	O
l	O
(	O
s	O
)	O
(	O
ci	O
)	O
is	O
the	O
majority	O
vote	O
among	O
the	O
set	B
{	O
yr	O
:	O
r	O
∈	O
[	O
m	O
]	O
,	O
xr	O
=	O
ci	O
}	O
.	O
second	O
,	O
we	O
lower	O
bound	O
equation	O
(	O
28.7	O
)	O
for	O
am	O
l.	O
lemma	O
28.1	O
among	O
all	O
algorithms	O
,	O
equation	O
(	O
28.4	O
)	O
is	O
minimized	O
for	O
a	O
being	O
the	O
maximum-likelihood	O
algorithm	O
,	O
am	O
l	O
,	O
deﬁned	O
as	O
∀i	O
,	O
am	O
l	O
(	O
s	O
)	O
(	O
ci	O
)	O
=	O
sign	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
yr	O
.	O
proof	O
fix	O
some	O
j	O
∈	O
[	O
d	O
]	O
m.	O
note	O
that	O
given	O
j	O
and	O
y	O
∈	O
{	O
±1	O
}	O
m	O
,	O
the	O
training	B
set	I
s	O
is	O
fully	O
determined	O
.	O
therefore	O
,	O
we	O
can	O
write	O
a	O
(	O
j	O
,	O
y	O
)	O
instead	O
of	O
a	O
(	O
s	O
)	O
.	O
let	O
us	O
also	O
ﬁx	O
i	O
∈	O
[	O
d	O
]	O
.	O
denote	O
b¬i	O
the	O
sequence	O
(	O
b1	O
,	O
.	O
.	O
.	O
,	O
bi−1	O
,	O
bi+1	O
,	O
.	O
.	O
.	O
,	O
bm	O
)	O
.	O
also	O
,	O
for	O
any	O
r	O
:	O
xr=ci	O
28.2	O
the	O
lower	O
bound	O
for	O
the	O
agnostic	O
case	O
397	O
y	O
∈	O
{	O
±1	O
}	O
m	O
,	O
let	O
yi	O
denote	O
the	O
elements	O
of	O
y	O
corresponding	O
to	O
indices	O
for	O
which	O
jr	O
=	O
i	O
and	O
let	O
y¬i	O
be	O
the	O
rest	O
of	O
the	O
elements	O
of	O
y.	O
we	O
have	O
b∼u	O
(	O
{	O
±1	O
}	O
d	O
)	O
∀r	O
,	O
yr∼bjr	O
1	O
[	O
a	O
(	O
s	O
)	O
(	O
ci	O
)	O
(	O
cid:54	O
)	O
=bi	O
]	O
e	O
(	O
cid:88	O
)	O
e	O
=	O
1	O
2	O
b¬i∼u	O
(	O
{	O
±1	O
}	O
d−1	O
)	O
bi∈	O
{	O
±1	O
}	O
(	O
cid:88	O
)	O
y	O
e	O
(	O
cid:88	O
)	O
y¬i	O
p	O
[	O
y|b¬i	O
,	O
bi	O
]	O
1	O
[	O
a	O
(	O
j	O
,	O
y	O
)	O
(	O
ci	O
)	O
(	O
cid:54	O
)	O
=bi	O
]	O
(	O
cid:88	O
)	O
	O
(	O
cid:88	O
)	O
yi	O
bi∈	O
{	O
±1	O
}	O
	O
.	O
=	O
e	O
b¬i∼u	O
(	O
{	O
±1	O
}	O
d−1	O
)	O
p	O
[	O
y¬i|b¬i	O
]	O
1	O
2	O
p	O
[	O
yi|bi	O
]	O
1	O
[	O
a	O
(	O
j	O
,	O
y	O
)	O
(	O
ci	O
)	O
(	O
cid:54	O
)	O
=bi	O
]	O
the	O
sum	O
within	O
the	O
parentheses	O
is	O
minimized	O
when	O
a	O
(	O
j	O
,	O
y	O
)	O
(	O
ci	O
)	O
is	O
the	O
maximizer	O
of	O
p	O
[	O
yi|bi	O
]	O
over	O
bi	O
∈	O
{	O
±1	O
}	O
,	O
which	O
is	O
exactly	O
the	O
maximum-likelihood	O
rule	O
.	O
re-	O
peating	O
the	O
same	O
argument	O
for	O
all	O
i	O
we	O
conclude	O
our	O
proof	O
.	O
fix	O
i.	O
for	O
every	O
j	O
,	O
let	O
ni	O
(	O
j	O
)	O
=	O
{	O
|t	O
:	O
jt	O
=	O
i|	O
}	O
be	O
the	O
number	O
of	O
instances	O
in	O
which	O
the	O
instance	B
is	O
ci	O
.	O
for	O
the	O
maximum-likelihood	O
rule	O
,	O
we	O
have	O
that	O
the	O
quantity	O
e	O
b∼u	O
(	O
{	O
±1	O
}	O
d	O
)	O
e	O
∀r	O
,	O
yr∼bjr	O
1	O
[	O
am	O
l	O
(	O
s	O
)	O
(	O
ci	O
)	O
(	O
cid:54	O
)	O
=bi	O
]	O
is	O
exactly	O
the	O
probability	O
that	O
a	O
binomial	O
(	O
ni	O
(	O
j	O
)	O
,	O
(	O
1	O
−	O
ρ	O
)	O
/2	O
)	O
random	O
variable	O
will	O
be	O
larger	O
than	O
ni	O
(	O
j	O
)	O
/2	O
.	O
using	O
lemma	O
b.11	O
,	O
and	O
the	O
assumption	O
ρ2	O
≤	O
1/2	O
,	O
we	O
have	O
that	O
(	O
cid:16	O
)	O
1	O
−	O
(	O
cid:112	O
)	O
1	O
−	O
e−2ni	O
(	O
j	O
)	O
ρ2	O
(	O
cid:17	O
)	O
.	O
p	O
[	O
b	O
≥	O
ni	O
(	O
j	O
)	O
/2	O
]	O
≥	O
1	O
2	O
we	O
have	O
thus	O
shown	O
that	O
d	O
(	O
cid:88	O
)	O
i=1	O
ρ	O
d	O
e	O
d	O
(	O
cid:88	O
)	O
d	O
(	O
cid:88	O
)	O
i=1	O
i=1	O
≥	O
ρ	O
2d	O
≥	O
ρ	O
2d	O
j∼u	O
(	O
[	O
d	O
]	O
)	O
m	O
b∼u	O
(	O
{	O
±1	O
}	O
d	O
)	O
e	O
e	O
1	O
[	O
a	O
(	O
s	O
)	O
(	O
ci	O
)	O
(	O
cid:54	O
)	O
=bi	O
]	O
(	O
cid:17	O
)	O
∀r	O
,	O
yr∼bjr	O
(	O
cid:16	O
)	O
1	O
−	O
(	O
cid:112	O
)	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
1	O
−	O
(	O
cid:112	O
)	O
2ρ2ni	O
(	O
j	O
)	O
1	O
−	O
e−2ρ2ni	O
(	O
j	O
)	O
,	O
e	O
j∼u	O
(	O
[	O
d	O
]	O
)	O
m	O
e	O
j∼u	O
(	O
[	O
d	O
]	O
)	O
m	O
where	O
in	O
the	O
last	O
inequality	O
we	O
used	O
the	O
inequality	O
1	O
−	O
e−a	O
≤	O
a.	O
since	O
the	O
square	O
root	O
function	B
is	O
concave	O
,	O
we	O
can	O
apply	O
jensen	O
’	O
s	O
inequality	O
to	O
obtain	O
that	O
the	O
above	O
is	O
lower	O
bounded	O
by	O
(	O
cid:33	O
)	O
j∼u	O
(	O
[	O
d	O
]	O
)	O
m	O
ni	O
(	O
j	O
)	O
2ρ2	O
(	O
cid:32	O
)	O
1	O
−	O
(	O
cid:114	O
)	O
d	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
d	O
(	O
cid:88	O
)	O
1	O
−	O
(	O
cid:112	O
)	O
2ρ2m/d	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
1	O
−	O
(	O
cid:112	O
)	O
2ρ2m/d	O
i=1	O
i=1	O
.	O
e	O
(	O
cid:17	O
)	O
≥	O
ρ	O
2d	O
=	O
=	O
ρ	O
2d	O
ρ	O
2	O
398	O
proof	O
of	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
as	O
long	O
as	O
m	O
<	O
d	O
8ρ2	O
,	O
this	O
term	O
would	O
be	O
larger	O
than	O
ρ/4	O
.	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
in	O
summary	O
,	O
we	O
have	O
shown	O
that	O
if	O
m	O
<	O
d	O
8ρ2	O
then	O
for	O
any	O
algorithm	O
there	O
exists	O
a	O
distribution	O
such	O
that	O
e	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
−	O
min	O
h∈h	O
ld	O
(	O
h	O
)	O
≥	O
ρ/4	O
.	O
s∼dm	O
ρ	O
(	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
−	O
minh∈h	O
ld	O
(	O
h	O
)	O
)	O
and	O
note	O
that	O
∆	O
∈	O
[	O
0	O
,	O
1	O
]	O
(	O
see	O
finally	O
,	O
let	O
∆	O
=	O
1	O
equation	O
(	O
28.5	O
)	O
)	O
.	O
therefore	O
,	O
using	O
lemma	O
b.1	O
,	O
we	O
get	O
that	O
p	O
[	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
−	O
min	O
h∈h	O
ld	O
(	O
h	O
)	O
>	O
	O
]	O
=	O
p	O
≥	O
1	O
4	O
∆	O
>	O
−	O
	O
ρ	O
.	O
	O
ρ	O
≥	O
e	O
[	O
∆	O
]	O
−	O
	O
ρ	O
choosing	O
ρ	O
=	O
8	O
we	O
conclude	O
that	O
if	O
m	O
<	O
d	O
1/8	O
we	O
will	O
have	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
−	O
minh∈h	O
ld	O
(	O
h	O
)	O
≥	O
	O
.	O
512	O
2	O
,	O
then	O
with	O
probability	O
of	O
at	O
least	O
28.3	O
the	O
upper	O
bound	O
for	O
the	O
realizable	O
case	O
here	O
we	O
prove	O
that	O
there	O
exists	O
c	O
such	O
that	O
h	O
is	O
pac	O
learnable	O
with	O
sample	B
complexity	I
mh	O
(	O
	O
,	O
δ	O
)	O
≤	O
c	O
d	O
ln	O
(	O
1/	O
)	O
+	O
ln	O
(	O
1/δ	O
)	O
	O
.	O
we	O
do	O
so	O
by	O
showing	O
that	O
for	O
m	O
≥	O
c	O
d	O
ln	O
(	O
1/	O
)	O
+ln	O
(	O
1/δ	O
)	O
erm	O
rule	O
.	O
we	O
prove	O
this	O
claim	O
based	O
on	O
the	O
notion	O
of	O
-nets	O
.	O
definition	O
28.2	O
(	O
-net	O
)	O
let	O
x	O
be	O
a	O
domain	B
.	O
s	O
⊂	O
x	O
is	O
an	O
-net	O
for	O
h	O
⊂	O
2x	O
with	O
respect	O
to	O
a	O
distribution	O
d	O
over	O
x	O
if	O
,	O
h	O
is	O
learnable	O
using	O
the	O
	O
∀h	O
∈	O
h	O
:	O
d	O
(	O
h	O
)	O
≥	O
	O
⇒	O
h	O
∩	O
s	O
(	O
cid:54	O
)	O
=	O
∅	O
.	O
theorem	O
28.3	O
let	O
h	O
⊂	O
2x	O
with	O
vcdim	O
(	O
h	O
)	O
=	O
d.	O
fix	O
	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
δ	O
∈	O
(	O
0	O
,	O
1/4	O
)	O
and	O
let	O
(	O
cid:18	O
)	O
m	O
≥	O
8	O
	O
(	O
cid:18	O
)	O
16e	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
2	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
2d	O
log	O
+	O
log	O
.	O
δ	O
	O
then	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
a	O
choice	O
of	O
s	O
∼	O
dm	O
we	O
have	O
that	O
s	O
is	O
an	O
-net	O
for	O
h.	O
proof	O
let	O
b	O
=	O
{	O
s	O
⊂	O
x	O
:	O
|s|	O
=	O
m	O
,	O
∃h	O
∈	O
h	O
,	O
d	O
(	O
h	O
)	O
≥	O
	O
,	O
h	O
∩	O
s	O
=	O
∅	O
}	O
be	O
the	O
set	B
of	O
sets	O
which	O
are	O
not	O
-nets	O
.	O
we	O
need	O
to	O
bound	O
p	O
[	O
s	O
∈	O
b	O
]	O
.	O
deﬁne	O
2	O
}	O
.	O
b	O
(	O
cid:48	O
)	O
=	O
{	O
(	O
s	O
,	O
t	O
)	O
⊂	O
x	O
:	O
|s|	O
=	O
|t|	O
=	O
m	O
,	O
∃h	O
∈	O
h	O
,	O
d	O
(	O
h	O
)	O
≥	O
	O
,	O
h	O
∩	O
s	O
=	O
∅	O
,	O
|t	O
∩	O
h|	O
>	O
m	O
28.3	O
the	O
upper	O
bound	O
for	O
the	O
realizable	O
case	O
399	O
claim	O
1	O
p	O
[	O
s	O
∈	O
b	O
]	O
≤	O
2	O
p	O
[	O
(	O
s	O
,	O
t	O
)	O
∈	O
b	O
(	O
cid:48	O
)	O
]	O
.	O
proof	O
of	O
claim	O
1	O
:	O
since	O
s	O
and	O
t	O
are	O
chosen	O
independently	O
we	O
can	O
write	O
(	O
cid:2	O
)	O
1	O
[	O
(	O
s	O
,	O
t	O
)	O
∈b	O
(	O
cid:48	O
)	O
]	O
(	O
cid:3	O
)	O
=	O
e	O
(	O
cid:104	O
)	O
e	O
(	O
cid:2	O
)	O
1	O
[	O
(	O
s	O
,	O
t	O
)	O
∈b	O
(	O
cid:48	O
)	O
]	O
(	O
cid:3	O
)	O
(	O
cid:105	O
)	O
.	O
p	O
[	O
(	O
s	O
,	O
t	O
)	O
∈	O
b	O
(	O
cid:48	O
)	O
]	O
=	O
e	O
(	O
s	O
,	O
t	O
)	O
∼d2m	O
s∼dm	O
t∼dm	O
note	O
that	O
(	O
s	O
,	O
t	O
)	O
∈	O
b	O
(	O
cid:48	O
)	O
implies	O
s	O
∈	O
b	O
and	O
therefore	O
1	O
[	O
(	O
s	O
,	O
t	O
)	O
∈b	O
(	O
cid:48	O
)	O
]	O
=	O
1	O
[	O
(	O
s	O
,	O
t	O
)	O
∈b	O
(	O
cid:48	O
)	O
]	O
1	O
[	O
s∈b	O
]	O
,	O
which	O
gives	O
p	O
[	O
(	O
s	O
,	O
t	O
)	O
∈	O
b	O
(	O
cid:48	O
)	O
]	O
=	O
e	O
s∼dm	O
=	O
e	O
s∼dm	O
e	O
t∼dm	O
1	O
[	O
s∈b	O
]	O
e	O
t∼dm	O
1	O
[	O
(	O
s	O
,	O
t	O
)	O
∈b	O
(	O
cid:48	O
)	O
]	O
1	O
[	O
s∈b	O
]	O
1	O
[	O
(	O
s	O
,	O
t	O
)	O
∈b	O
(	O
cid:48	O
)	O
]	O
.	O
fix	O
some	O
s.	O
then	O
,	O
either	O
1	O
[	O
s∈b	O
]	O
=	O
0	O
or	O
s	O
∈	O
b	O
and	O
then	O
∃hs	O
such	O
that	O
d	O
(	O
hs	O
)	O
≥	O
	O
and	O
|hs	O
∩	O
s|	O
=	O
0.	O
it	O
follows	O
that	O
a	O
suﬃcient	O
condition	O
for	O
(	O
s	O
,	O
t	O
)	O
∈	O
b	O
(	O
cid:48	O
)	O
is	O
that	O
|t	O
∩	O
hs|	O
>	O
m	O
2	O
.	O
therefore	O
,	O
whenever	O
s	O
∈	O
b	O
we	O
have	O
e	O
t∼dm	O
1	O
[	O
(	O
s	O
,	O
t	O
)	O
∈b	O
(	O
cid:48	O
)	O
]	O
≥	O
p	O
t∼dm	O
[	O
|t	O
∩	O
hs|	O
>	O
m	O
2	O
]	O
.	O
but	O
,	O
since	O
we	O
now	O
assume	O
s	O
∈	O
b	O
we	O
know	O
that	O
d	O
(	O
hs	O
)	O
=	O
ρ	O
≥	O
	O
.	O
therefore	O
,	O
|t	O
∩	O
hs|	O
is	O
a	O
binomial	O
random	O
variable	O
with	O
parameters	O
ρ	O
(	O
probability	O
of	O
success	O
for	O
a	O
single	O
try	O
)	O
and	O
m	O
(	O
number	O
of	O
tries	O
)	O
.	O
chernoﬀ	O
’	O
s	O
inequality	O
implies	O
p	O
[	O
|t∩hs|	O
≤	O
ρm	O
thus	O
,	O
p	O
[	O
|t	O
∩	O
hs|	O
>	O
m	O
combining	O
all	O
the	O
preceding	O
we	O
conclude	O
the	O
proof	O
of	O
claim	O
1	O
.	O
2	O
]	O
≥	O
1	O
−	O
p	O
[	O
|t	O
∩	O
hs|	O
≤	O
ρm	O
2	O
]	O
=	O
1	O
−	O
p	O
[	O
|t	O
∩	O
hs|	O
≤	O
m	O
−	O
2	O
mρ	O
(	O
mρ−mρ/2	O
)	O
2	O
2	O
]	O
≥	O
1/2	O
.	O
2	O
]	O
≤	O
e	O
=	O
e−mρ/2	O
≤	O
e−m/2	O
≤	O
e−d	O
log	O
(	O
1/δ	O
)	O
/2	O
=	O
δd/2	O
≤	O
1/2	O
.	O
claim	O
2	O
(	O
symmetrization	O
)	O
:	O
p	O
[	O
(	O
s	O
,	O
t	O
)	O
∈	O
b	O
(	O
cid:48	O
)	O
]	O
≤	O
e−m/4	O
τh	O
(	O
2m	O
)	O
.	O
proof	O
of	O
claim	O
2	O
:	O
to	O
simplify	O
notation	O
,	O
let	O
α	O
=	O
m/2	O
and	O
for	O
a	O
sequence	O
a	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
x2m	O
)	O
let	O
a0	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
.	O
using	O
the	O
deﬁnition	O
of	O
b	O
(	O
cid:48	O
)	O
we	O
get	O
that	O
p	O
[	O
a	O
∈	O
b	O
(	O
cid:48	O
)	O
]	O
=	O
e	O
≤	O
e	O
a∼d2m	O
a∼d2m	O
max	O
h∈h	O
max	O
h∈h	O
1	O
[	O
d	O
(	O
h	O
)	O
≥	O
]	O
1	O
[	O
|h∩a0|=0	O
]	O
1	O
[	O
|h∩a|≥α	O
]	O
1	O
[	O
|h∩a0|=0	O
]	O
1	O
[	O
|h∩a|≥α	O
]	O
.	O
now	O
,	O
let	O
us	O
deﬁne	O
by	O
ha	O
the	O
eﬀective	O
number	O
of	O
diﬀerent	O
hypotheses	O
on	O
a	O
,	O
namely	O
,	O
ha	O
=	O
{	O
h	O
∩	O
a	O
:	O
h	O
∈	O
h	O
}	O
.	O
it	O
follows	O
that	O
p	O
[	O
a	O
∈	O
b	O
(	O
cid:48	O
)	O
]	O
≤	O
e	O
≤	O
e	O
a∼d2m	O
a∼d2m	O
max	O
h∈ha	O
(	O
cid:88	O
)	O
h∈ha	O
1	O
[	O
|h∩a0|=0	O
]	O
1	O
[	O
|h∩a|≥α	O
]	O
1	O
[	O
|h∩a0|=0	O
]	O
1	O
[	O
|h∩a|≥α	O
]	O
.	O
let	O
j	O
=	O
{	O
j	O
⊂	O
[	O
2m	O
]	O
:	O
|j|	O
=	O
m	O
}	O
.	O
for	O
any	O
j	O
∈	O
j	O
and	O
a	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
x2m	O
)	O
deﬁne	O
aj	O
=	O
(	O
xj1	O
,	O
.	O
.	O
.	O
,	O
xjm	O
)	O
.	O
since	O
the	O
elements	O
of	O
a	O
are	O
chosen	O
i.i.d.	B
,	O
we	O
have	O
that	O
for	O
any	O
j	O
∈	O
j	O
and	O
any	O
function	B
f	O
(	O
a	O
,	O
a0	O
)	O
it	O
holds	O
that	O
ea∼d2m	O
[	O
f	O
(	O
a	O
,	O
a0	O
)	O
]	O
=	O
400	O
proof	O
of	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
(	O
cid:80	O
)	O
ea∼d2m	O
[	O
f	O
(	O
a	O
,	O
aj	O
)	O
]	O
.	O
since	O
this	O
holds	O
for	O
any	O
j	O
it	O
also	O
holds	O
for	O
the	O
expectation	O
of	O
j	O
chosen	O
at	O
random	O
from	O
j.	O
in	O
particular	O
,	O
it	O
holds	O
for	O
the	O
function	B
f	O
(	O
a	O
,	O
a0	O
)	O
=	O
h∈ha	O
1	O
[	O
|h∩a0|=0	O
]	O
1	O
[	O
|h∩a|≥α	O
]	O
.	O
we	O
therefore	O
obtain	O
that	O
(	O
cid:88	O
)	O
p	O
[	O
a	O
∈	O
b	O
(	O
cid:48	O
)	O
]	O
≤	O
e	O
a∼d2m	O
=	O
e	O
a∼d2m	O
e	O
j∼j	O
(	O
cid:88	O
)	O
h∈ha	O
1	O
[	O
|h∩aj|=0	O
]	O
1	O
[	O
|h∩a|≥α	O
]	O
h∈ha	O
1	O
[	O
|h∩a|≥α	O
]	O
e	O
j∼j	O
1	O
[	O
|h∩aj|=0	O
]	O
.	O
now	O
,	O
ﬁx	O
some	O
a	O
s.t	O
.	O
|h	O
∩	O
a|	O
≥	O
α.	O
then	O
,	O
ej	O
1	O
[	O
|h∩aj|=0	O
]	O
is	O
the	O
probability	O
that	O
when	O
choosing	O
m	O
balls	O
from	O
a	O
bag	O
with	O
at	O
least	O
α	O
red	O
balls	O
,	O
we	O
will	O
never	O
choose	O
a	O
red	O
ball	O
.	O
this	O
probability	O
is	O
at	O
most	O
(	O
1	O
−	O
α/	O
(	O
2m	O
)	O
)	O
m	O
=	O
(	O
1	O
−	O
/4	O
)	O
m	O
≤	O
e−m/4	O
.	O
we	O
therefore	O
get	O
that	O
p	O
[	O
a	O
∈	O
b	O
(	O
cid:48	O
)	O
]	O
≤	O
e	O
a∼d2m	O
(	O
cid:88	O
)	O
h∈ha	O
e−m/4	O
≤	O
e−m/4	O
e	O
a∼d2m	O
|ha|	O
.	O
using	O
the	O
deﬁnition	O
of	O
the	O
growth	B
function	I
we	O
conclude	O
the	O
proof	O
of	O
claim	O
2.	O
completing	O
the	O
proof	O
:	O
by	O
sauer	O
’	O
s	O
lemma	O
we	O
know	O
that	O
τh	O
(	O
2m	O
)	O
≤	O
(	O
2em/d	O
)	O
d.	O
combining	O
this	O
with	O
the	O
two	O
claims	O
we	O
obtain	O
that	O
p	O
[	O
s	O
∈	O
b	O
]	O
≤	O
2	O
(	O
2em/d	O
)	O
d	O
e−m/4	O
.	O
we	O
would	O
like	O
the	O
right-hand	O
side	O
of	O
the	O
inequality	O
to	O
be	O
at	O
most	O
δ	O
;	O
that	O
is	O
,	O
2	O
(	O
2em/d	O
)	O
d	O
e−m/4	O
≤	O
δ.	O
rearranging	O
,	O
we	O
obtain	O
the	O
requirement	O
m	O
≥	O
4	O
	O
(	O
d	O
log	O
(	O
2em/d	O
)	O
+	O
log	O
(	O
2/δ	O
)	O
)	O
=	O
4d	O
	O
log	O
(	O
m	O
)	O
+	O
4	O
	O
(	O
d	O
log	O
(	O
2e/d	O
)	O
+	O
log	O
(	O
2/δ	O
)	O
.	O
using	O
lemma	O
a.2	O
,	O
a	O
suﬃcient	O
condition	O
for	O
the	O
preceding	O
to	O
hold	O
is	O
that	O
a	O
suﬃcient	O
condition	O
for	O
this	O
is	O
that	O
	O
+	O
8	O
	O
(	O
cid:18	O
)	O
8d	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
8d	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
8d	O
2e	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
16e	O
16	O
	O
d	O
+	O
	O
m	O
≥	O
16d	O
	O
log	O
log	O
(	O
cid:18	O
)	O
log	O
m	O
≥	O
16d	O
	O
=	O
=	O
16d	O
(	O
cid:18	O
)	O
	O
8	O
	O
+	O
log	O
(	O
2/δ	O
)	O
8	O
	O
(	O
cid:18	O
)	O
2	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
2d	O
log	O
+	O
log	O
.	O
δ	O
	O
(	O
d	O
log	O
(	O
2e/d	O
)	O
+	O
log	O
(	O
2/δ	O
)	O
.	O
(	O
d	O
log	O
(	O
2e/d	O
)	O
+	O
1	O
2	O
log	O
(	O
2/δ	O
)	O
and	O
this	O
concludes	O
our	O
proof	O
.	O
28.3	O
the	O
upper	O
bound	O
for	O
the	O
realizable	O
case	O
401	O
28.3.1	O
from	O
-nets	O
to	O
pac	O
learnability	O
theorem	O
28.4	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
over	O
x	O
with	O
vcdim	O
(	O
h	O
)	O
=	O
d.	O
let	O
d	O
be	O
a	O
distribution	O
over	O
x	O
and	O
let	O
c	O
∈	O
h	O
be	O
a	O
target	O
hypothesis	O
.	O
fix	O
	O
,	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
and	O
let	O
m	O
be	O
as	O
deﬁned	O
in	O
theorem	O
28.3.	O
then	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
a	O
choice	O
of	O
m	O
i.i.d	O
.	O
instances	O
from	O
x	O
with	O
labels	O
according	O
to	O
c	O
we	O
have	O
that	O
any	O
erm	O
hypothesis	B
has	O
a	O
true	B
error	I
of	O
at	O
most	O
	O
.	O
proof	O
deﬁne	O
the	O
class	O
hc	O
=	O
{	O
c	O
(	O
cid:97	O
)	O
h	O
:	O
h	O
∈	O
h	O
}	O
,	O
where	O
c	O
(	O
cid:97	O
)	O
h	O
=	O
(	O
h\	O
c	O
)	O
∪	O
(	O
c\	O
h	O
)	O
.	O
it	O
is	O
note	O
that	O
ld	O
(	O
h	O
)	O
=	O
d	O
(	O
h	O
(	O
cid:97	O
)	O
c	O
)	O
.	O
therefore	O
,	O
for	O
any	O
h	O
∈	O
h	O
with	O
ld	O
(	O
h	O
)	O
≥	O
	O
we	O
have	O
that	O
|	O
(	O
h	O
(	O
cid:97	O
)	O
c	O
)	O
∩	O
s|	O
>	O
0	O
,	O
which	O
implies	O
that	O
h	O
can	O
not	O
be	O
an	O
erm	O
hypothesis	B
,	O
which	O
easy	O
to	O
verify	O
that	O
if	O
some	O
a	O
⊂	O
x	O
is	O
shattered	O
by	O
h	O
then	O
it	O
is	O
also	O
shattered	O
by	O
hc	O
and	O
vice	O
versa	O
.	O
hence	O
,	O
vcdim	O
(	O
h	O
)	O
=	O
vcdim	O
(	O
hc	O
)	O
.	O
therefore	O
,	O
using	O
theorem	O
28.3	O
we	O
know	O
that	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
,	O
the	O
sample	O
s	O
is	O
an	O
-net	O
for	O
hc	O
.	O
concludes	O
our	O
proof	O
.	O
29	O
multiclass	B
learnability	O
in	O
chapter	O
17	O
we	O
have	O
introduced	O
the	O
problem	O
of	O
multiclass	B
categorization	O
,	O
in	O
which	O
the	O
goal	O
is	O
to	O
learn	O
a	O
predictor	B
h	O
:	O
x	O
→	O
[	O
k	O
]	O
.	O
in	O
this	O
chapter	O
we	O
address	O
pac	O
learnability	O
of	O
multiclass	B
predictors	O
with	O
respect	O
to	O
the	O
0-1	B
loss	I
.	O
as	O
in	O
chapter	O
6	O
,	O
the	O
main	O
goal	O
of	O
this	O
chapter	O
is	O
to	O
:	O
•	O
characterize	O
which	O
classes	O
of	O
multiclass	B
hypotheses	O
are	O
learnable	O
in	O
the	O
(	O
mul-	O
ticlass	O
)	O
pac	O
model	O
.	O
•	O
quantify	O
the	O
sample	B
complexity	I
of	O
such	O
hypothesis	B
classes	O
.	O
in	O
view	O
of	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
(	O
theorem	O
6.8	O
)	O
,	O
it	O
is	O
natu-	O
ral	O
to	O
seek	O
a	O
generalization	O
of	O
the	O
vc	O
dimension	B
to	O
multiclass	B
hypothesis	O
classes	O
.	O
in	O
section	O
29.1	O
we	O
show	O
such	O
a	O
generalization	O
,	O
called	O
the	O
natarajan	O
dimension	B
,	O
and	O
state	O
a	O
generalization	O
of	O
the	O
fundamental	O
theorem	O
based	O
on	O
the	O
natarajan	O
dimension	B
.	O
then	O
,	O
we	O
demonstrate	O
how	O
to	O
calculate	O
the	O
natarajan	O
dimension	B
of	O
several	O
important	O
hypothesis	B
classes	O
.	O
recall	B
that	O
the	O
main	O
message	O
of	O
the	O
fundamental	O
theorem	O
of	O
learning	O
theory	O
is	O
that	O
a	O
hypothesis	B
class	I
of	O
binary	O
classiﬁers	O
is	O
learnable	O
(	O
with	O
respect	O
to	O
the	O
0-1	B
loss	I
)	O
if	O
and	O
only	O
if	O
it	O
has	O
the	O
uniform	B
convergence	I
property	O
,	O
and	O
then	O
it	O
is	O
learnable	O
by	O
any	O
erm	O
learner	O
.	O
in	O
chapter	O
13	O
,	O
exercise	O
2	O
,	O
we	O
have	O
shown	O
that	O
this	O
equivalence	O
breaks	O
down	O
for	O
a	O
certain	O
convex	B
learning	O
problem	O
.	O
the	O
last	O
section	O
of	O
this	O
chapter	O
is	O
devoted	O
to	O
showing	O
that	O
the	O
equivalence	O
between	O
learnability	O
and	O
uniform	B
convergence	I
breaks	O
down	O
even	O
in	O
multiclass	B
problems	O
with	O
the	O
0-1	B
loss	I
,	O
which	O
are	O
very	O
similar	O
to	O
binary	O
classiﬁcation	O
.	O
indeed	O
,	O
we	O
construct	O
a	O
hypothesis	B
class	I
which	O
is	O
learnable	O
by	O
a	O
speciﬁc	O
erm	O
learner	O
,	O
but	O
for	O
which	O
other	O
erm	O
learners	O
might	O
fail	O
and	O
the	O
uniform	B
convergence	I
property	O
does	O
not	O
hold	O
.	O
29.1	O
the	O
natarajan	O
dimension	B
in	O
this	O
section	O
we	O
deﬁne	O
the	O
natarajan	O
dimension	B
,	O
which	O
is	O
a	O
generalization	O
of	O
the	O
vc	O
dimension	B
to	O
classes	O
of	O
multiclass	B
predictors	O
.	O
throughout	O
this	O
section	O
,	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
of	O
multiclass	B
predictors	O
;	O
namely	O
,	O
each	O
h	O
∈	O
h	O
is	O
a	O
function	B
from	O
x	O
to	O
[	O
k	O
]	O
.	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
29.2	O
the	O
multiclass	B
fundamental	O
theorem	O
403	O
to	O
deﬁne	O
the	O
natarajan	O
dimension	B
,	O
we	O
ﬁrst	O
generalize	O
the	O
deﬁnition	O
of	O
shat-	O
tering	O
.	O
definition	O
29.1	O
(	O
shattering	B
(	O
multiclass	B
version	O
)	O
)	O
we	O
say	O
that	O
a	O
set	B
c	O
⊂	O
x	O
is	O
shattered	O
by	O
h	O
if	O
there	O
exist	O
two	O
functions	O
f0	O
,	O
f1	O
:	O
c	O
→	O
[	O
k	O
]	O
such	O
that	O
•	O
for	O
every	O
x	O
∈	O
c	O
,	O
f0	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
f1	O
(	O
x	O
)	O
.	O
•	O
for	O
every	O
b	O
⊂	O
c	O
,	O
there	O
exists	O
a	O
function	B
h	O
∈	O
h	O
such	O
that	O
∀x	O
∈	O
b	O
,	O
h	O
(	O
x	O
)	O
=	O
f0	O
(	O
x	O
)	O
and	O
∀x	O
∈	O
c	O
\	O
b	O
,	O
h	O
(	O
x	O
)	O
=	O
f1	O
(	O
x	O
)	O
.	O
definition	O
29.2	O
(	O
natarajan	O
dimension	B
)	O
the	O
natarajan	O
dimension	B
of	O
h	O
,	O
de-	O
noted	O
ndim	O
(	O
h	O
)	O
,	O
is	O
the	O
maximal	O
size	O
of	O
a	O
shattered	O
set	B
c	O
⊂	O
x	O
.	O
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
in	O
the	O
case	O
that	O
there	O
are	O
exactly	O
two	O
classes	O
,	O
ndim	O
(	O
h	O
)	O
=	O
vcdim	O
(	O
h	O
)	O
.	O
therefore	O
,	O
the	O
natarajan	O
dimension	B
generalizes	O
the	O
vc	O
dimension	B
.	O
we	O
next	O
show	O
that	O
the	O
natarajan	O
dimension	B
allows	O
us	O
to	O
general-	O
ize	O
the	O
fundamental	O
theorem	O
of	O
statistical	O
learning	O
from	O
binary	O
classiﬁcation	O
to	O
multiclass	B
classiﬁcation	O
.	O
29.2	O
the	O
multiclass	B
fundamental	O
theorem	O
theorem	O
29.3	O
(	O
the	O
multiclass	B
fundamental	O
theorem	O
)	O
there	O
exist	O
absolute	O
constants	O
c1	O
,	O
c2	O
>	O
0	O
such	O
that	O
the	O
following	O
holds	O
.	O
for	O
every	O
hypothesis	B
class	I
h	O
of	O
functions	O
from	O
x	O
to	O
[	O
k	O
]	O
,	O
such	O
that	O
the	O
natarajan	O
dimension	B
of	O
h	O
is	O
d	O
,	O
we	O
have	O
1.	O
h	O
has	O
the	O
uniform	B
convergence	I
property	O
with	O
sample	B
complexity	I
d	O
log	O
(	O
k	O
)	O
+	O
log	O
(	O
1/δ	O
)	O
d	O
+	O
log	O
(	O
1/δ	O
)	O
≤	O
much	O
(	O
	O
,	O
δ	O
)	O
≤	O
c2	O
2	O
2.	O
h	O
is	O
agnostic	O
pac	O
learnable	O
with	O
sample	B
complexity	I
c1	O
c1	O
2	O
2	O
d	O
+	O
log	O
(	O
1/δ	O
)	O
≤	O
mh	O
(	O
	O
,	O
δ	O
)	O
≤	O
c2	O
.	O
.	O
.	O
d	O
log	O
(	O
k	O
)	O
+	O
log	O
(	O
1/δ	O
)	O
2	O
d	O
log	O
(	O
cid:0	O
)	O
kd	O
(	O
cid:1	O
)	O
+	O
log	O
(	O
1/δ	O
)	O
	O
	O
3.	O
h	O
is	O
pac	O
learnable	O
(	O
assuming	O
realizability	B
)	O
with	O
sample	B
complexity	I
c1	O
d	O
+	O
log	O
(	O
1/δ	O
)	O
	O
≤	O
mh	O
(	O
	O
,	O
δ	O
)	O
≤	O
c2	O
29.2.1	O
on	O
the	O
proof	O
of	O
theorem	O
29.3	O
the	O
lower	O
bounds	O
in	O
theorem	O
29.3	O
can	O
be	O
deduced	O
by	O
a	O
reduction	O
from	O
the	O
binary	O
fundamental	O
theorem	O
(	O
see	O
exercise	O
5	O
)	O
.	O
the	O
upper	O
bounds	O
in	O
theorem	O
29.3	O
can	O
be	O
proved	O
along	O
the	O
same	O
lines	O
of	O
the	O
proof	O
of	O
the	O
fundamental	O
theorem	O
for	O
binary	O
classiﬁcation	O
,	O
given	O
in	O
chapter	O
28	O
(	O
see	O
exercise	O
4	O
)	O
.	O
the	O
sole	O
ingredient	O
of	O
that	O
proof	O
that	O
should	O
be	O
modiﬁed	O
in	O
a	O
nonstraightforward	O
manner	O
is	O
sauer	O
’	O
s	O
lemma	O
.	O
it	O
applies	O
only	O
to	O
binary	O
classes	O
and	O
therefore	O
must	O
be	O
replaced	O
.	O
an	O
appropriate	O
substitute	O
is	O
natarajan	O
’	O
s	O
lemma	O
:	O
404	O
multiclass	B
learnability	O
lemma	O
29.4	O
(	O
natarajan	O
)	O
|h|	O
≤	O
|x|ndim	O
(	O
h	O
)	O
·	O
k2ndim	O
(	O
h	O
)	O
.	O
the	O
proof	O
of	O
natarajan	O
’	O
s	O
lemma	O
shares	O
the	O
same	O
spirit	O
of	O
the	O
proof	O
of	O
sauer	O
’	O
s	O
lemma	O
and	O
is	O
left	O
as	O
an	O
exercise	O
(	O
see	O
exercise	O
3	O
)	O
.	O
29.3	O
calculating	O
the	O
natarajan	O
dimension	B
in	O
this	O
section	O
we	O
show	O
how	O
to	O
calculate	O
(	O
or	O
estimate	O
)	O
the	O
natarajan	O
dimen-	O
sion	O
of	O
several	O
popular	O
classes	O
,	O
some	O
of	O
which	O
were	O
studied	O
in	O
chapter	O
17.	O
as	O
these	O
calculations	O
indicate	O
,	O
the	O
natarajan	O
dimension	B
is	O
often	O
proportional	O
to	O
the	O
number	O
of	O
parameters	O
required	O
to	O
deﬁne	O
a	O
hypothesis	B
.	O
29.3.1	O
one-versus-all	O
based	O
classes	O
in	O
chapter	O
17	O
we	O
have	O
seen	O
two	O
reductions	B
of	O
multiclass	B
categorization	O
to	O
bi-	O
nary	O
classiﬁcation	O
:	O
one-versus-all	O
and	O
all-pairs	B
.	O
in	O
this	O
section	O
we	O
calculate	O
the	O
natarajan	O
dimension	B
of	O
the	O
one-versus-all	O
method	O
.	O
recall	B
that	O
in	O
one-versus-all	O
we	O
train	O
,	O
for	O
each	O
label	B
,	O
a	O
binary	O
classiﬁer	B
that	O
distinguishes	O
between	O
that	O
label	B
and	O
the	O
rest	O
of	O
the	O
labels	O
.	O
this	O
naturally	O
sug-	O
gests	O
considering	O
multiclass	B
hypothesis	O
classes	O
of	O
the	O
following	O
form	O
.	O
let	O
hbin	O
⊂	O
{	O
0	O
,	O
1	O
}	O
x	O
be	O
a	O
binary	O
hypothesis	B
class	I
.	O
for	O
every	O
¯h	O
=	O
(	O
h1	O
,	O
.	O
.	O
.	O
,	O
hk	O
)	O
∈	O
(	O
hbin	O
)	O
k	O
deﬁne	O
t	O
(	O
¯h	O
)	O
:	O
x	O
→	O
[	O
k	O
]	O
by	O
t	O
(	O
¯h	O
)	O
(	O
x	O
)	O
=	O
argmax	O
i∈	O
[	O
k	O
]	O
hi	O
(	O
x	O
)	O
.	O
if	O
there	O
are	O
two	O
labels	O
that	O
maximize	O
hi	O
(	O
x	O
)	O
,	O
we	O
choose	O
the	O
smaller	O
one	O
.	O
also	O
,	O
let	O
hova	O
,	O
k	O
bin	O
=	O
{	O
t	O
(	O
¯h	O
)	O
:	O
¯h	O
∈	O
(	O
hbin	O
)	O
k	O
}	O
.	O
what	O
“	O
should	O
”	O
be	O
the	O
natarajan	O
dimension	B
of	O
hova	O
,	O
k	O
?	O
intuitively	O
,	O
to	O
specify	O
a	O
hypothesis	B
in	O
hbin	O
we	O
need	O
d	O
=	O
vcdim	O
(	O
hbin	O
)	O
parameters	O
.	O
to	O
specify	O
a	O
hypothe-	O
sis	O
in	O
hova	O
,	O
k	O
,	O
we	O
need	O
to	O
specify	O
k	O
hypotheses	O
in	O
hbin	O
.	O
therefore	O
,	O
kd	O
parameters	O
should	O
suﬃce	O
.	O
the	O
following	O
lemma	O
establishes	O
this	O
intuition	O
.	O
bin	O
bin	O
lemma	O
29.5	O
if	O
d	O
=	O
vcdim	O
(	O
hbin	O
)	O
then	O
ndim	O
(	O
hova	O
,	O
k	O
bin	O
)	O
≤	O
3kd	O
log	O
(	O
kd	O
)	O
.	O
proof	O
let	O
c	O
⊂	O
x	O
be	O
a	O
shattered	O
set	B
.	O
by	O
the	O
deﬁnition	O
of	O
shattering	B
(	O
for	O
mul-	O
ticlass	O
hypotheses	O
)	O
is	O
determined	O
by	O
using	O
k	O
hypothe-	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:16	O
)	O
hova	O
,	O
k	O
ses	O
from	O
hbin	O
.	O
therefore	O
,	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:16	O
)	O
hova	O
,	O
k	O
(	O
cid:17	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
≥	O
2|c|	O
.	O
(	O
cid:17	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
≤	O
|	O
(	O
hbin	O
)	O
c	O
|k	O
.	O
on	O
the	O
other	O
hand	O
,	O
each	O
hypothesis	B
in	O
hova	O
,	O
k	O
bin	O
bin	O
c	O
bin	O
c	O
29.3	O
calculating	O
the	O
natarajan	O
dimension	B
405	O
by	O
sauer	O
’	O
s	O
lemma	O
,	O
|	O
(	O
hbin	O
)	O
c	O
|	O
≤	O
|c|d	O
.	O
we	O
conclude	O
that	O
2|c|	O
≤	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:16	O
)	O
hova	O
,	O
k	O
bin	O
(	O
cid:17	O
)	O
c	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
≤	O
|c|dk	O
.	O
the	O
proof	O
follows	O
by	O
taking	O
the	O
logarithm	O
and	O
applying	O
lemma	O
a.1	O
.	O
how	O
tight	O
is	O
lemma	O
29.5	O
?	O
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
for	O
some	O
classes	O
,	O
ndim	O
(	O
hova	O
,	O
k	O
bin	O
can	O
be	O
much	O
smaller	O
than	O
dk	O
(	O
see	O
exercise	O
1	O
)	O
.	O
however	O
there	O
are	O
several	O
natural	O
binary	O
classes	O
,	O
hbin	O
(	O
e.g.	O
,	O
halfspaces	O
)	O
,	O
for	O
which	O
ndim	O
(	O
hova	O
,	O
k	O
)	O
=	O
ω	O
(	O
dk	O
)	O
(	O
see	O
exercise	O
6	O
)	O
.	O
bin	O
)	O
29.3.2	O
general	O
multiclass-to-binary	O
reductions	B
the	O
same	O
reasoning	O
used	O
to	O
establish	O
lemma	O
29.5	O
can	O
be	O
used	O
to	O
upper	O
bound	O
the	O
natarajan	O
dimension	B
of	O
more	O
general	O
multiclass-to-binary	O
reductions	B
.	O
these	O
reductions	B
train	O
several	O
binary	O
classiﬁers	O
on	O
the	O
data	O
.	O
then	O
,	O
given	O
a	O
new	O
in-	O
stance	O
,	O
they	O
predict	O
its	O
label	B
by	O
using	O
some	O
rule	O
that	O
takes	O
into	O
account	O
the	O
labels	O
predicted	O
by	O
the	O
binary	O
classiﬁers	O
.	O
these	O
reductions	B
include	O
one-versus-	O
all	O
and	O
all-pairs	B
.	O
suppose	O
that	O
such	O
a	O
method	O
trains	O
l	O
binary	O
classiﬁers	O
from	O
a	O
binary	O
class	O
hbin	O
,	O
and	O
r	O
:	O
{	O
0	O
,	O
1	O
}	O
l	O
→	O
[	O
k	O
]	O
is	O
the	O
rule	O
that	O
determines	O
the	O
(	O
multiclass	B
)	O
label	B
according	O
to	O
the	O
predictions	O
of	O
the	O
binary	O
classiﬁers	O
.	O
the	O
hypothesis	B
class	I
corresponding	O
to	O
this	O
method	O
can	O
be	O
deﬁned	O
as	O
follows	O
.	O
for	O
every	O
¯h	O
=	O
(	O
h1	O
,	O
.	O
.	O
.	O
,	O
hl	O
)	O
∈	O
(	O
hbin	O
)	O
l	O
deﬁne	O
r	O
(	O
¯h	O
)	O
:	O
x	O
→	O
[	O
k	O
]	O
by	O
finally	O
,	O
let	O
r	O
(	O
¯h	O
)	O
(	O
x	O
)	O
=	O
r	O
(	O
h1	O
(	O
x	O
)	O
,	O
.	O
.	O
.	O
,	O
hl	O
(	O
x	O
)	O
)	O
.	O
hr	O
bin	O
=	O
{	O
r	O
(	O
¯h	O
)	O
:	O
¯h	O
∈	O
(	O
hbin	O
)	O
l	O
}	O
.	O
similarly	O
to	O
lemma	O
29.5	O
it	O
can	O
be	O
proven	O
that	O
:	O
lemma	O
29.6	O
if	O
d	O
=	O
vcdim	O
(	O
hbin	O
)	O
then	O
ndim	O
(	O
hr	O
bin	O
)	O
≤	O
3	O
l	O
d	O
log	O
(	O
l	O
d	O
)	O
.	O
the	O
proof	O
is	O
left	O
as	O
exercise	O
2	O
.	O
29.3.3	O
linear	O
multiclass	O
predictors	O
next	O
,	O
we	O
consider	O
the	O
class	O
of	O
linear	O
multiclass	O
predictors	O
(	O
see	O
section	O
17.2	O
)	O
.	O
let	O
ψ	O
:	O
x	O
×	O
[	O
k	O
]	O
→	O
rd	O
be	O
some	O
class-sensitive	B
feature	I
mapping	I
and	O
let	O
(	O
cid:40	O
)	O
(	O
cid:41	O
)	O
hψ	O
=	O
x	O
(	O
cid:55	O
)	O
→	O
argmax	O
i∈	O
[	O
k	O
]	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
i	O
)	O
(	O
cid:105	O
)	O
:	O
w	O
∈	O
rd	O
.	O
(	O
29.1	O
)	O
each	O
hypothesis	B
in	O
hψ	O
is	O
determined	O
by	O
d	O
parameters	O
,	O
namely	O
,	O
a	O
vector	O
w	O
∈	O
rd	O
.	O
therefore	O
,	O
we	O
would	O
expect	O
that	O
the	O
natarajan	O
dimension	B
would	O
be	O
upper	O
bounded	O
by	O
d.	O
indeed	O
:	O
406	O
multiclass	B
learnability	O
theorem	O
29.7	O
ndim	O
(	O
hψ	O
)	O
≤	O
d	O
.	O
proof	O
let	O
c	O
⊂	O
x	O
be	O
a	O
shattered	O
set	B
,	O
and	O
let	O
f0	O
,	O
f1	O
:	O
c	O
→	O
[	O
k	O
]	O
be	O
the	O
two	O
functions	O
that	O
witness	O
the	O
shattering	B
.	O
we	O
need	O
to	O
show	O
that	O
|c|	O
≤	O
d.	O
for	O
every	O
x	O
∈	O
c	O
let	O
ρ	O
(	O
x	O
)	O
=	O
ψ	O
(	O
x	O
,	O
f0	O
(	O
x	O
)	O
)	O
−	O
ψ	O
(	O
x	O
,	O
f1	O
(	O
x	O
)	O
)	O
.	O
we	O
claim	O
that	O
the	O
set	B
ρ	O
(	O
c	O
)	O
def=	O
{	O
ρ	O
(	O
x	O
)	O
:	O
x	O
∈	O
c	O
}	O
consists	O
of	O
|c|	O
elements	O
(	O
i.e.	O
,	O
ρ	O
is	O
one	O
to	O
one	O
)	O
and	O
is	O
shattered	O
by	O
the	O
binary	O
hypothesis	B
class	I
of	O
homogeneous	O
linear	O
separators	O
on	O
rd	O
,	O
h	O
=	O
{	O
x	O
(	O
cid:55	O
)	O
→	O
sign	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
:	O
w	O
∈	O
rd	O
}	O
.	O
since	O
vcdim	O
(	O
h	O
)	O
=	O
d	O
,	O
it	O
will	O
follow	O
that	O
|c|	O
=	O
|ρ	O
(	O
c	O
)	O
|	O
≤	O
d	O
,	O
as	O
required	O
.	O
to	O
establish	O
our	O
claim	O
it	O
is	O
enough	O
to	O
show	O
that	O
|hρ	O
(	O
c	O
)	O
|	O
=	O
2|c|	O
.	O
indeed	O
,	O
given	O
a	O
subset	O
b	O
⊂	O
c	O
,	O
by	O
the	O
deﬁnition	O
of	O
shattering	B
,	O
there	O
exists	O
hb	O
∈	O
hψ	O
for	O
which	O
∀x	O
∈	O
b	O
,	O
hb	O
(	O
x	O
)	O
=	O
f0	O
(	O
x	O
)	O
and	O
∀x	O
∈	O
c	O
\	O
b	O
,	O
hb	O
(	O
x	O
)	O
=	O
f1	O
(	O
x	O
)	O
.	O
let	O
wb	O
∈	O
rd	O
be	O
a	O
vector	O
that	O
deﬁnes	O
hb	O
.	O
we	O
have	O
that	O
,	O
for	O
every	O
x	O
∈	O
b	O
,	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
f0	O
(	O
x	O
)	O
)	O
(	O
cid:105	O
)	O
>	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
f1	O
(	O
x	O
)	O
)	O
(	O
cid:105	O
)	O
⇒	O
(	O
cid:104	O
)	O
w	O
,	O
ρ	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
>	O
0.	O
similarly	O
,	O
for	O
every	O
x	O
∈	O
c	O
\	O
b	O
,	O
(	O
cid:104	O
)	O
w	O
,	O
ρ	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
<	O
0.	O
it	O
follows	O
that	O
the	O
hypothesis	B
gb	O
∈	O
h	O
deﬁned	O
by	O
the	O
same	O
w	O
∈	O
rd	O
label	B
the	O
points	O
in	O
ρ	O
(	O
b	O
)	O
by	O
1	O
and	O
the	O
points	O
in	O
ρ	O
(	O
c	O
\	O
b	O
)	O
by	O
0.	O
since	O
this	O
holds	O
for	O
every	O
b	O
⊆	O
c	O
we	O
obtain	O
that	O
|c|	O
=	O
|ρ	O
(	O
c	O
)	O
|	O
and	O
|hρ	O
(	O
c	O
)	O
|	O
=	O
2|c|	O
,	O
which	O
concludes	O
our	O
proof	O
.	O
the	O
theorem	O
is	O
tight	O
in	O
the	O
sense	O
that	O
there	O
are	O
mappings	O
ψ	O
for	O
which	O
ndim	O
(	O
hψ	O
)	O
=	O
ω	O
(	O
d	O
)	O
.	O
for	O
example	O
,	O
this	O
is	O
true	O
for	O
the	O
multivector	O
construction	O
(	O
see	O
section	O
17.2	O
and	O
the	O
bibliographic	O
remarks	O
at	O
the	O
end	O
of	O
this	O
chapter	O
)	O
.	O
we	O
therefore	O
con-	O
clude	O
:	O
corollary	O
29.8	O
let	O
x	O
=	O
rn	O
and	O
let	O
ψ	O
:	O
x	O
×	O
[	O
k	O
]	O
→	O
rnk	O
be	O
the	O
class	O
sensitive	O
feature	B
mapping	O
for	O
the	O
multi-vector	B
construction	O
:	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
ψ	O
(	O
x	O
,	O
y	O
)	O
=	O
[	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
∈r	O
(	O
y−1	O
)	O
n	O
(	O
cid:125	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
∈r	O
(	O
k−y	O
)	O
n	O
]	O
.	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
∈rn	O
,	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
let	O
hψ	O
be	O
as	O
deﬁned	O
in	O
equation	O
(	O
29.1	O
)	O
.	O
then	O
,	O
the	O
natarajan	O
dimension	B
of	O
hψ	O
satisﬁes	O
(	O
k	O
−	O
1	O
)	O
(	O
n	O
−	O
1	O
)	O
≤	O
ndim	O
(	O
hψ	O
)	O
≤	O
kn	O
.	O
29.4	O
on	O
good	O
and	O
bad	O
erms	O
in	O
this	O
section	O
we	O
present	O
an	O
example	O
of	O
a	O
hypothesis	B
class	I
with	O
the	O
property	O
that	O
not	O
all	O
erms	O
for	O
the	O
class	O
are	O
equally	O
successful	O
.	O
furthermore	O
,	O
if	O
we	O
allow	O
an	O
inﬁnite	O
number	O
of	O
labels	O
,	O
we	O
will	O
also	O
obtain	O
an	O
example	O
of	O
a	O
class	O
that	O
is	O
29.4	O
on	O
good	O
and	O
bad	O
erms	O
407	O
learnable	O
by	O
some	O
erm	O
,	O
but	O
other	O
erms	O
will	O
fail	O
to	O
learn	O
it	O
.	O
clearly	O
,	O
this	O
also	O
implies	O
that	O
the	O
class	O
is	O
learnable	O
but	O
it	O
does	O
not	O
have	O
the	O
uniform	B
convergence	I
property	O
.	O
for	O
simplicity	O
,	O
we	O
consider	O
only	O
the	O
realizable	O
case	O
.	O
the	O
class	O
we	O
consider	O
is	O
deﬁned	O
as	O
follows	O
.	O
the	O
instance	B
space	I
x	O
will	O
be	O
any	O
ﬁnite	O
or	O
countable	O
set	B
.	O
let	O
pf	O
(	O
x	O
)	O
be	O
the	O
collection	O
of	O
all	O
ﬁnite	O
and	O
coﬁnite	O
subsets	O
of	O
x	O
(	O
that	O
is	O
,	O
for	O
each	O
a	O
∈	O
pf	O
(	O
x	O
)	O
,	O
either	O
a	O
or	O
x	O
\	O
a	O
must	O
be	O
ﬁnite	O
)	O
.	O
instead	O
of	O
[	O
k	O
]	O
,	O
the	O
label	B
set	O
is	O
y	O
=	O
pf	O
(	O
x	O
)	O
∪	O
{	O
∗	O
}	O
,	O
where	O
∗	O
is	O
some	O
special	O
label	B
.	O
for	O
every	O
a	O
∈	O
pf	O
(	O
x	O
)	O
deﬁne	O
ha	O
:	O
x	O
→	O
y	O
by	O
(	O
cid:40	O
)	O
ha	O
(	O
x	O
)	O
=	O
a	O
x	O
∈	O
a	O
∗	O
x	O
/∈	O
a	O
finally	O
,	O
the	O
hypothesis	B
class	I
we	O
take	O
is	O
h	O
=	O
{	O
ha	O
:	O
a	O
∈	O
pf	O
(	O
x	O
)	O
}	O
.	O
let	O
a	O
be	O
some	O
erm	O
algorithm	O
for	O
h.	O
assume	O
that	O
a	O
operates	O
on	O
a	O
sample	O
labeled	O
by	O
ha	O
∈	O
h.	O
since	O
ha	O
is	O
the	O
only	O
hypothesis	B
in	O
h	O
that	O
might	O
return	O
the	O
label	B
a	O
,	O
if	O
a	O
observes	O
the	O
label	B
a	O
,	O
it	O
“	O
knows	O
”	O
that	O
the	O
learned	O
hypothesis	B
is	O
ha	O
,	O
and	O
,	O
as	O
an	O
erm	O
,	O
must	O
return	O
it	O
(	O
note	O
that	O
in	O
this	O
case	O
the	O
error	O
of	O
the	O
returned	O
hypothesis	B
is	O
0	O
)	O
.	O
therefore	O
,	O
to	O
specify	O
an	O
erm	O
,	O
we	O
should	O
only	O
specify	O
the	O
hypothesis	B
it	O
returns	O
upon	O
receiving	O
a	O
sample	O
of	O
the	O
form	O
s	O
=	O
{	O
(	O
x1	O
,	O
∗	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
∗	O
)	O
}	O
.	O
we	O
consider	O
two	O
erms	O
:	O
the	O
ﬁrst	O
,	O
agood	O
,	O
is	O
deﬁned	O
by	O
agood	O
(	O
s	O
)	O
=	O
h∅	O
;	O
that	O
is	O
,	O
it	O
outputs	O
the	O
hypothesis	B
which	O
predicts	O
‘	O
*	O
’	O
for	O
every	O
x	O
∈	O
x	O
.	O
the	O
second	O
erm	O
,	O
abad	O
,	O
is	O
deﬁned	O
by	O
abad	O
(	O
s	O
)	O
=	O
h	O
{	O
x1	O
,	O
...	O
xm	O
}	O
c.	O
the	O
following	O
claim	O
shows	O
that	O
the	O
sample	B
complexity	I
of	O
abad	O
is	O
about	O
|x|-times	O
larger	O
than	O
the	O
sample	B
complexity	I
of	O
agood	O
.	O
this	O
establishes	O
a	O
gap	O
between	O
diﬀerent	O
erms	O
.	O
if	O
x	O
is	O
inﬁnite	O
,	O
we	O
even	O
obtain	O
a	O
learnable	O
class	O
that	O
is	O
not	O
learnable	O
by	O
every	O
erm	O
.	O
	O
log	O
(	O
cid:0	O
)	O
1	O
δ	O
(	O
cid:1	O
)	O
examples	O
,	O
sampled	O
according	O
to	O
d	O
and	O
labeled	O
by	O
claim	O
29.9	O
1.	O
let	O
	O
,	O
δ	O
>	O
0	O
,	O
d	O
a	O
distribution	O
over	O
x	O
and	O
ha	O
∈	O
h.	O
let	O
s	O
be	O
an	O
i.i.d	O
.	O
sample	O
consisting	O
of	O
m	O
≥	O
1	O
ha	O
.	O
then	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
,	O
the	O
hypothesis	B
returned	O
by	O
agood	O
will	O
have	O
an	O
error	O
of	O
at	O
most	O
	O
.	O
2.	O
there	O
exists	O
a	O
constant	O
a	O
>	O
0	O
such	O
that	O
for	O
every	O
0	O
<	O
	O
<	O
a	O
there	O
exists	O
a	O
distribution	O
d	O
over	O
x	O
and	O
ha	O
∈	O
h	O
such	O
that	O
the	O
following	O
holds	O
.	O
the	O
hypoth-	O
esis	O
returned	O
by	O
abad	O
upon	O
receiving	O
a	O
sample	O
of	O
size	O
m	O
≤	O
|x|−1	O
,	O
sampled	O
according	O
to	O
d	O
and	O
labeled	O
by	O
ha	O
,	O
will	O
have	O
error	O
≥	O
	O
with	O
probability	O
≥	O
e−	O
1	O
6	O
.	O
6	O
408	O
multiclass	B
learnability	O
proof	O
let	O
d	O
be	O
a	O
distribution	O
over	O
x	O
and	O
suppose	O
that	O
the	O
correct	O
labeling	O
is	O
ha	O
.	O
for	O
any	O
sample	O
,	O
agood	O
returns	O
either	O
h∅	O
or	O
ha	O
.	O
if	O
it	O
returns	O
ha	O
then	O
its	O
true	B
error	I
is	O
zero	O
.	O
thus	O
,	O
it	O
returns	O
a	O
hypothesis	B
with	O
error	O
≥	O
	O
only	O
if	O
all	O
the	O
m	O
examples	O
in	O
the	O
sample	O
are	O
from	O
x	O
\	O
a	O
while	O
the	O
error	O
of	O
h∅	O
,	O
ld	O
(	O
h∅	O
)	O
=	O
pd	O
[	O
a	O
]	O
,	O
is	O
≥	O
	O
.	O
assume	O
m	O
≥	O
1	O
δ	O
)	O
;	O
then	O
the	O
probability	O
of	O
the	O
latter	O
event	O
is	O
no	O
more	O
than	O
(	O
1	O
−	O
	O
)	O
m	O
≤	O
e−m	O
≤	O
δ.	O
this	O
establishes	O
item	O
1.	O
next	O
we	O
prove	O
item	O
2.	O
we	O
restrict	O
the	O
proof	O
to	O
the	O
case	O
that	O
|x|	O
=	O
d	O
<	O
∞	O
.	O
	O
log	O
(	O
1	O
the	O
proof	O
for	O
inﬁnite	O
x	O
is	O
similar	O
.	O
suppose	O
that	O
x	O
=	O
{	O
x0	O
,	O
.	O
.	O
.	O
,	O
xd−1	O
}	O
.	O
let	O
a	O
>	O
0	O
be	O
small	O
enough	O
such	O
that	O
1	O
−	O
2	O
≥	O
e−4	O
for	O
every	O
	O
<	O
a	O
and	O
ﬁx	O
some	O
	O
<	O
a.	O
deﬁne	O
a	O
distribution	O
on	O
x	O
by	O
setting	O
p	O
[	O
x0	O
]	O
=	O
1	O
−	O
2	O
and	O
for	O
all	O
1	O
≤	O
i	O
≤	O
d−	O
1	O
,	O
p	O
[	O
xi	O
]	O
=	O
2	O
d−1	O
.	O
suppose	O
that	O
the	O
correct	O
hypothesis	B
is	O
h∅	O
and	O
let	O
the	O
sample	O
size	O
be	O
m.	O
clearly	O
,	O
the	O
hypothesis	B
returned	O
by	O
abad	O
will	O
err	O
on	O
all	O
the	O
examples	O
from	O
x	O
which	O
are	O
not	O
in	O
the	O
sample	O
.	O
by	O
chernoﬀ	O
’	O
s	O
bound	O
,	O
if	O
m	O
≤	O
d−1	O
6	O
,	O
then	O
with	O
probability	O
≥	O
e−	O
1	O
examples	O
from	O
x	O
.	O
thus	O
the	O
returned	O
hypothesis	B
will	O
have	O
error	O
≥	O
	O
.	O
6	O
,	O
the	O
sample	O
will	O
include	O
no	O
more	O
than	O
d−1	O
2	O
the	O
conclusion	O
of	O
the	O
example	O
presented	O
is	O
that	O
in	O
multiclass	B
classiﬁcation	O
,	O
the	O
sample	B
complexity	I
of	O
diﬀerent	O
erms	O
may	O
diﬀer	O
.	O
are	O
there	O
“	O
good	O
”	O
erms	O
for	O
every	O
hypothesis	B
class	I
?	O
the	O
following	O
conjecture	O
asserts	O
that	O
the	O
answer	O
is	O
yes	O
.	O
conjecture	O
29.10	O
the	O
realizable	O
sample	B
complexity	I
of	O
every	O
hypothesis	B
class	I
h	O
⊂	O
[	O
k	O
]	O
is	O
x	O
(	O
cid:18	O
)	O
ndim	O
(	O
h	O
)	O
(	O
cid:19	O
)	O
mh	O
(	O
	O
,	O
δ	O
)	O
=	O
˜o	O
.	O
	O
we	O
emphasize	O
that	O
the	O
˜o	O
notation	O
may	O
hide	O
only	O
poly-log	O
factors	O
of	O
	O
,	O
δ	O
,	O
and	O
ndim	O
(	O
h	O
)	O
,	O
but	O
no	O
factor	O
of	O
k.	O
29.5	O
bibliographic	O
remarks	O
the	O
natarajan	O
dimension	B
is	O
due	O
to	O
natarajan	O
(	O
1989	O
)	O
.	O
that	O
paper	O
also	O
established	O
the	O
natarajan	O
lemma	O
and	O
the	O
generalization	O
of	O
the	O
fundamental	O
theorem	O
.	O
gen-	O
eralizations	O
and	O
sharper	O
versions	O
of	O
the	O
natarajan	O
lemma	O
are	O
studied	O
in	O
haussler	O
&	O
long	O
(	O
1995	O
)	O
.	O
ben-david	O
,	O
cesa-bianchi	O
,	O
haussler	O
&	O
long	O
(	O
1995	O
)	O
deﬁned	O
a	O
large	O
family	O
of	O
notions	O
of	O
dimensions	O
,	O
all	O
of	O
which	O
generalize	O
the	O
vc	O
dimension	B
and	O
may	O
be	O
used	O
to	O
estimate	O
the	O
sample	B
complexity	I
of	O
multiclass	B
classiﬁcation	O
.	O
the	O
calculation	O
of	O
the	O
natarajan	O
dimension	B
,	O
presented	O
here	O
,	O
together	O
with	O
calculation	O
of	O
other	O
classes	O
,	O
can	O
be	O
found	O
in	O
daniely	O
et	O
al	O
.	O
(	O
2012	O
)	O
.	O
the	O
example	O
of	O
good	O
and	O
bad	O
erms	O
,	O
as	O
well	O
as	O
conjecture	O
29.10	O
,	O
are	O
from	O
daniely	O
et	O
al	O
.	O
(	O
2011	O
)	O
.	O
29.6	O
exercises	O
409	O
29.6	O
exercises	O
1.	O
let	O
d	O
,	O
k	O
>	O
0.	O
show	O
that	O
there	O
exists	O
a	O
binary	O
hypothesis	B
hbin	O
of	O
vc	O
dimension	B
d	O
such	O
that	O
ndim	O
(	O
hova	O
,	O
k	O
)	O
=	O
d.	O
bin	O
2.	O
prove	O
lemma	O
29.6	O
.	O
3.	O
prove	O
natarajan	O
’	O
s	O
lemma	O
.	O
hint	O
:	O
fix	O
some	O
x0	O
∈	O
x	O
.	O
for	O
i	O
,	O
j	O
∈	O
[	O
k	O
]	O
,	O
denote	O
by	O
hij	O
all	O
the	O
functions	O
f	O
:	O
x	O
\	O
{	O
x0	O
}	O
→	O
[	O
k	O
]	O
that	O
can	O
be	O
extended	O
to	O
a	O
function	B
in	O
h	O
both	O
by	O
deﬁning	O
i	O
(	O
cid:54	O
)	O
=j	O
|hij|	O
f	O
(	O
x0	O
)	O
=	O
i	O
and	O
by	O
deﬁning	O
f	O
(	O
x0	O
)	O
=	O
j.	O
show	O
that	O
|h|	O
≤	O
|hx\	O
{	O
x0	O
}	O
|	O
+	O
(	O
cid:80	O
)	O
and	O
use	O
induction	O
.	O
4.	O
adapt	O
the	O
proof	O
of	O
the	O
binary	O
fundamental	O
theorem	O
and	O
natarajan	O
’	O
s	O
lemma	O
to	O
prove	O
that	O
,	O
for	O
some	O
universal	O
constant	O
c	O
>	O
0	O
and	O
for	O
every	O
hypothesis	B
class	I
of	O
natarajan	O
dimension	B
d	O
,	O
the	O
agnostic	O
sample	O
complexity	O
of	O
h	O
is	O
d	O
log	O
(	O
cid:0	O
)	O
kd	O
(	O
cid:1	O
)	O
+	O
log	O
(	O
1/δ	O
)	O
	O
.	O
mh	O
(	O
	O
,	O
δ	O
)	O
≤	O
c	O
2	O
5.	O
prove	O
that	O
,	O
for	O
some	O
universal	O
constant	O
c	O
>	O
0	O
and	O
for	O
every	O
hypothesis	B
class	I
of	O
natarajan	O
dimension	B
d	O
,	O
the	O
agnostic	O
sample	O
complexity	O
of	O
h	O
is	O
mh	O
(	O
	O
,	O
δ	O
)	O
≥	O
c	O
d	O
+	O
log	O
(	O
1/δ	O
)	O
2	O
.	O
hint	O
:	O
deduce	O
it	O
from	O
the	O
binary	O
fundamental	O
theorem	O
.	O
6.	O
let	O
h	O
be	O
the	O
binary	O
hypothesis	B
class	I
of	O
(	O
nonhomogenous	O
)	O
halfspaces	O
in	O
rd	O
.	O
the	O
goal	O
of	O
this	O
exercise	O
is	O
to	O
prove	O
that	O
ndim	O
(	O
hova	O
,	O
k	O
)	O
≥	O
(	O
d	O
−	O
1	O
)	O
·	O
(	O
k	O
−	O
1	O
)	O
.	O
1.	O
let	O
hdiscrete	O
be	O
the	O
class	O
of	O
all	O
functions	O
f	O
:	O
[	O
k	O
−	O
1	O
]	O
×	O
[	O
d	O
−	O
1	O
]	O
→	O
{	O
0	O
,	O
1	O
}	O
for	O
which	O
there	O
exists	O
some	O
i0	O
such	O
that	O
,	O
for	O
every	O
j	O
∈	O
[	O
d	O
−	O
1	O
]	O
∀i	O
<	O
i0	O
,	O
f	O
(	O
i	O
,	O
j	O
)	O
=	O
1	O
while	O
∀i	O
>	O
i0	O
,	O
f	O
(	O
i	O
,	O
j	O
)	O
=	O
0	O
.	O
2.	O
show	O
that	O
hdiscrete	O
can	O
be	O
realized	O
by	O
h.	O
that	O
is	O
,	O
show	O
that	O
there	O
exists	O
show	O
that	O
ndim	O
(	O
hova	O
,	O
k	O
discrete	O
)	O
=	O
(	O
d	O
−	O
1	O
)	O
·	O
(	O
k	O
−	O
1	O
)	O
.	O
a	O
mapping	O
ψ	O
:	O
[	O
k	O
−	O
1	O
]	O
×	O
[	O
d	O
−	O
1	O
]	O
→	O
rd	O
such	O
that	O
hdiscrete	O
⊂	O
{	O
h	O
◦	O
ψ	O
:	O
h	O
∈	O
h	O
}	O
.	O
hint	O
:	O
you	O
can	O
take	O
ψ	O
(	O
i	O
,	O
j	O
)	O
to	O
be	O
the	O
vector	O
whose	O
jth	O
coordinate	O
is	O
1	O
,	O
whose	O
last	O
coordinate	O
is	O
i	O
and	O
the	O
rest	O
are	O
zeros	O
.	O
3.	O
conclude	O
that	O
ndim	O
(	O
hova	O
,	O
k	O
)	O
≥	O
(	O
d	O
−	O
1	O
)	O
·	O
(	O
k	O
−	O
1	O
)	O
.	O
30	O
compression	B
bounds	I
throughout	O
the	O
book	O
,	O
we	O
have	O
tried	O
to	O
characterize	O
the	O
notion	O
of	O
learnability	O
using	O
diﬀerent	O
approaches	O
.	O
at	O
ﬁrst	O
we	O
have	O
shown	O
that	O
the	O
uniform	O
conver-	O
gence	O
property	O
of	O
a	O
hypothesis	B
class	I
guarantees	O
successful	O
learning	O
.	O
later	O
on	O
we	O
introduced	O
the	O
notion	O
of	O
stability	B
and	O
have	O
shown	O
that	O
stable	O
algorithms	O
are	O
guaranteed	O
to	O
be	O
good	O
learners	O
.	O
yet	O
there	O
are	O
other	O
properties	O
which	O
may	O
be	O
suﬃcient	O
for	O
learning	O
,	O
and	O
in	O
this	O
chapter	O
and	O
its	O
sequel	O
we	O
will	O
introduce	O
two	O
approaches	O
to	O
this	O
issue	O
:	O
compression	B
bounds	I
and	O
the	O
pac-bayes	O
approach	O
.	O
in	O
this	O
chapter	O
we	O
study	O
compression	B
bounds	I
.	O
roughly	O
speaking	O
,	O
we	O
shall	O
see	O
that	O
if	O
a	O
learning	O
algorithm	O
can	O
express	O
the	O
output	O
hypothesis	B
using	O
a	O
small	O
sub-	O
set	B
of	O
the	O
training	B
set	I
,	O
then	O
the	O
error	O
of	O
the	O
hypothesis	B
on	O
the	O
rest	O
of	O
the	O
examples	O
estimates	O
its	O
true	B
error	I
.	O
in	O
other	O
words	O
,	O
an	O
algorithm	O
that	O
can	O
“	O
compress	O
”	O
its	O
output	O
is	O
a	O
good	O
learner	O
.	O
30.1	O
compression	B
bounds	I
to	O
motivate	O
the	O
results	O
,	O
let	O
us	O
ﬁrst	O
consider	O
the	O
following	O
learning	O
protocol	O
.	O
first	O
,	O
we	O
sample	O
a	O
sequence	O
of	O
k	O
examples	O
denoted	O
t	O
.	O
on	O
the	O
basis	O
of	O
these	O
examples	O
,	O
we	O
construct	O
a	O
hypothesis	B
denoted	O
ht	O
.	O
now	O
we	O
would	O
like	O
to	O
estimate	O
the	O
performance	O
of	O
ht	O
so	O
we	O
sample	O
a	O
fresh	O
sequence	O
of	O
m−	O
k	O
examples	O
,	O
denoted	O
v	O
,	O
and	O
calculate	O
the	O
error	O
of	O
ht	O
on	O
v	O
.	O
since	O
v	O
and	O
t	O
are	O
independent	O
,	O
we	O
immediately	O
get	O
the	O
following	O
from	O
bernstein	O
’	O
s	O
inequality	O
(	O
see	O
lemma	O
b.10	O
)	O
.	O
lemma	O
30.1	O
assume	O
that	O
the	O
range	O
of	O
the	O
loss	B
function	I
is	O
[	O
0	O
,	O
1	O
]	O
.	O
then	O
,	O
(	O
cid:34	O
)	O
(	O
cid:115	O
)	O
(	O
cid:35	O
)	O
p	O
ld	O
(	O
ht	O
)	O
−	O
lv	O
(	O
ht	O
)	O
≥	O
2lv	O
(	O
ht	O
)	O
log	O
(	O
1/δ	O
)	O
|v	O
|	O
+	O
4	O
log	O
(	O
1/δ	O
)	O
|v	O
|	O
≤	O
δ.	O
to	O
derive	O
this	O
bound	O
,	O
all	O
we	O
needed	O
was	O
independence	O
between	O
t	O
and	O
v	O
.	O
therefore	O
,	O
we	O
can	O
redeﬁne	O
the	O
protocol	O
as	O
follows	O
.	O
first	O
,	O
we	O
agree	O
on	O
a	O
sequence	O
of	O
k	O
indices	O
i	O
=	O
(	O
i1	O
,	O
.	O
.	O
.	O
,	O
ik	O
)	O
∈	O
[	O
m	O
]	O
k.	O
then	O
,	O
we	O
sample	O
a	O
sequence	O
of	O
m	O
examples	O
s	O
=	O
(	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
)	O
.	O
now	O
,	O
deﬁne	O
t	O
=	O
si	O
=	O
(	O
zi1	O
,	O
.	O
.	O
.	O
,	O
zik	O
)	O
and	O
deﬁne	O
v	O
to	O
be	O
the	O
rest	O
of	O
the	O
examples	O
in	O
s.	O
note	O
that	O
this	O
protocol	O
is	O
equivalent	O
to	O
the	O
protocol	O
we	O
deﬁned	O
before	O
–	O
hence	O
lemma	O
30.1	O
still	O
holds	O
.	O
applying	O
a	O
union	B
bound	I
over	O
the	O
choice	O
of	O
the	O
sequence	O
of	O
indices	O
we	O
obtain	O
the	O
following	O
theorem	O
.	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
30.1	O
compression	B
bounds	I
411	O
theorem	O
30.2	O
let	O
k	O
be	O
an	O
integer	O
and	O
let	O
b	O
:	O
z	O
k	O
→	O
h	O
be	O
a	O
mapping	O
from	O
sequences	O
of	O
k	O
examples	O
to	O
the	O
hypothesis	B
class	I
.	O
let	O
m	O
≥	O
2k	O
be	O
a	O
training	B
set	I
size	O
and	O
let	O
a	O
:	O
z	O
m	O
→	O
h	O
be	O
a	O
learning	O
rule	O
that	O
receives	O
a	O
training	O
sequence	O
s	O
of	O
size	O
m	O
and	O
returns	O
a	O
hypothesis	B
such	O
that	O
a	O
(	O
s	O
)	O
=	O
b	O
(	O
zi1	O
,	O
.	O
.	O
.	O
,	O
zik	O
)	O
for	O
some	O
(	O
i1	O
,	O
.	O
.	O
.	O
,	O
ik	O
)	O
∈	O
[	O
m	O
]	O
k.	O
let	O
v	O
=	O
{	O
zj	O
:	O
j	O
/∈	O
(	O
i1	O
,	O
.	O
.	O
.	O
,	O
ik	O
)	O
}	O
be	O
the	O
set	B
of	O
examples	O
which	O
were	O
not	O
selected	O
for	O
deﬁning	O
a	O
(	O
s	O
)	O
.	O
then	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
over	O
the	O
choice	O
of	O
s	O
we	O
have	O
(	O
cid:114	O
)	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
lv	O
(	O
a	O
(	O
s	O
)	O
)	O
+	O
lv	O
(	O
a	O
(	O
s	O
)	O
)	O
4k	O
log	O
(	O
m/δ	O
)	O
m	O
+	O
8k	O
log	O
(	O
m/δ	O
)	O
m	O
.	O
(	O
cid:34	O
)	O
p	O
proof	O
for	O
any	O
i	O
∈	O
[	O
m	O
]	O
k	O
let	O
hi	O
=	O
b	O
(	O
zi1	O
,	O
.	O
.	O
.	O
,	O
zik	O
)	O
.	O
let	O
n	O
=	O
m	O
−	O
k.	O
combining	O
(	O
cid:35	O
)	O
lemma	O
30.1	O
with	O
the	O
union	B
bound	I
we	O
have	O
(	O
cid:35	O
)	O
∃i	O
∈	O
[	O
m	O
]	O
k	O
s.t	O
.	O
ld	O
(	O
hi	O
)	O
−	O
lv	O
(	O
hi	O
)	O
≥	O
(	O
cid:114	O
)	O
(	O
cid:114	O
)	O
2lv	O
(	O
hi	O
)	O
log	O
(	O
1/δ	O
)	O
4	O
log	O
(	O
1/δ	O
)	O
(	O
cid:34	O
)	O
+	O
p	O
ld	O
(	O
hi	O
)	O
−	O
lv	O
(	O
hi	O
)	O
≥	O
2lv	O
(	O
hi	O
)	O
log	O
(	O
1/δ	O
)	O
4	O
log	O
(	O
1/δ	O
)	O
+	O
n	O
n	O
n	O
n	O
≤	O
(	O
cid:88	O
)	O
i∈	O
[	O
m	O
]	O
k	O
≤	O
mkδ	O
.	O
denote	O
δ	O
(	O
cid:48	O
)	O
=	O
mkδ	O
.	O
using	O
the	O
assumption	O
k	O
≤	O
m/2	O
,	O
which	O
implies	O
that	O
n	O
=	O
m	O
−	O
k	O
≥	O
m/2	O
,	O
the	O
above	O
implies	O
that	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
(	O
cid:48	O
)	O
we	O
have	O
that	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
lv	O
(	O
a	O
(	O
s	O
)	O
)	O
+	O
lv	O
(	O
a	O
(	O
s	O
)	O
)	O
4k	O
log	O
(	O
m/δ	O
(	O
cid:48	O
)	O
)	O
m	O
+	O
8k	O
log	O
(	O
m/δ	O
(	O
cid:48	O
)	O
)	O
m	O
,	O
(	O
cid:114	O
)	O
which	O
concludes	O
our	O
proof	O
.	O
as	O
a	O
direct	O
corollary	O
we	O
obtain	O
:	O
corollary	O
30.3	O
assuming	O
the	O
conditions	O
of	O
theorem	O
30.2	O
,	O
and	O
further	O
as-	O
suming	O
that	O
lv	O
(	O
a	O
(	O
s	O
)	O
)	O
=	O
0	O
,	O
then	O
,	O
with	O
probability	O
of	O
at	O
least	O
1−	O
δ	O
over	O
the	O
choice	O
of	O
s	O
we	O
have	O
ld	O
(	O
a	O
(	O
s	O
)	O
)	O
≤	O
8k	O
log	O
(	O
m/δ	O
)	O
m	O
.	O
these	O
results	O
motivate	O
the	O
following	O
deﬁnition	O
:	O
(	O
compression	B
scheme	I
)	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
of	O
definition	O
30.4	O
functions	O
from	O
x	O
to	O
y	O
and	O
let	O
k	O
be	O
an	O
integer	O
.	O
we	O
say	O
that	O
h	O
has	O
a	O
compression	B
scheme	I
of	O
size	O
k	O
if	O
the	O
following	O
holds	O
:	O
for	O
all	O
m	O
there	O
exists	O
a	O
:	O
z	O
m	O
→	O
[	O
m	O
]	O
k	O
and	O
b	O
:	O
z	O
k	O
→	O
h	O
such	O
that	O
for	O
all	O
h	O
∈	O
h	O
,	O
if	O
we	O
feed	O
any	O
training	B
set	I
of	O
the	O
form	O
(	O
x1	O
,	O
h	O
(	O
x1	O
)	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
h	O
(	O
xm	O
)	O
)	O
into	O
a	O
and	O
then	O
feed	O
(	O
xi1	O
,	O
h	O
(	O
xi1	O
)	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xik	O
,	O
h	O
(	O
xik	O
)	O
)	O
into	O
b	O
,	O
where	O
(	O
i1	O
,	O
.	O
.	O
.	O
,	O
ik	O
)	O
is	O
the	O
output	O
of	O
a	O
,	O
then	O
the	O
output	O
of	O
b	O
,	O
denoted	O
h	O
(	O
cid:48	O
)	O
,	O
satisﬁes	O
ls	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
=	O
0.	O
it	O
is	O
possible	O
to	O
generalize	O
the	O
deﬁnition	O
for	O
unrealizable	O
sequences	O
as	O
follows	O
.	O
412	O
compression	B
bounds	I
definition	O
30.5	O
(	O
compression	B
scheme	I
for	O
unrealizable	O
sequences	O
)	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
of	O
functions	O
from	O
x	O
to	O
y	O
and	O
let	O
k	O
be	O
an	O
integer	O
.	O
we	O
say	O
that	O
h	O
has	O
a	O
compression	B
scheme	I
of	O
size	O
k	O
if	O
the	O
following	O
holds	O
:	O
for	O
all	O
m	O
there	O
exists	O
a	O
:	O
z	O
m	O
→	O
[	O
m	O
]	O
k	O
and	O
b	O
:	O
z	O
k	O
→	O
h	O
such	O
that	O
for	O
all	O
h	O
∈	O
h	O
,	O
if	O
we	O
feed	O
any	O
training	B
set	I
of	O
the	O
form	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xm	O
,	O
ym	O
)	O
into	O
a	O
and	O
then	O
feed	O
(	O
xi1	O
,	O
yi1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xik	O
,	O
yik	O
)	O
into	O
b	O
,	O
where	O
(	O
i1	O
,	O
.	O
.	O
.	O
,	O
ik	O
)	O
is	O
the	O
output	O
of	O
a	O
,	O
then	O
the	O
output	O
of	O
b	O
,	O
denoted	O
h	O
(	O
cid:48	O
)	O
,	O
satisﬁes	O
ls	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
≤	O
ls	O
(	O
h	O
)	O
.	O
the	O
following	O
lemma	O
shows	O
that	O
the	O
existence	O
of	O
a	O
compression	B
scheme	I
for	O
the	O
realizable	O
case	O
also	O
implies	O
the	O
existence	O
of	O
a	O
compression	B
scheme	I
for	O
the	O
unrealizable	O
case	O
.	O
lemma	O
30.6	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
for	O
binary	O
classiﬁcation	O
,	O
and	O
assume	O
it	O
has	O
a	O
compression	B
scheme	I
of	O
size	O
k	O
in	O
the	O
realizable	O
case	O
.	O
then	O
,	O
it	O
has	O
a	O
compression	B
scheme	I
of	O
size	O
k	O
for	O
the	O
unrealizable	O
case	O
as	O
well	O
.	O
proof	O
consider	O
the	O
following	O
scheme	O
:	O
first	O
,	O
ﬁnd	O
an	O
erm	O
hypothesis	B
and	O
denote	O
it	O
by	O
h.	O
then	O
,	O
discard	O
all	O
the	O
examples	O
on	O
which	O
h	O
errs	O
.	O
now	O
,	O
apply	O
the	O
realizable	O
compression	B
scheme	I
on	O
the	O
examples	O
that	O
have	O
not	O
been	O
removed	O
.	O
the	O
output	O
of	O
the	O
realizable	O
compression	B
scheme	I
,	O
denoted	O
h	O
(	O
cid:48	O
)	O
,	O
must	O
be	O
correct	O
on	O
the	O
examples	O
that	O
have	O
not	O
been	O
removed	O
.	O
since	O
h	O
errs	O
on	O
the	O
removed	O
examples	O
it	O
follows	O
that	O
the	O
error	O
of	O
h	O
(	O
cid:48	O
)	O
can	O
not	O
be	O
larger	O
than	O
the	O
error	O
of	O
h	O
;	O
hence	O
h	O
(	O
cid:48	O
)	O
is	O
also	O
an	O
erm	O
hypothesis	B
.	O
30.2	O
examples	O
in	O
the	O
examples	O
that	O
follows	O
,	O
we	O
present	O
compression	O
schemes	O
for	O
several	O
hy-	O
pothesis	O
classes	O
for	O
binary	O
classiﬁcation	O
.	O
in	O
light	O
of	O
lemma	O
30.6	O
we	O
focus	O
on	O
the	O
realizable	O
case	O
.	O
therefore	O
,	O
to	O
show	O
that	O
a	O
certain	O
hypothesis	B
class	I
has	O
a	O
com-	O
pression	O
scheme	O
,	O
it	O
is	O
necessary	O
to	O
show	O
that	O
there	O
exist	O
a	O
,	O
b	O
,	O
and	O
k	O
for	O
which	O
ls	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
=	O
0	O
.	O
30.2.1	O
axis	O
aligned	O
rectangles	O
note	O
that	O
this	O
is	O
an	O
uncountable	O
inﬁnite	O
class	O
.	O
we	O
show	O
that	O
there	O
is	O
a	O
simple	O
compression	B
scheme	I
.	O
consider	O
the	O
algorithm	O
a	O
that	O
works	O
as	O
follows	O
:	O
for	O
each	O
dimension	B
,	O
choose	O
the	O
two	O
positive	O
examples	O
with	O
extremal	O
values	O
at	O
this	O
dimen-	O
sion	O
.	O
deﬁne	O
b	O
to	O
be	O
the	O
function	B
that	O
returns	O
the	O
minimal	O
enclosing	O
rectangle	O
.	O
then	O
,	O
for	O
k	O
=	O
2d	O
,	O
we	O
have	O
that	O
in	O
the	O
realizable	O
case	O
,	O
ls	O
(	O
b	O
(	O
a	O
(	O
s	O
)	O
)	O
)	O
=	O
0	O
.	O
30.2.2	O
halfspaces	O
let	O
x	O
=	O
rd	O
and	O
consider	O
the	O
class	O
of	O
homogenous	B
halfspaces	O
,	O
{	O
x	O
(	O
cid:55	O
)	O
→	O
sign	O
(	O
(	O
cid:104	O
)	O
w	O
,	O
x	O
(	O
cid:105	O
)	O
)	O
:	O
w	O
∈	O
rd	O
}	O
.	O
30.2	O
examples	O
413	O
a	O
compression	B
scheme	I
:	O
w.l.o.g	O
.	O
assume	O
all	O
labels	O
are	O
positive	O
(	O
otherwise	O
,	O
replace	O
xi	O
by	O
yixi	O
)	O
.	O
the	O
com-	O
pression	O
scheme	O
we	O
propose	O
is	O
as	O
follows	O
.	O
first	O
,	O
a	O
ﬁnds	O
the	O
vector	O
w	O
which	O
is	O
in	O
the	O
convex	B
hull	O
of	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
}	O
and	O
has	O
minimal	O
norm	O
.	O
then	O
,	O
it	O
represents	O
it	O
as	O
a	O
convex	B
combination	O
of	O
d	O
points	O
in	O
the	O
sample	O
(	O
it	O
will	O
be	O
shown	O
later	O
that	O
this	O
is	O
always	O
possible	O
)	O
.	O
the	O
output	O
of	O
a	O
are	O
these	O
d	O
points	O
.	O
the	O
algorithm	O
b	O
receives	O
these	O
d	O
points	O
and	O
set	B
w	O
to	O
be	O
the	O
point	O
in	O
their	O
convex	B
hull	O
of	O
minimal	O
norm	O
.	O
next	O
we	O
prove	O
that	O
this	O
indeed	O
is	O
a	O
compression	O
sceme	O
.	O
since	O
the	O
data	O
is	O
linearly	O
separable	B
,	O
the	O
convex	B
hull	O
of	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
}	O
does	O
not	O
contain	O
the	O
origin	O
.	O
consider	O
the	O
point	O
w	O
in	O
this	O
convex	B
hull	O
closest	O
to	O
the	O
origin	O
.	O
(	O
this	O
is	O
a	O
unique	O
point	O
which	O
is	O
the	O
euclidean	O
projection	B
of	O
the	O
origin	O
onto	O
this	O
convex	B
hull	O
.	O
)	O
we	O
claim	O
that	O
w	O
separates	O
the	O
data.1	O
to	O
see	O
this	O
,	O
assume	O
by	O
contradiction	O
that	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
2+	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
∈	O
(	O
0	O
,	O
1	O
)	O
.	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
≤	O
0	O
for	O
some	O
i.	O
take	O
w	O
(	O
cid:48	O
)	O
=	O
(	O
1	O
−	O
α	O
)	O
w	O
+	O
αxi	O
for	O
α	O
=	O
then	O
w	O
(	O
cid:48	O
)	O
is	O
also	O
in	O
the	O
convex	B
hull	O
and	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
(	O
cid:107	O
)	O
w	O
(	O
cid:48	O
)	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
1	O
−	O
α	O
)	O
2	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
α2	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
2	O
+	O
2α	O
(	O
1	O
−	O
α	O
)	O
(	O
cid:104	O
)	O
w	O
,	O
xi	O
(	O
cid:105	O
)	O
≤	O
(	O
1	O
−	O
α	O
)	O
2	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
α2	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
2	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
4	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
2	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
4	O
(	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
2	O
)	O
2	O
=	O
=	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
2	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
+	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
·	O
<	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2	O
,	O
1	O
(	O
cid:107	O
)	O
w	O
(	O
cid:107	O
)	O
2/	O
(	O
cid:107	O
)	O
xi	O
(	O
cid:107	O
)	O
2	O
+	O
1	O
which	O
leads	O
to	O
a	O
contradiction	O
.	O
we	O
have	O
thus	O
shown	O
that	O
w	O
is	O
also	O
an	O
erm	O
.	O
finally	O
,	O
since	O
w	O
is	O
in	O
the	O
convex	B
hull	O
of	O
the	O
examples	O
,	O
we	O
can	O
apply	O
caratheodory	O
’	O
s	O
theorem	O
to	O
obtain	O
that	O
w	O
is	O
also	O
in	O
the	O
convex	B
hull	O
of	O
a	O
subset	O
of	O
d	O
+	O
1	O
points	O
of	O
the	O
polygon	O
.	O
furthermore	O
,	O
the	O
minimality	O
of	O
w	O
implies	O
that	O
w	O
must	O
be	O
on	O
a	O
face	O
of	O
the	O
polygon	O
and	O
this	O
implies	O
it	O
can	O
be	O
represented	O
as	O
a	O
convex	B
combination	O
of	O
d	O
points	O
.	O
it	O
remains	O
to	O
show	O
that	O
w	O
is	O
also	O
the	O
projection	B
onto	O
the	O
polygon	O
deﬁned	O
by	O
the	O
d	O
points	O
.	O
but	O
this	O
must	O
be	O
true	O
:	O
on	O
one	O
hand	O
,	O
the	O
smaller	O
polygon	O
is	O
a	O
subset	O
of	O
the	O
larger	O
one	O
;	O
hence	O
the	O
projection	B
onto	O
the	O
smaller	O
can	O
not	O
be	O
smaller	O
in	O
norm	O
.	O
on	O
the	O
other	O
hand	O
,	O
w	O
itself	O
is	O
a	O
valid	O
solution	O
.	O
the	O
uniqueness	O
of	O
projection	B
concludes	O
our	O
proof	O
.	O
30.2.3	O
separating	O
polynomials	O
let	O
x	O
=	O
rd	O
and	O
consider	O
the	O
class	O
x	O
(	O
cid:55	O
)	O
→	O
sign	O
(	O
p	O
(	O
x	O
)	O
)	O
where	O
p	O
is	O
a	O
degree	O
r	O
polyno-	O
mial	O
.	O
1	O
it	O
can	O
be	O
shown	O
that	O
w	O
is	O
the	O
direction	O
of	O
the	O
max-margin	O
solution	O
.	O
414	O
compression	B
bounds	I
note	O
that	O
p	O
(	O
x	O
)	O
can	O
be	O
rewritten	O
as	O
(	O
cid:104	O
)	O
w	O
,	O
ψ	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
where	O
the	O
elements	O
of	O
ψ	O
(	O
x	O
)	O
are	O
all	O
the	O
monomials	O
of	O
x	O
up	O
to	O
degree	O
r.	O
therefore	O
,	O
the	O
problem	O
of	O
constructing	O
a	O
com-	O
pression	O
scheme	O
for	O
p	O
(	O
x	O
)	O
reduces	O
to	O
the	O
problem	O
of	O
constructing	O
a	O
compression	B
scheme	I
for	O
halfspaces	O
in	O
rd	O
(	O
cid:48	O
)	O
where	O
d	O
(	O
cid:48	O
)	O
=	O
o	O
(	O
dr	O
)	O
.	O
30.2.4	O
separation	O
with	O
margin	B
suppose	O
that	O
a	O
training	B
set	I
is	O
separated	O
with	O
margin	B
γ.	O
the	O
perceptron	O
algorithm	O
guarantees	O
to	O
make	O
at	O
most	O
1/γ2	O
updates	O
before	O
converging	O
to	O
a	O
solution	O
that	O
makes	O
no	O
mistakes	O
on	O
the	O
entire	O
training	B
set	I
.	O
hence	O
,	O
we	O
have	O
a	O
compression	B
scheme	I
of	O
size	O
k	O
≤	O
1/γ2	O
.	O
30.3	O
bibliographic	O
remarks	O
compression	O
schemes	O
and	O
their	O
relation	O
to	O
learning	O
were	O
introduced	O
by	O
little-	O
stone	O
&	O
warmuth	O
(	O
1986	O
)	O
.	O
as	O
we	O
have	O
shown	O
,	O
if	O
a	O
class	O
has	O
a	O
compression	B
scheme	I
then	O
it	O
is	O
learnable	O
.	O
for	O
binary	O
classiﬁcation	O
problems	O
,	O
it	O
follows	O
from	O
the	O
funda-	O
mental	O
theorem	O
of	O
learning	O
that	O
the	O
class	O
has	O
a	O
ﬁnite	O
vc	O
dimension	B
.	O
the	O
other	O
direction	O
,	O
namely	O
,	O
whether	O
every	O
hypothesis	B
class	I
of	O
ﬁnite	O
vc	O
dimension	B
has	O
a	O
compression	B
scheme	I
of	O
ﬁnite	O
size	O
,	O
is	O
an	O
open	O
problem	O
posed	O
by	O
manfred	O
war-	O
muth	O
and	O
is	O
still	O
open	O
(	O
see	O
also	O
(	O
floyd	O
1989	O
,	O
floyd	O
&	O
warmuth	O
1995	O
,	O
ben-david	O
&	O
litman	O
1998	O
,	O
livni	O
&	O
simon	O
2013	O
)	O
.	O
31	O
pac-bayes	O
31.1	O
the	O
minimum	O
description	O
length	O
(	O
mdl	O
)	O
and	O
occam	O
’	O
s	O
razor	O
principles	O
allow	O
a	O
potentially	O
very	O
large	O
hypothesis	B
class	I
but	O
deﬁne	O
a	O
hierarchy	O
over	O
hypotheses	O
and	O
prefer	O
to	O
choose	O
hypotheses	O
that	O
appear	O
higher	O
in	O
the	O
hierarchy	O
.	O
in	O
this	O
chapter	O
we	O
describe	O
the	O
pac-bayesian	O
approach	O
that	O
further	O
generalizes	O
this	O
idea	O
.	O
in	O
the	O
pac-bayesian	O
approach	O
,	O
one	O
expresses	O
the	O
prior	B
knowledge	I
by	O
deﬁning	O
prior	O
distribution	O
over	O
the	O
hypothesis	B
class	I
.	O
pac-bayes	O
bounds	O
as	O
in	O
the	O
mdl	O
paradigm	O
,	O
we	O
deﬁne	O
a	O
hierarchy	O
over	O
hypotheses	O
in	O
our	O
class	O
h.	O
now	O
,	O
the	O
hierarchy	O
takes	O
the	O
form	O
of	O
a	O
prior	O
distribution	O
over	O
h.	O
that	O
is	O
,	O
we	O
assign	O
a	O
probability	O
(	O
or	O
density	O
if	O
h	O
is	O
continuous	O
)	O
p	O
(	O
h	O
)	O
≥	O
0	O
for	O
each	O
h	O
∈	O
h	O
and	O
refer	O
to	O
p	O
(	O
h	O
)	O
as	O
the	O
prior	O
score	O
of	O
h.	O
following	O
the	O
bayesian	O
reasoning	O
approach	O
,	O
the	O
output	O
of	O
the	O
learning	O
algorithm	O
is	O
not	O
necessarily	O
a	O
single	O
hy-	O
pothesis	O
.	O
instead	O
,	O
the	O
learning	O
process	O
deﬁnes	O
a	O
posterior	O
probability	O
over	O
h	O
,	O
which	O
we	O
denote	O
by	O
q.	O
in	O
the	O
context	O
of	O
a	O
supervised	O
learning	O
problem	O
,	O
where	O
h	O
contains	O
functions	O
from	O
x	O
to	O
y	O
,	O
one	O
can	O
think	O
of	O
q	O
as	O
deﬁning	O
a	O
randomized	O
prediction	O
rule	O
as	O
follows	O
.	O
whenever	O
we	O
get	O
a	O
new	O
instance	B
x	O
,	O
we	O
randomly	O
pick	O
a	O
hypothesis	B
h	O
∈	O
h	O
according	O
to	O
q	O
and	O
predict	O
h	O
(	O
x	O
)	O
.	O
we	O
deﬁne	O
the	O
loss	B
of	O
q	O
on	O
an	O
example	O
z	O
to	O
be	O
(	O
cid:96	O
)	O
(	O
q	O
,	O
z	O
)	O
def=	O
e	O
h∼q	O
[	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
z	O
)	O
]	O
.	O
by	O
the	O
linearity	O
of	O
expectation	O
,	O
the	O
generalization	O
loss	O
and	O
training	O
loss	O
of	O
q	O
can	O
be	O
written	O
as	O
ld	O
(	O
q	O
)	O
def=	O
e	O
h∼q	O
[	O
ld	O
(	O
h	O
)	O
]	O
and	O
ls	O
(	O
q	O
)	O
def=	O
e	O
h∼q	O
[	O
ls	O
(	O
h	O
)	O
]	O
.	O
the	O
following	O
theorem	O
tells	O
us	O
that	O
the	O
diﬀerence	O
between	O
the	O
generalization	O
loss	O
and	O
the	O
empirical	O
loss	O
of	O
a	O
posterior	O
q	O
is	O
bounded	O
by	O
an	O
expression	O
that	O
depends	O
on	O
the	O
kullback-leibler	O
divergence	O
between	O
q	O
and	O
the	O
prior	O
distribu-	O
tion	O
p	O
.	O
the	O
kullback-leibler	O
is	O
a	O
natural	O
measure	O
of	O
the	O
distance	O
between	O
two	O
distributions	O
.	O
the	O
theorem	O
suggests	O
that	O
if	O
we	O
would	O
like	O
to	O
minimize	O
the	O
gen-	O
eralization	O
loss	B
of	O
q	O
,	O
we	O
should	O
jointly	O
minimize	O
both	O
the	O
empirical	O
loss	O
of	O
q	O
and	O
the	O
kullback-leibler	O
distance	O
between	O
q	O
and	O
the	O
prior	O
distribution	O
.	O
we	O
will	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
416	O
pac-bayes	O
later	O
show	O
how	O
in	O
some	O
cases	O
this	O
idea	O
leads	O
to	O
the	O
regularized	O
risk	O
minimization	O
principle	O
.	O
theorem	O
31.1	O
let	O
d	O
be	O
an	O
arbitrary	O
distribution	O
over	O
an	O
example	O
domain	B
z.	O
let	O
h	O
be	O
a	O
hypothesis	B
class	I
and	O
let	O
(	O
cid:96	O
)	O
:	O
h×	O
z	O
→	O
[	O
0	O
,	O
1	O
]	O
be	O
a	O
loss	B
function	I
.	O
let	O
p	O
be	O
a	O
prior	O
distribution	O
over	O
h	O
and	O
let	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
.	O
then	O
,	O
with	O
probability	O
of	O
at	O
least	O
1−	O
δ	O
over	O
the	O
choice	O
of	O
an	O
i.i.d	O
.	O
training	B
set	I
s	O
=	O
{	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
}	O
sampled	O
according	O
to	O
d	O
,	O
for	O
all	O
distributions	O
q	O
over	O
h	O
(	O
even	O
such	O
that	O
depend	O
on	O
s	O
)	O
,	O
we	O
have	O
(	O
cid:115	O
)	O
ld	O
(	O
q	O
)	O
≤	O
ls	O
(	O
q	O
)	O
+	O
d	O
(	O
q||p	O
)	O
+	O
ln	O
m/δ	O
2	O
(	O
m	O
−	O
1	O
)	O
where	O
d	O
(	O
q||p	O
)	O
def=	O
e	O
h∼q	O
[	O
ln	O
(	O
q	O
(	O
h	O
)	O
/p	O
(	O
h	O
)	O
)	O
]	O
is	O
the	O
kullback-leibler	O
divergence	O
.	O
proof	O
for	O
any	O
function	B
f	O
(	O
s	O
)	O
,	O
using	O
markov	O
’	O
s	O
inequality	O
:	O
[	O
f	O
(	O
s	O
)	O
≥	O
	O
]	O
=	O
p	O
[	O
ef	O
(	O
s	O
)	O
≥	O
e	O
]	O
≤	O
es	O
[	O
ef	O
(	O
s	O
)	O
]	O
s	O
e	O
p	O
s	O
,	O
.	O
(	O
31.1	O
)	O
(	O
cid:19	O
)	O
let	O
∆	O
(	O
h	O
)	O
=	O
ld	O
(	O
h	O
)	O
−	O
ls	O
(	O
h	O
)	O
.	O
we	O
will	O
apply	O
equation	O
(	O
31.1	O
)	O
with	O
the	O
function	B
(	O
cid:18	O
)	O
f	O
(	O
s	O
)	O
=	O
sup	O
q	O
2	O
(	O
m	O
−	O
1	O
)	O
e	O
h∼q	O
(	O
∆	O
(	O
h	O
)	O
)	O
2	O
−	O
d	O
(	O
q||p	O
)	O
.	O
we	O
now	O
turn	O
to	O
bound	O
es	O
[	O
ef	O
(	O
s	O
)	O
]	O
.	O
the	O
main	O
trick	O
is	O
to	O
upper	O
bound	O
f	O
(	O
s	O
)	O
by	O
using	O
an	O
expression	O
that	O
does	O
not	O
depend	O
on	O
q	O
but	O
rather	O
depends	O
on	O
the	O
prior	O
probability	O
p	O
.	O
to	O
do	O
so	O
,	O
ﬁx	O
some	O
s	O
and	O
note	O
that	O
from	O
the	O
deﬁnition	O
of	O
d	O
(	O
q||p	O
)	O
we	O
get	O
that	O
for	O
all	O
q	O
,	O
2	O
(	O
m	O
−	O
1	O
)	O
e	O
h∼q	O
(	O
∆	O
(	O
h	O
)	O
)	O
2	O
−	O
d	O
(	O
q||p	O
)	O
=	O
e	O
h∼q	O
≤	O
ln	O
e	O
h∼q	O
=	O
ln	O
e	O
h∼p	O
[	O
ln	O
(	O
e2	O
(	O
m−1	O
)	O
∆	O
(	O
h	O
)	O
2	O
[	O
e2	O
(	O
m−1	O
)	O
∆	O
(	O
h	O
)	O
2	O
[	O
e2	O
(	O
m−1	O
)	O
∆	O
(	O
h	O
)	O
2	O
]	O
,	O
p	O
(	O
h	O
)	O
/q	O
(	O
h	O
)	O
)	O
]	O
p	O
(	O
h	O
)	O
/q	O
(	O
h	O
)	O
]	O
(	O
31.2	O
)	O
where	O
the	O
inequality	O
follows	O
from	O
jensen	O
’	O
s	O
inequality	O
and	O
the	O
concavity	O
of	O
the	O
log	O
function	B
.	O
therefore	O
,	O
[	O
ef	O
(	O
s	O
)	O
]	O
≤	O
e	O
e	O
s	O
s	O
e	O
h∼p	O
[	O
e2	O
(	O
m−1	O
)	O
∆	O
(	O
h	O
)	O
2	O
]	O
.	O
(	O
31.3	O
)	O
the	O
advantage	O
of	O
the	O
expression	O
on	O
the	O
right-hand	O
side	O
stems	O
from	O
the	O
fact	O
that	O
we	O
can	O
switch	O
the	O
order	O
of	O
expectations	O
(	O
because	O
p	O
is	O
a	O
prior	O
that	O
does	O
not	O
depend	O
on	O
s	O
)	O
,	O
which	O
yields	O
[	O
ef	O
(	O
s	O
)	O
]	O
≤	O
e	O
e	O
h∼p	O
s	O
[	O
e2	O
(	O
m−1	O
)	O
∆	O
(	O
h	O
)	O
2	O
e	O
s	O
(	O
31.4	O
)	O
]	O
.	O
31.2	O
bibliographic	O
remarks	O
417	O
next	O
,	O
we	O
claim	O
that	O
for	O
all	O
h	O
we	O
have	O
es	O
[	O
e2	O
(	O
m−1	O
)	O
∆	O
(	O
h	O
)	O
2	O
hoeﬀding	O
’	O
s	O
inequality	O
tells	O
us	O
that	O
]	O
≤	O
m.	O
to	O
do	O
so	O
,	O
recall	B
that	O
[	O
∆	O
(	O
h	O
)	O
≥	O
	O
]	O
≤	O
e−2m2	O
p	O
s	O
.	O
this	O
implies	O
that	O
es	O
[	O
e2	O
(	O
m−1	O
)	O
∆	O
(	O
h	O
)	O
2	O
equation	O
(	O
31.4	O
)	O
and	O
plugging	O
into	O
equation	O
(	O
31.1	O
)	O
we	O
get	O
]	O
≤	O
m	O
(	O
see	O
exercise	O
1	O
)	O
.	O
combining	O
this	O
with	O
p	O
s	O
[	O
f	O
(	O
s	O
)	O
≥	O
	O
]	O
≤	O
m	O
e	O
.	O
(	O
31.5	O
)	O
denote	O
the	O
right-hand	O
side	O
of	O
the	O
above	O
δ	O
,	O
thus	O
	O
=	O
ln	O
(	O
m/δ	O
)	O
,	O
and	O
we	O
therefore	O
obtain	O
that	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
we	O
have	O
that	O
for	O
all	O
q	O
2	O
(	O
m	O
−	O
1	O
)	O
e	O
h∼q	O
(	O
∆	O
(	O
h	O
)	O
)	O
2	O
−	O
d	O
(	O
q||p	O
)	O
≤	O
	O
=	O
ln	O
(	O
m/δ	O
)	O
.	O
rearranging	O
the	O
inequality	O
and	O
using	O
jensen	O
’	O
s	O
inequality	O
again	O
(	O
the	O
function	B
x2	O
is	O
convex	B
)	O
we	O
conclude	O
that	O
(	O
cid:18	O
)	O
(	O
cid:19	O
)	O
2	O
≤	O
e	O
h∼q	O
e	O
h∼q	O
∆	O
(	O
h	O
)	O
(	O
∆	O
(	O
h	O
)	O
)	O
2	O
≤	O
ln	O
(	O
m/δ	O
)	O
+	O
d	O
(	O
q||p	O
)	O
2	O
(	O
m	O
−	O
1	O
)	O
.	O
(	O
31.6	O
)	O
remark	O
31.1	O
(	O
regularization	B
)	O
the	O
pac-bayes	O
bound	O
leads	O
to	O
the	O
following	O
learning	O
rule	O
:	O
given	O
a	O
prior	O
p	O
,	O
return	O
a	O
posterior	O
q	O
that	O
minimizes	O
the	O
function	B
(	O
cid:115	O
)	O
ls	O
(	O
q	O
)	O
+	O
d	O
(	O
q||p	O
)	O
+	O
ln	O
m/δ	O
2	O
(	O
m	O
−	O
1	O
)	O
.	O
(	O
31.7	O
)	O
this	O
rule	O
is	O
similar	O
to	O
the	O
regularized	O
risk	O
minimization	O
principle	O
.	O
that	O
is	O
,	O
we	O
jointly	O
minimize	O
the	O
empirical	O
loss	O
of	O
q	O
on	O
the	O
sample	O
and	O
the	O
kullback-leibler	O
“	O
distance	O
”	O
between	O
q	O
and	O
p	O
.	O
31.2	O
bibliographic	O
remarks	O
pac-bayes	O
bounds	O
were	O
ﬁrst	O
introduced	O
by	O
mcallester	O
(	O
1998	O
)	O
.	O
see	O
also	O
(	O
mcallester	O
1999	O
,	O
mcallester	O
2003	O
,	O
seeger	O
2003	O
,	O
langford	O
&	O
shawe-taylor	O
2003	O
,	O
langford	O
2006	O
)	O
.	O
31.3	O
exercises	O
1.	O
let	O
x	O
be	O
a	O
random	O
variable	O
that	O
satisﬁes	O
p	O
[	O
x	O
≥	O
	O
]	O
≤	O
e−2m2	O
e	O
[	O
e2	O
(	O
m−1	O
)	O
x	O
2	O
]	O
≤	O
m.	O
.	O
prove	O
that	O
418	O
pac-bayes	O
2	O
.	O
•	O
suppose	O
that	O
h	O
is	O
a	O
ﬁnite	O
hypothesis	B
class	I
,	O
set	B
the	O
prior	O
to	O
be	O
uniform	O
over	O
h	O
,	O
and	O
set	B
the	O
posterior	O
to	O
be	O
q	O
(	O
hs	O
)	O
=	O
1	O
for	O
some	O
hs	O
and	O
q	O
(	O
h	O
)	O
=	O
0	O
for	O
all	O
other	O
h	O
∈	O
h.	O
show	O
that	O
(	O
cid:115	O
)	O
ld	O
(	O
hs	O
)	O
≤	O
ls	O
(	O
h	O
)	O
+	O
ln	O
(	O
|h|	O
)	O
+	O
ln	O
(	O
m/δ	O
)	O
2	O
(	O
m	O
−	O
1	O
)	O
.	O
compare	O
to	O
the	O
bounds	O
we	O
derived	O
using	O
uniform	B
convergence	I
.	O
•	O
derive	O
a	O
bound	O
similar	O
to	O
the	O
occam	O
bound	O
given	O
in	O
chapter	O
7	O
using	O
the	O
pac-bayes	O
bound	O
appendix	O
a	O
technical	O
lemmas	O
lemma	O
a.1	O
let	O
a	O
>	O
0.	O
then	O
:	O
x	O
≥	O
2a	O
log	O
(	O
a	O
)	O
⇒	O
x	O
≥	O
a	O
log	O
(	O
x	O
)	O
.	O
it	O
follows	O
that	O
a	O
necessary	O
condition	O
for	O
the	O
inequality	O
x	O
<	O
a	O
log	O
(	O
x	O
)	O
to	O
hold	O
is	O
that	O
x	O
<	O
2a	O
log	O
(	O
a	O
)	O
.	O
e	O
]	O
the	O
inequality	O
x	O
≥	O
a	O
log	O
(	O
x	O
)	O
holds	O
uncon-	O
proof	O
first	O
note	O
that	O
for	O
a	O
∈	O
(	O
0	O
,	O
√	O
e.	O
ditionally	O
and	O
therefore	O
the	O
claim	O
is	O
trivial	O
.	O
from	O
now	O
on	O
,	O
assume	O
that	O
a	O
>	O
consider	O
the	O
function	B
f	O
(	O
x	O
)	O
=	O
x	O
−	O
a	O
log	O
(	O
x	O
)	O
.	O
the	O
derivative	O
is	O
f	O
(	O
cid:48	O
)	O
(	O
x	O
)	O
=	O
1	O
−	O
a/x	O
.	O
thus	O
,	O
for	O
x	O
>	O
a	O
the	O
derivative	O
is	O
positive	O
and	O
the	O
function	B
increases	O
.	O
in	O
addition	O
,	O
√	O
f	O
(	O
2a	O
log	O
(	O
a	O
)	O
)	O
=	O
2a	O
log	O
(	O
a	O
)	O
−	O
a	O
log	O
(	O
2a	O
log	O
(	O
a	O
)	O
)	O
=	O
2a	O
log	O
(	O
a	O
)	O
−	O
a	O
log	O
(	O
a	O
)	O
−	O
a	O
log	O
(	O
2	O
log	O
(	O
a	O
)	O
)	O
=	O
a	O
log	O
(	O
a	O
)	O
−	O
a	O
log	O
(	O
2	O
log	O
(	O
a	O
)	O
)	O
.	O
since	O
a	O
−	O
2	O
log	O
(	O
a	O
)	O
>	O
0	O
for	O
all	O
a	O
>	O
0	O
,	O
the	O
proof	O
follows	O
.	O
lemma	O
a.2	O
let	O
a	O
≥	O
1	O
and	O
b	O
>	O
0.	O
then	O
:	O
x	O
≥	O
4a	O
log	O
(	O
2a	O
)	O
+2b	O
⇒	O
x	O
≥	O
a	O
log	O
(	O
x	O
)	O
+b	O
.	O
it	O
suﬃces	O
to	O
prove	O
that	O
x	O
≥	O
4a	O
log	O
(	O
2a	O
)	O
+	O
2b	O
implies	O
that	O
both	O
x	O
≥	O
proof	O
2a	O
log	O
(	O
x	O
)	O
and	O
x	O
≥	O
2b	O
.	O
since	O
we	O
assume	O
a	O
≥	O
1	O
we	O
clearly	O
have	O
that	O
x	O
≥	O
2b	O
.	O
in	O
addition	O
,	O
since	O
b	O
>	O
0	O
we	O
have	O
that	O
x	O
≥	O
4a	O
log	O
(	O
2a	O
)	O
which	O
using	O
lemma	O
a.1	O
implies	O
that	O
x	O
≥	O
2a	O
log	O
(	O
x	O
)	O
.	O
this	O
concludes	O
our	O
proof	O
.	O
lemma	O
a.3	O
let	O
x	O
be	O
a	O
random	O
variable	O
and	O
x	O
(	O
cid:48	O
)	O
∈	O
r	O
be	O
a	O
scalar	O
and	O
assume	O
that	O
there	O
exists	O
a	O
>	O
0	O
such	O
that	O
for	O
all	O
t	O
≥	O
0	O
we	O
have	O
p	O
[	O
|x	O
−	O
x	O
(	O
cid:48	O
)	O
|	O
>	O
t	O
]	O
≤	O
2e−t2/a2	O
.	O
then	O
,	O
e	O
[	O
|x	O
−	O
x	O
(	O
cid:48	O
)	O
|	O
]	O
≤	O
4	O
a.	O
we	O
have	O
that	O
e	O
[	O
|x	O
−	O
x	O
(	O
cid:48	O
)	O
|	O
]	O
is	O
at	O
most	O
(	O
cid:80	O
)	O
∞	O
with	O
the	O
assumption	O
in	O
the	O
lemma	O
we	O
get	O
that	O
e	O
[	O
|x	O
−	O
x	O
(	O
cid:48	O
)	O
|	O
]	O
≤	O
2	O
a	O
(	O
cid:80	O
)	O
∞	O
proof	O
for	O
all	O
i	O
=	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
denote	O
ti	O
=	O
a	O
i.	O
since	O
ti	O
is	O
monotonically	O
increasing	O
i=1	O
ti	O
p	O
[	O
|x	O
−	O
x	O
(	O
cid:48	O
)	O
|	O
>	O
ti−1	O
]	O
.	O
combining	O
this	O
i=1	O
ie−	O
(	O
i−1	O
)	O
2	O
.	O
(	O
cid:90	O
)	O
∞	O
the	O
proof	O
now	O
follows	O
from	O
the	O
inequalities	O
∞	O
(	O
cid:88	O
)	O
ie−	O
(	O
i−1	O
)	O
2	O
≤	O
5	O
(	O
cid:88	O
)	O
ie−	O
(	O
i−1	O
)	O
2	O
+	O
xe−	O
(	O
x−1	O
)	O
2	O
dx	O
<	O
1.8	O
+	O
10−7	O
<	O
2	O
.	O
i=1	O
i=1	O
5	O
lemma	O
a.4	O
let	O
x	O
be	O
a	O
random	O
variable	O
and	O
x	O
(	O
cid:48	O
)	O
∈	O
r	O
be	O
a	O
scalar	O
and	O
assume	O
that	O
there	O
exists	O
a	O
>	O
0	O
and	O
b	O
≥	O
e	O
such	O
that	O
for	O
all	O
t	O
≥	O
0	O
we	O
have	O
p	O
[	O
|x	O
−	O
x	O
(	O
cid:48	O
)	O
|	O
>	O
t	O
]	O
≤	O
2b	O
e−t2/a2	O
.	O
then	O
,	O
e	O
[	O
|x	O
−	O
x	O
(	O
cid:48	O
)	O
|	O
]	O
≤	O
a	O
(	O
2	O
+	O
(	O
cid:112	O
)	O
log	O
(	O
b	O
)	O
)	O
.	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
420	O
technical	O
lemmas	O
proof	O
for	O
all	O
i	O
=	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
denote	O
ti	O
=	O
a	O
(	O
i+	O
(	O
cid:112	O
)	O
log	O
(	O
b	O
)	O
)	O
.	O
since	O
ti	O
is	O
monotonically	O
increasing	O
we	O
have	O
that	O
e	O
[	O
|x	O
−	O
x	O
(	O
cid:48	O
)	O
|	O
]	O
≤	O
a	O
(	O
cid:112	O
)	O
log	O
(	O
b	O
)	O
+	O
∞	O
(	O
cid:88	O
)	O
i=1	O
ti	O
p	O
[	O
|x	O
−	O
x	O
(	O
cid:48	O
)	O
|	O
>	O
ti−1	O
]	O
.	O
using	O
the	O
assumption	O
in	O
the	O
lemma	O
we	O
have	O
∞	O
(	O
cid:88	O
)	O
∞	O
(	O
cid:88	O
)	O
(	O
i	O
+	O
(	O
cid:112	O
)	O
log	O
(	O
b	O
)	O
)	O
e−	O
(	O
i−1+	O
√	O
log	O
(	O
b	O
)	O
)	O
2	O
ti	O
p	O
[	O
|x	O
−	O
x	O
(	O
cid:48	O
)	O
|	O
>	O
ti−1	O
]	O
≤	O
2	O
a	O
b	O
1+	O
log	O
(	O
b	O
)	O
√	O
(	O
cid:90	O
)	O
∞	O
(	O
cid:90	O
)	O
∞	O
(	O
cid:90	O
)	O
∞	O
(	O
cid:104	O
)	O
−e−y2	O
(	O
cid:105	O
)	O
∞√	O
√	O
√	O
log	O
(	O
b	O
)	O
log	O
(	O
b	O
)	O
i=1	O
≤	O
2	O
a	O
b	O
=	O
2	O
a	O
b	O
≤	O
4	O
a	O
b	O
=	O
2	O
a	O
b	O
i=1	O
xe−	O
(	O
x−1	O
)	O
2	O
dx	O
(	O
y	O
+	O
1	O
)	O
e−y2	O
dy	O
ye−y2	O
dy	O
log	O
(	O
b	O
)	O
=	O
2	O
a	O
b/b	O
=	O
2	O
a.	O
combining	O
the	O
preceding	O
inequalities	O
we	O
conclude	O
our	O
proof	O
.	O
lemma	O
a.5	O
let	O
m	O
,	O
d	O
be	O
two	O
positive	O
integers	O
such	O
that	O
d	O
≤	O
m	O
−	O
2.	O
then	O
,	O
(	O
cid:18	O
)	O
m	O
(	O
cid:19	O
)	O
d	O
(	O
cid:88	O
)	O
k	O
k=0	O
≤	O
(	O
cid:16	O
)	O
e	O
m	O
(	O
cid:17	O
)	O
d	O
.	O
d	O
proof	O
we	O
prove	O
the	O
claim	O
by	O
induction	O
.	O
for	O
d	O
=	O
1	O
the	O
left-hand	O
side	O
equals	O
1	O
+	O
m	O
while	O
the	O
right-hand	O
side	O
equals	O
em	O
;	O
hence	O
the	O
claim	O
is	O
true	O
.	O
assume	O
that	O
the	O
claim	O
holds	O
for	O
d	O
and	O
let	O
us	O
prove	O
it	O
for	O
d	O
+	O
1.	O
by	O
the	O
induction	O
assumption	O
we	O
have	O
(	O
cid:18	O
)	O
m	O
(	O
cid:19	O
)	O
d+1	O
(	O
cid:88	O
)	O
k	O
k=0	O
+	O
d	O
≤	O
(	O
cid:16	O
)	O
e	O
m	O
(	O
cid:17	O
)	O
d	O
(	O
cid:17	O
)	O
d	O
(	O
cid:32	O
)	O
(	O
cid:16	O
)	O
e	O
m	O
(	O
cid:17	O
)	O
d	O
(	O
cid:32	O
)	O
≤	O
(	O
cid:16	O
)	O
em	O
=	O
d	O
d	O
d	O
+	O
1	O
(	O
cid:18	O
)	O
m	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
d	O
m	O
(	O
m	O
−	O
1	O
)	O
(	O
m	O
−	O
2	O
)	O
···	O
(	O
m	O
−	O
d	O
)	O
(	O
cid:18	O
)	O
d	O
(	O
cid:19	O
)	O
d	O
(	O
m	O
−	O
d	O
)	O
(	O
cid:18	O
)	O
d	O
(	O
d	O
+	O
1	O
)	O
d	O
!	O
(	O
cid:33	O
)	O
e	O
m	O
1	O
+	O
(	O
cid:33	O
)	O
1	O
+	O
e	O
(	O
d	O
+	O
1	O
)	O
d	O
!	O
.	O
technical	O
lemmas	O
421	O
(	O
cid:33	O
)	O
(	O
m	O
−	O
d	O
)	O
√	O
2πd	O
(	O
d/e	O
)	O
d	O
2πd	O
using	O
stirling	O
’	O
s	O
approximation	O
we	O
further	O
have	O
that	O
d	O
d	O
d	O
=	O
=	O
≤	O
(	O
cid:16	O
)	O
e	O
m	O
(	O
cid:16	O
)	O
e	O
m	O
(	O
cid:16	O
)	O
e	O
m	O
≤	O
(	O
cid:16	O
)	O
e	O
m	O
(	O
cid:16	O
)	O
e	O
m	O
≤	O
(	O
cid:16	O
)	O
e	O
m	O
(	O
cid:18	O
)	O
e	O
m	O
=	O
d	O
d	O
d	O
d	O
+	O
1	O
(	O
cid:19	O
)	O
1	O
+	O
1	O
+	O
d	O
+	O
1	O
d	O
+	O
1	O
(	O
d	O
+	O
1	O
)	O
(	O
cid:19	O
)	O
d	O
e	O
√	O
(	O
cid:18	O
)	O
d	O
m	O
−	O
d	O
√	O
2πd	O
(	O
d	O
+	O
1	O
)	O
(	O
cid:17	O
)	O
d	O
(	O
cid:32	O
)	O
(	O
cid:17	O
)	O
d	O
(	O
cid:18	O
)	O
(	O
cid:17	O
)	O
d	O
·	O
d	O
+	O
1	O
+	O
(	O
m	O
−	O
d	O
)	O
/	O
(	O
cid:17	O
)	O
d	O
·	O
d	O
+	O
1	O
+	O
(	O
m	O
−	O
d	O
)	O
/2	O
(	O
cid:17	O
)	O
d	O
·	O
d/2	O
+	O
1	O
+	O
m/2	O
(	O
cid:17	O
)	O
d	O
·	O
m	O
(	O
cid:19	O
)	O
d+1	O
(	O
cid:16	O
)	O
e	O
m	O
(	O
cid:16	O
)	O
e	O
m	O
≥	O
(	O
cid:16	O
)	O
e	O
m	O
(	O
cid:16	O
)	O
e	O
m	O
(	O
cid:17	O
)	O
d	O
·	O
em	O
(	O
cid:17	O
)	O
d	O
·	O
em	O
(	O
cid:17	O
)	O
d	O
·	O
em	O
(	O
cid:17	O
)	O
d	O
·	O
m	O
d	O
+	O
1	O
d	O
+	O
1	O
d	O
+	O
1	O
d	O
+	O
1	O
d	O
+	O
1	O
=	O
=	O
=	O
,	O
d	O
d	O
d	O
d	O
,	O
d	O
+	O
1	O
·	O
·	O
(	O
cid:18	O
)	O
d	O
(	O
cid:19	O
)	O
d	O
d	O
+	O
1	O
1	O
(	O
1	O
+	O
1/d	O
)	O
d	O
·	O
1	O
e	O
where	O
in	O
the	O
last	O
inequality	O
we	O
used	O
the	O
assumption	O
that	O
d	O
≤	O
m	O
−	O
2.	O
on	O
the	O
other	O
hand	O
,	O
which	O
proves	O
our	O
inductive	O
argument	O
.	O
lemma	O
a.6	O
for	O
all	O
a	O
∈	O
r	O
we	O
have	O
ea	O
+	O
e−a	O
proof	O
observe	O
that	O
therefore	O
,	O
and	O
2	O
ea	O
=	O
ea	O
+	O
e−a	O
2	O
ea2/2	O
=	O
≤	O
ea2/2	O
.	O
an	O
n	O
!	O
.	O
n=0	O
∞	O
(	O
cid:88	O
)	O
∞	O
(	O
cid:88	O
)	O
∞	O
(	O
cid:88	O
)	O
n=0	O
=	O
n=0	O
a2n	O
2n	O
n	O
!	O
.	O
a2n	O
(	O
2n	O
)	O
!	O
,	O
observing	O
that	O
(	O
2n	O
)	O
!	O
≥	O
2n	O
n	O
!	O
for	O
every	O
n	O
≥	O
0	O
we	O
conclude	O
our	O
proof	O
.	O
appendix	O
b	O
measure	B
concentration	I
(	O
cid:80	O
)	O
m	O
let	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
be	O
an	O
i.i.d	O
.	O
sequence	O
of	O
random	O
variables	O
and	O
let	O
µ	O
be	O
their	O
mean	O
.	O
the	O
strong	O
law	O
of	O
large	O
numbers	O
states	O
that	O
when	O
m	O
tends	O
to	O
inﬁnity	O
,	O
the	O
em-	O
pirical	O
average	O
,	O
1	O
i=1	O
zi	O
,	O
converges	O
to	O
the	O
expected	O
value	O
µ	O
,	O
with	O
probability	O
m	O
1.	O
measure	B
concentration	I
inequalities	O
quantify	O
the	O
deviation	O
of	O
the	O
empirical	O
average	O
from	O
the	O
expectation	O
when	O
m	O
is	O
ﬁnite	O
.	O
b.1	O
markov	O
’	O
s	O
inequality	O
(	O
cid:90	O
)	O
∞	O
x=0	O
(	O
cid:90	O
)	O
a	O
we	O
start	O
with	O
an	O
inequality	O
which	O
is	O
called	O
markov	O
’	O
s	O
inequality	O
.	O
let	O
z	O
be	O
a	O
nonnegative	O
random	O
variable	O
.	O
the	O
expectation	O
of	O
z	O
can	O
be	O
written	O
as	O
follows	O
:	O
e	O
[	O
z	O
]	O
=	O
p	O
[	O
z	O
≥	O
x	O
]	O
dx	O
.	O
(	O
b.1	O
)	O
(	O
cid:90	O
)	O
a	O
since	O
p	O
[	O
z	O
≥	O
x	O
]	O
is	O
monotonically	O
nonincreasing	O
we	O
obtain	O
∀a	O
≥	O
0	O
,	O
e	O
[	O
z	O
]	O
≥	O
p	O
[	O
z	O
≥	O
x	O
]	O
dx	O
≥	O
p	O
[	O
z	O
≥	O
a	O
]	O
dx	O
=	O
a	O
p	O
[	O
z	O
≥	O
a	O
]	O
.	O
(	O
b.2	O
)	O
rearranging	O
the	O
inequality	O
yields	O
markov	O
’	O
s	O
inequality	O
:	O
x=0	O
x=0	O
∀a	O
≥	O
0	O
,	O
p	O
[	O
z	O
≥	O
a	O
]	O
≤	O
e	O
[	O
z	O
]	O
a	O
.	O
(	O
b.3	O
)	O
for	O
random	O
variables	O
that	O
take	O
value	O
in	O
[	O
0	O
,	O
1	O
]	O
,	O
we	O
can	O
derive	O
from	O
markov	O
’	O
s	O
inequality	O
the	O
following	O
.	O
lemma	O
b.1	O
let	O
z	O
be	O
a	O
random	O
variable	O
that	O
takes	O
values	O
in	O
[	O
0	O
,	O
1	O
]	O
.	O
assume	O
that	O
e	O
[	O
z	O
]	O
=	O
µ.	O
then	O
,	O
for	O
any	O
a	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
p	O
[	O
z	O
>	O
1	O
−	O
a	O
]	O
≥	O
µ	O
−	O
(	O
1	O
−	O
a	O
)	O
.	O
a	O
this	O
also	O
implies	O
that	O
for	O
every	O
a	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
p	O
[	O
z	O
>	O
a	O
]	O
≥	O
µ	O
−	O
a	O
1	O
−	O
a	O
≥	O
µ	O
−	O
a.	O
proof	O
let	O
y	O
=	O
1	O
−	O
z.	O
then	O
y	O
is	O
a	O
nonnegative	O
random	O
variable	O
with	O
e	O
[	O
y	O
]	O
=	O
1	O
−	O
e	O
[	O
z	O
]	O
=	O
1	O
−	O
µ.	O
applying	O
markov	O
’	O
s	O
inequality	O
on	O
y	O
we	O
obtain	O
p	O
[	O
z	O
≤	O
1	O
−	O
a	O
]	O
=	O
p	O
[	O
1	O
−	O
z	O
≥	O
a	O
]	O
=	O
p	O
[	O
y	O
≥	O
a	O
]	O
≤	O
e	O
[	O
y	O
]	O
a	O
=	O
1	O
−	O
µ	O
.	O
a	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
b.2	O
chebyshev	O
’	O
s	O
inequality	O
423	O
therefore	O
,	O
p	O
[	O
z	O
>	O
1	O
−	O
a	O
]	O
≥	O
1	O
−	O
1	O
−	O
µ	O
a	O
a	O
+	O
µ	O
−	O
1	O
a	O
.	O
=	O
b.2	O
chebyshev	O
’	O
s	O
inequality	O
applying	O
markov	O
’	O
s	O
inequality	O
on	O
the	O
random	O
variable	O
(	O
z	O
−	O
e	O
[	O
z	O
]	O
)	O
2	O
we	O
obtain	O
chebyshev	O
’	O
s	O
inequality	O
:	O
∀a	O
>	O
0	O
,	O
p	O
[	O
|z	O
−	O
e	O
[	O
z	O
]	O
|	O
≥	O
a	O
]	O
=	O
p	O
[	O
(	O
z	O
−	O
e	O
[	O
z	O
]	O
)	O
2	O
≥	O
a2	O
]	O
≤	O
var	O
[	O
z	O
]	O
,	O
(	O
b.4	O
)	O
a2	O
where	O
var	O
[	O
z	O
]	O
=	O
e	O
[	O
(	O
z	O
−	O
e	O
[	O
z	O
]	O
)	O
2	O
]	O
is	O
the	O
variance	O
of	O
z.	O
consider	O
the	O
random	O
variable	O
1	O
m	O
i=1	O
zi	O
.	O
since	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
are	O
i.i.d	O
.	O
it	O
is	O
easy	O
to	O
verify	O
that	O
var	O
(	O
cid:80	O
)	O
m	O
m	O
(	O
cid:88	O
)	O
zi	O
i=1	O
(	O
cid:35	O
)	O
(	O
cid:34	O
)	O
1	O
m	O
=	O
var	O
[	O
z1	O
]	O
m	O
.	O
applying	O
chebyshev	O
’	O
s	O
inequality	O
,	O
we	O
obtain	O
the	O
following	O
:	O
lemma	O
b.2	O
let	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
be	O
a	O
sequence	O
of	O
i.i.d	O
.	O
random	O
variables	O
and	O
assume	O
that	O
e	O
[	O
z1	O
]	O
=	O
µ	O
and	O
var	O
[	O
z1	O
]	O
≤	O
1.	O
then	O
,	O
for	O
any	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
with	O
probability	O
of	O
at	O
least	O
1	O
−	O
δ	O
we	O
have	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
≤	O
(	O
cid:114	O
)	O
1	O
.	O
δ	O
m	O
m	O
(	O
cid:88	O
)	O
i=1	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
1	O
m	O
(	O
cid:88	O
)	O
m	O
i=1	O
zi	O
−	O
µ	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
>	O
a	O
(	O
cid:35	O
)	O
(	O
cid:34	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
1	O
m	O
p	O
zi	O
−	O
µ	O
≤	O
var	O
[	O
z1	O
]	O
m	O
a2	O
≤	O
1	O
m	O
a2	O
.	O
proof	O
applying	O
chebyshev	O
’	O
s	O
inequality	O
we	O
obtain	O
that	O
for	O
all	O
a	O
>	O
0	O
the	O
proof	O
follows	O
by	O
denoting	O
the	O
right-hand	O
side	O
δ	O
and	O
solving	O
for	O
a.	O
the	O
deviation	O
between	O
the	O
empirical	O
average	O
and	O
the	O
mean	O
given	O
previously	O
decreases	O
polynomially	O
with	O
m.	O
it	O
is	O
possible	O
to	O
obtain	O
a	O
signiﬁcantly	O
faster	O
decrease	O
.	O
in	O
the	O
sections	O
that	O
follow	O
we	O
derive	O
bounds	O
that	O
decrease	O
exponentially	O
fast	O
.	O
b.3	O
chernoﬀ	O
’	O
s	O
bounds	O
pi	O
and	O
p	O
[	O
zi	O
=	O
0	O
]	O
=	O
1	O
−	O
pi	O
.	O
let	O
p	O
=	O
(	O
cid:80	O
)	O
m	O
let	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
be	O
independent	O
bernoulli	O
variables	O
where	O
for	O
every	O
i	O
,	O
p	O
[	O
zi	O
=	O
1	O
]	O
=	O
i=1	O
zi	O
.	O
using	O
the	O
i=1	O
pi	O
and	O
let	O
z	O
=	O
(	O
cid:80	O
)	O
m	O
424	O
measure	B
concentration	I
monotonicity	O
of	O
the	O
exponent	O
function	B
and	O
markov	O
’	O
s	O
inequality	O
,	O
we	O
have	O
that	O
for	O
every	O
t	O
>	O
0	O
p	O
[	O
z	O
>	O
(	O
1	O
+	O
δ	O
)	O
p	O
]	O
=	O
p	O
[	O
etz	O
>	O
et	O
(	O
1+δ	O
)	O
p	O
]	O
≤	O
e	O
[	O
etz	O
]	O
e	O
(	O
1+δ	O
)	O
tp	O
.	O
(	O
b.5	O
)	O
next	O
,	O
etzi	O
]	O
i	O
=	O
e	O
[	O
etzi	O
]	O
i	O
zi	O
]	O
=	O
e	O
[	O
(	O
cid:89	O
)	O
(	O
cid:0	O
)	O
piet	O
+	O
(	O
1	O
−	O
pi	O
)	O
e0	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
1	O
+	O
pi	O
(	O
et	O
−	O
1	O
)	O
(	O
cid:1	O
)	O
e	O
[	O
etz	O
]	O
=	O
e	O
[	O
et	O
(	O
cid:80	O
)	O
(	O
cid:89	O
)	O
(	O
cid:89	O
)	O
(	O
cid:89	O
)	O
≤	O
(	O
cid:89	O
)	O
(	O
cid:80	O
)	O
i	O
pi	O
(	O
et−1	O
)	O
epi	O
(	O
et−1	O
)	O
i	O
i	O
i	O
i	O
=	O
=	O
=	O
e	O
=	O
e	O
(	O
et−1	O
)	O
p.	O
by	O
independence	O
using	O
1	O
+	O
x	O
≤	O
ex	O
combining	O
the	O
above	O
with	O
equation	O
(	O
b.5	O
)	O
and	O
choosing	O
t	O
=	O
log	O
(	O
1	O
+	O
δ	O
)	O
we	O
obtain	O
i	O
,	O
p	O
[	O
zi	O
=	O
1	O
]	O
=	O
pi	O
and	O
p	O
[	O
zi	O
=	O
0	O
]	O
=	O
1	O
−	O
pi	O
.	O
let	O
p	O
=	O
(	O
cid:80	O
)	O
m	O
lemma	O
b.3	O
let	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
be	O
independent	O
bernoulli	O
variables	O
where	O
for	O
every	O
i=1	O
zi	O
.	O
i=1	O
pi	O
and	O
let	O
z	O
=	O
(	O
cid:80	O
)	O
m	O
then	O
,	O
for	O
any	O
δ	O
>	O
0	O
,	O
p	O
[	O
z	O
>	O
(	O
1	O
+	O
δ	O
)	O
p	O
]	O
≤	O
e−h	O
(	O
δ	O
)	O
p	O
,	O
where	O
h	O
(	O
δ	O
)	O
=	O
(	O
1	O
+	O
δ	O
)	O
log	O
(	O
1	O
+	O
δ	O
)	O
−	O
δ.	O
using	O
the	O
inequality	O
h	O
(	O
a	O
)	O
≥	O
a2/	O
(	O
2	O
+	O
2a/3	O
)	O
we	O
obtain	O
lemma	O
b.4	O
using	O
the	O
notation	O
of	O
lemma	O
b.3	O
we	O
also	O
have	O
p	O
[	O
z	O
>	O
(	O
1	O
+	O
δ	O
)	O
p	O
]	O
≤	O
e	O
−p	O
δ2	O
2+2δ/3	O
.	O
for	O
the	O
other	O
direction	O
,	O
we	O
apply	O
similar	O
calculations	O
:	O
p	O
[	O
z	O
<	O
(	O
1−δ	O
)	O
p	O
]	O
=	O
p	O
[	O
−z	O
>	O
−	O
(	O
1−δ	O
)	O
p	O
]	O
=	O
p	O
[	O
e−tz	O
>	O
e−t	O
(	O
1−δ	O
)	O
p	O
]	O
≤	O
e	O
[	O
e−tz	O
]	O
e−	O
(	O
1−δ	O
)	O
tp	O
,	O
(	O
b.6	O
)	O
b.4	O
hoeﬀding	O
’	O
s	O
inequality	O
425	O
and	O
,	O
e−tzi	O
]	O
e	O
[	O
e−tz	O
]	O
=	O
e	O
[	O
e−t	O
(	O
cid:80	O
)	O
e	O
[	O
e−tzi	O
]	O
i	O
zi	O
]	O
=	O
e	O
[	O
(	O
cid:89	O
)	O
(	O
cid:0	O
)	O
1	O
+	O
pi	O
(	O
e−t	O
−	O
1	O
)	O
(	O
cid:1	O
)	O
i	O
=	O
(	O
cid:89	O
)	O
(	O
cid:89	O
)	O
≤	O
(	O
cid:89	O
)	O
=	O
i	O
i	O
by	O
independence	O
epi	O
(	O
e−t−1	O
)	O
using	O
1	O
+	O
x	O
≤	O
ex	O
i	O
=	O
e	O
(	O
e−t−1	O
)	O
p.	O
setting	O
t	O
=	O
−	O
log	O
(	O
1	O
−	O
δ	O
)	O
yields	O
p	O
[	O
z	O
<	O
(	O
1	O
−	O
δ	O
)	O
p	O
]	O
≤	O
it	O
is	O
easy	O
to	O
verify	O
that	O
h	O
(	O
−δ	O
)	O
≥	O
h	O
(	O
δ	O
)	O
and	O
hence	O
e−δp	O
e	O
(	O
1−δ	O
)	O
log	O
(	O
1−δ	O
)	O
p	O
=	O
e−ph	O
(	O
−δ	O
)	O
.	O
lemma	O
b.5	O
using	O
the	O
notation	O
of	O
lemma	O
b.3	O
we	O
also	O
have	O
p	O
[	O
z	O
<	O
(	O
1	O
−	O
δ	O
)	O
p	O
]	O
≤	O
e−ph	O
(	O
−δ	O
)	O
≤	O
e−ph	O
(	O
δ	O
)	O
≤	O
e	O
−p	O
δ2	O
2+2δ/3	O
.	O
b.4	O
hoeﬀding	O
’	O
s	O
inequality	O
(	O
cid:80	O
)	O
m	O
lemma	O
b.6	O
(	O
hoeﬀding	O
’	O
s	O
inequality	O
)	O
let	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
be	O
a	O
sequence	O
of	O
i.i.d	O
.	O
i=1	O
zi	O
.	O
assume	O
that	O
e	O
[	O
¯z	O
]	O
=	O
µ	O
and	O
p	O
[	O
a	O
≤	O
random	O
variables	O
and	O
let	O
¯z	O
=	O
1	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
>	O
	O
(	O
cid:35	O
)	O
zi	O
≤	O
b	O
]	O
=	O
1	O
for	O
every	O
i.	O
then	O
,	O
for	O
any	O
	O
>	O
0	O
m	O
≤	O
2	O
exp	O
(	O
cid:0	O
)	O
−2	O
m	O
2/	O
(	O
b	O
−	O
a	O
)	O
2	O
(	O
cid:1	O
)	O
.	O
(	O
cid:34	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
1	O
m	O
(	O
cid:88	O
)	O
zi	O
−	O
µ	O
p	O
m	O
i=1	O
proof	O
denote	O
xi	O
=	O
zi	O
−	O
e	O
[	O
zi	O
]	O
and	O
¯x	O
=	O
1	O
i	O
xi	O
.	O
using	O
the	O
monotonicity	O
of	O
the	O
exponent	O
function	B
and	O
markov	O
’	O
s	O
inequality	O
,	O
we	O
have	O
that	O
for	O
every	O
λ	O
>	O
0	O
and	O
	O
>	O
0	O
,	O
m	O
(	O
cid:80	O
)	O
p	O
[	O
¯x	O
≥	O
	O
]	O
=	O
p	O
[	O
eλ	O
¯x	O
≥	O
eλ	O
]	O
≤	O
e−λ	O
e	O
[	O
eλ	O
¯x	O
]	O
.	O
using	O
the	O
independence	O
assumption	O
we	O
also	O
have	O
(	O
cid:34	O
)	O
(	O
cid:89	O
)	O
(	O
cid:35	O
)	O
(	O
cid:89	O
)	O
e	O
[	O
eλ	O
¯x	O
]	O
=	O
e	O
eλxi/m	O
=	O
e	O
[	O
eλxi/m	O
]	O
.	O
by	O
hoeﬀding	O
’	O
s	O
lemma	O
(	O
lemma	O
b.7	O
later	O
)	O
,	O
for	O
every	O
i	O
we	O
have	O
i	O
i	O
e	O
[	O
eλxi/m	O
]	O
≤	O
e	O
λ2	O
(	O
b−a	O
)	O
2	O
8m2	O
.	O
426	O
measure	B
concentration	I
therefore	O
,	O
p	O
[	O
¯x	O
≥	O
	O
]	O
≤	O
e−λ	O
(	O
cid:89	O
)	O
i	O
setting	O
λ	O
=	O
4m/	O
(	O
b	O
−	O
a	O
)	O
2	O
we	O
obtain	O
λ2	O
(	O
b−a	O
)	O
2	O
8m2	O
=	O
e−λ+	O
λ2	O
(	O
b−a	O
)	O
2	O
8m	O
.	O
e	O
p	O
[	O
¯x	O
≥	O
	O
]	O
≤	O
e	O
−	O
2m2	O
(	O
b−a	O
)	O
2	O
.	O
applying	O
the	O
same	O
arguments	O
on	O
the	O
variable	O
−	O
¯x	O
we	O
obtain	O
that	O
p	O
[	O
¯x	O
≤	O
−	O
]	O
≤	O
−	O
2m2	O
e	O
(	O
b−a	O
)	O
2	O
.	O
the	O
theorem	O
follows	O
by	O
applying	O
the	O
union	B
bound	I
on	O
the	O
two	O
cases	O
.	O
lemma	O
b.7	O
(	O
hoeﬀding	O
’	O
s	O
lemma	O
)	O
let	O
x	O
be	O
a	O
random	O
variable	O
that	O
takes	O
values	O
in	O
the	O
interval	O
[	O
a	O
,	O
b	O
]	O
and	O
such	O
that	O
e	O
[	O
x	O
]	O
=	O
0.	O
then	O
,	O
for	O
every	O
λ	O
>	O
0	O
,	O
e	O
[	O
eλx	O
]	O
≤	O
e	O
λ2	O
(	O
b−a	O
)	O
2	O
8	O
.	O
proof	O
since	O
f	O
(	O
x	O
)	O
=	O
eλx	O
is	O
a	O
convex	B
function	O
,	O
we	O
have	O
that	O
for	O
every	O
α	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
and	O
x	O
∈	O
[	O
a	O
,	O
b	O
]	O
,	O
f	O
(	O
x	O
)	O
≤	O
αf	O
(	O
a	O
)	O
+	O
(	O
1	O
−	O
α	O
)	O
f	O
(	O
b	O
)	O
.	O
setting	O
α	O
=	O
b−x	O
b−a	O
∈	O
[	O
0	O
,	O
1	O
]	O
yields	O
eλx	O
≤	O
b	O
−	O
x	O
b	O
−	O
a	O
taking	O
the	O
expectation	O
,	O
we	O
obtain	O
that	O
eλa	O
+	O
x	O
−	O
a	O
b	O
−	O
a	O
eλb	O
.	O
e	O
[	O
eλx	O
]	O
≤	O
b	O
−	O
e	O
[	O
x	O
]	O
b	O
−	O
a	O
eλa	O
+	O
e	O
[	O
x	O
]	O
−	O
a	O
b	O
−	O
a	O
eλb	O
=	O
b	O
b	O
−	O
a	O
eλa	O
−	O
a	O
b	O
−	O
a	O
eλb	O
,	O
where	O
we	O
used	O
the	O
fact	O
that	O
e	O
[	O
x	O
]	O
=	O
0.	O
denote	O
h	O
=	O
λ	O
(	O
b	O
−	O
a	O
)	O
,	O
p	O
=	O
−a	O
b−a	O
,	O
and	O
l	O
(	O
h	O
)	O
=	O
−hp	O
+	O
log	O
(	O
1	O
−	O
p	O
+	O
peh	O
)	O
.	O
then	O
,	O
the	O
expression	O
on	O
the	O
right-hand	O
side	O
of	O
the	O
above	O
can	O
be	O
rewritten	O
as	O
el	O
(	O
h	O
)	O
.	O
therefore	O
,	O
to	O
conclude	O
our	O
proof	O
it	O
suﬃces	O
to	O
show	O
that	O
l	O
(	O
h	O
)	O
≤	O
h2	O
8	O
.	O
this	O
follows	O
from	O
taylor	O
’	O
s	O
theorem	O
using	O
the	O
facts	O
:	O
l	O
(	O
0	O
)	O
=	O
l	O
(	O
cid:48	O
)	O
(	O
0	O
)	O
=	O
0	O
and	O
l	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
(	O
h	O
)	O
≤	O
1/4	O
for	O
all	O
h.	O
b.5	O
bennet	O
’	O
s	O
and	O
bernstein	O
’	O
s	O
inequalities	O
bennet	O
’	O
s	O
and	O
bernsein	O
’	O
s	O
inequalities	O
are	O
similar	O
to	O
chernoﬀ	O
’	O
s	O
bounds	O
,	O
but	O
they	O
hold	O
for	O
any	O
sequence	O
of	O
independent	O
random	O
variables	O
.	O
we	O
state	O
the	O
inequalities	O
without	O
proof	O
,	O
which	O
can	O
be	O
found	O
,	O
for	O
example	O
,	O
in	O
cesa-bianchi	O
&	O
lugosi	O
(	O
2006	O
)	O
.	O
lemma	O
b.8	O
(	O
bennet	O
’	O
s	O
inequality	O
)	O
let	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
be	O
independent	O
random	O
vari-	O
ables	O
with	O
zero	O
mean	O
,	O
and	O
assume	O
that	O
zi	O
≤	O
1	O
with	O
probability	O
1.	O
let	O
m	O
(	O
cid:88	O
)	O
i=1	O
σ2	O
≥	O
1	O
m	O
e	O
[	O
z	O
2	O
i	O
]	O
.	O
b.5	O
bennet	O
’	O
s	O
and	O
bernstein	O
’	O
s	O
inequalities	O
427	O
then	O
for	O
all	O
	O
>	O
0	O
,	O
where	O
(	O
cid:35	O
)	O
zi	O
>	O
	O
(	O
cid:34	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
p	O
≤	O
e	O
−mσ2h	O
(	O
	O
mσ2	O
)	O
.	O
h	O
(	O
a	O
)	O
=	O
(	O
1	O
+	O
a	O
)	O
log	O
(	O
1	O
+	O
a	O
)	O
−	O
a.	O
by	O
using	O
the	O
inequality	O
h	O
(	O
a	O
)	O
≥	O
a2/	O
(	O
2	O
+	O
2a/3	O
)	O
it	O
is	O
possible	O
to	O
derive	O
the	O
following	O
:	O
lemma	O
b.9	O
(	O
bernstein	O
’	O
s	O
inequality	O
)	O
let	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
be	O
i.i.d	O
.	O
random	O
variables	O
with	O
a	O
zero	O
mean	O
.	O
if	O
for	O
all	O
i	O
,	O
p	O
(	O
|zi|	O
<	O
m	O
)	O
=	O
1	O
,	O
then	O
for	O
all	O
t	O
>	O
0	O
:	O
(	O
cid:34	O
)	O
m	O
(	O
cid:88	O
)	O
p	O
(	O
cid:35	O
)	O
(	O
cid:32	O
)	O
zi	O
>	O
t	O
≤	O
exp	O
−	O
(	O
cid:33	O
)	O
.	O
(	O
cid:80	O
)	O
e	O
z	O
2	O
t2/2	O
j	O
+	O
m	O
t/3	O
i=1	O
b.5.1	O
application	O
(	O
cid:34	O
)	O
(	O
cid:34	O
)	O
(	O
cid:114	O
)	O
(	O
cid:114	O
)	O
(	O
cid:35	O
)	O
(	O
cid:35	O
)	O
bernstein	O
’	O
s	O
inequality	O
can	O
be	O
used	O
to	O
interpolate	O
between	O
the	O
rate	O
1/	O
we	O
derived	O
for	O
pac	O
learning	O
in	O
the	O
realizable	O
case	O
(	O
in	O
chapter	O
2	O
)	O
and	O
the	O
rate	O
1/2	O
we	O
derived	O
for	O
the	O
unrealizable	O
case	O
(	O
in	O
chapter	O
4	O
)	O
.	O
lemma	O
b.10	O
let	O
(	O
cid:96	O
)	O
:	O
h	O
×	O
z	O
→	O
[	O
0	O
,	O
1	O
]	O
be	O
a	O
loss	B
function	I
.	O
let	O
d	O
be	O
an	O
arbitrary	O
distribution	O
over	O
z.	O
fix	O
some	O
h.	O
then	O
,	O
for	O
any	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
we	O
have	O
1	O
.	O
2.	O
p	O
s∼dm	O
p	O
s∼dm	O
ls	O
(	O
h	O
)	O
≥	O
ld	O
(	O
h	O
)	O
+	O
ld	O
(	O
h	O
)	O
≥	O
ls	O
(	O
h	O
)	O
+	O
2ld	O
(	O
h	O
)	O
log	O
(	O
1/δ	O
)	O
3	O
m	O
2ls	O
(	O
h	O
)	O
log	O
(	O
1/δ	O
)	O
m	O
+	O
+	O
2	O
log	O
(	O
1/δ	O
)	O
m	O
4	O
log	O
(	O
1/δ	O
)	O
m	O
≤	O
δ	O
≤	O
δ	O
proof	O
deﬁne	O
random	O
variables	O
α1	O
,	O
.	O
.	O
.	O
,	O
αm	O
s.t	O
.	O
αi	O
=	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
−	O
ld	O
(	O
h	O
)	O
.	O
note	O
that	O
e	O
[	O
αi	O
]	O
=	O
0	O
and	O
that	O
e	O
[	O
α2	O
i	O
]	O
=	O
e	O
[	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
2	O
]	O
−	O
2ld	O
(	O
h	O
)	O
e	O
[	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
]	O
+	O
ld	O
(	O
h	O
)	O
2	O
=	O
e	O
[	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
2	O
]	O
−	O
ld	O
(	O
h	O
)	O
2	O
≤	O
e	O
[	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
2	O
]	O
≤	O
e	O
[	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
]	O
=	O
ld	O
(	O
h	O
)	O
,	O
where	O
in	O
the	O
last	O
inequality	O
we	O
used	O
the	O
fact	O
that	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
∈	O
[	O
0	O
,	O
1	O
]	O
and	O
thus	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
2	O
≤	O
(	O
cid:96	O
)	O
(	O
h	O
,	O
zi	O
)	O
.	O
applying	O
bernsein	O
’	O
s	O
inequality	O
over	O
the	O
αi	O
’	O
s	O
yields	O
(	O
cid:34	O
)	O
m	O
(	O
cid:88	O
)	O
i=1	O
p	O
αi	O
>	O
t	O
(	O
cid:35	O
)	O
(	O
cid:32	O
)	O
(	O
cid:18	O
)	O
−	O
−	O
≤	O
exp	O
≤	O
exp	O
(	O
cid:33	O
)	O
(	O
cid:19	O
)	O
(	O
cid:80	O
)	O
e	O
α2	O
t2/2	O
j	O
+	O
t/3	O
t2/2	O
m	O
ld	O
(	O
h	O
)	O
+	O
t/3	O
def=	O
δ	O
.	O
428	O
measure	B
concentration	I
solving	O
for	O
t	O
yields	O
t2/2	O
m	O
ld	O
(	O
h	O
)	O
+	O
t/3	O
⇒	O
t2/2	O
−	O
log	O
(	O
1/δ	O
)	O
3	O
t	O
−	O
log	O
(	O
1/δ	O
)	O
m	O
ld	O
(	O
h	O
)	O
=	O
0	O
=	O
log	O
(	O
1/δ	O
)	O
(	O
cid:115	O
)	O
+	O
2	O
log	O
(	O
1/δ	O
)	O
m	O
ld	O
(	O
h	O
)	O
≤	O
2	O
(	O
cid:80	O
)	O
i	O
αi	O
=	O
ls	O
(	O
h	O
)	O
−	O
ld	O
(	O
h	O
)	O
,	O
it	O
follows	O
that	O
with	O
probability	O
of	O
at	O
least	O
1−	O
δ	O
,	O
3	O
since	O
1	O
m	O
⇒	O
t	O
=	O
log	O
(	O
1/δ	O
)	O
3	O
log	O
(	O
1/δ	O
)	O
log2	O
(	O
1/δ	O
)	O
+	O
32	O
+	O
(	O
cid:112	O
)	O
2	O
log	O
(	O
1/δ	O
)	O
m	O
ld	O
(	O
h	O
)	O
(	O
cid:114	O
)	O
log	O
(	O
1/δ	O
)	O
ls	O
(	O
h	O
)	O
−	O
ld	O
(	O
h	O
)	O
≤	O
2	O
+	O
3m	O
2	O
log	O
(	O
1/δ	O
)	O
ld	O
(	O
h	O
)	O
,	O
m	O
which	O
proves	O
the	O
ﬁrst	O
inequality	O
.	O
the	O
second	O
part	O
of	O
the	O
lemma	O
follows	O
in	O
a	O
similar	O
way	O
.	O
b.6	O
slud	O
’	O
s	O
inequality	O
let	O
x	O
be	O
a	O
(	O
m	O
,	O
p	O
)	O
binomial	O
variable	O
.	O
that	O
is	O
,	O
x	O
=	O
(	O
cid:80	O
)	O
m	O
bility	O
that	O
a	O
normal	O
variable	O
will	O
be	O
greater	O
than	O
or	O
equal	O
to	O
(	O
cid:112	O
)	O
m2/	O
(	O
1	O
−	O
2	O
)	O
.	O
the	O
i=1	O
zi	O
,	O
where	O
each	O
zi	O
is	O
1	O
with	O
probability	O
p	O
and	O
0	O
with	O
probability	O
1−p	O
.	O
assume	O
that	O
p	O
=	O
(	O
1−	O
)	O
/2	O
.	O
slud	O
’	O
s	O
inequality	O
(	O
slud	O
1977	O
)	O
tells	O
us	O
that	O
p	O
[	O
x	O
≥	O
m/2	O
]	O
is	O
lower	O
bounded	O
by	O
the	O
proba-	O
following	O
lemma	O
follows	O
by	O
standard	O
tail	O
bounds	O
for	O
the	O
normal	O
distribution	O
.	O
lemma	O
b.11	O
let	O
x	O
be	O
a	O
(	O
m	O
,	O
p	O
)	O
binomial	O
variable	O
and	O
assume	O
that	O
p	O
=	O
(	O
1−	O
)	O
/2	O
.	O
then	O
,	O
(	O
cid:16	O
)	O
1	O
−	O
(	O
cid:112	O
)	O
1	O
−	O
exp	O
(	O
−m2/	O
(	O
1	O
−	O
2	O
)	O
)	O
(	O
cid:17	O
)	O
.	O
p	O
[	O
x	O
≥	O
m/2	O
]	O
≥	O
1	O
2	O
b.7	O
concentration	O
of	O
χ2	O
variables	O
let	O
x1	O
,	O
.	O
.	O
.	O
,	O
xk	O
be	O
k	O
independent	O
normally	O
distributed	O
random	O
variables	O
.	O
that	O
is	O
,	O
for	O
all	O
i	O
,	O
xi	O
∼	O
n	O
(	O
0	O
,	O
1	O
)	O
.	O
the	O
distribution	O
of	O
the	O
random	O
variable	O
x	O
2	O
is	O
called	O
1	O
+	O
···	O
+	O
x	O
2	O
χ2	O
(	O
chi	O
square	O
)	O
and	O
the	O
distribution	O
of	O
the	O
random	O
variable	O
z	O
=	O
x	O
2	O
k	O
k	O
(	O
chi	O
square	O
with	O
k	O
degrees	O
of	O
freedom	O
)	O
.	O
clearly	O
,	O
e	O
[	O
x	O
2	O
is	O
called	O
χ2	O
i	O
]	O
=	O
1	O
and	O
e	O
[	O
z	O
]	O
=	O
k.	O
the	O
following	O
lemma	O
states	O
that	O
x	O
2	O
k	O
is	O
concentrated	O
around	O
its	O
mean	O
.	O
lemma	O
b.12	O
let	O
z	O
∼	O
χ2	O
i	O
k.	O
then	O
,	O
for	O
all	O
	O
>	O
0	O
we	O
have	O
p	O
[	O
z	O
≤	O
(	O
1	O
−	O
	O
)	O
k	O
]	O
≤	O
e−2k/6	O
,	O
and	O
for	O
all	O
	O
∈	O
(	O
0	O
,	O
3	O
)	O
we	O
have	O
p	O
[	O
z	O
≥	O
(	O
1	O
+	O
	O
)	O
k	O
]	O
≤	O
e−2k/6	O
.	O
b.7	O
concentration	O
of	O
χ2	O
variables	O
429	O
finally	O
,	O
for	O
all	O
	O
∈	O
(	O
0	O
,	O
3	O
)	O
,	O
proof	O
let	O
us	O
write	O
z	O
=	O
(	O
cid:80	O
)	O
k	O
p	O
[	O
(	O
1	O
−	O
	O
)	O
k	O
≤	O
z	O
≤	O
(	O
1	O
+	O
	O
)	O
k	O
]	O
≥	O
1	O
−	O
2e−2k/6	O
.	O
i	O
where	O
xi	O
∼	O
n	O
(	O
0	O
,	O
1	O
)	O
.	O
to	O
prove	O
both	O
bounds	O
we	O
use	O
chernoﬀ	O
’	O
s	O
bounding	O
method	O
.	O
for	O
the	O
ﬁrst	O
inequality	O
,	O
we	O
ﬁrst	O
bound	O
2	O
for	O
all	O
a	O
≥	O
0	O
e	O
[	O
e−λx	O
2	O
we	O
have	O
that	O
1	O
]	O
,	O
where	O
λ	O
>	O
0	O
will	O
be	O
speciﬁed	O
later	O
.	O
since	O
e−a	O
≤	O
1−	O
a	O
+	O
a2	O
i=1	O
x	O
2	O
e	O
[	O
e−λx	O
2	O
1	O
]	O
≤	O
1	O
−	O
λ	O
e	O
[	O
x	O
2	O
1	O
]	O
+	O
λ2	O
2	O
e	O
[	O
x	O
4	O
1	O
]	O
.	O
1	O
]	O
=	O
1	O
and	O
e	O
[	O
x	O
4	O
1	O
]	O
=	O
3	O
,	O
and	O
the	O
fact	O
that	O
using	O
the	O
well	O
known	O
equalities	O
,	O
e	O
[	O
x	O
2	O
1	O
−	O
a	O
≤	O
e−a	O
we	O
obtain	O
that	O
e	O
[	O
e−λx	O
2	O
1	O
]	O
≤	O
1	O
−	O
λ	O
+	O
3	O
p	O
[	O
−z	O
≥	O
−	O
(	O
1	O
−	O
	O
)	O
k	O
]	O
=	O
p	O
(	O
cid:104	O
)	O
now	O
,	O
applying	O
chernoﬀ	O
’	O
s	O
bounding	O
method	O
we	O
get	O
that	O
2	O
λ2	O
≤	O
e−λ+	O
3	O
2	O
λ2	O
.	O
e−λz	O
≥	O
e−	O
(	O
1−	O
)	O
kλ	O
(	O
cid:105	O
)	O
≤	O
e	O
(	O
1−	O
)	O
kλ	O
e	O
(	O
cid:2	O
)	O
e−λz	O
(	O
cid:3	O
)	O
=	O
e	O
(	O
1−	O
)	O
kλ	O
(	O
cid:16	O
)	O
e	O
(	O
cid:104	O
)	O
(	O
cid:105	O
)	O
(	O
cid:17	O
)	O
k	O
1	O
e−λx	O
2	O
2	O
λ2k	O
≤	O
e	O
(	O
1−	O
)	O
kλ	O
e−λk+	O
3	O
=	O
e	O
−kλ+	O
3	O
2	O
kλ2	O
.	O
choose	O
λ	O
=	O
/3	O
we	O
obtain	O
the	O
ﬁrst	O
inequality	O
stated	O
in	O
the	O
lemma	O
.	O
for	O
the	O
second	O
inequality	O
,	O
we	O
use	O
a	O
known	O
closed	O
form	O
expression	O
for	O
the	O
moment	O
generating	O
function	B
of	O
a	O
χ2	O
k	O
distributed	O
random	O
variable	O
:	O
(	O
b.7	O
)	O
=	O
(	O
1	O
−	O
2λ	O
)	O
−k/2	O
.	O
∀λ	O
<	O
1	O
2	O
,	O
e	O
(	O
cid:104	O
)	O
eλz2	O
(	O
cid:105	O
)	O
eλz	O
≥	O
e	O
(	O
1+	O
)	O
kλ	O
(	O
cid:105	O
)	O
p	O
[	O
z	O
≥	O
(	O
1	O
+	O
	O
)	O
k	O
)	O
]	O
=	O
p	O
(	O
cid:104	O
)	O
≤	O
e−	O
(	O
1+	O
)	O
kλ	O
e	O
(	O
cid:2	O
)	O
eλz	O
(	O
cid:3	O
)	O
on	O
the	O
basis	O
of	O
the	O
equation	O
and	O
using	O
chernoﬀ	O
’	O
s	O
bounding	O
method	O
we	O
have	O
−k/2	O
=	O
e−	O
(	O
1+	O
)	O
kλ	O
(	O
1	O
−	O
2λ	O
)	O
≤	O
e−	O
(	O
1+	O
)	O
kλ	O
ekλ	O
=	O
e−kλ	O
,	O
where	O
the	O
last	O
inequality	O
occurs	O
because	O
(	O
1	O
−	O
a	O
)	O
≤	O
e−a	O
.	O
setting	O
λ	O
=	O
/6	O
(	O
which	O
is	O
in	O
(	O
0	O
,	O
1/2	O
)	O
by	O
our	O
assumption	O
)	O
we	O
obtain	O
the	O
second	O
inequality	O
stated	O
in	O
the	O
lemma	O
.	O
finally	O
,	O
the	O
last	O
inequality	O
follows	O
from	O
the	O
ﬁrst	O
two	O
inequalities	O
and	O
the	O
union	B
bound	I
.	O
appendix	O
c	O
linear	O
algebra	O
c.1	O
basic	O
deﬁnitions	O
in	O
this	O
chapter	O
we	O
only	O
deal	O
with	O
linear	O
algebra	O
over	O
ﬁnite	O
dimensional	O
euclidean	O
spaces	O
.	O
we	O
refer	O
to	O
vectors	O
as	O
column	O
vectors	O
.	O
given	O
two	O
d	O
dimensional	O
vectors	O
u	O
,	O
v	O
∈	O
rd	O
,	O
their	O
inner	O
product	O
is	O
d	O
(	O
cid:88	O
)	O
uivi	O
.	O
(	O
cid:104	O
)	O
u	O
,	O
v	O
(	O
cid:105	O
)	O
=	O
i=1	O
the	O
euclidean	O
norm	O
(	O
a.k.a	O
.	O
the	O
(	O
cid:96	O
)	O
2	O
norm	O
)	O
is	O
(	O
cid:107	O
)	O
u	O
(	O
cid:107	O
)	O
=	O
(	O
cid:112	O
)	O
(	O
cid:104	O
)	O
u	O
,	O
u	O
(	O
cid:105	O
)	O
.	O
we	O
also	O
use	O
the	O
(	O
cid:96	O
)	O
1	O
norm	O
,	O
(	O
cid:107	O
)	O
u	O
(	O
cid:107	O
)	O
1	O
=	O
(	O
cid:80	O
)	O
d	O
i=1	O
|ui|	O
and	O
the	O
(	O
cid:96	O
)	O
∞	O
norm	O
(	O
cid:107	O
)	O
u	O
(	O
cid:107	O
)	O
∞	O
=	O
maxi	O
|ui|	O
.	O
a	O
subspace	O
of	O
rd	O
is	O
a	O
subset	O
of	O
rd	O
which	O
is	O
closed	O
under	O
addition	O
and	O
scalar	O
multiplication	O
.	O
the	O
span	O
of	O
a	O
set	B
of	O
vectors	O
u1	O
,	O
.	O
.	O
.	O
,	O
uk	O
is	O
the	O
subspace	O
containing	O
all	O
vectors	O
of	O
the	O
form	O
k	O
(	O
cid:88	O
)	O
αiui	O
i=1	O
where	O
for	O
all	O
i	O
,	O
αi	O
∈	O
r.	O
a	O
set	B
of	O
vectors	O
u	O
=	O
{	O
u1	O
,	O
.	O
.	O
.	O
,	O
uk	O
}	O
is	O
independent	O
if	O
for	O
every	O
i	O
,	O
ui	O
is	O
not	O
in	O
the	O
span	O
of	O
u1	O
,	O
.	O
.	O
.	O
,	O
ui−1	O
,	O
ui+1	O
,	O
.	O
.	O
.	O
,	O
uk	O
.	O
we	O
say	O
that	O
u	O
spans	O
a	O
subspace	O
v	O
if	O
v	O
is	O
the	O
span	O
of	O
the	O
vectors	O
in	O
u	O
.	O
we	O
say	O
that	O
u	O
is	O
a	O
basis	O
of	O
v	O
if	O
it	O
is	O
both	O
independent	O
and	O
spans	O
v.	O
the	O
dimension	B
of	O
v	O
is	O
the	O
size	O
of	O
a	O
basis	O
of	O
v	O
(	O
and	O
it	O
can	O
be	O
veriﬁed	O
that	O
all	O
bases	O
of	O
v	O
have	O
the	O
same	O
size	O
)	O
.	O
we	O
say	O
that	O
u	O
is	O
an	O
orthogonal	O
set	O
if	O
for	O
all	O
i	O
(	O
cid:54	O
)	O
=	O
j	O
,	O
(	O
cid:104	O
)	O
ui	O
,	O
uj	O
(	O
cid:105	O
)	O
=	O
0.	O
we	O
say	O
that	O
u	O
is	O
an	O
orthonormal	O
set	B
if	O
it	O
is	O
orthogonal	O
and	O
if	O
for	O
every	O
i	O
,	O
(	O
cid:107	O
)	O
ui	O
(	O
cid:107	O
)	O
=	O
1.	O
given	O
a	O
matrix	O
a	O
∈	O
rn	O
,	O
d	O
,	O
the	O
range	O
of	O
a	O
is	O
the	O
span	O
of	O
its	O
columns	O
and	O
the	O
null	O
space	O
of	O
a	O
is	O
the	O
subspace	O
of	O
all	O
vectors	O
that	O
satisfy	O
au	O
=	O
0.	O
the	O
rank	O
of	O
a	O
is	O
the	O
dimension	B
of	O
its	O
range	O
.	O
the	O
transpose	O
of	O
a	O
matrix	O
a	O
,	O
denoted	O
a	O
(	O
cid:62	O
)	O
,	O
is	O
the	O
matrix	O
whose	O
(	O
i	O
,	O
j	O
)	O
entry	O
equals	O
the	O
(	O
j	O
,	O
i	O
)	O
entry	O
of	O
a.	O
we	O
say	O
that	O
a	O
is	O
symmetric	O
if	O
a	O
=	O
a	O
(	O
cid:62	O
)	O
.	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
c.2	O
eigenvalues	O
and	O
eigenvectors	O
431	O
c.2	O
eigenvalues	O
and	O
eigenvectors	O
let	O
a	O
∈	O
rd	O
,	O
d	O
be	O
a	O
matrix	O
.	O
a	O
non-zero	O
vector	O
u	O
is	O
an	O
eigenvector	O
of	O
a	O
with	O
a	O
corresponding	O
eigenvalue	O
λ	O
if	O
au	O
=	O
λu	O
.	O
ui	O
is	O
an	O
eigenvector	O
of	O
a.	O
furthermore	O
,	O
a	O
can	O
be	O
written	O
as	O
a	O
=	O
(	O
cid:80	O
)	O
d	O
if	O
a	O
∈	O
rd	O
,	O
d	O
is	O
a	O
symmetric	O
matrix	O
of	O
theorem	O
c.1	O
(	O
spectral	B
decomposition	O
)	O
rank	O
k	O
,	O
then	O
there	O
exists	O
an	O
orthonormal	O
basis	O
of	O
rd	O
,	O
u1	O
,	O
.	O
.	O
.	O
,	O
ud	O
,	O
such	O
that	O
each	O
i=1	O
λiuiu	O
(	O
cid:62	O
)	O
i	O
,	O
where	O
each	O
λi	O
is	O
the	O
eigenvalue	O
corresponding	O
to	O
the	O
eigenvector	O
ui	O
.	O
this	O
can	O
be	O
written	O
equivalently	O
as	O
a	O
=	O
u	O
du	O
(	O
cid:62	O
)	O
,	O
where	O
the	O
columns	O
of	O
u	O
are	O
the	O
vectors	O
u1	O
,	O
.	O
.	O
.	O
,	O
ud	O
,	O
and	O
d	O
is	O
a	O
diagonal	O
matrix	O
with	O
di	O
,	O
i	O
=	O
λi	O
and	O
for	O
i	O
(	O
cid:54	O
)	O
=	O
j	O
,	O
di	O
,	O
j	O
=	O
0.	O
finally	O
,	O
the	O
number	O
of	O
λi	O
which	O
are	O
nonzero	O
is	O
the	O
rank	O
of	O
the	O
matrix	O
,	O
the	O
eigenvectors	O
which	O
correspond	O
to	O
the	O
nonzero	O
eigenvalues	O
span	O
the	O
range	O
of	O
a	O
,	O
and	O
the	O
eigenvectors	O
which	O
correspond	O
to	O
zero	O
eigenvalues	O
span	O
the	O
null	O
space	O
of	O
a.	O
c.3	O
c.4	O
positive	O
deﬁnite	O
matrices	O
a	O
symmetric	O
matrix	O
a	O
∈	O
rd	O
,	O
d	O
is	O
positive	O
deﬁnite	O
if	O
all	O
its	O
eigenvalues	O
are	O
positive	O
.	O
a	O
is	O
positive	O
semideﬁnite	O
if	O
all	O
its	O
eigenvalues	O
are	O
nonnegative	O
.	O
theorem	O
c.2	O
let	O
a	O
∈	O
rd	O
,	O
d	O
be	O
a	O
symmetric	O
matrix	O
.	O
then	O
,	O
the	O
following	O
are	O
equivalent	O
deﬁnitions	O
of	O
positive	O
semideﬁniteness	O
of	O
a	O
:	O
•	O
all	O
the	O
eigenvalues	O
of	O
a	O
are	O
nonnegative	O
.	O
•	O
for	O
every	O
vector	O
u	O
,	O
(	O
cid:104	O
)	O
u	O
,	O
au	O
(	O
cid:105	O
)	O
≥	O
0	O
.	O
•	O
there	O
exists	O
a	O
matrix	O
b	O
such	O
that	O
a	O
=	O
bb	O
(	O
cid:62	O
)	O
.	O
singular	O
value	O
decomposition	O
(	O
svd	O
)	O
let	O
a	O
∈	O
rm	O
,	O
n	O
be	O
a	O
matrix	O
of	O
rank	O
r.	O
when	O
m	O
(	O
cid:54	O
)	O
=	O
n	O
,	O
the	O
eigenvalue	O
decomposition	O
given	O
in	O
theorem	O
c.1	O
can	O
not	O
be	O
applied	O
.	O
we	O
will	O
describe	O
another	O
decomposition	O
of	O
a	O
,	O
which	O
is	O
called	O
singular	O
value	O
decomposition	O
,	O
or	O
svd	O
for	O
short	O
.	O
unit	O
vectors	O
v	O
∈	O
rn	O
and	O
u	O
∈	O
rm	O
are	O
called	O
right	O
and	O
left	O
singular	O
vectors	O
of	O
a	O
with	O
corresponding	O
singular	O
value	O
σ	O
>	O
0	O
if	O
av	O
=	O
σu	O
and	O
a	O
(	O
cid:62	O
)	O
u	O
=	O
σv	O
.	O
we	O
ﬁrst	O
show	O
that	O
if	O
we	O
can	O
ﬁnd	O
r	O
orthonormal	O
singular	O
vectors	O
with	O
positive	O
singular	O
values	O
,	O
then	O
we	O
can	O
decompose	O
a	O
=	O
u	O
dv	O
(	O
cid:62	O
)	O
,	O
with	O
the	O
columns	O
of	O
u	O
and	O
v	O
containing	O
the	O
left	O
and	O
right	O
singular	O
vectors	O
,	O
and	O
d	O
being	O
a	O
diagonal	O
r	O
×	O
r	O
matrix	O
with	O
the	O
singular	O
values	O
on	O
its	O
diagonal	O
.	O
432	O
linear	O
algebra	O
lemma	O
c.3	O
let	O
a	O
∈	O
rm	O
,	O
n	O
be	O
a	O
matrix	O
of	O
rank	O
r.	O
assume	O
that	O
v1	O
,	O
.	O
.	O
.	O
,	O
vr	O
is	O
an	O
orthonormal	O
set	B
of	O
right	O
singular	O
vectors	O
of	O
a	O
,	O
u1	O
,	O
.	O
.	O
.	O
,	O
ur	O
is	O
an	O
orthonormal	O
set	B
of	O
corresponding	O
left	O
singular	O
vectors	O
of	O
a	O
,	O
and	O
σ1	O
,	O
.	O
.	O
.	O
,	O
σr	O
are	O
the	O
corresponding	O
singular	O
values	O
.	O
then	O
,	O
r	O
(	O
cid:88	O
)	O
a	O
=	O
σiuiv	O
(	O
cid:62	O
)	O
i	O
.	O
it	O
follows	O
that	O
if	O
u	O
is	O
a	O
matrix	O
whose	O
columns	O
are	O
the	O
ui	O
’	O
s	O
,	O
v	O
is	O
a	O
matrix	O
whose	O
columns	O
are	O
the	O
vi	O
’	O
s	O
,	O
and	O
d	O
is	O
a	O
diagonal	O
matrix	O
with	O
di	O
,	O
i	O
=	O
σi	O
,	O
then	O
i=1	O
a	O
=	O
u	O
dv	O
(	O
cid:62	O
)	O
.	O
adding	O
the	O
vectors	O
vr+1	O
,	O
.	O
.	O
.	O
,	O
vn	O
.	O
deﬁne	O
b	O
=	O
(	O
cid:80	O
)	O
r	O
proof	O
any	O
right	O
singular	O
vector	O
of	O
a	O
must	O
be	O
in	O
the	O
range	O
of	O
a	O
(	O
cid:62	O
)	O
(	O
otherwise	O
,	O
the	O
singular	O
value	O
will	O
have	O
to	O
be	O
zero	O
)	O
.	O
therefore	O
,	O
v1	O
,	O
.	O
.	O
.	O
,	O
vr	O
is	O
an	O
orthonormal	O
basis	O
of	O
the	O
range	O
of	O
a.	O
let	O
us	O
complete	O
it	O
to	O
an	O
orthonormal	O
basis	O
of	O
rn	O
by	O
i	O
.	O
it	O
suﬃces	O
to	O
prove	O
that	O
for	O
all	O
i	O
,	O
avi	O
=	O
bvi	O
.	O
clearly	O
,	O
if	O
i	O
>	O
r	O
then	O
avi	O
=	O
0	O
and	O
bvi	O
=	O
0	O
as	O
well	O
.	O
for	O
i	O
≤	O
r	O
we	O
have	O
i=1	O
σiuiv	O
(	O
cid:62	O
)	O
r	O
(	O
cid:88	O
)	O
bvi	O
=	O
σjujv	O
(	O
cid:62	O
)	O
j	O
vi	O
=	O
σiui	O
=	O
avi	O
,	O
j=1	O
where	O
the	O
last	O
equality	O
follows	O
from	O
the	O
deﬁnition	O
.	O
the	O
next	O
lemma	O
relates	O
the	O
singular	O
values	O
of	O
a	O
to	O
the	O
eigenvalues	O
of	O
a	O
(	O
cid:62	O
)	O
a	O
and	O
aa	O
(	O
cid:62	O
)	O
.	O
lemma	O
c.4	O
v	O
,	O
u	O
are	O
right	O
and	O
left	O
singular	O
vectors	O
of	O
a	O
with	O
singular	O
value	O
σ	O
iﬀ	O
v	O
is	O
an	O
eigenvector	O
of	O
a	O
(	O
cid:62	O
)	O
a	O
with	O
corresponding	O
eigenvalue	O
σ2	O
and	O
u	O
=	O
σ−1av	O
is	O
an	O
eigenvector	O
of	O
aa	O
(	O
cid:62	O
)	O
with	O
corresponding	O
eigenvalue	O
σ2	O
.	O
proof	O
suppose	O
that	O
σ	O
is	O
a	O
singular	O
value	O
of	O
a	O
with	O
v	O
∈	O
rn	O
being	O
the	O
corre-	O
sponding	O
right	O
singular	O
vector	O
.	O
then	O
,	O
similarly	O
,	O
a	O
(	O
cid:62	O
)	O
av	O
=	O
σa	O
(	O
cid:62	O
)	O
u	O
=	O
σ2v	O
.	O
aa	O
(	O
cid:62	O
)	O
u	O
=	O
σav	O
=	O
σ2u	O
.	O
for	O
the	O
other	O
direction	O
,	O
if	O
λ	O
(	O
cid:54	O
)	O
=	O
0	O
is	O
an	O
eigenvalue	O
of	O
a	O
(	O
cid:62	O
)	O
a	O
,	O
with	O
v	O
being	O
the	O
corresponding	O
eigenvector	O
,	O
then	O
λ	O
>	O
0	O
because	O
a	O
(	O
cid:62	O
)	O
a	O
is	O
positive	O
semideﬁnite	O
.	O
let	O
σ	O
=	O
λ	O
,	O
u	O
=	O
σ−1av	O
.	O
then	O
,	O
√	O
σu	O
=	O
and	O
√	O
λ	O
av√	O
λ	O
=	O
av	O
,	O
a	O
(	O
cid:62	O
)	O
u	O
=	O
a	O
(	O
cid:62	O
)	O
av	O
=	O
1	O
σ	O
λ	O
σ	O
v	O
=	O
σv	O
.	O
c.4	O
singular	O
value	O
decomposition	O
(	O
svd	O
)	O
433	O
finally	O
,	O
we	O
show	O
that	O
if	O
a	O
has	O
rank	O
r	O
then	O
it	O
has	O
r	O
orthonormal	O
singular	O
vectors	O
.	O
lemma	O
c.5	O
let	O
a	O
∈	O
rm	O
,	O
n	O
with	O
rank	O
r.	O
deﬁne	O
the	O
following	O
vectors	O
:	O
(	O
cid:107	O
)	O
av	O
(	O
cid:107	O
)	O
(	O
cid:107	O
)	O
av	O
(	O
cid:107	O
)	O
v1	O
=	O
argmax	O
v∈rn	O
:	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
=1	O
v2	O
=	O
argmax	O
(	O
cid:104	O
)	O
v	O
,	O
v1	O
(	O
cid:105	O
)	O
=0	O
v∈rn	O
:	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
=1	O
...	O
vr	O
=	O
(	O
cid:107	O
)	O
av	O
(	O
cid:107	O
)	O
argmax	O
v∈rn	O
:	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
=1	O
∀i	O
<	O
r	O
,	O
(	O
cid:104	O
)	O
v	O
,	O
vi	O
(	O
cid:105	O
)	O
=0	O
then	O
,	O
v1	O
,	O
.	O
.	O
.	O
,	O
vr	O
is	O
an	O
orthonormal	O
set	B
of	O
right	O
singular	O
vectors	O
of	O
a.	O
proof	O
first	O
note	O
that	O
since	O
the	O
rank	O
of	O
a	O
is	O
r	O
,	O
the	O
range	O
of	O
a	O
is	O
a	O
subspace	O
of	O
dimension	B
r	O
,	O
and	O
therefore	O
it	O
is	O
easy	O
to	O
verify	O
that	O
for	O
all	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
r	O
,	O
(	O
cid:107	O
)	O
avi	O
(	O
cid:107	O
)	O
>	O
0.	O
let	O
w	O
∈	O
rn	O
,	O
n	O
be	O
an	O
orthonormal	O
matrix	O
obtained	O
by	O
the	O
eigenvalue	O
decompo-	O
sition	O
of	O
a	O
(	O
cid:62	O
)	O
a	O
,	O
namely	O
,	O
a	O
(	O
cid:62	O
)	O
a	O
=	O
w	O
dw	O
(	O
cid:62	O
)	O
,	O
with	O
d	O
being	O
a	O
diagonal	O
matrix	O
with	O
d1,1	O
≥	O
d2,2	O
≥	O
···	O
≥	O
0.	O
we	O
will	O
show	O
that	O
v1	O
,	O
.	O
.	O
.	O
,	O
vr	O
are	O
eigenvectors	O
of	O
a	O
(	O
cid:62	O
)	O
a	O
that	O
correspond	O
to	O
nonzero	O
eigenvalues	O
,	O
and	O
,	O
hence	O
,	O
using	O
lemma	O
c.4	O
it	O
follows	O
that	O
these	O
are	O
also	O
right	O
singular	O
vectors	O
of	O
a.	O
the	O
proof	O
is	O
by	O
induction	O
.	O
for	O
the	O
basis	O
of	O
the	O
induction	O
,	O
note	O
that	O
any	O
unit	O
vector	O
v	O
can	O
be	O
written	O
as	O
v	O
=	O
w	O
x	O
,	O
for	O
x	O
=	O
w	O
(	O
cid:62	O
)	O
v	O
,	O
and	O
note	O
that	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
=	O
1.	O
therefore	O
,	O
(	O
cid:107	O
)	O
av	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:107	O
)	O
aw	O
x	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:107	O
)	O
w	O
dw	O
(	O
cid:62	O
)	O
w	O
x	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:107	O
)	O
w	O
dx	O
(	O
cid:107	O
)	O
2	O
=	O
(	O
cid:107	O
)	O
dx	O
(	O
cid:107	O
)	O
2	O
=	O
d2	O
i	O
,	O
ixi	O
2.	O
therefore	O
,	O
max	O
v	O
:	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
=1	O
(	O
cid:107	O
)	O
av	O
(	O
cid:107	O
)	O
2	O
=	O
max	O
x	O
:	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
=1	O
i=1	O
n	O
(	O
cid:88	O
)	O
i=1	O
d2	O
i	O
,	O
ixi	O
2.	O
n	O
(	O
cid:88	O
)	O
the	O
solution	O
of	O
the	O
right-hand	O
side	O
is	O
to	O
set	B
x	O
=	O
(	O
1	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
)	O
,	O
which	O
implies	O
that	O
v1	O
is	O
the	O
ﬁrst	O
eigenvector	O
of	O
a	O
(	O
cid:62	O
)	O
a.	O
since	O
(	O
cid:107	O
)	O
av1	O
(	O
cid:107	O
)	O
>	O
0	O
it	O
follows	O
that	O
d1,1	O
>	O
0	O
as	O
required	O
.	O
for	O
the	O
induction	O
step	O
,	O
assume	O
that	O
the	O
claim	O
holds	O
for	O
some	O
1	O
≤	O
t	O
≤	O
r	O
−	O
1.	O
then	O
,	O
any	O
v	O
which	O
is	O
orthogonal	O
to	O
v1	O
,	O
.	O
.	O
.	O
,	O
vt	O
can	O
be	O
written	O
as	O
v	O
=	O
w	O
x	O
with	O
all	O
the	O
ﬁrst	O
t	O
elements	O
of	O
x	O
being	O
zero	O
.	O
it	O
follows	O
that	O
max	O
v	O
:	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
=1	O
,	O
∀i≤t	O
,	O
v	O
(	O
cid:62	O
)	O
vi=0	O
(	O
cid:107	O
)	O
av	O
(	O
cid:107	O
)	O
2	O
=	O
max	O
x	O
:	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
=1	O
d2	O
i	O
,	O
ixi	O
2.	O
n	O
(	O
cid:88	O
)	O
i=t+1	O
the	O
solution	O
of	O
the	O
right-hand	O
side	O
is	O
the	O
all	O
zeros	O
vector	O
except	O
xt+1	O
=	O
1.	O
this	O
implies	O
that	O
vt+1	O
is	O
the	O
(	O
t	O
+	O
1	O
)	O
th	O
column	O
of	O
w	O
.	O
finally	O
,	O
since	O
(	O
cid:107	O
)	O
avt+1	O
(	O
cid:107	O
)	O
>	O
0	O
it	O
follows	O
that	O
dt+1	O
,	O
t+1	O
>	O
0	O
as	O
required	O
.	O
this	O
concludes	O
our	O
proof	O
.	O
434	O
linear	O
algebra	O
corollary	O
c.6	O
(	O
the	O
svd	O
theorem	O
)	O
let	O
a	O
∈	O
rm	O
,	O
n	O
with	O
rank	O
r.	O
then	O
a	O
=	O
u	O
dv	O
(	O
cid:62	O
)	O
where	O
d	O
is	O
an	O
r	O
×	O
r	O
matrix	O
with	O
nonzero	O
singular	O
values	O
of	O
a	O
and	O
the	O
columns	O
of	O
u	O
,	O
v	O
are	O
orthonormal	O
left	O
and	O
right	O
singular	O
vectors	O
of	O
a.	O
further-	O
i	O
,	O
i	O
is	O
an	O
eigenvalue	O
of	O
a	O
(	O
cid:62	O
)	O
a	O
,	O
the	O
ith	O
column	O
of	O
v	O
is	O
the	O
cor-	O
more	O
,	O
for	O
all	O
i	O
,	O
d2	O
responding	O
eigenvector	O
of	O
a	O
(	O
cid:62	O
)	O
a	O
and	O
the	O
ith	O
column	O
of	O
u	O
is	O
the	O
corresponding	O
eigenvector	O
of	O
aa	O
(	O
cid:62	O
)	O
.	O
notes	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
references	O
abernethy	O
,	O
j.	O
,	O
bartlett	O
,	O
p.	O
l.	O
,	O
rakhlin	O
,	O
a	O
.	O
&	O
tewari	O
,	O
a	O
.	O
(	O
2008	O
)	O
,	O
optimal	O
strategies	O
and	O
minimax	O
lower	O
bounds	O
for	O
online	O
convex	O
games	O
,	O
in	O
‘	O
proceedings	O
of	O
the	O
nineteenth	O
annual	O
conference	O
on	O
computational	O
learning	O
theory	O
’	O
.	O
ackerman	O
,	O
m.	O
&	O
ben-david	O
,	O
s.	O
(	O
2008	O
)	O
,	O
measures	O
of	O
clustering	B
quality	O
:	O
a	O
working	O
set	B
of	O
axioms	O
for	O
clustering	B
,	O
in	O
‘	O
proceedings	O
of	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
’	O
,	O
pp	O
.	O
121–128	O
.	O
agarwal	O
,	O
s.	O
&	O
roth	O
,	O
d.	O
(	O
2005	O
)	O
,	O
learnability	O
of	O
bipartite	B
ranking	O
functions	O
,	O
in	O
‘	O
pro-	O
ceedings	O
of	O
the	O
18th	O
annual	O
conference	O
on	O
learning	O
theory	O
’	O
,	O
pp	O
.	O
16–31	O
.	O
agmon	O
,	O
s.	O
(	O
1954	O
)	O
,	O
‘	O
the	O
relaxation	O
method	O
for	O
linear	O
inequalities	O
’	O
,	O
canadian	O
journal	O
of	O
mathematics	O
6	O
(	O
3	O
)	O
,	O
382–392	O
.	O
aizerman	O
,	O
m.	O
a.	O
,	O
braverman	O
,	O
e.	O
m.	O
&	O
rozonoer	O
,	O
l.	O
i	O
.	O
(	O
1964	O
)	O
,	O
‘	O
theoretical	O
foundations	O
of	O
the	O
potential	O
function	B
method	O
in	O
pattern	O
recognition	O
learning	O
’	O
,	O
automation	O
and	O
remote	O
control	O
25	O
,	O
821–837	O
.	O
allwein	O
,	O
e.	O
l.	O
,	O
schapire	O
,	O
r.	O
&	O
singer	O
,	O
y	O
.	O
(	O
2000	O
)	O
,	O
‘	O
reducing	O
multiclass	B
to	O
binary	O
:	O
a	O
uni-	O
fying	O
approach	O
for	O
margin	B
classiﬁers	O
’	O
,	O
journal	O
of	O
machine	O
learning	O
research	O
1	O
,	O
113–	O
141.	O
alon	O
,	O
n.	O
,	O
ben-david	O
,	O
s.	O
,	O
cesa-bianchi	O
,	O
n.	O
&	O
haussler	O
,	O
d.	O
(	O
1997	O
)	O
,	O
‘	O
scale-sensitive	O
dimen-	O
sions	O
,	O
uniform	B
convergence	I
,	O
and	O
learnability	O
’	O
,	O
journal	O
of	O
the	O
acm	O
44	O
(	O
4	O
)	O
,	O
615–631	O
.	O
anthony	O
,	O
m.	O
&	O
bartlet	O
,	O
p.	O
(	O
1999	O
)	O
,	O
neural	O
network	O
learning	O
:	O
theoretical	O
foundations	O
,	O
cambridge	O
university	O
press	O
.	O
baraniuk	O
,	O
r.	O
,	O
davenport	O
,	O
m.	O
,	O
devore	O
,	O
r.	O
&	O
wakin	O
,	O
m.	O
(	O
2008	O
)	O
,	O
‘	O
a	O
simple	O
proof	O
of	O
the	O
restricted	O
isometry	O
property	O
for	O
random	O
matrices	O
’	O
,	O
constructive	O
approximation	O
28	O
(	O
3	O
)	O
,	O
253–263	O
.	O
barber	O
,	O
d.	O
(	O
2012	O
)	O
,	O
bayesian	O
reasoning	O
and	O
machine	O
learning	O
,	O
cambridge	O
university	O
press	O
.	O
bartlett	O
,	O
p.	O
,	O
bousquet	O
,	O
o	O
.	O
&	O
mendelson	O
,	O
s.	O
(	O
2005	O
)	O
,	O
‘	O
local	O
rademacher	O
complexities	O
’	O
,	O
annals	O
of	O
statistics	O
33	O
(	O
4	O
)	O
,	O
1497–1537	O
.	O
bartlett	O
,	O
p.	O
l.	O
&	O
ben-david	O
,	O
s.	O
(	O
2002	O
)	O
,	O
‘	O
hardness	O
results	O
for	O
neural	O
network	O
approxi-	O
mation	O
problems	O
’	O
,	O
theor	O
.	O
comput	O
.	O
sci	O
.	O
284	O
(	O
1	O
)	O
,	O
53–66	O
.	O
bartlett	O
,	O
p.	O
l.	O
,	O
long	O
,	O
p.	O
m.	O
&	O
williamson	O
,	O
r.	O
c.	O
(	O
1994	O
)	O
,	O
fat-shattering	O
and	O
the	O
learn-	O
ability	O
of	O
real-valued	O
functions	O
,	O
in	O
‘	O
proceedings	O
of	O
the	O
seventh	O
annual	O
conference	O
on	O
computational	O
learning	O
theory	O
’	O
,	O
acm	O
,	O
pp	O
.	O
299–310	O
.	O
bartlett	O
,	O
p.	O
l.	O
&	O
mendelson	O
,	O
s.	O
(	O
2001	O
)	O
,	O
rademacher	O
and	O
gaussian	O
complexities	O
:	O
risk	B
bounds	O
and	O
structural	O
results	O
,	O
in	O
‘	O
14th	O
annual	O
conference	O
on	O
computational	O
learn-	O
ing	O
theory	O
,	O
colt	O
2001	O
’	O
,	O
vol	O
.	O
2111	O
,	O
springer	O
,	O
berlin	O
,	O
pp	O
.	O
224–240	O
.	O
understanding	O
machine	O
learning	O
,	O
c	O
(	O
cid:13	O
)	O
2014	O
by	O
shai	O
shalev-shwartz	O
and	O
shai	O
ben-david	O
published	O
2014	O
by	O
cambridge	O
university	O
press	O
.	O
personal	O
use	O
only	O
.	O
not	O
for	O
distribution	O
.	O
do	O
not	O
post	O
.	O
please	O
link	O
to	O
http	O
:	O
//www.cs.huji.ac.il/~shais/understandingmachinelearning	O
438	O
references	O
bartlett	O
,	O
p.	O
l.	O
&	O
mendelson	O
,	O
s.	O
(	O
2002	O
)	O
,	O
‘	O
rademacher	O
and	O
gaussian	O
complexities	O
:	O
risk	B
bounds	O
and	O
structural	O
results	O
’	O
,	O
journal	O
of	O
machine	O
learning	O
research	O
3	O
,	O
463–482	O
.	O
ben-david	O
,	O
s.	O
,	O
cesa-bianchi	O
,	O
n.	O
,	O
haussler	O
,	O
d.	O
&	O
long	O
,	O
p.	O
(	O
1995	O
)	O
,	O
‘	O
characterizations	O
of	O
learnability	O
for	O
classes	O
of	O
{	O
0	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
-valued	O
functions	O
’	O
,	O
journal	O
of	O
computer	O
and	O
system	O
sciences	O
50	O
,	O
74–86	O
.	O
ben-david	O
,	O
s.	O
,	O
eiron	O
,	O
n.	O
&	O
long	O
,	O
p.	O
(	O
2003	O
)	O
,	O
‘	O
on	O
the	O
diﬃculty	O
of	O
approximately	O
maxi-	O
mizing	O
agreements	O
’	O
,	O
journal	O
of	O
computer	O
and	O
system	O
sciences	O
66	O
(	O
3	O
)	O
,	O
496–514	O
.	O
ben-david	O
,	O
s.	O
&	O
litman	O
,	O
a	O
.	O
(	O
1998	O
)	O
,	O
‘	O
combinatorial	O
variability	O
of	O
vapnik-chervonenkis	O
classes	O
with	O
applications	O
to	O
sample	O
compression	O
schemes	O
’	O
,	O
discrete	O
applied	O
mathe-	O
matics	O
86	O
(	O
1	O
)	O
,	O
3–25	O
.	O
ben-david	O
,	O
s.	O
,	O
pal	O
,	O
d.	O
,	O
&	O
shalev-shwartz	O
,	O
s.	O
(	O
2009	O
)	O
,	O
agnostic	O
online	O
learning	O
,	O
in	O
‘	O
con-	O
ference	O
on	O
learning	O
theory	O
(	O
colt	O
)	O
’	O
.	O
ben-david	O
,	O
s.	O
&	O
simon	O
,	O
h.	O
(	O
2001	O
)	O
,	O
‘	O
eﬃcient	O
learning	O
of	O
linear	O
perceptrons	O
’	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pp	O
.	O
189–195	O
.	O
bengio	O
,	O
y	O
.	O
(	O
2009	O
)	O
,	O
‘	O
learning	O
deep	O
architectures	O
for	O
ai	O
’	O
,	O
foundations	O
and	O
trends	O
in	O
machine	O
learning	O
2	O
(	O
1	O
)	O
,	O
1–127	O
.	O
bengio	O
,	O
y	O
.	O
&	O
lecun	O
,	O
y	O
.	O
(	O
2007	O
)	O
,	O
‘	O
scaling	O
learning	O
algorithms	O
towards	O
ai	O
’	O
,	O
large-scale	O
kernel	O
machines	O
34.	O
bertsekas	O
,	O
d.	O
(	O
1999	O
)	O
,	O
nonlinear	O
programming	O
,	O
athena	O
scientiﬁc	O
.	O
beygelzimer	O
,	O
a.	O
,	O
langford	O
,	O
j	O
.	O
&	O
ravikumar	O
,	O
p.	O
(	O
2007	O
)	O
,	O
‘	O
multiclass	B
classiﬁcation	O
with	O
ﬁlter	O
trees	O
’	O
,	O
preprint	O
,	O
june	O
.	O
birkhoﬀ	O
,	O
g.	O
(	O
1946	O
)	O
,	O
‘	O
three	O
observations	O
on	O
linear	O
algebra	O
’	O
,	O
revi	O
.	O
univ	O
.	O
nac	O
.	O
tucuman	O
,	O
ser	O
a	O
5	O
,	O
147–151	O
.	O
bishop	O
,	O
c.	O
m.	O
(	O
2006	O
)	O
,	O
pattern	O
recognition	O
and	O
machine	O
learning	O
,	O
vol	O
.	O
1	O
,	O
springer	O
new	O
york	O
.	O
blum	O
,	O
l.	O
,	O
shub	O
,	O
m.	O
&	O
smale	O
,	O
s.	O
(	O
1989	O
)	O
,	O
‘	O
on	O
a	O
theory	O
of	O
computation	O
and	O
complexity	O
over	O
the	O
real	O
numbers	O
:	O
np-completeness	O
,	O
recursive	O
functions	O
and	O
universal	O
machines	O
’	O
,	O
am	O
.	O
math	O
.	O
soc	O
21	O
(	O
1	O
)	O
,	O
1–46	O
.	O
blumer	O
,	O
a.	O
,	O
ehrenfeucht	O
,	O
a.	O
,	O
haussler	O
,	O
d.	O
&	O
warmuth	O
,	O
m.	O
k.	O
(	O
1987	O
)	O
,	O
‘	O
occam	O
’	O
s	O
razor	O
’	O
,	O
information	O
processing	O
letters	O
24	O
(	O
6	O
)	O
,	O
377–380	O
.	O
blumer	O
,	O
a.	O
,	O
ehrenfeucht	O
,	O
a.	O
,	O
haussler	O
,	O
d.	O
&	O
warmuth	O
,	O
m.	O
k.	O
(	O
1989	O
)	O
,	O
‘	O
learnability	O
and	O
the	O
vapnik-chervonenkis	O
dimension	B
’	O
,	O
journal	O
of	O
the	O
association	O
for	O
computing	O
machinery	O
36	O
(	O
4	O
)	O
,	O
929–965	O
.	O
borwein	O
,	O
j	O
.	O
&	O
lewis	O
,	O
a	O
.	O
(	O
2006	O
)	O
,	O
convex	B
analysis	O
and	O
nonlinear	O
optimization	O
,	O
springer	O
.	O
boser	O
,	O
b.	O
e.	O
,	O
guyon	O
,	O
i.	O
m.	O
&	O
vapnik	O
,	O
v.	O
n.	O
(	O
1992	O
)	O
,	O
a	O
training	O
algorithm	O
for	O
optimal	O
margin	B
classiﬁers	O
,	O
in	O
‘	O
conference	O
on	O
learning	O
theory	O
(	O
colt	O
)	O
’	O
,	O
pp	O
.	O
144–152	O
.	O
bottou	O
,	O
l.	O
&	O
bousquet	O
,	O
o	O
.	O
(	O
2008	O
)	O
,	O
the	O
tradeoﬀs	O
of	O
large	O
scale	O
learning	O
,	O
in	O
‘	O
nips	O
’	O
,	O
pp	O
.	O
161–168	O
.	O
boucheron	O
,	O
s.	O
,	O
bousquet	O
,	O
o	O
.	O
&	O
lugosi	O
,	O
g.	O
(	O
2005	O
)	O
,	O
‘	O
theory	O
of	O
classiﬁcation	O
:	O
a	O
survey	O
of	O
recent	O
advances	O
’	O
,	O
esaim	O
:	O
probability	O
and	O
statistics	O
9	O
,	O
323–375	O
.	O
bousquet	O
,	O
o	O
.	O
(	O
2002	O
)	O
,	O
concentration	O
inequalities	O
and	O
empirical	O
processes	O
theory	O
ap-	O
plied	O
to	O
the	O
analysis	O
of	O
learning	O
algorithms	O
,	O
phd	O
thesis	O
,	O
ecole	O
polytechnique	O
.	O
bousquet	O
,	O
o	O
.	O
&	O
elisseeﬀ	O
,	O
a	O
.	O
(	O
2002	O
)	O
,	O
‘	O
stability	B
and	O
generalization	O
’	O
,	O
journal	O
of	O
machine	O
learning	O
research	O
2	O
,	O
499–526	O
.	O
boyd	O
,	O
s.	O
&	O
vandenberghe	O
,	O
l.	O
(	O
2004	O
)	O
,	O
convex	B
optimization	O
,	O
cambridge	O
university	O
press	O
.	O
references	O
439	O
breiman	O
,	O
l.	O
(	O
1996	O
)	O
,	O
bias	B
,	O
variance	O
,	O
and	O
arcing	O
classiﬁers	O
,	O
technical	O
report	O
460	O
,	O
statis-	O
tics	O
department	O
,	O
university	O
of	O
california	O
at	O
berkeley	O
.	O
breiman	O
,	O
l.	O
(	O
2001	O
)	O
,	O
‘	O
random	B
forests	I
’	O
,	O
machine	O
learning	O
45	O
(	O
1	O
)	O
,	O
5–32	O
.	O
breiman	O
,	O
l.	O
,	O
friedman	O
,	O
j.	O
h.	O
,	O
olshen	O
,	O
r.	O
a	O
.	O
&	O
stone	O
,	O
c.	O
j	O
.	O
(	O
1984	O
)	O
,	O
classiﬁcation	O
and	O
regression	B
trees	O
,	O
wadsworth	O
&	O
brooks	O
.	O
cand`es	O
,	O
e.	O
(	O
2008	O
)	O
,	O
‘	O
the	O
restricted	O
isometry	O
property	O
and	O
its	O
implications	O
for	O
com-	O
pressed	O
sensing	O
’	O
,	O
comptes	O
rendus	O
mathematique	O
346	O
(	O
9	O
)	O
,	O
589–592	O
.	O
candes	O
,	O
e.	O
j	O
.	O
(	O
2006	O
)	O
,	O
compressive	O
sampling	O
,	O
in	O
‘	O
proc	O
.	O
of	O
the	O
int	O
.	O
congress	O
of	O
math.	O
,	O
madrid	O
,	O
spain	O
’	O
.	O
candes	O
,	O
e.	O
&	O
tao	O
,	O
t.	O
(	O
2005	O
)	O
,	O
‘	O
decoding	O
by	O
linear	B
programming	I
’	O
,	O
ieee	O
trans	O
.	O
on	O
information	O
theory	O
51	O
,	O
4203–4215	O
.	O
cesa-bianchi	O
,	O
n.	O
&	O
lugosi	O
,	O
g.	O
(	O
2006	O
)	O
,	O
prediction	O
,	O
learning	O
,	O
and	O
games	O
,	O
cambridge	O
university	O
press	O
.	O
chang	O
,	O
h.	O
s.	O
,	O
weiss	O
,	O
y	O
.	O
&	O
freeman	O
,	O
w.	O
t.	O
(	O
2009	O
)	O
,	O
‘	O
informative	O
sensing	O
’	O
,	O
arxiv	O
preprint	O
arxiv:0901.4275	O
.	O
chapelle	O
,	O
o.	O
,	O
le	O
,	O
q	O
.	O
&	O
smola	O
,	O
a	O
.	O
(	O
2007	O
)	O
,	O
large	O
margin	B
optimization	O
of	O
ranking	B
mea-	O
sures	O
,	O
in	O
‘	O
nips	O
workshop	O
:	O
machine	O
learning	O
for	O
web	O
search	O
’	O
.	O
collins	O
,	O
m.	O
(	O
2000	O
)	O
,	O
discriminative	B
reranking	O
for	O
natural	O
language	O
parsing	O
,	O
in	O
‘	O
machine	O
learning	O
’	O
.	O
collins	O
,	O
m.	O
(	O
2002	O
)	O
,	O
discriminative	B
training	O
methods	O
for	O
hidden	O
markov	O
models	O
:	O
theory	O
and	O
experiments	O
with	O
perceptron	O
algorithms	O
,	O
in	O
‘	O
conference	O
on	O
empirical	O
methods	O
in	O
natural	O
language	O
processing	O
’	O
.	O
collobert	O
,	O
r.	O
&	O
weston	O
,	O
j	O
.	O
(	O
2008	O
)	O
,	O
a	O
uniﬁed	O
architecture	O
for	O
natural	O
language	O
process-	O
ing	O
:	O
deep	O
neural	B
networks	I
with	O
multitask	O
learning	O
,	O
in	O
‘	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
)	O
’	O
.	O
cortes	O
,	O
c.	O
&	O
vapnik	O
,	O
v.	O
(	O
1995	O
)	O
,	O
‘	O
support-vector	O
networks	O
’	O
,	O
machine	O
learning	O
20	O
(	O
3	O
)	O
,	O
273–297	O
.	O
cover	O
,	O
t.	O
(	O
1965	O
)	O
,	O
‘	O
behavior	O
of	O
sequential	O
predictors	O
of	O
binary	O
sequences	O
’	O
,	O
trans	O
.	O
4th	O
prague	O
conf	O
.	O
information	O
theory	O
statistical	O
decision	O
functions	O
,	O
random	O
processes	O
pp	O
.	O
263–272	O
.	O
cover	O
,	O
t.	O
&	O
hart	O
,	O
p.	O
(	O
1967	O
)	O
,	O
‘	O
nearest	O
neighbor	O
pattern	O
classiﬁcation	O
’	O
,	O
information	O
theory	O
,	O
ieee	O
transactions	O
on	O
13	O
(	O
1	O
)	O
,	O
21–27	O
.	O
crammer	O
,	O
k.	O
&	O
singer	O
,	O
y	O
.	O
(	O
2001	O
)	O
,	O
‘	O
on	O
the	O
algorithmic	O
implementation	O
of	O
multiclass	B
kernel-based	O
vector	O
machines	O
’	O
,	O
journal	O
of	O
machine	O
learning	O
research	O
2	O
,	O
265–292	O
.	O
cristianini	O
,	O
n.	O
&	O
shawe-taylor	O
,	O
j	O
.	O
(	O
2000	O
)	O
,	O
an	O
introduction	O
to	O
support	O
vector	O
machines	O
,	O
cambridge	O
university	O
press	O
.	O
daniely	O
,	O
a.	O
,	O
sabato	O
,	O
s.	O
,	O
ben-david	O
,	O
s.	O
&	O
shalev-shwartz	O
,	O
s.	O
(	O
2011	O
)	O
,	O
multiclass	B
learn-	O
ability	O
and	O
the	O
erm	O
principle	O
,	O
in	O
‘	O
conference	O
on	O
learning	O
theory	O
(	O
colt	O
)	O
’	O
.	O
daniely	O
,	O
a.	O
,	O
sabato	O
,	O
s.	O
&	O
shwartz	O
,	O
s.	O
s.	O
(	O
2012	O
)	O
,	O
multiclass	B
learning	O
approaches	O
:	O
a	O
theoretical	O
comparison	O
with	O
implications	O
,	O
in	O
‘	O
nips	O
’	O
.	O
davis	O
,	O
g.	O
,	O
mallat	O
,	O
s.	O
&	O
avellaneda	O
,	O
m.	O
(	O
1997	O
)	O
,	O
‘	O
greedy	O
adaptive	O
approximation	O
’	O
,	O
jour-	O
nal	O
of	O
constructive	O
approximation	O
13	O
,	O
57–98	O
.	O
devroye	O
,	O
l.	O
&	O
gy¨orﬁ	O
,	O
l.	O
(	O
1985	O
)	O
,	O
nonparametric	O
density	O
estimation	O
:	O
the	O
l	O
b1	O
s	O
view	O
,	O
wiley	O
.	O
devroye	O
,	O
l.	O
,	O
gy¨orﬁ	O
,	O
l.	O
&	O
lugosi	O
,	O
g.	O
(	O
1996	O
)	O
,	O
a	O
probabilistic	O
theory	O
of	O
pattern	O
recog-	O
nition	O
,	O
springer	O
.	O
440	O
references	O
dietterich	O
,	O
t.	O
g.	O
&	O
bakiri	O
,	O
g.	O
(	O
1995	O
)	O
,	O
‘	O
solving	O
multiclass	B
learning	O
problems	O
via	O
error-	O
correcting	O
output	O
codes	O
’	O
,	O
journal	O
of	O
artiﬁcial	O
intelligence	O
research	O
2	O
,	O
263–286	O
.	O
donoho	O
,	O
d.	O
l.	O
(	O
2006	O
)	O
,	O
‘	O
compressed	B
sensing	I
’	O
,	O
information	O
theory	O
,	O
ieee	O
transactions	O
on	O
52	O
(	O
4	O
)	O
,	O
1289–1306	O
.	O
dudley	O
,	O
r.	O
,	O
gine	O
,	O
e.	O
&	O
zinn	O
,	O
j	O
.	O
(	O
1991	O
)	O
,	O
‘	O
uniform	O
and	O
universal	O
glivenko-cantelli	O
classes	O
’	O
,	O
journal	O
of	O
theoretical	O
probability	O
4	O
(	O
3	O
)	O
,	O
485–510	O
.	O
dudley	O
,	O
r.	O
m.	O
(	O
1987	O
)	O
,	O
‘	O
universal	O
donsker	O
classes	O
and	O
metric	O
entropy	B
’	O
,	O
annals	O
of	O
prob-	O
ability	O
15	O
(	O
4	O
)	O
,	O
1306–1326	O
.	O
fisher	O
,	O
r.	O
a	O
.	O
(	O
1922	O
)	O
,	O
‘	O
on	O
the	O
mathematical	O
foundations	O
of	O
theoretical	O
statistics	O
’	O
,	O
philo-	O
sophical	O
transactions	O
of	O
the	O
royal	O
society	O
of	O
london	O
.	O
series	O
a	O
,	O
containing	O
papers	O
of	O
a	O
mathematical	O
or	O
physical	O
character	O
222	O
,	O
309–368	O
.	O
floyd	O
,	O
s.	O
(	O
1989	O
)	O
,	O
space-bounded	O
learning	O
and	O
the	O
vapnik-chervonenkis	O
dimension	B
,	O
in	O
‘	O
conference	O
on	O
learning	O
theory	O
(	O
colt	O
)	O
’	O
,	O
pp	O
.	O
349–364	O
.	O
floyd	O
,	O
s.	O
&	O
warmuth	O
,	O
m.	O
(	O
1995	O
)	O
,	O
‘	O
sample	O
compression	O
,	O
learnability	O
,	O
and	O
the	O
vapnik-	O
chervonenkis	O
dimension	B
’	O
,	O
machine	O
learning	O
21	O
(	O
3	O
)	O
,	O
269–304	O
.	O
frank	O
,	O
m.	O
&	O
wolfe	O
,	O
p.	O
(	O
1956	O
)	O
,	O
‘	O
an	O
algorithm	O
for	O
quadratic	O
programming	O
’	O
,	O
naval	O
res	O
.	O
logist	O
.	O
quart	O
.	O
3	O
,	O
95–110	O
.	O
freund	O
,	O
y	O
.	O
&	O
schapire	O
,	O
r.	O
(	O
1995	O
)	O
,	O
a	O
decision-theoretic	O
generalization	O
of	O
on-line	O
learning	O
and	O
an	O
application	O
to	O
boosting	B
,	O
in	O
‘	O
european	O
conference	O
on	O
computational	O
learning	O
theory	O
(	O
eurocolt	O
)	O
’	O
,	O
springer-verlag	O
,	O
pp	O
.	O
23–37	O
.	O
freund	O
,	O
y	O
.	O
&	O
schapire	O
,	O
r.	O
e.	O
(	O
1999	O
)	O
,	O
‘	O
large	O
margin	B
classiﬁcation	O
using	O
the	O
perceptron	O
algorithm	O
’	O
,	O
machine	O
learning	O
37	O
(	O
3	O
)	O
,	O
277–296	O
.	O
garcia	O
,	O
j	O
.	O
&	O
koelling	O
,	O
r.	O
(	O
1996	O
)	O
,	O
‘	O
relation	O
of	O
cue	O
to	O
consequence	O
in	O
avoidance	O
learning	O
’	O
,	O
foundations	O
of	O
animal	O
behavior	O
:	O
classic	O
papers	O
with	O
commentaries	O
4	O
,	O
374.	O
gentile	O
,	O
c.	O
(	O
2003	O
)	O
,	O
‘	O
the	O
robustness	O
of	O
the	O
p-norm	O
algorithms	O
’	O
,	O
machine	O
learning	O
53	O
(	O
3	O
)	O
,	O
265–299	O
.	O
georghiades	O
,	O
a.	O
,	O
belhumeur	O
,	O
p.	O
&	O
kriegman	O
,	O
d.	O
(	O
2001	O
)	O
,	O
‘	O
from	O
few	O
to	O
many	O
:	O
illumina-	O
tion	O
cone	O
models	O
for	O
face	B
recognition	I
under	O
variable	O
lighting	O
and	O
pose	O
’	O
,	O
ieee	O
trans	O
.	O
pattern	O
anal	O
.	O
mach	O
.	O
intelligence	O
23	O
(	O
6	O
)	O
,	O
643–660	O
.	O
gordon	O
,	O
g.	O
(	O
1999	O
)	O
,	O
regret	O
bounds	O
for	O
prediction	O
problems	O
,	O
in	O
‘	O
conference	O
on	O
learning	O
theory	O
(	O
colt	O
)	O
’	O
.	O
gottlieb	O
,	O
l.-a.	O
,	O
kontorovich	O
,	O
l.	O
&	O
krauthgamer	O
,	O
r.	O
(	O
2010	O
)	O
,	O
eﬃcient	O
classiﬁcation	O
for	O
metric	O
data	O
,	O
in	O
‘	O
23rd	O
conference	O
on	O
learning	O
theory	O
’	O
,	O
pp	O
.	O
433–440	O
.	O
guyon	O
,	O
i	O
.	O
&	O
elisseeﬀ	O
,	O
a	O
.	O
(	O
2003	O
)	O
,	O
‘	O
an	O
introduction	O
to	O
variable	O
and	O
feature	B
selection	I
’	O
,	O
journal	O
of	O
machine	O
learning	O
research	O
,	O
special	O
issue	O
on	O
variable	O
and	O
feature	B
selec-	O
tion	O
3	O
,	O
1157–1182	O
.	O
hadamard	O
,	O
j	O
.	O
(	O
1902	O
)	O
,	O
‘	O
sur	O
les	O
probl`emes	O
aux	O
d´eriv´ees	O
partielles	O
et	O
leur	O
signiﬁcation	O
physique	O
’	O
,	O
princeton	O
university	O
bulletin	O
13	O
,	O
49–52	O
.	O
hastie	O
,	O
t.	O
,	O
tibshirani	O
,	O
r.	O
&	O
friedman	O
,	O
j	O
.	O
(	O
2001	O
)	O
,	O
the	O
elements	O
of	O
statistical	O
learning	O
,	O
springer	O
.	O
haussler	O
,	O
d.	O
(	O
1992	O
)	O
,	O
‘	O
decision	O
theoretic	O
generalizations	O
of	O
the	O
pac	O
model	O
for	O
neural	O
net	O
and	O
other	O
learning	O
applications	O
’	O
,	O
information	O
and	O
computation	O
100	O
(	O
1	O
)	O
,	O
78–150	O
.	O
haussler	O
,	O
d.	O
&	O
long	O
,	O
p.	O
m.	O
(	O
1995	O
)	O
,	O
‘	O
a	O
generalization	O
of	O
sauer	O
’	O
s	O
lemma	O
’	O
,	O
journal	O
of	O
combinatorial	O
theory	O
,	O
series	O
a	O
71	O
(	O
2	O
)	O
,	O
219–240	O
.	O
hazan	O
,	O
e.	O
,	O
agarwal	O
,	O
a	O
.	O
&	O
kale	O
,	O
s.	O
(	O
2007	O
)	O
,	O
‘	O
logarithmic	O
regret	O
algorithms	O
for	O
online	B
convex	I
optimization	I
’	O
,	O
machine	O
learning	O
69	O
(	O
2–3	O
)	O
,	O
169–192	O
.	O
references	O
441	O
hinton	O
,	O
g.	O
e.	O
,	O
osindero	O
,	O
s.	O
&	O
teh	O
,	O
y.-w.	O
(	O
2006	O
)	O
,	O
‘	O
a	O
fast	O
learning	O
algorithm	O
for	O
deep	O
belief	O
nets	O
’	O
,	O
neural	O
computation	O
18	O
(	O
7	O
)	O
,	O
1527–1554	O
.	O
hiriart-urruty	O
,	O
j.-b	O
.	O
&	O
lemar´echal	O
,	O
c.	O
(	O
1996	O
)	O
,	O
convex	B
analysis	O
and	O
minimization	O
al-	O
gorithms	O
:	O
part	O
1	O
:	O
fundamentals	O
,	O
vol	O
.	O
1	O
,	O
springer	O
.	O
hsu	O
,	O
c.-w.	O
,	O
chang	O
,	O
c.-c.	O
&	O
lin	O
,	O
c.-j	O
.	O
(	O
2003	O
)	O
,	O
‘	O
a	O
practical	O
guide	O
to	O
support	O
vector	O
classiﬁcation	O
’	O
.	O
hyaﬁl	O
,	O
l.	O
&	O
rivest	O
,	O
r.	O
l.	O
(	O
1976	O
)	O
,	O
‘	O
constructing	O
optimal	O
binary	O
decision	B
trees	I
is	O
np-	O
complete	O
’	O
,	O
information	O
processing	O
letters	O
5	O
(	O
1	O
)	O
,	O
15–17	O
.	O
joachims	O
,	O
t.	O
(	O
2005	O
)	O
,	O
a	O
support	O
vector	O
method	O
for	O
multivariate	B
performance	I
measures	I
,	O
in	O
‘	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
)	O
’	O
.	O
kakade	O
,	O
s.	O
,	O
sridharan	O
,	O
k.	O
&	O
tewari	O
,	O
a	O
.	O
(	O
2008	O
)	O
,	O
on	O
the	O
complexity	O
of	O
linear	O
prediction	O
:	O
risk	B
bounds	O
,	O
margin	B
bounds	O
,	O
and	O
regularization	B
,	O
in	O
‘	O
nips	O
’	O
.	O
karp	O
,	O
r.	O
m.	O
(	O
1972	O
)	O
,	O
reducibility	O
among	O
combinatorial	O
problems	O
,	O
springer	O
.	O
kearns	O
,	O
m.	O
j.	O
,	O
schapire	O
,	O
r.	O
e.	O
&	O
sellie	O
,	O
l.	O
m.	O
(	O
1994	O
)	O
,	O
‘	O
toward	O
eﬃcient	O
agnostic	O
learn-	O
ing	O
’	O
,	O
machine	O
learning	O
17	O
,	O
115–141	O
.	O
kearns	O
,	O
m.	O
&	O
mansour	O
,	O
y	O
.	O
(	O
1996	O
)	O
,	O
on	O
the	O
boosting	B
ability	O
of	O
top-down	O
decision	O
tree	O
learning	O
algorithms	O
,	O
in	O
‘	O
acm	O
symposium	O
on	O
the	O
theory	O
of	O
computing	O
(	O
stoc	O
)	O
’	O
.	O
kearns	O
,	O
m.	O
&	O
ron	O
,	O
d.	O
(	O
1999	O
)	O
,	O
‘	O
algorithmic	O
stability	B
and	O
sanity-check	O
bounds	O
for	O
leave-	O
one-out	O
cross-validation	O
’	O
,	O
neural	O
computation	O
11	O
(	O
6	O
)	O
,	O
1427–1453	O
.	O
kearns	O
,	O
m.	O
&	O
valiant	O
,	O
l.	O
g.	O
(	O
1988	O
)	O
,	O
learning	O
boolean	O
formulae	O
or	O
ﬁnite	O
automata	O
is	O
as	O
hard	O
as	O
factoring	O
,	O
technical	O
report	O
tr-14-88	O
,	O
harvard	O
university	O
aiken	O
compu-	O
tation	O
laboratory	O
.	O
kearns	O
,	O
m.	O
&	O
vazirani	O
,	O
u	O
.	O
(	O
1994	O
)	O
,	O
an	O
introduction	O
to	O
computational	O
learning	O
theory	O
,	O
mit	O
press	O
.	O
kleinberg	O
,	O
j	O
.	O
(	O
2003	O
)	O
,	O
‘	O
an	O
impossibility	O
theorem	O
for	O
clustering	B
’	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
pp	O
.	O
463–470	O
.	O
klivans	O
,	O
a.	O
r.	O
&	O
sherstov	O
,	O
a.	O
a	O
.	O
(	O
2006	O
)	O
,	O
cryptographic	O
hardness	O
for	O
learning	O
intersec-	O
tions	O
of	O
halfspaces	O
,	O
in	O
‘	O
focs	O
’	O
.	O
koller	O
,	O
d.	O
&	O
friedman	O
,	O
n.	O
(	O
2009	O
)	O
,	O
probabilistic	O
graphical	O
models	O
:	O
principles	O
and	O
tech-	O
niques	O
,	O
mit	O
press	O
.	O
koltchinskii	O
,	O
v.	O
&	O
panchenko	O
,	O
d.	O
(	O
2000	O
)	O
,	O
rademacher	O
processes	O
and	O
bounding	O
the	O
risk	B
of	O
function	B
learning	O
,	O
in	O
‘	O
high	O
dimensional	O
probability	O
ii	O
’	O
,	O
springer	O
,	O
pp	O
.	O
443–457	O
.	O
kuhn	O
,	O
h.	O
w.	O
(	O
1955	O
)	O
,	O
‘	O
the	O
hungarian	O
method	O
for	O
the	O
assignment	O
problem	O
’	O
,	O
naval	O
re-	O
search	O
logistics	O
quarterly	O
2	O
(	O
1-2	O
)	O
,	O
83–97	O
.	O
kutin	O
,	O
s.	O
&	O
niyogi	O
,	O
p.	O
(	O
2002	O
)	O
,	O
almost-everywhere	O
algorithmic	O
stability	B
and	O
general-	O
ization	O
error	O
,	O
in	O
‘	O
proceedings	O
of	O
the	O
18th	O
conference	O
in	O
uncertainty	O
in	O
artiﬁcial	O
intelligence	O
’	O
,	O
pp	O
.	O
275–282	O
.	O
laﬀerty	O
,	O
j.	O
,	O
mccallum	O
,	O
a	O
.	O
&	O
pereira	O
,	O
f.	O
(	O
2001	O
)	O
,	O
conditional	O
random	O
ﬁelds	O
:	O
probabilistic	O
models	O
for	O
segmenting	O
and	O
labeling	O
sequence	O
data	O
,	O
in	O
‘	O
international	O
conference	O
on	O
machine	O
learning	O
’	O
,	O
pp	O
.	O
282–289	O
.	O
langford	O
,	O
j	O
.	O
(	O
2006	O
)	O
,	O
‘	O
tutorial	O
on	O
practical	O
prediction	O
theory	O
for	O
classiﬁcation	O
’	O
,	O
journal	O
of	O
machine	O
learning	O
research	O
6	O
(	O
1	O
)	O
,	O
273.	O
langford	O
,	O
j	O
.	O
&	O
shawe-taylor	O
,	O
j	O
.	O
(	O
2003	O
)	O
,	O
pac-bayes	O
&	O
margins	O
,	O
in	O
‘	O
nips	O
’	O
,	O
pp	O
.	O
423–430	O
.	O
le	O
cun	O
,	O
l.	O
(	O
2004	O
)	O
,	O
large	O
scale	O
online	B
learning.	O
,	O
in	O
‘	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
16	O
:	O
proceedings	O
of	O
the	O
2003	O
conference	O
’	O
,	O
vol	O
.	O
16	O
,	O
mit	O
press	O
,	O
p.	O
217	O
.	O
442	O
references	O
le	O
,	O
q.	O
v.	O
,	O
ranzato	O
,	O
m.-a.	O
,	O
monga	O
,	O
r.	O
,	O
devin	O
,	O
m.	O
,	O
corrado	O
,	O
g.	O
,	O
chen	O
,	O
k.	O
,	O
dean	O
,	O
j	O
.	O
&	O
ng	O
,	O
a.	O
y	O
.	O
(	O
2012	O
)	O
,	O
building	O
high-level	O
features	O
using	O
large	O
scale	O
unsupervised	B
learning	I
,	O
in	O
‘	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
)	O
’	O
.	O
lecun	O
,	O
y	O
.	O
&	O
bengio	O
,	O
y	O
.	O
(	O
1995	O
)	O
,	O
convolutional	O
networks	O
for	O
images	O
,	O
speech	O
and	O
time	O
series	O
,	O
the	O
mit	O
press	O
,	O
pp	O
.	O
255–258	O
.	O
lee	O
,	O
h.	O
,	O
grosse	O
,	O
r.	O
,	O
ranganath	O
,	O
r.	O
&	O
ng	O
,	O
a	O
.	O
(	O
2009	O
)	O
,	O
convolutional	O
deep	O
belief	O
networks	O
for	O
scalable	O
unsupervised	B
learning	I
of	O
hierarchical	O
representations	O
,	O
in	O
‘	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
)	O
’	O
.	O
littlestone	O
,	O
n.	O
(	O
1988	O
)	O
,	O
‘	O
learning	O
quickly	O
when	O
irrelevant	O
attributes	O
abound	O
:	O
a	O
new	O
linear-threshold	O
algorithm	O
’	O
,	O
machine	O
learning	O
2	O
,	O
285–318	O
.	O
littlestone	O
,	O
n.	O
&	O
warmuth	O
,	O
m.	O
(	O
1986	O
)	O
,	O
relating	O
data	O
compression	O
and	O
learnability	O
.	O
unpublished	O
manuscript	O
.	O
littlestone	O
,	O
n.	O
&	O
warmuth	O
,	O
m.	O
k.	O
(	O
1994	O
)	O
,	O
‘	O
the	O
weighted	O
majority	O
algorithm	O
’	O
,	O
infor-	O
mation	O
and	O
computation	O
108	O
,	O
212–261	O
.	O
livni	O
,	O
r.	O
,	O
shalev-shwartz	O
,	O
s.	O
&	O
shamir	O
,	O
o	O
.	O
(	O
2013	O
)	O
,	O
‘	O
a	O
provably	O
eﬃcient	O
algorithm	O
for	O
training	O
deep	O
networks	O
’	O
,	O
arxiv	O
preprint	O
arxiv:1304.7045	O
.	O
livni	O
,	O
r.	O
&	O
simon	O
,	O
p.	O
(	O
2013	O
)	O
,	O
honest	O
compressions	O
and	O
their	O
application	O
to	O
compression	O
schemes	O
,	O
in	O
‘	O
conference	O
on	O
learning	O
theory	O
(	O
colt	O
)	O
’	O
.	O
mackay	O
,	O
d.	O
j	O
.	O
(	O
2003	O
)	O
,	O
information	O
theory	O
,	O
inference	O
and	O
learning	O
algorithms	O
,	O
cambridge	O
university	O
press	O
.	O
mallat	O
,	O
s.	O
&	O
zhang	O
,	O
z	O
.	O
(	O
1993	O
)	O
,	O
‘	O
matching	O
pursuits	O
with	O
time-frequency	O
dictionaries	O
’	O
,	O
ieee	O
transactions	O
on	O
signal	O
processing	O
41	O
,	O
3397–3415	O
.	O
mcallester	O
,	O
d.	O
a	O
.	O
(	O
1998	O
)	O
,	O
some	O
pac-bayesian	O
theorems	O
,	O
in	O
‘	O
conference	O
on	O
learning	O
theory	O
(	O
colt	O
)	O
’	O
.	O
mcallester	O
,	O
d.	O
a	O
.	O
(	O
1999	O
)	O
,	O
pac-bayesian	O
model	O
averaging	O
,	O
in	O
‘	O
conference	O
on	O
learning	O
theory	O
(	O
colt	O
)	O
’	O
,	O
pp	O
.	O
164–170	O
.	O
mcallester	O
,	O
d.	O
a	O
.	O
(	O
2003	O
)	O
,	O
simpliﬁed	O
pac-bayesian	O
margin	B
bounds.	O
,	O
in	O
‘	O
conference	O
on	O
learning	O
theory	O
(	O
colt	O
)	O
’	O
,	O
pp	O
.	O
203–215	O
.	O
minsky	O
,	O
m.	O
&	O
papert	O
,	O
s.	O
(	O
1969	O
)	O
,	O
perceptrons	O
:	O
an	O
introduction	O
to	O
computational	O
ge-	O
ometry	O
,	O
the	O
mit	O
press	O
.	O
mukherjee	O
,	O
s.	O
,	O
niyogi	O
,	O
p.	O
,	O
poggio	O
,	O
t.	O
&	O
rifkin	O
,	O
r.	O
(	O
2006	O
)	O
,	O
‘	O
learning	O
theory	O
:	O
stability	B
is	O
suﬃcient	O
for	O
generalization	O
and	O
necessary	O
and	O
suﬃcient	O
for	O
consistency	B
of	O
empirical	B
risk	I
minimization	O
’	O
,	O
advances	O
in	O
computational	O
mathematics	O
25	O
(	O
1-3	O
)	O
,	O
161–193	O
.	O
murata	O
,	O
n.	O
(	O
1998	O
)	O
,	O
‘	O
a	O
statistical	O
study	O
of	O
on-line	O
learning	O
’	O
,	O
online	B
learning	I
and	O
neural	B
networks	I
.	O
cambridge	O
university	O
press	O
,	O
cambridge	O
,	O
uk	O
.	O
murphy	O
,	O
k.	O
p.	O
(	O
2012	O
)	O
,	O
machine	O
learning	O
:	O
a	O
probabilistic	O
perspective	O
,	O
the	O
mit	O
press	O
.	O
natarajan	O
,	O
b	O
.	O
(	O
1995	O
)	O
,	O
‘	O
sparse	O
approximate	O
solutions	O
to	O
linear	O
systems	O
’	O
,	O
siam	O
j.	O
com-	O
puting	O
25	O
(	O
2	O
)	O
,	O
227–234	O
.	O
natarajan	O
,	O
b.	O
k.	O
(	O
1989	O
)	O
,	O
‘	O
on	O
learning	O
sets	O
and	O
functions	O
’	O
,	O
mach	O
.	O
learn	O
.	O
4	O
,	O
67–97	O
.	O
nemirovski	O
,	O
a.	O
,	O
juditsky	O
,	O
a.	O
,	O
lan	O
,	O
g.	O
&	O
shapiro	O
,	O
a	O
.	O
(	O
2009	O
)	O
,	O
‘	O
robust	O
stochastic	O
ap-	O
proximation	O
approach	O
to	O
stochastic	O
programming	O
’	O
,	O
siam	O
journal	O
on	O
optimization	O
19	O
(	O
4	O
)	O
,	O
1574–1609	O
.	O
nemirovski	O
,	O
a	O
.	O
&	O
yudin	O
,	O
d.	O
(	O
1978	O
)	O
,	O
problem	O
complexity	O
and	O
method	O
eﬃciency	O
in	O
opti-	O
mization	O
,	O
nauka	O
publishers	O
,	O
moscow	O
.	O
nesterov	O
,	O
y	O
.	O
(	O
2005	O
)	O
,	O
primal-dual	O
subgradient	O
methods	O
for	O
convex	B
problems	O
,	O
technical	O
report	O
,	O
center	O
for	O
operations	O
research	O
and	O
econometrics	O
(	O
core	O
)	O
,	O
catholic	O
univer-	O
sity	O
of	O
louvain	O
(	O
ucl	O
)	O
.	O
references	O
443	O
nesterov	O
,	O
y	O
.	O
&	O
nesterov	O
,	O
i	O
.	O
(	O
2004	O
)	O
,	O
introductory	O
lectures	O
on	O
convex	B
optimization	O
:	O
a	O
basic	O
course	O
,	O
vol	O
.	O
87	O
,	O
springer	O
netherlands	O
.	O
novikoﬀ	O
,	O
a.	O
b.	O
j	O
.	O
(	O
1962	O
)	O
,	O
on	O
convergence	O
proofs	O
on	O
perceptrons	O
,	O
in	O
‘	O
proceedings	O
of	O
the	O
symposium	O
on	O
the	O
mathematical	O
theory	O
of	O
automata	O
’	O
,	O
vol	O
.	O
xii	O
,	O
pp	O
.	O
615–622	O
.	O
parberry	O
,	O
i	O
.	O
(	O
1994	O
)	O
,	O
circuit	O
complexity	O
and	O
neural	B
networks	I
,	O
the	O
mit	O
press	O
.	O
pearson	O
,	O
k.	O
(	O
1901	O
)	O
,	O
‘	O
on	O
lines	O
and	O
planes	O
of	O
closest	O
ﬁt	O
to	O
systems	O
of	O
points	O
in	O
space	O
’	O
,	O
the	O
london	O
,	O
edinburgh	O
,	O
and	O
dublin	O
philosophical	O
magazine	O
and	O
journal	O
of	O
science	O
2	O
(	O
11	O
)	O
,	O
559–572	O
.	O
phillips	O
,	O
d.	O
l.	O
(	O
1962	O
)	O
,	O
‘	O
a	O
technique	O
for	O
the	O
numerical	O
solution	O
of	O
certain	O
integral	O
equa-	O
tions	O
of	O
the	O
ﬁrst	O
kind	O
’	O
,	O
journal	O
of	O
the	O
acm	O
9	O
(	O
1	O
)	O
,	O
84–97	O
.	O
pisier	O
,	O
g.	O
(	O
1980-1981	O
)	O
,	O
‘	O
remarques	O
sur	O
un	O
r´esultat	O
non	O
publi´e	O
de	O
b.	O
maurey	O
’	O
.	O
pitt	O
,	O
l.	O
&	O
valiant	O
,	O
l.	O
(	O
1988	O
)	O
,	O
‘	O
computational	O
limitations	O
on	O
learning	O
from	O
examples	O
’	O
,	O
journal	O
of	O
the	O
association	O
for	O
computing	O
machinery	O
35	O
(	O
4	O
)	O
,	O
965–984	O
.	O
poon	O
,	O
h.	O
&	O
domingos	O
,	O
p.	O
(	O
2011	O
)	O
,	O
sum-product	O
networks	O
:	O
a	O
new	O
deep	O
architecture	O
,	O
in	O
‘	O
conference	O
on	O
uncertainty	O
in	O
artiﬁcial	O
intelligence	O
(	O
uai	O
)	O
’	O
.	O
quinlan	O
,	O
j.	O
r.	O
(	O
1986	O
)	O
,	O
‘	O
induction	O
of	O
decision	B
trees	I
’	O
,	O
machine	O
learning	O
1	O
,	O
81–106	O
.	O
quinlan	O
,	O
j.	O
r.	O
(	O
1993	O
)	O
,	O
c4.5	O
:	O
programs	O
for	O
machine	O
learning	O
,	O
morgan	O
kaufmann	O
.	O
rabiner	O
,	O
l.	O
&	O
juang	O
,	O
b	O
.	O
(	O
1986	O
)	O
,	O
‘	O
an	O
introduction	O
to	O
hidden	O
markov	O
models	O
’	O
,	O
ieee	O
assp	O
magazine	O
3	O
(	O
1	O
)	O
,	O
4–16	O
.	O
rakhlin	O
,	O
a.	O
,	O
shamir	O
,	O
o	O
.	O
&	O
sridharan	O
,	O
k.	O
(	O
2012	O
)	O
,	O
making	O
gradient	B
descent	I
optimal	O
for	O
strongly	B
convex	I
stochastic	O
optimization	O
,	O
in	O
‘	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
)	O
’	O
.	O
rakhlin	O
,	O
a.	O
,	O
sridharan	O
,	O
k.	O
&	O
tewari	O
,	O
a	O
.	O
(	O
2010	O
)	O
,	O
online	B
learning	I
:	O
random	O
averages	O
,	O
combinatorial	O
parameters	O
,	O
and	O
learnability	O
,	O
in	O
‘	O
nips	O
’	O
.	O
rakhlin	O
,	O
s.	O
,	O
mukherjee	O
,	O
s.	O
&	O
poggio	O
,	O
t.	O
(	O
2005	O
)	O
,	O
‘	O
stability	B
results	O
in	O
learning	O
theory	O
’	O
,	O
analysis	O
and	O
applications	O
3	O
(	O
4	O
)	O
,	O
397–419	O
.	O
ranzato	O
,	O
m.	O
,	O
huang	O
,	O
f.	O
,	O
boureau	O
,	O
y	O
.	O
&	O
lecun	O
,	O
y	O
.	O
(	O
2007	O
)	O
,	O
unsupervised	B
learning	I
of	O
invariant	O
feature	B
hierarchies	O
with	O
applications	O
to	O
object	O
recognition	O
,	O
in	O
‘	O
computer	O
vision	O
and	O
pattern	O
recognition	O
,	O
2007.	O
cvpr	O
’	O
07	O
.	O
ieee	O
conference	O
on	O
’	O
,	O
ieee	O
,	O
pp	O
.	O
1–	O
8.	O
rissanen	O
,	O
j	O
.	O
(	O
1978	O
)	O
,	O
‘	O
modeling	O
by	O
shortest	O
data	O
description	O
’	O
,	O
automatica	O
14	O
,	O
465–471	O
.	O
rissanen	O
,	O
j	O
.	O
(	O
1983	O
)	O
,	O
‘	O
a	O
universal	O
prior	O
for	O
integers	O
and	O
estimation	O
by	O
minimum	O
descrip-	O
tion	O
length	O
’	O
,	O
the	O
annals	O
of	O
statistics	O
11	O
(	O
2	O
)	O
,	O
416–431	O
.	O
robbins	O
,	O
h.	O
&	O
monro	O
,	O
s.	O
(	O
1951	O
)	O
,	O
‘	O
a	O
stochastic	O
approximation	O
method	O
’	O
,	O
the	O
annals	O
of	O
mathematical	O
statistics	O
pp	O
.	O
400–407	O
.	O
rogers	O
,	O
w.	O
&	O
wagner	O
,	O
t.	O
(	O
1978	O
)	O
,	O
‘	O
a	O
ﬁnite	O
sample	O
distribution-free	O
performance	O
bound	O
for	O
local	O
discrimination	O
rules	O
’	O
,	O
the	O
annals	O
of	O
statistics	O
6	O
(	O
3	O
)	O
,	O
506–514	O
.	O
rokach	O
,	O
l.	O
(	O
2007	O
)	O
,	O
data	O
mining	O
with	O
decision	B
trees	I
:	O
theory	O
and	O
applications	O
,	O
vol	O
.	O
69	O
,	O
world	O
scientiﬁc	O
.	O
rosenblatt	O
,	O
f.	O
(	O
1958	O
)	O
,	O
‘	O
the	O
perceptron	O
:	O
a	O
probabilistic	O
model	O
for	O
information	O
storage	O
(	O
reprinted	O
in	O
and	O
organization	O
in	O
the	O
brain	O
’	O
,	O
psychological	O
review	O
65	O
,	O
386–407	O
.	O
neurocomputing	O
(	O
mit	O
press	O
,	O
1988	O
)	O
.	O
)	O
.	O
rumelhart	O
,	O
d.	O
e.	O
,	O
hinton	O
,	O
g.	O
e.	O
&	O
williams	O
,	O
r.	O
j	O
.	O
(	O
1986	O
)	O
,	O
learning	O
internal	O
represen-	O
tations	O
by	O
error	O
propagation	O
,	O
in	O
d.	O
e.	O
rumelhart	O
&	O
j.	O
l.	O
mcclelland	O
,	O
eds	O
,	O
‘	O
paral-	O
lel	O
distributed	O
processing	O
–	O
explorations	O
in	O
the	O
microstructure	O
of	O
cognition	O
’	O
,	O
mit	O
press	O
,	O
chapter	O
8	O
,	O
pp	O
.	O
318–362	O
.	O
444	O
references	O
sankaran	O
,	O
j.	O
k.	O
(	O
1993	O
)	O
,	O
‘	O
a	O
note	O
on	O
resolving	O
infeasibility	O
in	O
linear	O
programs	O
by	O
con-	O
straint	O
relaxation	O
’	O
,	O
operations	O
research	O
letters	O
13	O
(	O
1	O
)	O
,	O
19–20	O
.	O
sauer	O
,	O
n.	O
(	O
1972	O
)	O
,	O
‘	O
on	O
the	O
density	O
of	O
families	O
of	O
sets	O
’	O
,	O
journal	O
of	O
combinatorial	O
theory	O
series	O
a	O
13	O
,	O
145–147	O
.	O
schapire	O
,	O
r.	O
(	O
1990	O
)	O
,	O
‘	O
the	O
strength	O
of	O
weak	O
learnability	O
’	O
,	O
machine	O
learning	O
5	O
(	O
2	O
)	O
,	O
197–	O
227.	O
schapire	O
,	O
r.	O
e.	O
&	O
freund	O
,	O
y	O
.	O
(	O
2012	O
)	O
,	O
boosting	B
:	O
foundations	O
and	O
algorithms	O
,	O
mit	O
press	O
.	O
sch¨olkopf	O
,	O
b.	O
,	O
herbrich	O
,	O
r.	O
&	O
smola	O
,	O
a	O
.	O
(	O
2001	O
)	O
,	O
a	O
generalized	O
representer	O
theorem	O
,	O
in	O
‘	O
computational	O
learning	O
theory	O
’	O
,	O
pp	O
.	O
416–426	O
.	O
sch¨olkopf	O
,	O
b.	O
,	O
herbrich	O
,	O
r.	O
,	O
smola	O
,	O
a	O
.	O
&	O
williamson	O
,	O
r.	O
(	O
2000	O
)	O
,	O
a	O
generalized	O
repre-	O
senter	O
theorem	O
,	O
in	O
‘	O
neurocolt	O
’	O
.	O
sch¨olkopf	O
,	O
b	O
.	O
&	O
smola	O
,	O
a.	O
j	O
.	O
(	O
2002	O
)	O
,	O
learning	O
with	O
kernels	B
:	O
support	O
vector	O
machines	O
,	O
regularization	B
,	O
optimization	O
and	O
beyond	O
,	O
mit	O
press	O
.	O
sch¨olkopf	O
,	O
b.	O
,	O
smola	O
,	O
a	O
.	O
&	O
m¨uller	O
,	O
k.-r.	O
(	O
1998	O
)	O
,	O
‘	O
nonlinear	O
component	O
analysis	O
as	O
a	O
kernel	O
eigenvalue	O
problem	O
’	O
,	O
neural	O
computation	O
10	O
(	O
5	O
)	O
,	O
1299–1319	O
.	O
seeger	O
,	O
m.	O
(	O
2003	O
)	O
,	O
‘	O
pac-bayesian	O
generalisation	O
error	O
bounds	O
for	O
gaussian	O
process	O
clas-	O
siﬁcation	O
’	O
,	O
the	O
journal	O
of	O
machine	O
learning	O
research	O
3	O
,	O
233–269	O
.	O
shakhnarovich	O
,	O
g.	O
,	O
darrell	O
,	O
t.	O
&	O
indyk	O
,	O
p.	O
(	O
2006	O
)	O
,	O
nearest-neighbor	O
methods	O
in	O
learning	O
and	O
vision	O
:	O
theory	O
and	O
practice	O
,	O
mit	O
press	O
.	O
shalev-shwartz	O
,	O
s.	O
(	O
2007	O
)	O
,	O
online	B
learning	I
:	O
theory	O
,	O
algorithms	O
,	O
and	O
applications	O
,	O
phd	O
thesis	O
,	O
the	O
hebrew	O
university	O
.	O
shalev-shwartz	O
,	O
s.	O
(	O
2011	O
)	O
,	O
‘	O
online	B
learning	I
and	O
online	B
convex	I
optimization	I
’	O
,	O
founda-	O
tions	O
and	O
trends	O
r	O
(	O
cid:13	O
)	O
in	O
machine	O
learning	O
4	O
(	O
2	O
)	O
,	O
107–194	O
.	O
shalev-shwartz	O
,	O
s.	O
,	O
shamir	O
,	O
o.	O
,	O
srebro	O
,	O
n.	O
&	O
sridharan	O
,	O
k.	O
(	O
2010	O
)	O
,	O
‘	O
learnability	O
,	O
stability	B
and	O
uniform	B
convergence	I
’	O
,	O
the	O
journal	O
of	O
machine	O
learning	O
research	O
9999	O
,	O
2635–2670	O
.	O
shalev-shwartz	O
,	O
s.	O
,	O
shamir	O
,	O
o	O
.	O
&	O
sridharan	O
,	O
k.	O
(	O
2010	O
)	O
,	O
learning	O
kernel-based	O
halfs-	O
paces	O
with	O
the	O
zero-one	O
loss	B
,	O
in	O
‘	O
conference	O
on	O
learning	O
theory	O
(	O
colt	O
)	O
’	O
.	O
shalev-shwartz	O
,	O
s.	O
,	O
shamir	O
,	O
o.	O
,	O
sridharan	O
,	O
k.	O
&	O
srebro	O
,	O
n.	O
(	O
2009	O
)	O
,	O
stochastic	O
convex	B
optimization	O
,	O
in	O
‘	O
conference	O
on	O
learning	O
theory	O
(	O
colt	O
)	O
’	O
.	O
shalev-shwartz	O
,	O
s.	O
&	O
singer	O
,	O
y	O
.	O
(	O
2008	O
)	O
,	O
on	O
the	O
equivalence	O
of	O
weak	O
learnability	O
and	O
linear	O
separability	O
:	O
new	O
relaxations	O
and	O
eﬃcient	O
boosting	O
algorithms	O
,	O
in	O
‘	O
proceedings	O
of	O
the	O
nineteenth	O
annual	O
conference	O
on	O
computational	O
learning	O
theory	O
’	O
.	O
shalev-shwartz	O
,	O
s.	O
,	O
singer	O
,	O
y	O
.	O
&	O
srebro	O
,	O
n.	O
(	O
2007	O
)	O
,	O
pegasos	O
:	O
primal	O
estimated	O
sub-	O
gradient	B
solver	O
for	O
svm	O
,	O
in	O
‘	O
international	O
conference	O
on	O
machine	O
learning	O
’	O
,	O
pp	O
.	O
807–814	O
.	O
shalev-shwartz	O
,	O
s.	O
&	O
srebro	O
,	O
n.	O
(	O
2008	O
)	O
,	O
svm	O
optimization	O
:	O
inverse	O
dependence	O
on	O
training	B
set	I
size	O
,	O
in	O
‘	O
international	O
conference	O
on	O
machine	O
learning	O
’	O
,	O
pp	O
.	O
928–935	O
.	O
shalev-shwartz	O
,	O
s.	O
,	O
zhang	O
,	O
t.	O
&	O
srebro	O
,	O
n.	O
(	O
2010	O
)	O
,	O
‘	O
trading	O
accuracy	B
for	O
sparsity	O
in	O
optimization	O
problems	O
with	O
sparsity	O
constraints	O
’	O
,	O
siam	O
journal	O
on	O
optimization	O
20	O
,	O
2807–2832	O
.	O
shamir	O
,	O
o	O
.	O
&	O
zhang	O
,	O
t.	O
(	O
2013	O
)	O
,	O
stochastic	O
gradient	B
descent	I
for	O
non-smooth	O
optimiza-	O
tion	O
:	O
convergence	O
results	O
and	O
optimal	O
averaging	O
schemes	O
,	O
in	O
‘	O
international	O
confer-	O
ence	O
on	O
machine	O
learning	O
(	O
icml	O
)	O
’	O
.	O
shapiro	O
,	O
a.	O
,	O
dentcheva	O
,	O
d.	O
&	O
ruszczy´nski	O
,	O
a	O
.	O
(	O
2009	O
)	O
,	O
lectures	O
on	O
stochastic	O
program-	O
ming	O
:	O
modeling	O
and	O
theory	O
,	O
vol	O
.	O
9	O
,	O
society	O
for	O
industrial	O
and	O
applied	O
mathematics	O
.	O
references	O
445	O
shelah	O
,	O
s.	O
(	O
1972	O
)	O
,	O
‘	O
a	O
combinatorial	O
problem	O
;	O
stability	B
and	O
order	O
for	O
models	O
and	O
theories	O
in	O
inﬁnitary	O
languages	O
’	O
,	O
pac	O
.	O
j.	O
math	O
4	O
,	O
247–261	O
.	O
sipser	O
,	O
m.	O
(	O
2006	O
)	O
,	O
introduction	O
to	O
the	O
theory	O
of	O
computation	O
,	O
thomson	O
course	O
tech-	O
nology	O
.	O
slud	O
,	O
e.	O
v.	O
(	O
1977	O
)	O
,	O
‘	O
distribution	O
inequalities	O
for	O
the	O
binomial	O
law	O
’	O
,	O
the	O
annals	O
of	O
probability	O
5	O
(	O
3	O
)	O
,	O
404–412	O
.	O
steinwart	O
,	O
i	O
.	O
&	O
christmann	O
,	O
a	O
.	O
(	O
2008	O
)	O
,	O
support	O
vector	O
machines	O
,	O
springerverlag	O
new	O
york	O
.	O
stone	O
,	O
c.	O
(	O
1977	O
)	O
,	O
‘	O
consistent	O
nonparametric	O
regression	B
’	O
,	O
the	O
annals	O
of	O
statistics	O
5	O
(	O
4	O
)	O
,	O
595–620	O
.	O
taskar	O
,	O
b.	O
,	O
guestrin	O
,	O
c.	O
&	O
koller	O
,	O
d.	O
(	O
2003	O
)	O
,	O
max-margin	O
markov	O
networks	O
,	O
in	O
‘	O
nips	O
’	O
.	O
tibshirani	O
,	O
r.	O
(	O
1996	O
)	O
,	O
‘	O
regression	B
shrinkage	O
and	O
selection	O
via	O
the	O
lasso	O
’	O
,	O
j.	O
royal	O
.	O
statist	O
.	O
soc	O
b	O
.	O
58	O
(	O
1	O
)	O
,	O
267–288	O
.	O
tikhonov	O
,	O
a.	O
n.	O
(	O
1943	O
)	O
,	O
‘	O
on	O
the	O
stability	B
of	O
inverse	O
problems	O
’	O
,	O
dolk	O
.	O
akad	O
.	O
nauk	O
sssr	O
39	O
(	O
5	O
)	O
,	O
195–198	O
.	O
tishby	O
,	O
n.	O
,	O
pereira	O
,	O
f.	O
&	O
bialek	O
,	O
w.	O
(	O
1999	O
)	O
,	O
the	O
information	B
bottleneck	I
method	O
,	O
in	O
‘	O
the	O
37	O
’	O
th	O
allerton	O
conference	O
on	O
communication	O
,	O
control	O
,	O
and	O
computing	O
’	O
.	O
tsochantaridis	O
,	O
i.	O
,	O
hofmann	O
,	O
t.	O
,	O
joachims	O
,	O
t.	O
&	O
altun	O
,	O
y	O
.	O
(	O
2004	O
)	O
,	O
support	O
vector	O
machine	O
learning	O
for	O
interdependent	O
and	O
structured	O
output	O
spaces	O
,	O
in	O
‘	O
proceedings	O
of	O
the	O
twenty-first	O
international	O
conference	O
on	O
machine	O
learning	O
’	O
.	O
valiant	O
,	O
l.	O
g.	O
(	O
1984	O
)	O
,	O
‘	O
a	O
theory	O
of	O
the	O
learnable	O
’	O
,	O
communications	O
of	O
the	O
acm	O
27	O
(	O
11	O
)	O
,	O
1134–1142	O
.	O
vapnik	O
,	O
v.	O
(	O
1992	O
)	O
,	O
principles	O
of	O
risk	B
minimization	O
for	O
learning	O
theory	O
,	O
in	O
j.	O
e.	O
moody	O
,	O
s.	O
j.	O
hanson	O
&	O
r.	O
p.	O
lippmann	O
,	O
eds	O
,	O
‘	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
4	O
’	O
,	O
morgan	O
kaufmann	O
,	O
pp	O
.	O
831–838	O
.	O
vapnik	O
,	O
v.	O
(	O
1995	O
)	O
,	O
the	O
nature	O
of	O
statistical	O
learning	O
theory	O
,	O
springer	O
.	O
vapnik	O
,	O
v.	O
n.	O
(	O
1982	O
)	O
,	O
estimation	O
of	O
dependences	O
based	O
on	O
empirical	O
data	O
,	O
springer-	O
verlag	O
.	O
vapnik	O
,	O
v.	O
n.	O
(	O
1998	O
)	O
,	O
statistical	O
learning	O
theory	O
,	O
wiley	O
.	O
vapnik	O
,	O
v.	O
n.	O
&	O
chervonenkis	O
,	O
a.	O
y	O
.	O
(	O
1971	O
)	O
,	O
‘	O
on	O
the	O
uniform	B
convergence	I
of	O
relative	O
frequencies	O
of	O
events	O
to	O
their	O
probabilities	O
’	O
,	O
theory	O
of	O
probability	O
and	O
its	O
applications	O
xvi	O
(	O
2	O
)	O
,	O
264–280	O
.	O
vapnik	O
,	O
v.	O
n.	O
&	O
chervonenkis	O
,	O
a.	O
y	O
.	O
(	O
1974	O
)	O
,	O
theory	O
of	O
pattern	O
recognition	O
,	O
nauka	O
,	O
moscow	O
.	O
(	O
in	O
russian	O
)	O
.	O
von	O
luxburg	O
,	O
u	O
.	O
(	O
2007	O
)	O
,	O
‘	O
a	O
tutorial	O
on	O
spectral	B
clustering	I
’	O
,	O
statistics	O
and	O
computing	O
17	O
(	O
4	O
)	O
,	O
395–416	O
.	O
von	O
neumann	O
,	O
j	O
.	O
(	O
1928	O
)	O
,	O
‘	O
zur	O
theorie	O
der	O
gesellschaftsspiele	O
(	O
on	O
the	O
theory	O
of	O
parlor	O
games	O
)	O
’	O
,	O
math	O
.	O
ann	O
.	O
100	O
,	O
295—320	O
.	O
von	O
neumann	O
,	O
j	O
.	O
(	O
1953	O
)	O
,	O
‘	O
a	O
certain	O
zero-sum	O
two-person	O
game	O
equivalent	O
to	O
the	O
opti-	O
mal	O
assignment	O
problem	O
’	O
,	O
contributions	O
to	O
the	O
theory	O
of	O
games	O
2	O
,	O
5–12	O
.	O
vovk	O
,	O
v.	O
g.	O
(	O
1990	O
)	O
,	O
aggregating	O
strategies	O
,	O
in	O
‘	O
conference	O
on	O
learning	O
theory	O
(	O
colt	O
)	O
’	O
,	O
pp	O
.	O
371–383	O
.	O
warmuth	O
,	O
m.	O
,	O
glocer	O
,	O
k.	O
&	O
vishwanathan	O
,	O
s.	O
(	O
2008	O
)	O
,	O
entropy	B
regularized	O
lpboost	O
,	O
in	O
‘	O
algorithmic	O
learning	O
theory	O
(	O
alt	O
)	O
’	O
.	O
warmuth	O
,	O
m.	O
,	O
liao	O
,	O
j	O
.	O
&	O
ratsch	O
,	O
g.	O
(	O
2006	O
)	O
,	O
totally	O
corrective	O
boosting	B
algorithms	O
that	O
maximize	O
the	O
margin	B
,	O
in	O
‘	O
proceedings	O
of	O
the	O
23rd	O
international	O
conference	O
on	O
machine	O
learning	O
’	O
.	O
446	O
references	O
weston	O
,	O
j.	O
,	O
chapelle	O
,	O
o.	O
,	O
vapnik	O
,	O
v.	O
,	O
elisseeﬀ	O
,	O
a	O
.	O
&	O
sch¨olkopf	O
,	O
b	O
.	O
(	O
2002	O
)	O
,	O
kernel	O
depen-	O
dency	O
estimation	O
,	O
in	O
‘	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
’	O
,	O
pp	O
.	O
873–	O
880.	O
weston	O
,	O
j	O
.	O
&	O
watkins	O
,	O
c.	O
(	O
1999	O
)	O
,	O
support	O
vector	O
machines	O
for	O
multi-class	O
pattern	O
recognition	O
,	O
in	O
‘	O
proceedings	O
of	O
the	O
seventh	O
european	O
symposium	O
on	O
artiﬁcial	O
neural	B
networks	I
’	O
.	O
wolpert	O
,	O
d.	O
h.	O
&	O
macready	O
,	O
w.	O
g.	O
(	O
1997	O
)	O
,	O
‘	O
no	O
free	O
lunch	O
theorems	O
for	O
optimization	O
’	O
,	O
evolutionary	O
computation	O
,	O
ieee	O
transactions	O
on	O
1	O
(	O
1	O
)	O
,	O
67–82	O
.	O
zhang	O
,	O
t.	O
(	O
2004	O
)	O
,	O
solving	O
large	O
scale	O
linear	O
prediction	O
problems	O
using	O
stochastic	O
gradi-	O
ent	O
descent	O
algorithms	O
,	O
in	O
‘	O
proceedings	O
of	O
the	O
twenty-first	O
international	O
conference	O
on	O
machine	O
learning	O
’	O
.	O
zhao	O
,	O
p.	O
&	O
yu	O
,	O
b	O
.	O
(	O
2006	O
)	O
,	O
‘	O
on	O
model	B
selection	I
consistency	O
of	O
lasso	O
’	O
,	O
journal	O
of	O
machine	O
learning	O
research	O
7	O
,	O
2541–2567	O
.	O
zinkevich	O
,	O
m.	O
(	O
2003	O
)	O
,	O
online	O
convex	O
programming	O
and	O
generalized	O
inﬁnitesimal	O
gradient	B
ascent	O
,	O
in	O
‘	O
international	O
conference	O
on	O
machine	O
learning	O
’	O
.	O