reinforcement	B
learning	I
:	O
preface	O
to	O
the	O
second	O
edition	O
the	O
twenty	O
years	O
since	O
the	O
publication	O
of	O
the	O
ﬁrst	O
edition	O
of	O
this	O
book	O
have	O
seen	O
tremen-	O
dous	O
progress	O
in	O
artiﬁcial	O
intelligence	O
,	O
propelled	O
in	O
large	O
part	O
by	O
advances	O
in	O
machine	O
learning	O
,	O
including	O
advances	O
in	O
reinforcement	O
learning	O
.	O
although	O
the	O
impressive	O
com-	O
putational	O
power	O
that	O
became	O
available	O
is	O
responsible	O
for	O
some	O
of	O
these	O
advances	O
,	O
new	O
developments	O
in	O
theory	O
and	O
algorithms	O
have	O
been	O
driving	O
forces	O
as	O
well	O
.	O
in	O
the	O
face	O
of	O
this	O
progress	O
,	O
a	O
second	O
edition	O
of	O
our	O
1998	O
book	O
was	O
long	O
overdue	O
,	O
and	O
we	O
ﬁnally	O
began	O
the	O
project	O
in	O
2013.	O
our	O
goal	B
for	O
the	O
second	O
edition	O
was	O
the	O
same	O
as	O
our	O
goal	B
for	O
the	O
ﬁrst	O
:	O
to	O
provide	O
a	O
clear	O
and	O
simple	O
account	O
of	O
the	O
key	O
ideas	O
and	O
algorithms	O
of	O
reinforcement	O
learning	O
that	O
is	O
accessible	O
to	O
readers	O
in	O
all	O
the	O
related	O
disciplines	O
.	O
the	O
edition	O
remains	O
an	O
introduction	O
,	O
and	O
we	O
retain	O
a	O
focus	O
on	O
core	O
,	O
online	B
learning	O
algorithms	O
.	O
this	O
edition	O
includes	O
some	O
new	O
topics	O
that	O
rose	O
to	O
importance	O
over	O
the	O
intervening	O
years	O
,	O
and	O
we	O
expanded	O
coverage	O
of	O
topics	O
that	O
we	O
now	O
understand	O
better	O
.	O
but	O
we	O
made	O
no	O
attempt	O
to	O
provide	O
comprehensive	O
coverage	O
of	O
the	O
ﬁeld	O
,	O
which	O
has	O
exploded	O
in	O
many	O
diﬀerent	O
directions	O
with	O
outstanding	O
contributions	O
by	O
many	O
active	O
researchers	O
.	O
we	O
apologize	O
for	O
having	O
to	O
leave	O
out	O
all	O
but	O
a	O
handful	O
of	O
these	O
contributions	O
.	O
as	O
in	O
the	O
ﬁrst	O
edition	O
,	O
we	O
chose	O
not	O
to	O
produce	O
a	O
rigorous	O
formal	O
treatment	O
of	O
re-	O
inforcement	O
learning	O
,	O
or	O
to	O
formulate	O
it	O
in	O
the	O
most	O
general	O
terms	O
.	O
however	O
,	O
since	O
the	O
ﬁrst	O
edition	O
,	O
our	O
deeper	O
understanding	O
of	O
some	O
topics	O
required	O
a	O
bit	O
more	O
mathematics	O
to	O
explain	O
;	O
we	O
have	O
set	O
oﬀ	O
the	O
more	O
mathematical	O
parts	O
in	O
shaded	O
boxes	O
that	O
the	O
non-	O
mathematically-inclined	O
may	O
choose	O
to	O
skip	O
.	O
we	O
also	O
use	O
a	O
slightly	O
diﬀerent	O
notation	O
than	O
was	O
used	O
in	O
the	O
ﬁrst	O
edition	O
.	O
in	O
teaching	O
,	O
we	O
have	O
found	O
that	O
the	O
new	O
notation	O
helps	O
to	O
address	O
some	O
common	O
points	O
of	O
confusion	O
.	O
it	O
emphasizes	O
the	O
diﬀerence	O
between	O
random	O
variables	O
,	O
denoted	O
with	O
capital	O
letters	O
,	O
and	O
their	O
instantiations	O
,	O
denoted	O
in	O
lower	O
case	O
.	O
for	O
example	O
,	O
the	O
state	B
,	O
action	B
,	O
and	O
reward	O
at	O
time	O
step	O
t	O
are	O
denoted	O
st	O
,	O
at	O
,	O
and	O
rt	O
,	O
while	O
their	O
possible	O
values	O
might	O
be	O
denoted	O
s	O
,	O
a	O
,	O
and	O
r.	O
along	O
with	O
this	O
,	O
it	O
is	O
nat-	O
ural	O
to	O
use	O
lower	O
case	O
for	O
value	O
functions	O
(	O
e.g.	O
,	O
vπ	O
)	O
and	O
restrict	O
capitals	O
to	O
their	O
tabular	O
estimates	O
(	O
e.g.	O
,	O
qt	O
(	O
s	O
,	O
a	O
)	O
)	O
.	O
approximate	B
value	O
functions	O
are	O
deterministic	O
functions	O
of	O
random	O
parameters	O
and	O
are	O
thus	O
also	O
in	O
lower	O
case	O
(	O
e.g.	O
,	O
ˆv	O
(	O
s	O
,	O
wt	O
)	O
≈	O
vπ	O
(	O
s	O
)	O
)	O
.	O
vectors	O
,	O
such	O
as	O
the	O
weight	O
vector	B
wt	O
(	O
formerly	O
θt	O
)	O
and	O
the	O
feature	O
vector	B
xt	O
(	O
formerly	O
φt	O
)	O
,	O
are	O
bold	O
and	O
written	O
in	O
lowercase	O
even	O
if	O
they	O
are	O
random	O
variables	O
.	O
uppercase	O
bold	O
is	O
reserved	O
for	O
matrices	O
.	O
in	O
the	O
ﬁrst	O
edition	O
we	O
used	O
special	O
notations	O
,	O
pa	O
ss	O
(	O
cid:48	O
)	O
,	O
for	O
the	O
tran-	O
sition	O
probabilities	O
and	O
expected	O
rewards	O
.	O
one	O
weakness	O
of	O
that	O
notation	O
is	O
that	O
it	O
still	O
did	O
not	O
fully	O
characterize	O
the	O
dynamics	O
of	O
the	O
rewards	O
,	O
giving	O
only	O
their	O
expectations	O
,	O
ss	O
(	O
cid:48	O
)	O
and	O
ra	O
ix	O
x	O
preface	O
to	O
the	O
second	O
edition	O
which	O
is	O
suﬃcient	O
for	B
dynamic	I
programming	I
but	O
not	O
for	O
reinforcement	O
learning	O
.	O
another	O
weakness	O
is	O
the	O
excess	O
of	O
subscripts	O
and	O
superscripts	O
.	O
in	O
this	O
edition	O
we	O
use	O
the	O
explicit	O
notation	O
of	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
for	O
the	O
joint	O
probability	O
for	O
the	O
next	O
state	B
and	O
reward	O
given	O
the	O
current	O
state	B
and	O
action	B
.	O
all	O
the	O
changes	O
in	O
notation	O
are	O
summarized	O
in	O
a	O
table	O
on	O
page	O
xvii	O
.	O
the	O
second	O
edition	O
is	O
signiﬁcantly	O
expanded	O
,	O
and	O
its	O
top-level	O
organization	O
has	O
been	O
revamped	O
.	O
after	O
the	O
introductory	O
ﬁrst	O
chapter	O
,	O
the	O
second	O
edition	O
is	O
divided	O
into	O
three	O
new	O
parts	O
.	O
the	O
ﬁrst	O
part	O
(	O
chapters	O
2–8	O
)	O
treats	O
as	O
much	O
of	O
reinforcement	O
learning	O
as	O
possible	O
without	O
going	O
beyond	O
the	O
tabular	O
case	O
for	O
which	O
exact	O
solutions	O
can	O
be	O
found	O
.	O
we	O
cover	O
both	O
learning	O
and	O
planning	B
methods	O
for	O
the	O
tabular	O
case	O
,	O
as	O
well	O
as	O
their	O
uniﬁ-	O
cation	O
in	O
n-step	O
methods	O
and	O
in	O
dyna	O
.	O
many	O
algorithms	O
presented	O
in	O
this	O
part	O
are	O
new	O
to	O
the	O
second	O
edition	O
,	O
including	O
ucb	O
,	O
expected	O
sarsa	O
,	O
double	B
learning	I
,	O
tree-backup	O
,	O
q	O
(	O
σ	O
)	O
,	O
rtdp	O
,	O
and	O
mcts	O
.	O
doing	O
the	O
tabular	O
case	O
ﬁrst	O
,	O
and	O
thoroughly	O
,	O
enables	O
core	O
ideas	O
to	O
be	O
developed	O
in	O
the	O
simplest	O
possible	O
setting	O
.	O
the	O
whole	O
second	O
part	O
of	O
the	O
book	O
(	O
chapters	O
9–13	O
)	O
is	O
then	O
devoted	O
to	O
extending	O
the	O
ideas	O
to	O
function	B
approximation	I
.	O
it	O
has	O
new	O
sections	O
on	O
artiﬁcial	B
neural	I
networks	I
,	O
the	O
fourier	O
basis	O
,	O
lstd	O
,	O
kernel-based	O
methods	O
,	O
gradient-td	O
and	O
emphatic-td	O
methods	O
,	O
average-reward	O
methods	O
,	O
true	O
on-	O
line	O
td	O
(	O
λ	O
)	O
,	O
and	O
policy-gradient	O
methods	O
.	O
the	O
second	O
edition	O
signiﬁcantly	O
expands	O
the	O
treatment	O
of	O
oﬀ-policy	O
learning	O
,	O
ﬁrst	O
for	O
the	O
tabular	O
case	O
in	O
chapters	O
5–7	O
,	O
then	O
with	B
function	I
approximation	I
in	O
chapters	O
11	O
and	O
12.	O
another	O
change	O
is	O
that	O
the	O
second	O
edi-	O
tion	B
separates	O
the	O
forward-view	O
idea	O
of	O
n-step	O
bootstrapping	B
(	O
now	O
treated	O
more	O
fully	O
in	O
chapter	O
7	O
)	O
from	O
the	O
backward-view	O
idea	O
of	O
eligibility	O
traces	O
(	O
now	O
treated	O
independently	O
in	O
chapter	O
12	O
)	O
.	O
the	O
third	O
part	O
of	O
the	O
book	O
has	O
large	O
new	O
chapters	O
on	O
reinforcement	B
learning	I
’	O
s	O
relationships	O
to	O
psychology	B
(	O
chapter	O
14	O
)	O
and	O
neuroscience	O
(	O
chapter	O
15	O
)	O
,	O
as	O
well	O
as	O
an	O
updated	O
case-studies	O
chapter	O
including	O
atari	O
game	O
playing	O
,	O
watson	O
’	O
s	O
wagering	O
strategy	O
,	O
and	O
the	O
go	O
playing	O
programs	O
alphago	O
and	O
alphago	O
zero	O
(	O
chapter	O
16	O
)	O
.	O
still	O
,	O
out	O
of	O
necessity	O
we	O
have	O
included	O
only	O
a	O
small	O
subset	O
of	O
all	O
that	O
has	O
been	O
done	O
in	O
the	O
ﬁeld	O
.	O
our	O
choices	O
reﬂect	O
our	O
long-standing	O
interests	O
in	O
inexpensive	O
model-free	O
methods	O
that	O
should	O
scale	O
well	O
to	O
large	O
applications	O
.	O
the	O
ﬁnal	O
chapter	O
now	O
includes	O
a	O
discussion	O
of	O
the	O
future	O
societal	O
impacts	O
of	O
reinforcement	O
learning	O
.	O
for	O
better	O
or	O
worse	O
,	O
the	O
second	O
edition	O
is	O
about	O
twice	O
as	O
large	O
as	O
the	O
ﬁrst	O
.	O
this	O
book	O
is	O
designed	O
to	O
be	O
used	O
as	O
the	O
primary	O
text	O
for	O
a	O
one-	O
or	O
two-semester	O
course	O
on	O
reinforcement	B
learning	I
.	O
for	O
a	O
one-semester	O
course	O
,	O
the	O
ﬁrst	O
ten	O
chapters	O
should	O
be	O
covered	O
in	O
order	O
and	O
form	O
a	O
good	O
core	O
,	O
to	O
which	O
can	O
be	O
added	O
material	O
from	O
the	O
other	O
chapters	O
,	O
from	O
other	O
books	O
such	O
as	O
bertsekas	O
and	O
tsitsiklis	O
(	O
1996	O
)	O
,	O
wiering	O
and	O
van	O
otterlo	O
(	O
2012	O
)	O
,	O
and	O
szepesv´ari	O
(	O
2010	O
)	O
,	O
or	O
from	O
the	O
literature	O
,	O
according	O
to	O
taste	O
.	O
depending	O
of	O
the	O
students	O
’	O
background	O
,	O
some	O
additional	O
material	O
on	O
online	B
supervised	O
learning	O
may	O
be	O
helpful	O
.	O
the	O
ideas	O
of	O
options	O
and	O
option	O
models	O
are	O
a	O
natural	O
addition	O
(	O
sutton	O
,	O
precup	O
and	O
singh	O
,	O
1999	O
)	O
.	O
a	O
two-semester	O
course	O
can	O
cover	O
all	O
the	O
chapters	O
as	O
well	O
as	O
supplementary	O
material	O
.	O
the	O
book	O
can	O
also	O
be	O
used	O
as	O
part	O
of	O
broader	O
courses	O
on	O
machine	O
learning	O
,	O
artiﬁcial	B
intelligence	I
,	O
or	O
neural	B
networks	I
.	O
in	O
this	O
case	O
,	O
it	O
may	O
be	O
desirable	O
to	O
cover	O
only	O
a	O
subset	O
of	O
the	O
material	O
.	O
we	O
recommend	O
covering	O
chapter	O
1	O
for	O
a	O
brief	O
overview	O
,	O
chapter	O
2	O
through	O
section	O
2.4	O
,	O
chapter	O
3	O
,	O
and	O
then	O
selecting	O
sections	O
from	O
the	O
remaining	O
chapters	O
according	O
to	O
time	O
and	O
interests	O
.	O
chapter	O
6	O
is	O
the	O
most	O
important	O
for	O
the	O
subject	O
and	O
for	O
the	O
rest	O
of	O
the	O
book	O
.	O
a	O
course	O
focusing	O
on	O
machine	O
preface	O
to	O
the	O
second	O
edition	O
xi	O
learning	O
or	O
neural	B
networks	I
should	O
cover	O
chapters	O
9	O
and	O
10	O
,	O
and	O
a	O
course	O
focusing	O
on	O
artiﬁcial	B
intelligence	I
or	O
planning	B
should	O
cover	O
chapter	O
8.	O
throughout	O
the	O
book	O
,	O
sections	O
and	O
chapters	O
that	O
are	O
more	O
diﬃcult	O
and	O
not	O
essential	O
to	O
the	O
rest	O
of	O
the	O
book	O
are	O
marked	O
with	O
a	O
∗	O
.	O
these	O
can	O
be	O
omitted	O
on	O
ﬁrst	O
reading	O
without	O
creating	O
problems	O
later	O
on	O
.	O
some	O
exercises	O
are	O
also	O
marked	O
with	O
a	O
∗	O
to	O
indicate	O
that	O
they	O
are	O
more	O
advanced	O
and	O
not	O
essential	O
to	O
understanding	O
the	O
basic	O
material	O
of	O
the	O
chapter	O
.	O
most	O
chapters	O
end	O
with	O
a	O
section	O
entitled	O
“	O
bibliographical	O
and	O
historical	O
remarks	O
,	O
”	O
wherein	O
we	O
credit	O
the	O
sources	O
of	O
the	O
ideas	O
presented	O
in	O
that	O
chapter	O
,	O
provide	O
pointers	O
to	O
further	O
reading	O
and	O
ongoing	O
research	O
,	O
and	O
describe	O
relevant	O
historical	O
background	O
.	O
despite	O
our	O
attempts	O
to	O
make	O
these	O
sections	O
authoritative	O
and	O
complete	O
,	O
we	O
have	O
un-	O
doubtedly	O
left	O
out	O
some	O
important	O
prior	O
work	O
.	O
for	O
that	O
we	O
again	O
apologize	O
,	O
and	O
we	O
welcome	O
corrections	O
and	O
extensions	O
for	O
incorporation	O
into	O
the	O
electronic	O
version	O
of	O
the	O
book	O
.	O
like	O
the	O
ﬁrst	O
edition	O
,	O
this	O
edition	O
of	O
the	O
book	O
is	O
dedicated	O
to	O
the	O
memory	O
of	O
a.	O
harry	O
klopf	O
.	O
it	O
was	O
harry	O
who	O
introduced	O
us	O
to	O
each	O
other	O
,	O
and	O
it	O
was	O
his	O
ideas	O
about	O
the	O
brain	O
and	B
artiﬁcial	I
intelligence	I
that	O
launched	O
our	O
long	O
excursion	O
into	O
reinforcement	B
learning	I
.	O
trained	O
in	O
neurophysiology	O
and	O
long	O
interested	O
in	O
machine	O
intelligence	O
,	O
harry	O
was	O
a	O
se-	O
nior	O
scientist	O
aﬃliated	O
with	O
the	O
avionics	O
directorate	O
of	O
the	O
air	O
force	O
oﬃce	O
of	O
scientiﬁc	O
research	O
(	O
afosr	O
)	O
at	O
wright-patterson	O
air	O
force	O
base	O
,	O
ohio	O
.	O
he	O
was	O
dissatisﬁed	O
with	O
the	O
great	O
importance	O
attributed	O
to	O
equilibrium-seeking	O
processes	O
,	O
including	O
homeostasis	O
and	O
error-correcting	O
pattern	O
classiﬁcation	O
methods	O
,	O
in	O
explaining	O
natural	O
intelligence	O
and	O
in	O
providing	O
a	O
basis	O
for	O
machine	O
intelligence	O
.	O
he	O
noted	O
that	O
systems	O
that	O
try	O
to	O
maximize	O
something	O
(	O
whatever	O
that	O
might	O
be	O
)	O
are	O
qualitatively	O
diﬀerent	O
from	O
equilibrium-seeking	O
systems	O
,	O
and	O
he	O
argued	O
that	O
maximizing	O
systems	O
hold	O
the	O
key	O
to	O
understanding	O
im-	O
portant	O
aspects	O
of	O
natural	O
intelligence	O
and	O
for	O
building	O
artiﬁcial	O
intelligences	O
.	O
harry	O
was	O
instrumental	O
in	O
obtaining	O
funding	O
from	O
afosr	O
for	O
a	O
project	O
to	O
assess	O
the	O
scientiﬁc	O
merit	O
of	O
these	O
and	O
related	O
ideas	O
.	O
this	O
project	O
was	O
conducted	O
in	O
the	O
late	O
1970s	O
at	O
the	O
university	O
of	O
massachusetts	O
amherst	O
(	O
umass	O
amherst	O
)	O
,	O
initially	O
under	O
the	O
direction	O
of	O
michael	O
arbib	O
,	O
william	O
kilmer	O
,	O
and	O
nico	O
spinelli	O
,	O
professors	O
in	O
the	O
department	O
of	O
computer	O
and	O
information	O
science	O
at	O
umass	O
amherst	O
,	O
and	O
founding	O
members	O
of	O
the	O
cy-	O
bernetics	O
center	O
for	O
systems	O
neuroscience	B
at	O
the	O
university	O
,	O
a	O
farsighted	O
group	O
focusing	O
on	O
the	O
intersection	O
of	O
neuroscience	O
and	B
artiﬁcial	I
intelligence	I
.	O
barto	O
,	O
a	O
recent	O
ph.d.	O
from	O
the	O
university	O
of	O
michigan	O
,	O
was	O
hired	O
as	O
post	O
doctoral	O
researcher	O
on	O
the	O
project	O
.	O
mean-	O
while	O
,	O
sutton	O
,	O
an	O
undergraduate	O
studying	O
computer	O
science	O
and	O
psychology	O
at	O
stanford	O
,	O
had	O
been	O
corresponding	O
with	O
harry	O
regarding	O
their	O
mutual	O
interest	O
in	O
the	O
role	O
of	O
stim-	O
ulus	O
timing	O
in	O
classical	O
conditioning	B
.	O
harry	O
suggested	O
to	O
the	O
umass	O
group	O
that	O
sutton	O
would	O
be	O
a	O
great	O
addition	O
to	O
the	O
project	O
.	O
thus	O
,	O
sutton	O
became	O
a	O
umass	O
graduate	O
stu-	O
dent	O
,	O
whose	O
ph.d.	O
was	O
directed	O
by	O
barto	O
,	O
who	O
had	O
become	O
an	O
associate	O
professor	O
.	O
the	O
study	O
of	O
reinforcement	O
learning	O
as	O
presented	O
in	O
this	O
book	O
is	O
rightfully	O
an	O
outcome	O
of	O
that	O
project	O
instigated	O
by	O
harry	O
and	O
inspired	O
by	O
his	O
ideas	O
.	O
further	O
,	O
harry	O
was	O
responsible	O
for	O
bringing	O
us	O
,	O
the	O
authors	O
,	O
together	O
in	O
what	O
has	O
been	O
a	O
long	O
and	O
enjoyable	O
interaction	O
.	O
by	O
dedicating	O
this	O
book	O
to	O
harry	O
we	O
honor	O
his	O
essential	O
contributions	O
,	O
not	O
only	O
to	O
the	O
ﬁeld	O
of	O
reinforcement	O
learning	O
,	O
but	O
also	O
to	O
our	O
collaboration	O
.	O
we	O
also	O
thank	O
professors	O
arbib	O
,	O
kilmer	O
,	O
and	O
spinelli	O
for	O
the	O
opportunity	O
they	O
provided	O
to	O
us	O
to	O
begin	O
exploring	O
these	O
ideas	O
.	O
finally	O
,	O
we	O
thank	O
afosr	O
for	O
generous	O
support	O
over	O
the	O
early	O
years	O
of	O
our	O
xii	O
preface	O
to	O
the	O
second	O
edition	O
research	O
,	O
and	O
the	O
nsf	O
for	O
its	O
generous	O
support	O
over	O
many	O
of	O
the	O
following	O
years	O
.	O
we	O
have	O
very	O
many	O
people	O
to	O
thank	O
for	O
their	O
inspiration	O
and	O
help	O
with	O
this	O
second	O
edition	O
.	O
everyone	O
we	O
acknowledged	O
for	O
their	O
inspiration	O
and	O
help	O
with	O
the	O
ﬁrst	O
edition	O
deserve	O
our	O
deepest	O
gratitude	O
for	O
this	O
edition	O
as	O
well	O
,	O
which	O
would	O
not	O
exist	O
were	O
it	O
not	O
for	O
their	O
contributions	O
to	O
edition	O
number	O
one	O
.	O
to	O
that	O
long	O
list	O
we	O
must	O
add	O
many	O
others	O
who	O
contributed	O
speciﬁcally	O
to	O
the	O
second	O
edition	O
.	O
our	O
students	O
over	O
the	O
many	O
years	O
that	O
we	O
have	O
taught	O
this	O
material	O
contributed	O
in	O
countless	O
ways	O
:	O
exposing	O
errors	O
,	O
oﬀer-	O
ing	B
ﬁxes	O
,	O
and—not	O
the	O
least—being	O
confused	O
in	O
places	O
where	O
we	O
could	O
have	O
explained	O
things	O
better	O
.	O
we	O
especially	O
thank	O
martha	O
steenstrup	O
for	O
reading	O
and	O
providing	O
de-	O
tailed	O
comments	O
throughout	O
.	O
the	O
chapters	O
on	O
psychology	B
and	O
neuroscience	B
could	O
not	O
have	O
been	O
written	O
without	O
the	O
help	O
of	O
many	O
experts	O
in	O
those	O
ﬁelds	O
.	O
we	O
thank	O
john	O
moore	O
for	O
his	O
patient	O
tutoring	O
over	O
many	O
many	O
years	O
on	O
animal	O
learning	O
experiments	O
,	O
theory	O
,	O
and	O
neuroscience	O
,	O
and	O
for	O
his	O
careful	O
reading	O
of	O
multiple	O
drafts	O
of	O
chapters	O
14	O
and	O
15.	O
we	O
also	O
thank	O
matt	O
botvinick	O
,	O
nathaniel	O
daw	O
,	O
peter	O
dayan	O
,	O
and	O
yael	O
niv	O
for	O
their	O
penetrating	O
comments	O
on	O
drafts	O
of	O
these	O
chapter	O
,	O
their	O
essential	O
guidance	O
through	O
the	O
massive	O
literature	O
,	O
and	O
their	O
interception	O
of	O
many	O
of	O
our	O
errors	O
in	O
early	O
drafts	O
.	O
of	O
course	O
,	O
the	O
remaining	O
errors	O
in	O
these	O
chapters—and	O
there	O
must	O
still	O
be	O
some—are	O
to-	O
tally	O
our	O
own	O
.	O
we	O
thank	O
phil	O
thomas	O
for	O
helping	O
us	O
make	O
these	O
chapters	O
accessible	O
to	O
non-psychologists	O
and	O
non-neuroscientists	O
,	O
and	O
we	O
thank	O
peter	O
sterling	O
for	O
helping	O
us	O
improve	O
the	O
exposition	O
.	O
we	O
are	O
grateful	O
to	O
jim	O
houk	O
for	O
introducing	O
us	O
to	O
the	O
subject	O
of	O
information	O
processing	O
in	O
the	O
basal	O
ganglia	O
and	O
for	O
alerting	O
us	O
to	O
other	O
relevant	O
aspects	O
of	O
neuroscience	O
.	O
jos´e	O
mart´ınez	O
,	O
terry	O
sejnowski	O
,	O
david	O
silver	O
,	O
gerry	O
tesauro	O
,	O
geor-	O
gios	O
theocharous	O
,	O
and	O
phil	O
thomas	O
generously	O
helped	O
us	O
understand	O
details	O
of	O
their	O
reinforcement	B
learning	I
applications	O
for	O
inclusion	O
in	O
the	O
case-studies	O
chapter	O
,	O
and	O
they	O
provided	O
helpful	O
comments	O
on	O
drafts	O
of	O
these	O
sections	O
.	O
special	O
thanks	O
are	O
owed	O
to	O
david	O
silver	O
for	O
helping	O
us	O
better	O
understand	O
monte	O
carlo	O
tree	O
search	O
and	O
the	O
deepmind	O
go-playing	O
programs	O
.	O
we	O
thank	O
george	O
konidaris	O
for	O
his	O
help	O
with	O
the	O
section	O
on	O
the	O
fourier	O
basis	O
.	O
emilio	O
cartoni	O
,	O
thomas	O
cederborg	O
,	O
stefan	O
dernbach	O
,	O
clemens	O
rosen-	O
baum	O
,	O
patrick	O
taylor	O
,	O
and	O
pierre-luc	O
bacon	O
helped	O
us	O
in	O
a	O
number	O
important	O
ways	O
for	O
which	O
we	O
are	O
most	O
grateful	O
.	O
sutton	O
would	O
also	O
like	O
to	O
thank	O
the	O
members	O
of	O
the	O
reinforcement	B
learning	I
and	O
artiﬁcial	B
intelligence	I
laboratory	O
at	O
the	O
university	O
of	O
alberta	O
for	O
contributions	O
to	O
the	O
second	O
edition	O
.	O
he	O
owes	O
a	O
particular	O
debt	O
to	O
rupam	O
mahmood	O
for	O
essential	O
contributions	O
to	O
the	O
treatment	O
of	O
oﬀ-policy	O
monte	O
carlo	O
methods	O
in	O
chapter	O
5	O
,	O
to	O
hamid	O
maei	O
for	O
helping	O
develop	O
the	O
perspective	O
on	O
oﬀ-policy	B
learning	O
presented	O
in	O
chapter	O
11	O
,	O
to	O
eric	O
graves	O
for	O
conducting	O
the	O
experiments	O
in	O
chapter	O
13	O
,	O
to	O
shangtong	O
zhang	O
for	O
replicating	O
and	O
thus	O
verifying	O
almost	O
all	O
the	O
experimental	O
results	O
,	O
to	O
kris	O
de	O
asis	O
for	O
improving	O
the	O
new	O
technical	O
content	O
of	O
chapters	O
7	O
and	O
12	O
,	O
and	O
to	O
harm	O
van	O
seijen	O
for	O
insights	O
that	O
led	O
to	O
the	O
separation	O
of	O
n-step	O
methods	O
from	O
eligibility	B
traces	I
and	O
(	O
along	O
with	O
hado	O
van	O
hasselt	O
)	O
for	O
the	O
ideas	O
involving	O
exact	O
equivalence	O
of	O
forward	O
and	O
backward	O
views	O
of	O
eligibility	O
traces	O
presented	O
in	O
chapter	O
12.	O
sutton	O
also	O
gratefully	O
acknowledges	O
the	O
support	O
and	O
freedom	O
he	O
was	O
granted	O
by	O
the	O
government	O
of	O
alberta	O
and	O
the	O
national	O
science	O
and	O
engineering	O
research	O
council	O
of	O
canada	O
throughout	O
the	O
period	O
during	O
which	O
the	O
second	O
edition	O
was	O
conceived	O
and	O
written	O
.	O
in	O
particular	O
,	O
he	O
would	O
like	O
to	O
thank	O
randy	O
goebel	O
for	O
creating	O
a	O
supportive	O
and	O
far-sighted	O
environment	B
for	O
research	O
in	O
alberta	O
.	O
he	O
preface	O
to	O
the	O
second	O
edition	O
xiii	O
would	O
also	O
like	O
to	O
thank	O
deepmind	O
their	O
support	O
in	O
the	O
last	O
six	O
months	O
of	O
writing	O
the	O
book	O
.	O
finally	O
,	O
we	O
owe	O
thanks	O
to	O
the	O
many	O
careful	O
readers	O
of	O
drafts	O
of	O
the	O
second	O
edition	O
that	O
we	O
posted	O
on	O
the	O
internet	O
.	O
they	O
found	O
many	O
errors	O
that	O
we	O
had	O
missed	O
and	O
alerted	O
us	O
to	O
potential	O
points	O
of	O
confusion	O
.	O
preface	O
to	O
the	O
first	O
edition	O
we	O
ﬁrst	O
came	O
to	O
focus	O
on	O
what	O
is	O
now	O
known	O
as	O
reinforcement	O
learning	O
in	O
late	O
1979.	O
we	O
were	O
both	O
at	O
the	O
university	O
of	O
massachusetts	O
,	O
working	O
on	O
one	O
of	O
the	O
earliest	O
projects	O
to	O
revive	O
the	O
idea	O
that	O
networks	O
of	O
neuronlike	O
adaptive	O
elements	O
might	O
prove	O
to	O
be	O
a	O
promising	O
approach	O
to	O
artiﬁcial	O
adaptive	O
intelligence	O
.	O
the	O
project	O
explored	O
the	O
“	O
het-	O
erostatic	O
theory	O
of	O
adaptive	O
systems	O
”	O
developed	O
by	O
a.	O
harry	O
klopf	O
.	O
harry	O
’	O
s	O
work	O
was	O
a	O
rich	O
source	O
of	O
ideas	O
,	O
and	O
we	O
were	O
permitted	O
to	O
explore	O
them	O
critically	O
and	O
compare	O
them	O
with	O
the	O
long	O
history	B
of	I
prior	O
work	O
in	O
adaptive	O
systems	O
.	O
our	O
task	O
became	O
one	O
of	O
teasing	O
the	O
ideas	O
apart	O
and	O
understanding	O
their	O
relationships	O
and	O
relative	O
importance	O
.	O
this	O
continues	O
today	O
,	O
but	O
in	O
1979	O
we	O
came	O
to	O
realize	O
that	O
perhaps	O
the	O
simplest	O
of	O
the	O
ideas	O
,	O
which	O
had	O
long	O
been	O
taken	O
for	O
granted	O
,	O
had	O
received	O
surprisingly	O
little	O
attention	O
from	O
a	O
computational	O
perspective	O
.	O
this	O
was	O
simply	O
the	O
idea	O
of	O
a	O
learning	O
system	O
that	O
wants	O
something	O
,	O
that	O
adapts	O
its	O
behavior	O
in	O
order	O
to	O
maximize	O
a	O
special	O
signal	O
from	O
its	O
environment	B
.	O
this	O
was	O
the	O
idea	O
of	O
a	O
“	O
hedonistic	O
”	O
learning	O
system	O
,	O
or	O
,	O
as	O
we	O
would	O
say	O
now	O
,	O
the	O
idea	O
of	O
reinforcement	O
learning	O
.	O
like	O
others	O
,	O
we	O
had	O
a	O
sense	O
that	O
reinforcement	B
learning	I
had	O
been	O
thoroughly	O
explored	O
in	O
the	O
early	O
days	O
of	O
cybernetics	O
and	B
artiﬁcial	I
intelligence	I
.	O
on	O
closer	O
inspection	O
,	O
though	O
,	O
we	O
found	O
that	O
it	O
had	O
been	O
explored	O
only	O
slightly	O
.	O
while	O
reinforcement	B
learning	I
had	O
clearly	O
motivated	O
some	O
of	O
the	O
earliest	O
computational	O
studies	O
of	O
learning	O
,	O
most	O
of	O
these	O
researchers	O
had	O
gone	O
on	O
to	O
other	O
things	O
,	O
such	O
as	O
pattern	O
classiﬁcation	O
,	O
supervised	O
learn-	O
ing	B
,	O
and	O
adaptive	O
control	B
,	O
or	O
they	O
had	O
abandoned	O
the	O
study	O
of	O
learning	O
altogether	O
.	O
as	O
a	O
result	O
,	O
the	O
special	O
issues	O
involved	O
in	O
learning	O
how	O
to	O
get	O
something	O
from	O
the	O
environ-	O
ment	O
received	O
relatively	O
little	O
attention	O
.	O
in	O
retrospect	O
,	O
focusing	O
on	O
this	O
idea	O
was	O
the	O
critical	O
step	O
that	O
set	O
this	O
branch	O
of	O
research	O
in	O
motion	O
.	O
little	O
progress	O
could	O
be	O
made	O
in	O
the	O
computational	O
study	O
of	O
reinforcement	O
learning	O
until	O
it	O
was	O
recognized	O
that	O
such	O
a	O
fundamental	O
idea	O
had	O
not	O
yet	O
been	O
thoroughly	O
explored	O
.	O
the	O
ﬁeld	O
has	O
come	O
a	O
long	O
way	O
since	O
then	O
,	O
evolving	O
and	O
maturing	O
in	O
several	O
direc-	O
tions	O
.	O
reinforcement	B
learning	I
has	O
gradually	O
become	O
one	O
of	O
the	O
most	O
active	O
research	O
areas	O
in	O
machine	O
learning	O
,	O
artiﬁcial	B
intelligence	I
,	O
and	O
neural	O
network	O
research	O
.	O
the	O
ﬁeld	O
has	O
developed	O
strong	O
mathematical	O
foundations	O
and	O
impressive	O
applications	O
.	O
the	O
com-	O
putational	O
study	O
of	O
reinforcement	O
learning	O
is	O
now	O
a	O
large	O
ﬁeld	O
,	O
with	O
hundreds	O
of	O
active	O
researchers	O
around	O
the	O
world	O
in	O
diverse	O
disciplines	O
such	O
as	O
psychology	O
,	O
control	B
theory	I
,	O
artiﬁcial	B
intelligence	I
,	O
and	O
neuroscience	O
.	O
particularly	O
important	O
have	O
been	O
the	O
contribu-	O
tions	O
establishing	O
and	O
developing	O
the	O
relationships	O
to	O
the	O
theory	O
of	O
optimal	O
control	O
and	O
xv	O
xvi	O
preface	O
to	O
the	O
first	O
edition	O
dynamic	B
programming	I
.	O
the	O
overall	O
problem	O
of	O
learning	O
from	O
interaction	O
to	O
achieve	O
goals	O
is	O
still	O
far	O
from	O
being	O
solved	O
,	O
but	O
our	O
understanding	O
of	O
it	O
has	O
improved	O
signiﬁcantly	O
.	O
we	O
can	O
now	O
place	O
component	O
ideas	O
,	O
such	O
as	O
temporal-diﬀerence	O
learning	O
,	O
dynamic	O
program-	O
ming	O
,	O
and	B
function	I
approximation	I
,	O
within	O
a	O
coherent	O
perspective	O
with	O
respect	O
to	O
the	O
overall	O
problem	O
.	O
our	O
goal	B
in	O
writing	O
this	O
book	O
was	O
to	O
provide	O
a	O
clear	O
and	O
simple	O
account	O
of	O
the	O
key	O
ideas	O
and	O
algorithms	O
of	O
reinforcement	O
learning	O
.	O
we	O
wanted	O
our	O
treatment	O
to	O
be	O
accessible	O
to	O
readers	O
in	O
all	O
of	O
the	O
related	O
disciplines	O
,	O
but	O
we	O
could	O
not	O
cover	O
all	O
of	O
these	O
perspectives	O
in	O
detail	O
.	O
for	O
the	O
most	O
part	O
,	O
our	O
treatment	O
takes	O
the	O
point	O
of	O
view	O
of	O
artiﬁcial	O
intelligence	O
and	O
engineering	O
.	O
coverage	O
of	O
connections	O
to	O
other	O
ﬁelds	O
we	O
leave	O
to	O
others	O
or	O
to	O
another	O
time	O
.	O
we	O
also	O
chose	O
not	O
to	O
produce	O
a	O
rigorous	O
formal	O
treatment	O
of	O
reinforcement	O
learning	O
.	O
we	O
did	O
not	O
reach	O
for	O
the	O
highest	O
possible	O
level	O
of	O
mathematical	O
abstraction	O
and	O
did	O
not	O
rely	O
on	O
a	O
theorem–proof	O
format	O
.	O
we	O
tried	O
to	O
choose	O
a	O
level	O
of	O
mathematical	O
detail	O
that	O
points	O
the	O
mathematically	O
inclined	O
in	O
the	O
right	O
directions	O
without	O
distracting	O
from	O
the	O
simplicity	O
and	O
potential	O
generality	O
of	O
the	O
underlying	O
ideas	O
.	O
[	O
three	O
paragraphs	O
elided	O
in	O
favor	O
of	O
updated	O
content	O
in	O
the	O
second	O
edition	O
.	O
]	O
in	O
some	O
sense	O
we	O
have	O
been	O
working	O
toward	O
this	O
book	O
for	O
thirty	O
years	O
,	O
and	O
we	O
have	O
lots	O
of	O
people	O
to	O
thank	O
.	O
first	O
,	O
we	O
thank	O
those	O
who	O
have	O
personally	O
helped	O
us	O
develop	O
the	O
overall	O
view	O
presented	O
in	O
this	O
book	O
:	O
harry	O
klopf	O
,	O
for	O
helping	O
us	O
recognize	O
that	O
reinforce-	O
ment	O
learning	O
needed	O
to	O
be	O
revived	O
;	O
chris	O
watkins	O
,	O
dimitri	O
bertsekas	O
,	O
john	O
tsitsiklis	O
,	O
and	O
paul	O
werbos	O
,	O
for	O
helping	O
us	O
see	O
the	O
value	B
of	O
the	O
relationships	O
to	O
dynamic	O
program-	O
ming	O
;	O
john	O
moore	O
and	O
jim	O
kehoe	O
,	O
for	O
insights	O
and	O
inspirations	O
from	O
animal	O
learning	O
theory	O
;	O
oliver	O
selfridge	O
,	O
for	O
emphasizing	O
the	O
breadth	O
and	O
importance	O
of	O
adaptation	O
;	O
and	O
,	O
more	O
generally	O
,	O
our	O
colleagues	O
and	O
students	O
who	O
have	O
contributed	O
in	O
countless	O
ways	O
:	O
ron	O
williams	O
,	O
charles	O
anderson	O
,	O
satinder	O
singh	O
,	O
sridhar	O
mahadevan	O
,	O
steve	O
bradtke	O
,	O
bob	O
crites	O
,	O
peter	O
dayan	O
,	O
and	O
leemon	O
baird	O
.	O
our	O
view	O
of	O
reinforcement	O
learning	O
has	O
been	O
signiﬁcantly	O
enriched	O
by	O
discussions	O
with	O
paul	O
cohen	O
,	O
paul	O
utgoﬀ	O
,	O
martha	O
steenstrup	O
,	O
gerry	O
tesauro	O
,	O
mike	O
jordan	O
,	O
leslie	O
kaelbling	O
,	O
andrew	O
moore	O
,	O
chris	O
atkeson	O
,	O
tom	O
mitchell	O
,	O
nils	O
nilsson	O
,	O
stuart	O
russell	O
,	O
tom	O
dietterich	O
,	O
tom	O
dean	O
,	O
and	O
bob	O
narendra	O
.	O
we	O
thank	O
michael	O
littman	O
,	O
gerry	O
tesauro	O
,	O
bob	O
crites	O
,	O
satinder	O
singh	O
,	O
and	O
wei	O
zhang	O
for	O
providing	O
speciﬁcs	O
of	O
sections	O
4.7	O
,	O
15.1	O
,	O
15.4	O
,	O
15.5	O
,	O
and	O
15.6	O
respectively	O
.	O
we	O
thank	O
the	O
air	O
force	O
oﬃce	O
of	O
scientiﬁc	O
research	O
,	O
the	O
national	O
science	O
foundation	O
,	O
and	O
gte	O
laboratories	O
for	O
their	O
long	O
and	O
farsighted	O
support	O
.	O
we	O
also	O
wish	O
to	O
thank	O
the	O
many	O
people	O
who	O
have	O
read	O
drafts	O
of	O
this	O
book	O
and	O
provided	O
valuable	O
comments	O
,	O
including	O
tom	O
kalt	O
,	O
john	O
tsitsiklis	O
,	O
pawel	O
cichosz	O
,	O
olle	O
g¨allmo	O
,	O
chuck	O
anderson	O
,	O
stuart	O
russell	O
,	O
ben	O
van	O
roy	O
,	O
paul	O
steenstrup	O
,	O
paul	O
cohen	O
,	O
srid-	O
har	O
mahadevan	O
,	O
jette	O
randlov	O
,	O
brian	O
sheppard	O
,	O
thomas	O
o	O
’	O
connell	O
,	O
richard	O
coggins	O
,	O
cristina	O
versino	O
,	O
john	O
h.	O
hiett	O
,	O
andreas	O
badelt	O
,	O
jay	O
ponte	O
,	O
joe	O
beck	O
,	O
justus	O
piater	O
,	O
martha	O
steenstrup	O
,	O
satinder	O
singh	O
,	O
tommi	O
jaakkola	O
,	O
dimitri	O
bertsekas	O
,	O
torbj¨orn	O
ek-	O
man	O
,	O
christina	O
bj¨orkman	O
,	O
jakob	O
carlstr¨om	O
,	O
and	O
olle	O
palmgren	O
.	O
finally	O
,	O
we	O
thank	O
gwyn	O
mitchell	O
for	O
helping	O
in	O
many	O
ways	O
,	O
and	O
harry	O
stanton	O
and	O
bob	O
prior	O
for	O
being	O
our	O
champions	O
at	O
mit	O
press	O
.	O
summary	O
of	O
notation	O
capital	O
letters	O
are	O
used	O
for	O
random	O
variables	O
,	O
whereas	O
lower	O
case	O
letters	O
are	O
used	O
for	O
the	O
values	O
of	O
random	O
variables	O
and	O
for	O
scalar	O
functions	O
.	O
quantities	O
that	O
are	O
required	O
to	O
be	O
real-valued	O
vectors	O
are	O
written	O
in	O
bold	O
and	O
in	O
lower	O
case	O
(	O
even	O
if	O
random	O
variables	O
)	O
.	O
matrices	O
are	O
bold	O
capitals	O
.	O
equality	O
relationship	O
that	O
is	O
true	O
by	O
deﬁnition	O
approximately	O
equal	O
proportional	O
to	O
probability	O
that	O
a	O
random	O
variable	O
x	O
takes	O
on	O
the	O
value	B
x	O
random	O
variable	O
x	O
selected	O
from	O
distribution	O
p	O
(	O
x	O
)	O
.	O
=	O
≈	O
∝	O
pr	O
{	O
x	O
=	O
x	O
}	O
x	O
∼	O
p	O
e	O
[	O
x	O
]	O
argmaxa	O
f	O
(	O
a	O
)	O
a	O
value	B
of	O
a	O
at	O
which	O
f	O
(	O
a	O
)	O
takes	O
its	O
maximal	O
value	B
ln	O
x	O
ex	O
r	O
f	O
:	O
x	O
→	O
y	O
←	O
(	O
a	O
,	O
b	O
]	O
.	O
=	O
pr	O
{	O
x	O
=	O
x	O
}	O
expectation	O
of	O
a	O
random	O
variable	O
x	O
,	O
i.e.	O
,	O
e	O
[	O
x	O
]	O
=	O
(	O
cid:80	O
)	O
x	O
p	O
(	O
x	O
)	O
x	O
natural	O
logarithm	O
of	O
x	O
the	O
base	O
of	O
the	O
natural	O
logarithm	O
,	O
e	O
≈	O
2.71828	O
,	O
carried	O
to	O
the	O
power	O
x	O
;	O
eln	O
x	O
=	O
x	O
set	O
of	O
real	O
numbers	O
function	O
f	O
from	O
elements	O
of	O
set	O
x	O
to	O
elements	O
of	O
set	O
y	O
assignment	O
the	O
real	O
interval	O
between	O
a	O
and	O
b	O
including	O
b	O
but	O
not	O
including	O
a	O
ε	O
α	O
,	O
β	O
γ	O
λ	O
1predicate	O
probability	O
of	O
taking	O
a	O
random	O
action	O
in	O
an	O
ε-greedy	O
policy	O
step-size	O
parameters	O
discount-rate	O
parameter	O
decay-rate	O
parameter	O
for	O
eligibility	B
traces	I
indicator	O
function	O
(	O
1predicate	O
=	O
1	O
if	O
the	O
predicate	O
is	O
true	O
,	O
else	O
0	O
)	O
in	O
a	O
multi-arm	O
bandit	O
problem	O
:	O
k	O
t	O
q∗	O
(	O
a	O
)	O
qt	O
(	O
a	O
)	O
nt	O
(	O
a	O
)	O
ht	O
(	O
a	O
)	O
πt	O
(	O
a	O
)	O
¯rt	O
number	O
of	O
actions	O
(	O
arms	O
)	O
discrete	O
time	O
step	O
or	O
play	O
number	O
true	O
value	O
(	O
expected	O
reward	O
)	O
of	O
action	O
a	O
estimate	O
at	O
time	O
t	O
of	O
q∗	O
(	O
a	O
)	O
number	O
of	O
times	O
action	B
a	O
has	O
been	O
selected	O
up	O
prior	O
to	O
time	O
t	O
learned	O
preference	O
for	O
selecting	O
action	B
a	O
probability	O
of	O
selecting	O
action	B
a	O
at	O
time	O
t	O
estimate	O
at	O
time	O
t	O
of	O
the	O
expected	O
reward	O
given	O
π	O
xvii	O
xviii	O
summary	O
of	O
notation	O
in	O
a	O
markov	O
decision	O
process	O
:	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
a	O
r	O
s	O
s+	O
a	O
r	O
⊂	O
∈	O
|s|	O
t	O
t	O
,	O
t	O
(	O
t	O
)	O
at	O
st	O
rt	O
π	O
π	O
(	O
s	O
)	O
π	O
(	O
a|s	O
)	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
∇π	O
(	O
a|s	O
,	O
θ	O
)	O
gt	O
gt	O
:	O
t+n	O
,	O
gt	O
:	O
h	O
¯gt	O
:	O
h	O
gλ	O
t	O
gλ	O
t	O
:	O
h	O
gλs	O
t	O
gλa	O
t	O
states	O
an	O
action	B
a	O
reward	O
set	O
of	O
all	O
nonterminal	O
states	O
set	O
of	O
all	O
states	O
,	O
including	O
the	O
terminal	O
state	B
set	O
of	O
all	O
actions	O
set	O
of	O
all	O
possible	O
rewards	O
,	O
a	O
ﬁnite	O
subset	O
of	O
r	O
subset	O
of	O
;	O
e.g.	O
,	O
r	O
⊂	O
r	O
is	O
an	O
element	O
of	O
;	O
e.g.	O
,	O
s	O
∈	O
s	O
,	O
r	O
∈	O
r	O
number	O
of	O
elements	O
in	O
set	O
s	O
discrete	O
time	O
step	O
ﬁnal	O
time	O
step	O
of	O
an	O
episode	O
,	O
or	O
of	O
the	O
episode	O
including	O
time	O
step	O
t	O
action	B
at	O
time	O
t	O
state	B
at	O
time	O
t	O
,	O
typically	O
due	O
,	O
stochastically	O
,	O
to	O
st−1	O
and	O
at−1	O
reward	O
at	O
time	O
t	O
,	O
typically	O
due	O
,	O
stochastically	O
,	O
to	O
st−1	O
and	O
at−1	O
policy	B
(	O
decision-making	O
rule	O
)	O
action	B
taken	O
in	O
state	O
s	O
under	O
deterministic	O
policy	B
π	O
probability	O
of	O
taking	O
action	B
a	O
in	O
state	O
s	O
under	O
stochastic	O
policy	O
π	O
probability	O
of	O
taking	O
action	B
a	O
in	O
state	O
s	O
given	O
parameter	O
vector	O
θ	O
column	O
vector	B
of	O
partial	O
derivatives	O
of	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
with	O
respect	O
to	O
θ	O
return	B
following	O
time	O
t	O
(	O
section	O
3.3	O
)	O
n-step	B
return	O
from	O
t	O
+	O
1	O
to	O
h	O
(	O
discounted	O
and	O
corrected	O
)	O
ﬂat	O
return	O
(	O
undiscounted	O
and	O
uncorrected	O
)	O
from	O
t	O
+	O
1	O
to	O
h	O
(	O
section	O
5.8	O
)	O
λ-return	B
(	O
section	O
12.1	O
)	O
truncated	B
,	O
corrected	O
λ-return	B
(	O
section	O
12.3	O
)	O
λ-return	B
,	O
corrected	O
by	O
estimated	O
state	B
values	O
(	O
section	O
12.8	O
)	O
λ-return	B
,	O
corrected	O
by	O
estimated	O
action	B
values	O
(	O
section	O
12.8	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
a	O
)	O
r	O
(	O
s	O
,	O
a	O
,	O
s	O
(	O
cid:48	O
)	O
)	O
probability	O
of	O
transition	O
to	O
state	B
s	O
(	O
cid:48	O
)	O
with	O
reward	O
r	O
,	O
from	O
state	B
s	O
and	O
action	O
a	O
probability	O
of	O
transition	O
to	O
state	B
s	O
(	O
cid:48	O
)	O
,	O
from	O
state	B
s	O
taking	O
action	B
a	O
expected	O
immediate	O
reward	O
on	O
transition	O
from	O
s	O
to	O
s	O
(	O
cid:48	O
)	O
under	O
action	B
a	O
vπ	O
(	O
s	O
)	O
v∗	O
(	O
s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
q∗	O
(	O
s	O
,	O
a	O
)	O
v	O
,	O
vt	O
q	O
,	O
qt	O
¯vt	O
(	O
s	O
)	O
ut	O
value	B
of	O
state	B
s	O
under	O
policy	B
π	O
(	O
expected	O
return	O
)	O
value	B
of	O
state	B
s	O
under	O
the	O
optimal	O
policy	O
value	B
of	O
taking	O
action	B
a	O
in	O
state	O
s	O
under	O
policy	B
π	O
value	B
of	O
taking	O
action	B
a	O
in	O
state	O
s	O
under	O
the	O
optimal	O
policy	O
array	O
estimates	O
of	O
state-value	O
function	O
vπ	O
or	O
v∗	O
array	O
estimates	O
of	O
action-value	O
function	O
qπ	O
or	O
q∗	O
expected	O
approximate	O
action	O
value	B
,	O
e.g.	O
,	O
(	O
cid:80	O
)	O
a	O
π	O
(	O
a|s	O
)	O
qt	O
(	O
s	O
,	O
a	O
)	O
target	B
for	O
estimate	O
at	O
time	O
t	O
summary	O
of	O
notation	O
xix	O
temporal-diﬀerence	O
error	O
at	O
t	O
(	O
a	O
random	O
variable	O
)	O
(	O
section	O
6.1	O
)	O
d-vector	O
of	O
weights	O
underlying	O
an	O
approximate	B
value	O
function	O
ith	O
component	O
of	O
learnable	O
weight	O
vector	B
dimensionality—the	O
number	O
of	O
components	O
of	O
w	O
alternate	O
dimensionality—the	O
number	O
of	O
components	O
of	O
θ	O
number	O
of	O
1s	O
in	O
a	O
sparse	B
binary	O
feature	O
vector	O
approximate	B
value	O
of	O
state	O
s	O
given	O
weight	O
vector	B
w	O
alternate	O
notation	O
for	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
approximate	B
value	O
of	O
state–action	O
pair	O
s	O
,	O
a	O
given	O
weight	O
vector	B
w	O
vector	B
of	O
features	O
visible	O
when	O
in	O
state	O
s	O
vector	B
of	O
features	O
visible	O
when	O
in	O
state	O
s	O
taking	O
action	B
a	O
δt	O
w	O
,	O
wt	O
wi	O
,	O
wt	O
,	O
i	O
d	O
d	O
(	O
cid:48	O
)	O
m	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
vw	O
(	O
s	O
)	O
ˆq	O
(	O
s	O
,	O
a	O
,	O
w	O
)	O
x	O
(	O
s	O
)	O
x	O
(	O
s	O
,	O
a	O
)	O
xi	O
(	O
s	O
)	O
,	O
xi	O
(	O
s	O
,	O
a	O
)	O
ith	O
component	O
of	O
vector	O
x	O
(	O
s	O
)	O
or	O
x	O
(	O
s	O
,	O
a	O
)	O
xt	O
w	O
(	O
cid:62	O
)	O
x	O
µ	O
(	O
s	O
)	O
µ	O
(	O
cid:107	O
)	O
x	O
(	O
cid:107	O
)	O
2	O
v	O
,	O
vt	O
zt	O
∇ˆv	O
(	O
s	O
,	O
w	O
)	O
∇ˆq	O
(	O
s	O
,	O
a	O
,	O
w	O
)	O
θ	O
,	O
θt	O
πθ	O
j	O
(	O
π	O
)	O
,	O
j	O
(	O
θ	O
)	O
∇j	O
(	O
θ	O
)	O
h	O
(	O
s	O
,	O
a	O
,	O
θ	O
)	O
µ	O
shorthand	O
for	O
x	O
(	O
st	O
)	O
or	O
x	O
(	O
st	O
,	O
at	O
)	O
.	O
inner	O
product	O
of	O
vectors	O
,	O
w	O
(	O
cid:62	O
)	O
x	O
on-policy	B
distribution	I
over	O
states	O
(	O
section	O
9.2	O
)	O
|s|-vector	O
of	O
the	O
µ	O
(	O
s	O
)	O
for	O
all	O
s	O
∈	O
s	O
=	O
(	O
cid:80	O
)	O
i	O
wixi	O
;	O
e.g.	O
,	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
µ-weighted	O
norm	O
of	O
any	O
vector	B
x	O
(	O
s	O
)	O
,	O
i.e.	O
,	O
(	O
cid:80	O
)	O
i	O
µ	O
(	O
s	O
)	O
x	O
(	O
s	O
)	O
2	O
.	O
=	O
w	O
(	O
cid:62	O
)	O
x	O
(	O
s	O
)	O
i	O
(	O
section	O
11.4	O
)	O
secondary	O
d-vector	O
of	O
weights	O
,	O
used	O
to	O
learn	O
w	O
(	O
chapter	O
11	O
)	O
d-vector	O
of	O
eligibility	O
traces	O
at	O
time	O
t	O
(	O
chapter	O
12	O
)	O
column	O
vector	B
of	O
partial	O
derivatives	O
of	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
with	O
respect	O
to	O
w	O
column	O
vector	B
of	O
partial	O
derivatives	O
of	O
ˆq	O
(	O
s	O
,	O
a	O
,	O
w	O
)	O
with	O
respect	O
to	O
w	O
parameter	O
vector	O
of	O
target	O
policy	B
(	O
chapter	O
13	O
)	O
policy	B
corresponding	O
to	O
parameter	O
θ	O
performance	O
measure	O
for	O
policy	O
π	O
or	O
πθ	O
column	O
vector	B
of	O
partial	O
derivatives	O
of	O
j	O
(	O
θ	O
)	O
with	O
respect	O
to	O
θ	O
preference	O
for	O
selecting	O
action	B
a	O
in	O
state	O
s	O
based	O
on	O
θ	O
b	O
behavior	B
policy	I
used	O
to	O
select	O
actions	O
while	O
learning	O
about	O
target	B
policy	O
π	O
ρt	O
:	O
h	O
ρt	O
r	O
(	O
π	O
)	O
¯rt	O
a	O
b	O
wtd	O
i	O
p	O
d	O
x	O
or	O
a	O
baseline	B
function	O
b	O
:	O
s	O
(	O
cid:55	O
)	O
→	O
r	O
for	O
policy-gradient	O
methods	O
or	O
a	O
branching	B
factor	I
for	O
a	O
search	O
tree	O
importance	B
sampling	I
ratio	O
for	O
time	O
t	O
through	O
time	O
h	O
(	O
section	O
5.5	O
)	O
importance	B
sampling	I
ratio	O
for	O
time	O
t	O
alone	O
,	O
ρt	O
=	O
ρt	O
:	O
t	O
average	O
reward	O
(	O
reward	O
rate	O
)	O
for	O
policy	O
π	O
(	O
section	O
10.3	O
)	O
estimate	O
of	O
r	O
(	O
π	O
)	O
at	O
time	O
t	O
.	O
.	O
=	O
e	O
[	O
rt+1xt	O
]	O
=	O
e	O
(	O
cid:104	O
)	O
xt	O
(	O
cid:0	O
)	O
xt	O
−	O
γxt+1	O
(	O
cid:1	O
)	O
(	O
cid:62	O
)	O
(	O
cid:105	O
)	O
(	O
section	O
11.4	O
)	O
d	O
×	O
d	O
matrix	O
a	O
d-dimensional	O
vector	B
b	O
.	O
=	O
a−1b	O
(	O
a	O
d-vector	O
)	O
td	O
ﬁxed	O
point	O
,	O
wtd	O
identity	O
matrix	O
|s|	O
×	O
|s|	O
matrix	O
of	O
state-transition	O
probabilities	O
under	O
π	O
|s|	O
×	O
|s|	O
diagonal	O
matrix	O
with	O
the	O
µ	O
(	O
s	O
)	O
on	O
its	O
diagonal	O
|s|	O
×	O
d	O
matrix	O
with	O
x	O
(	O
s	O
)	O
as	O
its	O
rows	O
chapter	O
1	O
introduction	O
the	O
idea	O
that	O
we	O
learn	O
by	O
interacting	O
with	O
our	O
environment	B
is	O
probably	O
the	O
ﬁrst	O
to	O
occur	O
to	O
us	O
when	O
we	O
think	O
about	O
the	O
nature	O
of	O
learning	O
.	O
when	O
an	O
infant	O
plays	O
,	O
waves	O
its	O
arms	O
,	O
or	O
looks	O
about	O
,	O
it	O
has	O
no	O
explicit	O
teacher	O
,	O
but	O
it	O
does	O
have	O
a	O
direct	O
sensorimotor	O
con-	O
nection	O
to	O
its	O
environment	B
.	O
exercising	O
this	O
connection	O
produces	O
a	O
wealth	O
of	O
information	O
about	O
cause	O
and	O
eﬀect	O
,	O
about	O
the	O
consequences	O
of	O
actions	O
,	O
and	O
about	O
what	O
to	O
do	O
in	O
order	O
to	O
achieve	O
goals	O
.	O
throughout	O
our	O
lives	O
,	O
such	O
interactions	O
are	O
undoubtedly	O
a	O
major	O
source	O
of	O
knowledge	O
about	O
our	O
environment	B
and	O
ourselves	O
.	O
whether	O
we	O
are	O
learning	O
to	O
drive	O
a	O
car	O
or	O
to	O
hold	O
a	O
conversation	O
,	O
we	O
are	O
acutely	O
aware	O
of	O
how	O
our	O
environment	B
responds	O
to	O
what	O
we	O
do	O
,	O
and	O
we	O
seek	O
to	O
inﬂuence	O
what	O
happens	O
through	O
our	O
behavior	O
.	O
learning	O
from	O
interaction	O
is	O
a	O
foundational	O
idea	O
underlying	O
nearly	O
all	O
theories	O
of	O
learning	O
and	O
intelligence	O
.	O
in	O
this	O
book	O
we	O
explore	O
a	O
computational	O
approach	O
to	O
learning	O
from	O
interaction	O
.	O
rather	O
than	O
directly	O
theorizing	O
about	O
how	O
people	O
or	O
animals	O
learn	O
,	O
we	O
primarily	O
explore	O
ide-	O
alized	O
learning	O
situations	O
and	O
evaluate	O
the	O
eﬀectiveness	O
of	O
various	O
learning	O
methods.1	O
that	O
is	O
,	O
we	O
adopt	O
the	O
perspective	O
of	O
an	O
artiﬁcial	B
intelligence	I
researcher	O
or	O
engineer	O
.	O
we	O
explore	O
designs	O
for	O
machines	O
that	O
are	O
eﬀective	O
in	O
solving	O
learning	O
problems	O
of	O
scientiﬁc	O
or	O
economic	O
interest	O
,	O
evaluating	O
the	O
designs	O
through	O
mathematical	O
analysis	O
or	O
compu-	O
tational	O
experiments	O
.	O
the	O
approach	O
we	O
explore	O
,	O
called	O
reinforcement	B
learning	I
,	O
is	O
much	O
more	O
focused	O
on	O
goal-directed	O
learning	O
from	O
interaction	O
than	O
are	O
other	O
approaches	O
to	O
machine	O
learning	O
.	O
1.1	O
reinforcement	B
learning	I
reinforcement	O
learning	O
is	O
learning	O
what	O
to	O
do—how	O
to	O
map	O
situations	O
to	O
actions—so	O
as	O
to	O
maximize	O
a	O
numerical	O
reward	B
signal	I
.	O
the	O
learner	O
is	O
not	O
told	O
which	O
actions	O
to	O
take	O
,	O
but	O
instead	O
must	O
discover	O
which	O
actions	O
yield	O
the	O
most	O
reward	O
by	O
trying	O
them	O
.	O
in	O
the	O
most	O
interesting	O
and	O
challenging	O
cases	O
,	O
actions	O
may	O
aﬀect	O
not	O
only	O
the	O
immediate	O
1the	O
relationships	O
to	O
psychology	B
and	O
neuroscience	B
are	O
summarized	O
in	O
chapters	O
14	O
and	O
15	O
.	O
1	O
2	O
chapter	O
1	O
:	O
introduction	O
reward	O
but	O
also	O
the	O
next	O
situation	O
and	O
,	O
through	O
that	O
,	O
all	O
subsequent	O
rewards	O
.	O
these	O
two	O
characteristics—trial-and-error	O
search	O
and	O
delayed	O
reward—are	O
the	O
two	O
most	O
important	O
distinguishing	O
features	O
of	O
reinforcement	O
learning	O
.	O
reinforcement	B
learning	I
,	O
like	O
many	O
topics	O
whose	O
names	O
end	O
with	O
“	O
ing	B
,	O
”	O
such	O
as	O
machine	O
learning	O
and	O
mountaineering	O
,	O
is	O
simultaneously	O
a	O
problem	O
,	O
a	O
class	O
of	O
solution	O
methods	O
that	O
work	O
well	O
on	O
the	O
problem	O
,	O
and	O
the	O
ﬁeld	O
that	O
studies	O
this	O
problem	O
and	O
its	O
solution	O
methods	O
.	O
it	O
is	O
convenient	O
to	O
use	O
a	O
single	O
name	O
for	O
all	O
three	O
things	O
,	O
but	O
at	O
the	O
same	O
time	O
essential	O
to	O
keep	O
the	O
three	O
conceptually	O
separate	O
.	O
in	O
particular	O
,	O
the	O
distinction	O
between	O
problems	O
and	O
solution	O
methods	O
is	O
very	O
important	O
in	O
reinforcement	O
learning	O
;	O
failing	O
to	O
make	O
this	O
distinction	O
is	O
the	O
source	O
of	O
many	O
confusions	O
.	O
we	O
formalize	O
the	O
problem	O
of	O
reinforcement	O
learning	O
using	O
ideas	O
from	O
dynamical	O
sys-	O
tems	O
theory	O
,	O
speciﬁcally	O
,	O
as	O
the	O
optimal	B
control	I
of	O
incompletely-known	O
markov	O
decision	O
processes	O
.	O
the	O
details	O
of	O
this	O
formalization	O
must	O
wait	O
until	O
chapter	O
3	O
,	O
but	O
the	O
basic	O
idea	O
is	O
simply	O
to	O
capture	O
the	O
most	O
important	O
aspects	O
of	O
the	O
real	O
problem	O
facing	O
a	O
learning	O
agent	O
interacting	O
over	O
time	O
with	O
its	O
environment	B
to	O
achieve	O
a	O
goal	B
.	O
a	O
learning	O
agent	O
must	O
be	O
able	O
to	O
sense	O
the	O
state	B
of	O
its	O
environment	B
to	O
some	O
extent	O
and	O
must	O
be	O
able	O
to	O
take	O
actions	O
that	O
aﬀect	O
the	O
state	B
.	O
the	O
agent	O
also	O
must	O
have	O
a	O
goal	B
or	O
goals	O
relating	O
to	O
the	O
state	B
of	O
the	O
environment	B
.	O
markov	O
decision	O
processes	O
are	O
intended	O
to	O
include	O
just	O
these	O
three	O
aspects—sensation	O
,	O
action	B
,	O
and	O
goal—in	O
their	O
simplest	O
possible	O
forms	O
with-	O
out	O
trivializing	O
any	O
of	O
them	O
.	O
any	O
method	O
that	O
is	O
well	O
suited	O
to	O
solving	O
such	O
problems	O
we	O
consider	O
to	O
be	O
a	O
reinforcement	B
learning	I
method	O
.	O
reinforcement	B
learning	I
is	O
diﬀerent	O
from	O
supervised	B
learning	I
,	O
the	O
kind	O
of	O
learning	O
stud-	O
ied	O
in	O
most	O
current	O
research	O
in	O
the	O
ﬁeld	O
of	O
machine	O
learning	O
.	O
supervised	B
learning	I
is	O
learning	O
from	O
a	O
training	O
set	O
of	O
labeled	O
examples	O
provided	O
by	O
a	O
knowledgable	O
external	O
su-	O
pervisor	O
.	O
each	O
example	O
is	O
a	O
description	O
of	O
a	O
situation	O
together	O
with	O
a	O
speciﬁcation—the	O
label—of	O
the	O
correct	O
action	B
the	O
system	O
should	O
take	O
to	O
that	O
situation	O
,	O
which	O
is	O
often	O
to	O
identify	O
a	O
category	O
to	O
which	O
the	O
situation	O
belongs	O
.	O
the	O
object	O
of	O
this	O
kind	O
of	O
learning	O
is	O
for	O
the	O
system	O
to	O
extrapolate	O
,	O
or	O
generalize	O
,	O
its	O
responses	O
so	O
that	O
it	O
acts	O
correctly	O
in	O
situations	O
not	O
present	O
in	O
the	O
training	O
set	O
.	O
this	O
is	O
an	O
important	O
kind	O
of	O
learning	O
,	O
but	O
alone	O
it	O
is	O
not	O
adequate	O
for	O
learning	O
from	O
interaction	O
.	O
in	O
interactive	O
problems	O
it	O
is	O
often	O
impractical	O
to	O
obtain	O
examples	O
of	O
desired	O
behavior	O
that	O
are	O
both	O
correct	O
and	O
represen-	O
tative	O
of	O
all	O
the	O
situations	O
in	O
which	O
the	O
agent	O
has	O
to	O
act	O
.	O
in	O
uncharted	O
territory—where	O
one	O
would	O
expect	O
learning	O
to	O
be	O
most	O
beneﬁcial—an	O
agent	O
must	O
be	O
able	O
to	O
learn	O
from	O
its	O
own	O
experience	O
.	O
reinforcement	B
learning	I
is	O
also	O
diﬀerent	O
from	O
what	O
machine	O
learning	O
researchers	O
call	O
unsupervised	B
learning	I
,	O
which	O
is	O
typically	O
about	O
ﬁnding	O
structure	O
hidden	O
in	O
collections	O
of	O
unlabeled	O
data	O
.	O
the	O
terms	O
supervised	B
learning	I
and	O
unsupervised	B
learning	I
would	O
seem	O
to	O
exhaustively	O
classify	O
machine	O
learning	O
paradigms	O
,	O
but	O
they	O
do	O
not	O
.	O
although	O
one	O
might	O
be	O
tempted	O
to	O
think	O
of	O
reinforcement	O
learning	O
as	O
a	O
kind	O
of	O
unsupervised	O
learning	O
because	O
it	O
does	O
not	O
rely	O
on	O
examples	O
of	O
correct	O
behavior	O
,	O
reinforcement	B
learning	I
is	O
trying	O
to	O
maximize	O
a	O
reward	B
signal	I
instead	O
of	O
trying	O
to	O
ﬁnd	O
hidden	O
structure	O
.	O
uncovering	O
structure	O
in	O
an	O
agent	O
’	O
s	O
experience	O
can	O
certainly	O
be	O
useful	O
in	O
reinforcement	O
learning	O
,	O
but	O
by	O
itself	O
does	O
not	O
address	O
the	O
reinforcement	B
learning	I
problem	O
of	O
maximizing	O
a	O
reward	B
signal	I
.	O
we	O
therefore	O
consider	O
reinforcement	B
learning	I
to	O
be	O
a	O
third	O
machine	O
learning	O
1.1.	O
reinforcement	B
learning	I
3	O
paradigm	O
,	O
alongside	O
supervised	B
learning	I
and	O
unsupervised	B
learning	I
and	O
perhaps	O
other	O
paradigms	O
as	O
well	O
.	O
one	O
of	O
the	O
challenges	O
that	O
arise	O
in	O
reinforcement	O
learning	O
,	O
and	O
not	O
in	O
other	O
kinds	O
of	O
learning	O
,	O
is	O
the	O
trade-oﬀ	O
between	O
exploration	O
and	O
exploitation	O
.	O
to	O
obtain	O
a	O
lot	O
of	O
reward	O
,	O
a	O
reinforcement	B
learning	I
agent	O
must	O
prefer	O
actions	O
that	O
it	O
has	O
tried	O
in	O
the	O
past	O
and	O
found	O
to	O
be	O
eﬀective	O
in	O
producing	O
reward	O
.	O
but	O
to	O
discover	O
such	O
actions	O
,	O
it	O
has	O
to	O
try	O
actions	O
that	O
it	O
has	O
not	O
selected	O
before	O
.	O
the	O
agent	O
has	O
to	O
exploit	O
what	O
it	O
has	O
already	O
experienced	O
in	O
order	O
to	O
obtain	O
reward	O
,	O
but	O
it	O
also	O
has	O
to	O
explore	O
in	O
order	O
to	O
make	O
better	O
action	B
selections	O
in	O
the	O
future	O
.	O
the	O
dilemma	O
is	O
that	O
neither	O
exploration	O
nor	O
exploitation	O
can	O
be	O
pursued	O
exclusively	O
without	O
failing	O
at	O
the	O
task	O
.	O
the	O
agent	O
must	O
try	O
a	O
variety	O
of	O
actions	O
and	O
progressively	O
favor	O
those	O
that	O
appear	O
to	O
be	O
best	O
.	O
on	O
a	O
stochastic	O
task	O
,	O
each	O
action	B
must	O
be	O
tried	O
many	O
times	O
to	O
gain	O
a	O
reliable	O
estimate	O
of	O
its	O
expected	O
reward	O
.	O
the	O
exploration–exploitation	O
dilemma	O
has	O
been	O
intensively	O
studied	O
by	O
mathematicians	O
for	O
many	O
decades	O
,	O
yet	O
remains	O
unresolved	O
.	O
for	O
now	O
,	O
we	O
simply	O
note	O
that	O
the	O
entire	O
issue	O
of	O
balancing	O
exploration	O
and	O
exploitation	O
does	O
not	O
even	O
arise	O
in	O
supervised	O
and	O
unsupervised	O
learning	O
,	O
at	O
least	O
in	O
their	O
purest	O
forms	O
.	O
another	O
key	O
feature	O
of	O
reinforcement	B
learning	I
is	O
that	O
it	O
explicitly	O
considers	O
the	O
whole	O
problem	O
of	O
a	O
goal-directed	O
agent	O
interacting	O
with	O
an	O
uncertain	O
environment	B
.	O
this	O
is	O
in	O
contrast	O
to	O
many	O
approaches	O
that	O
consider	O
subproblems	O
without	O
addressing	O
how	O
they	O
might	O
ﬁt	O
into	O
a	O
larger	O
picture	O
.	O
for	O
example	O
,	O
we	O
have	O
mentioned	O
that	O
much	O
of	O
machine	O
learning	O
research	O
is	O
concerned	O
with	O
supervised	O
learning	O
without	O
explicitly	O
specifying	O
how	O
such	O
an	O
ability	O
would	O
ﬁnally	O
be	O
useful	O
.	O
other	O
researchers	O
have	O
developed	O
theories	O
of	O
planning	O
with	O
general	O
goals	O
,	O
but	O
without	O
considering	O
planning	B
’	O
s	O
role	O
in	O
real-time	O
decision	O
making	O
,	O
or	O
the	O
question	O
of	O
where	O
the	O
predictive	O
models	O
necessary	O
for	O
planning	O
would	O
come	O
from	O
.	O
although	O
these	O
approaches	O
have	O
yielded	O
many	O
useful	O
results	O
,	O
their	O
focus	O
on	O
isolated	O
subproblems	O
is	O
a	O
signiﬁcant	O
limitation	O
.	O
reinforcement	B
learning	I
takes	O
the	O
opposite	O
tack	O
,	O
starting	O
with	O
a	O
complete	O
,	O
interac-	O
tive	O
,	O
goal-seeking	O
agent	O
.	O
all	O
reinforcement	B
learning	I
agents	O
have	O
explicit	O
goals	O
,	O
can	O
sense	O
aspects	O
of	O
their	O
environments	O
,	O
and	O
can	O
choose	O
actions	O
to	O
inﬂuence	O
their	O
environments	O
.	O
moreover	O
,	O
it	O
is	O
usually	O
assumed	O
from	O
the	O
beginning	O
that	O
the	O
agent	O
has	O
to	O
operate	O
despite	O
signiﬁcant	O
uncertainty	O
about	O
the	O
environment	B
it	O
faces	O
.	O
when	O
reinforcement	B
learning	I
in-	O
volves	O
planning	B
,	O
it	O
has	O
to	O
address	O
the	O
interplay	O
between	O
planning	B
and	O
real-time	O
action	O
selection	O
,	O
as	O
well	O
as	O
the	O
question	O
of	O
how	O
environment	B
models	O
are	O
acquired	O
and	O
improved	O
.	O
when	O
reinforcement	B
learning	I
involves	O
supervised	B
learning	I
,	O
it	O
does	O
so	O
for	O
speciﬁc	O
reasons	O
that	O
determine	O
which	O
capabilities	O
are	O
critical	O
and	O
which	O
are	O
not	O
.	O
for	O
learning	O
research	O
to	O
make	O
progress	O
,	O
important	O
subproblems	O
have	O
to	O
be	O
isolated	O
and	O
studied	O
,	O
but	O
they	O
should	O
be	O
subproblems	O
that	O
play	O
clear	O
roles	O
in	O
complete	O
,	O
interactive	O
,	O
goal-seeking	O
agents	O
,	O
even	O
if	O
all	O
the	O
details	O
of	O
the	O
complete	O
agent	O
can	O
not	O
yet	O
be	O
ﬁlled	O
in	O
.	O
by	O
a	O
complete	O
,	O
interactive	O
,	O
goal-seeking	O
agent	O
we	O
do	O
not	O
always	O
mean	O
something	O
like	O
a	O
complete	O
organism	O
or	O
robot	O
.	O
these	O
are	O
clearly	O
examples	O
,	O
but	O
a	O
complete	O
,	O
interactive	O
,	O
goal-seeking	O
agent	O
can	O
also	O
be	O
a	O
component	O
of	O
a	O
larger	O
behaving	O
system	O
.	O
in	O
this	O
case	O
,	O
the	O
agent	O
directly	O
interacts	O
with	O
the	O
rest	O
of	O
the	O
larger	O
system	O
and	O
indirectly	O
interacts	O
with	O
the	O
larger	O
system	O
’	O
s	O
environment	B
.	O
a	O
simple	O
example	O
is	O
an	O
agent	O
that	O
monitors	O
the	O
charge	O
level	O
of	O
robot	O
’	O
s	O
battery	O
and	O
sends	O
commands	O
to	O
the	O
robot	O
’	O
s	O
control	B
architecture	O
.	O
4	O
chapter	O
1	O
:	O
introduction	O
this	O
agent	O
’	O
s	O
environment	B
is	O
the	O
rest	O
of	O
the	O
robot	O
together	O
with	O
the	O
robot	O
’	O
s	O
environment	B
.	O
one	O
must	O
look	O
beyond	O
the	O
most	O
obvious	O
examples	O
of	O
agents	O
and	O
their	O
environments	O
to	O
appreciate	O
the	O
generality	O
of	O
the	O
reinforcement	B
learning	I
framework	O
.	O
one	O
of	O
the	O
most	O
exciting	O
aspects	O
of	O
modern	O
reinforcement	B
learning	I
is	O
its	O
substantive	O
and	O
fruitful	O
interactions	O
with	O
other	O
engineering	O
and	O
scientiﬁc	O
disciplines	O
.	O
reinforcement	B
learning	I
is	O
part	O
of	O
a	O
decades-long	O
trend	O
within	O
artiﬁcial	B
intelligence	I
and	O
machine	O
learn-	O
ing	B
toward	O
greater	O
integration	O
with	O
statistics	O
,	O
optimization	O
,	O
and	O
other	O
mathematical	O
subjects	O
.	O
for	O
example	O
,	O
the	O
ability	O
of	O
some	O
reinforcement	B
learning	I
methods	O
to	O
learn	O
with	O
parameterized	O
approximators	O
addresses	O
the	O
classical	O
“	O
curse	B
of	I
dimensionality	I
”	O
in	O
oper-	O
ations	O
research	O
and	B
control	I
theory	O
.	O
more	O
distinctively	O
,	O
reinforcement	B
learning	I
has	O
also	O
interacted	O
strongly	O
with	O
psychology	O
and	O
neuroscience	O
,	O
with	O
substantial	O
beneﬁts	O
going	O
both	O
ways	O
.	O
of	O
all	O
the	O
forms	O
of	O
machine	O
learning	O
,	O
reinforcement	B
learning	I
is	O
the	O
clos-	O
est	O
to	O
the	O
kind	O
of	O
learning	O
that	O
humans	O
and	O
other	O
animals	O
do	O
,	O
and	O
many	O
of	O
the	O
core	O
algorithms	O
of	O
reinforcement	O
learning	O
were	O
originally	O
inspired	O
by	O
biological	O
learning	O
sys-	O
tems	O
.	O
reinforcement	B
learning	I
has	O
also	O
given	O
back	O
,	O
both	O
through	O
a	O
psychological	O
model	O
of	O
animal	O
learning	O
that	O
better	O
matches	O
some	O
of	O
the	O
empirical	O
data	O
,	O
and	O
through	O
an	O
in-	O
ﬂuential	O
model	O
of	O
parts	O
of	O
the	O
brain	O
’	O
s	O
reward	O
system	O
.	O
the	O
body	O
of	O
this	O
book	O
develops	O
the	O
ideas	O
of	O
reinforcement	O
learning	O
that	O
pertain	O
to	O
engineering	O
and	B
artiﬁcial	I
intelligence	I
,	O
with	O
connections	O
to	O
psychology	B
and	O
neuroscience	B
summarized	O
in	O
chapters	O
14	O
and	O
15.	O
finally	O
,	O
reinforcement	B
learning	I
is	O
also	O
part	O
of	O
a	O
larger	O
trend	O
in	O
artiﬁcial	O
intelligence	O
back	O
toward	O
simple	O
general	O
principles	O
.	O
since	O
the	O
late	O
1960	O
’	O
s	O
,	O
many	O
artiﬁcial	B
intelligence	I
researchers	O
presumed	O
that	O
there	O
are	O
no	O
general	O
principles	O
to	O
be	O
discovered	O
,	O
that	O
intelli-	O
gence	O
is	O
instead	O
due	O
to	O
the	O
possession	O
of	O
a	O
vast	O
number	O
of	O
special	O
purpose	O
tricks	O
,	O
proce-	O
dures	O
,	O
and	O
heuristics	O
.	O
it	O
was	O
sometimes	O
said	O
that	O
if	O
we	O
could	O
just	O
get	O
enough	O
relevant	O
facts	O
into	O
a	O
machine	O
,	O
say	O
one	O
million	O
,	O
or	O
one	O
billion	O
,	O
then	O
it	O
would	O
become	O
intelligent	O
.	O
methods	O
based	O
on	O
general	O
principles	O
,	O
such	O
as	O
search	O
or	O
learning	O
,	O
were	O
characterized	O
as	O
“	O
weak	O
methods	O
,	O
”	O
whereas	O
those	O
based	O
on	O
speciﬁc	O
knowledge	O
were	O
called	O
“	O
strong	O
meth-	O
ods.	O
”	O
this	O
view	O
is	O
still	O
common	O
today	O
,	O
but	O
not	O
dominant	O
.	O
from	O
our	O
point	O
of	O
view	O
,	O
it	O
was	O
simply	O
premature	O
:	O
too	O
little	O
eﬀort	O
had	O
been	O
put	O
into	O
the	O
search	O
for	O
general	O
principles	O
to	O
conclude	O
that	O
there	O
were	O
none	O
.	O
modern	O
artiﬁcial	B
intelligence	I
now	O
includes	O
much	O
research	O
looking	O
for	O
general	O
principles	O
of	O
learning	O
,	O
search	O
,	O
and	O
decision	O
making	O
,	O
as	O
well	O
as	O
trying	O
to	O
incorporate	O
vast	O
amounts	O
of	O
domain	O
knowledge	O
.	O
it	O
is	O
not	O
clear	O
how	O
far	O
back	O
the	O
pen-	O
dulum	O
will	O
swing	O
,	O
but	O
reinforcement	B
learning	I
research	O
is	O
certainly	O
part	O
of	O
the	O
swing	O
back	O
toward	O
simpler	O
and	O
fewer	O
general	O
principles	O
of	O
artiﬁcial	O
intelligence	O
.	O
1.2	O
examples	O
a	O
good	O
way	O
to	O
understand	O
reinforcement	B
learning	I
is	O
to	O
consider	O
some	O
of	O
the	O
examples	O
and	O
possible	O
applications	O
that	O
have	O
guided	O
its	O
development	O
.	O
•	O
a	O
master	O
chess	B
player	O
makes	O
a	O
move	O
.	O
the	O
choice	O
is	O
informed	O
both	O
by	O
planning—	O
anticipating	O
possible	O
replies	O
and	O
counterreplies—and	O
by	O
immediate	O
,	O
intuitive	O
judg-	O
ments	O
of	O
the	O
desirability	O
of	O
particular	O
positions	O
and	O
moves	O
.	O
•	O
an	O
adaptive	O
controller	O
adjusts	O
parameters	O
of	O
a	O
petroleum	O
reﬁnery	O
’	O
s	O
operation	O
in	O
1.2.	O
examples	O
5	O
real	O
time	O
.	O
the	O
controller	O
optimizes	O
the	O
yield/cost/quality	O
trade-oﬀ	O
on	O
the	O
basis	O
of	O
speciﬁed	O
marginal	O
costs	O
without	O
sticking	O
strictly	O
to	O
the	O
set	O
points	O
originally	O
suggested	O
by	O
engineers	O
.	O
•	O
a	O
gazelle	O
calf	O
struggles	O
to	O
its	O
feet	O
minutes	O
after	O
being	O
born	O
.	O
half	O
an	O
hour	O
later	O
it	O
is	O
running	O
at	O
20	O
miles	O
per	O
hour	O
.	O
•	O
a	O
mobile	O
robot	O
decides	O
whether	O
it	O
should	O
enter	O
a	O
new	O
room	O
in	O
search	O
of	O
more	O
trash	O
to	O
collect	O
or	O
start	O
trying	O
to	O
ﬁnd	O
its	O
way	O
back	O
to	O
its	O
battery	O
recharging	O
station	O
.	O
it	O
makes	O
its	O
decision	O
based	O
on	O
the	O
current	O
charge	O
level	O
of	O
its	O
battery	O
and	O
how	O
quickly	O
and	O
easily	O
it	O
has	O
been	O
able	O
to	O
ﬁnd	O
the	O
recharger	O
in	O
the	O
past	O
.	O
•	O
phil	O
prepares	O
his	O
breakfast	O
.	O
closely	O
examined	O
,	O
even	O
this	O
apparently	O
mundane	O
ac-	O
tivity	O
reveals	O
a	O
complex	O
web	O
of	O
conditional	O
behavior	O
and	O
interlocking	O
goal–subgoal	O
relationships	O
:	O
walking	O
to	O
the	O
cupboard	O
,	O
opening	O
it	O
,	O
selecting	O
a	O
cereal	O
box	O
,	O
then	O
reaching	O
for	O
,	O
grasping	O
,	O
and	O
retrieving	O
the	O
box	O
.	O
other	O
complex	O
,	O
tuned	O
,	O
interactive	O
sequences	O
of	O
behavior	O
are	O
required	O
to	O
obtain	O
a	O
bowl	O
,	O
spoon	O
,	O
and	O
milk	O
jug	O
.	O
each	O
step	O
involves	O
a	O
series	O
of	O
eye	O
movements	O
to	O
obtain	O
information	O
and	O
to	O
guide	O
reaching	O
and	O
locomotion	O
.	O
rapid	O
judgments	O
are	O
continually	O
made	O
about	O
how	O
to	O
carry	O
the	O
objects	O
or	O
whether	O
it	O
is	O
better	O
to	O
ferry	O
some	O
of	O
them	O
to	O
the	O
dining	O
table	O
before	O
obtaining	O
others	O
.	O
each	O
step	O
is	O
guided	O
by	O
goals	O
,	O
such	O
as	O
grasping	O
a	O
spoon	O
or	O
getting	O
to	O
the	O
refrigerator	O
,	O
and	O
is	O
in	O
service	O
of	O
other	O
goals	O
,	O
such	O
as	O
having	O
the	O
spoon	O
to	O
eat	O
with	O
once	O
the	O
cereal	O
is	O
prepared	O
and	O
ultimately	O
obtaining	O
nourishment	O
.	O
whether	O
he	O
is	O
aware	O
of	O
it	O
or	O
not	O
,	O
phil	O
is	O
accessing	O
information	O
about	O
the	O
state	B
of	O
his	O
body	O
that	O
determines	O
his	O
nutritional	O
needs	O
,	O
level	O
of	O
hunger	O
,	O
and	O
food	O
preferences	O
.	O
these	O
examples	O
share	O
features	O
that	O
are	O
so	O
basic	O
that	O
they	O
are	O
easy	O
to	O
overlook	O
.	O
all	O
involve	O
interaction	O
between	O
an	O
active	O
decision-making	O
agent	O
and	O
its	O
environment	B
,	O
within	O
which	O
the	O
agent	O
seeks	O
to	O
achieve	O
a	O
goal	B
despite	O
uncertainty	O
about	O
its	O
environment	B
.	O
the	O
agent	O
’	O
s	O
actions	O
are	O
permitted	O
to	O
aﬀect	O
the	O
future	O
state	B
of	O
the	O
environment	B
(	O
e.g.	O
,	O
the	O
next	O
chess	B
position	O
,	O
the	O
level	O
of	O
reservoirs	O
of	O
the	O
reﬁnery	O
,	O
the	O
robot	O
’	O
s	O
next	O
location	O
and	O
the	O
future	O
charge	O
level	O
of	O
its	O
battery	O
)	O
,	O
thereby	O
aﬀecting	O
the	O
options	B
and	O
opportunities	O
available	O
to	O
the	O
agent	O
at	O
later	O
times	O
.	O
correct	O
choice	O
requires	O
taking	O
into	O
account	O
indirect	O
,	O
delayed	O
consequences	O
of	O
actions	O
,	O
and	O
thus	O
may	O
require	O
foresight	O
or	O
planning	B
.	O
at	O
the	O
same	O
time	O
,	O
in	O
all	O
these	O
examples	O
the	O
eﬀects	O
of	O
actions	O
can	O
not	O
be	O
fully	O
pre-	O
dicted	O
;	O
thus	O
the	O
agent	O
must	O
monitor	O
its	O
environment	B
frequently	O
and	O
react	O
appropriately	O
.	O
for	O
example	O
,	O
phil	O
must	O
watch	O
the	O
milk	O
he	O
pours	O
into	O
his	O
cereal	O
bowl	O
to	O
keep	O
it	O
from	O
overﬂowing	O
.	O
all	O
these	O
examples	O
involve	O
goals	O
that	O
are	O
explicit	O
in	O
the	O
sense	O
that	O
the	O
agent	O
can	O
judge	O
progress	O
toward	O
its	O
goal	B
based	O
on	O
what	O
it	O
can	O
sense	O
directly	O
.	O
the	O
chess	B
player	O
knows	O
whether	O
or	O
not	O
he	O
wins	O
,	O
the	O
reﬁnery	O
controller	O
knows	O
how	O
much	O
petroleum	O
is	O
being	O
produced	O
,	O
the	O
gazelle	O
calf	O
knows	O
when	O
it	O
falls	O
,	O
the	O
mobile	O
robot	O
knows	O
when	O
its	O
batteries	O
run	O
down	O
,	O
and	O
phil	O
knows	O
whether	O
or	O
not	O
he	O
is	O
enjoying	O
his	O
breakfast	O
.	O
in	O
all	O
of	O
these	O
examples	O
the	O
agent	O
can	O
use	O
its	O
experience	O
to	O
improve	O
its	O
performance	O
over	O
time	O
.	O
the	O
chess	B
player	O
reﬁnes	O
the	O
intuition	O
he	O
uses	O
to	O
evaluate	O
positions	O
,	O
thereby	O
improving	O
his	O
play	O
;	O
the	O
gazelle	O
calf	O
improves	O
the	O
eﬃciency	O
with	O
which	O
it	O
can	O
run	O
;	O
phil	O
learns	O
to	O
streamline	O
making	O
his	O
breakfast	O
.	O
the	O
knowledge	O
the	O
agent	O
brings	O
to	O
the	O
task	O
6	O
chapter	O
1	O
:	O
introduction	O
at	O
the	O
start—either	O
from	O
previous	O
experience	O
with	O
related	O
tasks	O
or	O
built	O
into	O
it	O
by	O
de-	O
sign	O
or	O
evolution—inﬂuences	O
what	O
is	O
useful	O
or	O
easy	O
to	O
learn	O
,	O
but	O
interaction	O
with	O
the	O
environment	B
is	O
essential	O
for	O
adjusting	O
behavior	O
to	O
exploit	O
speciﬁc	O
features	O
of	O
the	O
task	O
.	O
1.3	O
elements	O
of	O
reinforcement	O
learning	O
beyond	O
the	O
agent	O
and	O
the	O
environment	O
,	O
one	O
can	O
identify	O
four	O
main	O
subelements	O
of	O
a	O
re-	O
inforcement	O
learning	O
system	O
:	O
a	O
policy	B
,	O
a	O
reward	B
signal	I
,	O
a	O
value	B
function	I
,	O
and	O
,	O
optionally	O
,	O
a	O
model	B
of	I
the	I
environment	I
.	O
a	O
policy	B
deﬁnes	O
the	O
learning	O
agent	O
’	O
s	O
way	O
of	O
behaving	O
at	O
a	O
given	O
time	O
.	O
roughly	O
speak-	O
ing	B
,	O
a	O
policy	B
is	O
a	O
mapping	O
from	O
perceived	O
states	O
of	O
the	O
environment	B
to	O
actions	O
to	O
be	O
taken	O
when	O
in	O
those	O
states	O
.	O
it	O
corresponds	O
to	O
what	O
in	B
psychology	I
would	O
be	O
called	O
a	O
set	O
of	O
stimulus–response	O
rules	O
or	O
associations	O
.	O
in	O
some	O
cases	O
the	O
policy	B
may	O
be	O
a	O
simple	O
function	O
or	O
lookup	O
table	O
,	O
whereas	O
in	O
others	O
it	O
may	O
involve	O
extensive	O
computation	O
such	O
as	O
a	O
search	O
process	O
.	O
the	O
policy	B
is	O
the	O
core	O
of	O
a	O
reinforcement	B
learning	I
agent	O
in	O
the	O
sense	O
that	O
it	O
alone	O
is	O
suﬃcient	O
to	O
determine	O
behavior	O
.	O
in	O
general	O
,	O
policies	O
may	O
be	O
stochastic	O
.	O
a	O
reward	B
signal	I
deﬁnes	O
the	O
goal	B
in	O
a	O
reinforcement	B
learning	I
problem	O
.	O
on	O
each	O
time	O
step	O
,	O
the	O
environment	B
sends	O
to	O
the	O
reinforcement	B
learning	I
agent	O
a	O
single	O
number	O
called	O
the	O
reward	O
.	O
the	O
agent	O
’	O
s	O
sole	O
objective	O
is	O
to	O
maximize	O
the	O
total	O
reward	O
it	O
receives	O
over	O
the	O
long	O
run	O
.	O
the	O
reward	B
signal	I
thus	O
deﬁnes	O
what	O
are	O
the	O
good	O
and	O
bad	O
events	O
for	O
the	O
agent	O
.	O
in	O
a	O
biological	O
system	O
,	O
we	O
might	O
think	O
of	O
rewards	O
as	O
analogous	O
to	O
the	O
experiences	O
of	O
pleasure	O
or	O
pain	O
.	O
they	O
are	O
the	O
immediate	O
and	O
deﬁning	O
features	O
of	O
the	O
problem	O
faced	O
by	O
the	O
agent	O
.	O
the	O
reward	B
signal	I
is	O
the	O
primary	O
basis	O
for	O
altering	O
the	O
policy	B
;	O
if	O
an	O
action	B
selected	O
by	O
the	O
policy	B
is	O
followed	O
by	O
low	O
reward	O
,	O
then	O
the	O
policy	B
may	O
be	O
changed	O
to	O
select	O
some	O
other	O
action	B
in	O
that	O
situation	O
in	O
the	O
future	O
.	O
in	O
general	O
,	O
reward	O
signals	O
may	O
be	O
stochastic	O
functions	O
of	O
the	O
state	B
of	O
the	O
environment	B
and	O
the	O
actions	O
taken	O
.	O
whereas	O
the	O
reward	B
signal	I
indicates	O
what	O
is	O
good	O
in	O
an	O
immediate	O
sense	O
,	O
a	O
value	B
function	I
speciﬁes	O
what	O
is	O
good	O
in	O
the	O
long	O
run	O
.	O
roughly	O
speaking	O
,	O
the	O
value	B
of	O
a	O
state	B
is	O
the	O
total	O
amount	O
of	O
reward	O
an	O
agent	O
can	O
expect	O
to	O
accumulate	O
over	O
the	O
future	O
,	O
starting	O
from	O
that	O
state	B
.	O
whereas	O
rewards	O
determine	O
the	O
immediate	O
,	O
intrinsic	B
desirability	O
of	O
environmental	O
states	O
,	O
values	O
indicate	O
the	O
long-term	O
desirability	O
of	O
states	O
after	O
taking	O
into	O
account	O
the	O
states	O
that	O
are	O
likely	O
to	O
follow	O
,	O
and	O
the	O
rewards	O
available	O
in	O
those	O
states	O
.	O
for	O
example	O
,	O
a	O
state	B
might	O
always	O
yield	O
a	O
low	O
immediate	O
reward	O
but	O
still	O
have	O
a	O
high	O
value	B
because	O
it	O
is	O
regularly	O
followed	O
by	O
other	O
states	O
that	O
yield	O
high	O
rewards	O
.	O
or	O
the	O
reverse	O
could	O
be	O
true	O
.	O
to	O
make	O
a	O
human	O
analogy	O
,	O
rewards	O
are	O
somewhat	O
like	O
pleasure	O
(	O
if	O
high	O
)	O
and	O
pain	O
(	O
if	O
low	O
)	O
,	O
whereas	O
values	O
correspond	O
to	O
a	O
more	O
reﬁned	O
and	O
farsighted	O
judgment	O
of	O
how	O
pleased	O
or	O
displeased	O
we	O
are	O
that	O
our	O
environment	B
is	O
in	O
a	O
particular	O
state	B
.	O
rewards	O
are	O
in	O
a	O
sense	O
primary	O
,	O
whereas	O
values	O
,	O
as	O
predictions	O
of	O
rewards	O
,	O
are	O
sec-	O
ondary	O
.	O
without	O
rewards	O
there	O
could	O
be	O
no	O
values	O
,	O
and	O
the	O
only	O
purpose	O
of	O
estimating	O
values	O
is	O
to	O
achieve	O
more	O
reward	O
.	O
nevertheless	O
,	O
it	O
is	O
values	O
with	O
which	O
we	O
are	O
most	O
concerned	O
when	O
making	O
and	O
evaluating	O
decisions	O
.	O
action	B
choices	O
are	O
made	O
based	O
on	O
value	B
judgments	O
.	O
we	O
seek	O
actions	O
that	O
bring	O
about	O
states	O
of	O
highest	O
value	B
,	O
not	O
high-	O
est	O
reward	O
,	O
because	O
these	O
actions	O
obtain	O
the	O
greatest	O
amount	O
of	O
reward	O
for	O
us	O
over	O
the	O
long	O
run	O
.	O
unfortunately	O
,	O
it	O
is	O
much	O
harder	O
to	O
determine	O
values	O
than	O
it	O
is	O
to	O
determine	O
1.4.	O
limitations	O
and	O
scope	O
7	O
rewards	O
.	O
rewards	O
are	O
basically	O
given	O
directly	O
by	O
the	O
environment	B
,	O
but	O
values	O
must	O
be	O
estimated	O
and	O
re-estimated	O
from	O
the	O
sequences	O
of	O
observations	O
an	O
agent	O
makes	O
over	O
its	O
entire	O
lifetime	O
.	O
in	O
fact	O
,	O
the	O
most	O
important	O
component	O
of	O
almost	O
all	O
reinforcement	O
learn-	O
ing	B
algorithms	O
we	O
consider	O
is	O
a	O
method	O
for	O
eﬃciently	O
estimating	O
values	O
.	O
the	O
central	O
role	O
of	O
value	O
estimation	O
is	O
arguably	O
the	O
most	O
important	O
thing	O
that	O
has	O
been	O
learned	O
about	O
reinforcement	B
learning	I
over	O
the	O
last	O
six	O
decades	O
.	O
the	O
fourth	O
and	O
ﬁnal	O
element	O
of	O
some	O
reinforcement	B
learning	I
systems	O
is	O
a	O
model	B
of	I
the	I
environment	I
.	O
this	O
is	O
something	O
that	O
mimics	O
the	O
behavior	O
of	O
the	O
environment	B
,	O
or	O
more	O
generally	O
,	O
that	O
allows	O
inferences	O
to	O
be	O
made	O
about	O
how	O
the	O
environment	B
will	O
behave	O
.	O
for	O
example	O
,	O
given	O
a	O
state	B
and	O
action	B
,	O
the	O
model	O
might	O
predict	O
the	O
resultant	O
next	O
state	B
and	O
next	O
reward	O
.	O
models	O
are	O
used	O
for	O
planning	O
,	O
by	O
which	O
we	O
mean	O
any	O
way	O
of	O
deciding	O
on	O
a	O
course	O
of	O
action	O
by	O
considering	O
possible	O
future	O
situations	O
before	O
they	O
are	O
actually	O
experienced	O
.	O
methods	O
for	O
solving	O
reinforcement	B
learning	I
problems	O
that	O
use	O
models	O
and	O
planning	O
are	O
called	O
model-based	O
methods	O
,	O
as	O
opposed	O
to	O
simpler	O
model-free	O
methods	O
that	O
are	O
explicitly	O
trial-and-error	B
learners—viewed	O
as	O
almost	O
the	O
opposite	O
of	O
planning	O
.	O
in	O
chapter	O
8	O
we	O
explore	O
reinforcement	B
learning	I
systems	O
that	O
simultaneously	O
learn	O
by	O
trial	O
and	O
error	O
,	O
learn	O
a	O
model	B
of	I
the	I
environment	I
,	O
and	O
use	O
the	O
model	O
for	O
planning	B
.	O
modern	O
reinforcement	B
learning	I
spans	O
the	O
spectrum	O
from	O
low-level	O
,	O
trial-and-error	B
learning	O
to	O
high-level	O
,	O
deliberative	O
planning	B
.	O
1.4	O
limitations	O
and	O
scope	O
reinforcement	B
learning	I
relies	O
heavily	O
on	O
the	O
concept	O
of	O
state—as	O
input	O
to	O
the	O
policy	B
and	O
value	B
function	I
,	O
and	O
as	O
both	O
input	O
to	O
and	O
output	O
from	O
the	O
model	O
.	O
informally	O
,	O
we	O
can	O
think	O
of	O
the	O
state	B
as	O
a	O
signal	O
conveying	O
to	O
the	O
agent	O
some	O
sense	O
of	O
“	O
how	O
the	O
environment	B
is	O
”	O
at	O
a	O
particular	O
time	O
.	O
the	O
formal	O
deﬁnition	O
of	O
state	O
as	O
we	O
use	O
it	O
here	O
is	O
given	O
by	O
the	O
framework	O
of	O
markov	O
decision	O
processes	O
presented	O
in	O
chapter	O
3.	O
more	O
generally	O
,	O
however	O
,	O
we	O
encourage	O
the	O
reader	O
to	O
follow	O
the	O
informal	O
meaning	O
and	O
think	O
of	O
the	O
state	B
as	O
whatever	O
information	O
is	O
available	O
to	O
the	O
agent	O
about	O
its	O
environment	B
.	O
in	O
eﬀect	O
,	O
we	O
assume	O
that	O
the	O
state	B
signal	O
is	O
produced	O
by	O
some	O
preprocessing	O
system	O
that	O
is	O
nominally	O
part	O
of	O
the	O
agent	O
’	O
s	O
environment	B
.	O
we	O
do	O
not	O
address	O
the	O
issues	O
of	O
constructing	O
,	O
changing	O
,	O
or	O
learning	O
the	O
state	B
signal	O
in	O
this	O
book	O
(	O
other	O
than	O
brieﬂy	O
in	O
section	O
17.3	O
)	O
.	O
we	O
take	O
this	O
approach	O
not	O
because	O
we	O
consider	O
state	B
representation	O
to	O
be	O
unimportant	O
,	O
but	O
in	O
order	O
to	O
focus	O
fully	O
on	O
the	O
decision-making	O
issues	O
.	O
in	O
other	O
words	O
,	O
our	O
main	O
concern	O
is	O
not	O
with	O
designing	O
the	O
state	B
signal	O
,	O
but	O
with	O
deciding	O
what	O
action	B
to	O
take	O
as	O
a	O
function	O
of	O
whatever	O
state	B
signal	O
is	O
available	O
.	O
most	O
of	O
the	O
reinforcement	B
learning	I
methods	O
we	O
consider	O
in	O
this	O
book	O
are	O
structured	O
around	O
estimating	O
value	B
functions	O
,	O
but	O
it	O
is	O
not	O
strictly	O
necessary	O
to	O
do	O
this	O
to	O
solve	O
rein-	O
forcement	O
learning	O
problems	O
.	O
for	O
example	O
,	O
solution	O
methods	O
such	O
as	O
genetic	O
algorithms	O
,	O
genetic	O
programming	O
,	O
simulated	O
annealing	O
,	O
and	O
other	O
optimization	O
methods	O
never	O
esti-	O
mate	O
value	B
functions	O
.	O
these	O
methods	O
apply	O
multiple	O
static	O
policies	O
each	O
interacting	O
over	O
an	O
extended	O
period	O
of	O
time	O
with	O
a	O
separate	O
instance	O
of	O
the	O
environment	B
.	O
the	O
policies	O
that	O
obtain	O
the	O
most	O
reward	O
,	O
and	O
random	O
variations	O
of	O
them	O
,	O
are	O
carried	O
over	O
to	O
the	O
next	O
generation	O
of	O
policies	O
,	O
and	O
the	O
process	O
repeats	O
.	O
we	O
call	O
these	O
evolutionary	B
methods	I
8	O
chapter	O
1	O
:	O
introduction	O
because	O
their	O
operation	O
is	O
analogous	O
to	O
the	O
way	O
biological	O
evolution	B
produces	O
organ-	O
isms	O
with	O
skilled	O
behavior	O
even	O
if	O
they	O
do	O
not	O
learn	O
during	O
their	O
individual	O
lifetimes	O
.	O
if	O
the	O
space	O
of	O
policies	O
is	O
suﬃciently	O
small	O
,	O
or	O
can	O
be	O
structured	O
so	O
that	O
good	O
policies	O
are	O
common	O
or	O
easy	O
to	O
ﬁnd—or	O
if	O
a	O
lot	O
of	O
time	O
is	O
available	O
for	O
the	O
search—then	O
evolu-	O
tionary	O
methods	O
can	O
be	O
eﬀective	O
.	O
in	O
addition	O
,	O
evolutionary	B
methods	I
have	O
advantages	O
on	O
problems	O
in	O
which	O
the	O
learning	O
agent	O
can	O
not	O
sense	O
the	O
complete	O
state	B
of	O
its	O
environment	B
.	O
our	O
focus	O
is	O
on	O
reinforcement	B
learning	I
methods	O
that	O
learn	O
while	O
interacting	O
with	O
the	O
environment	B
,	O
which	O
evolutionary	B
methods	I
do	O
not	O
do	O
.	O
methods	O
able	O
to	O
take	O
advantage	O
of	O
the	O
details	O
of	O
individual	O
behavioral	O
interactions	O
can	O
be	O
much	O
more	O
eﬃcient	O
than	O
evo-	O
lutionary	O
methods	O
in	O
many	O
cases	O
.	O
evolutionary	B
methods	I
ignore	O
much	O
of	O
the	O
useful	O
structure	O
of	O
the	O
reinforcement	B
learning	I
problem	O
:	O
they	O
do	O
not	O
use	O
the	O
fact	O
that	O
the	O
policy	B
they	O
are	O
searching	O
for	O
is	O
a	O
function	O
from	O
states	O
to	O
actions	O
;	O
they	O
do	O
not	O
notice	O
which	O
states	O
an	O
individual	O
passes	O
through	O
during	O
its	O
lifetime	O
,	O
or	O
which	O
actions	O
it	O
selects	O
.	O
in	O
some	O
cases	O
this	O
information	O
can	O
be	O
misleading	O
(	O
e.g.	O
,	O
when	O
states	O
are	O
misperceived	O
)	O
,	O
but	O
more	O
often	O
it	O
should	O
enable	O
more	O
eﬃcient	O
search	O
.	O
although	O
evolution	B
and	O
learning	O
share	O
many	O
features	O
and	O
naturally	O
work	O
together	O
,	O
we	O
do	O
not	O
consider	O
evolutionary	B
methods	I
by	O
themselves	O
to	O
be	O
especially	O
well	O
suited	O
to	O
reinforcement	B
learning	I
problems	O
and	O
,	O
accordingly	O
,	O
we	O
do	O
not	O
cover	O
them	O
in	O
this	O
book	O
.	O
1.5	O
an	O
extended	O
example	O
:	O
tic-tac-toe	B
to	O
illustrate	O
the	O
general	O
idea	O
of	O
reinforcement	O
learning	O
and	O
contrast	O
it	O
with	O
other	O
ap-	O
proaches	O
,	O
we	O
next	O
consider	O
a	O
single	O
example	O
in	O
more	O
detail	O
.	O
consider	O
the	O
familiar	O
child	O
’	O
s	O
game	O
of	O
tic-tac-toe	B
.	O
two	O
players	O
take	O
turns	O
playing	O
on	O
a	O
three-by-three	O
board	O
.	O
one	O
player	O
plays	O
xs	O
and	O
the	O
other	O
os	O
until	O
one	O
player	O
wins	O
by	O
placing	O
three	O
marks	O
in	O
a	O
row	O
,	O
horizontally	O
,	O
vertically	O
,	O
or	O
diagonally	O
,	O
as	O
the	O
x	O
player	O
has	O
in	O
the	O
game	O
shown	O
to	O
the	O
right	O
.	O
if	O
the	O
board	O
ﬁlls	O
up	O
with	O
neither	O
player	O
getting	O
three	O
in	O
a	O
row	O
,	O
the	O
game	O
is	O
a	O
draw	O
.	O
be-	O
cause	O
a	O
skilled	O
player	O
can	O
play	O
so	O
as	O
never	O
to	O
lose	O
,	O
let	O
us	O
assume	O
that	O
we	O
are	O
playing	O
against	O
an	O
imperfect	O
player	O
,	O
one	O
whose	O
play	O
is	O
sometimes	O
incorrect	O
and	O
allows	O
us	O
to	O
win	O
.	O
for	O
the	O
moment	O
,	O
in	O
fact	O
,	O
let	O
us	O
consider	O
draws	O
and	O
losses	O
to	O
be	O
equally	O
bad	O
for	O
us	O
.	O
how	O
might	O
we	O
construct	O
a	O
player	O
that	O
will	O
ﬁnd	O
the	O
imperfections	O
in	O
its	O
opponent	O
’	O
s	O
play	O
and	O
learn	O
to	O
maximize	O
its	O
chances	O
of	O
winning	O
?	O
although	O
this	O
is	O
a	O
simple	O
problem	O
,	O
it	O
can	O
not	O
readily	O
be	O
solved	O
in	O
a	O
satisfactory	O
way	O
through	O
classical	O
techniques	O
.	O
for	O
example	O
,	O
the	O
classical	O
“	O
minimax	O
”	O
solution	O
from	O
game	B
theory	I
is	O
not	O
correct	O
here	O
because	O
it	O
assumes	O
a	O
particular	O
way	O
of	O
playing	O
by	O
the	O
opponent	O
.	O
for	O
example	O
,	O
a	O
minimax	O
player	O
would	O
never	O
reach	O
a	O
game	O
state	O
from	O
which	O
it	O
could	O
lose	O
,	O
even	O
if	O
in	O
fact	O
it	O
always	O
won	O
from	O
that	O
state	B
because	O
of	O
incorrect	O
play	O
by	O
the	O
opponent	O
.	O
classical	O
optimization	O
methods	O
for	O
sequential	O
decision	O
problems	O
,	O
such	O
as	O
dynamic	O
programming	O
,	O
can	O
compute	O
an	O
optimal	O
solution	O
for	O
any	O
opponent	O
,	O
but	O
require	O
as	O
input	O
a	O
complete	O
speciﬁcation	O
of	O
that	O
opponent	O
,	O
including	O
the	O
probabilities	O
with	O
which	O
the	O
opponent	O
makes	O
each	O
move	O
in	O
each	O
board	O
state	B
.	O
let	O
us	O
assume	O
that	O
this	O
information	O
xxxooxo	O
1.5.	O
an	O
extended	O
example	O
:	O
tic-tac-toe	B
9	O
is	O
not	O
available	O
a	O
priori	O
for	O
this	O
problem	O
,	O
as	O
it	O
is	O
not	O
for	O
the	O
vast	O
majority	O
of	O
problems	O
of	O
practical	O
interest	O
.	O
on	O
the	O
other	O
hand	O
,	O
such	O
information	O
can	O
be	O
estimated	O
from	O
experience	O
,	O
in	O
this	O
case	O
by	O
playing	O
many	O
games	O
against	O
the	O
opponent	O
.	O
about	O
the	O
best	O
one	O
can	O
do	O
on	O
this	O
problem	O
is	O
ﬁrst	O
to	O
learn	O
a	O
model	O
of	O
the	O
opponent	O
’	O
s	O
behavior	O
,	O
up	O
to	O
some	O
level	O
of	O
conﬁdence	O
,	O
and	O
then	O
apply	O
dynamic	B
programming	I
to	O
compute	O
an	O
optimal	O
solution	O
given	O
the	O
approximate	B
opponent	O
model	O
.	O
in	O
the	O
end	O
,	O
this	O
is	O
not	O
that	O
diﬀerent	O
from	O
some	O
of	O
the	O
reinforcement	B
learning	I
methods	O
we	O
examine	O
later	O
in	O
this	O
book	O
.	O
an	O
evolutionary	O
method	O
applied	O
to	O
this	O
problem	O
would	O
directly	O
search	O
the	O
space	O
of	O
possible	O
policies	O
for	O
one	O
with	O
a	O
high	O
probability	O
of	O
winning	O
against	O
the	O
opponent	O
.	O
here	O
,	O
a	O
policy	B
is	O
a	O
rule	O
that	O
tells	O
the	O
player	O
what	O
move	O
to	O
make	O
for	O
every	O
state	B
of	O
the	O
game—	O
every	O
possible	O
conﬁguration	O
of	O
xs	O
and	O
os	O
on	O
the	O
three-by-three	O
board	O
.	O
for	O
each	O
policy	B
considered	O
,	O
an	O
estimate	O
of	O
its	O
winning	O
probability	O
would	O
be	O
obtained	O
by	O
playing	O
some	O
number	O
of	O
games	O
against	O
the	O
opponent	O
.	O
this	O
evaluation	O
would	O
then	O
direct	O
which	O
policy	B
or	O
policies	O
were	O
considered	O
next	O
.	O
a	O
typical	O
evolutionary	O
method	O
would	O
hill-climb	O
in	O
policy	O
space	O
,	O
successively	O
generating	O
and	O
evaluating	O
policies	O
in	O
an	O
attempt	O
to	O
obtain	O
incremental	O
improvements	O
.	O
or	O
,	O
perhaps	O
,	O
a	O
genetic-style	O
algorithm	O
could	O
be	O
used	O
that	O
would	O
maintain	O
and	O
evaluate	O
a	O
population	O
of	O
policies	O
.	O
literally	O
hundreds	O
of	O
diﬀerent	O
optimization	O
methods	O
could	O
be	O
applied	O
.	O
here	O
is	O
how	O
the	O
tic-tac-toe	B
problem	O
would	O
be	O
approached	O
with	O
a	O
method	O
making	O
use	O
of	O
a	O
value	B
function	I
.	O
first	O
we	O
set	O
up	O
a	O
table	O
of	O
numbers	O
,	O
one	O
for	O
each	O
possible	O
state	B
of	O
the	O
game	O
.	O
each	O
number	O
will	O
be	O
the	O
latest	O
estimate	O
of	O
the	O
probability	O
of	O
our	O
winning	O
from	O
that	O
state	B
.	O
we	O
treat	O
this	O
estimate	O
as	O
the	O
state	B
’	O
s	O
value	B
,	O
and	O
the	O
whole	O
table	O
is	O
the	O
learned	O
value	B
function	I
.	O
state	B
a	O
has	O
higher	O
value	B
than	O
state	B
b	O
,	O
or	O
is	O
considered	O
“	O
better	O
”	O
than	O
state	B
b	O
,	O
if	O
the	O
current	O
estimate	O
of	O
the	O
probability	O
of	O
our	O
winning	O
from	O
a	O
is	O
higher	O
than	O
it	O
is	O
from	O
b.	O
assuming	O
we	O
always	O
play	O
xs	O
,	O
then	O
for	O
all	O
states	O
with	O
three	O
xs	O
in	O
a	O
row	O
the	O
probability	O
of	O
winning	O
is	O
1	O
,	O
because	O
we	O
have	O
already	O
won	O
.	O
similarly	O
,	O
for	O
all	O
states	O
with	O
three	O
os	O
in	O
a	O
row	O
,	O
or	O
that	O
are	O
“	O
ﬁlled	O
up	O
,	O
”	O
the	O
correct	O
probability	O
is	O
0	O
,	O
as	O
we	O
can	O
not	O
win	O
from	O
them	O
.	O
we	O
set	O
the	O
initial	O
values	O
of	O
all	O
the	O
other	O
states	O
to	O
0.5	O
,	O
representing	O
a	O
guess	O
that	O
we	O
have	O
a	O
50	O
%	O
chance	O
of	O
winning	O
.	O
we	O
play	O
many	O
games	O
against	O
the	O
opponent	O
.	O
to	O
select	O
our	O
moves	O
we	O
examine	O
the	O
states	O
that	O
would	O
result	O
from	O
each	O
of	O
our	O
possible	O
moves	O
(	O
one	O
for	O
each	O
blank	O
space	O
on	O
the	O
board	O
)	O
and	O
look	O
up	O
their	O
current	O
values	O
in	O
the	O
table	O
.	O
most	O
of	O
the	O
time	O
we	O
move	O
greedily	O
,	O
selecting	O
the	O
move	O
that	O
leads	O
to	O
the	O
state	B
with	O
greatest	O
value	B
,	O
that	O
is	O
,	O
with	O
the	O
highest	O
estimated	O
probability	O
of	O
winning	O
.	O
occasionally	O
,	O
however	O
,	O
we	O
select	O
randomly	O
from	O
among	O
the	O
other	O
moves	O
instead	O
.	O
these	O
are	O
called	O
exploratory	O
moves	O
because	O
they	O
cause	O
us	O
to	O
experience	O
states	O
that	O
we	O
might	O
otherwise	O
never	O
see	O
.	O
a	O
sequence	O
of	O
moves	O
made	O
and	O
considered	O
during	O
a	O
game	O
can	O
be	O
diagrammed	O
as	O
in	O
figure	O
1.1.	O
while	O
we	O
are	O
playing	O
,	O
we	O
change	O
the	O
values	O
of	O
the	O
states	O
in	O
which	O
we	O
ﬁnd	O
ourselves	O
during	O
the	O
game	O
.	O
we	O
attempt	O
to	O
make	O
them	O
more	O
accurate	O
estimates	O
of	O
the	O
probabilities	O
of	O
winning	O
.	O
to	O
do	O
this	O
,	O
we	O
“	O
back	O
up	O
”	O
the	O
value	B
of	O
the	O
state	B
after	O
each	O
greedy	O
move	O
to	O
the	O
state	B
before	O
the	O
move	O
,	O
as	O
suggested	O
by	O
the	O
arrows	O
in	O
figure	O
1.1.	O
more	O
precisely	O
,	O
the	O
current	O
value	B
of	O
the	O
earlier	O
state	B
is	O
updated	O
to	O
be	O
closer	O
to	O
the	O
value	B
of	O
the	O
later	O
state	B
.	O
this	O
can	O
be	O
done	O
by	O
moving	O
the	O
earlier	O
state	B
’	O
s	O
value	B
a	O
fraction	O
of	O
the	O
way	O
toward	O
the	O
value	B
of	O
the	O
later	O
state	B
.	O
if	O
we	O
let	O
s	O
denote	O
the	O
state	B
before	O
the	O
greedy	O
move	O
,	O
and	O
s	O
(	O
cid:48	O
)	O
the	O
10	O
chapter	O
1	O
:	O
introduction	O
figure	O
1.1	O
:	O
a	O
sequence	O
of	O
tic-tac-toe	O
moves	O
.	O
the	O
solid	O
lines	O
represent	O
the	O
moves	O
taken	O
during	O
a	O
game	O
;	O
the	O
dashed	O
lines	O
represent	O
moves	O
that	O
we	O
(	O
our	O
reinforcement	B
learning	I
player	O
)	O
considered	O
but	O
did	O
not	O
make	O
.	O
our	O
second	O
move	O
was	O
an	O
exploratory	O
move	O
,	O
meaning	O
that	O
it	O
was	O
taken	O
even	O
though	O
another	O
sibling	O
move	O
,	O
the	O
one	O
leading	O
to	O
e∗	O
,	O
was	O
ranked	O
higher	O
.	O
exploratory	O
moves	O
do	O
not	O
result	O
in	O
any	O
learning	O
,	O
but	O
each	O
of	O
our	O
other	O
moves	O
does	O
,	O
causing	O
updates	O
as	O
suggested	O
by	O
the	O
red	O
arrows	O
in	O
which	O
estimated	O
values	O
are	O
moved	O
up	O
the	O
tree	O
from	O
later	O
nodes	O
to	O
earlier	O
as	O
detailed	O
in	O
the	O
text	O
.	O
state	B
after	O
the	O
move	O
,	O
then	O
the	O
update	O
to	O
the	O
estimated	O
value	B
of	O
s	O
,	O
denoted	O
v	O
(	O
s	O
)	O
,	O
can	O
be	O
written	O
as	O
v	O
(	O
s	O
)	O
←	O
v	O
(	O
s	O
)	O
+	O
α	O
(	O
cid:104	O
)	O
v	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
−	O
v	O
(	O
s	O
)	O
(	O
cid:105	O
)	O
,	O
where	O
α	O
is	O
a	O
small	O
positive	O
fraction	O
called	O
the	O
step-size	B
parameter	I
,	O
which	O
inﬂuences	O
the	O
rate	O
of	O
learning	O
.	O
this	O
update	O
rule	O
is	O
an	O
example	O
of	O
a	O
temporal-diﬀerence	B
learning	I
method	O
,	O
so	O
called	O
because	O
its	O
changes	O
are	O
based	O
on	O
a	O
diﬀerence	O
,	O
v	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
−	O
v	O
(	O
s	O
)	O
,	O
between	O
estimates	O
at	O
two	O
diﬀerent	O
times	O
.	O
the	O
method	O
described	O
above	O
performs	O
quite	O
well	O
on	O
this	O
task	O
.	O
for	O
example	O
,	O
if	O
the	O
step-size	B
parameter	I
is	O
reduced	O
properly	O
over	O
time	O
,	O
then	O
this	O
method	O
converges	O
,	O
for	O
any	O
ﬁxed	O
opponent	O
,	O
to	O
the	O
true	O
probabilities	O
of	O
winning	O
from	O
each	O
state	B
given	O
optimal	O
play	O
by	O
our	O
player	O
.	O
furthermore	O
,	O
the	O
moves	O
then	O
taken	O
(	O
except	O
on	O
exploratory	O
moves	O
)	O
are	O
in	O
fact	O
the	O
optimal	O
moves	O
against	O
this	O
(	O
imperfect	O
)	O
opponent	O
.	O
in	O
other	O
words	O
,	O
the	O
method	O
converges	O
to	O
an	O
optimal	O
policy	O
for	O
playing	O
the	O
game	O
against	O
this	O
opponent	O
.	O
if	O
the	O
step-	O
size	O
parameter	O
is	O
not	O
reduced	O
all	O
the	O
way	O
to	O
zero	O
over	O
time	O
,	O
then	O
this	O
player	O
also	O
plays	O
well	O
against	O
opponents	O
that	O
slowly	O
change	O
their	O
way	O
of	O
playing	O
.	O
..•our	O
move	O
{	O
opponent	O
's	O
move	O
{	O
our	O
move	O
{	O
starting	O
position•••abc*dee*opponent	O
's	O
move	O
{	O
c•f•g*gopponent	O
's	O
move	O
{	O
our	O
move	O
{	O
.•ag*cstarting	O
positionbc*de*efg…	O
1.5.	O
an	O
extended	O
example	O
:	O
tic-tac-toe	B
11	O
this	O
example	O
illustrates	O
the	O
diﬀerences	O
between	O
evolutionary	B
methods	I
and	O
methods	O
that	O
learn	O
value	B
functions	O
.	O
to	O
evaluate	O
a	O
policy	B
an	O
evolutionary	O
method	O
holds	O
the	O
policy	B
ﬁxed	O
and	O
plays	O
many	O
games	O
against	O
the	O
opponent	O
,	O
or	O
simulates	O
many	O
games	O
using	O
a	O
model	O
of	O
the	O
opponent	O
.	O
the	O
frequency	O
of	O
wins	O
gives	O
an	O
unbiased	O
estimate	O
of	O
the	O
prob-	O
ability	O
of	O
winning	O
with	O
that	O
policy	B
,	O
and	O
can	O
be	O
used	O
to	O
direct	O
the	O
next	O
policy	B
selection	O
.	O
but	O
each	O
policy	B
change	O
is	O
made	O
only	O
after	O
many	O
games	O
,	O
and	O
only	O
the	O
ﬁnal	O
outcome	O
of	O
each	O
game	O
is	O
used	O
:	O
what	O
happens	O
during	O
the	O
games	O
is	O
ignored	O
.	O
for	O
example	O
,	O
if	O
the	O
player	O
wins	O
,	O
then	O
all	O
of	O
its	O
behavior	O
in	O
the	O
game	O
is	O
given	O
credit	O
,	O
independently	O
of	O
how	O
speciﬁc	O
moves	O
might	O
have	O
been	O
critical	O
to	O
the	O
win	O
.	O
credit	O
is	O
even	O
given	O
to	O
moves	O
that	O
never	O
occurred	O
!	O
value	B
function	I
methods	O
,	O
in	O
contrast	O
,	O
allow	O
individual	O
states	O
to	O
be	O
evaluated	O
.	O
in	O
the	O
end	O
,	O
evolutionary	O
and	O
value	B
function	I
methods	O
both	O
search	O
the	O
space	O
of	O
policies	O
,	O
but	O
learning	O
a	O
value	B
function	I
takes	O
advantage	O
of	O
information	O
available	O
during	O
the	O
course	O
of	O
play	O
.	O
this	O
simple	O
example	O
illustrates	O
some	O
of	O
the	O
key	O
features	O
of	O
reinforcement	O
learning	O
methods	O
.	O
first	O
,	O
there	O
is	O
the	O
emphasis	O
on	O
learning	O
while	O
interacting	O
with	O
an	O
environment	B
,	O
in	O
this	O
case	O
with	O
an	O
opponent	O
player	O
.	O
second	O
,	O
there	O
is	O
a	O
clear	O
goal	B
,	O
and	O
correct	O
behavior	O
requires	O
planning	B
or	O
foresight	O
that	O
takes	O
into	O
account	O
delayed	O
eﬀects	O
of	O
one	O
’	O
s	O
choices	O
.	O
for	O
example	O
,	O
the	O
simple	O
reinforcement	B
learning	I
player	O
would	O
learn	O
to	O
set	O
up	O
multi-move	O
traps	O
for	O
a	O
shortsighted	O
opponent	O
.	O
it	O
is	O
a	O
striking	O
feature	O
of	O
the	O
reinforcement	B
learning	I
solution	O
that	O
it	O
can	O
achieve	O
the	O
eﬀects	O
of	O
planning	O
and	O
lookahead	O
without	O
using	O
a	O
model	O
of	O
the	O
opponent	O
and	O
without	O
conducting	O
an	O
explicit	O
search	O
over	O
possible	O
sequences	O
of	O
future	O
states	O
and	O
actions	O
.	O
while	O
this	O
example	O
illustrates	O
some	O
of	O
the	O
key	O
features	O
of	O
reinforcement	O
learning	O
,	O
it	O
is	O
so	O
simple	O
that	O
it	O
might	O
give	O
the	O
impression	O
that	O
reinforcement	B
learning	I
is	O
more	O
limited	O
than	O
it	O
really	O
is	O
.	O
although	O
tic-tac-toe	B
is	O
a	O
two-person	O
game	O
,	O
reinforcement	B
learning	I
also	O
applies	O
in	O
the	O
case	O
in	O
which	O
there	O
is	O
no	O
external	O
adversary	O
,	O
that	O
is	O
,	O
in	O
the	O
case	O
of	O
a	O
“	O
game	O
against	O
nature.	O
”	O
reinforcement	B
learning	I
also	O
is	O
not	O
restricted	O
to	O
problems	O
in	O
which	O
behavior	O
breaks	O
down	O
into	O
separate	O
episodes	B
,	O
like	O
the	O
separate	O
games	O
of	O
tic-tac-	O
toe	O
,	O
with	O
reward	O
only	O
at	O
the	O
end	O
of	O
each	O
episode	O
.	O
it	O
is	O
just	O
as	O
applicable	O
when	O
behavior	O
continues	O
indeﬁnitely	O
and	O
when	O
rewards	O
of	O
various	O
magnitudes	O
can	O
be	O
received	O
at	O
any	O
time	O
.	O
reinforcement	B
learning	I
is	O
also	O
applicable	O
to	O
problems	O
that	O
do	O
not	O
even	O
break	O
down	O
into	O
discrete	O
time	O
steps	O
,	O
like	O
the	O
plays	O
of	O
tic-tac-toe	O
.	O
the	O
general	O
principles	O
apply	O
to	O
continuous-time	O
problems	O
as	O
well	O
,	O
although	O
the	O
theory	O
gets	O
more	O
complicated	O
and	O
we	O
omit	O
it	O
from	O
this	O
introductory	O
treatment	O
.	O
tic-tac-toe	B
has	O
a	O
relatively	O
small	O
,	O
ﬁnite	O
state	B
set	O
,	O
whereas	O
reinforcement	B
learning	I
can	O
be	O
used	O
when	O
the	O
state	B
set	O
is	O
very	O
large	O
,	O
or	O
even	O
inﬁnite	O
.	O
for	O
example	O
,	O
gerry	O
tesauro	O
(	O
1992	O
,	O
1995	O
)	O
combined	O
the	O
algorithm	O
described	O
above	O
with	O
an	O
artiﬁcial	O
neural	O
network	O
to	O
learn	O
to	O
play	O
backgammon	B
,	O
which	O
has	O
approximately	O
1020	O
states	O
.	O
with	O
this	O
many	O
states	O
it	O
is	O
impossible	O
ever	O
to	O
experience	O
more	O
than	O
a	O
small	O
fraction	O
of	O
them	O
.	O
tesauro	O
’	O
s	O
program	O
learned	O
to	O
play	O
far	O
better	O
than	O
any	O
previous	O
program	O
,	O
and	O
now	O
plays	O
at	O
the	O
level	O
of	O
the	O
world	O
’	O
s	O
best	O
human	O
players	O
(	O
see	O
chapter	O
16	O
)	O
.	O
the	O
neural	B
network	O
provides	O
the	O
program	O
with	O
the	O
ability	O
to	O
generalize	O
from	O
its	O
experience	O
,	O
so	O
that	O
in	O
new	O
states	O
it	O
selects	O
moves	O
based	O
on	O
information	O
saved	O
from	O
similar	O
states	O
faced	O
in	O
the	O
past	O
,	O
as	O
determined	O
by	O
its	O
network	O
.	O
how	O
well	O
a	O
reinforcement	B
learning	I
system	O
can	O
work	O
in	O
problems	O
with	O
such	O
large	O
12	O
chapter	O
1	O
:	O
introduction	O
state	B
sets	O
is	O
intimately	O
tied	O
to	O
how	O
appropriately	O
it	O
can	O
generalize	O
from	O
past	O
experience	O
.	O
it	O
is	O
in	O
this	O
role	O
that	O
we	O
have	O
the	O
greatest	O
need	O
for	O
supervised	O
learning	O
methods	O
with	O
reinforcement	O
learning	O
.	O
neural	B
networks	I
and	O
deep	B
learning	I
(	O
section	O
9.7	O
)	O
are	O
not	O
the	O
only	O
,	O
or	O
necessarily	O
the	O
best	O
,	O
way	O
to	O
do	O
this	O
.	O
in	O
this	O
tic-tac-toe	B
example	O
,	O
learning	O
started	O
with	O
no	O
prior	B
knowledge	I
beyond	O
the	O
rules	O
of	O
the	O
game	O
,	O
but	O
reinforcement	B
learning	I
by	O
no	O
means	O
entails	O
a	O
tabula	O
rasa	O
view	O
of	O
learning	O
and	O
intelligence	O
.	O
on	O
the	O
contrary	O
,	O
prior	O
information	O
can	O
be	O
incorporated	O
into	O
reinforce-	O
ment	O
learning	O
in	O
a	O
variety	O
of	O
ways	O
that	O
can	O
be	O
critical	O
for	O
eﬃcient	O
learning	O
.	O
we	O
also	O
had	O
access	O
to	O
the	O
true	O
state	O
in	O
the	O
tic-tac-toe	O
example	O
,	O
whereas	O
reinforcement	B
learning	I
can	O
also	O
be	O
applied	O
when	O
part	O
of	O
the	O
state	B
is	O
hidden	O
,	O
or	O
when	O
diﬀerent	O
states	O
appear	O
to	O
the	O
learner	O
to	O
be	O
the	O
same	O
.	O
finally	O
,	O
the	O
tic-tac-toe	B
player	O
was	O
able	O
to	O
look	O
ahead	O
and	O
know	O
the	O
states	O
that	O
would	O
result	O
from	O
each	O
of	O
its	O
possible	O
moves	O
.	O
to	O
do	O
this	O
,	O
it	O
had	O
to	O
have	O
a	O
model	O
of	O
the	O
game	O
that	O
allowed	O
it	O
to	O
foresee	O
how	O
its	O
environment	B
would	O
change	O
in	O
response	O
to	O
moves	O
that	O
it	O
might	O
never	O
make	O
.	O
many	O
problems	O
are	O
like	O
this	O
,	O
but	O
in	O
others	O
even	O
a	O
short-term	O
model	O
of	O
the	O
eﬀects	O
of	O
actions	O
is	O
lacking	O
.	O
reinforcement	B
learning	I
can	O
be	O
applied	O
in	O
either	O
case	O
.	O
no	O
model	O
is	O
required	O
,	O
but	O
models	O
can	O
easily	O
be	O
used	O
if	O
they	O
are	O
available	O
or	O
can	O
be	O
learned	O
(	O
chapter	O
8	O
)	O
.	O
on	O
the	O
other	O
hand	O
,	O
there	O
are	O
reinforcement	B
learning	I
methods	O
that	O
do	O
not	O
need	O
any	O
kind	O
of	O
environment	O
model	O
at	O
all	O
.	O
model-free	O
systems	O
can	O
not	O
even	O
think	O
about	O
how	O
their	O
environments	O
will	O
change	O
in	O
response	O
to	O
a	O
single	O
action	B
.	O
the	O
tic-tac-toe	B
player	O
is	O
model-	O
free	O
in	O
this	O
sense	O
with	O
respect	O
to	O
its	O
opponent	O
:	O
it	O
has	O
no	O
model	O
of	O
its	O
opponent	O
of	O
any	O
kind	O
.	O
because	O
models	O
have	O
to	O
be	O
reasonably	O
accurate	O
to	O
be	O
useful	O
,	O
model-free	O
methods	O
can	O
have	O
advantages	O
over	O
more	O
complex	O
methods	O
when	O
the	O
real	O
bottleneck	O
in	O
solving	O
a	O
problem	O
is	O
the	O
diﬃculty	O
of	O
constructing	O
a	O
suﬃciently	O
accurate	O
environment	B
model	O
.	O
model-free	O
methods	O
are	O
also	O
important	O
building	O
blocks	O
for	O
model-based	O
methods	O
.	O
in	O
this	O
book	O
we	O
devote	O
several	O
chapters	O
to	O
model-free	O
methods	O
before	O
we	O
discuss	O
how	O
they	O
can	O
be	O
used	O
as	O
components	O
of	O
more	O
complex	O
model-based	O
methods	O
.	O
reinforcement	B
learning	I
can	O
be	O
used	O
at	O
both	O
high	O
and	O
low	O
levels	O
in	O
a	O
system	O
.	O
although	O
the	O
tic-tac-toe	B
player	O
learned	O
only	O
about	O
the	O
basic	O
moves	O
of	O
the	O
game	O
,	O
nothing	O
prevents	O
reinforcement	B
learning	I
from	O
working	O
at	O
higher	O
levels	O
where	O
each	O
of	O
the	O
“	O
actions	O
”	O
may	O
itself	O
be	O
the	O
application	O
of	O
a	O
possibly	O
elaborate	O
problem-solving	O
method	O
.	O
in	O
hierarchical	O
learning	O
systems	O
,	O
reinforcement	B
learning	I
can	O
work	O
simultaneously	O
on	O
several	O
levels	O
.	O
exercise	O
1.1	O
:	O
self-play	O
suppose	O
,	O
instead	O
of	O
playing	O
against	O
a	O
random	O
opponent	O
,	O
the	O
reinforcement	B
learning	I
algorithm	O
described	O
above	O
played	O
against	O
itself	O
,	O
with	O
both	O
sides	O
learning	O
.	O
what	O
do	O
you	O
think	O
would	O
happen	O
in	O
this	O
case	O
?	O
would	O
it	O
learn	O
a	O
diﬀerent	O
policy	B
(	O
cid:3	O
)	O
for	O
selecting	O
moves	O
?	O
exercise	O
1.2	O
:	O
symmetries	O
many	O
tic-tac-toe	B
positions	O
appear	O
diﬀerent	O
but	O
are	O
really	O
the	O
same	O
because	O
of	O
symmetries	O
.	O
how	O
might	O
we	O
amend	O
the	O
learning	O
process	O
described	O
above	O
to	O
take	O
advantage	O
of	O
this	O
?	O
in	O
what	O
ways	O
would	O
this	O
change	O
improve	O
the	O
learning	O
process	O
?	O
now	O
think	O
again	O
.	O
suppose	O
the	O
opponent	O
did	O
not	O
take	O
advantage	O
of	O
symmetries	O
.	O
in	O
that	O
case	O
,	O
should	O
we	O
?	O
is	O
it	O
true	O
,	O
then	O
,	O
that	O
symmetrically	O
equivalent	O
positions	O
should	O
(	O
cid:3	O
)	O
necessarily	O
have	O
the	O
same	O
value	B
?	O
exercise	O
1.3	O
:	O
greedy	O
play	O
suppose	O
the	O
reinforcement	B
learning	I
player	O
was	O
greedy	O
,	O
that	O
1.7.	O
early	O
history	B
of	I
reinforcement	I
learning	I
13	O
is	O
,	O
it	O
always	O
played	O
the	O
move	O
that	O
brought	O
it	O
to	O
the	O
position	O
that	O
it	O
rated	O
the	O
best	O
.	O
might	O
it	O
learn	O
to	O
play	O
better	O
,	O
or	O
worse	O
,	O
than	O
a	O
nongreedy	O
player	O
?	O
what	O
problems	O
might	O
occur	O
?	O
(	O
cid:3	O
)	O
exercise	O
1.4	O
:	O
learning	O
from	O
exploration	O
suppose	O
learning	O
updates	O
occurred	O
after	O
all	O
moves	O
,	O
including	O
exploratory	O
moves	O
.	O
if	O
the	O
step-size	B
parameter	I
is	O
appropriately	O
reduced	O
over	O
time	O
(	O
but	O
not	O
the	O
tendency	O
to	O
explore	O
)	O
,	O
then	O
the	O
state	B
values	O
would	O
converge	O
to	O
a	O
set	O
of	O
probabilities	O
.	O
what	O
are	O
the	O
two	O
sets	O
of	O
probabilities	O
computed	O
when	O
we	O
do	O
,	O
and	O
when	O
we	O
do	O
not	O
,	O
learn	O
from	O
exploratory	O
moves	O
?	O
assuming	O
that	O
we	O
do	O
continue	O
to	O
make	O
exploratory	O
moves	O
,	O
which	O
set	O
of	O
probabilities	O
might	O
be	O
better	O
to	O
learn	O
?	O
which	O
would	O
(	O
cid:3	O
)	O
result	O
in	O
more	O
wins	O
?	O
exercise	O
1.5	O
:	O
other	O
improvements	O
can	O
you	O
think	O
of	O
other	O
ways	O
to	O
improve	O
the	O
re-	O
inforcement	O
learning	O
player	O
?	O
can	O
you	O
think	O
of	O
any	O
better	O
way	O
to	O
solve	O
the	O
tic-tac-toe	B
(	O
cid:3	O
)	O
problem	O
as	O
posed	O
?	O
1.6	O
summary	O
reinforcement	B
learning	I
is	O
a	O
computational	O
approach	O
to	O
understanding	O
and	O
automating	O
goal-directed	O
learning	O
and	O
decision	O
making	O
.	O
it	O
is	O
distinguished	O
from	O
other	O
computa-	O
tional	O
approaches	O
by	O
its	O
emphasis	O
on	O
learning	O
by	O
an	O
agent	O
from	O
direct	O
interaction	O
with	O
its	O
environment	B
,	O
without	O
relying	O
on	O
exemplary	O
supervision	O
or	O
complete	O
models	O
of	O
the	O
environment	B
.	O
in	O
our	O
opinion	O
,	O
reinforcement	B
learning	I
is	O
the	O
ﬁrst	O
ﬁeld	O
to	O
seriously	O
address	O
the	O
computational	O
issues	O
that	O
arise	O
when	O
learning	O
from	O
interaction	O
with	O
an	O
environment	B
in	O
order	O
to	O
achieve	O
long-term	O
goals	O
.	O
reinforcement	B
learning	I
uses	O
the	O
formal	O
framework	O
of	O
markov	O
decision	O
processes	O
to	O
deﬁne	O
the	O
interaction	O
between	O
a	O
learning	O
agent	O
and	O
its	O
environment	B
in	O
terms	O
of	O
states	O
,	O
actions	O
,	O
and	O
rewards	O
.	O
this	O
framework	O
is	O
intended	O
to	O
be	O
a	O
simple	O
way	O
of	O
representing	O
essential	O
features	O
of	O
the	O
artiﬁcial	B
intelligence	I
problem	O
.	O
these	O
features	O
include	O
a	O
sense	O
of	O
cause	O
and	O
eﬀect	O
,	O
a	O
sense	O
of	O
uncertainty	O
and	O
nondeterminism	O
,	O
and	O
the	O
existence	O
of	O
explicit	O
goals	O
.	O
the	O
concepts	O
of	O
value	O
and	O
value	O
function	O
are	O
key	O
to	O
most	O
of	O
the	O
reinforcement	B
learning	I
methods	O
that	O
we	O
consider	O
in	O
this	O
book	O
.	O
we	O
take	O
the	O
position	O
that	O
value	B
functions	O
are	O
important	O
for	O
eﬃcient	O
search	O
in	O
the	O
space	O
of	O
policies	O
.	O
the	O
use	O
of	O
value	O
functions	O
distin-	O
guishes	O
reinforcement	B
learning	I
methods	O
from	O
evolutionary	B
methods	I
that	O
search	O
directly	O
in	O
policy	O
space	O
guided	O
by	O
scalar	O
evaluations	O
of	O
entire	O
policies	O
.	O
1.7	O
early	O
history	B
of	I
reinforcement	I
learning	I
the	O
early	O
history	B
of	I
reinforcement	I
learning	I
has	O
two	O
main	O
threads	O
,	O
both	O
long	O
and	O
rich	O
,	O
that	O
were	O
pursued	O
independently	O
before	O
intertwining	O
in	O
modern	O
reinforcement	B
learning	I
.	O
one	O
thread	O
concerns	O
learning	O
by	O
trial	O
and	O
error	O
that	O
started	O
in	O
the	O
psychology	O
of	O
animal	O
learning	O
.	O
this	O
thread	O
runs	O
through	O
some	O
of	O
the	O
earliest	O
work	O
in	O
artiﬁcial	O
intelligence	O
and	O
led	O
to	O
the	O
revival	O
of	O
reinforcement	O
learning	O
in	O
the	O
early	O
1980s	O
.	O
the	O
other	O
thread	O
14	O
chapter	O
1	O
:	O
introduction	O
concerns	O
the	O
problem	O
of	O
optimal	O
control	O
and	O
its	O
solution	O
using	O
value	B
functions	O
and	O
dy-	O
namic	O
programming	O
.	O
for	O
the	O
most	O
part	O
,	O
this	O
thread	O
did	O
not	O
involve	O
learning	O
.	O
although	O
the	O
two	O
threads	O
have	O
been	O
largely	O
independent	O
,	O
the	O
exceptions	O
revolve	O
around	O
a	O
third	O
,	O
less	O
distinct	O
thread	O
concerning	O
temporal-diﬀerence	O
methods	O
such	O
as	O
the	O
one	O
used	O
in	O
the	O
tic-tac-toe	O
example	O
in	O
this	O
chapter	O
.	O
all	O
three	O
threads	O
came	O
together	O
in	O
the	O
late	O
1980s	O
to	O
produce	O
the	O
modern	O
ﬁeld	O
of	O
reinforcement	O
learning	O
as	O
we	O
present	O
it	O
in	O
this	O
book	O
.	O
the	O
thread	O
focusing	O
on	O
trial-and-error	B
learning	O
is	O
the	O
one	O
with	O
which	O
we	O
are	O
most	O
familiar	O
and	O
about	O
which	O
we	O
have	O
the	O
most	O
to	O
say	O
in	O
this	O
brief	O
history	O
.	O
before	O
doing	O
that	O
,	O
however	O
,	O
we	O
brieﬂy	O
discuss	O
the	O
optimal	B
control	I
thread	O
.	O
the	O
term	O
“	O
optimal	B
control	I
”	O
came	O
into	O
use	O
in	O
the	O
late	O
1950s	O
to	O
describe	O
the	O
problem	O
of	O
designing	O
a	O
controller	O
to	O
minimize	O
a	O
measure	O
of	O
a	O
dynamical	O
system	O
’	O
s	O
behavior	O
over	O
time	O
.	O
one	O
of	O
the	O
approaches	O
to	O
this	O
problem	O
was	O
developed	O
in	O
the	O
mid-1950s	O
by	O
richard	O
bell-	O
man	O
and	O
others	O
through	O
extending	O
a	O
nineteenth	O
century	O
theory	O
of	O
hamilton	O
and	O
jacobi	O
.	O
this	O
approach	O
uses	O
the	O
concepts	O
of	O
a	O
dynamical	O
system	O
’	O
s	O
state	B
and	O
of	O
a	O
value	B
function	I
,	O
or	O
“	O
optimal	O
return	O
function	O
,	O
”	O
to	O
deﬁne	O
a	O
functional	O
equation	O
,	O
now	O
often	O
called	O
the	O
bell-	O
man	O
equation	O
.	O
the	O
class	O
of	O
methods	O
for	O
solving	O
optimal	B
control	I
problems	O
by	O
solving	O
this	O
equation	O
came	O
to	O
be	O
known	O
as	O
dynamic	O
programming	O
(	O
bellman	O
,	O
1957a	O
)	O
.	O
bellman	O
(	O
1957b	O
)	O
also	O
introduced	O
the	O
discrete	O
stochastic	O
version	O
of	O
the	O
optimal	B
control	I
problem	O
known	O
as	O
markov	O
decision	O
processes	O
(	O
mdps	O
)	O
,	O
and	O
ronald	O
howard	O
(	O
1960	O
)	O
devised	O
the	O
policy	B
iteration	I
method	O
for	O
mdps	O
.	O
all	O
of	O
these	O
are	O
essential	O
elements	O
underlying	O
the	O
theory	O
and	O
algorithms	O
of	O
modern	O
reinforcement	B
learning	I
.	O
dynamic	B
programming	I
is	O
widely	O
considered	O
the	O
only	O
feasible	O
way	O
of	O
solving	O
general	O
stochastic	O
optimal	B
control	I
problems	O
.	O
it	O
suﬀers	O
from	O
what	O
bellman	O
called	O
“	O
the	O
curse	B
of	I
dimensionality	I
,	O
”	O
meaning	O
that	O
its	O
computational	O
requirements	O
grow	O
exponentially	O
with	O
the	O
number	O
of	O
state	O
variables	O
,	O
but	O
it	O
is	O
still	O
far	O
more	O
eﬃcient	O
and	O
more	O
widely	O
applicable	O
than	O
any	O
other	O
general	O
method	O
.	O
dynamic	B
programming	I
has	O
been	O
extensively	O
developed	O
since	O
the	O
late	O
1950s	O
,	O
including	O
extensions	O
to	O
partially	O
observable	O
mdps	O
(	O
surveyed	O
by	O
lovejoy	O
,	O
1991	O
)	O
,	O
many	O
applications	O
(	O
surveyed	O
by	O
white	O
,	O
1985	O
,	O
1988	O
,	O
1993	O
)	O
,	O
approxima-	O
tion	B
methods	O
(	O
surveyed	O
by	O
rust	O
,	O
1996	O
)	O
,	O
and	O
asynchronous	O
methods	O
(	O
bertsekas	O
,	O
1982	O
,	O
1983	O
)	O
.	O
many	O
excellent	O
modern	O
treatments	O
of	O
dynamic	O
programming	O
are	O
available	O
(	O
e.g.	O
,	O
bertsekas	O
,	O
2005	O
,	O
2012	O
;	O
puterman	O
,	O
1994	O
;	O
ross	O
,	O
1983	O
;	O
and	O
whittle	O
,	O
1982	O
,	O
1983	O
)	O
.	O
bryson	O
(	O
1996	O
)	O
provides	O
an	O
authoritative	O
history	B
of	I
optimal	O
control	B
.	O
connections	O
between	O
optimal	B
control	I
and	O
dynamic	B
programming	I
,	O
on	O
the	O
one	O
hand	O
,	O
and	O
learning	O
,	O
on	O
the	O
other	O
,	O
were	O
slow	O
to	O
be	O
recognized	O
.	O
we	O
can	O
not	O
be	O
sure	O
about	O
what	O
accounted	O
for	O
this	O
separation	O
,	O
but	O
its	O
main	O
cause	O
was	O
likely	O
the	O
separation	O
between	O
the	O
disciplines	O
involved	O
and	O
their	O
diﬀerent	O
goals	O
.	O
also	O
contributing	O
may	O
have	O
been	O
the	O
preva-	O
lent	O
view	O
of	O
dynamic	O
programming	O
as	O
an	O
oﬀ-line	B
computation	O
depending	O
essentially	O
on	O
accurate	O
system	O
models	O
and	O
analytic	O
solutions	O
to	O
the	O
bellman	O
equation	O
.	O
further	O
,	O
the	O
simplest	O
form	O
of	O
dynamic	O
programming	O
is	O
a	O
computation	O
that	O
proceeds	O
backwards	O
in	O
time	O
,	O
making	O
it	O
diﬃcult	O
to	O
see	O
how	O
it	O
could	O
be	O
involved	O
in	O
a	O
learning	O
process	O
that	O
must	O
proceed	O
in	O
a	O
forward	O
direction	O
.	O
some	O
of	O
the	O
earliest	O
work	O
in	O
dynamic	O
programming	O
,	O
such	O
as	O
that	O
by	O
bellman	O
and	O
dreyfus	O
(	O
1959	O
)	O
,	O
might	O
now	O
be	O
classiﬁed	O
as	O
following	O
a	O
learning	O
approach	O
.	O
witten	O
’	O
s	O
(	O
1977	O
)	O
work	O
(	O
discussed	O
below	O
)	O
certainly	O
qualiﬁes	O
as	O
a	O
com-	O
bination	O
of	O
learning	O
and	O
dynamic-programming	O
ideas	O
.	O
werbos	O
(	O
1987	O
)	O
argued	O
explicitly	O
1.7.	O
early	O
history	B
of	I
reinforcement	I
learning	I
15	O
for	O
greater	O
interrelation	O
of	O
dynamic	O
programming	O
and	O
learning	O
methods	O
and	O
its	O
rele-	O
vance	O
to	O
understanding	O
neural	B
and	O
cognitive	O
mechanisms	O
.	O
for	O
us	O
the	O
full	O
integration	O
of	O
dynamic	O
programming	O
methods	O
with	O
online	O
learning	O
did	O
not	O
occur	O
until	O
the	O
work	O
of	O
chris	O
watkins	O
in	O
1989	O
,	O
whose	O
treatment	O
of	O
reinforcement	O
learning	O
using	O
the	O
mdp	O
formalism	O
has	O
been	O
widely	O
adopted	O
.	O
since	O
then	O
these	O
relationships	O
have	O
been	O
extensively	O
devel-	O
oped	O
by	O
many	O
researchers	O
,	O
most	O
particularly	O
by	O
dimitri	O
bertsekas	O
and	O
john	O
tsitsiklis	O
(	O
1996	O
)	O
,	O
who	O
coined	O
the	O
term	O
“	O
neurodynamic	B
programming	I
”	O
to	O
refer	O
to	O
the	O
combination	O
of	O
dynamic	O
programming	O
and	O
neural	O
networks	O
.	O
another	O
term	O
currently	O
in	O
use	O
is	O
“	O
approx-	O
imate	O
dynamic	O
programming.	O
”	O
these	O
various	O
approaches	O
emphasize	O
diﬀerent	O
aspects	O
of	O
the	O
subject	O
,	O
but	O
they	O
all	O
share	O
with	O
reinforcement	O
learning	O
an	O
interest	O
in	O
circumventing	O
the	O
classical	O
shortcomings	O
of	O
dynamic	O
programming	O
.	O
we	O
would	O
consider	O
all	O
of	O
the	O
work	O
in	O
optimal	O
control	B
also	O
to	O
be	O
,	O
in	O
a	O
sense	O
,	O
work	O
in	O
reinforcement	O
learning	O
.	O
we	O
deﬁne	O
a	O
reinforcement	B
learning	I
method	O
as	O
any	O
eﬀective	O
way	O
of	O
solving	O
reinforcement	B
learning	I
problems	O
,	O
and	O
it	O
is	O
now	O
clear	O
that	O
these	O
problems	O
are	O
closely	O
related	O
to	O
optimal	B
control	I
problems	O
,	O
particularly	O
stochastic	O
optimal	O
control	B
problems	O
such	O
as	O
those	O
formulated	O
as	O
mdps	O
.	O
accordingly	O
,	O
we	O
must	O
consider	O
the	O
solution	O
methods	O
of	O
optimal	O
control	B
,	O
such	O
as	O
dynamic	O
programming	O
,	O
also	O
to	O
be	O
reinforcement	B
learning	I
methods	O
.	O
because	O
almost	O
all	O
of	O
the	O
conventional	O
methods	O
require	O
complete	O
knowledge	O
of	O
the	O
system	O
to	O
be	O
controlled	O
,	O
it	O
feels	O
a	O
little	O
unnatural	O
to	O
say	O
that	O
they	O
are	O
part	O
of	O
reinforcement	O
learning	O
.	O
on	O
the	O
other	O
hand	O
,	O
many	O
dynamic	B
programming	I
algorithms	O
are	O
incremental	O
and	O
iterative	B
.	O
like	O
learning	O
methods	O
,	O
they	O
gradually	O
reach	O
the	O
correct	O
answer	O
through	O
successive	O
approximations	O
.	O
as	O
we	O
show	O
in	O
the	O
rest	O
of	O
this	O
book	O
,	O
these	O
similarities	O
are	O
far	O
more	O
than	O
superﬁcial	O
.	O
the	O
theories	O
and	O
solution	O
methods	O
for	O
the	O
cases	O
of	O
complete	O
and	O
incomplete	O
knowledge	O
are	O
so	O
closely	O
related	O
that	O
we	O
feel	O
they	O
must	O
be	O
considered	O
together	O
as	O
part	O
of	O
the	O
same	O
subject	O
matter	O
.	O
let	O
us	O
return	B
now	O
to	O
the	O
other	O
major	O
thread	O
leading	O
to	O
the	O
modern	O
ﬁeld	O
of	O
reinforce-	O
ment	O
learning	O
,	O
that	O
centered	O
on	O
the	O
idea	O
of	O
trial-and-error	O
learning	O
.	O
we	O
only	O
touch	O
on	O
the	O
major	O
points	O
of	O
contact	O
here	O
,	O
taking	O
up	O
this	O
topic	O
in	O
more	O
detail	O
in	O
section	O
14.3.	O
according	O
to	O
american	O
psychologist	O
r.	O
s.	O
woodworth	O
the	O
idea	O
of	O
trial-and-error	O
learn-	O
ing	B
goes	O
as	O
far	O
back	O
as	O
the	O
1850s	O
to	O
alexander	O
bain	O
’	O
s	O
discussion	O
of	O
learning	O
by	O
“	O
groping	O
and	O
experiment	O
”	O
and	O
more	O
explicitly	O
to	O
the	O
british	O
ethologist	O
and	O
psychologist	O
conway	O
lloyd	O
morgan	O
’	O
s	O
1894	O
use	O
of	O
the	O
term	O
to	O
describe	O
his	O
observations	O
of	O
animal	O
behavior	O
(	O
woodworth	O
,	O
1938	O
)	O
.	O
perhaps	O
the	O
ﬁrst	O
to	O
succinctly	O
express	O
the	O
essence	O
of	O
trial-and-error	O
learning	O
as	O
a	O
principle	O
of	O
learning	O
was	O
edward	O
thorndike	O
:	O
of	O
several	O
responses	O
made	O
to	O
the	O
same	O
situation	O
,	O
those	O
which	O
are	O
accompa-	O
nied	O
or	O
closely	O
followed	O
by	O
satisfaction	O
to	O
the	O
animal	O
will	O
,	O
other	O
things	O
being	O
equal	O
,	O
be	O
more	O
ﬁrmly	O
connected	O
with	O
the	O
situation	O
,	O
so	O
that	O
,	O
when	O
it	O
recurs	O
,	O
they	O
will	O
be	O
more	O
likely	O
to	O
recur	O
;	O
those	O
which	O
are	O
accompanied	O
or	O
closely	O
fol-	O
lowed	O
by	O
discomfort	O
to	O
the	O
animal	O
will	O
,	O
other	O
things	O
being	O
equal	O
,	O
have	O
their	O
connections	O
with	O
that	O
situation	O
weakened	O
,	O
so	O
that	O
,	O
when	O
it	O
recurs	O
,	O
they	O
will	O
be	O
less	O
likely	O
to	O
occur	O
.	O
the	O
greater	O
the	O
satisfaction	O
or	O
discomfort	O
,	O
the	O
greater	O
the	O
strengthening	O
or	O
weakening	O
of	O
the	O
bond	O
.	O
(	O
thorndike	O
,	O
1911	O
,	O
p.	O
244	O
)	O
thorndike	O
called	O
this	O
the	O
“	O
law	O
of	O
eﬀect	O
”	O
because	O
it	O
describes	O
the	O
eﬀect	O
of	O
reinforcing	O
events	O
on	O
the	O
tendency	O
to	O
select	O
actions	O
.	O
thorndike	O
later	O
modiﬁed	O
the	O
law	O
to	O
better	O
16	O
chapter	O
1	O
:	O
introduction	O
account	O
for	O
accumulating	O
data	O
on	O
animal	O
learning	O
(	O
such	O
as	O
diﬀerences	O
between	O
the	O
eﬀects	O
of	O
reward	O
and	O
punishment	O
)	O
,	O
and	O
the	O
law	O
in	O
its	O
various	O
forms	O
has	O
generated	O
considerable	O
controversy	O
among	O
learning	O
theorists	O
(	O
e.g.	O
,	O
see	O
gallistel	O
,	O
2005	O
;	O
herrnstein	O
,	O
1970	O
;	O
kimble	O
,	O
1961	O
,	O
1967	O
;	O
mazur	O
,	O
1994	O
)	O
.	O
despite	O
this	O
,	O
the	O
law	O
of	O
eﬀect—in	O
one	O
form	O
or	O
another—is	O
widely	O
regarded	O
as	O
a	O
basic	O
principle	O
underlying	O
much	O
behavior	O
(	O
e.g.	O
,	O
hilgard	O
and	O
bower	O
,	O
1975	O
;	O
dennett	O
,	O
1978	O
;	O
campbell	O
,	O
1960	O
;	O
cziko	O
,	O
1995	O
)	O
.	O
it	O
is	O
the	O
basis	O
of	O
the	O
inﬂuential	O
learning	O
theories	O
of	O
clark	O
hull	O
and	O
experimental	O
methods	O
of	O
b.	O
f.	O
skinner	O
(	O
e.g.	O
,	O
hull	O
,	O
1943	O
,	O
1952	O
;	O
skinner	O
,	O
1938	O
)	O
.	O
the	O
term	O
“	O
reinforcement	O
”	O
in	O
the	O
context	O
of	O
animal	O
learning	O
came	O
into	O
use	O
well	O
after	O
thorndike	O
’	O
s	O
expression	O
of	O
the	O
law	O
of	O
eﬀect	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
ﬁrst	O
appearing	O
in	O
this	O
context	O
in	O
the	O
1927	O
english	O
translation	O
of	O
pavlov	O
’	O
s	O
monograph	O
on	O
conditioned	O
reﬂexes	O
.	O
reinforcement	O
is	O
the	O
strengthening	O
of	O
a	O
pattern	O
of	O
behavior	O
as	O
a	O
result	O
of	O
an	O
animal	O
receiving	O
a	O
stimulus—a	O
reinforcer—in	O
an	O
appropriate	O
temporal	O
relationship	O
with	O
another	O
stimulus	O
or	O
with	O
a	O
response	O
.	O
some	O
psychologists	O
extended	O
its	O
meaning	O
to	O
include	O
the	O
process	O
of	O
weakening	O
in	O
addition	O
to	O
strengthening	O
,	O
as	O
well	O
applying	O
when	O
the	O
omission	O
or	O
termination	O
of	O
an	O
event	O
changes	O
behavior	O
.	O
reinforcement	O
produces	O
changes	O
in	O
behavior	O
that	O
persist	O
after	O
the	O
reinforcer	O
is	O
withdrawn	O
,	O
so	O
that	O
a	O
stimulus	O
that	O
attracts	O
an	O
animal	O
’	O
s	O
attention	O
or	O
that	O
energizes	O
its	O
behavior	O
without	O
producing	O
lasting	O
changes	O
is	O
not	O
considered	O
to	O
be	O
a	O
reinforcer	O
.	O
the	O
idea	O
of	O
implementing	O
trial-and-error	B
learning	O
in	O
a	O
computer	O
appeared	O
among	O
the	O
earliest	O
thoughts	O
about	O
the	O
possibility	O
of	O
artiﬁcial	O
intelligence	O
.	O
in	O
a	O
1948	O
report	O
,	O
alan	O
turing	O
described	O
a	O
design	O
for	O
a	O
“	O
pleasure-pain	O
system	O
”	O
that	O
worked	O
along	O
the	O
lines	O
of	O
the	O
law	O
of	O
eﬀect	O
:	O
when	O
a	O
conﬁguration	O
is	O
reached	O
for	O
which	O
the	O
action	B
is	O
undetermined	O
,	O
a	O
random	O
choice	O
for	O
the	O
missing	O
data	O
is	O
made	O
and	O
the	O
appropriate	O
entry	O
is	O
made	O
in	O
the	O
description	O
,	O
tentatively	O
,	O
and	O
is	O
applied	O
.	O
when	O
a	O
pain	O
stimulus	O
occurs	O
all	O
tentative	O
entries	O
are	O
cancelled	O
,	O
and	O
when	O
a	O
pleasure	O
stimulus	O
occurs	O
they	O
are	O
all	O
made	O
permanent	O
.	O
(	O
turing	O
,	O
1948	O
)	O
many	O
ingenious	O
electro-mechanical	O
machines	O
were	O
constructed	O
that	O
demonstrated	O
trial-	O
and-error	O
learning	O
.	O
the	O
earliest	O
may	O
have	O
been	O
a	O
machine	O
built	O
by	O
thomas	O
ross	O
(	O
1933	O
)	O
that	O
was	O
able	O
to	O
ﬁnd	O
its	O
way	O
through	O
a	O
simple	O
maze	O
and	O
remember	O
the	O
path	O
through	O
the	O
settings	O
of	O
switches	O
.	O
in	O
1951	O
w.	O
grey	O
walter	O
,	O
already	O
known	O
for	O
his	O
“	O
mechanical	O
tortoise	O
”	O
(	O
walter	O
,	O
1950	O
)	O
,	O
built	O
a	O
version	O
capable	O
of	O
a	O
simple	O
form	O
of	O
learning	O
.	O
in	O
1952	O
claude	O
shannon	O
demonstrated	O
a	O
maze-running	O
mouse	O
named	O
theseus	O
that	O
used	O
trial	O
and	O
error	O
to	O
ﬁnd	O
its	O
way	O
through	O
a	O
maze	O
,	O
with	O
the	O
maze	O
itself	O
remembering	O
the	O
successful	O
directions	O
via	O
magnets	O
and	O
relays	O
under	O
its	O
ﬂoor	O
(	O
see	O
also	O
shannon	O
,	O
1951	O
)	O
.	O
j.	O
a.	O
deutsch	O
(	O
1954	O
)	O
described	O
a	O
maze-solving	O
machine	O
based	O
on	O
his	O
behavior	O
theory	O
(	O
deutsch	O
,	O
1953	O
)	O
that	O
has	O
some	O
properties	O
in	O
common	O
with	O
model-based	O
reinforcement	B
learning	I
(	O
chapter	O
8	O
)	O
.	O
in	O
his	O
ph.d.	O
dissertation	O
,	O
marvin	O
minsky	O
(	O
1954	O
)	O
discussed	O
computational	O
models	O
of	O
reinforcement	O
learning	O
and	O
described	O
his	O
construction	O
of	O
an	O
analog	O
machine	O
composed	O
of	O
components	O
he	O
called	O
snarcs	O
(	O
stochastic	O
neural-analog	O
reinforcement	O
calculators	O
)	O
meant	O
to	O
resemble	O
modiﬁable	O
synaptic	O
connections	O
in	O
the	O
brain	O
(	O
chapter	O
15	O
)	O
.	O
the	O
fascinating	O
web	O
site	O
cyberneticzoo.com	O
contains	O
a	O
wealth	O
of	O
information	O
on	O
these	O
and	O
many	O
other	O
electro-mechanical	O
learning	O
machines	O
.	O
1.7.	O
early	O
history	B
of	I
reinforcement	I
learning	I
17	O
building	O
electro-mechanical	O
learning	O
machines	O
gave	O
way	O
to	O
programming	O
digital	O
com-	O
puters	O
to	O
perform	O
various	O
types	O
of	O
learning	O
,	O
some	O
of	O
which	O
implemented	O
trial-and-error	B
learning	O
.	O
farley	O
and	O
clark	O
(	O
1954	O
)	O
described	O
a	O
digital	O
simulation	O
of	O
a	O
neural-network	O
learning	O
machine	O
that	O
learned	O
by	O
trial	O
and	O
error	O
.	O
but	O
their	O
interests	O
soon	O
shifted	O
from	O
trial-and-error	B
learning	O
to	O
generalization	O
and	O
pattern	O
recognition	O
,	O
that	O
is	O
,	O
from	O
reinforce-	O
ment	O
learning	O
to	O
supervised	B
learning	I
(	O
clark	O
and	O
farley	O
,	O
1955	O
)	O
.	O
this	O
began	O
a	O
pattern	O
of	O
confusion	O
about	O
the	O
relationship	O
between	O
these	O
types	O
of	O
learning	O
.	O
many	O
researchers	O
seemed	O
to	O
believe	O
that	O
they	O
were	O
studying	O
reinforcement	B
learning	I
when	O
they	O
were	O
actually	O
studying	O
supervised	B
learning	I
.	O
for	O
example	O
,	O
neural	B
network	O
pioneers	O
such	O
as	O
rosenblatt	O
(	O
1962	O
)	O
and	O
widrow	O
and	O
hoﬀ	O
(	O
1960	O
)	O
were	O
clearly	O
motivated	O
by	O
reinforcement	O
learning—	O
they	O
used	O
the	O
language	O
of	O
rewards	O
and	O
punishments—but	O
the	O
systems	O
they	O
studied	O
were	O
supervised	B
learning	I
systems	O
suitable	O
for	O
pattern	O
recognition	O
and	O
perceptual	O
learning	O
.	O
even	O
today	O
,	O
some	O
researchers	O
and	O
textbooks	O
minimize	O
or	O
blur	O
the	O
distinction	O
between	O
these	O
types	O
of	O
learning	O
.	O
for	O
example	O
,	O
some	O
neural-network	O
textbooks	O
have	O
used	O
the	O
term	O
“	O
trial-and-error	B
”	O
to	O
describe	O
networks	O
that	O
learn	O
from	O
training	O
examples	O
.	O
this	O
is	O
an	O
understandable	O
confusion	O
because	O
these	O
networks	O
use	O
error	O
information	O
to	O
update	O
connection	O
weights	O
,	O
but	O
this	O
misses	O
the	O
essential	O
character	O
of	O
trial-and-error	O
learning	O
as	O
selecting	O
actions	O
on	O
the	O
basis	O
of	O
evaluative	O
feedback	O
that	O
does	O
not	O
rely	O
on	O
knowledge	O
of	O
what	O
the	O
correct	O
action	B
should	O
be	O
.	O
partly	O
as	O
a	O
result	O
of	O
these	O
confusions	O
,	O
research	O
into	O
genuine	O
trial-and-error	B
learning	O
be-	O
came	O
rare	O
in	O
the	O
1960s	O
and	O
1970s	O
,	O
although	O
there	O
were	O
notable	O
exceptions	O
.	O
in	O
the	O
1960s	O
the	O
terms	O
“	O
reinforcement	O
”	O
and	O
“	O
reinforcement	B
learning	I
”	O
were	O
used	O
in	O
the	O
engineering	O
literature	O
for	O
the	O
ﬁrst	O
time	O
to	O
describe	O
engineering	O
uses	O
of	O
trial-and-error	O
learning	O
(	O
e.g.	O
,	O
waltz	O
and	O
fu	O
,	O
1965	O
;	O
mendel	O
,	O
1966	O
;	O
fu	O
,	O
1970	O
;	O
mendel	O
and	O
mcclaren	O
,	O
1970	O
)	O
.	O
particularly	O
inﬂuential	O
was	O
minsky	O
’	O
s	O
paper	O
“	O
steps	O
toward	O
artiﬁcial	B
intelligence	I
”	O
(	O
minsky	O
,	O
1961	O
)	O
,	O
which	O
discussed	O
several	O
issues	O
relevant	O
to	O
trial-and-error	B
learning	O
,	O
including	O
prediction	B
,	O
expectation	O
,	O
and	O
what	O
he	O
called	O
the	O
basic	O
credit-assignment	O
problem	O
for	O
complex	O
rein-	O
forcement	O
learning	O
systems	O
:	O
how	O
do	O
you	O
distribute	O
credit	O
for	O
success	O
among	O
the	O
many	O
decisions	O
that	O
may	O
have	O
been	O
involved	O
in	O
producing	O
it	O
?	O
all	O
of	O
the	O
methods	O
we	O
discuss	O
in	O
this	O
book	O
are	O
,	O
in	O
a	O
sense	O
,	O
directed	O
toward	O
solving	O
this	O
problem	O
.	O
minsky	O
’	O
s	O
paper	O
is	O
well	O
worth	O
reading	O
today	O
.	O
in	O
the	O
next	O
few	O
paragraphs	O
we	O
discuss	O
some	O
of	O
the	O
other	O
exceptions	O
and	O
partial	O
ex-	O
ceptions	O
to	O
the	O
relative	O
neglect	O
of	O
computational	O
and	O
theoretical	O
study	O
of	O
genuine	O
trial-	O
and-error	O
learning	O
in	O
the	O
1960s	O
and	O
1970s	O
.	O
one	O
of	O
these	O
was	O
the	O
work	O
by	O
a	O
new	O
zealand	O
researcher	O
named	O
john	O
andreae	O
.	O
andreae	O
(	O
1963	O
)	O
developed	O
a	O
system	O
called	O
stella	O
that	O
learned	O
by	O
trial	O
and	O
error	O
in	O
interaction	O
with	O
its	O
environment	B
.	O
this	O
system	O
included	O
an	O
internal	O
model	O
of	O
the	O
world	O
and	O
,	O
later	O
,	O
an	O
“	O
internal	O
monologue	O
”	O
to	O
deal	O
with	O
problems	O
of	O
hidden	O
state	B
(	O
andreae	O
,	O
1969a	O
)	O
.	O
andreae	O
’	O
s	O
later	O
work	O
(	O
1977	O
)	O
placed	O
more	O
emphasis	O
on	O
learning	O
from	O
a	O
teacher	O
,	O
but	O
still	O
included	O
learning	O
by	O
trial	O
and	O
error	O
,	O
with	O
the	O
generation	O
of	O
novel	O
events	O
being	O
one	O
of	O
the	O
system	O
’	O
s	O
goals	O
.	O
a	O
feature	O
of	O
this	O
work	O
was	O
a	O
“	O
leakback	O
process	O
,	O
”	O
elaborated	O
more	O
fully	O
in	O
andreae	O
(	O
1998	O
)	O
,	O
that	O
implemented	O
a	O
credit-assignment	O
mechanism	O
similar	O
to	O
the	O
backing-up	O
up-	O
date	O
operations	O
that	O
we	O
describe	O
.	O
unfortunately	O
,	O
his	O
pioneering	O
research	O
was	O
not	O
well	O
known	O
,	O
and	O
did	O
not	O
greatly	O
impact	O
subsequent	O
reinforcement	B
learning	I
research	O
.	O
18	O
chapter	O
1	O
:	O
introduction	O
more	O
inﬂuential	O
was	O
the	O
work	O
of	O
donald	O
michie	O
.	O
in	O
1961	O
and	O
1963	O
he	O
described	O
a	O
simple	O
trial-and-error	B
learning	O
system	O
for	O
learning	O
how	O
to	O
play	O
tic-tac-toe	B
(	O
or	O
naughts	B
and	I
crosses	I
)	O
called	O
menace	O
(	O
for	O
matchbox	O
educable	O
naughts	B
and	I
crosses	I
engine	O
)	O
.	O
it	O
consisted	O
of	O
a	O
matchbox	O
for	O
each	O
possible	O
game	O
position	O
,	O
each	O
matchbox	O
containing	O
a	O
number	O
of	O
colored	O
beads	O
,	O
a	O
diﬀerent	O
color	O
for	O
each	O
possible	O
move	O
from	O
that	O
position	O
.	O
by	O
drawing	O
a	O
bead	O
at	O
random	O
from	O
the	O
matchbox	O
corresponding	O
to	O
the	O
current	O
game	O
posi-	O
tion	B
,	O
one	O
could	O
determine	O
menace	O
’	O
s	O
move	O
.	O
when	O
a	O
game	O
was	O
over	O
,	O
beads	O
were	O
added	O
to	O
or	O
removed	O
from	O
the	O
boxes	O
used	O
during	O
play	O
to	O
reinforce	O
or	O
punish	O
menace	O
’	O
s	O
deci-	O
sions	O
.	O
michie	O
and	O
chambers	O
(	O
1968	O
)	O
described	O
another	O
tic-tac-toe	B
reinforcement	O
learner	O
called	O
glee	O
(	O
game	O
learning	O
expectimaxing	O
engine	O
)	O
and	O
a	O
reinforcement	B
learning	I
con-	O
troller	O
called	O
boxes	O
.	O
they	O
applied	O
boxes	O
to	O
the	O
task	O
of	O
learning	O
to	O
balance	O
a	O
pole	O
hinged	O
to	O
a	O
movable	O
cart	O
on	O
the	O
basis	O
of	O
a	O
failure	O
signal	O
occurring	O
only	O
when	O
the	O
pole	O
fell	O
or	O
the	O
cart	O
reached	O
the	O
end	O
of	O
a	O
track	O
.	O
this	O
task	O
was	O
adapted	O
from	O
the	O
earlier	O
work	O
of	O
widrow	O
and	O
smith	O
(	O
1964	O
)	O
,	O
who	O
used	O
supervised	B
learning	I
methods	O
,	O
assuming	O
instruc-	O
tion	B
from	O
a	O
teacher	O
already	O
able	O
to	O
balance	O
the	O
pole	O
.	O
michie	O
and	O
chambers	O
’	O
s	O
version	O
of	O
pole-balancing	O
is	O
one	O
of	O
the	O
best	O
early	O
examples	O
of	O
a	O
reinforcement	B
learning	I
task	O
un-	O
der	O
conditions	O
of	O
incomplete	O
knowledge	O
.	O
it	O
inﬂuenced	O
much	O
later	O
work	O
in	O
reinforcement	O
learning	O
,	O
beginning	O
with	O
some	O
of	O
our	O
own	O
studies	O
(	O
barto	O
,	O
sutton	O
,	O
and	O
anderson	O
,	O
1983	O
;	O
sutton	O
,	O
1984	O
)	O
.	O
michie	O
consistently	O
emphasized	O
the	O
role	O
of	O
trial	O
and	O
error	O
and	O
learning	O
as	O
essential	O
aspects	O
of	O
artiﬁcial	O
intelligence	O
(	O
michie	O
,	O
1974	O
)	O
.	O
widrow	O
,	O
gupta	O
,	O
and	O
maitra	O
(	O
1973	O
)	O
modiﬁed	O
the	O
least-mean-square	O
(	O
lms	O
)	O
algorithm	O
of	O
widrow	O
and	O
hoﬀ	O
(	O
1960	O
)	O
to	O
produce	O
a	O
reinforcement	B
learning	I
rule	O
that	O
could	O
learn	O
from	O
success	O
and	O
failure	O
signals	O
instead	O
of	O
from	O
training	O
examples	O
.	O
they	O
called	O
this	O
form	O
of	O
learning	O
“	O
selective	B
bootstrap	I
adaptation	I
”	O
and	O
described	O
it	O
as	O
“	O
learning	O
with	O
a	O
critic	B
”	O
instead	O
of	O
“	O
learning	O
with	O
a	O
teacher.	O
”	O
they	O
analyzed	O
this	O
rule	O
and	O
showed	O
how	O
it	O
could	O
learn	O
to	O
play	O
blackjack	O
.	O
this	O
was	O
an	O
isolated	O
foray	O
into	O
reinforcement	B
learning	I
by	O
widrow	O
,	O
whose	O
contributions	O
to	O
supervised	B
learning	I
were	O
much	O
more	O
inﬂuential	O
.	O
our	O
use	O
of	O
the	O
term	O
“	O
critic	B
”	O
is	O
derived	O
from	O
widrow	O
,	O
gupta	O
,	O
and	O
maitra	O
’	O
s	O
paper	O
.	O
buchanan	O
,	O
mitchell	O
,	O
smith	O
,	O
and	O
johnson	O
(	O
1978	O
)	O
independently	O
used	O
the	O
term	O
critic	B
in	O
the	O
context	O
of	O
machine	O
learning	O
(	O
see	O
also	O
dietterich	O
and	O
buchanan	O
,	O
1984	O
)	O
,	O
but	O
for	O
them	O
a	O
critic	B
is	O
an	O
expert	O
system	O
able	O
to	O
do	O
more	O
than	O
evaluate	O
performance	O
.	O
research	O
on	O
learning	B
automata	I
had	O
a	O
more	O
direct	O
inﬂuence	O
on	O
the	O
trial-and-error	B
thread	O
leading	O
to	O
modern	O
reinforcement	B
learning	I
research	O
.	O
these	O
are	O
methods	O
for	O
solv-	O
ing	B
a	O
nonassociative	O
,	O
purely	O
selectional	O
learning	O
problem	O
known	O
as	O
the	O
k-armed	O
bandit	O
by	O
analogy	O
to	O
a	O
slot	O
machine	O
,	O
or	O
“	O
one-armed	O
bandit	O
,	O
”	O
except	O
with	O
k	O
levers	O
(	O
see	O
chapter	O
2	O
)	O
.	O
learning	B
automata	I
are	O
simple	O
,	O
low-memory	O
machines	O
for	O
improving	O
the	O
probability	O
of	O
reward	O
in	O
these	O
problems	O
.	O
learning	B
automata	I
originated	O
with	O
work	O
in	O
the	O
1960s	O
of	O
the	O
russian	O
mathematician	O
and	O
physicist	O
m.	O
l.	O
tsetlin	O
and	O
colleagues	O
(	O
published	O
posthu-	O
mously	O
in	O
tsetlin	O
,	O
1973	O
)	O
and	O
has	O
been	O
extensively	O
developed	O
since	O
then	O
within	O
engineer-	O
ing	B
(	O
see	O
narendra	O
and	O
thathachar	O
,	O
1974	O
,	O
1989	O
)	O
.	O
these	O
developments	O
included	O
the	O
study	O
of	O
stochastic	O
learning	B
automata	I
,	O
which	O
are	O
methods	O
for	O
updating	O
action	B
probabilities	O
on	O
the	O
basis	O
of	O
reward	O
signals	O
.	O
although	O
not	O
developed	O
in	O
the	O
tradition	O
of	O
stochastic	O
learning	B
automata	I
,	O
harth	O
and	O
tzanakou	O
’	O
s	O
(	O
1974	O
)	O
alopex	O
algorithm	O
(	O
for	O
algorithm	O
of	O
pattern	O
extraction	O
)	O
is	O
a	O
stochastic	O
method	O
for	O
detecting	O
correlations	O
between	O
actions	O
and	B
reinforcement	I
that	O
inﬂuenced	O
some	O
of	O
our	O
early	O
research	O
(	O
barto	O
,	O
sutton	O
,	O
and	O
brouwer	O
,	O
1.7.	O
early	O
history	B
of	I
reinforcement	I
learning	I
19	O
1981	O
)	O
.	O
stochastic	O
learning	O
automata	O
were	O
foreshadowed	O
by	O
earlier	O
work	O
in	B
psychology	I
,	O
beginning	O
with	O
william	O
estes	O
’	O
(	O
1950	O
)	O
eﬀort	O
toward	O
a	O
statistical	O
theory	O
of	O
learning	O
and	O
further	O
developed	O
by	O
others	O
,	O
most	O
famously	O
by	O
psychologist	O
robert	O
bush	O
and	O
statistician	O
frederick	O
mosteller	O
(	O
bush	O
and	O
mosteller	O
,	O
1955	O
)	O
.	O
the	O
statistical	O
learning	O
theories	O
developed	O
in	B
psychology	I
were	O
adopted	O
by	O
researchers	O
in	O
economics	O
,	O
leading	O
to	O
a	O
thread	O
of	O
research	O
in	O
that	O
ﬁeld	O
devoted	O
to	O
reinforcement	B
learning	I
.	O
this	O
work	O
began	O
in	O
1973	O
with	O
the	O
application	O
of	O
bush	O
and	O
mosteller	O
’	O
s	O
learning	O
theory	O
to	O
a	O
collection	O
of	O
classical	O
economic	O
models	O
(	O
cross	O
,	O
1973	O
)	O
.	O
one	O
goal	B
of	O
this	O
research	O
was	O
to	O
study	O
artiﬁcial	O
agents	O
that	O
act	O
more	O
like	O
real	O
people	O
than	O
do	O
traditional	O
idealized	O
economic	O
agents	O
(	O
arthur	O
,	O
1991	O
)	O
.	O
this	O
approach	O
expanded	O
to	O
the	O
study	O
of	O
reinforcement	O
learning	O
in	O
the	O
context	O
of	O
game	O
theory	O
.	O
although	O
reinforcement	B
learning	I
in	O
economics	O
developed	O
largely	O
independently	O
of	O
the	O
early	O
work	O
in	O
artiﬁcial	O
intelligence	O
,	O
reinforcement	B
learning	I
and	O
game	B
theory	I
is	O
a	O
topic	O
of	O
current	O
interest	O
in	O
both	O
ﬁelds	O
,	O
but	O
one	O
that	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
.	O
camerer	O
(	O
2011	O
)	O
discusses	O
the	O
reinforcement	B
learning	I
tradition	O
in	O
economics	O
,	O
and	O
now´e	O
et	O
al	O
.	O
(	O
2012	O
)	O
provide	O
an	O
overview	O
of	O
the	O
subject	O
from	O
the	O
point	O
of	O
view	O
of	O
multi-agent	O
extensions	O
to	O
the	O
approach	O
that	O
we	O
introduce	O
in	O
this	O
book	O
.	O
reinforcement	O
in	O
the	O
context	O
of	O
game	O
theory	O
is	O
a	O
much	O
diﬀerent	O
subject	O
than	O
reinforcement	B
learning	I
used	O
in	O
programs	O
to	O
play	O
tic-tac-toe	B
,	O
checkers	O
,	O
and	O
other	O
recreational	O
games	O
.	O
see	O
,	O
for	O
example	O
,	O
szita	O
(	O
2012	O
)	O
for	O
an	O
overview	O
of	O
this	O
aspect	O
of	O
reinforcement	O
learning	O
and	O
games	O
.	O
john	O
holland	O
(	O
1975	O
)	O
outlined	O
a	O
general	O
theory	O
of	O
adaptive	O
systems	O
based	O
on	O
selec-	O
tional	O
principles	O
.	O
his	O
early	O
work	O
concerned	O
trial	O
and	O
error	O
primarily	O
in	O
its	O
nonassociative	O
form	O
,	O
as	O
in	O
evolutionary	B
methods	I
and	O
the	O
k-armed	O
bandit	O
.	O
in	O
1976	O
and	O
more	O
fully	O
in	O
1986	O
,	O
he	O
introduced	O
classiﬁer	B
systems	I
,	O
true	O
reinforcement	O
learning	O
systems	O
including	O
as-	O
sociation	O
and	O
value	O
functions	O
.	O
a	O
key	O
component	O
of	O
holland	O
’	O
s	O
classiﬁer	B
systems	I
was	O
the	O
“	O
bucket-brigade	B
algorithm	I
”	O
for	O
credit	O
assignment	O
that	O
is	O
closely	O
related	O
to	O
the	O
temporal	O
diﬀerence	O
algorithm	O
used	O
in	O
our	O
tic-tac-toe	B
example	O
and	O
discussed	O
in	O
chapter	O
6.	O
an-	O
other	O
key	O
component	O
was	O
a	O
genetic	O
algorithm	O
,	O
an	O
evolutionary	O
method	O
whose	O
role	O
was	O
to	O
evolve	O
useful	O
representations	O
.	O
classiﬁer	B
systems	I
have	O
been	O
extensively	O
developed	O
by	O
many	O
researchers	O
to	O
form	O
a	O
major	O
branch	O
of	O
reinforcement	O
learning	O
research	O
(	O
reviewed	O
by	O
urbanowicz	O
and	O
moore	O
,	O
2009	O
)	O
,	O
but	O
genetic	O
algorithms—which	O
we	O
do	O
not	O
consider	O
to	O
be	O
reinforcement	B
learning	I
systems	O
by	O
themselves—have	O
received	O
much	O
more	O
attention	O
,	O
as	O
have	O
other	O
approaches	O
to	O
evolutionary	O
computation	O
(	O
e.g.	O
,	O
fogel	O
,	O
owens	O
and	O
walsh	O
,	O
1966	O
,	O
and	O
koza	O
,	O
1992	O
)	O
.	O
the	O
individual	O
most	O
responsible	O
for	O
reviving	O
the	O
trial-and-error	B
thread	O
to	O
reinforce-	O
ment	O
learning	O
within	O
artiﬁcial	B
intelligence	I
was	O
harry	O
klopf	O
(	O
1972	O
,	O
1975	O
,	O
1982	O
)	O
.	O
klopf	O
recognized	O
that	O
essential	O
aspects	O
of	O
adaptive	O
behavior	O
were	O
being	O
lost	O
as	O
learning	O
re-	O
searchers	O
came	O
to	O
focus	O
almost	O
exclusively	O
on	O
supervised	B
learning	I
.	O
what	O
was	O
missing	O
,	O
according	O
to	O
klopf	O
,	O
were	O
the	O
hedonic	O
aspects	O
of	O
behavior	O
,	O
the	O
drive	O
to	O
achieve	O
some	O
result	O
from	O
the	O
environment	B
,	O
to	O
control	B
the	O
environment	B
toward	O
desired	O
ends	O
and	O
away	O
from	O
undesired	O
ends	O
.	O
this	O
is	O
the	O
essential	O
idea	O
of	O
trial-and-error	O
learning	O
.	O
klopf	O
’	O
s	O
ideas	O
were	O
especially	O
inﬂuential	O
on	O
the	O
authors	O
because	O
our	O
assessment	B
of	I
them	O
(	O
barto	O
and	O
sutton	O
,	O
1981a	O
)	O
led	O
to	O
our	O
appreciation	O
of	O
the	O
distinction	O
between	O
supervised	O
and	O
reinforcement	B
learning	I
,	O
and	O
to	O
our	O
eventual	O
focus	O
on	O
reinforcement	B
learning	I
.	O
much	O
of	O
the	O
early	O
work	O
20	O
chapter	O
1	O
:	O
introduction	O
that	O
we	O
and	O
colleagues	O
accomplished	O
was	O
directed	O
toward	O
showing	O
that	O
reinforcement	B
learning	I
and	O
supervised	B
learning	I
were	O
indeed	O
diﬀerent	O
(	O
barto	O
,	O
sutton	O
,	O
and	O
brouwer	O
,	O
1981	O
;	O
barto	O
and	O
sutton	O
,	O
1981b	O
;	O
barto	O
and	O
anandan	O
,	O
1985	O
)	O
.	O
other	O
studies	O
showed	O
how	O
reinforcement	B
learning	I
could	O
address	O
important	O
problems	O
in	O
neural	O
network	O
learning	O
,	O
in	O
particular	O
,	O
how	O
it	O
could	O
produce	O
learning	O
algorithms	O
for	O
multilayer	O
networks	O
(	O
barto	O
,	O
anderson	O
,	O
and	O
sutton	O
,	O
1982	O
;	O
barto	O
and	O
anderson	O
,	O
1985	O
;	O
barto	O
,	O
1985	O
,	O
1986	O
;	O
barto	O
and	O
jordan	O
,	O
1987	O
)	O
.	O
we	O
say	O
more	O
about	O
reinforcement	B
learning	I
and	O
neural	B
networks	I
in	O
chap-	O
ter	O
15.	O
we	O
turn	O
now	O
to	O
the	O
third	O
thread	O
to	O
the	O
history	B
of	I
reinforcement	I
learning	I
,	O
that	O
concern-	O
ing	B
temporal-diﬀerence	O
learning	O
.	O
temporal-diﬀerence	B
learning	I
methods	O
are	O
distinctive	O
in	O
being	O
driven	O
by	O
the	O
diﬀerence	O
between	O
temporally	O
successive	O
estimates	O
of	O
the	O
same	O
quantity—for	O
example	O
,	O
of	O
the	O
probability	O
of	O
winning	O
in	O
the	O
tic-tac-toe	O
example	O
.	O
this	O
thread	O
is	O
smaller	O
and	O
less	O
distinct	O
than	O
the	O
other	O
two	O
,	O
but	O
it	O
has	O
played	O
a	O
particularly	O
important	O
role	O
in	O
the	O
ﬁeld	O
,	O
in	O
part	O
because	O
temporal-diﬀerence	O
methods	O
seem	O
to	O
be	O
new	O
and	O
unique	O
to	O
reinforcement	B
learning	I
.	O
the	O
origins	O
of	O
temporal-diﬀerence	O
learning	O
are	O
in	O
part	O
in	B
animal	I
learning	I
psychology	O
,	O
in	O
particular	O
,	O
in	O
the	O
notion	O
of	O
secondary	O
reinforcers	O
.	O
a	O
secondary	O
reinforcer	O
is	O
a	O
stimulus	O
that	O
has	O
been	O
paired	O
with	O
a	O
primary	O
reinforcer	O
such	O
as	O
food	O
or	O
pain	O
and	O
,	O
as	O
a	O
result	O
,	O
has	O
come	O
to	O
take	O
on	O
similar	O
reinforcing	O
properties	O
.	O
minsky	O
(	O
1954	O
)	O
may	O
have	O
been	O
the	O
ﬁrst	O
to	O
realize	O
that	O
this	O
psychological	O
principle	O
could	O
be	O
important	O
for	O
artiﬁcial	O
learning	O
systems	O
.	O
arthur	O
samuel	O
(	O
1959	O
)	O
was	O
the	O
ﬁrst	O
to	O
propose	O
and	O
implement	O
a	O
learning	O
method	O
that	O
included	O
temporal-diﬀerence	O
ideas	O
,	O
as	O
part	O
of	O
his	O
celebrated	O
checkers-playing	O
program	O
.	O
samuel	O
made	O
no	O
reference	O
to	O
minsky	O
’	O
s	O
work	O
or	O
to	O
possible	O
connections	O
to	O
animal	O
learning	O
.	O
his	O
inspiration	O
apparently	O
came	O
from	O
claude	O
shannon	O
’	O
s	O
(	O
1950	O
)	O
suggestion	O
that	O
a	O
computer	O
could	O
be	O
programmed	O
to	O
use	O
an	O
evaluation	O
function	O
to	O
play	O
chess	B
,	O
and	O
that	O
it	O
might	O
be	O
able	O
to	O
improve	O
its	O
play	O
by	O
modifying	O
this	O
function	O
online	O
.	O
(	O
it	O
is	O
possible	O
that	O
these	O
ideas	O
of	O
shannon	O
’	O
s	O
also	O
inﬂuenced	O
bellman	O
,	O
but	O
we	O
know	O
of	O
no	O
evidence	O
for	O
this	O
.	O
)	O
minsky	O
(	O
1961	O
)	O
extensively	O
discussed	O
samuel	O
’	O
s	O
work	O
in	O
his	O
“	O
steps	O
”	O
paper	O
,	O
suggesting	O
the	O
connection	O
to	O
secondary	B
reinforcement	I
theories	O
,	O
both	O
natural	O
and	O
artiﬁcial	O
.	O
as	O
we	O
have	O
discussed	O
,	O
in	O
the	O
decade	O
following	O
the	O
work	O
of	O
minsky	O
and	O
samuel	O
,	O
little	O
computational	O
work	O
was	O
done	O
on	O
trial-and-error	B
learning	O
,	O
and	O
apparently	O
no	O
computa-	O
tional	O
work	O
at	O
all	O
was	O
done	O
on	O
temporal-diﬀerence	B
learning	I
.	O
in	O
1972	O
,	O
klopf	O
brought	O
trial-and-error	B
learning	O
together	O
with	O
an	O
important	O
component	O
of	O
temporal-diﬀerence	O
learning	O
.	O
klopf	O
was	O
interested	O
in	O
principles	O
that	O
would	O
scale	O
to	O
learning	O
in	O
large	O
systems	O
,	O
and	O
thus	O
was	O
intrigued	O
by	O
notions	O
of	O
local	O
reinforcement	O
,	O
whereby	O
subcomponents	O
of	O
an	O
overall	O
learning	O
system	O
could	O
reinforce	O
one	O
another	O
.	O
he	O
developed	O
the	O
idea	O
of	O
“	O
gener-	O
alized	O
reinforcement	O
,	O
”	O
whereby	O
every	O
component	O
(	O
nominally	O
,	O
every	O
neuron	O
)	O
views	O
all	O
of	O
its	O
inputs	O
in	O
reinforcement	O
terms	O
:	O
excitatory	O
inputs	O
as	O
rewards	O
and	O
inhibitory	O
inputs	O
as	O
punishments	O
.	O
this	O
is	O
not	O
the	O
same	O
idea	O
as	O
what	O
we	O
now	O
know	O
as	O
temporal-diﬀerence	O
learning	O
,	O
and	O
in	O
retrospect	O
it	O
is	O
farther	O
from	O
it	O
than	O
was	O
samuel	O
’	O
s	O
work	O
.	O
on	O
the	O
other	O
hand	O
,	O
klopf	O
linked	O
the	O
idea	O
with	O
trial-and-error	O
learning	O
and	O
related	O
it	O
to	O
the	O
massive	O
empirical	O
database	O
of	O
animal	O
learning	O
psychology	O
.	O
sutton	O
(	O
1978a	O
,	O
1978b	O
,	O
1978c	O
)	O
developed	O
klopf	O
’	O
s	O
ideas	O
further	O
,	O
particularly	O
the	O
links	O
to	O
animal	O
learning	O
theories	O
,	O
describing	O
learning	O
rules	O
driven	O
by	O
changes	O
in	O
temporally	O
1.7.	O
early	O
history	B
of	I
reinforcement	I
learning	I
21	O
successive	O
predictions	O
.	O
he	O
and	O
barto	O
reﬁned	O
these	O
ideas	O
and	O
developed	O
a	O
psychological	O
model	O
of	O
classical	O
conditioning	B
based	O
on	O
temporal-diﬀerence	B
learning	I
(	O
sutton	O
and	O
barto	O
,	O
1981a	O
;	O
barto	O
and	O
sutton	O
,	O
1982	O
)	O
.	O
there	O
followed	O
several	O
other	O
inﬂuential	O
psychological	O
models	O
of	O
classical	O
conditioning	B
based	O
on	O
temporal-diﬀerence	B
learning	I
(	O
e.g.	O
,	O
klopf	O
,	O
1988	O
;	O
moore	O
et	O
al.	O
,	O
1986	O
;	O
sutton	O
and	O
barto	O
,	O
1987	O
,	O
1990	O
)	O
.	O
some	O
neuroscience	B
models	O
developed	O
at	O
this	O
time	O
are	O
well	O
interpreted	O
in	O
terms	O
of	O
temporal-diﬀerence	O
learning	O
(	O
hawkins	O
and	O
kandel	O
,	O
1984	O
;	O
byrne	O
,	O
gingrich	O
,	O
and	O
baxter	O
,	O
1990	O
;	O
gelperin	O
,	O
hopﬁeld	O
,	O
and	O
tank	O
,	O
1985	O
;	O
tesauro	O
,	O
1986	O
;	O
friston	O
et	O
al.	O
,	O
1994	O
)	O
,	O
although	O
in	O
most	O
cases	O
there	O
was	O
no	O
historical	O
connection	O
.	O
our	O
early	O
work	O
on	O
temporal-diﬀerence	B
learning	I
was	O
strongly	O
inﬂuenced	O
by	O
animal	O
learning	O
theories	O
and	O
by	O
klopf	O
’	O
s	O
work	O
.	O
relationships	O
to	O
minsky	O
’	O
s	O
“	O
steps	O
”	O
paper	O
and	O
to	O
samuel	O
’	O
s	O
checkers	O
players	O
appear	O
to	O
have	O
been	O
recognized	O
only	O
afterward	O
.	O
by	O
1981	O
,	O
however	O
,	O
we	O
were	O
fully	O
aware	O
of	O
all	O
the	O
prior	O
work	O
mentioned	O
above	O
as	O
part	O
of	O
the	O
temporal-diﬀerence	O
and	O
trial-and-error	B
threads	O
.	O
at	O
this	O
time	O
we	O
developed	O
a	O
method	O
for	O
using	O
temporal-diﬀerence	B
learning	I
combined	O
with	O
trial-and-error	O
learning	O
,	O
known	O
as	O
the	O
actor–critic	B
architecture	O
,	O
and	O
applied	O
this	O
method	O
to	O
michie	O
and	O
chambers	O
’	O
s	O
pole-	O
balancing	O
problem	O
(	O
barto	O
,	O
sutton	O
,	O
and	O
anderson	O
,	O
1983	O
)	O
.	O
this	O
method	O
was	O
extensively	O
studied	O
in	O
sutton	O
’	O
s	O
(	O
1984	O
)	O
ph.d.	O
dissertation	O
and	O
extended	O
to	O
use	O
backpropagation	B
neural	O
networks	O
in	O
anderson	O
’	O
s	O
(	O
1986	O
)	O
ph.d.	O
dissertation	O
.	O
around	O
this	O
time	O
,	O
holland	O
(	O
1986	O
)	O
incorporated	O
temporal-diﬀerence	O
ideas	O
explicitly	O
into	O
his	O
classiﬁer	B
systems	I
in	O
the	O
form	O
of	O
his	O
bucket-brigade	B
algorithm	I
.	O
a	O
key	O
step	O
was	O
taken	O
by	O
sutton	O
in	O
1988	O
by	O
separating	O
temporal-diﬀerence	B
learning	I
from	O
control	B
,	O
treating	O
it	O
as	O
a	O
general	O
prediction	O
method	O
.	O
that	O
paper	O
also	O
introduced	O
the	O
td	O
(	O
λ	O
)	O
algorithm	O
and	O
proved	O
some	O
of	O
its	O
convergence	O
properties	O
.	O
as	O
we	O
were	O
ﬁnalizing	O
our	O
work	O
on	O
the	O
actor–critic	B
architecture	O
in	O
1981	O
,	O
we	O
discov-	O
ered	O
a	O
paper	O
by	O
ian	O
witten	O
(	O
1977	O
)	O
which	O
appears	O
to	O
be	O
the	O
earliest	O
publication	O
of	O
a	O
temporal-diﬀerence	B
learning	I
rule	O
.	O
he	O
proposed	O
the	O
method	O
that	O
we	O
now	O
call	O
tabular	O
td	O
(	O
0	O
)	O
for	O
use	O
as	O
part	O
of	O
an	O
adaptive	O
controller	O
for	O
solving	O
mdps	O
.	O
witten	O
’	O
s	O
work	O
was	O
a	O
descendant	O
of	O
andreae	O
’	O
s	O
early	O
experiments	O
with	O
stella	O
and	O
other	O
trial-and-error	B
learn-	O
ing	B
systems	O
.	O
thus	O
,	O
witten	O
’	O
s	O
1977	O
paper	O
spanned	O
both	O
major	O
threads	O
of	O
reinforcement	O
learning	O
research—trial-and-error	O
learning	O
and	O
optimal	O
control—while	O
making	O
a	O
distinct	O
early	O
contribution	O
to	O
temporal-diﬀerence	B
learning	I
.	O
the	O
temporal-diﬀerence	O
and	O
optimal	B
control	I
threads	O
were	O
fully	O
brought	O
together	O
in	O
1989	O
with	O
chris	O
watkins	O
’	O
s	O
development	O
of	O
q-learning	O
.	O
this	O
work	O
extended	O
and	O
inte-	O
grated	O
prior	O
work	O
in	O
all	O
three	O
threads	O
of	O
reinforcement	O
learning	O
research	O
.	O
paul	O
werbos	O
(	O
1987	O
)	O
contributed	O
to	O
this	O
integration	O
by	O
arguing	O
for	O
the	O
convergence	O
of	O
trial-and-error	O
learning	O
and	O
dynamic	B
programming	I
since	O
1977.	O
by	O
the	O
time	O
of	O
watkins	O
’	O
s	O
work	O
there	O
had	O
been	O
tremendous	O
growth	O
in	O
reinforcement	O
learning	O
research	O
,	O
primarily	O
in	O
the	O
ma-	O
chine	O
learning	O
subﬁeld	O
of	O
artiﬁcial	O
intelligence	O
,	O
but	O
also	O
in	O
neural	O
networks	O
and	B
artiﬁcial	I
intelligence	I
more	O
broadly	O
.	O
in	O
1992	O
,	O
the	O
remarkable	O
success	O
of	O
gerry	O
tesauro	O
’	O
s	O
backgam-	O
mon	O
playing	O
program	O
,	O
td-gammon	O
,	O
brought	O
additional	O
attention	O
to	O
the	O
ﬁeld	O
.	O
in	O
the	O
time	O
since	O
publication	O
of	O
the	O
ﬁrst	O
edition	O
of	O
this	O
book	O
,	O
a	O
ﬂourishing	O
subﬁeld	O
of	O
neuroscience	O
developed	O
that	O
focuses	O
on	O
the	O
relationship	O
between	O
reinforcement	B
learning	I
algorithms	O
and	B
reinforcement	I
learning	O
in	O
the	O
nervous	O
system	O
.	O
most	O
responsible	O
for	O
this	O
22	O
chapter	O
1	O
:	O
introduction	O
is	O
an	O
uncanny	O
similarity	O
between	O
the	O
behavior	O
of	O
temporal-diﬀerence	O
algorithms	O
and	O
the	O
activity	O
of	O
dopamine	O
producing	O
neurons	O
in	O
the	O
brain	O
,	O
as	O
pointed	O
out	O
by	O
a	O
number	O
of	O
re-	O
searchers	O
(	O
friston	O
et	O
al.	O
,	O
1994	O
;	O
barto	O
,	O
1995a	O
;	O
houk	O
,	O
adams	O
,	O
and	O
barto	O
,	O
1995	O
;	O
montague	O
,	O
dayan	O
,	O
and	O
sejnowski	O
,	O
1996	O
;	O
and	O
schultz	O
,	O
dayan	O
,	O
and	O
montague	O
,	O
1997	O
)	O
.	O
chapter	O
15	O
pro-	O
vides	O
an	O
introduction	O
to	O
this	O
exciting	O
aspect	O
of	O
reinforcement	O
learning	O
.	O
other	O
important	O
contributions	O
made	O
in	O
the	O
recent	O
history	B
of	I
reinforcement	I
learning	I
are	O
too	O
numerous	O
to	O
mention	O
in	O
this	O
brief	O
account	O
;	O
we	O
cite	O
many	O
of	O
these	O
at	O
the	O
end	O
of	O
the	O
individual	O
chapters	O
in	O
which	O
they	O
arise	O
.	O
bibliographical	O
remarks	O
for	O
additional	O
general	O
coverage	O
of	O
reinforcement	O
learning	O
,	O
we	O
refer	O
the	O
reader	O
to	O
the	O
books	O
by	O
szepesv´ari	O
(	O
2010	O
)	O
,	O
bertsekas	O
and	O
tsitsiklis	O
(	O
1996	O
)	O
,	O
kaelbling	O
(	O
1993a	O
)	O
,	O
and	O
sugiyama	O
,	O
hachiya	O
,	O
and	O
morimura	O
(	O
2013	O
)	O
.	O
books	O
that	O
take	O
a	O
control	B
or	O
operations	O
research	O
perspective	O
include	O
those	O
of	O
si	O
et	O
al	O
.	O
(	O
2004	O
)	O
,	O
powell	O
(	O
2011	O
)	O
,	O
lewis	O
and	O
liu	O
(	O
2012	O
)	O
,	O
and	O
bertsekas	O
(	O
2012	O
)	O
.	O
cao	O
’	O
s	O
(	O
2009	O
)	O
review	O
places	O
reinforcement	B
learning	I
in	O
the	O
context	O
of	O
other	O
approaches	O
to	O
learning	O
and	O
optimization	O
of	O
stochastic	O
dynamic	O
systems	O
.	O
three	O
special	O
issues	O
of	O
the	O
journal	O
machine	O
learning	O
focus	O
on	O
reinforcement	B
learning	I
:	O
sutton	O
(	O
1992a	O
)	O
,	O
kaelbling	O
(	O
1996	O
)	O
,	O
and	O
singh	O
(	O
2002	O
)	O
.	O
useful	O
surveys	O
are	O
provided	O
by	O
barto	O
(	O
1995b	O
)	O
;	O
kaelbling	O
,	O
littman	O
,	O
and	O
moore	O
(	O
1996	O
)	O
;	O
and	O
keerthi	O
and	O
ravindran	O
(	O
1997	O
)	O
.	O
the	O
volume	O
edited	O
by	O
weiring	O
and	O
van	O
otterlo	O
(	O
2012	O
)	O
provides	O
an	O
excellent	O
overview	O
of	O
recent	O
developments	O
.	O
1.2	O
1.5	O
the	O
example	O
of	O
phil	O
’	O
s	O
breakfast	O
in	O
this	O
chapter	O
was	O
inspired	O
by	O
agre	O
(	O
1988	O
)	O
.	O
the	O
temporal-diﬀerence	O
method	O
used	O
in	O
the	O
tic-tac-toe	O
example	O
is	O
developed	O
in	O
chapter	O
6.	O
part	O
i	O
:	O
tabular	B
solution	I
methods	I
in	O
this	O
part	O
of	O
the	O
book	O
we	O
describe	O
almost	O
all	O
the	O
core	O
ideas	O
of	O
reinforcement	O
learning	O
algorithms	O
in	O
their	O
simplest	O
forms	O
:	O
that	O
in	O
which	O
the	O
state	B
and	O
action	B
spaces	O
are	O
small	O
enough	O
for	O
the	O
approximate	B
value	O
functions	O
to	O
be	O
represented	O
as	O
arrays	O
,	O
or	O
tables	O
.	O
in	O
this	O
case	O
,	O
the	O
methods	O
can	O
often	O
ﬁnd	O
exact	O
solutions	O
,	O
that	O
is	O
,	O
they	O
can	O
often	O
ﬁnd	O
exactly	O
the	O
optimal	O
value	O
function	O
and	O
the	O
optimal	O
policy	O
.	O
this	O
contrasts	O
with	O
the	O
approximate	B
methods	O
described	O
in	O
the	O
next	O
part	O
of	O
the	O
book	O
,	O
which	O
only	O
ﬁnd	O
approximate	B
solutions	O
,	O
but	O
which	O
in	O
return	O
can	O
be	O
applied	O
eﬀectively	O
to	O
much	O
larger	O
problems	O
.	O
the	O
ﬁrst	O
chapter	O
of	O
this	O
part	O
of	O
the	O
book	O
describes	O
solution	O
methods	O
for	O
the	O
special	O
case	O
of	O
the	O
reinforcement	B
learning	I
problem	O
in	O
which	O
there	O
is	O
only	O
a	O
single	O
state	B
,	O
called	O
bandit	B
problems	I
.	O
the	O
second	O
chapter	O
describes	O
the	O
general	O
problem	O
formulation	O
that	O
we	O
treat	O
throughout	O
the	O
rest	O
of	O
the	O
book—ﬁnite	O
markov	O
decision	O
processes—and	O
its	O
main	O
ideas	O
including	O
bellman	O
equations	O
and	O
value	O
functions	O
.	O
the	O
next	O
three	O
chapters	O
describe	O
three	O
fundamental	O
classes	O
of	O
methods	O
for	O
solving	O
ﬁnite	O
markov	O
decision	O
problems	O
:	O
dynamic	B
programming	I
,	O
monte	O
carlo	O
methods	O
,	O
and	O
temporal-	O
diﬀerence	O
learning	O
.	O
each	O
class	O
of	O
methods	O
has	O
its	O
strengths	O
and	O
weaknesses	O
.	O
dynamic	B
programming	I
methods	O
are	O
well	O
developed	O
mathematically	O
,	O
but	O
require	O
a	O
complete	O
and	O
accurate	O
model	B
of	I
the	I
environment	I
.	O
monte	O
carlo	O
methods	O
don	O
’	O
t	O
require	O
a	O
model	O
and	O
are	O
conceptually	O
simple	O
,	O
but	O
are	O
not	O
well	O
suited	O
for	O
step-by-step	O
incremental	O
computation	O
.	O
finally	O
,	O
temporal-diﬀerence	O
methods	O
require	O
no	O
model	O
and	O
are	O
fully	O
incremental	O
,	O
but	O
are	O
more	O
complex	O
to	O
analyze	O
.	O
the	O
methods	O
also	O
diﬀer	O
in	O
several	O
ways	O
with	O
respect	O
to	O
their	O
eﬃciency	O
and	O
speed	O
of	O
convergence	O
.	O
the	O
remaining	O
two	O
chapters	O
describe	O
how	O
these	O
three	O
classes	O
of	O
methods	O
can	O
be	O
com-	O
bined	O
to	O
obtain	O
the	O
best	O
features	O
of	O
each	O
of	O
them	O
.	O
in	O
one	O
chapter	O
we	O
describe	O
how	O
the	O
strengths	O
of	O
monte	O
carlo	O
methods	O
can	O
be	O
combined	O
with	O
the	O
strengths	O
of	O
temporal-	O
diﬀerence	O
methods	O
via	O
multi-step	O
bootstrapping	B
methods	O
.	O
in	O
the	O
ﬁnal	O
chapter	O
of	O
this	O
part	O
of	O
the	O
book	O
we	O
show	O
how	O
temporal-diﬀerence	B
learning	I
methods	O
can	O
be	O
combined	O
with	O
model	O
learning	O
and	O
planning	B
methods	O
(	O
such	O
as	O
dynamic	O
programming	O
)	O
for	O
a	O
com-	O
plete	O
and	O
uniﬁed	O
solution	O
to	O
the	O
tabular	O
reinforcement	O
learning	O
problem	O
.	O
23	O
chapter	O
2	O
multi-armed	B
bandits	I
the	O
most	O
important	O
feature	O
distinguishing	O
reinforcement	B
learning	I
from	O
other	O
types	O
of	O
learning	O
is	O
that	O
it	O
uses	O
training	O
information	O
that	O
evaluates	O
the	O
actions	O
taken	O
rather	O
than	O
instructs	O
by	O
giving	O
correct	O
actions	O
.	O
this	O
is	O
what	O
creates	O
the	O
need	O
for	O
active	O
exploration	O
,	O
for	O
an	O
explicit	O
search	O
for	O
good	O
behavior	O
.	O
purely	O
evaluative	B
feedback	I
indicates	O
how	O
good	O
the	O
action	B
taken	O
was	O
,	O
but	O
not	O
whether	O
it	O
was	O
the	O
best	O
or	O
the	O
worst	O
action	B
possible	O
.	O
purely	O
instructive	O
feedback	O
,	O
on	O
the	O
other	O
hand	O
,	O
indicates	O
the	O
correct	O
action	B
to	O
take	O
,	O
indepen-	O
dently	O
of	O
the	O
action	B
actually	O
taken	O
.	O
this	O
kind	O
of	O
feedback	O
is	O
the	O
basis	O
of	O
supervised	O
learning	O
,	O
which	O
includes	O
large	O
parts	O
of	O
pattern	O
classiﬁcation	O
,	O
artiﬁcial	B
neural	I
networks	I
,	O
and	O
system	O
identiﬁcation	O
.	O
in	O
their	O
pure	O
forms	O
,	O
these	O
two	O
kinds	O
of	O
feedback	O
are	O
quite	O
distinct	O
:	O
evaluative	B
feedback	I
depends	O
entirely	O
on	O
the	O
action	B
taken	O
,	O
whereas	O
instructive	O
feedback	O
is	O
independent	O
of	O
the	O
action	B
taken	O
.	O
in	O
this	O
chapter	O
we	O
study	O
the	O
evaluative	O
aspect	O
of	O
reinforcement	O
learning	O
in	O
a	O
simpliﬁed	O
setting	O
,	O
one	O
that	O
does	O
not	O
involve	O
learning	O
to	O
act	O
in	O
more	O
than	O
one	O
situation	O
.	O
this	O
nonassociative	O
setting	O
is	O
the	O
one	O
in	O
which	O
most	O
prior	O
work	O
involving	O
evaluative	B
feedback	I
has	O
been	O
done	O
,	O
and	O
it	O
avoids	O
much	O
of	O
the	O
complexity	O
of	O
the	O
full	O
reinforcement	B
learning	I
problem	O
.	O
studying	O
this	O
case	O
enables	O
us	O
to	O
see	O
most	O
clearly	O
how	O
evaluative	B
feedback	I
diﬀers	O
from	O
,	O
and	O
yet	O
can	O
be	O
combined	O
with	O
,	O
instructive	O
feedback	O
.	O
the	O
particular	O
nonassociative	O
,	O
evaluative	B
feedback	I
problem	O
that	O
we	O
explore	O
is	O
a	O
simple	O
version	O
of	O
the	O
k-armed	O
bandit	O
problem	O
.	O
we	O
use	O
this	O
problem	O
to	O
introduce	O
a	O
number	O
of	O
basic	O
learning	O
methods	O
which	O
we	O
extend	O
in	O
later	O
chapters	O
to	O
apply	O
to	O
the	O
full	O
reinforce-	O
ment	O
learning	O
problem	O
.	O
at	O
the	O
end	O
of	O
this	O
chapter	O
,	O
we	O
take	O
a	O
step	O
closer	O
to	O
the	O
full	O
reinforcement	B
learning	I
problem	O
by	O
discussing	O
what	O
happens	O
when	O
the	O
bandit	O
problem	O
becomes	O
associative	O
,	O
that	O
is	O
,	O
when	O
actions	O
are	O
taken	O
in	O
more	O
than	O
one	O
situation	O
.	O
2.1	O
a	O
k	O
-armed	O
bandit	O
problem	O
consider	O
the	O
following	O
learning	O
problem	O
.	O
you	O
are	O
faced	O
repeatedly	O
with	O
a	O
choice	O
among	O
k	O
diﬀerent	O
options	B
,	O
or	O
actions	O
.	O
after	O
each	O
choice	O
you	O
receive	O
a	O
numerical	O
reward	O
chosen	O
from	O
a	O
stationary	O
probability	O
distribution	O
that	O
depends	O
on	O
the	O
action	B
you	O
selected	O
.	O
your	O
25	O
26	O
chapter	O
2	O
:	O
multi-armed	B
bandits	I
objective	O
is	O
to	O
maximize	O
the	O
expected	O
total	O
reward	O
over	O
some	O
time	O
period	O
,	O
for	O
example	O
,	O
over	O
1000	O
action	B
selections	O
,	O
or	O
time	O
steps	O
.	O
this	O
is	O
the	O
original	O
form	O
of	O
the	O
k-armed	O
bandit	O
problem	O
,	O
so	O
named	O
by	O
analogy	O
to	O
a	O
slot	O
machine	O
,	O
or	O
“	O
one-armed	O
bandit	O
,	O
”	O
except	O
that	O
it	O
has	O
k	O
levers	O
instead	O
of	O
one	O
.	O
each	O
action	B
selection	O
is	O
like	O
a	O
play	O
of	O
one	O
of	O
the	O
slot	O
machine	O
’	O
s	O
levers	O
,	O
and	O
the	O
rewards	O
are	O
the	O
payoﬀs	O
for	O
hitting	O
the	O
jackpot	O
.	O
through	O
repeated	O
action	B
selections	O
you	O
are	O
to	O
maximize	O
your	O
winnings	O
by	O
concentrating	O
your	O
actions	O
on	O
the	O
best	O
levers	O
.	O
another	O
analogy	O
is	O
that	O
of	O
a	O
doctor	O
choosing	O
between	O
experimental	O
treatments	O
for	O
a	O
series	O
of	O
seriously	O
ill	O
patients	O
.	O
each	O
action	B
is	O
the	O
selection	O
of	O
a	O
treatment	O
,	O
and	O
each	O
reward	O
is	O
the	O
survival	O
or	O
well-being	O
of	O
the	O
patient	O
.	O
today	O
the	O
term	O
“	O
bandit	O
problem	O
”	O
is	O
sometimes	O
used	O
for	O
a	O
generalization	O
of	O
the	O
problem	O
described	O
above	O
,	O
but	O
in	O
this	O
book	O
we	O
use	O
it	O
to	O
refer	O
just	O
to	O
this	O
simple	O
case	O
.	O
in	O
our	O
k-armed	O
bandit	O
problem	O
,	O
each	O
of	O
the	O
k	O
actions	O
has	O
an	O
expected	O
or	O
mean	O
reward	O
given	O
that	O
that	O
action	B
is	O
selected	O
;	O
let	O
us	O
call	O
this	O
the	O
value	B
of	O
that	O
action	B
.	O
we	O
denote	O
the	O
action	B
selected	O
on	O
time	O
step	O
t	O
as	O
at	O
,	O
and	O
the	O
corresponding	O
reward	O
as	O
rt	O
.	O
the	O
value	B
then	O
of	O
an	O
arbitrary	O
action	B
a	O
,	O
denoted	O
q∗	O
(	O
a	O
)	O
,	O
is	O
the	O
expected	O
reward	O
given	O
that	O
a	O
is	O
selected	O
:	O
q∗	O
(	O
a	O
)	O
.	O
=	O
e	O
[	O
rt	O
|	O
at	O
=	O
a	O
]	O
.	O
if	O
you	O
knew	O
the	O
value	B
of	O
each	O
action	B
,	O
then	O
it	O
would	O
be	O
trivial	O
to	O
solve	O
the	O
k-armed	O
bandit	O
problem	O
:	O
you	O
would	O
always	O
select	O
the	O
action	B
with	O
highest	O
value	B
.	O
we	O
assume	O
that	O
you	O
do	O
not	O
know	O
the	O
action	B
values	O
with	O
certainty	O
,	O
although	O
you	O
may	O
have	O
estimates	O
.	O
we	O
denote	O
the	O
estimated	O
value	B
of	O
action	B
a	O
at	O
time	O
step	O
t	O
as	O
qt	O
(	O
a	O
)	O
.	O
we	O
would	O
like	O
qt	O
(	O
a	O
)	O
to	O
be	O
close	O
to	O
q∗	O
(	O
a	O
)	O
.	O
if	O
you	O
maintain	O
estimates	O
of	O
the	O
action	B
values	O
,	O
then	O
at	O
any	O
time	O
step	O
there	O
is	O
at	O
least	O
one	O
action	B
whose	O
estimated	O
value	B
is	O
greatest	O
.	O
we	O
call	O
these	O
the	O
greedy	O
actions	O
.	O
when	O
you	O
select	O
one	O
of	O
these	O
actions	O
,	O
we	O
say	O
that	O
you	O
are	O
exploiting	O
your	O
current	O
knowledge	O
of	O
the	O
values	O
of	O
the	O
actions	O
.	O
if	O
instead	O
you	O
select	O
one	O
of	O
the	O
nongreedy	O
actions	O
,	O
then	O
we	O
say	O
you	O
are	O
exploring	O
,	O
because	O
this	O
enables	O
you	O
to	O
improve	O
your	O
estimate	O
of	O
the	O
nongreedy	O
action	B
’	O
s	O
value	B
.	O
exploitation	O
is	O
the	O
right	O
thing	O
to	O
do	O
to	O
maximize	O
the	O
expected	O
reward	O
on	O
the	O
one	O
step	O
,	O
but	O
exploration	O
may	O
produce	O
the	O
greater	O
total	O
reward	O
in	O
the	O
long	O
run	O
.	O
for	O
example	O
,	O
suppose	O
a	O
greedy	O
action	O
’	O
s	O
value	B
is	O
known	O
with	O
certainty	O
,	O
while	O
several	O
other	O
actions	O
are	O
estimated	O
to	O
be	O
nearly	O
as	O
good	O
but	O
with	O
substantial	O
uncertainty	O
.	O
the	O
uncertainty	O
is	O
such	O
that	O
at	O
least	O
one	O
of	O
these	O
other	O
actions	O
probably	O
is	O
actually	O
better	O
than	O
the	O
greedy	O
action	O
,	O
but	O
you	O
don	O
’	O
t	O
know	O
which	O
one	O
.	O
if	O
you	O
have	O
many	O
time	O
steps	O
ahead	O
on	O
which	O
to	O
make	O
action	B
selections	O
,	O
then	O
it	O
may	O
be	O
better	O
to	O
explore	O
the	O
nongreedy	O
actions	O
and	O
discover	O
which	O
of	O
them	O
are	O
better	O
than	O
the	O
greedy	O
action	O
.	O
reward	O
is	O
lower	O
in	O
the	O
short	O
run	O
,	O
during	O
exploration	O
,	O
but	O
higher	O
in	O
the	O
long	O
run	O
because	O
after	O
you	O
have	O
discovered	O
the	O
better	O
actions	O
,	O
you	O
can	O
exploit	O
them	O
many	O
times	O
.	O
because	O
it	O
is	O
not	O
possible	O
both	O
to	O
explore	O
and	O
to	O
exploit	O
with	O
any	O
single	O
action	B
selection	O
,	O
one	O
often	O
refers	O
to	O
the	O
“	O
conﬂict	O
”	O
between	O
exploration	O
and	O
exploitation	O
.	O
in	O
any	O
speciﬁc	O
case	O
,	O
whether	O
it	O
is	O
better	O
to	O
explore	O
or	O
exploit	O
depends	O
in	O
a	O
complex	O
way	O
on	O
the	O
precise	O
values	O
of	O
the	O
estimates	O
,	O
uncertainties	O
,	O
and	O
the	O
number	O
of	O
remaining	O
steps	O
.	O
there	O
are	O
many	O
sophisticated	O
methods	O
for	O
balancing	O
exploration	O
and	O
exploitation	O
for	O
particular	O
mathematical	O
formulations	O
of	O
the	O
k-armed	O
bandit	O
and	O
related	O
problems	O
.	O
2.2.	O
action-value	B
methods	I
27	O
however	O
,	O
most	O
of	O
these	O
methods	O
make	O
strong	O
assumptions	O
about	O
stationarity	O
and	O
prior	O
knowledge	O
that	O
are	O
either	O
violated	O
or	O
impossible	O
to	O
verify	O
in	O
applications	O
and	O
in	O
the	O
full	O
reinforcement	B
learning	I
problem	O
that	O
we	O
consider	O
in	O
subsequent	O
chapters	O
.	O
the	O
guar-	O
antees	O
of	O
optimality	O
or	O
bounded	O
loss	O
for	O
these	O
methods	O
are	O
of	O
little	O
comfort	O
when	O
the	O
assumptions	O
of	O
their	O
theory	O
do	O
not	O
apply	O
.	O
in	O
this	O
book	O
we	O
do	O
not	O
worry	O
about	O
balancing	O
exploration	O
and	O
exploitation	O
in	O
a	O
so-	O
phisticated	O
way	O
;	O
we	O
worry	O
only	O
about	O
balancing	O
them	O
at	O
all	O
.	O
in	O
this	O
chapter	O
we	O
present	O
several	O
simple	O
balancing	O
methods	O
for	O
the	O
k-armed	O
bandit	O
problem	O
and	O
show	O
that	O
they	O
work	O
much	O
better	O
than	O
methods	O
that	O
always	O
exploit	O
.	O
the	O
need	O
to	O
balance	O
explo-	O
ration	O
and	O
exploitation	O
is	O
a	O
distinctive	O
challenge	O
that	O
arises	O
in	O
reinforcement	O
learning	O
;	O
the	O
simplicity	O
of	O
our	O
version	O
of	O
the	O
k-armed	O
bandit	O
problem	O
enables	O
us	O
to	O
show	O
this	O
in	O
a	O
particularly	O
clear	O
form	O
.	O
2.2	O
action-value	B
methods	I
we	O
begin	O
by	O
looking	O
more	O
closely	O
at	O
methods	O
for	O
estimating	O
the	O
values	O
of	O
actions	O
and	O
for	O
using	O
the	O
estimates	O
to	O
make	O
action	B
selection	O
decisions	O
,	O
which	O
we	O
collectively	O
call	O
action-	O
value	B
methods	O
.	O
recall	O
that	O
the	O
true	O
value	O
of	O
an	O
action	B
is	O
the	O
mean	O
reward	O
when	O
that	O
action	B
is	O
selected	O
.	O
one	O
natural	O
way	O
to	O
estimate	O
this	O
is	O
by	O
averaging	O
the	O
rewards	O
actually	O
received	O
:	O
qt	O
(	O
a	O
)	O
.	O
=	O
sum	O
of	O
rewards	O
when	O
a	O
taken	O
prior	O
to	O
t	O
number	O
of	O
times	O
a	O
taken	O
prior	O
to	O
t	O
,	O
(	O
2.1	O
)	O
i=1	O
ri	O
·	O
1ai=a	O
1ai=a	O
=	O
(	O
cid:80	O
)	O
t−1	O
(	O
cid:80	O
)	O
t−1	O
i=1	O
where	O
1predicate	O
denotes	O
the	O
random	O
variable	O
that	O
is	O
1	O
if	O
predicate	O
is	O
true	O
and	O
0	O
if	O
it	O
is	O
not	O
.	O
if	O
the	O
denominator	O
is	O
zero	O
,	O
then	O
we	O
instead	O
deﬁne	O
qt	O
(	O
a	O
)	O
as	O
some	O
default	O
value	B
,	O
such	O
as	O
0.	O
as	O
the	O
denominator	O
goes	O
to	O
inﬁnity	O
,	O
by	O
the	O
law	O
of	O
large	O
numbers	O
,	O
qt	O
(	O
a	O
)	O
converges	O
to	O
q∗	O
(	O
a	O
)	O
.	O
we	O
call	O
this	O
the	O
sample-average	B
method	I
for	O
estimating	O
action	B
values	O
because	O
each	O
estimate	O
is	O
an	O
average	O
of	O
the	O
sample	O
of	O
relevant	O
rewards	O
.	O
of	O
course	O
this	O
is	O
just	O
one	O
way	O
to	O
estimate	O
action	B
values	O
,	O
and	O
not	O
necessarily	O
the	O
best	O
one	O
.	O
nevertheless	O
,	O
for	O
now	O
let	O
us	O
stay	O
with	O
this	O
simple	O
estimation	O
method	O
and	O
turn	O
to	O
the	O
question	O
of	O
how	O
the	O
estimates	O
might	O
be	O
used	O
to	O
select	O
actions	O
.	O
the	O
simplest	O
action	B
selection	O
rule	O
is	O
to	O
select	O
one	O
of	O
the	O
actions	O
with	O
the	O
highest	O
estimated	O
value	B
,	O
that	O
is	O
,	O
one	O
of	O
the	O
greedy	O
actions	O
as	O
deﬁned	O
in	O
the	O
previous	O
section	O
.	O
if	O
there	O
is	O
more	O
than	O
one	O
greedy	O
action	O
,	O
then	O
a	O
selection	O
is	O
made	O
among	O
them	O
in	O
some	O
arbitrary	O
way	O
,	O
perhaps	O
randomly	O
.	O
we	O
write	O
this	O
greedy	O
action	O
selection	O
method	O
as	O
at	O
.	O
=	O
argmax	O
a	O
qt	O
(	O
a	O
)	O
,	O
(	O
2.2	O
)	O
where	O
argmaxa	O
denotes	O
the	O
action	B
a	O
for	O
which	O
the	O
expression	O
that	O
follows	O
is	O
maximized	O
(	O
again	O
,	O
with	O
ties	O
broken	O
arbitrarily	O
)	O
.	O
greedy	O
action	O
selection	O
always	O
exploits	O
current	O
knowledge	O
to	O
maximize	O
immediate	O
reward	O
;	O
it	O
spends	O
no	O
time	O
at	O
all	O
sampling	O
apparently	O
inferior	O
actions	O
to	O
see	O
if	O
they	O
might	O
really	O
be	O
better	O
.	O
a	O
simple	O
alternative	O
is	O
to	O
behave	O
greedily	O
most	O
of	O
the	O
time	O
,	O
but	O
every	O
once	O
in	O
a	O
while	O
,	O
say	O
with	O
small	O
probability	O
ε	O
,	O
instead	O
select	O
randomly	O
from	O
among	O
all	O
the	O
actions	O
with	O
equal	O
probability	O
,	O
independently	O
28	O
chapter	O
2	O
:	O
multi-armed	B
bandits	I
of	O
the	O
action-value	O
estimates	O
.	O
we	O
call	O
methods	O
using	O
this	O
near-greedy	O
action	B
selection	O
rule	O
ε-greedy	O
methods	O
.	O
an	O
advantage	O
of	O
these	O
methods	O
is	O
that	O
,	O
in	O
the	O
limit	O
as	O
the	O
number	O
of	O
steps	O
increases	O
,	O
every	O
action	B
will	O
be	O
sampled	O
an	O
inﬁnite	O
number	O
of	O
times	O
,	O
thus	O
ensuring	O
that	O
all	O
the	O
qt	O
(	O
a	O
)	O
converge	O
to	O
q∗	O
(	O
a	O
)	O
.	O
this	O
of	O
course	O
implies	O
that	O
the	O
probability	O
of	O
selecting	O
the	O
optimal	O
action	O
converges	O
to	O
greater	O
than	O
1	O
−	O
ε	O
,	O
that	O
is	O
,	O
to	O
near	O
certainty	O
.	O
these	O
are	O
just	O
asymptotic	O
guarantees	O
,	O
however	O
,	O
and	O
say	O
little	O
about	O
the	O
practical	O
eﬀectiveness	O
of	O
the	O
methods	O
.	O
exercise	O
2.1	O
in	O
ε-greedy	O
action	B
selection	O
,	O
for	O
the	O
case	O
of	O
two	O
actions	O
and	O
ε	O
=	O
0.5	O
,	O
what	O
(	O
cid:3	O
)	O
is	O
the	O
probability	O
that	O
the	O
greedy	O
action	O
is	O
selected	O
?	O
2.3	O
the	O
10-armed	O
testbed	O
to	O
roughly	O
assess	O
the	O
relative	O
eﬀectiveness	O
of	O
the	O
greedy	O
and	O
ε-greedy	O
action-value	O
meth-	O
ods	O
,	O
we	O
compared	O
them	O
numerically	O
on	O
a	O
suite	O
of	O
test	O
problems	O
.	O
this	O
was	O
a	O
set	O
of	O
2000	O
randomly	O
generated	O
k-armed	O
bandit	O
problems	O
with	O
k	O
=	O
10.	O
for	O
each	O
bandit	O
problem	O
,	O
such	O
as	O
the	O
one	O
shown	O
in	O
figure	O
2.1	O
,	O
the	O
action	B
values	O
,	O
q∗	O
(	O
a	O
)	O
,	O
a	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
10	O
,	O
were	O
selected	O
according	O
to	O
a	O
normal	O
(	O
gaussian	O
)	O
distribution	O
with	O
mean	O
0	O
and	O
variance	O
1.	O
then	O
,	O
when	O
figure	O
2.1	O
:	O
an	O
example	O
bandit	O
problem	O
from	O
the	O
10-armed	O
testbed	O
.	O
the	O
true	O
value	O
q∗	O
(	O
a	O
)	O
of	O
each	O
of	O
the	O
ten	O
actions	O
was	O
selected	O
according	O
to	O
a	O
normal	O
distribution	O
with	O
mean	O
zero	O
and	O
unit	O
variance	O
,	O
and	O
then	O
the	O
actual	O
rewards	O
were	O
selected	O
according	O
to	O
a	O
mean	O
q∗	O
(	O
a	O
)	O
unit	O
variance	O
normal	O
distribution	O
,	O
as	O
suggested	O
by	O
these	O
gray	O
distributions	O
.	O
0123-3-2-1q⇤	O
(	O
1	O
)	O
q⇤	O
(	O
2	O
)	O
q⇤	O
(	O
3	O
)	O
q⇤	O
(	O
4	O
)	O
q⇤	O
(	O
5	O
)	O
q⇤	O
(	O
6	O
)	O
q⇤	O
(	O
7	O
)	O
q⇤	O
(	O
8	O
)	O
q⇤	O
(	O
9	O
)	O
q⇤	O
(	O
10	O
)	O
rewarddistribution12635478910action	O
2.3.	O
the	O
10-armed	O
testbed	O
29	O
a	O
learning	O
method	O
applied	O
to	O
that	O
problem	O
selected	O
action	B
at	O
at	O
time	O
step	O
t	O
,	O
the	O
actual	O
reward	O
,	O
rt	O
,	O
was	O
selected	O
from	O
a	O
normal	O
distribution	O
with	O
mean	O
q∗	O
(	O
at	O
)	O
and	O
variance	O
1.	O
these	O
distributions	O
are	O
shown	O
in	O
gray	O
in	O
figure	O
2.1.	O
we	O
call	O
this	O
suite	O
of	O
test	O
tasks	O
the	O
10-armed	O
testbed	O
.	O
for	O
any	O
learning	O
method	O
,	O
we	O
can	O
measure	O
its	O
performance	O
and	O
behavior	O
as	O
it	O
improves	O
with	O
experience	O
over	O
1000	O
time	O
steps	O
when	O
applied	O
to	O
one	O
of	O
the	O
bandit	B
problems	I
.	O
this	O
makes	O
up	O
one	O
run	O
.	O
repeating	O
this	O
for	O
2000	O
independent	O
runs	O
,	O
each	O
with	O
a	O
diﬀerent	O
bandit	O
problem	O
,	O
we	O
obtained	O
measures	O
of	O
the	O
learning	O
algorithm	O
’	O
s	O
average	O
behavior	O
.	O
figure	O
2.2	O
compares	O
a	O
greedy	O
method	O
with	O
two	O
ε-greedy	O
methods	O
(	O
ε	O
=	O
0.01	O
and	O
ε	O
=	O
0.1	O
)	O
,	O
as	O
described	O
above	O
,	O
on	O
the	O
10-armed	O
testbed	O
.	O
all	O
the	O
methods	O
formed	O
their	O
action-value	O
estimates	O
using	O
the	O
sample-average	O
technique	O
.	O
the	O
upper	O
graph	O
shows	O
the	O
increase	O
in	O
expected	O
reward	O
with	O
experience	O
.	O
the	O
greedy	O
method	O
improved	O
slightly	O
faster	O
than	O
the	O
other	O
methods	O
at	O
the	O
very	O
beginning	O
,	O
but	O
then	O
leveled	O
oﬀ	O
at	O
a	O
lower	O
level	O
.	O
it	O
achieved	O
a	O
reward-per-step	O
of	O
only	O
about	O
1	O
,	O
compared	O
with	O
the	O
best	O
possible	O
of	O
about	O
1.55	O
on	O
this	O
testbed	O
.	O
the	O
greedy	O
method	O
performed	O
signiﬁcantly	O
worse	O
in	O
the	O
long	O
run	O
because	O
it	O
often	O
got	O
stuck	O
performing	O
suboptimal	O
actions	O
.	O
the	O
lower	O
graph	O
shows	O
that	O
the	O
greedy	O
figure	O
2.2	O
:	O
average	O
performance	O
of	O
ε-greedy	O
action-value	B
methods	I
on	O
the	O
10-armed	O
testbed	O
.	O
these	O
data	O
are	O
averages	O
over	O
2000	O
runs	O
with	O
diﬀerent	O
bandit	B
problems	I
.	O
all	O
methods	O
used	O
sample	O
averages	O
as	O
their	O
action-value	O
estimates	O
.	O
(	O
greedy	O
)	O
00.511.5averagereward02505007501000steps0	O
%	O
20	O
%	O
40	O
%	O
60	O
%	O
80	O
%	O
100	O
%	O
%	O
optimalaction02505007501000steps	O
11	O
''	O
=0.1	O
''	O
=0.01	O
''	O
=0.1	O
''	O
=0.01	O
''	O
=0	O
(	O
greedy	O
)	O
''	O
=0	O
30	O
chapter	O
2	O
:	O
multi-armed	B
bandits	I
method	O
found	O
the	O
optimal	O
action	O
in	O
only	O
approximately	O
one-third	O
of	O
the	O
tasks	O
.	O
in	O
the	O
other	O
two-thirds	O
,	O
its	O
initial	O
samples	O
of	O
the	O
optimal	O
action	O
were	O
disappointing	O
,	O
and	O
it	O
never	O
returned	O
to	O
it	O
.	O
the	O
ε-greedy	O
methods	O
eventually	O
performed	O
better	O
because	O
they	O
continued	O
to	O
explore	O
and	O
to	O
improve	O
their	O
chances	O
of	O
recognizing	O
the	O
optimal	O
action	O
.	O
the	O
ε	O
=	O
0.1	O
method	O
explored	O
more	O
,	O
and	O
usually	O
found	O
the	O
optimal	O
action	O
earlier	O
,	O
but	O
it	O
never	O
selected	O
that	O
action	B
more	O
than	O
91	O
%	O
of	O
the	O
time	O
.	O
the	O
ε	O
=	O
0.01	O
method	O
improved	O
more	O
slowly	O
,	O
but	O
eventually	O
would	O
perform	O
better	O
than	O
the	O
ε	O
=	O
0.1	O
method	O
on	O
both	O
performance	O
measures	O
shown	O
in	O
the	O
ﬁgure	O
.	O
it	O
is	O
also	O
possible	O
to	O
reduce	O
ε	O
over	O
time	O
to	O
try	O
to	O
get	O
the	O
best	O
of	O
both	O
high	O
and	O
low	O
values	O
.	O
the	O
advantage	O
of	O
ε-greedy	O
over	O
greedy	O
methods	O
depends	O
on	O
the	O
task	O
.	O
for	O
example	O
,	O
suppose	O
the	O
reward	O
variance	O
had	O
been	O
larger	O
,	O
say	O
10	O
instead	O
of	O
1.	O
with	O
noisier	O
rewards	O
it	O
takes	O
more	O
exploration	O
to	O
ﬁnd	O
the	O
optimal	O
action	O
,	O
and	O
ε-greedy	O
methods	O
should	O
fare	O
even	O
better	O
relative	O
to	O
the	O
greedy	O
method	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
the	O
reward	O
variances	O
were	O
zero	O
,	O
then	O
the	O
greedy	O
method	O
would	O
know	O
the	O
true	O
value	O
of	O
each	O
action	B
after	O
trying	O
it	O
once	O
.	O
in	O
this	O
case	O
the	O
greedy	O
method	O
might	O
actually	O
perform	O
best	O
because	O
it	O
would	O
soon	O
ﬁnd	O
the	O
optimal	O
action	O
and	O
then	O
never	O
explore	O
.	O
but	O
even	O
in	O
the	O
deterministic	O
case	O
there	O
is	O
a	O
large	O
advantage	O
to	O
exploring	O
if	O
we	O
weaken	O
some	O
of	O
the	O
other	O
assumptions	O
.	O
for	O
example	O
,	O
suppose	O
the	O
bandit	O
task	O
were	O
nonstationary	O
,	O
that	O
is	O
,	O
the	O
true	O
values	O
of	O
the	O
actions	O
changed	O
over	O
time	O
.	O
in	O
this	O
case	O
exploration	O
is	O
needed	O
even	O
in	O
the	O
deterministic	O
case	O
to	O
make	O
sure	O
one	O
of	O
the	O
nongreedy	O
actions	O
has	O
not	O
changed	O
to	O
become	O
better	O
than	O
the	O
greedy	O
one	O
.	O
as	O
we	O
shall	O
see	O
in	O
the	O
next	O
few	O
chapters	O
,	O
nonstationarity	B
is	O
the	O
case	O
most	O
commonly	O
encountered	O
in	O
reinforcement	O
learning	O
.	O
even	O
if	O
the	O
underlying	O
task	O
is	O
station-	O
ary	O
and	O
deterministic	O
,	O
the	O
learner	O
faces	O
a	O
set	O
of	O
banditlike	O
decision	O
tasks	O
each	O
of	O
which	O
changes	O
over	O
time	O
as	O
learning	O
proceeds	O
and	O
the	O
agent	O
’	O
s	O
decision-making	O
policy	B
changes	O
.	O
reinforcement	B
learning	I
requires	O
a	O
balance	O
between	O
exploration	O
and	O
exploitation	O
.	O
exercise	O
2.2	O
:	O
bandit	O
example	O
consider	O
a	O
k-armed	O
bandit	O
problem	O
with	O
k	O
=	O
4	O
actions	O
,	O
denoted	O
1	O
,	O
2	O
,	O
3	O
,	O
and	O
4.	O
consider	O
applying	O
to	O
this	O
problem	O
a	O
bandit	B
algorithm	I
using	O
ε-greedy	O
action	O
selection	O
,	O
sample-average	O
action-value	O
estimates	O
,	O
and	O
initial	O
estimates	O
of	O
q1	O
(	O
a	O
)	O
=	O
0	O
,	O
for	O
all	O
a.	O
suppose	O
the	O
initial	O
sequence	O
of	O
actions	O
and	O
rewards	O
is	O
a1	O
=	O
1	O
,	O
r1	O
=	O
1	O
,	O
a2	O
=	O
2	O
,	O
r2	O
=	O
1	O
,	O
a3	O
=	O
2	O
,	O
r3	O
=	O
2	O
,	O
a4	O
=	O
2	O
,	O
r4	O
=	O
2	O
,	O
a5	O
=	O
3	O
,	O
r5	O
=	O
0.	O
on	O
some	O
of	O
these	O
time	O
steps	O
the	O
ε	O
case	O
may	O
have	O
occurred	O
,	O
causing	O
an	O
action	B
to	O
be	O
selected	O
at	O
random	O
.	O
on	O
which	O
time	O
steps	O
did	O
this	O
deﬁnitely	O
occur	O
?	O
on	O
which	O
time	O
steps	O
could	O
this	O
(	O
cid:3	O
)	O
possibly	O
have	O
occurred	O
?	O
exercise	O
2.3	O
in	O
the	O
comparison	O
shown	O
in	O
figure	O
2.2	O
,	O
which	O
method	O
will	O
perform	O
best	O
in	O
the	O
long	O
run	O
in	O
terms	O
of	O
cumulative	O
reward	O
and	O
probability	O
of	O
selecting	O
the	O
best	O
action	B
?	O
(	O
cid:3	O
)	O
how	O
much	O
better	O
will	O
it	O
be	O
?	O
express	O
your	O
answer	O
quantitatively	O
.	O
2.4	O
incremental	B
implementation	I
the	O
action-value	B
methods	I
we	O
have	O
discussed	O
so	O
far	O
all	O
estimate	O
action	B
values	O
as	O
sample	O
averages	O
of	O
observed	O
rewards	O
.	O
we	O
now	O
turn	O
to	O
the	O
question	O
of	O
how	O
these	O
averages	O
can	O
be	O
computed	O
in	O
a	O
computationally	O
eﬃcient	O
manner	O
,	O
in	O
particular	O
,	O
with	O
constant	O
memory	O
and	O
constant	O
per-time-step	O
computation	O
.	O
2.4.	O
incremental	B
implementation	I
31	O
to	O
simplify	O
notation	O
we	O
concentrate	O
on	O
a	O
single	O
action	B
.	O
let	O
ri	O
now	O
denote	O
the	O
reward	O
received	O
after	O
the	O
ith	O
selection	O
of	O
this	O
action	B
,	O
and	O
let	O
qn	O
denote	O
the	O
estimate	O
of	O
its	O
action	B
value	O
after	O
it	O
has	O
been	O
selected	O
n	O
−	O
1	O
times	O
,	O
which	O
we	O
can	O
now	O
write	O
simply	O
as	O
.	O
=	O
qn	O
r1	O
+	O
r2	O
+	O
···	O
+	O
rn−1	O
n	O
−	O
1	O
.	O
the	O
obvious	O
implementation	O
would	O
be	O
to	O
maintain	O
a	O
record	O
of	O
all	O
the	O
rewards	O
and	O
then	O
perform	O
this	O
computation	O
whenever	O
the	O
estimated	O
value	B
was	O
needed	O
.	O
however	O
,	O
if	O
this	O
is	O
done	O
,	O
then	O
the	O
memory	O
and	O
computational	O
requirements	O
would	O
grow	O
over	O
time	O
as	O
more	O
rewards	O
are	O
seen	O
.	O
each	O
additional	O
reward	O
would	O
require	O
additional	O
memory	O
to	O
store	O
it	O
and	O
additional	O
computation	O
to	O
compute	O
the	O
sum	O
in	O
the	O
numerator	O
.	O
as	O
you	O
might	O
suspect	O
,	O
this	O
is	O
not	O
really	O
necessary	O
.	O
it	O
is	O
easy	O
to	O
devise	O
incremental	O
formulas	O
for	O
updating	O
averages	O
with	O
small	O
,	O
constant	O
computation	O
required	O
to	O
process	O
each	O
new	O
reward	O
.	O
given	O
qn	O
and	O
the	O
nth	O
reward	O
,	O
rn	O
,	O
the	O
new	O
average	O
of	O
all	O
n	O
rewards	O
can	O
be	O
computed	O
by	O
1	O
1	O
ri	O
1	O
n	O
n	O
(	O
cid:88	O
)	O
i=1	O
ri	O
(	O
cid:33	O
)	O
n	O
(	O
cid:32	O
)	O
rn	O
+	O
n−1	O
(	O
cid:88	O
)	O
i=1	O
n	O
(	O
cid:32	O
)	O
rn	O
+	O
(	O
n	O
−	O
1	O
)	O
n	O
(	O
cid:16	O
)	O
rn	O
+	O
(	O
n	O
−	O
1	O
)	O
qn	O
(	O
cid:17	O
)	O
n	O
(	O
cid:16	O
)	O
rn	O
+	O
nqn	O
−	O
qn	O
(	O
cid:17	O
)	O
n	O
(	O
cid:104	O
)	O
rn	O
−	O
qn	O
(	O
cid:105	O
)	O
,	O
1	O
1	O
1	O
n	O
−	O
1	O
ri	O
(	O
cid:33	O
)	O
n−1	O
(	O
cid:88	O
)	O
i=1	O
(	O
2.3	O
)	O
qn+1	O
=	O
=	O
=	O
=	O
=	O
=	O
qn	O
+	O
1	O
which	O
holds	O
even	O
for	O
n	O
=	O
1	O
,	O
obtaining	O
q2	O
=	O
r1	O
for	O
arbitrary	O
q1	O
.	O
this	O
implementation	O
requires	O
memory	O
only	O
for	O
qn	O
and	O
n	O
,	O
and	O
only	O
the	O
small	O
computation	O
(	O
2.3	O
)	O
for	O
each	O
new	O
reward	O
.	O
this	O
update	O
rule	O
(	O
2.3	O
)	O
is	O
of	O
a	O
form	O
that	O
occurs	O
frequently	O
throughout	O
this	O
book	O
.	O
the	O
general	O
form	O
is	O
newestimate	O
←	O
oldestimate	O
+	O
stepsize	O
(	O
cid:104	O
)	O
target	B
−	O
oldestimate	O
(	O
cid:105	O
)	O
.	O
the	O
expression	O
(	O
cid:2	O
)	O
target−oldestimate	O
(	O
cid:3	O
)	O
is	O
an	O
error	O
in	O
the	O
estimate	O
.	O
it	O
is	O
reduced	O
by	O
taking	O
a	O
step	O
toward	O
the	O
“	O
target.	O
”	O
the	O
target	B
is	O
presumed	O
to	O
indicate	O
a	O
desirable	O
direction	O
in	O
which	O
to	O
move	O
,	O
though	O
it	O
may	O
be	O
noisy	O
.	O
in	O
the	O
case	O
above	O
,	O
for	O
example	O
,	O
the	O
target	B
is	O
the	O
nth	O
reward	O
.	O
(	O
2.4	O
)	O
note	O
that	O
the	O
step-size	B
parameter	I
(	O
stepsize	O
)	O
used	O
in	O
the	O
incremental	O
method	O
(	O
2.3	O
)	O
in	O
processing	O
the	O
nth	O
reward	O
for	O
action	B
a	O
,	O
the	O
changes	O
from	O
time	O
step	O
to	O
time	O
step	O
.	O
32	O
chapter	O
2	O
:	O
multi-armed	B
bandits	I
method	O
uses	O
the	O
step-size	B
parameter	I
1	O
by	O
α	O
or	O
,	O
more	O
generally	O
,	O
by	O
αt	O
(	O
a	O
)	O
.	O
n	O
.	O
in	O
this	O
book	O
we	O
denote	O
the	O
step-size	B
parameter	I
pseudocode	O
for	O
a	O
complete	O
bandit	B
algorithm	I
using	O
incrementally	O
computed	O
sample	O
averages	O
and	O
ε-greedy	O
action	B
selection	O
is	O
shown	O
in	O
the	O
box	O
below	O
.	O
the	O
function	O
bandit	O
(	O
a	O
)	O
is	O
assumed	O
to	O
take	O
an	O
action	B
and	O
return	B
a	O
corresponding	O
reward	O
.	O
a	O
simple	O
bandit	B
algorithm	I
initialize	O
,	O
for	O
a	O
=	O
1	O
to	O
k	O
:	O
q	O
(	O
a	O
)	O
←	O
0	O
n	O
(	O
a	O
)	O
←	O
0	O
loop	O
forever	O
:	O
a	O
←	O
(	O
cid:26	O
)	O
arg	O
maxa	O
q	O
(	O
a	O
)	O
r	O
←	O
bandit	O
(	O
a	O
)	O
n	O
(	O
a	O
)	O
←	O
n	O
(	O
a	O
)	O
+	O
1	O
q	O
(	O
a	O
)	O
←	O
q	O
(	O
a	O
)	O
+	O
1	O
n	O
(	O
a	O
)	O
(	O
cid:2	O
)	O
r	O
−	O
q	O
(	O
a	O
)	O
(	O
cid:3	O
)	O
a	O
random	O
action	O
with	O
probability	O
ε	O
with	O
probability	O
1	O
−	O
ε	O
(	O
breaking	O
ties	O
randomly	O
)	O
2.5	O
tracking	O
a	O
nonstationary	O
problem	O
the	O
averaging	O
methods	O
discussed	O
so	O
far	O
are	O
appropriate	O
for	O
stationary	O
bandit	B
problems	I
,	O
that	O
is	O
,	O
for	B
bandit	I
problems	I
in	O
which	O
the	O
reward	O
probabilities	O
do	O
not	O
change	O
over	O
time	O
.	O
as	O
noted	O
earlier	O
,	O
we	O
often	O
encounter	O
reinforcement	B
learning	I
problems	O
that	O
are	O
eﬀectively	O
nonstationary	O
.	O
in	O
such	O
cases	O
it	O
makes	O
sense	O
to	O
give	O
more	O
weight	O
to	O
recent	O
rewards	O
than	O
to	O
long-past	O
rewards	O
.	O
one	O
of	O
the	O
most	O
popular	O
ways	O
of	O
doing	O
this	O
is	O
to	O
use	O
a	O
constant	O
step-size	B
parameter	I
.	O
for	O
example	O
,	O
the	O
incremental	O
update	O
rule	O
(	O
2.3	O
)	O
for	O
updating	O
an	O
average	O
qn	O
of	O
the	O
n	O
−	O
1	O
past	O
rewards	O
is	O
modiﬁed	O
to	O
be	O
where	O
the	O
step-size	B
parameter	I
α	O
∈	O
(	O
0	O
,	O
1	O
]	O
is	O
constant	O
.	O
this	O
results	O
in	O
qn+1	O
being	O
a	O
weighted	O
average	O
of	O
past	O
rewards	O
and	O
the	O
initial	O
estimate	O
q1	O
:	O
.	O
qn+1	O
=	O
qn	O
+	O
α	O
(	O
cid:104	O
)	O
rn	O
−	O
qn	O
(	O
cid:105	O
)	O
,	O
qn+1	O
=	O
qn	O
+	O
α	O
(	O
cid:104	O
)	O
rn	O
−	O
qn	O
(	O
cid:105	O
)	O
=	O
αrn	O
+	O
(	O
1	O
−	O
α	O
)	O
qn	O
=	O
αrn	O
+	O
(	O
1	O
−	O
α	O
)	O
[	O
αrn−1	O
+	O
(	O
1	O
−	O
α	O
)	O
qn−1	O
]	O
=	O
αrn	O
+	O
(	O
1	O
−	O
α	O
)	O
αrn−1	O
+	O
(	O
1	O
−	O
α	O
)	O
2qn−1	O
=	O
αrn	O
+	O
(	O
1	O
−	O
α	O
)	O
αrn−1	O
+	O
(	O
1	O
−	O
α	O
)	O
2αrn−2	O
+	O
=	O
(	O
1	O
−	O
α	O
)	O
nq1	O
+	O
···	O
+	O
(	O
1	O
−	O
α	O
)	O
n−1αr1	O
+	O
(	O
1	O
−	O
α	O
)	O
nq1	O
n	O
(	O
cid:88	O
)	O
i=1	O
α	O
(	O
1	O
−	O
α	O
)	O
n−iri	O
.	O
(	O
2.5	O
)	O
(	O
2.6	O
)	O
2.6.	O
optimistic	B
initial	I
values	I
33	O
we	O
call	O
this	O
a	O
weighted	O
average	O
because	O
the	O
sum	O
of	O
the	O
weights	O
is	O
(	O
1−	O
α	O
)	O
n	O
+	O
(	O
cid:80	O
)	O
n	O
i=1	O
α	O
(	O
1−	O
α	O
)	O
n−i	O
=	O
1	O
,	O
as	O
you	O
can	O
check	O
for	O
yourself	O
.	O
note	O
that	O
the	O
weight	O
,	O
α	O
(	O
1−	O
α	O
)	O
n−i	O
,	O
given	O
to	O
the	O
reward	O
ri	O
depends	O
on	O
how	O
many	O
rewards	O
ago	O
,	O
n−	O
i	O
,	O
it	O
was	O
observed	O
.	O
the	O
quantity	O
1−	O
α	O
is	O
less	O
than	O
1	O
,	O
and	O
thus	O
the	O
weight	O
given	O
to	O
ri	O
decreases	O
as	O
the	O
number	O
of	O
intervening	O
rewards	O
increases	O
.	O
in	O
fact	O
,	O
the	O
weight	O
decays	O
exponentially	O
according	O
to	O
the	O
exponent	O
on	O
1	O
−	O
α	O
.	O
(	O
if	O
1	O
−	O
α	O
=	O
0	O
,	O
then	O
all	O
the	O
weight	O
goes	O
on	O
the	O
very	O
last	O
reward	O
,	O
rn	O
,	O
because	O
of	O
the	O
convention	O
that	O
00	O
=	O
1	O
.	O
)	O
accordingly	O
,	O
this	O
is	O
sometimes	O
called	O
an	O
exponential	O
recency-weighted	O
average	O
.	O
sometimes	O
it	O
is	O
convenient	O
to	O
vary	O
the	O
step-size	B
parameter	I
from	O
step	O
to	O
step	O
.	O
let	O
αn	O
(	O
a	O
)	O
denote	O
the	O
step-size	B
parameter	I
used	O
to	O
process	O
the	O
reward	O
received	O
after	O
the	O
nth	O
selection	O
of	O
action	O
a.	O
as	O
we	O
have	O
noted	O
,	O
the	O
choice	O
αn	O
(	O
a	O
)	O
=	O
1	O
n	O
results	O
in	O
the	O
sample-	O
average	O
method	O
,	O
which	O
is	O
guaranteed	O
to	O
converge	O
to	O
the	O
true	O
action	O
values	O
by	O
the	O
law	O
of	O
large	O
numbers	O
.	O
but	O
of	O
course	O
convergence	O
is	O
not	O
guaranteed	O
for	O
all	O
choices	O
of	O
the	O
sequence	O
{	O
αn	O
(	O
a	O
)	O
}	O
.	O
a	O
well-known	O
result	O
in	O
stochastic	O
approximation	O
theory	O
gives	O
us	O
the	O
conditions	O
required	O
to	O
assure	O
convergence	O
with	O
probability	O
1	O
:	O
∞	O
(	O
cid:88	O
)	O
n=1	O
αn	O
(	O
a	O
)	O
=	O
∞	O
and	O
∞	O
(	O
cid:88	O
)	O
n=1	O
α2	O
n	O
(	O
a	O
)	O
<	O
∞	O
.	O
(	O
2.7	O
)	O
the	O
ﬁrst	O
condition	O
is	O
required	O
to	O
guarantee	O
that	O
the	O
steps	O
are	O
large	O
enough	O
to	O
eventually	O
overcome	O
any	O
initial	O
conditions	O
or	O
random	O
ﬂuctuations	O
.	O
the	O
second	O
condition	O
guarantees	O
that	O
eventually	O
the	O
steps	O
become	O
small	O
enough	O
to	O
assure	O
convergence	O
.	O
note	O
that	O
both	O
convergence	O
conditions	O
are	O
met	O
for	O
the	O
sample-average	O
case	O
,	O
αn	O
(	O
a	O
)	O
=	O
1	O
n	O
,	O
but	O
not	O
for	O
the	O
case	O
of	O
constant	O
step-size	B
parameter	I
,	O
αn	O
(	O
a	O
)	O
=	O
α.	O
in	O
the	O
latter	O
case	O
,	O
the	O
second	O
condition	O
is	O
not	O
met	O
,	O
indicating	O
that	O
the	O
estimates	O
never	O
completely	O
converge	O
but	O
continue	O
to	O
vary	O
in	O
response	O
to	O
the	O
most	O
recently	O
received	O
rewards	O
.	O
as	O
we	O
mentioned	O
above	O
,	O
this	O
is	O
actually	O
desirable	O
in	O
a	O
nonstationary	O
environment	B
,	O
and	O
problems	O
that	O
are	O
eﬀectively	O
nonstationary	O
are	O
the	O
most	O
common	O
in	O
reinforcement	O
learning	O
.	O
in	O
addition	O
,	O
se-	O
quences	O
of	O
step-size	O
parameters	O
that	O
meet	O
the	O
conditions	O
(	O
2.7	O
)	O
often	O
converge	O
very	O
slowly	O
or	O
need	O
considerable	O
tuning	O
in	O
order	O
to	O
obtain	O
a	O
satisfactory	O
convergence	O
rate	O
.	O
although	O
sequences	O
of	O
step-size	O
parameters	O
that	O
meet	O
these	O
convergence	O
conditions	O
are	O
often	O
used	O
in	O
theoretical	O
work	O
,	O
they	O
are	O
seldom	O
used	O
in	O
applications	O
and	O
empirical	O
research	O
.	O
exercise	O
2.4	O
if	O
the	O
step-size	O
parameters	O
,	O
αn	O
,	O
are	O
not	O
constant	O
,	O
then	O
the	O
estimate	O
qn	O
is	O
a	O
weighted	O
average	O
of	O
previously	O
received	O
rewards	O
with	O
a	O
weighting	O
diﬀerent	O
from	O
that	O
given	O
by	O
(	O
2.6	O
)	O
.	O
what	O
is	O
the	O
weighting	O
on	O
each	O
prior	O
reward	O
for	O
the	O
general	O
case	O
,	O
analogous	O
(	O
cid:3	O
)	O
to	O
(	O
2.6	O
)	O
,	O
in	O
terms	O
of	O
the	O
sequence	O
of	O
step-size	O
parameters	O
?	O
exercise	O
2.5	O
(	O
programming	O
)	O
design	O
and	O
conduct	O
an	O
experiment	O
to	O
demonstrate	O
the	O
diﬃculties	O
that	O
sample-average	O
methods	O
have	O
for	O
nonstationary	O
problems	O
.	O
use	O
a	O
modiﬁed	O
version	O
of	O
the	O
10-armed	O
testbed	O
in	O
which	O
all	O
the	O
q∗	O
(	O
a	O
)	O
start	O
out	O
equal	O
and	O
then	O
take	O
independent	O
random	O
walks	O
(	O
say	O
by	O
adding	O
a	O
normally	O
distributed	O
increment	O
with	O
mean	O
zero	O
and	O
standard	O
deviation	O
0.01	O
to	O
all	O
the	O
q∗	O
(	O
a	O
)	O
on	O
each	O
step	O
)	O
.	O
prepare	O
plots	O
like	O
figure	O
2.2	O
for	O
an	O
action-value	O
method	O
using	O
sample	O
averages	O
,	O
incrementally	O
computed	O
,	O
and	O
another	O
action-value	O
method	O
using	O
a	O
constant	O
step-size	B
parameter	I
,	O
α	O
=	O
0.1.	O
use	O
(	O
cid:3	O
)	O
ε	O
=	O
0.1	O
and	O
longer	O
runs	O
,	O
say	O
of	O
10,000	O
steps	O
.	O
34	O
chapter	O
2	O
:	O
multi-armed	B
bandits	I
2.6	O
optimistic	B
initial	I
values	I
all	O
the	O
methods	O
we	O
have	O
discussed	O
so	O
far	O
are	O
dependent	O
to	O
some	O
extent	O
on	O
the	O
initial	O
action-value	O
estimates	O
,	O
q1	O
(	O
a	O
)	O
.	O
in	O
the	O
language	O
of	O
statistics	O
,	O
these	O
methods	O
are	O
biased	O
by	O
their	O
initial	O
estimates	O
.	O
for	O
the	O
sample-average	O
methods	O
,	O
the	O
bias	O
disappears	O
once	O
all	O
actions	O
have	O
been	O
selected	O
at	O
least	O
once	O
,	O
but	O
for	O
methods	O
with	O
constant	O
α	O
,	O
the	O
bias	O
is	O
permanent	O
,	O
though	O
decreasing	O
over	O
time	O
as	O
given	O
by	O
(	O
2.6	O
)	O
.	O
in	O
practice	O
,	O
this	O
kind	O
of	O
bias	O
is	O
usually	O
not	O
a	O
problem	O
and	O
can	O
sometimes	O
be	O
very	O
helpful	O
.	O
the	O
downside	O
is	O
that	O
the	O
initial	O
estimates	O
become	O
,	O
in	O
eﬀect	O
,	O
a	O
set	O
of	O
parameters	O
that	O
must	O
be	O
picked	O
by	O
the	O
user	O
,	O
if	O
only	O
to	O
set	O
them	O
all	O
to	O
zero	O
.	O
the	O
upside	O
is	O
that	O
they	O
provide	O
an	O
easy	O
way	O
to	O
supply	O
some	O
prior	B
knowledge	I
about	O
what	O
level	O
of	O
rewards	O
can	O
be	O
expected	O
.	O
initial	O
action	B
values	O
can	O
also	O
be	O
used	O
as	O
a	O
simple	O
way	O
to	O
encourage	O
exploration	O
.	O
sup-	O
pose	O
that	O
instead	O
of	O
setting	O
the	O
initial	O
action	B
values	O
to	O
zero	O
,	O
as	O
we	O
did	O
in	O
the	O
10-armed	O
testbed	O
,	O
we	O
set	O
them	O
all	O
to	O
+5	O
.	O
recall	O
that	O
the	O
q∗	O
(	O
a	O
)	O
in	O
this	O
problem	O
are	O
selected	O
from	O
a	O
normal	O
distribution	O
with	O
mean	O
0	O
and	O
variance	O
1.	O
an	O
initial	O
estimate	O
of	O
+5	O
is	O
thus	O
wildly	O
optimistic	O
.	O
but	O
this	O
optimism	O
encourages	O
action-value	B
methods	I
to	O
explore	O
.	O
whichever	O
actions	O
are	O
initially	O
selected	O
,	O
the	O
reward	O
is	O
less	O
than	O
the	O
starting	O
estimates	O
;	O
the	O
learner	O
switches	O
to	O
other	O
actions	O
,	O
being	O
“	O
disappointed	O
”	O
with	O
the	O
rewards	O
it	O
is	O
receiving	O
.	O
the	O
result	O
is	O
that	O
all	O
actions	O
are	O
tried	O
several	O
times	O
before	O
the	O
value	B
estimates	O
converge	O
.	O
the	O
system	O
does	O
a	O
fair	O
amount	O
of	O
exploration	O
even	O
if	O
greedy	O
actions	O
are	O
selected	O
all	O
the	O
time	O
.	O
figure	O
2.3	O
shows	O
the	O
performance	O
on	O
the	O
10-armed	O
bandit	O
testbed	O
of	O
a	O
greedy	O
method	O
using	O
q1	O
(	O
a	O
)	O
=	O
+5	O
,	O
for	O
all	O
a.	O
for	O
comparison	O
,	O
also	O
shown	O
is	O
an	O
ε-greedy	O
method	O
with	O
q1	O
(	O
a	O
)	O
=	O
0.	O
initially	O
,	O
the	O
optimistic	O
method	O
performs	O
worse	O
because	O
it	O
explores	O
more	O
,	O
but	O
eventually	O
it	O
performs	O
better	O
because	O
its	O
exploration	O
decreases	O
with	O
time	O
.	O
we	O
call	O
this	O
technique	O
for	O
encouraging	O
exploration	O
optimistic	B
initial	I
values	I
.	O
we	O
regard	O
it	O
as	O
a	O
simple	O
trick	O
that	O
can	O
be	O
quite	O
eﬀective	O
on	O
stationary	O
problems	O
,	O
but	O
it	O
is	O
far	O
from	O
being	O
a	O
generally	O
useful	O
approach	O
to	O
encouraging	O
exploration	O
.	O
for	O
example	O
,	O
it	O
is	O
not	O
well	O
suited	O
to	O
nonstationary	O
problems	O
because	O
its	O
drive	O
for	O
exploration	O
is	O
inherently	O
figure	O
2.3	O
:	O
the	O
eﬀect	O
of	O
optimistic	O
initial	O
action-value	O
estimates	O
on	O
the	O
10-armed	O
testbed	O
.	O
both	O
methods	O
used	O
a	O
constant	O
step-size	B
parameter	I
,	O
α	O
=	O
0.1	O
.	O
0	O
%	O
20	O
%	O
40	O
%	O
60	O
%	O
80	O
%	O
100	O
%	O
%	O
optimalaction02004006008001000playsoptimistic	O
,	O
greedyq0	O
=	O
5	O
,	O
!	O
!	O
=	O
0realistic	O
,	O
!	O
-greedyq0	O
=	O
0	O
,	O
!	O
!	O
=	O
0.111steps1optimistic	O
,	O
greedyq1=5	O
,	O
''	O
=0realistic	O
,	O
-greedy	O
''	O
q1=0	O
,	O
''	O
=0.1	O
2.7.	O
upper-conﬁdence-bound	O
action	B
selection	O
35	O
temporary	O
.	O
if	O
the	O
task	O
changes	O
,	O
creating	O
a	O
renewed	O
need	O
for	O
exploration	O
,	O
this	O
method	O
can	O
not	O
help	O
.	O
indeed	O
,	O
any	O
method	O
that	O
focuses	O
on	O
the	O
initial	O
conditions	O
in	O
any	O
special	O
way	O
is	O
unlikely	O
to	O
help	O
with	O
the	O
general	O
nonstationary	O
case	O
.	O
the	O
beginning	O
of	O
time	O
occurs	O
only	O
once	O
,	O
and	O
thus	O
we	O
should	O
not	O
focus	O
on	O
it	O
too	O
much	O
.	O
this	O
criticism	O
applies	O
as	O
well	O
to	O
the	O
sample-average	O
methods	O
,	O
which	O
also	O
treat	O
the	O
beginning	O
of	O
time	O
as	O
a	O
special	O
event	O
,	O
averaging	O
all	O
subsequent	O
rewards	O
with	O
equal	O
weights	O
.	O
nevertheless	O
,	O
all	O
of	O
these	O
methods	O
are	O
very	O
simple	O
,	O
and	O
one	O
of	O
them—or	O
some	O
simple	O
combination	O
of	O
them—is	O
often	O
adequate	O
in	O
practice	O
.	O
in	O
the	O
rest	O
of	O
this	O
book	O
we	O
make	O
frequent	O
use	O
of	O
several	O
of	O
these	O
simple	O
exploration	O
techniques	O
.	O
exercise	O
2.6	O
:	O
mysterious	O
spikes	O
the	O
results	O
shown	O
in	O
figure	O
2.3	O
should	O
be	O
quite	O
reliable	O
because	O
they	O
are	O
averages	O
over	O
2000	O
individual	O
,	O
randomly	O
chosen	O
10-armed	O
bandit	O
tasks	O
.	O
why	O
,	O
then	O
,	O
are	O
there	O
oscillations	O
and	O
spikes	O
in	O
the	O
early	O
part	O
of	O
the	O
curve	O
for	O
the	O
optimistic	O
method	O
?	O
in	O
other	O
words	O
,	O
what	O
might	O
make	O
this	O
method	O
perform	O
particularly	O
better	O
or	O
(	O
cid:3	O
)	O
worse	O
,	O
on	O
average	O
,	O
on	O
particular	O
early	O
steps	O
?	O
exercise	O
2.7	O
:	O
unbiased	O
constant-step-size	O
trick	O
in	O
most	O
of	O
this	O
chapter	O
we	O
have	O
used	O
sample	O
averages	O
to	O
estimate	O
action	B
values	O
because	O
sample	O
averages	O
do	O
not	O
produce	O
the	O
initial	O
bias	O
that	O
constant	O
step	O
sizes	O
do	O
(	O
see	O
the	O
analysis	O
in	O
(	O
2.6	O
)	O
)	O
.	O
however	O
,	O
sample	O
averages	O
are	O
not	O
a	O
completely	O
satisfactory	O
solution	O
because	O
they	O
may	O
perform	O
poorly	O
on	O
nonstationary	O
problems	O
.	O
is	O
it	O
possible	O
to	O
avoid	O
the	O
bias	O
of	O
constant	O
step	O
sizes	O
while	O
retaining	O
their	O
advantages	O
on	O
nonstationary	O
problems	O
?	O
one	O
way	O
is	O
to	O
use	O
a	O
step	O
size	O
of	O
.	O
=	O
α/¯ot	O
,	O
βt	O
(	O
2.8	O
)	O
where	O
α	O
>	O
0	O
is	O
a	O
conventional	O
constant	O
step	O
size	O
,	O
and	O
¯ot	O
is	O
a	O
trace	O
of	O
one	O
that	O
starts	O
at	O
0	O
:	O
¯ot+1	O
.	O
=	O
¯ot	O
+	O
α	O
(	O
1	O
−	O
¯ot	O
)	O
,	O
for	O
t	O
≥	O
1	O
,	O
with	O
¯o1	O
.	O
=	O
α	O
.	O
(	O
2.9	O
)	O
carry	O
out	O
an	O
analysis	O
like	O
that	O
in	O
(	O
2.6	O
)	O
to	O
show	O
that	O
βt	O
is	O
an	O
exponential	O
recency-weighted	O
(	O
cid:3	O
)	O
average	O
without	O
initial	O
bias	O
.	O
2.7	O
upper-conﬁdence-bound	O
action	B
selection	O
exploration	O
is	O
needed	O
because	O
there	O
is	O
always	O
uncertainty	O
about	O
the	O
accuracy	O
of	O
the	O
action-value	O
estimates	O
.	O
the	O
greedy	O
actions	O
are	O
those	O
that	O
look	O
best	O
at	O
present	O
,	O
but	O
some	O
of	O
the	O
other	O
actions	O
may	O
actually	O
be	O
better	O
.	O
ε-greedy	O
action	O
selection	O
forces	O
the	O
non-greedy	O
actions	O
to	O
be	O
tried	O
,	O
but	O
indiscriminately	O
,	O
with	O
no	O
preference	O
for	O
those	O
that	O
are	O
nearly	O
greedy	O
or	O
particularly	O
uncertain	O
.	O
it	O
would	O
be	O
better	O
to	O
select	O
among	O
the	O
non-greedy	O
actions	O
according	O
to	O
their	O
potential	O
for	O
actually	O
being	O
optimal	O
,	O
taking	O
into	O
account	O
both	O
how	O
close	O
their	O
estimates	O
are	O
to	O
being	O
maximal	O
and	O
the	O
uncertainties	O
in	O
those	O
estimates	O
.	O
one	O
eﬀective	O
way	O
of	O
doing	O
this	O
is	O
to	O
select	O
actions	O
according	O
to	O
at	O
.	O
=	O
argmax	O
a	O
(	O
cid:34	O
)	O
qt	O
(	O
a	O
)	O
+	O
c	O
(	O
cid:115	O
)	O
ln	O
t	O
nt	O
(	O
a	O
)	O
(	O
cid:35	O
)	O
,	O
(	O
2.10	O
)	O
36	O
chapter	O
2	O
:	O
multi-armed	B
bandits	I
where	O
ln	O
t	O
denotes	O
the	O
natural	O
logarithm	O
of	O
t	O
(	O
the	O
number	O
that	O
e	O
≈	O
2.71828	O
would	O
have	O
to	O
be	O
raised	O
to	O
in	O
order	O
to	O
equal	O
t	O
)	O
,	O
nt	O
(	O
a	O
)	O
denotes	O
the	O
number	O
of	O
times	O
that	O
action	B
a	O
has	O
been	O
selected	O
prior	O
to	O
time	O
t	O
(	O
the	O
denominator	O
in	O
(	O
2.1	O
)	O
)	O
,	O
and	O
the	O
number	O
c	O
>	O
0	O
controls	O
the	O
degree	O
of	O
exploration	O
.	O
if	O
nt	O
(	O
a	O
)	O
=	O
0	O
,	O
then	O
a	O
is	O
considered	O
to	O
be	O
a	O
maximizing	O
action	B
.	O
the	O
idea	O
of	O
this	O
upper	O
conﬁdence	O
bound	O
(	O
ucb	O
)	O
action	B
selection	O
is	O
that	O
the	O
square-root	O
term	O
is	O
a	O
measure	O
of	O
the	O
uncertainty	O
or	O
variance	O
in	O
the	O
estimate	O
of	O
a	O
’	O
s	O
value	B
.	O
the	O
quantity	O
being	O
max	O
’	O
ed	O
over	O
is	O
thus	O
a	O
sort	O
of	O
upper	O
bound	O
on	O
the	O
possible	O
true	O
value	O
of	O
action	O
a	O
,	O
with	O
c	O
determining	O
the	O
conﬁdence	O
level	O
.	O
each	O
time	O
a	O
is	O
selected	O
the	O
uncertainty	O
is	O
presumably	O
reduced	O
:	O
nt	O
(	O
a	O
)	O
increments	O
,	O
and	O
,	O
as	O
it	O
appears	O
in	O
the	O
denominator	O
,	O
the	O
uncertainty	O
term	O
decreases	O
.	O
on	O
the	O
other	O
hand	O
,	O
each	O
time	O
an	O
action	B
other	O
than	O
a	O
is	O
selected	O
,	O
t	O
increases	O
but	O
nt	O
(	O
a	O
)	O
does	O
not	O
;	O
because	O
t	O
appears	O
in	O
the	O
numerator	O
,	O
the	O
uncertainty	O
estimate	O
increases	O
.	O
the	O
use	O
of	O
the	O
natural	O
logarithm	O
means	O
that	O
the	O
increases	O
get	O
smaller	O
over	O
time	O
,	O
but	O
are	O
unbounded	O
;	O
all	O
actions	O
will	O
eventually	O
be	O
selected	O
,	O
but	O
actions	O
with	O
lower	O
value	B
estimates	O
,	O
or	O
that	O
have	O
already	O
been	O
selected	O
frequently	O
,	O
will	O
be	O
selected	O
with	O
decreasing	O
frequency	O
over	O
time	O
.	O
results	O
with	O
ucb	O
on	O
the	O
10-armed	O
testbed	O
are	O
shown	O
in	O
figure	O
2.4.	O
ucb	O
often	O
performs	O
well	O
,	O
as	O
shown	O
here	O
,	O
but	O
is	O
more	O
diﬃcult	O
than	O
ε-greedy	O
to	O
extend	O
beyond	O
bandits	O
to	O
the	O
more	O
general	O
reinforcement	O
learning	O
settings	O
considered	O
in	O
the	O
rest	O
of	O
this	O
book	O
.	O
one	O
diﬃculty	O
is	O
in	O
dealing	O
with	O
nonstationary	O
problems	O
;	O
methods	O
more	O
complex	O
than	O
those	O
presented	O
in	O
section	O
2.5	O
would	O
be	O
needed	O
.	O
another	O
diﬃculty	O
is	O
dealing	O
with	O
large	O
state	B
spaces	O
,	O
particularly	O
when	O
using	O
function	B
approximation	I
as	O
developed	O
in	O
part	O
ii	O
of	O
this	O
book	O
.	O
in	O
these	O
more	O
advanced	O
settings	O
the	O
idea	O
of	O
ucb	O
action	B
selection	O
is	O
usually	O
not	O
practical	O
.	O
exercise	O
2.8	O
:	O
ucb	O
spikes	O
in	O
figure	O
2.4	O
the	O
ucb	O
algorithm	O
shows	O
a	O
distinct	O
spike	O
in	O
performance	O
on	O
the	O
11th	O
step	O
.	O
why	O
is	O
this	O
?	O
note	O
that	O
for	O
your	O
answer	O
to	O
be	O
fully	O
satisfactory	O
it	O
must	O
explain	O
both	O
why	O
the	O
reward	O
increases	O
on	O
the	O
11th	O
step	O
and	O
why	O
it	O
figure	O
2.4	O
:	O
average	O
performance	O
of	O
ucb	O
action	B
selection	O
on	O
the	O
10-armed	O
testbed	O
.	O
as	O
shown	O
,	O
ucb	O
generally	O
performs	O
better	O
than	O
ε-greedy	O
action	O
selection	O
,	O
except	O
in	O
the	O
ﬁrst	O
k	O
steps	O
,	O
when	O
it	O
selects	O
randomly	O
among	O
the	O
as-yet-untried	O
actions	O
.	O
11250500750100000.511.5-greedy	O
	O
=	O
0.1ucb	O
c	O
=	O
2averagerewardsteps	O
2.8.	O
gradient	B
bandit	O
algorithms	O
37	O
decreases	O
on	O
the	O
subsequent	O
steps	O
.	O
hint	O
:	O
if	O
c	O
=	O
1	O
,	O
then	O
the	O
spike	O
is	O
less	O
prominent	O
.	O
(	O
cid:3	O
)	O
2.8	O
gradient	B
bandit	O
algorithms	O
so	O
far	O
in	O
this	O
chapter	O
we	O
have	O
considered	O
methods	O
that	O
estimate	O
action	B
values	O
and	O
use	O
those	O
estimates	O
to	O
select	O
actions	O
.	O
this	O
is	O
often	O
a	O
good	O
approach	O
,	O
but	O
it	O
is	O
not	O
the	O
only	O
one	O
possible	O
.	O
in	O
this	O
section	O
we	O
consider	O
learning	O
a	O
numerical	O
preference	O
for	O
each	O
action	B
a	O
,	O
which	O
we	O
denote	O
ht	O
(	O
a	O
)	O
.	O
the	O
larger	O
the	O
preference	O
,	O
the	O
more	O
often	O
that	O
action	B
is	O
taken	O
,	O
but	O
the	O
preference	O
has	O
no	O
interpretation	O
in	O
terms	O
of	O
reward	O
.	O
only	O
the	O
relative	O
preference	O
of	O
one	O
action	B
over	O
another	O
is	O
important	O
;	O
if	O
we	O
add	O
1000	O
to	O
all	O
the	O
action	B
preferences	I
there	O
is	O
no	O
eﬀect	O
on	O
the	O
action	B
probabilities	O
,	O
which	O
are	O
determined	O
according	O
to	O
a	O
soft-max	B
distribution	O
(	O
i.e.	O
,	O
gibbs	O
or	O
boltzmann	O
distribution	O
)	O
as	O
follows	O
:	O
pr	O
{	O
at	O
=	O
a	O
}	O
.	O
=	O
eht	O
(	O
a	O
)	O
b=1	O
eht	O
(	O
b	O
)	O
(	O
cid:80	O
)	O
k	O
.	O
=	O
πt	O
(	O
a	O
)	O
,	O
(	O
2.11	O
)	O
where	O
here	O
we	O
have	O
also	O
introduced	O
a	O
useful	O
new	O
notation	O
,	O
πt	O
(	O
a	O
)	O
,	O
for	O
the	O
probability	O
of	O
taking	O
action	B
a	O
at	O
time	O
t.	O
initially	O
all	O
action	B
preferences	I
are	O
the	O
same	O
(	O
e.g.	O
,	O
h1	O
(	O
a	O
)	O
=	O
0	O
,	O
for	O
all	O
a	O
)	O
so	O
that	O
all	O
actions	O
have	O
an	O
equal	O
probability	O
of	O
being	O
selected	O
.	O
exercise	O
2.9	O
show	O
that	O
in	O
the	O
case	O
of	O
two	O
actions	O
,	O
the	O
soft-max	B
distribution	O
is	O
the	O
same	O
as	O
that	O
given	O
by	O
the	O
logistic	O
,	O
or	O
sigmoid	O
,	O
function	O
often	O
used	O
in	O
statistics	O
and	O
artiﬁcial	O
(	O
cid:3	O
)	O
neural	B
networks	I
.	O
there	O
is	O
a	O
natural	O
learning	O
algorithm	O
for	O
this	O
setting	O
based	O
on	O
the	O
idea	O
of	O
stochastic	O
gradient	B
ascent	O
.	O
on	O
each	O
step	O
,	O
after	O
selecting	O
action	O
at	O
and	O
receiving	O
the	O
reward	O
rt	O
,	O
the	O
action	B
preferences	I
are	O
updated	O
by	O
:	O
ht+1	O
(	O
at	O
)	O
ht+1	O
(	O
a	O
)	O
.	O
.	O
=	O
ht	O
(	O
at	O
)	O
+	O
α	O
(	O
cid:0	O
)	O
rt	O
−	O
¯rt	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
1	O
−	O
πt	O
(	O
at	O
)	O
(	O
cid:1	O
)	O
,	O
=	O
ht	O
(	O
a	O
)	O
−	O
α	O
(	O
cid:0	O
)	O
rt	O
−	O
¯rt	O
(	O
cid:1	O
)	O
πt	O
(	O
a	O
)	O
,	O
and	O
for	O
all	O
a	O
(	O
cid:54	O
)	O
=	O
at	O
,	O
(	O
2.12	O
)	O
where	O
α	O
>	O
0	O
is	O
a	O
step-size	B
parameter	I
,	O
and	O
¯rt	O
∈	O
r	O
is	O
the	O
average	O
of	O
all	O
the	O
rewards	O
up	O
through	O
and	O
including	O
time	O
t	O
,	O
which	O
can	O
be	O
computed	O
incrementally	O
as	O
described	O
in	O
section	O
2.4	O
(	O
or	O
section	O
2.5	O
if	O
the	O
problem	O
is	O
nonstationary	O
)	O
.	O
the	O
¯rt	O
term	O
serves	O
as	O
a	O
baseline	B
with	O
which	O
the	O
reward	O
is	O
compared	O
.	O
if	O
the	O
reward	O
is	O
higher	O
than	O
the	O
baseline	B
,	O
then	O
the	O
probability	O
of	O
taking	O
at	O
in	O
the	O
future	O
is	O
increased	O
,	O
and	O
if	O
the	O
reward	O
is	O
below	O
baseline	B
,	O
then	O
probability	O
is	O
decreased	O
.	O
the	O
non-selected	O
actions	O
move	O
in	O
the	O
opposite	O
direction	O
.	O
figure	O
2.5	O
shows	O
results	O
with	O
the	O
gradient	B
bandit	O
algorithm	O
on	O
a	O
variant	O
of	O
the	O
10-	O
armed	O
testbed	O
in	O
which	O
the	O
true	O
expected	O
rewards	O
were	O
selected	O
according	O
to	O
a	O
normal	O
distribution	O
with	O
a	O
mean	O
of	O
+4	O
instead	O
of	O
zero	O
(	O
and	O
with	O
unit	O
variance	O
as	O
before	O
)	O
.	O
this	O
shifting	O
up	O
of	O
all	O
the	O
rewards	O
has	O
absolutely	O
no	O
eﬀect	O
on	O
the	O
gradient	B
bandit	O
algorithm	O
because	O
of	O
the	O
reward	O
baseline	O
term	O
,	O
which	O
instantaneously	O
adapts	O
to	O
the	O
new	O
level	O
.	O
but	O
if	O
the	O
baseline	B
were	O
omitted	O
(	O
that	O
is	O
,	O
if	O
¯rt	O
was	O
taken	O
to	O
be	O
constant	O
zero	O
in	O
(	O
2.12	O
)	O
)	O
,	O
then	O
performance	O
would	O
be	O
signiﬁcantly	O
degraded	O
,	O
as	O
shown	O
in	O
the	O
ﬁgure	O
.	O
38	O
chapter	O
2	O
:	O
multi-armed	B
bandits	I
figure	O
2.5	O
:	O
average	O
performance	O
of	O
the	O
gradient	B
bandit	O
algorithm	O
with	O
and	O
without	O
a	O
reward	O
baseline	O
on	O
the	O
10-armed	O
testbed	O
when	O
the	O
q∗	O
(	O
a	O
)	O
are	O
chosen	O
to	O
be	O
near	O
+4	O
rather	O
than	O
near	O
zero	O
.	O
the	O
bandit	O
gradient	O
algorithm	O
as	O
stochastic	O
gradient	B
ascent	O
one	O
can	O
gain	O
a	O
deeper	O
insight	O
into	O
the	O
gradient	B
bandit	O
algorithm	O
by	O
understanding	O
it	O
as	O
a	O
stochastic	O
approximation	O
to	O
gradient	B
ascent	O
.	O
in	O
exact	O
gradient	B
ascent	O
,	O
each	O
action	B
preference	O
ht	O
(	O
a	O
)	O
would	O
be	O
incremented	O
proportional	O
to	O
the	O
increment	O
’	O
s	O
eﬀect	O
on	O
performance	O
:	O
ht+1	O
(	O
a	O
)	O
.	O
=	O
ht	O
(	O
a	O
)	O
+	O
α	O
∂	O
e	O
[	O
rt	O
]	O
∂ht	O
(	O
a	O
)	O
,	O
(	O
2.13	O
)	O
where	O
the	O
measure	O
of	O
performance	O
here	O
is	O
the	O
expected	O
reward	O
:	O
e	O
[	O
rt	O
]	O
=	O
(	O
cid:88	O
)	O
x	O
πt	O
(	O
x	O
)	O
q∗	O
(	O
x	O
)	O
,	O
and	O
the	O
measure	O
of	O
the	O
increment	O
’	O
s	O
eﬀect	O
is	O
the	O
partial	O
derivative	O
of	O
this	O
perfor-	O
mance	O
measure	O
with	O
respect	O
to	O
the	O
action	B
preference	O
.	O
of	O
course	O
,	O
it	O
is	O
not	O
possible	O
to	O
implement	O
gradient	B
ascent	O
exactly	O
in	O
our	O
case	O
because	O
by	O
assumption	O
we	O
do	O
not	O
know	O
the	O
q∗	O
(	O
x	O
)	O
,	O
but	O
in	O
fact	O
the	O
updates	O
of	O
our	O
algorithm	O
(	O
2.12	O
)	O
are	O
equal	O
to	O
(	O
2.13	O
)	O
in	O
expected	O
value	B
,	O
making	O
the	O
algorithm	O
an	O
instance	O
of	O
stochastic	O
gradient	B
ascent	O
.	O
the	O
calculations	O
showing	O
this	O
require	O
only	O
beginning	O
calculus	O
,	O
but	O
take	O
%	O
optimalactionstepsα	O
=	O
0.1100	O
%	O
80	O
%	O
60	O
%	O
40	O
%	O
20	O
%	O
0	O
%	O
α	O
=	O
0.4α	O
=	O
0.1α	O
=	O
0.4without	O
baselinewith	O
baseline12505007501000	O
2.8.	O
gradient	B
bandit	O
algorithms	O
39	O
several	O
steps	O
.	O
first	O
we	O
take	O
a	O
closer	O
look	O
at	O
the	O
exact	O
performance	O
gradient	B
:	O
∂	O
e	O
[	O
rt	O
]	O
∂ht	O
(	O
a	O
)	O
∂	O
=	O
πt	O
(	O
x	O
)	O
q∗	O
(	O
x	O
)	O
(	O
cid:35	O
)	O
∂ht	O
(	O
a	O
)	O
(	O
cid:34	O
)	O
(	O
cid:88	O
)	O
x	O
=	O
(	O
cid:88	O
)	O
x	O
=	O
(	O
cid:88	O
)	O
x	O
(	O
cid:0	O
)	O
q∗	O
(	O
x	O
)	O
−	O
bt	O
(	O
cid:1	O
)	O
∂	O
πt	O
(	O
x	O
)	O
∂	O
πt	O
(	O
x	O
)	O
∂ht	O
(	O
a	O
)	O
q∗	O
(	O
x	O
)	O
∂ht	O
(	O
a	O
)	O
,	O
where	O
bt	O
,	O
called	O
the	O
baseline	B
,	O
can	O
be	O
any	O
scalar	O
that	O
does	O
not	O
depend	O
on	O
x.	O
we	O
can	O
include	O
a	O
baseline	B
here	O
without	O
changing	O
the	O
equality	O
because	O
the	O
gradient	B
sums	O
∂	O
πt	O
(	O
x	O
)	O
∂ht	O
(	O
a	O
)	O
=	O
0—as	O
ht	O
(	O
a	O
)	O
is	O
changed	O
,	O
some	O
actions	O
’	O
probabilities	O
go	O
up	O
and	O
some	O
go	O
down	O
,	O
but	O
the	O
sum	O
of	O
the	O
changes	O
must	O
be	O
zero	O
because	O
the	O
sum	O
of	O
the	O
probabilities	O
is	O
always	O
one	O
.	O
to	O
zero	O
over	O
all	O
the	O
actions	O
,	O
(	O
cid:80	O
)	O
x	O
next	O
we	O
multiply	O
each	O
term	O
of	O
the	O
sum	O
by	O
πt	O
(	O
x	O
)	O
/πt	O
(	O
x	O
)	O
:	O
∂	O
e	O
[	O
rt	O
]	O
∂ht	O
(	O
a	O
)	O
=	O
(	O
cid:88	O
)	O
x	O
πt	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
q∗	O
(	O
x	O
)	O
−	O
bt	O
(	O
cid:1	O
)	O
∂	O
πt	O
(	O
x	O
)	O
∂ht	O
(	O
a	O
)	O
/πt	O
(	O
x	O
)	O
.	O
the	O
equation	O
is	O
now	O
in	O
the	O
form	O
of	O
an	O
expectation	O
,	O
summing	O
over	O
all	O
possible	O
values	O
x	O
of	O
the	O
random	O
variable	O
at	O
,	O
then	O
multiplying	O
by	O
the	O
probability	O
of	O
taking	O
those	O
values	O
.	O
thus	O
:	O
=	O
e	O
(	O
cid:20	O
)	O
(	O
cid:0	O
)	O
q∗	O
(	O
at	O
)	O
−	O
bt	O
(	O
cid:1	O
)	O
∂	O
πt	O
(	O
at	O
)	O
=	O
e	O
(	O
cid:20	O
)	O
(	O
cid:0	O
)	O
rt	O
−	O
¯rt	O
(	O
cid:1	O
)	O
∂	O
πt	O
(	O
at	O
)	O
/πt	O
(	O
at	O
)	O
(	O
cid:21	O
)	O
/πt	O
(	O
at	O
)	O
(	O
cid:21	O
)	O
,	O
∂ht	O
(	O
a	O
)	O
∂ht	O
(	O
a	O
)	O
where	O
here	O
we	O
have	O
chosen	O
the	O
baseline	B
bt	O
=	O
¯rt	O
and	O
substituted	O
rt	O
for	O
q∗	O
(	O
at	O
)	O
,	O
which	O
is	O
permitted	O
because	O
e	O
[	O
rt|at	O
]	O
=	O
q∗	O
(	O
at	O
)	O
.	O
shortly	O
we	O
will	O
establish	O
that	O
∂ht	O
(	O
a	O
)	O
=	O
πt	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
1a=x	O
−	O
πt	O
(	O
a	O
)	O
(	O
cid:1	O
)	O
,	O
where	O
1a=x	O
is	O
deﬁned	O
to	O
be	O
1	O
if	O
a	O
=	O
x	O
,	O
else	O
0.	O
assuming	O
that	O
for	O
now	O
,	O
we	O
have	O
∂	O
πt	O
(	O
x	O
)	O
=	O
e	O
(	O
cid:2	O
)	O
(	O
cid:0	O
)	O
rt	O
−	O
¯rt	O
(	O
cid:1	O
)	O
πt	O
(	O
at	O
)	O
(	O
cid:0	O
)	O
1a=at	O
−	O
πt	O
(	O
a	O
)	O
(	O
cid:1	O
)	O
/πt	O
(	O
at	O
)	O
(	O
cid:3	O
)	O
=	O
e	O
(	O
cid:2	O
)	O
(	O
cid:0	O
)	O
rt	O
−	O
¯rt	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
1a=at	O
−	O
πt	O
(	O
a	O
)	O
(	O
cid:1	O
)	O
(	O
cid:3	O
)	O
.	O
recall	O
that	O
our	O
plan	O
has	O
been	O
to	O
write	O
the	O
performance	O
gradient	B
as	O
an	O
expectation	O
of	O
something	O
that	O
we	O
can	O
sample	O
on	O
each	O
step	O
,	O
as	O
we	O
have	O
just	O
done	O
,	O
and	O
then	O
update	O
on	O
each	O
step	O
proportional	O
to	O
the	O
sample	O
.	O
substituting	O
a	O
sample	O
of	O
the	O
expectation	O
above	O
for	O
the	O
performance	O
gradient	B
in	O
(	O
2.13	O
)	O
yields	O
:	O
ht+1	O
(	O
a	O
)	O
=	O
ht	O
(	O
a	O
)	O
+	O
α	O
(	O
cid:0	O
)	O
rt	O
−	O
¯rt	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
1a=at	O
−	O
πt	O
(	O
a	O
)	O
(	O
cid:1	O
)	O
,	O
for	O
all	O
a	O
,	O
which	O
you	O
may	O
recognize	O
as	O
being	O
equivalent	O
to	O
our	O
original	O
algorithm	O
(	O
2.12	O
)	O
.	O
40	O
chapter	O
2	O
:	O
multi-armed	B
bandits	I
thus	O
it	O
remains	O
only	O
to	O
show	O
that	O
∂	O
πt	O
(	O
x	O
)	O
recall	O
the	O
standard	O
quotient	O
rule	O
for	O
derivatives	O
:	O
∂ht	O
(	O
a	O
)	O
=	O
πt	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
1a=x−	O
πt	O
(	O
a	O
)	O
(	O
cid:1	O
)	O
,	O
as	O
we	O
assumed	O
.	O
∂	O
∂x	O
(	O
cid:20	O
)	O
f	O
(	O
x	O
)	O
g	O
(	O
x	O
)	O
(	O
cid:21	O
)	O
=	O
∂f	O
(	O
x	O
)	O
∂x	O
g	O
(	O
x	O
)	O
−	O
f	O
(	O
x	O
)	O
∂g	O
(	O
x	O
)	O
∂x	O
g	O
(	O
x	O
)	O
2	O
.	O
using	O
this	O
,	O
we	O
can	O
write	O
∂	O
πt	O
(	O
x	O
)	O
∂ht	O
(	O
a	O
)	O
πt	O
(	O
x	O
)	O
∂ht	O
(	O
a	O
)	O
∂	O
∂	O
=	O
=	O
=	O
∂eht	O
(	O
x	O
)	O
eht	O
(	O
x	O
)	O
∂ht	O
(	O
a	O
)	O
(	O
cid:34	O
)	O
y=1	O
eht	O
(	O
y	O
)	O
(	O
cid:35	O
)	O
(	O
cid:80	O
)	O
k	O
y=1	O
eht	O
(	O
y	O
)	O
−	O
eht	O
(	O
x	O
)	O
∂	O
(	O
cid:80	O
)	O
k	O
∂ht	O
(	O
a	O
)	O
(	O
cid:80	O
)	O
k	O
(	O
cid:16	O
)	O
(	O
cid:80	O
)	O
k	O
y=1	O
eht	O
(	O
y	O
)	O
(	O
cid:17	O
)	O
2	O
1a=xeht	O
(	O
x	O
)	O
(	O
cid:80	O
)	O
k	O
(	O
cid:16	O
)	O
(	O
cid:80	O
)	O
k	O
y=1	O
eht	O
(	O
y	O
)	O
(	O
cid:17	O
)	O
2	O
1a=xeht	O
(	O
x	O
)	O
y=1	O
eht	O
(	O
y	O
)	O
−	O
(	O
cid:80	O
)	O
k	O
(	O
cid:16	O
)	O
(	O
cid:80	O
)	O
k	O
y=1	O
eht	O
(	O
y	O
)	O
(	O
cid:17	O
)	O
2	O
=	O
πt	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
1a=x	O
−	O
πt	O
(	O
a	O
)	O
(	O
cid:1	O
)	O
.	O
=	O
1a=xπt	O
(	O
x	O
)	O
−	O
πt	O
(	O
x	O
)	O
πt	O
(	O
a	O
)	O
eht	O
(	O
x	O
)	O
eht	O
(	O
a	O
)	O
=	O
=	O
y=1	O
eht	O
(	O
y	O
)	O
−	O
eht	O
(	O
x	O
)	O
eht	O
(	O
a	O
)	O
y=1	O
eht	O
(	O
y	O
)	O
∂ht	O
(	O
a	O
)	O
(	O
by	O
the	O
quotient	O
rule	O
)	O
(	O
because	O
∂ex	O
∂x	O
=	O
ex	O
)	O
q.e.d	O
.	O
we	O
have	O
just	O
shown	O
that	O
the	O
expected	B
update	I
of	O
the	O
gradient	B
bandit	O
algorithm	O
is	O
equal	O
to	O
the	O
gradient	B
of	O
expected	O
reward	O
,	O
and	O
thus	O
that	O
the	O
algorithm	O
is	O
an	O
instance	O
of	O
stochastic	O
gradient	B
ascent	O
.	O
this	O
assures	O
us	O
that	O
the	O
algorithm	O
has	O
robust	O
convergence	O
properties	O
.	O
note	O
that	O
we	O
did	O
not	O
require	O
any	O
properties	O
of	O
the	O
reward	O
baseline	O
other	O
than	O
that	O
it	O
does	O
not	O
depend	O
on	O
the	O
selected	O
action	B
.	O
for	O
example	O
,	O
we	O
could	O
have	O
set	O
it	O
to	O
zero	O
,	O
or	O
to	O
1000	O
,	O
and	O
the	O
algorithm	O
would	O
still	O
be	O
an	O
instance	O
of	O
stochastic	O
gradient	B
ascent	O
.	O
the	O
choice	O
of	O
the	O
baseline	B
does	O
not	O
aﬀect	O
the	O
expected	B
update	I
of	O
the	O
algorithm	O
,	O
but	O
it	O
does	O
aﬀect	O
the	O
variance	O
of	O
the	O
update	O
and	O
thus	O
the	O
rate	O
of	O
convergence	O
(	O
as	O
shown	O
,	O
e.g.	O
,	O
in	O
figure	O
2.5	O
)	O
.	O
choosing	O
it	O
as	O
the	O
average	O
of	O
the	O
rewards	O
may	O
not	O
be	O
the	O
very	O
best	O
,	O
but	O
it	O
is	O
simple	O
and	O
works	O
well	O
in	O
practice	O
.	O
2.10.	O
summary	O
41	O
2.9	O
associative	B
search	I
(	O
contextual	B
bandits	I
)	O
so	O
far	O
in	O
this	O
chapter	O
we	O
have	O
considered	O
only	O
nonassociative	O
tasks	O
,	O
that	O
is	O
,	O
tasks	O
in	O
which	O
there	O
is	O
no	O
need	O
to	O
associate	O
diﬀerent	O
actions	O
with	O
diﬀerent	O
situations	O
.	O
in	O
these	O
tasks	O
the	O
learner	O
either	O
tries	O
to	O
ﬁnd	O
a	O
single	O
best	O
action	B
when	O
the	O
task	O
is	O
stationary	O
,	O
or	O
tries	O
to	O
track	O
the	O
best	O
action	B
as	O
it	O
changes	O
over	O
time	O
when	O
the	O
task	O
is	O
nonstationary	O
.	O
however	O
,	O
in	O
a	O
general	O
reinforcement	O
learning	O
task	O
there	O
is	O
more	O
than	O
one	O
situation	O
,	O
and	O
the	O
goal	O
is	O
to	O
learn	O
a	O
policy	B
:	O
a	O
mapping	O
from	O
situations	O
to	O
the	O
actions	O
that	O
are	O
best	O
in	O
those	O
situations	O
.	O
to	O
set	O
the	O
stage	O
for	O
the	O
full	O
problem	O
,	O
we	O
brieﬂy	O
discuss	O
the	O
simplest	O
way	O
in	O
which	O
nonassociative	O
tasks	O
extend	O
to	O
the	O
associative	O
setting	O
.	O
as	O
an	O
example	O
,	O
suppose	O
there	O
are	O
several	O
diﬀerent	O
k-armed	O
bandit	O
tasks	O
,	O
and	O
that	O
on	O
each	O
step	O
you	O
confront	O
one	O
of	O
these	O
chosen	O
at	O
random	O
.	O
thus	O
,	O
the	O
bandit	O
task	O
changes	O
randomly	O
from	O
step	O
to	O
step	O
.	O
this	O
would	O
appear	O
to	O
you	O
as	O
a	O
single	O
,	O
nonstationary	O
k-armed	O
bandit	O
task	O
whose	O
true	O
action	O
values	O
change	O
randomly	O
from	O
step	O
to	O
step	O
.	O
you	O
could	O
try	O
using	O
one	O
of	O
the	O
methods	O
described	O
in	O
this	O
chapter	O
that	O
can	O
handle	O
nonstationarity	B
,	O
but	O
unless	O
the	O
true	O
action	O
values	O
change	O
slowly	O
,	O
these	O
methods	O
will	O
not	O
work	O
very	O
well	O
.	O
now	O
suppose	O
,	O
however	O
,	O
that	O
when	O
a	O
bandit	O
task	O
is	O
selected	O
for	O
you	O
,	O
you	O
are	O
given	O
some	O
distinctive	O
clue	O
about	O
its	O
identity	O
(	O
but	O
not	O
its	O
action	B
values	O
)	O
.	O
maybe	O
you	O
are	O
facing	O
an	O
actual	O
slot	O
machine	O
that	O
changes	O
the	O
color	O
of	O
its	O
display	O
as	O
it	O
changes	O
its	O
action	B
values	O
.	O
now	O
you	O
can	O
learn	O
a	O
policy	B
associating	O
each	O
task	O
,	O
signaled	O
by	O
the	O
color	O
you	O
see	O
,	O
with	O
the	O
best	O
action	B
to	O
take	O
when	O
facing	O
that	O
task—for	O
instance	O
,	O
if	O
red	O
,	O
select	O
arm	O
1	O
;	O
if	O
green	O
,	O
select	O
arm	O
2.	O
with	O
the	O
right	O
policy	B
you	O
can	O
usually	O
do	O
much	O
better	O
than	O
you	O
could	O
in	O
the	O
absence	O
of	O
any	O
information	O
distinguishing	O
one	O
bandit	O
task	O
from	O
another	O
.	O
this	O
is	O
an	O
example	O
of	O
an	O
associative	B
search	I
task	O
,	O
so	O
called	O
because	O
it	O
involves	O
both	O
trial-and-error	B
learning	O
to	O
search	O
for	O
the	O
best	O
actions	O
,	O
and	O
association	O
of	O
these	O
actions	O
with	O
the	O
situations	O
in	O
which	O
they	O
are	O
best	O
.	O
associative	B
search	I
tasks	O
are	O
often	O
now	O
called	O
contextual	B
bandits	I
in	O
the	O
literature	O
.	O
associative	B
search	I
tasks	O
are	O
intermediate	O
between	O
the	O
k-armed	O
bandit	O
problem	O
and	O
the	O
full	O
reinforcement	B
learning	I
problem	O
.	O
they	O
are	O
like	O
the	O
full	O
reinforcement	B
learning	I
problem	O
in	O
that	O
they	O
involve	O
learning	O
a	O
policy	B
,	O
but	O
like	O
our	O
version	O
of	O
the	O
k-armed	O
bandit	O
problem	O
in	O
that	O
each	O
action	B
aﬀects	O
only	O
the	O
immediate	O
reward	O
.	O
if	O
actions	O
are	O
allowed	O
to	O
aﬀect	O
the	O
next	O
situation	O
as	O
well	O
as	O
the	O
reward	O
,	O
then	O
we	O
have	O
the	O
full	O
reinforcement	B
learning	I
problem	O
.	O
we	O
present	O
this	O
problem	O
in	O
the	O
next	O
chapter	O
and	O
consider	O
its	O
ramiﬁcations	O
throughout	O
the	O
rest	O
of	O
the	O
book	O
.	O
exercise	O
2.10	O
suppose	O
you	O
face	O
a	O
2-armed	O
bandit	O
task	O
whose	O
true	O
action	O
values	O
change	O
randomly	O
from	O
time	O
step	O
to	O
time	O
step	O
.	O
speciﬁcally	O
,	O
suppose	O
that	O
,	O
for	O
any	O
time	O
step	O
,	O
the	O
true	O
values	O
of	O
actions	O
1	O
and	O
2	O
are	O
respectively	O
0.1	O
and	O
0.2	O
with	O
probability	O
0.5	O
(	O
case	O
a	O
)	O
,	O
and	O
0.9	O
and	O
0.8	O
with	O
probability	O
0.5	O
(	O
case	O
b	O
)	O
.	O
if	O
you	O
are	O
not	O
able	O
to	O
tell	O
which	O
case	O
you	O
face	O
at	O
any	O
step	O
,	O
what	O
is	O
the	O
best	O
expectation	O
of	O
success	O
you	O
can	O
achieve	O
and	O
how	O
should	O
you	O
behave	O
to	O
achieve	O
it	O
?	O
now	O
suppose	O
that	O
on	O
each	O
step	O
you	O
are	O
told	O
whether	O
you	O
are	O
facing	O
case	O
a	O
or	O
case	O
b	O
(	O
although	O
you	O
still	O
don	O
’	O
t	O
know	O
the	O
true	O
action	O
values	O
)	O
.	O
this	O
is	O
an	O
associative	B
search	I
task	O
.	O
what	O
is	O
the	O
best	O
expectation	O
of	O
success	O
you	O
can	O
achieve	O
in	O
(	O
cid:3	O
)	O
this	O
task	O
,	O
and	O
how	O
should	O
you	O
behave	O
to	O
achieve	O
it	O
?	O
42	O
chapter	O
2	O
:	O
multi-armed	B
bandits	I
2.10	O
summary	O
we	O
have	O
presented	O
in	O
this	O
chapter	O
several	O
simple	O
ways	O
of	O
balancing	O
exploration	O
and	O
ex-	O
ploitation	O
.	O
the	O
ε-greedy	O
methods	O
choose	O
randomly	O
a	O
small	O
fraction	O
of	O
the	O
time	O
,	O
whereas	O
ucb	O
methods	O
choose	O
deterministically	O
but	O
achieve	O
exploration	O
by	O
subtly	O
favoring	O
at	O
each	O
step	O
the	O
actions	O
that	O
have	O
so	O
far	O
received	O
fewer	O
samples	O
.	O
gradient	B
bandit	O
algorithms	O
es-	O
timate	O
not	O
action	B
values	O
,	O
but	O
action	B
preferences	I
,	O
and	O
favor	O
the	O
more	O
preferred	O
actions	O
in	O
a	O
graded	O
,	O
probabilistic	O
manner	O
using	O
a	O
soft-max	B
distribution	O
.	O
the	O
simple	O
expedient	O
of	O
initializing	O
estimates	O
optimistically	O
causes	O
even	O
greedy	O
methods	O
to	O
explore	O
signiﬁcantly	O
.	O
it	O
is	O
natural	O
to	O
ask	O
which	O
of	O
these	O
methods	O
is	O
best	O
.	O
although	O
this	O
is	O
a	O
diﬃcult	O
question	O
to	O
answer	O
in	O
general	O
,	O
we	O
can	O
certainly	O
run	O
them	O
all	O
on	O
the	O
10-armed	O
testbed	O
that	O
we	O
have	O
used	O
throughout	O
this	O
chapter	O
and	O
compare	O
their	O
performances	O
.	O
a	O
complication	O
is	O
that	O
they	O
all	O
have	O
a	O
parameter	O
;	O
to	O
get	O
a	O
meaningful	O
comparison	O
we	O
have	O
to	O
consider	O
their	O
performance	O
as	O
a	O
function	O
of	O
their	O
parameter	O
.	O
our	O
graphs	O
so	O
far	O
have	O
shown	O
the	O
course	O
of	O
learning	O
over	O
time	O
for	O
each	O
algorithm	O
and	O
parameter	O
setting	O
,	O
to	O
produce	O
a	O
learning	O
curve	O
for	O
that	O
algorithm	O
and	O
parameter	O
setting	O
.	O
if	O
we	O
plotted	O
learning	O
curves	O
for	O
all	O
algorithms	O
and	O
all	O
parameter	O
settings	O
,	O
then	O
the	O
graph	O
would	O
be	O
too	O
complex	O
and	O
crowded	O
to	O
make	O
clear	O
comparisons	O
.	O
instead	O
we	O
summarize	O
a	O
complete	O
learning	O
curve	O
by	O
its	O
average	O
value	O
over	O
the	O
1000	O
steps	O
;	O
this	O
value	B
is	O
proportional	O
to	O
the	O
area	O
under	O
the	O
learning	O
curve	O
.	O
figure	O
2.6	O
shows	O
this	O
measure	O
for	O
the	O
various	O
bandit	O
algorithms	O
from	O
this	O
chapter	O
,	O
each	O
as	O
a	O
function	O
of	O
its	O
own	O
parameter	O
shown	O
on	O
a	O
single	O
scale	O
on	O
the	O
x-axis	O
.	O
this	O
kind	O
of	O
graph	O
is	O
called	O
a	O
parameter	O
study	O
.	O
note	O
that	O
the	O
parameter	O
values	O
are	O
varied	O
by	O
factors	O
of	O
two	O
and	O
presented	O
on	O
a	O
log	O
scale	O
.	O
note	O
also	O
the	O
characteristic	O
inverted-u	O
shapes	O
of	O
each	O
algorithm	O
’	O
s	O
performance	O
;	O
all	O
the	O
algorithms	O
perform	O
best	O
at	O
an	O
intermediate	O
value	B
of	O
their	O
parameter	O
,	O
neither	O
too	O
large	O
nor	O
too	O
small	O
.	O
in	O
assessing	O
figure	O
2.6	O
:	O
a	O
parameter	O
study	O
of	O
the	O
various	O
bandit	O
algorithms	O
presented	O
in	O
this	O
chapter	O
.	O
each	O
point	O
is	O
the	O
average	O
reward	O
obtained	O
over	O
1000	O
steps	O
with	O
a	O
particular	O
algorithm	O
at	O
a	O
particular	O
setting	O
of	O
its	O
parameter	O
.	O
averagerewardover	O
ﬁrst	O
1000	O
steps1.51.41.31.21.11-greedyucbgradientbanditgreedy	O
withoptimisticinitializationα	O
=	O
0.11241/21/41/81/161/321/641/128	O
''	O
↵cq0	O
2.10.	O
summary	O
43	O
a	O
method	O
,	O
we	O
should	O
attend	O
not	O
just	O
to	O
how	O
well	O
it	O
does	O
at	O
its	O
best	O
parameter	O
setting	O
,	O
but	O
also	O
to	O
how	O
sensitive	O
it	O
is	O
to	O
its	O
parameter	O
value	O
.	O
all	O
of	O
these	O
algorithms	O
are	O
fairly	O
insensitive	O
,	O
performing	O
well	O
over	O
a	O
range	O
of	O
parameter	O
values	O
varying	O
by	O
about	O
an	O
order	O
of	O
magnitude	O
.	O
overall	O
,	O
on	O
this	O
problem	O
,	O
ucb	O
seems	O
to	O
perform	O
best	O
.	O
despite	O
their	O
simplicity	O
,	O
in	O
our	O
opinion	O
the	O
methods	O
presented	O
in	O
this	O
chapter	O
can	O
fairly	O
be	O
considered	O
the	O
state	B
of	O
the	O
art	O
.	O
there	O
are	O
more	O
sophisticated	O
methods	O
,	O
but	O
their	O
complexity	O
and	O
assumptions	O
make	O
them	O
impractical	O
for	O
the	O
full	O
reinforcement	B
learning	I
problem	O
that	O
is	O
our	O
real	O
focus	O
.	O
starting	O
in	O
chapter	O
5	O
we	O
present	O
learning	O
methods	O
for	O
solving	O
the	O
full	O
reinforcement	B
learning	I
problem	O
that	O
use	O
in	O
part	O
the	O
simple	O
methods	O
explored	O
in	O
this	O
chapter	O
.	O
although	O
the	O
simple	O
methods	O
explored	O
in	O
this	O
chapter	O
may	O
be	O
the	O
best	O
we	O
can	O
do	O
at	O
present	O
,	O
they	O
are	O
far	O
from	O
a	O
fully	O
satisfactory	O
solution	O
to	O
the	O
problem	O
of	O
balancing	O
exploration	O
and	O
exploitation	O
.	O
one	O
well-studied	O
approach	O
to	O
balancing	O
exploration	O
and	O
exploitation	O
in	O
k-armed	O
ban-	O
dit	O
problems	O
is	O
to	O
compute	O
special	O
functions	O
called	O
gittins	O
indices	O
.	O
these	O
provide	O
an	O
optimal	O
solution	O
to	O
a	O
certain	O
kind	O
of	O
bandit	O
problem	O
more	O
general	O
than	O
that	O
consid-	O
ered	O
here	O
,	O
but	O
this	O
approach	O
assumes	O
that	O
the	O
prior	O
distribution	O
of	O
possible	O
problems	O
is	O
known	O
.	O
unfortunately	O
,	O
neither	O
the	O
theory	O
nor	O
the	O
computational	O
tractability	O
of	O
this	O
method	O
appear	O
to	O
generalize	O
to	O
the	O
full	O
reinforcement	B
learning	I
problem	O
that	O
we	O
consider	O
in	O
the	O
rest	O
of	O
the	O
book	O
.	O
bayesian	O
methods	O
assume	O
a	O
known	O
initial	O
distribution	O
over	O
the	O
action	B
values	O
and	O
then	O
update	O
the	O
distribution	O
exactly	O
after	O
each	O
step	O
(	O
assuming	O
that	O
the	O
true	O
action	O
values	O
are	O
stationary	O
)	O
.	O
in	O
general	O
,	O
the	O
update	O
computations	O
can	O
be	O
very	O
complex	O
,	O
but	O
for	O
certain	O
special	O
distributions	O
(	O
called	O
conjugate	O
priors	O
)	O
they	O
are	O
easy	O
.	O
one	O
possibility	O
is	O
to	O
then	O
select	O
actions	O
at	O
each	O
step	O
according	O
to	O
their	O
posterior	O
probability	O
of	O
being	O
the	O
best	O
action	B
.	O
this	O
method	O
,	O
sometimes	O
called	O
posterior	O
sampling	O
or	O
thompson	O
sampling	O
,	O
often	O
performs	O
similarly	O
to	O
the	O
best	O
of	O
the	O
distribution-free	O
methods	O
we	O
have	O
presented	O
in	O
this	O
chapter	O
.	O
in	O
the	O
bayesian	O
setting	O
it	O
is	O
even	O
conceivable	O
to	O
compute	O
the	O
optimal	O
balance	O
between	O
exploration	O
and	O
exploitation	O
.	O
one	O
can	O
compute	O
for	O
any	O
possible	O
action	B
the	O
probability	O
of	O
each	O
possible	O
immediate	O
reward	O
and	O
the	O
resultant	O
posterior	O
distributions	O
over	O
action	B
values	O
.	O
this	O
evolving	O
distribution	O
becomes	O
the	O
information	O
state	B
of	O
the	O
problem	O
.	O
given	O
a	O
horizon	O
,	O
say	O
of	O
1000	O
steps	O
,	O
one	O
can	O
consider	O
all	O
possible	O
actions	O
,	O
all	O
possible	O
resulting	O
rewards	O
,	O
all	O
possible	O
next	O
actions	O
,	O
all	O
next	O
rewards	O
,	O
and	O
so	O
on	O
for	O
all	O
1000	O
steps	O
.	O
given	O
the	O
assumptions	O
,	O
the	O
rewards	O
and	O
probabilities	O
of	O
each	O
possible	O
chain	O
of	O
events	O
can	O
be	O
determined	O
,	O
and	O
one	O
need	O
only	O
pick	O
the	O
best	O
.	O
but	O
the	O
tree	O
of	O
possibilities	O
grows	O
extremely	O
rapidly	O
;	O
even	O
if	O
there	O
were	O
only	O
two	O
actions	O
and	O
two	O
rewards	O
,	O
the	O
tree	O
would	O
have	O
22000	O
leaves	O
.	O
it	O
is	O
generally	O
not	O
feasible	O
to	O
perform	O
this	O
immense	O
computation	O
exactly	O
,	O
but	O
perhaps	O
it	O
could	O
be	O
approximated	O
eﬃciently	O
.	O
this	O
approach	O
would	O
eﬀectively	O
turn	O
the	O
bandit	O
problem	O
into	O
an	O
instance	O
of	O
the	O
full	O
reinforcement	B
learning	I
problem	O
.	O
in	O
the	O
end	O
,	O
we	O
may	O
be	O
able	O
to	O
use	O
approximate	B
reinforcement	O
learning	O
methods	O
such	O
as	O
those	O
presented	O
in	O
part	O
ii	O
of	O
this	O
book	O
to	O
approach	O
this	O
optimal	O
solution	O
.	O
but	O
that	O
is	O
a	O
topic	O
for	O
research	O
and	O
beyond	O
the	O
scope	O
of	O
this	O
introductory	O
book	O
.	O
exercise	O
2.11	O
(	O
programming	O
)	O
make	O
a	O
ﬁgure	O
analogous	O
to	O
figure	O
2.6	O
for	O
the	O
nonstationary	O
44	O
chapter	O
2	O
:	O
multi-armed	B
bandits	I
case	O
outlined	O
in	O
exercise	O
2.5.	O
include	O
the	O
constant-step-size	O
ε-greedy	O
algorithm	O
with	O
α	O
=	O
0.1.	O
use	O
runs	O
of	O
200,000	O
steps	O
and	O
,	O
as	O
a	O
performance	O
measure	O
for	O
each	O
algorithm	O
and	O
(	O
cid:3	O
)	O
parameter	O
setting	O
,	O
use	O
the	O
average	O
reward	O
over	O
the	O
last	O
100,000	O
steps	O
.	O
bibliographical	O
and	O
historical	O
remarks	O
2.1	O
bandit	B
problems	I
have	O
been	O
studied	O
in	O
statistics	O
,	O
engineering	O
,	O
and	O
psychology	O
.	O
in	O
statistics	O
,	O
bandit	B
problems	I
fall	O
under	O
the	O
heading	O
“	O
sequential	O
design	B
of	I
experi-	O
ments	O
,	O
”	O
introduced	O
by	O
thompson	O
(	O
1933	O
,	O
1934	O
)	O
and	O
robbins	O
(	O
1952	O
)	O
,	O
and	O
studied	O
by	O
bellman	O
(	O
1956	O
)	O
.	O
berry	O
and	O
fristedt	O
(	O
1985	O
)	O
provide	O
an	O
extensive	O
treatment	O
of	O
bandit	O
problems	O
from	O
the	O
perspective	O
of	O
statistics	O
.	O
narendra	O
and	O
thathachar	O
(	O
1989	O
)	O
treat	O
bandit	B
problems	I
from	O
the	O
engineering	O
perspective	O
,	O
providing	O
a	O
good	O
discussion	O
of	O
the	O
various	O
theoretical	O
traditions	O
that	O
have	O
focused	O
on	O
them	O
.	O
in	B
psychology	I
,	O
bandit	B
problems	I
have	O
played	O
roles	O
in	O
statistical	O
learning	O
theory	O
(	O
e.g.	O
,	O
bush	O
and	O
mosteller	O
,	O
1955	O
;	O
estes	O
,	O
1950	O
)	O
.	O
the	O
term	O
greedy	O
is	O
often	O
used	O
in	O
the	O
heuristic	O
search	O
literature	O
(	O
e.g.	O
,	O
pearl	O
,	O
1984	O
)	O
.	O
the	O
conﬂict	O
between	O
exploration	O
and	O
exploitation	O
is	O
known	O
in	O
control	O
engineering	O
as	O
the	O
conﬂict	O
between	O
identiﬁcation	O
(	O
or	O
estimation	O
)	O
and	B
control	I
(	O
e.g.	O
,	O
witten	O
,	O
1976	O
)	O
.	O
feldbaum	O
(	O
1965	O
)	O
called	O
it	O
the	O
dual	O
control	B
problem	O
,	O
referring	O
to	O
the	O
need	O
to	O
solve	O
the	O
two	O
problems	O
of	O
identiﬁcation	O
and	B
control	I
simultaneously	O
when	O
trying	O
to	O
control	B
a	O
system	O
under	O
uncertainty	O
.	O
in	O
discussing	O
aspects	O
of	O
genetic	O
algorithms	O
,	O
holland	O
(	O
1975	O
)	O
emphasized	O
the	O
importance	O
of	O
this	O
conﬂict	O
,	O
referring	O
to	O
it	O
as	O
the	O
conﬂict	O
between	O
the	O
need	O
to	O
exploit	O
and	O
the	O
need	O
for	O
new	O
information	O
.	O
2.2	O
action-value	B
methods	I
for	O
our	O
k-armed	O
bandit	O
problem	O
were	O
ﬁrst	O
proposed	O
by	O
thathachar	O
and	O
sastry	O
(	O
1985	O
)	O
.	O
these	O
are	O
often	O
called	O
estimator	O
algorithms	O
in	O
the	O
learning	O
automata	O
literature	O
.	O
the	O
term	O
action	B
value	O
is	O
due	O
to	O
watkins	O
(	O
1989	O
)	O
.	O
the	O
ﬁrst	O
to	O
use	O
ε-greedy	O
methods	O
may	O
also	O
have	O
been	O
watkins	O
(	O
1989	O
,	O
p.	O
187	O
)	O
,	O
but	O
the	O
idea	O
is	O
so	O
simple	O
that	O
some	O
earlier	O
use	O
seems	O
likely	O
.	O
2.4–5	O
this	O
material	O
falls	O
under	O
the	O
general	O
heading	O
of	O
stochastic	O
iterative	B
algorithms	O
,	O
which	O
is	O
well	O
covered	O
by	O
bertsekas	O
and	O
tsitsiklis	O
(	O
1996	O
)	O
.	O
2.6	O
2.7	O
2.8	O
optimistic	O
initialization	O
was	O
used	O
in	O
reinforcement	O
learning	O
by	O
sutton	O
(	O
1996	O
)	O
.	O
early	O
work	O
on	O
using	O
estimates	O
of	O
the	O
upper	O
conﬁdence	O
bound	O
to	O
select	O
actions	O
was	O
done	O
by	O
lai	O
and	O
robbins	O
(	O
1985	O
)	O
,	O
kaelbling	O
(	O
1993b	O
)	O
,	O
and	O
agrawal	O
(	O
1995	O
)	O
.	O
the	O
ucb	O
algorithm	O
we	O
present	O
here	O
is	O
called	O
ucb1	O
in	O
the	O
literature	O
and	O
was	O
ﬁrst	O
developed	O
by	O
auer	O
,	O
cesa-bianchi	O
and	O
fischer	O
(	O
2002	O
)	O
.	O
gradient	B
bandit	O
algorithms	O
are	O
a	O
special	O
case	O
of	O
the	O
gradient-based	O
reinforcement	B
learning	I
algorithms	O
introduced	O
by	O
williams	O
(	O
1992	O
)	O
,	O
and	O
that	O
later	O
developed	O
into	O
the	O
actor–critic	B
and	O
policy-gradient	O
algorithms	O
that	O
we	O
treat	O
later	O
in	O
this	O
book	O
.	O
our	O
development	O
here	O
was	O
inﬂuenced	O
by	O
that	O
by	O
balaraman	O
ravindran	O
(	O
personal	O
communication	O
)	O
.	O
further	O
discussion	O
of	O
the	O
choice	O
of	O
baseline	O
is	O
provided	O
there	O
2.10.	O
summary	O
45	O
2.9	O
2.10	O
and	O
by	O
greensmith	O
,	O
bartlett	O
,	O
and	O
baxter	O
(	O
2002	O
,	O
2004	O
)	O
and	O
dick	O
(	O
2015	O
)	O
.	O
early	O
systematic	O
studies	O
of	O
algorithms	O
like	O
this	O
were	O
done	O
by	O
sutton	O
(	O
1984	O
)	O
.	O
the	O
term	O
soft-max	B
for	O
the	O
action	B
selection	O
rule	O
(	O
2.11	O
)	O
is	O
due	O
to	O
bridle	O
(	O
1990	O
)	O
.	O
this	O
rule	O
appears	O
to	O
have	O
been	O
ﬁrst	O
proposed	O
by	O
luce	O
(	O
1959	O
)	O
.	O
the	O
term	O
associative	B
search	I
and	O
the	O
corresponding	O
problem	O
were	O
introduced	O
by	O
barto	O
,	O
sutton	O
,	O
and	O
brouwer	O
(	O
1981	O
)	O
.	O
the	O
term	O
associative	B
reinforcement	I
learning	I
has	O
also	O
been	O
used	O
for	O
associative	O
search	O
(	O
barto	O
and	O
anandan	O
,	O
1985	O
)	O
,	O
but	O
we	O
prefer	O
to	O
reserve	O
that	O
term	O
as	O
a	O
synonym	O
for	O
the	O
full	O
reinforcement	B
learning	I
problem	O
(	O
as	O
in	O
sutton	O
,	O
1984	O
)	O
.	O
(	O
and	O
,	O
as	O
we	O
noted	O
,	O
the	O
modern	O
literature	O
also	O
uses	O
the	O
term	O
“	O
contextual	B
bandits	I
”	O
for	O
this	O
problem	O
.	O
)	O
we	O
note	O
that	O
thorndike	O
’	O
s	O
law	O
of	O
eﬀect	O
(	O
quoted	O
in	O
chapter	O
1	O
)	O
describes	O
associative	B
search	I
by	O
referring	O
to	O
the	O
formation	O
of	O
associative	O
links	O
between	O
situations	O
(	O
states	O
)	O
and	O
actions	O
.	O
according	O
to	O
the	O
terminology	O
of	O
operant	O
,	O
or	O
instrumental	O
,	O
conditioning	B
(	O
e.g.	O
,	O
skinner	O
,	O
1938	O
)	O
,	O
a	O
discriminative	O
stimulus	O
is	O
a	O
stimulus	O
that	O
signals	O
the	O
presence	O
of	O
a	O
particular	O
reinforcement	O
contingency	O
.	O
in	O
our	O
terms	O
,	O
diﬀerent	O
discriminative	O
stimuli	O
correspond	O
to	O
diﬀerent	O
states	O
.	O
bellman	O
(	O
1956	O
)	O
was	O
the	O
ﬁrst	O
to	O
show	O
how	O
dynamic	B
programming	I
could	O
be	O
used	O
to	O
compute	O
the	O
optimal	O
balance	O
between	O
exploration	O
and	O
exploitation	O
within	O
a	O
bayesian	O
formulation	O
of	O
the	O
problem	O
.	O
the	O
gittins	O
index	O
approach	O
is	O
due	O
to	O
gittins	O
and	O
jones	O
(	O
1974	O
)	O
.	O
duﬀ	O
(	O
1995	O
)	O
showed	O
how	O
it	O
is	O
possible	O
to	O
learn	O
gittins	O
indices	O
for	B
bandit	I
problems	I
through	O
reinforcement	B
learning	I
.	O
the	O
survey	O
by	O
ku-	O
mar	O
(	O
1985	O
)	O
provides	O
a	O
good	O
discussion	O
of	O
bayesian	O
and	O
non-bayesian	O
approaches	O
to	O
these	O
problems	O
.	O
the	O
term	O
information	O
state	B
comes	O
from	O
the	O
literature	O
on	O
par-	O
tially	O
observable	O
mdps	O
;	O
see	O
,	O
e.g.	O
,	O
lovejoy	O
(	O
1991	O
)	O
.	O
other	O
theoretical	O
research	O
focuses	O
on	O
the	O
eﬃciency	O
of	O
exploration	O
,	O
usually	O
ex-	O
pressed	O
as	O
how	O
quickly	O
an	O
algorithm	O
can	O
approach	O
an	O
optimal	O
decision-making	O
policy	B
.	O
one	O
way	O
to	O
formalize	O
exploration	O
eﬃciency	O
is	O
by	O
adapting	O
to	O
reinforce-	O
ment	O
learning	O
the	O
notion	O
of	O
sample	O
complexity	O
for	O
a	O
supervised	O
learning	O
algo-	O
rithm	O
,	O
which	O
is	O
the	O
number	O
of	O
training	O
examples	O
the	O
algorithm	O
needs	O
to	O
attain	O
a	O
desired	O
degree	O
of	O
accuracy	O
in	O
learning	O
the	O
target	B
function	O
.	O
a	O
deﬁnition	O
of	O
the	O
sample	O
complexity	O
of	O
exploration	O
for	O
a	O
reinforcement	O
learning	O
algorithm	O
is	O
the	O
number	O
of	O
time	O
steps	O
in	O
which	O
the	O
algorithm	O
does	O
not	O
select	O
near-optimal	O
actions	O
(	O
kakade	O
,	O
2003	O
)	O
.	O
li	O
(	O
2012	O
)	O
discusses	O
this	O
and	O
several	O
other	O
approaches	O
in	O
a	O
survey	O
of	O
theoretical	O
approaches	O
to	O
exploration	O
eﬃciency	O
in	O
reinforcement	O
learning	O
.	O
chapter	O
3	O
finite	O
markov	O
decision	O
processes	O
in	O
this	O
chapter	O
we	O
introduce	O
the	O
formal	O
problem	O
of	O
ﬁnite	O
markov	O
decision	O
processes	O
,	O
or	O
ﬁnite	O
mdps	O
,	O
which	O
we	O
try	O
to	O
solve	O
in	O
the	O
rest	O
of	O
the	O
book	O
.	O
this	O
problem	O
involves	O
evalu-	O
ative	O
feedback	O
,	O
as	O
in	O
bandits	O
,	O
but	O
also	O
an	O
associative	O
aspect—choosing	O
diﬀerent	O
actions	O
in	O
diﬀerent	O
situations	O
.	O
mdps	O
are	O
a	O
classical	O
formalization	O
of	O
sequential	O
decision	O
making	O
,	O
where	O
actions	O
inﬂuence	O
not	O
just	O
immediate	O
rewards	O
,	O
but	O
also	O
subsequent	O
situations	O
,	O
or	O
states	O
,	O
and	O
through	O
those	O
future	O
rewards	O
.	O
thus	O
mdps	O
involve	O
delayed	B
reward	I
and	O
the	O
need	O
to	O
tradeoﬀ	O
immediate	O
and	O
delayed	O
reward	O
.	O
whereas	O
in	B
bandit	I
problems	I
we	O
esti-	O
mated	O
the	O
value	B
q∗	O
(	O
a	O
)	O
of	O
each	O
action	B
a	O
,	O
in	O
mdps	O
we	O
estimate	O
the	O
value	B
q∗	O
(	O
s	O
,	O
a	O
)	O
of	O
each	O
action	B
a	O
in	O
each	O
state	B
s	O
,	O
or	O
we	O
estimate	O
the	O
value	B
v∗	O
(	O
s	O
)	O
of	O
each	O
state	B
given	O
optimal	O
action	O
selections	O
.	O
these	O
state-dependent	O
quantities	O
are	O
essential	O
to	O
accurately	O
assigning	O
credit	O
for	O
long-term	O
consequences	O
to	O
individual	O
action	B
selections	O
.	O
mdps	O
are	O
a	O
mathematically	O
idealized	O
form	O
of	O
the	O
reinforcement	B
learning	I
problem	O
for	O
which	O
precise	O
theoretical	O
statements	O
can	O
be	O
made	O
.	O
we	O
introduce	O
key	O
elements	O
of	O
the	O
problem	O
’	O
s	O
mathematical	O
structure	O
,	O
such	O
as	O
returns	O
,	O
value	B
functions	O
,	O
and	O
bellman	O
equations	O
.	O
we	O
try	O
to	O
convey	O
the	O
wide	O
range	O
of	O
applications	O
that	O
can	O
be	O
formulated	O
as	O
ﬁnite	O
mdps	O
.	O
as	O
in	O
all	O
of	O
artiﬁcial	O
intelligence	O
,	O
there	O
is	O
a	O
tension	O
between	O
breadth	O
of	O
applicability	O
and	O
mathematical	O
tractability	O
.	O
in	O
this	O
chapter	O
we	O
introduce	O
this	O
tension	O
and	O
discuss	O
some	O
of	O
the	O
trade-oﬀs	O
and	O
challenges	O
that	O
it	O
implies	O
.	O
some	O
ways	O
in	O
which	O
reinforcement	B
learning	I
can	O
be	O
taken	O
beyond	O
mdps	O
are	O
treated	O
in	O
chapter	O
17	O
.	O
3.1	O
the	O
agent–environment	B
interface	I
mdps	O
are	O
meant	O
to	O
be	O
a	O
straightforward	O
framing	O
of	O
the	O
problem	O
of	O
learning	O
from	O
inter-	O
action	B
to	O
achieve	O
a	O
goal	B
.	O
the	O
learner	O
and	O
decision	O
maker	O
is	O
called	O
the	O
agent	O
.	O
the	O
thing	O
it	O
interacts	O
with	O
,	O
comprising	O
everything	O
outside	O
the	O
agent	O
,	O
is	O
called	O
the	O
environment	B
.	O
these	O
interact	O
continually	O
,	O
the	O
agent	O
selecting	O
actions	O
and	O
the	O
environment	O
responding	O
to	O
these	O
47	O
48	O
chapter	O
3	O
:	O
finite	O
markov	O
decision	O
processes	O
actions	O
and	O
presenting	O
new	O
situations	O
to	O
the	O
agent.1	O
the	O
environment	B
also	O
gives	O
rise	O
to	O
rewards	O
,	O
special	O
numerical	O
values	O
that	O
the	O
agent	O
seeks	O
to	O
maximize	O
over	O
time	O
through	O
its	O
choice	O
of	O
actions	O
.	O
figure	O
3.1	O
:	O
the	O
agent–environment	O
interaction	O
in	O
a	O
markov	O
decision	O
process	O
.	O
more	O
speciﬁcally	O
,	O
the	O
agent	O
and	O
environment	O
interact	O
at	O
each	O
of	O
a	O
sequence	O
of	O
discrete	O
time	O
steps	O
,	O
t	O
=	O
0	O
,	O
1	O
,	O
2	O
,	O
3	O
,	O
.	O
.	O
..2	O
at	O
each	O
time	O
step	O
t	O
,	O
the	O
agent	O
receives	O
some	O
representation	O
of	O
the	O
environment	B
’	O
s	O
state	B
,	O
st	O
∈	O
s	O
,	O
and	O
on	O
that	O
basis	O
selects	O
an	O
action	B
,	O
at	O
∈	O
a	O
(	O
s	O
)	O
.3	O
one	O
time	O
step	O
later	O
,	O
in	O
part	O
as	O
a	O
consequence	O
of	O
its	O
action	B
,	O
the	O
agent	O
receives	O
a	O
numerical	O
reward	O
,	O
rt+1	O
∈	O
r	O
⊂	O
r	O
,	O
and	O
ﬁnds	O
itself	O
in	O
a	O
new	O
state	B
,	O
st+1.4	O
the	O
mdp	O
and	O
agent	O
together	O
thereby	O
give	O
rise	O
to	O
a	O
sequence	O
or	O
trajectory	O
that	O
begins	O
like	O
this	O
:	O
s0	O
,	O
a0	O
,	O
r1	O
,	O
s1	O
,	O
a1	O
,	O
r2	O
,	O
s2	O
,	O
a2	O
,	O
r3	O
,	O
.	O
.	O
.	O
(	O
3.1	O
)	O
in	O
a	O
ﬁnite	O
mdp	O
,	O
the	O
sets	O
of	O
states	O
,	O
actions	O
,	O
and	O
rewards	O
(	O
s	O
,	O
a	O
,	O
and	O
r	O
)	O
all	O
have	O
a	O
ﬁnite	O
number	O
of	O
elements	O
.	O
in	O
this	O
case	O
,	O
the	O
random	O
variables	O
rt	O
and	O
st	O
have	O
well	O
deﬁned	O
discrete	O
probability	O
distributions	O
dependent	O
only	O
on	O
the	O
preceding	O
state	B
and	O
action	B
.	O
that	O
is	O
,	O
for	O
particular	O
values	O
of	O
these	O
random	O
variables	O
,	O
s	O
(	O
cid:48	O
)	O
∈	O
s	O
and	O
r	O
∈	O
r	O
,	O
there	O
is	O
a	O
probability	O
of	O
those	O
values	O
occurring	O
at	O
time	O
t	O
,	O
given	O
particular	O
values	O
of	O
the	O
preceding	O
state	B
and	O
action	B
:	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
.	O
=	O
pr	O
{	O
st	O
=	O
s	O
(	O
cid:48	O
)	O
,	O
rt	O
=	O
r	O
|	O
st−1	O
=	O
s	O
,	O
at−1	O
=	O
a	O
}	O
,	O
(	O
3.2	O
)	O
for	O
all	O
s	O
(	O
cid:48	O
)	O
,	O
s	O
∈	O
s	O
,	O
r	O
∈	O
r	O
,	O
and	O
a	O
∈	O
a	O
(	O
s	O
)	O
.	O
the	O
function	O
p	O
deﬁnes	O
the	O
dynamics	O
of	O
the	O
mdp	O
.	O
the	O
dot	O
over	O
the	O
equals	O
sign	O
in	O
the	O
equation	O
reminds	O
us	O
that	O
it	O
is	O
a	O
deﬁnition	O
(	O
in	O
this	O
case	O
of	O
the	O
function	O
p	O
)	O
rather	O
than	O
a	O
fact	O
that	O
follows	O
from	O
previous	O
deﬁnitions	O
.	O
the	O
dynamics	O
function	O
p	O
:	O
s	O
×	O
r	O
×	O
s	O
×	O
a	O
→	O
[	O
0	O
,	O
1	O
]	O
is	O
an	O
ordinary	O
deterministic	O
function	O
of	O
four	O
arguments	O
.	O
the	O
‘	O
|	O
’	O
in	O
the	O
middle	O
of	O
it	O
comes	O
from	O
the	O
notation	O
for	O
conditional	O
1we	O
use	O
the	O
terms	O
agent	O
,	O
environment	B
,	O
and	O
action	O
instead	O
of	O
the	O
engineers	O
’	O
terms	O
controller	O
,	O
con-	O
trolled	O
system	O
(	O
or	O
plant	O
)	O
,	O
and	B
control	I
signal	O
because	O
they	O
are	O
meaningful	O
to	O
a	O
wider	O
audience	O
.	O
2we	O
restrict	O
attention	O
to	O
discrete	O
time	O
to	O
keep	O
things	O
as	O
simple	O
as	O
possible	O
,	O
even	O
though	O
many	O
of	O
the	O
ideas	O
can	O
be	O
extended	O
to	O
the	O
continuous-time	O
case	O
(	O
e.g.	O
,	O
see	O
bertsekas	O
and	O
tsitsiklis	O
,	O
1996	O
;	O
doya	O
,	O
1996	O
)	O
.	O
3to	O
simplify	O
notation	O
,	O
we	O
sometimes	O
assume	O
the	O
special	O
case	O
in	O
which	O
the	O
action	B
set	O
is	O
the	O
same	O
in	O
all	O
states	O
and	O
write	O
it	O
simply	O
as	O
a	O
.	O
4we	O
use	O
rt+1	O
instead	O
of	O
rt	O
to	O
denote	O
the	O
reward	O
due	O
to	O
at	O
because	O
it	O
emphasizes	O
that	O
the	O
next	O
reward	O
and	O
next	O
state	B
,	O
rt+1	O
and	O
st+1	O
,	O
are	O
jointly	O
determined	O
.	O
unfortunately	O
,	O
both	O
conventions	O
are	O
widely	O
used	O
in	O
the	O
literature	O
.	O
agentenvironmentactionatrewardrtstatestrt+1st+1	O
3.1.	O
the	O
agent–environment	B
interface	I
49	O
probability	O
,	O
but	O
here	O
it	O
just	O
reminds	O
us	O
that	O
p	O
speciﬁes	O
a	O
probability	O
distribution	O
for	O
each	O
choice	O
of	O
s	O
and	O
a	O
,	O
that	O
is	O
,	O
that	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
∈s	O
(	O
cid:88	O
)	O
r∈r	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
=	O
1	O
,	O
for	O
all	O
s	O
∈	O
s	O
,	O
a	O
∈	O
a	O
(	O
s	O
)	O
.	O
(	O
3.3	O
)	O
in	O
a	O
markov	O
decision	O
process	O
,	O
the	O
probabilities	O
given	O
by	O
p	O
completely	O
characterize	O
the	O
environment	B
’	O
s	O
dynamics	O
.	O
that	O
is	O
,	O
the	O
probability	O
of	O
each	O
possible	O
value	B
for	O
st	O
and	O
rt	O
depends	O
only	O
on	O
the	O
immediately	O
preceding	O
state	B
and	O
action	B
,	O
st−1	O
and	O
at−1	O
,	O
and	O
,	O
given	O
them	O
,	O
not	O
at	O
all	O
on	O
earlier	O
states	O
and	O
actions	O
.	O
this	O
is	O
best	O
viewed	O
a	O
restriction	O
not	O
on	O
the	O
decision	O
process	O
,	O
but	O
on	O
the	O
state	B
.	O
the	O
state	B
must	O
include	O
information	O
about	O
all	O
aspects	O
of	O
the	O
past	O
agent–environment	O
interaction	O
that	O
make	O
a	O
diﬀerence	O
for	O
the	O
future	O
.	O
if	O
it	O
does	O
,	O
then	O
the	O
state	B
is	O
said	O
to	O
have	O
the	O
markov	O
property	O
.	O
we	O
will	O
assume	O
the	O
markov	O
property	O
throughout	O
this	O
book	O
,	O
though	O
starting	O
in	O
part	O
ii	O
we	O
will	O
consider	O
approximation	O
methods	O
that	O
do	O
not	O
rely	O
on	O
it	O
,	O
and	O
in	O
chapter	O
17	O
we	O
consider	O
how	O
a	O
markov	O
state	B
can	O
be	O
learned	O
and	O
constructed	O
from	O
non-markov	O
observations	O
.	O
from	O
the	O
four-argument	O
dynamics	O
function	O
,	O
p	O
,	O
one	O
can	O
compute	O
anything	O
else	O
one	O
might	O
want	O
to	O
know	O
about	O
the	O
environment	B
,	O
such	O
as	O
the	O
state-transition	O
probabilities	O
(	O
which	O
we	O
denote	O
,	O
with	O
a	O
slight	O
abuse	O
of	O
notation	O
,	O
as	O
a	O
three-argument	O
function	O
p	O
:	O
s	O
×	O
s	O
×	O
a	O
→	O
[	O
0	O
,	O
1	O
]	O
)	O
,	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
a	O
)	O
.	O
=	O
pr	O
{	O
st	O
=	O
s	O
(	O
cid:48	O
)	O
|	O
st−1	O
=	O
s	O
,	O
at−1	O
=	O
a	O
}	O
=	O
(	O
cid:88	O
)	O
r∈r	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
.	O
(	O
3.4	O
)	O
we	O
can	O
also	O
compute	O
the	O
expected	O
rewards	O
for	O
state–action	O
pairs	O
as	O
a	O
two-argument	O
function	O
r	O
:	O
s	O
×	O
a	O
→	O
r	O
:	O
r	O
(	O
s	O
,	O
a	O
)	O
.	O
=	O
e	O
[	O
rt	O
|	O
st−1	O
=	O
s	O
,	O
at−1	O
=	O
a	O
]	O
=	O
(	O
cid:88	O
)	O
r∈r	O
r	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
∈s	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
,	O
(	O
3.5	O
)	O
and	O
the	O
expected	O
rewards	O
for	O
state–action–next-state	O
triples	O
as	O
a	O
three-argument	O
function	O
r	O
:	O
s	O
×	O
a	O
×	O
s	O
→	O
r	O
,	O
.	O
=	O
e	O
[	O
rt	O
|	O
st−1	O
=	O
s	O
,	O
at−1	O
=	O
a	O
,	O
st	O
=	O
s	O
(	O
cid:48	O
)	O
]	O
=	O
(	O
cid:88	O
)	O
r∈r	O
r	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
a	O
)	O
.	O
r	O
(	O
s	O
,	O
a	O
,	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
3.6	O
)	O
in	O
this	O
book	O
,	O
we	O
usually	O
use	O
the	O
four-argument	O
p	O
function	O
(	O
3.2	O
)	O
,	O
but	O
each	O
of	O
these	O
other	O
notations	O
are	O
also	O
occasionally	O
convenient	O
.	O
the	O
mdp	O
framework	O
is	O
abstract	O
and	O
ﬂexible	O
and	O
can	O
be	O
applied	O
to	O
many	O
diﬀerent	O
problems	O
in	O
many	O
diﬀerent	O
ways	O
.	O
for	O
example	O
,	O
the	O
time	O
steps	O
need	O
not	O
refer	O
to	O
ﬁxed	O
intervals	O
of	O
real	O
time	O
;	O
they	O
can	O
refer	O
to	O
arbitrary	O
successive	O
stages	O
of	O
decision	O
making	O
and	O
acting	O
.	O
the	O
actions	O
can	O
be	O
low-level	O
controls	O
,	O
such	O
as	O
the	O
voltages	O
applied	O
to	O
the	O
motors	O
of	O
a	O
robot	O
arm	O
,	O
or	O
high-level	O
decisions	O
,	O
such	O
as	O
whether	O
or	O
not	O
to	O
have	O
lunch	O
or	O
to	O
go	O
to	O
graduate	O
school	O
.	O
similarly	O
,	O
the	O
states	O
can	O
take	O
a	O
wide	O
variety	O
of	O
forms	O
.	O
they	O
can	O
be	O
completely	O
determined	O
by	O
low-level	O
sensations	O
,	O
such	O
as	O
direct	O
sensor	O
readings	O
,	O
or	O
they	O
can	O
be	O
more	O
high-level	O
and	O
abstract	O
,	O
such	O
as	O
symbolic	O
descriptions	O
of	O
objects	O
in	O
a	O
room	O
.	O
some	O
of	O
what	O
makes	O
up	O
a	O
state	B
could	O
be	O
based	O
on	O
memory	O
of	O
past	O
sensations	O
or	O
50	O
chapter	O
3	O
:	O
finite	O
markov	O
decision	O
processes	O
even	O
be	O
entirely	O
mental	O
or	O
subjective	O
.	O
for	O
example	O
,	O
an	O
agent	O
could	O
be	O
in	O
the	O
state	O
of	O
not	O
being	O
sure	O
where	O
an	O
object	O
is	O
,	O
or	O
of	O
having	O
just	O
been	O
surprised	O
in	O
some	O
clearly	O
deﬁned	O
sense	O
.	O
similarly	O
,	O
some	O
actions	O
might	O
be	O
totally	O
mental	O
or	O
computational	O
.	O
for	O
example	O
,	O
some	O
actions	O
might	O
control	B
what	O
an	O
agent	O
chooses	O
to	O
think	O
about	O
,	O
or	O
where	O
it	O
focuses	O
its	O
attention	O
.	O
in	O
general	O
,	O
actions	O
can	O
be	O
any	O
decisions	O
we	O
want	O
to	O
learn	O
how	O
to	O
make	O
,	O
and	O
the	O
states	O
can	O
be	O
anything	O
we	O
can	O
know	O
that	O
might	O
be	O
useful	O
in	O
making	O
them	O
.	O
in	O
particular	O
,	O
the	O
boundary	O
between	O
agent	O
and	O
environment	O
is	O
typically	O
not	O
the	O
same	O
as	O
the	O
physical	O
boundary	O
of	O
robot	O
’	O
s	O
or	O
animal	O
’	O
s	O
body	O
.	O
usually	O
,	O
the	O
boundary	O
is	O
drawn	O
closer	O
to	O
the	O
agent	O
than	O
that	O
.	O
for	O
example	O
,	O
the	O
motors	O
and	O
mechanical	O
linkages	O
of	O
a	O
robot	O
and	O
its	O
sensing	O
hardware	O
should	O
usually	O
be	O
considered	O
parts	O
of	O
the	O
environment	B
rather	O
than	O
parts	O
of	O
the	O
agent	O
.	O
similarly	O
,	O
if	O
we	O
apply	O
the	O
mdp	O
framework	O
to	O
a	O
person	O
or	O
animal	O
,	O
the	O
muscles	O
,	O
skeleton	O
,	O
and	O
sensory	O
organs	O
should	O
be	O
considered	O
part	O
of	O
the	O
environment	B
.	O
rewards	O
,	O
too	O
,	O
presumably	O
are	O
computed	O
inside	O
the	O
physical	O
bodies	O
of	O
natural	O
and	O
artiﬁcial	O
learning	O
systems	O
,	O
but	O
are	O
considered	O
external	O
to	O
the	O
agent	O
.	O
the	O
general	O
rule	O
we	O
follow	O
is	O
that	O
anything	O
that	O
can	O
not	O
be	O
changed	O
arbitrarily	O
by	O
the	O
agent	O
is	O
considered	O
to	O
be	O
outside	O
of	O
it	O
and	O
thus	O
part	O
of	O
its	O
environment	B
.	O
we	O
do	O
not	O
assume	O
that	O
everything	O
in	O
the	O
environment	O
is	O
unknown	O
to	O
the	O
agent	O
.	O
for	O
example	O
,	O
the	O
agent	O
often	O
knows	O
quite	O
a	O
bit	O
about	O
how	O
its	O
rewards	O
are	O
computed	O
as	O
a	O
function	O
of	O
its	O
actions	O
and	O
the	O
states	O
in	O
which	O
they	O
are	O
taken	O
.	O
but	O
we	O
always	O
consider	O
the	O
reward	O
computation	O
to	O
be	O
external	O
to	O
the	O
agent	O
because	O
it	O
deﬁnes	O
the	O
task	O
facing	O
the	O
agent	O
and	O
thus	O
must	O
be	O
beyond	O
its	O
ability	O
to	O
change	O
arbitrarily	O
.	O
in	O
fact	O
,	O
in	O
some	O
cases	O
the	O
agent	O
may	O
know	O
everything	O
about	O
how	O
its	O
environment	B
works	O
and	O
still	O
face	O
a	O
diﬃcult	O
reinforcement	B
learning	I
task	O
,	O
just	O
as	O
we	O
may	O
know	O
exactly	O
how	O
a	O
puzzle	O
like	O
rubik	O
’	O
s	O
cube	O
works	O
,	O
but	O
still	O
be	O
unable	O
to	O
solve	O
it	O
.	O
the	O
agent–environment	O
boundary	O
represents	O
the	O
limit	O
of	O
the	O
agent	O
’	O
s	O
absolute	O
control	B
,	O
not	O
of	O
its	O
knowledge	O
.	O
the	O
agent–environment	O
boundary	O
can	O
be	O
located	O
at	O
diﬀerent	O
places	O
for	O
diﬀerent	O
pur-	O
poses	O
.	O
in	O
a	O
complicated	O
robot	O
,	O
many	O
diﬀerent	O
agents	O
may	O
be	O
operating	O
at	O
once	O
,	O
each	O
with	O
its	O
own	O
boundary	O
.	O
for	O
example	O
,	O
one	O
agent	O
may	O
make	O
high-level	O
decisions	O
which	O
form	O
part	O
of	O
the	O
states	O
faced	O
by	O
a	O
lower-level	O
agent	O
that	O
implements	O
the	O
high-level	O
deci-	O
sions	O
.	O
in	O
practice	O
,	O
the	O
agent–environment	O
boundary	O
is	O
determined	O
once	O
one	O
has	O
selected	O
particular	O
states	O
,	O
actions	O
,	O
and	O
rewards	O
,	O
and	O
thus	O
has	O
identiﬁed	O
a	O
speciﬁc	O
decision	O
making	O
task	O
of	O
interest	O
.	O
the	O
mdp	O
framework	O
is	O
a	O
considerable	O
abstraction	O
of	O
the	O
problem	O
of	O
goal-directed	O
learning	O
from	O
interaction	O
.	O
it	O
proposes	O
that	O
whatever	O
the	O
details	O
of	O
the	O
sensory	O
,	O
memory	O
,	O
and	B
control	I
apparatus	O
,	O
and	O
whatever	O
objective	O
one	O
is	O
trying	O
to	O
achieve	O
,	O
any	O
problem	O
of	O
learning	O
goal-directed	O
behavior	O
can	O
be	O
reduced	O
to	O
three	O
signals	O
passing	O
back	O
and	O
forth	O
between	O
an	O
agent	O
and	O
its	O
environment	B
:	O
one	O
signal	O
to	O
represent	O
the	O
choices	O
made	O
by	O
the	O
agent	O
(	O
the	O
actions	O
)	O
,	O
one	O
signal	O
to	O
represent	O
the	O
basis	O
on	O
which	O
the	O
choices	O
are	O
made	O
(	O
the	O
states	O
)	O
,	O
and	O
one	O
signal	O
to	O
deﬁne	O
the	O
agent	O
’	O
s	O
goal	B
(	O
the	O
rewards	O
)	O
.	O
this	O
framework	O
may	O
not	O
be	O
suﬃcient	O
to	O
represent	O
all	O
decision-learning	O
problems	O
usefully	O
,	O
but	O
it	O
has	O
proved	O
to	O
be	O
widely	O
useful	O
and	O
applicable	O
.	O
of	O
course	O
,	O
the	O
particular	O
states	O
and	O
actions	O
vary	O
greatly	O
from	O
task	O
to	O
task	O
,	O
and	O
how	O
they	O
are	O
represented	O
can	O
strongly	O
aﬀect	O
performance	O
.	O
in	O
reinforcement	O
learning	O
,	O
as	O
in	O
other	O
kinds	O
of	O
learning	O
,	O
such	O
representational	O
choices	O
are	O
at	O
present	O
more	O
art	O
than	O
science	O
.	O
3.1.	O
the	O
agent–environment	B
interface	I
51	O
in	O
this	O
book	O
we	O
oﬀer	O
some	O
advice	O
and	O
examples	O
regarding	O
good	O
ways	O
of	O
representing	O
states	O
and	O
actions	O
,	O
but	O
our	O
primary	O
focus	O
is	O
on	O
general	O
principles	O
for	O
learning	O
how	O
to	O
behave	O
once	O
the	O
representations	O
have	O
been	O
selected	O
.	O
example	O
3.1	O
:	O
bioreactor	O
suppose	O
reinforcement	B
learning	I
is	O
being	O
applied	O
to	O
deter-	O
mine	O
moment-by-moment	O
temperatures	O
and	O
stirring	O
rates	O
for	O
a	O
bioreactor	O
(	O
a	O
large	O
vat	O
of	O
nutrients	O
and	O
bacteria	O
used	O
to	O
produce	O
useful	O
chemicals	O
)	O
.	O
the	O
actions	O
in	O
such	O
an	O
applica-	O
tion	B
might	O
be	O
target	B
temperatures	O
and	O
target	O
stirring	O
rates	O
that	O
are	O
passed	O
to	O
lower-level	O
control	B
systems	O
that	O
,	O
in	O
turn	O
,	O
directly	O
activate	O
heating	O
elements	O
and	O
motors	O
to	O
attain	O
the	O
targets	O
.	O
the	O
states	O
are	O
likely	O
to	O
be	O
thermocouple	O
and	O
other	O
sensory	O
readings	O
,	O
perhaps	O
ﬁltered	O
and	O
delayed	O
,	O
plus	O
symbolic	O
inputs	O
representing	O
the	O
ingredients	O
in	O
the	O
vat	O
and	O
the	O
target	O
chemical	O
.	O
the	O
rewards	O
might	O
be	O
moment-by-moment	O
measures	O
of	O
the	O
rate	O
at	O
which	O
the	O
useful	O
chemical	O
is	O
produced	O
by	O
the	O
bioreactor	O
.	O
notice	O
that	O
here	O
each	O
state	B
is	O
a	O
list	O
,	O
or	O
vector	B
,	O
of	O
sensor	O
readings	O
and	O
symbolic	O
inputs	O
,	O
and	O
each	O
action	B
is	O
a	O
vector	B
con-	O
sisting	O
of	O
a	O
target	B
temperature	O
and	O
a	O
stirring	O
rate	O
.	O
it	O
is	O
typical	O
of	O
reinforcement	O
learning	O
tasks	O
to	O
have	O
states	O
and	O
actions	O
with	O
such	O
structured	O
representations	O
.	O
rewards	O
,	O
on	O
the	O
other	O
hand	O
,	O
are	O
always	O
single	O
numbers	O
.	O
example	O
3.2	O
:	O
pick-and-place	O
robot	O
consider	O
using	O
reinforcement	B
learning	I
to	O
control	B
the	O
motion	O
of	O
a	O
robot	O
arm	O
in	O
a	O
repetitive	O
pick-and-place	O
task	O
.	O
if	O
we	O
want	O
to	O
learn	O
movements	O
that	O
are	O
fast	O
and	O
smooth	O
,	O
the	O
learning	O
agent	O
will	O
have	O
to	O
control	B
the	O
motors	O
directly	O
and	O
have	O
low-latency	O
information	O
about	O
the	O
current	O
positions	O
and	O
velocities	O
of	O
the	O
mechanical	O
linkages	O
.	O
the	O
actions	O
in	O
this	O
case	O
might	O
be	O
the	O
voltages	O
applied	O
to	O
each	O
motor	O
at	O
each	O
joint	O
,	O
and	O
the	O
states	O
might	O
be	O
the	O
latest	O
readings	O
of	O
joint	O
angles	O
and	O
velocities	O
.	O
the	O
reward	O
might	O
be	O
+1	O
for	O
each	O
object	O
successfully	O
picked	O
up	O
and	O
placed	O
.	O
to	O
encourage	O
smooth	O
movements	O
,	O
on	O
each	O
time	O
step	O
a	O
small	O
,	O
negative	O
reward	O
can	O
be	O
given	O
as	O
a	O
function	O
of	O
the	O
moment-to-moment	O
“	O
jerkiness	O
”	O
of	O
the	O
motion	O
.	O
exercise	O
3.1	O
devise	O
three	O
example	O
tasks	O
of	O
your	O
own	O
that	O
ﬁt	O
into	O
the	O
mdp	O
framework	O
,	O
identifying	O
for	O
each	O
its	O
states	O
,	O
actions	O
,	O
and	O
rewards	O
.	O
make	O
the	O
three	O
examples	O
as	O
diﬀerent	O
from	O
each	O
other	O
as	O
possible	O
.	O
the	O
framework	O
is	O
abstract	O
and	O
ﬂexible	O
and	O
can	O
be	O
applied	O
in	O
many	O
diﬀerent	O
ways	O
.	O
stretch	O
its	O
limits	O
in	O
some	O
way	O
in	O
at	O
least	O
one	O
of	O
your	O
examples	O
.	O
(	O
cid:3	O
)	O
exercise	O
3.2	O
is	O
the	O
mdp	O
framework	O
adequate	O
to	O
usefully	O
represent	O
all	O
goal-directed	O
(	O
cid:3	O
)	O
learning	O
tasks	O
?	O
can	O
you	O
think	O
of	O
any	O
clear	O
exceptions	O
?	O
exercise	O
3.3	O
consider	O
the	O
problem	O
of	O
driving	O
.	O
you	O
could	O
deﬁne	O
the	O
actions	O
in	O
terms	O
of	O
the	O
accelerator	O
,	O
steering	O
wheel	O
,	O
and	O
brake	O
,	O
that	O
is	O
,	O
where	O
your	O
body	O
meets	O
the	O
machine	O
.	O
or	O
you	O
could	O
deﬁne	O
them	O
farther	O
out—say	O
,	O
where	O
the	O
rubber	O
meets	O
the	O
road	O
,	O
considering	O
your	O
actions	O
to	O
be	O
tire	O
torques	O
.	O
or	O
you	O
could	O
deﬁne	O
them	O
farther	O
in—say	O
,	O
where	O
your	O
brain	O
meets	O
your	O
body	O
,	O
the	O
actions	O
being	O
muscle	O
twitches	O
to	O
control	B
your	O
limbs	O
.	O
or	O
you	O
could	O
go	O
to	O
a	O
really	O
high	O
level	O
and	O
say	O
that	O
your	O
actions	O
are	O
your	O
choices	O
of	O
where	O
to	O
drive	O
.	O
what	O
is	O
the	O
right	O
level	O
,	O
the	O
right	O
place	O
to	O
draw	O
the	O
line	O
between	O
agent	O
and	O
environment	O
?	O
on	O
what	O
basis	O
is	O
one	O
location	O
of	O
the	O
line	O
to	O
be	O
preferred	O
over	O
another	O
?	O
is	O
there	O
any	O
fundamental	O
reason	O
for	O
preferring	O
one	O
location	O
over	O
another	O
,	O
or	O
is	O
it	O
a	O
free	O
(	O
cid:3	O
)	O
choice	O
?	O
52	O
chapter	O
3	O
:	O
finite	O
markov	O
decision	O
processes	O
example	O
3.3	O
recycling	O
robot	O
a	O
mobile	O
robot	O
has	O
the	O
job	O
of	O
collecting	O
empty	O
soda	O
cans	O
in	O
an	O
oﬃce	O
environment	B
.	O
it	O
has	O
sensors	O
for	O
detecting	O
cans	O
,	O
and	O
an	O
arm	O
and	O
gripper	O
that	O
can	O
pick	O
them	O
up	O
and	O
place	O
them	O
in	O
an	O
onboard	O
bin	O
;	O
it	O
runs	O
on	O
a	O
rechargeable	O
battery	O
.	O
the	O
robot	O
’	O
s	O
control	B
system	O
has	O
components	O
for	O
interpreting	O
sensory	O
information	O
,	O
for	O
navigating	O
,	O
and	O
for	O
controlling	O
the	O
arm	O
and	O
gripper	O
.	O
high-level	O
decisions	O
about	O
how	O
to	O
search	O
for	O
cans	O
are	O
made	O
by	O
a	O
reinforcement	B
learning	I
agent	O
based	O
on	O
the	O
current	O
charge	O
level	O
of	O
the	O
battery	O
.	O
to	O
make	O
a	O
simple	O
example	O
,	O
we	O
assume	O
that	O
only	O
two	O
charge	O
levels	O
can	O
be	O
distinguished	O
,	O
comprising	O
a	O
small	O
state	B
set	O
s	O
=	O
{	O
high	O
,	O
low	O
}	O
.	O
in	O
each	O
state	B
,	O
the	O
agent	O
can	O
decide	O
whether	O
to	O
(	O
1	O
)	O
actively	O
search	O
for	O
a	O
can	O
for	O
a	O
certain	O
period	O
of	O
time	O
,	O
(	O
2	O
)	O
remain	O
stationary	O
and	O
wait	O
for	O
someone	O
to	O
bring	O
it	O
a	O
can	O
,	O
or	O
(	O
3	O
)	O
head	O
back	O
to	O
its	O
home	O
base	O
to	O
recharge	O
its	O
battery	O
.	O
when	O
the	O
energy	O
level	O
is	O
high	O
,	O
recharging	O
would	O
always	O
be	O
foolish	O
,	O
so	O
we	O
do	O
not	O
include	O
it	O
in	O
the	O
action	O
set	O
for	O
this	O
state	B
.	O
the	O
action	B
sets	O
are	O
then	O
a	O
(	O
high	O
)	O
=	O
{	O
search	O
,	O
wait	O
}	O
and	O
a	O
(	O
low	O
)	O
=	O
{	O
search	O
,	O
wait	O
,	O
recharge	O
}	O
.	O
the	O
rewards	O
are	O
zero	O
most	O
of	O
the	O
time	O
,	O
but	O
become	O
positive	O
when	O
the	O
robot	O
secures	O
an	O
empty	O
can	O
,	O
or	O
large	O
and	O
negative	O
if	O
the	O
battery	O
runs	O
all	O
the	O
way	O
down	O
.	O
the	O
best	O
way	O
to	O
ﬁnd	O
cans	O
is	O
to	O
actively	O
search	O
for	O
them	O
,	O
but	O
this	O
runs	O
down	O
the	O
robot	O
’	O
s	O
battery	O
,	O
whereas	O
waiting	O
does	O
not	O
.	O
whenever	O
the	O
robot	O
is	O
searching	O
,	O
the	O
possibility	O
exists	O
that	O
its	O
battery	O
will	O
become	O
depleted	O
.	O
in	O
this	O
case	O
the	O
robot	O
must	O
shut	O
down	O
and	O
wait	O
to	O
be	O
rescued	O
(	O
producing	O
a	O
low	O
reward	O
)	O
.	O
if	O
the	O
energy	O
level	O
is	O
high	O
,	O
then	O
a	O
period	O
of	O
active	O
search	O
can	O
always	O
be	O
completed	O
without	O
risk	O
of	O
depleting	O
the	O
battery	O
.	O
a	O
period	O
of	O
searching	O
that	O
begins	O
with	O
a	O
high	O
energy	O
level	O
leaves	O
the	O
energy	O
level	O
high	O
with	O
probability	O
α	O
and	O
reduces	O
it	O
to	O
low	O
with	O
probability	O
1	O
−	O
α.	O
on	O
the	O
other	O
hand	O
,	O
a	O
period	O
of	O
searching	O
undertaken	O
when	O
the	O
energy	O
level	O
is	O
low	O
leaves	O
it	O
low	O
with	O
probability	O
β	O
and	O
depletes	O
the	O
battery	O
with	O
probability	O
1	O
−	O
β.	O
in	O
the	O
latter	O
case	O
,	O
the	O
robot	O
must	O
be	O
rescued	O
,	O
and	O
the	O
battery	O
is	O
then	O
recharged	O
back	O
to	O
high	O
.	O
each	O
can	O
collected	O
by	O
the	O
robot	O
counts	O
as	O
a	O
unit	O
reward	O
,	O
whereas	O
a	O
reward	O
of	O
−3	O
results	O
whenever	O
the	O
robot	O
has	O
to	O
be	O
rescued	O
.	O
let	O
rsearch	O
and	O
rwait	O
,	O
with	O
rsearch	O
>	O
rwait	O
,	O
respectively	O
denote	O
the	O
expected	O
number	O
of	O
cans	O
the	O
robot	O
will	O
collect	O
(	O
and	O
hence	O
the	O
expected	O
reward	O
)	O
while	O
searching	O
and	O
while	O
waiting	O
.	O
finally	O
,	O
suppose	O
that	O
no	O
cans	O
can	O
be	O
collected	O
during	O
a	O
run	O
home	O
for	O
recharging	O
,	O
and	O
that	O
no	O
cans	O
can	O
be	O
collected	O
on	O
a	O
step	O
in	O
which	O
the	O
battery	O
is	O
depleted	O
.	O
this	O
system	O
is	O
then	O
a	O
ﬁnite	O
mdp	O
,	O
and	O
we	O
can	O
write	O
down	O
the	O
transition	B
probabilities	I
and	O
the	O
expected	O
rewards	O
,	O
with	O
dynamics	O
as	O
indicated	O
in	O
the	O
table	O
on	O
the	O
left	O
:	O
s	O
high	O
high	O
low	O
low	O
high	O
high	O
low	O
low	O
low	O
low	O
a	O
search	O
search	O
search	O
search	O
wait	O
wait	O
wait	O
wait	O
recharge	O
recharge	O
s	O
(	O
cid:48	O
)	O
high	O
low	O
high	O
low	O
high	O
low	O
high	O
low	O
high	O
low	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
a	O
)	O
α	O
1	O
−	O
α	O
1	O
−	O
β	O
β	O
1	O
0	O
0	O
1	O
1	O
0	O
r	O
(	O
s	O
,	O
a	O
,	O
s	O
(	O
cid:48	O
)	O
)	O
rsearch	O
rsearch	O
−3	O
rsearch	O
rwait	O
rwait	O
rwait	O
rwait	O
0	O
0	O
note	O
that	O
there	O
is	O
a	O
row	O
in	O
the	O
table	O
for	O
each	O
possible	O
combination	O
of	O
current	O
state	B
,	O
s	O
,	O
action	B
,	O
a	O
∈	O
a	O
(	O
s	O
)	O
,	O
and	O
next	O
state	B
,	O
s	O
(	O
cid:48	O
)	O
.	O
another	O
useful	O
way	O
of	O
summarizing	O
the	O
dynamics	O
of	O
a	O
ﬁnite	O
mdp	O
is	O
as	O
a	O
transition	O
graph	O
as	O
shown	O
above	O
on	O
the	O
right	O
.	O
there	O
are	O
two	O
kinds	O
of	O
3.2.	O
goals	O
and	O
rewards	O
53	O
nodes	O
:	O
state	B
nodes	O
and	O
action	O
nodes	O
.	O
there	O
is	O
a	O
state	B
node	O
for	O
each	O
possible	O
state	B
(	O
a	O
large	O
open	O
circle	O
labeled	O
by	O
the	O
name	O
of	O
the	O
state	B
)	O
,	O
and	O
an	O
action	B
node	O
for	O
each	O
state–action	O
pair	O
(	O
a	O
small	O
solid	O
circle	O
labeled	O
by	O
the	O
name	O
of	O
the	O
action	B
and	O
connected	O
by	O
a	O
line	O
to	O
the	O
state	B
node	O
)	O
.	O
starting	O
in	O
state	O
s	O
and	O
taking	O
action	B
a	O
moves	O
you	O
along	O
the	O
line	O
from	O
state	B
node	O
s	O
to	O
action	B
node	O
(	O
s	O
,	O
a	O
)	O
.	O
then	O
the	O
environment	B
responds	O
with	O
a	O
transition	O
to	O
the	O
next	O
state	B
’	O
s	O
node	O
via	O
one	O
of	O
the	O
arrows	O
leaving	O
action	B
node	O
(	O
s	O
,	O
a	O
)	O
.	O
each	O
arrow	O
corresponds	O
to	O
a	O
triple	O
(	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
)	O
,	O
where	O
s	O
(	O
cid:48	O
)	O
is	O
the	O
next	O
state	B
,	O
and	O
we	O
label	O
the	O
arrow	O
with	O
the	O
transition	O
probability	O
,	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
a	O
)	O
,	O
and	O
the	O
expected	O
reward	O
for	O
that	O
transition	O
,	O
r	O
(	O
s	O
,	O
a	O
,	O
s	O
(	O
cid:48	O
)	O
)	O
.	O
note	O
that	O
the	O
transition	B
probabilities	I
labeling	O
the	O
arrows	O
leaving	O
an	O
action	B
node	O
always	O
sum	O
to	O
1.	O
exercise	O
3.4	O
give	O
a	O
table	O
analogous	O
to	O
that	O
in	O
example	O
3.3	O
,	O
but	O
for	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
.	O
it	O
should	O
have	O
columns	O
for	O
s	O
,	O
a	O
,	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
,	O
and	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
,	O
and	O
a	O
row	O
for	O
every	O
4-tuple	O
for	O
(	O
cid:3	O
)	O
which	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
>	O
0	O
.	O
3.2	O
goals	O
and	O
rewards	O
in	O
reinforcement	O
learning	O
,	O
the	O
purpose	O
or	O
goal	B
of	O
the	O
agent	O
is	O
formalized	O
in	O
terms	O
of	O
a	O
special	O
signal	O
,	O
called	O
the	O
reward	O
,	O
passing	O
from	O
the	O
environment	B
to	O
the	O
agent	O
.	O
at	O
each	O
time	O
step	O
,	O
the	O
reward	O
is	O
a	O
simple	O
number	O
,	O
rt	O
∈	O
r.	O
informally	O
,	O
the	O
agent	O
’	O
s	O
goal	B
is	O
to	O
maximize	O
the	O
total	O
amount	O
of	O
reward	O
it	O
receives	O
.	O
this	O
means	O
maximizing	O
not	O
immediate	O
reward	O
,	O
but	O
cumulative	O
reward	O
in	O
the	O
long	O
run	O
.	O
we	O
can	O
clearly	O
state	B
this	O
informal	O
idea	O
as	O
the	O
reward	O
hypothesis	O
:	O
that	O
all	O
of	O
what	O
we	O
mean	O
by	O
goals	O
and	O
purposes	O
can	O
be	O
well	O
thought	O
of	O
as	O
the	O
maximization	O
of	O
the	O
expected	O
value	O
of	O
the	O
cumulative	O
sum	O
of	O
a	O
received	O
scalar	O
signal	O
(	O
called	O
reward	O
)	O
.	O
the	O
use	O
of	O
a	O
reward	B
signal	I
to	O
formalize	O
the	O
idea	O
of	O
a	O
goal	B
is	O
one	O
of	O
the	O
most	O
distinctive	O
features	O
of	O
reinforcement	O
learning	O
.	O
although	O
formulating	O
goals	O
in	O
terms	O
of	O
reward	O
signals	O
might	O
at	O
ﬁrst	O
appear	O
limiting	O
,	O
in	O
practice	O
it	O
has	O
proved	O
to	O
be	O
ﬂexible	O
and	O
widely	O
applicable	O
.	O
the	O
best	O
way	O
to	O
see	O
this	O
is	O
to	O
consider	O
examples	O
of	O
how	O
it	O
has	O
been	O
,	O
or	O
could	O
be	O
,	O
used	O
.	O
for	O
example	O
,	O
to	O
make	O
a	O
robot	O
learn	O
to	O
walk	O
,	O
researchers	O
have	O
provided	O
reward	O
on	O
each	O
time	O
step	O
proportional	O
to	O
the	O
robot	O
’	O
s	O
forward	O
motion	O
.	O
in	O
making	O
a	O
robot	O
learn	O
how	O
to	O
escape	O
from	O
a	O
maze	O
,	O
the	O
reward	O
is	O
often	O
−1	O
for	O
every	O
time	O
step	O
that	O
passes	O
prior	O
to	O
escape	O
;	O
this	O
encourages	O
the	O
agent	O
to	O
escape	O
as	O
quickly	O
as	O
possible	O
.	O
to	O
make	O
a	O
robot	O
learn	O
to	O
ﬁnd	O
and	O
collect	O
empty	O
soda	O
cans	O
for	O
recycling	O
,	O
one	O
might	O
give	O
it	O
a	O
reward	O
of	O
zero	O
most	O
of	O
the	O
time	O
,	O
and	O
then	O
a	O
reward	O
of	O
+1	O
for	O
each	O
can	O
collected	O
.	O
one	O
might	O
also	O
want	O
to	O
give	O
the	O
robot	O
negative	O
rewards	O
when	O
it	O
bumps	O
into	O
things	O
or	O
when	O
somebody	O
yells	O
at	O
it	O
.	O
for	O
an	O
agent	O
to	O
learn	O
to	O
play	O
checkers	O
or	O
chess	B
,	O
the	O
natural	O
rewards	O
are	O
+1	O
for	O
winning	O
,	O
−1	O
for	O
losing	O
,	O
and	O
0	O
for	O
drawing	O
and	O
for	O
all	O
nonterminal	O
positions	O
.	O
you	O
can	O
see	O
what	O
is	O
happening	O
in	O
all	O
of	O
these	O
examples	O
.	O
the	O
agent	O
always	O
learns	O
to	O
maximize	O
its	O
reward	O
.	O
if	O
we	O
want	O
it	O
to	O
do	O
something	O
for	O
us	O
,	O
we	O
must	O
provide	O
rewards	O
to	O
it	O
in	O
such	O
a	O
way	O
that	O
in	O
maximizing	O
them	O
the	O
agent	O
will	O
also	O
achieve	O
our	O
goals	O
.	O
it	O
is	O
thus	O
critical	O
that	O
the	O
rewards	O
we	O
set	O
up	O
truly	O
indicate	O
what	O
we	O
want	O
accomplished	O
.	O
54	O
chapter	O
3	O
:	O
finite	O
markov	O
decision	O
processes	O
in	O
particular	O
,	O
the	O
reward	B
signal	I
is	O
not	O
the	O
place	O
to	O
impart	O
to	O
the	O
agent	O
prior	B
knowledge	I
about	O
how	O
to	O
achieve	O
what	O
we	O
want	O
it	O
to	O
do.5	O
for	O
example	O
,	O
a	O
chess-playing	O
agent	O
should	O
be	O
rewarded	O
only	O
for	O
actually	O
winning	O
,	O
not	O
for	O
achieving	O
subgoals	O
such	O
as	O
taking	O
its	O
opponent	O
’	O
s	O
pieces	O
or	O
gaining	O
control	B
of	O
the	O
center	O
of	O
the	O
board	O
.	O
if	O
achieving	O
these	O
sorts	O
of	O
subgoals	O
were	O
rewarded	O
,	O
then	O
the	O
agent	O
might	O
ﬁnd	O
a	O
way	O
to	O
achieve	O
them	O
without	O
achieving	O
the	O
real	O
goal	B
.	O
for	O
example	O
,	O
it	O
might	O
ﬁnd	O
a	O
way	O
to	O
take	O
the	O
opponent	O
’	O
s	O
pieces	O
even	O
at	O
the	O
cost	O
of	O
losing	O
the	O
game	O
.	O
the	O
reward	B
signal	I
is	O
your	O
way	O
of	O
communicating	O
to	O
the	O
robot	O
what	O
you	O
want	O
it	O
to	O
achieve	O
,	O
not	O
how	O
you	O
want	O
it	O
achieved.6	O
3.3	O
returns	O
and	O
episodes	O
so	O
far	O
we	O
have	O
discussed	O
the	O
objective	O
of	O
learning	O
informally	O
.	O
we	O
have	O
said	O
that	O
the	O
agent	O
’	O
s	O
goal	B
is	O
to	O
maximize	O
the	O
cumulative	O
reward	O
it	O
receives	O
in	O
the	O
long	O
run	O
.	O
how	O
might	O
this	O
be	O
deﬁned	O
formally	O
?	O
if	O
the	O
sequence	O
of	O
rewards	O
received	O
after	O
time	O
step	O
t	O
is	O
denoted	O
rt+1	O
,	O
rt+2	O
,	O
rt+3	O
,	O
.	O
.	O
.	O
,	O
then	O
what	O
precise	O
aspect	O
of	O
this	O
sequence	O
do	O
we	O
wish	O
to	O
maximize	O
?	O
in	O
general	O
,	O
we	O
seek	O
to	O
maximize	O
the	O
expected	O
return	O
,	O
where	O
the	O
return	B
,	O
denoted	O
gt	O
,	O
is	O
deﬁned	O
as	O
some	O
speciﬁc	O
function	O
of	O
the	O
reward	O
sequence	O
.	O
in	O
the	O
simplest	O
case	O
the	O
return	B
is	O
the	O
sum	O
of	O
the	O
rewards	O
:	O
gt	O
.	O
=	O
rt+1	O
+	O
rt+2	O
+	O
rt+3	O
+	O
···	O
+	O
rt	O
,	O
(	O
3.7	O
)	O
where	O
t	O
is	O
a	O
ﬁnal	O
time	O
step	O
.	O
this	O
approach	O
makes	O
sense	O
in	O
applications	O
in	O
which	O
there	O
is	O
a	O
natural	O
notion	O
of	O
ﬁnal	O
time	O
step	O
,	O
that	O
is	O
,	O
when	O
the	O
agent–environment	O
interaction	O
breaks	O
naturally	O
into	O
subsequences	O
,	O
which	O
we	O
call	O
episodes,7	O
such	O
as	O
plays	O
of	O
a	O
game	O
,	O
trips	O
through	O
a	O
maze	O
,	O
or	O
any	O
sort	O
of	O
repeated	O
interaction	O
.	O
each	O
episode	O
ends	O
in	O
a	O
special	O
state	B
called	O
the	O
terminal	O
state	B
,	O
followed	O
by	O
a	O
reset	O
to	O
a	O
standard	O
starting	O
state	B
or	O
to	O
a	O
sample	O
from	O
a	O
standard	O
distribution	O
of	O
starting	O
states	O
.	O
even	O
if	O
you	O
think	O
of	O
episodes	O
as	O
ending	O
in	O
diﬀerent	O
ways	O
,	O
such	O
as	O
winning	O
and	O
losing	O
a	O
game	O
,	O
the	O
next	O
episode	O
begins	O
independently	O
of	O
how	O
the	O
previous	O
one	O
ended	O
.	O
thus	O
the	O
episodes	B
can	O
all	O
be	O
considered	O
to	O
end	O
in	O
the	O
same	O
terminal	O
state	B
,	O
with	O
diﬀerent	O
rewards	O
for	O
the	O
diﬀerent	O
outcomes	O
.	O
tasks	O
with	O
episodes	O
of	O
this	O
kind	O
are	O
called	O
episodic	O
tasks	O
.	O
in	O
episodic	O
tasks	O
we	O
sometimes	O
need	O
to	O
distinguish	O
the	O
set	O
of	O
all	O
nonterminal	O
states	O
,	O
denoted	O
s	O
,	O
from	O
the	O
set	O
of	O
all	O
states	O
plus	O
the	O
terminal	O
state	B
,	O
denoted	O
s+	O
.	O
the	O
time	O
of	O
termination	O
,	O
t	O
,	O
is	O
a	O
random	O
variable	O
that	O
normally	O
varies	O
from	O
episode	O
to	O
episode	O
.	O
on	O
the	O
other	O
hand	O
,	O
in	O
many	O
cases	O
the	O
agent–environment	O
interaction	O
does	O
not	O
break	O
naturally	O
into	O
identiﬁable	O
episodes	B
,	O
but	O
goes	O
on	O
continually	O
without	O
limit	O
.	O
for	O
example	O
,	O
this	O
would	O
be	O
the	O
natural	O
way	O
to	O
formulate	O
an	O
on-going	O
process-control	O
task	O
,	O
or	O
an	O
application	O
to	O
a	O
robot	O
with	O
a	O
long	O
life	O
span	O
.	O
we	O
call	O
these	O
continuing	B
tasks	I
.	O
the	O
return	B
formulation	O
(	O
3.7	O
)	O
is	O
problematic	O
for	O
continuing	O
tasks	O
because	O
the	O
ﬁnal	O
time	O
step	O
would	O
be	O
t	O
=	O
∞	O
,	O
and	O
the	O
return	O
,	O
which	O
is	O
what	O
we	O
are	O
trying	O
to	O
maximize	O
,	O
could	O
itself	O
easily	O
5better	O
places	O
for	O
imparting	O
this	O
kind	O
of	O
prior	O
knowledge	O
are	O
the	O
initial	O
policy	B
or	O
initial	O
value	B
function	I
,	O
or	O
in	O
inﬂuences	O
on	O
these	O
.	O
6section	O
17.4	O
delves	O
further	O
into	O
the	O
issue	O
of	O
designing	O
eﬀective	O
reward	O
signals	O
.	O
7episodes	O
are	O
sometimes	O
called	O
“	O
trials	O
”	O
in	O
the	O
literature	O
.	O
3.3.	O
returns	O
and	O
episodes	O
55	O
be	O
inﬁnite	O
.	O
(	O
for	O
example	O
,	O
suppose	O
the	O
agent	O
receives	O
a	O
reward	O
of	O
+1	O
at	O
each	O
time	O
step	O
.	O
)	O
thus	O
,	O
in	O
this	O
book	O
we	O
usually	O
use	O
a	O
deﬁnition	O
of	O
return	O
that	O
is	O
slightly	O
more	O
complex	O
conceptually	O
but	O
much	O
simpler	O
mathematically	O
.	O
the	O
additional	O
concept	O
that	O
we	O
need	O
is	O
that	O
of	O
discounting	O
.	O
according	O
to	O
this	O
approach	O
,	O
the	O
agent	O
tries	O
to	O
select	O
actions	O
so	O
that	O
the	O
sum	O
of	O
the	O
discounted	O
rewards	O
it	O
receives	O
over	O
the	O
future	O
is	O
maximized	O
.	O
in	O
particular	O
,	O
it	O
chooses	O
at	O
to	O
maximize	O
the	O
expected	O
discounted	O
return	B
:	O
gt	O
.	O
=	O
rt+1	O
+	O
γrt+2	O
+	O
γ2rt+3	O
+	O
···	O
=	O
γkrt+k+1	O
,	O
∞	O
(	O
cid:88	O
)	O
k=0	O
(	O
3.8	O
)	O
where	O
γ	O
is	O
a	O
parameter	O
,	O
0	O
≤	O
γ	O
≤	O
1	O
,	O
called	O
the	O
discount	O
rate	O
.	O
the	O
discount	O
rate	O
determines	O
the	O
present	O
value	B
of	O
future	O
rewards	O
:	O
a	O
reward	O
received	O
k	O
time	O
steps	O
in	O
the	O
future	O
is	O
worth	O
only	O
γk−1	O
times	O
what	O
it	O
would	O
be	O
worth	O
if	O
it	O
were	O
received	O
immediately	O
.	O
if	O
γ	O
<	O
1	O
,	O
the	O
inﬁnite	O
sum	O
in	O
(	O
3.8	O
)	O
has	O
a	O
ﬁnite	O
value	B
as	O
long	O
as	O
the	O
reward	O
sequence	O
{	O
rk	O
}	O
is	O
bounded	O
.	O
if	O
γ	O
=	O
0	O
,	O
the	O
agent	O
is	O
“	O
myopic	O
”	O
in	O
being	O
concerned	O
its	O
objective	O
in	O
this	O
case	O
is	O
to	O
learn	O
how	O
to	O
only	O
with	O
maximizing	O
immediate	O
rewards	O
:	O
choose	O
at	O
so	O
as	O
to	O
maximize	O
only	O
rt+1	O
.	O
if	O
each	O
of	O
the	O
agent	O
’	O
s	O
actions	O
happened	O
to	O
inﬂuence	O
only	O
the	O
immediate	O
reward	O
,	O
not	O
future	O
rewards	O
as	O
well	O
,	O
then	O
a	O
myopic	O
agent	O
could	O
maximize	O
(	O
3.8	O
)	O
by	O
separately	O
maximizing	O
each	O
immediate	O
reward	O
.	O
but	O
in	O
general	O
,	O
acting	O
to	O
maximize	O
immediate	O
reward	O
can	O
reduce	O
access	O
to	O
future	O
rewards	O
so	O
that	O
the	O
return	B
is	O
reduced	O
.	O
as	O
γ	O
approaches	O
1	O
,	O
the	O
return	B
objective	O
takes	O
future	O
rewards	O
into	O
account	O
more	O
strongly	O
;	O
the	O
agent	O
becomes	O
more	O
farsighted	O
.	O
returns	O
at	O
successive	O
time	O
steps	O
are	O
related	O
to	O
each	O
other	O
in	O
a	O
way	O
that	O
is	O
important	O
for	O
the	O
theory	O
and	O
algorithms	O
of	O
reinforcement	O
learning	O
:	O
gt	O
.	O
=	O
rt+1	O
+	O
γrt+2	O
+	O
γ2rt+3	O
+	O
γ3rt+4	O
+	O
···	O
=	O
rt+1	O
+	O
γ	O
(	O
cid:0	O
)	O
rt+2	O
+	O
γrt+3	O
+	O
γ2rt+4	O
+	O
···	O
(	O
cid:1	O
)	O
=	O
rt+1	O
+	O
γgt+1	O
(	O
3.9	O
)	O
note	O
that	O
this	O
works	O
for	O
all	O
time	O
steps	O
t	O
<	O
t	O
,	O
even	O
if	O
termination	O
occurs	O
at	O
t	O
+	O
1	O
,	O
if	O
we	O
deﬁne	O
gt	O
=	O
0.	O
this	O
often	O
makes	O
it	O
easy	O
to	O
compute	O
returns	O
from	O
reward	O
sequences	O
.	O
note	O
that	O
although	O
the	O
return	B
(	O
3.8	O
)	O
is	O
a	O
sum	O
of	O
an	O
inﬁnite	O
number	O
of	O
terms	O
,	O
it	O
is	O
still	O
ﬁnite	O
if	O
the	O
reward	O
is	O
nonzero	O
and	O
constant—if	O
γ	O
<	O
1.	O
for	O
example	O
,	O
if	O
the	O
reward	O
is	O
a	O
constant	O
+1	O
,	O
then	O
the	O
return	B
is	O
gt	O
=	O
∞	O
(	O
cid:88	O
)	O
k=0	O
γk	O
=	O
1	O
1	O
−	O
γ	O
.	O
(	O
3.10	O
)	O
exercise	O
3.5	O
the	O
equations	O
in	O
section	O
3.1	O
are	O
for	O
the	O
continuing	O
case	O
and	O
need	O
to	O
be	O
modiﬁed	O
(	O
very	O
slightly	O
)	O
to	O
apply	O
to	O
episodic	O
tasks	O
.	O
show	O
that	O
you	O
know	O
the	O
modiﬁcations	O
(	O
cid:3	O
)	O
needed	O
by	O
giving	O
the	O
modiﬁed	O
version	O
of	O
(	O
3.3	O
)	O
.	O
56	O
chapter	O
3	O
:	O
finite	O
markov	O
decision	O
processes	O
example	O
3.4	O
:	O
pole-balancing	O
the	O
objective	O
in	O
this	O
task	O
is	O
to	O
apply	O
forces	O
to	O
a	O
cart	O
moving	O
along	O
a	O
track	O
so	O
as	O
to	O
keep	O
a	O
pole	O
hinged	O
to	O
the	O
cart	O
from	O
falling	O
over	O
:	O
a	O
failure	O
is	O
said	O
to	O
occur	O
if	O
the	O
pole	O
falls	O
past	O
a	O
given	O
an-	O
gle	O
from	O
vertical	O
or	O
if	O
the	O
cart	O
runs	O
oﬀ	O
the	O
track	O
.	O
the	O
pole	O
is	O
reset	O
to	O
vertical	O
after	O
each	O
failure	O
.	O
this	O
task	O
could	O
be	O
treated	O
as	O
episodic	O
,	O
where	O
the	O
natural	O
episodes	B
are	O
the	O
repeated	O
attempts	O
to	O
balance	O
the	O
pole	O
.	O
the	O
reward	O
in	O
this	O
case	O
could	O
be	O
+1	O
for	O
every	O
time	O
step	O
on	O
which	O
failure	O
did	O
not	O
occur	O
,	O
so	O
that	O
the	O
return	B
at	O
each	O
time	O
would	O
be	O
the	O
number	O
of	O
steps	O
until	O
failure	O
.	O
in	O
this	O
case	O
,	O
successful	O
balancing	O
forever	O
would	O
mean	O
a	O
return	B
of	O
inﬁnity	O
.	O
alternatively	O
,	O
we	O
could	O
treat	O
pole-balancing	O
as	O
a	O
con-	O
tinuing	O
task	O
,	O
using	O
discounting	B
.	O
in	O
this	O
case	O
the	O
reward	O
would	O
be	O
−1	O
on	O
each	O
failure	O
and	O
zero	O
at	O
all	O
other	O
times	O
.	O
the	O
return	B
at	O
each	O
time	O
would	O
then	O
be	O
related	O
to	O
−γk	O
,	O
where	O
k	O
is	O
the	O
number	O
of	O
time	O
steps	O
before	O
failure	O
.	O
in	O
either	O
case	O
,	O
the	O
return	B
is	O
maximized	O
by	O
keeping	O
the	O
pole	O
balanced	O
for	O
as	O
long	O
as	O
possible	O
.	O
exercise	O
3.6	O
suppose	O
you	O
treated	O
pole-balancing	O
as	O
an	O
episodic	O
task	O
but	O
also	O
used	O
dis-	O
counting	O
,	O
with	O
all	O
rewards	O
zero	O
except	O
for	O
−1	O
upon	O
failure	O
.	O
what	O
then	O
would	O
the	O
return	B
be	O
at	O
each	O
time	O
?	O
how	O
does	O
this	O
return	B
diﬀer	O
from	O
that	O
in	O
the	O
discounted	O
,	O
continuing	O
(	O
cid:3	O
)	O
formulation	O
of	O
this	O
task	O
?	O
exercise	O
3.7	O
imagine	O
that	O
you	O
are	O
designing	O
a	O
robot	O
to	O
run	O
a	O
maze	O
.	O
you	O
decide	O
to	O
give	O
it	O
a	O
reward	O
of	O
+1	O
for	O
escaping	O
from	O
the	O
maze	O
and	O
a	O
reward	O
of	O
zero	O
at	O
all	O
other	O
times	O
.	O
the	O
task	O
seems	O
to	O
break	O
down	O
naturally	O
into	O
episodes—the	O
successive	O
runs	O
through	O
the	O
maze—so	O
you	O
decide	O
to	O
treat	O
it	O
as	O
an	O
episodic	O
task	O
,	O
where	O
the	O
goal	B
is	O
to	O
maximize	O
expected	O
total	O
reward	O
(	O
3.7	O
)	O
.	O
after	O
running	O
the	O
learning	O
agent	O
for	O
a	O
while	O
,	O
you	O
ﬁnd	O
that	O
it	O
is	O
showing	O
no	O
improvement	O
in	O
escaping	O
from	O
the	O
maze	O
.	O
what	O
is	O
going	O
wrong	O
?	O
have	O
(	O
cid:3	O
)	O
you	O
eﬀectively	O
communicated	O
to	O
the	O
agent	O
what	O
you	O
want	O
it	O
to	O
achieve	O
?	O
exercise	O
3.8	O
suppose	O
γ	O
=	O
0.5	O
and	O
the	O
following	O
sequence	O
of	O
rewards	O
is	O
received	O
r1	O
=	O
−1	O
,	O
r2	O
=	O
2	O
,	O
r3	O
=	O
6	O
,	O
r4	O
=	O
3	O
,	O
and	O
r5	O
=	O
2	O
,	O
with	O
t	O
=	O
5.	O
what	O
are	O
g0	O
,	O
g1	O
,	O
.	O
.	O
.	O
,	O
g5	O
?	O
hint	O
:	O
(	O
cid:3	O
)	O
work	O
backwards	O
.	O
exercise	O
3.9	O
suppose	O
γ	O
=	O
0.9	O
and	O
the	O
reward	O
sequence	O
is	O
r1	O
=	O
2	O
followed	O
by	O
an	O
inﬁnite	O
(	O
cid:3	O
)	O
sequence	O
of	O
7s	O
.	O
what	O
are	O
g1	O
and	O
g0	O
?	O
(	O
cid:3	O
)	O
exercise	O
3.10	O
prove	O
(	O
3.10	O
)	O
.	O
3.4.	O
uniﬁed	O
notation	O
for	O
episodic	O
and	O
continuing	O
tasks	O
57	O
3.4	O
uniﬁed	O
notation	O
for	O
episodic	O
and	O
continuing	O
tasks	O
in	O
the	O
preceding	O
section	O
we	O
described	O
two	O
kinds	O
of	O
reinforcement	O
learning	O
tasks	O
,	O
one	O
in	O
which	O
the	O
agent–environment	O
interaction	O
naturally	O
breaks	O
down	O
into	O
a	O
sequence	O
of	O
separate	O
episodes	B
(	O
episodic	O
tasks	O
)	O
,	O
and	O
one	O
in	O
which	O
it	O
does	O
not	O
(	O
continuing	B
tasks	I
)	O
.	O
the	O
former	O
case	O
is	O
mathematically	O
easier	O
because	O
each	O
action	B
aﬀects	O
only	O
the	O
ﬁnite	O
number	O
of	O
rewards	O
subsequently	O
received	O
during	O
the	O
episode	O
.	O
in	O
this	O
book	O
we	O
consider	O
sometimes	O
one	O
kind	O
of	O
problem	O
and	O
sometimes	O
the	O
other	O
,	O
but	O
often	O
both	O
.	O
it	O
is	O
therefore	O
useful	O
to	O
establish	O
one	O
notation	O
that	O
enables	O
us	O
to	O
talk	O
precisely	O
about	O
both	O
cases	O
simultaneously	O
.	O
to	O
be	O
precise	O
about	O
episodic	O
tasks	O
requires	O
some	O
additional	O
notation	O
.	O
rather	O
than	O
one	O
long	O
sequence	O
of	O
time	O
steps	O
,	O
we	O
need	O
to	O
consider	O
a	O
series	O
of	O
episodes	O
,	O
each	O
of	O
which	O
consists	O
of	O
a	O
ﬁnite	O
sequence	O
of	O
time	O
steps	O
.	O
we	O
number	O
the	O
time	O
steps	O
of	O
each	O
episode	O
starting	O
anew	O
from	O
zero	O
.	O
therefore	O
,	O
we	O
have	O
to	O
refer	O
not	O
just	O
to	O
st	O
,	O
the	O
state	B
representation	O
at	O
time	O
t	O
,	O
but	O
to	O
st	O
,	O
i	O
,	O
the	O
state	B
representation	O
at	O
time	O
t	O
of	O
episode	O
i	O
(	O
and	O
similarly	O
for	O
at	O
,	O
i	O
,	O
rt	O
,	O
i	O
,	O
πt	O
,	O
i	O
,	O
ti	O
,	O
etc.	O
)	O
.	O
however	O
,	O
it	O
turns	O
out	O
that	O
when	O
we	O
discuss	O
episodic	O
tasks	O
we	O
almost	O
never	O
have	O
to	O
distinguish	O
between	O
diﬀerent	O
episodes	B
.	O
we	O
are	O
almost	O
always	O
considering	O
a	O
particular	O
single	O
episode	O
,	O
or	O
stating	O
something	O
that	O
is	O
true	O
for	O
all	O
episodes	B
.	O
accordingly	O
,	O
in	O
practice	O
we	O
almost	O
always	O
abuse	O
notation	O
slightly	O
by	O
dropping	O
the	O
explicit	O
reference	O
to	O
episode	O
number	O
.	O
that	O
is	O
,	O
we	O
write	O
st	O
to	O
refer	O
to	O
st	O
,	O
i	O
,	O
and	O
so	O
on	O
.	O
we	O
need	O
one	O
other	O
convention	O
to	O
obtain	O
a	O
single	O
notation	O
that	O
covers	O
both	O
episodic	O
and	O
continuing	O
tasks	O
.	O
we	O
have	O
deﬁned	O
the	O
return	B
as	O
a	O
sum	O
over	O
a	O
ﬁnite	O
number	O
of	O
terms	O
in	O
one	O
case	O
(	O
3.7	O
)	O
and	O
as	O
a	O
sum	O
over	O
an	O
inﬁnite	O
number	O
of	O
terms	O
in	O
the	O
other	O
(	O
3.8	O
)	O
.	O
these	O
can	O
be	O
uniﬁed	O
by	O
considering	O
episode	O
termination	O
to	O
be	O
the	O
entering	O
of	O
a	O
special	O
absorbing	B
state	I
that	O
transitions	O
only	O
to	O
itself	O
and	O
that	O
generates	O
only	O
rewards	O
of	O
zero	O
.	O
for	O
example	O
,	O
consider	O
the	O
state	B
transition	O
diagram	O
:	O
here	O
the	O
solid	O
square	O
represents	O
the	O
special	O
absorbing	B
state	I
corresponding	O
to	O
the	O
end	O
of	O
an	O
episode	O
.	O
starting	O
from	O
s0	O
,	O
we	O
get	O
the	O
reward	O
sequence	O
+1	O
,	O
+1	O
,	O
+1	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
.	O
.	O
..	O
summing	O
these	O
,	O
we	O
get	O
the	O
same	O
return	B
whether	O
we	O
sum	O
over	O
the	O
ﬁrst	O
t	O
rewards	O
(	O
here	O
t	O
=	O
3	O
)	O
or	O
over	O
the	O
full	O
inﬁnite	O
sequence	O
.	O
this	O
remains	O
true	O
even	O
if	O
we	O
introduce	O
discounting	B
.	O
thus	O
,	O
we	O
can	O
deﬁne	O
the	O
return	B
,	O
in	O
general	O
,	O
according	O
to	O
(	O
3.8	O
)	O
,	O
using	O
the	O
convention	O
of	O
omitting	O
episode	O
numbers	O
when	O
they	O
are	O
not	O
needed	O
,	O
and	O
including	O
the	O
possibility	O
that	O
γ	O
=	O
1	O
if	O
the	O
sum	O
remains	O
deﬁned	O
(	O
e.g.	O
,	O
because	O
all	O
episodes	B
terminate	O
)	O
.	O
alternatively	O
,	O
we	O
can	O
also	O
write	O
the	O
return	B
as	O
.	O
=	O
gt	O
t	O
(	O
cid:88	O
)	O
k=t+1	O
γk−t−1rk	O
,	O
(	O
3.11	O
)	O
including	O
the	O
possibility	O
that	O
t	O
=	O
∞	O
or	O
γ	O
=	O
1	O
(	O
but	O
not	O
both	O
)	O
.	O
we	O
use	O
these	O
conventions	O
throughout	O
the	O
rest	O
of	O
the	O
book	O
to	O
simplify	O
notation	O
and	O
to	O
express	O
the	O
close	O
paral-	O
lels	O
between	O
episodic	O
and	O
continuing	O
tasks	O
.	O
(	O
later	O
,	O
in	O
chapter	O
10	O
,	O
we	O
will	O
introduce	O
a	O
r1	O
=	O
+1s0s1r2	O
=	O
+1s2r3	O
=	O
+1r4	O
=	O
0r5	O
=	O
0.	O
.	O
.	O
58	O
chapter	O
3	O
:	O
finite	O
markov	O
decision	O
processes	O
formulation	O
that	O
is	O
both	O
continuing	O
and	O
undiscounted	O
.	O
)	O
3.5	O
policies	O
and	O
value	O
functions	O
almost	O
all	O
reinforcement	B
learning	I
algorithms	O
involve	O
estimating	O
value	B
functions—functions	O
of	O
states	O
(	O
or	O
of	O
state–action	O
pairs	O
)	O
that	O
estimate	O
how	O
good	O
it	O
is	O
for	O
the	O
agent	O
to	O
be	O
in	O
a	O
given	O
state	B
(	O
or	O
how	O
good	O
it	O
is	O
to	O
perform	O
a	O
given	O
action	B
in	O
a	O
given	O
state	B
)	O
.	O
the	O
notion	O
of	O
“	O
how	O
good	O
”	O
here	O
is	O
deﬁned	O
in	O
terms	O
of	O
future	O
rewards	O
that	O
can	O
be	O
expected	O
,	O
or	O
,	O
to	O
be	O
precise	O
,	O
in	O
terms	O
of	O
expected	O
return	B
.	O
of	O
course	O
the	O
rewards	O
the	O
agent	O
can	O
expect	O
to	O
receive	O
in	O
the	O
future	O
depend	O
on	O
what	O
actions	O
it	O
will	O
take	O
.	O
accordingly	O
,	O
value	B
functions	O
are	O
deﬁned	O
with	O
respect	O
to	O
particular	O
ways	O
of	O
acting	O
,	O
called	O
policies	O
.	O
formally	O
,	O
a	O
policy	B
is	O
a	O
mapping	O
from	O
states	O
to	O
probabilities	O
of	O
selecting	O
each	O
possible	O
action	B
.	O
if	O
the	O
agent	O
is	O
following	O
policy	B
π	O
at	O
time	O
t	O
,	O
then	O
π	O
(	O
a|s	O
)	O
is	O
the	O
probability	O
that	O
at	O
=	O
a	O
if	O
st	O
=	O
s.	O
like	O
p	O
,	O
π	O
is	O
an	O
ordinary	O
function	O
;	O
the	O
“	O
|	O
”	O
in	O
the	O
middle	O
of	O
π	O
(	O
a|s	O
)	O
merely	O
reminds	O
that	O
it	O
deﬁnes	O
a	O
probability	O
distribution	O
over	O
a	O
∈	O
a	O
(	O
s	O
)	O
for	O
each	O
s	O
∈	O
s.	O
reinforcement	B
learning	I
methods	O
specify	O
how	O
the	O
agent	O
’	O
s	O
policy	B
is	O
changed	O
as	O
a	O
result	O
of	O
its	O
experience	O
.	O
exercise	O
3.11	O
if	O
the	O
current	O
state	B
is	O
st	O
,	O
and	O
actions	O
are	O
selected	O
according	O
to	O
stochastic	O
policy	O
π	O
,	O
then	O
what	O
is	O
the	O
expectation	O
of	O
rt+1	O
in	O
terms	O
of	O
π	O
and	O
the	O
four-argument	O
(	O
cid:3	O
)	O
function	O
p	O
(	O
3.2	O
)	O
?	O
the	O
value	B
function	I
of	O
a	O
state	B
s	O
under	O
a	O
policy	B
π	O
,	O
denoted	O
vπ	O
(	O
s	O
)	O
,	O
is	O
the	O
expected	O
return	O
when	O
starting	O
in	O
s	O
and	O
following	O
π	O
thereafter	O
.	O
for	O
mdps	O
,	O
we	O
can	O
deﬁne	O
vπ	O
formally	O
by	O
vπ	O
(	O
s	O
)	O
.	O
=	O
eπ	O
[	O
gt	O
|	O
st	O
=	O
s	O
]	O
=	O
eπ	O
(	O
cid:34	O
)	O
∞	O
(	O
cid:88	O
)	O
k=0	O
γkrt+k+1	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
st	O
=	O
s	O
(	O
cid:35	O
)	O
,	O
for	O
all	O
s	O
∈	O
s	O
,	O
(	O
3.12	O
)	O
where	O
eπ	O
[	O
·	O
]	O
denotes	O
the	O
expected	O
value	O
of	O
a	O
random	O
variable	O
given	O
that	O
the	O
agent	O
follows	O
policy	B
π	O
,	O
and	O
t	O
is	O
any	O
time	O
step	O
.	O
note	O
that	O
the	O
value	B
of	O
the	O
terminal	O
state	B
,	O
if	O
any	O
,	O
is	O
always	O
zero	O
.	O
we	O
call	O
the	O
function	O
vπ	O
the	O
state-value	O
function	O
for	O
policy	B
π.	O
similarly	O
,	O
we	O
deﬁne	O
the	O
value	B
of	O
taking	O
action	B
a	O
in	O
state	O
s	O
under	O
a	O
policy	B
π	O
,	O
denoted	O
qπ	O
(	O
s	O
,	O
a	O
)	O
,	O
as	O
the	O
expected	O
return	O
starting	O
from	O
s	O
,	O
taking	O
the	O
action	B
a	O
,	O
and	O
thereafter	O
following	O
policy	B
π	O
:	O
qπ	O
(	O
s	O
,	O
a	O
)	O
.	O
=	O
eπ	O
[	O
gt	O
|	O
st	O
=	O
s	O
,	O
at	O
=	O
a	O
]	O
=	O
eπ	O
(	O
cid:34	O
)	O
∞	O
(	O
cid:88	O
)	O
k=0	O
we	O
call	O
qπ	O
the	O
action-value	B
function	I
for	O
policy	B
π.	O
γkrt+k+1	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
st	O
=	O
s	O
,	O
at	O
=	O
a	O
(	O
cid:35	O
)	O
.	O
(	O
3.13	O
)	O
the	O
value	B
functions	O
vπ	O
and	O
qπ	O
can	O
be	O
estimated	O
from	O
experience	O
.	O
for	O
example	O
,	O
if	O
an	O
agent	O
follows	O
policy	B
π	O
and	O
maintains	O
an	O
average	O
,	O
for	O
each	O
state	B
encountered	O
,	O
of	O
the	O
actual	O
returns	O
that	O
have	O
followed	O
that	O
state	B
,	O
then	O
the	O
average	O
will	O
converge	O
to	O
the	O
state	B
’	O
s	O
value	B
,	O
vπ	O
(	O
s	O
)	O
,	O
as	O
the	O
number	O
of	O
times	O
that	O
state	B
is	O
encountered	O
approaches	O
inﬁnity	O
.	O
if	O
separate	O
averages	O
are	O
kept	O
for	O
each	O
action	B
taken	O
in	O
each	O
state	B
,	O
then	O
these	O
averages	O
will	O
similarly	O
converge	O
to	O
the	O
action	B
values	O
,	O
qπ	O
(	O
s	O
,	O
a	O
)	O
.	O
we	O
call	O
estimation	O
methods	O
of	O
this	O
kind	O
monte	O
carlo	O
methods	O
because	O
they	O
involve	O
averaging	O
over	O
many	O
random	O
samples	O
of	O
3.5.	O
uniﬁed	O
notation	O
for	O
episodic	O
and	O
continuing	O
tasks	O
59	O
actual	O
returns	O
.	O
these	O
kinds	O
of	O
methods	O
are	O
presented	O
in	O
chapter	O
5.	O
of	O
course	O
,	O
if	O
there	O
are	O
very	O
many	O
states	O
,	O
then	O
it	O
may	O
not	O
be	O
practical	O
to	O
keep	O
separate	O
averages	O
for	O
each	O
state	B
individually	O
.	O
instead	O
,	O
the	O
agent	O
would	O
have	O
to	O
maintain	O
vπ	O
and	O
qπ	O
as	O
parameter-	O
ized	O
functions	O
(	O
with	O
fewer	O
parameters	O
than	O
states	O
)	O
and	O
adjust	O
the	O
parameters	O
to	O
better	O
match	O
the	O
observed	O
returns	O
.	O
this	O
can	O
also	O
produce	O
accurate	O
estimates	O
,	O
although	O
much	O
depends	O
on	O
the	O
nature	O
of	O
the	O
parameterized	O
function	O
approximator	O
.	O
these	O
possibilities	O
are	O
discussed	O
in	O
part	O
ii	O
of	O
the	O
book	O
.	O
a	O
fundamental	O
property	O
of	O
value	O
functions	O
used	O
throughout	O
reinforcement	B
learning	I
and	O
dynamic	B
programming	I
is	O
that	O
they	O
satisfy	O
recursive	O
relationships	O
similar	O
to	O
that	O
which	O
we	O
have	O
already	O
established	O
for	O
the	O
return	B
(	O
3.9	O
)	O
.	O
for	O
any	O
policy	B
π	O
and	O
any	O
state	B
s	O
,	O
the	O
following	O
consistency	O
condition	O
holds	O
between	O
the	O
value	B
of	O
s	O
and	O
the	O
value	O
of	O
its	O
possible	O
successor	O
states	O
:	O
vπ	O
(	O
s	O
)	O
.	O
=	O
eπ	O
[	O
gt	O
|	O
st	O
=	O
s	O
]	O
=	O
eπ	O
[	O
rt+1	O
+	O
γgt+1	O
|	O
st	O
=	O
s	O
]	O
=	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
a|s	O
)	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
(	O
cid:88	O
)	O
r	O
π	O
(	O
a|s	O
)	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
=	O
(	O
cid:88	O
)	O
a	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:104	O
)	O
r	O
+	O
γeπ	O
[	O
gt+1|st+1	O
=	O
s	O
(	O
cid:48	O
)	O
]	O
(	O
cid:105	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:104	O
)	O
r	O
+	O
γvπ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
,	O
for	O
all	O
s	O
∈	O
s	O
,	O
(	O
by	O
(	O
3.9	O
)	O
)	O
(	O
3.14	O
)	O
where	O
it	O
is	O
implicit	O
that	O
the	O
actions	O
,	O
a	O
,	O
are	O
taken	O
from	O
the	O
set	O
a	O
(	O
s	O
)	O
,	O
that	O
the	O
next	O
states	O
,	O
s	O
(	O
cid:48	O
)	O
,	O
are	O
taken	O
from	O
the	O
set	O
s	O
(	O
or	O
from	O
s+	O
in	O
the	O
case	O
of	O
an	O
episodic	O
problem	O
)	O
,	O
and	O
that	O
the	O
rewards	O
,	O
r	O
,	O
are	O
taken	O
from	O
the	O
set	O
r.	O
note	O
also	O
how	O
in	O
the	O
last	O
equation	O
we	O
have	O
merged	O
the	O
two	O
sums	O
,	O
one	O
over	O
all	O
the	O
values	O
of	O
s	O
(	O
cid:48	O
)	O
and	O
the	O
other	O
over	O
all	O
the	O
values	O
of	O
r	O
,	O
into	O
one	O
sum	O
over	O
all	O
the	O
possible	O
values	O
of	O
both	O
.	O
we	O
use	O
this	O
kind	O
of	O
merged	O
sum	O
often	O
to	O
simplify	O
formulas	O
.	O
note	O
how	O
the	O
ﬁnal	O
expression	O
can	O
be	O
read	O
easily	O
as	O
an	O
expected	O
value	O
.	O
it	O
is	O
really	O
a	O
sum	O
over	O
all	O
values	O
of	O
the	O
three	O
variables	O
,	O
a	O
,	O
s	O
(	O
cid:48	O
)	O
,	O
and	O
r.	O
for	O
each	O
triple	O
,	O
we	O
compute	O
its	O
probability	O
,	O
π	O
(	O
a|s	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
,	O
weight	O
the	O
quantity	O
in	O
brackets	O
by	O
that	O
probability	O
,	O
then	O
sum	O
over	O
all	O
possibilities	O
to	O
get	O
an	O
expected	O
value	O
.	O
equation	O
(	O
3.14	O
)	O
is	O
the	O
bellman	O
equation	O
for	B
vπ	I
.	O
it	O
expresses	O
a	O
relationship	O
between	O
the	O
value	B
of	O
a	O
state	B
and	O
the	O
values	O
of	O
its	O
successor	O
states	O
.	O
think	O
of	O
looking	O
ahead	O
from	O
a	O
state	B
to	O
its	O
possible	O
successor	O
states	O
,	O
as	O
suggested	O
by	O
the	O
diagram	O
to	O
the	O
right	O
.	O
each	O
open	O
circle	O
represents	O
a	O
state	B
and	O
each	O
solid	O
circle	O
represents	O
a	O
state–action	O
pair	O
.	O
starting	O
from	O
state	B
s	O
,	O
the	O
root	O
node	O
at	O
the	O
top	O
,	O
the	O
agent	O
could	O
take	O
any	O
of	O
some	O
set	O
of	O
actions—	O
three	O
are	O
shown	O
in	O
the	O
diagram—based	O
on	O
its	O
policy	B
π.	O
from	O
each	O
of	O
these	O
,	O
the	O
environment	B
could	O
respond	O
with	O
one	O
of	O
several	O
next	O
states	O
,	O
s	O
(	O
cid:48	O
)	O
(	O
two	O
are	O
shown	O
in	O
the	O
ﬁgure	O
)	O
,	O
along	O
with	O
a	O
reward	O
,	O
r	O
,	O
depending	O
on	O
its	O
dynamics	O
given	O
by	O
the	O
function	O
p.	O
the	O
bellman	O
equation	O
(	O
3.14	O
)	O
averages	O
over	O
all	O
the	O
possibilities	O
,	O
weighting	O
each	O
by	O
its	O
probability	O
of	O
occurring	O
.	O
it	O
states	O
that	O
the	O
value	B
of	O
the	O
start	O
state	B
must	O
equal	O
the	O
(	O
discounted	O
)	O
value	B
of	O
the	O
expected	O
next	O
state	B
,	O
plus	O
the	O
reward	O
expected	O
along	O
the	O
way	O
.	O
backup	B
diagram	I
for	O
vπ	O
the	O
value	B
function	I
vπ	O
is	O
the	O
unique	O
solution	O
to	O
its	O
bellman	O
equation	O
.	O
we	O
show	O
in	O
⇡ss0⇡rpa	O
60	O
chapter	O
3	O
:	O
finite	O
markov	O
decision	O
processes	O
subsequent	O
chapters	O
how	O
this	O
bellman	O
equation	O
forms	O
the	O
basis	O
of	O
a	O
number	O
of	O
ways	O
to	O
compute	O
,	O
approximate	B
,	O
and	O
learn	O
vπ	O
.	O
we	O
call	O
diagrams	O
like	O
that	O
above	O
backup	O
diagrams	O
because	O
they	O
diagram	O
relationships	O
that	O
form	O
the	O
basis	O
of	O
the	O
update	O
or	O
backup	O
operations	O
that	O
are	O
at	O
the	O
heart	O
of	O
reinforcement	O
learning	O
methods	O
.	O
these	O
operations	O
transfer	O
value	B
information	O
back	O
to	O
a	O
state	B
(	O
or	O
a	O
state–action	O
pair	O
)	O
from	O
its	O
successor	O
states	O
(	O
or	O
state–action	O
pairs	O
)	O
.	O
we	O
use	O
backup	O
diagrams	O
throughout	O
the	O
book	O
to	O
provide	O
graphical	O
summaries	O
of	O
the	O
algorithms	O
we	O
discuss	O
.	O
(	O
note	O
that	O
,	O
unlike	O
transition	O
graphs	O
,	O
the	O
state	B
nodes	O
of	O
backup	O
diagrams	O
do	O
not	O
necessarily	O
represent	O
distinct	O
states	O
;	O
for	O
example	O
,	O
a	O
state	B
might	O
be	O
its	O
own	O
successor	O
.	O
)	O
example	O
3.5	O
:	O
gridworld	O
figure	O
3.2	O
(	O
left	O
)	O
shows	O
a	O
rectangular	O
gridworld	O
represen-	O
tation	O
of	O
a	O
simple	O
ﬁnite	O
mdp	O
.	O
the	O
cells	O
of	O
the	O
grid	O
correspond	O
to	O
the	O
states	O
of	O
the	O
environment	B
.	O
at	O
each	O
cell	O
,	O
four	O
actions	O
are	O
possible	O
:	O
north	O
,	O
south	O
,	O
east	O
,	O
and	O
west	O
,	O
which	O
deterministically	O
cause	O
the	O
agent	O
to	O
move	O
one	O
cell	O
in	O
the	O
respective	O
direction	O
on	O
the	O
grid	O
.	O
actions	O
that	O
would	O
take	O
the	O
agent	O
oﬀ	O
the	O
grid	O
leave	O
its	O
location	O
unchanged	O
,	O
but	O
also	O
result	O
in	O
a	O
reward	O
of	O
−1	O
.	O
other	O
actions	O
result	O
in	O
a	O
reward	O
of	O
0	O
,	O
except	O
those	O
that	O
move	O
the	O
agent	O
out	O
of	O
the	O
special	O
states	O
a	O
and	O
b.	O
from	O
state	B
a	O
,	O
all	O
four	O
actions	O
yield	O
a	O
reward	O
of	O
+10	O
and	O
take	O
the	O
agent	O
to	O
a	O
(	O
cid:48	O
)	O
.	O
from	O
state	B
b	O
,	O
all	O
actions	O
yield	O
a	O
reward	O
of	O
+5	O
and	O
take	O
the	O
agent	O
to	O
b	O
(	O
cid:48	O
)	O
.	O
figure	O
3.2	O
:	O
gridworld	O
example	O
:	O
exceptional	O
reward	O
dynamics	O
(	O
left	O
)	O
and	O
state-value	O
function	O
for	O
the	O
equiprobable	O
random	O
policy	O
(	O
right	O
)	O
.	O
suppose	O
the	O
agent	O
selects	O
all	O
four	O
actions	O
with	O
equal	O
probability	O
in	O
all	O
states	O
.	O
figure	O
3.2	O
(	O
right	O
)	O
shows	O
the	O
value	B
function	I
,	O
vπ	O
,	O
for	O
this	O
policy	B
,	O
for	O
the	O
discounted	O
reward	O
case	O
with	O
γ	O
=	O
0.9.	O
this	O
value	B
function	I
was	O
computed	O
by	O
solving	O
the	O
system	O
of	O
linear	O
equations	O
(	O
3.14	O
)	O
.	O
notice	O
the	O
negative	O
values	O
near	O
the	O
lower	O
edge	O
;	O
these	O
are	O
the	O
result	O
of	O
the	O
high	O
probability	O
of	O
hitting	O
the	O
edge	O
of	O
the	O
grid	O
there	O
under	O
the	O
random	O
policy	O
.	O
state	B
a	O
is	O
the	O
best	O
state	B
to	O
be	O
in	O
under	O
this	O
policy	B
,	O
but	O
its	O
expected	O
return	O
is	O
less	O
than	O
10	O
,	O
its	O
immediate	O
reward	O
,	O
because	O
from	O
a	O
the	O
agent	O
is	O
taken	O
to	O
a	O
(	O
cid:48	O
)	O
,	O
from	O
which	O
it	O
is	O
likely	O
to	O
run	O
into	O
the	O
edge	O
of	O
the	O
grid	O
.	O
state	B
b	O
,	O
on	O
the	O
other	O
hand	O
,	O
is	O
valued	O
more	O
than	O
5	O
,	O
its	O
immediate	O
reward	O
,	O
because	O
from	O
b	O
the	O
agent	O
is	O
taken	O
to	O
b	O
(	O
cid:48	O
)	O
,	O
which	O
has	O
a	O
positive	O
value	B
.	O
from	O
b	O
(	O
cid:48	O
)	O
the	O
expected	O
penalty	O
(	O
negative	O
reward	O
)	O
for	O
possibly	O
running	O
into	O
an	O
edge	O
is	O
more	O
than	O
compensated	O
for	O
by	O
the	O
expected	O
gain	O
for	O
possibly	O
stumbling	O
onto	O
a	O
or	O
b.	O
exercise	O
3.12	O
the	O
bellman	O
equation	O
(	O
3.14	O
)	O
must	O
hold	O
for	O
each	O
state	B
for	O
the	O
value	B
function	I
vπ	O
shown	O
in	O
figure	O
3.2	O
(	O
right	O
)	O
of	O
example	O
3.5.	O
show	O
numerically	O
that	O
this	O
equation	O
holds	O
for	O
the	O
center	O
state	B
,	O
valued	O
at	O
+0.7	O
,	O
with	O
respect	O
to	O
its	O
four	O
neighboring	O
states	O
,	O
valued	O
at	O
+2.3	O
,	O
+0.4	O
,	O
−0.4	O
,	O
and	O
+0.7	O
.	O
(	O
these	O
numbers	O
are	O
accurate	O
only	O
to	O
one	O
decimal	O
place	O
.	O
)	O
3.7.valuefunctions63s	O
,	O
asas'ra's'r	O
(	O
b	O
)	O
(	O
a	O
)	O
figure3.4	O
:	O
backupdiagramsfor	O
(	O
a	O
)	O
v⇡and	O
(	O
b	O
)	O
q⇡.thestatesoftheenvironment.ateachcell	O
,	O
fouractionsarepossible	O
:	O
north	O
,	O
south	O
,	O
east	O
,	O
andwest	O
,	O
whichdeterministicallycausetheagenttomoveonecellintherespectivedirectiononthegrid.actionsthatwouldtaketheagento↵thegridleaveitslocationunchanged	O
,	O
butalsoresultinarewardof 1.otheractionsresultinarewardof0	O
,	O
exceptthosethatmovetheagentoutofthespecialstatesaandb.fromstatea	O
,	O
allfouractionsyieldarewardof+10andtaketheagenttoa0.fromstateb	O
,	O
allactionsyieldarewardof+5andtaketheagenttob0.supposetheagentselectsallfouractionswithequalprobabilityinallstates.figure3.5bshowsthevaluefunction	O
,	O
v⇡	O
,	O
forthispolicy	O
,	O
forthedis-countedrewardcasewith =0.9.thisvaluefunctionwascomputedbysolv-ingthesystemofequations	O
(	O
3.10	O
)	O
.noticethenegativevaluesneartheloweredge	O
;	O
thesearetheresultofthehighprobabilityofhittingtheedgeofthegridthereundertherandompolicy.stateaisthebeststatetobeinunderthispol-icy	O
,	O
butitsexpectedreturnislessthan10	O
,	O
itsimmediatereward	O
,	O
becausefromatheagentistakentoa0	O
,	O
fromwhichitislikelytorunintotheedgeofthegrid.stateb	O
,	O
ontheotherhand	O
,	O
isvaluedmorethan5	O
,	O
itsimmediatereward	O
,	O
becausefrombtheagentistakentob0	O
,	O
whichhasapositivevalue.fromb0theexpectedpenalty	O
(	O
negativereward	O
)	O
forpossiblyrunningintoanedgeismore3.38.84.45.31.51.53.02.31.90.50.10.70.70.4-0.4-1.0-0.4-0.4-0.6-1.2-1.9-1.3-1.2-1.4-2.0aba'b'+10+5actions	O
(	O
a	O
)	O
(	O
b	O
)	O
figure3.5	O
:	O
gridexample	O
:	O
(	O
a	O
)	O
exceptionalrewarddynamics	O
;	O
(	O
b	O
)	O
state-valuefunctionfortheequiprobablerandompolicy	O
.	O
3.5.	O
uniﬁed	O
notation	O
for	O
episodic	O
and	O
continuing	O
tasks	O
61	O
(	O
cid:3	O
)	O
exercise	O
3.13	O
what	O
is	O
the	O
bellman	O
equation	O
for	B
action	I
values	I
,	O
that	O
is	O
,	O
for	B
qπ	I
?	O
it	O
must	O
give	O
the	O
action	B
value	O
qπ	O
(	O
s	O
,	O
a	O
)	O
in	O
terms	O
of	O
the	O
action	B
values	O
,	O
qπ	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
(	O
cid:48	O
)	O
)	O
,	O
of	O
possible	O
successors	O
to	O
the	O
state–action	O
pair	O
(	O
s	O
,	O
a	O
)	O
.	O
hint	O
:	O
the	O
backup	B
diagram	I
to	O
the	O
right	O
corresponds	O
to	O
this	O
equation	O
.	O
show	O
the	O
sequence	O
of	O
equations	O
analogous	O
to	O
(	O
3.14	O
)	O
,	O
but	O
for	O
action	O
(	O
cid:3	O
)	O
values	O
.	O
example	O
3.6	O
:	O
golf	O
to	O
formulate	O
playing	O
a	O
hole	O
of	O
golf	O
as	O
a	O
reinforcement	B
learning	I
task	O
,	O
we	O
count	O
a	O
penalty	O
(	O
nega-	O
tive	O
reward	O
)	O
of	O
−1	O
for	O
each	O
stroke	O
until	O
we	O
hit	O
the	O
ball	O
into	O
the	O
hole	O
.	O
the	O
state	B
is	O
the	O
location	O
of	O
the	O
ball	O
.	O
the	O
value	B
of	O
a	O
state	B
is	O
the	O
negative	O
of	O
the	O
number	O
of	O
strokes	O
to	O
the	O
hole	O
from	O
that	O
location	O
.	O
our	O
actions	O
are	O
how	O
we	O
aim	O
and	O
swing	O
at	O
the	O
ball	O
,	O
of	O
course	O
,	O
and	O
which	O
club	O
we	O
se-	O
lect	O
.	O
let	O
us	O
take	O
the	O
former	O
as	O
given	O
and	O
consider	O
just	O
the	O
choice	O
of	O
club	O
,	O
which	O
we	O
assume	O
is	O
either	O
a	O
putter	O
or	O
a	O
driver	O
.	O
the	O
upper	O
part	O
of	O
figure	O
3.3	O
shows	O
a	O
possi-	O
ble	O
state-value	O
function	O
,	O
vputt	O
(	O
s	O
)	O
,	O
for	O
the	O
policy	B
that	O
always	O
uses	O
the	O
putter	O
.	O
the	O
terminal	O
state	B
in-the-hole	O
has	O
a	O
value	B
of	O
0.	O
from	O
anywhere	O
on	O
the	O
green	O
we	O
as-	O
sume	O
we	O
can	O
make	O
a	O
putt	O
;	O
these	O
states	O
have	O
value	B
−1	O
.	O
oﬀ	O
the	O
green	O
we	O
can-	O
not	O
reach	O
the	O
hole	O
by	O
putting	O
,	O
and	O
the	O
value	O
is	O
greater	O
.	O
if	O
we	O
can	O
reach	O
the	O
green	O
from	O
a	O
state	B
by	O
putting	O
,	O
then	O
that	O
state	B
must	O
have	O
value	B
one	O
less	O
than	O
the	O
green	O
’	O
s	O
value	B
,	O
that	O
is	O
,	O
−2	O
.	O
for	O
simplicity	O
,	O
let	O
us	O
assume	O
we	O
can	O
putt	O
very	O
precisely	O
and	O
deter-	O
ministically	O
,	O
but	O
with	O
a	O
limited	O
range	O
.	O
this	O
gives	O
us	O
the	O
sharp	O
contour	O
line	O
labeled	O
−2	O
in	O
the	O
ﬁgure	O
;	O
all	O
locations	O
between	O
that	O
line	O
and	O
the	O
green	O
require	O
exactly	O
two	O
strokes	O
to	O
complete	O
the	O
hole	O
.	O
similarly	O
,	O
any	O
location	O
within	O
putting	O
range	O
of	O
the	O
−2	O
contour	O
line	O
must	O
have	O
a	O
value	B
of	O
−3	O
,	O
and	O
so	O
on	O
to	O
get	O
all	O
the	O
contour	O
lines	O
shown	O
in	O
the	O
ﬁgure	O
.	O
putting	O
doesn	O
’	O
t	O
get	O
us	O
out	O
of	O
sand	O
traps	O
,	O
so	O
they	O
have	O
a	O
value	B
of	O
−∞	O
.	O
overall	O
,	O
it	O
takes	O
us	O
six	O
strokes	O
to	O
get	O
from	O
the	O
tee	O
to	O
the	O
hole	O
by	O
putting	O
.	O
figure	O
3.3	O
:	O
a	O
golf	B
example	I
:	O
the	O
state-value	O
function	O
for	O
putting	O
(	O
upper	O
)	O
and	O
the	O
optimal	O
action-value	B
function	I
for	O
using	O
the	O
driver	O
(	O
lower	O
)	O
.	O
exercise	O
3.14	O
in	O
the	O
gridworld	O
example	O
,	O
rewards	O
are	O
positive	O
for	O
goals	O
,	O
negative	O
for	O
running	O
into	O
the	O
edge	O
of	O
the	O
world	O
,	O
and	O
zero	O
the	O
rest	O
of	O
the	O
time	O
.	O
are	O
the	O
signs	O
of	O
these	O
rewards	O
important	O
,	O
or	O
only	O
the	O
intervals	O
between	O
them	O
?	O
prove	O
,	O
using	O
(	O
3.8	O
)	O
,	O
that	O
adding	O
a	O
constant	O
c	O
to	O
all	O
the	O
rewards	O
adds	O
a	O
constant	O
,	O
vc	O
,	O
to	O
the	O
values	O
of	O
all	O
states	O
,	O
and	O
thus	O
does	O
not	O
aﬀect	O
the	O
relative	O
values	O
of	O
any	O
states	O
under	O
any	O
policies	O
.	O
what	O
is	O
vc	O
in	O
terms	O
rs0s	O
,	O
aa0⇡pq⇡backupdiagramq*	O
(	O
s	O
,	O
driver	O
)	O
vputtsandgreen	O
!	O
1sand	O
!	O
2	O
!	O
2	O
!	O
3	O
!	O
4	O
!	O
1	O
!	O
5	O
!	O
6	O
!	O
4	O
!	O
3	O
!	O
3	O
!	O
2	O
!	O
4sandgreen	O
!	O
1sand	O
!	O
2	O
!	O
3	O
!	O
200	O
!	O
''	O
!	O
``	O
vputtq⇤	O
(	O
s	O
,	O
driver	O
)	O
62	O
chapter	O
3	O
:	O
finite	O
markov	O
decision	O
processes	O
of	O
c	O
and	O
γ	O
?	O
(	O
cid:3	O
)	O
exercise	O
3.15	O
now	O
consider	O
adding	O
a	O
constant	O
c	O
to	O
all	O
the	O
rewards	O
in	O
an	O
episodic	O
task	O
,	O
such	O
as	O
maze	O
running	O
.	O
would	O
this	O
have	O
any	O
eﬀect	O
,	O
or	O
would	O
it	O
leave	O
the	O
task	O
unchanged	O
(	O
cid:3	O
)	O
as	O
in	O
the	O
continuing	O
task	O
above	O
?	O
why	O
or	O
why	O
not	O
?	O
give	O
an	O
example	O
.	O
exercise	O
3.16	O
the	O
value	B
of	O
a	O
state	B
depends	O
on	O
the	O
values	O
of	O
the	O
actions	O
possible	O
in	O
that	O
state	B
and	O
on	O
how	O
likely	O
each	O
action	B
is	O
to	O
be	O
taken	O
under	O
the	O
current	O
policy	B
.	O
we	O
can	O
think	O
of	O
this	O
in	O
terms	O
of	O
a	O
small	O
backup	B
diagram	I
rooted	O
at	O
the	O
state	B
and	O
considering	O
each	O
possible	O
action	B
:	O
give	O
the	O
equation	O
corresponding	O
to	O
this	O
intuition	O
and	O
diagram	O
for	O
the	O
value	B
at	O
the	O
root	O
node	O
,	O
vπ	O
(	O
s	O
)	O
,	O
in	O
terms	O
of	O
the	O
value	B
at	O
the	O
expected	O
leaf	O
node	O
,	O
qπ	O
(	O
s	O
,	O
a	O
)	O
,	O
given	O
st	O
=	O
s.	O
this	O
equation	O
should	O
include	O
an	O
expectation	O
conditioned	O
on	O
following	O
the	O
policy	B
,	O
π.	O
then	O
give	O
a	O
second	O
equation	O
in	O
which	O
the	O
expected	O
value	O
is	O
written	O
out	O
explicitly	O
in	O
terms	O
of	O
π	O
(	O
a|s	O
)	O
(	O
cid:3	O
)	O
such	O
that	O
no	O
expected	O
value	O
notation	O
appears	O
in	O
the	O
equation	O
.	O
exercise	O
3.17	O
the	O
value	B
of	O
an	O
action	B
,	O
qπ	O
(	O
s	O
,	O
a	O
)	O
,	O
depends	O
on	O
the	O
expected	O
next	O
reward	O
and	O
the	O
expected	O
sum	O
of	O
the	O
remaining	O
rewards	O
.	O
again	O
we	O
can	O
think	O
of	O
this	O
in	O
terms	O
of	O
a	O
small	O
backup	B
diagram	I
,	O
this	O
one	O
rooted	O
at	O
an	O
action	B
(	O
state–action	O
pair	O
)	O
and	O
branching	O
to	O
the	O
possible	O
next	O
states	O
:	O
give	O
the	O
equation	O
corresponding	O
to	O
this	O
intuition	O
and	O
diagram	O
for	O
the	O
action	B
value	O
,	O
qπ	O
(	O
s	O
,	O
a	O
)	O
,	O
in	O
terms	O
of	O
the	O
expected	O
next	O
reward	O
,	O
rt+1	O
,	O
and	O
the	O
expected	O
next	O
state	B
value	O
,	O
vπ	O
(	O
st+1	O
)	O
,	O
given	O
that	O
st	O
=	O
s	O
and	O
at	O
=	O
a.	O
this	O
equation	O
should	O
include	O
an	O
expectation	O
but	O
not	O
one	O
conditioned	O
on	O
following	O
the	O
policy	B
.	O
then	O
give	O
a	O
second	O
equation	O
,	O
writing	O
out	O
the	O
expected	O
value	O
explicitly	O
in	O
terms	O
of	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
deﬁned	O
by	O
(	O
3.2	O
)	O
,	O
such	O
that	O
no	O
(	O
cid:3	O
)	O
expected	O
value	O
notation	O
appears	O
in	O
the	O
equation	O
.	O
3.6	O
optimal	O
policies	O
and	O
optimal	O
value	B
functions	O
solving	O
a	O
reinforcement	B
learning	I
task	O
means	O
,	O
roughly	O
,	O
ﬁnding	O
a	O
policy	B
that	O
achieves	O
a	O
lot	O
of	O
reward	O
over	O
the	O
long	O
run	O
.	O
for	O
ﬁnite	O
mdps	O
,	O
we	O
can	O
precisely	O
deﬁne	O
an	O
optimal	O
policy	O
in	O
the	O
following	O
way	O
.	O
value	B
functions	O
deﬁne	O
a	O
partial	O
ordering	O
over	O
policies	O
.	O
a	O
policy	B
π	O
is	O
deﬁned	O
to	O
be	O
better	O
than	O
or	O
equal	O
to	O
a	O
policy	B
π	O
(	O
cid:48	O
)	O
if	O
its	O
expected	O
return	O
is	O
greater	O
than	O
or	O
equal	O
to	O
that	O
of	O
π	O
(	O
cid:48	O
)	O
for	O
all	O
states	O
.	O
in	O
other	O
words	O
,	O
π	O
≥	O
π	O
(	O
cid:48	O
)	O
if	O
and	O
only	O
if	O
vπ	O
(	O
s	O
)	O
≥	O
vπ	O
(	O
cid:48	O
)	O
(	O
s	O
)	O
for	O
all	O
s	O
∈	O
s.	O
there	O
is	O
always	O
at	O
least	O
one	O
policy	B
that	O
is	O
better	O
than	O
or	O
equal	O
to	O
all	O
other	O
policies	O
.	O
this	O
is	O
an	O
optimal	O
policy	O
.	O
although	O
there	O
may	O
be	O
more	O
than	O
one	O
,	O
we	O
denote	O
all	O
the	O
optimal	O
policies	O
by	O
π∗	O
.	O
they	O
share	O
the	O
same	O
state-value	O
function	O
,	O
called	O
the	O
optimal	O
stakenwithprobability⇡	O
(	O
a|s	O
)	O
v⇡	O
(	O
s	O
)	O
q⇡	O
(	O
s	O
,	O
a	O
)	O
a1a2a3s	O
,	O
aq⇡	O
(	O
s	O
,	O
a	O
)	O
s03s02s01r1r2r3expectedrewardsv⇡	O
(	O
s0	O
)	O
3.6.	O
optimal	O
policies	O
and	O
optimal	O
value	B
functions	O
state-value	O
function	O
,	O
denoted	O
v∗	O
,	O
and	O
deﬁned	O
as	O
.	O
=	O
max	O
π	O
vπ	O
(	O
s	O
)	O
,	O
v∗	O
(	O
s	O
)	O
for	O
all	O
s	O
∈	O
s.	O
deﬁned	O
as	O
63	O
(	O
3.15	O
)	O
optimal	O
policies	O
also	O
share	O
the	O
same	O
optimal	O
action-value	O
function	O
,	O
denoted	O
q∗	O
,	O
and	O
q∗	O
(	O
s	O
,	O
a	O
)	O
.	O
=	O
max	O
π	O
qπ	O
(	O
s	O
,	O
a	O
)	O
,	O
(	O
3.16	O
)	O
for	O
all	O
s	O
∈	O
s	O
and	O
a	O
∈	O
a	O
(	O
s	O
)	O
.	O
for	O
the	O
state–action	O
pair	O
(	O
s	O
,	O
a	O
)	O
,	O
this	O
function	O
gives	O
the	O
expected	O
return	O
for	O
taking	O
action	B
a	O
in	O
state	O
s	O
and	O
thereafter	O
following	O
an	O
optimal	O
policy	O
.	O
thus	O
,	O
we	O
can	O
write	O
q∗	O
in	O
terms	O
of	O
v∗	O
as	O
follows	O
:	O
q∗	O
(	O
s	O
,	O
a	O
)	O
=	O
e	O
[	O
rt+1	O
+	O
γv∗	O
(	O
st+1	O
)	O
|	O
st	O
=	O
s	O
,	O
at	O
=	O
a	O
]	O
.	O
(	O
3.17	O
)	O
example	O
3.7	O
:	O
optimal	O
value	O
functions	O
for	O
golf	O
the	O
lower	O
part	O
of	O
figure	O
3.3	O
shows	O
the	O
contours	O
of	O
a	O
possible	O
optimal	O
action-value	O
function	O
q∗	O
(	O
s	O
,	O
driver	O
)	O
.	O
these	O
are	O
the	O
values	O
of	O
each	O
state	B
if	O
we	O
ﬁrst	O
play	O
a	O
stroke	O
with	O
the	O
driver	O
and	O
afterward	O
select	O
either	O
the	O
driver	O
or	O
the	O
putter	O
,	O
whichever	O
is	O
better	O
.	O
the	O
driver	O
enables	O
us	O
to	O
hit	O
the	O
ball	O
farther	O
,	O
but	O
with	O
less	O
accuracy	O
.	O
we	O
can	O
reach	O
the	O
hole	O
in	O
one	O
shot	O
using	O
the	O
driver	O
only	O
if	O
we	O
are	O
already	O
very	O
close	O
;	O
thus	O
the	O
−1	O
contour	O
for	O
q∗	O
(	O
s	O
,	O
driver	O
)	O
covers	O
only	O
a	O
small	O
portion	O
of	O
the	O
green	O
.	O
if	O
we	O
have	O
two	O
strokes	O
,	O
however	O
,	O
then	O
we	O
can	O
reach	O
the	O
hole	O
from	O
much	O
farther	O
away	O
,	O
as	O
shown	O
by	O
the	O
−2	O
contour	O
.	O
in	O
this	O
case	O
we	O
don	O
’	O
t	O
have	O
to	O
drive	O
all	O
the	O
way	O
to	O
within	O
the	O
small	O
−1	O
contour	O
,	O
but	O
only	O
to	O
anywhere	O
on	O
the	O
green	O
;	O
from	O
there	O
we	O
can	O
use	O
the	O
putter	O
.	O
the	O
optimal	O
action-value	O
function	O
gives	O
the	O
values	O
after	O
committing	O
to	O
a	O
particular	O
ﬁrst	O
action	B
,	O
in	O
this	O
case	O
,	O
to	O
the	O
driver	O
,	O
but	O
afterward	O
using	O
whichever	O
actions	O
are	O
best	O
.	O
the	O
−3	O
contour	O
is	O
still	O
farther	O
out	O
and	O
includes	O
the	O
starting	O
tee	O
.	O
from	O
the	O
tee	O
,	O
the	O
best	O
sequence	O
of	O
actions	O
is	O
two	O
drives	O
and	O
one	O
putt	O
,	O
sinking	O
the	O
ball	O
in	O
three	O
strokes	O
.	O
because	O
v∗	O
is	O
the	O
value	B
function	I
for	O
a	O
policy	B
,	O
it	O
must	O
satisfy	O
the	O
self-consistency	O
con-	O
dition	O
given	O
by	O
the	O
bellman	O
equation	O
for	O
state	O
values	O
(	O
3.14	O
)	O
.	O
because	O
it	O
is	O
the	O
optimal	O
value	O
function	O
,	O
however	O
,	O
v∗	O
’	O
s	O
consistency	O
condition	O
can	O
be	O
written	O
in	O
a	O
special	O
form	O
with-	O
out	O
reference	O
to	O
any	O
speciﬁc	O
policy	B
.	O
this	O
is	O
the	O
bellman	O
equation	O
for	O
v∗	O
,	O
or	O
the	O
bellman	O
optimality	O
equation	O
.	O
intuitively	O
,	O
the	O
bellman	O
optimality	O
equation	O
expresses	O
the	O
fact	O
that	O
the	O
value	B
of	O
a	O
state	B
under	O
an	O
optimal	O
policy	O
must	O
equal	O
the	O
expected	O
return	O
for	O
the	O
best	O
action	B
from	O
that	O
state	B
:	O
v∗	O
(	O
s	O
)	O
=	O
max	O
a∈a	O
(	O
s	O
)	O
=	O
max	O
a	O
a	O
(	O
s	O
,	O
a	O
)	O
qπ∗	O
eπ∗	O
[	O
gt	O
|	O
st	O
=	O
s	O
,	O
at	O
=	O
a	O
]	O
eπ∗	O
[	O
rt+1	O
+	O
γgt+1	O
|	O
st	O
=	O
s	O
,	O
at	O
=	O
a	O
]	O
e	O
[	O
rt+1	O
+	O
γv∗	O
(	O
st+1	O
)	O
|	O
st	O
=	O
s	O
,	O
at	O
=	O
a	O
]	O
a	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:2	O
)	O
r	O
+	O
γv∗	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:3	O
)	O
.	O
a	O
=	O
max	O
=	O
max	O
=	O
max	O
(	O
by	O
(	O
3.9	O
)	O
)	O
(	O
3.18	O
)	O
(	O
3.19	O
)	O
64	O
chapter	O
3	O
:	O
finite	O
markov	O
decision	O
processes	O
the	O
last	O
two	O
equations	O
are	O
two	O
forms	O
of	O
the	O
bellman	O
optimality	O
equation	O
for	O
v∗	O
.	O
the	O
bellman	O
optimality	O
equation	O
for	O
q∗	O
is	O
q∗	O
(	O
s	O
,	O
a	O
)	O
=	O
e	O
(	O
cid:104	O
)	O
rt+1	O
+	O
γ	O
max	O
a	O
(	O
cid:48	O
)	O
=	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
q∗	O
(	O
st+1	O
,	O
a	O
(	O
cid:48	O
)	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
st	O
=	O
s	O
,	O
at	O
=	O
a	O
(	O
cid:105	O
)	O
q∗	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
.	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:104	O
)	O
r	O
+	O
γ	O
max	O
a	O
(	O
cid:48	O
)	O
(	O
3.20	O
)	O
the	O
backup	O
diagrams	O
in	O
figure	O
3.4	O
show	O
graphically	O
the	O
spans	O
of	O
future	O
states	O
and	O
actions	O
considered	O
in	O
the	O
bellman	O
optimality	O
equations	O
for	O
v∗	O
and	O
q∗	O
.	O
these	O
are	O
the	O
same	O
as	O
the	O
backup	O
diagrams	O
for	B
vπ	I
and	O
qπ	O
presented	O
earlier	O
except	O
that	O
arcs	O
have	O
been	O
added	O
at	O
the	O
agent	O
’	O
s	O
choice	O
points	O
to	O
represent	O
that	O
the	O
maximum	O
over	O
that	O
choice	O
is	O
taken	O
rather	O
than	O
the	O
expected	O
value	O
given	O
some	O
policy	B
.	O
the	O
backup	B
diagram	I
on	O
the	O
left	O
graphically	O
represents	O
the	O
bellman	O
optimality	O
equation	O
(	O
3.19	O
)	O
and	O
the	O
backup	O
diagram	O
on	O
the	O
right	O
graphically	O
represents	O
(	O
3.20	O
)	O
.	O
figure	O
3.4	O
:	O
backup	O
diagrams	O
for	O
v∗	O
and	O
q∗	O
for	O
ﬁnite	O
mdps	O
,	O
the	O
bellman	O
optimality	O
equation	O
for	B
vπ	I
(	O
3.19	O
)	O
has	O
a	O
unique	O
solution	O
independent	O
of	O
the	O
policy	B
.	O
the	O
bellman	O
optimality	O
equation	O
is	O
actually	O
a	O
system	O
of	O
equations	O
,	O
one	O
for	O
each	O
state	B
,	O
so	O
if	O
there	O
are	O
n	O
states	O
,	O
then	O
there	O
are	O
n	O
equations	O
in	O
n	O
unknowns	O
.	O
if	O
the	O
dynamics	O
p	O
of	O
the	O
environment	B
are	O
known	O
,	O
then	O
in	O
principle	O
one	O
can	O
solve	O
this	O
system	O
of	O
equations	O
for	O
v∗	O
using	O
any	O
one	O
of	O
a	O
variety	O
of	O
methods	O
for	O
solving	O
systems	O
of	O
nonlinear	O
equations	O
.	O
one	O
can	O
solve	O
a	O
related	O
set	O
of	O
equations	O
for	O
q∗	O
.	O
once	O
one	O
has	O
v∗	O
,	O
it	O
is	O
relatively	O
easy	O
to	O
determine	O
an	O
optimal	O
policy	O
.	O
for	O
each	O
state	B
s	O
,	O
there	O
will	O
be	O
one	O
or	O
more	O
actions	O
at	O
which	O
the	O
maximum	O
is	O
obtained	O
in	O
the	O
bellman	O
optimality	O
equation	O
.	O
any	O
policy	B
that	O
assigns	O
nonzero	O
probability	O
only	O
to	O
these	O
actions	O
is	O
an	O
optimal	O
policy	O
.	O
you	O
can	O
think	O
of	O
this	O
as	O
a	O
one-step	O
search	O
.	O
if	O
you	O
have	O
the	O
optimal	O
value	O
function	O
,	O
v∗	O
,	O
then	O
the	O
actions	O
that	O
appear	O
best	O
after	O
a	O
one-step	O
search	O
will	O
be	O
optimal	O
actions	O
.	O
another	O
way	O
of	O
saying	O
this	O
is	O
that	O
any	O
policy	B
that	O
is	O
greedy	O
with	O
respect	O
to	O
the	O
optimal	O
evaluation	O
function	O
v∗	O
is	O
an	O
optimal	O
policy	O
.	O
the	O
term	O
greedy	O
is	O
used	O
in	O
computer	O
science	O
to	O
describe	O
any	O
search	O
or	O
decision	O
procedure	O
that	O
selects	O
alternatives	O
based	O
only	O
on	O
local	O
or	O
immediate	O
considerations	O
,	O
without	O
considering	O
the	O
possibility	O
that	O
such	O
a	O
selection	O
may	O
prevent	O
future	O
access	O
to	O
even	O
better	O
alternatives	O
.	O
consequently	O
,	O
it	O
describes	O
policies	O
that	O
select	O
actions	O
based	O
only	O
on	O
their	O
short-term	O
consequences	O
.	O
the	O
beauty	O
of	O
v∗	O
is	O
that	O
if	O
one	O
uses	O
it	O
to	O
evaluate	O
the	O
short-term	O
consequences	O
of	O
actions—	O
speciﬁcally	O
,	O
the	O
one-step	O
consequences—then	O
a	O
greedy	O
policy	O
is	O
actually	O
optimal	O
in	O
the	O
long-term	O
sense	O
in	O
which	O
we	O
are	O
interested	O
because	O
v∗	O
already	O
takes	O
into	O
account	O
the	O
reward	O
consequences	O
of	O
all	O
possible	O
future	O
behavior	O
.	O
by	O
means	O
of	O
v∗	O
,	O
the	O
optimal	O
expected	O
ss0arrs0s	O
,	O
aa0maxmax	O
(	O
v⇤	O
)	O
(	O
q⇤	O
)	O
3.6.	O
optimal	O
policies	O
and	O
optimal	O
value	B
functions	O
65	O
long-term	O
return	B
is	O
turned	O
into	O
a	O
quantity	O
that	O
is	O
locally	O
and	O
immediately	O
available	O
for	O
each	O
state	B
.	O
hence	O
,	O
a	O
one-step-ahead	O
search	O
yields	O
the	O
long-term	O
optimal	O
actions	O
.	O
having	O
q∗	O
makes	O
choosing	O
optimal	O
actions	O
even	O
easier	O
.	O
with	O
q∗	O
,	O
the	O
agent	O
does	O
not	O
even	O
have	O
to	O
do	O
a	O
one-step-ahead	O
search	O
:	O
for	O
any	O
state	B
s	O
,	O
it	O
can	O
simply	O
ﬁnd	O
any	O
action	B
that	O
maximizes	O
q∗	O
(	O
s	O
,	O
a	O
)	O
.	O
the	O
action-value	B
function	I
eﬀectively	O
caches	O
the	O
results	O
of	O
all	O
one-step-ahead	O
searches	O
.	O
it	O
provides	O
the	O
optimal	O
expected	O
long-term	O
return	B
as	O
a	O
value	B
that	O
is	O
locally	O
and	O
immediately	O
available	O
for	O
each	O
state–action	O
pair	O
.	O
hence	O
,	O
at	O
the	O
cost	O
of	O
representing	O
a	O
function	O
of	O
state–action	O
pairs	O
,	O
instead	O
of	O
just	O
of	O
states	O
,	O
the	O
optimal	O
action-	O
value	B
function	I
allows	O
optimal	O
actions	O
to	O
be	O
selected	O
without	O
having	O
to	O
know	O
anything	O
about	O
possible	O
successor	O
states	O
and	O
their	O
values	O
,	O
that	O
is	O
,	O
without	O
having	O
to	O
know	O
anything	O
about	O
the	O
environment	B
’	O
s	O
dynamics	O
.	O
example	O
3.8	O
:	O
solving	O
the	O
gridworld	O
suppose	O
we	O
solve	O
the	O
bellman	O
equation	O
for	O
v∗	O
for	O
the	O
simple	O
grid	O
task	O
introduced	O
in	O
example	O
3.5	O
and	O
shown	O
again	O
in	O
figure	O
3.5	O
(	O
left	O
)	O
.	O
recall	O
that	O
state	B
a	O
is	O
followed	O
by	O
a	O
reward	O
of	O
+10	O
and	O
transition	O
to	O
state	B
a	O
(	O
cid:48	O
)	O
,	O
while	O
state	B
b	O
is	O
followed	O
by	O
a	O
reward	O
of	O
+5	O
and	O
transition	O
to	O
state	B
b	O
(	O
cid:48	O
)	O
.	O
figure	O
3.5	O
(	O
middle	O
)	O
shows	O
the	O
optimal	O
value	O
function	O
,	O
and	O
figure	O
3.5	O
(	O
right	O
)	O
shows	O
the	O
corresponding	O
optimal	O
policies	O
.	O
where	O
there	O
are	O
multiple	O
arrows	O
in	O
a	O
cell	O
,	O
all	O
of	O
the	O
corresponding	O
actions	O
are	O
optimal	O
.	O
figure	O
3.5	O
:	O
optimal	O
solutions	O
to	O
the	O
gridworld	O
example	O
.	O
example	O
3.9	O
:	O
bellman	O
optimality	O
equations	O
for	O
the	O
recycling	O
robot	O
using	O
(	O
3.19	O
)	O
,	O
we	O
can	O
explicitly	O
give	O
the	O
bellman	O
optimality	O
equation	O
for	O
the	O
recycling	B
robot	I
example	I
.	O
to	O
make	O
things	O
more	O
compact	O
,	O
we	O
abbreviate	O
the	O
states	O
high	O
and	O
low	O
,	O
and	O
the	O
actions	O
search	O
,	O
wait	O
,	O
and	O
recharge	O
respectively	O
by	O
h	O
,	O
l	O
,	O
s	O
,	O
w	O
,	O
and	O
re	O
.	O
because	O
there	O
are	O
only	O
two	O
states	O
,	O
the	O
bellman	O
optimality	O
equation	O
consists	O
of	O
two	O
equations	O
.	O
the	O
equation	O
for	O
v∗	O
(	O
h	O
)	O
can	O
be	O
written	O
as	O
follows	O
:	O
v∗	O
(	O
h	O
)	O
=	O
max	O
(	O
cid:26	O
)	O
p	O
(	O
h|h	O
,	O
s	O
)	O
[	O
r	O
(	O
h	O
,	O
s	O
,	O
h	O
)	O
+	O
γv∗	O
(	O
h	O
)	O
]	O
+	O
p	O
(	O
l|h	O
,	O
s	O
)	O
[	O
r	O
(	O
h	O
,	O
s	O
,	O
l	O
)	O
+	O
γv∗	O
(	O
l	O
)	O
]	O
,	O
p	O
(	O
h|h	O
,	O
w	O
)	O
[	O
r	O
(	O
h	O
,	O
w	O
,	O
h	O
)	O
+	O
γv∗	O
(	O
h	O
)	O
]	O
+	O
p	O
(	O
l|h	O
,	O
w	O
)	O
[	O
r	O
(	O
h	O
,	O
w	O
,	O
l	O
)	O
+	O
γv∗	O
(	O
l	O
)	O
]	O
(	O
cid:27	O
)	O
(	O
cid:27	O
)	O
=	O
max	O
(	O
cid:26	O
)	O
α	O
[	O
rs	O
+	O
γv∗	O
(	O
h	O
)	O
]	O
+	O
(	O
1	O
−	O
α	O
)	O
[	O
rs	O
+	O
γv∗	O
(	O
l	O
)	O
]	O
,	O
(	O
cid:27	O
)	O
.	O
=	O
max	O
(	O
cid:26	O
)	O
rs	O
+	O
γ	O
[	O
αv∗	O
(	O
h	O
)	O
+	O
(	O
1	O
−	O
α	O
)	O
v∗	O
(	O
l	O
)	O
]	O
,	O
1	O
[	O
rw	O
+	O
γv∗	O
(	O
h	O
)	O
]	O
+	O
0	O
[	O
rw	O
+	O
γv∗	O
(	O
l	O
)	O
]	O
rw	O
+	O
γv∗	O
(	O
h	O
)	O
a	O
)	O
gridworldb	O
)	O
v*c	O
)	O
!	O
*22.024.422.019.417.519.822.019.817.816.017.819.817.816.014.416.017.816.014.413.014.416.014.413.011.7aba'b'+10+5v*π*gridworldv⇤⇡⇤	O
v∗	O
(	O
l	O
)	O
=	O
max	O
βrs	O
−	O
3	O
(	O
1	O
−	O
β	O
)	O
+	O
γ	O
[	O
(	O
1	O
−	O
β	O
)	O
v∗	O
(	O
h	O
)	O
+	O
βv∗	O
(	O
l	O
)	O
]	O
,	O
rw	O
+	O
γv∗	O
(	O
l	O
)	O
,	O
γv∗	O
(	O
h	O
)	O
.	O
	O
66	O
chapter	O
3	O
:	O
finite	O
markov	O
decision	O
processes	O
following	O
the	O
same	O
procedure	O
for	O
v∗	O
(	O
l	O
)	O
yields	O
the	O
equation	O
for	O
any	O
choice	O
of	O
rs	O
,	O
rw	O
,	O
α	O
,	O
β	O
,	O
and	O
γ	O
,	O
with	O
0	O
≤	O
γ	O
<	O
1	O
,	O
0	O
≤	O
α	O
,	O
β	O
≤	O
1	O
,	O
there	O
is	O
exactly	O
one	O
pair	O
of	O
numbers	O
,	O
v∗	O
(	O
h	O
)	O
and	O
v∗	O
(	O
l	O
)	O
,	O
that	O
simultaneously	O
satisfy	O
these	O
two	O
nonlinear	O
equations	O
.	O
explicitly	O
solving	O
the	O
bellman	O
optimality	O
equation	O
provides	O
one	O
route	O
to	O
ﬁnding	O
an	O
optimal	O
policy	O
,	O
and	O
thus	O
to	O
solving	O
the	O
reinforcement	B
learning	I
problem	O
.	O
however	O
,	O
this	O
solution	O
is	O
rarely	O
directly	O
useful	O
.	O
it	O
is	O
akin	O
to	O
an	O
exhaustive	O
search	O
,	O
looking	O
ahead	O
at	O
all	O
possibilities	O
,	O
computing	O
their	O
probabilities	O
of	O
occurrence	O
and	O
their	O
desirabilities	O
in	O
terms	O
of	O
expected	O
rewards	O
.	O
this	O
solution	O
relies	O
on	O
at	O
least	O
three	O
assumptions	O
that	O
are	O
rarely	O
true	O
in	O
practice	O
:	O
(	O
1	O
)	O
we	O
accurately	O
know	O
the	O
dynamics	O
of	O
the	O
environment	B
;	O
(	O
2	O
)	O
we	O
have	O
enough	O
computational	O
resources	O
to	O
complete	O
the	O
computation	O
of	O
the	O
solution	O
;	O
and	O
(	O
3	O
)	O
the	O
markov	O
property	O
.	O
for	O
the	O
kinds	O
of	O
tasks	O
in	O
which	O
we	O
are	O
interested	O
,	O
one	O
is	O
generally	O
not	O
able	O
to	O
implement	O
this	O
solution	O
exactly	O
because	O
various	O
combinations	O
of	O
these	O
assumptions	O
are	O
violated	O
.	O
for	O
example	O
,	O
although	O
the	O
ﬁrst	O
and	O
third	O
assumptions	O
present	O
no	O
problems	O
for	O
the	O
game	O
of	O
backgammon	B
,	O
the	O
second	O
is	O
a	O
major	O
impediment	O
.	O
because	O
the	O
game	O
has	O
about	O
1020	O
states	O
,	O
it	O
would	O
take	O
thousands	O
of	O
years	O
on	O
today	O
’	O
s	O
fastest	O
computers	O
to	O
solve	O
the	O
bellman	O
equation	O
for	O
v∗	O
,	O
and	O
the	O
same	O
is	O
true	O
for	O
ﬁnding	O
q∗	O
.	O
in	O
reinforcement	O
learning	O
one	O
typically	O
has	O
to	O
settle	O
for	O
approximate	O
solutions	O
.	O
many	O
diﬀerent	O
decision-making	O
methods	O
can	O
be	O
viewed	O
as	O
ways	O
of	O
approximately	O
solving	O
the	O
bellman	O
optimality	O
equation	O
.	O
for	O
example	O
,	O
heuristic	B
search	I
methods	O
can	O
be	O
viewed	O
as	O
expanding	O
the	O
right-hand	O
side	O
of	O
(	O
3.19	O
)	O
several	O
times	O
,	O
up	O
to	O
some	O
depth	O
,	O
forming	O
a	O
“	O
tree	O
”	O
of	O
possibilities	O
,	O
and	O
then	O
using	O
a	O
heuristic	O
evaluation	O
function	O
to	O
approx-	O
imate	O
v∗	O
at	O
the	O
“	O
leaf	O
”	O
nodes	O
.	O
(	O
heuristic	B
search	I
methods	O
such	O
as	O
a∗	O
are	O
almost	O
always	O
based	O
on	O
the	O
episodic	O
case	O
.	O
)	O
the	O
methods	O
of	O
dynamic	O
programming	O
can	O
be	O
related	O
even	O
more	O
closely	O
to	O
the	O
bellman	O
optimality	O
equation	O
.	O
many	O
reinforcement	B
learning	I
methods	O
can	O
be	O
clearly	O
understood	O
as	O
approximately	O
solving	O
the	O
bellman	O
optimality	O
equation	O
,	O
using	O
actual	O
experienced	O
transitions	O
in	O
place	O
of	O
knowledge	O
of	O
the	O
expected	O
transitions	O
.	O
we	O
consider	O
a	O
variety	O
of	O
such	O
methods	O
in	O
the	O
following	O
chapters	O
.	O
exercise	O
3.18	O
draw	O
or	O
describe	O
the	O
optimal	O
state-value	O
function	O
for	O
the	O
golf	B
example	I
.	O
(	O
cid:3	O
)	O
exercise	O
3.19	O
draw	O
or	O
describe	O
the	O
contours	O
of	O
the	O
optimal	O
action-value	O
function	O
for	O
(	O
cid:3	O
)	O
putting	O
,	O
q∗	O
(	O
s	O
,	O
putter	O
)	O
,	O
for	O
the	O
golf	B
example	I
.	O
exercise	O
3.20	O
consider	O
the	O
continuing	O
mdp	O
shown	O
on	O
to	O
the	O
right	O
.	O
the	O
only	O
decision	O
to	O
be	O
made	O
is	O
that	O
in	O
the	O
top	O
state	B
,	O
where	O
two	O
actions	O
are	O
available	O
,	O
left	O
and	O
right	O
.	O
the	O
numbers	O
show	O
the	O
rewards	O
that	O
are	O
received	O
deterministi-	O
cally	O
after	O
each	O
action	B
.	O
there	O
are	O
exactly	O
two	O
deterministic	O
policies	O
,	O
πleft	O
and	O
πright	O
.	O
what	O
policy	B
is	O
optimal	O
if	O
γ	O
=	O
0	O
?	O
(	O
cid:3	O
)	O
if	O
γ	O
=	O
0.9	O
?	O
if	O
γ	O
=	O
0.5	O
?	O
+200+1leftright	O
3.7.	O
optimality	O
and	O
approximation	O
67	O
(	O
cid:3	O
)	O
exercise	O
3.21	O
give	O
the	O
bellman	O
equation	O
for	O
q∗	O
for	O
the	O
recycling	O
robot	O
.	O
exercise	O
3.22	O
figure	O
3.5	O
gives	O
the	O
optimal	O
value	O
of	O
the	O
best	O
state	B
of	O
the	O
gridworld	O
as	O
24.4	O
,	O
to	O
one	O
decimal	O
place	O
.	O
use	O
your	O
knowledge	O
of	O
the	O
optimal	O
policy	O
and	O
(	O
3.8	O
)	O
to	O
express	O
(	O
cid:3	O
)	O
this	O
value	B
symbolically	O
,	O
and	O
then	O
to	O
compute	O
it	O
to	O
three	O
decimal	O
places	O
.	O
(	O
cid:3	O
)	O
exercise	O
3.23	O
give	O
an	O
equation	O
for	O
v∗	O
in	O
terms	O
of	O
q∗	O
.	O
exercise	O
3.24	O
give	O
an	O
equation	O
for	O
q∗	O
in	O
terms	O
of	O
v∗	O
and	O
the	O
world	O
’	O
s	O
dynamics	O
,	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
.	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
exercise	O
3.25	O
give	O
an	O
equation	O
for	O
π∗	O
in	O
terms	O
of	O
q∗	O
.	O
exercise	O
3.26	O
give	O
an	O
equation	O
for	O
π∗	O
in	O
terms	O
of	O
v∗	O
and	O
the	O
world	O
’	O
s	O
dynamics	O
,	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
.	O
(	O
cid:3	O
)	O
3.7	O
optimality	O
and	O
approximation	O
we	O
have	O
deﬁned	O
optimal	O
value	O
functions	O
and	O
optimal	O
policies	O
.	O
clearly	O
,	O
an	O
agent	O
that	O
learns	O
an	O
optimal	O
policy	O
has	O
done	O
very	O
well	O
,	O
but	O
in	O
practice	O
this	O
rarely	O
happens	O
.	O
for	O
the	O
kinds	O
of	O
tasks	O
in	O
which	O
we	O
are	O
interested	O
,	O
optimal	O
policies	O
can	O
be	O
generated	O
only	O
with	O
extreme	O
computational	O
cost	O
.	O
a	O
well-deﬁned	O
notion	O
of	O
optimality	O
organizes	O
the	O
approach	O
to	O
learning	O
we	O
describe	O
in	O
this	O
book	O
and	O
provides	O
a	O
way	O
to	O
understand	O
the	O
theoretical	O
properties	O
of	O
various	O
learning	O
algorithms	O
,	O
but	O
it	O
is	O
an	O
ideal	O
that	O
agents	O
can	O
only	O
approximate	B
to	O
varying	O
degrees	O
.	O
as	O
we	O
discussed	O
above	O
,	O
even	O
if	O
we	O
have	O
a	O
complete	O
and	O
accurate	O
model	B
of	I
the	I
environment	I
’	O
s	O
dynamics	O
,	O
it	O
is	O
usually	O
not	O
possible	O
to	O
simply	O
compute	O
an	O
optimal	O
policy	O
by	O
solving	O
the	O
bellman	O
optimality	O
equation	O
.	O
for	O
example	O
,	O
board	O
games	O
such	O
as	O
chess	O
are	O
a	O
tiny	O
fraction	O
of	O
human	O
experience	O
,	O
yet	O
large	O
,	O
custom-	O
designed	O
computers	O
still	O
can	O
not	O
compute	O
the	O
optimal	O
moves	O
.	O
a	O
critical	O
aspect	O
of	O
the	O
problem	O
facing	O
the	O
agent	O
is	O
always	O
the	O
computational	O
power	O
available	O
to	O
it	O
,	O
in	O
particular	O
,	O
the	O
amount	O
of	O
computation	O
it	O
can	O
perform	O
in	O
a	O
single	O
time	O
step	O
.	O
the	O
memory	O
available	O
is	O
also	O
an	O
important	O
constraint	O
.	O
a	O
large	O
amount	O
of	O
memory	O
is	O
often	O
required	O
to	O
build	O
up	O
approximations	O
of	O
value	O
functions	O
,	O
policies	O
,	O
and	O
models	O
.	O
in	O
tasks	O
with	O
small	O
,	O
ﬁnite	O
state	B
sets	O
,	O
it	O
is	O
possible	O
to	O
form	O
these	O
approximations	O
using	O
arrays	O
or	O
tables	O
with	O
one	O
entry	O
for	O
each	O
state	B
(	O
or	O
state–action	O
pair	O
)	O
.	O
this	O
we	O
call	O
the	O
tabular	O
case	O
,	O
and	O
the	O
corresponding	O
methods	O
we	O
call	O
tabular	O
methods	O
.	O
in	O
many	O
cases	O
of	O
practical	O
interest	O
,	O
however	O
,	O
there	O
are	O
far	O
more	O
states	O
than	O
could	O
possibly	O
be	O
entries	O
in	O
a	O
table	O
.	O
in	O
these	O
cases	O
the	O
functions	O
must	O
be	O
approximated	O
,	O
using	O
some	O
sort	O
of	O
more	O
compact	O
parameterized	O
function	O
representation	O
.	O
our	O
framing	O
of	O
the	O
reinforcement	B
learning	I
problem	O
forces	O
us	O
to	O
settle	O
for	O
approxima-	O
tions	O
.	O
however	O
,	O
it	O
also	O
presents	O
us	O
with	O
some	O
unique	O
opportunities	O
for	O
achieving	O
useful	O
approximations	O
.	O
for	O
example	O
,	O
in	O
approximating	O
optimal	O
behavior	O
,	O
there	O
may	O
be	O
many	O
states	O
that	O
the	O
agent	O
faces	O
with	O
such	O
a	O
low	O
probability	O
that	O
selecting	O
suboptimal	O
ac-	O
tions	O
for	O
them	O
has	O
little	O
impact	O
on	O
the	O
amount	O
of	O
reward	O
the	O
agent	O
receives	O
.	O
tesauro	O
’	O
s	O
backgammon	B
player	O
,	O
for	O
example	O
,	O
plays	O
with	O
exceptional	O
skill	O
even	O
though	O
it	O
might	O
make	O
very	O
bad	O
decisions	O
on	O
board	O
conﬁgurations	O
that	O
never	O
occur	O
in	O
games	O
against	O
experts	O
.	O
in	O
fact	O
,	O
it	O
is	O
possible	O
that	O
td-gammon	O
makes	O
bad	O
decisions	O
for	O
a	O
large	O
fraction	O
of	O
the	O
68	O
chapter	O
3	O
:	O
finite	O
markov	O
decision	O
processes	O
game	O
’	O
s	O
state	B
set	O
.	O
the	O
online	B
nature	O
of	O
reinforcement	O
learning	O
makes	O
it	O
possible	O
to	O
ap-	O
proximate	O
optimal	O
policies	O
in	O
ways	O
that	O
put	O
more	O
eﬀort	O
into	O
learning	O
to	O
make	O
good	O
decisions	O
for	O
frequently	O
encountered	O
states	O
,	O
at	O
the	O
expense	O
of	O
less	O
eﬀort	O
for	O
infrequently	O
encountered	O
states	O
.	O
this	O
is	O
one	O
key	O
property	O
that	O
distinguishes	O
reinforcement	B
learning	I
from	O
other	O
approaches	O
to	O
approximately	O
solving	O
mdps	O
.	O
3.8	O
summary	O
let	O
us	O
summarize	O
the	O
elements	O
of	O
the	O
reinforcement	B
learning	I
problem	O
that	O
we	O
have	O
presented	O
in	O
this	O
chapter	O
.	O
reinforcement	B
learning	I
is	O
about	O
learning	O
from	O
interaction	O
how	O
to	O
behave	O
in	O
order	O
to	O
achieve	O
a	O
goal	B
.	O
the	O
reinforcement	B
learning	I
agent	O
and	O
its	O
environment	B
interact	O
over	O
a	O
sequence	O
of	O
discrete	O
time	O
steps	O
.	O
the	O
speciﬁcation	O
of	O
their	O
interface	O
deﬁnes	O
a	O
particular	O
task	O
:	O
the	O
actions	O
are	O
the	O
choices	O
made	O
by	O
the	O
agent	O
;	O
the	O
states	O
are	O
the	O
basis	O
for	O
making	O
the	O
choices	O
;	O
and	O
the	O
rewards	O
are	O
the	O
basis	O
for	O
evaluating	O
the	O
choices	O
.	O
everything	O
inside	O
the	O
agent	O
is	O
completely	O
known	O
and	O
controllable	O
by	O
the	O
agent	O
;	O
everything	O
outside	O
is	O
incompletely	O
controllable	O
but	O
may	O
or	O
may	O
not	O
be	O
completely	O
known	O
.	O
a	O
policy	B
is	O
a	O
stochastic	O
rule	O
by	O
which	O
the	O
agent	O
selects	O
actions	O
as	O
a	O
function	O
of	O
states	O
.	O
the	O
agent	O
’	O
s	O
objective	O
is	O
to	O
maximize	O
the	O
amount	O
of	O
reward	O
it	O
receives	O
over	O
time	O
.	O
when	O
the	O
reinforcement	B
learning	I
setup	O
described	O
above	O
is	O
formulated	O
with	O
well	O
deﬁned	O
transition	B
probabilities	I
it	O
constitutes	O
a	O
markov	O
decision	O
process	O
(	O
mdp	O
)	O
.	O
a	O
ﬁnite	O
mdp	O
is	O
an	O
mdp	O
with	O
ﬁnite	O
state	B
,	O
action	B
,	O
and	O
(	O
as	O
we	O
formulate	O
it	O
here	O
)	O
reward	O
sets	O
.	O
much	O
of	O
the	O
current	O
theory	O
of	O
reinforcement	O
learning	O
is	O
restricted	O
to	O
ﬁnite	O
mdps	O
,	O
but	O
the	O
methods	O
and	O
ideas	O
apply	O
more	O
generally	O
.	O
the	O
return	B
is	O
the	O
function	O
of	O
future	O
rewards	O
that	O
the	O
agent	O
seeks	O
to	O
maximize	O
(	O
in	O
expected	O
value	B
)	O
.	O
it	O
has	O
several	O
diﬀerent	O
deﬁnitions	O
depending	O
upon	O
the	O
nature	O
of	O
the	O
task	O
and	O
whether	O
one	O
wishes	O
to	O
discount	O
delayed	B
reward	I
.	O
the	O
undiscounted	O
formulation	O
is	O
appropriate	O
for	O
episodic	O
tasks	O
,	O
in	O
which	O
the	O
agent–environment	O
interaction	O
breaks	O
naturally	O
into	O
episodes	B
;	O
the	O
discounted	O
formulation	O
is	O
appropriate	O
for	O
continuing	O
tasks	O
,	O
in	O
which	O
the	O
interaction	O
does	O
not	O
naturally	O
break	O
into	O
episodes	B
but	O
continues	O
without	O
limit	O
.	O
we	O
try	O
to	O
deﬁne	O
the	O
returns	O
for	O
the	O
two	O
kinds	O
of	O
tasks	O
such	O
that	O
one	O
set	O
of	O
equations	O
can	O
apply	O
to	O
both	O
the	O
episodic	O
and	O
continuing	O
cases	O
.	O
a	O
policy	B
’	O
s	O
value	B
functions	O
assign	O
to	O
each	O
state	B
,	O
or	O
state–action	O
pair	O
,	O
the	O
expected	O
return	O
from	O
that	O
state	B
,	O
or	O
state–action	O
pair	O
,	O
given	O
that	O
the	O
agent	O
uses	O
the	O
policy	B
.	O
the	O
optimal	O
value	O
functions	O
assign	O
to	O
each	O
state	B
,	O
or	O
state–action	O
pair	O
,	O
the	O
largest	O
expected	O
return	O
achievable	O
by	O
any	O
policy	B
.	O
a	O
policy	B
whose	O
value	B
functions	O
are	O
optimal	O
is	O
an	O
optimal	O
policy	O
.	O
whereas	O
the	O
optimal	O
value	O
functions	O
for	O
states	O
and	O
state–action	O
pairs	O
are	O
unique	O
for	O
a	O
given	O
mdp	O
,	O
there	O
can	O
be	O
many	O
optimal	O
policies	O
.	O
any	O
policy	B
that	O
is	O
greedy	O
with	O
respect	O
to	O
the	O
optimal	O
value	O
functions	O
must	O
be	O
an	O
optimal	O
policy	O
.	O
the	O
bellman	O
optimality	O
equations	O
are	O
special	O
consistency	O
conditions	O
that	O
the	O
optimal	O
value	O
functions	O
must	O
satisfy	O
and	O
that	O
can	O
,	O
in	O
principle	O
,	O
be	O
solved	O
for	O
the	O
optimal	O
value	O
functions	O
,	O
from	O
which	O
an	O
optimal	O
policy	O
can	O
be	O
determined	O
with	O
relative	O
ease	O
.	O
a	O
reinforcement	B
learning	I
problem	O
can	O
be	O
posed	O
in	O
a	O
variety	O
of	O
diﬀerent	O
ways	O
depending	O
on	O
assumptions	O
about	O
the	O
level	O
of	O
knowledge	O
initially	O
available	O
to	O
the	O
agent	O
.	O
in	O
problems	O
of	O
complete	O
knowledge	O
,	O
the	O
agent	O
has	O
a	O
complete	O
and	O
accurate	O
model	B
of	I
the	I
environment	I
’	O
s	O
3.8.	O
summary	O
69	O
dynamics	O
.	O
if	O
the	O
environment	B
is	O
an	O
mdp	O
,	O
then	O
such	O
a	O
model	O
consists	O
of	O
the	O
complete	O
four-	O
argument	O
dynamics	O
function	O
p	O
(	O
3.2	O
)	O
.	O
in	O
problems	O
of	O
incomplete	O
knowledge	O
,	O
a	O
complete	O
and	O
perfect	O
model	B
of	I
the	I
environment	I
is	O
not	O
available	O
.	O
even	O
if	O
the	O
agent	O
has	O
a	O
complete	O
and	O
accurate	O
environment	B
model	O
,	O
the	O
agent	O
is	O
typi-	O
cally	O
unable	O
to	O
perform	O
enough	O
computation	O
per	O
time	O
step	O
to	O
fully	O
use	O
it	O
.	O
the	O
memory	O
available	O
is	O
also	O
an	O
important	O
constraint	O
.	O
memory	O
may	O
be	O
required	O
to	O
build	O
up	O
accurate	O
approximations	O
of	O
value	O
functions	O
,	O
policies	O
,	O
and	O
models	O
.	O
in	O
most	O
cases	O
of	O
practical	O
inter-	O
est	O
there	O
are	O
far	O
more	O
states	O
than	O
could	O
possibly	O
be	O
entries	O
in	O
a	O
table	O
,	O
and	O
approximations	O
must	O
be	O
made	O
.	O
a	O
well-deﬁned	O
notion	O
of	O
optimality	O
organizes	O
the	O
approach	O
to	O
learning	O
we	O
describe	O
in	O
this	O
book	O
and	O
provides	O
a	O
way	O
to	O
understand	O
the	O
theoretical	O
properties	O
of	O
various	O
learning	O
algorithms	O
,	O
but	O
it	O
is	O
an	O
ideal	O
that	O
reinforcement	B
learning	I
agents	O
can	O
only	O
approximate	B
to	O
varying	O
degrees	O
.	O
in	O
reinforcement	O
learning	O
we	O
are	O
very	O
much	O
concerned	O
with	O
cases	O
in	O
which	O
optimal	O
solutions	O
can	O
not	O
be	O
found	O
but	O
must	O
be	O
approximated	O
in	O
some	O
way	O
.	O
bibliographical	O
and	O
historical	O
remarks	O
the	O
reinforcement	B
learning	I
problem	O
is	O
deeply	O
indebted	O
to	O
the	O
idea	O
of	O
markov	O
decision	O
processes	O
(	O
mdps	O
)	O
from	O
the	O
ﬁeld	O
of	O
optimal	O
control	B
.	O
these	O
historical	O
inﬂuences	O
and	O
other	O
major	O
inﬂuences	O
from	O
psychology	B
are	O
described	O
in	O
the	O
brief	O
history	O
given	O
in	O
chapter	O
1.	O
reinforcement	B
learning	I
adds	O
to	O
mdps	O
a	O
focus	O
on	O
approximation	O
and	O
incomplete	O
infor-	O
mation	O
for	O
realistically	O
large	O
problems	O
.	O
mdps	O
and	O
the	O
reinforcement	O
learning	O
problem	O
are	O
only	O
weakly	O
linked	O
to	O
traditional	O
learning	O
and	O
decision-making	O
problems	O
in	O
artiﬁcial	O
intelligence	O
.	O
however	O
,	O
artiﬁcial	B
intelligence	I
is	O
now	O
vigorously	O
exploring	O
mdp	O
formula-	O
tions	O
for	O
planning	O
and	O
decision	O
making	O
from	O
a	O
variety	O
of	O
perspectives	O
.	O
mdps	O
are	O
more	O
general	O
than	O
previous	O
formulations	O
used	O
in	O
artiﬁcial	O
intelligence	O
in	O
that	O
they	O
permit	O
more	O
general	O
kinds	O
of	O
goals	O
and	O
uncertainty	O
.	O
the	O
theory	O
of	O
mdps	O
is	O
treated	O
by	O
,	O
e.g.	O
,	O
bertsekas	O
(	O
2005	O
)	O
,	O
white	O
(	O
1969	O
)	O
,	O
whittle	O
(	O
1982	O
,	O
1983	O
)	O
,	O
and	O
puterman	O
(	O
1994	O
)	O
.	O
a	O
particularly	O
compact	O
treatment	O
of	O
the	O
ﬁnite	O
case	O
is	O
given	O
by	O
ross	O
(	O
1983	O
)	O
.	O
mdps	O
are	O
also	O
studied	O
under	O
the	O
heading	O
of	O
stochastic	O
optimal	B
control	I
,	O
where	O
adaptive	O
optimal	B
control	I
methods	O
are	O
most	O
closely	O
related	O
to	O
reinforcement	B
learning	I
(	O
e.g.	O
,	O
kumar	O
,	O
1985	O
;	O
kumar	O
and	O
varaiya	O
,	O
1986	O
)	O
.	O
the	O
theory	O
of	O
mdps	O
evolved	O
from	O
eﬀorts	O
to	O
understand	O
the	O
problem	O
of	O
making	O
se-	O
quences	O
of	O
decisions	O
under	O
uncertainty	O
,	O
where	O
each	O
decision	O
can	O
depend	O
on	O
the	O
previous	O
decisions	O
and	O
their	O
outcomes	O
.	O
it	O
is	O
sometimes	O
called	O
the	O
theory	O
of	O
multistage	O
decision	O
processes	O
,	O
or	O
sequential	O
decision	O
processes	O
,	O
and	O
has	O
roots	O
in	O
the	O
statistical	O
literature	O
on	O
sequential	O
sampling	O
beginning	O
with	O
the	O
papers	O
by	O
thompson	O
(	O
1933	O
,	O
1934	O
)	O
and	O
rob-	O
bins	O
(	O
1952	O
)	O
that	O
we	O
cited	O
in	O
chapter	O
2	O
in	O
connection	O
with	O
bandit	O
problems	O
(	O
which	O
are	O
prototypical	O
mdps	O
if	O
formulated	O
as	O
multiple-situation	O
problems	O
)	O
.	O
the	O
earliest	O
instance	O
of	O
which	O
we	O
are	O
aware	O
in	O
which	O
reinforcement	B
learning	I
was	O
dis-	O
cussed	O
using	O
the	O
mdp	O
formalism	O
is	O
andreae	O
’	O
s	O
(	O
1969b	O
)	O
description	O
of	O
a	O
uniﬁed	O
view	O
of	O
learning	O
machines	O
.	O
witten	O
and	O
corbin	O
(	O
1973	O
)	O
experimented	O
with	O
a	O
reinforcement	O
learn-	O
ing	B
system	O
later	O
analyzed	O
by	O
witten	O
(	O
1977	O
)	O
using	O
the	O
mdp	O
formalism	O
.	O
although	O
he	O
did	O
not	O
explicitly	O
mention	O
mdps	O
,	O
werbos	O
(	O
1977	O
)	O
suggested	O
approximate	B
solution	O
methods	O
70	O
chapter	O
3	O
:	O
finite	O
markov	O
decision	O
processes	O
for	O
stochastic	O
optimal	B
control	I
problems	O
that	O
are	O
related	O
to	O
modern	O
reinforcement	B
learning	I
methods	O
(	O
see	O
also	O
werbos	O
,	O
1982	O
,	O
1987	O
,	O
1988	O
,	O
1989	O
,	O
1992	O
)	O
.	O
although	O
werbos	O
’	O
s	O
ideas	O
were	O
not	O
widely	O
recognized	O
at	O
the	O
time	O
,	O
they	O
were	O
prescient	O
in	O
emphasizing	O
the	O
importance	O
of	O
approximately	O
solving	O
optimal	B
control	I
problems	O
in	O
a	O
variety	O
of	O
domains	O
,	O
including	O
arti-	O
ﬁcial	O
intelligence	O
.	O
the	O
most	O
inﬂuential	O
integration	O
of	O
reinforcement	O
learning	O
and	O
mdps	O
is	O
due	O
to	O
watkins	O
(	O
1989	O
)	O
.	O
3.1	O
our	O
characterization	O
of	O
the	O
dynamics	O
of	O
an	O
mdp	O
in	O
terms	O
of	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
is	O
it	O
is	O
more	O
common	O
in	O
the	O
mdp	O
literature	O
to	O
describe	O
the	O
slightly	O
unusual	O
.	O
dynamics	O
in	O
terms	O
of	O
the	O
state	B
transition	O
probabilities	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
a	O
)	O
and	O
expected	O
next	O
rewards	O
r	O
(	O
s	O
,	O
a	O
)	O
.	O
in	O
reinforcement	O
learning	O
,	O
however	O
,	O
we	O
more	O
often	O
have	O
to	O
refer	O
to	O
individual	O
actual	O
or	O
sample	O
rewards	O
(	O
rather	O
than	O
just	O
their	O
expected	O
values	O
)	O
.	O
our	O
notation	O
also	O
makes	O
it	O
plainer	O
that	O
st	O
and	O
rt	O
are	O
in	O
general	O
jointly	O
determined	O
,	O
and	O
thus	O
must	O
have	O
the	O
same	O
time	O
index	O
.	O
in	O
teaching	O
reinforcement	B
learning	I
,	O
we	O
have	O
found	O
our	O
notation	O
to	O
be	O
more	O
straightforward	O
conceptually	O
and	O
easier	O
to	O
understand	O
.	O
for	O
a	O
good	O
intuitive	O
discussion	O
of	O
the	O
system-theoretic	O
concept	O
of	O
state	O
,	O
see	O
minsky	O
(	O
1967	O
)	O
.	O
the	O
bioreactor	B
example	I
is	O
based	O
on	O
the	O
work	O
of	O
ungar	O
(	O
1990	O
)	O
and	O
miller	O
and	O
williams	O
(	O
1992	O
)	O
.	O
the	O
recycling	B
robot	I
example	I
was	O
inspired	O
by	O
the	O
can-collecting	O
robot	O
built	O
by	O
jonathan	O
connell	O
(	O
1989	O
)	O
.	O
kober	O
and	O
peters	O
(	O
2012	O
)	O
present	O
a	O
collection	O
of	O
robotics	O
applications	O
of	O
reinforcement	B
learning	I
.	O
3.2	O
the	O
reward	O
hypothesis	O
was	O
suggested	O
by	O
michael	O
littman	O
(	O
personal	O
communi-	O
cation	O
)	O
.	O
3.3–4	O
the	O
terminology	O
of	O
episodic	O
and	O
continuing	O
tasks	O
is	O
diﬀerent	O
from	O
that	O
usu-	O
ally	O
used	O
in	O
the	O
mdp	O
literature	O
.	O
in	O
that	O
literature	O
it	O
is	O
common	O
to	O
distinguish	O
three	O
types	O
of	O
tasks	O
:	O
(	O
1	O
)	O
ﬁnite-horizon	O
tasks	O
,	O
in	O
which	O
interaction	O
terminates	O
after	O
a	O
particular	O
ﬁxed	O
number	O
of	O
time	O
steps	O
;	O
(	O
2	O
)	O
indeﬁnite-horizon	O
tasks	O
,	O
in	O
which	O
interaction	O
can	O
last	O
arbitrarily	O
long	O
but	O
must	O
eventually	O
terminate	O
;	O
and	O
(	O
3	O
)	O
inﬁnite-horizon	O
tasks	O
,	O
in	O
which	O
interaction	O
does	O
not	O
terminate	O
.	O
our	O
episodic	O
and	O
continuing	O
tasks	O
are	O
similar	O
to	O
indeﬁnite-horizon	O
and	O
inﬁnite-horizon	O
tasks	O
,	O
respectively	O
,	O
but	O
we	O
prefer	O
to	O
emphasize	O
the	O
diﬀerence	O
in	O
the	O
nature	O
of	O
the	O
in-	O
teraction	O
.	O
this	O
diﬀerence	O
seems	O
more	O
fundamental	O
than	O
the	O
diﬀerence	O
in	O
the	O
objective	O
functions	O
emphasized	O
by	O
the	O
usual	O
terms	O
.	O
often	O
episodic	O
tasks	O
use	O
an	O
indeﬁnite-horizon	O
objective	O
function	O
and	O
continuing	B
tasks	I
an	O
inﬁnite-horizon	O
objective	O
function	O
,	O
but	O
we	O
see	O
this	O
as	O
a	O
common	O
coincidence	O
rather	O
than	O
a	O
fun-	O
damental	O
diﬀerence	O
.	O
the	O
pole-balancing	O
example	O
is	O
from	O
michie	O
and	O
chambers	O
(	O
1968	O
)	O
and	O
barto	O
,	O
sutton	O
,	O
and	O
anderson	O
(	O
1983	O
)	O
.	O
3.5–6	O
assigning	O
value	B
on	O
the	O
basis	O
of	O
what	O
is	O
good	O
or	O
bad	O
in	O
the	O
long	O
run	O
has	O
ancient	O
roots	O
.	O
in	O
control	O
theory	O
,	O
mapping	O
states	O
to	O
numerical	O
values	O
representing	O
the	O
long-term	O
consequences	O
of	O
control	O
decisions	O
is	O
a	O
key	O
part	O
of	O
optimal	O
control	B
3.8.	O
summary	O
71	O
theory	O
,	O
which	O
was	O
developed	O
in	O
the	O
1950s	O
by	O
extending	O
nineteenth	O
century	O
state-	O
function	O
theories	O
of	O
classical	O
mechanics	O
(	O
see	O
,	O
e.g.	O
,	O
schultz	O
and	O
melsa	O
,	O
1967	O
)	O
.	O
in	O
describing	O
how	O
a	O
computer	O
could	O
be	O
programmed	O
to	O
play	O
chess	B
,	O
shannon	O
(	O
1950	O
)	O
suggested	O
using	O
an	O
evaluation	O
function	O
that	O
took	O
into	O
account	O
the	O
long-term	O
advantages	O
and	O
disadvantages	O
of	O
chess	O
positions	O
.	O
watkins	O
’	O
s	O
(	O
1989	O
)	O
q-learning	O
algorithm	O
for	O
estimating	O
q∗	O
(	O
chapter	O
6	O
)	O
made	O
action-	O
value	B
functions	O
an	O
important	O
part	O
of	O
reinforcement	O
learning	O
,	O
and	O
consequently	O
these	O
functions	O
are	O
often	O
called	O
“	O
q-functions.	O
”	O
but	O
the	O
idea	O
of	O
an	O
action-value	B
function	I
is	O
much	O
older	O
than	O
this	O
.	O
shannon	O
(	O
1950	O
)	O
suggested	O
that	O
a	O
function	O
h	O
(	O
p	O
,	O
m	O
)	O
could	O
be	O
used	O
by	O
a	O
chess-playing	O
program	O
to	O
decide	O
whether	O
a	O
move	O
m	O
in	O
position	O
p	O
is	O
worth	O
exploring	O
.	O
michie	O
’	O
s	O
(	O
1961	O
,	O
1963	O
)	O
menace	O
system	O
and	O
michie	O
and	O
chambers	O
’	O
s	O
(	O
1968	O
)	O
boxes	O
system	O
can	O
be	O
understood	O
as	O
estimating	O
action-value	O
functions	O
.	O
in	O
classical	O
physics	O
,	O
hamilton	O
’	O
s	O
principal	O
function	O
is	O
an	O
action-value	B
function	I
;	O
newtonian	O
dynamics	O
are	O
greedy	O
with	O
respect	O
to	O
this	O
func-	O
tion	B
(	O
e.g.	O
,	O
goldstein	O
,	O
1957	O
)	O
.	O
action-value	O
functions	O
also	O
played	O
a	O
central	O
role	O
in	O
denardo	O
’	O
s	O
(	O
1967	O
)	O
theoretical	O
treatment	O
of	O
dynamic	O
programming	O
in	O
terms	O
of	O
contraction	O
mappings	O
.	O
the	O
bellman	O
optimality	O
equation	O
(	O
for	O
v∗	O
)	O
was	O
popularized	O
by	O
richard	O
bellman	O
(	O
1957a	O
)	O
,	O
who	O
called	O
it	O
the	O
“	O
basic	O
functional	O
equation.	O
”	O
the	O
counterpart	O
of	O
the	O
bellman	O
optimality	O
equation	O
for	O
continuous	O
time	O
and	O
state	O
problems	O
is	O
known	O
as	O
the	O
hamilton–jacobi–bellman	O
equation	O
(	O
or	O
often	O
just	O
the	O
hamilton–jacobi	O
equation	O
)	O
,	O
indicating	O
its	O
roots	O
in	O
classical	O
physics	O
(	O
e.g.	O
,	O
schultz	O
and	O
melsa	O
,	O
1967	O
)	O
.	O
the	O
golf	B
example	I
was	O
suggested	O
by	O
chris	O
watkins	O
.	O
chapter	O
4	O
dynamic	B
programming	I
the	O
term	O
dynamic	B
programming	I
(	O
dp	O
)	O
refers	O
to	O
a	O
collection	O
of	O
algorithms	O
that	O
can	O
be	O
used	O
to	O
compute	O
optimal	O
policies	O
given	O
a	O
perfect	O
model	B
of	I
the	I
environment	I
as	O
a	O
markov	O
decision	O
process	O
(	O
mdp	O
)	O
.	O
classical	O
dp	O
algorithms	O
are	O
of	O
limited	O
utility	O
in	O
reinforcement	O
learning	O
both	O
because	O
of	O
their	O
assumption	O
of	O
a	O
perfect	O
model	O
and	O
because	O
of	O
their	O
great	O
computational	O
expense	O
,	O
but	O
they	O
are	O
still	O
important	O
theoretically	O
.	O
dp	O
provides	O
an	O
es-	O
sential	O
foundation	O
for	O
the	O
understanding	O
of	O
the	O
methods	O
presented	O
in	O
the	O
rest	O
of	O
this	O
book	O
.	O
in	O
fact	O
,	O
all	O
of	O
these	O
methods	O
can	O
be	O
viewed	O
as	O
attempts	O
to	O
achieve	O
much	O
the	O
same	O
eﬀect	O
as	O
dp	O
,	O
only	O
with	O
less	O
computation	O
and	O
without	O
assuming	O
a	O
perfect	O
model	B
of	I
the	I
environment	I
.	O
starting	O
with	O
this	O
chapter	O
,	O
we	O
usually	O
assume	O
that	O
the	O
environment	B
is	O
a	O
ﬁnite	O
mdp	O
.	O
that	O
is	O
,	O
we	O
assume	O
that	O
its	O
state	B
,	O
action	B
,	O
and	O
reward	O
sets	O
,	O
s	O
,	O
a	O
,	O
and	O
r	O
,	O
are	O
ﬁnite	O
,	O
and	O
that	O
its	O
dynamics	O
are	O
given	O
by	O
a	O
set	O
of	O
probabilities	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
,	O
for	O
all	O
s	O
∈	O
s	O
,	O
a	O
∈	O
a	O
(	O
s	O
)	O
,	O
r	O
∈	O
r	O
,	O
and	O
s	O
(	O
cid:48	O
)	O
∈	O
s+	O
(	O
s+	O
is	O
s	O
plus	O
a	O
terminal	O
state	B
if	O
the	O
problem	O
is	O
episodic	O
)	O
.	O
although	O
dp	O
ideas	O
can	O
be	O
applied	O
to	O
problems	O
with	O
continuous	O
state	B
and	O
action	B
spaces	O
,	O
exact	O
solutions	O
are	O
possible	O
only	O
in	O
special	O
cases	O
.	O
a	O
common	O
way	O
of	O
obtaining	O
approximate	B
solutions	O
for	O
tasks	O
with	O
continuous	O
states	O
and	O
actions	O
is	O
to	O
quantize	O
the	O
state	B
and	O
action	B
spaces	O
and	O
then	O
apply	O
ﬁnite-state	O
dp	O
methods	O
.	O
the	O
methods	O
we	O
explore	O
in	O
chapter	O
9	O
are	O
applicable	O
to	O
continuous	O
problems	O
and	O
are	O
a	O
signiﬁcant	O
extension	O
of	O
that	O
approach	O
.	O
the	O
key	O
idea	O
of	O
dp	O
,	O
and	O
of	O
reinforcement	B
learning	I
generally	O
,	O
is	O
the	O
use	O
of	O
value	O
func-	O
tions	O
to	O
organize	O
and	O
structure	O
the	O
search	O
for	O
good	O
policies	O
.	O
in	O
this	O
chapter	O
we	O
show	O
how	O
dp	O
can	O
be	O
used	O
to	O
compute	O
the	O
value	B
functions	O
deﬁned	O
in	O
chapter	O
3.	O
as	O
discussed	O
there	O
,	O
we	O
can	O
easily	O
obtain	O
optimal	O
policies	O
once	O
we	O
have	O
found	O
the	O
optimal	O
value	O
functions	O
,	O
v∗	O
or	O
q∗	O
,	O
which	O
satisfy	O
the	O
bellman	O
optimality	O
equations	O
:	O
v∗	O
(	O
s	O
)	O
=	O
max	O
=	O
max	O
a	O
e	O
[	O
rt+1	O
+	O
γv∗	O
(	O
st+1	O
)	O
|	O
st	O
=	O
s	O
,	O
at	O
=	O
a	O
]	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:104	O
)	O
r	O
+	O
γv∗	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
a	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
73	O
(	O
4.1	O
)	O
74	O
or	O
chapter	O
4	O
:	O
dynamic	B
programming	I
q∗	O
(	O
s	O
,	O
a	O
)	O
=	O
e	O
(	O
cid:104	O
)	O
rt+1	O
+	O
γ	O
max	O
a	O
(	O
cid:48	O
)	O
q∗	O
(	O
st+1	O
,	O
a	O
(	O
cid:48	O
)	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
st	O
=	O
s	O
,	O
at	O
=	O
a	O
(	O
cid:105	O
)	O
q∗	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
,	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:104	O
)	O
r	O
+	O
γ	O
max	O
a	O
(	O
cid:48	O
)	O
=	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
(	O
4.2	O
)	O
for	O
all	O
s	O
∈	O
s	O
,	O
a	O
∈	O
a	O
(	O
s	O
)	O
,	O
and	O
s	O
(	O
cid:48	O
)	O
∈	O
s+	O
.	O
as	O
we	O
shall	O
see	O
,	O
dp	O
algorithms	O
are	O
obtained	O
by	O
turning	O
bellman	O
equations	O
such	O
as	O
these	O
into	O
assignments	O
,	O
that	O
is	O
,	O
into	O
update	O
rules	O
for	O
improving	O
approximations	O
of	O
the	O
desired	O
value	B
functions	O
.	O
4.1	O
policy	B
evaluation	I
(	O
prediction	B
)	O
first	O
we	O
consider	O
how	O
to	O
compute	O
the	O
state-value	O
function	O
vπ	O
for	O
an	O
arbitrary	O
policy	B
π.	O
this	O
is	O
called	O
policy	B
evaluation	I
in	O
the	O
dp	O
literature	O
.	O
we	O
also	O
refer	O
to	O
it	O
as	O
the	O
prediction	B
problem	O
.	O
recall	O
from	O
chapter	O
3	O
that	O
,	O
for	O
all	O
s	O
∈	O
s	O
,	O
vπ	O
(	O
s	O
)	O
.	O
=	O
eπ	O
[	O
gt	O
|	O
st	O
=	O
s	O
]	O
=	O
eπ	O
[	O
rt+1	O
+	O
γgt+1	O
|	O
st	O
=	O
s	O
]	O
=	O
eπ	O
[	O
rt+1	O
+	O
γvπ	O
(	O
st+1	O
)	O
|	O
st	O
=	O
s	O
]	O
=	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
a|s	O
)	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:104	O
)	O
r	O
+	O
γvπ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
,	O
(	O
from	O
(	O
3.9	O
)	O
)	O
(	O
4.3	O
)	O
(	O
4.4	O
)	O
where	O
π	O
(	O
a|s	O
)	O
is	O
the	O
probability	O
of	O
taking	O
action	B
a	O
in	O
state	O
s	O
under	O
policy	B
π	O
,	O
and	O
the	O
ex-	O
pectations	O
are	O
subscripted	O
by	O
π	O
to	O
indicate	O
that	O
they	O
are	O
conditional	O
on	O
π	O
being	O
followed	O
.	O
the	O
existence	O
and	O
uniqueness	O
of	O
vπ	O
are	O
guaranteed	O
as	O
long	O
as	O
either	O
γ	O
<	O
1	O
or	O
eventual	O
termination	O
is	O
guaranteed	O
from	O
all	O
states	O
under	O
the	O
policy	B
π.	O
if	O
the	O
environment	B
’	O
s	O
dynamics	O
are	O
completely	O
known	O
,	O
then	O
(	O
4.4	O
)	O
is	O
a	O
system	O
of	O
|s|	O
si-	O
multaneous	O
linear	O
equations	O
in	O
|s|	O
unknowns	O
(	O
the	O
vπ	O
(	O
s	O
)	O
,	O
s	O
∈	O
s	O
)	O
.	O
in	O
principle	O
,	O
its	O
solution	O
is	O
a	O
straightforward	O
,	O
if	O
tedious	O
,	O
computation	O
.	O
for	O
our	O
purposes	O
,	O
iterative	B
solution	O
meth-	O
ods	O
are	O
most	O
suitable	O
.	O
consider	O
a	O
sequence	O
of	O
approximate	O
value	B
functions	O
v0	O
,	O
v1	O
,	O
v2	O
,	O
.	O
.	O
.	O
,	O
each	O
mapping	O
s+	O
to	O
r	O
(	O
the	O
real	O
numbers	O
)	O
.	O
the	O
initial	O
approximation	O
,	O
v0	O
,	O
is	O
chosen	O
arbi-	O
trarily	O
(	O
except	O
that	O
the	O
terminal	O
state	B
,	O
if	O
any	O
,	O
must	O
be	O
given	O
value	B
0	O
)	O
,	O
and	O
each	O
successive	O
approximation	O
is	O
obtained	O
by	O
using	O
the	O
bellman	O
equation	O
for	B
vπ	I
(	O
4.4	O
)	O
as	O
an	O
update	O
rule	O
:	O
vk+1	O
(	O
s	O
)	O
.	O
=	O
eπ	O
[	O
rt+1	O
+	O
γvk	O
(	O
st+1	O
)	O
|	O
st	O
=	O
s	O
]	O
=	O
(	O
cid:88	O
)	O
a	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:104	O
)	O
r	O
+	O
γvk	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
,	O
π	O
(	O
a|s	O
)	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
(	O
4.5	O
)	O
for	O
all	O
s	O
∈	O
s.	O
clearly	O
,	O
vk	O
=	O
vπ	O
is	O
a	O
ﬁxed	O
point	O
for	O
this	O
update	O
rule	O
because	O
the	O
bellman	O
equation	O
for	B
vπ	I
assures	O
us	O
of	O
equality	O
in	O
this	O
case	O
.	O
indeed	O
,	O
the	O
sequence	O
{	O
vk	O
}	O
can	O
be	O
shown	O
in	O
general	O
to	O
converge	O
to	O
vπ	O
as	O
k	O
→	O
∞	O
under	O
the	O
same	O
conditions	O
that	O
guarantee	O
the	O
existence	O
of	O
vπ	O
.	O
this	O
algorithm	O
is	O
called	O
iterative	B
policy	O
evaluation	O
.	O
to	O
produce	O
each	O
successive	O
approximation	O
,	O
vk+1	O
from	O
vk	O
,	O
iterative	B
policy	O
evaluation	O
applies	O
the	O
same	O
operation	O
to	O
each	O
state	B
s	O
:	O
it	O
replaces	O
the	O
old	O
value	B
of	O
s	O
with	O
a	O
new	O
value	B
4.1.	O
policy	B
evaluation	I
(	O
prediction	B
)	O
75	O
obtained	O
from	O
the	O
old	O
values	O
of	O
the	O
successor	O
states	O
of	O
s	O
,	O
and	O
the	O
expected	O
immediate	O
rewards	O
,	O
along	O
all	O
the	O
one-step	O
transitions	O
possible	O
under	O
the	O
policy	B
being	O
evaluated	O
.	O
we	O
call	O
this	O
kind	O
of	O
operation	O
an	O
expected	B
update	I
.	O
each	O
iteration	O
of	O
iterative	O
policy	B
evaluation	I
updates	O
the	O
value	B
of	O
every	O
state	B
once	O
to	O
produce	O
the	O
new	O
approximate	B
value	O
function	O
vk+1	O
.	O
there	O
are	O
several	O
diﬀerent	O
kinds	O
of	O
expected	O
updates	O
,	O
depending	O
on	O
whether	O
a	O
state	B
(	O
as	O
here	O
)	O
or	O
a	O
state–action	O
pair	O
is	O
being	O
updated	O
,	O
and	O
depending	O
on	O
the	O
precise	O
way	O
the	O
estimated	O
values	O
of	O
the	O
successor	O
states	O
are	O
combined	O
.	O
all	O
the	O
updates	O
done	O
in	O
dp	O
algorithms	O
are	O
called	O
expected	O
updates	O
because	O
they	O
are	O
based	O
on	O
an	O
expectation	O
over	O
all	O
possible	O
next	O
states	O
rather	O
than	O
on	O
a	O
sample	O
next	O
state	B
.	O
the	O
nature	O
of	O
an	O
update	O
can	O
be	O
expressed	O
in	O
an	O
equation	O
,	O
as	O
above	O
,	O
or	O
in	O
a	O
backup	B
diagram	I
like	O
those	O
introduced	O
in	O
chapter	O
3.	O
for	O
example	O
,	O
the	O
backup	B
diagram	I
corresponding	O
to	O
the	O
expected	B
update	I
used	O
in	O
iterative	O
policy	B
evaluation	I
is	O
shown	O
on	O
page	O
59.	O
to	O
write	O
a	O
sequential	O
computer	O
program	O
to	O
implement	O
iterative	B
policy	O
evaluation	O
as	O
given	O
by	O
(	O
4.5	O
)	O
you	O
would	O
have	O
to	O
use	O
two	O
arrays	O
,	O
one	O
for	O
the	O
old	O
values	O
,	O
vk	O
(	O
s	O
)	O
,	O
and	O
one	O
for	O
the	O
new	O
values	O
,	O
vk+1	O
(	O
s	O
)	O
.	O
with	O
two	O
arrays	O
,	O
the	O
new	O
values	O
can	O
be	O
computed	O
one	O
by	O
one	O
from	O
the	O
old	O
values	O
without	O
the	O
old	O
values	O
being	O
changed	O
.	O
of	O
course	O
it	O
is	O
easier	O
to	O
use	O
one	O
array	O
and	O
update	O
the	O
values	O
“	O
in	O
place	O
,	O
”	O
that	O
is	O
,	O
with	O
each	O
new	O
value	B
immediately	O
overwriting	O
the	O
old	O
one	O
.	O
then	O
,	O
depending	O
on	O
the	O
order	O
in	O
which	O
the	O
states	O
are	O
updated	O
,	O
sometimes	O
new	O
values	O
are	O
used	O
instead	O
of	O
old	O
ones	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
4.5	O
)	O
.	O
this	O
in-place	O
algorithm	O
also	O
converges	O
to	O
vπ	O
;	O
in	O
fact	O
,	O
it	O
usually	O
converges	O
faster	O
than	O
the	O
two-array	O
version	O
,	O
as	O
you	O
might	O
expect	O
,	O
because	O
it	O
uses	O
new	O
data	O
as	O
soon	O
as	O
they	O
are	O
available	O
.	O
we	O
think	O
of	O
the	O
updates	O
as	O
being	O
done	O
in	O
a	O
sweep	O
through	O
the	O
state	B
space	O
.	O
for	O
the	O
in-place	O
algorithm	O
,	O
the	O
order	O
in	O
which	O
states	O
have	O
their	O
values	O
updated	O
during	O
the	O
sweep	O
has	O
a	O
signiﬁcant	O
inﬂuence	O
on	O
the	O
rate	O
of	O
convergence	O
.	O
we	O
usually	O
have	O
the	O
in-place	O
version	O
in	O
mind	O
when	O
we	O
think	O
of	O
dp	O
algorithms	O
.	O
a	O
complete	O
in-place	O
version	O
of	O
iterative	O
policy	B
evaluation	I
is	O
shown	O
in	O
pseudocode	O
in	O
the	O
box	O
below	O
.	O
note	O
how	O
it	O
handles	O
termination	O
.	O
formally	O
,	O
iterative	B
policy	O
evaluation	O
converges	O
only	O
in	O
the	O
limit	O
,	O
but	O
in	O
practice	O
it	O
must	O
be	O
halted	O
short	O
of	O
this	O
.	O
the	O
pseu-	O
docode	O
tests	O
the	O
quantity	O
maxs∈s	O
|vk+1	O
(	O
s	O
)	O
−	O
vk	O
(	O
s	O
)	O
|	O
after	O
each	O
sweep	O
and	O
stops	O
when	O
it	O
is	O
suﬃciently	O
small	O
.	O
iterative	B
policy	O
evaluation	O
,	O
for	O
estimating	O
v	O
≈	O
vπ	O
input	O
π	O
,	O
the	O
policy	B
to	O
be	O
evaluated	O
algorithm	O
parameter	O
:	O
a	O
small	O
threshold	O
θ	O
>	O
0	O
determining	O
accuracy	O
of	O
estimation	O
initialize	O
v	O
(	O
s	O
)	O
,	O
for	O
all	O
s	O
∈	O
s+	O
,	O
arbitrarily	O
except	O
that	O
v	O
(	O
terminal	O
)	O
=	O
0	O
loop	O
:	O
∆	O
←	O
0	O
loop	O
for	O
each	O
s	O
∈	O
s	O
:	O
until	O
∆	O
<	O
θ	O
v	O
←	O
v	O
(	O
s	O
)	O
∆	O
←	O
max	O
(	O
∆	O
,	O
|v	O
−	O
v	O
(	O
s	O
)	O
|	O
)	O
v	O
(	O
s	O
)	O
←	O
(	O
cid:80	O
)	O
a	O
π	O
(	O
a|s	O
)	O
(	O
cid:80	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:2	O
)	O
r	O
+	O
γv	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:3	O
)	O
76	O
chapter	O
4	O
:	O
dynamic	B
programming	I
example	O
4.1	O
consider	O
the	O
4×4	O
gridworld	O
shown	O
below	O
.	O
the	O
nonterminal	O
states	O
are	O
s	O
=	O
{	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
14	O
}	O
.	O
there	O
are	O
four	O
actions	O
possible	O
in	O
each	O
state	B
,	O
a	O
=	O
{	O
up	O
,	O
down	O
,	O
right	O
,	O
left	O
}	O
,	O
which	O
deterministically	O
cause	O
the	O
corresponding	O
state	B
transitions	O
,	O
except	O
that	O
actions	O
that	O
would	O
take	O
the	O
agent	O
oﬀ	O
the	O
grid	O
in	O
fact	O
leave	O
the	O
state	B
unchanged	O
.	O
thus	O
,	O
for	O
instance	O
,	O
p	O
(	O
6	O
,	O
−1|5	O
,	O
right	O
)	O
=	O
1	O
,	O
p	O
(	O
7	O
,	O
−1|7	O
,	O
right	O
)	O
=	O
1	O
,	O
and	O
p	O
(	O
10	O
,	O
r|5	O
,	O
right	O
)	O
=	O
0	O
for	O
all	O
r	O
∈	O
r.	O
this	O
is	O
an	O
undiscounted	O
,	O
episodic	O
task	O
.	O
the	O
reward	O
is	O
−1	O
on	O
all	O
transitions	O
until	O
the	O
terminal	O
state	B
is	O
reached	O
.	O
the	O
terminal	O
state	B
is	O
shaded	O
in	O
the	O
ﬁgure	O
(	O
although	O
it	O
is	O
shown	O
in	O
two	O
places	O
,	O
it	O
is	O
formally	O
one	O
state	B
)	O
.	O
the	O
expected	O
reward	O
function	O
is	O
thus	O
r	O
(	O
s	O
,	O
a	O
,	O
s	O
(	O
cid:48	O
)	O
)	O
=	O
−1	O
for	O
all	O
states	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
and	O
actions	O
a.	O
suppose	O
the	O
agent	O
follows	O
the	O
equiprobable	O
random	O
policy	O
(	O
all	O
actions	O
equally	O
likely	O
)	O
.	O
the	O
left	O
side	O
of	O
figure	O
4.1	O
shows	O
the	O
sequence	O
of	O
value	O
functions	O
{	O
vk	O
}	O
computed	O
by	O
iterative	B
policy	O
evaluation	O
.	O
the	O
ﬁnal	O
estimate	O
is	O
in	O
fact	O
vπ	O
,	O
which	O
in	O
this	O
case	O
gives	O
for	O
each	O
state	B
the	O
negation	O
of	O
the	O
expected	O
number	O
of	O
steps	O
from	O
that	O
state	B
until	O
termination	O
.	O
exercise	O
4.1	O
in	O
example	O
4.1	O
,	O
if	O
π	O
is	O
the	O
equiprobable	O
random	O
policy	O
,	O
what	O
is	O
qπ	O
(	O
11	O
,	O
down	O
)	O
?	O
(	O
cid:3	O
)	O
what	O
is	O
qπ	O
(	O
7	O
,	O
down	O
)	O
?	O
exercise	O
4.2	O
in	O
example	O
4.1	O
,	O
suppose	O
a	O
new	O
state	B
15	O
is	O
added	O
to	O
the	O
gridworld	O
just	O
below	O
state	B
13	O
,	O
and	O
its	O
actions	O
,	O
left	O
,	O
up	O
,	O
right	O
,	O
and	O
down	O
,	O
take	O
the	O
agent	O
to	O
states	O
12	O
,	O
13	O
,	O
14	O
,	O
and	O
15	O
,	O
respectively	O
.	O
assume	O
that	O
the	O
transitions	O
from	O
the	O
original	O
states	O
are	O
unchanged	O
.	O
what	O
,	O
then	O
,	O
is	O
vπ	O
(	O
15	O
)	O
for	O
the	O
equiprobable	O
random	O
policy	O
?	O
now	O
suppose	O
the	O
dynamics	O
of	O
state	O
13	O
are	O
also	O
changed	O
,	O
such	O
that	O
action	B
down	O
from	O
state	B
13	O
takes	O
the	O
agent	O
to	O
the	O
new	O
state	B
15.	O
what	O
is	O
vπ	O
(	O
15	O
)	O
for	O
the	O
equiprobable	O
random	O
policy	O
in	O
this	O
(	O
cid:3	O
)	O
case	O
?	O
exercise	O
4.3	O
what	O
are	O
the	O
equations	O
analogous	O
to	O
(	O
4.3	O
)	O
,	O
(	O
4.4	O
)	O
,	O
and	O
(	O
4.5	O
)	O
for	O
the	O
action-	O
value	B
function	I
qπ	O
and	O
its	O
successive	O
approximation	O
by	O
a	O
sequence	O
of	O
functions	O
q0	O
,	O
q1	O
,	O
q2	O
,	O
.	O
.	O
.	O
(	O
cid:3	O
)	O
?	O
4.2	O
policy	B
improvement	I
our	O
reason	O
for	O
computing	O
the	O
value	B
function	I
for	O
a	O
policy	B
is	O
to	O
help	O
ﬁnd	O
better	O
policies	O
.	O
suppose	O
we	O
have	O
determined	O
the	O
value	B
function	I
vπ	O
for	O
an	O
arbitrary	O
deterministic	O
policy	B
π.	O
for	O
some	O
state	B
s	O
we	O
would	O
like	O
to	O
know	O
whether	O
or	O
not	O
we	O
should	O
change	O
the	O
policy	B
to	O
deterministically	O
choose	O
an	O
action	B
a	O
(	O
cid:54	O
)	O
=	O
π	O
(	O
s	O
)	O
.	O
we	O
know	O
how	O
good	O
it	O
is	O
to	O
follow	O
the	O
current	O
policy	B
from	O
s—that	O
is	O
vπ	O
(	O
s	O
)	O
—but	O
would	O
it	O
be	O
better	O
or	O
worse	O
to	O
change	O
to	O
the	O
new	O
policy	B
?	O
one	O
way	O
to	O
answer	O
this	O
question	O
is	O
to	O
consider	O
selecting	O
a	O
in	O
s	O
and	O
thereafter	O
actionsr	O
=	O
!	O
1on	O
all	O
transitions1234567891011121314rt= 1	O
4.2.	O
policy	B
improvement	I
77	O
figure	O
4.1	O
:	O
convergence	O
of	O
iterative	O
policy	B
evaluation	I
on	O
a	O
small	O
gridworld	O
.	O
the	O
left	O
column	O
is	O
the	O
sequence	O
of	O
approximations	O
of	O
the	O
state-value	O
function	O
for	O
the	O
random	O
policy	O
(	O
all	O
actions	O
equal	O
)	O
.	O
the	O
right	O
column	O
is	O
the	O
sequence	O
of	O
greedy	O
policies	O
corresponding	O
to	O
the	O
value	B
function	I
estimates	O
(	O
arrows	O
are	O
shown	O
for	O
all	O
actions	O
achieving	O
the	O
maximum	O
)	O
.	O
the	O
last	O
policy	B
is	O
guar-	O
anteed	O
only	O
to	O
be	O
an	O
improvement	O
over	O
the	O
random	O
policy	O
,	O
but	O
in	O
this	O
case	O
it	O
,	O
and	O
all	O
policies	O
after	O
the	O
third	O
iteration	O
,	O
are	O
optimal	O
.	O
0.0	O
0.0	O
0.0	O
0.0	O
0.0	O
0.0	O
0.0	O
0.0	O
0.0	O
0.0	O
0.0	O
0.0	O
0.0	O
0.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.7-2.0-2.0-1.7-2.0-2.0-2.0-2.0-2.0-2.0-1.7-2.0-2.0-1.7-2.4-2.9-3.0-2.4-2.9-3.0-2.9-2.9-3.0-2.9-2.4-3.0-2.9-2.4-6.1-8.4-9.0-6.1-7.7-8.4-8.4-8.4-8.4-7.7-6.1-9.0-8.4-6.1-14.-20.-22.-14.-18.-20.-20.-20.-20.-18.-14.-22.-20.-14.vk	O
for	O
therandom	O
policygreedy	O
policyw.r.t	O
.	O
vkk	O
=	O
0k	O
=	O
1k	O
=	O
2k	O
=	O
10k	O
=	O
!	O
k	O
=	O
3optimal	O
policyrandom	O
policy	B
0.0	O
0.0	O
0.0	O
0.0	O
0.0	O
0.0	O
0.0	O
0.0	O
0.0	O
0.0	O
0.0	O
0.0vk	O
for	O
therandom	O
policyvkgreedy	O
policy	B
w.r.t.vk	O
78	O
chapter	O
4	O
:	O
dynamic	B
programming	I
following	O
the	O
existing	O
policy	B
,	O
π.	O
the	O
value	B
of	O
this	O
way	O
of	O
behaving	O
is	O
qπ	O
(	O
s	O
,	O
a	O
)	O
.	O
=	O
e	O
[	O
rt+1	O
+	O
γvπ	O
(	O
st+1	O
)	O
|	O
st	O
=	O
s	O
,	O
at	O
=	O
a	O
]	O
=	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:104	O
)	O
r	O
+	O
γvπ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
.	O
(	O
4.6	O
)	O
the	O
key	O
criterion	O
is	O
whether	O
this	O
is	O
greater	O
than	O
or	O
less	O
than	O
vπ	O
(	O
s	O
)	O
.	O
if	O
it	O
is	O
greater—that	O
is	O
,	O
if	O
it	O
is	O
better	O
to	O
select	O
a	O
once	O
in	O
s	O
and	O
thereafter	O
follow	O
π	O
than	O
it	O
would	O
be	O
to	O
follow	O
π	O
all	O
the	O
time—then	O
one	O
would	O
expect	O
it	O
to	O
be	O
better	O
still	O
to	O
select	O
a	O
every	O
time	O
s	O
is	O
encountered	O
,	O
and	O
that	O
the	O
new	O
policy	B
would	O
in	O
fact	O
be	O
a	O
better	O
one	O
overall	O
.	O
that	O
this	O
is	O
true	O
is	O
a	O
special	O
case	O
of	O
a	O
general	O
result	O
called	O
the	O
policy	B
improvement	I
theorem	O
.	O
let	O
π	O
and	O
π	O
(	O
cid:48	O
)	O
be	O
any	O
pair	O
of	O
deterministic	O
policies	O
such	O
that	O
,	O
for	O
all	O
s	O
∈	O
s	O
,	O
qπ	O
(	O
s	O
,	O
π	O
(	O
cid:48	O
)	O
(	O
s	O
)	O
)	O
≥	O
vπ	O
(	O
s	O
)	O
.	O
(	O
4.7	O
)	O
then	O
the	O
policy	B
π	O
(	O
cid:48	O
)	O
must	O
be	O
as	O
good	O
as	O
,	O
or	O
better	O
than	O
,	O
π.	O
that	O
is	O
,	O
it	O
must	O
obtain	O
greater	O
or	O
equal	O
expected	O
return	O
from	O
all	O
states	O
s	O
∈	O
s	O
:	O
vπ	O
(	O
cid:48	O
)	O
(	O
s	O
)	O
≥	O
vπ	O
(	O
s	O
)	O
.	O
(	O
4.8	O
)	O
moreover	O
,	O
if	O
there	O
is	O
strict	O
inequality	O
of	O
(	O
4.7	O
)	O
at	O
any	O
state	B
,	O
then	O
there	O
must	O
be	O
strict	O
inequality	O
of	O
(	O
4.8	O
)	O
at	O
at	O
least	O
one	O
state	B
.	O
this	O
result	O
applies	O
in	O
particular	O
to	O
the	O
two	O
policies	O
that	O
we	O
considered	O
in	O
the	O
previous	O
paragraph	O
,	O
an	O
original	O
deterministic	O
policy	B
,	O
π	O
,	O
and	O
a	O
changed	O
policy	B
,	O
π	O
(	O
cid:48	O
)	O
,	O
that	O
is	O
identical	O
to	O
π	O
except	O
that	O
π	O
(	O
cid:48	O
)	O
(	O
s	O
)	O
=	O
a	O
(	O
cid:54	O
)	O
=	O
π	O
(	O
s	O
)	O
.	O
obviously	O
,	O
(	O
4.7	O
)	O
holds	O
at	O
all	O
states	O
other	O
than	O
s.	O
thus	O
,	O
if	O
qπ	O
(	O
s	O
,	O
a	O
)	O
>	O
vπ	O
(	O
s	O
)	O
,	O
then	O
the	O
changed	O
policy	B
is	O
indeed	O
better	O
than	O
π.	O
the	O
idea	O
behind	O
the	O
proof	B
of	O
the	O
policy	B
improvement	I
theorem	O
is	O
easy	O
to	O
understand	O
.	O
starting	O
from	O
(	O
4.7	O
)	O
,	O
we	O
keep	O
expanding	O
the	O
qπ	O
side	O
with	O
(	O
4.6	O
)	O
and	O
reapplying	O
(	O
4.7	O
)	O
until	O
we	O
get	O
vπ	O
(	O
cid:48	O
)	O
(	O
s	O
)	O
:	O
(	O
by	O
(	O
4.6	O
)	O
)	O
vπ	O
(	O
s	O
)	O
≤	O
qπ	O
(	O
s	O
,	O
π	O
(	O
cid:48	O
)	O
(	O
s	O
)	O
)	O
=	O
e	O
[	O
rt+1	O
+	O
γvπ	O
(	O
st+1	O
)	O
|	O
st	O
=	O
s	O
,	O
at	O
=	O
π	O
(	O
cid:48	O
)	O
(	O
s	O
)	O
]	O
=	O
eπ	O
(	O
cid:48	O
)	O
[	O
rt+1	O
+	O
γvπ	O
(	O
st+1	O
)	O
|	O
st	O
=	O
s	O
]	O
≤	O
eπ	O
(	O
cid:48	O
)	O
[	O
rt+1	O
+	O
γqπ	O
(	O
st+1	O
,	O
π	O
(	O
cid:48	O
)	O
(	O
st+1	O
)	O
)	O
|	O
st	O
=	O
s	O
]	O
=	O
eπ	O
(	O
cid:48	O
)	O
[	O
rt+1	O
+	O
γeπ	O
(	O
cid:48	O
)	O
[	O
rt+2	O
+	O
γvπ	O
(	O
st+2	O
)	O
|st+1	O
]	O
|	O
st	O
=	O
s	O
]	O
=	O
eπ	O
(	O
cid:48	O
)	O
(	O
cid:2	O
)	O
rt+1	O
+	O
γrt+2	O
+	O
γ2vπ	O
(	O
st+2	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
st	O
=	O
s	O
(	O
cid:3	O
)	O
≤	O
eπ	O
(	O
cid:48	O
)	O
(	O
cid:2	O
)	O
rt+1	O
+	O
γrt+2	O
+	O
γ2rt+3	O
+	O
γ3vπ	O
(	O
st+3	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
st	O
=	O
s	O
(	O
cid:3	O
)	O
≤	O
eπ	O
(	O
cid:48	O
)	O
(	O
cid:2	O
)	O
rt+1	O
+	O
γrt+2	O
+	O
γ2rt+3	O
+	O
γ3rt+4	O
+	O
···	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
st	O
=	O
s	O
(	O
cid:3	O
)	O
...	O
=	O
vπ	O
(	O
cid:48	O
)	O
(	O
s	O
)	O
.	O
so	O
far	O
we	O
have	O
seen	O
how	O
,	O
given	O
a	O
policy	B
and	O
its	O
value	B
function	I
,	O
we	O
can	O
easily	O
evaluate	O
a	O
change	O
in	O
the	O
policy	O
at	O
a	O
single	O
state	B
to	O
a	O
particular	O
action	B
.	O
it	O
is	O
a	O
natural	O
extension	O
4.2.	O
policy	B
improvement	I
79	O
to	O
consider	O
changes	O
at	O
all	O
states	O
and	O
to	O
all	O
possible	O
actions	O
,	O
selecting	O
at	O
each	O
state	B
the	O
action	B
that	O
appears	O
best	O
according	O
to	O
qπ	O
(	O
s	O
,	O
a	O
)	O
.	O
in	O
other	O
words	O
,	O
to	O
consider	O
the	O
new	O
greedy	O
policy	O
,	O
π	O
(	O
cid:48	O
)	O
,	O
given	O
by	O
π	O
(	O
cid:48	O
)	O
(	O
s	O
)	O
.	O
=	O
argmax	O
a	O
=	O
argmax	O
=	O
argmax	O
qπ	O
(	O
s	O
,	O
a	O
)	O
e	O
[	O
rt+1	O
+	O
γvπ	O
(	O
st+1	O
)	O
|	O
st	O
=	O
s	O
,	O
at	O
=	O
a	O
]	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:104	O
)	O
r	O
+	O
γvπ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
,	O
a	O
a	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
(	O
4.9	O
)	O
where	O
argmaxa	O
denotes	O
the	O
value	B
of	O
a	O
at	O
which	O
the	O
expression	O
that	O
follows	O
is	O
maximized	O
(	O
with	O
ties	O
broken	O
arbitrarily	O
)	O
.	O
the	O
greedy	O
policy	O
takes	O
the	O
action	B
that	O
looks	O
best	O
in	O
the	O
short	O
term—after	O
one	O
step	O
of	O
lookahead—according	O
to	O
vπ	O
.	O
by	O
construction	O
,	O
the	O
greedy	O
policy	O
meets	O
the	O
conditions	O
of	O
the	O
policy	B
improvement	I
theorem	O
(	O
4.7	O
)	O
,	O
so	O
we	O
know	O
that	O
it	O
is	O
as	O
good	O
as	O
,	O
or	O
better	O
than	O
,	O
the	O
original	O
policy	B
.	O
the	O
process	O
of	O
making	O
a	O
new	O
policy	B
that	O
improves	O
on	O
an	O
original	O
policy	B
,	O
by	O
making	O
it	O
greedy	O
with	O
respect	O
to	O
the	O
value	B
function	I
of	O
the	O
original	O
policy	B
,	O
is	O
called	O
policy	B
improvement	I
.	O
suppose	O
the	O
new	O
greedy	O
policy	O
,	O
π	O
(	O
cid:48	O
)	O
,	O
is	O
as	O
good	O
as	O
,	O
but	O
not	O
better	O
than	O
,	O
the	O
old	O
policy	B
π.	O
then	O
vπ	O
=	O
vπ	O
(	O
cid:48	O
)	O
,	O
and	O
from	O
(	O
4.9	O
)	O
it	O
follows	O
that	O
for	O
all	O
s	O
∈	O
s	O
:	O
vπ	O
(	O
cid:48	O
)	O
(	O
s	O
)	O
=	O
max	O
=	O
max	O
a	O
e	O
[	O
rt+1	O
+	O
γvπ	O
(	O
cid:48	O
)	O
(	O
st+1	O
)	O
|	O
st	O
=	O
s	O
,	O
at	O
=	O
a	O
]	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:104	O
)	O
r	O
+	O
γvπ	O
(	O
cid:48	O
)	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
.	O
a	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
but	O
this	O
is	O
the	O
same	O
as	O
the	O
bellman	O
optimality	O
equation	O
(	O
4.1	O
)	O
,	O
and	O
therefore	O
,	O
vπ	O
(	O
cid:48	O
)	O
must	O
be	O
v∗	O
,	O
and	O
both	O
π	O
and	O
π	O
(	O
cid:48	O
)	O
must	O
be	O
optimal	O
policies	O
.	O
policy	B
improvement	I
thus	O
must	O
give	O
us	O
a	O
strictly	O
better	O
policy	B
except	O
when	O
the	O
original	O
policy	B
is	O
already	O
optimal	O
.	O
so	O
far	O
in	O
this	O
section	O
we	O
have	O
considered	O
the	O
special	O
case	O
of	O
deterministic	O
policies	O
.	O
in	O
the	O
general	O
case	O
,	O
a	O
stochastic	O
policy	O
π	O
speciﬁes	O
probabilities	O
,	O
π	O
(	O
a|s	O
)	O
,	O
for	O
taking	O
each	O
action	B
,	O
a	O
,	O
in	O
each	O
state	B
,	O
s.	O
we	O
will	O
not	O
go	O
through	O
the	O
details	O
,	O
but	O
in	O
fact	O
all	O
the	O
ideas	O
of	O
this	O
section	O
extend	O
easily	O
to	O
stochastic	O
policies	O
.	O
in	O
particular	O
,	O
the	O
policy	B
improvement	I
theorem	O
carries	O
through	O
as	O
stated	O
for	O
the	O
stochastic	O
case	O
.	O
in	O
addition	O
,	O
if	O
there	O
are	O
ties	O
in	O
policy	O
improvement	O
steps	O
such	O
as	O
(	O
4.9	O
)	O
—that	O
is	O
,	O
if	O
there	O
are	O
several	O
actions	O
at	O
which	O
the	O
maximum	O
is	O
achieved—then	O
in	O
the	O
stochastic	O
case	O
we	O
need	O
not	O
select	O
a	O
single	O
action	B
from	O
among	O
them	O
.	O
instead	O
,	O
each	O
maximizing	O
action	B
can	O
be	O
given	O
a	O
portion	O
of	O
the	O
probability	O
of	O
being	O
selected	O
in	O
the	O
new	O
greedy	O
policy	O
.	O
any	O
apportioning	O
scheme	O
is	O
allowed	O
as	O
long	O
as	O
all	O
submaximal	O
actions	O
are	O
given	O
zero	O
probability	O
.	O
the	O
last	O
row	O
of	O
figure	O
4.1	O
shows	O
an	O
example	O
of	O
policy	O
improvement	O
for	O
stochastic	O
policies	O
.	O
here	O
the	O
original	O
policy	B
,	O
π	O
,	O
is	O
the	O
equiprobable	O
random	O
policy	O
,	O
and	O
the	O
new	O
policy	B
,	O
π	O
(	O
cid:48	O
)	O
,	O
is	O
greedy	O
with	O
respect	O
to	O
vπ	O
.	O
the	O
value	B
function	I
vπ	O
is	O
shown	O
in	O
the	O
bottom-	O
left	O
diagram	O
and	O
the	O
set	O
of	O
possible	O
π	O
(	O
cid:48	O
)	O
is	O
shown	O
in	O
the	O
bottom-right	O
diagram	O
.	O
the	O
states	O
with	O
multiple	O
arrows	O
in	O
the	O
π	O
(	O
cid:48	O
)	O
diagram	O
are	O
those	O
in	O
which	O
several	O
actions	O
achieve	O
the	O
maximum	O
in	O
(	O
4.9	O
)	O
;	O
any	O
apportionment	O
of	O
probability	O
among	O
these	O
actions	O
is	O
permitted	O
.	O
the	O
value	B
function	I
of	O
any	O
such	O
policy	B
,	O
vπ	O
(	O
cid:48	O
)	O
(	O
s	O
)	O
,	O
can	O
be	O
seen	O
by	O
inspection	O
to	O
be	O
either	O
−1	O
,	O
−2	O
,	O
or	O
−3	O
at	O
all	O
states	O
,	O
s	O
∈	O
s	O
,	O
whereas	O
vπ	O
(	O
s	O
)	O
is	O
at	O
most	O
−14	O
.	O
thus	O
,	O
vπ	O
(	O
cid:48	O
)	O
(	O
s	O
)	O
≥	O
vπ	O
(	O
s	O
)	O
,	O
for	O
all	O
80	O
chapter	O
4	O
:	O
dynamic	B
programming	I
s	O
∈	O
s	O
,	O
illustrating	O
policy	B
improvement	I
.	O
although	O
in	O
this	O
case	O
the	O
new	O
policy	B
π	O
(	O
cid:48	O
)	O
happens	O
to	O
be	O
optimal	O
,	O
in	O
general	O
only	O
an	O
improvement	O
is	O
guaranteed	O
.	O
4.3	O
policy	B
iteration	I
once	O
a	O
policy	B
,	O
π	O
,	O
has	O
been	O
improved	O
using	O
vπ	O
to	O
yield	O
a	O
better	O
policy	B
,	O
π	O
(	O
cid:48	O
)	O
,	O
we	O
can	O
then	O
compute	O
vπ	O
(	O
cid:48	O
)	O
and	O
improve	O
it	O
again	O
to	O
yield	O
an	O
even	O
better	O
π	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
.	O
we	O
can	O
thus	O
obtain	O
a	O
sequence	O
of	O
monotonically	O
improving	O
policies	O
and	O
value	O
functions	O
:	O
π0	O
i−→	O
π1	O
i−→	O
π2	O
e−→	O
v∗	O
,	O
e−→	O
vπ1	O
e−→	O
vπ0	O
e−→	O
···	O
e−→	O
denotes	O
a	O
policy	B
evaluation	I
and	O
i−→	O
π∗	O
i−→	O
denotes	O
a	O
policy	B
improvement	I
.	O
each	O
where	O
policy	B
is	O
guaranteed	O
to	O
be	O
a	O
strict	O
improvement	O
over	O
the	O
previous	O
one	O
(	O
unless	O
it	O
is	O
already	O
optimal	O
)	O
.	O
because	O
a	O
ﬁnite	O
mdp	O
has	O
only	O
a	O
ﬁnite	O
number	O
of	O
policies	O
,	O
this	O
process	O
must	O
converge	O
to	O
an	O
optimal	O
policy	O
and	O
optimal	O
value	B
function	I
in	O
a	O
ﬁnite	O
number	O
of	O
iterations	O
.	O
this	O
way	O
of	O
ﬁnding	O
an	O
optimal	O
policy	O
is	O
called	O
policy	B
iteration	I
.	O
a	O
complete	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
below	O
.	O
note	O
that	O
each	O
policy	B
evaluation	I
,	O
itself	O
an	O
iterative	B
computation	O
,	O
is	O
started	O
with	O
the	O
value	B
function	I
for	O
the	O
previous	O
policy	B
.	O
this	O
typically	O
results	O
in	O
a	O
great	O
increase	O
in	O
the	O
speed	O
of	O
convergence	O
of	O
policy	O
evaluation	O
(	O
presumably	O
because	O
the	O
value	B
function	I
changes	O
little	O
from	O
one	O
policy	B
to	O
the	O
next	O
)	O
.	O
policy	B
iteration	I
(	O
using	O
iterative	B
policy	O
evaluation	O
)	O
for	O
estimating	O
π	O
≈	O
π∗	O
1.	O
initialization	O
v	O
(	O
s	O
)	O
∈	O
r	O
and	O
π	O
(	O
s	O
)	O
∈	O
a	O
(	O
s	O
)	O
arbitrarily	O
for	O
all	O
s	O
∈	O
s	O
2.	O
policy	B
evaluation	I
loop	O
:	O
∆	O
←	O
0	O
loop	O
for	O
each	O
s	O
∈	O
s	O
:	O
v	O
←	O
v	O
(	O
s	O
)	O
∆	O
←	O
max	O
(	O
∆	O
,	O
|v	O
−	O
v	O
(	O
s	O
)	O
|	O
)	O
v	O
(	O
s	O
)	O
←	O
(	O
cid:80	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
π	O
(	O
s	O
)	O
)	O
(	O
cid:2	O
)	O
r	O
+	O
γv	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:3	O
)	O
until	O
∆	O
<	O
θ	O
(	O
a	O
small	O
positive	O
number	O
determining	O
the	O
accuracy	O
of	O
estimation	O
)	O
3.	O
policy	B
improvement	I
policy-stable	O
←	O
true	O
for	O
each	O
s	O
∈	O
s	O
:	O
old-action	O
←	O
π	O
(	O
s	O
)	O
if	O
old-action	O
(	O
cid:54	O
)	O
=	O
π	O
(	O
s	O
)	O
,	O
then	O
policy-stable	O
←	O
f	O
alse	O
π	O
(	O
s	O
)	O
←	O
argmaxa	O
(	O
cid:80	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:2	O
)	O
r	O
+	O
γv	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:3	O
)	O
if	O
policy-stable	O
,	O
then	O
stop	O
and	O
return	O
v	O
≈	O
v∗	O
and	O
π	O
≈	O
π∗	O
;	O
else	O
go	O
to	O
2	O
4.3.	O
policy	B
iteration	I
81	O
policy	B
iteration	I
often	O
converges	O
in	O
surprisingly	O
few	O
iterations	O
.	O
this	O
is	O
illustrated	O
by	O
the	O
example	O
in	O
figure	O
4.1.	O
the	O
bottom-left	O
diagram	O
shows	O
the	O
value	B
function	I
for	O
the	O
equiprobable	O
random	O
policy	O
,	O
and	O
the	O
bottom-right	O
diagram	O
shows	O
a	O
greedy	O
policy	O
for	O
this	O
value	B
function	I
.	O
the	O
policy	B
improvement	I
theorem	O
assures	O
us	O
that	O
these	O
policies	O
are	O
better	O
than	O
the	O
original	O
random	O
policy	O
.	O
in	O
this	O
case	O
,	O
however	O
,	O
these	O
policies	O
are	O
not	O
just	O
better	O
,	O
but	O
optimal	O
,	O
proceeding	O
to	O
the	O
terminal	O
states	O
in	O
the	O
minimum	O
number	O
of	O
steps	O
.	O
in	O
this	O
example	O
,	O
policy	B
iteration	I
would	O
ﬁnd	O
the	O
optimal	O
policy	O
after	O
just	O
one	O
iteration	O
.	O
exercise	O
4.4	O
the	O
policy	B
iteration	I
algorithm	O
on	O
the	O
previous	O
page	O
has	O
a	O
subtle	O
bug	O
in	O
that	O
it	O
may	O
never	O
terminate	O
if	O
the	O
policy	B
continually	O
switches	O
between	O
two	O
or	O
more	O
policies	O
that	O
are	O
equally	O
good	O
.	O
this	O
is	O
ok	O
for	O
pedagogy	O
,	O
but	O
not	O
for	O
actual	O
use	O
.	O
modify	O
the	O
(	O
cid:3	O
)	O
pseudocode	O
so	O
that	O
convergence	O
is	O
guaranteed	O
.	O
exercise	O
4.5	O
how	O
would	O
policy	B
iteration	I
be	O
deﬁned	O
for	B
action	I
values	I
?	O
give	O
a	O
complete	O
algorithm	O
for	O
computing	O
q∗	O
,	O
analogous	O
to	O
that	O
on	O
page	O
80	O
for	O
computing	O
v∗	O
.	O
please	O
pay	O
special	O
attention	O
to	O
this	O
exercise	O
,	O
because	O
the	O
ideas	O
involved	O
will	O
be	O
used	O
throughout	O
the	O
(	O
cid:3	O
)	O
rest	O
of	O
the	O
book	O
.	O
exercise	O
4.6	O
suppose	O
you	O
are	O
restricted	O
to	O
considering	O
only	O
policies	O
that	O
are	O
ε-soft	O
,	O
meaning	O
that	O
the	O
probability	O
of	O
selecting	O
each	O
action	B
in	O
each	O
state	B
,	O
s	O
,	O
is	O
at	O
least	O
ε/|a	O
(	O
s	O
)	O
|	O
.	O
describe	O
qualitatively	O
the	O
changes	O
that	O
would	O
be	O
required	O
in	O
each	O
of	O
the	O
steps	O
3	O
,	O
2	O
,	O
and	O
(	O
cid:3	O
)	O
1	O
,	O
in	O
that	O
order	O
,	O
of	O
the	O
policy	B
iteration	I
algorithm	O
for	O
v∗	O
(	O
page	O
80	O
)	O
.	O
example	O
4.2	O
:	O
jack	O
’	O
s	O
car	O
rental	O
jack	O
manages	O
two	O
locations	O
for	O
a	O
nationwide	O
car	O
rental	O
company	O
.	O
each	O
day	O
,	O
some	O
number	O
of	O
customers	O
arrive	O
at	O
each	O
location	O
to	O
rent	O
cars	O
.	O
if	O
jack	O
has	O
a	O
car	O
available	O
,	O
he	O
rents	O
it	O
out	O
and	O
is	O
credited	O
$	O
10	O
by	O
the	O
national	O
company	O
.	O
if	O
he	O
is	O
out	O
of	O
cars	O
at	O
that	O
location	O
,	O
then	O
the	O
business	O
is	O
lost	O
.	O
cars	O
become	O
available	O
for	O
renting	O
the	O
day	O
after	O
they	O
are	O
returned	O
.	O
to	O
help	O
ensure	O
that	O
cars	O
are	O
available	O
where	O
they	O
are	O
needed	O
,	O
jack	O
can	O
move	O
them	O
between	O
the	O
two	O
locations	O
overnight	O
,	O
at	O
a	O
cost	O
of	O
$	O
2	O
per	O
car	O
moved	O
.	O
we	O
assume	O
that	O
the	O
number	O
of	O
cars	O
requested	O
and	O
returned	O
at	O
each	O
location	O
are	O
poisson	O
random	O
variables	O
,	O
meaning	O
that	O
the	O
probability	O
that	O
the	O
number	O
is	O
n	O
is	O
λn	O
n	O
!	O
e−λ	O
,	O
where	O
λ	O
is	O
the	O
expected	O
number	O
.	O
suppose	O
λ	O
is	O
3	O
and	O
4	O
for	O
rental	O
requests	O
at	O
the	O
ﬁrst	O
and	O
second	O
locations	O
and	O
3	O
and	O
2	O
for	O
returns	O
.	O
to	O
simplify	O
the	O
problem	O
slightly	O
,	O
we	O
assume	O
that	O
there	O
can	O
be	O
no	O
more	O
than	O
20	O
cars	O
at	O
each	O
location	O
(	O
any	O
additional	O
cars	O
are	O
returned	O
to	O
the	O
nationwide	O
company	O
,	O
and	O
thus	O
disappear	O
from	O
the	O
problem	O
)	O
and	O
a	O
maximum	O
of	O
ﬁve	O
cars	O
can	O
be	O
moved	O
from	O
one	O
location	O
to	O
the	O
other	O
in	O
one	O
night	O
.	O
we	O
take	O
the	O
discount	O
rate	O
to	O
be	O
γ	O
=	O
0.9	O
and	O
formulate	O
this	O
as	O
a	O
continuing	O
ﬁnite	O
mdp	O
,	O
where	O
the	O
time	O
steps	O
are	O
days	O
,	O
the	O
state	B
is	O
the	O
number	O
of	O
cars	O
at	O
each	O
location	O
at	O
the	O
end	O
of	O
the	O
day	O
,	O
and	O
the	O
actions	O
are	O
the	O
net	O
numbers	O
of	O
cars	O
moved	O
between	O
the	O
two	O
locations	O
overnight	O
.	O
figure	O
4.2	O
shows	O
the	O
sequence	O
of	O
policies	O
found	O
by	O
policy	B
iteration	I
starting	O
from	O
the	O
policy	B
that	O
never	O
moves	O
any	O
cars	O
.	O
exercise	O
4.7	O
(	O
programming	O
)	O
write	O
a	O
program	O
for	O
policy	O
iteration	O
and	O
re-solve	O
jack	O
’	O
s	O
car	O
rental	O
problem	O
with	O
the	O
following	O
changes	O
.	O
one	O
of	O
jack	O
’	O
s	O
employees	O
at	O
the	O
ﬁrst	O
location	O
rides	O
a	O
bus	O
home	O
each	O
night	O
and	O
lives	O
near	O
the	O
second	O
location	O
.	O
she	O
is	O
happy	O
to	O
shuttle	O
one	O
car	O
to	O
the	O
second	O
location	O
for	O
free	O
.	O
each	O
additional	O
car	O
still	O
costs	O
$	O
2	O
,	O
as	O
do	O
all	O
cars	O
moved	O
in	O
the	O
other	O
direction	O
.	O
in	O
addition	O
,	O
jack	O
has	O
limited	O
parking	O
space	O
at	O
each	O
location	O
.	O
if	O
more	O
than	O
10	O
cars	O
are	O
kept	O
overnight	O
at	O
a	O
location	O
(	O
after	O
any	O
moving	O
of	O
cars	O
)	O
,	O
then	O
an	O
82	O
chapter	O
4	O
:	O
dynamic	B
programming	I
figure	O
4.2	O
:	O
the	O
sequence	O
of	O
policies	O
found	O
by	O
policy	B
iteration	I
on	O
jack	O
’	O
s	O
car	O
rental	O
problem	O
,	O
and	O
the	O
ﬁnal	O
state-value	O
function	O
.	O
the	O
ﬁrst	O
ﬁve	O
diagrams	O
show	O
,	O
for	O
each	O
number	O
of	O
cars	O
at	O
each	O
location	O
at	O
the	O
end	O
of	O
the	O
day	O
,	O
the	O
number	O
of	O
cars	O
to	O
be	O
moved	O
from	O
the	O
ﬁrst	O
location	O
to	O
the	O
second	O
(	O
negative	O
numbers	O
indicate	O
transfers	O
from	O
the	O
second	O
location	O
to	O
the	O
ﬁrst	O
)	O
.	O
each	O
successive	O
policy	B
is	O
a	O
strict	O
improvement	O
over	O
the	O
previous	O
policy	B
,	O
and	O
the	O
last	O
policy	B
is	O
optimal	O
.	O
additional	O
cost	O
of	O
$	O
4	O
must	O
be	O
incurred	O
to	O
use	O
a	O
second	O
parking	O
lot	O
(	O
independent	O
of	O
how	O
many	O
cars	O
are	O
kept	O
there	O
)	O
.	O
these	O
sorts	O
of	O
nonlinearities	O
and	O
arbitrary	O
dynamics	O
often	O
occur	O
in	O
real	O
problems	O
and	O
can	O
not	O
easily	O
be	O
handled	O
by	O
optimization	O
methods	O
other	O
than	O
dynamic	B
programming	I
.	O
to	O
check	O
your	O
program	O
,	O
ﬁrst	O
replicate	O
the	O
results	O
given	O
for	O
the	O
original	O
problem	O
.	O
if	O
your	O
computer	O
is	O
too	O
slow	O
for	O
the	O
full	O
problem	O
,	O
cut	O
all	O
the	O
numbers	O
(	O
cid:3	O
)	O
of	O
cars	O
in	O
half	O
.	O
4.4	O
value	B
iteration	I
one	O
drawback	O
to	O
policy	B
iteration	I
is	O
that	O
each	O
of	O
its	O
iterations	O
involves	O
policy	B
evaluation	I
,	O
which	O
may	O
itself	O
be	O
a	O
protracted	O
iterative	B
computation	O
requiring	O
multiple	O
sweeps	B
through	O
the	O
state	B
set	O
.	O
if	O
policy	B
evaluation	I
is	O
done	O
iteratively	O
,	O
then	O
convergence	O
exactly	O
to	O
vπ	O
occurs	O
only	O
in	O
the	O
limit	O
.	O
must	O
we	O
wait	O
for	O
exact	O
convergence	O
,	O
or	O
can	O
we	O
stop	O
short	O
of	O
that	O
?	O
the	O
example	O
in	O
figure	O
4.1	O
certainly	O
suggests	O
that	O
it	O
may	O
be	O
possible	O
to	O
truncate	O
policy	B
evaluation	I
.	O
in	O
that	O
example	O
,	O
policy	B
evaluation	I
iterations	O
beyond	O
the	O
ﬁrst	O
three	O
have	O
no	O
eﬀect	O
on	O
the	O
corresponding	O
greedy	O
policy	O
.	O
4v612	O
#	O
cars	O
at	O
second	O
location042020020	O
#	O
cars	O
at	O
first	O
location115	O
!	O
1	O
!	O
2-4432432	O
!	O
3005	O
!	O
1	O
!	O
2	O
!	O
3	O
!	O
412340	O
''	O
1	O
''	O
0	O
''	O
2	O
!	O
3	O
!	O
4	O
!	O
201234	O
!	O
1	O
''	O
32	O
!	O
4	O
!	O
3	O
!	O
201345	O
!	O
1	O
''	O
4	O
#	O
cars	O
at	O
second	O
location	O
#	O
cars	O
at	O
first	O
location5200020vk	O
4.4.	O
value	B
iteration	I
83	O
in	O
fact	O
,	O
the	O
policy	B
evaluation	I
step	O
of	O
policy	O
iteration	O
can	O
be	O
truncated	B
in	O
several	O
ways	O
without	O
losing	O
the	O
convergence	O
guarantees	O
of	O
policy	O
iteration	O
.	O
one	O
important	O
special	O
case	O
is	O
when	O
policy	B
evaluation	I
is	O
stopped	O
after	O
just	O
one	O
sweep	O
(	O
one	O
update	O
of	O
each	O
state	B
)	O
.	O
this	O
algorithm	O
is	O
called	O
value	B
iteration	I
.	O
it	O
can	O
be	O
written	O
as	O
a	O
particularly	O
simple	O
update	O
operation	O
that	O
combines	O
the	O
policy	B
improvement	I
and	O
truncated	B
policy	O
evaluation	O
steps	O
:	O
vk+1	O
(	O
s	O
)	O
.	O
=	O
max	O
=	O
max	O
a	O
e	O
[	O
rt+1	O
+	O
γvk	O
(	O
st+1	O
)	O
|	O
st	O
=	O
s	O
,	O
at	O
=	O
a	O
]	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:104	O
)	O
r	O
+	O
γvk	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
,	O
a	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
(	O
4.10	O
)	O
for	O
all	O
s	O
∈	O
s.	O
for	O
arbitrary	O
v0	O
,	O
the	O
sequence	O
{	O
vk	O
}	O
can	O
be	O
shown	O
to	O
converge	O
to	O
v∗	O
under	O
the	O
same	O
conditions	O
that	O
guarantee	O
the	O
existence	O
of	O
v∗	O
.	O
another	O
way	O
of	O
understanding	O
value	B
iteration	I
is	O
by	O
reference	O
to	O
the	O
bellman	O
optimality	O
equation	O
(	O
4.1	O
)	O
.	O
note	O
that	O
value	B
iteration	I
is	O
obtained	O
simply	O
by	O
turning	O
the	O
bellman	O
optimality	O
equation	O
into	O
an	O
update	O
rule	O
.	O
also	O
note	O
how	O
the	O
value	B
iteration	I
update	O
is	O
identical	O
to	O
the	O
policy	B
evaluation	I
update	O
(	O
4.5	O
)	O
except	O
that	O
it	O
requires	O
the	O
maximum	O
to	O
be	O
taken	O
over	O
all	O
actions	O
.	O
another	O
way	O
of	O
seeing	O
this	O
close	O
relationship	O
is	O
to	O
compare	O
the	O
backup	O
diagrams	O
for	O
these	O
algorithms	O
on	O
page	O
59	O
(	O
policy	B
evaluation	I
)	O
and	O
on	O
the	O
left	O
of	O
figure	O
3.4	O
(	O
value	B
iteration	I
)	O
.	O
these	O
two	O
are	O
the	O
natural	O
backup	O
operations	O
for	O
computing	O
vπ	O
and	O
v∗	O
.	O
finally	O
,	O
let	O
us	O
consider	O
how	O
value	B
iteration	I
terminates	O
.	O
like	O
policy	B
evaluation	I
,	O
value	B
iteration	I
formally	O
requires	O
an	O
inﬁnite	O
number	O
of	O
iterations	O
to	O
converge	O
exactly	O
to	O
v∗	O
.	O
in	O
practice	O
,	O
we	O
stop	O
once	O
the	O
value	B
function	I
changes	O
by	O
only	O
a	O
small	O
amount	O
in	O
a	O
sweep	O
.	O
the	O
box	O
below	O
shows	O
a	O
complete	O
algorithm	O
with	O
this	O
kind	O
of	O
termination	O
condition	O
.	O
value	B
iteration	I
,	O
for	O
estimating	O
π	O
≈	O
π∗	O
algorithm	O
parameter	O
:	O
a	O
small	O
threshold	O
θ	O
>	O
0	O
determining	O
accuracy	O
of	O
estimation	O
initialize	O
v	O
(	O
s	O
)	O
,	O
for	O
all	O
s	O
∈	O
s+	O
,	O
arbitrarily	O
except	O
that	O
v	O
(	O
terminal	O
)	O
=	O
0	O
loop	O
:	O
|	O
∆	O
←	O
0	O
|	O
loop	O
for	O
each	O
s	O
∈	O
s	O
:	O
|	O
|	O
|	O
until	O
∆	O
<	O
θ	O
output	O
a	O
deterministic	O
policy	B
,	O
π	O
≈	O
π∗	O
,	O
such	O
that	O
v	O
(	O
s	O
)	O
←	O
maxa	O
(	O
cid:80	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:2	O
)	O
r	O
+	O
γv	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:3	O
)	O
π	O
(	O
s	O
)	O
=	O
argmaxa	O
(	O
cid:80	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:2	O
)	O
r	O
+	O
γv	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:3	O
)	O
v	O
←	O
v	O
(	O
s	O
)	O
∆	O
←	O
max	O
(	O
∆	O
,	O
|v	O
−	O
v	O
(	O
s	O
)	O
|	O
)	O
value	B
iteration	I
eﬀectively	O
combines	O
,	O
in	O
each	O
of	O
its	O
sweeps	B
,	O
one	O
sweep	O
of	O
policy	O
eval-	O
uation	O
and	O
one	O
sweep	O
of	O
policy	O
improvement	O
.	O
faster	O
convergence	O
is	O
often	O
achieved	O
by	O
interposing	O
multiple	O
policy	B
evaluation	I
sweeps	O
between	O
each	O
policy	B
improvement	I
sweep	O
.	O
in	O
general	O
,	O
the	O
entire	O
class	O
of	O
truncated	O
policy	B
iteration	I
algorithms	O
can	O
be	O
thought	O
of	O
84	O
chapter	O
4	O
:	O
dynamic	B
programming	I
as	O
sequences	O
of	O
sweeps	O
,	O
some	O
of	O
which	O
use	O
policy	B
evaluation	I
updates	O
and	O
some	O
of	O
which	O
use	O
value	B
iteration	I
updates	O
.	O
because	O
the	O
max	O
operation	O
in	O
(	O
4.10	O
)	O
is	O
the	O
only	O
diﬀerence	O
between	O
these	O
updates	O
,	O
this	O
just	O
means	O
that	O
the	O
max	O
operation	O
is	O
added	O
to	O
some	O
sweeps	B
of	O
policy	B
evaluation	I
.	O
all	O
of	O
these	O
algorithms	O
converge	O
to	O
an	O
optimal	O
policy	O
for	O
discounted	O
ﬁnite	O
mdps	O
.	O
example	O
4.3	O
:	O
gambler	O
’	O
s	O
problem	O
a	O
gambler	O
has	O
the	O
opportunity	O
to	O
make	O
bets	O
on	O
the	O
outcomes	O
of	O
a	O
sequence	O
of	O
coin	O
ﬂips	O
.	O
if	O
the	O
coin	O
comes	O
up	O
heads	O
,	O
he	O
wins	O
as	O
many	O
dollars	O
as	O
he	O
has	O
staked	O
on	O
that	O
ﬂip	O
;	O
if	O
it	O
is	O
tails	O
,	O
he	O
loses	O
his	O
stake	O
.	O
the	O
game	O
ends	O
when	O
the	O
gambler	O
wins	O
by	O
reaching	O
his	O
goal	B
of	O
$	O
100	O
,	O
or	O
loses	O
by	O
running	O
out	O
of	O
money	O
.	O
on	O
each	O
ﬂip	O
,	O
the	O
gambler	O
must	O
decide	O
what	O
portion	O
of	O
his	O
capital	O
to	O
stake	O
,	O
in	O
integer	O
numbers	O
of	O
dollars	O
.	O
this	O
problem	O
can	O
be	O
formulated	O
as	O
an	O
undiscounted	O
,	O
episodic	O
,	O
ﬁnite	O
mdp	O
.	O
the	O
state	B
is	O
the	O
gam-	O
bler	O
’	O
s	O
capital	O
,	O
s	O
∈	O
{	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
99	O
}	O
and	O
the	O
actions	O
are	O
stakes	O
,	O
a	O
∈	O
{	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
min	O
(	O
s	O
,	O
100	O
−	O
s	O
)	O
}	O
.	O
the	O
re-	O
ward	O
is	O
zero	O
on	O
all	O
transitions	O
except	O
those	O
on	O
which	O
the	O
gambler	O
reaches	O
his	O
goal	B
,	O
when	O
it	O
is	O
+1	O
.	O
the	O
state-	O
value	B
function	I
then	O
gives	O
the	O
prob-	O
ability	O
of	O
winning	O
from	O
each	O
state	B
.	O
a	O
policy	B
is	O
a	O
mapping	O
from	O
levels	O
of	O
capital	O
to	O
stakes	O
.	O
the	O
optimal	O
policy	O
maximizes	O
the	O
probability	O
of	O
reaching	O
the	O
goal	B
.	O
let	O
ph	O
denote	O
the	O
probability	O
of	O
the	O
coin	O
coming	O
up	O
heads	O
.	O
if	O
ph	O
is	O
known	O
,	O
then	O
the	O
entire	O
problem	O
is	O
known	O
and	O
it	O
can	O
be	O
solved	O
,	O
for	O
instance	O
,	O
by	O
value	B
iter-	O
ation	O
.	O
figure	O
4.3	O
shows	O
the	O
change	O
in	O
the	O
value	O
function	O
over	O
successive	O
sweeps	B
of	O
value	B
iteration	I
,	O
and	O
the	O
ﬁnal	O
policy	B
found	O
,	O
for	O
the	O
case	O
of	O
ph	O
=	O
0.4.	O
this	O
policy	B
is	O
optimal	O
,	O
but	O
not	O
unique	O
.	O
in	O
fact	O
,	O
there	O
is	O
a	O
whole	O
family	O
of	O
optimal	O
policies	O
,	O
all	O
corre-	O
sponding	O
to	O
ties	O
for	O
the	O
argmax	O
ac-	O
tion	B
selection	O
with	O
respect	O
to	O
the	O
op-	O
timal	O
value	B
function	I
.	O
can	O
you	O
guess	O
what	O
the	O
entire	O
family	O
looks	O
like	O
?	O
figure	O
4.3	O
:	O
the	O
solution	O
to	O
the	O
gambler	O
’	O
s	O
problem	O
for	O
ph	O
=	O
0.4.	O
the	O
upper	O
graph	O
shows	O
the	O
value	B
func-	O
tion	B
found	O
by	O
successive	O
sweeps	B
of	O
value	B
iteration	I
.	O
the	O
lower	O
graph	O
shows	O
the	O
ﬁnal	O
policy	O
.	O
exercise	O
4.8	O
why	O
does	O
the	O
optimal	O
policy	O
for	O
the	O
gambler	O
’	O
s	O
problem	O
have	O
such	O
a	O
curious	O
form	O
?	O
in	O
particular	O
,	O
for	O
capital	O
of	O
50	O
it	O
bets	O
it	O
all	O
on	O
one	O
ﬂip	O
,	O
but	O
for	O
capital	O
of	O
51	O
it	O
(	O
cid:3	O
)	O
does	O
not	O
.	O
why	O
is	O
this	O
a	O
good	O
policy	B
?	O
exercise	O
4.9	O
(	O
programming	O
)	O
implement	O
value	B
iteration	I
for	O
the	O
gambler	O
’	O
s	O
problem	O
and	O
solve	O
it	O
for	O
ph	O
=	O
0.25	O
and	O
ph	O
=	O
0.55.	O
in	O
programming	O
,	O
you	O
may	O
ﬁnd	O
it	O
convenient	O
99755025111020304050100.20.40.60.8125507599capitalcapitalvalueestimatesfinalpolicy	O
(	O
stake	O
)	O
sweep	O
1sweep	O
2sweep	O
3sweep	O
32final	O
valuefunction	O
4.5.	O
asynchronous	B
dynamic	I
programming	I
85	O
to	O
introduce	O
two	O
dummy	O
states	O
corresponding	O
to	O
termination	O
with	O
capital	O
of	O
0	O
and	O
100	O
,	O
giving	O
them	O
values	O
of	O
0	O
and	O
1	O
respectively	O
.	O
show	O
your	O
results	O
graphically	O
,	O
as	O
in	O
figure	O
4.3	O
.	O
(	O
cid:3	O
)	O
are	O
your	O
results	O
stable	O
as	O
θ	O
→	O
0	O
?	O
exercise	O
4.10	O
what	O
is	O
the	O
analog	O
of	O
the	O
value	B
iteration	I
update	O
(	O
4.10	O
)	O
for	B
action	I
values	I
,	O
(	O
cid:3	O
)	O
qk+1	O
(	O
s	O
,	O
a	O
)	O
?	O
4.5	O
asynchronous	B
dynamic	I
programming	I
a	O
major	O
drawback	O
to	O
the	O
dp	O
methods	O
that	O
we	O
have	O
discussed	O
so	O
far	O
is	O
that	O
they	O
involve	O
operations	O
over	O
the	O
entire	O
state	B
set	O
of	O
the	O
mdp	O
,	O
that	O
is	O
,	O
they	O
require	O
sweeps	B
of	O
the	O
state	B
set	O
.	O
if	O
the	O
state	B
set	O
is	O
very	O
large	O
,	O
then	O
even	O
a	O
single	O
sweep	O
can	O
be	O
prohibitively	O
expensive	O
.	O
for	O
example	O
,	O
the	O
game	O
of	O
backgammon	B
has	O
over	O
1020	O
states	O
.	O
even	O
if	O
we	O
could	O
perform	O
the	O
value	B
iteration	I
update	O
on	O
a	O
million	O
states	O
per	O
second	O
,	O
it	O
would	O
take	O
over	O
a	O
thousand	O
years	O
to	O
complete	O
a	O
single	O
sweep	O
.	O
asynchronous	O
dp	O
algorithms	O
are	O
in-place	O
iterative	B
dp	O
algorithms	O
that	O
are	O
not	O
orga-	O
nized	O
in	O
terms	O
of	O
systematic	O
sweeps	B
of	O
the	O
state	B
set	O
.	O
these	O
algorithms	O
update	O
the	O
values	O
of	O
states	O
in	O
any	O
order	O
whatsoever	O
,	O
using	O
whatever	O
values	O
of	O
other	O
states	O
happen	O
to	O
be	O
available	O
.	O
the	O
values	O
of	O
some	O
states	O
may	O
be	O
updated	O
several	O
times	O
before	O
the	O
values	O
of	O
others	O
are	O
updated	O
once	O
.	O
to	O
converge	O
correctly	O
,	O
however	O
,	O
an	O
asynchronous	O
algorithm	O
must	O
continue	O
to	O
update	O
the	O
values	O
of	O
all	O
the	O
states	O
:	O
it	O
can	O
’	O
t	O
ignore	O
any	O
state	B
after	O
some	O
point	O
in	O
the	O
computation	O
.	O
asynchronous	O
dp	O
algorithms	O
allow	O
great	O
ﬂexibility	O
in	O
selecting	O
states	O
to	O
update	O
.	O
for	O
example	O
,	O
one	O
version	O
of	O
asynchronous	O
value	B
iteration	I
updates	O
the	O
value	B
,	O
in	O
place	O
,	O
of	O
only	O
one	O
state	B
,	O
sk	O
,	O
on	O
each	O
step	O
,	O
k	O
,	O
using	O
the	O
value	B
iteration	I
update	O
(	O
4.10	O
)	O
.	O
if	O
0	O
≤	O
γ	O
<	O
1	O
,	O
asymptotic	O
convergence	O
to	O
v∗	O
is	O
guaranteed	O
given	O
only	O
that	O
all	O
states	O
occur	O
in	O
the	O
sequence	O
{	O
sk	O
}	O
an	O
inﬁnite	O
number	O
of	O
times	O
(	O
the	O
sequence	O
could	O
even	O
be	O
stochastic	O
)	O
.	O
(	O
in	O
the	O
undiscounted	O
episodic	O
case	O
,	O
it	O
is	O
possible	O
that	O
there	O
are	O
some	O
orderings	O
of	O
updates	O
that	O
do	O
not	O
result	O
in	O
convergence	O
,	O
but	O
it	O
is	O
relatively	O
easy	O
to	O
avoid	O
these	O
.	O
)	O
similarly	O
,	O
it	O
is	O
possible	O
to	O
intermix	O
policy	B
evaluation	I
and	O
value	B
iteration	I
updates	O
to	O
produce	O
a	O
kind	O
of	O
asynchronous	O
truncated	B
policy	O
iteration	O
.	O
although	O
the	O
details	O
of	O
this	O
and	O
other	O
more	O
unusual	O
dp	O
algorithms	O
are	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
,	O
it	O
is	O
clear	O
that	O
a	O
few	O
diﬀerent	O
updates	O
form	O
building	O
blocks	O
that	O
can	O
be	O
used	O
ﬂexibly	O
in	O
a	O
wide	O
variety	O
of	O
sweepless	O
dp	O
algorithms	O
.	O
of	O
course	O
,	O
avoiding	O
sweeps	B
does	O
not	O
necessarily	O
mean	O
that	O
we	O
can	O
get	O
away	O
with	O
less	O
computation	O
.	O
it	O
just	O
means	O
that	O
an	O
algorithm	O
does	O
not	O
need	O
to	O
get	O
locked	O
into	O
any	O
hopelessly	O
long	O
sweep	O
before	O
it	O
can	O
make	O
progress	O
improving	O
a	O
policy	B
.	O
we	O
can	O
try	O
to	O
take	O
advantage	O
of	O
this	O
ﬂexibility	O
by	O
selecting	O
the	O
states	O
to	O
which	O
we	O
apply	O
updates	O
so	O
as	O
to	O
improve	O
the	O
algorithm	O
’	O
s	O
rate	O
of	O
progress	O
.	O
we	O
can	O
try	O
to	O
order	O
the	O
updates	O
to	O
let	O
value	B
information	O
propagate	O
from	O
state	B
to	O
state	B
in	O
an	O
eﬃcient	O
way	O
.	O
some	O
states	O
may	O
not	O
need	O
their	O
values	O
updated	O
as	O
often	O
as	O
others	O
.	O
we	O
might	O
even	O
try	O
to	O
skip	O
updating	O
some	O
states	O
entirely	O
if	O
they	O
are	O
not	O
relevant	O
to	O
optimal	O
behavior	O
.	O
some	O
ideas	O
for	O
doing	O
this	O
are	O
discussed	O
in	O
chapter	O
8.	O
asynchronous	O
algorithms	O
also	O
make	O
it	O
easier	O
to	O
intermix	O
computation	O
with	O
real-time	O
86	O
chapter	O
4	O
:	O
dynamic	B
programming	I
interaction	O
.	O
to	O
solve	O
a	O
given	O
mdp	O
,	O
we	O
can	O
run	O
an	O
iterative	B
dp	O
algorithm	O
at	O
the	O
same	O
time	O
that	O
an	O
agent	O
is	O
actually	O
experiencing	O
the	O
mdp	O
.	O
the	O
agent	O
’	O
s	O
experience	O
can	O
be	O
used	O
to	O
determine	O
the	O
states	O
to	O
which	O
the	O
dp	O
algorithm	O
applies	O
its	O
updates	O
.	O
at	O
the	O
same	O
time	O
,	O
the	O
latest	O
value	B
and	O
policy	B
information	O
from	O
the	O
dp	O
algorithm	O
can	O
guide	O
the	O
agent	O
’	O
s	O
decision	O
making	O
.	O
for	O
example	O
,	O
we	O
can	O
apply	O
updates	O
to	O
states	O
as	O
the	O
agent	O
visits	O
them	O
.	O
this	O
makes	O
it	O
possible	O
to	O
focus	O
the	O
dp	O
algorithm	O
’	O
s	O
updates	O
onto	O
parts	O
of	O
the	O
state	B
set	O
that	O
are	O
most	O
relevant	O
to	O
the	O
agent	O
.	O
this	O
kind	O
of	O
focusing	O
is	O
a	O
repeated	O
theme	O
in	O
reinforcement	O
learning	O
.	O
4.6	O
generalized	O
policy	O
iteration	O
policy	O
iteration	O
consists	O
of	O
two	O
simultaneous	O
,	O
interacting	O
processes	O
,	O
one	O
making	O
the	O
value	B
function	I
consistent	O
with	O
the	O
current	O
policy	B
(	O
policy	B
evaluation	I
)	O
,	O
and	O
the	O
other	O
making	O
the	O
policy	B
greedy	O
with	O
respect	O
to	O
the	O
current	O
value	B
function	I
(	O
policy	B
improvement	I
)	O
.	O
in	O
policy	O
iteration	O
,	O
these	O
two	O
processes	O
alternate	O
,	O
each	O
completing	O
before	O
the	O
other	O
begins	O
,	O
but	O
this	O
is	O
not	O
really	O
necessary	O
.	O
in	O
value	O
iteration	O
,	O
for	O
example	O
,	O
only	O
a	O
single	O
iteration	O
of	O
policy	O
evaluation	O
is	O
performed	O
in	O
between	O
each	O
policy	B
improvement	I
.	O
in	O
asynchronous	O
dp	O
methods	O
,	O
the	O
evaluation	O
and	O
improvement	O
processes	O
are	O
interleaved	O
at	O
an	O
even	O
ﬁner	O
grain	O
.	O
in	O
some	O
cases	O
a	O
single	O
state	B
is	O
updated	O
in	O
one	O
process	O
before	O
returning	O
to	O
the	O
other	O
.	O
as	O
long	O
as	O
both	O
processes	O
continue	O
to	O
update	O
all	O
states	O
,	O
the	O
ultimate	O
result	O
is	O
typically	O
the	O
same—convergence	O
to	O
the	O
optimal	O
value	O
function	O
and	O
an	O
optimal	O
policy	O
.	O
we	O
use	O
the	O
term	O
generalized	O
policy	O
iteration	O
(	O
gpi	O
)	O
to	O
refer	O
to	O
the	O
general	O
idea	O
of	O
letting	O
policy	B
evaluation	I
and	O
policy	B
im-	O
provement	O
processes	O
interact	O
,	O
independent	O
of	O
the	O
granularity	O
and	O
other	O
details	O
of	O
the	O
two	O
processes	O
.	O
almost	O
all	O
reinforcement	B
learning	I
methods	O
are	O
well	O
described	O
as	O
gpi	O
.	O
that	O
is	O
,	O
all	O
have	O
identiﬁable	O
policies	O
and	O
value	O
functions	O
,	O
with	O
the	O
policy	B
always	O
being	O
improved	O
with	O
respect	O
to	O
the	O
value	B
function	I
and	O
the	O
value	B
function	I
always	O
being	O
driven	O
toward	O
the	O
value	B
function	I
for	O
the	O
policy	B
,	O
as	O
suggested	O
by	O
the	O
diagram	O
to	O
the	O
right	O
.	O
it	O
is	O
easy	O
to	O
see	O
that	O
if	O
both	O
the	O
evaluation	O
process	O
and	O
the	O
improvement	O
process	O
stabilize	O
,	O
that	O
is	O
,	O
no	O
longer	O
produce	O
changes	O
,	O
then	O
the	O
value	B
function	I
and	O
policy	B
must	O
be	O
optimal	O
.	O
the	O
value	B
function	I
stabilizes	O
only	O
when	O
it	O
is	O
consistent	O
with	O
the	O
current	O
policy	B
,	O
and	O
the	O
policy	O
stabilizes	O
only	O
when	O
it	O
is	O
greedy	O
with	O
respect	O
to	O
the	O
current	O
value	B
function	I
.	O
thus	O
,	O
both	O
processes	O
stabilize	O
only	O
when	O
a	O
policy	B
has	O
been	O
found	O
that	O
is	O
greedy	O
with	O
respect	O
to	O
its	O
own	O
evaluation	O
function	O
.	O
this	O
implies	O
that	O
the	O
bell-	O
man	O
optimality	O
equation	O
(	O
4.1	O
)	O
holds	O
,	O
and	O
thus	O
that	O
the	O
policy	B
and	O
the	O
value	B
function	I
are	O
optimal	O
.	O
the	O
evaluation	O
and	O
improvement	O
processes	O
in	O
gpi	O
can	O
be	O
viewed	O
as	O
both	O
competing	O
and	O
cooperating	O
.	O
they	O
compete	O
in	O
the	O
sense	O
that	O
they	O
pull	O
in	O
opposing	O
directions	O
.	O
making	O
the	O
policy	B
greedy	O
with	O
respect	O
to	O
the	O
value	B
function	I
typically	O
makes	O
the	O
value	B
function	I
incorrect	O
for	O
the	O
changed	O
policy	B
,	O
and	O
making	O
the	O
value	B
function	I
consistent	O
with	O
the	O
policy	B
typically	O
causes	O
that	O
policy	B
no	O
longer	O
to	O
be	O
greedy	O
.	O
in	O
the	O
long	O
run	O
,	O
however	O
,	O
evaluationimprovement⇡	O
greedy	O
(	O
v	O
)	O
v⇡v	O
v⇡v⇤⇡⇤	O
4.7.	O
eﬃciency	O
of	O
dynamic	O
programming	O
87	O
these	O
two	O
processes	O
interact	O
to	O
ﬁnd	O
a	O
single	O
joint	O
solution	O
:	O
the	O
optimal	O
value	O
function	O
and	O
an	O
optimal	O
policy	O
.	O
one	O
might	O
also	O
think	O
of	O
the	O
interaction	O
between	O
the	O
evaluation	O
and	O
improvement	O
processes	O
in	O
gpi	O
in	O
terms	O
of	O
two	O
constraints	O
or	O
goals—for	O
example	O
,	O
as	O
two	O
lines	O
in	O
two-dimensional	O
space	O
as	O
suggested	O
by	O
the	O
diagram	O
to	O
the	O
right	O
.	O
although	O
the	O
real	O
geometry	O
is	O
much	O
more	O
complicated	O
than	O
this	O
,	O
the	O
diagram	O
suggests	O
what	O
happens	O
in	O
the	O
real	O
case	O
.	O
each	O
process	O
drives	O
the	O
value	B
function	I
or	O
policy	B
toward	O
one	O
of	O
the	O
lines	O
representing	O
a	O
solution	O
to	O
one	O
of	O
the	O
two	O
goals	O
.	O
the	O
goals	O
interact	O
because	O
the	O
two	O
lines	O
are	O
not	O
orthogonal	O
.	O
driving	O
directly	O
toward	O
one	O
goal	B
causes	O
some	O
movement	O
away	O
from	O
the	O
other	O
goal	B
.	O
inevitably	O
,	O
however	O
,	O
the	O
joint	O
process	O
is	O
brought	O
closer	O
to	O
the	O
overall	O
goal	B
of	O
optimality	O
.	O
the	O
arrows	O
in	O
this	O
diagram	O
correspond	O
to	O
the	O
behavior	O
of	O
policy	B
iteration	I
in	O
that	O
each	O
takes	O
the	O
system	O
all	O
the	O
way	O
to	O
achieving	O
one	O
of	O
the	O
two	O
goals	O
completely	O
.	O
in	O
gpi	O
one	O
could	O
also	O
take	O
smaller	O
,	O
incomplete	O
steps	O
toward	O
each	O
goal	B
.	O
in	O
either	O
case	O
,	O
the	O
two	O
processes	O
together	O
achieve	O
the	O
overall	O
goal	B
of	O
optimality	O
even	O
though	O
neither	O
is	O
attempting	O
to	O
achieve	O
it	O
directly	O
.	O
4.7	O
eﬃciency	O
of	O
dynamic	O
programming	O
dp	O
may	O
not	O
be	O
practical	O
for	O
very	O
large	O
problems	O
,	O
but	O
compared	O
with	O
other	O
methods	O
for	O
solving	O
mdps	O
,	O
dp	O
methods	O
are	O
actually	O
quite	O
eﬃcient	O
.	O
if	O
we	O
ignore	O
a	O
few	O
technical	O
details	O
,	O
then	O
the	O
(	O
worst	O
case	O
)	O
time	O
dp	O
methods	O
take	O
to	O
ﬁnd	O
an	O
optimal	O
policy	O
is	O
poly-	O
nomial	O
in	O
the	O
number	O
of	O
states	O
and	O
actions	O
.	O
if	O
n	O
and	O
k	O
denote	O
the	O
number	O
of	O
states	O
and	O
actions	O
,	O
this	O
means	O
that	O
a	O
dp	O
method	O
takes	O
a	O
number	O
of	O
computational	O
operations	O
that	O
is	O
less	O
than	O
some	O
polynomial	O
function	O
of	O
n	O
and	O
k.	O
a	O
dp	O
method	O
is	O
guaranteed	O
to	O
ﬁnd	O
an	O
optimal	O
policy	O
in	O
polynomial	O
time	O
even	O
though	O
the	O
total	O
number	O
of	O
(	O
determin-	O
istic	O
)	O
policies	O
is	O
kn	O
.	O
in	O
this	O
sense	O
,	O
dp	O
is	O
exponentially	O
faster	O
than	O
any	O
direct	O
search	O
in	O
policy	O
space	O
could	O
be	O
,	O
because	O
direct	O
search	O
would	O
have	O
to	O
exhaustively	O
examine	O
each	O
policy	B
to	O
provide	O
the	O
same	O
guarantee	O
.	O
linear	O
programming	O
methods	O
can	O
also	O
be	O
used	O
to	O
solve	O
mdps	O
,	O
and	O
in	O
some	O
cases	O
their	O
worst-case	O
convergence	O
guarantees	O
are	O
better	O
than	O
those	O
of	O
dp	O
methods	O
.	O
but	O
linear	O
programming	O
methods	O
become	O
impractical	O
at	O
a	O
much	O
smaller	O
number	O
of	O
states	O
than	O
do	O
dp	O
methods	O
(	O
by	O
a	O
factor	O
of	O
about	O
100	O
)	O
.	O
for	O
the	O
largest	O
problems	O
,	O
only	O
dp	O
methods	O
are	O
feasible	O
.	O
dp	O
is	O
sometimes	O
thought	O
to	O
be	O
of	O
limited	O
applicability	O
because	O
of	O
the	O
curse	O
of	O
dimen-	O
sionality	O
,	O
the	O
fact	O
that	O
the	O
number	O
of	O
states	O
often	O
grows	O
exponentially	O
with	O
the	O
number	O
of	O
state	O
variables	O
.	O
large	O
state	B
sets	O
do	O
create	O
diﬃculties	O
,	O
but	O
these	O
are	O
inherent	B
diﬃculties	O
of	O
the	O
problem	O
,	O
not	O
of	O
dp	O
as	O
a	O
solution	O
method	O
.	O
in	O
fact	O
,	O
dp	O
is	O
comparatively	O
better	O
suited	O
to	O
handling	O
large	O
state	B
spaces	O
than	O
competing	O
methods	O
such	O
as	O
direct	O
search	O
and	O
linear	O
programming	O
.	O
in	O
practice	O
,	O
dp	O
methods	O
can	O
be	O
used	O
with	O
today	O
’	O
s	O
computers	O
to	O
solve	O
mdps	O
with	O
millions	O
of	O
states	O
.	O
both	O
policy	B
iteration	I
and	O
value	B
iteration	I
are	O
widely	O
used	O
,	O
and	O
it	O
is	O
not	O
v⇤	O
,	O
⇡⇤⇡=greedy	O
(	O
v	O
)	O
v	O
,	O
⇡v=v⇡	O
88	O
chapter	O
4	O
:	O
dynamic	B
programming	I
clear	O
which	O
,	O
if	O
either	O
,	O
is	O
better	O
in	O
general	O
.	O
in	O
practice	O
,	O
these	O
methods	O
usually	O
converge	O
much	O
faster	O
than	O
their	O
theoretical	O
worst-case	O
run	O
times	O
,	O
particularly	O
if	O
they	O
are	O
started	O
with	O
good	O
initial	O
value	B
functions	O
or	O
policies	O
.	O
on	O
problems	O
with	O
large	O
state	B
spaces	O
,	O
asynchronous	O
dp	O
methods	O
are	O
often	O
preferred	O
.	O
to	O
complete	O
even	O
one	O
sweep	O
of	O
a	O
synchronous	O
method	O
requires	O
computation	O
and	O
mem-	O
ory	O
for	O
every	O
state	B
.	O
for	O
some	O
problems	O
,	O
even	O
this	O
much	O
memory	O
and	O
computation	O
is	O
impractical	O
,	O
yet	O
the	O
problem	O
is	O
still	O
potentially	O
solvable	O
because	O
relatively	O
few	O
states	O
occur	O
along	O
optimal	O
solution	O
trajectories	O
.	O
asynchronous	O
methods	O
and	O
other	O
variations	O
of	O
gpi	O
can	O
be	O
applied	O
in	O
such	O
cases	O
and	O
may	O
ﬁnd	O
good	O
or	O
optimal	O
policies	O
much	O
faster	O
than	O
synchronous	O
methods	O
can	O
.	O
4.8	O
summary	O
in	O
this	O
chapter	O
we	O
have	O
become	O
familiar	O
with	O
the	O
basic	O
ideas	O
and	O
algorithms	O
of	O
dynamic	O
programming	O
as	O
they	O
relate	O
to	O
solving	O
ﬁnite	O
mdps	O
.	O
policy	B
evaluation	I
refers	O
to	O
the	O
(	O
typi-	O
cally	O
)	O
iterative	B
computation	O
of	O
the	O
value	B
functions	O
for	O
a	O
given	O
policy	O
.	O
policy	B
improvement	I
refers	O
to	O
the	O
computation	O
of	O
an	O
improved	O
policy	B
given	O
the	O
value	B
function	I
for	O
that	O
policy	B
.	O
putting	O
these	O
two	O
computations	O
together	O
,	O
we	O
obtain	O
policy	B
iteration	I
and	O
value	B
iteration	I
,	O
the	O
two	O
most	O
popular	O
dp	O
methods	O
.	O
either	O
of	O
these	O
can	O
be	O
used	O
to	O
reliably	O
compute	O
optimal	O
policies	O
and	O
value	O
functions	O
for	O
ﬁnite	O
mdps	O
given	O
complete	O
knowledge	O
of	O
the	O
mdp	O
.	O
classical	O
dp	O
methods	O
operate	O
in	O
sweeps	O
through	O
the	O
state	B
set	O
,	O
performing	O
an	O
expected	B
update	I
operation	O
on	O
each	O
state	B
.	O
each	O
such	O
operation	O
updates	O
the	O
value	B
of	O
one	O
state	B
based	O
on	O
the	O
values	O
of	O
all	O
possible	O
successor	O
states	O
and	O
their	O
probabilities	O
of	O
occurring	O
.	O
ex-	O
pected	O
updates	O
are	O
closely	O
related	O
to	O
bellman	O
equations	O
:	O
they	O
are	O
little	O
more	O
than	O
these	O
equations	O
turned	O
into	O
assignment	O
statements	O
.	O
when	O
the	O
updates	O
no	O
longer	O
result	O
in	O
any	O
changes	O
in	O
value	O
,	O
convergence	O
has	O
occurred	O
to	O
values	O
that	O
satisfy	O
the	O
corresponding	O
bell-	O
man	O
equation	O
.	O
just	O
as	O
there	O
are	O
four	O
primary	O
value	B
functions	O
(	O
vπ	O
,	O
v∗	O
,	O
qπ	O
,	O
and	O
q∗	O
)	O
,	O
there	O
are	O
four	O
corresponding	O
bellman	O
equations	O
and	O
four	O
corresponding	O
expected	O
updates	O
.	O
an	O
intuitive	O
view	O
of	O
the	O
operation	O
of	O
dp	O
updates	O
is	O
given	O
by	O
their	O
backup	O
diagrams	O
.	O
insight	O
into	O
dp	O
methods	O
and	O
,	O
in	O
fact	O
,	O
into	O
almost	O
all	O
reinforcement	B
learning	I
methods	O
,	O
can	O
be	O
gained	O
by	O
viewing	O
them	O
as	O
generalized	O
policy	B
iteration	I
(	O
gpi	O
)	O
.	O
gpi	O
is	O
the	O
general	O
idea	O
of	O
two	O
interacting	O
processes	O
revolving	O
around	O
an	O
approximate	B
policy	O
and	O
an	O
approx-	O
imate	O
value	B
function	I
.	O
one	O
process	O
takes	O
the	O
policy	B
as	O
given	O
and	O
performs	O
some	O
form	O
of	O
policy	O
evaluation	O
,	O
changing	O
the	O
value	B
function	I
to	O
be	O
more	O
like	O
the	O
true	O
value	O
function	O
for	O
the	O
policy	B
.	O
the	O
other	O
process	O
takes	O
the	O
value	B
function	I
as	O
given	O
and	O
performs	O
some	O
form	O
of	O
policy	O
improvement	O
,	O
changing	O
the	O
policy	B
to	O
make	O
it	O
better	O
,	O
assuming	O
that	O
the	O
value	B
function	I
is	O
its	O
value	B
function	I
.	O
although	O
each	O
process	O
changes	O
the	O
basis	O
for	O
the	O
other	O
,	O
overall	O
they	O
work	O
together	O
to	O
ﬁnd	O
a	O
joint	O
solution	O
:	O
a	O
policy	B
and	O
value	B
function	I
that	O
are	O
unchanged	O
by	O
either	O
process	O
and	O
,	O
consequently	O
,	O
are	O
optimal	O
.	O
in	O
some	O
cases	O
,	O
gpi	O
can	O
be	O
proved	O
to	O
converge	O
,	O
most	O
notably	O
for	O
the	O
classical	O
dp	O
methods	O
that	O
we	O
have	O
presented	O
in	O
this	O
chapter	O
.	O
in	O
other	O
cases	O
convergence	O
has	O
not	O
been	O
proved	O
,	O
but	O
still	O
the	O
idea	O
of	O
gpi	O
improves	O
our	O
understanding	O
of	O
the	O
methods	O
.	O
it	O
is	O
not	O
necessary	O
to	O
perform	O
dp	O
methods	O
in	O
complete	O
sweeps	B
through	O
the	O
state	B
4.8.	O
summary	O
89	O
set	O
.	O
asynchronous	O
dp	O
methods	O
are	O
in-place	O
iterative	B
methods	O
that	O
update	O
states	O
in	O
an	O
arbitrary	O
order	O
,	O
perhaps	O
stochastically	O
determined	O
and	O
using	O
out-of-date	O
information	O
.	O
many	O
of	O
these	O
methods	O
can	O
be	O
viewed	O
as	O
ﬁne-grained	O
forms	O
of	O
gpi	O
.	O
finally	O
,	O
we	O
note	O
one	O
last	O
special	O
property	O
of	O
dp	O
methods	O
.	O
all	O
of	O
them	O
update	O
estimates	O
of	O
the	O
values	O
of	O
states	O
based	O
on	O
estimates	O
of	O
the	O
values	O
of	O
successor	O
states	O
.	O
that	O
is	O
,	O
they	O
update	O
estimates	O
on	O
the	O
basis	O
of	O
other	O
estimates	O
.	O
we	O
call	O
this	O
general	O
idea	O
bootstrapping	B
.	O
many	O
reinforcement	B
learning	I
methods	O
perform	O
bootstrapping	B
,	O
even	O
those	O
that	O
do	O
not	O
require	O
,	O
as	O
dp	O
requires	O
,	O
a	O
complete	O
and	O
accurate	O
model	B
of	I
the	I
environment	I
.	O
in	O
the	O
next	O
chapter	O
we	O
explore	O
reinforcement	B
learning	I
methods	O
that	O
do	O
not	O
require	O
a	O
model	O
and	O
do	O
not	O
bootstrap	O
.	O
in	O
the	O
chapter	O
after	O
that	O
we	O
explore	O
methods	O
that	O
do	O
not	O
require	O
a	O
model	O
but	O
do	O
bootstrap	O
.	O
these	O
key	O
features	O
and	O
properties	O
are	O
separable	O
,	O
yet	O
can	O
be	O
mixed	O
in	O
interesting	O
combinations	O
.	O
bibliographical	O
and	O
historical	O
remarks	O
the	O
term	O
“	O
dynamic	B
programming	I
”	O
is	O
due	O
to	O
bellman	O
(	O
1957a	O
)	O
,	O
who	O
showed	O
how	O
these	O
methods	O
could	O
be	O
applied	O
to	O
a	O
wide	O
range	O
of	O
problems	O
.	O
extensive	O
treatments	O
of	O
dp	O
can	O
be	O
found	O
in	O
many	O
texts	O
,	O
including	O
bertsekas	O
(	O
2005	O
,	O
2012	O
)	O
,	O
bertsekas	O
and	O
tsitsiklis	O
(	O
1996	O
)	O
,	O
dreyfus	O
and	O
law	O
(	O
1977	O
)	O
,	O
ross	O
(	O
1983	O
)	O
,	O
white	O
(	O
1969	O
)	O
,	O
and	O
whittle	O
(	O
1982	O
,	O
1983	O
)	O
.	O
our	O
interest	O
in	O
dp	O
is	O
restricted	O
to	O
its	O
use	O
in	O
solving	O
mdps	O
,	O
but	O
dp	O
also	O
applies	O
to	O
other	O
types	O
of	O
problems	O
.	O
kumar	O
and	O
kanal	O
(	O
1988	O
)	O
provide	O
a	O
more	O
general	O
look	O
at	O
dp	O
.	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
the	O
ﬁrst	O
connection	O
between	O
dp	O
and	B
reinforcement	I
learning	O
was	O
made	O
by	O
minsky	O
(	O
1961	O
)	O
in	O
commenting	O
on	O
samuel	O
’	O
s	O
checkers	O
player	O
.	O
in	O
a	O
footnote	O
,	O
minsky	O
mentioned	O
that	O
it	O
is	O
possible	O
to	O
apply	O
dp	O
to	O
problems	O
in	O
which	O
samuel	O
’	O
s	O
backing-up	O
process	O
can	O
be	O
handled	O
in	O
closed	O
analytic	O
form	O
.	O
this	O
remark	O
may	O
have	O
misled	O
artiﬁcial	B
intelligence	I
researchers	O
into	O
believing	O
that	O
dp	O
was	O
restricted	O
to	O
analytically	O
tractable	O
problems	O
and	O
therefore	O
largely	O
irrelevant	O
to	O
artiﬁcial	B
intelligence	I
.	O
andreae	O
(	O
1969b	O
)	O
mentioned	O
dp	O
in	O
the	O
context	O
of	O
reinforcement	O
learning	O
,	O
speciﬁcally	O
policy	B
iteration	I
,	O
although	O
he	O
did	O
not	O
make	O
speciﬁc	O
connections	O
between	O
dp	O
and	O
learning	O
algorithms	O
.	O
werbos	O
(	O
1977	O
)	O
suggested	O
an	O
approach	O
to	O
approximating	O
dp	O
called	O
“	O
heuristic	O
dynamic	O
programming	O
”	O
that	O
emphasizes	O
gradient-descent	O
methods	O
for	O
continuous-state	O
problems	O
(	O
werbos	O
,	O
1982	O
,	O
1987	O
,	O
1988	O
,	O
1989	O
,	O
1992	O
)	O
.	O
these	O
methods	O
are	O
closely	O
related	O
to	O
the	O
reinforcement	B
learning	I
algorithms	O
that	O
we	O
discuss	O
in	O
this	O
book	O
.	O
watkins	O
(	O
1989	O
)	O
was	O
explicit	O
in	O
connecting	O
reinforcement	B
learning	I
to	O
dp	O
,	O
characterizing	O
a	O
class	O
of	O
reinforce-	O
ment	O
learning	O
methods	O
as	O
“	O
incremental	O
dynamic	O
programming.	O
”	O
4.1–4	O
these	O
sections	O
describe	O
well-established	O
dp	O
algorithms	O
that	O
are	O
covered	O
in	O
any	O
of	O
the	O
general	O
dp	O
references	O
cited	O
above	O
.	O
the	O
policy	B
improvement	I
theorem	O
and	O
the	O
policy	O
iteration	O
algorithm	O
are	O
due	O
to	O
bellman	O
(	O
1957a	O
)	O
and	O
howard	O
(	O
1960	O
)	O
.	O
our	O
presentation	O
was	O
inﬂuenced	O
by	O
the	O
local	O
view	O
of	O
policy	O
improvement	O
taken	O
by	O
watkins	O
(	O
1989	O
)	O
.	O
our	O
discussion	O
of	O
value	O
iteration	O
as	O
a	O
form	O
of	O
truncated	O
policy	B
iteration	I
is	O
based	O
on	O
the	O
approach	O
of	O
puterman	O
and	O
shin	O
(	O
1978	O
)	O
,	O
who	O
presented	O
a	O
class	O
of	O
algorithms	O
called	O
modiﬁed	O
policy	B
iteration	I
,	O
which	O
includes	O
policy	B
iteration	I
and	O
value	B
iteration	I
as	O
special	O
cases	O
.	O
an	O
analysis	O
showing	O
how	O
90	O
chapter	O
4	O
:	O
dynamic	B
programming	I
value	O
iteration	O
can	O
be	O
made	O
to	O
ﬁnd	O
an	O
optimal	O
policy	O
in	O
ﬁnite	O
time	O
is	O
given	O
by	O
bertsekas	O
(	O
1987	O
)	O
.	O
iterative	B
policy	O
evaluation	O
is	O
an	O
example	O
of	O
a	O
classical	O
successive	O
approximation	O
algorithm	O
for	O
solving	O
a	O
system	O
of	O
linear	O
equations	O
.	O
the	O
version	O
of	O
the	O
algorithm	O
that	O
uses	O
two	O
arrays	O
,	O
one	O
holding	O
the	O
old	O
values	O
while	O
the	O
other	O
is	O
updated	O
,	O
is	O
often	O
called	O
a	O
jacobi-style	O
algorithm	O
,	O
after	O
jacobi	O
’	O
s	O
classical	O
use	O
of	O
this	O
method	O
.	O
it	O
is	O
also	O
sometimes	O
called	O
a	O
synchronous	O
algorithm	O
because	O
the	O
eﬀect	O
is	O
as	O
if	O
all	O
the	O
values	O
are	O
updated	O
at	O
the	O
same	O
time	O
.	O
the	O
second	O
array	O
is	O
needed	O
to	O
simulate	O
this	O
parallel	O
computation	O
sequentially	O
.	O
the	O
in-place	O
version	O
of	O
the	O
algorithm	O
is	O
often	O
called	O
a	O
gauss–seidel-style	O
algorithm	O
after	O
the	O
classical	O
gauss–seidel	O
algorithm	O
for	O
solving	O
systems	O
of	O
linear	O
equations	O
.	O
in	O
addition	O
to	O
iterative	B
policy	O
evaluation	O
,	O
other	O
dp	O
algorithms	O
can	O
be	O
implemented	O
in	O
these	O
diﬀerent	O
versions	O
.	O
bertsekas	O
and	O
tsitsiklis	O
(	O
1989	O
)	O
provide	O
excellent	O
coverage	O
of	O
these	O
variations	O
and	O
their	O
performance	O
diﬀerences	O
.	O
4.5	O
asynchronous	O
dp	O
algorithms	O
are	O
due	O
to	O
bertsekas	O
(	O
1982	O
,	O
1983	O
)	O
,	O
who	O
also	O
called	O
them	O
distributed	O
dp	O
algorithms	O
.	O
the	O
original	O
motivation	B
for	O
asynchronous	O
dp	O
was	O
its	O
implementation	O
on	O
a	O
multiprocessor	O
system	O
with	O
communication	O
de-	O
lays	O
between	O
processors	O
and	O
no	O
global	O
synchronizing	O
clock	O
.	O
these	O
algorithms	O
are	O
extensively	O
discussed	O
by	O
bertsekas	O
and	O
tsitsiklis	O
(	O
1989	O
)	O
.	O
jacobi-style	O
and	O
gauss–seidel-style	O
dp	O
algorithms	O
are	O
special	O
cases	O
of	O
the	O
asynchronous	O
version	O
.	O
williams	O
and	O
baird	O
(	O
1990	O
)	O
presented	O
dp	O
algorithms	O
that	O
are	O
asynchronous	O
at	O
a	O
ﬁner	O
grain	O
than	O
the	O
ones	O
we	O
have	O
discussed	O
:	O
the	O
update	O
operations	O
themselves	O
are	O
broken	O
into	O
steps	O
that	O
can	O
be	O
performed	O
asynchronously	O
.	O
4.7	O
this	O
section	O
,	O
written	O
with	O
the	O
help	O
of	O
michael	O
littman	O
,	O
is	O
based	O
on	O
littman	O
,	O
dean	O
,	O
and	O
kaelbling	O
(	O
1995	O
)	O
.	O
the	O
phrase	O
“	O
curse	B
of	I
dimensionality	I
”	O
is	O
due	O
to	O
bellman	O
(	O
1957a	O
)	O
.	O
chapter	O
5	O
monte	O
carlo	O
methods	O
in	O
this	O
chapter	O
we	O
consider	O
our	O
ﬁrst	O
learning	O
methods	O
for	O
estimating	O
value	B
functions	O
and	O
discovering	O
optimal	O
policies	O
.	O
unlike	O
the	O
previous	O
chapter	O
,	O
here	O
we	O
do	O
not	O
assume	O
complete	O
knowledge	O
of	O
the	O
environment	B
.	O
monte	O
carlo	O
methods	O
require	O
only	O
experience—	O
sample	O
sequences	O
of	O
states	O
,	O
actions	O
,	O
and	O
rewards	O
from	O
actual	O
or	O
simulated	O
interaction	O
with	O
an	O
environment	B
.	O
learning	O
from	O
actual	O
experience	O
is	O
striking	O
because	O
it	O
requires	O
no	O
prior	B
knowledge	I
of	O
the	O
environment	B
’	O
s	O
dynamics	O
,	O
yet	O
can	O
still	O
attain	O
optimal	O
behavior	O
.	O
learning	O
from	O
simulated	O
experience	O
is	O
also	O
powerful	O
.	O
although	O
a	O
model	O
is	O
required	O
,	O
the	O
model	O
need	O
only	O
generate	O
sample	O
transitions	O
,	O
not	O
the	O
complete	O
probability	O
distributions	O
of	O
all	O
possible	O
transitions	O
that	O
is	O
required	O
for	B
dynamic	I
programming	I
(	O
dp	O
)	O
.	O
in	O
surprisingly	O
many	O
cases	O
it	O
is	O
easy	O
to	O
generate	O
experience	O
sampled	O
according	O
to	O
the	O
desired	O
probability	O
distributions	O
,	O
but	O
infeasible	O
to	O
obtain	O
the	O
distributions	O
in	O
explicit	O
form	O
.	O
monte	O
carlo	O
methods	O
are	O
ways	O
of	O
solving	O
the	O
reinforcement	B
learning	I
problem	O
based	O
on	O
averaging	O
sample	O
returns	O
.	O
to	O
ensure	O
that	O
well-deﬁned	O
returns	O
are	O
available	O
,	O
here	O
we	O
deﬁne	O
monte	O
carlo	O
methods	O
only	O
for	O
episodic	O
tasks	O
.	O
that	O
is	O
,	O
we	O
assume	O
experience	O
is	O
divided	O
into	O
episodes	B
,	O
and	O
that	O
all	O
episodes	B
eventually	O
terminate	O
no	O
matter	O
what	O
actions	O
are	O
selected	O
.	O
only	O
on	O
the	O
completion	O
of	O
an	O
episode	O
are	O
value	B
estimates	O
and	O
policies	O
changed	O
.	O
monte	O
carlo	O
methods	O
can	O
thus	O
be	O
incremental	O
in	O
an	O
episode-by-episode	O
sense	O
,	O
but	O
not	O
in	O
a	O
step-by-step	O
(	O
online	B
)	O
sense	O
.	O
the	O
term	O
“	O
monte	O
carlo	O
”	O
is	O
often	O
used	O
more	O
broadly	O
for	O
any	O
estimation	O
method	O
whose	O
operation	O
involves	O
a	O
signiﬁcant	O
random	O
component	O
.	O
here	O
we	O
use	O
it	O
speciﬁcally	O
for	O
methods	O
based	O
on	O
averaging	O
complete	O
returns	O
(	O
as	O
opposed	O
to	O
methods	O
that	O
learn	O
from	O
partial	O
returns	O
,	O
considered	O
in	O
the	O
next	O
chapter	O
)	O
.	O
monte	O
carlo	O
methods	O
sample	O
and	O
average	O
returns	O
for	O
each	O
state–action	O
pair	O
much	O
like	O
the	O
bandit	O
methods	O
we	O
explored	O
in	O
chapter	O
2	O
sample	O
and	O
average	O
rewards	O
for	O
each	O
action	B
.	O
the	O
main	O
diﬀerence	O
is	O
that	O
now	O
there	O
are	O
multiple	O
states	O
,	O
each	O
acting	O
like	O
a	O
diﬀerent	O
bandit	O
problem	O
(	O
like	O
an	O
associative-search	O
or	O
contextual	O
bandit	O
)	O
and	O
the	O
diﬀerent	O
bandit	B
problems	I
are	O
interrelated	O
.	O
that	O
is	O
,	O
the	O
return	B
after	O
taking	O
an	O
action	B
in	O
one	O
state	B
depends	O
on	O
the	O
actions	O
taken	O
in	O
later	O
states	O
in	O
the	O
same	O
episode	O
.	O
because	O
all	O
the	O
action	B
selections	O
are	O
undergoing	O
learning	O
,	O
the	O
problem	O
becomes	O
nonstationary	O
from	O
the	O
point	O
of	O
view	O
of	O
the	O
earlier	O
state	B
.	O
91	O
92	O
chapter	O
5	O
:	O
monte	O
carlo	O
methods	O
to	O
handle	O
the	O
nonstationarity	B
,	O
we	O
adapt	O
the	O
idea	O
of	O
general	O
policy	B
iteration	I
(	O
gpi	O
)	O
developed	O
in	O
chapter	O
4	O
for	O
dp	O
.	O
whereas	O
there	O
we	O
computed	O
value	B
functions	O
from	O
knowl-	O
edge	O
of	O
the	O
mdp	O
,	O
here	O
we	O
learn	O
value	B
functions	O
from	O
sample	O
returns	O
with	O
the	O
mdp	O
.	O
the	O
value	B
functions	O
and	O
corresponding	O
policies	O
still	O
interact	O
to	O
attain	O
optimality	O
in	O
essentially	O
the	O
same	O
way	O
(	O
gpi	O
)	O
.	O
as	O
in	O
the	O
dp	O
chapter	O
,	O
ﬁrst	O
we	O
consider	O
the	O
prediction	B
problem	O
(	O
the	O
computation	O
of	O
vπ	O
and	O
qπ	O
for	O
a	O
ﬁxed	O
arbitrary	O
policy	B
π	O
)	O
then	O
policy	B
improvement	I
,	O
and	O
,	O
ﬁnally	O
,	O
the	O
control	B
problem	O
and	O
its	O
solution	O
by	O
gpi	O
.	O
each	O
of	O
these	O
ideas	O
taken	O
from	O
dp	O
is	O
extended	O
to	O
the	O
monte	O
carlo	O
case	O
in	O
which	O
only	O
sample	O
experience	O
is	O
available	O
.	O
5.1	O
monte	O
carlo	O
prediction	B
we	O
begin	O
by	O
considering	O
monte	O
carlo	O
methods	O
for	O
learning	O
the	O
state-value	O
function	O
for	O
a	O
given	O
policy	B
.	O
recall	O
that	O
the	O
value	B
of	O
a	O
state	B
is	O
the	O
expected	O
return—expected	O
cumulative	O
future	O
discounted	O
reward—starting	O
from	O
that	O
state	B
.	O
an	O
obvious	O
way	O
to	O
estimate	O
it	O
from	O
experience	O
,	O
then	O
,	O
is	O
simply	O
to	O
average	O
the	O
returns	O
observed	O
after	O
visits	O
to	O
that	O
state	B
.	O
as	O
more	O
returns	O
are	O
observed	O
,	O
the	O
average	O
should	O
converge	O
to	O
the	O
expected	O
value	O
.	O
this	O
idea	O
underlies	O
all	O
monte	O
carlo	O
methods	O
.	O
in	O
particular	O
,	O
suppose	O
we	O
wish	O
to	O
estimate	O
vπ	O
(	O
s	O
)	O
,	O
the	O
value	B
of	O
a	O
state	B
s	O
under	O
policy	B
π	O
,	O
given	O
a	O
set	O
of	O
episodes	O
obtained	O
by	O
following	O
π	O
and	O
passing	O
through	O
s.	O
each	O
occurrence	O
of	O
state	O
s	O
in	O
an	O
episode	O
is	O
called	O
a	O
visit	O
to	O
s.	O
of	O
course	O
,	O
s	O
may	O
be	O
visited	O
multiple	O
times	O
in	O
the	O
same	O
episode	O
;	O
let	O
us	O
call	O
the	O
ﬁrst	O
time	O
it	O
is	O
visited	O
in	O
an	O
episode	O
the	O
ﬁrst	O
visit	O
to	O
s.	O
the	O
ﬁrst-visit	O
mc	O
method	O
estimates	O
vπ	O
(	O
s	O
)	O
as	O
the	O
average	O
of	O
the	O
returns	O
following	O
ﬁrst	O
visits	O
to	O
s	O
,	O
whereas	O
the	O
every-visit	O
mc	O
method	O
averages	O
the	O
returns	O
following	O
all	O
visits	O
to	O
s.	O
these	O
two	O
monte	O
carlo	O
(	O
mc	O
)	O
methods	O
are	O
very	O
similar	O
but	O
have	O
slightly	O
diﬀerent	O
theoretical	O
properties	O
.	O
first-visit	O
mc	O
has	O
been	O
most	O
widely	O
studied	O
,	O
dating	O
back	O
to	O
the	O
1940s	O
,	O
and	O
is	O
the	O
one	O
we	O
focus	O
on	O
in	O
this	O
chapter	O
.	O
every-visit	O
mc	O
extends	O
more	O
naturally	O
to	O
function	B
approximation	I
and	O
eligibility	B
traces	I
,	O
as	O
discussed	O
in	O
chapters	O
9	O
and	O
12.	O
first-visit	O
mc	O
is	O
shown	O
in	O
procedural	O
form	O
in	O
the	O
box	O
.	O
every-visit	O
mc	O
would	O
be	O
the	O
same	O
except	O
without	O
the	O
check	O
for	O
st	O
having	O
occurred	O
earlier	O
in	O
the	O
episode	O
.	O
first-visit	O
mc	O
prediction	B
,	O
for	O
estimating	O
v	O
≈	O
vπ	O
input	O
:	O
a	O
policy	B
π	O
to	O
be	O
evaluated	O
initialize	O
:	O
v	O
(	O
s	O
)	O
∈	O
r	O
,	O
arbitrarily	O
,	O
for	O
all	O
s	O
∈	O
s	O
returns	O
(	O
s	O
)	O
←	O
an	O
empty	O
list	O
,	O
for	O
all	O
s	O
∈	O
s	O
loop	O
forever	O
(	O
for	O
each	O
episode	O
)	O
:	O
generate	O
an	O
episode	O
following	O
π	O
:	O
s0	O
,	O
a0	O
,	O
r1	O
,	O
s1	O
,	O
a1	O
,	O
r2	O
,	O
.	O
.	O
.	O
,	O
st−1	O
,	O
at−1	O
,	O
rt	O
g	O
←	O
0	O
loop	O
for	O
each	O
step	O
of	O
episode	O
,	O
t	O
=	O
t	O
−1	O
,	O
t	O
−2	O
,	O
.	O
.	O
.	O
,	O
0	O
:	O
g	O
←	O
g	O
+	O
rt+1	O
unless	O
st	O
appears	O
in	O
s0	O
,	O
s1	O
,	O
.	O
.	O
.	O
,	O
st−1	O
:	O
append	O
g	O
to	O
returns	O
(	O
st	O
)	O
v	O
(	O
st	O
)	O
←	O
average	O
(	O
returns	O
(	O
st	O
)	O
)	O
5.1.	O
monte	O
carlo	O
prediction	B
93	O
both	O
ﬁrst-visit	O
mc	O
and	O
every-visit	O
mc	O
converge	O
to	O
vπ	O
(	O
s	O
)	O
as	O
the	O
number	O
of	O
visits	O
(	O
or	O
ﬁrst	O
visits	O
)	O
to	O
s	O
goes	O
to	O
inﬁnity	O
.	O
this	O
is	O
easy	O
to	O
see	O
for	O
the	O
case	O
of	O
ﬁrst-visit	O
mc	O
.	O
in	O
this	O
case	O
each	O
return	B
is	O
an	O
independent	O
,	O
identically	O
distributed	O
estimate	O
of	O
vπ	O
(	O
s	O
)	O
with	O
ﬁnite	O
variance	O
.	O
by	O
the	O
law	O
of	O
large	O
numbers	O
the	O
sequence	O
of	B
averages	I
of	O
these	O
estimates	O
converges	O
to	O
their	O
expected	O
value	O
.	O
each	O
average	O
is	O
itself	O
an	O
unbiased	O
estimate	O
,	O
and	O
the	O
standard	O
deviation	O
of	O
its	O
error	O
falls	O
as	O
1/√n	O
,	O
where	O
n	O
is	O
the	O
number	O
of	O
returns	O
averaged	O
.	O
every-visit	O
mc	O
is	O
less	O
straightforward	O
,	O
but	O
its	O
estimates	O
also	O
converge	O
quadratically	O
to	O
vπ	O
(	O
s	O
)	O
(	O
singh	O
and	O
sutton	O
,	O
1996	O
)	O
.	O
the	O
use	O
of	O
monte	O
carlo	O
methods	O
is	O
best	O
illustrated	O
through	O
an	O
example	O
.	O
example	O
5.1	O
:	O
blackjack	O
the	O
object	O
of	O
the	O
popular	O
casino	O
card	O
game	O
of	O
blackjack	O
is	O
to	O
obtain	O
cards	O
the	O
sum	O
of	O
whose	O
numerical	O
values	O
is	O
as	O
great	O
as	O
possible	O
without	O
exceeding	O
21.	O
all	O
face	O
cards	O
count	O
as	O
10	O
,	O
and	O
an	O
ace	O
can	O
count	O
either	O
as	O
1	O
or	O
as	O
11.	O
we	O
consider	O
the	O
version	O
in	O
which	O
each	O
player	O
competes	O
independently	O
against	O
the	O
dealer	O
.	O
the	O
game	O
begins	O
with	O
two	O
cards	O
dealt	O
to	O
both	O
dealer	O
and	O
player	O
.	O
one	O
of	O
the	O
dealer	O
’	O
s	O
cards	O
is	O
face	O
up	O
and	O
the	O
other	O
is	O
face	O
down	O
.	O
if	O
the	O
player	O
has	O
21	O
immediately	O
(	O
an	O
ace	O
and	O
a	O
10-card	O
)	O
,	O
it	O
is	O
called	O
a	O
natural	O
.	O
he	O
then	O
wins	O
unless	O
the	O
dealer	O
also	O
has	O
a	O
natural	O
,	O
in	O
which	O
case	O
the	O
game	O
is	O
a	O
draw	O
.	O
if	O
the	O
player	O
does	O
not	O
have	O
a	O
natural	O
,	O
then	O
he	O
can	O
request	O
additional	O
cards	O
,	O
one	O
by	O
one	O
(	O
hits	O
)	O
,	O
until	O
he	O
either	O
stops	O
(	O
sticks	O
)	O
or	O
exceeds	O
21	O
(	O
goes	O
bust	O
)	O
.	O
if	O
he	O
goes	O
bust	O
,	O
he	O
loses	O
;	O
if	O
he	O
sticks	O
,	O
then	O
it	O
becomes	O
the	O
dealer	O
’	O
s	O
turn	O
.	O
the	O
dealer	O
hits	O
or	O
sticks	O
according	O
to	O
a	O
ﬁxed	O
strategy	O
without	O
choice	O
:	O
he	O
sticks	O
on	O
any	O
sum	O
of	O
17	O
or	O
greater	O
,	O
and	O
hits	O
otherwise	O
.	O
if	O
the	O
dealer	O
goes	O
bust	O
,	O
then	O
the	O
player	O
wins	O
;	O
otherwise	O
,	O
the	O
outcome—win	O
,	O
lose	O
,	O
or	O
draw—is	O
determined	O
by	O
whose	O
ﬁnal	O
sum	O
is	O
closer	O
to	O
21.	O
playing	O
blackjack	O
is	O
naturally	O
formulated	O
as	O
an	O
episodic	O
ﬁnite	O
mdp	O
.	O
each	O
game	O
of	O
blackjack	O
is	O
an	O
episode	O
.	O
rewards	O
of	O
+1	O
,	O
−1	O
,	O
and	O
0	O
are	O
given	O
for	O
winning	O
,	O
losing	O
,	O
and	O
drawing	O
,	O
respectively	O
.	O
all	O
rewards	O
within	O
a	O
game	O
are	O
zero	O
,	O
and	O
we	O
do	O
not	O
discount	O
(	O
γ	O
=	O
1	O
)	O
;	O
therefore	O
these	O
terminal	O
rewards	O
are	O
also	O
the	O
returns	O
.	O
the	O
player	O
’	O
s	O
actions	O
are	O
to	O
hit	O
or	O
to	O
stick	O
.	O
the	O
states	O
depend	O
on	O
the	O
player	O
’	O
s	O
cards	O
and	O
the	O
dealer	O
’	O
s	O
showing	O
card	O
.	O
we	O
assume	O
that	O
cards	O
are	O
dealt	O
from	O
an	O
inﬁnite	O
deck	O
(	O
i.e.	O
,	O
with	O
replacement	O
)	O
so	O
that	O
there	O
is	O
no	O
advantage	O
to	O
keeping	O
track	O
of	O
the	O
cards	O
already	O
dealt	O
.	O
if	O
the	O
player	O
holds	O
an	O
ace	O
that	O
he	O
could	O
count	O
as	O
11	O
without	O
going	O
bust	O
,	O
then	O
the	O
ace	O
is	O
said	O
to	O
be	O
usable	O
.	O
in	O
this	O
case	O
it	O
is	O
always	O
counted	O
as	O
11	O
because	O
counting	O
it	O
as	O
1	O
would	O
make	O
the	O
sum	O
11	O
or	O
less	O
,	O
in	O
which	O
case	O
there	O
is	O
no	O
decision	O
to	O
be	O
made	O
because	O
,	O
obviously	O
,	O
the	O
player	O
should	O
always	O
hit	O
.	O
thus	O
,	O
the	O
player	O
makes	O
decisions	O
on	O
the	O
basis	O
of	O
three	O
variables	O
:	O
his	O
current	O
sum	O
(	O
12–21	O
)	O
,	O
the	O
dealer	O
’	O
s	O
one	O
showing	O
card	O
(	O
ace–10	O
)	O
,	O
and	O
whether	O
or	O
not	O
he	O
holds	O
a	O
usable	O
ace	O
.	O
this	O
makes	O
for	O
a	O
total	O
of	O
200	O
states	O
.	O
consider	O
the	O
policy	B
that	O
sticks	O
if	O
the	O
player	O
’	O
s	O
sum	O
is	O
20	O
or	O
21	O
,	O
and	O
otherwise	O
hits	O
.	O
to	O
ﬁnd	O
the	O
state-value	O
function	O
for	O
this	O
policy	B
by	O
a	O
monte	O
carlo	O
approach	O
,	O
one	O
simulates	O
many	O
blackjack	O
games	O
using	O
the	O
policy	B
and	O
averages	O
the	O
returns	O
following	O
each	O
state	B
.	O
in	O
this	O
way	O
,	O
we	O
obtained	O
the	O
estimates	O
of	O
the	O
state-value	O
function	O
shown	O
in	O
figure	O
5.1.	O
the	O
estimates	O
for	O
states	O
with	O
a	O
usable	O
ace	O
are	O
less	O
certain	O
and	O
less	O
regular	O
because	O
these	O
states	O
are	O
less	O
common	O
.	O
in	O
any	O
event	O
,	O
after	O
500,000	O
games	O
the	O
value	B
function	I
is	O
very	O
well	O
approximated	O
.	O
94	O
chapter	O
5	O
:	O
monte	O
carlo	O
methods	O
figure	O
5.1	O
:	O
approximate	B
state-value	O
functions	O
for	O
the	O
blackjack	O
policy	O
that	O
sticks	O
only	O
on	O
20	O
or	O
21	O
,	O
computed	O
by	O
monte	O
carlo	O
policy	B
evaluation	I
.	O
exercise	O
5.1	O
consider	O
the	O
diagrams	O
on	O
the	O
right	O
in	O
figure	O
5.1.	O
why	O
does	O
the	O
estimated	O
value	B
function	I
jump	O
up	O
for	O
the	O
last	O
two	O
rows	O
in	O
the	O
rear	O
?	O
why	O
does	O
it	O
drop	O
oﬀ	O
for	O
the	O
whole	O
last	O
row	O
on	O
the	O
left	O
?	O
why	O
are	O
the	O
frontmost	O
values	O
higher	O
in	O
the	O
upper	O
diagrams	O
(	O
cid:3	O
)	O
than	O
in	O
the	O
lower	O
?	O
exercise	O
5.2	O
suppose	O
every-visit	O
mc	O
was	O
used	O
instead	O
of	O
ﬁrst-visit	O
mc	O
on	O
the	O
blackjack	O
(	O
cid:3	O
)	O
task	O
.	O
would	O
you	O
expect	O
the	O
results	O
to	O
be	O
very	O
diﬀerent	O
?	O
why	O
or	O
why	O
not	O
?	O
although	O
we	O
have	O
complete	O
knowledge	O
of	O
the	O
environment	B
in	O
the	O
blackjack	O
task	O
,	O
it	O
would	O
not	O
be	O
easy	O
to	O
apply	O
dp	O
methods	O
to	O
compute	O
the	O
value	B
function	I
.	O
dp	O
methods	O
require	O
the	O
distribution	O
of	O
next	O
events—in	O
particular	O
,	O
they	O
require	O
the	O
environments	O
dynamics	O
as	O
given	O
by	O
the	O
four-argument	O
function	O
p—and	O
it	O
is	O
not	O
easy	O
to	O
determine	O
this	O
for	O
blackjack	O
.	O
for	O
example	O
,	O
suppose	O
the	O
player	O
’	O
s	O
sum	O
is	O
14	O
and	O
he	O
chooses	O
to	O
stick	O
.	O
what	O
is	O
his	O
probability	O
of	O
terminating	O
with	O
a	O
reward	O
of	O
+1	O
as	O
a	O
function	O
of	O
the	O
dealer	O
’	O
s	O
showing	O
card	O
?	O
all	O
of	O
the	O
probabilities	O
must	O
be	O
computed	O
before	O
dp	O
can	O
be	O
applied	O
,	O
and	O
such	O
computations	O
are	O
often	O
complex	O
and	O
error-prone	O
.	O
in	O
contrast	O
,	O
generating	O
the	O
sample	O
games	O
required	O
by	O
monte	O
carlo	O
methods	O
is	O
easy	O
.	O
this	O
is	O
the	O
case	O
surprisingly	O
often	O
;	O
the	O
ability	O
of	O
monte	O
carlo	O
methods	O
to	O
work	O
with	O
sample	O
episodes	B
alone	O
can	O
be	O
a	O
signiﬁcant	O
advantage	O
even	O
when	O
one	O
has	O
complete	O
knowledge	O
of	O
the	O
environment	B
’	O
s	O
dynamics	O
.	O
can	O
we	O
generalize	O
the	O
idea	O
of	O
backup	O
diagrams	O
to	O
monte	O
carlo	O
algorithms	O
?	O
the	O
general	O
idea	O
of	O
a	O
backup	B
diagram	I
is	O
to	O
show	O
at	O
the	O
top	O
the	O
root	O
node	O
to	O
be	O
updated	O
and	O
to	O
show	O
below	O
all	O
the	O
transitions	O
and	O
leaf	O
nodes	O
whose	O
rewards	O
and	O
estimated	O
values	O
contribute	O
to	O
the	O
update	O
.	O
for	O
monte	O
carlo	O
estimation	O
of	O
vπ	O
,	O
the	O
root	O
is	O
a	O
state	B
node	O
,	O
and	O
below	O
it	O
is	O
the	O
entire	O
trajectory	O
of	O
transitions	O
along	O
a	O
particular	O
single	O
episode	O
,	O
ending	O
+1	O
!	O
1adealer	O
showing1012player	O
sum21after	O
500,000	O
episodesafter	O
10,000	O
episodesusableacenousableace	O
5.1.	O
monte	O
carlo	O
prediction	B
95	O
at	O
the	O
terminal	O
state	B
,	O
as	O
shown	O
to	O
the	O
right	O
.	O
whereas	O
the	O
dp	O
diagram	O
(	O
page	O
59	O
)	O
shows	O
all	O
possible	O
transitions	O
,	O
the	O
monte	O
carlo	O
diagram	O
shows	O
only	O
those	O
sampled	O
on	O
the	O
one	O
episode	O
.	O
whereas	O
the	O
dp	O
diagram	O
includes	O
only	O
one-step	O
transitions	O
,	O
the	O
monte	O
carlo	O
diagram	O
goes	O
all	O
the	O
way	O
to	O
the	O
end	O
of	O
the	O
episode	O
.	O
these	O
diﬀerences	O
in	O
the	O
diagrams	O
accurately	O
reﬂect	O
the	O
fundamental	O
diﬀerences	O
between	O
the	O
algorithms	O
.	O
an	O
important	O
fact	O
about	O
monte	O
carlo	O
methods	O
is	O
that	O
the	O
estimates	O
for	O
each	O
state	B
are	O
independent	O
.	O
the	O
estimate	O
for	O
one	O
state	B
does	O
not	O
build	O
upon	O
the	O
estimate	O
of	O
any	O
other	O
state	B
,	O
as	O
is	O
the	O
case	O
in	O
dp	O
.	O
in	O
other	O
words	O
,	O
monte	O
carlo	O
methods	O
do	O
not	O
bootstrap	O
as	O
we	O
deﬁned	O
it	O
in	O
the	O
previous	O
chapter	O
.	O
in	O
particular	O
,	O
note	O
that	O
the	O
computational	O
expense	O
of	O
estimating	O
the	O
value	B
of	O
a	O
single	O
state	B
is	O
independent	O
of	O
the	O
number	O
of	O
states	O
.	O
this	O
can	O
make	O
monte	O
carlo	O
methods	O
particularly	O
attractive	O
when	O
one	O
requires	O
the	O
value	B
of	O
only	O
one	O
or	O
a	O
subset	O
of	O
states	O
.	O
one	O
can	O
generate	O
many	O
sample	O
episodes	O
starting	O
from	O
the	O
states	O
of	O
interest	O
,	O
averaging	O
returns	O
from	O
only	O
these	O
states	O
,	O
ignoring	O
all	O
others	O
.	O
this	O
is	O
a	O
third	O
advantage	O
monte	O
carlo	O
methods	O
can	O
have	O
over	O
dp	O
methods	O
(	O
after	O
the	O
ability	O
to	O
learn	O
from	O
actual	O
experience	O
and	O
from	O
simulated	O
experience	O
)	O
.	O
example	O
5.2	O
:	O
soap	O
bubble	O
suppose	O
a	O
wire	O
frame	O
forming	O
a	O
closed	O
loop	O
is	O
dunked	O
in	O
soapy	O
water	O
to	O
form	O
a	O
soap	O
surface	O
or	O
bubble	O
conforming	O
at	O
its	O
edges	O
to	O
the	O
wire	O
frame	O
.	O
if	O
the	O
geometry	O
of	O
the	O
wire	O
frame	O
is	O
ir-	O
regular	O
but	O
known	O
,	O
how	O
can	O
you	O
compute	O
the	O
shape	O
of	O
the	O
surface	O
?	O
the	O
shape	O
has	O
the	O
prop-	O
erty	O
that	O
the	O
total	O
force	O
on	O
each	O
point	O
exerted	O
by	O
neighboring	O
points	O
is	O
zero	O
(	O
or	O
else	O
the	O
shape	O
would	O
change	O
)	O
.	O
this	O
means	O
that	O
the	O
surface	O
’	O
s	O
height	O
at	O
any	O
point	O
is	O
the	O
average	O
of	O
its	O
heights	O
at	O
points	O
in	O
a	O
small	O
circle	O
around	O
that	O
point	O
.	O
in	O
addition	O
,	O
the	O
surface	O
must	O
meet	O
at	O
its	O
bound-	O
aries	O
with	O
the	O
wire	O
frame	O
.	O
the	O
usual	O
approach	O
to	O
problems	O
of	O
this	O
kind	O
is	O
to	O
put	O
a	O
grid	O
over	O
the	O
area	O
covered	O
by	O
the	O
surface	O
and	O
solve	O
for	O
its	O
height	O
at	O
the	O
grid	O
points	O
by	O
an	O
iterative	B
com-	O
putation	O
.	O
grid	O
points	O
at	O
the	O
boundary	O
are	O
forced	O
to	O
the	O
wire	O
frame	O
,	O
and	O
all	O
others	O
are	O
adjusted	O
toward	O
the	O
average	O
of	O
the	O
heights	O
of	O
their	O
four	O
nearest	O
neighbors	O
.	O
this	O
process	O
then	O
iterates	O
,	O
much	O
like	O
dp	O
’	O
s	O
iterative	B
policy	O
evaluation	O
,	O
and	O
ultimately	O
converges	O
to	O
a	O
close	O
approximation	O
to	O
the	O
desired	O
surface	O
.	O
from	O
hersh	O
and	O
griego	O
(	O
1969	O
)	O
.	O
reproduced	O
with	O
permission	O
.	O
copyright	O
(	O
1969	O
)	O
scientiﬁc	O
american	O
,	O
a	O
division	O
of	O
nature	O
america	O
,	O
inc.	O
a	O
bubble	O
on	O
a	O
wire	O
loop	O
.	O
all	O
rights	O
reserved	O
.	O
this	O
is	O
similar	O
to	O
the	O
kind	O
of	O
problem	O
for	O
which	O
monte	O
carlo	O
methods	O
were	O
originally	O
designed	O
.	O
instead	O
of	O
the	O
iterative	B
computation	O
described	O
above	O
,	O
imagine	O
standing	O
on	O
the	O
surface	O
and	O
taking	O
a	O
random	B
walk	I
,	O
stepping	O
randomly	O
from	O
grid	O
point	O
to	O
neighboring	O
grid	O
point	O
,	O
with	O
equal	O
probability	O
,	O
until	O
you	O
reach	O
the	O
boundary	O
.	O
it	O
turns	O
out	O
that	O
the	O
expected	O
value	O
of	O
the	O
height	O
at	O
the	O
boundary	O
is	O
a	O
close	O
approximation	O
to	O
the	O
height	O
of	O
the	O
desired	O
surface	O
at	O
the	O
starting	O
point	O
(	O
in	O
fact	O
,	O
it	O
is	O
exactly	O
the	O
value	B
computed	O
by	O
the	O
96	O
chapter	O
5	O
:	O
monte	O
carlo	O
methods	O
iterative	B
method	O
described	O
above	O
)	O
.	O
thus	O
,	O
one	O
can	O
closely	O
approximate	B
the	O
height	O
of	O
the	O
surface	O
at	O
a	O
point	O
by	O
simply	O
averaging	O
the	O
boundary	O
heights	O
of	O
many	O
walks	O
started	O
at	O
the	O
point	O
.	O
if	O
one	O
is	O
interested	O
in	O
only	O
the	O
value	B
at	O
one	O
point	O
,	O
or	O
any	O
ﬁxed	O
small	O
set	O
of	O
points	O
,	O
then	O
this	O
monte	O
carlo	O
method	O
can	O
be	O
far	O
more	O
eﬃcient	O
than	O
the	O
iterative	B
method	O
based	O
on	O
local	O
consistency	O
.	O
5.2	O
monte	O
carlo	O
estimation	O
of	O
action	O
values	O
if	O
a	O
model	O
is	O
not	O
available	O
,	O
then	O
it	O
is	O
particularly	O
useful	O
to	O
estimate	O
action	B
values	O
(	O
the	O
val-	O
ues	O
of	O
state–action	O
pairs	O
)	O
rather	O
than	O
state	B
values	O
.	O
with	O
a	O
model	O
,	O
state	B
values	O
alone	O
are	O
suﬃcient	O
to	O
determine	O
a	O
policy	B
;	O
one	O
simply	O
looks	O
ahead	O
one	O
step	O
and	O
chooses	O
whichever	O
action	B
leads	O
to	O
the	O
best	O
combination	O
of	O
reward	O
and	O
next	O
state	B
,	O
as	O
we	O
did	O
in	O
the	O
chapter	O
on	O
dp	O
.	O
without	O
a	O
model	O
,	O
however	O
,	O
state	B
values	O
alone	O
are	O
not	O
suﬃcient	O
.	O
one	O
must	O
ex-	O
plicitly	O
estimate	O
the	O
value	B
of	O
each	O
action	B
in	O
order	O
for	O
the	O
values	O
to	O
be	O
useful	O
in	O
suggesting	O
a	O
policy	B
.	O
thus	O
,	O
one	O
of	O
our	O
primary	O
goals	O
for	O
monte	O
carlo	O
methods	O
is	O
to	O
estimate	O
q∗	O
.	O
to	O
achieve	O
this	O
,	O
we	O
ﬁrst	O
consider	O
the	O
policy	B
evaluation	I
problem	O
for	B
action	I
values	I
.	O
the	O
policy	B
evaluation	I
problem	O
for	B
action	I
values	I
is	O
to	O
estimate	O
qπ	O
(	O
s	O
,	O
a	O
)	O
,	O
the	O
expected	O
return	O
when	O
starting	O
in	O
state	O
s	O
,	O
taking	O
action	B
a	O
,	O
and	O
thereafter	O
following	O
policy	B
π.	O
the	O
monte	O
carlo	O
methods	O
for	O
this	O
are	O
essentially	O
the	O
same	O
as	O
just	O
presented	O
for	O
state	O
values	O
,	O
except	O
now	O
we	O
talk	O
about	O
visits	O
to	O
a	O
state–action	O
pair	O
rather	O
than	O
to	O
a	O
state	B
.	O
a	O
state–	O
action	B
pair	O
s	O
,	O
a	O
is	O
said	O
to	O
be	O
visited	O
in	O
an	O
episode	O
if	O
ever	O
the	O
state	B
s	O
is	O
visited	O
and	O
action	O
a	O
is	O
taken	O
in	O
it	O
.	O
the	O
every-visit	O
mc	O
method	O
estimates	O
the	O
value	B
of	O
a	O
state–action	O
pair	O
as	O
the	O
average	O
of	O
the	O
returns	O
that	O
have	O
followed	O
all	O
the	O
visits	O
to	O
it	O
.	O
the	O
ﬁrst-visit	O
mc	O
method	O
averages	O
the	O
returns	O
following	O
the	O
ﬁrst	O
time	O
in	O
each	O
episode	O
that	O
the	O
state	B
was	O
visited	O
and	O
the	O
action	O
was	O
selected	O
.	O
these	O
methods	O
converge	O
quadratically	O
,	O
as	O
before	O
,	O
to	O
the	O
true	O
expected	O
values	O
as	O
the	O
number	O
of	O
visits	O
to	O
each	O
state–action	O
pair	O
approaches	O
inﬁnity	O
.	O
the	O
only	O
complication	O
is	O
that	O
many	O
state–action	O
pairs	O
may	O
never	O
be	O
visited	O
.	O
if	O
π	O
is	O
a	O
deterministic	O
policy	B
,	O
then	O
in	O
following	O
π	O
one	O
will	O
observe	O
returns	O
only	O
for	O
one	O
of	O
the	O
actions	O
from	O
each	O
state	B
.	O
with	O
no	O
returns	O
to	O
average	O
,	O
the	O
monte	O
carlo	O
estimates	O
of	O
the	O
other	O
actions	O
will	O
not	O
improve	O
with	O
experience	O
.	O
this	O
is	O
a	O
serious	O
problem	O
because	O
the	O
purpose	O
of	O
learning	O
action	B
values	O
is	O
to	O
help	O
in	O
choosing	O
among	O
the	O
actions	O
available	O
in	O
each	O
state	B
.	O
to	O
compare	O
alternatives	O
we	O
need	O
to	O
estimate	O
the	O
value	B
of	O
all	O
the	O
actions	O
from	O
each	O
state	B
,	O
not	O
just	O
the	O
one	O
we	O
currently	O
favor	O
.	O
this	O
is	O
the	O
general	O
problem	O
of	O
maintaining	O
exploration	O
,	O
as	O
discussed	O
in	O
the	O
context	O
of	O
the	O
k-armed	O
bandit	O
problem	O
in	O
chapter	O
2.	O
for	O
policy	O
evaluation	O
to	O
work	O
for	B
action	I
values	I
,	O
we	O
must	O
assure	O
continual	O
exploration	O
.	O
one	O
way	O
to	O
do	O
this	O
is	O
by	O
specifying	O
that	O
the	O
episodes	B
start	O
in	O
a	O
state–action	O
pair	O
,	O
and	O
that	O
every	O
pair	O
has	O
a	O
nonzero	O
probability	O
of	O
being	O
selected	O
as	O
the	O
start	O
.	O
this	O
guarantees	O
that	O
all	O
state–action	O
pairs	O
will	O
be	O
visited	O
an	O
inﬁnite	O
number	O
of	O
times	O
in	O
the	O
limit	O
of	O
an	O
inﬁnite	O
number	O
of	O
episodes	O
.	O
we	O
call	O
this	O
the	O
assumption	O
of	O
exploring	O
starts	O
.	O
the	O
assumption	O
of	O
exploring	O
starts	O
is	O
sometimes	O
useful	O
,	O
but	O
of	O
course	O
it	O
can	O
not	O
be	O
relied	O
upon	O
in	O
general	O
,	O
particularly	O
when	O
learning	O
directly	O
from	O
actual	O
interaction	O
with	O
an	O
environment	B
.	O
in	O
that	O
case	O
the	O
starting	O
conditions	O
are	O
unlikely	O
to	O
be	O
so	O
helpful	O
.	O
the	O
5.3.	O
monte	O
carlo	O
control	B
97	O
most	O
common	O
alternative	O
approach	O
to	O
assuring	O
that	O
all	O
state–action	O
pairs	O
are	O
encountered	O
is	O
to	O
consider	O
only	O
policies	O
that	O
are	O
stochastic	O
with	O
a	O
nonzero	O
probability	O
of	O
selecting	O
all	O
actions	O
in	O
each	O
state	B
.	O
we	O
discuss	O
two	O
important	O
variants	O
of	O
this	O
approach	O
in	O
later	O
sections	O
.	O
for	O
now	O
,	O
we	O
retain	O
the	O
assumption	O
of	O
exploring	O
starts	O
and	O
complete	O
the	O
presentation	O
of	O
a	O
full	O
monte	O
carlo	O
control	B
method	O
.	O
exercise	O
5.3	O
what	O
is	O
the	O
backup	B
diagram	I
for	O
monte	O
carlo	O
estimation	O
of	O
qπ	O
?	O
(	O
cid:3	O
)	O
5.3	O
monte	O
carlo	O
control	B
we	O
are	O
now	O
ready	O
to	O
consider	O
how	O
monte	O
carlo	O
estimation	O
can	O
be	O
used	O
in	O
control	O
,	O
that	O
is	O
,	O
to	O
approximate	B
optimal	O
policies	O
.	O
the	O
overall	O
idea	O
is	O
to	O
proceed	O
according	O
to	O
the	O
same	O
pattern	O
as	O
in	O
the	O
dp	O
chapter	O
,	O
that	O
is	O
,	O
according	O
to	O
the	O
idea	O
of	O
generalized	O
pol-	O
icy	O
iteration	O
(	O
gpi	O
)	O
.	O
in	O
gpi	O
one	O
maintains	O
both	O
an	O
approximate	B
policy	O
and	O
an	O
approximate	B
value	O
function	O
.	O
the	O
value	B
function	I
is	O
repeatedly	O
altered	O
to	O
more	O
closely	O
approximate	B
the	O
value	B
func-	O
tion	B
for	O
the	O
current	O
policy	B
,	O
and	O
the	O
policy	O
is	O
repeatedly	O
improved	O
with	O
respect	O
to	O
the	O
current	O
value	B
function	I
,	O
as	O
suggested	O
by	O
the	O
diagram	O
to	O
the	O
right	O
.	O
these	O
two	O
kinds	O
of	O
changes	O
work	O
against	O
each	O
other	O
to	O
some	O
extent	O
,	O
as	O
each	O
creates	O
a	O
moving	O
target	B
for	O
the	O
other	O
,	O
but	O
together	O
they	O
cause	O
both	O
policy	B
and	O
value	B
function	I
to	O
approach	O
optimality	O
.	O
to	O
begin	O
,	O
let	O
us	O
consider	O
a	O
monte	O
carlo	O
version	O
of	O
classical	O
policy	B
iteration	I
.	O
in	O
this	O
method	O
,	O
we	O
perform	O
alternating	O
complete	O
steps	O
of	O
policy	O
evaluation	O
and	O
policy	O
improve-	O
ment	O
,	O
beginning	O
with	O
an	O
arbitrary	O
policy	B
π0	O
and	O
ending	O
with	O
the	O
optimal	O
policy	O
and	O
optimal	O
action-value	B
function	I
:	O
e−→	O
qπ1	O
e−→	O
···	O
i−→	O
π1	O
i−→	O
π2	O
e−→	O
qπ0	O
i−→	O
π∗	O
π0	O
e−→	O
denotes	O
a	O
complete	O
policy	B
evaluation	I
and	O
e−→	O
q∗	O
,	O
i−→	O
denotes	O
a	O
complete	O
policy	B
where	O
improvement	O
.	O
policy	B
evaluation	I
is	O
done	O
exactly	O
as	O
described	O
in	O
the	O
preceding	O
section	O
.	O
many	O
episodes	B
are	O
experienced	O
,	O
with	O
the	O
approximate	B
action-value	O
function	O
approaching	O
the	O
true	O
function	O
asymptotically	O
.	O
for	O
the	O
moment	O
,	O
let	O
us	O
assume	O
that	O
we	O
do	O
indeed	O
observe	O
an	O
inﬁnite	O
number	O
of	O
episodes	O
and	O
that	O
,	O
in	O
addition	O
,	O
the	O
episodes	B
are	O
generated	O
with	O
exploring	O
starts	O
.	O
under	O
these	O
assumptions	O
,	O
the	O
monte	O
carlo	O
methods	O
will	O
compute	O
each	O
qπk	O
exactly	O
,	O
for	O
arbitrary	O
πk	O
.	O
policy	B
improvement	I
is	O
done	O
by	O
making	O
the	O
policy	B
greedy	O
with	O
respect	O
to	O
the	O
current	O
value	B
function	I
.	O
in	O
this	O
case	O
we	O
have	O
an	O
action-value	B
function	I
,	O
and	O
therefore	O
no	O
model	O
is	O
needed	O
to	O
construct	O
the	O
greedy	O
policy	O
.	O
for	O
any	O
action-value	B
function	I
q	O
,	O
the	O
corresponding	O
greedy	O
policy	O
is	O
the	O
one	O
that	O
,	O
for	O
each	O
s	O
∈	O
s	O
,	O
deterministically	O
chooses	O
an	O
action	B
with	O
maximal	O
action-value	O
:	O
π	O
(	O
s	O
)	O
.	O
=	O
arg	O
max	O
a	O
q	O
(	O
s	O
,	O
a	O
)	O
.	O
(	O
5.1	O
)	O
policy	B
improvement	I
then	O
can	O
be	O
done	O
by	O
constructing	O
each	O
πk+1	O
as	O
the	O
greedy	O
policy	O
with	O
respect	O
to	O
qπk	O
.	O
the	O
policy	B
improvement	I
theorem	O
(	O
section	O
4.2	O
)	O
then	O
applies	O
to	O
πk	O
evaluationimprovement⇡q⇡	O
greedy	O
(	O
q	O
)	O
q	O
q⇡	O
98	O
chapter	O
5	O
:	O
monte	O
carlo	O
methods	O
and	O
πk+1	O
because	O
,	O
for	O
all	O
s	O
∈	O
s	O
,	O
qπk	O
(	O
s	O
,	O
πk+1	O
(	O
s	O
)	O
)	O
=	O
qπk	O
(	O
s	O
,	O
argmax	O
a	O
qπk	O
(	O
s	O
,	O
a	O
)	O
)	O
a	O
=	O
max	O
qπk	O
(	O
s	O
,	O
a	O
)	O
≥	O
qπk	O
(	O
s	O
,	O
πk	O
(	O
s	O
)	O
)	O
≥	O
vπk	O
(	O
s	O
)	O
.	O
as	O
we	O
discussed	O
in	O
the	O
previous	O
chapter	O
,	O
the	O
theorem	B
assures	O
us	O
that	O
each	O
πk+1	O
is	O
uni-	O
formly	O
better	O
than	O
πk	O
,	O
or	O
just	O
as	O
good	O
as	O
πk	O
,	O
in	O
which	O
case	O
they	O
are	O
both	O
optimal	O
policies	O
.	O
this	O
in	O
turn	O
assures	O
us	O
that	O
the	O
overall	O
process	O
converges	O
to	O
the	O
optimal	O
policy	O
and	O
opti-	O
mal	O
value	B
function	I
.	O
in	O
this	O
way	O
monte	O
carlo	O
methods	O
can	O
be	O
used	O
to	O
ﬁnd	O
optimal	O
policies	O
given	O
only	O
sample	O
episodes	O
and	O
no	O
other	O
knowledge	O
of	O
the	O
environment	B
’	O
s	O
dynamics	O
.	O
we	O
made	O
two	O
unlikely	O
assumptions	O
above	O
in	O
order	O
to	O
easily	O
obtain	O
this	O
guarantee	O
of	O
convergence	O
for	O
the	O
monte	O
carlo	O
method	O
.	O
one	O
was	O
that	O
the	O
episodes	B
have	O
exploring	B
starts	I
,	O
and	O
the	O
other	O
was	O
that	O
policy	B
evaluation	I
could	O
be	O
done	O
with	O
an	O
inﬁnite	O
number	O
of	O
episodes	O
.	O
to	O
obtain	O
a	O
practical	O
algorithm	O
we	O
will	O
have	O
to	O
remove	O
both	O
assumptions	O
.	O
we	O
postpone	O
consideration	O
of	O
the	O
ﬁrst	O
assumption	O
until	O
later	O
in	O
this	O
chapter	O
.	O
for	O
now	O
we	O
focus	O
on	O
the	O
assumption	O
that	O
policy	B
evaluation	I
operates	O
on	O
an	O
inﬁnite	O
number	O
of	O
episodes	O
.	O
this	O
assumption	O
is	O
relatively	O
easy	O
to	O
remove	O
.	O
in	O
fact	O
,	O
the	O
same	O
issue	O
arises	O
even	O
in	O
classical	O
dp	O
methods	O
such	O
as	O
iterative	O
policy	B
evaluation	I
,	O
which	O
also	O
converge	O
only	O
asymptotically	O
to	O
the	O
true	O
value	O
function	O
.	O
in	O
both	O
dp	O
and	O
monte	O
carlo	O
cases	O
there	O
are	O
two	O
ways	O
to	O
solve	O
the	O
problem	O
.	O
one	O
is	O
to	O
hold	O
ﬁrm	O
to	O
the	O
idea	O
of	O
approximating	O
qπk	O
in	O
each	O
policy	B
evaluation	I
.	O
measurements	O
and	O
assumptions	O
are	O
made	O
to	O
obtain	O
bounds	O
on	O
the	O
magnitude	O
and	O
probability	O
of	O
error	O
in	O
the	O
estimates	O
,	O
and	O
then	O
suﬃcient	O
steps	O
are	O
taken	O
during	O
each	O
policy	B
evaluation	I
to	O
assure	O
that	O
these	O
bounds	O
are	O
suﬃciently	O
small	O
.	O
this	O
approach	O
can	O
probably	O
be	O
made	O
completely	O
satisfactory	O
in	O
the	O
sense	O
of	O
guaranteeing	O
correct	O
convergence	O
up	O
to	O
some	O
level	O
of	O
approximation	O
.	O
however	O
,	O
it	O
is	O
also	O
likely	O
to	O
require	O
far	O
too	O
many	O
episodes	B
to	O
be	O
useful	O
in	O
practice	O
on	O
any	O
but	O
the	O
smallest	O
problems	O
.	O
there	O
is	O
a	O
second	O
approach	O
to	O
avoiding	O
the	O
inﬁnite	O
number	O
of	O
episodes	O
nominally	O
required	O
for	O
policy	O
evaluation	O
,	O
in	O
which	O
we	O
give	O
up	O
trying	O
to	O
complete	O
policy	B
evaluation	I
before	O
returning	O
to	O
policy	B
improvement	I
.	O
on	O
each	O
evaluation	O
step	O
we	O
move	O
the	O
value	B
function	I
toward	O
qπk	O
,	O
but	O
we	O
do	O
not	O
expect	O
to	O
actually	O
get	O
close	O
except	O
over	O
many	O
steps	O
.	O
we	O
used	O
this	O
idea	O
when	O
we	O
ﬁrst	O
introduced	O
the	O
idea	O
of	O
gpi	O
in	O
section	O
4.6.	O
one	O
extreme	O
form	O
of	O
the	O
idea	O
is	O
value	B
iteration	I
,	O
in	O
which	O
only	O
one	O
iteration	O
of	O
iterative	O
policy	B
evaluation	I
is	O
performed	O
between	O
each	O
step	O
of	O
policy	O
improvement	O
.	O
the	O
in-place	O
version	O
of	O
value	O
iteration	O
is	O
even	O
more	O
extreme	O
;	O
there	O
we	O
alternate	O
between	O
improvement	O
and	O
evaluation	O
steps	O
for	O
single	O
states	O
.	O
for	O
monte	O
carlo	O
policy	B
evaluation	I
it	O
is	O
natural	O
to	O
alternate	O
between	O
evaluation	O
and	O
improvement	O
on	O
an	O
episode-by-episode	O
basis	O
.	O
after	O
each	O
episode	O
,	O
the	O
observed	O
returns	O
are	O
used	O
for	O
policy	O
evaluation	O
,	O
and	O
then	O
the	O
policy	B
is	O
improved	O
at	O
all	O
the	O
states	O
visited	O
in	O
the	O
episode	O
.	O
a	O
complete	O
simple	O
algorithm	O
along	O
these	O
lines	O
,	O
which	O
we	O
call	O
monte	O
carlo	O
es	O
,	O
for	O
monte	O
carlo	O
with	O
exploring	O
starts	O
,	O
is	O
given	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
.	O
5.4.	O
monte	O
carlo	O
control	B
without	O
exploring	B
starts	I
99	O
monte	O
carlo	O
es	O
(	O
exploring	B
starts	I
)	O
,	O
for	O
estimating	O
π	O
≈	O
π∗	O
initialize	O
:	O
π	O
(	O
s	O
)	O
∈	O
a	O
(	O
s	O
)	O
(	O
arbitrarily	O
)	O
,	O
for	O
all	O
s	O
∈	O
s	O
q	O
(	O
s	O
,	O
a	O
)	O
∈	O
r	O
(	O
arbitrarily	O
)	O
,	O
for	O
all	O
s	O
∈	O
s	O
,	O
a	O
∈	O
a	O
(	O
s	O
)	O
returns	O
(	O
s	O
,	O
a	O
)	O
←	O
empty	O
list	O
,	O
for	O
all	O
s	O
∈	O
s	O
,	O
a	O
∈	O
a	O
(	O
s	O
)	O
loop	O
forever	O
(	O
for	O
each	O
episode	O
)	O
:	O
choose	O
s0	O
∈	O
s	O
and	O
a0	O
∈	O
a	O
(	O
s0	O
)	O
such	O
that	O
all	O
pairs	O
have	O
probability	O
>	O
0	O
generate	O
an	O
episode	O
from	O
s0	O
,	O
a0	O
,	O
following	O
π	O
:	O
s0	O
,	O
a0	O
,	O
r1	O
,	O
.	O
.	O
.	O
,	O
st−1	O
,	O
at−1	O
,	O
rt	O
g	O
←	O
0	O
loop	O
for	O
each	O
step	O
of	O
episode	O
,	O
t	O
=	O
t	O
−1	O
,	O
t	O
−2	O
,	O
.	O
.	O
.	O
,	O
0	O
:	O
g	O
←	O
g	O
+	O
rt+1	O
unless	O
the	O
pair	O
st	O
,	O
at	O
appears	O
in	O
s0	O
,	O
a0	O
,	O
s1	O
,	O
a1	O
.	O
.	O
.	O
,	O
st−1	O
,	O
at−1	O
:	O
append	O
g	O
to	O
returns	O
(	O
st	O
,	O
at	O
)	O
q	O
(	O
st	O
,	O
at	O
)	O
←	O
average	O
(	O
returns	O
(	O
st	O
,	O
at	O
)	O
)	O
π	O
(	O
st	O
)	O
←	O
argmaxa	O
q	O
(	O
st	O
,	O
a	O
)	O
in	O
monte	O
carlo	O
es	O
,	O
all	O
the	O
returns	O
for	O
each	O
state–action	O
pair	O
are	O
accumulated	O
and	O
averaged	O
,	O
irrespective	O
of	O
what	O
policy	B
was	O
in	O
force	O
when	O
they	O
were	O
observed	O
.	O
it	O
is	O
easy	O
to	O
see	O
that	O
monte	O
carlo	O
es	O
can	O
not	O
converge	O
to	O
any	O
suboptimal	O
policy	B
.	O
if	O
it	O
did	O
,	O
then	O
the	O
value	B
function	I
would	O
eventually	O
converge	O
to	O
the	O
value	B
function	I
for	O
that	O
policy	B
,	O
and	O
that	O
in	O
turn	O
would	O
cause	O
the	O
policy	B
to	O
change	O
.	O
stability	O
is	O
achieved	O
only	O
when	O
both	O
the	O
policy	B
and	O
the	O
value	B
function	I
are	O
optimal	O
.	O
convergence	O
to	O
this	O
optimal	O
ﬁxed	O
point	O
seems	O
inevitable	O
as	O
the	O
changes	O
to	O
the	O
action-value	B
function	I
decrease	O
over	O
time	O
,	O
but	O
has	O
not	O
yet	O
been	O
formally	O
proved	O
.	O
in	O
our	O
opinion	O
,	O
this	O
is	O
one	O
of	O
the	O
most	O
fundamental	O
open	O
theoretical	O
questions	O
in	O
reinforcement	O
learning	O
(	O
for	O
a	O
partial	O
solution	O
,	O
see	O
tsitsiklis	O
,	O
2002	O
)	O
.	O
example	O
5.3	O
:	O
solving	O
blackjack	O
it	O
is	O
straightforward	O
to	O
apply	O
monte	O
carlo	O
es	O
to	O
blackjack	O
.	O
because	O
the	O
episodes	B
are	O
all	O
simulated	O
games	O
,	O
it	O
is	O
easy	O
to	O
arrange	O
for	O
exploring	O
starts	O
that	O
include	O
all	O
possibilities	O
.	O
in	O
this	O
case	O
one	O
simply	O
picks	O
the	O
dealer	O
’	O
s	O
cards	O
,	O
the	O
player	O
’	O
s	O
sum	O
,	O
and	O
whether	O
or	O
not	O
the	O
player	O
has	O
a	O
usable	O
ace	O
,	O
all	O
at	O
random	O
with	O
equal	O
probability	O
.	O
as	O
the	O
initial	O
policy	B
we	O
use	O
the	O
policy	B
evaluated	O
in	O
the	O
previous	O
blackjack	B
example	I
,	O
that	O
which	O
sticks	O
only	O
on	O
20	O
or	O
21.	O
the	O
initial	O
action-value	B
function	I
can	O
be	O
zero	O
for	O
all	O
state–action	O
pairs	O
.	O
figure	O
5.2	O
shows	O
the	O
optimal	O
policy	O
for	O
blackjack	O
found	O
by	O
monte	O
carlo	O
es	O
.	O
this	O
policy	B
is	O
the	O
same	O
as	O
the	O
“	O
basic	O
”	O
strategy	O
of	O
thorp	O
(	O
1966	O
)	O
with	O
the	O
sole	O
exception	O
of	O
the	O
leftmost	O
notch	O
in	O
the	O
policy	O
for	O
a	O
usable	O
ace	O
,	O
which	O
is	O
not	O
present	O
in	O
thorp	O
’	O
s	O
strategy	O
.	O
we	O
are	O
uncertain	O
of	O
the	O
reason	O
for	O
this	O
discrepancy	O
,	O
but	O
conﬁdent	O
that	O
what	O
is	O
shown	O
here	O
is	O
indeed	O
the	O
optimal	O
policy	O
for	O
the	O
version	O
of	O
blackjack	O
we	O
have	O
described	O
.	O
100	O
chapter	O
5	O
:	O
monte	O
carlo	O
methods	O
figure	O
5.2	O
:	O
the	O
optimal	O
policy	O
and	O
state-value	O
function	O
for	O
blackjack	O
,	O
found	O
by	O
monte	O
carlo	O
es	O
.	O
the	O
state-value	O
function	O
shown	O
was	O
computed	O
from	O
the	O
action-value	B
function	I
found	O
by	O
monte	O
carlo	O
es	O
.	O
5.4	O
monte	O
carlo	O
control	B
without	O
exploring	B
starts	I
how	O
can	O
we	O
avoid	O
the	O
unlikely	O
assumption	O
of	O
exploring	O
starts	O
?	O
the	O
only	O
general	O
way	O
to	O
ensure	O
that	O
all	O
actions	O
are	O
selected	O
inﬁnitely	O
often	O
is	O
for	O
the	O
agent	O
to	O
continue	O
to	O
select	O
them	O
.	O
there	O
are	O
two	O
approaches	O
to	O
ensuring	O
this	O
,	O
resulting	O
in	O
what	O
we	O
call	O
on-policy	B
methods	I
and	O
oﬀ-policy	B
methods	I
.	O
on-policy	B
methods	I
attempt	O
to	O
evaluate	O
or	O
improve	O
the	O
policy	B
that	O
is	O
used	O
to	O
make	O
decisions	O
,	O
whereas	O
oﬀ-policy	B
methods	I
evaluate	O
or	O
improve	O
a	O
policy	B
diﬀerent	O
from	O
that	O
used	O
to	O
generate	O
the	O
data	O
.	O
the	O
monte	O
carlo	O
es	O
method	O
developed	O
above	O
is	O
an	O
example	O
of	O
an	O
on-policy	O
method	O
.	O
in	O
this	O
section	O
we	O
show	O
how	O
an	O
on-policy	O
monte	O
carlo	O
control	B
method	O
can	O
be	O
designed	O
that	O
does	O
not	O
use	O
the	O
unrealistic	O
assumption	O
of	O
exploring	O
starts	O
.	O
oﬀ-policy	B
methods	I
are	O
considered	O
in	O
the	O
next	O
section	O
.	O
in	O
on-policy	O
control	B
methods	O
the	O
policy	B
is	O
generally	O
soft	O
,	O
meaning	O
that	O
π	O
(	O
a|s	O
)	O
>	O
0	O
for	O
all	O
s	O
∈	O
s	O
and	O
all	O
a	O
∈	O
a	O
(	O
s	O
)	O
,	O
but	O
gradually	O
shifted	O
closer	O
and	O
closer	O
to	O
a	O
deterministic	O
optimal	O
policy	O
.	O
many	O
of	O
the	O
methods	O
discussed	O
in	O
chapter	O
2	O
provide	O
mechanisms	O
for	O
this	O
.	O
the	O
on-policy	O
method	O
we	O
present	O
in	O
this	O
section	O
uses	O
ε-greedy	B
policies	I
,	O
meaning	O
that	O
most	O
of	O
the	O
time	O
they	O
choose	O
an	O
action	B
that	O
has	O
maximal	O
estimated	O
action	B
value	O
,	O
but	O
with	O
probability	O
ε	O
they	O
instead	O
select	O
an	O
action	B
at	O
random	O
.	O
that	O
is	O
,	O
all	O
nongreedy	O
,	O
and	O
the	O
remaining	O
bulk	O
of	O
actions	O
are	O
given	O
the	O
minimal	O
probability	O
of	O
selection	O
,	O
the	O
probability	O
,	O
1	O
−	O
ε	O
+	O
ε	O
,	O
is	O
given	O
to	O
the	O
greedy	O
action	O
.	O
the	O
ε-greedy	B
policies	I
are	O
|a	O
(	O
s	O
)	O
|	O
examples	O
of	O
ε-soft	O
policies	O
,	O
deﬁned	O
as	O
policies	O
for	O
which	O
π	O
(	O
a|s	O
)	O
≥	O
ε	O
for	O
all	O
states	O
and	O
|a	O
(	O
s	O
)	O
|	O
actions	O
,	O
for	O
some	O
ε	O
>	O
0.	O
among	O
ε-soft	O
policies	O
,	O
ε-greedy	B
policies	I
are	O
in	O
some	O
sense	O
those	O
that	O
are	O
closest	O
to	O
greedy	O
.	O
|a	O
(	O
s	O
)	O
|	O
ε	O
usableacenousableace2010a23456789dealer	O
showingplayer	O
sumhitstick19211112131415161718	O
!	O
*10a23456789hitstick2019211112131415161718v*211012adealer	O
showingplayer	O
sum10a1221+1	O
''	O
1v*usableacenousableace2010a23456789dealer	O
showingplayer	O
sumhitstick19211112131415161718	O
!	O
*10a23456789hitstick2019211112131415161718v*211012adealer	O
showingplayer	O
sum10a1221+1	O
''	O
1v*usableacenousableace2010a23456789dealer	O
showingplayer	O
sumhitstick19211112131415161718	O
!	O
*10a23456789hitstick2019211112131415161718v*211012adealer	O
showingplayer	O
sum10a1221+1	O
''	O
1v*dealer	O
showingplayer	O
sum**	O
5.4.	O
monte	O
carlo	O
control	B
without	O
exploring	B
starts	I
101	O
the	O
overall	O
idea	O
of	O
on-policy	O
monte	O
carlo	O
control	B
is	O
still	O
that	O
of	O
gpi	O
.	O
as	O
in	O
monte	O
carlo	O
es	O
,	O
we	O
use	O
ﬁrst-visit	O
mc	O
methods	O
to	O
estimate	O
the	O
action-value	B
function	I
for	O
the	O
current	O
policy	B
.	O
without	O
the	O
assumption	O
of	O
exploring	O
starts	O
,	O
however	O
,	O
we	O
can	O
not	O
simply	O
improve	O
the	O
policy	B
by	O
making	O
it	O
greedy	O
with	O
respect	O
to	O
the	O
current	O
value	B
function	I
,	O
because	O
that	O
would	O
prevent	O
further	O
exploration	O
of	O
nongreedy	O
actions	O
.	O
fortunately	O
,	O
gpi	O
does	O
not	O
require	O
that	O
the	O
policy	B
be	O
taken	O
all	O
the	O
way	O
to	O
a	O
greedy	O
policy	O
,	O
only	O
that	O
it	O
be	O
moved	O
toward	O
a	O
greedy	O
policy	O
.	O
in	O
our	O
on-policy	O
method	O
we	O
will	O
move	O
it	O
only	O
to	O
an	O
ε-greedy	O
policy	O
.	O
for	O
any	O
ε-soft	O
policy	B
,	O
π	O
,	O
any	O
ε-greedy	O
policy	O
with	O
respect	O
to	O
qπ	O
is	O
guaranteed	O
to	O
be	O
better	O
than	O
or	O
equal	O
to	O
π.	O
the	O
complete	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
below	O
.	O
on-policy	O
ﬁrst-visit	O
mc	O
control	B
(	O
for	O
ε-soft	O
policies	O
)	O
,	O
estimates	O
π	O
≈	O
π∗	O
algorithm	O
parameter	O
:	O
small	O
ε	O
>	O
0	O
initialize	O
:	O
π	O
←	O
an	O
arbitrary	O
ε-soft	O
policy	B
q	O
(	O
s	O
,	O
a	O
)	O
∈	O
r	O
(	O
arbitrarily	O
)	O
,	O
for	O
all	O
s	O
∈	O
s	O
,	O
a	O
∈	O
a	O
(	O
s	O
)	O
returns	O
(	O
s	O
,	O
a	O
)	O
←	O
empty	O
list	O
,	O
for	O
all	O
s	O
∈	O
s	O
,	O
a	O
∈	O
a	O
(	O
s	O
)	O
repeat	O
forever	O
(	O
for	O
each	O
episode	O
)	O
:	O
generate	O
an	O
episode	O
following	O
π	O
:	O
s0	O
,	O
a0	O
,	O
r1	O
,	O
.	O
.	O
.	O
,	O
st−1	O
,	O
at−1	O
,	O
rt	O
g	O
←	O
0	O
loop	O
for	O
each	O
step	O
of	O
episode	O
,	O
t	O
=	O
t	O
−1	O
,	O
t	O
−2	O
,	O
.	O
.	O
.	O
,	O
0	O
:	O
g	O
←	O
g	O
+	O
rt+1	O
unless	O
the	O
pair	O
st	O
,	O
at	O
appears	O
in	O
s0	O
,	O
a0	O
,	O
s1	O
,	O
a1	O
.	O
.	O
.	O
,	O
st−1	O
,	O
at−1	O
:	O
append	O
g	O
to	O
returns	O
(	O
st	O
,	O
at	O
)	O
q	O
(	O
st	O
,	O
at	O
)	O
←	O
average	O
(	O
returns	O
(	O
st	O
,	O
at	O
)	O
)	O
a∗	O
←	O
arg	O
maxa	O
q	O
(	O
st	O
,	O
a	O
)	O
for	O
all	O
a	O
∈	O
a	O
(	O
st	O
)	O
:	O
π	O
(	O
a|st	O
)	O
←	O
(	O
cid:26	O
)	O
1	O
−	O
ε	O
+	O
ε/|a	O
(	O
st	O
)	O
|	O
ε/|a	O
(	O
st	O
)	O
|	O
(	O
with	O
ties	O
broken	O
arbitrarily	O
)	O
if	O
a	O
=	O
a∗	O
if	O
a	O
(	O
cid:54	O
)	O
=	O
a∗	O
that	O
any	O
ε-greedy	O
policy	O
with	O
respect	O
to	O
qπ	O
is	O
an	O
improvement	O
over	O
any	O
ε-soft	O
policy	B
π	O
is	O
assured	O
by	O
the	O
policy	B
improvement	I
theorem	O
.	O
let	O
π	O
(	O
cid:48	O
)	O
be	O
the	O
ε-greedy	O
policy	O
.	O
the	O
conditions	O
of	O
the	O
policy	B
improvement	I
theorem	O
apply	O
because	O
for	O
any	O
s	O
∈	O
s	O
:	O
qπ	O
(	O
s	O
,	O
π	O
(	O
cid:48	O
)	O
(	O
s	O
)	O
)	O
=	O
(	O
cid:88	O
)	O
a	O
ε	O
π	O
(	O
cid:48	O
)	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
|a	O
(	O
s	O
)	O
|	O
(	O
cid:88	O
)	O
a	O
|a	O
(	O
s	O
)	O
|	O
(	O
cid:88	O
)	O
a	O
ε	O
=	O
≥	O
qπ	O
(	O
s	O
,	O
a	O
)	O
+	O
(	O
1	O
−	O
ε	O
)	O
max	O
a	O
qπ	O
(	O
s	O
,	O
a	O
)	O
(	O
5.2	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
+	O
(	O
1	O
−	O
ε	O
)	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
a|s	O
)	O
−	O
ε	O
|a	O
(	O
s	O
)	O
|	O
1	O
−	O
ε	O
qπ	O
(	O
s	O
,	O
a	O
)	O
(	O
the	O
sum	O
is	O
a	O
weighted	O
average	O
with	O
nonnegative	O
weights	O
summing	O
to	O
1	O
,	O
and	O
as	O
such	O
it	O
102	O
chapter	O
5	O
:	O
monte	O
carlo	O
methods	O
must	O
be	O
less	O
than	O
or	O
equal	O
to	O
the	O
largest	O
number	O
averaged	O
)	O
=	O
ε	O
|a	O
(	O
s	O
)	O
|	O
(	O
cid:88	O
)	O
a	O
qπ	O
(	O
s	O
,	O
a	O
)	O
−	O
ε	O
|a	O
(	O
s	O
)	O
|	O
(	O
cid:88	O
)	O
a	O
qπ	O
(	O
s	O
,	O
a	O
)	O
+	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
=	O
vπ	O
(	O
s	O
)	O
.	O
thus	O
,	O
by	O
the	O
policy	B
improvement	I
theorem	O
,	O
π	O
(	O
cid:48	O
)	O
≥	O
π	O
(	O
i.e.	O
,	O
vπ	O
(	O
cid:48	O
)	O
(	O
s	O
)	O
≥	O
vπ	O
(	O
s	O
)	O
,	O
for	O
all	O
s	O
∈	O
s	O
)	O
.	O
we	O
now	O
prove	O
that	O
equality	O
can	O
hold	O
only	O
when	O
both	O
π	O
(	O
cid:48	O
)	O
and	O
π	O
are	O
optimal	O
among	O
the	O
ε-soft	O
policies	O
,	O
that	O
is	O
,	O
when	O
they	O
are	O
better	O
than	O
or	O
equal	O
to	O
all	O
other	O
ε-soft	O
policies	O
.	O
consider	O
a	O
new	O
environment	B
that	O
is	O
just	O
like	O
the	O
original	O
environment	B
,	O
except	O
with	O
the	O
requirement	O
that	O
policies	O
be	O
ε-soft	O
“	O
moved	O
inside	O
”	O
the	O
environment	B
.	O
the	O
new	O
environ-	O
ment	O
has	O
the	O
same	O
action	B
and	O
state	B
set	O
as	O
the	O
original	O
and	O
behaves	O
as	O
follows	O
.	O
if	O
in	O
state	O
s	O
and	O
taking	O
action	B
a	O
,	O
then	O
with	O
probability	O
1	O
−	O
ε	O
the	O
new	O
environment	B
behaves	O
exactly	O
like	O
the	O
old	O
environment	B
.	O
with	O
probability	O
ε	O
it	O
repicks	O
the	O
action	B
at	O
random	O
,	O
with	O
equal	O
probabilities	O
,	O
and	O
then	O
behaves	O
like	O
the	O
old	O
environment	B
with	O
the	O
new	O
,	O
random	O
action	O
.	O
the	O
best	O
one	O
can	O
do	O
in	O
this	O
new	O
environment	B
with	O
general	O
policies	O
is	O
the	O
same	O
as	O
the	O
the	O
optimal	O
value	O
functions	O
for	O
the	O
new	O
environment	B
.	O
then	O
a	O
policy	B
π	O
is	O
optimal	O
among	O
best	O
one	O
could	O
do	O
in	O
the	O
original	O
environment	B
with	O
ε-soft	O
policies	O
.	O
let	O
(	O
cid:101	O
)	O
v∗	O
and	O
(	O
cid:101	O
)	O
q∗	O
denote	O
ε-soft	O
policies	O
if	O
and	O
only	O
if	O
vπ	O
=	O
(	O
cid:101	O
)	O
v∗	O
.	O
from	O
the	O
deﬁnition	O
of	O
(	O
cid:101	O
)	O
v∗	O
we	O
know	O
that	O
it	O
is	O
the	O
unique	O
solution	O
to	O
ε	O
vπ	O
(	O
s	O
)	O
=	O
(	O
1	O
−	O
ε	O
)	O
max	O
a	O
qπ	O
(	O
s	O
,	O
a	O
)	O
+	O
=	O
(	O
1	O
−	O
ε	O
)	O
max	O
ε	O
a	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
|a	O
(	O
s	O
)	O
|	O
(	O
cid:88	O
)	O
a	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
+	O
however	O
,	O
this	O
equation	O
is	O
the	O
same	O
as	O
the	O
previous	O
one	O
,	O
except	O
for	O
the	O
substitution	O
of	O
vπ	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:104	O
)	O
r	O
+	O
γvπ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
.	O
for	O
(	O
cid:101	O
)	O
v∗	O
.	O
because	O
(	O
cid:101	O
)	O
v∗	O
is	O
the	O
unique	O
solution	O
,	O
it	O
must	O
be	O
that	O
vπ	O
=	O
(	O
cid:101	O
)	O
v∗	O
.	O
in	O
essence	O
,	O
we	O
have	O
shown	O
in	O
the	O
last	O
few	O
pages	O
that	O
policy	B
iteration	I
works	O
for	O
ε-soft	O
policies	O
.	O
using	O
the	O
natural	O
notion	O
of	O
greedy	O
policy	B
for	O
ε-soft	O
policies	O
,	O
one	O
is	O
assured	O
of	O
improvement	O
on	O
every	O
step	O
,	O
except	O
when	O
the	O
best	O
policy	B
has	O
been	O
found	O
among	O
the	O
ε-soft	O
policies	O
.	O
this	O
analysis	O
is	O
independent	O
of	O
how	O
the	O
action-value	O
functions	O
are	O
determined	O
at	O
each	O
stage	O
,	O
but	O
it	O
does	O
assume	O
that	O
they	O
are	O
computed	O
exactly	O
.	O
this	O
brings	O
us	O
to	O
(	O
cid:101	O
)	O
v∗	O
(	O
s	O
)	O
=	O
(	O
1	O
−	O
ε	O
)	O
max	O
a	O
(	O
cid:101	O
)	O
q∗	O
(	O
s	O
,	O
a	O
)	O
+	O
a	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
|a	O
(	O
s	O
)	O
|	O
(	O
cid:88	O
)	O
a	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
=	O
(	O
1	O
−	O
ε	O
)	O
max	O
ε	O
+	O
|a	O
(	O
s	O
)	O
|	O
(	O
cid:88	O
)	O
a	O
(	O
cid:101	O
)	O
q∗	O
(	O
s	O
,	O
a	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:104	O
)	O
r	O
+	O
γ	O
(	O
cid:101	O
)	O
v∗	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:104	O
)	O
r	O
+	O
γ	O
(	O
cid:101	O
)	O
v∗	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
.	O
|a	O
(	O
s	O
)	O
|	O
(	O
cid:88	O
)	O
a	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:104	O
)	O
r	O
+	O
γvπ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
ε	O
when	O
equality	O
holds	O
and	O
the	O
ε-soft	O
policy	B
π	O
is	O
no	O
longer	O
improved	O
,	O
then	O
we	O
also	O
know	O
,	O
from	O
(	O
5.2	O
)	O
,	O
that	O
5.5.	O
oﬀ-policy	B
prediction	I
via	O
importance	B
sampling	I
103	O
roughly	O
the	O
same	O
point	O
as	O
in	O
the	O
previous	O
section	O
.	O
now	O
we	O
only	O
achieve	O
the	O
best	O
policy	B
among	O
the	O
ε-soft	O
policies	O
,	O
but	O
on	O
the	O
other	O
hand	O
,	O
we	O
have	O
eliminated	O
the	O
assumption	O
of	O
exploring	O
starts	O
.	O
5.5	O
oﬀ-policy	B
prediction	I
via	O
importance	B
sampling	I
all	O
learning	O
control	O
methods	O
face	O
a	O
dilemma	O
:	O
they	O
seek	O
to	O
learn	O
action	B
values	O
conditional	O
on	O
subsequent	O
optimal	O
behavior	O
,	O
but	O
they	O
need	O
to	O
behave	O
non-optimally	O
in	O
order	O
to	O
explore	O
all	O
actions	O
(	O
to	O
ﬁnd	O
the	O
optimal	O
actions	O
)	O
.	O
how	O
can	O
they	O
learn	O
about	O
the	O
optimal	O
policy	O
while	O
behaving	O
according	O
to	O
an	O
exploratory	O
policy	B
?	O
the	O
on-policy	O
approach	O
in	O
the	O
preceding	O
section	O
is	O
actually	O
a	O
compromise—it	O
learns	O
action	B
values	O
not	O
for	O
the	O
optimal	O
policy	O
,	O
but	O
for	O
a	O
near-optimal	O
policy	B
that	O
still	O
explores	O
.	O
a	O
more	O
straightforward	O
approach	O
is	O
to	O
use	O
two	O
policies	O
,	O
one	O
that	O
is	O
learned	O
about	O
and	O
that	O
becomes	O
the	O
optimal	O
policy	O
,	O
and	O
one	O
that	O
is	O
more	O
exploratory	O
and	O
is	O
used	O
to	O
generate	O
behavior	O
.	O
the	O
policy	B
being	O
learned	O
about	O
is	O
called	O
the	O
target	B
policy	O
,	O
and	O
the	O
policy	O
used	O
to	O
generate	O
behavior	O
is	O
called	O
the	O
behavior	B
policy	I
.	O
in	O
this	O
case	O
we	O
say	O
that	O
learning	O
is	O
from	O
data	O
“	O
oﬀ	O
”	O
the	O
target	B
policy	O
,	O
and	O
the	O
overall	O
process	O
is	O
termed	O
oﬀ-policy	B
learning	O
.	O
throughout	O
the	O
rest	O
of	O
this	O
book	O
we	O
consider	O
both	O
on-policy	O
and	O
oﬀ-policy	B
methods	I
.	O
on-policy	B
methods	I
are	O
generally	O
simpler	O
and	O
are	O
considered	O
ﬁrst	O
.	O
oﬀ-policy	B
methods	I
require	O
additional	O
concepts	O
and	O
notation	O
,	O
and	O
because	O
the	O
data	O
is	O
due	O
to	O
a	O
diﬀerent	O
policy	B
,	O
oﬀ-policy	B
methods	I
are	O
often	O
of	O
greater	O
variance	O
and	O
are	O
slower	O
to	O
converge	O
.	O
on	O
the	O
other	O
hand	O
,	O
oﬀ-policy	B
methods	I
are	O
more	O
powerful	O
and	O
general	O
.	O
they	O
include	O
on-	O
policy	B
methods	O
as	O
the	O
special	O
case	O
in	O
which	O
the	O
target	B
and	O
behavior	O
policies	O
are	O
the	O
same	O
.	O
oﬀ-policy	B
methods	I
also	O
have	O
a	O
variety	O
of	O
additional	O
uses	O
in	O
applications	O
.	O
for	O
example	O
,	O
they	O
can	O
often	O
be	O
applied	O
to	O
learn	O
from	O
data	O
generated	O
by	O
a	O
conventional	O
non-	O
learning	O
controller	O
,	O
or	O
from	O
a	O
human	O
expert	O
.	O
oﬀ-policy	B
learning	O
is	O
also	O
seen	O
by	O
some	O
as	O
key	O
to	O
learning	O
multi-step	O
predictive	O
models	O
of	O
the	O
world	O
’	O
s	O
dynamics	O
(	O
see	O
section	O
17.2	O
;	O
sutton	O
,	O
2009	O
;	O
sutton	O
et	O
al.	O
,	O
2011	O
)	O
.	O
in	O
this	O
section	O
we	O
begin	O
the	O
study	O
of	O
oﬀ-policy	O
methods	O
by	O
considering	O
the	O
prediction	B
problem	O
,	O
in	O
which	O
both	O
target	B
and	O
behavior	O
policies	O
are	O
ﬁxed	O
.	O
that	O
is	O
,	O
suppose	O
we	O
wish	O
to	O
estimate	O
vπ	O
or	O
qπ	O
,	O
but	O
all	O
we	O
have	O
are	O
episodes	B
following	O
another	O
policy	B
b	O
,	O
where	O
b	O
(	O
cid:54	O
)	O
=	O
π.	O
in	O
this	O
case	O
,	O
π	O
is	O
the	O
target	B
policy	O
,	O
b	O
is	O
the	O
behavior	B
policy	I
,	O
and	O
both	O
policies	O
are	O
considered	O
ﬁxed	O
and	O
given	O
.	O
in	O
order	O
to	O
use	O
episodes	B
from	O
b	O
to	O
estimate	O
values	O
for	O
π	O
,	O
we	O
require	O
that	O
every	O
action	B
taken	O
under	O
π	O
is	O
also	O
taken	O
,	O
at	O
least	O
occasionally	O
,	O
under	O
b.	O
that	O
is	O
,	O
we	O
require	O
that	O
π	O
(	O
a|s	O
)	O
>	O
0	O
implies	O
b	O
(	O
a|s	O
)	O
>	O
0.	O
this	O
is	O
called	O
the	O
assumption	O
of	O
coverage	O
.	O
it	O
follows	O
from	O
coverage	O
that	O
b	O
must	O
be	O
stochastic	O
in	O
states	O
where	O
it	O
is	O
not	O
identical	O
to	O
π.	O
the	O
target	B
policy	O
π	O
,	O
on	O
the	O
other	O
hand	O
,	O
may	O
be	O
deterministic	O
,	O
and	O
,	O
in	O
fact	O
,	O
this	O
is	O
a	O
case	O
of	O
particular	O
interest	O
in	O
control	B
applications	O
.	O
in	O
control	O
,	O
the	O
target	B
policy	O
is	O
typically	O
the	O
deterministic	O
greedy	O
policy	O
with	O
respect	O
to	O
the	O
current	O
estimate	O
of	O
the	O
action-value	B
function	I
.	O
this	O
policy	B
becomes	O
a	O
deterministic	O
optimal	O
policy	O
while	O
the	O
behavior	B
policy	I
remains	O
stochastic	O
and	O
more	O
exploratory	O
,	O
for	O
example	O
,	O
an	O
ε-greedy	O
policy	O
.	O
in	O
this	O
section	O
,	O
however	O
,	O
we	O
consider	O
the	O
prediction	B
problem	O
,	O
in	O
which	O
π	O
is	O
unchanging	O
and	O
given	O
.	O
almost	O
all	O
oﬀ-policy	B
methods	I
utilize	O
importance	B
sampling	I
,	O
a	O
general	O
technique	O
for	O
104	O
chapter	O
5	O
:	O
monte	O
carlo	O
methods	O
estimating	O
expected	O
values	O
under	O
one	O
distribution	O
given	O
samples	O
from	O
another	O
.	O
we	O
apply	O
importance	B
sampling	I
to	O
oﬀ-policy	B
learning	O
by	O
weighting	O
returns	O
according	O
to	O
the	O
relative	O
probability	O
of	O
their	O
trajectories	O
occurring	O
under	O
the	O
target	B
and	O
behavior	O
policies	O
,	O
called	O
the	O
importance-sampling	O
ratio	B
.	O
given	O
a	O
starting	O
state	B
st	O
,	O
the	O
probability	O
of	O
the	O
subsequent	O
state–action	O
trajectory	O
,	O
at	O
,	O
st+1	O
,	O
at+1	O
,	O
.	O
.	O
.	O
,	O
st	O
,	O
occurring	O
under	O
any	O
policy	B
π	O
is	O
pr	O
{	O
at	O
,	O
st+1	O
,	O
at+1	O
,	O
.	O
.	O
.	O
,	O
st	O
|	O
st	O
,	O
at	O
:	O
t−1	O
∼	O
π	O
}	O
=	O
π	O
(	O
at|st	O
)	O
p	O
(	O
st+1|st	O
,	O
at	O
)	O
π	O
(	O
at+1|st+1	O
)	O
···	O
p	O
(	O
st	O
|st−1	O
,	O
at−1	O
)	O
=	O
π	O
(	O
ak|sk	O
)	O
p	O
(	O
sk+1|sk	O
,	O
ak	O
)	O
,	O
t−1	O
(	O
cid:89	O
)	O
k=t	O
where	O
p	O
here	O
is	O
the	O
state-transition	O
probability	O
function	O
deﬁned	O
by	O
(	O
3.4	O
)	O
.	O
thus	O
,	O
the	O
rela-	O
tive	O
probability	O
of	O
the	O
trajectory	O
under	O
the	O
target	B
and	O
behavior	O
policies	O
(	O
the	O
importance-	O
sampling	O
ratio	B
)	O
is	O
ρt	O
:	O
t−1	O
.	O
=	O
(	O
cid:81	O
)	O
t−1	O
(	O
cid:81	O
)	O
t−1	O
k=t	O
π	O
(	O
ak|sk	O
)	O
p	O
(	O
sk+1|sk	O
,	O
ak	O
)	O
k=t	O
b	O
(	O
ak|sk	O
)	O
p	O
(	O
sk+1|sk	O
,	O
ak	O
)	O
=	O
t−1	O
(	O
cid:89	O
)	O
k=t	O
π	O
(	O
ak|sk	O
)	O
b	O
(	O
ak|sk	O
)	O
.	O
(	O
5.3	O
)	O
although	O
the	O
trajectory	O
probabilities	O
depend	O
on	O
the	O
mdp	O
’	O
s	O
transition	B
probabilities	I
,	O
which	O
are	O
generally	O
unknown	O
,	O
they	O
appear	O
identically	O
in	O
both	O
the	O
numerator	O
and	O
de-	O
nominator	O
,	O
and	O
thus	O
cancel	O
.	O
the	O
importance	B
sampling	I
ratio	O
ends	O
up	O
depending	O
only	O
on	O
the	O
two	O
policies	O
and	O
the	O
sequence	O
,	O
not	O
on	O
the	O
mdp	O
.	O
recall	O
that	O
we	O
wish	O
to	O
estimate	O
the	O
expected	O
returns	O
(	O
values	O
)	O
under	O
the	O
target	B
policy	O
,	O
but	O
all	O
we	O
have	O
are	O
returns	O
gt	O
due	O
to	O
the	O
behavior	B
policy	I
.	O
these	O
returns	O
have	O
the	O
wrong	O
expectation	O
e	O
[	O
gt|st	O
=	O
s	O
]	O
=	O
vb	O
(	O
s	O
)	O
and	O
so	O
can	O
not	O
be	O
averaged	O
to	O
obtain	O
vπ	O
.	O
this	O
is	O
where	O
importance	B
sampling	I
comes	O
in	O
.	O
the	O
ratio	B
ρt	O
:	O
t−1	O
transforms	O
the	O
returns	O
to	O
have	O
the	O
right	O
expected	O
value	O
:	O
e	O
[	O
ρt	O
:	O
t−1gt	O
|	O
st	O
=	O
s	O
]	O
=	O
vπ	O
(	O
s	O
)	O
.	O
(	O
5.4	O
)	O
now	O
we	O
are	O
ready	O
to	O
give	O
a	O
monte	O
carlo	O
algorithm	O
that	O
averages	O
returns	O
from	O
a	O
batch	O
of	O
observed	O
episodes	B
following	O
policy	B
b	O
to	O
estimate	O
vπ	O
(	O
s	O
)	O
.	O
it	O
is	O
convenient	O
here	O
to	O
number	O
time	O
steps	O
in	O
a	O
way	O
that	O
increases	O
across	O
episode	O
boundaries	O
.	O
that	O
is	O
,	O
if	O
the	O
ﬁrst	O
episode	O
of	O
the	O
batch	O
ends	O
in	O
a	O
terminal	O
state	B
at	O
time	O
100	O
,	O
then	O
the	O
next	O
episode	O
begins	O
at	O
time	O
t	O
=	O
101.	O
this	O
enables	O
us	O
to	O
use	O
time-step	O
numbers	O
to	O
refer	O
to	O
particular	O
steps	O
in	O
particular	O
episodes	B
.	O
in	O
particular	O
,	O
we	O
can	O
deﬁne	O
the	O
set	O
of	O
all	O
time	O
steps	O
in	O
which	O
state	B
s	O
is	O
visited	O
,	O
denoted	O
t	O
(	O
s	O
)	O
.	O
this	O
is	O
for	O
an	O
every-visit	O
method	O
;	O
for	O
a	O
ﬁrst-visit	O
method	O
,	O
t	O
(	O
s	O
)	O
would	O
only	O
include	O
time	O
steps	O
that	O
were	O
ﬁrst	O
visits	O
to	O
s	O
within	O
their	O
episodes	B
.	O
also	O
,	O
let	O
t	O
(	O
t	O
)	O
denote	O
the	O
ﬁrst	O
time	O
of	O
termination	O
following	O
time	O
t	O
,	O
and	O
gt	O
denote	O
the	O
return	B
after	O
t	O
up	O
through	O
t	O
(	O
t	O
)	O
.	O
then	O
{	O
gt	O
}	O
t∈t	O
(	O
s	O
)	O
are	O
the	O
returns	O
that	O
pertain	O
to	O
state	B
s	O
,	O
and	O
(	O
cid:8	O
)	O
ρt	O
:	O
t	O
(	O
t	O
)	O
−1	O
(	O
cid:9	O
)	O
t∈t	O
(	O
s	O
)	O
are	O
the	O
corresponding	O
importance-sampling	O
ratios	O
.	O
to	O
estimate	O
vπ	O
(	O
s	O
)	O
,	O
we	O
simply	O
scale	O
the	O
returns	O
by	O
the	O
ratios	O
and	O
average	O
the	O
results	O
:	O
v	O
(	O
s	O
)	O
.	O
=	O
(	O
cid:80	O
)	O
t∈t	O
(	O
s	O
)	O
ρt	O
:	O
t	O
(	O
t	O
)	O
−1gt	O
|t	O
(	O
s	O
)	O
|	O
.	O
(	O
5.5	O
)	O
5.5.	O
oﬀ-policy	B
prediction	I
via	O
importance	B
sampling	I
105	O
when	O
importance	B
sampling	I
is	O
done	O
as	O
a	O
simple	O
average	O
in	O
this	O
way	O
it	O
is	O
called	O
ordinary	O
importance	B
sampling	I
.	O
an	O
important	O
alternative	O
is	O
weighted	O
importance	O
sampling	O
,	O
which	O
uses	O
a	O
weighted	O
av-	O
erage	O
,	O
deﬁned	O
as	O
v	O
(	O
s	O
)	O
,	O
(	O
5.6	O
)	O
.	O
=	O
(	O
cid:80	O
)	O
t∈t	O
(	O
s	O
)	O
ρt	O
:	O
t	O
(	O
t	O
)	O
−1gt	O
(	O
cid:80	O
)	O
t∈t	O
(	O
s	O
)	O
ρt	O
:	O
t	O
(	O
t	O
)	O
−1	O
or	O
zero	O
if	O
the	O
denominator	O
is	O
zero	O
.	O
to	O
understand	O
these	O
two	O
varieties	O
of	O
importance	O
sampling	O
,	O
consider	O
their	O
estimates	O
after	O
observing	O
a	O
single	O
return	B
.	O
in	O
the	O
weighted-	O
average	O
estimate	O
,	O
the	O
ratio	B
ρt	O
:	O
t	O
(	O
t	O
)	O
−1	O
for	O
the	O
single	O
return	B
cancels	O
in	O
the	O
numerator	O
and	O
denominator	O
,	O
so	O
that	O
the	O
estimate	O
is	O
equal	O
to	O
the	O
observed	O
return	B
independent	O
of	O
the	O
ratio	B
(	O
assuming	O
the	O
ratio	B
is	O
nonzero	O
)	O
.	O
given	O
that	O
this	O
return	B
was	O
the	O
only	O
one	O
observed	O
,	O
this	O
is	O
a	O
reasonable	O
estimate	O
,	O
but	O
its	O
expectation	O
is	O
vb	O
(	O
s	O
)	O
rather	O
than	O
vπ	O
(	O
s	O
)	O
,	O
and	O
in	O
this	O
statistical	O
sense	O
it	O
is	O
biased	O
.	O
in	O
contrast	O
,	O
the	O
simple	O
average	O
(	O
5.5	O
)	O
is	O
always	O
vπ	O
(	O
s	O
)	O
in	O
expectation	O
(	O
it	O
is	O
unbiased	O
)	O
,	O
but	O
it	O
can	O
be	O
extreme	O
.	O
suppose	O
the	O
ratio	B
were	O
ten	O
,	O
indicating	O
that	O
the	O
trajectory	O
observed	O
is	O
ten	O
times	O
as	O
likely	O
under	O
the	O
target	B
policy	O
as	O
under	O
the	O
behavior	B
policy	I
.	O
in	O
this	O
case	O
the	O
ordinary	O
importance-sampling	O
estimate	O
would	O
be	O
ten	O
times	O
the	O
observed	O
return	B
.	O
that	O
is	O
,	O
it	O
would	O
be	O
quite	O
far	O
from	O
the	O
observed	O
return	B
even	O
though	O
the	O
episode	O
’	O
s	O
trajectory	O
is	O
considered	O
very	O
representative	O
of	O
the	O
target	B
policy	O
.	O
formally	O
,	O
the	O
diﬀerence	O
between	O
the	O
two	O
kinds	O
of	O
importance	O
sampling	O
is	O
expressed	O
in	O
their	O
biases	O
and	O
variances	O
.	O
the	O
ordinary	O
importance-sampling	O
estimator	O
is	O
unbi-	O
ased	O
whereas	O
the	O
weighted	O
importance-sampling	O
estimator	O
is	O
biased	O
(	O
the	O
bias	O
converges	O
asymptotically	O
to	O
zero	O
)	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
variance	O
of	O
the	O
ordinary	O
importance-	O
sampling	O
estimator	O
is	O
in	O
general	O
unbounded	O
because	O
the	O
variance	O
of	O
the	O
ratios	O
can	O
be	O
unbounded	O
,	O
whereas	O
in	O
the	O
weighted	O
estimator	O
the	O
largest	O
weight	O
on	O
any	O
single	O
return	B
is	O
one	O
.	O
in	O
fact	O
,	O
assuming	O
bounded	O
returns	O
,	O
the	O
variance	O
of	O
the	O
weighted	O
importance-	O
sampling	O
estimator	O
converges	O
to	O
zero	O
even	O
if	O
the	O
variance	O
of	O
the	O
ratios	O
themselves	O
is	O
inﬁnite	O
(	O
precup	O
,	O
sutton	O
,	O
and	O
dasgupta	O
2001	O
)	O
.	O
in	O
practice	O
,	O
the	O
weighted	O
estimator	O
usu-	O
ally	O
has	O
dramatically	O
lower	O
variance	O
and	O
is	O
strongly	O
preferred	O
.	O
nevertheless	O
,	O
we	O
will	O
not	O
totally	O
abandon	O
ordinary	O
importance	B
sampling	I
as	O
it	O
is	O
easier	O
to	O
extend	O
to	O
the	O
approxi-	O
mate	O
methods	O
using	O
function	B
approximation	I
that	O
we	O
explore	O
in	O
the	O
second	O
part	O
of	O
this	O
book	O
.	O
a	O
complete	O
every-visit	O
mc	O
algorithm	O
for	O
oﬀ-policy	O
policy	B
evaluation	I
using	O
weighted	O
importance	O
sampling	O
is	O
given	O
in	O
the	O
next	O
section	O
on	O
page	O
109.	O
example	O
5.4	O
:	O
oﬀ-policy	B
estimation	O
of	O
a	O
blackjack	O
state	O
value	B
we	O
applied	O
both	O
ordinary	O
and	O
weighted	O
importance-sampling	O
methods	O
to	O
estimate	O
the	O
value	B
of	O
a	O
single	O
blackjack	O
state	O
from	O
oﬀ-policy	B
data	O
.	O
recall	O
that	O
one	O
of	O
the	O
advantages	B
of	I
monte	O
carlo	O
methods	O
is	O
that	O
they	O
can	O
be	O
used	O
to	O
evaluate	O
a	O
single	O
state	B
without	O
forming	O
estimates	O
for	O
any	O
other	O
states	O
.	O
in	O
this	O
example	O
,	O
we	O
evaluated	O
the	O
state	B
in	O
which	O
the	O
dealer	O
is	O
showing	O
a	O
deuce	O
,	O
the	O
sum	O
of	O
the	O
player	O
’	O
s	O
cards	O
is	O
13	O
,	O
and	O
the	O
player	O
has	O
a	O
usable	O
ace	O
(	O
that	O
is	O
,	O
the	O
player	O
holds	O
an	O
ace	O
and	O
a	O
deuce	O
,	O
or	O
equivalently	O
three	O
aces	O
)	O
.	O
the	O
data	O
was	O
generated	O
by	O
starting	O
in	O
this	O
state	B
then	O
choosing	O
to	O
hit	O
or	O
stick	O
at	O
random	O
with	O
equal	O
probability	O
(	O
the	O
behavior	B
policy	I
)	O
.	O
the	O
target	B
policy	O
was	O
to	O
stick	O
only	O
on	O
a	O
sum	O
of	O
20	O
or	O
21	O
,	O
as	O
in	O
example	O
5.1.	O
the	O
value	B
of	O
this	O
state	B
under	O
the	O
target	B
policy	O
106	O
chapter	O
5	O
:	O
monte	O
carlo	O
methods	O
is	O
approximately	O
−0.27726	O
(	O
this	O
was	O
determined	O
by	O
separately	O
generating	O
one-hundred	O
million	O
episodes	B
using	O
the	O
target	B
policy	O
and	O
averaging	O
their	O
returns	O
)	O
.	O
both	O
oﬀ-policy	B
methods	I
closely	O
approximated	O
this	O
value	B
after	O
1000	O
oﬀ-policy	B
episodes	O
using	O
the	O
random	O
policy	O
.	O
to	O
make	O
sure	O
they	O
did	O
this	O
reliably	O
,	O
we	O
performed	O
100	O
independent	O
runs	O
,	O
each	O
starting	O
from	O
estimates	O
of	O
zero	O
and	O
learning	O
for	O
10,000	O
episodes	B
.	O
figure	O
5.3	O
shows	O
the	O
resultant	O
learning	O
curves—the	O
squared	O
error	O
of	O
the	O
estimates	O
of	O
each	O
method	O
as	O
a	O
function	O
of	O
number	O
of	O
episodes	O
,	O
averaged	O
over	O
the	O
100	O
runs	O
.	O
the	O
error	O
approaches	O
zero	O
for	O
both	O
algorithms	O
,	O
but	O
the	O
weighted	O
importance-sampling	O
method	O
has	O
much	O
lower	O
error	O
at	O
the	O
beginning	O
,	O
as	O
is	O
typical	O
in	O
practice	O
.	O
figure	O
5.3	O
:	O
weighted	O
importance	O
sampling	O
produces	O
lower	O
error	O
estimates	O
of	O
the	O
value	B
of	O
a	O
single	O
blackjack	O
state	O
from	O
oﬀ-policy	B
episodes	O
(	O
see	O
example	O
5.3	O
)	O
.	O
example	O
5.5	O
:	O
inﬁnite	O
variance	O
the	O
estimates	O
of	O
ordinary	O
importance	B
sampling	I
will	O
typically	O
have	O
inﬁnite	O
variance	O
,	O
and	O
thus	O
unsatisfactory	O
convergence	O
properties	O
,	O
when-	O
ever	O
the	O
scaled	O
returns	O
have	O
inﬁnite	O
variance—and	O
this	O
can	O
easily	O
happen	O
in	O
oﬀ-policy	O
learning	O
when	O
trajectories	O
contain	O
loops	O
.	O
a	O
simple	O
example	O
is	O
shown	O
inset	O
in	O
figure	O
5.4.	O
there	O
is	O
only	O
one	O
nonterminal	O
state	B
s	O
and	O
two	O
actions	O
,	O
right	O
and	O
left	O
.	O
the	O
right	O
action	B
causes	O
a	O
deterministic	O
transition	O
to	O
termination	O
,	O
whereas	O
the	O
left	O
action	B
transitions	O
,	O
with	O
probability	O
0.9	O
,	O
back	O
to	O
s	O
or	O
,	O
with	O
probability	O
0.1	O
,	O
on	O
to	O
termination	O
.	O
the	O
rewards	O
are	O
+1	O
on	O
the	O
latter	O
transition	O
and	O
otherwise	O
zero	O
.	O
consider	O
the	O
target	B
policy	O
that	O
always	O
selects	O
left	O
.	O
all	O
episodes	B
under	O
this	O
policy	B
consist	O
of	O
some	O
number	O
(	O
possibly	O
zero	O
)	O
of	O
transitions	O
back	O
to	O
s	O
followed	O
by	O
termination	O
with	O
a	O
reward	O
and	O
return	B
of	O
+1	O
.	O
thus	O
the	O
value	B
of	O
s	O
under	O
the	O
target	B
policy	O
is	O
1	O
(	O
γ	O
=	O
1	O
)	O
.	O
suppose	O
we	O
are	O
estimating	O
this	O
value	B
from	O
oﬀ-policy	B
data	O
using	O
the	O
behavior	B
policy	I
that	O
selects	O
right	O
and	O
left	O
with	O
equal	O
probability	O
.	O
the	O
lower	O
part	O
of	O
figure	O
5.4	O
shows	O
ten	O
independent	O
runs	O
of	O
the	O
ﬁrst-visit	O
mc	O
algorithm	O
using	O
ordinary	O
importance	B
sampling	I
.	O
even	O
after	O
millions	O
of	O
episodes	O
,	O
the	O
estimates	O
fail	O
to	O
converge	O
to	O
the	O
correct	O
value	B
of	O
1.	O
in	O
contrast	O
,	O
the	O
weighted	O
importance-sampling	O
algorithm	O
would	O
give	O
an	O
estimate	O
of	O
exactly	O
1	O
forever	O
after	O
the	O
ﬁrst	O
episode	O
that	O
ended	O
with	O
the	O
left	O
action	B
.	O
all	O
returns	O
not	O
equal	O
to	O
1	O
(	O
that	O
is	O
,	O
ending	O
with	O
the	O
right	O
action	B
)	O
would	O
be	O
inconsistent	O
with	O
the	O
target	B
policy	O
and	O
thus	O
would	O
have	O
a	O
ρt	O
:	O
t	O
(	O
t	O
)	O
−1	O
of	O
zero	O
and	O
contribute	O
neither	O
to	O
the	O
numerator	O
nor	O
denominator	O
of	O
(	O
5.6	O
)	O
.	O
the	O
weighted	O
importance-	O
ordinary	O
importance	O
samplingweighted	O
importance	O
samplingepisodes	O
(	O
log	O
scale	O
)	O
010100100010,000meansquareerror	O
(	O
average	O
over100	O
runs	O
)	O
024	O
5.5.	O
oﬀ-policy	B
prediction	I
via	O
importance	B
sampling	I
107	O
figure	O
5.4	O
:	O
ordinary	O
importance	B
sampling	I
produces	O
surprisingly	O
unstable	O
estimates	O
on	O
the	O
one-state	O
mdp	O
shown	O
inset	O
(	O
example	O
5.5	O
)	O
.	O
the	O
correct	O
estimate	O
here	O
is	O
1	O
(	O
γ	O
=	O
1	O
)	O
,	O
and	O
,	O
even	O
though	O
this	O
is	O
the	O
expected	O
value	O
of	O
a	O
sample	O
return	O
(	O
after	O
importance	B
sampling	I
)	O
,	O
the	O
variance	O
of	O
the	O
samples	O
is	O
inﬁnite	O
,	O
and	O
the	O
estimates	O
do	O
not	O
converge	O
to	O
this	O
value	B
.	O
these	O
results	O
are	O
for	O
oﬀ-policy	O
ﬁrst-visit	O
mc	O
.	O
sampling	O
algorithm	O
produces	O
a	O
weighted	O
average	O
of	O
only	O
the	O
returns	O
consistent	O
with	O
the	O
target	B
policy	O
,	O
and	O
all	O
of	O
these	O
would	O
be	O
exactly	O
1.	O
we	O
can	O
verify	O
that	O
the	O
variance	O
of	O
the	O
importance-sampling-scaled	O
returns	O
is	O
inﬁnite	O
in	O
this	O
example	O
by	O
a	O
simple	O
calculation	O
.	O
the	O
variance	O
of	O
any	O
random	O
variable	O
x	O
is	O
the	O
expected	O
value	O
of	O
the	O
deviation	O
from	O
its	O
mean	O
¯x	O
,	O
which	O
can	O
be	O
written	O
var	O
[	O
x	O
]	O
.	O
=	O
e	O
(	O
cid:104	O
)	O
(	O
cid:0	O
)	O
x	O
−	O
¯x	O
(	O
cid:1	O
)	O
2	O
(	O
cid:105	O
)	O
=	O
e	O
(	O
cid:2	O
)	O
x	O
2	O
−	O
2x	O
¯x	O
+	O
¯x	O
2	O
(	O
cid:3	O
)	O
=	O
e	O
(	O
cid:2	O
)	O
x	O
2	O
(	O
cid:3	O
)	O
−	O
¯x	O
2.	O
thus	O
,	O
if	O
the	O
mean	O
is	O
ﬁnite	O
,	O
as	O
it	O
is	O
in	O
our	O
case	O
,	O
the	O
variance	O
is	O
inﬁnite	O
if	O
and	O
only	O
if	O
the	O
expectation	O
of	O
the	O
square	O
of	O
the	O
random	O
variable	O
is	O
inﬁnite	O
.	O
thus	O
,	O
we	O
need	O
only	O
show	O
that	O
the	O
expected	O
square	O
of	O
the	O
importance-sampling-scaled	O
return	B
is	O
inﬁnite	O
:	O
eb	O
(	O
cid:32	O
)	O
t−1	O
(	O
cid:89	O
)	O
t=0	O
π	O
(	O
at|st	O
)	O
b	O
(	O
at|st	O
)	O
g0	O
(	O
cid:33	O
)	O
2	O
.	O
to	O
compute	O
this	O
expectation	O
,	O
we	O
break	O
it	O
down	O
into	O
cases	O
based	O
on	O
episode	O
length	O
and	O
termination	O
.	O
first	O
note	O
that	O
,	O
for	O
any	O
episode	O
ending	O
with	O
the	O
right	O
action	B
,	O
the	O
importance	B
sampling	I
ratio	O
is	O
zero	O
,	O
because	O
the	O
target	B
policy	O
would	O
never	O
take	O
this	O
action	B
;	O
these	O
episodes	B
thus	O
contribute	O
nothing	O
to	O
the	O
expectation	O
(	O
the	O
quantity	O
in	O
parenthesis	O
will	O
be	O
zero	O
)	O
and	O
can	O
be	O
ignored	O
.	O
we	O
need	O
only	O
consider	O
episodes	B
that	O
involve	O
some	O
number	O
(	O
possibly	O
zero	O
)	O
of	O
left	O
actions	O
that	O
transition	O
back	O
to	O
the	O
nonterminal	O
state	B
,	O
followed	O
by	O
1100,0001,000,00010,000,000100,000,00020.10.9r=+1s⇡	O
(	O
left|s	O
)	O
=1leftrightr=0r=0b	O
(	O
left|s	O
)	O
=12v⇡	O
(	O
s	O
)	O
monte-carlo	O
estimate	O
of	O
with	O
ordinaryimportance	O
sampling	O
(	O
ten	O
runs	O
)	O
episodes	B
(	O
log	O
scale	O
)	O
110100100010,0000	O
108	O
chapter	O
5	O
:	O
monte	O
carlo	O
methods	O
a	O
left	O
action	B
transitioning	O
to	O
termination	O
.	O
all	O
of	O
these	O
episodes	B
have	O
a	O
return	B
of	O
1	O
,	O
so	O
the	O
g0	O
factor	O
can	O
be	O
ignored	O
.	O
to	O
get	O
the	O
expected	O
square	O
we	O
need	O
only	O
consider	O
each	O
length	O
of	O
episode	O
,	O
multiplying	O
the	O
probability	O
of	O
the	O
episode	O
’	O
s	O
occurrence	O
by	O
the	O
square	O
of	O
its	O
importance-sampling	O
ratio	B
,	O
and	O
add	O
these	O
up	O
:	O
(	O
the	O
length	O
1	O
episode	O
)	O
(	O
the	O
length	O
2	O
episode	O
)	O
(	O
the	O
length	O
3	O
episode	O
)	O
1	O
0.5	O
1	O
0.5	O
(	O
cid:19	O
)	O
2	O
1	O
0.5	O
(	O
cid:19	O
)	O
2	O
2	O
·	O
0.1	O
(	O
cid:18	O
)	O
1	O
2	O
·	O
0.1	O
(	O
cid:18	O
)	O
1	O
1	O
+	O
1	O
2	O
·	O
0.9	O
·	O
1	O
2	O
·	O
0.9	O
·	O
1	O
2	O
·	O
0.9	O
·	O
1	O
1	O
0.5	O
0.5	O
(	O
cid:19	O
)	O
2	O
2	O
·	O
0.1	O
(	O
cid:18	O
)	O
1	O
∞	O
(	O
cid:88	O
)	O
k=0	O
0.5	O
=	O
+	O
+	O
···	O
=	O
0.1	O
∞	O
(	O
cid:88	O
)	O
k=0	O
0.9k	O
·	O
2k	O
·	O
2	O
=	O
0.2	O
1.8k	O
=	O
∞	O
.	O
exercise	O
5.4	O
what	O
is	O
the	O
equation	O
analogous	O
to	O
(	O
5.6	O
)	O
for	B
action	I
values	I
q	O
(	O
s	O
,	O
a	O
)	O
instead	O
(	O
cid:3	O
)	O
of	O
state	O
values	O
v	O
(	O
s	O
)	O
,	O
again	O
given	O
returns	O
generated	O
using	O
b	O
?	O
exercise	O
5.5	O
in	O
learning	O
curves	O
such	O
as	O
those	O
shown	O
in	O
figure	O
5.3	O
error	O
generally	O
decreases	O
with	O
training	O
,	O
as	O
indeed	O
happened	O
for	O
the	O
ordinary	O
importance-sampling	O
method	O
.	O
but	O
for	O
the	O
weighted	O
importance-sampling	O
method	O
error	O
ﬁrst	O
increased	O
and	O
then	O
decreased	O
.	O
(	O
cid:3	O
)	O
why	O
do	O
you	O
think	O
this	O
happened	O
?	O
exercise	O
5.6	O
the	O
results	O
with	O
example	O
5.5	O
and	O
shown	O
in	O
figure	O
5.4	O
used	O
a	O
ﬁrst-visit	O
mc	O
method	O
.	O
suppose	O
that	O
instead	O
an	O
every-visit	O
mc	O
method	O
was	O
used	O
on	O
the	O
same	O
(	O
cid:3	O
)	O
problem	O
.	O
would	O
the	O
variance	O
of	O
the	O
estimator	O
still	O
be	O
inﬁnite	O
?	O
why	O
or	O
why	O
not	O
?	O
5.6	O
incremental	B
implementation	I
monte	O
carlo	O
prediction	B
methods	O
can	O
be	O
implemented	O
incrementally	O
,	O
on	O
an	O
episode-by-	O
episode	O
basis	O
,	O
using	O
extensions	O
of	O
the	O
techniques	O
described	O
in	O
chapter	O
2	O
(	O
section	O
2.4	O
)	O
.	O
whereas	O
in	O
chapter	O
2	O
we	O
averaged	O
rewards	O
,	O
in	O
monte	O
carlo	O
methods	O
we	O
average	O
returns	O
.	O
in	O
all	O
other	O
respects	O
exactly	O
the	O
same	O
methods	O
as	O
used	O
in	O
chapter	O
2	O
can	O
be	O
used	O
for	O
on-policy	O
monte	O
carlo	O
methods	O
.	O
for	O
oﬀ-policy	O
monte	O
carlo	O
methods	O
,	O
we	O
need	O
to	O
sepa-	O
rately	O
consider	O
those	O
that	O
use	O
ordinary	O
importance	B
sampling	I
and	O
those	O
that	O
use	O
weighted	O
importance	O
sampling	O
.	O
in	O
ordinary	O
importance	B
sampling	I
,	O
the	O
returns	O
are	O
scaled	O
by	O
the	O
importance	B
sampling	I
ratio	O
ρt	O
:	O
t	O
(	O
t	O
)	O
−1	O
(	O
5.3	O
)	O
,	O
then	O
simply	O
averaged	O
.	O
for	O
these	O
methods	O
we	O
can	O
again	O
use	O
the	O
incremental	O
methods	O
of	O
chapter	O
2	O
,	O
but	O
using	O
the	O
scaled	O
returns	O
in	O
place	O
of	O
the	O
rewards	O
of	O
that	O
chapter	O
.	O
this	O
leaves	O
the	O
case	O
of	O
oﬀ-policy	O
methods	O
using	O
weighted	O
importance	O
sampling	O
.	O
here	O
we	O
have	O
to	O
form	O
a	O
weighted	O
average	O
of	O
the	O
returns	O
,	O
and	O
a	O
slightly	O
diﬀerent	O
incremental	O
algorithm	O
is	O
required	O
.	O
suppose	O
we	O
have	O
a	O
sequence	O
of	O
returns	O
g1	O
,	O
g2	O
,	O
.	O
.	O
.	O
,	O
gn−1	O
,	O
all	O
starting	O
in	O
the	O
same	O
state	B
and	O
each	O
with	O
a	O
corresponding	O
random	O
weight	O
wi	O
(	O
e.g.	O
,	O
wi	O
=	O
ρt	O
:	O
t	O
(	O
t	O
)	O
−1	O
)	O
.	O
we	O
wish	O
to	O
5.7.	O
oﬀ-policy	B
monte	O
carlo	O
control	B
form	O
the	O
estimate	O
k=1	O
wkgk	O
k=1	O
wk	O
.	O
=	O
(	O
cid:80	O
)	O
n−1	O
(	O
cid:80	O
)	O
n−1	O
vn	O
,	O
n	O
≥	O
2	O
,	O
109	O
(	O
5.7	O
)	O
and	O
keep	O
it	O
up-to-date	O
as	O
we	O
obtain	O
a	O
single	O
additional	O
return	B
gn	O
.	O
in	O
addition	O
to	O
keeping	O
track	O
of	O
vn	O
,	O
we	O
must	O
maintain	O
for	O
each	O
state	B
the	O
cumulative	O
sum	O
cn	O
of	O
the	O
weights	O
given	O
to	O
the	O
ﬁrst	O
n	O
returns	O
.	O
the	O
update	O
rule	O
for	O
vn	O
is	O
vn+1	O
.	O
=	O
vn	O
+	O
wn	O
cn	O
(	O
cid:104	O
)	O
gn	O
−	O
vn	O
(	O
cid:105	O
)	O
,	O
n	O
≥	O
1	O
,	O
and	O
(	O
5.8	O
)	O
cn+1	O
.	O
=	O
cn	O
+	O
wn+1	O
,	O
.	O
where	O
c0	O
=	O
0	O
(	O
and	O
v1	O
is	O
arbitrary	O
and	O
thus	O
need	O
not	O
be	O
speciﬁed	O
)	O
.	O
the	O
box	O
below	O
contains	O
a	O
complete	O
episode-by-episode	O
incremental	O
algorithm	O
for	O
monte	O
carlo	O
policy	B
evaluation	I
.	O
the	O
algorithm	O
is	O
nominally	O
for	O
the	O
oﬀ-policy	B
case	O
,	O
using	O
weighted	O
importance	O
sampling	O
,	O
but	O
applies	O
as	O
well	O
to	O
the	O
on-policy	O
case	O
just	O
by	O
choosing	O
the	O
target	B
and	O
behavior	O
policies	O
as	O
the	O
same	O
(	O
in	O
which	O
case	O
(	O
π	O
=	O
b	O
)	O
,	O
w	O
is	O
always	O
1	O
)	O
.	O
the	O
approximation	O
q	O
converges	O
to	O
qπ	O
(	O
for	O
all	O
encountered	O
state–action	O
pairs	O
)	O
while	O
actions	O
are	O
selected	O
according	O
to	O
a	O
potentially	O
diﬀerent	O
policy	B
,	O
b.	O
oﬀ-policy	B
mc	O
prediction	B
(	O
policy	B
evaluation	I
)	O
for	O
estimating	O
q	O
≈	O
qπ	O
input	O
:	O
an	O
arbitrary	O
target	B
policy	O
π	O
initialize	O
,	O
for	O
all	O
s	O
∈	O
s	O
,	O
a	O
∈	O
a	O
(	O
s	O
)	O
:	O
q	O
(	O
s	O
,	O
a	O
)	O
∈	O
r	O
(	O
arbitrarily	O
)	O
c	O
(	O
s	O
,	O
a	O
)	O
←	O
0	O
loop	O
forever	O
(	O
for	O
each	O
episode	O
)	O
:	O
b	O
←	O
any	O
policy	B
with	O
coverage	O
of	O
π	O
generate	O
an	O
episode	O
following	O
b	O
:	O
s0	O
,	O
a0	O
,	O
r1	O
,	O
.	O
.	O
.	O
,	O
st−1	O
,	O
at−1	O
,	O
rt	O
g	O
←	O
0	O
w	O
←	O
1	O
loop	O
for	O
each	O
step	O
of	O
episode	O
,	O
t	O
=	O
t	O
−1	O
,	O
t	O
−2	O
,	O
.	O
.	O
.	O
,	O
0	O
:	O
g	O
←	O
γg	O
+	O
rt+1	O
c	O
(	O
st	O
,	O
at	O
)	O
←	O
c	O
(	O
st	O
,	O
at	O
)	O
+	O
w	O
q	O
(	O
st	O
,	O
at	O
)	O
←	O
q	O
(	O
st	O
,	O
at	O
)	O
+	O
w	O
w	O
←	O
w	O
π	O
(	O
at|st	O
)	O
b	O
(	O
at|st	O
)	O
if	O
w	O
=	O
0	O
then	O
exit	O
for	O
loop	O
c	O
(	O
st	O
,	O
at	O
)	O
[	O
g	O
−	O
q	O
(	O
st	O
,	O
at	O
)	O
]	O
exercise	O
5.7	O
modify	O
the	O
algorithm	O
for	O
ﬁrst-visit	O
mc	O
policy	B
evaluation	I
(	O
section	O
5.1	O
)	O
to	O
(	O
cid:3	O
)	O
use	O
the	O
incremental	B
implementation	I
for	O
sample	O
averages	O
described	O
in	O
section	O
2.4.	O
exercise	O
5.8	O
derive	O
the	O
weighted-average	O
update	O
rule	O
(	O
5.8	O
)	O
from	O
(	O
5.7	O
)	O
.	O
follow	O
the	O
pattern	O
(	O
cid:3	O
)	O
of	O
the	O
derivation	O
of	O
the	O
unweighted	O
rule	O
(	O
2.3	O
)	O
.	O
110	O
chapter	O
5	O
:	O
monte	O
carlo	O
methods	O
5.7	O
oﬀ-policy	B
monte	O
carlo	O
control	B
we	O
are	O
now	O
ready	O
to	O
present	O
an	O
example	O
of	O
the	O
second	O
class	O
of	O
learning	O
control	B
methods	O
we	O
consider	O
in	O
this	O
book	O
:	O
oﬀ-policy	B
methods	I
.	O
recall	O
that	O
the	O
distinguishing	O
feature	O
of	O
on-policy	B
methods	I
is	O
that	O
they	O
estimate	O
the	O
value	B
of	O
a	O
policy	B
while	O
using	O
it	O
for	O
control	O
.	O
in	O
oﬀ-policy	O
methods	O
these	O
two	O
functions	O
are	O
separated	O
.	O
the	O
policy	B
used	O
to	O
generate	O
behavior	O
,	O
called	O
the	O
behavior	B
policy	I
,	O
may	O
in	O
fact	O
be	O
unrelated	O
to	O
the	O
policy	B
that	O
is	O
evaluated	O
and	O
improved	O
,	O
called	O
the	O
target	B
policy	O
.	O
an	O
advantage	O
of	O
this	O
separation	O
is	O
that	O
the	O
target	B
policy	O
may	O
be	O
deterministic	O
(	O
e.g.	O
,	O
greedy	O
)	O
,	O
while	O
the	O
behavior	B
policy	I
can	O
continue	O
to	O
sample	O
all	O
possible	O
actions	O
.	O
oﬀ-policy	B
monte	O
carlo	O
control	B
methods	O
use	O
one	O
of	O
the	O
techniques	O
presented	O
in	O
the	O
pre-	O
ceding	O
two	O
sections	O
.	O
they	O
follow	O
the	O
behavior	B
policy	I
while	O
learning	O
about	O
and	O
improving	O
the	O
target	B
policy	O
.	O
these	O
techniques	O
require	O
that	O
the	O
behavior	B
policy	I
has	O
a	O
nonzero	O
prob-	O
ability	O
of	O
selecting	O
all	O
actions	O
that	O
might	O
be	O
selected	O
by	O
the	O
target	B
policy	O
(	O
coverage	O
)	O
.	O
to	O
explore	O
all	O
possibilities	O
,	O
we	O
require	O
that	O
the	O
behavior	B
policy	I
be	O
soft	O
(	O
i.e.	O
,	O
that	O
it	O
select	O
all	O
actions	O
in	O
all	O
states	O
with	O
nonzero	O
probability	O
)	O
.	O
the	O
box	O
below	O
shows	O
an	O
oﬀ-policy	B
monte	O
carlo	O
control	B
method	O
,	O
based	O
on	O
gpi	O
and	O
weighted	O
importance	B
sampling	I
,	O
for	O
estimating	O
π∗	O
and	O
q∗	O
.	O
the	O
target	B
policy	O
π	O
≈	O
π∗	O
is	O
the	O
greedy	O
policy	O
with	O
respect	O
to	O
q	O
,	O
which	O
is	O
an	O
estimate	O
of	O
qπ	O
.	O
the	O
behavior	B
policy	I
b	O
can	O
be	O
anything	O
,	O
but	O
in	O
order	O
to	O
assure	O
convergence	O
of	O
π	O
to	O
the	O
optimal	O
policy	O
,	O
an	O
inﬁnite	O
number	O
of	O
returns	O
must	O
be	O
obtained	O
for	O
each	O
pair	O
of	O
state	O
and	O
action	O
.	O
this	O
can	O
be	O
assured	O
by	O
choosing	O
b	O
to	O
be	O
ε-soft	O
.	O
the	O
policy	B
π	O
converges	O
to	O
optimal	O
at	O
all	O
encountered	O
states	O
even	O
though	O
actions	O
are	O
selected	O
according	O
to	O
a	O
diﬀerent	O
soft	O
policy	O
b	O
,	O
which	O
may	O
change	O
between	O
or	O
even	O
within	O
episodes	B
.	O
oﬀ-policy	B
mc	O
control	B
,	O
for	O
estimating	O
π	O
≈	O
π∗	O
initialize	O
,	O
for	O
all	O
s	O
∈	O
s	O
,	O
a	O
∈	O
a	O
(	O
s	O
)	O
:	O
q	O
(	O
s	O
,	O
a	O
)	O
∈	O
r	O
(	O
arbitrarily	O
)	O
c	O
(	O
s	O
,	O
a	O
)	O
←	O
0	O
π	O
(	O
s	O
)	O
←	O
argmaxa	O
q	O
(	O
s	O
,	O
a	O
)	O
loop	O
forever	O
(	O
for	O
each	O
episode	O
)	O
:	O
(	O
with	O
ties	O
broken	O
consistently	O
)	O
b	O
←	O
any	O
soft	O
policy	O
generate	O
an	O
episode	O
using	O
b	O
:	O
s0	O
,	O
a0	O
,	O
r1	O
,	O
.	O
.	O
.	O
,	O
st−1	O
,	O
at−1	O
,	O
rt	O
g	O
←	O
0	O
w	O
←	O
1	O
loop	O
for	O
each	O
step	O
of	O
episode	O
,	O
t	O
=	O
t	O
−1	O
,	O
t	O
−2	O
,	O
.	O
.	O
.	O
,	O
0	O
:	O
g	O
←	O
γg	O
+	O
rt+1	O
c	O
(	O
st	O
,	O
at	O
)	O
←	O
c	O
(	O
st	O
,	O
at	O
)	O
+	O
w	O
q	O
(	O
st	O
,	O
at	O
)	O
←	O
q	O
(	O
st	O
,	O
at	O
)	O
+	O
w	O
π	O
(	O
st	O
)	O
←	O
argmaxa	O
q	O
(	O
st	O
,	O
a	O
)	O
if	O
at	O
(	O
cid:54	O
)	O
=	O
π	O
(	O
st	O
)	O
then	O
exit	O
for	O
loop	O
w	O
←	O
w	O
1	O
b	O
(	O
at|st	O
)	O
c	O
(	O
st	O
,	O
at	O
)	O
[	O
g	O
−	O
q	O
(	O
st	O
,	O
at	O
)	O
]	O
(	O
with	O
ties	O
broken	O
consistently	O
)	O
5.7.	O
oﬀ-policy	B
monte	O
carlo	O
control	B
111	O
a	O
potential	O
problem	O
is	O
that	O
this	O
method	O
learns	O
only	O
from	O
the	O
tails	O
of	O
episodes	O
,	O
when	O
all	O
of	O
the	O
remaining	O
actions	O
in	O
the	O
episode	O
are	O
greedy	O
.	O
if	O
nongreedy	O
actions	O
are	O
common	O
,	O
then	O
learning	O
will	O
be	O
slow	O
,	O
particularly	O
for	O
states	O
appearing	O
in	O
the	O
early	O
portions	O
of	O
long	O
episodes	B
.	O
potentially	O
,	O
this	O
could	O
greatly	O
slow	O
learning	O
.	O
there	O
has	O
been	O
insuﬃcient	O
experience	O
with	O
oﬀ-policy	B
monte	O
carlo	O
methods	O
to	O
assess	O
how	O
serious	O
this	O
problem	O
is	O
.	O
if	O
it	O
is	O
serious	O
,	O
the	O
most	O
important	O
way	O
to	O
address	O
it	O
is	O
probably	O
by	O
incorporating	O
temporal-	O
diﬀerence	O
learning	O
,	O
the	O
algorithmic	O
idea	O
developed	O
in	O
the	O
next	O
chapter	O
.	O
alternatively	O
,	O
if	O
γ	O
is	O
less	O
than	O
1	O
,	O
then	O
the	O
idea	O
developed	O
in	O
the	O
next	O
section	O
may	O
also	O
help	O
signiﬁcantly	O
.	O
1	O
b	O
(	O
at|st	O
)	O
.	O
why	O
is	O
this	O
nevertheless	O
correct	O
?	O
exercise	O
5.9	O
in	O
the	O
boxed	O
algorithm	O
for	O
oﬀ-policy	O
mc	O
control	B
,	O
you	O
may	O
have	O
been	O
expecting	O
the	O
w	O
update	O
to	O
have	O
involved	O
the	O
importance-sampling	O
ratio	B
π	O
(	O
at|st	O
)	O
b	O
(	O
at|st	O
)	O
,	O
but	O
(	O
cid:3	O
)	O
instead	O
it	O
involves	O
exercise	O
5.10	O
:	O
racetrack	O
(	O
programming	O
)	O
consider	O
driving	O
a	O
race	O
car	O
around	O
a	O
turn	O
like	O
those	O
shown	O
in	O
figure	O
5.5.	O
you	O
want	O
to	O
go	O
as	O
fast	O
as	O
possible	O
,	O
but	O
not	O
so	O
fast	O
as	O
to	O
run	O
oﬀ	O
the	O
track	O
.	O
in	O
our	O
simpliﬁed	O
racetrack	O
,	O
the	O
car	O
is	O
at	O
one	O
of	O
a	O
discrete	O
set	O
of	O
grid	O
positions	O
,	O
the	O
cells	O
in	O
the	O
diagram	O
.	O
the	O
velocity	O
is	O
also	O
discrete	O
,	O
a	O
number	O
of	O
grid	O
cells	O
moved	O
horizontally	O
and	O
vertically	O
per	O
time	O
step	O
.	O
the	O
actions	O
are	O
increments	O
to	O
the	O
velocity	O
components	O
.	O
each	O
may	O
be	O
changed	O
by	O
+1	O
,	O
−1	O
,	O
or	O
0	O
in	O
each	O
step	O
,	O
for	O
a	O
total	O
of	O
nine	O
(	O
3	O
×	O
3	O
)	O
actions	O
.	O
both	O
velocity	O
components	O
are	O
restricted	O
to	O
be	O
nonnegative	O
and	O
less	O
than	O
5	O
,	O
and	O
they	O
can	O
not	O
both	O
be	O
zero	O
except	O
at	O
the	O
starting	O
line	O
.	O
each	O
episode	O
begins	O
in	O
one	O
of	O
the	O
randomly	O
selected	O
start	O
states	O
with	O
both	O
velocity	O
components	O
zero	O
and	O
ends	O
when	O
the	O
car	O
crosses	O
the	O
ﬁnish	O
line	O
.	O
the	O
rewards	O
are	O
−1	O
for	O
each	O
step	O
until	O
the	O
car	O
crosses	O
the	O
ﬁnish	O
line	O
.	O
if	O
the	O
car	O
hits	O
the	O
track	O
boundary	O
,	O
it	O
is	O
moved	O
back	O
to	O
a	O
random	O
position	O
on	O
the	O
starting	O
line	O
,	O
both	O
velocity	O
components	O
are	O
reduced	O
to	O
zero	O
,	O
and	O
the	O
episode	O
continues	O
.	O
before	O
updating	O
the	O
car	O
’	O
s	O
location	O
at	O
each	O
time	O
step	O
,	O
check	O
to	O
see	O
if	O
the	O
projected	O
path	O
of	O
the	O
car	O
intersects	O
the	O
track	O
boundary	O
.	O
if	O
it	O
intersects	O
the	O
ﬁnish	O
line	O
,	O
the	O
episode	O
ends	O
;	O
if	O
it	O
intersects	O
anywhere	O
else	O
,	O
the	O
car	O
is	O
considered	O
to	O
have	O
figure	O
5.5	O
:	O
a	O
couple	O
of	O
right	O
turns	O
for	O
the	O
racetrack	O
task	O
.	O
starting	O
linefinishlinestarting	O
linefinishline	O
112	O
chapter	O
5	O
:	O
monte	O
carlo	O
methods	O
hit	O
the	O
track	O
boundary	O
and	O
is	O
sent	O
back	O
to	O
the	O
starting	O
line	O
.	O
to	O
make	O
the	O
task	O
more	O
challenging	O
,	O
with	O
probability	O
0.1	O
at	O
each	O
time	O
step	O
the	O
velocity	O
increments	O
are	O
both	O
zero	O
,	O
independently	O
of	O
the	O
intended	O
increments	O
.	O
apply	O
a	O
monte	O
carlo	O
control	B
method	O
to	O
this	O
task	O
to	O
compute	O
the	O
optimal	O
policy	O
from	O
each	O
starting	O
state	B
.	O
exhibit	O
several	O
trajectories	O
(	O
cid:3	O
)	O
following	O
the	O
optimal	O
policy	O
(	O
but	O
turn	O
the	O
noise	O
oﬀ	O
for	O
these	O
trajectories	O
)	O
.	O
5.8	O
*discounting-aware	O
importance	B
sampling	I
the	O
oﬀ-policy	B
methods	I
that	O
we	O
have	O
considered	O
so	O
far	O
are	O
based	O
on	O
forming	O
importance-	O
sampling	O
weights	O
for	O
returns	O
considered	O
as	O
unitary	O
wholes	O
,	O
without	O
taking	O
into	O
account	O
the	O
returns	O
’	O
internal	O
structures	O
as	O
sums	O
of	O
discounted	O
rewards	O
.	O
we	O
now	O
brieﬂy	O
consider	O
cutting-edge	O
research	O
ideas	O
for	O
using	O
this	O
structure	O
to	O
signiﬁcantly	O
reduce	O
the	O
variance	O
of	O
oﬀ-policy	O
estimators	O
.	O
for	O
example	O
,	O
consider	O
the	O
case	O
where	O
episodes	B
are	O
long	O
and	O
γ	O
is	O
signiﬁcantly	O
less	O
than	O
1.	O
for	O
concreteness	O
,	O
say	O
that	O
episodes	B
last	O
100	O
steps	O
and	O
that	O
γ	O
=	O
0.	O
the	O
return	B
from	O
time	O
0	O
will	O
then	O
be	O
just	O
g0	O
=	O
r1	O
,	O
but	O
its	O
importance	B
sampling	I
ratio	O
will	O
be	O
a	O
product	O
of	O
100	O
factors	O
,	O
π	O
(	O
a0|s0	O
)	O
b	O
(	O
a99|s99	O
)	O
.	O
in	O
ordinary	O
importance	B
sampling	I
,	O
the	O
return	B
b	O
(	O
a0|s0	O
)	O
will	O
be	O
scaled	O
by	O
the	O
entire	O
product	O
,	O
but	O
it	O
is	O
really	O
only	O
necessary	O
to	O
scale	O
by	O
the	O
ﬁrst	O
factor	O
,	O
by	O
π	O
(	O
a0|s0	O
)	O
b	O
(	O
a99|s99	O
)	O
are	O
irrelevant	O
because	O
after	O
the	O
ﬁrst	O
reward	O
the	O
return	B
has	O
already	O
been	O
determined	O
.	O
these	O
later	O
factors	O
are	O
all	O
independent	O
of	O
the	O
return	B
and	O
of	O
expected	O
value	B
1	O
;	O
they	O
do	O
not	O
change	O
the	O
expected	B
update	I
,	O
but	O
they	O
add	O
enormously	O
to	O
its	O
variance	O
.	O
in	O
some	O
cases	O
they	O
could	O
even	O
make	O
the	O
variance	O
inﬁnite	O
.	O
let	O
us	O
now	O
consider	O
an	O
idea	O
for	O
avoiding	O
this	O
large	O
extraneous	O
variance	O
.	O
b	O
(	O
a0|s0	O
)	O
.	O
the	O
other	O
99	O
factors	O
π	O
(	O
a1|s1	O
)	O
π	O
(	O
a1|s1	O
)	O
b	O
(	O
a1|s1	O
)	O
···	O
π	O
(	O
a99|s99	O
)	O
b	O
(	O
a1|s1	O
)	O
···	O
π	O
(	O
a99|s99	O
)	O
the	O
essence	O
of	O
the	O
idea	O
is	O
to	O
think	O
of	O
discounting	O
as	O
determining	O
a	O
probability	O
of	O
termination	O
or	O
,	O
equivalently	O
,	O
a	O
degree	O
of	O
partial	O
termination	O
.	O
for	O
any	O
γ	O
∈	O
[	O
0	O
,	O
1	O
)	O
,	O
we	O
can	O
think	O
of	O
the	O
return	B
g0	O
as	O
partly	O
terminating	O
in	O
one	O
step	O
,	O
to	O
the	O
degree	O
1	O
−	O
γ	O
,	O
producing	O
a	O
return	B
of	O
just	O
the	O
ﬁrst	O
reward	O
,	O
r1	O
,	O
and	O
as	O
partly	O
terminating	O
after	O
two	O
steps	O
,	O
to	O
the	O
degree	O
(	O
1−	O
γ	O
)	O
γ	O
,	O
producing	O
a	O
return	B
of	O
r1	O
+	O
r2	O
,	O
and	O
so	O
on	O
.	O
the	O
latter	O
degree	O
corresponds	O
to	O
terminating	O
on	O
the	O
second	O
step	O
,	O
1	O
−	O
γ	O
,	O
and	O
not	O
having	O
already	O
terminated	O
on	O
the	O
ﬁrst	O
step	O
,	O
γ.	O
the	O
degree	O
of	O
termination	O
on	O
the	O
third	O
step	O
is	O
thus	O
(	O
1	O
−	O
γ	O
)	O
γ2	O
,	O
with	O
the	O
γ2	O
reﬂecting	O
that	O
termination	O
did	O
not	O
occur	O
on	O
either	O
of	O
the	O
ﬁrst	O
two	O
steps	O
.	O
the	O
partial	O
returns	O
here	O
are	O
called	O
ﬂat	B
partial	I
returns	O
:	O
¯gt	O
:	O
h	O
.	O
=	O
rt+1	O
+	O
rt+2	O
+	O
···	O
+	O
rh	O
,	O
0	O
≤	O
t	O
<	O
h	O
≤	O
t	O
,	O
where	O
“	O
ﬂat	O
”	O
denotes	O
the	O
absence	O
of	O
discounting	O
,	O
and	O
“	O
partial	O
”	O
denotes	O
that	O
these	O
returns	O
do	O
not	O
extend	O
all	O
the	O
way	O
to	O
termination	O
but	O
instead	O
stop	O
at	O
h	O
,	O
called	O
the	O
horizon	O
(	O
and	O
t	O
is	O
the	O
time	O
of	O
termination	O
of	O
the	O
episode	O
)	O
.	O
the	O
conventional	O
full	O
return	B
gt	O
can	O
be	O
5.9.	O
per-decision	B
importance	O
sampling	O
113	O
viewed	O
as	O
a	O
sum	O
of	O
ﬂat	O
partial	O
returns	O
as	O
suggested	O
above	O
as	O
follows	O
:	O
gt	O
.	O
=	O
rt+1	O
+	O
γrt+2	O
+	O
γ2rt+3	O
+	O
···	O
+	O
γt−t−1rt	O
=	O
(	O
1	O
−	O
γ	O
)	O
rt+1	O
+	O
(	O
1	O
−	O
γ	O
)	O
γ	O
(	O
rt+1	O
+	O
rt+2	O
)	O
+	O
(	O
1	O
−	O
γ	O
)	O
γ2	O
(	O
rt+1	O
+	O
rt+2	O
+	O
rt+3	O
)	O
...	O
+	O
(	O
1	O
−	O
γ	O
)	O
γt−t−2	O
(	O
rt+1	O
+	O
rt+2	O
+	O
···	O
+	O
rt−1	O
)	O
+	O
γt−t−1	O
(	O
rt+1	O
+	O
rt+2	O
+	O
···	O
+	O
rt	O
)	O
=	O
(	O
1	O
−	O
γ	O
)	O
γh−t−1	O
¯gt	O
:	O
h	O
+	O
γt−t−1	O
¯gt	O
:	O
t	O
.	O
t−1	O
(	O
cid:88	O
)	O
h=t+1	O
now	O
we	O
need	O
to	O
scale	O
the	O
ﬂat	B
partial	I
returns	O
by	O
an	O
importance	B
sampling	I
ratio	O
that	O
is	O
similarly	O
truncated	B
.	O
as	O
¯gt	O
:	O
h	O
only	O
involves	O
rewards	O
up	O
to	O
a	O
horizon	O
h	O
,	O
we	O
only	O
need	O
the	O
ratio	B
of	O
the	O
probabilities	O
up	O
to	O
h.	O
we	O
deﬁne	O
an	O
ordinary	O
importance-sampling	O
estimator	O
,	O
analogous	O
to	O
(	O
5.5	O
)	O
,	O
as	O
=	O
(	O
cid:80	O
)	O
t∈t	O
(	O
s	O
)	O
(	O
cid:16	O
)	O
(	O
1	O
−	O
γ	O
)	O
(	O
cid:80	O
)	O
t	O
(	O
t	O
)	O
−1	O
.	O
v	O
(	O
s	O
)	O
|t	O
(	O
s	O
)	O
|	O
h=t+1	O
γh−t−1ρt	O
:	O
h−1	O
¯gt	O
:	O
h	O
+	O
γt	O
(	O
t	O
)	O
−t−1ρt	O
:	O
t	O
(	O
t	O
)	O
−1	O
¯gt	O
:	O
t	O
(	O
t	O
)	O
(	O
cid:17	O
)	O
,	O
(	O
5.9	O
)	O
and	O
a	O
weighted	O
importance-sampling	O
estimator	O
,	O
analogous	O
to	O
(	O
5.6	O
)	O
,	O
as	O
v	O
(	O
s	O
)	O
.	O
=	O
(	O
cid:80	O
)	O
t∈t	O
(	O
s	O
)	O
(	O
cid:16	O
)	O
(	O
1	O
−	O
γ	O
)	O
(	O
cid:80	O
)	O
t	O
(	O
t	O
)	O
−1	O
(	O
cid:80	O
)	O
t∈t	O
(	O
s	O
)	O
(	O
cid:16	O
)	O
(	O
1	O
−	O
γ	O
)	O
(	O
cid:80	O
)	O
t	O
(	O
t	O
)	O
−1	O
h=t+1	O
γh−t−1ρt	O
:	O
h−1	O
¯gt	O
:	O
h	O
+	O
γt	O
(	O
t	O
)	O
−t−1ρt	O
:	O
t	O
(	O
t	O
)	O
−1	O
h=t+1	O
γh−t−1ρt	O
:	O
h−1	O
+	O
γt	O
(	O
t	O
)	O
−t−1ρt	O
:	O
t	O
(	O
t	O
)	O
−1	O
(	O
cid:17	O
)	O
¯gt	O
:	O
t	O
(	O
t	O
)	O
(	O
cid:17	O
)	O
.	O
(	O
5.10	O
)	O
we	O
call	O
these	O
two	O
estimators	O
discounting-aware	O
importance	B
sampling	I
estimators	O
.	O
they	O
take	O
into	O
account	O
the	O
discount	O
rate	O
but	O
have	O
no	O
eﬀect	O
(	O
are	O
the	O
same	O
as	O
the	O
oﬀ-policy	B
estimators	O
from	O
section	O
5.5	O
)	O
if	O
γ	O
=	O
1	O
.	O
5.9	O
*per-decision	O
importance	B
sampling	I
there	O
is	O
one	O
more	O
way	O
in	O
which	O
the	O
structure	O
of	O
the	O
return	B
as	O
a	O
sum	O
of	O
rewards	O
can	O
be	O
taken	O
into	O
account	O
in	O
oﬀ-policy	O
importance	B
sampling	I
,	O
a	O
way	O
that	O
may	O
be	O
able	O
to	O
reduce	O
variance	O
even	O
in	O
the	O
absence	O
of	O
discounting	O
(	O
that	O
is	O
,	O
even	O
if	O
γ	O
=	O
1	O
)	O
.	O
in	O
the	O
oﬀ-policy	O
estimators	O
(	O
5.5	O
)	O
and	O
(	O
5.6	O
)	O
,	O
each	O
term	O
of	O
the	O
sum	O
in	O
the	O
numerator	O
is	O
itself	O
a	O
sum	O
:	O
ρt	O
:	O
t−1gt	O
=	O
ρt	O
:	O
t−1	O
(	O
cid:0	O
)	O
rt+1	O
+	O
γrt+2	O
+	O
···	O
+	O
γt−t−1rt	O
(	O
cid:1	O
)	O
=	O
ρt	O
:	O
t−1rt+1	O
+	O
γρt	O
:	O
t−1rt+2	O
+	O
···	O
+	O
γt−t−1ρt	O
:	O
t−1rt	O
.	O
(	O
5.11	O
)	O
114	O
chapter	O
5	O
:	O
monte	O
carlo	O
methods	O
the	O
oﬀ-policy	B
estimators	O
rely	O
on	O
the	O
expected	O
values	O
of	O
these	O
terms	O
;	O
let	O
us	O
see	O
if	O
we	O
can	O
write	O
them	O
in	O
a	O
simpler	O
way	O
.	O
note	O
that	O
each	O
sub-term	O
of	O
(	O
5.11	O
)	O
is	O
a	O
product	O
of	O
a	O
random	O
reward	O
and	O
a	O
random	O
importance-sampling	O
ratio	B
.	O
for	O
example	O
,	O
the	O
ﬁrst	O
sub-term	O
can	O
be	O
written	O
,	O
using	O
(	O
5.3	O
)	O
,	O
as	O
ρt	O
:	O
t−1rt+1	O
=	O
π	O
(	O
at|st	O
)	O
b	O
(	O
at|st	O
)	O
π	O
(	O
at+1|st+1	O
)	O
b	O
(	O
at+1|st+1	O
)	O
π	O
(	O
at+2|st+2	O
)	O
b	O
(	O
at+2|st+2	O
)	O
···	O
π	O
(	O
at−1|st−1	O
)	O
b	O
(	O
at−1|st−1	O
)	O
rt+1	O
.	O
now	O
notice	O
that	O
,	O
of	O
all	O
these	O
factors	O
,	O
only	O
the	O
ﬁrst	O
and	O
the	O
last	O
(	O
the	O
reward	O
)	O
are	O
correlated	O
;	O
all	O
the	O
other	O
ratios	O
are	O
independent	O
random	O
variables	O
whose	O
expected	O
value	O
is	O
one	O
:	O
e	O
(	O
cid:20	O
)	O
π	O
(	O
ak|sk	O
)	O
b	O
(	O
ak|sk	O
)	O
(	O
cid:21	O
)	O
.	O
=	O
(	O
cid:88	O
)	O
a	O
b	O
(	O
a|sk	O
)	O
π	O
(	O
a|sk	O
)	O
b	O
(	O
a|sk	O
)	O
=	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
a|sk	O
)	O
=	O
1	O
.	O
(	O
5.12	O
)	O
thus	O
,	O
because	O
the	O
expectation	O
of	O
the	O
product	O
of	O
independent	O
random	O
variables	O
is	O
the	O
product	O
of	O
their	O
expectations	O
,	O
all	O
the	O
ratios	O
except	O
the	O
ﬁrst	O
drop	O
out	O
in	O
expectation	O
,	O
leaving	O
just	O
e	O
[	O
ρt	O
:	O
t−1rt+1	O
]	O
=	O
e	O
[	O
ρt	O
:	O
trt+1	O
]	O
.	O
if	O
we	O
repeat	O
this	O
analysis	O
for	O
the	O
kth	O
term	O
of	O
(	O
5.11	O
)	O
,	O
we	O
get	O
e	O
[	O
ρt	O
:	O
t−1rt+k	O
]	O
=	O
e	O
[	O
ρt	O
:	O
t+k−1rt+k	O
]	O
.	O
it	O
follows	O
then	O
that	O
the	O
expectation	O
of	O
our	O
original	O
term	O
(	O
5.11	O
)	O
can	O
be	O
written	O
where	O
e	O
[	O
ρt	O
:	O
t−1gt	O
]	O
=	O
e	O
(	O
cid:104	O
)	O
˜gt	O
(	O
cid:105	O
)	O
,	O
˜gt	O
=	O
ρt	O
:	O
trt+1	O
+	O
γρt	O
:	O
t+1rt+2	O
+	O
γ2ρt	O
:	O
t+2rt+3	O
+	O
···	O
+	O
γt−t−1ρt	O
:	O
t−1rt	O
.	O
we	O
call	O
this	O
idea	O
per-decision	B
importance	O
sampling	O
.	O
it	O
follows	O
immediately	O
that	O
there	O
is	O
an	O
alternate	O
importance-sampling	O
estimator	O
,	O
with	O
the	O
same	O
unbiased	O
expectation	O
as	O
the	O
ordinary-importance-sampling	O
estimator	O
(	O
5.5	O
)	O
,	O
using	O
˜gt	O
:	O
v	O
(	O
s	O
)	O
.	O
=	O
(	O
cid:80	O
)	O
t∈t	O
(	O
s	O
)	O
|t	O
(	O
s	O
)	O
|	O
˜gt	O
,	O
(	O
5.13	O
)	O
which	O
we	O
might	O
expect	O
to	O
sometimes	O
be	O
of	O
lower	O
variance	O
.	O
is	O
there	O
a	O
per-decision	B
version	O
of	O
weighted	O
importance	O
sampling	O
?	O
this	O
is	O
less	O
clear	O
.	O
so	O
far	O
,	O
all	O
the	O
estimators	O
that	O
have	O
been	O
proposed	O
for	O
this	O
that	O
we	O
know	O
of	O
are	O
not	O
consistent	O
(	O
that	O
is	O
,	O
they	O
do	O
not	O
converge	O
to	O
the	O
true	O
value	O
with	O
inﬁnite	O
data	O
)	O
.	O
∗exercise	O
5.11	O
modify	O
the	O
algorithm	O
for	O
oﬀ-policy	O
monte	O
carlo	O
control	B
(	O
page	O
110	O
)	O
to	O
use	O
the	O
idea	O
of	O
the	O
truncated	B
weighted-average	O
estimator	O
(	O
5.10	O
)	O
.	O
note	O
that	O
you	O
will	O
ﬁrst	O
(	O
cid:3	O
)	O
need	O
to	O
convert	O
this	O
equation	O
to	O
action	B
values	O
.	O
5.10.	O
summary	O
115	O
5.10	O
summary	O
the	O
monte	O
carlo	O
methods	O
presented	O
in	O
this	O
chapter	O
learn	O
value	B
functions	O
and	O
optimal	O
policies	O
from	O
experience	O
in	O
the	O
form	O
of	O
sample	O
episodes	B
.	O
this	O
gives	O
them	O
at	O
least	O
three	O
kinds	O
of	O
advantages	O
over	O
dp	O
methods	O
.	O
first	O
,	O
they	O
can	O
be	O
used	O
to	O
learn	O
optimal	O
behavior	O
directly	O
from	O
interaction	O
with	O
the	O
environment	B
,	O
with	O
no	O
model	B
of	I
the	I
environment	I
’	O
s	O
dynamics	O
.	O
second	O
,	O
they	O
can	O
be	O
used	O
with	O
simulation	O
or	O
sample	O
models	O
.	O
for	O
surprisingly	O
many	O
applications	O
it	O
is	O
easy	O
to	O
simulate	O
sample	O
episodes	O
even	O
though	O
it	O
is	O
diﬃcult	O
to	O
construct	O
the	O
kind	O
of	O
explicit	O
model	O
of	O
transition	O
probabilities	O
required	O
by	O
dp	O
methods	O
.	O
third	O
,	O
it	O
is	O
easy	O
and	O
eﬃcient	O
to	O
focus	O
monte	O
carlo	O
methods	O
on	O
a	O
small	O
subset	O
of	O
the	O
states	O
.	O
a	O
region	O
of	O
special	O
interest	O
can	O
be	O
accurately	O
evaluated	O
without	O
going	O
to	O
the	O
expense	O
of	O
accurately	O
evaluating	O
the	O
rest	O
of	O
the	O
state	B
set	O
(	O
we	O
explore	O
this	O
further	O
in	O
chapter	O
8	O
)	O
.	O
a	O
fourth	O
advantage	O
of	O
monte	O
carlo	O
methods	O
,	O
which	O
we	O
discuss	O
later	O
in	O
the	O
book	O
,	O
is	O
that	O
they	O
may	O
be	O
less	O
harmed	O
by	O
violations	O
of	O
the	O
markov	O
property	O
.	O
this	O
is	O
because	O
they	O
do	O
not	O
update	O
their	O
value	B
estimates	O
on	O
the	O
basis	O
of	O
the	O
value	B
estimates	O
of	O
successor	O
states	O
.	O
in	O
other	O
words	O
,	O
it	O
is	O
because	O
they	O
do	O
not	O
bootstrap	O
.	O
in	O
designing	O
monte	O
carlo	O
control	B
methods	O
we	O
have	O
followed	O
the	O
overall	O
schema	O
of	O
generalized	O
policy	B
iteration	I
(	O
gpi	O
)	O
introduced	O
in	O
chapter	O
4.	O
gpi	O
involves	O
interacting	O
processes	O
of	O
policy	O
evaluation	O
and	O
policy	O
improvement	O
.	O
monte	O
carlo	O
methods	O
provide	O
an	O
alternative	O
policy	B
evaluation	I
process	O
.	O
rather	O
than	O
use	O
a	O
model	O
to	O
compute	O
the	O
value	B
of	O
each	O
state	B
,	O
they	O
simply	O
average	O
many	O
returns	O
that	O
start	O
in	O
the	O
state	O
.	O
because	O
a	O
state	B
’	O
s	O
value	B
is	O
the	O
expected	O
return	O
,	O
this	O
average	O
can	O
become	O
a	O
good	O
approximation	O
to	O
the	O
value	B
.	O
in	O
control	O
methods	O
we	O
are	O
particularly	O
interested	O
in	O
approximating	O
action-value	O
functions	O
,	O
because	O
these	O
can	O
be	O
used	O
to	O
improve	O
the	O
policy	B
without	O
requiring	O
a	O
model	B
of	I
the	I
environment	I
’	O
s	O
transition	O
dynamics	O
.	O
monte	O
carlo	O
methods	O
intermix	O
policy	B
evaluation	I
and	O
policy	B
improvement	I
steps	O
on	O
an	O
episode-by-episode	O
basis	O
,	O
and	O
can	O
be	O
incrementally	O
implemented	O
on	O
an	O
episode-by-episode	O
basis	O
.	O
maintaining	O
suﬃcient	O
exploration	O
is	O
an	O
issue	O
in	O
monte	O
carlo	O
control	B
methods	O
.	O
it	O
is	O
not	O
enough	O
just	O
to	O
select	O
the	O
actions	O
currently	O
estimated	O
to	O
be	O
best	O
,	O
because	O
then	O
no	O
returns	O
will	O
be	O
obtained	O
for	O
alternative	O
actions	O
,	O
and	O
it	O
may	O
never	O
be	O
learned	O
that	O
they	O
are	O
actually	O
better	O
.	O
one	O
approach	O
is	O
to	O
ignore	O
this	O
problem	O
by	O
assuming	O
that	O
episodes	B
begin	O
with	O
state–action	O
pairs	O
randomly	O
selected	O
to	O
cover	O
all	O
possibilities	O
.	O
such	O
exploring	B
starts	I
can	O
sometimes	O
be	O
arranged	O
in	O
applications	O
with	O
simulated	O
episodes	B
,	O
but	O
are	O
unlikely	O
in	O
learning	O
from	O
real	O
experience	O
.	O
in	O
on-policy	O
methods	O
,	O
the	O
agent	O
commits	O
to	O
always	O
exploring	O
and	O
tries	O
to	O
ﬁnd	O
the	O
best	O
policy	B
that	O
still	O
explores	O
.	O
in	O
oﬀ-policy	O
methods	O
,	O
the	O
agent	O
also	O
explores	O
,	O
but	O
learns	O
a	O
deterministic	O
optimal	O
policy	O
that	O
may	O
be	O
unrelated	O
to	O
the	O
policy	B
followed	O
.	O
oﬀ-policy	B
prediction	I
refers	O
to	O
learning	O
the	O
value	B
function	I
of	O
a	O
target	B
policy	O
from	O
data	O
generated	O
by	O
a	O
diﬀerent	O
behavior	B
policy	I
.	O
such	O
learning	O
methods	O
are	O
based	O
on	O
some	O
form	O
of	O
importance	O
sampling	O
,	O
that	O
is	O
,	O
on	O
weighting	O
returns	O
by	O
the	O
ratio	B
of	O
the	O
probabilities	O
of	O
taking	O
the	O
observed	O
actions	O
under	O
the	O
two	O
policies	O
,	O
thereby	O
transforming	O
their	O
expecta-	O
tions	O
from	O
the	O
behavior	B
policy	I
to	O
the	O
target	B
policy	O
.	O
ordinary	O
importance	B
sampling	I
uses	O
a	O
simple	O
average	O
of	O
the	O
weighted	O
returns	O
,	O
whereas	O
weighted	O
importance	O
sampling	O
uses	O
116	O
chapter	O
5	O
:	O
monte	O
carlo	O
methods	O
a	O
weighted	O
average	O
.	O
ordinary	O
importance	B
sampling	I
produces	O
unbiased	O
estimates	O
,	O
but	O
has	O
larger	O
,	O
possibly	O
inﬁnite	O
,	O
variance	O
,	O
whereas	O
weighted	O
importance	O
sampling	O
always	O
has	O
ﬁnite	O
variance	O
and	O
is	O
preferred	O
in	O
practice	O
.	O
despite	O
their	O
conceptual	O
simplicity	O
,	O
oﬀ-policy	B
monte	O
carlo	O
methods	O
for	O
both	O
prediction	B
and	O
control	B
remain	O
unsettled	O
and	O
are	O
a	O
subject	O
of	O
ongoing	O
research	O
.	O
the	O
monte	O
carlo	O
methods	O
treated	O
in	O
this	O
chapter	O
diﬀer	O
from	O
the	O
dp	O
methods	O
treated	O
in	O
the	O
previous	O
chapter	O
in	O
two	O
major	O
ways	O
.	O
first	O
,	O
they	O
operate	O
on	O
sample	O
experience	O
,	O
and	O
thus	O
can	O
be	O
used	O
for	O
direct	O
learning	O
without	O
a	O
model	O
.	O
second	O
,	O
they	O
do	O
not	O
bootstrap	O
.	O
that	O
is	O
,	O
they	O
do	O
not	O
update	O
their	O
value	B
estimates	O
on	O
the	O
basis	O
of	O
other	O
value	B
estimates	O
.	O
these	O
two	O
diﬀerences	O
are	O
not	O
tightly	O
linked	O
,	O
and	O
can	O
be	O
separated	O
.	O
in	O
the	O
next	O
chapter	O
we	O
consider	O
methods	O
that	O
learn	O
from	O
experience	O
,	O
like	O
monte	O
carlo	O
methods	O
,	O
but	O
also	O
bootstrap	O
,	O
like	O
dp	O
methods	O
.	O
bibliographical	O
and	O
historical	O
remarks	O
the	O
term	O
“	O
monte	O
carlo	O
”	O
dates	O
from	O
the	O
1940s	O
,	O
when	O
physicists	O
at	O
los	O
alamos	O
devised	O
games	O
of	O
chance	O
that	O
they	O
could	O
study	O
to	O
help	O
understand	O
complex	O
physical	O
phenomena	O
relating	O
to	O
the	O
atom	O
bomb	O
.	O
coverage	O
of	O
monte	O
carlo	O
methods	O
in	O
this	O
sense	O
can	O
be	O
found	O
in	O
several	O
textbooks	O
(	O
e.g.	O
,	O
kalos	O
and	O
whitlock	O
,	O
1986	O
;	O
rubinstein	O
,	O
1981	O
)	O
.	O
5.1–2	O
singh	O
and	O
sutton	O
(	O
1996	O
)	O
distinguished	O
between	O
every-visit	O
and	O
ﬁrst-visit	O
mc	O
methods	O
and	O
proved	O
results	O
relating	O
these	O
methods	O
to	O
reinforcement	B
learning	I
algorithms	O
.	O
the	O
blackjack	B
example	I
is	O
based	O
on	O
an	O
example	O
used	O
by	O
widrow	O
,	O
gupta	O
,	O
and	O
maitra	O
(	O
1973	O
)	O
.	O
the	O
soap	B
bubble	I
example	I
is	O
a	O
classical	O
dirichlet	O
problem	O
whose	O
monte	O
carlo	O
solution	O
was	O
ﬁrst	O
proposed	O
by	O
kakutani	O
(	O
1945	O
;	O
see	O
hersh	O
and	O
griego	O
,	O
1969	O
;	O
doyle	O
and	O
snell	O
,	O
1984	O
)	O
.	O
barto	O
and	O
duﬀ	O
(	O
1994	O
)	O
discussed	O
policy	B
evaluation	I
in	O
the	O
context	O
of	O
classical	O
monte	O
carlo	O
algorithms	O
for	O
solving	O
systems	O
of	O
linear	O
equations	O
.	O
they	O
used	O
the	O
analysis	O
of	O
curtiss	O
(	O
1954	O
)	O
to	O
point	O
out	O
the	O
computational	O
advantages	O
of	O
monte	O
carlo	O
policy	B
evaluation	I
for	O
large	O
problems	O
.	O
5.3–4	O
monte	O
carlo	O
es	O
was	O
introduced	O
in	O
the	O
1998	O
edition	O
of	O
this	O
book	O
.	O
that	O
may	O
have	O
been	O
the	O
ﬁrst	O
explicit	O
connection	O
between	O
monte	O
carlo	O
estimation	O
and	B
control	I
methods	O
based	O
on	O
policy	B
iteration	I
.	O
an	O
early	O
use	O
of	O
monte	O
carlo	O
methods	O
to	O
estimate	O
action	B
values	O
in	O
a	O
reinforcement	B
learning	I
context	O
was	O
by	O
michie	O
and	O
chambers	O
(	O
1968	O
)	O
.	O
in	B
pole	I
balancing	I
(	O
page	O
56	O
)	O
,	O
they	O
used	O
averages	O
of	O
episode	O
durations	O
to	O
assess	O
the	O
worth	O
(	O
expected	O
balancing	O
“	O
life	O
”	O
)	O
of	O
each	O
possible	O
action	B
in	O
each	O
state	B
,	O
and	O
then	O
used	O
these	O
assessments	O
to	O
control	B
action	O
selections	O
.	O
their	O
method	O
is	O
similar	O
in	O
spirit	O
to	O
monte	O
carlo	O
es	O
with	O
every-visit	O
mc	O
estimates	O
.	O
narendra	O
and	O
wheeler	O
(	O
1986	O
)	O
studied	O
a	O
monte	O
carlo	O
method	O
for	O
ergodic	O
ﬁnite	O
markov	O
chains	O
that	O
used	O
the	O
return	B
accumulated	O
between	O
successive	O
visits	O
to	O
the	O
same	O
state	B
as	O
a	O
reward	O
for	O
adjusting	O
a	O
learning	O
automaton	O
’	O
s	O
action	B
probabilities	O
.	O
5.5	O
eﬃcient	O
oﬀ-policy	B
learning	O
has	O
become	O
recognized	O
as	O
an	O
important	O
challenge	O
that	O
arises	O
in	O
several	O
ﬁelds	O
.	O
for	O
example	O
,	O
it	O
is	O
closely	O
related	O
to	O
the	O
idea	O
of	O
“	O
in-	O
5.10.	O
summary	O
117	O
terventions	O
”	O
and	O
“	O
counterfactuals	O
”	O
in	O
probabalistic	O
graphical	O
(	O
bayesian	O
)	O
models	O
(	O
e.g.	O
,	O
pearl	O
,	O
1995	O
;	O
balke	O
and	O
pearl	O
,	O
1994	O
)	O
.	O
oﬀ-policy	B
methods	I
using	O
importance	B
sampling	I
have	O
a	O
long	O
history	O
and	O
yet	O
still	O
are	O
not	O
well	O
understood	O
.	O
weighted	O
importance	O
sampling	O
,	O
which	O
is	O
also	O
sometimes	O
called	O
normalized	O
importance	B
sampling	I
(	O
e.g.	O
,	O
koller	O
and	O
friedman	O
,	O
2009	O
)	O
,	O
is	O
discussed	O
by	O
rubinstein	O
(	O
1981	O
)	O
,	O
hesterberg	O
(	O
1988	O
)	O
,	O
shelton	O
(	O
2001	O
)	O
,	O
and	O
liu	O
(	O
2001	O
)	O
among	O
others	O
.	O
the	O
target	B
policy	O
in	O
oﬀ-policy	O
learning	O
is	O
sometimes	O
referred	O
to	O
in	O
the	O
literature	O
as	O
the	O
“	O
estimation	O
”	O
policy	B
,	O
as	O
it	O
was	O
in	O
the	O
ﬁrst	O
edition	O
of	O
this	O
book	O
.	O
5.7	O
5.8	O
5.9	O
the	O
racetrack	B
exercise	I
is	O
adapted	O
from	O
barto	O
,	O
bradtke	O
,	O
and	O
singh	O
(	O
1995	O
)	O
,	O
and	O
from	O
gardner	O
(	O
1973	O
)	O
.	O
our	O
treatment	O
of	O
the	O
idea	O
of	O
discounting-aware	O
importance	B
sampling	I
is	O
based	O
on	O
the	O
analysis	O
of	O
sutton	O
,	O
mahmood	O
,	O
precup	O
,	O
and	O
van	O
hasselt	O
(	O
2014	O
)	O
.	O
it	O
has	O
been	O
worked	O
out	O
most	O
fully	O
to	O
date	O
by	O
mahmood	O
(	O
2017	O
;	O
mahmood	O
,	O
van	O
hasselt	O
,	O
and	O
sutton	O
,	O
2014	O
)	O
.	O
per-decision	B
importance	O
sampling	O
was	O
introduced	O
by	O
precup	O
,	O
sutton	O
,	O
and	O
singh	O
(	O
2000	O
)	O
.	O
they	O
also	O
combined	O
oﬀ-policy	B
learning	O
with	O
temporal-diﬀerence	O
learn-	O
ing	B
,	O
eligibility	B
traces	I
,	O
and	O
approximation	O
methods	O
,	O
introducing	O
subtle	O
issues	O
that	O
we	O
consider	O
in	O
later	O
chapters	O
.	O
chapter	O
6	O
temporal-diﬀerence	B
learning	I
if	O
one	O
had	O
to	O
identify	O
one	O
idea	O
as	O
central	O
and	O
novel	O
to	O
reinforcement	B
learning	I
,	O
it	O
would	O
un-	O
doubtedly	O
be	O
temporal-diﬀerence	O
(	O
td	O
)	O
learning	O
.	O
td	O
learning	O
is	O
a	O
combination	O
of	O
monte	O
carlo	O
ideas	O
and	B
dynamic	I
programming	I
(	O
dp	O
)	O
ideas	O
.	O
like	O
monte	O
carlo	O
methods	O
,	O
td	O
methods	O
can	O
learn	O
directly	O
from	O
raw	O
experience	O
without	O
a	O
model	B
of	I
the	I
environment	I
’	O
s	O
dynamics	O
.	O
like	O
dp	O
,	O
td	O
methods	O
update	O
estimates	O
based	O
in	O
part	O
on	O
other	O
learned	O
esti-	O
mates	O
,	O
without	O
waiting	O
for	O
a	O
ﬁnal	O
outcome	O
(	O
they	O
bootstrap	O
)	O
.	O
the	O
relationship	O
between	O
td	O
,	O
dp	O
,	O
and	O
monte	O
carlo	O
methods	O
is	O
a	O
recurring	O
theme	O
in	O
the	O
theory	O
of	O
reinforcement	O
learning	O
;	O
this	O
chapter	O
is	O
the	O
beginning	O
of	O
our	O
exploration	O
of	O
it	O
.	O
before	O
we	O
are	O
done	O
,	O
we	O
will	O
see	O
that	O
these	O
ideas	O
and	O
methods	O
blend	O
into	O
each	O
other	O
and	O
can	O
be	O
combined	O
in	O
many	O
ways	O
.	O
in	O
particular	O
,	O
in	O
chapter	O
7	O
we	O
introduce	O
n-step	B
algorithms	O
,	O
which	O
provide	O
a	O
bridge	O
from	O
td	O
to	O
monte	O
carlo	O
methods	O
,	O
and	O
in	O
chapter	O
12	O
we	O
introduce	O
the	O
td	O
(	O
λ	O
)	O
algorithm	O
,	O
which	O
seamlessly	O
uniﬁes	O
them	O
.	O
as	O
usual	O
,	O
we	O
start	O
by	O
focusing	O
on	O
the	O
policy	B
evaluation	I
or	O
prediction	B
problem	O
,	O
the	O
problem	O
of	O
estimating	O
the	O
value	B
function	I
vπ	O
for	O
a	O
given	O
policy	O
π.	O
for	O
the	O
control	B
prob-	O
lem	O
(	O
ﬁnding	O
an	O
optimal	O
policy	O
)	O
,	O
dp	O
,	O
td	O
,	O
and	O
monte	O
carlo	O
methods	O
all	O
use	O
some	O
varia-	O
tion	B
of	O
generalized	O
policy	O
iteration	O
(	O
gpi	O
)	O
.	O
the	O
diﬀerences	O
in	O
the	O
methods	O
are	O
primarily	O
diﬀerences	O
in	O
their	O
approaches	O
to	O
the	O
prediction	B
problem	O
.	O
6.1	O
td	O
prediction	B
both	O
td	O
and	O
monte	O
carlo	O
methods	O
use	O
experience	O
to	O
solve	O
the	O
prediction	B
problem	O
.	O
given	O
some	O
experience	O
following	O
a	O
policy	B
π	O
,	O
both	O
methods	O
update	O
their	O
estimate	O
v	O
of	O
vπ	O
for	O
the	O
nonterminal	O
states	O
st	O
occurring	O
in	O
that	O
experience	O
.	O
roughly	O
speaking	O
,	O
monte	O
carlo	O
methods	O
wait	O
until	O
the	O
return	B
following	O
the	O
visit	O
is	O
known	O
,	O
then	O
use	O
that	O
return	B
as	O
a	O
target	B
for	O
v	O
(	O
st	O
)	O
.	O
a	O
simple	O
every-visit	O
monte	O
carlo	O
method	O
suitable	O
for	O
nonstationary	O
environments	O
is	O
v	O
(	O
st	O
)	O
←	O
v	O
(	O
st	O
)	O
+	O
α	O
(	O
cid:104	O
)	O
gt	O
−	O
v	O
(	O
st	O
)	O
(	O
cid:105	O
)	O
,	O
119	O
(	O
6.1	O
)	O
120	O
chapter	O
6	O
:	O
temporal-diﬀerence	B
learning	I
where	O
gt	O
is	O
the	O
actual	O
return	B
following	O
time	O
t	O
,	O
and	O
α	O
is	O
a	O
constant	O
step-size	B
parameter	I
(	O
c.f.	O
,	O
equation	O
2.4	O
)	O
.	O
let	O
us	O
call	O
this	O
method	O
constant-α	O
mc	O
.	O
whereas	O
monte	O
carlo	O
methods	O
must	O
wait	O
until	O
the	O
end	O
of	O
the	O
episode	O
to	O
determine	O
the	O
increment	O
to	O
v	O
(	O
st	O
)	O
(	O
only	O
then	O
is	O
gt	O
known	O
)	O
,	O
td	O
methods	O
need	O
to	O
wait	O
only	O
until	O
the	O
next	O
time	O
step	O
.	O
at	O
time	O
t	O
+	O
1	O
they	O
immediately	O
form	O
a	O
target	B
and	O
make	O
a	O
useful	O
update	O
using	O
the	O
observed	O
reward	O
rt+1	O
and	O
the	O
estimate	O
v	O
(	O
st+1	O
)	O
.	O
the	O
simplest	O
td	O
method	O
makes	O
the	O
update	O
v	O
(	O
st	O
)	O
←	O
v	O
(	O
st	O
)	O
+	O
α	O
(	O
cid:104	O
)	O
rt+1	O
+	O
γv	O
(	O
st+1	O
)	O
−	O
v	O
(	O
st	O
)	O
(	O
cid:105	O
)	O
(	O
6.2	O
)	O
immediately	O
on	O
transition	O
to	O
st+1	O
and	O
receiving	O
rt+1	O
.	O
in	O
eﬀect	O
,	O
the	O
target	B
for	O
the	O
monte	O
carlo	O
update	O
is	O
gt	O
,	O
whereas	O
the	O
target	B
for	O
the	O
td	O
update	O
is	O
rt+1	O
+	O
γv	O
(	O
st+1	O
)	O
.	O
this	O
td	O
method	O
is	O
called	O
td	O
(	O
0	O
)	O
,	O
or	O
one-step	O
td	O
,	O
because	O
it	O
is	O
a	O
special	O
case	O
of	O
the	O
td	O
(	O
λ	O
)	O
and	O
n-step	O
td	O
methods	O
developed	O
in	O
chapter	O
12	O
and	O
chapter	O
7.	O
the	O
box	O
below	O
speciﬁes	O
td	O
(	O
0	O
)	O
completely	O
in	O
procedural	O
form	O
.	O
tabular	O
td	O
(	O
0	O
)	O
for	O
estimating	O
vπ	O
input	O
:	O
the	O
policy	B
π	O
to	O
be	O
evaluated	O
algorithm	O
parameter	O
:	O
step	O
size	O
α	O
∈	O
(	O
0	O
,	O
1	O
]	O
initialize	O
v	O
(	O
s	O
)	O
,	O
for	O
all	O
s	O
∈	O
s+	O
,	O
arbitrarily	O
except	O
that	O
v	O
(	O
terminal	O
)	O
=	O
0	O
loop	O
for	O
each	O
episode	O
:	O
initialize	O
s	O
loop	O
for	O
each	O
step	O
of	O
episode	O
:	O
a	O
←	O
action	B
given	O
by	O
π	O
for	O
s	O
take	O
action	B
a	O
,	O
observe	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
v	O
(	O
s	O
)	O
←	O
v	O
(	O
s	O
)	O
+	O
α	O
(	O
cid:2	O
)	O
r	O
+	O
γv	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
−	O
v	O
(	O
s	O
)	O
(	O
cid:3	O
)	O
s	O
←	O
s	O
(	O
cid:48	O
)	O
until	O
s	O
is	O
terminal	O
because	O
td	O
(	O
0	O
)	O
bases	O
its	O
update	O
in	O
part	O
on	O
an	O
existing	O
estimate	O
,	O
we	O
say	O
that	O
it	O
is	O
a	O
bootstrapping	B
method	O
,	O
like	O
dp	O
.	O
we	O
know	O
from	O
chapter	O
3	O
that	O
vπ	O
(	O
s	O
)	O
.	O
=	O
eπ	O
[	O
gt	O
|	O
st	O
=	O
s	O
]	O
=	O
eπ	O
[	O
rt+1	O
+	O
γgt+1	O
|	O
st	O
=	O
s	O
]	O
=	O
eπ	O
[	O
rt+1	O
+	O
γvπ	O
(	O
st+1	O
)	O
|	O
st	O
=	O
s	O
]	O
.	O
(	O
6.3	O
)	O
(	O
from	O
(	O
3.9	O
)	O
)	O
(	O
6.4	O
)	O
roughly	O
speaking	O
,	O
monte	O
carlo	O
methods	O
use	O
an	O
estimate	O
of	O
(	O
6.3	O
)	O
as	O
a	O
target	B
,	O
whereas	O
dp	O
methods	O
use	O
an	O
estimate	O
of	O
(	O
6.4	O
)	O
as	O
a	O
target	B
.	O
the	O
monte	O
carlo	O
target	B
is	O
an	O
estimate	O
because	O
the	O
expected	O
value	O
in	O
(	O
6.3	O
)	O
is	O
not	O
known	O
;	O
a	O
sample	O
return	O
is	O
used	O
in	O
place	O
of	O
the	O
real	O
expected	O
return	O
.	O
the	O
dp	O
target	B
is	O
an	O
estimate	O
not	O
because	O
of	O
the	O
expected	O
values	O
,	O
which	O
are	O
assumed	O
to	O
be	O
completely	O
provided	O
by	O
a	O
model	B
of	I
the	I
environment	I
,	O
but	O
because	O
vπ	O
(	O
st+1	O
)	O
is	O
not	O
known	O
and	O
the	O
current	O
estimate	O
,	O
v	O
(	O
st+1	O
)	O
,	O
is	O
used	O
instead	O
.	O
the	O
td	O
target	B
is	O
an	O
estimate	O
for	O
both	O
reasons	O
:	O
it	O
samples	O
the	O
expected	O
values	O
in	O
(	O
6.4	O
)	O
and	O
it	O
uses	O
the	O
current	O
estimate	O
v	O
instead	O
of	O
the	O
true	O
vπ	O
.	O
thus	O
,	O
td	O
methods	O
combine	O
6.1.	O
td	O
prediction	B
121	O
the	O
sampling	O
of	O
monte	O
carlo	O
with	O
the	O
bootstrapping	B
of	O
dp	O
.	O
as	O
we	O
shall	O
see	O
,	O
with	O
care	O
and	O
imagination	O
this	O
can	O
take	O
us	O
a	O
long	O
way	O
toward	O
obtaining	O
the	O
advantages	B
of	I
both	O
monte	O
carlo	O
and	O
dp	O
methods	O
.	O
shown	O
to	O
the	O
right	O
is	O
the	O
backup	B
diagram	I
for	O
tabular	O
td	O
(	O
0	O
)	O
.	O
the	O
value	B
estimate	O
for	O
the	O
state	B
node	O
at	O
the	O
top	O
of	O
the	O
backup	B
diagram	I
is	O
updated	O
on	O
the	O
basis	O
of	O
the	O
one	O
sample	O
transition	O
from	O
it	O
to	O
the	O
immediately	O
following	O
state	B
.	O
we	O
refer	O
to	O
td	O
and	O
monte	O
carlo	O
updates	O
as	O
sample	O
updates	O
because	O
they	O
involve	O
looking	O
ahead	O
to	O
a	O
sample	O
successor	O
state	B
(	O
or	O
state–action	O
pair	O
)	O
,	O
using	O
the	O
value	B
of	O
the	O
successor	O
and	O
the	O
reward	O
along	O
the	O
way	O
to	O
compute	O
a	O
backed-up	O
value	B
,	O
and	O
then	O
updating	O
the	O
value	B
of	O
the	O
original	O
state	B
(	O
or	O
state–	O
action	B
pair	O
)	O
accordingly	O
.	O
sample	O
updates	O
diﬀer	O
from	O
the	O
expected	O
updates	O
of	O
dp	O
methods	O
in	O
that	O
they	O
are	O
based	O
on	O
a	O
single	O
sample	O
successor	O
rather	O
than	O
on	O
a	O
complete	O
distribution	O
of	O
all	O
possible	O
successors	O
.	O
td	O
(	O
0	O
)	O
finally	O
,	O
note	O
that	O
the	O
quantity	O
in	O
brackets	O
in	O
the	O
td	O
(	O
0	O
)	O
update	O
is	O
a	O
sort	O
of	O
error	O
,	O
measuring	O
the	O
diﬀerence	O
between	O
the	O
estimated	O
value	B
of	O
st	O
and	O
the	O
better	O
estimate	O
rt+1	O
+	O
γv	O
(	O
st+1	O
)	O
.	O
this	O
quantity	O
,	O
called	O
the	O
td	O
error	O
,	O
arises	O
in	O
various	O
forms	O
throughout	O
reinforcement	B
learning	I
:	O
δt	O
.	O
=	O
rt+1	O
+	O
γv	O
(	O
st+1	O
)	O
−	O
v	O
(	O
st	O
)	O
.	O
(	O
6.5	O
)	O
notice	O
that	O
the	O
td	O
error	O
at	O
each	O
time	O
is	O
the	O
error	O
in	O
the	O
estimate	O
made	O
at	O
that	O
time	O
.	O
because	O
the	O
td	O
error	O
depends	O
on	O
the	O
next	O
state	B
and	O
next	O
reward	O
,	O
it	O
is	O
not	O
actually	O
available	O
until	O
one	O
time	O
step	O
later	O
.	O
that	O
is	O
,	O
δt	O
is	O
the	O
error	O
in	O
v	O
(	O
st	O
)	O
,	O
available	O
at	O
time	O
t	O
+	O
1.	O
also	O
note	O
that	O
if	O
the	O
array	O
v	O
does	O
not	O
change	O
during	O
the	O
episode	O
(	O
as	O
it	O
does	O
not	O
in	O
monte	O
carlo	O
methods	O
)	O
,	O
then	O
the	O
monte	O
carlo	O
error	O
can	O
be	O
written	O
as	O
a	O
sum	O
of	O
td	O
errors	O
:	O
gt	O
−	O
v	O
(	O
st	O
)	O
=	O
rt+1	O
+	O
γgt+1	O
−	O
v	O
(	O
st	O
)	O
+	O
γv	O
(	O
st+1	O
)	O
−	O
γv	O
(	O
st+1	O
)	O
(	O
from	O
(	O
3.9	O
)	O
)	O
=	O
δt	O
+	O
γ	O
(	O
cid:0	O
)	O
gt+1	O
−	O
v	O
(	O
st+1	O
)	O
(	O
cid:1	O
)	O
=	O
δt	O
+	O
γδt+1	O
+	O
γ2	O
(	O
cid:0	O
)	O
gt+2	O
−	O
v	O
(	O
st+2	O
)	O
(	O
cid:1	O
)	O
=	O
δt	O
+	O
γδt+1	O
+	O
γ2δt+2	O
+	O
···	O
+	O
γt−t−1δt−1	O
+	O
γt−t	O
(	O
cid:0	O
)	O
gt	O
−	O
v	O
(	O
st	O
)	O
(	O
cid:1	O
)	O
=	O
δt	O
+	O
γδt+1	O
+	O
γ2δt+2	O
+	O
···	O
+	O
γt−t−1δt−1	O
+	O
γt−t	O
(	O
cid:0	O
)	O
0	O
−	O
0	O
(	O
cid:1	O
)	O
t−1	O
(	O
cid:88	O
)	O
k=t	O
γk−tδk	O
.	O
=	O
(	O
6.6	O
)	O
this	O
identity	O
is	O
not	O
exact	O
if	O
v	O
is	O
updated	O
during	O
the	O
episode	O
(	O
as	O
it	O
is	O
in	O
td	O
(	O
0	O
)	O
)	O
,	O
but	O
if	O
the	O
step	O
size	O
is	O
small	O
then	O
it	O
may	O
still	O
hold	O
approximately	O
.	O
generalizations	O
of	O
this	O
identity	O
play	O
an	O
important	O
role	O
in	O
the	O
theory	O
and	O
algorithms	O
of	O
temporal-diﬀerence	O
learning	O
.	O
exercise	O
6.1	O
if	O
v	O
changes	O
during	O
the	O
episode	O
,	O
then	O
(	O
6.6	O
)	O
only	O
holds	O
approximately	O
;	O
what	O
would	O
the	O
diﬀerence	O
be	O
between	O
the	O
two	O
sides	O
?	O
let	O
vt	O
denote	O
the	O
array	O
of	O
state	O
values	O
used	O
at	O
time	O
t	O
in	O
the	O
td	O
error	O
(	O
6.5	O
)	O
and	O
in	O
the	O
td	O
update	O
(	O
6.2	O
)	O
.	O
redo	O
the	O
derivation	O
above	O
to	O
determine	O
the	O
additional	O
amount	O
that	O
must	O
be	O
added	O
to	O
the	O
sum	O
of	O
td	O
errors	O
(	O
cid:3	O
)	O
in	O
order	O
to	O
equal	O
the	O
monte	O
carlo	O
error	O
.	O
122	O
chapter	O
6	O
:	O
temporal-diﬀerence	B
learning	I
example	O
6.1	O
:	O
driving	O
home	O
each	O
day	O
as	O
you	O
drive	O
home	O
from	O
work	O
,	O
you	O
try	O
to	O
predict	O
how	O
long	O
it	O
will	O
take	O
to	O
get	O
home	O
.	O
when	O
you	O
leave	O
your	O
oﬃce	O
,	O
you	O
note	O
the	O
time	O
,	O
the	O
day	O
of	O
week	O
,	O
the	O
weather	O
,	O
and	O
anything	O
else	O
that	O
might	O
be	O
relevant	O
.	O
say	O
on	O
this	O
friday	O
you	O
are	O
leaving	O
at	O
exactly	O
6	O
o	O
’	O
clock	O
,	O
and	O
you	O
estimate	O
that	O
it	O
will	O
take	O
30	O
minutes	O
to	O
get	O
home	O
.	O
as	O
you	O
reach	O
your	O
car	O
it	O
is	O
6:05	O
,	O
and	O
you	O
notice	O
it	O
is	O
starting	O
to	O
rain	O
.	O
traﬃc	O
is	O
often	O
slower	O
in	O
the	O
rain	O
,	O
so	O
you	O
reestimate	O
that	O
it	O
will	O
take	O
35	O
minutes	O
from	O
then	O
,	O
or	O
a	O
total	O
of	O
40	O
minutes	O
.	O
fifteen	O
minutes	O
later	O
you	O
have	O
completed	O
the	O
highway	O
portion	O
of	O
your	O
journey	O
in	O
good	O
time	O
.	O
as	O
you	O
exit	O
onto	O
a	O
secondary	O
road	O
you	O
cut	O
your	O
estimate	O
of	O
total	O
travel	O
time	O
to	O
35	O
minutes	O
.	O
unfortunately	O
,	O
at	O
this	O
point	O
you	O
get	O
stuck	O
behind	O
a	O
slow	O
truck	O
,	O
and	O
the	O
road	O
is	O
too	O
narrow	O
to	O
pass	O
.	O
you	O
end	O
up	O
having	O
to	O
follow	O
the	O
truck	O
until	O
you	O
turn	O
onto	O
the	O
side	O
street	O
where	O
you	O
live	O
at	O
6:40.	O
three	O
minutes	O
later	O
you	O
are	O
home	O
.	O
the	O
sequence	O
of	O
states	O
,	O
times	O
,	O
and	O
predictions	O
is	O
thus	O
as	O
follows	O
:	O
state	B
leaving	O
oﬃce	O
,	O
friday	O
at	O
6	O
reach	O
car	O
,	O
raining	O
exiting	O
highway	O
2ndary	O
road	O
,	O
behind	O
truck	O
entering	O
home	O
street	O
arrive	O
home	O
elapsed	O
time	O
(	O
minutes	O
)	O
predicted	O
predicted	O
time	O
to	O
go	O
total	O
time	O
0	O
5	O
20	O
30	O
40	O
43	O
30	O
35	O
15	O
10	O
3	O
0	O
30	O
40	O
35	O
40	O
43	O
43	O
the	O
rewards	O
in	O
this	O
example	O
are	O
the	O
elapsed	O
times	O
on	O
each	O
leg	O
of	O
the	O
journey.1	O
we	O
are	O
not	O
discounting	B
(	O
γ	O
=	O
1	O
)	O
,	O
and	O
thus	O
the	O
return	B
for	O
each	O
state	B
is	O
the	O
actual	O
time	O
to	O
go	O
from	O
that	O
state	B
.	O
the	O
value	B
of	O
each	O
state	B
is	O
the	O
expected	O
time	O
to	O
go	O
.	O
the	O
second	O
column	O
of	O
numbers	O
gives	O
the	O
current	O
estimated	O
value	B
for	O
each	O
state	B
encountered	O
.	O
a	O
simple	O
way	O
to	O
view	O
the	O
operation	O
of	O
monte	O
carlo	O
methods	O
is	O
to	O
plot	O
the	O
predicted	O
total	O
time	O
(	O
the	O
last	O
column	O
)	O
over	O
the	O
sequence	O
,	O
as	O
in	O
figure	O
6.1	O
(	O
left	O
)	O
.	O
the	O
red	O
arrows	O
show	O
the	O
changes	O
in	O
predictions	O
recommended	O
by	O
the	O
constant-α	O
mc	O
method	O
(	O
6.1	O
)	O
,	O
for	O
α	O
=	O
1.	O
these	O
are	O
exactly	O
the	O
errors	O
between	O
the	O
estimated	O
value	B
(	O
predicted	O
time	O
to	O
go	O
)	O
in	O
each	O
state	B
and	O
the	O
actual	O
return	B
(	O
actual	O
time	O
to	O
go	O
)	O
.	O
for	O
example	O
,	O
when	O
you	O
exited	O
the	O
highway	O
you	O
thought	O
it	O
would	O
take	O
only	O
15	O
minutes	O
more	O
to	O
get	O
home	O
,	O
but	O
in	O
fact	O
it	O
took	O
23	O
minutes	O
.	O
equation	O
6.1	O
applies	O
at	O
this	O
point	O
and	O
determines	O
an	O
increment	O
in	O
the	O
estimate	O
of	O
time	O
to	O
go	O
after	O
exiting	O
the	O
highway	O
.	O
the	O
error	O
,	O
gt	O
−	O
v	O
(	O
st	O
)	O
,	O
at	O
this	O
time	O
is	O
eight	O
minutes	O
.	O
suppose	O
the	O
step-size	B
parameter	I
,	O
α	O
,	O
is	O
1/2	O
.	O
then	O
the	O
predicted	O
time	O
to	O
go	O
after	O
exiting	O
the	O
highway	O
would	O
be	O
revised	O
upward	O
by	O
four	O
minutes	O
as	O
a	O
result	O
of	O
this	O
experience	O
.	O
this	O
is	O
probably	O
too	O
large	O
a	O
change	O
in	O
this	O
case	O
;	O
the	O
truck	O
was	O
probably	O
just	O
an	O
unlucky	O
break	O
.	O
in	O
any	O
event	O
,	O
the	O
change	O
can	O
only	O
be	O
made	O
oﬀ-line	B
,	O
that	O
is	O
,	O
after	O
you	O
have	O
reached	O
home	O
.	O
only	O
at	O
this	O
point	O
do	O
you	O
know	O
any	O
of	O
the	O
actual	O
returns	O
.	O
is	O
it	O
necessary	O
to	O
wait	O
until	O
the	O
ﬁnal	O
outcome	O
is	O
known	O
before	O
learning	O
can	O
begin	O
?	O
suppose	O
on	O
another	O
day	O
you	O
again	O
estimate	O
when	O
leaving	O
your	O
oﬃce	O
that	O
it	O
will	O
take	O
30	O
minutes	O
to	O
drive	O
home	O
,	O
but	O
then	O
you	O
become	O
stuck	O
in	O
a	O
massive	O
traﬃc	O
jam	O
.	O
twenty-ﬁve	O
minutes	O
after	O
leaving	O
the	O
oﬃce	O
you	O
are	O
still	O
bumper-to-bumper	O
on	O
the	O
highway	O
.	O
you	O
now	O
1if	O
this	O
were	O
a	O
control	B
problem	O
with	O
the	O
objective	O
of	O
minimizing	O
travel	O
time	O
,	O
then	O
we	O
would	O
of	O
course	O
make	O
the	O
rewards	O
the	O
negative	O
of	O
the	O
elapsed	O
time	O
.	O
but	O
because	O
we	O
are	O
concerned	O
here	O
only	O
with	O
prediction	O
(	O
policy	B
evaluation	I
)	O
,	O
we	O
can	O
keep	O
things	O
simple	O
by	O
using	O
positive	O
numbers	O
.	O
6.2.	O
advantages	B
of	I
td	O
prediction	B
methods	O
123	O
figure	O
6.1	O
:	O
changes	O
recommended	O
in	O
the	O
driving	O
home	O
example	O
by	O
monte	O
carlo	O
methods	O
(	O
left	O
)	O
and	O
td	O
methods	O
(	O
right	O
)	O
.	O
estimate	O
that	O
it	O
will	O
take	O
another	O
25	O
minutes	O
to	O
get	O
home	O
,	O
for	O
a	O
total	O
of	O
50	O
minutes	O
.	O
as	O
you	O
wait	O
in	O
traﬃc	O
,	O
you	O
already	O
know	O
that	O
your	O
initial	O
estimate	O
of	O
30	O
minutes	O
was	O
too	O
optimistic	O
.	O
must	O
you	O
wait	O
until	O
you	O
get	O
home	O
before	O
increasing	O
your	O
estimate	O
for	O
the	O
initial	O
state	B
?	O
according	O
to	O
the	O
monte	O
carlo	O
approach	O
you	O
must	O
,	O
because	O
you	O
don	O
’	O
t	O
yet	O
know	O
the	O
true	O
return	O
.	O
according	O
to	O
a	O
td	O
approach	O
,	O
on	O
the	O
other	O
hand	O
,	O
you	O
would	O
learn	O
immediately	O
,	O
shifting	O
your	O
initial	O
estimate	O
from	O
30	O
minutes	O
toward	O
50.	O
in	O
fact	O
,	O
each	O
estimate	O
would	O
be	O
shifted	O
toward	O
the	O
estimate	O
that	O
immediately	O
follows	O
it	O
.	O
returning	O
to	O
our	O
ﬁrst	O
day	O
of	O
driving	O
,	O
figure	O
6.1	O
(	O
right	O
)	O
shows	O
the	O
changes	O
in	O
the	O
predictions	O
recommended	O
by	O
the	O
td	O
rule	O
(	O
6.2	O
)	O
(	O
these	O
are	O
the	O
changes	O
made	O
by	O
the	O
rule	O
if	O
α	O
=	O
1	O
)	O
.	O
each	O
error	O
is	O
proportional	O
to	O
the	O
change	O
over	O
time	O
of	O
the	O
prediction	B
,	O
that	O
is	O
,	O
to	O
the	O
temporal	O
diﬀerences	O
in	O
predictions	O
.	O
besides	O
giving	O
you	O
something	O
to	O
do	O
while	O
waiting	O
in	O
traﬃc	O
,	O
there	O
are	O
several	O
compu-	O
tational	O
reasons	O
why	O
it	O
is	O
advantageous	O
to	O
learn	O
based	O
on	O
your	O
current	O
predictions	O
rather	O
than	O
waiting	O
until	O
termination	O
when	O
you	O
know	O
the	O
actual	O
return	B
.	O
we	O
brieﬂy	O
discuss	O
some	O
of	O
these	O
in	O
the	O
next	O
section	O
.	O
exercise	O
6.2	O
this	O
is	O
an	O
exercise	O
to	O
help	O
develop	O
your	O
intuition	O
about	O
why	O
td	O
methods	O
are	O
often	O
more	O
eﬃcient	O
than	O
monte	O
carlo	O
methods	O
.	O
consider	O
the	O
driving	O
home	O
example	O
and	O
how	O
it	O
is	O
addressed	O
by	O
td	O
and	O
monte	O
carlo	O
methods	O
.	O
can	O
you	O
imagine	O
a	O
scenario	O
in	O
which	O
a	O
td	O
update	O
would	O
be	O
better	O
on	O
average	O
than	O
a	O
monte	O
carlo	O
update	O
?	O
give	O
an	O
example	O
scenario—a	O
description	O
of	O
past	O
experience	O
and	O
a	O
current	O
state—in	O
which	O
you	O
would	O
expect	O
the	O
td	O
update	O
to	O
be	O
better	O
.	O
here	O
’	O
s	O
a	O
hint	O
:	O
suppose	O
you	O
have	O
lots	O
of	O
experience	O
driving	O
home	O
from	O
work	O
.	O
then	O
you	O
move	O
to	O
a	O
new	O
building	O
and	O
a	O
new	O
parking	O
lot	O
(	O
but	O
you	O
still	O
enter	O
the	O
highway	O
at	O
the	O
same	O
place	O
)	O
.	O
now	O
you	O
are	O
starting	O
to	O
learn	O
predictions	O
for	O
the	O
new	O
building	O
.	O
can	O
you	O
see	O
why	O
td	O
updates	O
are	O
likely	O
to	O
be	O
much	O
better	O
,	O
at	O
least	O
initially	O
,	O
in	O
this	O
case	O
?	O
might	O
the	O
same	O
sort	O
of	O
thing	O
happen	O
in	O
the	O
(	O
cid:3	O
)	O
original	O
task	O
?	O
road30354045predictedtotaltraveltimeleavingofficeexitinghighway2ndaryhomearrivesituationactual	O
outcomereachcarstreethomeactualoutcomesituation30354045predictedtotaltraveltimeroadleavingofficeexitinghighway2ndaryhomearrivereachcarstreethome	O
124	O
chapter	O
6	O
:	O
temporal-diﬀerence	B
learning	I
6.2	O
advantages	B
of	I
td	O
prediction	B
methods	O
td	O
methods	O
update	O
their	O
estimates	O
based	O
in	O
part	O
on	O
other	O
estimates	O
.	O
they	O
learn	O
a	O
guess	O
from	O
a	O
guess—they	O
bootstrap	O
.	O
is	O
this	O
a	O
good	O
thing	O
to	O
do	O
?	O
what	O
advantages	O
do	O
td	O
methods	O
have	O
over	O
monte	O
carlo	O
and	O
dp	O
methods	O
?	O
developing	O
and	O
answering	O
such	O
questions	O
will	O
take	O
the	O
rest	O
of	O
this	O
book	O
and	O
more	O
.	O
in	O
this	O
section	O
we	O
brieﬂy	O
anticipate	O
some	O
of	O
the	O
answers	O
.	O
obviously	O
,	O
td	O
methods	O
have	O
an	O
advantage	O
over	O
dp	O
methods	O
in	O
that	O
they	O
do	O
not	O
require	O
a	O
model	B
of	I
the	I
environment	I
,	O
of	O
its	O
reward	O
and	O
next-state	O
probability	O
distributions	O
.	O
the	O
next	O
most	O
obvious	O
advantage	O
of	O
td	O
methods	O
over	O
monte	O
carlo	O
methods	O
is	O
that	O
they	O
are	O
naturally	O
implemented	O
in	O
an	O
online	B
,	O
fully	O
incremental	O
fashion	O
.	O
with	O
monte	O
carlo	O
methods	O
one	O
must	O
wait	O
until	O
the	O
end	O
of	O
an	O
episode	O
,	O
because	O
only	O
then	O
is	O
the	O
return	B
known	O
,	O
whereas	O
with	O
td	O
methods	O
one	O
need	O
wait	O
only	O
one	O
time	O
step	O
.	O
surprisingly	O
often	O
this	O
turns	O
out	O
to	O
be	O
a	O
critical	O
consideration	O
.	O
some	O
applications	O
have	O
very	O
long	O
episodes	B
,	O
so	O
that	O
delaying	O
all	O
learning	O
until	O
the	O
end	O
of	O
the	O
episode	O
is	O
too	O
slow	O
.	O
other	O
applications	O
are	O
continuing	B
tasks	I
and	O
have	O
no	O
episodes	B
at	O
all	O
.	O
finally	O
,	O
as	O
we	O
noted	O
in	O
the	O
previous	O
chapter	O
,	O
some	O
monte	O
carlo	O
methods	O
must	O
ignore	O
or	O
discount	O
episodes	B
on	O
which	O
experimental	O
actions	O
are	O
taken	O
,	O
which	O
can	O
greatly	O
slow	O
learning	O
.	O
td	O
methods	O
are	O
much	O
less	O
susceptible	O
to	O
these	O
problems	O
because	O
they	O
learn	O
from	O
each	O
transition	O
regardless	O
of	O
what	O
subsequent	O
actions	O
are	O
taken	O
.	O
but	O
are	O
td	O
methods	O
sound	O
?	O
certainly	O
it	O
is	O
convenient	O
to	O
learn	O
one	O
guess	O
from	O
the	O
next	O
,	O
without	O
waiting	O
for	O
an	O
actual	O
outcome	O
,	O
but	O
can	O
we	O
still	O
guarantee	O
convergence	O
to	O
the	O
correct	O
answer	O
?	O
happily	O
,	O
the	O
answer	O
is	O
yes	O
.	O
for	O
any	O
ﬁxed	O
policy	B
π	O
,	O
td	O
(	O
0	O
)	O
has	O
been	O
proved	O
to	O
converge	O
to	O
vπ	O
,	O
in	O
the	O
mean	O
for	O
a	O
constant	O
step-size	B
parameter	I
if	O
it	O
is	O
suﬃciently	O
small	O
,	O
and	O
with	O
probability	O
1	O
if	O
the	O
step-size	B
parameter	I
decreases	O
according	O
to	O
the	O
usual	O
stochastic	O
approximation	O
conditions	O
(	O
2.7	O
)	O
.	O
most	O
convergence	O
proofs	O
apply	O
only	O
to	O
the	O
table-based	O
case	O
of	O
the	O
algorithm	O
presented	O
above	O
(	O
6.2	O
)	O
,	O
but	O
some	O
also	O
apply	O
to	O
the	O
case	O
of	O
general	O
linear	B
function	I
approximation	I
.	O
these	O
results	O
are	O
discussed	O
in	O
a	O
more	O
general	O
setting	O
in	O
chapter	O
9.	O
if	O
both	O
td	O
and	O
monte	O
carlo	O
methods	O
converge	O
asymptotically	O
to	O
the	O
correct	O
predic-	O
tions	O
,	O
then	O
a	O
natural	O
next	O
question	O
is	O
“	O
which	O
gets	O
there	O
ﬁrst	O
?	O
”	O
in	O
other	O
words	O
,	O
which	O
method	O
learns	O
faster	O
?	O
which	O
makes	O
the	O
more	O
eﬃcient	O
use	O
of	O
limited	O
data	O
?	O
at	O
the	O
current	O
time	O
this	O
is	O
an	O
open	O
question	O
in	O
the	O
sense	O
that	O
no	O
one	O
has	O
been	O
able	O
to	O
prove	O
mathe-	O
matically	O
that	O
one	O
method	O
converges	O
faster	O
than	O
the	O
other	O
.	O
in	O
fact	O
,	O
it	O
is	O
not	O
even	O
clear	O
what	O
is	O
the	O
most	O
appropriate	O
formal	O
way	O
to	O
phrase	O
this	O
question	O
!	O
in	O
practice	O
,	O
however	O
,	O
td	O
methods	O
have	O
usually	O
been	O
found	O
to	O
converge	O
faster	O
than	O
constant-α	O
mc	O
methods	O
on	O
stochastic	O
tasks	O
,	O
as	O
illustrated	O
in	O
example	O
6.2	O
.	O
6.2.	O
advantages	B
of	I
td	O
prediction	B
methods	O
125	O
example	O
6.2	O
random	B
walk	I
in	O
this	O
example	O
we	O
empirically	O
compare	O
the	O
prediction	B
abilities	O
of	O
td	O
(	O
0	O
)	O
and	O
constant-α	O
mc	O
when	O
applied	O
to	O
the	O
following	O
markov	O
reward	O
process	O
:	O
a	O
markov	O
reward	O
process	O
,	O
or	O
mrp	O
,	O
is	O
a	O
markov	O
decision	O
process	O
without	O
actions	O
.	O
we	O
will	O
often	O
use	O
mrps	O
when	O
focusing	O
on	O
the	O
prediction	B
problem	O
,	O
in	O
which	O
there	O
is	O
no	O
need	O
to	O
distinguish	O
the	O
dynamics	O
due	O
to	O
the	O
environment	B
from	O
those	O
due	O
to	O
the	O
agent	O
.	O
in	O
this	O
mrp	O
,	O
all	O
episodes	B
start	O
in	O
the	O
center	O
state	B
,	O
c	O
,	O
then	O
proceed	O
either	O
left	O
or	O
right	O
by	O
one	O
state	B
on	O
each	O
step	O
,	O
with	O
equal	O
probability	O
.	O
episodes	B
terminate	O
either	O
on	O
the	O
extreme	O
left	O
or	O
the	O
extreme	O
right	O
.	O
when	O
an	O
episode	O
terminates	O
on	O
the	O
right	O
,	O
a	O
reward	O
of	O
+1	O
occurs	O
;	O
all	O
other	O
rewards	O
are	O
zero	O
.	O
for	O
example	O
,	O
a	O
typical	O
episode	O
might	O
consist	O
of	O
the	O
following	O
state-and-reward	O
sequence	O
:	O
c	O
,	O
0	O
,	O
b	O
,	O
0	O
,	O
c	O
,	O
0	O
,	O
d	O
,	O
0	O
,	O
e	O
,	O
1.	O
because	O
this	O
task	O
is	O
undiscounted	O
,	O
the	O
true	O
value	O
of	O
each	O
state	B
is	O
the	O
probability	O
of	O
terminating	O
on	O
the	O
right	O
if	O
starting	O
from	O
that	O
state	B
.	O
thus	O
,	O
the	O
true	O
value	O
of	O
the	O
center	O
state	B
is	O
vπ	O
(	O
c	O
)	O
=	O
0.5.	O
the	O
true	O
values	O
of	O
all	O
the	O
states	O
,	O
a	O
through	O
e	O
,	O
are	O
1	O
6	O
,	O
2	O
6	O
,	O
and	O
5	O
6	O
.	O
6	O
,	O
3	O
6	O
,	O
4	O
the	O
left	O
graph	O
above	O
shows	O
the	O
values	O
learned	O
after	O
various	O
numbers	O
of	O
episodes	O
on	O
a	O
single	O
run	O
of	O
td	O
(	O
0	O
)	O
.	O
the	O
estimates	O
after	O
100	O
episodes	B
are	O
about	O
as	O
close	O
as	O
they	O
ever	O
come	O
to	O
the	O
true	O
values—with	O
a	O
constant	O
step-size	B
parameter	I
(	O
α	O
=	O
0.1	O
in	O
this	O
example	O
)	O
,	O
the	O
values	O
ﬂuctuate	O
indeﬁnitely	O
in	O
response	O
to	O
the	O
outcomes	O
of	O
the	O
most	O
recent	O
episodes	B
.	O
the	O
right	O
graph	O
shows	O
learning	O
curves	O
for	O
the	O
two	O
methods	O
for	O
various	O
values	O
of	O
α.	O
the	O
performance	O
measure	O
shown	O
is	O
the	O
root	O
mean-squared	O
(	O
rms	O
)	O
error	O
between	O
the	O
value	B
function	I
learned	O
and	O
the	O
true	O
value	B
function	I
,	O
averaged	O
over	O
the	O
ﬁve	O
states	O
,	O
then	O
averaged	O
over	O
100	O
runs	O
.	O
in	O
all	O
cases	O
the	O
approximate	B
value	O
function	O
was	O
initialized	O
to	O
the	O
intermediate	O
value	B
v	O
(	O
s	O
)	O
=	O
0.5	O
,	O
for	O
all	O
s.	O
the	O
td	O
method	O
was	O
consistently	O
better	O
than	O
the	O
mc	O
method	O
on	O
this	O
task	O
.	O
abcde100000start0.800.20.40.6abcde0101100stateestimatedvaluetrue	O
valuesestimatedvalue00.050.10.150.20.250255075100walks	O
/	O
episodestdmcrms	O
error	O
,	O
averagedover	O
statesα=.01α=.1α=.02α=.03α=.04α=.15α=.05empirical	O
rms	O
error	O
,	O
averaged	O
over	O
states	O
126	O
chapter	O
6	O
:	O
temporal-diﬀerence	B
learning	I
exercise	O
6.3	O
from	O
the	O
results	O
shown	O
in	O
the	O
left	O
graph	O
of	O
the	O
random	B
walk	I
example	O
it	O
appears	O
that	O
the	O
ﬁrst	O
episode	O
results	O
in	O
a	O
change	O
in	O
only	O
v	O
(	O
a	O
)	O
.	O
what	O
does	O
this	O
tell	O
you	O
about	O
what	O
happened	O
on	O
the	O
ﬁrst	O
episode	O
?	O
why	O
was	O
only	O
the	O
estimate	O
for	O
this	O
one	O
state	B
(	O
cid:3	O
)	O
changed	O
?	O
by	O
exactly	O
how	O
much	O
was	O
it	O
changed	O
?	O
exercise	O
6.4	O
the	O
speciﬁc	O
results	O
shown	O
in	O
the	O
right	O
graph	O
of	O
the	O
random	B
walk	I
example	O
are	O
dependent	O
on	O
the	O
value	B
of	O
the	O
step-size	B
parameter	I
,	O
α.	O
do	O
you	O
think	O
the	O
conclusions	O
about	O
which	O
algorithm	O
is	O
better	O
would	O
be	O
aﬀected	O
if	O
a	O
wider	O
range	O
of	O
α	O
values	O
were	O
used	O
?	O
is	O
there	O
a	O
diﬀerent	O
,	O
ﬁxed	O
value	B
of	O
α	O
at	O
which	O
either	O
algorithm	O
would	O
have	O
performed	O
(	O
cid:3	O
)	O
signiﬁcantly	O
better	O
than	O
shown	O
?	O
why	O
or	O
why	O
not	O
?	O
∗exercise	O
6.5	O
in	O
the	O
right	O
graph	O
of	O
the	O
random	B
walk	I
example	O
,	O
the	O
rms	O
error	O
of	O
the	O
td	O
method	O
seems	O
to	O
go	O
down	O
and	O
then	O
up	O
again	O
,	O
particularly	O
at	O
high	O
α	O
’	O
s	O
.	O
what	O
could	O
have	O
caused	O
this	O
?	O
do	O
you	O
think	O
this	O
always	O
occurs	O
,	O
or	O
might	O
it	O
be	O
a	O
function	O
of	O
how	O
the	O
(	O
cid:3	O
)	O
approximate	B
value	O
function	O
was	O
initialized	O
?	O
exercise	O
6.6	O
in	O
example	O
6.2	O
we	O
stated	O
that	O
the	O
true	O
values	O
for	O
the	O
random	B
walk	I
example	O
are	O
1	O
6	O
,	O
for	O
states	O
a	O
through	O
e.	O
describe	O
at	O
least	O
two	O
diﬀerent	O
ways	O
that	O
these	O
could	O
have	O
been	O
computed	O
.	O
which	O
would	O
you	O
guess	O
we	O
actually	O
used	O
?	O
why	O
?	O
(	O
cid:3	O
)	O
6	O
,	O
and	O
5	O
6	O
,	O
4	O
6	O
,	O
2	O
6	O
,	O
3	O
6.3	O
optimality	B
of	I
td	O
(	O
0	O
)	O
suppose	O
there	O
is	O
available	O
only	O
a	O
ﬁnite	O
amount	O
of	O
experience	O
,	O
say	O
10	O
episodes	B
or	O
100	O
time	O
steps	O
.	O
in	O
this	O
case	O
,	O
a	O
common	O
approach	O
with	O
incremental	O
learning	O
methods	O
is	O
to	O
present	O
the	O
experience	O
repeatedly	O
until	O
the	O
method	O
converges	O
upon	O
an	O
answer	O
.	O
given	O
an	O
approximate	B
value	O
function	O
,	O
v	O
,	O
the	O
increments	O
speciﬁed	O
by	O
(	O
6.1	O
)	O
or	O
(	O
6.2	O
)	O
are	O
computed	O
for	O
every	O
time	O
step	O
t	O
at	O
which	O
a	O
nonterminal	O
state	B
is	O
visited	O
,	O
but	O
the	O
value	B
function	I
is	O
changed	O
only	O
once	O
,	O
by	O
the	O
sum	O
of	O
all	O
the	O
increments	O
.	O
then	O
all	O
the	O
available	O
experience	O
is	O
processed	O
again	O
with	O
the	O
new	O
value	B
function	I
to	O
produce	O
a	O
new	O
overall	O
increment	O
,	O
and	O
so	O
on	O
,	O
until	O
the	O
value	B
function	I
converges	O
.	O
we	O
call	O
this	O
batch	O
updating	O
because	O
updates	O
are	O
made	O
only	O
after	O
processing	O
each	O
complete	O
batch	O
of	O
training	O
data	O
.	O
under	O
batch	O
updating	O
,	O
td	O
(	O
0	O
)	O
converges	O
deterministically	O
to	O
a	O
single	O
answer	O
indepen-	O
dent	O
of	O
the	O
step-size	B
parameter	I
,	O
α	O
,	O
as	O
long	O
as	O
α	O
is	O
chosen	O
to	O
be	O
suﬃciently	O
small	O
.	O
the	O
constant-α	O
mc	O
method	O
also	O
converges	O
deterministically	O
under	O
the	O
same	O
conditions	O
,	O
but	O
to	O
a	O
diﬀerent	O
answer	O
.	O
understanding	O
these	O
two	O
answers	O
will	O
help	O
us	O
understand	O
the	O
diﬀerence	O
between	O
the	O
two	O
methods	O
.	O
under	O
normal	O
updating	O
the	O
methods	O
do	O
not	O
move	O
all	O
the	O
way	O
to	O
their	O
respective	O
batch	O
answers	O
,	O
but	O
in	O
some	O
sense	O
they	O
take	O
steps	O
in	O
these	O
directions	O
.	O
before	O
trying	O
to	O
understand	O
the	O
two	O
answers	O
in	O
general	O
,	O
for	O
all	O
possible	O
tasks	O
,	O
we	O
ﬁrst	O
look	O
at	O
a	O
few	O
examples	O
.	O
example	O
6.3	O
:	O
random	B
walk	I
under	O
batch	O
updating	O
batch-updating	O
versions	O
of	O
td	O
(	O
0	O
)	O
and	O
constant-α	O
mc	O
were	O
applied	O
as	O
follows	O
to	O
the	O
random	B
walk	I
prediction	O
example	O
(	O
example	O
6.2	O
)	O
.	O
after	O
each	O
new	O
episode	O
,	O
all	O
episodes	B
seen	O
so	O
far	O
were	O
treated	O
as	O
a	O
batch	O
.	O
they	O
were	O
repeatedly	O
presented	O
to	O
the	O
algorithm	O
,	O
either	O
td	O
(	O
0	O
)	O
or	O
constant-α	O
mc	O
,	O
with	O
α	O
suﬃciently	O
small	O
that	O
the	O
value	B
function	I
converged	O
.	O
the	O
resulting	O
value	B
function	I
was	O
then	O
compared	O
with	O
vπ	O
,	O
and	O
the	O
average	O
root	O
mean-squared	O
error	O
across	O
the	O
ﬁve	O
states	O
(	O
and	O
across	O
100	O
independent	O
repetitions	O
of	O
the	O
whole	O
experiment	O
)	O
was	O
plotted	O
to	O
obtain	O
6.3.	O
optimality	B
of	I
td	O
(	O
0	O
)	O
127	O
the	O
learning	O
curves	O
shown	O
in	O
figure	O
6.2.	O
note	O
that	O
the	O
batch	O
td	O
method	O
was	O
consistently	O
better	O
than	O
the	O
batch	O
monte	O
carlo	O
method	O
.	O
under	O
batch	O
training	O
,	O
constant-α	O
mc	O
converges	O
to	O
values	O
,	O
v	O
(	O
s	O
)	O
,	O
that	O
are	O
sample	O
averages	O
of	O
the	O
actual	O
re-	O
turns	O
experienced	O
after	O
visiting	O
each	O
state	B
s.	O
these	O
are	O
optimal	O
esti-	O
mates	O
in	O
the	O
sense	O
that	O
they	O
min-	O
imize	O
the	O
mean-squared	O
error	O
from	O
the	O
actual	O
returns	O
in	O
the	O
training	O
set	O
.	O
in	O
this	O
sense	O
it	O
is	O
surprising	O
that	O
the	O
batch	O
td	O
method	O
was	O
able	O
to	O
perform	O
better	O
according	O
to	O
the	O
root	O
mean-squared	O
error	O
measure	O
shown	O
in	O
the	O
ﬁgure	O
to	O
the	O
right	O
.	O
how	O
is	O
it	O
that	O
batch	O
td	O
was	O
able	O
to	O
perform	O
better	O
than	O
this	O
optimal	O
method	O
?	O
the	O
answer	O
is	O
that	O
the	O
monte	O
carlo	O
method	O
is	O
optimal	O
only	O
in	O
a	O
limited	O
way	O
,	O
and	O
that	O
td	O
is	O
optimal	O
in	O
a	O
way	O
that	O
is	O
more	O
relevant	O
to	O
predicting	O
returns	O
.	O
figure	O
6.2	O
:	O
performance	O
of	O
td	O
(	O
0	O
)	O
and	O
constant-α	O
mc	O
under	O
batch	O
training	O
on	O
the	O
random	B
walk	I
task	O
.	O
example	O
6.4	O
:	O
you	O
are	O
the	O
predictor	O
place	O
yourself	O
now	O
in	O
the	O
role	O
of	O
the	O
predictor	O
of	O
returns	O
for	O
an	O
unknown	O
markov	O
reward	O
process	O
.	O
suppose	O
you	O
observe	O
the	O
following	O
eight	O
episodes	B
:	O
a	O
,	O
0	O
,	O
b	O
,	O
0	O
b	O
,	O
1	O
b	O
,	O
1	O
b	O
,	O
1	O
b	O
,	O
1	O
b	O
,	O
1	O
b	O
,	O
1	O
b	O
,	O
0	O
this	O
means	O
that	O
the	O
ﬁrst	O
episode	O
started	O
in	O
state	O
a	O
,	O
transitioned	O
to	O
b	O
with	O
a	O
reward	O
of	O
0	O
,	O
and	O
then	O
terminated	O
from	O
b	O
with	O
a	O
reward	O
of	O
0.	O
the	O
other	O
seven	O
episodes	B
were	O
even	O
shorter	O
,	O
starting	O
from	O
b	O
and	O
terminating	O
immediately	O
.	O
given	O
this	O
batch	O
of	O
data	O
,	O
what	O
would	O
you	O
say	O
are	O
the	O
optimal	O
predictions	O
,	O
the	O
best	O
values	O
for	O
the	O
estimates	O
v	O
(	O
a	O
)	O
and	O
v	O
(	O
b	O
)	O
?	O
everyone	O
would	O
probably	O
agree	O
that	O
the	O
optimal	O
value	O
for	O
v	O
(	O
b	O
)	O
is	O
3	O
4	O
,	O
because	O
six	O
out	O
of	O
the	O
eight	O
times	O
in	O
state	O
b	O
the	O
process	O
terminated	O
immediately	O
with	O
a	O
return	B
of	O
1	O
,	O
and	O
the	O
other	O
two	O
times	O
in	O
b	O
the	O
process	O
terminated	O
immediately	O
with	O
a	O
return	B
of	O
0.	O
but	O
what	O
is	O
the	O
optimal	O
value	O
for	O
the	O
estimate	O
v	O
(	O
a	O
)	O
given	O
this	O
data	O
?	O
here	O
there	O
are	O
two	O
reasonable	O
answers	O
.	O
one	O
is	O
to	O
observe	O
that	O
100	O
%	O
of	O
the	O
times	O
the	O
process	O
was	O
in	O
state	O
a	O
it	O
traversed	O
immediately	O
to	O
b	O
(	O
with	O
a	O
reward	O
of	O
0	O
)	O
;	O
and	O
because	O
we	O
have	O
already	O
decided	O
that	O
b	O
has	O
value	B
3	O
4	O
,	O
there-	O
fore	O
a	O
must	O
have	O
value	B
3	O
4	O
as	O
well	O
.	O
one	O
way	O
of	O
viewing	O
this	O
answer	O
is	O
that	O
it	O
is	O
based	O
on	O
ﬁrst	O
modeling	O
the	O
markov	O
pro-	O
cess	O
,	O
in	O
this	O
case	O
as	O
shown	O
to	O
the	O
right	O
,	O
and	O
then	O
computing	O
the	O
correct	O
estimates	O
given	O
the	O
model	O
,	O
which	O
indeed	O
in	O
this	O
case	O
gives	O
v	O
(	O
a	O
)	O
=	O
3	O
4	O
.	O
this	O
is	O
.0.05.1.15.2.250255075100tdmcbatch	O
trainingwalks	O
/	O
episodesrms	O
error	O
,	O
averagedover	O
statesabr	O
=	O
1100	O
%	O
75	O
%	O
25	O
%	O
r	O
=	O
0r	O
=	O
0	O
128	O
chapter	O
6	O
:	O
temporal-diﬀerence	B
learning	I
also	O
the	O
answer	O
that	O
batch	O
td	O
(	O
0	O
)	O
gives	O
.	O
the	O
other	O
reasonable	O
answer	O
is	O
simply	O
to	O
observe	O
that	O
we	O
have	O
seen	O
a	O
once	O
and	O
the	O
return	O
that	O
followed	O
it	O
was	O
0	O
;	O
we	O
therefore	O
estimate	O
v	O
(	O
a	O
)	O
as	O
0.	O
this	O
is	O
the	O
answer	O
that	O
batch	O
monte	O
carlo	O
methods	O
give	O
.	O
notice	O
that	O
it	O
is	O
also	O
the	O
answer	O
that	O
gives	O
minimum	O
squared	O
error	O
on	O
the	O
training	O
data	O
.	O
in	O
fact	O
,	O
it	O
gives	O
zero	O
error	O
on	O
the	O
data	O
.	O
but	O
still	O
we	O
expect	O
the	O
ﬁrst	O
answer	O
to	O
be	O
better	O
.	O
if	O
the	O
process	O
is	O
markov	O
,	O
we	O
expect	O
that	O
the	O
ﬁrst	O
answer	O
will	O
produce	O
lower	O
error	O
on	O
future	O
data	O
,	O
even	O
though	O
the	O
monte	O
carlo	O
answer	O
is	O
better	O
on	O
the	O
existing	O
data	O
.	O
example	O
6.4	O
illustrates	O
a	O
general	O
diﬀerence	O
between	O
the	O
estimates	O
found	O
by	O
batch	O
td	O
(	O
0	O
)	O
and	O
batch	O
monte	O
carlo	O
methods	O
.	O
batch	O
monte	O
carlo	O
methods	O
always	O
ﬁnd	O
the	O
estimates	O
that	O
minimize	O
mean-squared	O
error	O
on	O
the	O
training	O
set	O
,	O
whereas	O
batch	O
td	O
(	O
0	O
)	O
always	O
ﬁnds	O
the	O
estimates	O
that	O
would	O
be	O
exactly	O
correct	O
for	O
the	O
maximum-likelihood	O
model	O
of	O
the	O
markov	O
process	O
.	O
in	O
general	O
,	O
the	O
maximum-likelihood	B
estimate	I
of	O
a	O
parameter	O
is	O
the	O
parameter	O
value	O
whose	O
probability	O
of	O
generating	O
the	O
data	O
is	O
greatest	O
.	O
in	O
this	O
case	O
,	O
the	O
maximum-likelihood	B
estimate	I
is	O
the	O
model	O
of	O
the	O
markov	O
process	O
formed	O
in	O
the	O
obvious	O
way	O
from	O
the	O
observed	O
episodes	B
:	O
the	O
estimated	O
transition	O
probability	O
from	O
i	O
to	O
j	O
is	O
the	O
fraction	O
of	O
observed	O
transitions	O
from	O
i	O
that	O
went	O
to	O
j	O
,	O
and	O
the	O
associated	O
expected	O
reward	O
is	O
the	O
average	O
of	O
the	O
rewards	O
observed	O
on	O
those	O
transitions	O
.	O
given	O
this	O
model	O
,	O
we	O
can	O
compute	O
the	O
estimate	O
of	O
the	O
value	B
function	I
that	O
would	O
be	O
exactly	O
correct	O
if	O
the	O
model	O
were	O
exactly	O
correct	O
.	O
this	O
is	O
called	O
the	O
certainty-equivalence	B
estimate	I
because	O
it	O
is	O
equivalent	O
to	O
assuming	O
that	O
the	O
estimate	O
of	O
the	O
underlying	O
process	O
was	O
known	O
with	O
certainty	O
rather	O
than	O
being	O
approximated	O
.	O
in	O
general	O
,	O
batch	O
td	O
(	O
0	O
)	O
converges	O
to	O
the	O
certainty-equivalence	B
estimate	I
.	O
this	O
helps	O
explain	O
why	O
td	O
methods	O
converge	O
more	O
quickly	O
than	O
monte	O
carlo	O
methods	O
.	O
in	O
batch	O
form	O
,	O
td	O
(	O
0	O
)	O
is	O
faster	O
than	O
monte	O
carlo	O
methods	O
because	O
it	O
computes	O
the	O
true	O
certainty-equivalence	O
estimate	O
.	O
this	O
explains	O
the	O
advantage	O
of	O
td	O
(	O
0	O
)	O
shown	O
in	O
the	O
batch	O
results	O
on	O
the	O
random	B
walk	I
task	O
(	O
figure	O
6.2	O
)	O
.	O
the	O
relationship	O
to	O
the	O
certainty-	O
equivalence	O
estimate	O
may	O
also	O
explain	O
in	O
part	O
the	O
speed	O
advantage	O
of	O
nonbatch	O
td	O
(	O
0	O
)	O
(	O
e.g.	O
,	O
example	O
6.2	O
,	O
page	O
125	O
,	O
right	O
graph	O
)	O
.	O
although	O
the	O
nonbatch	O
methods	O
do	O
not	O
achieve	O
either	O
the	O
certainty-equivalence	O
or	O
the	O
minimum	O
squared-error	O
estimates	O
,	O
they	O
can	O
be	O
understood	O
as	O
moving	O
roughly	O
in	O
these	O
directions	O
.	O
nonbatch	O
td	O
(	O
0	O
)	O
may	O
be	O
faster	O
than	O
constant-α	O
mc	O
because	O
it	O
is	O
moving	O
toward	O
a	O
better	O
estimate	O
,	O
even	O
though	O
it	O
is	O
not	O
getting	O
all	O
the	O
way	O
there	O
.	O
at	O
the	O
current	O
time	O
nothing	O
more	O
deﬁnite	O
can	O
be	O
said	O
about	O
the	O
relative	O
eﬃciency	O
of	O
online	O
td	O
and	O
monte	O
carlo	O
methods	O
.	O
finally	O
,	O
it	O
is	O
worth	O
noting	O
that	O
although	O
the	O
certainty-equivalence	B
estimate	I
is	O
in	O
some	O
sense	O
an	O
optimal	O
solution	O
,	O
it	O
is	O
almost	O
never	O
feasible	O
to	O
compute	O
it	O
directly	O
.	O
if	O
n	O
=	O
|s|	O
is	O
the	O
number	O
of	O
states	O
,	O
then	O
just	O
forming	O
the	O
maximum-likelihood	B
estimate	I
of	O
the	O
process	O
may	O
require	O
on	O
the	O
order	O
of	O
n2	O
memory	O
,	O
and	O
computing	O
the	O
corresponding	O
value	B
function	I
requires	O
on	O
the	O
order	O
of	O
n3	O
computational	O
steps	O
if	O
done	O
conventionally	O
.	O
in	O
these	O
terms	O
it	O
is	O
indeed	O
striking	O
that	O
td	O
methods	O
can	O
approximate	B
the	O
same	O
solution	O
using	O
memory	O
no	O
more	O
than	O
order	O
n	O
and	O
repeated	O
computations	O
over	O
the	O
training	O
set	O
.	O
on	O
tasks	O
with	O
large	O
state	B
spaces	O
,	O
td	O
methods	O
may	O
be	O
the	O
only	O
feasible	O
way	O
of	O
approximating	O
the	O
certainty-equivalence	O
solution	O
.	O
∗exercise	O
6.7	O
design	O
an	O
oﬀ-policy	B
version	O
of	O
the	O
td	O
(	O
0	O
)	O
update	O
that	O
can	O
be	O
used	O
with	O
arbi-	O
6.4.	O
sarsa	O
:	O
on-policy	O
td	O
control	B
129	O
trary	O
target	B
policy	O
π	O
and	O
covering	O
behavior	B
policy	I
b	O
,	O
using	O
at	O
each	O
step	O
t	O
the	O
importance	O
(	O
cid:3	O
)	O
sampling	O
ratio	B
ρt	O
:	O
t	O
(	O
5.3	O
)	O
.	O
6.4	O
sarsa	O
:	O
on-policy	O
td	O
control	B
we	O
turn	O
now	O
to	O
the	O
use	O
of	O
td	O
prediction	B
methods	O
for	O
the	O
control	B
problem	O
.	O
as	O
usual	O
,	O
we	O
follow	O
the	O
pattern	O
of	O
generalized	O
policy	B
iteration	I
(	O
gpi	O
)	O
,	O
only	O
this	O
time	O
using	O
td	O
methods	O
for	O
the	O
evaluation	O
or	O
prediction	B
part	O
.	O
as	O
with	O
monte	O
carlo	O
methods	O
,	O
we	O
face	O
the	O
need	O
to	O
trade	O
oﬀ	O
exploration	O
and	O
exploitation	O
,	O
and	O
again	O
approaches	O
fall	O
into	O
two	O
main	O
classes	O
:	O
on-policy	O
and	O
oﬀ-policy	B
.	O
in	O
this	O
section	O
we	O
present	O
an	O
on-policy	O
td	O
control	B
method	O
.	O
the	O
ﬁrst	O
step	O
is	O
to	O
learn	O
an	O
action-value	B
function	I
rather	O
than	O
a	O
state-value	O
function	O
.	O
in	O
particular	O
,	O
for	O
an	O
on-policy	O
method	O
we	O
must	O
estimate	O
qπ	O
(	O
s	O
,	O
a	O
)	O
for	O
the	O
current	O
behavior	B
policy	I
π	O
and	O
for	O
all	O
states	O
s	O
and	O
actions	O
a.	O
this	O
can	O
be	O
done	O
using	O
essentially	O
the	O
same	O
td	O
method	O
described	O
above	O
for	O
learning	O
vπ	O
.	O
recall	O
that	O
an	O
episode	O
consists	O
of	O
an	O
alternating	O
sequence	O
of	O
states	O
and	O
state–action	O
pairs	O
:	O
in	O
the	O
previous	O
section	O
we	O
considered	O
transitions	O
from	O
state	B
to	O
state	B
and	O
learned	O
the	O
values	O
of	O
states	O
.	O
now	O
we	O
consider	O
transitions	O
from	O
state–action	O
pair	O
to	O
state–action	O
pair	O
,	O
and	O
learn	O
the	O
values	O
of	O
state–action	O
pairs	O
.	O
formally	O
these	O
cases	O
are	O
identical	O
:	O
they	O
are	O
both	O
markov	O
chains	O
with	O
a	O
reward	O
process	O
.	O
the	O
theorems	O
assuring	O
the	O
convergence	O
of	O
state	O
values	O
under	O
td	O
(	O
0	O
)	O
also	O
apply	O
to	O
the	O
corresponding	O
algorithm	O
for	B
action	I
values	I
:	O
q	O
(	O
st	O
,	O
at	O
)	O
←	O
q	O
(	O
st	O
,	O
at	O
)	O
+	O
α	O
(	O
cid:104	O
)	O
rt+1	O
+	O
γq	O
(	O
st+1	O
,	O
at+1	O
)	O
−	O
q	O
(	O
st	O
,	O
at	O
)	O
(	O
cid:105	O
)	O
.	O
if	O
this	O
update	O
is	O
done	O
after	O
every	O
transition	O
from	O
a	O
nonterminal	O
state	B
st.	O
st+1	O
is	O
terminal	O
,	O
then	O
q	O
(	O
st+1	O
,	O
at+1	O
)	O
is	O
deﬁned	O
as	O
zero	O
.	O
this	O
rule	O
uses	O
every	O
element	O
of	O
the	O
quintuple	O
of	O
events	O
,	O
(	O
st	O
,	O
at	O
,	O
rt+1	O
,	O
st+1	O
,	O
at+1	O
)	O
,	O
that	O
make	O
up	O
a	O
transition	O
from	O
one	O
state–action	O
pair	O
to	O
the	O
next	O
.	O
this	O
quintuple	O
gives	O
rise	O
to	O
the	O
name	O
sarsa	O
for	O
the	O
algorithm	O
.	O
the	O
backup	B
diagram	I
for	O
sarsa	O
is	O
as	O
shown	O
to	O
the	O
right	O
.	O
(	O
6.7	O
)	O
sarsa	O
exercise	O
6.8	O
show	O
that	O
an	O
action-value	O
version	O
of	O
(	O
6.6	O
)	O
holds	O
for	O
the	O
action-value	O
form	O
of	O
the	O
td	O
error	O
δt	O
=	O
rt+1	O
+	O
γq	O
(	O
st+1	O
,	O
at+1	O
)	O
−	O
q	O
(	O
st	O
,	O
at	O
)	O
,	O
again	O
assuming	O
that	O
the	O
values	O
(	O
cid:3	O
)	O
don	O
’	O
t	O
change	O
from	O
step	O
to	O
step	O
.	O
it	O
is	O
straightforward	O
to	O
design	O
an	O
on-policy	O
control	O
algorithm	O
based	O
on	O
the	O
sarsa	O
prediction	B
method	O
.	O
as	O
in	O
all	O
on-policy	B
methods	I
,	O
we	O
continually	O
estimate	O
qπ	O
for	O
the	O
behavior	B
policy	I
π	O
,	O
and	O
at	O
the	O
same	O
time	O
change	O
π	O
toward	O
greediness	O
with	O
respect	O
to	O
qπ	O
.	O
the	O
general	O
form	O
of	O
the	O
sarsa	O
control	B
algorithm	O
is	O
given	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
.	O
the	O
convergence	O
properties	O
of	O
the	O
sarsa	O
algorithm	O
depend	O
on	O
the	O
nature	O
of	O
the	O
policy	B
’	O
s	O
dependence	O
on	O
q.	O
for	O
example	O
,	O
one	O
could	O
use	O
ε-greedy	O
or	O
ε-soft	O
policies	O
.	O
sarsa	O
converges	O
with	O
probability	O
1	O
to	O
an	O
optimal	O
policy	O
and	O
action-value	O
function	O
as	O
long	O
as	O
all	O
state–	O
action	B
pairs	O
are	O
visited	O
an	O
inﬁnite	O
number	O
of	O
times	O
and	O
the	O
policy	O
converges	O
in	O
the	O
limit	O
to	O
the	O
greedy	O
policy	O
(	O
which	O
can	O
be	O
arranged	O
,	O
for	O
example	O
,	O
with	O
ε-greedy	O
policies	O
by	O
setting	O
ε	O
=	O
1/t	O
)	O
.	O
atrt+1stat+1rt+2st+1at+2rt+3st+2at+3st+3	O
.	O
.	O
..	O
.	O
.	O
130	O
chapter	O
6	O
:	O
temporal-diﬀerence	B
learning	I
sarsa	O
(	O
on-policy	O
td	O
control	B
)	O
for	O
estimating	O
q	O
≈	O
q∗	O
algorithm	O
parameters	O
:	O
step	O
size	O
α	O
∈	O
(	O
0	O
,	O
1	O
]	O
,	O
small	O
ε	O
>	O
0	O
initialize	O
q	O
(	O
s	O
,	O
a	O
)	O
,	O
for	O
all	O
s	O
∈	O
s+	O
,	O
a	O
∈	O
a	O
(	O
s	O
)	O
,	O
arbitrarily	O
except	O
that	O
q	O
(	O
terminal	O
,	O
·	O
)	O
=	O
0	O
loop	O
for	O
each	O
episode	O
:	O
initialize	O
s	O
choose	O
a	O
from	O
s	O
using	O
policy	B
derived	O
from	O
q	O
(	O
e.g.	O
,	O
ε-greedy	O
)	O
loop	O
for	O
each	O
step	O
of	O
episode	O
:	O
take	O
action	B
a	O
,	O
observe	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
choose	O
a	O
(	O
cid:48	O
)	O
from	O
s	O
(	O
cid:48	O
)	O
using	O
policy	B
derived	O
from	O
q	O
(	O
e.g.	O
,	O
ε-greedy	O
)	O
q	O
(	O
s	O
,	O
a	O
)	O
←	O
q	O
(	O
s	O
,	O
a	O
)	O
+	O
α	O
(	O
cid:2	O
)	O
r	O
+	O
γq	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
(	O
cid:48	O
)	O
)	O
−	O
q	O
(	O
s	O
,	O
a	O
)	O
(	O
cid:3	O
)	O
s	O
←	O
s	O
(	O
cid:48	O
)	O
;	O
a	O
←	O
a	O
(	O
cid:48	O
)	O
;	O
until	O
s	O
is	O
terminal	O
example	O
6.5	O
:	O
windy	B
gridworld	O
shown	O
inset	O
below	O
is	O
a	O
standard	O
gridworld	O
,	O
with	O
start	O
and	O
goal	O
states	O
,	O
but	O
with	O
one	O
diﬀerence	O
:	O
there	O
is	O
a	O
crosswind	O
running	O
upward	O
through	O
the	O
middle	O
of	O
the	O
grid	O
.	O
the	O
actions	O
are	O
the	O
standard	O
four—up	O
,	O
down	O
,	O
right	O
,	O
and	O
left—but	O
in	O
the	O
middle	O
region	O
the	O
resultant	O
next	O
states	O
are	O
shifted	O
upward	O
by	O
a	O
“	O
wind	O
,	O
”	O
the	O
strength	O
of	O
which	O
varies	O
from	O
column	O
to	O
column	O
.	O
the	O
strength	O
of	O
the	O
wind	O
is	O
given	O
below	O
each	O
column	O
,	O
in	O
num-	O
ber	O
of	O
cells	O
shifted	O
upward	O
.	O
for	O
ex-	O
ample	O
,	O
if	O
you	O
are	O
one	O
cell	O
to	O
the	O
right	O
of	O
the	O
goal	B
,	O
then	O
the	O
action	B
left	O
takes	O
you	O
to	O
the	O
cell	O
just	O
above	O
the	O
goal	B
.	O
this	O
is	O
an	O
undiscounted	O
episodic	O
task	O
,	O
with	O
constant	O
rewards	O
of	O
−1	O
until	O
the	O
goal	B
state	O
is	O
reached	O
.	O
the	O
graph	O
shows	O
the	O
results	O
of	O
applying	O
ε-greedy	O
sarsa	O
to	O
this	O
task	O
,	O
with	O
ε	O
=	O
0.1	O
,	O
α	O
=	O
0.5	O
,	O
and	O
the	O
ini-	O
tial	O
values	O
q	O
(	O
s	O
,	O
a	O
)	O
=	O
0	O
for	O
all	O
s	O
,	O
a.	O
the	O
increasing	O
slope	O
of	O
the	O
graph	O
shows	O
that	O
the	O
goal	B
was	O
reached	O
more	O
quickly	O
over	O
time	O
.	O
by	O
8000	O
time	O
steps	O
,	O
the	O
greedy	O
policy	O
was	O
long	O
since	O
optimal	O
(	O
a	O
trajectory	O
from	O
it	O
is	O
shown	O
inset	O
)	O
;	O
continued	O
ε-greedy	O
exploration	O
kept	O
the	O
average	O
episode	O
length	O
at	O
about	O
17	O
steps	O
,	O
two	O
more	O
than	O
the	O
minimum	O
of	O
15.	O
note	O
that	O
monte	O
carlo	O
methods	O
can	O
not	O
easily	O
be	O
used	O
on	O
this	O
task	O
because	O
termination	O
is	O
not	O
guaranteed	O
for	O
all	O
policies	O
.	O
if	O
a	O
policy	B
was	O
ever	O
found	O
that	O
caused	O
the	O
agent	O
to	O
stay	O
in	O
the	O
same	O
state	B
,	O
then	O
the	O
next	O
episode	O
would	O
never	O
end	O
.	O
step-by-step	O
learning	O
methods	O
such	O
as	O
sarsa	O
do	O
not	O
have	O
this	O
problem	O
be-	O
cause	O
they	O
quickly	O
learn	O
during	O
the	O
episode	O
that	O
such	O
policies	O
are	O
poor	O
,	O
and	O
switch	O
to	O
something	O
else	O
.	O
exercise	O
6.9	O
:	O
windy	B
gridworld	O
with	O
king	O
’	O
s	O
moves	O
re-solve	O
the	O
windy	B
gridworld	O
assum-	O
010002000300040005000600070008000050100150170time	O
stepssg0000111122actionsepisodes	O
6.5.	O
q-learning	O
:	O
oﬀ-policy	B
td	O
control	B
131	O
ing	B
eight	O
possible	O
actions	O
,	O
including	O
the	O
diagonal	O
moves	O
,	O
rather	O
than	O
the	O
usual	O
four	O
.	O
how	O
much	O
better	O
can	O
you	O
do	O
with	O
the	O
extra	O
actions	O
?	O
can	O
you	O
do	O
even	O
better	O
by	O
including	O
a	O
(	O
cid:3	O
)	O
ninth	O
action	B
that	O
causes	O
no	O
movement	O
at	O
all	O
other	O
than	O
that	O
caused	O
by	O
the	O
wind	O
?	O
exercise	O
6.10	O
:	O
stochastic	O
wind	O
re-solve	O
the	O
windy	B
gridworld	O
task	O
with	O
king	O
’	O
s	O
moves	O
,	O
assuming	O
that	O
the	O
eﬀect	O
of	O
the	O
wind	O
,	O
if	O
there	O
is	O
any	O
,	O
is	O
stochastic	O
,	O
sometimes	O
varying	O
by	O
1	O
from	O
the	O
mean	O
values	O
given	O
for	O
each	O
column	O
.	O
that	O
is	O
,	O
a	O
third	O
of	O
the	O
time	O
you	O
move	O
exactly	O
according	O
to	O
these	O
values	O
,	O
as	O
in	O
the	O
previous	O
exercise	O
,	O
but	O
also	O
a	O
third	O
of	O
the	O
time	O
you	O
move	O
one	O
cell	O
above	O
that	O
,	O
and	O
another	O
third	O
of	O
the	O
time	O
you	O
move	O
one	O
cell	O
below	O
that	O
.	O
for	O
example	O
,	O
if	O
you	O
are	O
one	O
cell	O
to	O
the	O
right	O
of	O
the	O
goal	B
and	O
you	O
move	O
left	O
,	O
then	O
one-third	O
of	O
the	O
time	O
you	O
move	O
one	O
cell	O
above	O
the	O
goal	B
,	O
one-third	O
of	O
the	O
time	O
you	O
move	O
(	O
cid:3	O
)	O
two	O
cells	O
above	O
the	O
goal	B
,	O
and	O
one-third	O
of	O
the	O
time	O
you	O
move	O
to	O
the	O
goal	B
.	O
6.5	O
q-learning	O
:	O
oﬀ-policy	B
td	O
control	B
one	O
of	O
the	O
early	O
breakthroughs	O
in	O
reinforcement	O
learning	O
was	O
the	O
development	O
of	O
an	O
oﬀ-policy	B
td	O
control	B
algorithm	O
known	O
as	O
q-learning	O
(	O
watkins	O
,	O
1989	O
)	O
,	O
deﬁned	O
by	O
q	O
(	O
st	O
,	O
at	O
)	O
←	O
q	O
(	O
st	O
,	O
at	O
)	O
+	O
α	O
(	O
cid:104	O
)	O
rt+1	O
+	O
γ	O
max	O
a	O
q	O
(	O
st+1	O
,	O
a	O
)	O
−	O
q	O
(	O
st	O
,	O
at	O
)	O
(	O
cid:105	O
)	O
.	O
(	O
6.8	O
)	O
in	O
this	O
case	O
,	O
the	O
learned	O
action-value	B
function	I
,	O
q	O
,	O
directly	O
approximates	O
q∗	O
,	O
the	O
optimal	O
action-value	O
function	O
,	O
independent	O
of	O
the	O
policy	B
being	O
followed	O
.	O
this	O
dramatically	O
sim-	O
pliﬁes	O
the	O
analysis	O
of	O
the	O
algorithm	O
and	O
enabled	O
early	O
convergence	O
proofs	O
.	O
the	O
policy	B
still	O
has	O
an	O
eﬀect	O
in	O
that	O
it	O
determines	O
which	O
state–action	O
pairs	O
are	O
visited	O
and	O
updated	O
.	O
however	O
,	O
all	O
that	O
is	O
required	O
for	O
correct	O
convergence	O
is	O
that	O
all	O
pairs	O
continue	O
to	O
be	O
up-	O
dated	O
.	O
as	O
we	O
observed	O
in	O
chapter	O
5	O
,	O
this	O
is	O
a	O
minimal	O
requirement	O
in	O
the	O
sense	O
that	O
any	O
method	O
guaranteed	O
to	O
ﬁnd	O
optimal	O
behavior	O
in	O
the	O
general	O
case	O
must	O
require	O
it	O
.	O
under	O
this	O
assumption	O
and	O
a	O
variant	O
of	O
the	O
usual	O
stochastic	O
approximation	O
conditions	O
on	O
the	O
sequence	O
of	O
step-size	O
parameters	O
,	O
q	O
has	O
been	O
shown	O
to	O
converge	O
with	O
probability	O
1	O
to	O
q∗	O
.	O
the	O
q-learning	O
algorithm	O
is	O
shown	O
below	O
in	O
procedural	O
form	O
.	O
q-learning	O
(	O
oﬀ-policy	B
td	O
control	B
)	O
for	O
estimating	O
π	O
≈	O
π∗	O
algorithm	O
parameters	O
:	O
step	O
size	O
α	O
∈	O
(	O
0	O
,	O
1	O
]	O
,	O
small	O
ε	O
>	O
0	O
initialize	O
q	O
(	O
s	O
,	O
a	O
)	O
,	O
for	O
all	O
s	O
∈	O
s+	O
,	O
a	O
∈	O
a	O
(	O
s	O
)	O
,	O
arbitrarily	O
except	O
that	O
q	O
(	O
terminal	O
,	O
·	O
)	O
=	O
0	O
loop	O
for	O
each	O
episode	O
:	O
initialize	O
s	O
loop	O
for	O
each	O
step	O
of	O
episode	O
:	O
choose	O
a	O
from	O
s	O
using	O
policy	B
derived	O
from	O
q	O
(	O
e.g.	O
,	O
ε-greedy	O
)	O
take	O
action	B
a	O
,	O
observe	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
q	O
(	O
s	O
,	O
a	O
)	O
←	O
q	O
(	O
s	O
,	O
a	O
)	O
+	O
α	O
(	O
cid:2	O
)	O
r	O
+	O
γ	O
maxa	O
q	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
)	O
−	O
q	O
(	O
s	O
,	O
a	O
)	O
(	O
cid:3	O
)	O
s	O
←	O
s	O
(	O
cid:48	O
)	O
until	O
s	O
is	O
terminal	O
132	O
chapter	O
6	O
:	O
temporal-diﬀerence	B
learning	I
what	O
is	O
the	O
backup	B
diagram	I
for	O
q-learning	O
?	O
the	O
rule	O
(	O
6.8	O
)	O
updates	O
a	O
state–action	O
pair	O
,	O
so	O
the	O
top	O
node	O
,	O
the	O
root	O
of	O
the	O
update	O
,	O
must	O
be	O
a	O
small	O
,	O
ﬁlled	O
action	B
node	O
.	O
the	O
update	O
is	O
also	O
from	O
action	B
nodes	O
,	O
maximizing	O
over	O
all	O
those	O
actions	O
possible	O
in	O
the	O
next	O
state	B
.	O
thus	O
the	O
bottom	O
nodes	O
of	O
the	O
backup	B
diagram	I
should	O
be	O
all	O
these	O
action	B
nodes	O
.	O
finally	O
,	O
remember	O
that	O
we	O
indicate	O
taking	O
the	O
maximum	O
of	O
these	O
“	O
next	O
action	B
”	O
nodes	O
with	O
an	O
arc	O
across	O
them	O
(	O
figure	O
3.4-right	O
)	O
.	O
can	O
you	O
guess	O
now	O
what	O
the	O
diagram	O
is	O
?	O
if	O
so	O
,	O
please	O
do	O
make	O
a	O
guess	O
before	O
turning	O
to	O
the	O
answer	O
in	O
figure	O
6.4	O
on	O
page	O
134.	O
example	O
6.6	O
:	O
cliﬀ	B
walking	I
this	O
gridworld	O
example	O
compares	O
sarsa	O
and	O
q-learning	O
,	O
highlighting	O
the	O
diﬀerence	O
between	O
on-policy	O
(	O
sarsa	O
)	O
and	O
oﬀ-policy	O
(	O
q-learning	O
)	O
meth-	O
ods	O
.	O
consider	O
the	O
gridworld	O
shown	O
to	O
the	O
right	O
.	O
this	O
is	O
a	O
standard	O
undiscounted	O
,	O
episodic	O
task	O
,	O
with	O
start	O
and	O
goal	O
states	O
,	O
and	O
the	O
usual	O
actions	O
causing	O
movement	O
up	O
,	O
down	O
,	O
right	O
,	O
and	O
left	O
.	O
reward	O
is	O
−1	O
on	O
all	O
transitions	O
except	O
those	O
into	O
the	O
re-	O
gion	O
marked	O
“	O
the	O
cliﬀ.	O
”	O
stepping	O
into	O
this	O
region	O
incurs	O
a	O
reward	O
of	O
−100	O
and	O
sends	O
the	O
agent	O
instantly	O
back	O
to	O
the	O
start	O
.	O
the	O
graph	O
to	O
the	O
right	O
shows	O
the	O
performance	O
of	O
the	O
sarsa	O
and	O
q-	O
learning	O
methods	O
with	O
ε-greedy	O
ac-	O
tion	B
selection	O
,	O
ε	O
=	O
0.1.	O
after	O
an	O
initial	O
transient	O
,	O
q-learning	O
learns	O
values	O
for	O
the	O
optimal	O
policy	O
,	O
that	O
which	O
travels	O
right	O
along	O
the	O
edge	O
of	O
the	O
cliﬀ	O
.	O
unfortunately	O
,	O
this	O
results	O
in	O
its	O
occasionally	O
falling	O
oﬀ	O
the	O
cliﬀ	O
because	O
of	O
the	O
ε-greedy	O
action	O
se-	O
lection	O
.	O
sarsa	O
,	O
on	O
the	O
other	O
hand	O
,	O
takes	O
the	O
action	B
selection	O
into	O
ac-	O
count	O
and	O
learns	O
the	O
longer	O
but	O
safer	O
path	O
through	O
the	O
upper	O
part	O
of	O
the	O
grid	O
.	O
although	O
q-learning	O
actually	O
learns	O
the	O
values	O
of	O
the	O
optimal	O
pol-	O
icy	O
,	O
its	O
online	B
performance	O
is	O
worse	O
than	O
that	O
of	O
sarsa	O
,	O
which	O
learns	O
the	O
roundabout	O
policy	B
.	O
of	O
course	O
,	O
if	O
ε	O
were	O
gradually	O
reduced	O
,	O
then	O
both	O
methods	O
would	O
asymptotically	O
converge	O
to	O
the	O
optimal	O
policy	O
.	O
exercise	O
6.11	O
why	O
is	O
q-learning	O
considered	O
an	O
oﬀ-policy	B
control	I
method	O
?	O
(	O
cid:3	O
)	O
exercise	O
6.12	O
suppose	O
action	B
selection	O
is	O
greedy	O
.	O
is	O
q-learning	O
then	O
exactly	O
the	O
same	O
algorithm	O
as	O
sarsa	O
?	O
will	O
they	O
make	O
exactly	O
the	O
same	O
action	B
selections	O
and	O
weight	O
(	O
cid:3	O
)	O
updates	O
?	O
rewardperepsiode	O
!	O
!	O
∀∀	O
!	O
#	O
∃	O
!	O
∃∀	O
!	O
%	O
∃∀	O
!	O
∀∀	O
%	O
∀∀	O
&	O
∀∀∍∀∀∃∀∀episodessarsaq-learningsg	O
the	O
cliff	O
rsum	O
of	O
rewardsduringepisoder	O
=	O
-1safer	O
pathoptimal	O
pathr	O
=	O
-100episodessarsaq-learningsgr	O
=	O
!	O
!	O
∀∀the	O
cliffr	O
=	O
(	O
!	O
!	O
saoprrsum	O
of	O
rewardsduringepisoder	O
=	O
-1safe	O
pathoptimal	O
pathr	O
=	O
-100episodes-25-50-75-1000100200300400500	O
6.6.	O
expected	O
sarsa	O
133	O
6.6	O
expected	O
sarsa	O
consider	O
the	O
learning	O
algorithm	O
that	O
is	O
just	O
like	O
q-learning	O
except	O
that	O
instead	O
of	O
the	O
maximum	O
over	O
next	O
state–action	O
pairs	O
it	O
uses	O
the	O
expected	O
value	O
,	O
taking	O
into	O
account	O
how	O
likely	O
each	O
action	B
is	O
under	O
the	O
current	O
policy	B
.	O
that	O
is	O
,	O
consider	O
the	O
algorithm	O
with	O
the	O
update	O
rule	O
q	O
(	O
st	O
,	O
at	O
)	O
←	O
q	O
(	O
st	O
,	O
at	O
)	O
+	O
α	O
(	O
cid:104	O
)	O
rt+1	O
+	O
γ	O
e	O
[	O
q	O
(	O
st+1	O
,	O
at+1	O
)	O
|	O
st+1	O
]	O
−	O
q	O
(	O
st	O
,	O
at	O
)	O
(	O
cid:105	O
)	O
π	O
(	O
a|st+1	O
)	O
q	O
(	O
st+1	O
,	O
a	O
)	O
−	O
q	O
(	O
st	O
,	O
at	O
)	O
(	O
cid:105	O
)	O
,	O
←	O
q	O
(	O
st	O
,	O
at	O
)	O
+	O
α	O
(	O
cid:104	O
)	O
rt+1	O
+	O
γ	O
(	O
cid:88	O
)	O
a	O
(	O
6.9	O
)	O
but	O
that	O
otherwise	O
follows	O
the	O
schema	O
of	O
q-learning	O
.	O
given	O
the	O
next	O
state	B
,	O
st+1	O
,	O
this	O
algorithm	O
moves	O
deterministically	O
in	O
the	O
same	O
direction	O
as	O
sarsa	O
moves	O
in	O
expectation	O
,	O
and	O
accordingly	O
it	O
is	O
called	O
expected	O
sarsa	O
.	O
its	O
backup	B
diagram	I
is	O
shown	O
on	O
the	O
right	O
in	O
figure	O
6.4.	O
expected	O
sarsa	O
is	O
more	O
complex	O
computationally	O
than	O
sarsa	O
but	O
,	O
in	O
return	O
,	O
it	O
elim-	O
inates	O
the	O
variance	O
due	O
to	O
the	O
random	O
selection	O
of	O
at+1	O
.	O
given	O
the	O
same	O
amount	O
of	O
experience	O
we	O
might	O
expect	O
it	O
to	O
perform	O
slightly	O
better	O
than	O
sarsa	O
,	O
and	O
indeed	O
it	O
gen-	O
erally	O
does	O
.	O
figure	O
6.3	O
shows	O
summary	O
results	O
on	O
the	O
cliﬀ-walking	O
task	O
with	O
expected	O
sarsa	O
compared	O
to	O
sarsa	O
and	O
q-learning	O
.	O
expected	O
sarsa	O
retains	O
the	O
signiﬁcant	O
advan-	O
tage	O
of	O
sarsa	O
over	O
q-learning	O
on	O
this	O
problem	O
.	O
in	O
addition	O
,	O
expected	O
sarsa	O
shows	O
a	O
figure	O
6.3	O
:	O
interim	O
and	O
asymptotic	O
performance	O
of	O
td	O
control	B
methods	O
on	O
the	O
cliﬀ-walking	O
task	O
as	O
a	O
function	O
of	O
α.	O
all	O
algorithms	O
used	O
an	O
ε-greedy	O
policy	O
with	O
ε	O
=	O
0.1.	O
asymptotic	O
performance	O
is	O
an	O
average	O
over	O
100,000	O
episodes	B
whereas	O
interim	O
performance	O
is	O
an	O
average	O
over	O
the	O
ﬁrst	O
100	O
episodes	B
.	O
these	O
data	O
are	O
averages	O
of	O
over	O
50,000	O
and	O
10	O
runs	O
for	O
the	O
interim	O
and	O
asymptotic	O
cases	O
respectively	O
.	O
the	O
solid	O
circles	O
mark	O
the	O
best	O
interim	O
performance	O
of	O
each	O
method	O
.	O
adapted	O
from	O
van	O
seijen	O
et	O
al	O
.	O
(	O
2009	O
)	O
.	O
wethenpresentresultsontwoversionsofthewindygridworldproblem	O
,	O
onewithadeterministicenvironmentandonewithastochasticenvironment.wedosoinordertoevaluatetheinﬂuenceofenvironmentstochasticityontheperformancedifferencebetweenexpectedsarsaandsarsaandconﬁrmtheﬁrstpartofhypothesis2.wethenpresentresultsfordifferentamountsofpolicystochasticitytoconﬁrmthesecondpartofhypothesis2.forcompleteness	O
,	O
wealsoshowtheperformanceofq-learningonthisproblem.finally	O
,	O
wepresentresultsinotherdomainsverifyingtheadvantagesofexpectedsarsainabroadersetting.allresultspresentedbelowareaveragedovernumerousindependenttrialssuchthatthestandarderrorbecomesnegligible.a.cliffwalkingwebeginbytestinghypothesis1usingthecliffwalkingtask	O
,	O
anundiscounted	O
,	O
episodicnavigationtaskinwhichtheagenthastoﬁnditswayfromstarttogoalinadeterministicgridworld.alongtheedgeofthegridworldisacliff	O
(	O
seefigure1	O
)	O
.theagentcantakeanyoffourmovementactions	O
:	O
up	O
,	O
down	O
,	O
leftandright	O
,	O
eachofwhichmovestheagentonesquareinthecorrespondingdirection.eachstepresultsinarewardof-1	O
,	O
exceptwhentheagentstepsintothecliffarea	O
,	O
whichresultsinarewardof-100andanimmediatereturntothestartstate.theepisodeendsuponreachingthegoalstate.sgfig.1.thecliffwalkingtask.theagenthastomovefromthestart	O
[	O
s	O
]	O
tothegoal	O
[	O
g	O
]	O
,	O
whileavoidingsteppingintothecliff	O
(	O
greyarea	O
)	O
.weevaluatedtheperformanceovertheﬁrstnepisodesasafunctionofthelearningrateæusingan≤-greedypolicywith≤=0.1.figure2showstheresultforn=100andn=100,000.weaveragedtheresultsover50,000runsand10runs	O
,	O
respectively.discussion.expectedsarsaoutperformsq-learningandsarsaforalllearningratevalues	O
,	O
conﬁrminghypothesis1andprovidingsomeevidenceforhypothesis2.theoptimalævalueofexpectedsarsaforn=100is1	O
,	O
whileforsarsaitislower	O
,	O
asexpectedforadeterministicproblem.thattheoptimalvalueofq-learningisalsolowerthan1issurprising	O
,	O
sinceq-learningalsohasnostochasticityinitsupdatesinadeterministicenvironment.ourexplanationisthatq-learningﬁrstlearnspoliciesthataresub-optimalinthegreedysense	O
,	O
i.e.walkingtowardsthegoalwithadetourfurtherfromthecliff.q-learningiterativelyoptimizestheseearlypolicies	O
,	O
resultinginapathmorecloselyalongthecliff.however	O
,	O
althoughthispathisbetterintheoff-linesense	O
,	O
intermsofon-lineperformanceitisworse.alargevalueofæensuresthegoalisreachedquickly	O
,	O
butavaluesomewhatlowerthan1ensuresthattheagentdoesnottrytowalkrightontheedgeofthecliffimmediately	O
,	O
resultinginaslightlybetteron-lineperformance.forn=100,000	O
,	O
theaveragereturnisequalforallævaluesincaseofexpectedsarsaandq-learning.thisindicatesthatthealgorithmshaveconvergedlongbeforetheendoftherunforallævalues	O
,	O
sincewedonotseeanyeffectoftheinitiallearningphase.forsarsatheperformancecomesclosetotheperformanceofexpectedsarsaonlyforæ=0.1	O
,	O
whileforlargeæ	O
,	O
theperformanceforn=100,000evendropsbelowtheperformanceforn=100.thereasonisthatforlargevaluesofætheqvaluesofsarsadiverge.althoughthepolicyisstillimprovedovertheinitialrandompolicyduringtheearlystagesoflearning	O
,	O
divergencecausesthepolicytogetworseinthelongrun.0.10.20.30.40.50.60.70.80.91−160−140−120−100−80−60−40−200alphaaverage	O
return	B
n	O
=	O
100	O
,	O
sarsan	O
=	O
100	O
,	O
q−learningn	O
=	O
100	O
,	O
expected	O
sarsan	O
=	O
1e5	O
,	O
sarsan	O
=	O
1e5	O
,	O
q−learningn	O
=	O
1e5	O
,	O
expected	O
sarsafig.2.averagereturnonthecliffwalkingtaskovertheﬁrstnepisodesforn=100andn=100,000usingan≤-greedypolicywith≤=0.1.thebigdotsindicatethemaximalvalues.b.windygridworldweturntothewindygridworldtasktofurthertesthy-pothesis2.thewindygridworldtaskisanothernavigationtask	O
,	O
wheretheagenthastoﬁnditswayfromstarttogoal.thegridhasaheightof7andawidthof10squares.thereisawindblowinginthe	O
’	O
up	O
’	O
directioninthemiddlepartofthegrid	O
,	O
withastrengthof1or2dependingonthecolumn.figure3showsthegridworldwithanumberbeloweachcolumnindicatingthewindstrength.again	O
,	O
theagentcanchoosebetweenfourmovementactions	O
:	O
up	O
,	O
down	O
,	O
leftandright	O
,	O
eachresultinginarewardof-1.theresultofanactionisamovementof1squareinthecorrespondingdirectionplusanadditionalmovementinthe	O
’	O
up	O
’	O
direction	O
,	O
correspondingwiththewindstrength.forexample	O
,	O
whentheagentisinthesquarerightofthegoalandtakesa	O
’	O
left	O
’	O
action	B
,	O
itendsupinthesquarejustabovethegoal.1	O
)	O
deterministicenvironment	O
:	O
weﬁrstconsiderade-terministicenvironment.asinthecliffwalkingtask	O
,	O
weusean≤-greedypolicywith≤=0.1.figure4showstheperformanceasafunctionofthelearningrateæovertheﬁrstnepisodesforn=100andn=100,000.forn=100expected	O
sarsasarsaq-learningasymptotic	O
performanceinterim	O
performanceq-learningsum	O
of	O
rewardsper	O
episode↵10.10.20.40.60.80.30.50.70.90-40-80-120	O
134	O
chapter	O
6	O
:	O
temporal-diﬀerence	B
learning	I
q-learning	O
expected	O
sarsa	O
figure	O
6.4	O
:	O
the	O
backup	O
diagrams	O
for	O
q-learning	O
and	O
expected	O
sarsa	O
.	O
signiﬁcant	O
improvement	O
over	O
sarsa	O
over	O
a	O
wide	O
range	O
of	O
values	O
for	O
the	O
step-size	O
parame-	O
ter	O
α.	O
in	O
cliﬀ	O
walking	O
the	O
state	B
transitions	O
are	O
all	O
deterministic	O
and	O
all	O
randomness	O
comes	O
from	O
the	O
policy	B
.	O
in	O
such	O
cases	O
,	O
expected	O
sarsa	O
can	O
safely	O
set	O
α	O
=	O
1	O
without	O
suﬀering	O
any	O
degradation	O
of	O
asymptotic	O
performance	O
,	O
whereas	O
sarsa	O
can	O
only	O
perform	O
well	O
in	O
the	O
long	O
run	O
at	O
a	O
small	O
value	B
of	O
α	O
,	O
at	O
which	O
short-term	O
performance	O
is	O
poor	O
.	O
in	O
this	O
and	O
other	O
examples	O
there	O
is	O
a	O
consistent	O
empirical	O
advantage	O
of	O
expected	O
sarsa	O
over	O
sarsa	O
.	O
in	O
these	O
cliﬀ	B
walking	I
results	O
expected	O
sarsa	O
was	O
used	O
on-policy	O
,	O
but	O
in	O
general	O
it	O
might	O
use	O
a	O
policy	B
diﬀerent	O
from	O
the	O
target	B
policy	O
π	O
to	O
generate	O
behavior	O
,	O
in	O
which	O
case	O
it	O
becomes	O
an	O
oﬀ-policy	B
algorithm	O
.	O
for	O
example	O
,	O
suppose	O
π	O
is	O
the	O
greedy	O
policy	O
while	O
behavior	O
is	O
more	O
exploratory	O
;	O
then	O
expected	O
sarsa	O
is	O
exactly	O
q-learning	O
.	O
in	O
this	O
sense	O
expected	O
sarsa	O
subsumes	O
and	O
generalizes	O
q-learning	O
while	O
reliably	O
improving	O
over	O
sarsa	O
.	O
except	O
for	O
the	O
small	O
additional	O
computational	O
cost	O
,	O
expected	O
sarsa	O
may	O
completely	O
dominate	O
both	O
of	O
the	O
other	O
more-well-known	O
td	O
control	B
algorithms	O
.	O
6.7	O
maximization	B
bias	I
and	O
double	B
learning	I
all	O
the	O
control	B
algorithms	O
that	O
we	O
have	O
discussed	O
so	O
far	O
involve	O
maximization	O
in	O
the	O
construction	O
of	O
their	O
target	B
policies	O
.	O
for	O
example	O
,	O
in	O
q-learning	O
the	O
target	B
policy	O
is	O
the	O
greedy	O
policy	O
given	O
the	O
current	O
action	B
values	O
,	O
which	O
is	O
deﬁned	O
with	O
a	O
max	O
,	O
and	O
in	O
sarsa	O
the	O
policy	B
is	O
often	O
ε-greedy	O
,	O
which	O
also	O
involves	O
a	O
maximization	O
operation	O
.	O
in	O
these	O
algorithms	O
,	O
a	O
maximum	O
over	O
estimated	O
values	O
is	O
used	O
implicitly	O
as	O
an	O
estimate	O
of	O
the	O
maximum	O
value	B
,	O
which	O
can	O
lead	O
to	O
a	O
signiﬁcant	O
positive	O
bias	O
.	O
to	O
see	O
why	O
,	O
consider	O
a	O
single	O
state	B
s	O
where	O
there	O
are	O
many	O
actions	O
a	O
whose	O
true	O
values	O
,	O
q	O
(	O
s	O
,	O
a	O
)	O
,	O
are	O
all	O
zero	O
but	O
whose	O
estimated	O
values	O
,	O
q	O
(	O
s	O
,	O
a	O
)	O
,	O
are	O
uncertain	O
and	O
thus	O
distributed	O
some	O
above	O
and	O
some	O
below	O
zero	O
.	O
the	O
maximum	O
of	O
the	O
true	O
values	O
is	O
zero	O
,	O
but	O
the	O
maximum	O
of	O
the	O
estimates	O
is	O
positive	O
,	O
a	O
positive	O
bias	O
.	O
we	O
call	O
this	O
maximization	B
bias	I
.	O
example	O
6.7	O
:	O
maximization	B
bias	I
example	O
the	O
small	O
mdp	O
shown	O
inset	O
in	O
fig-	O
ure	O
6.5	O
provides	O
a	O
simple	O
example	O
of	O
how	O
maximization	B
bias	I
can	O
harm	O
the	O
performance	O
of	O
td	O
control	B
algorithms	O
.	O
the	O
mdp	O
has	O
two	O
non-terminal	O
states	O
a	O
and	O
b.	O
episodes	B
always	O
start	O
in	O
a	O
with	O
a	O
choice	O
between	O
two	O
actions	O
,	O
left	O
and	O
right	O
.	O
the	O
right	O
action	B
transitions	O
immediately	O
to	O
the	O
terminal	O
state	B
with	O
a	O
reward	O
and	O
return	B
of	O
zero	O
.	O
the	O
left	O
action	B
tran-	O
sitions	O
to	O
b	O
,	O
also	O
with	O
a	O
reward	O
of	O
zero	O
,	O
from	O
which	O
there	O
are	O
many	O
possible	O
actions	O
all	O
of	O
which	O
cause	O
immediate	O
termination	O
with	O
a	O
reward	O
drawn	O
from	O
a	O
normal	O
distribution	O
with	O
mean	O
−0.1	O
and	O
variance	O
1.0.	O
thus	O
,	O
the	O
expected	O
return	O
for	O
any	O
trajectory	O
starting	O
with	O
left	O
is	O
−0.1	O
,	O
and	O
thus	O
taking	O
left	O
in	O
state	O
a	O
is	O
always	O
a	O
mistake	O
.	O
nevertheless	O
,	O
our	O
6.7.	O
maximization	B
bias	I
and	O
double	B
learning	I
135	O
figure	O
6.5	O
:	O
comparison	O
of	O
q-learning	O
and	O
double	O
q-learning	O
on	O
a	O
simple	O
episodic	O
mdp	O
(	O
shown	O
inset	O
)	O
.	O
q-learning	O
initially	O
learns	O
to	O
take	O
the	O
left	O
action	B
much	O
more	O
often	O
than	O
the	O
right	O
action	B
,	O
and	O
always	O
takes	O
it	O
signiﬁcantly	O
more	O
often	O
than	O
the	O
5	O
%	O
minimum	O
probability	O
enforced	O
by	O
ε-greedy	O
action	O
selection	O
with	O
ε	O
=	O
0.1.	O
in	O
contrast	O
,	O
double	B
q-learning	O
is	O
essentially	O
unaﬀected	O
by	O
maximization	B
bias	I
.	O
these	O
data	O
are	O
averaged	O
over	O
10,000	O
runs	O
.	O
the	O
initial	O
action-	O
value	B
estimates	O
were	O
zero	O
.	O
any	O
ties	O
in	O
ε-greedy	O
action	B
selection	O
were	O
broken	O
randomly	O
.	O
control	B
methods	O
may	O
favor	O
left	O
because	O
of	O
maximization	O
bias	O
making	O
b	O
appear	O
to	O
have	O
a	O
positive	O
value	B
.	O
figure	O
6.5	O
shows	O
that	O
q-learning	O
with	O
ε-greedy	O
action	B
selection	O
initially	O
learns	O
to	O
strongly	O
favor	O
the	O
left	O
action	B
on	O
this	O
example	O
.	O
even	O
at	O
asymptote	O
,	O
q-learning	O
takes	O
the	O
left	O
action	B
about	O
5	O
%	O
more	O
often	O
than	O
is	O
optimal	O
at	O
our	O
parameter	O
settings	O
(	O
ε	O
=	O
0.1	O
,	O
α	O
=	O
0.1	O
,	O
and	O
γ	O
=	O
1	O
)	O
.	O
are	O
there	O
algorithms	O
that	O
avoid	O
maximization	B
bias	I
?	O
to	O
start	O
,	O
consider	O
a	O
bandit	O
case	O
in	O
which	O
we	O
have	O
noisy	O
estimates	O
of	O
the	O
value	B
of	O
each	O
of	O
many	O
actions	O
,	O
obtained	O
as	O
sample	O
averages	O
of	O
the	O
rewards	O
received	O
on	O
all	O
the	O
plays	O
with	O
each	O
action	B
.	O
as	O
we	O
discussed	O
above	O
,	O
there	O
will	O
be	O
a	O
positive	O
maximization	B
bias	I
if	O
we	O
use	O
the	O
maximum	O
of	O
the	O
estimates	O
as	O
an	O
estimate	O
of	O
the	O
maximum	O
of	O
the	O
true	O
values	O
.	O
one	O
way	O
to	O
view	O
the	O
problem	O
is	O
that	O
it	O
is	O
due	O
to	O
using	O
the	O
same	O
samples	O
(	O
plays	O
)	O
both	O
to	O
determine	O
the	O
maximizing	O
action	B
and	O
to	O
estimate	O
its	O
value	B
.	O
suppose	O
we	O
divided	O
the	O
plays	O
in	O
two	O
sets	O
and	O
used	O
them	O
to	O
learn	O
two	O
independent	O
estimates	O
,	O
call	O
them	O
q1	O
(	O
a	O
)	O
and	O
q2	O
(	O
a	O
)	O
,	O
each	O
an	O
estimate	O
of	O
the	O
true	O
value	O
q	O
(	O
a	O
)	O
,	O
for	O
all	O
a	O
∈	O
a.	O
we	O
could	O
then	O
use	O
one	O
estimate	O
,	O
say	O
q1	O
,	O
to	O
determine	O
the	O
maximizing	O
action	B
a∗	O
=	O
argmaxa	O
q1	O
(	O
a	O
)	O
,	O
and	O
the	O
other	O
,	O
q2	O
,	O
to	O
provide	O
the	O
estimate	O
of	O
its	O
value	B
,	O
q2	O
(	O
a∗	O
)	O
=	O
q2	O
(	O
argmaxa	O
q1	O
(	O
a	O
)	O
)	O
.	O
this	O
estimate	O
will	O
then	O
be	O
unbiased	O
in	O
the	O
sense	O
that	O
e	O
[	O
q2	O
(	O
a∗	O
)	O
]	O
=	O
q	O
(	O
a∗	O
)	O
.	O
we	O
can	O
also	O
repeat	O
the	O
process	O
with	O
the	O
role	O
of	O
the	O
two	O
estimates	O
reversed	O
to	O
yield	O
a	O
second	O
unbiased	O
estimate	O
q1	O
(	O
argmaxa	O
q2	O
(	O
a	O
)	O
)	O
.	O
this	O
is	O
the	O
idea	O
of	O
double	O
learning	O
.	O
note	O
that	O
although	O
we	O
learn	O
two	O
estimates	O
,	O
only	O
one	O
estimate	O
is	O
updated	O
on	O
each	O
play	O
;	O
double	B
learning	I
doubles	O
the	O
memory	O
requirements	O
,	O
but	O
does	O
not	O
increase	O
the	O
amount	O
of	O
computation	O
per	O
step	O
.	O
the	O
idea	O
of	O
double	O
learning	O
extends	O
naturally	O
to	O
algorithms	O
for	O
full	O
mdps	O
.	O
for	O
ex-	O
ample	O
,	O
the	O
double	B
learning	I
algorithm	O
analogous	O
to	O
q-learning	O
,	O
called	O
double	B
q-learning	O
,	O
divides	O
the	O
time	O
steps	O
in	O
two	O
,	O
perhaps	O
by	O
ﬂipping	O
a	O
coin	O
on	O
each	O
step	O
.	O
if	O
the	O
coin	O
comes	O
barightleft0	O
.	O
.	O
.n	O
(	O
 0.1,1	O
)	O
0q-learningdoubleq-learningepisodes1001200300	O
%	O
leftactionsfrom	O
a100	O
%	O
75	O
%	O
50	O
%	O
25	O
%	O
5	O
%	O
0optimal	O
136	O
chapter	O
6	O
:	O
temporal-diﬀerence	B
learning	I
up	O
heads	O
,	O
the	O
update	O
is	O
q1	O
(	O
st	O
,	O
at	O
)	O
←	O
q1	O
(	O
st	O
,	O
at	O
)	O
+α	O
(	O
cid:104	O
)	O
rt+1+γq2	O
(	O
cid:0	O
)	O
st+1	O
,	O
argmax	O
a	O
q1	O
(	O
st+1	O
,	O
a	O
)	O
(	O
cid:1	O
)	O
−q1	O
(	O
st	O
,	O
at	O
)	O
(	O
cid:105	O
)	O
.	O
(	O
6.10	O
)	O
if	O
the	O
coin	O
comes	O
up	O
tails	O
,	O
then	O
the	O
same	O
update	O
is	O
done	O
with	O
q1	O
and	O
q2	O
switched	O
,	O
so	O
that	O
q2	O
is	O
updated	O
.	O
the	O
two	O
approximate	B
value	O
functions	O
are	O
treated	O
completely	O
symmetrically	O
.	O
the	O
behavior	B
policy	I
can	O
use	O
both	O
action-value	O
estimates	O
.	O
for	O
example	O
,	O
an	O
ε-greedy	O
policy	O
for	O
double	O
q-learning	O
could	O
be	O
based	O
on	O
the	O
average	O
(	O
or	O
sum	O
)	O
of	O
the	O
two	O
action-value	O
estimates	O
.	O
a	O
complete	O
algorithm	O
for	O
double	O
q-learning	O
is	O
given	O
in	O
the	O
box	O
below	O
.	O
this	O
is	O
the	O
algorithm	O
used	O
to	O
produce	O
the	O
results	O
in	O
figure	O
6.5.	O
in	O
that	O
example	O
,	O
double	B
learning	I
seems	O
to	O
eliminate	O
the	O
harm	O
caused	O
by	O
maximization	B
bias	I
.	O
of	O
course	O
there	O
are	O
also	O
double	B
versions	O
of	O
sarsa	O
and	O
expected	O
sarsa	O
.	O
double	B
q-learning	O
,	O
for	O
estimating	O
q1	O
≈	O
q2	O
≈	O
q∗	O
algorithm	O
parameters	O
:	O
step	O
size	O
α	O
∈	O
(	O
0	O
,	O
1	O
]	O
,	O
small	O
ε	O
>	O
0	O
initialize	O
q1	O
(	O
s	O
,	O
a	O
)	O
and	O
q2	O
(	O
s	O
,	O
a	O
)	O
,	O
for	O
all	O
s	O
∈	O
s+	O
,	O
a	O
∈	O
a	O
(	O
s	O
)	O
,	O
such	O
that	O
q	O
(	O
terminal	O
,	O
·	O
)	O
=	O
0	O
loop	O
for	O
each	O
episode	O
:	O
initialize	O
s	O
loop	O
for	O
each	O
step	O
of	O
episode	O
:	O
choose	O
a	O
from	O
s	O
using	O
the	O
policy	B
ε-greedy	O
in	O
q1	O
+	O
q2	O
take	O
action	B
a	O
,	O
observe	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
with	O
0.5	O
probabilility	O
:	O
else	O
:	O
q1	O
(	O
s	O
,	O
a	O
)	O
←	O
q1	O
(	O
s	O
,	O
a	O
)	O
+	O
α	O
(	O
cid:16	O
)	O
r	O
+	O
γq2	O
(	O
cid:0	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
argmaxa	O
q1	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
)	O
(	O
cid:1	O
)	O
−	O
q1	O
(	O
s	O
,	O
a	O
)	O
(	O
cid:17	O
)	O
q2	O
(	O
s	O
,	O
a	O
)	O
←	O
q2	O
(	O
s	O
,	O
a	O
)	O
+	O
α	O
(	O
cid:16	O
)	O
r	O
+	O
γq1	O
(	O
cid:0	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
argmaxa	O
q2	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
)	O
(	O
cid:1	O
)	O
−	O
q2	O
(	O
s	O
,	O
a	O
)	O
(	O
cid:17	O
)	O
s	O
←	O
s	O
(	O
cid:48	O
)	O
until	O
s	O
is	O
terminal	O
∗exercise	O
6.13	O
what	O
are	O
the	O
update	O
equations	O
for	O
double	O
expected	O
sarsa	O
with	O
an	O
ε-	O
(	O
cid:3	O
)	O
greedy	O
target	O
policy	B
?	O
6.8	O
games	O
,	O
afterstates	B
,	O
and	O
other	O
special	O
cases	O
in	O
this	O
book	O
we	O
try	O
to	O
present	O
a	O
uniform	O
approach	O
to	O
a	O
wide	O
class	O
of	O
tasks	O
,	O
but	O
of	O
course	O
there	O
are	O
always	O
exceptional	O
tasks	O
that	O
are	O
better	O
treated	O
in	O
a	O
specialized	O
way	O
.	O
for	O
ex-	O
ample	O
,	O
our	O
general	O
approach	O
involves	O
learning	O
an	O
action-value	B
function	I
,	O
but	O
in	O
chapter	O
1	O
we	O
presented	O
a	O
td	O
method	O
for	O
learning	O
to	O
play	O
tic-tac-toe	B
that	O
learned	O
something	O
much	O
more	O
like	O
a	O
state-value	O
function	O
.	O
if	O
we	O
look	O
closely	O
at	O
that	O
example	O
,	O
it	O
becomes	O
apparent	O
that	O
the	O
function	O
learned	O
there	O
is	O
neither	O
an	O
action-value	B
function	I
nor	O
a	O
state-value	O
func-	O
tion	B
in	O
the	O
usual	O
sense	O
.	O
a	O
conventional	O
state-value	O
function	O
evaluates	O
states	O
in	O
which	O
the	O
agent	O
has	O
the	O
option	O
of	O
selecting	O
an	O
action	B
,	O
but	O
the	O
state-value	O
function	O
used	O
in	O
tic-	O
tac-toe	O
evaluates	O
board	O
positions	O
after	O
the	O
agent	O
has	O
made	O
its	O
move	O
.	O
let	O
us	O
call	O
these	O
6.9.	O
summary	O
137	O
afterstates	B
,	O
and	O
value	O
functions	O
over	O
these	O
,	O
afterstate	O
value	B
functions	O
.	O
afterstates	B
are	O
useful	O
when	O
we	O
have	O
knowledge	O
of	O
an	O
initial	O
part	O
of	O
the	O
environment	B
’	O
s	O
dynamics	O
but	O
not	O
necessarily	O
of	O
the	O
full	O
dynamics	O
.	O
for	O
example	O
,	O
in	O
games	O
we	O
typically	O
know	O
the	O
immediate	O
eﬀects	O
of	O
our	O
moves	O
.	O
we	O
know	O
for	O
each	O
possible	O
chess	B
move	O
what	O
the	O
resulting	O
position	O
will	O
be	O
,	O
but	O
not	O
how	O
our	O
opponent	O
will	O
reply	O
.	O
afterstate	O
value	B
functions	O
are	O
a	O
natural	O
way	O
to	O
take	O
advantage	O
of	O
this	O
kind	O
of	O
knowledge	O
and	O
thereby	O
produce	O
a	O
more	O
eﬃcient	O
learning	O
method	O
.	O
the	O
reason	O
it	O
is	O
more	O
eﬃcient	O
to	O
design	O
algorithms	O
in	O
terms	O
of	O
afterstates	O
is	O
apparent	O
from	O
the	O
tic-tac-toe	B
example	O
.	O
a	O
conventional	O
action-value	B
function	I
would	O
map	O
from	O
posi-	O
tions	O
and	O
moves	O
to	O
an	O
estimate	O
of	O
the	O
value	B
.	O
but	O
many	O
position–move	O
pairs	O
pro-	O
duce	O
the	O
same	O
resulting	O
position	O
,	O
as	O
in	O
the	O
example	O
shown	O
to	O
the	O
right	O
.	O
in	O
such	O
cases	O
the	O
position–move	O
pairs	O
are	O
diﬀerent	O
but	O
produce	O
the	O
same	O
“	O
afterposition	O
,	O
”	O
and	O
thus	O
must	O
have	O
the	O
same	O
value	B
.	O
a	O
conventional	O
action-value	B
function	I
would	O
have	O
to	O
sepa-	O
rately	O
assess	O
both	O
pairs	O
,	O
whereas	O
an	O
after-	O
state	B
value	O
function	O
would	O
immediately	O
as-	O
sess	O
both	O
equally	O
.	O
any	O
learning	O
about	O
the	O
position–move	O
pair	O
on	O
the	O
left	O
would	O
immedi-	O
ately	O
transfer	O
to	O
the	O
pair	O
on	O
the	O
right	O
.	O
afterstates	B
arise	O
in	O
many	O
tasks	O
,	O
not	O
just	O
games	O
.	O
for	O
example	O
,	O
in	O
queuing	O
tasks	O
there	O
are	O
actions	O
such	O
as	O
assigning	O
customers	O
to	O
servers	O
,	O
rejecting	O
customers	O
,	O
or	O
discarding	O
information	O
.	O
in	O
such	O
cases	O
the	O
actions	O
are	O
in	O
fact	O
deﬁned	O
in	O
terms	O
of	O
their	O
immediate	O
eﬀects	O
,	O
which	O
are	O
completely	O
known	O
.	O
it	O
is	O
impossible	O
to	O
describe	O
all	O
the	O
possible	O
kinds	O
of	O
specialized	O
problems	O
and	O
corre-	O
sponding	O
specialized	O
learning	O
algorithms	O
.	O
however	O
,	O
the	O
principles	O
developed	O
in	O
this	O
book	O
should	O
apply	O
widely	O
.	O
for	O
example	O
,	O
afterstate	O
methods	O
are	O
still	O
aptly	O
described	O
in	O
terms	O
of	O
generalized	O
policy	B
iteration	I
,	O
with	O
a	O
policy	B
and	O
(	O
afterstate	O
)	O
value	B
function	I
interacting	O
in	O
essentially	O
the	O
same	O
way	O
.	O
in	O
many	O
cases	O
one	O
will	O
still	O
face	O
the	O
choice	O
between	O
on-policy	O
and	O
oﬀ-policy	B
methods	I
for	O
managing	O
the	O
need	O
for	O
persistent	O
exploration	O
.	O
exercise	O
6.14	O
describe	O
how	O
the	O
task	O
of	O
jack	O
’	O
s	O
car	O
rental	O
(	O
example	O
4.2	O
)	O
could	O
be	O
reformulated	O
in	O
terms	O
of	O
afterstates	O
.	O
why	O
,	O
in	O
terms	O
of	O
this	O
speciﬁc	O
task	O
,	O
would	O
such	O
a	O
(	O
cid:3	O
)	O
reformulation	O
be	O
likely	O
to	O
speed	O
convergence	O
?	O
6.9	O
summary	O
in	O
this	O
chapter	O
we	O
introduced	O
a	O
new	O
kind	O
of	O
learning	O
method	O
,	O
temporal-diﬀerence	O
(	O
td	O
)	O
learning	O
,	O
and	O
showed	O
how	O
it	O
can	O
be	O
applied	O
to	O
the	O
reinforcement	B
learning	I
problem	O
.	O
as	O
usual	O
,	O
we	O
divided	O
the	O
overall	O
problem	O
into	O
a	O
prediction	B
problem	O
and	O
a	O
control	B
problem	O
.	O
td	O
methods	O
are	O
alternatives	O
to	O
monte	O
carlo	O
methods	O
for	O
solving	O
the	O
prediction	B
problem	O
.	O
in	O
both	O
cases	O
,	O
the	O
extension	O
to	O
the	O
control	B
problem	O
is	O
via	O
the	O
idea	O
of	O
generalized	O
policy	B
iteration	I
(	O
gpi	O
)	O
that	O
we	O
abstracted	O
from	O
dynamic	B
programming	I
.	O
this	O
is	O
the	O
idea	O
that	O
xoxxo+xo+xx	O
138	O
chapter	O
6	O
:	O
temporal-diﬀerence	B
learning	I
approximate	O
policy	B
and	O
value	B
functions	O
should	O
interact	O
in	O
such	O
a	O
way	O
that	O
they	O
both	O
move	O
toward	O
their	O
optimal	O
values	O
.	O
one	O
of	O
the	O
two	O
processes	O
making	O
up	O
gpi	O
drives	O
the	O
value	B
function	I
to	O
accurately	O
predict	O
returns	O
for	O
the	O
current	O
policy	B
;	O
this	O
is	O
the	O
prediction	B
problem	O
.	O
the	O
other	O
process	O
drives	O
the	O
policy	B
to	O
improve	O
locally	O
(	O
e.g.	O
,	O
to	O
be	O
ε-greedy	O
)	O
with	O
respect	O
to	O
the	O
current	O
value	B
function	I
.	O
when	O
the	O
ﬁrst	O
process	O
is	O
based	O
on	O
experience	O
,	O
a	O
complication	O
arises	O
concerning	O
maintaining	O
suﬃcient	O
exploration	O
.	O
we	O
can	O
classify	O
td	O
control	B
methods	O
according	O
to	O
whether	O
they	O
deal	O
with	O
this	O
complication	O
by	O
using	O
an	O
on-policy	O
or	O
oﬀ-policy	B
approach	O
.	O
sarsa	O
is	O
an	O
on-policy	O
method	O
,	O
and	O
q-learning	O
is	O
an	O
oﬀ-policy	B
method	O
.	O
expected	O
sarsa	O
is	O
also	O
an	O
oﬀ-policy	B
method	O
as	O
we	O
present	O
it	O
here	O
.	O
there	O
is	O
a	O
third	O
way	O
in	O
which	O
td	O
methods	O
can	O
be	O
extended	O
to	O
control	B
which	O
we	O
did	O
not	O
include	O
in	O
this	O
chapter	O
,	O
called	O
actor–critic	B
methods	O
.	O
these	O
methods	O
are	O
covered	O
in	O
full	O
in	O
chapter	O
13.	O
the	O
methods	O
presented	O
in	O
this	O
chapter	O
are	O
today	O
the	O
most	O
widely	O
used	O
reinforcement	B
learning	I
methods	O
.	O
this	O
is	O
probably	O
due	O
to	O
their	O
great	O
simplicity	O
:	O
they	O
can	O
be	O
applied	O
online	B
,	O
with	O
a	O
minimal	O
amount	O
of	O
computation	O
,	O
to	O
experience	O
generated	O
from	O
interac-	O
tion	B
with	O
an	O
environment	B
;	O
they	O
can	O
be	O
expressed	O
nearly	O
completely	O
by	O
single	O
equations	O
that	O
can	O
be	O
implemented	O
with	O
small	O
computer	O
programs	O
.	O
in	O
the	O
next	O
few	O
chapters	O
we	O
extend	O
these	O
algorithms	O
,	O
making	O
them	O
slightly	O
more	O
complicated	O
and	O
signiﬁcantly	O
more	O
powerful	O
.	O
all	O
the	O
new	O
algorithms	O
will	O
retain	O
the	O
essence	O
of	O
those	O
introduced	O
here	O
:	O
they	O
will	O
be	O
able	O
to	O
process	O
experience	O
online	O
,	O
with	O
relatively	O
little	O
computation	O
,	O
and	O
they	O
will	O
be	O
driven	O
by	O
td	O
errors	O
.	O
the	O
special	O
cases	O
of	O
td	O
methods	O
introduced	O
in	O
the	O
present	O
chapter	O
should	O
rightly	O
be	O
called	O
one-step	O
,	O
tabular	O
,	O
model-free	O
td	O
methods	O
.	O
in	O
the	O
next	O
two	O
chapters	O
we	O
extend	O
them	O
to	O
n-step	B
forms	O
(	O
a	O
link	O
to	O
monte	O
carlo	O
methods	O
)	O
and	O
forms	O
that	O
include	O
a	O
model	B
of	I
the	I
environment	I
(	O
a	O
link	O
to	O
planning	B
and	O
dynamic	O
pro-	O
gramming	O
)	O
.	O
then	O
,	O
in	O
the	O
second	O
part	O
of	O
the	O
book	O
we	O
extend	O
them	O
to	O
various	O
forms	O
of	O
function	O
approximation	O
rather	O
than	O
tables	O
(	O
a	O
link	O
to	O
deep	B
learning	I
and	O
artiﬁcial	B
neural	I
networks	I
)	O
.	O
finally	O
,	O
in	O
this	O
chapter	O
we	O
have	O
discussed	O
td	O
methods	O
entirely	O
within	O
the	O
context	O
of	O
reinforcement	O
learning	O
problems	O
,	O
but	O
td	O
methods	O
are	O
actually	O
more	O
general	O
than	O
this	O
.	O
they	O
are	O
general	O
methods	O
for	O
learning	O
to	O
make	O
long-term	O
predictions	O
about	O
dynami-	O
cal	O
systems	O
.	O
for	O
example	O
,	O
td	O
methods	O
may	O
be	O
relevant	O
to	O
predicting	O
ﬁnancial	O
data	O
,	O
life	O
spans	O
,	O
election	O
outcomes	O
,	O
weather	O
patterns	O
,	O
animal	O
behavior	O
,	O
demands	O
on	O
power	O
stations	O
,	O
or	O
customer	O
purchases	O
.	O
it	O
was	O
only	O
when	O
td	O
methods	O
were	O
analyzed	O
as	O
pure	O
prediction	B
methods	O
,	O
independent	O
of	O
their	O
use	O
in	O
reinforcement	O
learning	O
,	O
that	O
their	O
theoretical	O
prop-	O
erties	O
ﬁrst	O
came	O
to	O
be	O
well	O
understood	O
.	O
even	O
so	O
,	O
these	O
other	O
potential	O
applications	O
of	O
td	O
learning	O
methods	O
have	O
not	O
yet	O
been	O
extensively	O
explored	O
.	O
6.9.	O
summary	O
139	O
bibliographical	O
and	O
historical	O
remarks	O
as	O
we	O
outlined	O
in	O
chapter	O
1	O
,	O
the	O
idea	O
of	O
td	O
learning	O
has	O
its	O
early	O
roots	O
in	B
animal	I
learning	I
psychology	O
and	B
artiﬁcial	I
intelligence	I
,	O
most	O
notably	O
the	O
work	O
of	O
samuel	O
(	O
1959	O
)	O
and	O
klopf	O
(	O
1972	O
)	O
.	O
samuel	O
’	O
s	O
work	O
is	O
described	O
as	O
a	O
case	O
study	O
in	O
section	O
16.2.	O
also	O
related	O
to	O
td	O
learning	O
are	O
holland	O
’	O
s	O
(	O
1975	O
,	O
1976	O
)	O
early	O
ideas	O
about	O
consistency	O
among	O
value	B
predictions	O
.	O
these	O
inﬂuenced	O
one	O
of	O
the	O
authors	O
(	O
barto	O
)	O
,	O
who	O
was	O
a	O
graduate	O
student	O
from	O
1970	O
to	O
1975	O
at	O
the	O
university	O
of	O
michigan	O
,	O
where	O
holland	O
was	O
teaching	O
.	O
holland	O
’	O
s	O
ideas	O
led	O
to	O
a	O
number	O
of	O
td-related	O
systems	O
,	O
including	O
the	O
work	O
of	O
booker	O
(	O
1982	O
)	O
and	O
the	O
bucket	O
brigade	O
of	O
holland	O
(	O
1986	O
)	O
,	O
which	O
is	O
related	O
to	O
sarsa	O
as	O
discussed	O
below	O
.	O
6.1–2	O
most	O
of	O
the	O
speciﬁc	O
material	O
from	O
these	O
sections	O
is	O
from	O
sutton	O
(	O
1988	O
)	O
,	O
includ-	O
ing	B
the	O
td	O
(	O
0	O
)	O
algorithm	O
,	O
the	O
random	B
walk	I
example	O
,	O
and	O
the	O
term	O
“	O
temporal-	O
diﬀerence	O
learning.	O
”	O
the	O
characterization	O
of	O
the	O
relationship	O
to	O
dynamic	O
pro-	O
gramming	O
and	O
monte	O
carlo	O
methods	O
was	O
inﬂuenced	O
by	O
watkins	O
(	O
1989	O
)	O
,	O
werbos	O
(	O
1987	O
)	O
,	O
and	O
others	O
.	O
the	O
use	O
of	O
backup	O
diagrams	O
was	O
new	O
to	O
the	O
ﬁrst	O
edition	O
of	O
this	O
book	O
.	O
6.3	O
6.4	O
tabular	O
td	O
(	O
0	O
)	O
was	O
proved	O
to	O
converge	O
in	O
the	O
mean	O
by	O
sutton	O
(	O
1988	O
)	O
and	O
with	O
probability	O
1	O
by	O
dayan	O
(	O
1992	O
)	O
,	O
based	O
on	O
the	O
work	O
of	O
watkins	O
and	O
dayan	O
(	O
1992	O
)	O
.	O
these	O
results	O
were	O
extended	O
and	O
strengthened	O
by	O
jaakkola	O
,	O
jordan	O
,	O
and	O
singh	O
(	O
1994	O
)	O
and	O
tsitsiklis	O
(	O
1994	O
)	O
by	O
using	O
extensions	O
of	O
the	O
powerful	O
existing	O
theory	O
of	O
stochastic	O
approximation	O
.	O
other	O
extensions	O
and	O
generalizations	O
are	O
covered	O
in	O
later	O
chapters	O
.	O
the	O
optimality	B
of	I
the	O
td	O
algorithm	O
under	O
batch	O
training	O
was	O
established	O
by	O
sutton	O
(	O
1988	O
)	O
.	O
illuminating	O
this	O
result	O
is	O
barnard	O
’	O
s	O
(	O
1993	O
)	O
derivation	O
of	O
the	O
td	O
algorithm	O
as	O
a	O
combination	O
of	O
one	O
step	O
of	O
an	O
incremental	O
method	O
for	O
learning	O
a	O
model	O
of	O
the	O
markov	O
chain	O
and	O
one	O
step	O
of	O
a	O
method	O
for	O
computing	O
predictions	O
from	O
the	O
model	O
.	O
the	O
term	O
certainty	O
equivalence	O
is	O
from	O
the	O
adaptive	O
control	B
literature	O
(	O
e.g.	O
,	O
goodwin	O
and	O
sin	O
,	O
1984	O
)	O
.	O
the	O
sarsa	O
algorithm	O
was	O
introduced	O
by	O
rummery	O
and	O
niranjan	O
(	O
1994	O
)	O
.	O
they	O
explored	O
it	O
in	O
conjunction	O
with	O
neural	O
networks	O
and	O
called	O
it	O
“	O
modiﬁed	O
con-	O
nectionist	O
q-learning	O
”	O
.	O
the	O
name	O
“	O
sarsa	O
”	O
was	O
introduced	O
by	O
sutton	O
(	O
1996	O
)	O
.	O
the	O
convergence	O
of	O
one-step	O
tabular	O
sarsa	O
(	O
the	O
form	O
treated	O
in	O
this	O
chapter	O
)	O
has	O
been	O
proved	O
by	O
singh	O
,	O
jaakkola	O
,	O
littman	O
,	O
and	O
szepesv´ari	O
(	O
2000	O
)	O
.	O
the	O
“	O
windy	B
gridworld	O
”	O
example	O
was	O
suggested	O
by	O
tom	O
kalt	O
.	O
holland	O
’	O
s	O
(	O
1986	O
)	O
bucket	O
brigade	O
idea	O
evolved	O
into	O
an	O
algorithm	O
closely	O
related	O
to	O
sarsa	O
.	O
the	O
original	O
idea	O
of	O
the	O
bucket	O
brigade	O
involved	O
chains	O
of	O
rules	O
triggering	O
each	O
other	O
;	O
it	O
focused	O
on	O
passing	O
credit	O
back	O
from	O
the	O
current	O
rule	O
to	O
the	O
rules	O
that	O
triggered	O
it	O
.	O
over	O
time	O
,	O
the	O
bucket	O
brigade	O
came	O
to	O
be	O
more	O
like	O
td	O
learning	O
in	O
passing	O
credit	O
back	O
to	O
any	O
temporally	O
preceding	O
rule	O
,	O
not	O
just	O
to	O
the	O
ones	O
that	O
triggered	O
the	O
current	O
rule	O
.	O
the	O
modern	O
form	O
of	O
the	O
bucket	O
brigade	O
,	O
when	O
simpliﬁed	O
in	O
various	O
natural	O
ways	O
,	O
is	O
nearly	O
identical	O
to	O
one-step	O
sarsa	O
,	O
as	O
detailed	O
by	O
wilson	O
(	O
1994	O
)	O
.	O
140	O
6.5	O
6.6	O
6.7	O
6.8	O
chapter	O
6	O
:	O
temporal-diﬀerence	B
learning	I
q-learning	O
was	O
introduced	O
by	O
watkins	O
(	O
1989	O
)	O
,	O
whose	O
outline	O
of	O
a	O
convergence	O
proof	B
was	O
made	O
rigorous	O
by	O
watkins	O
and	O
dayan	O
(	O
1992	O
)	O
.	O
more	O
general	O
conver-	O
gence	O
results	O
were	O
proved	O
by	O
jaakkola	O
,	O
jordan	O
,	O
and	O
singh	O
(	O
1994	O
)	O
and	O
tsitsiklis	O
(	O
1994	O
)	O
.	O
the	O
expected	O
sarsa	O
algorithm	O
was	O
introduced	O
by	O
george	O
john	O
(	O
1994	O
)	O
,	O
who	O
called	O
it	O
“	O
q-learning	O
”	O
and	O
stressed	O
its	O
advantages	O
over	O
q-learning	O
as	O
an	O
oﬀ-	O
policy	B
algorithm	O
.	O
john	O
’	O
s	O
work	O
was	O
not	O
known	O
to	O
us	O
when	O
we	O
presented	O
expected	O
sarsa	O
in	O
the	O
ﬁrst	O
edition	O
of	O
this	O
book	O
as	O
an	O
exercise	O
,	O
or	O
to	O
van	O
seijen	O
,	O
van	O
hasselt	O
,	O
whiteson	O
,	O
and	O
weiring	O
(	O
2009	O
)	O
when	O
they	O
established	O
expected	O
sarsa	O
’	O
s	O
convergence	O
properties	O
and	O
conditions	O
under	O
which	O
it	O
will	O
outperform	O
regular	O
sarsa	O
and	O
q-learning	O
.	O
our	O
figure	O
6.3	O
is	O
adapted	O
from	O
their	O
results	O
.	O
van	O
seijen	O
et	O
al	O
.	O
deﬁned	O
“	O
expected	O
sarsa	O
”	O
to	O
be	O
an	O
on-policy	O
method	O
exclusively	O
(	O
as	O
we	O
did	O
in	O
the	O
ﬁrst	O
edition	O
)	O
,	O
whereas	O
now	O
we	O
use	O
this	O
name	O
for	O
the	O
general	O
algorithm	O
in	O
which	O
the	O
target	B
and	O
behavior	O
policies	O
may	O
diﬀer	O
.	O
the	O
general	O
oﬀ-policy	O
view	O
of	O
expected	O
sarsa	O
was	O
noted	O
by	O
van	O
hasselt	O
(	O
2011	O
)	O
,	O
who	O
called	O
it	O
“	O
general	O
q-learning.	O
”	O
maximization	B
bias	I
and	O
double	B
learning	I
were	O
introduced	O
and	O
extensively	O
investi-	O
gated	O
by	O
van	O
hasselt	O
(	O
2010	O
,	O
2011	O
)	O
.	O
the	O
example	O
mdp	O
in	O
figure	O
6.5	O
was	O
adapted	O
from	O
that	O
in	O
his	O
figure	O
4.1	O
(	O
van	O
hasselt	O
,	O
2011	O
)	O
.	O
the	O
notion	O
of	O
an	O
afterstate	O
is	O
the	O
same	O
as	O
that	O
of	O
a	O
“	O
post-decision	O
state	B
”	O
(	O
van	O
roy	O
,	O
bertsekas	O
,	O
lee	O
,	O
and	O
tsitsiklis	O
,	O
1997	O
;	O
powell	O
,	O
2011	O
)	O
.	O
chapter	O
7	O
n-step	B
bootstrapping	O
in	O
this	O
chapter	O
we	O
unify	O
the	O
monte	O
carlo	O
(	O
mc	O
)	O
methods	O
and	O
the	O
one-step	O
temporal-	O
diﬀerence	O
(	O
td	O
)	O
methods	O
presented	O
in	O
the	O
previous	O
two	O
chapters	O
.	O
neither	O
mc	O
methods	O
nor	O
one-step	O
td	O
methods	O
are	O
always	O
the	O
best	O
.	O
in	O
this	O
chapter	O
we	O
present	O
n-step	B
td	O
meth-	O
ods	O
that	O
generalize	O
both	O
methods	O
so	O
that	O
one	O
can	O
shift	O
from	O
one	O
to	O
the	O
other	O
smoothly	O
as	O
needed	O
to	O
meet	O
the	O
demands	O
of	O
a	O
particular	O
task	O
.	O
n-step	B
methods	I
span	O
a	O
spectrum	O
with	O
mc	O
methods	O
at	O
one	O
end	O
and	O
one-step	O
td	O
methods	O
at	O
the	O
other	O
.	O
the	O
best	O
methods	O
are	O
often	O
intermediate	O
between	O
the	O
two	O
extremes	O
.	O
another	O
way	O
of	O
looking	O
at	O
the	O
beneﬁts	O
of	O
n-step	O
methods	O
is	O
that	O
they	O
free	O
you	O
from	O
the	O
tyranny	O
of	O
the	O
time	O
step	O
.	O
with	O
one-step	O
td	O
methods	O
the	O
same	O
time	O
step	O
determines	O
how	O
often	O
the	O
action	B
can	O
be	O
changed	O
and	O
the	O
time	O
interval	O
over	O
which	O
bootstrapping	B
is	O
done	O
.	O
in	O
many	O
applications	O
one	O
wants	O
to	O
be	O
able	O
to	O
update	O
the	O
action	B
very	O
fast	O
to	O
take	O
into	O
account	O
anything	O
that	O
has	O
changed	O
,	O
but	O
bootstrapping	B
works	O
best	O
if	O
it	O
is	O
over	O
a	O
length	O
of	O
time	O
in	O
which	O
a	O
signiﬁcant	O
and	O
recognizable	O
state	B
change	O
has	O
occurred	O
.	O
with	O
one-step	O
td	O
methods	O
,	O
these	O
time	O
intervals	O
are	O
the	O
same	O
,	O
and	O
so	O
a	O
compromise	O
must	O
be	O
made	O
.	O
n-step	B
methods	I
enable	O
bootstrapping	B
to	O
occur	O
over	O
multiple	O
steps	O
,	O
freeing	O
us	O
from	O
the	O
tyranny	O
of	O
the	O
single	O
time	O
step	O
.	O
the	O
idea	O
of	O
n-step	O
methods	O
is	O
usually	O
used	O
as	O
an	O
introduction	O
to	O
the	O
algorithmic	O
idea	O
of	O
eligibility	O
traces	O
(	O
chapter	O
12	O
)	O
,	O
which	O
enable	O
bootstrapping	B
over	O
multiple	O
time	O
intervals	O
simultaneously	O
.	O
here	O
we	O
instead	O
consider	O
the	O
n-step	B
bootstrapping	O
idea	O
on	O
its	O
own	O
,	O
postponing	O
the	O
treatment	O
of	O
eligibility-trace	O
mechanisms	O
until	O
later	O
.	O
this	O
allows	O
us	O
to	O
separate	O
the	O
issues	O
better	O
,	O
dealing	O
with	O
as	O
many	O
of	O
them	O
as	O
possible	O
in	O
the	O
simpler	O
n-step	B
setting	O
.	O
as	O
usual	O
,	O
we	O
ﬁrst	O
consider	O
the	O
prediction	B
problem	O
and	O
then	O
the	O
control	B
problem	O
.	O
that	O
is	O
,	O
we	O
ﬁrst	O
consider	O
how	O
n-step	B
methods	I
can	O
help	O
in	O
predicting	O
returns	O
as	O
a	O
function	O
of	O
state	B
for	O
a	O
ﬁxed	O
policy	B
(	O
i.e.	O
,	O
in	O
estimating	O
vπ	O
)	O
.	O
then	O
we	O
extend	O
the	O
ideas	O
to	O
action	B
values	O
and	B
control	I
methods	O
.	O
141	O
142	O
chapter	O
7	O
:	O
n-step	B
bootstrapping	O
7.1	O
n-step	B
td	O
prediction	B
what	O
is	O
the	O
space	O
of	O
methods	O
lying	O
between	O
monte	O
carlo	O
and	O
td	O
methods	O
?	O
consider	O
estimating	O
vπ	O
from	O
sample	O
episodes	O
generated	O
using	O
π.	O
monte	O
carlo	O
methods	O
perform	O
an	O
update	O
for	O
each	O
state	B
based	O
on	O
the	O
entire	O
sequence	O
of	O
observed	O
rewards	O
from	O
that	O
state	B
until	O
the	O
end	O
of	O
the	O
episode	O
.	O
the	O
update	O
of	O
one-step	O
td	O
methods	O
,	O
on	O
the	O
other	O
hand	O
,	O
is	O
based	O
on	O
just	O
the	O
one	O
next	O
reward	O
,	O
bootstrapping	B
from	O
the	O
value	B
of	O
the	O
state	B
one	O
step	O
later	O
as	O
a	O
proxy	O
for	O
the	O
remaining	O
rewards	O
.	O
one	O
kind	O
of	O
intermediate	O
method	O
,	O
then	O
,	O
would	O
perform	O
an	O
update	O
based	O
on	O
an	O
intermediate	O
number	O
of	O
rewards	O
:	O
more	O
than	O
one	O
,	O
but	O
less	O
than	O
all	O
of	O
them	O
until	O
termination	O
.	O
for	O
example	O
,	O
a	O
two-step	O
update	O
would	O
be	O
based	O
on	O
the	O
ﬁrst	O
two	O
rewards	O
and	O
the	O
estimated	O
value	B
of	O
the	O
state	B
two	O
steps	O
later	O
.	O
similarly	O
,	O
we	O
could	O
have	O
three-step	O
updates	O
,	O
four-step	O
updates	O
,	O
and	O
so	O
on	O
.	O
figure	O
7.1	O
shows	O
the	O
backup	O
diagrams	O
of	O
the	O
spectrum	O
of	O
n-step	O
updates	O
for	B
vπ	I
,	O
with	O
the	O
one-step	O
td	O
update	O
on	O
the	O
left	O
and	O
the	O
up-until-termination	O
monte	O
carlo	O
update	O
on	O
the	O
right	O
.	O
figure	O
7.1	O
:	O
the	O
backup	O
diagrams	O
of	O
n-step	O
methods	O
.	O
these	O
methods	O
form	O
a	O
spectrum	O
ranging	O
from	O
one-step	O
td	O
methods	O
to	O
monte	O
carlo	O
methods	O
.	O
the	O
methods	O
that	O
use	O
n-step	B
updates	O
are	O
still	O
td	O
methods	O
because	O
they	O
still	O
change	O
an	O
earlier	O
estimate	O
based	O
on	O
how	O
it	O
diﬀers	O
from	O
a	O
later	O
estimate	O
.	O
now	O
the	O
later	O
estimate	O
is	O
not	O
one	O
step	O
later	O
,	O
but	O
n	O
steps	O
later	O
.	O
methods	O
in	O
which	O
the	O
temporal	O
diﬀerence	O
extends	O
over	O
n	O
steps	O
are	O
called	O
n-step	B
td	O
methods	O
.	O
the	O
td	O
methods	O
introduced	O
in	O
the	O
previous	O
chapter	O
all	O
used	O
one-step	O
updates	O
,	O
which	O
is	O
why	O
we	O
called	O
them	O
one-step	O
td	O
methods	O
.	O
more	O
formally	O
,	O
consider	O
the	O
update	O
of	O
the	O
estimated	O
value	B
of	O
state	B
st	O
as	O
a	O
result	O
of	O
the	O
state–reward	O
sequence	O
,	O
st	O
,	O
rt+1	O
,	O
st+1	O
,	O
rt+2	O
,	O
.	O
.	O
.	O
,	O
rt	O
,	O
st	O
(	O
omitting	O
the	O
actions	O
)	O
.	O
we	O
know	O
that	O
in	O
monte	O
carlo	O
updates	O
the	O
estimate	O
of	O
vπ	O
(	O
st	O
)	O
is	O
updated	O
in	O
the	O
direction	O
of	O
1-step	O
tdand	O
td	O
(	O
0	O
)	O
2-step	O
td3-step	O
tdn-step	O
td∞-step	O
tdand	O
monte	O
carlo············	O
7.1.	O
n-step	B
td	O
prediction	B
143	O
the	O
complete	O
return	B
:	O
gt	O
.	O
=	O
rt+1	O
+	O
γrt+2	O
+	O
γ2rt+3	O
+	O
···	O
+	O
γt−t−1rt	O
,	O
where	O
t	O
is	O
the	O
last	O
time	O
step	O
of	O
the	O
episode	O
.	O
let	O
us	O
call	O
this	O
quantity	O
the	O
target	B
of	O
the	O
update	O
.	O
whereas	O
in	O
monte	O
carlo	O
updates	O
the	O
target	B
is	O
the	O
return	B
,	O
in	O
one-step	O
updates	O
the	O
target	B
is	O
the	O
ﬁrst	O
reward	O
plus	O
the	O
discounted	O
estimated	O
value	B
of	O
the	O
next	O
state	B
,	O
which	O
we	O
call	O
the	O
one-step	O
return	O
:	O
gt	O
:	O
t+1	O
.	O
=	O
rt+1	O
+	O
γvt	O
(	O
st+1	O
)	O
,	O
where	O
vt	O
:	O
s	O
→	O
r	O
here	O
is	O
the	O
estimate	O
at	O
time	O
t	O
of	O
vπ	O
.	O
the	O
subscripts	O
on	O
gt	O
:	O
t+1	O
indicate	O
that	O
it	O
is	O
a	O
truncated	B
return	O
for	O
time	O
t	O
using	O
rewards	O
up	O
until	O
time	O
t	O
+	O
1	O
,	O
with	O
the	O
discounted	O
estimate	O
γvt	O
(	O
st+1	O
)	O
taking	O
the	O
place	O
of	O
the	O
other	O
terms	O
γrt+2	O
+	O
γ2rt+3	O
+	O
···	O
+	O
γt−t−1rt	O
of	O
the	O
full	O
return	B
,	O
as	O
discussed	O
in	O
the	O
previous	O
chapter	O
.	O
our	O
point	O
now	O
is	O
that	O
this	O
idea	O
makes	O
just	O
as	O
much	O
sense	O
after	O
two	O
steps	O
as	O
it	O
does	O
after	O
one	O
.	O
the	O
target	B
for	O
a	O
two-step	O
update	O
is	O
the	O
two-step	O
return	B
:	O
gt	O
:	O
t+2	O
.	O
=	O
rt+1	O
+	O
γrt+2	O
+	O
γ2vt+1	O
(	O
st+2	O
)	O
,	O
where	O
now	O
γ2vt+1	O
(	O
st+2	O
)	O
corrects	O
for	O
the	O
absence	O
of	O
the	O
terms	O
γ2rt+3	O
+	O
γ3rt+4	O
+	O
···	O
+	O
γt−t−1rt	O
.	O
similarly	O
,	O
the	O
target	B
for	O
an	O
arbitrary	O
n-step	B
update	O
is	O
the	O
n-step	B
return	O
:	O
gt	O
:	O
t+n	O
.	O
=	O
rt+1	O
+	O
γrt+2	O
+	O
···	O
+	O
γn−1rt+n	O
+	O
γnvt+n−1	O
(	O
st+n	O
)	O
,	O
(	O
7.1	O
)	O
for	O
all	O
n	O
,	O
t	O
such	O
that	O
n	O
≥	O
1	O
and	O
0	O
≤	O
t	O
<	O
t	O
−	O
n.	O
all	O
n-step	B
returns	O
can	O
be	O
considered	O
approximations	O
to	O
the	O
full	O
return	B
,	O
truncated	B
after	O
n	O
steps	O
and	O
then	O
corrected	O
for	O
the	O
remaining	O
missing	O
terms	O
by	O
vt+n−1	O
(	O
st+n	O
)	O
.	O
if	O
t	O
+	O
n	O
≥	O
t	O
(	O
if	O
the	O
n-step	B
return	O
extends	O
to	O
or	O
beyond	O
termination	O
)	O
,	O
then	O
all	O
the	O
missing	O
terms	O
are	O
taken	O
as	O
zero	O
,	O
and	O
the	O
n-step	O
return	B
deﬁned	O
to	O
be	O
equal	O
to	O
the	O
ordinary	O
full	O
return	B
(	O
gt	O
:	O
t+n	O
.	O
=	O
gt	O
if	O
t	O
+	O
n	O
≥	O
t	O
)	O
.	O
note	O
that	O
n-step	B
returns	O
for	O
n	O
>	O
1	O
involve	O
future	O
rewards	O
and	O
states	O
that	O
are	O
not	O
available	O
at	O
the	O
time	O
of	O
transition	O
from	O
t	O
to	O
t	O
+	O
1.	O
no	O
real	O
algorithm	O
can	O
use	O
the	O
n-	O
step	O
return	B
until	O
after	O
it	O
has	O
seen	O
rt+n	O
and	O
computed	O
vt+n−1	O
.	O
the	O
ﬁrst	O
time	O
these	O
are	O
available	O
is	O
t	O
+	O
n.	O
the	O
natural	O
state-value	O
learning	O
algorithm	O
for	O
using	O
n-step	B
returns	O
is	O
thus	O
vt+n	O
(	O
st	O
)	O
.	O
=	O
vt+n−1	O
(	O
st	O
)	O
+	O
α	O
(	O
cid:2	O
)	O
gt	O
:	O
t+n	O
−	O
vt+n−1	O
(	O
st	O
)	O
(	O
cid:3	O
)	O
,	O
0	O
≤	O
t	O
<	O
t	O
,	O
(	O
7.2	O
)	O
while	O
the	O
values	O
of	O
all	O
other	O
states	O
remain	O
unchanged	O
:	O
vt+n	O
(	O
s	O
)	O
=	O
vt+n−1	O
(	O
s	O
)	O
,	O
for	O
all	O
s	O
(	O
cid:54	O
)	O
=	O
st.	O
we	O
call	O
this	O
algorithm	O
n-step	B
td	O
.	O
note	O
that	O
no	O
changes	O
at	O
all	O
are	O
made	O
during	O
the	O
ﬁrst	O
n−	O
1	O
steps	O
of	O
each	O
episode	O
.	O
to	O
make	O
up	O
for	O
that	O
,	O
an	O
equal	O
number	O
of	O
additional	O
updates	O
are	O
made	O
at	O
the	O
end	O
of	O
the	O
episode	O
,	O
after	O
termination	O
and	O
before	O
starting	O
the	O
next	O
episode	O
.	O
complete	O
pseudocode	O
is	O
given	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
.	O
exercise	O
7.1	O
in	O
chapter	O
6	O
we	O
noted	O
that	O
the	O
monte	O
carlo	O
error	O
can	O
be	O
written	O
as	O
the	O
sum	O
of	O
td	O
errors	O
(	O
6.6	O
)	O
if	O
the	O
value	B
estimates	O
don	O
’	O
t	O
change	O
from	O
step	O
to	O
step	O
.	O
show	O
that	O
the	O
n-step	B
error	O
used	O
in	O
(	O
7.2	O
)	O
can	O
also	O
be	O
written	O
as	O
a	O
sum	O
td	O
errors	O
(	O
again	O
if	O
the	O
value	B
(	O
cid:3	O
)	O
estimates	O
don	O
’	O
t	O
change	O
)	O
generalizing	O
the	O
earlier	O
result	O
.	O
exercise	O
7.2	O
(	O
programming	O
)	O
with	O
an	O
n-step	B
method	O
,	O
the	O
value	B
estimates	O
do	O
change	O
from	O
step	O
to	O
step	O
,	O
so	O
an	O
algorithm	O
that	O
used	O
the	O
sum	O
of	O
td	O
errors	O
(	O
see	O
previous	O
exercise	O
)	O
in	O
144	O
chapter	O
7	O
:	O
n-step	B
bootstrapping	O
n-step	B
td	O
for	O
estimating	O
v	O
≈	O
vπ	O
input	O
:	O
a	O
policy	B
π	O
algorithm	O
parameters	O
:	O
step	O
size	O
α	O
∈	O
(	O
0	O
,	O
1	O
]	O
,	O
a	O
positive	O
integer	O
n	O
initialize	O
v	O
(	O
s	O
)	O
arbitrarily	O
,	O
for	O
all	O
s	O
∈	O
s	O
all	O
store	O
and	O
access	O
operations	O
(	O
for	O
st	O
and	O
rt	O
)	O
can	O
take	O
their	O
index	O
mod	O
n	O
+	O
1	O
loop	O
for	O
each	O
episode	O
:	O
take	O
an	O
action	B
according	O
to	O
π	O
(	O
·|st	O
)	O
observe	O
and	O
store	O
the	O
next	O
reward	O
as	O
rt+1	O
and	O
the	O
next	O
state	B
as	O
st+1	O
if	O
st+1	O
is	O
terminal	O
,	O
then	O
t	O
←	O
t	O
+	O
1	O
(	O
τ	O
is	O
the	O
time	O
whose	O
state	B
’	O
s	O
estimate	O
is	O
being	O
updated	O
)	O
if	O
t	O
<	O
t	O
,	O
then	O
:	O
initialize	O
and	O
store	O
s0	O
(	O
cid:54	O
)	O
=	O
terminal	O
t	O
←	O
∞loop	O
for	O
t	O
=	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
:	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
until	O
τ	O
=	O
t	O
−	O
1	O
g	O
←	O
(	O
cid:80	O
)	O
min	O
(	O
τ	O
+n	O
,	O
t	O
)	O
τ	O
←	O
t	O
−	O
n	O
+	O
1	O
if	O
τ	O
≥	O
0	O
:	O
γi−τ−1ri	O
i=τ	O
+1	O
if	O
τ	O
+	O
n	O
<	O
t	O
,	O
then	O
:	O
g	O
←	O
g	O
+	O
γnv	O
(	O
sτ	O
+n	O
)	O
v	O
(	O
sτ	O
)	O
←	O
v	O
(	O
sτ	O
)	O
+	O
α	O
[	O
g	O
−	O
v	O
(	O
sτ	O
)	O
]	O
(	O
gτ	O
:	O
τ	O
+n	O
)	O
place	O
of	O
the	O
error	O
in	O
(	O
7.2	O
)	O
would	O
actually	O
be	O
a	O
slightly	O
diﬀerent	O
algorithm	O
.	O
would	O
it	O
be	O
a	O
better	O
algorithm	O
or	O
a	O
worse	O
one	O
?	O
devise	O
and	O
program	O
a	O
small	O
experiment	O
to	O
answer	O
(	O
cid:3	O
)	O
this	O
question	O
empirically	O
.	O
the	O
n-step	B
return	O
uses	O
the	O
value	B
function	I
vt+n−1	O
to	O
correct	O
for	O
the	O
missing	O
rewards	O
beyond	O
rt+n	O
.	O
an	O
important	O
property	O
of	O
n-step	O
returns	O
is	O
that	O
their	O
expectation	O
is	O
guaranteed	O
to	O
be	O
a	O
better	O
estimate	O
of	O
vπ	O
than	O
vt+n−1	O
is	O
,	O
in	O
a	O
worst-state	O
sense	O
.	O
that	O
is	O
,	O
the	O
worst	O
error	O
of	O
the	O
expected	O
n-step	O
return	B
is	O
guaranteed	O
to	O
be	O
less	O
than	O
or	O
equal	O
to	O
γn	O
times	O
the	O
worst	O
error	O
under	O
vt+n−1	O
:	O
max	O
s	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
eπ	O
[	O
gt	O
:	O
t+n|st	O
=	O
s	O
]	O
−	O
vπ	O
(	O
s	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
≤	O
γn	O
max	O
s	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
vt+n−1	O
(	O
s	O
)	O
−	O
vπ	O
(	O
s	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
,	O
for	O
all	O
n	O
≥	O
1.	O
this	O
is	O
called	O
the	O
error	B
reduction	I
property	I
of	O
n-step	B
returns	O
.	O
because	O
of	O
the	O
error	B
reduction	I
property	I
,	O
one	O
can	O
show	O
formally	O
that	O
all	O
n-step	B
td	O
methods	O
converge	O
to	O
the	O
correct	O
predictions	O
under	O
appropriate	O
technical	O
conditions	O
.	O
the	O
n-step	B
td	O
methods	O
thus	O
form	O
a	O
family	O
of	O
sound	O
methods	O
,	O
with	O
one-step	O
td	O
methods	O
and	O
monte	O
carlo	O
methods	O
as	O
extreme	O
members	O
.	O
(	O
7.3	O
)	O
example	O
7.1	O
:	O
n-step	B
td	O
methods	O
on	O
the	O
random	B
walk	I
consider	O
using	O
n-step	B
td	O
methods	O
on	O
the	O
5-state	B
random	O
walk	O
task	O
described	O
in	O
example	O
6.2	O
(	O
page	O
125	O
)	O
.	O
suppose	O
the	O
ﬁrst	O
episode	O
progressed	O
directly	O
from	O
the	O
center	O
state	B
,	O
c	O
,	O
to	O
the	O
right	O
,	O
through	O
d	O
and	O
e	O
,	O
and	O
then	O
terminated	O
on	O
the	O
right	O
with	O
a	O
return	B
of	O
1.	O
recall	O
that	O
the	O
estimated	O
values	O
of	O
all	O
the	O
states	O
started	O
at	O
an	O
intermediate	O
value	B
,	O
v	O
(	O
s	O
)	O
=	O
0.5.	O
as	O
a	O
result	O
of	O
this	O
experience	O
,	O
a	O
one-step	O
method	O
would	O
change	O
only	O
the	O
estimate	O
for	O
the	O
last	O
state	B
,	O
7.2.	O
n-step	B
sarsa	O
145	O
v	O
(	O
e	O
)	O
,	O
which	O
would	O
be	O
incremented	O
toward	O
1	O
,	O
the	O
observed	O
return	B
.	O
a	O
two-step	O
method	O
,	O
on	O
the	O
other	O
hand	O
,	O
would	O
increment	O
the	O
values	O
of	O
the	O
two	O
states	O
preceding	O
termination	O
:	O
v	O
(	O
d	O
)	O
and	O
v	O
(	O
e	O
)	O
both	O
would	O
be	O
incremented	O
toward	O
1.	O
a	O
three-step	O
method	O
,	O
or	O
any	O
n-step	B
method	O
for	O
n	O
>	O
2	O
,	O
would	O
increment	O
the	O
values	O
of	O
all	O
three	O
of	O
the	O
visited	O
states	O
toward	O
1	O
,	O
all	O
by	O
the	O
same	O
amount	O
.	O
figure	O
7.2	O
:	O
performance	O
of	O
n-step	O
td	O
methods	O
as	O
a	O
function	O
of	O
α	O
,	O
for	O
various	O
values	O
of	O
n	O
,	O
on	O
a	O
19-state	B
random	O
walk	O
task	O
(	O
example	O
7.1	O
)	O
.	O
which	O
value	B
of	O
n	O
is	O
better	O
?	O
figure	O
7.2	O
shows	O
the	O
results	O
of	O
a	O
simple	O
empirical	O
test	O
for	O
a	O
larger	O
random	B
walk	I
process	O
,	O
with	O
19	O
states	O
instead	O
of	O
5	O
(	O
and	O
with	O
a	O
−1	O
outcome	O
on	O
the	O
left	O
,	O
all	O
values	O
initialized	O
to	O
0	O
)	O
,	O
which	O
we	O
use	O
as	O
a	O
running	O
example	O
in	O
this	O
chapter	O
.	O
results	O
are	O
shown	O
for	O
n-step	O
td	O
methods	O
with	O
a	O
range	O
of	O
values	O
for	O
n	O
and	O
α.	O
the	O
performance	O
measure	O
for	O
each	O
parameter	O
setting	O
,	O
shown	O
on	O
the	O
vertical	O
axis	O
,	O
is	O
the	O
square-root	O
of	O
the	O
average	O
squared	O
error	O
between	O
the	O
predictions	O
at	O
the	O
end	O
of	O
the	O
episode	O
for	O
the	O
19	O
states	O
and	O
their	O
true	O
values	O
,	O
then	O
averaged	O
over	O
the	O
ﬁrst	O
10	O
episodes	B
and	O
100	O
repetitions	O
of	O
the	O
whole	O
experiment	O
(	O
the	O
same	O
sets	O
of	O
walks	O
were	O
used	O
for	O
all	O
parameter	O
settings	O
)	O
.	O
note	O
that	O
methods	O
with	O
an	O
intermediate	O
value	B
of	O
n	O
worked	O
best	O
.	O
this	O
illustrates	O
how	O
the	O
generalization	O
of	O
td	O
and	O
monte	O
carlo	O
methods	O
to	O
n-step	B
methods	I
can	O
potentially	O
perform	O
better	O
than	O
either	O
of	O
the	O
two	O
extreme	O
methods	O
.	O
exercise	O
7.3	O
why	O
do	O
you	O
think	O
a	O
larger	O
random	B
walk	I
task	O
(	O
19	O
states	O
instead	O
of	O
5	O
)	O
was	O
used	O
in	O
the	O
examples	O
of	O
this	O
chapter	O
?	O
would	O
a	O
smaller	O
walk	O
have	O
shifted	O
the	O
advantage	O
to	O
a	O
diﬀerent	O
value	B
of	O
n	O
?	O
how	O
about	O
the	O
change	O
in	O
left-side	O
outcome	O
from	O
0	O
to	O
−1	O
made	O
(	O
cid:3	O
)	O
in	O
the	O
larger	O
walk	O
?	O
do	O
you	O
think	O
that	O
made	O
any	O
diﬀerence	O
in	O
the	O
best	O
value	B
of	O
n	O
?	O
7.2	O
n-step	B
sarsa	O
how	O
can	O
n-step	B
methods	I
be	O
used	O
not	O
just	O
for	O
prediction	O
,	O
but	O
for	O
control	O
?	O
in	O
this	O
section	O
we	O
show	O
how	O
n-step	B
methods	I
can	O
be	O
combined	O
with	O
sarsa	O
in	O
a	O
straightforward	O
way	O
to	O
↵averagerms	O
errorover	O
19	O
statesand	O
ﬁrst	O
10	O
episodesn=1n=2n=4n=8n=16n=32n=32n=641285122560.550.50.450.350.30.250.40.40.200.80.61	O
146	O
chapter	O
7	O
:	O
n-step	B
bootstrapping	O
produce	O
an	O
on-policy	O
td	O
control	B
method	O
.	O
the	O
n-step	B
version	O
of	O
sarsa	O
we	O
call	O
n-step	B
sarsa	O
,	O
and	O
the	O
original	O
version	O
presented	O
in	O
the	O
previous	O
chapter	O
we	O
henceforth	O
call	O
one-step	O
sarsa	O
,	O
or	O
sarsa	O
(	O
0	O
)	O
.	O
the	O
main	O
idea	O
is	O
to	O
simply	O
switch	O
states	O
for	O
actions	O
(	O
state–action	O
pairs	O
)	O
and	O
then	O
use	O
an	O
ε-greedy	O
policy	O
.	O
the	O
backup	O
diagrams	O
for	O
n-step	O
sarsa	O
(	O
shown	O
in	O
figure	O
7.3	O
)	O
,	O
like	O
those	O
of	O
n-step	O
td	O
(	O
figure	O
7.1	O
)	O
,	O
are	O
strings	O
of	O
alternating	O
states	O
and	O
actions	O
,	O
except	O
that	O
the	O
sarsa	O
ones	O
all	O
start	O
and	O
end	O
with	O
an	O
action	B
rather	O
a	O
state	B
.	O
we	O
redeﬁne	O
n-step	B
returns	O
(	O
update	O
targets	O
)	O
in	O
terms	O
of	O
estimated	O
action	B
values	O
:	O
gt	O
:	O
t+n	O
.	O
=	O
rt+1	O
+γrt+2	O
+···+γn−1rt+n	O
+γnqt+n−1	O
(	O
st+n	O
,	O
at+n	O
)	O
,	O
n	O
≥	O
1	O
,	O
0	O
≤	O
t	O
<	O
t	O
−n	O
,	O
(	O
7.4	O
)	O
with	O
gt	O
:	O
t+n	O
.	O
=	O
gt	O
if	O
t	O
+	O
n	O
≥	O
t	O
.	O
the	O
natural	O
algorithm	O
is	O
then	O
qt+n	O
(	O
st	O
,	O
at	O
)	O
.	O
=	O
qt+n−1	O
(	O
st	O
,	O
at	O
)	O
+	O
α	O
[	O
gt	O
:	O
t+n	O
−	O
qt+n−1	O
(	O
st	O
,	O
at	O
)	O
]	O
,	O
0	O
≤	O
t	O
<	O
t	O
,	O
(	O
7.5	O
)	O
while	O
the	O
values	O
of	O
all	O
other	O
states	O
remain	O
unchanged	O
:	O
qt+n	O
(	O
s	O
,	O
a	O
)	O
=	O
qt+n−1	O
(	O
s	O
,	O
a	O
)	O
,	O
for	O
all	O
s	O
,	O
a	O
such	O
that	O
s	O
(	O
cid:54	O
)	O
=	O
st	O
or	O
a	O
(	O
cid:54	O
)	O
=	O
at	O
.	O
this	O
is	O
the	O
algorithm	O
we	O
call	O
n-step	B
sarsa	O
.	O
pseudocode	O
is	O
shown	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
,	O
and	O
an	O
example	O
of	O
why	O
it	O
can	O
speed	O
up	O
learning	O
compared	O
to	O
one-step	O
methods	O
is	O
given	O
in	O
figure	O
7.4.	O
figure	O
7.3	O
:	O
the	O
backup	O
diagrams	O
for	O
the	O
spectrum	O
of	O
n-step	O
methods	O
for	O
state–action	O
values	O
.	O
they	O
range	O
from	O
the	O
one-step	O
update	O
of	O
sarsa	O
(	O
0	O
)	O
to	O
the	O
up-until-termination	O
update	O
of	O
the	O
monte	O
carlo	O
method	O
.	O
in	O
between	O
are	O
the	O
n-step	B
updates	O
,	O
based	O
on	O
n	O
steps	O
of	O
real	O
rewards	O
and	O
the	O
estimated	O
value	B
of	O
the	O
nth	O
next	O
state–action	O
pair	O
,	O
all	O
appropriately	O
discounted	O
.	O
on	O
the	O
far	O
right	O
is	O
the	O
backup	B
diagram	I
for	O
n-step	B
expected	O
sarsa	O
.	O
1-step	O
sarsaaka	O
sarsa	O
(	O
0	O
)	O
2-step	O
sarsa3-step	O
sarsan-step	O
sarsa∞-step	O
sarsaaka	O
monte	O
carlon-step	O
expected	O
sarsa	O
7.2.	O
n-step	B
sarsa	O
147	O
n-step	B
sarsa	O
for	O
estimating	O
q	O
≈	O
q∗	O
or	O
qπ	O
initialize	O
q	O
(	O
s	O
,	O
a	O
)	O
arbitrarily	O
,	O
for	O
all	O
s	O
∈	O
s	O
,	O
a	O
∈	O
a	O
initialize	O
π	O
to	O
be	O
ε-greedy	O
with	O
respect	O
to	O
q	O
,	O
or	O
to	O
a	O
ﬁxed	O
given	O
policy	B
algorithm	O
parameters	O
:	O
step	O
size	O
α	O
∈	O
(	O
0	O
,	O
1	O
]	O
,	O
small	O
ε	O
>	O
0	O
,	O
a	O
positive	O
integer	O
n	O
all	O
store	O
and	O
access	O
operations	O
(	O
for	O
st	O
,	O
at	O
,	O
and	O
rt	O
)	O
can	O
take	O
their	O
index	O
mod	O
n	O
+	O
1	O
loop	O
for	O
each	O
episode	O
:	O
if	O
t	O
<	O
t	O
,	O
then	O
:	O
initialize	O
and	O
store	O
s0	O
(	O
cid:54	O
)	O
=	O
terminal	O
select	O
and	O
store	O
an	O
action	B
a0	O
∼	O
π	O
(	O
·|s0	O
)	O
t	O
←	O
∞loop	O
for	O
t	O
=	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
:	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
until	O
τ	O
=	O
t	O
−	O
1	O
g	O
←	O
(	O
cid:80	O
)	O
min	O
(	O
τ	O
+n	O
,	O
t	O
)	O
τ	O
←	O
t	O
−	O
n	O
+	O
1	O
if	O
τ	O
≥	O
0	O
:	O
i=τ	O
+1	O
γi−τ−1ri	O
else	O
:	O
t	O
←	O
t	O
+	O
1	O
select	O
and	O
store	O
an	O
action	B
at+1	O
∼	O
π	O
(	O
·|st+1	O
)	O
(	O
τ	O
is	O
the	O
time	O
whose	O
estimate	O
is	O
being	O
updated	O
)	O
take	O
action	B
at	O
observe	O
and	O
store	O
the	O
next	O
reward	O
as	O
rt+1	O
and	O
the	O
next	O
state	B
as	O
st+1	O
if	O
st+1	O
is	O
terminal	O
,	O
then	O
:	O
if	O
τ	O
+	O
n	O
<	O
t	O
,	O
then	O
g	O
←	O
g	O
+	O
γnq	O
(	O
sτ	O
+n	O
,	O
aτ	O
+n	O
)	O
q	O
(	O
sτ	O
,	O
aτ	O
)	O
←	O
q	O
(	O
sτ	O
,	O
aτ	O
)	O
+	O
α	O
[	O
g	O
−	O
q	O
(	O
sτ	O
,	O
aτ	O
)	O
]	O
if	O
π	O
is	O
being	O
learned	O
,	O
then	O
ensure	O
that	O
π	O
(	O
·|sτ	O
)	O
is	O
ε-greedy	O
wrt	O
q	O
(	O
gτ	O
:	O
τ	O
+n	O
)	O
figure	O
7.4	O
:	O
gridworld	O
example	O
of	O
the	O
speedup	O
of	O
policy	O
learning	O
due	O
to	O
the	O
use	O
of	O
n-step	O
methods	O
.	O
the	O
ﬁrst	O
panel	O
shows	O
the	O
path	O
taken	O
by	O
an	O
agent	O
in	O
a	O
single	O
episode	O
,	O
ending	O
at	O
a	O
location	O
of	O
high	O
reward	O
,	O
marked	O
by	O
the	O
g.	O
in	O
this	O
example	O
the	O
values	O
were	O
all	O
initially	O
0	O
,	O
and	O
all	O
rewards	O
were	O
zero	O
except	O
for	O
a	O
positive	O
reward	O
at	O
g.	O
the	O
arrows	O
in	O
the	O
other	O
two	O
panels	O
show	O
which	O
action	B
values	O
were	O
strengthened	O
as	O
a	O
result	O
of	O
this	O
path	O
by	O
one-step	O
and	O
n-step	B
sarsa	O
methods	O
.	O
the	O
one-step	O
method	O
strengthens	O
only	O
the	O
last	O
action	B
of	O
the	O
sequence	O
of	O
actions	O
that	O
led	O
to	O
the	O
high	O
reward	O
,	O
whereas	O
the	O
n-step	B
method	O
strengthens	O
the	O
last	O
n	O
actions	O
of	O
the	O
sequence	O
,	O
so	O
that	O
much	O
more	O
is	O
learned	O
from	O
the	O
one	O
episode	O
.	O
path	O
takenaction	O
values	O
increasedby	O
one-step	O
sarsaaction	O
values	O
increased	O
by	O
10-step	O
sarsaggg	O
148	O
chapter	O
7	O
:	O
n-step	B
bootstrapping	O
exercise	O
7.4	O
prove	O
that	O
the	O
n-step	B
return	O
of	O
sarsa	O
(	O
7.4	O
)	O
can	O
be	O
written	O
exactly	O
in	O
terms	O
of	O
a	O
novel	O
td	O
error	O
,	O
as	O
min	O
(	O
t+n	O
,	O
t	O
)	O
−1	O
(	O
cid:88	O
)	O
k=t	O
gt	O
:	O
t+n	O
=	O
qt−1	O
(	O
st	O
,	O
at	O
)	O
+	O
γk−t	O
[	O
rk+1	O
+	O
γqk	O
(	O
sk+1	O
,	O
ak+1	O
)	O
−	O
qk−1	O
(	O
sk	O
,	O
ak	O
)	O
]	O
.	O
(	O
7.6	O
)	O
(	O
cid:3	O
)	O
what	O
about	O
expected	O
sarsa	O
?	O
the	O
backup	B
diagram	I
for	O
the	O
n-step	B
version	O
of	O
expected	O
sarsa	O
is	O
shown	O
on	O
the	O
far	O
right	O
in	O
figure	O
7.3.	O
it	O
consists	O
of	O
a	O
linear	O
string	O
of	O
sample	O
actions	O
and	O
states	O
,	O
just	O
as	O
in	O
n-step	B
sarsa	O
,	O
except	O
that	O
its	O
last	O
element	O
is	O
a	O
branch	O
over	O
all	O
action	B
possibilities	O
weighted	O
,	O
as	O
always	O
,	O
by	O
their	O
probability	O
under	O
π.	O
this	O
algorithm	O
can	O
be	O
described	O
by	O
the	O
same	O
equation	O
as	O
n-step	O
sarsa	O
(	O
above	O
)	O
except	O
with	O
the	O
n-step	B
return	O
redeﬁned	O
as	O
gt	O
:	O
t+n	O
.	O
¯vt	O
(	O
s	O
)	O
(	O
7.8	O
)	O
(	O
with	O
gt	O
:	O
t+n	O
s	O
,	O
using	O
the	O
estimated	O
action	B
values	O
at	O
time	O
t	O
,	O
under	O
the	O
target	B
policy	O
:	O
.	O
=	O
rt+1	O
+	O
···	O
+	O
γn−1rt+n	O
+	O
γn	O
¯vt+n−1	O
(	O
st+n	O
)	O
,	O
(	O
7.7	O
)	O
.	O
=	O
gt	O
for	O
t	O
+	O
n	O
≥	O
t	O
)	O
where	O
¯vt	O
(	O
s	O
)	O
is	O
the	O
expected	B
approximate	I
value	I
of	O
state	B
=	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
a|s	O
)	O
qt	O
(	O
s	O
,	O
a	O
)	O
,	O
for	O
all	O
s	O
∈	O
s.	O
expected	O
approximate	O
values	O
are	O
used	O
in	O
developing	O
many	O
of	O
the	O
action-value	B
methods	I
in	O
the	O
rest	O
of	O
this	O
book	O
.	O
if	O
s	O
is	O
terminal	O
,	O
then	O
its	O
expected	B
approximate	I
value	I
is	O
deﬁned	O
to	O
be	O
0.	O
t	O
+	O
n	O
<	O
t	O
,	O
7.3	O
n-step	B
oﬀ-policy	I
learning	O
by	O
importance	O
sam-	O
pling	O
recall	O
that	O
oﬀ-policy	B
learning	O
is	O
learning	O
the	O
value	B
function	I
for	O
one	O
policy	B
,	O
π	O
,	O
while	O
following	O
another	O
policy	B
,	O
b.	O
often	O
,	O
π	O
is	O
the	O
greedy	O
policy	O
for	O
the	O
current	O
action-value-	O
function	O
estimate	O
,	O
and	O
b	O
is	O
a	O
more	O
exploratory	O
policy	B
,	O
perhaps	O
ε-greedy	O
.	O
in	O
order	O
to	O
use	O
the	O
data	O
from	O
b	O
we	O
must	O
take	O
into	O
account	O
the	O
diﬀerence	O
between	O
the	O
two	O
policies	O
,	O
using	O
their	O
relative	O
probability	O
of	O
taking	O
the	O
actions	O
that	O
were	O
taken	O
(	O
see	O
section	O
5.5	O
)	O
.	O
in	O
n-step	O
methods	O
,	O
returns	O
are	O
constructed	O
over	O
n	O
steps	O
,	O
so	O
we	O
are	O
interested	O
in	O
the	O
relative	O
probability	O
of	O
just	O
those	O
n	O
actions	O
.	O
for	O
example	O
,	O
to	O
make	O
a	O
simple	O
oﬀ-policy	B
version	O
of	O
n-step	O
td	O
,	O
the	O
update	O
for	O
time	O
t	O
(	O
actually	O
made	O
at	O
time	O
t	O
+	O
n	O
)	O
can	O
simply	O
be	O
weighted	O
by	O
ρt	O
:	O
t+n−1	O
:	O
.	O
=	O
vt+n−1	O
(	O
st	O
)	O
+	O
αρt	O
:	O
t+n−1	O
[	O
gt	O
:	O
t+n	O
−	O
vt+n−1	O
(	O
st	O
)	O
]	O
,	O
(	O
7.9	O
)	O
where	O
ρt	O
:	O
t+n−1	O
,	O
called	O
the	O
importance	B
sampling	I
ratio	O
,	O
is	O
the	O
relative	O
probability	O
under	O
the	O
two	O
policies	O
of	O
taking	O
the	O
n	O
actions	O
from	O
at	O
to	O
at+n−1	O
(	O
cf	O
.	O
eq	O
.	O
5.3	O
)	O
:	O
0	O
≤	O
t	O
<	O
t	O
,	O
vt+n	O
(	O
st	O
)	O
.	O
=	O
ρt	O
:	O
h	O
min	O
(	O
h	O
,	O
t−1	O
)	O
(	O
cid:89	O
)	O
k=t	O
π	O
(	O
ak|sk	O
)	O
b	O
(	O
ak|sk	O
)	O
.	O
(	O
7.10	O
)	O
7.3.	O
n-step	B
oﬀ-policy	I
learning	O
by	O
importance	B
sampling	I
149	O
for	O
example	O
,	O
if	O
any	O
one	O
of	O
the	O
actions	O
would	O
never	O
be	O
taken	O
by	O
π	O
(	O
i.e.	O
,	O
π	O
(	O
ak|sk	O
)	O
=	O
0	O
)	O
then	O
the	O
n-step	B
return	O
should	O
be	O
given	O
zero	O
weight	O
and	O
be	O
totally	O
ignored	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
by	O
chance	O
an	O
action	B
is	O
taken	O
that	O
π	O
would	O
take	O
with	O
much	O
greater	O
probability	O
than	O
b	O
does	O
,	O
then	O
this	O
will	O
increase	O
the	O
weight	O
that	O
would	O
otherwise	O
be	O
given	O
to	O
the	O
return	B
.	O
this	O
makes	O
sense	O
because	O
that	O
action	B
is	O
characteristic	O
of	O
π	O
(	O
and	O
therefore	O
we	O
want	O
to	O
learn	O
about	O
it	O
)	O
but	O
is	O
selected	O
only	O
rarely	O
by	O
b	O
and	O
thus	O
rarely	O
appears	O
in	O
the	O
data	O
.	O
to	O
make	O
up	O
for	O
this	O
we	O
have	O
to	O
over-weight	O
it	O
when	O
it	O
does	O
occur	O
.	O
note	O
that	O
if	O
the	O
two	O
policies	O
are	O
actually	O
the	O
same	O
(	O
the	O
on-policy	O
case	O
)	O
then	O
the	O
importance	B
sampling	I
ratio	O
is	O
always	O
1.	O
thus	O
our	O
new	O
update	O
(	O
7.9	O
)	O
generalizes	O
and	O
can	O
completely	O
replace	O
our	O
earlier	O
n-step	B
td	O
update	O
.	O
similarly	O
,	O
our	O
previous	O
n-step	B
sarsa	O
update	O
can	O
be	O
completely	O
replaced	O
by	O
a	O
simple	O
oﬀ-policy	B
form	O
:	O
qt+n	O
(	O
st	O
,	O
at	O
)	O
.	O
=	O
qt+n−1	O
(	O
st	O
,	O
at	O
)	O
+	O
αρt+1	O
:	O
t+n−1	O
[	O
gt	O
:	O
t+n	O
−	O
qt+n−1	O
(	O
st	O
,	O
at	O
)	O
]	O
,	O
(	O
7.11	O
)	O
for	O
0	O
≤	O
t	O
<	O
t	O
.	O
note	O
that	O
the	O
importance	B
sampling	I
ratio	O
here	O
starts	O
one	O
step	O
later	O
than	O
for	O
n-step	O
td	O
(	O
above	O
)	O
.	O
this	O
is	O
because	O
here	O
we	O
are	O
updating	O
a	O
state–action	O
pair	O
.	O
we	O
do	O
not	O
have	O
to	O
care	O
how	O
likely	O
we	O
were	O
to	O
select	O
the	O
action	B
;	O
now	O
that	O
we	O
have	O
selected	O
it	O
we	O
want	O
to	O
learn	O
fully	O
from	O
what	O
happens	O
,	O
with	O
importance	O
sampling	O
only	O
for	O
subsequent	O
actions	O
.	O
pseudocode	O
for	O
the	O
full	O
algorithm	O
is	O
shown	O
in	O
the	O
box	O
below	O
.	O
oﬀ-policy	B
n-step	O
sarsa	O
for	O
estimating	O
q	O
≈	O
q∗	O
or	O
qπ	O
input	O
:	O
an	O
arbitrary	O
behavior	B
policy	I
b	O
such	O
that	O
b	O
(	O
a|s	O
)	O
>	O
0	O
,	O
for	O
all	O
s	O
∈	O
s	O
,	O
a	O
∈	O
a	O
initialize	O
q	O
(	O
s	O
,	O
a	O
)	O
arbitrarily	O
,	O
for	O
all	O
s	O
∈	O
s	O
,	O
a	O
∈	O
a	O
initialize	O
π	O
to	O
be	O
greedy	O
with	O
respect	O
to	O
q	O
,	O
or	O
as	O
a	O
ﬁxed	O
given	O
policy	B
algorithm	O
parameters	O
:	O
step	O
size	O
α	O
∈	O
(	O
0	O
,	O
1	O
]	O
,	O
a	O
positive	O
integer	O
n	O
all	O
store	O
and	O
access	O
operations	O
(	O
for	O
st	O
,	O
at	O
,	O
and	O
rt	O
)	O
can	O
take	O
their	O
index	O
mod	O
n	O
+	O
1	O
take	O
action	B
at	O
observe	O
and	O
store	O
the	O
next	O
reward	O
as	O
rt+1	O
and	O
the	O
next	O
state	B
as	O
st+1	O
if	O
st+1	O
is	O
terminal	O
,	O
then	O
:	O
t	O
←	O
t	O
+	O
1	O
select	O
and	O
store	O
an	O
action	B
at+1	O
∼	O
b	O
(	O
·|st+1	O
)	O
else	O
:	O
(	O
τ	O
is	O
the	O
time	O
whose	O
estimate	O
is	O
being	O
updated	O
)	O
loop	O
for	O
each	O
episode	O
:	O
if	O
t	O
<	O
t	O
,	O
then	O
:	O
initialize	O
and	O
store	O
s0	O
(	O
cid:54	O
)	O
=	O
terminal	O
select	O
and	O
store	O
an	O
action	B
a0	O
∼	O
b	O
(	O
·|s0	O
)	O
t	O
←	O
∞	O
loop	O
for	O
t	O
=	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
:	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
until	O
τ	O
=	O
t	O
−	O
1	O
ρ	O
←	O
(	O
cid:81	O
)	O
min	O
(	O
τ	O
+n−1	O
,	O
t−1	O
)	O
g	O
←	O
(	O
cid:80	O
)	O
min	O
(	O
τ	O
+n	O
,	O
t	O
)	O
τ	O
←	O
t	O
−	O
n	O
+	O
1	O
if	O
τ	O
≥	O
0	O
:	O
π	O
(	O
ai|si	O
)	O
b	O
(	O
ai|si	O
)	O
γi−τ−1ri	O
i=τ	O
+1	O
i=τ	O
+1	O
if	O
τ	O
+	O
n	O
<	O
t	O
,	O
then	O
:	O
g	O
←	O
g	O
+	O
γnq	O
(	O
sτ	O
+n	O
,	O
aτ	O
+n	O
)	O
q	O
(	O
sτ	O
,	O
aτ	O
)	O
←	O
q	O
(	O
sτ	O
,	O
aτ	O
)	O
+	O
αρ	O
[	O
g	O
−	O
q	O
(	O
sτ	O
,	O
aτ	O
)	O
]	O
if	O
π	O
is	O
being	O
learned	O
,	O
then	O
ensure	O
that	O
π	O
(	O
·|sτ	O
)	O
is	O
greedy	O
wrt	O
q	O
(	O
ρτ	O
+1	O
:	O
t+n−1	O
)	O
(	O
gτ	O
:	O
τ	O
+n	O
)	O
150	O
chapter	O
7	O
:	O
n-step	B
bootstrapping	O
the	O
oﬀ-policy	B
version	O
of	O
n-step	O
expected	O
sarsa	O
would	O
use	O
the	O
same	O
update	O
as	O
above	O
for	O
n-step	O
sarsa	O
except	O
that	O
the	O
importance	B
sampling	I
ratio	O
would	O
have	O
one	O
less	O
factor	O
in	O
it	O
.	O
that	O
is	O
,	O
the	O
above	O
equation	O
would	O
use	O
ρt+1	O
:	O
t+n−2	O
instead	O
of	O
ρt+1	O
:	O
t+n−1	O
,	O
and	O
of	O
course	O
it	O
would	O
use	O
the	O
expected	O
sarsa	O
version	O
of	O
the	O
n-step	B
return	O
(	O
7.7	O
)	O
.	O
this	O
is	O
because	O
in	O
expected	O
sarsa	O
all	O
possible	O
actions	O
are	O
taken	O
into	O
account	O
in	O
the	O
last	O
state	B
;	O
the	O
one	O
actually	O
taken	O
has	O
no	O
eﬀect	O
and	O
does	O
not	O
have	O
to	O
be	O
corrected	O
for	O
.	O
7.4	O
*per-decision	O
oﬀ-policy	B
methods	I
with	O
control	B
variates	I
the	O
multi-step	O
oﬀ-policy	B
methods	I
presented	O
in	O
the	O
previous	O
section	O
are	O
simple	O
and	O
con-	O
ceptually	O
clear	O
,	O
but	O
are	O
probably	O
not	O
the	O
most	O
eﬃcient	O
.	O
a	O
more	O
sophisticated	O
approach	O
would	O
use	O
per-decision	B
importance	O
sampling	O
ideas	O
such	O
as	O
were	O
introduced	O
in	O
section	O
5.9.	O
to	O
understand	O
this	O
approach	O
,	O
ﬁrst	O
note	O
that	O
the	O
ordinary	O
n-step	B
return	O
(	O
7.1	O
)	O
,	O
like	O
all	O
re-	O
turns	O
,	O
can	O
be	O
written	O
recursively	O
.	O
for	O
the	O
n	O
steps	O
ending	O
at	O
horizon	O
h	O
,	O
the	O
n-step	B
return	O
can	O
be	O
written	O
gt	O
:	O
h	O
=	O
rt+1	O
+	O
γgt+1	O
:	O
h	O
,	O
t	O
<	O
h	O
<	O
t	O
,	O
(	O
7.12	O
)	O
.	O
where	O
gh	O
:	O
h	O
=	O
vh−1	O
(	O
sh	O
)	O
.	O
(	O
recall	O
that	O
this	O
return	B
is	O
used	O
at	O
time	O
h	O
,	O
previously	O
denoted	O
t	O
+	O
n.	O
)	O
now	O
consider	O
the	O
eﬀect	O
of	O
following	O
a	O
behavior	B
policy	I
b	O
that	O
is	O
not	O
the	O
same	O
as	O
the	O
target	B
policy	O
π.	O
all	O
of	O
the	O
resulting	O
experience	O
,	O
including	O
the	O
ﬁrst	O
reward	O
rt+1	O
and	O
the	O
next	O
state	B
st+1	O
must	O
be	O
weighted	O
by	O
the	O
importance	B
sampling	I
ratio	O
for	O
time	O
t	O
,	O
ρt	O
=	O
π	O
(	O
at|st	O
)	O
b	O
(	O
at|st	O
)	O
.	O
one	O
might	O
be	O
tempted	O
to	O
simply	O
weight	O
the	O
righthand	O
side	O
of	O
the	O
above	O
equation	O
,	O
but	O
one	O
can	O
do	O
better	O
.	O
suppose	O
the	O
action	B
at	O
time	O
t	O
would	O
never	O
be	O
selected	O
by	O
π	O
,	O
so	O
that	O
ρt	O
is	O
zero	O
.	O
then	O
a	O
simple	O
weighting	O
would	O
result	O
in	O
the	O
n-step	O
return	B
being	O
zero	O
,	O
which	O
could	O
result	O
in	O
high	O
variance	O
when	O
it	O
was	O
used	O
as	O
a	O
target	B
.	O
instead	O
,	O
in	O
this	O
more	O
sophisticated	O
approach	O
,	O
one	O
uses	O
an	O
alternate	O
,	O
oﬀ-policy	B
deﬁnition	O
of	O
the	O
n-step	B
return	O
ending	O
at	O
horizon	O
h	O
,	O
as	O
gt	O
:	O
h	O
.	O
=	O
ρt	O
(	O
rt+1	O
+	O
γgt+1	O
:	O
h	O
)	O
+	O
(	O
1	O
−	O
ρt	O
)	O
vh−1	O
(	O
st	O
)	O
,	O
t	O
<	O
h	O
<	O
t	O
,	O
(	O
7.13	O
)	O
.	O
where	O
again	O
gh	O
:	O
h	O
=	O
vh−1	O
(	O
sh	O
)	O
.	O
in	O
this	O
approach	O
,	O
if	O
ρt	O
is	O
zero	O
,	O
then	O
instead	O
of	O
the	O
target	B
being	O
zero	O
and	O
causing	O
the	O
estimate	O
to	O
shrink	O
,	O
the	O
target	B
is	O
the	O
same	O
as	O
the	O
estimate	O
and	O
causes	O
no	O
change	O
.	O
the	O
importance	B
sampling	I
ratio	O
being	O
zero	O
means	O
we	O
should	O
ignore	O
the	O
sample	O
,	O
so	O
leaving	O
the	O
estimate	O
unchanged	O
seems	O
appropriate	O
.	O
the	O
second	O
,	O
additional	O
term	O
in	O
(	O
7.13	O
)	O
is	O
called	O
a	O
control	B
variate	O
(	O
for	O
obscure	O
reasons	O
)	O
.	O
notice	O
that	O
the	O
control	B
variate	O
does	O
not	O
change	O
the	O
expected	B
update	I
;	O
the	O
importance	B
sampling	I
ratio	O
has	O
expected	O
value	O
one	O
(	O
section	O
5.9	O
)	O
and	O
is	O
uncorrelated	O
with	O
the	O
estimate	O
,	O
so	O
the	O
expected	O
value	O
of	O
the	O
control	B
variate	O
is	O
zero	O
.	O
also	O
note	O
that	O
the	O
oﬀ-policy	B
deﬁnition	O
(	O
7.13	O
)	O
is	O
a	O
strict	O
generalization	O
of	O
the	O
earlier	O
on-policy	O
deﬁnition	O
of	O
the	O
n-step	B
return	O
(	O
7.1	O
)	O
,	O
as	O
the	O
two	O
are	O
identical	O
in	O
the	O
on-policy	O
case	O
,	O
in	O
which	O
ρt	O
is	O
always	O
1.	O
for	O
a	O
conventional	O
n-step	B
method	O
,	O
the	O
learning	O
rule	O
to	O
use	O
in	O
conjunction	O
with	O
(	O
7.13	O
)	O
is	O
the	O
n-step	B
td	O
update	O
(	O
7.2	O
)	O
,	O
which	O
has	O
no	O
explicit	O
importance	B
sampling	I
ratios	O
other	O
7.4.	O
per-decision	B
oﬀ-policy	O
methods	O
with	B
control	I
variates	I
151	O
than	O
those	O
embedded	O
in	O
the	O
return	O
.	O
exercise	O
7.5	O
write	O
the	O
pseudocode	O
for	O
the	O
oﬀ-policy	B
state-value	O
prediction	B
algorithm	O
(	O
cid:3	O
)	O
described	O
above	O
.	O
for	B
action	I
values	I
,	O
the	O
oﬀ-policy	B
deﬁnition	O
of	O
the	O
n-step	B
return	O
is	O
a	O
little	O
diﬀerent	O
because	O
the	O
ﬁrst	O
action	B
does	O
not	O
play	O
a	O
role	O
in	O
the	O
importance	O
sampling	O
.	O
that	O
ﬁrst	O
action	B
is	O
the	O
one	O
being	O
learned	O
;	O
it	O
does	O
not	O
matter	O
if	O
it	O
was	O
unlikely	O
or	O
even	O
impossible	O
under	O
the	O
target	B
policy—it	O
has	O
been	O
taken	O
and	O
now	O
full	O
unit	O
weight	O
must	O
be	O
given	O
to	O
the	O
reward	O
and	O
state	B
that	O
follows	O
it	O
.	O
importance	B
sampling	I
will	O
apply	O
only	O
to	O
the	O
actions	O
that	O
follow	O
it	O
.	O
first	O
note	O
that	O
for	B
action	I
values	I
the	O
n-step	B
on-policy	O
return	B
ending	O
at	O
horizon	O
h	O
,	O
expectation	O
form	O
(	O
7.7	O
)	O
,	O
can	O
be	O
written	O
recursively	O
just	O
as	O
in	O
(	O
7.12	O
)	O
,	O
except	O
that	O
for	O
.	O
=	O
¯vh−1	O
(	O
sh	O
)	O
as	O
in	O
(	O
7.8	O
)	O
.	O
an	O
oﬀ-policy	B
form	O
action	B
values	O
the	O
recursion	O
ends	O
with	O
gh	O
:	O
h	O
with	B
control	I
variates	I
is	O
gt	O
:	O
h	O
.	O
=	O
rt+1	O
+	O
γ	O
(	O
cid:16	O
)	O
ρt+1gt+1	O
:	O
h	O
+	O
¯vh−1	O
(	O
st+1	O
)	O
−	O
ρt+1qh−1	O
(	O
st+1	O
,	O
at+1	O
)	O
(	O
cid:17	O
)	O
,	O
=	O
rt+1	O
+	O
γρt+1	O
(	O
cid:16	O
)	O
gt+1	O
:	O
h	O
−	O
qh−1	O
(	O
st+1	O
,	O
at+1	O
)	O
(	O
cid:17	O
)	O
+	O
γ	O
¯vh−1	O
(	O
st+1	O
)	O
,	O
t	O
<	O
h	O
≤	O
t.	O
(	O
7.14	O
)	O
if	O
h	O
<	O
t	O
,	O
then	O
the	O
recursion	O
ends	O
with	O
gh	O
:	O
h	O
sion	O
ends	O
with	O
and	O
gt−1	O
:	O
t	O
with	O
(	O
7.5	O
)	O
)	O
is	O
analogous	O
to	O
expected	O
sarsa	O
.	O
.	O
=	O
qh−1	O
(	O
sh	O
,	O
ah	O
)	O
,	O
whereas	O
,	O
if	O
h	O
≥	O
t	O
,	O
the	O
recur-	O
.	O
=	O
rt	O
.	O
the	O
resultant	O
prediction	B
algorithm	O
(	O
after	O
combining	O
exercise	O
7.6	O
prove	O
that	O
the	O
control	B
variate	O
in	O
the	O
above	O
equations	O
does	O
not	O
change	O
the	O
(	O
cid:3	O
)	O
expected	O
value	O
of	O
the	O
return	B
.	O
∗exercise	O
7.7	O
write	O
the	O
pseudocode	O
for	O
the	O
oﬀ-policy	B
action-value	O
prediction	B
algorithm	O
described	O
immediately	O
above	O
.	O
pay	O
particular	O
attention	O
to	O
the	O
termination	O
conditions	O
for	O
(	O
cid:3	O
)	O
the	O
recursion	O
upon	O
hitting	O
the	O
horizon	O
or	O
the	O
end	O
of	O
episode	O
.	O
exercise	O
7.8	O
show	O
that	O
the	O
general	O
(	O
oﬀ-policy	B
)	O
version	O
of	O
the	O
n-step	B
return	O
(	O
7.13	O
)	O
can	O
still	O
be	O
written	O
exactly	O
and	O
compactly	O
as	O
the	O
sum	O
of	O
state-based	O
td	O
errors	O
(	O
6.5	O
)	O
if	O
the	O
(	O
cid:3	O
)	O
approximate	O
state	O
value	O
function	O
does	O
not	O
change	O
.	O
exercise	O
7.9	O
repeat	O
the	O
above	O
exercise	O
for	O
the	O
action	B
version	O
of	O
the	O
oﬀ-policy	B
n-step	O
return	B
(	O
7.14	O
)	O
and	O
the	O
expected	O
sarsa	O
td	O
error	O
(	O
the	O
quantity	O
in	O
brackets	O
in	O
equation	O
6.9	O
)	O
.	O
(	O
cid:3	O
)	O
exercise	O
7.10	O
(	O
programming	O
)	O
devise	O
a	O
small	O
oﬀ-policy	B
prediction	I
problem	O
and	O
use	O
it	O
to	O
show	O
that	O
the	O
oﬀ-policy	B
learning	O
algorithm	O
using	O
(	O
7.13	O
)	O
and	O
(	O
7.2	O
)	O
is	O
more	O
data	O
eﬃcient	O
(	O
cid:3	O
)	O
than	O
the	O
simpler	O
algorithm	O
using	O
(	O
7.1	O
)	O
and	O
(	O
7.9	O
)	O
.	O
the	O
importance	B
sampling	I
that	O
we	O
have	O
used	O
in	O
this	O
section	O
,	O
the	O
previous	O
section	O
,	O
and	O
in	O
chapter	O
5	O
,	O
enables	O
sound	O
oﬀ-policy	B
learning	O
,	O
but	O
also	O
results	O
in	O
high	O
variance	O
updates	O
,	O
forcing	O
the	O
use	O
of	O
a	O
small	O
step-size	B
parameter	I
and	O
thereby	O
causing	O
learning	O
to	O
be	O
slow	O
.	O
it	O
is	O
probably	O
inevitable	O
that	O
oﬀ-policy	B
training	O
is	O
slower	O
than	O
on-policy	O
training—after	O
all	O
,	O
the	O
data	O
is	O
less	O
relevant	O
to	O
what	O
is	O
being	O
learned	O
.	O
however	O
,	O
it	O
is	O
probably	O
also	O
true	O
that	O
these	O
methods	O
can	O
be	O
improved	O
on	O
.	O
the	O
control	B
variates	I
are	O
one	O
way	O
of	O
reducing	O
the	O
variance	O
.	O
another	O
is	O
to	O
rapidly	O
adapt	O
the	O
step	O
sizes	O
to	O
the	O
observed	O
variance	O
,	O
as	O
in	O
the	O
152	O
chapter	O
7	O
:	O
n-step	B
bootstrapping	O
autostep	O
method	O
(	O
mahmood	O
,	O
sutton	O
,	O
degris	O
and	O
pilarski	O
,	O
2012	O
)	O
.	O
yet	O
another	O
promising	O
approach	O
is	O
the	O
invariant	O
updates	O
of	O
karampatziakis	O
and	O
langford	O
(	O
2010	O
)	O
as	O
extended	O
to	O
td	O
by	O
tian	O
(	O
in	O
preparation	O
)	O
.	O
the	O
usage	O
technique	O
of	O
mahmood	O
(	O
2017	O
;	O
mahmood	O
and	O
sutton	O
,	O
2015	O
)	O
may	O
also	O
be	O
part	O
of	O
the	O
solution	O
.	O
in	O
the	O
next	O
section	O
we	O
consider	O
an	O
oﬀ-policy	B
learning	O
method	O
that	O
does	O
not	O
use	O
importance	B
sampling	I
.	O
7.5	O
oﬀ-policy	B
learning	O
without	O
importance	B
sampling	I
:	O
the	O
n-step	B
tree	O
backup	O
algorithm	O
is	O
oﬀ-policy	B
learning	O
possible	O
without	O
importance	B
sampling	I
?	O
q-learning	O
and	O
expected	O
sarsa	O
from	O
chapter	O
6	O
do	O
this	O
for	O
the	O
one-step	O
case	O
,	O
but	O
is	O
there	O
a	O
corresponding	O
multi-step	O
algorithm	O
?	O
in	O
this	O
section	O
we	O
present	O
just	O
such	O
an	O
n-step	B
method	O
,	O
called	O
the	O
tree-backup	O
algorithm	O
.	O
the	O
idea	O
of	O
the	O
algorithm	O
is	O
suggested	O
by	O
the	O
3-step	O
tree-backup	O
backup	B
diagram	I
shown	O
to	O
the	O
right	O
.	O
down	O
the	O
central	O
spine	O
and	O
labeled	O
in	O
the	O
diagram	O
are	O
three	O
sample	O
states	O
and	O
rewards	O
,	O
and	O
two	O
sample	O
actions	O
.	O
these	O
are	O
the	O
random	O
variables	O
representing	O
the	O
events	O
occur-	O
ring	O
after	O
the	O
initial	O
state–action	O
pair	O
st	O
,	O
at	O
.	O
hanging	O
oﬀ	O
to	O
the	O
sides	O
of	O
each	O
state	B
are	O
the	O
actions	O
that	O
were	O
not	O
selected	O
.	O
(	O
for	O
the	O
last	O
state	B
,	O
all	O
the	O
actions	O
are	O
considered	O
to	O
have	O
not	O
(	O
yet	O
)	O
been	O
selected	O
.	O
)	O
because	O
we	O
have	O
no	O
sample	O
data	O
for	O
the	O
unselected	O
actions	O
,	O
we	O
bootstrap	O
and	O
use	O
the	O
estimates	O
of	O
their	O
values	O
in	O
forming	O
the	O
target	B
for	O
the	O
update	O
.	O
this	O
slightly	O
extends	O
the	O
idea	O
of	O
a	O
backup	B
diagram	I
.	O
so	O
far	O
we	O
have	O
always	O
up-	O
dated	O
the	O
estimated	O
value	B
of	O
the	O
node	O
at	O
the	O
top	O
of	O
the	O
diagram	O
toward	O
a	O
target	B
combining	O
the	O
rewards	O
along	O
the	O
way	O
(	O
appropriately	O
discounted	O
)	O
and	O
the	O
estimated	O
values	O
of	O
the	O
nodes	O
at	O
the	O
bottom	O
.	O
in	O
the	O
tree-backup	O
update	O
,	O
the	O
target	B
includes	O
all	O
these	O
things	O
plus	O
the	O
estimated	O
values	O
of	O
the	O
dangling	O
action	B
nodes	O
hanging	O
oﬀ	O
the	O
sides	O
,	O
at	O
all	O
levels	O
.	O
this	O
is	O
why	O
it	O
is	O
called	O
a	O
tree-backup	O
update	O
;	O
it	O
is	O
an	O
update	O
from	O
the	O
entire	O
tree	O
of	O
estimated	O
action	B
values	O
.	O
the	O
3-step	O
tree-backup	O
update	O
more	O
precisely	O
,	O
the	O
update	O
is	O
from	O
the	O
estimated	O
action	B
values	O
of	O
the	O
leaf	O
nodes	O
of	O
the	O
tree	O
.	O
the	O
action	B
nodes	O
in	O
the	O
interior	O
,	O
corresponding	O
to	O
the	O
actual	O
actions	O
taken	O
,	O
do	O
not	O
participate	O
.	O
each	O
leaf	O
node	O
contributes	O
to	O
the	O
target	B
with	O
a	O
weight	O
proportional	O
to	O
its	O
probability	O
of	O
occurring	O
under	O
the	O
target	B
policy	O
π.	O
thus	O
each	O
ﬁrst-level	O
action	B
a	O
contributes	O
with	O
a	O
weight	O
of	O
π	O
(	O
a|st+1	O
)	O
,	O
except	O
that	O
the	O
action	B
actually	O
taken	O
,	O
at+1	O
,	O
does	O
not	O
contribute	O
at	O
all	O
.	O
its	O
probability	O
,	O
π	O
(	O
at+1|st+1	O
)	O
,	O
is	O
used	O
to	O
weight	O
all	O
the	O
second-level	O
action	B
values	O
.	O
thus	O
,	O
each	O
non-selected	O
second-	O
level	O
action	B
a	O
(	O
cid:48	O
)	O
contributes	O
with	O
weight	O
π	O
(	O
at+1|st+1	O
)	O
π	O
(	O
a	O
(	O
cid:48	O
)	O
|st+2	O
)	O
.	O
each	O
third-level	O
action	B
contributes	O
with	O
weight	O
π	O
(	O
at+1|st+1	O
)	O
π	O
(	O
at+2|st+2	O
)	O
π	O
(	O
a	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
|st+3	O
)	O
,	O
and	O
so	O
on	O
.	O
it	O
is	O
as	O
if	O
each	O
arrow	O
to	O
an	O
action	B
node	O
in	O
the	O
diagram	O
is	O
weighted	O
by	O
the	O
action	B
’	O
s	O
probability	O
of	O
being	O
selected	O
under	O
the	O
target	B
policy	O
and	O
,	O
if	O
there	O
is	O
a	O
tree	O
below	O
the	O
action	B
,	O
then	O
that	O
weight	O
applies	O
to	O
all	O
the	O
leaf	O
nodes	O
in	O
the	O
tree	O
.	O
st	O
,	O
atat+1rt+1st+1st+2rt+2at+2rt+3st+3	O
7.5.	O
oﬀ-policy	B
learning	O
without	O
importance	B
sampling	I
:	O
n-step	B
tree	O
backup	O
153	O
we	O
can	O
think	O
of	O
the	O
3-step	O
tree-backup	O
update	O
as	O
consisting	O
of	O
6	O
half-steps	O
,	O
alternating	O
between	O
sample	O
half-steps	O
from	O
an	O
action	B
to	O
a	O
subsequent	O
state	B
,	O
and	O
expected	O
half-steps	O
considering	O
from	O
that	O
state	B
all	O
possible	O
actions	O
with	O
their	O
probabilities	O
of	O
occuring	O
under	O
the	O
policy	B
.	O
now	O
let	O
us	O
develop	O
the	O
detailed	O
equations	O
for	O
the	O
n-step	B
tree-backup	O
algorithm	O
.	O
the	O
one-step	O
return	O
(	O
target	B
)	O
is	O
the	O
same	O
as	O
that	O
of	O
expected	O
sarsa	O
,	O
π	O
(	O
a|st+1	O
)	O
qt	O
(	O
st+1	O
,	O
a	O
)	O
,	O
t	O
<	O
t	O
−	O
1	O
,	O
(	O
7.15	O
)	O
and	O
the	O
two-step	O
tree-backup	O
return	B
is	O
gt	O
:	O
t+1	O
gt	O
:	O
t+2	O
.	O
=	O
rt+1	O
+	O
γ	O
(	O
cid:88	O
)	O
a	O
=	O
rt+1	O
+	O
γ	O
(	O
cid:88	O
)	O
a	O
(	O
cid:54	O
)	O
=at+1	O
=	O
rt+1	O
+	O
γ	O
(	O
cid:88	O
)	O
a	O
(	O
cid:54	O
)	O
=at+1	O
.	O
π	O
(	O
a|st+1	O
)	O
qt+1	O
(	O
st+1	O
,	O
a	O
)	O
+	O
γπ	O
(	O
at+1|st+1	O
)	O
(	O
cid:16	O
)	O
rt+2	O
+	O
γ	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
a|st+1	O
)	O
qt+1	O
(	O
st+1	O
,	O
a	O
)	O
+	O
γπ	O
(	O
at+1|st+1	O
)	O
gt+1	O
:	O
t+2	O
,	O
π	O
(	O
a|st+2	O
)	O
qt+1	O
(	O
st+2	O
,	O
a	O
)	O
(	O
cid:17	O
)	O
t	O
<	O
t	O
−	O
2.	O
the	O
latter	O
form	O
suggests	O
the	O
general	O
recursive	O
deﬁnition	O
of	O
the	O
tree-backup	O
n-step	B
return	O
:	O
gt	O
:	O
t+n	O
.	O
=	O
rt+1	O
+γ	O
(	O
cid:88	O
)	O
a	O
(	O
cid:54	O
)	O
=at+1	O
π	O
(	O
a|st+1	O
)	O
qt+n−1	O
(	O
st+1	O
,	O
a	O
)	O
+	O
γ	O
π	O
(	O
at+1|st+1	O
)	O
gt+1	O
:	O
t+n	O
,	O
t	O
<	O
t	O
−1	O
,	O
(	O
7.16	O
)	O
with	O
the	O
n	O
=	O
1	O
case	O
handled	O
by	O
gt−1	O
:	O
t	O
the	O
usual	O
action-value	O
update	O
rule	O
from	O
n-step	B
sarsa	O
:	O
.	O
=	O
rt	O
and	O
(	O
7.15	O
)	O
.	O
this	O
target	B
is	O
then	O
used	O
with	O
qt+n	O
(	O
st	O
,	O
at	O
)	O
.	O
=	O
qt+n−1	O
(	O
st	O
,	O
at	O
)	O
+	O
α	O
[	O
gt	O
:	O
t+n	O
−	O
qt+n−1	O
(	O
st	O
,	O
at	O
)	O
]	O
,	O
0	O
≤	O
t	O
<	O
t	O
,	O
(	O
7.5	O
)	O
while	O
the	O
values	O
of	O
all	O
other	O
state–action	O
pairs	O
remain	O
unchanged	O
:	O
qt+n	O
(	O
s	O
,	O
a	O
)	O
=	O
qt+n−1	O
(	O
s	O
,	O
a	O
)	O
,	O
for	O
all	O
s	O
,	O
a	O
such	O
that	O
s	O
(	O
cid:54	O
)	O
=	O
st	O
or	O
a	O
(	O
cid:54	O
)	O
=	O
at	O
.	O
pseudocode	O
for	O
this	O
algorithm	O
is	O
shown	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
.	O
exercise	O
7.11	O
show	O
that	O
if	O
the	O
approximate	O
action	O
values	O
are	O
unchanging	O
,	O
then	O
the	O
tree-backup	O
return	B
(	O
7.16	O
)	O
can	O
be	O
written	O
as	O
a	O
sum	O
of	O
expectation-based	O
td	O
errors	O
:	O
gt	O
:	O
t+n	O
=	O
q	O
(	O
st	O
,	O
at	O
)	O
+	O
min	O
(	O
t+n−1	O
,	O
t−1	O
)	O
(	O
cid:88	O
)	O
k=t	O
δk	O
k	O
(	O
cid:89	O
)	O
i=t+1	O
γπ	O
(	O
ai|si	O
)	O
,	O
where	O
δt	O
.	O
=	O
rt+1	O
+	O
γ	O
¯vt	O
(	O
st+1	O
)	O
−	O
q	O
(	O
st	O
,	O
at	O
)	O
and	O
¯vt	O
is	O
given	O
by	O
(	O
7.8	O
)	O
.	O
(	O
cid:3	O
)	O
154	O
chapter	O
7	O
:	O
n-step	B
bootstrapping	O
n-step	B
tree	O
backup	O
for	O
estimating	O
q	O
≈	O
q∗	O
or	O
qπ	O
initialize	O
q	O
(	O
s	O
,	O
a	O
)	O
arbitrarily	O
,	O
for	O
all	O
s	O
∈	O
s	O
,	O
a	O
∈	O
a	O
initialize	O
π	O
to	O
be	O
greedy	O
with	O
respect	O
to	O
q	O
,	O
or	O
as	O
a	O
ﬁxed	O
given	O
policy	B
algorithm	O
parameters	O
:	O
step	O
size	O
α	O
∈	O
(	O
0	O
,	O
1	O
]	O
,	O
a	O
positive	O
integer	O
n	O
all	O
store	O
and	O
access	O
operations	O
can	O
take	O
their	O
index	O
mod	O
n	O
+	O
1	O
loop	O
for	O
each	O
episode	O
:	O
if	O
t	O
<	O
t	O
:	O
take	O
action	B
at	O
;	O
observe	O
and	O
store	O
the	O
next	O
reward	O
and	O
state	B
as	O
rt+1	O
,	O
st+1	O
if	O
st+1	O
is	O
terminal	O
:	O
else	O
:	O
t	O
←	O
t	O
+	O
1	O
choose	O
an	O
action	B
at+1	O
arbitrarily	O
as	O
a	O
function	O
of	O
st+1	O
;	O
store	O
at+1	O
initialize	O
and	O
store	O
s0	O
(	O
cid:54	O
)	O
=	O
terminal	O
choose	O
an	O
action	B
a0	O
arbitrarily	O
as	O
a	O
function	O
of	O
s0	O
;	O
store	O
a0	O
t	O
←	O
∞loop	O
for	O
t	O
=	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
:	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
until	O
τ	O
=	O
t	O
−	O
1	O
g	O
←	O
rt+1	O
+	O
γ	O
(	O
cid:80	O
)	O
a	O
π	O
(	O
a|st+1	O
)	O
q	O
(	O
st+1	O
,	O
a	O
)	O
g	O
←	O
rk	O
+	O
γ	O
(	O
cid:80	O
)	O
a	O
(	O
cid:54	O
)	O
=ak	O
τ	O
←	O
t	O
+	O
1	O
−	O
n	O
if	O
τ	O
≥	O
0	O
:	O
if	O
t	O
+	O
1	O
≥	O
t	O
:	O
g	O
←	O
rt	O
else	O
loop	O
for	O
k	O
=	O
min	O
(	O
t	O
,	O
t	O
−	O
1	O
)	O
down	O
through	O
τ	O
+	O
1	O
:	O
q	O
(	O
sτ	O
,	O
aτ	O
)	O
←	O
q	O
(	O
sτ	O
,	O
aτ	O
)	O
+	O
α	O
[	O
g	O
−	O
q	O
(	O
sτ	O
,	O
aτ	O
)	O
]	O
if	O
π	O
is	O
being	O
learned	O
,	O
then	O
ensure	O
that	O
π	O
(	O
·|sτ	O
)	O
is	O
greedy	O
wrt	O
q	O
π	O
(	O
a|sk	O
)	O
q	O
(	O
sk	O
,	O
a	O
)	O
+	O
γπ	O
(	O
ak|sk	O
)	O
g	O
(	O
τ	O
is	O
the	O
time	O
whose	O
estimate	O
is	O
being	O
updated	O
)	O
7.6	O
*a	O
unifying	O
algorithm	O
:	O
n-step	B
q	O
(	O
σ	O
)	O
so	O
far	O
in	O
this	O
chapter	O
we	O
have	O
considered	O
three	O
diﬀerent	O
kinds	O
of	O
action-value	O
algorithms	O
,	O
corresponding	O
to	O
the	O
ﬁrst	O
three	O
backup	O
diagrams	O
shown	O
in	O
figure	O
7.5.	O
n-step	B
sarsa	O
has	O
all	O
sample	O
transitions	O
,	O
the	O
tree-backup	O
algorithm	O
has	O
all	O
state-to-action	O
transitions	O
fully	O
branched	O
without	O
sampling	O
,	O
and	O
n-step	O
expected	O
sarsa	O
has	O
all	O
sample	O
transitions	O
except	O
for	O
the	O
last	O
state-to-action	O
one	O
,	O
which	O
is	O
fully	O
branched	O
with	O
an	O
expected	O
value	O
.	O
to	O
what	O
extent	O
can	O
these	O
algorithms	O
be	O
uniﬁed	O
?	O
one	O
idea	O
for	O
uniﬁcation	O
is	O
suggested	O
by	O
the	O
fourth	O
backup	B
diagram	I
in	O
figure	O
7.5.	O
this	O
is	O
the	O
idea	O
that	O
one	O
might	O
decide	O
on	O
a	O
step-by-step	O
basis	O
whether	O
one	O
wanted	O
to	O
take	O
the	O
action	B
as	O
a	O
sample	O
,	O
as	O
in	O
sarsa	O
,	O
or	O
consider	O
the	O
expectation	O
over	O
all	O
actions	O
instead	O
,	O
as	O
in	O
the	O
tree-backup	O
update	O
.	O
then	O
,	O
if	O
one	O
chose	O
always	O
to	O
sample	O
,	O
one	O
would	O
obtain	O
sarsa	O
,	O
whereas	O
if	O
one	O
chose	O
never	O
to	O
sample	O
,	O
one	O
would	O
get	O
the	O
tree-backup	O
algorithm	O
.	O
expected	O
sarsa	O
would	O
be	O
the	O
case	O
where	O
one	O
chose	O
to	O
sample	O
for	O
all	O
steps	O
except	O
for	O
7.6.	O
a	O
unifying	O
algorithm	O
:	O
n-step	B
q	O
(	O
σ	O
)	O
155	O
figure	O
7.5	O
:	O
the	O
backup	O
diagrams	O
of	O
the	O
three	O
kinds	O
of	O
n-step	O
action-value	O
updates	O
considered	O
so	O
far	O
in	O
this	O
chapter	O
(	O
4-step	O
case	O
)	O
plus	O
the	O
backup	B
diagram	I
of	O
a	O
fourth	O
kind	O
of	B
update	I
that	O
uniﬁes	O
them	O
all	O
.	O
the	O
‘	O
ρ	O
’	O
s	O
indicate	O
half	O
transitions	O
on	O
which	O
importance	B
sampling	I
is	O
required	O
in	O
the	O
oﬀ-policy	O
case	O
.	O
the	O
fourth	O
kind	O
of	B
update	I
uniﬁes	O
all	O
the	O
others	O
by	O
choosing	O
on	O
a	O
state-by-state	O
basis	O
whether	O
to	O
sample	O
(	O
σt	O
=	O
1	O
)	O
or	O
not	O
(	O
σt	O
=	O
0	O
)	O
.	O
the	O
last	O
one	O
.	O
and	O
of	O
course	O
there	O
would	O
be	O
many	O
other	O
possibilities	O
,	O
as	O
suggested	O
by	O
the	O
last	O
diagram	O
in	O
the	O
ﬁgure	O
.	O
to	O
increase	O
the	O
possibilities	O
even	O
further	O
we	O
can	O
consider	O
a	O
continuous	O
variation	O
between	O
sampling	O
and	O
expectation	O
.	O
let	O
σt	O
∈	O
[	O
0	O
,	O
1	O
]	O
denote	O
the	O
degree	O
of	O
sampling	O
on	O
step	O
t	O
,	O
with	O
σ	O
=	O
1	O
denoting	O
full	O
sampling	O
and	O
σ	O
=	O
0	O
denoting	O
a	O
pure	O
expectation	O
with	O
no	O
sampling	O
.	O
the	O
random	O
variable	O
σt	O
might	O
be	O
set	O
as	O
a	O
function	O
of	O
the	O
state	B
,	O
action	B
,	O
or	O
state–action	O
pair	O
at	O
time	O
t.	O
we	O
call	O
this	O
proposed	O
new	O
algorithm	O
n-step	B
q	O
(	O
σ	O
)	O
.	O
now	O
let	O
us	O
develop	O
the	O
equations	O
of	O
n-step	O
q	O
(	O
σ	O
)	O
.	O
first	O
we	O
write	O
the	O
tree-backup	O
n-step	B
return	O
(	O
7.16	O
)	O
in	O
terms	O
of	O
the	O
horizon	O
h	O
=	O
t	O
+	O
n	O
and	O
then	O
the	O
expected	B
approximate	I
value	I
¯v	O
(	O
7.8	O
)	O
:	O
gt	O
:	O
h	O
=	O
rt+1	O
+	O
γ	O
(	O
cid:88	O
)	O
a	O
(	O
cid:54	O
)	O
=at+1	O
π	O
(	O
a|st+1	O
)	O
qh−1	O
(	O
st+1	O
,	O
a	O
)	O
+	O
γ	O
π	O
(	O
at+1|st+1	O
)	O
gt+1	O
:	O
h	O
=	O
rt+1	O
+	O
γ	O
¯vh−1	O
(	O
st+1	O
)	O
−	O
γπ	O
(	O
at+1|st+1	O
)	O
qh−1	O
(	O
st+1	O
,	O
at+1	O
)	O
+	O
γπ	O
(	O
at+1|st+1	O
)	O
gt+1	O
:	O
h	O
=	O
rt+1	O
+	O
γπ	O
(	O
at+1|st+1	O
)	O
(	O
cid:16	O
)	O
gt+1	O
:	O
h	O
−	O
qh−1	O
(	O
st+1	O
,	O
at+1	O
)	O
(	O
cid:17	O
)	O
+	O
γ	O
¯vh−1	O
(	O
st+1	O
)	O
,	O
after	O
which	O
it	O
is	O
exactly	O
like	O
the	O
n-step	B
return	O
for	O
sarsa	O
with	B
control	I
variates	I
(	O
7.14	O
)	O
except	O
with	O
the	O
action	B
probability	O
π	O
(	O
at+1|st+1	O
)	O
substituted	O
for	O
the	O
importance-sampling	O
ratio	B
⇢⇢⇢⇢⇢⇢⇢⇢⇢ =1 =0 =1 =04-stepsarsa4-steptree	O
backup4-stepexpected	O
sarsa4-step	O
q	O
(	O
 	O
)	O
156	O
chapter	O
7	O
:	O
n-step	B
bootstrapping	O
ρt+1	O
.	O
for	O
q	O
(	O
σ	O
)	O
,	O
we	O
slide	O
linearly	O
between	O
these	O
two	O
cases	O
:	O
gt	O
:	O
h	O
.	O
=	O
rt+1+γ	O
(	O
cid:16	O
)	O
σt+1ρt+1+	O
(	O
1−σt+1	O
)	O
π	O
(	O
at+1|st+1	O
)	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
gt+1	O
:	O
h−qh−1	O
(	O
st+1	O
,	O
at+1	O
)	O
(	O
cid:17	O
)	O
+γ	O
¯vh−1	O
(	O
st+1	O
)	O
,	O
(	O
7.17	O
)	O
.	O
for	O
t	O
<	O
h	O
≤	O
t	O
.	O
the	O
recursion	O
ends	O
with	O
gh	O
:	O
h	O
=	O
rt	O
if	O
h	O
=	O
t	O
.	O
then	O
we	O
the	O
usual	O
general	O
(	O
oﬀ-policy	B
)	O
update	O
for	O
n-step	O
sarsa	O
(	O
7.11	O
)	O
.	O
a	O
complete	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
.	O
.	O
=	O
0	O
if	O
h	O
<	O
t	O
,	O
or	O
with	O
gt−1	O
:	O
t	O
oﬀ-policy	B
n-step	O
q	O
(	O
σ	O
)	O
for	O
estimating	O
q	O
≈	O
q∗	O
or	O
qπ	O
input	O
:	O
an	O
arbitrary	O
behavior	B
policy	I
b	O
such	O
that	O
b	O
(	O
a|s	O
)	O
>	O
0	O
,	O
for	O
all	O
s	O
∈	O
s	O
,	O
a	O
∈	O
a	O
initialize	O
q	O
(	O
s	O
,	O
a	O
)	O
arbitrarily	O
,	O
for	O
all	O
s	O
∈	O
s	O
,	O
a	O
∈	O
a	O
initialize	O
π	O
to	O
be	O
ε-greedy	O
with	O
respect	O
to	O
q	O
,	O
or	O
as	O
a	O
ﬁxed	O
given	O
policy	B
algorithm	O
parameters	O
:	O
step	O
size	O
α	O
∈	O
(	O
0	O
,	O
1	O
]	O
,	O
small	O
ε	O
>	O
0	O
,	O
a	O
positive	O
integer	O
n	O
all	O
store	O
and	O
access	O
operations	O
can	O
take	O
their	O
index	O
mod	O
n	O
+	O
1	O
loop	O
for	O
each	O
episode	O
:	O
take	O
action	B
at	O
;	O
observe	O
and	O
store	O
the	O
next	O
reward	O
and	O
state	B
as	O
rt+1	O
,	O
st+1	O
if	O
st+1	O
is	O
terminal	O
:	O
else	O
:	O
t	O
←	O
t	O
+	O
1	O
choose	O
and	O
store	O
an	O
action	B
at+1	O
∼	O
b	O
(	O
·|st+1	O
)	O
select	O
and	O
store	O
σt+1	O
store	O
π	O
(	O
at+1|st+1	O
)	O
if	O
t	O
<	O
t	O
:	O
initialize	O
and	O
store	O
s0	O
(	O
cid:54	O
)	O
=	O
terminal	O
choose	O
and	O
store	O
an	O
action	B
a0	O
∼	O
b	O
(	O
·|s0	O
)	O
t	O
←	O
∞loop	O
for	O
t	O
=	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
:	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
until	O
τ	O
=	O
t	O
−	O
1	O
b	O
(	O
at+1|st+1	O
)	O
as	O
ρt+1	O
g	O
←	O
rt	O
if	O
k	O
=	O
t	O
:	O
(	O
τ	O
is	O
the	O
time	O
whose	O
estimate	O
is	O
being	O
updated	O
)	O
τ	O
←	O
t	O
−	O
n	O
+	O
1	O
if	O
τ	O
≥	O
0	O
:	O
g	O
←	O
0	O
:	O
loop	O
for	O
k	O
=	O
min	O
(	O
t	O
+	O
1	O
,	O
t	O
)	O
down	O
through	O
τ	O
+	O
1	O
:	O
else	O
:	O
¯v	O
←	O
(	O
cid:80	O
)	O
a	O
π	O
(	O
a|sk	O
)	O
q	O
(	O
sk	O
,	O
a	O
)	O
g	O
←	O
rk	O
+	O
γ	O
(	O
cid:0	O
)	O
σkρk	O
+	O
(	O
1	O
−	O
σk	O
)	O
π	O
(	O
ak|sk	O
)	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
g	O
−	O
q	O
(	O
sk	O
,	O
ak	O
)	O
(	O
cid:1	O
)	O
+	O
γ	O
¯v	O
q	O
(	O
sτ	O
,	O
aτ	O
)	O
←	O
q	O
(	O
sτ	O
,	O
aτ	O
)	O
+	O
α	O
[	O
g	O
−	O
q	O
(	O
sτ	O
,	O
aτ	O
)	O
]	O
if	O
π	O
is	O
being	O
learned	O
,	O
then	O
ensure	O
that	O
π	O
(	O
·|sτ	O
)	O
is	O
greedy	O
wrt	O
q	O
7.7.	O
summary	O
7.7	O
summary	O
157	O
in	O
this	O
chapter	O
we	O
have	O
developed	O
a	O
range	O
of	O
temporal-diﬀerence	O
learning	O
methods	O
that	O
lie	O
in	O
between	O
the	O
one-step	O
td	O
methods	O
of	O
the	O
previous	O
chapter	O
and	O
the	O
monte	O
carlo	O
methods	O
of	O
the	O
chapter	O
before	O
.	O
methods	O
that	O
involve	O
an	O
intermediate	O
amount	O
of	O
boot-	O
strapping	O
are	O
important	O
because	O
they	O
will	O
typically	O
perform	O
better	O
than	O
either	O
extreme	O
.	O
our	O
focus	O
in	O
this	O
chapter	O
has	O
been	O
on	O
n-step	B
methods	I
,	O
which	O
look	O
ahead	O
to	O
the	O
next	O
n	O
rewards	O
,	O
states	O
,	O
and	O
actions	O
.	O
the	O
two	O
4-step	O
backup	O
diagrams	O
to	O
the	O
right	O
together	O
summarize	O
most	O
of	O
the	O
methods	O
introduced	O
.	O
the	O
state-value	O
update	O
shown	O
is	O
for	O
n-	O
step	O
td	O
with	O
importance	O
sampling	O
,	O
and	O
the	O
action-value	O
update	O
is	O
for	O
n-step	O
q	O
(	O
σ	O
)	O
,	O
which	O
generalizes	O
expected	O
sarsa	O
and	O
q-learning	O
.	O
all	O
n-step	B
methods	I
involve	O
a	O
delay	O
of	O
n	O
time	O
steps	O
before	O
updating	O
,	O
as	O
only	O
then	O
are	O
all	O
the	O
required	O
future	O
events	O
known	O
.	O
a	O
further	O
drawback	O
is	O
that	O
they	O
involve	O
more	O
computation	O
per	O
time	O
step	O
than	O
previous	O
methods	O
.	O
compared	O
to	O
one-step	O
methods	O
,	O
n-step	B
methods	I
also	O
require	O
more	O
memory	O
to	O
record	O
the	O
states	O
,	O
actions	O
,	O
rewards	O
,	O
and	O
sometimes	O
other	O
variables	O
over	O
the	O
last	O
n	O
time	O
steps	O
.	O
eventually	O
,	O
in	O
chapter	O
12	O
,	O
we	O
will	O
see	O
how	O
multi-step	O
td	O
meth-	O
ods	O
can	O
be	O
implemented	O
with	O
minimal	O
memory	O
and	O
computational	O
complexity	O
using	O
eligibility	B
traces	I
,	O
but	O
there	O
will	O
always	O
be	O
some	O
additional	O
computation	O
beyond	O
one-step	O
methods	O
.	O
such	O
costs	O
can	O
be	O
well	O
worth	O
paying	O
to	O
escape	O
the	O
tyranny	O
of	O
the	O
single	O
time	O
step	O
.	O
although	O
n-step	B
methods	I
are	O
more	O
complex	O
than	O
those	O
using	O
eligibility	B
traces	I
,	O
they	O
have	O
the	O
great	O
beneﬁt	O
of	O
being	O
conceptually	O
clear	O
.	O
we	O
have	O
sought	O
to	O
take	O
advantage	O
of	O
this	O
by	O
developing	O
two	O
approaches	O
to	O
oﬀ-policy	B
learning	O
in	O
the	O
n-step	O
case	O
.	O
one	O
,	O
based	O
on	O
importance	B
sampling	I
is	O
conceptually	O
simple	O
but	O
can	O
be	O
of	O
high	O
variance	O
.	O
if	O
the	O
target	B
and	O
behavior	O
policies	O
are	O
very	O
diﬀerent	O
it	O
probably	O
needs	O
some	O
new	O
algorithmic	O
ideas	O
before	O
it	O
can	O
be	O
eﬃcient	O
and	O
practical	O
.	O
the	O
other	O
,	O
based	O
on	O
tree-backup	O
updates	O
,	O
is	O
the	O
natural	O
extension	O
of	O
q-learning	O
to	O
the	O
multi-step	O
case	O
with	O
stochastic	O
target	B
policies	O
.	O
it	O
involves	O
no	O
importance	B
sampling	I
but	O
,	O
again	O
if	O
the	O
target	B
and	O
behavior	O
policies	O
are	O
substantially	O
diﬀerent	O
,	O
the	O
bootstrapping	B
may	O
span	O
only	O
a	O
few	O
steps	O
even	O
if	O
n	O
is	O
large	O
.	O
⇢⇢ =1 =0 =1 =04-step	O
q	O
(	O
 	O
)	O
⇢⇢⇢⇢4-steptd	O
158	O
chapter	O
7	O
:	O
n-step	B
bootstrapping	O
bibliographical	O
and	O
historical	O
remarks	O
the	O
notion	O
of	O
n-step	O
returns	O
is	O
due	O
to	O
watkins	O
(	O
1989	O
)	O
,	O
who	O
also	O
ﬁrst	O
discussed	O
their	O
error	B
reduction	I
property	I
.	O
n-step	B
algorithms	O
were	O
explored	O
in	O
the	O
ﬁrst	O
edition	O
of	O
this	O
book	O
,	O
in	O
which	O
they	O
were	O
treated	O
as	O
of	O
conceptual	O
interest	O
,	O
but	O
not	O
feasible	O
in	O
practice	O
.	O
the	O
work	O
of	O
cichosz	O
(	O
1995	O
)	O
and	O
particularly	O
van	O
seijen	O
(	O
2016	O
)	O
showed	O
that	O
they	O
are	O
actually	O
completely	O
practical	O
algorithms	O
.	O
given	O
this	O
,	O
and	O
their	O
conceptual	O
clarity	O
and	O
simplicity	O
,	O
we	O
have	O
chosen	O
to	O
highlight	O
them	O
here	O
in	O
the	O
second	O
edition	O
.	O
in	O
particular	O
,	O
we	O
now	O
postpone	O
all	O
discussion	O
of	O
the	O
backward	O
view	O
and	O
of	O
eligibility	O
traces	O
until	O
chapter	O
12	O
.	O
7.1–2	O
the	O
results	O
in	O
the	O
random	O
walk	O
examples	O
were	O
made	O
for	O
this	O
text	O
based	O
on	O
work	O
of	O
sutton	O
(	O
1988	O
)	O
and	O
singh	O
and	O
sutton	O
(	O
1996	O
)	O
.	O
the	O
use	O
of	O
backup	O
diagrams	O
to	O
describe	O
these	O
and	O
other	O
algorithms	O
in	O
this	O
chapter	O
is	O
new	O
.	O
7.3–5	O
the	O
developments	O
in	O
these	O
sections	O
are	O
based	O
on	O
the	O
work	O
of	O
precup	O
,	O
sutton	O
,	O
and	O
singh	O
(	O
2000	O
)	O
,	O
precup	O
,	O
sutton	O
,	O
and	O
dasgupta	O
(	O
2001	O
)	O
,	O
and	O
sutton	O
,	O
mahmood	O
,	O
precup	O
,	O
and	O
van	O
hasselt	O
(	O
2014	O
)	O
.	O
the	O
tree-backup	O
algorithm	O
is	O
due	O
to	O
precup	O
,	O
sutton	O
,	O
and	O
singh	O
(	O
2000	O
)	O
,	O
but	O
the	O
presentation	O
of	O
it	O
here	O
is	O
new	O
.	O
7.6	O
the	O
q	O
(	O
σ	O
)	O
algorithm	O
is	O
new	O
to	O
this	O
text	O
,	O
but	O
closely	O
related	O
algorithms	O
have	O
been	O
explored	O
further	O
by	O
de	O
asis	O
,	O
hernandez-garcia	O
,	O
holland	O
,	O
and	O
sutton	O
(	O
2017	O
)	O
.	O
chapter	O
8	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
in	O
this	O
chapter	O
we	O
develop	O
a	O
uniﬁed	O
view	O
of	O
reinforcement	O
learning	O
methods	O
that	O
require	O
a	O
model	B
of	I
the	I
environment	I
,	O
such	O
as	O
dynamic	O
programming	O
and	O
heuristic	O
search	O
,	O
and	O
methods	O
that	O
can	O
be	O
used	O
without	O
a	O
model	O
,	O
such	O
as	O
monte	O
carlo	O
and	O
temporal-diﬀerence	O
methods	O
.	O
these	O
are	O
respectively	O
called	O
model-based	O
and	O
model-free	O
reinforcement	O
learn-	O
ing	B
methods	O
.	O
model-based	O
methods	O
rely	O
on	O
planning	B
as	O
their	O
primary	O
component	O
,	O
while	O
model-free	O
methods	O
primarily	O
rely	O
on	O
learning	O
.	O
although	O
there	O
are	O
real	O
diﬀerences	O
be-	O
tween	O
these	O
two	O
kinds	O
of	O
methods	O
,	O
there	O
are	O
also	O
great	O
similarities	O
.	O
in	O
particular	O
,	O
the	O
heart	O
of	O
both	O
kinds	O
of	O
methods	O
is	O
the	O
computation	O
of	O
value	O
functions	O
.	O
moreover	O
,	O
all	O
the	O
methods	O
are	O
based	O
on	O
looking	O
ahead	O
to	O
future	O
events	O
,	O
computing	O
a	O
backed-up	O
value	B
,	O
and	O
then	O
using	O
it	O
as	O
an	O
update	O
target	B
for	O
an	O
approximate	B
value	O
function	O
.	O
earlier	O
in	O
this	O
book	O
we	O
presented	O
monte	O
carlo	O
and	O
temporal-diﬀerence	O
methods	O
as	O
distinct	O
alternatives	O
,	O
then	O
showed	O
how	O
they	O
can	O
be	O
uniﬁed	O
by	O
n-step	B
methods	I
.	O
our	O
goal	B
in	O
this	O
chapter	O
is	O
a	O
similar	O
integration	O
of	O
model-based	O
and	O
model-free	O
methods	O
.	O
having	O
established	O
these	O
as	O
distinct	O
in	O
earlier	O
chapters	O
,	O
we	O
now	O
explore	O
the	O
extent	O
to	O
which	O
they	O
can	O
be	O
intermixed	O
.	O
8.1	O
models	O
and	O
planning	O
by	O
a	O
model	B
of	I
the	I
environment	I
we	O
mean	O
anything	O
that	O
an	O
agent	O
can	O
use	O
to	O
predict	O
how	O
the	O
environment	B
will	O
respond	O
to	O
its	O
actions	O
.	O
given	O
a	O
state	B
and	O
an	O
action	B
,	O
a	O
model	O
produces	O
a	O
prediction	B
of	O
the	O
resultant	O
next	O
state	B
and	O
next	O
reward	O
.	O
if	O
the	O
model	O
is	O
stochastic	O
,	O
then	O
there	O
are	O
several	O
possible	O
next	O
states	O
and	O
next	O
rewards	O
,	O
each	O
with	O
some	O
probability	O
of	O
occurring	O
.	O
some	O
models	O
produce	O
a	O
description	O
of	O
all	O
possibilities	O
and	O
their	O
probabilities	O
;	O
these	O
we	O
call	O
distribution	B
models	I
.	O
other	O
models	O
produce	O
just	O
one	O
of	O
the	O
possibilities	O
,	O
sampled	O
according	O
to	O
the	O
probabilities	O
;	O
these	O
we	O
call	O
sample	O
models	O
.	O
for	O
example	O
,	O
consider	O
modeling	O
the	O
sum	O
of	O
a	O
dozen	O
dice	O
.	O
a	O
distribution	O
model	O
would	O
produce	O
all	O
possible	O
sums	O
and	O
their	O
probabilities	O
of	O
occurring	O
,	O
whereas	O
a	O
sample	O
model	O
159	O
160	O
chapter	O
8	O
:	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
would	O
produce	O
an	O
individual	O
sum	O
drawn	O
according	O
to	O
this	O
probability	O
distribution	O
.	O
the	O
kind	O
of	O
model	O
assumed	O
in	O
dynamic	O
programming—estimates	O
of	O
the	O
mdp	O
’	O
s	O
dynamics	O
,	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
—is	O
a	O
distribution	O
model	O
.	O
the	O
kind	O
of	O
model	O
used	O
in	O
the	O
blackjack	O
example	O
in	O
chapter	O
5	O
is	O
a	O
sample	O
model	O
.	O
distribution	B
models	I
are	O
stronger	O
than	O
sample	O
models	O
in	O
that	O
they	O
can	O
always	O
be	O
used	O
to	O
produce	O
samples	O
.	O
however	O
,	O
in	O
many	O
applications	O
it	O
is	O
much	O
easier	O
to	O
obtain	O
sample	O
models	O
than	O
distribution	B
models	I
.	O
the	O
dozen	O
dice	O
are	O
a	O
simple	O
example	O
of	O
this	O
.	O
it	O
would	O
be	O
easy	O
to	O
write	O
a	O
computer	O
program	O
to	O
simulate	O
the	O
dice	O
rolls	O
and	O
return	O
the	O
sum	O
,	O
but	O
harder	O
and	O
more	O
error-prone	O
to	O
ﬁgure	O
out	O
all	O
the	O
possible	O
sums	O
and	O
their	O
probabilities	O
.	O
models	O
can	O
be	O
used	O
to	O
mimic	O
or	O
simulate	O
experience	O
.	O
given	O
a	O
starting	O
state	B
and	O
action	B
,	O
a	O
sample	O
model	O
produces	O
a	O
possible	O
transition	O
,	O
and	O
a	O
distribution	O
model	O
generates	O
all	O
possible	O
transitions	O
weighted	O
by	O
their	O
probabilities	O
of	O
occurring	O
.	O
given	O
a	O
starting	O
state	B
and	O
a	O
policy	B
,	O
a	O
sample	O
model	O
could	O
produce	O
an	O
entire	O
episode	O
,	O
and	O
a	O
distribution	O
model	O
could	O
generate	O
all	O
possible	O
episodes	B
and	O
their	O
probabilities	O
.	O
in	O
either	O
case	O
,	O
we	O
say	O
the	O
model	O
is	O
used	O
to	O
simulate	O
the	O
environment	B
and	O
produce	O
simulated	O
experience	O
.	O
the	O
word	O
planning	B
is	O
used	O
in	O
several	O
diﬀerent	O
ways	O
in	O
diﬀerent	O
ﬁelds	O
.	O
we	O
use	O
the	O
term	O
to	O
refer	O
to	O
any	O
computational	O
process	O
that	O
takes	O
a	O
model	O
as	O
input	O
and	O
produces	O
or	O
improves	O
a	O
policy	B
for	O
interacting	O
with	O
the	O
modeled	O
environment	B
:	O
in	O
artiﬁcial	O
intelligence	O
,	O
there	O
are	O
two	O
distinct	O
approaches	O
to	O
planning	B
according	O
to	O
our	O
deﬁnition	O
.	O
state-space	O
planning	B
,	O
which	O
includes	O
the	O
approach	O
we	O
take	O
in	O
this	O
book	O
,	O
is	O
viewed	O
primarily	O
as	O
a	O
search	O
through	O
the	O
state	B
space	O
for	O
an	O
optimal	O
policy	O
or	O
an	O
opti-	O
mal	O
path	O
to	O
a	O
goal	B
.	O
actions	O
cause	O
transitions	O
from	O
state	B
to	O
state	B
,	O
and	O
value	O
functions	O
are	O
computed	O
over	O
states	O
.	O
in	O
what	O
we	O
call	O
plan-space	O
planning	B
,	O
planning	B
is	O
instead	O
a	O
search	O
through	O
the	O
space	O
of	O
plans	O
.	O
operators	O
transform	O
one	O
plan	O
into	O
another	O
,	O
and	O
value	O
functions	O
,	O
if	O
any	O
,	O
are	O
deﬁned	O
over	O
the	O
space	O
of	O
plans	O
.	O
plan-space	O
planning	B
includes	O
evo-	O
lutionary	O
methods	O
and	O
“	O
partial-order	O
planning	B
,	O
”	O
a	O
common	O
kind	O
of	O
planning	O
in	O
artiﬁcial	O
intelligence	O
in	O
which	O
the	O
ordering	O
of	O
steps	O
is	O
not	O
completely	O
determined	O
at	O
all	O
stages	O
of	O
planning	O
.	O
plan-space	O
methods	O
are	O
diﬃcult	O
to	O
apply	O
eﬃciently	O
to	O
the	O
stochastic	O
se-	O
quential	O
decision	O
problems	O
that	O
are	O
the	O
focus	O
in	O
reinforcement	O
learning	O
,	O
and	O
we	O
do	O
not	O
consider	O
them	O
further	O
(	O
but	O
see	O
,	O
e.g.	O
,	O
russell	O
and	O
norvig	O
,	O
2010	O
)	O
.	O
the	O
uniﬁed	O
view	O
we	O
present	O
in	O
this	O
chapter	O
is	O
that	O
all	O
state-space	O
planning	B
methods	O
share	O
a	O
common	O
structure	O
,	O
a	O
structure	O
that	O
is	O
also	O
present	O
in	O
the	O
learning	O
methods	O
pre-	O
sented	O
in	O
this	O
book	O
.	O
it	O
takes	O
the	O
rest	O
of	O
the	O
chapter	O
to	O
develop	O
this	O
view	O
,	O
but	O
there	O
are	O
two	O
basic	O
ideas	O
:	O
(	O
1	O
)	O
all	O
state-space	O
planning	B
methods	O
involve	O
computing	O
value	B
functions	O
as	O
a	O
key	O
intermediate	O
step	O
toward	O
improving	O
the	O
policy	B
,	O
and	O
(	O
2	O
)	O
they	O
compute	O
value	B
func-	O
tions	O
by	O
updates	O
or	O
backup	O
operations	O
applied	O
to	O
simulated	O
experience	O
.	O
this	O
common	O
structure	O
can	O
be	O
diagrammed	O
as	O
follows	O
:	O
dynamic	B
programming	I
methods	O
clearly	O
ﬁt	O
this	O
structure	O
:	O
they	O
make	O
sweeps	B
through	O
the	O
space	O
of	O
states	O
,	O
generating	O
for	O
each	O
state	B
the	O
distribution	O
of	O
possible	O
transitions	O
.	O
each	O
planningmodelpolicyvaluesbackupsmodelsimulatedexperiencepolicyupdatesbackups	O
8.2.	O
dyna	O
:	O
integrated	O
planning	B
,	O
acting	O
,	O
and	O
learning	O
161	O
distribution	O
is	O
then	O
used	O
to	O
compute	O
a	O
backed-up	O
value	B
(	O
update	O
target	B
)	O
and	O
update	O
the	O
state	B
’	O
s	O
estimated	O
value	B
.	O
in	O
this	O
chapter	O
we	O
argue	O
that	O
various	O
other	O
state-space	O
planning	B
methods	O
also	O
ﬁt	O
this	O
structure	O
,	O
with	O
individual	O
methods	O
diﬀering	O
only	O
in	O
the	O
kinds	O
of	O
updates	O
they	O
do	O
,	O
the	O
order	O
in	O
which	O
they	O
do	O
them	O
,	O
and	O
in	O
how	O
long	O
the	O
backed-up	O
information	O
is	O
retained	O
.	O
viewing	O
planning	B
methods	O
in	O
this	O
way	O
emphasizes	O
their	O
relationship	O
to	O
the	O
learning	O
methods	O
that	O
we	O
have	O
described	O
in	O
this	O
book	O
.	O
the	O
heart	O
of	O
both	O
learning	O
and	O
planning	B
methods	O
is	O
the	O
estimation	O
of	O
value	O
functions	O
by	O
backing-up	O
update	O
operations	O
.	O
the	O
diﬀerence	O
is	O
that	O
whereas	O
planning	B
uses	O
simulated	O
experience	O
generated	O
by	O
a	O
model	O
,	O
learning	O
methods	O
use	O
real	O
experience	O
generated	O
by	O
the	O
environment	B
.	O
of	O
course	O
this	O
diﬀerence	O
leads	O
to	O
a	O
number	O
of	O
other	O
diﬀerences	O
,	O
for	O
example	O
,	O
in	O
how	O
performance	O
is	O
assessed	O
and	O
in	O
how	O
ﬂexibly	O
experience	O
can	O
be	O
generated	O
.	O
but	O
the	O
common	O
structure	O
means	O
that	O
many	O
ideas	O
and	O
algorithms	O
can	O
be	O
transferred	O
between	O
planning	B
and	O
learning	O
.	O
in	O
particular	O
,	O
in	O
many	O
cases	O
a	O
learning	O
algorithm	O
can	O
be	O
substituted	O
for	O
the	O
key	O
update	O
step	O
of	O
a	O
planning	B
method	O
.	O
learning	O
methods	O
require	O
only	O
experience	O
as	O
input	O
,	O
and	O
in	O
many	O
cases	O
they	O
can	O
be	O
applied	O
to	O
simulated	O
experience	O
just	O
as	O
well	O
as	O
to	O
real	O
experience	O
.	O
the	O
box	O
below	O
shows	O
a	O
simple	O
example	O
of	O
a	O
planning	B
method	O
based	O
on	O
one-step	O
tabular	O
q-learning	O
and	O
on	O
random	O
samples	O
from	O
a	O
sample	O
model	O
.	O
this	O
method	O
,	O
which	O
we	O
call	O
random-sample	O
one-step	O
tabular	O
q-planning	O
,	O
converges	O
to	O
the	O
optimal	O
policy	O
for	O
the	O
model	O
under	O
the	O
same	O
conditions	O
that	O
one-step	O
tabular	O
q-learning	O
converges	O
to	O
the	O
optimal	O
policy	O
for	O
the	O
real	O
environment	B
(	O
each	O
state–action	O
pair	O
must	O
be	O
selected	O
an	O
inﬁnite	O
number	O
of	O
times	O
in	O
step	O
1	O
,	O
and	O
α	O
must	O
decrease	O
appropriately	O
over	O
time	O
)	O
.	O
random-sample	O
one-step	O
tabular	O
q-planning	O
loop	O
forever	O
:	O
1.	O
select	O
a	O
state	B
,	O
s	O
∈	O
s	O
,	O
and	O
an	O
action	B
,	O
a	O
∈	O
a	O
(	O
s	O
)	O
,	O
at	O
random	O
2.	O
send	O
s	O
,	O
a	O
to	O
a	O
sample	O
model	O
,	O
and	O
obtain	O
a	O
sample	O
next	O
reward	O
,	O
r	O
,	O
and	O
a	O
sample	O
next	O
state	B
,	O
s	O
(	O
cid:48	O
)	O
3.	O
apply	O
one-step	O
tabular	O
q-learning	O
to	O
s	O
,	O
a	O
,	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
:	O
q	O
(	O
s	O
,	O
a	O
)	O
←	O
q	O
(	O
s	O
,	O
a	O
)	O
+	O
α	O
(	O
cid:2	O
)	O
r	O
+	O
γ	O
maxa	O
q	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
)	O
−	O
q	O
(	O
s	O
,	O
a	O
)	O
(	O
cid:3	O
)	O
in	O
addition	O
to	O
the	O
uniﬁed	O
view	O
of	O
planning	O
and	O
learning	O
methods	O
,	O
a	O
second	O
theme	O
in	O
this	O
chapter	O
is	O
the	O
beneﬁts	O
of	O
planning	O
in	O
small	O
,	O
incremental	O
steps	O
.	O
this	O
enables	O
planning	B
to	O
be	O
interrupted	O
or	O
redirected	O
at	O
any	O
time	O
with	O
little	O
wasted	O
computation	O
,	O
which	O
appears	O
to	O
be	O
a	O
key	O
requirement	O
for	O
eﬃciently	O
intermixing	O
planning	O
with	O
acting	O
and	O
with	O
learning	O
of	O
the	O
model	O
.	O
planning	B
in	O
very	O
small	O
steps	O
may	O
be	O
the	O
most	O
eﬃcient	O
approach	O
even	O
on	O
pure	O
planning	B
problems	O
if	O
the	O
problem	O
is	O
too	O
large	O
to	O
be	O
solved	O
exactly	O
.	O
8.2	O
dyna	O
:	O
integrated	O
planning	B
,	O
acting	O
,	O
and	O
learning	O
when	O
planning	B
is	O
done	O
online	B
,	O
while	O
interacting	O
with	O
the	O
environment	B
,	O
a	O
number	O
of	O
interesting	O
issues	O
arise	O
.	O
new	O
information	O
gained	O
from	O
the	O
interaction	O
may	O
change	O
the	O
model	O
and	O
thereby	O
interact	O
with	O
planning	O
.	O
it	O
may	O
be	O
desirable	O
to	O
customize	O
the	O
planning	B
process	O
in	O
some	O
way	O
to	O
the	O
states	O
or	O
decisions	O
currently	O
under	O
consideration	O
,	O
or	O
expected	O
162	O
chapter	O
8	O
:	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
in	O
the	O
near	O
future	O
.	O
if	O
decision	O
making	O
and	O
model	O
learning	O
are	O
both	O
computation-intensive	O
processes	O
,	O
then	O
the	O
available	O
computational	O
resources	O
may	O
need	O
to	O
be	O
divided	O
between	O
them	O
.	O
to	O
begin	O
exploring	O
these	O
issues	O
,	O
in	O
this	O
section	O
we	O
present	O
dyna-q	O
,	O
a	O
simple	O
architecture	O
integrating	O
the	O
major	O
functions	O
needed	O
in	O
an	O
online	B
planning	O
agent	O
.	O
each	O
function	O
appears	O
in	O
dyna-q	O
in	O
a	O
simple	O
,	O
almost	O
trivial	O
,	O
form	O
.	O
in	O
subsequent	O
sections	O
we	O
elaborate	O
some	O
of	O
the	O
alternate	O
ways	O
of	O
achieving	O
each	O
function	O
and	O
the	O
trade-oﬀs	O
between	O
them	O
.	O
for	O
now	O
,	O
we	O
seek	O
merely	O
to	O
illustrate	O
the	O
ideas	O
and	O
stimulate	O
your	O
intuition	O
.	O
within	O
a	O
planning	B
agent	O
,	O
there	O
are	O
at	O
least	O
two	O
roles	O
for	O
real	O
experience	O
:	O
it	O
can	O
be	O
used	O
to	O
improve	O
the	O
model	O
(	O
to	O
make	O
it	O
more	O
accurately	O
match	O
the	O
real	O
environment	B
)	O
and	O
it	O
can	O
be	O
used	O
to	O
directly	O
improve	O
the	O
value	B
function	I
and	O
policy	B
using	O
the	O
kinds	O
of	O
reinforcement	O
learning	O
methods	O
we	O
have	O
discussed	O
in	O
previous	O
chapters	O
.	O
the	O
former	O
we	O
call	O
model-	O
learning	O
,	O
and	O
the	O
latter	O
we	O
call	O
direct	O
reinforce-	O
ment	O
learning	O
(	O
direct	O
rl	O
)	O
.	O
the	O
possible	O
relation-	O
ships	O
between	O
experience	O
,	O
model	O
,	O
values	O
,	O
and	O
pol-	O
icy	O
are	O
summarized	O
in	O
the	O
diagram	O
to	O
the	O
right	O
.	O
each	O
arrow	O
shows	O
a	O
relationship	O
of	O
inﬂuence	O
and	O
presumed	O
improvement	O
.	O
note	O
how	O
experience	O
can	O
improve	O
value	B
functions	O
and	O
policies	O
either	O
di-	O
rectly	O
or	O
indirectly	O
via	O
the	O
model	O
.	O
it	O
is	O
the	O
latter	O
,	O
which	O
is	O
sometimes	O
called	O
indirect	O
reinforcement	B
learning	I
,	O
that	O
is	O
involved	O
in	O
planning	O
.	O
both	O
direct	O
and	O
indirect	O
methods	O
have	O
advantages	O
and	O
disadvantages	O
.	O
indirect	O
meth-	O
ods	O
often	O
make	O
fuller	O
use	O
of	O
a	O
limited	O
amount	O
of	O
experience	O
and	O
thus	O
achieve	O
a	O
better	O
policy	B
with	O
fewer	O
environmental	O
interactions	O
.	O
on	O
the	O
other	O
hand	O
,	O
direct	O
methods	O
are	O
much	O
simpler	O
and	O
are	O
not	O
aﬀected	O
by	O
biases	O
in	O
the	O
design	O
of	O
the	O
model	O
.	O
some	O
have	O
argued	O
that	O
indirect	O
methods	O
are	O
always	O
superior	O
to	O
direct	O
ones	O
,	O
while	O
others	O
have	O
ar-	O
gued	O
that	O
direct	O
methods	O
are	O
responsible	O
for	O
most	O
human	O
and	O
animal	O
learning	O
.	O
related	O
debates	O
in	B
psychology	I
and	O
artiﬁcial	B
intelligence	I
concern	O
the	O
relative	O
importance	O
of	O
cog-	O
nition	O
as	O
opposed	O
to	O
trial-and-error	B
learning	O
,	O
and	O
of	O
deliberative	O
planning	B
as	O
opposed	O
to	O
reactive	O
decision	O
making	O
(	O
see	O
chapter	O
14	O
for	O
discussion	O
of	O
some	O
of	O
these	O
issues	O
from	O
the	O
perspective	O
of	O
psychology	O
)	O
.	O
our	O
view	O
is	O
that	O
the	O
contrast	O
between	O
the	O
alternatives	O
in	O
all	O
these	O
debates	O
has	O
been	O
exaggerated	O
,	O
that	O
more	O
insight	O
can	O
be	O
gained	O
by	O
recog-	O
nizing	O
the	O
similarities	O
between	O
these	O
two	O
sides	O
than	O
by	O
opposing	O
them	O
.	O
for	O
example	O
,	O
in	O
this	O
book	O
we	O
have	O
emphasized	O
the	O
deep	O
similarities	O
between	O
dynamic	B
programming	I
and	O
temporal-diﬀerence	O
methods	O
,	O
even	O
though	O
one	O
was	O
designed	O
for	O
planning	O
and	O
the	O
other	O
for	O
model-free	O
learning	O
.	O
dyna-q	O
includes	O
all	O
of	O
the	O
processes	O
shown	O
in	O
the	O
diagram	O
above—planning	O
,	O
acting	O
,	O
model-learning	O
,	O
and	O
direct	O
rl—all	O
occurring	O
continually	O
.	O
the	O
planning	B
method	O
is	O
the	O
random-sample	O
one-step	O
tabular	O
q-planning	O
method	O
on	O
page	O
161.	O
the	O
direct	O
rl	O
method	O
is	O
one-step	O
tabular	O
q-learning	O
.	O
the	O
model-learning	O
method	O
is	O
also	O
table-based	O
and	O
as-	O
sumes	O
the	O
environment	B
is	O
deterministic	O
.	O
after	O
each	O
transition	O
st	O
,	O
at	O
→	O
rt+1	O
,	O
st+1	O
,	O
the	O
model	O
records	O
in	O
its	O
table	O
entry	O
for	O
st	O
,	O
at	O
the	O
prediction	B
that	O
rt+1	O
,	O
st+1	O
will	O
determin-	O
istically	O
follow	O
.	O
thus	O
,	O
if	O
the	O
model	O
is	O
queried	O
with	O
a	O
state–action	O
pair	O
that	O
has	O
been	O
planningvalue/policyexperiencemodelmodellearningactingdirectrl	O
8.2.	O
dyna	O
:	O
integrated	O
planning	B
,	O
acting	O
,	O
and	O
learning	O
163	O
experienced	O
before	O
,	O
it	O
simply	O
returns	O
the	O
last-observed	O
next	O
state	B
and	O
next	O
reward	O
as	O
its	O
prediction	B
.	O
during	O
planning	B
,	O
the	O
q-planning	O
algorithm	O
randomly	O
samples	O
only	O
from	O
state–action	O
pairs	O
that	O
have	O
previously	O
been	O
experienced	O
(	O
in	O
step	O
1	O
)	O
,	O
so	O
the	O
model	O
is	O
never	O
queried	O
with	O
a	O
pair	O
about	O
which	O
it	O
has	O
no	O
information	O
.	O
the	O
overall	O
architecture	O
of	O
dyna	O
agents	O
,	O
of	O
which	O
the	O
dyna-q	O
algorithm	O
is	O
one	O
exam-	O
ple	O
,	O
is	O
shown	O
in	O
figure	O
8.1.	O
the	O
central	O
column	O
represents	O
the	O
basic	O
interaction	O
between	O
agent	O
and	O
environment	O
,	O
giving	O
rise	O
to	O
a	O
trajectory	O
of	O
real	O
experience	O
.	O
the	O
arrow	O
on	O
the	O
left	O
of	O
the	O
ﬁgure	O
represents	O
direct	O
reinforcement	O
learning	O
operating	O
on	O
real	O
experience	O
to	O
improve	O
the	O
value	B
function	I
and	O
the	O
policy	B
.	O
on	O
the	O
right	O
are	O
model-based	O
processes	O
.	O
the	O
model	O
is	O
learned	O
from	O
real	O
experience	O
and	O
gives	O
rise	O
to	O
simulated	O
experience	O
.	O
we	O
use	O
the	O
term	O
search	B
control	I
to	O
refer	O
to	O
the	O
process	O
that	O
selects	O
the	O
starting	O
states	O
and	O
actions	O
for	O
the	O
simulated	O
experiences	O
generated	O
by	O
the	O
model	O
.	O
finally	O
,	O
planning	B
is	O
achieved	O
by	O
applying	O
reinforcement	B
learning	I
methods	O
to	O
the	O
simulated	O
experiences	O
just	O
as	O
if	O
they	O
had	O
really	O
happened	O
.	O
typically	O
,	O
as	O
in	O
dyna-q	O
,	O
the	O
same	O
reinforcement	B
learning	I
method	O
is	O
used	O
both	O
for	O
learning	O
from	O
real	O
experience	O
and	O
for	O
planning	O
from	O
simulated	O
experience	O
.	O
the	O
reinforcement	B
learning	I
method	O
is	O
thus	O
the	O
“	O
ﬁnal	O
common	O
path	O
”	O
for	O
both	O
learning	O
and	O
planning	B
.	O
learning	O
and	O
planning	B
are	O
deeply	O
integrated	O
in	O
the	O
sense	O
that	O
they	O
share	O
almost	O
all	O
the	O
same	O
machinery	O
,	O
diﬀering	O
only	O
in	O
the	O
source	O
of	O
their	O
experience	O
.	O
figure	O
8.1	O
:	O
the	O
general	O
dyna	O
architecture	O
.	O
real	O
experience	O
,	O
passing	O
back	O
and	O
forth	O
between	O
the	O
environment	B
and	O
the	O
policy	B
,	O
aﬀects	O
policy	B
and	O
value	B
functions	O
in	O
much	O
the	O
same	O
way	O
as	O
does	O
simulated	O
experience	O
generated	O
by	O
the	O
model	B
of	I
the	I
environment	I
.	O
conceptually	O
,	O
planning	B
,	O
acting	O
,	O
model-learning	O
,	O
and	O
direct	O
rl	O
occur	O
simultaneously	O
and	O
in	O
parallel	O
in	O
dyna	O
agents	O
.	O
for	O
concreteness	O
and	O
implementation	O
on	O
a	O
serial	O
com-	O
puter	O
,	O
however	O
,	O
we	O
fully	O
specify	O
the	O
order	O
in	O
which	O
they	O
occur	O
within	O
a	O
time	O
step	O
.	O
in	O
dyna-q	O
,	O
the	O
acting	O
,	O
model-learning	O
,	O
and	O
direct	O
rl	O
processes	O
require	O
little	O
computation	O
,	O
and	O
we	O
assume	O
they	O
consume	O
just	O
a	O
fraction	O
of	O
the	O
time	O
.	O
the	O
remaining	O
time	O
in	O
each	O
step	O
can	O
be	O
devoted	O
to	O
the	O
planning	B
process	O
,	O
which	O
is	O
inherently	O
computation-intensive	O
.	O
realdirect	O
rlupdatemodelplanning	O
updatesearchcontrolpolicy/value	O
functionsexperiencemodellearningenvironmentsimulatedexperience	O
164	O
chapter	O
8	O
:	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
let	O
us	O
assume	O
that	O
there	O
is	O
time	O
in	O
each	O
step	O
,	O
after	O
acting	O
,	O
model-learning	O
,	O
and	O
direct	O
rl	O
,	O
to	O
complete	O
n	O
iterations	O
(	O
steps	O
1–3	O
)	O
of	O
the	O
q-planning	O
algorithm	O
.	O
in	O
the	O
pseudocode	O
algorithm	O
for	O
dyna-q	O
in	O
the	O
box	O
below	O
,	O
m	O
odel	O
(	O
s	O
,	O
a	O
)	O
denotes	O
the	O
contents	O
of	O
the	O
(	O
pre-	O
dicted	O
next	O
state	B
and	O
reward	O
)	O
for	O
state–action	O
pair	O
(	O
s	O
,	O
a	O
)	O
.	O
direct	O
reinforcement	O
learning	O
,	O
model-learning	O
,	O
and	O
planning	O
are	O
implemented	O
by	O
steps	O
(	O
d	O
)	O
,	O
(	O
e	O
)	O
,	O
and	O
(	O
f	O
)	O
,	O
respectively	O
.	O
if	O
(	O
e	O
)	O
and	O
(	O
f	O
)	O
were	O
omitted	O
,	O
the	O
remaining	O
algorithm	O
would	O
be	O
one-step	O
tabular	O
q-learning	O
.	O
tabular	O
dyna-q	O
initialize	O
q	O
(	O
s	O
,	O
a	O
)	O
and	O
m	O
odel	O
(	O
s	O
,	O
a	O
)	O
for	O
all	O
s	O
∈	O
s	O
and	O
a	O
∈	O
a	O
(	O
s	O
)	O
loop	O
forever	O
:	O
(	O
a	O
)	O
s	O
←	O
current	O
(	O
nonterminal	O
)	O
state	B
(	O
b	O
)	O
a	O
←	O
ε-greedy	O
(	O
s	O
,	O
q	O
)	O
(	O
c	O
)	O
take	O
action	B
a	O
;	O
observe	O
resultant	O
reward	O
,	O
r	O
,	O
and	O
state	O
,	O
s	O
(	O
cid:48	O
)	O
(	O
d	O
)	O
q	O
(	O
s	O
,	O
a	O
)	O
←	O
q	O
(	O
s	O
,	O
a	O
)	O
+	O
α	O
(	O
cid:2	O
)	O
r	O
+	O
γ	O
maxa	O
q	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
)	O
−	O
q	O
(	O
s	O
,	O
a	O
)	O
(	O
cid:3	O
)	O
(	O
e	O
)	O
m	O
odel	O
(	O
s	O
,	O
a	O
)	O
←	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
(	O
assuming	O
deterministic	O
environment	B
)	O
(	O
f	O
)	O
loop	O
repeat	O
n	O
times	O
:	O
s	O
←	O
random	O
previously	O
observed	O
state	B
a	O
←	O
random	O
action	O
previously	O
taken	O
in	O
s	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
←	O
m	O
odel	O
(	O
s	O
,	O
a	O
)	O
q	O
(	O
s	O
,	O
a	O
)	O
←	O
q	O
(	O
s	O
,	O
a	O
)	O
+	O
α	O
(	O
cid:2	O
)	O
r	O
+	O
γ	O
maxa	O
q	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
)	O
−	O
q	O
(	O
s	O
,	O
a	O
)	O
(	O
cid:3	O
)	O
example	O
8.1	O
:	O
dyna	O
maze	O
consider	O
the	O
simple	O
maze	O
shown	O
inset	O
in	O
figure	O
8.2.	O
in	O
each	O
of	O
the	O
47	O
states	O
there	O
are	O
four	O
actions	O
,	O
up	O
,	O
down	O
,	O
right	O
,	O
and	O
left	O
,	O
which	O
take	O
the	O
agent	O
deterministically	O
to	O
the	O
corresponding	O
neighboring	O
states	O
,	O
except	O
when	O
movement	O
is	O
blocked	O
by	O
an	O
obstacle	O
or	O
the	O
edge	O
of	O
the	O
maze	O
,	O
in	O
which	O
case	O
the	O
agent	O
remains	O
where	O
it	O
is	O
.	O
reward	O
is	O
zero	O
on	O
all	O
transitions	O
,	O
except	O
those	O
into	O
the	O
goal	B
state	O
,	O
on	O
which	O
it	O
is	O
+1	O
.	O
after	O
reaching	O
the	O
goal	B
state	O
(	O
g	O
)	O
,	O
the	O
agent	O
returns	O
to	O
the	O
start	O
state	B
(	O
s	O
)	O
to	O
begin	O
a	O
new	O
episode	O
.	O
this	O
is	O
a	O
discounted	O
,	O
episodic	O
task	O
with	O
γ	O
=	O
0.95.	O
the	O
main	O
part	O
of	O
figure	O
8.2	O
shows	O
average	O
learning	O
curves	O
from	O
an	O
experiment	O
in	O
which	O
dyna-q	O
agents	O
were	O
applied	O
to	O
the	O
maze	O
task	O
.	O
the	O
initial	O
action	B
values	O
were	O
zero	O
,	O
the	O
step-size	B
parameter	I
was	O
α	O
=	O
0.1	O
,	O
and	O
the	O
exploration	O
parameter	O
was	O
ε	O
=	O
0.1.	O
when	O
selecting	O
greedily	O
among	O
actions	O
,	O
ties	O
were	O
broken	O
randomly	O
.	O
the	O
agents	O
varied	O
in	O
the	O
number	O
of	O
planning	O
steps	O
,	O
n	O
,	O
they	O
performed	O
per	O
real	O
step	O
.	O
for	O
each	O
n	O
,	O
the	O
curves	O
show	O
the	O
number	O
of	O
steps	O
taken	O
by	O
the	O
agent	O
to	O
reach	O
the	O
goal	B
in	O
each	O
episode	O
,	O
averaged	O
over	O
30	O
repetitions	O
of	O
the	O
experiment	O
.	O
in	O
each	O
repetition	O
,	O
the	O
initial	O
seed	O
for	O
the	O
random	O
number	O
generator	O
was	O
held	O
constant	O
across	O
algorithms	O
.	O
because	O
of	O
this	O
,	O
the	O
ﬁrst	O
episode	O
was	O
exactly	O
the	O
same	O
(	O
about	O
1700	O
steps	O
)	O
for	O
all	O
values	O
of	O
n	O
,	O
and	O
its	O
data	O
are	O
not	O
shown	O
in	O
the	O
ﬁgure	O
.	O
after	O
the	O
ﬁrst	O
episode	O
,	O
performance	O
improved	O
for	O
all	O
values	O
of	O
n	O
,	O
but	O
much	O
more	O
rapidly	O
for	O
larger	O
values	O
.	O
recall	O
that	O
the	O
n	O
=	O
0	O
agent	O
is	O
a	O
nonplanning	O
agent	O
,	O
using	O
only	O
direct	O
reinforcement	O
learning	O
(	O
one-step	O
tabular	O
q-learning	O
)	O
.	O
this	O
was	O
by	O
far	O
the	O
slowest	O
agent	O
on	O
this	O
problem	O
,	O
despite	O
the	O
fact	O
that	O
the	O
parameter	O
values	O
(	O
α	O
and	O
ε	O
)	O
were	O
optimized	O
for	O
it	O
.	O
the	O
nonplanning	O
agent	O
took	O
about	O
25	O
episodes	B
to	O
reach	O
(	O
ε-	O
)	O
optimal	O
performance	O
,	O
whereas	O
the	O
n	O
=	O
5	O
agent	O
took	O
about	O
ﬁve	O
episodes	B
,	O
and	O
the	O
n	O
=	O
50	O
agent	O
took	O
only	O
three	O
episodes	B
.	O
8.2.	O
dyna	O
:	O
integrated	O
planning	B
,	O
acting	O
,	O
and	O
learning	O
165	O
figure	O
8.2	O
:	O
a	O
simple	O
maze	O
(	O
inset	O
)	O
and	O
the	O
average	O
learning	O
curves	O
for	O
dyna-q	O
agents	O
varying	O
in	O
their	O
number	O
of	O
planning	O
steps	O
(	O
n	O
)	O
per	O
real	O
step	O
.	O
the	O
task	O
is	O
to	O
travel	O
from	O
s	O
to	O
g	O
as	O
quickly	O
as	O
possible	O
.	O
figure	O
8.3	O
shows	O
why	O
the	O
planning	B
agents	O
found	O
the	O
solution	O
so	O
much	O
faster	O
than	O
the	O
nonplanning	O
agent	O
.	O
shown	O
are	O
the	O
policies	O
found	O
by	O
the	O
n	O
=	O
0	O
and	O
n	O
=	O
50	O
agents	O
halfway	O
through	O
the	O
second	O
episode	O
.	O
without	O
planning	B
(	O
n	O
=	O
0	O
)	O
,	O
each	O
episode	O
adds	O
only	O
one	O
additional	O
step	O
to	O
the	O
policy	B
,	O
and	O
so	O
only	O
one	O
step	O
(	O
the	O
last	O
)	O
has	O
been	O
learned	O
so	O
far	O
.	O
with	O
planning	O
,	O
again	O
only	O
one	O
step	O
is	O
learned	O
during	O
the	O
ﬁrst	O
episode	O
,	O
but	O
here	O
during	O
the	O
second	O
episode	O
an	O
extensive	O
policy	B
has	O
been	O
developed	O
that	O
by	O
the	O
end	O
of	O
the	O
episode	O
will	O
reach	O
almost	O
back	O
to	O
the	O
start	O
state	B
.	O
this	O
policy	B
is	O
built	O
by	O
the	O
planning	B
process	O
while	O
the	O
agent	O
is	O
still	O
wandering	O
near	O
the	O
start	O
state	B
.	O
by	O
the	O
end	O
of	O
the	O
third	O
episode	O
a	O
complete	O
optimal	O
policy	O
will	O
have	O
been	O
found	O
and	O
perfect	O
performance	O
attained	O
.	O
figure	O
8.3	O
:	O
policies	O
found	O
by	O
planning	B
and	O
nonplanning	O
dyna-q	O
agents	O
halfway	O
through	O
the	O
second	O
episode	O
.	O
the	O
arrows	O
indicate	O
the	O
greedy	O
action	O
in	O
each	O
state	B
;	O
if	O
no	O
arrow	O
is	O
shown	O
for	O
a	O
state	O
,	O
then	O
all	O
of	O
its	O
action	B
values	O
were	O
equal	O
.	O
the	O
black	O
square	O
indicates	O
the	O
location	O
of	O
the	O
agent	O
.	O
2800600400200142010304050	O
0planningsteps	O
(	O
direct	O
rl	O
only	O
)	O
episodesstepsperepisode5	O
planning	B
steps	O
50planningstepssgactionssgsgwithout	O
planning	B
(	O
=0	O
)	O
with	O
planning	O
(	O
=50	O
)	O
nn	O
166	O
chapter	O
8	O
:	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
in	O
dyna-q	O
,	O
learning	O
and	O
planning	B
are	O
accomplished	O
by	O
exactly	O
the	O
same	O
algorithm	O
,	O
op-	O
erating	O
on	O
real	O
experience	O
for	O
learning	O
and	O
on	O
simulated	O
experience	O
for	O
planning	B
.	O
because	O
planning	B
proceeds	O
incrementally	O
,	O
it	O
is	O
trivial	O
to	O
intermix	O
planning	B
and	O
acting	O
.	O
both	O
pro-	O
ceed	O
as	O
fast	O
as	O
they	O
can	O
.	O
the	O
agent	O
is	O
always	O
reactive	O
and	O
always	O
deliberative	O
,	O
responding	O
instantly	O
to	O
the	O
latest	O
sensory	O
information	O
and	O
yet	O
always	O
planning	B
in	O
the	O
background	O
.	O
also	O
ongoing	O
in	O
the	O
background	O
is	O
the	O
model-learning	O
process	O
.	O
as	O
new	O
information	O
is	O
gained	O
,	O
the	O
model	O
is	O
updated	O
to	O
better	O
match	O
reality	O
.	O
as	O
the	O
model	O
changes	O
,	O
the	O
ongoing	O
planning	B
process	O
will	O
gradually	O
compute	O
a	O
diﬀerent	O
way	O
of	O
behaving	O
to	O
match	O
the	O
new	O
model	O
.	O
exercise	O
8.1	O
the	O
nonplanning	O
method	O
looks	O
particularly	O
poor	O
in	O
figure	O
8.3	O
because	O
it	O
is	O
a	O
one-step	O
method	O
;	O
a	O
method	O
using	O
multi-step	O
bootstrapping	B
would	O
do	O
better	O
.	O
do	O
you	O
think	O
one	O
of	O
the	O
multi-step	O
bootstrapping	B
methods	O
from	O
chapter	O
7	O
could	O
do	O
as	O
well	O
as	O
(	O
cid:3	O
)	O
the	O
dyna	O
method	O
?	O
explain	O
why	O
or	O
why	O
not	O
.	O
8.3	O
when	O
the	O
model	O
is	O
wrong	O
in	O
the	O
maze	O
example	O
presented	O
in	O
the	O
previous	O
section	O
,	O
the	O
changes	O
in	O
the	O
model	O
were	O
relatively	O
modest	O
.	O
the	O
model	O
started	O
out	O
empty	O
,	O
and	O
was	O
then	O
ﬁlled	O
only	O
with	O
exactly	O
correct	O
information	O
.	O
in	O
general	O
,	O
we	O
can	O
not	O
expect	O
to	O
be	O
so	O
fortunate	O
.	O
models	O
may	O
be	O
incorrect	O
because	O
the	O
environment	B
is	O
stochastic	O
and	O
only	O
a	O
limited	O
number	O
of	O
samples	O
have	O
been	O
observed	O
,	O
or	O
because	O
the	O
model	O
was	O
learned	O
using	O
function	B
approximation	I
that	O
has	O
generalized	O
imperfectly	O
,	O
or	O
simply	O
because	O
the	O
environment	B
has	O
changed	O
and	O
its	O
new	O
behavior	O
has	O
not	O
yet	O
been	O
observed	O
.	O
when	O
the	O
model	O
is	O
incorrect	O
,	O
the	O
planning	B
process	O
is	O
likely	O
to	O
compute	O
a	O
suboptimal	O
policy	B
.	O
in	O
some	O
cases	O
,	O
the	O
suboptimal	O
policy	B
computed	O
by	O
planning	B
quickly	O
leads	O
to	O
the	O
dis-	O
covery	O
and	O
correction	O
of	O
the	O
modeling	O
error	O
.	O
this	O
tends	O
to	O
happen	O
when	O
the	O
model	O
is	O
optimistic	O
in	O
the	O
sense	O
of	O
predicting	O
greater	O
reward	O
or	O
better	O
state	B
transitions	O
than	O
are	O
actually	O
possible	O
.	O
the	O
planned	O
policy	B
attempts	O
to	O
exploit	O
these	O
opportunities	O
and	O
in	O
doing	O
so	O
discovers	O
that	O
they	O
do	O
not	O
exist	O
.	O
example	O
8.2	O
:	O
blocking	O
maze	O
a	O
maze	O
example	O
illustrating	O
this	O
relatively	O
minor	O
kind	O
of	O
modeling	O
error	O
and	O
recovery	O
from	O
it	O
is	O
shown	O
in	O
figure	O
8.4.	O
initially	O
,	O
there	O
is	O
a	O
short	O
path	O
from	O
start	O
to	O
goal	B
,	O
to	O
the	O
right	O
of	O
the	O
barrier	O
,	O
as	O
shown	O
in	O
the	O
upper	O
left	O
of	O
the	O
ﬁgure	O
.	O
after	O
1000	O
time	O
steps	O
,	O
the	O
short	O
path	O
is	O
“	O
blocked	O
,	O
”	O
and	O
a	O
longer	O
path	O
is	O
opened	O
up	O
along	O
the	O
left-hand	O
side	O
of	O
the	O
barrier	O
,	O
as	O
shown	O
in	O
upper	O
right	O
of	O
the	O
ﬁgure	O
.	O
the	O
graph	O
shows	O
average	O
cumulative	O
reward	O
for	O
a	O
dyna-q	O
agent	O
and	O
an	O
enhanced	O
dyna-q+	O
agent	O
to	O
be	O
described	O
shortly	O
.	O
the	O
ﬁrst	O
part	O
of	O
the	O
graph	O
shows	O
that	O
both	O
dyna	O
agents	O
found	O
the	O
short	O
path	O
within	O
1000	O
steps	O
.	O
when	O
the	O
environment	B
changed	O
,	O
the	O
graphs	O
become	O
ﬂat	O
,	O
indicating	O
a	O
period	O
during	O
which	O
the	O
agents	O
obtained	O
no	O
reward	O
because	O
they	O
were	O
wandering	O
around	O
behind	O
the	O
barrier	O
.	O
after	O
a	O
while	O
,	O
however	O
,	O
they	O
were	O
able	O
to	O
ﬁnd	O
the	O
new	O
opening	O
and	O
the	O
new	O
optimal	O
behavior	O
.	O
greater	O
diﬃculties	O
arise	O
when	O
the	O
environment	B
changes	O
to	O
become	O
better	O
than	O
it	O
was	O
before	O
,	O
and	O
yet	O
the	O
formerly	O
correct	O
policy	B
does	O
not	O
reveal	O
the	O
improvement	O
.	O
in	O
these	O
cases	O
the	O
modeling	O
error	O
may	O
not	O
be	O
detected	O
for	O
a	O
long	O
time	O
,	O
if	O
ever	O
.	O
8.3.	O
when	O
the	O
model	O
is	O
wrong	O
167	O
figure	O
8.4	O
:	O
average	O
performance	O
of	O
dyna	O
agents	O
on	O
a	O
blocking	B
task	O
.	O
the	O
left	O
environment	B
was	O
used	O
for	O
the	O
ﬁrst	O
1000	O
steps	O
,	O
the	O
right	O
environment	B
for	O
the	O
rest	O
.	O
dyna-q+	O
is	O
dyna-q	O
with	O
an	O
exploration	O
bonus	O
that	O
encourages	O
exploration	O
.	O
example	O
8.3	O
:	O
shortcut	O
maze	O
the	O
problem	O
caused	O
by	O
this	O
kind	O
of	O
environmental	O
change	O
is	O
illus-	O
trated	O
by	O
the	O
maze	O
example	O
shown	O
in	O
figure	O
8.5.	O
initially	O
,	O
the	O
optimal	O
path	O
is	O
to	O
go	O
around	O
the	O
left	O
side	O
of	O
the	O
barrier	O
(	O
upper	O
left	O
)	O
.	O
after	O
3000	O
steps	O
,	O
however	O
,	O
a	O
shorter	O
path	O
is	O
opened	O
up	O
along	O
the	O
right	O
side	O
,	O
without	O
disturbing	O
the	O
longer	O
path	O
(	O
upper	O
right	O
)	O
.	O
the	O
graph	O
shows	O
that	O
the	O
regular	O
dyna-q	O
agent	O
never	O
switched	O
to	O
the	O
shortcut	O
.	O
in	O
fact	O
,	O
it	O
never	O
realized	O
that	O
it	O
existed	O
.	O
its	O
model	O
said	O
that	O
there	O
was	O
no	O
short-	O
cut	O
,	O
so	O
the	O
more	O
it	O
planned	O
,	O
the	O
less	O
likely	O
it	O
was	O
to	O
step	O
to	O
the	O
right	O
and	O
discover	O
it	O
.	O
even	O
with	O
an	O
ε-greedy	O
policy	O
,	O
it	O
is	O
very	O
unlikely	O
that	O
an	O
agent	O
will	O
take	O
so	O
many	O
exploratory	O
actions	O
as	O
to	O
discover	O
the	O
shortcut	O
.	O
figure	O
8.5	O
:	O
average	O
performance	O
of	O
dyna	O
agents	O
on	O
a	O
shortcut	O
task	O
.	O
the	O
left	O
environment	B
was	O
used	O
for	O
the	O
ﬁrst	O
3000	O
steps	O
,	O
the	O
right	O
environment	B
for	O
the	O
rest	O
.	O
the	O
general	O
problem	O
here	O
is	O
another	O
version	O
of	O
the	O
conﬂict	O
between	O
exploration	O
and	O
exploitation	O
.	O
in	O
a	O
planning	B
context	O
,	O
exploration	O
means	O
trying	O
actions	O
that	O
improve	O
the	O
model	O
,	O
whereas	O
exploitation	O
means	O
behaving	O
in	O
the	O
optimal	O
way	O
given	O
the	O
current	O
model	O
.	O
cumulativereward0100020003000time	O
steps1500dyna-q+sggsdyna-qcumulativerewardsggs030006000time	O
steps4000dyna-q+dyna-q	O
168	O
chapter	O
8	O
:	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
we	O
want	O
the	O
agent	O
to	O
explore	O
to	O
ﬁnd	O
changes	O
in	O
the	O
environment	O
,	O
but	O
not	O
so	O
much	O
that	O
performance	O
is	O
greatly	O
degraded	O
.	O
as	O
in	O
the	O
earlier	O
exploration/exploitation	O
conﬂict	O
,	O
there	O
probably	O
is	O
no	O
solution	O
that	O
is	O
both	O
perfect	O
and	O
practical	O
,	O
but	O
simple	O
heuristics	O
are	O
often	O
eﬀective	O
.	O
the	O
dyna-q+	O
agent	O
that	O
did	O
solve	O
the	O
shortcut	O
maze	O
uses	O
one	O
such	O
heuristic	O
.	O
this	O
agent	O
keeps	O
track	O
for	O
each	O
state–action	O
pair	O
of	O
how	O
many	O
time	O
steps	O
have	O
elapsed	O
since	O
the	O
pair	O
was	O
last	O
tried	O
in	O
a	O
real	O
interaction	O
with	O
the	O
environment	B
.	O
the	O
more	O
time	O
that	O
has	O
elapsed	O
,	O
the	O
greater	O
(	O
we	O
might	O
presume	O
)	O
the	O
chance	O
that	O
the	O
dynamics	O
of	O
this	O
pair	O
has	O
changed	O
and	O
that	O
the	O
model	O
of	O
it	O
is	O
incorrect	O
.	O
to	O
encourage	O
behavior	O
that	O
tests	O
long-	O
untried	O
actions	O
,	O
a	O
special	O
“	O
bonus	O
reward	O
”	O
is	O
given	O
on	O
simulated	O
experiences	O
involving	O
these	O
actions	O
.	O
in	O
particular	O
,	O
if	O
the	O
modeled	O
reward	O
for	O
a	O
transition	O
is	O
r	O
,	O
and	O
the	O
transition	O
has	O
not	O
been	O
tried	O
in	O
τ	O
time	O
steps	O
,	O
then	O
planning	B
updates	O
are	O
done	O
as	O
if	O
that	O
transition	O
produced	O
a	O
reward	O
of	O
r	O
+	O
κ√τ	O
,	O
for	O
some	O
small	O
κ.	O
this	O
encourages	O
the	O
agent	O
to	O
keep	O
testing	O
all	O
accessible	O
state	B
transitions	O
and	O
even	O
to	O
ﬁnd	O
long	O
sequences	O
of	O
actions	O
in	O
order	O
to	O
carry	O
out	O
such	O
tests.1	O
of	O
course	O
all	O
this	O
testing	O
has	O
its	O
cost	O
,	O
but	O
in	O
many	O
cases	O
,	O
as	O
in	O
the	O
shortcut	O
maze	O
,	O
this	O
kind	O
of	O
computational	O
curiosity	B
is	O
well	O
worth	O
the	O
extra	O
exploration	O
.	O
exercise	O
8.2	O
why	O
did	O
the	O
dyna	O
agent	O
with	O
exploration	O
bonus	O
,	O
dyna-q+	O
,	O
perform	O
better	O
in	O
the	O
ﬁrst	O
phase	O
as	O
well	O
as	O
in	O
the	O
second	O
phase	O
of	O
the	O
blocking	B
and	O
shortcut	O
experiments	O
?	O
(	O
cid:3	O
)	O
exercise	O
8.3	O
careful	O
inspection	O
of	O
figure	O
8.5	O
reveals	O
that	O
the	O
diﬀerence	O
between	O
dyna-	O
q+	O
and	O
dyna-q	O
narrowed	O
slightly	O
over	O
the	O
ﬁrst	O
part	O
of	O
the	O
experiment	O
.	O
what	O
is	O
the	O
(	O
cid:3	O
)	O
reason	O
for	O
this	O
?	O
exercise	O
8.4	O
(	O
programming	O
)	O
the	O
exploration	O
bonus	O
described	O
above	O
actually	O
changes	O
the	O
estimated	O
values	O
of	O
states	O
and	O
actions	O
.	O
is	O
this	O
necessary	O
?	O
suppose	O
the	O
bonus	O
κ√τ	O
was	O
used	O
not	O
in	O
updates	O
,	O
but	O
solely	O
in	O
action	O
selection	O
.	O
that	O
is	O
,	O
suppose	O
the	O
action	B
selected	O
was	O
always	O
that	O
for	O
which	O
q	O
(	O
st	O
,	O
a	O
)	O
+	O
κ	O
(	O
cid:112	O
)	O
τ	O
(	O
st	O
,	O
a	O
)	O
was	O
maximal	O
.	O
carry	O
out	O
a	O
gridworld	O
experiment	O
that	O
tests	O
and	O
illustrates	O
the	O
strengths	O
and	O
weaknesses	O
of	O
this	O
(	O
cid:3	O
)	O
alternate	O
approach	O
.	O
exercise	O
8.5	O
how	O
might	O
the	O
tabular	O
dyna-q	O
algorithm	O
shown	O
on	O
page	O
164	O
be	O
modiﬁed	O
to	O
handle	O
stochastic	O
environments	O
?	O
how	O
might	O
this	O
modiﬁcation	O
perform	O
poorly	O
on	O
changing	O
environments	O
such	O
as	O
considered	O
in	O
this	O
section	O
?	O
how	O
could	O
the	O
algorithm	O
be	O
(	O
cid:3	O
)	O
modiﬁed	O
to	O
handle	O
stochastic	O
environments	O
and	O
changing	O
environments	O
?	O
8.4	O
prioritized	B
sweeping	I
in	O
the	O
dyna	O
agents	O
presented	O
in	O
the	O
preceding	O
sections	O
,	O
simulated	O
transitions	O
are	O
started	O
in	O
state–action	O
pairs	O
selected	O
uniformly	O
at	O
random	O
from	O
all	O
previously	O
experienced	O
pairs	O
.	O
but	O
a	O
uniform	O
selection	O
is	O
usually	O
not	O
the	O
best	O
;	O
planning	B
can	O
be	O
much	O
more	O
eﬃcient	O
if	O
simulated	O
transitions	O
and	O
updates	O
are	O
focused	O
on	O
particular	O
state–action	O
pairs	O
.	O
for	O
1the	O
dyna-q+	O
agent	O
was	O
changed	O
in	O
two	O
other	O
ways	O
as	O
well	O
.	O
first	O
,	O
actions	O
that	O
had	O
never	O
been	O
tried	O
before	O
from	O
a	O
state	B
were	O
allowed	O
to	O
be	O
considered	O
in	O
the	O
planning	O
step	O
(	O
f	O
)	O
of	O
the	O
tabular	O
dyna-q	O
algorithm	O
in	O
the	O
box	O
above	O
.	O
second	O
,	O
the	O
initial	O
model	O
for	O
such	O
actions	O
was	O
that	O
they	O
would	O
lead	O
back	O
to	O
the	O
same	O
state	B
with	O
a	O
reward	O
of	O
zero	O
.	O
8.4.	O
prioritized	B
sweeping	I
169	O
example	O
,	O
consider	O
what	O
happens	O
during	O
the	O
second	O
episode	O
of	O
the	O
ﬁrst	O
maze	O
task	O
(	O
fig-	O
ure	O
8.3	O
)	O
.	O
at	O
the	O
beginning	O
of	O
the	O
second	O
episode	O
,	O
only	O
the	O
state–action	O
pair	O
leading	O
directly	O
into	O
the	O
goal	B
has	O
a	O
positive	O
value	B
;	O
the	O
values	O
of	O
all	O
other	O
pairs	O
are	O
still	O
zero	O
.	O
this	O
means	O
that	O
it	O
is	O
pointless	O
to	O
perform	O
updates	O
along	O
almost	O
all	O
transitions	O
,	O
because	O
they	O
take	O
the	O
agent	O
from	O
one	O
zero-valued	O
state	B
to	O
another	O
,	O
and	O
thus	O
the	O
updates	O
would	O
have	O
no	O
eﬀect	O
.	O
only	O
an	O
update	O
along	O
a	O
transition	O
into	O
the	O
state	B
just	O
prior	O
to	O
the	O
goal	B
,	O
or	O
from	O
it	O
,	O
will	O
change	O
any	O
values	O
.	O
if	O
simulated	O
transitions	O
are	O
generated	O
uniformly	O
,	O
then	O
many	O
wasteful	O
updates	O
will	O
be	O
made	O
before	O
stumbling	O
onto	O
one	O
of	O
these	O
useful	O
ones	O
.	O
as	O
planning	O
progresses	O
,	O
the	O
region	O
of	O
useful	O
updates	O
grows	O
,	O
but	O
planning	B
is	O
still	O
far	O
less	O
eﬃcient	O
than	O
it	O
would	O
be	O
if	O
focused	O
where	O
it	O
would	O
do	O
the	O
most	O
good	O
.	O
in	O
the	O
much	O
larger	O
problems	O
that	O
are	O
our	O
real	O
objective	O
,	O
the	O
number	O
of	O
states	O
is	O
so	O
large	O
that	O
an	O
unfocused	O
search	O
would	O
be	O
extremely	O
ineﬃcient	O
.	O
this	O
example	O
suggests	O
that	O
search	O
might	O
be	O
usefully	O
focused	O
by	O
working	O
backward	O
from	O
goal	B
states	O
.	O
of	O
course	O
,	O
we	O
do	O
not	O
really	O
want	O
to	O
use	O
any	O
methods	O
speciﬁc	O
to	O
the	O
idea	O
of	O
“	O
goal	B
state.	O
”	O
we	O
want	O
methods	O
that	O
work	O
for	O
general	O
reward	O
functions	O
.	O
goal	B
states	O
are	O
just	O
a	O
special	O
case	O
,	O
convenient	O
for	O
stimulating	O
intuition	O
.	O
in	O
general	O
,	O
we	O
want	O
to	O
work	O
back	O
not	O
just	O
from	O
goal	B
states	O
but	O
from	O
any	O
state	B
whose	O
value	B
has	O
changed	O
.	O
suppose	O
that	O
the	O
values	O
are	O
initially	O
correct	O
given	O
the	O
model	O
,	O
as	O
they	O
were	O
in	O
the	O
maze	O
example	O
prior	O
to	O
discovering	O
the	O
goal	B
.	O
suppose	O
now	O
that	O
the	O
agent	O
discovers	O
a	O
change	O
in	O
the	O
environment	O
and	O
changes	O
its	O
estimated	O
value	B
of	O
one	O
state	B
,	O
either	O
up	O
or	O
down	O
.	O
typically	O
,	O
this	O
will	O
imply	O
that	O
the	O
values	O
of	O
many	O
other	O
states	O
should	O
also	O
be	O
changed	O
,	O
but	O
the	O
only	O
useful	O
one-step	O
updates	O
are	O
those	O
of	O
actions	O
that	O
lead	O
directly	O
into	O
the	O
one	O
state	B
whose	O
value	B
has	O
been	O
changed	O
.	O
if	O
the	O
values	O
of	O
these	O
actions	O
are	O
updated	O
,	O
then	O
the	O
values	O
of	O
the	O
predecessor	O
states	O
may	O
change	O
in	O
turn	O
.	O
if	O
so	O
,	O
then	O
actions	O
leading	O
into	O
them	O
need	O
to	O
be	O
updated	O
,	O
and	O
then	O
their	O
predecessor	O
states	O
may	O
have	O
changed	O
.	O
in	O
this	O
way	O
one	O
can	O
work	O
backward	O
from	O
arbitrary	O
states	O
that	O
have	O
changed	O
in	O
value	O
,	O
either	O
performing	O
useful	O
updates	O
or	O
terminating	O
the	O
propagation	O
.	O
this	O
general	O
idea	O
might	O
be	O
termed	O
backward	O
focusing	O
of	O
planning	O
computations	O
.	O
as	O
the	O
frontier	O
of	O
useful	O
updates	O
propagates	O
backward	O
,	O
it	O
often	O
grows	O
rapidly	O
,	O
produc-	O
ing	B
many	O
state–action	O
pairs	O
that	O
could	O
usefully	O
be	O
updated	O
.	O
but	O
not	O
all	O
of	O
these	O
will	O
be	O
equally	O
useful	O
.	O
the	O
values	O
of	O
some	O
states	O
may	O
have	O
changed	O
a	O
lot	O
,	O
whereas	O
others	O
may	O
have	O
changed	O
little	O
.	O
the	O
predecessor	O
pairs	O
of	O
those	O
that	O
have	O
changed	O
a	O
lot	O
are	O
more	O
likely	O
to	O
also	O
change	O
a	O
lot	O
.	O
in	O
a	O
stochastic	O
environment	O
,	O
variations	O
in	O
estimated	O
transition	B
probabilities	I
also	O
contribute	O
to	O
variations	O
in	O
the	O
sizes	O
of	O
changes	O
and	O
in	O
the	O
urgency	O
with	O
which	O
pairs	O
need	O
to	O
be	O
updated	O
.	O
it	O
is	O
natural	O
to	O
prioritize	O
the	O
updates	O
according	O
to	O
a	O
measure	O
of	O
their	O
urgency	O
,	O
and	O
perform	O
them	O
in	O
order	O
of	O
priority	O
.	O
this	O
is	O
the	O
idea	O
behind	O
prioritized	B
sweeping	I
.	O
a	O
queue	O
is	O
maintained	O
of	O
every	O
state–action	O
pair	O
whose	O
estimated	O
value	B
would	O
change	O
nontrivially	O
if	O
updated	O
,	O
prioritized	O
by	O
the	O
size	O
of	O
the	O
change	O
.	O
when	O
the	O
top	O
pair	O
in	O
the	O
queue	O
is	O
updated	O
,	O
the	O
eﬀect	O
on	O
each	O
of	O
its	O
pre-	O
decessor	O
pairs	O
is	O
computed	O
.	O
if	O
the	O
eﬀect	O
is	O
greater	O
than	O
some	O
small	O
threshold	O
,	O
then	O
the	O
pair	O
is	O
inserted	O
in	O
the	O
queue	O
with	O
the	O
new	O
priority	O
(	O
if	O
there	O
is	O
a	O
previous	O
entry	O
of	O
the	O
pair	O
in	O
the	O
queue	O
,	O
then	O
insertion	O
results	O
in	O
only	O
the	O
higher	O
priority	O
entry	O
remaining	O
in	O
the	O
queue	O
)	O
.	O
in	O
this	O
way	O
the	O
eﬀects	O
of	O
changes	O
are	O
eﬃciently	O
propagated	O
backward	O
until	O
quiescence	O
.	O
the	O
full	O
algorithm	O
for	O
the	O
case	O
of	O
deterministic	O
environments	O
is	O
given	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
.	O
170	O
chapter	O
8	O
:	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
prioritized	B
sweeping	I
for	O
a	O
deterministic	O
environment	B
initialize	O
q	O
(	O
s	O
,	O
a	O
)	O
,	O
m	O
odel	O
(	O
s	O
,	O
a	O
)	O
,	O
for	O
all	O
s	O
,	O
a	O
,	O
and	O
p	O
queue	O
to	O
empty	O
loop	O
forever	O
:	O
(	O
a	O
)	O
s	O
←	O
current	O
(	O
nonterminal	O
)	O
state	B
(	O
b	O
)	O
a	O
←	O
policy	B
(	O
s	O
,	O
q	O
)	O
(	O
c	O
)	O
take	O
action	B
a	O
;	O
observe	O
resultant	O
reward	O
,	O
r	O
,	O
and	O
state	O
,	O
s	O
(	O
cid:48	O
)	O
(	O
d	O
)	O
m	O
odel	O
(	O
s	O
,	O
a	O
)	O
←	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
(	O
e	O
)	O
p	O
←	O
|r	O
+	O
γ	O
maxa	O
q	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
)	O
−	O
q	O
(	O
s	O
,	O
a	O
)	O
|	O
.	O
(	O
f	O
)	O
if	O
p	O
>	O
θ	O
,	O
then	O
insert	O
s	O
,	O
a	O
into	O
p	O
queue	O
with	O
priority	O
p	O
(	O
g	O
)	O
loop	O
repeat	O
n	O
times	O
,	O
while	O
p	O
queue	O
is	O
not	O
empty	O
:	O
s	O
,	O
a	O
←	O
f	O
irst	O
(	O
p	O
queue	O
)	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
←	O
m	O
odel	O
(	O
s	O
,	O
a	O
)	O
q	O
(	O
s	O
,	O
a	O
)	O
←	O
q	O
(	O
s	O
,	O
a	O
)	O
+	O
α	O
(	O
cid:2	O
)	O
r	O
+	O
γ	O
maxa	O
q	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
)	O
−	O
q	O
(	O
s	O
,	O
a	O
)	O
(	O
cid:3	O
)	O
loop	O
for	O
all	O
¯s	O
,	O
¯a	O
predicted	O
to	O
lead	O
to	O
s	O
:	O
¯r	O
←	O
predicted	O
reward	O
for	O
¯s	O
,	O
¯a	O
,	O
s	O
p	O
←	O
|	O
¯r	O
+	O
γ	O
maxa	O
q	O
(	O
s	O
,	O
a	O
)	O
−	O
q	O
(	O
¯s	O
,	O
¯a	O
)	O
|	O
.	O
if	O
p	O
>	O
θ	O
then	O
insert	O
¯s	O
,	O
¯a	O
into	O
p	O
queue	O
with	O
priority	O
p	O
example	O
8.4	O
:	O
prioritized	O
sweep-	O
ing	B
on	O
mazes	O
prioritized	B
sweeping	I
has	O
been	O
found	O
to	O
dramatically	O
increase	O
the	O
speed	O
at	O
which	O
optimal	O
solutions	O
are	O
found	O
in	O
maze	O
tasks	O
,	O
often	O
by	O
a	O
factor	O
of	O
5	O
to	O
10.	O
a	O
typical	O
example	O
is	O
shown	O
to	O
the	O
right	O
.	O
these	O
data	O
are	O
for	O
a	O
sequence	O
of	O
maze	O
tasks	O
of	O
exactly	O
the	O
same	O
struc-	O
ture	O
as	O
the	O
one	O
shown	O
in	O
figure	O
8.2	O
,	O
ex-	O
cept	O
that	O
they	O
vary	O
in	O
the	O
grid	O
resolution	O
.	O
prioritized	B
sweeping	I
maintained	O
a	O
deci-	O
sive	O
advantage	O
over	O
unprioritized	O
dyna-	O
q.	O
both	O
systems	O
made	O
at	O
most	O
n	O
=	O
5	O
updates	O
per	O
environmental	O
interaction	O
.	O
adapted	O
from	O
peng	O
and	O
williams	O
(	O
1993	O
)	O
.	O
extensions	O
of	O
prioritized	O
sweeping	O
to	O
stochastic	O
environments	O
are	O
straightforward	O
.	O
the	O
model	O
is	O
maintained	O
by	O
keeping	O
counts	O
of	O
the	O
number	O
of	O
times	O
each	O
state–action	O
pair	O
has	O
been	O
experienced	O
and	O
of	O
what	O
the	O
next	O
states	O
were	O
.	O
it	O
is	O
natural	O
then	O
to	O
update	O
each	O
pair	O
not	O
with	O
a	O
sample	O
update	O
,	O
as	O
we	O
have	O
been	O
using	O
so	O
far	O
,	O
but	O
with	O
an	O
expected	B
update	I
,	O
taking	O
into	O
account	O
all	O
possible	O
next	O
states	O
and	O
their	O
probabilities	O
of	O
occurring	O
.	O
prioritized	B
sweeping	I
is	O
just	O
one	O
way	O
of	O
distributing	O
computations	O
to	O
improve	O
planning	B
eﬃciency	O
,	O
and	O
probably	O
not	O
the	O
best	O
way	O
.	O
one	O
of	O
prioritized	O
sweeping	O
’	O
s	O
limitations	O
is	O
that	O
it	O
uses	O
expected	O
updates	O
,	O
which	O
in	O
stochastic	O
environments	O
may	O
waste	O
lots	O
of	O
computation	O
on	O
low-probability	O
transitions	O
.	O
as	O
we	O
show	O
in	O
the	O
following	O
section	O
,	O
sample	O
updates	O
can	O
in	O
many	O
cases	O
get	O
closer	O
to	O
the	O
true	O
value	O
function	O
with	O
less	O
computation	O
despite	O
the	O
variance	O
introduced	O
by	O
sampling	O
.	O
sample	O
updates	O
can	O
win	O
because	O
they	O
backupsuntiloptimalsolution1010310410510610710204794186376752150430086016gridworld	O
size	O
(	O
#	O
states	O
)	O
dyna-qprioritizedsweepingupdatesupdatesuntiloptimalsolution	O
8.5.	O
expected	O
vs.	O
sample	O
updates	O
171	O
example	O
8.5	O
prioritized	B
sweeping	I
for	O
rod	O
maneuvering	O
the	O
objective	O
in	O
this	O
task	O
is	O
to	O
ma-	O
neuver	O
a	O
rod	O
around	O
some	O
awk-	O
wardly	O
placed	O
obstacles	O
within	O
a	O
limited	O
rectangular	O
work	O
space	O
to	O
a	O
goal	B
position	O
in	O
the	O
fewest	O
num-	O
ber	O
of	O
steps	O
.	O
the	O
rod	O
can	O
be	O
trans-	O
lated	O
along	O
its	O
long	O
axis	O
or	O
perpen-	O
dicular	O
to	O
that	O
axis	O
,	O
or	O
it	O
can	O
be	O
ro-	O
tated	O
in	O
either	O
direction	O
around	O
its	O
center	O
.	O
the	O
distance	O
of	O
each	O
move-	O
ment	O
is	O
approximately	O
1/20	O
of	O
the	O
work	O
space	O
,	O
and	O
the	O
rotation	O
incre-	O
ment	O
is	O
10	O
degrees	O
.	O
translations	O
are	O
deterministic	O
and	O
quantized	O
to	O
one	O
of	O
20	O
×	O
20	O
positions	O
.	O
to	O
the	O
right	O
is	O
shown	O
the	O
obstacles	O
and	O
the	O
shortest	O
solution	O
from	O
start	O
to	O
goal	B
,	O
found	O
by	O
prioritized	B
sweeping	I
.	O
this	O
problem	O
is	O
deterministic	O
,	O
but	O
has	O
four	O
actions	O
and	O
14,400	O
potential	O
states	O
(	O
some	O
of	O
these	O
are	O
unreachable	O
because	O
of	O
the	O
obsta-	O
cles	O
)	O
.	O
this	O
problem	O
is	O
probably	O
too	O
large	O
to	O
be	O
solved	O
with	O
unprioritized	O
methods	O
.	O
figure	O
reprinted	O
from	O
moore	O
and	O
atkeson	O
(	O
1993	O
)	O
.	O
break	O
the	O
overall	O
backing-up	O
computation	O
into	O
smaller	O
pieces—those	O
corresponding	O
to	O
individual	O
transitions—which	O
then	O
enables	O
it	O
to	O
be	O
focused	O
more	O
narrowly	O
on	O
the	O
pieces	O
that	O
will	O
have	O
the	O
largest	O
impact	O
.	O
this	O
idea	O
was	O
taken	O
to	O
what	O
may	O
be	O
its	O
logical	O
limit	O
in	O
the	O
“	O
small	O
backups	O
”	O
introduced	O
by	O
van	O
seijen	O
and	O
sutton	O
(	O
2013	O
)	O
.	O
these	O
are	O
updates	O
along	O
a	O
single	O
transition	O
,	O
like	O
a	O
sample	O
update	O
,	O
but	O
based	O
on	O
the	O
probability	O
of	O
the	O
transition	O
without	O
sampling	O
,	O
as	O
in	O
an	O
expected	B
update	I
.	O
by	O
selecting	O
the	O
order	O
in	O
which	O
small	O
updates	O
are	O
done	O
it	O
is	O
possible	O
to	O
greatly	O
improve	O
planning	B
eﬃciency	O
beyond	O
that	O
possible	O
with	O
prioritized	O
sweeping	O
.	O
we	O
have	O
suggested	O
in	O
this	O
chapter	O
that	O
all	O
kinds	O
of	O
state-space	O
planning	B
can	O
be	O
viewed	O
as	O
sequences	O
of	O
value	O
updates	O
,	O
varying	O
only	O
in	O
the	O
type	O
of	B
update	I
,	O
expected	O
or	O
sample	O
,	O
large	O
or	O
small	O
,	O
and	O
in	O
the	O
order	O
in	O
which	O
the	O
updates	O
are	O
done	O
.	O
in	O
this	O
section	O
we	O
have	O
emphasized	O
backward	O
focusing	O
,	O
but	O
this	O
is	O
just	O
one	O
strategy	O
.	O
for	O
example	O
,	O
another	O
would	O
be	O
to	O
focus	O
on	O
states	O
according	O
to	O
how	O
easily	O
they	O
can	O
be	O
reached	O
from	O
the	O
states	O
that	O
are	O
visited	O
frequently	O
under	O
the	O
current	O
policy	B
,	O
which	O
might	O
be	O
called	O
forward	O
focusing	O
.	O
peng	O
and	O
williams	O
(	O
1993	O
)	O
and	O
barto	O
,	O
bradtke	O
and	O
singh	O
(	O
1995	O
)	O
have	O
explored	O
versions	O
of	O
forward	O
focusing	O
,	O
and	O
the	O
methods	O
introduced	O
in	O
the	O
next	O
few	O
sections	O
take	O
it	O
to	O
an	O
extreme	O
form	O
.	O
startgoal	O
172	O
chapter	O
8	O
:	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
8.5	O
expected	O
vs.	O
sample	O
updates	O
the	O
examples	O
in	O
the	O
previous	O
sections	O
give	O
some	O
idea	O
of	O
the	O
range	O
of	O
possibilities	O
for	O
combining	O
methods	O
of	O
learning	O
and	O
planning	O
.	O
in	O
the	O
rest	O
of	O
this	O
chapter	O
,	O
we	O
analyze	O
some	O
of	O
the	O
component	O
ideas	O
involved	O
,	O
starting	O
with	O
the	O
relative	O
advantages	B
of	I
expected	O
and	O
sample	O
updates	O
.	O
much	O
of	O
this	O
book	O
has	O
been	O
about	O
diﬀerent	O
kinds	O
of	O
value-function	O
updates	O
,	O
and	O
we	O
have	O
considered	O
a	O
great	O
many	O
varieties	O
.	O
focusing	O
for	O
the	O
moment	O
on	O
one-step	O
updates	O
,	O
they	O
vary	O
primarily	O
along	O
three	O
binary	O
dimensions	O
.	O
the	O
ﬁrst	O
two	O
dimensions	O
are	O
whether	O
they	O
update	O
state	B
values	O
or	O
action	B
values	O
and	O
whether	O
they	O
estimate	O
the	O
value	B
for	O
the	O
optimal	O
policy	O
or	O
for	O
an	O
arbitrary	O
given	O
policy	B
.	O
these	O
two	O
dimensions	O
give	O
rise	O
to	O
four	O
classes	O
of	O
updates	O
for	O
approximating	O
the	O
four	O
value	B
functions	O
,	O
q∗	O
,	O
v∗	O
,	O
qπ	O
,	O
and	O
vπ	O
.	O
the	O
other	O
binary	O
dimension	O
is	O
whether	O
the	O
updates	O
are	O
expected	O
updates	O
,	O
consider-	O
ing	B
all	O
possible	O
events	O
that	O
might	O
hap-	O
pen	O
,	O
or	O
sample	O
updates	O
,	O
considering	O
a	O
single	O
sample	O
of	O
what	O
might	O
hap-	O
pen	O
.	O
these	O
three	O
binary	O
dimensions	O
give	O
rise	O
to	O
eight	O
cases	O
,	O
seven	O
of	O
which	O
correspond	O
to	O
speciﬁc	O
algorithms	O
,	O
as	O
shown	O
in	O
the	O
ﬁgure	O
to	O
the	O
right	O
.	O
(	O
the	O
eighth	O
case	O
does	O
not	O
seem	O
to	O
correspond	O
to	O
any	O
useful	O
update	O
.	O
)	O
any	O
of	O
these	O
one-step	O
updates	O
can	O
be	O
used	O
in	O
plan-	O
ning	O
methods	O
.	O
the	O
dyna-q	O
agents	O
dis-	O
cussed	O
earlier	O
use	O
q∗	O
sample	O
updates	O
,	O
but	O
they	O
could	O
just	O
as	O
well	O
use	O
q∗	O
ex-	O
pected	O
updates	O
,	O
or	O
either	O
expected	O
or	O
sample	O
qπ	O
updates	O
.	O
the	O
dyna-ac	O
sys-	O
tem	O
uses	O
vπ	O
sample	O
updates	O
together	O
with	O
a	O
learning	O
policy	O
structure	O
(	O
as	O
in	O
chapter	O
13	O
)	O
.	O
for	O
stochastic	O
problems	O
,	O
prioritized	B
sweeping	I
is	O
always	O
done	O
us-	O
ing	B
one	O
of	O
the	O
expected	O
updates	O
.	O
when	O
we	O
introduced	O
one-step	O
sam-	O
ple	O
updates	O
in	O
chapter	O
6	O
,	O
we	O
presented	O
them	O
as	O
substitutes	O
for	O
expected	O
up-	O
dates	O
.	O
in	O
the	O
absence	O
of	O
a	O
distribution	O
model	O
,	O
expected	O
updates	O
are	O
not	O
pos-	O
sible	O
,	O
but	O
sample	O
updates	O
can	O
be	O
done	O
using	O
sample	O
transitions	O
from	O
the	O
envi-	O
ronment	O
or	O
a	O
sample	O
model	O
.	O
implicit	O
in	O
that	O
point	O
of	O
view	O
is	O
that	O
expected	O
up-	O
dates	O
,	O
if	O
possible	O
,	O
are	O
preferable	O
to	O
sam-	O
ple	O
updates	O
.	O
but	O
are	O
they	O
?	O
expected	O
figure	O
8.6	O
:	O
backup	O
diagrams	O
for	O
all	O
the	O
one-step	O
updates	O
considered	O
in	O
this	O
book	O
.	O
valueestimatedexpected	O
updates	O
(	O
dp	O
)	O
sample	O
updates	O
(	O
one-step	O
td	O
)	O
⇡ss0⇡rpaq⇡	O
(	O
s	O
,	O
a	O
)	O
q⇤	O
(	O
s	O
,	O
a	O
)	O
v⇡	O
(	O
s	O
)	O
v⇤	O
(	O
s	O
)	O
ss0rmaxappolicy	O
evaluationvalue	O
iterationrs0s	O
,	O
aa0⇡pq-policy	O
evaluationrs0s	O
,	O
aa0maxpq-value	O
iterationsas0rrs0s	O
,	O
aa0rs0s	O
,	O
amaxtd	O
(	O
0	O
)	O
sarsaq-learninga0	O
8.5.	O
expected	O
vs.	O
sample	O
updates	O
173	O
updates	O
certainly	O
yield	O
a	O
better	O
estimate	O
because	O
they	O
are	O
uncorrupted	O
by	O
sampling	O
error	O
,	O
but	O
they	O
also	O
require	O
more	O
computation	O
,	O
and	O
computation	O
is	O
often	O
the	O
limiting	O
resource	O
in	O
planning	O
.	O
to	O
properly	O
assess	O
the	O
relative	O
merits	O
of	O
expected	O
and	O
sample	O
updates	O
for	O
planning	O
we	O
must	O
control	B
for	O
their	O
diﬀerent	O
computational	O
requirements	O
.	O
for	O
concreteness	O
,	O
consider	O
the	O
expected	O
and	O
sample	O
updates	O
for	O
approximating	O
q∗	O
,	O
and	O
the	O
special	O
case	O
of	O
discrete	O
states	O
and	O
actions	O
,	O
a	O
table-lookup	O
representation	O
of	O
the	O
approximate	B
value	O
function	O
,	O
q	O
,	O
and	O
a	O
model	O
in	O
the	O
form	O
of	O
estimated	O
dynamics	O
,	O
ˆp	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
.	O
the	O
expected	B
update	I
for	O
a	O
state–action	O
pair	O
,	O
s	O
,	O
a	O
,	O
is	O
:	O
q	O
(	O
s	O
,	O
a	O
)	O
←	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
ˆp	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:104	O
)	O
r	O
+	O
γ	O
max	O
a	O
(	O
cid:48	O
)	O
q	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
.	O
(	O
8.1	O
)	O
the	O
corresponding	O
sample	O
update	O
for	O
s	O
,	O
a	O
,	O
given	O
a	O
sample	O
next	O
state	B
and	O
reward	O
,	O
s	O
(	O
cid:48	O
)	O
and	O
r	O
(	O
from	O
the	O
model	O
)	O
,	O
is	O
the	O
q-learning-like	O
update	O
:	O
q	O
(	O
s	O
,	O
a	O
)	O
←	O
q	O
(	O
s	O
,	O
a	O
)	O
+	O
α	O
(	O
cid:104	O
)	O
r	O
+	O
γ	O
max	O
a	O
(	O
cid:48	O
)	O
q	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
(	O
cid:48	O
)	O
)	O
−	O
q	O
(	O
s	O
,	O
a	O
)	O
(	O
cid:105	O
)	O
,	O
where	O
α	O
is	O
the	O
usual	O
positive	O
step-size	B
parameter	I
.	O
(	O
8.2	O
)	O
the	O
diﬀerence	O
between	O
these	O
expected	O
and	O
sample	O
updates	O
is	O
signiﬁcant	O
to	O
the	O
extent	O
that	O
the	O
environment	B
is	O
stochastic	O
,	O
speciﬁcally	O
,	O
to	O
the	O
extent	O
that	O
,	O
given	O
a	O
state	B
and	O
action	B
,	O
many	O
possible	O
next	O
states	O
may	O
occur	O
with	O
various	O
probabilities	O
.	O
if	O
only	O
one	O
next	O
state	B
is	O
possible	O
,	O
then	O
the	O
expected	O
and	O
sample	O
updates	O
given	O
above	O
are	O
identical	O
(	O
taking	O
α	O
=	O
1	O
)	O
.	O
if	O
there	O
are	O
many	O
possible	O
next	O
states	O
,	O
then	O
there	O
may	O
be	O
signiﬁcant	O
diﬀerences	O
.	O
in	O
favor	O
of	O
the	O
expected	B
update	I
is	O
that	O
it	O
is	O
an	O
exact	O
computation	O
,	O
resulting	O
in	O
a	O
new	O
q	O
(	O
s	O
,	O
a	O
)	O
whose	O
correctness	O
is	O
limited	O
only	O
by	O
the	O
correctness	O
of	O
the	O
q	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
(	O
cid:48	O
)	O
)	O
at	O
successor	O
states	O
.	O
the	O
sample	O
update	O
is	O
in	O
addition	O
aﬀected	O
by	O
sampling	O
error	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
sample	O
update	O
is	O
cheaper	O
computationally	O
because	O
it	O
considers	O
only	O
one	O
next	O
state	B
,	O
not	O
all	O
possible	O
next	O
states	O
.	O
in	O
practice	O
,	O
the	O
computation	O
required	O
by	O
update	O
operations	O
is	O
usually	O
dominated	O
by	O
the	O
number	O
of	O
state–action	O
pairs	O
at	O
which	O
q	O
is	O
evaluated	O
.	O
for	O
a	O
particular	O
starting	O
pair	O
,	O
s	O
,	O
a	O
,	O
let	O
b	O
be	O
the	O
branching	B
factor	I
(	O
i.e.	O
,	O
the	O
number	O
of	O
possible	O
next	O
states	O
,	O
s	O
(	O
cid:48	O
)	O
,	O
for	O
which	O
ˆp	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
a	O
)	O
>	O
0	O
)	O
.	O
then	O
an	O
expected	B
update	I
of	O
this	O
pair	O
requires	O
roughly	O
b	O
times	O
as	O
much	O
computation	O
as	O
a	O
sample	O
update	O
.	O
if	O
there	O
is	O
enough	O
time	O
to	O
complete	O
an	O
expected	B
update	I
,	O
then	O
the	O
resulting	O
estimate	O
is	O
generally	O
better	O
than	O
that	O
of	O
b	O
sample	O
updates	O
because	O
of	O
the	O
absence	O
of	O
sampling	O
error	O
.	O
but	O
if	O
there	O
is	O
insuﬃcient	O
time	O
to	O
complete	O
an	O
expected	B
update	I
,	O
then	O
sample	O
updates	O
are	O
always	O
preferable	O
because	O
they	O
at	O
least	O
make	O
some	O
improvement	O
in	O
the	O
value	O
estimate	O
with	O
fewer	O
than	O
b	O
updates	O
.	O
in	O
a	O
large	O
problem	O
with	O
many	O
state–action	O
pairs	O
,	O
we	O
are	O
often	O
in	O
the	O
latter	O
situation	O
.	O
with	O
so	O
many	O
state–action	O
pairs	O
,	O
expected	O
updates	O
of	O
all	O
of	O
them	O
would	O
take	O
a	O
very	O
long	O
time	O
.	O
before	O
that	O
we	O
may	O
be	O
much	O
better	O
oﬀ	O
with	O
a	O
few	O
sample	O
updates	O
at	O
many	O
state–action	O
pairs	O
than	O
with	O
expected	O
updates	O
at	O
a	O
few	O
pairs	O
.	O
given	O
a	O
unit	O
of	O
computational	O
eﬀort	O
,	O
is	O
it	O
better	O
devoted	O
to	O
a	O
few	O
expected	O
updates	O
or	O
to	O
b	O
times	O
as	O
many	O
sample	O
updates	O
?	O
figure	O
8.7	O
shows	O
the	O
results	O
of	O
an	O
analysis	O
that	O
suggests	O
an	O
answer	O
to	O
this	O
question	O
.	O
it	O
shows	O
the	O
estimation	O
error	O
as	O
a	O
function	O
of	O
computation	O
time	O
for	O
expected	O
and	O
sample	O
updates	O
for	O
a	O
variety	O
of	O
branching	O
factors	O
,	O
b.	O
the	O
case	O
considered	O
is	O
that	O
in	O
which	O
all	O
b	O
174	O
chapter	O
8	O
:	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
figure	O
8.7	O
:	O
comparison	O
of	O
eﬃciency	O
of	O
expected	O
and	O
sample	O
updates	O
.	O
to	O
(	O
cid:113	O
)	O
b−1	O
successor	O
states	O
are	O
equally	O
likely	O
and	O
in	O
which	O
the	O
error	O
in	O
the	O
initial	O
estimate	O
is	O
1.	O
the	O
values	O
at	O
the	O
next	O
states	O
are	O
assumed	O
correct	O
,	O
so	O
the	O
expected	B
update	I
reduces	O
the	O
error	O
to	O
zero	O
upon	O
its	O
completion	O
.	O
in	O
this	O
case	O
,	O
sample	O
updates	O
reduce	O
the	O
error	O
according	O
bt	O
where	O
t	O
is	O
the	O
number	O
of	O
sample	O
updates	O
that	O
have	O
been	O
performed	O
(	O
assuming	O
sample	O
averages	O
,	O
i.e.	O
,	O
α	O
=	O
1/t	O
)	O
.	O
the	O
key	O
observation	O
is	O
that	O
for	O
moderately	O
large	O
b	O
the	O
error	O
falls	O
dramatically	O
with	O
a	O
tiny	O
fraction	O
of	O
b	O
updates	O
.	O
for	O
these	O
cases	O
,	O
many	O
state–	O
action	B
pairs	O
could	O
have	O
their	O
values	O
improved	O
dramatically	O
,	O
to	O
within	O
a	O
few	O
percent	O
of	O
the	O
eﬀect	O
of	O
an	O
expected	B
update	I
,	O
in	O
the	O
same	O
time	O
that	O
a	O
single	O
state–action	O
pair	O
could	O
undergo	O
an	O
expected	B
update	I
.	O
the	O
advantage	O
of	O
sample	O
updates	O
shown	O
in	O
figure	O
8.7	O
is	O
probably	O
an	O
underestimate	O
of	O
the	O
real	O
eﬀect	O
.	O
in	O
a	O
real	O
problem	O
,	O
the	O
values	O
of	O
the	O
successor	O
states	O
would	O
be	O
estimates	O
that	O
are	O
themselves	O
updated	O
.	O
by	O
causing	O
estimates	O
to	O
be	O
more	O
accurate	O
sooner	O
,	O
sample	O
updates	O
will	O
have	O
a	O
second	O
advantage	O
in	O
that	O
the	O
values	O
backed	O
up	O
from	O
the	O
successor	O
states	O
will	O
be	O
more	O
accurate	O
.	O
these	O
results	O
suggest	O
that	O
sample	O
updates	O
are	O
likely	O
to	O
be	O
superior	O
to	O
expected	O
updates	O
on	O
problems	O
with	O
large	O
stochastic	O
branching	O
factors	O
and	O
too	O
many	O
states	O
to	O
be	O
solved	O
exactly	O
.	O
exercise	O
8.6	O
the	O
analysis	O
above	O
assumed	O
that	O
all	O
of	O
the	O
b	O
possible	O
next	O
states	O
were	O
equally	O
likely	O
to	O
occur	O
.	O
suppose	O
instead	O
that	O
the	O
distribution	O
was	O
highly	O
skewed	O
,	O
that	O
some	O
of	O
the	O
b	O
states	O
were	O
much	O
more	O
likely	O
to	O
occur	O
than	O
most	O
.	O
would	O
this	O
strengthen	O
or	O
weaken	O
the	O
case	O
for	O
sample	O
updates	O
over	O
expected	O
updates	O
?	O
support	O
your	O
answer	O
.	O
(	O
cid:3	O
)	O
8.6	O
trajectory	B
sampling	I
in	O
this	O
section	O
we	O
compare	O
two	O
ways	O
of	O
distributing	O
updates	O
.	O
the	O
classical	O
approach	O
,	O
from	O
dynamic	B
programming	I
,	O
is	O
to	O
perform	O
sweeps	B
through	O
the	O
entire	O
state	B
(	O
or	O
state–action	O
)	O
space	O
,	O
updating	O
each	O
state	B
(	O
or	O
state–action	O
pair	O
)	O
once	O
per	O
sweep	O
.	O
this	O
is	O
problematic	O
b	O
=	O
2	O
(	O
branching	B
factor	I
)	O
b	O
=10b	O
=100b	O
=1000b	O
=10,000sampleupdatesexpectedupdates1001b2brms	O
errorin	O
valueestimatenumber	O
of	O
computationsmaxa0q	O
(	O
s0	O
,	O
a0	O
)	O
8.6.	O
trajectory	B
sampling	I
175	O
on	O
large	O
tasks	O
because	O
there	O
may	O
not	O
be	O
time	O
to	O
complete	O
even	O
one	O
sweep	O
.	O
in	O
many	O
tasks	O
the	O
vast	O
majority	O
of	O
the	O
states	O
are	O
irrelevant	O
because	O
they	O
are	O
visited	O
only	O
under	O
very	O
poor	O
policies	O
or	O
with	O
very	O
low	O
probability	O
.	O
exhaustive	O
sweeps	B
implicitly	O
devote	O
equal	O
time	O
to	O
all	O
parts	O
of	O
the	O
state	B
space	O
rather	O
than	O
focusing	O
where	O
it	O
is	O
needed	O
.	O
as	O
we	O
discussed	O
in	O
chapter	O
4	O
,	O
exhaustive	O
sweeps	B
and	O
the	O
equal	O
treatment	O
of	O
all	O
states	O
that	O
they	O
imply	O
are	O
not	O
necessary	O
properties	O
of	O
dynamic	O
programming	O
.	O
in	O
principle	O
,	O
updates	O
can	O
be	O
distributed	O
any	O
way	O
one	O
likes	O
(	O
to	O
assure	O
convergence	O
,	O
all	O
states	O
or	O
state–action	O
pairs	O
must	O
be	O
visited	O
in	O
the	O
limit	O
an	O
inﬁnite	O
number	O
of	O
times	O
;	O
although	O
an	O
exception	O
to	O
this	O
is	O
discussed	O
in	O
section	O
8.7	O
below	O
)	O
,	O
but	O
in	O
practice	O
exhaustive	O
sweeps	B
are	O
often	O
used	O
.	O
the	O
second	O
approach	O
is	O
to	O
sample	O
from	O
the	O
state	B
or	O
state–action	O
space	O
according	O
to	O
some	O
distribution	O
.	O
one	O
could	O
sample	O
uniformly	O
,	O
as	O
in	O
the	O
dyna-q	O
agent	O
,	O
but	O
this	O
would	O
suﬀer	O
from	O
some	O
of	O
the	O
same	O
problems	O
as	O
exhaustive	O
sweeps	B
.	O
more	O
appealing	O
is	O
to	O
distribute	O
updates	O
according	O
to	O
the	O
on-policy	B
distribution	I
,	O
that	O
is	O
,	O
according	O
to	O
the	O
distribution	O
observed	O
when	O
following	O
the	O
current	O
policy	B
.	O
one	O
advantage	O
of	O
this	O
distribution	O
is	O
that	O
it	O
is	O
easily	O
generated	O
;	O
one	O
simply	O
interacts	O
with	O
the	O
model	O
,	O
following	O
the	O
current	O
policy	B
.	O
in	O
an	O
episodic	O
task	O
,	O
one	O
starts	O
in	O
a	O
start	O
state	B
(	O
or	O
according	O
to	O
the	O
starting-state	O
distribution	O
)	O
and	O
simulates	O
until	O
the	O
terminal	O
state	B
.	O
in	O
a	O
continuing	O
task	O
,	O
one	O
starts	O
anywhere	O
and	O
just	O
keeps	O
simulating	O
.	O
in	O
either	O
case	O
,	O
sample	O
state	O
transitions	O
and	O
rewards	O
are	O
given	O
by	O
the	O
model	O
,	O
and	O
sample	O
actions	O
are	O
given	O
by	O
the	O
current	O
policy	B
.	O
in	O
other	O
words	O
,	O
one	O
simulates	O
explicit	O
individual	O
trajectories	O
and	O
performs	O
updates	O
at	O
the	O
state	B
or	O
state–action	O
pairs	O
encountered	O
along	O
the	O
way	O
.	O
we	O
call	O
this	O
way	O
of	O
generating	O
experience	O
and	O
updates	O
trajectory	B
sampling	I
.	O
it	O
is	O
hard	O
to	O
imagine	O
any	O
eﬃcient	O
way	O
of	O
distributing	O
updates	O
according	O
to	O
the	O
on-	O
policy	B
distribution	O
other	O
than	O
by	O
trajectory	B
sampling	I
.	O
if	O
one	O
had	O
an	O
explicit	O
represen-	O
tation	O
of	O
the	O
on-policy	B
distribution	I
,	O
then	O
one	O
could	O
sweep	O
through	O
all	O
states	O
,	O
weighting	O
the	O
update	O
of	O
each	O
according	O
to	O
the	O
on-policy	B
distribution	I
,	O
but	O
this	O
leaves	O
us	O
again	O
with	O
all	O
the	O
computational	O
costs	O
of	O
exhaustive	O
sweeps	B
.	O
possibly	O
one	O
could	O
sample	O
and	O
update	O
individual	O
state–action	O
pairs	O
from	O
the	O
distribution	O
,	O
but	O
even	O
if	O
this	O
could	O
be	O
done	O
eﬃ-	O
ciently	O
,	O
what	O
beneﬁt	O
would	O
this	O
provide	O
over	O
simulating	O
trajectories	O
?	O
even	O
knowing	O
the	O
on-policy	B
distribution	I
in	O
an	O
explicit	O
form	O
is	O
unlikely	O
.	O
the	O
distribution	O
changes	O
whenever	O
the	O
policy	B
changes	O
,	O
and	O
computing	O
the	O
distribution	O
requires	O
computation	O
comparable	O
to	O
a	O
complete	O
policy	B
evaluation	I
.	O
consideration	O
of	O
such	O
other	O
possibilities	O
makes	O
trajectory	B
sampling	I
seem	O
both	O
eﬃcient	O
and	O
elegant	O
.	O
is	O
the	O
on-policy	B
distribution	I
of	O
updates	O
a	O
good	O
one	O
?	O
intuitively	O
it	O
seems	O
like	O
a	O
good	O
choice	O
,	O
at	O
least	O
better	O
than	O
the	O
uniform	O
distribution	O
.	O
for	O
example	O
,	O
if	O
you	O
are	O
learning	O
to	O
play	O
chess	B
,	O
you	O
study	O
positions	O
that	O
might	O
arise	O
in	O
real	O
games	O
,	O
not	O
random	O
positions	O
of	O
chess	O
pieces	O
.	O
the	O
latter	O
may	O
be	O
valid	O
states	O
,	O
but	O
to	O
be	O
able	O
to	O
accurately	O
value	B
them	O
is	O
a	O
diﬀerent	O
skill	O
from	O
evaluating	O
positions	O
in	O
real	O
games	O
.	O
we	O
will	O
also	O
see	O
in	O
part	O
ii	O
that	O
the	O
on-policy	B
distribution	I
has	O
signiﬁcant	O
advantages	O
when	O
function	B
approximation	I
is	O
used	O
.	O
whether	O
or	O
not	O
function	B
approximation	I
is	O
used	O
,	O
one	O
might	O
expect	O
on-policy	O
focusing	O
to	O
signiﬁcantly	O
improve	O
the	O
speed	O
of	O
planning	O
.	O
focusing	O
on	O
the	O
on-policy	B
distribution	I
could	O
be	O
beneﬁcial	O
because	O
it	O
causes	O
vast	O
,	O
unin-	O
teresting	O
parts	O
of	O
the	O
space	O
to	O
be	O
ignored	O
,	O
or	O
it	O
could	O
be	O
detrimental	O
because	O
it	O
causes	O
the	O
same	O
old	O
parts	O
of	O
the	O
space	O
to	O
be	O
updated	O
over	O
and	O
over	O
.	O
we	O
conducted	O
a	O
small	O
experi-	O
176	O
chapter	O
8	O
:	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
ment	O
to	O
assess	O
the	O
eﬀect	O
empirically	O
.	O
to	O
isolate	O
the	O
eﬀect	O
of	O
the	O
update	O
distribution	O
,	O
we	O
used	O
entirely	O
one-step	O
expected	O
tabular	O
updates	O
,	O
as	O
deﬁned	O
by	O
(	O
8.1	O
)	O
.	O
in	O
the	O
uniform	O
case	O
,	O
we	O
cycled	O
through	O
all	O
state–action	O
pairs	O
,	O
updating	O
each	O
in	O
place	O
,	O
and	O
in	O
the	O
on-policy	O
case	O
we	O
simulated	O
episodes	B
,	O
all	O
starting	O
in	O
the	O
same	O
state	B
,	O
updating	O
each	O
state–action	O
pair	O
that	O
occurred	O
under	O
the	O
current	O
ε-greedy	O
policy	O
(	O
ε	O
=	O
0.1	O
)	O
.	O
the	O
tasks	O
were	O
undiscounted	O
episodic	O
tasks	O
,	O
generated	O
randomly	O
as	O
follows	O
.	O
from	O
each	O
of	O
the	O
|s|	O
states	O
,	O
two	O
actions	O
were	O
possible	O
,	O
each	O
of	O
which	O
resulted	O
in	O
one	O
of	O
b	O
next	O
states	O
,	O
all	O
equally	O
likely	O
,	O
with	O
a	O
diﬀerent	O
random	O
selection	O
of	O
b	O
states	O
for	O
each	O
state–action	O
pair	O
.	O
the	O
branching	B
factor	I
,	O
b	O
,	O
was	O
the	O
same	O
for	O
all	O
state–action	O
pairs	O
.	O
in	O
addition	O
,	O
on	O
all	O
transitions	O
there	O
was	O
a	O
0.1	O
probability	O
of	O
transition	O
to	O
the	O
terminal	O
state	B
,	O
ending	O
the	O
episode	O
.	O
the	O
expected	O
reward	O
on	O
each	O
transition	O
was	O
selected	O
from	O
a	O
gaussian	O
distribution	O
with	O
mean	O
0	O
and	O
variance	O
1.	O
at	O
any	O
point	O
in	O
the	O
planning	O
process	O
one	O
can	O
stop	O
and	O
exhaustively	O
compute	O
v˜π	O
(	O
s0	O
)	O
,	O
the	O
true	O
value	O
of	O
the	O
start	O
state	B
under	O
the	O
greedy	O
policy	O
,	O
˜π	O
,	O
given	O
the	O
current	O
action-value	B
function	I
q	O
,	O
as	O
an	O
indication	O
of	O
how	O
well	O
the	O
agent	O
would	O
do	O
on	O
a	O
new	O
episode	O
on	O
which	O
it	O
acted	O
greedily	O
(	O
all	O
the	O
while	O
assuming	O
the	O
model	O
is	O
correct	O
)	O
.	O
the	O
upper	O
part	O
of	O
the	O
ﬁgure	O
to	O
the	O
right	O
shows	O
results	O
averaged	O
over	O
200	O
sample	O
tasks	O
with	O
1000	O
states	O
and	O
branching	O
factors	O
of	O
1	O
,	O
3	O
,	O
and	O
10.	O
the	O
quality	O
of	O
the	O
policies	O
found	O
is	O
plotted	O
as	O
a	O
function	O
of	O
the	O
number	O
of	O
expected	O
updates	O
completed	O
.	O
in	O
all	O
cases	O
,	O
sam-	O
pling	O
according	O
to	O
the	O
on-policy	O
distri-	O
bution	O
resulted	O
in	O
faster	O
planning	B
ini-	O
tially	O
and	O
retarded	O
planning	B
in	O
the	O
long	O
run	O
.	O
the	O
eﬀect	O
was	O
stronger	O
,	O
and	O
the	O
initial	O
period	O
of	O
faster	O
planning	B
was	O
longer	O
,	O
at	O
smaller	O
branching	O
factors	O
.	O
in	O
other	O
experiments	O
,	O
we	O
found	O
that	O
these	O
eﬀects	O
also	O
became	O
stronger	O
as	O
the	O
num-	O
ber	O
of	O
states	O
increased	O
.	O
for	O
example	O
,	O
the	O
lower	O
part	O
of	O
the	O
ﬁgure	O
shows	O
re-	O
sults	O
for	O
a	O
branching	O
factor	O
of	O
1	O
for	O
tasks	O
with	O
10,000	O
states	O
.	O
in	O
this	O
case	O
the	O
ad-	O
vantage	O
of	O
on-policy	O
focusing	O
is	O
large	O
and	O
long-lasting	O
.	O
all	O
of	O
these	O
results	O
make	O
sense	O
.	O
in	O
the	O
short	O
term	O
,	O
sampling	O
according	O
to	O
the	O
on-policy	B
distribution	I
helps	O
by	O
fo-	O
cusing	O
on	O
states	O
that	O
are	O
near	O
descen-	O
figure	O
8.8	O
:	O
relative	O
eﬃciency	O
of	O
updates	O
dis-	O
tributed	O
uniformly	O
across	O
the	O
state	B
space	O
versus	O
focused	O
on	O
simulated	O
on-policy	O
trajectories	O
,	O
each	O
starting	O
in	O
the	O
same	O
state	B
.	O
results	O
are	O
for	O
ran-	O
domly	O
generated	O
tasks	O
of	O
two	O
sizes	O
and	O
various	O
branching	O
factors	O
,	O
b.	O
b=10b=3b=1b=1ion-polcyion-polcyuniformuniform	O
0123value	O
ofstart	O
stateundergreedypolicy05,00010,00015,00020,000computation	O
time	O
,	O
in	O
full	O
backups0123value	O
ofstart	O
stateundergreedypolicy050,000100,000150,000200,000computation	O
time	O
,	O
in	O
full	O
backupsuniformuniformon-policyon-policyexpected	O
updatesexpected	O
updates1,000	O
states10,000	O
states	O
8.7.	O
real-time	B
dynamic	I
programming	I
177	O
if	O
there	O
are	O
dants	O
of	O
the	O
start	O
state	B
.	O
many	O
states	O
and	O
a	O
small	O
branching	B
factor	I
,	O
this	O
eﬀect	O
will	O
be	O
large	O
and	O
long-lasting	O
.	O
in	O
the	O
long	O
run	O
,	O
focusing	O
on	O
the	O
on-policy	B
distribution	I
may	O
hurt	O
because	O
the	O
commonly	O
occurring	O
states	O
all	O
already	O
have	O
their	O
correct	O
values	O
.	O
sampling	O
them	O
is	O
useless	O
,	O
whereas	O
sampling	O
other	O
states	O
may	O
actually	O
perform	O
some	O
useful	O
work	O
.	O
this	O
presumably	O
is	O
why	O
the	O
exhaustive	O
,	O
unfocused	O
approach	O
does	O
better	O
in	O
the	O
long	O
run	O
,	O
at	O
least	O
for	O
small	O
prob-	O
lems	O
.	O
these	O
results	O
are	O
not	O
conclusive	O
because	O
they	O
are	O
only	O
for	O
problems	O
generated	O
in	O
a	O
particular	O
,	O
random	O
way	O
,	O
but	O
they	O
do	O
suggest	O
that	O
sampling	O
according	O
to	O
the	O
on-policy	B
distribution	I
can	O
be	O
a	O
great	O
advantage	O
for	O
large	O
problems	O
,	O
in	O
particular	O
for	O
problems	O
in	O
which	O
a	O
small	O
subset	O
of	O
the	O
state–action	O
space	O
is	O
visited	O
under	O
the	O
on-policy	B
distribution	I
.	O
exercise	O
8.7	O
some	O
of	O
the	O
graphs	O
in	O
figure	O
8.8	O
seem	O
to	O
be	O
scalloped	O
in	O
their	O
early	O
portions	O
,	O
particularly	O
the	O
upper	O
graph	O
for	O
b	O
=	O
1	O
and	O
the	O
uniform	O
distribution	O
.	O
why	O
do	O
you	O
think	O
(	O
cid:3	O
)	O
this	O
is	O
?	O
what	O
aspects	O
of	O
the	O
data	O
shown	O
support	O
your	O
hypothesis	O
?	O
exercise	O
8.8	O
(	O
programming	O
)	O
replicate	O
the	O
experiment	O
whose	O
results	O
are	O
shown	O
in	O
the	O
lower	O
part	O
of	O
figure	O
8.8	O
,	O
then	O
try	O
the	O
same	O
experiment	O
but	O
with	O
b	O
=	O
3.	O
discuss	O
the	O
(	O
cid:3	O
)	O
meaning	O
of	O
your	O
results	O
.	O
8.7	O
real-time	B
dynamic	I
programming	I
real-time	O
dynamic	B
programming	I
,	O
or	O
rtdp	O
,	O
is	O
an	O
on-policy	O
trajectory-sampling	O
version	O
of	O
the	O
value-iteration	O
algorithm	O
of	O
dynamic	O
programming	O
(	O
dp	O
)	O
.	O
because	O
it	O
is	O
closely	O
related	O
to	O
conventional	O
sweep-based	O
policy	B
iteration	I
,	O
rtdp	O
illustrates	O
in	O
a	O
particularly	O
clear	O
way	O
some	O
of	O
the	O
advantages	O
that	O
on-policy	O
trajectory	O
sampling	O
can	O
provide	O
.	O
rtdp	O
updates	O
the	O
values	O
of	O
states	O
visited	O
in	O
actual	O
or	O
simulated	O
trajectories	O
by	O
means	O
of	O
expected	O
tabular	O
value-iteration	O
updates	O
as	O
deﬁned	O
by	O
(	O
4.10	O
)	O
.	O
it	O
is	O
basically	O
the	O
algorithm	O
that	O
produced	O
the	O
on-policy	O
results	O
shown	O
in	O
figure	O
8.8.	O
the	O
close	O
connection	O
between	O
rtdp	O
and	O
conventional	O
dp	O
makes	O
it	O
possible	O
to	O
derive	O
some	O
theoretical	O
results	O
by	O
adapting	O
existing	O
theory	O
.	O
rtdp	O
is	O
an	O
example	O
of	O
an	O
asyn-	O
chronous	O
dp	O
algorithm	O
as	O
described	O
in	O
section	O
4.5.	O
asynchronous	O
dp	O
algorithms	O
are	O
not	O
organized	O
in	O
terms	O
of	O
systematic	O
sweeps	B
of	O
the	O
state	B
set	O
;	O
they	O
update	O
state	B
values	O
in	O
any	O
order	O
whatsoever	O
,	O
using	O
whatever	O
values	O
of	O
other	O
states	O
happen	O
to	O
be	O
available	O
.	O
in	O
rtdp	O
,	O
the	O
update	O
order	O
is	O
dictated	O
by	O
the	O
order	O
states	O
are	O
visited	O
in	O
real	O
or	O
simulated	O
trajectories	O
.	O
if	O
trajectories	O
can	O
start	O
only	O
from	O
a	O
designated	O
set	O
of	O
start	O
states	O
,	O
and	O
if	O
you	O
are	O
interested	O
in	O
the	O
prediction	O
problem	O
for	O
a	O
given	O
policy	O
,	O
then	O
on-policy	O
trajectory	O
sampling	O
allows	O
the	O
algorithm	O
to	O
completely	O
skip	O
states	O
that	O
can	O
not	O
be	O
reached	O
by	O
the	O
given	O
policy	B
from	O
any	O
of	O
the	O
start	O
states	O
:	O
unreachable	O
states	O
are	O
irrelevant	O
to	O
the	O
prediction	B
problem	O
.	O
for	O
a	O
control	O
problem	O
,	O
where	O
the	O
goal	B
is	O
to	O
ﬁnd	O
an	O
optimal	O
policy	O
instead	O
of	O
evaluating	O
a	O
given	O
policy	B
,	O
there	O
might	O
well	O
be	O
states	O
that	O
can	O
not	O
be	O
reached	O
by	O
any	O
optimal	O
policy	O
from	O
any	O
of	O
the	O
start	O
states	O
,	O
and	O
there	O
is	O
no	O
need	O
to	O
specify	O
optimal	O
actions	O
for	O
these	O
irrelevant	O
states	O
.	O
what	O
is	O
needed	O
is	O
an	O
optimal	O
partial	O
policy	B
,	O
meaning	O
a	O
policy	B
that	O
is	O
optimal	O
for	O
the	O
relevant	O
states	O
but	O
can	O
specify	O
arbitrary	O
actions	O
,	O
or	O
even	O
be	O
undeﬁned	O
,	O
178	O
chapter	O
8	O
:	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
for	O
the	O
irrelevant	O
states	O
(	O
see	O
illustra-	O
tion	B
to	O
the	O
right	O
)	O
.	O
but	O
ﬁnding	O
such	O
an	O
optimal	O
partial	O
policy	B
with	O
an	O
on-policy	O
trajectory-sampling	O
control	B
method	O
,	O
such	O
as	O
sarsa	O
(	O
section	O
6.4	O
)	O
,	O
in	O
gen-	O
eral	O
requires	O
visiting	O
all	O
state–action	O
pairs—even	O
those	O
that	O
will	O
turn	O
out	O
to	O
be	O
irrelevant—an	O
inﬁnite	O
number	O
of	O
times	O
.	O
this	O
can	O
be	O
done	O
,	O
for	O
ex-	O
ample	O
,	O
by	O
using	O
exploring	B
starts	I
(	O
section	O
5.3	O
)	O
.	O
this	O
is	O
true	O
for	O
rtdp	O
as	O
well	O
:	O
for	O
episodic	O
tasks	O
with	O
exploring	O
starts	O
,	O
rtdp	O
is	O
an	O
asynchronous	O
value-iteration	O
algorithm	O
that	O
converges	O
to	O
optimal	O
polices	O
for	O
discounted	O
ﬁnite	O
mdps	O
(	O
and	O
for	O
the	O
undiscounted	O
case	O
under	O
certain	O
conditions	O
)	O
.	O
unlike	O
the	O
situation	O
for	O
a	O
prediction	O
problem	O
,	O
it	O
is	O
gen-	O
erally	O
not	O
possible	O
to	O
stop	O
updating	O
any	O
state	B
or	O
state–action	O
pair	O
if	O
convergence	O
to	O
an	O
optimal	O
policy	O
is	O
important	O
.	O
the	O
most	O
interesting	O
result	O
for	O
rtdp	O
is	O
that	O
for	O
certain	O
types	O
of	O
problems	O
satisfying	O
reasonable	O
conditions	O
,	O
rtdp	O
is	O
guaranteed	O
to	O
ﬁnd	O
a	O
policy	B
that	O
is	O
optimal	O
on	O
the	O
relevant	O
states	O
without	O
visiting	O
every	O
state	B
inﬁnitely	O
often	O
,	O
or	O
even	O
without	O
visiting	O
some	O
states	O
at	O
all	O
.	O
indeed	O
,	O
in	O
some	O
problems	O
,	O
only	O
a	O
small	O
fraction	O
of	O
the	O
states	O
need	O
to	O
be	O
visited	O
.	O
this	O
can	O
be	O
a	O
great	O
advantage	O
for	O
problems	O
with	O
very	O
large	O
state	B
sets	O
,	O
where	O
even	O
a	O
single	O
sweep	O
may	O
not	O
be	O
feasible	O
.	O
the	O
tasks	O
for	O
which	O
this	O
result	O
holds	O
are	O
undiscounted	O
episodic	O
tasks	O
for	O
mdps	O
with	O
absorbing	O
goal	B
states	O
that	O
generate	O
zero	O
rewards	O
,	O
as	O
described	O
in	O
section	O
3.4.	O
at	O
every	O
step	O
of	O
a	O
real	O
or	O
simulated	O
trajectory	O
,	O
rtdp	O
selects	O
a	O
greedy	O
action	O
(	O
breaking	O
ties	O
ran-	O
domly	O
)	O
and	O
applies	O
the	O
expected	O
value-iteration	O
update	O
operation	O
to	O
the	O
current	O
state	B
.	O
it	O
can	O
also	O
update	O
the	O
values	O
of	O
an	O
arbitrary	O
collection	O
of	O
other	O
states	O
at	O
each	O
step	O
;	O
for	O
example	O
,	O
it	O
can	O
update	O
the	O
values	O
of	O
states	O
visited	O
in	O
a	O
limited-horizon	O
look-ahead	O
search	O
from	O
the	O
current	O
state	B
.	O
for	O
these	O
problems	O
,	O
with	O
each	O
episode	O
beginning	O
in	O
a	O
state	B
randomly	O
chosen	O
from	O
the	O
set	O
of	O
start	O
states	O
and	O
ending	O
at	O
a	O
goal	B
state	O
,	O
rtdp	O
converges	O
with	O
probability	O
one	O
to	O
a	O
policy	B
that	O
is	O
optimal	O
for	O
all	O
the	O
relevant	O
states	O
provided	O
:	O
1	O
)	O
the	O
initial	O
value	B
of	O
every	O
goal	B
state	O
is	O
zero	O
,	O
2	O
)	O
there	O
exists	O
at	O
least	O
one	O
policy	B
that	O
guarantees	O
that	O
a	O
goal	B
state	O
will	O
be	O
reached	O
with	O
probability	O
one	O
from	O
any	O
start	O
state	B
,	O
3	O
)	O
all	O
rewards	O
for	O
transitions	O
from	O
non-goal	O
states	O
are	O
strictly	O
negative	O
,	O
and	O
4	O
)	O
all	O
the	O
initial	O
values	O
are	O
equal	O
to	O
,	O
or	O
greater	O
than	O
,	O
their	O
optimal	O
values	O
(	O
which	O
can	O
be	O
satisﬁed	O
by	O
simply	O
setting	O
the	O
initial	O
values	O
of	O
all	O
states	O
to	O
zero	O
)	O
.	O
this	O
result	O
was	O
proved	O
by	O
barto	O
,	O
bradtke	O
,	O
and	O
singh	O
(	O
1995	O
)	O
by	O
combining	O
results	O
for	O
asynchronous	O
dp	O
with	O
results	O
about	O
a	O
heuristic	B
search	I
algorithm	O
known	O
as	O
learning	O
real-time	O
a*	O
due	O
to	O
korf	O
(	O
1990	O
)	O
.	O
tasks	O
having	O
these	O
properties	O
are	O
examples	O
of	O
stochastic	O
optimal	O
path	O
problems	O
,	O
which	O
are	O
usually	O
stated	O
in	O
terms	O
of	O
cost	O
minimization	O
instead	O
of	O
as	O
reward	O
maximization	O
as	O
we	O
do	O
here	O
.	O
maximizing	O
the	O
negative	O
returns	O
in	O
our	O
version	O
is	O
equivalent	O
to	O
minimizing	O
the	O
costs	O
of	O
paths	O
from	O
a	O
start	O
state	B
to	O
a	O
goal	B
state	O
.	O
examples	O
of	O
this	O
kind	O
of	O
task	O
are	O
minimum-time	O
control	B
tasks	O
,	O
where	O
each	O
time	O
step	O
required	O
to	O
reach	O
a	O
goal	B
produces	O
a	O
start	O
statesirrelevant	O
states	O
:	O
unreachable	O
from	O
any	O
start	O
stateunder	O
any	O
optimal	O
policyrelevant	O
statesreachable	O
from	O
some	O
start	O
state	B
under	O
some	O
optimal	O
policy	O
8.7.	O
real-time	B
dynamic	I
programming	I
179	O
reward	O
of	O
−1	O
,	O
or	O
problems	O
like	O
the	O
golf	B
example	I
in	O
section	O
3.5	O
,	O
whose	O
objective	O
is	O
to	O
hit	O
the	O
hole	O
with	O
the	O
fewest	O
strokes	O
.	O
example	O
8.6	O
:	O
rtdp	O
on	O
the	O
racetrack	O
the	O
racetrack	O
problem	O
of	O
exercise	O
5.10	O
(	O
page	O
111	O
)	O
is	O
a	O
stochastic	O
optimal	O
path	O
problem	O
.	O
comparing	O
rtdp	O
and	O
the	O
conventional	O
dp	O
value	B
iteration	I
algorithm	O
on	O
an	O
example	O
racetrack	O
problem	O
illustrates	O
some	O
of	O
the	O
advantages	B
of	I
on-policy	O
trajectory	B
sampling	I
.	O
recall	O
from	O
the	O
exercise	O
that	O
an	O
agent	O
has	O
to	O
learn	O
how	O
to	O
drive	O
a	O
car	O
around	O
a	O
turn	O
like	O
those	O
shown	O
in	O
figure	O
5.5	O
and	O
cross	O
the	O
ﬁnish	O
line	O
as	O
quickly	O
as	O
possible	O
while	O
staying	O
on	O
the	O
track	O
.	O
start	O
states	O
are	O
all	O
the	O
zero-speed	O
states	O
on	O
the	O
starting	O
line	O
;	O
the	O
goal	B
states	O
are	O
all	O
the	O
states	O
that	O
can	O
be	O
reached	O
in	O
one	O
time	O
step	O
by	O
crossing	O
the	O
ﬁnish	O
line	O
from	O
inside	O
the	O
track	O
.	O
unlike	O
exercise	O
5.10	O
,	O
here	O
there	O
is	O
no	O
limit	O
on	O
the	O
car	O
’	O
s	O
speed	O
,	O
so	O
the	O
state	B
set	O
is	O
potentially	O
inﬁnite	O
.	O
however	O
,	O
the	O
set	O
of	O
states	O
that	O
can	O
be	O
reached	O
from	O
the	O
set	O
of	O
start	O
states	O
via	O
any	O
policy	B
is	O
ﬁnite	O
and	O
can	O
be	O
considered	O
to	O
be	O
the	O
state	B
set	O
of	O
the	O
problem	O
.	O
each	O
episode	O
begins	O
in	O
a	O
randomly	O
selected	O
start	O
state	B
and	O
ends	O
when	O
the	O
car	O
crosses	O
the	O
ﬁnish	O
line	O
.	O
the	O
rewards	O
are	O
−1	O
for	O
each	O
step	O
until	O
the	O
car	O
crosses	O
the	O
ﬁnish	O
line	O
.	O
if	O
the	O
car	O
hits	O
the	O
track	O
boundary	O
,	O
it	O
is	O
moved	O
back	O
to	O
a	O
random	O
start	O
state	B
,	O
and	O
the	O
episode	O
continues	O
.	O
a	O
racetrack	O
similar	O
to	O
the	O
small	O
racetrack	O
on	O
the	O
left	O
of	O
figure	O
5.5	O
has	O
9,115	O
states	O
reachable	O
from	O
start	O
states	O
by	O
any	O
policy	B
,	O
only	O
599	O
of	O
which	O
are	O
relevant	O
,	O
meaning	O
that	O
they	O
are	O
reachable	O
from	O
some	O
start	O
state	B
via	O
some	O
optimal	O
policy	O
.	O
(	O
the	O
number	O
of	O
relevant	O
states	O
was	O
estimated	O
by	O
counting	O
the	O
states	O
visited	O
while	O
executing	O
optimal	O
actions	O
for	O
107	O
episodes	B
.	O
)	O
the	O
table	O
below	O
compares	O
solving	O
this	O
task	O
by	O
conventional	O
dp	O
and	O
by	O
rtdp	O
.	O
these	O
results	O
are	O
averages	O
over	O
25	O
runs	O
,	O
each	O
begun	O
with	O
a	O
diﬀerent	O
random	O
number	O
seed	O
.	O
conventional	O
dp	O
in	O
this	O
case	O
is	O
value	B
iteration	I
using	O
exhaustive	O
sweeps	B
of	O
the	O
state	B
set	O
,	O
with	O
values	O
updated	O
one	O
state	B
at	O
a	O
time	O
in	O
place	O
,	O
meaning	O
that	O
the	O
update	O
for	O
each	O
state	B
uses	O
the	O
most	O
recent	O
values	O
of	O
the	O
other	O
states	O
(	O
this	O
is	O
the	O
gauss-seidel	O
version	O
of	O
value	O
iteration	O
,	O
which	O
was	O
found	O
to	O
be	O
approximately	O
twice	O
as	O
fast	O
as	O
the	O
jacobi	O
version	O
on	O
this	O
problem	O
.	O
see	O
section	O
4.8	O
.	O
)	O
no	O
special	O
attention	O
was	O
paid	O
to	O
the	O
ordering	O
of	O
the	O
updates	O
;	O
other	O
orderings	O
could	O
have	O
produced	O
faster	O
convergence	O
.	O
initial	O
values	O
were	O
all	O
zero	O
for	O
each	O
run	O
of	O
both	O
methods	O
.	O
dp	O
was	O
judged	O
to	O
have	O
converged	O
when	O
the	O
maximum	O
change	O
in	O
a	O
state	B
value	O
over	O
a	O
sweep	O
was	O
less	O
than	O
10−4	O
,	O
and	O
rtdp	O
was	O
judged	O
to	O
have	O
converged	O
when	O
the	O
average	O
time	O
to	O
cross	O
the	O
ﬁnish	O
line	O
over	O
20	O
episodes	B
appeared	O
to	O
stabilize	O
at	O
an	O
asymptotic	O
number	O
of	O
steps	O
.	O
this	O
version	O
of	O
rtdp	O
updated	O
only	O
the	O
value	B
of	O
the	O
current	O
state	B
on	O
each	O
step	O
.	O
average	O
computation	O
to	O
convergence	O
average	O
number	O
of	O
updates	O
to	O
convergence	O
average	O
number	O
of	O
updates	O
per	O
episode	O
%	O
of	O
states	O
updated	O
≤	O
100	O
times	O
%	O
of	O
states	O
updated	O
≤	O
10	O
times	O
%	O
of	O
states	O
updated	O
0	O
times	O
dp	O
28	O
sweeps	B
252,784	O
—	O
—	O
—	O
—	O
rtdp	O
4000	O
episodes	B
127,600	O
31.9	O
98.45	O
80.51	O
3.18	O
both	O
methods	O
produced	O
policies	O
averaging	O
between	O
14	O
and	O
15	O
steps	O
to	O
cross	O
the	O
ﬁnish	O
180	O
chapter	O
8	O
:	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
line	O
,	O
but	O
rtdp	O
required	O
only	O
roughly	O
half	O
of	O
the	O
updates	O
that	O
dp	O
did	O
.	O
this	O
is	O
the	O
result	O
of	O
rtdp	O
’	O
s	O
on-policy	O
trajectory	O
sampling	O
.	O
whereas	O
the	O
value	B
of	O
every	O
state	B
was	O
updated	O
in	O
each	O
sweep	O
of	O
dp	O
,	O
rtdp	O
focused	O
updates	O
on	O
fewer	O
states	O
.	O
in	O
an	O
average	O
run	O
,	O
rtdp	O
updated	O
the	O
values	O
of	O
98.45	O
%	O
of	O
the	O
states	O
no	O
more	O
than	O
100	O
times	O
and	O
80.51	O
%	O
of	O
the	O
states	O
no	O
more	O
than	O
10	O
times	O
;	O
the	O
values	O
of	O
about	O
290	O
states	O
were	O
not	O
updated	O
at	O
all	O
in	O
an	O
average	O
run	O
.	O
another	O
advantage	O
of	O
rtdp	O
is	O
that	O
as	O
the	O
value	B
function	I
approaches	O
the	O
optimal	O
value	O
function	O
v∗	O
,	O
the	O
policy	B
used	O
by	O
the	O
agent	O
to	O
generate	O
trajectories	O
approaches	O
an	O
optimal	O
policy	O
because	O
it	O
is	O
always	O
greedy	O
with	O
respect	O
to	O
the	O
current	O
value	B
function	I
.	O
this	O
is	O
in	O
contrast	O
to	O
the	O
situation	O
in	O
conventional	O
value	B
iteration	I
.	O
in	O
practice	O
,	O
value	B
iteration	I
terminates	O
when	O
the	O
value	B
function	I
changes	O
by	O
only	O
a	O
small	O
amount	O
in	O
a	O
sweep	O
,	O
which	O
is	O
how	O
we	O
terminated	O
it	O
to	O
obtain	O
the	O
results	O
in	O
the	O
table	O
above	O
.	O
at	O
this	O
point	O
,	O
the	O
value	B
function	I
closely	O
approximates	O
v∗	O
,	O
and	O
a	O
greedy	O
policy	O
is	O
close	O
to	O
an	O
optimal	O
policy	O
.	O
however	O
,	O
it	O
is	O
possible	O
that	O
policies	O
that	O
are	O
greedy	O
with	O
respect	O
to	O
the	O
latest	O
value	B
function	I
were	O
optimal	O
,	O
or	O
nearly	O
so	O
,	O
long	O
before	O
value	B
iteration	I
terminates	O
.	O
(	O
re-	O
call	O
from	O
chapter	O
4	O
that	O
optimal	O
policies	O
can	O
be	O
greedy	O
with	O
respect	O
to	O
many	O
diﬀerent	O
value	B
functions	O
,	O
not	O
just	O
v∗	O
.	O
)	O
checking	O
for	O
the	O
emergence	O
of	O
an	O
optimal	O
policy	O
before	O
value	B
iteration	I
converges	O
is	O
not	O
a	O
part	O
of	O
the	O
conventional	O
dp	O
algorithm	O
and	O
requires	O
considerable	O
additional	O
computation	O
.	O
in	O
the	O
racetrack	O
example	O
,	O
by	O
running	O
many	O
test	O
episodes	B
after	O
each	O
dp	O
sweep	O
,	O
with	O
actions	O
selected	O
greedily	O
according	O
to	O
the	O
result	O
of	O
that	O
sweep	O
,	O
it	O
was	O
possible	O
to	O
estimate	O
the	O
earliest	O
point	O
in	O
the	O
dp	O
computation	O
at	O
which	O
the	O
approximated	O
optimal	O
evaluation	O
function	O
was	O
good	O
enough	O
so	O
that	O
the	O
corresponding	O
greedy	O
policy	O
was	O
nearly	O
optimal	O
.	O
for	O
this	O
racetrack	O
,	O
a	O
close-to-optimal	O
policy	B
emerged	O
after	O
15	O
sweeps	B
of	O
value	B
iteration	I
,	O
or	O
after	O
136,725	O
value-iteration	O
updates	O
.	O
this	O
is	O
considerably	O
less	O
than	O
the	O
252,784	O
updates	O
dp	O
needed	O
to	O
converge	O
to	O
v∗	O
,	O
but	O
sill	O
more	O
than	O
the	O
127,600	O
updates	O
rtdp	O
required	O
.	O
although	O
these	O
simulations	O
are	O
certainly	O
not	O
deﬁnitive	O
comparisons	O
of	O
the	O
rtdp	O
with	O
conventional	O
sweep-based	O
value	B
iteration	I
,	O
they	O
illustrate	O
some	O
of	O
advantages	O
of	O
on-policy	O
trajectory	B
sampling	I
.	O
whereas	O
conventional	O
value	B
iteration	I
continued	O
to	O
update	O
the	O
value	B
of	O
all	O
the	O
states	O
,	O
rtdp	O
strongly	O
focused	O
on	O
subsets	O
of	O
the	O
states	O
that	O
were	O
relevant	O
to	O
the	O
problem	O
’	O
s	O
objective	O
.	O
this	O
focus	O
became	O
increasingly	O
narrow	O
as	O
learning	O
continued	O
.	O
because	O
the	O
convergence	O
theorem	B
for	O
rtdp	O
applies	O
to	O
the	O
simulations	O
,	O
we	O
know	O
that	O
rtdp	O
eventually	O
would	O
have	O
focused	O
only	O
on	O
relevant	O
states	O
,	O
i.e.	O
,	O
on	O
states	O
making	O
up	O
optimal	O
paths	O
.	O
rtdp	O
achieved	O
nearly	O
optimal	B
control	I
with	O
about	O
50	O
%	O
of	O
the	O
computa-	O
tion	B
required	O
by	O
sweep-based	O
value	B
iteration	I
.	O
8.8	O
planning	B
at	O
decision	O
time	O
planning	B
can	O
be	O
used	O
in	O
at	O
least	O
two	O
ways	O
.	O
the	O
one	O
we	O
have	O
considered	O
so	O
far	O
in	O
this	O
chapter	O
,	O
typiﬁed	O
by	O
dynamic	B
programming	I
and	O
dyna	O
,	O
is	O
to	O
use	O
planning	B
to	O
gradually	O
improve	O
a	O
policy	B
or	O
value	B
function	I
on	O
the	O
basis	O
of	O
simulated	O
experience	O
obtained	O
from	O
a	O
model	O
(	O
either	O
a	O
sample	O
or	O
a	O
distribution	O
model	O
)	O
.	O
selecting	O
actions	O
is	O
then	O
a	O
matter	O
of	O
comparing	O
the	O
current	O
state	B
’	O
s	O
action	B
values	O
obtained	O
from	O
a	O
table	O
in	O
the	O
tabular	O
case	O
we	O
have	O
thus	O
far	O
considered	O
,	O
or	O
by	O
evaluating	O
a	O
mathematical	O
expression	O
in	O
the	O
8.9.	O
heuristic	B
search	I
181	O
approximate	B
methods	O
we	O
consider	O
in	O
part	O
ii	O
below	O
.	O
well	O
before	O
an	O
action	B
is	O
selected	O
for	O
any	O
current	O
state	B
st	O
,	O
planning	B
has	O
played	O
a	O
part	O
in	O
improving	O
the	O
table	O
entries	O
,	O
or	O
the	O
mathematical	O
expression	O
,	O
needed	O
to	O
select	O
the	O
action	B
for	O
many	O
states	O
,	O
including	O
st.	O
used	O
this	O
way	O
,	O
planning	B
is	O
not	O
focussed	O
on	O
the	O
current	O
state	B
.	O
we	O
call	O
planning	B
used	O
in	O
this	O
way	O
background	O
planning	B
.	O
the	O
other	O
way	O
to	O
use	O
planning	B
is	O
to	O
begin	O
and	O
complete	O
it	O
after	O
encountering	O
each	O
new	O
state	B
st	O
,	O
as	O
a	O
computation	O
whose	O
output	O
is	O
the	O
selection	O
of	O
a	O
single	O
action	B
at	O
;	O
on	O
the	O
next	O
step	O
planning	B
begins	O
anew	O
with	O
st+1	O
to	O
produce	O
at+1	O
,	O
and	O
so	O
on	O
.	O
the	O
simplest	O
,	O
and	O
almost	O
degenerate	O
,	O
example	O
of	O
this	O
use	O
of	O
planning	O
is	O
when	O
only	O
state	B
values	O
are	O
available	O
,	O
and	O
an	O
action	B
is	O
selected	O
by	O
comparing	O
the	O
values	O
of	O
model-predicted	O
next	O
states	O
for	O
each	O
action	B
(	O
or	O
by	O
comparing	O
the	O
values	O
of	O
afterstates	O
as	O
in	O
the	O
tic-tac-toe	B
example	O
in	O
chapter	O
1	O
)	O
.	O
more	O
generally	O
,	O
planning	B
used	O
in	O
this	O
way	O
can	O
look	O
much	O
deeper	O
than	O
one-step-ahead	O
and	O
evaluate	O
action	B
choices	O
leading	O
to	O
many	O
diﬀerent	O
predicted	O
state	B
and	O
reward	O
trajectories	O
.	O
unlike	O
the	O
ﬁrst	O
use	O
of	O
planning	O
,	O
here	O
planning	B
focuses	O
on	O
a	O
particular	O
state	B
.	O
we	O
call	O
this	O
decision-time	O
planning	B
.	O
these	O
two	O
ways	O
of	O
thinking	O
about	O
planning—using	O
simulated	O
experience	O
to	O
gradually	O
improve	O
a	O
policy	B
or	O
value	B
function	I
,	O
or	O
using	O
simulated	O
experience	O
to	O
select	O
an	O
action	B
for	O
the	O
current	O
state—can	O
blend	O
together	O
in	O
natural	O
and	O
interesting	O
ways	O
,	O
but	O
they	O
have	O
tended	O
to	O
be	O
studied	O
separately	O
,	O
and	O
that	O
is	O
a	O
good	O
way	O
to	O
ﬁrst	O
understand	O
them	O
.	O
let	O
us	O
now	O
take	O
a	O
closer	O
look	O
at	O
decision-time	O
planning	B
.	O
even	O
when	O
planning	B
is	O
only	O
done	O
at	O
decision	O
time	O
,	O
we	O
can	O
still	O
view	O
it	O
,	O
as	O
we	O
did	O
in	O
section	O
8.1	O
,	O
as	O
proceeding	O
from	O
simulated	O
experience	O
to	O
updates	O
and	O
values	O
,	O
and	O
ultimately	O
to	O
a	O
policy	B
.	O
it	O
is	O
just	O
that	O
now	O
the	O
values	O
and	O
policy	O
are	O
speciﬁc	O
to	O
the	O
current	O
state	B
and	O
the	O
action	B
choices	O
available	O
there	O
,	O
so	O
much	O
so	O
that	O
the	O
values	O
and	O
policy	O
created	O
by	O
the	O
planning	B
process	O
are	O
typically	O
discarded	O
after	O
being	O
used	O
to	O
select	O
the	O
current	O
action	B
.	O
in	O
many	O
applications	O
this	O
is	O
not	O
a	O
great	O
loss	O
because	O
there	O
are	O
very	O
many	O
states	O
and	O
we	O
are	O
unlikely	O
to	O
return	B
to	O
the	O
same	O
state	B
for	O
a	O
long	O
time	O
.	O
in	O
general	O
,	O
one	O
may	O
want	O
to	O
do	O
a	O
mix	O
of	O
both	O
:	O
focus	O
planning	B
on	O
the	O
current	O
state	B
and	O
store	O
the	O
results	O
of	O
planning	O
so	O
as	O
to	O
be	O
that	O
much	O
farther	O
along	O
should	O
one	O
return	B
to	O
the	O
same	O
state	B
later	O
.	O
decision-time	O
planning	B
is	O
most	O
useful	O
in	O
applications	O
in	O
which	O
fast	O
responses	O
are	O
not	O
required	O
.	O
in	O
chess	O
playing	O
programs	O
,	O
for	O
example	O
,	O
one	O
may	O
be	O
permitted	O
seconds	O
or	O
minutes	O
of	O
computation	O
for	O
each	O
move	O
,	O
and	O
strong	O
programs	O
may	O
plan	O
dozens	O
of	O
moves	O
ahead	O
within	O
this	O
time	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
low	O
latency	O
action	B
selection	O
is	O
the	O
priority	O
,	O
then	O
one	O
is	O
generally	O
better	O
oﬀ	O
doing	O
planning	B
in	O
the	O
background	O
to	O
compute	O
a	O
policy	B
that	O
can	O
then	O
be	O
rapidly	O
applied	O
to	O
each	O
newly	O
encountered	O
state	B
.	O
8.9	O
heuristic	B
search	I
the	O
classical	O
state-space	O
planning	B
methods	O
in	O
artiﬁcial	O
intelligence	O
are	O
decision-time	O
planning	B
methods	O
collectively	O
known	O
as	O
heuristic	O
search	O
.	O
in	O
heuristic	O
search	O
,	O
for	O
each	O
state	B
encountered	O
,	O
a	O
large	O
tree	O
of	O
possible	O
continuations	O
is	O
considered	O
.	O
the	O
approximate	B
value	O
function	O
is	O
applied	O
to	O
the	O
leaf	O
nodes	O
and	O
then	O
backed	O
up	O
toward	O
the	O
current	O
state	B
at	O
the	O
root	O
.	O
the	O
backing	O
up	O
within	O
the	O
search	O
tree	O
is	O
just	O
the	O
same	O
as	O
in	O
the	O
expected	O
updates	O
with	O
maxes	O
(	O
those	O
for	O
v∗	O
and	O
q∗	O
)	O
discussed	O
throughout	O
this	O
book	O
.	O
the	O
backing	O
182	O
chapter	O
8	O
:	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
up	O
stops	O
at	O
the	O
state–action	O
nodes	O
for	O
the	O
current	O
state	B
.	O
once	O
the	O
backed-up	O
values	O
of	O
these	O
nodes	O
are	O
computed	O
,	O
the	O
best	O
of	O
them	O
is	O
chosen	O
as	O
the	O
current	O
action	B
,	O
and	O
then	O
all	O
backed-up	O
values	O
are	O
discarded	O
.	O
in	O
conventional	O
heuristic	B
search	I
no	O
eﬀort	O
is	O
made	O
to	O
save	O
the	O
backed-up	O
values	O
by	O
changing	O
the	O
approximate	B
value	O
function	O
.	O
in	O
fact	O
,	O
the	O
value	B
function	I
is	O
generally	O
designed	O
by	O
people	O
and	O
never	O
changed	O
as	O
a	O
result	O
of	O
search	O
.	O
however	O
,	O
it	O
is	O
natural	O
to	O
consider	O
allowing	O
the	O
value	B
function	I
to	O
be	O
improved	O
over	O
time	O
,	O
using	O
either	O
the	O
backed-up	O
values	O
computed	O
during	O
heuristic	B
search	I
or	O
any	O
of	O
the	O
other	O
methods	O
presented	O
throughout	O
this	O
book	O
.	O
in	O
a	O
sense	O
we	O
have	O
taken	O
this	O
approach	O
all	O
along	O
.	O
our	O
greedy	O
,	O
ε-greedy	O
,	O
and	O
ucb	O
(	O
section	O
2.7	O
)	O
action-selection	O
methods	O
are	O
not	O
unlike	O
heuristic	B
search	I
,	O
albeit	O
on	O
a	O
smaller	O
scale	O
.	O
for	O
example	O
,	O
to	O
compute	O
the	O
greedy	O
action	O
given	O
a	O
model	O
and	O
a	O
state-value	O
function	O
,	O
we	O
must	O
look	O
ahead	O
from	O
each	O
possible	O
action	B
to	O
each	O
possible	O
next	O
state	B
,	O
take	O
into	O
account	O
the	O
rewards	O
and	O
estimated	O
values	O
,	O
and	O
then	O
pick	O
the	O
best	O
action	B
.	O
just	O
as	O
in	O
conventional	O
heuristic	B
search	I
,	O
this	O
process	O
computes	O
backed-up	O
values	O
of	O
the	O
possible	O
actions	O
,	O
but	O
does	O
not	O
attempt	O
to	O
save	O
them	O
.	O
thus	O
,	O
heuristic	B
search	I
can	O
be	O
viewed	O
as	O
an	O
extension	O
of	O
the	O
idea	O
of	O
a	O
greedy	O
policy	O
beyond	O
a	O
single	O
step	O
.	O
the	O
point	O
of	O
searching	O
deeper	O
than	O
one	O
step	O
is	O
to	O
obtain	O
better	O
action	B
selections	O
.	O
if	O
one	O
has	O
a	O
perfect	O
model	O
and	O
an	O
imperfect	O
action-value	B
function	I
,	O
then	O
in	O
fact	O
deeper	O
search	O
will	O
usually	O
yield	O
better	O
policies.2	O
certainly	O
,	O
if	O
the	O
search	O
is	O
all	O
the	O
way	O
to	O
the	O
end	O
of	O
the	O
episode	O
,	O
then	O
the	O
eﬀect	O
of	O
the	O
imperfect	O
value	B
function	I
is	O
eliminated	O
,	O
and	O
the	O
action	O
determined	O
in	O
this	O
way	O
must	O
be	O
optimal	O
.	O
if	O
the	O
search	O
is	O
of	O
suﬃcient	O
depth	O
k	O
such	O
that	O
γk	O
is	O
very	O
small	O
,	O
then	O
the	O
actions	O
will	O
be	O
correspondingly	O
near	O
optimal	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
deeper	O
the	O
search	O
,	O
the	O
more	O
computation	O
is	O
required	O
,	O
usually	O
resulting	O
in	O
a	O
slower	O
response	O
time	O
.	O
a	O
good	O
example	O
is	O
provided	O
by	O
tesauro	O
’	O
s	O
grandmaster-level	O
backgammon	B
player	O
,	O
td-gammon	O
(	O
section	O
16.1	O
)	O
.	O
this	O
system	O
used	O
td	O
learning	O
to	O
learn	O
an	O
afterstate	O
value	B
function	I
through	O
many	O
games	O
of	O
self-play	O
,	O
using	O
a	O
form	O
of	O
heuristic	O
search	O
to	O
make	O
its	O
moves	O
.	O
as	O
a	O
model	O
,	O
td-gammon	O
used	O
a	O
priori	O
knowledge	O
of	O
the	O
probabilities	O
of	O
dice	O
rolls	O
and	O
the	O
assumption	O
that	O
the	O
opponent	O
always	O
selected	O
the	O
actions	O
that	O
td-gammon	O
rated	O
as	O
best	O
for	O
it	O
.	O
tesauro	O
found	O
that	O
the	O
deeper	O
the	O
heuristic	B
search	I
,	O
the	O
better	O
the	O
moves	O
made	O
by	O
td-gammon	O
,	O
but	O
the	O
longer	O
it	O
took	O
to	O
make	O
each	O
move	O
.	O
backgammon	B
has	O
a	O
large	O
branching	B
factor	I
,	O
yet	O
moves	O
must	O
be	O
made	O
within	O
a	O
few	O
seconds	O
.	O
it	O
was	O
only	O
feasible	O
to	O
search	O
ahead	O
selectively	O
a	O
few	O
steps	O
,	O
but	O
even	O
so	O
the	O
search	O
resulted	O
in	O
signiﬁcantly	O
better	O
action	B
selections	O
.	O
we	O
should	O
not	O
overlook	O
the	O
most	O
obvious	O
way	O
in	O
which	O
heuristic	B
search	I
focuses	O
up-	O
dates	O
:	O
on	O
the	O
current	O
state	B
.	O
much	O
of	O
the	O
eﬀectiveness	O
of	O
heuristic	O
search	O
is	O
due	O
to	O
its	O
search	O
tree	O
being	O
tightly	O
focused	O
on	O
the	O
states	O
and	O
actions	O
that	O
might	O
immediately	O
fol-	O
low	O
the	O
current	O
state	B
.	O
you	O
may	O
spend	O
more	O
of	O
your	O
life	O
playing	O
chess	B
than	O
checkers	O
,	O
but	O
when	O
you	O
play	O
checkers	O
,	O
it	O
pays	O
to	O
think	O
about	O
checkers	O
and	O
about	O
your	O
particular	O
checkers	O
position	O
,	O
your	O
likely	O
next	O
moves	O
,	O
and	O
successor	O
positions	O
.	O
no	O
matter	O
how	O
you	O
select	O
actions	O
,	O
it	O
is	O
these	O
states	O
and	O
actions	O
that	O
are	O
of	O
highest	O
priority	O
for	O
updates	O
and	O
where	O
you	O
most	O
urgently	O
want	O
your	O
approximate	B
value	O
function	O
to	O
be	O
accurate	O
.	O
not	O
only	O
should	O
your	O
computation	O
be	O
preferentially	O
devoted	O
to	O
imminent	O
events	O
,	O
but	O
so	O
should	O
your	O
limited	O
memory	O
resources	O
.	O
in	O
chess	O
,	O
for	O
example	O
,	O
there	O
are	O
far	O
too	O
many	O
possible	O
2there	O
are	O
interesting	O
exceptions	O
to	O
this	O
.	O
see	O
,	O
e.g.	O
,	O
pearl	O
(	O
1984	O
)	O
.	O
8.10.	O
rollout	B
algorithms	I
183	O
positions	O
to	O
store	O
distinct	O
value	B
estimates	O
for	O
each	O
of	O
them	O
,	O
but	O
chess	B
programs	O
based	O
on	O
heuristic	B
search	I
can	O
easily	O
store	O
distinct	O
estimates	O
for	O
the	O
millions	O
of	O
positions	O
they	O
encounter	O
looking	O
ahead	O
from	O
a	O
single	O
position	O
.	O
this	O
great	O
focusing	O
of	O
memory	O
and	O
computational	O
resources	O
on	O
the	O
current	O
decision	O
is	O
presumably	O
the	O
reason	O
why	O
heuristic	B
search	I
can	O
be	O
so	O
eﬀective	O
.	O
the	O
distribution	O
of	O
updates	O
can	O
be	O
altered	O
in	O
similar	O
ways	O
to	O
focus	O
on	O
the	O
current	O
state	B
and	O
its	O
likely	O
successors	O
.	O
as	O
a	O
limiting	O
case	O
we	O
might	O
use	O
exactly	O
the	O
methods	O
of	O
heuristic	O
search	O
to	O
construct	O
a	O
search	O
tree	O
,	O
and	O
then	O
perform	O
the	O
individual	O
,	O
one-step	O
updates	O
from	O
bottom	O
up	O
,	O
as	O
suggested	O
by	O
figure	O
8.9.	O
if	O
the	O
updates	O
are	O
ordered	O
in	O
this	O
way	O
and	O
a	O
tabular	O
representation	O
is	O
used	O
,	O
then	O
exactly	O
the	O
same	O
overall	O
update	O
would	O
be	O
achieved	O
as	O
in	O
depth-ﬁrst	O
heuristic	B
search	I
.	O
any	O
state-space	O
search	O
can	O
be	O
viewed	O
in	O
this	O
way	O
as	O
the	O
piecing	O
together	O
of	O
a	O
large	O
number	O
of	O
individual	O
one-step	O
updates	O
.	O
thus	O
,	O
the	O
performance	O
improvement	O
observed	O
with	O
deeper	O
searches	O
is	O
not	O
due	O
to	O
the	O
use	O
of	O
multistep	O
updates	O
as	O
such	O
.	O
instead	O
,	O
it	O
is	O
due	O
to	O
the	O
focus	O
and	O
concentration	O
of	O
updates	O
on	O
states	O
and	O
actions	O
immediately	O
downstream	O
from	O
the	O
current	O
state	B
.	O
by	O
devoting	O
a	O
large	O
amount	O
of	O
computation	O
speciﬁcally	O
relevant	O
to	O
the	O
candidate	O
actions	O
,	O
decision-time	O
planning	B
can	O
produce	O
better	O
decisions	O
than	O
can	O
be	O
produced	O
by	O
relying	O
on	O
unfocused	O
updates	O
.	O
figure	O
8.9	O
:	O
heuristic	B
search	I
can	O
be	O
implemented	O
as	O
a	O
sequence	O
of	O
one-step	O
updates	O
(	O
shown	O
here	O
outlined	O
in	O
blue	O
)	O
backing	O
up	O
values	O
from	O
the	O
leaf	O
nodes	O
toward	O
the	O
root	O
.	O
the	O
ordering	O
shown	O
here	O
is	O
for	O
a	O
selective	O
depth-ﬁrst	O
search	O
.	O
8.10	O
rollout	B
algorithms	I
rollout	O
algorithms	O
are	O
decision-time	O
planning	B
algorithms	O
based	O
on	O
monte	O
carlo	O
control	B
applied	O
to	O
simulated	O
trajectories	O
that	O
all	O
begin	O
at	O
the	O
current	O
environment	B
state	O
.	O
they	O
estimate	O
action	B
values	O
for	O
a	O
given	O
policy	O
by	O
averaging	O
the	O
returns	O
of	O
many	O
simulated	O
12345678910	O
184	O
chapter	O
8	O
:	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
trajectories	O
that	O
start	O
with	O
each	O
possible	O
action	B
and	O
then	O
follow	O
the	O
given	O
policy	B
.	O
when	O
the	O
action-value	O
estimates	O
are	O
considered	O
to	O
be	O
accurate	O
enough	O
,	O
the	O
action	B
(	O
or	O
one	O
of	O
the	O
actions	O
)	O
having	O
the	O
highest	O
estimated	O
value	B
is	O
executed	O
,	O
after	O
which	O
the	O
process	O
is	O
carried	O
out	O
anew	O
from	O
the	O
resulting	O
next	O
state	B
.	O
as	O
explained	O
by	O
tesauro	O
and	O
galperin	O
(	O
1997	O
)	O
,	O
who	O
experimented	O
with	O
rollout	O
algorithms	O
for	O
playing	O
backgammon	B
,	O
the	O
term	O
“	O
rollout	O
”	O
comes	O
from	O
estimating	O
the	O
value	B
of	O
a	O
backgammon	B
position	O
by	O
playing	O
out	O
,	O
i.e.	O
,	O
“	O
rolling	O
out	O
,	O
”	O
the	O
position	O
many	O
times	O
to	O
the	O
game	O
’	O
s	O
end	O
with	O
randomly	O
generated	O
sequences	O
of	O
dice	O
rolls	O
,	O
where	O
the	O
moves	O
of	O
both	O
players	O
are	O
made	O
by	O
some	O
ﬁxed	O
policy	B
.	O
unlike	O
the	O
monte	O
carlo	O
control	B
algorithms	O
described	O
in	O
chapter	O
5	O
,	O
the	O
goal	B
of	O
a	O
rollout	O
algorithm	O
is	O
not	O
to	O
estimate	O
a	O
complete	O
optimal	O
action-value	O
function	O
,	O
q∗	O
,	O
or	O
a	O
complete	O
action-value	B
function	I
,	O
qπ	O
,	O
for	O
a	O
given	O
policy	O
π.	O
instead	O
,	O
they	O
produce	O
monte	O
carlo	O
estimates	O
of	O
action	O
values	O
only	O
for	O
each	O
current	O
state	B
and	O
for	O
a	O
given	O
policy	O
usually	O
called	O
the	O
rollout	O
policy	O
.	O
as	O
decision-time	O
planning	B
algorithms	O
,	O
rollout	B
algorithms	I
make	O
immediate	O
use	O
of	O
these	O
action-value	O
estimates	O
,	O
then	O
discard	O
them	O
.	O
this	O
makes	O
rollout	B
algorithms	I
relatively	O
simple	O
to	O
implement	O
because	O
there	O
is	O
no	O
need	O
to	O
sample	O
outcomes	O
for	O
every	O
state-action	O
pair	O
,	O
and	O
there	O
is	O
no	O
need	O
to	O
approximate	B
a	O
function	O
over	O
either	O
the	O
state	B
space	O
or	O
the	O
state-action	O
space	O
.	O
what	O
then	O
do	O
rollout	B
algorithms	I
accomplish	O
?	O
the	O
policy	B
improvement	I
theorem	O
de-	O
scribed	O
in	O
section	O
4.2	O
tells	O
us	O
that	O
given	O
any	O
two	O
policies	O
π	O
and	O
π	O
(	O
cid:48	O
)	O
that	O
are	O
identical	O
except	O
that	O
π	O
(	O
cid:48	O
)	O
(	O
s	O
)	O
=	O
a	O
(	O
cid:54	O
)	O
=	O
π	O
(	O
s	O
)	O
for	O
some	O
state	B
s	O
,	O
if	O
qπ	O
(	O
s	O
,	O
a	O
)	O
≥	O
vπ	O
(	O
s	O
)	O
,	O
then	O
policy	B
π	O
(	O
cid:48	O
)	O
is	O
as	O
good	O
as	O
,	O
or	O
better	O
,	O
than	O
π.	O
moreover	O
,	O
if	O
the	O
inequality	O
is	O
strict	O
,	O
then	O
π	O
(	O
cid:48	O
)	O
is	O
in	O
fact	O
better	O
than	O
π.	O
this	O
applies	O
to	O
rollout	B
algorithms	I
where	O
s	O
is	O
the	O
current	O
state	B
and	O
π	O
is	O
the	O
rollout	O
policy	O
.	O
averaging	O
the	O
returns	O
of	O
the	O
simulated	O
trajectories	O
produces	O
estimates	O
of	O
qπ	O
(	O
s	O
,	O
a	O
(	O
cid:48	O
)	O
)	O
for	O
each	O
action	B
a	O
(	O
cid:48	O
)	O
∈	O
a	O
(	O
s	O
)	O
.	O
then	O
the	O
policy	B
that	O
selects	O
an	O
action	B
in	O
s	O
that	O
maximizes	O
these	O
estimates	O
and	O
thereafter	O
follows	O
π	O
is	O
a	O
good	O
candidate	O
for	O
a	O
policy	O
that	O
improves	O
over	O
π.	O
the	O
result	O
is	O
like	O
one	O
step	O
of	O
the	O
policy-iteration	O
algorithm	O
of	O
dynamic	O
programming	O
discussed	O
in	O
section	O
4.3	O
(	O
though	O
it	O
is	O
more	O
like	O
one	O
step	O
of	O
asynchronous	O
value	B
iteration	I
described	O
in	O
section	O
4.5	O
because	O
it	O
changes	O
the	O
action	B
for	O
just	O
the	O
current	O
state	B
)	O
.	O
in	O
other	O
words	O
,	O
the	O
aim	O
of	O
a	O
rollout	O
algorithm	O
is	O
to	O
improve	O
upon	O
the	O
rollout	O
policy	O
;	O
not	O
to	O
ﬁnd	O
an	O
optimal	O
policy	O
.	O
experience	O
has	O
shown	O
that	O
rollout	B
algorithms	I
can	O
be	O
surprisingly	O
eﬀective	O
.	O
for	O
example	O
,	O
tesauro	O
and	O
galperin	O
(	O
1997	O
)	O
were	O
surprised	O
by	O
the	O
dramatic	O
improvements	O
in	O
backgammon	O
playing	O
ability	O
produced	O
by	O
the	O
rollout	O
method	O
.	O
in	O
some	O
applications	O
,	O
a	O
rollout	O
algorithm	O
can	O
produce	O
good	O
performance	O
even	O
if	O
the	O
rollout	O
policy	O
is	O
completely	O
random	O
.	O
but	O
the	O
performance	O
of	O
the	O
improved	O
policy	B
depends	O
on	O
properties	O
of	O
the	O
rollout	O
policy	O
and	O
the	O
ranking	O
of	O
actions	O
produced	O
by	O
the	O
monte	O
carlo	O
value	B
estimates	O
.	O
intuition	O
suggests	O
that	O
the	O
better	O
the	O
rollout	O
policy	O
and	O
the	O
more	O
accurate	O
the	O
value	B
estimates	O
,	O
the	O
better	O
the	O
policy	B
produced	O
by	O
a	O
rollout	O
algorithm	O
is	O
likely	O
be	O
(	O
but	O
see	O
gelly	O
and	O
silver	O
,	O
2007	O
)	O
.	O
this	O
involves	O
important	O
tradeoﬀs	O
because	O
better	O
rollout	O
policies	O
typically	O
mean	O
that	O
more	O
time	O
is	O
needed	O
to	O
simulate	O
enough	O
trajectories	O
to	O
obtain	O
good	O
value	B
estimates	O
.	O
as	O
decision-time	O
planning	B
methods	O
,	O
rollout	B
algorithms	I
usually	O
have	O
to	O
meet	O
strict	O
time	O
constraints	O
.	O
the	O
computation	O
time	O
needed	O
by	O
a	O
rollout	O
algorithm	O
depends	O
on	O
the	O
number	O
of	O
actions	O
that	O
have	O
to	O
be	O
evaluated	O
for	O
each	O
decision	O
,	O
the	O
number	O
of	O
time	O
steps	O
in	O
the	O
8.11.	O
monte	O
carlo	O
tree	O
search	O
185	O
simulated	O
trajectories	O
needed	O
to	O
obtain	O
useful	O
sample	O
returns	O
,	O
the	O
time	O
it	O
takes	O
the	O
rollout	O
policy	O
to	O
make	O
decisions	O
,	O
and	O
the	O
number	O
of	O
simulated	O
trajectories	O
needed	O
to	O
obtain	O
good	O
monte	O
carlo	O
action-value	O
estimates	O
.	O
balancing	O
these	O
factors	O
is	O
important	O
in	O
any	O
application	O
of	O
rollout	O
methods	O
,	O
though	O
there	O
are	O
several	O
ways	O
to	O
ease	O
the	O
challenge	O
.	O
because	O
the	O
monte	O
carlo	O
trials	O
are	O
inde-	O
pendent	O
of	O
one	O
another	O
,	O
it	O
is	O
possible	O
to	O
run	O
many	O
trials	O
in	O
parallel	O
on	O
separate	O
processors	O
.	O
another	O
approach	O
is	O
to	O
truncate	O
the	O
simulated	O
trajectories	O
short	O
of	O
complete	O
episodes	B
,	O
correcting	O
the	O
truncated	B
returns	O
by	O
means	O
of	O
a	O
stored	O
evaluation	O
function	O
(	O
which	O
brings	O
into	O
play	O
all	O
that	O
we	O
have	O
said	O
about	O
truncated	B
returns	O
and	O
updates	O
in	O
the	O
preceding	O
chapters	O
)	O
.	O
it	O
is	O
also	O
possible	O
,	O
as	O
tesauro	O
and	O
galperin	O
(	O
1997	O
)	O
suggest	O
,	O
to	O
monitor	O
the	O
monte	O
carlo	O
simulations	O
and	O
prune	O
away	O
candidate	O
actions	O
that	O
are	O
unlikely	O
to	O
turn	O
out	O
to	O
be	O
the	O
best	O
,	O
or	O
whose	O
values	O
are	O
close	O
enough	O
to	O
that	O
of	O
the	O
current	O
best	O
that	O
choosing	O
them	O
instead	O
would	O
make	O
no	O
real	O
diﬀerence	O
(	O
though	O
tesauro	O
and	O
galperin	O
point	O
out	O
that	O
this	O
would	O
complicate	O
a	O
parallel	O
implementation	O
)	O
.	O
we	O
do	O
not	O
ordinarily	O
think	O
of	O
rollout	O
algorithms	O
as	O
learning	O
algorithms	O
because	O
they	O
do	O
not	O
maintain	O
long-term	O
memories	O
of	O
values	O
or	O
policies	O
.	O
however	O
,	O
these	O
algorithms	O
take	O
advantage	O
of	O
some	O
of	O
the	O
features	O
of	O
reinforcement	O
learning	O
that	O
we	O
have	O
emphasized	O
in	O
this	O
book	O
.	O
as	O
instances	O
of	O
monte	O
carlo	O
control	B
,	O
they	O
estimate	O
action	B
values	O
by	O
averaging	O
the	O
returns	O
of	O
a	O
collection	O
of	O
sample	O
trajectories	O
,	O
in	O
this	O
case	O
trajectories	O
of	O
simulated	O
interactions	O
with	O
a	O
sample	O
model	O
of	O
the	O
environment	B
.	O
in	O
this	O
way	O
they	O
are	O
like	O
reinforce-	O
ment	O
learning	O
algorithms	O
in	O
avoiding	O
the	O
exhaustive	O
sweeps	B
of	O
dynamic	B
programming	I
by	O
trajectory	B
sampling	I
,	O
and	O
in	O
avoiding	O
the	O
need	O
for	O
distribution	O
models	O
by	O
relying	O
on	O
sample	O
,	O
instead	O
of	O
expected	O
,	O
updates	O
.	O
finally	O
,	O
rollout	B
algorithms	I
take	O
advantage	O
of	O
the	O
policy	B
improvement	I
property	O
by	O
acting	O
greedily	O
with	O
respect	O
to	O
the	O
estimated	O
action	B
values	O
.	O
8.11	O
monte	O
carlo	O
tree	O
search	O
monte	O
carlo	O
tree	O
search	O
(	O
mcts	O
)	O
is	O
a	O
recent	O
and	O
strikingly	O
successful	O
example	O
of	O
decision-time	O
planning	B
.	O
at	O
its	O
base	O
,	O
mcts	O
is	O
a	O
rollout	O
algorithm	O
as	O
described	O
above	O
,	O
but	O
enhanced	O
by	O
the	O
addition	O
of	O
a	O
means	O
for	O
accumulating	O
value	B
estimates	O
obtained	O
from	O
the	O
monte	O
carlo	O
simulations	O
in	O
order	O
to	O
successively	O
direct	O
simulations	O
toward	O
more	O
highly-	O
rewarding	O
trajectories	O
.	O
mcts	O
is	O
largely	O
responsible	O
for	O
the	O
improvement	O
in	O
computer	O
go	O
from	O
a	O
weak	O
amateur	O
level	O
in	O
2005	O
to	O
a	O
grandmaster	O
level	O
(	O
6	O
dan	O
or	O
more	O
)	O
in	O
2015.	O
many	O
variations	O
of	O
the	O
basic	O
algorithm	O
have	O
been	O
developed	O
,	O
including	O
a	O
variant	O
that	O
we	O
discuss	O
in	O
section	O
16.6	O
that	O
was	O
critical	O
for	O
the	O
stunning	O
2016	O
victories	O
of	O
the	O
program	O
alphago	O
over	O
an	O
18-time	O
world	O
champion	O
go	O
player	O
.	O
mcts	O
has	O
proved	O
to	O
be	O
eﬀective	O
in	O
a	O
wide	O
variety	O
of	O
competitive	O
settings	O
,	O
including	O
general	O
game	O
playing	O
(	O
e.g.	O
,	O
see	O
finnsson	O
and	O
bj¨ornsson	O
,	O
2008	O
;	O
genesereth	O
and	O
thielscher	O
,	O
2014	O
)	O
,	O
but	O
it	O
is	O
not	O
limited	O
to	O
games	O
;	O
it	O
can	O
be	O
eﬀective	O
for	O
single-agent	O
sequential	O
decision	O
problems	O
if	O
there	O
is	O
an	O
environment	B
model	O
simple	O
enough	O
for	O
fast	O
multistep	O
simulation	O
.	O
mcts	O
is	O
executed	O
after	O
encountering	O
each	O
new	O
state	B
to	O
select	O
the	O
agent	O
’	O
s	O
action	B
for	O
that	O
state	B
;	O
it	O
is	O
executed	O
again	O
to	O
select	O
the	O
action	B
for	O
the	O
next	O
state	B
,	O
and	O
so	O
on	O
.	O
as	O
in	O
a	O
rollout	O
algorithm	O
,	O
each	O
execution	O
is	O
an	O
iterative	B
process	O
that	O
simulates	O
many	O
trajectories	O
186	O
chapter	O
8	O
:	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
starting	O
from	O
the	O
current	O
state	B
and	O
running	O
to	O
a	O
terminal	O
state	B
(	O
or	O
until	O
discounting	B
makes	O
any	O
further	O
reward	O
negligible	O
as	O
a	O
contribution	O
to	O
the	O
return	B
)	O
.	O
the	O
core	O
idea	O
of	O
mcts	O
is	O
to	O
successively	O
focus	O
multiple	O
simulations	O
starting	O
at	O
the	O
current	O
state	B
by	O
extending	O
the	O
initial	O
portions	O
of	O
trajectories	O
that	O
have	O
received	O
high	O
evaluations	O
from	O
earlier	O
simulations	O
.	O
mcts	O
does	O
not	O
have	O
to	O
retain	O
approximate	B
value	O
functions	O
or	O
policies	O
from	O
one	O
action	B
selection	O
to	O
the	O
next	O
,	O
though	O
in	O
many	O
implementations	O
it	O
retains	O
selected	O
action	B
values	O
likely	O
to	O
be	O
useful	O
for	O
its	O
next	O
execution	O
.	O
for	O
the	O
most	O
part	O
,	O
the	O
actions	O
in	O
the	O
simulated	O
trajectories	O
are	O
generated	O
using	O
a	O
simple	O
policy	B
,	O
usually	O
called	O
a	O
rollout	O
policy	O
as	O
it	O
is	O
for	O
simpler	O
rollout	B
algorithms	I
.	O
when	O
both	O
the	O
rollout	O
policy	O
and	O
the	O
model	O
do	O
not	O
require	O
a	O
lot	O
of	O
computation	O
,	O
many	O
simulated	O
trajectories	O
can	O
be	O
generated	O
in	O
a	O
short	O
period	O
of	O
time	O
.	O
as	O
in	O
any	O
tabular	O
monte	O
carlo	O
method	O
,	O
the	O
value	B
of	O
a	O
state–action	O
pair	O
is	O
estimated	O
as	O
the	O
average	O
of	O
the	O
(	O
simulated	O
)	O
returns	O
from	O
that	O
pair	O
.	O
monte	O
carlo	O
value	B
estimates	O
are	O
maintained	O
only	O
for	O
the	O
subset	O
of	O
state–action	O
pairs	O
that	O
are	O
most	O
likely	O
to	O
be	O
reached	O
in	O
a	O
few	O
steps	O
,	O
which	O
form	O
a	O
tree	O
rooted	O
at	O
the	O
current	O
state	B
,	O
as	O
illustrated	O
in	O
figure	O
8.10.	O
mcts	O
incrementally	O
extends	O
the	O
tree	O
by	O
adding	O
nodes	O
representing	O
states	O
that	O
look	O
promising	O
based	O
on	O
the	O
results	O
of	O
the	O
simulated	O
trajectories	O
.	O
any	O
simulated	O
trajectory	O
will	O
pass	O
through	O
the	O
tree	O
and	O
then	O
exit	O
it	O
at	O
some	O
leaf	O
node	O
.	O
outside	O
the	O
tree	O
and	O
at	O
the	O
leaf	O
nodes	O
the	O
rollout	O
policy	O
is	O
used	O
for	O
action	O
selections	O
,	O
but	O
at	O
the	O
states	O
inside	O
the	O
tree	O
something	O
better	O
is	O
possible	O
.	O
for	O
these	O
states	O
we	O
have	O
value	B
estimates	O
for	O
of	O
at	O
least	O
some	O
of	O
the	O
actions	O
,	O
so	O
we	O
can	O
pick	O
among	O
them	O
using	O
an	O
informed	O
policy	B
,	O
called	O
the	O
tree	O
policy	B
,	O
that	O
balances	O
exploration	O
and	O
exploitation	O
.	O
for	O
example	O
,	O
the	O
tree	O
policy	B
could	O
select	O
actions	O
using	O
an	O
ε-greedy	O
or	O
ucb	O
selection	O
rule	O
(	O
chapter	O
2	O
)	O
.	O
in	O
more	O
detail	O
,	O
each	O
iteration	O
of	O
a	O
basic	O
version	O
of	O
mcts	O
consists	O
of	O
the	O
following	O
four	O
steps	O
as	O
illustrated	O
in	O
figure	O
8.10	O
:	O
1.	O
selection	O
.	O
starting	O
at	O
the	O
root	O
node	O
,	O
a	O
tree	O
policy	B
based	O
on	O
the	O
action	B
values	O
attached	O
to	O
the	O
edges	O
of	O
the	O
tree	O
traverses	O
the	O
tree	O
to	O
select	O
a	O
leaf	O
node	O
.	O
2.	O
expansion	O
.	O
on	O
some	O
iterations	O
(	O
depending	O
on	O
details	O
of	O
the	O
application	O
)	O
,	O
the	O
tree	O
is	O
expanded	O
from	O
the	O
selected	O
leaf	O
node	O
by	O
adding	O
one	O
or	O
more	O
child	O
nodes	O
reached	O
from	O
the	O
selected	O
node	O
via	O
unexplored	O
actions	O
.	O
3.	O
simulation	O
.	O
from	O
the	O
selected	O
node	O
,	O
or	O
from	O
one	O
of	O
its	O
newly-added	O
child	O
nodes	O
(	O
if	O
any	O
)	O
,	O
simulation	O
of	O
a	O
complete	O
episode	O
is	O
run	O
with	O
actions	O
selected	O
by	O
the	O
rollout	O
policy	O
.	O
the	O
result	O
is	O
a	O
monte	O
carlo	O
trial	O
with	O
actions	O
selected	O
ﬁrst	O
by	O
the	O
tree	O
policy	B
and	O
beyond	O
the	O
tree	O
by	O
the	O
rollout	O
policy	O
.	O
4.	O
backup	O
.	O
the	O
return	B
generated	O
by	O
the	O
simulated	O
episode	O
is	O
backed	O
up	O
to	O
update	O
,	O
or	O
to	O
initialize	O
,	O
the	O
action	B
values	O
attached	O
to	O
the	O
edges	O
of	O
the	O
tree	O
traversed	O
by	O
the	O
tree	O
policy	B
in	O
this	O
iteration	O
of	O
mcts	O
.	O
no	O
values	O
are	O
saved	O
for	O
the	O
states	O
and	O
actions	O
visited	O
by	O
the	O
rollout	O
policy	O
beyond	O
the	O
tree	O
.	O
figure	O
8.10	O
illustrates	O
this	O
by	O
showing	O
a	O
backup	O
from	O
the	O
terminal	O
state	B
of	O
the	O
simulated	O
trajectory	O
directly	O
to	O
the	O
state–action	O
node	O
in	O
the	O
tree	O
where	O
the	O
rollout	O
policy	O
began	O
(	O
though	O
in	O
general	O
,	O
the	O
entire	O
return	B
over	O
the	O
simulated	O
trajectory	O
is	O
backed	O
up	O
to	O
this	O
state–action	O
node	O
)	O
.	O
8.11.	O
monte	O
carlo	O
tree	O
search	O
187	O
figure	O
8.10	O
:	O
monte	O
carlo	O
tree	O
search	O
.	O
when	O
the	O
environment	B
changes	O
to	O
a	O
new	O
state	B
,	O
mcts	O
executes	O
as	O
many	O
iterations	O
as	O
possible	O
before	O
an	O
action	B
needs	O
to	O
be	O
selected	O
,	O
incrementally	O
building	O
a	O
tree	O
whose	O
root	O
node	O
represents	O
the	O
current	O
state	B
.	O
each	O
iteration	O
consists	O
of	O
the	O
four	O
operations	O
selection	O
,	O
expansion	O
(	O
though	O
possibly	O
skipped	O
on	O
some	O
iterations	O
)	O
,	O
simulation	O
,	O
and	O
backup	O
,	O
as	O
explained	O
in	O
the	O
text	O
and	O
illustrated	O
by	O
the	O
bold	O
arrows	O
in	O
the	O
trees	O
.	O
adapted	O
from	O
chaslot	O
,	O
bakkes	O
,	O
szita	O
,	O
and	O
spronck	O
(	O
2008	O
)	O
.	O
mcts	O
continues	O
executing	O
these	O
four	O
steps	O
,	O
starting	O
each	O
time	O
at	O
the	O
tree	O
’	O
s	O
root	O
node	O
,	O
until	O
no	O
more	O
time	O
is	O
left	O
,	O
or	O
some	O
other	O
computational	O
resource	O
is	O
exhausted	O
.	O
then	O
,	O
ﬁnally	O
,	O
an	O
action	B
from	O
the	O
root	O
node	O
(	O
which	O
still	O
represents	O
the	O
current	O
state	B
of	O
the	O
environment	B
)	O
is	O
selected	O
according	O
to	O
some	O
mechanism	O
that	O
depends	O
on	O
the	O
accumulated	O
statistics	O
in	O
the	O
tree	O
;	O
for	O
example	O
,	O
it	O
may	O
be	O
an	O
action	B
having	O
the	O
largest	O
action	B
value	O
of	O
all	O
the	O
actions	O
available	O
from	O
the	O
root	O
state	O
,	O
or	O
perhaps	O
the	O
action	B
with	O
the	O
largest	O
visit	O
count	O
to	O
avoid	O
selecting	O
outliers	O
.	O
this	O
is	O
the	O
action	B
mcts	O
actually	O
selects	O
.	O
after	O
the	O
environment	B
transitions	O
to	O
a	O
new	O
state	B
,	O
mcts	O
is	O
run	O
again	O
,	O
sometimes	O
starting	O
with	O
a	O
tree	O
of	O
a	O
single	O
root	O
node	O
representing	O
the	O
new	O
state	B
,	O
but	O
often	O
starting	O
with	O
a	O
tree	O
containing	O
any	O
descendants	O
of	O
this	O
node	O
left	O
over	O
from	O
the	O
tree	O
constructed	O
by	O
the	O
previous	O
execution	O
of	O
mcts	O
;	O
all	O
the	O
remaining	O
nodes	O
are	O
discarded	O
,	O
along	O
with	O
the	O
action	B
values	O
associated	O
with	O
them	O
.	O
mcts	O
was	O
ﬁrst	O
proposed	O
to	O
select	O
moves	O
in	O
programs	O
playing	O
two-person	O
competitive	O
games	O
,	O
such	O
as	O
go	O
.	O
for	O
game	O
playing	O
,	O
each	O
simulated	O
episode	O
is	O
one	O
complete	O
play	O
of	O
the	O
game	O
in	O
which	O
both	O
players	O
select	O
actions	O
by	O
the	O
tree	O
and	O
rollout	O
policies	O
.	O
section	O
16.6	O
describes	O
an	O
extension	O
of	O
mcts	O
used	O
in	O
the	O
alphago	O
program	O
that	O
combines	O
the	O
monte	O
selectionsimulationexpansionbackuprepeat	O
while	O
time	O
remains	O
tree	O
policyrolloutpolicy	O
188	O
chapter	O
8	O
:	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
carlo	O
evaluations	O
of	O
mcts	O
with	O
action	O
values	O
learned	O
by	O
a	O
deep	O
ann	O
via	O
self-play	O
reinforcement	B
learning	I
.	O
relating	O
mcts	O
to	O
the	O
reinforcement	B
learning	I
principles	O
we	O
describe	O
in	O
this	O
book	O
provides	O
some	O
insight	O
into	O
how	O
it	O
achieves	O
such	O
impressive	O
results	O
.	O
at	O
its	O
base	O
,	O
mcts	O
is	O
a	O
decision-time	O
planning	B
algorithm	O
based	O
on	O
monte	O
carlo	O
control	B
applied	O
to	O
simulations	O
that	O
start	O
from	O
the	O
root	O
state	O
;	O
that	O
is	O
,	O
it	O
is	O
a	O
kind	O
of	O
rollout	O
algorithm	O
as	O
described	O
in	O
the	O
previous	O
section	O
.	O
it	O
therefore	O
beneﬁts	O
from	O
online	B
,	O
incremental	O
,	O
sample-based	O
value	B
estimation	O
and	O
policy	O
improvement	O
.	O
beyond	O
this	O
,	O
it	O
saves	O
action-value	O
estimates	O
attached	O
to	O
the	O
tree	O
edges	O
and	O
updates	O
them	O
using	O
reinforcement	B
learning	I
’	O
s	O
sample	O
updates	O
.	O
this	O
has	O
the	O
eﬀect	O
of	O
focusing	O
the	O
monte	O
carlo	O
trials	O
on	O
trajectories	O
whose	O
initial	O
segments	O
are	O
common	O
to	O
high-return	O
trajectories	O
previously	O
simulated	O
.	O
further	O
,	O
by	O
incrementally	O
expanding	O
the	O
tree	O
,	O
mcts	O
eﬀectively	O
grows	O
a	O
lookup	O
table	O
to	O
store	O
a	O
partial	O
action-value	B
function	I
,	O
with	O
memory	O
allocated	O
to	O
the	O
estimated	O
values	O
of	O
state–action	O
pairs	O
visited	O
in	O
the	O
initial	O
segments	O
of	O
high-yielding	O
sample	O
trajectories	O
.	O
mcts	O
thus	O
avoids	O
the	O
problem	O
of	O
globally	O
approximating	O
an	O
action-value	B
function	I
while	O
it	O
retains	O
the	O
beneﬁt	O
of	O
using	O
past	O
experience	O
to	O
guide	O
exploration	O
.	O
the	O
striking	O
success	O
of	O
decision-time	O
planning	B
by	O
mcts	O
has	O
deeply	O
inﬂuenced	O
artiﬁcial	B
intelligence	I
,	O
and	O
many	O
researchers	O
are	O
studying	O
modiﬁcations	O
and	O
extensions	O
of	O
the	O
basic	O
procedure	O
for	O
use	O
in	O
both	O
games	O
and	O
single-agent	O
applications	O
.	O
8.12	O
summary	O
of	O
the	O
chapter	O
planning	B
requires	O
a	O
model	B
of	I
the	I
environment	I
.	O
a	O
distribution	O
model	O
consists	O
of	O
the	O
probabilities	O
of	O
next	O
states	O
and	O
rewards	O
for	O
possible	O
actions	O
;	O
a	O
sample	O
model	O
produces	O
single	O
transitions	O
and	O
rewards	O
generated	O
according	O
to	O
these	O
probabilities	O
.	O
dynamic	O
pro-	O
gramming	O
requires	O
a	O
distribution	O
model	O
because	O
it	O
uses	O
expected	O
updates	O
,	O
which	O
involve	O
computing	O
expectations	O
over	O
all	O
the	O
possible	O
next	O
states	O
and	O
rewards	O
.	O
a	O
sample	O
model	O
,	O
on	O
the	O
other	O
hand	O
,	O
is	O
what	O
is	O
needed	O
to	O
simulate	O
interacting	O
with	O
the	O
environment	B
during	O
which	O
sample	O
updates	O
,	O
like	O
those	O
used	O
by	O
many	O
reinforcement	B
learning	I
algorithms	O
,	O
can	O
be	O
used	O
.	O
sample	O
models	O
are	O
generally	O
much	O
easier	O
to	O
obtain	O
than	O
distribution	B
models	I
.	O
we	O
have	O
presented	O
a	O
perspective	O
emphasizing	O
the	O
surprisingly	O
close	O
relationships	O
be-	O
tween	O
planning	B
optimal	O
behavior	O
and	O
learning	O
optimal	O
behavior	O
.	O
both	O
involve	O
estimating	O
the	O
same	O
value	B
functions	O
,	O
and	O
in	O
both	O
cases	O
it	O
is	O
natural	O
to	O
update	O
the	O
estimates	O
incre-	O
mentally	O
,	O
in	O
a	O
long	O
series	O
of	O
small	O
backing-up	O
operations	O
.	O
this	O
makes	O
it	O
straightforward	O
to	O
integrate	O
learning	O
and	O
planning	B
processes	O
simply	O
by	O
allowing	O
both	O
to	O
update	O
the	O
same	O
estimated	O
value	B
function	I
.	O
in	O
addition	O
,	O
any	O
of	O
the	O
learning	O
methods	O
can	O
be	O
converted	O
into	O
planning	B
methods	O
simply	O
by	O
applying	O
them	O
to	O
simulated	O
(	O
model-generated	O
)	O
expe-	O
rience	O
rather	O
than	O
to	O
real	O
experience	O
.	O
in	O
this	O
case	O
learning	O
and	O
planning	B
become	O
even	O
more	O
similar	O
;	O
they	O
are	O
possibly	O
identical	O
algorithms	O
operating	O
on	O
two	O
diﬀerent	O
sources	O
of	O
experience	O
.	O
it	O
is	O
straightforward	O
to	O
integrate	O
incremental	O
planning	O
methods	O
with	O
acting	O
and	O
model-	O
learning	O
.	O
planning	B
,	O
acting	O
,	O
and	O
model-learning	O
interact	O
in	O
a	O
circular	O
fashion	O
(	O
as	O
in	O
the	O
diagram	O
on	O
page	O
162	O
)	O
,	O
each	O
producing	O
what	O
the	O
other	O
needs	O
to	O
improve	O
;	O
no	O
other	O
inter-	O
action	B
among	O
them	O
is	O
either	O
required	O
or	O
prohibited	O
.	O
the	O
most	O
natural	O
approach	O
is	O
for	O
all	O
8.13.	O
summary	O
of	O
part	O
i	O
:	O
dimensions	O
189	O
processes	O
to	O
proceed	O
asynchronously	O
and	O
in	O
parallel	O
.	O
if	O
the	O
processes	O
must	O
share	O
com-	O
putational	O
resources	O
,	O
then	O
the	O
division	O
can	O
be	O
handled	O
almost	O
arbitrarily—by	O
whatever	O
organization	O
is	O
most	O
convenient	O
and	O
eﬃcient	O
for	O
the	O
task	O
at	O
hand	O
.	O
in	O
this	O
chapter	O
we	O
have	O
touched	O
upon	O
a	O
number	O
of	O
dimensions	O
of	O
variation	O
among	O
state-space	O
planning	B
methods	O
.	O
one	O
dimension	O
is	O
the	O
variation	O
in	O
the	O
size	O
of	O
updates	O
.	O
the	O
smaller	O
the	O
updates	O
,	O
the	O
more	O
incremental	O
the	O
planning	B
methods	O
can	O
be	O
.	O
among	O
the	O
smallest	O
updates	O
are	O
one-step	O
sample	O
updates	O
,	O
as	O
in	O
dyna	O
.	O
another	O
important	O
dimension	O
is	O
the	O
distribution	O
of	O
updates	O
,	O
that	O
is	O
,	O
of	O
the	O
focus	O
of	O
search	O
.	O
prioritized	B
sweeping	I
focuses	O
backward	O
on	O
the	O
predecessors	O
of	O
states	O
whose	O
values	O
have	O
recently	O
changed	O
.	O
on-policy	O
trajectory	O
sampling	O
focuses	O
on	O
states	O
or	O
state–action	O
pairs	O
that	O
the	O
agent	O
is	O
likely	O
to	O
encounter	O
when	O
controlling	O
its	O
environment	B
.	O
this	O
can	O
allow	O
computation	O
to	O
skip	O
over	O
parts	O
of	O
the	O
state	B
space	O
that	O
are	O
irrelevant	O
to	O
the	O
prediction	B
or	O
control	B
problem	O
.	O
real-	O
time	O
dynamic	B
programming	I
,	O
an	O
on-policy	O
trajectory	O
sampling	O
version	O
of	O
value	O
iteration	O
,	O
illustrates	O
some	O
of	O
the	O
advantages	O
this	O
strategy	O
has	O
over	O
conventional	O
sweep-based	O
policy	B
iteration	I
.	O
planning	B
can	O
also	O
focus	O
forward	O
from	O
pertinent	O
states	O
,	O
such	O
as	O
states	O
actually	O
en-	O
countered	O
during	O
an	O
agent-environment	O
interaction	O
.	O
the	O
most	O
important	O
form	O
of	O
this	O
is	O
when	O
planning	B
is	O
done	O
at	O
decision	O
time	O
,	O
that	O
is	O
,	O
as	O
part	O
of	O
the	O
action-selection	O
process	O
.	O
classical	O
heuristic	O
search	O
as	O
studied	O
in	O
artiﬁcial	O
intelligence	O
is	O
an	O
example	O
of	O
this	O
.	O
other	O
examples	O
are	O
rollout	B
algorithms	I
and	O
monte	O
carlo	O
tree	O
search	O
that	O
beneﬁt	O
from	O
online	B
,	O
incremental	O
,	O
sample-based	O
value	B
estimation	O
and	O
policy	O
improvement	O
.	O
8.13	O
summary	O
of	O
part	O
i	O
:	O
dimensions	O
this	O
chapter	O
concludes	O
part	O
i	O
of	O
this	O
book	O
.	O
in	O
it	O
we	O
have	O
tried	O
to	O
present	O
reinforcement	B
learning	I
not	O
as	O
a	O
collection	O
of	O
individual	O
methods	O
,	O
but	O
as	O
a	O
coherent	O
set	O
of	O
ideas	O
cutting	O
across	O
methods	O
.	O
each	O
idea	O
can	O
be	O
viewed	O
as	O
a	O
dimension	O
along	O
which	O
methods	O
vary	O
.	O
the	O
set	O
of	O
such	O
dimensions	O
spans	O
a	O
large	O
space	O
of	O
possible	O
methods	O
.	O
by	O
exploring	O
this	O
space	O
at	O
the	O
level	O
of	O
dimensions	O
we	O
hope	O
to	O
obtain	O
the	O
broadest	O
and	O
most	O
lasting	O
understanding	O
.	O
in	O
this	O
section	O
we	O
use	O
the	O
concept	O
of	O
dimensions	O
in	O
method	O
space	O
to	O
recapitulate	O
the	O
view	O
of	O
reinforcement	O
learning	O
developed	O
so	O
far	O
in	O
this	O
book	O
.	O
all	O
of	O
the	O
methods	O
we	O
have	O
explored	O
so	O
far	O
in	O
this	O
book	O
have	O
three	O
key	O
ideas	O
in	O
common	O
:	O
ﬁrst	O
,	O
they	O
all	O
seek	O
to	O
estimate	O
value	B
functions	O
;	O
second	O
,	O
they	O
all	O
operate	O
by	O
backing	O
up	O
values	O
along	O
actual	O
or	O
possible	O
state	B
trajectories	O
;	O
and	O
third	O
,	O
they	O
all	O
follow	O
the	O
general	O
strategy	O
of	O
generalized	O
policy	B
iteration	I
(	O
gpi	O
)	O
,	O
meaning	O
that	O
they	O
maintain	O
an	O
approximate	B
value	O
function	O
and	O
an	O
approximate	B
policy	O
,	O
and	O
they	O
continually	O
try	O
to	O
improve	O
each	O
on	O
the	O
basis	O
of	O
the	O
other	O
.	O
these	O
three	O
ideas	O
are	O
central	O
to	O
the	O
subjects	O
covered	O
in	O
this	O
book	O
.	O
we	O
suggest	O
that	O
value	B
functions	O
,	O
backing	O
up	O
value	B
updates	O
,	O
and	O
gpi	O
are	O
powerful	O
organizing	O
principles	O
potentially	O
relevant	O
to	O
any	O
model	O
of	O
intelligence	O
,	O
whether	O
artiﬁcial	O
or	O
natural	O
.	O
two	O
of	O
the	O
most	O
important	O
dimensions	O
along	O
which	O
the	O
methods	O
vary	O
are	O
shown	O
in	O
figure	O
8.11.	O
these	O
dimensions	O
have	O
to	O
do	O
with	O
the	O
kind	O
of	B
update	I
used	O
to	O
improve	O
the	O
value	B
function	I
.	O
the	O
horizontal	O
dimension	O
is	O
whether	O
they	O
are	O
sample	O
updates	O
(	O
based	O
on	O
a	O
sample	O
trajectory	O
)	O
or	O
expected	O
updates	O
(	O
based	O
on	O
a	O
distribution	O
of	O
possible	O
trajectories	O
)	O
.	O
190	O
chapter	O
8	O
:	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
figure	O
8.11	O
:	O
a	O
slice	O
through	O
the	O
space	O
of	O
reinforcement	O
learning	O
methods	O
,	O
highlighting	O
the	O
two	O
of	O
the	O
most	O
important	O
dimensions	O
explored	O
in	O
part	O
i	O
of	O
this	O
book	O
:	O
the	O
depth	O
and	O
width	O
of	O
the	O
updates	O
.	O
expected	O
updates	O
require	O
a	O
distribution	O
model	O
,	O
whereas	O
sample	O
updates	O
need	O
only	O
a	O
sample	O
model	O
,	O
or	O
can	O
be	O
done	O
from	O
actual	O
experience	O
with	O
no	O
model	O
at	O
all	O
(	O
another	O
dimension	O
of	O
variation	O
)	O
.	O
the	O
vertical	O
dimension	O
of	O
figure	O
8.11	O
corresponds	O
to	O
the	O
depth	O
of	O
updates	O
,	O
that	O
is	O
,	O
to	O
the	O
degree	O
of	O
bootstrapping	O
.	O
at	O
three	O
of	O
the	O
four	O
corners	O
of	O
the	O
space	O
are	O
the	O
three	O
primary	O
methods	O
for	O
estimating	O
values	O
:	O
dynamic	B
programming	I
,	O
td	O
,	O
and	O
monte	O
carlo	O
.	O
along	O
the	O
left	O
edge	O
of	O
the	O
space	O
are	O
the	O
sample-update	O
methods	O
,	O
ranging	O
from	O
one-step	O
td	O
updates	O
to	O
full-return	O
monte	O
carlo	O
updates	O
.	O
between	O
these	O
is	O
a	O
spectrum	O
including	O
methods	O
based	O
on	O
n-step	B
updates	O
(	O
and	O
in	O
chapter	O
12	O
we	O
will	O
extend	O
this	O
to	O
mixtures	O
of	O
n-step	O
updates	O
such	O
as	O
the	O
λ-updates	O
implemented	O
by	O
eligibility	B
traces	I
)	O
.	O
dynamic	B
programming	I
methods	O
are	O
shown	O
in	O
the	O
extreme	O
upper-right	O
corner	O
of	O
the	O
space	O
because	O
they	O
involve	O
one-step	O
expected	O
updates	O
.	O
the	O
lower-right	O
corner	O
is	O
the	O
extreme	O
case	O
of	O
expected	O
updates	O
so	O
deep	O
that	O
they	O
run	O
all	O
the	O
way	O
to	O
terminal	O
states	O
(	O
or	O
,	O
in	O
a	O
continuing	O
task	O
,	O
until	O
discounting	B
has	O
reduced	O
the	O
contribution	O
of	O
any	O
further	O
rewards	O
to	O
a	O
negligible	O
level	O
)	O
.	O
this	O
is	O
the	O
case	O
of	O
exhaustive	O
search	O
.	O
intermediate	O
methods	O
along	O
this	O
dimension	O
include	O
heuristic	B
search	I
and	O
related	O
methods	O
that	O
search	O
and	O
update	O
up	O
to	O
a	O
limited	O
depth	O
,	O
perhaps	O
selectively	O
.	O
there	O
are	O
also	O
methods	O
that	O
are	O
intermediate	O
widthof	O
updatedepth	O
(	O
length	O
)	O
of	O
updatetemporal-differencelearningdynamicprogrammingmontecarlo	O
...	O
exhaustivesearch	O
8.13.	O
summary	O
of	O
part	O
i	O
:	O
dimensions	O
191	O
along	O
the	O
horizontal	O
dimension	O
.	O
these	O
include	O
methods	O
that	O
mix	O
expected	O
and	O
sample	O
updates	O
,	O
as	O
well	O
as	O
the	O
possibility	O
of	O
methods	O
that	O
mix	O
samples	O
and	O
distributions	O
within	O
a	O
single	O
update	O
.	O
the	O
interior	O
of	O
the	O
square	O
is	O
ﬁlled	O
in	O
to	O
represent	O
the	O
space	O
of	O
all	O
such	O
intermediate	O
methods	O
.	O
a	O
third	O
dimension	O
that	O
we	O
have	O
emphasized	O
in	O
this	O
book	O
is	O
the	O
binary	O
distinction	O
between	O
on-policy	O
and	O
oﬀ-policy	B
methods	I
.	O
in	O
the	O
former	O
case	O
,	O
the	O
agent	O
learns	O
the	O
value	B
function	I
for	O
the	O
policy	B
it	O
is	O
currently	O
following	O
,	O
whereas	O
in	O
the	O
latter	O
case	O
it	O
learns	O
the	O
value	B
function	I
for	O
the	O
policy	B
for	O
a	O
diﬀerent	O
policy	B
,	O
often	O
the	O
one	O
that	O
the	O
agent	O
currently	O
thinks	O
is	O
best	O
.	O
the	O
policy	B
generating	O
behavior	O
is	O
typically	O
diﬀerent	O
from	O
what	O
is	O
currently	O
thought	O
best	O
because	O
of	O
the	O
need	O
to	O
explore	O
.	O
this	O
third	O
dimension	O
might	O
be	O
visualized	O
as	O
perpendicular	O
to	O
the	O
plane	O
of	O
the	O
page	O
in	O
figure	O
8.11.	O
in	O
addition	O
to	O
the	O
three	O
dimensions	O
just	O
discussed	O
,	O
we	O
have	O
identiﬁed	O
a	O
number	O
of	O
others	O
throughout	O
the	O
book	O
:	O
deﬁnition	O
of	O
return	O
is	O
the	O
task	O
episodic	O
or	O
continuing	O
,	O
discounted	O
or	O
undiscounted	O
?	O
action	B
values	O
vs.	O
state	B
values	O
vs.	O
afterstate	O
values	O
what	O
kind	O
of	O
values	O
should	O
be	O
estimated	O
?	O
if	O
only	O
state	B
values	O
are	O
estimated	O
,	O
then	O
either	O
a	O
model	O
or	O
a	O
separate	O
policy	B
(	O
as	O
in	O
actor–critic	B
methods	O
)	O
is	O
required	O
for	O
action	O
selection	O
.	O
action	B
selection/exploration	O
how	O
are	O
actions	O
selected	O
to	O
ensure	O
a	O
suitable	O
trade-	O
oﬀ	O
between	O
exploration	O
and	O
exploitation	O
?	O
we	O
have	O
considered	O
only	O
the	O
simplest	O
ways	O
to	O
do	O
this	O
:	O
ε-greedy	O
,	O
optimistic	O
initialization	O
of	O
values	O
,	O
soft-max	B
,	O
and	O
upper	O
conﬁdence	O
bound	O
.	O
synchronous	O
vs.	O
asynchronous	O
are	O
the	O
updates	O
for	O
all	O
states	O
performed	O
simultane-	O
ously	O
or	O
one	O
by	O
one	O
in	O
some	O
order	O
?	O
real	O
vs.	O
simulated	O
should	O
one	O
update	O
based	O
on	O
real	O
experience	O
or	O
simulated	O
experi-	O
ence	O
?	O
if	O
both	O
,	O
how	O
much	O
of	O
each	O
?	O
location	O
of	O
updates	O
what	O
states	O
or	O
state–action	O
pairs	O
should	O
be	O
updated	O
?	O
model-	O
free	O
methods	O
can	O
choose	O
only	O
among	O
the	O
states	O
and	O
state–action	O
pairs	O
actually	O
encountered	O
,	O
but	O
model-based	O
methods	O
can	O
choose	O
arbitrarily	O
.	O
there	O
are	O
many	O
possibilities	O
here	O
.	O
timing	O
of	O
updates	O
should	O
updates	O
be	O
done	O
as	O
part	O
of	O
selecting	O
actions	O
,	O
or	O
only	O
after-	O
ward	O
?	O
memory	O
for	O
updates	O
how	O
long	O
should	O
updated	O
values	O
be	O
retained	O
?	O
should	O
they	O
be	O
retained	O
permanently	O
,	O
or	O
only	O
while	O
computing	O
an	O
action	B
selection	O
,	O
as	O
in	O
heuristic	B
search	I
?	O
of	O
course	O
,	O
these	O
dimensions	O
are	O
neither	O
exhaustive	O
nor	O
mutually	O
exclusive	O
.	O
individual	O
algorithms	O
diﬀer	O
in	O
many	O
other	O
ways	O
as	O
well	O
,	O
and	O
many	O
algorithms	O
lie	O
in	O
several	O
places	O
along	O
several	O
dimensions	O
.	O
for	O
example	O
,	O
dyna	O
methods	O
use	O
both	O
real	O
and	O
simulated	O
experience	O
to	O
aﬀect	O
the	O
same	O
value	B
function	I
.	O
it	O
is	O
also	O
perfectly	O
sensible	O
to	O
maintain	O
multiple	O
value	B
functions	O
computed	O
in	O
diﬀerent	O
ways	O
or	O
over	O
diﬀerent	O
state	B
and	O
action	B
192	O
chapter	O
8	O
:	O
planning	B
and	O
learning	O
with	O
tabular	O
methods	O
representations	O
.	O
these	O
dimensions	O
do	O
,	O
however	O
,	O
constitute	O
a	O
coherent	O
set	O
of	O
ideas	O
for	O
describing	O
and	O
exploring	O
a	O
wide	O
space	O
of	O
possible	O
methods	O
.	O
the	O
most	O
important	O
dimension	O
not	O
mentioned	O
here	O
,	O
and	O
not	O
covered	O
in	O
part	O
i	O
of	O
this	O
book	O
,	O
is	O
that	O
of	O
function	O
approximation	O
.	O
function	B
approximation	I
can	O
be	O
viewed	O
as	O
an	O
orthogonal	O
spectrum	O
of	O
possibilities	O
ranging	O
from	O
tabular	O
methods	O
at	O
one	O
extreme	O
through	O
state	B
aggregation	I
,	O
a	O
variety	O
of	O
linear	O
methods	O
,	O
and	O
then	O
a	O
diverse	O
set	O
of	O
nonlinear	O
methods	O
.	O
this	O
dimension	O
is	O
explored	O
in	O
part	O
ii	O
.	O
bibliographical	O
and	O
historical	O
remarks	O
8.1	O
8.2	O
8.3	O
8.4	O
the	O
overall	O
view	O
of	O
planning	O
and	O
learning	O
presented	O
here	O
has	O
developed	O
gradually	O
over	O
a	O
number	O
of	O
years	O
,	O
in	O
part	O
by	O
the	O
authors	O
(	O
sutton	O
,	O
1990	O
,	O
1991a	O
,	O
1991b	O
;	O
barto	O
,	O
bradtke	O
,	O
and	O
singh	O
,	O
1991	O
,	O
1995	O
;	O
sutton	O
and	O
pinette	O
,	O
1985	O
;	O
sutton	O
and	O
barto	O
,	O
1981b	O
)	O
;	O
it	O
has	O
been	O
strongly	O
inﬂuenced	O
by	O
agre	O
and	O
chapman	O
(	O
1990	O
;	O
agre	O
1988	O
)	O
,	O
bertsekas	O
and	O
tsitsiklis	O
(	O
1989	O
)	O
,	O
singh	O
(	O
1993	O
)	O
,	O
and	O
others	O
.	O
the	O
authors	O
were	O
also	O
strongly	O
inﬂuenced	O
by	O
psychological	O
studies	O
of	O
latent	O
learning	O
(	O
tolman	O
,	O
1932	O
)	O
and	O
by	O
psychological	O
views	O
of	O
the	O
nature	O
of	O
thought	O
(	O
e.g.	O
,	O
galanter	O
and	O
gerstenhaber	O
,	O
1956	O
;	O
craik	O
,	O
1943	O
;	O
campbell	O
,	O
1960	O
;	O
dennett	O
,	O
1978	O
)	O
.	O
in	O
part	O
iii	O
of	O
the	O
book	O
,	O
section	O
14.6	O
relates	O
model-based	B
and	I
model-free	I
methods	I
to	O
psychological	O
theories	O
of	O
learning	O
and	O
behavior	O
,	O
and	O
section	O
15.11	O
discusses	O
ideas	O
about	O
how	O
the	O
brain	O
might	O
implement	O
these	O
types	O
of	O
methods	O
.	O
the	O
terms	O
direct	O
and	O
indirect	O
,	O
which	O
we	O
use	O
to	O
describe	O
diﬀerent	O
kinds	O
of	O
re-	O
inforcement	O
learning	O
,	O
are	O
from	O
the	O
adaptive	O
control	B
literature	O
(	O
e.g.	O
,	O
goodwin	O
and	O
sin	O
,	O
1984	O
)	O
,	O
where	O
they	O
are	O
used	O
to	O
make	O
the	O
same	O
kind	O
of	O
distinction	O
.	O
the	O
term	O
system	B
identiﬁcation	I
is	O
used	O
in	O
adaptive	O
control	B
for	O
what	O
we	O
call	O
model-	O
learning	O
(	O
e.g.	O
,	O
goodwin	O
and	O
sin	O
,	O
1984	O
;	O
ljung	O
and	O
s¨oderstrom	O
,	O
1983	O
;	O
young	O
,	O
1984	O
)	O
.	O
the	O
dyna	O
architecture	O
is	O
due	O
to	O
sutton	O
(	O
1990	O
)	O
,	O
and	O
the	O
results	O
in	O
this	O
and	O
the	O
next	O
section	O
are	O
based	O
on	O
results	O
reported	O
there	O
.	O
barto	O
and	O
singh	O
(	O
1990	O
)	O
consider	O
some	O
of	O
the	O
issues	O
in	O
comparing	O
direct	O
and	O
indirect	O
reinforcement	O
learn-	O
ing	B
methods	O
.	O
there	O
have	O
been	O
several	O
works	O
with	O
model-based	O
reinforcement	B
learning	I
that	O
take	O
the	O
idea	O
of	O
exploration	O
bonuses	O
and	O
optimistic	O
initialization	O
to	O
its	O
logical	O
extreme	O
,	O
in	O
which	O
all	O
incompletely	O
explored	O
choices	O
are	O
assumed	O
maximally	O
re-	O
warding	O
and	O
optimal	O
paths	O
are	O
computed	O
to	O
test	O
them	O
.	O
the	O
e3	O
algorithm	O
of	O
kearns	O
and	O
singh	O
(	O
2002	O
)	O
and	O
the	O
r-max	O
algorithm	O
of	O
brafman	O
and	O
tennenholtz	O
(	O
2003	O
)	O
are	O
guaranteed	O
to	O
ﬁnd	O
a	O
near-optimal	O
solution	O
in	O
time	O
polynomial	O
in	O
the	O
number	O
of	O
states	O
and	O
actions	O
.	O
this	O
is	O
usually	O
too	O
slow	O
for	O
practical	O
algorithms	O
but	O
is	O
probably	O
the	O
best	O
that	O
can	O
be	O
done	O
in	O
the	O
worst	O
case	O
.	O
prioritized	B
sweeping	I
was	O
developed	O
simultaneously	O
and	O
independently	O
by	O
moore	O
and	O
atkeson	O
(	O
1993	O
)	O
and	O
peng	O
and	O
williams	O
(	O
1993	O
)	O
.	O
the	O
results	O
in	O
the	O
box	O
on	O
page	O
170	O
are	O
due	O
to	O
peng	O
and	O
williams	O
(	O
1993	O
)	O
.	O
the	O
results	O
in	O
the	O
box	O
on	O
page	O
171	O
are	O
due	O
to	O
moore	O
and	O
atkeson	O
.	O
key	O
subsequent	O
work	O
in	O
this	O
area	O
8.13.	O
summary	O
of	O
part	O
i	O
:	O
dimensions	O
193	O
includes	O
that	O
by	O
mcmahan	O
and	O
gordon	O
(	O
2005	O
)	O
and	O
by	O
van	O
seijen	O
and	O
sutton	O
(	O
2013	O
)	O
.	O
8.5	O
this	O
section	O
was	O
strongly	O
inﬂuenced	O
by	O
the	O
experiments	O
of	O
singh	O
(	O
1993	O
)	O
.	O
8.6–7	O
trajectory	B
sampling	I
has	O
implicitly	O
been	O
a	O
part	O
of	O
reinforcement	O
learning	O
from	O
the	O
outset	O
,	O
but	O
it	O
was	O
most	O
explicitly	O
emphasized	O
by	O
barto	O
,	O
bradtke	O
,	O
and	O
singh	O
(	O
1995	O
)	O
in	O
their	O
introduction	O
of	O
rtdp	O
.	O
they	O
recognized	O
that	O
korf	O
’	O
s	O
(	O
1990	O
)	O
learn-	O
ing	B
real-time	O
a*	O
(	O
lrta*	O
)	O
algorithm	O
is	O
an	O
asynchronous	O
dp	O
algorithm	O
that	O
applies	O
to	O
stochastic	O
problems	O
as	O
well	O
as	O
the	O
deterministic	O
problems	O
on	O
which	O
korf	O
focused	O
.	O
beyond	O
lrta*	O
,	O
rtdp	O
includes	O
the	O
option	O
of	O
updating	O
the	O
values	O
of	O
many	O
states	O
in	O
the	O
time	O
intervals	O
between	O
the	O
execution	O
of	O
actions	O
.	O
barto	O
et	O
al	O
.	O
(	O
1995	O
)	O
proved	O
the	O
convergence	O
result	O
described	O
here	O
by	O
combining	O
korf	O
’	O
s	O
(	O
1990	O
)	O
convergence	O
proof	B
for	O
lrta*	O
with	O
the	O
result	O
of	O
bertsekas	O
(	O
1982	O
)	O
(	O
also	O
bertsekas	O
and	O
tsitsiklis	O
,	O
1989	O
)	O
ensuring	O
convergence	O
of	O
asynchronous	O
dp	O
for	O
stochastic	O
shortest	O
path	O
problems	O
in	O
the	O
undiscounted	O
case	O
.	O
combining	O
model-learning	O
with	O
rtdp	O
is	O
called	O
adaptive	O
rtdp	O
,	O
also	O
presented	O
by	O
barto	O
et	O
al	O
.	O
(	O
1995	O
)	O
and	O
discussed	O
by	O
barto	O
(	O
2011	O
)	O
.	O
8.9	O
8.10	O
for	O
further	O
reading	O
on	O
heuristic	B
search	I
,	O
the	O
reader	O
is	O
encouraged	O
to	O
consult	O
texts	O
and	O
surveys	O
such	O
as	O
those	O
by	O
russell	O
and	O
norvig	O
(	O
2009	O
)	O
and	O
korf	O
(	O
1988	O
)	O
.	O
peng	O
and	O
williams	O
(	O
1993	O
)	O
explored	O
a	O
forward	O
focusing	O
of	O
updates	O
much	O
as	O
is	O
suggested	O
in	O
this	O
section	O
.	O
abramson	O
’	O
s	O
(	O
1990	O
)	O
expected-outcome	O
model	O
is	O
a	O
rollout	O
algorithm	O
applied	O
to	O
two-person	O
games	O
in	O
which	O
the	O
play	O
of	O
both	O
simulated	O
players	O
is	O
random	O
.	O
he	O
argued	O
that	O
even	O
with	O
random	O
play	O
,	O
it	O
is	O
a	O
“	O
powerful	O
heuristic	O
”	O
that	O
is	O
“	O
pre-	O
cise	O
,	O
accurate	O
,	O
easily	O
estimable	O
,	O
eﬃciently	O
calculable	O
,	O
and	O
domain-independent.	O
”	O
tesauro	O
and	O
galperin	O
(	O
1997	O
)	O
demonstrated	O
the	O
eﬀectiveness	O
of	O
rollout	O
algorithms	O
for	O
improving	O
the	O
play	O
of	O
backgammon	O
programs	O
,	O
adopting	O
the	O
term	O
“	O
rollout	O
”	O
from	O
its	O
use	O
in	O
evaluating	O
backgammon	B
positions	O
by	O
playing	O
out	O
positions	O
with	O
diﬀerent	O
randomly	O
generating	O
sequences	O
of	O
dice	O
rolls	O
.	O
bertsekas	O
,	O
tsitsiklis	O
,	O
and	O
wu	O
(	O
1997	O
)	O
examine	O
rollout	B
algorithms	I
applied	O
to	O
combinatorial	O
optimization	O
problems	O
,	O
and	O
bertsekas	O
(	O
2013	O
)	O
surveys	O
their	O
use	O
in	O
discrete	O
deterministic	O
opti-	O
mization	O
problems	O
,	O
remarking	O
that	O
they	O
are	O
“	O
often	O
surprisingly	O
eﬀective.	O
”	O
8.11	O
the	O
central	O
ideas	O
of	O
mcts	O
were	O
introduced	O
by	O
coulom	O
(	O
2006	O
)	O
and	O
by	O
kocsis	O
and	O
szepesv´ari	O
(	O
2006	O
)	O
.	O
they	O
built	O
upon	O
previous	O
research	O
with	O
monte	O
carlo	O
planning	B
algorithms	O
as	O
reviewed	O
by	O
these	O
authors	O
.	O
browne	O
,	O
powley	O
,	O
whitehouse	O
,	O
lucas	O
,	O
cowling	O
,	O
rohlfshagen	O
,	O
tavener	O
,	O
perez	O
,	O
samothrakis	O
,	O
and	O
colton	O
(	O
2012	O
)	O
is	O
an	O
excellent	O
survey	O
of	O
mcts	O
methods	O
and	O
their	O
applications	O
.	O
david	O
silver	O
contributed	O
to	O
the	O
ideas	O
and	O
presentation	O
in	O
this	O
section	O
.	O
part	O
ii	O
:	O
approximate	B
solution	O
methods	O
in	O
the	O
second	O
part	O
of	O
the	O
book	O
we	O
extend	O
the	O
tabular	O
methods	O
presented	O
in	O
the	O
ﬁrst	O
part	O
to	O
apply	O
to	O
problems	O
with	O
arbitrarily	O
large	O
state	B
spaces	O
.	O
in	O
many	O
of	O
the	O
tasks	O
to	O
which	O
we	O
would	O
like	O
to	O
apply	O
reinforcement	B
learning	I
the	O
state	B
space	O
is	O
combinatorial	O
and	O
enormous	O
;	O
the	O
number	O
of	O
possible	O
camera	O
images	O
,	O
for	O
example	O
,	O
is	O
much	O
larger	O
than	O
the	O
number	O
of	O
atoms	O
in	O
the	O
universe	O
.	O
in	O
such	O
cases	O
we	O
can	O
not	O
expect	O
to	O
ﬁnd	O
an	O
optimal	O
policy	O
or	O
the	O
optimal	O
value	O
function	O
even	O
in	O
the	O
limit	O
of	O
inﬁnite	O
time	O
and	O
data	O
;	O
our	O
goal	B
instead	O
is	O
to	O
ﬁnd	O
a	O
good	O
approximate	B
solution	O
using	O
limited	O
computational	O
resources	O
.	O
in	O
this	O
part	O
of	O
the	O
book	O
we	O
explore	O
such	O
approximate	B
solution	O
methods	O
.	O
the	O
problem	O
with	O
large	O
state	B
spaces	O
is	O
not	O
just	O
the	O
memory	O
needed	O
for	O
large	O
tables	O
,	O
but	O
the	O
time	O
and	O
data	O
needed	O
to	O
ﬁll	O
them	O
accurately	O
.	O
in	O
many	O
of	O
our	O
target	B
tasks	O
,	O
almost	O
every	O
state	B
encountered	O
will	O
never	O
have	O
been	O
seen	O
before	O
.	O
to	O
make	O
sensible	O
decisions	O
in	O
such	O
states	O
it	O
is	O
necessary	O
to	O
generalize	O
from	O
previous	O
encounters	O
with	O
diﬀerent	O
states	O
that	O
are	O
in	O
some	O
sense	O
similar	O
to	O
the	O
current	O
one	O
.	O
in	O
other	O
words	O
,	O
the	O
key	O
issue	O
is	O
that	O
of	O
generalization	O
.	O
how	O
can	O
experience	O
with	O
a	O
limited	O
subset	O
of	O
the	O
state	B
space	O
be	O
usefully	O
generalized	O
to	O
produce	O
a	O
good	O
approximation	O
over	O
a	O
much	O
larger	O
subset	O
?	O
fortunately	O
,	O
generalization	O
from	O
examples	O
has	O
already	O
been	O
extensively	O
studied	O
,	O
and	O
we	O
do	O
not	O
need	O
to	O
invent	O
totally	O
new	O
methods	O
for	O
use	O
in	O
reinforcement	O
learning	O
.	O
to	O
some	O
extent	O
we	O
need	O
only	O
combine	O
reinforcement	B
learning	I
methods	O
with	O
existing	O
generalization	O
methods	O
.	O
the	O
kind	O
of	O
generalization	O
we	O
require	O
is	O
often	O
called	O
function	B
approximation	I
because	O
it	O
takes	O
examples	O
from	O
a	O
desired	O
function	O
(	O
e.g.	O
,	O
a	O
value	B
function	I
)	O
and	O
attempts	O
to	O
generalize	O
from	O
them	O
to	O
construct	O
an	O
approximation	O
of	O
the	O
entire	O
function	O
.	O
function	B
approximation	I
is	O
an	O
instance	O
of	O
supervised	O
learning	O
,	O
the	O
primary	O
topic	O
studied	O
in	O
machine	O
learning	O
,	O
artiﬁcial	B
neural	I
networks	I
,	O
pattern	O
recognition	O
,	O
and	O
statistical	O
curve	O
ﬁtting	O
.	O
in	O
theory	O
,	O
any	O
of	O
the	O
methods	O
studied	O
in	O
these	O
ﬁelds	O
can	O
be	O
used	O
in	O
the	O
role	O
of	O
function	O
approximator	O
within	O
reinforcement	B
learning	I
algorithms	O
,	O
although	O
in	O
practice	O
some	O
ﬁt	O
more	O
easily	O
into	O
this	O
role	O
than	O
others	O
.	O
reinforcement	B
learning	I
with	O
function	B
approximation	I
involves	O
a	O
number	O
of	O
new	O
issues	O
that	O
do	O
not	O
normally	O
arise	O
in	O
conventional	O
supervised	B
learning	I
,	O
such	O
as	O
nonstationarity	O
,	O
bootstrapping	B
,	O
and	O
delayed	O
targets	O
.	O
we	O
introduce	O
these	O
and	O
other	O
issues	O
successively	O
over	O
the	O
ﬁve	O
chapters	O
of	O
this	O
part	O
.	O
initially	O
we	O
restrict	O
attention	O
to	O
on-policy	O
training	O
,	O
treating	O
in	O
chapter	O
9	O
the	O
prediction	B
case	O
,	O
in	O
which	O
the	O
policy	B
is	O
given	O
and	O
only	O
its	O
value	B
function	I
is	O
approximated	O
,	O
and	O
then	O
in	O
chapter	O
10	O
the	O
control	B
case	O
,	O
in	O
which	O
an	O
approximation	O
to	O
the	O
optimal	O
policy	O
is	O
found	O
.	O
the	O
challenging	O
problem	O
of	O
oﬀ-policy	O
learning	O
with	O
function	B
approximation	I
is	O
treated	O
in	O
chapter	O
11.	O
in	O
each	O
of	O
these	O
three	O
chapters	O
we	O
will	O
have	O
to	O
return	B
to	O
ﬁrst	O
principles	O
and	O
re-examine	O
the	O
objectives	O
of	O
the	O
learning	O
to	O
take	O
into	O
account	O
function	B
approximation	I
.	O
chapter	O
12	O
introduces	O
and	O
analyzes	O
the	O
algorithmic	O
195	O
mechanism	O
of	O
eligibility	O
traces	O
,	O
which	O
dramatically	O
improves	O
the	O
computational	O
properties	O
of	O
multi-step	O
reinforcement	B
learning	I
methods	O
in	O
many	O
cases	O
.	O
the	O
ﬁnal	O
chapter	O
of	O
this	O
part	O
explores	O
a	O
diﬀerent	O
approach	O
to	O
control	B
,	O
policy-gradient	O
methods	O
,	O
which	O
approximate	B
the	O
optimal	O
policy	O
directly	O
and	O
need	O
never	O
form	O
an	O
approximate	B
value	O
function	O
(	O
although	O
they	O
may	O
be	O
much	O
more	O
eﬃcient	O
if	O
they	O
do	O
approximate	B
a	O
value	B
function	I
as	O
well	O
the	O
policy	B
)	O
.	O
chapter	O
9	O
on-policy	O
prediction	O
with	B
approximation	I
in	O
this	O
chapter	O
,	O
we	O
begin	O
our	O
study	O
of	O
function	O
approximation	O
in	O
reinforcement	O
learning	O
by	O
considering	O
its	O
use	O
in	O
estimating	O
the	O
state-value	O
function	O
from	O
on-policy	O
data	O
,	O
that	O
is	O
,	O
in	O
approximating	O
vπ	O
from	O
experience	O
generated	O
using	O
a	O
known	O
policy	B
π.	O
the	O
novelty	O
in	O
this	O
chapter	O
is	O
that	O
the	O
approximate	B
value	O
function	O
is	O
represented	O
not	O
as	O
a	O
table	O
but	O
as	O
a	O
parameterized	O
functional	O
form	O
with	O
weight	O
vector	B
w	O
∈	O
rd	O
.	O
we	O
will	O
write	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
≈	O
vπ	O
(	O
s	O
)	O
for	O
the	O
approximate	B
value	O
of	O
state	O
s	O
given	O
weight	O
vector	B
w.	O
for	O
example	O
,	O
ˆv	O
might	O
be	O
a	O
linear	O
function	O
in	O
features	O
of	O
the	O
state	B
,	O
with	O
w	O
the	O
vector	B
of	O
feature	O
weights	O
.	O
more	O
generally	O
,	O
ˆv	O
might	O
be	O
the	O
function	O
computed	O
by	O
a	O
multi-layer	O
artiﬁcial	O
neural	O
network	O
,	O
with	O
w	O
the	O
vector	B
of	O
connection	O
weights	O
in	O
all	O
the	O
layers	O
.	O
by	O
adjusting	O
the	O
weights	O
,	O
any	O
of	O
a	O
wide	O
range	O
of	O
diﬀerent	O
functions	O
can	O
be	O
implemented	O
by	O
the	O
network	O
.	O
or	O
ˆv	O
might	O
be	O
the	O
function	O
computed	O
by	O
a	O
decision	O
tree	O
,	O
where	O
w	O
is	O
all	O
the	O
numbers	O
deﬁning	O
the	O
split	O
points	O
and	O
leaf	O
values	O
of	O
the	O
tree	O
.	O
typically	O
,	O
the	O
number	O
of	O
weights	O
(	O
the	O
dimensionality	O
of	O
w	O
)	O
is	O
much	O
less	O
than	O
the	O
number	O
of	O
states	O
(	O
d	O
(	O
cid:28	O
)	O
|s|	O
)	O
,	O
and	O
changing	O
one	O
weight	O
changes	O
the	O
estimated	O
value	B
of	O
many	O
states	O
.	O
consequently	O
,	O
when	O
a	O
single	O
state	B
is	O
updated	O
,	O
the	O
change	O
generalizes	O
from	O
that	O
state	B
to	O
aﬀect	O
the	O
values	O
of	O
many	O
other	O
states	O
.	O
such	O
generalization	O
makes	O
the	O
learning	O
potentially	O
more	O
powerful	O
but	O
also	O
potentially	O
more	O
diﬃcult	O
to	O
manage	O
and	O
understand	O
.	O
perhaps	O
surprisingly	O
,	O
extending	O
reinforcement	B
learning	I
to	O
function	B
approximation	I
also	O
makes	O
it	O
applicable	O
to	O
partially	O
observable	O
problems	O
,	O
in	O
which	O
the	O
full	O
state	B
is	O
not	O
avail-	O
able	O
to	O
the	O
agent	O
.	O
if	O
the	O
parameterized	O
function	O
form	O
for	O
ˆv	O
does	O
not	O
allow	O
the	O
estimated	O
value	B
to	O
depend	O
on	O
certain	O
aspects	O
of	O
the	O
state	B
,	O
then	O
it	O
is	O
just	O
as	O
if	O
those	O
aspects	O
are	O
un-	O
observable	O
.	O
in	O
fact	O
,	O
all	O
the	O
theoretical	O
results	O
for	O
methods	O
using	O
function	B
approximation	I
presented	O
in	O
this	O
part	O
of	O
the	O
book	O
apply	O
equally	O
well	O
to	O
cases	O
of	O
partial	O
observability	O
.	O
what	O
function	B
approximation	I
can	O
’	O
t	O
do	O
,	O
however	O
,	O
is	O
augment	O
the	O
state	B
representation	O
with	O
memories	O
of	O
past	O
observations	O
.	O
some	O
such	O
possible	O
further	O
extensions	O
are	O
discussed	O
brieﬂy	O
in	O
section	O
17.3	O
.	O
197	O
198	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
9.1	O
value-function	B
approximation	I
all	O
of	O
the	O
prediction	B
methods	O
covered	O
in	O
this	O
book	O
have	O
been	O
described	O
as	O
updates	O
to	O
an	O
estimated	O
value	B
function	I
that	O
shift	O
its	O
value	B
at	O
particular	O
states	O
toward	O
a	O
“	O
backed-	O
up	O
value	B
,	O
”	O
or	O
update	O
target	B
,	O
for	O
that	O
state	B
.	O
let	O
us	O
refer	O
to	O
an	O
individual	O
update	O
by	O
the	O
notation	O
s	O
(	O
cid:55	O
)	O
→	O
u	O
,	O
where	O
s	O
is	O
the	O
state	B
updated	O
and	O
u	O
is	O
the	O
update	O
target	B
that	O
s	O
’	O
s	O
estimated	O
value	B
is	O
shifted	O
toward	O
.	O
for	O
example	O
,	O
the	O
monte	O
carlo	O
update	O
for	O
value	O
prediction	B
is	O
st	O
(	O
cid:55	O
)	O
→	O
gt	O
,	O
the	O
td	O
(	O
0	O
)	O
update	O
is	O
st	O
(	O
cid:55	O
)	O
→	O
rt+1	O
+	O
γˆv	O
(	O
st+1	O
,	O
wt	O
)	O
,	O
and	O
the	O
n-step	O
td	O
update	O
is	O
st	O
(	O
cid:55	O
)	O
→	O
gt	O
:	O
t+n	O
.	O
in	O
the	O
dp	O
(	O
dynamic	B
programming	I
)	O
policy-evaluation	O
update	O
,	O
s	O
(	O
cid:55	O
)	O
→	O
eπ	O
[	O
rt+1	O
+	O
γˆv	O
(	O
st+1	O
,	O
wt	O
)	O
|	O
st	O
=	O
s	O
]	O
,	O
an	O
arbitrary	O
state	B
s	O
is	O
updated	O
,	O
whereas	O
in	O
the	O
other	O
cases	O
the	O
state	B
encountered	O
in	O
actual	O
experience	O
,	O
st	O
,	O
is	O
updated	O
.	O
it	O
is	O
natural	O
to	O
interpret	O
each	O
update	O
as	O
specifying	O
an	O
example	O
of	O
the	O
desired	O
input–	O
output	O
behavior	O
of	O
the	O
value	B
function	I
.	O
in	O
a	O
sense	O
,	O
the	O
update	O
s	O
(	O
cid:55	O
)	O
→	O
u	O
means	O
that	O
the	O
estimated	O
value	B
for	O
state	B
s	O
should	O
be	O
more	O
like	O
the	O
update	O
target	B
u.	O
up	O
to	O
now	O
,	O
the	O
actual	O
update	O
has	O
been	O
trivial	O
:	O
the	O
table	O
entry	O
for	O
s	O
’	O
s	O
estimated	O
value	B
has	O
simply	O
been	O
shifted	O
a	O
fraction	O
of	O
the	O
way	O
toward	O
u	O
,	O
and	O
the	O
estimated	O
values	O
of	O
all	O
other	O
states	O
were	O
left	O
unchanged	O
.	O
now	O
we	O
permit	O
arbitrarily	O
complex	O
and	O
sophisticated	O
methods	O
to	O
implement	O
the	O
update	O
,	O
and	O
updating	O
at	O
s	O
generalizes	O
so	O
that	O
the	O
estimated	O
values	O
of	O
many	O
other	O
states	O
are	O
changed	O
as	O
well	O
.	O
machine	O
learning	O
methods	O
that	O
learn	O
to	O
mimic	O
input–output	O
examples	O
in	O
this	O
way	O
are	O
called	O
supervised	B
learning	I
methods	O
,	O
and	O
when	O
the	O
outputs	O
are	O
numbers	O
,	O
like	O
u	O
,	O
the	O
process	O
is	O
often	O
called	O
function	B
approximation	I
.	O
function	B
approximation	I
methods	O
expect	O
to	O
receive	O
examples	O
of	O
the	O
desired	O
input–output	O
behavior	O
of	O
the	O
function	O
they	O
are	O
trying	O
to	O
approximate	B
.	O
we	O
use	O
these	O
methods	O
for	O
value	O
prediction	B
simply	O
by	O
passing	O
to	O
them	O
the	O
s	O
(	O
cid:55	O
)	O
→	O
g	O
of	O
each	O
update	O
as	O
a	O
training	O
example	O
.	O
we	O
then	O
interpret	O
the	O
approximate	B
function	O
they	O
produce	O
as	O
an	O
estimated	O
value	B
function	I
.	O
viewing	O
each	O
update	O
as	O
a	O
conventional	O
training	O
example	O
in	O
this	O
way	O
enables	O
us	O
to	O
use	O
any	O
of	O
a	O
wide	O
range	O
of	O
existing	O
function	B
approximation	I
methods	O
for	O
value	O
prediction	B
.	O
in	O
principle	O
,	O
we	O
can	O
use	O
any	O
method	O
for	O
supervised	O
learning	O
from	O
examples	O
,	O
including	O
artiﬁcial	B
neural	I
networks	I
,	O
decision	O
trees	O
,	O
and	O
various	O
kinds	O
of	O
multivariate	O
regression	O
.	O
however	O
,	O
not	O
all	O
function	B
approximation	I
methods	O
are	O
equally	O
well	O
suited	O
for	O
use	O
in	O
reinforcement	O
learning	O
.	O
the	O
most	O
sophisticated	O
neural	B
network	O
and	O
statistical	O
methods	O
all	O
assume	O
a	O
static	O
training	O
set	O
over	O
which	O
multiple	O
passes	O
are	O
made	O
.	O
in	O
reinforcement	O
learning	O
,	O
however	O
,	O
it	O
is	O
important	O
that	O
learning	O
be	O
able	O
to	O
occur	O
online	B
,	O
while	O
the	O
agent	O
interacts	O
with	O
its	O
environment	B
or	O
with	O
a	O
model	O
of	O
its	O
environment	B
.	O
to	O
do	O
this	O
requires	O
methods	O
that	O
are	O
able	O
to	O
learn	O
eﬃciently	O
from	O
incrementally	O
acquired	O
data	O
.	O
in	O
addition	O
,	O
reinforcement	B
learning	I
generally	O
requires	O
function	B
approximation	I
methods	O
able	O
to	O
handle	O
nonstationary	O
target	B
functions	O
(	O
target	B
functions	O
that	O
change	O
over	O
time	O
)	O
.	O
for	O
example	O
,	O
in	O
control	O
methods	O
based	O
on	O
gpi	O
(	O
generalized	O
policy	O
iteration	O
)	O
we	O
often	O
seek	O
to	O
learn	O
qπ	O
while	O
π	O
changes	O
.	O
even	O
if	O
the	O
policy	B
remains	O
the	O
same	O
,	O
the	O
target	B
values	O
of	O
training	O
examples	O
are	O
nonstationary	O
if	O
they	O
are	O
generated	O
by	O
bootstrapping	B
methods	O
(	O
dp	O
and	O
td	O
learning	O
)	O
.	O
methods	O
that	O
can	O
not	O
easily	O
handle	O
such	O
nonstationarity	B
are	O
less	O
suitable	O
for	O
reinforcement	O
learning	O
.	O
9.2.	O
the	O
prediction	B
objective	O
(	O
ve	O
)	O
199	O
9.2	O
the	O
prediction	B
objective	O
(	O
ve	O
)	O
up	O
to	O
now	O
we	O
have	O
not	O
speciﬁed	O
an	O
explicit	O
objective	O
for	O
prediction	O
.	O
in	O
the	O
tabular	O
case	O
a	O
continuous	O
measure	O
of	O
prediction	O
quality	O
was	O
not	O
necessary	O
because	O
the	O
learned	O
value	B
function	I
could	O
come	O
to	O
equal	O
the	O
true	O
value	O
function	O
exactly	O
.	O
moreover	O
,	O
the	O
learned	O
values	O
at	O
each	O
state	B
were	O
decoupled—an	O
update	O
at	O
one	O
state	B
aﬀected	O
no	O
other	O
.	O
but	O
with	O
genuine	O
approximation	O
,	O
an	O
update	O
at	O
one	O
state	B
aﬀects	O
many	O
others	O
,	O
and	O
it	O
is	O
not	O
possible	O
to	O
get	O
the	O
values	O
of	O
all	O
states	O
exactly	O
correct	O
.	O
by	O
assumption	O
we	O
have	O
far	O
more	O
states	O
than	O
weights	O
,	O
so	O
making	O
one	O
state	B
’	O
s	O
estimate	O
more	O
accurate	O
invariably	O
means	O
making	O
others	O
’	O
less	O
accurate	O
.	O
we	O
are	O
obligated	O
then	O
to	O
say	O
which	O
states	O
we	O
care	O
most	O
about	O
.	O
we	O
must	O
specify	O
a	O
state	B
distribution	O
µ	O
(	O
s	O
)	O
≥	O
0	O
,	O
(	O
cid:80	O
)	O
s	O
µ	O
(	O
s	O
)	O
=	O
1	O
,	O
representing	O
how	O
much	O
we	O
care	O
about	O
the	O
error	O
in	O
each	O
state	B
s.	O
by	O
the	O
error	O
in	O
a	O
state	B
s	O
we	O
mean	O
the	O
square	O
of	O
the	O
diﬀerence	O
between	O
the	O
approximate	B
value	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
and	O
the	O
true	O
value	B
vπ	O
(	O
s	O
)	O
.	O
weighting	O
this	O
over	O
the	O
state	B
space	O
by	O
µ	O
,	O
we	O
obtain	O
a	O
natural	O
objective	O
function	O
,	O
the	O
mean	O
squared	O
value	B
error	O
,	O
denoted	O
ve	O
:	O
ve	O
(	O
w	O
)	O
.	O
=	O
(	O
cid:88	O
)	O
s∈s	O
µ	O
(	O
s	O
)	O
(	O
cid:104	O
)	O
vπ	O
(	O
s	O
)	O
−	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
(	O
cid:105	O
)	O
2	O
.	O
(	O
9.1	O
)	O
the	O
square	O
root	O
of	O
this	O
measure	O
,	O
the	O
root	O
ve	O
,	O
gives	O
a	O
rough	O
measure	O
of	O
how	O
much	O
the	O
approximate	B
values	O
diﬀer	O
from	O
the	O
true	O
values	O
and	O
is	O
often	O
used	O
in	O
plots	O
.	O
often	O
µ	O
(	O
s	O
)	O
is	O
chosen	O
to	O
be	O
the	O
fraction	O
of	O
time	O
spent	O
in	O
s.	O
under	O
on-policy	O
training	O
this	O
is	O
called	O
the	O
on-policy	B
distribution	I
;	O
we	O
focus	O
entirely	O
on	O
this	O
case	O
in	O
this	O
chapter	O
.	O
in	O
continuing	O
tasks	O
,	O
the	O
on-policy	B
distribution	I
is	O
the	O
stationary	O
distribution	O
under	O
π.	O
the	O
on-policy	B
distribution	I
in	O
episodic	O
tasks	O
in	O
an	O
episodic	O
task	O
,	O
the	O
on-policy	B
distribution	I
is	O
a	O
little	O
diﬀerent	O
in	O
that	O
it	O
depends	O
on	O
how	O
the	O
initial	O
states	O
of	O
episodes	O
are	O
chosen	O
.	O
let	O
h	O
(	O
s	O
)	O
denote	O
the	O
probability	O
that	O
an	O
episode	O
begins	O
in	O
each	O
state	B
s	O
,	O
and	O
let	O
η	O
(	O
s	O
)	O
denote	O
the	O
number	O
of	O
time	O
steps	O
spent	O
,	O
on	O
average	O
,	O
in	O
state	O
s	O
in	O
a	O
single	O
episode	O
.	O
time	O
is	O
spent	O
in	O
a	O
state	B
s	O
if	O
episodes	B
start	O
in	O
s	O
,	O
or	O
if	O
transitions	O
are	O
made	O
into	O
s	O
from	O
a	O
preceding	O
state	B
¯s	O
in	O
which	O
time	O
is	O
spent	O
:	O
η	O
(	O
s	O
)	O
=	O
h	O
(	O
s	O
)	O
+	O
(	O
cid:88	O
)	O
¯s	O
η	O
(	O
¯s	O
)	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
a|¯s	O
)	O
p	O
(	O
s|	O
¯s	O
,	O
a	O
)	O
,	O
for	O
all	O
s	O
∈	O
s.	O
(	O
9.2	O
)	O
this	O
system	O
of	O
equations	O
can	O
be	O
solved	O
for	O
the	O
expected	O
number	O
of	O
visits	O
η	O
(	O
s	O
)	O
.	O
the	O
on-policy	B
distribution	I
is	O
then	O
the	O
fraction	O
of	O
time	O
spent	O
in	O
each	O
state	B
normalized	O
to	O
sum	O
to	O
one	O
:	O
µ	O
(	O
s	O
)	O
=	O
,	O
for	O
all	O
s	O
∈	O
s.	O
(	O
9.3	O
)	O
η	O
(	O
s	O
)	O
(	O
cid:80	O
)	O
s	O
(	O
cid:48	O
)	O
η	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
this	O
is	O
the	O
natural	O
choice	O
without	O
discounting	B
.	O
if	O
there	O
is	O
discounting	B
(	O
γ	O
<	O
1	O
)	O
it	O
should	O
be	O
treated	O
as	O
a	O
form	O
of	O
termination	O
,	O
which	O
can	O
be	O
done	O
simply	O
by	O
including	O
a	O
factor	O
of	O
γ	O
in	O
the	O
second	O
term	O
of	O
(	O
9.2	O
)	O
.	O
200	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
the	O
two	O
cases	O
,	O
continuing	O
and	O
episodic	O
,	O
behave	O
similarly	O
,	O
but	O
with	B
approximation	I
they	O
must	O
be	O
treated	O
separately	O
in	O
formal	O
analyses	O
,	O
as	O
we	O
will	O
see	O
repeatedly	O
in	O
this	O
part	O
of	O
the	O
book	O
.	O
this	O
completes	O
the	O
speciﬁcation	O
of	O
the	O
learning	O
objective	O
.	O
but	O
it	O
is	O
not	O
completely	O
clear	O
that	O
the	O
ve	O
is	O
the	O
right	O
performance	O
objective	O
for	O
re-	O
inforcement	O
learning	O
.	O
remember	O
that	O
our	O
ultimate	O
purpose—the	O
reason	O
we	O
are	O
learning	O
a	O
value	B
function—is	O
to	O
ﬁnd	O
a	O
better	O
policy	B
.	O
the	O
best	O
value	B
function	I
for	O
this	O
purpose	O
is	O
not	O
necessarily	O
the	O
best	O
for	O
minimizing	O
ve	O
.	O
nevertheless	O
,	O
it	O
is	O
not	O
yet	O
clear	O
what	O
a	O
more	O
useful	O
alternative	O
goal	B
for	O
value	B
prediction	O
might	O
be	O
.	O
for	O
now	O
,	O
we	O
will	O
focus	O
on	O
ve	O
.	O
an	O
ideal	O
goal	B
in	O
terms	O
of	O
ve	O
would	O
be	O
to	O
ﬁnd	O
a	O
global	O
optimum	O
,	O
a	O
weight	O
vector	B
w∗	O
for	O
which	O
ve	O
(	O
w∗	O
)	O
≤	O
ve	O
(	O
w	O
)	O
for	O
all	O
possible	O
w.	O
reaching	O
this	O
goal	B
is	O
sometimes	O
possible	O
for	O
simple	O
function	O
approximators	O
such	O
as	O
linear	O
ones	O
,	O
but	O
is	O
rarely	O
possible	O
for	O
complex	O
function	O
approximators	O
such	O
as	O
artiﬁcial	O
neural	B
networks	I
and	O
decision	O
trees	O
.	O
short	O
of	O
this	O
,	O
complex	O
function	O
approximators	O
may	O
seek	O
to	O
converge	O
instead	O
to	O
a	O
local	O
optimum	O
,	O
a	O
weight	O
vector	B
w∗	O
for	O
which	O
ve	O
(	O
w∗	O
)	O
≤	O
ve	O
(	O
w	O
)	O
for	O
all	O
w	O
in	O
some	O
neighborhood	O
of	O
w∗	O
.	O
although	O
this	O
guarantee	O
is	O
only	O
slightly	O
reassuring	O
,	O
it	O
is	O
typically	O
the	O
best	O
that	O
can	O
be	O
said	O
for	O
nonlinear	O
function	O
approximators	O
,	O
and	O
often	O
it	O
is	O
enough	O
.	O
still	O
,	O
for	O
many	O
cases	O
of	O
interest	O
in	O
reinforcement	O
learning	O
there	O
is	O
no	O
guarantee	O
of	O
convergence	O
to	O
an	O
optimum	O
,	O
or	O
even	O
to	O
within	O
a	O
bounded	O
distance	O
of	O
an	O
optimum	O
.	O
some	O
methods	O
may	O
in	O
fact	O
diverge	O
,	O
with	O
their	O
ve	O
approaching	O
inﬁnity	O
in	O
the	O
limit	O
.	O
in	O
the	O
last	O
two	O
sections	O
we	O
outlined	O
a	O
framework	O
for	O
combining	O
a	O
wide	O
range	O
of	O
rein-	O
forcement	O
learning	O
methods	O
for	O
value	O
prediction	B
with	O
a	O
wide	O
range	O
of	O
function	O
approxi-	O
mation	O
methods	O
,	O
using	O
the	O
updates	O
of	O
the	O
former	O
to	O
generate	O
training	O
examples	O
for	O
the	O
latter	O
.	O
we	O
also	O
described	O
a	O
ve	O
performance	O
measure	O
which	O
these	O
methods	O
may	O
aspire	O
to	O
minimize	O
.	O
the	O
range	O
of	O
possible	O
function	B
approximation	I
methods	O
is	O
far	O
too	O
large	O
to	O
cover	O
all	O
,	O
and	O
anyway	O
too	O
little	O
is	O
known	O
about	O
most	O
of	O
them	O
to	O
make	O
a	O
reliable	O
evaluation	O
or	O
recommendation	O
.	O
of	O
necessity	O
,	O
we	O
consider	O
only	O
a	O
few	O
possibilities	O
.	O
in	O
the	O
rest	O
of	O
this	O
chapter	O
we	O
focus	O
on	O
function	B
approximation	I
methods	O
based	O
on	O
gradient	B
principles	O
,	O
and	O
on	O
linear	O
gradient-descent	O
methods	O
in	O
particular	O
.	O
we	O
focus	O
on	O
these	O
methods	O
in	O
part	O
because	O
we	O
consider	O
them	O
to	O
be	O
particularly	O
promising	O
and	O
because	O
they	O
reveal	O
key	O
theoretical	O
issues	O
,	O
but	O
also	O
because	O
they	O
are	O
simple	O
and	O
our	O
space	O
is	O
limited	O
.	O
9.3	O
stochastic-gradient	O
and	O
semi-gradient	O
methods	O
we	O
now	O
develop	O
in	O
detail	O
one	O
class	O
of	O
learning	O
methods	O
for	O
function	O
approximation	O
in	O
value	O
prediction	B
,	O
those	O
based	O
on	O
stochastic	O
gradient	O
descent	O
(	O
sgd	O
)	O
.	O
sgd	O
methods	O
are	O
among	O
the	O
most	O
widely	O
used	O
of	O
all	O
function	B
approximation	I
methods	O
and	O
are	O
particularly	O
well	O
suited	O
to	O
online	B
reinforcement	O
learning	O
.	O
in	O
gradient-descent	O
methods	O
,	O
the	O
weight	O
vector	B
is	O
a	O
column	O
vector	B
with	O
a	O
ﬁxed	O
number	O
.	O
=	O
(	O
w1	O
,	O
w2	O
,	O
.	O
.	O
.	O
,	O
wd	O
)	O
(	O
cid:62	O
)	O
,1	O
and	O
the	O
approximate	O
value	B
function	I
of	O
real	O
valued	O
components	O
,	O
w	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
is	O
a	O
diﬀerentiable	O
function	O
of	O
w	O
for	O
all	O
s	O
∈	O
s.	O
we	O
will	O
be	O
updating	O
w	O
at	O
each	O
of	O
a	O
series	O
of	O
discrete	O
time	O
steps	O
,	O
t	O
=	O
0	O
,	O
1	O
,	O
2	O
,	O
3	O
,	O
.	O
.	O
.	O
,	O
so	O
we	O
will	O
need	O
a	O
notation	O
wt	O
for	O
the	O
1the	O
(	O
cid:62	O
)	O
denotes	O
transpose	O
,	O
needed	O
here	O
to	O
turn	O
the	O
horizontal	O
row	O
vector	B
in	O
the	O
text	O
into	O
a	O
vertical	O
column	O
vector	B
;	O
in	O
this	O
book	O
vectors	O
are	O
generally	O
taken	O
to	O
be	O
column	O
vectors	O
unless	O
explicitly	O
written	O
out	O
horizontally	O
or	O
transposed	O
.	O
9.3.	O
stochastic-gradient	O
and	O
semi-gradient	O
methods	O
201	O
weight	O
vector	B
at	O
each	O
step	O
.	O
for	O
now	O
,	O
let	O
us	O
assume	O
that	O
,	O
on	O
each	O
step	O
,	O
we	O
observe	O
a	O
new	O
example	O
st	O
(	O
cid:55	O
)	O
→	O
vπ	O
(	O
st	O
)	O
consisting	O
of	O
a	O
(	O
possibly	O
randomly	O
selected	O
)	O
state	B
st	O
and	O
its	O
true	O
value	O
under	O
the	O
policy	B
.	O
these	O
states	O
might	O
be	O
successive	O
states	O
from	O
an	O
interaction	O
with	O
the	O
environment	B
,	O
but	O
for	O
now	O
we	O
do	O
not	O
assume	O
so	O
.	O
even	O
though	O
we	O
are	O
given	O
the	O
exact	O
,	O
correct	O
values	O
,	O
vπ	O
(	O
st	O
)	O
for	O
each	O
st	O
,	O
there	O
is	O
still	O
a	O
diﬃcult	O
problem	O
because	O
our	O
function	O
approximator	O
has	O
limited	O
resources	O
and	O
thus	O
limited	O
resolution	O
.	O
in	O
particular	O
,	O
there	O
is	O
generally	O
no	O
w	O
that	O
gets	O
all	O
the	O
states	O
,	O
or	O
even	O
all	O
the	O
examples	O
,	O
exactly	O
correct	O
.	O
in	O
addition	O
,	O
we	O
must	O
generalize	O
to	O
all	O
the	O
other	O
states	O
that	O
have	O
not	O
appeared	O
in	O
examples	O
.	O
we	O
assume	O
that	O
states	O
appear	O
in	O
examples	O
with	O
the	O
same	O
distribution	O
,	O
µ	O
,	O
over	O
which	O
we	O
are	O
trying	O
to	O
minimize	O
the	O
ve	O
as	O
given	O
by	O
(	O
9.1	O
)	O
.	O
a	O
good	O
strategy	O
in	O
this	O
case	O
is	O
to	O
try	O
to	O
minimize	O
error	O
on	O
the	O
observed	O
examples	O
.	O
stochastic	O
gradient-descent	O
(	O
sgd	O
)	O
methods	O
do	O
this	O
by	O
adjusting	O
the	O
weight	O
vector	B
after	O
each	O
example	O
by	O
a	O
small	O
amount	O
in	O
the	O
direction	O
that	O
would	O
most	O
reduce	O
the	O
error	O
on	O
that	O
example	O
:	O
.	O
=	O
wt	O
−	O
1	O
2	O
wt+1	O
α∇	O
(	O
cid:104	O
)	O
vπ	O
(	O
st	O
)	O
−	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
(	O
cid:105	O
)	O
2	O
=	O
wt	O
+	O
α	O
(	O
cid:104	O
)	O
vπ	O
(	O
st	O
)	O
−	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
(	O
cid:105	O
)	O
∇ˆv	O
(	O
st	O
,	O
wt	O
)	O
,	O
(	O
9.4	O
)	O
(	O
9.5	O
)	O
where	O
α	O
is	O
a	O
positive	O
step-size	B
parameter	I
,	O
and	O
∇f	O
(	O
w	O
)	O
,	O
for	O
any	O
scalar	O
expression	O
f	O
(	O
w	O
)	O
that	O
is	O
a	O
function	O
of	O
a	O
vector	B
(	O
here	O
w	O
)	O
,	O
denotes	O
the	O
column	O
vector	B
of	O
partial	O
derivatives	O
of	O
the	O
expression	O
with	O
respect	O
to	O
the	O
components	O
of	O
the	O
vector	B
:	O
∇f	O
(	O
w	O
)	O
.	O
=	O
(	O
cid:18	O
)	O
∂f	O
(	O
w	O
)	O
∂w1	O
,	O
∂f	O
(	O
w	O
)	O
∂w2	O
,	O
.	O
.	O
.	O
,	O
∂f	O
(	O
w	O
)	O
∂wd	O
(	O
cid:19	O
)	O
(	O
cid:62	O
)	O
.	O
(	O
9.6	O
)	O
this	O
derivative	O
vector	B
is	O
the	O
gradient	B
of	O
f	O
with	O
respect	O
to	O
w.	O
sgd	O
methods	O
are	O
“	O
gradient	B
descent	I
”	O
methods	O
because	O
the	O
overall	O
step	O
in	O
wt	O
is	O
proportional	O
to	O
the	O
negative	O
gradient	B
of	O
the	O
example	O
’	O
s	O
squared	O
error	O
(	O
9.4	O
)	O
.	O
this	O
is	O
the	O
direction	O
in	O
which	O
the	O
error	O
falls	O
most	O
rapidly	O
.	O
gradient	B
descent	I
methods	O
are	O
called	O
“	O
stochastic	O
”	O
when	O
the	O
update	O
is	O
done	O
,	O
as	O
here	O
,	O
on	O
only	O
a	O
single	O
example	O
,	O
which	O
might	O
have	O
been	O
selected	O
stochastically	O
.	O
over	O
many	O
examples	O
,	O
making	O
small	O
steps	O
,	O
the	O
overall	O
eﬀect	O
is	O
to	O
minimize	O
an	O
average	O
performance	O
measure	O
such	O
as	O
the	O
ve	O
.	O
it	O
may	O
not	O
be	O
immediately	O
apparent	O
why	O
sgd	O
takes	O
only	O
a	O
small	O
step	O
in	O
the	O
direction	O
of	O
the	O
gradient	B
.	O
could	O
we	O
not	O
move	O
all	O
the	O
way	O
in	O
this	O
direction	O
and	O
completely	O
eliminate	O
the	O
error	O
on	O
the	O
example	O
?	O
in	O
many	O
cases	O
this	O
could	O
be	O
done	O
,	O
but	O
usually	O
it	O
is	O
not	O
desirable	O
.	O
remember	O
that	O
we	O
do	O
not	O
seek	O
or	O
expect	O
to	O
ﬁnd	O
a	O
value	B
function	I
that	O
has	O
zero	O
error	O
for	O
all	O
states	O
,	O
but	O
only	O
an	O
approximation	O
that	O
balances	O
the	O
errors	O
in	O
diﬀerent	O
states	O
.	O
if	O
we	O
completely	O
corrected	O
each	O
example	O
in	O
one	O
step	O
,	O
then	O
we	O
would	O
not	O
ﬁnd	O
such	O
a	O
balance	O
.	O
in	O
fact	O
,	O
the	O
convergence	O
results	O
for	O
sgd	O
methods	O
assume	O
that	O
α	O
decreases	O
over	O
time	O
.	O
if	O
it	O
decreases	O
in	O
such	O
a	O
way	O
as	O
to	O
satisfy	O
the	O
standard	O
stochastic	O
approximation	O
conditions	O
(	O
2.7	O
)	O
,	O
then	O
the	O
sgd	O
method	O
(	O
9.5	O
)	O
is	O
guaranteed	O
to	O
converge	O
to	O
a	O
local	O
optimum	O
.	O
we	O
turn	O
now	O
to	O
the	O
case	O
in	O
which	O
the	O
target	B
output	O
,	O
here	O
denoted	O
ut	O
∈	O
r	O
,	O
of	O
the	O
tth	O
training	O
example	O
,	O
st	O
(	O
cid:55	O
)	O
→	O
ut	O
,	O
is	O
not	O
the	O
true	O
value	O
,	O
vπ	O
(	O
st	O
)	O
,	O
but	O
some	O
,	O
possibly	O
random	O
,	O
approximation	O
to	O
it	O
.	O
for	O
example	O
,	O
ut	O
might	O
be	O
a	O
noise-corrupted	O
version	O
of	O
vπ	O
(	O
st	O
)	O
,	O
or	O
it	O
might	O
be	O
one	O
of	O
the	O
bootstrapping	B
targets	O
using	O
ˆv	O
mentioned	O
in	O
the	O
previous	O
section	O
.	O
in	O
202	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
these	O
cases	O
we	O
can	O
not	O
perform	O
the	O
exact	O
update	O
(	O
9.5	O
)	O
because	O
vπ	O
(	O
st	O
)	O
is	O
unknown	O
,	O
but	O
we	O
can	O
approximate	B
it	O
by	O
substituting	O
ut	O
in	O
place	O
of	O
vπ	O
(	O
st	O
)	O
.	O
this	O
yields	O
the	O
following	O
general	O
sgd	O
method	O
for	O
state-value	O
prediction	B
:	O
wt+1	O
.	O
=	O
wt	O
+	O
α	O
(	O
cid:104	O
)	O
ut	O
−	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
(	O
cid:105	O
)	O
∇ˆv	O
(	O
st	O
,	O
wt	O
)	O
.	O
(	O
9.7	O
)	O
if	O
ut	O
is	O
an	O
unbiased	O
estimate	O
,	O
that	O
is	O
,	O
if	O
e	O
[	O
ut|st	O
=	O
s	O
]	O
=	O
vπ	O
(	O
st	O
)	O
,	O
for	O
each	O
t	O
,	O
then	O
wt	O
is	O
guaranteed	O
to	O
converge	O
to	O
a	O
local	O
optimum	O
under	O
the	O
usual	O
stochastic	O
approximation	O
conditions	O
(	O
2.7	O
)	O
for	O
decreasing	O
α.	O
for	O
example	O
,	O
suppose	O
the	O
states	O
in	O
the	O
examples	O
are	O
the	O
states	O
generated	O
by	O
interaction	O
(	O
or	O
simulated	O
interaction	O
)	O
with	O
the	O
environment	B
using	O
policy	B
π.	O
because	O
the	O
true	O
value	O
of	O
.	O
a	O
state	B
is	O
the	O
expected	O
value	O
of	O
the	O
return	B
following	O
it	O
,	O
the	O
monte	O
carlo	O
target	B
ut	O
=	O
gt	O
is	O
by	O
deﬁnition	O
an	O
unbiased	O
estimate	O
of	O
vπ	O
(	O
st	O
)	O
.	O
with	O
this	O
choice	O
,	O
the	O
general	O
sgd	O
method	O
(	O
9.7	O
)	O
converges	O
to	O
a	O
locally	O
optimal	O
approximation	O
to	O
vπ	O
(	O
st	O
)	O
.	O
thus	O
,	O
the	O
gradient-descent	O
version	O
of	O
monte	O
carlo	O
state-value	O
prediction	B
is	O
guaranteed	O
to	O
ﬁnd	O
a	O
locally	O
optimal	O
solution	O
.	O
pseudocode	O
for	O
a	O
complete	O
algorithm	O
is	O
shown	O
in	O
the	O
box	O
below	O
.	O
gradient	B
monte	O
carlo	O
algorithm	O
for	O
estimating	O
ˆv	O
≈	O
vπ	O
input	O
:	O
the	O
policy	B
π	O
to	O
be	O
evaluated	O
input	O
:	O
a	O
diﬀerentiable	O
function	O
ˆv	O
:	O
s	O
×	O
rd	O
→	O
r	O
algorithm	O
parameter	O
:	O
step	O
size	O
α	O
>	O
0	O
initialize	O
value-function	O
weights	O
w	O
∈	O
rd	O
arbitrarily	O
(	O
e.g.	O
,	O
w	O
=	O
0	O
)	O
loop	O
forever	O
(	O
for	O
each	O
episode	O
)	O
:	O
generate	O
an	O
episode	O
s0	O
,	O
a0	O
,	O
r1	O
,	O
s1	O
,	O
a1	O
,	O
.	O
.	O
.	O
,	O
rt	O
,	O
st	O
using	O
π	O
loop	O
for	O
each	O
step	O
of	O
episode	O
,	O
t	O
=	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
−	O
1	O
:	O
w	O
←	O
w	O
+	O
α	O
(	O
cid:2	O
)	O
gt	O
−	O
ˆv	O
(	O
st	O
,	O
w	O
)	O
(	O
cid:3	O
)	O
∇ˆv	O
(	O
st	O
,	O
w	O
)	O
one	O
does	O
not	O
obtain	O
the	O
same	O
guarantees	O
if	O
a	O
bootstrapping	B
estimate	O
of	O
vπ	O
(	O
st	O
)	O
is	O
used	O
as	O
the	O
target	B
ut	O
in	O
(	O
9.7	O
)	O
.	O
bootstrapping	B
targets	O
such	O
as	O
n-step	O
returns	O
gt	O
:	O
t+n	O
or	O
the	O
dp	O
target	B
(	O
cid:80	O
)	O
a	O
,	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
π	O
(	O
a|st	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|st	O
,	O
a	O
)	O
[	O
r	O
+	O
γˆv	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
wt	O
)	O
]	O
all	O
depend	O
on	O
the	O
current	O
value	B
of	O
the	O
weight	O
vector	B
wt	O
,	O
which	O
implies	O
that	O
they	O
will	O
be	O
biased	O
and	O
that	O
they	O
will	O
not	O
produce	O
a	O
true	O
gradient-descent	O
method	O
.	O
one	O
way	O
to	O
look	O
at	O
this	O
is	O
that	O
the	O
key	O
step	O
from	O
(	O
9.4	O
)	O
to	O
(	O
9.5	O
)	O
relies	O
on	O
the	O
target	B
being	O
independent	O
of	O
wt	O
.	O
this	O
step	O
would	O
not	O
be	O
valid	O
if	O
a	O
bootstrapping	B
estimate	O
were	O
used	O
in	O
place	O
of	O
vπ	O
(	O
st	O
)	O
.	O
bootstrapping	B
methods	O
are	O
not	O
in	O
fact	O
instances	O
of	O
true	O
gradient	B
descent	I
(	O
barnard	O
,	O
1993	O
)	O
.	O
they	O
take	O
into	O
account	O
the	O
eﬀect	O
of	O
changing	O
the	O
weight	O
vector	B
wt	O
on	O
the	O
estimate	O
,	O
but	O
ignore	O
its	O
eﬀect	O
on	O
the	O
target	B
.	O
they	O
include	O
only	O
a	O
part	O
of	O
the	O
gradient	B
and	O
,	O
accordingly	O
,	O
we	O
call	O
them	O
semi-gradient	B
methods	I
.	O
although	O
semi-gradient	O
(	O
bootstrapping	B
)	O
methods	O
do	O
not	O
converge	O
as	O
robustly	O
as	O
gra-	O
dient	O
methods	O
,	O
they	O
do	O
converge	O
reliably	O
in	O
important	O
cases	O
such	O
as	O
the	O
linear	O
case	O
dis-	O
cussed	O
in	O
the	O
next	O
section	O
.	O
moreover	O
,	O
they	O
oﬀer	O
important	O
advantages	O
that	O
make	O
them	O
often	O
clearly	O
preferred	O
.	O
one	O
reason	O
for	O
this	O
is	O
that	O
they	O
typically	O
enable	O
signiﬁcantly	O
faster	O
learning	O
,	O
as	O
we	O
have	O
seen	O
in	O
chapters	O
6	O
and	O
7.	O
another	O
is	O
that	O
they	O
enable	O
learning	O
9.3.	O
stochastic-gradient	O
and	O
semi-gradient	O
methods	O
203	O
to	O
be	O
continual	O
and	O
online	O
,	O
without	O
waiting	O
for	O
the	O
end	O
of	O
an	O
episode	O
.	O
this	O
enables	O
them	O
to	O
be	O
used	O
on	O
continuing	O
problems	O
and	O
provides	O
computational	O
advantages	O
.	O
a	O
prototyp-	O
.	O
ical	O
semi-gradient	O
method	O
is	O
semi-gradient	O
td	O
(	O
0	O
)	O
,	O
which	O
uses	O
ut	O
=	O
rt+1	O
+	O
γˆv	O
(	O
st+1	O
,	O
w	O
)	O
as	O
its	O
target	B
.	O
complete	O
pseudocode	O
for	O
this	O
method	O
is	O
given	O
in	O
the	O
box	O
below	O
.	O
semi-gradient	O
td	O
(	O
0	O
)	O
for	O
estimating	O
ˆv	O
≈	O
vπ	O
input	O
:	O
the	O
policy	B
π	O
to	O
be	O
evaluated	O
input	O
:	O
a	O
diﬀerentiable	O
function	O
ˆv	O
:	O
s+	O
×	O
rd	O
→	O
r	O
such	O
that	O
ˆv	O
(	O
terminal	O
,	O
·	O
)	O
=	O
0	O
algorithm	O
parameter	O
:	O
step	O
size	O
α	O
>	O
0	O
initialize	O
value-function	O
weights	O
w	O
∈	O
rd	O
arbitrarily	O
(	O
e.g.	O
,	O
w	O
=	O
0	O
)	O
loop	O
for	O
each	O
episode	O
:	O
initialize	O
s	O
loop	O
for	O
each	O
step	O
of	O
episode	O
:	O
choose	O
a	O
∼	O
π	O
(	O
·|s	O
)	O
take	O
action	B
a	O
,	O
observe	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
w	O
←	O
w	O
+	O
α	O
(	O
cid:2	O
)	O
r	O
+	O
γˆv	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
w	O
)	O
−	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
(	O
cid:3	O
)	O
∇ˆv	O
(	O
s	O
,	O
w	O
)	O
s	O
←	O
s	O
(	O
cid:48	O
)	O
until	O
s	O
(	O
cid:48	O
)	O
is	O
terminal	O
state	B
aggregation	I
is	O
a	O
simple	O
form	O
of	O
generalizing	O
function	B
approximation	I
in	O
which	O
states	O
are	O
grouped	O
together	O
,	O
with	O
one	O
estimated	O
value	B
(	O
one	O
component	O
of	O
the	O
weight	O
vector	B
w	O
)	O
for	O
each	O
group	O
.	O
the	O
value	B
of	O
a	O
state	B
is	O
estimated	O
as	O
its	O
group	O
’	O
s	O
component	O
,	O
and	O
when	O
the	O
state	B
is	O
updated	O
,	O
that	O
component	O
alone	O
is	O
updated	O
.	O
state	B
aggregation	I
is	O
a	O
special	O
case	O
of	O
sgd	O
(	O
9.7	O
)	O
in	O
which	O
the	O
gradient	B
,	O
∇ˆv	O
(	O
st	O
,	O
wt	O
)	O
,	O
is	O
1	O
for	O
st	O
’	O
s	O
group	O
’	O
s	O
component	O
and	O
0	O
for	O
the	O
other	O
components	O
.	O
example	O
9.1	O
:	O
state	B
aggregation	I
on	O
the	O
1000-state	B
random	O
walk	O
consider	O
a	O
1000-state	B
version	O
of	O
the	O
random	B
walk	I
task	O
(	O
examples	O
6.2	O
and	O
7.1	O
on	O
pages	O
125	O
and	O
144	O
)	O
.	O
the	O
states	O
are	O
numbered	O
from	O
1	O
to	O
1000	O
,	O
left	O
to	O
right	O
,	O
and	O
all	O
episodes	B
begin	O
near	O
the	O
center	O
,	O
in	O
state	O
500.	O
state	B
transitions	O
are	O
from	O
the	O
current	O
state	B
to	O
one	O
of	O
the	O
100	O
neighboring	O
states	O
to	O
its	O
left	O
,	O
or	O
to	O
one	O
of	O
the	O
100	O
neighboring	O
states	O
to	O
its	O
right	O
,	O
all	O
with	O
equal	O
probability	O
.	O
of	O
course	O
,	O
if	O
the	O
current	O
state	B
is	O
near	O
an	O
edge	O
,	O
then	O
there	O
may	O
be	O
fewer	O
than	O
100	O
neighbors	O
on	O
that	O
side	O
of	O
it	O
.	O
in	O
this	O
case	O
,	O
all	O
the	O
probability	O
that	O
would	O
have	O
gone	O
into	O
those	O
missing	O
neighbors	O
goes	O
into	O
the	O
probability	O
of	O
terminating	O
on	O
that	O
side	O
(	O
thus	O
,	O
state	B
1	O
has	O
a	O
0.5	O
chance	O
of	O
terminating	O
on	O
the	O
left	O
,	O
and	O
state	O
950	O
has	O
a	O
0.25	O
chance	O
of	O
terminating	O
on	O
the	O
right	O
)	O
.	O
as	O
usual	O
,	O
termination	O
on	O
the	O
left	O
produces	O
a	O
reward	O
of	O
−1	O
,	O
and	O
termination	O
on	O
the	O
right	O
produces	O
a	O
reward	O
of	O
+1	O
.	O
all	O
other	O
transitions	O
have	O
a	O
reward	O
of	O
zero	O
.	O
we	O
use	O
this	O
task	O
as	O
a	O
running	O
example	O
throughout	O
this	O
section	O
.	O
figure	O
9.1	O
shows	O
the	O
true	O
value	O
function	O
vπ	O
for	O
this	O
task	O
.	O
it	O
is	O
nearly	O
a	O
straight	O
line	O
,	O
but	O
curving	O
slightly	O
toward	O
the	O
horizontal	O
for	O
the	O
last	O
100	O
states	O
at	O
each	O
end	O
.	O
also	O
shown	O
is	O
the	O
ﬁnal	O
approximate	O
value	B
function	I
learned	O
by	O
the	O
gradient	B
monte-carlo	O
algorithm	O
with	O
state	O
aggregation	O
after	O
100,000	O
episodes	B
with	O
a	O
step	O
size	O
of	O
α	O
=	O
2	O
×	O
10−5	O
.	O
for	O
the	O
state	B
aggregation	I
,	O
the	O
1000	O
states	O
were	O
partitioned	O
into	O
10	O
groups	O
of	O
100	O
states	O
each	O
(	O
i.e.	O
,	O
states	O
1–100	O
were	O
one	O
group	O
,	O
states	O
101–200	O
were	O
another	O
,	O
and	O
so	O
on	O
)	O
.	O
the	O
204	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
figure	O
9.1	O
:	O
function	B
approximation	I
by	O
state	B
aggregation	I
on	O
the	O
1000-state	B
random	O
walk	O
task	O
,	O
using	O
the	O
gradient	B
monte	O
carlo	O
algorithm	O
(	O
page	O
202	O
)	O
.	O
staircase	O
eﬀect	O
shown	O
in	O
the	O
ﬁgure	O
is	O
typical	O
of	O
state	O
aggregation	O
;	O
within	O
each	O
group	O
,	O
the	O
approximate	B
value	O
is	O
constant	O
,	O
and	O
it	O
changes	O
abruptly	O
from	O
one	O
group	O
to	O
the	O
next	O
.	O
these	O
approximate	B
values	O
are	O
close	O
to	O
the	O
global	O
minimum	O
of	O
the	O
ve	O
(	O
9.1	O
)	O
.	O
some	O
of	O
the	O
details	O
of	O
the	O
approximate	B
values	O
are	O
best	O
appreciated	O
by	O
reference	O
to	O
the	O
state	B
distribution	O
µ	O
for	O
this	O
task	O
,	O
shown	O
in	O
the	O
lower	O
portion	O
of	O
the	O
ﬁgure	O
with	O
a	O
right-	O
side	O
scale	O
.	O
state	B
500	O
,	O
in	O
the	O
center	O
,	O
is	O
the	O
ﬁrst	O
state	B
of	O
every	O
episode	O
,	O
but	O
is	O
rarely	O
visited	O
again	O
.	O
on	O
average	O
,	O
about	O
1.37	O
%	O
of	O
the	O
time	O
steps	O
are	O
spent	O
in	O
the	O
start	O
state	B
.	O
the	O
states	O
reachable	O
in	O
one	O
step	O
from	O
the	O
start	O
state	B
are	O
the	O
second	O
most	O
visited	O
,	O
with	O
about	O
0.17	O
%	O
of	O
the	O
time	O
steps	O
being	O
spent	O
in	O
each	O
of	O
them	O
.	O
from	O
there	O
µ	O
falls	O
oﬀ	O
almost	O
linearly	O
,	O
reaching	O
about	O
0.0147	O
%	O
at	O
the	O
extreme	O
states	O
1	O
and	O
1000.	O
the	O
most	O
visible	O
eﬀect	O
of	O
the	O
distribution	O
is	O
on	O
the	O
leftmost	O
groups	O
,	O
whose	O
values	O
are	O
clearly	O
shifted	O
higher	O
than	O
the	O
unweighted	O
average	O
of	O
the	O
true	O
values	O
of	O
states	O
within	O
the	O
group	O
,	O
and	O
on	O
the	O
rightmost	O
groups	O
,	O
whose	O
values	O
are	O
clearly	O
shifted	O
lower	O
.	O
this	O
is	O
due	O
to	O
the	O
states	O
in	O
these	O
areas	O
having	O
the	O
greatest	O
asymmetry	O
in	O
their	O
weightings	O
by	O
µ.	O
for	O
example	O
,	O
in	O
the	O
leftmost	O
group	O
,	O
state	B
100	O
is	O
weighted	O
more	O
than	O
3	O
times	O
more	O
strongly	O
than	O
state	B
1.	O
thus	O
the	O
estimate	O
for	O
the	O
group	O
is	O
biased	O
toward	O
the	O
true	O
value	O
of	O
state	O
100	O
,	O
which	O
is	O
higher	O
than	O
the	O
true	O
value	O
of	O
state	O
1	O
.	O
9.4	O
linear	O
methods	O
one	O
of	O
the	O
most	O
important	O
special	O
cases	O
of	O
function	O
approximation	O
is	O
that	O
in	O
which	O
the	O
approximate	B
function	O
,	O
ˆv	O
(	O
·	O
,	O
w	O
)	O
,	O
is	O
a	O
linear	O
function	O
of	O
the	O
weight	O
vector	B
,	O
w.	O
corresponding	O
.	O
=	O
(	O
x1	O
(	O
s	O
)	O
,	O
x2	O
(	O
s	O
)	O
,	O
.	O
.	O
.	O
,	O
xd	O
(	O
s	O
)	O
)	O
(	O
cid:62	O
)	O
,	O
with	O
the	O
to	O
every	O
state	B
s	O
,	O
there	O
is	O
a	O
real-valued	O
vector	B
x	O
(	O
s	O
)	O
same	O
number	O
of	O
components	O
as	O
w.	O
linear	O
methods	O
approximate	B
state-value	O
function	O
by	O
0statevaluescale	O
true	O
valuev⇡	O
approximate	B
mc	O
valueˆv	O
state	B
distribution	O
0.00170.0137distributionscale100010-11µ	O
9.4.	O
linear	O
methods	O
the	O
inner	O
product	O
between	O
w	O
and	O
x	O
(	O
s	O
)	O
:	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
.	O
=	O
w	O
(	O
cid:62	O
)	O
x	O
(	O
s	O
)	O
.	O
=	O
wixi	O
(	O
s	O
)	O
.	O
d	O
(	O
cid:88	O
)	O
i=1	O
205	O
(	O
9.8	O
)	O
in	O
this	O
case	O
the	O
approximate	B
value	O
function	O
is	O
said	O
to	O
be	O
linear	O
in	O
the	O
weights	O
,	O
or	O
simply	O
linear	O
.	O
the	O
vector	B
x	O
(	O
s	O
)	O
is	O
called	O
a	O
feature	O
vector	O
representing	O
state	B
s.	O
each	O
component	O
xi	O
(	O
s	O
)	O
of	O
x	O
(	O
s	O
)	O
is	O
the	O
value	B
of	O
a	O
function	O
xi	O
:	O
s	O
→	O
r.	O
we	O
think	O
of	O
a	O
feature	O
as	O
the	O
entirety	O
of	O
one	O
of	O
these	O
functions	O
,	O
and	O
we	O
call	O
its	O
value	B
for	O
a	O
state	B
s	O
a	O
feature	O
of	O
s.	O
for	O
linear	O
methods	O
,	O
features	O
are	O
basis	O
functions	O
because	O
they	O
form	O
a	O
linear	O
basis	O
for	O
the	O
set	O
of	O
approximate	O
functions	O
.	O
constructing	O
d-dimensional	O
feature	O
vectors	O
to	O
represent	O
states	O
is	O
the	O
same	O
as	O
selecting	O
a	O
set	O
of	O
d	O
basis	O
functions	O
.	O
features	O
may	O
be	O
deﬁned	O
in	O
many	O
diﬀerent	O
ways	O
;	O
we	O
cover	O
a	O
few	O
possibilities	O
in	O
the	O
next	O
sections	O
.	O
it	O
is	O
natural	O
to	O
use	O
sgd	O
updates	O
with	O
linear	O
function	B
approximation	I
.	O
the	O
gradient	B
of	O
the	O
approximate	B
value	O
function	O
with	O
respect	O
to	O
w	O
in	O
this	O
case	O
is	O
∇ˆv	O
(	O
s	O
,	O
w	O
)	O
=	O
x	O
(	O
s	O
)	O
.	O
thus	O
,	O
in	O
the	O
linear	O
case	O
the	O
general	O
sgd	O
update	O
(	O
9.7	O
)	O
reduces	O
to	O
a	O
particularly	O
simple	O
form	O
:	O
wt+1	O
.	O
=	O
wt	O
+	O
α	O
(	O
cid:104	O
)	O
ut	O
−	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
(	O
cid:105	O
)	O
x	O
(	O
st	O
)	O
.	O
because	O
it	O
is	O
so	O
simple	O
,	O
the	O
linear	O
sgd	O
case	O
is	O
one	O
of	O
the	O
most	O
favorable	O
for	O
mathematical	O
analysis	O
.	O
almost	O
all	O
useful	O
convergence	O
results	O
for	O
learning	O
systems	O
of	O
all	O
kinds	O
are	O
for	O
linear	O
(	O
or	O
simpler	O
)	O
function	B
approximation	I
methods	O
.	O
in	O
particular	O
,	O
in	O
the	O
linear	O
case	O
there	O
is	O
only	O
one	O
optimum	O
(	O
or	O
,	O
in	O
degenerate	O
cases	O
,	O
one	O
set	O
of	O
equally	O
good	O
optima	O
)	O
,	O
and	O
thus	O
any	O
method	O
that	O
is	O
guaranteed	O
to	O
converge	O
to	O
or	O
near	O
a	O
local	O
optimum	O
is	O
automatically	O
guaranteed	O
to	O
converge	O
to	O
or	O
near	O
the	O
global	O
optimum	O
.	O
for	O
example	O
,	O
the	O
gradient	B
monte	O
carlo	O
algorithm	O
presented	O
in	O
the	O
previous	O
section	O
converges	O
to	O
the	O
global	O
optimum	O
of	O
the	O
ve	O
under	O
linear	B
function	I
approximation	I
if	O
α	O
is	O
reduced	O
over	O
time	O
according	O
to	O
the	O
usual	O
conditions	O
.	O
the	O
semi-gradient	O
td	O
(	O
0	O
)	O
algorithm	O
presented	O
in	O
the	O
previous	O
section	O
also	O
converges	O
under	O
linear	B
function	I
approximation	I
,	O
but	O
this	O
does	O
not	O
follow	O
from	O
general	O
results	O
on	O
sgd	O
;	O
a	O
separate	O
theorem	B
is	O
necessary	O
.	O
the	O
weight	O
vector	B
converged	O
to	O
is	O
also	O
not	O
the	O
global	O
optimum	O
,	O
but	O
rather	O
a	O
point	O
near	O
the	O
local	O
optimum	O
.	O
it	O
is	O
useful	O
to	O
consider	O
this	O
important	O
case	O
in	O
more	O
detail	O
,	O
speciﬁcally	O
for	O
the	O
continuing	O
case	O
.	O
the	O
update	O
at	O
each	O
time	O
t	O
is	O
wt+1	O
.	O
=	O
wt	O
+	O
α	O
(	O
cid:16	O
)	O
rt+1	O
+	O
γw	O
(	O
cid:62	O
)	O
t	O
xt+1	O
−	O
w	O
(	O
cid:62	O
)	O
t	O
xt	O
(	O
cid:17	O
)	O
xt	O
=	O
wt	O
+	O
α	O
(	O
cid:16	O
)	O
rt+1xt	O
−	O
xt	O
(	O
cid:0	O
)	O
xt	O
−	O
γxt+1	O
(	O
cid:1	O
)	O
(	O
cid:62	O
)	O
wt	O
(	O
cid:17	O
)	O
,	O
where	O
here	O
we	O
have	O
used	O
the	O
notational	O
shorthand	O
xt	O
=	O
x	O
(	O
st	O
)	O
.	O
once	O
the	O
system	O
has	O
reached	O
steady	O
state	B
,	O
for	O
any	O
given	O
wt	O
,	O
the	O
expected	O
next	O
weight	O
vector	B
can	O
be	O
written	O
e	O
[	O
wt+1|wt	O
]	O
=	O
wt	O
+	O
α	O
(	O
b	O
−	O
awt	O
)	O
,	O
(	O
9.10	O
)	O
(	O
9.9	O
)	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
.	O
=	O
e	O
(	O
cid:104	O
)	O
xt	O
(	O
cid:0	O
)	O
xt	O
−	O
γxt+1	O
(	O
cid:1	O
)	O
(	O
cid:62	O
)	O
(	O
cid:105	O
)	O
∈	O
rd	O
×	O
rd	O
(	O
9.11	O
)	O
206	O
where	O
b	O
.	O
=	O
e	O
[	O
rt+1xt	O
]	O
∈	O
rd	O
and	O
a	O
from	O
(	O
9.10	O
)	O
it	O
is	O
clear	O
that	O
,	O
if	O
the	O
system	O
converges	O
,	O
it	O
must	O
converge	O
to	O
the	O
weight	O
vector	B
wtd	O
at	O
which	O
b	O
−	O
awtd	O
=	O
0	O
b	O
=	O
awtd	O
.	O
=	O
a−1b	O
.	O
wtd	O
⇒	O
⇒	O
(	O
9.12	O
)	O
this	O
quantity	O
is	O
called	O
the	O
td	O
ﬁxed	O
point	O
.	O
in	O
fact	O
linear	O
semi-gradient	O
td	O
(	O
0	O
)	O
converges	O
to	O
this	O
point	O
.	O
some	O
of	O
the	O
theory	O
proving	O
its	O
convergence	O
,	O
and	O
the	O
existence	O
of	O
the	O
inverse	O
above	O
,	O
is	O
given	O
in	O
the	O
box	O
.	O
proof	B
of	O
convergence	O
of	O
linear	O
td	O
(	O
0	O
)	O
what	O
properties	O
assure	O
convergence	O
of	O
the	O
linear	O
td	O
(	O
0	O
)	O
algorithm	O
(	O
9.9	O
)	O
?	O
some	O
insight	O
can	O
be	O
gained	O
by	O
rewriting	O
(	O
9.10	O
)	O
as	O
e	O
[	O
wt+1|wt	O
]	O
=	O
(	O
i	O
−	O
αa	O
)	O
wt	O
+	O
αb	O
.	O
(	O
9.13	O
)	O
note	O
that	O
the	O
matrix	O
a	O
multiplies	O
the	O
weight	O
vector	B
wt	O
and	O
not	O
b	O
;	O
only	O
a	O
is	O
important	O
to	O
convergence	O
.	O
to	O
develop	O
intuition	O
,	O
consider	O
the	O
special	O
case	O
in	O
which	O
a	O
is	O
a	O
diagonal	O
matrix	O
.	O
if	O
any	O
of	O
the	O
diagonal	O
elements	O
are	O
negative	O
,	O
then	O
the	O
corresponding	O
diagonal	O
element	O
of	O
i	O
−	O
αa	O
will	O
be	O
greater	O
than	O
one	O
,	O
and	O
the	O
cor-	O
responding	O
component	O
of	O
wt	O
will	O
be	O
ampliﬁed	O
,	O
which	O
will	O
lead	O
to	O
divergence	O
if	O
continued	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
the	O
diagonal	O
elements	O
of	O
a	O
are	O
all	O
positive	O
,	O
then	O
α	O
can	O
be	O
chosen	O
smaller	O
than	O
one	O
over	O
the	O
largest	O
of	O
them	O
,	O
such	O
that	O
i	O
−	O
αa	O
is	O
diagonal	O
with	O
all	O
diagonal	O
elements	O
between	O
0	O
and	O
1.	O
in	O
this	O
case	O
the	O
ﬁrst	O
term	O
of	O
the	O
update	O
tends	O
to	O
shrink	O
wt	O
,	O
and	B
stability	I
is	O
assured	O
.	O
in	O
general	O
case	O
,	O
wt	O
will	O
be	O
reduced	O
toward	O
zero	O
whenever	O
a	O
is	O
positive	O
deﬁnite	O
,	O
meaning	O
y	O
(	O
cid:62	O
)	O
ay	O
>	O
0	O
for	O
real	O
vector	B
y.	O
positive	O
deﬁniteness	O
also	O
ensures	O
that	O
the	O
inverse	O
a−1	O
exists	O
.	O
for	O
linear	O
td	O
(	O
0	O
)	O
,	O
in	O
the	O
continuing	O
case	O
with	O
γ	O
<	O
1	O
,	O
the	O
a	O
matrix	O
(	O
9.11	O
)	O
can	O
be	O
written	O
p	O
(	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
a	O
)	O
x	O
(	O
s	O
)	O
(	O
cid:0	O
)	O
x	O
(	O
s	O
)	O
−	O
γx	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:1	O
)	O
(	O
cid:62	O
)	O
µ	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
a	O
µ	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
µ	O
(	O
s	O
)	O
x	O
(	O
s	O
)	O
(	O
cid:18	O
)	O
x	O
(	O
s	O
)	O
−	O
γ	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
π	O
(	O
a|s	O
)	O
(	O
cid:88	O
)	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
)	O
x	O
(	O
s	O
)	O
(	O
cid:0	O
)	O
x	O
(	O
s	O
)	O
−	O
γx	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:1	O
)	O
(	O
cid:62	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
)	O
x	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:19	O
)	O
(	O
cid:62	O
)	O
a	O
=	O
(	O
cid:88	O
)	O
s	O
=	O
(	O
cid:88	O
)	O
s	O
=	O
(	O
cid:88	O
)	O
s	O
=	O
x	O
(	O
cid:62	O
)	O
d	O
(	O
i	O
−	O
γp	O
)	O
x	O
,	O
where	O
µ	O
(	O
s	O
)	O
is	O
the	O
stationary	O
distribution	O
under	O
π	O
,	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
)	O
is	O
the	O
probability	O
of	O
9.4.	O
linear	O
methods	O
207	O
transition	O
from	O
s	O
to	O
s	O
(	O
cid:48	O
)	O
under	O
policy	B
π	O
,	O
p	O
is	O
the	O
|s|×|s|	O
matrix	O
of	O
these	O
probabilities	O
,	O
d	O
is	O
the	O
|s|×|s|	O
diagonal	O
matrix	O
with	O
the	O
µ	O
(	O
s	O
)	O
on	O
its	O
diagonal	O
,	O
and	O
x	O
is	O
the	O
|s|×	O
d	O
matrix	O
with	O
x	O
(	O
s	O
)	O
as	O
its	O
rows	O
.	O
from	O
here	O
it	O
is	O
clear	O
that	O
the	O
inner	O
matrix	O
d	O
(	O
i−	O
γp	O
)	O
is	O
key	O
to	O
determining	O
the	O
positive	O
deﬁniteness	O
of	O
a.	O
for	O
a	O
key	O
matrix	O
of	O
this	O
type	O
,	O
positive	O
deﬁniteness	O
is	O
assured	O
if	O
all	O
of	O
its	O
columns	O
sum	O
to	O
a	O
nonnegative	O
number	O
.	O
this	O
was	O
shown	O
by	O
sutton	O
(	O
1988	O
,	O
p.	O
27	O
)	O
based	O
on	O
two	O
previously	O
established	O
theorems	O
.	O
one	O
theorem	B
says	O
that	O
any	O
matrix	O
m	O
is	O
positive	O
deﬁnite	O
if	O
and	O
only	O
if	O
the	O
symmetric	O
matrix	O
s	O
=	O
m+m	O
(	O
cid:62	O
)	O
is	O
positive	O
deﬁnite	O
(	O
sutton	O
1988	O
,	O
appendix	O
)	O
.	O
the	O
second	O
theorem	B
says	O
that	O
any	O
symmetric	O
real	O
matrix	O
s	O
is	O
positive	O
deﬁnite	O
if	O
all	O
of	O
its	O
diagonal	O
entries	O
are	O
positive	O
and	O
greater	O
than	O
the	O
sum	O
of	O
the	O
absolute	O
values	O
of	O
the	O
corresponding	O
oﬀ-diagonal	O
entries	O
(	O
varga	O
1962	O
,	O
p.	O
23	O
)	O
.	O
for	O
our	O
key	O
matrix	O
,	O
d	O
(	O
i	O
−	O
γp	O
)	O
,	O
the	O
diagonal	O
entries	O
are	O
positive	O
and	O
the	O
oﬀ-diagonal	O
entries	O
are	O
negative	O
,	O
so	O
all	O
we	O
have	O
to	O
show	O
is	O
that	O
each	O
row	O
sum	O
plus	O
the	O
corresponding	O
column	O
sum	O
is	O
positive	O
.	O
the	O
row	O
sums	O
are	O
all	O
positive	O
because	O
p	O
is	O
a	O
stochastic	O
matrix	O
and	O
γ	O
<	O
1.	O
thus	O
it	O
only	O
remains	O
to	O
show	O
that	O
the	O
column	O
sums	O
are	O
nonnegative	O
.	O
note	O
that	O
the	O
row	O
vector	B
of	O
the	O
column	O
sums	O
of	O
any	O
matrix	O
m	O
can	O
be	O
written	O
as	O
1	O
(	O
cid:62	O
)	O
m	O
,	O
where	O
1	O
is	O
the	O
column	O
vector	B
with	O
all	O
components	O
equal	O
to	O
1.	O
let	O
µ	O
denote	O
the	O
|s|-vector	O
of	O
the	O
µ	O
(	O
s	O
)	O
,	O
where	O
µ	O
=	O
p	O
(	O
cid:62	O
)	O
µ	O
by	O
virtue	O
of	O
µ	O
being	O
the	O
stationary	O
distribution	O
.	O
the	O
column	O
sums	O
of	O
our	O
key	O
matrix	O
,	O
then	O
,	O
are	O
:	O
1	O
(	O
cid:62	O
)	O
d	O
(	O
i	O
−	O
γp	O
)	O
=	O
µ	O
(	O
cid:62	O
)	O
(	O
i	O
−	O
γp	O
)	O
=	O
µ	O
(	O
cid:62	O
)	O
−	O
γµ	O
(	O
cid:62	O
)	O
p	O
=	O
µ	O
(	O
cid:62	O
)	O
−	O
γµ	O
(	O
cid:62	O
)	O
=	O
(	O
1	O
−	O
γ	O
)	O
µ	O
(	O
cid:62	O
)	O
,	O
(	O
because	O
µ	O
is	O
the	O
stationary	O
distribution	O
)	O
all	O
components	O
of	O
which	O
are	O
positive	O
.	O
thus	O
,	O
the	O
key	O
matrix	O
and	O
its	O
a	O
matrix	O
are	O
positive	O
deﬁnite	O
,	O
and	O
on-policy	O
td	O
(	O
0	O
)	O
is	O
stable	O
.	O
(	O
additional	O
conditions	O
and	O
a	O
schedule	O
for	O
reducing	O
α	O
over	O
time	O
are	O
needed	O
to	O
prove	O
convergence	O
with	O
probability	O
one	O
.	O
)	O
at	O
the	O
td	O
ﬁxed	O
point	O
,	O
it	O
has	O
also	O
been	O
proven	O
(	O
in	O
the	O
continuing	O
case	O
)	O
that	O
the	O
ve	O
is	O
within	O
a	O
bounded	O
expansion	O
of	O
the	O
lowest	O
possible	O
error	O
:	O
ve	O
(	O
wtd	O
)	O
≤	O
1	O
1	O
−	O
γ	O
min	O
w	O
ve	O
(	O
w	O
)	O
.	O
(	O
9.14	O
)	O
that	O
is	O
,	O
the	O
asymptotic	O
error	O
of	O
the	O
td	O
method	O
is	O
no	O
more	O
than	O
1	O
1−γ	O
times	O
the	O
smallest	O
possible	O
error	O
,	O
that	O
attained	O
in	O
the	O
limit	O
by	O
the	O
monte	O
carlo	O
method	O
.	O
because	O
γ	O
is	O
often	O
near	O
one	O
,	O
this	O
expansion	O
factor	O
can	O
be	O
quite	O
large	O
,	O
so	O
there	O
is	O
substantial	O
potential	O
loss	O
in	O
asymptotic	O
performance	O
with	O
the	O
td	O
method	O
.	O
on	O
the	O
other	O
hand	O
,	O
recall	O
that	O
the	O
td	O
methods	O
are	O
often	O
of	O
vastly	O
reduced	O
variance	O
compared	O
to	O
monte	O
carlo	O
methods	O
,	O
and	O
thus	O
faster	O
,	O
as	O
we	O
saw	O
in	O
chapters	O
6	O
and	O
7.	O
which	O
method	O
will	O
be	O
best	O
depends	O
on	O
the	O
nature	O
of	O
the	O
approximation	O
and	O
problem	O
,	O
and	O
on	O
how	O
long	O
learning	O
continues	O
.	O
208	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
a	O
bound	O
analogous	O
to	O
(	O
9.14	O
)	O
applies	O
to	O
other	O
on-policy	O
bootstrapping	O
methods	O
as	O
well	O
.	O
.	O
for	O
example	O
,	O
linear	O
semi-gradient	O
dp	O
(	O
eq	O
.	O
9.7	O
with	O
ut	O
γˆv	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
wt	O
)	O
]	O
)	O
with	O
updates	O
according	O
to	O
the	O
on-policy	B
distribution	I
will	O
also	O
converge	O
to	O
the	O
td	O
ﬁxed	O
point	O
.	O
one-step	O
semi-gradient	O
action-value	B
methods	I
,	O
such	O
as	O
semi-gradient	O
sarsa	O
(	O
0	O
)	O
covered	O
in	O
the	O
next	O
chapter	O
converge	O
to	O
an	O
analogous	O
ﬁxed	O
point	O
and	O
an	O
anal-	O
ogous	O
bound	O
.	O
for	O
episodic	O
tasks	O
,	O
there	O
is	O
a	O
slightly	O
diﬀerent	O
but	O
related	O
bound	O
(	O
see	O
bertsekas	O
and	O
tsitsiklis	O
,	O
1996	O
)	O
.	O
there	O
are	O
also	O
a	O
few	O
technical	O
conditions	O
on	O
the	O
re-	O
wards	O
,	O
features	O
,	O
and	O
decrease	O
in	O
the	O
step-size	O
parameter	O
,	O
which	O
we	O
have	O
omitted	O
here	O
.	O
the	O
full	O
details	O
can	O
be	O
found	O
in	O
the	O
original	O
paper	O
(	O
tsitsiklis	O
and	O
van	O
roy	O
,	O
1997	O
)	O
.	O
=	O
(	O
cid:80	O
)	O
a	O
π	O
(	O
a|st	O
)	O
(	O
cid:80	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|st	O
,	O
a	O
)	O
[	O
r+	O
critical	O
to	O
the	O
these	O
convergence	O
results	O
is	O
that	O
states	O
are	O
updated	O
according	O
to	O
the	O
on-policy	B
distribution	I
.	O
for	O
other	O
update	O
distributions	O
,	O
bootstrapping	B
methods	O
using	O
function	B
approximation	I
may	O
actually	O
diverge	O
to	O
inﬁnity	O
.	O
examples	O
of	O
this	O
and	O
a	O
discus-	O
sion	O
of	O
possible	O
solution	O
methods	O
are	O
given	O
in	O
chapter	O
11.	O
example	O
9.2	O
:	O
bootstrapping	B
on	O
the	O
1000-state	B
random	O
walk	O
state	B
aggregation	I
is	O
a	O
special	O
case	O
of	O
linear	O
function	B
approximation	I
,	O
so	O
let	O
’	O
s	O
return	B
to	O
the	O
1000-state	B
random	O
walk	O
to	O
illustrate	O
some	O
of	O
the	O
observations	O
made	O
in	O
this	O
chapter	O
.	O
the	O
left	O
panel	O
of	O
figure	O
9.2	O
shows	O
the	O
ﬁnal	O
value	O
function	O
learned	O
by	O
the	O
semi-gradient	O
td	O
(	O
0	O
)	O
algorithm	O
(	O
page	O
203	O
)	O
using	O
the	O
same	O
state	B
aggregation	I
as	O
in	O
example	O
9.1.	O
we	O
see	O
that	O
the	O
near-asymptotic	O
td	O
approximation	O
is	O
indeed	O
farther	O
from	O
the	O
true	O
values	O
than	O
the	O
monte	O
carlo	O
approximation	O
shown	O
in	O
figure	O
9.1.	O
nevertheless	O
,	O
td	O
methods	O
retain	O
large	O
potential	O
advantages	O
in	O
learning	O
rate	O
,	O
and	O
generalize	O
monte	O
carlo	O
methods	O
,	O
as	O
we	O
investigated	O
fully	O
with	O
n-step	O
td	O
methods	O
in	O
chapter	O
7.	O
the	O
right	O
panel	O
of	O
figure	O
9.2	O
shows	O
results	O
with	O
an	O
n-step	B
semi-gradient	O
td	O
method	O
using	O
state	B
aggregation	I
on	O
the	O
1000-state	B
random	O
walk	O
that	O
are	O
strikingly	O
similar	O
to	O
those	O
we	O
obtained	O
earlier	O
with	O
tabular	O
methods	O
and	O
the	O
19-state	O
random	B
walk	I
(	O
figure	O
7.2	O
)	O
.	O
to	O
obtain	O
such	O
quantitatively	O
similar	O
results	O
we	O
switched	O
the	O
state	B
aggregation	I
to	O
20	O
groups	O
of	O
50	O
states	O
each	O
.	O
the	O
20	O
groups	O
were	O
then	O
quantitatively	O
close	O
figure	O
9.2	O
:	O
bootstrapping	B
with	O
state	B
aggregation	I
on	O
the	O
1000-state	B
random	O
walk	O
task	O
.	O
left	O
:	O
asymptotic	O
values	O
of	O
semi-gradient	O
td	O
are	O
worse	O
than	O
the	O
asymptotic	O
monte	O
carlo	O
values	O
in	O
figure	O
9.1.	O
right	O
:	O
performance	O
of	O
n-step	O
methods	O
with	O
state-aggregation	O
are	O
strikingly	O
similar	O
to	O
those	O
with	O
tabular	O
representations	O
(	O
cf	O
.	O
figure	O
7.2	O
)	O
.	O
these	O
data	O
are	O
averages	O
over	O
100	O
runs	O
.	O
0.550.50.450.350.30.250.40.40.200.80.61↵averagerms	O
errorover	O
1000	O
statesand	O
ﬁrst	O
10	O
episodesn=1n=2n=4n=8n=16n=32n=64128512256state	O
true	O
valuev⇡	O
approximate	B
td	O
value10-111000ˆv	O
9.5.	O
feature	B
construction	I
for	O
linear	O
methods	O
209	O
to	O
the	O
19	O
states	O
of	O
the	O
tabular	O
problem	O
.	O
in	O
particular	O
,	O
recall	O
that	O
state	B
transitions	O
were	O
up	O
to	O
100	O
states	O
to	O
the	O
left	O
or	O
right	O
.	O
a	O
typical	O
transition	O
would	O
then	O
be	O
of	O
50	O
states	O
to	O
the	O
right	O
or	O
left	O
,	O
which	O
is	O
quantitively	O
analogous	O
to	O
the	O
single-state	O
state	B
transitions	O
of	O
the	O
19-state	B
tabular	O
system	O
.	O
to	O
complete	O
the	O
match	O
,	O
we	O
use	O
here	O
the	O
same	O
performance	O
measure—an	O
unweighted	O
average	O
of	O
the	O
rms	O
error	O
over	O
all	O
states	O
and	O
over	O
the	O
ﬁrst	O
10	O
episodes—rather	O
than	O
a	O
ve	O
objective	O
as	O
is	O
otherwise	O
more	O
appropriate	O
when	O
using	O
function	B
approximation	I
.	O
the	O
semi-gradient	O
n-step	O
td	O
algorithm	O
used	O
in	O
this	O
example	O
is	O
the	O
natural	O
extension	O
of	O
the	O
tabular	O
n-step	O
td	O
algorithm	O
presented	O
in	O
chapter	O
7	O
to	O
semi-gradient	O
function	O
approximation	O
.	O
pseudocode	O
is	O
given	O
in	O
the	O
box	O
below	O
.	O
n-step	B
semi-gradient	O
td	O
for	O
estimating	O
ˆv	O
≈	O
vπ	O
input	O
:	O
the	O
policy	B
π	O
to	O
be	O
evaluated	O
input	O
:	O
a	O
diﬀerentiable	O
function	O
ˆv	O
:	O
s+	O
×	O
rd	O
→	O
r	O
such	O
that	O
ˆv	O
(	O
terminal	O
,	O
·	O
)	O
=	O
0	O
algorithm	O
parameters	O
:	O
step	O
size	O
α	O
>	O
0	O
,	O
a	O
positive	O
integer	O
n	O
initialize	O
value-function	O
weights	O
w	O
arbitrarily	O
(	O
e.g.	O
,	O
w	O
=	O
0	O
)	O
all	O
store	O
and	O
access	O
operations	O
(	O
st	O
and	O
rt	O
)	O
can	O
take	O
their	O
index	O
mod	O
n	O
+	O
1	O
loop	O
for	O
each	O
episode	O
:	O
if	O
t	O
<	O
t	O
,	O
then	O
:	O
initialize	O
and	O
store	O
s0	O
(	O
cid:54	O
)	O
=	O
terminal	O
t	O
←	O
∞loop	O
for	O
t	O
=	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
:	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
until	O
τ	O
=	O
t	O
−	O
1	O
g	O
←	O
(	O
cid:80	O
)	O
min	O
(	O
τ	O
+n	O
,	O
t	O
)	O
τ	O
←	O
t	O
−	O
n	O
+	O
1	O
if	O
τ	O
≥	O
0	O
:	O
i=τ	O
+1	O
γi−τ−1ri	O
if	O
τ	O
+	O
n	O
<	O
t	O
,	O
then	O
:	O
g	O
←	O
g	O
+	O
γnˆv	O
(	O
sτ	O
+n	O
,	O
w	O
)	O
w	O
←	O
w	O
+	O
α	O
[	O
g	O
−	O
ˆv	O
(	O
sτ	O
,	O
w	O
)	O
]	O
∇ˆv	O
(	O
sτ	O
,	O
w	O
)	O
take	O
an	O
action	B
according	O
to	O
π	O
(	O
·|st	O
)	O
observe	O
and	O
store	O
the	O
next	O
reward	O
as	O
rt+1	O
and	O
the	O
next	O
state	B
as	O
st+1	O
if	O
st+1	O
is	O
terminal	O
,	O
then	O
t	O
←	O
t	O
+	O
1	O
(	O
τ	O
is	O
the	O
time	O
whose	O
state	B
’	O
s	O
estimate	O
is	O
being	O
updated	O
)	O
(	O
gτ	O
:	O
τ	O
+n	O
)	O
the	O
key	O
equation	O
of	O
this	O
algorithm	O
,	O
analogous	O
to	O
(	O
7.2	O
)	O
,	O
is	O
wt+n	O
.	O
=	O
wt+n−1	O
+	O
α	O
[	O
gt	O
:	O
t+n	O
−	O
ˆv	O
(	O
st	O
,	O
wt+n−1	O
)	O
]	O
∇ˆv	O
(	O
st	O
,	O
wt+n−1	O
)	O
,	O
0	O
≤	O
t	O
<	O
t	O
,	O
(	O
9.15	O
)	O
where	O
the	O
n-step	B
return	O
is	O
generalized	O
from	O
(	O
7.1	O
)	O
to	O
gt	O
:	O
t+n	O
.	O
=	O
rt+1	O
+	O
γrt+2	O
+···	O
+	O
γn−1rt+n	O
+	O
γnˆv	O
(	O
st+n	O
,	O
wt+n−1	O
)	O
,	O
0	O
≤	O
t	O
≤	O
t	O
−	O
n.	O
(	O
9.16	O
)	O
exercise	O
9.1	O
show	O
that	O
tabular	O
methods	O
such	O
as	O
presented	O
in	O
part	O
i	O
of	O
this	O
book	O
are	O
a	O
special	O
case	O
of	O
linear	O
function	B
approximation	I
.	O
what	O
would	O
the	O
feature	O
vectors	O
be	O
?	O
(	O
cid:3	O
)	O
210	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
9.5	O
feature	B
construction	I
for	O
linear	O
methods	O
linear	O
methods	O
are	O
interesting	O
because	O
of	O
their	O
convergence	O
guarantees	O
,	O
but	O
also	O
because	O
in	O
practice	O
they	O
can	O
be	O
very	O
eﬃcient	O
in	O
terms	O
of	O
both	O
data	O
and	O
computation	O
.	O
whether	O
or	O
not	O
this	O
is	O
so	O
depends	O
critically	O
on	O
how	O
the	O
states	O
are	O
represented	O
in	O
terms	O
of	O
features	O
,	O
which	O
we	O
investigate	O
in	O
this	O
large	O
section	O
.	O
choosing	O
features	O
appropriate	O
to	O
the	O
task	O
is	O
an	O
important	O
way	O
of	O
adding	O
prior	O
domain	O
knowledge	O
to	O
reinforcement	B
learning	I
systems	O
.	O
intuitively	O
,	O
the	O
features	O
should	O
correspond	O
to	O
the	O
aspects	O
of	O
the	O
state	B
space	O
along	O
which	O
generalization	O
may	O
be	O
appropriate	O
.	O
if	O
we	O
are	O
valuing	O
geometric	O
objects	O
,	O
for	O
example	O
,	O
we	O
might	O
want	O
to	O
have	O
features	O
for	O
each	O
possible	O
shape	O
,	O
color	O
,	O
size	O
,	O
or	O
function	O
.	O
if	O
we	O
are	O
valuing	O
states	O
of	O
a	O
mobile	O
robot	O
,	O
then	O
we	O
might	O
want	O
to	O
have	O
features	O
for	O
locations	O
,	O
degrees	O
of	O
remaining	O
battery	O
power	O
,	O
recent	O
sonar	O
readings	O
,	O
and	O
so	O
on	O
.	O
a	O
limitation	O
of	O
the	O
linear	O
form	O
is	O
that	O
it	O
can	O
not	O
take	O
into	O
account	O
any	O
interactions	O
between	O
features	O
,	O
such	O
as	O
the	O
presence	O
of	O
feature	O
i	O
being	O
good	O
only	O
in	O
the	O
absence	O
of	O
feature	O
j.	O
for	O
example	O
,	O
in	O
the	O
pole-balancing	O
task	O
(	O
example	O
3.4	O
,	O
high	O
angular	O
velocity	O
can	O
be	O
either	O
good	O
or	O
bad	O
depending	O
on	O
the	O
angle	O
.	O
if	O
the	O
angle	O
is	O
high	O
,	O
then	O
high	O
angular	O
velocity	O
means	O
an	O
imminent	O
danger	O
of	O
falling—a	O
bad	O
state—whereas	O
if	O
the	O
angle	O
is	O
low	O
,	O
then	O
high	O
angular	O
velocity	O
means	O
the	O
pole	O
is	O
righting	O
itself—a	O
good	O
state	B
.	O
a	O
linear	O
value	O
function	O
could	O
not	O
represent	O
this	O
if	O
its	O
features	O
coded	O
separately	O
for	O
the	O
angle	O
and	O
the	O
angular	O
velocity	O
.	O
it	O
needs	O
instead	O
,	O
or	O
in	O
addition	O
,	O
features	O
for	O
combinations	O
of	O
these	O
two	O
underlying	O
state	B
dimensions	O
.	O
in	O
the	O
following	O
subsections	O
we	O
consider	O
a	O
variety	O
of	O
general	O
ways	O
of	O
doing	O
this	O
.	O
9.5.1	O
polynomials	O
the	O
states	O
of	O
many	O
problems	O
are	O
initially	O
expressed	O
as	O
numbers	O
,	O
such	O
as	O
positions	O
and	O
velocities	O
in	O
the	O
pole-balancing	O
task	O
(	O
example	O
3.4	O
)	O
,	O
the	O
number	O
of	O
cars	O
in	O
each	O
lot	O
in	O
the	O
jack	O
’	O
s	O
car	O
rental	O
problem	O
(	O
example	O
4.2	O
)	O
,	O
or	O
the	O
gambler	O
’	O
s	O
capital	O
in	O
the	O
gambler	O
problem	O
(	O
example	O
4.3	O
)	O
.	O
in	O
these	O
types	O
of	O
problems	O
,	O
function	B
approximation	I
for	O
reinforcement	B
learning	I
has	O
much	O
in	O
common	O
with	O
the	O
familiar	O
tasks	O
of	O
interpolation	O
and	O
regression	O
.	O
various	O
families	O
of	O
features	O
commonly	O
used	O
for	O
interpolation	O
and	O
regression	O
can	O
also	O
be	O
used	O
in	O
reinforcement	O
learning	O
.	O
polynomials	O
make	O
up	O
one	O
of	O
the	O
simplest	O
families	O
of	O
features	O
used	O
for	O
interpolation	O
and	O
regression	O
.	O
while	O
the	O
basic	O
polynomial	O
features	O
we	O
discuss	O
here	O
do	O
not	O
work	O
as	O
well	O
as	O
other	O
types	O
of	O
features	O
in	O
reinforcement	O
learning	O
,	O
they	O
serve	O
as	O
a	O
good	O
introduction	O
because	O
they	O
are	O
simple	O
and	O
familiar	O
.	O
as	O
an	O
example	O
,	O
suppose	O
a	O
reinforcement	B
learning	I
problem	O
has	O
states	O
with	O
two	O
nu-	O
merical	O
dimensions	O
.	O
for	O
a	O
single	O
representative	O
state	B
s	O
,	O
let	O
its	O
two	O
numbers	O
be	O
s1	O
∈	O
r	O
and	O
s2	O
∈	O
r.	O
you	O
might	O
choose	O
to	O
represent	O
s	O
simply	O
by	O
its	O
two	O
state	B
dimensions	O
,	O
so	O
that	O
x	O
(	O
s	O
)	O
=	O
(	O
s1	O
,	O
s2	O
)	O
(	O
cid:62	O
)	O
,	O
but	O
then	O
you	O
would	O
not	O
be	O
able	O
to	O
take	O
into	O
account	O
any	O
inter-	O
actions	O
between	O
these	O
dimensions	O
.	O
in	O
addition	O
,	O
if	O
both	O
s1	O
and	O
s2	O
were	O
zero	O
,	O
then	O
the	O
approximate	B
value	O
would	O
have	O
to	O
also	O
be	O
zero	O
.	O
both	O
limitations	O
can	O
be	O
overcome	O
by	O
instead	O
representing	O
s	O
by	O
the	O
four-dimensional	O
feature	O
vector	O
x	O
(	O
s	O
)	O
=	O
(	O
1	O
,	O
s1	O
,	O
s2	O
,	O
s1s2	O
)	O
(	O
cid:62	O
)	O
.	O
the	O
initial	O
1	O
feature	O
allows	O
the	O
representation	O
of	O
aﬃne	O
functions	O
in	O
the	O
original	O
state	B
numbers	O
,	O
and	O
the	O
ﬁnal	O
product	O
feature	O
,	O
s1s2	O
,	O
enables	O
interactions	O
to	O
be	O
taken	O
into	O
account	O
.	O
or	O
you	O
might	O
choose	O
to	O
use	O
higher-dimensional	O
feature	O
vectors	O
like	O
x	O
(	O
s	O
)	O
=	O
9.5.	O
feature	B
construction	I
for	O
linear	O
methods	O
211	O
1	O
,	O
s2	O
2	O
,	O
s2	O
1s2	O
,	O
s2	O
2	O
,	O
s1s2	O
2	O
)	O
(	O
cid:62	O
)	O
to	O
take	O
more	O
complex	O
interactions	O
into	O
account	O
.	O
(	O
1	O
,	O
s1	O
,	O
s2	O
,	O
s1s2	O
,	O
s2	O
such	O
feature	O
vectors	O
enable	O
approximations	O
as	O
arbitrary	O
quadratic	O
functions	O
of	O
the	O
state	B
numbers—even	O
though	O
the	O
approximation	O
is	O
still	O
linear	O
in	O
the	O
weights	O
that	O
have	O
to	O
be	O
learned	O
.	O
generalizing	O
this	O
example	O
from	O
two	O
to	O
k	O
numbers	O
,	O
we	O
can	O
represent	O
highly-	O
complex	O
interactions	O
among	O
a	O
problem	O
’	O
s	O
state	B
dimensions	O
:	O
1s2	O
suppose	O
each	O
state	B
s	O
corresponds	O
to	O
k	O
numbers	O
,	O
s1	O
,	O
s2	O
,	O
...	O
,	O
sk	O
,	O
with	O
each	O
si	O
∈	O
r.	O
for	O
this	O
k-dimensional	O
state	B
space	O
,	O
each	O
order-n	O
polynomial-basis	O
feature	O
xi	O
can	O
be	O
written	O
as	O
xi	O
(	O
s	O
)	O
=	O
πk	O
j=1sci	O
,	O
j	O
j	O
,	O
(	O
9.17	O
)	O
where	O
each	O
ci	O
,	O
j	O
is	O
an	O
integer	O
in	O
the	O
set	O
{	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
for	O
an	O
integer	O
n	O
≥	O
0.	O
these	O
features	O
make	O
up	O
the	O
order-n	O
polynomial	B
basis	I
for	O
dimension	O
k	O
,	O
which	O
contains	O
(	O
n	O
+	O
1	O
)	O
k	O
diﬀerent	O
features	O
.	O
higher-order	O
polynomial	O
bases	O
allow	O
for	O
more	O
accurate	O
approximations	O
of	O
more	O
com-	O
plicated	O
functions	O
.	O
but	O
because	O
the	O
number	O
of	O
features	O
in	O
an	O
order-n	O
polynomial	B
basis	I
grows	O
exponentially	O
with	O
the	O
dimension	O
k	O
of	O
the	O
natural	O
state	B
space	O
(	O
if	O
n	O
>	O
0	O
)	O
,	O
it	O
is	O
generally	O
necessary	O
to	O
select	O
a	O
subset	O
of	O
them	O
for	O
function	O
approximation	O
.	O
this	O
can	O
be	O
done	O
using	O
prior	O
beliefs	O
about	O
the	O
nature	O
of	O
the	O
function	O
to	O
be	O
approximated	O
,	O
and	O
some	O
automated	O
selection	O
methods	O
developed	O
for	O
polynomial	O
regression	O
can	O
be	O
adapted	O
to	O
deal	O
with	O
the	O
incremental	O
and	O
nonstationary	O
nature	O
of	O
reinforcement	O
learning	O
.	O
exercise	O
9.2	O
why	O
does	O
(	O
9.17	O
)	O
deﬁne	O
(	O
n	O
+	O
1	O
)	O
k	O
distinct	O
features	O
for	O
dimension	O
k	O
?	O
exercise	O
9.3	O
what	O
n	O
and	O
ci	O
,	O
j	O
produce	O
the	O
feature	O
vectors	O
x	O
(	O
s	O
)	O
=	O
(	O
1	O
,	O
s1	O
,	O
s2	O
,	O
s1s2	O
,	O
s2	O
s1s2	O
(	O
cid:3	O
)	O
1	O
,	O
s2	O
2	O
,	O
(	O
cid:3	O
)	O
1s2	O
,	O
s2	O
2	O
)	O
(	O
cid:62	O
)	O
?	O
2	O
,	O
s2	O
1s2	O
9.5.2	O
fourier	O
basis	O
another	O
linear	B
function	I
approximation	I
method	O
is	O
based	O
on	O
the	O
time-honored	O
fourier	O
series	O
,	O
which	O
expresses	O
periodic	O
functions	O
as	O
weighted	O
sums	O
of	O
sine	O
and	O
cosine	O
basis	O
functions	O
(	O
features	O
)	O
of	O
diﬀerent	O
frequencies	O
.	O
(	O
a	O
function	O
f	O
is	O
periodic	O
if	O
f	O
(	O
x	O
)	O
=	O
f	O
(	O
x	O
+	O
τ	O
)	O
for	O
all	O
x	O
and	O
some	O
period	O
τ	O
.	O
)	O
the	O
fourier	O
series	O
and	O
the	O
more	O
general	O
fourier	O
transform	O
are	O
widely	O
used	O
in	O
applied	O
sciences	O
in	O
part	O
because	O
if	O
a	O
function	O
to	O
be	O
approximated	O
is	O
known	O
,	O
then	O
the	O
basis	O
function	O
weights	O
are	O
given	O
by	O
simple	O
formulae	O
and	O
,	O
further	O
,	O
with	O
enough	O
basis	O
functions	O
essentially	O
any	O
function	O
can	O
be	O
approximated	O
as	O
accurately	O
as	O
desired	O
.	O
in	O
reinforcement	O
learning	O
,	O
where	O
the	O
functions	O
to	O
be	O
approximated	O
are	O
unknown	O
,	O
fourier	O
basis	O
functions	O
are	O
of	O
interest	O
because	O
they	O
are	O
easy	O
to	O
use	O
and	O
can	O
perform	O
well	O
in	O
a	O
range	O
of	O
reinforcement	O
learning	O
problems	O
.	O
first	O
consider	O
the	O
one-dimensional	O
case	O
.	O
the	O
usual	O
fourier	O
series	O
representation	O
of	O
a	O
function	O
of	O
one	O
dimension	O
having	O
period	O
τ	O
represents	O
the	O
function	O
as	O
a	O
linear	O
combination	O
of	O
sine	O
and	O
cosine	O
functions	O
that	O
are	O
each	O
periodic	O
with	O
periods	O
that	O
evenly	O
divide	O
τ	O
(	O
in	O
other	O
words	O
,	O
whose	O
frequencies	O
are	O
integer	O
multiples	O
of	O
a	O
fundamental	O
frequency	O
1/τ	O
)	O
.	O
but	O
if	O
you	O
are	O
interested	O
in	O
approximating	O
an	O
aperiodic	O
function	O
deﬁned	O
over	O
a	O
bounded	O
interval	O
,	O
then	O
you	O
can	O
use	O
these	O
fourier	O
basis	O
featues	O
with	O
τ	O
set	O
to	O
the	O
length	O
the	O
interval	O
.	O
212	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
the	O
function	O
of	O
interest	O
is	O
then	O
just	O
one	O
period	O
of	O
the	O
periodic	O
linear	O
combination	O
of	O
the	O
sine	O
and	O
cosine	O
features	O
.	O
furthermore	O
,	O
if	O
you	O
set	O
τ	O
to	O
twice	O
the	O
length	O
of	O
the	O
interval	O
of	O
interest	O
and	O
restrict	O
attention	O
to	O
the	O
approximation	O
over	O
the	O
half	O
interval	O
[	O
0	O
,	O
τ	O
/2	O
]	O
,	O
then	O
you	O
can	O
use	O
just	O
the	O
cosine	O
features	O
.	O
this	O
is	O
possible	O
because	O
you	O
can	O
represent	O
any	O
even	O
function	O
,	O
that	O
is	O
,	O
any	O
function	O
that	O
is	O
symmetric	O
about	O
the	O
origin	O
,	O
with	O
just	O
the	O
cosine	O
basis	O
.	O
so	O
any	O
func-	O
tion	B
over	O
the	O
half-period	O
[	O
0	O
,	O
τ	O
/2	O
]	O
can	O
be	O
approximated	O
as	O
closely	O
as	O
desired	O
with	O
enough	O
cosine	O
features	O
.	O
(	O
saying	O
“	O
any	O
function	O
”	O
is	O
not	O
exactly	O
correct	O
because	O
the	O
function	O
has	O
to	O
be	O
mathematically	O
well-behaved	O
,	O
but	O
we	O
skip	O
this	O
technicality	O
here	O
.	O
)	O
alternatively	O
,	O
it	O
is	O
possible	O
to	O
use	O
just	O
sine	O
features	O
,	O
linear	O
combinations	O
of	O
which	O
are	O
always	O
odd	O
functions	O
,	O
that	O
is	O
functions	O
that	O
are	O
anti-symmetric	O
about	O
the	O
origin	O
.	O
but	O
it	O
is	O
generally	O
better	O
to	O
keep	O
just	O
the	O
cosine	O
features	O
because	O
“	O
half-even	O
”	O
functions	O
tend	O
to	O
be	O
easier	O
to	O
approxi-	O
mate	O
than	O
“	O
half-odd	O
”	O
functions	O
because	O
the	O
latter	O
are	O
often	O
discontinuous	O
at	O
the	O
origin	O
.	O
of	O
course	O
,	O
this	O
does	O
not	O
rule	O
out	O
using	O
both	O
sine	O
and	O
cosine	O
features	O
to	O
approximate	B
over	O
the	O
interval	O
[	O
0	O
,	O
τ	O
/2	O
]	O
,	O
which	O
might	O
be	O
advantageous	O
in	O
some	O
circumstances	O
.	O
following	O
this	O
logic	O
and	O
letting	O
τ	O
=	O
2	O
so	O
that	O
the	O
features	O
are	O
deﬁned	O
over	O
the	O
half-	O
τ	O
interval	O
[	O
0	O
,	O
1	O
]	O
,	O
the	O
one-dimensional	O
order-n	O
fourier	O
cosine	O
basis	O
consists	O
of	O
the	O
n	O
+	O
1	O
features	O
xi	O
(	O
s	O
)	O
=	O
cos	O
(	O
iπs	O
)	O
,	O
s	O
∈	O
[	O
0	O
,	O
1	O
]	O
,	O
for	O
i	O
=	O
0	O
,	O
.	O
.	O
.	O
,	O
n.	O
figure	O
9.3	O
shows	O
one-dimensional	O
fourier	O
cosine	O
features	O
xi	O
,	O
for	O
i	O
=	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
;	O
x0	O
is	O
a	O
constant	O
function	O
.	O
figure	O
9.3	O
:	O
one-dimensional	O
fourier	O
cosine-basis	O
features	O
xi	O
,	O
i	O
=	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
,	O
for	O
approximating	O
functions	O
over	O
the	O
interval	O
[	O
0	O
,	O
1	O
]	O
.	O
after	O
konidaris	O
et	O
al	O
.	O
(	O
2011	O
)	O
.	O
this	O
same	O
reasoning	O
applies	O
to	O
the	O
fourier	O
cosine	O
series	O
approximation	O
in	O
the	O
multi-	O
dimensional	O
case	O
as	O
described	O
in	O
the	O
box	O
below	O
.	O
suppose	O
each	O
state	B
s	O
corresponds	O
to	O
a	O
vector	B
of	O
k	O
numbers	O
,	O
s	O
=	O
(	O
s1	O
,	O
s2	O
,	O
...	O
,	O
sk	O
)	O
(	O
cid:62	O
)	O
,	O
with	O
each	O
si	O
∈	O
[	O
0	O
,	O
1	O
]	O
.	O
the	O
ith	O
feature	O
in	O
the	O
order-n	O
fourier	O
cosine	O
basis	O
can	O
then	O
be	O
written	O
xi	O
(	O
s	O
)	O
=	O
cos	O
(	O
cid:0	O
)	O
π	O
s	O
(	O
cid:62	O
)	O
ci	O
(	O
cid:1	O
)	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
ci	O
where	O
ci	O
=	O
(	O
ci	O
j	O
∈	O
{	O
0	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
for	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
and	O
i	O
=	O
0	O
,	O
.	O
.	O
.	O
,	O
(	O
n	O
+	O
1	O
)	O
k.	O
this	O
deﬁnes	O
a	O
feature	O
for	O
each	O
of	O
the	O
(	O
n	O
+	O
1	O
)	O
k	O
possible	O
integer	O
vectors	O
ci	O
.	O
k	O
)	O
(	O
cid:62	O
)	O
,	O
with	O
ci	O
(	O
9.18	O
)	O
00.10.20.30.40.50.60.70.80.91−1−0.8−0.6−0.4−0.200.20.40.60.81univariate	O
fourier	O
basis	O
function	O
k=11-11000.10.20.30.40.50.60.70.80.91−1−0.8−0.6−0.4−0.200.20.40.60.81univariate	O
fourier	O
basis	O
function	O
k=21-11000.10.20.30.40.50.60.70.80.91−1−0.8−0.6−0.4−0.200.20.40.60.81univariate	O
fourier	O
basis	O
function	O
k=31-11000.10.20.30.40.50.60.70.80.91−1−0.8−0.6−0.4−0.200.20.40.60.81univariate	O
fourier	O
basis	O
function	O
k=41-110	O
9.5.	O
feature	B
construction	I
for	O
linear	O
methods	O
213	O
the	O
inner	O
product	O
s	O
(	O
cid:62	O
)	O
ci	O
has	O
the	O
eﬀect	O
of	O
assigning	O
an	O
integer	O
in	O
{	O
0	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
to	O
each	O
dimension	O
of	O
s.	O
as	O
in	O
the	O
one-dimensional	O
case	O
,	O
this	O
integer	O
determines	O
the	O
feature	O
’	O
s	O
frequency	O
along	O
that	O
dimension	O
.	O
the	O
features	O
can	O
of	O
course	O
be	O
shifted	O
and	O
scaled	O
to	O
suit	O
the	O
bounded	O
state	B
space	O
of	O
a	O
particular	O
application	O
.	O
as	O
an	O
example	O
,	O
consider	O
the	O
k	O
=	O
2	O
case	O
in	O
which	O
s	O
=	O
(	O
s1	O
,	O
s2	O
)	O
(	O
cid:62	O
)	O
,	O
where	O
each	O
ci	O
=	O
(	O
ci	O
2	O
)	O
(	O
cid:62	O
)	O
.	O
figure	O
9.4	O
shows	O
a	O
selection	O
of	O
six	O
fourier	O
cosine	O
features	O
,	O
each	O
labeled	O
by	O
the	O
vector	B
ci	O
that	O
deﬁnes	O
it	O
(	O
s1	O
is	O
the	O
horizontal	O
axis	O
and	O
ci	O
is	O
shown	O
as	O
a	O
row	O
vector	B
with	O
the	O
index	O
i	O
omitted	O
)	O
.	O
any	O
zero	O
in	O
c	O
means	O
the	O
feature	O
is	O
constant	O
along	O
that	O
state	B
dimension	O
.	O
so	O
if	O
c	O
=	O
(	O
0	O
,	O
0	O
)	O
(	O
cid:62	O
)	O
,	O
the	O
feature	O
is	O
constant	O
over	O
both	O
dimensions	O
;	O
if	O
c	O
=	O
(	O
c1	O
,	O
0	O
)	O
(	O
cid:62	O
)	O
the	O
feature	O
is	O
constant	O
over	O
the	O
second	O
dimension	O
and	O
varies	O
over	O
the	O
ﬁrst	O
with	O
frequency	O
depending	O
on	O
c1	O
;	O
and	O
similarly	O
,	O
for	O
c	O
=	O
(	O
0	O
,	O
c2	O
)	O
(	O
cid:62	O
)	O
.	O
when	O
c	O
=	O
(	O
c1	O
,	O
c2	O
)	O
(	O
cid:62	O
)	O
with	O
neither	O
cj	O
=	O
0	O
,	O
the	O
feature	O
varies	O
along	O
both	O
dimensions	O
and	O
represents	O
an	O
interaction	O
between	O
the	O
two	O
state	B
variables	O
.	O
the	O
values	O
of	O
c1	O
and	O
c2	O
determine	O
the	O
frequency	O
along	O
each	O
dimension	O
,	O
and	O
their	O
ratio	B
gives	O
the	O
direction	O
of	O
the	O
interaction	O
.	O
1	O
,	O
ci	O
figure	O
9.4	O
:	O
a	O
selection	O
of	O
six	O
two-dimensional	O
fourier	O
cosine	O
features	O
,	O
each	O
labeled	O
by	O
the	O
vector	B
ci	O
that	O
deﬁnes	O
it	O
(	O
s1	O
is	O
the	O
horizontal	O
axis	O
,	O
and	O
ci	O
is	O
shown	O
with	O
the	O
index	O
i	O
omitted	O
)	O
.	O
after	O
konidaris	O
et	O
al	O
.	O
(	O
2011	O
)	O
.	O
when	O
using	O
fourier	O
cosine	O
features	O
with	O
a	O
learning	O
algorithm	O
such	O
as	O
(	O
9.7	O
)	O
,	O
semi-	O
gradient	B
td	O
(	O
0	O
)	O
,	O
or	O
semi-gradient	O
sarsa	O
,	O
it	O
may	O
be	O
helpful	O
to	O
use	O
a	O
diﬀerent	O
step-size	B
parameter	I
for	O
each	O
feature	O
.	O
if	O
α	O
is	O
the	O
basic	O
step-size	B
parameter	I
,	O
then	O
konidaris	O
,	O
os-	O
entoski	O
,	O
and	O
thomas	O
(	O
2011	O
)	O
suggest	O
setting	O
the	O
step-size	B
parameter	I
for	O
feature	O
xi	O
to	O
k	O
)	O
2	O
(	O
except	O
when	O
each	O
ci	O
j	O
=	O
0	O
,	O
in	O
which	O
case	O
αi	O
=	O
α	O
)	O
.	O
αi	O
=	O
α/	O
(	O
cid:112	O
)	O
(	O
ci	O
1	O
)	O
2	O
+	O
···	O
+	O
(	O
ci	O
fourier	O
cosine	O
features	O
with	O
sarsa	O
can	O
produce	O
good	O
performance	O
compared	O
to	O
several	O
c	O
=	O
(	O
0	O
,	O
1	O
)	O
11001100c=	O
(	O
0,1	O
)	O
>	O
c	O
=	O
(	O
1	O
,	O
0	O
)	O
11001100c=	O
(	O
1,0	O
)	O
>	O
c	O
=	O
(	O
1	O
,	O
1	O
)	O
11001100c=	O
(	O
1,1	O
)	O
>	O
c	O
=	O
(	O
1	O
,	O
5	O
)	O
11001100c=	O
(	O
0,5	O
)	O
>	O
c	O
=	O
(	O
2	O
,	O
5	O
)	O
11001100c=	O
(	O
2,5	O
)	O
>	O
1100c=	O
(	O
5,2	O
)	O
>	O
214	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
other	O
collections	O
of	O
basis	O
functions	O
,	O
including	O
polynomial	O
and	O
radial	O
basis	O
functions	O
.	O
not	O
surprisingly	O
,	O
however	O
,	O
fourier	O
features	O
have	O
trouble	O
with	O
discontinuities	O
because	O
it	O
is	O
diﬃcult	O
to	O
avoid	O
“	O
ringing	O
”	O
around	O
points	O
of	O
discontinuity	O
unless	O
very	O
high	O
frequency	O
basis	O
functions	O
are	O
included	O
.	O
the	O
number	O
of	O
features	O
in	O
the	O
order-n	O
fourier	O
basis	O
grows	O
exponentially	O
with	O
the	O
dimension	O
of	O
the	O
state	B
space	O
,	O
but	O
if	O
that	O
dimension	O
is	O
small	O
enough	O
(	O
e.g.	O
,	O
k	O
≤	O
5	O
)	O
,	O
then	O
one	O
can	O
select	O
n	O
so	O
that	O
all	O
of	O
the	O
order-n	O
fourier	O
features	O
can	O
be	O
used	O
.	O
this	O
makes	O
the	O
selection	O
of	O
features	O
more-or-less	O
automatic	O
.	O
for	O
high	O
dimension	O
state	B
spaces	O
,	O
however	O
,	O
it	O
is	O
necessary	O
to	O
select	O
a	O
subset	O
of	O
these	O
features	O
.	O
this	O
can	O
be	O
done	O
using	O
prior	O
beliefs	O
about	O
the	O
nature	O
of	O
the	O
function	O
to	O
be	O
approximated	O
,	O
and	O
some	O
automated	O
selection	O
methods	O
can	O
be	O
adapted	O
to	O
deal	O
with	O
the	O
incremental	O
and	O
nonstationary	O
nature	O
of	O
reinforcement	O
learning	O
.	O
an	O
advantage	O
of	O
fourier	O
basis	O
features	O
in	O
this	O
regard	O
is	O
that	O
it	O
is	O
easy	O
to	O
select	O
features	O
by	O
setting	O
the	O
ci	O
vectors	O
to	O
account	O
for	O
suspected	O
interactions	O
among	O
the	O
state	B
variables	O
and	O
by	O
limiting	O
the	O
values	O
in	O
the	O
cj	O
vectors	O
so	O
that	O
the	O
approximation	O
can	O
ﬁlter	O
out	O
high	O
frequency	O
components	O
considered	O
to	O
be	O
noise	O
.	O
on	O
the	O
other	O
hand	O
,	O
because	O
fourier	O
features	O
are	O
non-zero	O
over	O
the	O
entire	O
state	B
space	O
(	O
with	O
the	O
few	O
zeros	O
excepted	O
)	O
,	O
they	O
represent	O
global	O
properties	O
of	O
states	O
,	O
which	O
can	O
make	O
it	O
diﬃcult	O
to	O
ﬁnd	O
good	O
ways	O
to	O
represent	O
local	O
properties	O
.	O
figure	O
9.5	O
shows	O
learning	O
curves	O
comparing	O
the	O
fourier	O
and	O
polynomial	O
bases	O
on	O
the	O
1000-state	B
random	O
walk	O
example	O
.	O
in	O
general	O
,	O
we	O
do	O
not	O
recommend	O
using	O
polynomials	O
for	O
online	O
learning.2	O
figure	O
9.5	O
:	O
fourier	O
basis	O
vs	O
polynomials	O
on	O
the	O
1000-state	B
random	O
walk	O
.	O
shown	O
are	O
learning	O
curves	O
for	O
the	O
gradient	B
monte	O
carlo	O
method	O
with	O
fourier	O
and	O
polynomial	O
bases	O
of	O
order	O
5	O
,	O
10	O
,	O
and	O
20.	O
the	O
step-size	O
parameters	O
were	O
roughly	O
optimized	O
for	O
each	O
case	O
:	O
α	O
=	O
0.0001	O
for	O
the	O
polynomial	B
basis	I
and	O
α	O
=	O
0.00005	O
for	O
the	O
fourier	O
basis	O
.	O
the	O
performance	O
measure	O
(	O
y-axis	O
)	O
is	O
the	O
root	O
mean	O
squared	O
value	B
error	O
(	O
9.1	O
)	O
.	O
2there	O
are	O
families	O
of	O
polynomials	O
more	O
complicated	O
than	O
those	O
we	O
have	O
discussed	O
,	O
for	O
example	O
,	O
diﬀerent	O
families	O
of	O
orthogonal	O
polynomials	O
,	O
and	O
these	O
might	O
work	O
better	O
,	O
but	O
at	O
present	O
there	O
is	O
little	O
experience	O
with	O
them	O
in	O
reinforcement	O
learning	O
.	O
.4.3.2.1005000episodespolynomial	O
basisfourier	O
basispve	O
averagedover	O
30	O
runs	O
9.5.	O
feature	B
construction	I
for	O
linear	O
methods	O
215	O
9.5.3	O
coarse	B
coding	I
consider	O
a	O
task	O
in	O
which	O
the	O
natural	O
repre-	O
sentation	O
of	O
the	O
state	B
set	O
is	O
a	O
continuous	O
two-	O
dimensional	O
space	O
.	O
one	O
kind	O
of	O
representation	O
for	O
this	O
case	O
is	O
made	O
up	O
of	O
features	O
corresponding	O
to	O
circles	O
in	O
state	O
space	O
,	O
as	O
shown	O
to	O
the	O
right	O
.	O
if	O
the	O
state	B
is	O
inside	O
a	O
circle	O
,	O
then	O
the	O
corresponding	O
feature	O
has	O
the	O
value	B
1	O
and	O
is	O
said	O
to	O
be	O
present	O
;	O
otherwise	O
the	O
feature	O
is	O
0	O
and	O
is	O
said	O
to	O
be	O
ab-	O
sent	O
.	O
this	O
kind	O
of	O
1–0-valued	O
feature	O
is	O
called	O
a	O
binary	O
feature	O
.	O
given	O
a	O
state	B
,	O
which	O
binary	O
fea-	O
tures	O
are	O
present	O
indicate	O
within	O
which	O
circles	O
the	O
state	B
lies	O
,	O
and	O
thus	O
coarsely	O
code	O
for	O
its	O
location	O
.	O
representing	O
a	O
state	B
with	O
features	O
that	O
overlap	O
in	O
this	O
way	O
(	O
although	O
they	O
need	O
not	O
be	O
circles	O
or	O
binary	O
)	O
is	O
known	O
as	O
coarse	O
coding	O
.	O
figure	O
9.6	O
:	O
coarse	B
coding	I
.	O
general-	O
ization	O
from	O
state	B
s	O
to	O
state	B
s	O
(	O
cid:48	O
)	O
depends	O
on	O
the	O
number	O
of	O
their	O
features	O
whose	O
re-	O
ceptive	O
ﬁelds	O
(	O
in	O
this	O
case	O
,	O
circles	O
)	O
over-	O
lap	O
.	O
these	O
states	O
have	O
one	O
feature	O
in	O
common	O
,	O
so	O
there	O
will	O
be	O
slight	O
general-	O
ization	O
between	O
them	O
.	O
assuming	O
linear	O
gradient-descent	O
function	O
ap-	O
proximation	O
,	O
consider	O
the	O
eﬀect	O
of	O
the	O
size	O
and	O
density	O
of	O
the	O
circles	O
.	O
corresponding	O
to	O
each	O
cir-	O
cle	O
is	O
a	O
single	O
weight	O
(	O
a	O
component	O
of	O
w	O
)	O
that	O
is	O
aﬀected	O
by	O
learning	O
.	O
if	O
we	O
train	O
at	O
one	O
state	B
,	O
a	O
point	O
in	O
the	O
space	O
,	O
then	O
the	O
weights	O
of	O
all	O
circles	O
intersecting	O
that	O
state	B
will	O
be	O
aﬀected	O
.	O
thus	O
,	O
by	O
(	O
9.8	O
)	O
,	O
the	O
approximate	B
value	O
function	O
will	O
be	O
aﬀected	O
at	O
all	O
states	O
within	O
the	O
union	O
of	O
the	O
circles	O
,	O
with	O
a	O
greater	O
eﬀect	O
the	O
more	O
circles	O
a	O
point	O
has	O
“	O
in	O
common	O
”	O
with	O
the	O
state	B
,	O
as	O
shown	O
in	O
figure	O
9.6.	O
if	O
the	O
circles	O
are	O
small	O
,	O
then	O
the	O
generalization	O
will	O
be	O
over	O
a	O
short	O
distance	O
,	O
as	O
in	O
figure	O
9.7	O
(	O
left	O
)	O
,	O
whereas	O
if	O
they	O
are	O
large	O
,	O
it	O
will	O
be	O
over	O
a	O
large	O
distance	O
,	O
as	O
in	O
figure	O
9.7	O
(	O
mid-	O
figure	O
9.7	O
:	O
generalization	O
in	O
linear	O
function	B
approximation	I
methods	O
is	O
determined	O
by	O
the	O
sizes	O
and	O
shapes	O
of	O
the	O
features	O
’	O
receptive	O
ﬁelds	O
.	O
all	O
three	O
of	O
these	O
cases	O
have	O
roughly	O
the	O
same	O
number	O
and	O
density	O
of	O
features	O
.	O
s0sa	O
)	O
narrow	O
generalizationb	O
)	O
broad	O
generalizationc	O
)	O
asymmetric	O
generalization	O
216	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
dle	O
)	O
.	O
moreover	O
,	O
the	O
shape	O
of	O
the	O
features	O
will	O
determine	O
the	O
nature	O
of	O
the	O
generalization	O
.	O
for	O
example	O
,	O
if	O
they	O
are	O
not	O
strictly	O
circular	O
,	O
but	O
are	O
elongated	O
in	O
one	O
direction	O
,	O
then	O
generalization	O
will	O
be	O
similarly	O
aﬀected	O
,	O
as	O
in	O
figure	O
9.7	O
(	O
right	O
)	O
.	O
features	O
with	O
large	O
receptive	O
ﬁelds	O
give	O
broad	O
generalization	O
,	O
but	O
might	O
also	O
seem	O
to	O
limit	O
the	O
learned	O
function	O
to	O
a	O
coarse	O
approximation	O
,	O
unable	O
to	O
make	O
discriminations	O
much	O
ﬁner	O
than	O
the	O
width	O
of	O
the	O
receptive	O
ﬁelds	O
.	O
happily	O
,	O
this	O
is	O
not	O
the	O
case	O
.	O
initial	O
generalization	O
from	O
one	O
point	O
to	O
another	O
is	O
indeed	O
controlled	O
by	O
the	O
size	O
and	O
shape	O
of	O
the	O
receptive	O
ﬁelds	O
,	O
but	O
acuity	O
,	O
the	O
ﬁnest	O
discrimination	O
ultimately	O
possible	O
,	O
is	O
controlled	O
more	O
by	O
the	O
total	O
number	O
of	O
features	O
.	O
example	O
9.3	O
:	O
coarseness	O
of	O
coarse	O
coding	O
this	O
example	O
illustrates	O
the	O
eﬀect	O
on	O
learning	O
of	O
the	O
size	O
of	O
the	O
receptive	O
ﬁelds	O
in	O
coarse	O
coding	O
.	O
linear	O
function	O
approxima-	O
tion	B
based	O
on	O
coarse	B
coding	I
and	O
(	O
9.7	O
)	O
was	O
used	O
to	O
learn	O
a	O
one-dimensional	O
square-wave	O
function	O
(	O
shown	O
at	O
the	O
top	O
of	O
figure	O
9.8	O
)	O
.	O
the	O
values	O
of	O
this	O
function	O
were	O
used	O
as	O
the	O
targets	O
,	O
ut	O
.	O
with	O
just	O
one	O
dimension	O
,	O
the	O
receptive	O
ﬁelds	O
were	O
intervals	O
rather	O
than	O
cir-	O
cles	O
.	O
learning	O
was	O
repeated	O
with	O
three	O
diﬀerent	O
sizes	O
of	O
the	O
intervals	O
:	O
narrow	O
,	O
medium	O
,	O
and	O
broad	O
,	O
as	O
shown	O
at	O
the	O
bottom	O
of	O
the	O
ﬁgure	O
.	O
all	O
three	O
cases	O
had	O
the	O
same	O
density	O
of	O
features	O
,	O
about	O
50	O
over	O
the	O
extent	O
of	O
the	O
function	O
being	O
learned	O
.	O
training	O
examples	O
were	O
generated	O
uniformly	O
at	O
random	O
over	O
this	O
extent	O
.	O
the	O
step-size	B
parameter	I
was	O
α	O
=	O
0.2	O
n	O
,	O
where	O
n	O
is	O
the	O
number	O
of	O
features	O
that	O
were	O
present	O
at	O
one	O
time	O
.	O
figure	O
9.8	O
shows	O
the	O
functions	O
learned	O
in	O
all	O
three	O
cases	O
over	O
the	O
course	O
of	O
learning	O
.	O
note	O
that	O
the	O
width	O
of	O
the	O
features	O
had	O
a	O
strong	O
eﬀect	O
early	O
in	O
learning	O
.	O
with	O
broad	O
features	O
,	O
the	O
generalization	O
tended	O
to	O
be	O
broad	O
;	O
with	O
narrow	O
features	O
,	O
only	O
the	O
close	O
neighbors	O
of	O
each	O
trained	O
point	O
were	O
changed	O
,	O
causing	O
the	O
function	O
learned	O
to	O
be	O
more	O
bumpy	O
.	O
however	O
,	O
the	O
ﬁnal	O
func-	O
tion	B
learned	O
was	O
aﬀected	O
only	O
slightly	O
by	O
the	O
width	O
of	O
the	O
features	O
.	O
receptive	O
ﬁeld	O
shape	O
figure	O
9.8	O
:	O
example	O
of	O
feature	O
width	O
’	O
s	O
strong	O
eﬀect	O
on	O
initial	O
generalization	O
(	O
ﬁrst	O
row	O
)	O
and	O
weak	O
eﬀect	O
on	O
asymptotic	O
accuracy	O
(	O
last	O
row	O
)	O
.	O
1040160640256010240narrowfeaturesdesiredfunctionmediumfeaturesbroadfeatures	O
#	O
examplesapprox-imationfeaturewidth	O
9.5.	O
feature	B
construction	I
for	O
linear	O
methods	O
217	O
tends	O
to	O
have	O
a	O
strong	O
eﬀect	O
on	O
generalization	O
but	O
little	O
eﬀect	O
on	O
asymptotic	O
solution	O
quality	O
.	O
9.5.4	O
tile	B
coding	I
tile	O
coding	O
is	O
a	O
form	O
of	O
coarse	O
coding	O
for	O
multi-dimensional	O
continuous	O
spaces	O
that	O
is	O
ﬂexible	O
and	O
computationally	O
eﬃcient	O
.	O
it	O
may	O
be	O
the	O
most	O
practical	O
feature	O
representation	O
for	O
modern	O
sequential	O
digital	O
computers	O
.	O
open-source	O
software	O
is	O
available	O
for	O
many	O
kinds	O
of	O
tile	O
coding	O
.	O
in	O
tile	O
coding	O
the	O
receptive	O
ﬁelds	O
of	O
the	O
features	O
are	O
grouped	O
into	O
partitions	O
of	O
the	O
state	B
space	O
.	O
each	O
such	O
partition	O
is	O
called	O
a	O
tiling	O
,	O
and	O
each	O
element	O
of	O
the	O
partition	O
is	O
called	O
a	O
tile	O
.	O
for	O
example	O
,	O
the	O
simplest	O
tiling	O
of	O
a	O
two-dimensional	O
state	B
space	O
is	O
a	O
uniform	O
grid	O
such	O
as	O
that	O
shown	O
on	O
the	O
left	O
side	O
of	O
figure	O
9.9.	O
the	O
tiles	O
or	O
receptive	O
ﬁeld	O
here	O
are	O
squares	O
rather	O
than	O
the	O
circles	O
in	O
figure	O
9.6.	O
if	O
just	O
this	O
single	O
tiling	O
were	O
used	O
,	O
then	O
the	O
state	B
indicated	O
by	O
the	O
white	O
spot	O
would	O
be	O
represented	O
by	O
the	O
single	O
feature	O
whose	O
tile	O
it	O
falls	O
within	O
;	O
generalization	O
would	O
be	O
complete	O
to	O
all	O
states	O
within	O
the	O
same	O
tile	O
and	O
nonexistent	O
to	O
states	O
outside	O
it	O
.	O
with	O
just	O
one	O
tiling	O
,	O
we	O
would	O
not	O
have	O
coarse	B
coding	I
but	O
just	O
a	O
case	O
of	O
state	O
aggregation	O
.	O
figure	O
9.9	O
:	O
multiple	O
,	O
overlapping	O
grid-tilings	O
on	O
a	O
limited	O
two-dimensional	O
space	O
.	O
these	O
tilings	O
are	O
oﬀset	O
from	O
one	O
another	O
by	O
a	O
uniform	O
amount	O
in	O
each	O
dimension	O
.	O
to	O
get	O
the	O
strengths	O
of	O
coarse	O
coding	O
requires	O
overlapping	O
receptive	O
ﬁelds	O
,	O
and	O
by	O
deﬁnition	O
the	O
tiles	O
of	O
a	O
partition	O
do	O
not	O
overlap	O
.	O
to	O
get	O
true	O
coarse	O
coding	O
with	B
tile	I
coding	I
,	O
multiple	O
tilings	O
are	O
used	O
,	O
each	O
oﬀset	O
by	O
a	O
fraction	O
of	O
a	O
tile	O
width	O
.	O
a	O
simple	O
case	O
with	O
four	O
tilings	O
is	O
shown	O
on	O
the	O
right	O
side	O
of	O
figure	O
9.9.	O
every	O
state	B
,	O
such	O
as	O
that	O
indicated	O
by	O
the	O
white	O
spot	O
,	O
falls	O
in	O
exactly	O
one	O
tile	O
in	O
each	O
of	O
the	O
four	O
tilings	O
.	O
these	O
four	O
tiles	O
correspond	O
to	O
four	O
features	O
that	O
become	O
active	O
when	O
the	O
state	B
occurs	O
.	O
speciﬁcally	O
,	O
the	O
feature	O
vector	O
x	O
(	O
s	O
)	O
has	O
one	O
component	O
for	O
each	O
tile	O
in	O
each	O
tiling	O
.	O
in	O
this	O
example	O
there	O
are	O
4	O
×	O
4	O
×	O
4	O
=	O
64	O
components	O
,	O
all	O
of	O
which	O
will	O
be	O
0	O
except	O
for	O
the	O
four	O
corresponding	O
to	O
the	O
tiles	O
that	O
s	O
falls	O
within	O
.	O
figure	O
9.10	O
shows	O
the	O
advantage	O
of	O
multiple	O
oﬀset	O
tilings	O
(	O
coarse	B
coding	I
)	O
over	O
a	O
single	O
tiling	O
on	O
the	O
1000-state	B
random	O
walk	O
example	O
.	O
point	O
in	O
state	O
spaceto	O
berepresentedtiling	O
1tiling	O
2tiling	O
3tiling	O
4continuous	O
2d	O
state	B
spacefour	O
activetiles/features	O
overlap	O
the	O
pointand	O
are	O
used	O
to	O
represent	O
it	O
218	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
figure	O
9.10	O
:	O
why	O
we	O
use	O
coarse	B
coding	I
.	O
shown	O
are	O
learning	O
curves	O
on	O
the	O
1000-state	B
random	O
walk	O
example	O
for	O
the	O
gradient	B
monte	O
carlo	O
algorithm	O
with	O
a	O
single	O
tiling	O
and	O
with	O
multiple	O
tilings	O
.	O
the	O
space	O
of	O
1000	O
states	O
was	O
treated	O
as	O
a	O
single	O
continuous	O
dimension	O
,	O
covered	O
with	O
tiles	O
each	O
200	O
states	O
wide	O
.	O
the	O
multiple	O
tilings	O
were	O
oﬀset	O
from	O
each	O
other	O
by	O
4	O
states	O
.	O
the	O
step-size	B
parameter	I
was	O
set	O
so	O
that	O
the	O
initial	O
learning	O
rate	O
in	O
the	O
two	O
cases	O
was	O
the	O
same	O
,	O
α	O
=	O
0.0001	O
for	O
the	O
single	O
tiling	O
and	O
α	O
=	O
0.0001/50	O
for	O
the	O
50	O
tilings	O
.	O
an	O
immediate	O
practical	O
advantage	O
of	O
tile	O
coding	O
is	O
that	O
,	O
because	O
it	O
works	O
with	O
parti-	O
tions	O
,	O
the	O
overall	O
number	O
of	O
features	O
that	O
are	O
active	O
at	O
one	O
time	O
is	O
the	O
same	O
for	O
any	O
state	B
.	O
exactly	O
one	O
feature	O
is	O
present	O
in	O
each	O
tiling	O
,	O
so	O
the	O
total	O
number	O
of	O
features	O
present	O
is	O
always	O
the	O
same	O
as	O
the	O
number	O
of	O
tilings	O
.	O
this	O
allows	O
the	O
step-size	B
parameter	I
,	O
α	O
,	O
to	O
be	O
set	O
in	O
an	O
easy	O
,	O
intuitive	O
way	O
.	O
for	O
example	O
,	O
choosing	O
α	O
=	O
1	O
n	O
,	O
where	O
n	O
is	O
the	O
number	O
of	O
tilings	O
,	O
results	O
in	O
exact	O
one-trial	O
learning	O
.	O
if	O
the	O
example	O
s	O
(	O
cid:55	O
)	O
→	O
v	O
is	O
trained	O
on	O
,	O
then	O
whatever	O
the	O
prior	O
estimate	O
,	O
ˆv	O
(	O
s	O
,	O
wt	O
)	O
,	O
the	O
new	O
estimate	O
will	O
be	O
ˆv	O
(	O
s	O
,	O
wt+1	O
)	O
=	O
v.	O
usually	O
one	O
wishes	O
to	O
change	O
more	O
slowly	O
than	O
this	O
,	O
to	O
allow	O
for	O
generalization	O
and	O
stochastic	O
variation	O
in	O
target	O
outputs	O
.	O
for	O
example	O
,	O
one	O
might	O
choose	O
α	O
=	O
1	O
10n	O
,	O
in	O
which	O
case	O
the	O
estimate	O
for	O
the	O
trained	O
state	B
would	O
move	O
one-tenth	O
of	O
the	O
way	O
to	O
the	O
target	B
in	O
one	O
update	O
,	O
and	O
neighboring	O
states	O
will	O
be	O
moved	O
less	O
,	O
proportional	O
to	O
the	O
number	O
of	O
tiles	O
they	O
have	O
in	O
common	O
.	O
tile	B
coding	I
also	O
gains	O
computational	O
advantages	O
from	O
its	O
use	O
of	O
binary	O
feature	O
vectors	O
.	O
because	O
each	O
component	O
is	O
either	O
0	O
or	O
1	O
,	O
the	O
weighted	O
sum	O
making	O
up	O
the	O
approximate	B
value	O
function	O
(	O
9.8	O
)	O
is	O
almost	O
trivial	O
to	O
compute	O
.	O
rather	O
than	O
performing	O
d	O
multiplica-	O
tions	O
and	O
additions	O
,	O
one	O
simply	O
computes	O
the	O
indices	O
of	O
the	O
n	O
(	O
cid:28	O
)	O
d	O
active	O
features	O
and	O
then	O
adds	O
up	O
the	O
n	O
corresponding	O
components	O
of	O
the	O
weight	O
vector	B
.	O
generalization	O
occurs	O
to	O
states	O
other	O
than	O
the	O
one	O
trained	O
if	O
those	O
states	O
fall	O
within	O
any	O
of	O
the	O
same	O
tiles	O
,	O
proportional	O
to	O
the	O
number	O
of	O
tiles	O
in	O
common	O
.	O
even	O
the	O
choice	O
of	O
how	O
to	O
oﬀset	O
the	O
tilings	O
from	O
each	O
other	O
aﬀects	O
generalization	O
.	O
if	O
they	O
are	O
oﬀset	O
uniformly	O
in	O
each	O
dimension	O
,	O
as	O
they	O
were	O
in	O
figure	O
9.9	O
,	O
then	O
diﬀerent	O
states	O
can	O
generalize	O
in	O
qualitatively	O
diﬀerent	O
ways	O
,	O
as	O
shown	O
in	O
the	O
upper	O
half	O
of	O
figure	O
9.11.	O
each	O
of	O
the	O
eight	O
subﬁgures	O
show	O
the	O
pattern	O
of	O
generalization	O
from	O
a	O
trained	O
state	B
to	O
nearby	O
points	O
.	O
in	O
this	O
example	O
there	O
are	O
eight	O
tilings	O
,	O
thus	O
64	O
subregions	O
within	O
a	O
tile	O
that	O
generalize	O
distinctly	O
,	O
but	O
all	O
according	O
to	O
one	O
of	O
these	O
eight	O
patterns	O
.	O
note	O
how	O
uniform	O
oﬀsets	O
.4.3.2.10	O
averagedover	O
30	O
runs05000episodesstate	O
aggregation	O
(	O
one	O
tiling	O
)	O
tile	B
coding	I
(	O
50	O
tilings	O
)	O
pve	O
9.5.	O
feature	B
construction	I
for	O
linear	O
methods	O
219	O
figure	O
9.11	O
:	O
why	O
tile	O
asymmetrical	O
oﬀsets	O
are	O
preferred	O
in	O
tile	O
coding	O
.	O
shown	O
is	O
the	O
strength	O
of	O
generalization	O
from	O
a	O
trained	O
state	B
,	O
indicated	O
by	O
the	O
small	O
black	O
plus	O
,	O
to	O
nearby	O
states	O
,	O
for	O
the	O
case	O
of	O
eight	O
tilings	O
.	O
if	O
the	O
tilings	O
are	O
uniformly	O
oﬀset	O
(	O
above	O
)	O
,	O
then	O
there	O
are	O
diagonal	O
artifacts	O
and	O
substantial	O
variations	O
in	O
the	O
generalization	O
,	O
whereas	O
with	O
asymmetrically	O
oﬀset	O
tilings	O
the	O
generalization	O
is	O
more	O
spherical	O
and	O
homogeneous	O
.	O
result	O
in	O
a	O
strong	O
eﬀect	O
along	O
the	O
diagonal	O
in	O
many	O
patterns	O
.	O
these	O
artifacts	O
can	O
be	O
avoided	O
if	O
the	O
tilings	O
are	O
oﬀset	O
asymmetrically	O
,	O
as	O
shown	O
in	O
the	O
lower	O
half	O
of	O
the	O
ﬁgure	O
.	O
these	O
lower	O
generalization	O
patterns	O
are	O
better	O
because	O
they	O
are	O
all	O
well	O
centered	O
on	O
the	O
trained	O
state	B
with	O
no	O
obvious	O
asymmetries	O
.	O
tilings	O
in	O
all	O
cases	O
are	O
oﬀset	O
from	O
each	O
other	O
by	O
a	O
fraction	O
of	O
a	O
tile	O
width	O
in	O
each	O
dimension	O
.	O
if	O
w	O
denotes	O
the	O
tile	O
width	O
and	O
n	O
the	O
number	O
of	O
tilings	O
,	O
then	O
w	O
n	O
is	O
a	O
funda-	O
mental	O
unit	O
.	O
within	O
small	O
squares	O
w	O
n	O
on	O
a	O
side	O
,	O
all	O
states	O
activate	O
the	O
same	O
tiles	O
,	O
have	O
the	O
same	O
feature	O
representation	O
,	O
and	O
the	O
same	O
approximated	O
value	B
.	O
if	O
a	O
state	B
is	O
moved	O
by	O
w	O
n	O
in	O
any	O
cartesian	O
direction	O
,	O
the	O
feature	O
representation	O
changes	O
by	O
one	O
component/tile	O
.	O
uniformly	O
oﬀset	O
tilings	O
are	O
oﬀset	O
from	O
each	O
other	O
by	O
exactly	O
this	O
unit	O
distance	O
.	O
for	O
a	O
two-dimensional	O
space	O
,	O
we	O
say	O
that	O
each	O
tiling	O
is	O
oﬀset	O
by	O
the	O
displacement	O
vector	B
(	O
1	O
,	O
1	O
)	O
,	O
meaning	O
that	O
it	O
is	O
oﬀset	O
from	O
the	O
previous	O
tiling	O
by	O
w	O
n	O
times	O
this	O
vector	B
.	O
in	O
these	O
terms	O
,	O
possible	O
generalizations	O
for	O
uniformly	O
oﬀset	O
tilingspossible	O
generalizationsfor	O
asymmetrically	O
oﬀset	O
tilings	O
220	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
the	O
asymmetrically	O
oﬀset	O
tilings	O
shown	O
in	O
the	O
lower	O
part	O
of	O
figure	O
9.11	O
are	O
oﬀset	O
by	O
a	O
displacement	O
vector	B
of	O
(	O
1	O
,	O
3	O
)	O
.	O
extensive	O
studies	O
have	O
been	O
made	O
of	O
the	O
eﬀect	O
of	O
diﬀerent	O
displacement	O
vectors	O
on	O
the	O
generalization	O
of	O
tile	O
coding	O
(	O
parks	O
and	O
militzer	O
,	O
1991	O
;	O
an	O
,	O
1991	O
;	O
an	O
,	O
miller	O
and	O
parks	O
,	O
1991	O
;	O
miller	O
,	O
an	O
,	O
glanz	O
and	O
carter	O
,	O
1990	O
)	O
,	O
assessing	O
their	O
homegeneity	O
and	O
tendency	O
toward	O
diagonal	O
artifacts	O
like	O
those	O
seen	O
for	O
the	O
(	O
1	O
,	O
1	O
)	O
displacement	O
vectors	O
.	O
based	O
on	O
this	O
work	O
,	O
miller	O
and	O
glanz	O
(	O
1996	O
)	O
recommend	O
using	O
displacement	O
vectors	O
consisting	O
of	O
the	O
ﬁrst	O
odd	O
integers	O
.	O
in	O
particular	O
,	O
for	O
a	O
continuous	O
space	O
of	O
dimension	O
k	O
,	O
a	O
good	O
choice	O
is	O
to	O
use	O
the	O
ﬁrst	O
odd	O
integers	O
(	O
1	O
,	O
3	O
,	O
5	O
,	O
7	O
,	O
.	O
.	O
.	O
,	O
2k	O
−	O
1	O
)	O
,	O
with	O
n	O
(	O
the	O
number	O
of	O
tilings	O
)	O
set	O
to	O
an	O
integer	O
power	O
of	O
2	O
greater	O
than	O
or	O
equal	O
to	O
4k	O
.	O
this	O
is	O
what	O
we	O
have	O
done	O
to	O
produce	O
the	O
tilings	O
in	O
the	O
lower	O
half	O
of	O
figure	O
9.11	O
,	O
in	O
which	O
k	O
=	O
2	O
,	O
n	O
=	O
23	O
≥	O
4k	O
,	O
and	O
the	O
displacement	O
vector	B
is	O
(	O
1	O
,	O
3	O
)	O
.	O
in	O
a	O
three-dimensional	O
case	O
,	O
the	O
ﬁrst	O
four	O
tilings	O
would	O
be	O
oﬀset	O
in	O
total	O
from	O
a	O
base	O
position	O
by	O
(	O
0	O
,	O
0	O
,	O
0	O
)	O
,	O
(	O
1	O
,	O
3	O
,	O
5	O
)	O
,	O
(	O
2	O
,	O
6	O
,	O
10	O
)	O
,	O
and	O
(	O
3	O
,	O
9	O
,	O
15	O
)	O
.	O
open-	O
source	O
software	O
that	O
can	O
eﬃciently	O
make	O
tilings	O
like	O
this	O
for	O
any	O
k	O
is	O
readily	O
available	O
.	O
in	O
choosing	O
a	O
tiling	O
strategy	O
,	O
one	O
has	O
to	O
pick	O
the	O
number	O
of	O
the	O
tilings	O
and	O
the	O
shape	O
of	O
the	O
tiles	O
.	O
the	O
number	O
of	O
tilings	O
,	O
along	O
with	O
the	O
size	O
of	O
the	O
tiles	O
,	O
determines	O
the	O
resolution	O
or	O
ﬁneness	O
of	O
the	O
asymptotic	O
approximation	O
,	O
as	O
in	O
general	O
coarse	O
coding	O
and	O
illustrated	O
in	O
figure	O
9.8.	O
the	O
shape	O
of	O
the	O
tiles	O
will	O
determine	O
the	O
nature	O
of	O
generalization	O
as	O
in	O
figure	O
9.7.	O
square	O
tiles	O
will	O
generalize	O
roughly	O
equally	O
in	O
each	O
dimension	O
as	O
indicated	O
in	O
figure	O
9.11	O
(	O
lower	O
)	O
.	O
tiles	O
that	O
are	O
elongated	O
along	O
one	O
dimension	O
,	O
such	O
as	O
the	O
stripe	O
tilings	O
in	O
figure	O
9.12	O
(	O
middle	O
)	O
,	O
will	O
promote	O
generalization	O
along	O
that	O
dimension	O
.	O
the	O
tilings	O
in	O
figure	O
9.12	O
(	O
middle	O
)	O
are	O
also	O
denser	O
and	O
thinner	O
on	O
the	O
left	O
,	O
promoting	O
dis-	O
crimination	O
along	O
the	O
horizonal	O
dimension	O
at	O
lower	O
values	O
along	O
that	O
dimension	O
.	O
the	O
diagonal	O
stripe	O
tiling	O
in	O
figure	O
9.12	O
(	O
right	O
)	O
will	O
promote	O
generalization	O
along	O
one	O
di-	O
agonal	O
.	O
in	O
higher	O
dimensions	O
,	O
axis-aligned	O
stripes	O
correspond	O
to	O
ignoring	O
some	O
of	O
the	O
dimensions	O
in	O
some	O
of	O
the	O
tilings	O
,	O
that	O
is	O
,	O
to	O
hyperplanar	O
slices	O
.	O
irregular	O
tilings	O
such	O
as	O
shown	O
in	O
figure	O
9.12	O
(	O
left	O
)	O
are	O
also	O
possible	O
,	O
though	O
rare	O
in	O
practice	O
and	O
beyond	O
the	O
standard	O
software	O
.	O
figure	O
9.12	O
:	O
tilings	O
need	O
not	O
be	O
grids	O
.	O
they	O
can	O
be	O
arbitrarily	O
shaped	O
and	O
non-uniform	O
,	O
while	O
still	O
in	O
many	O
cases	O
being	O
computationally	O
eﬃcient	O
to	O
compute	O
.	O
in	O
practice	O
,	O
it	O
is	O
often	O
desirable	O
to	O
use	O
diﬀerent	O
shaped	O
tiles	O
in	O
diﬀerent	O
tilings	O
.	O
for	O
example	O
,	O
one	O
might	O
use	O
some	O
vertical	O
stripe	O
tilings	O
and	O
some	O
horizontal	O
stripe	O
tilings	O
.	O
this	O
would	O
encourage	O
generalization	O
along	O
either	O
dimension	O
.	O
however	O
,	O
with	O
stripe	O
tilings	O
alone	O
it	O
is	O
not	O
possible	O
to	O
learn	O
that	O
a	O
particular	O
conjunction	O
of	O
horizontal	O
and	O
vertical	O
a	O
)	O
irregularb	O
)	O
log	O
stripesc	O
)	O
diagonal	O
stripes	O
9.5.	O
feature	B
construction	I
for	O
linear	O
methods	O
221	O
coordinates	O
has	O
a	O
distinctive	O
value	B
(	O
whatever	O
is	O
learned	O
for	O
it	O
will	O
bleed	O
into	O
states	O
with	O
the	O
same	O
horizontal	O
and	O
vertical	O
coordinates	O
)	O
.	O
for	O
this	O
one	O
needs	O
the	O
conjunctive	O
rectan-	O
gular	O
tiles	O
such	O
as	O
originally	O
shown	O
in	O
figure	O
9.9.	O
with	O
multiple	O
tilings—some	O
horizontal	O
,	O
some	O
vertical	O
,	O
and	O
some	O
conjunctive—one	O
can	O
get	O
everything	O
:	O
a	O
preference	O
for	O
general-	O
izing	O
along	O
each	O
dimension	O
,	O
yet	O
the	O
ability	O
to	O
learn	O
speciﬁc	O
values	O
for	O
conjunctions	O
(	O
see	O
sutton	O
,	O
1996	O
for	O
examples	O
)	O
.	O
the	O
choice	O
of	O
tilings	O
determines	O
generalization	O
,	O
and	O
until	O
this	O
choice	O
can	O
be	O
eﬀectively	O
automated	O
,	O
it	O
is	O
important	O
that	O
tile	B
coding	I
enables	O
the	O
choice	O
to	O
be	O
made	O
ﬂexibly	O
and	O
in	O
a	O
way	O
that	O
makes	O
sense	O
to	O
people	O
.	O
another	O
useful	O
trick	O
for	O
reducing	O
memory	O
requirements	O
is	O
hashing—a	O
consistent	O
pseudo-	O
random	O
collapsing	O
of	O
a	O
large	O
tiling	O
into	O
a	O
much	O
smaller	O
set	O
of	O
tiles	O
.	O
hashing	O
produces	O
tiles	O
consisting	O
of	O
noncontiguous	O
,	O
disjoint	O
regions	O
randomly	O
spread	O
throughout	O
the	O
state	B
space	O
,	O
but	O
that	O
still	O
form	O
an	O
exhaustive	O
partition	O
.	O
for	O
example	O
,	O
one	O
tile	O
might	O
consist	O
of	O
the	O
four	O
subtiles	O
shown	O
to	O
the	O
right	O
.	O
through	O
hashing	O
,	O
memory	O
requirements	O
are	O
often	O
reduced	O
by	O
large	O
factors	O
with	O
little	O
loss	O
of	O
performance	O
.	O
this	O
is	O
possible	O
because	O
high	O
resolution	O
is	O
needed	O
in	O
only	O
a	O
small	O
fraction	O
of	O
the	O
state	B
space	O
.	O
hashing	O
frees	O
us	O
from	O
the	O
curse	B
of	I
dimensionality	I
in	O
the	O
sense	O
that	O
memory	O
requirements	O
need	O
not	O
be	O
exponential	O
in	O
the	O
number	O
of	O
dimensions	O
,	O
but	O
need	O
merely	O
match	O
the	O
real	O
demands	O
of	O
the	O
task	O
.	O
good	O
open-source	O
implementations	O
of	O
tile	O
coding	O
,	O
including	O
hashing	O
,	O
are	O
widely	O
available	O
.	O
exercise	O
9.4	O
suppose	O
we	O
believe	O
that	O
one	O
of	O
two	O
state	B
dimensions	O
is	O
more	O
likely	O
to	O
have	O
an	O
eﬀect	O
on	O
the	O
value	B
function	I
than	O
is	O
the	O
other	O
,	O
that	O
generalization	O
should	O
be	O
primarily	O
across	O
this	O
dimension	O
rather	O
than	O
along	O
it	O
.	O
what	O
kind	O
of	O
tilings	O
could	O
be	O
used	O
to	O
take	O
(	O
cid:3	O
)	O
advantage	O
of	O
this	O
prior	B
knowledge	I
?	O
9.5.5	O
radial	O
basis	O
functions	O
radial	O
basis	O
functions	O
(	O
rbfs	O
)	O
are	O
the	O
natural	O
generalization	O
of	O
coarse	O
coding	O
to	O
continuous-	O
valued	O
features	O
.	O
rather	O
than	O
each	O
feature	O
being	O
either	O
0	O
or	O
1	O
,	O
it	O
can	O
be	O
anything	O
in	O
the	O
interval	O
[	O
0	O
,	O
1	O
]	O
,	O
reﬂecting	O
various	O
degrees	O
to	O
which	O
the	O
feature	O
is	O
present	O
.	O
a	O
typical	O
rbf	O
feature	O
,	O
xi	O
,	O
has	O
a	O
gaussian	O
(	O
bell-shaped	O
)	O
response	O
xi	O
(	O
s	O
)	O
dependent	O
only	O
on	O
the	O
distance	O
between	O
the	O
state	B
,	O
s	O
,	O
and	O
the	O
feature	O
’	O
s	O
prototypical	O
or	O
center	O
state	B
,	O
ci	O
,	O
and	O
relative	O
to	O
the	O
feature	O
’	O
s	O
width	O
,	O
σi	O
:	O
xi	O
(	O
s	O
)	O
.	O
=	O
exp	O
(	O
cid:18	O
)	O
−||s	O
−	O
ci||2	O
2σ2	O
i	O
(	O
cid:19	O
)	O
.	O
the	O
norm	O
or	O
distance	O
metric	O
of	O
course	O
can	O
be	O
chosen	O
in	O
whatever	O
way	O
seems	O
most	O
ap-	O
propriate	O
to	O
the	O
states	O
and	O
task	O
at	O
hand	O
.	O
figure	O
9.13	O
shows	O
a	O
one-dimensional	O
example	O
with	O
a	O
euclidean	O
distance	O
metric	O
.	O
the	O
primary	O
advantage	O
of	O
rbfs	O
over	O
binary	B
features	I
is	O
that	O
they	O
produce	O
approximate	B
functions	O
that	O
vary	O
smoothly	O
and	O
are	O
diﬀerentiable	O
.	O
although	O
this	O
is	O
appealing	O
,	O
in	O
most	O
cases	O
it	O
has	O
no	O
practical	O
signiﬁcance	O
.	O
nevertheless	O
,	O
extensive	O
studies	O
have	O
been	O
made	O
of	O
graded	O
response	O
functions	O
such	O
as	O
rbfs	O
in	O
the	O
context	O
of	O
tile	O
coding	O
(	O
an	O
,	O
1991	O
;	O
miller	O
onetile	O
222	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
figure	O
9.13	O
:	O
one-dimensional	O
radial	O
basis	O
functions	O
.	O
et	O
al.	O
,	O
1991	O
;	O
an	O
et	O
al.	O
,	O
1991	O
;	O
lane	O
,	O
handelman	O
and	O
gelfand	O
,	O
1992	O
)	O
.	O
all	O
of	O
these	O
methods	O
require	O
substantial	O
additional	O
computational	O
complexity	O
(	O
over	O
tile	B
coding	I
)	O
and	O
often	O
reduce	O
performance	O
when	O
there	O
are	O
more	O
than	O
two	O
state	B
dimensions	O
.	O
in	O
high	O
dimensions	O
the	O
edges	O
of	O
tiles	O
are	O
much	O
more	O
important	O
,	O
and	O
it	O
has	O
proven	O
diﬃcult	O
to	O
obtain	O
well	O
controlled	O
graded	O
tile	O
activations	O
near	O
the	O
edges	O
.	O
an	O
rbf	O
network	O
is	O
a	O
linear	O
function	O
approximator	O
using	O
rbfs	O
for	O
its	O
features	O
.	O
learn-	O
ing	B
is	O
deﬁned	O
by	O
equations	O
(	O
9.7	O
)	O
and	O
(	O
9.8	O
)	O
,	O
exactly	O
as	O
in	O
other	O
linear	O
function	O
approxi-	O
mators	O
.	O
in	O
addition	O
,	O
some	O
learning	O
methods	O
for	O
rbf	O
networks	O
change	O
the	O
centers	O
and	O
widths	O
of	O
the	O
features	O
as	O
well	O
,	O
bringing	O
them	O
into	O
the	O
realm	O
of	O
nonlinear	O
function	O
approx-	O
imators	O
.	O
nonlinear	O
methods	O
may	O
be	O
able	O
to	O
ﬁt	O
target	B
functions	O
much	O
more	O
precisely	O
.	O
the	O
downside	O
to	O
rbf	O
networks	O
,	O
and	O
to	O
nonlinear	O
rbf	O
networks	O
especially	O
,	O
is	O
greater	O
computational	O
complexity	O
and	O
,	O
often	O
,	O
more	O
manual	O
tuning	O
before	O
learning	O
is	O
robust	O
and	O
eﬃcient	O
.	O
9.6	O
selecting	O
step-size	O
parameters	O
manually	O
most	O
sgd	O
methods	O
require	O
the	O
designer	O
to	O
select	O
an	O
appropriate	O
step-size	B
parameter	I
α.	O
ideally	O
this	O
selection	O
would	O
be	O
automated	O
,	O
and	O
in	O
some	O
cases	O
it	O
has	O
been	O
,	O
but	O
for	O
most	O
cases	O
it	O
is	O
still	O
common	O
practice	O
to	O
set	O
it	O
manually	O
.	O
to	O
do	O
this	O
,	O
and	O
to	O
better	O
understand	O
the	O
algorithms	O
,	O
it	O
is	O
useful	O
to	O
develop	O
some	O
intuitive	O
sense	O
of	O
the	O
role	O
of	O
the	O
step-size	B
parameter	I
.	O
can	O
we	O
say	O
in	O
general	O
how	O
it	O
should	O
be	O
set	O
?	O
theoretical	O
considerations	O
are	O
unfortunately	O
of	O
little	O
help	O
.	O
the	O
theory	O
of	O
stochastic	O
approximation	O
gives	O
us	O
conditions	O
(	O
2.7	O
)	O
on	O
a	O
slowly	O
decreasing	O
step-size	O
sequence	O
that	O
are	O
suﬃcient	O
to	O
guarantee	O
convergence	O
,	O
but	O
these	O
tend	O
to	O
result	O
in	O
learning	O
that	O
is	O
too	O
slow	O
.	O
the	O
classical	O
choice	O
αt	O
=	O
1/t	O
,	O
which	O
produces	O
sample	O
averages	O
in	O
tabular	O
mc	O
methods	O
,	O
is	O
not	O
appropriate	O
for	O
td	O
methods	O
,	O
for	O
nonstationary	O
problems	O
,	O
or	O
for	O
any	O
method	O
using	O
function	B
approximation	I
.	O
for	O
linear	O
methods	O
,	O
there	O
are	O
recursive	O
least-squares	O
methods	O
that	O
set	O
an	O
optimal	O
matrix	O
step	O
size	O
,	O
and	O
these	O
methods	O
can	O
be	O
extended	O
to	O
temporal-	O
diﬀerence	O
learning	O
as	O
in	O
the	O
lstd	O
method	O
described	O
in	O
section	O
9.8	O
,	O
but	O
these	O
require	O
o	O
(	O
d2	O
)	O
step-size	O
parameters	O
,	O
or	O
d	O
times	O
more	O
parameters	O
than	O
we	O
are	O
learning	O
.	O
for	O
this	O
reason	O
we	O
rule	O
them	O
out	O
for	O
use	O
on	O
large	O
problems	O
where	O
function	B
approximation	I
is	O
most	O
needed	O
.	O
to	O
get	O
some	O
intuitive	O
feel	O
for	O
how	O
to	O
set	O
the	O
step-size	B
parameter	I
manually	O
,	O
it	O
is	O
best	O
to	O
go	O
back	O
momentarily	O
to	O
the	O
tabular	O
case	O
.	O
there	O
we	O
can	O
understand	O
that	O
a	O
step	O
size	O
of	O
α	O
=	O
1	O
will	O
result	O
in	O
a	O
complete	O
elimination	O
of	O
the	O
sample	O
error	O
after	O
one	O
target	B
(	O
see	O
ci	O
!	O
ici+1ci-1	O
9.7.	O
nonlinear	O
function	B
approximation	I
:	O
artiﬁcial	B
neural	I
networks	I
223	O
(	O
2.4	O
)	O
with	O
a	O
step	O
size	O
of	O
one	O
)	O
.	O
as	O
discussed	O
on	O
page	O
201	O
,	O
we	O
usually	O
want	O
to	O
learn	O
slower	O
than	O
this	O
.	O
in	O
the	O
tabular	O
case	O
,	O
a	O
step	O
size	O
of	O
α	O
=	O
1	O
10	O
would	O
take	O
about	O
10	O
experiences	O
to	O
converge	O
approximately	O
to	O
their	O
mean	O
target	B
,	O
and	O
if	O
we	O
wanted	O
to	O
learn	O
in	O
100	O
experiences	O
we	O
would	O
use	O
α	O
=	O
1	O
τ	O
,	O
then	O
the	O
tabular	O
estimate	O
for	O
a	O
state	O
will	O
approach	O
the	O
mean	O
of	O
its	O
targets	O
,	O
with	O
the	O
most	O
recent	O
targets	O
having	O
the	O
greatest	O
eﬀect	O
,	O
after	O
about	O
τ	O
experiences	O
with	O
the	O
state	B
.	O
100	O
.	O
in	O
general	O
,	O
if	O
α	O
=	O
1	O
with	O
general	O
function	B
approximation	I
there	O
is	O
not	O
such	O
a	O
clear	O
notion	O
of	O
number	O
of	O
experiences	O
with	O
a	O
state	B
,	O
as	O
each	O
state	B
may	O
be	O
similar	O
to	O
and	O
dissimilar	O
from	O
all	O
the	O
others	O
to	O
various	O
degrees	O
.	O
however	O
,	O
there	O
is	O
a	O
similar	O
rule	O
that	O
gives	O
similar	O
behavior	O
in	O
the	O
case	O
of	O
linear	O
function	B
approximation	I
.	O
suppose	O
you	O
wanted	O
to	O
learn	O
in	O
about	O
τ	O
experiences	O
with	O
substantially	O
the	O
same	O
feature	O
vector	O
.	O
a	O
good	O
rule	O
of	O
thumb	O
for	O
setting	O
the	O
step-size	B
parameter	I
of	O
linear	O
sgd	O
methods	O
is	O
then	O
α	O
.	O
=	O
(	O
cid:0	O
)	O
τe	O
(	O
cid:2	O
)	O
x	O
(	O
cid:62	O
)	O
x	O
(	O
cid:3	O
)	O
(	O
cid:1	O
)	O
−1	O
,	O
(	O
9.19	O
)	O
where	O
x	O
is	O
a	O
random	O
feature	O
vector	B
chosen	O
from	O
the	O
same	O
distribution	O
as	O
input	O
vectors	O
will	O
be	O
in	O
the	O
sgd	O
.	O
this	O
method	O
works	O
best	O
if	O
the	O
feature	O
vectors	O
do	O
not	O
vary	O
greatly	O
in	O
length	O
;	O
ideally	O
x	O
(	O
cid:62	O
)	O
x	O
is	O
a	O
constant	O
.	O
exercise	O
9.5	O
suppose	O
you	O
are	O
using	O
tile	B
coding	I
to	O
transform	O
a	O
seven-dimensional	O
contin-	O
uous	O
state	B
space	O
into	O
binary	O
feature	O
vectors	O
to	O
estimate	O
a	O
state	B
value	O
function	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
≈	O
vπ	O
(	O
s	O
)	O
.	O
you	O
believe	O
that	O
the	O
dimensions	O
do	O
not	O
interact	O
strongly	O
,	O
so	O
you	O
decide	O
to	O
use	O
eight	O
tilings	O
of	O
each	O
dimension	O
separately	O
(	O
stripe	O
tilings	O
)	O
,	O
for	O
7	O
×	O
8	O
=	O
56	O
tilings	O
.	O
in	O
addition	O
,	O
in	O
case	O
there	O
are	O
some	O
pairwise	O
interactions	O
between	O
the	O
dimensions	O
,	O
you	O
also	O
take	O
all	O
(	O
cid:0	O
)	O
7	O
2	O
(	O
cid:1	O
)	O
=	O
21	O
pairs	O
of	O
dimensions	O
and	O
tile	O
each	O
pair	O
conjunctively	O
with	O
rectangular	O
tiles	O
.	O
you	O
make	O
two	O
tilings	O
for	O
each	O
pair	O
of	O
dimensions	O
,	O
making	O
a	O
grand	O
total	O
of	O
21	O
×	O
2	O
+	O
56	O
=	O
98	O
tilings	O
.	O
given	O
these	O
feature	O
vectors	O
,	O
you	O
suspect	O
that	O
you	O
still	O
have	O
to	O
average	O
out	O
some	O
noise	O
,	O
so	O
you	O
decide	O
that	O
you	O
want	O
learning	O
to	O
be	O
gradual	O
,	O
taking	O
about	O
10	O
presenta-	O
tions	O
with	O
the	O
same	O
feature	O
vector	O
before	O
learning	O
nears	O
its	O
asymptote	O
.	O
what	O
step-size	O
(	O
cid:3	O
)	O
parameter	O
α	O
should	O
you	O
use	O
?	O
why	O
?	O
9.7	O
nonlinear	O
function	B
approximation	I
:	O
artiﬁcial	O
neu-	O
ral	O
networks	O
artiﬁcial	B
neural	I
networks	I
(	O
anns	O
)	O
are	O
widely	O
used	O
for	O
nonlinear	O
function	B
approximation	I
.	O
an	O
ann	O
is	O
a	O
network	O
of	O
interconnected	O
units	O
that	O
have	O
some	O
of	O
the	O
properties	O
of	O
neurons	O
,	O
the	O
main	O
components	O
of	O
nervous	O
systems	O
.	O
anns	O
have	O
a	O
long	O
history	O
,	O
with	O
the	O
latest	O
advances	O
in	O
training	O
deeply-layered	O
anns	O
(	O
deep	B
learning	I
)	O
being	O
responsible	O
for	O
some	O
of	O
the	O
most	O
impressive	O
abilities	O
of	O
machine	O
learning	O
systems	O
,	O
including	O
reinforcement	B
learning	I
systems	O
.	O
in	O
chapter	O
16	O
we	O
describe	O
several	O
impressive	O
examples	O
of	O
reinforcement	O
learning	O
systems	O
that	O
use	O
ann	O
function	B
approximation	I
.	O
figure	O
9.14	O
shows	O
a	O
generic	O
feedforward	O
ann	O
,	O
meaning	O
that	O
there	O
are	O
no	O
loops	O
in	O
the	O
network	O
,	O
that	O
is	O
,	O
there	O
are	O
no	O
paths	O
within	O
the	O
network	O
by	O
which	O
a	O
unit	O
’	O
s	O
output	O
can	O
inﬂuence	O
its	O
input	O
.	O
the	O
network	O
in	O
the	O
ﬁgure	O
has	O
an	O
output	O
layer	O
consisting	O
of	O
two	O
output	O
units	O
,	O
an	O
input	O
layer	O
with	O
four	O
input	O
units	O
,	O
and	O
two	O
“	O
hidden	O
layers	O
”	O
:	O
layers	O
that	O
224	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
figure	O
9.14	O
:	O
a	O
generic	O
feedforward	O
neural	B
network	O
with	O
four	O
input	O
units	O
,	O
two	O
output	O
units	O
,	O
and	O
two	O
hidden	O
layers	O
.	O
are	O
neither	O
input	O
nor	O
output	O
layers	O
.	O
a	O
real-valued	O
weight	O
is	O
associated	O
with	O
each	O
link	O
.	O
a	O
weight	O
roughly	O
corresponds	O
to	O
the	O
eﬃcacy	O
of	O
a	O
synaptic	O
connection	O
in	O
a	O
real	O
neural	B
network	O
(	O
see	O
section	O
15.1	O
)	O
.	O
if	O
an	O
ann	O
has	O
at	O
least	O
one	O
loop	O
in	O
its	O
connections	O
,	O
it	O
is	O
a	O
re-	O
current	O
rather	O
than	O
a	O
feedforward	O
ann	O
.	O
although	O
both	O
feedforward	O
and	O
recurrent	O
anns	O
have	O
been	O
used	O
in	O
reinforcement	O
learning	O
,	O
here	O
we	O
look	O
only	O
at	O
the	O
simpler	O
feedforward	O
case	O
.	O
the	O
units	O
(	O
the	O
circles	O
in	O
figure	O
9.14	O
)	O
are	O
typically	O
semi-linear	O
units	O
,	O
meaning	O
that	O
they	O
compute	O
a	O
weighted	O
sum	O
of	O
their	O
input	O
signals	O
and	O
then	O
apply	O
to	O
the	O
result	O
a	O
nonlinear	O
function	O
,	O
called	O
the	O
activation	O
function	O
,	O
to	O
produce	O
the	O
unit	O
’	O
s	O
output	O
,	O
or	O
activation	O
.	O
diﬀerent	O
activation	O
functions	O
are	O
used	O
,	O
but	O
they	O
are	O
typically	O
s-shaped	O
,	O
or	O
sigmoid	O
,	O
functions	O
such	O
as	O
the	O
logistic	O
function	O
f	O
(	O
x	O
)	O
=	O
1/	O
(	O
1	O
+	O
e−x	O
)	O
,	O
though	O
sometimes	O
the	O
rectiﬁer	O
nonlinearity	O
f	O
(	O
x	O
)	O
=	O
max	O
(	O
0	O
,	O
x	O
)	O
is	O
used	O
.	O
a	O
step	O
function	O
like	O
f	O
(	O
x	O
)	O
=	O
1	O
if	O
x	O
≥	O
θ	O
,	O
and	O
0	O
otherwise	O
,	O
results	O
in	O
a	O
binary	O
unit	O
with	O
threshold	O
θ.	O
the	O
units	O
in	O
a	O
network	O
’	O
s	O
input	O
layer	O
are	O
somewhat	O
diﬀerent	O
in	O
having	O
their	O
activations	O
set	O
to	O
externally-supplied	O
values	O
that	O
are	O
the	O
inputs	O
to	O
the	O
function	O
the	O
network	O
is	O
approximating	O
.	O
the	O
activation	O
of	O
each	O
output	O
unit	O
of	O
a	O
feedforward	O
ann	O
is	O
a	O
nonlinear	O
function	O
of	O
the	O
activation	O
patterns	O
over	O
the	O
network	O
’	O
s	O
input	O
units	O
.	O
the	O
functions	O
are	O
parameterized	O
by	O
the	O
network	O
’	O
s	O
connection	O
weights	O
.	O
an	O
ann	O
with	O
no	O
hidden	O
layers	O
can	O
represent	O
only	O
a	O
very	O
small	O
fraction	O
of	O
the	O
possible	O
input-output	O
functions	O
.	O
however	O
an	O
ann	O
with	O
a	O
single	O
hidden	O
layer	O
containing	O
a	O
large	O
enough	O
ﬁnite	O
number	O
of	O
sigmoid	O
units	O
can	O
approximate	B
any	O
continuous	O
function	O
on	O
a	O
compact	O
region	O
of	O
the	O
network	O
’	O
s	O
input	O
space	O
to	O
any	O
degree	O
of	O
accuracy	O
(	O
cybenko	O
,	O
1989	O
)	O
.	O
this	O
is	O
also	O
true	O
for	O
other	O
nonlinear	O
activation	O
functions	O
that	O
satisfy	O
mild	O
conditions	O
,	O
but	O
nonlinearity	O
is	O
essential	O
:	O
if	O
all	O
the	O
units	O
in	O
a	O
multi-layer	O
feedforward	O
ann	O
have	O
linear	O
activation	O
functions	O
,	O
the	O
entire	O
network	O
is	O
equivalent	O
to	O
a	O
network	O
with	O
no	O
hidden	O
layers	O
(	O
because	O
linear	O
functions	O
of	O
linear	O
functions	O
are	O
themselves	O
linear	O
)	O
.	O
9.7.	O
nonlinear	O
function	B
approximation	I
:	O
artiﬁcial	B
neural	I
networks	I
225	O
despite	O
this	O
“	O
universal	O
approximation	O
”	O
property	O
of	O
one-hidden-layer	O
anns	O
,	O
both	O
ex-	O
perience	O
and	O
theory	O
show	O
that	O
approximating	O
the	O
complex	O
functions	O
needed	O
for	O
many	O
artiﬁcial	B
intelligence	I
tasks	O
is	O
made	O
easier—indeed	O
may	O
require—abstractions	O
that	O
are	O
hierarchical	O
compositions	O
of	O
many	O
layers	O
of	O
lower-level	O
abstractions	O
,	O
that	O
is	O
,	O
abstractions	O
produced	O
by	O
deep	O
architectures	O
such	O
as	O
anns	O
with	O
many	O
hidden	O
layers	O
.	O
(	O
see	O
bengio	O
,	O
2009	O
,	O
for	O
a	O
thorough	O
review	O
.	O
)	O
the	O
successive	O
layers	O
of	O
a	O
deep	O
ann	O
compute	O
increasingly	O
abstract	O
representations	O
of	O
the	O
network	O
’	O
s	O
“	O
raw	O
”	O
input	O
,	O
with	O
each	O
unit	O
providing	O
a	O
feature	O
contributing	O
to	O
a	O
hierarchical	O
representation	O
of	O
the	O
overall	O
input-output	O
function	O
of	O
the	O
network	O
.	O
training	O
the	O
hidden	O
layers	O
of	O
an	O
ann	O
is	O
therefore	O
a	O
way	O
to	O
automatically	O
create	O
features	O
appropriate	O
for	O
a	O
given	O
problem	O
so	O
that	O
hierarchical	O
representations	O
can	O
be	O
pro-	O
duced	O
without	O
relying	O
exclusively	O
on	O
hand-crafted	O
features	O
.	O
this	O
has	O
been	O
an	O
enduring	O
challenge	O
for	O
artiﬁcial	O
intelligence	O
and	O
explains	O
why	O
learning	O
algorithms	O
for	O
anns	O
with	O
hidden	O
layers	O
have	O
received	O
so	O
much	O
attention	O
over	O
the	O
years	O
.	O
anns	O
typically	O
learn	O
by	O
a	O
stochastic	O
gradient	O
method	O
(	O
section	O
9.3	O
)	O
.	O
each	O
weight	O
is	O
adjusted	O
in	O
a	O
direction	O
aimed	O
at	O
improving	O
the	O
network	O
’	O
s	O
overall	O
performance	O
as	O
measured	O
by	O
an	O
objective	O
function	O
to	O
be	O
either	O
minimized	O
or	O
maximized	O
.	O
in	O
the	O
most	O
common	O
supervised	B
learning	I
case	O
,	O
the	O
objective	O
function	O
is	O
the	O
expected	O
error	O
,	O
or	O
loss	O
,	O
over	O
a	O
set	O
of	O
labeled	O
training	O
examples	O
.	O
in	O
reinforcement	O
learning	O
,	O
anns	O
can	O
use	O
td	O
errors	O
to	O
learn	O
value	B
functions	O
,	O
or	O
they	O
can	O
aim	O
to	O
maximize	O
expected	O
reward	O
as	O
in	O
a	O
gradient	B
bandit	O
(	O
section	O
2.8	O
)	O
or	O
a	O
policy-	O
gradient	B
algorithm	O
(	O
chapter	O
13	O
)	O
.	O
in	O
all	O
of	O
these	O
cases	O
it	O
is	O
necessary	O
to	O
estimate	O
how	O
a	O
change	O
in	O
each	O
connection	O
weight	O
would	O
inﬂuence	O
the	O
network	O
’	O
s	O
overall	O
performance	O
,	O
in	O
other	O
words	O
,	O
to	O
estimate	O
the	O
partial	O
derivative	O
of	O
an	O
objective	O
function	O
with	O
respect	O
to	O
each	O
weight	O
,	O
given	O
the	O
current	O
values	O
of	O
all	O
the	O
network	O
’	O
s	O
weights	O
.	O
the	O
gradient	B
is	O
the	O
vector	B
of	O
these	O
partial	O
derivatives	O
.	O
the	O
most	O
successful	O
way	O
to	O
do	O
this	O
for	O
anns	O
with	O
hidden	O
layers	O
(	O
provided	O
the	O
units	O
have	O
diﬀerentiable	O
activation	O
functions	O
)	O
is	O
the	O
backpropagation	B
algorithm	O
,	O
which	O
consists	O
of	O
alternating	O
forward	O
and	O
backward	O
passes	O
through	O
the	O
network	O
.	O
each	O
forward	O
pass	O
computes	O
the	O
activation	O
of	O
each	O
unit	O
given	O
the	O
current	O
activations	O
of	O
the	O
network	O
’	O
s	O
input	O
units	O
.	O
after	O
each	O
forward	O
pass	O
,	O
a	O
backward	O
pass	O
eﬃciently	O
computes	O
a	O
partial	O
derivative	O
for	O
each	O
weight	O
.	O
(	O
as	O
in	O
other	O
stochastic	O
gradient	O
learning	O
algorithms	O
,	O
the	O
vector	B
of	O
these	O
partial	O
derivatives	O
is	O
an	O
estimate	O
of	O
the	O
true	O
gradient	O
.	O
)	O
in	O
section	O
15.10	O
we	O
discuss	O
methods	O
for	O
training	O
anns	O
with	O
hidden	O
layers	O
that	O
use	O
reinforcement	B
learning	I
principles	O
instead	O
of	O
backpropagation	O
.	O
these	O
methods	O
are	O
less	O
eﬃcient	O
than	O
the	O
backpropagation	B
algorithm	O
,	O
but	O
they	O
may	O
be	O
closer	O
to	O
how	O
real	O
neural	B
networks	I
learn	O
.	O
the	O
backpropagation	B
algorithm	O
can	O
produce	O
good	O
results	O
for	O
shallow	O
networks	O
having	O
1	O
or	O
2	O
hidden	O
layers	O
,	O
but	O
it	O
may	O
not	O
work	O
well	O
for	O
deeper	O
anns	O
.	O
in	O
fact	O
,	O
training	O
a	O
network	O
with	O
k	O
+	O
1	O
hidden	O
layers	O
can	O
actually	O
result	O
in	O
poorer	O
performance	O
than	O
training	O
a	O
network	O
with	O
k	O
hidden	O
layers	O
,	O
even	O
though	O
the	O
deeper	O
network	O
can	O
represent	O
all	O
the	O
functions	O
that	O
the	O
shallower	O
network	O
can	O
(	O
bengio	O
,	O
2009	O
)	O
.	O
explaining	O
results	O
like	O
these	O
is	O
not	O
easy	O
,	O
but	O
several	O
factors	O
are	O
important	O
.	O
first	O
,	O
the	O
large	O
number	O
of	O
weights	O
in	O
a	O
typical	O
deep	O
ann	O
makes	O
it	O
diﬃcult	O
to	O
avoid	O
the	O
problem	O
of	O
overﬁtting	O
,	O
that	O
is	O
,	O
the	O
problem	O
of	O
failing	O
to	O
generalize	O
correctly	O
to	O
cases	O
on	O
which	O
the	O
network	O
has	O
not	O
been	O
trained	O
.	O
second	O
,	O
backpropagation	B
does	O
not	O
work	O
well	O
for	O
deep	O
anns	O
because	O
the	O
partial	O
226	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
derivatives	O
computed	O
by	O
its	O
backward	O
passes	O
either	O
decay	O
rapidly	O
toward	O
the	O
input	O
side	O
of	O
the	O
network	O
,	O
making	O
learning	O
by	O
deep	O
layers	O
extremely	O
slow	O
,	O
or	O
the	O
partial	O
derivatives	O
grow	O
rapidly	O
toward	O
the	O
input	O
side	O
of	O
the	O
network	O
,	O
making	O
learning	O
unstable	O
.	O
methods	O
for	O
dealing	O
with	O
these	O
problems	O
are	O
largely	O
responsible	O
for	O
many	O
impressive	O
recent	O
results	O
achieved	O
by	O
systems	O
that	O
use	O
deep	O
anns	O
.	O
overﬁtting	O
is	O
a	O
problem	O
for	O
any	O
function	B
approximation	I
method	O
that	O
adjusts	O
func-	O
tions	O
with	O
many	O
degrees	O
of	O
freedom	O
on	O
the	O
basis	O
of	O
limited	O
training	O
data	O
.	O
it	O
is	O
less	O
of	O
a	O
problem	O
for	O
online	O
reinforcement	B
learning	I
that	O
does	O
not	O
rely	O
on	O
limited	O
training	O
sets	O
,	O
but	O
generalizing	O
eﬀectively	O
is	O
still	O
an	O
important	O
issue	O
.	O
overﬁtting	O
is	O
a	O
problem	O
for	O
anns	O
in	O
general	O
,	O
but	O
especially	O
so	O
for	O
deep	O
anns	O
because	O
they	O
tend	O
to	O
have	O
very	O
large	O
numbers	O
of	O
weights	O
.	O
many	O
methods	O
have	O
been	O
developed	O
for	O
reducing	O
overﬁtting	O
.	O
these	O
include	O
stopping	O
training	O
when	O
performance	O
begins	O
to	O
decrease	O
on	O
validation	O
data	O
diﬀerent	O
from	O
the	O
training	O
data	O
(	O
cross	O
validation	O
)	O
,	O
modifying	O
the	O
objective	O
function	O
to	O
discourage	O
com-	O
plexity	O
of	O
the	O
approximation	O
(	O
regularization	O
)	O
,	O
and	O
introducing	O
dependencies	O
among	O
the	O
weights	O
to	O
reduce	O
the	O
number	O
of	O
degrees	O
of	O
freedom	O
(	O
e.g.	O
,	O
weight	O
sharing	O
)	O
.	O
a	O
particularly	O
eﬀective	O
method	O
for	O
reducing	O
overﬁtting	O
by	O
deep	O
anns	O
is	O
the	O
dropout	O
method	O
introduced	O
by	O
srivastava	O
,	O
hinton	O
,	O
krizhevsky	O
,	O
sutskever	O
,	O
and	O
salakhutdinov	O
(	O
2014	O
)	O
.	O
during	O
training	O
,	O
units	O
are	O
randomly	O
removed	O
from	O
the	O
network	O
(	O
dropped	O
out	O
)	O
along	O
with	O
their	O
connections	O
.	O
this	O
can	O
be	O
thought	O
of	O
as	O
training	O
a	O
large	O
number	O
of	O
“	O
thinned	O
”	O
networks	O
.	O
combining	O
the	O
results	O
of	O
these	O
thinned	O
networks	O
at	O
test	O
time	O
is	O
a	O
way	O
to	O
improve	O
generalization	O
performance	O
.	O
the	O
dropout	O
method	O
eﬃciently	O
approxi-	O
mates	O
this	O
combination	O
by	O
multiplying	O
each	O
outgoing	O
weight	O
of	O
a	O
unit	O
by	O
the	O
probability	O
that	O
that	O
unit	O
was	O
retained	O
during	O
training	O
.	O
srivastava	O
et	O
al	O
.	O
found	O
that	O
this	O
method	O
signiﬁcantly	O
improves	O
generalization	O
performance	O
.	O
it	O
encourages	O
individual	O
hidden	O
units	O
to	O
learn	O
features	O
that	O
work	O
well	O
with	O
random	O
collections	O
of	O
other	O
features	O
.	O
this	O
increases	O
the	O
versatility	O
of	O
the	O
features	O
formed	O
by	O
the	O
hidden	O
units	O
so	O
that	O
the	O
network	O
does	O
not	O
overly	O
specialize	O
to	O
rarely-occurring	O
cases	O
.	O
hinton	O
,	O
osindero	O
,	O
and	O
teh	O
(	O
2006	O
)	O
took	O
a	O
major	O
step	O
toward	O
solving	O
the	O
problem	O
of	O
training	O
the	O
deep	O
layers	O
of	O
a	O
deep	O
ann	O
in	O
their	O
work	O
with	O
deep	O
belief	O
networks	O
,	O
layered	O
networks	O
closely	O
related	O
to	O
the	O
deep	O
anns	O
discussed	O
here	O
.	O
in	O
their	O
method	O
,	O
the	O
deepest	O
layers	O
are	O
trained	O
one	O
at	O
a	O
time	O
using	O
an	O
unsupervised	B
learning	I
algorithm	O
.	O
without	O
relying	O
on	O
the	O
overall	O
objective	O
function	O
,	O
unsupervised	B
learning	I
can	O
extract	O
features	O
that	O
capture	O
statistical	O
regularities	O
of	O
the	O
input	O
stream	O
.	O
the	O
deepest	O
layer	O
is	O
trained	O
ﬁrst	O
,	O
then	O
with	O
input	O
provided	O
by	O
this	O
trained	O
layer	O
,	O
the	O
next	O
deepest	O
layer	O
is	O
trained	O
,	O
and	O
so	O
on	O
,	O
until	O
the	O
weights	O
in	O
all	O
,	O
or	O
many	O
,	O
of	O
the	O
network	O
’	O
s	O
layers	O
are	O
set	O
to	O
values	O
that	O
now	O
act	O
as	O
initial	O
values	O
for	O
supervised	O
learning	O
.	O
the	O
network	O
is	O
then	O
ﬁne-tuned	O
by	O
backpropagation	B
with	O
respect	O
to	O
the	O
overall	O
objective	O
function	O
.	O
studies	O
show	O
that	O
this	O
approach	O
generally	O
works	O
much	O
better	O
than	O
backpropagation	B
with	O
weights	O
initialized	O
with	O
random	O
values	O
.	O
the	O
better	O
performance	O
of	O
networks	O
trained	O
with	O
weights	O
initialized	O
this	O
way	O
could	O
be	O
due	O
to	O
many	O
factors	O
,	O
but	O
one	O
idea	O
is	O
that	O
this	O
method	O
places	O
the	O
network	O
in	O
a	O
region	O
of	O
weight	O
space	O
from	O
which	O
a	O
gradient-based	O
algorithm	O
can	O
make	O
good	O
progress	O
.	O
batch	O
normalization	O
(	O
ioﬀe	O
and	O
szegedy	O
,	O
2015	O
)	O
is	O
another	O
technique	O
that	O
makes	O
it	O
easier	O
to	O
train	O
deep	O
anns	O
.	O
it	O
has	O
long	O
been	O
known	O
that	O
ann	O
learning	O
is	O
easier	O
if	O
the	O
network	O
input	O
is	O
normalized	O
,	O
for	O
example	O
,	O
by	O
adjusting	O
each	O
input	O
variable	O
to	O
have	O
zero	O
9.7.	O
nonlinear	O
function	B
approximation	I
:	O
artiﬁcial	B
neural	I
networks	I
227	O
mean	O
and	O
unit	O
variance	O
.	O
batch	O
normalization	O
for	O
training	O
deep	O
anns	O
normalizes	O
the	O
output	O
of	O
deep	O
layers	O
before	O
they	O
feed	O
into	O
the	O
following	O
layer	O
.	O
ioﬀe	O
and	O
szegedy	O
(	O
2015	O
)	O
used	O
statistics	O
from	O
subsets	O
,	O
or	O
“	O
mini-batches	O
,	O
”	O
of	O
training	O
examples	O
to	O
normalize	O
these	O
between-layer	O
signals	O
to	O
improve	O
the	O
learning	O
rate	O
of	O
deep	O
anns	O
.	O
another	O
technique	O
useful	O
for	O
training	O
deep	O
anns	O
is	O
deep	B
residual	I
learning	I
(	O
he	O
,	O
zhang	O
,	O
ren	O
,	O
and	O
sun	O
,	O
2016	O
)	O
.	O
sometimes	O
it	O
is	O
easier	O
to	O
learn	O
how	O
a	O
function	O
diﬀers	O
from	O
the	O
iden-	O
tity	O
function	O
than	O
to	O
learn	O
the	O
function	O
itself	O
.	O
then	O
adding	O
this	O
diﬀerence	O
,	O
or	O
residual	O
function	O
,	O
to	O
the	O
input	O
produces	O
the	O
desired	O
function	O
.	O
in	O
deep	O
anns	O
,	O
a	O
block	O
of	O
layers	O
can	O
be	O
made	O
to	O
learn	O
a	O
residual	O
function	O
simply	O
by	O
adding	O
shortcut	O
,	O
or	O
skip	O
,	O
connec-	O
tions	O
around	O
the	O
block	O
.	O
these	O
connections	O
add	O
the	O
input	O
to	O
the	O
block	O
to	O
its	O
output	O
,	O
and	O
no	O
additional	O
weights	O
are	O
needed	O
.	O
he	O
et	O
al	O
.	O
(	O
2016	O
)	O
evaluated	O
this	O
method	O
using	O
deep	O
convolutional	O
networks	O
with	O
skip	O
connections	O
around	O
every	O
pair	O
of	O
adjacent	O
layers	O
,	O
ﬁnd-	O
ing	B
substantial	O
improvement	O
over	O
networks	O
without	O
the	O
skip	O
connections	O
on	O
benchmark	O
image	O
classiﬁcation	O
tasks	O
.	O
both	O
batch	O
normalization	O
and	O
deep	O
residual	O
learning	O
were	O
used	O
in	O
the	O
reinforcement	O
learning	O
application	O
to	O
the	O
game	O
of	O
go	O
that	O
we	O
describe	O
in	O
chapter	O
16.	O
a	O
type	O
of	O
deep	O
ann	O
that	O
has	O
proven	O
to	O
be	O
very	O
successful	O
in	O
applications	O
,	O
including	O
impressive	O
reinforcement	B
learning	I
applications	O
(	O
chapter	O
16	O
)	O
,	O
is	O
the	O
deep	O
convolutional	O
network	O
.	O
this	O
type	O
of	O
network	O
is	O
specialized	O
for	O
processing	O
high-dimensional	O
data	O
ar-	O
ranged	O
in	O
spatial	O
arrays	O
,	O
such	O
as	O
images	O
.	O
it	O
was	O
inspired	O
by	O
how	O
early	O
visual	O
processing	O
works	O
in	O
the	O
brain	O
(	O
lecun	O
,	O
bottou	O
,	O
bengio	O
and	O
haﬀner	O
,	O
1998	O
)	O
.	O
because	O
of	O
its	O
special	O
architecture	O
,	O
a	O
deep	O
convolutional	O
network	O
can	O
be	O
trained	O
by	O
backpropagation	B
without	O
resorting	O
to	O
methods	O
like	O
those	O
described	O
above	O
to	O
train	O
the	O
deep	O
layers	O
.	O
figure	O
9.15	O
illustrates	O
the	O
architecture	O
of	O
a	O
deep	O
convolutional	O
network	O
.	O
this	O
instance	O
,	O
from	O
lecun	O
et	O
al	O
.	O
(	O
1998	O
)	O
,	O
was	O
designed	O
to	O
recognize	O
hand-written	O
characters	O
.	O
it	O
consists	O
of	O
alternating	O
convolutional	O
and	O
subsampling	O
layers	O
,	O
followed	O
by	O
several	O
fully	O
connected	O
ﬁnal	O
layers	O
.	O
each	O
convolutional	O
layer	O
produces	O
a	O
number	O
of	O
feature	O
maps	O
.	O
a	O
feature	O
map	O
is	O
a	O
pattern	O
of	O
activity	O
over	O
an	O
array	O
of	O
units	O
,	O
where	O
each	O
unit	O
performs	O
the	O
same	O
figure	O
9.15	O
:	O
deep	O
convolutional	O
network	O
.	O
republished	O
with	O
permission	O
of	O
proceedings	O
of	O
the	O
ieee	O
,	O
from	O
gradient-based	O
learning	O
applied	O
to	O
document	O
recognition	O
,	O
lecun	O
,	O
bottou	O
,	O
bengio	O
,	O
and	O
haﬀner	O
,	O
volume	O
86	O
,	O
1998	O
;	O
permission	O
conveyed	O
through	O
copyright	O
clearance	O
center	O
,	O
inc.	O
228	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
operation	O
on	O
data	O
in	O
its	O
receptive	O
ﬁeld	O
,	O
which	O
is	O
the	O
part	O
of	O
the	O
data	O
it	O
“	O
sees	O
”	O
from	O
the	O
preceding	O
layer	O
(	O
or	O
from	O
the	O
external	O
input	O
in	O
the	O
case	O
of	O
the	O
ﬁrst	O
convolutional	O
layer	O
)	O
.	O
the	O
units	O
of	O
a	O
feature	O
map	O
are	O
identical	O
to	O
one	O
another	O
except	O
that	O
their	O
receptive	O
ﬁelds	O
,	O
which	O
are	O
all	O
the	O
same	O
size	O
and	O
shape	O
,	O
are	O
shifted	O
to	O
diﬀerent	O
locations	O
on	O
the	O
arrays	O
of	O
incoming	O
data	O
.	O
units	O
in	O
the	O
same	O
feature	O
map	O
share	O
the	O
same	O
weights	O
.	O
this	O
means	O
that	O
a	O
feature	O
map	O
detects	O
the	O
same	O
feature	O
no	O
matter	O
where	O
it	O
is	O
located	O
in	O
the	O
input	O
array	O
.	O
in	O
the	O
network	O
in	O
figure	O
9.15	O
,	O
for	O
example	O
,	O
the	O
ﬁrst	O
convolutional	O
layer	O
produces	O
6	O
feature	O
maps	O
,	O
each	O
consisting	O
of	O
28	O
×	O
28	O
units	O
.	O
each	O
unit	O
in	O
each	O
feature	O
map	O
has	O
a	O
5	O
×	O
5	O
receptive	O
ﬁeld	O
,	O
and	O
these	O
receptive	O
ﬁelds	O
overlap	O
(	O
in	O
this	O
case	O
by	O
four	O
columns	O
and	O
four	O
rows	O
)	O
.	O
consequently	O
,	O
each	O
of	O
the	O
6	O
feature	O
maps	O
is	O
speciﬁed	O
by	O
just	O
25	O
adjustable	O
weights	O
.	O
the	O
subsampling	O
layers	O
of	O
a	O
deep	O
convolutional	O
network	O
reduce	O
the	O
spatial	O
resolution	O
of	O
the	O
feature	O
maps	O
.	O
each	O
feature	O
map	O
in	O
a	O
subsampling	O
layer	O
consists	O
of	O
units	O
that	O
average	O
over	O
a	O
receptive	O
ﬁeld	O
of	O
units	O
in	O
the	O
feature	O
maps	O
of	O
the	O
preceding	O
convolutional	O
layer	O
.	O
for	O
example	O
,	O
each	O
unit	O
in	O
each	O
of	O
the	O
6	O
feature	O
maps	O
in	O
the	O
ﬁrst	O
subsampling	O
layer	O
of	O
the	O
network	O
of	O
figure	O
9.15	O
averages	O
over	O
a	O
2	O
×	O
2	O
non-overlapping	O
receptive	O
ﬁeld	O
over	O
one	O
of	O
the	O
feature	O
maps	O
produced	O
by	O
the	O
ﬁrst	O
convolutional	O
layer	O
,	O
resulting	O
in	O
six	O
14	O
×	O
14	O
feature	O
maps	O
.	O
subsampling	O
layers	O
reduce	O
the	O
network	O
’	O
s	O
sensitivity	O
to	O
the	O
spatial	O
locations	O
of	O
the	O
features	O
detected	O
,	O
that	O
is	O
,	O
they	O
help	O
make	O
the	O
network	O
’	O
s	O
responses	O
spatially	O
invariant	O
.	O
this	O
is	O
useful	O
because	O
a	O
feature	O
detected	O
at	O
one	O
place	O
in	O
an	O
image	O
is	O
likely	O
to	O
be	O
useful	O
at	O
other	O
places	O
as	O
well	O
.	O
advances	O
in	O
the	O
design	O
and	O
training	O
of	O
anns—of	O
which	O
we	O
have	O
only	O
mentioned	O
a	O
few—all	O
contribute	O
to	O
reinforcement	B
learning	I
.	O
although	O
current	O
reinforcement	O
learn-	O
ing	B
theory	O
is	O
mostly	O
limited	O
to	O
methods	O
using	O
tabular	O
or	O
linear	B
function	I
approximation	I
methods	O
,	O
the	O
impressive	O
performances	O
of	O
notable	O
reinforcement	B
learning	I
applications	O
owe	O
much	O
of	O
their	O
success	O
to	O
nonlinear	O
function	B
approximation	I
by	O
multi-layer	O
anns	O
.	O
we	O
discuss	O
several	O
of	O
these	O
applications	O
in	O
chapter	O
16	O
.	O
9.8	O
least-squares	O
td	O
all	O
the	O
methods	O
we	O
have	O
discussed	O
so	O
far	O
in	O
this	O
chapter	O
have	O
required	O
computation	O
per	O
time	O
step	O
proportional	O
to	O
the	O
number	O
of	O
parameters	O
.	O
with	O
more	O
computation	O
,	O
however	O
,	O
one	O
can	O
do	O
better	O
.	O
in	O
this	O
section	O
we	O
present	O
a	O
method	O
for	O
linear	O
function	B
approximation	I
that	O
is	O
arguably	O
the	O
best	O
that	O
can	O
be	O
done	O
for	O
this	O
case	O
.	O
as	O
we	O
established	O
in	O
section	O
9.4	O
td	O
(	O
0	O
)	O
with	O
linear	O
function	B
approximation	I
converges	O
asymptotically	O
(	O
for	O
appropriately	O
decreasing	O
step	O
sizes	O
)	O
to	O
the	O
td	O
ﬁxed	O
point	O
:	O
wtd	O
=	O
a−1b	O
,	O
where	O
a	O
and	O
.	O
=	O
e	O
[	O
rt+1xt	O
]	O
.	O
b	O
.	O
=	O
e	O
(	O
cid:2	O
)	O
xt	O
(	O
xt	O
−	O
γxt+1	O
)	O
(	O
cid:62	O
)	O
(	O
cid:3	O
)	O
why	O
,	O
one	O
might	O
ask	O
,	O
must	O
we	O
compute	O
this	O
solution	O
iteratively	O
?	O
this	O
is	O
wasteful	O
of	O
data	O
!	O
could	O
one	O
not	O
do	O
better	O
by	O
computing	O
estimates	O
of	O
a	O
and	O
b	O
,	O
and	O
then	O
directly	O
9.8.	O
least-squares	O
td	O
229	O
computing	O
the	O
td	O
ﬁxed	O
point	O
?	O
the	O
least-squares	O
td	O
algorithm	O
,	O
commonly	O
known	O
as	O
lstd	O
,	O
does	O
exactly	O
this	O
.	O
it	O
forms	O
the	O
natural	O
estimates	O
.	O
=	O
t−1	O
(	O
cid:88	O
)	O
k=0	O
xk	O
(	O
xk	O
−	O
γxk+1	O
)	O
(	O
cid:62	O
)	O
+	O
εi	O
and	O
.	O
=	O
t−1	O
(	O
cid:88	O
)	O
k=0	O
rt+1xk	O
,	O
(	O
9.20	O
)	O
(	O
cid:98	O
)	O
bt	O
where	O
i	O
is	O
the	O
identity	O
matrix	O
,	O
and	O
εi	O
,	O
for	O
some	O
small	O
ε	O
>	O
0	O
,	O
ensures	O
that	O
(	O
cid:98	O
)	O
at	O
is	O
always	O
invertible	O
.	O
it	O
might	O
seem	O
that	O
these	O
estimates	O
should	O
both	O
be	O
divided	O
by	O
t	O
+	O
1	O
,	O
and	O
indeed	O
they	O
should	O
;	O
as	O
deﬁned	O
here	O
,	O
these	O
are	O
really	O
estimates	O
of	O
t	O
+	O
1	O
times	O
a	O
and	O
t	O
+	O
1	O
times	O
b.	O
however	O
,	O
the	O
t	O
+	O
1	O
factor	O
will	O
not	O
matter	O
,	O
as	O
when	O
we	O
use	O
these	O
estimates	O
we	O
will	O
be	O
eﬀectively	O
dividing	O
one	O
by	O
the	O
other	O
.	O
lstd	O
estimates	O
the	O
td	O
ﬁxed	O
point	O
as	O
(	O
cid:98	O
)	O
at	O
wt	O
.	O
t	O
(	O
cid:98	O
)	O
bt	O
.	O
=	O
(	O
cid:98	O
)	O
a−1	O
(	O
9.21	O
)	O
this	O
algorithm	O
is	O
the	O
most	O
data	O
eﬃcient	O
form	O
of	O
linear	O
td	O
(	O
0	O
)	O
,	O
but	O
it	O
is	O
also	O
more	O
expen-	O
sive	O
computationally	O
.	O
recall	O
that	O
semi-gradient	O
td	O
(	O
0	O
)	O
requires	O
memory	O
and	O
per-step	O
computation	O
that	O
is	O
only	O
o	O
(	O
d	O
)	O
.	O
how	O
complex	O
is	O
lstd	O
?	O
as	O
it	O
is	O
written	O
above	O
the	O
complexity	O
seems	O
to	O
increase	O
with	O
t	O
,	O
but	O
the	O
two	O
approximations	O
in	O
(	O
9.20	O
)	O
could	O
be	O
implemented	O
incrementally	O
using	O
the	O
techniques	O
we	O
have	O
covered	O
earlier	O
(	O
e.g.	O
,	O
in	O
chapter	O
2	O
)	O
so	O
that	O
they	O
can	O
be	O
done	O
in	O
would	O
be	O
o	O
(	O
d2	O
)	O
.	O
column	O
vector	B
times	O
a	O
row	O
vector	B
)	O
and	O
thus	O
would	O
be	O
a	O
matrix	O
update	O
;	O
its	O
computational	O
constant	O
time	O
per	O
step	O
.	O
even	O
so	O
,	O
the	O
update	O
for	O
(	O
cid:98	O
)	O
at	O
would	O
involve	O
an	O
outer	O
product	O
(	O
a	O
complexity	O
would	O
be	O
o	O
(	O
d2	O
)	O
,	O
and	O
of	O
course	O
the	O
memory	O
required	O
to	O
hold	O
the	O
(	O
cid:98	O
)	O
at	O
matrix	O
of	O
(	O
cid:98	O
)	O
at	O
,	O
and	O
the	O
computational	O
complexity	O
of	O
a	O
general	O
inverse	O
computation	O
is	O
o	O
(	O
d3	O
)	O
.	O
fortunately	O
,	O
an	O
inverse	O
of	O
a	O
matrix	O
of	O
our	O
special	O
form—a	O
sum	O
of	O
outer	O
products—can	O
also	O
be	O
updated	O
incrementally	O
with	O
only	O
o	O
(	O
d2	O
)	O
computations	O
,	O
as	O
a	O
potentially	O
greater	O
problem	O
is	O
that	O
our	O
ﬁnal	O
computation	O
(	O
9.21	O
)	O
uses	O
the	O
inverse	O
t−1	O
−	O
(	O
cid:98	O
)	O
a−1	O
t	O
=	O
(	O
cid:16	O
)	O
(	O
cid:98	O
)	O
at−1	O
+	O
xt	O
(	O
xt	O
−	O
γxt+1	O
)	O
(	O
cid:62	O
)	O
(	O
cid:17	O
)	O
−1	O
(	O
cid:98	O
)	O
a−1	O
t−1xt	O
(	O
xt	O
−	O
γxt+1	O
)	O
(	O
cid:62	O
)	O
(	O
cid:98	O
)	O
a−1	O
=	O
(	O
cid:98	O
)	O
a−1	O
1	O
+	O
(	O
xt	O
−	O
γxt+1	O
)	O
(	O
cid:62	O
)	O
(	O
cid:98	O
)	O
a−1	O
for	O
t	O
>	O
0	O
,	O
with	O
(	O
cid:98	O
)	O
a0	O
multiplications	O
and	O
thus	O
is	O
only	O
o	O
(	O
d2	O
)	O
.	O
thus	O
we	O
can	O
store	O
the	O
inverse	O
matrix	O
(	O
cid:98	O
)	O
a−1	O
.	O
=	O
εi	O
.	O
although	O
the	O
identity	O
(	O
9.22	O
)	O
,	O
known	O
as	O
the	O
sherman-morrison	O
formula	O
,	O
is	O
superﬁcially	O
complicated	O
,	O
it	O
involves	O
only	O
vector-matrix	O
and	O
vector-vector	O
,	O
maintain	O
it	O
with	O
(	O
9.22	O
)	O
,	O
and	O
then	O
use	O
it	O
in	O
(	O
9.21	O
)	O
,	O
all	O
with	O
only	O
o	O
(	O
d2	O
)	O
memory	O
and	O
per-step	O
computation	O
.	O
the	O
complete	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
.	O
t−1	O
t−1xt	O
(	O
from	O
(	O
9.20	O
)	O
)	O
(	O
9.22	O
)	O
,	O
t	O
of	O
course	O
,	O
o	O
(	O
d2	O
)	O
is	O
still	O
signiﬁcantly	O
more	O
expensive	O
than	O
the	O
o	O
(	O
d	O
)	O
of	O
semi-gradient	O
td	O
.	O
whether	O
the	O
greater	O
data	O
eﬃciency	O
of	O
lstd	O
is	O
worth	O
this	O
computational	O
expense	O
depends	O
on	O
how	O
large	O
d	O
is	O
,	O
how	O
important	O
it	O
is	O
to	O
learn	O
quickly	O
,	O
and	O
the	O
expense	O
of	O
other	O
parts	O
of	O
the	O
system	O
.	O
the	O
fact	O
that	O
lstd	O
requires	O
no	O
step-size	B
parameter	I
is	O
sometimes	O
also	O
touted	O
,	O
but	O
the	O
advantage	O
of	O
this	O
is	O
probably	O
overstated	O
.	O
lstd	O
does	O
not	O
require	O
a	O
230	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
lstd	O
for	O
estimating	O
ˆv	O
=	O
w	O
(	O
cid:62	O
)	O
x	O
(	O
·	O
)	O
≈	O
vπ	O
(	O
o	O
(	O
d2	O
)	O
version	O
)	O
input	O
:	O
feature	O
representation	O
x	O
:	O
s+	O
→	O
rd	O
such	O
that	O
x	O
(	O
terminal	O
)	O
=	O
0	O
algorithm	O
parameter	O
:	O
small	O
ε	O
>	O
0	O
(	O
cid:100	O
)	O
a−1	O
←	O
ε−1i	O
(	O
cid:98	O
)	O
b	O
←	O
0	O
loop	O
for	O
each	O
episode	O
:	O
a	O
d	O
×	O
d	O
matrix	O
a	O
d-dimensional	O
vector	B
initialize	O
s	O
;	O
x	O
←	O
x	O
(	O
s	O
)	O
loop	O
for	O
each	O
step	O
of	O
episode	O
:	O
choose	O
and	O
take	O
action	B
a	O
∼	O
π	O
(	O
·|s	O
)	O
,	O
observe	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
;	O
x	O
(	O
cid:48	O
)	O
←	O
x	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
v	O
←	O
(	O
cid:100	O
)	O
a−1	O
(	O
cid:62	O
)	O
(	O
x	O
−	O
γx	O
(	O
cid:48	O
)	O
)	O
(	O
cid:100	O
)	O
a−1	O
←	O
(	O
cid:100	O
)	O
a−1	O
−	O
(	O
cid:0	O
)	O
(	O
cid:100	O
)	O
a−1x	O
(	O
cid:1	O
)	O
v	O
(	O
cid:62	O
)	O
/	O
(	O
cid:0	O
)	O
1	O
+	O
v	O
(	O
cid:62	O
)	O
x	O
(	O
cid:1	O
)	O
(	O
cid:98	O
)	O
b	O
←	O
(	O
cid:98	O
)	O
b	O
+	O
rx	O
w	O
←	O
(	O
cid:100	O
)	O
a−1	O
(	O
cid:98	O
)	O
b	O
s	O
←	O
s	O
(	O
cid:48	O
)	O
;	O
x	O
←	O
x	O
(	O
cid:48	O
)	O
until	O
s	O
(	O
cid:48	O
)	O
is	O
terminal	O
step	O
size	O
,	O
but	O
it	O
does	O
requires	O
ε	O
;	O
if	O
ε	O
is	O
chosen	O
too	O
small	O
the	O
sequence	O
of	O
inverses	O
can	O
vary	O
wildly	O
,	O
and	O
if	O
ε	O
is	O
chosen	O
too	O
large	O
then	O
learning	O
is	O
slowed	O
.	O
in	O
addition	O
,	O
lstd	O
’	O
s	O
lack	O
of	O
a	O
step-size	B
parameter	I
means	O
that	O
it	O
never	O
forgets	O
.	O
this	O
is	O
sometimes	O
desirable	O
,	O
but	O
it	O
is	O
problematic	O
if	O
the	O
target	B
policy	O
π	O
changes	O
as	O
it	O
does	O
in	O
reinforcement	O
learning	O
and	O
gpi	O
.	O
in	O
control	O
applications	O
,	O
lstd	O
typically	O
has	O
to	O
be	O
combined	O
with	O
some	O
other	O
mechanism	O
to	O
induce	O
forgeting	O
,	O
mooting	O
any	O
initial	O
advantage	O
of	O
not	O
requiring	O
a	O
step-size	B
parameter	I
.	O
9.9	O
memory-based	O
function	O
approximation	O
so	O
far	O
we	O
have	O
discussed	O
the	O
parametric	O
approach	O
to	O
approximating	O
value	B
functions	O
.	O
in	O
this	O
approach	O
,	O
a	O
learning	O
algorithm	O
adjusts	O
the	O
parameters	O
of	O
a	O
functional	O
form	O
intended	O
to	O
approximate	B
the	O
value	B
function	I
over	O
a	O
problem	O
’	O
s	O
entire	O
state	B
space	O
.	O
each	O
update	O
,	O
s	O
(	O
cid:55	O
)	O
→	O
g	O
,	O
is	O
a	O
training	O
example	O
used	O
by	O
the	O
learning	O
algorithm	O
to	O
change	O
the	O
parameters	O
with	O
the	O
aim	O
of	O
reducing	O
the	O
approximation	O
error	O
.	O
after	O
the	O
update	O
,	O
the	O
training	O
example	O
can	O
be	O
discarded	O
(	O
although	O
it	O
might	O
be	O
saved	O
to	O
be	O
used	O
again	O
)	O
.	O
when	O
an	O
approximate	B
value	O
of	O
a	O
state	B
(	O
which	O
we	O
will	O
call	O
the	O
query	O
state	B
)	O
is	O
needed	O
,	O
the	O
function	O
is	O
simply	O
evaluated	O
at	O
that	O
state	B
using	O
the	O
latest	O
parameters	O
produced	O
by	O
the	O
learning	O
algorithm	O
.	O
memory-based	O
function	O
approximation	O
methods	O
are	O
very	O
diﬀerent	O
.	O
they	O
simply	O
save	O
training	O
examples	O
in	O
memory	O
as	O
they	O
arrive	O
(	O
or	O
at	O
least	O
save	O
a	O
subset	O
of	O
the	O
examples	O
)	O
without	O
updating	O
any	O
parameters	O
.	O
then	O
,	O
whenever	O
a	O
query	O
state	B
’	O
s	O
value	B
estimate	O
is	O
needed	O
,	O
a	O
set	O
of	O
examples	O
is	O
retrieved	O
from	O
memory	O
and	O
used	O
to	O
compute	O
a	O
value	B
estimate	O
for	O
the	O
query	O
state	B
.	O
this	O
approach	O
is	O
sometimes	O
called	O
lazy	O
learning	O
because	O
processing	O
training	O
examples	O
is	O
postponed	O
until	O
the	O
system	O
is	O
queried	O
to	O
provide	O
an	O
output	O
.	O
memory-based	O
function	O
approximation	O
methods	O
are	O
prime	O
examples	O
of	O
nonparametric	O
9.9.	O
memory-based	O
function	O
approximation	O
231	O
methods	O
.	O
unlike	O
parametric	O
methods	O
,	O
the	O
approximating	O
function	O
’	O
s	O
form	O
is	O
not	O
limited	O
to	O
a	O
ﬁxed	O
parameterized	O
class	O
of	O
functions	O
,	O
such	O
as	O
linear	O
functions	O
or	O
polynomials	O
,	O
but	O
is	O
instead	O
determined	O
by	O
the	O
training	O
examples	O
themselves	O
,	O
together	O
with	O
some	O
means	O
for	O
combining	O
them	O
to	O
output	O
estimated	O
values	O
for	O
query	O
states	O
.	O
as	O
more	O
training	O
exam-	O
ples	O
accumulate	O
in	O
memory	O
,	O
one	O
expects	O
nonparametric	O
methods	O
to	O
produce	O
increasingly	O
accurate	O
approximations	O
of	O
any	O
target	B
function	O
.	O
there	O
are	O
many	O
diﬀerent	O
memory-based	O
methods	O
depending	O
on	O
how	O
the	O
stored	O
train-	O
ing	B
examples	O
are	O
selected	O
and	O
how	O
they	O
are	O
used	O
to	O
respond	O
to	O
a	O
query	O
.	O
here	O
,	O
we	O
focus	O
on	O
local-learning	O
methods	O
that	O
approximate	B
a	O
value	B
function	I
only	O
locally	O
in	O
the	O
neigh-	O
borhood	O
of	O
the	O
current	O
query	O
state	B
.	O
these	O
methods	O
retrieve	O
a	O
set	O
of	O
training	O
examples	O
from	O
memory	O
whose	O
states	O
are	O
judged	O
to	O
be	O
the	O
most	O
relevant	O
to	O
the	O
query	O
state	B
,	O
where	O
relevance	O
usually	O
depends	O
on	O
the	O
distance	O
between	O
states	O
:	O
the	O
closer	O
a	O
training	O
exam-	O
ple	O
’	O
s	O
state	B
is	O
to	O
the	O
query	O
state	B
,	O
the	O
more	O
relevant	O
it	O
is	O
considered	O
to	O
be	O
,	O
where	O
distance	O
can	O
be	O
deﬁned	O
in	O
many	O
diﬀerent	O
ways	O
.	O
after	O
the	O
query	O
state	B
is	O
given	O
a	O
value	B
,	O
the	O
local	O
approximation	O
is	O
discarded	O
.	O
the	O
simplest	O
example	O
of	O
the	O
memory-based	O
approach	O
is	O
the	O
nearest	O
neighbor	O
method	O
,	O
which	O
simply	O
ﬁnds	O
the	O
example	O
in	O
memory	O
whose	O
state	B
is	O
closest	O
to	O
the	O
query	O
state	B
and	O
returns	O
that	O
example	O
’	O
s	O
value	B
as	O
the	O
approximate	B
value	O
of	O
the	O
query	O
state	B
.	O
in	O
other	O
words	O
,	O
if	O
the	O
query	O
state	B
is	O
s	O
,	O
and	O
s	O
(	O
cid:48	O
)	O
(	O
cid:55	O
)	O
→	O
g	O
is	O
the	O
example	O
in	O
memory	O
in	O
which	O
s	O
(	O
cid:48	O
)	O
is	O
the	O
closest	O
state	B
to	O
s	O
,	O
then	O
g	O
is	O
returned	O
as	O
the	O
approximate	B
value	O
of	O
s.	O
slightly	O
more	O
complicated	O
are	O
weighted	O
average	O
methods	O
that	O
retrieve	O
a	O
set	O
of	O
nearest	O
neighbor	O
examples	O
and	O
re-	O
turn	O
a	O
weighted	O
average	O
of	O
their	O
target	B
values	O
,	O
where	O
the	O
weights	O
generally	O
decrease	O
with	O
increasing	O
distance	O
between	O
their	O
states	O
and	O
the	O
query	O
state	B
.	O
locally	O
weighted	O
regression	O
is	O
similar	O
,	O
but	O
it	O
ﬁts	O
a	O
surface	O
to	O
the	O
values	O
of	O
a	O
set	O
of	O
nearest	O
states	O
by	O
means	O
of	O
a	O
para-	O
metric	O
approximation	O
method	O
that	O
minimizes	O
a	O
weighted	O
error	O
measure	O
like	O
(	O
9.1	O
)	O
,	O
where	O
the	O
weights	O
depend	O
on	O
distances	O
from	O
the	O
query	O
state	B
.	O
the	O
value	B
returned	O
is	O
the	O
evalua-	O
tion	B
of	O
the	O
locally-ﬁtted	O
surface	O
at	O
the	O
query	O
state	B
,	O
after	O
which	O
the	O
local	O
approximation	O
surface	O
is	O
discarded	O
.	O
being	O
nonparametric	O
,	O
memory-based	O
methods	O
have	O
the	O
advantage	O
over	O
parametric	O
methods	O
of	O
not	O
limiting	O
approximations	O
to	O
pre-speciﬁed	O
functional	O
forms	O
.	O
this	O
allows	O
accuracy	O
to	O
improve	O
as	O
more	O
data	O
accumulates	O
.	O
memory-based	O
local	O
approximation	O
methods	O
have	O
other	O
properties	O
that	O
make	O
them	O
well	O
suited	O
for	O
reinforcement	O
learning	O
.	O
because	O
trajectory	B
sampling	I
is	O
of	O
such	O
importance	O
in	O
reinforcement	B
learning	I
,	O
as	O
discussed	O
in	O
section	O
8.6	O
,	O
memory-based	O
local	O
methods	O
can	O
focus	O
function	B
approximation	I
on	O
local	O
neighborhoods	O
of	O
states	O
(	O
or	O
state–action	O
pairs	O
)	O
visited	O
in	O
real	O
or	O
simulated	O
trajectories	O
.	O
there	O
may	O
be	O
no	O
need	O
for	O
global	O
approximation	O
because	O
many	O
areas	O
of	O
the	O
state	B
space	O
will	O
never	O
(	O
or	O
almost	O
never	O
)	O
be	O
reached	O
.	O
in	O
addition	O
,	O
memory-based	O
methods	O
allow	O
an	O
agent	O
’	O
s	O
experience	O
to	O
have	O
a	O
relatively	O
immediate	O
aﬀect	O
on	O
value	B
estimates	O
in	O
the	O
neighborhood	O
of	O
the	O
current	O
state	B
,	O
in	O
contrast	O
with	O
a	O
parametric	O
method	O
’	O
s	O
need	O
to	O
incrementally	O
adjust	O
parameters	O
of	O
a	O
global	O
approximation	O
.	O
avoiding	O
global	O
approximation	O
is	O
also	O
a	O
way	O
to	O
address	O
the	O
curse	B
of	I
dimensionality	I
.	O
for	O
example	O
,	O
for	O
a	O
state	O
space	O
with	O
k	O
dimensions	O
,	O
a	O
tabular	O
method	O
storing	O
a	O
global	O
approximation	O
requires	O
memory	O
exponential	O
in	O
k.	O
on	O
the	O
other	O
hand	O
,	O
in	O
storing	O
examples	O
for	O
a	O
memory-based	O
method	O
,	O
each	O
example	O
requires	O
memory	O
proportional	O
to	O
k	O
,	O
and	O
the	O
232	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
memory	O
required	O
to	O
store	O
,	O
say	O
,	O
n	O
examples	O
is	O
linear	O
in	O
n.	O
nothing	O
is	O
exponential	O
in	O
k	O
or	O
n.	O
of	O
course	O
,	O
the	O
critical	O
remaining	O
issue	O
is	O
whether	O
a	O
memory-based	O
method	O
can	O
answer	O
queries	O
quickly	O
enough	O
to	O
be	O
useful	O
to	O
an	O
agent	O
.	O
a	O
related	O
concern	O
is	O
how	O
speed	O
degrades	O
as	O
the	O
size	O
of	O
the	O
memory	O
grows	O
.	O
finding	O
nearest	O
neighbors	O
in	O
a	O
large	O
database	O
can	O
take	O
too	O
long	O
to	O
be	O
practical	O
in	O
many	O
applications	O
.	O
proponents	O
of	O
memory-based	O
methods	O
have	O
developed	O
ways	O
to	O
accelerate	O
the	O
nearest	O
neighbor	O
search	O
.	O
using	O
parallel	O
computers	O
or	O
special	O
purpose	O
hardware	O
is	O
one	O
approach	O
;	O
another	O
is	O
the	O
use	O
of	O
special	O
multi-dimensional	O
data	O
structures	O
to	O
store	O
the	O
training	O
data	O
.	O
one	O
data	O
structure	O
studied	O
for	O
this	O
application	O
is	O
the	O
k-d	O
tree	O
(	O
short	O
for	O
k-dimensional	O
tree	O
)	O
,	O
which	O
recursively	O
splits	O
a	O
k-dimensional	O
space	O
into	O
regions	O
arranged	O
as	O
nodes	O
of	O
a	O
binary	O
tree	O
.	O
depending	O
on	O
the	O
amount	O
of	O
data	O
and	O
how	O
it	O
is	O
distributed	O
over	O
the	O
state	B
space	O
,	O
nearest-neighbor	O
search	O
using	O
k-d	O
trees	O
can	O
quickly	O
eliminate	O
large	O
regions	O
of	O
the	O
space	O
in	O
the	O
search	O
for	O
neighbors	O
,	O
making	O
the	O
searches	O
feasible	O
in	O
some	O
problems	O
where	O
naive	B
searches	O
would	O
take	O
too	O
long	O
.	O
locally	O
weighted	O
regression	O
additionally	O
requires	O
fast	O
ways	O
to	O
do	O
the	O
local	O
regression	O
computations	O
which	O
have	O
to	O
be	O
repeated	O
to	O
answer	O
each	O
query	O
.	O
researchers	O
have	O
devel-	O
oped	O
many	O
ways	O
to	O
address	O
these	O
problems	O
,	O
including	O
methods	O
for	O
forgetting	O
entries	O
in	O
order	O
to	O
keep	O
the	O
size	O
of	O
the	O
database	O
within	O
bounds	O
.	O
the	O
bibliographic	O
and	O
historical	O
comments	O
section	O
at	O
the	O
end	O
of	O
this	O
chapter	O
points	O
to	O
some	O
of	O
the	O
relevant	O
literature	O
,	O
including	O
a	O
selection	O
of	O
papers	O
describing	O
applications	O
of	O
memory-based	O
learning	O
to	O
re-	O
inforcement	O
learning	O
.	O
9.10	O
kernel-based	B
function	I
approximation	I
memory-based	O
methods	O
such	O
as	O
the	O
weighted	O
average	O
and	O
locally	O
weighted	O
regression	O
methods	O
described	O
above	O
depend	O
on	O
assigning	O
weights	O
to	O
examples	O
s	O
(	O
cid:48	O
)	O
(	O
cid:55	O
)	O
→	O
g	O
in	O
the	O
database	O
depending	O
on	O
the	O
distance	O
between	O
s	O
(	O
cid:48	O
)	O
and	O
a	O
query	O
states	O
s.	O
the	O
function	O
that	O
assigns	O
these	O
weights	O
is	O
called	O
a	O
kernel	O
function	O
,	O
or	O
simply	O
a	O
kernel	O
.	O
in	O
the	O
weighted	O
average	O
and	O
locally	O
weighted	O
regressions	O
methods	O
,	O
for	O
example	O
,	O
a	O
kernel	O
function	O
k	O
:	O
r	O
→	O
r	O
assigns	O
weights	O
to	O
distances	O
between	O
states	O
.	O
more	O
generally	O
,	O
weights	O
do	O
not	O
have	O
to	O
depend	O
on	O
distances	O
;	O
they	O
can	O
depend	O
on	O
some	O
other	O
measure	O
of	O
similarity	O
between	O
states	O
.	O
in	O
this	O
case	O
,	O
k	O
:	O
s	O
×	O
s	O
→	O
r	O
,	O
so	O
that	O
k	O
(	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
)	O
is	O
the	O
weight	O
given	O
to	O
data	O
about	O
s	O
(	O
cid:48	O
)	O
in	O
its	O
inﬂuence	O
on	O
answering	O
queries	O
about	O
s.	O
viewed	O
slightly	O
diﬀerently	O
,	O
k	O
(	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
)	O
is	O
a	O
measure	O
of	O
the	O
strength	O
of	O
generalization	O
from	O
s	O
(	O
cid:48	O
)	O
to	O
s.	O
kernel	O
functions	O
numerically	O
express	O
how	O
relevant	O
knowledge	O
about	O
any	O
state	B
is	O
to	O
any	O
other	O
state	B
.	O
as	O
an	O
example	O
,	O
the	O
strengths	O
of	O
generalization	O
for	O
tile	O
coding	O
shown	O
in	O
figure	O
9.11	O
correspond	O
to	O
diﬀerent	O
kernel	O
functions	O
resulting	O
from	O
uniform	O
and	O
asymmetrical	O
tile	O
oﬀsets	O
.	O
although	O
tile	B
coding	I
does	O
not	O
explicitly	O
use	O
a	O
kernel	O
function	O
in	O
its	O
operation	O
,	O
it	O
generalizes	O
according	O
to	O
one	O
.	O
in	O
fact	O
,	O
as	O
we	O
discuss	O
more	O
below	O
,	O
the	O
strength	O
of	O
generalization	O
resulting	O
from	O
linear	O
parametric	O
function	B
approximation	I
can	O
always	O
be	O
described	O
by	O
a	O
kernel	O
function	O
.	O
kernel	O
regression	O
is	O
the	O
memory-based	O
method	O
that	O
computes	O
a	O
kernel	O
weighted	O
av-	O
erage	O
of	O
the	O
targets	O
of	O
all	O
examples	O
stored	O
in	O
memory	O
,	O
assigning	O
the	O
result	O
to	O
the	O
query	O
state	B
.	O
if	O
d	O
is	O
the	O
set	O
of	O
stored	O
examples	O
,	O
and	O
g	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
denotes	O
the	O
target	B
for	O
state	B
s	O
(	O
cid:48	O
)	O
in	O
a	O
9.11.	O
looking	O
deeper	O
at	O
on-policy	O
learning	O
:	O
interest	B
and	I
emphasis	I
233	O
stored	O
example	O
,	O
then	O
kernel	O
regression	O
approximates	O
the	O
target	B
function	O
,	O
in	O
this	O
case	O
a	O
value	B
function	I
depending	O
on	O
d	O
,	O
as	O
k	O
(	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
)	O
g	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
.	O
(	O
9.23	O
)	O
ˆv	O
(	O
s	O
,	O
d	O
)	O
=	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
∈d	O
the	O
weighted	O
average	O
method	O
described	O
above	O
is	O
a	O
special	O
case	O
in	O
which	O
k	O
(	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
)	O
is	O
non-	O
zero	O
only	O
when	O
s	O
and	O
s	O
(	O
cid:48	O
)	O
are	O
close	O
to	O
one	O
another	O
so	O
that	O
the	O
sum	O
need	O
not	O
be	O
computed	O
over	O
all	O
of	O
d.	O
a	O
common	O
kernel	O
is	O
the	O
gaussian	O
radial	O
basis	O
function	O
(	O
rbf	O
)	O
used	O
in	O
rbf	O
function	B
approximation	I
as	O
described	O
in	O
section	O
9.5.5.	O
in	O
the	O
method	O
described	O
there	O
,	O
rbfs	O
are	O
features	O
whose	O
centers	O
and	O
widths	O
are	O
either	O
ﬁxed	O
from	O
the	O
start	O
,	O
with	O
centers	O
presumably	O
concentrated	O
in	O
areas	O
where	O
many	O
examples	O
are	O
expected	O
to	O
fall	O
,	O
or	O
are	O
adjusted	O
in	O
some	O
way	O
during	O
learning	O
.	O
barring	O
methods	O
that	O
adjust	O
centers	O
and	O
widths	O
,	O
this	O
is	O
a	O
linear	O
parametric	O
method	O
whose	O
parameters	O
are	O
the	O
weights	O
of	O
each	O
rbf	O
,	O
which	O
are	O
typically	O
learned	O
by	O
stochastic	O
gradient	O
,	O
or	O
semi-gradient	O
,	O
descent	O
.	O
the	O
form	O
of	O
the	O
approximation	O
is	O
a	O
linear	O
combination	O
of	O
the	O
pre-determined	O
rbfs	O
.	O
kernel	O
regression	O
with	O
an	O
rbf	O
kernel	O
diﬀers	O
from	O
this	O
in	O
two	O
ways	O
.	O
first	O
,	O
it	O
is	O
memory-based	O
:	O
the	O
rbfs	O
are	O
centered	O
on	O
the	O
states	O
of	O
the	O
stored	O
examples	O
.	O
second	O
,	O
it	O
is	O
nonparametric	O
:	O
there	O
are	O
no	O
parameters	O
to	O
learn	O
;	O
the	O
response	O
to	O
a	O
query	O
is	O
given	O
by	O
(	O
9.23	O
)	O
.	O
of	O
course	O
,	O
many	O
issues	O
have	O
to	O
be	O
addressed	O
for	O
practical	O
implementation	O
of	O
kernel	O
regression	O
,	O
issues	O
that	O
are	O
beyond	O
the	O
scope	O
or	O
our	O
brief	O
discussion	O
.	O
however	O
,	O
it	O
turns	O
out	O
that	O
any	O
linear	O
parametric	O
regression	O
method	O
like	O
those	O
we	O
described	O
in	O
section	O
9.4	O
,	O
with	O
states	O
represented	O
by	O
feature	O
vectors	O
x	O
(	O
s	O
)	O
=	O
(	O
x1	O
(	O
s	O
)	O
,	O
x2	O
(	O
s	O
)	O
,	O
.	O
.	O
.	O
,	O
xd	O
(	O
s	O
)	O
)	O
(	O
cid:62	O
)	O
,	O
can	O
be	O
recast	O
as	O
kernel	O
regression	O
where	O
k	O
(	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
)	O
is	O
the	O
inner	O
product	O
of	O
the	O
feature	O
vector	O
representations	O
of	O
s	O
and	O
s	O
(	O
cid:48	O
)	O
;	O
that	O
is	O
k	O
(	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
)	O
=	O
x	O
(	O
s	O
)	O
(	O
cid:62	O
)	O
x	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
.	O
(	O
9.24	O
)	O
kernel	O
regression	O
with	O
this	O
kernel	O
function	O
produces	O
the	O
same	O
approximation	O
that	O
a	O
linear	O
parametric	O
method	O
would	O
if	O
it	O
used	O
these	O
feature	O
vectors	O
and	O
learned	O
with	O
the	O
same	O
training	O
data	O
.	O
we	O
skip	O
the	O
mathematical	O
justiﬁcation	O
for	O
this	O
,	O
which	O
can	O
be	O
found	O
in	O
any	O
modern	O
machine	O
learning	O
text	O
,	O
such	O
as	O
bishop	O
(	O
2006	O
)	O
,	O
and	O
simply	O
point	O
out	O
an	O
important	O
im-	O
plication	O
.	O
instead	O
of	O
constructing	O
features	O
for	O
linear	O
parametric	O
function	O
approximators	O
,	O
one	O
can	O
instead	O
construct	O
kernel	O
functions	O
directly	O
without	O
referring	O
at	O
all	O
to	O
feature	O
vectors	O
.	O
not	O
all	O
kernel	O
functions	O
can	O
be	O
expressed	O
as	O
inner	O
products	O
of	O
feature	O
vectors	O
as	O
in	O
(	O
9.24	O
)	O
,	O
but	O
a	O
kernel	O
function	O
that	O
can	O
be	O
expressed	O
like	O
this	O
can	O
oﬀer	O
signiﬁcant	O
advantages	O
over	O
the	O
equivalent	O
parametric	O
method	O
.	O
for	O
many	O
sets	O
of	O
feature	O
vectors	O
,	O
(	O
9.24	O
)	O
has	O
a	O
compact	O
functional	O
form	O
that	O
can	O
be	O
evaluated	O
without	O
any	O
computation	O
taking	O
place	O
in	O
the	O
d-dimensional	O
feature	O
space	O
.	O
in	O
these	O
cases	O
,	O
kernel	O
regression	O
is	O
much	O
less	O
complex	O
than	O
directly	O
using	O
a	O
linear	O
parametric	O
method	O
with	O
states	O
represented	O
by	O
these	O
feature	O
vectors	O
.	O
this	O
is	O
the	O
so-called	O
“	O
kernel	O
trick	O
”	O
that	O
allows	O
eﬀectively	O
working	O
in	O
the	O
high-dimension	O
of	O
an	O
expansive	O
feature	O
space	O
while	O
actually	O
working	O
only	O
with	O
the	O
set	O
of	O
stored	O
training	O
examples	O
.	O
the	O
kernel	O
trick	O
is	O
the	O
basis	O
of	O
many	O
machine	O
learn-	O
ing	B
methods	O
,	O
and	O
researchers	O
have	O
shown	O
how	O
it	O
can	O
sometimes	O
beneﬁt	O
reinforcement	B
learning	I
.	O
234	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
9.11	O
looking	O
deeper	O
at	O
on-policy	O
learning	O
:	O
interest	B
and	I
emphasis	I
the	O
algorithms	O
we	O
have	O
considered	O
so	O
far	O
in	O
this	O
chapter	O
have	O
treated	O
all	O
the	O
states	O
encountered	O
equally	O
,	O
as	O
if	O
they	O
were	O
all	O
equally	O
important	O
.	O
in	O
some	O
cases	O
,	O
however	O
,	O
we	O
are	O
more	O
interested	O
in	O
some	O
states	O
than	O
others	O
.	O
in	O
discounted	O
episodic	O
problems	O
,	O
for	O
example	O
,	O
we	O
may	O
be	O
more	O
interested	O
in	O
accurately	O
valuing	O
early	O
states	O
in	O
the	O
episode	O
than	O
in	O
later	O
states	O
where	O
discounting	B
may	O
have	O
made	O
the	O
rewards	O
much	O
less	O
important	O
to	O
the	O
value	B
of	O
the	O
start	O
state	B
.	O
or	O
,	O
if	O
an	O
action-value	B
function	I
is	O
being	O
learned	O
,	O
it	O
may	O
be	O
less	O
important	O
to	O
accurately	O
value	B
poor	O
actions	O
whose	O
value	B
is	O
much	O
less	O
than	O
the	O
greedy	O
action	O
.	O
function	B
approximation	I
resources	O
are	O
always	O
limited	O
,	O
and	O
if	O
they	O
were	O
used	O
in	O
a	O
more	O
targeted	O
way	O
,	O
then	O
performance	O
could	O
be	O
improved	O
.	O
one	O
reason	O
we	O
have	O
treated	O
all	O
states	O
encountered	O
equally	O
is	O
that	O
then	O
we	O
are	O
updating	O
according	O
to	O
the	O
on-policy	B
distribution	I
,	O
for	O
which	O
stronger	O
theoretical	O
results	O
are	O
available	O
for	O
semi-gradient	O
methods	O
.	O
recall	O
that	O
the	O
on-policy	B
distribution	I
was	O
deﬁned	O
as	O
the	O
distribution	O
of	O
states	O
encountered	O
in	O
an	O
mdp	O
while	O
following	O
the	O
target	B
policy	O
.	O
now	O
we	O
will	O
generalize	O
this	O
concept	O
signiﬁcantly	O
.	O
rather	O
than	O
having	O
one	O
on-policy	B
distribution	I
for	O
the	O
mdp	O
,	O
we	O
will	O
have	O
many	O
.	O
all	O
of	O
them	O
will	O
have	O
in	O
common	O
that	O
they	O
are	O
a	O
distribution	O
of	O
states	O
encountered	O
in	O
trajectories	O
while	O
following	O
the	O
target	B
policy	O
,	O
but	O
they	O
will	O
vary	O
in	O
how	O
the	O
trajectories	O
are	O
,	O
in	O
a	O
sense	O
,	O
initiated	O
.	O
we	O
now	O
introduce	O
some	O
new	O
concepts	O
.	O
first	O
we	O
introduce	O
a	O
non-negative	O
scalar	O
mea-	O
sure	O
,	O
a	O
random	O
variable	O
it	O
called	O
interest	O
,	O
indicating	O
the	O
degree	O
to	O
which	O
we	O
are	O
interested	O
in	O
accurately	O
valuing	O
the	O
state	B
(	O
or	O
state–action	O
pair	O
)	O
at	O
time	O
t.	O
if	O
we	O
don	O
’	O
t	O
care	O
at	O
all	O
about	O
the	O
state	B
,	O
then	O
the	O
interest	O
should	O
be	O
zero	O
;	O
if	O
we	O
fully	O
care	O
,	O
it	O
might	O
be	O
one	O
,	O
though	O
it	O
is	O
formally	O
allowed	O
take	O
any	O
non-negative	O
value	B
.	O
the	O
interest	O
can	O
be	O
set	O
in	O
any	O
causal	O
way	O
;	O
for	O
example	O
,	O
it	O
may	O
depend	O
on	O
the	O
trajectory	O
up	O
to	O
time	O
t	O
or	O
the	O
learned	O
parame-	O
ters	O
at	O
time	O
t.	O
the	O
distribution	O
µ	O
in	O
the	O
ve	O
(	O
9.1	O
)	O
is	O
then	O
deﬁned	O
as	O
the	O
distribution	O
of	O
states	O
encountered	O
while	O
following	O
the	O
target	B
policy	O
,	O
weighted	O
by	O
the	O
interest	O
.	O
second	O
,	O
we	O
introduce	O
another	O
non-negative	O
scalar	O
random	O
variable	O
,	O
the	O
emphasis	O
mt	O
.	O
this	O
scalar	O
multiplies	O
the	O
learning	O
update	O
and	O
thus	O
emphasizes	O
or	O
de-emphasizes	O
the	O
learning	O
done	O
at	O
time	O
t.	O
the	O
general	O
n-step	O
learning	O
rule	O
,	O
replacing	B
(	O
9.15	O
)	O
,	O
is	O
wt+n	O
.	O
=	O
wt+n−1	O
+	O
αmt	O
[	O
gt	O
:	O
t+n	O
−	O
ˆv	O
(	O
st	O
,	O
wt+n−1	O
)	O
]	O
∇ˆv	O
(	O
st	O
,	O
wt+n−1	O
)	O
,	O
0	O
≤	O
t	O
<	O
t	O
,	O
(	O
9.25	O
)	O
with	O
the	O
n-step	B
return	O
given	O
by	O
(	O
9.16	O
)	O
and	O
the	O
emphasis	O
determined	O
recursively	O
from	O
the	O
interest	O
by	O
:	O
mt	O
=	O
it	O
+	O
γnmt−n	O
,	O
0	O
≤	O
t	O
<	O
t	O
,	O
.	O
with	O
mt	O
=	O
0	O
,	O
for	O
all	O
t	O
<	O
0.	O
these	O
equations	O
are	O
taken	O
to	O
include	O
the	O
monte	O
carlo	O
case	O
,	O
for	O
which	O
gt	O
:	O
t+n	O
=	O
gt	O
,	O
all	O
the	O
updates	O
are	O
made	O
at	O
end	O
of	O
the	O
episode	O
,	O
n	O
=	O
t	O
−	O
t	O
,	O
and	O
mt	O
=	O
it	O
.	O
(	O
9.26	O
)	O
9.12.	O
summary	O
235	O
example	O
9.3	O
illustrates	O
how	O
interest	B
and	I
emphasis	I
can	O
result	O
in	O
more	O
accurate	O
value	B
estimates	O
.	O
example	O
9.3	O
:	O
interest	B
and	I
emphasis	I
to	O
see	O
the	O
potential	O
beneﬁts	O
of	O
using	O
interest	B
and	I
emphasis	I
,	O
consider	O
the	O
four-state	O
markov	O
reward	O
process	O
shown	O
below	O
:	O
episodes	B
start	O
in	O
the	O
leftmost	O
state	B
,	O
then	O
transition	O
one	O
state	B
to	O
the	O
right	O
,	O
with	O
a	O
reward	O
of	O
+1	O
,	O
on	O
each	O
step	O
until	O
the	O
terminal	O
state	B
is	O
reached	O
.	O
the	O
true	O
value	O
of	O
the	O
ﬁrst	O
state	B
is	O
thus	O
4	O
,	O
of	O
the	O
second	O
state	B
3	O
,	O
and	O
so	O
on	O
as	O
shown	O
below	O
each	O
state	B
.	O
these	O
are	O
the	O
true	O
values	O
;	O
the	O
estimated	O
values	O
can	O
only	O
approximate	B
these	O
because	O
they	O
are	O
constrained	O
by	O
the	O
parameterization	O
.	O
there	O
are	O
two	O
components	O
to	O
the	O
parameter	O
vector	O
w	O
=	O
(	O
w1	O
,	O
w2	O
)	O
(	O
cid:62	O
)	O
,	O
and	O
the	O
parameterization	O
is	O
as	O
written	O
inside	O
each	O
state	B
.	O
the	O
estimated	O
values	O
of	O
the	O
ﬁrst	O
two	O
states	O
are	O
given	O
by	O
w1	O
alone	O
and	O
thus	O
must	O
be	O
the	O
same	O
even	O
though	O
their	O
true	O
values	O
are	O
diﬀerent	O
.	O
similarly	O
,	O
the	O
estimated	O
values	O
of	O
the	O
third	O
and	O
fourth	O
states	O
are	O
given	O
by	O
w2	O
alone	O
and	O
must	O
be	O
the	O
same	O
even	O
though	O
their	O
true	O
values	O
are	O
diﬀerent	O
.	O
suppose	O
that	O
we	O
are	O
interested	O
in	O
accurately	O
valuing	O
only	O
the	O
leftmost	O
state	B
;	O
we	O
assign	O
it	O
an	O
interest	O
of	O
1	O
while	O
all	O
the	O
other	O
states	O
are	O
assigned	O
an	O
interest	O
of	O
0	O
,	O
as	O
indicated	O
above	O
the	O
states	O
.	O
first	O
consider	O
applying	O
gradient	B
monte	O
carlo	O
algorithms	O
to	O
this	O
problem	O
.	O
the	O
algorithms	O
presented	O
earlier	O
in	O
this	O
chapter	O
that	O
do	O
not	O
take	O
into	O
account	O
interest	B
and	I
emphasis	I
(	O
in	O
(	O
9.7	O
)	O
and	O
the	O
box	O
on	O
page	O
202	O
)	O
will	O
converge	O
(	O
for	O
decreasing	O
step	O
sizes	O
)	O
to	O
the	O
parameter	O
vector	O
w∞	O
=	O
(	O
3.5	O
,	O
1.5	O
)	O
,	O
which	O
gives	O
the	O
ﬁrst	O
state—the	O
only	O
one	O
we	O
are	O
interested	O
in—a	O
value	B
of	O
3.5	O
(	O
i.e.	O
,	O
intermediate	O
between	O
the	O
true	O
values	O
of	O
the	O
ﬁrst	O
and	O
second	O
states	O
)	O
.	O
the	O
methods	O
presented	O
in	O
this	O
section	O
that	O
do	O
use	O
interest	B
and	I
emphasis	I
,	O
on	O
the	O
other	O
hand	O
,	O
will	O
learn	O
the	O
value	B
of	O
the	O
ﬁrst	O
state	B
exactly	O
correctly	O
;	O
w1	O
will	O
converge	O
to	O
4	O
while	O
w2	O
will	O
never	O
be	O
updated	O
because	O
the	O
emphasis	O
is	O
zero	O
in	O
all	O
states	O
save	O
the	O
leftmost	O
.	O
now	O
consider	O
applying	O
two-step	O
semi-gradient	O
td	O
methods	O
.	O
the	O
methods	O
from	O
earlier	O
in	O
this	O
chapter	O
without	O
interest	B
and	I
emphasis	I
(	O
in	O
(	O
9.15	O
)	O
and	O
(	O
9.16	O
)	O
and	O
the	O
box	O
on	O
page	O
209	O
)	O
will	O
again	O
converge	O
to	O
w∞	O
=	O
(	O
3.5	O
,	O
1.5	O
)	O
,	O
while	O
the	O
methods	O
with	O
interest	O
and	O
emphasis	O
converge	O
to	O
w∞	O
=	O
(	O
4	O
,	O
2	O
)	O
.	O
the	O
latter	O
produces	O
the	O
exactly	O
correct	O
values	O
for	O
the	O
ﬁrst	O
state	B
and	O
for	O
the	O
third	O
state	B
(	O
which	O
the	O
ﬁrst	O
state	B
bootstraps	O
from	O
)	O
while	O
never	O
making	O
any	O
updates	O
corresponding	O
to	O
the	O
second	O
or	O
fourth	O
states	O
.	O
+1+1+1+1v⇡=4v⇡=3v⇡=2v⇡=1i=1i=0i=0i=0w1w1w2w2	O
236	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
9.12	O
summary	O
reinforcement	B
learning	I
systems	O
must	O
be	O
capable	O
of	O
generalization	O
if	O
they	O
are	O
to	O
be	O
ap-	O
plicable	O
to	O
artiﬁcial	B
intelligence	I
or	O
to	O
large	O
engineering	O
applications	O
.	O
to	O
achieve	O
this	O
,	O
any	O
of	O
a	O
broad	O
range	O
of	O
existing	O
methods	O
for	O
supervised-learning	O
function	B
approximation	I
can	O
be	O
used	O
simply	O
by	O
treating	O
each	O
update	O
as	O
a	O
training	O
example	O
.	O
perhaps	O
the	O
most	O
suitable	O
supervised	B
learning	I
methods	O
are	O
those	O
using	O
parameterized	O
function	B
approximation	I
,	O
in	O
which	O
the	O
policy	B
is	O
parameterized	O
by	O
a	O
weight	O
vector	B
w.	O
although	O
the	O
weight	O
vector	B
has	O
many	O
components	O
,	O
the	O
state	B
space	O
is	O
much	O
larger	O
still	O
,	O
and	O
we	O
must	O
settle	O
for	O
an	O
approximate	O
solution	O
.	O
we	O
deﬁned	O
the	O
mean	O
squared	O
value	B
error	O
,	O
ve	O
(	O
w	O
)	O
,	O
as	O
a	O
measure	O
of	O
the	O
error	O
in	O
the	O
values	O
vπw	O
(	O
s	O
)	O
for	O
a	O
weight	O
vector	B
w	O
under	O
the	O
on-policy	B
distribution	I
,	O
µ.	O
the	O
ve	O
gives	O
us	O
a	O
clear	O
way	O
to	O
rank	O
diﬀerent	O
value-function	O
approximations	O
in	O
the	O
on-policy	O
case	O
.	O
to	O
ﬁnd	O
a	O
good	O
weight	O
vector	B
,	O
the	O
most	O
popular	O
methods	O
are	O
variations	O
of	O
stochastic	O
gradient	B
descent	I
(	O
sgd	O
)	O
.	O
in	O
this	O
chapter	O
we	O
have	O
focused	O
on	O
the	O
on-policy	O
case	O
with	O
a	O
ﬁxed	O
policy	B
,	O
also	O
known	O
as	O
policy	O
evaluation	O
or	O
prediction	B
;	O
a	O
natural	O
learning	O
algorithm	O
for	O
this	O
case	O
is	O
n-step	B
semi-gradient	O
td	O
,	O
which	O
includes	O
gradient	B
monte	O
carlo	O
and	O
semi-gradient	O
td	O
(	O
0	O
)	O
algorithms	O
as	O
the	O
special	O
cases	O
when	O
n	O
=∞	O
and	O
n	O
=	O
1	O
respectively	O
.	O
semi-gradient	O
td	O
methods	O
are	O
not	O
true	O
gradient	O
methods	O
.	O
in	O
such	O
bootstrapping	B
methods	O
(	O
including	O
dp	O
)	O
,	O
the	O
weight	O
vector	B
appears	O
in	O
the	O
update	O
target	B
,	O
yet	O
this	O
is	O
not	O
taken	O
into	O
account	O
in	O
computing	O
the	O
gradient—thus	O
they	O
are	O
semi	O
-gradient	O
methods	O
.	O
as	O
such	O
,	O
they	O
can	O
not	O
rely	O
on	O
classical	O
sgd	O
results	O
.	O
nevertheless	O
,	O
good	O
results	O
can	O
be	O
obtained	O
for	O
semi-gradient	O
methods	O
in	O
the	O
special	O
case	O
of	O
linear	O
function	B
approximation	I
,	O
in	O
which	O
the	O
value	B
estimates	O
are	O
sums	O
of	O
features	O
times	O
corresponding	O
weights	O
.	O
the	O
linear	O
case	O
is	O
the	O
most	O
well	O
understood	O
theoretically	O
and	O
works	O
well	O
in	O
practice	O
when	O
provided	O
with	O
appropriate	O
features	O
.	O
choosing	O
the	O
fea-	O
tures	O
is	O
one	O
of	O
the	O
most	O
important	O
ways	O
of	O
adding	O
prior	O
domain	O
knowledge	O
to	O
reinforce-	O
ment	O
learning	O
systems	O
.	O
they	O
can	O
be	O
chosen	O
as	O
polynomials	O
,	O
but	O
this	O
case	O
generalizes	O
poorly	O
in	O
the	O
online	O
learning	O
setting	O
typically	O
considered	O
in	O
reinforcement	O
learning	O
.	O
bet-	O
ter	O
is	O
to	O
choose	O
features	O
according	O
the	O
fourier	O
basis	O
,	O
or	O
according	O
to	O
some	O
form	O
of	O
coarse	O
coding	O
with	O
sparse	O
overlapping	O
receptive	O
ﬁelds	O
.	O
tile	B
coding	I
is	O
a	O
form	O
of	O
coarse	O
coding	O
that	O
is	O
particularly	O
computationally	O
eﬃcient	O
and	O
ﬂexible	O
.	O
radial	O
basis	O
functions	O
are	O
use-	O
ful	O
for	O
one-	O
or	O
two-dimensional	O
tasks	O
in	O
which	O
a	O
smoothly	O
varying	O
response	O
is	O
important	O
.	O
lstd	O
is	O
the	O
most	O
data-eﬃcient	O
linear	O
td	O
prediction	B
method	O
,	O
but	O
requires	O
computation	O
proportional	O
to	O
the	O
square	O
of	O
the	O
number	O
of	O
weights	O
,	O
whereas	O
all	O
the	O
other	O
methods	O
are	O
of	O
complexity	O
linear	O
in	O
the	O
number	O
of	O
weights	O
.	O
nonlinear	O
methods	O
include	O
artiﬁcial	B
neural	I
networks	I
trained	O
by	O
backpropagation	B
and	O
variations	O
of	O
sgd	O
;	O
these	O
methods	O
have	O
become	O
very	O
popular	O
in	O
recent	O
years	O
under	O
the	O
name	O
deep	B
reinforcement	I
learning	I
.	O
linear	O
semi-gradient	O
n-step	B
td	O
is	O
guaranteed	O
to	O
converge	O
under	O
standard	O
conditions	O
,	O
for	O
all	O
n	O
,	O
to	O
a	O
ve	O
that	O
is	O
within	O
a	O
bound	O
of	O
the	O
optimal	O
error	O
(	O
achieved	O
asymptotically	O
by	O
monte	O
carlo	O
methods	O
)	O
.	O
this	O
bound	O
is	O
always	O
tighter	O
for	O
higher	O
n	O
and	O
approaches	O
zero	O
as	O
n	O
→	O
∞	O
.	O
however	O
,	O
in	O
practice	O
very	O
high	O
n	O
results	O
in	O
very	O
slow	O
learning	O
,	O
and	O
some	O
degree	O
of	O
bootstrapping	O
(	O
1	O
≥	O
n	O
<	O
∞	O
)	O
is	O
usually	O
preferrable	O
,	O
just	O
as	O
we	O
saw	O
in	O
comparisons	O
of	O
tabular	O
n-step	B
methods	I
in	O
chapter	O
7	O
and	O
in	O
comparisons	O
of	O
tabular	O
td	O
and	O
monte	O
carlo	O
methods	O
in	O
chapter	O
6	O
.	O
9.12.	O
summary	O
237	O
bibliographical	O
and	O
historical	O
remarks	O
generalization	O
and	B
function	I
approximation	I
have	O
always	O
been	O
an	O
integral	O
part	O
of	O
rein-	O
forcement	O
learning	O
.	O
bertsekas	O
and	O
tsitsiklis	O
(	O
1996	O
)	O
,	O
bertsekas	O
(	O
2012	O
)	O
,	O
and	O
sugiyama	O
et	O
al	O
.	O
(	O
2013	O
)	O
present	O
the	O
state	B
of	O
the	O
art	O
in	O
function	O
approximation	O
in	O
reinforcement	O
learn-	O
ing	B
.	O
some	O
of	O
the	O
early	O
work	O
with	B
function	I
approximation	I
in	O
reinforcement	B
learning	I
is	O
discussed	O
at	O
the	O
end	O
of	O
this	O
section	O
.	O
9.3	O
gradient-descent	O
methods	O
for	O
minimizing	O
mean-squared	O
error	O
in	O
supervised	O
learn-	O
ing	B
are	O
well	O
known	O
.	O
widrow	O
and	O
hoﬀ	O
(	O
1960	O
)	O
introduced	O
the	O
least-mean-square	O
(	O
lms	O
)	O
algorithm	O
,	O
which	O
is	O
the	O
prototypical	O
incremental	O
gradient-descent	O
algo-	O
rithm	O
.	O
details	O
of	O
this	O
and	O
related	O
algorithms	O
are	O
provided	O
in	O
many	O
texts	O
(	O
e.g.	O
,	O
widrow	O
and	O
stearns	O
,	O
1985	O
;	O
bishop	O
,	O
1995	O
;	O
duda	O
and	O
hart	O
,	O
1973	O
)	O
.	O
9.4	O
semi-gradient	O
td	O
(	O
0	O
)	O
was	O
ﬁrst	O
explored	O
by	O
sutton	O
(	O
1984	O
,	O
1988	O
)	O
,	O
as	O
part	O
of	O
the	O
linear	O
td	O
(	O
λ	O
)	O
algorithm	O
that	O
we	O
will	O
treat	O
in	O
chapter	O
12.	O
the	O
term	O
“	O
semi-	O
gradient	B
”	O
to	O
describe	O
these	O
bootstrapping	B
methods	O
is	O
new	O
to	O
the	O
second	O
edition	O
of	O
this	O
book	O
.	O
the	O
earliest	O
use	O
of	O
state	O
aggregation	O
in	O
reinforcement	O
learning	O
may	O
have	O
been	O
michie	O
and	O
chambers	O
’	O
s	O
boxes	O
system	O
(	O
1968	O
)	O
.	O
the	O
theory	O
of	O
state	O
aggregation	O
in	O
reinforcement	O
learning	O
has	O
been	O
developed	O
by	O
singh	O
,	O
jaakkola	O
,	O
and	O
jordan	O
(	O
1995	O
)	O
and	O
tsitsiklis	O
and	O
van	O
roy	O
(	O
1996	O
)	O
.	O
state	B
aggregation	I
has	O
been	O
used	O
in	O
dynamic	O
programming	O
from	O
its	O
earliest	O
days	O
(	O
e.g.	O
,	O
bellman	O
,	O
1957a	O
)	O
.	O
sutton	O
(	O
1988	O
)	O
proved	O
convergence	O
of	O
linear	O
td	O
(	O
0	O
)	O
in	O
the	O
mean	O
to	O
the	O
minimal	O
ve	O
solution	O
for	O
the	O
case	O
in	O
which	O
the	O
feature	O
vectors	O
,	O
{	O
x	O
(	O
s	O
)	O
:	O
s	O
∈	O
s	O
}	O
,	O
are	O
linearly	O
independent	O
.	O
convergence	O
with	O
probability	O
1	O
was	O
proved	O
by	O
several	O
researchers	O
at	O
about	O
the	O
same	O
time	O
(	O
peng	O
,	O
1993	O
;	O
dayan	O
and	O
sejnowski	O
,	O
1994	O
;	O
tsitsiklis	O
,	O
1994	O
;	O
gurvits	O
,	O
lin	O
,	O
and	O
hanson	O
,	O
1994	O
)	O
.	O
in	O
addition	O
,	O
jaakkola	O
,	O
jordan	O
,	O
and	O
singh	O
(	O
1994	O
)	O
proved	O
convergence	O
under	O
online	B
updating	O
.	O
all	O
of	O
these	O
results	O
assumed	O
linearly	O
independent	O
feature	O
vectors	O
,	O
which	O
implies	O
at	O
least	O
as	O
many	O
components	O
to	O
wt	O
as	O
there	O
are	O
states	O
.	O
convergence	O
for	O
the	O
more	O
important	O
case	O
of	O
general	O
(	O
dependent	O
)	O
feature	O
vectors	O
was	O
ﬁrst	O
shown	O
by	O
dayan	O
(	O
1992	O
)	O
.	O
a	O
signiﬁcant	O
generalization	O
and	O
strengthening	O
of	O
dayan	O
’	O
s	O
result	O
was	O
proved	O
by	O
tsitsiklis	O
and	O
van	O
roy	O
(	O
1997	O
)	O
.	O
they	O
proved	O
the	O
main	O
result	O
presented	O
in	O
this	O
section	O
,	O
the	O
bound	O
on	O
the	O
asymptotic	O
error	O
of	O
linear	O
bootstrapping	O
methods	O
.	O
9.5	O
our	O
presentation	O
of	O
the	O
range	O
of	O
possibilities	O
for	O
linear	O
function	B
approximation	I
is	O
based	O
on	O
that	O
by	O
barto	O
(	O
1990	O
)	O
.	O
9.5.2	O
konidaris	O
,	O
osentoski	O
,	O
and	O
thomas	O
(	O
2011	O
)	O
introduced	O
the	O
fourier	O
basis	O
in	O
a	O
sim-	O
ple	O
form	O
suitable	O
for	O
reinforcement	O
learning	O
problems	O
with	O
multi-dimensional	O
continuous	B
state	I
spaces	O
and	O
functions	O
that	O
do	O
not	O
have	O
to	O
be	O
periodic	O
.	O
9.5.3	O
the	O
term	O
coarse	B
coding	I
is	O
due	O
to	O
hinton	O
(	O
1984	O
)	O
,	O
and	O
our	O
figure	O
9.6	O
is	O
based	O
on	O
one	O
of	O
his	O
ﬁgures	O
.	O
waltz	O
and	O
fu	O
(	O
1965	O
)	O
provide	O
an	O
early	O
example	O
of	O
this	O
type	O
of	O
function	O
approximation	O
in	O
a	O
reinforcement	B
learning	I
system	O
.	O
238	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
9.5.4	O
tile	B
coding	I
,	O
including	O
hashing	O
,	O
was	O
introduced	O
by	O
albus	O
(	O
1971	O
,	O
1981	O
)	O
.	O
he	O
de-	O
scribed	O
it	O
in	O
terms	O
of	O
his	O
“	O
cerebellar	O
model	O
articulator	O
controller	O
,	O
”	O
or	O
cmac	O
,	O
as	O
tile	O
coding	O
is	O
sometimes	O
known	O
in	O
the	O
literature	O
.	O
the	O
term	O
“	O
tile	B
coding	I
”	O
was	O
new	O
to	O
the	O
ﬁrst	O
edition	O
of	O
this	O
book	O
,	O
though	O
the	O
idea	O
of	O
describing	O
cmac	O
in	O
these	O
terms	O
is	O
taken	O
from	O
watkins	O
(	O
1989	O
)	O
.	O
tile	B
coding	I
has	O
been	O
used	O
in	O
many	O
reinforcement	B
learning	I
systems	O
(	O
e.g.	O
,	O
shewchuk	O
and	O
dean	O
,	O
1990	O
;	O
lin	O
and	O
kim	O
,	O
1991	O
;	O
miller	O
,	O
scalera	O
,	O
and	O
kim	O
,	O
1994	O
;	O
sofge	O
and	O
white	O
,	O
1992	O
;	O
tham	O
,	O
1994	O
;	O
sutton	O
,	O
1996	O
;	O
watkins	O
,	O
1989	O
)	O
as	O
well	O
as	O
in	O
other	O
types	O
of	O
learning	O
con-	O
trol	O
systems	O
(	O
e.g.	O
,	O
kraft	O
and	O
campagna	O
,	O
1990	O
;	O
kraft	O
,	O
miller	O
,	O
and	O
dietz	O
,	O
1992	O
)	O
.	O
this	O
section	O
draws	O
heavily	O
on	O
the	O
work	O
of	O
miller	O
and	O
glanz	O
(	O
1996	O
)	O
.	O
general	O
software	O
for	O
tile	O
coding	O
is	O
available	O
on	O
the	O
web	O
in	O
several	O
languages	O
(	O
e.g.	O
,	O
see	O
http	O
:	O
//incompleteideas.net/tiles/tiles3.html	O
.	O
9.5.5	O
function	B
approximation	I
using	O
radial	O
basis	O
functions	O
has	O
received	O
wide	O
attention	O
ever	O
since	O
being	O
related	O
to	O
neural	B
networks	I
by	O
broomhead	O
and	O
lowe	O
(	O
1988	O
)	O
.	O
powell	O
(	O
1987	O
)	O
reviewed	O
earlier	O
uses	O
of	O
rbfs	O
,	O
and	O
poggio	O
and	O
girosi	O
(	O
1989	O
,	O
1990	O
)	O
extensively	O
developed	O
and	O
applied	O
this	O
approach	O
.	O
9.6	O
9.7	O
automatic	O
methods	O
for	O
adapting	O
the	O
step-size	B
parameter	I
include	O
rmsprop	O
(	O
tiele-	O
man	O
and	O
hinton	O
,	O
2012	O
)	O
,	O
adam	O
(	O
kingma	O
and	O
ba	O
,	O
2015	O
)	O
,	O
stochastic	O
meta-descent	O
methods	O
such	O
as	O
delta-bar-delta	O
(	O
jacobs	O
,	O
1988	O
)	O
,	O
its	O
incremental	O
generalization	O
(	O
sutton	O
,	O
1992b	O
,	O
c	O
;	O
mahmood	O
et	O
al.	O
,	O
2012	O
)	O
,	O
and	O
nonlinear	O
generalizations	O
(	O
schrau-	O
dolph	O
,	O
1999	O
,	O
2002	O
)	O
.	O
methods	O
explicitly	O
designed	O
for	O
reinforcement	O
learning	O
in-	O
clude	O
alphabound	O
(	O
dabney	O
and	O
barto	O
,	O
2012	O
)	O
,	O
sid	O
and	O
nosid	O
(	O
dabney	O
,	O
2014	O
)	O
,	O
tidbd	O
(	O
kearney	O
et	O
al.	O
,	O
in	O
preparation	O
)	O
and	O
the	O
application	O
of	O
stochastic	O
meta-	O
descent	O
to	O
policy	O
gradient	O
learning	O
(	O
schraudolph	O
,	O
yu	O
,	O
and	O
aberdeen	O
,	O
2006	O
)	O
.	O
the	O
introduction	O
of	O
the	O
threshold	O
logic	O
unit	O
as	O
an	O
abstract	O
model	O
neuron	O
by	O
mcculloch	O
and	O
pitts	O
(	O
1943	O
)	O
was	O
the	O
beginning	O
of	O
artiﬁcial	O
neural	B
networks	I
.	O
the	O
history	B
of	I
anns	O
as	O
learning	O
methods	O
for	O
classiﬁcation	O
or	O
regression	O
has	O
passed	O
through	O
several	O
stages	O
:	O
roughly	O
,	O
the	O
perceptron	O
(	O
rosenblatt	O
,	O
1962	O
)	O
and	O
ada-	O
line	O
(	O
adaptive	O
linear	O
element	O
)	O
(	O
widrow	O
and	O
hoﬀ	O
,	O
1960	O
)	O
stage	O
of	O
learning	O
by	O
single-layer	O
anns	O
,	O
the	O
error-backpropagation	O
stage	O
(	O
lecun	O
,	O
1985	O
;	O
rumelhart	O
,	O
hinton	O
,	O
and	O
williams	O
,	O
1986	O
)	O
of	O
learning	O
by	O
multi-layer	O
anns	O
,	O
and	O
the	O
current	O
deep-learning	O
stage	O
with	O
its	O
emphasis	O
on	O
representation	B
learning	I
(	O
e.g.	O
,	O
bengio	O
,	O
courville	O
,	O
and	O
vincent	O
,	O
2012	O
;	O
goodfellow	O
,	O
bengio	O
,	O
and	O
courville	O
,	O
2016	O
)	O
.	O
exam-	O
ples	O
of	O
the	O
many	O
books	O
on	O
anns	O
are	O
haykin	O
(	O
1994	O
)	O
,	O
bishop	O
(	O
1995	O
)	O
,	O
and	O
ripley	O
(	O
2007	O
)	O
.	O
anns	O
as	O
function	O
approximation	O
for	O
reinforcement	O
learning	O
goes	O
back	O
to	O
the	O
early	O
neural	B
networks	I
of	O
farley	O
and	O
clark	O
(	O
1954	O
)	O
,	O
who	O
used	O
reinforcement-like	O
learning	O
to	O
modify	O
the	O
weights	O
of	O
linear	O
threshold	O
functions	O
representing	O
policies	O
.	O
widrow	O
,	O
gupta	O
,	O
and	O
maitra	O
(	O
1973	O
)	O
presented	O
a	O
neuron-like	O
linear	O
threshold	O
unit	O
implementing	O
a	O
learning	O
process	O
they	O
called	O
learning	O
with	O
a	O
critic	B
or	O
selective	B
bootstrap	I
adaptation	I
,	O
a	O
reinforcement-learning	O
variant	O
of	O
the	O
adaline	O
algo-	O
rithm	O
.	O
werbos	O
(	O
1987	O
,	O
1994	O
)	O
developed	O
an	O
approach	O
to	O
prediction	B
and	O
control	B
9.12.	O
summary	O
239	O
that	O
uses	O
anns	O
trained	O
by	O
error	O
backpropation	O
to	O
learn	O
policies	O
and	O
value	O
func-	O
tions	O
using	O
td-like	O
algorithms	O
.	O
barto	O
,	O
sutton	O
,	O
and	O
brouwer	O
(	O
1981	O
)	O
and	O
barto	O
and	O
sutton	O
(	O
1981b	O
)	O
extended	O
the	O
idea	O
of	O
an	O
associative	O
memory	O
network	O
(	O
e.g.	O
,	O
kohonen	O
,	O
1977	O
;	O
anderson	O
,	O
silverstein	O
,	O
ritz	O
,	O
and	O
jones	O
,	O
1977	O
)	O
to	O
reinforcement	B
learning	I
.	O
barto	O
,	O
anderson	O
,	O
and	O
sutton	O
(	O
1982	O
)	O
used	O
a	O
two-layer	O
ann	O
to	O
learn	O
a	O
nonlinear	O
control	B
policy	O
,	O
and	O
emphasized	O
the	O
ﬁrst	O
layer	O
’	O
s	O
role	O
of	O
learning	O
a	O
suitable	O
representation	O
.	O
hampson	O
(	O
1983	O
,	O
1989	O
)	O
was	O
an	O
early	O
proponent	O
of	O
mul-	O
tilayer	O
anns	O
for	O
learning	O
value	B
functions	O
.	O
barto	O
,	O
sutton	O
,	O
and	O
anderson	O
(	O
1983	O
)	O
presented	O
an	O
actor–critic	B
algorithm	O
in	O
the	O
form	O
of	O
an	O
ann	O
learning	O
to	O
balance	O
a	O
simulated	O
pole	O
(	O
see	O
sections	O
15.7	O
and	O
15.8	O
)	O
.	O
barto	O
and	O
anandan	O
(	O
1985	O
)	O
intro-	O
duced	O
a	O
stochastic	O
version	O
of	O
widrow	O
et	O
al.	O
’	O
s	O
(	O
1973	O
)	O
selective	O
bootstrap	O
algorithm	O
called	O
the	O
associative	O
reward-penalty	O
(	O
ar−p	O
)	O
algorithm	O
.	O
barto	O
(	O
1985	O
,	O
1986	O
)	O
and	O
barto	O
and	O
jordan	O
(	O
1987	O
)	O
described	O
multi-layer	O
anns	O
consisting	O
of	O
ar−p	O
units	O
trained	O
with	O
a	O
globally-broadcast	O
reinforcement	B
signal	I
to	O
learn	O
classiﬁcation	O
rules	O
that	O
are	O
not	O
linearly	O
separable	O
.	O
barto	O
(	O
1985	O
)	O
discussed	O
this	O
approach	O
to	O
anns	O
and	O
how	O
this	O
type	O
of	O
learning	O
rule	O
is	O
related	O
to	O
others	O
in	O
the	O
literature	O
at	O
that	O
time	O
.	O
(	O
see	O
section	O
15.10	O
for	O
additional	O
discussion	O
of	O
this	O
approach	O
to	O
training	O
multi-layer	O
anns	O
.	O
)	O
anderson	O
(	O
1986	O
,	O
1987	O
,	O
1989	O
)	O
evaluated	O
numerous	O
methods	O
for	O
training	O
multilayer	O
anns	O
and	O
showed	O
that	O
an	O
actor–critic	B
algorithm	O
in	O
which	O
both	O
the	O
actor	O
and	O
critic	O
were	O
implemented	O
by	O
two-layer	O
anns	O
trained	O
by	O
er-	O
ror	O
backpropagation	B
outperformed	O
single-layer	O
anns	O
in	O
the	O
pole-balancing	O
and	O
tower	O
of	O
hanoi	O
tasks	O
.	O
williams	O
(	O
1988	O
)	O
described	O
several	O
ways	O
that	O
backpropa-	O
gation	O
and	B
reinforcement	I
learning	O
can	O
be	O
combined	O
for	O
training	O
anns	O
.	O
gulla-	O
palli	O
(	O
1990	O
)	O
and	O
williams	O
(	O
1992	O
)	O
devised	O
reinforcement	B
learning	I
algorithms	O
for	O
neuron-like	O
units	O
having	O
continuous	O
,	O
rather	O
than	O
binary	O
,	O
outputs	O
.	O
barto	O
,	O
sutton	O
,	O
and	O
watkins	O
(	O
1990	O
)	O
argued	O
that	O
anns	O
can	O
play	O
signiﬁcant	O
roles	O
for	O
approximat-	O
ing	B
functions	O
required	O
for	O
solving	O
sequential	O
decision	O
problems	O
.	O
williams	O
(	O
1992	O
)	O
related	O
reinforce	O
learning	O
rules	O
(	O
section	O
13.3	O
)	O
to	O
the	O
error	O
backpropagation	O
method	O
for	O
training	O
multi-layer	O
anns	O
.	O
tesauro	O
’	O
s	O
td-gammon	O
(	O
tesauro	O
1992	O
,	O
1994	O
;	O
section	O
16.1	O
)	O
inﬂuentially	O
demonstrated	O
the	O
learning	O
abilities	O
of	O
td	O
(	O
λ	O
)	O
algorithm	O
with	B
function	I
approximation	I
by	O
multi-layer	O
anns	O
in	O
learning	O
to	O
play	O
backgammon	B
.	O
the	O
alphago	O
,	O
alphago	O
zero	O
,	O
and	O
alphazero	O
programs	O
of	O
silver	O
et	O
al	O
.	O
(	O
2016	O
,	O
2017a	O
,	O
b	O
;	O
section	O
16.6	O
)	O
used	O
reinforcement	B
learning	I
with	O
deep	O
con-	O
volutional	O
anns	O
in	O
achieving	O
impressive	O
results	O
with	O
the	O
game	O
of	O
go	O
.	O
schmid-	O
huber	O
(	O
2015	O
)	O
reviews	O
applications	O
of	O
anns	O
in	O
reinforcement	O
learning	O
,	O
including	O
applications	O
of	O
recurrent	O
anns	O
.	O
9.8	O
lstd	O
is	O
due	O
to	O
bradtke	O
and	O
barto	O
(	O
see	O
bradtke	O
,	O
1993	O
,	O
1994	O
;	O
bradtke	O
and	O
barto	O
,	O
1996	O
;	O
bradtke	O
,	O
ydstie	O
,	O
and	O
barto	O
,	O
1994	O
)	O
,	O
and	O
was	O
further	O
developed	O
by	O
boyan	O
(	O
1999	O
,	O
2002	O
)	O
,	O
nedi´c	O
and	O
bertsekas	O
(	O
2003	O
)	O
,	O
and	O
yu	O
(	O
2010	O
)	O
.	O
the	O
incremental	O
up-	O
date	O
of	O
the	O
inverse	O
matrix	O
has	O
been	O
known	O
at	O
least	O
since	O
1949	O
(	O
sherman	O
and	O
morrison	O
,	O
1949	O
)	O
.	O
an	O
extension	O
of	O
least-squares	O
methods	O
to	O
control	B
was	O
intro-	O
duced	O
by	O
lagoudakis	O
and	O
parr	O
(	O
2003	O
;	O
bu¸soniu	O
,	O
lazaric	O
,	O
ghavamzadeh	O
,	O
munos	O
,	O
babu˘ska	O
,	O
and	O
de	O
schutter	O
,	O
2012	O
)	O
.	O
240	O
9.9	O
9.10	O
chapter	O
9	O
:	O
on-policy	O
prediction	O
with	B
approximation	I
our	O
discussion	O
of	O
memory-based	O
function	B
approximation	I
is	O
largely	O
based	O
on	O
the	O
review	O
of	O
locally	O
weighted	O
learning	O
by	O
atkeson	O
,	O
moore	O
,	O
and	O
schaal	O
(	O
1997	O
)	O
.	O
atke-	O
son	O
(	O
1992	O
)	O
discussed	O
the	O
use	O
of	O
locally	O
weighted	O
regression	O
in	O
memory-based	O
robot	O
learning	O
and	O
supplied	O
an	O
extensive	O
bibliography	O
covering	O
the	O
history	B
of	I
the	O
idea	O
.	O
stanﬁll	O
and	O
waltz	O
(	O
1986	O
)	O
inﬂuentially	O
argued	O
for	O
the	O
importance	O
of	O
memory	O
based	O
methods	O
in	O
artiﬁcial	O
intelligence	O
,	O
especially	O
in	O
light	O
of	O
parallel	O
ar-	O
chitectures	O
then	O
becoming	O
available	O
,	O
such	O
as	O
the	O
connection	O
machine	O
.	O
baird	O
and	O
klopf	O
(	O
1993	O
)	O
introduced	O
a	O
novel	O
memory-based	O
approach	O
and	O
used	O
it	O
as	O
the	O
func-	O
tion	B
approximation	O
method	O
for	O
q-learning	O
applied	O
to	O
the	O
pole-balancing	O
task	O
.	O
schaal	O
and	O
atkeson	O
(	O
1994	O
)	O
applied	O
locally	O
weighted	O
regression	O
to	O
a	O
robot	O
juggling	O
control	B
problem	O
,	O
where	O
it	O
was	O
used	O
to	O
learn	O
a	O
system	O
model	O
.	O
peng	O
(	O
1995	O
)	O
used	O
the	O
pole-balancing	O
task	O
to	O
experiment	O
with	O
several	O
nearest-neighbor	O
methods	O
for	O
approximating	O
value	B
functions	O
,	O
policies	O
,	O
and	O
environment	O
models	O
.	O
tadepalli	O
and	O
ok	O
(	O
1996	O
)	O
obtained	O
promising	O
results	O
with	O
locally-weighted	O
linear	O
regression	O
to	O
learn	O
a	O
value	B
function	I
for	O
a	O
simulated	O
automatic	O
guided	O
vehicle	O
task	O
.	O
bottou	O
and	O
vapnik	O
(	O
1992	O
)	O
demonstrated	O
surprising	O
eﬃciency	O
of	O
several	O
local	O
learning	O
algorithms	O
compared	O
to	O
non-local	O
algorithms	O
in	O
some	O
pattern	O
recognition	O
tasks	O
,	O
discussing	O
the	O
impact	O
of	O
local	O
learning	O
on	O
generalization	O
.	O
bentley	O
(	O
1975	O
)	O
introduced	O
k-d	O
trees	O
and	O
reported	O
observing	O
average	O
running	O
time	O
of	O
o	O
(	O
log	O
n	O
)	O
for	O
nearest	O
neighbor	O
search	O
over	O
n	O
records	O
.	O
friedman	O
,	O
bentley	O
,	O
and	O
finkel	O
(	O
1977	O
)	O
clariﬁed	O
the	O
algorithm	O
for	O
nearest	O
neighbor	O
search	O
with	O
k-	O
d	O
trees	O
.	O
omohundro	O
(	O
1987	O
)	O
discussed	O
eﬃciency	O
gains	O
possible	O
with	O
hierarchical	O
data	O
structures	O
such	O
as	O
k-d-trees	O
.	O
moore	O
,	O
schneider	O
,	O
and	O
deng	O
(	O
1997	O
)	O
introduced	O
the	O
use	O
of	O
k-d	O
trees	O
for	O
eﬃcient	O
locally	O
weighted	O
regression	O
.	O
the	O
origin	O
of	O
kernel	O
regression	O
is	O
the	O
method	O
of	O
potential	O
functions	O
of	O
aizer-	O
man	O
,	O
braverman	O
,	O
and	O
rozonoer	O
(	O
1964	O
)	O
.	O
they	O
likened	O
the	O
data	O
to	O
point	O
electric	O
charges	O
of	O
various	O
signs	O
and	O
magnitudes	O
distributed	O
over	O
space	O
.	O
the	O
resulting	O
electric	O
potential	O
over	O
space	O
produced	O
by	O
summing	O
the	O
potentials	O
of	O
the	O
point	O
charges	O
corresponded	O
to	O
the	O
interpolated	O
surface	O
.	O
in	O
this	O
analogy	O
,	O
the	O
kernel	O
function	O
is	O
the	O
potential	O
of	O
a	O
point	O
charge	O
,	O
which	O
falls	O
oﬀ	O
as	O
the	O
reciprocal	O
of	O
the	O
distance	O
from	O
the	O
charge	O
.	O
connell	O
and	O
utgoﬀ	O
(	O
1987	O
)	O
applied	O
an	O
actor–critic	B
method	O
to	O
the	O
pole-balancing	O
task	O
in	O
which	O
the	O
critic	B
approximated	O
the	O
value	B
function	I
using	O
kernel	O
regression	O
with	O
an	O
inverse-distance	O
weighting	O
.	O
predating	O
widespread	O
interest	O
in	O
kernel	O
regression	O
in	O
machine	O
learning	O
,	O
these	O
authors	O
did	O
not	O
use	O
the	O
term	O
kernel	O
,	O
but	O
referred	O
to	O
“	O
shepard	O
’	O
s	O
method	O
”	O
(	O
shepard	O
,	O
1968	O
)	O
.	O
other	O
kernel-based	O
approaches	O
to	O
reinforcement	B
learning	I
include	O
those	O
of	O
or-	O
moneit	O
and	O
sen	O
(	O
2002	O
)	O
,	O
dietterich	O
and	O
wang	O
(	O
2002	O
)	O
,	O
xu	O
,	O
xie	O
,	O
hu	O
,	O
and	O
lu	O
(	O
2005	O
)	O
,	O
taylor	O
and	O
parr	O
(	O
2009	O
)	O
,	O
barreto	O
,	O
precup	O
,	O
and	O
pineau	O
(	O
2011	O
)	O
,	O
and	O
bhat	O
,	O
farias	O
,	O
and	O
moallemi	O
(	O
2012	O
)	O
.	O
9.11	O
for	O
emphatic-td	O
methods	O
,	O
see	O
the	O
bibliographical	O
notes	O
to	O
section	O
11.8	O
.	O
9.12.	O
summary	O
241	O
the	O
earliest	O
example	O
we	O
know	O
of	O
in	O
which	O
function	B
approximation	I
methods	O
were	O
used	O
for	O
learning	O
value	B
functions	O
was	O
samuel	O
’	O
s	O
checkers	O
player	O
(	O
1959	O
,	O
1967	O
)	O
.	O
samuel	O
followed	O
shannon	O
’	O
s	O
(	O
1950	O
)	O
suggestion	O
that	O
a	O
value	B
function	I
did	O
not	O
have	O
to	O
be	O
exact	O
to	O
be	O
a	O
useful	O
guide	O
to	O
selecting	O
moves	O
in	O
a	O
game	O
and	O
that	O
it	O
might	O
be	O
approximated	O
by	O
linear	O
combi-	O
nation	O
of	O
features	O
.	O
in	O
addition	O
to	O
linear	B
function	I
approximation	I
,	O
samuel	O
experimented	O
with	O
lookup	O
tables	O
and	O
hierarchical	O
lookup	O
tables	O
called	O
signature	O
tables	O
(	O
griﬃth	O
,	O
1966	O
,	O
1974	O
;	O
page	O
,	O
1977	O
;	O
biermann	O
,	O
fairﬁeld	O
,	O
and	O
beres	O
,	O
1982	O
)	O
.	O
at	O
about	O
the	O
same	O
time	O
as	O
samuel	O
’	O
s	O
work	O
,	O
bellman	O
and	O
dreyfus	O
(	O
1959	O
)	O
proposed	O
using	O
function	B
approximation	I
methods	O
with	O
dp	O
.	O
(	O
it	O
is	O
tempting	O
to	O
think	O
that	O
bellman	O
and	O
samuel	O
had	O
some	O
inﬂuence	O
on	O
one	O
another	O
,	O
but	O
we	O
know	O
of	O
no	O
reference	O
to	O
the	O
other	O
in	O
the	O
work	O
of	O
either	O
.	O
)	O
there	O
is	O
now	O
a	O
fairly	O
extensive	O
literature	O
on	O
function	B
approximation	I
methods	O
and	O
dp	O
,	O
such	O
as	O
multigrid	O
methods	O
and	O
methods	O
using	O
splines	O
and	O
orthogonal	O
polynomials	O
(	O
e.g.	O
,	O
bellman	O
and	O
dreyfus	O
,	O
1959	O
;	O
bellman	O
,	O
kalaba	O
,	O
and	O
kotkin	O
,	O
1973	O
;	O
daniel	O
,	O
1976	O
;	O
whitt	O
,	O
1978	O
;	O
reetz	O
,	O
1977	O
;	O
schweitzer	O
and	O
seidmann	O
,	O
1985	O
;	O
chow	O
and	O
tsitsiklis	O
,	O
1991	O
;	O
kushner	O
and	O
dupuis	O
,	O
1992	O
;	O
rust	O
,	O
1996	O
)	O
.	O
holland	O
’	O
s	O
(	O
1986	O
)	O
classiﬁer	O
system	O
used	O
a	O
selective	O
feature-match	O
technique	O
to	O
gener-	O
alize	O
evaluation	O
information	O
across	O
state–action	O
pairs	O
.	O
each	O
classiﬁer	O
matched	O
a	O
subset	O
of	O
states	O
having	O
speciﬁed	O
values	O
for	O
a	O
subset	O
of	O
features	O
,	O
with	O
the	O
remaining	O
features	O
having	O
arbitrary	O
values	O
(	O
“	O
wild	O
cards	O
”	O
)	O
.	O
these	O
subsets	O
were	O
then	O
used	O
in	O
a	O
conventional	O
state-aggregation	O
approach	O
to	O
function	B
approximation	I
.	O
holland	O
’	O
s	O
idea	O
was	O
to	O
use	O
a	O
ge-	O
netic	O
algorithm	O
to	O
evolve	O
a	O
set	O
of	O
classiﬁers	O
that	O
collectively	O
would	O
implement	O
a	O
useful	O
action-value	B
function	I
.	O
holland	O
’	O
s	O
ideas	O
inﬂuenced	O
the	O
early	O
research	O
of	O
the	O
authors	O
on	O
reinforcement	B
learning	I
,	O
but	O
we	O
focused	O
on	O
diﬀerent	O
approaches	O
to	O
function	O
approxima-	O
tion	B
.	O
as	O
function	O
approximators	O
,	O
classiﬁers	O
are	O
limited	O
in	O
several	O
ways	O
.	O
first	O
,	O
they	O
are	O
state-aggregation	O
methods	O
,	O
with	O
concomitant	O
limitations	O
in	O
scaling	O
and	O
in	O
representing	O
smooth	O
functions	O
eﬃciently	O
.	O
in	O
addition	O
,	O
the	O
matching	O
rules	O
of	O
classiﬁers	O
can	O
implement	O
only	O
aggregation	O
boundaries	O
that	O
are	O
parallel	O
to	O
the	O
feature	O
axes	O
.	O
perhaps	O
the	O
most	O
important	O
limitation	O
of	O
conventional	O
classiﬁer	B
systems	I
is	O
that	O
the	O
classiﬁers	O
are	O
learned	O
via	O
the	O
genetic	O
algorithm	O
,	O
an	O
evolutionary	O
method	O
.	O
as	O
we	O
discussed	O
in	O
chapter	O
1	O
,	O
there	O
is	O
available	O
during	O
learning	O
much	O
more	O
detailed	O
information	O
about	O
how	O
to	O
learn	O
than	O
can	O
be	O
used	O
by	O
evolutionary	B
methods	I
.	O
this	O
perspective	O
led	O
us	O
to	O
instead	O
adapt	O
super-	O
vised	O
learning	O
methods	O
for	O
use	O
in	O
reinforcement	O
learning	O
,	O
speciﬁcally	O
gradient-descent	O
and	O
neural	O
network	O
methods	O
.	O
these	O
diﬀerences	O
between	O
holland	O
’	O
s	O
approach	O
and	O
ours	O
are	O
not	O
surprising	O
because	O
holland	O
’	O
s	O
ideas	O
were	O
developed	O
during	O
a	O
period	O
when	O
neural	B
networks	I
were	O
generally	O
regarded	O
as	O
being	O
too	O
weak	O
in	O
computational	O
power	O
to	O
be	O
useful	O
,	O
whereas	O
our	O
work	O
was	O
at	O
the	O
beginning	O
of	O
the	O
period	O
that	O
saw	O
widespread	O
questioning	O
of	O
that	O
conventional	O
wisdom	O
.	O
there	O
remain	O
many	O
opportunities	O
for	O
combining	O
aspects	O
of	O
these	O
diﬀerent	O
approaches	O
.	O
christensen	O
and	O
korf	O
(	O
1986	O
)	O
experimented	O
with	O
regression	O
methods	O
for	O
modifying	O
coeﬃcients	O
of	O
linear	O
value	B
function	I
approximations	O
in	O
the	O
game	O
of	O
chess	O
.	O
chapman	O
and	O
kaelbling	O
(	O
1991	O
)	O
and	O
tan	O
(	O
1991	O
)	O
adapted	O
decision-tree	O
methods	O
for	O
learning	O
value	B
functions	O
.	O
explanation-based	O
learning	O
methods	O
have	O
also	O
been	O
adapted	O
for	O
learning	O
value	B
functions	O
,	O
yielding	O
compact	O
representations	O
(	O
yee	O
,	O
saxena	O
,	O
utgoﬀ	O
,	O
and	O
barto	O
,	O
1990	O
;	O
dietterich	O
and	O
flann	O
,	O
1995	O
)	O
.	O
chapter	O
10	O
on-policy	O
control	O
with	B
approximation	I
in	O
this	O
chapter	O
we	O
return	B
to	O
the	O
control	B
problem	O
,	O
now	O
with	O
parametric	O
approximation	O
of	O
the	O
action-value	B
function	I
ˆq	O
(	O
s	O
,	O
a	O
,	O
w	O
)	O
≈	O
q∗	O
(	O
s	O
,	O
a	O
)	O
,	O
where	O
w	O
∈	O
rd	O
is	O
a	O
ﬁnite-dimensional	O
weight	O
vector	B
.	O
we	O
continue	O
to	O
restrict	O
attention	O
to	O
the	O
on-policy	O
case	O
,	O
leaving	O
oﬀ-policy	B
methods	I
to	O
chapter	O
11.	O
the	O
present	O
chapter	O
features	O
the	O
semi-gradient	O
sarsa	O
algorithm	O
,	O
the	O
natural	O
extension	O
of	O
semi-gradient	O
td	O
(	O
0	O
)	O
(	O
last	O
chapter	O
)	O
to	O
action	B
values	O
and	O
to	O
on-	O
policy	B
control	O
.	O
in	O
the	O
episodic	O
case	O
,	O
the	O
extension	O
is	O
straightforward	O
,	O
but	O
in	O
the	O
continuing	O
case	O
we	O
have	O
to	O
take	O
a	O
few	O
steps	O
backward	O
and	O
re-examine	O
how	O
we	O
have	O
used	O
discounting	B
to	O
deﬁne	O
an	O
optimal	O
policy	O
.	O
surprisingly	O
,	O
once	O
we	O
have	O
genuine	O
function	B
approximation	I
we	O
have	O
to	O
give	O
up	O
discounting	B
and	O
switch	O
to	O
a	O
new	O
“	O
average-reward	O
”	O
formulation	O
of	O
the	O
control	B
problem	O
,	O
with	O
new	O
“	O
diﬀerential	B
”	O
value	B
functions	O
.	O
starting	O
ﬁrst	O
in	O
the	O
episodic	O
case	O
,	O
we	O
extend	O
the	O
function	B
approximation	I
ideas	O
pre-	O
sented	O
in	O
the	O
last	O
chapter	O
from	O
state	B
values	O
to	O
action	B
values	O
.	O
then	O
we	O
extend	O
them	O
to	O
control	B
following	O
the	O
general	O
pattern	O
of	O
on-policy	O
gpi	O
,	O
using	O
ε-greedy	O
for	O
action	B
selection	O
.	O
we	O
show	O
results	O
for	O
n-step	O
linear	O
sarsa	O
on	O
the	O
mountain	O
car	O
problem	O
.	O
then	O
we	O
turn	O
to	O
the	O
continuing	O
case	O
and	O
repeat	O
the	O
development	O
of	O
these	O
ideas	O
for	O
the	O
average-reward	O
case	O
with	O
diﬀerential	O
values	O
.	O
10.1	O
episodic	O
semi-gradient	O
control	O
the	O
extension	O
of	O
the	O
semi-gradient	O
prediction	O
methods	O
of	O
chapter	O
9	O
to	O
action	B
values	O
is	O
straightforward	O
.	O
in	O
this	O
case	O
it	O
is	O
the	O
approximate	B
action-value	O
function	O
,	O
ˆq	O
≈	O
qπ	O
,	O
that	O
is	O
represented	O
as	O
a	O
parameterized	O
functional	O
form	O
with	O
weight	O
vector	B
w.	O
whereas	O
before	O
we	O
considered	O
random	O
training	O
examples	O
of	O
the	O
form	O
st	O
(	O
cid:55	O
)	O
→	O
ut	O
,	O
now	O
we	O
consider	O
examples	O
of	O
the	O
form	O
st	O
,	O
at	O
(	O
cid:55	O
)	O
→	O
ut	O
.	O
the	O
update	O
target	B
ut	O
can	O
be	O
any	O
approximation	O
of	O
qπ	O
(	O
st	O
,	O
at	O
)	O
,	O
including	O
the	O
usual	O
backed-up	O
values	O
such	O
as	O
the	O
full	O
monte	O
carlo	O
return	B
(	O
gt	O
)	O
or	O
any	O
of	O
the	O
n-step	B
sarsa	O
returns	O
(	O
7.4	O
)	O
.	O
the	O
general	O
gradient-descent	O
update	O
for	O
action-value	O
243	O
244	O
chapter	O
10	O
:	O
on-policy	O
control	O
with	B
approximation	I
prediction	O
is	O
.	O
wt+1	O
wt+1	O
(	O
10.1	O
)	O
(	O
10.2	O
)	O
for	O
example	O
,	O
the	O
update	O
for	O
the	O
one-step	O
sarsa	O
method	O
is	O
=	O
wt	O
+	O
α	O
(	O
cid:104	O
)	O
ut	O
−	O
ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
(	O
cid:105	O
)	O
∇ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
.	O
=	O
wt	O
+	O
α	O
(	O
cid:104	O
)	O
rt+1	O
+	O
γ	O
ˆq	O
(	O
st+1	O
,	O
at+1	O
,	O
wt	O
)	O
−	O
ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
(	O
cid:105	O
)	O
∇ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
.	O
.	O
we	O
call	O
this	O
method	O
episodic	O
semi-gradient	O
one-step	O
sarsa	O
.	O
for	O
a	O
constant	O
policy	B
,	O
this	O
method	O
converges	O
in	O
the	O
same	O
way	O
that	O
td	O
(	O
0	O
)	O
does	O
,	O
with	O
the	O
same	O
kind	O
of	O
error	O
bound	O
(	O
9.14	O
)	O
.	O
to	O
form	O
control	B
methods	O
,	O
we	O
need	O
to	O
couple	O
such	O
action-value	O
prediction	O
methods	O
with	O
techniques	O
for	O
policy	O
improvement	O
and	O
action	O
selection	O
.	O
suitable	O
techniques	O
applicable	O
to	O
continuous	O
actions	O
,	O
or	O
to	O
actions	O
from	O
large	O
discrete	O
sets	O
,	O
are	O
a	O
topic	O
of	O
ongoing	O
research	O
with	O
as	O
yet	O
no	O
clear	O
resolution	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
the	O
action	B
set	O
is	O
discrete	O
and	O
not	O
too	O
large	O
,	O
then	O
we	O
can	O
use	O
the	O
techniques	O
already	O
developed	O
in	O
previous	O
chapters	O
.	O
that	O
is	O
,	O
for	O
each	O
possible	O
action	B
a	O
available	O
in	O
the	O
current	O
state	B
st	O
,	O
we	O
can	O
compute	O
ˆq	O
(	O
st	O
,	O
a	O
,	O
wt	O
)	O
and	O
then	O
ﬁnd	O
the	O
greedy	O
action	O
a∗t	O
=	O
argmaxa	O
ˆq	O
(	O
st	O
,	O
a	O
,	O
wt	O
)	O
.	O
policy	B
improvement	I
is	O
then	O
done	O
(	O
in	O
the	O
on-policy	O
case	O
treated	O
in	O
this	O
chapter	O
)	O
by	O
changing	O
the	O
estimation	O
policy	B
to	O
a	O
soft	O
approximation	O
of	O
the	O
greedy	O
policy	O
such	O
as	O
the	O
ε-greedy	O
policy	O
.	O
actions	O
are	O
selected	O
according	O
to	O
this	O
same	O
policy	B
.	O
pseudocode	O
for	O
the	O
complete	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
.	O
episodic	O
semi-gradient	O
sarsa	O
for	O
estimating	O
ˆq	O
≈	O
q∗	O
input	O
:	O
a	O
diﬀerentiable	O
action-value	B
function	I
parameterization	O
ˆq	O
:	O
s	O
×	O
a	O
×	O
rd	O
→	O
r	O
algorithm	O
parameters	O
:	O
step	O
size	O
α	O
>	O
0	O
,	O
small	O
ε	O
>	O
0	O
initialize	O
value-function	O
weights	O
w	O
∈	O
rd	O
arbitrarily	O
(	O
e.g.	O
,	O
w	O
=	O
0	O
)	O
loop	O
for	O
each	O
episode	O
:	O
s	O
,	O
a	O
←	O
initial	O
state	B
and	O
action	B
of	O
episode	O
(	O
e.g.	O
,	O
ε-greedy	O
)	O
loop	O
for	O
each	O
step	O
of	O
episode	O
:	O
take	O
action	B
a	O
,	O
observe	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
if	O
s	O
(	O
cid:48	O
)	O
is	O
terminal	O
:	O
go	O
to	O
next	O
episode	O
w	O
←	O
w	O
+	O
α	O
(	O
cid:2	O
)	O
r	O
−	O
ˆq	O
(	O
s	O
,	O
a	O
,	O
w	O
)	O
(	O
cid:3	O
)	O
∇ˆq	O
(	O
s	O
,	O
a	O
,	O
w	O
)	O
choose	O
a	O
(	O
cid:48	O
)	O
as	O
a	O
function	O
of	O
ˆq	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
·	O
,	O
w	O
)	O
(	O
e.g.	O
,	O
ε-greedy	O
)	O
w	O
←	O
w	O
+	O
α	O
(	O
cid:2	O
)	O
r	O
+	O
γ	O
ˆq	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
(	O
cid:48	O
)	O
,	O
w	O
)	O
−	O
ˆq	O
(	O
s	O
,	O
a	O
,	O
w	O
)	O
(	O
cid:3	O
)	O
∇ˆq	O
(	O
s	O
,	O
a	O
,	O
w	O
)	O
s	O
←	O
s	O
(	O
cid:48	O
)	O
a	O
←	O
a	O
(	O
cid:48	O
)	O
example	O
10.1	O
:	O
mountain	O
car	O
task	O
consider	O
the	O
task	O
of	O
driving	O
an	O
underpowered	O
car	O
up	O
a	O
steep	O
mountain	O
road	O
,	O
as	O
suggested	O
by	O
the	O
diagram	O
in	O
the	O
upper	O
left	O
of	O
figure	O
10.1.	O
the	O
diﬃculty	O
is	O
that	O
gravity	O
is	O
stronger	O
than	O
the	O
car	O
’	O
s	O
engine	O
,	O
and	O
even	O
at	O
full	O
throttle	O
the	O
car	O
can	O
not	O
accelerate	O
up	O
the	O
steep	O
slope	O
.	O
the	O
only	O
solution	O
is	O
to	O
ﬁrst	O
move	O
away	O
from	O
10.1.	O
episodic	O
semi-gradient	O
control	O
245	O
figure	O
10.1	O
:	O
the	O
mountain	O
car	O
task	O
(	O
upper	O
left	O
panel	O
)	O
and	O
the	O
cost-to-go	O
function	O
(	O
−	O
maxa	O
ˆq	O
(	O
s	O
,	O
a	O
,	O
w	O
)	O
)	O
learned	O
during	O
one	O
run	O
.	O
the	O
goal	B
and	O
up	O
the	O
opposite	O
slope	O
on	O
the	O
left	O
.	O
then	O
,	O
by	O
applying	O
full	O
throttle	O
the	O
car	O
can	O
build	O
up	O
enough	O
inertia	O
to	O
carry	O
it	O
up	O
the	O
steep	O
slope	O
even	O
though	O
it	O
is	O
slowing	O
down	O
the	O
whole	O
way	O
.	O
this	O
is	O
a	O
simple	O
example	O
of	O
a	O
continuous	O
control	O
task	O
where	O
things	O
have	O
to	O
get	O
worse	O
in	O
a	O
sense	O
(	O
farther	O
from	O
the	O
goal	B
)	O
before	O
they	O
can	O
get	O
better	O
.	O
many	O
control	B
methodologies	O
have	O
great	O
diﬃculties	O
with	O
tasks	O
of	O
this	O
kind	O
unless	O
explicitly	O
aided	O
by	O
a	O
human	O
designer	O
.	O
the	O
reward	O
in	O
this	O
problem	O
is	O
−1	O
on	O
all	O
time	O
steps	O
until	O
the	O
car	O
moves	O
past	O
its	O
goal	B
position	O
at	O
the	O
top	O
of	O
the	O
mountain	O
,	O
which	O
ends	O
the	O
episode	O
.	O
there	O
are	O
three	O
possible	O
actions	O
:	O
full	O
throttle	O
forward	O
(	O
+1	O
)	O
,	O
full	O
throttle	O
reverse	O
(	O
−1	O
)	O
,	O
and	O
zero	O
throttle	O
(	O
0	O
)	O
.	O
the	O
car	O
moves	O
according	O
to	O
a	O
simpliﬁed	O
physics	O
.	O
its	O
position	O
,	O
xt	O
,	O
and	O
velocity	O
,	O
˙xt	O
,	O
are	O
updated	O
by	O
xt+1	O
˙xt+1	O
.	O
.	O
=	O
bound	O
(	O
cid:2	O
)	O
xt	O
+	O
˙xt+1	O
(	O
cid:3	O
)	O
=	O
bound	O
(	O
cid:2	O
)	O
˙xt	O
+	O
0.001at	O
−	O
0.0025	O
cos	O
(	O
3xt	O
)	O
(	O
cid:3	O
)	O
,	O
where	O
the	O
bound	O
operation	O
enforces	O
−1.2	O
≤	O
xt+1	O
≤	O
0.5	O
and	O
−0.07	O
≤	O
˙xt+1	O
≤	O
0.07.	O
in	O
addition	O
,	O
when	O
xt+1	O
reached	O
the	O
left	O
bound	O
,	O
˙xt+1	O
was	O
reset	O
to	O
zero	O
.	O
when	O
it	O
reached	O
the	O
right	O
bound	O
,	O
the	O
goal	B
was	O
reached	O
and	O
the	O
episode	O
was	O
terminated	O
.	O
each	O
episode	O
started	O
from	O
a	O
random	O
position	O
xt	O
∈	O
[	O
−0.6	O
,	O
−0.4	O
)	O
and	O
zero	O
velocity	O
.	O
to	O
convert	O
the	O
two	O
contin-	O
uous	O
state	B
variables	O
to	O
binary	B
features	I
,	O
we	O
used	O
grid-tilings	O
as	O
in	O
figure	O
9.9.	O
we	O
used	O
8	O
tilings	O
,	O
with	O
each	O
tile	O
covering	O
1/8th	O
of	O
the	O
bounded	O
distance	O
in	O
each	O
dimension	O
,	O
and	O
!	O
1.2position0.6step	O
428goalposition40	O
!	O
.07.07velocityvelocityvelocityvelocityvelocityvelocitypositionpositionposition02701200104046episode	O
12episode	O
104episode	O
1000episode	O
9000mountain	O
cargoal	O
246	O
chapter	O
10	O
:	O
on-policy	O
control	O
with	B
approximation	I
asymmetrical	O
oﬀsets	O
as	O
described	O
in	O
section	O
9.5.4.1	O
the	O
feature	O
vectors	O
x	O
(	O
s	O
,	O
a	O
)	O
created	O
by	O
tile	B
coding	I
were	O
then	O
combined	O
linearly	O
with	O
the	O
parameter	O
vector	O
to	O
approximate	B
the	O
action-value	B
function	I
:	O
ˆq	O
(	O
s	O
,	O
a	O
,	O
w	O
)	O
.	O
=	O
w	O
(	O
cid:62	O
)	O
x	O
(	O
s	O
,	O
a	O
)	O
=	O
d	O
(	O
cid:88	O
)	O
i=1	O
wi	O
·	O
xi	O
(	O
s	O
,	O
a	O
)	O
,	O
for	O
each	O
pair	O
of	O
state	O
,	O
s	O
,	O
and	O
action	O
,	O
a	O
.	O
(	O
10.3	O
)	O
figure	O
10.1	O
shows	O
what	O
typically	O
happens	O
while	O
learning	O
to	O
solve	O
this	O
task	O
with	O
this	O
form	O
of	O
function	O
approximation.2	O
shown	O
is	O
the	O
negative	O
of	O
the	O
value	B
function	I
(	O
the	O
cost-	O
to-go	O
function	O
)	O
learned	O
on	O
a	O
single	O
run	O
.	O
the	O
initial	O
action	B
values	O
were	O
all	O
zero	O
,	O
which	O
was	O
optimistic	O
(	O
all	O
true	O
values	O
are	O
negative	O
in	O
this	O
task	O
)	O
,	O
causing	O
extensive	O
exploration	O
to	O
occur	O
even	O
though	O
the	O
exploration	O
parameter	O
,	O
ε	O
,	O
was	O
0.	O
this	O
can	O
be	O
seen	O
in	O
the	O
middle-top	O
panel	O
of	O
the	O
ﬁgure	O
,	O
labeled	O
“	O
step	O
428	O
”	O
.	O
at	O
this	O
time	O
not	O
even	O
one	O
episode	O
had	O
been	O
completed	O
,	O
but	O
the	O
car	O
has	O
oscillated	O
back	O
and	O
forth	O
in	O
the	O
valley	O
,	O
following	O
circular	O
trajectories	O
in	O
state	O
space	O
.	O
all	O
the	O
states	O
visited	O
frequently	O
are	O
valued	O
worse	O
than	O
unexplored	O
states	O
,	O
because	O
the	O
actual	O
rewards	O
have	O
been	O
worse	O
than	O
what	O
was	O
(	O
unrealistically	O
)	O
expected	O
.	O
this	O
continually	O
drives	O
the	O
agent	O
away	O
from	O
wherever	O
it	O
has	O
been	O
,	O
to	O
explore	O
new	O
states	O
,	O
until	O
a	O
solution	O
is	O
found	O
.	O
figure	O
10.2	O
shows	O
several	O
learning	O
curves	O
for	O
semi-gradient	O
sarsa	O
on	O
this	O
problem	O
,	O
with	O
various	O
step	O
sizes	O
.	O
figure	O
10.2	O
:	O
mountain	O
car	O
learning	O
curves	O
for	O
the	O
semi-gradient	O
sarsa	O
method	O
with	O
tile-coding	O
function	B
approximation	I
and	O
ε-greedy	O
action	O
selection	O
.	O
1in	O
particular	O
,	O
we	O
used	O
the	O
tile-coding	O
software	O
,	O
available	O
at	O
http	O
:	O
//incompleteideas.net/tiles/	O
tiles3.html	O
,	O
with	O
iht=iht	O
(	O
4096	O
)	O
and	O
tiles	O
(	O
iht,8	O
,	O
[	O
8*x/	O
(	O
0.5+1.2	O
)	O
,8*xdot/	O
(	O
0.07+0.07	O
)	O
]	O
,	O
a	O
)	O
to	O
get	O
the	O
indices	O
of	O
the	O
ones	O
in	O
the	O
feature	O
vector	B
for	O
state	B
(	O
x	O
,	O
xdot	O
)	O
and	O
action	O
a	O
.	O
2this	O
data	O
is	O
actually	O
from	O
the	O
“	O
semi-gradient	O
sarsa	O
(	O
λ	O
)	O
”	O
algorithm	O
that	O
we	O
will	O
not	O
meet	O
until	O
chapter	O
12	O
,	O
but	O
semi-gradient	O
sarsa	O
would	O
behave	O
similarly	O
.	O
10020040010000mountain	O
carsteps	O
per	O
episodelog	O
scaleaveraged	O
over	O
100	O
runsepisode500↵=0.5/8↵=0.1/8↵=0.2/8	O
10.2.	O
semi-gradient	O
n-step	O
sarsa	O
247	O
10.2	O
semi-gradient	O
n-step	O
sarsa	O
we	O
can	O
obtain	O
an	O
n-step	B
version	O
of	O
episodic	O
semi-gradient	O
sarsa	O
by	O
using	O
an	O
n-step	B
return	O
as	O
the	O
update	O
target	B
in	O
the	O
semi-gradient	O
sarsa	O
update	O
equation	O
(	O
10.1	O
)	O
.	O
the	O
n-step	B
return	O
immediately	O
generalizes	O
from	O
its	O
tabular	O
form	O
(	O
7.4	O
)	O
to	O
a	O
function	B
approximation	I
form	O
:	O
gt	O
:	O
t+n	O
with	O
gt	O
:	O
t+n	O
.	O
=	O
rt+1	O
+γrt+2	O
+···+γn−1rt+n	O
+γn	O
ˆq	O
(	O
st+n	O
,	O
at+n	O
,	O
wt+n−1	O
)	O
,	O
.	O
=	O
gt	O
if	O
t	O
+	O
n	O
≥	O
t	O
,	O
as	O
usual	O
.	O
the	O
n-step	B
update	O
equation	O
is	O
.	O
=	O
wt+n−1	O
+	O
α	O
[	O
gt	O
:	O
t+n	O
−	O
ˆq	O
(	O
st	O
,	O
at	O
,	O
wt+n−1	O
)	O
]	O
∇ˆq	O
(	O
st	O
,	O
at	O
,	O
wt+n−1	O
)	O
,	O
wt+n	O
t+n	O
<	O
t	O
,	O
(	O
10.4	O
)	O
0	O
≤	O
t	O
<	O
t.	O
(	O
10.5	O
)	O
complete	O
pseudocode	O
is	O
given	O
in	O
the	O
box	O
below	O
.	O
episodic	O
semi-gradient	O
n-step	O
sarsa	O
for	O
estimating	O
ˆq	O
≈	O
q∗	O
or	O
qπ	O
input	O
:	O
a	O
diﬀerentiable	O
action-value	B
function	I
parameterization	O
ˆq	O
:	O
s	O
×	O
a	O
×	O
rd	O
→	O
r	O
input	O
:	O
a	O
policy	B
π	O
(	O
if	O
estimating	O
qπ	O
)	O
algorithm	O
parameters	O
:	O
step	O
size	O
α	O
>	O
0	O
,	O
small	O
ε	O
>	O
0	O
,	O
a	O
positive	O
integer	O
n	O
initialize	O
value-function	O
weights	O
w	O
∈	O
rd	O
arbitrarily	O
(	O
e.g.	O
,	O
w	O
=	O
0	O
)	O
all	O
store	O
and	O
access	O
operations	O
(	O
st	O
,	O
at	O
,	O
and	O
rt	O
)	O
can	O
take	O
their	O
index	O
mod	O
n	O
+	O
1	O
loop	O
for	O
each	O
episode	O
:	O
if	O
t	O
<	O
t	O
,	O
then	O
:	O
take	O
action	B
at	O
observe	O
and	O
store	O
the	O
next	O
reward	O
as	O
rt+1	O
and	O
the	O
next	O
state	B
as	O
st+1	O
if	O
st+1	O
is	O
terminal	O
,	O
then	O
:	O
initialize	O
and	O
store	O
s0	O
(	O
cid:54	O
)	O
=	O
terminal	O
select	O
and	O
store	O
an	O
action	B
a0	O
∼	O
π	O
(	O
·|s0	O
)	O
or	O
ε-greedy	O
wrt	O
ˆq	O
(	O
s0	O
,	O
·	O
,	O
w	O
)	O
t	O
←	O
∞loop	O
for	O
t	O
=	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
:	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
until	O
τ	O
=	O
t	O
−	O
1	O
if	O
τ	O
+	O
n	O
<	O
t	O
,	O
then	O
g	O
←	O
g	O
+	O
γn	O
ˆq	O
(	O
sτ	O
+n	O
,	O
aτ	O
+n	O
,	O
w	O
)	O
w	O
←	O
w	O
+	O
α	O
[	O
g	O
−	O
ˆq	O
(	O
sτ	O
,	O
aτ	O
,	O
w	O
)	O
]	O
∇ˆq	O
(	O
sτ	O
,	O
aτ	O
,	O
w	O
)	O
g	O
←	O
(	O
cid:80	O
)	O
min	O
(	O
τ	O
+n	O
,	O
t	O
)	O
t	O
←	O
t	O
+	O
1	O
select	O
and	O
store	O
at+1	O
∼	O
π	O
(	O
·|st+1	O
)	O
or	O
ε-greedy	O
wrt	O
ˆq	O
(	O
st+1	O
,	O
·	O
,	O
w	O
)	O
(	O
τ	O
is	O
the	O
time	O
whose	O
estimate	O
is	O
being	O
updated	O
)	O
τ	O
←	O
t	O
−	O
n	O
+	O
1	O
if	O
τ	O
≥	O
0	O
:	O
i=τ	O
+1	O
γi−τ−1ri	O
else	O
:	O
(	O
gτ	O
:	O
τ	O
+n	O
)	O
as	O
we	O
have	O
seen	O
before	O
,	O
performance	O
is	O
best	O
if	O
an	O
intermediate	O
level	O
of	O
bootstrapping	O
is	O
used	O
,	O
corresponding	O
to	O
an	O
n	O
larger	O
than	O
1.	O
figure	O
10.3	O
shows	O
how	O
this	O
algorithm	O
tends	O
to	O
learn	O
faster	O
and	O
obtain	O
a	O
better	O
asymptotic	O
performance	O
at	O
n	O
=	O
8	O
than	O
at	O
n	O
=	O
1	O
on	O
the	O
mountain	O
car	O
task	O
.	O
figure	O
10.4	O
shows	O
the	O
results	O
of	O
a	O
more	O
detailed	O
study	O
of	O
the	O
eﬀect	O
of	O
the	O
parameters	O
α	O
and	O
n	O
on	O
the	O
rate	O
of	O
learning	O
on	O
this	O
task	O
.	O
248	O
chapter	O
10	O
:	O
on-policy	O
control	O
with	B
approximation	I
figure	O
10.3	O
:	O
performance	O
of	O
one-step	O
vs	O
8-step	O
semi-gradient	O
sarsa	O
on	O
the	O
mountain	O
car	O
task	O
.	O
good	O
step	O
sizes	O
were	O
used	O
:	O
α	O
=	O
0.5/8	O
for	O
n	O
=	O
1	O
and	O
α	O
=	O
0.3/8	O
for	O
n	O
=	O
8.	O
figure	O
10.4	O
:	O
eﬀect	O
of	O
the	O
α	O
and	O
n	O
on	O
early	O
performance	O
of	O
n-step	O
semi-gradient	O
sarsa	O
and	O
tile-coding	O
function	B
approximation	I
on	O
the	O
mountain	O
car	O
task	O
.	O
as	O
usual	O
,	O
an	O
intermediate	O
level	O
of	O
bootstrapping	O
(	O
n	O
=	O
4	O
)	O
performed	O
best	O
.	O
these	O
results	O
are	O
for	O
selected	O
α	O
values	O
,	O
on	O
a	O
log	O
scale	O
,	O
and	O
then	O
connected	O
by	O
straight	O
lines	O
.	O
the	O
standard	O
errors	O
ranged	O
from	O
0.5	O
(	O
less	O
than	O
the	O
line	O
width	O
)	O
for	O
n	O
=	O
1	O
to	O
about	O
4	O
for	O
n	O
=	O
16	O
,	O
so	O
the	O
main	O
eﬀects	O
are	O
all	O
statistically	O
signiﬁcant	O
.	O
exercise	O
10.1	O
we	O
not	O
explicitly	O
considered	O
or	O
given	O
pseudocode	O
for	O
any	O
monte	O
carlo	O
methods	O
or	O
in	O
this	O
chapter	O
.	O
what	O
would	O
they	O
be	O
like	O
?	O
why	O
is	O
it	O
reasonable	O
not	O
to	O
give	O
(	O
cid:3	O
)	O
pseudocode	O
for	O
them	O
?	O
how	O
would	O
they	O
perform	O
on	O
the	O
mountain	O
car	O
task	O
?	O
exercise	O
10.2	O
give	O
pseudocode	O
for	O
semi-gradient	O
one-step	O
expected	O
sarsa	O
for	O
control	O
.	O
(	O
cid:3	O
)	O
exercise	O
10.3	O
why	O
do	O
the	O
results	O
shown	O
in	O
figure	O
10.4	O
have	O
higher	O
standard	O
errors	O
at	O
(	O
cid:3	O
)	O
large	O
n	O
than	O
at	O
small	O
n	O
?	O
10020040010000mountain	O
carsteps	O
per	O
episodelog	O
scaleaveraged	O
over	O
100	O
runsepisode500n=1n=822024026030000.511.5mountain	O
carsteps	O
per	O
episodeaveraged	O
overﬁrst	O
50	O
episodesand	O
100	O
runs↵×	O
number	O
of	O
tilings	O
(	O
8	O
)	O
280n=1n=2n=4n=8n=16n=8n=4n=2n=16n=1	O
10.3.	O
average	O
reward	O
:	O
a	O
new	O
problem	O
setting	O
for	O
continuing	O
tasks	O
249	O
10.3	O
average	O
reward	O
:	O
a	O
new	O
problem	O
setting	O
for	O
continuing	O
tasks	O
we	O
now	O
introduce	O
a	O
third	O
classical	O
setting—alongside	O
the	O
episodic	O
and	O
discounted	O
settings—	O
for	O
formulating	O
the	O
goal	B
in	O
markov	O
decision	O
problems	O
(	O
mdps	O
)	O
.	O
like	O
the	O
discounted	O
set-	O
ting	O
,	O
the	O
average	B
reward	I
setting	I
applies	O
to	O
continuing	O
problems	O
,	O
problems	O
for	O
which	O
the	O
interaction	O
between	O
agent	O
and	O
environment	O
goes	O
on	O
and	O
on	O
forever	O
without	O
termination	O
or	O
start	O
states	O
.	O
unlike	O
that	O
setting	O
,	O
however	O
,	O
there	O
is	O
no	O
discounting—the	O
agent	O
cares	O
just	O
as	O
much	O
about	O
delayed	O
rewards	O
as	O
it	O
does	O
about	O
immediate	O
reward	O
.	O
the	O
average-	O
reward	O
setting	O
is	O
one	O
of	O
the	O
major	O
settings	O
commonly	O
considered	O
in	O
the	O
classical	O
theory	O
of	O
dynamic	O
programming	O
and	O
less-commonly	O
in	O
reinforcement	O
learning	O
.	O
as	O
we	O
discuss	O
in	O
the	O
next	O
section	O
,	O
the	O
discounted	O
setting	O
is	O
problematic	O
with	B
function	I
approximation	I
,	O
and	O
thus	O
the	O
average-reward	O
setting	O
is	O
needed	O
to	O
replace	O
it	O
.	O
in	O
the	O
average-reward	O
setting	O
,	O
the	O
quality	O
of	O
a	O
policy	B
π	O
is	O
deﬁned	O
as	O
the	O
average	O
rate	O
of	O
reward	O
,	O
or	O
simply	O
average	O
reward	O
,	O
while	O
following	O
that	O
policy	B
,	O
which	O
we	O
denote	O
as	O
r	O
(	O
π	O
)	O
:	O
r	O
(	O
π	O
)	O
.	O
=	O
lim	O
h→∞	O
=	O
lim	O
t→∞	O
=	O
(	O
cid:88	O
)	O
s	O
e	O
[	O
rt	O
|	O
a0	O
:	O
t−1	O
∼	O
π	O
]	O
h	O
(	O
cid:88	O
)	O
t=1	O
1	O
h	O
e	O
[	O
rt	O
|	O
a0	O
:	O
t−1	O
∼	O
π	O
]	O
,	O
π	O
(	O
a|s	O
)	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
µπ	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
a	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
r	O
,	O
(	O
10.6	O
)	O
(	O
10.7	O
)	O
where	O
the	O
expectations	O
are	O
conditioned	O
on	O
the	O
prior	O
actions	O
,	O
a0	O
,	O
a1	O
,	O
.	O
.	O
.	O
,	O
at−1	O
,	O
being	O
.	O
=	O
limt→∞	O
pr	O
{	O
st	O
=	O
s	O
taken	O
according	O
to	O
π	O
,	O
and	O
µπ	O
is	O
the	O
steady-state	O
distribution	O
,	O
µπ	O
(	O
s	O
)	O
|a0	O
:	O
t−1∼	O
π	O
}	O
,	O
which	O
is	O
assumed	O
to	O
exist	O
for	O
any	O
π	O
and	O
to	O
be	O
independent	O
of	O
s0	O
.	O
this	O
assumption	O
about	O
the	O
mdp	O
is	O
known	O
as	O
ergodicity	O
.	O
it	O
means	O
that	O
where	O
the	O
mdp	O
starts	O
or	O
any	O
early	O
decision	O
made	O
by	O
the	O
agent	O
can	O
have	O
only	O
a	O
temporary	O
eﬀect	O
;	O
in	O
the	O
long	O
run	O
the	O
expectation	O
of	O
being	O
in	O
a	O
state	B
depends	O
only	O
on	O
the	O
policy	B
and	O
the	O
mdp	O
transition	B
probabilities	I
.	O
ergodicity	O
is	O
suﬃcient	O
to	O
guarantee	O
the	O
existence	O
of	O
the	O
limits	O
in	O
the	O
equations	O
above	O
.	O
there	O
are	O
subtle	O
distinctions	O
that	O
can	O
be	O
drawn	O
between	O
diﬀerent	O
kinds	O
of	O
optimality	O
in	O
the	O
undiscounted	O
continuing	O
case	O
.	O
nevertheless	O
,	O
for	O
most	O
practical	O
purposes	O
it	O
may	O
be	O
adequate	O
simply	O
to	O
order	O
policies	O
according	O
to	O
their	O
average	O
reward	O
per	O
time	O
step	O
,	O
in	O
other	O
words	O
,	O
according	O
to	O
their	O
r	O
(	O
π	O
)	O
.	O
this	O
quantity	O
is	O
essentially	O
the	O
average	O
reward	O
under	O
π	O
,	O
as	O
suggested	O
by	O
(	O
10.7	O
)	O
.	O
in	O
particular	O
,	O
we	O
consider	O
all	O
policies	O
that	O
attain	O
the	O
maximal	O
value	B
of	O
r	O
(	O
π	O
)	O
to	O
be	O
optimal	O
.	O
note	O
that	O
the	O
steady	O
state	B
distribution	O
is	O
the	O
special	O
distribution	O
under	O
which	O
,	O
if	O
you	O
select	O
actions	O
according	O
to	O
π	O
,	O
you	O
remain	O
in	O
the	O
same	O
distribution	O
.	O
that	O
is	O
,	O
for	O
which	O
(	O
cid:88	O
)	O
s	O
µπ	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
a|s	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
a	O
)	O
=	O
µπ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
.	O
(	O
10.8	O
)	O
in	O
the	O
average-reward	O
setting	O
,	O
returns	O
are	O
deﬁned	O
in	O
terms	O
of	O
diﬀerences	O
between	O
250	O
chapter	O
10	O
:	O
on-policy	O
control	O
with	B
approximation	I
rewards	O
and	O
the	O
average	O
reward	O
:	O
gt	O
.	O
=	O
rt+1	O
−	O
r	O
(	O
π	O
)	O
+	O
rt+2	O
−	O
r	O
(	O
π	O
)	O
+	O
rt+3	O
−	O
r	O
(	O
π	O
)	O
+	O
···	O
.	O
(	O
10.9	O
)	O
this	O
is	O
known	O
as	O
the	O
diﬀerential	B
return	O
,	O
and	O
the	O
corresponding	O
value	B
functions	O
are	O
known	O
as	O
diﬀerential	O
value	B
functions	O
.	O
they	O
are	O
deﬁned	O
in	O
the	O
same	O
way	O
and	O
we	O
will	O
use	O
the	O
.	O
same	O
notation	O
for	O
them	O
as	O
we	O
have	O
all	O
along	O
:	O
vπ	O
(	O
s	O
)	O
=	O
eπ	O
[	O
gt|st	O
=	O
s	O
,	O
at	O
=	O
a	O
]	O
(	O
similarly	O
for	O
v∗	O
and	O
q∗	O
)	O
.	O
diﬀerential	B
value	O
functions	O
also	O
have	O
bellman	O
equations	O
,	O
just	O
slightly	O
diﬀerent	O
from	O
those	O
we	O
have	O
seen	O
earlier	O
.	O
we	O
simply	O
remove	O
all	O
γs	O
and	O
replace	O
all	O
rewards	O
by	O
the	O
diﬀerence	O
between	O
the	O
reward	O
and	O
the	O
true	O
average	O
reward	O
:	O
.	O
=	O
eπ	O
[	O
gt|st	O
=	O
s	O
]	O
and	O
qπ	O
(	O
s	O
,	O
a	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:104	O
)	O
r	O
−	O
r	O
(	O
π	O
)	O
+	O
vπ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
,	O
vπ	O
(	O
s	O
)	O
=	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
a|s	O
)	O
(	O
cid:88	O
)	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:104	O
)	O
r	O
−	O
r	O
(	O
π	O
)	O
+	O
(	O
cid:88	O
)	O
a	O
(	O
cid:48	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
=	O
(	O
cid:88	O
)	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:104	O
)	O
r	O
−	O
max	O
a	O
(	O
cid:88	O
)	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:104	O
)	O
r	O
−	O
max	O
q∗	O
(	O
s	O
,	O
a	O
)	O
=	O
(	O
cid:88	O
)	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
v∗	O
(	O
s	O
)	O
=	O
max	O
π	O
π	O
(	O
a	O
(	O
cid:48	O
)	O
|s	O
(	O
cid:48	O
)	O
)	O
qπ	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
,	O
r	O
(	O
π	O
)	O
+	O
v∗	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
,	O
and	O
q∗	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
a	O
(	O
cid:48	O
)	O
r	O
(	O
π	O
)	O
+	O
max	O
π	O
(	O
cf	O
.	O
(	O
3.14	O
)	O
,	O
exercise	O
3.13	O
,	O
(	O
3.19	O
)	O
,	O
and	O
(	O
3.20	O
)	O
)	O
.	O
there	O
is	O
also	O
a	O
diﬀerential	B
form	O
of	O
the	O
two	O
td	O
errors	O
:	O
δt	O
and	O
δt	O
.	O
=	O
rt+1−	O
¯rt+1	O
+	O
ˆv	O
(	O
st+1	O
,	O
wt	O
)	O
−	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
,	O
.	O
=	O
rt+1−	O
¯rt+1	O
+	O
ˆq	O
(	O
st+1	O
,	O
at+1	O
,	O
wt	O
)	O
−	O
ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
,	O
(	O
10.10	O
)	O
(	O
10.11	O
)	O
where	O
¯rt	O
is	O
an	O
estimate	O
at	O
time	O
t	O
of	O
the	O
average	O
reward	O
r	O
(	O
π	O
)	O
.	O
with	O
these	O
alternate	O
deﬁnitions	O
,	O
most	O
of	O
our	O
algorithms	O
and	O
many	O
theoretical	O
results	O
carry	O
through	O
to	O
the	O
average-reward	O
setting	O
without	O
change	O
.	O
for	O
example	O
,	O
the	O
average	O
reward	O
version	O
of	O
semi-gradient	O
sarsa	O
is	O
deﬁned	O
just	O
as	O
in	O
(	O
10.2	O
)	O
except	O
with	O
the	O
diﬀerential	B
version	O
of	O
the	O
td	O
error	O
.	O
that	O
is	O
,	O
by	O
wt+1	O
.	O
=	O
wt	O
+	O
αδt∇ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
,	O
(	O
10.12	O
)	O
with	O
δt	O
given	O
by	O
(	O
10.11	O
)	O
.	O
the	O
pseudocode	O
for	O
the	O
complete	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
.	O
exercise	O
10.4	O
give	O
pseudocode	O
for	O
a	O
diﬀerential	O
version	O
of	O
semi-gradient	O
q-learning	O
.	O
(	O
cid:3	O
)	O
exercise	O
10.5	O
what	O
equations	O
are	O
needed	O
(	O
beyond	O
10.10	O
)	O
to	O
specify	O
the	O
diﬀerential	B
(	O
cid:3	O
)	O
version	O
of	O
td	O
(	O
0	O
)	O
?	O
10.3.	O
average	O
reward	O
:	O
a	O
new	O
problem	O
setting	O
for	O
continuing	O
tasks	O
251	O
diﬀerential	B
semi-gradient	O
sarsa	O
for	O
estimating	O
ˆq	O
≈	O
q∗	O
input	O
:	O
a	O
diﬀerentiable	O
action-value	B
function	I
parameterization	O
ˆq	O
:	O
s	O
×	O
a	O
×	O
rd	O
→	O
r	O
algorithm	O
parameters	O
:	O
step	O
sizes	O
α	O
,	O
β	O
>	O
0	O
initialize	O
value-function	O
weights	O
w	O
∈	O
rd	O
arbitrarily	O
(	O
e.g.	O
,	O
w	O
=	O
0	O
)	O
initialize	O
average	O
reward	O
estimate	O
¯r	O
∈	O
r	O
arbitrarily	O
(	O
e.g.	O
,	O
¯r	O
=	O
0	O
)	O
initialize	O
state	B
s	O
,	O
and	O
action	O
a	O
loop	O
for	O
each	O
step	O
:	O
take	O
action	B
a	O
,	O
observe	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
choose	O
a	O
(	O
cid:48	O
)	O
as	O
a	O
function	O
of	O
ˆq	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
·	O
,	O
w	O
)	O
(	O
e.g.	O
,	O
ε-greedy	O
)	O
δ	O
←	O
r	O
−	O
¯r	O
+	O
ˆq	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
(	O
cid:48	O
)	O
,	O
w	O
)	O
−	O
ˆq	O
(	O
s	O
,	O
a	O
,	O
w	O
)	O
¯r	O
←	O
¯r	O
+	O
βδ	O
w	O
←	O
w	O
+	O
αδ∇ˆq	O
(	O
s	O
,	O
a	O
,	O
w	O
)	O
s	O
←	O
s	O
(	O
cid:48	O
)	O
a	O
←	O
a	O
(	O
cid:48	O
)	O
example	O
10.2	O
:	O
an	O
access-control	O
queuing	O
task	O
this	O
is	O
a	O
decision	O
task	O
involving	O
access	O
control	B
to	O
a	O
set	O
of	O
k	O
servers	O
.	O
customers	O
of	O
four	O
diﬀerent	O
priorities	O
arrive	O
at	O
a	O
single	O
queue	O
.	O
if	O
given	O
access	O
to	O
a	O
server	O
,	O
the	O
customers	O
pay	O
a	O
reward	O
of	O
1	O
,	O
2	O
,	O
4	O
,	O
or	O
8	O
to	O
the	O
server	O
,	O
depending	O
on	O
their	O
priority	O
,	O
with	O
higher	O
priority	O
customers	O
paying	O
more	O
.	O
in	O
each	O
time	O
step	O
,	O
the	O
customer	O
at	O
the	O
head	O
of	O
the	O
queue	O
is	O
either	O
accepted	O
(	O
assigned	O
to	O
one	O
of	O
the	O
servers	O
)	O
or	O
rejected	O
(	O
removed	O
from	O
the	O
queue	O
,	O
with	O
a	O
reward	O
of	O
zero	O
)	O
.	O
in	O
either	O
case	O
,	O
on	O
the	O
next	O
time	O
step	O
the	O
next	O
customer	O
in	O
the	O
queue	O
is	O
considered	O
.	O
the	O
queue	O
never	O
empties	O
,	O
and	O
the	O
priorities	O
of	O
the	O
customers	O
in	O
the	O
queue	O
are	O
equally	O
randomly	O
distributed	O
.	O
of	O
course	O
a	O
customer	O
can	O
not	O
be	O
served	O
if	O
there	O
is	O
no	O
free	O
server	O
;	O
the	O
customer	O
is	O
always	O
rejected	O
in	O
this	O
case	O
.	O
each	O
busy	O
server	O
becomes	O
free	O
with	O
probability	O
p	O
on	O
each	O
time	O
step	O
.	O
although	O
we	O
have	O
just	O
described	O
them	O
for	O
deﬁniteness	O
,	O
let	O
us	O
assume	O
the	O
statistics	O
of	O
arrivals	O
and	O
departures	O
are	O
unknown	O
.	O
the	O
task	O
is	O
to	O
decide	O
on	O
each	O
step	O
whether	O
to	O
accept	O
or	O
reject	O
the	O
next	O
customer	O
,	O
on	O
the	O
basis	O
of	O
his	O
priority	O
and	O
the	O
number	O
of	O
free	O
servers	O
,	O
so	O
as	O
to	O
maximize	O
long-term	O
reward	O
without	O
discounting	B
.	O
in	O
this	O
example	O
we	O
consider	O
a	O
tabular	O
solution	O
to	O
this	O
problem	O
.	O
although	O
there	O
is	O
no	O
generalization	O
between	O
states	O
,	O
we	O
can	O
still	O
consider	O
it	O
in	O
the	O
general	O
function	O
approxima-	O
tion	B
setting	O
as	O
this	O
setting	O
generalizes	O
the	O
tabular	O
setting	O
.	O
thus	O
we	O
have	O
a	O
diﬀerential	B
action-value	O
estimate	O
for	O
each	O
pair	O
of	O
state	O
(	O
number	O
of	O
free	O
servers	O
and	O
priority	O
of	O
the	O
customer	O
at	O
the	O
head	O
of	O
the	O
queue	O
)	O
and	O
action	O
(	O
accept	O
or	O
reject	O
)	O
.	O
figure	O
10.5	O
shows	O
the	O
solution	O
found	O
by	O
diﬀerential	B
semi-gradient	O
sarsa	O
for	O
this	O
task	O
with	O
k	O
=	O
10	O
and	O
p	O
=	O
0.06.	O
the	O
algorithm	O
parameters	O
were	O
α	O
=	O
0.01	O
,	O
β	O
=	O
0.01	O
,	O
and	O
ε	O
=	O
0.1.	O
the	O
initial	O
action	B
values	O
and	O
¯r	O
were	O
zero	O
.	O
252	O
chapter	O
10	O
:	O
on-policy	O
control	O
with	B
approximation	I
figure	O
10.5	O
:	O
the	O
policy	B
and	O
value	B
function	I
found	O
by	O
diﬀerential	B
semi-gradient	O
one-step	O
sarsa	O
on	O
the	O
access-control	O
queuing	O
task	O
after	O
2	O
million	O
steps	O
.	O
the	O
drop	O
on	O
the	O
right	O
of	O
the	O
graph	O
is	O
probably	O
due	O
to	O
insuﬃcient	O
data	O
;	O
many	O
of	O
these	O
states	O
were	O
never	O
experienced	O
.	O
the	O
value	B
learned	O
for	O
¯r	O
was	O
about	O
2.31.	O
exercise	O
10.6	O
consider	O
an	O
markov	O
reward	O
process	O
consisting	O
of	O
a	O
ring	O
of	O
three	O
states	O
a	O
,	O
b	O
,	O
and	O
c	O
,	O
with	O
state	O
transitions	O
going	O
deterministically	O
around	O
the	O
ring	O
.	O
a	O
reward	O
of	O
+1	O
is	O
received	O
upon	O
arrival	O
in	O
a	O
and	O
otherwise	O
the	O
reward	O
is	O
0.	O
what	O
are	O
the	O
diﬀerential	B
(	O
cid:3	O
)	O
values	O
of	O
the	O
three	O
states	O
?	O
exercise	O
10.7	O
the	O
pseudocode	O
in	O
the	O
box	O
on	O
page	O
251	O
updates	O
¯rt+1	O
using	O
δt	O
as	O
an	O
error	O
rather	O
than	O
simply	O
rt+1	O
−	O
¯rt+1	O
.	O
both	O
errors	O
work	O
,	O
but	O
using	O
δt	O
is	O
better	O
.	O
to	O
see	O
why	O
,	O
consider	O
the	O
ring	O
mrp	O
of	O
three	O
states	O
from	O
the	O
previous	O
exercise	O
.	O
the	O
estimate	O
of	O
the	O
average	O
reward	O
should	O
tend	O
towards	O
its	O
true	O
value	O
of	O
1	O
3	O
.	O
suppose	O
it	O
was	O
already	O
there	O
and	O
was	O
held	O
stuck	O
there	O
.	O
what	O
would	O
the	O
sequence	O
of	O
rt	O
−	O
¯rt	O
errors	O
be	O
?	O
what	O
would	O
the	O
sequence	O
of	O
δt	O
errors	O
be	O
(	O
using	O
(	O
10.10	O
)	O
)	O
?	O
which	O
error	O
sequence	O
would	O
produce	O
a	O
more	O
stable	O
estimate	O
of	O
the	O
average	O
reward	O
if	O
the	O
estimate	O
were	O
allowed	O
to	O
change	O
in	O
response	O
(	O
cid:3	O
)	O
to	O
the	O
errors	O
?	O
why	O
?	O
exercise	O
10.8	O
suppose	O
there	O
is	O
an	O
mdp	O
that	O
under	O
any	O
policy	B
produces	O
the	O
determin-	O
istic	O
sequence	O
of	O
rewards	O
+1	O
,	O
0	O
,	O
+1	O
,	O
0	O
,	O
+1	O
,	O
0	O
,	O
.	O
.	O
.	O
going	O
on	O
forever	O
.	O
technically	O
,	O
this	O
is	O
not	O
allowed	O
because	O
it	O
violates	O
ergodicity	O
;	O
there	O
is	O
no	O
stationary	O
limiting	O
distribution	O
µπ	O
and	O
the	O
limit	O
(	O
10.7	O
)	O
does	O
not	O
exist	O
.	O
nevertheless	O
,	O
the	O
average	O
reward	O
(	O
10.6	O
)	O
is	O
well	O
deﬁned	O
;	O
what	O
is	O
it	O
?	O
now	O
consider	O
two	O
states	O
in	O
this	O
mdp	O
.	O
from	O
a	O
,	O
the	O
reward	O
sequence	O
is	O
exactly	O
as	O
described	O
above	O
,	O
starting	O
with	O
a	O
+1	O
,	O
whereas	O
,	O
from	O
b	O
,	O
the	O
reward	O
sequence	O
starts	O
with	O
-10-50100diﬀerentialvalue	O
of	O
best	O
actionnumber	O
of	O
free	O
servers123456789100	O
!	O
15	O
!	O
10	O
!	O
5057priority	O
8priority	O
4priority	O
2priority	O
1number	O
of	O
free	O
servers428acceptreject12345678910number	O
of	O
free	O
serverspriority1policyvalue	O
ofbest	O
actionvaluefunction512345678910priority	O
8priority	O
4priority	O
2priority	O
1policyvaluefunction	O
10.4.	O
deprecating	O
the	O
discounted	O
setting	O
253	O
a	O
0	O
and	O
then	O
continues	O
with	O
+1	O
,	O
0	O
,	O
+1	O
,	O
0	O
,	O
.	O
.	O
..	O
the	O
diﬀerential	B
return	O
(	O
10.9	O
)	O
is	O
not	O
well	O
deﬁned	O
for	O
this	O
case	O
as	O
the	O
limit	O
does	O
not	O
exist	O
.	O
to	O
repair	O
this	O
,	O
one	O
could	O
alternately	O
deﬁne	O
the	O
value	B
of	O
a	O
state	B
as	O
vπ	O
(	O
s	O
)	O
.	O
=	O
lim	O
h→∞	O
under	O
this	O
deﬁnition	O
,	O
what	O
are	O
the	O
values	O
of	O
states	O
a	O
and	O
b	O
?	O
h	O
(	O
cid:88	O
)	O
t=0	O
γt	O
(	O
cid:16	O
)	O
e	O
[	O
rt+1|s0	O
=	O
s	O
]	O
−	O
r	O
(	O
π	O
)	O
(	O
cid:17	O
)	O
.	O
(	O
10.13	O
)	O
(	O
cid:3	O
)	O
10.4	O
deprecating	O
the	O
discounted	O
setting	O
the	O
continuing	O
,	O
discounted	O
problem	O
formulation	O
has	O
been	O
very	O
useful	O
in	O
the	O
tabular	O
case	O
,	O
in	O
which	O
the	O
returns	O
from	O
each	O
state	B
can	O
be	O
separately	O
identiﬁed	O
and	O
averaged	O
.	O
but	O
in	O
the	O
approximate	O
case	O
it	O
is	O
questionable	O
whether	O
one	O
should	O
ever	O
use	O
this	O
problem	O
formulation	O
.	O
to	O
see	O
why	O
,	O
consider	O
an	O
inﬁnite	O
sequence	O
of	O
returns	O
with	O
no	O
beginning	O
or	O
end	O
,	O
and	O
no	O
clearly	O
identiﬁed	O
states	O
.	O
the	O
states	O
might	O
be	O
represented	O
only	O
by	O
feature	O
vectors	O
,	O
which	O
may	O
do	O
little	O
to	O
distinguish	O
the	O
states	O
from	O
each	O
other	O
.	O
as	O
a	O
special	O
case	O
,	O
all	O
of	O
the	O
feature	O
vectors	O
may	O
be	O
the	O
same	O
.	O
thus	O
one	O
really	O
has	O
only	O
the	O
reward	O
sequence	O
(	O
and	O
the	O
actions	O
)	O
,	O
and	O
performance	O
has	O
to	O
be	O
assessed	O
purely	O
from	O
these	O
.	O
how	O
could	O
it	O
be	O
done	O
?	O
one	O
way	O
is	O
by	O
averaging	O
the	O
rewards	O
over	O
a	O
long	O
interval—this	O
is	O
the	O
idea	O
of	O
the	O
average-reward	O
setting	O
.	O
how	O
could	O
discounting	B
be	O
used	O
?	O
well	O
,	O
for	O
each	O
time	O
step	O
we	O
could	O
measure	O
the	O
discounted	O
return	B
.	O
some	O
returns	O
would	O
be	O
small	O
and	O
some	O
big	O
,	O
so	O
again	O
we	O
would	O
have	O
to	O
average	O
them	O
over	O
a	O
suﬃciently	O
large	O
time	O
interval	O
.	O
in	O
the	O
continuing	O
setting	O
there	O
are	O
no	O
starts	O
and	O
ends	O
,	O
and	O
no	O
special	O
time	O
steps	O
,	O
so	O
there	O
is	O
nothing	O
else	O
that	O
could	O
be	O
done	O
.	O
however	O
,	O
if	O
you	O
do	O
this	O
,	O
it	O
turns	O
out	O
that	O
the	O
average	O
of	O
the	O
discounted	O
returns	O
is	O
proportional	O
to	O
the	O
average	O
reward	O
.	O
in	O
fact	O
,	O
for	O
policy	O
π	O
,	O
the	O
average	O
of	O
the	O
discounted	O
returns	O
is	O
always	O
r	O
(	O
π	O
)	O
/	O
(	O
1	O
−	O
γ	O
)	O
,	O
that	O
is	O
,	O
it	O
is	O
essentially	O
the	O
average	O
reward	O
,	O
r	O
(	O
π	O
)	O
.	O
in	O
particular	O
,	O
the	O
ordering	O
of	O
all	O
policies	O
in	O
the	O
average	O
discounted	O
return	B
setting	O
would	O
be	O
exactly	O
the	O
same	O
as	O
in	O
the	O
average-reward	O
setting	O
.	O
the	O
discount	O
rate	O
γ	O
thus	O
has	O
no	O
eﬀect	O
on	O
the	O
problem	O
formulation	O
.	O
it	O
could	O
in	O
fact	O
be	O
zero	O
and	O
the	O
ranking	O
would	O
be	O
unchanged	O
.	O
this	O
surprising	O
fact	O
is	O
proven	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
,	O
but	O
the	O
basic	O
idea	O
can	O
be	O
seen	O
via	O
a	O
symmetry	O
argument	O
.	O
each	O
time	O
step	O
is	O
exactly	O
the	O
same	O
as	O
every	O
other	O
.	O
with	O
discounting	O
,	O
every	O
reward	O
will	O
appear	O
exactly	O
once	O
in	O
each	O
position	O
in	O
some	O
return	B
.	O
the	O
tth	O
reward	O
will	O
appear	O
undiscounted	O
in	O
the	O
t−	O
1st	O
return	B
,	O
discounted	O
once	O
in	O
the	O
t−	O
2nd	O
return	B
,	O
and	O
discounted	O
999	O
times	O
in	O
the	O
t−	O
1000th	O
return	B
.	O
the	O
weight	O
on	O
the	O
tth	O
reward	O
is	O
thus	O
1	O
+	O
γ	O
+	O
γ2	O
+	O
γ3	O
+	O
···	O
=	O
1/	O
(	O
1	O
−	O
γ	O
)	O
.	O
because	O
all	O
states	O
are	O
the	O
same	O
,	O
they	O
are	O
all	O
weighted	O
by	O
this	O
,	O
and	O
thus	O
the	O
average	O
of	O
the	O
returns	O
will	O
be	O
this	O
times	O
the	O
average	O
reward	O
,	O
or	O
r	O
(	O
π	O
)	O
/	O
(	O
1	O
−	O
γ	O
)	O
.	O
this	O
example	O
and	O
the	O
more	O
general	O
argument	O
in	O
the	O
box	O
show	O
that	O
if	O
we	O
optimized	O
discounted	O
value	B
over	O
the	O
on-policy	B
distribution	I
,	O
then	O
the	O
eﬀect	O
would	O
be	O
identical	O
to	O
optimizing	O
undiscounted	O
average	O
reward	O
;	O
the	O
actual	O
value	B
of	O
γ	O
would	O
have	O
no	O
eﬀect	O
.	O
this	O
strongly	O
suggests	O
that	O
discounting	B
has	O
no	O
role	O
to	O
play	O
in	O
the	O
deﬁnition	O
of	O
the	O
control	B
prob-	O
lem	O
with	B
function	I
approximation	I
.	O
one	O
can	O
nevertheless	O
go	O
ahead	O
and	O
use	O
discounting	B
in	O
254	O
chapter	O
10	O
:	O
on-policy	O
control	O
with	B
approximation	I
the	O
futility	O
of	O
discounting	O
in	O
continuing	O
problems	O
perhaps	O
discounting	B
can	O
be	O
saved	O
by	O
choosing	O
an	O
objective	O
that	O
sums	O
discounted	O
values	O
over	O
the	O
distribution	O
with	O
which	O
states	O
occur	O
under	O
the	O
policy	B
:	O
(	O
where	O
vγ	O
π	O
is	O
the	O
discounted	O
value	B
function	I
)	O
π	O
(	O
a|s	O
)	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
(	O
cid:88	O
)	O
r	O
µπ	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:88	O
)	O
s	O
π	O
(	O
a|s	O
)	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
(	O
cid:88	O
)	O
r	O
µπ	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
a	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
[	O
r	O
+	O
γvγ	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
γvγ	O
π	O
(	O
a|s	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
a	O
)	O
vγ	O
π	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
]	O
π	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
π	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
µπ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
vγ	O
(	O
bellman	O
eq	O
.	O
)	O
(	O
from	O
(	O
10.7	O
)	O
)	O
(	O
from	O
(	O
3.4	O
)	O
)	O
(	O
from	O
(	O
10.8	O
)	O
)	O
π	O
(	O
s	O
)	O
µπ	O
(	O
s	O
)	O
vγ	O
j	O
(	O
π	O
)	O
=	O
(	O
cid:88	O
)	O
s	O
µπ	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
a	O
=	O
(	O
cid:88	O
)	O
s	O
=	O
r	O
(	O
π	O
)	O
+	O
(	O
cid:88	O
)	O
s	O
=	O
r	O
(	O
π	O
)	O
+	O
γ	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
=	O
r	O
(	O
π	O
)	O
+	O
γ	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
=	O
r	O
(	O
π	O
)	O
+	O
γj	O
(	O
π	O
)	O
=	O
r	O
(	O
π	O
)	O
+	O
γr	O
(	O
π	O
)	O
+	O
γ2j	O
(	O
π	O
)	O
=	O
r	O
(	O
π	O
)	O
+	O
γr	O
(	O
π	O
)	O
+	O
γ2r	O
(	O
π	O
)	O
+	O
γ3r	O
(	O
π	O
)	O
+	O
···	O
=	O
r	O
(	O
π	O
)	O
.	O
1	O
1	O
−	O
γ	O
the	O
proposed	O
discounted	O
objective	O
orders	O
policies	O
identically	O
to	O
the	O
undiscounted	O
(	O
average	O
reward	O
)	O
objective	O
.	O
the	O
discount	O
rate	O
γ	O
does	O
not	O
inﬂuence	O
the	O
ordering	O
!	O
solution	O
methods	O
.	O
the	O
discounting	B
parameter	O
γ	O
changes	O
from	O
a	O
problem	O
parameter	O
to	O
a	O
solution	O
method	O
parameter	O
!	O
however	O
,	O
in	O
this	O
case	O
we	O
unfortunately	O
would	O
not	O
be	O
guar-	O
anteed	O
to	O
optimize	O
average	O
reward	O
(	O
or	O
the	O
equivalent	O
discounted	O
value	B
over	O
the	O
on-policy	B
distribution	I
)	O
.	O
the	O
root	O
cause	O
of	O
the	O
diﬃculties	O
with	O
the	O
discounted	O
control	B
setting	O
is	O
that	O
with	B
function	I
approximation	I
we	O
have	O
lost	O
the	O
policy	B
improvement	I
theorem	O
(	O
section	O
4.2	O
)	O
.	O
it	O
is	O
no	O
longer	O
true	O
that	O
if	O
we	O
change	O
the	O
policy	B
to	O
improve	O
the	O
discounted	O
value	B
of	O
one	O
state	B
then	O
we	O
are	O
guaranteed	O
to	O
have	O
improved	O
the	O
overall	O
policy	B
in	O
any	O
useful	O
sense	O
.	O
that	O
guarantee	O
was	O
key	O
to	O
the	O
theory	O
of	O
our	O
reinforcement	B
learning	I
control	O
methods	O
.	O
with	B
function	I
approximation	I
we	O
have	O
lost	O
it	O
!	O
in	O
fact	O
,	O
the	O
lack	O
of	O
a	O
policy	B
improvement	I
theorem	O
is	O
also	O
a	O
theoretical	O
lacuna	O
for	O
the	O
total-episodic	O
and	O
average-reward	O
settings	O
.	O
once	O
we	O
introduce	O
function	B
approximation	I
we	O
can	O
no	O
longer	O
guarantee	O
improvement	O
for	O
any	O
setting	O
.	O
in	O
chapter	O
13	O
we	O
introduce	O
an	O
alternative	O
class	O
of	O
reinforcement	O
learning	O
algorithms	O
based	O
on	O
parameterized	O
policies	O
,	O
and	O
there	O
we	O
have	O
a	O
theoretical	O
guarantee	O
called	O
the	O
“	O
policy-gradient	O
theorem	B
”	O
which	O
plays	O
a	O
similar	O
role	O
as	O
the	O
policy	B
improvement	I
theorem	O
.	O
but	O
for	O
methods	O
that	O
learn	O
action	B
values	O
we	O
seem	O
to	O
be	O
currently	O
without	O
a	O
local	O
improvement	O
guarantee	O
(	O
possibly	O
10.5.	O
diﬀerential	B
semi-gradient	O
n-step	B
sarsa	O
255	O
the	O
approach	O
taken	O
by	O
perkins	O
and	O
precup	O
(	O
2003	O
)	O
may	O
provide	O
a	O
part	O
of	O
the	O
answer	O
)	O
.	O
we	O
do	O
know	O
that	O
ε-greediﬁcation	O
may	O
sometimes	O
result	O
in	O
an	O
inferior	O
policy	B
,	O
as	O
policies	O
may	O
chatter	O
among	O
good	O
policies	O
rather	O
than	O
converge	O
(	O
gordon	O
,	O
1996a	O
)	O
.	O
this	O
is	O
an	O
area	O
with	O
multiple	O
open	O
theoretical	O
questions	O
.	O
10.5	O
diﬀerential	B
semi-gradient	O
n-step	B
sarsa	O
in	O
order	O
to	O
generalize	O
to	O
n-step	B
bootstrapping	O
,	O
we	O
need	O
an	O
n-step	B
version	O
of	O
the	O
td	O
error	O
.	O
we	O
begin	O
by	O
generalizing	O
the	O
n-step	B
return	O
(	O
7.4	O
)	O
to	O
its	O
diﬀerential	B
form	O
,	O
with	B
function	I
approximation	I
:	O
gt	O
:	O
t+n	O
.	O
=	O
rt+1−	O
¯rt+1	O
+	O
rt+2−	O
¯rt+2	O
+	O
···+rt+n−	O
¯rt+n	O
+	O
ˆq	O
(	O
st+n	O
,	O
at+n	O
,	O
wt+n−1	O
)	O
,	O
(	O
10.14	O
)	O
where	O
¯r	O
is	O
an	O
estimate	O
of	O
r	O
(	O
π	O
)	O
,	O
n	O
≥	O
1	O
,	O
and	O
t	O
+	O
n	O
<	O
t	O
.	O
if	O
t	O
+	O
n	O
≥	O
t	O
,	O
then	O
we	O
deﬁne	O
gt	O
:	O
t+n	O
.	O
=	O
gt	O
as	O
usual	O
.	O
the	O
n-step	B
td	O
error	O
is	O
then	O
δt	O
.	O
=	O
gt	O
:	O
t+n	O
−	O
ˆq	O
(	O
st	O
,	O
at	O
,	O
w	O
)	O
,	O
(	O
10.15	O
)	O
after	O
which	O
we	O
can	O
apply	O
our	O
usual	O
semi-gradient	O
sarsa	O
update	O
(	O
10.12	O
)	O
.	O
pseudocode	O
for	O
the	O
complete	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
.	O
diﬀerential	B
semi-gradient	O
n-step	B
sarsa	O
for	O
estimating	O
ˆq	O
≈	O
qπ	O
or	O
q∗	O
input	O
:	O
a	O
diﬀerentiable	O
function	O
ˆq	O
:	O
s	O
×	O
a	O
×	O
rd	O
→	O
r	O
,	O
a	O
policy	B
π	O
initialize	O
value-function	O
weights	O
w	O
∈	O
rd	O
arbitrarily	O
(	O
e.g.	O
,	O
w	O
=	O
0	O
)	O
initialize	O
average-reward	O
estimate	O
¯r	O
∈	O
r	O
arbitrarily	O
(	O
e.g.	O
,	O
¯r	O
=	O
0	O
)	O
algorithm	O
parameters	O
:	O
step	O
size	O
α	O
,	O
β	O
>	O
0	O
,	O
a	O
positive	O
integer	O
n	O
all	O
store	O
and	O
access	O
operations	O
(	O
st	O
,	O
at	O
,	O
and	O
rt	O
)	O
can	O
take	O
their	O
index	O
mod	O
n	O
+	O
1	O
initialize	O
and	O
store	O
s0	O
and	O
a0	O
loop	O
for	O
each	O
step	O
,	O
t	O
=	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
:	O
take	O
action	B
at	O
observe	O
and	O
store	O
the	O
next	O
reward	O
as	O
rt+1	O
and	O
the	O
next	O
state	B
as	O
st+1	O
select	O
and	O
store	O
an	O
action	B
at+1	O
∼	O
π	O
(	O
·|st+1	O
)	O
,	O
or	O
ε-greedy	O
wrt	O
ˆq	O
(	O
s0	O
,	O
·	O
,	O
w	O
)	O
τ	O
←	O
t	O
−	O
n	O
+	O
1	O
if	O
τ	O
≥	O
0	O
:	O
(	O
τ	O
is	O
the	O
time	O
whose	O
estimate	O
is	O
being	O
updated	O
)	O
i=τ	O
+1	O
(	O
ri	O
−	O
¯r	O
)	O
+	O
ˆq	O
(	O
sτ	O
+n	O
,	O
aτ	O
+n	O
,	O
w	O
)	O
−	O
ˆq	O
(	O
sτ	O
,	O
aτ	O
,	O
w	O
)	O
δ	O
←	O
(	O
cid:80	O
)	O
τ	O
+n	O
¯r	O
←	O
¯r	O
+	O
βδ	O
w	O
←	O
w	O
+	O
αδ∇ˆq	O
(	O
sτ	O
,	O
aτ	O
,	O
w	O
)	O
exercise	O
10.9	O
in	O
the	O
diﬀerential	O
semi-gradient	O
n-step	O
sarsa	O
algorithm	O
,	O
the	O
step-size	B
parameter	I
on	O
the	O
average	O
reward	O
,	O
β	O
,	O
needs	O
to	O
be	O
quite	O
small	O
so	O
that	O
¯r	O
becomes	O
a	O
good	O
long-term	O
estimate	O
of	O
the	O
average	O
reward	O
.	O
unfortunately	O
,	O
¯r	O
will	O
then	O
be	O
biased	O
by	O
its	O
256	O
chapter	O
10	O
:	O
on-policy	O
control	O
with	B
approximation	I
initial	O
value	B
for	O
many	O
steps	O
,	O
which	O
may	O
make	O
learning	O
ineﬃcient	O
.	O
alternatively	O
,	O
one	O
could	O
use	O
a	O
sample	O
average	O
of	O
the	O
observed	O
rewards	O
for	O
¯r	O
.	O
that	O
would	O
initially	O
adapt	O
rapidly	O
but	O
in	O
the	O
long	O
run	O
would	O
also	O
adapt	O
slowly	O
.	O
as	O
the	O
policy	B
slowly	O
changed	O
,	O
¯r	O
would	O
also	O
change	O
;	O
the	O
potential	O
for	O
such	O
long-term	O
nonstationarity	B
makes	O
sample-average	O
methods	O
ill-suited	O
.	O
in	O
fact	O
,	O
the	O
step-size	B
parameter	I
on	O
the	O
average	O
reward	O
is	O
a	O
perfect	O
place	O
to	O
use	O
the	O
unbiased	O
constant-step-size	O
trick	O
from	O
exercise	O
2.7.	O
describe	O
the	O
speciﬁc	O
changes	O
needed	O
to	O
the	O
boxed	O
algorithm	O
for	O
diﬀerential	O
semi-gradient	O
n-step	O
sarsa	O
to	O
use	O
this	O
trick	O
.	O
(	O
cid:3	O
)	O
10.6	O
summary	O
in	O
this	O
chapter	O
we	O
have	O
extended	O
the	O
ideas	O
of	O
parameterized	O
function	B
approximation	I
and	O
semi-gradient	O
descent	O
,	O
introduced	O
in	O
the	O
previous	O
chapter	O
,	O
to	O
control	B
.	O
the	O
extension	O
is	O
immediate	O
for	O
the	O
episodic	O
case	O
,	O
but	O
for	O
the	O
continuing	O
case	O
we	O
have	O
to	O
introduce	O
a	O
whole	O
new	O
problem	O
formulation	O
based	O
on	O
maximizing	O
the	O
average	B
reward	I
setting	I
per	O
time	O
step	O
.	O
surprisingly	O
,	O
the	O
discounted	O
formulation	O
can	O
not	O
be	O
carried	O
over	O
to	O
control	B
in	O
the	O
presence	O
of	O
approximations	O
.	O
in	O
the	O
approximate	O
case	O
most	O
policies	O
can	O
not	O
be	O
represented	O
by	O
a	O
value	B
function	I
.	O
the	O
arbitrary	O
policies	O
that	O
remain	O
need	O
to	O
be	O
ranked	O
,	O
and	O
the	O
scalar	O
average	O
reward	O
r	O
(	O
π	O
)	O
provides	O
an	O
eﬀective	O
way	O
to	O
do	O
this	O
.	O
the	O
average	O
reward	O
formulation	O
involves	O
new	O
diﬀerential	B
versions	O
of	O
value	O
functions	O
,	O
bellman	O
equations	O
,	O
and	O
td	O
errors	O
,	O
but	O
all	O
of	O
these	O
parallel	O
the	O
old	O
ones	O
,	O
and	O
the	O
con-	O
ceptual	O
changes	O
are	O
small	O
.	O
there	O
is	O
also	O
a	O
new	O
parallel	O
set	O
of	O
diﬀerential	O
algorithms	O
for	O
the	O
average-reward	O
case	O
.	O
bibliographical	O
and	O
historical	O
remarks	O
10.1	O
10.2	O
10.3	O
semi-gradient	O
sarsa	O
with	B
function	I
approximation	I
was	O
ﬁrst	O
explored	O
by	O
rum-	O
mery	O
and	O
niranjan	O
(	O
1994	O
)	O
.	O
linear	O
semi-gradient	O
sarsa	O
with	O
ε-greedy	O
action	B
selection	O
does	O
not	O
converge	O
in	O
the	O
usual	O
sense	O
,	O
but	O
does	O
enter	O
a	O
bounded	O
re-	O
gion	O
near	O
the	O
best	O
solution	O
(	O
gordon	O
,	O
1996a	O
,	O
2001	O
)	O
.	O
precup	O
and	O
perkins	O
(	O
2003	O
)	O
showed	O
convergence	O
in	O
a	O
diﬀerentiable	O
action	B
selection	O
setting	O
.	O
see	O
also	O
perkins	O
and	O
pendrith	O
(	O
2002	O
)	O
and	O
melo	O
,	O
meyn	O
,	O
and	O
ribeiro	O
(	O
2008	O
)	O
.	O
the	O
mountain–car	O
example	O
is	O
based	O
on	O
a	O
similar	O
task	O
studied	O
by	O
moore	O
(	O
1990	O
)	O
,	O
but	O
the	O
exact	O
form	O
used	O
here	O
is	O
from	O
sutton	O
(	O
1996	O
)	O
.	O
episodic	O
n-step	B
semi-gradient	O
sarsa	O
is	O
based	O
on	O
the	O
forward	O
sarsa	O
(	O
λ	O
)	O
algorithm	O
of	O
van	O
seijen	O
(	O
2016	O
)	O
.	O
the	O
empirical	O
results	O
shown	O
here	O
are	O
new	O
to	O
the	O
second	O
edition	O
of	O
this	O
text	O
.	O
the	O
average-reward	O
formulation	O
has	O
been	O
described	O
for	B
dynamic	I
programming	I
(	O
e.g.	O
,	O
puterman	O
,	O
1994	O
)	O
and	O
from	O
the	O
point	O
of	O
view	O
of	O
reinforcement	O
learning	O
(	O
ma-	O
hadevan	O
,	O
1996	O
;	O
tadepalli	O
and	O
ok	O
,	O
1994	O
;	O
bertsekas	O
and	O
tsitiklis	O
,	O
1996	O
;	O
tsitsiklis	O
and	O
van	O
roy	O
,	O
1999	O
)	O
.	O
the	O
algorithm	O
described	O
here	O
is	O
the	O
on-policy	O
analog	O
of	O
the	O
“	O
r-learning	O
”	O
algorithm	O
introduced	O
by	O
schwartz	O
(	O
1993	O
)	O
.	O
the	O
name	O
r-learning	O
10.6.	O
summary	O
257	O
was	O
probably	O
meant	O
to	O
be	O
the	O
alphabetic	O
successor	O
to	O
q-learning	O
,	O
but	O
we	O
prefer	O
to	O
think	O
of	O
it	O
as	O
a	O
reference	O
to	O
the	O
learning	O
of	O
diﬀerential	B
or	O
relative	O
values	O
.	O
the	O
access-control	B
queuing	I
example	I
was	O
suggested	O
by	O
the	O
work	O
of	O
carlstr¨om	O
and	O
nordstr¨om	O
(	O
1997	O
)	O
.	O
10.4	O
the	O
recognition	O
of	O
the	O
limitations	O
of	O
discounting	O
as	O
a	O
formulation	O
of	O
the	O
rein-	O
forcement	O
learning	O
problem	O
with	B
function	I
approximation	I
became	O
apparent	O
to	O
the	O
authors	O
shortly	O
after	O
the	O
publication	O
of	O
the	O
ﬁrst	O
edition	O
of	O
this	O
text	O
.	O
singh	O
,	O
jaakkola	O
,	O
and	O
jordan	O
(	O
1994	O
)	O
may	O
have	O
been	O
the	O
ﬁrst	O
to	O
observe	O
it	O
in	O
print	O
.	O
10.5	O
the	O
diﬀerential	B
version	O
of	O
n-step	O
semi-gradient	O
sarsa	O
is	O
new	O
to	O
this	O
text	O
and	O
has	O
not	O
been	O
signiﬁcantly	O
studied	O
.	O
chapter	O
11	O
*oﬀ-policy	O
methods	O
with	B
approximation	I
this	O
book	O
has	O
treated	O
on-policy	O
and	O
oﬀ-policy	B
learning	O
methods	O
since	O
chapter	O
5	O
primar-	O
ily	O
as	O
two	O
alternative	O
ways	O
of	O
handling	O
the	O
conﬂict	O
between	O
exploitation	O
and	O
exploration	O
inherent	B
in	O
learning	O
forms	O
of	O
generalized	O
policy	B
iteration	I
.	O
the	O
two	O
chapters	O
preceding	O
this	O
have	O
treated	O
the	O
on-policy	O
case	O
with	B
function	I
approximation	I
,	O
and	O
in	O
this	O
chapter	O
we	O
treat	O
the	O
oﬀ	O
-policy	O
case	O
with	B
function	I
approximation	I
.	O
the	O
extension	O
to	O
function	B
approximation	I
turns	O
out	O
to	O
be	O
signiﬁcantly	O
diﬀerent	O
and	O
harder	O
for	O
oﬀ-policy	O
learning	O
than	O
it	O
is	O
for	O
on-policy	O
learning	O
.	O
the	O
tabular	O
oﬀ-policy	O
methods	O
developed	O
in	O
chap-	O
ters	O
6	O
and	O
7	O
readily	O
extend	O
to	O
semi-gradient	O
algorithms	O
,	O
but	O
these	O
algorithms	O
do	O
not	O
converge	O
as	O
robustly	O
as	O
they	O
do	O
under	O
on-policy	O
training	O
.	O
in	O
this	O
chapter	O
we	O
explore	O
the	O
convergence	O
problems	O
,	O
take	O
a	O
closer	O
look	O
at	O
the	O
theory	O
of	O
linear	O
function	B
approximation	I
,	O
introduce	O
a	O
notion	O
of	O
learnability	O
,	O
and	O
then	O
discuss	O
new	O
algorithms	O
with	O
stronger	O
con-	O
vergence	O
guarantees	O
for	O
the	O
oﬀ-policy	B
case	O
.	O
in	O
the	O
end	O
we	O
will	O
have	O
improved	O
methods	O
,	O
but	O
the	O
theoretical	O
results	O
will	O
not	O
be	O
as	O
strong	O
,	O
nor	O
the	O
empirical	O
results	O
as	O
satisfying	O
,	O
as	O
they	O
are	O
for	O
on-policy	O
learning	O
.	O
along	O
the	O
way	O
,	O
we	O
will	O
gain	O
a	O
deeper	O
understanding	O
of	O
approximation	O
in	O
reinforcement	O
learning	O
for	O
on-policy	O
learning	O
as	O
well	O
as	O
oﬀ-policy	O
learning	O
.	O
recall	O
that	O
in	O
oﬀ-policy	O
learning	O
we	O
seek	O
to	O
learn	O
a	O
value	B
function	I
for	O
a	O
target	B
policy	O
π	O
,	O
given	O
data	O
due	O
to	O
a	O
diﬀerent	O
behavior	B
policy	I
b.	O
in	O
the	O
prediction	O
case	O
,	O
both	O
policies	O
are	O
static	O
and	O
given	O
,	O
and	O
we	O
seek	O
to	O
learn	O
either	O
state	B
values	O
ˆv	O
≈	O
vπ	O
or	O
action	B
values	O
ˆq	O
≈	O
qπ	O
.	O
in	O
the	O
control	O
case	O
,	O
action	B
values	O
are	O
learned	O
,	O
and	O
both	O
policies	O
typically	O
change	O
during	O
learning—π	O
being	O
the	O
greedy	O
policy	O
with	O
respect	O
to	O
ˆq	O
,	O
and	O
b	O
being	O
something	O
more	O
exploratory	O
such	O
as	O
the	O
ε-greedy	O
policy	O
with	O
respect	O
to	O
ˆq	O
.	O
the	O
challenge	O
of	O
oﬀ-policy	O
learning	O
can	O
be	O
divided	O
into	O
two	O
parts	O
,	O
one	O
that	O
arises	O
in	O
the	O
tabular	O
case	O
and	O
one	O
that	O
arises	O
only	O
with	B
function	I
approximation	I
.	O
the	O
ﬁrst	O
part	O
of	O
the	O
challenge	O
has	O
to	O
do	O
with	O
the	O
target	B
of	O
the	O
update	O
(	O
not	O
to	O
be	O
confused	O
with	O
the	O
target	B
policy	O
)	O
,	O
and	O
the	O
second	O
part	O
has	O
to	O
do	O
with	O
the	O
distribution	O
of	O
the	O
updates	O
.	O
the	O
techniques	O
related	O
to	O
importance	B
sampling	I
developed	O
in	O
chapters	O
5	O
and	O
7	O
deal	O
with	O
259	O
260	O
chapter	O
11	O
:	O
*oﬀ-policy	O
methods	O
with	B
approximation	I
the	O
ﬁrst	O
part	O
;	O
these	O
may	O
increase	O
variance	O
but	O
are	O
needed	O
in	O
all	O
successful	O
algorithms	O
,	O
tabular	O
and	O
approximate	B
.	O
the	O
extension	O
of	O
these	O
techniques	O
to	O
function	B
approximation	I
are	O
quickly	O
dealt	O
with	O
in	O
the	O
ﬁrst	O
section	O
of	O
this	O
chapter	O
.	O
something	O
more	O
is	O
needed	O
for	O
the	O
second	O
part	O
of	O
the	O
challenge	O
of	O
oﬀ-policy	O
learning	O
with	O
function	B
approximation	I
because	O
the	O
distribution	O
of	O
updates	O
in	O
the	O
oﬀ-policy	O
case	O
is	O
not	O
according	O
to	O
the	O
on-policy	B
distribution	I
.	O
the	O
on-policy	B
distribution	I
is	O
important	O
to	O
the	O
stability	O
of	O
semi-gradient	O
methods	O
.	O
two	O
general	O
approaches	O
have	O
been	O
explored	O
to	O
deal	O
with	O
this	O
.	O
one	O
is	O
to	O
use	O
importance	B
sampling	I
methods	O
again	O
,	O
this	O
time	O
to	O
warp	O
the	O
update	O
distribution	O
back	O
to	O
the	O
on-policy	B
distribution	I
,	O
so	O
that	O
semi-gradient	B
methods	I
are	O
guaranteed	O
to	O
converge	O
(	O
in	O
the	O
linear	O
case	O
)	O
.	O
the	O
other	O
is	O
to	O
develop	O
true	O
gradient	O
methods	O
that	O
do	O
not	O
rely	O
on	O
any	O
special	O
distribution	O
for	O
stability	O
.	O
we	O
present	O
methods	O
based	O
on	O
both	O
approaches	O
.	O
this	O
is	O
a	O
cutting-edge	O
research	O
area	O
,	O
and	O
it	O
is	O
not	O
clear	O
which	O
of	O
these	O
approaches	O
is	O
most	O
eﬀective	O
in	O
practice	O
.	O
11.1	O
semi-gradient	B
methods	I
we	O
begin	O
by	O
describing	O
how	O
the	O
methods	O
developed	O
in	O
earlier	O
chapters	O
for	O
the	O
oﬀ-policy	B
case	O
extend	O
readily	O
to	O
function	B
approximation	I
as	O
semi-gradient	B
methods	I
.	O
these	O
methods	O
address	O
the	O
ﬁrst	O
part	O
of	O
the	O
challenge	O
of	O
oﬀ-policy	O
learning	O
(	O
changing	O
the	O
update	O
targets	O
)	O
but	O
not	O
the	O
second	O
part	O
(	O
changing	O
the	O
update	O
distribution	O
)	O
.	O
accordingly	O
,	O
these	O
methods	O
may	O
diverge	O
in	O
some	O
cases	O
,	O
and	O
in	O
that	O
sense	O
are	O
not	O
sound	O
,	O
but	O
still	O
they	O
are	O
often	O
suc-	O
cessfully	O
used	O
.	O
remember	O
that	O
these	O
methods	O
are	O
guaranteed	O
stable	O
and	O
asymptotically	O
unbiased	O
for	O
the	O
tabular	O
case	O
,	O
which	O
corresponds	O
to	O
a	O
special	O
case	O
of	O
function	O
approxima-	O
tion	B
.	O
so	O
it	O
may	O
still	O
be	O
possible	O
to	O
combine	O
them	O
with	O
feature	O
selection	O
methods	O
in	O
such	O
a	O
way	O
that	O
the	O
combined	O
system	O
could	O
be	O
assured	O
stable	O
.	O
in	O
any	O
event	O
,	O
these	O
methods	O
are	O
simple	O
and	O
thus	O
a	O
good	O
place	O
to	O
start	O
.	O
in	O
chapter	O
7	O
we	O
described	O
a	O
variety	O
of	O
tabular	O
oﬀ-policy	B
algorithms	O
.	O
to	O
convert	O
them	O
to	O
semi-gradient	O
form	O
,	O
we	O
simply	O
replace	O
the	O
update	O
to	O
an	O
array	O
(	O
v	O
or	O
q	O
)	O
to	O
an	O
update	O
to	O
a	O
weight	O
vector	B
(	O
w	O
)	O
,	O
using	O
the	O
approximate	B
value	O
function	O
(	O
ˆv	O
or	O
ˆq	O
)	O
and	O
its	O
gradient	B
.	O
many	O
of	O
these	O
algorithms	O
use	O
the	O
per-step	O
importance	B
sampling	I
ratio	O
:	O
.	O
(	O
11.1	O
)	O
.	O
=	O
ρt	O
:	O
t	O
=	O
ρt	O
π	O
(	O
at|st	O
)	O
b	O
(	O
at|st	O
)	O
for	O
example	O
,	O
the	O
one-step	O
,	O
state-value	O
algorithm	O
is	O
semi-gradient	O
oﬀ-policy	O
td	O
(	O
0	O
)	O
,	O
which	O
is	O
just	O
like	O
the	O
corresponding	O
on-policy	O
algorithm	O
(	O
page	O
203	O
)	O
except	O
for	O
the	O
addition	O
of	O
ρt	O
:	O
wt+1	O
.	O
=	O
wt	O
+	O
αρtδt∇ˆv	O
(	O
st	O
,	O
wt	O
)	O
,	O
(	O
11.2	O
)	O
where	O
δt	O
is	O
deﬁned	O
appropriately	O
depending	O
on	O
whether	O
the	O
problem	O
is	O
episodic	O
and	O
discounted	O
,	O
or	O
continuing	O
and	O
undiscounted	O
using	O
average	O
reward	O
:	O
δt	O
.	O
=	O
rt+1	O
+	O
γˆv	O
(	O
st+1	O
,	O
wt	O
)	O
−	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
,	O
or	O
δt	O
.	O
=	O
rt+1	O
−	O
¯rt	O
+	O
ˆv	O
(	O
st+1	O
,	O
wt	O
)	O
−	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
.	O
(	O
11.3	O
)	O
(	O
11.4	O
)	O
11.2.	O
examples	O
of	O
oﬀ-policy	O
divergence	O
261	O
for	B
action	I
values	I
,	O
the	O
one-step	O
algorithm	O
is	O
semi-gradient	O
expected	O
sarsa	O
:	O
.	O
=	O
wt	O
+	O
αδt∇ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
,	O
with	O
wt+1	O
.	O
δt	O
δt	O
=	O
rt+1	O
+	O
γ	O
(	O
cid:88	O
)	O
a	O
=	O
rt+1	O
−	O
¯rt	O
+	O
(	O
cid:88	O
)	O
a	O
.	O
π	O
(	O
a|st+1	O
)	O
ˆq	O
(	O
st+1	O
,	O
a	O
,	O
wt	O
)	O
−	O
ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
,	O
or	O
(	O
11.5	O
)	O
(	O
episodic	O
)	O
π	O
(	O
a|st+1	O
)	O
ˆq	O
(	O
st+1	O
,	O
a	O
,	O
wt	O
)	O
−	O
ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
.	O
(	O
continuing	O
)	O
note	O
that	O
this	O
algorithm	O
does	O
not	O
use	O
importance	B
sampling	I
.	O
in	O
the	O
tabular	O
case	O
it	O
is	O
clear	O
that	O
this	O
is	O
appropriate	O
because	O
the	O
only	O
sample	O
action	O
is	O
at	O
,	O
and	O
in	O
learning	O
its	O
value	B
we	O
do	O
not	O
have	O
to	O
consider	O
any	O
other	O
actions	O
.	O
with	B
function	I
approximation	I
it	O
is	O
less	O
clear	O
because	O
we	O
might	O
want	O
to	O
weight	O
diﬀerent	O
state–action	O
pairs	O
diﬀerently	O
once	O
they	O
all	O
contribute	O
to	O
the	O
same	O
overall	O
approximation	O
.	O
proper	O
resolution	O
of	O
this	O
issue	O
awaits	O
a	O
more	O
thorough	O
understanding	O
of	O
the	O
theory	O
of	O
function	O
approximation	O
in	O
reinforcement	O
learning	O
.	O
in	O
the	O
multi-step	O
generalizations	O
of	O
these	O
algorithms	O
,	O
both	O
the	O
state-value	O
and	O
action-	O
value	B
algorithms	O
involve	O
importance	B
sampling	I
.	O
for	O
example	O
,	O
the	O
n-step	B
version	O
of	O
semi-	O
gradient	B
expected	O
sarsa	O
is	O
wt+n	O
.	O
=	O
wt+n−1	O
+	O
αρt+1	O
···	O
ρt+n−1	O
[	O
gt	O
:	O
t+n	O
−	O
ˆq	O
(	O
st	O
,	O
at	O
,	O
wt+n−1	O
)	O
]	O
∇ˆq	O
(	O
st	O
,	O
at	O
,	O
wt+n−1	O
)	O
(	O
11.6	O
)	O
with	O
gt	O
:	O
t+n	O
gt	O
:	O
t+n	O
.	O
=	O
rt+1	O
+	O
···	O
+	O
γn−1rt+n	O
+	O
γn	O
ˆq	O
(	O
st+n	O
,	O
at+n	O
,	O
wt+n−1	O
)	O
,	O
or	O
(	O
episodic	O
)	O
.	O
=	O
rt+1	O
−	O
¯rt	O
+	O
···	O
+	O
rt+n	O
−	O
¯rt+n−1	O
+	O
ˆq	O
(	O
st+n	O
,	O
at+n	O
,	O
wt+n−1	O
)	O
,	O
(	O
continuing	O
)	O
where	O
here	O
we	O
are	O
being	O
slightly	O
informal	O
in	O
our	O
treatment	O
of	O
the	O
ends	O
of	O
episodes	O
.	O
in	O
the	O
ﬁrst	O
equation	O
,	O
the	O
ρks	O
for	O
k	O
≥	O
t	O
(	O
where	O
t	O
is	O
the	O
last	O
time	O
step	O
of	O
the	O
episode	O
)	O
should	O
be	O
taken	O
to	O
be	O
1	O
,	O
and	O
gt	O
:	O
n	O
should	O
be	O
taken	O
to	O
be	O
gt	O
if	O
t	O
+	O
n	O
≥	O
t	O
.	O
recall	O
that	O
we	O
also	O
presented	O
in	O
chapter	O
7	O
an	O
oﬀ-policy	B
algorithm	O
that	O
does	O
not	O
involve	O
importance	B
sampling	I
at	O
all	O
:	O
the	O
n-step	B
tree-backup	O
algorithm	O
.	O
here	O
is	O
its	O
semi-gradient	O
version	O
:	O
wt+n	O
gt	O
:	O
t+n	O
.	O
=	O
wt+n−1	O
+	O
α	O
[	O
gt	O
:	O
t+n	O
−	O
ˆq	O
(	O
st	O
,	O
at	O
,	O
wt+n−1	O
)	O
]	O
∇ˆq	O
(	O
st	O
,	O
at	O
,	O
wt+n−1	O
)	O
,	O
.	O
=	O
ˆq	O
(	O
st	O
,	O
at	O
,	O
wt−1	O
)	O
+	O
γπ	O
(	O
ai|si	O
)	O
,	O
δk	O
t+n−1	O
(	O
cid:88	O
)	O
k=t	O
k	O
(	O
cid:89	O
)	O
i=t+1	O
(	O
11.7	O
)	O
(	O
11.8	O
)	O
with	O
δt	O
as	O
deﬁned	O
at	O
the	O
top	O
of	O
this	O
page	O
for	O
expected	O
sarsa	O
.	O
we	O
also	O
deﬁned	O
in	O
chapter	O
7	O
an	O
algorithm	O
that	O
uniﬁes	O
all	O
action-value	O
algorithms	O
:	O
n-step	B
q	O
(	O
σ	O
)	O
.	O
we	O
leave	O
the	O
semi-	O
gradient	B
form	O
of	O
that	O
algorithm	O
,	O
and	O
also	O
of	O
the	O
n-step	B
state-value	O
algorithm	O
,	O
as	O
exercises	O
for	O
the	O
reader	O
.	O
exercise	O
11.1	O
convert	O
the	O
equation	O
of	O
n-step	O
oﬀ-policy	B
td	O
(	O
7.9	O
)	O
to	O
semi-gradient	O
form	O
.	O
give	O
accompanying	O
deﬁnitions	O
of	O
the	O
return	B
for	O
both	O
the	O
episodic	O
and	O
continuing	O
cases	O
.	O
(	O
cid:3	O
)	O
∗exercise	O
11.2	O
convert	O
the	O
equations	O
of	O
n-step	O
q	O
(	O
σ	O
)	O
(	O
7.11	O
and	O
7.17	O
)	O
to	O
semi-gradient	O
(	O
cid:3	O
)	O
form	O
.	O
give	O
deﬁnitions	O
that	O
cover	O
both	O
the	O
episodic	O
and	O
continuing	O
cases	O
.	O
262	O
chapter	O
11	O
:	O
*oﬀ-policy	O
methods	O
with	B
approximation	I
11.2	O
examples	O
of	O
oﬀ-policy	O
divergence	O
in	O
this	O
section	O
we	O
begin	O
to	O
discuss	O
the	O
second	O
part	O
of	O
the	O
challenge	O
of	O
oﬀ-policy	O
learning	O
with	O
function	O
approximation—that	O
the	O
distribution	O
of	O
updates	O
does	O
not	O
match	O
the	O
on-	O
policy	B
distribution	O
.	O
we	O
describe	O
some	O
instructive	O
counterexamples	O
to	O
oﬀ-policy	B
learning—	O
cases	O
where	O
semi-gradient	O
and	O
other	O
simple	O
algorithms	O
are	O
unstable	O
and	O
diverge	O
.	O
to	O
establish	O
intuitions	O
,	O
it	O
is	O
best	O
to	O
consider	O
ﬁrst	O
a	O
very	O
simple	O
example	O
.	O
suppose	O
,	O
perhaps	O
as	O
part	O
of	O
a	O
larger	O
mdp	O
,	O
there	O
are	O
two	O
states	O
whose	O
estimated	O
values	O
are	O
of	O
the	O
functional	O
form	O
w	O
and	O
2w	O
,	O
where	O
the	O
parameter	O
vector	O
w	O
consists	O
of	O
only	O
a	O
single	O
component	O
w.	O
this	O
occurs	O
under	O
linear	B
function	I
approximation	I
if	O
the	O
feature	O
vectors	O
for	O
the	O
two	O
states	O
are	O
each	O
simple	O
numbers	O
(	O
single-component	O
vectors	O
)	O
,	O
in	O
this	O
case	O
1	O
and	O
2.	O
in	O
the	O
ﬁrst	O
state	B
,	O
only	O
one	O
action	B
is	O
available	O
,	O
and	O
it	O
results	O
deterministically	O
in	O
a	O
transition	O
to	O
the	O
second	O
state	B
with	O
a	O
reward	O
of	O
0	O
:	O
where	O
the	O
expressions	O
inside	O
the	O
two	O
circles	O
indicate	O
the	O
two	O
state	B
’	O
s	O
values	O
.	O
suppose	O
initially	O
w	O
=	O
10.	O
the	O
transition	O
will	O
then	O
be	O
from	O
a	O
state	B
of	O
estimated	O
value	B
10	O
to	O
a	O
state	B
of	O
estimated	O
value	B
20.	O
it	O
will	O
look	O
like	O
a	O
good	O
transition	O
,	O
and	O
w	O
will	O
be	O
increased	O
to	O
raise	O
the	O
ﬁrst	O
state	B
’	O
s	O
estimated	O
value	B
.	O
if	O
γ	O
is	O
nearly	O
1	O
,	O
then	O
the	O
td	O
error	O
will	O
be	O
nearly	O
10	O
,	O
and	O
,	O
if	O
α	O
=	O
0.1	O
,	O
then	O
w	O
will	O
be	O
increased	O
to	O
nearly	O
11	O
in	O
trying	O
to	O
reduce	O
the	O
td	O
error	O
.	O
however	O
,	O
the	O
second	O
state	B
’	O
s	O
estimated	O
value	B
will	O
also	O
be	O
increased	O
,	O
to	O
nearly	O
22.	O
if	O
the	O
transition	O
occurs	O
again	O
,	O
then	O
it	O
will	O
be	O
from	O
a	O
state	B
of	O
estimated	O
value	B
≈11	O
to	O
a	O
state	B
of	O
estimated	O
value	B
≈22	O
,	O
for	O
a	O
td	O
error	O
of	O
≈11—larger	O
,	O
not	O
smaller	O
,	O
than	O
before	O
.	O
it	O
will	O
look	O
even	O
more	O
like	O
the	O
ﬁrst	O
state	B
is	O
undervalued	O
,	O
and	O
its	O
value	B
will	O
be	O
increased	O
again	O
,	O
this	O
time	O
to	O
≈12.1	O
.	O
this	O
looks	O
bad	O
,	O
and	O
in	O
fact	O
with	O
further	O
updates	O
w	O
will	O
diverge	O
to	O
inﬁnity	O
.	O
to	O
see	O
this	O
deﬁnitively	O
we	O
have	O
to	O
look	O
more	O
carefully	O
at	O
the	O
sequence	O
of	O
updates	O
.	O
the	O
td	O
error	O
on	O
a	O
transition	O
between	O
the	O
two	O
states	O
is	O
δt	O
=	O
rt+1	O
+	O
γˆv	O
(	O
st+1	O
,	O
wt	O
)	O
−	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
=	O
0	O
+	O
γ2wt	O
−	O
wt	O
=	O
(	O
2γ	O
−	O
1	O
)	O
wt	O
,	O
and	O
the	O
oﬀ-policy	O
semi-gradient	O
td	O
(	O
0	O
)	O
update	O
(	O
from	O
(	O
11.2	O
)	O
)	O
is	O
wt+1	O
=	O
wt	O
+	O
αρtδt∇ˆv	O
(	O
st	O
,	O
wt	O
)	O
=	O
wt	O
+	O
α	O
·	O
1	O
·	O
(	O
2γ	O
−	O
1	O
)	O
wt	O
·	O
1	O
=	O
(	O
cid:0	O
)	O
1	O
+	O
α	O
(	O
2γ	O
−	O
1	O
)	O
(	O
cid:1	O
)	O
wt	O
.	O
note	O
that	O
the	O
importance	B
sampling	I
ratio	O
,	O
ρt	O
,	O
is	O
1	O
on	O
this	O
transition	O
because	O
there	O
is	O
only	O
one	O
action	B
available	O
from	O
the	O
ﬁrst	O
state	B
,	O
so	O
its	O
probabilities	O
of	O
being	O
taken	O
under	O
the	O
target	B
and	O
behavior	O
policies	O
must	O
both	O
be	O
1.	O
in	O
the	O
ﬁnal	O
update	O
above	O
,	O
the	O
new	O
parameter	O
is	O
the	O
old	O
parameter	O
times	O
a	O
scalar	O
constant	O
,	O
1	O
+	O
α	O
(	O
2γ	O
−	O
1	O
)	O
.	O
if	O
this	O
constant	O
is	O
greater	O
than	O
1	O
,	O
then	O
the	O
system	O
is	O
unstable	O
and	O
w	O
will	O
go	O
to	O
positive	O
or	O
negative	O
inﬁnity	O
depending	O
on	O
its	O
initial	O
value	B
.	O
here	O
this	O
constant	O
is	O
greater	O
than	O
1	O
whenever	O
γ	O
>	O
0.5.	O
note	O
that	O
stability	O
does	O
not	O
depend	O
on	O
the	O
speciﬁc	O
step	O
size	O
,	O
as	O
long	O
as	O
α	O
>	O
0.	O
smaller	O
or	O
larger	O
step	O
sizes	O
would	O
aﬀect	O
the	O
rate	O
at	O
which	O
w	O
goes	O
to	O
inﬁnity	O
,	O
but	O
not	O
whether	O
it	O
goes	O
there	O
or	O
not	O
.	O
key	O
to	O
this	O
example	O
is	O
that	O
the	O
one	O
transition	O
occurs	O
repeatedly	O
without	O
w	O
being	O
updated	O
on	O
other	O
transitions	O
.	O
this	O
is	O
possible	O
under	O
oﬀ-policy	B
training	O
because	O
the	O
2w02w	O
11.2.	O
examples	O
of	O
oﬀ-policy	O
divergence	O
263	O
behavior	B
policy	I
might	O
select	O
actions	O
on	O
those	O
other	O
transitions	O
which	O
the	O
target	B
policy	O
never	O
would	O
.	O
for	O
these	O
transitions	O
,	O
ρt	O
would	O
be	O
zero	O
and	O
no	O
update	O
would	O
be	O
made	O
.	O
under	O
on-policy	O
training	O
,	O
however	O
,	O
ρt	O
is	O
always	O
one	O
.	O
each	O
time	O
there	O
is	O
a	O
transition	O
from	O
the	O
w	O
state	B
to	O
the	O
2w	O
state	B
,	O
increasing	O
w	O
,	O
there	O
would	O
also	O
have	O
to	O
be	O
a	O
transition	O
out	O
of	O
the	O
2w	O
state	B
.	O
that	O
transition	O
would	O
reduce	O
w	O
,	O
unless	O
it	O
were	O
to	O
a	O
state	B
whose	O
value	B
was	O
higher	O
(	O
because	O
γ	O
<	O
1	O
)	O
than	O
2w	O
,	O
and	O
then	O
that	O
state	B
would	O
have	O
to	O
be	O
followed	O
by	O
a	O
state	B
of	O
even	O
higher	O
value	B
,	O
or	O
else	O
again	O
w	O
would	O
be	O
reduced	O
.	O
each	O
state	B
can	O
support	O
the	O
one	O
before	O
only	O
by	O
creating	O
a	O
higher	O
expectation	O
.	O
eventually	O
the	O
piper	O
must	O
be	O
paid	O
.	O
in	O
the	O
on-policy	O
case	O
the	O
promise	O
of	O
future	O
reward	O
must	O
be	O
kept	O
and	O
the	O
system	O
is	O
kept	O
in	O
check	O
.	O
but	O
in	O
the	O
oﬀ-policy	O
case	O
,	O
a	O
promise	O
can	O
be	O
made	O
and	O
then	O
,	O
after	O
taking	O
an	O
action	B
that	O
the	O
target	B
policy	O
never	O
would	O
,	O
forgotten	O
and	O
forgiven	O
.	O
this	O
simple	O
example	O
communicates	O
much	O
of	O
the	O
reason	O
why	O
oﬀ-policy	B
training	O
can	O
lead	O
to	O
divergence	O
,	O
but	O
it	O
is	O
not	O
completely	O
convincing	O
because	O
it	O
is	O
not	O
complete—it	O
is	O
just	O
a	O
fragment	O
of	O
a	O
complete	O
mdp	O
.	O
can	O
there	O
really	O
be	O
a	O
complete	O
system	O
with	O
instability	O
?	O
a	O
simple	O
complete	O
example	O
of	O
divergence	O
is	O
baird	O
’	O
s	O
counterexample	O
.	O
consider	O
the	O
episodic	O
seven-state	O
,	O
two-action	O
mdp	O
shown	O
in	O
figure	O
11.1.	O
the	O
dashed	O
action	B
takes	O
the	O
system	O
to	O
one	O
of	O
the	O
six	O
upper	O
states	O
with	O
equal	O
probability	O
,	O
whereas	O
the	O
solid	O
action	B
takes	O
the	O
system	O
to	O
the	O
seventh	O
state	B
.	O
the	O
behavior	B
policy	I
b	O
selects	O
the	O
dashed	O
and	O
solid	O
actions	O
with	O
probabilities	O
6	O
7	O
,	O
so	O
that	O
the	O
next-state	O
distribution	O
under	O
it	O
is	O
uniform	O
(	O
the	O
same	O
for	O
all	O
nonterminal	O
states	O
)	O
,	O
which	O
is	O
also	O
the	O
starting	O
distribution	O
for	O
each	O
episode	O
.	O
the	O
target	B
policy	O
π	O
always	O
takes	O
the	O
solid	O
action	B
,	O
and	O
so	O
the	O
on-policy	B
distribution	I
(	O
for	O
π	O
)	O
is	O
concentrated	O
in	O
the	O
seventh	O
state	B
.	O
the	O
reward	O
is	O
zero	O
on	O
all	O
transitions	O
.	O
the	O
discount	O
rate	O
is	O
γ	O
=	O
0.99	O
.	O
7	O
and	O
1	O
consider	O
estimating	O
the	O
state-value	O
under	O
the	O
linear	O
parameterization	O
indicated	O
by	O
the	O
expression	O
shown	O
in	O
each	O
state	B
circle	O
.	O
for	O
example	O
,	O
the	O
estimated	O
value	B
of	O
the	O
leftmost	O
state	B
is	O
2w1	O
+	O
w8	O
,	O
where	O
the	O
subscript	O
corresponds	O
to	O
the	O
component	O
of	O
the	O
figure	O
11.1	O
:	O
baird	O
’	O
s	O
counterexample	O
.	O
the	O
approximate	B
state-value	O
function	O
for	O
this	O
markov	O
process	O
is	O
of	O
the	O
form	O
shown	O
by	O
the	O
linear	O
expressions	O
inside	O
each	O
state	B
.	O
the	O
solid	O
action	B
usually	O
results	O
in	O
the	O
seventh	O
state	B
,	O
and	O
the	O
dashed	O
action	B
usually	O
results	O
in	O
one	O
of	O
the	O
other	O
six	O
states	O
,	O
each	O
with	O
equal	O
probability	O
.	O
the	O
reward	O
is	O
always	O
zero	O
.	O
2w2+w82w1+w82w3+w82w4+w82w5+w82w6+w8w7+2w8b	O
(	O
dashed|·	O
)	O
=6/7b	O
(	O
solid|·	O
)	O
=1/7⇡	O
(	O
solid|·	O
)	O
=1 =0.99	O
264	O
chapter	O
11	O
:	O
*oﬀ-policy	O
methods	O
with	B
approximation	I
overall	O
weight	O
vector	B
w	O
∈	O
r8	O
;	O
this	O
corresponds	O
to	O
a	O
feature	O
vector	O
for	O
the	O
ﬁrst	O
state	B
being	O
x	O
(	O
1	O
)	O
=	O
(	O
2	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
1	O
)	O
(	O
cid:62	O
)	O
.	O
the	O
reward	O
is	O
zero	O
on	O
all	O
transitions	O
,	O
so	O
the	O
true	O
value	O
function	O
is	O
vπ	O
(	O
s	O
)	O
=	O
0	O
,	O
for	O
all	O
s	O
,	O
which	O
can	O
be	O
exactly	O
approximated	O
if	O
w	O
=	O
0.	O
in	O
fact	O
,	O
there	O
are	O
many	O
solutions	O
,	O
as	O
there	O
are	O
more	O
components	O
to	O
the	O
weight	O
vector	B
(	O
8	O
)	O
than	O
there	O
are	O
nonterminal	O
states	O
(	O
7	O
)	O
.	O
moreover	O
,	O
the	O
set	O
of	O
feature	O
vectors	O
,	O
{	O
x	O
(	O
s	O
)	O
:	O
s	O
∈	O
s	O
}	O
,	O
is	O
a	O
linearly	O
independent	O
set	O
.	O
in	O
all	O
these	O
ways	O
this	O
task	O
seems	O
a	O
favorable	O
case	O
for	O
linear	O
function	B
approximation	I
.	O
if	O
we	O
apply	O
semi-gradient	O
td	O
(	O
0	O
)	O
to	O
this	O
problem	O
(	O
11.2	O
)	O
,	O
then	O
the	O
weights	O
diverge	O
to	O
inﬁnity	O
,	O
as	O
shown	O
in	O
figure	O
11.2	O
(	O
left	O
)	O
.	O
the	O
instability	O
occurs	O
for	O
any	O
positive	O
step	O
size	O
,	O
no	O
matter	O
how	O
small	O
.	O
in	O
fact	O
,	O
it	O
even	O
occurs	O
if	O
a	O
expected	B
update	I
is	O
done	O
as	O
in	O
dynamic	B
programming	I
(	O
dp	O
)	O
.	O
if	O
we	O
do	O
a	O
dp-style	O
expected	B
update	I
instead	O
of	O
a	O
sample	O
(	O
learning	O
)	O
update	O
,	O
as	O
shown	O
in	O
figure	O
11.2	O
(	O
right	O
)	O
.	O
that	O
is	O
,	O
if	O
the	O
weight	O
vector	B
,	O
wk	O
,	O
is	O
updated	O
for	O
all	O
states	O
all	O
at	O
the	O
same	O
time	O
in	O
a	O
semi-gradient	O
way	O
,	O
using	O
the	O
dp	O
(	O
expectation-based	O
)	O
target	B
:	O
wk+1	O
.	O
=	O
wk	O
+	O
α	O
|s|	O
(	O
cid:88	O
)	O
s	O
(	O
cid:16	O
)	O
e	O
[	O
rt+1	O
+	O
γˆv	O
(	O
st+1	O
,	O
wk	O
)	O
|	O
st	O
=	O
s	O
]	O
−	O
ˆv	O
(	O
s	O
,	O
wk	O
)	O
(	O
cid:17	O
)	O
∇ˆv	O
(	O
s	O
,	O
wk	O
)	O
.	O
(	O
11.9	O
)	O
in	O
this	O
case	O
,	O
there	O
is	O
no	O
randomness	O
and	O
no	O
asynchrony	O
,	O
just	O
as	O
in	O
a	O
classical	O
dp	O
update	O
.	O
the	O
method	O
is	O
conventional	O
except	O
in	O
its	O
use	O
of	O
semi-gradient	O
function	B
approximation	I
.	O
yet	O
still	O
the	O
system	O
is	O
unstable	O
.	O
if	O
we	O
alter	O
just	O
the	O
distribution	O
of	O
dp	O
updates	O
in	O
baird	O
’	O
s	O
counterexample	O
,	O
from	O
the	O
uniform	O
distribution	O
to	O
the	O
on-policy	B
distribution	I
(	O
which	O
generally	O
requires	O
asynchronous	O
updating	O
)	O
,	O
then	O
convergence	O
is	O
guaranteed	O
to	O
a	O
solution	O
with	O
error	O
bounded	O
by	O
(	O
9.14	O
)	O
.	O
figure	O
11.2	O
:	O
demonstration	O
of	O
instability	O
on	O
baird	O
’	O
s	O
counterexample	O
.	O
shown	O
are	O
the	O
evolution	B
of	O
the	O
components	O
of	O
the	O
parameter	O
vector	O
w	O
of	O
the	O
two	O
semi-gradient	O
algorithms	O
.	O
the	O
step	O
size	O
was	O
α	O
=	O
0.01	O
,	O
and	O
the	O
initial	O
weights	O
were	O
w	O
=	O
(	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
10	O
,	O
1	O
)	O
(	O
cid:62	O
)	O
.	O
w8w83002001001011000010000w1–w6stepsw7sweepssemi-gradient	O
off-policy	O
tdsemi-gradient	O
dpw1–w6w7	O
11.2.	O
examples	O
of	O
oﬀ-policy	O
divergence	O
265	O
this	O
example	O
is	O
striking	O
because	O
the	O
td	O
and	O
dp	O
methods	O
used	O
are	O
arguably	O
the	O
simplest	O
and	O
best-understood	O
bootstrapping	B
methods	O
,	O
and	O
the	O
linear	O
,	O
semi-descent	O
method	O
used	O
is	O
arguably	O
the	O
simplest	O
and	O
best-understood	O
kind	O
of	O
function	O
approximation	O
.	O
the	O
example	O
shows	O
that	O
even	O
the	O
simplest	O
combination	O
of	O
bootstrapping	O
and	B
function	I
approximation	I
can	O
be	O
unstable	O
if	O
the	O
updates	O
are	O
not	O
done	O
according	O
to	O
the	O
on-policy	B
distribution	I
.	O
there	O
are	O
also	O
counterexamples	O
similar	O
to	O
baird	O
’	O
s	O
showing	O
divergence	O
for	O
q-learning	O
.	O
this	O
is	O
cause	O
for	O
concern	O
because	O
otherwise	O
q-learning	O
has	O
the	O
best	O
convergence	O
guar-	O
antees	O
of	O
all	O
control	B
methods	O
.	O
considerable	O
eﬀort	O
has	O
gone	O
into	O
trying	O
to	O
ﬁnd	O
a	O
remedy	O
to	O
this	O
problem	O
or	O
to	O
obtain	O
some	O
weaker	O
,	O
but	O
still	O
workable	O
,	O
guarantee	O
.	O
for	O
example	O
,	O
it	O
may	O
be	O
possible	O
to	O
guarantee	O
convergence	O
of	O
q-learning	O
as	O
long	O
as	O
the	O
behavior	B
policy	I
is	O
suﬃciently	O
close	O
to	O
the	O
target	B
policy	O
,	O
for	O
example	O
,	O
when	O
it	O
is	O
the	O
ε-greedy	O
policy	O
.	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
q-learning	O
has	O
never	O
been	O
found	O
to	O
diverge	O
in	O
this	O
case	O
,	O
but	O
there	O
has	O
been	O
no	O
theoretical	O
analysis	O
.	O
in	O
the	O
rest	O
of	O
this	O
section	O
we	O
present	O
several	O
other	O
ideas	O
that	O
have	O
been	O
explored	O
.	O
suppose	O
that	O
instead	O
of	O
taking	O
just	O
a	O
step	O
toward	O
the	O
expected	O
one-step	O
return	B
on	O
each	O
iteration	O
,	O
as	O
in	O
baird	O
’	O
s	O
counterexample	O
,	O
we	O
actually	O
change	O
the	O
value	B
function	I
all	O
the	O
way	O
to	O
the	O
best	O
,	O
least-squares	O
approximation	O
.	O
would	O
this	O
solve	O
the	O
instability	O
problem	O
?	O
of	O
course	O
it	O
would	O
if	O
the	O
feature	O
vectors	O
,	O
{	O
x	O
(	O
s	O
)	O
:	O
s	O
∈	O
s	O
}	O
,	O
formed	O
a	O
linearly	O
independent	O
set	O
,	O
as	O
they	O
do	O
in	O
baird	O
’	O
s	O
counterexample	O
,	O
because	O
then	O
exact	O
approximation	O
is	O
possible	O
on	O
each	O
iteration	O
and	O
the	O
method	O
reduces	O
to	O
standard	O
tabular	O
dp	O
.	O
but	O
of	O
course	O
the	O
point	O
here	O
is	O
to	O
consider	O
the	O
case	O
when	O
an	O
exact	O
solution	O
is	O
not	O
possible	O
.	O
in	O
this	O
case	O
stability	O
is	O
not	O
guaranteed	O
even	O
when	O
forming	O
the	O
best	O
approximation	O
at	O
each	O
iteration	O
,	O
as	O
shown	O
in	O
the	O
example	O
.	O
example	O
11.1	O
:	O
tsitsiklis	O
and	O
van	O
roy	O
’	O
s	O
counterexample	O
this	O
example	O
shows	O
that	O
linear	B
function	I
approximation	I
would	O
not	O
work	O
with	O
dp	O
even	O
if	O
the	O
least-squares	O
solution	O
was	O
found	O
at	O
each	O
step	O
.	O
the	O
counterexample	O
is	O
formed	O
by	O
extending	O
the	O
w-to-2w	O
example	O
(	O
from	O
earlier	O
in	O
this	O
section	O
)	O
with	O
a	O
terminal	O
state	B
,	O
as	O
shown	O
to	O
the	O
right	O
.	O
as	O
before	O
,	O
the	O
estimated	O
value	B
of	O
the	O
ﬁrst	O
state	B
is	O
w	O
,	O
and	O
the	O
estimated	O
value	B
of	O
the	O
second	O
state	B
is	O
2w	O
.	O
the	O
reward	O
is	O
zero	O
on	O
all	O
transitions	O
,	O
so	O
the	O
true	O
values	O
are	O
zero	O
at	O
both	O
states	O
,	O
which	O
is	O
exactly	O
representable	O
with	O
w	O
=	O
0.	O
if	O
we	O
set	O
wk+1	O
at	O
each	O
step	O
so	O
as	O
to	O
minimize	O
the	O
ve	O
between	O
the	O
estimated	O
value	B
and	O
the	O
expected	O
one-step	O
return	B
,	O
then	O
we	O
have	O
wk+1	O
=	O
arg	O
min	O
w∈r	O
(	O
cid:88	O
)	O
s∈s	O
(	O
cid:16	O
)	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
−	O
eπ	O
(	O
cid:2	O
)	O
rt+1	O
+	O
γˆv	O
(	O
st+1	O
,	O
wk	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
st	O
=	O
s	O
(	O
cid:3	O
)	O
(	O
cid:17	O
)	O
2	O
w∈r	O
(	O
cid:0	O
)	O
w	O
−	O
γ2wk	O
(	O
cid:1	O
)	O
2	O
+	O
(	O
cid:0	O
)	O
2w	O
−	O
(	O
1	O
−	O
ε	O
)	O
γ2wk	O
(	O
cid:1	O
)	O
2	O
(	O
11.10	O
)	O
=	O
arg	O
min	O
6	O
−	O
4ε	O
=	O
γwk	O
.	O
5	O
the	O
sequence	O
{	O
wk	O
}	O
diverges	O
when	O
γ	O
>	O
5	O
6−4ε	O
and	O
w0	O
(	O
cid:54	O
)	O
=	O
0	O
.	O
1 ✏✏w2w	O
266	O
chapter	O
11	O
:	O
*oﬀ-policy	O
methods	O
with	B
approximation	I
another	O
way	O
to	O
try	O
to	O
prevent	O
instability	O
is	O
to	O
use	O
special	O
methods	O
for	O
function	O
approx-	O
imation	O
.	O
in	O
particular	O
,	O
stability	O
is	O
guaranteed	O
for	O
function	O
approximation	O
methods	O
that	O
do	O
not	O
extrapolate	O
from	O
the	O
observed	O
targets	O
.	O
these	O
methods	O
,	O
called	O
averagers	B
,	O
include	O
nearest	O
neighbor	O
methods	O
and	O
locally	O
weighted	O
regression	O
,	O
but	O
not	O
popular	O
methods	O
such	O
as	O
tile	O
coding	O
and	O
artiﬁcial	O
neural	O
networks	O
.	O
exercise	O
11.3	O
(	O
programming	O
)	O
apply	O
one-step	O
semi-gradient	O
q-learning	O
to	O
baird	O
’	O
s	O
coun-	O
(	O
cid:3	O
)	O
terexample	O
and	O
show	O
empirically	O
that	O
it	O
’	O
s	O
weights	O
diverge	O
.	O
11.3	O
the	O
deadly	B
triad	I
our	O
discussion	O
so	O
far	O
can	O
be	O
summarized	O
by	O
saying	O
that	O
the	O
danger	O
of	O
instability	O
and	O
divergence	O
arises	O
whenever	O
we	O
combine	O
all	O
of	O
the	O
following	O
three	O
elements	O
,	O
making	O
up	O
what	O
we	O
call	O
the	O
deadly	B
triad	I
:	O
function	B
approximation	I
a	O
powerful	O
,	O
scalable	O
way	O
of	O
generalizing	O
from	O
a	O
state	B
space	O
much	O
larger	O
than	O
the	O
memory	O
and	O
computational	O
resources	O
(	O
e.g.	O
,	O
linear	B
function	I
approximation	I
or	O
artiﬁcial	B
neural	I
networks	I
)	O
.	O
bootstrapping	B
update	O
targets	O
that	O
include	O
existing	O
estimates	O
(	O
as	O
in	O
dynamic	O
pro-	O
gramming	O
or	O
td	O
methods	O
)	O
rather	O
than	O
relying	O
exclusively	O
on	O
actual	O
rewards	O
and	O
complete	O
returns	O
(	O
as	O
in	O
mc	O
methods	O
)	O
.	O
oﬀ-policy	B
training	O
training	O
on	O
a	O
distribution	O
of	O
transitions	O
other	O
than	O
that	O
produced	O
by	O
the	O
target	B
policy	O
.	O
sweeping	O
through	O
the	O
state	B
space	O
and	O
updating	O
all	O
states	O
uniformly	O
,	O
as	O
in	O
dynamic	B
programming	I
,	O
does	O
not	O
respect	O
the	O
target	B
policy	O
and	O
is	O
an	O
example	O
of	O
oﬀ-policy	O
training	O
.	O
in	O
particular	O
,	O
note	O
that	O
the	O
danger	O
is	O
not	O
due	O
to	O
control	B
,	O
or	O
to	O
generalized	O
policy	O
itera-	O
tion	B
.	O
those	O
cases	O
are	O
more	O
complex	O
to	O
analyze	O
,	O
but	O
the	O
instability	O
arises	O
in	O
the	O
simpler	O
prediction	B
case	O
whenever	O
it	O
includes	O
all	O
three	O
elements	O
of	O
the	O
deadly	B
triad	I
.	O
the	O
danger	O
is	O
also	O
not	O
due	O
to	O
learning	O
or	O
to	O
uncertainties	O
about	O
the	O
environment	B
,	O
because	O
it	O
oc-	O
curs	O
just	O
as	O
strongly	O
in	O
planning	O
methods	O
,	O
such	O
as	O
dynamic	O
programming	O
,	O
in	O
which	O
the	O
environment	B
is	O
completely	O
known	O
.	O
if	O
any	O
two	O
elements	O
of	O
the	O
deadly	B
triad	I
are	O
present	O
,	O
but	O
not	O
all	O
three	O
,	O
then	O
instability	O
can	O
be	O
avoided	O
.	O
it	O
is	O
natural	O
,	O
then	O
,	O
to	O
go	O
through	O
the	O
three	O
and	O
see	O
if	O
there	O
is	O
any	O
one	O
that	O
can	O
be	O
given	O
up	O
.	O
of	O
the	O
three	O
,	O
function	B
approximation	I
most	O
clearly	O
can	O
not	O
be	O
given	O
up	O
.	O
we	O
need	O
methods	O
that	O
scale	O
to	O
large	O
problems	O
and	O
to	O
great	O
expressive	O
power	O
.	O
we	O
need	O
at	O
least	O
linear	B
function	I
approximation	I
with	O
many	O
features	O
and	O
parameters	O
.	O
state	B
aggregation	I
or	O
nonparametric	O
methods	O
whose	O
complexity	O
grows	O
with	O
data	O
are	O
too	O
weak	O
or	O
too	O
expensive	O
.	O
least-squares	O
methods	O
such	O
as	O
lstd	O
are	O
of	O
quadratic	O
complexity	O
and	O
are	O
therefore	O
too	O
expensive	O
for	O
large	O
problems	O
.	O
doing	O
without	O
bootstrapping	B
is	O
possible	O
,	O
at	O
the	O
cost	O
of	O
computational	O
and	O
data	O
eﬃ-	O
ciency	O
.	O
perhaps	O
most	O
important	O
are	O
the	O
losses	O
in	O
computational	O
eﬃciency	O
.	O
monte	O
carlo	O
(	O
non-bootstrapping	O
)	O
methods	O
require	O
memory	O
to	O
save	O
everything	O
that	O
happens	O
between	O
11.4.	O
linear	O
value-function	O
geometry	O
267	O
making	O
each	O
prediction	B
and	O
obtaining	O
the	O
ﬁnal	O
return	O
,	O
and	O
all	O
their	O
computation	O
is	O
done	O
once	O
the	O
ﬁnal	O
return	O
is	O
obtained	O
.	O
the	O
cost	O
of	O
these	O
computational	O
issues	O
is	O
not	O
apparent	O
on	O
serial	O
von	O
neumann	O
computers	O
,	O
but	O
would	O
be	O
on	O
specialized	O
hardware	O
.	O
with	O
boot-	O
strapping	O
and	B
eligibility	I
traces	I
(	O
chapter	O
12	O
)	O
,	O
data	O
can	O
be	O
dealt	O
with	O
when	O
and	O
where	O
it	O
is	O
generated	O
,	O
then	O
need	O
never	O
be	O
used	O
again	O
.	O
the	O
savings	O
in	O
communication	O
and	O
memory	O
made	O
possible	O
by	O
bootstrapping	B
are	O
great	O
.	O
the	O
losses	O
in	O
data	O
eﬃciency	O
by	O
giving	O
up	O
bootstrapping	B
are	O
also	O
signiﬁcant	O
.	O
we	O
have	O
seen	O
this	O
repeatedly	O
,	O
such	O
as	O
in	O
chapters	O
7	O
(	O
figure	O
7.2	O
)	O
and	O
9	O
(	O
figure	O
9.2	O
)	O
,	O
where	O
some	O
de-	O
gree	O
of	O
bootstrapping	O
performed	O
much	O
better	O
than	O
monte	O
carlo	O
methods	O
on	O
the	O
random-	O
walk	O
prediction	B
task	O
,	O
and	O
in	O
chapter	O
10	O
where	O
the	O
same	O
was	O
seen	O
on	O
the	O
mountain-car	O
control	B
task	O
(	O
figure	O
10.4	O
)	O
.	O
many	O
other	O
problems	O
show	O
much	O
faster	O
learning	O
with	O
boot-	O
strapping	O
(	O
e.g.	O
,	O
see	O
figure	O
12.14	O
)	O
.	O
bootstrapping	B
often	O
results	O
in	O
faster	O
learning	O
because	O
it	O
allows	O
learning	O
to	O
take	O
advantage	O
of	O
the	O
state	B
property	O
,	O
the	O
ability	O
to	O
recognize	O
a	O
state	B
upon	O
returning	O
to	O
it	O
.	O
on	O
the	O
other	O
hand	O
,	O
bootstrapping	B
can	O
impair	O
learning	O
on	O
problems	O
where	O
the	O
state	B
representation	O
is	O
poor	O
and	O
causes	O
poor	O
generalization	O
(	O
e.g.	O
,	O
this	O
seems	O
to	O
be	O
the	O
case	O
on	O
tetris	O
,	O
see	O
s¸im¸sek	O
,	O
alg´orta	O
,	O
and	O
kothiyal	O
,	O
2016	O
)	O
.	O
a	O
poor	O
state	B
represen-	O
tation	O
can	O
also	O
result	O
in	O
bias	O
;	O
this	O
is	O
the	O
reason	O
for	O
the	O
poorer	O
bound	O
on	O
the	O
asymptotic	O
approximation	O
quality	O
of	O
bootstrapping	O
methods	O
(	O
equation	O
9.14	O
)	O
.	O
on	O
balance	O
,	O
the	O
abil-	O
ity	O
to	O
bootstrap	O
has	O
to	O
be	O
considered	O
extremely	O
valuable	O
.	O
one	O
may	O
sometimes	O
choose	O
not	O
to	O
use	O
it	O
by	O
selecting	O
long	O
n-step	B
updates	O
(	O
or	O
a	O
large	O
bootstrapping	B
parameter	O
,	O
λ	O
≈	O
1	O
;	O
see	O
chapter	O
12	O
)	O
but	O
often	O
bootstrapping	B
greatly	O
increases	O
eﬃciency	O
.	O
it	O
is	O
an	O
ability	O
that	O
we	O
would	O
very	O
much	O
like	O
to	O
keep	O
in	O
our	O
toolkit	O
.	O
finally	O
,	O
there	O
is	O
oﬀ-policy	B
learning	O
;	O
can	O
we	O
give	O
that	O
up	O
?	O
on-policy	B
methods	I
are	O
often	O
adequate	O
.	O
for	O
model-free	O
reinforcement	B
learning	I
,	O
one	O
can	O
simply	O
use	O
sarsa	O
rather	O
than	O
q-learning	O
.	O
oﬀ-policy	B
methods	I
free	O
behavior	O
from	O
the	O
target	B
policy	O
.	O
this	O
could	O
be	O
considered	O
an	O
appealing	O
convenience	O
but	O
not	O
a	O
necessity	O
.	O
however	O
,	O
oﬀ-policy	B
learning	O
is	O
essential	O
to	O
other	O
anticipated	O
use	O
cases	O
,	O
cases	O
that	O
we	O
have	O
not	O
yet	O
mentioned	O
in	O
this	O
book	O
but	O
may	O
be	O
important	O
to	O
the	O
larger	O
goal	B
of	O
creating	O
a	O
powerful	O
intelligent	O
agent	O
.	O
in	O
these	O
use	O
cases	O
,	O
the	O
agent	O
learns	O
not	O
just	O
a	O
single	O
value	B
function	I
and	O
single	O
policy	B
,	O
but	O
large	O
numbers	O
of	O
them	O
in	O
parallel	O
.	O
there	O
is	O
extensive	O
psychological	O
evidence	O
that	O
people	O
and	O
animals	O
learn	O
to	O
predict	O
many	O
diﬀerent	O
sensory	O
events	O
,	O
not	O
just	O
rewards	O
.	O
we	O
can	O
be	O
surprised	O
by	O
unusual	O
events	O
,	O
and	O
correct	O
our	O
predictions	O
about	O
them	O
,	O
even	O
if	O
they	O
are	O
of	O
neutral	O
valence	O
(	O
neither	O
good	O
nor	O
bad	O
)	O
.	O
this	O
kind	O
of	O
prediction	O
presumably	O
underlies	O
predictive	O
models	O
of	O
the	O
world	O
such	O
as	O
are	O
used	O
in	O
planning	O
.	O
we	O
predict	O
what	O
we	O
will	O
see	O
after	O
eye	O
movements	O
,	O
how	O
long	O
it	O
will	O
take	O
to	O
walk	O
home	O
,	O
the	O
probability	O
of	O
making	O
a	O
jump	O
shot	O
in	O
basketball	O
,	O
and	O
the	O
satisfaction	O
we	O
will	O
get	O
from	O
taking	O
on	O
a	O
new	O
project	O
.	O
in	O
all	O
these	O
cases	O
,	O
the	O
events	O
we	O
would	O
like	O
to	O
predict	O
depend	O
on	O
our	O
acting	O
in	O
a	O
certain	O
way	O
.	O
to	O
learn	O
them	O
all	O
,	O
in	O
parallel	O
,	O
requires	O
learning	O
from	O
the	O
one	O
stream	O
of	O
experience	O
.	O
there	O
are	O
many	O
target	B
policies	O
,	O
and	O
thus	O
the	O
one	O
behavior	B
policy	I
can	O
not	O
equal	O
all	O
of	O
them	O
.	O
yet	O
parallel	O
learning	O
is	O
conceptually	O
possible	O
because	O
the	O
behavior	B
policy	I
may	O
overlap	O
in	O
part	O
with	O
many	O
of	O
the	O
target	B
policies	O
.	O
to	O
take	O
full	O
advantage	O
of	O
this	O
requires	O
oﬀ-policy	B
learning	O
.	O
268	O
chapter	O
11	O
:	O
*oﬀ-policy	O
methods	O
with	B
approximation	I
11.4	O
linear	O
value-function	O
geometry	O
to	O
better	O
understand	O
the	O
stability	O
challenge	O
of	O
oﬀ-policy	O
learning	O
,	O
it	O
is	O
helpful	O
to	O
think	O
about	O
value	B
function	I
approximation	O
more	O
abstractly	O
and	O
independently	O
of	O
how	O
learning	O
is	O
done	O
.	O
we	O
can	O
imagine	O
the	O
space	O
of	O
all	O
possible	O
state-value	O
functions—all	O
functions	O
from	O
states	O
to	O
real	O
numbers	O
v	O
:	O
s	O
→	O
r.	O
most	O
of	O
these	O
value	B
functions	O
do	O
not	O
correspond	O
to	O
any	O
policy	B
.	O
more	O
important	O
for	O
our	O
purposes	O
is	O
that	O
most	O
are	O
not	O
representable	O
by	O
the	O
function	O
approximator	O
,	O
which	O
by	O
design	O
has	O
far	O
fewer	O
parameters	O
than	O
there	O
are	O
states	O
.	O
given	O
an	O
enumeration	O
of	O
the	O
state	B
space	O
s	O
=	O
{	O
s1	O
,	O
s2	O
,	O
.	O
.	O
.	O
,	O
s|s|	O
}	O
,	O
any	O
value	B
function	I
v	O
corresponds	O
to	O
a	O
vector	B
listing	O
the	O
value	B
of	O
each	O
state	B
in	O
order	O
[	O
v	O
(	O
s1	O
)	O
,	O
v	O
(	O
s2	O
)	O
,	O
.	O
.	O
.	O
,	O
v	O
(	O
s|s|	O
)	O
]	O
(	O
cid:62	O
)	O
.	O
this	O
vector	B
representation	O
of	O
a	O
value	B
function	I
has	O
as	O
many	O
components	O
as	O
there	O
are	O
states	O
.	O
in	O
most	O
cases	O
where	O
we	O
want	O
to	O
use	O
function	B
approximation	I
,	O
this	O
would	O
be	O
far	O
too	O
many	O
components	O
to	O
represent	O
the	O
vector	B
explicitly	O
.	O
nevertheless	O
,	O
the	O
idea	O
of	O
this	O
vector	B
is	O
conceptually	O
useful	O
.	O
in	O
the	O
following	O
,	O
we	O
treat	O
a	O
value	B
function	I
and	O
its	O
vector	B
representation	O
interchangably	O
.	O
to	O
develop	O
intuitions	O
,	O
consider	O
the	O
case	O
with	O
three	O
states	O
s	O
=	O
{	O
s1	O
,	O
s2	O
,	O
s3	O
}	O
and	O
two	O
parameters	O
w	O
=	O
(	O
w1	O
,	O
w2	O
)	O
(	O
cid:62	O
)	O
.	O
we	O
can	O
then	O
view	O
all	O
value	B
functions/vectors	O
as	O
points	O
in	O
a	O
three-dimensional	O
space	O
.	O
the	O
parameters	O
provide	O
an	O
alternative	O
coordinate	O
system	O
over	O
a	O
two-dimensional	O
subspace	O
.	O
any	O
weight	O
vector	B
w	O
=	O
(	O
w1	O
,	O
w2	O
)	O
(	O
cid:62	O
)	O
is	O
a	O
point	O
in	O
the	O
two-dimensional	O
subspace	O
and	O
thus	O
also	O
a	O
complete	O
value	B
function	I
vw	O
that	O
assigns	O
values	O
to	O
all	O
three	O
states	O
.	O
with	O
general	O
function	B
approximation	I
the	O
relationship	O
between	O
the	O
full	O
space	O
and	O
the	O
subspace	O
of	O
representable	O
functions	O
could	O
be	O
complex	O
,	O
but	O
in	O
the	O
case	O
of	O
linear	O
value-function	B
approximation	I
the	O
subspace	O
is	O
a	O
simple	O
plane	O
,	O
as	O
suggested	O
by	O
figure	O
11.3.	O
now	O
consider	O
a	O
single	O
ﬁxed	O
policy	B
π.	O
we	O
assume	O
that	O
its	O
true	O
value	O
function	O
,	O
vπ	O
,	O
is	O
too	O
complex	O
to	O
be	O
represented	O
exactly	O
as	O
an	O
approximation	O
.	O
thus	O
vπ	O
is	O
not	O
in	O
the	O
subspace	O
;	O
in	O
the	O
ﬁgure	O
it	O
is	O
depicted	O
as	O
being	O
above	O
the	O
planar	O
subspace	O
of	O
representable	O
functions	O
.	O
if	O
vπ	O
can	O
not	O
be	O
represented	O
exactly	O
,	O
what	O
representable	O
value	B
function	I
is	O
closest	O
to	O
it	O
?	O
this	O
turns	O
out	O
to	O
be	O
a	O
subtle	O
question	O
with	O
multiple	O
answers	O
.	O
to	O
begin	O
,	O
we	O
need	O
a	O
measure	O
of	O
the	O
distance	O
between	O
two	O
value	B
functions	O
.	O
given	O
two	O
value	B
functions	O
v1	O
and	O
v2	O
,	O
we	O
can	O
talk	O
about	O
the	O
vector	B
diﬀerence	O
between	O
them	O
,	O
v	O
=	O
v1	O
−	O
v2	O
.	O
if	O
v	O
is	O
small	O
,	O
then	O
the	O
two	O
value	B
functions	O
are	O
close	O
to	O
each	O
other	O
.	O
but	O
how	O
are	O
we	O
to	O
measure	O
the	O
size	O
of	O
this	O
diﬀerence	O
vector	B
?	O
the	O
conventional	O
euclidean	O
norm	O
is	O
not	O
appropriate	O
because	O
,	O
as	O
discussed	O
in	O
section	O
9.2	O
,	O
some	O
states	O
are	O
more	O
important	O
than	O
others	O
because	O
they	O
occur	O
more	O
frequently	O
or	O
because	O
we	O
are	O
more	O
interested	O
in	O
them	O
(	O
section	O
9.11	O
)	O
.	O
as	O
in	O
section	O
9.2	O
,	O
let	O
us	O
use	O
the	O
distribution	O
µ	O
:	O
s	O
→	O
[	O
0	O
,	O
1	O
]	O
to	O
specify	O
the	O
degree	O
to	O
which	O
we	O
care	O
about	O
diﬀerent	O
states	O
being	O
accurately	O
valued	O
(	O
often	O
taken	O
to	O
be	O
the	O
on-policy	B
distribution	I
)	O
.	O
we	O
can	O
then	O
deﬁne	O
the	O
distance	O
between	O
value	B
functions	O
using	O
the	O
norm	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
2	O
µ	O
.	O
=	O
(	O
cid:88	O
)	O
s∈s	O
µ	O
(	O
s	O
)	O
v	O
(	O
s	O
)	O
2	O
.	O
(	O
11.11	O
)	O
note	O
that	O
the	O
ve	O
from	O
section	O
9.2	O
can	O
be	O
written	O
simply	O
using	O
this	O
norm	O
as	O
ve	O
(	O
w	O
)	O
=	O
(	O
cid:107	O
)	O
vw	O
−	O
vπ	O
(	O
cid:107	O
)	O
2	O
µ.	O
for	O
any	O
value	B
function	I
v	O
,	O
the	O
operation	O
of	O
ﬁnding	O
its	O
closest	O
value	B
function	I
in	O
the	O
subspace	O
of	O
representable	O
value	B
functions	O
is	O
a	O
projection	O
operation	O
.	O
we	O
deﬁne	O
a	O
11.4.	O
linear	O
value-function	O
geometry	O
269	O
figure	O
11.3	O
:	O
the	O
geometry	O
of	O
linear	O
value-function	B
approximation	I
.	O
shown	O
is	O
the	O
three-	O
dimensional	O
space	O
of	O
all	O
value	B
functions	O
over	O
three	O
states	O
,	O
while	O
shown	O
as	O
a	O
plane	O
is	O
the	O
sub-	O
space	O
of	O
all	O
value	B
functions	O
representable	O
by	O
a	O
linear	O
function	O
approximator	O
with	O
parameter	O
w	O
=	O
(	O
w1	O
,	O
w2	O
)	O
(	O
cid:62	O
)	O
.	O
the	O
true	O
value	O
function	O
vπ	O
is	O
in	O
the	O
larger	O
space	O
and	O
can	O
be	O
projected	O
down	O
(	O
into	O
the	O
subspace	O
,	O
using	O
a	O
projection	O
operator	O
π	O
)	O
to	O
its	O
best	O
approximation	O
in	O
the	O
value	O
error	O
(	O
ve	O
)	O
sense	O
.	O
the	O
best	O
approximators	O
in	O
the	O
bellman	O
error	O
(	O
be	O
)	O
,	O
projected	O
bellman	O
error	O
(	O
pbe	O
)	O
,	O
and	O
temporal	O
diﬀerence	O
error	O
(	O
tde	O
)	O
senses	O
are	O
all	O
potentially	O
diﬀerent	O
and	O
are	O
shown	O
in	O
the	O
lower	O
right	O
.	O
(	O
ve	O
,	O
be	O
,	O
and	O
pbe	O
are	O
all	O
treated	O
as	O
the	O
corresponding	O
vectors	O
in	O
this	O
ﬁgure	O
.	O
)	O
the	O
bellman	O
operator	O
takes	O
a	O
value	B
function	I
in	O
the	O
plane	O
to	O
one	O
outside	O
,	O
which	O
can	O
then	O
be	O
projected	O
back	O
.	O
if	O
you	O
iteratively	O
applied	O
the	O
bellman	O
operator	O
outside	O
the	O
space	O
(	O
shown	O
in	O
gray	O
above	O
)	O
you	O
would	O
reach	O
the	O
true	O
value	O
function	O
,	O
as	O
in	O
conventional	O
dynamic	B
programming	I
.	O
if	O
instead	O
you	O
kept	O
projecting	O
back	O
into	O
the	O
subspace	O
at	O
each	O
step	O
,	O
as	O
in	O
the	O
lower	O
step	O
shown	O
in	O
gray	O
,	O
then	O
the	O
ﬁxed	O
point	O
would	O
be	O
the	O
point	O
of	O
vector-zero	O
pbe	O
.	O
projection	O
operator	O
π	O
that	O
takes	O
an	O
arbitrary	O
value	B
function	I
to	O
the	O
representable	O
function	O
that	O
is	O
closest	O
in	O
our	O
norm	O
:	O
πv	O
.	O
=	O
vw	O
where	O
w	O
=	O
arg	O
min	O
w	O
(	O
cid:107	O
)	O
v	O
−	O
vw	O
(	O
cid:107	O
)	O
2	O
µ	O
.	O
(	O
11.12	O
)	O
the	O
representable	O
value	B
function	I
that	O
is	O
closest	O
to	O
the	O
true	O
value	O
function	O
vπ	O
is	O
thus	O
its	O
projection	O
,	O
πvπ	O
,	O
as	O
suggested	O
in	O
figure	O
11.3.	O
this	O
is	O
the	O
solution	O
asymptotically	O
found	O
by	O
monte	O
carlo	O
methods	O
,	O
albeit	O
often	O
very	O
slowly	O
.	O
the	O
projection	O
operation	O
is	O
discussed	O
more	O
fully	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
.	O
td	O
methods	O
ﬁnd	O
diﬀerent	O
solutions	O
.	O
to	O
understand	O
their	O
rationale	O
,	O
recall	O
that	O
the	O
bellman	O
equation	O
for	O
value	O
function	O
vπ	O
is	O
vπ	O
(	O
s	O
)	O
=	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
a|s	O
)	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
[	O
r	O
+	O
γvπ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
]	O
,	O
for	O
all	O
s	O
∈	O
s.	O
(	O
11.16	O
)	O
accordingtoastationarydecisionmakingpolicy⇡	O
:	O
s⇥a	O
!	O
[	O
0,1	O
]	O
where⇡	O
(	O
s	O
,	O
a	O
)	O
istheprobabilitythatat=agiventhatst=s	O
,	O
forallt.tosolvethemdpistoﬁndanoptimalpolicy⇡⇤	O
,	O
deﬁnedasapolicythatmaximizestheexpected -discountedrewardreceivedfromeachstate	O
:	O
⇡⇤=argmax⇡v⇡	O
(	O
s	O
)	O
,8s2s	O
,	O
wherev⇡	O
(	O
s	O
)	O
=e⇡⇥rt+1+ rt+2+ 2rt+3+···  st=s⇤,8s2s	O
,	O
(	O
1	O
)	O
where 2	O
[	O
0,1	O
)	O
isknownasthediscount-rateparameter	O
,	O
andthesubscriptontheeindicatesthattheexpectationisconditionalonthepolicy⇡beingusedtoselectactions.thefunctionv⇡iscalledthestate-valuefunctionforpolicy⇡.akeysubproblemunderlyingalmostalle cientsolutionstrategiesformdpsispolicyevaluation	O
,	O
thecomputationorestimationofv⇡foragivenpolicy⇡.forexample	O
,	O
thepopulardpalgorithmknownaspolicyiterationinvolvescomputingthevaluefunctionforasequenceofpolicies	O
,	O
eachofwhichisbetterthantheprevious	O
,	O
untilanoptimalpolicyisfound.intdl	O
,	O
algorithmssuchastd	O
(	O
 	O
)	O
areusedtoapproximatethevaluefunctionforthecurrentpolicy	O
,	O
forexampleaspartofactor–criticmethods.ifthestatespaceisﬁnite	O
,	O
thentheestimatedvaluefunctionmayberepresentedinacomputerasalargearraywithoneentryforeachstateandtheentriesdirectlyupdatedtoformtheestimate.suchtabularmethodscanhandlelargestatespaces	O
,	O
evencontinuousones	O
,	O
throughdiscretization	O
,	O
stateaggregation	O
,	O
andinterpolation	O
,	O
butasthedimensionalityofthestatespaceincreases	O
,	O
thesemethodsrapidlybecomecomputationallyinfeasibleorine↵ective.thisisthee↵ectwhichgaverisetothephrase	O
“	O
thecurseofdimensionality.	O
”	O
amoregeneralandﬂexibleapproachistorepresentthevaluefunctionbyafunctionalformofﬁxedsizeandﬁxedstructurewithmanyvariableparametersorweights.theweightsarethenchangedtoreshapetheapproximatevaluefunctiontobettermatchthetruevaluefunction.wedenotetheparameterizedvaluefunctionapproximatorasv✓	O
(	O
s	O
)	O
⇡v⇡	O
(	O
s	O
)	O
,8s2s	O
,	O
(	O
2	O
)	O
where✓2rn	O
,	O
withn⌧|s|	O
,	O
istheweight/parametervector.theapproximatevaluefunctioncanhavearbitraryformaslongasitiseverywheredi↵erentiablewithrespecttotheweights.forexample	O
,	O
itcouldbeacubicspline	O
,	O
oritcouldimplementedbyamulti-layerneuralnetworkwhere✓istheconcatenationofalltheconnectionweights.henceforthreferto✓exclusivelyastheweights	O
,	O
orweightvector	O
,	O
andreservetheword	O
“	O
parameter	O
”	O
forthingslikethediscount-rateparameter	O
,	O
 	O
,	O
andstep-sizeparameters.animportantspecialcaseisthatinwhichtheapproximatevaluefunctionislinearintheweightsandinfeaturesofthestate	O
:	O
v✓	O
(	O
s	O
)	O
=✓	O
>	O
 	O
(	O
s	O
)	O
,	O
(	O
3	O
)	O
wherethe 	O
(	O
s	O
)	O
2rn,8s2s	O
,	O
arefeaturevectorscharacterizingeachstates	O
,	O
andx	O
>	O
ydenotestheinnerproductoftwovectorsxandy.2the	O
subspace	O
of	O
all	O
value	B
functions	O
representable	O
as	O
bellman	O
error	O
vector	O
(	O
be	O
)	O
theothertwogoalsforapproximationarerelatedtothebellmanequation	O
,	O
whichcanbewrittencompactlyinvectorformasv⇡=b⇡v⇡	O
,	O
(	O
7	O
)	O
whereb⇡	O
:	O
r|s|	O
!	O
r|s|isthebellmanoperatorforpolicy⇡	O
,	O
deﬁnedby	O
(	O
b⇡v	O
)	O
(	O
s	O
)	O
=xa2a⇡	O
(	O
s	O
,	O
a	O
)	O
''	O
r	O
(	O
s	O
,	O
a	O
)	O
+ xs02sp	O
(	O
s0|s	O
,	O
a	O
)	O
v	O
(	O
s0	O
)	O
#	O
,8s2s,8v	O
:	O
s	O
!	O
r.	O
(	O
8	O
)	O
(	O
ifthestateandactionspacesarecontinuous	O
,	O
thenthesumsarereplacedbyintegralsandthefunctionp	O
(	O
·|s	O
,	O
a	O
)	O
istakentobeaprobabilitydensity.	O
)	O
thetruevaluefunctionv⇡istheuniquesolutiontothebellmanequation	O
;	O
thebellmanequationcanbeviewedasanalternatewayofdeﬁningv⇡.foranyvaluefunctionv	O
:	O
s	O
!	O
rnotequaltov⇡	O
,	O
therewillalwaysbeatleastonestatesatwhichv	O
(	O
s	O
)	O
6=	O
(	O
b⇡v	O
)	O
(	O
s	O
)	O
.thediscrepancybetweenthetwosidesofthebellmanequation	O
,	O
v⇡ b⇡v⇡	O
,	O
isanerrorvector	O
,	O
andreducingitisthebasisforoursecondandthirdgoalsforapproximation.thesecondgoalistominimizetheerrorvector	O
’	O
slengthinthed-metric.thatis	O
,	O
tominimizethemean-squaredbellmanerror	O
:	O
be	O
(	O
✓	O
)	O
=xs2sd	O
(	O
s	O
)	O
⇥	O
(	O
b⇡v✓	O
)	O
(	O
s	O
)	O
 v✓	O
(	O
s	O
)	O
⇤2.	O
(	O
9	O
)	O
notethatifv⇡isnotrepresentable	O
,	O
thenitisnotbepossibletoreducethebellmanerrortozero.foranyv✓	O
,	O
thecorrespondingb⇡v✓willgenerallynotberepresentable	O
;	O
itwilllieoutsidethespaceofrepresentablefunctions	O
,	O
assuggestedbytheﬁgure	O
...	O
finally	O
,	O
inourthirdgoalofapproximation	O
,	O
weﬁrstprojectthebellmanerrorandthenminimizeitslength.thatis	O
,	O
weminimizetheerrornotinthebellmanequation	O
(	O
7	O
)	O
butinitsprojectedform	O
:	O
v✓=⇧b⇡v✓	O
,	O
(	O
10	O
)	O
unliketheoriginalbellmanequation	O
,	O
formostfunctionapproximators	O
(	O
e.g.	O
,	O
linearones	O
)	O
theprojectedbellmanequationcanbesolvedexactly.ifitcan	O
’	O
tbesolvedexactly	O
,	O
youcanminimizethemean-squaredprojectedbellmanerror	O
:	O
pbe	O
(	O
✓	O
)	O
=xs2sd	O
(	O
s	O
)	O
⇥	O
(	O
⇧	O
(	O
b⇡v✓ v✓	O
)	O
)	O
(	O
s	O
)	O
⇤2.	O
(	O
11	O
)	O
theminimumisachievedattheprojectionﬁxpoint	O
,	O
atwhichxs2sd	O
(	O
s	O
)	O
⇥	O
(	O
b⇡v✓	O
)	O
(	O
s	O
)	O
 v✓	O
(	O
s	O
)	O
⇤r✓v✓	O
(	O
s	O
)	O
=~0.	O
(	O
12	O
)	O
pvepbeppbe⇧v⇡=v✓⇤vev✓⇤pbev✓⇤be42.2bellmanerrorthesecondgoalforapproximationistoapproximatelysolvethebellmanequation	O
:	O
v⇡=b⇡v⇡	O
,	O
(	O
8	O
)	O
whereb⇡	O
:	O
r|s|	O
!	O
r|s|isthebellmanoperatorforpolicy⇡	O
,	O
deﬁnedby	O
(	O
b⇡v	O
)	O
(	O
s	O
)	O
=xa2a⇡	O
(	O
s	O
,	O
a	O
)	O
''	O
r	O
(	O
s	O
,	O
a	O
)	O
+ xs02sp	O
(	O
s0|s	O
,	O
a	O
)	O
v	O
(	O
s0	O
)	O
#	O
,8s2s,8v	O
:	O
s	O
!	O
r.	O
(	O
9	O
)	O
(	O
ifthestateandactionspacesarecontinuous	O
,	O
thenthesumsarereplacedbyintegralsandthefunctionp	O
(	O
·|s	O
,	O
a	O
)	O
istakentobeaprobabilitydensity.	O
)	O
thetruevaluefunctionv⇡istheuniquesolutiontothebellmanequation	O
,	O
andinthissensethebellmanequationcanbeviewedasanalternatewayofdeﬁningv⇡.foranyvaluefunctionv✓notequaltov⇡	O
,	O
wecanaskthebellmanequationtoholdapproximately	O
,	O
v✓⇡b⇡v✓.thatis	O
,	O
wecanminimizethebellmanerror	O
:	O
be	O
(	O
✓	O
)	O
=||v✓ b⇡v✓||	O
,	O
(	O
10	O
)	O
thoughwecannotexpecttodriveittozeroifv⇡isoutsidetherepresentablesubspace.figure1showsthegeometricrelationships	O
;	O
notethatthebellmanoperatorisshownastakingvaluefunctionsinsidethesubspaceoutsidetosomethingthatisnotrepresentable	O
,	O
andthatthepointofminimumbeisingeneraldi↵erentfromthatofminimumve.thebewasﬁrstproposedasanobjectivefunctionfordpbyschweitzerandseidmann	O
(	O
1985	O
)	O
.baird	O
(	O
1995,1999	O
)	O
extendedittotdlbasedonstochasticgradientdescent	O
,	O
andengel	O
,	O
mannor	O
,	O
andmeir	O
(	O
2003	O
)	O
extendedittoleastsquares	O
(	O
o	O
(	O
n2	O
)	O
)	O
methodsknownasgaussianprocesstdl.intheliterature	O
,	O
beminimizationisoftenreferredtoasbellmanresidualminimization.2.3projectedbellmanerrorthethirdgoalforapproximationistoapproximatelysolvetheprojectedbellmanequation	O
:	O
v✓=⇧b⇡v✓.	O
(	O
11	O
)	O
unliketheoriginalbellmanequation	O
,	O
formostfunctionapproximators	O
(	O
e.g.	O
,	O
linearones	O
)	O
theprojectedbellmanequationcanbesolvedexactly.theoriginaltdlmethods	O
(	O
sutton1988	O
,	O
dayan1992	O
)	O
convergetothissolution	O
,	O
asdoesleast-squarestdl	O
(	O
bradke	O
&	O
barto1996	O
,	O
boyan1999	O
)	O
.thegoalofachieving	O
(	O
11	O
)	O
exactlyiscommon	O
;	O
lesscommonistoconsiderapproximatingitasanobjective.theearlyworkongradient-td	O
(	O
e.g.	O
,	O
suttonetal.2009	O
)	O
appearstobeﬁrsttohaveexplicitlyproposedminimizingthed-weightednormoftheerrorin	O
(	O
11	O
)	O
,	O
whichweherecalltheprojectedbellmanerror	O
:	O
pbe	O
(	O
✓	O
)	O
=||v✓ ⇧b⇡v✓||	O
.	O
(	O
12	O
)	O
thisobjectiveisbestunderstoodbylookingattheleftsideoffigure1.startingatv✓	O
,	O
thebellmanoperatortakesusoutsidethesubspace	O
,	O
andtheprojectionoperatortakesusbackintoit.thedistancebetweenwhereweendupandwherewestartedisthepbe.thedistanceisminimal	O
(	O
zero	O
)	O
whenthetripupandbackleavesusinthesameplace.8value	O
error	O
(	O
ve	O
)	O
w1w2vwvwb⇡vw⇧b⇡vw	O
(	O
minve=kvek2µ	O
)	O
minbe=kbek2µwtdtde=0pbe=~0the	O
3d	O
space	O
of	O
all	O
value	B
functions	O
over	O
3	O
states	O
270	O
chapter	O
11	O
:	O
*oﬀ-policy	O
methods	O
with	B
approximation	I
the	O
projection	O
matrix	O
for	O
a	O
linear	O
function	O
approximator	O
,	O
the	O
projection	O
operation	O
is	O
linear	O
,	O
which	O
implies	O
that	O
it	O
can	O
be	O
represented	O
as	O
an	O
|s|	O
×	O
|s|	O
matrix	O
:	O
x	O
(	O
cid:62	O
)	O
d	O
,	O
(	O
11.13	O
)	O
π	O
.	O
=	O
x	O
(	O
cid:0	O
)	O
x	O
(	O
cid:62	O
)	O
dx	O
(	O
cid:1	O
)	O
−1	O
where	O
,	O
as	O
in	O
section	O
9.4	O
,	O
d	O
denotes	O
the	O
|s|	O
×	O
|s|	O
diagonal	O
matrix	O
with	O
the	O
µ	O
(	O
s	O
)	O
on	O
the	O
diagonal	O
,	O
and	O
x	O
denotes	O
the	O
|s|	O
×	O
d	O
matrix	O
whose	O
rows	O
are	O
the	O
feature	O
vectors	O
x	O
(	O
s	O
)	O
(	O
cid:62	O
)	O
,	O
one	O
for	O
each	O
state	B
s.	O
if	O
the	O
inverse	O
in	O
does	O
not	O
exist	O
,	O
then	O
the	O
pseudoinverse	O
is	O
substituted	O
.	O
using	O
these	O
matrices	O
,	O
the	O
norm	O
of	O
a	O
vector	B
can	O
be	O
written	O
(	O
cid:107	O
)	O
v	O
(	O
cid:107	O
)	O
2	O
µ	O
=	O
v	O
(	O
cid:62	O
)	O
dv	O
,	O
and	O
the	O
approximate	O
linear	O
value	O
function	O
can	O
be	O
written	O
vw	O
=	O
xw	O
.	O
(	O
11.14	O
)	O
(	O
11.15	O
)	O
the	O
true	O
value	O
function	O
vπ	O
is	O
the	O
only	O
value	B
function	I
that	O
solves	O
(	O
11.16	O
)	O
exactly	O
.	O
if	O
an	O
approximate	B
value	O
function	O
vw	O
were	O
substituted	O
for	B
vπ	I
,	O
the	O
diﬀerence	O
between	O
the	O
right	O
and	O
left	O
sides	O
of	O
the	O
modiﬁed	O
equation	O
could	O
be	O
used	O
as	O
a	O
measure	O
of	O
how	O
far	O
oﬀ	O
vw	O
is	O
from	O
vπ	O
.	O
we	O
call	O
this	O
the	O
bellman	O
error	O
at	O
state	B
s	O
:	O
¯δw	O
(	O
s	O
)	O
.	O
π	O
(	O
a|s	O
)	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
=	O
(	O
cid:88	O
)	O
a	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
[	O
r	O
+	O
γvw	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
]	O
	O
−	O
vw	O
(	O
s	O
)	O
=	O
e	O
(	O
cid:2	O
)	O
rt+1	O
+	O
γvw	O
(	O
st+1	O
)	O
−	O
vw	O
(	O
st	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
st	O
=	O
s	O
,	O
at	O
∼	O
π	O
(	O
cid:3	O
)	O
,	O
which	O
shows	O
clearly	O
the	O
relationship	O
of	O
the	O
bellman	O
error	O
to	O
the	O
td	O
error	O
(	O
11.3	O
)	O
.	O
the	O
bellman	O
error	O
is	O
the	O
expectation	O
of	O
the	O
td	O
error	O
.	O
the	O
vector	B
of	O
all	O
the	O
bellman	O
errors	O
,	O
at	O
all	O
states	O
,	O
¯δw	O
∈	O
r|s|	O
,	O
is	O
called	O
the	O
bellman	O
error	O
vector	O
(	O
shown	O
as	O
be	O
in	O
figure	O
11.3	O
)	O
.	O
the	O
overall	O
size	O
of	O
this	O
vector	B
,	O
in	O
the	O
norm	O
,	O
is	O
an	O
overall	O
measure	O
of	O
the	O
error	O
in	O
the	O
value	B
function	I
,	O
called	O
the	O
mean	O
squared	O
bellman	O
error	O
:	O
(	O
11.17	O
)	O
(	O
11.18	O
)	O
(	O
11.19	O
)	O
be	O
(	O
w	O
)	O
=	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
¯δw	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
2	O
µ	O
.	O
it	O
is	O
not	O
possible	O
in	O
general	O
to	O
reduce	O
the	O
be	O
to	O
zero	O
(	O
at	O
which	O
point	O
vw	O
=	O
vπ	O
)	O
,	O
but	O
for	O
linear	O
function	B
approximation	I
there	O
is	O
a	O
unique	O
value	B
of	O
w	O
for	O
which	O
the	O
be	O
is	O
minimized	O
.	O
this	O
point	O
in	O
the	O
representable-function	O
subspace	O
(	O
labeled	O
min	O
be	O
in	O
figure	O
11.3	O
)	O
is	O
diﬀerent	O
in	O
general	O
from	O
that	O
which	O
minimizes	O
the	O
ve	O
(	O
shown	O
as	O
πvπ	O
)	O
.	O
methods	O
that	O
seek	O
to	O
minimize	O
the	O
be	O
are	O
discussed	O
in	O
the	O
next	O
two	O
sections	O
.	O
the	O
bellman	O
error	O
vector	O
is	O
shown	O
in	O
figure	O
11.3	O
as	O
the	O
result	O
of	O
applying	O
the	O
bellman	O
operator	O
bπ	O
:	O
r|s|	O
→	O
r|s|	O
to	O
the	O
approximate	B
value	O
function	O
.	O
the	O
bellman	O
operator	O
is	O
11.5.	O
gradient	B
descent	I
in	O
the	O
bellman	O
error	O
deﬁned	O
by	O
(	O
bπv	O
)	O
(	O
s	O
)	O
.	O
=	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
a|s	O
)	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
[	O
r	O
+	O
γv	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
]	O
,	O
271	O
(	O
11.20	O
)	O
for	O
all	O
s	O
∈	O
s	O
and	O
v	O
:	O
s	O
→	O
r.	O
the	O
bellman	O
error	O
vector	O
for	O
v	O
can	O
be	O
written	O
¯δw	O
=	O
bπvw	O
−	O
vw	O
.	O
if	O
the	O
bellman	O
operator	O
is	O
applied	O
to	O
a	O
value	B
function	I
in	O
the	O
representable	O
subspace	O
,	O
then	O
,	O
in	O
general	O
,	O
it	O
will	O
produce	O
a	O
new	O
value	B
function	I
that	O
is	O
outside	O
the	O
subspace	O
,	O
as	O
suggested	O
in	O
the	O
ﬁgure	O
.	O
in	O
dynamic	O
programming	O
(	O
without	O
function	B
approximation	I
)	O
,	O
this	O
operator	O
is	O
applied	O
repeatedly	O
to	O
the	O
points	O
outside	O
the	O
representable	O
space	O
,	O
as	O
suggested	O
by	O
the	O
gray	O
arrows	O
in	O
the	O
top	O
of	O
figure	O
11.3.	O
eventually	O
that	O
process	O
converges	O
to	O
the	O
true	O
value	O
function	O
vπ	O
,	O
the	O
only	O
ﬁxed	O
point	O
for	O
the	O
bellman	O
operator	O
,	O
the	O
only	O
value	B
function	I
for	O
which	O
vπ	O
=	O
bπvπ	O
,	O
(	O
11.21	O
)	O
which	O
is	O
just	O
another	O
way	O
of	O
writing	O
the	O
bellman	O
equation	O
for	O
π	O
(	O
11.16	O
)	O
.	O
with	B
function	I
approximation	I
,	O
however	O
,	O
the	O
intermediate	O
value	B
functions	O
lying	O
outside	O
the	O
subspace	O
can	O
not	O
be	O
represented	O
.	O
the	O
gray	O
arrows	O
in	O
the	O
upper	O
part	O
of	O
figure	O
11.3	O
can	O
not	O
be	O
followed	O
because	O
after	O
the	O
ﬁrst	O
update	O
(	O
dark	O
line	O
)	O
the	O
value	B
function	I
must	O
be	O
projected	O
back	O
into	O
something	O
representable	O
.	O
the	O
next	O
iteration	O
then	O
begins	O
within	O
the	O
subspace	O
;	O
the	O
value	B
function	I
is	O
again	O
taken	O
outside	O
of	O
the	O
subspace	O
by	O
the	O
bellman	O
operator	O
and	O
then	O
mapped	O
back	O
by	O
the	O
projection	O
operator	O
,	O
as	O
suggested	O
by	O
the	O
lower	O
gray	O
arrow	O
and	O
line	O
.	O
following	O
these	O
arrows	O
is	O
a	O
dp-like	O
process	O
with	B
approximation	I
.	O
in	O
this	O
case	O
we	O
are	O
interested	O
in	O
the	O
projection	O
of	O
the	O
bellman	O
error	O
vector	O
back	O
into	O
the	O
representable	O
space	O
.	O
this	O
is	O
the	O
projected	O
bellman	O
error	O
vector	O
π¯δvw	O
,	O
shown	O
in	O
figure	O
11.3	O
as	O
pbe	O
.	O
the	O
size	O
of	O
this	O
vector	B
,	O
in	O
the	O
norm	O
,	O
is	O
another	O
measure	O
of	O
error	O
in	O
the	O
approximate	O
value	B
function	I
.	O
for	O
any	O
approximate	B
value	O
function	O
v	O
,	O
we	O
deﬁne	O
the	O
mean	O
square	O
projected	O
bellman	O
error	O
,	O
denoted	O
pbe	O
,	O
as	O
(	O
11.22	O
)	O
pbe	O
(	O
w	O
)	O
=	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
π¯δw	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
2	O
µ	O
.	O
with	O
linear	O
function	B
approximation	I
there	O
always	O
exists	O
an	O
approximate	B
value	O
function	O
(	O
within	O
the	O
subspace	O
)	O
with	O
zero	O
pbe	O
;	O
this	O
is	O
the	O
td	O
ﬁxed	O
point	O
,	O
wtd	O
,	O
introduced	O
in	O
section	O
9.4.	O
as	O
we	O
have	O
seen	O
,	O
this	O
point	O
is	O
not	O
always	O
stable	O
under	O
semi-gradient	O
td	O
methods	O
and	O
oﬀ-policy	O
training	O
.	O
as	O
shown	O
in	O
the	O
ﬁgure	O
,	O
this	O
value	B
function	I
is	O
generally	O
diﬀerent	O
from	O
those	O
minimizing	O
ve	O
or	O
be	O
.	O
methods	O
that	O
are	O
guaranteed	O
to	O
converge	O
to	O
it	O
are	O
discussed	O
in	O
sections	O
11.7	O
and	O
11.8	O
.	O
11.5	O
gradient	B
descent	I
in	O
the	O
bellman	O
error	O
armed	O
with	O
a	O
better	O
understanding	O
of	O
value	O
function	B
approximation	I
and	O
its	O
various	O
objectives	O
,	O
we	O
return	B
now	O
to	O
the	O
challenge	O
of	O
stability	O
in	O
oﬀ-policy	O
learning	O
.	O
we	O
would	O
like	O
to	O
apply	O
the	O
approach	O
of	O
stochastic	O
gradient	B
descent	I
(	O
sgd	O
,	O
section	O
9.3	O
)	O
,	O
in	O
which	O
updates	O
272	O
chapter	O
11	O
:	O
*oﬀ-policy	O
methods	O
with	B
approximation	I
are	O
made	O
that	O
in	O
expectation	O
are	O
equal	O
to	O
the	O
negative	O
gradient	B
of	O
an	O
objective	O
function	O
.	O
these	O
methods	O
always	O
go	O
downhill	O
(	O
in	O
expectation	O
)	O
in	O
the	O
objective	O
and	O
because	O
of	O
this	O
are	O
typically	O
stable	O
with	O
excellent	O
convergence	O
properties	O
.	O
among	O
the	O
algorithms	O
investigated	O
so	O
far	O
in	O
this	O
book	O
,	O
only	O
the	O
monte	O
carlo	O
methods	O
are	O
true	O
sgd	O
methods	O
.	O
these	O
methods	O
converge	O
robustly	O
under	O
both	O
on-policy	O
and	O
oﬀ-policy	B
training	O
as	O
well	O
as	O
for	O
general	O
nonlinear	O
(	O
diﬀerentiable	O
)	O
function	O
approximators	O
,	O
though	O
they	O
are	O
often	O
slower	O
than	O
semi-gradient	B
methods	I
with	O
bootstrapping	B
,	O
which	O
are	O
not	O
sgd	O
methods	O
.	O
semi-gradient	B
methods	I
may	O
diverge	O
under	O
oﬀ-policy	B
training	O
,	O
as	O
we	O
have	O
seen	O
earlier	O
in	O
this	O
chapter	O
,	O
and	O
under	O
contrived	O
cases	O
of	O
nonlinear	O
function	B
approximation	I
(	O
tsitsiklis	O
and	O
van	O
roy	O
,	O
1997	O
)	O
.	O
with	O
a	O
true	O
sgd	O
method	O
such	O
divergence	O
would	O
not	O
be	O
possible	O
.	O
the	O
appeal	O
of	O
sgd	O
is	O
so	O
strong	O
that	O
great	O
eﬀort	O
has	O
gone	O
into	O
ﬁnding	O
a	O
practical	O
way	O
of	O
harnessing	O
it	O
for	O
reinforcement	O
learning	O
.	O
the	O
starting	O
place	O
of	O
all	O
such	O
eﬀorts	O
is	O
the	O
choice	O
of	O
an	O
error	O
or	O
objective	O
function	O
to	O
optimize	O
.	O
in	O
this	O
and	O
the	O
next	O
section	O
we	O
explore	O
the	O
origins	O
and	O
limits	O
of	O
the	O
most	O
popular	O
proposed	O
objective	O
function	O
,	O
that	O
based	O
on	O
the	O
bellman	O
error	O
introduced	O
in	O
the	O
previous	O
section	O
.	O
although	O
this	O
has	O
been	O
a	O
popular	O
and	O
inﬂuential	O
approach	O
,	O
the	O
conclusion	O
that	O
we	O
reach	O
here	O
is	O
that	O
it	O
is	O
a	O
misstep	O
and	O
yields	O
no	O
good	O
learning	O
algorithms	O
.	O
on	O
the	O
other	O
hand	O
,	O
this	O
approach	O
fails	O
in	O
an	O
interesting	O
way	O
that	O
oﬀers	O
insight	O
into	O
what	O
might	O
constitute	O
a	O
good	O
approach	O
.	O
to	O
begin	O
,	O
let	O
us	O
consider	O
not	O
the	O
bellman	O
error	O
,	O
but	O
something	O
more	O
immediate	O
and	O
naive	O
.	O
temporal	O
diﬀerence	O
learning	O
is	O
driven	O
by	O
the	O
td	O
error	O
.	O
why	O
not	O
take	O
the	O
mini-	O
mization	O
of	O
the	O
expected	O
square	O
of	O
the	O
td	O
error	O
as	O
the	O
objective	O
?	O
in	O
the	O
general	O
function-	O
approximation	O
case	O
,	O
the	O
one-step	O
td	O
error	O
with	O
discounting	B
is	O
δt	O
=	O
rt+1	O
+	O
γˆv	O
(	O
st+1	O
,	O
wt	O
)	O
−	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
.	O
a	O
possible	O
objective	O
function	O
then	O
is	O
what	O
one	O
might	O
call	O
the	O
mean	O
squared	O
td	O
error	O
:	O
tde	O
(	O
w	O
)	O
=	O
(	O
cid:88	O
)	O
s∈s	O
µ	O
(	O
s	O
)	O
e	O
(	O
cid:2	O
)	O
δ2	O
=	O
(	O
cid:88	O
)	O
s∈s	O
µ	O
(	O
s	O
)	O
e	O
(	O
cid:2	O
)	O
ρtδ2	O
=	O
eb	O
(	O
cid:2	O
)	O
ρtδ2	O
t	O
(	O
cid:3	O
)	O
.	O
t	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
st	O
=	O
s	O
,	O
at∼	O
π	O
(	O
cid:3	O
)	O
t	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
st	O
=	O
s	O
,	O
at∼	O
b	O
(	O
cid:3	O
)	O
(	O
if	O
µ	O
is	O
the	O
distribution	O
encountered	O
under	O
b	O
)	O
the	O
last	O
equation	O
is	O
of	O
the	O
form	O
needed	O
for	O
sgd	O
;	O
it	O
gives	O
the	O
objective	O
as	O
an	O
expectation	O
that	O
can	O
be	O
sampled	O
from	O
experience	O
(	O
remember	O
the	O
experience	O
is	O
due	O
to	O
the	O
behavior	B
policy	I
b.	O
thus	O
,	O
following	O
the	O
standard	O
sgd	O
approach	O
,	O
one	O
can	O
derive	O
the	O
per-step	O
update	O
based	O
on	O
a	O
sample	O
of	O
this	O
expected	O
value	O
:	O
1	O
2	O
wt+1	O
=	O
wt	O
−	O
α∇	O
(	O
ρtδ2	O
t	O
)	O
=	O
wt	O
−	O
αρtδt∇δt	O
=	O
wt	O
+	O
αρtδt	O
(	O
cid:0	O
)	O
∇ˆv	O
(	O
st	O
,	O
wt	O
)	O
−	O
γ∇ˆv	O
(	O
st+1	O
,	O
wt	O
)	O
(	O
cid:1	O
)	O
,	O
which	O
you	O
will	O
recognize	O
as	O
the	O
same	O
as	O
the	O
semi-gradient	O
td	O
algorithm	O
(	O
11.2	O
)	O
except	O
for	O
the	O
additional	O
ﬁnal	O
term	O
.	O
this	O
term	O
completes	O
the	O
gradient	B
and	O
makes	O
this	O
a	O
true	O
sgd	O
(	O
11.23	O
)	O
11.5.	O
gradient	B
descent	I
in	O
the	O
bellman	O
error	O
273	O
algorithm	O
with	O
excellent	O
convergence	O
guarantees	O
.	O
let	O
us	O
call	O
this	O
algorithm	O
the	O
naive	B
residual-gradient	O
algorithm	O
(	O
after	O
baird	O
,	O
1995	O
)	O
.	O
although	O
the	O
naive	B
residual-gradient	O
algorithm	O
converges	O
robustly	O
,	O
it	O
does	O
not	O
necessarily	O
converge	O
to	O
a	O
desirable	O
place	O
.	O
example	O
11.2	O
:	O
a-split	O
example	O
,	O
showing	O
the	O
naivet´e	O
of	O
the	O
naive	B
residual-gradient	O
algorithm	O
consider	O
the	O
three-state	O
episodic	O
mrp	O
shown	O
to	O
the	O
right	O
.	O
episodes	B
begin	O
in	O
state	O
a	O
and	O
then	O
‘	O
split	O
’	O
stochastically	O
,	O
half	O
the	O
time	O
going	O
to	O
b	O
(	O
and	O
then	O
invariably	O
going	O
on	O
to	O
terminate	O
with	O
a	O
reward	O
of	O
1	O
)	O
and	O
half	O
the	O
time	O
going	O
to	O
state	B
c	O
(	O
and	O
then	O
invariably	O
terminating	O
with	O
a	O
reward	O
of	O
zero	O
)	O
.	O
reward	O
for	O
the	O
ﬁrst	O
transition	O
,	O
out	O
of	O
a	O
,	O
is	O
always	O
zero	O
whichever	O
way	O
the	O
episode	O
goes	O
.	O
as	O
this	O
is	O
an	O
episodic	O
problem	O
,	O
we	O
can	O
take	O
γ	O
to	O
be	O
1.	O
we	O
also	O
assume	O
on-policy	O
training	O
,	O
so	O
that	O
ρt	O
is	O
always	O
1	O
,	O
and	O
tabular	O
function	B
approximation	I
,	O
so	O
that	O
the	O
learning	O
algorithms	O
are	O
free	O
to	O
give	O
arbitrary	O
,	O
independent	O
values	O
to	O
all	O
three	O
states	O
.	O
thus	O
,	O
this	O
should	O
be	O
an	O
easy	O
problem	O
.	O
what	O
should	O
the	O
values	O
be	O
?	O
from	O
a	O
,	O
half	O
the	O
time	O
the	O
return	B
is	O
1	O
,	O
and	O
half	O
the	O
time	O
the	O
return	B
is	O
0	O
;	O
a	O
should	O
have	O
value	B
1	O
2	O
.	O
from	O
b	O
the	O
return	B
is	O
always	O
1	O
,	O
so	O
its	O
value	B
should	O
be	O
1	O
,	O
and	O
similarly	O
from	O
c	O
the	O
return	B
is	O
always	O
0	O
,	O
so	O
its	O
value	B
should	O
be	O
0.	O
these	O
are	O
the	O
true	O
values	O
and	O
,	O
as	O
this	O
is	O
a	O
tabular	O
problem	O
,	O
all	O
the	O
methods	O
presented	O
previously	O
converge	O
to	O
them	O
exactly	O
.	O
however	O
,	O
the	O
naive	B
residual-gradient	O
algorithm	O
ﬁnds	O
diﬀerent	O
values	O
for	O
b	O
and	O
c.	O
4	O
(	O
a	O
converges	O
it	O
converges	O
with	O
b	O
having	O
a	O
value	B
of	O
3	O
correctly	O
to	O
1	O
4	O
and	O
c	O
having	O
a	O
value	B
of	O
1	O
2	O
)	O
.	O
these	O
are	O
in	O
fact	O
the	O
values	O
that	O
minimize	O
the	O
tde	O
.	O
2	O
to	O
b	O
’	O
s	O
3	O
2	O
to	O
c	O
’	O
s	O
1	O
4	O
,	O
a	O
change	O
of	O
1	O
4	O
,	O
or	O
down	O
from	O
a	O
’	O
s	O
1	O
let	O
us	O
compute	O
the	O
tde	O
for	O
these	O
values	O
.	O
the	O
ﬁrst	O
transition	O
of	O
each	O
episode	O
is	O
4	O
,	O
a	O
change	O
4	O
.	O
because	O
the	O
reward	O
is	O
zero	O
on	O
these	O
transitions	O
,	O
and	O
γ	O
=	O
1	O
,	O
these	O
changes	O
are	O
16	O
on	O
the	O
ﬁrst	O
transition	O
.	O
4	O
to	O
a	O
reward	O
of	O
1	O
(	O
and	O
a	O
4	O
to	O
a	O
reward	O
of	O
0	O
(	O
again	O
with	O
a	O
terminal	O
16	O
on	O
the	O
either	O
up	O
from	O
a	O
’	O
s	O
1	O
of	O
−	O
1	O
the	O
td	O
errors	O
,	O
and	O
thus	O
the	O
squared	O
td	O
error	O
is	O
always	O
1	O
the	O
second	O
transition	O
is	O
similar	O
;	O
it	O
is	O
either	O
up	O
from	O
b	O
’	O
s	O
3	O
terminal	O
state	B
of	O
value	B
0	O
)	O
,	O
or	O
down	O
from	O
c	O
’	O
s	O
1	O
state	B
of	O
value	B
0	O
)	O
.	O
thus	O
,	O
the	O
td	O
error	O
is	O
always	O
±	O
1	O
second	O
step	O
.	O
thus	O
,	O
for	O
this	O
set	O
of	O
values	O
,	O
the	O
tde	O
on	O
both	O
steps	O
is	O
1	O
16	O
.	O
4	O
,	O
for	O
a	O
squared	O
error	O
of	O
1	O
2	O
up	O
to	O
1	O
,	O
at	O
b	O
,	O
or	O
from	O
1	O
now	O
let	O
’	O
s	O
compute	O
the	O
tde	O
for	O
the	O
true	O
values	O
(	O
b	O
at	O
1	O
,	O
c	O
at	O
0	O
,	O
and	O
a	O
at	O
1	O
2	O
)	O
.	O
in	O
this	O
case	O
the	O
ﬁrst	O
transition	O
is	O
either	O
from	O
1	O
2	O
down	O
to	O
0	O
,	O
at	O
c	O
;	O
in	O
either	O
case	O
the	O
absolute	O
error	O
is	O
1	O
4	O
.	O
the	O
second	O
transition	O
has	O
zero	O
error	O
because	O
the	O
starting	O
value	B
,	O
either	O
1	O
or	O
0	O
depending	O
on	O
whether	O
the	O
transition	O
is	O
from	O
b	O
or	O
c	O
,	O
always	O
exactly	O
matches	O
the	O
immediate	O
reward	O
and	O
return	B
.	O
thus	O
the	O
squared	O
td	O
error	O
is	O
1	O
4	O
on	O
the	O
ﬁrst	O
transition	O
and	O
0	O
on	O
the	O
second	O
,	O
for	O
a	O
mean	O
reward	O
over	O
the	O
two	O
transitions	O
of	O
1	O
16	O
,	O
this	O
solution	O
is	O
worse	O
according	O
to	O
the	O
tde	O
.	O
on	O
this	O
simple	O
problem	O
,	O
the	O
true	O
values	O
do	O
not	O
have	O
the	O
smallest	O
tde	O
.	O
2	O
and	O
the	O
squared	O
error	O
is	O
1	O
8	O
is	O
bigger	O
that	O
1	O
8	O
.	O
as	O
1	O
abc0001	O
274	O
chapter	O
11	O
:	O
*oﬀ-policy	O
methods	O
with	B
approximation	I
a	O
tabular	O
representation	O
is	O
used	O
in	O
the	O
a-split	O
example	O
,	O
so	O
the	O
true	O
state	O
values	O
can	O
be	O
exactly	O
represented	O
,	O
yet	O
the	O
naive	B
residual-gradient	O
algorithm	O
ﬁnds	O
diﬀerent	O
values	O
,	O
and	O
these	O
values	O
have	O
lower	O
tde	O
than	O
do	O
the	O
true	O
values	O
.	O
minimizing	O
the	O
tde	O
is	O
naive	B
;	O
by	O
penalizing	O
all	O
td	O
errors	O
it	O
achieves	O
something	O
more	O
like	O
temporal	O
smoothing	O
than	O
accurate	O
prediction	B
.	O
a	O
better	O
idea	O
would	O
seem	O
to	O
be	O
minimizing	O
the	O
bellman	O
error	O
.	O
if	O
the	O
exact	O
values	O
are	O
learned	O
,	O
the	O
bellman	O
error	O
is	O
zero	O
everywhere	O
.	O
thus	O
,	O
a	O
bellman-error-minimizing	O
algorithm	O
should	O
have	O
no	O
trouble	O
with	O
the	O
a-split	O
example	O
.	O
we	O
can	O
not	O
expect	O
to	O
achieve	O
zero	O
bellman	O
error	O
in	O
general	O
,	O
as	O
it	O
would	O
involve	O
ﬁnding	O
the	O
true	O
value	O
function	O
,	O
which	O
we	O
presume	O
is	O
outside	O
the	O
space	O
of	O
representable	O
value	B
functions	O
.	O
but	O
getting	O
close	O
to	O
this	O
ideal	O
is	O
a	O
natural-seeming	O
goal	B
.	O
as	O
we	O
have	O
seen	O
,	O
the	O
bellman	O
error	O
is	O
also	O
closely	O
related	O
to	O
the	O
td	O
error	O
.	O
the	O
bellman	O
error	O
for	O
a	O
state	B
is	O
the	O
expected	O
td	O
error	O
in	O
that	O
state	B
.	O
so	O
let	O
’	O
s	O
repeat	O
the	O
derivation	O
above	O
with	O
the	O
expected	O
td	O
error	O
(	O
all	O
expectations	O
here	O
are	O
implicitly	O
conditional	O
on	O
st	O
)	O
:	O
1	O
2	O
1	O
2	O
α∇	O
(	O
eπ	O
[	O
δt	O
]	O
2	O
)	O
α∇	O
(	O
eb	O
[	O
ρtδt	O
]	O
2	O
)	O
wt+1	O
=	O
wt	O
−	O
=	O
wt	O
−	O
=	O
wt	O
−	O
αeb	O
[	O
ρtδt	O
]	O
∇eb	O
[	O
ρtδt	O
]	O
=	O
wt	O
−	O
αeb	O
(	O
cid:2	O
)	O
ρt	O
(	O
rt+1	O
+	O
γˆv	O
(	O
st+1	O
,	O
w	O
)	O
−	O
ˆv	O
(	O
st	O
,	O
w	O
)	O
)	O
(	O
cid:3	O
)	O
eb	O
[	O
ρt∇δt	O
]	O
=	O
wt	O
+	O
α	O
(	O
cid:104	O
)	O
eb	O
(	O
cid:2	O
)	O
ρt	O
(	O
rt+1	O
+	O
γˆv	O
(	O
st+1	O
,	O
w	O
)	O
)	O
(	O
cid:3	O
)	O
−	O
ˆv	O
(	O
st	O
,	O
w	O
)	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
∇ˆv	O
(	O
st	O
,	O
w	O
)	O
−	O
γeb	O
(	O
cid:2	O
)	O
ρt∇ˆv	O
(	O
st+1	O
,	O
w	O
)	O
(	O
cid:3	O
)	O
(	O
cid:105	O
)	O
.	O
this	O
update	O
and	O
various	O
ways	O
of	O
sampling	O
it	O
are	O
referred	O
to	O
as	O
the	O
residual-gradient	B
algorithm	I
.	O
if	O
you	O
simply	O
used	O
the	O
sample	O
values	O
in	O
all	O
the	O
expectations	O
,	O
then	O
the	O
equation	O
above	O
reduces	O
almost	O
exactly	O
to	O
(	O
11.23	O
)	O
,	O
the	O
naive	B
residual-gradient	O
algorithm.1	O
but	O
this	O
is	O
naive	B
,	O
because	O
the	O
equation	O
above	O
involves	O
the	O
next	O
state	B
,	O
st+1	O
,	O
appearing	O
in	O
two	O
expectations	O
that	O
are	O
multiplied	O
together	O
.	O
to	O
get	O
an	O
unbiased	O
sample	O
of	O
the	O
product	O
,	O
two	O
independent	O
samples	O
of	O
the	O
next	O
state	B
are	O
required	O
,	O
but	O
during	O
normal	O
interaction	O
with	O
an	O
external	O
environment	B
only	O
one	O
is	O
obtained	O
.	O
one	O
expectation	O
or	O
the	O
other	O
can	O
be	O
sampled	O
,	O
but	O
not	O
both	O
.	O
there	O
are	O
two	O
ways	O
to	O
make	O
the	O
residual-gradient	B
algorithm	I
work	O
.	O
one	O
is	O
in	O
the	O
case	O
of	O
deterministic	O
environments	O
.	O
if	O
the	O
transition	O
to	O
the	O
next	O
state	B
is	O
deterministic	O
,	O
then	O
the	O
two	O
samples	O
will	O
necessarily	O
be	O
the	O
same	O
,	O
and	O
the	O
naive	O
algorithm	O
is	O
valid	O
.	O
the	O
other	O
way	O
is	O
to	O
obtain	O
two	O
independent	O
samples	O
of	O
the	O
next	O
state	B
,	O
st+1	O
,	O
from	O
st	O
,	O
one	O
for	O
the	O
ﬁrst	O
expectation	O
and	O
another	O
for	O
the	O
second	O
expectation	O
.	O
in	O
real	O
interaction	O
with	O
an	O
environment	B
,	O
this	O
would	O
not	O
seem	O
possible	O
,	O
but	O
when	O
interacting	O
with	O
a	O
simulated	O
environment	B
,	O
it	O
is	O
.	O
one	O
simply	O
rolls	O
back	O
to	O
the	O
previous	O
state	B
and	O
obtains	O
an	O
alternate	O
next	O
state	B
before	O
proceeding	O
forward	O
from	O
the	O
ﬁrst	O
next	O
state	B
.	O
in	O
either	O
of	O
these	O
cases	O
the	O
residual-gradient	B
algorithm	I
is	O
guaranteed	O
to	O
converge	O
to	O
a	O
minimum	O
of	O
the	O
be	O
under	O
the	O
usual	O
conditions	O
on	O
the	O
step-size	B
parameter	I
.	O
as	O
a	O
true	O
sgd	O
method	O
,	O
this	O
convergence	O
1for	O
state	B
values	O
there	O
remains	O
a	O
small	O
diﬀerence	O
in	O
the	O
treatment	O
of	O
the	O
importance	B
sampling	I
ratio	O
ρt	O
.	O
in	O
the	O
analagous	O
action-value	O
case	O
(	O
which	O
is	O
the	O
most	O
important	O
case	O
for	O
control	O
algorithms	O
)	O
,	O
the	O
residual-gradient	B
algorithm	I
would	O
reduce	O
exactly	O
to	O
the	O
naive	B
version	O
.	O
11.5.	O
gradient	B
descent	I
in	O
the	O
bellman	O
error	O
275	O
is	O
robust	O
,	O
applying	O
to	O
both	O
linear	O
and	O
nonlinear	O
function	O
approximators	O
.	O
in	O
the	O
linear	O
case	O
,	O
convergence	O
is	O
always	O
to	O
the	O
unique	O
w	O
that	O
minimizes	O
the	O
be	O
.	O
however	O
,	O
there	O
remain	O
at	O
least	O
three	O
ways	O
in	O
which	O
the	O
convergence	O
of	O
the	O
residual-	O
gradient	O
method	O
is	O
unsatisfactory	O
.	O
the	O
ﬁrst	O
of	O
these	O
is	O
that	O
empirically	O
it	O
is	O
slow	O
,	O
much	O
slower	O
that	O
semi-gradient	B
methods	I
.	O
indeed	O
,	O
proponents	O
of	O
this	O
method	O
have	O
proposed	O
increasing	O
its	O
speed	O
by	O
combining	O
it	O
with	O
faster	O
semi-gradient	B
methods	I
initially	O
,	O
then	O
gradually	O
switching	O
over	O
to	O
residual	O
gradient	B
for	O
the	O
convergence	O
guarantee	O
(	O
baird	O
and	O
moore	O
,	O
1999	O
)	O
.	O
the	O
second	O
way	O
in	O
which	O
the	O
residual-gradient	B
algorithm	I
is	O
unsatisfactory	O
is	O
that	O
it	O
still	O
seems	O
to	O
converge	O
to	O
the	O
wrong	O
values	O
.	O
it	O
does	O
get	O
the	O
right	O
values	O
in	O
all	O
tabular	O
cases	O
,	O
such	O
as	O
the	O
a-split	O
example	O
,	O
as	O
for	O
those	O
an	O
exact	O
solution	O
to	O
the	O
bellman	O
equation	O
is	O
possible	O
.	O
but	O
if	O
we	O
examine	O
examples	O
with	O
genuine	O
function	B
approximation	I
,	O
example	O
11.3	O
:	O
a-presplit	O
example	O
,	O
a	O
counterexample	O
for	O
the	O
be	O
consider	O
the	O
three-state	O
episodic	O
mrp	O
shown	O
to	O
the	O
right	O
:	O
episodes	B
start	O
in	O
either	O
a1	O
or	O
a2	O
,	O
with	O
equal	O
prob-	O
ability	O
.	O
these	O
two	O
states	O
look	O
exactly	O
the	O
same	O
to	O
the	O
function	O
approximator	O
,	O
like	O
a	O
single	O
state	B
a	O
whose	O
fea-	O
ture	O
representation	O
is	O
distinct	O
from	O
and	O
unrelated	O
to	O
the	O
feature	O
representation	O
of	O
the	O
other	O
two	O
states	O
,	O
b	O
and	O
c	O
,	O
which	O
are	O
also	O
distinct	O
from	O
each	O
other	O
.	O
speciﬁcally	O
,	O
the	O
parameter	O
of	O
the	O
function	O
approximator	O
has	O
three	O
com-	O
ponents	O
,	O
one	O
giving	O
the	O
value	B
of	O
state	B
b	O
,	O
one	O
giving	O
the	O
value	B
of	O
state	B
c	O
,	O
and	O
one	O
giving	O
the	O
value	B
of	O
both	O
states	O
a1	O
and	O
a2	O
.	O
other	O
than	O
the	O
selection	O
of	O
the	O
initial	O
state	B
,	O
the	O
system	O
is	O
deterministic	O
.	O
if	O
it	O
starts	O
in	O
a1	O
,	O
then	O
it	O
transitions	O
to	O
b	O
with	O
a	O
reward	O
of	O
0	O
and	O
then	O
on	O
to	O
termination	O
with	O
a	O
reward	O
of	O
1.	O
if	O
it	O
starts	O
in	O
a2	O
,	O
then	O
it	O
transitions	O
to	O
c	O
,	O
and	O
then	O
to	O
termination	O
,	O
with	O
both	O
rewards	O
zero	O
.	O
to	O
a	O
learning	O
algorithm	O
,	O
seeing	O
only	O
the	O
features	O
,	O
the	O
system	O
looks	O
identical	O
to	O
the	O
a-split	O
example	O
.	O
the	O
system	O
seems	O
to	O
always	O
start	O
in	O
a	O
,	O
followed	O
by	O
either	O
b	O
or	O
c	O
with	O
equal	O
probability	O
,	O
and	O
then	O
terminating	O
with	O
a	O
1	O
or	O
a	O
0	O
depending	O
deterministically	O
on	O
the	O
previous	O
state	B
.	O
as	O
in	O
the	O
a-split	O
example	O
,	O
the	O
true	O
values	O
of	O
b	O
and	O
c	O
are	O
1	O
and	O
0	O
,	O
and	O
the	O
best	O
shared	O
value	B
of	O
a1	O
and	O
a2	O
is	O
1	O
2	O
,	O
by	O
symmetry	O
.	O
because	O
this	O
problem	O
appears	O
externally	O
identical	O
to	O
the	O
a-split	O
example	O
,	O
we	O
already	O
know	O
what	O
values	O
will	O
be	O
found	O
by	O
the	O
algorithms	O
.	O
semi-gradient	O
td	O
con-	O
verges	O
to	O
the	O
ideal	O
values	O
just	O
mentioned	O
,	O
while	O
the	O
naive	B
residual-gradient	O
algorithm	O
converges	O
to	O
values	O
of	O
3	O
4	O
for	O
b	O
and	O
c	O
respectively	O
.	O
all	O
state	B
transitions	O
are	O
de-	O
terministic	O
,	O
so	O
the	O
non-naive	O
residual-gradient	B
algorithm	I
will	O
also	O
converge	O
to	O
these	O
values	O
(	O
it	O
is	O
the	O
same	O
algorithm	O
in	O
this	O
case	O
)	O
.	O
it	O
follows	O
then	O
that	O
this	O
‘	O
naive	B
’	O
solu-	O
tion	B
must	O
also	O
be	O
the	O
one	O
that	O
minimizes	O
the	O
be	O
,	O
and	O
so	O
it	O
is	O
.	O
on	O
a	O
deterministic	O
problem	O
,	O
the	O
bellman	O
errors	O
and	O
td	O
errors	O
are	O
all	O
the	O
same	O
,	O
so	O
the	O
be	O
is	O
always	O
the	O
same	O
as	O
the	O
tde	O
.	O
optimizing	O
the	O
be	O
on	O
this	O
example	O
gives	O
rise	O
to	O
the	O
same	O
failure	O
mode	O
as	O
with	O
the	O
naive	B
residual-gradient	O
algorithm	O
on	O
the	O
a-split	O
example	O
.	O
4	O
and	O
1	O
a1bc0001a2a	O
276	O
chapter	O
11	O
:	O
*oﬀ-policy	O
methods	O
with	B
approximation	I
then	O
the	O
residual-gradient	B
algorithm	I
,	O
and	O
indeed	O
the	O
be	O
objective	O
,	O
seem	O
to	O
ﬁnd	O
the	O
wrong	O
value	B
functions	O
.	O
one	O
of	O
the	O
most	O
telling	O
such	O
examples	O
is	O
the	O
variation	O
on	O
the	O
a-split	O
example	O
known	O
as	O
the	O
a-presplit	O
example	O
,	O
shown	O
on	O
the	O
preceding	O
page	O
,	O
in	O
which	O
the	O
residual-gradient	B
algorithm	I
ﬁnds	O
the	O
same	O
poor	O
solution	O
as	O
its	O
naive	B
version	O
.	O
this	O
example	O
shows	O
intuitively	O
that	O
minimizing	O
the	O
be	O
(	O
which	O
the	O
residual-gradient	B
algorithm	I
surely	O
does	O
)	O
may	O
not	O
be	O
a	O
desirable	O
goal	B
.	O
the	O
third	O
way	O
in	O
which	O
the	O
convergence	O
of	O
the	O
residual-gradient	B
algorithm	I
is	O
not	O
satisfactory	O
is	O
explained	O
in	O
the	O
next	O
section	O
.	O
like	O
the	O
second	O
way	O
,	O
the	O
third	O
way	O
is	O
also	O
a	O
problem	O
with	O
the	O
be	O
objective	O
itself	O
rather	O
than	O
with	O
any	O
particular	O
algorithm	O
for	O
achieving	O
it	O
.	O
11.6	O
the	O
bellman	O
error	O
is	O
not	O
learnable	O
the	O
concept	O
of	O
learnability	O
that	O
we	O
introduce	O
in	O
this	O
section	O
is	O
diﬀerent	O
from	O
that	O
com-	O
monly	O
used	O
in	O
machine	O
learning	O
.	O
there	O
,	O
a	O
hypothesis	O
is	O
said	O
to	O
be	O
“	O
learnable	O
”	O
if	O
it	O
is	O
eﬃciently	O
learnable	O
,	O
meaning	O
that	O
it	O
can	O
be	O
learned	O
within	O
a	O
polynomial	O
rather	O
than	O
an	O
exponential	O
number	O
of	O
examples	O
.	O
here	O
we	O
use	O
the	O
term	O
in	O
a	O
more	O
basic	O
way	O
,	O
to	O
mean	O
learnable	O
at	O
all	O
,	O
with	O
any	O
amount	O
of	O
experience	O
.	O
it	O
turns	O
out	O
many	O
quantities	O
of	O
apparent	O
interest	O
in	O
reinforcement	B
learning	I
can	O
not	O
be	O
learned	O
even	O
from	O
an	O
inﬁnite	O
amount	O
of	O
ex-	O
periential	O
data	O
.	O
these	O
quantities	O
are	O
well	O
deﬁned	O
and	O
can	O
be	O
computed	O
given	O
knowledge	O
of	O
the	O
internal	O
structure	O
of	O
the	O
environment	B
,	O
but	O
can	O
not	O
be	O
computed	O
or	O
estimated	O
from	O
the	O
observed	O
sequence	O
of	O
feature	O
vectors	O
,	O
actions	O
,	O
and	O
rewards.2	O
we	O
say	O
that	O
they	O
are	O
not	O
learnable	O
.	O
it	O
will	O
turn	O
out	O
that	O
the	O
bellman	O
error	O
objective	O
(	O
be	O
)	O
introduced	O
in	O
the	O
last	O
two	O
sections	O
is	O
not	O
learnable	O
in	O
this	O
sense	O
.	O
that	O
the	O
bellman	O
error	O
objective	O
can	O
not	O
be	O
learned	O
from	O
the	O
observable	O
data	O
is	O
probably	O
the	O
strongest	O
reason	O
not	O
to	O
seek	O
it	O
.	O
to	O
make	O
the	O
concept	O
of	O
learnability	O
clear	O
,	O
let	O
’	O
s	O
start	O
with	O
some	O
simple	O
examples	O
.	O
consider	O
the	O
two	O
markov	O
reward	O
processes3	O
(	O
mrps	O
)	O
diagrammed	O
below	O
:	O
where	O
two	O
edges	O
leave	O
a	O
state	B
,	O
both	O
transitions	O
are	O
assumed	O
to	O
occur	O
with	O
equal	O
prob-	O
ability	O
,	O
and	O
the	O
numbers	O
indicate	O
the	O
reward	O
received	O
.	O
all	O
the	O
states	O
appear	O
the	O
same	O
;	O
they	O
all	O
produce	O
the	O
same	O
single-component	O
feature	O
vector	O
x	O
=	O
1	O
and	O
have	O
approximated	O
value	B
w.	O
thus	O
,	O
the	O
only	O
varying	O
part	O
of	O
the	O
data	O
trajectory	O
is	O
the	O
reward	O
sequence	O
.	O
the	O
left	O
mrp	O
stays	O
in	O
the	O
same	O
state	B
and	O
emits	O
an	O
endless	O
stream	O
of	O
0s	O
and	O
2s	O
at	O
random	O
,	O
each	O
with	O
0.5	O
probability	O
.	O
the	O
right	O
mrp	O
,	O
on	O
every	O
step	O
,	O
either	O
stays	O
in	O
its	O
current	O
state	B
or	O
switches	O
to	O
the	O
other	O
,	O
with	O
equal	O
probability	O
.	O
the	O
reward	O
is	O
deterministic	O
in	O
this	O
mrp	O
,	O
always	O
a	O
0	O
from	O
one	O
state	B
and	O
always	O
a	O
2	O
from	O
the	O
other	O
,	O
but	O
because	O
the	O
each	O
state	B
is	O
2they	O
would	O
of	O
course	O
be	O
estimated	O
if	O
the	O
state	B
sequence	O
were	O
observed	O
rather	O
than	O
only	O
the	O
corre-	O
sponding	O
feature	O
vectors	O
.	O
3all	O
mrps	O
can	O
be	O
considered	O
mdps	O
with	O
a	O
single	O
action	B
in	O
all	O
states	O
;	O
what	O
we	O
conclude	O
about	O
mrps	O
here	O
applies	O
as	O
well	O
to	O
mdps	O
.	O
020220www	O
11.6.	O
the	O
bellman	O
error	O
is	O
not	O
learnable	O
277	O
equally	O
likely	O
on	O
each	O
step	O
,	O
the	O
observable	O
data	O
is	O
again	O
an	O
endless	O
stream	O
of	O
0s	O
and	O
2s	O
at	O
random	O
,	O
identical	O
to	O
that	O
produced	O
by	O
the	O
left	O
mrp	O
.	O
(	O
we	O
can	O
assume	O
the	O
right	O
mrp	O
starts	O
in	O
one	O
of	O
two	O
states	O
at	O
random	O
with	O
equal	O
probability	O
.	O
)	O
thus	O
,	O
even	O
given	O
even	O
an	O
inﬁnite	O
amount	O
of	O
data	O
,	O
it	O
would	O
not	O
be	O
possible	O
to	O
tell	O
which	O
of	O
these	O
two	O
mrps	O
was	O
generating	O
it	O
.	O
in	O
particular	O
,	O
we	O
could	O
not	O
tell	O
if	O
the	O
mrp	O
has	O
one	O
state	B
or	O
two	O
,	O
is	O
stochastic	O
or	O
deterministic	O
.	O
these	O
things	O
are	O
not	O
learnable	O
.	O
this	O
pair	O
of	O
mrps	O
also	O
illustrates	O
that	O
the	O
ve	O
objective	O
(	O
9.1	O
)	O
is	O
not	O
learnable	O
.	O
if	O
γ	O
=	O
0	O
,	O
then	O
the	O
true	O
values	O
of	O
the	O
three	O
states	O
(	O
in	O
both	O
mrps	O
)	O
,	O
left	O
to	O
right	O
,	O
are	O
1	O
,	O
0	O
,	O
and	O
2.	O
suppose	O
w	O
=	O
1.	O
then	O
the	O
ve	O
is	O
0	O
for	O
the	O
left	O
mrp	O
and	O
1	O
for	O
the	O
right	O
mrp	O
.	O
because	O
the	O
ve	O
is	O
diﬀerent	O
in	O
the	O
two	O
problems	O
,	O
yet	O
the	O
data	O
generated	O
has	O
the	O
same	O
distribution	O
,	O
the	O
ve	O
can	O
not	O
be	O
learned	O
.	O
the	O
ve	O
is	O
not	O
a	O
unique	O
function	O
of	O
the	O
data	O
distribution	O
.	O
and	O
if	O
it	O
can	O
not	O
be	O
learned	O
,	O
then	O
how	O
could	O
the	O
ve	O
possibly	O
be	O
useful	O
as	O
an	O
objective	O
for	O
learning	O
?	O
if	O
an	O
objective	O
can	O
not	O
be	O
learned	O
,	O
it	O
does	O
indeed	O
draw	O
its	O
utility	O
into	O
question	O
.	O
in	O
the	O
case	O
of	O
the	O
ve	O
,	O
however	O
,	O
there	O
is	O
a	O
way	O
out	O
.	O
note	O
that	O
the	O
same	O
solution	O
,	O
w	O
=	O
1	O
,	O
is	O
optimal	O
for	O
both	O
mrps	O
above	O
(	O
assuming	O
µ	O
is	O
the	O
same	O
for	O
the	O
two	O
indistinguishable	O
states	O
in	O
the	O
right	O
mrp	O
)	O
.	O
is	O
this	O
a	O
coincidence	O
,	O
or	O
could	O
it	O
be	O
generally	O
true	O
that	O
all	O
mdps	O
with	O
the	O
same	O
data	O
distribution	O
also	O
have	O
the	O
same	O
optimal	O
parameter	O
vector	B
?	O
if	O
this	O
is	O
true—and	O
we	O
will	O
show	O
next	O
that	O
it	O
is—then	O
the	O
ve	O
remains	O
a	O
usable	O
objective	O
.	O
the	O
ve	O
is	O
not	O
learnable	O
,	O
but	O
the	O
parameter	O
that	O
optimizes	O
it	O
is	O
!	O
to	O
understand	O
this	O
,	O
it	O
is	O
useful	O
to	O
bring	O
in	O
another	O
natural	O
objective	O
function	O
,	O
this	O
time	O
one	O
that	O
is	O
clearly	O
learnable	O
.	O
one	O
error	O
that	O
is	O
always	O
observable	O
is	O
that	O
between	O
the	O
value	B
estimate	O
at	O
each	O
time	O
and	O
the	O
return	O
from	O
that	O
time	O
.	O
the	O
mean	O
square	O
return	B
error	O
,	O
denoted	O
re	O
,	O
is	O
the	O
expectation	O
,	O
under	O
µ	O
,	O
of	O
the	O
square	O
of	O
this	O
error	O
.	O
in	O
the	O
on-policy	O
case	O
the	O
re	O
can	O
be	O
written	O
re	O
(	O
w	O
)	O
=	O
e	O
(	O
cid:104	O
)	O
(	O
cid:0	O
)	O
gt	O
−	O
ˆv	O
(	O
st	O
,	O
w	O
)	O
(	O
cid:1	O
)	O
2	O
(	O
cid:105	O
)	O
=	O
ve	O
(	O
w	O
)	O
+	O
e	O
(	O
cid:104	O
)	O
(	O
cid:0	O
)	O
gt	O
−	O
vπ	O
(	O
st	O
)	O
(	O
cid:1	O
)	O
2	O
(	O
cid:105	O
)	O
.	O
(	O
11.24	O
)	O
thus	O
,	O
the	O
two	O
objectives	O
are	O
the	O
same	O
except	O
for	O
a	O
variance	O
term	O
that	O
does	O
not	O
depend	O
on	O
the	O
parameter	O
vector	O
.	O
the	O
two	O
objectives	O
must	O
therefore	O
have	O
the	O
same	O
optimal	O
param-	O
eter	O
value	B
w∗	O
.	O
the	O
overall	O
relationships	O
are	O
summarized	O
in	O
the	O
left	O
side	O
of	O
figure	O
11.4	O
.	O
∗exercise	O
11.4	O
prove	O
(	O
11.24	O
)	O
.	O
hint	O
:	O
write	O
the	O
re	O
as	O
an	O
expectation	O
over	O
possible	O
states	O
s	O
of	O
the	O
expectation	O
of	O
the	O
squared	O
error	O
given	O
that	O
st	O
=	O
s.	O
then	O
add	O
and	O
subtract	O
the	O
true	O
value	O
of	O
state	O
s	O
from	O
the	O
error	O
(	O
before	O
squaring	O
)	O
,	O
grouping	O
the	O
subtracted	O
true	O
value	O
with	O
the	O
return	B
and	O
the	O
added	O
true	O
value	O
with	O
the	O
estimated	O
value	B
.	O
then	O
,	O
if	O
you	O
expand	O
the	O
square	O
,	O
the	O
most	O
complex	O
term	O
will	O
end	O
up	O
being	O
zero	O
,	O
leaving	O
you	O
with	O
(	O
11.24	O
)	O
.	O
(	O
cid:3	O
)	O
now	O
let	O
us	O
return	B
to	O
the	O
be	O
.	O
the	O
be	O
is	O
like	O
the	O
ve	O
in	O
that	O
it	O
can	O
be	O
computed	O
from	O
knowledge	O
of	O
the	O
mdp	O
but	O
is	O
not	O
learnable	O
from	O
data	O
.	O
but	O
it	O
is	O
not	O
like	O
the	O
ve	O
in	O
that	O
its	O
minimum	O
solution	O
is	O
not	O
learnable	O
.	O
the	O
box	O
on	O
the	O
next	O
page	O
presents	O
a	O
counterexample—two	O
mrps	O
that	O
generate	O
the	O
same	O
data	O
distribution	O
but	O
whose	O
mini-	O
mizing	O
parameter	O
vector	O
is	O
diﬀerent	O
,	O
proving	O
that	O
the	O
optimal	O
parameter	O
vector	B
is	O
not	O
a	O
function	O
of	O
the	O
data	O
and	O
thus	O
can	O
not	O
be	O
learned	O
from	O
it	O
.	O
the	O
other	O
bootstrapping	B
ob-	O
jectives	O
that	O
we	O
have	O
considered	O
,	O
the	O
pbe	O
and	O
tde	O
,	O
can	O
be	O
determined	O
from	O
data	O
(	O
are	O
278	O
chapter	O
11	O
:	O
*oﬀ-policy	O
methods	O
with	B
approximation	I
example	O
11.4	O
:	O
counterexample	O
to	O
the	O
learnability	B
of	I
the	O
bellman	O
error	O
to	O
show	O
the	O
full	O
range	O
of	O
possibilities	O
we	O
need	O
a	O
slightly	O
more	O
complex	O
pair	O
of	O
markov	O
reward	O
processes	O
(	O
mrps	O
)	O
than	O
those	O
considered	O
earlier	O
.	O
consider	O
the	O
following	O
two	O
mrps	O
:	O
where	O
two	O
edges	O
leave	O
a	O
state	B
,	O
both	O
transitions	O
are	O
assumed	O
to	O
occur	O
with	O
equal	O
probability	O
,	O
and	O
the	O
numbers	O
indicate	O
the	O
reward	O
received	O
.	O
the	O
mrp	O
on	O
the	O
left	O
has	O
two	O
states	O
that	O
are	O
represented	O
distinctly	O
.	O
the	O
mrp	O
on	O
the	O
right	O
has	O
three	O
states	O
,	O
two	O
of	O
which	O
,	O
b	O
and	O
b	O
(	O
cid:48	O
)	O
,	O
appear	O
the	O
same	O
and	O
must	O
be	O
given	O
the	O
same	O
approximate	B
value	O
.	O
speciﬁcally	O
,	O
w	O
has	O
two	O
components	O
and	O
the	O
value	O
of	O
state	O
a	O
is	O
given	O
by	O
the	O
ﬁrst	O
component	O
and	O
the	O
value	O
of	O
b	O
and	O
b	O
(	O
cid:48	O
)	O
is	O
given	O
by	O
the	O
second	O
.	O
the	O
second	O
mrp	O
has	O
been	O
designed	O
so	O
that	O
equal	O
time	O
is	O
spent	O
in	O
all	O
three	O
states	O
,	O
so	O
we	O
can	O
take	O
µ	O
(	O
s	O
)	O
=	O
1	O
3	O
,	O
for	O
all	O
s.	O
note	O
that	O
the	O
observable	O
data	O
distribution	O
is	O
identical	O
for	O
the	O
two	O
mrps	O
.	O
in	O
both	O
cases	O
the	O
agent	O
will	O
see	O
single	O
occurrences	O
of	O
a	O
followed	O
by	O
a	O
0	O
,	O
then	O
some	O
number	O
of	O
apparent	O
bs	O
,	O
each	O
followed	O
by	O
a	O
−1	O
except	O
the	O
last	O
,	O
which	O
is	O
followed	O
by	O
a	O
1	O
,	O
then	O
we	O
start	O
all	O
over	O
again	O
with	O
a	O
single	O
a	O
and	O
a	O
0	O
,	O
etc	O
.	O
all	O
the	O
statistical	O
details	O
are	O
the	O
same	O
as	O
well	O
;	O
in	O
both	O
mrps	O
,	O
the	O
probability	O
of	O
a	O
string	O
of	O
k	O
bs	O
is	O
2−k	O
.	O
now	O
suppose	O
w	O
=	O
0.	O
in	O
the	O
ﬁrst	O
mrp	O
,	O
this	O
is	O
an	O
exact	O
solution	O
,	O
and	O
the	O
be	O
is	O
zero	O
.	O
in	O
the	O
second	O
mrp	O
,	O
this	O
solution	O
produces	O
a	O
squared	O
error	O
in	O
both	O
b	O
and	O
b	O
(	O
cid:48	O
)	O
of	O
1	O
,	O
such	O
that	O
be	O
=	O
µ	O
(	O
b	O
)	O
1	O
+	O
µ	O
(	O
b	O
(	O
cid:48	O
)	O
)	O
1	O
=	O
2	O
3	O
.	O
these	O
two	O
mrps	O
,	O
which	O
generate	O
the	O
same	O
data	O
distribution	O
,	O
have	O
diﬀerent	O
bes	O
;	O
the	O
be	O
is	O
not	O
learnable	O
.	O
moreover	O
(	O
and	O
unlike	O
the	O
earlier	O
example	O
for	O
the	O
ve	O
)	O
the	O
minimizing	O
value	B
of	O
w	O
is	O
diﬀerent	O
for	O
the	O
two	O
mrps	O
.	O
for	O
the	O
ﬁrst	O
mrp	O
,	O
w	O
=	O
0	O
minimizes	O
the	O
be	O
for	O
any	O
γ.	O
for	O
the	O
second	O
mrp	O
,	O
the	O
minimizing	O
w	O
is	O
a	O
complicated	O
function	O
of	O
γ	O
,	O
but	O
in	O
the	O
limit	O
,	O
as	O
γ	O
→	O
1	O
,	O
it	O
is	O
(	O
−	O
1	O
2	O
,	O
0	O
)	O
(	O
cid:62	O
)	O
.	O
thus	O
the	O
solution	O
that	O
minimizes	O
be	O
can	O
not	O
be	O
estimated	O
from	O
data	O
alone	O
;	O
knowledge	O
of	O
the	O
mrp	O
beyond	O
what	O
is	O
revealed	O
in	O
the	O
data	O
is	O
required	O
.	O
in	O
this	O
sense	O
,	O
it	O
is	O
impossible	O
in	O
principle	O
to	O
pursue	O
the	O
be	O
as	O
an	O
objective	O
for	O
learning	O
.	O
it	O
may	O
be	O
surprising	O
that	O
in	O
the	O
second	O
mrp	O
the	O
be-minimizing	O
value	B
of	O
a	O
is	O
so	O
far	O
from	O
zero	O
.	O
recall	O
that	O
a	O
has	O
a	O
dedicated	O
weight	O
and	O
thus	O
its	O
value	B
is	O
unconstrained	O
by	O
function	B
approximation	I
.	O
a	O
is	O
followed	O
by	O
a	O
reward	O
of	O
0	O
and	O
transition	O
to	O
a	O
state	B
with	O
a	O
value	B
of	O
nearly	O
0	O
,	O
which	O
suggests	O
vw	O
(	O
a	O
)	O
should	O
be	O
0	O
;	O
why	O
is	O
its	O
optimal	O
value	O
substantially	O
negative	O
rather	O
than	O
0	O
?	O
the	O
answer	O
is	O
that	O
making	O
vw	O
(	O
a	O
)	O
negative	O
re-	O
duces	O
the	O
error	O
upon	O
arriving	O
in	O
a	O
from	O
b.	O
the	O
reward	O
on	O
this	O
deterministic	O
transition	O
is	O
1	O
,	O
which	O
implies	O
that	O
b	O
should	O
have	O
a	O
value	B
1	O
more	O
than	O
a.	O
because	O
b	O
’	O
s	O
value	B
is	O
approximately	O
zero	O
,	O
a	O
’	O
s	O
value	B
is	O
driven	O
toward	O
−1	O
.	O
the	O
be-minimizing	O
value	B
of	O
≈	O
−	O
1	O
for	O
a	O
is	O
a	O
compromise	O
between	O
reducing	O
the	O
errors	O
on	O
leaving	O
and	O
on	O
entering	O
a	O
.	O
2	O
ba10-1ba0-1b	O
(	O
cid:1	O
)	O
01-1	O
11.6.	O
the	O
bellman	O
error	O
is	O
not	O
learnable	O
279	O
figure	O
11.4	O
:	O
causal	O
relationships	O
among	O
the	O
data	O
distribution	O
,	O
mdps	O
,	O
and	O
various	O
objectives	O
.	O
left	O
,	O
monte	O
carlo	O
objectives	O
:	O
two	O
diﬀerent	O
mdps	O
can	O
produce	O
the	O
same	O
data	O
distribution	O
yet	O
also	O
produce	O
diﬀerent	O
ves	O
,	O
proving	O
that	O
the	O
ve	O
objective	O
can	O
not	O
be	O
determined	O
from	O
data	O
and	O
is	O
not	O
learnable	O
.	O
however	O
,	O
all	O
such	O
ves	O
must	O
have	O
the	O
same	O
optimal	O
parameter	O
vector	B
,	O
w∗	O
!	O
moreover	O
,	O
this	O
same	O
w∗	O
can	O
be	O
determined	O
from	O
another	O
objective	O
,	O
the	O
re	O
,	O
which	O
is	O
uniquely	O
determined	O
from	O
the	O
data	O
distribution	O
.	O
thus	O
w∗	O
and	O
the	O
re	O
are	O
learnable	O
even	O
though	O
the	O
ves	O
are	O
not	O
.	O
right	O
,	O
bootstrapping	B
objectives	O
:	O
two	O
diﬀerent	O
mdps	O
can	O
produce	O
the	O
same	O
data	O
distribution	O
yet	O
also	O
produce	O
diﬀerent	O
bes	O
and	O
have	O
diﬀerent	O
minimizing	O
parameter	O
vectors	O
;	O
these	O
are	O
not	O
learnable	O
from	O
the	O
data	O
distribution	O
.	O
the	O
pbe	O
and	O
tde	O
objectives	O
and	O
their	O
(	O
diﬀerent	O
)	O
minima	O
can	O
be	O
directly	O
determined	O
from	O
data	O
and	O
thus	O
are	O
learnable	O
.	O
learnable	O
)	O
and	O
determine	O
optimal	O
solutions	O
that	O
are	O
in	O
general	O
diﬀerent	O
from	O
each	O
other	O
and	O
the	O
be	O
minimums	O
.	O
the	O
general	O
case	O
is	O
summarized	O
in	O
the	O
right	O
side	O
of	O
figure	O
11.4.	O
thus	O
,	O
the	O
be	O
is	O
not	O
learnable	O
;	O
it	O
can	O
not	O
be	O
estimated	O
from	O
feature	O
vectors	O
and	O
other	O
observable	O
data	O
.	O
this	O
limits	O
the	O
be	O
to	O
model-based	O
settings	O
.	O
there	O
can	O
be	O
no	O
algorithm	O
that	O
minimizes	O
the	O
be	O
without	O
access	O
to	O
the	O
underlying	O
mdp	O
states	O
beyond	O
the	O
feature	O
vectors	O
.	O
the	O
residual-gradient	B
algorithm	I
is	O
only	O
able	O
to	O
minimize	O
be	O
because	O
it	O
is	O
allowed	O
to	O
double	B
sample	O
from	O
the	O
same	O
state—not	O
a	O
state	B
that	O
has	O
the	O
same	O
feature	O
vector	O
,	O
but	O
one	O
that	O
is	O
guaranteed	O
to	O
be	O
the	O
same	O
underlying	O
state	B
.	O
we	O
can	O
see	O
now	O
that	O
there	O
is	O
no	O
way	O
around	O
this	O
.	O
minimizing	O
the	O
be	O
requires	O
some	O
such	O
access	O
to	O
the	O
nominal	O
,	O
underlying	O
mdp	O
.	O
this	O
is	O
an	O
important	O
limitation	O
of	O
the	O
be	O
beyond	O
that	O
identiﬁed	O
in	O
the	O
a-presplit	O
example	O
on	O
page	O
275.	O
all	O
this	O
directs	O
more	O
attention	O
toward	O
the	O
pbe	O
.	O
mdp1mdp2msbe1msbe2policytogethercompletelydeterminetheprobabilitydistributionoverdatatrajectories.assumeforthemomentthatthestate	O
,	O
action	B
,	O
andrewardsetsareallﬁnite.then	O
,	O
foranyﬁnitesequence⇠= 0	O
,	O
a0	O
,	O
r1	O
,	O
...	O
,	O
rk	O
,	O
 k	O
,	O
thereisawelldeﬁnedprobability	O
(	O
pos-siblyzero	O
)	O
ofitoccuringastheinitialportionofatrajectory	O
,	O
whichwemaydenotedp	O
(	O
⇠	O
)	O
=pr	O
{	O
 	O
(	O
s0	O
)	O
= 0	O
,	O
a0=a0	O
,	O
r1=r1	O
,	O
...	O
,	O
rk=rk	O
,	O
 	O
(	O
sk	O
)	O
= k	O
}	O
.thedistributionpthenisacompletecharacterizationofasourceofdatatrajectories.toknowpistoknoweverythingaboutthestatisticsofthedata	O
,	O
butitisstilllessthanknowingthemdp.inparticular	O
,	O
theveandbeobjectivesarereadilycomputedfromthemdpasdescribedinsection3	O
,	O
butthesecannotbedeterminedfrompalone.✓1✓2✓3✓4be1be2mdp1mdp2✓⇤1✓⇤2✓⇤3✓⇤4be1be2mdp1mdp2tderevetheproblemcanbeseeninverysimple	O
,	O
pomdp-likeexamples	O
,	O
inwhichtheobservabledataproducedbytwodi↵erentmdpsisidenticalineveryrespect	O
,	O
yetthebeisdi↵erent.insuchacasethebeisliterallynotafunctionofthedata	O
,	O
andthusthereisnowaytoestimateitfromdata.oneofthesimplestexamplesisthepairofmdpsshownbelow	O
:	O
ba10-1ba0-1b	O
(	O
cid:1	O
)	O
01-1thesemdpshaveonlyoneaction	O
(	O
or	O
,	O
equivalently	O
,	O
noactions	O
)	O
,	O
sotheyareine↵ectmarkovchains.wheretwoedgesleaveastate	O
,	O
bothpossibilitiesareassumedtooccurwithequalprobability.thenumbersontheedgesindicatetherewardemittedifthatedgeistraversed.themdponthelefthastwostatesthatarerepresenteddistinctly	O
;	O
eachhasaseparateweightsothattheycantakeonanyvalue.themdpontherighthasthreestates	O
,	O
twoofwhich	O
,	O
bandb0	O
,	O
arerepresentedidenticallyandmustbegiventhesameapproximatevalue.wecanimaginethatthevalueofstateaisgivenbytheﬁrstcomponentof✓andthevalueofbandb0isgivenbythesecond.noticethattheobservabledataisidenticalforthetwomdps.inbothcasestheagentwillseesingleoccurrencesofafollowedbya0	O
,	O
thensomenumberofbseachfollowedbya 1	O
,	O
exceptthelastwhichisfollowedbya1	O
,	O
thenwestartalloveragainwithasingleaanda0	O
,	O
etc.allthedetailsarethesameaswell	O
;	O
inbothmdps	O
,	O
theprobabilityofastringofkbsis2 k.nowconsiderthevaluefunctionv✓=~0.intheﬁrstmdp	O
,	O
thisisanexactsolution	O
,	O
andtheoverallbeiszero.inthesecondmdp	O
,	O
thissolutionproducesanerrorinbothbandb0of1	O
,	O
foranoverallbeofpd	O
(	O
b	O
)	O
+d	O
(	O
b0	O
)	O
,	O
orp2/3ifthethreestatesareequallyweightedbyd.thetwomdps	O
,	O
whichgeneratethesamedata	O
,	O
havedi↵erentbes.thus	O
,	O
thebecannotbeestimatedfromdataalone	O
;	O
knowledgeofthemdpbeyondwhatisrevealedinthedataisrequired.moreover	O
,	O
thetwomdpshavedi↵erentminimal-bevaluefunctions.2fortheﬁrstmdp	O
,	O
theminimal-bevaluefunctionistheexactsolutionv✓=~0forany .forthesecondmdp,2.thisisacriticalobservation	O
,	O
asitispossibleforanerrorfunctiontobeunobservableandyetstillbeperfectlysatisfactoryforuseinlearningsettingsbecausethevaluethatminimizesitcanbedeterminedfromdata.forexample	O
,	O
thisiswhathappenswiththeve.theveisnotobservablefromdata	O
,	O
butits20policytogethercompletelydeterminetheprobabilitydistributionoverdatatrajectories.assumeforthemomentthatthestate	O
,	O
action	B
,	O
andrewardsetsareallﬁnite.then	O
,	O
foranyﬁnitesequence⇠= 0	O
,	O
a0	O
,	O
r1	O
,	O
...	O
,	O
rk	O
,	O
 k	O
,	O
thereisawelldeﬁnedprobability	O
(	O
pos-siblyzero	O
)	O
ofitoccuringastheinitialportionofatrajectory	O
,	O
whichwemaydenotedp	O
(	O
⇠	O
)	O
=pr	O
{	O
 	O
(	O
s0	O
)	O
= 0	O
,	O
a0=a0	O
,	O
r1=r1	O
,	O
...	O
,	O
rk=rk	O
,	O
 	O
(	O
sk	O
)	O
= k	O
}	O
.thedistributionpthenisacompletecharacterizationofasourceofdatatrajectories.toknowpistoknoweverythingaboutthestatisticsofthedata	O
,	O
butitisstilllessthanknowingthemdp.inparticular	O
,	O
theveandbeobjectivesarereadilycomputedfromthemdpasdescribedinsection3	O
,	O
butthesecannotbedeterminedfrompalone.✓1✓2✓3✓4be1be2mdp1mdp2✓⇤1✓⇤2✓⇤3✓⇤4be1be2mdp1mdp2tderevetheproblemcanbeseeninverysimple	O
,	O
pomdp-likeexamples	O
,	O
inwhichtheobservabledataproducedbytwodi↵erentmdpsisidenticalineveryrespect	O
,	O
yetthebeisdi↵erent.insuchacasethebeisliterallynotafunctionofthedata	O
,	O
andthusthereisnowaytoestimateitfromdata.oneofthesimplestexamplesisthepairofmdpsshownbelow	O
:	O
ba10-1ba0-1b	O
(	O
cid:1	O
)	O
01-1thesemdpshaveonlyoneaction	O
(	O
or	O
,	O
equivalently	O
,	O
noactions	O
)	O
,	O
sotheyareine↵ectmarkovchains.wheretwoedgesleaveastate	O
,	O
bothpossibilitiesareassumedtooccurwithequalprobability.thenumbersontheedgesindicatetherewardemittedifthatedgeistraversed.themdponthelefthastwostatesthatarerepresenteddistinctly	O
;	O
eachhasaseparateweightsothattheycantakeonanyvalue.themdpontherighthasthreestates	O
,	O
twoofwhich	O
,	O
bandb0	O
,	O
arerepresentedidenticallyandmustbegiventhesameapproximatevalue.wecanimaginethatthevalueofstateaisgivenbytheﬁrstcomponentof✓andthevalueofbandb0isgivenbythesecond.noticethattheobservabledataisidenticalforthetwomdps.inbothcasestheagentwillseesingleoccurrencesofafollowedbya0	O
,	O
thensomenumberofbseachfollowedbya 1	O
,	O
exceptthelastwhichisfollowedbya1	O
,	O
thenwestartalloveragainwithasingleaanda0	O
,	O
etc.allthedetailsarethesameaswell	O
;	O
inbothmdps	O
,	O
theprobabilityofastringofkbsis2 k.nowconsiderthevaluefunctionv✓=~0.intheﬁrstmdp	O
,	O
thisisanexactsolution	O
,	O
andtheoverallbeiszero.inthesecondmdp	O
,	O
thissolutionproducesanerrorinbothbandb0of1	O
,	O
foranoverallbeofpd	O
(	O
b	O
)	O
+d	O
(	O
b0	O
)	O
,	O
orp2/3ifthethreestatesareequallyweightedbyd.thetwomdps	O
,	O
whichgeneratethesamedata	O
,	O
havedi↵erentbes.thus	O
,	O
thebecannotbeestimatedfromdataalone	O
;	O
knowledgeofthemdpbeyondwhatisrevealedinthedataisrequired.moreover	O
,	O
thetwomdpshavedi↵erentminimal-bevaluefunctions.2fortheﬁrstmdp	O
,	O
theminimal-bevaluefunctionistheexactsolutionv✓=~0forany .forthesecondmdp,2.thisisacriticalobservation	O
,	O
asitispossibleforanerrorfunctiontobeunobservableandyetstillbeperfectlysatisfactoryforuseinlearningsettingsbecausethevaluethatminimizesitcanbedeterminedfromdata.forexample	O
,	O
thisiswhathappenswiththeve.theveisnotobservablefromdata	O
,	O
butits20mspbepolicytogethercompletelydeterminetheprobabilitydistributionoverdatatrajectories.assumeforthemomentthatthestate	O
,	O
action	B
,	O
andrewardsetsareallﬁnite.then	O
,	O
foranyﬁnitesequence⇠= 0	O
,	O
a0	O
,	O
r1	O
,	O
...	O
,	O
rk	O
,	O
 k	O
,	O
thereisawelldeﬁnedprobability	O
(	O
pos-siblyzero	O
)	O
ofitoccuringastheinitialportionofatrajectory	O
,	O
whichwemaydenotedp	O
(	O
⇠	O
)	O
=pr	O
{	O
 	O
(	O
s0	O
)	O
= 0	O
,	O
a0=a0	O
,	O
r1=r1	O
,	O
...	O
,	O
rk=rk	O
,	O
 	O
(	O
sk	O
)	O
= k	O
}	O
.thedistributionpthenisacompletecharacterizationofasourceofdatatrajectories.toknowpistoknoweverythingaboutthestatisticsofthedata	O
,	O
butitisstilllessthanknowingthemdp.inparticular	O
,	O
theveandbeobjectivesarereadilycomputedfromthemdpasdescribedinsection3	O
,	O
butthesecannotbedeterminedfrompalone.✓1✓2✓3✓4be1be2mdp1mdp2✓⇤1✓⇤2✓⇤3✓⇤4be1be2mdp1mdp2tderevetheproblemcanbeseeninverysimple	O
,	O
pomdp-likeexamples	O
,	O
inwhichtheobservabledataproducedbytwodi↵erentmdpsisidenticalineveryrespect	O
,	O
yetthebeisdi↵erent.insuchacasethebeisliterallynotafunctionofthedata	O
,	O
andthusthereisnowaytoestimateitfromdata.oneofthesimplestexamplesisthepairofmdpsshownbelow	O
:	O
ba10-1ba0-1b	O
(	O
cid:1	O
)	O
01-1thesemdpshaveonlyoneaction	O
(	O
or	O
,	O
equivalently	O
,	O
noactions	O
)	O
,	O
sotheyareine↵ectmarkovchains.wheretwoedgesleaveastate	O
,	O
bothpossibilitiesareassumedtooccurwithequalprobability.thenumbersontheedgesindicatetherewardemittedifthatedgeistraversed.themdponthelefthastwostatesthatarerepresenteddistinctly	O
;	O
eachhasaseparateweightsothattheycantakeonanyvalue.themdpontherighthasthreestates	O
,	O
twoofwhich	O
,	O
bandb0	O
,	O
arerepresentedidenticallyandmustbegiventhesameapproximatevalue.wecanimaginethatthevalueofstateaisgivenbytheﬁrstcomponentof✓andthevalueofbandb0isgivenbythesecond.noticethattheobservabledataisidenticalforthetwomdps.inbothcasestheagentwillseesingleoccurrencesofafollowedbya0	O
,	O
thensomenumberofbseachfollowedbya 1	O
,	O
exceptthelastwhichisfollowedbya1	O
,	O
thenwestartalloveragainwithasingleaanda0	O
,	O
etc.allthedetailsarethesameaswell	O
;	O
inbothmdps	O
,	O
theprobabilityofastringofkbsis2 k.nowconsiderthevaluefunctionv✓=~0.intheﬁrstmdp	O
,	O
thisisanexactsolution	O
,	O
andtheoverallbeiszero.inthesecondmdp	O
,	O
thissolutionproducesanerrorinbothbandb0of1	O
,	O
foranoverallbeofpd	O
(	O
b	O
)	O
+d	O
(	O
b0	O
)	O
,	O
orp2/3ifthethreestatesareequallyweightedbyd.thetwomdps	O
,	O
whichgeneratethesamedata	O
,	O
havedi↵erentbes.thus	O
,	O
thebecannotbeestimatedfromdataalone	O
;	O
knowledgeofthemdpbeyondwhatisrevealedinthedataisrequired.moreover	O
,	O
thetwomdpshavedi↵erentminimal-bevaluefunctions.2fortheﬁrstmdp	O
,	O
theminimal-bevaluefunctionistheexactsolutionv✓=~0forany .forthesecondmdp,2.thisisacriticalobservation	O
,	O
asitispossibleforanerrorfunctiontobeunobservableandyetstillbeperfectlysatisfactoryforuseinlearningsettingsbecausethevaluethatminimizesitcanbedeterminedfromdata.forexample	O
,	O
thisiswhathappenswiththeve.theveisnotobservablefromdata	O
,	O
butits20mstdepolicytogethercompletelydeterminetheprobabilitydistributionoverdatatrajectories.assumeforthemomentthatthestate	O
,	O
action	B
,	O
andrewardsetsareallﬁnite.then	O
,	O
foranyﬁnitesequence⇠= 0	O
,	O
a0	O
,	O
r1	O
,	O
...	O
,	O
rk	O
,	O
 k	O
,	O
thereisawelldeﬁnedprobability	O
(	O
pos-siblyzero	O
)	O
ofitoccuringastheinitialportionofatrajectory	O
,	O
whichwemaydenotedp	O
(	O
⇠	O
)	O
=pr	O
{	O
 	O
(	O
s0	O
)	O
= 0	O
,	O
a0=a0	O
,	O
r1=r1	O
,	O
...	O
,	O
rk=rk	O
,	O
 	O
(	O
sk	O
)	O
= k	O
}	O
.thedistributionpthenisacompletecharacterizationofasourceofdatatrajectories.toknowpistoknoweverythingaboutthestatisticsofthedata	O
,	O
butitisstilllessthanknowingthemdp.inparticular	O
,	O
theveandbeobjectivesarereadilycomputedfromthemdpasdescribedinsection3	O
,	O
butthesecannotbedeterminedfrompalone.✓1✓2✓3✓4be1be2mdp1mdp2✓⇤1✓⇤2✓⇤3✓⇤4be1be2mdp1mdp2tderevetheproblemcanbeseeninverysimple	O
,	O
pomdp-likeexamples	O
,	O
inwhichtheobservabledataproducedbytwodi↵erentmdpsisidenticalineveryrespect	O
,	O
yetthebeisdi↵erent.insuchacasethebeisliterallynotafunctionofthedata	O
,	O
andthusthereisnowaytoestimateitfromdata.oneofthesimplestexamplesisthepairofmdpsshownbelow	O
:	O
ba10-1ba0-1b	O
(	O
cid:1	O
)	O
01-1thesemdpshaveonlyoneaction	O
(	O
or	O
,	O
equivalently	O
,	O
noactions	O
)	O
,	O
sotheyareine↵ectmarkovchains.wheretwoedgesleaveastate	O
,	O
bothpossibilitiesareassumedtooccurwithequalprobability.thenumbersontheedgesindicatetherewardemittedifthatedgeistraversed.themdponthelefthastwostatesthatarerepresenteddistinctly	O
;	O
eachhasaseparateweightsothattheycantakeonanyvalue.themdpontherighthasthreestates	O
,	O
twoofwhich	O
,	O
bandb0	O
,	O
arerepresentedidenticallyandmustbegiventhesameapproximatevalue.wecanimaginethatthevalueofstateaisgivenbytheﬁrstcomponentof✓andthevalueofbandb0isgivenbythesecond.noticethattheobservabledataisidenticalforthetwomdps.inbothcasestheagentwillseesingleoccurrencesofafollowedbya0	O
,	O
thensomenumberofbseachfollowedbya 1	O
,	O
exceptthelastwhichisfollowedbya1	O
,	O
thenwestartalloveragainwithasingleaanda0	O
,	O
etc.allthedetailsarethesameaswell	O
;	O
inbothmdps	O
,	O
theprobabilityofastringofkbsis2 k.nowconsiderthevaluefunctionv✓=~0.intheﬁrstmdp	O
,	O
thisisanexactsolution	O
,	O
andtheoverallbeiszero.inthesecondmdp	O
,	O
thissolutionproducesanerrorinbothbandb0of1	O
,	O
foranoverallbeofpd	O
(	O
b	O
)	O
+d	O
(	O
b0	O
)	O
,	O
orp2/3ifthethreestatesareequallyweightedbyd.thetwomdps	O
,	O
whichgeneratethesamedata	O
,	O
havedi↵erentbes.thus	O
,	O
thebecannotbeestimatedfromdataalone	O
;	O
knowledgeofthemdpbeyondwhatisrevealedinthedataisrequired.moreover	O
,	O
thetwomdpshavedi↵erentminimal-bevaluefunctions.2fortheﬁrstmdp	O
,	O
theminimal-bevaluefunctionistheexactsolutionv✓=~0forany .forthesecondmdp,2.thisisacriticalobservation	O
,	O
asitispossibleforanerrorfunctiontobeunobservableandyetstillbeperfectlysatisfactoryforuseinlearningsettingsbecausethevaluethatminimizesitcanbedeterminedfromdata.forexample	O
,	O
thisiswhathappenswiththeve.theveisnotobservablefromdata	O
,	O
butits20datadistributionmdp1mdp2msve1msve2msredatadistributionpolicytogethercompletelydeterminetheprobabilitydistributionoverdatatrajectories.assumeforthemomentthatthestate	O
,	O
action	B
,	O
andrewardsetsareallﬁnite.then	O
,	O
foranyﬁnitesequence⇠= 0	O
,	O
a0	O
,	O
r1	O
,	O
...	O
,	O
rk	O
,	O
 k	O
,	O
thereisawelldeﬁnedprobability	O
(	O
pos-siblyzero	O
)	O
ofitoccuringastheinitialportionofatrajectory	O
,	O
whichwemaydenotedp	O
(	O
⇠	O
)	O
=pr	O
{	O
 	O
(	O
s0	O
)	O
= 0	O
,	O
a0=a0	O
,	O
r1=r1	O
,	O
...	O
,	O
rk=rk	O
,	O
 	O
(	O
sk	O
)	O
= k	O
}	O
.thedistributionpthenisacompletecharacterizationofasourceofdatatrajectories.toknowpistoknoweverythingaboutthestatisticsofthedata	O
,	O
butitisstilllessthanknowingthemdp.inparticular	O
,	O
theveandbeobjectivesarereadilycomputedfromthemdpasdescribedinsection3	O
,	O
butthesecannotbedeterminedfrompalone.✓1✓2✓3✓4be1be2mdp1mdp2pbe✓⇤✓⇤1✓⇤2✓⇤3✓⇤4be1be2mdp1mdp2tderevetheproblemcanbeseeninverysimple	O
,	O
pomdp-likeexamples	O
,	O
inwhichtheobservabledataproducedbytwodi↵erentmdpsisidenticalineveryrespect	O
,	O
yetthebeisdi↵erent.insuchacasethebeisliterallynotafunctionofthedata	O
,	O
andthusthereisnowaytoestimateitfromdata.oneofthesimplestexamplesisthepairofmdpsshownbelow	O
:	O
ba10-1ba0-1b	O
(	O
cid:1	O
)	O
01-1thesemdpshaveonlyoneaction	O
(	O
or	O
,	O
equivalently	O
,	O
noactions	O
)	O
,	O
sotheyareine↵ectmarkovchains.wheretwoedgesleaveastate	O
,	O
bothpossibilitiesareassumedtooccurwithequalprobability.thenumbersontheedgesindicatetherewardemittedifthatedgeistraversed.themdponthelefthastwostatesthatarerepresenteddistinctly	O
;	O
eachhasaseparateweightsothattheycantakeonanyvalue.themdpontherighthasthreestates	O
,	O
twoofwhich	O
,	O
bandb0	O
,	O
arerepresentedidenticallyandmustbegiventhesameapproximatevalue.wecanimaginethatthevalueofstateaisgivenbytheﬁrstcomponentof✓andthevalueofbandb0isgivenbythesecond.noticethattheobservabledataisidenticalforthetwomdps.inbothcasestheagentwillseesingleoccurrencesofafollowedbya0	O
,	O
thensomenumberofbseachfollowedbya 1	O
,	O
exceptthelastwhichisfollowedbya1	O
,	O
thenwestartalloveragainwithasingleaanda0	O
,	O
etc.allthedetailsarethesameaswell	O
;	O
inbothmdps	O
,	O
theprobabilityofastringofkbsis2 k.nowconsiderthevaluefunctionv✓=~0.intheﬁrstmdp	O
,	O
thisisanexactsolution	O
,	O
andtheoverallbeiszero.inthesecondmdp	O
,	O
thissolutionproducesanerrorinbothbandb0of1	O
,	O
foranoverallbeofpd	O
(	O
b	O
)	O
+d	O
(	O
b0	O
)	O
,	O
orp2/3ifthethreestatesareequallyweightedbyd.thetwomdps	O
,	O
whichgeneratethesamedata	O
,	O
havedi↵erentbes.thus	O
,	O
thebecannotbeestimatedfromdataalone	O
;	O
knowledgeofthemdpbeyondwhatisrevealedinthedataisrequired.moreover	O
,	O
thetwomdpshavedi↵erentminimal-bevaluefunctions.2fortheﬁrstmdp	O
,	O
theminimal-bevaluefunctionistheexactsolutionv✓=~0forany .forthesecondmdp,2.thisisacriticalobservation	O
,	O
asitispossibleforanerrorfunctiontobeunobservableandyetstillbeperfectlysatisfactoryforuseinlearningsettingsbecausethevaluethatminimizesitcanbedeterminedfromdata.forexample	O
,	O
thisiswhathappenswiththeve.theveisnotobservablefromdata	O
,	O
butits20w⇤w⇤1w⇤2w⇤3w⇤4ve1ve2rebe2be1pbetdemonte	O
carloobjectivesbootstrappingobjectives	O
280	O
chapter	O
11	O
:	O
*oﬀ-policy	O
methods	O
with	B
approximation	I
11.7	O
gradient-td	O
methods	O
we	O
now	O
consider	O
sgd	O
methods	O
for	O
minimizing	O
the	O
pbe	O
.	O
as	O
true	O
sgd	O
methods	O
,	O
these	O
gradient-td	O
methods	O
have	O
robust	O
convergence	O
properties	O
even	O
under	O
oﬀ-policy	B
training	O
and	O
nonlinear	O
function	B
approximation	I
.	O
remember	O
that	O
in	O
the	O
linear	O
case	O
there	O
is	O
always	O
an	O
exact	O
solution	O
,	O
the	O
td	O
ﬁxed	O
point	O
wtd	O
,	O
at	O
which	O
the	O
pbe	O
is	O
zero	O
.	O
this	O
solution	O
could	O
be	O
found	O
by	O
least-squares	O
methods	O
(	O
section	O
9.8	O
)	O
,	O
but	O
only	O
by	O
methods	O
of	O
quadratic	O
o	O
(	O
d2	O
)	O
complexity	O
in	O
the	O
number	O
of	O
parameters	O
.	O
we	O
seek	O
instead	O
an	O
sgd	O
method	O
,	O
which	O
should	O
be	O
o	O
(	O
d	O
)	O
and	O
have	O
robust	O
convergence	O
properties	O
.	O
gradient-td	O
methods	O
come	O
close	O
to	O
achieving	O
these	O
goals	O
,	O
at	O
the	O
cost	O
of	O
a	O
rough	O
doubling	O
of	O
computational	O
complexity	O
.	O
to	O
derive	O
an	O
sgd	O
method	O
for	O
the	O
pbe	O
(	O
assuming	O
linear	B
function	I
approximation	I
)	O
we	O
begin	O
by	O
expanding	O
and	O
rewriting	O
the	O
objective	O
(	O
11.22	O
)	O
in	O
matrix	O
terms	O
:	O
pbe	O
(	O
w	O
)	O
=	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
π¯δw	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
2	O
µ	O
=	O
(	O
π¯δw	O
)	O
(	O
cid:62	O
)	O
dπ¯δw	O
=	O
¯δ	O
(	O
cid:62	O
)	O
wπ	O
(	O
cid:62	O
)	O
dπ¯δw	O
x	O
(	O
cid:62	O
)	O
d¯δw	O
(	O
using	O
(	O
11.13	O
)	O
and	O
the	O
identity	O
π	O
(	O
cid:62	O
)	O
dπ	O
=	O
dx	O
(	O
cid:0	O
)	O
x	O
(	O
cid:62	O
)	O
dx	O
(	O
cid:1	O
)	O
−1	O
=	O
¯δ	O
(	O
cid:62	O
)	O
wdx	O
(	O
cid:0	O
)	O
x	O
(	O
cid:62	O
)	O
dx	O
(	O
cid:1	O
)	O
−1	O
=	O
(	O
cid:0	O
)	O
x	O
(	O
cid:62	O
)	O
d¯δw	O
(	O
cid:1	O
)	O
(	O
cid:62	O
)	O
(	O
cid:0	O
)	O
x	O
(	O
cid:62	O
)	O
dx	O
(	O
cid:1	O
)	O
−1	O
(	O
cid:0	O
)	O
x	O
(	O
cid:62	O
)	O
d¯δw	O
(	O
cid:1	O
)	O
.	O
the	O
gradient	B
with	O
respect	O
to	O
w	O
is	O
(	O
from	O
(	O
11.14	O
)	O
)	O
(	O
11.25	O
)	O
(	O
11.26	O
)	O
x	O
(	O
cid:62	O
)	O
d	O
)	O
∇pbe	O
(	O
w	O
)	O
=	O
2∇	O
(	O
cid:2	O
)	O
x	O
(	O
cid:62	O
)	O
d¯δw	O
(	O
cid:3	O
)	O
(	O
cid:62	O
)	O
(	O
cid:0	O
)	O
x	O
(	O
cid:62	O
)	O
dx	O
(	O
cid:1	O
)	O
−1	O
(	O
cid:0	O
)	O
x	O
(	O
cid:62	O
)	O
d¯δw	O
(	O
cid:1	O
)	O
.	O
to	O
turn	O
this	O
into	O
an	O
sgd	O
method	O
,	O
we	O
have	O
to	O
sample	O
something	O
on	O
every	O
time	O
step	O
that	O
has	O
this	O
quantity	O
as	O
its	O
expected	O
value	O
.	O
let	O
us	O
take	O
µ	O
to	O
be	O
the	O
distribution	O
of	O
states	O
visited	O
under	O
the	O
behavior	B
policy	I
.	O
all	O
three	O
of	O
the	O
factors	O
above	O
can	O
then	O
be	O
written	O
in	O
terms	O
of	O
expectations	O
under	O
this	O
distribution	O
.	O
for	O
example	O
,	O
the	O
last	O
factor	O
can	O
be	O
written	O
x	O
(	O
cid:62	O
)	O
d¯δw	O
=	O
(	O
cid:88	O
)	O
s	O
µ	O
(	O
s	O
)	O
x	O
(	O
s	O
)	O
¯δw	O
(	O
s	O
)	O
=	O
e	O
[	O
ρtδtxt	O
]	O
,	O
which	O
is	O
just	O
the	O
expectation	O
of	O
the	O
semi-gradient	O
td	O
(	O
0	O
)	O
update	O
(	O
11.2	O
)	O
.	O
the	O
ﬁrst	O
factor	O
is	O
the	O
transpose	O
of	O
the	O
gradient	B
of	O
this	O
update	O
:	O
∇e	O
[	O
ρtδtxt	O
]	O
(	O
cid:62	O
)	O
=	O
e	O
(	O
cid:2	O
)	O
ρt∇δ	O
(	O
cid:62	O
)	O
t	O
x	O
(	O
cid:62	O
)	O
t	O
(	O
cid:3	O
)	O
(	O
using	O
episodic	O
δt	O
)	O
finally	O
,	O
the	O
middle	O
factor	O
is	O
the	O
inverse	O
of	O
the	O
expected	O
outer-product	O
matrix	O
of	O
the	O
feature	O
vectors	O
:	O
=	O
e	O
(	O
cid:2	O
)	O
ρt∇	O
(	O
rt+1	O
+	O
γw	O
(	O
cid:62	O
)	O
xt+1	O
−	O
w	O
(	O
cid:62	O
)	O
xt	O
)	O
(	O
cid:62	O
)	O
x	O
(	O
cid:62	O
)	O
t	O
(	O
cid:3	O
)	O
=	O
e	O
(	O
cid:2	O
)	O
ρt	O
(	O
γxt+1	O
−	O
xt	O
)	O
x	O
(	O
cid:62	O
)	O
t	O
(	O
cid:3	O
)	O
.	O
µ	O
(	O
s	O
)	O
xsx	O
(	O
cid:62	O
)	O
s	O
=	O
e	O
(	O
cid:2	O
)	O
xtx	O
(	O
cid:62	O
)	O
t	O
(	O
cid:3	O
)	O
.	O
x	O
(	O
cid:62	O
)	O
dx	O
=	O
(	O
cid:88	O
)	O
s	O
11.7.	O
gradient-td	O
methods	O
281	O
substituting	O
these	O
expectations	O
for	O
the	O
three	O
factors	O
in	O
our	O
expression	O
for	O
the	O
gradient	B
of	O
the	O
pbe	O
,	O
we	O
get	O
∇pbe	O
(	O
w	O
)	O
=	O
2e	O
(	O
cid:2	O
)	O
ρt	O
(	O
γxt+1	O
−	O
xt	O
)	O
x	O
(	O
cid:62	O
)	O
t	O
(	O
cid:3	O
)	O
e	O
(	O
cid:2	O
)	O
xtx	O
(	O
cid:62	O
)	O
t	O
(	O
cid:3	O
)	O
−1	O
e	O
[	O
ρtδtxt	O
]	O
.	O
it	O
might	O
not	O
be	O
obvious	O
that	O
we	O
have	O
made	O
any	O
progress	O
by	O
writing	O
the	O
gradient	B
in	O
this	O
form	O
.	O
it	O
is	O
a	O
product	O
of	O
three	O
expressions	O
and	O
the	O
ﬁrst	O
and	O
last	O
are	O
not	O
independent	O
.	O
they	O
both	O
depend	O
on	O
the	O
next	O
feature	O
vector	O
xt+1	O
;	O
we	O
can	O
not	O
simply	O
sample	O
both	O
of	O
these	O
expectations	O
and	O
then	O
multiply	O
the	O
samples	O
.	O
this	O
would	O
give	O
us	O
a	O
biased	O
estmate	O
of	O
the	O
gradient	B
just	O
as	O
in	O
the	O
naive	B
residual-gradient	O
algorithm	O
.	O
(	O
11.27	O
)	O
another	O
idea	O
would	O
be	O
to	O
estimate	O
the	O
three	O
expectations	O
separately	O
and	O
then	O
combine	O
them	O
to	O
produce	O
an	O
unbiased	O
estimate	O
of	O
the	O
gradient	B
.	O
this	O
would	O
work	O
,	O
but	O
would	O
require	O
a	O
lot	O
of	O
computational	O
resources	O
,	O
particularly	O
to	O
store	O
the	O
ﬁrst	O
two	O
expectations	O
,	O
which	O
are	O
d	O
×	O
d	O
matrices	O
,	O
and	O
to	O
compute	O
the	O
inverse	O
of	O
the	O
second	O
.	O
this	O
idea	O
can	O
be	O
improved	O
.	O
if	O
two	O
of	O
the	O
three	O
expectations	O
are	O
estimated	O
and	O
stored	O
,	O
then	O
the	O
third	O
could	O
be	O
sampled	O
and	O
used	O
in	O
conjunction	O
with	O
the	O
two	O
stored	O
quantities	O
.	O
for	O
example	O
,	O
you	O
could	O
store	O
estimates	O
of	O
the	O
second	O
two	O
quantities	O
(	O
using	O
the	O
increment	O
inverse-updating	O
techniques	O
in	O
section	O
9.8	O
)	O
and	O
then	O
sample	O
the	O
ﬁrst	O
expression	O
.	O
unfortunately	O
,	O
the	O
overall	O
algorithm	O
would	O
still	O
be	O
of	O
quadratic	O
complexity	O
(	O
of	O
order	O
o	O
(	O
d2	O
)	O
)	O
.	O
the	O
idea	O
of	O
storing	O
some	O
estimates	O
separately	O
and	O
then	O
combining	O
them	O
with	O
samples	O
is	O
a	O
good	O
one	O
and	O
is	O
also	O
used	O
in	O
gradient-td	O
methods	O
.	O
gradient-td	O
methods	O
estimate	O
and	O
store	O
the	O
product	O
of	O
the	O
second	O
two	O
factors	O
in	O
(	O
11.27	O
)	O
.	O
these	O
factors	O
are	O
a	O
d	O
×	O
d	O
matrix	O
and	O
a	O
d-vector	O
,	O
so	O
their	O
product	O
is	O
just	O
a	O
d-vector	O
,	O
like	O
w	O
itself	O
.	O
we	O
denote	O
this	O
second	O
learned	O
vector	B
as	O
v	O
:	O
where	O
β	O
>	O
0	O
is	O
another	O
step-size	B
parameter	I
.	O
we	O
can	O
use	O
this	O
method	O
to	O
eﬀectively	O
achieve	O
(	O
11.28	O
)	O
with	O
o	O
(	O
d	O
)	O
storage	O
and	O
per-step	O
computation	O
.	O
given	O
a	O
stored	O
estimate	O
vt	O
approximating	O
(	O
11.28	O
)	O
,	O
we	O
can	O
update	O
our	O
main	O
parameter	O
vector	O
wt	O
using	O
sgd	O
methods	O
based	O
on	O
(	O
11.27	O
)	O
.	O
the	O
simplest	O
such	O
rule	O
is	O
wt+1	O
=	O
wt	O
−	O
=	O
wt	O
−	O
1	O
2	O
1	O
2	O
α∇pbe	O
(	O
wt	O
)	O
α2e	O
(	O
cid:2	O
)	O
ρt	O
(	O
γxt+1	O
−	O
xt	O
)	O
x	O
(	O
cid:62	O
)	O
t	O
(	O
cid:3	O
)	O
e	O
(	O
cid:2	O
)	O
xtx	O
(	O
cid:62	O
)	O
t	O
(	O
cid:3	O
)	O
−1	O
e	O
[	O
ρtδtxt	O
]	O
=	O
wt	O
+	O
αe	O
(	O
cid:2	O
)	O
ρt	O
(	O
xt	O
−	O
γxt+1	O
)	O
x	O
(	O
cid:62	O
)	O
t	O
(	O
cid:3	O
)	O
e	O
(	O
cid:2	O
)	O
xtx	O
(	O
cid:62	O
)	O
t	O
(	O
cid:3	O
)	O
−1	O
e	O
[	O
ρtδtxt	O
]	O
=	O
wt	O
+	O
αe	O
(	O
cid:2	O
)	O
ρt	O
(	O
xt	O
−	O
γxt+1	O
)	O
x	O
(	O
cid:62	O
)	O
t	O
(	O
cid:3	O
)	O
vt	O
=	O
wt	O
+	O
αρt	O
(	O
xt	O
−	O
γxt+1	O
)	O
x	O
(	O
cid:62	O
)	O
t	O
vt.	O
(	O
from	O
(	O
11.27	O
)	O
)	O
(	O
11.29	O
)	O
(	O
based	O
on	O
(	O
11.28	O
)	O
)	O
(	O
sampling	O
)	O
(	O
the	O
general	O
sgd	O
rule	O
)	O
this	O
form	O
is	O
familiar	O
to	O
students	O
of	O
linear	O
supervised	B
learning	I
.	O
it	O
is	O
the	O
solution	O
to	O
a	O
linear	O
least-squares	O
problem	O
that	O
tries	O
to	O
approximate	B
ρtδt	O
from	O
the	O
features	O
.	O
the	O
standard	O
sgd	O
method	O
for	O
incrementally	O
ﬁnding	O
the	O
vector	B
v	O
that	O
minimizes	O
the	O
expected	O
squared	O
is	O
known	O
as	O
the	O
least	O
mean	O
square	O
(	O
lms	O
)	O
rule	O
(	O
here	O
augmented	O
(	O
11.28	O
)	O
v	O
≈	O
e	O
(	O
cid:2	O
)	O
xtx	O
(	O
cid:62	O
)	O
t	O
(	O
cid:3	O
)	O
−1	O
e	O
[	O
ρtδtxt	O
]	O
.	O
error	O
(	O
cid:0	O
)	O
v	O
(	O
cid:62	O
)	O
xt	O
−	O
ρtδt	O
(	O
cid:1	O
)	O
2	O
with	O
an	O
importance	B
sampling	I
ratio	O
)	O
:	O
vt+1	O
=	O
vt	O
+	O
βρt	O
(	O
cid:0	O
)	O
δt	O
−	O
v	O
(	O
cid:62	O
)	O
t	O
xt	O
(	O
cid:1	O
)	O
xt	O
,	O
282	O
chapter	O
11	O
:	O
*oﬀ-policy	O
methods	O
with	B
approximation	I
this	O
algorithm	O
is	O
called	O
gtd2	O
.	O
note	O
that	O
if	O
the	O
ﬁnal	O
inner	O
product	O
(	O
x	O
(	O
cid:62	O
)	O
t	O
vt	O
)	O
is	O
done	O
ﬁrst	O
,	O
then	O
the	O
entire	O
algorithm	O
is	O
of	O
o	O
(	O
d	O
)	O
complexity	O
.	O
a	O
slightly	O
better	O
algorithm	O
can	O
be	O
derived	O
by	O
doing	O
a	O
few	O
more	O
analytic	O
steps	O
before	O
substituting	O
in	O
vt.	O
continuing	O
from	O
(	O
11.29	O
)	O
:	O
wt+1	O
=	O
wt	O
+	O
αe	O
(	O
cid:2	O
)	O
ρt	O
(	O
xt	O
−	O
γxt+1	O
)	O
x	O
(	O
cid:62	O
)	O
t	O
(	O
cid:3	O
)	O
e	O
(	O
cid:2	O
)	O
xtx	O
(	O
cid:62	O
)	O
t	O
(	O
cid:3	O
)	O
−1	O
e	O
[	O
ρtδtxt	O
]	O
=	O
wt	O
+	O
α	O
(	O
cid:0	O
)	O
e	O
(	O
cid:2	O
)	O
ρtxtx	O
(	O
cid:62	O
)	O
t	O
(	O
cid:3	O
)	O
−	O
γe	O
(	O
cid:2	O
)	O
ρtxt+1x	O
(	O
cid:62	O
)	O
t	O
(	O
cid:3	O
)	O
(	O
cid:1	O
)	O
e	O
(	O
cid:2	O
)	O
xtx	O
(	O
cid:62	O
)	O
t	O
(	O
cid:3	O
)	O
−1	O
e	O
[	O
ρtδtxt	O
]	O
=	O
wt	O
+	O
α	O
(	O
cid:16	O
)	O
e	O
[	O
xtρtδt	O
]	O
−	O
γe	O
(	O
cid:2	O
)	O
ρtxt+1x	O
(	O
cid:62	O
)	O
t	O
(	O
cid:3	O
)	O
e	O
(	O
cid:2	O
)	O
xtx	O
(	O
cid:62	O
)	O
t	O
(	O
cid:3	O
)	O
−1	O
e	O
[	O
ρtδtxt	O
]	O
(	O
cid:17	O
)	O
=	O
wt	O
+	O
α	O
(	O
cid:0	O
)	O
e	O
[	O
xtρtδt	O
]	O
−	O
γe	O
(	O
cid:2	O
)	O
ρtxt+1x	O
(	O
cid:62	O
)	O
t	O
(	O
cid:3	O
)	O
vt	O
(	O
cid:1	O
)	O
=	O
wt	O
+	O
αρt	O
(	O
cid:0	O
)	O
δtxt	O
−	O
γxt+1x	O
(	O
cid:62	O
)	O
t	O
vt	O
(	O
cid:1	O
)	O
,	O
(	O
sampling	O
)	O
which	O
again	O
is	O
o	O
(	O
d	O
)	O
if	O
the	O
ﬁnal	O
product	O
(	O
x	O
(	O
cid:62	O
)	O
t	O
vt	O
)	O
is	O
done	O
ﬁrst	O
.	O
this	O
algorithm	O
is	O
known	O
as	O
either	O
td	O
(	O
0	O
)	O
with	O
gradient	O
correction	O
(	O
tdc	O
)	O
or	O
,	O
alternatively	O
,	O
as	O
gtd	O
(	O
0	O
)	O
.	O
figure	O
11.5	O
shows	O
a	O
sample	O
and	O
the	O
expected	O
behavior	O
of	O
tdc	O
on	O
baird	O
’	O
s	O
counterex-	O
ample	O
.	O
as	O
intended	O
,	O
the	O
pbe	O
falls	O
to	O
zero	O
,	O
but	O
note	O
that	O
the	O
individual	O
components	O
of	O
the	O
parameter	O
vector	O
do	O
not	O
approach	O
zero	O
.	O
in	O
fact	O
,	O
these	O
values	O
are	O
still	O
far	O
from	O
an	O
optimal	O
solution	O
,	O
ˆv	O
(	O
s	O
)	O
=	O
0	O
,	O
for	O
all	O
s	O
,	O
for	O
which	O
w	O
would	O
have	O
to	O
be	O
proportional	O
to	O
(	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
4	O
,	O
−2	O
)	O
(	O
cid:62	O
)	O
.	O
after	O
1000	O
iterations	O
we	O
are	O
still	O
far	O
from	O
an	O
optimal	O
solution	O
,	O
as	O
figure	O
11.5	O
:	O
the	O
behavior	O
of	O
the	O
tdc	O
algorithm	O
on	O
baird	O
’	O
s	O
counterexample	O
.	O
on	O
the	O
left	O
is	O
shown	O
a	O
typical	O
single	O
run	O
,	O
and	O
on	O
the	O
right	O
is	O
shown	O
the	O
expected	O
behavior	O
of	O
this	O
algorithm	O
if	O
the	O
updates	O
are	O
done	O
synchronously	O
(	O
analogous	O
to	O
(	O
11.9	O
)	O
,	O
except	O
for	O
the	O
two	O
tdc	O
parameter	O
vectors	O
)	O
.	O
the	O
step	O
sizes	O
were	O
α	O
=	O
0.005	O
and	O
β	O
=	O
0.05.	O
w1–w6w8w8pvepveppbeppbe10520-2.340100001000stepsw7w1–w6tdcexpected	O
tdcsweepsw7	O
11.8.	O
emphatic-td	O
methods	O
283	O
we	O
can	O
see	O
from	O
the	O
ve	O
,	O
which	O
remains	O
almost	O
2.	O
the	O
system	O
is	O
actually	O
converging	O
to	O
an	O
optimal	O
solution	O
,	O
but	O
progress	O
is	O
extremely	O
slow	O
because	O
the	O
pbe	O
is	O
already	O
so	O
close	O
to	O
zero	O
.	O
gtd2	O
and	O
tdc	O
both	O
involve	O
two	O
learning	O
processes	O
,	O
a	O
primary	O
one	O
for	O
w	O
and	O
a	O
secondary	O
one	O
for	O
v.	O
the	O
logic	O
of	O
the	O
primary	O
learning	O
process	O
relies	O
on	O
the	O
secondary	O
learning	O
process	O
having	O
ﬁnished	O
,	O
at	O
least	O
approximately	O
,	O
whereas	O
the	O
secondary	O
learning	O
process	O
proceeds	O
without	O
being	O
inﬂuenced	O
by	O
the	O
ﬁrst	O
.	O
we	O
call	O
this	O
sort	O
of	O
asymmetrical	O
dependence	O
a	O
cascade	O
.	O
in	O
cascades	O
we	O
often	O
assume	O
that	O
the	O
secondary	O
learning	O
process	O
is	O
proceeding	O
faster	O
and	O
thus	O
is	O
always	O
at	O
its	O
asymptotic	O
value	B
,	O
ready	O
and	O
accurate	O
to	O
assist	O
the	O
primary	O
learning	O
process	O
.	O
the	O
convergence	O
proofs	O
for	O
these	O
methods	O
often	O
make	O
this	O
assumption	O
explicitly	O
.	O
these	O
are	O
called	O
two-time-scale	O
proofs	O
.	O
the	O
fast	O
time	O
scale	O
is	O
that	O
of	O
the	O
secondary	O
learning	O
process	O
,	O
and	O
the	O
slower	O
time	O
scale	O
is	O
that	O
of	O
the	O
primary	O
learning	O
process	O
.	O
if	O
α	O
is	O
the	O
step	O
size	O
of	O
the	O
primary	O
learning	O
process	O
,	O
and	O
β	O
is	O
the	O
step	O
size	O
of	O
the	O
secondary	O
learning	O
process	O
,	O
then	O
these	O
convergence	O
proofs	O
will	O
typically	O
require	O
that	O
in	O
the	O
limit	O
β	O
→	O
0	O
and	O
α	O
gradient-td	O
methods	O
are	O
currently	O
the	O
most	O
well	O
understood	O
and	O
widely	O
used	O
stable	O
oﬀ-policy	B
methods	I
.	O
there	O
are	O
extensions	O
to	O
action	B
values	O
and	B
control	I
(	O
gq	O
,	O
maei	O
et	O
al.	O
,	O
2010	O
)	O
,	O
to	O
eligibility	B
traces	I
(	O
gtd	O
(	O
λ	O
)	O
and	O
gq	O
(	O
λ	O
)	O
,	O
maei	O
,	O
2011	O
;	O
maei	O
and	O
sutton	O
,	O
2010	O
)	O
,	O
and	O
to	O
nonlinear	O
function	B
approximation	I
(	O
maei	O
et	O
al.	O
,	O
2009	O
)	O
.	O
there	O
have	O
also	O
been	O
proposed	O
hybrid	O
algorithms	O
midway	O
between	O
semi-gradient	O
td	O
and	O
gradient	O
td	O
(	O
hackman	O
,	O
2012	O
;	O
white	O
and	O
white	O
,	O
2016	O
)	O
.	O
hybrid-td	O
algorithms	O
behave	O
like	O
gradient-td	O
algorithms	O
in	O
states	O
where	O
the	O
target	B
and	O
behavior	O
policies	O
are	O
very	O
diﬀerent	O
,	O
and	O
behave	O
like	O
semi-	O
gradient	B
algorithms	O
in	O
states	O
where	O
the	O
target	B
and	O
behavior	O
policies	O
are	O
the	O
same	O
.	O
finally	O
,	O
the	O
gradient-td	O
idea	O
has	O
been	O
combined	O
with	O
the	O
ideas	O
of	O
proximal	O
methods	O
and	B
control	I
variates	O
to	O
produce	O
more	O
eﬃcient	O
methods	O
(	O
mahadevan	O
et	O
al.	O
,	O
2014	O
)	O
.	O
β	O
→	O
0	O
.	O
11.8	O
emphatic-td	O
methods	O
we	O
turn	O
now	O
to	O
the	O
second	O
major	O
strategy	O
that	O
has	O
been	O
extensively	O
explored	O
for	O
obtain-	O
ing	B
a	O
cheap	O
and	O
eﬃcient	O
oﬀ-policy	B
learning	O
method	O
with	B
function	I
approximation	I
.	O
recall	O
that	O
linear	O
semi-gradient	O
td	O
methods	O
are	O
eﬃcient	O
and	O
stable	O
when	O
trained	O
under	O
the	O
on-policy	B
distribution	I
,	O
and	O
that	O
we	O
showed	O
in	O
section	O
9.4	O
that	O
this	O
has	O
to	O
do	O
with	O
the	O
positive	O
deﬁniteness	O
of	O
the	O
matrix	O
a	O
(	O
9.11	O
)	O
and	O
the	O
match	O
between	O
the	O
on-policy	O
state	O
distribution	O
µπ	O
and	O
the	O
state-transition	O
probabilities	O
p	O
(	O
s|s	O
,	O
a	O
)	O
under	O
the	O
target	B
policy	O
.	O
in	O
oﬀ-policy	O
learning	O
,	O
we	O
reweight	O
the	O
state	B
transitions	O
using	O
importance	B
sampling	I
so	O
that	O
they	O
become	O
appropriate	O
for	O
learning	O
about	O
the	O
target	B
policy	O
,	O
but	O
the	O
state	B
distribution	O
is	O
still	O
that	O
of	O
the	O
behavior	B
policy	I
.	O
there	O
is	O
a	O
mismatch	O
.	O
a	O
natural	O
idea	O
is	O
to	O
somehow	O
reweight	O
the	O
states	O
,	O
emphasizing	O
some	O
and	O
de-emphasizing	O
others	O
,	O
so	O
as	O
to	O
return	B
the	O
distribution	O
of	O
updates	O
to	O
the	O
on-policy	B
distribution	I
.	O
there	O
would	O
then	O
be	O
a	O
match	O
,	O
and	B
stability	I
and	O
convergence	O
would	O
follow	O
from	O
existing	O
results	O
.	O
this	O
is	O
the	O
idea	O
of	O
emphatic-td	O
methods	O
,	O
ﬁrst	O
introduced	O
for	O
on-policy	O
training	O
in	O
section	O
9.11.	O
actually	O
,	O
the	O
notion	O
of	O
“	O
the	O
on-policy	B
distribution	I
”	O
is	O
not	O
quite	O
right	O
,	O
as	O
there	O
are	O
many	O
on-policy	O
distributions	O
,	O
and	O
any	O
one	O
of	O
these	O
is	O
suﬃcient	O
to	O
guarantee	O
stability	O
.	O
consider	O
an	O
undiscounted	O
episodic	O
problem	O
.	O
the	O
way	O
episodes	B
terminate	O
is	O
fully	O
determined	O
by	O
284	O
chapter	O
11	O
:	O
*oﬀ-policy	O
methods	O
with	B
approximation	I
the	O
transition	B
probabilities	I
,	O
but	O
there	O
may	O
be	O
several	O
diﬀerent	O
ways	O
the	O
episodes	B
might	O
begin	O
.	O
however	O
the	O
episodes	B
start	O
,	O
if	O
all	O
state	B
transitions	O
are	O
due	O
to	O
the	O
target	B
policy	O
,	O
then	O
the	O
state	B
distribution	O
that	O
results	O
is	O
an	O
on-policy	B
distribution	I
.	O
you	O
might	O
start	O
close	O
to	O
the	O
terminal	O
state	B
and	O
visit	O
only	O
a	O
few	O
states	O
with	O
high	O
probability	O
before	O
ending	O
the	O
episode	O
.	O
or	O
you	O
might	O
start	O
far	O
away	O
and	O
pass	O
through	O
many	O
states	O
before	O
terminating	O
.	O
both	O
are	O
on-policy	O
distributions	O
,	O
and	O
training	O
on	O
both	O
with	O
a	O
linear	O
semi-gradient	O
method	O
would	O
be	O
guaranteed	O
to	O
be	O
stable	O
.	O
however	O
the	O
process	O
starts	O
,	O
an	O
on-policy	B
distribution	I
results	O
as	O
long	O
as	O
all	O
states	O
encountered	O
are	O
updated	O
up	O
until	O
termination	O
.	O
if	O
there	O
is	O
discounting	B
,	O
it	O
can	O
be	O
treated	O
as	O
partial	O
or	O
probabilistic	O
termination	O
for	O
these	O
purposes	O
.	O
if	O
γ	O
=	O
0.9	O
,	O
then	O
we	O
can	O
consider	O
that	O
with	O
probability	O
0.1	O
the	O
process	O
terminates	O
on	O
every	O
time	O
step	O
and	O
then	O
immediately	O
restarts	O
in	O
the	O
state	O
that	O
is	O
tran-	O
sitioned	O
to	O
.	O
a	O
discounted	O
problem	O
is	O
one	O
that	O
is	O
continually	O
terminating	O
and	O
restarting	O
with	O
probability	O
1	O
−	O
γ	O
on	O
every	O
step	O
.	O
this	O
way	O
of	O
thinking	O
about	O
discounting	B
is	O
an	O
ex-	O
ample	O
of	O
a	O
more	O
general	O
notion	O
of	O
pseudo	O
termination—termination	O
that	O
does	O
not	O
aﬀect	O
the	O
sequence	O
of	O
state	O
transitions	O
,	O
but	O
does	O
aﬀect	O
the	O
learning	O
process	O
and	O
the	O
quantities	O
being	O
learned	O
.	O
this	O
kind	O
of	O
pseudo	O
termination	O
is	O
important	O
to	O
oﬀ-policy	B
learning	O
be-	O
cause	O
the	O
restarting	O
is	O
optional—remember	O
we	O
can	O
start	O
any	O
way	O
we	O
want	O
to—and	O
the	O
termination	O
relieves	O
the	O
need	O
to	O
keep	O
including	O
encountered	O
states	O
within	O
the	O
on-policy	B
distribution	I
.	O
that	O
is	O
,	O
if	O
we	O
don	O
’	O
t	O
consider	O
the	O
new	O
states	O
as	O
restarts	O
,	O
then	O
discounting	B
quickly	O
gives	O
us	O
a	O
limited	O
on-policy	B
distribution	I
.	O
the	O
one-step	O
emphatic-td	O
algorithm	O
for	O
learning	O
episodic	O
state	B
values	O
is	O
deﬁned	O
by	O
:	O
δt	O
=	O
rt+1	O
+	O
γˆv	O
(	O
st+1	O
,	O
wt	O
)	O
−	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
,	O
wt+1	O
=	O
wt	O
+	O
αmtρtδt∇ˆv	O
(	O
st	O
,	O
wt	O
)	O
,	O
mt	O
=	O
γρt−1mt−1	O
+	O
it	O
,	O
figure	O
11.6	O
:	O
the	O
behavior	O
of	O
the	O
one-step	O
emphatic-td	O
algorithm	O
in	O
expectation	O
on	O
baird	O
’	O
s	O
counterexample	O
.	O
the	O
step	O
size	O
was	O
α	O
=	O
0.03.	O
sweepsw1–w6w7w8pve10520-501000	O
11.9.	O
reducing	B
variance	I
285	O
with	O
it	O
,	O
the	O
interest	O
,	O
being	O
arbitrary	O
and	O
mt	O
,	O
the	O
emphasis	O
,	O
being	O
initialized	O
to	O
mt−1	O
=	O
0.	O
how	O
does	O
this	O
algorithm	O
perform	O
on	O
baird	O
’	O
s	O
counterexample	O
?	O
figure	O
11.6	O
shows	O
the	O
trajectory	O
in	O
expectation	O
of	O
the	O
components	O
of	O
the	O
parameter	O
vector	O
(	O
for	O
the	O
case	O
in	O
which	O
it	O
=	O
1	O
,	O
for	O
all	O
t	O
)	O
.	O
there	O
are	O
some	O
oscillations	O
but	O
eventually	O
everything	O
converges	O
and	O
the	O
ve	O
goes	O
to	O
zero	O
.	O
these	O
trajectories	O
are	O
obtained	O
by	O
iteratively	O
computing	O
the	O
expectation	O
of	O
the	O
parameter	O
vector	O
trajectory	O
without	O
any	O
of	O
the	O
variance	O
due	O
to	O
sampling	O
of	O
transitions	O
and	O
rewards	O
.	O
we	O
do	O
not	O
show	O
the	O
results	O
of	O
applying	O
the	O
emphatic-td	O
algorithm	O
directly	O
because	O
its	O
variance	O
on	O
baird	O
’	O
s	O
counterexample	O
is	O
so	O
high	O
that	O
it	O
is	O
nigh	O
impossible	O
to	O
get	O
consistent	O
results	O
in	O
computational	O
experiments	O
.	O
the	O
algorithm	O
converges	O
to	O
the	O
optimal	O
solution	O
in	O
theory	O
on	O
this	O
problem	O
,	O
but	O
in	O
practice	O
it	O
does	O
not	O
.	O
we	O
turn	O
to	O
the	O
topic	O
of	O
reducing	O
the	O
variance	O
of	O
all	O
these	O
algorithms	O
in	O
the	O
next	O
section	O
.	O
11.9	O
reducing	B
variance	I
oﬀ-policy	O
learning	O
is	O
inherently	O
of	O
greater	O
variance	O
than	O
on-policy	O
learning	O
.	O
this	O
is	O
not	O
surprising	O
;	O
if	O
you	O
receive	O
data	O
less	O
closely	O
related	O
to	O
a	O
policy	B
,	O
you	O
should	O
expect	O
to	O
learn	O
less	O
about	O
the	O
policy	B
’	O
s	O
values	O
.	O
in	O
the	O
extreme	O
,	O
one	O
may	O
be	O
able	O
to	O
learn	O
nothing	O
.	O
you	O
can	O
’	O
t	O
expect	O
to	O
learn	O
how	O
to	O
drive	O
by	O
cooking	O
dinner	O
,	O
for	O
example	O
.	O
only	O
if	O
the	O
target	B
and	O
behavior	O
policies	O
are	O
related	O
,	O
if	O
they	O
visit	O
similar	O
states	O
and	O
take	O
similar	O
actions	O
,	O
should	O
one	O
be	O
able	O
to	O
make	O
signiﬁcant	O
progress	O
in	O
oﬀ-policy	O
training	O
.	O
on	O
the	O
other	O
hand	O
,	O
any	O
policy	B
has	O
many	O
neighbors	O
,	O
many	O
similar	O
policies	O
with	O
con-	O
siderable	O
overlap	O
in	O
states	O
visited	O
and	O
actions	O
chosen	O
,	O
and	O
yet	O
which	O
are	O
not	O
identical	O
.	O
the	O
raison	O
d	O
’	O
ˆetre	O
of	O
oﬀ-policy	O
learning	O
is	O
to	O
enable	O
generalization	O
to	O
this	O
vast	O
number	O
of	O
related-but-not-identical	O
policies	O
.	O
the	O
problem	O
remains	O
of	O
how	O
to	O
make	O
the	O
best	O
use	O
of	O
the	O
experience	O
.	O
now	O
that	O
we	O
have	O
some	O
methods	O
that	O
are	O
stable	O
in	O
expected	O
value	B
(	O
if	O
the	O
step	O
sizes	O
are	O
set	O
right	O
)	O
,	O
attention	O
naturally	O
turns	O
to	O
reducing	O
the	O
variance	O
of	O
the	O
estimates	O
.	O
there	O
are	O
many	O
possible	O
ideas	O
,	O
and	O
we	O
can	O
just	O
touch	O
on	O
of	O
a	O
few	O
of	O
them	O
in	O
this	O
introductory	O
text	O
.	O
why	O
is	O
controlling	O
variance	O
especially	O
critical	O
in	O
oﬀ-policy	O
methods	O
based	O
on	O
impor-	O
tance	O
sampling	O
?	O
as	O
we	O
have	O
seen	O
,	O
importance	B
sampling	I
often	O
involves	O
products	O
of	O
policy	O
ratios	O
.	O
the	O
ratios	O
are	O
always	O
one	O
in	O
expectation	O
(	O
5.12	O
)	O
,	O
but	O
their	O
actual	O
values	O
may	O
be	O
very	O
high	O
or	O
as	O
low	O
as	O
zero	O
.	O
successive	O
ratios	O
are	O
uncorrelated	O
,	O
so	O
their	O
products	O
are	O
also	O
always	O
one	O
in	O
expected	O
value	B
,	O
but	O
they	O
can	O
be	O
of	O
very	O
high	O
variance	O
.	O
recall	O
that	O
these	O
ratios	O
multiply	O
the	O
step	O
size	O
in	O
sgd	O
methods	O
,	O
so	O
high	O
variance	O
means	O
taking	O
steps	O
that	O
vary	O
greatly	O
in	O
their	O
sizes	O
.	O
this	O
is	O
problematic	O
for	O
sgd	O
because	O
of	O
the	O
occasional	O
very	O
large	O
steps	O
.	O
they	O
must	O
not	O
be	O
so	O
large	O
as	O
to	O
take	O
the	O
parameter	O
to	O
a	O
part	O
of	O
the	O
space	O
with	O
a	O
very	O
diﬀerent	O
gradient	B
.	O
sgd	O
methods	O
rely	O
on	O
averaging	O
over	O
multiple	O
steps	O
to	O
get	O
a	O
good	O
sense	O
of	O
the	O
gradient	B
,	O
and	O
if	O
they	O
make	O
large	O
moves	O
from	O
single	O
samples	O
they	O
be-	O
come	O
unreliable	O
.	O
if	O
the	O
step-size	B
parameter	I
is	O
set	O
small	O
enough	O
to	O
prevent	O
this	O
,	O
then	O
the	O
expected	O
step	O
can	O
end	O
up	O
being	O
very	O
small	O
,	O
resulting	O
in	O
very	O
slow	O
learning	O
.	O
the	O
notions	O
of	O
momentum	O
(	O
derthick	O
,	O
1984	O
)	O
,	O
of	O
polyak-ruppert	O
averaging	O
(	O
polyak	O
,	O
1990	O
;	O
ruppert	O
,	O
1988	O
;	O
polyak	O
and	O
juditsky	O
,	O
1992	O
)	O
,	O
or	O
further	O
extensions	O
of	O
these	O
ideas	O
may	O
signiﬁcantly	O
help	O
.	O
methods	O
for	O
adaptively	O
setting	O
separate	O
step	O
sizes	O
for	O
diﬀerent	O
components	O
of	O
the	O
286	O
chapter	O
11	O
:	O
*oﬀ-policy	O
methods	O
with	B
approximation	I
parameter	O
vector	B
are	O
also	O
pertinent	O
(	O
e.g.	O
,	O
jacobs	O
,	O
1988	O
;	O
sutton	O
,	O
1992b	O
,	O
c	O
)	O
,	O
as	O
are	O
the	O
“	O
importance	O
weight	O
aware	O
”	O
updates	O
of	O
karampatziakis	O
and	O
langford	O
(	O
2010	O
)	O
.	O
in	O
chapter	O
5	O
we	O
saw	O
how	O
weighted	O
importance	O
sampling	O
is	O
signiﬁcantly	O
better	O
behaved	O
,	O
with	O
lower	O
variance	O
updates	O
,	O
than	O
ordinary	O
importance	B
sampling	I
.	O
however	O
,	O
adapting	O
weighted	O
importance	O
sampling	O
to	O
function	B
approximation	I
is	O
challenging	O
and	O
can	O
probably	O
only	O
be	O
done	O
approximately	O
with	O
o	O
(	O
d	O
)	O
complexity	O
(	O
mahmood	O
and	O
sutton	O
,	O
2015	O
)	O
.	O
the	O
tree	O
backup	O
algorithm	O
(	O
section	O
7.5	O
)	O
shows	O
that	O
it	O
is	O
possible	O
to	O
perform	O
some	O
oﬀ-policy	B
learning	O
without	O
using	O
importance	B
sampling	I
.	O
this	O
idea	O
has	O
been	O
extended	O
to	O
the	O
oﬀ-policy	B
case	O
to	O
produce	O
stable	O
and	O
more	O
eﬃcient	O
methods	O
by	O
munos	O
,	O
stepleton	O
,	O
harutyunyan	O
,	O
and	O
bellemare	O
(	O
2016	O
)	O
and	O
by	O
mahmood	O
,	O
yu	O
and	O
sutton	O
(	O
2017	O
)	O
.	O
another	O
,	O
complementary	O
strategy	O
is	O
to	O
allow	O
the	O
target	B
policy	O
to	O
be	O
determined	O
in	O
part	O
by	O
the	O
behavior	B
policy	I
,	O
in	O
such	O
a	O
way	O
that	O
it	O
never	O
can	O
be	O
so	O
diﬀerent	O
from	O
it	O
to	O
create	O
large	O
importance	B
sampling	I
ratios	O
.	O
for	O
example	O
,	O
the	O
target	B
policy	O
can	O
be	O
deﬁned	O
by	O
reference	O
to	O
the	O
behavior	B
policy	I
,	O
as	O
in	O
the	O
“	O
recognizers	O
”	O
proposed	O
by	O
precup	O
et	O
al	O
.	O
(	O
2006	O
)	O
.	O
11.10	O
summary	O
oﬀ-policy	B
learning	O
is	O
a	O
tempting	O
challenge	O
,	O
testing	O
our	O
ingenuity	O
in	O
designing	O
stable	O
and	O
eﬃcient	O
learning	O
algorithms	O
.	O
tabular	O
q-learning	O
makes	O
oﬀ-policy	B
learning	O
seem	O
easy	O
,	O
and	O
it	O
has	O
natural	O
generalizations	O
to	O
expected	O
sarsa	O
and	O
to	O
the	O
tree	O
backup	O
algorithm	O
.	O
but	O
as	O
we	O
have	O
seen	O
in	O
this	O
chapter	O
,	O
the	O
extension	O
of	O
these	O
ideas	O
to	O
signiﬁcant	O
function	B
approximation	I
,	O
even	O
linear	B
function	I
approximation	I
,	O
involves	O
new	O
challenges	O
and	O
forces	O
us	O
to	O
deepen	O
our	O
understanding	O
of	O
reinforcement	O
learning	O
algorithms	O
.	O
why	O
go	O
to	O
such	O
lengths	O
?	O
one	O
reason	O
to	O
seek	O
oﬀ-policy	B
algorithms	O
is	O
to	O
give	O
ﬂexibility	O
in	O
dealing	O
with	O
the	O
tradeoﬀ	O
between	O
exploration	O
and	O
exploitation	O
.	O
another	O
is	O
to	O
free	O
behavior	O
from	O
learning	O
,	O
and	O
avoid	O
the	O
tyranny	O
of	O
the	O
target	B
policy	O
.	O
td	O
learning	O
appears	O
to	O
hold	O
out	O
the	O
possibility	O
of	O
learning	O
about	O
multiple	O
things	O
in	O
parallel	O
,	O
of	O
using	O
one	O
stream	O
of	O
experience	O
to	O
solve	O
many	O
tasks	O
simultaneously	O
.	O
we	O
can	O
certainly	O
do	O
this	O
in	O
special	O
cases	O
,	O
just	O
not	O
in	O
every	O
case	O
that	O
we	O
would	O
like	O
to	O
or	O
as	O
eﬃciently	O
as	O
we	O
would	O
like	O
to	O
.	O
in	O
this	O
chapter	O
we	O
divided	O
the	O
challenge	O
of	O
oﬀ-policy	O
learning	O
into	O
two	O
parts	O
.	O
the	O
ﬁrst	O
part	O
,	O
correcting	O
the	O
targets	O
of	O
learning	O
for	O
the	O
behavior	B
policy	I
,	O
is	O
straightforwardly	O
dealt	O
with	O
using	O
the	O
techniques	O
devised	O
earlier	O
for	O
the	O
tabular	O
case	O
,	O
albeit	O
at	O
the	O
cost	O
of	O
increasing	O
the	O
variance	O
of	O
the	O
updates	O
and	O
thereby	O
slowing	O
learning	O
.	O
high	O
variance	O
will	O
probably	O
always	O
remains	O
a	O
challenge	O
for	O
oﬀ-policy	O
learning	O
.	O
the	O
second	O
part	O
of	O
the	O
challenge	O
of	O
oﬀ-policy	O
learning	O
emerges	O
as	O
the	O
instability	O
of	O
semi-gradient	O
td	O
methods	O
that	O
involve	O
bootstrapping	B
.	O
we	O
seek	O
powerful	O
function	O
ap-	O
proximation	O
,	O
oﬀ-policy	B
learning	O
,	O
and	O
the	O
eﬃciency	O
and	O
ﬂexibility	O
of	O
bootstrapping	O
td	O
methods	O
,	O
but	O
it	O
is	O
challenging	O
to	O
combine	O
all	O
three	O
aspects	O
of	O
this	O
deadly	B
triad	I
in	O
one	O
algorithm	O
without	O
introducing	O
the	O
potential	O
for	O
instability	O
.	O
there	O
have	O
been	O
several	O
at-	O
tempts	O
.	O
the	O
most	O
popular	O
has	O
been	O
to	O
seek	O
to	O
perform	O
true	O
stochastic	O
gradient	B
descent	I
(	O
sgd	O
)	O
in	O
the	O
bellman	O
error	O
(	O
a.k.a	O
.	O
the	O
bellman	O
residual	O
)	O
.	O
however	O
,	O
our	O
analysis	O
con-	O
11.10.	O
summary	O
287	O
cludes	O
that	O
this	O
is	O
not	O
an	O
appealing	O
goal	B
in	O
many	O
cases	O
,	O
and	O
that	O
anyway	O
it	O
is	O
impossible	O
to	O
achieve	O
with	O
a	O
learning	O
algorithm—the	O
gradient	B
of	O
the	O
be	O
is	O
not	O
learnable	O
from	O
ex-	O
perience	O
that	O
reveals	O
only	O
feature	O
vectors	O
and	O
not	O
underlying	O
states	O
.	O
another	O
approach	O
,	O
gradient-td	O
methods	O
,	O
performs	O
sgd	O
in	O
the	O
projected	O
bellman	O
error	O
.	O
the	O
gradient	B
of	O
the	O
pbe	O
is	O
learnable	O
with	O
o	O
(	O
d	O
)	O
complexity	O
,	O
but	O
at	O
the	O
cost	O
of	O
a	O
second	O
parameter	O
vector	O
with	O
a	O
second	O
step	O
size	O
.	O
the	O
newest	O
family	O
of	O
methods	O
,	O
emphatic-td	O
methods	O
,	O
reﬁne	O
an	O
old	O
idea	O
for	O
reweighting	O
updates	O
,	O
emphasizing	O
some	O
and	O
de-emphasizing	O
others	O
.	O
in	O
this	O
way	O
they	O
restore	O
the	O
special	O
properties	O
that	O
make	O
on-policy	O
learning	O
stable	O
with	O
computationally	O
simple	O
semi-gradient	B
methods	I
.	O
the	O
whole	O
area	O
of	O
oﬀ-policy	O
learning	O
is	O
relatively	O
new	O
and	O
unsettled	O
.	O
which	O
methods	O
are	O
best	O
or	O
even	O
adequate	O
is	O
not	O
yet	O
clear	O
.	O
are	O
the	O
complexities	O
of	O
the	O
new	O
methods	O
introduced	O
at	O
the	O
end	O
of	O
this	O
chapter	O
really	O
necessary	O
?	O
which	O
of	O
them	O
can	O
be	O
combined	O
eﬀectively	O
with	O
variance	O
reduction	O
methods	O
?	O
the	O
potential	O
for	O
oﬀ-policy	O
learning	O
remains	O
tantalizing	O
,	O
the	O
best	O
way	O
to	O
achieve	O
it	O
still	O
a	O
mystery	O
.	O
bibliographical	O
and	O
historical	O
remarks	O
11.1	O
11.2	O
11.3	O
11.4	O
the	O
ﬁrst	O
semi-gradient	O
method	O
was	O
linear	O
td	O
(	O
λ	O
)	O
(	O
sutton	O
,	O
1988	O
)	O
.	O
the	O
name	O
“	O
semi-gradient	O
”	O
is	O
more	O
recent	O
(	O
sutton	O
,	O
2015a	O
)	O
.	O
semi-gradient	O
oﬀ-policy	O
td	O
(	O
0	O
)	O
with	O
general	O
importance-sampling	O
ratio	B
may	O
not	O
have	O
been	O
explicitly	O
stated	O
until	O
sutton	O
,	O
mahmood	O
,	O
and	O
white	O
(	O
2016	O
)	O
,	O
but	O
the	O
action-value	O
forms	O
were	O
introduced	O
by	O
precup	O
,	O
sutton	O
,	O
and	O
singh	O
(	O
2000	O
)	O
,	O
who	O
also	O
did	O
eligibility	O
trace	O
forms	O
of	O
these	O
algorithms	O
(	O
see	O
chapter	O
12	O
)	O
.	O
their	O
continuing	O
,	O
undiscounted	O
forms	O
have	O
not	O
been	O
signiﬁcantly	O
explored	O
.	O
the	O
atomic	O
n-step	B
forms	O
given	O
here	O
are	O
new	O
.	O
the	O
earliest	O
w-to-2w	O
example	O
was	O
given	O
by	O
tsitsiklis	O
and	O
van	O
roy	O
(	O
1996	O
)	O
,	O
who	O
also	O
introduced	O
the	O
speciﬁc	O
counterexample	O
in	O
the	O
box	O
on	O
page	O
265.	O
baird	O
’	O
s	O
counterexample	O
is	O
due	O
to	O
baird	O
(	O
1995	O
)	O
,	O
though	O
the	O
version	O
we	O
present	O
here	O
is	O
slightly	O
modiﬁed	O
.	O
averaging	O
methods	O
for	O
function	O
approximation	O
were	O
developed	O
by	O
gordon	O
(	O
1995	O
,	O
1996b	O
)	O
.	O
other	O
examples	O
of	O
instability	O
with	O
oﬀ-policy	O
dp	O
methods	O
and	O
more	O
complex	O
methods	O
of	O
function	O
approximation	O
are	O
given	O
by	O
boyan	O
and	O
moore	O
(	O
1995	O
)	O
.	O
bradtke	O
(	O
1993	O
)	O
gives	O
an	O
example	O
in	O
which	O
q-learning	O
using	O
linear	B
function	I
approximation	I
in	O
a	O
linear	O
quadratic	O
regulation	O
problem	O
converges	O
to	O
a	O
destabilizing	O
policy	B
.	O
the	O
deadly	B
triad	I
was	O
ﬁrst	O
identiﬁed	O
by	O
sutton	O
(	O
1995b	O
)	O
and	O
thoroughly	O
analyzed	O
by	O
tsitsiklis	O
and	O
van	O
roy	O
(	O
1997	O
)	O
.	O
the	O
name	O
“	O
deadly	B
triad	I
”	O
is	O
due	O
to	O
sutton	O
(	O
2015a	O
)	O
.	O
this	O
kind	O
of	O
linear	O
analysis	O
was	O
pioneered	O
by	O
tsitsiklis	O
and	O
van	O
roy	O
(	O
1996	O
;	O
1997	O
)	O
,	O
including	O
the	O
dynamic	B
programming	I
operator	O
.	O
diagrams	O
like	O
figure	O
11.3	O
were	O
introduced	O
by	O
lagoudakis	O
and	O
parr	O
(	O
2003	O
)	O
.	O
what	O
we	O
have	O
called	O
the	O
bellman	O
operator	O
,	O
and	O
denoted	O
bπ	O
,	O
is	O
more	O
commonly	O
denoted	O
t	O
π	O
and	O
called	O
a	O
“	O
dynamic	B
programming	I
operator	O
,	O
”	O
while	O
a	O
generalized	O
288	O
11.5	O
chapter	O
11	O
:	O
*oﬀ-policy	O
methods	O
with	B
approximation	I
form	O
,	O
denoted	O
t	O
(	O
λ	O
)	O
,	O
is	O
called	O
the	O
“	O
td	O
(	O
λ	O
)	O
operator	O
”	O
(	O
tsitsiklis	O
and	O
van	O
roy	O
,	O
1996	O
,	O
1997	O
)	O
.	O
the	O
be	O
was	O
ﬁrst	O
proposed	O
as	O
an	O
objective	O
function	O
for	O
dynamic	B
programming	I
by	O
schweitzer	O
and	O
seidmann	O
(	O
1985	O
)	O
.	O
baird	O
(	O
1995	O
,	O
1999	O
)	O
extended	O
it	O
to	O
td	O
learning	O
based	O
on	O
stochastic	O
gradient	O
descent	O
.	O
in	O
the	O
literature	O
,	O
be	O
minimization	O
is	O
often	O
referred	O
to	O
as	O
bellman	O
residual	O
minimization	O
.	O
the	O
earliest	O
a-split	O
example	O
is	O
due	O
to	O
dayan	O
(	O
1992	O
)	O
.	O
the	O
two	O
forms	O
given	O
here	O
were	O
introduced	O
by	O
sutton	O
et	O
al	O
.	O
(	O
2009a	O
)	O
.	O
11.6	O
the	O
contents	O
of	O
this	O
section	O
are	O
new	O
to	O
this	O
text	O
.	O
11.7	O
gradient-td	O
methods	O
were	O
introduced	O
by	O
sutton	O
,	O
szepesv´ari	O
,	O
and	O
maei	O
(	O
2009b	O
)	O
.	O
the	O
methods	O
highlighted	O
in	O
this	O
section	O
were	O
introduced	O
by	O
sutton	O
et	O
al	O
.	O
(	O
2009a	O
)	O
and	O
mahmood	O
et	O
al	O
.	O
(	O
2014	O
)	O
.	O
the	O
most	O
sensitive	O
empirical	O
investigations	O
to	O
date	O
of	O
gradient-td	O
and	O
related	O
methods	O
are	O
given	O
by	O
geist	O
and	O
scherrer	O
(	O
2014	O
)	O
,	O
dann	O
,	O
neumann	O
,	O
and	O
peters	O
(	O
2014	O
)	O
,	O
and	O
white	O
(	O
2015	O
)	O
.	O
the	O
latest	O
developments	O
in	O
the	O
theory	O
of	O
gradient-td	O
methods	O
are	O
developed	O
by	O
yu	O
(	O
2017	O
)	O
.	O
11.8	O
emphatic-td	O
methods	O
were	O
introduced	O
by	O
sutton	O
,	O
mahmood	O
,	O
and	O
white	O
(	O
2016	O
)	O
.	O
full	O
convergence	O
proofs	O
and	O
other	O
theory	O
were	O
later	O
established	O
by	O
yu	O
(	O
2015	O
;	O
2016	O
;	O
yu	O
,	O
mahmood	O
,	O
and	O
sutton	O
,	O
2017	O
)	O
,	O
hallak	O
,	O
tamar	O
,	O
and	O
mannor	O
(	O
2015	O
)	O
,	O
and	O
hallak	O
,	O
tamar	O
,	O
munos	O
,	O
and	O
mannor	O
(	O
2016	O
)	O
.	O
chapter	O
12	O
eligibility	B
traces	I
eligibility	O
traces	O
are	O
one	O
of	O
the	O
basic	O
mechanisms	O
of	O
reinforcement	O
learning	O
.	O
for	O
example	O
,	O
in	O
the	O
popular	O
td	O
(	O
λ	O
)	O
algorithm	O
,	O
the	O
λ	O
refers	O
to	O
the	O
use	O
of	O
an	O
eligibility	O
trace	O
.	O
almost	O
any	O
temporal-diﬀerence	O
(	O
td	O
)	O
method	O
,	O
such	O
as	O
q-learning	O
or	O
sarsa	O
,	O
can	O
be	O
combined	O
with	B
eligibility	I
traces	I
to	O
obtain	O
a	O
more	O
general	O
method	O
that	O
may	O
learn	O
more	O
eﬃciently	O
.	O
eligibility	B
traces	I
unify	O
and	O
generalize	O
td	O
and	O
monte	O
carlo	O
methods	O
.	O
when	O
td	O
meth-	O
ods	O
are	O
augmented	O
with	B
eligibility	I
traces	I
,	O
they	O
produce	O
a	O
family	O
of	O
methods	O
spanning	O
a	O
spectrum	O
that	O
has	O
monte	O
carlo	O
methods	O
at	O
one	O
end	O
(	O
λ	O
=	O
1	O
)	O
and	O
one-step	O
td	O
methods	O
at	O
the	O
other	O
(	O
λ	O
=	O
0	O
)	O
.	O
in	O
between	O
are	O
intermediate	O
methods	O
that	O
are	O
often	O
better	O
than	O
either	O
extreme	O
method	O
.	O
eligibility	B
traces	I
also	O
provide	O
a	O
way	O
of	O
implementing	O
monte	O
carlo	O
methods	O
online	B
and	O
on	O
continuing	O
problems	O
without	O
episodes	B
.	O
of	O
course	O
,	O
we	O
have	O
already	O
seen	O
one	O
way	O
of	O
unifying	O
td	O
and	O
monte	O
carlo	O
methods	O
:	O
the	O
n-step	B
td	O
methods	O
of	O
chapter	O
7.	O
what	O
eligibility	B
traces	I
oﬀer	O
beyond	O
these	O
is	O
an	O
elegant	O
algorithmic	O
mechanism	O
with	O
signiﬁcant	O
computational	O
advantages	O
.	O
the	O
mechanism	O
is	O
a	O
short-term	O
memory	O
vector	B
,	O
the	O
eligibility	O
trace	O
zt	O
∈	O
rd	O
,	O
that	O
parallels	O
the	O
long-term	O
weight	O
vector	B
wt	O
∈	O
rd	O
.	O
the	O
rough	O
idea	O
is	O
that	O
when	O
a	O
component	O
of	O
wt	O
participates	O
in	O
producing	O
an	O
estimated	O
value	B
,	O
then	O
the	O
corresponding	O
component	O
of	O
zt	O
is	O
bumped	O
up	O
and	O
then	O
begins	O
to	O
fade	O
away	O
.	O
learning	O
will	O
then	O
occur	O
in	O
that	O
component	O
of	O
wt	O
if	O
a	O
nonzero	O
td	O
error	O
occurs	O
before	O
the	O
trace	O
falls	O
back	O
to	O
zero	O
.	O
the	O
trace-decay	O
parameter	O
λ	O
∈	O
[	O
0	O
,	O
1	O
]	O
determines	O
the	O
rate	O
at	O
which	O
the	O
trace	O
falls	O
.	O
the	O
primary	O
computational	O
advantage	O
of	O
eligibility	O
traces	O
over	O
n-step	B
methods	I
is	O
that	O
only	O
a	O
single	O
trace	O
vector	B
is	O
required	O
rather	O
than	O
a	O
store	O
of	O
the	O
last	O
n	O
feature	O
vectors	O
.	O
learning	O
also	O
occurs	O
continually	O
and	O
uniformly	O
in	O
time	O
rather	O
than	O
being	O
delayed	O
and	O
then	O
catching	O
up	O
at	O
the	O
end	O
of	O
the	O
episode	O
.	O
in	O
addition	O
learning	O
can	O
occur	O
and	O
aﬀect	O
behavior	O
immediately	O
after	O
a	O
state	B
is	O
encountered	O
rather	O
than	O
being	O
delayed	O
n	O
steps	O
.	O
eligibility	B
traces	I
illustrate	O
that	O
a	O
learning	O
algorithm	O
can	O
sometimes	O
be	O
implemented	O
in	O
a	O
diﬀerent	O
way	O
to	O
obtain	O
computational	O
advantages	O
.	O
many	O
algorithms	O
are	O
most	O
nat-	O
urally	O
formulated	O
and	O
understood	O
as	O
an	O
update	O
of	O
a	O
state	B
’	O
s	O
value	B
based	O
on	O
events	O
that	O
follow	O
that	O
state	B
over	O
multiple	O
future	O
time	O
steps	O
.	O
for	O
example	O
,	O
monte	O
carlo	O
methods	O
(	O
chapter	O
5	O
)	O
update	O
a	O
state	B
based	O
on	O
all	O
the	O
future	O
rewards	O
,	O
and	O
n-step	O
td	O
methods	O
289	O
290	O
chapter	O
12	O
:	O
eligibility	B
traces	I
(	O
chapter	O
7	O
)	O
update	O
based	O
on	O
the	O
next	O
n	O
rewards	O
and	O
state	O
n	O
steps	O
in	O
the	O
future	O
.	O
such	O
formulations	O
,	O
based	O
on	O
looking	O
forward	O
from	O
the	O
updated	O
state	B
,	O
are	O
called	O
forward	O
views	O
.	O
forward	O
views	O
are	O
always	O
somewhat	O
complex	O
to	O
implement	O
because	O
the	O
update	O
depends	O
on	O
later	O
things	O
that	O
are	O
not	O
available	O
at	O
the	O
time	O
.	O
however	O
,	O
as	O
we	O
show	O
in	O
this	O
chap-	O
ter	O
it	O
is	O
often	O
possible	O
to	O
achieve	O
nearly	O
the	O
same	O
updates—and	O
sometimes	O
exactly	O
the	O
same	O
updates—with	O
an	O
algorithm	O
that	O
uses	O
the	O
current	O
td	O
error	O
,	O
looking	O
backward	O
to	O
recently	O
visited	O
states	O
using	O
an	O
eligibility	O
trace	O
.	O
these	O
alternate	O
ways	O
of	O
looking	O
at	O
and	O
implementing	O
learning	O
algorithms	O
are	O
called	O
backward	O
views	O
.	O
backward	O
views	O
,	O
trans-	O
formations	O
between	O
forward-views	O
and	O
backward-views	O
,	O
and	O
equivalences	O
between	O
them	O
date	O
back	O
to	O
the	O
introduction	O
of	O
temporal	O
diﬀerence	O
learning	O
,	O
but	O
have	O
become	O
much	O
more	O
powerful	O
and	O
sophisticated	O
since	O
2014.	O
here	O
we	O
present	O
the	O
basics	O
of	O
the	O
modern	O
view	O
.	O
as	O
usual	O
,	O
ﬁrst	O
we	O
fully	O
develop	O
the	O
ideas	O
for	O
state	O
values	O
and	O
prediction	O
,	O
then	O
extend	O
them	O
to	O
action	B
values	O
and	B
control	I
.	O
we	O
develop	O
them	O
ﬁrst	O
for	O
the	O
on-policy	O
case	O
then	O
extend	O
them	O
to	O
oﬀ-policy	B
learning	O
.	O
our	O
treatment	O
pays	O
special	O
attention	O
to	O
the	O
case	O
of	O
linear	O
function	B
approximation	I
,	O
for	O
which	O
the	O
results	O
with	B
eligibility	I
traces	I
are	O
stronger	O
.	O
all	O
these	O
results	O
apply	O
also	O
to	O
the	O
tabular	O
and	O
state	B
aggregation	I
cases	O
because	O
these	O
are	O
special	O
cases	O
of	O
linear	O
function	B
approximation	I
.	O
12.1	O
the	O
λ-return	B
in	O
chapter	O
7	O
we	O
deﬁned	O
an	O
n-step	B
return	O
as	O
the	O
sum	O
of	O
the	O
ﬁrst	O
n	O
rewards	O
plus	O
the	O
estimated	O
value	B
of	O
the	O
state	B
reached	O
in	O
n	O
steps	O
,	O
each	O
appropriately	O
discounted	O
(	O
7.1	O
)	O
.	O
the	O
general	O
form	O
of	O
that	O
equation	O
,	O
for	O
any	O
parameterized	O
function	O
approximator	O
,	O
is	O
gt	O
:	O
t+n	O
.	O
=	O
rt+1	O
+	O
γrt+2	O
+···	O
+	O
γn−1rt+n	O
+	O
γnˆv	O
(	O
st+n	O
,	O
wt+n−1	O
)	O
,	O
0	O
≤	O
t	O
≤	O
t	O
−	O
n.	O
(	O
12.1	O
)	O
we	O
noted	O
in	O
chapter	O
7	O
that	O
each	O
n-step	B
return	O
,	O
for	O
n	O
≥	O
1	O
,	O
is	O
a	O
valid	O
update	O
target	B
for	O
a	O
tabular	O
learning	O
update	O
,	O
just	O
as	O
it	O
is	O
for	O
an	O
approximate	O
sgd	O
learning	O
update	O
such	O
as	O
(	O
9.7	O
)	O
.	O
now	O
we	O
note	O
that	O
a	O
valid	O
update	O
can	O
be	O
done	O
not	O
just	O
toward	O
any	O
n-step	B
return	O
,	O
but	O
toward	O
any	O
average	O
of	O
n-step	B
returns	O
.	O
for	O
example	O
,	O
an	O
update	O
can	O
be	O
done	O
toward	O
a	O
target	B
that	O
is	O
half	O
of	O
a	O
two-step	O
return	B
and	O
half	O
of	O
a	O
four-step	O
return	B
:	O
1	O
2	O
gt	O
:	O
t+4	O
.	O
any	O
set	O
of	O
n-step	O
returns	O
can	O
be	O
averaged	O
in	O
this	O
way	O
,	O
even	O
an	O
inﬁnite	O
set	O
,	O
as	O
long	O
as	O
the	O
weights	O
on	O
the	O
component	O
returns	O
are	O
positive	O
and	O
sum	O
to	O
1.	O
the	O
composite	O
return	B
possesses	O
an	O
error	B
reduction	I
property	I
similar	O
to	O
that	O
of	O
individual	O
n-step	B
returns	O
(	O
7.3	O
)	O
and	O
thus	O
can	O
be	O
used	O
to	O
construct	O
updates	O
with	O
guaranteed	O
convergence	O
properties	O
.	O
averaging	O
produces	O
a	O
substantial	O
new	O
range	O
of	O
algorithms	O
.	O
for	O
example	O
,	O
one	O
could	O
average	O
one-step	O
and	O
inﬁnite-step	O
returns	O
to	O
obtain	O
another	O
way	O
of	O
interrelating	O
td	O
and	O
monte	O
carlo	O
methods	O
.	O
in	O
principle	O
,	O
one	O
could	O
even	O
average	O
experience-based	O
updates	O
with	O
dp	O
updates	O
to	O
get	O
a	O
simple	O
combination	O
of	O
experience-based	O
and	O
model-based	O
methods	O
(	O
cf	O
.	O
chapter	O
8	O
)	O
.	O
2	O
gt	O
:	O
t+2	O
+	O
1	O
an	O
update	O
that	O
averages	O
simpler	O
component	O
updates	O
is	O
called	O
a	O
compound	B
update	O
.	O
the	O
backup	B
diagram	I
for	O
a	O
compound	B
update	O
consists	O
of	O
the	O
backup	O
diagrams	O
for	O
each	O
of	O
the	O
component	O
updates	O
with	O
a	O
horizontal	O
line	O
above	O
them	O
and	O
the	O
weighting	O
fractions	O
below	O
.	O
12.1.	O
the	O
λ-return	B
291	O
for	O
example	O
,	O
the	O
compound	B
update	O
for	O
the	O
case	O
mentioned	O
at	O
the	O
start	O
of	O
this	O
section	O
,	O
mixing	O
half	O
of	O
a	O
two-step	O
return	B
and	O
half	O
of	O
a	O
four-step	O
return	B
,	O
has	O
the	O
diagram	O
shown	O
to	O
the	O
right	O
.	O
a	O
compound	B
update	O
can	O
only	O
be	O
done	O
when	O
the	O
longest	O
of	O
its	O
component	O
updates	O
is	O
complete	O
.	O
the	O
update	O
at	O
the	O
right	O
,	O
for	O
example	O
,	O
could	O
only	O
be	O
done	O
at	O
time	O
t+4	O
for	O
the	O
estimate	O
formed	O
at	O
time	O
t.	O
in	O
general	O
one	O
would	O
like	O
to	O
limit	O
the	O
length	O
of	O
the	O
longest	O
component	O
update	O
because	O
of	O
the	O
corresponding	O
delay	O
in	O
the	O
updates	O
.	O
the	O
td	O
(	O
λ	O
)	O
algorithm	O
can	O
be	O
understood	O
as	O
one	O
particular	O
way	O
of	O
aver-	O
aging	O
n-step	B
updates	O
.	O
this	O
average	O
contains	O
all	O
the	O
n-step	B
updates	O
,	O
each	O
weighted	O
proportional	O
to	O
λn−1	O
(	O
where	O
λ	O
∈	O
[	O
0	O
,	O
1	O
]	O
)	O
,	O
and	O
is	O
normalized	O
by	O
a	O
factor	O
of	O
1	O
−	O
λ	O
to	O
ensure	O
that	O
the	O
weights	O
sum	O
to	O
1	O
(	O
see	O
figure	O
12.1	O
)	O
.	O
the	O
resulting	O
update	O
is	O
toward	O
a	O
return	B
,	O
called	O
the	O
λ-return	B
,	O
deﬁned	O
in	O
its	O
state-	O
based	O
form	O
by	O
gλ	O
t	O
.	O
=	O
(	O
1	O
−	O
λ	O
)	O
∞	O
(	O
cid:88	O
)	O
n=1	O
λn−1gt	O
:	O
t+n	O
.	O
(	O
12.2	O
)	O
figure	O
12.2	O
further	O
illustrates	O
the	O
weighting	O
on	O
the	O
sequence	O
of	O
n-step	O
returns	O
in	O
the	O
λ-return	O
.	O
the	O
one-step	O
return	O
is	O
given	O
the	O
largest	O
weight	O
,	O
1	O
−	O
λ	O
;	O
the	O
two-step	O
return	B
is	O
given	O
the	O
next	O
largest	O
weight	O
,	O
(	O
1−λ	O
)	O
λ	O
;	O
the	O
three-step	O
return	B
is	O
given	O
the	O
weight	O
(	O
1−λ	O
)	O
λ2	O
;	O
and	O
so	O
on	O
.	O
the	O
weight	O
fades	O
by	O
λ	O
with	O
each	O
additional	O
step	O
.	O
after	O
a	O
terminal	O
state	B
has	O
been	O
reached	O
,	O
all	O
subsequent	O
n-step	B
returns	O
are	O
equal	O
to	O
gt	O
.	O
if	O
we	O
want	O
,	O
we	O
can	O
separate	O
figure	O
12.1	O
:	O
the	O
backup	O
digram	O
for	O
td	O
(	O
λ	O
)	O
.	O
if	O
λ	O
=	O
0	O
,	O
then	O
the	O
overall	O
update	O
reduces	O
to	O
its	O
ﬁrst	O
component	O
,	O
the	O
one-step	O
td	O
update	O
,	O
whereas	O
if	O
λ	O
=	O
1	O
,	O
then	O
the	O
overall	O
update	O
reduces	O
to	O
its	O
last	O
component	O
,	O
the	O
monte	O
carlo	O
update	O
.	O
12121  	O
(	O
1  	O
)	O
 	O
(	O
1  	O
)	O
 2 t t 1······statat+1at 1st+1rt+1strt···st+2rt+2at+2td	O
(	O
 	O
)	O
x=1	O
292	O
chapter	O
12	O
:	O
eligibility	B
traces	I
figure	O
12.2	O
:	O
weighting	O
given	O
in	O
the	O
λ-return	O
to	O
each	O
of	O
the	O
n-step	B
returns	O
.	O
these	O
post-termination	O
terms	O
from	O
the	O
main	O
sum	O
,	O
yielding	O
gλ	O
t	O
=	O
(	O
1	O
−	O
λ	O
)	O
t−t−1	O
(	O
cid:88	O
)	O
n=1	O
λn−1gt	O
:	O
t+n	O
+	O
λt−t−1gt	O
,	O
(	O
12.3	O
)	O
as	O
indicated	O
in	O
the	O
ﬁgures	O
.	O
this	O
equation	O
makes	O
it	O
clearer	O
what	O
happens	O
when	O
λ	O
=	O
1.	O
in	O
this	O
case	O
the	O
main	O
sum	O
goes	O
to	O
zero	O
,	O
and	O
the	O
remaining	O
term	O
reduces	O
to	O
the	O
conventional	O
return	B
,	O
gt	O
.	O
thus	O
,	O
for	O
λ	O
=	O
1	O
,	O
updating	O
according	O
to	O
the	O
λ-return	B
is	O
a	O
monte	O
carlo	O
algorithm	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
λ	O
=	O
0	O
,	O
then	O
the	O
λ-return	B
reduces	O
to	O
gt	O
:	O
t+1	O
,	O
the	O
one-	O
step	O
return	B
.	O
thus	O
,	O
for	O
λ	O
=	O
0	O
,	O
updating	O
according	O
to	O
the	O
λ-return	B
is	O
a	O
one-step	O
td	O
method	O
.	O
exercise	O
12.1	O
just	O
as	O
the	O
return	B
can	O
be	O
written	O
recursively	O
in	O
terms	O
of	O
the	O
ﬁrst	O
reward	O
and	O
itself	O
one-step	O
later	O
(	O
3.9	O
)	O
,	O
so	O
can	O
the	O
λ-return	B
.	O
derive	O
the	O
analogous	O
recursive	O
relationship	O
(	O
cid:3	O
)	O
from	O
(	O
12.2	O
)	O
and	O
(	O
12.1	O
)	O
.	O
exercise	O
12.2	O
the	O
parameter	O
λ	O
characterizes	O
how	O
fast	O
the	O
exponential	O
weighting	O
in	O
figure	O
12.2	O
falls	O
oﬀ	O
,	O
and	O
thus	O
how	O
far	O
into	O
the	O
future	O
the	O
λ-return	B
algorithm	O
looks	O
in	O
determining	O
its	O
update	O
.	O
but	O
a	O
rate	O
factor	O
such	O
as	O
λ	O
is	O
sometimes	O
an	O
awkward	O
way	O
of	O
characterizing	O
the	O
speed	O
of	O
the	O
decay	O
.	O
for	O
some	O
purposes	O
it	O
is	O
better	O
to	O
specify	O
a	O
time	O
constant	O
,	O
or	O
half-life	O
.	O
what	O
is	O
the	O
equation	O
relating	O
λ	O
and	O
the	O
half-life	O
,	O
τλ	O
,	O
the	O
time	O
by	O
(	O
cid:3	O
)	O
which	O
the	O
weighting	O
sequence	O
will	O
have	O
fallen	O
to	O
half	O
of	O
its	O
initial	O
value	B
?	O
we	O
are	O
now	O
ready	O
to	O
deﬁne	O
our	O
ﬁrst	O
learning	O
algorithm	O
based	O
on	O
the	O
λ-return	B
:	O
the	O
oﬀ-line	B
λ-return	O
algorithm	O
.	O
as	O
an	O
oﬀ-line	B
algorithm	O
,	O
it	O
makes	O
no	O
changes	O
to	O
the	O
weight	O
vector	B
during	O
the	O
episode	O
.	O
then	O
,	O
at	O
the	O
end	O
of	O
the	O
episode	O
,	O
a	O
whole	O
sequence	O
of	O
oﬀ-line	O
updates	O
are	O
made	O
according	O
to	O
our	O
usual	O
semi-gradient	O
rule	O
,	O
using	O
the	O
λ-return	B
as	O
the	O
target	B
:	O
wt+1	O
.	O
=	O
wt	O
+	O
α	O
(	O
cid:104	O
)	O
gλ	O
t	O
−	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
(	O
cid:105	O
)	O
∇ˆv	O
(	O
st	O
,	O
wt	O
)	O
,	O
t	O
=	O
0	O
,	O
.	O
.	O
.	O
,	O
t	O
−	O
1	O
.	O
(	O
12.4	O
)	O
1	O
!	O
``	O
weight	O
given	O
tothe	O
3-step	O
returndecay	O
by	O
``	O
weight	O
given	O
toactual	O
,	O
final	O
returntttimeweighttotal	O
area	O
=	O
1is	O
(	O
1  	O
)	O
 2is t t 1weighting	O
12.1.	O
the	O
λ-return	B
293	O
the	O
λ-return	B
gives	O
us	O
an	O
alternative	O
way	O
of	O
moving	O
smoothly	O
between	O
monte	O
carlo	O
and	O
one-step	O
td	O
methods	O
that	O
can	O
be	O
compared	O
with	O
the	O
n-step	B
td	O
way	O
of	O
chapter	O
7.	O
there	O
we	O
assessed	O
eﬀectiveness	O
on	O
a	O
19-state	B
random	O
walk	O
task	O
(	O
example	O
7.1	O
,	O
page	O
144	O
)	O
.	O
figure	O
12.3	O
shows	O
the	O
performance	O
of	O
the	O
oﬀ-line	B
λ-return	O
algorithm	O
on	O
this	O
task	O
along-	O
side	O
that	O
of	O
the	O
n-step	B
methods	I
(	O
repeated	O
from	O
figure	O
7.2	O
)	O
.	O
the	O
experiment	O
was	O
just	O
as	O
described	O
earlier	O
except	O
that	O
for	O
the	O
λ-return	B
algorithm	O
we	O
varied	O
λ	O
instead	O
of	O
n.	O
the	O
performance	O
measure	O
used	O
is	O
the	O
estimated	O
root-mean-squared	O
error	O
between	O
the	O
correct	O
and	O
estimated	O
values	O
of	O
each	O
state	B
measured	O
at	O
the	O
end	O
of	O
the	O
episode	O
,	O
averaged	O
over	O
the	O
ﬁrst	O
10	O
episodes	B
and	O
the	O
19	O
states	O
.	O
note	O
that	O
overall	O
performance	O
of	O
the	O
oﬀ-line	B
λ-return	O
algorithms	O
is	O
comparable	O
to	O
that	O
of	O
the	O
n-step	B
algorithms	O
.	O
in	O
both	O
cases	O
we	O
get	O
best	O
performance	O
with	O
an	O
intermediate	O
value	B
of	O
the	O
bootstrapping	B
parameter	O
,	O
n	O
for	O
n-step	O
methods	O
and	O
λ	O
for	O
the	O
oﬀ-line	B
λ-return	O
algorithm	O
.	O
figure	O
12.3	O
:	O
19-state	B
random	O
walk	O
results	O
(	O
example	O
7.1	O
)	O
:	O
performance	O
of	O
the	O
oﬀ-line	B
λ-	O
return	B
algorithm	O
alongside	O
that	O
of	O
the	O
n-step	B
td	O
methods	O
.	O
in	O
both	O
case	O
,	O
intermediate	O
values	O
of	O
the	O
bootstrapping	B
parameter	O
(	O
λ	O
or	O
n	O
)	O
performed	O
best	O
.	O
the	O
results	O
with	O
the	O
oﬀ-line	B
λ-return	O
algorithm	O
are	O
slightly	O
better	O
at	O
the	O
best	O
values	O
of	O
α	O
and	O
λ	O
,	O
and	O
at	O
high	O
α.	O
the	O
approach	O
that	O
we	O
have	O
been	O
taking	O
so	O
far	O
is	O
what	O
we	O
call	O
the	O
theoretical	O
,	O
or	O
forward	O
,	O
view	O
of	O
a	O
learning	O
algorithm	O
.	O
for	O
each	O
state	B
visited	O
,	O
we	O
look	O
forward	O
in	O
time	O
to	O
all	O
the	O
future	O
rewards	O
and	O
decide	O
how	O
best	O
to	O
combine	O
them	O
.	O
we	O
might	O
imagine	O
ourselves	O
riding	O
the	O
stream	O
of	O
states	O
,	O
looking	O
forward	O
from	O
each	O
state	B
to	O
determine	O
its	O
update	O
,	O
as	O
suggested	O
by	O
figure	O
12.4.	O
after	O
looking	O
forward	O
from	O
and	O
updating	O
one	O
state	B
,	O
we	O
move	O
on	O
to	O
the	O
next	O
and	O
never	O
have	O
to	O
work	O
with	O
the	O
preceding	O
state	B
again	O
.	O
future	O
states	O
,	O
on	O
the	O
other	O
hand	O
,	O
are	O
viewed	O
and	O
processed	O
repeatedly	O
,	O
once	O
from	O
each	O
vantage	O
point	O
preceding	O
them	O
.	O
n-step	B
td	O
methods	O
(	O
from	O
chapter	O
7	O
)	O
↵averagerms	O
errorover	O
19	O
statesand	O
ﬁrst	O
10	O
episodesn=1n=2n=4n=8n=16n=32n=32n=641285122560.550.50.450.350.30.250.40.40.200.80.610.550.50.450.350.30.250.40.40.200.80.61off-line	O
λ-return	B
algorithm↵rms	O
errorat	O
the	O
end	O
of	O
the	O
episodeover	O
the	O
ﬁrst10	O
episodesλ=0λ=.4λ=.8λ=.9λ=.95λ=.975λ=.99λ=1λ=.95	O
294	O
chapter	O
12	O
:	O
eligibility	B
traces	I
figure	O
12.4	O
:	O
the	O
forward	O
view	O
.	O
we	O
decide	O
how	O
to	O
update	O
each	O
state	B
by	O
looking	O
forward	O
to	O
future	O
rewards	O
and	O
states	O
.	O
12.2	O
td	O
(	O
λ	O
)	O
td	O
(	O
λ	O
)	O
is	O
one	O
of	O
the	O
oldest	O
and	O
most	O
widely	O
used	O
algorithms	O
in	O
reinforcement	O
learning	O
.	O
it	O
was	O
the	O
ﬁrst	O
algorithm	O
for	O
which	O
a	O
formal	O
relationship	O
was	O
shown	O
between	O
a	O
more	O
theoretical	O
forward	O
view	O
and	O
a	O
more	O
computationally	O
congenial	O
backward	O
view	O
using	O
eligibility	B
traces	I
.	O
here	O
we	O
will	O
show	O
empirically	O
that	O
it	O
approximates	O
the	O
oﬀ-line	B
λ-return	O
algorithm	O
presented	O
in	O
the	O
previous	O
section	O
.	O
td	O
(	O
λ	O
)	O
improves	O
over	O
the	O
oﬀ-line	B
λ-return	O
algorithm	O
in	O
three	O
ways	O
.	O
first	O
it	O
updates	O
the	O
weight	O
vector	B
on	O
every	O
step	O
of	O
an	O
episode	O
rather	O
than	O
only	O
at	O
the	O
end	O
,	O
and	O
thus	O
its	O
estimates	O
may	O
be	O
better	O
sooner	O
.	O
second	O
,	O
its	O
computations	O
are	O
equally	O
distributed	O
in	O
time	O
rather	O
that	O
all	O
at	O
the	O
end	O
of	O
the	O
episode	O
.	O
and	O
third	O
,	O
it	O
can	O
be	O
applied	O
to	O
continuing	O
problems	O
rather	O
than	O
just	O
episodic	O
problems	O
.	O
in	O
this	O
section	O
we	O
present	O
the	O
semi-gradient	O
version	O
of	O
td	O
(	O
λ	O
)	O
with	B
function	I
approximation	I
.	O
with	B
function	I
approximation	I
,	O
the	O
eligibility	O
trace	O
is	O
a	O
vector	B
zt	O
∈	O
rd	O
with	O
the	O
same	O
number	O
of	O
components	O
as	O
the	O
weight	O
vector	B
wt	O
.	O
whereas	O
the	O
weight	O
vector	B
is	O
a	O
long-term	O
memory	O
,	O
accumulating	B
over	O
the	O
lifetime	O
of	O
the	O
system	O
,	O
the	O
eligibility	O
trace	O
is	O
a	O
short-term	O
memory	O
,	O
typically	O
lasting	O
less	O
time	O
than	O
the	O
length	O
of	O
an	O
episode	O
.	O
eligibility	B
traces	I
assist	O
in	O
the	O
learning	O
process	O
;	O
their	O
only	O
consequence	O
is	O
that	O
they	O
aﬀect	O
the	O
weight	O
vector	B
,	O
and	O
then	O
the	O
weight	O
vector	B
determines	O
the	O
estimated	O
value	B
.	O
in	O
td	O
(	O
λ	O
)	O
,	O
the	O
eligibility	O
trace	O
vector	B
is	O
initialized	O
to	O
zero	O
at	O
the	O
beginning	O
of	O
the	O
episode	O
,	O
is	O
incremented	O
on	O
each	O
time	O
step	O
by	O
the	O
value	B
gradient	O
,	O
and	O
then	O
fades	O
away	O
by	O
γλ	O
:	O
.	O
=	O
0	O
,	O
z−1	O
.	O
=	O
γλzt−1	O
+	O
∇ˆv	O
(	O
st	O
,	O
wt	O
)	O
,	O
zt	O
0	O
≤	O
t	O
≤	O
t	O
,	O
(	O
12.5	O
)	O
where	O
γ	O
is	O
the	O
discount	O
rate	O
and	O
λ	O
is	O
the	O
parameter	O
introduced	O
in	O
the	O
previous	O
section	O
,	O
which	O
we	O
henceforth	O
call	O
the	O
trace-decay	O
parameter	O
.	O
the	O
eligibility	O
trace	O
keeps	O
track	O
of	O
which	O
components	O
of	O
the	O
weight	O
vector	B
have	O
contributed	O
,	O
positively	O
or	O
negatively	O
,	O
to	O
recent	O
state	B
valuations	O
,	O
where	O
“	O
recent	O
”	O
is	O
deﬁned	O
in	O
terms	O
of	O
γλ	O
.	O
(	O
recall	O
that	O
in	O
linear	O
function	B
approximation	I
,	O
∇ˆv	O
(	O
st	O
,	O
wt	O
)	O
is	O
just	O
the	O
feature	O
vector	O
,	O
xt	O
,	O
in	O
which	O
case	O
the	O
eligibility	O
trace	O
vector	B
is	O
just	O
a	O
sum	O
of	O
past	O
,	O
fading	O
,	O
input	O
vectors	O
.	O
)	O
the	O
trace	O
is	O
said	O
to	O
indicate	O
the	O
eligibility	O
of	O
each	O
component	O
of	O
the	O
weight	O
vector	B
for	O
undergoing	O
learning	O
timert+3rt+2rt+1rtst+1st+2st+3stst+1stst+2st+3rt+3rt+2rt+1rt	O
12.2.	O
td	O
(	O
λ	O
)	O
295	O
changes	O
should	O
a	O
reinforcing	O
event	O
occur	O
.	O
the	O
reinforcing	O
events	O
we	O
are	O
concerned	O
with	O
are	O
the	O
moment-by-moment	O
one-step	O
td	O
errors	O
.	O
the	O
td	O
error	O
for	O
state-value	O
prediction	B
is	O
δt	O
.	O
=	O
rt+1	O
+	O
γˆv	O
(	O
st+1	O
,	O
wt	O
)	O
−	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
.	O
(	O
12.6	O
)	O
in	O
td	O
(	O
λ	O
)	O
,	O
the	O
weight	O
vector	B
is	O
updated	O
on	O
each	O
step	O
proportional	O
to	O
the	O
scalar	O
td	O
error	O
and	O
the	O
vector	B
eligibility	O
trace	O
:	O
wt+1	O
.	O
=	O
wt	O
+	O
αδt	O
zt	O
.	O
(	O
12.7	O
)	O
semi-gradient	O
td	O
(	O
λ	O
)	O
for	O
estimating	O
ˆv	O
≈	O
vπ	O
input	O
:	O
the	O
policy	B
π	O
to	O
be	O
evaluated	O
input	O
:	O
a	O
diﬀerentiable	O
function	O
ˆv	O
:	O
s+	O
×	O
rd	O
→	O
r	O
such	O
that	O
ˆv	O
(	O
terminal	O
,	O
·	O
)	O
=	O
0	O
algorithm	O
parameters	O
:	O
step	O
size	O
α	O
>	O
0	O
,	O
trace	O
decay	O
rate	O
λ	O
∈	O
[	O
0	O
,	O
1	O
]	O
initialize	O
value-function	O
weights	O
w	O
arbitrarily	O
(	O
e.g.	O
,	O
w	O
=	O
0	O
)	O
loop	O
for	O
each	O
episode	O
:	O
initialize	O
s	O
z	O
←	O
0	O
loop	O
for	O
each	O
step	O
of	O
episode	O
:	O
|	O
choose	O
a	O
∼	O
π	O
(	O
·|s	O
)	O
|	O
take	O
action	B
a	O
,	O
observe	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
|	O
z	O
←	O
γλz	O
+	O
∇ˆv	O
(	O
s	O
,	O
w	O
)	O
δ	O
←	O
r	O
+	O
γˆv	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
w	O
)	O
−	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
|	O
|	O
w	O
←	O
w	O
+	O
αδ	O
z	O
|	O
s	O
←	O
s	O
(	O
cid:48	O
)	O
until	O
s	O
(	O
cid:48	O
)	O
is	O
terminal	O
(	O
a	O
d-dimensional	O
vector	B
)	O
figure	O
12.5	O
:	O
the	O
backward	O
or	O
mechanistic	O
view	O
of	O
td	O
(	O
λ	O
)	O
.	O
each	O
update	O
depends	O
on	O
the	O
current	O
td	O
error	O
combined	O
with	O
the	O
current	O
eligibility	B
traces	I
of	O
past	O
events	O
.	O
!	O
tetetetettimestst+1st-1st-2st-3stst+1st-1st-2st-3 tztztztzt	O
296	O
chapter	O
12	O
:	O
eligibility	B
traces	I
td	O
(	O
λ	O
)	O
is	O
oriented	O
backward	O
in	O
time	O
.	O
at	O
each	O
moment	O
we	O
look	O
at	O
the	O
current	O
td	O
error	O
and	O
assign	O
it	O
backward	O
to	O
each	O
prior	O
state	O
according	O
to	O
how	O
much	O
that	O
state	B
contributed	O
to	O
the	O
current	O
eligibility	O
trace	O
at	O
that	O
time	O
.	O
we	O
might	O
imagine	O
ourselves	O
riding	O
along	O
the	O
stream	O
of	O
states	O
,	O
computing	O
td	O
errors	O
,	O
and	O
shouting	O
them	O
back	O
to	O
the	O
previously	O
visited	O
states	O
,	O
as	O
suggested	O
by	O
figure	O
12.5.	O
where	O
the	O
td	O
error	O
and	O
traces	O
come	O
together	O
,	O
we	O
get	O
the	O
update	O
given	O
by	O
(	O
12.7	O
)	O
,	O
changing	O
the	O
values	O
of	O
those	O
past	O
states	O
for	O
when	O
they	O
occur	O
again	O
in	O
the	O
future	O
.	O
to	O
better	O
understand	O
the	O
backward	O
view	O
of	O
td	O
(	O
λ	O
)	O
,	O
consider	O
what	O
happens	O
at	O
various	O
values	O
of	O
λ.	O
if	O
λ	O
=	O
0	O
,	O
then	O
by	O
(	O
12.5	O
)	O
the	O
trace	O
at	O
t	O
is	O
exactly	O
the	O
value	B
gradient	O
cor-	O
responding	O
to	O
st.	O
thus	O
the	O
td	O
(	O
λ	O
)	O
update	O
(	O
12.7	O
)	O
reduces	O
to	O
the	O
one-step	O
semi-gradient	O
td	O
update	O
treated	O
in	O
chapter	O
9	O
(	O
and	O
,	O
in	O
the	O
tabular	O
case	O
,	O
to	O
the	O
simple	O
td	O
rule	O
(	O
6.2	O
)	O
)	O
.	O
this	O
is	O
why	O
that	O
algorithm	O
was	O
called	O
td	O
(	O
0	O
)	O
.	O
in	O
terms	O
of	O
figure	O
12.5	O
,	O
td	O
(	O
0	O
)	O
is	O
the	O
case	O
in	O
which	O
only	O
the	O
one	O
state	B
preceding	O
the	O
current	O
one	O
is	O
changed	O
by	O
the	O
td	O
error	O
.	O
for	O
larger	O
values	O
of	O
λ	O
,	O
but	O
still	O
λ	O
<	O
1	O
,	O
more	O
of	O
the	O
preceding	O
states	O
are	O
changed	O
,	O
but	O
each	O
more	O
temporally	O
distant	O
state	B
is	O
changed	O
less	O
because	O
the	O
corresponding	O
eligibility	O
trace	O
is	O
smaller	O
,	O
as	O
suggested	O
by	O
the	O
ﬁgure	O
.	O
we	O
say	O
that	O
the	O
earlier	O
states	O
are	O
given	O
less	O
credit	O
for	O
the	O
td	O
error	O
.	O
if	O
λ	O
=	O
1	O
,	O
then	O
the	O
credit	O
given	O
to	O
earlier	O
states	O
falls	O
only	O
by	O
γ	O
per	O
step	O
.	O
this	O
turns	O
out	O
to	O
be	O
just	O
the	O
right	O
thing	O
to	O
do	O
to	O
achieve	O
monte	O
carlo	O
behavior	O
.	O
for	O
example	O
,	O
remember	O
that	O
the	O
td	O
error	O
,	O
δt	O
,	O
includes	O
an	O
undiscounted	O
term	O
of	O
rt+1	O
.	O
in	O
passing	O
this	O
back	O
k	O
steps	O
it	O
needs	O
to	O
be	O
discounted	O
,	O
like	O
any	O
reward	O
in	O
a	O
return	B
,	O
by	O
γk	O
,	O
which	O
is	O
just	O
what	O
the	O
falling	O
eligibility	O
trace	O
achieves	O
.	O
if	O
λ	O
=	O
1	O
and	O
γ	O
=	O
1	O
,	O
then	O
the	O
eligibility	B
traces	I
do	O
not	O
decay	O
at	O
all	O
with	O
time	O
.	O
in	O
this	O
case	O
the	O
method	O
behaves	O
like	O
a	O
monte	O
carlo	O
method	O
for	O
an	O
undiscounted	O
,	O
episodic	O
task	O
.	O
if	O
λ	O
=	O
1	O
,	O
the	O
algorithm	O
is	O
also	O
known	O
as	O
td	O
(	O
1	O
)	O
.	O
td	O
(	O
1	O
)	O
is	O
a	O
way	O
of	O
implementing	O
monte	O
carlo	O
algorithms	O
that	O
is	O
more	O
general	O
than	O
those	O
presented	O
earlier	O
and	O
that	O
signiﬁcantly	O
increases	O
their	O
range	O
of	O
applicability	O
.	O
whereas	O
the	O
earlier	O
monte	O
carlo	O
methods	O
were	O
limited	O
to	O
episodic	O
tasks	O
,	O
td	O
(	O
1	O
)	O
can	O
be	O
applied	O
to	O
discounted	O
continuing	B
tasks	I
as	O
well	O
.	O
moreover	O
,	O
td	O
(	O
1	O
)	O
can	O
be	O
performed	O
incremen-	O
tally	O
and	O
online	O
.	O
one	O
disadvantage	O
of	O
monte	O
carlo	O
methods	O
is	O
that	O
they	O
learn	O
nothing	O
from	O
an	O
episode	O
until	O
it	O
is	O
over	O
.	O
for	O
example	O
,	O
if	O
a	O
monte	O
carlo	O
control	B
method	O
takes	O
an	O
action	B
that	O
produces	O
a	O
very	O
poor	O
reward	O
but	O
does	O
not	O
end	O
the	O
episode	O
,	O
then	O
the	O
agent	O
’	O
s	O
tendency	O
to	O
repeat	O
the	O
action	B
will	O
be	O
undiminished	O
during	O
the	O
episode	O
.	O
online	B
td	O
(	O
1	O
)	O
,	O
on	O
the	O
other	O
hand	O
,	O
learns	O
in	O
an	O
n-step	B
td	O
way	O
from	O
the	O
incomplete	O
ongoing	O
episode	O
,	O
where	O
the	O
n	O
steps	O
are	O
all	O
the	O
way	O
up	O
to	O
the	O
current	O
step	O
.	O
if	O
something	O
unusually	O
good	O
or	O
bad	O
happens	O
during	O
an	O
episode	O
,	O
control	B
methods	O
based	O
on	O
td	O
(	O
1	O
)	O
can	O
learn	O
immediately	O
and	O
alter	O
their	O
behavior	O
on	O
that	O
same	O
episode	O
.	O
it	O
is	O
revealing	O
to	O
revisit	O
the	O
19-state	B
random	O
walk	O
example	O
(	O
example	O
7.1	O
)	O
to	O
see	O
how	O
well	O
td	O
(	O
λ	O
)	O
ddoes	O
in	O
approximating	O
the	O
oﬀ-line	B
λ-return	O
algorithm	O
.	O
the	O
results	O
for	O
both	O
algorithms	O
are	O
shown	O
in	O
figure	O
12.6.	O
for	O
each	O
λ	O
value	B
,	O
if	O
α	O
is	O
selected	O
optimally	O
for	O
it	O
(	O
or	O
smaller	O
)	O
,	O
then	O
the	O
two	O
algorithms	O
perform	O
virtually	O
identically	O
.	O
if	O
α	O
is	O
chosen	O
larger	O
than	O
is	O
optimal	O
,	O
however	O
,	O
then	O
the	O
λ-return	B
algorithm	O
is	O
only	O
a	O
little	O
worse	O
whereas	O
td	O
(	O
λ	O
)	O
is	O
much	O
worse	O
and	O
may	O
even	O
be	O
unstable	O
.	O
this	O
is	O
not	O
catastrophic	O
for	O
td	O
(	O
λ	O
)	O
on	O
this	O
problem	O
,	O
as	O
these	O
higher	O
parameter	O
values	O
are	O
not	O
what	O
one	O
would	O
want	O
to	O
use	O
anyway	O
,	O
but	O
for	O
other	O
problems	O
it	O
can	O
be	O
a	O
signiﬁcant	O
weakness	O
.	O
12.3.	O
n-step	B
truncated	O
λ-return	B
methods	I
297	O
figure	O
12.6	O
:	O
19-state	B
random	O
walk	O
results	O
(	O
example	O
7.1	O
)	O
:	O
performance	O
of	O
td	O
(	O
λ	O
)	O
alongside	O
that	O
of	O
the	O
oﬀ-line	B
λ-return	O
algorithm	O
.	O
the	O
two	O
algorithms	O
performed	O
virtually	O
identically	O
at	O
low	O
(	O
less	O
than	O
optimal	O
)	O
α	O
values	O
,	O
but	O
td	O
(	O
λ	O
)	O
was	O
worse	O
at	O
high	O
α	O
values	O
.	O
linear	O
td	O
(	O
λ	O
)	O
has	O
been	O
proved	O
to	O
converge	O
in	O
the	O
on-policy	O
case	O
if	O
the	O
step-size	O
pa-	O
rameter	O
is	O
reduced	O
over	O
time	O
according	O
to	O
the	O
usual	O
conditions	O
(	O
2.7	O
)	O
.	O
just	O
as	O
discussed	O
in	O
section	O
9.4	O
,	O
convergence	O
is	O
not	O
to	O
the	O
minimum-error	O
weight	O
vector	B
,	O
but	O
to	O
a	O
nearby	O
weight	O
vector	B
that	O
depends	O
on	O
λ.	O
the	O
bound	O
on	O
solution	O
quality	O
presented	O
in	O
that	O
section	O
(	O
9.14	O
)	O
can	O
now	O
be	O
generalized	O
to	O
apply	O
to	O
any	O
λ.	O
for	O
the	O
continuing	O
discounted	O
case	O
,	O
ve	O
(	O
w∞	O
)	O
≤	O
1	O
−	O
γλ	O
1	O
−	O
γ	O
min	O
w	O
ve	O
(	O
w	O
)	O
.	O
(	O
12.8	O
)	O
that	O
is	O
,	O
the	O
asymptotic	O
error	O
is	O
no	O
more	O
than	O
1−γλ	O
1−γ	O
times	O
the	O
smallest	O
possible	O
error	O
.	O
as	O
λ	O
approaches	O
1	O
,	O
the	O
bound	O
approaches	O
the	O
minimum	O
error	O
(	O
and	O
it	O
is	O
loosest	O
at	O
λ	O
=	O
0	O
)	O
.	O
in	O
practice	O
,	O
however	O
,	O
λ	O
=	O
1	O
is	O
often	O
the	O
poorest	O
choice	O
,	O
as	O
will	O
be	O
illustrated	O
later	O
in	O
figure	O
12.14.	O
exercise	O
12.3	O
some	O
insight	O
into	O
how	O
td	O
(	O
λ	O
)	O
can	O
closely	O
approximate	B
the	O
oﬀ-line	B
λ-return	O
algorithm	O
can	O
be	O
gained	O
by	O
seeing	O
that	O
the	O
latter	O
’	O
s	O
error	O
term	O
(	O
in	O
brackets	O
in	O
(	O
12.4	O
)	O
)	O
can	O
be	O
written	O
as	O
the	O
sum	O
of	O
td	O
errors	O
(	O
12.6	O
)	O
for	O
a	O
single	O
ﬁxed	O
w.	O
show	O
this	O
,	O
following	O
the	O
pattern	O
of	O
(	O
6.6	O
)	O
,	O
and	O
using	O
the	O
recursive	O
relationship	O
for	O
the	O
λ-return	B
you	O
obtained	O
in	O
(	O
cid:3	O
)	O
exercise	O
12.1.	O
exercise	O
12.4	O
use	O
your	O
result	O
from	O
the	O
preceding	O
exercise	O
to	O
show	O
that	O
,	O
if	O
the	O
weight	O
updates	O
over	O
an	O
episode	O
were	O
computed	O
on	O
each	O
step	O
but	O
not	O
actually	O
used	O
to	O
change	O
the	O
weights	O
(	O
w	O
remained	O
ﬁxed	O
)	O
,	O
then	O
the	O
sum	O
of	O
td	O
(	O
λ	O
)	O
’	O
s	O
weight	O
updates	O
would	O
be	O
the	O
(	O
cid:3	O
)	O
same	O
as	O
the	O
sum	O
of	O
the	O
oﬀ-line	B
λ-return	O
algorithm	O
’	O
s	O
updates	O
.	O
12.3	O
n-step	B
truncated	O
λ-return	B
methods	I
the	O
oﬀ-line	B
λ-return	O
algorithm	O
is	O
an	O
important	O
ideal	O
,	O
but	O
it	O
’	O
s	O
of	O
limited	O
utility	O
because	O
it	O
uses	O
the	O
λ-return	B
(	O
12.2	O
)	O
,	O
which	O
is	O
not	O
known	O
until	O
the	O
end	O
of	O
the	O
episode	O
.	O
in	O
the	O
off-line	O
λ-return	B
algorithm	O
(	O
from	O
the	O
previous	O
section	O
)	O
↵λ=0λ=.4λ=.8λ=.9λ=.95.975.991td	O
(	O
λ	O
)	O
↵λ=.8λ=.9rms	O
errorat	O
the	O
end	O
of	O
the	O
episodeover	O
the	O
ﬁrst10	O
episodes0.40.200.80.61λ=0λ=.4λ=.8λ=.9λ=.95λ=.975λ=.99λ=1λ=.950.550.50.450.350.30.250.40.40.200.80.61	O
298	O
chapter	O
12	O
:	O
eligibility	B
traces	I
continuing	O
case	O
,	O
the	O
λ-return	B
is	O
technically	O
never	O
known	O
,	O
as	O
it	O
depends	O
on	O
n-step	B
returns	O
for	O
arbitrarily	O
large	O
n	O
,	O
and	O
thus	O
on	O
rewards	O
arbitrarily	O
far	O
in	O
the	O
future	O
.	O
however	O
,	O
the	O
dependence	O
gets	O
weaker	O
for	O
long-delayed	O
rewards	O
,	O
falling	O
by	O
γλ	O
for	O
each	O
step	O
of	O
delay	O
.	O
a	O
natural	O
approximation	O
then	O
would	O
be	O
to	O
truncate	O
the	O
sequence	O
after	O
some	O
number	O
of	O
steps	O
.	O
our	O
existing	O
notion	O
of	O
n-step	O
returns	O
provides	O
a	O
natural	O
way	O
to	O
do	O
this	O
in	O
which	O
the	O
missing	O
rewards	O
are	O
replaced	O
with	O
estimated	O
values	O
.	O
in	O
general	O
,	O
we	O
deﬁne	O
the	O
truncated	B
λ-return	I
for	O
time	O
t	O
,	O
given	O
data	O
only	O
up	O
to	O
some	O
later	O
horizon	O
,	O
h	O
,	O
as	O
gλ	O
t	O
:	O
h	O
.	O
=	O
(	O
1	O
−	O
λ	O
)	O
h−t−1	O
(	O
cid:88	O
)	O
n=1	O
λn−1gt	O
:	O
t+n	O
+	O
λh−t−1gt	O
:	O
h	O
,	O
0	O
≤	O
t	O
<	O
h	O
≤	O
t.	O
(	O
12.9	O
)	O
if	O
you	O
compare	O
this	O
equation	O
with	O
the	O
λ-return	B
(	O
12.3	O
)	O
,	O
it	O
is	O
clear	O
that	O
the	O
horizon	O
h	O
is	O
playing	O
the	O
same	O
role	O
as	O
was	O
previously	O
played	O
by	O
t	O
,	O
the	O
time	O
of	O
termination	O
.	O
whereas	O
in	O
the	O
λ-return	O
there	O
is	O
a	O
residual	O
weighting	O
given	O
to	O
the	O
true	O
return	O
,	O
here	O
it	O
is	O
given	O
to	O
the	O
longest	O
available	O
n-step	B
return	O
,	O
the	O
(	O
h−t	O
)	O
-step	O
return	B
(	O
figure	O
12.2	O
)	O
.	O
the	O
truncated	B
λ-return	I
immediately	O
gives	O
rise	O
to	O
a	O
family	O
of	O
n-step	O
λ-return	B
algorithms	O
similar	O
to	O
the	O
n-step	B
methods	I
of	O
chapter	O
7.	O
in	O
all	O
these	O
algorithms	O
,	O
updates	O
are	O
delayed	O
by	O
n	O
steps	O
and	O
only	O
take	O
into	O
account	O
the	O
ﬁrst	O
n	O
rewards	O
,	O
but	O
now	O
all	O
the	O
k-step	O
returns	O
are	O
included	O
for	O
1	O
≤	O
k	O
≤	O
n	O
(	O
whereas	O
the	O
earlier	O
n-step	B
algorithms	O
used	O
only	O
the	O
n-step	B
return	O
)	O
,	O
weighted	O
geometrically	O
as	O
in	O
figure	O
12.2.	O
in	O
the	O
state-value	O
case	O
,	O
this	O
family	O
of	O
algorithms	O
is	O
known	O
as	O
truncated	O
td	O
(	O
λ	O
)	O
,	O
or	O
ttd	O
(	O
λ	O
)	O
.	O
the	O
compound	B
backup	O
diagram	O
,	O
shown	O
in	O
figure	O
12.7	O
,	O
is	O
similar	O
to	O
that	O
for	O
td	O
(	O
λ	O
)	O
(	O
figure	O
12.1	O
)	O
except	O
that	O
the	O
longest	O
component	O
update	O
is	O
at	O
most	O
n	O
steps	O
rather	O
than	O
always	O
going	O
all	O
the	O
way	O
to	O
the	O
end	O
of	O
figure	O
12.7	O
:	O
the	O
backup	B
diagram	I
for	O
truncated	B
td	O
(	O
λ	O
)	O
.	O
1  	O
(	O
1  	O
)	O
 	O
(	O
1  	O
)	O
 2 t t 1or	O
,	O
ift+n t········· n 1statat+1at 1st+nrt+nst+1rt+1strtat+n 1n-steptruncatedtd	O
(	O
 	O
)	O
12.4.	O
redoing	O
updates	O
:	O
the	O
online	B
λ-return	O
algorithm	O
299	O
the	O
episode	O
.	O
ttd	O
(	O
λ	O
)	O
is	O
deﬁned	O
by	O
(	O
cf	O
.	O
(	O
9.15	O
)	O
)	O
:	O
.	O
wt+n	O
=	O
wt+n−1	O
+	O
α	O
(	O
cid:2	O
)	O
gλ	O
t	O
:	O
t+n	O
−	O
ˆv	O
(	O
st	O
,	O
wt+n−1	O
)	O
(	O
cid:3	O
)	O
∇ˆv	O
(	O
st	O
,	O
wt+n−1	O
)	O
,	O
0	O
≤	O
t	O
<	O
t.	O
this	O
algorithm	O
can	O
be	O
implemented	O
eﬃciently	O
so	O
that	O
per-step	O
computation	O
does	O
not	O
scale	O
with	O
n	O
(	O
though	O
of	O
course	O
memory	O
must	O
)	O
.	O
much	O
as	O
in	O
n-step	B
td	O
methods	O
,	O
no	O
updates	O
are	O
made	O
on	O
the	O
ﬁrst	O
n	O
−	O
1	O
time	O
steps	O
,	O
and	O
n	O
−	O
1	O
additional	O
updates	O
are	O
made	O
upon	O
termination	O
.	O
eﬃcient	O
implementation	O
relies	O
on	O
the	O
fact	O
that	O
the	O
k-step	O
λ-return	B
can	O
be	O
written	O
exactly	O
as	O
gλ	O
t	O
:	O
t+k	O
=	O
ˆv	O
(	O
st	O
,	O
wt−1	O
)	O
+	O
t+k−1	O
(	O
cid:88	O
)	O
i=t	O
(	O
γλ	O
)	O
i−tδ	O
(	O
cid:48	O
)	O
i	O
,	O
where	O
δ	O
(	O
cid:48	O
)	O
t	O
.	O
=	O
rt+1	O
+	O
γˆv	O
(	O
st+1	O
,	O
wt	O
)	O
−	O
ˆv	O
(	O
st	O
,	O
wt−1	O
)	O
.	O
(	O
12.10	O
)	O
exercise	O
12.5	O
several	O
times	O
in	O
this	O
book	O
(	O
often	O
in	O
exercises	O
)	O
we	O
have	O
established	O
that	O
returns	O
can	O
be	O
written	O
as	O
sums	O
of	O
td	O
errors	O
if	O
the	O
value	B
function	I
is	O
held	O
constant	O
.	O
why	O
(	O
cid:3	O
)	O
is	O
(	O
12.10	O
)	O
another	O
instance	O
of	O
this	O
?	O
prove	O
(	O
12.10	O
)	O
.	O
12.4	O
redoing	O
updates	O
:	O
the	O
online	B
λ-return	O
algo-	O
rithm	O
choosing	O
the	O
truncation	O
parameter	O
n	O
in	O
truncated	O
td	O
(	O
λ	O
)	O
involves	O
a	O
tradeoﬀ	O
.	O
n	O
should	O
be	O
large	O
so	O
that	O
the	O
method	O
closely	O
approximates	O
the	O
oﬀ-line	B
λ-return	O
algorithm	O
,	O
but	O
it	O
should	O
also	O
be	O
small	O
so	O
that	O
the	O
updates	O
can	O
be	O
made	O
sooner	O
and	O
can	O
inﬂuence	O
behavior	O
sooner	O
.	O
can	O
we	O
get	O
the	O
best	O
of	O
both	O
?	O
well	O
,	O
yes	O
,	O
in	O
principle	O
we	O
can	O
,	O
albeit	O
at	O
the	O
cost	O
of	O
computational	O
complexity	O
.	O
the	O
idea	O
is	O
that	O
,	O
on	O
each	O
time	O
step	O
as	O
you	O
gather	O
a	O
new	O
increment	O
of	O
data	O
,	O
you	O
go	O
back	O
and	O
redo	O
all	O
the	O
updates	O
since	O
the	O
beginning	O
of	O
the	O
current	O
episode	O
.	O
the	O
new	O
updates	O
will	O
be	O
better	O
than	O
the	O
ones	O
you	O
previously	O
made	O
because	O
now	O
they	O
can	O
take	O
into	O
account	O
the	O
time	O
step	O
’	O
s	O
new	O
data	O
.	O
that	O
is	O
,	O
the	O
updates	O
are	O
always	O
towards	O
an	O
n-step	B
truncated	O
λ-return	B
target	O
,	O
but	O
they	O
always	O
use	O
the	O
latest	O
horizon	O
.	O
in	O
each	O
pass	O
over	O
that	O
episode	O
you	O
can	O
use	O
a	O
slightly	O
longer	O
horizon	O
and	O
obtain	O
slightly	O
better	O
results	O
.	O
recall	O
that	O
the	O
n-step	B
truncated	O
λ-return	B
is	O
deﬁned	O
by	O
gλ	O
t	O
:	O
h	O
.	O
=	O
(	O
1	O
−	O
λ	O
)	O
h−t−1	O
(	O
cid:88	O
)	O
n=1	O
λn−1gt	O
:	O
t+n	O
+	O
λh−t−1gt	O
:	O
h.	O
(	O
12.9	O
)	O
let	O
us	O
step	O
through	O
how	O
this	O
target	B
could	O
ideally	O
be	O
used	O
if	O
computational	O
complexity	O
was	O
not	O
an	O
issue	O
.	O
the	O
episode	O
begins	O
with	O
an	O
estimate	O
at	O
time	O
0	O
using	O
the	O
weights	O
w0	O
from	O
the	O
end	O
of	O
the	O
previous	O
episode	O
.	O
learning	O
begins	O
when	O
the	O
data	O
horizon	O
is	O
extended	O
to	O
time	O
step	O
1.	O
the	O
target	B
for	O
the	O
estimate	O
at	O
step	O
0	O
,	O
given	O
the	O
data	O
up	O
to	O
horizon	O
1	O
,	O
could	O
300	O
chapter	O
12	O
:	O
eligibility	B
traces	I
only	O
be	O
the	O
one-step	O
return	O
g0:1	O
,	O
which	O
includes	O
r1	O
and	O
bootstraps	O
from	O
the	O
estimate	O
ˆv	O
(	O
s1	O
,	O
w0	O
)	O
.	O
note	O
that	O
this	O
is	O
exactly	O
what	O
gλ	O
0:1	O
is	O
,	O
with	O
the	O
sum	O
in	O
the	O
ﬁrst	O
term	O
of	O
(	O
12.9	O
)	O
degenerating	O
to	O
zero	O
.	O
using	O
this	O
update	O
target	B
,	O
we	O
construct	O
w1	O
.	O
then	O
,	O
after	O
advancing	O
the	O
data	O
horizon	O
to	O
step	O
2	O
,	O
what	O
do	O
we	O
do	O
?	O
we	O
have	O
new	O
data	O
in	O
the	O
form	O
of	O
r2	O
and	O
s2	O
,	O
as	O
well	O
as	O
the	O
new	O
w1	O
,	O
so	O
now	O
we	O
can	O
construct	O
a	O
better	O
update	O
target	B
gλ	O
0:2	O
for	O
the	O
ﬁrst	O
update	O
from	O
s0	O
as	O
well	O
as	O
a	O
better	O
update	O
target	B
gλ	O
1:2	O
for	O
the	O
second	O
update	O
from	O
s1	O
.	O
using	O
these	O
improved	O
targets	O
,	O
we	O
redo	O
the	O
updates	O
at	O
s1	O
and	O
s2	O
,	O
starting	O
again	O
from	O
w0	O
,	O
to	O
produce	O
w2	O
.	O
now	O
we	O
advance	O
the	O
horizon	O
to	O
step	O
3	O
and	O
repeat	O
,	O
going	O
all	O
the	O
way	O
back	O
to	O
produce	O
three	O
new	O
targets	O
,	O
redoing	O
all	O
updates	O
starting	O
from	O
the	O
original	O
w0	O
to	O
produce	O
w3	O
,	O
and	O
so	O
on	O
.	O
each	O
time	O
the	O
horizon	O
is	O
advanced	O
,	O
all	O
the	O
updates	O
are	O
redone	O
starting	O
from	O
w0	O
using	O
the	O
weight	O
vector	B
from	O
the	O
preceding	O
horizon	O
.	O
this	O
conceptual	O
algorithm	O
involves	O
multiple	O
passes	O
over	O
the	O
episode	O
,	O
one	O
at	O
each	O
hori-	O
zon	O
,	O
each	O
generating	O
a	O
diﬀerent	O
sequence	O
of	O
weight	O
vectors	O
.	O
to	O
describe	O
it	O
clearly	O
we	O
have	O
to	O
distinguish	O
between	O
the	O
weight	O
vectors	O
computed	O
at	O
the	O
diﬀerent	O
horizons	O
.	O
let	O
us	O
use	O
wh	O
t	O
to	O
denote	O
the	O
weights	O
used	O
to	O
generate	O
the	O
value	B
at	O
time	O
t	O
in	O
the	O
sequence	O
at	O
horizon	O
h.	O
the	O
ﬁrst	O
weight	O
vector	B
wh	O
0	O
in	O
each	O
sequence	O
is	O
that	O
inherited	O
from	O
the	O
previous	O
episode	O
,	O
and	O
the	O
last	O
weight	O
vector	B
wh	O
h	O
in	O
each	O
sequence	O
deﬁnes	O
the	O
ultimate	O
weight-vector	O
sequence	O
of	O
the	O
algorithm	O
.	O
at	O
the	O
ﬁnal	O
horizon	O
h	O
=	O
t	O
we	O
obtain	O
the	O
ﬁnal	O
weights	O
wt	O
t	O
which	O
will	O
be	O
passed	O
on	O
to	O
form	O
the	O
initial	O
weights	O
of	O
the	O
next	O
episode	O
.	O
with	O
these	O
conventions	O
,	O
the	O
three	O
ﬁrst	O
sequences	O
described	O
in	O
the	O
previous	O
paragraph	O
can	O
be	O
given	O
explicitly	O
:	O
h	O
=	O
1	O
:	O
w1	O
1	O
h	O
=	O
2	O
:	O
w2	O
1	O
w2	O
2	O
h	O
=	O
3	O
:	O
w3	O
1	O
w3	O
2	O
w3	O
3	O
.	O
=	O
w1	O
.	O
=	O
w2	O
.	O
=	O
w2	O
.	O
=	O
w3	O
.	O
=	O
w3	O
.	O
=	O
w3	O
0	O
)	O
(	O
cid:3	O
)	O
∇ˆv	O
(	O
s0	O
,	O
w1	O
0	O
)	O
(	O
cid:3	O
)	O
∇ˆv	O
(	O
s0	O
,	O
w2	O
1	O
)	O
(	O
cid:3	O
)	O
∇ˆv	O
(	O
s1	O
,	O
w2	O
0	O
)	O
(	O
cid:3	O
)	O
∇ˆv	O
(	O
s0	O
,	O
w3	O
1	O
)	O
(	O
cid:3	O
)	O
∇ˆv	O
(	O
s1	O
,	O
w3	O
2	O
)	O
(	O
cid:3	O
)	O
∇ˆv	O
(	O
s2	O
,	O
w3	O
0	O
)	O
,	O
0	O
)	O
,	O
1	O
)	O
,	O
0	O
)	O
,	O
1	O
)	O
,	O
2	O
)	O
.	O
0:1	O
−	O
ˆv	O
(	O
s0	O
,	O
w1	O
0:2	O
−	O
ˆv	O
(	O
s0	O
,	O
w2	O
1:2	O
−	O
ˆv	O
(	O
s1	O
,	O
w2	O
0:3	O
−	O
ˆv	O
(	O
s0	O
,	O
w3	O
1:3	O
−	O
ˆv	O
(	O
s1	O
,	O
w3	O
2:3	O
−	O
ˆv	O
(	O
s2	O
,	O
w3	O
0	O
+	O
α	O
(	O
cid:2	O
)	O
gλ	O
0	O
+	O
α	O
(	O
cid:2	O
)	O
gλ	O
1	O
+	O
α	O
(	O
cid:2	O
)	O
gλ	O
0	O
+	O
α	O
(	O
cid:2	O
)	O
gλ	O
1	O
+	O
α	O
(	O
cid:2	O
)	O
gλ	O
2	O
+	O
α	O
(	O
cid:2	O
)	O
gλ	O
t	O
:	O
h	O
−	O
ˆv	O
(	O
st	O
,	O
wh	O
.	O
=	O
wt	O
the	O
general	O
form	O
for	O
the	O
update	O
is	O
wh	O
t+1	O
.	O
=	O
wh	O
t	O
+	O
α	O
(	O
cid:2	O
)	O
gλ	O
t	O
)	O
(	O
cid:3	O
)	O
∇ˆv	O
(	O
st	O
,	O
wh	O
t	O
)	O
,	O
0	O
≤	O
t	O
<	O
h	O
≤	O
t.	O
this	O
update	O
,	O
together	O
with	O
wt	O
t	O
deﬁnes	O
the	O
online	B
λ-return	O
algorithm	O
.	O
the	O
online	B
λ-return	O
algorithm	O
is	O
fully	O
online	B
,	O
determining	O
a	O
new	O
weight	O
vector	B
wt	O
at	O
each	O
step	O
t	O
during	O
an	O
episode	O
,	O
using	O
only	O
information	O
available	O
at	O
time	O
t.	O
its	O
main	O
drawback	O
is	O
that	O
it	O
is	O
computationally	O
complex	O
,	O
passing	O
over	O
the	O
entire	O
episode	O
so	O
far	O
on	O
every	O
step	O
.	O
note	O
that	O
it	O
is	O
strictly	O
more	O
complex	O
than	O
the	O
oﬀ-line	B
λ-return	O
algorithm	O
,	O
which	O
passes	O
through	O
all	O
the	O
steps	O
at	O
the	O
time	O
of	O
termination	O
but	O
does	O
not	O
make	O
any	O
updates	O
during	O
the	O
episode	O
.	O
in	O
return	O
,	O
the	O
online	B
algorithm	O
can	O
be	O
expected	O
to	O
perform	O
better	O
than	O
the	O
oﬀ-line	B
one	O
,	O
not	O
only	O
during	O
the	O
episode	O
when	O
it	O
makes	O
an	O
update	O
while	O
12.5.	O
true	B
online	I
td	O
(	O
λ	O
)	O
301	O
the	O
oﬀ-line	B
algorithm	O
makes	O
none	O
,	O
but	O
also	O
at	O
the	O
end	O
of	O
the	O
episode	O
because	O
the	O
weight	O
vector	B
used	O
in	O
bootstrapping	O
(	O
in	O
gλ	O
t	O
:	O
h	O
)	O
has	O
had	O
a	O
greater	O
number	O
of	O
informative	O
updates	O
.	O
this	O
eﬀect	O
can	O
be	O
seen	O
if	O
one	O
looks	O
carefully	O
at	O
figure	O
12.8	O
,	O
which	O
compares	O
the	O
two	O
algorithms	O
on	O
the	O
19-state	B
random	O
walk	O
task	O
.	O
figure	O
12.8	O
:	O
19-state	B
random	O
walk	O
results	O
(	O
example	O
7.1	O
)	O
:	O
performance	O
of	O
online	O
and	O
oﬀ-line	O
λ-return	B
algorithms	O
.	O
the	O
performance	O
measure	O
here	O
is	O
the	O
ve	O
at	O
the	O
end	O
of	O
the	O
episode	O
,	O
which	O
should	O
be	O
the	O
best	O
case	O
for	O
the	O
oﬀ-line	B
algorithm	O
.	O
nevertheless	O
,	O
the	O
online	B
algorithm	O
performs	O
subtly	O
better	O
.	O
for	O
comparison	O
,	O
the	O
λ	O
=	O
0	O
line	O
is	O
the	O
same	O
for	O
both	O
methods	O
.	O
12.5	O
true	B
online	I
td	O
(	O
λ	O
)	O
the	O
online	B
λ-return	O
algorithm	O
just	O
presented	O
is	O
currently	O
the	O
best	O
performing	O
temporal-	O
diﬀerence	O
algorithm	O
.	O
it	O
is	O
an	O
ideal	O
which	O
online	B
td	O
(	O
λ	O
)	O
only	O
approximates	O
.	O
as	O
presented	O
,	O
however	O
,	O
the	O
online	B
λ-return	O
algorithm	O
is	O
very	O
complex	O
.	O
is	O
there	O
a	O
way	O
to	O
invert	O
this	O
forward-view	O
algorithm	O
to	O
produce	O
an	O
eﬃcient	O
backward-view	O
algorithm	O
using	O
eligibility	B
traces	I
?	O
it	O
turns	O
out	O
that	O
there	O
is	O
indeed	O
an	O
exact	O
computationally	O
congenial	O
implemen-	O
tation	O
of	O
the	O
online	B
λ-return	O
algorithm	O
for	O
the	O
case	O
of	O
linear	O
function	B
approximation	I
.	O
this	O
implementation	O
is	O
known	O
as	O
the	O
true	B
online	I
td	O
(	O
λ	O
)	O
algorithm	O
because	O
it	O
is	O
“	O
truer	O
”	O
to	O
the	O
ideal	O
of	O
the	O
online	B
λ-return	O
algorithm	O
than	O
the	O
td	O
(	O
λ	O
)	O
algorithm	O
is	O
.	O
the	O
derivation	O
of	O
true	O
online	B
td	O
(	O
λ	O
)	O
is	O
a	O
little	O
too	O
complex	O
to	O
present	O
here	O
(	O
see	O
the	O
next	O
section	O
and	O
the	O
appendix	O
to	O
the	O
paper	O
by	O
van	O
seijen	O
et	O
al.	O
,	O
2016	O
)	O
but	O
its	O
strategy	O
is	O
simple	O
.	O
the	O
sequence	O
of	O
weight	O
vectors	O
produced	O
by	O
the	O
online	B
λ-return	O
algorithm	O
can	O
be	O
arranged	O
in	O
a	O
triangle	O
:	O
w0	O
0	O
0	O
w1	O
w1	O
1	O
1	O
w2	O
0	O
w2	O
w2	O
2	O
0	O
w3	O
w3	O
1	O
w3	O
2	O
w3	O
3	O
...	O
...	O
0	O
wt	O
wt	O
1	O
wt	O
2	O
wt	O
3	O
...	O
...	O
.	O
.	O
.	O
···	O
wt	O
t	O
off-line	O
λ-return	B
algorithm	O
(	O
from	O
section	O
12.1	O
)	O
↵↵rms	O
errorat	O
the	O
end	O
of	O
the	O
episodeover	O
the	O
ﬁrst10	O
episodes0.40.200.80.61λ=0λ=.4λ=.8λ=.9λ=.95λ=.975λ=.99λ=1λ=.950.550.50.450.350.30.250.40.40.200.80.61on-line	O
λ-return	B
algorithm=	O
true	B
online	I
td	O
(	O
λ	O
)	O
λ=0λ=.4λ=.8λ=.9λ=.95λ=.975λ=.99λ=1λ=.95	O
302	O
chapter	O
12	O
:	O
eligibility	B
traces	I
t	O
,	O
are	O
really	O
needed	O
.	O
the	O
ﬁrst	O
,	O
w0	O
t	O
,	O
is	O
the	O
output	O
,	O
and	O
each	O
weight	O
vector	B
along	O
the	O
way	O
,	O
wt	O
it	O
turns	O
out	O
that	O
only	O
the	O
one	O
row	O
of	O
this	O
triangle	O
is	O
produced	O
on	O
each	O
time	O
step	O
.	O
weight	O
vectors	O
on	O
the	O
diagonal	O
,	O
the	O
wt	O
0	O
,	O
is	O
the	O
input	O
,	O
the	O
last	O
,	O
wt	O
t	O
,	O
plays	O
a	O
role	O
in	O
bootstrapping	O
in	O
the	O
n-step	O
returns	O
of	O
the	O
updates	O
.	O
in	O
the	O
ﬁnal	O
algorithm	O
the	O
diagonal	O
weight	O
vectors	O
are	O
renamed	O
without	O
a	O
superscript	O
,	O
wt	O
t.	O
the	O
strategy	O
then	O
is	O
to	O
ﬁnd	O
a	O
compact	O
,	O
eﬃcient	O
way	O
of	O
computing	O
each	O
wt	O
t	O
from	O
the	O
one	O
before	O
.	O
if	O
this	O
is	O
done	O
,	O
for	O
the	O
linear	O
case	O
in	O
which	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
=	O
w	O
(	O
cid:62	O
)	O
x	O
(	O
s	O
)	O
,	O
then	O
we	O
arrive	O
at	O
the	O
true	B
online	I
td	O
(	O
λ	O
)	O
algorithm	O
:	O
.	O
=	O
wt	O
.	O
=	O
x	O
(	O
st	O
)	O
,	O
δt	O
is	O
deﬁned	O
as	O
in	O
td	O
(	O
λ	O
)	O
(	O
12.6	O
)	O
,	O
and	O
zt	O
.	O
wt+1	O
where	O
we	O
have	O
used	O
the	O
shorthand	O
xt	O
is	O
deﬁned	O
by	O
=	O
wt	O
+	O
αδt	O
zt	O
+	O
α	O
(	O
cid:0	O
)	O
w	O
(	O
cid:62	O
)	O
t	O
xt	O
−	O
w	O
(	O
cid:62	O
)	O
t−1xt	O
(	O
cid:1	O
)	O
(	O
zt	O
−	O
xt	O
)	O
,	O
=	O
γλzt−1	O
+	O
(	O
cid:0	O
)	O
1	O
−	O
αγλz	O
(	O
cid:62	O
)	O
t−1xt	O
(	O
cid:1	O
)	O
xt	O
.	O
.	O
zt	O
(	O
12.11	O
)	O
this	O
algorithm	O
has	O
been	O
proven	O
to	O
produce	O
exactly	O
the	O
same	O
sequence	O
of	O
weight	O
vectors	O
,	O
wt	O
,	O
0	O
≤	O
t	O
≤	O
t	O
,	O
as	O
the	O
online	B
λ-return	O
algorithm	O
(	O
van	O
seijen	O
et	O
al	O
.	O
2016	O
)	O
.	O
thus	O
the	O
results	O
on	O
the	O
random	B
walk	I
task	O
on	O
the	O
left	O
of	O
figure	O
12.8	O
are	O
also	O
its	O
results	O
on	O
that	O
task	O
.	O
now	O
,	O
however	O
,	O
the	O
algorithm	O
is	O
much	O
less	O
expensive	O
.	O
the	O
memory	O
requirements	O
of	O
true	O
online	B
td	O
(	O
λ	O
)	O
are	O
identical	O
to	O
those	O
of	O
conventional	O
td	O
(	O
λ	O
)	O
,	O
while	O
the	O
per-step	O
computation	O
is	O
increased	O
by	O
about	O
50	O
%	O
(	O
there	O
is	O
one	O
more	O
inner	O
product	O
in	O
the	O
eligibility-trace	O
update	O
)	O
.	O
overall	O
,	O
the	O
per-step	O
computational	O
complexity	O
remains	O
of	O
o	O
(	O
d	O
)	O
,	O
the	O
same	O
as	O
td	O
(	O
λ	O
)	O
.	O
pseudocode	O
for	O
the	O
complete	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
.	O
true	B
online	I
td	O
(	O
λ	O
)	O
for	O
estimating	O
w	O
(	O
cid:62	O
)	O
x	O
≈	O
vπ	O
input	O
:	O
the	O
policy	B
π	O
to	O
be	O
evaluated	O
input	O
:	O
a	O
feature	O
function	O
x	O
:	O
s+	O
→	O
rd	O
such	O
that	O
x	O
(	O
terminal	O
,	O
·	O
)	O
=	O
0	O
algorithm	O
parameters	O
:	O
step	O
size	O
α	O
>	O
0	O
,	O
trace	O
decay	O
rate	O
λ	O
∈	O
[	O
0	O
,	O
1	O
]	O
initialize	O
value-function	O
weights	O
w	O
∈	O
rd	O
(	O
e.g.	O
,	O
w	O
=	O
0	O
)	O
loop	O
for	O
each	O
episode	O
:	O
(	O
a	O
d-dimensional	O
vector	B
)	O
(	O
a	O
temporary	O
scalar	O
variable	O
)	O
initialize	O
state	B
and	O
obtain	O
initial	O
feature	O
vector	O
x	O
z	O
←	O
0	O
vold	O
←	O
0	O
loop	O
for	O
each	O
step	O
of	O
episode	O
:	O
|	O
choose	O
a	O
∼	O
π	O
|	O
take	O
action	B
a	O
,	O
observe	O
r	O
,	O
x	O
(	O
cid:48	O
)	O
(	O
feature	O
vector	O
of	O
the	O
next	O
state	B
)	O
|	O
v	O
←	O
w	O
(	O
cid:62	O
)	O
x	O
|	O
v	O
(	O
cid:48	O
)	O
←	O
w	O
(	O
cid:62	O
)	O
x	O
(	O
cid:48	O
)	O
δ	O
←	O
r	O
+	O
γv	O
(	O
cid:48	O
)	O
−	O
v	O
|	O
z	O
←	O
γλz	O
+	O
(	O
cid:0	O
)	O
1	O
−	O
αγλz	O
(	O
cid:62	O
)	O
x	O
(	O
cid:1	O
)	O
x	O
|	O
|	O
w	O
←	O
w	O
+	O
α	O
(	O
δ	O
+	O
v	O
−	O
vold	O
)	O
z	O
−	O
α	O
(	O
v	O
−	O
vold	O
)	O
x	O
|	O
vold	O
←	O
v	O
(	O
cid:48	O
)	O
|	O
x	O
←	O
x	O
(	O
cid:48	O
)	O
until	O
x	O
(	O
cid:48	O
)	O
=	O
0	O
(	O
signaling	O
arrival	O
at	O
a	O
terminal	O
state	B
)	O
12.6.	O
dutch	B
traces	O
in	O
monte	O
carlo	O
learning	O
303	O
the	O
eligibility	O
trace	O
(	O
12.11	O
)	O
used	O
in	O
true	O
online	B
td	O
(	O
λ	O
)	O
is	O
called	O
a	O
dutch	B
trace	O
to	O
dis-	O
tinguish	O
it	O
from	O
the	O
trace	O
(	O
12.5	O
)	O
used	O
in	O
td	O
(	O
λ	O
)	O
,	O
which	O
is	O
called	O
an	O
accumulating	B
trace	O
.	O
earlier	O
work	O
often	O
used	O
a	O
third	O
kind	O
of	O
trace	O
called	O
the	O
replacing	B
trace	O
,	O
deﬁned	O
only	O
for	O
the	O
tabular	O
case	O
or	O
for	O
binary	O
feature	O
vectors	O
such	O
as	O
those	O
produced	O
by	O
tile	B
coding	I
.	O
the	O
replacing	B
trace	O
is	O
deﬁned	O
on	O
a	O
component-by-component	O
basis	O
depending	O
on	O
whether	O
the	O
component	O
of	O
the	O
feature	O
vector	O
was	O
1	O
or	O
0	O
:	O
zi	O
,	O
t	O
.	O
=	O
(	O
cid:26	O
)	O
1	O
γλzi	O
,	O
t−1	O
if	O
xi	O
,	O
t	O
=	O
1	O
otherwise	O
.	O
(	O
12.12	O
)	O
nowadays	O
,	O
use	O
of	O
the	O
replacing	B
trace	O
is	O
deprecated	B
;	O
a	O
dutch	B
trace	O
should	O
almost	O
always	O
be	O
used	O
instead	O
.	O
12.6	O
dutch	B
traces	O
in	O
monte	O
carlo	O
learning	O
although	O
eligibility	B
traces	I
are	O
closely	O
associated	O
historically	O
with	O
td	O
learning	O
,	O
in	O
fact	O
they	O
have	O
nothing	O
to	O
do	O
with	O
it	O
.	O
in	O
fact	O
,	O
eligibility	B
traces	I
arise	O
even	O
in	O
monte	O
carlo	O
learning	O
,	O
as	O
we	O
show	O
in	O
this	O
section	O
.	O
we	O
show	O
that	O
the	O
linear	O
mc	O
algorithm	O
(	O
chapter	O
9	O
)	O
,	O
taken	O
as	O
a	O
forward	O
view	O
,	O
can	O
be	O
used	O
to	O
derive	O
an	O
equivalent	O
yet	O
computationally	O
cheaper	O
backward-view	O
algorithm	O
using	O
dutch	B
traces	O
.	O
this	O
is	O
the	O
only	O
equivalence	O
of	O
forward-	O
and	O
backward-views	O
that	O
we	O
explicitly	O
demonstrate	O
in	O
this	O
book	O
.	O
it	O
gives	O
some	O
of	O
the	O
ﬂavor	O
of	O
the	O
proof	B
of	O
equivalence	O
of	O
true	O
online	B
td	O
(	O
λ	O
)	O
and	O
the	O
online	O
λ-return	B
algorithm	O
,	O
but	O
is	O
much	O
simpler	O
.	O
the	O
linear	O
version	O
of	O
the	O
gradient	B
monte	O
carlo	O
prediction	B
algorithm	O
(	O
page	O
202	O
)	O
makes	O
the	O
following	O
sequence	O
of	O
updates	O
,	O
one	O
for	O
each	O
time	O
step	O
of	O
the	O
episode	O
:	O
wt+1	O
.	O
=	O
wt	O
+	O
α	O
(	O
cid:2	O
)	O
g	O
−	O
w	O
(	O
cid:62	O
)	O
t	O
xt	O
(	O
cid:3	O
)	O
xt	O
,	O
0	O
≤	O
t	O
<	O
t.	O
(	O
12.13	O
)	O
to	O
make	O
the	O
example	O
simpler	O
,	O
we	O
assume	O
here	O
that	O
the	O
return	B
g	O
is	O
a	O
single	O
reward	O
received	O
at	O
the	O
end	O
of	O
the	O
episode	O
(	O
this	O
is	O
why	O
g	O
is	O
not	O
subscripted	O
by	O
time	O
)	O
and	O
that	O
there	O
is	O
no	O
discounting	B
.	O
in	O
this	O
case	O
the	O
update	O
is	O
also	O
known	O
as	O
the	O
least	O
mean	O
square	O
(	O
lms	O
)	O
rule	O
.	O
as	O
a	O
monte	O
carlo	O
algorithm	O
,	O
all	O
the	O
updates	O
depend	O
on	O
the	O
ﬁnal	O
reward/return	O
,	O
so	O
none	O
can	O
be	O
made	O
until	O
the	O
end	O
of	O
the	O
episode	O
.	O
the	O
mc	O
algorithm	O
is	O
an	O
oﬀ-line	B
algorithm	O
and	O
we	O
do	O
not	O
seek	O
to	O
improve	O
this	O
aspect	O
of	O
it	O
.	O
rather	O
we	O
seek	O
merely	O
an	O
implementation	O
of	O
this	O
algorithm	O
with	O
computational	O
advantages	O
.	O
we	O
will	O
still	O
update	O
the	O
weight	O
vector	B
only	O
at	O
the	O
end	O
of	O
the	O
episode	O
,	O
but	O
we	O
will	O
do	O
some	O
computation	O
during	O
each	O
step	O
of	O
the	O
episode	O
and	O
less	O
at	O
its	O
end	O
.	O
this	O
will	O
give	O
a	O
more	O
equal	O
distribution	O
of	O
computation—o	O
(	O
d	O
)	O
per	O
step—and	O
also	O
remove	O
the	O
need	O
to	O
store	O
the	O
feature	O
vectors	O
at	O
each	O
step	O
for	O
use	O
later	O
at	O
the	O
end	O
of	O
each	O
episode	O
.	O
instead	O
,	O
we	O
will	O
introduce	O
an	O
additional	O
vector	B
memory	O
,	O
the	O
eligibility	O
trace	O
,	O
keeping	O
in	O
it	O
a	O
summary	O
of	O
all	O
the	O
feature	O
vectors	O
seen	O
so	O
far	O
.	O
this	O
will	O
be	O
suﬃcient	O
to	O
eﬃciently	O
recreate	O
exactly	O
the	O
same	O
overall	O
update	O
304	O
chapter	O
12	O
:	O
eligibility	B
traces	I
as	O
the	O
sequence	O
of	O
mc	O
updates	O
(	O
12.13	O
)	O
,	O
by	O
the	O
end	O
of	O
the	O
episode	O
:	O
wt	O
=	O
wt−1	O
+	O
α	O
(	O
cid:0	O
)	O
g	O
−	O
w	O
(	O
cid:62	O
)	O
t−1xt−1	O
(	O
cid:1	O
)	O
xt−1	O
=	O
wt−1	O
+	O
αxt−1	O
(	O
cid:0	O
)	O
−x	O
(	O
cid:62	O
)	O
t−1wt−1	O
(	O
cid:1	O
)	O
+	O
αgxt−1	O
=	O
(	O
cid:0	O
)	O
i	O
−	O
αxt−1x	O
(	O
cid:62	O
)	O
t−1	O
(	O
cid:1	O
)	O
wt−1	O
+	O
αgxt−1	O
.	O
=	O
i	O
−	O
αxtx	O
(	O
cid:62	O
)	O
t	O
=	O
ft−1wt−1	O
+	O
αgxt−1	O
where	O
ft	O
is	O
a	O
forgetting	O
,	O
or	O
fading	O
,	O
matrix	O
.	O
now	O
,	O
recursing	O
,	O
=	O
ft−1	O
(	O
ft−2wt−2	O
+	O
αgxt−2	O
)	O
+	O
αgxt−1	O
=	O
ft−1ft−2wt−2	O
+	O
αg	O
(	O
ft−1xt−2	O
+	O
xt−1	O
)	O
=	O
ft−1ft−2	O
(	O
ft−3wt−3	O
+	O
αgxt−3	O
)	O
+	O
αg	O
(	O
ft−1xt−2	O
+	O
xt−1	O
)	O
=	O
ft−1ft−2ft−3wt−3	O
+	O
αg	O
(	O
ft−1ft−2xt−3	O
+	O
ft−1xt−2	O
+	O
xt−1	O
)	O
...	O
=	O
ft−1ft−2	O
···	O
f0w0	O
ft−1ft−2	O
···	O
fk+1xk	O
(	O
cid:124	O
)	O
at−1	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
=	O
at−1	O
+	O
αgzt−1	O
,	O
+	O
αg	O
t−1	O
(	O
cid:88	O
)	O
k=0	O
(	O
cid:124	O
)	O
(	O
cid:125	O
)	O
zt−1	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
(	O
12.14	O
)	O
where	O
at−1	O
and	O
zt−1	O
are	O
the	O
values	O
at	O
time	O
t	O
−	O
1	O
of	O
two	O
auxilary	O
memory	O
vectors	O
that	O
can	O
be	O
updated	O
incrementally	O
without	O
knowledge	O
of	O
g	O
and	O
with	O
o	O
(	O
d	O
)	O
complexity	O
per	O
time	O
step	O
.	O
the	O
zt	O
vector	B
is	O
in	O
fact	O
a	O
dutch-style	O
eligibility	O
trace	O
.	O
it	O
is	O
initialized	O
to	O
z0	O
=	O
x0	O
and	O
then	O
updated	O
according	O
to	O
ftft−1	O
···	O
fk+1xk	O
,	O
1	O
≤	O
t	O
<	O
t	O
which	O
is	O
the	O
dutch	B
trace	O
for	O
the	O
case	O
of	O
γλ	O
=	O
1	O
(	O
cf	O
.	O
eq	O
.	O
12.11	O
)	O
.	O
the	O
at	O
auxilary	O
vector	B
is	O
initialized	O
to	O
a0	O
=	O
w0	O
and	O
then	O
updated	O
according	O
to	O
at	O
.	O
=	O
ftft−1	O
···	O
f0w0	O
=	O
ftat−1	O
=	O
at−1	O
−	O
αxtx	O
(	O
cid:62	O
)	O
t	O
at−1	O
,	O
1	O
≤	O
t	O
<	O
t.	O
.	O
=	O
zt	O
=	O
t	O
(	O
cid:88	O
)	O
k=0	O
t−1	O
(	O
cid:88	O
)	O
k=0	O
t−1	O
(	O
cid:88	O
)	O
k=0	O
ftft−1	O
···	O
fk+1xk	O
+	O
xt	O
=	O
ft	O
ft−1ft−2	O
···	O
fk+1xk	O
+	O
xt	O
=	O
ftzt−1	O
+	O
xt	O
=	O
(	O
cid:0	O
)	O
i	O
−	O
αxtx	O
(	O
cid:62	O
)	O
t	O
(	O
cid:1	O
)	O
zt−1	O
+	O
xt	O
=	O
zt−1	O
−	O
αxtx	O
(	O
cid:62	O
)	O
t	O
zt−1	O
+	O
xt	O
=	O
zt−1	O
−	O
α	O
(	O
cid:0	O
)	O
z	O
(	O
cid:62	O
)	O
t−1xt	O
(	O
cid:1	O
)	O
xt	O
+	O
xt	O
=	O
zt−1	O
+	O
(	O
cid:0	O
)	O
1	O
−	O
αz	O
(	O
cid:62	O
)	O
t−1xt	O
(	O
cid:1	O
)	O
xt	O
,	O
12.7.	O
sarsa	O
(	O
λ	O
)	O
305	O
the	O
auxiliary	O
vectors	O
,	O
at	O
and	O
zt	O
,	O
are	O
updated	O
on	O
each	O
time	O
step	O
t	O
<	O
t	O
and	O
then	O
,	O
at	O
time	O
t	O
when	O
g	O
is	O
observed	O
,	O
they	O
are	O
used	O
in	O
(	O
12.14	O
)	O
to	O
compute	O
wt	O
.	O
in	O
this	O
way	O
we	O
achieve	O
exactly	O
the	O
same	O
ﬁnal	O
result	O
as	O
the	O
mc/lms	O
algorithm	O
that	O
has	O
poor	O
computational	O
properties	O
(	O
12.13	O
)	O
,	O
but	O
now	O
with	O
an	O
incremental	O
algorithm	O
whose	O
time	O
and	O
memory	O
complexity	O
per	O
step	O
is	O
o	O
(	O
d	O
)	O
.	O
this	O
is	O
surprising	O
and	O
intriguing	O
because	O
the	O
notion	O
of	O
an	O
eligibility	O
trace	O
(	O
and	O
the	O
dutch	O
trace	O
in	O
particular	O
)	O
has	O
arisen	O
in	O
a	O
setting	O
without	O
temporal-diﬀerence	O
(	O
td	O
)	O
learning	O
(	O
in	O
contrast	O
to	O
van	O
seijen	O
and	O
sutton	O
,	O
2014	O
)	O
.	O
it	O
seems	O
eligibility	B
traces	I
are	O
not	O
speciﬁc	O
to	O
td	O
learning	O
at	O
all	O
;	O
they	O
are	O
more	O
fundamental	O
than	O
that	O
.	O
the	O
need	O
for	O
eligibility	O
traces	O
seems	O
to	O
arise	O
whenever	O
one	O
tries	O
to	O
learn	O
long-term	O
predictions	O
in	O
an	O
eﬃcient	O
manner	O
.	O
12.7	O
sarsa	O
(	O
λ	O
)	O
very	O
few	O
changes	O
in	O
the	O
ideas	O
already	O
presented	O
in	O
this	O
chapter	O
are	O
required	O
in	O
order	O
to	O
extend	O
eligibility-traces	O
to	O
action-value	B
methods	I
.	O
to	O
learn	O
approximate	O
action	O
values	O
,	O
ˆq	O
(	O
s	O
,	O
a	O
,	O
w	O
)	O
,	O
rather	O
than	O
approximate	O
state	O
values	O
,	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
,	O
we	O
need	O
to	O
use	O
the	O
action-value	O
form	O
of	O
the	O
n-step	B
return	O
,	O
from	O
chapter	O
10	O
:	O
gt	O
:	O
t+n	O
.	O
=	O
rt+1	O
+	O
···	O
+	O
γn−1rt+n	O
+	O
γn	O
ˆq	O
(	O
st+n	O
,	O
at+n	O
,	O
wt+n−1	O
)	O
,	O
.	O
=	O
gt	O
if	O
t	O
+	O
n	O
≥	O
t	O
.	O
using	O
this	O
,	O
we	O
can	O
form	O
the	O
action-value	O
form	O
of	O
the	O
with	O
gt	O
:	O
t+n	O
truncated	B
λ-return	I
,	O
which	O
is	O
otherwise	O
identical	O
to	O
the	O
state-value	O
form	O
(	O
12.9	O
)	O
.	O
the	O
action-value	O
form	O
of	O
the	O
oﬀ-line	B
λ-return	O
algorithm	O
(	O
12.4	O
)	O
simply	O
uses	O
ˆq	O
rather	O
than	O
ˆv	O
:	O
t	O
+	O
n	O
<	O
t	O
,	O
.	O
wt+1	O
=	O
wt	O
+	O
α	O
(	O
cid:104	O
)	O
gλ	O
t	O
−	O
ˆq	O
(	O
st	O
,	O
atwt	O
)	O
(	O
cid:105	O
)	O
∇ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
,	O
t	O
=	O
0	O
,	O
.	O
.	O
.	O
,	O
t	O
−	O
1	O
,	O
(	O
12.15	O
)	O
.	O
=	O
gλ	O
where	O
gλ	O
t	O
:	O
∞	O
.	O
the	O
compound	B
backup	O
diagram	O
for	O
this	O
forward	O
view	O
is	O
shown	O
in	O
t	O
figure	O
12.9.	O
notice	O
the	O
similarity	O
to	O
the	O
diagram	O
of	O
the	O
td	O
(	O
λ	O
)	O
algorithm	O
(	O
figure	O
12.1	O
)	O
.	O
the	O
ﬁrst	O
update	O
looks	O
ahead	O
one	O
full	O
step	O
,	O
to	O
the	O
next	O
state–action	O
pair	O
,	O
the	O
second	O
looks	O
ahead	O
two	O
steps	O
,	O
to	O
the	O
second	O
state–action	O
pair	O
,	O
and	O
so	O
on	O
.	O
a	O
ﬁnal	O
update	O
is	O
based	O
on	O
the	O
complete	O
return	B
.	O
the	O
weighting	O
of	O
each	O
n-step	B
update	O
in	O
the	O
λ-return	O
is	O
just	O
as	O
in	O
td	O
(	O
λ	O
)	O
and	O
the	O
λ-return	O
algorithm	O
(	O
12.3	O
)	O
.	O
the	O
temporal-diﬀerence	O
method	O
for	B
action	I
values	I
,	O
known	O
as	O
sarsa	O
(	O
λ	O
)	O
,	O
approximates	O
this	O
forward	O
view	O
.	O
it	O
has	O
the	O
same	O
update	O
rule	O
as	O
given	O
earlier	O
for	O
td	O
(	O
λ	O
)	O
:	O
wt+1	O
.	O
=	O
wt	O
+	O
αδt	O
zt	O
,	O
except	O
,	O
naturally	O
,	O
using	O
the	O
action-value	O
form	O
of	O
the	O
td	O
error	O
:	O
δt	O
.	O
=	O
rt+1	O
+	O
γ	O
ˆq	O
(	O
st+1	O
,	O
at+1	O
,	O
wt	O
)	O
−	O
ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
,	O
and	O
the	O
action-value	O
form	O
of	O
the	O
eligibility	O
trace	O
:	O
.	O
=	O
0	O
,	O
z−1	O
.	O
=	O
γλzt−1	O
+	O
∇ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
,	O
zt	O
0	O
≤	O
t	O
≤	O
t	O
(	O
12.16	O
)	O
306	O
chapter	O
12	O
:	O
eligibility	B
traces	I
figure	O
12.9	O
:	O
sarsa	O
(	O
λ	O
)	O
’	O
s	O
backup	B
diagram	I
.	O
compare	O
with	O
figure	O
12.1	O
.	O
(	O
or	O
,	O
alternatively	O
,	O
the	O
replacing	B
trace	O
given	O
by	O
(	O
12.12	O
)	O
)	O
.	O
complete	O
pseudocode	O
for	O
sarsa	O
(	O
λ	O
)	O
with	O
linear	O
function	B
approximation	I
,	O
binary	B
features	I
,	O
and	O
either	O
accumulating	B
or	O
replacing	B
traces	O
is	O
given	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
.	O
this	O
pseudocode	O
highlights	O
a	O
few	O
optimiza-	O
tions	O
possible	O
in	O
the	O
special	O
case	O
of	O
binary	O
features	O
(	O
features	O
are	O
either	O
active	O
(	O
=1	O
)	O
or	O
inactive	O
(	O
=0	O
)	O
.	O
example	O
12.1	O
:	O
traces	O
in	O
gridworld	O
the	O
use	O
of	O
eligibility	O
traces	O
can	O
substantially	O
increase	O
the	O
eﬃciency	O
of	O
control	O
algorithms	O
over	O
one-step	O
methods	O
and	O
even	O
over	O
n-step	B
methods	I
.	O
the	O
reason	O
for	O
this	O
is	O
illustrated	O
by	O
the	O
gridworld	O
example	O
below	O
.	O
the	O
ﬁrst	O
panel	O
shows	O
the	O
path	O
taken	O
by	O
an	O
agent	O
in	O
a	O
single	O
episode	O
.	O
the	O
initial	O
estimated	O
values	O
were	O
zero	O
,	O
and	O
all	O
rewards	O
were	O
zero	O
except	O
for	O
a	O
positive	O
reward	O
at	O
the	O
goal	B
location	O
marked	O
by	O
g.	O
the	O
arrows	O
in	O
the	O
other	O
panels	O
show	O
,	O
for	O
various	O
algorithms	O
,	O
which	O
action-values	O
would	O
be	O
increased	O
,	O
and	O
by	O
how	O
much	O
,	O
upon	O
reaching	O
the	O
goal	B
.	O
a	O
one-step	O
method	O
would	O
increment	O
only	O
the	O
last	O
action	B
value	O
,	O
whereas	O
an	O
n-step	B
method	O
would	O
equally	O
increment	O
the	O
last	O
n	O
actions	O
’	O
values	O
,	O
and	O
an	O
eligibility	O
trace	O
method	O
would	O
update	O
all	O
the	O
action	B
values	O
up	O
to	O
the	O
beginning	O
of	O
the	O
episode	O
,	O
to	O
diﬀerent	O
degrees	O
,	O
fading	O
with	O
recency	O
.	O
the	O
fading	O
strategy	O
is	O
often	O
the	O
best	O
.	O
1  	O
(	O
1  	O
)	O
 	O
(	O
1  	O
)	O
 2 t t 1······statat+1at 1st+1rt+1strt···st+2rt+2at+2x=1sarsa	O
(	O
 	O
)	O
path	O
takenaction	O
values	O
increasedby	O
one-step	O
sarsaaction	O
values	O
increasedby	O
sarsa	O
(	O
)	O
with	O
=0.9gggpath	O
takenaction	O
values	O
increasedby	O
one-step	O
sarsaaction	O
values	O
increasedby	O
sarsa	O
(	O
!	O
)	O
with	O
!	O
=0.9by	O
10-step	O
sarsagggpath	O
takenaction	O
values	O
increasedby	O
one-step	O
sarsaaction	O
values	O
increased	O
by	O
10-step	O
sarsagggλλ	O
12.7.	O
sarsa	O
(	O
λ	O
)	O
307	O
sarsa	O
(	O
λ	O
)	O
with	O
binary	O
features	O
and	O
linear	O
function	B
approximation	I
for	O
estimating	O
w	O
(	O
cid:62	O
)	O
x	O
≈	O
qπ	O
or	O
q∗	O
input	O
:	O
a	O
function	O
f	O
(	O
s	O
,	O
a	O
)	O
returning	O
the	O
set	O
of	O
(	O
indices	O
of	O
)	O
active	O
features	O
for	O
s	O
,	O
a	O
input	O
:	O
a	O
policy	B
π	O
(	O
if	O
estimating	O
qπ	O
)	O
algorithm	O
parameters	O
:	O
step	O
size	O
α	O
>	O
0	O
,	O
trace	O
decay	O
rate	O
λ	O
∈	O
[	O
0	O
,	O
1	O
]	O
initialize	O
:	O
w	O
=	O
(	O
w1	O
,	O
.	O
.	O
.	O
,	O
wd	O
)	O
(	O
cid:62	O
)	O
∈	O
rd	O
(	O
e.g.	O
,	O
w	O
=	O
0	O
)	O
,	O
z	O
=	O
(	O
z1	O
,	O
.	O
.	O
.	O
,	O
zd	O
)	O
(	O
cid:62	O
)	O
∈	O
rd	O
loop	O
for	O
each	O
episode	O
:	O
initialize	O
s	O
choose	O
a	O
∼	O
π	O
(	O
·|s	O
)	O
or	O
ε-greedy	O
according	O
to	O
ˆq	O
(	O
s	O
,	O
·	O
,	O
w	O
)	O
z	O
←	O
0	O
loop	O
for	O
each	O
step	O
of	O
episode	O
:	O
take	O
action	B
a	O
,	O
observe	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
δ	O
←	O
r	O
loop	O
for	O
i	O
in	O
f	O
(	O
s	O
,	O
a	O
)	O
:	O
δ	O
←	O
δ	O
−	O
wi	O
zi	O
←	O
zi	O
+	O
1	O
or	O
zi	O
←	O
1	O
w	O
←	O
w	O
+	O
αδ	O
z	O
go	O
to	O
next	O
episode	O
if	O
s	O
(	O
cid:48	O
)	O
is	O
terminal	O
then	O
:	O
choose	O
a	O
(	O
cid:48	O
)	O
∼	O
π	O
(	O
·|s	O
(	O
cid:48	O
)	O
)	O
or	O
near	O
greedily	O
∼	O
ˆq	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
·	O
,	O
w	O
)	O
loop	O
for	O
i	O
in	O
f	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
(	O
cid:48	O
)	O
)	O
:	O
δ	O
←	O
δ	O
+	O
γwi	O
w	O
←	O
w	O
+	O
αδ	O
z	O
z	O
←	O
γλz	O
s	O
←	O
s	O
(	O
cid:48	O
)	O
;	O
a	O
←	O
a	O
(	O
cid:48	O
)	O
(	O
accumulating	B
traces	O
)	O
(	O
replacing	B
traces	O
)	O
exercise	O
12.6	O
modify	O
the	O
pseudocode	O
for	O
sarsa	O
(	O
λ	O
)	O
to	O
use	O
dutch	B
traces	O
(	O
12.11	O
)	O
without	O
the	O
other	O
features	O
of	O
a	O
true	B
online	I
algorithm	O
.	O
assume	O
linear	B
function	I
approximation	I
and	O
(	O
cid:3	O
)	O
binary	B
features	I
.	O
example	O
12.2	O
:	O
sarsa	O
(	O
λ	O
)	O
on	O
mountain	O
car	O
figure	O
12.10	O
(	O
left	O
)	O
shows	O
results	O
with	O
sarsa	O
(	O
λ	O
)	O
on	O
the	O
mountain	O
car	O
task	O
introduced	O
in	O
example	O
10.1.	O
the	O
function	O
approx-	O
imation	O
,	O
action	B
selection	O
,	O
and	O
environmental	O
details	O
were	O
exactly	O
as	O
in	O
chapter	O
10	O
,	O
and	O
thus	O
it	O
is	O
appropriate	O
to	O
numerically	O
compare	O
these	O
results	O
with	O
the	O
chapter	O
10	O
results	O
for	O
n-step	O
sarsa	O
(	O
right	O
side	O
of	O
the	O
ﬁgure	O
)	O
.	O
the	O
earlier	O
results	O
varied	O
the	O
update	O
length	O
n	O
whereas	O
here	O
for	O
sarsa	O
(	O
λ	O
)	O
we	O
vary	O
the	O
trace	O
parameter	O
λ	O
,	O
which	O
plays	O
a	O
similar	O
role	O
.	O
the	O
fading-trace	O
bootstrapping	B
strategy	O
of	O
sarsa	O
(	O
λ	O
)	O
appears	O
to	O
result	O
in	O
more	O
eﬃcient	O
learning	O
on	O
this	O
problem	O
.	O
there	O
is	O
also	O
an	O
action-value	O
version	O
of	O
our	O
ideal	O
td	O
method	O
,	O
the	O
online	B
λ-return	O
algorithm	O
presented	O
in	O
section	O
12.4.	O
everything	O
in	O
that	O
section	O
goes	O
through	O
without	O
change	O
other	O
than	O
to	O
use	O
the	O
action-value	O
form	O
of	O
the	O
n-step	B
return	O
given	O
at	O
the	O
beginning	O
of	O
this	O
section	O
.	O
in	O
the	O
case	O
of	O
linear	O
function	B
approximation	I
,	O
the	O
ideal	O
algorithm	O
again	O
has	O
an	O
exact	O
,	O
eﬃcient	O
o	O
(	O
d	O
)	O
implementation	O
,	O
called	O
true	B
online	I
sarsa	O
(	O
λ	O
)	O
.	O
the	O
analyses	O
308	O
chapter	O
12	O
:	O
eligibility	B
traces	I
figure	O
12.10	O
:	O
early	O
performance	O
on	O
the	O
mountain	O
car	O
task	O
of	O
sarsa	O
(	O
λ	O
)	O
with	O
replacing	O
traces	O
and	O
n-step	O
sarsa	O
(	O
copied	O
from	O
figure	O
10.4	O
)	O
as	O
a	O
function	O
of	O
the	O
step	O
size	O
,	O
α.	O
in	O
sections	O
12.5	O
and	O
12.6	O
carry	O
through	O
without	O
change	O
other	O
than	O
to	O
use	O
state–action	O
feature	O
vectors	O
xt	O
=	O
x	O
(	O
st	O
,	O
at	O
)	O
instead	O
of	O
state	O
feature	O
vectors	O
xt	O
=	O
x	O
(	O
st	O
)	O
.	O
the	O
pseu-	O
docode	O
for	O
this	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
.	O
figure	O
12.11	O
compares	O
the	O
performance	O
of	O
various	O
versions	O
of	O
sarsa	O
(	O
λ	O
)	O
on	O
the	O
mountain	B
car	I
example	I
.	O
figure	O
12.11	O
:	O
summary	O
comparison	O
of	O
sarsa	O
(	O
λ	O
)	O
algorithms	O
on	O
the	O
mountain	O
car	O
task	O
.	O
true	B
online	I
sarsa	O
(	O
λ	O
)	O
performed	O
better	O
than	O
regular	O
sarsa	O
(	O
λ	O
)	O
with	O
both	O
accumulating	B
and	O
replacing	B
traces	O
.	O
also	O
included	O
is	O
a	O
version	O
of	O
sarsa	O
(	O
λ	O
)	O
with	O
replacing	O
traces	O
in	O
which	O
,	O
on	O
each	O
time	O
step	O
,	O
the	O
traces	O
for	O
the	O
state	B
and	O
the	O
actions	O
not	O
selected	O
were	O
set	O
to	O
zero	O
.	O
22024026030000.511.5mountain	O
carsteps	O
per	O
episodeaveraged	O
overﬁrst	O
50	O
episodesand	O
100	O
runs280200180×	O
number	O
of	O
tilings	O
(	O
8	O
)	O
↵λ=.96λ=.92λ=.99λ=.84λ=.68λ=022024026030000.511.528000.511.5n=1n=2n=4n=8n=16n=8n=4n=2n=16n=1λ=.98×	O
number	O
of	O
tilings	O
(	O
8	O
)	O
↵sarsa	O
(	O
λ	O
)	O
with	O
replacing	O
tracesn-step	O
sarsaλ=.92λ=.84trueonlinetd	O
(	O
 	O
)	O
00.511.500.20.40.60.81step−sizerms	O
error	O
td	O
(	O
λ	O
)	O
,	O
accumulating	B
traces	O
−	O
task	O
1λ	O
=	O
0λ	O
=	O
0.975λ	O
=	O
1λ	O
=	O
0.95λ	O
=	O
0.2λ	O
=	O
0.100.511.500.20.40.60.81step−sizerms	O
error	O
td	O
(	O
λ	O
)	O
,	O
replacing	B
traces	O
−	O
task	O
1λ	O
=	O
0λ	O
=	O
100.511.500.20.40.60.81step−sizerms	O
error	O
true	O
online	B
td	O
(	O
λ	O
)	O
−	O
task	O
1λ	O
=	O
0λ	O
=	O
100.511.500.20.40.60.81step−sizerms	O
error	O
td	O
(	O
λ	O
)	O
,	O
accumulating	B
traces	O
−	O
task	O
2λ	O
=	O
1λ	O
=	O
000.511.500.20.40.60.81step−sizerms	O
error	O
td	O
(	O
λ	O
)	O
,	O
replacing	B
traces	O
−	O
task	O
200.511.500.20.40.60.81step−sizerms	O
error	O
true	O
online	B
td	O
(	O
λ	O
)	O
−	O
task	O
2λ	O
=	O
1λ	O
=	O
0figure2.rmserrorofstatevaluesattheendofeachepisode	O
,	O
averagedovertheﬁrst10episodes	O
,	O
aswellas100independentruns	O
,	O
fordifferentvaluesof↵and .figure4comparestrueonlinesarsa	O
(	O
 	O
)	O
withthetraditionalsarsa	O
(	O
 	O
)	O
implementationonthestandardmountaincartask	O
(	O
sutton	O
&	O
barto,1998	O
)	O
using10tilingsofeach10⇥10tiles.resultsareplottedfor =0.9and↵=↵0/10	O
,	O
for↵0from0.2to2.0withstepsof0.2.clearing/noclearingreferstowhetherthetracevaluesofnon-selectedactionsaresetto0	O
(	O
clearing	O
)	O
ornot	O
(	O
noclearing	O
)	O
,	O
incaseofreplacingtraces.theresultssuggestthatthetrueonlineprincipleisalsoeffectiveinacontrolsetting.6.conclusionwepresentedfortheﬁrsttimeanonlineversionofthefor-wardviewwhichformsthetheoreticalandintuitivefoun-dationforthetd	O
(	O
 	O
)	O
algorithm.inaddition	O
,	O
wehavepre-sentedanewvariantoftd	O
(	O
 	O
)	O
,	O
withthesamecompu-tationalcomplexityastheclassicalalgorithm	O
,	O
whichwecalltrueonlinetd	O
(	O
 	O
)	O
.weprovedthattrueonlinetd	O
(	O
 	O
)	O
matchesthenewonlineforwardviewexactly	O
,	O
incontrasttoclassicalonlinetd	O
(	O
 	O
)	O
,	O
whichonlyapproximatesitsforwardview.inaddition	O
,	O
wedemonstratedempiricallythattrueonlinetd	O
(	O
 	O
)	O
outperformsconventionaltd	O
(	O
 	O
)	O
onthreebenchmarkproblems.itseems	O
,	O
byadheringmoretrulytotheoriginalgoaloftd	O
(	O
 	O
)	O
—matchinganintuitivelyclearforwardviewevenintheonlinecase—thatwehavefoundanewalgorithmthatsimplyimprovesontd	O
(	O
 	O
)	O
.0.20.40.60.811.21.41.61.82−550−500−450−400−350−300−250−200−150α0return	O
sarsa	O
(	O
λ	O
)	O
,	O
replacing	B
,	O
clearingsarsa	O
(	O
λ	O
)	O
,	O
replacing	B
,	O
no	O
clearingsarsa	O
(	O
λ	O
)	O
,	O
accumulatingtrue	O
online	B
sarsa	O
(	O
λ	O
)	O
figure4.averagereturnoverﬁrst20episodesonmountaincartaskfor =0.9anddifferent↵0.resultsareaveragedover100independentruns.acknowledgementstheauthorsthankhadovanhasseltandrupammahmoodforextensivediscussionsleadingtothereﬁnementoftheseideas.thisworkwassupportedbygrantsfromalbertainnovates–technologyfuturesandthenationalscienceandengineeringresearchcouncilofcanada.mountain	O
carreward	O
per	O
episodeaveraged	O
overﬁrst	O
20	O
episodesand	O
100	O
runs×	O
number	O
of	O
tilings	O
(	O
8	O
)	O
↵true	O
online	B
sarsa	O
(	O
λ	O
)	O
sarsa	O
(	O
λ	O
)	O
with	O
replacing	O
tracessarsa	O
(	O
λ	O
)	O
with	O
replacing	O
tracesand	O
clearing	O
the	O
traces	O
of	O
other	O
actionssarsa	O
(	O
λ	O
)	O
with	O
accumulating	O
traces	O
12.8.	O
variable	O
λ	O
and	O
γ	O
309	O
true	B
online	I
sarsa	O
(	O
λ	O
)	O
for	O
estimating	O
w	O
(	O
cid:62	O
)	O
x	O
≈	O
qπ	O
or	O
q∗	O
input	O
:	O
a	O
feature	O
function	O
x	O
:	O
s+	O
×	O
a	O
→	O
rd	O
such	O
that	O
x	O
(	O
terminal	O
,	O
·	O
)	O
=	O
0	O
input	O
:	O
a	O
policy	B
π	O
(	O
if	O
estimating	O
qπ	O
)	O
algorithm	O
parameters	O
:	O
step	O
size	O
α	O
>	O
0	O
,	O
trace	O
decay	O
rate	O
λ	O
∈	O
[	O
0	O
,	O
1	O
]	O
initialize	O
:	O
w	O
∈	O
rd	O
(	O
e.g.	O
,	O
w	O
=	O
0	O
)	O
loop	O
for	O
each	O
episode	O
:	O
initialize	O
s	O
choose	O
a	O
∼	O
π	O
(	O
·|s	O
)	O
or	O
near	O
greedily	O
from	O
s	O
using	O
w	O
x	O
←	O
x	O
(	O
s	O
,	O
a	O
)	O
z	O
←	O
0	O
qold	O
←	O
0	O
loop	O
for	O
each	O
step	O
of	O
episode	O
:	O
|	O
take	O
action	B
a	O
,	O
observe	O
r	O
,	O
s	O
(	O
cid:48	O
)	O
|	O
choose	O
a	O
(	O
cid:48	O
)	O
∼	O
π	O
(	O
·|s	O
(	O
cid:48	O
)	O
)	O
or	O
near	O
greedily	O
from	O
s	O
(	O
cid:48	O
)	O
using	O
w	O
|	O
x	O
(	O
cid:48	O
)	O
←	O
x	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
(	O
cid:48	O
)	O
)	O
|	O
q	O
←	O
w	O
(	O
cid:62	O
)	O
x	O
|	O
q	O
(	O
cid:48	O
)	O
←	O
w	O
(	O
cid:62	O
)	O
x	O
(	O
cid:48	O
)	O
δ	O
←	O
r	O
+	O
γq	O
(	O
cid:48	O
)	O
−	O
q	O
|	O
z	O
←	O
γλz	O
+	O
(	O
cid:0	O
)	O
1	O
−	O
αγλz	O
(	O
cid:62	O
)	O
x	O
(	O
cid:1	O
)	O
x	O
|	O
|	O
w	O
←	O
w	O
+	O
α	O
(	O
δ	O
+	O
q	O
−	O
qold	O
)	O
z	O
−	O
α	O
(	O
q	O
−	O
qold	O
)	O
x	O
|	O
qold	O
←	O
q	O
(	O
cid:48	O
)	O
|	O
x	O
←	O
x	O
(	O
cid:48	O
)	O
|	O
a	O
←	O
a	O
(	O
cid:48	O
)	O
until	O
s	O
(	O
cid:48	O
)	O
is	O
terminal	O
12.8	O
variable	O
λ	O
and	O
γ	O
we	O
are	O
starting	O
now	O
to	O
reach	O
the	O
end	O
of	O
our	O
development	O
of	O
fundamental	O
td	O
learning	O
algorithms	O
.	O
to	O
present	O
the	O
ﬁnal	O
algorithms	O
in	O
their	O
most	O
general	O
forms	O
,	O
it	O
is	O
useful	O
to	O
generalize	O
the	O
degree	O
of	O
bootstrapping	O
and	O
discounting	O
beyond	O
constant	O
parameters	O
to	O
functions	O
potentially	O
dependent	O
on	O
the	O
state	B
and	O
action	B
.	O
that	O
is	O
,	O
each	O
time	O
step	O
will	O
have	O
a	O
diﬀerent	O
λ	O
and	O
γ	O
,	O
denoted	O
λt	O
and	O
γt	O
.	O
we	O
change	O
notation	O
now	O
so	O
that	O
λ	O
:	O
s	O
×	O
a	O
→	O
[	O
0	O
,	O
1	O
]	O
is	O
now	O
a	O
whole	O
function	O
from	O
states	O
and	O
actions	O
to	O
the	O
unit	O
interval	O
.	O
=	O
λ	O
(	O
st	O
,	O
at	O
)	O
,	O
and	O
similarly	O
,	O
γ	O
:	O
s	O
→	O
[	O
0	O
,	O
1	O
]	O
is	O
a	O
function	O
from	O
states	O
to	O
the	O
such	O
that	O
λt	O
unit	O
interval	O
such	O
that	O
γt	O
introducing	O
the	O
function	O
γ	O
,	O
the	O
termination	O
function	O
,	O
is	O
particularly	O
signiﬁcant	O
because	O
it	O
changes	O
the	O
return	B
,	O
the	O
fundamental	O
random	O
variable	O
whose	O
expectation	O
we	O
seek	O
to	O
.	O
=	O
γ	O
(	O
st	O
)	O
.	O
310	O
chapter	O
12	O
:	O
eligibility	B
traces	I
estimate	O
.	O
now	O
the	O
return	B
is	O
deﬁned	O
more	O
generally	O
as	O
gt	O
.	O
=	O
rt+1	O
+	O
γt+1gt+1	O
=	O
rt+1	O
+	O
γt+1rt+2	O
+	O
γt+1γt+2rt+3	O
+	O
γt+1γt+2γt+3rt+4	O
+	O
···	O
∞	O
(	O
cid:88	O
)	O
k=t	O
k	O
(	O
cid:89	O
)	O
i=t+1	O
=	O
rk+1	O
γi	O
,	O
(	O
12.17	O
)	O
where	O
,	O
to	O
assure	O
the	O
sums	O
are	O
ﬁnite	O
,	O
we	O
require	O
that	O
(	O
cid:81	O
)	O
∞k=t	O
γk	O
=	O
0	O
with	O
probability	O
one	O
for	O
all	O
t.	O
one	O
convenient	O
aspect	O
of	O
this	O
deﬁnition	O
is	O
that	O
it	O
allows	O
us	O
to	O
dispense	O
with	O
episodes	O
,	O
start	O
and	O
terminal	O
states	O
,	O
and	O
t	O
as	O
special	O
cases	O
and	O
quantities	O
.	O
a	O
terminal	O
state	B
just	O
becomes	O
a	O
state	B
at	O
which	O
γ	O
(	O
s	O
)	O
=	O
0	O
and	O
which	O
transitions	O
to	O
the	O
start	O
state	B
.	O
in	O
that	O
way	O
(	O
and	O
by	O
choosing	O
γ	O
(	O
·	O
)	O
as	O
a	O
constant	O
function	O
)	O
we	O
can	O
recover	O
the	O
classical	O
episodic	O
setting	O
as	O
a	O
special	O
case	O
.	O
state	B
dependent	I
termination	O
includes	O
other	O
prediction	B
cases	O
such	O
as	O
pseudo	O
termination	O
,	O
in	O
which	O
we	O
seek	O
to	O
predict	O
a	O
quantity	O
that	O
becomes	O
complete	O
but	O
does	O
not	O
alter	O
the	O
ﬂow	O
of	O
the	O
markov	O
process	O
.	O
discounted	O
returns	O
themselves	O
can	O
be	O
thought	O
of	O
as	O
such	O
a	O
quantity	O
,	O
and	O
state	O
dependent	O
termination	O
is	O
a	O
deep	O
uniﬁcation	O
of	O
the	O
episodic	O
and	O
discounted-continuing	O
cases	O
.	O
(	O
the	O
undiscounted-continuing	O
case	O
still	O
needs	O
some	O
special	O
treatment	O
.	O
)	O
the	O
generalization	O
to	O
variable	O
bootstrapping	B
is	O
not	O
a	O
change	O
in	O
the	O
problem	O
,	O
like	O
discounting	B
,	O
but	O
a	O
change	O
in	O
the	O
solution	O
strategy	O
.	O
the	O
generalization	O
aﬀects	O
the	O
λ-	O
returns	O
for	O
states	O
and	O
actions	O
.	O
the	O
new	O
state-based	O
λ-return	B
can	O
be	O
written	O
recursively	O
as	O
(	O
12.18	O
)	O
gλs	O
t	O
.	O
=	O
rt+1	O
+	O
γt+1	O
(	O
cid:0	O
)	O
(	O
1	O
−	O
λt+1	O
)	O
ˆv	O
(	O
st+1	O
,	O
wt	O
)	O
+	O
λt+1gλs	O
t+1	O
(	O
cid:1	O
)	O
,	O
where	O
now	O
we	O
have	O
added	O
the	O
“	O
s	O
”	O
to	O
the	O
superscript	O
λ	O
to	O
remind	O
us	O
that	O
this	O
is	O
a	O
return	B
that	O
bootstraps	O
from	O
state	B
values	O
,	O
distinguishing	O
it	O
from	O
returns	O
that	O
bootstrap	O
from	O
action	B
values	O
,	O
which	O
we	O
present	O
below	O
with	O
“	O
a	O
”	O
in	O
the	O
superscript	O
.	O
this	O
equation	O
says	O
that	O
the	O
λ-return	B
is	O
the	O
ﬁrst	O
reward	O
,	O
undiscounted	O
and	O
unaﬀected	O
by	O
bootstrapping	B
,	O
plus	O
possibly	O
a	O
second	O
term	O
to	O
the	O
extent	O
that	O
we	O
are	O
not	O
discounting	B
at	O
the	O
next	O
state	B
(	O
that	O
is	O
,	O
according	O
to	O
γt+1	O
;	O
recall	O
that	O
this	O
is	O
zero	O
if	O
the	O
next	O
state	B
is	O
terminal	O
)	O
.	O
to	O
the	O
extent	O
that	O
we	O
aren	O
’	O
t	O
terminating	O
at	O
the	O
next	O
state	B
,	O
we	O
have	O
a	O
second	O
term	O
which	O
is	O
itself	O
divided	O
into	O
two	O
cases	O
depending	O
on	O
the	O
degree	O
of	O
bootstrapping	O
in	O
the	O
state	O
.	O
to	O
the	O
extent	O
we	O
are	O
bootstrapping	B
,	O
this	O
term	O
is	O
the	O
estimated	O
value	B
at	O
the	O
state	B
,	O
whereas	O
,	O
to	O
the	O
extent	O
that	O
we	O
not	O
bootstrapping	B
,	O
the	O
term	O
is	O
the	O
λ-return	B
for	O
the	O
next	O
time	O
step	O
.	O
the	O
action-based	O
λ-return	B
is	O
either	O
the	O
sarsa	O
form	O
or	O
the	O
expected	O
sarsa	O
form	O
,	O
gλa	O
t	O
gλa	O
t	O
.	O
¯vt	O
(	O
s	O
)	O
.	O
.	O
=	O
rt+1	O
+	O
γt+1	O
(	O
cid:16	O
)	O
(	O
1	O
−	O
λt+1	O
)	O
ˆq	O
(	O
st+1	O
,	O
at+1	O
,	O
wt	O
)	O
+	O
λt+1gλa	O
t+1	O
(	O
cid:17	O
)	O
,	O
t+1	O
(	O
cid:17	O
)	O
,	O
=	O
rt+1	O
+	O
γt+1	O
(	O
cid:16	O
)	O
(	O
1	O
−	O
λt+1	O
)	O
¯vt	O
(	O
st+1	O
)	O
+	O
λt+1gλa	O
=	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
a|s	O
)	O
ˆq	O
(	O
s	O
,	O
a	O
,	O
wt	O
)	O
.	O
where	O
(	O
7.8	O
)	O
is	O
generalized	O
to	O
function	B
approximation	I
as	O
(	O
12.19	O
)	O
(	O
12.20	O
)	O
(	O
12.21	O
)	O
12.9.	O
oﬀ-policy	B
eligibility	O
traces	O
with	B
control	I
variates	I
311	O
exercise	O
12.7	O
generalize	O
the	O
three	O
recursive	O
equations	O
above	O
to	O
their	O
truncated	B
versions	O
,	O
(	O
cid:3	O
)	O
deﬁning	O
gλs	O
t	O
:	O
h	O
and	O
gλa	O
t	O
:	O
h.	O
12.9	O
oﬀ-policy	B
eligibility	O
traces	O
with	O
control	O
vari-	O
ates	O
the	O
ﬁnal	O
step	O
is	O
to	O
incorporate	O
importance	B
sampling	I
.	O
unlike	O
in	O
the	O
case	O
of	O
n-step	O
methods	O
,	O
for	O
full	O
non-truncated	O
λ-returns	O
one	O
does	O
not	O
have	O
a	O
practical	O
option	O
in	O
which	O
the	O
importance	B
sampling	I
is	O
done	O
outside	O
the	O
target	B
return	O
.	O
instead	O
,	O
we	O
move	O
directly	O
to	O
the	O
bootstrapping	B
generalization	O
of	O
per-decision	O
importance	B
sampling	I
with	O
control	B
variates	I
(	O
section	O
7.4	O
)	O
.	O
in	O
the	O
state	O
case	O
,	O
our	O
ﬁnal	O
deﬁnition	O
of	O
the	O
λ-return	B
generalizes	O
(	O
12.18	O
)	O
,	O
after	O
the	O
model	O
of	O
(	O
7.13	O
)	O
,	O
to	O
.	O
gλs	O
t	O
=	O
ρt	O
(	O
cid:16	O
)	O
rt+1	O
+	O
γt+1	O
(	O
cid:0	O
)	O
(	O
1−	O
λt+1	O
)	O
ˆv	O
(	O
st+1	O
,	O
wt	O
)	O
+	O
λt+1gλs	O
t+1	O
(	O
cid:1	O
)	O
(	O
cid:17	O
)	O
+	O
(	O
1−	O
ρt	O
)	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
(	O
12.22	O
)	O
where	O
ρt	O
=	O
π	O
(	O
at|st	O
)	O
b	O
(	O
at|st	O
)	O
is	O
the	O
usual	O
single-step	O
importance	B
sampling	I
ratio	O
.	O
much	O
like	O
the	O
other	O
returns	O
we	O
have	O
seen	O
in	O
this	O
book	O
,	O
the	O
truncated	B
version	O
of	O
this	O
return	B
can	O
be	O
approximated	O
simply	O
in	O
terms	O
of	O
sums	O
of	O
the	O
state-based	O
td	O
error	O
,	O
δs	O
t	O
=	O
rt+1	O
+	O
γt+1ˆv	O
(	O
st+1	O
,	O
wt	O
)	O
−	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
,	O
as	O
gλs	O
t	O
≈	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
+	O
ρt	O
δs	O
k	O
∞	O
(	O
cid:88	O
)	O
k=t	O
k	O
(	O
cid:89	O
)	O
i=t+1	O
γiλiρi	O
(	O
12.23	O
)	O
(	O
12.24	O
)	O
with	O
the	O
approximation	O
becoming	O
exact	O
if	O
the	O
approximate	B
value	O
function	O
does	O
not	O
change	O
.	O
exercise	O
12.8	O
prove	O
that	O
(	O
12.24	O
)	O
becomes	O
exact	O
if	O
the	O
value	B
function	I
does	O
not	O
change	O
.	O
(	O
cid:3	O
)	O
to	O
save	O
writing	O
,	O
consider	O
the	O
case	O
of	O
t	O
=	O
0	O
,	O
and	O
use	O
the	O
notation	O
vk	O
exercise	O
12.9	O
the	O
truncated	B
version	O
of	O
the	O
general	O
oﬀ-policy	O
return	B
is	O
denoted	O
gλs	O
t	O
:	O
h.	O
(	O
cid:3	O
)	O
guess	O
the	O
correct	O
equation	O
,	O
based	O
on	O
(	O
12.24	O
)	O
.	O
the	O
above	O
form	O
of	O
the	O
λ-return	B
(	O
12.24	O
)	O
is	O
convenient	O
to	O
use	O
in	O
a	O
forward-view	O
update	O
,	O
.	O
=	O
ˆv	O
(	O
sk	O
,	O
w	O
)	O
.	O
wt+1	O
=	O
wt	O
+	O
α	O
(	O
cid:0	O
)	O
gλs	O
≈	O
wt	O
+	O
αρt	O
(	O
cid:32	O
)	O
∞	O
(	O
cid:88	O
)	O
k=t	O
δs	O
k	O
k	O
(	O
cid:89	O
)	O
i=t+1	O
t	O
−	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
(	O
cid:1	O
)	O
∇ˆv	O
(	O
st	O
,	O
wt	O
)	O
γiλiρi	O
(	O
cid:33	O
)	O
∇ˆv	O
(	O
st	O
,	O
wt	O
)	O
,	O
which	O
to	O
the	O
experienced	O
eye	O
looks	O
like	O
an	O
eligibility-based	O
td	O
update—the	O
product	O
is	O
like	O
an	O
eligibility	O
trace	O
and	O
it	O
is	O
multiplied	O
by	O
td	O
errors	O
.	O
but	O
this	O
is	O
just	O
one	O
time	O
step	O
of	O
a	O
forward	O
view	O
.	O
the	O
relationship	O
that	O
we	O
are	O
looking	O
for	O
is	O
that	O
the	O
forward-view	O
update	O
,	O
summed	O
over	O
time	O
,	O
is	O
approximately	O
equal	O
to	O
a	O
backward-view	O
update	O
,	O
summed	O
312	O
chapter	O
12	O
:	O
eligibility	B
traces	I
over	O
time	O
(	O
this	O
relationship	O
is	O
only	O
approximate	B
because	O
again	O
we	O
ignore	O
changes	O
in	O
the	O
value	O
function	O
)	O
.	O
the	O
sum	O
of	O
the	O
forward-view	O
update	O
over	O
time	O
is	O
∞	O
(	O
cid:88	O
)	O
t=1	O
(	O
wt+1	O
−	O
wt	O
)	O
≈	O
=	O
=	O
∞	O
(	O
cid:88	O
)	O
t=1	O
∞	O
(	O
cid:88	O
)	O
k=1	O
∞	O
(	O
cid:88	O
)	O
k=1	O
∞	O
(	O
cid:88	O
)	O
k=t	O
k	O
(	O
cid:88	O
)	O
t=1	O
αδs	O
k	O
αρtδs	O
k∇ˆv	O
(	O
st	O
,	O
wt	O
)	O
γiλiρi	O
k	O
γiλiρi	O
αρt∇ˆv	O
(	O
st	O
,	O
wt	O
)	O
δs	O
(	O
using	O
the	O
summation	O
rule	O
:	O
(	O
cid:80	O
)	O
y	O
k	O
(	O
cid:88	O
)	O
t=1	O
ρt∇ˆv	O
(	O
st	O
,	O
wt	O
)	O
γiλiρi	O
,	O
k	O
(	O
cid:89	O
)	O
i=t+1	O
k	O
(	O
cid:89	O
)	O
i=t+1	O
k	O
(	O
cid:89	O
)	O
i=t+1	O
t=x	O
(	O
cid:80	O
)	O
y	O
k=t	O
=	O
(	O
cid:80	O
)	O
y	O
k=x	O
(	O
cid:80	O
)	O
k	O
t=x	O
)	O
which	O
would	O
be	O
in	O
the	O
form	O
of	O
the	O
sum	O
of	O
a	O
backward-view	O
td	O
update	O
if	O
the	O
entire	O
expression	O
from	O
the	O
second	O
sum	O
left	O
could	O
be	O
written	O
and	O
updated	O
incrementally	O
as	O
an	O
eligibility	O
trace	O
,	O
which	O
we	O
now	O
show	O
can	O
be	O
done	O
.	O
that	O
is	O
,	O
we	O
show	O
that	O
if	O
this	O
expression	O
was	O
the	O
trace	O
at	O
time	O
k	O
,	O
then	O
we	O
could	O
update	O
it	O
from	O
its	O
value	B
at	O
time	O
k	O
−	O
1	O
by	O
:	O
zk	O
=	O
=	O
k	O
(	O
cid:88	O
)	O
t=1	O
k−1	O
(	O
cid:88	O
)	O
t=1	O
ρt∇ˆv	O
(	O
st	O
,	O
wt	O
)	O
ρt∇ˆv	O
(	O
st	O
,	O
wt	O
)	O
k	O
(	O
cid:89	O
)	O
i=t+1	O
k	O
(	O
cid:89	O
)	O
i=t+1	O
γiλiρi	O
γiλiρi	O
+	O
ρk∇ˆv	O
(	O
sk	O
,	O
wk	O
)	O
=	O
γkλkρk	O
ρt∇ˆv	O
(	O
st	O
,	O
wt	O
)	O
k−1	O
(	O
cid:88	O
)	O
t=1	O
(	O
cid:124	O
)	O
k−1	O
(	O
cid:89	O
)	O
i=t+1	O
=	O
ρk	O
(	O
cid:0	O
)	O
γkλkzk−1	O
+	O
∇ˆv	O
(	O
sk	O
,	O
wk	O
)	O
(	O
cid:1	O
)	O
,	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
zk−1	O
γiλiρi	O
+	O
ρk∇ˆv	O
(	O
sk	O
,	O
wk	O
)	O
(	O
cid:125	O
)	O
which	O
,	O
changing	O
the	O
index	O
from	O
k	O
to	O
t	O
,	O
is	O
the	O
general	O
accumulating	O
trace	O
update	O
for	O
state	O
values	O
:	O
(	O
12.25	O
)	O
zt	O
.	O
=	O
ρt	O
(	O
cid:0	O
)	O
γtλtzt−1	O
+	O
∇ˆv	O
(	O
st	O
,	O
wt	O
)	O
(	O
cid:1	O
)	O
,	O
this	O
eligibility	O
trace	O
,	O
together	O
with	O
the	O
usual	O
semi-gradient	O
parameter-update	O
rule	O
for	O
td	O
(	O
λ	O
)	O
(	O
12.7	O
)	O
,	O
forms	O
a	O
general	O
td	O
(	O
λ	O
)	O
algorithm	O
that	O
can	O
be	O
applied	O
to	O
either	O
on-policy	O
or	O
oﬀ-policy	B
data	O
.	O
in	O
the	O
on-policy	O
case	O
,	O
the	O
algorithm	O
is	O
exactly	O
td	O
(	O
λ	O
)	O
because	O
ρt	O
is	O
alway	O
1	O
and	O
(	O
12.25	O
)	O
becomes	O
the	O
usual	O
accumulating	B
trace	O
(	O
12.5	O
)	O
(	O
extended	O
to	O
variable	O
λ	O
and	O
γ	O
)	O
.	O
in	O
the	O
oﬀ-policy	O
case	O
,	O
the	O
algorithm	O
often	O
works	O
well	O
but	O
,	O
as	O
a	O
semi-gradient	O
method	O
,	O
is	O
not	O
guaranteed	O
to	O
be	O
stable	O
.	O
in	O
the	O
next	O
few	O
sections	O
we	O
will	O
consider	O
extensions	O
of	O
it	O
that	O
do	O
guarantee	O
stability	O
.	O
a	O
very	O
similar	O
series	O
of	O
steps	O
can	O
be	O
followed	O
to	O
derive	O
the	O
oﬀ-policy	B
eligibility	O
traces	O
for	O
action-value	O
methods	O
and	O
corresponding	O
general	O
sarsa	O
(	O
λ	O
)	O
algorithms	O
.	O
one	O
could	O
start	O
with	O
either	O
recursive	O
form	O
for	O
the	O
general	O
action-based	O
λ-return	B
,	O
(	O
12.19	O
)	O
or	O
(	O
12.20	O
)	O
,	O
12.9.	O
oﬀ-policy	B
eligibility	O
traces	O
with	B
control	I
variates	I
313	O
but	O
the	O
latter	O
(	O
the	O
expected	O
sarsa	O
form	O
)	O
works	O
out	O
to	O
be	O
simpler	O
.	O
we	O
extend	O
(	O
12.20	O
)	O
to	O
the	O
oﬀ-policy	B
case	O
after	O
the	O
model	O
of	O
(	O
7.14	O
)	O
to	O
produce	O
.	O
gλa	O
t	O
=	O
rt+1	O
+	O
γt+1	O
(	O
cid:16	O
)	O
(	O
1	O
−	O
λt+1	O
)	O
¯vt	O
(	O
st+1	O
)	O
+	O
λt+1	O
(	O
cid:2	O
)	O
ρt+1gλa	O
=	O
rt+1	O
+	O
γt+1	O
(	O
cid:16	O
)	O
¯vt	O
(	O
st+1	O
)	O
+	O
λt+1ρt+1	O
(	O
cid:2	O
)	O
gλa	O
t+1	O
−	O
ˆq	O
(	O
st+1	O
,	O
at+1	O
,	O
wt	O
)	O
(	O
cid:3	O
)	O
(	O
cid:17	O
)	O
t+1	O
+	O
¯vt	O
(	O
st+1	O
)	O
−	O
ρt+1	O
ˆq	O
(	O
st+1	O
,	O
at+1	O
,	O
wt	O
)	O
(	O
cid:3	O
)	O
(	O
cid:17	O
)	O
(	O
12.26	O
)	O
where	O
¯vt	O
(	O
st+1	O
)	O
is	O
as	O
given	O
by	O
(	O
12.21	O
)	O
.	O
again	O
the	O
λ-return	B
can	O
be	O
written	O
approximately	O
as	O
the	O
sum	O
of	O
td	O
errors	O
,	O
gλa	O
t	O
≈	O
ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
+	O
δa	O
k	O
∞	O
(	O
cid:88	O
)	O
k=t	O
k	O
(	O
cid:89	O
)	O
i=t+1	O
γiλiρi	O
,	O
using	O
the	O
expectation	O
form	O
of	O
the	O
action-based	O
td	O
error	O
:	O
δa	O
t	O
=	O
rt+1	O
+	O
γt+1	O
¯vt	O
(	O
st+1	O
)	O
−	O
ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
.	O
(	O
12.27	O
)	O
(	O
12.28	O
)	O
as	O
before	O
,	O
the	O
approximation	O
becomes	O
exact	O
if	O
the	O
approximate	B
value	O
function	O
does	O
not	O
change	O
.	O
exercise	O
12.10	O
prove	O
that	O
(	O
12.27	O
)	O
becomes	O
exact	O
if	O
the	O
value	B
function	I
does	O
not	O
change	O
.	O
to	O
save	O
writing	O
,	O
consider	O
the	O
case	O
of	O
t	O
=	O
0	O
,	O
and	O
use	O
the	O
notation	O
qk	O
=	O
ˆq	O
(	O
sk	O
,	O
ak	O
,	O
w	O
)	O
.	O
(	O
cid:3	O
)	O
hint	O
:	O
start	O
by	O
writing	O
out	O
δa	O
exercise	O
12.11	O
the	O
truncated	B
version	O
of	O
the	O
general	O
oﬀ-policy	O
return	B
is	O
denoted	O
gλa	O
t	O
:	O
h.	O
(	O
cid:3	O
)	O
guess	O
the	O
correct	O
equation	O
for	O
it	O
,	O
based	O
on	O
(	O
12.27	O
)	O
.	O
using	O
steps	O
entirely	O
analogous	O
to	O
those	O
for	O
the	O
state	B
case	O
,	O
one	O
can	O
write	O
a	O
forward-view	O
update	O
based	O
on	O
(	O
12.27	O
)	O
,	O
transform	O
the	O
sum	O
of	O
the	O
updates	O
using	O
the	O
summation	O
rule	O
,	O
and	O
ﬁnally	O
derive	O
the	O
following	O
form	O
for	O
the	O
eligibility	O
trace	O
for	B
action	I
values	I
:	O
0	O
,	O
then	O
gλa	O
0	O
−	O
q0	O
.	O
0	O
and	O
gλa	O
zt	O
.	O
=	O
γtλtρtzt−1	O
+	O
∇ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
.	O
(	O
12.29	O
)	O
this	O
eligibility	O
trace	O
,	O
together	O
with	O
the	O
expectation-based	O
td	O
error	O
(	O
12.28	O
)	O
and	O
the	O
usual	O
semi-gradient	O
parameter-update	O
rule	O
(	O
12.7	O
)	O
,	O
forms	O
an	O
elegant	O
,	O
eﬃcient	O
expected	O
sarsa	O
(	O
λ	O
)	O
algorithm	O
that	O
can	O
be	O
applied	O
to	O
either	O
on-policy	O
or	O
oﬀ-policy	B
data	O
.	O
it	O
is	O
probably	O
the	O
best	O
algorithm	O
of	O
this	O
type	O
at	O
the	O
current	O
time	O
(	O
though	O
of	O
course	O
it	O
is	O
not	O
guaranteed	O
to	O
be	O
stable	O
until	O
combined	O
in	O
some	O
way	O
with	O
one	O
of	O
the	O
methods	O
presented	O
in	O
the	O
following	O
sections	O
)	O
.	O
in	O
the	O
on-policy	O
case	O
with	O
constant	O
λ	O
and	O
γ	O
,	O
and	O
the	O
usual	O
state-action	O
td	O
error	O
(	O
12.16	O
)	O
,	O
the	O
algorithm	O
would	O
be	O
identical	O
to	O
the	O
sarsa	O
(	O
λ	O
)	O
algorithm	O
presented	O
in	O
section	O
12.7.	O
exercise	O
12.12	O
show	O
in	O
detail	O
the	O
steps	O
outlined	O
above	O
for	O
deriving	O
(	O
12.29	O
)	O
from	O
(	O
12.27	O
)	O
.	O
start	O
with	O
the	O
update	O
(	O
12.15	O
)	O
,	O
substitute	O
gλa	O
t	O
,	O
then	O
follow	O
similar	O
(	O
cid:3	O
)	O
steps	O
as	O
led	O
to	O
(	O
12.25	O
)	O
.	O
at	O
λ	O
=	O
1	O
,	O
these	O
algorithms	O
become	O
closely	O
related	O
to	O
corresponding	O
monte	O
carlo	O
algo-	O
rithms	O
.	O
one	O
might	O
expect	O
that	O
an	O
exact	O
equivalence	O
would	O
hold	O
for	O
episodic	O
problems	O
and	O
oﬀ-line	O
updating	O
,	O
but	O
in	O
fact	O
the	O
relationship	O
is	O
subtler	O
and	O
slightly	O
weaker	O
than	O
that	O
.	O
under	O
these	O
most	O
favorable	O
conditions	O
still	O
there	O
is	O
not	O
an	O
episode	O
by	O
episode	O
equivalence	O
from	O
(	O
12.26	O
)	O
for	O
gλ	O
t	O
314	O
chapter	O
12	O
:	O
eligibility	B
traces	I
of	O
updates	O
,	O
only	O
of	O
their	O
expectations	O
.	O
this	O
should	O
not	O
be	O
surprising	O
as	O
these	O
method	O
make	O
irrevocable	O
updates	O
as	O
a	O
trajectory	O
unfolds	O
,	O
whereas	O
true	O
monte	O
carlo	O
methods	O
would	O
make	O
no	O
update	O
for	O
a	O
trajectory	O
if	O
any	O
action	B
within	O
it	O
has	O
zero	O
probability	O
under	O
the	O
target	B
policy	O
.	O
in	O
particular	O
,	O
all	O
of	O
these	O
methods	O
,	O
even	O
at	O
λ	O
=	O
1	O
,	O
still	O
bootstrap	O
in	O
the	O
sense	O
that	O
their	O
targets	O
depend	O
on	O
the	O
current	O
value	B
estimates—it	O
’	O
s	O
just	O
that	O
the	O
dependence	O
cancels	O
out	O
in	O
expected	O
value	B
.	O
whether	O
this	O
is	O
a	O
good	O
or	O
bad	O
property	O
in	O
practice	O
is	O
another	O
question	O
.	O
recently	O
,	O
methods	O
have	O
been	O
proposed	O
that	O
do	O
achieve	O
an	O
exact	O
equivalence	O
(	O
sutton	O
,	O
mahmood	O
,	O
precup	O
and	O
van	O
hasselt	O
,	O
2014	O
)	O
.	O
these	O
methods	O
require	O
an	O
additional	O
vector	B
of	O
“	O
provisional	O
weights	O
”	O
that	O
keep	O
track	O
of	O
updates	O
which	O
have	O
been	O
made	O
but	O
may	O
need	O
to	O
be	O
retracted	O
(	O
or	O
emphasized	O
)	O
depending	O
on	O
the	O
actions	O
taken	O
later	O
.	O
the	O
state	B
and	O
state–action	O
versions	O
of	O
these	O
methods	O
are	O
called	O
ptd	O
(	O
λ	O
)	O
and	O
pq	O
(	O
λ	O
)	O
respectively	O
,	O
where	O
the	O
‘	O
p	O
’	O
stands	O
for	O
provisional	O
.	O
the	O
practical	O
consequences	O
of	O
all	O
these	O
new	O
oﬀ-policy	B
methods	I
have	O
not	O
yet	O
been	O
established	O
.	O
undoubtedly	O
,	O
issues	O
of	O
high	O
variance	O
will	O
arise	O
as	O
they	O
do	O
in	O
all	O
oﬀ-policy	B
methods	I
using	O
importance	B
sampling	I
(	O
section	O
11.9	O
)	O
.	O
if	O
λ	O
<	O
1	O
,	O
then	O
all	O
these	O
oﬀ-policy	B
algorithms	O
involve	O
bootstrapping	B
and	O
the	O
deadly	B
triad	I
applies	O
(	O
section	O
11.3	O
)	O
,	O
meaning	O
that	O
they	O
can	O
be	O
guaranteed	O
stable	O
only	O
for	O
the	O
tabular	O
case	O
,	O
for	O
state	O
aggregation	O
,	O
and	O
for	O
other	O
limited	O
forms	O
of	O
function	O
approximation	O
.	O
for	O
linear	O
and	O
more-general	O
forms	O
of	O
function	O
approximation	O
the	O
parameter	O
vector	O
may	O
diverge	O
to	O
inﬁnity	O
as	O
in	O
the	O
examples	O
in	O
chapter	O
11.	O
as	O
we	O
discussed	O
there	O
,	O
the	O
challenge	O
of	O
oﬀ-policy	O
learning	O
has	O
two	O
parts	O
.	O
oﬀ-policy	B
eligibility	O
traces	O
deal	O
eﬀectively	O
with	O
the	O
ﬁrst	O
part	O
of	O
the	O
challenge	O
,	O
correcting	O
for	O
the	O
expected	O
value	O
of	O
the	O
targets	O
,	O
but	O
not	O
at	O
all	O
with	O
the	O
second	O
part	O
of	O
the	O
challenge	O
,	O
having	O
to	O
do	O
with	O
the	O
distribution	O
of	O
updates	O
.	O
algorithmic	O
strategies	O
for	O
meeting	O
the	O
second	O
part	O
of	O
the	O
challenge	O
of	O
oﬀ-policy	O
learning	O
with	O
eligibility	B
traces	I
are	O
summarized	O
in	O
section	O
12.11.	O
exercise	O
12.13	O
what	O
are	O
the	O
dutch-trace	O
and	O
replacing-trace	O
versions	O
of	O
oﬀ-policy	O
eligi-	O
(	O
cid:3	O
)	O
bility	O
traces	O
for	O
state-value	O
and	O
action-value	O
methods	O
?	O
12.10	O
watkins	O
’	O
s	O
q	O
(	O
λ	O
)	O
to	O
tree-backup	O
(	O
λ	O
)	O
several	O
methods	O
have	O
been	O
proposed	O
over	O
the	O
years	O
to	O
extend	O
q-learning	O
to	O
eligibility	B
traces	I
.	O
the	O
original	O
is	O
watkins	O
’	O
s	O
q	O
(	O
λ	O
)	O
,	O
which	O
decays	O
its	O
eligibility	B
traces	I
in	O
the	O
usual	O
way	O
as	O
long	O
as	O
a	O
greedy	O
action	O
was	O
taken	O
,	O
then	O
cuts	O
the	O
traces	O
to	O
zero	O
after	O
the	O
ﬁrst	O
non-greedy	O
action	B
.	O
the	O
backup	B
diagram	I
for	O
watkins	O
’	O
s	O
q	O
(	O
λ	O
)	O
is	O
shown	O
in	O
figure	O
12.12.	O
in	O
chapter	O
6	O
,	O
we	O
uniﬁed	O
q-learning	O
and	O
expected	O
sarsa	O
in	O
the	O
oﬀ-policy	O
version	O
of	O
the	O
latter	O
,	O
which	O
includes	O
q-learning	O
as	O
a	O
special	O
case	O
,	O
and	O
generalizes	O
it	O
to	O
arbitrary	O
target	B
policies	O
,	O
and	O
in	O
the	O
previous	O
section	O
of	O
this	O
chapter	O
we	O
completed	O
our	O
treatment	O
of	O
expected	O
sarsa	O
by	O
generalizing	O
it	O
to	O
oﬀ-policy	B
eligibility	O
traces	O
.	O
in	O
chapter	O
7	O
,	O
however	O
,	O
we	O
distinguished	O
n-step	B
expected	O
sarsa	O
from	O
n-step	B
tree	O
backup	O
,	O
where	O
the	O
latter	O
retained	O
the	O
property	O
of	O
not	O
using	O
importance	B
sampling	I
.	O
it	O
remains	O
then	O
to	O
present	O
the	O
eligibility	O
trace	O
version	O
of	O
tree	O
backup	O
,	O
which	O
we	O
well	O
call	O
tree-backup	O
(	O
λ	O
)	O
,	O
or	O
tb	O
(	O
λ	O
)	O
for	O
short	O
.	O
this	O
is	O
arguably	O
the	O
true	O
successor	O
to	O
q-learning	O
because	O
it	O
retains	O
its	O
appealing	O
absence	O
of	O
importance	O
sampling	O
even	O
though	O
it	O
can	O
be	O
applied	O
to	O
oﬀ-policy	B
data	O
.	O
12.10.	O
watkins	O
’	O
s	O
q	O
(	O
λ	O
)	O
to	O
tree-backup	O
(	O
λ	O
)	O
315	O
figure	O
12.12	O
:	O
the	O
backup	B
diagram	I
for	O
watkins	O
’	O
s	O
q	O
(	O
λ	O
)	O
.	O
the	O
series	O
of	O
component	O
updates	O
ends	O
either	O
with	O
the	O
end	O
of	O
the	O
episode	O
or	O
with	O
the	O
ﬁrst	O
nongreedy	O
action	B
,	O
whichever	O
comes	O
ﬁrst	O
.	O
the	O
concept	O
of	O
tb	O
(	O
λ	O
)	O
is	O
straightforward	O
.	O
as	O
shown	O
in	O
its	O
backup	B
diagram	I
in	O
fig-	O
ure	O
12.13	O
,	O
the	O
tree-backup	O
updates	O
of	O
each	O
length	O
(	O
from	O
section	O
7.5	O
)	O
are	O
weighted	O
in	O
the	O
usual	O
way	O
dependent	O
on	O
the	O
bootstrapping	B
parameter	O
λ.	O
to	O
get	O
the	O
detailed	O
equations	O
,	O
with	O
the	O
right	O
indices	O
on	O
the	O
general	O
bootstrapping	O
and	O
discounting	O
parameters	O
,	O
it	O
is	O
best	O
to	O
start	O
with	O
a	O
recursive	O
form	O
(	O
12.20	O
)	O
for	O
the	O
λ-return	B
using	O
action	B
values	O
,	O
and	O
then	O
expand	O
the	O
bootstrapping	B
case	O
of	O
the	O
target	B
after	O
the	O
model	O
of	O
(	O
7.16	O
)	O
:	O
as	O
per	O
the	O
usual	O
pattern	O
,	O
it	O
can	O
also	O
be	O
written	O
approximately	O
(	O
ignoring	O
changes	O
in	O
the	O
approximate	O
value	B
function	I
)	O
as	O
a	O
sum	O
of	O
td	O
errors	O
,	O
gλa	O
t	O
≈	O
ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
+	O
δa	O
k	O
∞	O
(	O
cid:88	O
)	O
k=t	O
k	O
(	O
cid:89	O
)	O
i=t+1	O
γiλiπ	O
(	O
ai|si	O
)	O
,	O
using	O
the	O
expectation	O
form	O
of	O
the	O
action-based	O
td	O
error	O
(	O
12.28	O
)	O
.	O
following	O
the	O
same	O
steps	O
as	O
in	O
the	O
previous	O
section	O
,	O
we	O
arrive	O
at	O
a	O
special	O
eligibility	O
trace	O
update	O
involving	O
the	O
target-policy	O
probabilities	O
of	O
the	O
selected	O
actions	O
,	O
zt	O
.	O
=	O
γtλtπ	O
(	O
at|st	O
)	O
zt−1	O
+	O
∇ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
.	O
gλa	O
t	O
.	O
=	O
rt+1	O
+	O
γt+1	O
(	O
cid:18	O
)	O
(	O
1	O
−	O
λt+1	O
)	O
¯vt	O
(	O
st+1	O
)	O
+	O
λt+1	O
(	O
cid:104	O
)	O
(	O
cid:88	O
)	O
a	O
(	O
cid:54	O
)	O
=at+1	O
=	O
rt+1	O
+	O
γt+1	O
(	O
cid:18	O
)	O
¯vt	O
(	O
st+1	O
)	O
+	O
λt+1π	O
(	O
at+1|st+1	O
)	O
(	O
cid:16	O
)	O
gλa	O
π	O
(	O
a|st+1	O
)	O
ˆq	O
(	O
st+1	O
,	O
a	O
,	O
wt	O
)	O
+	O
π	O
(	O
at+1|st+1	O
)	O
gλa	O
t+1	O
−	O
ˆq	O
(	O
st+1	O
,	O
at+1	O
,	O
wt	O
)	O
(	O
cid:17	O
)	O
(	O
cid:19	O
)	O
t+1	O
(	O
cid:105	O
)	O
(	O
cid:19	O
)	O
1  	O
(	O
1  	O
)	O
 	O
(	O
1  	O
)	O
 2 t t 1···statat+1st+1rt+1strt···st+2rt+2at+2x=1or······st+nrt+nfirstnon-greedyaction n 1watkins	O
’	O
sq	O
(	O
 	O
)	O
316	O
chapter	O
12	O
:	O
eligibility	B
traces	I
figure	O
12.13	O
:	O
the	O
backup	B
diagram	I
for	O
the	O
λ	O
version	O
of	O
the	O
tree	O
backup	O
algorithm	O
.	O
this	O
,	O
together	O
with	O
the	O
usual	O
parameter-update	O
rule	O
(	O
12.7	O
)	O
,	O
deﬁnes	O
the	O
tb	O
(	O
λ	O
)	O
algorithm	O
.	O
like	O
all	O
semi-gradient	O
algorithms	O
,	O
tb	O
(	O
λ	O
)	O
is	O
not	O
guaranteed	O
to	O
be	O
stable	O
when	O
used	O
with	O
oﬀ-policy	O
data	O
and	O
with	O
a	O
powerful	O
function	O
approximator	O
.	O
to	O
obtain	O
those	O
assurances	O
,	O
tb	O
(	O
λ	O
)	O
would	O
have	O
to	O
be	O
combined	O
with	O
one	O
of	O
the	O
methods	O
presented	O
in	O
the	O
next	O
section	O
.	O
∗exercise	O
12.14	O
how	O
might	O
double	B
expected	O
sarsa	O
be	O
extended	O
to	O
eligibility	B
traces	I
?	O
(	O
cid:3	O
)	O
12.11	O
stable	O
oﬀ-policy	B
methods	I
with	O
traces	O
several	O
methods	O
using	O
eligibility	B
traces	I
have	O
been	O
proposed	O
that	O
achieve	O
guarantees	O
of	O
stability	O
under	O
oﬀ-policy	B
training	O
,	O
and	O
here	O
we	O
present	O
four	O
of	O
the	O
most	O
important	O
using	O
this	O
book	O
’	O
s	O
standard	O
notation	O
,	O
including	O
general	O
bootstrapping	O
and	O
discounting	O
func-	O
tions	O
.	O
all	O
are	O
based	O
on	O
either	O
the	O
gradient-td	O
or	O
the	O
emphatic-td	O
ideas	O
presented	O
in	O
sections	O
11.7	O
and	O
11.8.	O
all	O
the	O
algorithms	O
assume	O
linear	B
function	I
approximation	I
,	O
though	O
extensions	O
to	O
nonlinear	O
function	B
approximation	I
can	O
also	O
be	O
found	O
in	O
the	O
literature	O
.	O
gtd	O
(	O
λ	O
)	O
is	O
the	O
eligibility-trace	O
algorithm	O
analogous	O
to	O
tdc	O
,	O
the	O
better	O
of	O
the	O
two	O
state-value	O
gradient-td	O
prediction	B
algorithms	O
discussed	O
in	O
section	O
11.7.	O
its	O
goal	B
is	O
to	O
.	O
=	O
w	O
(	O
cid:62	O
)	O
t	O
x	O
(	O
s	O
)	O
≈	O
vπ	O
(	O
s	O
)	O
,	O
even	O
from	O
data	O
that	O
is	O
due	O
learn	O
a	O
parameter	O
wt	O
such	O
that	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
to	O
following	O
another	O
policy	B
b.	O
its	O
update	O
is	O
wt+1	O
.	O
=	O
wt	O
+	O
αδs	O
t	O
zt	O
−	O
αγt+1	O
(	O
1	O
−	O
λt+1	O
)	O
(	O
cid:0	O
)	O
z	O
(	O
cid:62	O
)	O
t	O
vt	O
(	O
cid:1	O
)	O
xt+1	O
,	O
1  	O
(	O
1  	O
)	O
 	O
(	O
1  	O
)	O
 2 t t 1···statat+1at 1st+1rt+1strt···st+2rt+2at+2x=1···treebackup	O
(	O
 	O
)	O
st 1	O
12.11.	O
stable	O
oﬀ-policy	B
methods	I
with	O
traces	O
317	O
with	O
δs	O
t	O
,	O
zt	O
,	O
and	O
ρt	O
deﬁned	O
in	O
the	O
usual	O
ways	O
for	O
state	O
values	O
(	O
12.23	O
)	O
(	O
12.25	O
)	O
(	O
11.1	O
)	O
,	O
and	O
vt+1	O
.	O
=	O
vt	O
+	O
βδs	O
t	O
zt	O
−	O
β	O
(	O
cid:0	O
)	O
v	O
(	O
cid:62	O
)	O
t	O
xt	O
(	O
cid:1	O
)	O
xt	O
,	O
where	O
,	O
as	O
in	O
section	O
11.7	O
,	O
v	O
∈	O
rd	O
is	O
a	O
vector	B
of	O
the	O
same	O
dimension	O
as	O
w	O
,	O
initialized	O
to	O
v0	O
=	O
0	O
,	O
and	O
β	O
>	O
0	O
is	O
a	O
second	O
step-size	B
parameter	I
.	O
gq	O
(	O
λ	O
)	O
is	O
the	O
gradient-td	O
algorithm	O
for	B
action	I
values	I
with	O
eligibility	B
traces	I
.	O
its	O
goal	B
.	O
=	O
w	O
(	O
cid:62	O
)	O
t	O
x	O
(	O
s	O
,	O
a	O
)	O
≈	O
qπ	O
(	O
s	O
,	O
a	O
)	O
from	O
oﬀ-policy	B
is	O
to	O
learn	O
a	O
parameter	O
wt	O
such	O
that	O
ˆq	O
(	O
s	O
,	O
a	O
,	O
wt	O
)	O
data	O
.	O
if	O
the	O
target	B
policy	O
is	O
ε-greedy	O
,	O
or	O
otherwise	O
biased	O
toward	O
the	O
greedy	O
policy	O
for	O
ˆq	O
,	O
then	O
gq	O
(	O
λ	O
)	O
can	O
be	O
used	O
as	O
a	O
control	B
algorithm	O
.	O
its	O
update	O
is	O
(	O
12.30	O
)	O
wt+1	O
.	O
=	O
wt	O
+	O
αδa	O
t	O
zt	O
−	O
αγt+1	O
(	O
1	O
−	O
λt+1	O
)	O
(	O
cid:0	O
)	O
z	O
(	O
cid:62	O
)	O
t	O
vt	O
(	O
cid:1	O
)	O
¯xt+1	O
,	O
where	O
¯xt	O
is	O
the	O
average	O
feature	O
vector	B
for	O
st	O
under	O
the	O
target	B
policy	O
,	O
¯xt	O
.	O
=	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
a|st	O
)	O
x	O
(	O
st	O
,	O
a	O
)	O
,	O
δa	O
t	O
is	O
the	O
expectation	O
form	O
of	O
the	O
td	O
error	O
,	O
which	O
can	O
be	O
written	O
δa	O
t	O
.	O
=	O
rt+1	O
+	O
γt+1w	O
(	O
cid:62	O
)	O
t	O
¯xt+1	O
−	O
w	O
(	O
cid:62	O
)	O
t	O
xt	O
,	O
zt	O
is	O
deﬁned	O
in	O
the	O
usual	O
way	O
for	B
action	I
values	I
(	O
12.29	O
)	O
,	O
and	O
the	O
rest	O
is	O
as	O
in	O
gtd	O
(	O
λ	O
)	O
,	O
including	O
the	O
update	O
for	O
vt	O
(	O
12.30	O
)	O
.	O
htd	O
(	O
λ	O
)	O
is	O
a	O
hybrid	O
state-value	O
algorithm	O
combining	O
aspects	O
of	O
gtd	O
(	O
λ	O
)	O
and	O
td	O
(	O
λ	O
)	O
.	O
its	O
most	O
appealing	O
feature	O
is	O
that	O
it	O
is	O
a	O
strict	O
generalization	O
of	O
td	O
(	O
λ	O
)	O
to	O
oﬀ-policy	B
learning	O
,	O
meaning	O
that	O
if	O
the	O
behavior	B
policy	I
happens	O
to	O
be	O
the	O
same	O
as	O
the	O
target	B
policy	O
,	O
then	O
htd	O
(	O
λ	O
)	O
becomes	O
the	O
same	O
as	O
td	O
(	O
λ	O
)	O
,	O
which	O
is	O
not	O
true	O
for	O
gtd	O
(	O
λ	O
)	O
.	O
this	O
is	O
appealing	O
because	O
td	O
(	O
λ	O
)	O
is	O
often	O
faster	O
than	O
gtd	O
(	O
λ	O
)	O
when	O
both	O
algorithms	O
converge	O
,	O
and	O
td	O
(	O
λ	O
)	O
requires	O
setting	O
only	O
a	O
single	O
step	O
size	O
.	O
htd	O
(	O
λ	O
)	O
is	O
deﬁned	O
by	O
wt+1	O
vt+1	O
zt	O
zb	O
t	O
.	O
=	O
wt	O
+	O
αδs	O
.	O
=	O
vt	O
+	O
βδs	O
.	O
t	O
zt	O
+	O
α	O
(	O
cid:0	O
)	O
(	O
zt	O
−	O
zb	O
t	O
zt	O
−	O
β	O
(	O
cid:16	O
)	O
zb	O
=	O
ρt	O
(	O
cid:0	O
)	O
γtλtzt−1	O
+	O
xt	O
(	O
cid:1	O
)	O
,	O
.	O
=	O
γtλtzb	O
with	O
zb	O
t−1	O
+	O
xt	O
,	O
t	O
t	O
)	O
(	O
cid:62	O
)	O
vt	O
(	O
cid:1	O
)	O
(	O
xt	O
−	O
γt+1xt+1	O
)	O
,	O
(	O
cid:62	O
)	O
vt	O
(	O
cid:17	O
)	O
(	O
xt	O
−	O
γt+1xt+1	O
)	O
,	O
.	O
=	O
0	O
,	O
with	O
z−1	O
.	O
=	O
0	O
,	O
−1	O
with	O
v0	O
.	O
=	O
0	O
,	O
in	O
addition	O
to	O
the	O
second	O
set	O
of	O
where	O
β	O
>	O
0	O
again	O
is	O
a	O
second	O
step-size	B
parameter	I
.	O
weights	O
,	O
vt	O
,	O
htd	O
(	O
λ	O
)	O
also	O
has	O
a	O
second	O
set	O
of	O
eligibility	O
traces	O
,	O
zb	O
t.	O
these	O
are	O
conventional	O
accumulating	B
eligibility	O
traces	O
for	O
the	O
behavior	B
policy	I
and	O
become	O
equal	O
to	O
zt	O
if	O
all	O
the	O
ρt	O
are	O
1	O
,	O
which	O
causes	O
the	O
last	O
term	O
in	O
the	O
wt	O
update	O
to	O
be	O
zero	O
and	O
the	O
overall	O
update	O
to	O
reduce	O
to	O
td	O
(	O
λ	O
)	O
.	O
emphatic	O
td	O
(	O
λ	O
)	O
is	O
the	O
extension	O
of	O
the	O
one-step	O
emphatic-td	O
algorithm	O
(	O
sections	O
9.11	O
and	O
11.8	O
)	O
to	O
eligibility	B
traces	I
.	O
the	O
resultant	O
algorithm	O
retains	O
strong	O
oﬀ-policy	O
convergence	O
guarantees	O
while	O
enabling	O
any	O
degree	O
of	O
bootstrapping	O
,	O
albeit	O
at	O
the	O
cost	O
of	O
318	O
chapter	O
12	O
:	O
eligibility	B
traces	I
high	O
variance	O
and	O
potentially	O
slow	O
convergence	O
.	O
emphatic	O
td	O
(	O
λ	O
)	O
is	O
deﬁned	O
by	O
wt+1	O
δt	O
zt	O
mt	O
ft	O
.	O
=	O
wt	O
+	O
αδtzt	O
.	O
=	O
rt+1	O
+	O
γt+1w	O
(	O
cid:62	O
)	O
t	O
xt+1	O
−	O
w	O
(	O
cid:62	O
)	O
t	O
xt	O
=	O
ρt	O
(	O
cid:0	O
)	O
γtλtzt−1	O
+	O
mtxt	O
(	O
cid:1	O
)	O
,	O
.	O
.	O
=	O
λt	O
it	O
+	O
(	O
1	O
−	O
λt	O
)	O
ft	O
.	O
=	O
ρt−1γtft−1	O
+	O
it	O
,	O
with	O
f0	O
with	O
z−1	O
.	O
=	O
0	O
,	O
.	O
=	O
i	O
(	O
s0	O
)	O
,	O
where	O
mt	O
≥	O
0	O
is	O
the	O
general	O
form	O
of	O
emphasis	O
,	O
ft	O
≥	O
0	O
is	O
termed	O
the	O
followon	O
trace	O
,	O
and	O
it	O
≥	O
0	O
is	O
the	O
interest	O
,	O
as	O
described	O
in	O
section	O
11.8.	O
note	O
that	O
mt	O
,	O
like	O
δt	O
,	O
is	O
not	O
really	O
an	O
additional	O
memory	O
variable	O
.	O
it	O
can	O
be	O
removed	O
from	O
the	O
algorithm	O
by	O
substituting	O
its	O
deﬁnition	O
into	O
the	O
eligibility-trace	O
equation	O
.	O
pseudocode	O
and	O
software	O
for	O
the	O
true	B
online	I
version	O
of	O
emphatic-td	O
(	O
λ	O
)	O
are	O
available	O
on	O
the	O
web	O
(	O
sutton	O
,	O
2015b	O
)	O
.	O
in	O
the	O
on-policy	O
case	O
(	O
ρt	O
=	O
0	O
,	O
for	O
all	O
t	O
)	O
,	O
emphatic-td	O
(	O
λ	O
)	O
is	O
similar	O
to	O
conventional	O
td	O
(	O
λ	O
)	O
,	O
but	O
still	O
signiﬁcantly	O
diﬀerent	O
.	O
in	O
fact	O
,	O
whereas	O
emphatic-td	O
(	O
λ	O
)	O
is	O
guaranteed	O
to	O
converge	O
for	O
all	O
state-dependent	O
λ	O
functions	O
,	O
td	O
(	O
λ	O
)	O
is	O
not	O
.	O
td	O
(	O
λ	O
)	O
is	O
guaranteed	O
convergent	O
only	O
for	O
all	O
constant	O
λ.	O
see	O
yu	O
’	O
s	O
counterexample	O
(	O
ghiassian	O
,	O
raﬁee	O
,	O
and	O
sutton	O
,	O
2016	O
)	O
.	O
12.12	O
implementation	O
issues	O
it	O
might	O
at	O
ﬁrst	O
appear	O
that	O
tabular	O
methods	O
using	O
eligibility	B
traces	I
are	O
much	O
more	O
complex	O
than	O
one-step	O
methods	O
.	O
a	O
naive	B
implementation	O
would	O
require	O
every	O
state	B
(	O
or	O
state–action	O
pair	O
)	O
to	O
update	O
both	O
its	O
value	B
estimate	O
and	O
its	O
eligibility	O
trace	O
on	O
every	O
time	O
step	O
.	O
this	O
would	O
not	O
be	O
a	O
problem	O
for	O
implementations	O
on	O
single-instruction	O
,	O
multiple-	O
data	O
,	O
parallel	O
computers	O
or	O
in	O
plausible	O
neural	B
implementations	O
,	O
but	O
it	O
is	O
a	O
problem	O
for	O
implementations	O
on	O
conventional	O
serial	O
computers	O
.	O
fortunately	O
,	O
for	O
typical	O
values	O
of	O
λ	O
and	O
γ	O
the	O
eligibility	B
traces	I
of	O
almost	O
all	O
states	O
are	O
almost	O
always	O
nearly	O
zero	O
;	O
only	O
those	O
states	O
that	O
have	O
recently	O
been	O
visited	O
will	O
have	O
traces	O
signiﬁcantly	O
greater	O
than	O
zero	O
and	O
only	O
these	O
few	O
states	O
need	O
to	O
be	O
updated	O
to	O
closely	O
approximate	B
these	O
algorithms	O
.	O
in	O
practice	O
,	O
then	O
,	O
implementations	O
on	O
conventional	O
computers	O
may	O
keep	O
track	O
of	O
and	O
update	O
only	O
the	O
few	O
traces	O
that	O
are	O
signiﬁcantly	O
greater	O
than	O
zero	O
.	O
using	O
this	O
trick	O
,	O
the	O
computational	O
expense	O
of	O
using	O
traces	O
in	O
tabular	O
methods	O
is	O
typically	O
just	O
a	O
few	O
times	O
that	O
of	O
a	O
one-step	O
method	O
.	O
the	O
exact	O
multiple	O
of	O
course	O
depends	O
on	O
λ	O
and	O
γ	O
and	O
on	O
the	O
expense	O
of	O
the	O
other	O
computations	O
.	O
note	O
that	O
the	O
tabular	O
case	O
is	O
in	O
some	O
sense	O
the	O
worst	O
case	O
for	O
the	O
computational	O
complexity	O
of	O
eligibility	O
traces	O
.	O
when	O
function	O
approx-	O
imation	O
is	O
used	O
,	O
the	O
computational	O
advantages	O
of	O
not	O
using	O
traces	O
generally	O
decrease	O
.	O
for	O
example	O
,	O
if	O
artiﬁcial	B
neural	I
networks	I
and	O
backpropagation	B
are	O
used	O
,	O
then	O
eligibility	B
traces	I
generally	O
cause	O
only	O
a	O
doubling	O
of	O
the	O
required	O
memory	O
and	O
computation	O
per	O
step	O
.	O
truncated	B
λ-return	I
methods	O
(	O
section	O
12.3	O
)	O
can	O
be	O
computationally	O
eﬃcient	O
on	O
conventional	O
computers	O
though	O
they	O
always	O
require	O
some	O
additional	O
memory	O
.	O
12.13.	O
conclusions	O
319	O
12.13	O
conclusions	O
eligibility	B
traces	I
in	O
conjunction	O
with	O
td	O
errors	O
provide	O
an	O
eﬃcient	O
,	O
incremental	O
way	O
of	O
shifting	O
and	O
choosing	O
between	O
monte	O
carlo	O
and	O
td	O
methods	O
.	O
the	O
atomic	O
n-step	B
methods	I
of	O
chapter	O
7	O
also	O
enabled	O
this	O
,	O
but	O
eligibility	O
trace	O
methods	O
are	O
more	O
gen-	O
eral	O
,	O
often	O
faster	O
to	O
learn	O
,	O
and	O
oﬀer	O
diﬀerent	O
computational	O
complexity	O
tradeoﬀs	O
.	O
this	O
chapter	O
has	O
oﬀered	O
an	O
introduction	O
to	O
the	O
elegant	O
,	O
emerging	O
theoretical	O
understanding	O
of	O
eligibility	O
traces	O
for	O
on-	O
and	O
oﬀ-policy	O
learning	O
and	O
for	O
variable	O
bootstrapping	B
and	O
discounting	B
.	O
one	O
aspect	O
of	O
this	O
elegant	O
theory	O
is	O
true	B
online	I
methods	O
,	O
which	O
exactly	O
reproduce	O
the	O
behavior	O
of	O
expensive	O
ideal	O
methods	O
while	O
retaining	O
the	O
computational	O
congeniality	O
of	O
conventional	O
td	O
methods	O
.	O
another	O
aspect	O
is	O
the	O
possibility	O
of	O
deriva-	O
tions	O
that	O
automatically	O
convert	O
from	O
intuitive	O
forward-view	O
methods	O
to	O
more	O
eﬃcient	O
incremental	O
backward-view	O
algorithms	O
.	O
we	O
illustrated	O
this	O
general	O
idea	O
in	O
a	O
derivation	O
that	O
started	O
with	O
a	O
classical	O
,	O
expensive	O
monte	O
carlo	O
algorithm	O
and	O
ended	O
with	O
a	O
cheap	O
incremental	O
non-td	O
implementation	O
using	O
the	O
same	O
novel	O
eligibility	O
trace	O
used	O
in	O
true	O
online	B
td	O
methods	O
.	O
as	O
we	O
mentioned	O
in	O
chapter	O
5	O
,	O
monte	O
carlo	O
methods	O
may	O
have	O
advantages	O
in	O
non-	O
markov	O
tasks	O
because	O
they	O
do	O
not	O
bootstrap	O
.	O
because	O
eligibility	B
traces	I
make	O
td	O
methods	O
more	O
like	O
monte	O
carlo	O
methods	O
,	O
they	O
also	O
can	O
have	O
advantages	O
in	O
these	O
cases	O
.	O
if	O
one	O
wants	O
to	O
use	O
td	O
methods	O
because	O
of	O
their	O
other	O
advantages	O
,	O
but	O
the	O
task	O
is	O
at	O
least	O
partially	O
non-markov	O
,	O
then	O
the	O
use	O
of	O
an	O
eligibility	O
trace	O
method	O
is	O
indicated	O
.	O
eligibility	B
traces	I
are	O
the	O
ﬁrst	O
line	O
of	O
defense	O
against	O
both	O
long-delayed	O
rewards	O
and	O
non-markov	O
tasks	O
.	O
by	O
adjusting	O
λ	O
,	O
we	O
can	O
place	O
eligibility	O
trace	O
methods	O
anywhere	O
along	O
a	O
continuum	O
from	O
monte	O
carlo	O
to	O
one-step	O
td	O
methods	O
.	O
where	O
shall	O
we	O
place	O
them	O
?	O
we	O
do	O
not	O
yet	O
have	O
a	O
good	O
theoretical	O
answer	O
to	O
this	O
question	O
,	O
but	O
a	O
clear	O
empirical	O
answer	O
appears	O
to	O
be	O
emerging	O
.	O
on	O
tasks	O
with	O
many	O
steps	O
per	O
episode	O
,	O
or	O
many	O
steps	O
within	O
the	O
half-life	O
of	O
discounting	O
,	O
it	O
appears	O
signiﬁcantly	O
better	O
to	O
use	O
eligibility	B
traces	I
than	O
not	O
to	O
(	O
e.g.	O
,	O
see	O
figure	O
12.14	O
)	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
the	O
traces	O
are	O
so	O
long	O
as	O
to	O
produce	O
a	O
pure	O
monte	O
carlo	O
method	O
,	O
or	O
nearly	O
so	O
,	O
then	O
performance	O
degrades	O
sharply	O
.	O
an	O
intermediate	O
mixture	O
appears	O
to	O
be	O
the	O
best	O
choice	O
.	O
eligibility	B
traces	I
should	O
be	O
used	O
to	O
bring	O
us	O
toward	O
monte	O
carlo	O
methods	O
,	O
but	O
not	O
all	O
the	O
way	O
there	O
.	O
in	O
the	O
future	O
it	O
may	O
be	O
possible	O
to	O
more	O
ﬁnely	O
vary	O
the	O
trade-oﬀ	O
between	O
td	O
and	O
monte	O
carlo	O
methods	O
by	O
using	O
variable	O
λ	O
,	O
but	O
at	O
present	O
it	O
is	O
not	O
clear	O
how	O
this	O
can	O
be	O
done	O
reliably	O
and	O
usefully	O
.	O
methods	O
using	O
eligibility	B
traces	I
require	O
more	O
computation	O
than	O
one-step	O
methods	O
,	O
but	O
in	O
return	O
they	O
oﬀer	O
signiﬁcantly	O
faster	O
learning	O
,	O
particularly	O
when	O
rewards	O
are	O
delayed	O
by	O
many	O
steps	O
.	O
thus	O
it	O
often	O
makes	O
sense	O
to	O
use	O
eligibility	B
traces	I
when	O
data	O
are	O
scarce	O
and	O
can	O
not	O
be	O
repeatedly	O
processed	O
,	O
as	O
is	O
often	O
the	O
case	O
in	O
online	O
applications	O
.	O
on	O
the	O
other	O
hand	O
,	O
in	O
oﬀ-line	O
applications	O
in	O
which	O
data	O
can	O
be	O
generated	O
cheaply	O
,	O
perhaps	O
from	O
an	O
inexpensive	O
simulation	O
,	O
then	O
it	O
often	O
does	O
not	O
pay	O
to	O
use	O
eligibility	B
traces	I
.	O
in	O
these	O
cases	O
the	O
objective	O
is	O
not	O
to	O
get	O
more	O
out	O
of	O
a	O
limited	O
amount	O
of	O
data	O
,	O
but	O
simply	O
to	O
process	O
as	O
much	O
data	O
as	O
possible	O
as	O
quickly	O
as	O
possible	O
.	O
in	O
these	O
cases	O
the	O
speedup	O
per	O
datum	O
due	O
to	O
traces	O
is	O
typically	O
not	O
worth	O
their	O
computational	O
cost	O
,	O
and	O
one-step	O
methods	O
are	O
favored	O
.	O
320	O
chapter	O
12	O
:	O
eligibility	B
traces	I
figure	O
12.14	O
:	O
the	O
eﬀect	O
of	O
λ	O
on	O
reinforcement	B
learning	I
performance	O
in	O
four	O
diﬀerent	O
test	O
problems	O
.	O
in	O
all	O
cases	O
,	O
performance	O
is	O
generally	O
best	O
(	O
a	O
lower	O
number	O
in	O
the	O
graph	O
)	O
at	O
an	O
intermediate	O
value	B
of	O
λ.	O
the	O
two	O
left	O
panels	O
are	O
applications	O
to	O
simple	O
continuous-state	O
control	B
tasks	O
using	O
the	O
sarsa	O
(	O
λ	O
)	O
algorithm	O
and	O
tile	O
coding	O
,	O
with	O
either	O
replacing	B
or	O
accumulating	B
traces	O
(	O
sutton	O
,	O
1996	O
)	O
.	O
the	O
upper-right	O
panel	O
is	O
for	O
policy	O
evaluation	O
on	O
a	O
random	B
walk	I
task	O
using	O
td	O
(	O
λ	O
)	O
(	O
singh	O
and	O
sutton	O
,	O
1996	O
)	O
.	O
the	O
lower	O
right	O
panel	O
is	O
unpublished	O
data	O
for	O
the	O
pole-	O
balancing	O
task	O
(	O
example	O
3.4	O
)	O
from	O
an	O
earlier	O
study	O
(	O
sutton	O
,	O
1984	O
)	O
.	O
accumulatingtraces	O
0.20.30.40.500.20.40.60.81	O
!	O
random	O
walk50100150200250300failures	O
per100,000	O
steps00.20.40.60.81	O
!	O
cart	O
and	O
pole400450500550600650700	O
steps	O
perepisode00.20.40.60.81	O
!	O
mountain	O
carreplacingtraces150160170180190200210220230240cost	O
perepisode00.20.40.60.81	O
!	O
puddle	O
worldreplacingtracesaccumulatingtraces	O
replacingtracesaccumulatingtracesrms	O
error	O
12.13.	O
conclusions	O
321	O
bibliographical	O
and	O
historical	O
remarks	O
eligibility	B
traces	I
came	O
into	O
reinforcement	B
learning	I
via	O
the	O
fecund	O
ideas	O
of	O
klopf	O
(	O
1972	O
)	O
.	O
our	O
use	O
of	O
eligibility	O
traces	O
is	O
based	O
on	O
klopf	O
’	O
s	O
work	O
(	O
sutton	O
,	O
1978a	O
,	O
1978b	O
,	O
1978c	O
;	O
barto	O
and	O
sutton	O
,	O
1981a	O
,	O
1981b	O
;	O
sutton	O
and	O
barto	O
,	O
1981a	O
;	O
barto	O
,	O
sutton	O
,	O
and	O
anderson	O
,	O
1983	O
;	O
sutton	O
,	O
1984	O
)	O
.	O
we	O
may	O
have	O
been	O
the	O
ﬁrst	O
to	O
use	O
the	O
term	O
“	O
eligibility	O
trace	O
”	O
(	O
sutton	O
and	O
barto	O
,	O
1981a	O
)	O
.	O
the	O
idea	O
that	O
stimuli	O
produce	O
after	O
eﬀects	O
in	O
the	O
nervous	O
system	O
that	O
are	O
important	O
for	O
learning	O
is	O
very	O
old	O
(	O
see	O
chapter	O
14	O
)	O
.	O
some	O
of	O
the	O
earliest	O
uses	O
of	O
eligibility	O
traces	O
were	O
in	O
the	O
actor–critic	O
methods	O
discussed	O
in	O
chapter	O
13	O
(	O
barto	O
,	O
sutton	O
,	O
and	O
anderson	O
,	O
1983	O
;	O
sutton	O
,	O
1984	O
)	O
.	O
12.1	O
compound	B
updates	O
were	O
called	O
“	O
complex	B
backups	I
”	O
in	O
the	O
ﬁrst	O
edition	O
of	O
this	O
book	O
.	O
the	O
λ-return	B
and	O
its	O
error-reduction	O
properties	O
were	O
introduced	O
by	O
watkins	O
(	O
1989	O
)	O
and	O
further	O
developed	O
by	O
jaakkola	O
,	O
jordan	O
,	O
and	O
singh	O
(	O
1994	O
)	O
.	O
the	O
ran-	O
dom	O
walk	O
results	O
in	O
this	O
and	O
subsequent	O
sections	O
are	O
new	O
to	O
this	O
text	O
,	O
as	O
are	O
the	O
terms	O
“	O
forward	O
view	O
”	O
and	O
“	O
backward	O
view.	O
”	O
the	O
notion	O
of	O
a	O
λ-return	B
algorithm	O
was	O
introduced	O
in	O
the	O
ﬁrst	O
edition	O
of	O
this	O
text	O
.	O
the	O
more	O
reﬁned	O
treatment	O
pre-	O
sented	O
here	O
was	O
developed	O
in	O
conjunction	O
with	O
harm	O
van	O
seijen	O
(	O
e.g.	O
,	O
van	O
seijen	O
and	O
sutton	O
,	O
2014	O
)	O
.	O
12.2	O
td	O
(	O
λ	O
)	O
with	O
accumulating	O
traces	O
was	O
introduced	O
by	O
sutton	O
(	O
1988	O
,	O
1984	O
)	O
.	O
con-	O
vergence	O
in	O
the	O
mean	O
was	O
proved	O
by	O
dayan	O
(	O
1992	O
)	O
,	O
and	O
with	O
probability	O
1	O
by	O
many	O
researchers	O
,	O
including	O
peng	O
(	O
1993	O
)	O
,	O
dayan	O
and	O
sejnowski	O
(	O
1994	O
)	O
,	O
tsitsiklis	O
(	O
1994	O
)	O
,	O
and	O
gurvits	O
,	O
lin	O
,	O
and	O
hanson	O
(	O
1994	O
)	O
.	O
the	O
bound	O
on	O
the	O
error	O
of	O
the	O
asymptotic	O
λ-dependent	O
solution	O
of	O
linear	O
td	O
(	O
λ	O
)	O
is	O
due	O
to	O
tsitsiklis	O
and	O
van	O
roy	O
(	O
1997	O
)	O
.	O
12.3-5	O
truncated	B
td	O
methods	O
were	O
developed	O
by	O
cichosz	O
(	O
1995	O
)	O
and	O
van	O
seijen	O
(	O
2016	O
)	O
.	O
true	B
online	I
td	O
(	O
λ	O
)	O
and	O
the	O
other	O
ideas	O
presented	O
in	O
these	O
sections	O
are	O
primarily	O
due	O
to	O
work	O
of	O
van	O
seijen	O
(	O
van	O
seijen	O
and	O
sutton	O
,	O
2014	O
;	O
van	O
seijen	O
et	O
al.	O
,	O
2016	O
)	O
replacing	B
traces	O
are	O
due	O
to	O
singh	O
and	O
sutton	O
(	O
1996	O
)	O
.	O
12.6	O
the	O
material	O
in	O
this	O
section	O
is	O
from	O
van	O
hasselt	O
and	O
sutton	O
(	O
2015	O
)	O
.	O
12.7	O
sarsa	O
(	O
λ	O
)	O
with	O
accumulating	O
traces	O
was	O
ﬁrst	O
explored	O
as	O
a	O
control	B
method	O
by	O
rummery	O
and	O
niranjan	O
(	O
1994	O
;	O
rummery	O
,	O
1995	O
)	O
.	O
true	B
online	I
sarsa	O
(	O
λ	O
)	O
was	O
introduced	O
by	O
van	O
seijen	O
and	O
sutton	O
(	O
2014	O
)	O
.	O
the	O
algorithm	O
on	O
page	O
309	O
was	O
adapted	O
from	O
van	O
seijen	O
et	O
al	O
.	O
(	O
2016	O
)	O
.	O
the	O
mountain	O
car	O
results	O
were	O
made	O
for	O
this	O
text	O
,	O
except	O
for	O
figure	O
12.11	O
which	O
is	O
adapted	O
from	O
van	O
seijen	O
and	O
sutton	O
(	O
2014	O
)	O
.	O
12.8	O
perhaps	O
the	O
ﬁrst	O
published	O
discussion	O
of	O
variable	O
λ	O
was	O
by	O
watkins	O
(	O
1989	O
)	O
,	O
who	O
pointed	O
out	O
that	O
the	O
cutting	O
oﬀ	O
of	O
the	O
update	O
sequence	O
(	O
figure	O
12.12	O
)	O
in	O
his	O
q	O
(	O
λ	O
)	O
when	O
a	O
nongreedy	O
action	B
was	O
selected	O
could	O
be	O
implemented	O
by	O
temporarily	O
setting	O
λ	O
to	O
0	O
.	O
322	O
12.9	O
chapter	O
12	O
:	O
eligibility	B
traces	I
variable	O
λ	O
was	O
introduced	O
in	O
the	O
ﬁrst	O
edition	O
of	O
this	O
text	O
.	O
the	O
roots	O
of	O
variable	O
γ	O
are	O
in	O
the	O
work	O
on	O
options	B
(	O
sutton	O
,	O
precup	O
,	O
and	O
singh	O
,	O
1999	O
)	O
and	O
its	O
precursors	O
(	O
sutton	O
,	O
1995a	O
)	O
,	O
becoming	O
explicit	O
in	O
the	O
gq	O
(	O
λ	O
)	O
paper	O
(	O
maei	O
and	O
sutton	O
,	O
2010	O
)	O
,	O
which	O
also	O
introduced	O
some	O
of	O
these	O
recursive	O
forms	O
for	O
the	O
λ-returns	O
.	O
a	O
diﬀerent	O
notion	O
of	O
variable	O
λ	O
has	O
been	O
developed	O
by	O
yu	O
(	O
2012	O
)	O
.	O
oﬀ-policy	B
eligibility	O
traces	O
were	O
introduced	O
by	O
precup	O
et	O
al	O
.	O
(	O
2000	O
,	O
2001	O
)	O
,	O
then	O
further	O
developed	O
by	O
bertsekas	O
and	O
yu	O
(	O
2009	O
)	O
,	O
maei	O
(	O
2011	O
;	O
maei	O
and	O
sutton	O
,	O
2010	O
)	O
,	O
yu	O
(	O
2012	O
)	O
,	O
and	O
by	O
sutton	O
,	O
mahmood	O
,	O
precup	O
,	O
and	O
van	O
hasselt	O
(	O
2014	O
)	O
.	O
the	O
last	O
reference	O
in	O
particular	O
gives	O
a	O
powerful	O
forward	O
view	O
for	O
oﬀ-policy	O
td	O
methods	O
with	O
general	O
state-dependent	O
λ	O
and	O
γ.	O
the	O
presentation	O
here	O
seems	O
to	O
be	O
new	O
.	O
this	O
section	O
ends	O
with	O
an	O
elegant	O
expected	O
sarsa	O
(	O
λ	O
)	O
algorithm	O
.	O
although	O
it	O
is	O
a	O
natural	O
algorithm	O
,	O
to	O
our	O
knowledge	O
it	O
has	O
not	O
previously	O
been	O
described	O
or	O
tested	O
in	O
the	O
literature	O
.	O
12.10	O
watkins	O
’	O
s	O
q	O
(	O
λ	O
)	O
is	O
due	O
to	O
watkins	O
(	O
1989	O
)	O
.	O
the	O
tabular	O
,	O
episodic	O
,	O
oﬀ-line	B
ver-	O
sion	O
has	O
been	O
proven	O
convergent	O
by	O
munos	O
,	O
stepleton	O
,	O
harutyunyan	O
,	O
and	O
belle-	O
mare	O
(	O
2016	O
)	O
.	O
alternative	O
q	O
(	O
λ	O
)	O
algorithms	O
were	O
proposed	O
by	O
peng	O
and	O
williams	O
(	O
1994	O
,	O
1996	O
)	O
and	O
by	O
sutton	O
,	O
mahmood	O
,	O
precup	O
,	O
and	O
van	O
hasselt	O
(	O
2014	O
)	O
.	O
tree	O
backup	O
(	O
λ	O
)	O
is	O
due	O
to	O
precup	O
,	O
sutton	O
,	O
and	O
singh	O
(	O
2000	O
)	O
.	O
12.11	O
gtd	O
(	O
λ	O
)	O
is	O
due	O
to	O
maei	O
(	O
2011	O
)	O
.	O
gq	O
(	O
λ	O
)	O
is	O
due	O
to	O
maei	O
and	O
sutton	O
(	O
2010	O
)	O
.	O
htd	O
(	O
λ	O
)	O
is	O
due	O
to	O
white	O
and	O
white	O
(	O
2016	O
)	O
based	O
on	O
the	O
one-step	O
htd	O
al-	O
gorithm	O
introduced	O
by	O
hackman	O
(	O
2012	O
)	O
.	O
the	O
latest	O
developments	O
in	O
the	O
theory	O
of	O
gradient-td	O
methods	O
are	O
by	O
yu	O
(	O
2017	O
)	O
.	O
emphatic	O
td	O
(	O
λ	O
)	O
was	O
introduced	O
by	O
sutton	O
,	O
mahmood	O
,	O
and	O
white	O
(	O
2016	O
)	O
,	O
who	O
proved	O
its	O
stability	O
.	O
yu	O
(	O
2015	O
,	O
2016	O
)	O
proved	O
its	O
convergence	O
,	O
and	O
the	O
algorithm	O
was	O
developed	O
further	O
by	O
hallak	O
et	O
al	O
.	O
(	O
2015	O
,	O
2016	O
)	O
.	O
chapter	O
13	O
policy	B
gradient	I
methods	I
in	O
this	O
chapter	O
we	O
consider	O
something	O
new	O
.	O
so	O
far	O
in	O
this	O
book	O
almost	O
all	O
the	O
methods	O
have	O
been	O
action-value	B
methods	I
;	O
they	O
learned	O
the	O
values	O
of	O
actions	O
and	O
then	O
selected	O
actions	O
based	O
on	O
their	O
estimated	O
action	B
values1	O
;	O
their	O
policies	O
would	O
not	O
even	O
exist	O
with-	O
out	O
the	O
action-value	O
estimates	O
.	O
in	O
this	O
chapter	O
we	O
consider	O
methods	O
that	O
instead	O
learn	O
a	O
parameterized	O
policy	B
that	O
can	O
select	O
actions	O
without	O
consulting	O
a	O
value	B
function	I
.	O
a	O
value	B
function	I
may	O
still	O
be	O
used	O
to	O
learn	O
the	O
policy	B
parameter	O
,	O
but	O
is	O
not	O
required	O
for	O
action	O
selection	O
.	O
we	O
use	O
the	O
notation	O
θ	O
∈	O
rd	O
(	O
cid:48	O
)	O
for	O
the	O
policy	B
’	O
s	O
parameter	O
vector	O
.	O
thus	O
we	O
write	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
=	O
pr	O
{	O
at	O
=	O
a	O
|	O
st	O
=	O
s	O
,	O
θt	O
=	O
θ	O
}	O
for	O
the	O
probability	O
that	O
action	B
a	O
is	O
taken	O
at	O
time	O
t	O
given	O
that	O
the	O
environment	B
is	O
in	O
state	O
s	O
at	O
time	O
t	O
with	O
parameter	O
θ.	O
if	O
a	O
method	O
uses	O
a	O
learned	O
value	B
function	I
as	O
well	O
,	O
then	O
the	O
value	B
function	I
’	O
s	O
weight	O
vector	B
is	O
denoted	O
w	O
∈	O
rd	O
as	O
usual	O
,	O
as	O
in	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
.	O
in	O
this	O
chapter	O
we	O
consider	O
methods	O
for	O
learning	O
the	O
policy	B
parameter	O
based	O
on	O
the	O
gradient	B
of	O
some	O
performance	O
measure	O
j	O
(	O
θ	O
)	O
with	O
respect	O
to	O
the	O
policy	B
parameter	O
.	O
these	O
methods	O
seek	O
to	O
maximize	O
performance	O
,	O
so	O
their	O
updates	O
approximate	B
gradient	O
ascent	O
in	O
j	O
:	O
θt+1	O
=	O
θt	O
+	O
α	O
(	O
cid:92	O
)	O
∇j	O
(	O
θt	O
)	O
,	O
(	O
13.1	O
)	O
where	O
(	O
cid:92	O
)	O
∇j	O
(	O
θt	O
)	O
is	O
a	O
stochastic	O
estimate	O
whose	O
expectation	O
approximates	O
the	O
gradient	B
of	O
the	O
performance	O
measure	O
with	O
respect	O
to	O
its	O
argument	O
θt	O
.	O
all	O
methods	O
that	O
follow	O
this	O
general	O
schema	O
we	O
call	O
policy	B
gradient	I
methods	I
,	O
whether	O
or	O
not	O
they	O
also	O
learn	O
an	O
approximate	B
value	O
function	O
.	O
methods	O
that	O
learn	O
approximations	O
to	O
both	O
policy	B
and	O
value	B
functions	O
are	O
often	O
called	O
actor–critic	B
methods	O
,	O
where	O
‘	O
actor	O
’	O
is	O
a	O
reference	O
to	O
the	O
learned	O
policy	B
,	O
and	O
‘	O
critic	B
’	O
refers	O
to	O
the	O
learned	O
value	B
function	I
,	O
usually	O
a	O
state-value	O
function	O
.	O
first	O
we	O
treat	O
the	O
episodic	O
case	O
,	O
in	O
which	O
performance	O
is	O
deﬁned	O
as	O
the	O
value	B
of	O
the	O
start	O
state	B
under	O
the	O
parameterized	O
policy	B
,	O
before	O
going	O
on	O
to	O
consider	O
the	O
continuing	O
case	O
,	O
in	O
which	O
performance	O
is	O
deﬁned	O
as	O
the	O
average	O
reward	O
rate	O
,	O
as	O
in	O
section	O
10.3.	O
in	O
the	O
end	O
,	O
1the	O
lone	O
exception	O
is	O
the	O
gradient	B
bandit	O
algorithms	O
of	O
section	O
2.8.	O
in	O
fact	O
,	O
that	O
section	O
goes	O
through	O
many	O
of	O
the	O
same	O
steps	O
,	O
in	O
the	O
single-state	O
bandit	O
case	O
,	O
as	O
we	O
go	O
through	O
here	O
for	O
full	O
mdps	O
.	O
reviewing	O
that	O
section	O
would	O
be	O
good	O
preparation	O
for	O
fully	O
understanding	O
this	O
chapter	O
.	O
323	O
324	O
chapter	O
13	O
:	O
policy	B
gradient	I
methods	I
we	O
are	O
able	O
to	O
express	O
the	O
algorithms	O
for	O
both	O
cases	O
in	O
very	O
similar	O
terms	O
.	O
13.1	O
policy	B
approximation	I
and	O
its	O
advantages	O
in	O
policy	B
gradient	I
methods	I
,	O
the	O
policy	B
can	O
be	O
parameterized	O
in	O
any	O
way	O
,	O
as	O
long	O
as	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
is	O
diﬀerentiable	O
with	O
respect	O
to	O
its	O
parameters	O
,	O
that	O
is	O
,	O
as	O
long	O
as	O
∇π	O
(	O
a|s	O
,	O
θ	O
)	O
(	O
the	O
column	O
vector	B
of	O
partial	O
derivatives	O
of	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
with	O
respect	O
to	O
the	O
components	O
of	O
θ	O
)	O
exists	O
and	O
is	O
ﬁnite	O
for	O
all	O
s	O
∈	O
s	O
,	O
a	O
∈	O
a	O
(	O
s	O
)	O
,	O
and	O
θ	O
∈	O
rd	O
(	O
cid:48	O
)	O
.	O
in	O
practice	O
,	O
to	O
ensure	O
exploration	O
we	O
generally	O
require	O
that	O
the	O
policy	B
never	O
becomes	O
deterministic	O
(	O
i.e.	O
,	O
that	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
∈	O
(	O
0	O
,	O
1	O
)	O
,	O
for	O
all	O
s	O
,	O
a	O
,	O
θ	O
)	O
.	O
in	O
this	O
section	O
we	O
introduce	O
the	O
most	O
common	O
parameterization	O
for	O
discrete	O
action	B
spaces	O
and	O
point	O
out	O
the	O
advantages	O
it	O
oﬀers	O
over	O
action-value	B
methods	I
.	O
policy-based	O
methods	O
also	O
oﬀer	O
useful	O
ways	O
of	O
dealing	O
with	O
continuous	O
action	B
spaces	O
,	O
as	O
we	O
describe	O
later	O
in	O
section	O
13.7.	O
if	O
the	O
action	B
space	O
is	O
discrete	O
and	O
not	O
too	O
large	O
,	O
then	O
a	O
natural	O
and	O
common	O
kind	O
of	O
parameterization	O
is	O
to	O
form	O
parameterized	O
numerical	O
preferences	O
h	O
(	O
s	O
,	O
a	O
,	O
θ	O
)	O
∈	O
r	O
for	O
each	O
state–action	O
pair	O
.	O
the	O
actions	O
with	O
the	O
highest	O
preferences	O
in	O
each	O
state	B
are	O
given	O
the	O
highest	O
probabilities	O
of	O
being	O
selected	O
,	O
for	O
example	O
,	O
according	O
to	O
an	O
exponential	O
soft-max	B
distribution	O
:	O
(	O
13.2	O
)	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
.	O
=	O
eh	O
(	O
s	O
,	O
a	O
,	O
θ	O
)	O
(	O
cid:80	O
)	O
b	O
eh	O
(	O
s	O
,	O
b	O
,	O
θ	O
)	O
,	O
where	O
e	O
≈	O
2.71828	O
is	O
the	O
base	O
of	O
the	O
natural	O
logarithm	O
.	O
note	O
that	O
the	O
denominator	O
here	O
is	O
just	O
what	O
is	O
required	O
so	O
that	O
the	O
action	B
probabilities	O
in	O
each	O
state	B
sum	O
to	O
one	O
.	O
we	O
call	O
this	O
kind	O
of	O
policy	O
parameterization	O
soft-max	B
in	O
action	B
preferences	I
.	O
the	O
action	B
preferences	I
themselves	O
can	O
be	O
parameterized	O
arbitrarily	O
.	O
for	O
example	O
,	O
they	O
might	O
be	O
computed	O
by	O
a	O
deep	O
neural	O
network	O
,	O
where	O
θ	O
is	O
the	O
vector	B
of	O
all	O
the	O
connection	O
weights	O
of	O
the	O
network	O
(	O
as	O
in	O
the	O
alphago	O
system	O
described	O
in	O
section	O
16.6	O
)	O
.	O
or	O
the	O
preferences	O
could	O
simply	O
be	O
linear	O
in	O
features	O
,	O
h	O
(	O
s	O
,	O
a	O
,	O
θ	O
)	O
=	O
θ	O
(	O
cid:62	O
)	O
x	O
(	O
s	O
,	O
a	O
)	O
,	O
(	O
13.3	O
)	O
using	O
feature	O
vectors	O
x	O
(	O
s	O
,	O
a	O
)	O
∈	O
rd	O
(	O
cid:48	O
)	O
constructed	O
by	O
any	O
of	O
the	O
methods	O
described	O
in	O
chapter	O
9.	O
one	O
advantage	O
of	O
parameterizing	O
policies	O
according	O
to	O
the	O
soft-max	B
in	O
action	B
prefer-	O
ences	O
is	O
that	O
the	O
approximate	B
policy	O
can	O
approach	O
a	O
deterministic	O
policy	B
,	O
whereas	O
with	O
ε-greedy	O
action	B
selection	O
over	O
action	B
values	O
there	O
is	O
always	O
an	O
ε	O
probability	O
of	O
selecting	O
a	O
random	O
action	O
.	O
of	O
course	O
,	O
one	O
could	O
select	O
according	O
to	O
a	O
soft-max	B
distribution	O
based	O
on	O
action	B
values	O
,	O
but	O
this	O
alone	O
would	O
not	O
allow	O
the	O
policy	B
to	O
approach	O
a	O
determinis-	O
tic	O
policy	B
.	O
instead	O
,	O
the	O
action-value	O
estimates	O
would	O
converge	O
to	O
their	O
corresponding	O
true	O
values	O
,	O
which	O
would	O
diﬀer	O
by	O
a	O
ﬁnite	O
amount	O
,	O
translating	O
to	O
speciﬁc	O
probabilities	O
other	O
than	O
0	O
and	O
1.	O
if	O
the	O
soft-max	B
distribution	O
included	O
a	O
temperature	O
parameter	O
,	O
then	O
the	O
temperature	O
could	O
be	O
reduced	O
over	O
time	O
to	O
approach	O
determinism	O
,	O
but	O
in	O
practice	O
it	O
would	O
be	O
diﬃcult	O
to	O
choose	O
the	O
reduction	O
schedule	O
,	O
or	O
even	O
the	O
initial	O
temperature	O
,	O
without	O
more	O
prior	B
knowledge	I
of	O
the	O
true	O
action	O
values	O
than	O
we	O
would	O
like	O
to	O
assume	O
.	O
13.1.	O
policy	B
approximation	I
and	O
its	O
advantages	O
325	O
action	B
preferences	I
are	O
diﬀerent	O
because	O
they	O
do	O
not	O
approach	O
speciﬁc	O
values	O
;	O
instead	O
they	O
are	O
driven	O
to	O
produce	O
the	O
optimal	O
stochastic	O
policy	B
.	O
if	O
the	O
optimal	O
policy	O
is	O
deter-	O
ministic	O
,	O
then	O
the	O
preferences	O
of	O
the	O
optimal	O
actions	O
will	O
be	O
driven	O
inﬁnitely	O
higher	O
than	O
all	O
suboptimal	O
actions	O
(	O
if	O
permitted	O
by	O
the	O
parameterization	O
)	O
.	O
a	O
second	O
advantage	O
of	O
parameterizing	O
policies	O
according	O
to	O
the	O
soft-max	B
in	O
action	B
preferences	I
is	O
that	O
it	O
enables	O
the	O
selection	O
of	O
actions	O
with	O
arbitrary	O
probabilities	O
.	O
in	O
problems	O
with	O
signiﬁcant	O
function	B
approximation	I
,	O
the	O
best	O
approximate	B
policy	O
may	O
be	O
stochastic	O
.	O
for	O
example	O
,	O
in	O
card	O
games	O
with	O
imperfect	O
information	O
the	O
optimal	O
play	O
is	O
often	O
to	O
do	O
two	O
diﬀerent	O
things	O
with	O
speciﬁc	O
probabilities	O
,	O
such	O
as	O
when	O
bluﬃng	O
in	O
poker	O
.	O
action-value	B
methods	I
have	O
no	O
natural	O
way	O
of	O
ﬁnding	O
stochastic	O
optimal	O
policies	O
,	O
whereas	O
policy	B
approximating	O
methods	O
can	O
,	O
as	O
shown	O
in	O
example	O
13.1.	O
example	O
13.1	O
short	O
corridor	O
with	O
switched	O
actions	O
consider	O
the	O
small	O
corridor	O
gridworld	O
shown	O
inset	O
in	O
the	O
graph	O
below	O
.	O
the	O
reward	O
is	O
−1	O
per	O
step	O
,	O
as	O
usual	O
.	O
in	O
each	O
of	O
the	O
three	O
nonterminal	O
states	O
there	O
are	O
only	O
two	O
actions	O
,	O
right	O
and	O
left	O
.	O
these	O
actions	O
have	O
their	O
usual	O
consequences	O
in	O
the	O
ﬁrst	O
and	O
third	O
states	O
(	O
left	O
causes	O
no	O
movement	O
in	O
the	O
ﬁrst	O
state	B
)	O
,	O
but	O
in	O
the	O
second	O
state	B
they	O
are	O
reversed	O
,	O
so	O
that	O
right	O
moves	O
to	O
the	O
left	O
and	O
left	O
moves	O
to	O
the	O
right	O
.	O
the	O
problem	O
is	O
diﬃcult	O
because	O
all	O
the	O
states	O
appear	O
identical	O
under	O
the	O
function	B
approximation	I
.	O
in	O
particular	O
,	O
we	O
deﬁne	O
x	O
(	O
s	O
,	O
right	O
)	O
=	O
[	O
1	O
,	O
0	O
]	O
(	O
cid:62	O
)	O
and	O
x	O
(	O
s	O
,	O
left	O
)	O
=	O
[	O
0	O
,	O
1	O
]	O
(	O
cid:62	O
)	O
,	O
for	O
all	O
s.	O
an	O
action-value	O
method	O
with	O
ε-greedy	O
action	B
selection	O
is	O
forced	O
to	O
choose	O
between	O
just	O
two	O
policies	O
:	O
choosing	O
right	O
with	O
high	O
probability	O
1	O
−	O
ε/2	O
on	O
all	O
steps	O
or	O
choosing	O
left	O
with	O
the	O
same	O
high	O
probability	O
on	O
all	O
time	O
steps	O
.	O
if	O
ε	O
=	O
0.1	O
,	O
then	O
these	O
two	O
policies	O
achieve	O
a	O
value	B
(	O
at	O
the	O
start	O
state	B
)	O
of	O
less	O
than	O
−44	O
and	O
−82	O
,	O
respectively	O
,	O
as	O
shown	O
in	O
the	O
graph	O
.	O
a	O
method	O
can	O
do	O
signiﬁcantly	O
better	O
if	O
it	O
can	O
learn	O
a	O
speciﬁc	O
probability	O
with	O
which	O
to	O
select	O
right	O
.	O
the	O
best	O
probability	O
is	O
about	O
0.59	O
,	O
which	O
achieves	O
a	O
value	B
of	O
about	O
−11.6	O
.	O
probability	O
of	O
right	O
action-11.60.10.2-20-40-60-80-1000.30.400.60.70.80.90.51-greedy	O
left	O
-greedy	O
right	O
optimalstochasticpolicy	O
j	O
(	O
✓	O
)	O
=v⇡✓	O
(	O
s	O
)	O
gs	O
326	O
chapter	O
13	O
:	O
policy	B
gradient	I
methods	I
perhaps	O
the	O
simplest	O
advantage	O
that	O
policy	B
parameterization	O
may	O
have	O
over	O
action-	O
value	B
parameterization	O
is	O
that	O
the	O
policy	B
may	O
be	O
a	O
simpler	O
function	O
to	O
approximate	B
.	O
problems	O
vary	O
in	O
the	O
complexity	O
of	O
their	O
policies	O
and	O
action-value	O
functions	O
.	O
for	O
some	O
,	O
the	O
action-value	B
function	I
is	O
simpler	O
and	O
thus	O
easier	O
to	O
approximate	B
.	O
for	O
others	O
,	O
the	O
policy	B
is	O
simpler	O
.	O
in	O
the	O
latter	O
case	O
a	O
policy-based	O
method	O
will	O
typically	O
learn	O
faster	O
and	O
yield	O
a	O
superior	O
asymptotic	O
policy	B
(	O
as	O
seems	O
to	O
be	O
the	O
case	O
with	O
tetris	O
;	O
see	O
s¸im¸sek	O
,	O
alg´orta	O
,	O
and	O
kothiyal	O
,	O
2016	O
)	O
.	O
finally	O
,	O
we	O
note	O
that	O
the	O
choice	O
of	O
policy	O
parameterization	O
is	O
sometimes	O
a	O
good	O
way	O
of	O
injecting	O
prior	B
knowledge	I
about	O
the	O
desired	O
form	O
of	O
the	O
policy	B
into	O
the	O
reinforcement	B
learning	I
system	O
.	O
this	O
is	O
often	O
the	O
most	O
important	O
reason	O
for	O
using	O
a	O
policy-based	O
learning	O
method	O
.	O
exercise	O
13.1	O
use	O
your	O
knowledge	O
of	O
the	O
gridworld	O
and	O
its	O
dynamics	O
to	O
determine	O
an	O
exact	O
symbolic	O
expression	O
for	O
the	O
optimal	O
probability	O
of	O
selecting	O
the	O
right	O
action	B
in	O
(	O
cid:3	O
)	O
example	O
13.1	O
.	O
13.2	O
the	O
policy	B
gradient	I
theorem	I
in	O
addition	O
to	O
the	O
practical	O
advantages	B
of	I
policy	O
parameterization	O
over	O
ε-greedy	O
action	O
selection	O
,	O
there	O
is	O
also	O
an	O
important	O
theoretical	O
advantage	O
.	O
with	O
continuous	O
policy	B
pa-	O
rameterization	O
the	O
action	B
probabilities	O
change	O
smoothly	O
as	O
a	O
function	O
of	O
the	O
learned	O
pa-	O
rameter	O
,	O
whereas	O
in	O
ε-greedy	O
selection	O
the	O
action	B
probabilities	O
may	O
change	O
dramatically	O
for	O
an	O
arbitrarily	O
small	O
change	O
in	O
the	O
estimated	O
action	B
values	O
,	O
if	O
that	O
change	O
results	O
in	O
a	O
diﬀerent	O
action	B
having	O
the	O
maximal	O
value	B
.	O
largely	O
because	O
of	O
this	O
stronger	O
convergence	O
guarantees	O
are	O
available	O
for	O
policy-gradient	O
methods	O
than	O
for	O
action-value	O
methods	O
.	O
in	O
particular	O
,	O
it	O
is	O
the	O
continuity	O
of	O
the	O
policy	B
dependence	O
on	O
the	O
parameters	O
that	O
enables	O
policy-gradient	O
methods	O
to	O
approximate	B
gradient	O
ascent	O
(	O
13.1	O
)	O
.	O
the	O
episodic	O
and	O
continuing	O
cases	O
deﬁne	O
the	O
performance	O
measure	O
,	O
j	O
(	O
θ	O
)	O
,	O
diﬀerently	O
and	O
thus	O
have	O
to	O
be	O
treated	O
separately	O
to	O
some	O
extent	O
.	O
nevertheless	O
,	O
we	O
will	O
try	O
to	O
present	O
both	O
cases	O
uniformly	O
,	O
and	O
we	O
develop	O
a	O
notation	O
so	O
that	O
the	O
major	O
theoretical	O
results	O
can	O
be	O
described	O
with	O
a	O
single	O
set	O
of	O
equations	O
.	O
in	O
this	O
section	O
we	O
treat	O
the	O
episodic	O
case	O
,	O
for	O
which	O
we	O
deﬁne	O
the	O
performance	O
measure	O
as	O
the	O
value	B
of	O
the	O
start	O
state	B
of	O
the	O
episode	O
.	O
we	O
can	O
simplify	O
the	O
notation	O
without	O
losing	O
any	O
meaningful	O
generality	O
by	O
assuming	O
that	O
every	O
episode	O
starts	O
in	O
some	O
particular	O
(	O
non-	O
random	O
)	O
state	B
s0	O
.	O
then	O
,	O
in	O
the	O
episodic	O
case	O
we	O
deﬁne	O
performance	O
as	O
j	O
(	O
θ	O
)	O
.	O
=	O
vπθ	O
(	O
s0	O
)	O
,	O
(	O
13.4	O
)	O
where	O
vπθ	O
is	O
the	O
true	O
value	O
function	O
for	O
πθ	O
,	O
the	O
policy	B
determined	O
by	O
θ.	O
from	O
here	O
on	O
in	O
our	O
discussion	O
we	O
will	O
assume	O
no	O
discounting	B
(	O
γ	O
=	O
1	O
)	O
for	O
the	O
episodic	O
case	O
,	O
although	O
for	O
completeness	O
we	O
do	O
include	O
the	O
possibility	O
of	O
discounting	O
in	O
the	O
boxed	O
algorithms	O
.	O
with	B
function	I
approximation	I
,	O
it	O
may	O
seem	O
challenging	O
to	O
change	O
the	O
policy	B
parameter	O
in	O
a	O
way	O
that	O
ensures	O
improvement	O
.	O
the	O
problem	O
is	O
that	O
performance	O
depends	O
on	O
both	O
the	O
action	B
selections	O
and	O
the	O
distribution	O
of	O
states	O
in	O
which	O
those	O
selections	O
are	O
made	O
,	O
and	O
that	O
both	O
of	O
these	O
are	O
aﬀected	O
by	O
the	O
policy	B
parameter	O
.	O
given	O
a	O
state	B
,	O
the	O
eﬀect	O
of	O
13.2.	O
the	O
policy	B
gradient	I
theorem	I
327	O
proof	B
of	O
the	O
policy	B
gradient	I
theorem	I
(	O
episodic	O
case	O
)	O
with	O
just	O
elementary	O
calculus	O
and	O
re-arranging	O
of	O
terms	O
,	O
we	O
can	O
prove	O
the	O
policy	B
gradient	I
theorem	I
from	O
ﬁrst	O
principles	O
.	O
to	O
keep	O
the	O
notation	O
simple	O
,	O
we	O
leave	O
it	O
implicit	O
in	O
all	O
cases	O
that	O
π	O
is	O
a	O
function	O
of	O
θ	O
,	O
and	O
all	O
gradients	O
are	O
also	O
implicitly	O
with	O
respect	O
to	O
θ.	O
first	O
note	O
that	O
the	O
gradient	B
of	O
the	O
state-value	O
function	O
can	O
be	O
written	O
in	O
terms	O
of	O
the	O
action-value	B
function	I
as	O
∇vπ	O
(	O
s	O
)	O
=	O
∇	O
(	O
cid:34	O
)	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
(	O
cid:35	O
)	O
,	O
for	O
all	O
s	O
∈	O
s	O
(	O
exercise	O
3.16	O
)	O
=	O
(	O
cid:88	O
)	O
a	O
(	O
cid:104	O
)	O
∇π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
+	O
π	O
(	O
a|s	O
)	O
∇qπ	O
(	O
s	O
,	O
a	O
)	O
(	O
cid:105	O
)	O
(	O
product	O
rule	O
of	O
calculus	O
)	O
=	O
(	O
cid:88	O
)	O
a	O
(	O
cid:104	O
)	O
∇π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
+	O
π	O
(	O
a|s	O
)	O
∇	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
=	O
(	O
cid:88	O
)	O
a	O
(	O
cid:104	O
)	O
∇π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
+	O
π	O
(	O
a|s	O
)	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
=	O
(	O
cid:88	O
)	O
a	O
(	O
cid:104	O
)	O
∇π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
+	O
π	O
(	O
a|s	O
)	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:0	O
)	O
r	O
+	O
vπ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:1	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
a	O
)	O
∇vπ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
a	O
)	O
(	O
exercise	O
3.17	O
and	O
equation	O
3.2	O
)	O
(	O
unrolling	O
)	O
(	O
eq	O
.	O
3.4	O
)	O
(	O
cid:88	O
)	O
a	O
(	O
cid:48	O
)	O
(	O
cid:2	O
)	O
∇π	O
(	O
a	O
(	O
cid:48	O
)	O
|s	O
(	O
cid:48	O
)	O
)	O
qπ	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
a	O
(	O
cid:48	O
)	O
)	O
+	O
π	O
(	O
a	O
(	O
cid:48	O
)	O
|s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
∞	O
(	O
cid:88	O
)	O
k=0	O
pr	O
(	O
s→	O
x	O
,	O
k	O
,	O
π	O
)	O
(	O
cid:88	O
)	O
a	O
∇π	O
(	O
a|x	O
)	O
qπ	O
(	O
x	O
,	O
a	O
)	O
,	O
=	O
(	O
cid:88	O
)	O
x∈s	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
|s	O
(	O
cid:48	O
)	O
,	O
a	O
(	O
cid:48	O
)	O
)	O
∇vπ	O
(	O
s	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
)	O
(	O
cid:3	O
)	O
(	O
cid:105	O
)	O
after	O
repeated	O
unrolling	O
,	O
where	O
pr	O
(	O
s	O
→	O
x	O
,	O
k	O
,	O
π	O
)	O
is	O
the	O
probability	O
of	O
transitioning	O
from	O
state	B
s	O
to	O
state	B
x	O
in	O
k	O
steps	O
under	O
policy	B
π.	O
it	O
is	O
then	O
immediate	O
that	O
∇j	O
(	O
θ	O
)	O
=	O
∇vπ	O
(	O
s0	O
)	O
∇π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
=	O
(	O
cid:88	O
)	O
s	O
(	O
cid:32	O
)	O
∞	O
(	O
cid:88	O
)	O
k=0	O
η	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
a	O
=	O
(	O
cid:88	O
)	O
s	O
=	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
η	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:88	O
)	O
s	O
η	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:88	O
)	O
s	O
=	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
µ	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
a	O
∝	O
(	O
cid:88	O
)	O
s	O
∇π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
pr	O
(	O
s0→	O
s	O
,	O
k	O
,	O
π	O
)	O
(	O
cid:33	O
)	O
(	O
cid:88	O
)	O
a	O
(	O
cid:80	O
)	O
s	O
(	O
cid:48	O
)	O
η	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:88	O
)	O
a	O
µ	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
a	O
η	O
(	O
s	O
)	O
∇π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
∇π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
∇π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
(	O
box	O
page	O
199	O
)	O
(	O
eq	O
.	O
9.3	O
)	O
(	O
q.e.d	O
.	O
)	O
328	O
chapter	O
13	O
:	O
policy	B
gradient	I
methods	I
the	O
policy	B
parameter	O
on	O
the	O
actions	O
,	O
and	O
thus	O
on	O
reward	O
,	O
can	O
be	O
computed	O
in	O
a	O
relatively	O
straightforward	O
way	O
from	O
knowledge	O
of	O
the	O
parameterization	O
.	O
but	O
the	O
eﬀect	O
of	O
the	O
policy	B
on	O
the	O
state	B
distribution	O
is	O
a	O
function	O
of	O
the	O
environment	B
and	O
is	O
typically	O
unknown	O
.	O
how	O
can	O
we	O
estimate	O
the	O
performance	O
gradient	B
with	O
respect	O
to	O
the	O
policy	B
parameter	O
when	O
the	O
gradient	B
depends	O
on	O
the	O
unknown	O
eﬀect	O
of	O
policy	O
changes	O
on	O
the	O
state	B
distribution	O
?	O
fortunately	O
,	O
there	O
is	O
an	O
excellent	O
theoretical	O
answer	O
to	O
this	O
challenge	O
in	O
the	O
form	O
of	O
the	O
policy	B
gradient	I
theorem	I
,	O
which	O
provides	O
an	O
analytic	O
expression	O
for	O
the	O
gradient	B
of	O
performance	O
with	O
respect	O
to	O
the	O
policy	B
parameter	O
(	O
which	O
is	O
what	O
we	O
need	O
to	O
approximate	B
for	O
gradient	B
ascent	O
(	O
13.1	O
)	O
)	O
that	O
does	O
not	O
involve	O
the	O
derivative	O
of	O
the	O
state	B
distribution	O
.	O
the	O
policy	B
gradient	I
theorem	I
for	O
the	O
episodic	O
case	O
establishes	O
that	O
∇j	O
(	O
θ	O
)	O
∝	O
(	O
cid:88	O
)	O
s	O
µ	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
a	O
qπ	O
(	O
s	O
,	O
a	O
)	O
∇π	O
(	O
a|s	O
,	O
θ	O
)	O
,	O
(	O
13.5	O
)	O
where	O
the	O
gradients	O
are	O
column	O
vectors	O
of	O
partial	O
derivatives	O
with	O
respect	O
to	O
the	O
compo-	O
nents	O
of	O
θ	O
,	O
and	O
π	O
denotes	O
the	O
policy	B
corresponding	O
to	O
parameter	O
vector	O
θ.	O
the	O
symbol	O
∝	O
here	O
means	O
“	O
proportional	O
to	O
”	O
.	O
in	O
the	O
episodic	O
case	O
,	O
the	O
constant	O
of	O
proportionality	O
is	O
the	O
average	O
length	O
of	O
an	O
episode	O
,	O
and	O
in	O
the	O
continuing	O
case	O
it	O
is	O
1	O
,	O
so	O
that	O
the	O
rela-	O
tionship	O
is	O
actually	O
an	O
equality	O
.	O
the	O
distribution	O
µ	O
here	O
(	O
as	O
in	O
chapters	O
9	O
and	O
10	O
)	O
is	O
the	O
on-policy	B
distribution	I
under	O
π	O
(	O
see	O
page	O
199	O
)	O
.	O
the	O
policy	B
gradient	I
theorem	I
is	O
proved	O
for	O
the	O
episodic	O
case	O
in	O
the	O
box	O
on	O
the	O
previous	O
page	O
.	O
13.3	O
reinforce	O
:	O
monte	O
carlo	O
policy	O
gradient	O
we	O
are	O
now	O
ready	O
for	O
our	O
ﬁrst	O
policy-gradient	O
learning	O
algorithm	O
.	O
recall	O
our	O
overall	O
strategy	O
of	O
stochastic	O
gradient	B
ascent	O
(	O
13.1	O
)	O
,	O
which	O
requires	O
a	O
way	O
to	O
obtain	O
samples	O
such	O
that	O
the	O
expectation	O
of	O
the	O
sample	O
gradient	O
is	O
proportional	O
to	O
the	O
actual	O
gradient	B
of	O
the	O
performance	O
measure	O
as	O
a	O
function	O
of	O
the	O
parameter	O
.	O
the	O
sample	O
gradients	O
need	O
only	O
be	O
proportional	O
to	O
the	O
gradient	B
because	O
any	O
constant	O
of	O
proportionality	O
can	O
be	O
absorbed	O
into	O
the	O
step	O
size	O
α	O
,	O
which	O
is	O
otherwise	O
arbitrary	O
.	O
the	O
policy	B
gradient	I
theorem	I
gives	O
an	O
exact	O
expression	O
proportional	O
to	O
the	O
gradient	B
;	O
all	O
that	O
is	O
needed	O
is	O
some	O
way	O
of	O
sampling	O
whose	O
expectation	O
equals	O
or	O
approximates	O
this	O
expression	O
.	O
notice	O
that	O
the	O
right-hand	O
side	O
of	O
the	O
policy	B
gradient	I
theorem	I
is	O
a	O
sum	O
over	O
states	O
weighted	O
by	O
how	O
often	O
the	O
states	O
occur	O
under	O
the	O
target	B
policy	O
π	O
;	O
if	O
π	O
is	O
followed	O
,	O
then	O
states	O
will	O
be	O
encountered	O
in	O
these	O
proportions	O
.	O
thus	O
qπ	O
(	O
s	O
,	O
a	O
)	O
∇π	O
(	O
a|s	O
,	O
θ	O
)	O
∇j	O
(	O
θ	O
)	O
∝	O
(	O
cid:88	O
)	O
s	O
µ	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
a	O
qπ	O
(	O
st	O
,	O
a	O
)	O
∇π	O
(	O
a|st	O
,	O
θ	O
)	O
(	O
cid:35	O
)	O
.	O
=	O
eπ	O
(	O
cid:34	O
)	O
(	O
cid:88	O
)	O
a	O
(	O
13.5	O
)	O
this	O
is	O
good	O
progress	O
,	O
and	O
we	O
would	O
like	O
to	O
carry	O
it	O
further	O
and	O
handle	O
the	O
action	B
in	O
the	O
same	O
way—replacing	O
a	O
with	O
the	O
sample	O
action	O
at	O
.	O
the	O
remaining	O
part	O
of	O
the	O
expectation	O
above	O
is	O
a	O
sum	O
over	O
actions	O
;	O
if	O
only	O
each	O
term	O
were	O
weighted	O
by	O
the	O
probability	O
of	O
selecting	O
the	O
actions	O
,	O
that	O
is	O
,	O
according	O
to	O
π	O
(	O
a|st	O
,	O
θ	O
)	O
,	O
then	O
the	O
replacement	O
could	O
be	O
done	O
.	O
we	O
13.3.	O
reinforce	O
:	O
monte	O
carlo	O
policy	O
gradient	O
329	O
can	O
arrange	O
for	O
this	O
by	O
multiplying	O
and	O
dividing	O
by	O
this	O
probability	O
.	O
continuing	O
from	O
the	O
previous	O
equation	O
,	O
this	O
gives	O
∇j	O
(	O
θ	O
)	O
=	O
eπ	O
(	O
cid:34	O
)	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
a|st	O
,	O
θ	O
)	O
(	O
cid:35	O
)	O
π	O
(	O
a|st	O
,	O
θ	O
)	O
qπ	O
(	O
st	O
,	O
a	O
)	O
∇π	O
(	O
a|st	O
,	O
θ	O
)	O
π	O
(	O
at|st	O
,	O
θ	O
)	O
(	O
cid:21	O
)	O
=	O
eπ	O
(	O
cid:20	O
)	O
qπ	O
(	O
st	O
,	O
at	O
)	O
∇π	O
(	O
at|st	O
,	O
θ	O
)	O
=	O
eπ	O
(	O
cid:20	O
)	O
gt	O
∇π	O
(	O
at|st	O
,	O
θ	O
)	O
π	O
(	O
at|st	O
,	O
θ	O
)	O
(	O
cid:21	O
)	O
,	O
(	O
replacing	B
a	O
by	O
the	O
sample	O
at	O
∼	O
π	O
)	O
(	O
because	O
eπ	O
[	O
gt|st	O
,	O
at	O
]	O
=	O
qπ	O
(	O
st	O
,	O
at	O
)	O
)	O
where	O
gt	O
is	O
the	O
return	B
as	O
usual	O
.	O
the	O
ﬁnal	O
expression	O
in	O
the	O
brackets	O
is	O
exactly	O
what	O
is	O
needed	O
,	O
a	O
quantity	O
that	O
can	O
be	O
sampled	O
on	O
each	O
time	O
step	O
whose	O
expectation	O
is	O
equal	O
to	O
the	O
gradient	B
.	O
using	O
this	O
sample	O
to	O
instantiate	O
our	O
generic	O
stochastic	O
gradient	O
ascent	O
algorithm	O
(	O
13.1	O
)	O
,	O
yields	O
the	O
update	O
θt+1	O
=	O
θt	O
+	O
αgt	O
∇π	O
(	O
at|st	O
,	O
θt	O
)	O
.	O
π	O
(	O
at|st	O
,	O
θt	O
)	O
.	O
(	O
13.6	O
)	O
we	O
call	O
this	O
algorithm	O
reinforce	O
(	O
after	O
williams	O
,	O
1992	O
)	O
.	O
its	O
update	O
has	O
an	O
intuitive	O
appeal	O
.	O
each	O
increment	O
is	O
proportional	O
to	O
the	O
product	O
of	O
a	O
return	B
gt	O
and	O
a	O
vector	B
,	O
the	O
gradient	B
of	O
the	O
probability	O
of	O
taking	O
the	O
action	B
actually	O
taken	O
divided	O
by	O
the	O
probability	O
of	O
taking	O
that	O
action	B
.	O
the	O
vector	B
is	O
the	O
direction	O
in	O
parameter	O
space	O
that	O
most	O
increases	O
the	O
probability	O
of	O
repeating	O
the	O
action	B
at	O
on	O
future	O
visits	O
to	O
state	B
st.	O
the	O
update	O
increases	O
the	O
parameter	O
vector	O
in	O
this	O
direction	O
proportional	O
to	O
the	O
return	B
,	O
and	O
inversely	O
proportional	O
to	O
the	O
action	B
probability	O
.	O
the	O
former	O
makes	O
sense	O
because	O
it	O
causes	O
the	O
parameter	O
to	O
move	O
most	O
in	O
the	O
directions	O
that	O
favor	O
actions	O
that	O
yield	O
the	O
highest	O
return	B
.	O
the	O
latter	O
makes	O
sense	O
because	O
otherwise	O
actions	O
that	O
are	O
selected	O
frequently	O
are	O
at	O
an	O
advantage	O
(	O
the	O
updates	O
will	O
be	O
more	O
often	O
in	O
their	O
direction	O
)	O
and	O
might	O
win	O
out	O
even	O
if	O
they	O
do	O
not	O
yield	O
the	O
highest	O
return	B
.	O
note	O
that	O
reinforce	O
uses	O
the	O
complete	O
return	B
from	O
time	O
t	O
,	O
which	O
includes	O
all	O
future	O
rewards	O
up	O
until	O
the	O
end	O
of	O
the	O
episode	O
.	O
in	O
this	O
sense	O
reinforce	O
is	O
a	O
monte	O
carlo	O
algorithm	O
and	O
is	O
well	O
deﬁned	O
only	O
for	O
the	O
episodic	O
case	O
with	O
all	O
updates	O
made	O
in	O
retrospect	O
after	O
the	O
episode	O
is	O
completed	O
(	O
like	O
the	O
monte	O
carlo	O
algorithms	O
in	O
chapter	O
5	O
)	O
.	O
this	O
is	O
shown	O
explicitly	O
in	O
the	O
boxed	O
on	O
the	O
next	O
page	O
.	O
notice	O
that	O
the	O
update	O
in	O
the	O
last	O
line	O
of	O
pseudocode	O
appears	O
rather	O
diﬀerent	O
from	O
the	O
reinforce	O
update	O
rule	O
(	O
13.6	O
)	O
.	O
one	O
diﬀerence	O
is	O
that	O
the	O
pseudocode	O
uses	O
the	O
compact	O
expression	O
∇	O
ln	O
π	O
(	O
at|st	O
,	O
θt	O
)	O
for	O
the	O
fractional	O
vector	B
∇π	O
(	O
at|st	O
,	O
θt	O
)	O
in	O
(	O
13.6	O
)	O
.	O
that	O
these	O
π	O
(	O
at|st	O
,	O
θt	O
)	O
two	O
expressions	O
for	O
the	O
vector	B
are	O
equivalent	O
follows	O
from	O
the	O
identity	O
∇	O
ln	O
x	O
=	O
∇x	O
x	O
.	O
this	O
vector	B
has	O
been	O
given	O
several	O
names	O
and	O
notations	O
in	O
the	O
literature	O
;	O
we	O
will	O
refer	O
to	O
it	O
simply	O
as	O
the	O
eligibility	O
vector	O
.	O
note	O
that	O
it	O
is	O
the	O
only	O
place	O
that	O
the	O
policy	B
parameterization	O
appears	O
in	O
the	O
algorithm	O
.	O
330	O
chapter	O
13	O
:	O
policy	B
gradient	I
methods	I
reinforce	O
:	O
monte-carlo	O
policy-gradient	O
control	B
(	O
episodic	O
)	O
for	O
π∗	O
input	O
:	O
a	O
diﬀerentiable	O
policy	B
parameterization	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
algorithm	O
parameter	O
:	O
step	O
size	O
α	O
>	O
0	O
initialize	O
policy	B
parameter	O
θ	O
∈	O
rd	O
(	O
cid:48	O
)	O
(	O
e.g.	O
,	O
to	O
0	O
)	O
loop	O
forever	O
(	O
for	O
each	O
episode	O
)	O
:	O
generate	O
an	O
episode	O
s0	O
,	O
a0	O
,	O
r1	O
,	O
.	O
.	O
.	O
,	O
st−1	O
,	O
at−1	O
,	O
rt	O
,	O
following	O
π	O
(	O
·|·	O
,	O
θ	O
)	O
loop	O
for	O
each	O
step	O
of	O
the	O
episode	O
t	O
=	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
−	O
1	O
:	O
(	O
gt	O
)	O
g	O
←	O
(	O
cid:80	O
)	O
t	O
k=t+1	O
rk	O
θ	O
←	O
θ	O
+	O
αγt	O
g∇	O
ln	O
π	O
(	O
at|st	O
,	O
θ	O
)	O
the	O
second	O
diﬀerence	O
between	O
the	O
pseudocode	O
update	O
and	O
the	O
reinforce	O
update	O
equation	O
(	O
13.6	O
)	O
is	O
that	O
the	O
former	O
includes	O
a	O
factor	O
of	O
γt	O
.	O
this	O
is	O
because	O
,	O
as	O
mentioned	O
earlier	O
,	O
in	O
the	O
text	O
we	O
are	O
treating	O
the	O
non-discounted	O
case	O
(	O
γ	O
=	O
1	O
)	O
while	O
in	O
the	O
boxed	O
algorithms	O
we	O
are	O
giving	O
the	O
algorithms	O
for	O
the	O
general	O
discounted	O
case	O
.	O
all	O
of	O
the	O
ideas	O
go	O
through	O
in	O
the	O
discounted	O
case	O
with	O
appropriate	O
adjustments	O
(	O
including	O
to	O
the	O
box	O
on	O
page	O
199	O
)	O
but	O
involve	O
additional	O
complexity	O
that	O
distracts	O
from	O
the	O
main	O
ideas	O
.	O
∗exercise	O
13.2	O
generalize	O
the	O
box	O
on	O
page	O
199	O
,	O
the	O
policy	B
gradient	I
theorem	I
(	O
13.5	O
)	O
,	O
the	O
proof	B
of	O
the	O
policy	B
gradient	I
theorem	I
(	O
page	O
327	O
)	O
,	O
and	O
the	O
steps	O
leading	O
to	O
the	O
rein-	O
force	O
update	O
equation	O
(	O
13.6	O
)	O
,	O
so	O
that	O
(	O
13.6	O
)	O
ends	O
up	O
with	O
a	O
factor	O
of	O
γt	O
and	O
thus	O
(	O
cid:3	O
)	O
aligns	O
with	O
the	O
general	O
algorithm	O
given	O
in	O
the	O
pseudocode	O
.	O
figure	O
13.1	O
shows	O
the	O
performance	O
of	O
reinforce	O
,	O
averaged	O
over	O
100	O
runs	O
,	O
on	O
the	O
short-corridor	O
gridworld	O
from	O
example	O
13.1.	O
figure	O
13.1	O
:	O
reinforce	O
on	O
the	O
short-corridor	O
gridworld	O
(	O
example	O
13.1	O
)	O
.	O
with	O
a	O
good	O
step	O
size	O
,	O
the	O
total	O
reward	O
per	O
episode	O
approaches	O
the	O
optimal	O
value	O
of	O
the	O
start	O
state	B
.	O
↵=2 13↵=2 12episode10008006004002001-80-90-60-40-20-10total	O
rewardon	O
episodeg0v⇤	O
(	O
s0	O
)	O
↵=2 14	O
13.4.	O
reinforce	O
with	B
baseline	I
331	O
as	O
a	O
stochastic	O
gradient	O
method	O
,	O
reinforce	O
has	O
good	O
theoretical	O
convergence	O
prop-	O
erties	O
.	O
by	O
construction	O
,	O
the	O
expected	B
update	I
over	O
an	O
episode	O
is	O
in	O
the	O
same	O
direction	O
as	O
the	O
performance	O
gradient	B
.	O
this	O
assures	O
an	O
improvement	O
in	O
expected	O
performance	O
for	O
suﬃciently	O
small	O
α	O
,	O
and	O
convergence	O
to	O
a	O
local	O
optimum	O
under	O
standard	O
stochastic	O
approximation	O
conditions	O
for	O
decreasing	O
α.	O
however	O
,	O
as	O
a	O
monte	O
carlo	O
method	O
rein-	O
force	O
may	O
be	O
of	O
high	O
variance	O
and	O
thus	O
produce	O
slow	O
learning	O
.	O
exercise	O
13.3	O
in	O
section	O
13.1	O
we	O
considered	O
policy	B
parameterizations	O
using	O
the	O
soft-max	B
in	O
action	B
preferences	I
(	O
13.2	O
)	O
with	O
linear	O
action	B
preferences	I
(	O
13.3	O
)	O
.	O
for	O
this	O
parameteriza-	O
tion	B
,	O
prove	O
that	O
the	O
eligibility	O
vector	O
is	O
∇	O
ln	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
=	O
x	O
(	O
s	O
,	O
a	O
)	O
−	O
(	O
cid:88	O
)	O
b	O
π	O
(	O
b|s	O
,	O
θ	O
)	O
x	O
(	O
s	O
,	O
b	O
)	O
,	O
using	O
the	O
deﬁnitions	O
and	O
elementary	O
calculus	O
.	O
13.4	O
reinforce	O
with	B
baseline	I
(	O
13.7	O
)	O
(	O
cid:3	O
)	O
the	O
policy	B
gradient	I
theorem	I
(	O
13.5	O
)	O
can	O
be	O
generalized	O
to	O
include	O
a	O
comparison	O
of	O
the	O
action	B
value	O
to	O
an	O
arbitrary	O
baseline	B
b	O
(	O
s	O
)	O
:	O
∇j	O
(	O
θ	O
)	O
∝	O
(	O
cid:88	O
)	O
s	O
µ	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
a	O
(	O
cid:16	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
−	O
b	O
(	O
s	O
)	O
(	O
cid:17	O
)	O
∇π	O
(	O
a|s	O
,	O
θ	O
)	O
.	O
(	O
13.8	O
)	O
the	O
baseline	B
can	O
be	O
any	O
function	O
,	O
even	O
a	O
random	O
variable	O
,	O
as	O
long	O
as	O
it	O
does	O
not	O
vary	O
with	O
a	O
;	O
the	O
equation	O
remains	O
valid	O
because	O
the	O
subtracted	O
quantity	O
is	O
zero	O
:	O
(	O
cid:88	O
)	O
a	O
b	O
(	O
s	O
)	O
∇π	O
(	O
a|s	O
,	O
θ	O
)	O
=	O
b	O
(	O
s	O
)	O
∇	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
=	O
b	O
(	O
s	O
)	O
∇1	O
=	O
0.	O
the	O
policy	B
gradient	I
theorem	I
with	O
baseline	B
(	O
13.8	O
)	O
can	O
be	O
used	O
to	O
derive	O
an	O
update	O
rule	O
using	O
similar	O
steps	O
as	O
in	O
the	O
previous	O
section	O
.	O
the	O
update	O
rule	O
that	O
we	O
end	O
up	O
with	O
is	O
a	O
new	O
version	O
of	O
reinforce	O
that	O
includes	O
a	O
general	O
baseline	O
:	O
.	O
(	O
13.9	O
)	O
.	O
θt+1	O
=	O
θt	O
+	O
α	O
(	O
cid:16	O
)	O
gt	O
−	O
b	O
(	O
st	O
)	O
(	O
cid:17	O
)	O
∇π	O
(	O
at|st	O
,	O
θt	O
)	O
π	O
(	O
at|st	O
,	O
θt	O
)	O
because	O
the	O
baseline	B
could	O
be	O
uniformly	O
zero	O
,	O
this	O
update	O
is	O
a	O
strict	O
generalization	O
of	O
reinforce	O
.	O
in	O
general	O
,	O
the	O
baseline	B
leaves	O
the	O
expected	O
value	O
of	O
the	O
update	O
unchanged	O
,	O
but	O
it	O
can	O
have	O
a	O
large	O
eﬀect	O
on	O
its	O
variance	O
.	O
for	O
example	O
,	O
we	O
saw	O
in	O
section	O
2.8	O
that	O
an	O
analogous	O
baseline	B
can	O
signiﬁcantly	O
reduce	O
the	O
variance	O
(	O
and	O
thus	O
speed	O
the	O
learning	O
)	O
of	O
gradient	O
bandit	O
algorithms	O
.	O
in	O
the	O
bandit	O
algorithms	O
the	O
baseline	B
was	O
just	O
a	O
number	O
(	O
the	O
average	O
of	O
the	O
rewards	O
seen	O
so	O
far	O
)	O
,	O
but	O
for	O
mdps	O
the	O
baseline	B
should	O
vary	O
with	O
state	O
.	O
in	O
some	O
states	O
all	O
actions	O
have	O
high	O
values	O
and	O
we	O
need	O
a	O
high	O
baseline	B
to	O
diﬀerentiate	O
the	O
higher	O
valued	O
actions	O
from	O
the	O
less	O
highly	O
valued	O
ones	O
;	O
in	O
other	O
states	O
all	O
actions	O
will	O
have	O
low	O
values	O
and	O
a	O
low	O
baseline	B
is	O
appropriate	O
.	O
one	O
natural	O
choice	O
for	O
the	O
baseline	B
is	O
an	O
estimate	O
of	O
the	O
state	B
value	O
,	O
ˆv	O
(	O
st	O
,	O
w	O
)	O
,	O
where	O
w	O
∈	O
rm	O
is	O
a	O
weight	O
vector	B
learned	O
by	O
one	O
of	O
the	O
methods	O
presented	O
in	O
previous	O
chapters	O
.	O
332	O
chapter	O
13	O
:	O
policy	B
gradient	I
methods	I
because	O
reinforce	O
is	O
a	O
monte	O
carlo	O
method	O
for	O
learning	O
the	O
policy	B
parameter	O
,	O
θ	O
,	O
it	O
seems	O
natural	O
to	O
also	O
use	O
a	O
monte	O
carlo	O
method	O
to	O
learn	O
the	O
state-value	O
weights	O
,	O
w.	O
a	O
complete	O
pseudocode	O
algorithm	O
for	O
reinforce	O
with	B
baseline	I
using	O
such	O
a	O
learned	O
state-value	O
function	O
as	O
the	O
baseline	B
is	O
given	O
in	O
the	O
box	O
below	O
.	O
reinforce	O
with	B
baseline	I
(	O
episodic	O
)	O
,	O
for	O
estimating	O
πθ	O
≈	O
π∗	O
input	O
:	O
a	O
diﬀerentiable	O
policy	B
parameterization	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
input	O
:	O
a	O
diﬀerentiable	O
state-value	O
function	O
parameterization	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
algorithm	O
parameters	O
:	O
step	O
sizes	O
αθ	O
>	O
0	O
,	O
αw	O
>	O
0	O
initialize	O
policy	B
parameter	O
θ	O
∈	O
rd	O
(	O
cid:48	O
)	O
and	O
state-value	O
weights	O
w	O
∈	O
rd	O
(	O
e.g.	O
,	O
to	O
0	O
)	O
loop	O
forever	O
(	O
for	O
each	O
episode	O
)	O
:	O
generate	O
an	O
episode	O
s0	O
,	O
a0	O
,	O
r1	O
,	O
.	O
.	O
.	O
,	O
st−1	O
,	O
at−1	O
,	O
rt	O
,	O
following	O
π	O
(	O
·|·	O
,	O
θ	O
)	O
loop	O
for	O
each	O
step	O
of	O
the	O
episode	O
t	O
=	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
−	O
1	O
:	O
g	O
←	O
(	O
cid:80	O
)	O
t	O
k=t+1	O
rk	O
δ	O
←	O
g	O
−	O
ˆv	O
(	O
st	O
,	O
w	O
)	O
w	O
←	O
w	O
+	O
αw	O
γt	O
δ∇ˆv	O
(	O
st	O
,	O
w	O
)	O
θ	O
←	O
θ	O
+	O
αθ	O
γt	O
δ∇	O
ln	O
π	O
(	O
at|st	O
,	O
θ	O
)	O
(	O
gt	O
)	O
this	O
algorithm	O
has	O
two	O
step	O
sizes	O
,	O
denoted	O
αθ	O
and	O
αw	O
(	O
where	O
αθ	O
is	O
the	O
α	O
in	O
(	O
13.9	O
)	O
)	O
.	O
choosing	O
the	O
step	O
size	O
for	O
values	O
(	O
here	O
αw	O
)	O
is	O
relatively	O
easy	O
;	O
in	O
the	O
linear	O
case	O
we	O
have	O
rules	O
of	O
thumb	O
for	O
setting	O
it	O
,	O
such	O
as	O
αw	O
=	O
0.1/e	O
(	O
cid:2	O
)	O
(	O
cid:107	O
)	O
∇ˆv	O
(	O
st	O
,	O
w	O
)	O
(	O
cid:107	O
)	O
2	O
much	O
less	O
clear	O
how	O
to	O
set	O
the	O
step	O
size	O
for	O
the	O
policy	B
parameters	O
,	O
αθ	O
,	O
whose	O
best	O
value	B
depends	O
on	O
the	O
range	O
of	O
variation	O
of	O
the	O
rewards	O
and	O
on	O
the	O
policy	B
parameterization	O
.	O
µ	O
(	O
cid:3	O
)	O
(	O
see	O
section	O
9.6	O
)	O
.	O
it	O
is	O
figure	O
13.2	O
:	O
adding	O
a	O
baseline	B
to	O
reinforce	O
can	O
make	O
it	O
learn	O
much	O
faster	O
,	O
as	O
illustrated	O
here	O
on	O
the	O
short-corridor	O
gridworld	O
(	O
example	O
13.1	O
)	O
.	O
the	O
step	O
size	O
used	O
here	O
for	O
plain	O
rein-	O
force	O
is	O
that	O
at	O
which	O
it	O
performs	O
best	O
(	O
to	O
the	O
nearest	O
power	O
of	O
two	O
;	O
see	O
figure	O
13.1	O
)	O
.	O
each	O
line	O
is	O
an	O
average	O
over	O
100	O
independent	O
runs	O
.	O
↵=2 13episode10008006004002001-80-90-60-40-20-10v⇤	O
(	O
s0	O
)	O
reinforcereinforce	O
with	O
baseline↵=2 9↵✓=2 9	O
,	O
↵w=2 6total	O
rewardon	O
episodeg0	O
13.5.	O
actor–critic	B
methods	O
333	O
figure	O
13.2	O
compares	O
the	O
behavior	O
of	O
reinforce	O
with	O
and	O
without	O
a	O
baseline	B
on	O
the	O
short-corridor	O
gridword	O
(	O
example	O
13.1	O
)	O
.	O
here	O
the	O
approximate	B
state-value	O
function	O
used	O
in	O
the	O
baseline	O
is	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
=	O
w.	O
that	O
is	O
,	O
w	O
is	O
a	O
single	O
component	O
,	O
w.	O
13.5	O
actor–critic	B
methods	O
although	O
the	O
reinforce-with-baseline	O
method	O
learns	O
both	O
a	O
policy	B
and	O
a	O
state-value	O
function	O
,	O
we	O
do	O
not	O
consider	O
it	O
to	O
be	O
an	O
actor–critic	B
method	O
because	O
its	O
state-value	O
func-	O
tion	B
is	O
used	O
only	O
as	O
a	O
baseline	B
,	O
not	O
as	O
a	O
critic	B
.	O
that	O
is	O
,	O
it	O
is	O
not	O
used	O
for	O
bootstrapping	O
(	O
updating	O
the	O
value	B
estimate	O
for	O
a	O
state	O
from	O
the	O
estimated	O
values	O
of	O
subsequent	O
states	O
)	O
,	O
but	O
only	O
as	O
a	O
baseline	B
for	O
the	O
state	B
whose	O
estimate	O
is	O
being	O
updated	O
.	O
this	O
is	O
a	O
use-	O
ful	O
distinction	O
,	O
for	O
only	O
through	O
bootstrapping	B
do	O
we	O
introduce	O
bias	O
and	O
an	O
asymptotic	O
dependence	O
on	O
the	O
quality	O
of	O
the	O
function	B
approximation	I
.	O
as	O
we	O
have	O
seen	O
,	O
the	O
bias	O
introduced	O
through	O
bootstrapping	B
and	O
reliance	O
on	O
the	O
state	B
representation	O
is	O
often	O
ben-	O
eﬁcial	O
because	O
it	O
reduces	O
variance	O
and	O
accelerates	O
learning	O
.	O
reinforce	O
with	B
baseline	I
is	O
unbiased	O
and	O
will	O
converge	O
asymptotically	O
to	O
a	O
local	O
minimum	O
,	O
but	O
like	O
all	O
monte	O
carlo	O
methods	O
it	O
tends	O
to	O
learn	O
slowly	O
(	O
produce	O
estimates	O
of	O
high	O
variance	O
)	O
and	O
to	O
be	O
inconvenient	O
to	O
implement	O
online	B
or	O
for	O
continuing	O
problems	O
.	O
as	O
we	O
have	O
seen	O
earlier	O
in	O
this	O
book	O
,	O
with	O
temporal-diﬀerence	O
methods	O
we	O
can	O
eliminate	O
these	O
inconveniences	O
,	O
and	O
through	O
multi-step	O
methods	O
we	O
can	O
ﬂexibly	O
choose	O
the	O
degree	O
of	O
bootstrapping	O
.	O
in	O
order	O
to	O
gain	O
these	O
advantages	O
in	O
the	O
case	O
of	O
policy	O
gradient	B
methods	O
we	O
use	O
actor–critic	B
methods	O
with	O
a	O
bootstrapping	B
critic	O
.	O
first	O
consider	O
one-step	O
actor–critic	O
methods	O
,	O
the	O
analog	O
of	O
the	O
td	O
methods	O
introduced	O
in	O
chapter	O
6	O
such	O
as	O
td	O
(	O
0	O
)	O
,	O
sarsa	O
(	O
0	O
)	O
,	O
and	O
q-learning	O
.	O
the	O
main	O
appeal	O
of	O
one-step	O
methods	O
is	O
that	O
they	O
are	O
fully	O
online	B
and	O
incremental	O
,	O
yet	O
avoid	O
the	O
complexities	O
of	O
eligibility	O
traces	O
.	O
they	O
are	O
a	O
special	O
case	O
of	O
the	O
eligibility	O
trace	O
methods	O
,	O
and	O
not	O
as	O
general	O
,	O
but	O
easier	O
to	O
understand	O
.	O
one-step	O
actor–critic	O
methods	O
replace	O
the	O
full	O
return	B
of	O
reinforce	O
(	O
13.9	O
)	O
with	O
the	O
one-step	O
return	O
(	O
and	O
use	O
a	O
learned	O
state-value	O
function	O
as	O
the	O
baseline	B
)	O
as	O
follows	O
:	O
.	O
θt+1	O
=	O
θt	O
+	O
α	O
(	O
cid:16	O
)	O
gt	O
:	O
t+1	O
−	O
ˆv	O
(	O
st	O
,	O
w	O
)	O
(	O
cid:17	O
)	O
∇π	O
(	O
at|st	O
,	O
θt	O
)	O
=	O
θt	O
+	O
α	O
(	O
cid:16	O
)	O
rt+1	O
+	O
γˆv	O
(	O
st+1	O
,	O
w	O
)	O
−	O
ˆv	O
(	O
st	O
,	O
w	O
)	O
(	O
cid:17	O
)	O
∇π	O
(	O
at|st	O
,	O
θt	O
)	O
π	O
(	O
at|st	O
,	O
θt	O
)	O
π	O
(	O
at|st	O
,	O
θt	O
)	O
=	O
θt	O
+	O
αδt∇π	O
(	O
at|st	O
,	O
θt	O
)	O
π	O
(	O
at|st	O
,	O
θt	O
)	O
.	O
(	O
13.10	O
)	O
(	O
13.11	O
)	O
(	O
13.12	O
)	O
the	O
natural	O
state-value-function	O
learning	O
method	O
to	O
pair	O
with	O
this	O
is	O
semi-gradient	O
td	O
(	O
0	O
)	O
.	O
pseudocode	O
for	O
the	O
complete	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
.	O
note	O
that	O
it	O
is	O
now	O
a	O
fully	O
online	B
,	O
incremental	O
algorithm	O
,	O
with	O
states	O
,	O
actions	O
,	O
and	O
rewards	O
processed	O
as	O
they	O
occur	O
and	O
then	O
never	O
revisited	O
.	O
334	O
chapter	O
13	O
:	O
policy	B
gradient	I
methods	I
one-step	O
actor–critic	B
(	O
episodic	O
)	O
,	O
for	O
estimating	O
πθ	O
≈	O
π∗	O
input	O
:	O
a	O
diﬀerentiable	O
policy	B
parameterization	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
input	O
:	O
a	O
diﬀerentiable	O
state-value	O
function	O
parameterization	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
parameters	O
:	O
step	O
sizes	O
αθ	O
>	O
0	O
,	O
αw	O
>	O
0	O
initialize	O
policy	B
parameter	O
θ	O
∈	O
rd	O
(	O
cid:48	O
)	O
and	O
state-value	O
weights	O
w	O
∈	O
rd	O
(	O
e.g.	O
,	O
to	O
0	O
)	O
loop	O
forever	O
(	O
for	O
each	O
episode	O
)	O
:	O
initialize	O
s	O
(	O
ﬁrst	O
state	B
of	O
episode	O
)	O
i	O
←	O
1	O
loop	O
while	O
s	O
is	O
not	O
terminal	O
(	O
for	O
each	O
time	O
step	O
)	O
:	O
a	O
∼	O
π	O
(	O
·|s	O
,	O
θ	O
)	O
take	O
action	B
a	O
,	O
observe	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
δ	O
←	O
r	O
+	O
γ	O
ˆv	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
w	O
)	O
−	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
w	O
←	O
w	O
+	O
αw	O
i	O
δ∇ˆv	O
(	O
s	O
,	O
w	O
)	O
θ	O
←	O
θ	O
+	O
αθ	O
i	O
δ∇	O
ln	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
i	O
←	O
γi	O
s	O
←	O
s	O
(	O
cid:48	O
)	O
(	O
if	O
s	O
(	O
cid:48	O
)	O
is	O
terminal	O
,	O
then	O
ˆv	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
w	O
)	O
.	O
=	O
0	O
)	O
the	O
generalizations	O
to	O
the	O
forward	O
view	O
of	O
n-step	O
methods	O
and	O
then	O
to	O
a	O
λ-return	B
al-	O
gorithm	O
are	O
straightforward	O
.	O
the	O
one-step	O
return	O
in	O
(	O
13.10	O
)	O
is	O
merely	O
replaced	O
by	O
gt	O
:	O
t+n	O
or	O
gλ	O
t	O
respectively	O
.	O
the	O
backward	O
view	O
of	O
the	O
λ-return	B
algorithm	O
is	O
also	O
straightfor-	O
ward	O
,	O
using	O
separate	O
eligibility	B
traces	I
for	O
the	O
actor	O
and	O
critic	O
,	O
each	O
after	O
the	O
patterns	O
in	O
chapter	O
12.	O
pseudocode	O
for	O
the	O
complete	O
algorithm	O
is	O
given	O
in	O
the	O
box	O
below	O
.	O
actor–critic	B
with	O
eligibility	B
traces	I
(	O
episodic	O
)	O
,	O
for	O
estimating	O
πθ	O
≈	O
π∗	O
input	O
:	O
a	O
diﬀerentiable	O
policy	B
parameterization	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
input	O
:	O
a	O
diﬀerentiable	O
state-value	O
function	O
parameterization	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
parameters	O
:	O
trace-decay	O
rates	O
λθ	O
∈	O
[	O
0	O
,	O
1	O
]	O
,	O
λw	O
∈	O
[	O
0	O
,	O
1	O
]	O
;	O
step	O
sizes	O
αθ	O
>	O
0	O
,	O
αw	O
>	O
0	O
initialize	O
policy	B
parameter	O
θ	O
∈	O
rd	O
(	O
cid:48	O
)	O
and	O
state-value	O
weights	O
w	O
∈	O
rd	O
(	O
e.g.	O
,	O
to	O
0	O
)	O
loop	O
forever	O
(	O
for	O
each	O
episode	O
)	O
:	O
initialize	O
s	O
(	O
ﬁrst	O
state	B
of	O
episode	O
)	O
zθ	O
←	O
0	O
(	O
d	O
(	O
cid:48	O
)	O
-component	O
eligibility	O
trace	O
vector	B
)	O
zw	O
←	O
0	O
(	O
d-component	O
eligibility	O
trace	O
vector	B
)	O
i	O
←	O
1	O
loop	O
while	O
s	O
is	O
not	O
terminal	O
(	O
for	O
each	O
time	O
step	O
)	O
:	O
(	O
if	O
s	O
(	O
cid:48	O
)	O
is	O
terminal	O
,	O
then	O
ˆv	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
w	O
)	O
.	O
=	O
0	O
)	O
a	O
∼	O
π	O
(	O
·|s	O
,	O
θ	O
)	O
take	O
action	B
a	O
,	O
observe	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
δ	O
←	O
r	O
+	O
γ	O
ˆv	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
w	O
)	O
−	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
zw	O
←	O
γλwzw	O
+	O
i∇ˆv	O
(	O
s	O
,	O
w	O
)	O
zθ	O
←	O
γλθzθ	O
+	O
i∇	O
ln	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
w	O
←	O
w	O
+	O
αw	O
δ	O
zw	O
θ	O
←	O
θ	O
+	O
αθ	O
δ	O
zθ	O
i	O
←	O
γi	O
s	O
←	O
s	O
(	O
cid:48	O
)	O
13.6.	O
policy	O
gradient	O
for	O
continuing	O
problems	O
335	O
13.6	O
policy	O
gradient	O
for	O
continuing	O
problems	O
as	O
discussed	O
in	O
section	O
10.3	O
,	O
for	O
continuing	O
problems	O
without	O
episode	O
boundaries	O
we	O
need	O
to	O
deﬁne	O
performance	O
in	O
terms	O
of	O
the	O
average	O
rate	O
of	O
reward	O
per	O
time	O
step	O
:	O
j	O
(	O
θ	O
)	O
.	O
=	O
r	O
(	O
π	O
)	O
(	O
13.13	O
)	O
e	O
[	O
rt	O
|	O
a0	O
:	O
t−1∼	O
π	O
]	O
.	O
=	O
lim	O
h→∞	O
=	O
lim	O
t→∞	O
=	O
(	O
cid:88	O
)	O
s	O
h	O
(	O
cid:88	O
)	O
t=1	O
1	O
h	O
e	O
[	O
rt	O
|	O
a0	O
:	O
t−1∼	O
π	O
]	O
µ	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
a|s	O
)	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
r	O
,	O
.	O
=	O
limt→∞	O
pr	O
{	O
st	O
=	O
s|a0	O
:	O
t∼	O
π	O
}	O
,	O
where	O
µ	O
is	O
the	O
steady-state	O
distribution	O
under	O
π	O
,	O
µ	O
(	O
s	O
)	O
which	O
is	O
assumed	O
to	O
exist	O
and	O
to	O
be	O
independent	O
of	O
s0	O
(	O
an	O
ergodicity	O
assumption	O
)	O
.	O
remember	O
that	O
this	O
is	O
the	O
special	O
distribution	O
under	O
which	O
,	O
if	O
you	O
select	O
actions	O
according	O
to	O
π	O
,	O
you	O
remain	O
in	O
the	O
same	O
distribution	O
:	O
(	O
cid:88	O
)	O
s	O
µ	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
a	O
)	O
=	O
µ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
,	O
for	O
all	O
s	O
(	O
cid:48	O
)	O
∈	O
s.	O
(	O
13.14	O
)	O
we	O
also	O
deﬁne	O
values	O
,	O
vπ	O
(	O
s	O
)	O
respect	O
to	O
the	O
diﬀerential	B
return	O
:	O
.	O
=	O
eπ	O
[	O
gt|st	O
=	O
s	O
]	O
and	O
qπ	O
(	O
s	O
,	O
a	O
)	O
.	O
=	O
eπ	O
[	O
gt|st	O
=	O
s	O
,	O
at	O
=	O
a	O
]	O
,	O
with	O
gt	O
.	O
=	O
rt+1	O
−	O
r	O
(	O
π	O
)	O
+	O
rt+2	O
−	O
r	O
(	O
π	O
)	O
+	O
rt+3	O
−	O
r	O
(	O
π	O
)	O
+	O
···	O
.	O
(	O
13.15	O
)	O
with	O
these	O
alternate	O
deﬁnitions	O
,	O
the	O
policy	B
gradient	I
theorem	I
as	O
given	O
for	O
the	O
episodic	O
case	O
(	O
13.5	O
)	O
remains	O
true	O
for	O
the	O
continuing	O
case	O
.	O
a	O
proof	B
is	O
given	O
in	O
the	O
box	O
on	O
the	O
next	O
page	O
.	O
the	O
forward	O
and	O
backward	O
view	O
equations	O
also	O
remain	O
the	O
same	O
.	O
complete	O
pseudocode	O
for	O
the	O
actor–critic	B
algorithm	O
in	O
the	O
continuing	O
case	O
(	O
backward	O
view	O
)	O
is	O
given	O
in	O
the	O
box	O
on	O
the	O
page	O
after	O
next	O
.	O
336	O
chapter	O
13	O
:	O
policy	B
gradient	I
methods	I
proof	O
of	O
the	O
policy	B
gradient	I
theorem	I
(	O
continuing	O
case	O
)	O
the	O
proof	B
of	O
the	O
policy	B
gradient	I
theorem	I
for	O
the	O
continuing	O
case	O
begins	O
similarly	O
to	O
the	O
episodic	O
case	O
.	O
again	O
we	O
leave	O
it	O
implicit	O
in	O
all	O
cases	O
that	O
π	O
is	O
a	O
function	O
of	O
θ	O
and	O
that	O
the	O
gradients	O
are	O
with	O
respect	O
to	O
θ.	O
recall	O
that	O
in	O
the	O
continuing	O
case	O
j	O
(	O
θ	O
)	O
=	O
r	O
(	O
π	O
)	O
(	O
13.13	O
)	O
and	O
that	O
vπ	O
and	O
qπ	O
denote	O
values	O
with	O
respect	O
to	O
the	O
diﬀerential	B
return	O
(	O
13.15	O
)	O
.	O
the	O
gradient	B
of	O
the	O
state-value	O
function	O
can	O
be	O
written	O
,	O
for	O
any	O
s	O
∈	O
s	O
,	O
as	O
(	O
exercise	O
3.16	O
)	O
(	O
product	O
rule	O
of	O
calculus	O
)	O
∇vπ	O
(	O
s	O
)	O
=	O
∇	O
(	O
cid:34	O
)	O
(	O
cid:88	O
)	O
a	O
π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
(	O
cid:35	O
)	O
,	O
for	O
all	O
s	O
∈	O
s	O
=	O
(	O
cid:88	O
)	O
a	O
(	O
cid:104	O
)	O
∇π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
+	O
π	O
(	O
a|s	O
)	O
∇qπ	O
(	O
s	O
,	O
a	O
)	O
(	O
cid:105	O
)	O
=	O
(	O
cid:88	O
)	O
a	O
(	O
cid:104	O
)	O
∇π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
+	O
π	O
(	O
a|s	O
)	O
∇	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
=	O
(	O
cid:88	O
)	O
a	O
(	O
cid:104	O
)	O
∇π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
+	O
π	O
(	O
a|s	O
)	O
(	O
cid:2	O
)	O
−∇r	O
(	O
θ	O
)	O
+	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
∇r	O
(	O
θ	O
)	O
=	O
(	O
cid:88	O
)	O
a	O
(	O
cid:104	O
)	O
∇π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
+	O
π	O
(	O
a|s	O
)	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
r|s	O
,	O
a	O
)	O
(	O
cid:0	O
)	O
r	O
−	O
r	O
(	O
θ	O
)	O
+	O
vπ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:1	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
a	O
)	O
∇vπ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:3	O
)	O
(	O
cid:105	O
)	O
.	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
a	O
)	O
∇vπ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
−	O
∇vπ	O
(	O
s	O
)	O
.	O
after	O
re-arranging	O
terms	O
,	O
we	O
obtain	O
notice	O
that	O
the	O
left-hand	O
side	O
can	O
be	O
written	O
∇j	O
(	O
θ	O
)	O
,	O
and	O
that	O
it	O
does	O
not	O
depend	O
on	O
s.	O
thus	O
the	O
right-hand	O
side	O
does	O
not	O
depend	O
on	O
s	O
either	O
,	O
and	O
we	O
can	O
safely	O
sum	O
it	O
over	O
all	O
s	O
∈	O
s	O
,	O
weighted	O
by	O
µ	O
(	O
s	O
)	O
,	O
without	O
changing	O
it	O
(	O
because	O
(	O
cid:80	O
)	O
s	O
µ	O
(	O
s	O
)	O
=	O
1	O
)	O
:	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
a	O
)	O
∇vπ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:105	O
)	O
−	O
∇vπ	O
(	O
s	O
)	O
(	O
cid:33	O
)	O
∇j	O
(	O
θ	O
)	O
=	O
(	O
cid:88	O
)	O
s	O
=	O
(	O
cid:88	O
)	O
s	O
=	O
(	O
cid:88	O
)	O
s	O
µ	O
(	O
s	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
a	O
(	O
cid:104	O
)	O
∇π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
+	O
π	O
(	O
a|s	O
)	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
µ	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
a	O
µ	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
a	O
+	O
(	O
cid:88	O
)	O
s	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
a	O
)	O
∇vπ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
−	O
(	O
cid:88	O
)	O
s	O
µ	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
a	O
∇vπ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
−	O
(	O
cid:88	O
)	O
s	O
µ	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
a	O
+	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
(	O
cid:88	O
)	O
s	O
π	O
(	O
a|s	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
a	O
)	O
(	O
cid:125	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:124	O
)	O
µ	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
a	O
∇π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
+	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
µ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
∇vπ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
−	O
(	O
cid:88	O
)	O
s	O
µ	O
(	O
s	O
)	O
(	O
cid:88	O
)	O
a	O
=	O
(	O
cid:88	O
)	O
s	O
=	O
(	O
cid:88	O
)	O
s	O
∇π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
.	O
µ	O
(	O
s	O
)	O
∇vπ	O
(	O
s	O
)	O
∇π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
π	O
(	O
a|s	O
)	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
∇π	O
(	O
a|s	O
)	O
qπ	O
(	O
s	O
,	O
a	O
)	O
µ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
13.14	O
)	O
µ	O
(	O
s	O
)	O
∇vπ	O
(	O
s	O
)	O
µ	O
(	O
s	O
)	O
∇vπ	O
(	O
s	O
)	O
q.e.d	O
.	O
13.7.	O
policy	B
parameterization	O
for	O
continuous	O
actions	O
337	O
actor–critic	B
with	O
eligibility	B
traces	I
(	O
continuing	O
)	O
,	O
for	O
estimating	O
πθ	O
≈	O
π∗	O
input	O
:	O
a	O
diﬀerentiable	O
policy	B
parameterization	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
input	O
:	O
a	O
diﬀerentiable	O
state-value	O
function	O
parameterization	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
algorithm	O
parameters	O
:	O
λw	O
∈	O
[	O
0	O
,	O
1	O
]	O
,	O
λθ	O
∈	O
[	O
0	O
,	O
1	O
]	O
,	O
αw	O
>	O
0	O
,	O
αθ	O
>	O
0	O
,	O
α	O
¯r	O
>	O
0	O
initialize	O
¯r	O
∈	O
r	O
(	O
e.g.	O
,	O
to	O
0	O
)	O
initialize	O
state-value	O
weights	O
w	O
∈	O
rd	O
and	O
policy	O
parameter	O
θ	O
∈	O
rd	O
(	O
cid:48	O
)	O
(	O
e.g.	O
,	O
to	O
0	O
)	O
initialize	O
s	O
∈	O
s	O
(	O
e.g.	O
,	O
to	O
s0	O
)	O
zw	O
←	O
0	O
(	O
d-component	O
eligibility	O
trace	O
vector	B
)	O
zθ	O
←	O
0	O
(	O
d	O
(	O
cid:48	O
)	O
-component	O
eligibility	O
trace	O
vector	B
)	O
loop	O
forever	O
(	O
for	O
each	O
time	O
step	O
)	O
:	O
a	O
∼	O
π	O
(	O
·|s	O
,	O
θ	O
)	O
take	O
action	B
a	O
,	O
observe	O
s	O
(	O
cid:48	O
)	O
,	O
r	O
δ	O
←	O
r	O
−	O
¯r	O
+	O
ˆv	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
w	O
)	O
−	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
¯r	O
←	O
¯r	O
+	O
α	O
¯r	O
δ	O
zw	O
←	O
λwzw	O
+	O
∇ˆv	O
(	O
s	O
,	O
w	O
)	O
zθ	O
←	O
λθzθ	O
+	O
∇	O
ln	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
w	O
←	O
w	O
+	O
αw	O
δ	O
zw	O
θ	O
←	O
θ	O
+	O
αθ	O
δ	O
zθ	O
s	O
←	O
s	O
(	O
cid:48	O
)	O
13.7	O
policy	B
parameterization	O
for	O
continuous	O
actions	O
policy-based	O
methods	O
oﬀer	O
practical	O
ways	O
of	O
dealing	O
with	O
large	O
actions	O
spaces	O
,	O
even	O
continuous	O
spaces	O
with	O
an	O
inﬁnite	O
number	O
of	O
actions	O
.	O
instead	O
of	O
computing	O
learned	O
probabilities	O
for	O
each	O
of	O
the	O
many	O
actions	O
,	O
we	O
instead	O
learn	O
statistics	O
of	O
the	O
probability	O
distribution	O
.	O
for	O
example	O
,	O
the	O
action	B
set	O
might	O
be	O
the	O
real	O
numbers	O
,	O
with	O
actions	O
chosen	O
from	O
a	O
normal	O
(	O
gaussian	O
)	O
distribution	O
.	O
the	O
probability	O
density	O
function	O
for	O
the	O
normal	O
distribution	O
is	O
conventionally	O
written	O
p	O
(	O
x	O
)	O
.	O
=	O
1	O
σ√2π	O
exp	O
(	O
cid:18	O
)	O
−	O
(	O
x	O
−	O
µ	O
)	O
2	O
2σ2	O
(	O
cid:19	O
)	O
,	O
(	O
13.16	O
)	O
where	O
µ	O
and	O
σ	O
here	O
are	O
the	O
mean	O
and	O
standard	O
deviation	O
of	O
the	O
normal	O
distribution	O
,	O
and	O
of	O
course	O
π	O
here	O
is	O
just	O
the	O
number	O
π	O
≈	O
3.14159.	O
the	O
probability	O
density	O
functions	O
for	O
several	O
diﬀerent	O
means	O
and	O
standard	O
deviations	O
are	O
shown	O
in	O
figure	O
13.3.	O
the	O
value	B
p	O
(	O
x	O
)	O
is	O
the	O
density	O
of	O
the	O
probability	O
at	O
x	O
,	O
not	O
the	O
probability	O
.	O
it	O
can	O
be	O
greater	O
than	O
1	O
;	O
it	O
is	O
the	O
total	O
area	O
under	O
p	O
(	O
x	O
)	O
that	O
must	O
sum	O
to	O
1.	O
in	O
general	O
,	O
one	O
can	O
take	O
the	O
integral	O
under	O
p	O
(	O
x	O
)	O
for	O
any	O
range	O
of	O
x	O
values	O
to	O
get	O
the	O
probability	O
of	O
x	O
falling	O
within	O
that	O
range	O
.	O
to	O
produce	O
a	O
policy	B
parameterization	O
,	O
the	O
policy	B
can	O
be	O
deﬁned	O
as	O
the	O
normal	O
prob-	O
ability	O
density	O
over	O
a	O
real-valued	O
scalar	O
action	B
,	O
with	O
mean	O
and	O
standard	O
deviation	O
given	O
338	O
chapter	O
13	O
:	O
policy	B
gradient	I
methods	I
figure	O
13.3	O
:	O
the	O
probability	O
density	O
function	O
of	O
the	O
normal	O
distribution	O
for	O
diﬀerent	O
means	O
and	O
variances	O
.	O
by	O
parametric	O
function	O
approximators	O
that	O
depend	O
on	O
the	O
state	B
.	O
that	O
is	O
,	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
.	O
=	O
1	O
σ	O
(	O
s	O
,	O
θ	O
)	O
√2π	O
exp	O
(	O
cid:18	O
)	O
−	O
(	O
a	O
−	O
µ	O
(	O
s	O
,	O
θ	O
)	O
)	O
2	O
2σ	O
(	O
s	O
,	O
θ	O
)	O
2	O
(	O
cid:19	O
)	O
,	O
(	O
13.17	O
)	O
where	O
µ	O
:	O
s	O
×	O
rd	O
(	O
cid:48	O
)	O
→	O
r	O
and	O
σ	O
:	O
s	O
×	O
rd	O
(	O
cid:48	O
)	O
→	O
r+	O
are	O
two	O
parameterized	O
function	O
approx-	O
imators	O
.	O
to	O
complete	O
the	O
example	O
we	O
need	O
only	O
give	O
a	O
form	O
for	O
these	O
approximators	O
.	O
for	O
this	O
we	O
divide	O
the	O
policy	B
’	O
s	O
parameter	O
vector	O
into	O
two	O
parts	O
,	O
θ	O
=	O
[	O
θµ	O
,	O
θσ	O
]	O
(	O
cid:62	O
)	O
,	O
one	O
part	O
to	O
be	O
used	O
for	O
the	O
approximation	O
of	O
the	O
mean	O
and	O
one	O
part	O
for	O
the	O
approximation	O
of	O
the	O
standard	O
deviation	O
.	O
the	O
mean	O
can	O
be	O
approximated	O
as	O
a	O
linear	O
function	O
.	O
the	O
standard	O
deviation	O
must	O
always	O
be	O
positive	O
and	O
is	O
better	O
approximated	O
as	O
the	O
exponential	O
of	O
a	O
linear	O
function	O
.	O
thus	O
µ	O
(	O
s	O
,	O
θ	O
)	O
.	O
=	O
θµ	O
(	O
cid:62	O
)	O
xµ	O
(	O
s	O
)	O
and	O
σ	O
(	O
s	O
,	O
θ	O
)	O
(	O
13.18	O
)	O
.	O
=	O
exp	O
(	O
cid:16	O
)	O
θσ	O
(	O
cid:62	O
)	O
xσ	O
(	O
s	O
)	O
(	O
cid:17	O
)	O
,	O
where	O
xµ	O
(	O
s	O
)	O
and	O
xσ	O
(	O
s	O
)	O
are	O
state	B
feature	O
vectors	O
perhaps	O
constructed	O
by	O
one	O
of	O
the	O
meth-	O
ods	O
described	O
in	O
chapter	O
9.	O
with	O
these	O
deﬁnitions	O
,	O
all	O
the	O
algorithms	O
described	O
in	O
the	O
rest	O
of	O
this	O
chapter	O
can	O
be	O
applied	O
to	O
learn	O
to	O
select	O
real-valued	O
actions	O
.	O
exercise	O
13.4	O
show	O
that	O
for	O
the	O
gaussian	O
policy	B
parameterization	O
(	O
13.17	O
)	O
the	O
eligibility	O
vector	O
has	O
the	O
following	O
two	O
parts	O
:	O
∇	O
ln	O
π	O
(	O
a|s	O
,	O
θµ	O
)	O
=	O
∇π	O
(	O
a|s	O
,	O
θµ	O
)	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
∇	O
ln	O
π	O
(	O
a|s	O
,	O
θσ	O
)	O
=	O
∇π	O
(	O
a|s	O
,	O
θσ	O
)	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
1	O
=	O
σ	O
(	O
s	O
,	O
θ	O
)	O
2	O
(	O
cid:0	O
)	O
a	O
−	O
µ	O
(	O
s	O
,	O
θ	O
)	O
(	O
cid:1	O
)	O
xµ	O
(	O
s	O
)	O
,	O
and	O
−	O
1	O
(	O
cid:33	O
)	O
xσ	O
(	O
s	O
)	O
.	O
=	O
(	O
cid:32	O
)	O
(	O
cid:0	O
)	O
a	O
−	O
µ	O
(	O
s	O
,	O
θ	O
)	O
(	O
cid:1	O
)	O
2	O
σ	O
(	O
s	O
,	O
θ	O
)	O
2	O
(	O
cid:3	O
)	O
p	O
(	O
x	O
)	O
.=1 p2⇡exp✓ 	O
(	O
x µ	O
)	O
22 2◆φµ	O
,	O
σ2	O
(	O
0.80.60.40.20.0−5−3135x1.0−1024−2−4x	O
)	O
0	O
,	O
µ=0	O
,	O
µ=0	O
,	O
µ=−2	O
,	O
µ=20.2	O
,	O
σ=21.0	O
,	O
σ=25.0	O
,	O
σ=20.5	O
,	O
σ=p	O
(	O
x	O
)	O
.=1 p2⇡exp✓ 	O
(	O
x µ	O
)	O
22 2◆	O
13.8.	O
summary	O
339	O
exercise	O
13.5	O
a	O
bernoulli-logistic	O
unit	O
is	O
a	O
stochastic	O
neuron-like	O
unit	O
used	O
in	O
some	O
artiﬁcial	B
neural	I
networks	I
(	O
section	O
9.7	O
)	O
.	O
its	O
input	O
at	O
time	O
t	O
is	O
a	O
feature	O
vector	O
x	O
(	O
st	O
)	O
;	O
its	O
output	O
,	O
at	O
,	O
is	O
a	O
random	O
variable	O
having	O
two	O
values	O
,	O
0	O
and	O
1	O
,	O
with	O
pr	O
{	O
at	O
=	O
1	O
}	O
=	O
pt	O
and	O
pr	O
{	O
at	O
=	O
0	O
}	O
=	O
1	O
−	O
pt	O
(	O
the	O
bernoulli	O
distribution	O
)	O
.	O
let	O
h	O
(	O
s	O
,	O
0	O
,	O
θ	O
)	O
and	O
h	O
(	O
s	O
,	O
1	O
,	O
θ	O
)	O
be	O
the	O
preferences	O
in	O
state	O
s	O
for	O
the	O
unit	O
’	O
s	O
two	O
actions	O
given	O
policy	B
parameter	O
θ.	O
assume	O
that	O
the	O
diﬀerence	O
between	O
the	O
action	B
preferences	I
is	O
given	O
by	O
a	O
weighted	O
sum	O
of	O
the	O
unit	O
’	O
s	O
input	O
vector	B
,	O
that	O
is	O
,	O
assume	O
that	O
h	O
(	O
s	O
,	O
1	O
,	O
θ	O
)	O
−	O
h	O
(	O
s	O
,	O
0	O
,	O
θ	O
)	O
=	O
θ	O
(	O
cid:62	O
)	O
x	O
(	O
s	O
)	O
,	O
where	O
θ	O
is	O
the	O
unit	O
’	O
s	O
weight	O
vector	B
.	O
(	O
a	O
)	O
show	O
that	O
if	O
the	O
exponential	O
soft-max	B
distribution	O
(	O
13.2	O
)	O
is	O
used	O
to	O
convert	O
action	B
preferences	I
to	O
policies	O
,	O
then	O
pt	O
=	O
π	O
(	O
1|st	O
,	O
θt	O
)	O
=	O
1/	O
(	O
1	O
+	O
exp	O
(	O
−θ	O
(	O
cid:62	O
)	O
t	O
x	O
(	O
st	O
)	O
)	O
)	O
(	O
the	O
logistic	O
function	O
)	O
.	O
(	O
b	O
)	O
what	O
is	O
the	O
monte-carlo	O
reinforce	O
update	O
of	O
θt	O
to	O
θt+1	O
upon	O
receipt	O
of	O
return	O
gt	O
?	O
(	O
c	O
)	O
express	O
the	O
eligibility	O
∇	O
ln	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
for	O
a	O
bernoulli-logistic	O
unit	O
,	O
in	O
terms	O
of	O
a	O
,	O
x	O
(	O
s	O
)	O
,	O
and	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
by	O
calculating	O
the	O
gradient	B
.	O
hint	O
:	O
separately	O
for	O
each	O
action	B
compute	O
the	O
derivative	O
of	O
the	O
logarithm	O
ﬁrst	O
with	O
respect	O
to	O
pt	O
=	O
π	O
(	O
a|s	O
,	O
θt	O
)	O
,	O
combine	O
the	O
two	O
results	O
into	O
one	O
expression	O
that	O
depends	O
on	O
a	O
and	O
pt	O
,	O
and	O
then	O
use	O
the	O
chain	O
rule	O
,	O
noting	O
that	O
the	O
derivative	O
of	O
the	O
logistic	O
function	O
f	O
(	O
x	O
)	O
(	O
cid:3	O
)	O
is	O
f	O
(	O
x	O
)	O
(	O
1	O
−	O
f	O
(	O
x	O
)	O
)	O
.	O
13.8	O
summary	O
prior	O
to	O
this	O
chapter	O
,	O
this	O
book	O
focused	O
on	O
action-value	O
methods—meaning	O
methods	O
that	O
learn	O
action	B
values	O
and	O
then	O
use	O
them	O
to	O
determine	O
action	B
selections	O
.	O
in	O
this	O
chapter	O
,	O
on	O
the	O
other	O
hand	O
,	O
we	O
considered	O
methods	O
that	O
learn	O
a	O
parameterized	O
policy	B
that	O
enables	O
actions	O
to	O
be	O
taken	O
without	O
consulting	O
action-value	O
estimates	O
.	O
in	O
particular	O
,	O
we	O
have	O
considered	O
policy-gradient	O
methods—meaning	O
methods	O
that	O
update	O
the	O
policy	B
parameter	O
on	O
each	O
step	O
in	O
the	O
direction	O
of	O
an	O
estimate	O
of	O
the	O
gradient	B
of	O
performance	O
with	O
respect	O
to	O
the	O
policy	B
parameter	O
.	O
methods	O
that	O
learn	O
and	O
store	O
a	O
policy	B
parameter	O
have	O
many	O
advantages	O
.	O
they	O
can	O
learn	O
speciﬁc	O
probabilities	O
for	O
taking	O
the	O
actions	O
.	O
they	O
can	O
learn	O
appropriate	O
levels	O
of	O
exploration	O
and	O
approach	O
deterministic	O
policies	O
asymptotically	O
.	O
they	O
can	O
naturally	O
handle	O
continuous	B
action	I
spaces	O
.	O
all	O
these	O
things	O
are	O
easy	O
for	O
policy-based	O
methods	O
but	O
awkward	O
or	O
impossible	O
for	O
ε-greedy	O
methods	O
and	O
for	O
action-value	B
methods	I
in	O
general	O
.	O
in	O
addition	O
,	O
on	O
some	O
problems	O
the	O
policy	B
is	O
just	O
simpler	O
to	O
represent	O
parametrically	O
than	O
the	O
value	B
function	I
;	O
these	O
problems	O
are	O
more	O
suited	O
to	O
parameterized	O
policy	B
methods	O
.	O
parameterized	O
policy	B
methods	O
also	O
have	O
an	O
important	O
theoretical	O
advantage	O
over	O
action-value	B
methods	I
in	O
the	O
form	O
of	O
the	O
policy	B
gradient	I
theorem	I
,	O
which	O
gives	O
an	O
exact	O
formula	O
for	O
how	O
performance	O
is	O
aﬀected	O
by	O
the	O
policy	B
parameter	O
that	O
does	O
not	O
involve	O
derivatives	O
of	O
the	O
state	B
distribution	O
.	O
this	O
theorem	B
provides	O
a	O
theoretical	O
foundation	O
for	O
all	O
policy	B
gradient	I
methods	I
.	O
340	O
chapter	O
13	O
:	O
policy	B
gradient	I
methods	I
the	O
reinforce	O
method	O
follows	O
directly	O
from	O
the	O
policy	B
gradient	I
theorem	I
.	O
adding	O
a	O
state-value	O
function	O
as	O
a	O
baseline	B
reduces	O
reinforce	O
’	O
s	O
variance	O
without	O
introduc-	O
ing	B
bias	O
.	O
using	O
the	O
state-value	O
function	O
for	O
bootstrapping	B
introduces	O
bias	O
but	O
is	O
often	O
desirable	O
for	O
the	O
same	O
reason	O
that	O
bootstrapping	B
td	O
methods	O
are	O
often	O
superior	O
to	O
monte	O
carlo	O
methods	O
(	O
substantially	O
reduced	O
variance	O
)	O
.	O
the	O
state-value	O
function	O
assigns	O
credit	O
to—critizes—the	O
policy	B
’	O
s	O
action	B
selections	O
,	O
and	O
accordingly	O
the	O
former	O
is	O
termed	O
the	O
critic	B
and	O
the	O
latter	O
the	O
actor	O
,	O
and	O
these	O
overall	O
methods	O
are	O
termed	O
actor–critic	B
methods	O
.	O
overall	O
,	O
policy-gradient	O
methods	O
provide	O
a	O
signiﬁcantly	O
diﬀerent	O
set	O
of	O
strengths	O
and	O
weaknesses	O
than	O
action-value	B
methods	I
.	O
today	O
they	O
are	O
less	O
well	O
understood	O
in	O
some	O
respects	O
,	O
but	O
a	O
subject	O
of	O
excitement	O
and	O
ongoing	O
research	O
.	O
bibliographical	O
and	O
historical	O
remarks	O
methods	O
that	O
we	O
now	O
see	O
as	O
related	O
to	O
policy	B
gradients	O
were	O
actually	O
some	O
of	O
the	O
earliest	O
to	O
be	O
studied	O
in	O
reinforcement	O
learning	O
(	O
witten	O
,	O
1977	O
;	O
barto	O
,	O
sutton	O
,	O
and	O
anderson	O
,	O
1983	O
;	O
sutton	O
,	O
1984	O
;	O
williams	O
,	O
1987	O
,	O
1992	O
)	O
and	O
in	O
predecessor	O
ﬁelds	O
(	O
phansalkar	O
and	O
thathachar	O
,	O
1995	O
)	O
.	O
they	O
were	O
largely	O
supplanted	O
in	O
the	O
1990s	O
by	O
the	O
action-value	O
meth-	O
ods	O
that	O
are	O
the	O
focus	O
of	O
the	O
other	O
chapters	O
of	O
this	O
book	O
.	O
in	O
recent	O
years	O
,	O
however	O
,	O
attention	O
has	O
returned	O
to	O
actor–critic	B
methods	O
and	O
to	O
policy-gradient	O
methods	O
in	O
gen-	O
eral	O
.	O
among	O
the	O
further	O
developments	O
beyond	O
what	O
we	O
cover	O
here	O
are	O
natural-gradient	O
methods	O
(	O
amari	O
,	O
1998	O
;	O
kakade	O
,	O
2002	O
,	O
peters	O
,	O
vijayakumar	O
and	O
schaal	O
,	O
2005	O
;	O
peters	O
and	O
schall	O
,	O
2008	O
;	O
park	O
,	O
kim	O
and	O
kang	O
,	O
2005	O
;	O
bhatnagar	O
,	O
sutton	O
,	O
ghavamzadeh	O
and	O
lee	O
,	O
2009	O
;	O
see	O
grondman	O
,	O
busoniu	O
,	O
lopes	O
and	O
babuska	O
,	O
2012	O
)	O
,	O
and	O
deterministic	O
policy	O
gradient	O
(	O
silver	O
et	O
al.	O
,	O
2014	O
)	O
.	O
major	O
applications	O
include	O
acrobatic	O
helicopter	O
autopilots	O
and	O
alphago	O
(	O
see	O
section	O
16.6	O
)	O
.	O
our	O
presentation	O
in	O
this	O
chapter	O
is	O
based	O
primarily	O
on	O
that	O
by	O
sutton	O
,	O
mcallester	O
,	O
singh	O
,	O
and	O
mansour	O
(	O
2000	O
,	O
see	O
also	O
sutton	O
,	O
singh	O
,	O
and	O
mcallester	O
,	O
2000	O
)	O
,	O
who	O
introduced	O
the	O
term	O
“	O
policy	O
gradient	O
methods.	O
”	O
a	O
useful	O
overview	O
is	O
provided	O
by	O
bhatnagar	O
et	O
al	O
.	O
(	O
2009	O
)	O
.	O
one	O
of	O
the	O
earliest	O
related	O
works	O
is	O
by	O
aleksandrov	O
,	O
sysoyev	O
,	O
and	O
shemeneva	O
(	O
1968	O
)	O
.	O
thomas	O
(	O
2014	O
)	O
ﬁrst	O
realized	O
that	O
the	O
factor	O
of	O
γt	O
,	O
as	O
speciﬁed	O
in	O
the	O
boxed	O
algorithms	O
of	O
this	O
chapter	O
,	O
was	O
needed	O
in	O
the	O
case	O
of	O
discounted	O
episodic	O
problems	O
.	O
13.1	O
13.2	O
example	O
13.1	O
and	O
the	O
results	O
with	O
it	O
in	O
this	O
chapter	O
were	O
developed	O
with	O
eric	O
graves	O
.	O
the	O
policy	B
gradient	I
theorem	I
here	O
and	O
on	O
page	O
336	O
was	O
ﬁrst	O
obtained	O
by	O
marbach	O
and	O
tsitsiklis	O
(	O
1998	O
,	O
2001	O
)	O
and	O
then	O
independently	O
by	O
sutton	O
et	O
al	O
.	O
(	O
2000	O
)	O
.	O
a	O
similar	O
expression	O
was	O
obtained	O
by	O
cao	O
and	O
chen	O
(	O
1997	O
)	O
.	O
other	O
early	O
results	O
are	O
due	O
to	O
konda	O
and	O
tsitsiklis	O
(	O
2000	O
,	O
2003	O
)	O
,	O
baxter	O
and	O
bartlett	O
(	O
2001	O
)	O
,	O
and	O
baxter	O
,	O
bartlett	O
,	O
and	O
weaver	O
(	O
2001	O
)	O
.	O
some	O
additional	O
results	O
are	O
developed	O
by	O
sutton	O
,	O
singh	O
,	O
and	O
mcallester	O
(	O
2000	O
)	O
.	O
13.3	O
reinforce	O
is	O
due	O
to	O
williams	O
(	O
1987	O
,	O
1992	O
)	O
.	O
the	O
use	O
of	O
a	O
power	O
of	O
the	O
discount	O
factor	O
in	O
the	O
update	O
(	O
as	O
in	O
the	O
boxed	B
algorithms	I
)	O
is	O
due	O
to	O
thomas	O
(	O
2014	O
)	O
.	O
13.8.	O
summary	O
341	O
phansalkar	O
and	O
thathachar	O
(	O
1995	O
)	O
proved	O
both	O
local	O
and	O
global	O
convergence	O
theorems	O
for	O
modiﬁed	O
versions	O
of	O
reinforce	O
algorithms	O
.	O
13.4	O
the	O
baseline	B
was	O
introduced	O
in	O
williams	O
’	O
s	O
(	O
1987	O
,	O
1992	O
)	O
original	O
work	O
.	O
green-	O
smith	O
,	O
bartlett	O
,	O
and	O
baxter	O
(	O
2004	O
)	O
analyzed	O
an	O
arguably	O
better	O
baseline	B
(	O
see	O
dick	O
,	O
2015	O
)	O
.	O
13.5–6	O
actor–critic	B
methods	O
were	O
among	O
the	O
earliest	O
to	O
be	O
investigated	O
in	O
reinforcement	O
learning	O
(	O
witten	O
,	O
1977	O
;	O
barto	O
,	O
sutton	O
,	O
and	O
anderson	O
,	O
1983	O
;	O
sutton	O
,	O
1984	O
)	O
.	O
the	O
algorithms	O
presented	O
here	O
and	O
in	O
section	O
13.6	O
are	O
based	O
on	O
the	O
work	O
of	O
degris	O
,	O
white	O
,	O
and	O
sutton	O
(	O
2012	O
)	O
,	O
who	O
also	O
introduced	O
the	O
study	O
of	O
oﬀ-policy	O
policy-	O
gradient	B
methods	O
.	O
13.7	O
the	O
ﬁrst	O
to	O
show	O
how	O
continuous	O
actions	O
could	O
be	O
handled	O
this	O
way	O
appears	O
to	O
have	O
been	O
williams	O
(	O
1987	O
,	O
1992	O
)	O
.	O
figure	O
13.3	O
is	O
adapted	O
from	O
wikipedia	O
.	O
part	O
iii	O
:	O
looking	O
deeper	O
in	O
this	O
last	O
part	O
of	O
the	O
book	O
we	O
look	O
beyond	O
the	O
standard	O
reinforcement	B
learning	I
ideas	O
presented	O
in	O
the	O
ﬁrst	O
two	O
parts	O
of	O
the	O
book	O
to	O
brieﬂy	O
survey	O
their	O
relationships	O
with	O
psychology	O
and	O
neuroscience	O
,	O
a	O
sampling	O
of	O
reinforcement	O
learning	O
applications	O
,	O
and	O
some	O
of	O
the	O
active	O
frontiers	O
for	O
future	O
reinforcement	B
learning	I
research	O
.	O
343	O
chapter	O
14	O
psychology	B
in	O
previous	O
chapters	O
we	O
developed	O
ideas	O
for	O
algorithms	O
based	O
on	O
computational	O
con-	O
siderations	O
alone	O
.	O
in	O
this	O
chapter	O
we	O
look	O
at	O
some	O
of	O
these	O
algorithms	O
from	O
another	O
perspective	O
:	O
the	O
perspective	O
of	O
psychology	O
and	O
its	O
study	O
of	O
how	O
animals	O
learn	O
.	O
the	O
goals	O
of	O
this	O
chapter	O
are	O
,	O
ﬁrst	O
,	O
to	O
discuss	O
ways	O
that	O
reinforcement	B
learning	I
ideas	O
and	O
algorithms	O
correspond	O
to	O
what	O
psychologists	O
have	O
discovered	O
about	O
animal	O
learning	O
,	O
and	O
second	O
,	O
to	O
explain	O
the	O
inﬂuence	O
reinforcement	B
learning	I
is	O
having	O
on	O
the	O
study	O
of	O
animal	O
learning	O
.	O
the	O
clear	O
formalism	O
provided	O
by	O
reinforcement	B
learning	I
that	O
systemizes	O
tasks	O
,	O
returns	O
,	O
and	O
algorithms	O
is	O
proving	O
to	O
be	O
enormously	O
useful	O
in	O
making	O
sense	O
of	O
experimental	O
data	O
,	O
in	O
suggesting	O
new	O
kinds	O
of	O
experiments	O
,	O
and	O
in	O
pointing	O
to	O
factors	O
that	O
may	O
be	O
critical	O
to	O
manipulate	O
and	O
to	O
measure	O
.	O
the	O
idea	O
of	O
optimizing	O
return	B
over	O
the	O
long	O
term	O
that	O
is	O
at	O
the	O
core	O
of	O
reinforcement	O
learning	O
is	O
contributing	O
to	O
our	O
understanding	O
of	O
otherwise	O
puzzling	O
features	O
of	O
animal	O
learning	O
and	O
behavior	O
.	O
some	O
of	O
the	O
correspondences	O
between	O
reinforcement	B
learning	I
and	O
psychological	O
theo-	O
ries	O
are	O
not	O
surprising	O
because	O
the	O
development	O
of	O
reinforcement	O
learning	O
drew	O
inspiration	O
from	O
psychological	O
learning	O
theories	O
.	O
however	O
,	O
as	O
developed	O
in	O
this	O
book	O
,	O
reinforcement	B
learning	I
explores	O
idealized	O
situations	O
from	O
the	O
perspective	O
of	O
an	O
artiﬁcial	B
intelligence	I
researcher	O
or	O
engineer	O
,	O
with	O
the	O
goal	B
of	O
solving	O
computational	O
problems	O
with	O
eﬃcient	O
algorithms	O
,	O
rather	O
than	O
to	O
replicate	O
or	O
explain	O
in	O
detail	O
how	O
animals	O
learn	O
.	O
as	O
a	O
result	O
,	O
some	O
of	O
the	O
correspondences	O
we	O
describe	O
connect	O
ideas	O
that	O
arose	O
independently	O
in	O
their	O
respective	O
ﬁelds	O
.	O
we	O
believe	O
these	O
points	O
of	O
contact	O
are	O
specially	O
meaningful	O
because	O
they	O
expose	O
computational	O
principles	O
important	O
to	O
learning	O
,	O
whether	O
it	O
is	O
learning	O
by	O
artiﬁcial	O
or	O
by	O
natural	O
systems	O
.	O
for	O
the	O
most	O
part	O
,	O
we	O
describe	O
correspondences	O
between	O
reinforcement	B
learning	I
and	O
learning	O
theories	O
developed	O
to	O
explain	O
how	O
animals	O
like	O
rats	O
,	O
pigeons	O
,	O
and	O
rabbits	O
learn	O
in	O
controlled	O
laboratory	O
experiments	O
.	O
thousands	O
of	O
these	O
experiments	O
were	O
conducted	O
throughout	O
the	O
20th	O
century	O
,	O
and	O
many	O
are	O
still	O
being	O
conducted	O
today	O
.	O
although	O
some-	O
times	O
dismissed	O
as	O
irrelevant	O
to	O
wider	O
issues	O
in	B
psychology	I
,	O
these	O
experiments	O
probe	O
subtle	O
properties	O
of	O
animal	O
learning	O
,	O
often	O
motivated	O
by	O
precise	O
theoretical	O
questions	O
.	O
as	O
psychology	O
shifted	O
its	O
focus	O
to	O
more	O
cognitive	O
aspects	O
of	O
behavior	O
,	O
that	O
is	O
,	O
to	O
mental	O
processes	O
such	O
as	O
thought	O
and	O
reasoning	O
,	O
animal	O
learning	O
experiments	O
came	O
to	O
play	O
less	O
345	O
346	O
chapter	O
14	O
:	O
psychology	B
of	O
a	O
role	O
in	B
psychology	I
than	O
they	O
once	O
did	O
.	O
but	O
this	O
experimentation	O
led	O
to	O
the	O
discovery	O
of	O
learning	O
principles	O
that	O
are	O
elemental	O
and	O
widespread	O
throughout	O
the	O
animal	O
king-	O
dom	O
,	O
principles	O
that	O
should	O
not	O
be	O
neglected	O
in	O
designing	O
artiﬁcial	O
learning	O
systems	O
.	O
in	O
addition	O
,	O
as	O
we	O
shall	O
see	O
,	O
some	O
aspects	O
of	O
cognitive	O
processing	O
connect	O
naturally	O
to	O
the	O
computational	O
perspective	O
provided	O
by	O
reinforcement	B
learning	I
.	O
this	O
chapter	O
’	O
s	O
ﬁnal	O
section	O
includes	O
references	O
relevant	O
to	O
the	O
connections	O
we	O
discuss	O
as	O
well	O
as	O
to	O
connections	O
we	O
neglect	O
.	O
we	O
hope	O
this	O
chapter	O
encourages	O
readers	O
to	O
probe	O
all	O
of	O
these	O
connections	O
more	O
deeply	O
.	O
also	O
included	O
in	O
this	O
ﬁnal	O
section	O
is	O
a	O
discussion	O
of	O
how	O
the	O
terminology	O
used	O
in	O
reinforcement	O
learning	O
relates	O
to	O
that	O
of	O
psychology	O
.	O
many	O
of	O
the	O
terms	O
and	O
phrases	O
used	O
in	O
reinforcement	O
learning	O
are	O
borrowed	O
from	O
animal	O
learning	O
theories	O
,	O
but	O
the	O
computational/engineering	O
meanings	O
of	O
these	O
terms	O
and	O
phrases	O
do	O
not	O
always	O
coincide	O
with	O
their	O
meanings	O
in	B
psychology	I
.	O
14.1	O
prediction	B
and	O
control	B
the	O
algorithms	O
we	O
describe	O
in	O
this	O
book	O
fall	O
into	O
two	O
broad	O
categories	O
:	O
algorithms	O
for	O
pre-	O
diction	O
and	O
algorithms	O
for	O
control	O
.	O
these	O
categories	O
arise	O
naturally	O
in	O
solution	O
methods	O
for	O
the	O
reinforcement	B
learning	I
problem	O
presented	O
in	O
chapter	O
3.	O
in	O
many	O
ways	O
these	O
cat-	O
egories	O
respectively	O
correspond	O
to	O
categories	O
of	O
learning	O
extensively	O
studied	O
by	O
psychol-	O
ogists	O
:	O
classical	O
,	O
or	O
pavlovian	O
,	O
conditioning	B
and	O
instrumental	O
,	O
or	O
operant	O
,	O
conditioning	B
.	O
these	O
correspondences	O
are	O
not	O
completely	O
accidental	O
because	O
of	O
psychology	O
’	O
s	O
inﬂuence	O
on	O
reinforcement	B
learning	I
,	O
but	O
they	O
are	O
nevertheless	O
striking	O
because	O
they	O
connect	O
ideas	O
arising	O
from	O
diﬀerent	O
objectives	O
.	O
the	O
prediction	B
algorithms	O
presented	O
in	O
this	O
book	O
estimate	O
quantities	O
that	O
depend	O
on	O
how	O
features	O
of	O
an	O
agent	O
’	O
s	O
environment	B
are	O
expected	O
to	O
unfold	O
over	O
the	O
future	O
.	O
we	O
speciﬁcally	O
focus	O
on	O
estimating	O
the	O
amount	O
of	O
reward	O
an	O
agent	O
can	O
expect	O
to	O
receive	O
over	O
the	O
future	O
while	O
it	O
interacts	O
with	O
its	O
environment	B
.	O
in	O
this	O
role	O
,	O
prediction	B
algorithms	O
are	O
policy	B
evaluation	I
algorithms	O
,	O
which	O
are	O
integral	O
components	O
of	O
algorithms	O
for	O
improving	O
policies	O
.	O
but	O
prediction	B
algorithms	O
are	O
not	O
limited	O
to	O
predicting	O
future	O
reward	O
;	O
they	O
can	O
predict	O
any	O
feature	O
of	O
the	O
environment	B
(	O
see	O
,	O
for	O
example	O
,	O
modayil	O
,	O
white	O
,	O
and	O
sutton	O
,	O
2014	O
)	O
.	O
the	O
correspondence	O
between	O
prediction	B
algorithms	O
and	O
classical	O
conditioning	B
rests	O
on	O
their	O
common	O
property	O
of	O
predicting	O
upcoming	O
stimuli	O
,	O
whether	O
or	O
not	O
those	O
stimuli	O
are	O
rewarding	O
(	O
or	O
punishing	O
)	O
.	O
the	O
situation	O
in	O
an	O
instrumental	O
,	O
or	O
operant	O
,	O
conditioning	B
experiment	O
is	O
diﬀerent	O
.	O
here	O
,	O
the	O
experimental	O
apparatus	O
is	O
set	O
up	O
so	O
that	O
an	O
animal	O
is	O
given	O
something	O
it	O
likes	O
(	O
a	O
reward	O
)	O
or	O
something	O
it	O
dislikes	O
(	O
a	O
penalty	O
)	O
depending	O
on	O
what	O
the	O
animal	O
did	O
.	O
the	O
animal	O
learns	O
to	O
increase	O
its	O
tendency	O
to	O
produce	O
rewarded	O
behavior	O
and	O
to	O
decrease	O
its	O
tendency	O
to	O
produce	O
penalized	O
behavior	O
.	O
the	O
reinforcing	O
stimulus	O
is	O
said	O
to	O
be	O
contin-	O
gent	O
on	O
the	O
animal	O
’	O
s	O
behavior	O
,	O
whereas	O
in	O
classical	O
conditioning	B
it	O
is	O
not	O
(	O
although	O
it	O
is	O
diﬃcult	O
to	O
remove	O
all	O
behavior	O
contingencies	O
in	O
a	O
classical	B
conditioning	I
experiment	O
)	O
.	O
instrumental	B
conditioning	I
experiments	O
are	O
like	O
those	O
that	O
inspired	O
thorndike	O
’	O
s	O
law	O
of	O
eﬀect	O
that	O
we	O
brieﬂy	O
discuss	O
in	O
chapter	O
1.	O
control	B
is	O
at	O
the	O
core	O
of	O
this	O
form	O
of	O
learn-	O
ing	B
,	O
which	O
corresponds	O
to	O
the	O
operation	O
of	O
reinforcement	O
learning	O
’	O
s	O
policy-improvement	O
14.2.	O
classical	B
conditioning	I
347	O
algorithms.1	O
thinking	O
of	O
classical	O
conditioning	B
in	O
terms	O
of	O
prediction	O
,	O
and	O
instrumental	O
condition-	O
ing	B
in	O
terms	O
of	O
control	O
,	O
is	O
a	O
starting	O
point	O
for	O
connecting	O
our	O
computational	O
view	O
of	O
rein-	O
forcement	O
learning	O
to	O
animal	O
learning	O
,	O
but	O
in	O
reality	O
,	O
the	O
situation	O
is	O
more	O
complicated	O
than	O
this	O
.	O
there	O
is	O
more	O
to	O
classical	B
conditioning	I
than	O
prediction	B
;	O
it	O
also	O
involves	O
action	B
,	O
and	O
so	O
is	O
a	O
mode	O
of	O
control	O
,	O
sometimes	O
called	O
pavlovian	O
control	B
.	O
further	O
,	O
classical	O
and	O
instrumental	B
conditioning	I
interact	O
in	O
interesting	O
ways	O
,	O
with	O
both	O
sorts	O
of	O
learning	O
likely	O
being	O
engaged	O
in	O
most	O
experimental	O
situations	O
.	O
despite	O
these	O
complications	O
,	O
aligning	O
the	O
classical/instrumental	O
distinction	O
with	O
the	O
prediction/control	O
distinction	O
is	O
a	O
convenient	O
ﬁrst	O
approximation	O
in	O
connecting	O
reinforcement	B
learning	I
to	O
animal	O
learning	O
.	O
in	B
psychology	I
,	O
the	O
term	O
reinforcement	O
is	O
used	O
to	O
describe	O
learning	O
in	O
both	O
classical	O
and	O
instrumental	B
conditioning	I
.	O
originally	O
referring	O
only	O
to	O
the	O
strengthening	O
of	O
a	O
pattern	O
of	O
behavior	O
,	O
it	O
is	O
frequently	O
also	O
used	O
for	O
the	O
weakening	O
of	O
a	O
pattern	O
of	O
behavior	O
.	O
a	O
stimulus	O
considered	O
to	O
be	O
the	O
cause	O
of	O
the	O
change	O
in	O
behavior	O
is	O
called	O
a	O
reinforcer	O
,	O
whether	O
or	O
not	O
it	O
is	O
contingent	O
on	O
the	O
animal	O
’	O
s	O
previous	O
behavior	O
.	O
at	O
the	O
end	O
of	O
this	O
chapter	O
we	O
discuss	O
this	O
terminology	O
in	O
more	O
detail	O
and	O
how	O
it	O
relates	O
to	O
terminology	O
used	O
in	O
machine	O
learning	O
.	O
14.2	O
classical	B
conditioning	I
while	O
studying	O
the	O
activity	O
of	O
the	O
digestive	O
system	O
,	O
the	O
celebrated	O
russian	O
physiologist	O
ivan	O
pavlov	O
found	O
that	O
an	O
animal	O
’	O
s	O
innate	O
responses	O
to	O
certain	O
triggering	O
stimuli	O
can	O
come	O
to	O
be	O
triggered	O
by	O
other	O
stimuli	O
that	O
are	O
quite	O
unrelated	O
to	O
the	O
inborn	O
triggers	O
.	O
his	O
experimental	O
subjects	O
were	O
dogs	O
that	O
had	O
undergone	O
minor	O
surgery	O
to	O
allow	O
the	O
intensity	O
of	O
their	O
salivary	O
reﬂex	O
to	O
be	O
accurately	O
measured	O
.	O
in	O
one	O
case	O
he	O
describes	O
,	O
the	O
dog	O
did	O
not	O
salivate	O
under	O
most	O
circumstances	O
,	O
but	O
about	O
5	O
seconds	O
after	O
being	O
presented	O
with	O
food	O
it	O
produced	O
about	O
six	O
drops	O
of	O
saliva	O
over	O
the	O
next	O
several	O
seconds	O
.	O
after	O
several	O
repetitions	O
of	O
presenting	O
another	O
stimulus	O
,	O
one	O
not	O
related	O
to	O
food	O
,	O
in	O
this	O
case	O
the	O
sound	O
of	O
a	O
metronome	O
,	O
shortly	O
before	O
the	O
introduction	O
of	O
food	O
,	O
the	O
dog	O
salivated	O
in	O
response	O
to	O
the	O
sound	O
of	O
the	O
metronome	O
in	O
the	O
same	O
way	O
it	O
did	O
to	O
the	O
food	O
.	O
“	O
the	O
activity	O
of	O
the	O
salivary	O
gland	O
has	O
thus	O
been	O
called	O
into	O
play	O
by	O
impulses	O
of	O
sound—a	O
stimulus	O
quite	O
alien	O
to	O
food	O
”	O
(	O
pavlov	O
,	O
1927	O
,	O
p.	O
22	O
)	O
.	O
summarizing	O
the	O
signiﬁcance	O
of	O
this	O
ﬁnding	O
,	O
pavlov	O
wrote	O
:	O
it	O
is	O
pretty	O
evident	O
that	O
under	O
natural	O
conditions	O
the	O
normal	O
animal	O
must	O
respond	O
not	O
only	O
to	O
stimuli	O
which	O
themselves	O
bring	O
immediate	O
beneﬁt	O
or	O
harm	O
,	O
but	O
also	O
to	O
other	O
physical	O
or	O
chemical	O
agencies—waves	O
of	O
sound	O
,	O
light	O
,	O
and	O
the	O
like—which	O
in	O
themselves	O
only	O
signal	O
the	O
approach	O
of	O
these	O
stimuli	O
;	O
though	O
it	O
is	O
not	O
the	O
sight	O
and	O
sound	O
of	O
the	O
beast	O
of	O
prey	O
which	O
is	O
in	O
itself	O
harmful	O
to	O
the	O
smaller	O
animal	O
,	O
but	O
its	O
teeth	O
and	O
claws	O
.	O
(	O
pavlov	O
,	O
1927	O
,	O
p.	O
14	O
)	O
1what	O
control	B
means	O
for	O
us	O
is	O
diﬀerent	O
from	O
what	O
it	O
typically	O
means	O
in	B
animal	I
learning	I
theories	O
;	O
there	O
the	O
environment	B
controls	O
the	O
agent	O
instead	O
of	O
the	O
other	O
way	O
around	O
.	O
see	O
our	O
comments	O
on	O
terminology	O
at	O
the	O
end	O
of	O
this	O
chapter	O
.	O
348	O
chapter	O
14	O
:	O
psychology	B
connecting	O
new	O
stimuli	O
to	O
innate	O
reﬂexes	O
in	O
this	O
way	O
is	O
now	O
called	O
classical	O
,	O
or	O
pavlo-	O
vian	O
,	O
conditioning	B
.	O
pavlov	O
(	O
or	O
more	O
exactly	O
,	O
his	O
translators	O
)	O
called	O
inborn	O
responses	O
(	O
e.g.	O
,	O
salivation	O
in	O
his	O
demonstration	O
described	O
above	O
)	O
“	O
unconditioned	O
responses	O
”	O
(	O
urs	O
)	O
,	O
their	O
natural	O
triggering	O
stimuli	O
(	O
e.g.	O
,	O
food	O
)	O
“	O
unconditioned	O
stimuli	O
”	O
(	O
uss	O
)	O
,	O
and	O
new	O
responses	O
triggered	O
by	O
predictive	O
stimuli	O
(	O
e.g.	O
,	O
here	O
also	O
salivation	O
)	O
“	O
conditioned	O
responses	O
”	O
(	O
crs	O
)	O
.	O
a	O
stimulus	O
that	O
is	O
initially	O
neutral	O
,	O
meaning	O
that	O
it	O
does	O
not	O
normally	O
elicit	O
strong	O
re-	O
sponses	O
(	O
e.g.	O
,	O
the	O
metronome	O
sound	O
)	O
,	O
becomes	O
a	O
“	O
conditioned	O
stimulus	O
”	O
(	O
cs	O
)	O
as	O
the	O
animal	O
learns	O
that	O
it	O
predicts	O
the	O
us	O
and	O
so	O
comes	O
to	O
produce	O
a	O
cr	O
in	O
response	O
to	O
the	O
cs	O
.	O
these	O
terms	O
are	O
still	O
used	O
in	O
describing	O
classical	B
conditioning	I
experiments	O
(	O
though	O
better	O
translations	O
would	O
have	O
been	O
“	O
conditional	O
”	O
and	O
“	O
unconditional	O
”	O
instead	O
of	O
condi-	O
tioned	O
and	O
unconditioned	O
)	O
.	O
the	O
us	O
is	O
called	O
a	O
reinforcer	O
because	O
it	O
reinforces	O
producing	O
a	O
cr	O
in	O
response	O
to	O
the	O
cs	O
.	O
the	O
arrangement	O
of	O
stimuli	O
in	O
two	O
common	O
types	O
of	O
classical	O
condition-	O
ing	B
experiments	O
is	O
shown	O
to	O
the	O
right	O
.	O
in	O
delay	O
conditioning	B
,	O
the	O
cs	O
extends	O
throughout	O
the	O
interstimulus	O
interval	O
,	O
or	O
isi	O
,	O
which	O
is	O
the	O
time	O
interval	O
be-	O
tween	O
the	O
cs	O
onset	O
and	O
the	O
us	O
onset	O
(	O
with	O
the	O
cs	O
ending	O
when	O
the	O
us	O
ends	O
in	O
a	O
common	O
version	O
shown	O
here	O
)	O
.	O
in	O
trace	O
conditioning	B
,	O
the	O
us	O
begins	O
after	O
the	O
cs	O
ends	O
,	O
and	O
the	O
time	O
interval	O
be-	O
tween	O
cs	O
oﬀset	O
and	O
us	O
onset	O
is	O
called	O
the	O
trace	O
interval	O
.	O
the	O
salivation	O
of	O
pavlov	O
’	O
s	O
dogs	O
to	O
the	O
sound	O
of	O
a	O
metronome	O
is	O
just	O
one	O
exam-	O
ple	O
of	O
classical	O
conditioning	B
,	O
which	O
has	O
been	O
intensively	O
studied	O
across	O
many	O
response	O
systems	O
of	O
many	O
species	O
of	O
animals	O
.	O
urs	O
are	O
often	O
preparatory	O
in	O
some	O
way	O
,	O
like	O
the	O
salivation	O
of	O
pavlov	O
’	O
s	O
dog	O
,	O
or	O
protective	O
in	O
some	O
way	O
,	O
like	O
an	O
eye	O
blink	O
in	O
response	O
to	O
something	O
irritating	O
to	O
the	O
eye	O
,	O
or	O
freezing	O
in	O
response	O
to	O
seeing	O
a	O
predator	O
.	O
experiencing	O
the	O
cs-us	O
predictive	O
relationship	O
over	O
a	O
series	O
of	O
trials	O
causes	O
the	O
animal	O
to	O
learn	O
that	O
the	O
cs	O
predicts	O
the	O
us	O
so	O
that	O
the	O
animal	O
can	O
respond	O
to	O
the	O
cs	O
with	O
a	O
cr	O
that	O
prepares	O
the	O
animal	O
for	O
,	O
or	O
protects	O
it	O
from	O
,	O
the	O
predicted	O
us	O
.	O
some	O
crs	O
are	O
similar	O
to	O
the	O
ur	O
but	O
begin	O
earlier	O
and	O
diﬀer	O
in	O
ways	O
that	O
increase	O
their	O
eﬀectiveness	O
.	O
in	O
one	O
intensively	O
studied	O
type	O
of	O
experiment	O
,	O
for	O
example	O
,	O
a	O
tone	O
cs	O
reliably	O
predicts	O
a	O
puﬀ	O
of	O
air	O
(	O
the	O
us	O
)	O
to	O
a	O
rabbit	O
’	O
s	O
eye	O
,	O
triggering	O
a	O
ur	O
consisting	O
of	O
the	O
closure	O
of	O
a	O
protective	O
inner	O
eyelid	O
called	O
the	O
nictitating	O
membrane	O
.	O
after	O
one	O
or	O
more	O
trials	O
,	O
the	O
tone	O
comes	O
to	O
trigger	O
a	O
cr	O
consisting	O
of	O
membrane	O
closure	O
that	O
begins	O
before	O
the	O
air	O
puﬀ	O
and	O
eventually	O
becomes	O
timed	O
so	O
that	O
peak	O
closure	O
occurs	O
just	O
when	O
the	O
air	O
puﬀ	O
is	O
likely	O
to	O
occur	O
.	O
this	O
cr	O
,	O
being	O
initiated	O
in	O
anticipation	O
of	O
the	O
air	O
puﬀ	O
and	O
appropriately	O
timed	O
,	O
oﬀers	O
better	O
protection	O
than	O
simply	O
initiating	O
closure	O
as	O
a	O
reaction	O
to	O
the	O
irritating	O
us	O
.	O
the	O
ability	O
to	O
act	O
in	O
anticipation	O
of	O
important	O
events	O
by	O
learning	O
about	O
predictive	O
relationships	O
among	O
stimuli	O
is	O
so	O
beneﬁcial	O
that	O
it	O
is	O
widely	O
present	O
across	O
the	O
animal	O
kingdom	O
.	O
ttrace	O
conditioningdelay	O
conditioningcsuscsusisi	O
14.2.	O
classical	B
conditioning	I
349	O
14.2.1	O
blocking	B
and	O
higher-order	O
conditioning	B
many	O
interesting	O
properties	O
of	O
classical	O
conditioning	B
have	O
been	O
observed	O
in	O
experiments	O
.	O
beyond	O
the	O
anticipatory	O
nature	O
of	O
crs	O
,	O
two	O
widely	O
observed	O
properties	O
ﬁgured	O
promi-	O
nently	O
in	O
the	O
development	O
of	O
classical	O
conditioning	B
models	O
:	O
blocking	B
and	O
higher-order	O
conditioning	B
.	O
blocking	B
occurs	O
when	O
an	O
animal	O
fails	O
to	O
learn	O
a	O
cr	O
when	O
a	O
potential	O
cs	O
is	O
presented	O
along	O
with	O
another	O
cs	O
that	O
had	O
been	O
used	O
previously	O
to	O
condition	O
the	O
animal	O
to	O
produce	O
that	O
cr	O
.	O
for	O
example	O
,	O
in	O
the	O
ﬁrst	O
stage	O
of	O
a	O
blocking	B
experiment	O
involving	O
rabbit	O
nictitating	O
membrane	O
conditioning	B
,	O
a	O
rabbit	O
is	O
ﬁrst	O
conditioned	O
with	O
a	O
tone	O
cs	O
and	O
an	O
air	O
puﬀ	O
us	O
to	O
produce	O
the	O
cr	O
of	O
closing	O
its	O
nictitating	O
membrane	O
in	O
anticipation	O
of	O
the	O
air	O
puﬀ	O
.	O
the	O
experiment	O
’	O
s	O
second	O
stage	O
consists	O
of	O
additional	O
trials	O
in	O
which	O
a	O
second	O
stimulus	O
,	O
say	O
a	O
light	O
,	O
is	O
added	O
to	O
the	O
tone	O
to	O
form	O
a	O
compound	B
tone/light	O
cs	O
followed	O
by	O
the	O
same	O
air	O
puﬀ	O
us	O
.	O
in	O
the	O
experiment	O
’	O
s	O
third	O
phase	O
,	O
the	O
second	O
stimulus	O
alone—the	O
light—is	O
presented	O
to	O
the	O
rabbit	O
to	O
see	O
if	O
the	O
rabbit	O
has	O
learned	O
to	O
respond	O
to	O
it	O
with	O
a	O
cr	O
.	O
it	O
turns	O
out	O
that	O
the	O
rabbit	O
produces	O
very	O
few	O
,	O
or	O
no	O
,	O
crs	O
in	O
response	O
to	O
the	O
light	O
:	O
learning	O
to	O
the	O
light	O
had	O
been	O
blocked	O
by	O
the	O
previous	O
learning	O
to	O
the	O
tone.2	O
blocking	B
results	O
like	O
this	O
challenged	O
the	O
idea	O
that	O
conditioning	B
depends	O
only	O
on	O
simple	O
temporal	O
contiguity	O
,	O
that	O
is	O
,	O
that	O
a	O
necessary	O
and	O
suﬃcient	O
condition	O
for	O
conditioning	O
is	O
that	O
a	O
us	O
frequently	O
follows	O
a	O
cs	O
closely	O
in	O
time	O
.	O
in	O
the	O
next	O
section	O
we	O
describe	O
the	O
rescorla–wagner	O
model	O
(	O
rescorla	O
and	O
wagner	O
,	O
1972	O
)	O
that	O
oﬀered	O
an	O
inﬂuential	O
explanation	O
for	O
blocking	O
.	O
higher-order	O
conditioning	B
occurs	O
when	O
a	O
previously-conditioned	O
cs	O
acts	O
as	O
a	O
us	O
in	O
conditioning	O
another	O
initially	O
neutral	O
stimulus	O
.	O
pavlov	O
described	O
an	O
experiment	O
in	O
which	O
his	O
assistant	O
ﬁrst	O
conditioned	O
a	O
dog	O
to	O
salivate	O
to	O
the	O
sound	O
of	O
a	O
metronome	O
that	O
predicted	O
a	O
food	O
us	O
,	O
as	O
described	O
above	O
.	O
after	O
this	O
stage	O
of	O
conditioning	O
,	O
a	O
number	O
of	O
trials	O
were	O
conducted	O
in	O
which	O
a	O
black	O
square	O
,	O
to	O
which	O
the	O
dog	O
was	O
initially	O
indiﬀerent	O
,	O
was	O
placed	O
in	O
the	O
dog	O
’	O
s	O
line	O
of	O
vision	O
followed	O
by	O
the	O
sound	O
of	O
the	O
metronome—and	O
this	O
was	O
not	O
followed	O
by	O
food	O
.	O
in	O
just	O
ten	O
trials	O
,	O
the	O
dog	O
began	O
to	O
salivate	O
merely	O
upon	O
seeing	O
the	O
black	O
square	O
,	O
despite	O
the	O
fact	O
that	O
the	O
sight	O
of	O
it	O
had	O
never	O
been	O
followed	O
by	O
food	O
.	O
the	O
sound	O
of	O
the	O
metronome	O
itself	O
acted	O
as	O
a	O
us	O
in	O
conditioning	O
a	O
salivation	O
cr	O
to	O
the	O
black	O
square	O
cs	O
.	O
this	O
was	O
second-order	O
conditioning	B
.	O
if	O
the	O
black	O
square	O
had	O
been	O
used	O
as	O
a	O
us	O
to	O
establish	O
salivation	O
crs	O
to	O
another	O
otherwise	O
neutral	O
cs	O
,	O
it	O
would	O
have	O
been	O
third-order	O
conditioning	B
,	O
and	O
so	O
on	O
.	O
higher-order	O
conditioning	B
is	O
diﬃcult	O
to	O
demonstrate	O
,	O
especially	O
above	O
the	O
second	O
order	O
,	O
in	O
part	O
because	O
a	O
higher-order	O
reinforcer	O
loses	O
its	O
reinforcing	O
value	B
due	O
to	O
not	O
being	O
repeatedly	O
followed	O
by	O
the	O
original	O
us	O
during	O
higher-order	O
conditioning	B
trials	O
.	O
but	O
under	O
the	O
right	O
conditions	O
,	O
such	O
as	O
intermixing	O
ﬁrst-order	O
trials	O
with	O
higher-order	O
trials	O
or	O
by	O
providing	O
a	O
general	O
energizing	O
stimulus	O
,	O
higher-order	O
conditioning	B
beyond	O
the	O
second	O
order	O
can	O
be	O
demonstrated	O
.	O
as	O
we	O
describe	O
below	O
,	O
the	O
td	O
model	O
of	O
classical	O
conditioning	B
uses	O
the	O
bootstrapping	B
idea	O
that	O
is	O
central	O
to	O
our	O
approach	O
to	O
extend	O
the	O
rescorla–wagner	O
model	O
’	O
s	O
account	O
of	O
blocking	O
to	O
include	O
both	O
the	O
anticipatory	O
nature	O
of	O
crs	O
and	B
higher-order	I
conditioning	I
.	O
2comparison	O
with	O
a	O
control	B
group	O
is	O
necessary	O
to	O
show	O
that	O
the	O
previous	O
conditioning	B
to	O
the	O
tone	O
is	O
responsible	O
for	O
blocking	O
learning	O
to	O
the	O
light	O
.	O
this	O
is	O
done	O
by	O
trials	O
with	O
the	O
tone/light	O
cs	O
but	O
with	O
no	O
prior	O
conditioning	O
to	O
the	O
tone	O
.	O
learning	O
to	O
the	O
light	O
in	O
this	O
case	O
is	O
unimpaired	O
.	O
moore	O
and	O
schmajuk	O
(	O
2008	O
)	O
give	O
a	O
full	O
account	O
of	O
this	O
procedure	O
.	O
350	O
chapter	O
14	O
:	O
psychology	B
higher-order	O
instrumental	B
conditioning	I
occurs	O
as	O
well	O
.	O
in	O
this	O
case	O
,	O
a	O
stimulus	O
that	O
consistently	O
predicts	O
primary	O
reinforcement	O
becomes	O
a	O
reinforcer	O
itself	O
,	O
where	O
reinforce-	O
ment	O
is	O
primary	O
if	O
its	O
rewarding	O
or	O
penalizing	O
quality	O
has	O
been	O
built	O
into	O
the	O
animal	O
by	O
evolution	B
.	O
the	O
predicting	O
stimulus	O
becomes	O
a	O
secondary	O
reinforcer	O
,	O
or	O
more	O
generally	O
,	O
a	O
higher-order	O
or	O
conditioned	O
reinforcer	O
—the	O
latter	O
being	O
a	O
better	O
term	O
when	O
the	O
pre-	O
dicted	O
reinforcing	O
stimulus	O
is	O
itself	O
a	O
secondary	O
,	O
or	O
an	O
even	O
higher-order	O
,	O
reinforcer	O
.	O
a	O
conditioned	O
reinforcer	O
delivers	O
conditioned	O
reinforcement	O
:	O
conditioned	O
reward	O
or	O
condi-	O
tioned	O
penalty	O
.	O
conditioned	O
reinforcement	O
acts	O
like	O
primary	O
reinforcement	O
in	O
increasing	O
an	O
animal	O
’	O
s	O
tendency	O
to	O
produce	O
behavior	O
that	O
leads	O
to	O
conditioned	O
reward	O
,	O
and	O
to	O
de-	O
crease	O
an	O
animal	O
’	O
s	O
tendency	O
to	O
produce	O
behavior	O
that	O
leads	O
to	O
conditioned	O
penalty	O
.	O
(	O
see	O
our	O
comments	O
at	O
the	O
end	O
of	O
this	O
chapter	O
that	O
explain	O
how	O
our	O
terminology	O
sometimes	O
diﬀers	O
,	O
as	O
it	O
does	O
here	O
,	O
from	O
terminology	O
used	O
in	B
psychology	I
.	O
)	O
conditioned	O
reinforcement	O
is	O
a	O
key	O
phenomenon	O
that	O
explains	O
,	O
for	O
instance	O
,	O
why	O
we	O
work	O
for	O
the	O
conditioned	O
reinforcer	O
money	O
,	O
whose	O
worth	O
derives	O
solely	O
from	O
what	O
is	O
pre-	O
dicted	O
by	O
having	O
it	O
.	O
in	O
actor–critic	O
methods	O
described	O
in	O
section	O
13.5	O
(	O
and	O
discussed	O
in	O
the	O
context	O
of	O
neuroscience	O
in	O
sections	O
15.7	O
and	O
15.8	O
)	O
,	O
the	O
critic	B
uses	O
a	O
td	O
method	O
to	O
evaluate	O
the	O
actor	O
’	O
s	O
policy	B
,	O
and	O
its	O
value	B
estimates	O
provide	O
conditioned	O
reinforcement	O
to	O
the	O
actor	O
,	O
allowing	O
the	O
actor	O
to	O
improve	O
its	O
policy	B
.	O
this	O
analog	O
of	O
higher-order	O
instrumen-	O
tal	O
conditioning	B
helps	O
address	O
the	O
credit-assignment	O
problem	O
mentioned	O
in	O
section	O
1.7	O
because	O
the	O
critic	B
gives	O
moment-by-moment	O
reinforcement	O
to	O
the	O
actor	O
when	O
the	O
primary	O
reward	B
signal	I
is	O
delayed	O
.	O
we	O
discuss	O
this	O
more	O
below	O
in	O
section	O
14.4	O
.	O
14.2.2	O
the	O
rescorla–wagner	O
model	O
rescorla	O
and	O
wagner	O
created	O
their	O
model	O
mainly	O
to	O
account	O
for	O
blocking	O
.	O
the	O
core	O
idea	O
of	O
the	O
rescorla–wagner	O
model	O
is	O
that	O
an	O
animal	O
only	O
learns	O
when	O
events	O
violate	O
its	O
expectations	O
,	O
in	O
other	O
words	O
,	O
only	O
when	O
the	O
animal	O
is	O
surprised	O
(	O
although	O
without	O
nec-	O
essarily	O
implying	O
any	O
conscious	O
expectation	O
or	O
emotion	O
)	O
.	O
we	O
ﬁrst	O
present	O
rescorla	O
and	O
wagner	O
’	O
s	O
model	O
using	O
their	O
terminology	O
and	O
notation	O
before	O
shifting	O
to	O
the	O
terminology	O
and	O
notation	O
we	O
use	O
to	O
describe	O
the	O
td	O
model	O
.	O
here	O
is	O
how	O
rescorla	O
and	O
wagner	O
described	O
their	O
model	O
.	O
the	O
model	O
adjusts	O
the	O
“	O
as-	O
sociative	O
strength	O
”	O
of	O
each	O
component	O
stimulus	O
of	O
a	O
compound	B
cs	O
,	O
which	O
is	O
a	O
number	O
representing	O
how	O
strongly	O
or	O
reliably	O
that	O
component	O
is	O
predictive	O
of	O
a	O
us	O
.	O
when	O
a	O
compound	B
cs	O
consisting	O
of	O
several	O
component	O
stimuli	O
is	O
presented	O
in	O
a	O
classical	O
condi-	O
tioning	O
trial	O
,	O
the	O
associative	O
strength	O
of	O
each	O
component	O
stimulus	O
changes	O
in	O
a	O
way	O
that	O
depends	O
on	O
an	O
associative	O
strength	O
associated	O
with	O
the	O
entire	O
stimulus	O
compound	B
,	O
called	O
the	O
“	O
aggregate	O
associative	O
strength	O
,	O
”	O
and	O
not	O
just	O
on	O
the	O
associative	O
strength	O
of	O
each	O
component	O
itself	O
.	O
rescorla	O
and	O
wagner	O
considered	O
a	O
compound	B
cs	O
ax	O
,	O
consisting	O
of	O
component	O
stimuli	O
a	O
and	O
x	O
,	O
where	O
the	O
animal	O
may	O
have	O
already	O
experienced	O
stimulus	O
a	O
,	O
and	O
stimulus	O
x	O
might	O
be	O
new	O
to	O
the	O
animal	O
.	O
let	O
va	O
,	O
vx	O
,	O
and	O
vax	O
respectively	O
denote	O
the	O
associative	O
strengths	O
of	O
stimuli	O
a	O
,	O
x	O
,	O
and	O
the	O
compound	O
ax	O
.	O
suppose	O
that	O
on	O
a	O
trial	O
the	O
compound	B
cs	O
ax	O
is	O
followed	O
by	O
a	O
us	O
,	O
which	O
we	O
label	O
stimulus	O
y.	O
then	O
the	O
associative	O
strengths	O
14.2.	O
classical	B
conditioning	I
351	O
of	O
the	O
stimulus	O
components	O
change	O
according	O
to	O
these	O
expressions	O
:	O
∆va	O
=	O
αaβy	O
(	O
ry	O
−	O
vax	O
)	O
∆vx	O
=	O
αxβy	O
(	O
ry	O
−	O
vax	O
)	O
,	O
where	O
αaβy	O
and	O
αxβy	O
are	O
the	O
step-size	O
parameters	O
,	O
which	O
depend	O
on	O
the	O
identities	O
of	O
the	O
cs	O
components	O
and	O
the	O
us	O
,	O
and	O
ry	O
is	O
the	O
asymptotic	O
level	O
of	O
associative	O
strength	O
that	O
the	O
us	O
y	O
can	O
support	O
.	O
(	O
rescorla	O
and	O
wagner	O
used	O
λ	O
here	O
instead	O
of	O
r	O
,	O
but	O
we	O
use	O
r	O
to	O
avoid	O
confusion	O
with	O
our	O
use	O
of	O
λ	O
and	O
because	O
we	O
usually	O
think	O
of	O
this	O
as	O
the	O
magnitude	O
of	O
a	O
reward	B
signal	I
,	O
with	O
the	O
caveat	O
that	O
the	O
us	O
in	O
classical	O
conditioning	B
is	O
not	O
necessarily	O
rewarding	O
or	O
penalizing	O
.	O
)	O
a	O
key	O
assumption	O
of	O
the	O
model	O
is	O
that	O
the	O
aggregate	O
associative	O
strength	O
vax	O
is	O
equal	O
to	O
va	O
+	O
vx	O
.	O
the	O
associative	O
strengths	O
as	O
changed	O
by	O
these	O
∆s	O
become	O
the	O
associative	O
strengths	O
at	O
the	O
beginning	O
of	O
the	O
next	O
trial	O
.	O
to	O
be	O
complete	O
,	O
the	O
model	O
needs	O
a	O
response-generation	O
mechanism	O
,	O
which	O
is	O
a	O
way	O
of	O
mapping	O
values	O
of	O
v	O
s	O
to	O
crs	O
.	O
because	O
this	O
mapping	O
would	O
depend	O
on	O
details	O
of	O
the	O
experimental	O
situation	O
,	O
rescorla	O
and	O
wagner	O
did	O
not	O
specify	O
a	O
mapping	O
but	O
simply	O
assumed	O
that	O
larger	O
v	O
s	O
would	O
produce	O
stronger	O
or	O
more	O
likely	O
crs	O
,	O
and	O
that	O
negative	O
v	O
s	O
would	O
mean	O
that	O
there	O
would	O
be	O
no	O
crs	O
.	O
the	O
rescorla–wagner	O
model	O
accounts	O
for	O
the	O
acquisition	O
of	O
crs	O
in	O
a	O
way	O
that	O
ex-	O
plains	O
blocking	B
.	O
as	O
long	O
as	O
the	O
aggregate	O
associative	O
strength	O
,	O
vax	O
,	O
of	O
the	O
stimulus	O
compound	B
is	O
below	O
the	O
asymptotic	O
level	O
of	O
associative	O
strength	O
,	O
ry	O
,	O
that	O
the	O
us	O
y	O
can	O
support	O
,	O
the	O
prediction	B
error	O
ry	O
−	O
vax	O
is	O
positive	O
.	O
this	O
means	O
that	O
over	O
successive	O
trials	O
the	O
associative	O
strengths	O
va	O
and	O
vx	O
of	O
the	O
component	O
stimuli	O
increase	O
until	O
the	O
aggregate	O
associative	O
strength	O
vax	O
equals	O
ry	O
,	O
at	O
which	O
point	O
the	O
associative	O
strengths	O
stop	O
changing	O
(	O
unless	O
the	O
us	O
changes	O
)	O
.	O
when	O
a	O
new	O
component	O
is	O
added	O
to	O
a	O
compound	B
cs	O
to	O
which	O
the	O
animal	O
has	O
already	O
been	O
conditioned	O
,	O
further	O
conditioning	B
with	O
the	O
aug-	O
mented	O
compound	B
produces	O
little	O
or	O
no	O
increase	O
in	O
the	O
associative	O
strength	O
of	O
the	O
added	O
cs	O
component	O
because	O
the	O
error	O
has	O
already	O
been	O
reduced	O
to	O
zero	O
,	O
or	O
to	O
a	O
low	O
value	B
.	O
the	O
occurrence	O
of	O
the	O
us	O
is	O
already	O
predicted	O
nearly	O
perfectly	O
,	O
so	O
little	O
or	O
no	O
error—or	O
surprise—is	O
introduced	O
by	O
the	O
new	O
cs	O
component	O
.	O
prior	O
learning	O
blocks	O
learning	O
to	O
the	O
new	O
component	O
.	O
to	O
transition	O
from	O
rescorla	O
and	O
wagner	O
’	O
s	O
model	O
to	O
the	O
td	O
model	O
of	O
classical	O
condi-	O
tioning	O
(	O
which	O
we	O
just	O
call	O
the	O
td	O
model	O
)	O
,	O
we	O
ﬁrst	O
recast	O
their	O
model	O
in	O
terms	O
of	O
the	O
concepts	O
that	O
we	O
are	O
using	O
throughout	O
this	O
book	O
.	O
speciﬁcally	O
,	O
we	O
match	O
the	O
notation	O
we	O
use	O
for	O
learning	O
with	O
linear	O
function	B
approximation	I
(	O
section	O
9.4	O
)	O
,	O
and	O
we	O
think	O
of	O
the	O
conditioning	B
process	O
as	O
one	O
of	O
learning	O
to	O
predict	O
the	O
“	O
magnitude	O
of	O
the	O
us	O
”	O
on	O
a	O
trial	O
on	O
the	O
basis	O
of	O
the	O
compound	B
cs	O
presented	O
on	O
that	O
trial	O
,	O
where	O
the	O
magnitude	O
of	O
a	O
us	O
y	O
is	O
the	O
ry	O
of	O
the	O
rescorla–wagner	O
model	O
as	O
given	O
above	O
.	O
we	O
also	O
introduce	O
states	O
.	O
because	O
the	O
rescorla–wagner	O
model	O
is	O
a	O
trial-level	O
model	O
,	O
meaning	O
that	O
it	O
deals	O
with	O
how	O
associative	O
strengths	O
change	O
from	O
trial	O
to	O
trial	O
without	O
considering	O
any	O
details	O
about	O
what	O
happens	O
within	O
and	O
between	O
trials	O
,	O
we	O
do	O
not	O
have	O
to	O
consider	O
how	O
states	O
change	O
during	O
a	O
trial	O
until	O
we	O
present	O
the	O
full	O
td	O
model	O
in	O
the	O
following	O
section	O
.	O
instead	O
,	O
here	O
we	O
simply	O
think	O
of	O
a	O
state	B
as	O
a	O
way	O
of	O
labeling	O
a	O
trial	O
in	O
terms	O
of	O
the	O
collection	O
of	O
component	O
css	O
that	O
are	O
present	O
on	O
the	O
trial	O
.	O
therefore	O
,	O
assume	O
that	O
trial-type	O
,	O
or	O
state	B
,	O
s	O
is	O
described	O
by	O
a	O
real-valued	O
vector	B
of	O
352	O
chapter	O
14	O
:	O
psychology	B
features	O
x	O
(	O
s	O
)	O
=	O
(	O
x1	O
(	O
s	O
)	O
,	O
x2	O
(	O
s	O
)	O
,	O
.	O
.	O
.	O
,	O
xd	O
(	O
s	O
)	O
)	O
(	O
cid:62	O
)	O
where	O
xi	O
(	O
s	O
)	O
=	O
1	O
if	O
csi	O
,	O
the	O
ith	O
component	O
of	O
a	O
compound	B
cs	O
,	O
is	O
present	O
on	O
the	O
trial	O
and	O
0	O
otherwise	O
.	O
then	O
if	O
the	O
d-dimensional	O
vector	B
of	O
associative	O
strengths	O
is	O
w	O
,	O
the	O
aggregate	O
associative	O
strength	O
for	O
trial-type	O
s	O
is	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
=	O
w	O
(	O
cid:62	O
)	O
x	O
(	O
s	O
)	O
.	O
(	O
14.1	O
)	O
this	O
corresponds	O
to	O
a	O
value	B
estimate	O
in	O
reinforcement	O
learning	O
,	O
and	O
we	O
think	O
of	O
it	O
as	O
the	O
us	O
prediction	B
.	O
now	O
temporally	O
let	O
t	O
denote	O
the	O
number	O
of	O
a	O
complete	O
trial	O
and	O
not	O
its	O
usual	O
meaning	O
as	O
a	O
time	O
step	O
(	O
we	O
revert	O
to	O
t	O
’	O
s	O
usual	O
meaning	O
when	O
we	O
extend	O
this	O
to	O
the	O
td	O
model	O
below	O
)	O
,	O
and	O
assume	O
that	O
st	O
is	O
the	O
state	B
corresponding	O
to	O
trial	O
t.	O
conditioning	B
trial	O
t	O
updates	O
the	O
associative	O
strength	O
vector	B
wt	O
to	O
wt+1	O
as	O
follows	O
:	O
wt+1	O
=	O
wt	O
+	O
αδt	O
x	O
(	O
st	O
)	O
,	O
(	O
14.2	O
)	O
where	O
α	O
is	O
the	O
step-size	B
parameter	I
,	O
and—because	O
here	O
we	O
are	O
describing	O
the	O
rescorla–	O
wagner	O
model—δt	O
is	O
the	O
prediction	B
error	O
δt	O
=	O
rt	O
−	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
.	O
(	O
14.3	O
)	O
rt	O
is	O
the	O
target	B
of	O
the	O
prediction	B
on	O
trial	O
t	O
,	O
that	O
is	O
,	O
the	O
magnitude	O
of	O
the	O
us	O
,	O
or	O
in	O
rescorla	O
and	O
wagner	O
’	O
s	O
terms	O
,	O
the	O
associative	O
strength	O
that	O
the	O
us	O
on	O
the	O
trial	O
can	O
support	O
.	O
note	O
that	O
because	O
of	O
the	O
factor	O
x	O
(	O
st	O
)	O
in	O
(	O
14.2	O
)	O
,	O
only	O
the	O
associative	O
strengths	O
of	O
cs	O
components	O
present	O
on	O
a	O
trial	O
are	O
adjusted	O
as	O
a	O
result	O
of	O
that	O
trial	O
.	O
you	O
can	O
think	O
of	O
the	O
prediction	B
error	O
as	O
a	O
measure	O
of	O
surprise	O
,	O
and	O
the	O
aggregate	O
associative	O
strength	O
as	O
the	O
animal	O
’	O
s	O
expectation	O
that	O
is	O
violated	O
when	O
it	O
does	O
not	O
match	O
the	O
target	B
us	O
magnitude	O
.	O
from	O
the	O
perspective	O
of	O
machine	O
learning	O
,	O
the	O
rescorla–wagner	O
model	O
is	O
an	O
error-	O
correction	O
supervised	B
learning	I
rule	O
.	O
it	O
is	O
essentially	O
the	O
same	O
as	O
the	O
least	O
mean	O
square	O
(	O
lms	O
)	O
,	O
or	O
widrow-hoﬀ	O
,	O
learning	O
rule	O
(	O
widrow	O
and	O
hoﬀ	O
,	O
1960	O
)	O
that	O
ﬁnds	O
the	O
weights—	O
here	O
the	O
associative	O
strengths—that	O
make	O
the	O
average	O
of	O
the	O
squares	O
of	O
all	O
the	O
errors	O
as	O
close	O
to	O
zero	O
as	O
possible	O
.	O
it	O
is	O
a	O
“	O
curve-ﬁtting	O
,	O
”	O
or	O
regression	O
,	O
algorithm	O
that	O
is	O
widely	O
used	O
in	O
engineering	O
and	O
scientiﬁc	O
applications	O
(	O
see	O
section	O
9.4	O
)	O
.3	O
the	O
rescorla–wagner	O
model	O
was	O
very	O
inﬂuential	O
in	O
the	O
history	O
of	O
animal	O
learning	O
theory	O
because	O
it	O
showed	O
that	O
a	O
“	O
mechanistic	O
”	O
theory	O
could	O
account	O
for	O
the	O
main	O
facts	O
about	O
blocking	B
without	O
resorting	O
to	O
more	O
complex	O
cognitive	O
theories	O
involving	O
,	O
for	O
exam-	O
ple	O
,	O
an	O
animal	O
’	O
s	O
explicit	O
recognition	O
that	O
another	O
stimulus	O
component	O
had	O
been	O
added	O
and	O
then	O
scanning	O
its	O
short-term	O
memory	O
backward	O
to	O
reassess	O
the	O
predictive	O
relation-	O
ships	O
involving	O
the	O
us	O
.	O
the	O
rescorla–wagner	O
model	O
showed	O
how	O
traditional	O
contiguity	O
theories	O
of	O
conditioning—that	O
temporal	O
contiguity	O
of	O
stimuli	O
was	O
a	O
necessary	O
and	O
suﬃ-	O
cient	O
condition	O
for	O
learning—could	O
be	O
adjusted	O
in	O
a	O
simple	O
way	O
to	O
account	O
for	O
blocking	O
(	O
moore	O
and	O
schmajuk	O
,	O
2008	O
)	O
.	O
3the	O
only	O
diﬀerences	O
between	O
the	O
lms	O
rule	O
and	O
the	O
rescorla–wagner	O
model	O
are	O
that	O
for	O
lms	O
the	O
input	O
vectors	O
xt	O
can	O
have	O
any	O
real	O
numbers	O
as	O
components	O
,	O
and—at	O
least	O
in	O
the	O
simplest	O
version	O
of	O
the	O
lms	O
rule—the	O
step-size	B
parameter	I
α	O
does	O
not	O
depend	O
on	O
the	O
input	O
vector	B
or	O
the	O
identity	O
of	O
the	O
stimulus	O
setting	O
the	O
prediction	B
target	O
.	O
14.2.	O
classical	B
conditioning	I
353	O
the	O
rescorla–wagner	O
model	O
provides	O
a	O
simple	O
account	O
of	O
blocking	O
and	O
some	O
other	O
features	O
of	O
classical	O
conditioning	B
,	O
but	O
it	O
is	O
not	O
a	O
complete	O
or	O
perfect	O
model	O
of	O
classical	O
conditioning	B
.	O
diﬀerent	O
ideas	O
account	O
for	O
a	O
variety	O
of	O
other	O
observed	O
eﬀects	O
,	O
and	O
progress	O
is	O
still	O
being	O
made	O
toward	O
understanding	O
the	O
many	O
subtleties	O
of	O
classical	O
conditioning	B
.	O
the	O
td	O
model	O
,	O
which	O
we	O
describe	O
next	O
,	O
though	O
also	O
not	O
a	O
complete	O
or	O
perfect	O
model	O
model	O
of	O
classical	O
conditioning	B
,	O
extends	O
the	O
rescorla–wagner	O
model	O
to	O
address	O
how	O
within-trial	O
and	O
between-trial	O
timing	O
relationships	O
among	O
stimuli	O
can	O
inﬂuence	O
learning	O
and	O
how	O
higher-order	O
conditioning	B
might	O
arise	O
.	O
14.2.3	O
the	O
td	O
model	O
the	O
td	O
model	O
is	O
a	O
real-time	O
model	O
,	O
as	O
opposed	O
to	O
a	O
trial-level	O
model	O
like	O
the	O
rescorla–	O
wagner	O
model	O
.	O
a	O
single	O
step	O
t	O
in	O
the	O
rescorla–wagner	O
model	O
represents	O
an	O
entire	O
conditioning	B
trial	O
.	O
the	O
model	O
does	O
not	O
apply	O
to	O
details	O
about	O
what	O
happens	O
during	O
the	O
time	O
a	O
trial	O
is	O
taking	O
place	O
,	O
or	O
what	O
might	O
happen	O
between	O
trials	O
.	O
within	O
each	O
trial	O
an	O
animal	O
might	O
experience	O
various	O
stimuli	O
whose	O
onsets	O
occur	O
at	O
particular	O
times	O
and	O
that	O
have	O
particular	O
durations	O
.	O
these	O
timing	O
relationships	O
strongly	O
inﬂuence	O
learning	O
.	O
the	O
rescorla–wagner	O
model	O
also	O
does	O
not	O
include	O
a	O
mechanism	O
for	O
higher-order	O
conditioning	B
,	O
whereas	O
for	O
the	O
td	O
model	O
,	O
higher-order	O
conditioning	B
is	O
a	O
natural	O
consequence	O
of	O
the	O
bootstrapping	B
idea	O
that	O
is	O
at	O
the	O
base	O
of	O
td	O
algorithms	O
.	O
to	O
describe	O
the	O
td	O
model	O
we	O
begin	O
with	O
the	O
formulation	O
of	O
the	O
rescorla–wagner	O
model	O
above	O
,	O
but	O
t	O
now	O
labels	O
time	O
steps	O
within	O
or	O
between	O
trials	O
instead	O
of	O
complete	O
trials	O
.	O
think	O
of	O
the	O
time	O
between	O
t	O
and	O
t	O
+	O
1	O
as	O
a	O
small	O
time	O
interval	O
,	O
say	O
.01	O
second	O
,	O
and	O
think	O
of	O
a	O
trial	O
as	O
a	O
sequences	O
of	O
states	O
,	O
one	O
associated	O
with	O
each	O
time	O
step	O
,	O
where	O
the	O
state	B
at	O
step	O
t	O
now	O
represents	O
details	O
of	O
how	O
stimuli	O
are	O
represented	O
at	O
t	O
instead	O
of	O
just	O
a	O
label	O
for	O
the	O
cs	O
components	O
present	O
on	O
a	O
trial	O
.	O
in	O
fact	O
,	O
we	O
can	O
completely	O
abandon	O
the	O
idea	O
of	O
trials	O
.	O
from	O
the	O
point	O
of	O
view	O
of	O
the	O
animal	O
,	O
a	O
trial	O
is	O
just	O
a	O
fragment	O
of	O
its	O
continuing	O
experience	O
interacting	O
with	O
its	O
world	O
.	O
following	O
our	O
usual	O
view	O
of	O
an	O
agent	O
interacting	O
with	O
its	O
environment	B
,	O
imagine	O
that	O
the	O
animal	O
is	O
experiencing	O
an	O
endless	O
sequence	O
of	O
states	O
s	O
,	O
each	O
represented	O
by	O
a	O
feature	O
vector	O
x	O
(	O
s	O
)	O
.	O
that	O
said	O
,	O
it	O
is	O
still	O
often	O
convenient	O
to	O
refer	O
to	O
trials	O
as	O
fragments	O
of	O
time	O
during	O
which	O
patterns	O
of	O
stimuli	O
repeat	O
in	O
an	O
experiment	O
.	O
state	B
features	O
are	O
not	O
restricted	O
to	O
describing	O
the	O
external	O
stimuli	O
that	O
an	O
animal	O
experiences	O
;	O
they	O
can	O
describe	O
neural	B
activity	O
patterns	O
that	O
external	O
stimuli	O
produce	O
in	O
an	O
animal	O
’	O
s	O
brain	O
,	O
and	O
these	O
patterns	O
can	O
be	O
history-dependent	O
,	O
meaning	O
that	O
they	O
can	O
be	O
persistent	O
patterns	O
produced	O
by	O
sequences	O
of	O
external	O
stimuli	O
.	O
of	O
course	O
,	O
we	O
do	O
not	O
know	O
exactly	O
what	O
these	O
neural	B
activity	O
patterns	O
are	O
,	O
but	O
a	O
real-time	O
model	O
like	O
the	O
td	O
model	O
allows	O
one	O
to	O
explore	O
the	O
consequences	O
on	O
learning	O
of	O
diﬀerent	O
hypotheses	O
about	O
the	O
internal	O
representations	O
of	O
external	O
stimuli	O
.	O
for	O
these	O
reasons	O
,	O
the	O
td	O
model	O
does	O
not	O
commit	O
to	O
any	O
particular	O
state	B
representation	O
.	O
in	O
addition	O
,	O
because	O
the	O
td	O
model	O
includes	O
discounting	B
and	O
eligibility	B
traces	I
that	O
span	O
time	O
intervals	O
between	O
stimuli	O
,	O
the	O
model	O
also	O
makes	O
it	O
possible	O
to	O
explore	O
how	O
discounting	B
and	O
eligibility	B
traces	I
interact	O
with	O
stimulus	O
representations	O
in	O
making	O
predictions	O
about	O
the	O
results	O
of	O
classical	O
conditioning	B
experiments	O
.	O
354	O
chapter	O
14	O
:	O
psychology	B
below	O
we	O
describe	O
some	O
of	O
the	O
state	B
representations	O
that	O
have	O
been	O
used	O
with	O
the	O
td	O
model	O
and	O
some	O
of	O
their	O
implications	O
,	O
but	O
for	O
the	O
moment	O
we	O
stay	O
agnostic	O
about	O
the	O
representation	O
and	O
just	O
assume	O
that	O
each	O
state	B
s	O
is	O
represented	O
by	O
a	O
feature	O
vector	O
x	O
(	O
s	O
)	O
=	O
(	O
x1	O
(	O
s	O
)	O
,	O
x2	O
(	O
s	O
)	O
,	O
.	O
.	O
.	O
,	O
xn	O
(	O
s	O
)	O
)	O
(	O
cid:62	O
)	O
.	O
then	O
the	O
aggregate	O
associative	O
strength	O
corresponding	O
to	O
a	O
state	B
s	O
is	O
given	O
by	O
(	O
14.1	O
)	O
,	O
the	O
same	O
as	O
for	O
the	O
rescorla-wgner	O
model	O
,	O
but	O
the	O
td	O
model	O
updates	O
the	O
associative	O
strength	O
vector	B
,	O
w	O
,	O
diﬀerently	O
.	O
with	O
t	O
now	O
labeling	O
a	O
time	O
step	O
instead	O
of	O
a	O
complete	O
trial	O
,	O
the	O
td	O
model	O
governs	O
learning	O
according	O
to	O
this	O
update	O
:	O
wt+1	O
=	O
wt	O
+	O
αδt	O
zt	O
,	O
(	O
14.4	O
)	O
which	O
replaces	O
xt	O
(	O
st	O
)	O
in	O
the	O
rescorla–wagner	O
update	O
(	O
14.2	O
)	O
with	O
zt	O
,	O
a	O
vector	B
of	O
eligibility	B
traces	I
,	O
and	O
instead	O
of	O
the	O
δt	O
of	O
(	O
14.3	O
)	O
,	O
here	O
δt	O
is	O
a	O
td	O
error	O
:	O
δt	O
=	O
rt+1	O
+	O
γˆv	O
(	O
st+1	O
,	O
wt	O
)	O
−	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
,	O
(	O
14.5	O
)	O
where	O
γ	O
is	O
a	O
discount	O
factor	O
(	O
between	O
0	O
and	O
1	O
)	O
,	O
rt	O
is	O
the	O
prediction	B
target	O
at	O
time	O
t	O
,	O
and	O
ˆv	O
(	O
st+1	O
,	O
wt	O
)	O
and	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
are	O
aggregate	O
associative	O
strengths	O
at	O
t	O
+	O
1	O
and	O
t	O
as	O
deﬁned	O
by	O
(	O
14.1	O
)	O
.	O
each	O
component	O
i	O
of	O
the	O
eligibility-trace	O
vector	B
zt	O
increments	O
or	O
decrements	O
according	O
to	O
the	O
component	O
xi	O
(	O
st	O
)	O
of	O
the	O
feature	O
vector	O
x	O
(	O
st	O
)	O
,	O
and	O
otherwise	O
decays	O
with	O
a	O
rate	O
determined	O
by	O
γλ	O
:	O
zt+1	O
=	O
γλzt	O
+	O
x	O
(	O
st	O
)	O
.	O
(	O
14.6	O
)	O
here	O
λ	O
is	O
the	O
usual	O
eligibility	O
trace	O
decay	O
parameter	O
.	O
note	O
that	O
if	O
γ	O
=	O
0	O
,	O
the	O
td	O
model	O
reduces	O
to	O
the	O
rescorla–wagner	O
model	O
with	O
the	O
ex-	O
ceptions	O
that	O
:	O
the	O
meaning	O
of	O
t	O
is	O
diﬀerent	O
in	O
each	O
case	O
(	O
a	O
trial	O
number	O
for	O
the	O
rescorla–	O
wagner	O
model	O
and	O
a	O
time	O
step	O
for	O
the	O
td	O
model	O
)	O
,	O
and	O
in	O
the	O
td	O
model	O
there	O
is	O
a	O
one-time-step	O
lead	O
in	O
the	O
prediction	O
target	B
r.	O
the	O
td	O
model	O
is	O
equivalent	O
to	O
the	O
back-	O
ward	O
view	O
of	O
the	O
semi-gradient	O
td	O
(	O
λ	O
)	O
algorithm	O
with	O
linear	O
function	B
approximation	I
(	O
chapter	O
12	O
)	O
,	O
except	O
that	O
rt	O
in	O
the	O
model	O
does	O
not	O
have	O
to	O
be	O
a	O
reward	B
signal	I
as	O
it	O
does	O
when	O
the	O
td	O
algorithm	O
is	O
used	O
to	O
learn	O
a	O
value	B
function	I
for	O
policy-improvement	O
.	O
14.2.4	O
td	O
model	O
simulations	O
real-time	O
conditioning	O
models	O
like	O
the	O
td	O
model	O
are	O
interesting	O
primarily	O
because	O
they	O
make	O
predictions	O
for	O
a	O
wide	O
range	O
of	O
situations	O
that	O
can	O
not	O
be	O
represented	O
by	O
trial-level	O
models	O
.	O
these	O
situations	O
involve	O
the	O
timing	O
and	O
durations	O
of	O
conditionable	O
stimuli	O
,	O
the	O
timing	O
of	O
these	O
stimuli	O
in	O
relation	O
to	O
the	O
timing	O
of	O
the	O
us	O
,	O
and	O
the	O
timing	O
and	O
shapes	O
of	O
crs	O
.	O
for	O
example	O
,	O
the	O
us	O
generally	O
must	O
begin	O
after	O
the	O
onset	O
of	O
a	O
neutral	O
stimulus	O
for	O
conditioning	O
to	O
occur	O
,	O
with	O
the	O
rate	O
and	O
eﬀectiveness	O
of	O
learning	O
depending	O
on	O
the	O
inter-	O
stimulus	O
interval	O
,	O
or	O
isi	O
,	O
the	O
interval	O
between	O
the	O
onsets	O
of	O
the	O
cs	O
and	O
the	O
us	O
.	O
when	O
crs	O
appear	O
,	O
they	O
generally	O
begin	O
before	O
the	O
appearance	O
of	O
the	O
us	O
and	O
their	O
temporal	O
proﬁles	O
change	O
during	O
learning	O
.	O
in	O
conditioning	O
with	O
compound	O
css	O
,	O
the	O
component	O
stimuli	O
of	O
the	O
compound	B
css	O
may	O
not	O
all	O
begin	O
and	O
end	O
at	O
the	O
same	O
time	O
,	O
sometimes	O
14.2.	O
classical	B
conditioning	I
355	O
forming	O
what	O
is	O
called	O
a	O
serial	O
compound	B
in	O
which	O
the	O
component	O
stimuli	O
occur	O
in	O
a	O
sequence	O
over	O
time	O
.	O
timing	O
considerations	O
like	O
these	O
make	O
it	O
important	O
to	O
consider	O
how	O
stimuli	O
are	O
represented	O
,	O
how	O
these	O
representations	O
unfold	O
over	O
time	O
during	O
and	O
between	O
trials	O
,	O
and	O
how	O
they	O
interact	O
with	O
discounting	O
and	B
eligibility	I
traces	I
.	O
figure	O
14.1	O
shows	O
three	O
of	O
the	O
stimulus	O
representations	O
that	O
have	O
been	O
used	O
in	O
explor-	O
ing	B
the	O
behavior	O
of	O
the	O
td	O
model	O
:	O
the	O
complete	O
serial	O
compound	B
(	O
csc	O
)	O
,	O
the	O
microstim-	O
ulus	O
(	O
ms	O
)	O
,	O
and	O
the	O
presence	O
representations	O
(	O
ludvig	O
,	O
sutton	O
,	O
and	O
kehoe	O
,	O
2012	O
)	O
.	O
these	O
representations	O
diﬀer	O
in	O
the	O
degree	O
to	O
which	O
they	O
force	O
generalization	O
among	O
nearby	O
time	O
points	O
during	O
which	O
a	O
stimulus	O
is	O
present	O
.	O
the	O
simplest	O
of	O
the	O
representations	O
shown	O
in	O
figure	O
14.1	O
is	O
the	O
presence	O
representation	O
in	O
the	O
ﬁgure	O
’	O
s	O
right	O
column	O
.	O
this	O
representation	O
has	O
a	O
single	O
feature	O
for	O
each	O
component	O
cs	O
present	O
on	O
a	O
trial	O
,	O
where	O
the	O
feature	O
has	O
value	B
1	O
whenever	O
that	O
component	O
is	O
present	O
,	O
figure	O
14.1	O
:	O
three	O
stimulus	O
representations	O
(	O
in	O
columns	O
)	O
sometimes	O
used	O
with	O
the	O
td	O
model	O
.	O
each	O
row	O
represents	O
one	O
element	O
of	O
the	O
stimulus	O
representation	O
.	O
the	O
three	O
representations	O
vary	O
along	O
a	O
temporal	O
generalization	O
gradient	B
,	O
with	O
no	O
generalization	O
between	O
nearby	O
time	O
points	O
in	O
the	O
complete	O
serial	O
compound	B
(	O
left	O
column	O
)	O
and	O
complete	O
generalization	O
between	O
nearby	O
time	O
points	O
in	O
the	O
presence	O
representation	O
(	O
right	O
column	O
)	O
.	O
the	O
microstimulus	O
representation	O
occupies	O
a	O
middle	O
ground	O
.	O
the	O
degree	O
of	O
temporal	O
generalization	O
determines	O
the	O
temporal	O
granularity	O
with	O
which	O
us	O
predictions	O
are	O
learned	O
.	O
adapted	O
with	O
minor	O
changes	O
from	O
learning	O
&	O
behavior	O
,	O
evaluating	O
the	O
td	O
model	O
of	O
classical	O
conditioning	B
,	O
volume	O
40	O
,	O
2012	O
,	O
p.	O
311	O
,	O
e.	O
a.	O
ludvig	O
,	O
r.	O
s.	O
sutton	O
,	O
e.	O
j.	O
kehoe	O
.	O
with	O
permission	O
of	O
springer	O
.	O
presencecomplete	O
serial	O
compoundstimulus	O
representationmicrostimulicsus	O
356	O
chapter	O
14	O
:	O
psychology	B
and	O
0	O
otherwise.4	O
the	O
presence	O
representation	O
is	O
not	O
a	O
realistic	O
hypothesis	O
about	O
how	O
stimuli	O
are	O
represented	O
in	O
an	O
animal	O
’	O
s	O
brain	O
,	O
but	O
as	O
we	O
describe	O
below	O
,	O
the	O
td	O
model	O
with	O
this	O
representation	O
can	O
produce	O
many	O
of	O
the	O
timing	O
phenomena	O
seen	O
in	O
classical	O
conditioning	B
.	O
for	O
the	O
csc	O
representation	O
(	O
left	O
column	O
of	O
figure	O
14.1	O
)	O
,	O
the	O
onset	O
of	O
each	O
external	O
stimulus	O
initiates	O
a	O
sequence	O
of	O
precisely-timed	O
short-duration	O
internal	O
signals	O
that	O
con-	O
tinues	O
until	O
the	O
external	O
stimulus	O
ends.5	O
this	O
is	O
like	O
assuming	O
the	O
animal	O
’	O
s	O
nervous	O
system	O
has	O
a	O
clock	O
that	O
keeps	O
precise	O
track	O
of	O
time	O
during	O
stimulus	O
presentations	O
;	O
it	O
is	O
what	O
engineers	O
call	O
a	O
“	O
tapped	O
delay	O
line.	O
”	O
like	O
the	O
presence	O
representation	O
,	O
the	O
csc	O
representation	O
is	O
unrealistic	O
as	O
a	O
hypothesis	O
about	O
how	O
the	O
brain	O
internally	O
represents	O
stimuli	O
,	O
but	O
ludvig	O
et	O
al	O
.	O
(	O
2012	O
)	O
call	O
it	O
a	O
“	O
useful	O
ﬁction	O
”	O
because	O
it	O
can	O
reveal	O
details	O
of	O
how	O
the	O
td	O
model	O
works	O
when	O
relatively	O
unconstrained	O
by	O
the	O
stimulus	O
representation	O
.	O
the	O
csc	O
representation	O
is	O
also	O
used	O
in	O
most	O
td	O
models	O
of	O
dopamine-producing	O
neurons	O
in	O
the	O
brain	O
,	O
a	O
topic	O
we	O
take	O
up	O
in	O
chapter	O
15.	O
the	O
csc	O
representation	O
is	O
often	O
viewed	O
as	O
an	O
essential	O
part	O
of	O
the	O
td	O
model	O
,	O
although	O
this	O
view	O
is	O
mistaken	O
.	O
the	O
ms	O
representation	O
(	O
center	O
column	O
of	O
figure	O
14.1	O
)	O
is	O
like	O
the	O
csc	O
representation	O
in	O
that	O
each	O
external	O
stimulus	O
initiates	O
a	O
cascade	O
of	O
internal	O
stimuli	O
,	O
but	O
in	O
this	O
case	O
the	O
internal	O
stimuli—the	O
microstimuli—are	O
not	O
of	O
such	O
limited	O
and	O
non-overlapping	O
form	O
;	O
they	O
are	O
extended	O
over	O
time	O
and	O
overlap	O
.	O
as	O
time	O
elapses	O
from	O
stimulus	O
onset	O
,	O
diﬀerent	O
sets	O
of	O
microstimuli	O
become	O
more	O
or	O
less	O
active	O
,	O
and	O
each	O
subsequent	O
microstimulus	O
becomes	O
progressively	O
wider	O
in	O
time	O
and	O
reaches	O
a	O
lower	O
maximal	O
level	O
.	O
of	O
course	O
,	O
there	O
are	O
many	O
ms	O
representations	O
depending	O
on	O
the	O
nature	O
of	O
the	O
microstimuli	O
,	O
and	O
a	O
number	O
of	O
examples	O
of	O
ms	O
representations	O
have	O
been	O
studied	O
in	O
the	O
literature	O
,	O
in	O
some	O
cases	O
along	O
with	O
proposals	O
for	O
how	O
an	O
animal	O
’	O
s	O
brain	O
might	O
generate	O
them	O
(	O
see	O
the	O
bibliographic	O
and	O
historical	O
comments	O
at	O
the	O
end	O
of	O
this	O
chapter	O
)	O
.	O
ms	O
representations	O
are	O
more	O
realistic	O
than	O
the	O
presence	O
or	O
csc	O
representations	O
as	O
hypotheses	O
about	O
neural	B
representations	O
of	O
stimuli	O
,	O
and	O
they	O
allow	O
the	O
behavior	O
of	O
the	O
td	O
model	O
to	O
be	O
related	O
to	O
a	O
broader	O
collection	O
of	O
phenomena	O
observed	O
in	O
animal	O
experiments	O
.	O
in	O
particular	O
,	O
by	O
assuming	O
that	O
cascades	O
of	O
microstimuli	O
are	O
initiated	O
by	O
uss	O
as	O
well	O
as	O
by	O
css	O
,	O
and	O
by	O
studying	O
the	O
signiﬁcant	O
eﬀects	O
on	O
learning	O
of	O
interactions	O
between	O
microstimuli	O
,	O
eligibility	B
traces	I
,	O
and	O
discounting	O
,	O
the	O
td	O
model	O
is	O
helping	O
to	O
frame	O
hypotheses	O
to	O
account	O
for	O
many	O
of	O
the	O
subtle	O
phenomena	O
of	O
classical	O
conditioning	B
and	O
how	O
an	O
animal	O
’	O
s	O
brain	O
might	O
produce	O
them	O
.	O
we	O
say	O
more	O
about	O
this	O
below	O
,	O
particularly	O
in	O
chapter	O
15	O
where	O
we	O
discuss	O
reinforcement	B
learning	I
and	O
neuroscience	B
.	O
even	O
with	O
the	O
simple	O
presence	O
representation	O
,	O
however	O
,	O
the	O
td	O
model	O
produces	O
all	O
the	O
basic	O
properties	O
of	O
classical	O
conditioning	B
that	O
are	O
accounted	O
for	O
by	O
the	O
rescorla–	O
4in	O
our	O
formalism	O
,	O
there	O
is	O
a	O
diﬀerent	O
state	B
,	O
st	O
,	O
for	O
each	O
time	O
step	O
t	O
during	O
a	O
trial	O
,	O
and	O
for	O
a	O
trial	O
in	O
which	O
a	O
compound	B
cs	O
consists	O
of	O
n	O
component	O
css	O
of	O
various	O
durations	O
occurring	O
at	O
various	O
times	O
throughout	O
the	O
trial	O
,	O
there	O
is	O
a	O
feature	O
,	O
xi	O
,	O
for	O
each	O
component	O
csi	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
where	O
xi	O
(	O
st	O
)	O
=	O
1	O
for	O
all	O
times	O
t	O
when	O
the	O
csi	O
is	O
present	O
,	O
and	O
equals	O
zero	O
otherwise	O
.	O
5in	O
our	O
formalism	O
,	O
for	O
each	O
cs	O
component	O
csi	O
present	O
on	O
a	O
trial	O
,	O
and	O
for	O
each	O
time	O
step	O
t	O
during	O
a	O
i	O
(	O
st	O
(	O
cid:48	O
)	O
)	O
=	O
1	O
if	O
t	O
=	O
t	O
(	O
cid:48	O
)	O
for	O
any	O
t	O
(	O
cid:48	O
)	O
at	O
which	O
csi	O
is	O
present	O
,	O
and	O
trial	O
,	O
there	O
is	O
a	O
separate	O
feature	O
xt	O
equals	O
0	O
otherwise	O
.	O
this	O
is	O
diﬀerent	O
from	O
the	O
csc	O
representation	O
in	O
sutton	O
and	O
barto	O
(	O
1990	O
)	O
in	O
which	O
there	O
are	O
the	O
same	O
distinct	O
features	O
for	O
each	O
time	O
step	O
but	O
no	O
reference	O
to	O
external	O
stimuli	O
;	O
hence	O
the	O
name	O
complete	O
serial	O
compound	B
.	O
i	O
,	O
where	O
xt	O
14.2.	O
classical	B
conditioning	I
357	O
wagner	O
model	O
,	O
plus	O
features	O
of	O
conditioning	O
that	O
are	O
beyond	O
the	O
scope	O
of	O
trial-level	O
models	O
.	O
for	O
example	O
,	O
as	O
we	O
have	O
already	O
mentioned	O
,	O
a	O
conspicuous	O
feature	O
of	O
classical	B
conditioning	I
is	O
that	O
the	O
us	O
generally	O
must	O
begin	O
after	O
the	O
onset	O
of	O
a	O
neutral	O
stimulus	O
for	O
conditioning	O
to	O
occur	O
,	O
and	O
that	O
after	O
conditioning	B
,	O
the	O
cr	O
begins	O
before	O
the	O
appearance	O
of	O
the	O
us	O
.	O
in	O
other	O
words	O
,	O
conditioning	B
generally	O
requires	O
a	O
positive	O
isi	O
,	O
and	O
the	O
cr	O
generally	O
anticipates	O
the	O
us	O
.	O
how	O
the	O
strength	O
of	O
conditioning	O
(	O
e.g.	O
,	O
the	O
percentage	O
of	O
crs	O
elicited	O
by	O
a	O
cs	O
)	O
depends	O
on	O
the	O
isi	O
varies	O
substantially	O
across	O
species	O
and	O
response	O
systems	O
,	O
but	O
it	O
typically	O
has	O
the	O
following	O
properties	O
:	O
it	O
is	O
negligible	O
for	O
a	O
zero	O
or	O
negative	O
isi	O
,	O
i.e.	O
,	O
when	O
the	O
us	O
onset	O
occurs	O
simultaneously	O
with	O
,	O
or	O
earlier	O
than	O
,	O
the	O
cs	O
onset	O
(	O
although	O
research	O
has	O
found	O
that	O
associative	O
strengths	O
sometimes	O
increase	O
slightly	O
or	O
become	O
negative	O
with	O
negative	O
isis	O
)	O
;	O
it	O
increases	O
to	O
a	O
maximum	O
at	O
a	O
positive	O
isi	O
where	O
conditioning	B
is	O
most	O
eﬀective	O
;	O
and	O
it	O
then	O
decreases	O
to	O
zero	O
after	O
an	O
interval	O
that	O
varies	O
widely	O
with	O
response	O
systems	O
.	O
the	O
precise	O
shape	O
of	O
this	O
dependency	O
for	O
the	O
td	O
model	O
depends	O
on	O
the	O
values	O
of	O
its	O
parameters	O
and	O
details	O
of	O
the	O
stimulus	O
representation	O
,	O
but	O
these	O
basic	O
features	O
of	O
isi-dependency	O
are	O
core	O
properties	O
of	O
the	O
td	O
model	O
.	O
one	O
of	O
the	O
theoretical	O
issues	O
aris-	O
ing	B
with	O
serial-compound	O
conditioning	B
,	O
that	O
is	O
,	O
conditioning	B
with	O
a	O
compound	B
cs	O
whose	O
components	O
occur	O
in	O
a	O
se-	O
quence	O
,	O
concerns	O
the	O
facilitation	O
of	O
re-	O
mote	O
associations	O
.	O
it	O
has	O
been	O
found	O
that	O
if	O
the	O
empty	O
trace	O
interval	O
between	O
a	O
ﬁrst	O
cs	O
(	O
csa	O
)	O
and	O
the	O
us	O
is	O
ﬁlled	O
with	O
a	O
second	O
cs	O
(	O
csb	O
)	O
to	O
form	O
a	O
serial-compound	O
stimulus	O
,	O
then	O
condi-	O
tioning	O
to	O
csa	O
is	O
facilitated	O
.	O
shown	O
to	O
the	O
right	O
is	O
the	O
behavior	O
of	O
the	O
td	O
model	O
with	O
the	O
presence	O
representation	O
in	O
a	O
simulation	O
of	O
such	O
an	O
experiment	O
whose	O
timing	O
details	O
are	O
shown	O
above	O
.	O
consistent	O
with	O
the	O
experimental	O
re-	O
sults	O
(	O
kehoe	O
,	O
1982	O
)	O
,	O
the	O
model	O
shows	O
facilitation	O
of	O
both	O
the	O
rate	O
of	O
condi-	O
tioning	O
and	O
the	O
asymptotic	O
level	O
of	O
con-	O
ditioning	O
of	O
the	O
ﬁrst	O
cs	O
due	O
to	O
the	O
pres-	O
ence	O
of	O
the	O
second	O
cs	O
.	O
figure	O
14.2	O
:	O
facilitation	O
of	O
a	O
remote	O
associa-	O
tion	B
by	O
an	O
intervening	O
stimulus	O
in	O
the	O
td	O
model	O
.	O
adapted	O
from	O
sutton	O
and	O
barto	O
(	O
1990	O
)	O
.	O
wcsa	O
358	O
chapter	O
14	O
:	O
psychology	B
a	O
well-known	O
demonstration	O
of	O
the	O
eﬀects	O
on	O
conditioning	B
of	O
temporal	O
re-	O
lationships	O
among	O
stimuli	O
within	O
a	O
trial	O
is	O
an	O
experiment	O
by	O
egger	O
and	O
miller	O
(	O
1962	O
)	O
that	O
involved	O
two	O
overlapping	O
css	O
in	O
a	O
delay	O
conﬁguration	O
as	O
shown	O
to	O
the	O
right	O
(	O
top	O
)	O
.	O
although	O
csb	O
was	O
in	O
a	O
better	O
temporal	O
relationship	O
with	O
the	O
us	O
,	O
the	O
presence	O
of	O
csa	O
substantially	O
reduced	O
conditioning	B
to	O
csb	O
as	O
com-	O
pared	O
to	O
controls	O
in	O
which	O
csa	O
was	O
ab-	O
sent	O
.	O
the	O
bottom	O
panel	O
shows	O
the	O
same	O
result	O
being	O
generated	O
by	O
the	O
td	O
model	O
in	O
a	O
simulation	O
of	O
this	O
experiment	O
with	O
the	O
presence	O
representation	O
.	O
i.e.	O
,	O
figure	O
14.3	O
:	O
the	O
egger-miller	O
,	O
or	O
primacy	O
,	O
eﬀect	O
in	O
the	O
td	O
model	O
.	O
adapted	O
from	O
sutton	O
and	O
barto	O
(	O
1990	O
)	O
.	O
the	O
td	O
model	O
accounts	O
for	O
block-	O
ing	B
because	O
it	O
is	O
an	O
error-correcting	O
learning	O
rule	O
like	O
the	O
rescorla–wagner	O
model	O
.	O
beyond	O
accounting	O
for	O
ba-	O
sic	O
blocking	B
results	O
,	O
however	O
,	O
the	O
td	O
model	O
predicts	O
(	O
with	O
the	O
presence	O
representation	O
and	O
more	O
complex	O
representations	O
a	O
well	O
)	O
that	O
blocking	B
is	O
reversed	O
if	O
the	O
blocked	O
stimulus	O
is	O
moved	O
earlier	O
in	O
time	O
so	O
that	O
its	O
onset	O
occurs	O
before	O
the	O
onset	O
of	O
the	O
blocking	B
stimulus	O
.	O
this	O
feature	O
of	O
the	O
td	O
model	O
’	O
s	O
behavior	O
deserves	O
attention	O
because	O
it	O
had	O
not	O
been	O
observed	O
at	O
the	O
time	O
of	O
the	O
model	O
’	O
s	O
introduction	O
.	O
recall	O
that	O
in	O
blocking	O
,	O
if	O
an	O
animal	O
has	O
already	O
learned	O
that	O
one	O
cs	O
pre-	O
dicts	O
a	O
us	O
,	O
then	O
learning	O
that	O
a	O
newly-added	O
second	O
cs	O
also	O
predicts	O
the	O
us	O
is	O
much	O
reduced	O
,	O
is	O
blocked	O
.	O
but	O
if	O
the	O
newly-added	O
second	O
cs	O
begins	O
ear-	O
lier	O
than	O
the	O
pretrained	O
cs	O
,	O
then—	O
according	O
to	O
the	O
td	O
model—learning	O
to	O
the	O
newly-added	O
cs	O
is	O
not	O
blocked	O
.	O
in	O
fact	O
,	O
as	O
training	O
continues	O
and	O
the	O
newly-added	O
cs	O
gains	O
associative	O
strength	O
,	O
the	O
pretrained	O
cs	O
loses	O
asso-	O
ciative	O
strength	O
.	O
the	O
behavior	O
of	O
the	O
td	O
model	O
under	O
these	O
conditions	O
is	O
shown	O
to	O
the	O
right	O
.	O
this	O
simulation	O
ex-	O
periment	O
diﬀered	O
from	O
the	O
egger-miller	O
experiment	O
of	O
figure	O
14.3	O
in	O
that	O
the	O
shorter	O
cs	O
with	O
the	O
later	O
onset	O
was	O
given	O
prior	O
training	O
until	O
it	O
was	O
fully	O
associated	O
with	O
the	O
us	O
.	O
this	O
surpris-	O
ing	B
prediction	O
led	O
kehoe	O
,	O
schreurs	O
,	O
and	O
graham	O
(	O
1987	O
)	O
to	O
conduct	O
the	O
experi-	O
ment	O
using	O
the	O
well-studied	O
rabbit	O
nic-	O
figure	O
14.4	O
:	O
temporal	O
primacy	O
overriding	O
block-	O
ing	B
in	O
the	O
td	O
model	O
.	O
adapted	O
from	O
sutton	O
and	O
barto	O
(	O
1990	O
)	O
.	O
wcsbwcsb	O
14.2.	O
classical	B
conditioning	I
359	O
titating	O
membrane	O
preparation	O
.	O
their	O
results	O
conﬁrmed	O
the	O
model	O
’	O
s	O
prediction	B
,	O
and	O
they	O
noted	O
that	O
non-td	O
models	O
have	O
considerable	O
diﬃculty	O
explaining	O
their	O
data	O
.	O
with	O
the	O
td	O
model	O
,	O
an	O
earlier	O
predictive	O
stimulus	O
takes	O
precedence	O
over	O
a	O
later	O
pre-	O
dictive	O
stimulus	O
because	O
,	O
like	O
all	O
the	O
prediction	B
methods	O
described	O
in	O
this	O
book	O
,	O
the	O
td	O
model	O
is	O
based	O
on	O
the	O
backing-up	O
or	O
bootstrapping	B
idea	O
:	O
updates	O
to	O
associative	O
strengths	O
shift	O
the	O
strengths	O
at	O
a	O
particular	O
state	B
toward	O
the	O
strength	O
at	O
later	O
states	O
.	O
another	O
consequence	O
of	O
bootstrapping	O
is	O
that	O
the	O
td	O
model	O
provides	O
an	O
account	O
of	O
higher-order	O
conditioning	B
,	O
a	O
feature	O
of	O
classical	B
conditioning	I
that	O
is	O
beyond	O
the	O
scope	O
of	O
the	O
rescoral-wagner	O
and	O
similar	O
models	O
.	O
as	O
we	O
described	O
above	O
,	O
higher-order	O
con-	O
ditioning	O
is	O
the	O
phenomenon	O
in	O
which	O
a	O
previously-conditioned	O
cs	O
can	O
act	O
as	O
a	O
us	O
in	O
conditioning	O
another	O
initially	O
neutral	O
stimulus	O
.	O
to	O
the	O
right	O
is	O
shown	O
the	O
be-	O
havior	O
of	O
the	O
td	O
model	O
(	O
again	O
with	O
the	O
presence	O
representation	O
)	O
in	O
a	O
higher-	O
order	O
conditioning	B
experiment—in	O
this	O
case	O
it	O
is	O
second-order	O
conditioning	B
.	O
in	O
the	O
ﬁrst	O
phase	O
(	O
not	O
shown	O
in	O
the	O
ﬁg-	O
ure	O
)	O
,	O
csb	O
is	O
trained	O
to	O
predict	O
a	O
us	O
so	O
that	O
its	O
associative	O
strength	O
increases	O
,	O
here	O
to	O
1.6.	O
in	O
the	O
second	O
phase	O
,	O
csa	O
is	O
paired	O
with	O
csb	O
in	O
the	O
absence	O
of	O
the	O
us	O
,	O
in	O
the	O
sequential	O
arrangement	O
shown	O
at	O
the	O
top	O
of	O
the	O
ﬁgure	O
.	O
csa	O
ac-	O
quires	O
associative	O
strength	O
even	O
though	O
it	O
is	O
never	O
paired	O
with	O
the	O
us	O
.	O
with	O
continued	O
training	O
,	O
csa	O
’	O
s	O
associative	O
strength	O
reaches	O
a	O
peak	O
and	O
then	O
de-	O
creases	O
because	O
the	O
associative	O
strength	O
of	O
csb	O
,	O
the	O
secondary	O
reinforcer	O
,	O
de-	O
creases	O
so	O
that	O
it	O
loses	O
its	O
ability	O
to	O
pro-	O
vide	O
secondary	B
reinforcement	I
.	O
csb	O
’	O
s	O
associative	O
strength	O
decreases	O
because	O
the	O
us	O
does	O
not	O
occur	O
in	O
these	O
higher-order	O
conditioning	B
trials	O
.	O
these	O
are	O
extinction	O
tri-	O
als	O
for	O
csb	O
because	O
its	O
predictive	O
relationship	O
to	O
the	O
us	O
is	O
disrupted	O
so	O
that	O
its	O
ability	O
to	O
act	O
as	O
a	O
reinforcer	O
decreases	O
.	O
this	O
same	O
pattern	O
is	O
seen	O
in	O
animal	O
experiments	O
.	O
this	O
extinction	O
of	O
conditioned	O
reinforcement	O
in	O
higher-order	O
conditioning	B
trials	O
makes	O
it	O
diﬃ-	O
cult	O
to	O
demonstrate	O
higher-order	O
conditioning	B
unless	O
the	O
original	O
predictive	O
relationships	O
are	O
periodically	O
refreshed	O
by	O
occasionally	O
inserting	O
ﬁrst-order	O
trials	O
.	O
figure	O
14.5	O
:	O
second-order	O
conditioning	B
with	O
the	O
td	O
model	O
.	O
csb	O
has	O
an	O
initial	O
associative	O
strength	O
of	O
1.65	O
at	O
the	O
beginning	O
of	O
the	O
simulation	O
.	O
adapted	O
from	O
sutton	O
and	O
barto	O
(	O
1990	O
)	O
.	O
the	O
td	O
model	O
produces	O
an	O
analog	O
of	O
second-	O
and	B
higher-order	I
conditioning	I
because	O
γˆv	O
(	O
st+1	O
,	O
wt	O
)	O
−	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
appears	O
in	O
the	O
td	O
error	O
δt	O
(	O
14.5	O
)	O
.	O
this	O
means	O
that	O
as	O
a	O
result	O
of	O
previous	O
learning	O
,	O
γˆv	O
(	O
st+1	O
,	O
wt	O
)	O
can	O
diﬀer	O
from	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
,	O
making	O
δt	O
non-zero	O
(	O
a	O
temporal	O
diﬀerence	O
)	O
.	O
this	O
diﬀerence	O
has	O
the	O
same	O
status	O
as	O
rt+1	O
in	O
(	O
14.5	O
)	O
,	O
implying	O
that	O
as	O
far	O
as	O
learning	O
is	O
concerned	O
there	O
is	O
no	O
diﬀerence	O
between	O
a	O
temporal	O
diﬀerence	O
and	O
the	O
occurrence	O
of	O
a	O
us	O
.	O
in	O
fact	O
,	O
this	O
feature	O
of	O
the	O
td	O
algorithm	O
is	O
one	O
of	O
the	O
major	O
reasons	O
for	O
its	O
development	O
,	O
which	O
we	O
now	O
understand	O
through	O
its	O
connection	O
to	O
dynamic	O
w	O
360	O
chapter	O
14	O
:	O
psychology	B
programming	O
as	O
described	O
in	O
chapter	O
6.	O
bootstrapping	B
values	O
is	O
intimately	O
related	O
to	O
second-order	O
,	O
and	O
higher-order	O
,	O
conditioning	B
.	O
in	O
the	O
examples	O
of	O
the	O
td	O
model	O
’	O
s	O
behavior	O
described	O
above	O
,	O
we	O
examined	O
only	O
the	O
changes	O
in	O
the	O
associative	O
strengths	O
of	O
the	O
cs	O
components	O
;	O
we	O
did	O
not	O
look	O
at	O
what	O
the	O
model	O
predicts	O
about	O
properties	O
of	O
an	O
animal	O
’	O
s	O
conditioned	O
responses	O
(	O
crs	O
)	O
:	O
their	O
timing	O
,	O
shape	O
,	O
and	O
how	O
they	O
develop	O
over	O
conditioning	B
trials	O
.	O
these	O
properties	O
depend	O
on	O
the	O
species	O
,	O
the	O
response	O
system	O
being	O
observed	O
,	O
and	O
parameters	O
of	O
the	O
conditioning	B
trials	O
,	O
but	O
in	O
many	O
experiments	O
with	O
diﬀerent	O
animals	O
and	O
diﬀerent	O
response	O
systems	O
,	O
the	O
magnitude	O
of	O
the	O
cr	O
,	O
or	O
the	O
probability	O
of	O
a	O
cr	O
,	O
increases	O
as	O
the	O
expected	O
time	O
of	O
the	O
us	O
approaches	O
.	O
for	O
example	O
,	O
in	O
classical	O
conditioning	B
of	O
a	O
rabbit	O
’	O
s	O
nictitating	O
membrane	O
response	O
that	O
we	O
mentioned	O
above	O
,	O
over	O
conditioning	B
trials	O
the	O
delay	O
from	O
cs	O
onset	O
to	O
when	O
the	O
nictitating	O
membrane	O
begins	O
to	O
move	O
across	O
the	O
eye	O
decreases	O
over	O
trials	O
,	O
and	O
the	O
amplitude	O
of	O
this	O
anticipatory	O
closure	O
gradually	O
increases	O
over	O
the	O
interval	O
between	O
the	O
cs	O
and	O
the	O
us	O
until	O
the	O
membrane	O
reaches	O
maximal	O
closure	O
at	O
the	O
expected	O
time	O
of	O
the	O
us	O
.	O
the	O
timing	O
and	O
shape	O
of	O
this	O
cr	O
is	O
critical	O
to	O
its	O
adaptive	O
signiﬁcance—covering	O
the	O
eye	O
too	O
early	O
reduces	O
vision	O
(	O
even	O
though	O
the	O
nictitating	O
membrane	O
is	O
translucent	O
)	O
,	O
while	O
covering	O
it	O
too	O
late	O
is	O
of	O
little	O
protective	O
value	B
.	O
capturing	O
cr	O
features	O
like	O
these	O
is	O
challenging	O
for	O
models	O
of	O
classical	O
conditioning	B
.	O
the	O
td	O
model	O
does	O
not	O
include	O
as	O
part	O
of	O
its	O
deﬁnition	O
any	O
mechanism	O
for	O
translating	O
the	O
time	O
course	O
of	O
the	O
us	O
prediction	B
,	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
,	O
into	O
a	O
proﬁle	O
that	O
can	O
be	O
compared	O
with	O
the	O
properties	O
of	O
an	O
animal	O
’	O
s	O
cr	O
.	O
the	O
simplest	O
choice	O
is	O
to	O
let	O
the	O
time	O
course	O
of	O
a	O
sim-	O
ulated	O
cr	O
equal	O
the	O
time	O
course	O
of	O
the	O
us	O
prediction	B
.	O
in	O
this	O
case	O
,	O
features	O
of	O
simulated	O
crs	O
and	O
how	O
they	O
change	O
over	O
trials	O
depend	O
only	O
on	O
the	O
stimulus	O
representation	O
chosen	O
and	O
the	O
values	O
of	O
the	O
model	O
’	O
s	O
parameters	O
α	O
,	O
γ	O
,	O
and	O
λ.	O
figure	O
14.6	O
shows	O
the	O
time	O
courses	O
of	O
us	O
predictions	O
at	O
diﬀerent	O
points	O
during	O
learning	O
with	O
the	O
three	O
representations	O
shown	O
in	O
figure	O
14.1.	O
for	O
these	O
simulations	O
the	O
us	O
occurred	O
25	O
time	O
steps	O
after	O
the	O
onset	O
of	O
the	O
cs	O
,	O
and	O
α	O
=	O
.05	O
,	O
λ	O
=	O
.95	O
and	O
γ	O
=	O
.97.	O
with	O
the	O
csc	O
representation	O
(	O
figure	O
14.6	O
left	O
)	O
,	O
the	O
curve	O
of	O
the	O
us	O
prediction	B
formed	O
by	O
the	O
td	O
model	O
increases	O
exponentially	O
throughout	O
the	O
interval	O
between	O
the	O
cs	O
and	O
the	O
us	O
until	O
it	O
reaches	O
a	O
maximum	O
exactly	O
when	O
the	O
us	O
occurs	O
(	O
at	O
time	O
step	O
25	O
)	O
.	O
this	O
exponential	O
increase	O
is	O
the	O
result	O
of	O
discounting	O
in	O
the	O
td	O
model	O
learning	O
rule	O
.	O
with	O
the	O
presence	O
representation	O
(	O
figure	O
14.6	O
middle	O
)	O
,	O
the	O
us	O
prediction	B
is	O
nearly	O
constant	O
while	O
the	O
stimulus	O
is	O
present	O
because	O
there	O
is	O
only	O
one	O
weight	O
,	O
or	O
associative	O
strength	O
,	O
to	O
be	O
learned	O
for	O
each	O
stimulus	O
.	O
consequently	O
,	O
the	O
td	O
model	O
with	O
the	O
presence	O
representation	O
can	O
not	O
recreate	O
many	O
features	O
of	O
cr	O
timing	O
.	O
with	O
an	O
ms	O
representation	O
(	O
figure	O
14.6	O
right	O
)	O
,	O
the	O
development	O
of	O
the	O
td	O
model	O
’	O
s	O
us	O
prediction	B
is	O
more	O
complicated	O
.	O
after	O
200	O
trials	O
the	O
prediction	B
’	O
s	O
proﬁle	O
is	O
a	O
reasonable	O
approximation	O
of	O
the	O
us	O
prediction	B
curve	O
produced	O
with	O
the	O
csc	O
representation	O
.	O
the	O
us	O
prediction	B
curves	O
shown	O
in	O
figure	O
14.6	O
were	O
not	O
intended	O
to	O
precisely	O
match	O
proﬁles	O
of	O
crs	O
as	O
they	O
develop	O
during	O
conditioning	B
in	O
any	O
particular	O
animal	O
experiment	O
,	O
but	O
they	O
illustrate	O
the	O
strong	O
inﬂuence	O
that	O
the	O
stimulus	O
representation	O
has	O
on	O
predic-	O
tions	O
derived	O
from	O
the	O
td	O
model	O
.	O
further	O
,	O
although	O
we	O
can	O
only	O
mention	O
it	O
here	O
,	O
how	O
the	O
stimulus	O
representation	O
interacts	O
with	O
discounting	O
and	B
eligibility	I
traces	I
is	O
impor-	O
tant	O
in	O
determining	O
properties	O
of	O
the	O
us	O
prediction	B
proﬁles	O
produced	O
by	O
the	O
td	O
model	O
.	O
14.2.	O
classical	B
conditioning	I
361	O
figure	O
14.6	O
:	O
time	O
course	O
of	O
us	O
prediction	B
over	O
the	O
course	O
of	O
acquisition	O
for	O
the	O
td	O
model	O
with	O
three	O
diﬀerent	O
stimulus	O
representations	O
.	O
left	O
:	O
with	O
the	O
complete	O
serial	O
compound	B
(	O
csc	O
)	O
,	O
the	O
us	O
prediction	B
increases	O
exponentially	O
through	O
the	O
interval	O
,	O
peaking	O
at	O
the	O
time	O
of	O
the	O
us	O
.	O
at	O
asymptote	O
(	O
trial	O
200	O
)	O
,	O
the	O
us	O
prediction	B
peaks	O
at	O
the	O
us	O
intensity	O
(	O
1	O
in	O
these	O
simulations	O
)	O
.	O
middle	O
:	O
with	O
the	O
presence	O
representation	O
,	O
the	O
us	O
prediction	B
converges	O
to	O
an	O
almost	O
constant	O
level	O
.	O
this	O
constant	O
level	O
is	O
determined	O
by	O
the	O
us	O
intensity	O
and	O
the	O
length	O
of	O
the	O
cs–us	O
interval	O
.	O
right	O
:	O
with	O
the	O
microstimulus	O
representation	O
,	O
at	O
asymptote	O
,	O
the	O
td	O
model	O
approximates	O
the	O
exponentially	O
increasing	O
time	O
course	O
depicted	O
with	O
the	O
csc	O
through	O
a	O
linear	O
combination	O
of	O
the	O
diﬀerent	O
microstimuli	O
.	O
adapted	O
with	O
minor	O
changes	O
from	O
learning	O
&	O
behavior	O
,	O
evaluating	O
the	O
td	O
model	O
of	O
classical	O
conditioning	B
,	O
volume	O
40	O
,	O
2012	O
,	O
e.	O
a.	O
ludvig	O
,	O
r.	O
s.	O
sutton	O
,	O
e.	O
j.	O
kehoe	O
.	O
with	O
permission	O
of	O
springer	O
.	O
another	O
dimension	O
beyond	O
what	O
we	O
can	O
discuss	O
here	O
is	O
the	O
inﬂuence	O
of	O
diﬀerent	O
response-	O
generation	O
mechanisms	O
that	O
translate	O
us	O
predictions	O
into	O
cr	O
proﬁles	O
;	O
the	O
proﬁles	O
shown	O
in	O
figure	O
14.6	O
are	O
“	O
raw	O
”	O
us	O
prediction	B
proﬁles	O
.	O
even	O
without	O
any	O
special	O
assumption	O
about	O
how	O
an	O
animal	O
’	O
s	O
brain	O
might	O
produce	O
overt	O
responses	O
from	O
us	O
predictions	O
,	O
how-	O
ever	O
,	O
the	O
proﬁles	O
in	O
figure	O
14.6	O
for	O
the	O
csc	O
and	O
ms	O
representations	O
increase	O
as	O
the	O
time	O
of	O
the	O
us	O
approaches	O
and	O
reach	O
a	O
maximum	O
at	O
the	O
time	O
of	O
the	O
us	O
,	O
as	O
is	O
seen	O
in	O
many	O
animal	O
conditioning	B
experiments	O
.	O
the	O
td	O
model	O
,	O
when	O
combined	O
with	O
particular	O
stimulus	O
representations	O
and	O
response-	O
generation	O
mechanisms	O
,	O
is	O
able	O
to	O
account	O
for	O
a	O
surprisingly-wide	O
range	O
of	O
phenomena	O
observed	O
in	O
animal	O
classical	O
conditioning	B
experiments	O
,	O
but	O
it	O
is	O
far	O
from	O
being	O
a	O
perfect	O
model	O
.	O
to	O
generate	O
other	O
details	O
of	O
classical	O
conditioning	B
the	O
model	O
needs	O
to	O
be	O
extended	O
,	O
perhaps	O
by	O
adding	O
model-based	O
elements	O
and	O
mechanisms	O
for	O
adaptively	O
altering	O
some	O
of	O
its	O
parameters	O
.	O
other	O
approaches	O
to	O
modeling	O
classical	B
conditioning	I
depart	O
signiﬁcantly	O
from	O
the	O
rescorla–wagner-style	O
error-correction	O
process	O
.	O
bayesian	O
models	O
,	O
for	O
example	O
,	O
work	O
within	O
a	O
probabilistic	O
framework	O
in	O
which	O
experience	O
revises	O
probability	O
estimates	O
.	O
all	O
of	O
these	O
models	O
usefully	O
contribute	O
to	O
our	O
understanding	O
of	O
classical	O
conditioning	B
.	O
perhaps	O
the	O
most	O
notable	O
feature	O
of	O
the	O
td	O
model	O
is	O
that	O
it	O
is	O
based	O
on	O
a	O
theory—the	O
theory	O
we	O
have	O
described	O
in	O
this	O
book—that	O
suggests	O
an	O
account	O
of	O
what	O
an	O
animal	O
’	O
s	O
nervous	O
system	O
is	O
trying	O
to	O
do	O
while	O
undergoing	O
conditioning	B
:	O
it	O
is	O
trying	O
to	O
form	O
accurate	O
long-term	O
predictions	O
,	O
consistent	O
with	O
the	O
limitations	O
imposed	O
by	O
the	O
way	O
stimuli	O
are	O
represented	O
and	O
how	O
the	O
nervous	O
system	O
works	O
.	O
in	O
other	O
words	O
,	O
it	O
suggests	O
a	O
normative	O
account	O
of	O
classical	O
conditioning	B
in	O
which	O
long-term	O
,	O
instead	O
of	O
immediate	O
,	O
prediction	B
is	O
ˆvˆvˆv	O
362	O
a	O
key	O
feature	O
.	O
chapter	O
14	O
:	O
psychology	B
the	O
development	O
of	O
the	O
td	O
model	O
of	O
classical	O
conditioning	B
is	O
one	O
instance	O
in	O
which	O
the	O
explicit	O
goal	B
was	O
to	O
model	O
some	O
of	O
the	O
details	O
of	O
animal	O
learning	O
behavior	O
.	O
in	O
addition	O
to	O
its	O
standing	O
as	O
an	O
algorithm	O
,	O
then	O
,	O
td	O
learning	O
is	O
also	O
the	O
basis	O
of	O
this	O
model	O
of	O
aspects	O
of	O
biological	O
learning	O
.	O
as	O
we	O
discuss	O
in	O
chapter	O
15	O
,	O
td	O
learning	O
has	O
also	O
turned	O
out	O
to	O
underlie	O
an	O
inﬂuential	O
model	O
of	O
the	O
activity	O
of	O
neurons	O
that	O
produce	O
dopamine	B
,	O
a	O
chemical	O
in	O
the	O
brain	O
of	O
mammals	O
that	O
is	O
deeply	O
involved	O
in	O
reward	O
processing	O
.	O
these	O
are	O
instances	O
in	O
which	O
reinforcement	B
learning	I
theory	O
makes	O
detailed	O
contact	O
with	O
animal	O
behavioral	O
and	O
neural	O
data	O
.	O
we	O
now	O
turn	O
to	O
considering	O
correspondences	O
between	O
reinforcement	B
learning	I
and	O
ani-	O
mal	O
behavior	O
in	O
instrumental	B
conditioning	I
experiments	O
,	O
the	O
other	O
major	O
type	O
of	O
labora-	O
tory	O
experiment	O
studied	O
by	O
animal	O
learning	O
psychologists	O
.	O
14.3	O
instrumental	B
conditioning	I
in	O
instrumental	B
conditioning	I
experiments	O
learning	O
depends	O
on	O
the	O
consequences	O
of	O
be-	O
havior	O
:	O
the	O
delivery	O
of	O
a	O
reinforcing	O
stimulus	O
is	O
contingent	O
on	O
what	O
the	O
animal	O
does	O
.	O
in	O
classical	O
conditioning	B
experiments	O
,	O
in	O
contrast	O
,	O
the	O
reinforcing	O
stimulus—the	O
us—is	O
delivered	O
independently	O
of	O
the	O
animal	O
’	O
s	O
behavior	O
.	O
instrumental	B
conditioning	I
is	O
usually	O
considered	O
to	O
be	O
the	O
same	O
as	O
operant	O
conditioning	B
,	O
the	O
term	O
b.	O
f.	O
skinner	O
(	O
1938	O
,	O
1963	O
)	O
introduced	O
for	O
experiments	O
with	O
behavior-contingent	O
reinforcement	O
,	O
though	O
the	O
experi-	O
ments	O
and	O
theories	O
of	O
those	O
who	O
use	O
these	O
two	O
terms	O
diﬀer	O
in	O
a	O
number	O
of	O
ways	O
,	O
some	O
of	O
which	O
we	O
touch	O
on	O
below	O
.	O
we	O
will	O
exclusively	O
use	O
the	O
term	O
instrumental	B
conditioning	I
for	O
experiments	O
in	O
which	O
reinforcement	O
is	O
contingent	O
upon	O
behavior	O
.	O
the	O
roots	O
of	O
in-	O
strumental	O
conditioning	B
go	O
back	O
to	O
experiments	O
performed	O
by	O
the	O
american	O
psychologist	O
edward	O
thorndike	O
one	O
hundred	O
years	O
before	O
publication	O
of	O
the	O
ﬁrst	O
edition	O
of	O
this	O
book	O
.	O
thorndike	O
observed	O
the	O
behavior	O
of	O
cats	O
when	O
they	O
were	O
placed	O
in	O
“	O
puzzle	O
boxes	O
,	O
”	O
such	O
as	O
the	O
one	O
at	O
the	O
right	O
,	O
from	O
which	O
they	O
could	O
escape	O
by	O
appropriate	O
actions	O
.	O
for	O
example	O
,	O
a	O
cat	O
could	O
open	O
the	O
door	O
of	O
one	O
box	O
by	O
performing	O
a	O
sequence	O
of	O
three	O
separate	O
actions	O
:	O
depressing	O
a	O
plat-	O
form	O
at	O
the	O
back	O
of	O
the	O
box	O
,	O
pulling	O
a	O
string	O
by	O
clawing	O
at	O
it	O
,	O
and	O
pushing	O
a	O
bar	O
up	O
or	O
down	O
.	O
when	O
ﬁrst	O
placed	O
in	O
a	O
puzzle	O
box	O
,	O
with	O
food	O
visible	O
outside	O
,	O
all	O
but	O
a	O
few	O
of	O
thorndike	O
’	O
s	O
cats	O
displayed	O
“	O
evident	O
signs	O
of	O
discomfort	O
”	O
and	O
extraordinarily	O
vigor-	O
ous	O
activity	O
“	O
to	O
strive	O
instinctively	O
to	O
es-	O
cape	O
from	O
conﬁnement	O
”	O
(	O
thorndike	O
,	O
1898	O
)	O
.	O
in	O
experiments	O
with	O
diﬀerent	O
cats	O
and	O
one	O
of	O
thorndike	O
’	O
s	O
puzzle	O
boxes	O
.	O
reprinted	O
from	O
thorndike	O
,	O
animal	O
intelligence	O
:	O
an	O
experimental	O
study	O
of	O
the	O
associative	O
processes	O
in	O
animals	O
,	O
the	O
psychological	O
review	O
,	O
series	O
of	O
mono-	O
graph	O
supplements	O
ii	O
(	O
4	O
)	O
,	O
macmillan	O
,	O
new	O
york	O
,	O
1898.	O
boxes	O
with	O
diﬀerent	O
escape	O
mechanisms	O
,	O
thorndike	O
recorded	O
the	O
amounts	O
of	O
time	O
each	O
cat	O
took	O
to	O
escape	O
over	O
multiple	O
experiences	O
in	O
each	O
box	O
.	O
he	O
observed	O
that	O
the	O
time	O
14.3.	O
instrumental	B
conditioning	I
363	O
almost	O
invariably	O
decreased	O
with	O
successive	O
experiences	O
,	O
for	O
example	O
,	O
from	O
300	O
seconds	O
to	O
6	O
or	O
7	O
seconds	O
.	O
he	O
described	O
cats	O
’	O
behavior	O
in	O
a	O
puzzle	O
box	O
like	O
this	O
:	O
the	O
cat	O
that	O
is	O
clawing	O
all	O
over	O
the	O
box	O
in	O
her	O
impulsive	O
struggle	O
will	O
probably	O
claw	O
the	O
string	O
or	O
loop	O
or	O
button	O
so	O
as	O
to	O
open	O
the	O
door	O
.	O
and	O
gradually	O
all	O
the	O
other	O
non-successful	O
impulses	O
will	O
be	O
stamped	O
out	O
and	O
the	O
particular	O
impulse	O
leading	O
to	O
the	O
successful	O
act	O
will	O
be	O
stamped	O
in	O
by	O
the	O
resulting	O
pleasure	O
,	O
until	O
,	O
after	O
many	O
trials	O
,	O
the	O
cat	O
will	O
,	O
when	O
put	O
in	O
the	O
box	O
,	O
immediately	O
claw	O
the	O
button	O
or	O
loop	O
in	O
a	O
deﬁnite	O
way	O
.	O
(	O
thorndike	O
1898	O
,	O
p.	O
13	O
)	O
these	O
and	O
other	O
experiments	O
(	O
some	O
with	O
dogs	O
,	O
chicks	O
,	O
monkeys	O
,	O
and	O
even	O
ﬁsh	O
)	O
led	O
thorndike	O
to	O
formulate	O
a	O
number	O
of	O
“	O
laws	O
”	O
of	O
learning	O
,	O
the	O
most	O
inﬂuential	O
being	O
the	O
law	O
of	O
eﬀect	O
,	O
a	O
version	O
of	O
which	O
we	O
quoted	O
in	O
chapter	O
1.	O
this	O
law	O
describes	O
what	O
is	O
generally	O
known	O
as	O
learning	O
by	O
trial	O
and	O
error	O
.	O
as	O
mentioned	O
in	O
chapter	O
1	O
,	O
many	O
aspects	O
of	O
the	O
law	O
of	O
eﬀect	O
have	O
generated	O
controversy	O
,	O
and	O
its	O
details	O
have	O
been	O
modiﬁed	O
over	O
the	O
years	O
.	O
still	O
the	O
law—in	O
one	O
form	O
or	O
another—expresses	O
an	O
enduring	O
principle	O
of	O
learning	O
.	O
essential	O
features	O
of	O
reinforcement	O
learning	O
algorithms	O
correspond	O
to	O
features	O
of	O
ani-	O
mal	O
learning	O
described	O
by	O
the	O
law	O
of	O
eﬀect	O
.	O
first	O
,	O
reinforcement	B
learning	I
algorithms	O
are	O
selectional	O
,	O
meaning	O
that	O
they	O
try	O
alternatives	O
and	O
select	O
among	O
them	O
by	O
comparing	O
their	O
consequences	O
.	O
second	O
,	O
reinforcement	B
learning	I
algorithms	O
are	O
associative	O
,	O
meaning	O
that	O
the	O
alternatives	O
found	O
by	O
selection	O
are	O
associated	O
with	O
particular	O
situations	O
,	O
or	O
states	O
,	O
to	O
form	O
the	O
agent	O
’	O
s	O
policy	B
.	O
like	O
learning	O
described	O
by	O
the	O
law	O
of	O
eﬀect	O
,	O
reinforcement	B
learning	I
is	O
not	O
just	O
the	O
process	O
of	O
ﬁnding	O
actions	O
that	O
produce	O
a	O
lot	O
of	O
reward	O
,	O
but	O
also	O
of	O
connecting	O
these	O
actions	O
to	O
situations	O
or	O
states	O
.	O
thorndike	O
used	O
the	O
phrase	O
learning	O
by	O
“	O
selecting	O
and	O
connecting	O
”	O
(	O
hilgard	O
,	O
1956	O
)	O
.	O
natural	O
selection	O
in	O
evolution	O
is	O
a	O
prime	O
example	O
of	O
a	O
selectional	O
process	O
,	O
but	O
it	O
is	O
not	O
associative	O
(	O
at	O
least	O
as	O
it	O
is	O
commonly	O
understood	O
)	O
;	O
supervised	B
learning	I
is	O
associative	O
,	O
but	O
it	O
is	O
not	O
selectional	O
because	O
it	O
relies	O
on	O
instructions	O
that	O
directly	O
tell	O
the	O
agent	O
how	O
to	O
change	O
its	O
behavior	O
.	O
in	O
computational	O
terms	O
,	O
the	O
law	O
of	O
eﬀect	O
describes	O
an	O
elementary	O
way	O
of	O
combining	O
search	O
and	O
memory	O
:	O
search	O
in	O
the	O
form	O
of	O
trying	O
and	O
selecting	O
among	O
many	O
actions	O
in	O
each	O
situation	O
,	O
and	O
memory	O
in	O
the	O
form	O
of	O
associations	O
linking	O
situations	O
with	O
the	O
actions	O
found—so	O
far—to	O
work	O
best	O
in	O
those	O
situations	O
.	O
search	O
and	O
memory	O
are	O
essential	O
components	O
of	O
all	O
reinforcement	B
learning	I
algorithms	O
,	O
whether	O
memory	O
takes	O
the	O
form	O
of	O
an	O
agent	O
’	O
s	O
policy	B
,	O
value	B
function	I
,	O
or	O
environment	B
model	O
.	O
a	O
reinforcement	B
learning	I
algorithm	O
’	O
s	O
need	O
to	O
search	O
means	O
that	O
it	O
has	O
to	O
explore	O
in	O
some	O
way	O
.	O
animals	O
clearly	O
explore	O
as	O
well	O
,	O
and	O
early	O
animal	O
learning	O
researchers	O
dis-	O
agreed	O
about	O
the	O
degree	O
of	O
guidance	O
an	O
animal	O
uses	O
in	O
selecting	O
its	O
actions	O
in	O
situations	O
like	O
thorndike	O
’	O
s	O
puzzle	O
boxes	O
.	O
are	O
actions	O
the	O
result	O
of	O
“	O
absolutely	O
random	O
,	O
blind	O
grop-	O
ing	B
”	O
(	O
woodworth	O
,	O
1938	O
,	O
p.	O
777	O
)	O
,	O
or	O
is	O
there	O
some	O
degree	O
of	O
guidance	O
,	O
either	O
from	O
prior	O
learning	O
,	O
reasoning	O
,	O
or	O
other	O
means	O
?	O
although	O
some	O
thinkers	O
,	O
including	O
thorndike	O
,	O
seem	O
to	O
have	O
taken	O
the	O
former	O
position	O
,	O
others	O
favored	O
more	O
deliberate	O
exploration	O
.	O
reinforce-	O
ment	O
learning	O
algorithms	O
allow	O
wide	O
latitude	O
for	O
how	O
much	O
guidance	O
an	O
agent	O
can	O
employ	O
in	O
selecting	O
actions	O
.	O
the	O
forms	O
of	O
exploration	O
we	O
have	O
used	O
in	O
the	O
algorithms	O
presented	O
in	O
this	O
book	O
,	O
such	O
as	O
ε-greedy	O
and	O
upper-conﬁdence-bound	O
action	B
selection	O
,	O
are	O
merely	O
364	O
chapter	O
14	O
:	O
psychology	B
among	O
the	O
simplest	O
.	O
more	O
sophisticated	O
methods	O
are	O
possible	O
,	O
with	O
the	O
only	O
stipulation	O
being	O
that	O
there	O
has	O
to	O
be	O
some	O
form	O
of	O
exploration	O
for	O
the	O
algorithms	O
to	O
work	O
eﬀectively	O
.	O
the	O
feature	O
of	O
our	O
treatment	O
of	O
reinforcement	O
learning	O
allowing	O
the	O
set	O
of	O
actions	O
available	O
at	O
any	O
time	O
to	O
depend	O
on	O
the	O
environment	B
’	O
s	O
current	O
state	B
echoes	O
something	O
thorndike	O
observed	O
in	O
his	O
cats	O
’	O
puzzle-box	O
behaviors	O
.	O
the	O
cats	O
selected	O
actions	O
from	O
those	O
that	O
they	O
instinctively	O
perform	O
in	O
their	O
current	O
situation	O
,	O
which	O
thorndike	O
called	O
their	O
“	O
instinctual	O
impulses.	O
”	O
first	O
placed	O
in	O
a	O
puzzle	O
box	O
,	O
a	O
cat	O
instinctively	O
scratches	O
,	O
claws	O
,	O
and	O
bites	O
with	O
great	O
energy	O
:	O
a	O
cat	O
’	O
s	O
instinctual	O
responses	O
to	O
ﬁnding	O
itself	O
in	O
a	O
conﬁned	O
space	O
.	O
successful	O
actions	O
are	O
selected	O
from	O
these	O
and	O
not	O
from	O
every	O
possible	O
action	B
or	O
activity	O
.	O
this	O
is	O
like	O
the	O
feature	O
of	O
our	O
formalism	O
where	O
the	O
action	B
selected	O
from	O
a	O
state	B
s	O
belongs	O
to	O
a	O
set	O
of	O
admissible	O
actions	O
,	O
a	O
(	O
s	O
)	O
.	O
specifying	O
these	O
sets	O
is	O
an	O
important	O
aspect	O
of	O
reinforcement	O
learning	O
because	O
it	O
can	O
radically	O
simplify	O
learning	O
.	O
they	O
are	O
like	O
an	O
animal	O
’	O
s	O
instinctual	O
impulses	O
.	O
on	O
the	O
other	O
hand	O
,	O
thorndike	O
’	O
s	O
cats	O
might	O
have	O
been	O
exploring	O
according	O
to	O
an	O
instinctual	O
context-speciﬁc	O
ordering	O
over	O
actions	O
rather	O
than	O
by	O
just	O
selecting	O
from	O
a	O
set	O
of	O
instinctual	O
impulses	O
.	O
this	O
is	O
another	O
way	O
to	O
make	O
reinforcement	B
learning	I
easier	O
.	O
among	O
the	O
most	O
prominent	O
animal	O
learning	O
researchers	O
inﬂuenced	O
by	O
the	O
law	O
of	O
eﬀect	O
were	O
clark	O
hull	O
(	O
e.g.	O
,	O
hull	O
,	O
1943	O
)	O
and	O
b.	O
f.	O
skinner	O
(	O
e.g.	O
,	O
skinner	O
,	O
1938	O
)	O
.	O
at	O
the	O
center	O
of	O
their	O
research	O
was	O
the	O
idea	O
of	O
selecting	O
behavior	O
on	O
the	O
basis	O
of	O
its	O
con-	O
sequences	O
.	O
reinforcement	B
learning	I
has	O
features	O
in	O
common	O
with	O
hull	O
’	O
s	O
theory	O
,	O
which	O
included	O
eligibility-like	O
mechanisms	O
and	O
secondary	O
reinforcement	O
to	O
account	O
for	O
the	O
abil-	O
ity	O
to	O
learn	O
when	O
there	O
is	O
a	O
signiﬁcant	O
time	O
interval	O
between	O
an	O
action	B
and	O
the	O
consequent	O
reinforcing	O
stimulus	O
(	O
see	O
section	O
14.4	O
)	O
.	O
randomness	O
also	O
played	O
a	O
role	O
in	O
hull	O
’	O
s	O
theory	O
through	O
what	O
he	O
called	O
“	O
behavioral	O
oscillation	O
”	O
to	O
introduce	O
exploratory	O
behavior	O
.	O
skinner	O
did	O
not	O
fully	O
subscribe	O
to	O
the	O
memory	O
aspect	O
of	O
the	O
law	O
of	O
eﬀect	O
.	O
being	O
averse	O
to	O
the	O
idea	O
of	O
associative	O
linkages	O
,	O
he	O
instead	O
emphasized	O
selection	O
from	O
spontaneously-	O
emitted	O
behavior	O
.	O
he	O
introduced	O
the	O
term	O
“	O
operant	O
”	O
to	O
emphasize	O
the	O
key	O
role	O
of	O
an	O
action	B
’	O
s	O
eﬀects	O
on	O
an	O
animal	O
’	O
s	O
environment	B
.	O
unlike	O
the	O
experiments	O
of	O
thorndike	O
and	O
others	O
,	O
which	O
consisted	O
of	O
sequences	O
of	O
separate	O
trials	O
,	O
skinner	O
’	O
s	O
operant	B
conditioning	I
experiments	O
allowed	O
animal	O
subjects	O
to	O
behave	O
for	O
extended	O
periods	O
of	O
time	O
without	O
interruption	O
.	O
he	O
invented	O
the	O
operant	B
conditioning	I
chamber	O
,	O
now	O
called	O
a	O
“	O
skinner	O
box	O
,	O
”	O
the	O
most	O
basic	O
version	O
of	O
which	O
contains	O
a	O
lever	O
or	O
key	O
that	O
an	O
animal	O
can	O
press	O
to	O
obtain	O
a	O
reward	O
,	O
such	O
as	O
food	O
or	O
water	O
,	O
which	O
would	O
be	O
delivered	O
according	O
to	O
a	O
well-	O
deﬁned	O
rule	O
,	O
called	O
a	O
reinforcement	O
schedule	O
.	O
by	O
recording	O
the	O
cumulative	O
number	O
of	O
lever	O
presses	O
as	O
a	O
function	O
of	O
time	O
,	O
skinner	O
and	O
his	O
followers	O
could	O
investigate	O
the	O
eﬀect	O
of	O
diﬀerent	O
reinforcement	O
schedules	O
on	O
the	O
animal	O
’	O
s	O
rate	O
of	O
lever-pressing	O
.	O
modeling	O
results	O
from	O
experiments	O
likes	O
these	O
using	O
the	O
reinforcement	B
learning	I
principles	O
we	O
present	O
in	O
this	O
book	O
is	O
not	O
well	O
developed	O
,	O
but	O
we	O
mention	O
some	O
exceptions	O
in	O
the	O
bibliographic	O
and	O
historical	O
remarks	O
section	O
at	O
the	O
end	O
of	O
this	O
chapter	O
.	O
another	O
of	O
skinner	O
’	O
s	O
contributions	O
resulted	O
from	O
his	O
recognition	O
of	O
the	O
eﬀectiveness	O
of	O
training	O
an	O
animal	O
by	O
reinforcing	O
successive	O
approximations	O
of	O
the	O
desired	O
behavior	O
,	O
a	O
process	O
he	O
called	O
shaping	B
.	O
although	O
this	O
technique	O
had	O
been	O
used	O
by	O
others	O
,	O
including	O
skinner	O
himself	O
,	O
its	O
signiﬁcance	O
was	O
impressed	O
upon	O
him	O
when	O
he	O
and	O
colleagues	O
were	O
attempting	O
to	O
train	O
a	O
pigeon	O
to	O
bowl	O
by	O
swiping	O
a	O
wooden	O
ball	O
with	O
its	O
beak	O
.	O
after	O
14.3.	O
instrumental	B
conditioning	I
365	O
waiting	O
for	O
a	O
long	O
time	O
without	O
seeing	O
any	O
swipe	O
that	O
they	O
could	O
reinforce	O
,	O
they	O
...	O
decided	O
to	O
reinforce	O
any	O
response	O
that	O
had	O
the	O
slightest	O
resemblance	O
to	O
a	O
swipe—perhaps	O
,	O
at	O
ﬁrst	O
,	O
merely	O
the	O
behavior	O
of	O
looking	O
at	O
the	O
ball—and	O
then	O
to	O
select	O
responses	O
which	O
more	O
closely	O
approximated	O
the	O
ﬁnal	O
form	O
.	O
the	O
result	O
amazed	O
us	O
.	O
in	O
a	O
few	O
minutes	O
,	O
the	O
ball	O
was	O
caroming	O
oﬀ	O
the	O
walls	O
of	O
the	O
box	O
as	O
if	O
the	O
pigeon	O
had	O
been	O
a	O
champion	O
squash	O
player	O
.	O
(	O
skinner	O
,	O
1958	O
,	O
p.	O
94	O
)	O
not	O
only	O
did	O
the	O
pigeon	O
learn	O
a	O
behavior	O
that	O
is	O
unusual	O
for	O
pigeons	O
,	O
it	O
learned	O
quickly	O
through	O
an	O
interactive	O
process	O
in	O
which	O
its	O
behavior	O
and	O
the	O
reinforcement	O
contingencies	O
changed	O
in	O
response	O
to	O
each	O
other	O
.	O
skinner	O
compared	O
the	O
process	O
of	O
altering	O
reinforce-	O
ment	O
contingencies	O
to	O
the	O
work	O
of	O
a	O
sculptor	O
shaping	B
clay	O
into	O
a	O
desired	O
form	O
.	O
shaping	B
is	O
a	O
powerful	O
technique	O
for	O
computational	O
reinforcement	B
learning	I
systems	O
as	O
well	O
.	O
when	O
it	O
is	O
diﬃcult	O
for	O
an	O
agent	O
to	O
receive	O
any	O
non-zero	O
reward	B
signal	I
at	O
all	O
,	O
either	O
due	O
to	O
sparseness	O
of	O
rewarding	O
situations	O
or	O
their	O
inaccessibility	O
given	O
initial	O
behavior	O
,	O
starting	O
with	O
an	O
easier	O
problem	O
and	O
incrementally	O
increasing	O
its	O
diﬃculty	O
as	O
the	O
agent	O
learns	O
can	O
be	O
an	O
eﬀective	O
,	O
and	O
sometimes	O
indispensable	O
,	O
strategy	O
.	O
a	O
concept	O
from	O
psychology	B
that	O
is	O
especially	O
relevant	O
in	O
the	O
context	O
of	O
instrumental	O
conditioning	B
is	O
motivation	B
,	O
which	O
refers	O
to	O
processes	O
that	O
inﬂuence	O
the	O
direction	O
and	O
strength	O
,	O
or	O
vigor	O
,	O
of	O
behavior	O
.	O
thorndike	O
’	O
s	O
cats	O
,	O
for	O
example	O
,	O
were	O
motivated	O
to	O
escape	O
from	O
puzzle	O
boxes	O
because	O
they	O
wanted	O
the	O
food	O
that	O
was	O
sitting	O
just	O
outside	O
.	O
obtaining	O
this	O
goal	B
was	O
rewarding	O
to	O
them	O
and	O
reinforced	O
the	O
actions	O
allowing	O
them	O
to	O
escape	O
.	O
it	O
is	O
diﬃcult	O
to	O
link	O
the	O
concept	O
of	O
motivation	O
,	O
which	O
has	O
many	O
dimensions	O
,	O
in	O
a	O
precise	O
way	O
to	O
reinforcement	B
learning	I
’	O
s	O
computational	O
perspective	O
,	O
but	O
there	O
are	O
clear	O
links	O
with	O
some	O
of	O
its	O
dimensions	O
.	O
in	O
one	O
sense	O
,	O
a	O
reinforcement	B
learning	I
agent	O
’	O
s	O
reward	B
signal	I
is	O
at	O
the	O
base	O
of	O
its	O
mo-	O
tivation	O
:	O
the	O
agent	O
is	O
motivated	O
to	O
maximize	O
the	O
total	O
reward	O
it	O
receives	O
over	O
the	O
long	O
run	O
.	O
a	O
key	O
facet	O
of	O
motivation	O
,	O
then	O
,	O
is	O
what	O
makes	O
an	O
agent	O
’	O
s	O
experience	O
rewarding	O
.	O
in	O
reinforcement	O
learning	O
,	O
reward	O
signals	O
depend	O
on	O
the	O
state	B
of	O
the	O
reinforcement	B
learning	I
agent	O
’	O
s	O
environment	B
and	O
the	O
agent	O
’	O
s	O
actions	O
.	O
further	O
,	O
as	O
pointed	O
out	O
in	O
chapter	O
1	O
,	O
the	O
state	B
of	O
the	O
agent	O
’	O
s	O
environment	B
not	O
only	O
includes	O
information	O
about	O
what	O
is	O
external	O
to	O
the	O
machine	O
,	O
like	O
an	O
organism	O
or	O
a	O
robot	O
,	O
that	O
houses	O
the	O
agent	O
,	O
but	O
also	O
what	O
is	O
internal	O
to	O
this	O
machine	O
.	O
some	O
internal	O
state	B
components	O
correspond	O
to	O
what	O
psychologists	O
call	O
an	O
animal	O
’	O
s	O
motivational	O
state	B
,	O
which	O
inﬂuences	O
what	O
is	O
rewarding	O
to	O
the	O
animal	O
.	O
for	O
example	O
,	O
an	O
animal	O
will	O
be	O
more	O
rewarded	O
by	O
eating	O
when	O
it	O
is	O
hungry	O
than	O
when	O
it	O
has	O
just	O
ﬁnished	O
a	O
satisfying	O
meal	O
.	O
the	O
concept	O
of	O
state	O
dependence	O
is	O
broad	O
enough	O
to	O
allow	O
for	O
many	O
types	O
of	O
modulating	O
inﬂuences	O
on	O
the	O
generation	O
of	O
reward	O
signals	O
.	O
value	B
functions	O
provide	O
a	O
further	O
link	O
to	O
psychologists	O
’	O
concept	O
of	O
motivation	O
.	O
if	O
the	O
most	O
basic	O
motive	O
for	O
selecting	O
an	O
action	B
is	O
to	O
obtain	O
as	O
much	O
reward	O
as	O
possible	O
,	O
for	O
a	O
reinforcement	O
learning	O
agent	O
that	O
selects	O
actions	O
using	O
a	O
value	B
function	I
,	O
a	O
more	O
proximal	O
motive	O
is	O
to	O
ascend	O
the	O
gradient	B
of	O
its	O
value	B
function	I
,	O
that	O
is	O
,	O
to	O
select	O
actions	O
expected	O
to	O
lead	O
to	O
the	O
most	O
highly-valued	O
next	O
states	O
(	O
or	O
what	O
is	O
essentially	O
the	O
same	O
thing	O
,	O
to	O
select	O
actions	O
with	O
the	O
greatest	O
action-values	O
)	O
.	O
for	O
these	O
agents	O
,	O
value	B
functions	O
are	O
the	O
main	O
driving	O
force	O
determining	O
the	O
direction	O
of	O
their	O
behavior	O
.	O
366	O
chapter	O
14	O
:	O
psychology	B
another	O
dimension	O
of	O
motivation	O
is	O
that	O
an	O
animal	O
’	O
s	O
motivational	O
state	B
not	O
only	O
inﬂu-	O
ences	O
learning	O
,	O
but	O
also	O
inﬂuences	O
the	O
strength	O
,	O
or	O
vigor	O
,	O
of	O
the	O
animal	O
’	O
s	O
behavior	O
after	O
learning	O
.	O
for	O
example	O
,	O
after	O
learning	O
to	O
ﬁnd	O
food	O
in	O
the	O
goal	O
box	O
of	O
a	O
maze	O
,	O
a	O
hungry	O
rat	O
will	O
run	O
faster	O
to	O
the	O
goal	B
box	O
than	O
one	O
that	O
is	O
not	O
hungry	O
.	O
this	O
aspect	O
of	O
motivation	O
does	O
not	O
link	O
so	O
cleanly	O
to	O
the	O
reinforcement	B
learning	I
framework	O
we	O
present	O
here	O
,	O
but	O
in	O
the	O
bibliographical	O
and	O
historical	O
remarks	O
section	O
at	O
the	O
end	O
of	O
this	O
chapter	O
we	O
cite	O
several	O
publications	O
that	O
propose	O
theories	O
of	O
behavioral	O
vigor	O
based	O
on	O
reinforcement	B
learning	I
.	O
we	O
turn	O
now	O
to	O
the	O
subject	O
of	O
learning	O
when	O
reinforcing	O
stimuli	O
occur	O
well	O
after	O
the	O
events	O
they	O
reinforce	O
.	O
the	O
mechanisms	O
used	O
by	O
reinforcement	B
learning	I
algorithms	O
to	O
enable	O
learning	O
with	O
delayed	O
reinforcement—eligibility	O
traces	O
and	O
td	O
learning—closely	O
correspond	O
to	O
psychologists	O
’	O
hypotheses	O
about	O
how	O
animals	O
can	O
learn	O
under	O
these	O
con-	O
ditions	O
.	O
14.4	O
delayed	B
reinforcement	I
the	O
law	O
of	O
eﬀect	O
requires	O
a	O
backward	O
eﬀect	O
on	O
connections	O
,	O
and	O
some	O
early	O
critics	O
of	O
the	O
law	O
could	O
not	O
conceive	O
of	O
how	O
the	O
present	O
could	O
aﬀect	O
something	O
that	O
was	O
in	O
the	O
past	O
.	O
this	O
concern	O
was	O
ampliﬁed	O
by	O
the	O
fact	O
that	O
learning	O
can	O
even	O
occur	O
when	O
there	O
is	O
a	O
considerable	O
delay	O
between	O
an	O
action	B
and	O
the	O
consequent	O
reward	O
or	O
penalty	O
.	O
simi-	O
larly	O
,	O
in	O
classical	O
conditioning	B
,	O
learning	O
can	O
occur	O
when	O
us	O
onset	O
follows	O
cs	O
oﬀset	O
by	O
a	O
non-negligible	O
time	O
interval	O
.	O
we	O
call	O
this	O
the	O
problem	O
of	O
delayed	O
reinforcement	O
,	O
which	O
is	O
related	O
to	O
what	O
minsky	O
(	O
1961	O
)	O
called	O
the	O
“	O
credit-assignment	O
problem	O
for	O
learning	O
sys-	O
tems	O
”	O
:	O
how	O
do	O
you	O
distribute	O
credit	O
for	O
success	O
among	O
the	O
many	O
decisions	O
that	O
may	O
have	O
been	O
involved	O
in	O
producing	O
it	O
?	O
the	O
reinforcement	B
learning	I
algorithms	O
presented	O
in	O
this	O
book	O
include	O
two	O
basic	O
mechanisms	O
for	O
addressing	O
this	O
problem	O
.	O
the	O
ﬁrst	O
is	O
the	O
use	O
of	O
eligibility	O
traces	O
,	O
and	O
the	O
second	O
is	O
the	O
use	O
of	O
td	O
methods	O
to	O
learn	O
value	B
functions	O
that	O
provide	O
nearly	O
immediate	O
evaluations	O
of	O
actions	O
(	O
in	O
tasks	O
like	O
instrumental	B
conditioning	I
experiments	O
)	O
or	O
that	O
provide	O
immediate	O
prediction	B
targets	O
(	O
in	O
tasks	O
like	O
classical	O
condi-	O
tioning	O
experiments	O
)	O
.	O
both	O
of	O
these	O
methods	O
correspond	O
to	O
similar	O
mechanisms	O
proposed	O
in	O
theories	O
of	O
animal	O
learning	O
.	O
pavlov	O
(	O
1927	O
)	O
pointed	O
out	O
that	O
every	O
stimulus	O
must	O
leave	O
a	O
trace	O
in	O
the	O
nervous	O
system	O
that	O
persists	O
for	O
some	O
time	O
after	O
the	O
stimulus	O
ends	O
,	O
and	O
he	O
proposed	O
that	O
stimulus	O
traces	O
make	O
learning	O
possible	O
when	O
there	O
is	O
a	O
temporal	O
gap	O
between	O
the	O
cs	O
oﬀset	O
and	O
the	O
us	O
onset	O
.	O
to	O
this	O
day	O
,	O
conditioning	B
under	O
these	O
conditions	O
is	O
called	O
trace	O
conditioning	B
(	O
page	O
348	O
)	O
.	O
assuming	O
a	O
trace	O
of	O
the	O
cs	O
remains	O
when	O
the	O
us	O
arrives	O
,	O
learning	O
occurs	O
through	O
the	O
simultaneous	O
presence	O
of	O
the	O
trace	O
and	O
the	O
us	O
.	O
we	O
discuss	O
some	O
proposals	O
for	O
trace	O
mechanisms	O
in	O
the	O
nervous	O
system	O
in	O
chapter	O
15.	O
stimulus	O
traces	O
were	O
also	O
proposed	O
as	O
a	O
means	O
for	O
bridging	O
the	O
time	O
interval	O
be-	O
tween	O
actions	O
and	O
consequent	O
rewards	O
or	O
penalties	O
in	O
instrumental	O
conditioning	B
.	O
in	O
hull	O
’	O
s	O
inﬂuential	O
learning	O
theory	O
,	O
for	O
example	O
,	O
“	O
molar	O
stimulus	O
traces	O
”	O
accounted	O
for	O
what	O
he	O
called	O
an	O
animal	O
’	O
s	O
goal	B
gradient	O
,	O
a	O
description	O
of	O
how	O
the	O
maximum	O
strength	O
of	O
an	O
instrumentally-conditioned	O
response	O
decreases	O
with	O
increasing	O
delay	O
of	O
reinforcement	O
(	O
hull	O
,	O
1932	O
,	O
1943	O
)	O
.	O
hull	O
hypothesized	O
that	O
an	O
animal	O
’	O
s	O
actions	O
leave	O
internal	O
stimuli	O
14.5.	O
cognitive	B
maps	I
367	O
whose	O
traces	O
decay	O
exponentially	O
as	O
functions	O
of	O
time	O
since	O
an	O
action	B
was	O
taken	O
.	O
look-	O
ing	B
at	O
the	O
animal	O
learning	O
data	O
available	O
at	O
the	O
time	O
,	O
he	O
hypothesized	O
that	O
the	O
traces	O
eﬀectively	O
reach	O
zero	O
after	O
30	O
to	O
40	O
seconds	O
.	O
the	O
eligibility	B
traces	I
used	O
in	O
the	O
algorithms	O
described	O
in	O
this	O
book	O
are	O
like	O
hull	O
’	O
s	O
traces	O
:	O
they	O
are	O
decaying	O
traces	O
of	O
past	O
state	B
visitations	O
,	O
or	O
of	O
past	O
state–action	O
pairs	O
.	O
eligibility	B
traces	I
were	O
introduced	O
by	O
klopf	O
(	O
1972	O
)	O
in	O
his	O
neuronal	O
theory	O
in	O
which	O
they	O
are	O
temporally-extended	O
traces	O
of	O
past	O
activity	O
at	O
synapses	O
,	O
the	O
connections	O
between	O
neurons	O
.	O
klopf	O
’	O
s	O
traces	O
are	O
more	O
complex	O
than	O
the	O
exponentially-decaying	O
traces	O
our	O
algorithms	O
use	O
,	O
and	O
we	O
discuss	O
this	O
more	O
when	O
we	O
take	O
up	O
his	O
theory	O
in	O
section	O
15.9.	O
to	O
account	O
for	O
goal	O
gradients	O
that	O
extend	O
over	O
longer	O
time	O
periods	O
than	O
spanned	O
by	O
stimulus	O
traces	O
,	O
hull	O
(	O
1943	O
)	O
proposed	O
that	O
longer	O
gradients	O
result	O
from	O
conditioned	O
re-	O
inforcement	O
passing	O
backwards	O
from	O
the	O
goal	B
,	O
a	O
process	O
acting	O
in	O
conjunction	O
with	O
his	O
molar	O
stimulus	O
traces	O
.	O
animal	O
experiments	O
showed	O
that	O
if	O
conditions	O
favor	O
the	O
devel-	O
opment	O
of	O
conditioned	O
reinforcement	O
during	O
a	O
delay	O
period	O
,	O
learning	O
does	O
not	O
decrease	O
with	O
increased	O
delay	O
as	O
much	O
as	O
it	O
does	O
under	O
conditions	O
that	O
obstruct	O
secondary	O
rein-	O
forcement	O
.	O
conditioned	O
reinforcement	O
is	O
favored	O
if	O
there	O
are	O
stimuli	O
that	O
regularly	O
occur	O
during	O
the	O
delay	O
interval	O
.	O
then	O
it	O
is	O
as	O
if	O
reward	O
is	O
not	O
actually	O
delayed	O
because	O
there	O
is	O
more	O
immediate	O
conditioned	O
reinforcement	O
.	O
hull	O
therefore	O
envisioned	O
that	O
there	O
is	O
a	O
primary	O
gradient	B
based	O
on	O
the	O
delay	O
of	O
the	O
primary	O
reinforcement	O
mediated	O
by	O
stimulus	O
traces	O
,	O
and	O
that	O
this	O
is	O
progressively	O
modiﬁed	O
,	O
and	O
lengthened	O
,	O
by	O
conditioned	O
reinforce-	O
ment	O
.	O
algorithms	O
presented	O
in	O
this	O
book	O
that	O
use	O
both	O
eligibility	B
traces	I
and	O
value	B
functions	O
to	O
enable	O
learning	O
with	O
delayed	B
reinforcement	I
correspond	O
to	O
hull	O
’	O
s	O
hypothesis	O
about	O
how	O
animals	O
are	O
able	O
to	O
learn	O
under	O
these	O
conditions	O
.	O
the	O
actor–critic	B
architecture	O
discussed	O
in	O
sections	O
13.5	O
,	O
15.7	O
,	O
and	O
15.8	O
illustrates	O
this	O
correspondence	O
most	O
clearly	O
.	O
the	O
critic	B
uses	O
a	O
td	O
algorithm	O
to	O
learn	O
a	O
value	B
function	I
associated	O
with	O
the	O
system	O
’	O
s	O
current	O
behavior	O
,	O
that	O
is	O
,	O
to	O
predict	O
the	O
current	O
policy	B
’	O
s	O
return	B
.	O
the	O
actor	O
updates	O
the	O
current	O
policy	B
based	O
on	O
the	O
critic	B
’	O
s	O
predictions	O
,	O
or	O
more	O
exactly	O
,	O
on	O
changes	O
in	O
the	O
critic	O
’	O
s	O
predictions	O
.	O
the	O
td	O
error	O
produced	O
by	O
the	O
critic	B
acts	O
as	O
a	O
conditioned	O
reinforcement	O
signal	O
for	O
the	O
actor	O
,	O
providing	O
an	O
immediate	O
evaluation	O
of	O
performance	O
even	O
when	O
the	O
primary	O
reward	B
signal	I
itself	O
is	O
considerably	O
delayed	O
.	O
algorithms	O
that	O
estimate	O
action-	O
value	B
functions	O
,	O
such	O
as	O
q-learning	O
and	O
sarsa	O
,	O
similarly	O
use	O
td	O
learning	O
principles	O
to	O
enable	O
learning	O
with	O
delayed	B
reinforcement	I
by	O
means	O
of	O
conditioned	O
reinforcement	O
.	O
the	O
close	O
parallel	O
between	O
td	O
learning	O
and	O
the	O
activity	O
of	O
dopamine	O
producing	O
neurons	O
that	O
we	O
discuss	O
in	O
chapter	O
15	O
lends	O
additional	O
support	O
to	O
links	O
between	O
reinforcement	B
learning	I
algorithms	O
and	O
this	O
aspect	O
of	O
hull	O
’	O
s	O
learning	O
theory	O
.	O
14.5	O
cognitive	B
maps	I
model-based	O
reinforcement	B
learning	I
algorithms	O
use	O
environment	B
models	O
that	O
have	O
ele-	O
ments	O
in	O
common	O
with	O
what	O
psychologists	O
call	O
cognitive	B
maps	I
.	O
recall	O
from	O
our	O
discussion	O
of	O
planning	O
and	O
learning	O
in	O
chapter	O
8	O
that	O
by	O
an	O
environment	B
model	O
we	O
mean	O
anything	O
an	O
agent	O
can	O
use	O
to	O
predict	O
how	O
its	O
environment	B
will	O
respond	O
to	O
its	O
actions	O
in	O
terms	O
of	O
state	O
transitions	O
and	O
rewards	O
,	O
and	O
by	O
planning	B
we	O
mean	O
any	O
process	O
that	O
computes	O
a	O
368	O
chapter	O
14	O
:	O
psychology	B
policy	O
from	O
such	O
a	O
model	O
.	O
environment	B
models	O
consist	O
of	O
two	O
parts	O
:	O
the	O
state-transition	O
part	O
encodes	O
knowledge	O
about	O
the	O
eﬀect	O
of	O
actions	O
on	O
state	B
changes	O
,	O
and	O
the	O
reward-	O
model	O
part	O
encodes	O
knowledge	O
about	O
the	O
reward	O
signals	O
expected	O
for	O
each	O
state	B
or	O
each	O
state–action	O
pair	O
.	O
a	O
model-based	O
algorithm	O
selects	O
actions	O
by	O
using	O
a	O
model	O
to	O
predict	O
the	O
consequences	O
of	O
possible	O
courses	O
of	O
action	O
in	O
terms	O
of	O
future	O
states	O
and	O
the	O
reward	O
signals	O
expected	O
to	O
arise	O
from	O
those	O
states	O
.	O
the	O
simplest	O
kind	O
of	O
planning	O
is	O
to	O
compare	O
the	O
predicted	O
consequences	O
of	O
collections	O
of	O
“	O
imagined	O
”	O
sequences	O
of	O
decisions	O
.	O
questions	O
about	O
whether	O
or	O
not	O
animals	O
use	O
environment	B
models	O
,	O
and	O
if	O
so	O
,	O
what	O
are	O
the	O
models	O
like	O
and	O
how	O
are	O
they	O
learned	O
,	O
have	O
played	O
inﬂuential	O
roles	O
in	O
the	O
history	O
of	O
animal	O
learning	O
research	O
.	O
some	O
researchers	O
challenged	O
the	O
then-prevailing	O
stimulus-	O
response	O
(	O
s–r	O
)	O
view	O
of	O
learning	O
and	O
behavior	O
,	O
which	O
corresponds	O
to	O
the	O
simplest	O
model-	O
free	O
way	O
of	O
learning	O
policies	O
,	O
by	O
demonstrating	O
latent	B
learning	I
.	O
in	O
the	O
earliest	O
latent	B
learning	I
experiment	O
,	O
two	O
groups	O
of	O
rats	O
were	O
run	O
in	O
a	O
maze	O
.	O
for	O
the	O
experimental	O
group	O
,	O
there	O
was	O
no	O
reward	O
during	O
the	O
ﬁrst	O
stage	O
of	O
the	O
experiment	O
,	O
but	O
food	O
was	O
suddenly	O
introduced	O
into	O
the	O
goal	B
box	O
of	O
the	O
maze	O
at	O
the	O
start	O
of	O
the	O
second	O
stage	O
.	O
for	O
the	O
control	B
group	O
,	O
food	O
was	O
in	O
the	O
goal	O
box	O
throughout	O
both	O
stages	O
.	O
the	O
question	O
was	O
whether	O
or	O
not	O
rats	O
in	O
the	O
experimental	O
group	O
would	O
have	O
learned	O
anything	O
during	O
the	O
ﬁrst	O
stage	O
in	O
the	O
absence	O
of	O
food	O
reward	O
.	O
although	O
the	O
experimental	O
rats	O
did	O
not	O
appear	O
to	O
learn	O
much	O
during	O
the	O
ﬁrst	O
,	O
unrewarded	O
,	O
stage	O
,	O
as	O
soon	O
as	O
they	O
discovered	O
the	O
food	O
that	O
was	O
introduced	O
in	O
the	O
second	O
stage	O
,	O
they	O
rapidly	O
caught	O
up	O
with	O
the	O
rats	O
in	O
the	O
control	O
group	O
.	O
it	O
was	O
concluded	O
that	O
“	O
during	O
the	O
non-reward	O
period	O
,	O
the	O
rats	O
[	O
in	O
the	O
experimental	O
group	O
]	O
were	O
developing	O
a	O
latent	B
learning	I
of	O
the	O
maze	O
which	O
they	O
were	O
able	O
to	O
utilize	O
as	O
soon	O
as	O
reward	O
was	O
introduced	O
”	O
(	O
blodgett	O
,	O
1929	O
)	O
.	O
latent	B
learning	I
is	O
most	O
closely	O
associated	O
with	O
the	O
psychologist	O
edward	O
tolman	O
,	O
who	O
interpreted	O
this	O
result	O
,	O
and	O
others	O
like	O
it	O
,	O
as	O
showing	O
that	O
animals	O
could	O
learn	O
a	O
“	O
cognitive	O
map	O
of	O
the	O
environment	B
”	O
in	O
the	O
absence	O
of	O
rewards	O
or	O
penalties	O
,	O
and	O
that	O
they	O
could	O
use	O
the	O
map	O
later	O
when	O
they	O
were	O
motivated	O
to	O
reach	O
a	O
goal	B
(	O
tolman	O
,	O
1948	O
)	O
.	O
a	O
cognitive	O
map	O
could	O
also	O
allow	O
a	O
rat	O
to	O
plan	O
a	O
route	O
to	O
the	O
goal	B
that	O
was	O
diﬀerent	O
from	O
the	O
route	O
the	O
rat	O
had	O
used	O
in	O
its	O
initial	O
exploration	O
.	O
explanations	O
of	O
results	O
like	O
these	O
led	O
to	O
the	O
enduring	O
controversy	O
lying	O
at	O
the	O
heart	O
of	O
the	O
behaviorist/cognitive	O
dichotomy	O
in	B
psychology	I
.	O
in	O
modern	O
terms	O
,	O
cognitive	B
maps	I
are	O
not	O
restricted	O
to	O
models	O
of	O
spatial	O
layouts	O
but	O
are	O
more	O
generally	O
environment	B
models	O
,	O
or	O
models	O
of	O
an	O
animal	O
’	O
s	O
“	O
task	O
space	O
”	O
(	O
e.g.	O
,	O
wilson	O
,	O
takahashi	O
,	O
schoenbaum	O
,	O
and	O
niv	O
,	O
2014	O
)	O
.	O
the	O
cognitive	O
map	O
explanation	O
of	O
latent	O
learning	O
experiments	O
is	O
analogous	O
to	O
the	O
claim	O
that	O
animals	O
use	O
model-based	O
algorithms	O
,	O
and	O
that	O
environment	B
models	O
can	O
be	O
learned	O
even	O
without	O
explicit	O
rewards	O
or	O
penalties	O
.	O
models	O
are	O
then	O
used	O
for	O
planning	O
when	O
the	O
animal	O
is	O
motivated	O
by	O
the	O
appearance	O
of	O
rewards	O
or	O
penalties	O
.	O
tolman	O
’	O
s	O
account	O
of	O
how	O
animals	O
learn	O
cognitive	B
maps	I
was	O
that	O
they	O
learn	O
stimulus-	O
stimulus	O
,	O
or	O
s–s	O
,	O
associations	O
by	O
experiencing	O
successions	O
of	O
stimuli	O
as	O
they	O
explore	O
an	O
environment	B
.	O
in	B
psychology	I
this	O
is	O
called	O
expectancy	O
theory	O
:	O
given	O
s–s	O
associations	O
,	O
the	O
occurrence	O
of	O
a	O
stimulus	O
generates	O
an	O
expectation	O
about	O
the	O
stimulus	O
to	O
come	O
next	O
.	O
this	O
is	O
much	O
like	O
what	O
control	B
engineers	O
call	O
system	B
identiﬁcation	I
,	O
in	O
which	O
a	O
model	O
of	O
a	O
system	O
with	O
unknown	O
dynamics	O
is	O
learned	O
from	O
labeled	O
training	O
examples	O
.	O
in	O
the	O
simplest	O
discrete-time	O
versions	O
,	O
training	O
examples	O
are	O
s–s	O
(	O
cid:48	O
)	O
pairs	O
,	O
where	O
s	O
is	O
a	O
state	B
14.6.	O
habitual	O
and	O
goal-directed	O
behavior	O
369	O
and	O
s	O
(	O
cid:48	O
)	O
,	O
the	O
subsequent	O
state	B
,	O
is	O
the	O
label	O
.	O
when	O
s	O
is	O
observed	O
,	O
the	O
model	O
creates	O
the	O
“	O
expectation	O
”	O
that	O
s	O
(	O
cid:48	O
)	O
will	O
be	O
observed	O
next	O
.	O
models	O
more	O
useful	O
for	O
planning	O
involve	O
actions	O
as	O
well	O
,	O
so	O
that	O
examples	O
look	O
like	O
sa–s	O
(	O
cid:48	O
)	O
,	O
where	O
s	O
(	O
cid:48	O
)	O
is	O
expected	O
when	O
action	B
a	O
is	O
executed	O
in	O
state	O
s.	O
it	O
is	O
also	O
useful	O
to	O
learn	O
how	O
the	O
environment	B
generates	O
rewards	O
.	O
in	O
this	O
case	O
,	O
examples	O
are	O
of	O
the	O
form	O
s–r	O
or	O
sa–r	O
,	O
where	O
r	O
is	O
a	O
reward	B
signal	I
associated	O
with	O
s	O
or	O
the	O
sa	O
pair	O
.	O
these	O
are	O
all	O
forms	O
of	O
supervised	O
learning	O
by	O
which	O
an	O
agent	O
can	O
acquire	O
cognitive-like	O
maps	O
whether	O
or	O
not	O
it	O
receives	O
any	O
non-zero	O
reward	O
signals	O
while	O
exploring	O
its	O
environment	B
.	O
14.6	O
habitual	O
and	O
goal-directed	O
behavior	O
the	O
distinction	O
between	O
model-free	O
and	O
model-based	O
reinforcement	B
learning	I
algorithms	O
corresponds	O
to	O
the	O
distinction	O
psychologists	O
make	O
between	O
habitual	B
and	I
goal-directed	I
control	I
of	O
learned	O
behavioral	O
patterns	O
.	O
habits	O
are	O
behavior	O
patterns	O
triggered	O
by	O
appro-	O
priate	O
stimuli	O
and	O
then	O
performed	O
more-or-less	O
automatically	O
.	O
goal-directed	O
behavior	O
,	O
according	O
to	O
how	O
psychologists	O
use	O
the	O
phrase	O
,	O
is	O
purposeful	O
in	O
the	O
sense	O
that	O
it	O
is	O
con-	O
trolled	O
by	O
knowledge	O
of	O
the	O
value	B
of	O
goals	O
and	O
the	O
relationship	O
between	O
actions	O
and	O
their	O
consequences	O
.	O
habits	O
are	O
sometimes	O
said	O
to	O
be	O
controlled	O
by	O
antecedent	O
stimuli	O
,	O
whereas	O
goal-directed	O
behavior	O
is	O
said	O
to	O
be	O
controlled	O
by	O
its	O
consequences	O
(	O
dickinson	O
,	O
1980	O
,	O
1985	O
)	O
.	O
goal-directed	O
control	B
has	O
the	O
advantage	O
that	O
it	O
can	O
rapidly	O
change	O
an	O
animal	O
’	O
s	O
behavior	O
when	O
the	O
environment	B
changes	O
its	O
way	O
of	O
reacting	O
to	O
the	O
animal	O
’	O
s	O
actions	O
.	O
while	O
habitual	O
behavior	O
responds	O
quickly	O
to	O
input	O
from	O
an	O
accustomed	O
environment	B
,	O
it	O
is	O
un-	O
able	O
to	O
quickly	O
adjust	O
to	O
changes	O
in	O
the	O
environment	O
.	O
the	O
development	O
of	O
goal-directed	O
behavioral	O
control	B
was	O
likely	O
a	O
major	O
advance	O
in	O
the	O
evolution	O
of	O
animal	O
intelligence	O
.	O
figure	O
14.7	O
illustrates	O
the	O
diﬀerence	O
between	O
model-free	O
and	O
model-based	O
decision	O
strategies	O
in	O
a	O
hypothetical	O
task	O
in	O
which	O
a	O
rat	O
has	O
to	O
navigate	O
a	O
maze	O
that	O
has	O
distinctive	O
goal	B
boxes	O
,	O
each	O
delivering	O
an	O
associated	O
reward	O
of	O
the	O
magnitude	O
shown	O
(	O
figure	O
14.7	O
top	O
)	O
.	O
starting	O
at	O
s1	O
,	O
the	O
rat	O
has	O
to	O
ﬁrst	O
select	O
left	O
(	O
l	O
)	O
or	O
right	O
(	O
r	O
)	O
and	O
then	O
has	O
to	O
select	O
l	O
or	O
r	O
again	O
at	O
s2	O
or	O
s3	O
to	O
reach	O
one	O
of	O
the	O
goal	B
boxes	O
.	O
the	O
goal	B
boxes	O
are	O
the	O
terminal	O
states	O
of	O
each	O
episode	O
of	O
the	O
rat	O
’	O
s	O
episodic	O
task	O
.	O
a	O
model-free	O
strategy	O
(	O
figure	O
14.7	O
lower	O
left	O
)	O
relies	O
on	O
stored	O
values	O
for	O
state–action	O
pairs	O
.	O
these	O
action	B
values	O
(	O
q-values	O
)	O
are	O
estimates	O
of	O
the	O
highest	O
return	B
the	O
rat	O
can	O
expect	O
for	O
each	O
action	B
taken	O
from	O
each	O
(	O
nonterminal	O
)	O
state	B
.	O
they	O
are	O
obtained	O
over	O
many	O
trials	O
of	O
running	O
the	O
maze	O
from	O
start	O
to	O
ﬁnish	O
.	O
when	O
the	O
action	B
values	O
have	O
become	O
good	O
enough	O
estimates	O
of	O
the	O
optimal	O
returns	O
,	O
the	O
rat	O
just	O
has	O
to	O
select	O
at	O
each	O
state	B
the	O
action	B
with	O
the	O
largest	O
action	B
value	O
in	O
order	O
to	O
make	O
optimal	O
decisions	O
.	O
in	O
this	O
case	O
,	O
when	O
the	O
action-value	O
estimates	O
become	O
accurate	O
enough	O
,	O
the	O
rat	O
selects	O
l	O
from	O
s1	O
and	O
r	O
from	O
s2	O
to	O
obtain	O
the	O
maximum	O
return	B
of	O
4.	O
a	O
diﬀerent	O
model-free	O
strategy	O
might	O
simply	O
rely	O
on	O
a	O
cached	O
policy	B
instead	O
of	O
action	O
values	O
,	O
making	O
direct	O
links	O
from	O
s1	O
to	O
l	O
and	O
from	O
s2	O
to	O
r.	O
in	O
neither	O
of	O
these	O
strategies	O
do	O
decisions	O
rely	O
on	O
an	O
environment	B
model	O
.	O
there	O
is	O
no	O
need	O
to	O
consult	O
a	O
state-transition	O
model	O
,	O
and	O
no	O
connection	O
is	O
required	O
between	O
the	O
features	O
of	O
the	O
goal	B
boxes	O
and	O
the	O
rewards	O
they	O
deliver	O
.	O
figure	O
14.7	O
(	O
lower	O
right	O
)	O
illustrates	O
a	O
model-based	O
strategy	O
.	O
it	O
uses	O
an	O
environment	B
model	O
consisting	O
of	O
a	O
state-transition	O
model	O
and	O
a	O
reward	O
model	O
.	O
the	O
state-transition	O
370	O
chapter	O
14	O
:	O
psychology	B
figure	O
14.7	O
:	O
model-based	O
and	O
model-free	O
strategies	O
to	O
solve	O
a	O
hypothetical	O
sequential	O
action-	O
selection	O
problem	O
.	O
top	O
:	O
a	O
rat	O
navigates	O
a	O
maze	O
with	O
distinctive	O
goal	B
boxes	O
,	O
each	O
associated	O
with	O
a	O
reward	O
having	O
the	O
value	B
shown	O
.	O
lower	O
left	O
:	O
a	O
model-free	O
strategy	O
relies	O
on	O
stored	O
action	B
values	O
for	O
all	O
the	O
state–action	O
pairs	O
obtained	O
over	O
many	O
learning	O
trials	O
.	O
to	O
make	O
decisions	O
the	O
rat	O
just	O
has	O
to	O
select	O
at	O
each	O
state	B
the	O
action	B
with	O
the	O
largest	O
action	B
value	O
for	O
that	O
state	B
.	O
lower	O
right	O
:	O
in	O
a	O
model-based	O
strategy	O
,	O
the	O
rat	O
learns	O
an	O
environment	B
model	O
,	O
consisting	O
of	O
knowledge	O
of	O
state–action-next-state	O
transitions	O
and	O
a	O
reward	O
model	O
consisting	O
of	O
knowledge	O
of	O
the	O
reward	O
associated	O
with	O
each	O
distinctive	O
goal	B
box	O
.	O
the	O
rat	O
can	O
decide	O
which	O
way	O
to	O
turn	O
at	O
each	O
state	B
by	O
using	O
the	O
model	O
to	O
simulate	O
sequences	O
of	O
action	O
choices	O
to	O
ﬁnd	O
a	O
path	O
yielding	O
the	O
highest	O
return	B
.	O
adapted	O
from	O
trends	O
in	O
cognitive	O
science	O
,	O
volume	O
10	O
,	O
number	O
8	O
,	O
y.	O
niv	O
,	O
d.	O
joel	O
,	O
and	O
p.	O
dayan	O
,	O
a	O
normative	O
perspective	O
on	O
motivation	B
,	O
p.	O
376	O
,	O
2006	O
,	O
with	O
permission	O
from	O
elsevier	O
.	O
model	O
is	O
shown	O
as	O
a	O
decision	O
tree	O
,	O
and	O
the	O
reward	O
model	O
associates	O
the	O
distinctive	O
features	O
of	O
the	O
goal	B
boxes	O
with	O
the	O
rewards	O
to	O
be	O
found	O
in	O
each	O
.	O
(	O
the	O
rewards	O
associated	O
with	O
states	O
s1	O
,	O
s2	O
,	O
and	O
s3	O
are	O
also	O
part	O
of	O
the	O
reward	O
model	O
,	O
but	O
here	O
they	O
are	O
zero	O
and	O
are	O
not	O
shown	O
.	O
)	O
a	O
model-based	O
agent	O
can	O
decide	O
which	O
way	O
to	O
turn	O
at	O
each	O
state	B
by	O
using	O
the	O
model	O
to	O
simulate	O
sequences	O
of	O
action	O
choices	O
to	O
ﬁnd	O
a	O
path	O
yielding	O
the	O
highest	O
return	B
.	O
in	O
this	O
case	O
the	O
return	B
is	O
the	O
reward	O
obtained	O
from	O
the	O
outcome	O
at	O
the	O
end	O
of	O
the	O
path	O
.	O
here	O
,	O
with	O
a	O
suﬃciently	O
accurate	O
model	O
,	O
the	O
rat	O
would	O
select	O
l	O
and	O
then	O
r	O
to	O
obtain	O
reward	O
of	O
4.	O
comparing	O
the	O
predicted	O
returns	O
of	O
simulated	O
paths	O
is	O
a	O
simple	O
form	O
of	O
planning	O
,	O
which	O
can	O
be	O
done	O
in	O
a	O
variety	O
of	O
ways	O
as	O
discussed	O
in	O
chapter	O
8.	O
when	O
the	O
environment	B
of	O
a	O
model-free	O
agent	O
changes	O
the	O
way	O
it	O
reacts	O
to	O
the	O
agent	O
’	O
s	O
actions	O
,	O
the	O
agent	O
has	O
to	O
acquire	O
new	O
experience	O
in	O
the	O
changed	O
environment	B
during	O
which	O
it	O
can	O
update	O
its	O
policy	B
and/or	O
value	B
function	I
.	O
in	O
the	O
model-free	O
strategy	O
shown	O
s1	O
,	O
ls2	O
,	O
ls2	O
,	O
rs3	O
,	O
ls3	O
,	O
rs1	O
,	O
r430432model-free=	O
4=	O
0=	O
2=	O
3rewardmodel-baseds1lrs2s3lrlr	O
(	O
4	O
)	O
s2s3s14023	O
14.6.	O
habitual	O
and	O
goal-directed	O
behavior	O
371	O
in	O
figure	O
14.7	O
(	O
lower	O
left	O
)	O
,	O
for	O
example	O
,	O
if	O
one	O
of	O
the	O
goal	B
boxes	O
were	O
to	O
somehow	O
shift	O
to	O
delivering	O
a	O
diﬀerent	O
reward	O
,	O
the	O
rat	O
would	O
have	O
to	O
traverse	O
the	O
maze	O
,	O
possibly	O
many	O
times	O
,	O
to	O
experience	O
the	O
new	O
reward	O
upon	O
reaching	O
that	O
goal	B
box	O
,	O
all	O
the	O
while	O
updating	O
either	O
its	O
policy	B
or	O
its	O
action-value	B
function	I
(	O
or	O
both	O
)	O
based	O
on	O
this	O
experience	O
.	O
the	O
key	O
point	O
is	O
that	O
for	O
a	O
model-free	O
agent	O
to	O
change	O
the	O
action	B
its	O
policy	B
speciﬁes	O
for	O
a	O
state	O
,	O
or	O
to	O
change	O
an	O
action	B
value	O
associated	O
with	O
a	O
state	B
,	O
it	O
has	O
to	O
move	O
to	O
that	O
state	B
,	O
act	O
from	O
it	O
,	O
possibly	O
many	O
times	O
,	O
and	O
experience	O
the	O
consequences	O
of	O
its	O
actions	O
.	O
a	O
model-based	O
agent	O
can	O
accommodate	O
changes	O
in	O
its	O
environment	B
without	O
this	O
kind	O
of	O
‘	O
personal	O
experience	O
’	O
with	O
the	O
states	O
and	O
actions	O
aﬀected	O
by	O
the	O
change	O
.	O
a	O
change	O
in	O
its	O
model	O
automatically	O
(	O
through	O
planning	B
)	O
changes	O
its	O
policy	B
.	O
planning	B
can	O
determine	O
the	O
consequences	O
of	O
changes	O
in	O
the	O
environment	O
that	O
have	O
never	O
been	O
linked	O
together	O
in	O
the	O
agent	O
’	O
s	O
own	O
experience	O
.	O
for	O
example	O
,	O
again	O
referring	O
to	O
the	O
maze	O
task	O
of	O
figure	O
14.7	O
,	O
imagine	O
that	O
a	O
rat	O
with	O
a	O
previously	O
learned	O
transition	O
and	O
reward	O
model	O
is	O
placed	O
directly	O
in	O
the	O
goal	O
box	O
to	O
the	O
right	O
of	O
s2	O
to	O
ﬁnd	O
that	O
the	O
reward	O
available	O
there	O
now	O
has	O
value	B
1	O
instead	O
of	O
4.	O
the	O
rat	O
’	O
s	O
reward	O
model	O
will	O
change	O
even	O
though	O
the	O
action	B
choices	O
required	O
to	O
ﬁnd	O
that	O
goal	B
box	O
in	O
the	O
maze	O
were	O
not	O
involved	O
.	O
the	O
planning	B
process	O
will	O
bring	O
knowledge	O
of	O
the	O
new	O
reward	O
to	O
bear	O
on	O
maze	O
running	O
without	O
the	O
need	O
for	O
additional	O
experience	O
in	O
the	O
maze	O
;	O
in	O
this	O
case	O
changing	O
the	O
policy	B
to	O
right	O
turns	O
at	O
both	O
s1	O
and	O
s3	O
to	O
obtain	O
a	O
return	B
of	O
3.	O
exactly	O
this	O
logic	O
is	O
the	O
basis	O
of	O
outcome-devaluation	O
experiments	O
with	O
animals	O
.	O
re-	O
sults	O
from	O
these	O
experiments	O
provide	O
insight	O
into	O
whether	O
an	O
animal	O
has	O
learned	O
a	O
habit	O
or	O
if	O
its	O
behavior	O
is	O
under	O
goal-directed	O
control	B
.	O
outcome-devaluation	O
experiments	O
are	O
like	O
latent-learning	O
experiments	O
in	O
that	O
the	O
reward	O
changes	O
from	O
one	O
stage	O
to	O
the	O
next	O
.	O
after	O
an	O
initial	O
rewarded	O
stage	O
of	O
learning	O
,	O
the	O
reward	O
value	O
of	O
an	O
outcome	O
is	O
changed	O
,	O
including	O
being	O
shifted	O
to	O
zero	O
or	O
even	O
to	O
a	O
negative	O
value	B
.	O
an	O
early	O
important	O
experiment	O
of	O
this	O
type	O
was	O
conducted	O
by	O
adams	O
and	O
dickinson	O
(	O
1981	O
)	O
.	O
they	O
trained	O
rats	O
via	O
instrumental	B
conditioning	I
until	O
the	O
rats	O
energetically	O
pressed	O
a	O
lever	O
for	O
sucrose	O
pellets	O
in	O
a	O
training	O
chamber	O
.	O
the	O
rats	O
were	O
then	O
placed	O
in	O
the	O
same	O
chamber	O
with	O
the	O
lever	O
retracted	O
and	O
allowed	O
non-contingent	O
food	O
,	O
meaning	O
that	O
pellets	O
were	O
made	O
available	O
to	O
them	O
independently	O
of	O
their	O
actions	O
.	O
after	O
15-minutes	O
of	O
this	O
free-access	O
to	O
the	O
pellets	O
,	O
rats	O
in	O
one	O
group	O
were	O
injected	O
with	O
the	O
nausea-inducing	O
poison	O
lithium	O
chloride	O
.	O
this	O
was	O
repeated	O
for	O
three	O
sessions	O
,	O
in	O
the	O
last	O
of	O
which	O
none	O
of	O
the	O
injected	O
rats	O
consumed	O
any	O
of	O
the	O
non-contingent	O
pellets	O
,	O
indicating	O
that	O
the	O
reward	O
value	O
of	O
the	O
pellets	O
had	O
been	O
decreased—the	O
pellets	O
had	O
been	O
devalued	O
.	O
in	O
the	O
next	O
stage	O
taking	O
place	O
a	O
day	O
later	O
,	O
the	O
rats	O
were	O
again	O
placed	O
in	O
the	O
chamber	O
and	O
given	O
a	O
session	O
of	O
extinction	O
training	O
,	O
meaning	O
that	O
the	O
response	O
lever	O
was	O
back	O
in	O
place	O
but	O
disconnected	O
from	O
the	O
pellet	O
dispenser	O
so	O
that	O
pressing	O
it	O
did	O
not	O
release	O
pellets	O
.	O
the	O
question	O
was	O
whether	O
the	O
rats	O
that	O
had	O
the	O
reward	O
value	O
of	O
the	O
pellets	O
decreased	O
would	O
lever-press	O
less	O
than	O
rats	O
that	O
did	O
not	O
have	O
the	O
reward	O
value	O
of	O
the	O
pellets	O
decreased	O
,	O
even	O
without	O
experiencing	O
the	O
devalued	O
reward	O
as	O
a	O
result	O
of	O
lever-pressing	O
.	O
it	O
turned	O
out	O
that	O
the	O
injected	O
rats	O
had	O
signiﬁcantly	O
lower	O
response	O
rates	O
than	O
the	O
non-injected	O
rats	O
right	O
from	O
the	O
start	O
of	O
the	O
extinction	O
trials	O
.	O
adams	O
and	O
dickinson	O
concluded	O
that	O
the	O
injected	O
rats	O
associated	O
lever	O
pressing	O
with	O
consequent	O
nausea	O
by	O
means	O
of	O
a	O
cognitive	O
map	O
linking	O
lever	O
pressing	O
to	O
pellets	O
,	O
and	O
372	O
chapter	O
14	O
:	O
psychology	B
pellets	O
to	O
nausea	O
.	O
hence	O
,	O
in	O
the	O
extinction	O
trials	O
,	O
the	O
rats	O
“	O
knew	O
”	O
that	O
the	O
consequences	O
of	O
pressing	O
the	O
lever	O
would	O
be	O
something	O
they	O
did	O
not	O
want	O
,	O
and	O
so	O
they	O
reduced	O
their	O
lever-pressing	O
right	O
from	O
the	O
start	O
.	O
the	O
important	O
point	O
is	O
that	O
they	O
reduced	O
lever-	O
pressing	O
without	O
ever	O
having	O
experienced	O
lever-pressing	O
directly	O
followed	O
by	O
being	O
sick	O
:	O
no	O
lever	O
was	O
present	O
when	O
they	O
were	O
made	O
sick	O
.	O
they	O
seemed	O
able	O
to	O
combine	O
knowledge	O
of	O
the	O
outcome	O
of	O
a	O
behavioral	O
choice	O
(	O
pressing	O
the	O
lever	O
will	O
be	O
followed	O
by	O
getting	O
a	O
pellet	O
)	O
with	O
the	O
reward	O
value	O
of	O
the	O
outcome	O
(	O
pellets	O
are	O
to	O
be	O
avoided	O
)	O
and	O
hence	O
could	O
alter	O
their	O
behavior	O
accordingly	O
.	O
not	O
every	O
psychologist	O
agrees	O
with	O
this	O
“	O
cognitive	O
”	O
account	O
of	O
this	O
kind	O
of	O
experiment	O
,	O
and	O
it	O
is	O
not	O
the	O
only	O
possible	O
way	O
to	O
explain	O
these	O
results	O
,	O
but	O
the	O
model-based	O
planning	O
explanation	O
is	O
widely	O
accepted	O
.	O
nothing	O
prevents	O
an	O
agent	O
from	O
using	O
both	O
model-free	O
and	O
model-based	O
algorithms	O
,	O
and	O
there	O
are	O
good	O
reasons	O
for	O
using	O
both	O
.	O
we	O
know	O
from	O
our	O
own	O
experience	O
that	O
with	O
enough	O
repetition	O
,	O
goal-directed	O
behavior	O
tends	O
to	O
turn	O
into	O
habitual	O
behavior	O
.	O
experi-	O
ments	O
show	O
that	O
this	O
happens	O
for	O
rats	O
too	O
.	O
adams	O
(	O
1982	O
)	O
conducted	O
an	O
experiment	O
to	O
see	O
if	O
extended	O
training	O
would	O
convert	O
goal-directed	O
behavior	O
into	O
habitual	O
behavior	O
.	O
he	O
did	O
this	O
by	O
comparing	O
the	O
eﬀect	O
of	O
outcome	O
devaluation	O
on	O
rats	O
that	O
experienced	O
diﬀer-	O
ent	O
amounts	O
of	O
training	O
.	O
if	O
extended	O
training	O
made	O
the	O
rats	O
less	O
sensitive	O
to	O
devaluation	O
compared	O
to	O
rats	O
that	O
received	O
less	O
training	O
,	O
this	O
would	O
be	O
evidence	O
that	O
extended	O
train-	O
ing	B
made	O
the	O
behavior	O
more	O
habitual	O
.	O
adams	O
’	O
experiment	O
closely	O
followed	O
the	O
adams	O
and	O
dickinson	O
(	O
1981	O
)	O
experiment	O
just	O
described	O
.	O
simplifying	O
a	O
bit	O
,	O
rats	O
in	O
one	O
group	O
were	O
trained	O
until	O
they	O
made	O
100	O
rewarded	O
lever-presses	O
,	O
and	O
rats	O
in	O
the	O
other	O
group—	O
the	O
overtrained	O
group—were	O
trained	O
until	O
they	O
made	O
500	O
rewarded	O
lever-presses	O
.	O
after	O
this	O
training	O
,	O
the	O
reward	O
value	O
of	O
the	O
pellets	O
was	O
decreased	O
(	O
using	O
lithium	O
chloride	O
in-	O
jections	O
)	O
for	O
rats	O
in	O
both	O
groups	O
.	O
then	O
both	O
groups	O
of	O
rats	O
were	O
given	O
a	O
session	O
of	O
extinction	O
training	O
.	O
adams	O
’	O
question	O
was	O
whether	O
devaluation	O
would	O
eﬀect	O
the	O
rate	O
of	O
lever-pressing	O
for	O
the	O
overtrained	O
rats	O
less	O
than	O
it	O
would	O
for	O
the	O
non-overtrained	O
rats	O
,	O
which	O
would	O
be	O
evidence	O
that	O
extended	O
training	O
reduces	O
sensitivity	O
to	O
outcome	O
devalu-	O
ation	O
.	O
it	O
turned	O
out	O
that	O
devaluation	O
strongly	O
decreased	O
the	O
lever-pressing	O
rate	O
of	O
the	O
non-overtrained	O
rats	O
.	O
for	O
the	O
overtrained	O
rats	O
,	O
in	O
contrast	O
,	O
devaluation	O
had	O
little	O
eﬀect	O
on	O
their	O
lever-pressing	O
;	O
in	O
fact	O
,	O
if	O
anything	O
,	O
it	O
made	O
it	O
more	O
vigorous	O
.	O
(	O
the	O
full	O
experi-	O
ment	O
included	O
control	B
groups	O
showing	O
that	O
the	O
diﬀerent	O
amounts	O
of	O
training	O
did	O
not	O
by	O
themselves	O
signiﬁcantly	O
eﬀect	O
lever-pressing	O
rates	O
after	O
learning	O
.	O
)	O
this	O
result	O
suggested	O
that	O
while	O
the	O
non-overtrained	O
rats	O
were	O
acting	O
in	O
a	O
goal-directed	O
manner	O
sensitive	O
to	O
their	O
knowledge	O
of	O
the	O
outcome	O
of	O
their	O
actions	O
,	O
the	O
overtrained	O
rats	O
had	O
developed	O
a	O
lever-pressing	O
habit	O
.	O
viewing	O
this	O
and	O
other	O
results	O
like	O
it	O
from	O
a	O
computational	O
perspective	O
provides	O
in-	O
sight	O
as	O
to	O
why	O
one	O
might	O
expect	O
animals	O
to	O
behave	O
habitually	O
in	O
some	O
circumstances	O
,	O
in	O
a	O
goal-directed	O
way	O
in	O
others	O
,	O
and	O
why	O
they	O
shift	O
from	O
one	O
mode	O
of	O
control	O
to	O
another	O
as	O
they	O
continue	O
to	O
learn	O
.	O
while	O
animals	O
undoubtedly	O
use	O
algorithms	O
that	O
do	O
not	O
exactly	O
match	O
those	O
we	O
have	O
presented	O
in	O
this	O
book	O
,	O
one	O
can	O
gain	O
insight	O
into	O
animal	O
behavior	O
by	O
considering	O
the	O
tradeoﬀs	O
that	O
various	O
reinforcement	B
learning	I
algorithms	O
imply	O
.	O
an	O
idea	O
developed	O
by	O
computational	O
neuroscientists	O
daw	O
,	O
niv	O
,	O
and	O
dayan	O
(	O
2005	O
)	O
is	O
that	O
animals	O
use	O
both	O
model-free	O
and	O
model-based	O
processes	O
.	O
each	O
process	O
proposes	O
an	O
action	B
,	O
and	O
the	O
action	O
chosen	O
for	O
execution	O
is	O
the	O
one	O
proposed	O
by	O
the	O
process	O
judged	O
to	O
be	O
the	O
more	O
trustworthy	O
of	O
the	O
two	O
as	O
determined	O
by	O
measures	O
of	O
conﬁdence	O
that	O
are	O
maintained	O
14.7.	O
summary	O
373	O
throughout	O
learning	O
.	O
early	O
in	O
learning	O
the	O
planning	B
process	O
of	O
a	O
model-based	O
system	O
is	O
more	O
trustworthy	O
because	O
it	O
chains	O
together	O
short-term	O
predictions	O
which	O
can	O
become	O
accurate	O
with	O
less	O
experience	O
than	O
long-term	O
predictions	O
of	O
the	O
model-free	O
process	O
.	O
but	O
with	O
continued	O
experience	O
,	O
the	O
model-free	O
process	O
becomes	O
more	O
trustworthy	O
because	O
planning	B
is	O
prone	O
to	O
making	O
mistakes	O
due	O
to	O
model	O
inaccuracies	O
and	O
short-cuts	O
neces-	O
sary	O
to	O
make	O
planning	B
feasible	O
,	O
such	O
as	O
various	O
forms	O
of	O
“	O
tree-pruning	O
”	O
:	O
the	O
removal	O
of	O
unpromising	O
search	O
tree	O
branches	O
.	O
according	O
to	O
this	O
idea	O
one	O
would	O
expect	O
a	O
shift	O
from	O
goal-directed	O
behavior	O
to	O
habitual	O
behavior	O
as	O
more	O
experience	O
accumulates	O
.	O
other	O
ideas	O
have	O
been	O
proposed	O
for	O
how	O
animals	O
arbitrate	O
between	O
goal-directed	O
and	O
habitual	O
control	B
,	O
and	O
both	O
behavioral	O
and	O
neuroscience	O
research	O
continues	O
to	O
examine	O
this	O
and	O
related	O
questions	O
.	O
the	O
distinction	O
between	O
model-free	O
and	O
model-based	O
algorithms	O
is	O
proving	O
to	O
be	O
useful	O
for	O
this	O
research	O
.	O
one	O
can	O
examine	O
the	O
computational	O
implications	O
of	O
these	O
types	O
of	O
algorithms	O
in	O
abstract	O
settings	O
that	O
expose	O
basic	O
advantages	O
and	O
limitations	O
of	O
each	O
type	O
.	O
this	O
serves	O
both	O
to	O
suggest	O
and	O
to	O
sharpen	O
questions	O
that	O
guide	O
the	O
design	B
of	I
experiments	O
necessary	O
for	O
increasing	O
psychologists	O
’	O
understanding	O
of	O
habitual	O
and	O
goal-	O
directed	O
behavioral	O
control	B
.	O
14.7	O
summary	O
our	O
goal	B
in	O
this	O
chapter	O
has	O
been	O
to	O
discuss	O
correspondences	O
between	O
reinforcement	B
learning	I
and	O
the	O
experimental	O
study	O
of	O
animal	O
learning	O
in	O
psychology	B
.	O
we	O
emphasized	O
at	O
the	O
outset	O
that	O
reinforcement	B
learning	I
as	O
described	O
in	O
this	O
book	O
is	O
not	O
intended	O
to	O
model	O
details	O
of	O
animal	O
behavior	O
.	O
it	O
is	O
an	O
abstract	O
computational	O
framework	O
that	O
ex-	O
plores	O
idealized	O
situations	O
from	O
the	O
perspective	O
of	O
artiﬁcial	O
intelligence	O
and	O
engineering	O
.	O
but	O
many	O
of	O
the	O
basic	O
reinforcement	B
learning	I
algorithms	O
were	O
inspired	O
by	O
psychologi-	O
cal	O
theories	O
,	O
and	O
in	O
some	O
cases	O
,	O
these	O
algorithms	O
have	O
contributed	O
to	O
the	O
development	O
of	O
new	O
animal	O
learning	O
models	O
.	O
this	O
chapter	O
described	O
the	O
most	O
conspicuous	O
of	O
these	O
correspondences	O
.	O
the	O
distinction	O
in	O
reinforcement	O
learning	O
between	O
algorithms	O
for	O
prediction	O
and	O
al-	O
gorithms	O
for	O
control	O
parallels	O
animal	O
learning	O
theory	O
’	O
s	O
distinction	O
between	O
classical	O
,	O
or	O
pavlovian	O
,	O
conditioning	B
and	O
instrumental	B
conditioning	I
.	O
the	O
key	O
diﬀerence	O
between	O
in-	O
strumental	O
and	O
classical	O
conditioning	B
experiments	O
is	O
that	O
in	O
the	O
former	O
the	O
reinforcing	O
stimulus	O
is	O
contingent	O
upon	O
the	O
animal	O
’	O
s	O
behavior	O
,	O
whereas	O
in	O
the	O
latter	O
it	O
is	O
not	O
.	O
learn-	O
ing	B
to	O
predict	O
via	O
a	O
td	O
algorithm	O
corresponds	O
to	O
classical	B
conditioning	I
,	O
and	O
we	O
described	O
the	O
td	O
model	O
of	O
classical	O
conditioning	B
as	O
one	O
instance	O
in	O
which	O
reinforcement	B
learning	I
principles	O
account	O
for	O
some	O
details	O
of	O
animal	O
learning	O
behavior	O
.	O
this	O
model	O
general-	O
izes	O
the	O
inﬂuential	O
rescorla–wagner	O
model	O
by	O
including	O
the	O
temporal	O
dimension	O
where	O
events	O
within	O
individual	O
trials	O
inﬂuence	O
learning	O
,	O
and	O
it	O
provides	O
an	O
account	O
of	O
second-	O
order	O
conditioning	B
,	O
where	O
predictors	O
of	O
reinforcing	O
stimuli	O
become	O
reinforcing	O
themselves	O
.	O
it	O
also	O
is	O
the	O
basis	O
of	O
an	O
inﬂuential	O
view	O
of	O
the	O
activity	O
of	O
dopamine	O
neurons	O
in	O
the	O
brain	O
,	O
something	O
we	O
take	O
up	O
in	O
chapter	O
15.	O
learning	O
by	O
trial	O
and	O
error	O
is	O
at	O
the	O
base	O
of	O
the	O
control	B
aspect	O
of	O
reinforcement	O
learning	O
.	O
we	O
presented	O
some	O
details	O
about	O
thorndike	O
’	O
s	O
experiments	O
with	O
cats	O
and	O
other	O
animals	O
374	O
chapter	O
14	O
:	O
psychology	B
that	O
led	O
to	O
his	O
law	O
of	O
eﬀect	O
,	O
which	O
we	O
discussed	O
here	O
and	O
in	O
chapter	O
1.	O
we	O
pointed	O
out	O
that	O
in	O
reinforcement	O
learning	O
,	O
exploration	O
does	O
not	O
have	O
to	O
be	O
limited	O
to	O
“	O
blind	O
groping	O
”	O
;	O
trials	O
can	O
be	O
generated	O
by	O
sophisticated	O
methods	O
using	O
innate	O
and	O
previously	O
learned	O
knowledge	O
as	O
long	O
as	O
there	O
is	O
some	O
exploration	O
.	O
we	O
discussed	O
the	O
training	O
method	O
b.	O
f.	O
skinner	O
called	O
shaping	B
in	O
which	O
reward	O
contingencies	O
are	O
progressively	O
altered	O
to	O
train	O
an	O
animal	O
to	O
successively	O
approximate	B
a	O
desired	O
behavior	O
.	O
shaping	B
is	O
not	O
only	O
indispensable	O
for	O
animal	O
training	O
,	O
it	O
is	O
also	O
an	O
eﬀective	O
tool	O
for	O
training	O
reinforcement	B
learning	I
agents	O
.	O
there	O
is	O
also	O
a	O
connection	O
to	O
the	O
idea	O
of	O
an	O
animal	O
’	O
s	O
motivational	O
state	B
,	O
which	O
inﬂuences	O
what	O
an	O
animal	O
will	O
approach	O
or	O
avoid	O
and	O
what	O
events	O
are	O
rewarding	O
or	O
punishing	O
for	O
the	O
animal	O
.	O
the	O
reinforcement	B
learning	I
algorithms	O
presented	O
in	O
this	O
book	O
include	O
two	O
basic	O
mech-	O
anisms	O
for	O
addressing	O
the	O
problem	O
of	O
delayed	O
reinforcement	O
:	O
eligibility	B
traces	I
and	O
value	B
functions	O
learned	O
via	O
td	O
algorithms	O
.	O
both	O
mechanisms	O
have	O
antecedents	O
in	O
theories	O
of	O
animal	O
learning	O
.	O
eligibility	B
traces	I
are	O
similar	O
to	O
stimulus	O
traces	O
of	O
early	O
theories	O
,	O
and	O
value	O
functions	O
correspond	O
to	O
the	O
role	O
of	O
secondary	O
reinforcement	O
in	O
providing	O
nearly	O
immediate	O
evaluative	B
feedback	I
.	O
the	O
next	O
correspondence	O
the	O
chapter	O
addressed	O
is	O
that	O
between	O
reinforcement	O
learn-	O
ing	B
’	O
s	O
environment	B
models	O
and	O
what	O
psychologists	O
call	O
cognitive	B
maps	I
.	O
experiments	O
con-	O
ducted	O
in	O
the	O
mid	O
20th	O
century	O
purported	O
to	O
demonstrate	O
the	O
ability	O
of	O
animals	O
to	O
learn	O
cognitive	B
maps	I
as	O
alternatives	O
to	O
,	O
or	O
as	O
additions	O
to	O
,	O
state–action	O
associations	O
,	O
and	O
later	O
use	O
them	O
to	O
guide	O
behavior	O
,	O
especially	O
when	O
the	O
environment	B
changes	O
unexpectedly	O
.	O
en-	O
vironment	O
models	O
in	O
reinforcement	O
learning	O
are	O
like	O
cognitive	B
maps	I
in	O
that	O
they	O
can	O
be	O
learned	O
by	O
supervised	B
learning	I
methods	O
without	O
relying	O
on	O
reward	O
signals	O
,	O
and	O
then	O
they	O
can	O
be	O
used	O
later	O
to	O
plan	O
behavior	O
.	O
reinforcement	B
learning	I
’	O
s	O
distinction	O
between	O
model-free	O
and	O
model-based	O
algorithms	O
corresponds	O
to	O
the	O
distinction	O
in	B
psychology	I
between	O
habitual	O
and	O
goal-directed	O
behavior	O
.	O
model-free	O
algorithms	O
make	O
decisions	O
by	O
accessing	O
information	O
that	O
has	O
been	O
strored	O
in	O
a	O
policy	B
or	O
an	O
action-value	B
function	I
,	O
whereas	O
model-based	O
methods	O
select	O
actions	O
as	O
the	O
result	O
of	O
planning	O
ahead	O
using	O
a	O
model	O
of	O
the	O
agent	O
’	O
s	O
environment	B
.	O
outcome-devaluation	O
experiments	O
provide	O
information	O
about	O
whether	O
an	O
animal	O
’	O
s	O
behavior	O
is	O
habitual	O
or	O
under	O
goal-directed	O
control	B
.	O
reinforcement	B
learning	I
theory	O
has	O
helped	O
clarify	O
thinking	O
about	O
these	O
issues	O
.	O
animal	O
learning	O
clearly	O
informs	O
reinforcement	B
learning	I
,	O
but	O
as	O
a	O
type	O
of	O
machine	O
learning	O
,	O
reinforcement	B
learning	I
is	O
directed	O
toward	O
designing	O
and	O
understanding	O
eﬀec-	O
tive	O
learning	O
algorithms	O
,	O
not	O
toward	O
replicating	O
or	O
explaining	O
details	O
of	O
animal	O
behavior	O
.	O
we	O
focused	O
on	O
aspects	O
of	O
animal	O
learning	O
that	O
relate	O
in	O
clear	O
ways	O
to	O
methods	O
for	O
solving	O
prediction	B
and	O
control	B
problems	O
,	O
highlighting	O
the	O
fruitful	O
two-way	O
ﬂow	O
of	O
ideas	O
between	O
reinforcement	B
learning	I
and	O
psychology	B
without	O
venturing	O
deeply	O
into	O
many	O
of	O
the	O
be-	O
havioral	O
details	O
and	O
controversies	O
that	O
have	O
occupied	O
the	O
attention	O
of	O
animal	O
learning	O
researchers	O
.	O
future	O
development	O
of	O
reinforcement	O
learning	O
theory	O
and	O
algorithms	O
will	O
likely	O
exploit	O
links	O
to	O
many	O
other	O
features	O
of	O
animal	O
learning	O
as	O
the	O
computational	O
utility	O
of	O
these	O
features	O
becomes	O
better	O
appreciated	O
.	O
we	O
expect	O
that	O
a	O
ﬂow	O
of	O
ideas	O
between	O
reinforcement	B
learning	I
and	O
psychology	B
will	O
continue	O
to	O
bear	O
fruit	O
for	O
both	O
disciplines	O
.	O
many	O
connections	O
between	O
reinforcement	B
learning	I
and	O
areas	O
of	O
psychology	O
and	O
other	O
14.7.	O
summary	O
375	O
behavioral	O
sciences	O
are	O
beyond	O
the	O
scope	O
of	O
this	O
chapter	O
.	O
we	O
largely	O
omit	O
discussing	O
links	O
to	O
the	O
psychology	B
of	O
decision	O
making	O
,	O
which	O
focuses	O
on	O
how	O
actions	O
are	O
selected	O
,	O
or	O
how	O
decisions	O
are	O
made	O
,	O
after	O
learning	O
has	O
taken	O
place	O
.	O
we	O
also	O
do	O
not	O
discuss	O
links	O
to	O
ecological	O
and	O
evolutionary	O
aspects	O
of	O
behavior	O
studied	O
by	O
ethologists	O
and	O
behav-	O
ioral	O
ecologists	O
:	O
how	O
animals	O
relate	O
to	O
one	O
another	O
and	O
to	O
their	O
physical	O
surroundings	O
,	O
and	O
how	O
their	O
behavior	O
contributes	O
to	O
evolutionary	O
ﬁtness	O
.	O
optimization	O
,	O
mdps	O
,	O
and	B
dynamic	I
programming	I
ﬁgure	O
prominently	O
in	O
these	O
ﬁelds	O
,	O
and	O
our	O
emphasis	O
on	O
agent	O
in-	O
teraction	O
with	O
dynamic	O
environments	O
connects	O
to	O
the	O
study	O
of	O
agent	O
behavior	O
in	O
complex	O
“	O
ecologies.	O
”	O
multi-agent	O
reinforcement	B
learning	I
,	O
omitted	O
in	O
this	O
book	O
,	O
has	O
connections	O
to	O
social	O
aspects	O
of	O
behavior	O
.	O
despite	O
the	O
lack	O
of	O
treatment	O
here	O
,	O
reinforcement	O
learn-	O
ing	B
should	O
by	O
no	O
means	O
be	O
interpreted	O
as	O
dismissing	O
evolutionary	O
perspectives	O
.	O
nothing	O
about	O
reinforcement	B
learning	I
implies	O
a	O
tabula	O
rasa	O
view	O
of	O
learning	O
and	O
behavior	O
.	O
indeed	O
,	O
experience	O
with	O
engineering	O
applications	O
has	O
highlighted	O
the	O
importance	O
of	O
building	O
into	O
reinforcement	B
learning	I
systems	O
knowledge	O
that	O
is	O
analogous	O
to	O
what	O
evolution	B
provides	O
to	O
animals	O
.	O
bibliographical	O
and	O
historical	O
remarks	O
ludvig	O
,	O
bellemare	O
,	O
and	O
pearson	O
(	O
2011	O
)	O
and	O
shah	O
(	O
2012	O
)	O
review	O
reinforcement	B
learning	I
in	O
the	O
contexts	O
of	O
psychology	O
and	O
neuroscience	O
.	O
these	O
publications	O
are	O
useful	O
companions	O
to	O
this	O
chapter	O
and	O
the	O
following	O
chapter	O
on	O
reinforcement	B
learning	I
and	O
neuroscience	B
.	O
14.1	O
dayan	O
,	O
niv	O
,	O
seymour	O
,	O
and	O
daw	O
(	O
2006	O
)	O
focused	O
on	O
interactions	O
between	O
clas-	O
sical	O
and	O
instrumental	O
conditioning	B
,	O
particularly	O
situations	O
where	O
classically-	O
conditioned	O
and	O
instrumental	O
responses	O
are	O
in	O
conﬂict	O
.	O
they	O
proposed	O
a	O
q-	O
learning	O
framework	O
for	O
modeling	O
aspects	O
of	O
this	O
interaction	O
.	O
modayil	O
and	O
sut-	O
ton	O
(	O
2014	O
)	O
used	O
a	O
mobile	O
robot	O
to	O
demonstrate	O
the	O
eﬀectiveness	O
of	O
a	O
control	B
method	O
combining	O
a	O
ﬁxed	O
response	O
with	O
online	O
prediction	B
learning	O
.	O
calling	O
this	O
pavlovian	O
control	B
,	O
they	O
emphasized	O
that	O
it	O
diﬀers	O
from	O
the	O
usual	O
control	B
methods	O
of	O
reinforcement	O
learning	O
,	O
being	O
based	O
on	O
predictively	O
executing	O
ﬁxed	O
responses	O
and	O
not	O
on	O
reward	O
maximization	O
.	O
the	O
electro-mechanical	O
machine	O
of	O
ross	O
(	O
1933	O
)	O
and	O
especially	O
the	O
learning	O
version	O
of	O
walter	O
’	O
s	O
turtle	O
(	O
walter	O
,	O
1951	O
)	O
were	O
very	O
early	O
illustrations	O
of	O
pavlovian	O
control	B
.	O
14.2.1	O
kamin	O
(	O
1968	O
)	O
ﬁrst	O
reported	O
blocking	B
,	O
now	O
commonly	O
known	O
as	O
kamin	O
block-	O
ing	B
,	O
in	O
classical	O
conditioning	B
.	O
moore	O
and	O
schmajuk	O
(	O
2008	O
)	O
provide	O
an	O
excel-	O
lent	O
summary	O
of	O
the	O
blocking	B
phenomenon	O
,	O
the	O
research	O
it	O
stimulated	O
,	O
and	O
its	O
lasting	O
inﬂuence	O
on	O
animal	O
learning	O
theory	O
.	O
gibbs	O
,	O
cool	O
,	O
land	O
,	O
kehoe	O
,	O
and	O
gormezano	O
(	O
1991	O
)	O
describe	O
second-order	O
conditioning	B
of	O
the	O
rabbit	O
’	O
s	O
nictitating	O
membrane	O
response	O
and	O
its	O
relationship	O
to	O
conditioning	B
with	O
serial-compound	O
stimuli	O
.	O
finch	O
and	O
culler	O
(	O
1934	O
)	O
reported	O
obtaining	O
ﬁfth-order	O
conditioning	B
of	O
a	O
dog	O
’	O
s	O
foreleg	O
withdrawal	O
“	O
when	O
the	O
motivation	B
of	O
the	O
animal	O
is	O
maintained	O
through	O
the	O
various	O
orders.	O
”	O
14.2.2	O
the	O
idea	O
built	O
into	O
the	O
rescorla–wagner	O
model	O
that	O
learning	O
occurs	O
when	O
ani-	O
376	O
chapter	O
14	O
:	O
psychology	B
mals	O
are	O
surprised	O
is	O
derived	O
from	O
kamin	O
(	O
1969	O
)	O
.	O
models	O
of	O
classical	O
conditioning	B
other	O
than	O
rescorla	O
and	O
wagner	O
’	O
s	O
include	O
the	O
models	O
of	O
klopf	O
(	O
1988	O
)	O
,	O
grossberg	O
(	O
1975	O
)	O
,	O
mackintosh	O
(	O
1975	O
)	O
,	O
moore	O
and	O
stickney	O
(	O
1980	O
)	O
,	O
pearce	O
and	O
hall	O
(	O
1980	O
)	O
,	O
and	O
courville	O
,	O
daw	O
,	O
and	O
touretzky	O
(	O
2006	O
)	O
.	O
schmajuk	O
(	O
2008	O
)	O
review	O
models	O
of	O
classical	O
conditioning	B
.	O
14.2.3	O
an	O
early	O
version	O
of	O
the	O
td	O
model	O
of	O
classical	O
conditioning	B
appeared	O
in	O
sutton	O
and	O
barto	O
(	O
1981a	O
)	O
,	O
which	O
also	O
included	O
the	O
early	O
model	O
’	O
s	O
prediction	B
that	O
tem-	O
poral	O
primacy	O
overrides	O
blocking	B
,	O
later	O
shown	O
by	O
kehoe	O
,	O
schreurs	O
,	O
and	O
graham	O
(	O
1987	O
)	O
to	O
occur	O
in	O
the	O
rabbit	O
nictitating	O
membrane	O
preparation	O
.	O
sutton	O
and	O
barto	O
(	O
1981a	O
)	O
contains	O
the	O
earliest	O
recognition	O
of	O
the	O
near	O
identity	O
between	O
the	O
rescorla–wagner	O
model	O
and	O
the	O
least-mean-square	O
(	O
lms	O
)	O
,	O
or	O
widrow-hoﬀ	O
,	O
learning	O
rule	O
(	O
widrow	O
and	O
hoﬀ	O
,	O
1960	O
)	O
.	O
this	O
early	O
model	O
was	O
revised	O
following	O
sutton	O
’	O
s	O
development	O
of	O
the	O
td	O
algorithm	O
(	O
sutton	O
,	O
1984	O
,	O
1988	O
)	O
and	O
was	O
ﬁrst	O
presented	O
as	O
the	O
td	O
model	O
in	O
sutton	O
and	O
barto	O
(	O
1987	O
)	O
and	O
more	O
completely	O
in	O
sutton	O
and	O
barto	O
(	O
1990	O
)	O
,	O
upon	O
which	O
this	O
section	O
is	O
largely	O
based	O
.	O
additional	O
ex-	O
ploration	O
of	O
the	O
td	O
model	O
and	O
its	O
possible	O
neural	B
implementation	O
was	O
conducted	O
by	O
moore	O
and	O
colleagues	O
(	O
moore	O
,	O
desmond	O
,	O
berthier	O
,	O
blazis	O
,	O
sutton	O
,	O
and	O
barto	O
,	O
1986	O
;	O
moore	O
and	O
blazis	O
,	O
1989	O
;	O
moore	O
,	O
choi	O
,	O
and	O
brunzell	O
,	O
1998	O
;	O
moore	O
,	O
marks	O
,	O
castagna	O
,	O
and	O
polewan	O
,	O
2001	O
)	O
.	O
klopf	O
’	O
s	O
(	O
1988	O
)	O
drive-reinforcement	O
theory	O
of	O
classical	O
conditioning	B
extends	O
the	O
td	O
model	O
to	O
address	O
additional	O
experimental	O
details	O
,	O
such	O
as	O
the	O
s-shape	O
of	O
acquisition	O
curves	O
.	O
in	O
some	O
of	O
these	O
publications	O
td	O
is	O
taken	O
to	O
mean	O
time	O
derivative	O
instead	O
of	O
temporal	O
diﬀerence	O
.	O
14.2.4	O
ludvig	O
,	O
sutton	O
,	O
and	O
kehoe	O
(	O
2012	O
)	O
evaluated	O
the	O
performance	O
of	O
the	O
td	O
model	O
in	O
previously	O
unexplored	O
tasks	O
involving	O
classical	B
conditioning	I
and	O
examined	O
the	O
inﬂuence	O
of	O
various	O
stimulus	O
representations	O
,	O
including	O
the	O
microstimulus	O
representation	O
that	O
they	O
introduced	O
earlier	O
(	O
ludvig	O
,	O
sutton	O
,	O
and	O
kehoe	O
,	O
2008	O
)	O
.	O
earlier	O
investigations	O
of	O
the	O
inﬂuence	O
of	O
various	O
stimulus	O
representations	O
and	O
their	O
possible	O
neural	B
implementations	O
on	O
response	O
timing	O
and	O
topography	O
in	O
the	O
context	O
of	O
the	O
td	O
model	O
are	O
those	O
of	O
moore	O
and	O
colleagues	O
cited	O
above	O
.	O
although	O
not	O
in	O
the	O
context	O
of	O
the	O
td	O
model	O
,	O
representations	O
like	O
the	O
micros-	O
timulus	O
representation	O
of	O
ludvig	O
et	O
al	O
.	O
(	O
2012	O
)	O
have	O
been	O
proposed	O
and	O
studied	O
by	O
grossberg	O
and	O
schmajuk	O
(	O
1989	O
)	O
,	O
brown	O
,	O
bullock	O
,	O
and	O
grossberg	O
(	O
1999	O
)	O
,	O
buhusi	O
and	O
schmajuk	O
(	O
1999	O
)	O
,	O
and	O
machado	O
(	O
1997	O
)	O
.	O
14.3	O
section	O
1.7	O
includes	O
comments	O
on	O
the	O
history	B
of	I
trial-and-error	O
learning	O
and	O
the	O
law	O
of	O
eﬀect	O
.	O
the	O
idea	O
that	O
thorndikes	O
cats	O
might	O
have	O
been	O
exploring	O
according	O
to	O
an	O
instinctual	O
context-speciﬁc	O
ordering	O
over	O
actions	O
rather	O
than	O
by	O
just	O
selecting	O
from	O
a	O
set	O
of	O
instinctual	O
impulses	O
was	O
suggested	O
by	O
peter	O
dayan	O
(	O
personal	O
communication	O
)	O
.	O
selfridge	O
,	O
sutton	O
,	O
and	O
barto	O
(	O
1985	O
)	O
illustrated	O
the	O
eﬀectiveness	O
of	O
shaping	O
in	O
a	O
pole-balancing	O
reinforcement	B
learning	I
task	O
.	O
other	O
examples	O
of	O
shaping	O
in	O
reinforcement	O
learning	O
are	O
gullapalli	O
and	O
barto	O
(	O
1992	O
)	O
,	O
mahadevan	O
and	O
connell	O
(	O
1992	O
)	O
,	O
mataric	O
(	O
1994	O
)	O
,	O
dorigo	O
and	O
colombette	O
(	O
1994	O
)	O
,	O
saksida	O
,	O
raymond	O
,	O
and	O
touretzky	O
(	O
1997	O
)	O
,	O
and	O
randløv	O
and	O
alstrøm	O
(	O
1998	O
)	O
.	O
ng	O
(	O
2003	O
)	O
and	O
ng	O
,	O
harada	O
,	O
and	O
russell	O
(	O
1999	O
)	O
used	O
the	O
term	O
shaping	B
in	O
a	O
sense	O
14.7.	O
summary	O
377	O
14.4	O
14.5	O
14.6	O
somewhat	O
diﬀerent	O
from	O
skinner	O
’	O
s	O
,	O
focussing	O
on	O
the	O
problem	O
of	O
how	O
to	O
alter	O
the	O
reward	B
signal	I
without	O
altering	O
the	O
set	O
of	O
optimal	O
policies	O
.	O
dickinson	O
and	O
balleine	O
(	O
2002	O
)	O
discuss	O
the	O
complexity	O
of	O
the	O
interaction	O
between	O
learning	O
and	O
motivation	B
.	O
wise	O
(	O
2004	O
)	O
provides	O
an	O
overview	O
of	O
reinforcement	O
learning	O
and	O
its	O
relation	O
to	O
motivation	B
.	O
daw	O
and	O
shohamy	O
(	O
2008	O
)	O
link	O
motivation	B
and	O
learning	O
to	O
aspects	O
of	O
reinforcement	O
learning	O
theory	O
.	O
see	O
also	O
mcclure	O
,	O
daw	O
,	O
and	O
montague	O
(	O
2003	O
)	O
,	O
niv	O
,	O
joel	O
,	O
and	O
dayan	O
(	O
2006	O
)	O
,	O
rangel	O
,	O
camerer	O
,	O
and	O
montague	O
(	O
2008	O
)	O
,	O
and	O
dayan	O
and	O
berridge	O
(	O
2014	O
)	O
.	O
mcclure	O
et	O
al	O
.	O
(	O
2003	O
)	O
,	O
niv	O
,	O
daw	O
,	O
and	O
dayan	O
(	O
2006	O
)	O
,	O
and	O
niv	O
,	O
daw	O
,	O
joel	O
,	O
and	O
dayan	O
(	O
2007	O
)	O
present	O
theories	O
of	O
behavioral	O
vigor	O
related	O
to	O
the	O
reinforcement	B
learning	I
framework	O
.	O
spence	O
,	O
hull	O
’	O
s	O
student	O
and	O
collaborator	O
at	O
yale	O
,	O
elaborated	O
the	O
role	O
of	O
higher-	O
order	O
reinforcement	O
in	O
addressing	O
the	O
problem	O
of	O
delayed	O
reinforcement	O
(	O
spence	O
,	O
1947	O
)	O
.	O
learning	O
over	O
very	O
long	O
delays	O
,	O
as	O
in	O
taste-aversion	O
conditioning	B
with	O
delays	O
up	O
to	O
several	O
hours	O
,	O
led	O
to	O
interference	O
theories	O
as	O
alternatives	O
to	O
decaying-	O
trace	O
theories	O
(	O
e.g.	O
,	O
revusky	O
and	O
garcia	O
,	O
1970	O
;	O
boakes	O
and	O
costa	O
,	O
2014	O
)	O
.	O
other	O
views	O
of	O
learning	O
under	O
delayed	B
reinforcement	I
invoke	O
roles	O
for	O
awareness	O
and	O
working	O
memory	O
(	O
e.g.	O
,	O
clark	O
and	O
squire	O
,	O
1998	O
;	O
seo	O
,	O
barraclough	O
,	O
and	O
lee	O
,	O
2007	O
)	O
.	O
thistlethwaite	O
(	O
1951	O
)	O
provides	O
an	O
extensive	O
review	O
of	O
latent	O
learning	O
experiments	O
up	O
to	O
the	O
time	O
of	O
its	O
publication	O
.	O
ljung	O
(	O
1998	O
)	O
provides	O
an	O
overview	O
of	O
model	O
learning	O
,	O
or	O
system	B
identiﬁcation	I
,	O
techniques	O
in	O
engineering	O
.	O
gopnik	O
,	O
glymour	O
,	O
sobel	O
,	O
schulz	O
,	O
kushnir	O
,	O
and	O
danks	O
(	O
2004	O
)	O
present	O
a	O
bayesian	O
theory	O
about	O
how	O
children	O
learn	O
models	O
.	O
connections	O
between	O
habitual	O
and	O
goal-directed	O
behavior	O
and	O
model-free	O
and	O
model-based	O
reinforcement	B
learning	I
were	O
ﬁrst	O
proposed	O
by	O
daw	O
,	O
niv	O
,	O
and	O
dayan	O
(	O
2005	O
)	O
.	O
the	O
hypothetical	O
maze	O
task	O
used	O
to	O
explain	O
habitual	O
and	O
goal-directed	O
behavioral	O
control	B
is	O
based	O
on	O
the	O
explanation	O
of	O
niv	O
,	O
joel	O
,	O
and	O
dayan	O
(	O
2006	O
)	O
.	O
dolan	O
and	O
dayan	O
(	O
2013	O
)	O
review	O
four	O
generations	O
of	O
experimental	O
research	O
re-	O
lated	O
to	O
this	O
issue	O
and	O
discuss	O
how	O
it	O
can	O
move	O
forward	O
on	O
the	O
basis	O
of	O
reinforce-	O
ment	O
learning	O
’	O
s	O
model-free/model-based	O
distinction	O
.	O
dickinson	O
(	O
1980	O
,	O
1985	O
)	O
and	O
dickinson	O
and	O
balleine	O
(	O
2002	O
)	O
discuss	O
experimental	O
evidence	O
related	O
to	O
this	O
dis-	O
tinction	O
.	O
donahoe	O
and	O
burgos	O
(	O
2000	O
)	O
alternatively	O
argue	O
that	O
model-free	O
pro-	O
cesses	O
can	O
account	O
for	O
the	O
results	O
of	O
outcome-devaluation	O
experiments	O
.	O
dayan	O
and	O
berridge	O
(	O
2014	O
)	O
argue	O
that	O
classical	B
conditioning	I
involves	O
model-based	O
pro-	O
cesses	O
.	O
rangel	O
,	O
camerer	O
,	O
and	O
montague	O
(	O
2008	O
)	O
review	O
many	O
of	O
the	O
outstanding	O
issues	O
involving	O
habitual	O
,	O
goal-directed	O
,	O
and	O
pavlovian	O
modes	O
of	O
control	O
.	O
comments	O
on	O
terminology—	O
the	O
traditional	O
meaning	O
of	O
reinforcement	O
in	O
psychol-	O
ogy	O
is	O
the	O
strengthening	O
of	O
a	O
pattern	O
of	O
behavior	O
(	O
by	O
increasing	O
either	O
its	O
intensity	O
or	O
frequency	O
)	O
as	O
a	O
result	O
of	O
an	O
animal	O
receiving	O
a	O
stimulus	O
(	O
or	O
experiencing	O
the	O
omission	O
of	O
a	O
stimulus	O
)	O
in	O
an	O
appropriate	O
temporal	O
relationship	O
with	O
another	O
stimulus	O
or	O
with	O
a	O
response	O
.	O
reinforcement	O
produces	O
changes	O
that	O
remain	O
in	O
future	O
behavior	O
.	O
sometimes	O
in	B
psychology	I
reinforcement	O
refers	O
to	O
the	O
process	O
of	O
producing	O
lasting	O
changes	O
in	O
behav-	O
ior	O
,	O
whether	O
the	O
changes	O
strengthen	O
or	O
weaken	O
a	O
behavior	O
pattern	O
(	O
mackintosh	O
,	O
1983	O
)	O
.	O
378	O
chapter	O
14	O
:	O
psychology	B
letting	O
reinforcement	O
refer	O
to	O
weakening	O
in	O
addition	O
to	O
strengthening	O
is	O
at	O
odds	O
with	O
the	O
everyday	O
meaning	O
of	O
reinforce	O
,	O
and	O
its	O
traditional	O
use	O
in	B
psychology	I
,	O
but	O
it	O
is	O
a	O
useful	O
extension	O
that	O
we	O
have	O
adopted	O
here	O
.	O
in	O
either	O
case	O
,	O
a	O
stimulus	O
considered	O
to	O
be	O
the	O
cause	O
of	O
the	O
behavioral	O
change	O
is	O
called	O
a	O
reinforcer	O
.	O
psychologists	O
do	O
not	O
generally	O
use	O
the	O
speciﬁc	O
phrase	O
reinforcement	B
learning	I
as	O
we	O
do	O
.	O
animal	O
learning	O
pioneers	O
probably	O
regarded	O
reinforcement	O
and	O
learning	O
as	O
being	O
synonymous	O
,	O
so	O
it	O
would	O
be	O
redundant	O
to	O
use	O
both	O
words	O
.	O
our	O
use	O
of	O
the	O
phrase	O
follows	O
its	O
use	O
in	O
computational	O
and	O
engineering	O
research	O
,	O
inﬂuenced	O
mostly	O
by	O
minsky	O
(	O
1961	O
)	O
.	O
but	O
the	O
phrase	O
is	O
lately	O
gaining	O
currency	O
in	B
psychology	I
and	O
neuroscience	B
,	O
likely	O
because	O
strong	O
parallels	O
have	O
surfaced	O
between	O
reinforcement	B
learning	I
algorithms	O
and	O
animal	O
learning—parallels	O
described	O
in	O
this	O
chapter	O
and	O
the	O
next	O
.	O
according	O
to	O
common	O
usage	O
,	O
a	O
reward	O
is	O
an	O
object	O
or	O
event	O
that	O
an	O
animal	O
will	O
ap-	O
proach	O
and	O
work	O
for	O
.	O
a	O
reward	O
may	O
be	O
given	O
to	O
an	O
animal	O
in	O
recognition	O
of	O
its	O
‘	O
good	O
’	O
behavior	O
,	O
or	O
given	O
in	O
order	O
to	O
make	O
the	O
animal	O
’	O
s	O
behavior	O
‘	O
better.	O
’	O
similarly	O
,	O
a	O
penalty	O
is	O
an	O
object	O
or	O
event	O
that	O
the	O
animal	O
usually	O
avoids	O
and	O
that	O
is	O
given	O
as	O
a	O
consequence	O
of	O
‘	O
bad	O
’	O
behavior	O
,	O
usually	O
in	O
order	O
to	O
change	O
that	O
behavior	O
.	O
primary	O
reward	O
is	O
reward	O
due	O
to	O
machinery	O
built	O
into	O
an	O
animal	O
’	O
s	O
nervous	O
system	O
by	O
evolution	B
to	O
improve	O
its	O
chances	O
of	O
survival	O
and	O
reproduction	O
,	O
e.g.	O
,	O
reward	O
produced	O
by	O
the	O
taste	O
of	O
nourishing	O
food	O
,	O
sexual	O
contact	O
,	O
successful	O
escape	O
,	O
and	O
many	O
other	O
stimuli	O
and	O
events	O
that	O
predicted	O
reproductive	O
success	O
over	O
the	O
animal	O
’	O
s	O
ancestral	O
history	O
.	O
as	O
explained	O
in	O
section	O
14.2.1	O
,	O
higher-order	O
reward	O
is	O
reward	O
delivered	O
by	O
stimuli	O
that	O
predict	O
primary	O
reward	O
,	O
either	O
directly	O
or	O
indirectly	O
by	O
predicting	O
other	O
stimuli	O
that	O
predict	O
primary	O
reward	O
.	O
reward	O
is	O
secondary	O
if	O
its	O
rewarding	O
quality	O
is	O
the	O
result	O
of	O
directly	O
predicting	O
primary	O
reward	O
.	O
in	O
this	O
book	O
we	O
call	O
rt	O
the	O
‘	O
reward	B
signal	I
at	O
time	O
t	O
’	O
or	O
sometimes	O
just	O
the	O
‘	O
reward	O
at	O
time	O
t	O
,	O
’	O
but	O
we	O
do	O
not	O
think	O
of	O
it	O
as	O
an	O
object	O
or	O
event	O
in	O
the	O
agent	O
’	O
s	O
environment	B
.	O
because	O
rt	O
is	O
a	O
number—not	O
an	O
object	O
or	O
an	O
event—it	O
is	O
more	O
like	O
a	O
reward	B
signal	I
in	O
neuroscience	B
,	O
which	O
is	O
a	O
signal	O
internal	O
to	O
the	O
brain	O
,	O
like	O
the	O
activity	O
of	O
neurons	O
,	O
that	O
inﬂuences	O
decision	O
making	O
and	O
learning	O
.	O
this	O
signal	O
might	O
be	O
triggered	O
when	O
the	O
animal	O
perceives	O
an	O
attractive	O
(	O
or	O
an	O
aversive	O
)	O
object	O
,	O
but	O
it	O
can	O
also	O
be	O
triggered	O
by	O
things	O
that	O
do	O
not	O
physically	O
exist	O
in	O
the	O
animal	O
’	O
s	O
external	O
environment	B
,	O
such	O
as	O
memories	O
,	O
ideas	O
,	O
or	O
hallucinations	O
.	O
because	O
our	O
rt	O
can	O
be	O
positive	O
,	O
negative	O
,	O
or	O
zero	O
,	O
it	O
might	O
be	O
better	O
to	O
call	O
a	O
negative	O
rt	O
a	O
penalty	O
,	O
and	O
an	O
rt	O
equal	O
to	O
zero	O
a	O
neutral	O
signal	O
,	O
but	O
for	O
simplicity	O
we	O
generally	O
avoid	O
these	O
terms	O
.	O
in	O
reinforcement	O
learning	O
,	O
the	O
process	O
that	O
generates	O
all	O
the	O
rts	O
deﬁnes	O
the	O
problem	O
the	O
agent	O
is	O
trying	O
to	O
solve	O
.	O
the	O
agent	O
’	O
s	O
objective	O
is	O
to	O
keep	O
the	O
magnitude	O
of	O
rt	O
as	O
large	O
as	O
possible	O
over	O
time	O
.	O
in	O
this	O
respect	O
,	O
rt	O
is	O
like	O
primary	O
reward	O
for	O
an	O
animal	O
if	O
we	O
think	O
of	O
the	O
problem	O
the	O
animal	O
faces	O
as	O
the	O
problem	O
of	O
obtaining	O
as	O
much	O
primary	O
reward	O
as	O
possible	O
over	O
its	O
lifetime	O
(	O
and	O
thereby	O
,	O
through	O
the	O
prospective	O
“	O
wisdom	O
”	O
of	O
evolution	O
,	O
improve	O
its	O
chances	O
of	O
solving	O
its	O
real	O
problem	O
,	O
which	O
is	O
to	O
pass	O
its	O
genes	O
on	O
to	O
future	O
generations	O
)	O
.	O
however	O
,	O
as	O
we	O
suggest	O
in	O
chapter	O
15	O
,	O
it	O
is	O
unlikely	O
that	O
there	O
is	O
a	O
single	O
“	O
master	O
”	O
reward	B
signal	I
like	O
rt	O
in	O
an	O
animal	O
’	O
s	O
brain	O
.	O
not	O
all	O
reinforcers	O
are	O
rewards	O
or	O
penalties	O
.	O
sometimes	O
reinforcement	O
is	O
not	O
the	O
result	O
of	O
an	O
animal	O
receiving	O
a	O
stimulus	O
that	O
evaluates	O
its	O
behavior	O
by	O
labeling	O
the	O
behavior	O
good	O
or	O
bad	O
.	O
a	O
behavior	O
pattern	O
can	O
be	O
reinforced	O
by	O
a	O
stimulus	O
that	O
arrives	O
to	O
an	O
animal	O
14.7.	O
summary	O
379	O
no	O
matter	O
how	O
the	O
animal	O
behaved	O
.	O
as	O
described	O
in	O
section	O
14.1	O
,	O
whether	O
the	O
delivery	O
of	O
reinforcer	O
depends	O
,	O
or	O
does	O
not	O
depend	O
,	O
on	O
preceding	O
behavior	O
is	O
the	O
deﬁning	O
diﬀerence	O
between	O
instrumental	O
,	O
or	O
operant	O
,	O
conditioning	B
experiments	O
and	O
classical	O
,	O
or	O
pavlovian	O
,	O
conditioning	B
experiments	O
.	O
reinforcement	O
is	O
at	O
work	O
in	O
both	O
types	O
of	O
experiments	O
,	O
but	O
only	O
in	O
the	O
former	O
is	O
it	O
feedback	O
that	O
evaluates	O
past	O
behavior	O
.	O
(	O
though	O
it	O
has	O
often	O
been	O
pointed	O
out	O
that	O
even	O
when	O
the	O
reinforcing	O
us	O
in	O
a	O
classical	B
conditioning	I
experiment	O
is	O
not	O
contingent	O
on	O
the	O
subject	O
’	O
s	O
preceding	O
behavior	O
,	O
its	O
reinforcing	O
value	B
can	O
be	O
inﬂuenced	O
by	O
this	O
behavior	O
,	O
an	O
example	O
being	O
that	O
a	O
closed	O
eye	O
makes	O
an	O
air	O
puﬀ	O
to	O
the	O
eye	O
less	O
aversive	O
.	O
)	O
the	O
distinction	O
between	O
reward	O
signals	O
and	B
reinforcement	I
signals	O
is	O
a	O
crucial	O
point	O
when	O
we	O
discuss	O
neural	B
correlates	O
of	O
these	O
signals	O
in	O
the	O
next	O
chapter	O
.	O
like	O
a	O
reward	B
signal	I
,	O
for	O
us	O
,	O
the	O
reinforcement	B
signal	I
at	O
any	O
speciﬁc	O
time	O
is	O
a	O
positive	O
or	O
negative	O
number	O
,	O
or	O
zero	O
.	O
a	O
reinforcement	B
signal	I
is	O
the	O
major	O
factor	O
directing	O
changes	O
a	O
learning	O
algorithm	O
makes	O
in	O
an	O
agent	O
’	O
s	O
policy	B
,	O
value	B
estimates	O
,	O
or	O
environment	B
models	O
.	O
the	O
deﬁnition	O
that	O
makes	O
the	O
most	O
sense	O
to	O
us	O
is	O
that	O
a	O
reinforcement	B
signal	I
at	O
any	O
time	O
is	O
a	O
number	O
that	O
multiplies	O
(	O
possibly	O
along	O
with	O
some	O
constants	O
)	O
a	O
vector	B
to	O
determine	O
parameter	O
updates	O
in	O
some	O
learning	O
algorithm	O
.	O
for	O
some	O
algorithms	O
,	O
the	O
reward	B
signal	I
alone	O
is	O
the	O
critical	O
multiplier	O
in	O
the	O
parameter-	O
update	O
equation	O
.	O
for	O
these	O
algorithms	O
the	O
reinforcement	B
signal	I
is	O
the	O
same	O
as	O
the	O
re-	O
ward	O
signal	O
.	O
but	O
for	O
most	O
of	O
the	O
algorithms	O
we	O
discuss	O
in	O
this	O
book	O
,	O
reinforcement	O
signals	O
include	O
terms	O
in	O
addition	O
to	O
the	O
reward	B
signal	I
,	O
an	O
example	O
being	O
a	O
td	O
error	O
δt	O
=	O
rt+1	O
+	O
γv	O
(	O
st+1	O
)	O
−	O
v	O
(	O
st	O
)	O
,	O
which	O
is	O
the	O
reinforcement	B
signal	I
for	O
td	O
state-value	O
learning	O
(	O
and	O
analogous	O
td	O
errors	O
for	O
action-value	O
learning	O
)	O
.	O
in	O
this	O
reinforcement	O
sig-	O
nal	O
,	O
rt+1	O
is	O
the	O
primary	O
reinforcement	O
contribution	O
,	O
and	O
the	O
temporal	O
diﬀerence	O
in	O
pre-	O
dicted	O
values	O
,	O
γv	O
(	O
st+1	O
)	O
−	O
v	O
(	O
st	O
)	O
(	O
or	O
an	O
analogous	O
temporal	O
diﬀerence	O
for	B
action	I
values	I
)	O
,	O
is	O
the	O
conditioned	O
reinforcement	O
contribution	O
.	O
thus	O
,	O
whenever	O
γv	O
(	O
st+1	O
)	O
−	O
v	O
(	O
st	O
)	O
=	O
0	O
,	O
δt	O
signals	O
‘	O
pure	O
’	O
primary	O
reinforcement	O
;	O
and	O
whenever	O
rt+1	O
=	O
0	O
,	O
it	O
signals	O
‘	O
pure	O
’	O
con-	O
ditioned	O
reinforcement	O
,	O
but	O
it	O
often	O
signals	O
a	O
mixture	O
of	O
these	O
.	O
note	O
as	O
we	O
mentioned	O
in	O
section	O
6.1	O
,	O
this	O
δt	O
is	O
not	O
available	O
until	O
time	O
t	O
+	O
1.	O
we	O
therefore	O
think	O
of	O
δt	O
as	O
the	O
reinforcement	B
signal	I
at	O
time	O
t+1	O
,	O
which	O
is	O
ﬁtting	O
because	O
it	O
reinforces	O
predictions	O
and/or	O
actions	O
made	O
earlier	O
at	O
step	O
t.	O
a	O
possible	O
source	O
of	O
confusion	O
is	O
the	O
terminology	O
used	O
by	O
the	O
famous	O
psychologist	O
b.	O
f.	O
skinner	O
and	O
his	O
followers	O
.	O
for	O
skinner	O
,	O
positive	O
reinforcement	O
occurs	O
when	O
the	O
con-	O
sequences	O
of	O
an	O
animal	O
’	O
s	O
behavior	O
increase	O
the	O
frequency	O
of	O
that	O
behavior	O
;	O
punishment	O
occurs	O
when	O
the	O
behavior	O
’	O
s	O
consequences	O
decrease	O
that	O
behavior	O
’	O
s	O
frequency	O
.	O
negative	O
reinforcement	O
occurs	O
when	O
behavior	O
leads	O
to	O
the	O
removal	O
of	O
an	O
aversive	O
stimulus	O
(	O
that	O
is	O
,	O
a	O
stimulus	O
the	O
animal	O
does	O
not	O
like	O
)	O
,	O
thereby	O
increasing	O
the	O
frequency	O
of	O
that	O
behavior	O
.	O
negative	O
punishment	O
,	O
on	O
the	O
other	O
hand	O
,	O
occurs	O
when	O
behavior	O
leads	O
to	O
the	O
removal	O
of	O
an	O
appetitive	O
stimulus	O
(	O
that	O
is	O
,	O
a	O
stimulus	O
the	O
animal	O
likes	O
)	O
,	O
thereby	O
decreasing	O
the	O
frequency	O
of	O
that	O
behavior	O
.	O
we	O
ﬁnd	O
no	O
critical	O
need	O
for	O
these	O
distinctions	O
because	O
our	O
approach	O
is	O
more	O
abstract	O
than	O
this	O
,	O
with	O
both	O
reward	O
and	O
reinforcement	O
signals	O
al-	O
lowed	O
to	O
take	O
on	O
both	O
positive	O
and	O
negative	O
values	O
.	O
(	O
but	O
note	O
especially	O
that	O
when	O
our	O
reinforcement	B
signal	I
is	O
negative	O
,	O
it	O
is	O
not	O
the	O
same	O
as	O
skinner	O
’	O
s	O
negative	O
reinforcement	O
.	O
)	O
380	O
chapter	O
14	O
:	O
psychology	B
on	O
the	O
other	O
hand	O
,	O
it	O
has	O
often	O
been	O
pointed	O
out	O
that	O
using	O
a	O
single	O
number	O
as	O
a	O
reward	O
or	O
a	O
penalty	O
signal	O
,	O
depending	O
only	O
on	O
its	O
sign	O
,	O
is	O
at	O
odds	O
with	O
the	O
fact	O
that	O
animals	O
’	O
appetitive	O
and	O
aversive	O
systems	O
have	O
qualitatively	O
diﬀerent	O
properties	O
and	O
in-	O
volve	O
diﬀerent	O
brain	O
mechanisms	O
.	O
this	O
points	O
to	O
a	O
direction	O
in	O
which	O
the	O
reinforcement	B
learning	I
framework	O
might	O
be	O
developed	O
in	O
the	O
future	O
to	O
exploit	O
computational	O
advan-	O
tages	O
of	O
separate	O
appetitive	O
and	O
aversive	O
systems	O
,	O
but	O
for	O
now	O
we	O
are	O
passing	O
over	O
these	O
possibilities	O
.	O
another	O
discrepancy	O
in	O
terminology	O
is	O
how	O
we	O
use	O
the	O
word	O
action	B
.	O
to	O
many	O
cognitive	O
scientists	O
,	O
an	O
action	B
is	O
purposeful	O
in	O
the	O
sense	O
of	O
being	O
the	O
result	O
of	O
an	O
animal	O
’	O
s	O
knowledge	O
about	O
the	O
relationship	O
between	O
the	O
behavior	O
in	O
question	O
and	O
the	O
consequences	O
of	O
that	O
behavior	O
.	O
an	O
action	B
is	O
goal-directed	O
and	O
the	O
result	O
of	O
a	O
decision	O
,	O
in	O
contrast	O
to	O
a	O
response	O
,	O
which	O
is	O
triggered	O
by	O
a	O
stimulus	O
;	O
the	O
result	O
of	O
a	O
reﬂex	O
or	O
a	O
habit	O
.	O
we	O
use	O
the	O
word	O
action	B
without	O
diﬀerentiating	O
among	O
what	O
others	O
call	O
actions	O
,	O
decisions	O
,	O
and	O
responses	O
.	O
these	O
are	O
important	O
distinctions	O
,	O
but	O
for	O
us	O
they	O
are	O
encompassed	O
by	O
diﬀerences	O
between	O
model-free	O
and	O
model-based	O
reinforcement	B
learning	I
algorithms	O
,	O
which	O
we	O
discussed	O
above	O
in	O
relation	O
to	O
habitual	O
and	O
goal-directed	O
behavior	O
in	O
section	O
14.6.	O
dickinson	O
(	O
1985	O
)	O
discusses	O
the	O
distinction	O
between	O
responses	O
and	O
actions	O
.	O
a	O
term	O
used	O
a	O
lot	O
in	O
this	O
book	O
is	O
control	B
.	O
what	O
we	O
mean	O
by	O
control	B
is	O
entirely	O
diﬀerent	O
from	O
what	O
it	O
means	O
to	O
animal	O
learning	O
psychologists	O
.	O
by	O
control	B
we	O
mean	O
that	O
an	O
agent	O
inﬂuences	O
its	O
environment	B
to	O
bring	O
about	O
states	O
or	O
events	O
that	O
the	O
agent	O
prefers	O
:	O
the	O
agent	O
exerts	O
control	B
over	O
its	O
environment	B
.	O
this	O
is	O
the	O
sense	O
of	O
control	O
used	O
by	O
control	B
engineers	O
.	O
in	B
psychology	I
,	O
on	O
the	O
other	O
hand	O
,	O
control	B
typically	O
means	O
that	O
an	O
animal	O
’	O
s	O
behavior	O
is	O
inﬂuenced	O
by—is	O
controlled	O
by—the	O
stimuli	O
the	O
animal	O
receives	O
(	O
stimulus	O
control	B
)	O
or	O
the	O
reinforcement	O
schedule	O
it	O
experiences	O
.	O
here	O
the	O
environment	B
is	O
controlling	O
the	O
agent	O
.	O
control	B
in	O
this	O
sense	O
is	O
the	O
basis	O
of	O
behavior	O
modiﬁcation	O
therapy	O
.	O
of	O
course	O
,	O
both	O
of	O
these	O
directions	O
of	O
control	O
are	O
at	O
play	O
when	O
an	O
agent	O
interacts	O
with	O
its	O
environment	B
,	O
but	O
our	O
focus	O
is	O
on	O
the	O
agent	O
as	O
controller	O
;	O
not	O
the	O
environment	B
as	O
controller	O
.	O
a	O
view	O
equivalent	O
to	O
ours	O
,	O
and	O
perhaps	O
more	O
illuminating	O
,	O
is	O
that	O
the	O
agent	O
is	O
actually	O
controlling	O
the	O
input	O
it	O
receives	O
from	O
its	O
environment	B
(	O
powers	O
,	O
1973	O
)	O
.	O
this	O
is	O
not	O
what	O
psychologists	O
mean	O
by	O
stimulus	O
control	B
.	O
sometimes	O
reinforcement	B
learning	I
is	O
understood	O
to	O
refer	O
solely	O
to	O
learning	O
policies	O
directly	O
from	O
rewards	O
(	O
and	O
penalties	O
)	O
without	O
the	O
involvement	O
of	O
value	O
functions	O
or	O
en-	O
vironment	O
models	O
.	O
this	O
is	O
what	O
psychologists	O
call	O
stimulus-response	O
,	O
or	O
s-r	O
,	O
learning	O
.	O
but	O
for	O
us	O
,	O
along	O
with	O
most	O
of	O
today	O
’	O
s	O
psychologists	O
,	O
reinforcement	B
learning	I
is	O
much	O
broader	O
than	O
this	O
,	O
including	O
in	O
addition	O
to	O
s-r	O
learning	O
,	O
methods	O
involving	O
value	B
func-	O
tions	O
,	O
environment	B
models	O
,	O
planning	B
,	O
and	O
other	O
processes	O
that	O
are	O
commonly	O
thought	O
to	O
belong	O
to	O
the	O
more	O
cognitive	O
side	O
of	O
mental	O
functioning	O
.	O
chapter	O
15	O
neuroscience	B
neuroscience	O
is	O
the	O
multidisciplinary	O
study	O
of	O
nervous	O
systems	O
:	O
how	O
they	O
regulate	O
bodily	O
functions	O
;	O
control	B
behavior	O
;	O
change	O
over	O
time	O
as	O
a	O
result	O
of	O
development	O
,	O
learning	O
,	O
and	O
aging	O
;	O
and	O
how	O
cellular	O
and	O
molecular	O
mechanisms	O
make	O
these	O
functions	O
possible	O
.	O
one	O
of	O
the	O
most	O
exciting	O
aspects	O
of	O
reinforcement	O
learning	O
is	O
the	O
mounting	O
evidence	O
from	O
neuroscience	B
that	O
the	O
nervous	O
systems	O
of	O
humans	O
and	O
many	O
other	O
animals	O
implement	O
algorithms	O
that	O
correspond	O
in	O
striking	O
ways	O
to	O
reinforcement	B
learning	I
algorithms	O
.	O
the	O
main	O
objective	O
of	O
this	O
chapter	O
is	O
to	O
explain	O
these	O
parallels	O
and	O
what	O
they	O
suggest	O
about	O
the	O
neural	B
basis	O
of	O
reward-related	O
learning	O
in	O
animals	O
.	O
the	O
most	O
remarkable	O
point	O
of	O
contact	O
between	O
reinforcement	B
learning	I
and	O
neuro-	O
science	O
involves	O
dopamine	B
,	O
a	O
chemical	O
deeply	O
involved	O
in	O
reward	O
processing	O
in	O
the	O
brains	O
of	O
mammals	O
.	O
dopamine	B
appears	O
to	O
convey	O
temporal-diﬀerence	O
(	O
td	O
)	O
errors	O
to	O
brain	O
structures	O
where	O
learning	O
and	O
decision	O
making	O
take	O
place	O
.	O
this	O
parallel	O
is	O
expressed	O
by	O
the	O
reward	B
prediction	I
error	I
hypothesis	I
of	O
dopamine	B
neuron	O
activity	O
,	O
a	O
hypothesis	O
that	O
resulted	O
from	O
the	O
convergence	O
of	O
computational	O
reinforcement	B
learning	I
and	O
results	O
of	O
neuroscience	O
experiments	O
.	O
in	O
this	O
chapter	O
we	O
discuss	O
this	O
hypothesis	O
,	O
the	O
neuroscience	B
ﬁndings	O
that	O
led	O
to	O
it	O
,	O
and	O
why	O
it	O
is	O
a	O
signiﬁcant	O
contribution	O
to	O
understanding	O
brain	O
re-	O
ward	O
systems	O
.	O
we	O
also	O
discuss	O
parallels	O
between	O
reinforcement	B
learning	I
and	O
neuroscience	B
that	O
are	O
less	O
striking	O
than	O
this	O
dopamine/td-error	O
parallel	O
but	O
that	O
provide	O
useful	O
con-	O
ceptual	O
tools	O
for	O
thinking	O
about	O
reward-based	O
learning	O
in	O
animals	O
.	O
other	O
elements	O
of	O
reinforcement	O
learning	O
have	O
the	O
potential	O
to	O
impact	O
the	O
study	O
of	O
nervous	O
systems	O
,	O
but	O
their	O
connections	O
to	O
neuroscience	B
are	O
still	O
relatively	O
undeveloped	O
.	O
we	O
discuss	O
several	O
of	O
these	O
evolving	O
connections	O
that	O
we	O
think	O
will	O
grow	O
in	O
importance	O
over	O
time	O
.	O
as	O
we	O
outlined	O
in	O
the	O
history	O
section	O
of	O
this	O
book	O
’	O
s	O
introductory	O
chapter	O
(	O
section	O
1.7	O
)	O
,	O
many	O
aspects	O
of	O
reinforcement	O
learning	O
were	O
inﬂuenced	O
by	O
neuroscience	B
.	O
a	O
second	O
ob-	O
jective	O
of	O
this	O
chapter	O
is	O
to	O
acquaint	O
readers	O
with	O
ideas	O
about	O
brain	O
function	O
that	O
have	O
contributed	O
to	O
our	O
approach	O
to	O
reinforcement	B
learning	I
.	O
some	O
elements	O
of	O
reinforcement	O
learning	O
are	O
easier	O
to	O
understand	O
when	O
seen	O
in	O
light	O
of	O
theories	O
of	O
brain	O
function	O
.	O
this	O
is	O
particularly	O
true	O
for	O
the	O
idea	O
of	O
the	O
eligibility	O
trace	O
,	O
one	O
of	O
the	O
basic	O
mechanisms	O
of	O
rein-	O
forcement	O
learning	O
,	O
that	O
originated	O
as	O
a	O
conjectured	O
property	O
of	O
synapses	O
,	O
the	O
structures	O
by	O
which	O
nerve	O
cells—neurons—communicate	O
with	O
one	O
another	O
.	O
381	O
382	O
chapter	O
15	O
:	O
neuroscience	B
in	O
this	O
chapter	O
we	O
do	O
not	O
delve	O
very	O
deeply	O
into	O
the	O
enormous	O
complexity	O
of	O
the	O
neural	B
systems	O
underlying	O
reward-based	O
learning	O
in	O
animals	O
:	O
this	O
chapter	O
is	O
too	O
short	O
,	O
and	O
we	O
are	O
not	O
neuroscientists	O
.	O
we	O
do	O
not	O
try	O
to	O
describe—or	O
even	O
to	O
name—the	O
very	O
many	O
brain	O
structures	O
and	O
pathways	O
,	O
or	O
any	O
of	O
the	O
molecular	O
mechanisms	O
,	O
believed	O
to	O
be	O
involved	O
in	O
these	O
processes	O
.	O
we	O
also	O
do	O
not	O
do	O
justice	O
to	O
hypotheses	O
and	O
models	O
that	O
are	O
alternatives	O
to	O
those	O
that	O
align	O
so	O
well	O
with	O
reinforcement	O
learning	O
.	O
it	O
should	O
not	O
be	O
surprising	O
that	O
there	O
are	O
diﬀering	O
views	O
among	O
experts	O
in	O
the	O
ﬁeld	O
.	O
we	O
can	O
only	O
provide	O
a	O
glimpse	O
into	O
this	O
fascinating	O
and	O
developing	O
story	O
.	O
we	O
hope	O
,	O
though	O
,	O
that	O
this	O
chapter	O
convinces	O
you	O
that	O
a	O
very	O
fruitful	O
channel	O
has	O
emerged	O
connecting	O
reinforcement	B
learning	I
and	O
its	O
theoretical	O
underpinnings	O
to	O
the	O
neuroscience	B
of	O
reward-based	O
learning	O
in	O
animals	O
.	O
many	O
excellent	O
publications	O
cover	O
links	O
between	O
reinforcement	B
learning	I
and	O
neuro-	O
science	O
,	O
some	O
of	O
which	O
we	O
cite	O
in	O
this	O
chapter	O
’	O
s	O
ﬁnal	O
section	O
.	O
our	O
treatment	O
diﬀers	O
from	O
most	O
of	O
these	O
because	O
we	O
assume	O
familiarity	O
with	O
reinforcement	O
learning	O
as	O
presented	O
in	O
the	O
earlier	O
chapters	O
of	O
this	O
book	O
,	O
but	O
we	O
do	O
not	O
assume	O
knowledge	O
of	O
neuroscience	O
.	O
we	O
begin	O
with	O
a	O
brief	O
introduction	O
to	O
the	O
neuroscience	B
concepts	O
needed	O
for	O
a	O
basic	O
under-	O
standing	O
of	O
what	O
is	O
to	O
follow	O
.	O
15.1	O
neuroscience	B
basics	O
some	O
basic	O
information	O
about	O
nervous	O
systems	O
is	O
helpful	O
for	O
following	O
what	O
we	O
cover	O
in	O
this	O
chapter	O
.	O
terms	O
that	O
we	O
refer	O
to	O
later	O
are	O
italicized	O
.	O
skipping	O
this	O
section	O
will	O
not	O
be	O
a	O
problem	O
if	O
you	O
already	O
have	O
an	O
elementary	O
knowledge	O
of	O
neuroscience	O
.	O
neurons	O
,	O
the	O
main	O
components	O
of	O
nervous	O
systems	O
,	O
are	O
cells	O
specialized	O
for	O
processing	O
and	O
transmitting	O
information	O
using	O
electrical	O
and	O
chemical	O
signals	O
.	O
they	O
come	O
in	O
many	O
forms	O
,	O
but	O
a	O
neuron	O
typically	O
has	O
a	O
cell	O
body	O
,	O
dendrites	O
,	O
and	O
a	O
single	O
axon	O
.	O
dendrites	O
are	O
structures	O
that	O
branch	O
from	O
the	O
cell	O
body	O
to	O
receive	O
input	O
from	O
other	O
neurons	O
(	O
or	O
to	O
also	O
receive	O
external	O
signals	O
in	O
the	O
case	O
of	O
sensory	O
neurons	O
)	O
.	O
a	O
neuron	O
’	O
s	O
axon	O
is	O
a	O
ﬁber	O
that	O
carries	O
the	O
neuron	O
’	O
s	O
output	O
to	O
other	O
neurons	O
(	O
or	O
to	O
muscles	O
or	O
glands	O
)	O
.	O
a	O
neuron	O
’	O
s	O
output	O
consists	O
of	O
sequences	O
of	O
electrical	O
pulses	O
called	O
action	B
potentials	O
that	O
travel	O
along	O
the	O
axon	O
.	O
action	B
potentials	O
are	O
also	O
called	O
spikes	O
,	O
and	O
a	O
neuron	O
is	O
said	O
to	O
ﬁre	O
when	O
it	O
generates	O
a	O
spike	O
.	O
in	O
models	O
of	O
neural	O
networks	O
it	O
is	O
common	O
to	O
use	O
real	O
numbers	O
to	O
represent	O
a	O
neuron	O
’	O
s	O
ﬁring	O
rate	O
,	O
the	O
average	O
number	O
of	O
spikes	O
per	O
some	O
unit	O
of	O
time	O
.	O
a	O
neuron	O
’	O
s	O
axon	O
can	O
branch	O
widely	O
so	O
that	O
the	O
neuron	O
’	O
s	O
action	B
potentials	O
reach	O
many	O
targets	O
.	O
the	O
branching	O
structure	O
of	O
a	O
neuron	O
’	O
s	O
axon	O
is	O
called	O
the	O
neuron	O
’	O
s	O
axonal	O
ar-	O
bor	O
.	O
because	O
the	O
conduction	O
of	O
an	O
action	B
potential	O
is	O
an	O
active	O
process	O
,	O
not	O
unlike	O
the	O
burning	O
of	O
a	O
fuse	O
,	O
when	O
an	O
action	B
potential	O
reaches	O
an	O
axonal	O
branch	O
point	O
it	O
“	O
lights	O
up	O
”	O
action	B
potentials	O
on	O
all	O
of	O
the	O
outgoing	O
branches	O
(	O
although	O
propagation	O
to	O
a	O
branch	O
can	O
sometimes	O
fail	O
)	O
.	O
as	O
a	O
result	O
,	O
the	O
activity	O
of	O
a	O
neuron	O
with	O
a	O
large	O
axonal	O
arbor	O
can	O
inﬂuence	O
many	O
target	B
sites	O
.	O
a	O
synapse	O
is	O
a	O
structure	O
generally	O
at	O
the	O
termination	O
of	O
an	O
axon	O
branch	O
that	O
mediates	O
the	O
communication	O
of	O
one	O
neuron	O
to	O
another	O
.	O
a	O
synapse	O
transmits	O
information	O
from	O
the	O
presynaptic	O
neuron	O
’	O
s	O
axon	O
to	O
a	O
dendrite	O
or	O
cell	O
body	O
of	O
the	O
postsynaptic	O
neuron	O
.	O
with	O
a	O
few	O
exceptions	O
,	O
synapses	O
release	O
a	O
chemical	O
neurotransmitter	O
upon	O
the	O
arrival	O
15.2.	O
reward	O
signals	O
,	O
reinforcement	O
signals	O
,	O
values	O
,	O
and	O
prediction	O
errors	O
383	O
of	O
an	O
action	B
potential	O
from	O
the	O
presynaptic	O
neuron	O
.	O
(	O
the	O
exceptions	O
are	O
cases	O
of	O
direct	O
electric	O
coupling	O
between	O
neurons	O
,	O
but	O
these	O
will	O
not	O
concern	O
us	O
here	O
.	O
)	O
neurotransmitter	O
molecules	O
released	O
from	O
the	O
presynaptic	O
side	O
of	O
the	O
synapse	O
diﬀuse	O
across	O
the	O
synaptic	O
cleft	O
,	O
the	O
very	O
small	O
space	O
between	O
the	O
presynaptic	O
ending	O
and	O
the	O
postsynaptic	O
neuron	O
,	O
and	O
then	O
bind	O
to	O
receptors	O
on	O
the	O
surface	O
of	O
the	O
postsynaptic	O
neuron	O
to	O
excite	O
or	O
inhibit	O
its	O
spike-generating	O
activity	O
,	O
or	O
to	O
modulate	O
its	O
behavior	O
in	O
other	O
ways	O
.	O
a	O
particular	O
neurotransmitter	O
may	O
bind	O
to	O
several	O
diﬀerent	O
types	O
of	O
receptors	O
,	O
with	O
each	O
producing	O
a	O
diﬀerent	O
eﬀect	O
on	O
the	O
postsynaptic	O
neuron	O
.	O
for	O
example	O
,	O
there	O
are	O
at	O
least	O
ﬁve	O
diﬀerent	O
receptor	O
types	O
by	O
which	O
the	O
neurotransmitter	O
dopamine	B
can	O
aﬀect	O
a	O
postsynaptic	O
neuron	O
.	O
many	O
diﬀerent	O
chemicals	O
have	O
been	O
identiﬁed	O
as	O
neurotransmitters	O
in	O
animal	O
nervous	O
systems	O
.	O
a	O
neuron	O
’	O
s	O
background	O
activity	O
is	O
its	O
level	O
of	O
activity	O
,	O
usually	O
its	O
ﬁring	O
rate	O
,	O
when	O
the	O
neuron	O
does	O
not	O
appear	O
to	O
be	O
driven	O
by	O
synaptic	O
input	O
related	O
to	O
the	O
task	O
of	O
interest	O
to	O
the	O
experimenter	O
,	O
for	O
example	O
,	O
when	O
the	O
neuron	O
’	O
s	O
activity	O
is	O
not	O
correlated	O
with	O
a	O
stimulus	O
delivered	O
to	O
a	O
subject	O
as	O
part	O
of	O
an	O
experiment	O
.	O
background	O
activity	O
can	O
be	O
irregular	O
due	O
to	O
input	O
from	O
the	O
wider	O
network	O
,	O
or	O
due	O
to	O
noise	O
within	O
the	O
neuron	O
or	O
its	O
synapses	O
.	O
sometimes	O
background	O
activity	O
is	O
the	O
result	O
of	O
dynamic	O
processes	O
intrinsic	B
to	O
the	O
neuron	O
.	O
a	O
neuron	O
’	O
s	O
phasic	O
activity	O
,	O
in	O
contrast	O
to	O
its	O
background	O
activity	O
,	O
consists	O
of	O
bursts	O
of	O
spiking	O
activity	O
usually	O
caused	O
by	O
synaptic	O
input	O
.	O
activity	O
that	O
varies	O
slowly	O
and	O
often	O
in	O
a	O
graded	O
manner	O
,	O
whether	O
as	O
background	O
activity	O
or	O
not	O
,	O
is	O
called	O
a	O
neuron	O
’	O
s	O
tonic	O
activity	O
.	O
the	O
strength	O
or	O
eﬀectiveness	O
by	O
which	O
the	O
neurotransmitter	O
released	O
at	O
a	O
synapse	O
in-	O
ﬂuences	O
the	O
postsynaptic	O
neuron	O
is	O
the	O
synapse	O
’	O
s	O
eﬃcacy	O
.	O
one	O
way	O
a	O
nervous	O
system	O
can	O
change	O
through	O
experience	O
is	O
through	O
changes	O
in	O
synaptic	O
eﬃcacies	O
as	O
a	O
result	O
of	O
combinations	O
of	O
the	O
activities	O
of	O
the	O
presynaptic	O
and	O
postsynaptic	O
neurons	O
,	O
and	O
some-	O
times	O
by	O
the	O
presence	O
of	O
a	O
neuromodulator	O
,	O
which	O
is	O
a	O
neurotransmitter	O
having	O
eﬀects	O
other	O
than	O
,	O
or	O
in	O
addition	O
to	O
,	O
direct	O
fast	O
excitation	O
or	O
inhibition	O
.	O
brains	O
contain	O
several	O
diﬀerent	O
neuromodulation	O
systems	O
consisting	O
of	O
clusters	O
of	O
neu-	O
rons	O
with	O
widely	O
branching	O
axonal	O
arbors	O
,	O
with	O
each	O
system	O
using	O
a	O
diﬀerent	O
neurotrans-	O
mitter	O
.	O
neuromodulation	O
can	O
alter	O
the	O
function	O
of	O
neural	B
circuits	O
,	O
mediate	O
motivation	B
,	O
arousal	O
,	O
attention	O
,	O
memory	O
,	O
mood	O
,	O
emotion	O
,	O
sleep	O
,	O
and	O
body	O
temperature	O
.	O
important	O
here	O
is	O
that	O
a	O
neuromodulatory	O
system	O
can	O
distribute	O
something	O
like	O
a	O
scalar	O
signal	O
,	O
such	O
as	O
a	O
reinforcement	B
signal	I
,	O
to	O
alter	O
the	O
operation	O
of	O
synapses	O
in	O
widely	O
distributed	O
sites	O
critical	O
for	O
learning	O
.	O
the	O
ability	O
of	O
synaptic	O
eﬃcacies	O
to	O
change	O
is	O
called	O
synaptic	B
plasticity	I
.	O
it	O
is	O
one	O
of	O
the	O
primary	O
mechanisms	O
responsible	O
for	O
learning	O
.	O
the	O
parameters	O
,	O
or	O
weights	O
,	O
adjusted	O
by	O
learning	O
algorithms	O
correspond	O
to	O
synaptic	O
eﬃcacies	O
.	O
as	O
we	O
detail	O
below	O
,	O
modulation	O
of	O
synaptic	O
plasticity	O
via	O
the	O
neuromodulator	O
dopamine	B
is	O
a	O
plausible	O
mechanism	O
for	O
how	O
the	O
brain	O
might	O
implement	O
learning	O
algorithms	O
like	O
many	O
of	O
those	O
described	O
in	O
this	O
book	O
.	O
384	O
chapter	O
15	O
:	O
neuroscience	B
15.2	O
reward	O
signals	O
,	O
reinforcement	O
signals	O
,	O
values	O
,	O
and	O
prediction	O
errors	O
links	O
between	O
neuroscience	B
and	O
computational	O
reinforcement	O
learning	O
begin	O
as	O
parallels	O
between	O
signals	O
in	O
the	O
brain	O
and	O
signals	O
playing	O
prominent	O
roles	O
in	O
reinforcement	O
learning	O
theory	O
and	O
algorithms	O
.	O
in	O
chapter	O
3	O
we	O
said	O
that	O
any	O
problem	O
of	O
learning	O
goal-directed	O
behavior	O
can	O
be	O
reduced	O
to	O
the	O
three	O
signals	O
representing	O
actions	O
,	O
states	O
,	O
and	O
rewards	O
.	O
however	O
,	O
to	O
explain	O
links	O
that	O
have	O
been	O
made	O
between	O
neuroscience	B
and	O
reinforcement	B
learning	I
,	O
we	O
have	O
to	O
be	O
less	O
abstract	O
than	O
this	O
and	O
consider	O
other	O
reinforcement	B
learning	I
signals	O
that	O
correspond	O
,	O
in	O
certain	O
ways	O
,	O
to	O
signals	O
in	O
the	O
brain	O
.	O
in	O
addition	O
to	O
reward	O
signals	O
,	O
these	O
include	O
reinforcement	O
signals	O
(	O
which	O
we	O
argue	O
are	O
diﬀerent	O
from	O
reward	O
signals	O
)	O
,	O
value	B
signals	O
,	O
and	O
signals	O
conveying	O
prediction	B
errors	O
.	O
when	O
we	O
label	O
a	O
signal	O
by	O
its	O
function	O
in	O
this	O
way	O
,	O
we	O
are	O
doing	O
it	O
in	O
the	O
context	O
of	O
reinforcement	O
learning	O
theory	O
in	O
which	O
the	O
signal	O
corresponds	O
to	O
a	O
term	O
in	O
an	O
equation	O
or	O
an	O
algorithm	O
.	O
on	O
the	O
other	O
hand	O
,	O
when	O
we	O
refer	O
to	O
a	O
signal	O
in	O
the	O
brain	O
,	O
we	O
mean	O
a	O
physiological	O
event	O
such	O
as	O
a	O
burst	O
of	O
action	O
potentials	O
or	O
the	O
secretion	O
of	O
a	O
neurotransmitter	O
.	O
labeling	O
a	O
neural	B
signal	O
by	O
its	O
function	O
,	O
for	O
example	O
calling	O
the	O
phasic	O
activity	O
of	O
a	O
dopamine	B
neuron	O
a	O
reinforcement	B
signal	I
,	O
means	O
that	O
the	O
neural	B
signal	O
behaves	O
like	O
,	O
and	O
is	O
conjectured	O
to	O
function	O
like	O
,	O
the	O
corresponding	O
theoretical	O
signal	O
.	O
uncovering	O
evidence	O
for	O
these	O
correspondences	O
involves	O
many	O
challenges	O
.	O
neural	B
ac-	O
tivity	O
related	O
to	O
reward	O
processing	O
can	O
be	O
found	O
in	O
nearly	O
every	O
part	O
of	O
the	O
brain	O
,	O
and	O
it	O
is	O
diﬃcult	O
to	O
interpret	O
results	O
unambiguously	O
because	O
representations	O
of	O
diﬀerent	O
reward-	O
related	O
signals	O
tend	O
to	O
be	O
highly	O
correlated	O
with	O
one	O
another	O
.	O
experiments	O
need	O
to	O
be	O
carefully	O
designed	O
to	O
allow	O
one	O
type	O
of	O
reward-related	O
signal	O
to	O
be	O
distinguished	O
with	O
any	O
degree	O
of	O
certainty	O
from	O
others—or	O
from	O
an	O
abundance	O
of	O
other	O
signals	O
not	O
related	O
to	O
reward	O
processing	O
.	O
despite	O
these	O
diﬃculties	O
,	O
many	O
experiments	O
have	O
been	O
conducted	O
with	O
the	O
aim	O
of	O
reconciling	O
aspects	O
of	O
reinforcement	O
learning	O
theory	O
and	O
algorithms	O
with	O
neural	O
signals	O
,	O
and	O
some	O
compelling	O
links	O
have	O
been	O
established	O
.	O
to	O
prepare	O
for	O
ex-	O
amining	O
these	O
links	O
,	O
in	O
the	O
rest	O
of	O
this	O
section	O
we	O
remind	O
the	O
reader	O
of	O
what	O
various	O
reward-related	O
signals	O
mean	O
according	O
to	O
reinforcement	B
learning	I
theory	O
.	O
in	O
our	O
comments	O
on	O
terminology	O
at	O
the	O
end	O
of	O
the	O
previous	O
chapter	O
,	O
we	O
said	O
that	O
rt	O
is	O
like	O
a	O
reward	B
signal	I
in	O
an	O
animal	O
’	O
s	O
brain	O
and	O
not	O
an	O
object	O
or	O
event	O
in	O
the	O
ani-	O
mal	O
’	O
s	O
environment	B
.	O
in	O
reinforcement	O
learning	O
,	O
the	O
reward	B
signal	I
(	O
along	O
with	O
an	O
agent	O
’	O
s	O
environment	B
)	O
deﬁnes	O
the	O
problem	O
a	O
reinforcement	B
learning	I
agent	O
is	O
trying	O
to	O
solve	O
.	O
in	O
this	O
respect	O
,	O
rt	O
is	O
like	O
a	O
signal	O
in	O
an	O
animal	O
’	O
s	O
brain	O
that	O
distributes	O
primary	O
reward	O
to	O
sites	O
throughout	O
the	O
brain	O
.	O
but	O
it	O
is	O
unlikely	O
that	O
a	O
unitary	O
master	O
reward	B
signal	I
like	O
rt	O
exists	O
in	O
an	O
animal	O
’	O
s	O
brain	O
.	O
it	O
is	O
best	O
to	O
think	O
of	O
rt	O
as	O
an	O
abstraction	O
summarizing	O
the	O
overall	O
eﬀect	O
of	O
a	O
multitude	O
of	O
neural	O
signals	O
generated	O
by	O
many	O
systems	O
in	O
the	O
brain	O
that	O
assess	O
the	O
rewarding	O
or	O
punishing	O
qualities	O
of	O
sensations	O
and	O
states	O
.	O
reinforcement	O
signals	O
in	O
reinforcement	O
learning	O
are	O
diﬀerent	O
from	O
reward	O
signals	O
.	O
the	O
function	O
of	O
a	O
reinforcement	B
signal	I
is	O
to	O
direct	O
the	O
changes	O
a	O
learning	O
algorithm	O
makes	O
in	O
an	O
agent	O
’	O
s	O
policy	B
,	O
value	B
estimates	O
,	O
or	O
environment	B
models	O
.	O
for	O
a	O
td	O
method	O
,	O
for	O
instance	O
,	O
the	O
reinforcement	B
signal	I
at	O
time	O
t	O
is	O
the	O
td	O
error	O
δt−1	O
=	O
rt	O
+	O
γv	O
(	O
st	O
)	O
−	O
v	O
(	O
st−1	O
)	O
.1	O
the	O
1	O
as	O
we	O
mentioned	O
in	O
section	O
6.1	O
,	O
δt	O
in	O
our	O
notation	O
is	O
deﬁned	O
to	O
be	O
rt+1	O
+	O
γv	O
(	O
st+1	O
)	O
−	O
v	O
(	O
st	O
)	O
,	O
so	O
δt	O
15.3.	O
the	O
reward	B
prediction	I
error	I
hypothesis	I
385	O
reinforcement	B
signal	I
for	O
some	O
algorithms	O
could	O
be	O
just	O
the	O
reward	B
signal	I
,	O
but	O
for	O
most	O
of	O
the	O
algorithms	O
we	O
consider	O
the	O
reinforcement	B
signal	I
is	O
the	O
reward	B
signal	I
adjusted	O
by	O
other	O
information	O
,	O
such	O
as	O
the	O
value	B
estimates	O
in	O
td	O
errors	O
.	O
estimates	O
of	O
state	O
values	O
or	O
of	O
action	O
values	O
,	O
that	O
is	O
,	O
v	O
or	O
q	O
,	O
specify	O
what	O
is	O
good	O
or	O
bad	O
for	O
the	O
agent	O
over	O
the	O
long	O
run	O
.	O
they	O
are	O
predictions	O
of	O
the	O
total	O
reward	O
an	O
agent	O
can	O
expect	O
to	O
accumulate	O
over	O
the	O
future	O
.	O
agents	O
make	O
good	O
decisions	O
by	O
selecting	O
actions	O
leading	O
to	O
states	O
with	O
the	O
largest	O
estimated	O
state	B
values	O
,	O
or	O
by	O
selecting	O
actions	O
with	O
the	O
largest	O
estimated	O
action	B
values	O
.	O
prediction	B
errors	O
measure	O
discrepancies	O
between	O
expected	O
and	O
actual	O
signals	O
or	O
sen-	O
sations	O
.	O
reward	O
prediction	O
errors	O
(	O
rpes	O
)	O
speciﬁcally	O
measure	O
discrepancies	O
between	O
the	O
expected	O
and	O
the	O
received	O
reward	B
signal	I
,	O
being	O
positive	O
when	O
the	O
reward	B
signal	I
is	O
greater	O
than	O
expected	O
,	O
and	O
negative	O
otherwise	O
.	O
td	O
errors	O
like	O
(	O
6.5	O
)	O
are	O
special	O
kinds	O
of	O
rpes	O
that	O
signal	O
discrepancies	O
between	O
current	O
and	O
earlier	O
expectations	O
of	O
reward	O
over	O
the	O
long-term	O
.	O
when	O
neuroscientists	O
refer	O
to	O
rpes	O
they	O
generally	O
(	O
though	O
not	O
always	O
)	O
mean	O
td	O
rpes	O
,	O
which	O
we	O
simply	O
call	O
td	O
errors	O
throughout	O
this	O
chapter	O
.	O
also	O
in	O
this	O
chapter	O
,	O
a	O
td	O
error	O
is	O
generally	O
one	O
that	O
does	O
not	O
depend	O
on	O
actions	O
,	O
as	O
opposed	O
to	O
td	O
errors	O
used	O
in	O
learning	O
action-values	O
by	O
algorithms	O
like	O
sarsa	O
and	O
q-learning	O
.	O
this	O
is	O
because	O
the	O
most	O
well-known	O
links	O
to	O
neuroscience	B
are	O
stated	O
in	O
terms	O
of	O
action-free	O
td	O
errors	O
,	O
but	O
we	O
do	O
not	O
mean	O
to	O
rule	O
out	O
possible	O
similar	O
links	O
involving	O
action-dependent	O
td	O
errors	O
.	O
(	O
td	O
errors	O
for	O
predicting	O
signals	O
other	O
than	O
rewards	O
are	O
useful	O
too	O
,	O
but	O
that	O
case	O
will	O
not	O
concern	O
us	O
here	O
.	O
see	O
,	O
for	O
example	O
,	O
modayil	O
,	O
white	O
,	O
and	O
sutton	O
,	O
2014	O
.	O
)	O
one	O
can	O
ask	O
many	O
questions	O
about	O
links	O
between	O
neuroscience	B
data	O
and	O
these	O
theoretically-	O
deﬁned	O
signals	O
.	O
is	O
an	O
observed	O
signal	O
more	O
like	O
a	O
reward	B
signal	I
,	O
a	O
value	B
signal	O
,	O
a	O
pre-	O
diction	O
error	O
,	O
a	O
reinforcement	B
signal	I
,	O
or	O
something	O
altogether	O
diﬀerent	O
?	O
and	O
if	O
it	O
is	O
an	O
error	O
signal	O
,	O
is	O
it	O
an	O
rpe	O
,	O
a	O
td	O
error	O
,	O
or	O
a	O
simpler	O
error	O
like	O
the	O
rescorla–wagner	O
error	O
(	O
14.3	O
)	O
?	O
and	O
if	O
it	O
is	O
a	O
td	O
error	O
,	O
does	O
it	O
depend	O
on	O
actions	O
like	O
the	O
td	O
error	O
of	O
q-	O
learning	O
or	O
sarsa	O
?	O
as	O
indicated	O
above	O
,	O
probing	O
the	O
brain	O
to	O
answer	O
questions	O
like	O
these	O
is	O
extremely	O
diﬃcult	O
.	O
but	O
experimental	O
evidence	O
suggests	O
that	O
one	O
neurotransmitter	O
,	O
speciﬁcally	O
the	O
neurotransmitter	O
dopamine	B
,	O
signals	O
rpes	O
,	O
and	O
further	O
,	O
that	O
the	O
phasic	O
activity	O
of	O
dopamine-producing	O
neurons	O
in	O
fact	O
conveys	O
td	O
errors	O
(	O
see	O
section	O
15.1	O
for	O
a	O
deﬁnition	O
of	O
phasic	O
activity	O
)	O
.	O
this	O
evidence	O
led	O
to	O
the	O
reward	B
prediction	I
error	I
hypothesis	I
of	O
dopamine	B
neuron	O
activity	O
,	O
which	O
we	O
describe	O
next	O
.	O
15.3	O
the	O
reward	B
prediction	I
error	I
hypothesis	I
the	O
reward	B
prediction	I
error	I
hypothesis	I
of	O
dopamine	B
neuron	O
activity	O
proposes	O
that	O
one	O
of	O
the	O
functions	O
of	O
the	O
phasic	O
activity	O
of	O
dopamine-producing	O
neurons	O
in	O
mammals	O
is	O
to	O
deliver	O
an	O
error	O
between	O
an	O
old	O
and	O
a	O
new	O
estimate	O
of	O
expected	O
future	O
reward	O
to	O
tar-	O
get	O
areas	O
throughout	O
the	O
brain	O
.	O
this	O
hypothesis	O
(	O
though	O
not	O
in	O
these	O
exact	O
words	O
)	O
was	O
ﬁrst	O
explicitly	O
stated	O
by	O
montague	O
,	O
dayan	O
,	O
and	O
sejnowski	O
(	O
1996	O
)	O
,	O
who	O
showed	O
how	O
the	O
td	O
error	O
concept	O
from	O
reinforcement	B
learning	I
accounts	O
for	O
many	O
features	O
of	O
the	O
phasic	O
is	O
not	O
available	O
until	O
time	O
t	O
+	O
1.	O
the	O
td	O
error	O
available	O
at	O
t	O
is	O
actually	O
δt−1	O
=	O
rt	O
+	O
γv	O
(	O
st	O
)	O
−	O
v	O
(	O
st−1	O
)	O
.	O
because	O
we	O
are	O
thinking	O
of	O
time	O
steps	O
as	O
very	O
small	O
,	O
or	O
even	O
inﬁnitesimal	O
,	O
time	O
intervals	O
,	O
one	O
should	O
not	O
attribute	O
undue	O
importance	O
to	O
this	O
one-step	O
time	O
shift	O
.	O
386	O
chapter	O
15	O
:	O
neuroscience	B
activity	O
of	O
dopamine	O
neurons	O
in	O
mammals	O
.	O
the	O
experiments	O
that	O
led	O
to	O
this	O
hypothesis	O
were	O
performed	O
in	O
the	O
1980s	O
and	O
early	O
1990s	O
in	O
the	O
laboratory	O
of	O
neuroscientist	O
wol-	O
fram	O
schultz	O
.	O
section	O
15.5	O
describes	O
these	O
inﬂuential	O
experiments	O
,	O
section	O
15.6	O
explains	O
how	O
the	O
results	O
of	O
these	O
experiments	O
align	O
with	O
td	O
errors	O
,	O
and	O
the	O
bibliographical	O
and	O
historical	O
remarks	O
section	O
at	O
the	O
end	O
of	O
this	O
chapter	O
includes	O
a	O
guide	O
to	O
the	O
literature	O
surrounding	O
the	O
development	O
of	O
this	O
inﬂuential	O
hypothesis	O
.	O
montague	O
et	O
al	O
.	O
(	O
1996	O
)	O
compared	O
the	O
td	O
errors	O
of	O
the	O
td	O
model	O
of	O
classical	O
condition-	O
ing	B
with	O
the	O
phasic	O
activity	O
of	O
dopamine-producing	O
neurons	O
during	O
classical	B
conditioning	I
experiments	O
.	O
recall	O
from	O
section	O
14.2	O
that	O
the	O
td	O
model	O
of	O
classical	O
conditioning	B
is	O
ba-	O
sically	O
the	O
semi-gradient-descent	O
td	O
(	O
λ	O
)	O
algorithm	O
with	O
linear	O
function	B
approximation	I
.	O
montague	O
et	O
al	O
.	O
made	O
several	O
assumptions	O
to	O
set	O
up	O
this	O
comparison	O
.	O
first	O
,	O
because	O
a	O
td	O
error	O
can	O
be	O
negative	O
but	O
neurons	O
can	O
not	O
have	O
a	O
negative	O
ﬁring	O
rate	O
,	O
they	O
assumed	O
that	O
the	O
quantity	O
corresponding	O
to	O
dopamine	B
neuron	O
activity	O
is	O
δt−1	O
+	O
bt	O
,	O
where	O
bt	O
is	O
the	O
background	O
ﬁring	O
rate	O
of	O
the	O
neuron	O
.	O
a	O
negative	O
td	O
error	O
corresponds	O
to	O
a	O
drop	O
in	O
a	O
dopamine	B
neuron	O
’	O
s	O
ﬁring	O
rate	O
below	O
its	O
background	O
rate.2	O
a	O
second	O
assumption	O
was	O
needed	O
about	O
the	O
states	O
visited	O
in	O
each	O
classical	B
conditioning	I
trial	O
and	O
how	O
they	O
are	O
represented	O
as	O
inputs	O
to	O
the	O
learning	O
algorithm	O
.	O
this	O
is	O
the	O
same	O
issue	O
we	O
discussed	O
in	O
section	O
14.2.4	O
for	O
the	O
td	O
model	O
.	O
montague	O
et	O
al	O
.	O
chose	O
a	O
complete	O
serial	O
compound	B
(	O
csc	O
)	O
representation	O
as	O
shown	O
in	O
the	O
left	O
column	O
of	O
figure	O
14.1	O
,	O
but	O
where	O
the	O
sequence	O
of	O
short-duration	O
internal	O
signals	O
continues	O
until	O
the	O
onset	O
of	O
the	O
us	O
,	O
which	O
here	O
is	O
the	O
arrival	O
of	O
a	O
non-zero	O
reward	B
signal	I
.	O
this	O
representation	O
allows	O
the	O
td	O
error	O
to	O
mimic	O
the	O
fact	O
that	O
dopamine	B
neuron	O
activity	O
not	O
only	O
predicts	O
a	O
future	O
reward	O
,	O
but	O
that	O
it	O
is	O
also	O
sensitive	O
to	O
when	O
after	O
a	O
predictive	O
cue	O
that	O
reward	O
is	O
expected	O
to	O
arrive	O
.	O
there	O
has	O
to	O
be	O
some	O
way	O
to	O
keep	O
track	O
of	O
the	O
time	O
between	O
sensory	O
cues	O
and	O
the	O
arrival	O
of	O
reward	O
.	O
if	O
a	O
stimulus	O
initiates	O
a	O
sequence	O
of	O
internal	O
signals	O
that	O
continues	O
after	O
the	O
stimulus	O
ends	O
,	O
and	O
if	O
there	O
is	O
a	O
diﬀerent	O
signal	O
for	O
each	O
time	O
step	O
following	O
the	O
stimulus	O
,	O
then	O
each	O
time	O
step	O
after	O
the	O
stimulus	O
is	O
represented	O
by	O
a	O
distinct	O
state	B
.	O
thus	O
,	O
the	O
td	O
error	O
,	O
being	O
state-dependent	O
,	O
can	O
be	O
sensitive	O
to	O
the	O
timing	O
of	O
events	O
within	O
a	O
trial	O
.	O
in	O
simulated	O
trials	O
with	O
these	O
assumptions	O
about	O
background	O
ﬁring	O
rate	O
and	O
input	O
representation	O
,	O
td	O
errors	O
of	O
the	O
td	O
model	O
are	O
remarkably	O
similar	O
to	O
dopamine	B
neu-	O
ron	O
phasic	O
activity	O
.	O
previewing	O
our	O
description	O
of	O
details	O
about	O
these	O
similarities	O
in	O
section	O
15.5	O
below	O
,	O
the	O
td	O
errors	O
parallel	O
the	O
following	O
features	O
of	O
dopamine	O
neuron	O
ac-	O
tivity	O
:	O
1	O
)	O
the	O
phasic	O
response	O
of	O
a	O
dopamine	B
neuron	O
only	O
occurs	O
when	O
a	O
rewarding	O
event	O
is	O
unpredicted	O
;	O
2	O
)	O
early	O
in	O
learning	O
,	O
neutral	O
cues	O
that	O
precede	O
a	O
reward	O
do	O
not	O
cause	O
substantial	O
phasic	O
dopamine	B
responses	O
,	O
but	O
with	O
continued	O
learning	O
these	O
cues	O
gain	O
pre-	O
dictive	O
value	B
and	O
come	O
to	O
elicit	O
phasic	O
dopamine	B
responses	O
;	O
3	O
)	O
if	O
an	O
even	O
earlier	O
cue	O
reliably	O
precedes	O
a	O
cue	O
that	O
has	O
already	O
acquired	O
predictive	O
value	O
,	O
the	O
phasic	O
dopamine	B
response	O
shifts	O
to	O
the	O
earlier	O
cue	O
,	O
ceasing	O
for	O
the	O
later	O
cue	O
;	O
and	O
4	O
)	O
if	O
after	O
learning	O
,	O
the	O
predicted	O
rewarding	O
event	O
is	O
omitted	O
,	O
a	O
dopamine	B
neuron	O
’	O
s	O
response	O
decreases	O
below	O
its	O
baseline	B
level	O
shortly	O
after	O
the	O
expected	O
time	O
of	O
the	O
rewarding	O
event	O
.	O
although	O
not	O
every	O
dopamine	B
neuron	O
monitored	O
in	O
the	O
experiments	O
of	O
schultz	O
and	O
col-	O
2in	O
the	O
literature	O
relating	O
td	O
errors	O
to	O
the	O
activity	O
of	O
dopamine	O
neurons	O
,	O
their	O
δt	O
is	O
the	O
same	O
as	O
our	O
δt−1	O
=	O
rt	O
+	O
γv	O
(	O
st	O
)	O
−	O
v	O
(	O
st−1	O
)	O
.	O
15.4.	O
dopamine	B
387	O
leagues	O
behaved	O
in	O
all	O
of	O
these	O
ways	O
,	O
the	O
striking	O
correspondence	O
between	O
the	O
activities	O
of	O
most	O
of	O
the	O
monitored	O
neurons	O
and	O
td	O
errors	O
lends	O
strong	O
support	O
to	O
the	O
reward	O
predic-	O
tion	B
error	O
hypothesis	O
.	O
there	O
are	O
situations	O
,	O
however	O
,	O
in	O
which	O
predictions	O
based	O
on	O
the	O
hypothesis	O
do	O
not	O
match	O
what	O
is	O
observed	O
in	O
experiments	O
.	O
the	O
choice	O
of	O
input	O
represen-	O
tation	O
is	O
critical	O
to	O
how	O
closely	O
td	O
errors	O
match	O
some	O
of	O
the	O
details	O
of	O
dopamine	O
neuron	O
activity	O
,	O
particularly	O
details	O
about	O
the	O
timing	O
of	O
dopamine	O
neuron	O
responses	O
.	O
diﬀerent	O
ideas	O
,	O
some	O
of	O
which	O
we	O
discuss	O
below	O
,	O
have	O
been	O
proposed	O
about	O
input	O
representations	O
and	O
other	O
features	O
of	O
td	O
learning	O
to	O
make	O
the	O
td	O
errors	O
ﬁt	O
the	O
data	O
better	O
,	O
though	O
the	O
main	O
parallels	O
appear	O
with	O
the	O
csc	O
representation	O
that	O
montague	O
et	O
al	O
.	O
used	O
.	O
overall	O
,	O
the	O
reward	B
prediction	I
error	I
hypothesis	I
has	O
received	O
wide	O
acceptance	O
among	O
neuroscien-	O
tists	O
studying	O
reward-based	O
learning	O
,	O
and	O
it	O
has	O
proven	O
to	O
be	O
remarkably	O
resilient	O
in	O
the	O
face	O
of	O
accumulating	O
results	O
from	O
neuroscience	B
experiments	O
.	O
to	O
prepare	O
for	O
our	O
description	O
of	O
the	O
neuroscience	B
experiments	O
supporting	O
the	O
reward	B
prediction	I
error	I
hypothesis	I
,	O
and	O
to	O
provide	O
some	O
context	O
so	O
that	O
the	O
signiﬁcance	O
of	O
the	O
hypothesis	O
can	O
be	O
appreciated	O
,	O
we	O
next	O
present	O
some	O
of	O
what	O
is	O
known	O
about	O
dopamine	B
,	O
the	O
brain	O
structures	O
it	O
inﬂuences	O
,	O
and	O
how	O
it	O
is	O
involved	O
in	O
reward-based	O
learning	O
.	O
15.4	O
dopamine	B
dopamine	O
is	O
produced	O
as	O
a	O
neurotransmitter	O
by	O
neurons	O
whose	O
cell	O
bodies	O
lie	O
mainly	O
in	O
two	O
clusters	O
of	O
neurons	O
in	O
the	O
midbrain	O
of	O
mammals	O
:	O
the	O
substantia	O
nigra	O
pars	O
com-	O
pacta	O
(	O
snpc	O
)	O
and	O
the	O
ventral	O
tegmental	O
area	O
(	O
vta	O
)	O
.	O
dopamine	B
plays	O
essential	O
roles	O
in	O
many	O
processes	O
in	O
the	O
mammalian	O
brain	O
.	O
prominent	O
among	O
these	O
are	O
motivation	B
,	O
learning	O
,	O
action-selection	O
,	O
most	O
forms	O
of	O
addiction	O
,	O
and	O
the	O
disorders	O
schizophrenia	O
and	O
parkinson	O
’	O
s	O
disease	O
.	O
dopamine	B
is	O
called	O
a	O
neuromodulator	O
because	O
it	O
performs	O
many	O
functions	O
other	O
than	O
direct	O
fast	O
excitation	O
or	O
inhibition	O
of	O
targeted	O
neurons	O
.	O
although	O
much	O
remains	O
unknown	O
about	O
dopamine	B
’	O
s	O
functions	O
and	O
details	O
of	O
its	O
cellular	O
eﬀects	O
,	O
it	O
is	O
clear	O
that	O
it	O
is	O
fundamental	O
to	O
reward	O
processing	O
in	O
the	O
mammalian	O
brain	O
.	O
dopamine	B
is	O
not	O
the	O
only	O
neuromodulator	O
involved	O
in	O
reward	O
processing	O
,	O
and	O
its	O
role	O
in	O
aversive	O
situations—punishment—remains	O
controversial	O
.	O
dopamine	B
also	O
can	O
function	O
diﬀerently	O
in	O
non-mammals	O
.	O
but	O
no	O
one	O
doubts	O
that	O
dopamine	B
is	O
essential	O
for	O
reward-related	O
pro-	O
cesses	O
in	O
mammals	O
,	O
including	O
humans	O
.	O
an	O
early	O
,	O
traditional	O
view	O
is	O
that	O
dopamine	B
neurons	O
broadcast	O
a	O
reward	B
signal	I
to	O
multiple	O
brain	O
regions	O
implicated	O
in	O
learning	O
and	O
motivation	O
.	O
this	O
view	O
followed	O
from	O
a	O
famous	O
1954	O
paper	O
by	O
james	O
olds	O
and	O
peter	O
milner	O
that	O
described	O
the	O
eﬀects	O
of	O
electrical	O
stimulation	O
on	O
certain	O
areas	O
of	O
a	O
rat	O
’	O
s	O
brain	O
.	O
they	O
found	O
that	O
electrical	O
stimulation	O
to	O
particular	O
regions	O
acted	O
as	O
a	O
very	O
powerful	O
reward	O
in	O
controlling	O
the	O
rat	O
’	O
s	O
behavior	O
:	O
“	O
...	O
the	O
control	B
exercised	O
over	O
the	O
animal	O
’	O
s	O
behavior	O
by	O
means	O
of	O
this	O
reward	O
is	O
extreme	O
,	O
possibly	O
exceeding	O
that	O
exercised	O
by	O
any	O
other	O
reward	O
previously	O
used	O
in	O
animal	O
experimentation	O
”	O
(	O
olds	O
and	O
milner	O
,	O
1954	O
)	O
.	O
later	O
research	O
revealed	O
that	O
the	O
sites	O
at	O
which	O
stimulation	O
was	O
most	O
eﬀective	O
in	O
producing	O
this	O
rewarding	O
eﬀect	O
excited	O
dopamine	B
pathways	O
,	O
either	O
directly	O
or	O
indirectly	O
,	O
that	O
ordinarily	O
are	O
excited	O
by	O
natural	O
rewarding	O
stimuli	O
.	O
eﬀects	O
similar	O
to	O
these	O
with	O
rats	O
were	O
also	O
observed	O
with	O
human	O
subjects	O
.	O
these	O
observations	O
strongly	O
suggested	O
that	O
dopamine	B
neuron	O
activity	O
signals	O
reward	O
.	O
388	O
chapter	O
15	O
:	O
neuroscience	B
but	O
if	O
the	O
reward	B
prediction	I
error	I
hypothesis	I
is	O
correct—even	O
if	O
it	O
accounts	O
for	O
only	O
some	O
features	O
of	O
a	O
dopamine	B
neuron	O
’	O
s	O
activity—this	O
traditional	O
view	O
of	O
dopamine	O
neuron	O
activity	O
is	O
not	O
entirely	O
correct	O
:	O
phasic	O
responses	O
of	O
dopamine	O
neurons	O
signal	O
reward	O
pre-	O
diction	O
errors	O
,	O
not	O
reward	O
itself	O
.	O
in	O
reinforcement	O
learning	O
’	O
s	O
terms	O
,	O
a	O
dopamine	B
neuron	O
’	O
s	O
phasic	O
response	O
at	O
a	O
time	O
t	O
corresponds	O
to	O
δt−1	O
=	O
rt	O
+	O
γv	O
(	O
st	O
)	O
−	O
v	O
(	O
st−1	O
)	O
,	O
not	O
to	O
rt	O
.	O
reinforcement	B
learning	I
theory	O
and	O
algorithms	O
help	O
reconcile	O
the	O
reward-prediction-	O
error	O
view	O
with	O
the	O
conventional	O
notion	O
that	O
dopamine	B
signals	O
reward	O
.	O
in	O
many	O
of	O
the	O
algorithms	O
we	O
discuss	O
in	O
this	O
book	O
,	O
δ	O
functions	O
as	O
a	O
reinforcement	B
signal	I
,	O
meaning	O
that	O
it	O
is	O
the	O
main	O
driver	O
of	O
learning	O
.	O
for	O
example	O
,	O
δ	O
is	O
the	O
critical	O
factor	O
in	O
the	O
td	O
model	O
of	O
classical	O
conditioning	B
,	O
and	O
δ	O
is	O
the	O
reinforcement	B
signal	I
for	O
learning	O
both	O
a	O
value	B
function	I
and	O
a	O
policy	B
in	O
an	O
actor–critic	B
architecture	O
(	O
sections	O
13.5	O
and	O
15.7	O
)	O
.	O
action-dependent	O
forms	O
of	O
δ	O
are	O
reinforcement	O
signals	O
for	O
q-learning	O
and	O
sarsa	O
.	O
the	O
reward	B
signal	I
rt	O
is	O
a	O
crucial	O
component	O
of	O
δt−1	O
,	O
but	O
it	O
is	O
not	O
the	O
complete	O
determinant	O
of	O
its	O
reinforcing	O
eﬀect	O
in	O
these	O
algorithms	O
.	O
the	O
additional	O
term	O
γv	O
(	O
st	O
)	O
−	O
v	O
(	O
st−1	O
)	O
is	O
the	O
higher-order	O
reinforcement	O
part	O
of	O
δt−1	O
,	O
and	O
even	O
if	O
reward	O
occurs	O
(	O
rt	O
(	O
cid:54	O
)	O
=	O
0	O
)	O
,	O
the	O
td	O
error	O
can	O
be	O
silent	O
if	O
the	O
reward	O
is	O
fully	O
predicted	O
(	O
which	O
is	O
fully	O
explained	O
in	O
section	O
15.6	O
below	O
)	O
.	O
a	O
closer	O
look	O
at	O
olds	O
’	O
and	O
milner	O
’	O
s	O
1954	O
paper	O
,	O
in	O
fact	O
,	O
reveals	O
that	O
it	O
is	O
mainly	O
about	O
the	O
reinforcing	O
eﬀect	O
of	O
electrical	O
stimulation	O
in	O
an	O
instrumental	B
conditioning	I
task	O
.	O
electrical	O
stimulation	O
not	O
only	O
energized	O
the	O
rats	O
’	O
behavior—through	O
dopamine	B
’	O
s	O
eﬀect	O
on	O
motivation—it	O
also	O
led	O
to	O
the	O
rats	O
quickly	O
learning	O
to	O
stimulate	O
themselves	O
by	O
pressing	O
a	O
lever	O
,	O
which	O
they	O
would	O
do	O
frequently	O
for	O
long	O
periods	O
of	O
time	O
.	O
the	O
activity	O
of	O
dopamine	O
neurons	O
triggered	O
by	O
electrical	O
stimulation	O
reinforced	O
the	O
rats	O
’	O
lever	O
pressing	O
.	O
more	O
recent	O
experiments	O
using	O
optogenetic	O
methods	O
clinch	O
the	O
role	O
of	O
phasic	O
responses	O
of	O
dopamine	O
neurons	O
as	O
reinforcement	O
signals	O
.	O
these	O
methods	O
allow	O
neuroscientists	O
to	O
precisely	O
control	B
the	O
activity	O
of	O
selected	O
neuron	O
types	O
at	O
a	O
millisecond	O
timescale	O
in	O
awake	O
behaving	O
animals	O
.	O
optogenetic	O
methods	O
introduce	O
light-sensitive	O
proteins	O
into	O
selected	O
neuron	O
types	O
so	O
that	O
these	O
neurons	O
can	O
be	O
activated	O
or	O
silenced	O
by	O
means	O
of	O
ﬂashes	O
of	O
laser	O
light	O
.	O
the	O
ﬁrst	O
experiment	O
using	O
optogenetic	O
methods	O
to	O
study	O
dopamine	B
neurons	O
showed	O
that	O
optogenetic	O
stimulation	O
producing	O
phasic	O
activation	O
of	O
dopamine	O
neurons	O
in	O
mice	O
was	O
enough	O
to	O
condition	O
the	O
mice	O
to	O
prefer	O
the	O
side	O
of	O
a	O
chamber	O
where	O
they	O
received	O
this	O
stimulation	O
as	O
compared	O
to	O
the	O
chamber	O
’	O
s	O
other	O
side	O
where	O
they	O
received	O
no	O
,	O
or	O
lower-frequency	O
,	O
stimulation	O
(	O
tsai	O
et	O
al	O
.	O
2009	O
)	O
.	O
in	O
another	O
example	O
,	O
steinberg	O
et	O
al	O
.	O
(	O
2013	O
)	O
used	O
optogenetic	O
activation	O
of	O
dopamine	O
neurons	O
to	O
create	O
artiﬁcial	O
bursts	O
of	O
dopamine	O
neuron	O
activity	O
in	O
rats	O
at	O
the	O
times	O
when	O
rewarding	O
stimuli	O
were	O
expected	O
but	O
omitted—times	O
when	O
dopamine	B
neuron	O
activity	O
normally	O
pauses	O
.	O
with	O
these	O
pauses	O
replaced	O
by	O
artiﬁcial	O
bursts	O
,	O
responding	O
was	O
sustained	O
when	O
it	O
would	O
ordinarily	O
decrease	O
due	O
to	O
lack	O
of	O
reinforcement	O
(	O
in	O
extinction	O
trials	O
)	O
,	O
and	O
learning	O
was	O
enabled	O
when	O
it	O
would	O
ordinarily	O
be	O
blocked	O
due	O
to	O
the	O
reward	O
being	O
already	O
predicted	O
(	O
the	O
blocking	B
paradigm	O
;	O
section	O
14.2.1	O
)	O
.	O
additional	O
evidence	O
for	O
the	O
reinforcing	O
function	O
of	O
dopamine	B
comes	O
from	O
optogenetic	O
experiments	O
with	O
fruit	O
ﬂies	O
,	O
except	O
in	O
these	O
animals	O
dopamine	B
’	O
s	O
eﬀect	O
is	O
the	O
opposite	O
of	O
its	O
eﬀect	O
in	O
mammals	O
:	O
optically	O
triggered	O
bursts	O
of	O
dopamine	O
neuron	O
activity	O
act	O
just	O
like	O
electric	O
foot	O
shock	O
in	O
reinforcing	O
avoidance	O
behavior	O
,	O
at	O
least	O
for	O
the	O
population	O
of	O
dopamine	O
neurons	O
activated	O
(	O
claridge-chang	O
et	O
al	O
.	O
2009	O
)	O
.	O
although	O
none	O
of	O
these	O
15.4.	O
dopamine	B
389	O
optogenetic	O
experiments	O
showed	O
that	O
phasic	O
dopamine	B
neuron	O
activity	O
is	O
speciﬁcally	O
like	O
a	O
td	O
error	O
,	O
they	O
convincingly	O
demonstrated	O
that	O
phasic	O
dopamine	B
neuron	O
activity	O
acts	O
just	O
like	O
δ	O
acts	O
(	O
or	O
perhaps	O
like	O
minus	O
δ	O
acts	O
in	O
fruit	O
ﬂies	O
)	O
as	O
the	O
reinforcement	B
signal	I
in	O
algorithms	O
for	O
both	O
prediction	B
(	O
classical	B
conditioning	I
)	O
and	B
control	I
(	O
instrumental	B
conditioning	I
)	O
.	O
dopamine	B
neurons	O
are	O
particularly	O
well	O
suited	O
to	O
broadcasting	O
a	O
reinforcement	B
signal	I
to	O
many	O
areas	O
of	O
the	O
brain	O
.	O
these	O
neurons	O
have	O
huge	O
axonal	O
arbors	O
,	O
each	O
releasing	O
dopamine	B
at	O
100	O
to	O
1,000	O
times	O
more	O
synaptic	O
sites	O
than	O
reached	O
by	O
the	O
axons	O
of	O
typical	O
neurons	O
.	O
shown	O
to	O
the	O
right	O
is	O
the	O
axonal	O
arbor	O
of	O
a	O
single	O
dopamine	B
neuron	O
whose	O
cell	O
body	O
is	O
in	O
the	O
snpc	O
of	O
a	O
rat	O
’	O
s	O
brain	O
.	O
each	O
axon	O
of	O
a	O
snpc	O
or	O
vta	O
dopamine	B
neuron	O
makes	O
roughly	O
500,000	O
synap-	O
tic	O
contacts	O
on	O
the	O
dendrites	O
of	O
neurons	O
in	O
tar-	O
geted	O
brain	O
areas	O
.	O
axonal	O
arbor	O
of	O
a	O
single	O
neuron	O
producing	O
dopamine	B
as	O
a	O
neurotransmitter	O
.	O
these	O
axons	O
make	O
synaptic	O
contacts	O
with	O
a	O
huge	O
number	O
of	O
dendrites	O
of	O
neurons	O
in	O
tar-	O
geted	O
brain	O
areas	O
.	O
adapted	O
from	O
the	O
jour-	O
nal	O
of	O
neuroscience	O
,	O
matsuda	O
,	O
furuta	O
,	O
naka-	O
if	O
dopamine	B
neurons	O
broadcast	O
a	O
reinforce-	O
ment	O
signal	O
like	O
reinforcement	B
learning	I
’	O
s	O
δ	O
,	O
then	O
because	O
this	O
is	O
a	O
scalar	O
signal	O
,	O
i.e.	O
,	O
a	O
single	O
num-	O
ber	O
,	O
all	O
dopamine	B
neurons	O
in	O
both	O
the	O
snpc	O
and	O
vta	O
would	O
be	O
expected	O
to	O
activate	O
more-or-	O
less	O
identically	O
so	O
that	O
they	O
would	O
act	O
in	O
near	O
synchrony	O
to	O
send	O
the	O
same	O
signal	O
to	O
all	O
of	O
the	O
sites	O
their	O
axons	O
target	B
.	O
although	O
it	O
has	O
been	O
a	O
common	O
belief	O
that	O
dopamine	B
neurons	O
do	O
act	O
together	O
like	O
this	O
,	O
modern	O
evidence	O
is	O
pointing	O
to	O
the	O
more	O
complicated	O
picture	O
that	O
diﬀerent	O
subpopulations	O
of	O
dopamine	O
neurons	O
respond	O
to	O
input	O
diﬀerently	O
depending	O
on	O
the	O
structures	O
to	O
which	O
they	O
send	O
their	O
signals	O
and	O
the	O
diﬀerent	O
ways	O
these	O
signals	O
act	O
on	O
their	O
target	B
structures	O
.	O
dopamine	B
has	O
functions	O
other	O
than	O
signaling	O
rpes	O
,	O
and	O
even	O
for	O
dopamine	O
neurons	O
that	O
do	O
signal	O
rpes	O
,	O
it	O
can	O
make	O
sense	O
to	O
send	O
diﬀerent	O
rpes	O
to	O
diﬀerent	O
structures	O
depending	O
on	O
the	O
roles	O
these	O
structures	O
play	O
in	O
producing	O
reinforced	O
behavior	O
.	O
this	O
is	O
beyond	O
what	O
we	O
treat	O
in	O
any	O
detail	O
in	O
this	O
book	O
,	O
but	O
vector-valued	O
rpe	O
signals	O
make	O
sense	O
from	O
the	O
perspective	O
of	O
reinforcement	O
learning	O
when	O
decisions	O
can	O
be	O
decomposed	O
into	O
separate	O
sub-decisions	O
,	O
or	O
more	O
gener-	O
ally	O
,	O
as	O
a	O
way	O
to	O
address	O
the	O
structural	B
version	O
of	O
the	O
credit	B
assignment	I
problem	O
:	O
how	O
do	O
you	O
distribute	O
credit	O
for	O
success	O
(	O
or	O
blame	O
for	O
failure	O
)	O
of	O
a	O
decision	O
among	O
the	O
many	O
component	O
structures	O
that	O
could	O
have	O
been	O
involved	O
in	O
producing	O
it	O
?	O
we	O
say	O
a	O
bit	O
more	O
about	O
this	O
in	O
section	O
15.10	O
below	O
.	O
mura	O
,	O
hioki	O
,	O
fujiyama	O
,	O
arai	O
,	O
and	O
kaneko	O
,	O
vol-	O
ume	O
29	O
,	O
2009	O
,	O
page	O
451.	O
the	O
axons	O
of	O
most	O
dopamine	B
neurons	O
make	O
synaptic	O
contact	O
with	O
neurons	O
in	O
the	O
frontal	O
cortex	O
and	O
the	O
basal	O
ganglia	O
,	O
areas	O
of	O
the	O
brain	O
involved	O
in	O
voluntary	O
movement	O
,	O
decision	O
making	O
,	O
learning	O
,	O
and	O
cognitive	O
functions	O
such	O
as	O
planning	O
.	O
because	O
most	O
ideas	O
relating	O
390	O
chapter	O
15	O
:	O
neuroscience	B
dopamine	O
to	O
reinforcement	B
learning	I
focus	O
on	O
the	O
basal	B
ganglia	I
,	O
and	O
the	O
connections	O
from	O
dopamine	B
neurons	O
are	O
particularly	O
dense	O
there	O
,	O
we	O
focus	O
on	O
the	O
basal	B
ganglia	I
here	O
.	O
the	O
basal	B
ganglia	I
are	O
a	O
collection	O
of	O
neuron	O
groups	O
,	O
or	O
nuclei	O
,	O
lying	O
at	O
the	O
base	O
of	O
the	O
forebrain	O
.	O
the	O
main	O
input	O
structure	O
of	O
the	O
basal	B
ganglia	I
is	O
called	O
the	O
striatum	O
.	O
essentially	O
all	O
of	O
the	O
cerebral	O
cortex	O
,	O
among	O
other	O
structures	O
,	O
provides	O
input	O
to	O
the	O
striatum	O
.	O
the	O
activity	O
of	O
cortical	O
neurons	O
conveys	O
a	O
wealth	O
of	O
information	O
about	O
sensory	O
input	O
,	O
internal	O
states	O
,	O
and	O
motor	O
activity	O
.	O
the	O
axons	O
of	O
cortical	O
neurons	O
make	O
synaptic	O
contacts	O
on	O
the	O
dendrites	O
of	O
the	O
main	O
input/output	O
neurons	O
of	O
the	O
striatum	O
,	O
called	O
medium	O
spiny	O
neurons	O
.	O
output	O
from	O
the	O
striatum	O
loops	O
back	O
via	O
other	O
basal	B
ganglia	I
nuclei	O
and	O
the	O
thalamus	O
to	O
frontal	O
areas	O
of	O
cortex	O
,	O
and	O
to	O
motor	O
areas	O
,	O
making	O
it	O
possible	O
for	O
the	O
striatum	O
to	O
inﬂuence	O
movement	O
,	O
abstract	O
decision	O
processes	O
,	O
and	O
reward	O
processing	O
.	O
two	O
main	O
subdivisions	O
of	O
the	O
striatum	O
are	O
important	O
for	O
reinforcement	O
learning	O
:	O
the	O
dorsal	O
striatum	O
,	O
primarily	O
implicated	O
in	O
inﬂuencing	O
action	B
selection	O
,	O
and	O
the	O
ventral	O
striatum	O
,	O
thought	O
to	O
be	O
critical	O
for	O
diﬀerent	O
aspects	O
of	O
reward	O
processing	O
,	O
including	O
the	O
assignment	O
of	O
aﬀective	O
value	B
to	O
sensations	O
.	O
the	O
dendrites	O
of	O
medium	O
spiny	O
neurons	O
are	O
covered	O
with	O
spines	O
on	O
whose	O
tips	O
the	O
axons	O
of	O
neurons	O
in	O
the	O
cortex	O
make	O
synaptic	O
contact	O
.	O
also	O
making	O
synaptic	O
contact	O
with	O
these	O
spines—in	O
this	O
case	O
contacting	O
the	O
spine	O
stems—are	O
axons	O
of	O
dopamine	O
neurons	O
(	O
figure	O
15.1	O
)	O
.	O
this	O
arrangement	O
brings	O
together	O
presynaptic	O
activity	O
of	O
cortical	O
neurons	O
,	O
figure	O
15.1	O
:	O
spine	O
of	O
a	O
striatal	O
neuron	O
showing	O
input	O
from	O
both	O
cortical	O
and	O
dopamine	O
neu-	O
rons	O
.	O
axons	O
of	O
cortical	O
neurons	O
inﬂuence	O
striatal	O
neurons	O
via	O
corticostriatal	O
synapses	O
releasing	O
the	O
neurotransmitter	O
glutamate	O
at	O
the	O
tips	O
of	O
spines	O
covering	O
the	O
dendrites	O
of	O
striatal	O
neurons	O
.	O
an	O
axon	O
of	O
a	O
vta	O
or	O
snpc	O
dopamine	B
neuron	O
is	O
shown	O
passing	O
by	O
the	O
spine	O
(	O
from	O
the	O
lower	O
right	O
)	O
.	O
“	O
dopamine	B
varicosities	O
”	O
on	O
this	O
axon	O
release	O
dopamine	B
at	O
or	O
near	O
the	O
spine	O
stem	O
,	O
in	O
an	O
arrangement	O
that	O
brings	O
together	O
presynaptic	O
input	O
from	O
cortex	O
,	O
postsynaptic	O
activity	O
of	O
the	O
striatal	O
neuron	O
,	O
and	O
dopamine	O
,	O
making	O
it	O
possible	O
that	O
several	O
types	O
of	O
learning	O
rules	O
govern	O
the	O
plasticity	O
of	O
corticostriatal	O
synapses	O
.	O
each	O
axon	O
of	O
a	O
dopamine	B
neuron	O
makes	O
synaptic	O
contact	O
with	O
the	O
stems	O
of	O
roughly	O
500,000	O
spines	O
.	O
some	O
of	O
the	O
complexity	O
omitted	O
from	O
our	O
discussion	O
is	O
shown	O
here	O
by	O
other	O
neurotransmitter	O
pathways	O
and	O
multiple	O
receptor	O
types	O
,	O
such	O
as	O
d1	O
an	O
d2	O
dopamine	B
receptors	O
by	O
which	O
dopamine	B
can	O
produce	O
diﬀerent	O
eﬀects	O
at	O
spines	O
and	O
other	O
postsynaptic	O
sites	O
.	O
from	O
journal	O
of	O
neurophysiology	O
,	O
w.	O
schultz	O
,	O
vol	O
.	O
80	O
,	O
1998	O
,	O
page	O
10	O
.	O
15.5.	O
experimental	O
support	O
for	O
the	O
reward	B
prediction	I
error	I
hypothesis	I
391	O
postsynaptic	O
activity	O
of	O
medium	O
spiny	O
neurons	O
,	O
and	O
input	O
from	O
dopamine	B
neurons	O
.	O
what	O
actually	O
occurs	O
at	O
these	O
spines	O
is	O
complex	O
and	O
not	O
completely	O
understood	O
.	O
figure	O
15.1	O
hints	O
at	O
the	O
complexity	O
by	O
showing	O
two	O
types	O
of	O
receptors	O
for	O
dopamine	O
,	O
receptors	O
for	O
glutamate—the	O
neurotransmitter	O
of	O
the	O
cortical	O
inputs—and	O
multiple	O
ways	O
that	O
the	O
various	O
signals	O
can	O
interact	O
.	O
but	O
evidence	O
is	O
mounting	O
that	O
changes	O
in	O
the	O
eﬃcacies	O
of	O
the	O
synapses	O
on	O
the	O
pathway	O
from	O
the	O
cortex	O
to	O
the	O
striatum	O
,	O
which	O
neuroscientists	O
call	O
corticostriatal	O
synapses	O
,	O
depend	O
critically	O
on	O
appropriately-timed	O
dopamine	B
signals	O
.	O
15.5	O
experimental	O
support	O
for	O
the	O
reward	O
predic-	O
tion	B
error	O
hypothesis	O
dopamine	B
neurons	O
respond	O
with	O
bursts	O
of	O
activity	O
to	O
intense	O
,	O
novel	O
,	O
or	O
unexpected	O
vi-	O
sual	O
and	O
auditory	O
stimuli	O
that	O
trigger	O
eye	O
and	O
body	O
movements	O
,	O
but	O
very	O
little	O
of	O
their	O
activity	O
is	O
related	O
to	O
the	O
movements	O
themselves	O
.	O
this	O
is	O
surprising	O
because	O
degenera-	O
tion	B
of	O
dopamine	B
neurons	O
is	O
a	O
cause	O
of	O
parkinson	O
’	O
s	O
disease	O
,	O
whose	O
symptoms	O
include	O
motor	O
disorders	O
,	O
particularly	O
deﬁcits	O
in	O
self-initiated	O
movement	O
.	O
motivated	O
by	O
the	O
weak	O
relationship	O
between	O
dopamine	B
neuron	O
activity	O
and	O
stimulus-triggered	O
eye	O
and	O
body	O
movements	O
,	O
romo	O
and	O
schultz	O
(	O
1990	O
)	O
and	O
schultz	O
and	O
romo	O
(	O
1990	O
)	O
took	O
the	O
ﬁrst	O
steps	O
toward	O
the	O
reward	B
prediction	I
error	I
hypothesis	I
by	O
recording	O
the	O
activity	O
of	O
dopamine	O
neurons	O
and	O
muscle	O
activity	O
while	O
monkeys	O
moved	O
their	O
arms	O
.	O
they	O
trained	O
two	O
monkeys	O
to	O
reach	O
from	O
a	O
resting	O
hand	O
position	O
into	O
a	O
bin	O
containing	O
a	O
bit	O
of	O
apple	O
,	O
a	O
piece	O
of	O
cookie	O
,	O
or	O
a	O
raisin	O
,	O
when	O
the	O
monkey	O
saw	O
and	O
heard	O
the	O
bin	O
’	O
s	O
door	O
open	O
.	O
the	O
monkey	O
could	O
then	O
grab	O
and	O
bring	O
the	O
food	O
to	O
its	O
mouth	O
.	O
after	O
a	O
monkey	O
became	O
good	O
at	O
this	O
,	O
it	O
was	O
trained	O
on	O
two	O
additional	O
tasks	O
.	O
the	O
purpose	O
of	O
the	O
ﬁrst	O
task	O
was	O
to	O
see	O
what	O
dopamine	B
neurons	O
do	O
when	O
movements	O
are	O
self-initiated	O
.	O
the	O
bin	O
was	O
left	O
open	O
but	O
covered	O
from	O
above	O
so	O
that	O
the	O
monkey	O
could	O
not	O
see	O
inside	O
but	O
could	O
reach	O
in	O
from	O
below	O
.	O
no	O
triggering	O
stimuli	O
were	O
presented	O
,	O
and	O
after	O
the	O
monkey	O
reached	O
for	O
and	O
ate	O
the	O
food	O
morsel	O
,	O
the	O
experimenter	O
usually	O
(	O
though	O
not	O
always	O
)	O
,	O
silently	O
and	O
unseen	O
by	O
the	O
monkey	O
,	O
replaced	O
food	O
in	O
the	O
bin	O
by	O
sticking	O
it	O
onto	O
a	O
rigid	O
wire	O
.	O
here	O
too	O
,	O
the	O
activity	O
of	O
the	O
dopamine	B
neurons	O
romo	O
and	O
schultz	O
monitored	O
was	O
not	O
related	O
to	O
the	O
monkey	O
’	O
s	O
movements	O
,	O
but	O
a	O
large	O
percentage	O
of	O
these	O
neurons	O
produced	O
phasic	O
responses	O
whenever	O
the	O
monkey	O
ﬁrst	O
touched	O
a	O
food	O
morsel	O
.	O
these	O
neurons	O
did	O
not	O
respond	O
when	O
the	O
monkey	O
touched	O
just	O
the	O
wire	O
or	O
explored	O
the	O
bin	O
when	O
no	O
food	O
was	O
there	O
.	O
this	O
was	O
good	O
evidence	O
that	O
the	O
neurons	O
were	O
responding	O
to	O
the	O
food	O
and	O
not	O
to	O
other	O
aspects	O
of	O
the	O
task	O
.	O
the	O
purpose	O
of	O
romo	O
and	O
schultz	O
’	O
s	O
second	O
task	O
was	O
to	O
see	O
what	O
happens	O
when	O
move-	O
ments	O
are	O
triggered	O
by	O
stimuli	O
.	O
this	O
task	O
used	O
a	O
diﬀerent	O
bin	O
with	O
a	O
movable	O
cover	O
.	O
the	O
sight	O
and	O
sound	O
of	O
the	O
bin	O
opening	O
triggered	O
reaching	O
movements	O
to	O
the	O
bin	O
.	O
in	O
this	O
case	O
,	O
romo	O
and	O
schultz	O
found	O
that	O
after	O
some	O
period	O
of	O
training	O
,	O
the	O
dopamine	B
neurons	O
no	O
longer	O
responded	O
to	O
the	O
touch	O
of	O
the	O
food	O
but	O
instead	O
responded	O
to	O
the	O
sight	O
and	O
sound	O
of	O
the	O
opening	O
cover	O
of	O
the	O
food	O
bin	O
.	O
the	O
phasic	O
responses	O
of	O
these	O
neurons	O
had	O
shifted	O
from	O
the	O
reward	O
itself	O
to	O
stimuli	O
predicting	O
the	O
availability	O
of	O
the	O
reward	O
.	O
in	O
a	O
followup	O
study	O
,	O
romo	O
and	O
schultz	O
found	O
that	O
most	O
of	O
the	O
dopamine	B
neurons	O
whose	O
activity	O
they	O
392	O
chapter	O
15	O
:	O
neuroscience	B
monitored	O
did	O
not	O
respond	O
to	O
the	O
sight	O
and	O
sound	O
of	O
the	O
bin	O
opening	O
outside	O
the	O
context	O
of	O
the	O
behavioral	O
task	O
.	O
these	O
observations	O
suggested	O
that	O
the	O
dopamine	B
neurons	O
were	O
responding	O
neither	O
to	O
the	O
initiation	O
of	O
a	O
movement	O
nor	O
to	O
the	O
sensory	O
properties	O
of	O
the	O
stimuli	O
,	O
but	O
were	O
rather	O
signaling	O
an	O
expectation	O
of	O
reward	O
.	O
schultz	O
’	O
s	O
group	O
conducted	O
many	O
additional	O
studies	O
involving	O
both	O
snpc	O
and	O
vta	O
dopamine	B
neurons	O
.	O
a	O
particular	O
series	O
of	O
experiments	O
was	O
inﬂuential	O
in	O
suggesting	O
that	O
the	O
phasic	O
responses	O
of	O
dopamine	O
neurons	O
correspond	O
to	O
td	O
errors	O
and	O
not	O
to	O
simpler	O
errors	O
like	O
those	O
in	O
the	O
rescorla–wagner	O
model	O
(	O
14.3	O
)	O
.	O
in	O
the	O
ﬁrst	O
of	O
these	O
experiments	O
(	O
ljungberg	O
,	O
apicella	O
,	O
and	O
schultz	O
,	O
1992	O
)	O
,	O
monkeys	O
were	O
trained	O
to	O
depress	O
a	O
lever	O
after	O
a	O
light	O
was	O
illuminated	O
as	O
a	O
‘	O
trigger	O
cue	O
’	O
to	O
obtain	O
a	O
drop	O
of	O
apple	O
juice	O
.	O
as	O
romo	O
and	O
schultz	O
had	O
observed	O
earlier	O
,	O
many	O
dopamine	B
neurons	O
initially	O
responded	O
to	O
the	O
reward—	O
the	O
drop	O
of	O
juice	O
(	O
figure	O
15.2	O
,	O
top	O
panel	O
)	O
.	O
but	O
many	O
of	O
these	O
neurons	O
lost	O
that	O
reward	O
response	O
as	O
training	O
continued	O
and	O
developed	O
responses	O
instead	O
to	O
the	O
illumination	O
of	O
the	O
light	O
that	O
predicted	O
the	O
reward	O
(	O
figure	O
15.2	O
,	O
middle	O
panel	O
)	O
.	O
with	O
continued	O
training	O
,	O
lever	O
pressing	O
became	O
faster	O
while	O
the	O
number	O
of	O
dopamine	O
neurons	O
responding	O
to	O
the	O
trigger	O
cue	O
decreased	O
.	O
figure	O
15.2	O
:	O
the	O
response	O
of	O
dopamine	O
neurons	O
shifts	O
from	O
initial	O
responses	O
to	O
primary	O
reward	O
to	O
earlier	O
predictive	O
stimuli	O
.	O
these	O
are	O
plots	O
of	O
the	O
number	O
of	O
action	O
potentials	O
produced	O
by	O
monitored	O
dopamine	B
neurons	O
within	O
small	O
time	O
intervals	O
,	O
averaged	O
over	O
all	O
the	O
monitored	O
dopamine	B
neurons	O
(	O
ranging	O
from	O
23	O
to	O
44	O
neurons	O
for	O
these	O
data	O
)	O
.	O
top	O
:	O
dopamine	B
neurons	O
are	O
activated	O
by	O
the	O
unpredicted	O
delivery	O
of	O
drop	O
of	O
apple	O
juice	O
.	O
middle	O
:	O
with	O
learning	O
,	O
dopamine	B
neurons	O
developed	O
responses	O
to	O
the	O
reward-predicting	O
trigger	O
cue	O
and	O
lost	O
responsiveness	O
to	O
the	O
delivery	O
of	O
reward	O
.	O
bottom	O
:	O
with	O
the	O
addition	O
of	O
an	O
instruction	O
cue	O
preceding	O
the	O
trigger	O
cue	O
by	O
1	O
second	O
,	O
dopamine	B
neurons	O
shifted	O
their	O
responses	O
from	O
the	O
trigger	O
cue	O
to	O
the	O
earlier	O
instruction	O
cue	O
.	O
from	O
schultz	O
et	O
al	O
.	O
(	O
1995	O
)	O
,	O
mit	O
press	O
.	O
following	O
this	O
study	O
,	O
the	O
same	O
monkeys	O
were	O
trained	O
on	O
a	O
new	O
task	O
(	O
schultz	O
,	O
apicella	O
,	O
and	O
ljungberg	O
,	O
1993	O
)	O
.	O
here	O
the	O
monkeys	O
faced	O
two	O
levers	O
,	O
each	O
with	O
a	O
light	O
above	O
it	O
.	O
15.5.	O
experimental	O
support	O
for	O
the	O
reward	B
prediction	I
error	I
hypothesis	I
393	O
illuminating	O
one	O
of	O
these	O
lights	O
was	O
an	O
‘	O
instruction	O
cue	O
’	O
indicating	O
which	O
of	O
the	O
two	O
levers	O
would	O
produce	O
a	O
drop	O
of	O
apple	O
juice	O
.	O
in	O
this	O
task	O
,	O
the	O
instruction	O
cue	O
preceded	O
the	O
trig-	O
ger	O
cue	O
of	O
the	O
previous	O
task	O
by	O
a	O
ﬁxed	O
interval	O
of	O
1	O
second	O
.	O
the	O
monkeys	O
learned	O
to	O
withhold	O
reach-	O
ing	B
until	O
seeing	O
the	O
trigger	O
cue	O
,	O
and	O
dopamine	O
neuron	O
activity	O
increased	O
,	O
but	O
now	O
the	O
responses	O
of	O
the	O
mon-	O
itored	O
dopamine	B
neurons	O
occurred	O
almost	O
exclusively	O
to	O
the	O
earlier	O
in-	O
struction	O
cue	O
and	O
not	O
to	O
the	O
trig-	O
ger	O
cue	O
(	O
figure	O
15.2	O
,	O
bottom	O
panel	O
)	O
.	O
here	O
again	O
the	O
number	O
of	O
dopamine	O
neurons	O
responding	O
to	O
the	O
instruc-	O
tion	B
cue	O
was	O
much	O
reduced	O
when	O
the	O
task	O
was	O
well	O
learned	O
.	O
during	O
learn-	O
ing	B
across	O
these	O
tasks	O
,	O
dopamine	B
neuron	O
activity	O
shifted	O
from	O
initially	O
responding	O
to	O
the	O
reward	O
to	O
re-	O
sponding	O
to	O
the	O
earlier	O
predictive	O
stimuli	O
,	O
ﬁrst	O
progressing	O
to	O
the	O
trig-	O
ger	O
stimulus	O
then	O
to	O
the	O
still	O
ear-	O
lier	O
instruction	O
cue	O
.	O
as	O
responding	O
moved	O
earlier	O
in	O
time	O
it	O
disappeared	O
from	O
the	O
later	O
stimuli	O
.	O
this	O
shifting	O
of	O
responses	O
to	O
earlier	O
reward	O
predic-	O
tors	O
,	O
while	O
losing	O
responses	O
to	O
later	O
predictors	O
is	O
a	O
hallmark	O
of	O
td	O
learn-	O
ing	B
(	O
see	O
,	O
for	O
example	O
,	O
figure	O
14.4	O
)	O
.	O
the	O
task	O
just	O
described	O
revealed	O
another	O
property	O
of	O
dopamine	O
neu-	O
ron	O
activity	O
shared	O
with	O
td	O
learn-	O
ing	B
.	O
sometimes	O
pressed	O
the	O
wrong	O
key	O
,	O
that	O
is	O
,	O
the	O
key	O
other	O
than	O
the	O
instructed	O
one	O
,	O
and	O
consequently	O
received	O
no	O
re-	O
ward	O
.	O
in	O
these	O
trials	O
,	O
many	O
of	O
the	O
dopamine	B
neurons	O
showed	O
a	O
sharp	O
decrease	O
in	O
their	O
ﬁring	O
rates	O
below	O
baseline	B
shortly	O
after	O
the	O
reward	O
’	O
s	O
usual	O
time	O
of	O
delivery	O
,	O
and	O
this	O
hap-	O
pened	O
without	O
the	O
availability	O
of	O
any	O
the	O
monkeys	O
figure	O
15.3	O
:	O
the	O
response	O
of	O
dopamine	O
neurons	O
drops	O
below	O
baseline	B
shortly	O
after	O
the	O
time	O
when	O
an	O
ex-	O
pected	O
reward	O
fails	O
to	O
occur	O
.	O
top	O
:	O
dopamine	B
neurons	O
are	O
activated	O
by	O
the	O
unpredicted	O
delivery	O
of	O
a	O
drop	O
of	O
apple	O
juice	O
.	O
middle	O
:	O
dopamine	B
neurons	O
respond	O
to	O
a	O
conditioned	O
stimulus	O
(	O
cs	O
)	O
that	O
predicts	O
reward	O
and	O
do	O
not	O
respond	O
to	O
the	O
reward	O
itself	O
.	O
bottom	O
:	O
when	O
the	O
reward	O
predicted	O
by	O
the	O
cs	O
fails	O
to	O
occur	O
,	O
the	O
activ-	O
ity	O
of	O
dopamine	O
neurons	O
drops	O
below	O
baseline	B
shortly	O
after	O
the	O
time	O
the	O
reward	O
is	O
expected	O
to	O
occur	O
.	O
at	O
the	O
top	O
of	O
each	O
of	O
these	O
panels	O
is	O
shown	O
the	O
average	O
number	O
of	O
action	O
potentials	O
produced	O
by	O
monitored	O
dopamine	B
neurons	O
within	O
small	O
time	O
intervals	O
around	O
the	O
indicated	O
times	O
.	O
the	O
raster	O
plots	O
below	O
show	O
the	O
activity	O
patterns	O
of	O
the	O
individual	O
dopamine	B
neurons	O
that	O
were	O
monitored	O
;	O
each	O
dot	O
represents	O
an	O
action	B
potential	O
.	O
from	O
schultz	O
,	O
dayan	O
,	O
and	O
montague	O
,	O
a	O
neural	B
substrate	O
of	O
prediction	O
and	O
reward	O
,	O
science	O
,	O
vol	O
.	O
275	O
,	O
issue	O
5306	O
,	O
pages	O
1593-1598	O
,	O
march	O
14	O
,	O
1997.	O
reprinted	O
with	O
permission	O
from	O
aaas	O
.	O
394	O
chapter	O
15	O
:	O
neuroscience	B
external	O
cue	O
to	O
mark	O
the	O
usual	O
time	O
of	O
reward	O
delivery	O
(	O
figure	O
15.3	O
)	O
.	O
somehow	O
the	O
mon-	O
keys	O
were	O
internally	O
keeping	O
track	O
of	O
the	O
timing	O
of	O
the	O
reward	O
.	O
(	O
response	O
timing	O
is	O
one	O
area	O
where	O
the	O
simplest	O
version	O
of	O
td	O
learning	O
needs	O
to	O
be	O
modiﬁed	O
to	O
account	O
for	O
some	O
of	O
the	O
details	O
of	O
the	O
timing	O
of	O
dopamine	O
neuron	O
responses	O
.	O
we	O
consider	O
this	O
issue	O
in	O
the	O
following	O
section	O
.	O
)	O
the	O
observations	O
from	O
the	O
studies	O
described	O
above	O
led	O
schultz	O
and	O
his	O
group	O
to	O
con-	O
clude	O
that	O
dopamine	B
neurons	O
respond	O
to	O
unpredicted	O
rewards	O
,	O
to	O
the	O
earliest	O
predictors	O
of	O
reward	O
,	O
and	O
that	O
dopamine	B
neuron	O
activity	O
decreases	O
below	O
baseline	B
if	O
a	O
reward	O
,	O
or	O
a	O
predictor	O
of	O
reward	O
,	O
does	O
not	O
occur	O
at	O
its	O
expected	O
time	O
.	O
researchers	O
familiar	O
with	O
reinforcement	O
learning	O
were	O
quick	O
to	O
recognize	O
that	O
these	O
results	O
are	O
strikingly	O
similar	O
to	O
how	O
the	O
td	O
error	O
behaves	O
as	O
the	O
reinforcement	B
signal	I
in	O
a	O
td	O
algorithm	O
.	O
the	O
next	O
section	O
explores	O
this	O
similarity	O
by	O
working	O
through	O
a	O
speciﬁc	O
example	O
in	O
detail	O
.	O
15.6	O
td	O
error/dopamine	O
correspondence	O
this	O
section	O
explains	O
the	O
correspondence	O
between	O
the	O
td	O
error	O
δ	O
and	O
the	O
phasic	O
responses	O
of	O
dopamine	O
neurons	O
observed	O
in	O
the	O
experiments	O
just	O
described	O
.	O
we	O
examine	O
how	O
δ	O
changes	O
over	O
the	O
course	O
of	O
learning	O
in	O
a	O
task	O
something	O
like	O
the	O
one	O
described	O
above	O
where	O
a	O
monkey	O
ﬁrst	O
sees	O
an	O
instruction	O
cue	O
and	O
then	O
a	O
ﬁxed	O
time	O
later	O
has	O
to	O
respond	O
correctly	O
to	O
a	O
trigger	O
cue	O
in	O
order	O
to	O
obtain	O
reward	O
.	O
we	O
use	O
a	O
simple	O
idealized	O
version	O
of	O
this	O
task	O
,	O
but	O
we	O
go	O
into	O
a	O
lot	O
more	O
detail	O
than	O
is	O
usual	O
because	O
we	O
want	O
to	O
emphasize	O
the	O
theoretical	O
basis	O
of	O
the	O
parallel	O
between	O
td	O
errors	O
and	O
dopamine	O
neuron	O
activity	O
.	O
the	O
ﬁrst	O
simplifying	O
assumption	O
is	O
that	O
the	O
agent	O
has	O
already	O
learned	O
the	O
actions	O
required	O
to	O
obtain	O
reward	O
.	O
then	O
its	O
task	O
is	O
just	O
to	O
learn	O
accurate	O
predictions	O
of	O
future	O
reward	O
for	O
the	O
sequence	O
of	O
states	O
it	O
experiences	O
.	O
this	O
is	O
then	O
a	O
prediction	B
task	O
,	O
or	O
more	O
technically	O
,	O
a	O
policy-evaluation	O
task	O
:	O
learning	O
the	O
value	B
function	I
for	O
a	O
ﬁxed	O
policy	B
(	O
sections	O
4.1	O
and	O
6.1	O
)	O
.	O
the	O
value	B
function	I
to	O
be	O
learned	O
assigns	O
to	O
each	O
state	B
a	O
value	B
that	O
predicts	O
the	O
return	B
that	O
will	O
follow	O
that	O
state	B
if	O
the	O
agent	O
selects	O
actions	O
according	O
to	O
the	O
given	O
policy	B
,	O
where	O
the	O
return	B
is	O
the	O
(	O
possibly	O
discounted	O
)	O
sum	O
of	O
all	O
the	O
future	O
rewards	O
.	O
this	O
is	O
unrealistic	O
as	O
a	O
model	O
of	O
the	O
monkey	O
’	O
s	O
situation	O
because	O
the	O
monkey	O
would	O
likely	O
learn	O
these	O
predictions	O
at	O
the	O
same	O
time	O
that	O
it	O
is	O
learning	O
to	O
act	O
correctly	O
(	O
as	O
would	O
a	O
reinforcement	B
learning	I
algorithm	O
that	O
learns	O
policies	O
as	O
well	O
as	O
value	O
functions	O
,	O
such	O
as	O
an	O
actor–critic	B
algorithm	O
)	O
,	O
but	O
this	O
scenario	O
is	O
simpler	O
to	O
describe	O
than	O
one	O
in	O
which	O
a	O
policy	B
and	O
a	O
value	B
function	I
are	O
learned	O
simultaneously	O
.	O
now	O
imagine	O
that	O
the	O
agent	O
’	O
s	O
experience	O
divides	O
into	O
multiple	O
trials	O
,	O
in	O
each	O
of	O
which	O
the	O
same	O
sequence	O
of	O
states	O
repeats	O
,	O
with	O
a	O
distinct	O
state	B
occurring	O
on	O
each	O
time	O
step	O
during	O
the	O
trial	O
.	O
further	O
imagine	O
that	O
the	O
return	B
being	O
predicted	O
is	O
limited	O
to	O
the	O
return	B
over	O
a	O
trial	O
,	O
which	O
makes	O
a	O
trial	O
analogous	O
to	O
a	O
reinforcement	B
learning	I
episode	O
as	O
we	O
have	O
deﬁned	O
it	O
.	O
in	O
reality	O
,	O
of	O
course	O
,	O
the	O
returns	O
being	O
predicted	O
are	O
not	O
conﬁned	O
to	O
single	O
trials	O
,	O
and	O
the	O
time	O
interval	O
between	O
trials	O
is	O
an	O
important	O
factor	O
in	O
determining	O
what	O
an	O
animal	O
learns	O
.	O
this	O
is	O
true	O
for	O
td	O
learning	O
as	O
well	O
,	O
but	O
here	O
we	O
assume	O
that	O
returns	O
do	O
not	O
accumulate	O
over	O
multiple	O
trials	O
.	O
given	O
this	O
,	O
then	O
,	O
a	O
trial	O
in	O
experiments	O
like	O
those	O
conducted	O
by	O
schultz	O
and	O
colleagues	O
is	O
equivalent	O
to	O
an	O
episode	O
of	O
reinforcement	O
learning	O
.	O
(	O
though	O
in	O
this	O
discussion	O
,	O
we	O
will	O
use	O
the	O
term	O
trial	O
instead	O
of	O
episode	O
to	O
15.6.	O
td	O
error/dopamine	O
correspondence	O
395	O
relate	O
better	O
to	O
the	O
experiments	O
.	O
)	O
as	O
usual	O
,	O
we	O
also	O
need	O
to	O
make	O
an	O
assumption	O
about	O
how	O
states	O
are	O
represented	O
as	O
inputs	O
to	O
the	O
learning	O
algorithm	O
,	O
an	O
assumption	O
that	O
inﬂuences	O
how	O
closely	O
the	O
td	O
error	O
corresponds	O
to	O
dopamine	B
neuron	O
activity	O
.	O
we	O
discuss	O
this	O
issue	O
later	O
,	O
but	O
for	O
now	O
we	O
assume	O
the	O
same	O
csc	O
representation	O
used	O
by	O
montague	O
et	O
al	O
.	O
(	O
1996	O
)	O
in	O
which	O
there	O
is	O
a	O
separate	O
internal	O
stimulus	O
for	O
each	O
state	B
visited	O
at	O
each	O
time	O
step	O
in	O
a	O
trial	O
.	O
this	O
reduces	O
the	O
process	O
to	O
the	O
tabular	O
case	O
covered	O
in	O
the	O
ﬁrst	O
part	O
of	O
this	O
book	O
.	O
finally	O
,	O
we	O
assume	O
that	O
the	O
agent	O
uses	O
td	O
(	O
0	O
)	O
to	O
learn	O
a	O
value	B
function	I
,	O
v	O
,	O
stored	O
in	O
a	O
lookup	O
table	O
initialized	O
to	O
be	O
zero	O
for	O
all	O
the	O
states	O
.	O
we	O
also	O
assume	O
that	O
this	O
is	O
a	O
deterministic	O
task	O
and	O
that	O
the	O
discount	O
factor	O
,	O
γ	O
,	O
is	O
very	O
nearly	O
one	O
so	O
that	O
we	O
can	O
ignore	O
it	O
.	O
figure	O
15.4	O
shows	O
the	O
time	O
courses	O
of	O
r	O
,	O
v	O
,	O
and	O
δ	O
at	O
several	O
stages	O
of	O
learning	O
in	O
this	O
policy-evaluation	O
task	O
.	O
the	O
time	O
axes	O
represent	O
the	O
time	O
interval	O
over	O
which	O
a	O
sequence	O
of	O
states	O
is	O
visited	O
in	O
a	O
trial	O
(	O
where	O
for	O
clarity	O
we	O
omit	O
showing	O
individual	O
states	O
)	O
.	O
the	O
reward	B
signal	I
is	O
zero	O
throughout	O
each	O
trial	O
except	O
when	O
the	O
agent	O
reaches	O
the	O
rewarding	O
state	B
,	O
shown	O
near	O
the	O
right	O
end	O
of	O
the	O
time	O
line	O
,	O
when	O
the	O
reward	B
signal	I
becomes	O
some	O
positive	O
number	O
,	O
say	O
r	O
(	O
cid:63	O
)	O
.	O
the	O
goal	B
of	O
td	O
learning	O
is	O
to	O
predict	O
the	O
return	B
for	O
each	O
state	B
visited	O
in	O
a	O
trial	O
,	O
which	O
in	O
this	O
undiscounted	O
case	O
and	O
given	O
our	O
assumption	O
that	O
predictions	O
are	O
conﬁned	O
to	O
individual	O
trials	O
,	O
is	O
simply	O
r	O
(	O
cid:63	O
)	O
for	O
each	O
state	B
.	O
figure	O
15.4	O
:	O
the	O
behavior	O
of	O
the	O
td	O
error	O
δ	O
during	O
td	O
learning	O
is	O
consistent	O
with	O
features	O
of	O
the	O
phasic	O
activation	O
of	O
dopamine	O
neurons	O
.	O
(	O
here	O
δ	O
is	O
the	O
td	O
error	O
available	O
at	O
time	O
t	O
,	O
i.e.	O
,	O
δt−1	O
)	O
.	O
top	O
:	O
a	O
sequence	O
of	O
states	O
,	O
shown	O
as	O
an	O
interval	O
of	O
regular	O
predictors	O
,	O
is	O
followed	O
by	O
a	O
non-zero	O
reward	O
r	O
(	O
cid:63	O
)	O
.	O
early	O
in	O
learning	O
:	O
the	O
initial	O
value	B
function	I
,	O
v	O
,	O
and	O
initial	O
δ	O
,	O
which	O
at	O
ﬁrst	O
is	O
equal	O
to	O
r	O
(	O
cid:63	O
)	O
.	O
learning	O
complete	O
:	O
the	O
value	B
function	I
accurately	O
predicts	O
future	O
reward	O
,	O
δ	O
is	O
positive	O
at	O
the	O
earliest	O
predictive	O
state	O
,	O
and	O
δ	O
=	O
0	O
at	O
the	O
time	O
of	O
the	O
non-zero	O
reward	O
.	O
r	O
(	O
cid:63	O
)	O
omitted	O
:	O
at	O
the	O
time	O
the	O
predicted	O
reward	O
is	O
omitted	O
,	O
δ	O
becomes	O
negative	O
.	O
see	O
text	O
for	O
a	O
complete	O
explanation	O
of	O
why	O
this	O
happens	O
.	O
rrrrrtr t 1=rt+vt vt 1rvv   early	O
inlearninglearningcompleteomittedrregular	O
predictors	O
of	O
over	O
this	O
intervalrr	O
?	O
396	O
chapter	O
15	O
:	O
neuroscience	B
preceding	O
the	O
rewarding	O
state	B
is	O
a	O
sequence	O
of	O
reward-predicting	O
states	O
,	O
with	O
the	O
earliest	O
reward-predicting	O
state	B
shown	O
near	O
the	O
left	O
end	O
of	O
the	O
time	O
line	O
.	O
this	O
is	O
like	O
the	O
state	B
near	O
the	O
start	O
of	O
a	O
trial	O
,	O
for	O
example	O
like	O
the	O
state	B
marked	O
by	O
the	O
instruction	O
cue	O
in	O
a	O
trial	O
of	O
the	O
monkey	O
experiment	O
of	O
schultz	O
et	O
al	O
.	O
(	O
1993	O
)	O
described	O
above	O
.	O
it	O
is	O
the	O
ﬁrst	O
state	B
in	O
a	O
trial	O
that	O
reliably	O
predicts	O
that	O
trial	O
’	O
s	O
reward	O
.	O
(	O
of	O
course	O
,	O
in	O
reality	O
states	O
visited	O
on	O
preceding	O
trials	O
are	O
even	O
earlier	O
reward-predicting	O
states	O
,	O
but	O
because	O
we	O
are	O
conﬁning	O
predictions	O
to	O
individual	O
trials	O
,	O
these	O
do	O
not	O
qualify	O
as	O
predictors	O
of	O
this	O
trial	O
’	O
s	O
reward	O
.	O
below	O
we	O
give	O
a	O
more	O
satisfactory	O
,	O
though	O
more	O
abstract	O
,	O
description	O
of	O
an	O
earliest	O
reward-predicting	O
state	B
.	O
)	O
the	O
latest	O
reward-predicting	O
state	B
in	O
a	O
trial	O
is	O
the	O
state	B
immediately	O
preceding	O
the	O
trial	O
’	O
s	O
rewarding	O
state	B
.	O
this	O
is	O
the	O
state	B
near	O
the	O
far	O
right	O
end	O
of	O
the	O
time	O
line	O
in	O
figure	O
15.4.	O
note	O
that	O
the	O
rewarding	O
state	B
of	O
a	O
trial	O
does	O
not	O
predict	O
the	O
return	B
for	O
that	O
trial	O
:	O
the	O
value	B
of	O
this	O
state	B
would	O
come	O
to	O
predict	O
the	O
return	B
over	O
all	O
the	O
following	O
trials	O
,	O
which	O
here	O
we	O
are	O
assuming	O
to	O
be	O
zero	O
in	O
this	O
episodic	O
formulation	O
.	O
figure	O
15.4	O
shows	O
the	O
ﬁrst-trial	O
time	O
courses	O
of	O
v	O
and	O
δ	O
as	O
the	O
graphs	O
labeled	O
‘	O
early	O
in	O
learning.	O
’	O
because	O
the	O
reward	B
signal	I
is	O
zero	O
throughout	O
the	O
trial	O
except	O
when	O
the	O
rewarding	O
state	B
is	O
reached	O
,	O
and	O
all	O
the	O
v	O
-values	O
are	O
zero	O
,	O
the	O
td	O
error	O
is	O
also	O
zero	O
until	O
it	O
becomes	O
r	O
(	O
cid:63	O
)	O
at	O
the	O
rewarding	O
state	B
.	O
this	O
follows	O
because	O
δt−1	O
=	O
rt	O
+	O
vt	O
−	O
vt−1	O
=	O
rt	O
+	O
0	O
−	O
0	O
=	O
rt	O
,	O
which	O
is	O
zero	O
until	O
it	O
equals	O
r	O
(	O
cid:63	O
)	O
when	O
the	O
reward	O
occurs	O
.	O
here	O
vt	O
and	O
vt−1	O
are	O
respectively	O
the	O
estimated	O
values	O
of	O
the	O
states	O
visited	O
at	O
times	O
t	O
and	O
t	O
−	O
1	O
in	O
a	O
trial	O
.	O
the	O
td	O
error	O
at	O
this	O
stage	O
of	O
learning	O
is	O
analogous	O
to	O
a	O
dopamine	B
neuron	O
responding	O
to	O
an	O
unpredicted	O
reward	O
,	O
e.g.	O
,	O
a	O
drop	O
apple	O
juice	O
,	O
at	O
the	O
start	O
of	O
training	O
.	O
throughout	O
this	O
ﬁrst	O
trial	O
and	O
all	O
successive	O
trials	O
,	O
td	O
(	O
0	O
)	O
updates	O
occur	O
at	O
each	O
state	B
transition	O
as	O
described	O
in	O
chapter	O
6.	O
this	O
successively	O
increases	O
the	O
values	O
of	O
the	O
reward-predicting	O
states	O
,	O
with	O
the	O
increases	O
spreading	O
backwards	O
from	O
the	O
rewarding	O
state	B
,	O
until	O
the	O
values	O
converge	O
to	O
the	O
correct	O
return	B
predictions	O
.	O
in	O
this	O
case	O
(	O
because	O
we	O
are	O
assuming	O
no	O
discounting	B
)	O
the	O
correct	O
predictions	O
are	O
equal	O
to	O
r	O
(	O
cid:63	O
)	O
for	O
all	O
the	O
reward-predicting	O
states	O
.	O
this	O
can	O
be	O
seen	O
in	O
figure	O
15.4	O
as	O
the	O
graph	O
of	O
v	O
labeled	O
‘	O
learning	O
complete	O
’	O
where	O
the	O
values	O
of	O
all	O
the	O
states	O
from	O
the	O
earliest	O
to	O
the	O
latest	O
reward-predicting	O
states	O
all	O
equal	O
r	O
(	O
cid:63	O
)	O
.	O
the	O
values	O
of	O
the	O
states	O
preceding	O
the	O
earliest	O
reward-predicting	O
state	B
remain	O
low	O
(	O
which	O
figure	O
15.4	O
shows	O
as	O
zero	O
)	O
because	O
they	O
are	O
not	O
reliable	O
predictors	O
of	O
reward	O
.	O
when	O
learning	O
is	O
complete	O
,	O
that	O
is	O
,	O
when	O
v	O
attains	O
its	O
correct	O
values	O
,	O
the	O
td	O
errors	O
associated	O
with	O
transitions	O
from	O
any	O
reward-predicting	O
state	B
are	O
zero	O
because	O
the	O
predic-	O
tions	O
are	O
now	O
accurate	O
.	O
this	O
is	O
because	O
for	O
a	O
transition	O
from	O
a	O
reward-predicting	O
state	B
to	O
another	O
reward-predicting	O
state	B
,	O
we	O
have	O
δt−1	O
=	O
rt	O
+	O
vt	O
−	O
vt−1	O
=	O
0	O
+	O
r	O
(	O
cid:63	O
)	O
−	O
r	O
(	O
cid:63	O
)	O
=	O
0	O
,	O
and	O
for	O
the	O
transition	O
from	O
the	O
latest	O
reward-predicting	O
state	B
to	O
the	O
rewarding	O
state	B
,	O
we	O
have	O
δt−1	O
=	O
rt	O
+	O
vt	O
−	O
vt−1	O
=	O
r	O
(	O
cid:63	O
)	O
+	O
0	O
−	O
r	O
(	O
cid:63	O
)	O
=	O
0.	O
on	O
the	O
other	O
hand	O
,	O
the	O
td	O
error	O
on	O
a	O
transition	O
from	O
any	O
state	B
to	O
the	O
earliest	O
reward-predicting	O
state	B
is	O
positive	O
because	O
of	O
the	O
mismatch	O
between	O
this	O
state	B
’	O
s	O
low	O
value	B
and	O
the	O
larger	O
value	B
of	O
the	O
following	O
reward-	O
predicting	O
state	B
.	O
indeed	O
,	O
if	O
the	O
value	B
of	O
a	O
state	B
preceding	O
the	O
earliest	O
reward-predicting	O
state	B
were	O
zero	O
,	O
then	O
after	O
the	O
transition	O
to	O
the	O
earliest	O
reward-predicting	O
state	B
,	O
we	O
would	O
have	O
that	O
δt−1	O
=	O
rt	O
+	O
vt	O
−	O
vt−1	O
=	O
0	O
+	O
r	O
(	O
cid:63	O
)	O
−	O
0	O
=	O
r	O
(	O
cid:63	O
)	O
.	O
the	O
‘	O
learning	O
complete	O
’	O
graph	O
of	O
δ	O
in	O
figure	O
15.4	O
shows	O
this	O
positive	O
value	B
at	O
the	O
earliest	O
reward-predicting	O
state	B
,	O
and	O
zeros	O
15.6.	O
td	O
error/dopamine	O
correspondence	O
397	O
everywhere	O
else	O
.	O
the	O
positive	O
td	O
error	O
upon	O
transitioning	O
to	O
the	O
earliest	O
reward-predicting	O
state	B
is	O
analogous	O
to	O
the	O
persistence	O
of	O
dopamine	O
responses	O
to	O
the	O
earliest	O
stimuli	O
predicting	O
reward	O
.	O
by	O
the	O
same	O
token	O
,	O
when	O
learning	O
is	O
complete	O
,	O
a	O
transition	O
from	O
the	O
latest	O
reward-predicting	O
state	B
to	O
the	O
rewarding	O
state	B
produces	O
a	O
zero	O
td	O
error	O
because	O
the	O
lat-	O
est	O
reward-predicting	O
state	B
’	O
s	O
value	B
,	O
being	O
correct	O
,	O
cancels	O
the	O
reward	O
.	O
this	O
parallels	O
the	O
observation	O
that	O
fewer	O
dopamine	B
neurons	O
generate	O
a	O
phasic	O
response	O
to	O
a	O
fully	O
predicted	O
reward	O
than	O
to	O
an	O
unpredicted	O
reward	O
.	O
after	O
learning	O
,	O
if	O
the	O
reward	O
is	O
suddenly	O
omitted	O
,	O
the	O
td	O
error	O
goes	O
negative	O
at	O
the	O
usual	O
time	O
of	O
reward	O
because	O
the	O
value	B
of	O
the	O
latest	O
reward-predicting	O
state	B
is	O
then	O
too	O
high	O
:	O
δt−1	O
=	O
rt	O
+	O
vt	O
−	O
vt−1	O
=	O
0	O
+	O
0	O
−	O
r	O
(	O
cid:63	O
)	O
=	O
−r	O
(	O
cid:63	O
)	O
,	O
as	O
shown	O
at	O
the	O
right	O
end	O
of	O
the	O
‘	O
r	O
omitted	O
’	O
graph	O
of	O
δ	O
in	O
figure	O
15.4.	O
this	O
is	O
like	O
dopamine	B
neuron	O
activity	O
decreasing	O
below	O
baseline	B
at	O
the	O
time	O
an	O
expected	O
reward	O
is	O
omitted	O
as	O
seen	O
in	O
the	O
experiment	O
of	O
schultz	O
et	O
al	O
.	O
(	O
1993	O
)	O
described	O
above	O
and	O
shown	O
in	O
figure	O
15.3.	O
the	O
idea	O
of	O
an	O
earliest	O
reward-predicting	O
state	B
deserves	O
more	O
attention	O
.	O
in	O
the	O
sce-	O
nario	O
described	O
above	O
,	O
because	O
experience	O
is	O
divided	O
into	O
trials	O
,	O
and	O
we	O
assumed	O
that	O
predictions	O
are	O
conﬁned	O
to	O
individual	O
trials	O
,	O
the	O
earliest	O
reward-predicting	O
state	B
is	O
al-	O
ways	O
the	O
ﬁrst	O
state	B
of	O
a	O
trial	O
.	O
clearly	O
this	O
is	O
artiﬁcial	O
.	O
a	O
more	O
general	O
way	O
to	O
think	O
of	O
an	O
earliest	O
reward-predicting	O
state	B
is	O
that	O
it	O
is	O
an	O
unpredicted	O
predictor	O
of	O
reward	O
,	O
and	O
there	O
can	O
be	O
many	O
such	O
states	O
.	O
in	O
an	O
animal	O
’	O
s	O
life	O
,	O
many	O
diﬀerent	O
states	O
may	O
precede	O
an	O
earliest	O
reward-predicting	O
state	B
.	O
however	O
,	O
because	O
these	O
states	O
are	O
more	O
often	O
fol-	O
lowed	O
by	O
other	O
states	O
that	O
do	O
not	O
predict	O
reward	O
,	O
their	O
reward-predicting	O
powers	O
,	O
that	O
is	O
,	O
their	O
values	O
,	O
remain	O
low	O
.	O
a	O
td	O
algorithm	O
,	O
if	O
operating	O
throughout	O
the	O
animal	O
’	O
s	O
life	O
,	O
would	O
update	O
the	O
values	O
of	O
these	O
states	O
too	O
,	O
but	O
the	O
updates	O
would	O
not	O
consistently	O
accumulate	O
because	O
,	O
by	O
assumption	O
,	O
none	O
of	O
these	O
states	O
reliably	O
precedes	O
an	O
earliest	O
reward-predicting	O
state	B
.	O
if	O
any	O
of	O
them	O
did	O
,	O
they	O
would	O
be	O
reward-predicting	O
states	O
as	O
well	O
.	O
this	O
might	O
explain	O
why	O
with	O
overtraining	O
,	O
dopamine	B
responses	O
decrease	O
to	O
even	O
the	O
earliest	O
reward-predicting	O
stimulus	O
in	O
a	O
trial	O
.	O
with	O
overtraining	O
one	O
would	O
expect	O
that	O
even	O
a	O
formerly-unpredicted	O
predictor	O
state	B
would	O
become	O
predicted	O
by	O
stimuli	O
as-	O
sociated	O
with	O
earlier	O
states	O
:	O
the	O
animal	O
’	O
s	O
interaction	O
with	O
its	O
environment	B
both	O
inside	O
and	O
outside	O
of	O
an	O
experimental	O
task	O
would	O
become	O
commonplace	O
.	O
upon	O
breaking	O
this	O
routine	O
with	O
the	O
introduction	O
of	O
a	O
new	O
task	O
,	O
however	O
,	O
one	O
would	O
see	O
td	O
errors	O
reappear	O
,	O
as	O
indeed	O
is	O
observed	O
in	O
dopamine	O
neuron	O
activity	O
.	O
the	O
example	O
described	O
above	O
explains	O
why	O
the	O
td	O
error	O
shares	O
key	O
features	O
with	O
the	O
phasic	O
activity	O
of	O
dopamine	O
neurons	O
when	O
the	O
animal	O
is	O
learning	O
in	O
a	O
task	O
similar	O
to	O
the	O
idealized	O
task	O
of	O
our	O
example	O
.	O
but	O
not	O
every	O
property	O
of	O
the	O
phasic	O
activity	O
of	O
dopamine	O
neurons	O
coincides	O
so	O
neatly	O
with	O
properties	O
of	O
δ.	O
one	O
of	O
the	O
most	O
troubling	O
discrepancies	O
involves	O
what	O
happens	O
when	O
a	O
reward	O
occurs	O
earlier	O
than	O
expected	O
.	O
we	O
have	O
seen	O
that	O
the	O
omission	O
of	O
an	O
expected	O
reward	O
produces	O
a	O
negative	O
prediction	B
error	O
at	O
the	O
reward	O
’	O
s	O
expected	O
time	O
,	O
which	O
corresponds	O
to	O
the	O
activity	O
of	O
dopamine	O
neurons	O
decreasing	O
below	O
baseline	B
when	O
this	O
happens	O
.	O
if	O
the	O
reward	O
arrives	O
later	O
than	O
expected	O
,	O
it	O
is	O
then	O
an	O
unexpected	O
reward	O
and	O
generates	O
a	O
positive	O
prediction	B
error	O
.	O
this	O
happens	O
with	O
both	O
td	O
errors	O
and	O
dopamine	O
neuron	O
responses	O
.	O
but	O
when	O
reward	O
arrives	O
earlier	O
than	O
expected	O
,	O
dopamine	B
neurons	O
do	O
not	O
do	O
what	O
the	O
td	O
error	O
does—at	O
least	O
with	O
the	O
398	O
chapter	O
15	O
:	O
neuroscience	B
csc	O
representation	O
used	O
by	O
montague	O
et	O
al	O
.	O
(	O
1996	O
)	O
and	O
by	O
us	O
in	O
our	O
example	O
.	O
dopamine	B
neurons	O
do	O
respond	O
to	O
the	O
early	O
reward	O
,	O
which	O
is	O
consistent	O
with	O
a	O
positive	O
td	O
error	O
because	O
the	O
reward	O
is	O
not	O
predicted	O
to	O
occur	O
then	O
.	O
however	O
,	O
at	O
the	O
later	O
time	O
when	O
the	O
reward	O
is	O
expected	O
but	O
omitted	O
,	O
the	O
td	O
error	O
is	O
negative	O
whereas	O
,	O
in	O
contrast	O
to	O
this	O
prediction	B
,	O
dopamine	B
neuron	O
activity	O
does	O
not	O
drop	O
below	O
baseline	B
in	O
the	O
way	O
the	O
td	O
model	O
predicts	O
(	O
hollerman	O
and	O
schultz	O
,	O
1998	O
)	O
.	O
something	O
more	O
complicated	O
is	O
going	O
on	O
in	O
the	O
animal	O
’	O
s	O
brain	O
than	O
simply	O
td	O
learning	O
with	O
a	O
csc	O
representation	O
.	O
some	O
of	O
the	O
mismatches	O
between	O
the	O
td	O
error	O
and	O
dopamine	B
neuron	O
activity	O
can	O
be	O
addressed	O
by	O
selecting	O
suitable	O
parameter	O
values	O
for	O
the	O
td	O
algorithm	O
and	O
by	O
using	O
stimulus	O
representations	O
other	O
than	O
the	O
csc	O
representation	O
.	O
for	O
instance	O
,	O
to	O
address	O
the	O
early-reward	O
mismatch	O
just	O
described	O
,	O
suri	O
and	O
schultz	O
(	O
1999	O
)	O
proposed	O
a	O
csc	O
represen-	O
tation	O
in	O
which	O
the	O
sequences	O
of	O
internal	O
signals	O
initiated	O
by	O
earlier	O
stimuli	O
are	O
cancelled	O
by	O
the	O
occurrence	O
of	O
a	O
reward	O
.	O
another	O
proposal	O
by	O
daw	O
,	O
courville	O
,	O
and	O
touretzky	O
(	O
2006	O
)	O
is	O
that	O
the	O
brain	O
’	O
s	O
td	O
system	O
uses	O
representations	O
produced	O
by	O
statistical	O
mod-	O
eling	O
carried	O
out	O
in	O
sensory	O
cortex	O
rather	O
than	O
simpler	O
representations	O
based	O
on	O
raw	O
sensory	O
input	O
.	O
ludvig	O
,	O
sutton	O
,	O
and	O
kehoe	O
(	O
2008	O
)	O
found	O
that	O
td	O
learning	O
with	O
a	O
mi-	O
crostimulus	O
(	O
ms	O
)	O
representation	O
(	O
figure	O
14.1	O
)	O
ﬁts	O
the	O
activity	O
of	O
dopamine	O
neurons	O
in	O
the	O
early-reward	O
and	O
other	O
situations	O
better	O
than	O
when	O
a	O
csc	O
representation	O
is	O
used	O
.	O
pan	O
,	O
schmidt	O
,	O
wickens	O
,	O
and	O
hyland	O
(	O
2005	O
)	O
found	O
that	O
even	O
with	O
the	O
csc	O
represen-	O
tation	O
,	O
prolonged	O
eligibility	B
traces	I
improve	O
the	O
ﬁt	O
of	O
the	O
td	O
error	O
to	O
some	O
aspects	O
of	O
dopamine	O
neuron	O
activity	O
.	O
in	O
general	O
,	O
many	O
ﬁne	O
details	O
of	O
td-error	O
behavior	O
depend	O
on	O
subtle	O
interactions	O
between	O
eligibility	B
traces	I
,	O
discounting	B
,	O
and	O
stimulus	O
representations	O
.	O
findings	O
like	O
these	O
elaborate	O
and	O
reﬁne	O
the	O
reward	B
prediction	I
error	I
hypothesis	I
without	O
refuting	O
its	O
core	O
claim	O
that	O
the	O
phasic	O
activity	O
of	O
dopamine	O
neurons	O
is	O
well	O
characterized	O
as	O
signaling	O
td	O
errors	O
.	O
on	O
the	O
other	O
hand	O
,	O
there	O
are	O
other	O
discrepancies	O
between	O
the	O
td	O
theory	O
and	O
exper-	O
imental	O
data	O
that	O
are	O
not	O
so	O
easily	O
accommodated	O
by	O
selecting	O
parameter	O
values	O
and	O
stimulus	O
representations	O
(	O
we	O
mention	O
some	O
of	O
these	O
discrepancies	O
in	O
the	O
bibliographical	O
and	O
historical	O
remarks	O
section	O
at	O
the	O
end	O
of	O
this	O
chapter	O
)	O
,	O
and	O
more	O
mismatches	O
are	O
likely	O
to	O
be	O
discovered	O
as	O
neuroscientists	O
conduct	O
ever	O
more	O
reﬁned	O
experiments	O
.	O
but	O
the	O
reward	B
prediction	I
error	I
hypothesis	I
has	O
been	O
functioning	O
very	O
eﬀectively	O
as	O
a	O
catalyst	O
for	O
improving	O
our	O
understanding	O
of	O
how	O
the	O
brain	O
’	O
s	O
reward	O
system	O
works	O
.	O
intricate	O
experi-	O
ments	O
have	O
been	O
designed	O
to	O
validate	O
or	O
refute	O
predictions	O
derived	O
from	O
the	O
hypothesis	O
,	O
and	O
experimental	O
results	O
have	O
,	O
in	O
turn	O
,	O
led	O
to	O
reﬁnement	O
and	O
elaboration	O
of	O
the	O
td	O
error/dopamine	O
hypothesis	O
.	O
a	O
remarkable	O
aspect	O
of	O
these	O
developments	O
is	O
that	O
the	O
reinforcement	B
learning	I
algo-	O
rithms	O
and	O
theory	O
that	O
connect	O
so	O
well	O
with	O
properties	O
of	O
the	O
dopamine	B
system	O
were	O
developed	O
from	O
a	O
computational	O
perspective	O
in	O
total	O
absence	O
of	O
any	O
knowledge	O
about	O
the	O
relevant	O
properties	O
of	O
dopamine	O
neurons—remember	O
,	O
td	O
learning	O
and	O
its	O
connec-	O
tions	O
to	O
optimal	B
control	I
and	O
dynamic	B
programming	I
were	O
developed	O
many	O
years	O
before	O
any	O
of	O
the	O
experiments	O
were	O
conducted	O
that	O
revealed	O
the	O
td-like	O
nature	O
of	O
dopamine	O
neuron	O
activity	O
.	O
this	O
unplanned	O
correspondence	O
,	O
despite	O
not	O
being	O
perfect	O
,	O
suggests	O
that	O
the	O
td	O
error/dopamine	O
parallel	O
captures	O
something	O
signiﬁcant	O
about	O
brain	O
reward	O
processes	O
.	O
15.7.	O
neural	B
actor–critic	O
399	O
in	O
addition	O
to	O
accounting	O
for	O
many	O
features	O
of	O
the	O
phasic	O
activity	O
of	O
dopamine	O
neurons	O
,	O
the	O
reward	B
prediction	I
error	I
hypothesis	I
links	O
neuroscience	B
to	O
other	O
aspects	O
of	O
reinforce-	O
ment	O
learning	O
,	O
in	O
particular	O
,	O
to	O
learning	O
algorithms	O
that	O
use	O
td	O
errors	O
as	O
reinforcement	O
signals	O
.	O
neuroscience	B
is	O
still	O
far	O
from	O
reaching	O
complete	O
understanding	O
of	O
the	O
circuits	O
,	O
molecular	O
mechanisms	O
,	O
and	O
functions	O
of	O
the	O
phasic	O
activity	O
of	O
dopamine	O
neurons	O
,	O
but	O
evi-	O
dence	O
supporting	O
the	O
reward	B
prediction	I
error	I
hypothesis	I
,	O
along	O
with	O
evidence	O
that	O
phasic	O
dopamine	B
responses	O
are	O
reinforcement	O
signals	O
for	O
learning	O
,	O
suggest	O
that	O
the	O
brain	O
might	O
implement	O
something	O
like	O
an	O
actor–critic	B
algorithm	O
in	O
which	O
td	O
errors	O
play	O
critical	O
roles	O
.	O
other	O
reinforcement	B
learning	I
algorithms	O
are	O
plausible	O
candidates	O
too	O
,	O
but	O
actor–critic	B
algorithms	O
ﬁt	O
the	O
anatomy	O
and	O
physiology	O
of	O
the	O
mammalian	O
brain	O
particularly	O
well	O
,	O
as	O
we	O
describe	O
in	O
the	O
following	O
two	O
sections	O
.	O
15.7	O
neural	B
actor–critic	O
actor–critic	B
algorithms	O
learn	O
both	O
policies	O
and	O
value	O
functions	O
.	O
the	O
‘	O
actor	O
’	O
is	O
the	O
com-	O
ponent	O
that	O
learns	O
policies	O
,	O
and	O
the	O
‘	O
critic	B
’	O
is	O
the	O
component	O
that	O
learns	O
about	O
whatever	O
policy	B
is	O
currently	O
being	O
followed	O
by	O
the	O
actor	O
in	O
order	O
to	O
‘	O
criticize	O
’	O
the	O
actor	O
’	O
s	O
action	B
choices	O
.	O
the	O
critic	B
uses	O
a	O
td	O
algorithm	O
to	O
learn	O
the	O
state-value	O
function	O
for	O
the	O
actor	O
’	O
s	O
current	O
policy	B
.	O
the	O
value	B
function	I
allows	O
the	O
critic	B
to	O
critique	O
the	O
actor	O
’	O
s	O
action	B
choices	O
by	O
sending	O
td	O
errors	O
,	O
δ	O
,	O
to	O
the	O
actor	O
.	O
a	O
positive	O
δ	O
means	O
that	O
the	O
action	B
was	O
‘	O
good	O
’	O
because	O
it	O
led	O
to	O
a	O
state	B
with	O
a	O
better-than-expected	O
value	B
;	O
a	O
negative	O
δ	O
means	O
that	O
the	O
action	B
was	O
‘	O
bad	O
’	O
because	O
it	O
led	O
to	O
a	O
state	B
with	O
a	O
worse-than-expected	O
value	B
.	O
based	O
on	O
these	O
critiques	O
,	O
the	O
actor	O
continually	O
updates	O
its	O
policy	B
.	O
two	O
distinctive	O
features	O
of	O
actor–critic	O
algorithms	O
are	O
responsible	O
for	O
thinking	O
that	O
the	O
brain	O
might	O
implement	O
an	O
algorithm	O
like	O
this	O
.	O
first	O
,	O
the	O
two	O
components	O
of	O
an	O
actor–	O
critic	B
algorithm—the	O
actor	O
and	O
the	O
critic—suggest	O
that	O
two	O
parts	O
of	O
the	O
striatum—the	O
dorsal	O
and	O
ventral	O
subdivisions	O
(	O
section	O
15.4	O
)	O
,	O
both	O
critical	O
for	O
reward-based	O
learning—	O
may	O
function	O
respectively	O
something	O
like	O
an	O
actor	O
and	O
a	O
critic	B
.	O
a	O
second	O
property	O
of	O
actor–critic	O
algorithms	O
that	O
suggests	O
a	O
brain	O
implementation	O
is	O
that	O
the	O
td	O
error	O
has	O
the	O
dual	O
role	O
of	O
being	O
the	O
reinforcement	B
signal	I
for	O
both	O
the	O
actor	O
and	O
the	O
critic	O
,	O
though	O
it	O
has	O
a	O
diﬀerent	O
inﬂuence	O
on	O
learning	O
in	O
each	O
of	O
these	O
components	O
.	O
this	O
ﬁts	O
well	O
with	O
several	O
properties	O
of	O
the	O
neural	B
circuitry	O
:	O
axons	O
of	O
dopamine	O
neurons	O
target	B
both	O
the	O
dorsal	O
and	O
ventral	O
subdivisions	O
of	O
the	O
striatum	O
;	O
dopamine	B
appears	O
to	O
be	O
critical	O
for	O
modulating	O
synaptic	B
plasticity	I
in	O
both	O
structures	O
;	O
and	O
how	O
a	O
neuromodulator	O
such	O
as	O
dopamine	O
acts	O
on	O
a	O
target	B
structure	O
depends	O
on	O
properties	O
of	O
the	O
target	B
structure	O
and	O
not	O
just	O
on	O
properties	O
of	O
the	O
neuromodulator	O
.	O
section	O
13.5	O
presents	O
actor–critic	B
algorithms	O
as	O
policy	O
gradient	B
methods	O
,	O
but	O
the	O
actor–critic	B
algorithm	O
of	O
barto	O
,	O
sutton	O
,	O
and	O
anderson	O
(	O
1983	O
)	O
was	O
simpler	O
and	O
was	O
pre-	O
sented	O
as	O
an	O
artiﬁcial	O
neural	O
network	O
.	O
here	O
we	O
describe	O
an	O
artiﬁcial	O
neural	O
network	O
implementation	O
something	O
like	O
that	O
of	O
barto	O
et	O
al.	O
,	O
and	O
we	O
follow	O
takahashi	O
,	O
schoen-	O
baum	O
,	O
and	O
niv	O
(	O
2008	O
)	O
in	O
giving	O
a	O
schematic	O
proposal	O
for	O
how	O
this	O
artiﬁcial	O
neural	O
network	O
might	O
be	O
implemented	O
by	O
real	O
neural	B
networks	I
in	O
the	O
brain	O
.	O
we	O
postpone	O
discussion	O
of	O
the	O
actor	O
and	O
critic	O
learning	O
rules	O
until	O
section	O
15.8	O
,	O
where	O
we	O
present	O
them	O
as	O
spe-	O
cial	O
cases	O
of	O
the	O
policy-gradient	O
formulation	O
and	O
discuss	O
what	O
they	O
suggest	O
about	O
how	O
400	O
chapter	O
15	O
:	O
neuroscience	B
dopamine	O
might	O
modulate	O
synaptic	B
plasticity	I
.	O
figure	O
15.5a	O
shows	O
an	O
implementation	O
of	O
an	O
actor–critic	B
algorithm	O
as	O
an	O
artiﬁcial	O
neural	O
network	O
with	O
component	O
networks	O
implementing	O
the	O
actor	O
and	O
the	O
critic	O
.	O
the	O
critic	B
consists	O
of	O
a	O
single	O
neuron-like	O
unit	O
,	O
v	O
,	O
whose	O
output	O
activity	O
represents	O
state	B
values	O
,	O
and	O
a	O
component	O
shown	O
as	O
the	O
diamond	O
labeled	O
td	O
that	O
computes	O
td	O
errors	O
by	O
combining	O
v	O
’	O
s	O
output	O
with	O
reward	O
signals	O
and	O
with	O
previous	O
state	B
values	O
(	O
as	O
suggested	O
by	O
the	O
loop	O
from	O
the	O
td	O
diamond	O
to	O
itself	O
)	O
.	O
the	O
actor	O
network	O
has	O
a	O
single	O
layer	O
of	O
k	O
actor	O
units	O
labeled	O
ai	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k.	O
the	O
output	O
of	O
each	O
actor	O
unit	O
is	O
a	O
component	O
of	O
a	O
k-dimensional	O
action	B
vector	O
.	O
an	O
alternative	O
is	O
that	O
there	O
are	O
k	O
separate	O
actions	O
,	O
one	O
commanded	O
by	O
each	O
actor	O
unit	O
,	O
that	O
compete	O
with	O
one	O
another	O
to	O
be	O
executed	O
,	O
but	O
here	O
we	O
will	O
think	O
of	O
the	O
entire	O
a-vector	O
as	O
an	O
action	B
.	O
figure	O
15.5	O
:	O
actor–critic	B
artiﬁcial	O
neural	B
network	O
and	O
a	O
hypothetical	O
neural	B
implementation	O
.	O
a	O
)	O
actor–critic	B
algorithm	O
as	O
an	O
artiﬁcial	O
neural	O
network	O
.	O
the	O
actor	O
adjusts	O
a	O
policy	B
based	O
on	O
the	O
td	O
error	O
δ	O
it	O
receives	O
from	O
the	O
critic	B
;	O
the	O
critic	B
adjusts	O
state-value	O
parameters	O
using	O
the	O
same	O
δ.	O
the	O
critic	B
produces	O
a	O
td	O
error	O
from	O
the	O
reward	B
signal	I
,	O
r	O
,	O
and	O
the	O
current	O
change	O
in	O
its	O
estimate	O
of	O
state	O
values	O
.	O
the	O
actor	O
does	O
not	O
have	O
direct	O
access	O
to	O
the	O
reward	B
signal	I
,	O
and	O
the	O
critic	O
does	O
not	O
have	O
direct	O
access	O
to	O
the	O
action	B
.	O
b	O
)	O
hypothetical	O
neural	B
implementation	O
of	O
an	O
actor–critic	B
algorithm	O
.	O
the	O
actor	O
and	O
the	O
value-learning	O
part	O
of	O
the	O
critic	B
are	O
respectively	O
placed	O
in	O
the	O
ventral	O
and	O
dorsal	O
subdivisions	O
of	O
the	O
striatum	O
.	O
the	O
td	O
error	O
is	O
transmitted	O
by	O
dopamine	B
neurons	O
located	O
in	O
the	O
vta	O
and	O
snpc	O
to	O
modulate	O
changes	O
in	O
synaptic	O
eﬃcacies	O
of	O
input	O
from	O
cortical	O
areas	O
to	O
the	O
ventral	O
and	O
dorsal	O
striatum	O
.	O
adapted	O
from	O
frontiers	O
in	B
neuroscience	I
,	O
vol	O
.	O
2	O
(	O
1	O
)	O
,	O
2008	O
,	O
y.	O
takahashi	O
,	O
g.	O
schoenbaum	O
,	O
and	O
y.	O
niv	O
,	O
silencing	O
the	O
critics	O
:	O
understanding	O
the	O
eﬀects	O
of	O
cocaine	O
sensitization	O
on	O
dorsolateral	O
and	O
ventral	O
striatum	O
in	O
the	O
context	O
of	O
an	O
actor/critic	O
model	O
.	O
rewardactortd	O
.	O
.	O
..	O
.	O
.critic	O
.	O
.	O
.δ	O
actionsstates/stimulitd	O
errorenvironmentactorvtasncs2s1sn	O
.	O
.	O
..	O
.	O
.critic	O
.	O
.	O
.s1sns2actionsstates/stimulidopaminestriatumventraldorsal	O
striatumcortex	O
(	O
multiple	O
areas	O
)	O
environmentreward 1 2 n 1 2 na1a2a3akv 	O
(	O
a	O
)	O
(	O
b	O
)	O
x1x2xnx1x2xn	O
15.7.	O
neural	B
actor–critic	O
401	O
both	O
the	O
critic	B
and	O
actor	O
networks	O
receive	O
input	O
consisting	O
of	O
multiple	O
features	O
rep-	O
resenting	O
the	O
state	B
of	O
the	O
agent	O
’	O
s	O
environment	B
.	O
(	O
recall	O
from	O
chapter	O
1	O
that	O
the	O
envi-	O
ronment	O
of	O
a	O
reinforcement	B
learning	I
agent	O
includes	O
components	O
both	O
inside	O
and	O
outside	O
of	O
the	O
‘	O
organism	O
’	O
containing	O
the	O
agent	O
.	O
)	O
the	O
ﬁgure	O
shows	O
these	O
features	O
as	O
the	O
circles	O
labeled	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
shown	O
twice	O
just	O
to	O
keep	O
the	O
ﬁgure	O
simple	O
.	O
a	O
weight	O
representing	O
the	O
eﬃcacy	O
of	O
a	O
synapse	O
is	O
associated	O
with	O
each	O
connection	O
from	O
each	O
feature	O
xi	O
to	O
the	O
critic	B
unit	O
,	O
v	O
,	O
and	O
to	O
each	O
of	O
the	O
action	B
units	O
,	O
ai	O
.	O
the	O
weights	O
in	O
the	O
critic	O
network	O
parameterize	O
the	O
value	B
function	I
,	O
and	O
the	O
weights	O
in	O
the	O
actor	O
network	O
parameterize	O
the	O
policy	B
.	O
the	O
networks	O
learn	O
as	O
these	O
weights	O
change	O
according	O
to	O
the	O
critic	B
and	O
actor	O
learning	O
rules	O
that	O
we	O
describe	O
in	O
the	O
following	O
section	O
.	O
the	O
td	O
error	O
produced	O
by	O
circuitry	O
in	O
the	O
critic	O
is	O
the	O
reinforcement	B
signal	I
for	O
chang-	O
ing	B
the	O
weights	O
in	O
both	O
the	O
critic	B
and	O
the	O
actor	O
networks	O
.	O
this	O
is	O
shown	O
in	O
figure	O
15.5a	O
by	O
the	O
line	O
labeled	O
‘	O
td	O
error	O
δ	O
’	O
extending	O
across	O
all	O
of	O
the	O
connections	O
in	O
the	O
critic	O
and	O
actor	O
networks	O
.	O
this	O
aspect	O
of	O
the	O
network	O
implementation	O
,	O
together	O
with	O
the	O
re-	O
ward	O
prediction	O
error	O
hypothesis	O
and	O
the	O
fact	O
that	O
the	O
activity	O
of	O
dopamine	O
neurons	O
is	O
so	O
widely	O
distributed	O
by	O
the	O
extensive	O
axonal	O
arbors	O
of	O
these	O
neurons	O
,	O
suggests	O
that	O
an	O
actor–critic	B
network	O
something	O
like	O
this	O
may	O
not	O
be	O
too	O
farfetched	O
as	O
a	O
hypothesis	O
about	O
how	O
reward-related	O
learning	O
might	O
happen	O
in	O
the	O
brain	O
.	O
figure	O
15.5b	O
suggests—very	O
schematically—how	O
the	O
artiﬁcial	O
neural	O
network	O
on	O
the	O
ﬁgure	O
’	O
s	O
left	O
might	O
map	O
onto	O
structures	O
in	O
the	O
brain	O
according	O
to	O
the	O
hypothesis	O
of	O
taka-	O
hashi	O
et	O
al	O
.	O
(	O
2008	O
)	O
.	O
the	O
hypothesis	O
puts	O
the	O
actor	O
and	O
the	O
value-learning	O
part	O
of	O
the	O
critic	B
respectively	O
in	O
the	O
dorsal	O
and	O
ventral	O
subdivisions	O
of	O
the	O
striatum	O
,	O
the	O
input	O
structure	O
of	O
the	O
basal	B
ganglia	I
.	O
recall	O
from	O
section	O
15.4	O
that	O
the	O
dorsal	O
striatum	O
is	O
primarily	O
im-	O
plicated	O
in	O
inﬂuencing	O
action	B
selection	O
,	O
and	O
the	O
ventral	O
striatum	O
is	O
thought	O
to	O
be	O
critical	O
for	O
diﬀerent	O
aspects	O
of	O
reward	O
processing	O
,	O
including	O
the	O
assignment	O
of	O
aﬀective	O
value	B
to	O
sensations	O
.	O
the	O
cerebral	O
cortex	O
,	O
along	O
with	O
other	O
structures	O
,	O
sends	O
input	O
to	O
the	O
striatum	O
conveying	O
information	O
about	O
stimuli	O
,	O
internal	O
states	O
,	O
and	O
motor	O
activity	O
.	O
in	O
this	O
hypothetical	O
actor–critic	B
brain	O
implementation	O
,	O
the	O
ventral	O
striatum	O
sends	O
value	B
information	O
to	O
the	O
vta	O
and	O
snpc	O
,	O
where	O
dopamine	B
neurons	O
in	O
these	O
nuclei	O
combine	O
it	O
with	O
information	O
about	O
reward	O
to	O
generate	O
activity	O
corresponding	O
to	O
td	O
errors	O
(	O
though	O
exactly	O
how	O
dopaminergic	O
neurons	O
calculate	O
these	O
errors	O
is	O
not	O
yet	O
understood	O
)	O
.	O
the	O
‘	O
td	O
error	O
δ	O
’	O
line	O
in	O
figure	O
15.5a	O
becomes	O
the	O
line	O
labeled	O
‘	O
dopamine	B
’	O
in	O
figure	O
15.5b	O
,	O
which	O
represents	O
the	O
widely	O
branching	O
axons	O
of	O
dopamine	O
neurons	O
whose	O
cell	O
bodies	O
are	O
in	O
the	O
vta	O
and	O
snpc	O
.	O
referring	O
back	O
to	O
figure	O
15.1	O
,	O
these	O
axons	O
make	O
synaptic	O
contact	O
with	O
the	O
spines	O
on	O
the	O
dendrites	O
of	O
medium	O
spiny	O
neurons	O
,	O
the	O
main	O
input/output	O
neurons	O
of	O
both	O
the	O
dorsal	O
and	O
ventral	O
divisions	O
of	O
the	O
striatum	O
.	O
axons	O
of	O
the	O
cortical	O
neurons	O
that	O
send	O
input	O
to	O
the	O
striatum	O
make	O
synaptic	O
contact	O
on	O
the	O
tips	O
of	O
these	O
spines	O
.	O
according	O
to	O
the	O
hypothesis	O
,	O
it	O
is	O
at	O
these	O
spines	O
where	O
changes	O
in	O
the	O
eﬃcacies	O
of	O
the	O
synapses	O
from	O
cortical	O
regions	O
to	O
the	O
stratum	O
are	O
governed	O
by	O
learning	O
rules	O
that	O
critically	O
depend	O
on	O
a	O
reinforcement	B
signal	I
supplied	O
by	O
dopamine	B
.	O
an	O
important	O
implication	O
of	O
the	O
hypothesis	O
illustrated	O
in	O
figure	O
15.5b	O
is	O
that	O
the	O
dopamine	B
signal	O
is	O
not	O
the	O
‘	O
master	O
’	O
reward	B
signal	I
like	O
the	O
scalar	O
rt	O
of	O
reinforcement	O
learning	O
.	O
in	O
fact	O
,	O
the	O
hypothesis	O
implies	O
that	O
one	O
should	O
not	O
necessarily	O
be	O
able	O
to	O
probe	O
the	O
brain	O
and	O
record	O
any	O
signal	O
like	O
rt	O
in	O
the	O
activity	O
of	O
any	O
single	O
neuron	O
.	O
many	O
inter-	O
402	O
chapter	O
15	O
:	O
neuroscience	B
connected	O
neural	B
systems	O
generate	O
reward-related	O
information	O
,	O
with	O
diﬀerent	O
structures	O
being	O
recruited	O
depending	O
on	O
diﬀerent	O
types	O
of	O
rewards	O
.	O
dopamine	B
neurons	O
receive	O
in-	O
formation	O
from	O
many	O
diﬀerent	O
brain	O
areas	O
,	O
so	O
the	O
input	O
to	O
the	O
snpc	O
and	O
vta	O
labeled	O
‘	O
reward	O
’	O
in	O
figure	O
15.5b	O
should	O
be	O
thought	O
of	O
as	O
vector	B
of	O
reward-related	O
information	O
arriving	O
to	O
neurons	O
in	O
these	O
nuclei	O
along	O
multiple	O
input	O
channels	O
.	O
what	O
the	O
theoretical	O
scalar	O
reward	B
signal	I
rt	O
might	O
correspond	O
to	O
,	O
then	O
,	O
is	O
the	O
net	O
contribution	O
of	O
all	O
reward-	O
related	O
information	O
to	O
dopamine	B
neuron	O
activity	O
.	O
it	O
is	O
the	O
result	O
of	O
a	O
pattern	O
of	O
activity	O
across	O
many	O
neurons	O
in	O
diﬀerent	O
areas	O
of	O
the	O
brain	O
.	O
although	O
the	O
actor–critic	B
neural	O
implementation	O
illustrated	O
in	O
figure	O
15.5b	O
may	O
be	O
correct	O
on	O
some	O
counts	O
,	O
it	O
clearly	O
needs	O
to	O
be	O
reﬁned	O
,	O
extended	O
,	O
and	O
modiﬁed	O
to	O
qualify	O
as	O
a	O
full-ﬂedged	O
model	O
of	O
the	O
function	O
of	O
the	O
phasic	O
activity	O
of	O
dopamine	O
neurons	O
.	O
the	O
historical	O
and	O
bibliographic	O
remarks	O
section	O
at	O
the	O
end	O
of	O
this	O
chapter	O
cites	O
publications	O
that	O
discuss	O
in	O
more	O
detail	O
both	O
empirical	O
support	O
for	O
this	O
hypothesis	O
and	O
places	O
where	O
it	O
falls	O
short	O
.	O
we	O
now	O
look	O
in	O
detail	O
at	O
what	O
the	O
actor	O
and	O
critic	O
learning	O
algorithms	O
suggest	O
about	O
the	O
rules	O
governing	O
changes	O
in	O
synaptic	O
eﬃcacies	O
of	O
corticostriatal	O
synapses	O
.	O
15.8	O
actor	O
and	O
critic	O
learning	O
rules	O
if	O
the	O
brain	O
does	O
implement	O
something	O
like	O
the	O
actor–critic	B
algorithm—and	O
assuming	O
populations	O
of	O
dopamine	O
neurons	O
broadcast	O
a	O
common	O
reinforcement	B
signal	I
to	O
the	O
corti-	O
costriatal	O
synapses	O
of	O
both	O
the	O
dorsal	O
and	O
ventral	O
striatum	O
as	O
illustrated	O
in	O
figure	O
15.5b	O
(	O
which	O
is	O
likely	O
an	O
oversimpliﬁcation	O
as	O
we	O
mentioned	O
above	O
)	O
—then	O
this	O
reinforcement	B
signal	I
aﬀects	O
the	O
synapses	O
of	O
these	O
two	O
structures	O
in	O
diﬀerent	O
ways	O
.	O
the	O
learning	O
rules	O
for	O
the	O
critic	B
and	O
the	O
actor	O
use	O
the	O
same	O
reinforcement	B
signal	I
,	O
the	O
td	O
error	O
δ	O
,	O
but	O
its	O
eﬀect	O
on	O
learning	O
is	O
diﬀerent	O
for	O
these	O
two	O
components	O
.	O
the	O
td	O
error	O
(	O
combined	O
with	B
eligibility	I
traces	I
)	O
tells	O
the	O
actor	O
how	O
to	O
update	O
action	B
probabilities	O
in	O
order	O
to	O
reach	O
higher-valued	O
states	O
.	O
learning	O
by	O
the	O
actor	O
is	O
like	O
instrumental	B
conditioning	I
using	O
a	O
law-of-eﬀect-type	O
learning	O
rule	O
(	O
section	O
1.7	O
)	O
:	O
the	O
actor	O
works	O
to	O
keep	O
δ	O
as	O
positive	O
as	O
possible	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
td	O
error	O
(	O
when	O
combined	O
with	B
eligibility	I
traces	I
)	O
tells	O
the	O
critic	B
the	O
direction	O
and	O
magnitude	O
in	O
which	O
to	O
change	O
the	O
parameters	O
of	O
the	O
value	B
function	I
in	O
order	O
to	O
improve	O
its	O
predictive	O
accuracy	O
.	O
the	O
critic	B
works	O
to	O
reduce	O
δ	O
’	O
s	O
mag-	O
nitude	O
to	O
be	O
as	O
close	O
to	O
zero	O
as	O
possible	O
using	O
a	O
learning	O
rule	O
like	O
the	O
td	O
model	O
of	O
classical	O
conditioning	B
(	O
section	O
14.2	O
)	O
.	O
the	O
diﬀerence	O
between	O
the	O
critic	B
and	O
actor	O
learning	O
rules	O
is	O
relatively	O
simple	O
,	O
but	O
this	O
diﬀerence	O
has	O
a	O
profound	O
eﬀect	O
on	O
learning	O
and	O
is	O
essential	O
to	O
how	O
the	O
actor–critic	B
algorithm	O
works	O
.	O
the	O
diﬀerence	O
lies	O
solely	O
in	O
the	O
eligibility	O
traces	O
each	O
type	O
of	O
learning	O
rule	O
uses	O
.	O
more	O
than	O
one	O
set	O
of	O
learning	O
rules	O
can	O
be	O
used	O
in	O
actor–critic	O
neural	B
networks	I
like	O
those	O
in	O
figure	O
15.5b	O
but	O
,	O
to	O
be	O
speciﬁc	O
,	O
here	O
we	O
focus	O
on	O
the	O
actor–critic	B
algorithm	O
for	O
continuing	O
problems	O
with	B
eligibility	I
traces	I
presented	O
in	O
section	O
13.6.	O
on	O
each	O
transition	O
from	O
state	B
st	O
to	O
state	B
st+1	O
,	O
taking	O
action	B
at	O
and	O
receiving	O
action	B
rt+1	O
,	O
that	O
algorithm	O
computes	O
the	O
td	O
error	O
(	O
δ	O
)	O
and	O
then	O
updates	O
the	O
eligibility	O
trace	O
vectors	O
(	O
zw	O
t	O
and	O
zθ	O
t	O
)	O
15.8.	O
actor	O
and	O
critic	O
learning	O
rules	O
403	O
and	O
the	O
parameters	O
for	O
the	O
critic	B
and	O
actor	O
(	O
w	O
and	O
θ	O
)	O
,	O
according	O
to	O
t−1	O
+	O
∇ˆv	O
(	O
st	O
,	O
w	O
)	O
,	O
t−1	O
+	O
∇	O
ln	O
π	O
(	O
at|st	O
,	O
θ	O
)	O
,	O
δt	O
=	O
rt+1	O
+	O
γ	O
ˆv	O
(	O
st+1	O
,	O
w	O
)	O
−	O
ˆv	O
(	O
st	O
,	O
w	O
)	O
,	O
t	O
=	O
λwzw	O
zw	O
t	O
=	O
λθzθ	O
zθ	O
w	O
←	O
w	O
+	O
αw	O
δt	O
zw	O
t	O
,	O
θ	O
←	O
θ	O
+	O
αθ	O
δ	O
zθ	O
t	O
,	O
where	O
γ	O
∈	O
[	O
0	O
,	O
1	O
)	O
is	O
a	O
discount-rate	O
parameter	O
,	O
λwc	O
∈	O
[	O
0	O
,	O
1	O
]	O
and	O
λwa	O
∈	O
[	O
0	O
,	O
1	O
]	O
are	O
bootstrap-	O
ping	O
parameters	O
for	O
the	O
critic	B
and	O
the	O
actor	O
respectively	O
,	O
and	O
αw	O
>	O
0	O
and	O
αθ	O
>	O
0	O
are	O
analogous	O
step-size	O
parameters	O
.	O
think	O
of	O
the	O
approximate	B
value	O
function	O
ˆv	O
as	O
the	O
output	O
of	O
a	O
single	O
linear	O
neuron-like	O
unit	O
,	O
called	O
the	O
critic	B
unit	O
and	O
labeled	O
v	O
in	O
figure	O
15.5a	O
.	O
then	O
the	O
value	B
function	I
is	O
a	O
linear	O
function	O
of	O
the	O
feature-vector	O
representation	O
of	O
state	B
s	O
,	O
x	O
(	O
s	O
)	O
=	O
(	O
x1	O
(	O
s	O
)	O
,	O
.	O
.	O
.	O
,	O
xn	O
(	O
s	O
)	O
)	O
(	O
cid:62	O
)	O
,	O
parameterized	O
by	O
a	O
weight	O
vector	B
w	O
=	O
(	O
w1	O
,	O
.	O
.	O
.	O
,	O
wn	O
)	O
(	O
cid:62	O
)	O
:	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
=	O
w	O
(	O
cid:62	O
)	O
x	O
(	O
s	O
)	O
.	O
(	O
15.1	O
)	O
each	O
xi	O
(	O
s	O
)	O
is	O
like	O
the	O
presynaptic	O
signal	O
to	O
a	O
neuron	O
’	O
s	O
synapse	O
whose	O
eﬃcacy	O
is	O
wi	O
.	O
the	O
weights	O
of	O
the	O
critic	B
are	O
incremented	O
according	O
to	O
the	O
rule	O
above	O
by	O
αwδtzw	O
t	O
,	O
where	O
the	O
reinforcement	B
signal	I
,	O
δt	O
,	O
corresponds	O
to	O
a	O
dopamine	B
signal	O
being	O
broadcast	O
to	O
all	O
of	O
the	O
critic	B
unit	O
’	O
s	O
synapses	O
.	O
the	O
eligibility	O
trace	O
vector	B
,	O
zw	O
t	O
,	O
for	O
the	O
critic	B
unit	O
is	O
a	O
trace	O
(	O
average	O
of	O
recent	O
values	O
)	O
of	O
∇ˆv	O
(	O
st	O
,	O
w	O
)	O
.	O
because	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
is	O
linear	O
in	O
the	O
weights	O
,	O
∇ˆv	O
(	O
st	O
,	O
w	O
)	O
=	O
x	O
(	O
st	O
)	O
.	O
in	O
neural	O
terms	O
,	O
this	O
means	O
that	O
each	O
synapse	O
has	O
its	O
own	O
eligibility	O
trace	O
,	O
which	O
is	O
one	O
component	O
of	O
the	O
vector	B
zw	O
t	O
.	O
a	O
synapse	O
’	O
s	O
eligibility	O
trace	O
accumulates	O
according	O
to	O
the	O
level	O
of	O
activity	O
arriving	O
at	O
that	O
synapse	O
,	O
that	O
is	O
,	O
the	O
level	O
of	O
presynaptic	O
activity	O
,	O
represented	O
here	O
by	O
the	O
component	O
of	O
the	O
feature	O
vector	O
x	O
(	O
st	O
)	O
arriving	O
at	O
that	O
synapse	O
.	O
the	O
trace	O
otherwise	O
decays	O
toward	O
zero	O
at	O
a	O
rate	O
governed	O
by	O
the	O
fraction	O
λw	O
.	O
a	O
synapse	O
is	O
eligible	O
for	O
modiﬁcation	O
as	O
long	O
as	O
its	O
eligibility	O
trace	O
is	O
non-zero	O
.	O
how	O
the	O
synapse	O
’	O
s	O
eﬃcacy	O
is	O
actually	O
modiﬁed	O
depends	O
on	O
the	O
reinforcement	O
signals	O
that	O
arrive	O
while	O
the	O
synapse	O
is	O
eligible	O
.	O
we	O
call	O
eligibility	B
traces	I
like	O
these	O
of	O
the	O
critic	B
unit	O
’	O
s	O
synapses	O
non-	O
contingent	O
eligibility	O
traces	O
because	O
they	O
only	O
depend	O
on	O
presynaptic	O
activity	O
and	O
are	O
not	O
contingent	O
in	O
any	O
way	O
on	O
postsynaptic	O
activity	O
.	O
the	O
non-contingent	O
eligibility	B
traces	I
of	O
the	O
critic	B
unit	O
’	O
s	O
synapses	O
mean	O
that	O
the	O
critic	B
unit	O
’	O
s	O
learning	O
rule	O
is	O
essentially	O
the	O
td	O
model	O
of	O
classical	O
conditioning	B
described	O
in	O
section	O
14.2.	O
with	O
the	O
deﬁnition	O
we	O
have	O
given	O
above	O
of	O
the	O
critic	B
unit	O
and	O
its	O
learning	O
rule	O
,	O
the	O
critic	B
in	O
figure	O
15.5a	O
is	O
the	O
same	O
as	O
the	O
critic	B
in	O
the	O
neural	B
network	O
actor–critic	B
of	O
barto	O
et	O
al	O
.	O
(	O
1983	O
)	O
.	O
clearly	O
,	O
a	O
critic	B
like	O
this	O
consisting	O
of	O
just	O
one	O
linear	O
neuron-like	O
unit	O
is	O
the	O
simplest	O
starting	O
point	O
;	O
this	O
critic	B
unit	O
is	O
a	O
proxy	O
for	O
a	O
more	O
complicated	O
neural	B
network	O
able	O
to	O
learn	O
value	B
functions	O
of	O
greater	O
complexity	O
.	O
the	O
actor	O
in	O
figure	O
15.5a	O
is	O
a	O
one-layer	O
network	O
of	O
k	O
neuron-like	O
actor	O
units	O
,	O
each	O
receiving	O
at	O
time	O
t	O
the	O
same	O
feature	O
vector	O
,	O
x	O
(	O
st	O
)	O
,	O
that	O
the	O
critic	B
unit	O
receives	O
.	O
each	O
actor	O
unit	O
j	O
,	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
,	O
has	O
its	O
own	O
weight	O
vector	B
,	O
θj	O
,	O
but	O
because	O
the	O
actor	O
units	O
404	O
chapter	O
15	O
:	O
neuroscience	B
are	O
all	O
identical	O
,	O
we	O
describe	O
just	O
one	O
of	O
the	O
units	O
and	O
omit	O
the	O
subscript	O
.	O
one	O
way	O
for	O
these	O
units	O
to	O
follow	O
the	O
actor–critic	B
algorithm	O
given	O
in	O
the	O
equations	O
above	O
is	O
for	O
each	O
to	O
be	O
a	O
bernoulli-logistic	O
unit	O
.	O
this	O
means	O
that	O
the	O
output	O
of	O
each	O
actor	O
unit	O
at	O
each	O
time	O
is	O
a	O
random	O
variable	O
,	O
at	O
,	O
taking	O
value	B
0	O
or	O
1.	O
think	O
of	O
value	O
1	O
as	O
the	O
neuron	O
ﬁring	O
,	O
that	O
is	O
,	O
emitting	O
an	O
action	B
potential	O
.	O
the	O
weighted	O
sum	O
,	O
θ	O
(	O
cid:62	O
)	O
x	O
(	O
st	O
)	O
,	O
of	O
a	O
unit	O
’	O
s	O
input	O
vector	B
determines	O
the	O
unit	O
’	O
s	O
action	B
probabilities	O
via	O
the	O
exponential	O
soft-max	B
distribution	O
(	O
13.2	O
)	O
,	O
which	O
for	O
two	O
actions	O
is	O
the	O
logistic	O
function	O
:	O
π	O
(	O
1|s	O
,	O
θ	O
)	O
=	O
1	O
−	O
π	O
(	O
0|s	O
,	O
θ	O
)	O
=	O
1	O
1	O
+	O
exp	O
(	O
−θ	O
(	O
cid:62	O
)	O
x	O
(	O
s	O
)	O
)	O
.	O
(	O
15.2	O
)	O
the	O
weights	O
of	O
each	O
actor	O
unit	O
are	O
incremented	O
,	O
as	O
above	O
,	O
by	O
:	O
θ	O
←	O
θ	O
+	O
αθ	O
δt	O
zθ	O
t	O
,	O
where	O
δ	O
again	O
corresponds	O
to	O
the	O
dopamine	B
signal	O
:	O
the	O
same	O
reinforcement	B
signal	I
that	O
is	O
sent	O
to	O
all	O
the	O
critic	B
unit	O
’	O
s	O
synapses	O
.	O
figure	O
15.5a	O
shows	O
δt	O
being	O
broadcast	O
to	O
all	O
the	O
synapses	O
of	O
all	O
the	O
actor	O
units	O
(	O
which	O
makes	O
this	O
actor	O
network	O
a	O
team	O
of	O
reinforcement	O
learning	O
agents	O
,	O
something	O
we	O
discuss	O
in	O
section	O
15.10	O
below	O
)	O
.	O
the	O
actor	O
eligibility	O
trace	O
vector	B
zθ	O
is	O
a	O
trace	O
(	O
average	O
of	O
recent	O
values	O
)	O
of	O
∇	O
ln	O
π	O
(	O
at|st	O
,	O
θ	O
)	O
.	O
to	O
understand	O
this	O
t	O
eligibility	O
trace	O
refer	O
to	O
exercise	O
13.5	O
,	O
which	O
deﬁnes	O
this	O
kind	O
of	O
unit	O
and	O
asks	O
you	O
to	O
give	O
a	O
learning	O
rule	O
for	O
it	O
.	O
that	O
exercise	O
asked	O
you	O
to	O
express	O
∇	O
ln	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
in	O
terms	O
of	O
a	O
,	O
x	O
(	O
s	O
)	O
,	O
and	O
π	O
(	O
a|s	O
,	O
θ	O
)	O
(	O
for	O
arbitrary	O
state	B
s	O
and	O
action	O
a	O
)	O
by	O
calculating	O
the	O
gradient	B
.	O
for	O
the	O
action	B
and	O
state	B
actually	O
occurring	O
at	O
time	O
t	O
,	O
the	O
answer	O
is	O
∇π	O
(	O
at|st	O
,	O
θ	O
)	O
=	O
(	O
cid:0	O
)	O
at	O
−	O
π	O
(	O
at|st	O
,	O
θ	O
)	O
(	O
cid:1	O
)	O
x	O
(	O
st	O
)	O
.	O
(	O
15.3	O
)	O
unlike	O
the	O
non-contingent	O
eligibility	O
trace	O
of	O
a	O
critic	B
synapse	O
that	O
only	O
accumulates	O
the	O
presynaptic	O
activity	O
x	O
(	O
st	O
)	O
,	O
the	O
eligibility	O
trace	O
of	O
an	O
actor	O
unit	O
’	O
s	O
synapse	O
in	O
addition	O
depends	O
on	O
the	O
activity	O
of	O
the	O
actor	O
unit	O
itself	O
.	O
we	O
call	O
this	O
a	O
contingent	O
eligibility	O
trace	O
because	O
it	O
is	O
contingent	O
on	O
this	O
postsynaptic	O
activity	O
.	O
the	O
eligibility	O
trace	O
at	O
each	O
synapse	O
continually	O
decays	O
,	O
but	O
increments	O
or	O
decrements	O
depending	O
on	O
the	O
activity	O
of	O
the	O
presynaptic	O
neuron	O
and	O
whether	O
or	O
not	O
the	O
postsynaptic	O
neuron	O
ﬁres	O
.	O
the	O
factor	O
at−	O
π	O
(	O
at|st	O
,	O
θ	O
)	O
in	O
(	O
15.3	O
)	O
is	O
positive	O
when	O
at	O
=	O
1	O
and	O
negative	O
otherwise	O
.	O
the	O
postsynaptic	O
contingency	O
in	O
the	O
eligibility	O
traces	O
of	O
actor	O
units	O
is	O
the	O
only	O
diﬀerence	O
between	O
the	O
critic	B
and	O
actor	O
learning	O
rules	O
.	O
by	O
keeping	O
information	O
about	O
what	O
actions	O
were	O
taken	O
in	O
what	O
states	O
,	O
contingent	O
eligibility	O
traces	O
allow	O
credit	O
for	O
reward	O
(	O
positive	O
δ	O
)	O
,	O
or	O
blame	O
for	O
punishment	O
(	O
negative	O
δ	O
)	O
,	O
to	O
be	O
apportioned	O
among	O
the	O
policy	B
parameters	O
(	O
the	O
eﬃcacies	O
of	O
the	O
actor	O
units	O
’	O
synapses	O
)	O
according	O
to	O
the	O
contributions	O
these	O
parameters	O
made	O
to	O
the	O
units	O
’	O
outputs	O
that	O
could	O
have	O
inﬂuenced	O
later	O
values	O
of	O
δ.	O
contingent	O
eligibility	O
traces	O
mark	O
the	O
synapses	O
as	O
to	O
how	O
they	O
should	O
be	O
modiﬁed	O
to	O
alter	O
the	O
units	O
’	O
future	O
responses	O
to	O
favor	O
positive	O
values	O
of	O
δ.	O
what	O
do	O
the	O
critic	B
and	O
actor	O
learning	O
rules	O
suggest	O
about	O
how	O
eﬃcacies	O
of	O
corticostri-	O
atal	O
synapses	O
change	O
?	O
both	O
learning	O
rules	O
are	O
related	O
to	O
donald	O
hebb	O
’	O
s	O
classic	O
proposal	O
that	O
whenever	O
a	O
presynaptic	O
signal	O
participates	O
in	O
activating	O
the	O
postsynaptic	O
neuron	O
,	O
the	O
synapse	O
’	O
s	O
eﬃcacy	O
increases	O
(	O
hebb	O
,	O
1949	O
)	O
.	O
the	O
critic	B
and	O
actor	O
learning	O
rules	O
share	O
with	O
hebb	O
’	O
s	O
proposal	O
the	O
idea	O
that	O
changes	O
in	O
a	O
synapse	O
’	O
s	O
eﬃcacy	O
depend	O
on	O
the	O
interaction	O
of	O
several	O
factors	O
.	O
in	O
the	O
critic	O
learning	O
rule	O
the	O
interaction	O
is	O
between	O
the	O
reinforcement	B
signal	I
δ	O
and	B
eligibility	I
traces	I
that	O
depend	O
only	O
on	O
presynaptic	O
signals	O
.	O
neuroscientists	O
15.8.	O
actor	O
and	O
critic	O
learning	O
rules	O
405	O
call	O
this	O
a	O
two-factor	O
learning	O
rule	O
because	O
the	O
interaction	O
is	O
between	O
two	O
signals	O
or	O
quantities	O
.	O
the	O
actor	O
learning	O
rule	O
,	O
on	O
the	O
other	O
hand	O
,	O
is	O
a	O
three-factor	O
learning	O
rule	O
because	O
,	O
in	O
addition	O
to	O
depending	O
on	O
δ	O
,	O
its	O
eligibility	B
traces	I
depend	O
on	O
both	O
presynaptic	O
and	O
postsynaptic	O
activity	O
.	O
unlike	O
hebb	O
’	O
s	O
proposal	O
,	O
however	O
,	O
the	O
relative	O
timing	O
of	O
the	O
factors	O
is	O
critical	O
to	O
how	O
synaptic	O
eﬃcacies	O
change	O
,	O
with	B
eligibility	I
traces	I
intervening	O
to	O
allow	O
the	O
reinforcement	B
signal	I
to	O
aﬀect	O
synapses	O
that	O
were	O
active	O
in	O
the	O
recent	O
past	O
.	O
some	O
subtleties	O
about	O
signal	O
timing	O
for	O
the	O
actor	O
and	O
critic	O
learning	O
rules	O
deserve	O
closer	O
attention	O
.	O
in	O
deﬁning	O
the	O
neuron-like	O
actor	O
and	O
critic	O
units	O
,	O
we	O
ignored	O
the	O
small	O
amount	O
of	O
time	O
it	O
takes	O
synaptic	O
input	O
to	O
eﬀect	O
the	O
ﬁring	O
of	O
a	O
real	O
neuron	O
.	O
when	O
an	O
action	B
potential	O
from	O
the	O
presynaptic	O
neuron	O
arrives	O
at	O
a	O
synapse	O
,	O
neurotransmitter	O
molecules	O
are	O
released	O
that	O
diﬀuse	O
across	O
the	O
synaptic	O
cleft	O
to	O
the	O
postsynaptic	O
neuron	O
,	O
where	O
they	O
bind	O
to	O
receptors	O
on	O
the	O
postsynaptic	O
neuron	O
’	O
s	O
surface	O
;	O
this	O
activates	O
molec-	O
ular	O
machinery	O
that	O
causes	O
the	O
postsynaptic	O
neuron	O
to	O
ﬁre	O
(	O
or	O
to	O
inhibit	O
its	O
ﬁring	O
in	O
the	O
case	O
of	O
inhibitory	O
synaptic	O
input	O
)	O
.	O
this	O
process	O
can	O
take	O
several	O
tens	O
of	O
milliseconds	O
.	O
according	O
to	O
(	O
15.1	O
)	O
and	O
(	O
15.2	O
)	O
,	O
though	O
,	O
the	O
input	O
to	O
a	O
critic	B
and	O
actor	O
unit	O
instanta-	O
neously	O
produces	O
the	O
unit	O
’	O
s	O
output	O
.	O
ignoring	O
activation	O
time	O
like	O
this	O
is	O
common	O
in	O
abstract	O
models	O
of	O
hebbian-style	O
plasticity	O
in	O
which	O
synaptic	O
eﬃcacies	O
change	O
according	O
to	O
a	O
simple	O
product	O
of	O
simultaneous	O
pre-	O
and	O
postsynaptic	O
activity	O
.	O
more	O
realistic	O
models	O
must	O
take	O
activation	O
time	O
into	O
account	O
.	O
activation	O
time	O
is	O
especially	O
important	O
for	O
a	O
more	O
realistic	O
actor	O
unit	O
because	O
it	O
inﬂu-	O
ences	O
how	O
contingent	O
eligibility	O
traces	O
have	O
to	O
work	O
in	O
order	O
to	O
properly	O
apportion	O
credit	O
deﬁning	O
contingent	O
eligibility	O
traces	O
for	O
the	O
actor	O
unit	O
’	O
s	O
learning	O
rule	O
given	O
above	O
in-	O
works	O
because	O
by	O
ignoring	O
activation	O
time	O
,	O
the	O
presynaptic	O
activity	O
x	O
(	O
st	O
)	O
participates	O
for	O
reinforcement	O
to	O
the	O
appropriate	O
synapses	O
.	O
the	O
expression	O
(	O
cid:0	O
)	O
at	O
−	O
π	O
(	O
at|st	O
,	O
θ	O
)	O
(	O
cid:1	O
)	O
x	O
(	O
st	O
)	O
cludes	O
the	O
postsynaptic	O
factor	O
(	O
cid:0	O
)	O
at	O
−	O
π	O
(	O
at|st	O
,	O
θ	O
)	O
(	O
cid:1	O
)	O
and	O
the	O
presynaptic	O
factor	O
x	O
(	O
st	O
)	O
.	O
this	O
in	O
causing	O
the	O
postsynaptic	O
activity	O
appearing	O
in	O
(	O
cid:0	O
)	O
at	O
−	O
π	O
(	O
at|st	O
,	O
θ	O
)	O
(	O
cid:1	O
)	O
.	O
to	O
assign	O
credit	O
for	O
reinforcement	O
correctly	O
,	O
the	O
presynaptic	O
factor	O
deﬁning	O
the	O
eligibility	O
trace	O
must	O
be	O
a	O
cause	O
of	O
the	O
postsynaptic	O
factor	O
that	O
also	O
deﬁnes	O
the	O
trace	O
.	O
contingent	O
eligibility	O
traces	O
for	O
a	O
more	O
realistic	O
actor	O
unit	O
would	O
have	O
to	O
take	O
activation	O
time	O
into	O
account	O
.	O
(	O
activation	O
time	O
should	O
not	O
be	O
confused	O
with	O
the	O
time	O
required	O
for	O
a	O
neuron	O
to	O
receive	O
a	O
reinforcement	B
signal	I
inﬂuenced	O
by	O
that	O
neuron	O
’	O
s	O
activity	O
.	O
the	O
function	O
of	O
eligibility	B
traces	I
is	O
to	O
span	O
this	O
time	O
interval	O
which	O
is	O
generally	O
much	O
longer	O
than	O
the	O
activation	O
time	O
.	O
we	O
discuss	O
this	O
further	O
in	O
the	O
following	O
section	O
.	O
)	O
there	O
are	O
hints	O
from	O
neuroscience	B
for	O
how	O
this	O
process	O
might	O
work	O
in	O
the	O
brain	O
.	O
neu-	O
roscientists	O
have	O
discovered	O
a	O
form	O
of	O
hebbian	O
plasticity	O
called	O
spike-timing-dependent	O
plasticity	O
(	O
stdp	O
)	O
that	O
lends	O
plausibility	O
to	O
the	O
existence	O
of	O
actor-like	O
synaptic	B
plasticity	I
in	O
the	O
brain	O
.	O
stdp	O
is	O
a	O
hebbian-style	O
plasticity	O
,	O
but	O
changes	O
in	O
a	O
synapse	O
’	O
s	O
eﬃcacy	O
depend	O
on	O
the	O
relative	O
timing	O
of	O
presynaptic	O
and	O
postsynaptic	O
action	B
potentials	O
.	O
the	O
dependence	O
can	O
take	O
diﬀerent	O
forms	O
,	O
but	O
in	O
the	O
one	O
most	O
studied	O
,	O
a	O
synapse	O
increases	O
in	O
strength	O
if	O
spikes	O
incoming	O
via	O
that	O
synapse	O
arrive	O
shortly	O
before	O
the	O
postsynaptic	O
neuron	O
ﬁres	O
.	O
if	O
the	O
timing	O
relation	O
is	O
reversed	O
,	O
with	O
a	O
presynaptic	O
spike	O
arriving	O
shortly	O
after	O
the	O
postsynaptic	O
neuron	O
ﬁres	O
,	O
then	O
the	O
strength	O
of	O
the	O
synapse	O
decreases	O
.	O
stdp	O
is	O
a	O
type	O
of	O
hebbian	O
plasticity	O
that	O
takes	O
the	O
activation	O
time	O
of	O
a	O
neuron	O
into	O
account	O
,	O
which	O
is	O
one	O
of	O
the	O
ingredients	O
needed	O
for	O
actor-like	O
learning	O
.	O
406	O
chapter	O
15	O
:	O
neuroscience	B
the	O
discovery	O
of	O
stdp	O
has	O
led	O
neuroscientists	O
to	O
investigate	O
the	O
possibility	O
of	O
a	O
three-	O
factor	O
form	O
of	O
stdp	O
in	O
which	O
neuromodulatory	O
input	O
must	O
follow	O
appropriately-timed	O
pre-	O
and	O
postsynaptic	O
spikes	O
.	O
this	O
form	O
of	O
synaptic	O
plasticity	O
,	O
called	O
reward-modulated	O
stdp	O
,	O
is	O
much	O
like	O
the	O
actor	O
learning	O
rule	O
discussed	O
here	O
.	O
synaptic	O
changes	O
that	O
would	O
be	O
produced	O
by	O
regular	O
stdp	O
only	O
occur	O
if	O
there	O
is	O
neuromodulatory	O
input	O
within	O
a	O
time	O
window	O
after	O
a	O
presynaptic	O
spike	O
is	O
closely	O
followed	O
by	O
a	O
postsynaptic	O
spike	O
.	O
ev-	O
idence	O
is	O
accumulating	B
that	O
reward-modulated	O
stdp	O
occurs	O
at	O
the	O
spines	O
of	O
medium	O
spiny	O
neurons	O
of	O
the	O
dorsal	O
striatum	O
,	O
with	O
dopamine	O
providing	O
the	O
neuromodulatory	O
factor—the	O
sites	O
where	O
actor	O
learning	O
takes	O
place	O
in	O
the	O
hypothetical	O
neural	B
imple-	O
mentation	O
of	O
an	O
actor–critic	B
algorithm	O
illustrated	O
in	O
figure	O
15.5b	O
.	O
experiments	O
have	O
demonstrated	O
reward-modulated	O
stdp	O
in	O
which	O
lasting	O
changes	O
in	O
the	O
eﬃcacies	O
of	O
cor-	O
ticostriatal	O
synapses	O
occur	O
only	O
if	O
a	O
neuromodulatory	O
pulse	O
arrives	O
within	O
a	O
time	O
window	O
that	O
can	O
last	O
up	O
to	O
10	O
seconds	O
after	O
a	O
presynaptic	O
spike	O
is	O
closely	O
followed	O
by	O
a	O
postsy-	O
naptic	O
spike	O
(	O
yagishita	O
et	O
al	O
.	O
2014	O
)	O
.	O
although	O
the	O
evidence	O
is	O
indirect	O
,	O
these	O
experiments	O
point	O
to	O
the	O
existence	O
of	O
contingent	O
eligibility	B
traces	I
having	O
prolonged	O
time	O
courses	O
.	O
the	O
molecular	O
mechanisms	O
producing	O
these	O
traces	O
,	O
as	O
well	O
as	O
the	O
much	O
shorter	O
traces	O
that	O
likely	O
underly	O
stdp	O
,	O
are	O
not	O
yet	O
understood	O
,	O
but	O
research	O
focusing	O
on	O
time-dependent	O
and	O
neuromodulator-dependent	O
synaptic	B
plasticity	I
is	O
continuing	O
.	O
the	O
neuron-like	O
actor	O
unit	O
that	O
we	O
have	O
described	O
here	O
,	O
with	O
its	O
law-of-eﬀect-style	O
learning	O
rule	O
,	O
appeared	O
in	O
somewhat	O
simpler	O
form	O
in	O
the	O
actor–critic	O
network	O
of	O
barto	O
et	O
al	O
.	O
(	O
1983	O
)	O
.	O
that	O
network	O
was	O
inspired	O
by	O
the	O
“	O
hedonistic	O
neuron	O
”	O
hypothesis	O
proposed	O
by	O
physiologist	O
a.	O
h.	O
klopf	O
(	O
1972	O
,	O
1982	O
)	O
.	O
not	O
all	O
the	O
details	O
of	O
klopf	O
’	O
s	O
hypothesis	O
are	O
consistent	O
with	O
what	O
has	O
been	O
learned	O
about	O
synaptic	B
plasticity	I
,	O
but	O
the	O
discovery	O
of	O
stdp	O
and	O
the	O
growing	O
evidence	O
for	O
a	O
reward-modulated	O
form	O
of	O
stdp	O
suggest	O
that	O
klopf	O
’	O
s	O
ideas	O
may	O
not	O
have	O
been	O
far	O
oﬀ	O
the	O
mark	O
.	O
we	O
discuss	O
klopf	O
’	O
s	O
hedonistic	O
neuron	O
hypothesis	O
next	O
.	O
15.9	O
hedonistic	B
neurons	I
in	O
his	O
hedonistic	O
neuron	O
hypothesis	O
,	O
klopf	O
(	O
1972	O
,	O
1982	O
)	O
conjectured	O
that	O
individual	O
neu-	O
rons	O
seek	O
to	O
maximize	O
the	O
diﬀerence	O
between	O
synaptic	O
input	O
treated	O
as	O
rewarding	O
and	O
synaptic	O
input	O
treated	O
as	O
punishing	O
by	O
adjusting	O
the	O
eﬃcacies	O
of	O
their	O
synapses	O
on	O
the	O
basis	O
of	O
rewarding	O
or	O
punishing	O
consequences	O
of	O
their	O
own	O
action	B
potentials	O
.	O
in	O
other	O
words	O
,	O
individual	O
neurons	O
can	O
be	O
trained	O
with	O
response-contingent	O
reinforcement	O
like	O
an	O
animal	O
can	O
be	O
trained	O
in	O
an	O
instrumental	B
conditioning	I
task	O
.	O
his	O
hypothesis	O
included	O
the	O
idea	O
that	O
rewards	O
and	O
punishments	O
are	O
conveyed	O
to	O
a	O
neuron	O
via	O
the	O
same	O
synaptic	O
input	O
that	O
excites	O
or	O
inhibits	O
the	O
neuron	O
’	O
s	O
spike-generating	O
activity	O
.	O
(	O
had	O
klopf	O
known	O
what	O
we	O
know	O
today	O
about	O
neuromodulatory	O
systems	O
,	O
he	O
might	O
have	O
assigned	O
the	O
re-	O
inforcing	O
role	O
to	O
neuromodulatory	O
input	O
,	O
but	O
he	O
wanted	O
to	O
avoid	O
any	O
centralized	O
source	O
of	O
training	O
information	O
.	O
)	O
synaptically-local	O
traces	O
of	O
past	O
pre-	O
and	O
postsynaptic	O
activity	O
had	O
the	O
key	O
function	O
in	O
klopf	O
’	O
s	O
hypothesis	O
of	O
making	O
synapses	O
eligible—the	O
term	O
he	O
introduced—for	O
modiﬁcation	O
by	O
later	O
reward	O
or	O
punishment	O
.	O
he	O
conjectured	O
that	O
these	O
traces	O
are	O
implemented	O
by	O
molecular	O
mechanisms	O
local	O
to	O
each	O
synapse	O
and	O
therefore	O
diﬀerent	O
from	O
the	O
electrical	O
activity	O
of	O
both	O
the	O
pre-	O
and	O
the	O
postsynaptic	O
neurons	O
.	O
in	O
15.9.	O
hedonistic	B
neurons	I
407	O
the	O
bibliographical	O
and	O
historical	O
remarks	O
section	O
of	O
this	O
chapter	O
we	O
bring	O
attention	O
to	O
some	O
similar	O
proposals	O
made	O
by	O
others	O
.	O
klopf	O
speciﬁcally	O
conjectured	O
that	O
synaptic	O
eﬃcacies	O
change	O
in	O
the	O
following	O
way	O
.	O
when	O
a	O
neuron	O
ﬁres	O
an	O
action	B
potential	O
,	O
all	O
of	O
its	O
synapses	O
that	O
were	O
active	O
in	O
con-	O
tributing	O
to	O
that	O
action	B
potential	O
become	O
eligible	O
to	O
undergo	O
changes	O
in	O
their	O
eﬃcacies	O
.	O
if	O
the	O
action	B
potential	O
is	O
followed	O
within	O
an	O
appropriate	O
time	O
period	O
by	O
an	O
increase	O
of	O
reward	O
,	O
the	O
eﬃcacies	O
of	O
all	O
the	O
eligible	O
synapses	O
increase	O
.	O
symmetrically	O
,	O
if	O
the	O
action	B
potential	O
is	O
followed	O
within	O
an	O
appropriate	O
time	O
period	O
by	O
an	O
increase	O
of	O
punishment	O
,	O
the	O
eﬃcacies	O
of	O
eligible	O
synapses	O
decrease	O
.	O
this	O
is	O
implemented	O
by	O
triggering	O
an	O
eligibility	O
trace	O
at	O
a	O
synapse	O
upon	O
a	O
coincidence	O
of	O
presynaptic	O
and	O
postsynaptic	O
activity	O
(	O
or	O
more	O
exactly	O
,	O
upon	O
pairing	O
of	O
presynaptic	O
activity	O
with	O
the	O
postsynaptic	O
activity	O
that	O
that	O
presynaptic	O
activity	O
participates	O
in	O
causing	O
)	O
—what	O
we	O
call	O
a	O
contingent	O
eligibility	O
trace	O
.	O
this	O
is	O
essentially	O
the	O
three-factor	O
learning	O
rule	O
of	O
an	O
actor	O
unit	O
described	O
in	O
the	O
previous	O
section	O
.	O
the	O
shape	O
and	O
time	O
course	O
of	O
an	O
eligibility	O
trace	O
in	O
klopf	O
’	O
s	O
theory	O
reﬂects	O
the	O
dura-	O
tions	O
of	O
the	O
many	O
feedback	O
loops	O
in	O
which	O
the	O
neuron	O
is	O
embedded	O
,	O
some	O
of	O
which	O
lie	O
entirely	O
within	O
the	O
brain	O
and	O
body	O
of	O
the	O
organism	O
,	O
while	O
others	O
extend	O
out	O
through	O
the	O
organism	O
’	O
s	O
external	O
environment	B
as	O
mediated	O
by	O
its	O
motor	O
and	O
sensory	O
systems	O
.	O
his	O
idea	O
was	O
that	O
the	O
shape	O
of	O
a	O
synaptic	O
eligibility	O
trace	O
is	O
like	O
a	O
histogram	O
of	O
the	O
durations	O
of	O
the	O
feedback	O
loops	O
in	O
which	O
the	O
neuron	O
is	O
embedded	O
.	O
the	O
peak	O
of	O
an	O
eligibility	O
trace	O
would	O
then	O
occur	O
at	O
the	O
duration	O
of	O
the	O
most	O
prevalent	O
feedback	O
loops	O
in	O
which	O
that	O
neuron	O
participates	O
.	O
the	O
eligibility	B
traces	I
used	O
by	O
algorithms	O
described	O
in	O
this	O
book	O
are	O
simpli-	O
ﬁed	O
versions	O
of	O
klopf	O
’	O
s	O
original	O
idea	O
,	O
being	O
exponentially	O
(	O
or	O
geometrically	O
)	O
decreasing	O
functions	O
controlled	O
by	O
the	O
parameters	O
λ	O
and	O
γ.	O
this	O
simpliﬁes	O
simulations	O
as	O
well	O
as	O
theory	O
,	O
but	O
we	O
regard	O
these	O
simple	O
eligibility	B
traces	I
as	O
a	O
placeholders	O
for	O
traces	O
closer	O
to	O
klopf	O
’	O
s	O
original	O
conception	O
,	O
which	O
would	O
have	O
computational	O
advantages	O
in	O
complex	O
reinforcement	B
learning	I
systems	O
by	O
reﬁning	O
the	O
credit-assignment	O
process	O
.	O
klopf	O
’	O
s	O
hedonistic	O
neuron	O
hypothesis	O
is	O
not	O
as	O
implausible	O
as	O
it	O
may	O
at	O
ﬁrst	O
appear	O
.	O
a	O
well-studied	O
example	O
of	O
a	O
single	O
cell	O
that	O
seeks	O
some	O
stimuli	O
and	O
avoids	O
others	O
is	O
the	O
bacterium	O
escherichia	O
coli	O
.	O
the	O
movement	O
of	O
this	O
single-cell	O
organism	O
is	O
inﬂuenced	O
by	O
chemical	O
stimuli	O
in	O
its	O
environment	B
,	O
behavior	O
known	O
as	O
chemotaxis	O
.	O
it	O
swims	O
in	O
its	O
liquid	O
environment	B
by	O
rotating	O
hairlike	O
structures	O
called	O
ﬂagella	O
attached	O
to	O
its	O
surface	O
.	O
(	O
yes	O
,	O
it	O
rotates	O
them	O
!	O
)	O
molecules	O
in	O
the	O
bacterium	O
’	O
s	O
environment	B
bind	O
to	O
receptors	O
on	O
its	O
sur-	O
face	O
.	O
binding	O
events	O
modulate	O
the	O
frequency	O
with	O
which	O
the	O
bacterium	O
reverses	O
ﬂagellar	O
rotation	O
.	O
each	O
reversal	O
causes	O
the	O
bacterium	O
to	O
tumble	O
in	O
place	O
and	O
then	O
head	O
oﬀ	O
in	O
a	O
random	O
new	O
direction	O
.	O
a	O
little	O
chemical	O
memory	O
and	O
computation	O
causes	O
the	O
frequency	O
of	O
ﬂagellar	O
reversal	O
to	O
decrease	O
when	O
the	O
bacterium	O
swims	O
toward	O
higher	O
concentrations	O
of	O
molecules	O
it	O
needs	O
to	O
survive	O
(	O
attractants	O
)	O
and	O
increase	O
when	O
the	O
bacterium	O
swims	O
toward	O
higher	O
concentrations	O
of	O
molecules	O
that	O
are	O
harmful	O
(	O
repellants	O
)	O
.	O
the	O
result	O
is	O
that	O
the	O
bacterium	O
tends	O
to	O
persist	O
in	O
swimming	O
up	O
attractant	O
gradients	O
and	O
tends	O
to	O
avoid	O
swimming	O
up	O
repellant	O
gradients	O
.	O
the	O
chemotactic	O
behavior	O
just	O
described	O
is	O
called	O
klinokinesis	O
.	O
it	O
is	O
a	O
kind	O
of	O
trial-	O
and-error	O
behavior	O
,	O
although	O
it	O
is	O
unlikely	O
that	O
learning	O
is	O
involved	O
:	O
the	O
bacterium	O
needs	O
a	O
modicum	O
of	O
short-term	O
memory	O
to	O
detect	O
molecular	O
concentration	O
gradients	O
,	O
but	O
it	O
408	O
chapter	O
15	O
:	O
neuroscience	B
probably	O
does	O
not	O
maintain	O
long-term	O
memories	O
.	O
artiﬁcial	B
intelligence	I
pioneer	O
oliver	O
selfridge	O
called	O
this	O
strategy	O
“	O
run	O
and	O
twiddle	O
,	O
”	O
pointing	O
out	O
its	O
utility	O
as	O
a	O
basic	O
adaptive	O
strategy	O
:	O
“	O
keep	O
going	O
in	O
the	O
same	O
way	O
if	O
things	O
are	O
getting	O
better	O
,	O
and	O
otherwise	O
move	O
around	O
”	O
(	O
selfridge	O
,	O
1978	O
,	O
1984	O
)	O
.	O
similarly	O
,	O
one	O
might	O
think	O
of	O
a	O
neuron	O
“	O
swimming	O
”	O
(	O
not	O
literally	O
of	O
course	O
)	O
in	O
a	O
medium	O
composed	O
of	O
the	O
complex	O
collection	O
of	O
feedback	O
loops	O
in	O
which	O
it	O
is	O
embedded	O
,	O
acting	O
to	O
obtain	O
one	O
type	O
of	O
input	O
signal	O
and	O
to	O
avoid	O
others	O
.	O
unlike	O
the	O
bacterium	O
,	O
however	O
,	O
the	O
neuron	O
’	O
s	O
synaptic	O
strengths	O
retain	O
information	O
about	O
its	O
past	O
trial-and-error	B
behavior	O
.	O
if	O
this	O
view	O
of	O
the	O
behavior	O
of	O
a	O
neuron	O
(	O
or	O
just	O
one	O
type	O
of	O
neuron	O
)	O
is	O
plausible	O
,	O
then	O
the	O
closed-loop	O
nature	O
of	O
how	O
the	O
neuron	O
interacts	O
with	O
its	O
environment	B
is	O
important	O
for	O
understanding	O
its	O
behavior	O
,	O
where	O
the	O
neuron	O
’	O
s	O
environment	B
consists	O
of	O
the	O
rest	O
of	O
the	O
animal	O
together	O
with	O
the	O
environment	B
with	O
which	O
the	O
animal	O
as	O
a	O
whole	O
interacts	O
.	O
klopf	O
’	O
s	O
hedonistic	O
neuron	O
hypothesis	O
extended	O
beyond	O
the	O
idea	O
that	O
individual	O
neurons	O
are	O
reinforcement	B
learning	I
agents	O
.	O
he	O
argued	O
that	O
many	O
aspects	O
of	O
intelligent	O
behavior	O
can	O
be	O
understood	O
as	O
the	O
result	O
of	O
the	O
collective	O
behavior	O
of	O
a	O
population	O
of	O
self-interested	O
hedonistic	B
neurons	I
interacting	O
with	O
one	O
another	O
in	O
an	O
immense	O
society	O
or	O
economic	O
system	O
making	O
up	O
an	O
animal	O
’	O
s	O
nervous	O
system	O
.	O
whether	O
or	O
not	O
this	O
view	O
of	O
nervous	O
systems	O
is	O
useful	O
,	O
the	O
collective	O
behavior	O
of	O
reinforcement	O
learning	O
agents	O
has	O
implications	O
for	O
neuroscience	O
.	O
we	O
take	O
up	O
this	O
subject	O
next	O
.	O
15.10	O
collective	B
reinforcement	I
learning	I
the	O
behavior	O
of	O
populations	O
of	O
reinforcement	O
learning	O
agents	O
is	O
deeply	O
relevant	O
to	O
the	O
study	O
of	O
social	O
and	O
economic	O
systems	O
,	O
and	O
if	O
anything	O
like	O
klopf	O
’	O
s	O
hedonistic	O
neuron	O
hypothesis	O
is	O
correct	O
,	O
to	O
neuroscience	B
as	O
well	O
.	O
the	O
hypothesis	O
described	O
above	O
about	O
how	O
an	O
actor–critic	B
algorithm	O
might	O
be	O
implemented	O
in	O
the	O
brain	O
only	O
narrowly	O
addresses	O
the	O
implications	O
of	O
the	O
fact	O
that	O
the	O
dorsal	O
and	O
ventral	O
subdivisions	O
of	O
the	O
striatum	O
,	O
the	O
respective	O
locations	O
of	O
the	O
actor	O
and	O
the	O
critic	O
according	O
to	O
the	O
hypothesis	O
,	O
each	O
contain	O
millions	O
of	O
medium	O
spiny	O
neurons	O
whose	O
synapses	O
undergo	O
change	O
modulated	O
by	O
phasic	O
bursts	O
of	O
dopamine	O
neuron	O
activity	O
.	O
the	O
actor	O
in	O
figure	O
15.5a	O
is	O
a	O
single-layer	O
network	O
of	O
k	O
actor	O
units	O
.	O
the	O
actions	O
produced	O
by	O
this	O
network	O
are	O
vectors	O
(	O
a1	O
,	O
a2	O
,	O
···	O
,	O
ak	O
)	O
(	O
cid:62	O
)	O
presumed	O
to	O
drive	O
the	O
animal	O
’	O
s	O
behavior	O
.	O
changes	O
in	O
the	O
eﬃcacies	O
of	O
the	O
synapses	O
of	O
all	O
of	O
these	O
units	O
depend	O
on	O
the	O
reinforcement	B
signal	I
δ.	O
because	O
actor	O
units	O
attempt	O
to	O
make	O
δ	O
as	O
large	O
as	O
possible	O
,	O
δ	O
eﬀectively	O
acts	O
as	O
a	O
reward	B
signal	I
for	O
them	O
(	O
so	O
in	O
this	O
case	O
reinforcement	O
is	O
the	O
same	O
as	O
reward	O
)	O
.	O
thus	O
,	O
each	O
actor	O
unit	O
is	O
itself	O
a	O
reinforcement	B
learning	I
agent—a	O
hedonistic	O
neuron	O
if	O
you	O
will	O
.	O
now	O
,	O
to	O
make	O
the	O
situation	O
as	O
simple	O
as	O
possible	O
,	O
assume	O
that	O
each	O
of	O
these	O
units	O
receives	O
the	O
same	O
reward	B
signal	I
at	O
the	O
same	O
time	O
(	O
although	O
,	O
as	O
indicated	O
above	O
,	O
the	O
assumption	O
that	O
dopamine	B
is	O
released	O
at	O
all	O
the	O
corticostriatal	O
synapses	O
under	O
the	O
same	O
conditions	O
and	O
at	O
the	O
same	O
times	O
is	O
likely	O
an	O
oversimpliﬁcation	O
)	O
.	O
what	O
can	O
reinforcement	B
learning	I
theory	O
tell	O
us	O
about	O
what	O
happens	O
when	O
all	O
members	O
of	O
a	O
population	O
of	O
reinforcement	O
learning	O
agents	O
learn	O
according	O
to	O
a	O
common	O
reward	B
signal	I
?	O
the	O
ﬁeld	O
of	O
multi-agent	O
reinforcement	B
learning	I
considers	O
many	O
aspects	O
of	O
learn-	O
ing	B
by	O
populations	O
of	O
reinforcement	O
learning	O
agents	O
.	O
although	O
this	O
ﬁeld	O
is	O
beyond	O
the	O
15.10.	O
collective	B
reinforcement	I
learning	I
409	O
scope	O
of	O
this	O
book	O
,	O
we	O
believe	O
that	O
some	O
of	O
its	O
basic	O
concepts	O
and	O
results	O
are	O
relevant	O
to	O
thinking	O
about	O
the	O
brain	O
’	O
s	O
diﬀuse	O
neuromodulatory	O
systems	O
.	O
in	O
multi-agent	O
reinforce-	O
ment	O
learning	O
(	O
and	O
in	O
game	B
theory	I
)	O
,	O
the	O
scenario	O
in	O
which	O
all	O
the	O
agents	O
try	O
to	O
maximize	O
a	O
common	O
reward	B
signal	I
that	O
they	O
simultaneously	O
receive	O
is	O
known	O
as	O
a	O
cooperative	O
game	O
or	O
a	O
team	O
problem	O
.	O
what	O
makes	O
a	O
team	O
problem	O
interesting	O
and	O
challenging	O
is	O
that	O
the	O
common	O
reward	B
signal	I
sent	O
to	O
each	O
agent	O
evaluates	O
the	O
pattern	O
of	O
activity	O
produced	O
by	O
the	O
entire	O
popula-	O
tion	B
,	O
that	O
is	O
,	O
it	O
evaluates	O
the	O
collective	O
action	O
of	O
the	O
team	O
members	O
.	O
this	O
means	O
that	O
any	O
individual	O
agent	O
has	O
only	O
limited	O
ability	O
to	O
aﬀect	O
the	O
reward	B
signal	I
because	O
any	O
single	O
agent	O
contributes	O
just	O
one	O
component	O
of	O
the	O
collective	O
action	O
evaluated	O
by	O
the	O
common	O
reward	B
signal	I
.	O
eﬀective	O
learning	O
in	O
this	O
scenario	O
requires	O
addressing	O
a	O
structural	B
credit	O
assignment	O
problem	O
:	O
which	O
team	O
members	O
,	O
or	O
groups	O
of	O
team	O
members	O
,	O
deserve	O
credit	O
for	O
a	O
favorable	O
reward	B
signal	I
,	O
or	O
blame	O
for	O
an	O
unfavorable	O
reward	B
signal	I
?	O
it	O
is	O
a	O
cooperative	O
game	O
,	O
or	O
a	O
team	O
problem	O
,	O
because	O
the	O
agents	O
are	O
united	O
in	O
seeking	O
to	O
increase	O
the	O
same	O
reward	B
signal	I
:	O
there	O
are	O
no	O
conﬂicts	O
of	O
interest	O
among	O
the	O
agents	O
.	O
the	O
scenario	O
would	O
be	O
a	O
competitive	O
game	O
if	O
diﬀerent	O
agents	O
receive	O
diﬀerent	O
reward	O
signals	O
,	O
where	O
each	O
reward	B
signal	I
again	O
evaluates	O
the	O
collective	O
action	O
of	O
the	O
population	O
,	O
and	O
the	O
objective	O
of	O
each	O
agent	O
is	O
to	O
increase	O
its	O
own	O
reward	B
signal	I
.	O
in	O
this	O
case	O
there	O
might	O
be	O
conﬂicts	O
of	O
interest	O
among	O
the	O
agents	O
,	O
meaning	O
that	O
actions	O
that	O
are	O
good	O
for	O
some	O
agents	O
are	O
bad	O
for	O
others	O
.	O
even	O
deciding	O
what	O
the	O
best	O
collective	O
action	O
should	O
be	O
is	O
a	O
non-trivial	O
aspect	O
of	O
game	O
theory	O
.	O
this	O
competitive	O
setting	O
might	O
be	O
relevant	O
to	O
neuroscience	B
too	O
(	O
for	O
example	O
,	O
to	O
account	O
for	O
heterogeneity	O
of	O
dopamine	O
neuron	O
activity	O
)	O
,	O
but	O
here	O
we	O
focus	O
only	O
on	O
the	O
cooperative	O
,	O
or	O
team	O
,	O
case	O
.	O
how	O
can	O
each	O
reinforcement	B
learning	I
agent	O
in	O
a	O
team	O
learn	O
to	O
“	O
do	O
the	O
right	O
thing	O
”	O
so	O
that	O
the	O
collective	O
action	O
of	O
the	O
team	O
is	O
highly	O
rewarded	O
?	O
an	O
interesting	O
result	O
is	O
that	O
if	O
each	O
agent	O
can	O
learn	O
eﬀectively	O
despite	O
its	O
reward	B
signal	I
being	O
corrupted	O
by	O
a	O
large	O
amount	O
of	O
noise	O
,	O
and	O
despite	O
its	O
lack	O
of	O
access	O
to	O
complete	O
state	B
information	O
,	O
then	O
the	O
population	O
as	O
a	O
whole	O
will	O
learn	O
to	O
produce	O
collective	O
actions	O
that	O
improve	O
as	O
evaluated	O
by	O
the	O
common	O
reward	B
signal	I
,	O
even	O
when	O
the	O
agents	O
can	O
not	O
communicate	O
with	O
one	O
another	O
.	O
each	O
agent	O
faces	O
its	O
own	O
reinforcement	B
learning	I
task	O
in	O
which	O
its	O
inﬂuence	O
on	O
the	O
reward	B
signal	I
is	O
deeply	O
buried	O
in	O
the	O
noise	O
created	O
by	O
the	O
inﬂuences	O
of	O
other	O
agents	O
.	O
in	O
fact	O
,	O
for	O
any	O
agent	O
,	O
all	O
the	O
other	O
agents	O
are	O
part	O
of	O
its	O
environment	B
because	O
its	O
input	O
,	O
both	O
the	O
part	O
conveying	O
state	B
information	O
and	O
the	O
reward	O
part	O
,	O
depends	O
on	O
how	O
all	O
the	O
other	O
agents	O
are	O
behaving	O
.	O
furthermore	O
,	O
lacking	O
access	O
to	O
the	O
actions	O
of	O
the	O
other	O
agents	O
,	O
indeed	O
lacking	O
access	O
to	O
the	O
parameters	O
determining	O
their	O
policies	O
,	O
each	O
agent	O
can	O
only	O
partially	O
observe	O
the	O
state	B
of	O
its	O
environment	B
.	O
this	O
makes	O
each	O
team	O
member	O
’	O
s	O
learning	O
task	O
very	O
diﬃcult	O
,	O
but	O
if	O
each	O
uses	O
a	O
reinforcement	B
learning	I
algorithm	O
able	O
to	O
increase	O
a	O
reward	B
signal	I
even	O
under	O
these	O
diﬃcult	O
conditions	O
,	O
teams	O
of	O
reinforcement	O
learning	O
agents	O
can	O
learn	O
to	O
produce	O
collective	O
actions	O
that	O
improve	O
over	O
time	O
as	O
evaluated	O
by	O
the	O
team	O
’	O
s	O
common	O
reward	B
signal	I
.	O
if	O
the	O
team	O
members	O
are	O
neuron-like	O
units	O
,	O
then	O
each	O
unit	O
has	O
to	O
have	O
the	O
goal	B
of	O
increasing	O
the	O
amount	O
of	O
reward	O
it	O
receives	O
over	O
time	O
,	O
as	O
the	O
actor	O
unit	O
does	O
that	O
we	O
described	O
in	O
section	O
15.8.	O
each	O
unit	O
’	O
s	O
learning	O
algorithm	O
has	O
to	O
have	O
two	O
essential	O
fea-	O
tures	O
.	O
first	O
,	O
it	O
has	O
to	O
use	O
contingent	O
eligibility	O
traces	O
.	O
recall	O
that	O
a	O
contingent	O
eligibility	O
410	O
chapter	O
15	O
:	O
neuroscience	B
trace	O
,	O
in	O
neural	O
terms	O
,	O
is	O
initiated	O
(	O
or	O
increased	O
)	O
at	O
a	O
synapse	O
when	O
its	O
presynaptic	O
in-	O
put	O
participates	O
in	O
causing	O
the	O
postsynaptic	O
neuron	O
to	O
ﬁre	O
.	O
a	O
non-contingent	O
eligibility	O
trace	O
,	O
in	O
contrast	O
,	O
is	O
initiated	O
or	O
increased	O
by	O
presynaptic	O
input	O
independently	O
of	O
what	O
the	O
postsynaptic	O
neuron	O
does	O
.	O
as	O
explained	O
in	O
section	O
15.8	O
,	O
by	O
keeping	O
information	O
about	O
what	O
actions	O
were	O
taken	O
in	O
what	O
states	O
,	O
contingent	O
eligibility	O
traces	O
allow	O
credit	O
for	O
reward	O
,	O
or	O
blame	O
for	O
punishment	O
,	O
to	O
be	O
apportioned	O
to	O
an	O
agent	O
’	O
s	O
policy	B
parameters	O
according	O
to	O
the	O
contribution	O
the	O
values	O
of	O
these	O
parameters	O
made	O
in	O
determining	O
the	O
agent	O
’	O
s	O
action	B
.	O
by	O
similar	O
reasoning	O
,	O
a	O
team	O
member	O
must	O
remember	O
its	O
recent	O
action	B
so	O
that	O
it	O
can	O
either	O
increase	O
or	O
decrease	O
the	O
likelihood	O
of	O
producing	O
that	O
action	B
according	O
to	O
the	O
reward	B
signal	I
that	O
is	O
subsequently	O
received	O
.	O
the	O
action	B
component	O
of	O
a	O
contin-	O
gent	O
eligibility	O
trace	O
implements	O
this	O
action	B
memory	O
.	O
because	O
of	O
the	O
complexity	O
of	O
the	O
learning	O
task	O
,	O
however	O
,	O
contingent	O
eligibility	O
is	O
merely	O
a	O
preliminary	O
step	O
in	O
the	O
credit	O
assignment	O
process	O
:	O
the	O
relationship	O
between	O
a	O
single	O
team	O
member	O
’	O
s	O
action	B
and	O
changes	O
in	O
the	O
team	O
’	O
s	O
reward	B
signal	I
is	O
a	O
statistical	O
correlation	O
that	O
has	O
to	O
be	O
estimated	O
over	O
many	O
trials	O
.	O
contingent	O
eligibility	O
is	O
an	O
essential	O
but	O
preliminary	O
step	O
in	O
this	O
process	O
.	O
learning	O
with	O
non-contingent	O
eligibility	B
traces	I
does	O
not	O
work	O
at	O
all	O
in	O
the	O
team	O
setting	O
because	O
it	O
does	O
not	O
provide	O
a	O
way	O
to	O
correlate	O
actions	O
with	O
consequent	O
changes	O
in	O
the	O
reward	O
signal	O
.	O
non-contingent	O
eligibility	B
traces	I
are	O
adequate	O
for	O
learning	O
to	O
predict	O
,	O
as	O
the	O
critic	B
component	O
of	O
the	O
actor–critic	B
algorithm	O
does	O
,	O
but	O
they	O
do	O
not	O
support	O
learning	O
to	O
control	B
,	O
as	O
the	O
actor	O
component	O
must	O
do	O
.	O
the	O
members	O
of	O
a	O
population	O
of	O
critic-like	O
agents	O
may	O
still	O
receive	O
a	O
common	O
reinforcement	B
signal	I
,	O
but	O
they	O
would	O
all	O
learn	O
to	O
predict	O
the	O
same	O
quantity	O
(	O
which	O
in	O
the	O
case	O
of	O
an	O
actor–critic	B
method	O
,	O
would	O
be	O
the	O
expected	O
return	O
for	O
the	O
current	O
policy	B
)	O
.	O
how	O
successful	O
each	O
member	O
of	O
the	O
population	O
would	O
be	O
in	O
learning	O
to	O
predict	O
the	O
expected	O
return	O
would	O
depend	O
on	O
the	O
information	O
it	O
receives	O
,	O
which	O
could	O
be	O
very	O
diﬀerent	O
for	O
diﬀerent	O
members	O
of	O
the	O
population	O
.	O
there	O
would	O
be	O
no	O
need	O
for	O
the	O
population	O
to	O
produce	O
diﬀerentiated	O
patterns	O
of	O
activity	O
.	O
this	O
is	O
not	O
a	O
team	O
problem	O
as	O
deﬁned	O
here	O
.	O
a	O
second	O
requirement	O
for	O
collective	O
learning	O
in	O
a	O
team	O
problem	O
is	O
that	O
there	O
has	O
to	O
be	O
variability	O
in	O
the	O
actions	O
of	O
the	O
team	O
members	O
in	O
order	O
for	O
the	O
team	O
to	O
explore	O
the	O
space	O
of	O
collective	O
actions	O
.	O
the	O
simplest	O
way	O
for	O
a	O
team	O
of	O
reinforcement	O
learning	O
agents	O
to	O
do	O
this	O
is	O
for	O
each	O
member	O
to	O
independently	O
explore	O
its	O
own	O
action	B
space	O
through	O
persistent	O
variability	O
in	O
its	O
output	O
.	O
this	O
will	O
cause	O
the	O
team	O
as	O
a	O
whole	O
to	O
vary	O
its	O
collective	O
actions	O
.	O
for	O
example	O
,	O
a	O
team	O
of	O
the	O
actor	O
units	O
described	O
in	O
section	O
15.8	O
explores	O
the	O
space	O
of	O
collective	O
actions	O
because	O
the	O
output	O
of	O
each	O
unit	O
,	O
being	O
a	O
bernoulli-logistic	O
unit	O
,	O
probabilistically	O
depends	O
on	O
the	O
weighted	O
sum	O
of	O
its	O
input	O
vector	B
’	O
s	O
components	O
.	O
the	O
weighted	O
sum	O
biases	O
ﬁring	O
probability	O
up	O
or	O
down	O
,	O
but	O
there	O
is	O
always	O
variability	O
.	O
because	O
each	O
unit	O
uses	O
a	O
reinforce	O
policy	O
gradient	O
algorithm	O
(	O
chapter	O
13	O
)	O
,	O
each	O
unit	O
adjusts	O
its	O
weights	O
with	O
the	O
goal	B
of	O
maximizing	O
the	O
average	O
reward	O
rate	O
it	O
experiences	O
while	O
stochastically	O
exploring	O
its	O
own	O
action	B
space	O
.	O
one	O
can	O
show	O
,	O
as	O
williams	O
(	O
1992	O
)	O
did	O
,	O
that	O
a	O
team	O
of	O
bernoulli-logistic	O
reinforce	O
units	O
implements	O
a	O
policy	O
gradient	O
algorithm	O
as	O
a	O
whole	O
with	O
respect	O
to	O
average	O
rate	O
of	O
the	O
team	O
’	O
s	O
common	O
reward	B
signal	I
,	O
where	O
the	O
actions	O
are	O
the	O
collective	O
actions	O
of	O
the	O
team	O
.	O
further	O
,	O
williams	O
(	O
1992	O
)	O
showed	O
that	O
a	O
team	O
of	O
bernoulli-logistic	O
units	O
using	O
rein-	O
force	O
ascends	O
the	O
average	O
reward	O
gradient	O
when	O
the	O
units	O
in	O
the	O
team	O
are	O
intercon-	O
15.11.	O
model-based	O
methods	O
in	O
the	O
brain	O
411	O
nected	O
to	O
form	O
a	O
multilayer	O
neural	B
network	O
.	O
in	O
this	O
case	O
,	O
the	O
reward	B
signal	I
is	O
broadcast	O
to	O
all	O
the	O
units	O
in	O
the	O
network	O
,	O
though	O
reward	O
may	O
depend	O
only	O
on	O
the	O
collective	O
actions	O
of	O
the	O
network	O
’	O
s	O
output	O
units	O
.	O
this	O
means	O
that	O
a	O
multilayer	O
team	O
of	O
bernoulli-logistic	O
reinforce	O
units	O
learns	O
like	O
a	O
multilayer	O
network	O
trained	O
by	O
the	O
widely-used	O
error	O
backpropagation	O
method	O
,	O
but	O
in	O
this	O
case	O
the	O
backpropagation	B
process	O
is	O
replaced	O
by	O
the	O
broadcasted	O
reward	B
signal	I
.	O
in	O
practice	O
,	O
the	O
error	O
backpropagation	O
method	O
is	O
consid-	O
erably	O
faster	O
,	O
but	O
the	O
reinforcement	B
learning	I
team	O
method	O
is	O
more	O
plausible	O
as	O
a	O
neural	B
mechanism	O
,	O
especially	O
in	O
light	O
of	O
what	O
is	O
being	O
learned	O
about	O
reward-modulated	O
stdp	O
as	O
discussed	O
in	O
section	O
15.8.	O
exploration	O
through	O
independent	O
exploration	O
by	O
team	O
members	O
is	O
only	O
the	O
simplest	O
way	O
for	O
a	O
team	O
to	O
explore	O
;	O
more	O
sophisticated	O
methods	O
are	O
possible	O
if	O
the	O
team	O
members	O
coordinate	O
their	O
actions	O
to	O
focus	O
on	O
particular	O
parts	O
of	O
the	O
collective	O
action	O
space	O
,	O
either	O
by	O
communicating	O
with	O
one	O
another	O
or	O
by	O
responding	O
to	O
common	O
inputs	O
.	O
there	O
are	O
also	O
mechanisms	O
more	O
sophisticated	O
than	O
contingent	O
eligibility	O
traces	O
for	O
addressing	O
structural	B
credit	O
assignment	O
,	O
which	O
is	O
easier	O
in	O
a	O
team	O
problem	O
when	O
the	O
set	O
of	O
possible	O
collective	O
actions	O
is	O
restricted	O
in	O
some	O
way	O
.	O
an	O
extreme	O
case	O
is	O
a	O
winner-take-all	O
arrangement	O
(	O
for	O
example	O
,	O
the	O
result	O
of	O
lateral	O
inhibition	O
in	O
the	O
brain	O
)	O
that	O
restricts	O
collective	O
actions	O
to	O
those	O
to	O
which	O
only	O
one	O
,	O
or	O
a	O
few	O
,	O
team	O
members	O
contribute	O
.	O
in	O
this	O
case	O
the	O
winners	O
get	O
the	O
credit	O
or	O
blame	O
for	O
resulting	O
reward	O
or	O
punishment	O
.	O
details	O
of	O
learning	O
in	O
cooperative	O
games	O
(	O
or	O
team	O
problems	O
)	O
and	O
non-cooperative	O
game	O
problems	O
are	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
.	O
the	O
bibliographical	O
and	O
historical	O
remarks	O
section	O
at	O
the	O
end	O
of	O
this	O
chapter	O
cites	O
a	O
selection	O
of	O
the	O
relevant	O
publications	O
,	O
including	O
extensive	O
references	O
to	O
research	O
on	O
implications	O
for	O
neuroscience	O
of	O
collective	O
reinforcement	B
learning	I
.	O
15.11	O
model-based	O
methods	O
in	O
the	O
brain	O
reinforcement	B
learning	I
’	O
s	O
distinction	O
between	O
model-free	O
and	O
model-based	O
algorithms	O
is	O
proving	O
to	O
be	O
useful	O
for	O
thinking	O
about	O
animal	O
learning	O
and	O
decision	O
processes	O
.	O
sec-	O
tion	B
14.6	O
discusses	O
how	O
this	O
distinction	O
aligns	O
with	O
that	O
between	O
habitual	O
and	O
goal-	O
directed	O
animal	O
behavior	O
.	O
the	O
hypothesis	O
discussed	O
above	O
about	O
how	O
the	O
brain	O
might	O
implement	O
an	O
actor–critic	B
algorithm	O
is	O
relevant	O
only	O
to	O
an	O
animal	O
’	O
s	O
habitual	O
mode	O
of	O
be-	O
havior	O
because	O
the	O
basic	O
actor–critic	B
method	O
is	O
model-free	O
.	O
what	O
neural	B
mechanisms	O
are	O
responsible	O
for	O
producing	O
goal-directed	O
behavior	O
,	O
and	O
how	O
do	O
they	O
interact	O
with	O
those	O
underlying	O
habitual	O
behavior	O
?	O
one	O
way	O
to	O
investigate	O
questions	O
about	O
the	O
brain	O
structures	O
involved	O
in	O
these	O
modes	O
of	O
behavior	O
is	O
to	O
inactivate	O
an	O
area	O
of	O
a	O
rat	O
’	O
s	O
brain	O
and	O
then	O
observe	O
what	O
the	O
rat	O
does	O
in	O
an	O
outcome-devaluation	O
experiment	O
(	O
section	O
14.6	O
)	O
.	O
results	O
from	O
experiments	O
like	O
these	O
indicate	O
that	O
the	O
actor–critic	B
hypothesis	O
described	O
above	O
is	O
too	O
simple	O
in	O
placing	O
the	O
actor	O
in	O
the	O
dorsal	O
striatum	O
.	O
inactivating	O
one	O
part	O
of	O
the	O
dorsal	O
striatum	O
,	O
the	O
dorsolateral	O
striatum	O
(	O
dls	O
)	O
,	O
impairs	O
habit	O
learning	O
,	O
causing	O
the	O
animal	O
to	O
rely	O
more	O
on	O
goal-directed	O
processes	O
.	O
on	O
the	O
other	O
hand	O
,	O
inactivating	O
the	O
dorsomedial	O
striatum	O
(	O
dms	O
)	O
impairs	O
goal-directed	O
processes	O
,	O
requiring	O
the	O
animal	O
to	O
rely	O
more	O
on	O
habit	O
learning	O
.	O
results	O
like	O
these	O
support	O
the	O
view	O
that	O
the	O
dls	O
in	O
rodents	O
is	O
more	O
involved	O
in	O
model-free	O
processes	O
,	O
412	O
chapter	O
15	O
:	O
neuroscience	B
whereas	O
their	O
dms	O
is	O
more	O
involved	O
in	O
model-based	O
processes	O
.	O
results	O
of	O
studies	O
with	O
human	O
subjects	O
in	O
similar	O
experiments	O
using	O
functional	O
neuroimaging	O
,	O
and	O
with	O
non-	O
human	O
primates	O
,	O
support	O
the	O
view	O
that	O
the	O
analogous	O
structures	O
in	O
the	O
primate	O
brain	O
are	O
diﬀerentially	O
involved	O
in	O
habitual	O
and	O
goal-directed	O
modes	O
of	O
behavior	O
.	O
other	O
studies	O
identify	O
activity	O
associated	O
with	O
model-based	O
processes	O
in	O
the	O
prefrontal	O
cortex	O
of	O
the	O
human	O
brain	O
,	O
the	O
front-most	O
part	O
of	O
the	O
frontal	O
cortex	O
implicated	O
in	O
ex-	O
ecutive	O
function	O
,	O
including	O
planning	B
and	O
decision	O
making	O
.	O
speciﬁcally	O
implicated	O
is	O
the	O
orbitofrontal	O
cortex	O
(	O
ofc	O
)	O
,	O
the	O
part	O
of	O
the	O
prefrontal	O
cortex	O
immediately	O
above	O
the	O
eyes	O
.	O
functional	O
neuroimaging	O
in	O
humans	O
,	O
and	O
also	O
recordings	O
of	O
the	O
activities	O
of	O
single	O
neu-	O
rons	O
in	O
monkeys	O
,	O
reveals	O
strong	O
activity	O
in	O
the	O
ofc	O
related	O
to	O
the	O
subjective	O
reward	O
value	O
of	O
biologically	O
signiﬁcant	O
stimuli	O
,	O
as	O
well	O
as	O
activity	O
related	O
to	O
the	O
reward	O
expected	O
as	O
a	O
consequence	O
of	O
actions	O
.	O
although	O
not	O
free	O
of	O
controversy	O
,	O
these	O
results	O
suggest	O
sig-	O
niﬁcant	O
involvement	O
of	O
the	O
ofc	O
in	O
goal-directed	O
choice	O
.	O
it	O
may	O
be	O
critical	O
for	O
the	O
reward	O
part	O
of	O
an	O
animal	O
’	O
s	O
environment	B
model	O
.	O
another	O
structure	O
involved	O
in	O
model-based	O
behavior	O
is	O
the	O
hippocampus	O
,	O
a	O
structure	O
critical	O
for	O
memory	O
and	O
spatial	O
navigation	O
.	O
a	O
rat	O
’	O
s	O
hippocampus	O
plays	O
a	O
critical	O
role	O
in	O
the	O
rat	O
’	O
s	O
ability	O
to	O
navigate	O
a	O
maze	O
in	O
the	O
goal-directed	O
manner	O
that	O
led	O
tolman	O
to	O
the	O
idea	O
that	O
animals	O
use	O
models	O
,	O
or	O
cognitive	B
maps	I
,	O
in	O
selecting	O
actions	O
(	O
section	O
14.5	O
)	O
.	O
the	O
hippocampus	O
may	O
also	O
be	O
a	O
critical	O
component	O
of	O
our	O
human	O
ability	O
to	O
imagine	O
new	O
experiences	O
(	O
hassabis	O
and	O
maguire	O
,	O
2007	O
;	O
´olafsd´ottir	O
,	O
barry	O
,	O
saleem	O
,	O
hassabis	O
,	O
and	O
spiers	O
,	O
2015	O
)	O
.	O
the	O
ﬁndings	O
that	O
most	O
directly	O
implicate	O
the	O
hippocampus	O
in	O
planning—the	O
process	O
needed	O
to	O
enlist	O
an	O
environment	B
model	O
in	O
making	O
decisions—come	O
from	O
experiments	O
that	O
decode	O
the	O
activity	O
of	O
neurons	O
in	O
the	O
hippocampus	O
to	O
determine	O
what	O
part	O
of	O
space	O
hippocampal	O
activity	O
is	O
representing	O
on	O
a	O
moment-to-moment	O
basis	O
.	O
when	O
a	O
rat	O
pauses	O
at	O
a	O
choice	O
point	O
in	O
a	O
maze	O
,	O
the	O
representation	O
of	O
space	O
in	O
the	O
hippocampus	O
sweeps	B
forward	O
(	O
and	O
not	O
backwards	O
)	O
along	O
the	O
possible	O
paths	O
the	O
animal	O
can	O
take	O
from	O
that	O
point	O
(	O
johnson	O
and	O
redish	O
,	O
2007	O
)	O
.	O
furthermore	O
,	O
the	O
spatial	O
trajectories	O
represented	O
by	O
these	O
sweeps	B
closely	O
correspond	O
to	O
the	O
rat	O
’	O
s	O
subsequent	O
navigational	O
behavior	O
(	O
pfeiﬀer	O
and	O
foster	O
,	O
2013	O
)	O
.	O
these	O
results	O
suggest	O
that	O
the	O
hippocampus	O
is	O
critical	O
for	O
the	O
state-	O
transition	O
part	O
of	O
an	O
animal	O
’	O
s	O
environment	B
model	O
,	O
and	O
that	O
it	O
is	O
part	O
of	O
a	O
system	O
that	O
uses	O
the	O
model	O
to	O
simulate	O
possible	O
future	O
state	B
sequences	O
to	O
assess	O
the	O
consequences	O
of	O
possible	O
courses	O
of	O
action	O
:	O
a	O
form	O
of	O
planning	O
.	O
the	O
results	O
described	O
above	O
add	O
to	O
a	O
voluminous	O
literature	O
on	O
neural	B
mechanisms	O
underlying	O
goal-directed	O
,	O
or	O
model-based	O
,	O
learning	O
and	O
decision	O
making	O
,	O
but	O
many	O
ques-	O
tions	O
remain	O
unanswered	O
.	O
for	O
example	O
,	O
how	O
can	O
areas	O
as	O
structurally	O
similar	O
as	O
the	O
dls	O
and	O
dms	O
be	O
essential	O
components	O
of	O
modes	O
of	O
learning	O
and	O
behavior	O
that	O
are	O
as	O
diﬀer-	O
ent	O
as	O
model-free	O
and	O
model-based	O
algorithms	O
?	O
are	O
separate	O
structures	O
responsible	O
for	O
(	O
what	O
we	O
call	O
)	O
the	O
transition	O
and	O
reward	O
components	O
of	O
an	O
environment	B
model	O
?	O
is	O
all	O
planning	B
conducted	O
at	O
decision	O
time	O
via	O
simulations	O
of	O
possible	O
future	O
courses	O
of	O
action	O
as	O
the	O
forward	O
sweeping	O
activity	O
in	O
the	O
hippocampus	O
suggests	O
?	O
in	O
other	O
words	O
,	O
is	O
all	O
planning	B
something	O
like	O
a	O
rollout	O
algorithm	O
(	O
section	O
8.10	O
)	O
?	O
or	O
are	O
models	O
sometimes	O
engaged	O
in	O
the	O
background	O
to	O
reﬁne	O
or	O
recompute	O
value	B
information	O
as	O
illustrated	O
by	O
the	O
dyna	O
architecture	O
(	O
section	O
8.2	O
)	O
?	O
how	O
does	O
the	O
brain	O
arbitrate	O
between	O
the	O
use	O
of	O
the	O
15.12.	O
addiction	B
413	O
habit	O
and	O
goal-directed	O
systems	O
?	O
is	O
there	O
,	O
in	O
fact	O
,	O
a	O
clear	O
separation	O
between	O
the	O
neural	B
substrates	O
of	O
these	O
systems	O
?	O
the	O
evidence	O
is	O
not	O
pointing	O
to	O
a	O
positive	O
answer	O
to	O
this	O
last	O
question	O
.	O
summarizing	O
the	O
situation	O
,	O
doll	O
,	O
simon	O
,	O
and	O
daw	O
(	O
2012	O
)	O
wrote	O
that	O
“	O
model-based	O
inﬂuences	O
appear	O
ubiquitous	O
more	O
or	O
less	O
wherever	O
the	O
brain	O
processes	O
reward	O
information	O
,	O
”	O
and	O
this	O
is	O
true	O
even	O
in	O
the	O
regions	O
thought	O
to	O
be	O
critical	O
for	O
model-free	O
learning	O
.	O
this	O
includes	O
the	O
dopamine	B
signals	O
themselves	O
,	O
which	O
can	O
exhibit	O
the	O
inﬂuence	O
of	O
model-based	O
information	O
in	O
addition	O
to	O
the	O
reward	O
prediction	O
errors	O
thought	O
to	O
be	O
the	O
basis	O
of	O
model-free	O
processes	O
.	O
continuing	O
neuroscience	O
research	O
informed	O
by	O
reinforcement	B
learning	I
’	O
s	O
model-free	O
and	O
model-based	O
distinction	O
has	O
the	O
potential	O
to	O
sharpen	O
our	O
understanding	O
of	O
habitual	O
and	O
goal-directed	O
processes	O
in	O
the	O
brain	O
.	O
a	O
better	O
grasp	O
of	O
these	O
neural	B
mechanisms	O
may	O
lead	O
to	O
algorithms	O
combining	O
model-free	O
and	O
model-based	O
methods	O
in	O
ways	O
that	O
have	O
not	O
yet	O
been	O
explored	O
in	O
computational	O
reinforcement	B
learning	I
.	O
15.12	O
addiction	B
understanding	O
the	O
neural	B
basis	O
of	O
drug	O
abuse	O
is	O
a	O
high-priority	O
goal	B
of	O
neuroscience	B
with	O
the	O
potential	O
to	O
produce	O
new	O
treatments	O
for	O
this	O
serious	O
public	O
health	O
problem	O
.	O
one	O
view	O
is	O
that	O
drug	O
craving	O
is	O
the	O
result	O
of	O
the	O
same	O
motivation	B
and	O
learning	O
pro-	O
cesses	O
that	O
lead	O
us	O
to	O
seek	O
natural	O
rewarding	O
experiences	O
that	O
serve	O
our	O
biological	O
needs	O
.	O
addictive	O
substances	O
,	O
by	O
being	O
intensely	O
reinforcing	O
,	O
eﬀectively	O
co-opt	O
our	O
natural	O
mech-	O
anisms	O
of	O
learning	O
and	O
decision	O
making	O
.	O
this	O
is	O
plausible	O
given	O
that	O
many—though	O
not	O
all—drugs	O
of	O
abuse	O
increase	O
levels	O
of	O
dopamine	O
either	O
directly	O
or	O
indirectly	O
in	O
regions	O
around	O
terminals	O
of	O
dopamine	O
neuron	O
axons	O
in	O
the	O
striatum	O
,	O
a	O
brain	O
structure	O
ﬁrmly	O
implicated	O
in	O
normal	O
reward-based	O
learning	O
(	O
section	O
15.7	O
)	O
.	O
but	O
the	O
self-destructive	O
be-	O
havior	O
associated	O
with	O
drug	O
addiction	B
is	O
not	O
characteristic	O
of	O
normal	O
learning	O
.	O
what	O
is	O
diﬀerent	O
about	O
dopamine-mediated	O
learning	O
when	O
the	O
reward	O
is	O
the	O
result	O
of	O
an	O
ad-	O
dictive	O
drug	O
?	O
is	O
addiction	B
the	O
result	O
of	O
normal	O
learning	O
in	O
response	O
to	O
substances	O
that	O
were	O
largely	O
unavailable	O
throughout	O
our	O
evolutionary	O
history	O
,	O
so	O
that	O
evolution	B
could	O
not	O
select	O
against	O
their	O
damaging	O
eﬀects	O
?	O
or	O
do	O
addictive	O
substances	O
somehow	O
interfere	O
with	O
normal	O
dopamine-mediated	O
learning	O
?	O
the	O
reward	B
prediction	I
error	I
hypothesis	I
of	O
dopamine	B
neuron	O
activity	O
and	O
its	O
connection	O
to	O
td	O
learning	O
are	O
the	O
basis	O
of	O
a	O
model	O
due	O
to	O
redish	O
(	O
2004	O
)	O
of	O
some—but	O
certainly	O
not	O
all—features	O
of	O
addiction	O
.	O
the	O
model	O
is	O
based	O
on	O
the	O
observation	O
that	O
administration	O
of	O
cocaine	O
and	O
some	O
other	O
addictive	O
drugs	O
produces	O
a	O
transient	O
increase	O
in	O
dopamine	O
.	O
in	O
the	O
model	O
,	O
this	O
dopamine	B
surge	O
is	O
assumed	O
to	O
increase	O
the	O
td	O
error	O
,	O
δ	O
,	O
in	O
a	O
way	O
that	O
can	O
not	O
be	O
cancelled	O
out	O
by	O
changes	O
in	O
the	O
value	O
function	O
.	O
in	O
other	O
words	O
,	O
whereas	O
δ	O
is	O
reduced	O
to	O
the	O
degree	O
that	O
a	O
normal	O
reward	O
is	O
predicted	O
by	O
antecedent	O
events	O
(	O
section	O
15.6	O
)	O
,	O
the	O
contribution	O
to	O
δ	O
due	O
to	O
an	O
addictive	O
stimulus	O
does	O
not	O
decrease	O
as	O
the	O
reward	B
signal	I
becomes	O
predicted	O
:	O
drug	O
rewards	O
can	O
not	O
be	O
“	O
predicted	O
away.	O
”	O
the	O
model	O
does	O
this	O
by	O
preventing	O
δ	O
from	O
ever	O
becoming	O
negative	O
when	O
the	O
reward	B
signal	I
is	O
due	O
to	O
an	O
addictive	O
drug	O
,	O
thus	O
eliminating	O
the	O
error-correcting	O
feature	O
of	O
td	O
learning	O
for	O
states	O
associated	O
with	O
administration	O
of	O
the	O
drug	O
.	O
the	O
result	O
is	O
that	O
the	O
values	O
of	O
these	O
states	O
increase	O
without	O
bound	O
,	O
making	O
actions	O
leading	O
to	O
these	O
states	O
preferred	O
above	O
all	O
others	O
.	O
414	O
chapter	O
15	O
:	O
neuroscience	B
addictive	O
behavior	O
is	O
much	O
more	O
complicated	O
than	O
this	O
result	O
from	O
redish	O
’	O
s	O
model	O
,	O
but	O
the	O
model	O
’	O
s	O
main	O
idea	O
may	O
be	O
a	O
piece	O
of	O
the	O
puzzle	O
.	O
or	O
the	O
model	O
might	O
be	O
misleading	O
.	O
dopamine	B
appears	O
not	O
to	O
play	O
a	O
critical	O
role	O
in	O
all	O
forms	O
of	O
addiction	O
,	O
and	O
not	O
everyone	O
is	O
equally	O
susceptible	O
to	O
developing	O
addictive	O
behavior	O
.	O
moreover	O
,	O
the	O
model	O
does	O
not	O
include	O
the	O
changes	O
in	O
many	O
circuits	O
and	O
brain	O
regions	O
that	O
accompany	O
chronic	O
drug	O
taking	O
,	O
for	O
example	O
,	O
changes	O
that	O
lead	O
to	O
a	O
drug	O
’	O
s	O
diminishing	O
eﬀect	O
with	O
repeated	O
use	O
.	O
it	O
is	O
also	O
likely	O
that	O
addiction	B
involves	O
model-based	O
processes	O
.	O
still	O
,	O
redish	O
’	O
s	O
model	O
illustrates	O
how	O
reinforcement	B
learning	I
theory	O
can	O
be	O
enlisted	O
in	O
the	O
eﬀort	O
to	O
understand	O
a	O
major	O
health	O
problem	O
.	O
in	O
a	O
similar	O
manner	O
,	O
reinforcement	B
learning	I
theory	O
has	O
been	O
inﬂuential	O
in	O
the	O
development	O
of	O
the	O
new	O
ﬁeld	O
of	O
computational	O
psychiatry	O
,	O
which	O
aims	O
to	O
improve	O
understanding	O
of	O
mental	O
disorders	O
through	O
mathematical	O
and	O
computational	O
methods	O
.	O
15.13	O
summary	O
the	O
neural	B
pathways	O
involved	O
in	O
the	O
brain	O
’	O
s	O
reward	O
system	O
are	O
complex	O
and	O
incompletely	O
understood	O
,	O
but	O
neuroscience	B
research	O
directed	O
toward	O
understanding	O
these	O
pathways	O
and	O
their	O
roles	O
in	O
behavior	O
is	O
progressing	O
rapidly	O
.	O
this	O
research	O
is	O
revealing	O
striking	O
correspondences	O
between	O
the	O
brain	O
’	O
s	O
reward	O
system	O
and	O
the	O
theory	O
of	O
reinforcement	O
learning	O
as	O
presented	O
in	O
this	O
book	O
.	O
the	O
reward	B
prediction	I
error	I
hypothesis	I
of	O
dopamine	B
neuron	O
activity	O
was	O
proposed	O
by	O
scientists	O
who	O
recognized	O
striking	O
parallels	O
between	O
the	O
behavior	O
of	O
td	O
errors	O
and	O
the	O
activity	O
of	O
neurons	O
that	O
produce	O
dopamine	B
,	O
a	O
neurotransmitter	O
essential	O
in	O
mammals	O
for	O
reward-related	O
learning	O
and	O
behavior	O
.	O
experiments	O
conducted	O
in	O
the	O
late	O
1980s	O
and	O
1990s	O
in	O
the	O
laboratory	O
of	O
neuroscientist	O
wolfram	O
schultz	O
showed	O
that	O
dopamine	B
neurons	O
respond	O
to	O
rewarding	O
events	O
with	O
substantial	O
bursts	O
of	O
activity	O
,	O
called	O
phasic	O
responses	O
,	O
only	O
if	O
the	O
animal	O
does	O
not	O
expect	O
those	O
events	O
,	O
suggesting	O
that	O
dopamine	B
neurons	O
are	O
signaling	O
reward	O
prediction	O
errors	O
instead	O
of	O
reward	O
itself	O
.	O
further	O
,	O
these	O
experiments	O
showed	O
that	O
as	O
an	O
animal	O
learns	O
to	O
predict	O
a	O
rewarding	O
event	O
on	O
the	O
basis	O
of	O
preceding	O
sensory	O
cues	O
,	O
the	O
phasic	O
activity	O
of	O
dopamine	O
neurons	O
shifts	O
to	O
earlier	O
predictive	O
cues	O
while	O
decreasing	O
to	O
later	O
predictive	O
cues	O
.	O
this	O
parallels	O
the	O
backing-up	O
eﬀect	O
of	O
the	O
td	O
error	O
as	O
a	O
reinforcement	B
learning	I
agent	O
learns	O
to	O
predict	O
reward	O
.	O
other	O
experimental	O
results	O
ﬁrmly	O
establish	O
that	O
the	O
phasic	O
activity	O
of	O
dopamine	O
neu-	O
rons	O
is	O
a	O
reinforcement	B
signal	I
for	O
learning	O
that	O
reaches	O
multiple	O
areas	O
of	O
the	O
brain	O
by	O
means	O
of	O
profusely	O
branching	O
axons	O
of	O
dopamine	O
producing	O
neurons	O
.	O
these	O
results	O
are	O
consistent	O
with	O
the	O
distinction	O
we	O
make	O
between	O
a	O
reward	B
signal	I
,	O
rt	O
,	O
and	O
a	O
reinforcement	B
signal	I
,	O
which	O
is	O
the	O
td	O
error	O
δt	O
in	O
most	O
of	O
the	O
algorithms	O
we	O
present	O
.	O
phasic	O
responses	O
of	O
dopamine	O
neurons	O
are	O
reinforcement	O
signals	O
,	O
not	O
reward	O
signals	O
.	O
a	O
prominent	O
hypothesis	O
is	O
that	O
the	O
brain	O
implements	O
something	O
like	O
an	O
actor–critic	B
algorithm	O
.	O
two	O
structures	O
in	O
the	O
brain	O
(	O
the	O
dorsal	O
and	O
ventral	O
subdivisions	O
of	O
the	O
stria-	O
tum	O
)	O
,	O
both	O
of	O
which	O
play	O
critical	O
roles	O
in	O
reward-based	O
learning	O
,	O
may	O
function	O
respectively	O
like	O
an	O
actor	O
and	O
a	O
critic	B
.	O
that	O
the	O
td	O
error	O
is	O
the	O
reinforcement	B
signal	I
for	O
both	O
the	O
actor	O
and	O
the	O
critic	O
ﬁts	O
well	O
with	O
the	O
facts	O
that	O
dopamine	B
neuron	O
axons	O
target	B
both	O
the	O
dorsal	O
and	O
ventral	O
subdivisions	O
of	O
the	O
striatum	O
;	O
that	O
dopamine	B
appears	O
to	O
be	O
critical	O
15.13.	O
summary	O
415	O
for	O
modulating	O
synaptic	B
plasticity	I
in	O
both	O
structures	O
;	O
and	O
that	O
the	O
eﬀect	O
on	O
a	O
target	B
structure	O
of	O
a	O
neuromodulator	O
such	O
as	O
dopamine	O
depends	O
on	O
properties	O
of	O
the	O
target	B
structure	O
and	O
not	O
just	O
on	O
properties	O
of	O
the	O
neuromodulator	O
.	O
the	O
actor	O
and	O
the	O
critic	O
can	O
be	O
implemented	O
by	O
artiﬁcial	B
neural	I
networks	I
consisting	O
of	O
neuron-like	O
units	O
having	O
learning	O
rules	O
based	O
on	O
the	O
policy-gradient	O
actor–critic	B
method	O
described	O
in	O
section	O
13.5.	O
each	O
connection	O
in	O
these	O
networks	O
is	O
like	O
a	O
synapse	O
between	O
neurons	O
in	O
the	O
brain	O
,	O
and	O
the	O
learning	O
rules	O
correspond	O
to	O
rules	O
governing	O
how	O
synaptic	O
eﬃcacies	O
change	O
as	O
functions	O
of	O
the	O
activities	O
of	O
the	O
presynaptic	O
and	O
the	O
postsynaptic	O
neurons	O
,	O
together	O
with	O
neuromodulatory	O
input	O
corresponding	O
to	O
input	O
from	O
dopamine	B
neurons	O
.	O
in	O
this	O
setting	O
,	O
each	O
synapse	O
has	O
its	O
own	O
eligibility	O
trace	O
that	O
records	O
past	O
activity	O
involving	O
that	O
synapse	O
.	O
the	O
only	O
diﬀerence	O
between	O
the	O
actor	O
and	O
critic	O
learning	O
rules	O
is	O
that	O
they	O
use	O
diﬀerent	O
kinds	O
of	O
eligibility	O
traces	O
:	O
the	O
critic	B
unit	O
’	O
s	O
traces	O
are	O
non-	O
contingent	O
because	O
they	O
do	O
not	O
involve	O
the	O
critic	B
unit	O
’	O
s	O
output	O
,	O
whereas	O
the	O
actor	O
unit	O
’	O
s	O
traces	O
are	O
contingent	O
because	O
in	O
addition	O
to	O
the	O
actor	O
unit	O
’	O
s	O
input	O
,	O
they	O
depend	O
on	O
the	O
actor	O
unit	O
’	O
s	O
output	O
.	O
in	O
the	O
hypothetical	O
implementation	O
of	O
an	O
actor–critic	B
system	O
in	O
the	O
brain	O
,	O
these	O
learning	O
rules	O
respectively	O
correspond	O
to	O
rules	O
governing	O
plasticity	O
of	O
corticostriatal	O
synapses	O
that	O
convey	O
signals	O
from	O
the	O
cortex	O
to	O
the	O
principal	O
neurons	O
in	O
the	O
dorsal	O
and	O
ventral	O
striatal	O
subdivisions	O
,	O
synapses	O
that	O
also	O
receive	O
inputs	O
from	O
dopamine	B
neurons	O
.	O
the	O
learning	O
rule	O
of	O
an	O
actor	O
unit	O
in	O
the	O
actor–critic	O
network	O
closely	O
corresponds	O
to	O
reward-modulated	O
spike-timing-dependent	O
plasticity	O
.	O
in	O
spike-timing-dependent	O
plasticity	O
(	O
stdp	O
)	O
,	O
the	O
relative	O
timing	O
of	O
pre-	O
and	O
postsynaptic	O
activity	O
determines	O
the	O
direction	O
of	O
synaptic	O
change	O
.	O
in	O
reward-modulated	O
stdp	O
,	O
changes	O
in	O
synapses	O
in	O
addition	O
depend	O
on	O
a	O
neuromodulator	O
,	O
such	O
as	O
dopamine	O
,	O
arriving	O
within	O
a	O
time	O
window	O
that	O
can	O
last	O
up	O
to	O
10	O
seconds	O
after	O
the	O
conditions	O
for	O
stdp	O
are	O
met	O
.	O
evidence	O
is	O
accumulating	B
that	O
reward-modulated	O
stdp	O
occurs	O
at	O
corticostriatal	O
synapses	O
,	O
where	O
the	O
actor	O
’	O
s	O
learning	O
takes	O
place	O
in	O
the	O
hypothetical	O
neural	B
implementation	O
of	O
an	O
actor–critic	B
system	O
,	O
adds	O
to	O
the	O
plausibility	O
of	O
the	O
hypothesis	O
that	O
something	O
like	O
an	O
actor–critic	B
system	O
exists	O
in	O
the	O
brains	O
of	O
some	O
animals	O
.	O
the	O
idea	O
of	O
synaptic	O
eligibility	O
and	O
basic	O
features	O
of	O
the	O
actor	O
learning	O
rule	O
derive	O
from	O
klopf	O
’	O
s	O
hypothesis	O
of	O
the	O
“	O
hedonistic	O
neuron	O
”	O
(	O
klopf	O
,	O
1972	O
,	O
1981	O
)	O
.	O
he	O
conjectured	O
that	O
individual	O
neurons	O
seek	O
to	O
obtain	O
reward	O
and	O
to	O
avoid	O
punishment	O
by	O
adjusting	O
the	O
eﬃcacies	O
of	O
their	O
synapses	O
on	O
the	O
basis	O
of	O
rewarding	O
or	O
punishing	O
consequences	O
of	O
their	O
action	B
potentials	O
.	O
a	O
neuron	O
’	O
s	O
activity	O
can	O
aﬀect	O
its	O
later	O
input	O
because	O
the	O
neuron	O
is	O
embedded	O
in	O
many	O
feedback	O
loops	O
,	O
some	O
within	O
the	O
animal	O
’	O
s	O
nervous	O
system	O
and	O
body	O
and	O
others	O
passing	O
through	O
the	O
animal	O
’	O
s	O
external	O
environment	B
.	O
klopf	O
’	O
s	O
idea	O
of	O
eligibility	O
is	O
that	O
synapses	O
are	O
temporarily	O
marked	O
as	O
eligible	O
for	O
modiﬁcation	O
if	O
they	O
participated	O
in	O
the	O
neuron	O
’	O
s	O
ﬁring	O
(	O
making	O
this	O
the	O
contingent	O
form	O
of	O
eligibility	O
trace	O
)	O
.	O
a	O
synapse	O
’	O
s	O
eﬃcacy	O
is	O
modiﬁed	O
if	O
a	O
reinforcing	O
signal	O
arrives	O
while	O
the	O
synapse	O
is	O
eligible	O
.	O
we	O
alluded	O
to	O
the	O
chemotactic	O
behavior	O
of	O
a	O
bacterium	O
as	O
an	O
example	O
of	O
a	O
single	O
cell	O
that	O
directs	O
its	O
movements	O
in	O
order	O
to	O
seek	O
some	O
molecules	O
and	O
to	O
avoid	O
others	O
.	O
a	O
conspicuous	O
feature	O
of	O
the	O
dopamine	B
system	O
is	O
that	O
ﬁbers	O
releasing	O
dopamine	B
project	O
widely	O
to	O
multiple	O
parts	O
of	O
the	O
brain	O
.	O
although	O
it	O
is	O
likely	O
that	O
only	O
some	O
populations	O
of	O
dopamine	O
neurons	O
broadcast	O
the	O
same	O
reinforcement	B
signal	I
,	O
if	O
this	O
signal	O
reaches	O
416	O
chapter	O
15	O
:	O
neuroscience	B
the	O
synapses	O
of	O
many	O
neurons	O
involved	O
in	O
actor-type	O
learning	O
,	O
then	O
the	O
situation	O
can	O
be	O
modeled	O
as	O
a	O
team	O
problem	O
.	O
in	O
this	O
type	O
of	O
problem	O
,	O
each	O
agent	O
in	O
a	O
collection	O
of	O
reinforcement	O
learning	O
agents	O
receives	O
the	O
same	O
reinforcement	B
signal	I
,	O
where	O
that	O
signal	O
depends	O
on	O
the	O
activities	O
of	O
all	O
members	O
of	O
the	O
collection	O
,	O
or	O
team	O
.	O
if	O
each	O
team	O
member	O
uses	O
a	O
suﬃciently	O
capable	O
learning	O
algorithm	O
,	O
the	O
team	O
can	O
learn	O
collectively	O
to	O
improve	O
performance	O
of	O
the	O
entire	O
team	O
as	O
evaluated	O
by	O
the	O
globally-broadcast	O
reinforcement	B
signal	I
,	O
even	O
if	O
the	O
team	O
members	O
do	O
not	O
directly	O
communicate	O
with	O
one	O
another	O
.	O
this	O
is	O
consistent	O
with	O
the	O
wide	O
dispersion	O
of	O
dopamine	O
signals	O
in	O
the	O
brain	O
and	O
provides	O
a	O
neurally	O
plausible	O
alternative	O
to	O
the	O
widely-used	O
error-backpropagation	O
method	O
for	O
training	O
multilayer	O
networks	O
.	O
the	O
distinction	O
between	O
model-free	O
and	O
model-based	O
reinforcement	B
learning	I
is	O
helping	O
neuroscientists	O
investigate	O
the	O
neural	B
bases	O
of	O
habitual	O
and	O
goal-directed	O
learning	O
and	O
decision	O
making	O
.	O
research	O
so	O
far	O
points	O
to	O
their	O
being	O
some	O
brain	O
regions	O
more	O
involved	O
in	O
one	O
type	O
of	O
process	O
than	O
the	O
other	O
,	O
but	O
the	O
picture	O
remains	O
unclear	O
because	O
model-	O
free	O
and	O
model-based	O
processes	O
do	O
not	O
appear	O
to	O
be	O
neatly	O
separated	O
in	O
the	O
brain	O
.	O
many	O
questions	O
remain	O
unanswered	O
.	O
perhaps	O
most	O
intriguing	O
is	O
evidence	O
that	O
the	O
hippocam-	O
pus	O
,	O
a	O
structure	O
traditionally	O
associated	O
with	O
spatial	O
navigation	O
and	O
memory	O
,	O
appears	O
to	O
be	O
involved	O
in	O
simulating	O
possible	O
future	O
courses	O
of	O
action	O
as	O
part	O
of	O
an	O
animal	O
’	O
s	O
decision-	O
making	O
process	O
.	O
this	O
suggests	O
that	O
it	O
is	O
part	O
of	O
a	O
system	O
that	O
uses	O
an	O
environment	B
model	O
for	O
planning	O
.	O
reinforcement	B
learning	I
theory	O
is	O
also	O
inﬂuencing	O
thinking	O
about	O
neural	B
processes	O
un-	O
derlying	O
drug	O
abuse	O
.	O
a	O
model	O
of	O
some	O
features	O
of	O
drug	O
addiction	B
is	O
based	O
on	O
the	O
reward	B
prediction	I
error	I
hypothesis	I
.	O
it	O
proposes	O
that	O
an	O
addicting	O
stimulant	O
,	O
such	O
as	O
cocaine	O
,	O
destabilizes	O
td	O
learning	O
to	O
produce	O
unbounded	O
growth	O
in	O
the	O
values	O
of	O
actions	O
associated	O
with	O
drug	O
intake	O
.	O
this	O
is	O
far	O
from	O
a	O
complete	O
model	O
of	O
addiction	O
,	O
but	O
it	O
illustrates	O
how	O
a	O
computational	O
perspective	O
suggests	O
theories	O
that	O
can	O
be	O
tested	O
with	O
further	O
research	O
.	O
the	O
new	O
ﬁeld	O
of	O
computational	O
psychiatry	O
similarly	O
focuses	O
on	O
the	O
use	O
of	O
computational	O
models	O
,	O
some	O
derived	O
from	O
reinforcement	B
learning	I
,	O
to	O
better	O
understand	O
mental	O
disor-	O
ders	O
.	O
this	O
chapter	O
only	O
touched	O
the	O
surface	O
of	O
how	O
the	O
neuroscience	B
of	O
reinforcement	O
learn-	O
ing	B
and	O
the	O
development	O
of	O
reinforcement	O
learning	O
in	O
computer	O
science	O
and	O
engineering	O
have	O
inﬂuenced	O
one	O
another	O
.	O
most	O
features	O
of	O
reinforcement	O
learning	O
algorithms	O
owe	O
their	O
design	O
to	O
purely	O
computational	O
considerations	O
,	O
but	O
some	O
have	O
been	O
inﬂuenced	O
by	O
hypotheses	O
about	O
neural	B
learning	O
mechanisms	O
.	O
remarkably	O
,	O
as	O
experimental	O
data	O
has	O
accumulated	O
about	O
the	O
brain	O
’	O
s	O
reward	O
processes	O
,	O
many	O
of	O
the	O
purely	O
computationally-	O
motivated	O
features	O
of	O
reinforcement	O
learning	O
algorithms	O
are	O
turning	O
out	O
to	O
be	O
consistent	O
with	O
neuroscience	O
data	O
.	O
other	O
features	O
of	O
computational	O
reinforcement	B
learning	I
,	O
such	O
eligibility	B
traces	I
and	O
the	O
ability	O
of	O
teams	O
of	O
reinforcement	O
learning	O
agents	O
to	O
learn	O
to	O
act	O
collectively	O
under	O
the	O
inﬂuence	O
of	O
a	O
globally-broadcast	O
reinforcement	B
signal	I
,	O
may	O
also	O
turn	O
out	O
to	O
parallel	O
experimental	O
data	O
as	O
neuroscientists	O
continue	O
to	O
unravel	O
the	O
neural	B
basis	O
of	O
reward-based	O
animal	O
learning	O
and	O
behavior	O
.	O
15.13.	O
summary	O
417	O
bibliographical	O
and	O
historical	O
remarks	O
the	O
number	O
of	O
publications	O
treating	O
parallels	O
between	O
the	O
neuroscience	B
of	O
learning	O
and	O
decision	O
making	O
and	O
the	O
approach	O
to	O
reinforcement	B
learning	I
presented	O
in	O
this	O
book	O
is	O
enormous	O
.	O
we	O
can	O
cite	O
only	O
a	O
small	O
selection	O
.	O
niv	O
(	O
2009	O
)	O
,	O
dayan	O
and	O
niv	O
(	O
2008	O
)	O
,	O
gimcher	O
(	O
2011	O
)	O
,	O
ludvig	O
,	O
bellemare	O
,	O
and	O
pearson	O
(	O
2011	O
)	O
,	O
and	O
shah	O
(	O
2012	O
)	O
are	O
good	O
places	O
to	O
start	O
.	O
together	O
with	O
economics	O
,	O
evolutionary	O
biology	O
,	O
and	O
mathematical	O
psychology	B
,	O
rein-	O
forcement	O
learning	O
theory	O
is	O
helping	O
to	O
formulate	O
quantitative	O
models	O
of	O
the	O
neural	B
mech-	O
anisms	O
of	O
choice	O
in	O
humans	O
and	O
non-human	O
primates	O
.	O
with	O
its	O
focus	O
on	O
learning	O
,	O
this	O
chapter	O
only	O
lightly	O
touches	O
upon	O
the	O
neuroscience	B
of	O
decision	O
making	O
.	O
glimcher	O
(	O
2003	O
)	O
introduced	O
the	O
ﬁeld	O
of	O
“	O
neuroeconomics	B
,	O
”	O
in	O
which	O
reinforcement	B
learning	I
contributes	O
to	O
the	O
study	O
of	O
the	O
neural	B
basis	O
of	O
decision	O
making	O
from	O
an	O
economics	O
perspective	O
.	O
see	O
also	O
glimcher	O
and	O
fehr	O
(	O
2013	O
)	O
.	O
the	O
text	O
on	O
computational	O
and	O
mathematical	O
modeling	O
in	B
neuroscience	I
by	O
dayan	O
and	O
abbott	O
(	O
2001	O
)	O
includes	O
reinforcement	B
learning	I
’	O
s	O
role	O
in	O
these	O
approaches	O
.	O
sterling	O
and	O
laughlin	O
(	O
2015	O
)	O
examined	O
the	O
neural	B
basis	O
of	O
learning	O
in	O
terms	O
of	O
general	O
design	O
principles	O
that	O
enable	O
eﬃcient	O
adaptive	O
behavior	O
.	O
15.1	O
15.2	O
15.3	O
there	O
are	O
many	O
good	O
expositions	O
of	O
basic	O
neuroscience	B
.	O
kandel	O
,	O
schwartz	O
,	O
jes-	O
sell	O
,	O
siegelbaum	O
,	O
and	O
hudspeth	O
(	O
2013	O
)	O
is	O
an	O
authoritative	O
and	O
very	O
comprehen-	O
sive	O
source	O
.	O
berridge	O
and	O
kringelbach	O
(	O
2008	O
)	O
reviewed	O
the	O
neural	B
basis	O
of	O
reward	O
and	O
plea-	O
sure	O
,	O
pointing	O
out	O
that	O
reward	O
processing	O
has	O
many	O
dimensions	O
and	O
involves	O
many	O
neural	B
systems	O
.	O
space	O
prevents	O
discussion	O
of	O
the	O
inﬂuential	O
research	O
of	O
berridge	O
and	O
robinson	O
(	O
1998	O
)	O
,	O
who	O
distinguish	O
between	O
the	O
hedonic	O
impact	O
of	O
a	O
stimulus	O
,	O
which	O
they	O
call	O
“	O
liking	O
,	O
”	O
and	O
the	O
motivational	O
eﬀect	O
,	O
which	O
they	O
call	O
“	O
wanting.	O
”	O
hare	O
,	O
o	O
’	O
doherty	O
,	O
camerer	O
,	O
schultz	O
,	O
and	O
rangel	O
(	O
2008	O
)	O
examined	O
the	O
neural	B
basis	O
of	O
value-related	O
signals	O
from	O
an	O
economic	O
perspective	O
,	O
distin-	O
guishing	O
between	O
goal	B
values	O
,	O
decision	O
values	O
,	O
and	O
prediction	O
errors	O
.	O
decision	O
value	B
is	O
goal	B
value	O
minus	O
action	B
cost	O
.	O
see	O
also	O
rangel	O
,	O
camerer	O
,	O
and	O
montague	O
(	O
2008	O
)	O
,	O
rangel	O
and	O
hare	O
(	O
2010	O
)	O
,	O
and	O
peters	O
and	O
b¨uchel	O
(	O
2010	O
)	O
.	O
the	O
reward	B
prediction	I
error	I
hypothesis	I
of	O
dopamine	B
neuron	O
activity	O
is	O
most	O
prominently	O
discussed	O
by	O
schultz	O
,	O
dayan	O
,	O
and	O
montague	O
(	O
1997	O
)	O
.	O
the	O
hypothe-	O
sis	O
was	O
ﬁrst	O
explicitly	O
put	O
forward	O
by	O
montague	O
,	O
dayan	O
,	O
and	O
sejnowski	O
(	O
1996	O
)	O
.	O
as	O
they	O
stated	O
the	O
hypothesis	O
,	O
it	O
referred	O
to	O
reward	O
prediction	O
errors	O
(	O
rpes	O
)	O
but	O
not	O
speciﬁcally	O
to	O
td	O
errors	O
;	O
however	O
,	O
their	O
development	O
of	O
the	O
hypothesis	O
made	O
it	O
clear	O
that	O
they	O
were	O
referring	O
to	O
td	O
errors	O
.	O
the	O
earliest	O
recognition	O
of	O
the	O
td-	O
error/dopamine	O
connection	O
of	O
which	O
we	O
are	O
aware	O
is	O
that	O
of	O
montague	O
,	O
dayan	O
,	O
nowlan	O
,	O
pouget	O
,	O
and	O
sejnowski	O
(	O
1993	O
)	O
,	O
who	O
proposed	O
a	O
td-error-modulated	O
hebbian	O
learning	O
rule	O
motivated	O
by	O
results	O
on	O
dopamine	B
signaling	O
from	O
schultz	O
’	O
s	O
group	O
.	O
the	O
connection	O
was	O
also	O
pointed	O
out	O
in	O
an	O
abstract	O
by	O
quartz	O
,	O
dayan	O
,	O
montague	O
,	O
and	O
sejnowski	O
(	O
1992	O
)	O
.	O
montague	O
and	O
sejnowski	O
(	O
1994	O
)	O
emphasized	O
the	O
importance	O
of	O
prediction	B
in	O
the	O
brain	O
and	O
outlined	O
how	O
predictive	O
hebbian	O
418	O
chapter	O
15	O
:	O
neuroscience	B
learning	O
modulated	O
by	O
td	O
errors	O
could	O
be	O
implemented	O
via	O
a	O
diﬀuse	O
neuromod-	O
ulatory	O
system	O
,	O
such	O
as	O
the	O
dopamine	B
system	O
.	O
friston	O
,	O
tononi	O
,	O
reeke	O
,	O
sporns	O
,	O
and	O
edelman	O
(	O
1994	O
)	O
presented	O
a	O
model	O
of	O
value-dependent	O
learning	O
in	O
the	O
brain	O
in	O
which	O
synaptic	O
changes	O
are	O
mediated	O
by	O
a	O
td-like	O
error	O
provided	O
by	O
a	O
global	O
neuromodulatory	O
signal	O
(	O
although	O
they	O
did	O
not	O
single	O
out	O
dopamine	B
)	O
.	O
montague	O
,	O
dayan	O
,	O
person	O
,	O
and	O
sejnowski	O
(	O
1995	O
)	O
presented	O
a	O
model	O
of	O
honeybee	O
foraging	O
using	O
the	O
td	O
error	O
.	O
the	O
model	O
is	O
based	O
on	O
research	O
by	O
hammer	O
,	O
menzel	O
,	O
and	O
colleagues	O
(	O
hammer	O
and	O
menzel	O
,	O
1995	O
;	O
hammer	O
,	O
1997	O
)	O
showing	O
that	O
the	O
neu-	O
romodulator	O
octopamine	O
acts	O
as	O
a	O
reinforcement	B
signal	I
in	O
the	O
honeybee	O
.	O
mon-	O
tague	O
et	O
al	O
.	O
(	O
1995	O
)	O
pointed	O
out	O
that	O
dopamine	B
likely	O
plays	O
a	O
similar	O
role	O
in	O
the	O
vertebrate	O
brain	O
.	O
barto	O
(	O
1995a	O
)	O
related	O
the	O
actor–critic	B
architecture	O
to	O
basal-	O
ganglionic	O
circuits	O
and	O
discussed	O
the	O
relationship	O
between	O
td	O
learning	O
and	O
the	O
main	O
results	O
from	O
schultz	O
’	O
s	O
group	O
.	O
houk	O
,	O
adams	O
,	O
and	O
barto	O
(	O
1995	O
)	O
suggested	O
how	O
td	O
learning	O
and	O
the	O
actor–critic	B
architecture	O
might	O
map	O
onto	O
the	O
anatomy	O
,	O
physiology	O
,	O
and	O
molecular	O
mechanism	O
of	O
the	O
basal	B
ganglia	I
.	O
doya	O
and	O
sejnowski	O
(	O
1998	O
)	O
extended	O
their	O
earlier	O
paper	O
on	O
a	O
model	O
of	O
birdsong	O
learning	O
(	O
doya	O
and	O
sejnowski	O
,	O
1995	O
)	O
by	O
including	O
a	O
td-like	O
error	O
identiﬁed	O
with	O
dopamine	O
to	O
rein-	O
force	O
the	O
selection	O
of	O
auditory	O
input	O
to	O
be	O
memorized	O
.	O
o	O
’	O
reilly	O
and	O
frank	O
(	O
2006	O
)	O
and	O
o	O
’	O
reilly	O
,	O
frank	O
,	O
hazy	O
,	O
and	O
watz	O
(	O
2007	O
)	O
argued	O
that	O
phasic	O
dopamine	B
sig-	O
nals	O
are	O
rpes	O
but	O
not	O
td	O
errors	O
.	O
in	O
support	O
of	O
their	O
theory	O
they	O
cited	O
results	O
with	O
variable	O
interstimulus	O
intervals	O
that	O
do	O
not	O
match	O
predictions	O
of	O
a	O
simple	O
td	O
model	O
,	O
as	O
well	O
as	O
the	O
observation	O
that	O
higher-order	O
conditioning	B
beyond	O
second-order	O
conditioning	B
is	O
rarely	O
observed	O
,	O
while	O
td	O
learning	O
is	O
not	O
so	O
lim-	O
ited	O
.	O
dayan	O
and	O
niv	O
(	O
2008	O
)	O
discussed	O
“	O
the	O
good	O
,	O
the	O
bad	O
,	O
and	O
the	O
ugly	O
”	O
of	O
how	O
reinforcement	B
learning	I
theory	O
and	O
the	O
reward	O
prediction	B
error	O
hypothesis	O
align	O
with	O
experimental	O
data	O
.	O
glimcher	O
(	O
2011	O
)	O
reviewed	O
the	O
empirical	O
ﬁndings	O
that	O
support	O
the	O
reward	B
prediction	I
error	I
hypothesis	I
and	O
emphasized	O
the	O
signiﬁcance	O
of	O
the	O
hypothesis	O
for	O
contemporary	O
neuroscience	B
.	O
15.4	O
graybiel	O
(	O
2000	O
)	O
is	O
a	O
brief	O
primer	O
on	O
the	O
basal	B
ganglia	I
.	O
the	O
experiments	O
men-	O
tioned	O
that	O
involve	O
optogenetic	O
activation	O
of	O
dopamine	O
neurons	O
were	O
conducted	O
by	O
tsai	O
,	O
zhang	O
,	O
adamantidis	O
,	O
stuber	O
,	O
bonci	O
,	O
de	O
lecea	O
,	O
and	O
deisseroth	O
(	O
2009	O
)	O
,	O
steinberg	O
,	O
keiﬂin	O
,	O
boivin	O
,	O
witten	O
,	O
deisseroth	O
,	O
and	O
janak	O
(	O
2013	O
)	O
,	O
and	O
claridge-	O
chang	O
,	O
roorda	O
,	O
vrontou	O
,	O
sjulson	O
,	O
li	O
,	O
hirsh	O
,	O
and	O
miesenb¨ock	O
(	O
2009	O
)	O
.	O
fiorillo	O
,	O
yun	O
,	O
and	O
song	O
(	O
2013	O
)	O
,	O
lammel	O
,	O
lim	O
,	O
and	O
malenka	O
(	O
2014	O
)	O
,	O
and	O
saddoris	O
,	O
cac-	O
ciapaglia	O
,	O
wightmman	O
,	O
and	O
carelli	O
(	O
2015	O
)	O
are	O
among	O
studies	O
showing	O
that	O
the	O
signaling	O
properties	O
of	O
dopamine	O
neurons	O
are	O
specialized	O
for	O
diﬀerent	O
target	B
re-	O
gions	O
.	O
rpe-signaling	O
neurons	O
may	O
belong	O
to	O
one	O
among	O
multiple	O
populations	O
of	O
dopamine	O
neurons	O
having	O
diﬀerent	O
targets	O
and	O
subserving	O
diﬀerent	O
functions	O
.	O
eshel	O
,	O
tian	O
,	O
bukwich	O
,	O
and	O
uchida	O
(	O
2016	O
)	O
found	O
homogeneity	O
of	O
reward	O
predic-	O
tion	B
error	O
responses	O
of	O
dopamine	O
neurons	O
in	O
the	O
lateral	O
vta	O
during	O
classical	B
conditioning	I
in	O
mice	O
,	O
though	O
their	O
results	O
do	O
not	O
rule	O
out	O
response	O
diversity	O
across	O
wider	O
areas	O
.	O
gershman	O
,	O
pesaran	O
,	O
and	O
daw	O
(	O
2009	O
)	O
studied	O
reinforcement	B
learning	I
tasks	O
that	O
can	O
be	O
decomposed	O
into	O
independent	O
components	O
with	O
sep-	O
arate	O
reward	O
signals	O
,	O
ﬁnding	O
evidence	O
in	O
human	O
neuroimaging	O
data	O
suggesting	O
15.13.	O
summary	O
419	O
15.5	O
15.6	O
15.7	O
that	O
the	O
brain	O
exploits	O
this	O
kind	O
of	O
structure	O
.	O
schultz	O
’	O
s	O
1998	O
survey	O
article	O
is	O
a	O
good	O
entr´ee	O
into	O
the	O
very	O
extensive	O
literature	O
on	O
reward	O
predicting	O
signaling	O
of	O
dopamine	O
neurons	O
.	O
berns	O
,	O
mcclure	O
,	O
pagnoni	O
,	O
and	O
montague	O
(	O
2001	O
)	O
,	O
breiter	O
,	O
aharon	O
,	O
kahneman	O
,	O
dale	O
,	O
and	O
shizgal	O
(	O
2001	O
)	O
,	O
pagnoni	O
,	O
zink	O
,	O
montague	O
,	O
and	O
berns	O
(	O
2002	O
)	O
,	O
and	O
o	O
’	O
doherty	O
,	O
dayan	O
,	O
friston	O
,	O
critchley	O
,	O
and	O
dolan	O
(	O
2003	O
)	O
described	O
functional	O
brain	O
imaging	O
studies	O
support-	O
ing	B
the	O
existence	O
of	O
signals	O
like	O
td	O
errors	O
in	O
the	O
human	O
brain	O
.	O
this	O
section	O
roughly	O
follows	O
barto	O
(	O
1995a	O
)	O
in	O
explaining	O
how	O
td	O
errors	O
mimic	O
the	O
main	O
results	O
from	O
schultz	O
’	O
s	O
group	O
on	O
the	O
phasic	O
responses	O
of	O
dopamine	O
neu-	O
rons	O
.	O
this	O
section	O
is	O
largely	O
based	O
on	O
takahashi	O
,	O
schoenbaum	O
,	O
and	O
niv	O
(	O
2008	O
)	O
and	O
niv	O
(	O
2009	O
)	O
.	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
barto	O
(	O
1995a	O
)	O
and	O
houk	O
,	O
adams	O
,	O
and	O
barto	O
(	O
1995	O
)	O
ﬁrst	O
speculated	O
about	O
possible	O
implementations	O
of	O
actor–critic	O
algo-	O
rithms	O
in	O
the	O
basal	O
ganglia	O
.	O
on	O
the	O
basis	O
of	O
functional	O
magnetic	O
resonance	O
imag-	O
ing	B
of	O
human	O
subjects	O
while	O
engaged	O
in	O
instrumental	O
conditioning	B
,	O
o	O
’	O
doherty	O
,	O
dayan	O
,	O
schultz	O
,	O
deichmann	O
,	O
friston	O
,	O
and	O
dolan	O
(	O
2004	O
)	O
suggested	O
that	O
the	O
ac-	O
tor	O
and	O
the	O
critic	O
are	O
most	O
likely	O
located	O
respectively	O
in	O
the	O
dorsal	O
and	O
ventral	O
striatum	O
.	O
gershman	O
,	O
moustafa	O
,	O
and	O
ludvig	O
(	O
2014	O
)	O
focused	O
on	O
how	O
time	O
is	O
repre-	O
sented	O
in	O
reinforcement	O
learning	O
models	O
of	O
the	O
basal	B
ganglia	I
,	O
discussing	O
evidence	O
for	O
,	O
and	O
implications	O
of	O
,	O
various	O
computational	O
approaches	O
to	O
time	O
representa-	O
tion	B
.	O
the	O
hypothetical	O
neural	B
implementation	O
of	O
the	O
actor–critic	B
architecture	O
de-	O
scribed	O
in	O
this	O
section	O
includes	O
very	O
little	O
detail	O
about	O
known	O
basal	B
ganglia	I
anatomy	O
and	O
physiology	O
.	O
in	O
addition	O
to	O
the	O
more	O
detailed	O
hypothesis	O
of	O
houk	O
,	O
adams	O
,	O
and	O
barto	O
(	O
1995	O
)	O
,	O
a	O
number	O
of	O
other	O
hypotheses	O
include	O
more	O
speciﬁc	O
connections	O
to	O
anatomy	O
and	O
physiology	O
and	O
are	O
claimed	O
to	O
explain	O
additional	O
data	O
.	O
these	O
include	O
hypotheses	O
proposed	O
by	O
suri	O
and	O
schultz	O
(	O
1998	O
,	O
1999	O
)	O
,	O
brown	O
,	O
bullock	O
,	O
and	O
grossberg	O
(	O
1999	O
)	O
,	O
contreras-vidal	O
and	O
schultz	O
(	O
1999	O
)	O
,	O
suri	O
,	O
bargas	O
,	O
and	O
arbib	O
(	O
2001	O
)	O
,	O
o	O
’	O
reilly	O
and	O
frank	O
(	O
2006	O
)	O
,	O
and	O
o	O
’	O
reilly	O
,	O
frank	O
,	O
hazy	O
,	O
and	O
watz	O
(	O
2007	O
)	O
.	O
joel	O
,	O
niv	O
,	O
and	O
ruppin	O
(	O
2002	O
)	O
critically	O
evaluated	O
the	O
anatomical	O
plausibility	O
of	O
several	O
of	O
these	O
models	O
and	O
present	O
an	O
alternative	O
in-	O
tended	O
to	O
accommodate	O
some	O
neglected	O
features	O
of	O
basal	O
ganglionic	O
circuitry	O
.	O
15.8	O
the	O
actor	O
learning	O
rule	O
discussed	O
here	O
is	O
more	O
complicated	O
than	O
the	O
one	O
in	O
the	O
early	O
actor–critic	B
network	O
of	O
barto	O
et	O
al	O
.	O
(	O
1983	O
)	O
.	O
actor-unit	O
eligibility	B
traces	I
in	O
that	O
network	O
were	O
traces	O
of	O
just	O
at	O
×	O
x	O
(	O
st	O
)	O
instead	O
of	O
the	O
full	O
(	O
at	O
−	O
π	O
(	O
at|st	O
,	O
θ	O
)	O
)	O
x	O
(	O
st	O
)	O
.	O
that	O
work	O
did	O
not	O
beneﬁt	O
from	O
the	O
policy-gradient	O
theory	O
presented	O
in	O
chapter	O
13	O
or	O
the	O
contributions	O
of	O
williams	O
(	O
1986	O
,	O
1992	O
)	O
,	O
who	O
showed	O
how	O
an	O
artiﬁcial	O
neural	O
network	O
of	O
bernoulli-logistic	O
units	O
could	O
imple-	O
ment	O
a	O
policy-gradient	O
method	O
.	O
420	O
chapter	O
15	O
:	O
neuroscience	B
reynolds	O
and	O
wickens	O
(	O
2002	O
)	O
proposed	O
a	O
three-factor	O
rule	O
for	O
synaptic	O
plasticity	O
in	O
the	O
corticostriatal	O
pathway	O
in	O
which	O
dopamine	B
modulates	O
changes	O
in	O
corti-	O
costriatal	O
synaptic	O
eﬃcacy	O
.	O
they	O
discussed	O
the	O
experimental	O
support	O
for	O
this	O
kind	O
of	O
learning	O
rule	O
and	O
its	O
possible	O
molecular	O
basis	O
.	O
the	O
deﬁnitive	O
demon-	O
stration	O
of	O
spike-timing-dependent	O
plasticity	O
(	O
stdp	O
)	O
is	O
attributed	O
to	O
markram	O
,	O
l¨ubke	O
,	O
frotscher	O
,	O
and	O
sakmann	O
(	O
1997	O
)	O
,	O
with	O
evidence	O
from	O
earlier	O
experiments	O
by	O
levy	O
and	O
steward	O
(	O
1983	O
)	O
and	O
others	O
that	O
the	O
relative	O
timing	O
of	O
pre-	O
and	O
postsynaptic	O
spikes	O
is	O
critical	O
for	O
inducing	O
changes	O
in	O
synaptic	O
eﬃcacy	O
.	O
rao	O
and	O
sejnowski	O
(	O
2001	O
)	O
suggested	O
how	O
stdp	O
could	O
be	O
the	O
result	O
of	O
a	O
td-like	O
mechanism	O
at	O
synapses	O
with	O
non-contingent	O
eligibility	B
traces	I
lasting	O
about	O
10	O
milliseconds	O
.	O
dayan	O
(	O
2002	O
)	O
commented	O
that	O
this	O
would	O
require	O
an	O
error	O
as	O
in	O
sutton	O
and	O
barto	O
’	O
s	O
(	O
1981a	O
)	O
early	O
model	O
of	O
classical	O
conditioning	B
and	O
not	O
a	O
true	O
td	O
error	O
.	O
representative	O
publications	O
from	O
the	O
extensive	O
literature	O
on	O
reward-	O
modulated	O
stdp	O
are	O
wickens	O
(	O
1990	O
)	O
,	O
reynolds	O
and	O
wickens	O
(	O
2002	O
)	O
,	O
and	O
cal-	O
abresi	O
,	O
picconi	O
,	O
tozzi	O
and	O
di	O
filippo	O
(	O
2007	O
)	O
.	O
pawlak	O
and	O
kerr	O
(	O
2008	O
)	O
showed	O
that	O
dopamine	B
is	O
necessary	O
to	O
induce	O
stdp	O
at	O
the	O
corticostriatal	O
synapses	O
of	O
medium	O
spiny	O
neurons	O
.	O
see	O
also	O
pawlak	O
,	O
wickens	O
,	O
kirkwood	O
,	O
and	O
kerr	O
(	O
2010	O
)	O
.	O
yagishita	O
,	O
hayashi-takagi	O
,	O
ellis-davies	O
,	O
urakubo	O
,	O
ishii	O
,	O
and	O
kasai	O
(	O
2014	O
)	O
found	O
that	O
dopamine	B
promotes	O
spine	O
enlargement	O
of	O
the	O
medium	O
spiny	O
neurons	O
of	O
mice	O
only	O
during	O
a	O
time	O
window	O
of	O
from	O
0.3	O
to	O
2	O
seconds	O
after	O
stdp	O
stimulation	O
.	O
izhikevich	O
(	O
2007	O
)	O
proposed	O
and	O
explored	O
the	O
idea	O
of	O
using	O
stdp	O
timing	O
condi-	O
tions	O
to	O
trigger	O
contingent	O
eligibility	O
traces	O
.	O
fr´emaux	O
,	O
sprekeler	O
,	O
and	O
gerstner	O
(	O
2010	O
)	O
proposed	O
theoretical	O
conditions	O
for	O
successful	O
learning	O
by	O
rules	O
based	O
on	O
reward-modulated	O
stdp	O
.	O
15.9	O
klopf	O
’	O
s	O
hedonistic	O
neuron	O
hypothesis	O
(	O
klopf	O
1972	O
,	O
1982	O
)	O
inspired	O
our	O
actor–critic	B
algorithm	O
implemented	O
as	O
an	O
artiﬁcial	O
neural	O
network	O
with	O
a	O
single	O
neuron-	O
like	O
unit	O
,	O
called	O
the	O
actor	O
unit	O
,	O
implementing	O
a	O
law-of-eﬀect-like	O
learning	O
rule	O
(	O
barto	O
,	O
sutton	O
,	O
and	O
anderson	O
,	O
1983	O
)	O
.	O
ideas	O
related	O
to	O
klopf	O
’	O
s	O
synaptically-local	O
eligibility	O
have	O
been	O
proposed	O
by	O
others	O
.	O
crow	O
(	O
1968	O
)	O
proposed	O
that	O
changes	O
in	O
the	O
synapses	O
of	O
cortical	O
neurons	O
are	O
sensitive	O
to	O
the	O
consequences	O
of	O
neural	O
activity	O
.	O
emphasizing	O
the	O
need	O
to	O
address	O
the	O
time	O
delay	O
between	O
neural	B
ac-	O
tivity	O
and	O
its	O
consequences	O
in	O
a	O
reward-modulated	O
form	O
of	O
synaptic	O
plasticity	O
,	O
he	O
proposed	O
a	O
contingent	O
form	O
of	O
eligibility	O
,	O
but	O
associated	O
with	O
entire	O
neurons	O
instead	O
of	O
individual	O
synapses	O
.	O
according	O
to	O
his	O
hypothesis	O
,	O
a	O
wave	O
of	O
neuronal	O
activity	O
leads	O
to	O
a	O
short-term	O
change	O
in	O
the	O
cells	O
involved	O
in	O
the	O
wave	O
such	O
that	O
they	O
are	O
picked	O
out	O
from	O
a	O
background	O
of	O
cells	O
not	O
so	O
activated	O
.	O
...	O
such	O
cells	O
are	O
rendered	O
sensitive	O
by	O
the	O
short-term	O
change	O
to	O
a	O
reward	B
signal	I
...	O
in	O
such	O
a	O
way	O
that	O
if	O
such	O
a	O
signal	O
occurs	O
before	O
the	O
end	O
of	O
the	O
decay	O
time	O
of	O
the	O
change	O
the	O
synaptic	O
connexions	O
between	O
the	O
cells	O
are	O
made	O
more	O
eﬀective	O
.	O
(	O
crow	O
,	O
1968	O
)	O
crow	O
argued	O
against	O
previous	O
proposals	O
that	O
reverberating	O
neural	B
circuits	O
play	O
this	O
role	O
by	O
pointing	O
out	O
that	O
the	O
eﬀect	O
of	O
a	O
reward	B
signal	I
on	O
such	O
a	O
circuit	O
would	O
15.13.	O
summary	O
421	O
“	O
...	O
establish	O
the	O
synaptic	O
connexions	O
leading	O
to	O
the	O
reverberation	O
(	O
that	O
is	O
to	O
say	O
,	O
those	O
involved	O
in	O
activity	O
at	O
the	O
time	O
of	O
the	O
reward	B
signal	I
)	O
and	O
not	O
those	O
on	O
the	O
path	O
which	O
led	O
to	O
the	O
adaptive	O
motor	O
output.	O
”	O
crow	O
further	O
postulated	O
that	O
reward	O
signals	O
are	O
delivered	O
via	O
a	O
“	O
distinct	O
neural	B
ﬁber	O
system	O
,	O
”	O
presumably	O
the	O
one	O
into	O
which	O
olds	O
and	O
milner	O
(	O
1954	O
)	O
tapped	O
,	O
that	O
would	O
transform	O
synaptic	O
connections	O
“	O
from	O
a	O
short	O
into	O
a	O
long-term	O
form.	O
”	O
in	O
another	O
farsighted	O
hypothesis	O
,	O
miller	O
proposed	O
a	O
law-of-eﬀect-like	O
learning	O
rule	O
that	O
includes	O
synaptically-local	O
contingent	O
eligibility	O
traces	O
:	O
it	O
is	O
envisaged	O
that	O
in	O
a	O
particular	O
sensory	O
situation	O
neurone	O
b	O
,	O
by	O
...	O
chance	O
,	O
ﬁres	O
a	O
‘	O
meaningful	O
burst	O
’	O
of	O
activity	O
,	O
which	O
is	O
then	O
translated	O
into	O
motor	O
acts	O
,	O
which	O
then	O
change	O
the	O
situation	O
.	O
it	O
must	O
be	O
supposed	O
that	O
the	O
meaningful	O
burst	O
has	O
an	O
inﬂuence	O
,	O
at	O
the	O
neuronal	O
level	O
,	O
on	O
all	O
of	O
its	O
own	O
synapses	O
which	O
are	O
active	O
at	O
the	O
time	O
...	O
thereby	O
making	O
a	O
prelimi-	O
nary	O
selection	O
of	O
the	O
synapses	O
to	O
be	O
strengthened	O
,	O
though	O
not	O
yet	O
actually	O
strengthening	O
them	O
.	O
...	O
the	O
strengthening	O
signal	O
...	O
makes	O
the	O
ﬁnal	O
selec-	O
tion	B
...	O
and	O
accomplishes	O
the	O
deﬁnitive	O
change	O
in	O
the	O
appropriate	O
synapses	O
.	O
(	O
miller	O
,	O
1981	O
,	O
p.	O
81	O
)	O
miller	O
’	O
s	O
hypothesis	O
also	O
included	O
a	O
critic-like	O
mechanism	O
,	O
which	O
he	O
called	O
a	O
“	O
sen-	O
sory	O
analyzer	O
unit	O
,	O
”	O
that	O
worked	O
according	O
to	O
classical	B
conditioning	I
principles	O
to	O
provide	O
reinforcement	O
signals	O
to	O
neurons	O
so	O
that	O
they	O
would	O
learn	O
to	O
move	O
from	O
lower-	O
to	O
higher-valued	O
states	O
,	O
thus	O
anticipating	O
the	O
use	O
of	O
the	O
td	O
error	O
as	O
a	O
rein-	O
forcement	O
signal	O
in	O
the	O
actor–critic	O
architecture	O
.	O
miller	O
’	O
s	O
idea	O
not	O
only	O
parallels	O
klopf	O
’	O
s	O
(	O
with	O
the	O
exception	O
of	O
its	O
explicit	O
invocation	O
of	O
a	O
distinct	O
“	O
strengthening	O
signal	O
”	O
)	O
,	O
it	O
also	O
anticipated	O
the	O
general	O
features	O
of	O
reward-modulated	O
stdp	O
.	O
a	O
related	O
though	O
diﬀerent	O
idea	O
,	O
which	O
seung	O
(	O
2003	O
)	O
called	O
the	O
“	O
hedonistic	O
synapse	O
,	O
”	O
is	O
that	O
synapses	O
individually	O
adjust	O
the	O
probability	O
that	O
they	O
release	O
neurotrans-	O
mitter	O
in	O
the	O
manner	O
of	O
the	O
law	O
of	O
eﬀect	O
:	O
if	O
reward	O
follows	O
release	O
,	O
the	O
release	O
probability	O
increases	O
,	O
and	O
decreases	O
if	O
reward	O
follows	O
failure	O
to	O
release	O
.	O
this	O
is	O
essentially	O
the	O
same	O
as	O
the	O
learning	O
scheme	O
minsky	O
used	O
in	O
his	O
1954	O
princeton	O
ph.d.	O
dissertation	O
,	O
where	O
he	O
called	O
the	O
synapse-like	O
learning	O
element	O
a	O
snarc	O
(	O
stochastic	O
neural-analog	O
reinforcement	O
calculator	O
)	O
.	O
contingent	O
eligibility	O
is	O
involved	O
in	O
these	O
ideas	O
too	O
,	O
although	O
it	O
is	O
contingent	O
on	O
the	O
activity	O
of	O
an	O
indi-	O
vidual	O
synapse	O
instead	O
of	O
the	O
postsynaptic	O
neuron	O
.	O
also	O
related	O
is	O
the	O
proposal	O
of	O
unnikrishman	O
and	O
venugopal	O
(	O
1994	O
)	O
that	O
uses	O
the	O
correlation-based	O
method	O
of	O
harth	O
and	O
tzanakou	O
(	O
1974	O
)	O
to	O
adjust	O
neural	B
network	O
weights	O
.	O
frey	O
and	O
morris	O
(	O
1997	O
)	O
proposed	O
the	O
idea	O
of	O
a	O
“	O
synaptic	O
tag	O
”	O
for	O
the	O
induction	O
of	O
long-lasting	O
strengthening	O
of	O
synaptic	O
eﬃcacy	O
.	O
though	O
not	O
unlike	O
klopf	O
’	O
s	O
eligibility	O
,	O
their	O
tag	O
was	O
hypothesized	O
to	O
consist	O
of	O
a	O
temporary	O
strengthening	O
of	O
a	O
synapse	O
that	O
could	O
be	O
transformed	O
into	O
a	O
long-lasting	O
strengthening	O
by	O
subse-	O
quent	O
neuron	O
activation	O
.	O
the	O
model	O
of	O
o	O
’	O
reilly	O
and	O
frank	O
(	O
2006	O
)	O
and	O
o	O
’	O
reilly	O
,	O
frank	O
,	O
hazy	O
,	O
and	O
watz	O
(	O
2007	O
)	O
uses	O
working	O
memory	O
to	O
bridge	O
temporal	O
intervals	O
instead	O
of	O
eligibility	O
traces	O
.	O
wickens	O
and	O
kotter	O
(	O
1995	O
)	O
discuss	O
possible	O
mecha-	O
nisms	O
for	O
synaptic	O
eligibility	O
.	O
he	O
,	O
huertas	O
,	O
hong	O
,	O
tie	O
,	O
hell	O
,	O
shouval	O
,	O
kirkwood	O
422	O
chapter	O
15	O
:	O
neuroscience	B
(	O
2015	O
)	O
provide	O
evidence	O
supporting	O
the	O
existence	O
of	O
contingent	O
eligibility	B
traces	I
in	O
synapses	O
of	O
cortical	O
neurons	O
with	O
time	O
courses	O
like	O
those	O
of	O
the	O
eligibility	B
traces	I
klopf	O
postulated	O
.	O
the	O
metaphor	O
of	O
a	O
neuron	O
using	O
a	O
learning	O
rule	O
related	O
to	O
bacterial	O
chemotaxis	O
was	O
discussed	O
by	O
barto	O
(	O
1989	O
)	O
.	O
koshland	O
’	O
s	O
extensive	O
study	O
of	O
bacterial	O
chemo-	O
taxis	O
was	O
in	O
part	O
motivated	O
by	O
similarities	O
between	O
features	O
of	O
bacteria	O
and	O
features	O
of	O
neurons	O
(	O
koshland	O
,	O
1980	O
)	O
.	O
see	O
also	O
berg	O
(	O
1975	O
)	O
.	O
shimansky	O
(	O
2009	O
)	O
proposed	O
a	O
synaptic	O
learning	O
rule	O
somewhat	O
similar	O
to	O
seung	O
’	O
s	O
mentioned	O
above	O
in	O
which	O
each	O
synapse	O
individually	O
acts	O
like	O
a	O
chemotactic	O
bacterium	O
.	O
in	O
this	O
case	O
a	O
collection	O
of	O
synapses	O
“	O
swims	O
”	O
toward	O
attractants	O
in	O
the	O
high-dimensional	O
space	O
of	O
synaptic	O
weight	O
values	O
.	O
montague	O
,	O
dayan	O
,	O
person	O
,	O
and	O
sejnowski	O
(	O
1995	O
)	O
proposed	O
a	O
chemotactic-like	O
model	O
of	O
the	O
bee	O
’	O
s	O
foraging	O
behavior	O
involving	O
the	O
neuromodulator	O
octopamine	O
.	O
15.10	O
research	O
on	O
the	O
behavior	O
of	O
reinforcement	B
learning	I
agents	O
in	O
team	O
and	O
game	O
problems	O
has	O
a	O
long	O
history	O
roughly	O
occurring	O
in	O
three	O
phases	O
.	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
the	O
ﬁrst	O
phase	O
began	O
with	O
investigations	O
by	O
the	O
russian	O
math-	O
ematician	O
and	O
physicist	O
m.	O
l.	O
tsetlin	O
.	O
a	O
collection	O
of	O
his	O
work	O
was	O
published	O
as	O
tsetlin	O
(	O
1973	O
)	O
after	O
his	O
death	O
in	O
1966.	O
our	O
sections	O
1.7	O
and	O
4.8	O
refer	O
to	O
his	O
study	O
of	O
learning	O
automata	O
in	O
connection	O
to	O
bandit	B
problems	I
.	O
the	O
tsetlin	O
col-	O
lection	O
also	O
includes	O
studies	O
of	O
learning	O
automata	O
in	O
team	O
and	O
game	O
problems	O
,	O
which	O
led	O
to	O
later	O
work	O
in	O
this	O
area	O
using	O
stochastic	O
learning	O
automata	O
as	O
de-	O
scribed	O
by	O
narendra	O
and	O
thathachar	O
(	O
1974	O
,	O
1989	O
)	O
,	O
viswanathan	O
and	O
narendra	O
(	O
1974	O
)	O
,	O
lakshmivarahan	O
and	O
narendra	O
(	O
1982	O
)	O
,	O
narendra	O
and	O
wheeler	O
(	O
1983	O
)	O
,	O
and	O
thathachar	O
and	O
sastry	O
(	O
2002	O
)	O
.	O
thathachar	O
and	O
sastry	O
(	O
2011	O
)	O
is	O
a	O
more	O
recent	O
comprehensive	O
account	O
.	O
these	O
studies	O
were	O
mostly	O
restricted	O
to	O
non-	O
associative	O
learning	O
automata	O
,	O
meaning	O
that	O
they	O
did	O
not	O
address	O
associative	O
,	O
or	O
contextual	O
,	O
bandit	B
problems	I
(	O
section	O
2.9	O
)	O
.	O
the	O
second	O
phase	O
began	O
with	O
the	O
extension	O
of	O
learning	O
automata	O
to	O
the	O
asso-	O
ciative	O
,	O
or	O
contextual	O
,	O
case	O
.	O
barto	O
,	O
sutton	O
,	O
and	O
brouwer	O
(	O
1981	O
)	O
and	O
barto	O
and	O
sutton	O
(	O
1981b	O
)	O
experimented	O
with	O
associative	O
stochastic	O
learning	O
automata	O
in	O
single-layer	O
artiﬁcial	B
neural	I
networks	I
to	O
which	O
a	O
global	O
reinforcement	B
signal	I
was	O
broadcast	O
.	O
the	O
learning	O
algorithm	O
was	O
an	O
associative	O
extension	O
of	O
the	O
alopex	O
algorithm	O
of	O
harth	O
and	O
tzanakou	O
(	O
1974	O
)	O
.	O
barto	O
et	O
al	O
.	O
called	O
neuron-like	O
ele-	O
ments	O
implementing	O
this	O
kind	O
of	O
learning	O
associative	B
search	I
elements	O
(	O
ases	O
)	O
.	O
barto	O
and	O
anandan	O
(	O
1985	O
)	O
introduced	O
an	O
associative	B
reinforcement	I
learning	I
al-	O
gorithm	O
called	O
the	O
associative	O
reward-penalty	O
(	O
ar−p	O
)	O
algorithm	O
.	O
they	O
proved	O
a	O
convergence	O
result	O
by	O
combining	O
theory	O
of	O
stochastic	O
learning	B
automata	I
with	O
the-	O
ory	O
of	O
pattern	O
classiﬁcation	O
.	O
barto	O
(	O
1985	O
,	O
1986	O
)	O
and	O
barto	O
and	O
jordan	O
(	O
1987	O
)	O
described	O
results	O
with	O
teams	O
of	O
ar−p	O
units	O
connected	O
into	O
multi-layer	O
neural	B
networks	I
,	O
showing	O
that	O
they	O
could	O
learn	O
nonlinear	O
functions	O
,	O
such	O
as	O
xor	O
and	O
others	O
,	O
with	O
a	O
globally-broadcast	O
reinforcement	B
signal	I
.	O
barto	O
(	O
1985	O
)	O
extensively	O
discussed	O
this	O
approach	O
to	O
artiﬁcial	B
neural	I
networks	I
and	O
how	O
this	O
type	O
of	O
learn-	O
ing	B
rule	O
is	O
related	O
to	O
others	O
in	O
the	O
literature	O
at	O
that	O
time	O
.	O
williams	O
(	O
1992	O
)	O
mathematically	O
analyzed	O
and	O
broadened	O
this	O
class	O
of	O
learning	O
rules	O
and	O
related	O
15.13.	O
summary	O
423	O
their	O
use	O
to	O
the	O
error	O
backpropagation	O
method	O
for	O
training	O
multilayer	O
artiﬁcial	B
neural	I
networks	I
.	O
williams	O
(	O
1988	O
)	O
described	O
several	O
ways	O
that	O
backpropagation	B
and	O
reinforcement	B
learning	I
can	O
be	O
combined	O
for	O
training	O
artiﬁcial	O
neural	O
net-	O
works	O
.	O
williams	O
(	O
1992	O
)	O
showed	O
that	O
a	O
special	O
case	O
of	O
the	O
ar−p	O
algorithm	O
is	O
a	O
reinforce	O
algorithm	O
,	O
although	O
better	O
results	O
were	O
obtained	O
with	O
the	O
general	O
ar−p	O
algorithm	O
(	O
barto	O
,	O
1985	O
)	O
.	O
the	O
third	O
phase	O
of	O
interest	O
in	O
teams	O
of	O
reinforcement	O
learning	O
agents	O
was	O
inﬂu-	O
enced	O
by	O
increased	O
understanding	O
of	O
the	O
role	O
of	O
dopamine	O
as	O
a	O
widely	O
broad-	O
cast	O
neuromodulator	O
and	O
speculation	O
about	O
the	O
existence	O
of	O
reward-modulated	O
stdp	O
.	O
much	O
more	O
so	O
than	O
earlier	O
research	O
,	O
this	O
research	O
considers	O
details	O
of	O
synaptic	O
plasticity	O
and	O
other	O
constraints	O
from	O
neuroscience	B
.	O
publications	O
include	O
the	O
following	O
(	O
chronologically	O
and	O
alphabetically	O
)	O
:	O
bartlett	O
and	O
baxter	O
(	O
1999	O
,	O
2000	O
)	O
,	O
xie	O
and	O
seung	O
(	O
2004	O
)	O
,	O
baras	O
and	O
meir	O
(	O
2007	O
)	O
,	O
farries	O
and	O
fairhall	O
(	O
2007	O
)	O
,	O
florian	O
(	O
2007	O
)	O
,	O
izhikevich	O
(	O
2007	O
)	O
,	O
pecevski	O
,	O
maass	O
,	O
and	O
legenstein	O
(	O
2008	O
)	O
,	O
leg-	O
enstein	O
,	O
pecevski	O
,	O
and	O
maass	O
(	O
2008	O
)	O
,	O
kolodziejski	O
,	O
porr	O
,	O
and	O
w¨org¨otter	O
(	O
2009	O
)	O
,	O
urbanczik	O
and	O
senn	O
(	O
2009	O
)	O
,	O
and	O
vasilaki	O
,	O
fr´emaux	O
,	O
urbanczik	O
,	O
senn	O
,	O
and	O
ger-	O
stner	O
(	O
2009	O
)	O
.	O
now´e	O
,	O
vrancx	O
,	O
and	O
de	O
hauwere	O
(	O
2012	O
)	O
reviewed	O
more	O
recent	O
developments	O
in	O
the	O
wider	O
ﬁeld	O
of	O
multi-agent	O
reinforcement	B
learning	I
15.11	O
yin	O
and	O
knowlton	O
(	O
2006	O
)	O
reviewed	O
ﬁndings	O
from	O
outcome-devaluation	O
exper-	O
iments	O
with	O
rodents	O
supporting	O
the	O
view	O
that	O
habitual	O
and	O
goal-directed	O
be-	O
havior	O
(	O
as	O
psychologists	O
use	O
the	O
phrase	O
)	O
are	O
respectively	O
most	O
associated	O
with	O
processing	O
in	O
the	O
dorsolateral	O
striatum	O
(	O
dls	O
)	O
and	O
the	O
dorsomedial	O
striatum	O
(	O
dms	O
)	O
.	O
results	O
of	O
functional	O
imaging	O
experiments	O
with	O
human	O
subjects	O
in	O
the	O
outcome-devaluation	O
setting	O
by	O
valentin	O
,	O
dickinson	O
,	O
and	O
o	O
’	O
doherty	O
(	O
2007	O
)	O
sug-	O
gest	O
that	O
the	O
orbitofrontal	O
cortex	O
(	O
ofc	O
)	O
is	O
an	O
important	O
component	O
of	O
goal-	O
directed	O
choice	O
.	O
single	O
unit	O
recordings	O
in	O
monkeys	O
by	O
padoa-schioppa	O
and	O
assad	O
(	O
2006	O
)	O
support	O
the	O
role	O
of	O
the	O
ofc	O
in	O
encoding	O
values	O
guiding	O
choice	O
behavior	O
.	O
rangel	O
,	O
camerer	O
,	O
and	O
montague	O
(	O
2008	O
)	O
and	O
rangel	O
and	O
hare	O
(	O
2010	O
)	O
reviewed	O
ﬁndings	O
from	O
the	O
perspective	O
of	O
neuroeconomics	O
about	O
how	O
the	O
brain	O
makes	O
goal-directed	O
decisions	O
.	O
pezzulo	O
,	O
van	O
der	O
meer	O
,	O
lansink	O
,	O
and	O
pennartz	O
(	O
2014	O
)	O
re-	O
viewed	O
the	O
neuroscience	B
of	O
internally	O
generated	O
sequences	O
and	O
presented	O
a	O
model	O
of	O
how	O
these	O
mechanisms	O
might	O
be	O
components	O
of	O
model-based	O
planning	B
.	O
daw	O
and	O
shohamy	O
(	O
2008	O
)	O
proposed	O
that	O
while	O
dopamine	B
signaling	O
connects	O
well	O
to	O
habitual	O
,	O
or	O
model-free	O
,	O
behavior	O
,	O
other	O
processes	O
are	O
involved	O
in	O
goal-directed	O
,	O
or	O
model-based	O
,	O
behavior	O
.	O
data	O
from	O
experiments	O
by	O
bromberg-martin	O
,	O
mat-	O
sumoto	O
,	O
hong	O
,	O
and	O
hikosaka	O
(	O
2010	O
)	O
indicate	O
that	O
dopamine	B
signals	O
contain	O
infor-	O
mation	O
pertinent	O
to	O
both	O
habitual	O
and	O
goal-directed	O
behavior	O
.	O
doll	O
,	O
simon	O
,	O
and	O
daw	O
(	O
2012	O
)	O
argued	O
that	O
there	O
may	O
not	O
a	O
clear	O
separation	O
in	O
the	O
brain	O
between	O
mechanisms	O
that	O
subserve	O
habitual	O
and	O
goal-directed	O
learning	O
and	O
choice	O
.	O
15.12	O
keiﬂin	O
and	O
janak	O
(	O
2015	O
)	O
reviewed	O
connections	O
between	O
td	O
errors	O
and	O
addic-	O
tion	B
.	O
nutt	O
,	O
lingford-hughes	O
,	O
erritzoe	O
,	O
and	O
stokes	O
(	O
2015	O
)	O
critically	O
evaluated	O
the	O
hypothesis	O
that	O
addiction	B
is	O
due	O
to	O
a	O
disorder	O
of	O
the	O
dopamine	B
system	O
.	O
mon-	O
tague	O
,	O
dolan	O
,	O
friston	O
,	O
and	O
dayan	O
(	O
2012	O
)	O
outlined	O
the	O
goals	O
and	O
early	O
eﬀorts	O
424	O
chapter	O
15	O
:	O
neuroscience	B
in	O
the	O
ﬁeld	O
of	O
computational	O
psychiatry	O
,	O
and	O
adams	O
,	O
huys	O
,	O
and	O
roiser	O
(	O
2015	O
)	O
reviewed	O
more	O
recent	O
progress	O
.	O
chapter	O
16	O
applications	B
and	I
case	I
studies	I
in	O
this	O
chapter	O
we	O
present	O
a	O
few	O
case	O
studies	O
of	O
reinforcement	O
learning	O
.	O
several	O
of	O
these	O
are	O
substantial	O
applications	O
of	O
potential	O
economic	O
signiﬁcance	O
.	O
one	O
,	O
samuel	O
’	O
s	O
checkers	O
player	O
,	O
is	O
primarily	O
of	O
historical	O
interest	O
.	O
our	O
presentations	O
are	O
intended	O
to	O
illustrate	O
some	O
of	O
the	O
trade-oﬀs	O
and	O
issues	O
that	O
arise	O
in	O
real	O
applications	O
.	O
for	O
example	O
,	O
we	O
em-	O
phasize	O
how	O
domain	O
knowledge	O
is	O
incorporated	O
into	O
the	O
formulation	O
and	O
solution	O
of	O
the	O
problem	O
.	O
we	O
also	O
highlight	O
the	O
representation	O
issues	O
that	O
are	O
so	O
often	O
critical	O
to	O
suc-	O
cessful	O
applications	O
.	O
the	O
algorithms	O
used	O
in	O
some	O
of	O
these	O
case	O
studies	O
are	O
substantially	O
more	O
complex	O
than	O
those	O
we	O
have	O
presented	O
in	O
the	O
rest	O
of	O
the	O
book	O
.	O
applications	O
of	O
reinforcement	B
learning	I
are	O
still	O
far	O
from	O
routine	O
and	O
typically	O
require	O
as	O
much	O
art	O
as	O
sci-	O
ence	O
.	O
making	O
applications	O
easier	O
and	O
more	O
straightforward	O
is	O
one	O
of	O
the	O
goals	O
of	O
current	O
research	O
in	O
reinforcement	O
learning	O
.	O
16.1	O
td-gammon	O
one	O
of	O
the	O
most	O
impressive	O
applications	O
of	O
reinforcement	B
learning	I
to	O
date	O
is	O
that	O
by	O
ger-	O
ald	O
tesauro	O
to	O
the	O
game	O
of	O
backgammon	B
(	O
tesauro	O
,	O
1992	O
,	O
1994	O
,	O
1995	O
,	O
2002	O
)	O
.	O
tesauro	O
’	O
s	O
program	O
,	O
td-gammon	O
,	O
required	O
little	O
backgammon	B
knowledge	O
,	O
yet	O
learned	O
to	O
play	O
ex-	O
tremely	O
well	O
,	O
near	O
the	O
level	O
of	O
the	O
world	O
’	O
s	O
strongest	O
grandmasters	O
.	O
the	O
learning	O
algorithm	O
in	O
td-gammon	O
was	O
a	O
straightforward	O
combination	O
of	O
the	O
td	O
(	O
λ	O
)	O
algorithm	O
and	O
nonlinear	O
function	B
approximation	I
using	O
a	O
multilayer	O
neural	B
network	O
trained	O
by	O
backpropagating	O
td	O
errors	O
.	O
backgammon	B
is	O
a	O
major	O
game	O
in	O
the	O
sense	O
that	O
it	O
is	O
played	O
throughout	O
the	O
world	O
,	O
with	O
numerous	O
tournaments	O
and	O
regular	O
world	O
championship	O
matches	O
.	O
it	O
is	O
in	O
part	O
a	O
game	O
of	O
chance	O
,	O
and	O
it	O
is	O
a	O
popular	O
vehicle	O
for	O
waging	O
signiﬁcant	O
sums	O
of	O
money	O
.	O
there	O
are	O
probably	O
more	O
professional	O
backgammon	B
players	O
than	O
there	O
are	O
professional	O
chess	B
players	O
.	O
the	O
game	O
is	O
played	O
with	O
15	O
white	O
and	O
15	O
black	O
pieces	O
on	O
a	O
board	O
of	O
24	O
locations	O
,	O
called	O
points	O
.	O
to	O
the	O
right	O
on	O
the	O
next	O
page	O
is	O
shown	O
a	O
typical	O
position	O
early	O
in	O
the	O
game	O
,	O
seen	O
from	O
the	O
perspective	O
of	O
the	O
white	O
player	O
.	O
white	O
here	O
has	O
just	O
rolled	O
the	O
dice	O
and	O
obtained	O
a	O
5	O
and	O
a	O
2.	O
this	O
means	O
that	O
he	O
can	O
move	O
one	O
of	O
his	O
pieces	O
5	O
steps	O
and	O
one	O
425	O
426	O
chapter	O
16	O
:	O
applications	B
and	I
case	I
studies	I
(	O
possibly	O
the	O
same	O
piece	O
)	O
2	O
steps	O
.	O
for	O
example	O
,	O
he	O
could	O
move	O
two	O
pieces	O
from	O
the	O
12	O
point	O
,	O
one	O
to	O
the	O
17	O
point	O
,	O
and	O
one	O
to	O
the	O
14	O
point	O
.	O
white	O
’	O
s	O
objective	O
is	O
to	O
advance	O
all	O
of	O
his	O
pieces	O
into	O
the	O
last	O
quadrant	O
(	O
points	O
19–24	O
)	O
and	O
then	O
oﬀ	O
the	O
board	O
.	O
the	O
ﬁrst	O
player	O
to	O
re-	O
move	O
all	O
his	O
pieces	O
wins	O
.	O
one	O
compli-	O
cation	O
is	O
that	O
the	O
pieces	O
interact	O
as	O
they	O
pass	O
each	O
other	O
going	O
in	O
diﬀerent	O
direc-	O
tions	O
.	O
for	O
example	O
,	O
if	O
it	O
were	O
black	O
’	O
s	O
move	O
,	O
he	O
could	O
use	O
the	O
dice	O
roll	O
of	O
2	O
to	O
move	O
a	O
piece	O
from	O
the	O
24	O
point	O
to	O
the	O
22	O
point	O
,	O
“	O
hitting	O
”	O
the	O
white	O
piece	O
there	O
.	O
pieces	O
that	O
have	O
been	O
hit	O
are	O
placed	O
on	O
the	O
“	O
bar	O
”	O
in	O
the	O
middle	O
of	O
the	O
board	O
(	O
where	O
we	O
already	O
see	O
one	O
previously	O
hit	O
black	O
piece	O
)	O
,	O
from	O
whence	O
they	O
reenter	O
the	O
race	O
from	O
the	O
start	O
.	O
however	O
,	O
if	O
there	O
are	O
two	O
pieces	O
on	O
a	O
point	O
,	O
then	O
the	O
opponent	O
can	O
not	O
move	O
to	O
that	O
point	O
;	O
the	O
pieces	O
are	O
protected	O
from	O
being	O
hit	O
.	O
thus	O
,	O
white	O
can	O
not	O
use	O
his	O
5–2	O
dice	O
roll	O
to	O
move	O
either	O
of	O
his	O
pieces	O
on	O
the	O
1	O
point	O
,	O
because	O
their	O
possible	O
resulting	O
points	O
are	O
occupied	O
by	O
groups	O
of	O
black	O
pieces	O
.	O
forming	O
contiguous	O
blocks	O
of	O
occupied	O
points	O
to	O
block	O
the	O
opponent	O
is	O
one	O
of	O
the	O
elementary	O
strategies	O
of	O
the	O
game	O
.	O
backgammon	B
involves	O
several	O
further	O
complications	O
,	O
but	O
the	O
above	O
description	O
gives	O
the	O
basic	O
idea	O
.	O
with	O
30	O
pieces	O
and	O
24	O
possible	O
locations	O
(	O
26	O
,	O
counting	O
the	O
bar	O
and	O
oﬀ-the-	O
board	O
)	O
it	O
should	O
be	O
clear	O
that	O
the	O
number	O
of	O
possible	O
backgammon	B
positions	O
is	O
enormous	O
,	O
far	O
more	O
than	O
the	O
number	O
of	O
memory	O
elements	O
one	O
could	O
have	O
in	O
any	O
physically	O
realizable	O
computer	O
.	O
the	O
number	O
of	O
moves	O
possible	O
from	O
each	O
position	O
is	O
also	O
large	O
.	O
for	O
a	O
typical	O
dice	O
roll	O
there	O
might	O
be	O
20	O
diﬀerent	O
ways	O
of	O
playing	O
.	O
in	O
considering	O
future	O
moves	O
,	O
such	O
as	O
the	O
response	O
of	O
the	O
opponent	O
,	O
one	O
must	O
consider	O
the	O
possible	O
dice	O
rolls	O
as	O
well	O
.	O
the	O
result	O
is	O
that	O
the	O
game	O
tree	O
has	O
an	O
eﬀective	O
branching	B
factor	I
of	O
about	O
400.	O
this	O
is	O
far	O
too	O
large	O
to	O
permit	O
eﬀective	O
use	O
of	O
the	O
conventional	O
heuristic	B
search	I
methods	O
that	O
have	O
proved	O
so	O
eﬀective	O
in	O
games	O
like	O
chess	B
and	O
checkers	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
game	O
is	O
a	O
good	O
match	O
to	O
the	O
capabilities	O
of	O
td	O
learning	O
methods	O
.	O
although	O
the	O
game	O
is	O
highly	O
stochastic	O
,	O
a	O
complete	O
description	O
of	O
the	O
game	O
’	O
s	O
state	B
is	O
available	O
at	O
all	O
times	O
.	O
the	O
game	O
evolves	O
over	O
a	O
sequence	O
of	O
moves	O
and	O
positions	O
until	O
ﬁnally	O
ending	O
in	O
a	O
win	O
for	O
one	O
player	O
or	O
the	O
other	O
,	O
ending	O
the	O
game	O
.	O
the	O
outcome	O
can	O
be	O
interpreted	O
as	O
a	O
ﬁnal	O
reward	O
to	O
be	O
predicted	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
theoretical	O
results	O
we	O
have	O
described	O
so	O
far	O
can	O
not	O
be	O
usefully	O
applied	O
to	O
this	O
task	O
.	O
the	O
number	O
of	O
states	O
is	O
so	O
large	O
that	O
a	O
lookup	O
table	O
can	O
not	O
be	O
used	O
,	O
and	O
the	O
opponent	O
is	O
a	O
source	O
of	O
uncertainty	O
and	O
time	O
variation	O
.	O
td-gammon	O
used	O
a	O
nonlinear	O
form	O
of	O
td	O
(	O
λ	O
)	O
.	O
the	O
estimated	O
value	B
,	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
,	O
of	O
any	O
state	B
(	O
board	O
position	O
)	O
s	O
was	O
meant	O
to	O
estimate	O
the	O
probability	O
of	O
winning	O
starting	O
from	O
state	B
s.	O
to	O
achieve	O
this	O
,	O
rewards	O
were	O
deﬁned	O
as	O
zero	O
for	O
all	O
time	O
steps	O
except	O
those	O
on	O
which	O
the	O
game	O
is	O
won	O
.	O
to	O
implement	O
the	O
value	B
function	I
,	O
td-gammon	O
used	O
a	O
standard	O
multilayer	O
neural	B
network	O
,	O
much	O
as	O
shown	O
to	O
the	O
right	O
on	O
the	O
next	O
page	O
.	O
(	O
the	O
real	O
network	O
had	O
two	O
additional	O
units	O
in	O
its	O
ﬁnal	O
layer	O
to	O
estimate	O
the	O
probability	O
of	O
each	O
white	O
pieces	O
move	O
counterclockwise123456789101112181716151413192021222324	O
black	O
pieces	O
move	O
clockwise	O
16.1.	O
td-gammon	O
427	O
player	O
’	O
s	O
winning	O
in	O
a	O
special	O
way	O
called	O
a	O
“	O
gammon	O
”	O
or	O
“	O
backgammon.	O
”	O
)	O
the	O
network	O
consisted	O
of	O
a	O
layer	O
of	O
input	O
units	O
,	O
a	O
layer	O
of	O
hidden	O
units	O
,	O
and	O
a	O
ﬁnal	O
output	O
unit	O
.	O
the	O
input	O
to	O
the	O
network	O
was	O
a	O
representation	O
of	O
a	O
backgammon	B
position	O
,	O
and	O
the	O
output	O
was	O
an	O
esti-	O
mate	O
of	O
the	O
value	B
of	O
that	O
position	O
.	O
figure	O
16.1	O
:	O
the	O
td-gammon	O
neural	B
network	O
in	O
the	O
ﬁrst	O
version	O
of	O
td-gammon	O
,	O
td-gammon	O
0.0	O
,	O
backgammon	B
posi-	O
tions	O
were	O
represented	O
to	O
the	O
network	O
in	O
a	O
relatively	O
direct	O
way	O
that	O
involved	O
little	O
backgammon	B
knowledge	O
.	O
it	O
did	O
,	O
however	O
,	O
involve	O
substantial	O
knowledge	O
of	O
how	O
neural	B
networks	I
work	O
and	O
how	O
information	O
is	O
best	O
presented	O
to	O
them	O
.	O
it	O
is	O
instructive	O
to	O
note	O
the	O
exact	O
representation	O
tesauro	O
chose	O
.	O
there	O
were	O
a	O
total	O
of	O
198	O
input	O
units	O
to	O
the	O
network	O
.	O
for	O
each	O
point	O
on	O
the	O
backgammon	B
board	O
,	O
four	O
units	O
indicated	O
the	O
number	O
of	O
white	O
pieces	O
on	O
the	O
point	O
.	O
if	O
there	O
were	O
no	O
white	O
pieces	O
,	O
then	O
all	O
four	O
units	O
took	O
on	O
the	O
value	B
zero	O
.	O
if	O
there	O
was	O
one	O
piece	O
,	O
then	O
the	O
ﬁrst	O
unit	O
took	O
on	O
the	O
value	B
1.	O
this	O
encoded	O
the	O
elementary	O
concept	O
of	O
a	O
“	O
blot	O
,	O
”	O
i.e.	O
,	O
a	O
piece	O
that	O
can	O
be	O
hit	O
by	O
the	O
opponent	O
.	O
if	O
there	O
were	O
two	O
or	O
more	O
pieces	O
,	O
then	O
the	O
second	O
unit	O
was	O
set	O
to	O
1.	O
this	O
encoded	O
the	O
basic	O
concept	O
of	O
a	O
“	O
made	O
point	O
”	O
on	O
which	O
the	O
opponent	O
can	O
not	O
land	O
.	O
if	O
there	O
were	O
exactly	O
three	O
pieces	O
on	O
the	O
point	O
,	O
then	O
the	O
third	O
unit	O
was	O
set	O
to	O
1.	O
this	O
encoded	O
the	O
basic	O
concept	O
of	O
a	O
“	O
single	O
spare	O
,	O
”	O
i.e.	O
,	O
an	O
extra	O
piece	O
in	O
addition	O
to	O
the	O
two	O
pieces	O
that	O
made	O
the	O
point	O
.	O
finally	O
,	O
if	O
there	O
were	O
more	O
than	O
three	O
pieces	O
,	O
the	O
fourth	O
unit	O
was	O
set	O
to	O
a	O
value	B
proportionate	O
to	O
the	O
number	O
of	O
additional	O
pieces	O
beyond	O
three	O
.	O
letting	O
n	O
denote	O
the	O
total	O
number	O
of	O
pieces	O
on	O
the	O
point	O
,	O
if	O
n	O
>	O
3	O
,	O
then	O
the	O
fourth	O
unit	O
took	O
on	O
the	O
value	B
(	O
n	O
−	O
3	O
)	O
/2	O
.	O
this	O
encoded	O
a	O
linear	O
representation	O
of	O
“	O
multiple	O
spares	O
”	O
at	O
the	O
given	O
point	O
.	O
with	O
four	O
units	O
for	O
white	O
and	O
four	O
for	O
black	O
at	O
each	O
of	O
the	O
24	O
points	O
,	O
that	O
made	O
a	O
total	O
of	O
192	O
units	O
.	O
two	O
additional	O
units	O
encoded	O
the	O
number	O
of	O
white	O
and	O
black	O
pieces	O
on	O
the	O
bar	O
(	O
each	O
took	O
the	O
value	B
n/2	O
,	O
where	O
n	O
is	O
the	O
number	O
of	O
pieces	O
on	O
the	O
bar	O
)	O
,	O
and	O
two	O
more	O
encoded	O
the	O
number	O
of	O
black	O
and	O
white	O
pieces	O
already	O
successfully	O
removed	O
from	O
the	O
board	O
(	O
these	O
took	O
the	O
value	B
n/15	O
,	O
where	O
n	O
is	O
the	O
number	O
of	O
pieces	O
already	O
borne	O
oﬀ	O
)	O
.	O
finally	O
,	O
two	O
units	O
indicated	O
in	O
a	O
binary	O
fashion	O
whether	O
it	O
was	O
white	O
’	O
s	O
or	O
black	O
’	O
s	O
turn	O
to	O
move	O
.	O
the	O
general	O
logic	O
behind	O
these	O
choices	O
should	O
be	O
clear	O
.	O
basically	O
,	O
tesauro	O
tried	O
to	O
represent	O
the	O
position	O
in	O
a	O
straightforward	O
way	O
,	O
while	O
keeping	O
the	O
number	O
of	O
units	O
relatively	O
small	O
.	O
he	O
provided	O
one	O
unit	O
for	O
each	O
conceptually	O
distinct	O
possibility	O
that	O
seemed	O
likely	O
to	O
be	O
relevant	O
,	O
and	O
he	O
scaled	O
them	O
to	O
roughly	O
the	O
same	O
range	O
,	O
in	O
this	O
case	O
between	O
0	O
and	O
1.	O
given	O
a	O
representation	O
of	O
a	O
backgammon	B
position	O
,	O
the	O
network	O
computed	O
its	O
estimated	O
value	B
in	O
the	O
standard	O
way	O
.	O
corresponding	O
to	O
each	O
connection	O
from	O
an	O
input	O
unit	O
to	O
a	O
hidden	O
unit	O
was	O
a	O
real-valued	O
weight	O
.	O
signals	O
from	O
each	O
input	O
unit	O
were	O
multiplied	O
by	O
their	O
corresponding	O
weights	O
and	O
summed	O
at	O
the	O
hidden	O
unit	O
.	O
the	O
output	O
,	O
h	O
(	O
j	O
)	O
,	O
of	O
hidden	O
vt+1	O
!	O
vthidden	O
units	O
(	O
40-80	O
)	O
backgammon	B
position	O
(	O
198	O
input	O
units	O
)	O
predicted	O
probabilityof	O
winning	O
,	O
vttd	O
error	O
,	O
.	O
.	O
..	O
.	O
..	O
.	O
..	O
.	O
..	O
.	O
..	O
.	O
.15.1.td-gammon263gationalgorithm	O
(	O
rumelhart	O
,	O
hinton	O
,	O
andwilliams,1986	O
)	O
.recallthatthegeneralupdateruleforthiscaseiswt+1=wt+↵hrt+1+ ˆv	O
(	O
st+1	O
,	O
wt	O
)	O
 ˆv	O
(	O
st	O
,	O
wt	O
)	O
iet	O
,	O
(	O
15.1	O
)	O
wherewtisthevectorofallmodiﬁableparameters	O
(	O
inthiscase	O
,	O
theweightsofthenetwork	O
)	O
andetisavectorofeligibilitytraces	O
,	O
oneforeachcomponentofwt	O
,	O
updatedbyet=  et 1+rwtˆv	O
(	O
st	O
,	O
wt	O
)	O
,	O
withe0=0.thegradientinthisequationcanbecomputede cientlybythebackpropagationprocedure.forthebackgammonapplication	O
,	O
inwhich =1andtherewardisalwayszeroexceptuponwinning	O
,	O
thetderrorportionofthelearningruleisusuallyjustˆv	O
(	O
st+1	O
,	O
w	O
)	O
 ˆv	O
(	O
st	O
,	O
w	O
)	O
,	O
assuggestedinfigure15.2.toapplythelearningruleweneedasourceofbackgammongames.tesauroobtainedanunendingsequenceofgamesbyplayinghislearningbackgammonplayeragainstitself.tochooseitsmoves	O
,	O
td-gammonconsideredeachofthe20orsowaysitcouldplayitsdicerollandthecorrespondingpositionsthatwouldresult.theresultingpositionsareafterstatesasdiscussedinsection6.8.thenetworkwasconsultedtoestimateeachoftheirvalues.themovewasthenselectedthatwouldleadtothepositionwiththehighestestimatedvalue.continuinginthisway	O
,	O
withtd-gammonmakingthemovesforbothsides	O
,	O
itwaspossibletoeasilygeneratelargenumbersofbackgammongames.eachgamewastreatedasanepisode	O
,	O
withthesequenceofpositionsactingasthestates	O
,	O
s0	O
,	O
s1	O
,	O
s2	O
,	O
...	O
.tesauroappliedthenonlineartdrule	O
(	O
15.1	O
)	O
fullyincrementally	O
,	O
thatis	O
,	O
aftereachindividualmove.theweightsofthenetworkweresetinitiallytosmallrandomvalues.theinitialevaluationswerethusentirelyarbitrary.sincethemoveswereselectedonthebasisoftheseevaluations	O
,	O
theinitialmoveswereinevitablypoor	O
,	O
andtheinitialgamesoftenlastedhundredsorthousandsofmovesbeforeonesideortheotherwon	O
,	O
almostbyaccident.afterafewdozengameshowever	O
,	O
performanceimprovedrapidly.afterplayingabout300,000gamesagainstitself	O
,	O
td-gammon0.0asde-scribedabovelearnedtoplayapproximatelyaswellasthebestpreviousbackgammoncomputerprograms.thiswasastrikingresultbecausealltheprevioushigh-performancecomputerprogramshadusedextensivebackgam-monknowledge.forexample	O
,	O
thereigningchampionprogramatthetimewas	O
,	O
arguably	O
,	O
neurogammon	O
,	O
anotherprogramwrittenbytesaurothatusedaneuralnetworkbutnottdlearning.neurogammon	O
’	O
snetworkwastrainedonalargetrainingcorpusofexemplarymovesprovidedbybackgammonex-perts	O
,	O
and	O
,	O
inaddition	O
,	O
startedwithasetoffeaturesspeciallycraftedfortd	O
error15.1.td-gammon263gationalgorithm	O
(	O
rumelhart	O
,	O
hinton	O
,	O
andwilliams,1986	O
)	O
.recallthatthegeneralupdateruleforthiscaseiswt+1=wt+↵hrt+1+ ˆv	O
(	O
st+1	O
,	O
wt	O
)	O
 ˆv	O
(	O
st	O
,	O
wt	O
)	O
iet	O
,	O
(	O
15.1	O
)	O
wherewtisthevectorofallmodiﬁableparameters	O
(	O
inthiscase	O
,	O
theweightsofthenetwork	O
)	O
andetisavectorofeligibilitytraces	O
,	O
oneforeachcomponentofwt	O
,	O
updatedbyet=  et 1+rwtˆv	O
(	O
st	O
,	O
wt	O
)	O
,	O
withe0=0.thegradientinthisequationcanbecomputede cientlybythebackpropagationprocedure.forthebackgammonapplication	O
,	O
inwhich =1andtherewardisalwayszeroexceptuponwinning	O
,	O
thetderrorportionofthelearningruleisusuallyjustˆv	O
(	O
st+1	O
,	O
w	O
)	O
 ˆv	O
(	O
st	O
,	O
w	O
)	O
,	O
assuggestedinfigure15.2.toapplythelearningruleweneedasourceofbackgammongames.tesauroobtainedanunendingsequenceofgamesbyplayinghislearningbackgammonplayeragainstitself.tochooseitsmoves	O
,	O
td-gammonconsideredeachofthe20orsowaysitcouldplayitsdicerollandthecorrespondingpositionsthatwouldresult.theresultingpositionsareafterstatesasdiscussedinsection6.8.thenetworkwasconsultedtoestimateeachoftheirvalues.themovewasthenselectedthatwouldleadtothepositionwiththehighestestimatedvalue.continuinginthisway	O
,	O
withtd-gammonmakingthemovesforbothsides	O
,	O
itwaspossibletoeasilygeneratelargenumbersofbackgammongames.eachgamewastreatedasanepisode	O
,	O
withthesequenceofpositionsactingasthestates	O
,	O
s0	O
,	O
s1	O
,	O
s2	O
,	O
...	O
.tesauroappliedthenonlineartdrule	O
(	O
15.1	O
)	O
fullyincrementally	O
,	O
thatis	O
,	O
aftereachindividualmove.theweightsofthenetworkweresetinitiallytosmallrandomvalues.theinitialevaluationswerethusentirelyarbitrary.sincethemoveswereselectedonthebasisoftheseevaluations	O
,	O
theinitialmoveswereinevitablypoor	O
,	O
andtheinitialgamesoftenlastedhundredsorthousandsofmovesbeforeonesideortheotherwon	O
,	O
almostbyaccident.afterafewdozengameshowever	O
,	O
performanceimprovedrapidly.afterplayingabout300,000gamesagainstitself	O
,	O
td-gammon0.0asde-scribedabovelearnedtoplayapproximatelyaswellasthebestpreviousbackgammoncomputerprograms.thiswasastrikingresultbecausealltheprevioushigh-performancecomputerprogramshadusedextensivebackgam-monknowledge.forexample	O
,	O
thereigningchampionprogramatthetimewas	O
,	O
arguably	O
,	O
neurogammon	O
,	O
anotherprogramwrittenbytesaurothatusedaneuralnetworkbutnottdlearning.neurogammon	O
’	O
snetworkwastrainedonalargetrainingcorpusofexemplarymovesprovidedbybackgammonex-perts	O
,	O
and	O
,	O
inaddition	O
,	O
startedwithasetoffeaturesspeciallycraftedforhidden	O
units	O
(	O
40-80	O
)	O
428	O
chapter	O
16	O
:	O
applications	B
and	I
case	I
studies	I
unit	O
j	O
was	O
a	O
nonlinear	O
sigmoid	O
function	O
of	O
the	O
weighted	O
sum	O
:	O
h	O
(	O
j	O
)	O
=	O
σ	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
i	O
wijxi	O
(	O
cid:33	O
)	O
=	O
1	O
+	O
e−	O
(	O
cid:80	O
)	O
1	O
i	O
wij	O
xi	O
,	O
where	O
xi	O
is	O
the	O
value	B
of	O
the	O
ith	O
input	O
unit	O
and	O
wij	O
is	O
the	O
weight	O
of	O
its	O
connection	O
to	O
the	O
jth	O
hidden	O
unit	O
(	O
all	O
the	O
weights	O
in	O
the	O
network	O
together	O
make	O
up	O
the	O
parameter	O
vector	O
w	O
)	O
.	O
the	O
output	O
of	O
the	O
sigmoid	O
is	O
always	O
between	O
0	O
and	O
1	O
,	O
and	O
has	O
a	O
natural	O
interpretation	O
as	O
a	O
probability	O
based	O
on	O
a	O
summation	O
of	O
evidence	O
.	O
the	O
computation	O
from	O
hidden	O
units	O
to	O
the	O
output	O
unit	O
was	O
entirely	O
analogous	O
.	O
each	O
connection	O
from	O
a	O
hidden	O
unit	O
to	O
the	O
output	O
unit	O
had	O
a	O
separate	O
weight	O
.	O
the	O
output	O
unit	O
formed	O
the	O
weighted	O
sum	O
and	O
then	O
passed	O
it	O
through	O
the	O
same	O
sigmoid	O
nonlinearity	O
.	O
td-gammon	O
used	O
the	O
semi-gradient	O
form	O
of	O
the	O
td	O
(	O
λ	O
)	O
algorithm	O
described	O
in	O
sec-	O
tion	B
12.2	O
,	O
with	O
the	O
gradients	O
computed	O
by	O
the	O
error	O
backpropagation	O
algorithm	O
(	O
rumel-	O
hart	O
,	O
hinton	O
,	O
and	O
williams	O
,	O
1986	O
)	O
.	O
recall	O
that	O
the	O
general	O
update	O
rule	O
for	O
this	O
case	O
is	O
wt+1	O
.	O
=	O
wt	O
+	O
α	O
(	O
cid:104	O
)	O
rt+1	O
+	O
γˆv	O
(	O
st+1	O
,	O
wt	O
)	O
−	O
ˆv	O
(	O
st	O
,	O
wt	O
)	O
(	O
cid:105	O
)	O
zt	O
,	O
(	O
16.1	O
)	O
where	O
wt	O
is	O
the	O
vector	B
of	O
all	O
modiﬁable	O
parameters	O
(	O
in	O
this	O
case	O
,	O
the	O
weights	O
of	O
the	O
network	O
)	O
and	O
zt	O
is	O
a	O
vector	B
of	O
eligibility	B
traces	I
,	O
one	O
for	O
each	O
component	O
of	O
wt	O
,	O
updated	O
by	O
zt	O
.	O
=	O
γλzt−1	O
+	O
∇ˆv	O
(	O
st	O
,	O
wt	O
)	O
,	O
.	O
with	O
z0	O
=	O
0.	O
the	O
gradient	B
in	O
this	O
equation	O
can	O
be	O
computed	O
eﬃciently	O
by	O
the	O
backprop-	O
agation	O
procedure	O
.	O
for	O
the	O
backgammon	B
application	O
,	O
in	O
which	O
γ	O
=	O
1	O
and	O
the	O
reward	O
is	O
always	O
zero	O
except	O
upon	O
winning	O
,	O
the	O
td	O
error	O
portion	O
of	O
the	O
learning	O
rule	O
is	O
usually	O
just	O
ˆv	O
(	O
st+1	O
,	O
w	O
)	O
−	O
ˆv	O
(	O
st	O
,	O
w	O
)	O
,	O
as	O
suggested	O
in	O
figure	O
16.1.	O
to	O
apply	O
the	O
learning	O
rule	O
we	O
need	O
a	O
source	O
of	O
backgammon	O
games	O
.	O
tesauro	O
obtained	O
an	O
unending	O
sequence	O
of	O
games	O
by	O
playing	O
his	O
learning	O
backgammon	O
player	O
against	O
itself	O
.	O
to	O
choose	O
its	O
moves	O
,	O
td-gammon	O
considered	O
each	O
of	O
the	O
20	O
or	O
so	O
ways	O
it	O
could	O
play	O
its	O
dice	O
roll	O
and	O
the	O
corresponding	O
positions	O
that	O
would	O
result	O
.	O
the	O
resulting	O
positions	O
are	O
afterstates	B
as	O
discussed	O
in	O
section	O
6.8.	O
the	O
network	O
was	O
consulted	O
to	O
estimate	O
each	O
of	O
their	O
values	O
.	O
the	O
move	O
was	O
then	O
selected	O
that	O
would	O
lead	O
to	O
the	O
position	O
with	O
the	O
highest	O
estimated	O
value	B
.	O
continuing	O
in	O
this	O
way	O
,	O
with	O
td-gammon	O
making	O
the	O
moves	O
for	O
both	O
sides	O
,	O
it	O
was	O
possible	O
to	O
easily	O
generate	O
large	O
numbers	O
of	O
backgammon	O
games	O
.	O
each	O
game	O
was	O
treated	O
as	O
an	O
episode	O
,	O
with	O
the	O
sequence	O
of	O
positions	O
acting	O
as	O
the	O
states	O
,	O
s0	O
,	O
s1	O
,	O
s2	O
,	O
.	O
.	O
..	O
tesauro	O
applied	O
the	O
nonlinear	O
td	O
rule	O
(	O
16.1	O
)	O
fully	O
incrementally	O
,	O
that	O
is	O
,	O
after	O
each	O
individual	O
move	O
.	O
the	O
weights	O
of	O
the	O
network	O
were	O
set	O
initially	O
to	O
small	O
random	O
values	O
.	O
the	O
initial	O
evaluations	O
were	O
thus	O
entirely	O
arbitrary	O
.	O
because	O
the	O
moves	O
were	O
selected	O
on	O
the	O
basis	O
of	O
these	O
evaluations	O
,	O
the	O
initial	O
moves	O
were	O
inevitably	O
poor	O
,	O
and	O
the	O
initial	O
games	O
often	O
lasted	O
hundreds	O
or	O
thousands	O
of	O
moves	O
before	O
one	O
side	O
or	O
the	O
other	O
won	O
,	O
almost	O
by	O
accident	O
.	O
after	O
a	O
few	O
dozen	O
games	O
however	O
,	O
performance	O
improved	O
rapidly	O
.	O
16.1.	O
td-gammon	O
429	O
after	O
playing	O
about	O
300,000	O
games	O
against	O
itself	O
,	O
td-gammon	O
0.0	O
as	O
described	O
above	O
learned	O
to	O
play	O
approximately	O
as	O
well	O
as	O
the	O
best	O
previous	O
backgammon	B
computer	O
pro-	O
grams	O
.	O
this	O
was	O
a	O
striking	O
result	O
because	O
all	O
the	O
previous	O
high-performance	O
computer	O
programs	O
had	O
used	O
extensive	O
backgammon	B
knowledge	O
.	O
for	O
example	O
,	O
the	O
reigning	O
cham-	O
pion	O
program	O
at	O
the	O
time	O
was	O
,	O
arguably	O
,	O
neurogammon	O
,	O
another	O
program	O
written	O
by	O
tesauro	O
that	O
used	O
a	O
neural	B
network	O
but	O
not	O
td	O
learning	O
.	O
neurogammon	O
’	O
s	O
network	O
was	O
trained	O
on	O
a	O
large	O
training	O
corpus	O
of	O
exemplary	O
moves	O
provided	O
by	O
backgammon	B
ex-	O
perts	O
,	O
and	O
,	O
in	O
addition	O
,	O
started	O
with	O
a	O
set	O
of	O
features	O
specially	O
crafted	O
for	O
backgammon	O
.	O
neurogammon	O
was	O
a	O
highly	O
tuned	O
,	O
highly	O
eﬀective	O
backgammon	B
program	O
that	O
decisively	O
won	O
the	O
world	O
backgammon	B
olympiad	O
in	O
1989.	O
td-gammon	O
0.0	O
,	O
on	O
the	O
other	O
hand	O
,	O
was	O
constructed	O
with	O
essentially	O
zero	O
backgammon	B
knowledge	O
.	O
that	O
it	O
was	O
able	O
to	O
do	O
as	O
well	O
as	O
neurogammon	O
and	O
all	O
other	O
approaches	O
is	O
striking	O
testimony	O
to	O
the	O
potential	O
of	O
self-play	O
learning	O
methods	O
.	O
the	O
tournament	O
success	O
of	O
td-gammon	O
0.0	O
with	O
zero	O
expert	O
backgammon	B
knowl-	O
edge	O
suggested	O
an	O
obvious	O
modiﬁcation	O
:	O
add	O
the	O
specialized	O
backgammon	B
features	O
but	O
keep	O
the	O
self-play	O
td	O
learning	O
method	O
.	O
this	O
produced	O
td-gammon	O
1.0.	O
td-gammon	O
1.0	O
was	O
clearly	O
substantially	O
better	O
than	O
all	O
previous	O
backgammon	B
programs	O
and	O
found	O
serious	O
competition	O
only	O
among	O
human	O
experts	O
.	O
later	O
versions	O
of	O
the	O
program	O
,	O
td-	O
gammon	O
2.0	O
(	O
40	O
hidden	O
units	O
)	O
and	O
td-gammon	O
2.1	O
(	O
80	O
hidden	O
units	O
)	O
,	O
were	O
augmented	O
with	O
a	O
selective	O
two-ply	O
search	O
procedure	O
.	O
to	O
select	O
moves	O
,	O
these	O
programs	O
looked	O
ahead	O
not	O
just	O
to	O
the	O
positions	O
that	O
would	O
immediately	O
result	O
,	O
but	O
also	O
to	O
the	O
opponent	O
’	O
s	O
pos-	O
sible	O
dice	O
rolls	O
and	O
moves	O
.	O
assuming	O
the	O
opponent	O
always	O
took	O
the	O
move	O
that	O
appeared	O
immediately	O
best	O
for	O
him	O
,	O
the	O
expected	O
value	O
of	O
each	O
candidate	O
move	O
was	O
computed	O
and	O
the	O
best	O
was	O
selected	O
.	O
to	O
save	O
computer	O
time	O
,	O
the	O
second	O
ply	O
of	O
search	O
was	O
conducted	O
only	O
for	O
candidate	O
moves	O
that	O
were	O
ranked	O
highly	O
after	O
the	O
ﬁrst	O
ply	O
,	O
about	O
four	O
or	O
ﬁve	O
moves	O
on	O
average	O
.	O
two-ply	O
search	O
aﬀected	O
only	O
the	O
moves	O
selected	O
;	O
the	O
learning	O
process	O
proceeded	O
exactly	O
as	O
before	O
.	O
the	O
ﬁnal	O
versions	O
of	O
the	O
program	O
,	O
td-gammon	O
3.0	O
and	O
3.1	O
,	O
used	O
160	O
hidden	O
units	O
and	O
a	O
selective	O
three-ply	O
search	O
.	O
td-gammon	O
illustrates	O
the	O
combination	O
of	O
learned	O
value	B
functions	O
and	O
decision-time	O
search	O
as	O
in	O
heuristic	O
search	O
and	O
mcts	O
methods	O
.	O
in	O
follow-on	O
work	O
,	O
tesauro	O
and	O
galperin	O
(	O
1997	O
)	O
explored	O
trajectory	B
sampling	I
methods	O
as	O
an	O
alternative	O
to	O
full-width	O
search	O
,	O
which	O
reduced	O
the	O
error	O
rate	O
of	O
live	O
play	O
by	O
large	O
numerical	O
factors	O
(	O
4x–6x	O
)	O
while	O
keeping	O
the	O
think	O
time	O
reasonable	O
at	O
∼5–10	O
seconds	O
per	O
move	O
.	O
during	O
the	O
1990s	O
,	O
tesauro	O
was	O
able	O
to	O
play	O
his	O
programs	O
in	O
a	O
signiﬁcant	O
number	O
of	O
program	O
td-gammon	O
0.0	O
td-gammon	O
1.0	O
td-gammon	O
2.0	O
td-gammon	O
2.1	O
td-gammon	O
3.0	O
hidden	O
training	O
games	O
units	O
300,000	O
300,000	O
800,000	O
1,500,000	O
1,500,000	O
40	O
80	O
40	O
80	O
80	O
opponents	O
results	O
other	O
programs	O
tied	O
for	O
best	O
robertie	O
,	O
magriel	O
,	O
...	O
−13	O
pts	O
/	O
51	O
games	O
various	O
grandmasters	O
−7	O
pts	O
/	O
38	O
games	O
−1	O
pt	O
/	O
40	O
games	O
+6	O
pts	O
/	O
20	O
games	O
robertie	O
kazaros	O
table	O
16.1	O
:	O
summary	O
of	O
td-gammon	O
results	O
430	O
chapter	O
16	O
:	O
applications	B
and	I
case	I
studies	I
games	O
against	O
world-class	O
human	O
players	O
.	O
a	O
summary	O
of	O
the	O
results	O
is	O
given	O
in	O
table	O
16.1.	O
based	O
on	O
these	O
results	O
and	O
analyses	O
by	O
backgammon	B
grandmasters	O
(	O
robertie	O
,	O
1992	O
;	O
see	O
tesauro	O
,	O
1995	O
)	O
,	O
td-gammon	O
3.0	O
appeared	O
to	O
play	O
at	O
close	O
to	O
,	O
or	O
possibly	O
better	O
than	O
,	O
the	O
playing	O
strength	O
of	O
the	O
best	O
human	O
players	O
in	O
the	O
world	O
.	O
tesauro	O
reported	O
in	O
a	O
subsequent	O
article	O
(	O
tesauro	O
,	O
2002	O
)	O
the	O
results	O
of	O
an	O
extensive	O
rollout	O
analysis	O
of	O
the	O
move	O
decisions	O
and	O
doubling	O
decisions	O
of	O
td-gammon	O
relative	O
to	O
top	O
human	O
players	O
.	O
the	O
conclusion	O
was	O
that	O
td-gammon	O
3.1	O
had	O
a	O
“	O
lopsided	O
advantage	O
”	O
in	O
piece-movement	O
decisions	O
,	O
and	O
a	O
“	O
slight	O
edge	O
”	O
in	O
doubling	O
decisions	O
,	O
over	O
top	O
humans	O
.	O
td-gammon	O
had	O
a	O
signiﬁcant	O
impact	O
on	O
the	O
way	O
the	O
best	O
human	O
players	O
play	O
the	O
game	O
.	O
for	O
example	O
,	O
it	O
learned	O
to	O
play	O
certain	O
opening	O
positions	O
diﬀerently	O
than	O
was	O
the	O
convention	O
among	O
the	O
best	O
human	O
players	O
.	O
based	O
on	O
td-gammon	O
’	O
s	O
success	O
and	O
further	O
analysis	O
,	O
the	O
best	O
human	O
players	O
now	O
play	O
these	O
positions	O
as	O
td-gammon	O
does	O
(	O
tesauro	O
,	O
1995	O
)	O
.	O
the	O
impact	O
on	O
human	O
play	O
was	O
greatly	O
accelerated	O
when	O
several	O
other	O
self-teaching	O
neural	B
net	O
backgammon	B
programs	O
inspired	O
by	O
td-gammon	O
,	O
such	O
as	O
jelly-	O
ﬁsh	O
,	O
snowie	O
,	O
and	O
gnubackgammon	O
,	O
became	O
widely	O
available	O
.	O
these	O
programs	O
enabled	O
wide	O
dissemination	O
of	O
new	O
knowledge	O
generated	O
by	O
the	O
neural	B
nets	O
,	O
resulting	O
in	O
great	O
improvements	O
in	O
the	O
overall	O
caliber	O
of	O
human	O
tournament	O
play	O
(	O
tesauro	O
,	O
2002	O
)	O
.	O
16.2	O
samuel	O
’	O
s	O
checkers	O
player	O
an	O
important	O
precursor	O
to	O
tesauro	O
’	O
s	O
td-gammon	O
was	O
the	O
seminal	O
work	O
of	O
arthur	O
samuel	O
(	O
1959	O
,	O
1967	O
)	O
in	O
constructing	O
programs	O
for	O
learning	O
to	O
play	O
checkers	O
.	O
samuel	O
was	O
one	O
of	O
the	O
ﬁrst	O
to	O
make	O
eﬀective	O
use	O
of	O
heuristic	O
search	O
methods	O
and	O
of	O
what	O
we	O
would	O
now	O
call	O
temporal-diﬀerence	B
learning	I
.	O
his	O
checkers	O
players	O
are	O
instructive	O
case	O
studies	O
in	O
addition	O
to	O
being	O
of	O
historical	O
interest	O
.	O
we	O
emphasize	O
the	O
relationship	O
of	O
samuel	O
’	O
s	O
methods	O
to	O
modern	O
reinforcement	B
learning	I
methods	O
and	O
try	O
to	O
convey	O
some	O
of	O
samuel	O
’	O
s	O
motivation	B
for	O
using	O
them	O
.	O
samuel	O
ﬁrst	O
wrote	O
a	O
checkers-playing	O
program	O
for	O
the	O
ibm	O
701	O
in	O
1952.	O
his	O
ﬁrst	O
learning	O
program	O
was	O
completed	O
in	O
1955	O
and	O
was	O
demonstrated	O
on	O
television	O
in	O
1956.	O
later	O
versions	O
of	O
the	O
program	O
achieved	O
good	O
,	O
though	O
not	O
expert	O
,	O
playing	O
skill	O
.	O
samuel	O
was	O
attracted	O
to	O
game-playing	O
as	O
a	O
domain	O
for	O
studying	O
machine	O
learning	O
because	O
games	O
are	O
less	O
complicated	O
than	O
problems	O
“	O
taken	O
from	O
life	O
”	O
while	O
still	O
allowing	O
fruitful	O
study	O
of	O
how	O
heuristic	O
procedures	O
and	O
learning	O
can	O
be	O
used	O
together	O
.	O
he	O
chose	O
to	O
study	O
checkers	O
instead	O
of	O
chess	O
because	O
its	O
relative	O
simplicity	O
made	O
it	O
possible	O
to	O
focus	O
more	O
strongly	O
on	O
learning	O
.	O
samuel	O
’	O
s	O
programs	O
played	O
by	O
performing	O
a	O
lookahead	O
search	O
from	O
each	O
current	O
posi-	O
tion	B
.	O
they	O
used	O
what	O
we	O
now	O
call	O
heuristic	B
search	I
methods	O
to	O
determine	O
how	O
to	O
expand	O
the	O
search	O
tree	O
and	O
when	O
to	O
stop	O
searching	O
.	O
the	O
terminal	O
board	O
positions	O
of	O
each	O
search	O
were	O
evaluated	O
,	O
or	O
“	O
scored	O
,	O
”	O
by	O
a	O
value	B
function	I
,	O
or	O
“	O
scoring	O
polynomial	O
,	O
”	O
using	O
linear	B
function	I
approximation	I
.	O
in	O
this	O
and	O
other	O
respects	O
samuel	O
’	O
s	O
work	O
seems	O
to	O
have	O
been	O
inspired	O
by	O
the	O
suggestions	O
of	O
shannon	O
(	O
1950	O
)	O
.	O
in	O
particular	O
,	O
samuel	O
’	O
s	O
program	O
was	O
based	O
on	O
shannon	O
’	O
s	O
minimax	O
procedure	O
to	O
ﬁnd	O
the	O
best	O
move	O
from	O
the	O
current	O
position	O
.	O
working	O
backward	O
through	O
the	O
search	O
tree	O
from	O
the	O
scored	O
terminal	O
positions	O
,	O
each	O
posi-	O
tion	B
was	O
given	O
the	O
score	O
of	O
the	O
position	O
that	O
would	O
result	O
from	O
the	O
best	O
move	O
,	O
assuming	O
16.2.	O
samuel	O
’	O
s	O
checkers	O
player	O
431	O
that	O
the	O
machine	O
would	O
always	O
try	O
to	O
maximize	O
the	O
score	O
,	O
while	O
the	O
opponent	O
would	O
always	O
try	O
to	O
minimize	O
it	O
.	O
samuel	O
called	O
this	O
the	O
“	O
backed-up	O
score	O
”	O
of	O
the	O
position	O
.	O
when	O
the	O
minimax	O
procedure	O
reached	O
the	O
search	O
tree	O
’	O
s	O
root—the	O
current	O
position—it	O
yielded	O
the	O
best	O
move	O
under	O
the	O
assumption	O
that	O
the	O
opponent	O
would	O
be	O
using	O
the	O
same	O
evaluation	O
criterion	O
,	O
shifted	O
to	O
its	O
point	O
of	O
view	O
.	O
some	O
versions	O
of	O
samuel	O
’	O
s	O
programs	O
used	O
sophisticated	O
search	B
control	I
methods	O
analogous	O
to	O
what	O
are	O
known	O
as	O
“	O
alpha-beta	O
”	O
cutoﬀs	O
(	O
e.g.	O
,	O
see	O
pearl	O
,	O
1984	O
)	O
.	O
samuel	O
used	O
two	O
main	O
learning	O
methods	O
,	O
the	O
simplest	O
of	O
which	O
he	O
called	O
rote	O
learning	O
.	O
it	O
consisted	O
simply	O
of	O
saving	O
a	O
description	O
of	O
each	O
board	O
position	O
encountered	O
during	O
play	O
together	O
with	O
its	O
backed-up	O
value	B
determined	O
by	O
the	O
minimax	O
procedure	O
.	O
the	O
result	O
was	O
that	O
if	O
a	O
position	O
that	O
had	O
already	O
been	O
encountered	O
were	O
to	O
occur	O
again	O
as	O
a	O
terminal	O
position	O
of	O
a	O
search	O
tree	O
,	O
the	O
depth	O
of	O
the	O
search	O
was	O
eﬀectively	O
ampliﬁed	O
because	O
this	O
position	O
’	O
s	O
stored	O
value	B
cached	O
the	O
results	O
of	O
one	O
or	O
more	O
searches	O
conducted	O
earlier	O
.	O
one	O
initial	O
problem	O
was	O
that	O
the	O
program	O
was	O
not	O
encouraged	O
to	O
move	O
along	O
the	O
most	O
direct	O
path	O
to	O
a	O
win	O
.	O
samuel	O
gave	O
it	O
a	O
“	O
a	O
sense	O
of	O
direction	O
”	O
by	O
decreasing	O
a	O
position	O
’	O
s	O
value	B
a	O
small	O
amount	O
each	O
time	O
it	O
was	O
backed	O
up	O
a	O
level	O
(	O
called	O
a	O
ply	O
)	O
during	O
the	O
minimax	O
analysis	O
.	O
“	O
if	O
the	O
program	O
is	O
now	O
faced	O
with	O
a	O
choice	O
of	O
board	O
positions	O
whose	O
scores	O
diﬀer	O
only	O
by	O
the	O
ply	O
number	O
,	O
it	O
will	O
automatically	O
make	O
the	O
most	O
advantageous	O
choice	O
,	O
choosing	O
a	O
low-ply	O
alternative	O
if	O
winning	O
and	O
a	O
high-ply	O
alternative	O
if	O
losing	O
”	O
(	O
samuel	O
,	O
1959	O
,	O
p.	O
80	O
)	O
.	O
samuel	O
found	O
this	O
discounting-like	O
technique	O
essential	O
to	O
successful	O
learning	O
.	O
rote	O
learning	O
produced	O
slow	O
but	O
continual	O
improvement	O
that	O
was	O
most	O
eﬀective	O
for	O
opening	O
and	O
endgame	O
play	O
.	O
his	O
program	O
became	O
a	O
“	O
better-than-average	O
novice	O
”	O
after	O
learning	O
from	O
many	O
games	O
against	O
itself	O
,	O
a	O
variety	O
of	O
human	O
opponents	O
,	O
and	O
from	O
book	O
games	O
in	O
a	O
supervised	B
learning	I
mode	O
.	O
rote	O
learning	O
and	O
other	O
aspects	O
of	O
samuel	O
’	O
s	O
work	O
strongly	O
suggest	O
the	O
essential	O
idea	O
of	O
temporal-diﬀerence	O
learning—that	O
the	O
value	B
of	O
a	O
state	B
should	O
equal	O
the	O
value	B
of	O
likely	O
following	O
states	O
.	O
samuel	O
came	O
closest	O
to	O
this	O
idea	O
in	O
his	O
second	O
learning	O
method	O
,	O
his	O
“	O
learning	O
by	O
generalization	O
”	O
procedure	O
for	O
modifying	O
the	O
parameters	O
of	O
the	O
value	B
func-	O
tion	B
.	O
samuel	O
’	O
s	O
method	O
was	O
the	O
same	O
in	O
concept	O
as	O
that	O
used	O
much	O
later	O
by	O
tesauro	O
in	O
td-gammon	O
.	O
he	O
played	O
his	O
program	O
many	O
games	O
against	O
another	O
version	O
of	O
itself	O
and	O
performed	O
an	O
update	O
after	O
each	O
move	O
.	O
the	O
idea	O
of	O
samuel	O
’	O
s	O
update	O
is	O
suggested	O
by	O
the	O
backup	B
diagram	I
in	O
figure	O
16.2.	O
each	O
open	O
circle	O
represents	O
a	O
position	O
where	O
the	O
program	O
moves	O
next	O
,	O
an	O
on-move	O
position	O
,	O
and	O
each	O
solid	O
circle	O
represents	O
a	O
position	O
where	O
the	O
opponent	O
moves	O
next	O
.	O
an	O
update	O
was	O
made	O
to	O
the	O
value	B
of	O
each	O
on-move	O
position	O
after	O
a	O
move	O
by	O
each	O
side	O
,	O
resulting	O
in	O
a	O
second	O
on-move	O
position	O
.	O
the	O
update	O
was	O
toward	O
the	O
minimax	O
value	B
of	O
a	O
search	O
launched	O
from	O
the	O
second	O
on-move	O
position	O
.	O
thus	O
,	O
the	O
overall	O
eﬀect	O
was	O
that	O
of	O
a	O
backing-up	O
over	O
one	O
full	O
move	O
of	O
real	O
events	O
and	O
then	O
a	O
search	O
over	O
possible	O
events	O
,	O
as	O
suggested	O
by	O
figure	O
16.2.	O
samuel	O
’	O
s	O
actual	O
algorithm	O
was	O
signiﬁcantly	O
more	O
complex	O
than	O
this	O
for	O
computational	O
reasons	O
,	O
but	O
this	O
was	O
the	O
basic	O
idea	O
.	O
samuel	O
did	O
not	O
include	O
explicit	O
rewards	O
.	O
instead	O
,	O
he	O
ﬁxed	O
the	O
weight	O
of	O
the	O
most	O
important	O
feature	O
,	O
the	O
piece	O
advantage	O
feature	O
,	O
which	O
measured	O
the	O
number	O
of	O
pieces	O
the	O
program	O
had	O
relative	O
to	O
how	O
many	O
its	O
opponent	O
had	O
,	O
giving	O
higher	O
weight	O
to	O
kings	O
,	O
and	O
including	O
reﬁnements	O
so	O
that	O
it	O
was	O
better	O
to	O
trade	O
pieces	O
when	O
winning	O
than	O
when	O
losing	O
.	O
thus	O
,	O
the	O
goal	B
of	O
samuel	O
’	O
s	O
program	O
was	O
to	O
improve	O
its	O
piece	O
advantage	O
,	O
which	O
in	O
432	O
chapter	O
16	O
:	O
applications	B
and	I
case	I
studies	I
figure	O
16.2	O
:	O
the	O
backup	B
diagram	I
for	O
samuel	O
’	O
s	O
checkers	O
player	O
.	O
checkers	O
is	O
highly	O
correlated	O
with	O
winning	O
.	O
however	O
,	O
samuel	O
’	O
s	O
learning	O
method	O
may	O
have	O
been	O
missing	O
an	O
essential	O
part	O
of	O
a	O
sound	O
temporal-diﬀerence	O
algorithm	O
.	O
temporal-diﬀerence	B
learning	I
can	O
be	O
viewed	O
as	O
a	O
way	O
of	O
making	O
a	O
value	B
function	I
consistent	O
with	O
itself	O
,	O
and	O
this	O
we	O
can	O
clearly	O
see	O
in	O
samuel	O
’	O
s	O
method	O
.	O
but	O
also	O
needed	O
is	O
a	O
way	O
of	O
tying	O
the	O
value	B
function	I
to	O
the	O
true	O
value	O
of	O
the	O
states	O
.	O
we	O
have	O
enforced	O
this	O
via	O
rewards	O
and	O
by	O
discounting	B
or	O
giving	O
a	O
ﬁxed	O
value	B
to	O
the	O
terminal	O
state	B
.	O
but	O
samuel	O
’	O
s	O
method	O
included	O
no	O
rewards	O
and	O
no	O
special	O
treatment	O
of	O
the	O
terminal	O
positions	O
of	O
games	O
.	O
as	O
samuel	O
himself	O
pointed	O
out	O
,	O
his	O
value	B
function	I
could	O
have	O
become	O
consistent	O
merely	O
by	O
giving	O
a	O
constant	O
value	B
to	O
all	O
positions	O
.	O
he	O
hoped	O
to	O
discourage	O
such	O
solutions	O
by	O
giving	O
his	O
piece-advantage	O
term	O
a	O
large	O
,	O
nonmodiﬁable	O
weight	O
.	O
but	O
although	O
this	O
may	O
decrease	O
the	O
likelihood	O
of	O
ﬁnding	O
useless	O
evaluation	O
functions	O
,	O
it	O
does	O
not	O
prohibit	O
them	O
.	O
for	O
example	O
,	O
a	O
constant	O
function	O
could	O
still	O
be	O
attained	O
by	O
setting	O
the	O
modiﬁable	O
weights	O
so	O
as	O
to	O
cancel	O
the	O
eﬀect	O
of	O
the	O
nonmodiﬁable	O
one	O
.	O
because	O
samuel	O
’	O
s	O
learning	O
procedure	O
was	O
not	O
constrained	O
to	O
ﬁnd	O
useful	O
evaluation	O
functions	O
,	O
it	O
should	O
have	O
been	O
possible	O
for	O
it	O
to	O
become	O
worse	O
with	O
experience	O
.	O
in	O
fact	O
,	O
samuel	O
reported	O
observing	O
this	O
during	O
extensive	O
self-play	O
training	O
sessions	O
.	O
to	O
get	O
the	O
program	O
improving	O
again	O
,	O
samuel	O
had	O
to	O
intervene	O
and	O
set	O
the	O
weight	O
with	O
the	O
largest	O
absolute	O
value	B
back	O
to	O
zero	O
.	O
his	O
interpretation	O
was	O
that	O
this	O
drastic	O
intervention	O
jarred	O
the	O
program	O
out	O
of	O
local	O
optima	O
,	O
but	O
another	O
possibility	O
is	O
that	O
it	O
jarred	O
the	O
program	O
out	O
of	O
evaluation	O
functions	O
that	O
were	O
consistent	O
but	O
had	O
little	O
to	O
do	O
with	O
winning	O
or	O
losing	O
the	O
game	O
.	O
despite	O
these	O
potential	O
problems	O
,	O
samuel	O
’	O
s	O
checkers	O
player	O
using	O
the	O
generalization	O
learning	O
method	O
approached	O
“	O
better-than-average	O
”	O
play	O
.	O
fairly	O
good	O
amateur	O
opponents	O
characterized	O
it	O
as	O
“	O
tricky	O
but	O
beatable	O
”	O
(	O
samuel	O
,	O
1959	O
)	O
.	O
in	O
contrast	O
to	O
the	O
rote-learning	O
version	O
,	O
this	O
version	O
was	O
able	O
to	O
develop	O
a	O
good	O
middle	O
game	O
but	O
remained	O
weak	O
in	O
open-	O
ing	B
and	O
endgame	O
play	O
.	O
this	O
program	O
also	O
included	O
an	O
ability	O
to	O
search	O
through	O
sets	O
of	O
hypothetical	O
eventsactual	O
eventsbackup	O
16.3.	O
watson	O
’	O
s	O
daily-double	O
wagering	O
433	O
features	O
to	O
ﬁnd	O
those	O
that	O
were	O
most	O
useful	O
in	O
forming	O
the	O
value	B
function	I
.	O
a	O
later	O
version	O
(	O
samuel	O
,	O
1967	O
)	O
included	O
reﬁnements	O
in	O
its	O
search	O
procedure	O
,	O
such	O
as	O
alpha-beta	O
prun-	O
ing	B
,	O
extensive	O
use	O
of	O
a	O
supervised	B
learning	I
mode	O
called	O
“	O
book	O
learning	O
,	O
”	O
and	O
hierarchical	O
lookup	O
tables	O
called	O
signature	O
tables	O
(	O
griﬃth	O
,	O
1966	O
)	O
to	O
represent	O
the	O
value	B
function	I
in-	O
stead	O
of	O
linear	O
function	B
approximation	I
.	O
this	O
version	O
learned	O
to	O
play	O
much	O
better	O
than	O
the	O
1959	O
program	O
,	O
though	O
still	O
not	O
at	O
a	O
master	O
level	O
.	O
samuel	O
’	O
s	O
checkers-playing	O
program	O
was	O
widely	O
recognized	O
as	O
a	O
signiﬁcant	O
achievement	O
in	O
artiﬁcial	O
intelligence	O
and	O
machine	O
learning	O
.	O
16.3	O
watson	O
’	O
s	O
daily-double	O
wagering	O
ibm	O
watson1	O
is	O
the	O
system	O
developed	O
by	O
a	O
team	O
of	O
ibm	O
researchers	O
to	O
play	O
the	O
popular	O
tv	O
quiz	O
show	O
jeopardy	O
!	O
.2	O
it	O
gained	O
fame	O
in	O
2011	O
by	O
winning	O
ﬁrst	O
prize	O
in	O
an	O
exhibition	O
match	O
against	O
human	O
champions	O
.	O
although	O
the	O
main	O
technical	O
achievement	O
demon-	O
strated	O
by	O
watson	O
was	O
its	O
ability	O
to	O
quickly	O
and	O
accurately	O
answer	O
natural	O
language	O
questions	O
over	O
broad	O
areas	O
of	O
general	O
knowledge	O
,	O
its	O
winning	O
jeopardy	O
!	O
performance	O
also	O
relied	O
on	O
sophisticated	O
decision-making	O
strategies	O
for	O
critical	O
parts	O
of	O
the	O
game	O
.	O
tesauro	O
,	O
gondek	O
,	O
lechner	O
,	O
fan	O
,	O
and	O
prager	O
(	O
2012	O
,	O
2013	O
)	O
adapted	O
tesauro	O
’	O
s	O
td-gammon	O
system	O
described	O
above	O
to	O
create	O
the	O
strategy	O
used	O
by	O
watson	O
in	O
“	O
daily-double	O
”	O
(	O
dd	O
)	O
wa-	O
gering	O
in	O
its	O
celebrated	O
winning	O
performance	O
against	O
human	O
champions	O
.	O
these	O
authors	O
report	O
that	O
the	O
eﬀectiveness	O
of	O
this	O
wagering	O
strategy	O
went	O
well	O
beyond	O
what	O
human	O
players	O
are	O
able	O
to	O
do	O
in	O
live	O
game	O
play	O
,	O
and	O
that	O
it	O
,	O
along	O
with	O
other	O
advanced	O
strate-	O
gies	O
,	O
was	O
an	O
important	O
contributor	O
to	O
watson	O
’	O
s	O
impressive	O
winning	O
performance	O
.	O
here	O
we	O
focus	O
only	O
on	O
dd	O
wagering	O
because	O
it	O
is	O
the	O
component	O
of	O
watson	O
that	O
owes	O
the	O
most	O
to	O
reinforcement	B
learning	I
.	O
jeopardy	O
!	O
is	O
played	O
by	O
three	O
contestants	O
who	O
face	O
a	O
board	O
showing	O
30	O
squares	O
,	O
each	O
of	O
which	O
hides	O
a	O
clue	O
and	O
has	O
a	O
dollar	O
value	B
.	O
the	O
squares	O
are	O
arranged	O
in	O
six	O
columns	O
,	O
each	O
corresponding	O
to	O
a	O
diﬀerent	O
category	O
.	O
a	O
contestant	O
selects	O
a	O
square	O
,	O
the	O
host	O
reads	O
the	O
square	O
’	O
s	O
clue	O
,	O
and	O
each	O
contestant	O
may	O
choose	O
to	O
respond	O
to	O
the	O
clue	O
by	O
sounding	O
a	O
buzzer	O
(	O
“	O
buzzing	O
in	O
”	O
)	O
.	O
the	O
ﬁrst	O
contestant	O
to	O
buzz	O
in	O
gets	O
to	O
try	O
responding	O
to	O
the	O
clue	O
.	O
if	O
this	O
contestant	O
’	O
s	O
response	O
is	O
correct	O
,	O
their	O
score	O
increases	O
by	O
the	O
dollar	O
value	B
of	O
the	O
square	O
;	O
if	O
their	O
response	O
is	O
not	O
correct	O
,	O
or	O
if	O
they	O
do	O
not	O
respond	O
within	O
ﬁve	O
seconds	O
,	O
their	O
score	O
decreases	O
by	O
that	O
amount	O
,	O
and	O
the	O
other	O
contestants	O
get	O
a	O
chance	O
to	O
buzz	O
in	O
to	O
respond	O
to	O
the	O
same	O
clue	O
.	O
one	O
or	O
two	O
squares	O
(	O
depending	O
on	O
the	O
game	O
’	O
s	O
current	O
round	O
)	O
are	O
special	O
dd	O
squares	O
.	O
a	O
contestant	O
who	O
selects	O
one	O
of	O
these	O
gets	O
an	O
exclusive	O
opportunity	O
to	O
respond	O
to	O
the	O
square	O
’	O
s	O
clue	O
and	O
has	O
to	O
decide—before	O
the	O
clue	O
is	O
revealed—on	O
how	O
much	O
to	O
wager	O
,	O
or	O
bet	O
.	O
the	O
bet	O
has	O
to	O
be	O
greater	O
than	O
ﬁve	O
dollars	O
but	O
not	O
greater	O
than	O
the	O
contestant	O
’	O
s	O
current	O
score	O
.	O
if	O
the	O
contestant	O
responds	O
correctly	O
to	O
the	O
dd	O
clue	O
,	O
their	O
score	O
increases	O
by	O
the	O
bet	O
amount	O
;	O
otherwise	O
it	O
decreases	O
by	O
the	O
bet	O
amount	O
.	O
at	O
the	O
end	O
of	O
each	O
game	O
is	O
a	O
“	O
final	O
jeopardy	O
”	O
(	O
fj	O
)	O
round	O
in	O
which	O
each	O
contestant	O
writes	O
down	O
a	O
sealed	O
bet	O
and	O
then	O
writes	O
an	O
answer	O
after	O
the	O
clue	O
is	O
read	O
.	O
1registered	O
trademark	O
of	O
ibm	O
corp.	O
2registered	O
trademark	O
of	O
jeopardy	O
productions	O
inc.	O
434	O
chapter	O
16	O
:	O
applications	B
and	I
case	I
studies	I
the	O
contestant	O
with	O
the	O
highest	O
score	O
after	O
three	O
rounds	O
of	O
play	O
(	O
where	O
a	O
round	O
consists	O
of	O
revealing	O
all	O
30	O
clues	O
)	O
is	O
the	O
winner	O
.	O
the	O
game	O
has	O
many	O
other	O
details	O
,	O
but	O
these	O
are	O
enough	O
to	O
appreciate	O
the	O
importance	O
of	O
dd	O
wagering	O
.	O
winning	O
or	O
losing	O
often	O
depends	O
on	O
a	O
contestant	O
’	O
s	O
dd	O
wagering	O
strategy	O
.	O
whenever	O
watson	O
selected	O
a	O
dd	O
square	O
,	O
it	O
chose	O
its	O
bet	O
by	O
comparing	O
action	B
values	O
,	O
ˆq	O
(	O
s	O
,	O
bet	O
)	O
,	O
that	O
estimated	O
the	O
probability	O
of	O
a	O
win	O
from	O
the	O
current	O
game	O
state	O
,	O
s	O
,	O
for	O
each	O
round-dollar	O
legal	O
bet	O
.	O
except	O
for	O
some	O
risk-abatement	O
measures	O
described	O
below	O
,	O
watson	O
selected	O
the	O
bet	O
with	O
the	O
maximum	O
action	B
value	O
.	O
action	B
values	O
were	O
computed	O
whenever	O
a	O
betting	O
decision	O
was	O
needed	O
by	O
using	O
two	O
types	O
of	O
estimates	O
that	O
were	O
learned	O
before	O
any	O
live	O
game	O
play	O
took	O
place	O
.	O
the	O
ﬁrst	O
were	O
estimated	O
values	O
of	O
the	O
afterstates	B
(	O
section	O
6.8	O
)	O
that	O
would	O
result	O
from	O
selecting	O
each	O
legal	O
bet	O
.	O
these	O
estimates	O
were	O
ob-	O
tained	O
from	O
a	O
state-value	O
function	O
,	O
ˆv	O
(	O
·	O
,	O
w	O
)	O
,	O
deﬁned	O
by	O
parameters	O
w	O
,	O
that	O
gave	O
estimates	O
of	O
the	O
probability	O
of	O
a	O
win	O
for	O
watson	O
from	O
any	O
game	O
state	O
.	O
the	O
second	O
estimates	O
used	O
to	O
compute	O
action	B
values	O
gave	O
the	O
“	O
in-category	O
dd	O
conﬁdence	O
,	O
”	O
pdd	O
,	O
which	O
estimated	O
the	O
likelihood	O
that	O
watson	O
would	O
respond	O
correctly	O
to	O
the	O
as-yet	O
unrevealed	O
dd	O
clue	O
.	O
tesauro	O
et	O
al	O
.	O
used	O
the	O
reinforcement	B
learning	I
approach	O
of	O
td-gammon	O
described	O
above	O
to	O
learn	O
ˆv	O
(	O
·	O
,	O
w	O
)	O
:	O
a	O
straightforward	O
combination	O
of	O
nonlinear	O
td	O
(	O
λ	O
)	O
using	O
a	O
multi-	O
layer	O
neural	B
network	O
with	O
weights	O
w	O
trained	O
by	O
backpropagating	O
td	O
errors	O
during	O
many	O
simulated	O
games	O
.	O
states	O
were	O
represented	O
to	O
the	O
network	O
by	O
feature	O
vectors	O
speciﬁcally	O
designed	O
for	O
jeopardy	O
!	O
.	O
features	O
included	O
the	O
current	O
scores	O
of	O
the	O
three	O
players	O
,	O
how	O
many	O
dds	O
remained	O
,	O
the	O
total	O
dollar	O
value	B
of	O
the	O
remaining	O
clues	O
,	O
and	O
other	O
information	O
related	O
to	O
the	O
amount	O
of	O
play	O
left	O
in	O
the	O
game	O
.	O
unlike	O
td-gammon	O
,	O
which	O
learned	O
by	O
self-play	O
,	O
watson	O
’	O
s	O
ˆv	O
was	O
learned	O
over	O
millions	O
of	O
simulated	O
games	O
against	O
carefully-	O
crafted	O
models	O
of	O
human	O
players	O
.	O
in-category	O
conﬁdence	O
estimates	O
were	O
conditioned	O
on	O
the	O
number	O
of	O
right	O
responses	O
r	O
and	O
wrong	O
responses	O
w	O
that	O
watson	O
gave	O
in	O
previously-	O
played	O
clues	O
in	O
the	O
current	O
category	O
.	O
the	O
dependencies	O
on	O
(	O
r	O
,	O
w	O
)	O
were	O
estimated	O
from	O
watson	O
’	O
s	O
actual	O
accuracies	O
over	O
many	O
thousands	O
of	O
historical	O
categories	O
.	O
with	O
the	O
previously	O
learned	O
value	B
function	I
ˆv	O
and	O
in-category	O
dd	O
conﬁdence	O
pdd	O
,	O
watson	O
computed	O
ˆq	O
(	O
s	O
,	O
bet	O
)	O
for	O
each	O
legal	O
round-dollar	O
bet	O
as	O
follows	O
:	O
ˆq	O
(	O
s	O
,	O
bet	O
)	O
=	O
pdd	O
×	O
ˆv	O
(	O
sw	O
+	O
bet	O
,	O
.	O
.	O
.	O
)	O
+	O
(	O
1	O
−	O
pdd	O
)	O
×	O
ˆv	O
(	O
sw	O
−	O
bet	O
,	O
.	O
.	O
.	O
)	O
,	O
(	O
16.2	O
)	O
where	O
sw	O
is	O
watson	O
’	O
s	O
current	O
score	O
,	O
and	O
ˆv	O
gives	O
the	O
estimated	O
value	B
for	O
the	O
game	O
state	O
after	O
watson	O
’	O
s	O
response	O
to	O
the	O
dd	O
clue	O
,	O
which	O
is	O
either	O
correct	O
or	O
incorrect	O
.	O
computing	O
an	O
action	B
value	O
this	O
way	O
corresponds	O
to	O
the	O
insight	O
from	O
exercise	O
3.17	O
that	O
an	O
action	B
value	O
is	O
the	O
expected	O
next	O
state	B
value	O
given	O
the	O
action	B
(	O
except	O
that	O
here	O
it	O
is	O
the	O
expected	O
next	O
afterstate	O
value	B
because	O
the	O
full	O
next	O
state	B
of	O
the	O
entire	O
game	O
depends	O
on	O
the	O
next	O
square	O
selection	O
)	O
.	O
tesauro	O
et	O
al	O
.	O
found	O
that	O
selecting	O
bets	O
by	O
maximizing	O
action	B
values	O
incurred	O
“	O
a	O
fright-	O
ening	O
amount	O
of	O
risk	O
,	O
”	O
meaning	O
that	O
if	O
watson	O
’	O
s	O
response	O
to	O
the	O
clue	O
happened	O
to	O
be	O
wrong	O
,	O
the	O
loss	O
could	O
be	O
disastrous	O
for	O
its	O
chances	O
of	O
winning	O
.	O
to	O
decrease	O
the	O
downside	O
risk	O
of	O
a	O
wrong	O
answer	O
,	O
tesauro	O
et	O
al	O
.	O
adjusted	O
(	O
16.2	O
)	O
by	O
subtracting	O
a	O
small	O
fraction	O
of	O
the	O
standard	O
deviation	O
over	O
watson	O
’	O
s	O
correct/incorrect	O
afterstate	O
evaluations	O
.	O
they	O
further	O
reduced	O
risk	O
by	O
prohibiting	O
bets	O
that	O
would	O
cause	O
the	O
wrong-answer	O
afterstate	O
value	B
to	O
decrease	O
below	O
a	O
certain	O
limit	O
.	O
these	O
measures	O
slightly	O
reduced	O
watson	O
’	O
s	O
ex-	O
pectation	O
of	O
winning	O
,	O
but	O
they	O
signiﬁcantly	O
reduced	O
downside	O
risk	O
,	O
not	O
only	O
in	O
terms	O
of	O
16.3.	O
watson	O
’	O
s	O
daily-double	O
wagering	O
435	O
average	O
risk	O
per	O
dd	O
bet	O
,	O
but	O
even	O
more	O
so	O
in	O
extreme-risk	O
scenarios	O
where	O
a	O
risk-neutral	O
watson	O
would	O
bet	O
most	O
or	O
all	O
of	O
its	O
bankroll	O
.	O
why	O
was	O
the	O
td-gammon	O
method	O
of	O
self-play	O
not	O
used	O
to	O
learn	O
the	O
critical	O
value	B
function	I
ˆv	O
?	O
learning	O
from	O
self-play	O
in	O
jeopardy	O
!	O
would	O
not	O
have	O
worked	O
very	O
well	O
because	O
watson	O
was	O
so	O
diﬀerent	O
from	O
any	O
human	O
contestant	O
.	O
self-play	O
would	O
have	O
led	O
to	O
exploration	O
of	O
state	O
space	O
regions	O
that	O
are	O
not	O
typical	O
for	O
play	O
against	O
human	O
opponents	O
,	O
particularly	O
human	O
champions	O
.	O
in	O
addition	O
,	O
unlike	O
backgammon	B
,	O
jeopardy	O
!	O
is	O
a	O
game	O
of	O
imperfect	O
information	O
because	O
contestants	O
do	O
not	O
have	O
access	O
to	O
all	O
the	O
information	O
inﬂuencing	O
their	O
opponents	O
’	O
play	O
.	O
in	O
particular	O
,	O
jeopardy	O
!	O
contestants	O
do	O
not	O
know	O
how	O
much	O
conﬁdence	O
their	O
opponents	O
have	O
for	O
responding	O
to	O
clues	O
in	O
the	O
various	O
categories	O
.	O
self-play	O
would	O
have	O
been	O
something	O
like	O
playing	O
poker	O
with	O
someone	O
who	O
is	O
holding	O
the	O
same	O
cards	O
that	O
you	O
hold	O
.	O
as	O
a	O
result	O
of	O
these	O
complications	O
,	O
much	O
of	O
the	O
eﬀort	O
in	O
developing	O
watson	O
’	O
s	O
dd-	O
wagering	O
strategy	O
was	O
devoted	O
to	O
creating	O
good	O
models	O
of	O
human	O
opponents	O
.	O
the	O
models	O
did	O
not	O
address	O
the	O
natural	O
language	O
aspect	O
of	O
the	O
game	O
,	O
but	O
were	O
instead	O
stochastic	O
process	O
models	O
of	O
events	O
that	O
can	O
occur	O
during	O
play	O
.	O
statistics	O
were	O
extracted	O
from	O
an	O
extensive	O
fan-created	O
archive	O
of	O
game	O
information	O
from	O
the	O
beginning	O
of	O
the	O
show	O
to	O
the	O
present	O
day	O
.	O
the	O
archive	O
includes	O
information	O
such	O
as	O
the	O
ordering	O
of	O
the	O
clues	O
,	O
right	O
and	O
wrong	O
contestant	O
answers	O
,	O
dd	O
locations	O
,	O
and	O
dd	O
and	O
fj	O
bets	O
for	O
nearly	O
300,000	O
clues	O
.	O
three	O
models	O
were	O
constructed	O
:	O
an	O
average	O
contestant	O
model	O
(	O
based	O
on	O
all	O
the	O
data	O
)	O
,	O
a	O
champion	O
model	O
(	O
based	O
on	O
statistics	O
from	O
games	O
with	O
the	O
100	O
best	O
players	O
)	O
,	O
and	O
a	O
grand	O
champion	O
model	O
(	O
based	O
on	O
statistics	O
from	O
games	O
with	O
the	O
10	O
best	O
players	O
)	O
.	O
in	O
addition	O
to	O
serving	O
as	O
opponents	O
during	O
learning	O
,	O
the	O
models	O
were	O
used	O
to	O
asses	O
the	O
beneﬁts	O
produced	O
by	O
the	O
learned	O
dd-wagering	O
strategy	O
.	O
watson	O
’	O
s	O
win	O
rate	O
in	O
simulation	O
when	O
it	O
used	O
a	O
baseline	B
heuristic	O
dd-wagering	O
strategy	O
was	O
61	O
%	O
;	O
when	O
it	O
used	O
the	O
learned	O
values	O
and	O
a	O
default	O
conﬁdence	O
value	B
,	O
its	O
win	O
rate	O
increased	O
to	O
64	O
%	O
;	O
and	O
with	O
live	O
in-category	O
conﬁdence	O
,	O
it	O
was	O
67	O
%	O
.	O
tesauro	O
et	O
al	O
.	O
regarded	O
this	O
as	O
a	O
signiﬁcant	O
improvement	O
,	O
given	O
that	O
the	O
dd	O
wagering	O
was	O
needed	O
only	O
about	O
1.5	O
to	O
2	O
times	O
in	O
each	O
game	O
.	O
because	O
watson	O
had	O
only	O
a	O
few	O
seconds	O
to	O
bet	O
,	O
as	O
well	O
as	O
to	O
select	O
squares	O
and	O
decide	O
whether	O
or	O
not	O
to	O
buzz	O
in	O
,	O
the	O
computation	O
time	O
needed	O
to	O
make	O
these	O
decisions	O
was	O
a	O
critical	O
factor	O
.	O
the	O
neural	B
network	O
implementation	O
of	O
ˆv	O
allowed	O
dd	O
bets	O
to	O
be	O
made	O
quickly	O
enough	O
to	O
meet	O
the	O
time	O
constraints	O
of	O
live	O
play	O
.	O
however	O
,	O
once	O
games	O
could	O
be	O
simulated	O
fast	O
enough	O
through	O
improvements	O
in	O
the	O
simulation	O
software	O
,	O
near	O
the	O
end	O
of	O
a	O
game	O
it	O
was	O
feasible	O
to	O
estimate	O
the	O
value	B
of	O
bets	O
by	O
averaging	O
over	O
many	O
monte-carlo	O
trials	O
in	O
which	O
the	O
consequence	O
of	O
each	O
bet	O
was	O
determined	O
by	O
simulating	O
play	O
to	O
the	O
game	O
’	O
s	O
end	O
.	O
selecting	O
endgame	O
dd	O
bets	O
in	O
live	O
play	O
based	O
on	O
monte-carlo	O
trials	O
instead	O
of	O
the	O
neural	B
network	O
signiﬁcantly	O
improved	O
watson	O
’	O
s	O
performance	O
because	O
errors	O
in	O
value	O
estimates	O
in	O
endgames	O
could	O
seriously	O
aﬀect	O
its	O
chances	O
of	O
winning	O
.	O
making	O
all	O
the	O
decisions	O
via	O
monte-carlo	O
trials	O
might	O
have	O
led	O
to	O
better	O
wagering	O
decisions	O
,	O
but	O
this	O
was	O
simply	O
impossible	O
given	O
the	O
complexity	O
of	O
the	O
game	O
and	O
the	O
time	O
constraints	O
of	O
live	O
play	O
.	O
although	O
its	O
ability	O
to	O
quickly	O
and	O
accurately	O
answer	O
natural	O
language	O
questions	O
stands	O
out	O
as	O
watson	O
’	O
s	O
major	O
achievement	O
,	O
all	O
of	O
its	O
sophisticated	O
decision	O
strategies	O
436	O
chapter	O
16	O
:	O
applications	B
and	I
case	I
studies	I
contributed	O
to	O
its	O
impressive	O
defeat	O
of	O
human	O
champions	O
.	O
according	O
to	O
tesauro	O
et	O
al	O
.	O
(	O
2012	O
)	O
:	O
...	O
it	O
is	O
plainly	O
evident	O
that	O
our	O
strategy	O
algorithms	O
achieve	O
a	O
level	O
of	O
quanti-	O
tative	O
precision	O
and	O
real-time	O
performance	O
that	O
exceeds	O
human	O
capabilities	O
.	O
this	O
is	O
particularly	O
true	O
in	O
the	O
cases	O
of	O
dd	O
wagering	O
and	O
endgame	O
buzzing	O
,	O
where	O
humans	O
simply	O
can	O
not	O
come	O
close	O
to	O
matching	O
the	O
precise	O
equity	O
and	O
conﬁdence	O
estimates	O
and	O
complex	O
decision	O
calculations	O
performed	O
by	O
watson	O
.	O
16.4	O
optimizing	B
memory	I
control	I
most	O
computers	O
use	O
dynamic	O
random	O
access	O
memory	O
(	O
dram	O
)	O
as	O
their	O
main	O
memory	O
because	O
of	O
its	O
low	O
cost	O
and	O
high	O
capacity	O
.	O
the	O
job	O
of	O
a	O
dram	O
memory	O
controller	O
is	O
to	O
eﬃciently	O
use	O
the	O
interface	O
between	O
the	O
processor	O
chip	O
and	O
an	O
oﬀ-chip	O
dram	O
system	O
to	O
provide	O
the	O
high-bandwidth	O
and	O
low-latency	O
data	O
transfer	O
necessary	O
for	O
high-	O
speed	O
program	O
execution	O
.	O
a	O
memory	O
controller	O
needs	O
to	O
deal	O
with	O
dynamically	O
changing	O
patterns	O
of	O
read/write	O
requests	O
while	O
adhering	O
to	O
a	O
large	O
number	O
of	O
timing	O
and	O
resource	O
constraints	O
required	O
by	O
the	O
hardware	O
.	O
this	O
is	O
a	O
formidable	O
scheduling	O
problem	O
,	O
especially	O
with	O
modern	O
processors	O
with	O
multiple	O
cores	O
sharing	O
the	O
same	O
dram	O
.	O
˙ipek	O
,	O
mutlu	O
,	O
mart´ınez	O
,	O
and	O
caruana	O
(	O
2008	O
)	O
(	O
also	O
mart´ınez	O
and	O
˙ipek	O
,	O
2009	O
)	O
designed	O
a	O
reinforcement	B
learning	I
memory	O
controller	O
and	O
demonstrated	O
that	O
it	O
can	O
signiﬁcantly	O
improve	O
the	O
speed	O
of	O
program	O
execution	O
over	O
what	O
was	O
possible	O
with	O
conventional	O
con-	O
trollers	O
at	O
the	O
time	O
of	O
their	O
research	O
.	O
they	O
were	O
motivated	O
by	O
limitations	O
of	O
existing	O
state-of-the-art	O
controllers	O
that	O
used	O
policies	O
that	O
did	O
not	O
take	O
advantage	O
of	O
past	O
schedul-	O
ing	B
experience	O
and	O
did	O
not	O
account	O
for	O
long-term	O
consequences	O
of	O
scheduling	O
decisions	O
.	O
˙ipek	O
et	O
al.	O
’	O
s	O
project	O
was	O
carried	O
out	O
by	O
means	O
of	O
simulation	O
,	O
but	O
they	O
designed	O
the	O
controller	O
at	O
the	O
detailed	O
level	O
of	O
the	O
hardware	O
needed	O
to	O
implement	O
it—including	O
the	O
learning	O
algorithm—directly	O
on	O
a	O
processor	O
chip	O
.	O
accessing	O
dram	O
involves	O
a	O
number	O
of	O
steps	O
that	O
have	O
to	O
be	O
done	O
according	O
to	O
strict	O
time	O
constraints	O
.	O
dram	O
systems	O
consist	O
of	O
multiple	O
dram	O
chips	O
,	O
each	O
containing	O
multiple	O
rectangular	O
arrays	O
of	O
storage	O
cells	O
arranged	O
in	O
rows	O
and	O
columns	O
.	O
each	O
cell	O
stores	O
a	O
bit	O
as	O
the	O
charge	O
on	O
a	O
capacitor	O
.	O
because	O
the	O
charge	O
decreases	O
over	O
time	O
,	O
each	O
dram	O
cell	O
needs	O
to	O
be	O
recharged—refreshed—every	O
few	O
milliseconds	O
to	O
prevent	O
memory	O
content	O
from	O
being	O
lost	O
.	O
this	O
need	O
to	O
refresh	O
the	O
cells	O
is	O
why	O
dram	O
is	O
called	O
“	O
dynamic.	O
”	O
each	O
cell	O
array	O
has	O
a	O
row	O
buﬀer	O
that	O
holds	O
a	O
row	O
of	O
bits	O
that	O
can	O
be	O
transferred	O
into	O
or	O
out	O
of	O
one	O
of	O
the	O
array	O
’	O
s	O
rows	O
.	O
an	O
activate	O
command	O
“	O
opens	O
a	O
row	O
,	O
”	O
which	O
means	O
moving	O
the	O
contents	O
of	O
the	O
row	O
whose	O
address	O
is	O
indicated	O
by	O
the	O
command	O
into	O
the	O
row	O
buﬀer	O
.	O
with	O
a	O
row	O
open	O
,	O
the	O
controller	O
can	O
issue	O
read	O
and	O
write	O
commands	O
to	O
the	O
cell	O
array	O
.	O
each	O
read	O
command	O
transfers	O
a	O
word	O
(	O
a	O
short	O
sequence	O
of	O
consecutive	O
bits	O
)	O
in	O
the	O
row	O
buﬀer	O
to	O
the	O
external	O
data	O
bus	O
,	O
and	O
each	O
write	O
command	O
transfers	O
a	O
word	O
in	O
the	O
external	O
data	O
bus	O
to	O
the	O
row	O
buﬀer	O
.	O
before	O
a	O
diﬀerent	O
row	O
can	O
be	O
opened	O
,	O
a	O
precharge	O
command	O
must	O
be	O
issued	O
which	O
transfers	O
the	O
(	O
possibly	O
updated	O
)	O
data	O
in	O
the	O
row	O
buﬀer	O
back	O
into	O
the	O
addressed	O
row	O
of	O
the	O
cell	O
array	O
.	O
after	O
this	O
,	O
another	O
activate	O
command	O
can	O
open	O
a	O
new	O
row	O
to	O
be	O
accessed	O
.	O
read	O
and	O
write	O
commands	O
are	O
column	O
16.4.	O
optimizing	B
memory	I
control	I
437	O
commands	O
because	O
they	O
sequentially	O
transfer	O
bits	O
into	O
or	O
out	O
of	O
columns	O
of	O
the	O
row	O
buﬀer	O
;	O
multiple	O
bits	O
can	O
be	O
transferred	O
without	O
re-opening	O
the	O
row	O
.	O
read	O
and	O
write	O
commands	O
to	O
the	O
currently-open	O
row	O
can	O
be	O
carried	O
out	O
more	O
quickly	O
than	O
accessing	O
a	O
diﬀerent	O
row	O
,	O
which	O
would	O
involve	O
additional	O
row	O
commands	O
:	O
precharge	O
and	O
activate	O
;	O
this	O
is	O
sometimes	O
referred	O
to	O
as	O
“	O
row	O
locality.	O
”	O
a	O
memory	O
controller	O
maintains	O
a	O
memory	O
transaction	O
queue	O
that	O
stores	O
memory-access	O
requests	O
from	O
the	O
processors	O
sharing	O
the	O
memory	O
system	O
.	O
the	O
controller	O
has	O
to	O
process	O
requests	O
by	O
issuing	O
commands	O
to	O
the	O
memory	O
system	O
while	O
adhering	O
to	O
a	O
large	O
number	O
of	O
timing	O
constraints	O
.	O
a	O
controller	O
’	O
s	O
policy	B
for	O
scheduling	O
access	O
requests	O
can	O
have	O
a	O
large	O
eﬀect	O
on	O
the	O
performance	O
of	O
the	O
memory	O
system	O
,	O
such	O
as	O
the	O
average	O
latency	O
with	O
which	O
requests	O
can	O
be	O
satisﬁed	O
and	O
the	O
throughput	O
the	O
system	O
is	O
capable	O
of	O
achieving	O
.	O
the	O
simplest	O
scheduling	O
strategy	O
handles	O
access	O
requests	O
in	O
the	O
order	O
in	O
which	O
they	O
arrive	O
by	O
issuing	O
all	O
the	O
commands	O
required	O
by	O
the	O
request	O
before	O
beginning	O
to	O
service	O
the	O
next	O
one	O
.	O
but	O
if	O
the	O
system	O
is	O
not	O
ready	O
for	O
one	O
of	O
these	O
commands	O
,	O
or	O
executing	O
a	O
command	O
would	O
result	O
in	O
resources	O
being	O
underutilized	O
(	O
e.g.	O
,	O
due	O
to	O
timing	O
constraints	O
arising	O
from	O
servicing	O
that	O
one	O
command	O
)	O
,	O
it	O
makes	O
sense	O
to	O
begin	O
servicing	O
a	O
newer	O
request	O
before	O
ﬁnishing	O
the	O
older	O
one	O
.	O
policies	O
can	O
gain	O
eﬃciency	O
by	O
reordering	O
requests	O
,	O
for	O
example	O
,	O
by	O
giving	O
priority	O
to	O
read	O
requests	O
over	O
write	O
requests	O
,	O
or	O
by	O
giving	O
priority	O
to	O
read/write	O
commands	O
to	O
already	O
open	O
rows	O
.	O
the	O
policy	B
called	O
first-ready	O
,	O
first-	O
come-first-serve	O
(	O
fr-fcfs	O
)	O
,	O
gives	O
priority	O
to	O
column	O
commands	O
(	O
read	O
and	O
write	O
)	O
over	O
row	O
commands	O
(	O
activate	O
and	O
precharge	O
)	O
,	O
and	O
in	O
case	O
of	O
a	O
tie	O
gives	O
priority	O
to	O
the	O
oldest	O
command	O
.	O
fr-fcfs	O
was	O
shown	O
to	O
outperform	O
other	O
scheduling	O
policies	O
in	O
terms	O
of	O
average	O
memory-access	O
latency	O
under	O
conditions	O
commonly	O
encountered	O
(	O
rixner	O
,	O
2004	O
)	O
.	O
figure	O
16.3	O
is	O
a	O
high-level	O
view	O
of	O
˙ipek	O
et	O
al.	O
’	O
s	O
reinforcement	B
learning	I
memory	O
con-	O
troller	O
.	O
they	O
modeled	O
the	O
dram	O
access	O
process	O
as	O
an	O
mdp	O
whose	O
states	O
are	O
the	O
con-	O
tents	O
of	O
the	O
transaction	O
queue	O
and	O
whose	O
actions	O
are	O
commands	O
to	O
the	O
dram	O
system	O
:	O
precharge	O
,	O
activate	O
,	O
read	O
,	O
write	O
,	O
and	O
noop	O
.	O
the	O
reward	B
signal	I
is	O
1	O
whenever	O
the	O
action	B
figure	O
16.3	O
:	O
high-level	O
view	O
of	O
the	O
reinforcement	B
learning	I
dram	O
controller	O
.	O
the	O
scheduler	O
is	O
the	O
reinforcement	B
learning	I
agent	O
.	O
its	O
environment	B
is	O
represented	O
by	O
features	O
of	O
the	O
transaction	O
c	O
(	O
cid:13	O
)	O
2009	O
ieee	O
.	O
reprinted	O
,	O
with	O
queue	O
,	O
and	O
its	O
actions	O
are	O
commands	O
to	O
the	O
dram	O
system	O
.	O
permission	O
,	O
from	O
j.	O
f.	O
mart´ınez	O
and	O
e.	O
˙ipek	O
,	O
dynamic	O
multicore	O
resource	O
management	O
:	O
a	O
machine	O
learning	O
approach	O
,	O
micro	O
,	O
ieee	O
,	O
29	O
(	O
5	O
)	O
,	O
p.	O
12	O
.	O
438	O
chapter	O
16	O
:	O
applications	B
and	I
case	I
studies	I
is	O
read	O
or	O
write	O
,	O
and	O
otherwise	O
it	O
is	O
0.	O
state	B
transitions	O
were	O
considered	O
to	O
be	O
stochastic	O
because	O
the	O
next	O
state	B
of	O
the	O
system	O
not	O
only	O
depends	O
on	O
the	O
scheduler	O
’	O
s	O
command	O
,	O
but	O
also	O
on	O
aspects	O
of	O
the	O
system	O
’	O
s	O
behavior	O
that	O
the	O
scheduler	O
can	O
not	O
control	B
,	O
such	O
as	O
the	O
workloads	O
of	O
the	O
processor	O
cores	O
accessing	O
the	O
dram	O
system	O
.	O
critical	O
to	O
this	O
mdp	O
are	O
constraints	O
on	O
the	O
actions	O
available	O
in	O
each	O
state	B
.	O
recall	O
from	O
chapter	O
3	O
that	O
the	O
set	O
of	O
available	O
actions	O
can	O
depend	O
on	O
the	O
state	B
:	O
at	O
∈	O
a	O
(	O
st	O
)	O
,	O
where	O
at	O
is	O
the	O
action	B
at	O
time	O
step	O
t	O
and	O
a	O
(	O
st	O
)	O
is	O
the	O
set	O
of	O
actions	O
available	O
in	O
state	O
st.	O
in	O
this	O
application	O
,	O
the	O
integrity	O
of	O
the	O
dram	O
system	O
was	O
assured	O
by	O
not	O
allowing	O
actions	O
that	O
would	O
violate	O
timing	O
or	O
resource	O
constraints	O
.	O
although	O
˙ipek	O
et	O
al	O
.	O
did	O
not	O
make	O
it	O
explicit	O
,	O
they	O
eﬀectively	O
accomplished	O
this	O
by	O
pre-deﬁning	O
the	O
sets	O
a	O
(	O
st	O
)	O
for	O
all	O
possible	O
states	O
st.	O
these	O
constraints	O
explain	O
why	O
the	O
mdp	O
has	O
a	O
noop	O
action	B
and	O
why	O
the	O
reward	B
signal	I
is	O
0	O
except	O
when	O
a	O
read	O
or	O
write	O
command	O
is	O
issued	O
.	O
noop	O
is	O
issued	O
when	O
it	O
is	O
the	O
sole	O
legal	O
action	B
in	O
a	O
state	B
.	O
to	O
maximize	O
utilization	O
of	O
the	O
memory	O
system	O
,	O
the	O
controller	O
’	O
s	O
task	O
is	O
to	O
drive	O
the	O
system	O
to	O
states	O
in	O
which	O
either	O
a	O
read	O
or	O
a	O
write	O
action	B
can	O
be	O
selected	O
:	O
only	O
these	O
actions	O
result	O
in	O
sending	O
data	O
over	O
the	O
external	O
data	O
bus	O
,	O
so	O
it	O
is	O
only	O
these	O
that	O
contribute	O
to	O
the	O
throughput	O
of	O
the	O
system	O
.	O
although	O
precharge	O
and	O
activate	O
produce	O
no	O
immediate	O
reward	O
,	O
the	O
agent	O
needs	O
to	O
select	O
these	O
actions	O
to	O
make	O
it	O
possible	O
to	O
later	O
select	O
the	O
rewarded	O
read	O
and	O
write	O
actions	O
.	O
the	O
scheduling	O
agent	O
used	O
sarsa	O
(	O
section	O
6.4	O
)	O
to	O
learn	O
an	O
action-value	B
function	I
.	O
states	O
were	O
represented	O
by	O
six	O
integer-valued	O
features	O
.	O
to	O
approximate	B
the	O
action-value	O
func-	O
tion	B
,	O
the	O
algorithm	O
used	O
linear	B
function	I
approximation	I
implemented	O
by	O
tile	B
coding	I
with	O
hashing	O
(	O
section	O
9.5.4	O
)	O
.	O
the	O
tile	B
coding	I
had	O
32	O
tilings	O
,	O
each	O
storing	O
256	O
action	B
values	O
as	O
16-bit	O
ﬁxed	O
point	O
numbers	O
.	O
exploration	O
was	O
ε-greedy	O
with	O
ε	O
=	O
0.05.	O
state	B
features	O
included	O
the	O
number	O
of	O
read	O
requests	O
in	O
the	O
transaction	O
queue	O
,	O
the	O
number	O
of	O
write	O
requests	O
in	O
the	O
transaction	O
queue	O
,	O
the	O
number	O
of	O
write	O
requests	O
in	O
the	O
transaction	O
queue	O
waiting	O
for	O
their	O
row	O
to	O
be	O
opened	O
,	O
and	O
the	O
number	O
of	O
read	O
requests	O
in	O
the	O
transaction	O
queue	O
waiting	O
for	O
their	O
row	O
to	O
be	O
opened	O
that	O
are	O
the	O
oldest	O
issued	O
by	O
their	O
requesting	O
processors	O
.	O
(	O
the	O
other	O
features	O
depended	O
on	O
how	O
the	O
dram	O
interacts	O
with	O
cache	O
memory	O
,	O
details	O
we	O
omit	O
here	O
.	O
)	O
the	O
selection	O
of	O
the	O
state	B
features	O
was	O
based	O
on	O
˙ipek	O
et	O
al.	O
’	O
s	O
understanding	O
of	O
factors	O
that	O
impact	O
dram	O
performance	O
.	O
for	O
example	O
,	O
balancing	O
the	O
rate	O
of	O
servicing	O
reads	O
and	O
writes	O
based	O
on	O
how	O
many	O
of	O
each	O
are	O
in	O
the	O
transaction	O
queue	O
can	O
help	O
avoid	O
stalling	O
the	O
dram	O
system	O
’	O
s	O
interaction	O
with	O
cache	O
memory	O
.	O
the	O
authors	O
in	O
fact	O
generated	O
a	O
relatively	O
long	O
list	O
of	O
potential	O
features	O
,	O
and	O
then	O
pared	O
them	O
down	O
to	O
a	O
handful	O
using	O
simulations	O
guided	O
by	O
stepwise	O
feature	O
selection	O
.	O
an	O
interesting	O
aspect	O
of	O
this	O
formulation	O
of	O
the	O
scheduling	O
problem	O
as	O
an	O
mdp	O
is	O
that	O
the	O
features	O
input	O
to	O
the	O
tile	B
coding	I
for	O
deﬁning	O
the	O
action-value	B
function	I
were	O
diﬀerent	O
from	O
the	O
features	O
used	O
to	O
specify	O
the	O
action-constraint	O
sets	O
a	O
(	O
st	O
)	O
.	O
whereas	O
the	O
tile	B
coding	I
input	O
was	O
derived	O
from	O
the	O
contents	O
of	O
the	O
transaction	O
queue	O
,	O
the	O
constraint	O
sets	O
depended	O
on	O
a	O
host	O
of	O
other	O
features	O
related	O
to	O
timing	O
and	O
resource	O
constraints	O
that	O
had	O
to	O
be	O
satisﬁed	O
by	O
the	O
hardware	O
implementation	O
of	O
the	O
entire	O
system	O
.	O
in	O
this	O
way	O
,	O
the	O
action	B
constraints	O
ensured	O
that	O
the	O
learning	O
algorithm	O
’	O
s	O
exploration	O
could	O
not	O
endanger	O
the	O
integrity	O
of	O
the	O
physical	O
system	O
,	O
while	O
learning	O
was	O
eﬀectively	O
limited	O
to	O
a	O
“	O
safe	O
”	O
region	O
of	O
the	O
much	O
larger	O
state	B
space	O
of	O
the	O
hardware	O
implementation	O
.	O
16.4.	O
optimizing	B
memory	I
control	I
439	O
because	O
an	O
objective	O
of	O
this	O
work	O
was	O
that	O
the	O
learning	O
controller	O
could	O
be	O
imple-	O
mented	O
on	O
a	O
chip	O
so	O
that	O
learning	O
could	O
occur	O
online	B
while	O
a	O
computer	O
is	O
running	O
,	O
hardware	O
implementation	O
details	O
were	O
important	O
considerations	O
.	O
the	O
design	O
included	O
two	O
ﬁve-stage	O
pipelines	O
to	O
calculate	O
and	O
compare	O
two	O
action	B
values	O
at	O
every	O
processor	O
clock	O
cycle	O
,	O
and	O
to	O
update	O
the	O
appropriate	O
action	B
value	O
.	O
this	O
included	O
accessing	O
the	O
tile	B
coding	I
which	O
was	O
stored	O
on-chip	O
in	O
static	O
ram	O
.	O
for	O
the	O
conﬁguration	O
˙ipek	O
et	O
al	O
.	O
simu-	O
lated	O
,	O
which	O
was	O
a	O
4ghz	O
4-core	O
chip	O
typical	O
of	O
high-end	O
workstations	O
at	O
the	O
time	O
of	O
their	O
research	O
,	O
there	O
were	O
10	O
processor	O
cycles	O
for	O
every	O
dram	O
cycle	O
.	O
considering	O
the	O
cycles	O
needed	O
to	O
ﬁll	O
the	O
pipes	O
,	O
up	O
to	O
12	O
actions	O
could	O
be	O
evaluated	O
in	O
each	O
dram	O
cycle	O
.	O
˙ipek	O
et	O
al	O
.	O
found	O
that	O
the	O
number	O
of	O
legal	O
commands	O
for	O
any	O
state	B
was	O
rarely	O
greater	O
than	O
this	O
,	O
and	O
that	O
performance	O
loss	O
was	O
negligible	O
if	O
enough	O
time	O
was	O
not	O
always	O
available	O
to	O
consider	O
all	O
legal	O
commands	O
.	O
these	O
and	O
other	O
clever	O
design	O
details	O
made	O
it	O
feasible	O
to	O
implement	O
the	O
complete	O
controller	O
and	O
learning	O
algorithm	O
on	O
a	O
multi-processor	O
chip	O
.	O
˙ipek	O
et	O
al	O
.	O
evaluated	O
their	O
learning	O
controller	O
in	O
simulation	O
by	O
comparing	O
it	O
with	O
three	O
other	O
controllers	O
:	O
1	O
)	O
the	O
fr-fcfs	O
controller	O
mentioned	O
above	O
that	O
produces	O
the	O
best	O
on-average	O
performance	O
,	O
2	O
)	O
a	O
conventional	O
controller	O
that	O
processes	O
each	O
request	O
in	O
order	O
,	O
and	O
3	O
)	O
an	O
unrealizable	O
ideal	O
controller	O
,	O
called	O
the	O
optimistic	O
controller	O
,	O
able	O
to	O
sustain	O
100	O
%	O
dram	O
throughput	O
if	O
given	O
enough	O
demand	O
by	O
ignoring	O
all	O
timing	O
and	O
resource	O
constraints	O
,	O
but	O
otherwise	O
modeling	O
dram	O
latency	O
(	O
as	O
row	O
buﬀer	O
hits	O
)	O
and	O
bandwidth	O
.	O
they	O
simulated	O
nine	O
memory-intensive	O
parallel	O
workloads	O
consisting	O
of	O
scientiﬁc	O
and	O
data-mining	O
applications	O
.	O
figure	O
16.4	O
shows	O
the	O
performance	O
(	O
the	O
inverse	O
of	O
execution	O
time	O
normalized	O
to	O
the	O
performance	O
of	O
fr-fcfs	O
)	O
of	O
each	O
controller	O
for	O
the	O
nine	O
applications	O
,	O
together	O
with	O
the	O
geometric	O
mean	O
of	O
their	O
performances	O
over	O
the	O
applications	O
.	O
the	O
learning	O
controller	O
,	O
labeled	O
rl	O
in	O
the	O
ﬁgure	O
,	O
improved	O
over	O
that	O
of	O
fr-fcfs	O
by	O
from	O
7	O
%	O
to	O
33	O
%	O
over	O
the	O
nine	O
applications	O
,	O
with	O
an	O
average	O
improvement	O
of	O
19	O
%	O
.	O
of	O
course	O
,	O
no	O
realizable	O
controller	O
can	O
match	O
the	O
performance	O
of	O
optimistic	O
,	O
which	O
ignores	O
all	O
timing	O
and	O
resource	O
constraints	O
,	O
but	O
the	O
learning	O
controller	O
’	O
s	O
performance	O
figure	O
16.4	O
:	O
performances	O
of	O
four	O
controllers	O
over	O
a	O
suite	O
of	O
9	O
simulated	O
benchmark	O
applica-	O
tions	O
.	O
the	O
controllers	O
are	O
:	O
the	O
simplest	O
‘	O
in-order	O
’	O
controller	O
,	O
fr-fcfs	O
,	O
the	O
learning	O
controller	O
rl	O
,	O
and	O
the	O
unrealizable	O
optimistic	O
controller	O
which	O
ignores	O
all	O
timing	O
and	O
resource	O
constraints	O
to	O
provide	O
a	O
performance	O
upper	O
bound	O
.	O
performance	O
,	O
normalized	O
to	O
that	O
of	O
fr-fcfs	O
,	O
is	O
the	O
inverse	O
of	O
execution	O
time	O
.	O
at	O
far	O
right	O
is	O
the	O
geometric	O
mean	O
of	O
performances	O
over	O
the	O
9	O
bench-	O
mark	O
applications	O
for	O
each	O
controller	O
.	O
controller	O
rl	O
comes	O
closest	O
to	O
the	O
ideal	O
performance	O
.	O
c	O
(	O
cid:13	O
)	O
2009	O
ieee	O
.	O
reprinted	O
,	O
with	O
permission	O
,	O
from	O
j.	O
f.	O
mart´ınez	O
and	O
e.	O
˙ipek	O
,	O
dynamic	O
multicore	O
resource	O
management	O
:	O
a	O
machine	O
learning	O
approach	O
,	O
micro	O
,	O
ieee	O
,	O
29	O
(	O
5	O
)	O
,	O
p.	O
13	O
.	O
440	O
chapter	O
16	O
:	O
applications	B
and	I
case	I
studies	I
closed	O
the	O
gap	O
with	O
optimistic	O
’	O
s	O
upper	O
bound	O
by	O
an	O
impressive	O
27	O
%	O
.	O
because	O
the	O
rationale	O
for	O
on-chip	O
implementation	O
of	O
the	O
learning	O
algorithm	O
was	O
to	O
al-	O
low	O
the	O
scheduling	O
policy	B
to	O
adapt	O
online	B
to	O
changing	O
workloads	O
,	O
˙ipek	O
et	O
al	O
.	O
analyzed	O
the	O
impact	O
of	O
online	O
learning	O
compared	O
to	O
a	O
previously-learned	O
ﬁxed	O
policy	B
.	O
they	O
trained	O
their	O
controller	O
with	O
data	O
from	O
all	O
nine	O
benchmark	O
applications	O
and	O
then	O
held	O
the	O
re-	O
sulting	O
action	B
values	O
ﬁxed	O
throughout	O
the	O
simulated	O
execution	O
of	O
the	O
applications	O
.	O
they	O
found	O
that	O
the	O
average	O
performance	O
of	O
the	O
controller	O
that	O
learned	O
online	B
was	O
8	O
%	O
better	O
than	O
that	O
of	O
the	O
controller	O
using	O
the	O
ﬁxed	O
policy	B
,	O
leading	O
them	O
to	O
conclude	O
that	O
online	B
learning	O
is	O
an	O
important	O
feature	O
of	O
their	O
approach	O
.	O
this	O
learning	O
memory	O
controller	O
was	O
never	O
committed	O
to	O
physical	O
hardware	O
because	O
of	O
the	O
large	O
cost	O
of	O
fabrication	O
.	O
nevertheless	O
,	O
˙ipek	O
et	O
al	O
.	O
could	O
convincingly	O
argue	O
on	O
the	O
basis	O
of	O
their	O
simulation	O
results	O
that	O
a	O
memory	O
controller	O
that	O
learns	O
online	B
via	O
reinforce-	O
ment	O
learning	O
has	O
the	O
potential	O
to	O
improve	O
performance	O
to	O
levels	O
that	O
would	O
otherwise	O
require	O
more	O
complex	O
and	O
more	O
expensive	O
memory	O
systems	O
,	O
while	O
removing	O
from	O
human	O
designers	O
some	O
of	O
the	O
burden	O
required	O
to	O
manually	O
design	O
eﬃcient	O
scheduling	O
policies	O
.	O
mukundan	O
and	O
mart´ınez	O
(	O
2012	O
)	O
took	O
this	O
project	O
forward	O
by	O
investigating	O
learning	O
con-	O
trollers	O
with	O
additional	O
actions	O
,	O
other	O
performance	O
criteria	O
,	O
and	O
more	O
complex	O
reward	O
functions	O
derived	O
using	O
genetic	B
algorithms	I
.	O
they	O
considered	O
additional	O
performance	O
cri-	O
teria	O
related	O
to	O
energy	O
eﬃciency	O
.	O
the	O
results	O
of	O
these	O
studies	O
surpassed	O
the	O
earlier	O
results	O
described	O
above	O
and	O
signiﬁcantly	O
surpassed	O
the	O
2012	O
state-of-the-art	O
for	O
all	O
of	O
the	O
per-	O
formance	O
criteria	O
they	O
considered	O
.	O
the	O
approach	O
is	O
especially	O
promising	O
for	O
developing	O
sophisticated	O
power-aware	O
dram	O
interfaces	O
.	O
16.5	O
human-level	O
video	O
game	O
play	O
one	O
of	O
the	O
greatest	O
challenges	O
in	O
applying	O
reinforcement	B
learning	I
to	O
real-world	O
problems	O
is	O
deciding	O
how	O
to	O
represent	O
and	O
store	O
value	B
functions	O
and/or	O
policies	O
.	O
unless	O
the	O
state	B
set	O
is	O
ﬁnite	O
and	O
small	O
enough	O
to	O
allow	O
exhaustive	O
representation	O
by	O
a	O
lookup	O
table—as	O
in	O
many	O
of	O
our	O
illustrative	O
examples—one	O
must	O
use	O
a	O
parameterized	O
function	O
approxi-	O
mation	O
scheme	O
.	O
whether	O
linear	O
or	O
nonlinear	O
,	O
function	B
approximation	I
relies	O
on	O
features	O
that	O
have	O
to	O
be	O
readily	O
accessible	O
to	O
the	O
learning	O
system	O
and	O
able	O
to	O
convey	O
the	O
infor-	O
mation	O
necessary	O
for	O
skilled	O
performance	O
.	O
most	O
successful	O
applications	O
of	O
reinforcement	B
learning	I
owe	O
much	O
to	O
sets	O
of	O
features	O
carefully	O
handcrafted	O
based	O
on	O
human	O
knowledge	O
and	O
intuition	O
about	O
the	O
speciﬁc	O
problem	O
to	O
be	O
tackled	O
.	O
a	O
team	O
of	O
researchers	O
at	O
google	O
deepmind	O
developed	O
an	O
impressive	O
demonstration	O
that	O
a	O
deep	O
multi-layer	O
artiﬁcial	O
neural	O
network	O
(	O
ann	O
)	O
can	O
automate	O
the	O
feature	O
de-	O
sign	O
process	O
(	O
mnih	O
et	O
al.	O
,	O
2013	O
,	O
2015	O
)	O
.	O
multi-layer	O
anns	O
have	O
been	O
used	O
for	O
function	O
approximation	O
in	O
reinforcement	O
learning	O
ever	O
since	O
the	O
1986	O
popularization	O
of	O
the	O
back-	O
propagation	O
algorithm	O
as	O
a	O
method	O
for	O
learning	O
internal	O
representations	O
(	O
rumelhart	O
,	O
hinton	O
,	O
and	O
williams	O
,	O
1986	O
;	O
see	O
section	O
9.7	O
)	O
.	O
striking	O
results	O
have	O
been	O
obtained	O
by	O
coupling	O
reinforcement	B
learning	I
with	O
backpropagation	B
.	O
the	O
results	O
obtained	O
by	O
tesauro	O
and	O
colleages	O
with	O
td-gammon	O
and	O
watson	O
discussed	O
above	O
are	O
notable	O
examples	O
.	O
these	O
and	O
other	O
applications	O
beneﬁted	O
from	O
the	O
ability	O
of	O
multi-layer	O
anns	O
to	O
learn	O
task-relevant	O
features	O
.	O
however	O
,	O
in	O
all	O
the	O
examples	O
of	O
which	O
we	O
are	O
aware	O
,	O
the	O
most	O
16.5.	O
human-level	O
video	O
game	O
play	O
441	O
impressive	O
demonstrations	O
required	O
the	O
network	O
’	O
s	O
input	O
to	O
be	O
represented	O
in	O
terms	O
of	O
specialized	O
features	O
handcrafted	O
for	O
the	O
given	O
problem	O
.	O
this	O
is	O
vividly	O
apparent	O
in	O
the	O
td-gammon	O
results	O
.	O
td-gammon	O
0.0	O
,	O
whose	O
network	O
input	O
was	O
essentially	O
a	O
“	O
raw	O
”	O
representation	O
of	O
he	O
backgammon	B
board	O
,	O
meaning	O
that	O
it	O
involved	O
very	O
little	O
knowledge	O
of	O
backgammon	O
,	O
learned	O
to	O
play	O
approximately	O
as	O
well	O
as	O
the	O
best	O
previous	O
backgammon	B
computer	O
programs	O
.	O
adding	O
specialized	O
backgammon	B
features	O
produced	O
td-gammon	O
1.0	O
which	O
was	O
substantially	O
better	O
than	O
all	O
previous	O
backgammon	B
programs	O
and	O
com-	O
peted	O
well	O
against	O
human	O
experts	O
.	O
mnih	O
et	O
al	O
.	O
developed	O
a	O
reinforcement	B
learning	I
agent	O
called	O
deep	O
q-network	O
(	O
dqn	O
)	O
that	O
combined	O
q-learning	O
with	O
a	O
deep	O
convolutional	O
ann	O
,	O
a	O
many-layered	O
,	O
or	O
deep	O
,	O
ann	O
specialized	O
for	O
processing	O
spatial	O
arrays	O
of	O
data	O
such	O
as	O
images	O
.	O
we	O
describe	O
deep	O
convolutional	O
anns	O
in	O
section	O
9.7.	O
by	O
the	O
time	O
of	O
mnih	O
et	O
al.	O
’	O
s	O
work	O
with	O
dqn	O
,	O
deep	O
anns	O
,	O
including	O
deep	O
convolutional	O
anns	O
,	O
had	O
produced	O
impressive	O
results	O
in	O
many	O
applications	O
,	O
but	O
they	O
had	O
not	O
been	O
widely	O
used	O
in	O
reinforcement	O
learning	O
.	O
mnih	O
et	O
al	O
.	O
used	O
dqn	O
to	O
show	O
how	O
a	O
single	O
reinforcement	B
learning	I
agent	O
can	O
achieve	O
high	O
levels	O
of	O
performance	O
in	O
many	O
diﬀerent	O
problems	O
without	O
relying	O
on	O
diﬀerent	O
problem-	O
speciﬁc	O
feature	O
sets	O
.	O
to	O
demonstrate	O
this	O
,	O
they	O
let	O
dqn	O
learn	O
to	O
play	O
49	O
diﬀerent	O
atari	O
2600	O
video	O
games	O
by	O
interacting	O
with	O
a	O
game	O
emulator	O
.	O
for	O
learning	O
each	O
game	O
,	O
dqn	O
used	O
the	O
same	O
raw	O
input	O
,	O
the	O
same	O
network	O
architecture	O
,	O
and	O
the	O
same	O
parameter	O
val-	O
ues	O
(	O
e.g.	O
,	O
step	O
size	O
,	O
discount	O
rate	O
,	O
exploration	O
parameters	O
,	O
and	O
many	O
more	O
speciﬁc	O
to	O
the	O
implementation	O
)	O
.	O
dqn	O
achieved	O
levels	O
of	O
play	O
at	O
or	O
beyond	O
human	O
level	O
on	O
a	O
large	O
fraction	O
of	O
these	O
games	O
.	O
although	O
the	O
games	O
were	O
alike	O
in	O
being	O
played	O
by	O
watching	O
streams	O
of	O
video	O
images	O
,	O
they	O
varied	O
widely	O
in	O
other	O
respects	O
.	O
their	O
actions	O
had	O
dif-	O
ferent	O
eﬀects	O
,	O
they	O
had	O
diﬀerent	O
state-transition	O
dynamics	O
,	O
and	O
they	O
needed	O
diﬀerent	O
policies	O
for	O
earning	O
high	O
scores	O
.	O
the	O
deep	O
convolutional	O
ann	O
learned	O
to	O
transform	O
the	O
raw	O
input	O
common	O
to	O
all	O
the	O
games	O
into	O
features	O
specialized	O
for	O
representing	O
the	O
action	B
values	O
required	O
for	O
playing	O
at	O
the	O
high	O
level	O
dqn	O
achieved	O
for	O
most	O
of	O
the	O
games	O
.	O
the	O
atari	O
2600	O
is	O
a	O
home	O
video	O
game	O
console	O
that	O
was	O
sold	O
in	O
various	O
versions	O
by	O
atari	O
inc.	O
from	O
1977	O
to	O
1992.	O
it	O
introduced	O
or	O
popularized	O
many	O
arcade	O
video	O
games	O
that	O
are	O
now	O
considered	O
classics	O
,	O
such	O
as	O
pong	O
,	O
breakout	O
,	O
space	O
invaders	O
,	O
and	O
asteroids	O
.	O
al-	O
though	O
much	O
simpler	O
than	O
modern	O
video	O
games	O
,	O
atari	O
2600	O
games	O
are	O
still	O
entertaining	O
and	O
challenging	O
for	O
human	O
players	O
,	O
and	O
they	O
have	O
been	O
attractive	O
as	O
testbeds	O
for	O
de-	O
veloping	O
and	O
evaluating	O
reinforcement	B
learning	I
methods	O
(	O
diuk	O
,	O
cohen	O
,	O
littman	O
,	O
2008	O
;	O
naddaf	O
,	O
2010	O
;	O
cobo	O
,	O
zang	O
,	O
isbell	O
,	O
and	O
thomaz	O
,	O
2011	O
;	O
bellemare	O
,	O
veness	O
,	O
and	O
bowling	O
,	O
2013	O
)	O
.	O
bellemare	O
,	O
naddaf	O
,	O
veness	O
,	O
and	O
bowling	O
(	O
2012	O
)	O
developed	O
the	O
publicly	O
available	O
arcade	O
learning	O
environment	O
(	O
ale	O
)	O
to	O
encourage	O
and	O
simplify	O
using	O
atari	O
2600	O
games	O
to	O
study	O
learning	O
and	O
planning	B
algorithms	O
.	O
these	O
previous	O
studies	O
and	O
the	O
availability	O
of	O
ale	O
made	O
the	O
atari	O
2600	O
game	O
collection	O
a	O
good	O
choice	O
for	O
mnih	O
et	O
al.	O
’	O
s	O
demonstration	O
,	O
which	O
was	O
also	O
inﬂuenced	O
by	O
the	O
impressive	O
human-level	O
performance	O
that	O
td-gammon	O
was	O
able	O
to	O
achieve	O
in	O
backgammon	O
.	O
dqn	O
is	O
similar	O
to	O
td-gammon	O
in	O
using	O
a	O
multi-layer	O
ann	O
as	O
the	O
function	B
approximation	I
method	O
for	O
a	O
semi-gradient	O
form	O
of	O
a	O
td	O
algorithm	O
,	O
with	O
the	O
gradients	O
computed	O
by	O
the	O
backpropagation	B
algorithm	O
.	O
however	O
,	O
instead	O
of	O
using	O
td	O
(	O
λ	O
)	O
as	O
td-gammon	O
did	O
,	O
dqn	O
used	O
the	O
semi-gradient	O
form	O
of	O
q-learning	O
.	O
td-gammon	O
estimated	O
the	O
values	O
of	O
442	O
chapter	O
16	O
:	O
applications	B
and	I
case	I
studies	I
afterstates	O
,	O
which	O
were	O
easily	O
obtained	O
from	O
the	O
rules	O
for	O
making	O
backgammon	B
moves	O
.	O
to	O
use	O
the	O
same	O
algorithm	O
for	O
the	O
atari	O
games	O
would	O
have	O
required	O
generating	O
the	O
next	O
states	O
for	O
each	O
possible	O
action	B
(	O
which	O
would	O
not	O
have	O
been	O
afterstates	B
in	O
that	O
case	O
)	O
.	O
this	O
could	O
have	O
been	O
done	O
by	O
using	O
the	O
game	O
emulator	O
to	O
run	O
single-step	O
simulations	O
for	O
all	O
the	O
possible	O
actions	O
(	O
which	O
ale	O
makes	O
possible	O
)	O
.	O
or	O
a	O
model	O
of	O
each	O
game	O
’	O
s	O
state-transition	O
function	O
could	O
have	O
been	O
learned	O
and	O
used	O
to	O
predict	O
next	O
states	O
(	O
oh	O
,	O
guo	O
,	O
lee	O
,	O
lewis	O
,	O
and	O
singh	O
,	O
2015	O
)	O
.	O
while	O
these	O
methods	O
might	O
have	O
produced	O
results	O
comparable	O
to	O
dqn	O
’	O
s	O
,	O
they	O
would	O
have	O
been	O
more	O
complicated	O
to	O
implement	O
and	O
would	O
have	O
signiﬁcantly	O
increased	O
the	O
time	O
needed	O
for	O
learning	O
.	O
another	O
motivation	B
for	O
using	O
q-learning	O
was	O
that	O
dqn	O
used	O
the	O
experience	B
replay	I
method	O
,	O
described	O
below	O
,	O
which	O
requires	O
an	O
oﬀ-policy	B
algorithm	O
.	O
being	O
model-free	O
and	O
oﬀ-policy	O
made	O
q-learning	O
a	O
natural	O
choice	O
.	O
before	O
describing	O
the	O
details	O
of	O
dqn	O
and	O
how	O
the	O
experiments	O
were	O
conducted	O
,	O
we	O
look	O
at	O
the	O
skill	O
levels	O
dqn	O
was	O
able	O
to	O
achieve	O
.	O
mnih	O
et	O
al	O
.	O
compared	O
the	O
scores	O
of	O
dqn	O
with	O
the	O
scores	O
of	O
the	O
best	O
performing	O
learning	O
system	O
in	O
the	O
literature	O
at	O
the	O
time	O
,	O
the	O
scores	O
of	O
a	O
professional	O
human	O
games	O
tester	O
,	O
and	O
the	O
scores	O
of	O
an	O
agent	O
that	O
selected	O
actions	O
at	O
random	O
.	O
the	O
best	O
system	O
from	O
the	O
literature	O
used	O
linear	B
function	I
approximation	I
with	O
features	O
hand	O
designed	O
using	O
some	O
knowledge	O
about	O
atari	O
2600	O
games	O
(	O
bellemare	O
,	O
naddaf	O
,	O
veness	O
,	O
and	O
bowling	O
,	O
2013	O
)	O
.	O
dqn	O
learned	O
on	O
each	O
game	O
by	O
interacting	O
with	O
the	O
game	O
emulator	O
for	O
50	O
million	O
frames	O
,	O
which	O
corresponds	O
to	O
about	O
38	O
days	O
of	O
experience	O
with	O
the	O
game	O
.	O
at	O
the	O
start	O
of	O
learning	O
on	O
each	O
game	O
,	O
the	O
weights	O
of	O
dqn	O
’	O
s	O
network	O
were	O
reset	O
to	O
random	O
values	O
.	O
to	O
evaluate	O
dqn	O
’	O
s	O
skill	O
level	O
after	O
learning	O
,	O
its	O
score	O
was	O
averaged	O
over	O
30	O
sessions	O
on	O
each	O
game	O
,	O
each	O
lasting	O
up	O
to	O
5	O
minutes	O
and	O
beginning	O
with	O
a	O
random	O
initial	O
game	O
state	O
.	O
the	O
professional	O
human	O
tester	O
played	O
using	O
the	O
same	O
emulator	O
(	O
with	O
the	O
sound	O
turned	O
oﬀ	O
to	O
remove	O
any	O
possible	O
advantage	O
over	O
dqn	O
which	O
did	O
not	O
process	O
audio	O
)	O
.	O
after	O
2	O
hours	O
of	O
practice	O
,	O
the	O
human	O
played	O
about	O
20	O
episodes	B
of	O
each	O
game	O
for	O
up	O
to	O
5	O
minutes	O
each	O
and	O
was	O
not	O
allowed	O
to	O
take	O
any	O
break	O
during	O
this	O
time	O
.	O
dqn	O
learned	O
to	O
play	O
better	O
than	O
the	O
best	O
previous	O
reinforcement	B
learning	I
systems	O
on	O
all	O
but	O
6	O
of	O
the	O
games	O
,	O
and	O
played	O
better	O
than	O
the	O
human	O
player	O
on	O
22	O
of	O
the	O
games	O
.	O
by	O
considering	O
any	O
performance	O
that	O
scored	O
at	O
or	O
above	O
75	O
%	O
of	O
the	O
human	O
score	O
to	O
be	O
comparable	O
to	O
,	O
or	O
better	O
than	O
,	O
human-level	O
play	O
,	O
mnih	O
et	O
al	O
.	O
concluded	O
that	O
the	O
levels	O
of	O
play	O
dqn	O
learned	O
reached	O
or	O
exceeded	O
human	O
level	O
on	O
29	O
of	O
the	O
46	O
games	O
.	O
see	O
mnih	O
et	O
al	O
.	O
(	O
2015	O
)	O
for	O
a	O
more	O
detailed	O
account	O
of	O
these	O
results	O
.	O
for	O
an	O
artiﬁcial	O
learning	O
system	O
to	O
achieve	O
these	O
levels	O
of	O
play	O
would	O
be	O
impressive	O
enough	O
,	O
but	O
what	O
makes	O
these	O
results	O
remarkable—and	O
what	O
many	O
at	O
the	O
time	O
consid-	O
ered	O
to	O
be	O
breakthrough	O
results	O
for	O
artiﬁcial	O
intelligence—is	O
that	O
the	O
very	O
same	O
learning	O
system	O
achieved	O
these	O
levels	O
of	O
play	O
on	O
widely	O
varying	O
games	O
without	O
relying	O
on	O
any	O
game-speciﬁc	O
modiﬁcations	O
.	O
a	O
human	O
playing	O
any	O
of	O
these	O
49	O
atari	O
games	O
sees	O
210×160	O
pixel	O
image	O
frames	O
with	O
128	O
colors	O
at	O
60hz	O
.	O
in	O
principle	O
,	O
exactly	O
these	O
images	O
could	O
have	O
formed	O
the	O
raw	O
input	O
to	O
dqn	O
,	O
but	O
to	O
reduce	O
memory	O
and	O
processing	O
requirements	O
,	O
mnih	O
et	O
al	O
.	O
preprocessed	O
each	O
frame	O
to	O
produce	O
an	O
84×84	O
array	O
of	O
luminance	O
values	O
.	O
because	O
the	O
full	O
states	O
of	O
many	O
of	O
the	O
atari	O
games	O
are	O
not	O
completely	O
observable	O
from	O
the	O
image	O
frames	O
,	O
mnih	O
et	O
al	O
.	O
“	O
stacked	O
”	O
the	O
four	O
most	O
recent	O
frames	O
so	O
that	O
the	O
inputs	O
to	O
the	O
network	O
had	O
dimension	O
16.5.	O
human-level	O
video	O
game	O
play	O
443	O
84×84×4	O
.	O
this	O
did	O
not	O
eliminate	O
partial	O
observability	O
for	O
all	O
of	O
the	O
games	O
,	O
but	O
it	O
was	O
helpful	O
in	O
making	O
many	O
of	O
them	O
more	O
markovian	O
.	O
an	O
essential	O
point	O
here	O
is	O
that	O
these	O
preprocessing	O
steps	O
were	O
exactly	O
the	O
same	O
for	O
all	O
46	O
games	O
.	O
no	O
game-speciﬁc	O
prior	B
knowledge	I
was	O
involved	O
beyond	O
the	O
general	O
understanding	O
that	O
it	O
should	O
still	O
be	O
possible	O
to	O
learn	O
good	O
policies	O
with	O
this	O
reduced	O
dimension	O
and	O
that	O
stacking	O
adjacent	O
frames	O
should	O
help	O
with	O
the	O
partial	O
observability	O
of	O
some	O
of	O
the	O
games	O
.	O
because	O
no	O
game-speciﬁc	O
prior	B
knowledge	I
beyond	O
this	O
minimal	O
amount	O
was	O
used	O
in	O
preprocessing	O
the	O
image	O
frames	O
,	O
we	O
can	O
think	O
of	O
the	O
84×84×4	O
input	O
vectors	O
as	O
being	O
“	O
raw	O
”	O
input	O
to	O
dqn	O
.	O
the	O
basic	O
architecture	O
of	O
dqn	O
is	O
similar	O
to	O
the	O
deep	O
convolutional	O
ann	O
illustrated	O
in	O
figure	O
9.15	O
(	O
though	O
unlike	O
that	O
network	O
,	O
subsampling	O
in	O
dqn	O
is	O
treated	O
as	O
part	O
of	O
each	O
convolutional	O
layer	O
,	O
with	O
feature	O
maps	O
consisting	O
of	O
units	O
having	O
only	O
a	O
selection	O
of	O
the	O
possible	O
receptive	O
ﬁelds	O
)	O
.	O
dqn	O
has	O
three	O
hidden	O
convolutional	O
layers	O
,	O
followed	O
by	O
one	O
fully	O
connected	O
hidden	O
layer	O
,	O
followed	O
by	O
the	O
output	O
layer	O
.	O
the	O
three	O
successive	O
hidden	O
convolutional	O
layers	O
of	O
dqn	O
produce	O
32	O
20×	O
20	O
feature	O
maps	O
,	O
64	O
9×	O
9	O
feature	O
maps	O
,	O
and	O
64	O
7×7	O
feature	O
maps	O
.	O
the	O
activation	O
function	O
of	O
the	O
units	O
of	O
each	O
feature	O
map	O
is	O
a	O
rectiﬁer	O
nonlinearity	O
(	O
max	O
(	O
0	O
,	O
x	O
)	O
)	O
.	O
the	O
3,136	O
(	O
64×7×7	O
)	O
units	O
in	O
this	O
third	O
convolutional	O
layer	O
all	O
connect	O
to	O
each	O
of	O
512	O
units	O
in	O
the	O
fully	O
connected	O
hidden	O
layer	O
,	O
which	O
then	O
each	O
connect	O
to	O
all	O
18	O
units	O
in	O
the	O
output	O
layer	O
,	O
one	O
for	O
each	O
possible	O
action	B
in	O
an	O
atari	O
game	O
.	O
the	O
activation	O
levels	O
of	O
dqn	O
’	O
s	O
output	O
units	O
were	O
the	O
estimated	O
optimal	O
action	O
values	O
(	O
optimal	O
q-values	O
)	O
of	O
the	O
corresponding	O
state–action	O
pairs	O
,	O
for	O
the	O
state	B
represented	O
by	O
the	O
network	O
’	O
s	O
input	O
.	O
the	O
assignment	O
of	O
output	O
units	O
to	O
a	O
game	O
’	O
s	O
actions	O
varied	O
from	O
game	O
to	O
game	O
,	O
and	O
because	O
the	O
number	O
of	O
valid	O
actions	O
varied	O
between	O
4	O
and	O
18	O
for	O
the	O
games	O
,	O
not	O
all	O
output	O
units	O
had	O
functional	O
roles	O
in	O
all	O
of	O
the	O
games	O
.	O
it	O
helps	O
to	O
think	O
of	O
the	O
network	O
as	O
if	O
it	O
were	O
18	O
separate	O
networks	O
,	O
one	O
for	O
estimating	O
the	O
optimal	O
action	O
value	B
of	O
each	O
possible	O
action	B
.	O
in	O
reality	O
,	O
these	O
networks	O
shared	O
their	O
initial	O
layers	O
,	O
but	O
the	O
output	O
units	O
learned	O
to	O
use	O
the	O
features	O
extracted	O
by	O
these	O
layers	O
in	O
diﬀerent	O
ways	O
.	O
dqn	O
’	O
s	O
reward	B
signal	I
indicated	O
how	O
a	O
games	O
’	O
s	O
score	O
changed	O
from	O
one	O
time	O
step	O
to	O
the	O
next	O
:	O
+1	O
whenever	O
it	O
increased	O
,	O
−1	O
whenever	O
it	O
decreased	O
,	O
and	O
0	O
otherwise	O
.	O
this	O
standardized	O
the	O
reward	B
signal	I
across	O
the	O
games	O
and	O
made	O
a	O
single	O
step-size	B
parameter	I
work	O
well	O
for	O
all	O
the	O
games	O
despite	O
their	O
varying	O
ranges	O
of	O
scores	O
.	O
dqn	O
used	O
an	O
ε-greedy	O
policy	O
,	O
with	O
ε	O
decreasing	O
linearly	O
over	O
the	O
ﬁrst	O
million	O
frames	O
and	O
remaining	O
at	O
a	O
low	O
value	B
for	O
the	O
rest	O
of	O
the	O
learning	O
session	O
.	O
the	O
values	O
of	O
the	O
various	O
other	O
parameters	O
,	O
such	O
as	O
the	O
learning	O
step	O
size	O
,	O
discount	O
rate	O
,	O
and	O
others	O
speciﬁc	O
to	O
the	O
implementation	O
,	O
were	O
selected	O
by	O
performing	O
informal	O
searches	O
to	O
see	O
which	O
values	O
worked	O
best	O
for	O
a	O
small	O
selection	O
of	O
the	O
games	O
.	O
these	O
values	O
were	O
then	O
held	O
ﬁxed	O
for	O
all	O
of	O
the	O
games	O
.	O
after	O
dqn	O
selected	O
an	O
action	B
,	O
the	O
action	B
was	O
executed	O
by	O
the	O
game	O
emulator	O
,	O
which	O
returned	O
a	O
reward	O
and	O
the	O
next	O
video	O
frame	O
.	O
the	O
frame	O
was	O
preprocessed	O
and	O
added	O
to	O
the	O
four-frame	O
stack	O
that	O
became	O
the	O
next	O
input	O
to	O
the	O
network	O
.	O
skipping	O
for	O
the	O
moment	O
the	O
changes	O
to	O
the	O
basic	O
q-learning	O
procedure	O
made	O
by	O
mnih	O
et	O
al.	O
,	O
dqn	O
used	O
the	O
following	O
semi-gradient	O
form	O
of	O
q-learning	O
to	O
update	O
the	O
network	O
’	O
s	O
weights	O
:	O
wt+1	O
=	O
wt	O
+	O
α	O
(	O
cid:104	O
)	O
rt+1	O
+	O
γ	O
max	O
a	O
ˆq	O
(	O
st+1	O
,	O
a	O
,	O
wt	O
)	O
−	O
ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
(	O
cid:105	O
)	O
∇ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
,	O
(	O
16.3	O
)	O
444	O
chapter	O
16	O
:	O
applications	B
and	I
case	I
studies	I
where	O
wt	O
is	O
the	O
vector	B
of	O
the	O
network	O
’	O
s	O
weights	O
,	O
at	O
is	O
the	O
action	B
selected	O
at	O
time	O
step	O
t	O
,	O
and	O
st	O
and	O
st+1	O
are	O
respectively	O
the	O
preprocessed	O
image	O
stacks	O
input	O
to	O
the	O
network	O
at	O
time	O
steps	O
t	O
and	O
t	O
+	O
1.	O
the	O
gradient	B
in	O
(	O
16.3	O
)	O
was	O
computed	O
by	O
backpropagation	B
.	O
imagining	O
again	O
that	O
there	O
was	O
a	O
separate	O
network	O
for	O
each	O
action	B
,	O
for	O
the	O
update	O
at	O
time	O
step	O
t	O
,	O
backpropagation	B
was	O
applied	O
only	O
to	O
the	O
network	O
corresponding	O
to	O
at	O
.	O
mnih	O
et	O
al	O
.	O
took	O
advantage	O
of	O
techniques	O
shown	O
to	O
improve	O
the	O
basic	O
backpropagation	B
algorithm	O
when	O
applied	O
to	O
large	O
networks	O
.	O
they	O
used	O
a	O
mini-batch	O
method	O
that	O
updated	O
weights	O
only	O
after	O
accumulating	B
gradient	O
information	O
over	O
a	O
small	O
batch	O
of	O
images	O
(	O
here	O
after	O
32	O
images	O
)	O
.	O
this	O
yielded	O
smoother	O
sample	O
gradients	O
compared	O
to	O
the	O
usual	O
procedure	O
that	O
updates	O
weights	O
after	O
each	O
action	B
.	O
they	O
also	O
used	O
a	O
gradient-ascent	O
algorithm	O
called	O
rmsprop	O
(	O
tieleman	O
and	O
hinton	O
,	O
2012	O
)	O
that	O
accelerates	O
learning	O
by	O
adjusting	O
the	O
step-size	B
parameter	I
for	O
each	O
weight	O
based	O
on	O
a	O
running	O
average	O
of	O
the	O
magnitudes	O
of	O
recent	O
gradients	O
for	O
that	O
weight	O
.	O
mnih	O
et	O
al	O
.	O
modiﬁed	O
the	O
basic	O
q-learning	O
procedure	O
in	O
three	O
ways	O
.	O
first	O
,	O
they	O
used	O
a	O
method	O
called	O
experience	B
replay	I
ﬁrst	O
studied	O
by	O
lin	O
(	O
1992	O
)	O
.	O
this	O
method	O
stores	O
the	O
agent	O
’	O
s	O
experience	O
at	O
each	O
time	O
step	O
in	O
a	O
replay	O
memory	O
that	O
is	O
accessed	O
to	O
perform	O
the	O
weight	O
updates	O
.	O
it	O
worked	O
like	O
this	O
in	O
dqn	O
.	O
after	O
the	O
game	O
emulator	O
executed	O
action	B
at	O
in	O
a	O
state	B
represented	O
by	O
the	O
image	O
stack	O
st	O
,	O
and	O
returned	O
reward	O
rt+1	O
and	O
image	O
stack	O
st+1	O
,	O
it	O
added	O
the	O
tuple	O
(	O
st	O
,	O
at	O
,	O
rt+1	O
,	O
st+1	O
)	O
to	O
the	O
replay	O
memory	O
.	O
this	O
memory	O
accumulated	O
experiences	O
over	O
many	O
plays	O
of	O
the	O
same	O
game	O
.	O
at	O
each	O
time	O
step	O
multiple	O
q-learning	O
updates—a	O
mini-batch—were	O
performed	O
based	O
on	O
experiences	O
sampled	O
uni-	O
formly	O
at	O
random	O
from	O
the	O
replay	O
memory	O
.	O
instead	O
of	O
st+1	O
becoming	O
the	O
new	O
st	O
for	O
the	O
next	O
update	O
as	O
it	O
would	O
in	O
the	O
usual	O
form	O
of	O
q-learning	O
,	O
a	O
new	O
unconnected	O
expe-	O
rience	O
was	O
drawn	O
from	O
the	O
replay	O
memory	O
to	O
supply	O
data	O
for	O
the	O
next	O
update	O
.	O
because	O
q-learning	O
is	O
an	O
oﬀ-policy	B
algorithm	O
,	O
it	O
does	O
not	O
need	O
to	O
be	O
applied	O
along	O
connected	O
trajectories	O
.	O
q-learning	O
with	O
experience	O
replay	O
provided	O
several	O
advantages	O
over	O
the	O
usual	O
form	O
of	O
q-learning	O
.	O
the	O
ability	O
to	O
use	O
each	O
stored	O
experience	O
for	O
many	O
updates	O
allowed	O
dqn	O
to	O
learn	O
more	O
eﬃciently	O
from	O
its	O
experiences	O
.	O
experience	B
replay	I
reduced	O
the	O
variance	O
of	O
the	O
updates	O
because	O
successive	O
updates	O
were	O
not	O
correlated	O
with	O
one	O
another	O
as	O
they	O
would	O
be	O
with	O
standard	O
q-learning	O
.	O
and	O
by	O
removing	O
the	O
dependence	O
of	O
successive	O
experiences	O
on	O
the	O
current	O
weights	O
,	O
experience	B
replay	I
eliminated	O
one	O
source	O
of	O
instability	O
.	O
mnih	O
et	O
al	O
.	O
modiﬁed	O
standard	O
q-learning	O
in	O
a	O
second	O
way	O
to	O
improve	O
its	O
stability	O
.	O
as	O
in	O
other	O
methods	O
that	O
bootstrap	O
,	O
the	O
target	B
for	O
a	O
q-learning	O
update	O
depends	O
on	O
the	O
current	O
action-value	B
function	I
estimate	O
.	O
when	O
a	O
parameterized	O
function	B
approximation	I
method	O
is	O
used	O
to	O
represent	O
action	B
values	O
,	O
the	O
target	B
is	O
a	O
function	O
of	O
the	O
same	O
param-	O
eters	O
that	O
are	O
being	O
updated	O
.	O
for	O
example	O
,	O
the	O
target	B
in	O
the	O
update	O
given	O
by	O
(	O
16.3	O
)	O
is	O
γ	O
maxa	O
ˆq	O
(	O
st+1	O
,	O
a	O
,	O
wt	O
)	O
.	O
its	O
dependence	O
on	O
wt	O
complicates	O
the	O
process	O
compared	O
to	O
the	O
simpler	O
supervised-learning	O
situation	O
in	O
which	O
the	O
targets	O
do	O
not	O
depend	O
on	O
the	O
param-	O
eters	O
being	O
updated	O
.	O
as	O
discussed	O
in	O
chapter	O
11	O
this	O
can	O
lead	O
to	O
oscillations	O
and/or	O
divergence	O
.	O
to	O
address	O
this	O
problem	O
mnih	O
et	O
al	O
.	O
used	O
a	O
technique	O
that	O
brought	O
q-learning	O
closer	O
to	O
the	O
simpler	O
supervised-learning	O
case	O
while	O
still	O
allowing	O
it	O
to	O
bootstrap	O
.	O
whenever	O
a	O
certain	O
number	O
,	O
c	O
,	O
of	O
updates	O
had	O
been	O
done	O
to	O
the	O
weights	O
w	O
of	O
the	O
action-value	O
16.6.	O
mastering	O
the	O
game	O
of	O
go	O
445	O
network	O
,	O
they	O
inserted	O
the	O
network	O
’	O
s	O
current	O
weights	O
into	O
another	O
network	O
and	O
held	O
these	O
duplicate	O
weights	O
ﬁxed	O
for	O
the	O
next	O
c	O
updates	O
of	O
w.	O
the	O
outputs	O
of	O
this	O
duplicate	O
network	O
over	O
the	O
next	O
c	O
updates	O
of	O
w	O
were	O
used	O
as	O
the	O
q-learning	O
targets	O
.	O
letting	O
˜q	O
denote	O
the	O
output	O
of	O
this	O
duplicate	O
network	O
,	O
then	O
instead	O
of	O
(	O
16.3	O
)	O
the	O
update	O
rule	O
was	O
:	O
wt+1	O
=	O
wt	O
+	O
α	O
(	O
cid:104	O
)	O
rt+1	O
+	O
γ	O
max	O
a	O
˜q	O
(	O
st+1	O
,	O
a	O
,	O
wt	O
)	O
−	O
ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
(	O
cid:105	O
)	O
∇ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
.	O
a	O
ﬁnal	O
modiﬁcation	O
of	O
standard	O
q-learning	O
was	O
also	O
found	O
to	O
improve	O
stability	O
.	O
they	O
clipped	O
the	O
error	O
term	O
rt+1	O
+	O
γ	O
maxa	O
˜q	O
(	O
st+1	O
,	O
a	O
,	O
wt	O
)	O
−	O
ˆq	O
(	O
st	O
,	O
at	O
,	O
wt	O
)	O
so	O
that	O
it	O
remained	O
in	O
the	O
interval	O
[	O
−1	O
,	O
1	O
]	O
.	O
mnih	O
et	O
al	O
.	O
conducted	O
a	O
large	O
number	O
of	O
learning	O
runs	O
on	O
5	O
of	O
the	O
games	O
to	O
gain	O
insight	O
into	O
the	O
eﬀect	O
that	O
various	O
of	O
dqn	O
’	O
s	O
design	O
features	O
had	O
on	O
its	O
performance	O
.	O
they	O
ran	O
dqn	O
with	O
the	O
four	O
combinations	O
of	O
experience	O
replay	O
and	O
the	O
duplicate	O
target	B
network	O
being	O
included	O
or	O
not	O
included	O
.	O
although	O
the	O
results	O
varied	O
from	O
game	O
to	O
game	O
,	O
each	O
of	O
these	O
features	O
alone	O
signiﬁcantly	O
improved	O
performance	O
,	O
and	O
very	O
dramatically	O
im-	O
proved	O
performance	O
when	O
used	O
together	O
.	O
mnih	O
et	O
al	O
.	O
also	O
studied	O
the	O
role	O
played	O
by	O
the	O
deep	O
convolutional	O
ann	O
in	O
dqn	O
’	O
s	O
learning	O
ability	O
by	O
comparing	O
the	O
deep	O
convolutional	O
version	O
of	O
dqn	O
with	O
a	O
version	O
having	O
a	O
network	O
of	O
just	O
one	O
linear	O
layer	O
,	O
both	O
receiving	O
the	O
same	O
stacked	O
preprocessed	O
video	O
frames	O
.	O
here	O
,	O
the	O
improvement	O
of	O
the	O
deep	O
convo-	O
lutional	O
version	O
over	O
the	O
linear	O
version	O
was	O
particularly	O
striking	O
across	O
all	O
5	O
of	O
the	O
test	O
games	O
.	O
creating	O
artiﬁcial	O
agents	O
that	O
excel	O
over	O
a	O
diverse	O
collection	O
of	O
challenging	O
tasks	O
has	O
been	O
an	O
enduring	O
goal	B
of	O
artiﬁcial	B
intelligence	I
.	O
the	O
promise	O
of	O
machine	O
learning	O
as	O
a	O
means	O
for	O
achieving	O
this	O
has	O
been	O
frustrated	O
by	O
the	O
need	O
to	O
craft	O
problem-speciﬁc	O
representations	O
.	O
deepmind	O
’	O
s	O
dqn	O
stands	O
as	O
a	O
major	O
step	O
forward	O
by	O
demonstrating	O
that	O
a	O
single	O
agent	O
can	O
learn	O
problem-speciﬁc	O
features	O
enabling	O
it	O
to	O
acquire	O
human-	O
competitive	O
skills	O
over	O
a	O
range	O
of	O
tasks	O
.	O
but	O
as	O
mnih	O
et	O
al	O
.	O
point	O
out	O
,	O
dqn	O
is	O
not	O
a	O
complete	O
solution	O
to	O
the	O
problem	O
of	O
task-independent	O
learning	O
.	O
although	O
the	O
skills	O
needed	O
to	O
excel	O
on	O
the	O
atari	O
games	O
were	O
markedly	O
diverse	O
,	O
all	O
the	O
games	O
were	O
played	O
by	O
observing	O
video	O
images	O
,	O
which	O
made	O
a	O
deep	O
convolutional	O
ann	O
a	O
natural	O
choice	O
for	O
this	O
collection	O
of	O
tasks	O
.	O
in	O
addition	O
,	O
dqn	O
’	O
s	O
performance	O
on	O
some	O
of	O
the	O
atari	O
2600	O
games	O
fell	O
considerably	O
short	O
of	O
human	O
skill	O
levels	O
on	O
these	O
games	O
.	O
the	O
games	O
most	O
diﬃcult	O
for	O
dqn—especially	O
montezuma	O
’	O
s	O
revenge	O
on	O
which	O
dqn	O
learned	O
to	O
perform	O
about	O
as	O
well	O
as	O
the	O
random	O
player—require	O
deep	O
planning	O
beyond	O
what	O
dqn	O
was	O
designed	O
to	O
do	O
.	O
further	O
,	O
learning	O
control	O
skills	O
through	O
extensive	O
practice	O
,	O
like	O
dqn	O
learned	O
how	O
to	O
play	O
the	O
atari	O
games	O
,	O
is	O
just	O
one	O
of	O
the	O
types	O
of	O
learning	O
humans	O
routinely	O
accomplish	O
.	O
despite	O
these	O
limitations	O
,	O
dqn	O
advanced	O
the	O
state-of-the-art	O
in	O
machine	O
learning	O
by	O
impressively	O
demonstrating	O
the	O
promise	O
of	O
combining	O
reinforcement	B
learning	I
with	O
modern	O
methods	O
of	O
deep	O
learning	O
.	O
16.6	O
mastering	O
the	O
game	O
of	O
go	O
the	O
ancient	O
chinese	O
game	O
of	O
go	O
has	O
challenged	O
artiﬁcial	B
intelligence	I
researchers	O
for	O
many	O
decades	O
.	O
methods	O
that	O
achieve	O
human-level	O
skill	O
,	O
or	O
even	O
superhuman-level	O
skill	O
,	O
446	O
chapter	O
16	O
:	O
applications	B
and	I
case	I
studies	I
in	O
other	O
games	O
have	O
not	O
been	O
successful	O
in	O
producing	O
strong	O
go	O
programs	O
.	O
thanks	O
to	O
a	O
very	O
active	O
community	O
of	O
go	O
programmers	O
and	O
international	O
competitions	O
,	O
the	O
level	O
of	O
go	O
program	O
play	O
has	O
improved	O
signiﬁcantly	O
over	O
the	O
years	O
.	O
until	O
recently	O
,	O
however	O
,	O
no	O
go	O
program	O
had	O
been	O
able	O
to	O
play	O
anywhere	O
near	O
the	O
level	O
of	O
a	O
human	O
go	O
master	O
.	O
a	O
team	O
at	O
deepmind	O
(	O
silver	O
et	O
al.	O
,	O
2016	O
)	O
developed	O
the	O
program	O
alphago	O
that	O
broke	O
this	O
barrier	O
by	O
combining	O
deep	O
artiﬁcial	O
neural	B
networks	I
(	O
deep	O
anns	O
,	O
section	O
9.7	O
)	O
,	O
su-	O
pervised	O
learning	O
,	O
monte	O
carlo	O
tree	O
search	O
(	O
mcts	O
,	O
section	O
8.11	O
)	O
,	O
and	B
reinforcement	I
learning	O
.	O
by	O
the	O
time	O
of	O
silver	O
et	O
al.	O
’	O
s	O
2016	O
publication	O
,	O
alphago	O
had	O
been	O
shown	O
to	O
be	O
decisively	O
stronger	O
than	O
other	O
current	O
go	O
programs	O
,	O
and	O
it	O
had	O
defeated	O
the	O
european	O
go	O
champion	O
fan	O
hui	O
5	O
games	O
to	O
0.	O
these	O
were	O
the	O
ﬁrst	O
victories	O
of	O
a	O
go	O
program	O
over	O
a	O
professional	O
human	O
go	O
player	O
without	O
handicap	O
in	O
full	O
go	O
games	O
.	O
shortly	O
thereafter	O
,	O
a	O
similar	O
version	O
of	O
alphago	O
won	O
stunning	O
victories	O
over	O
the	O
18-time	O
world	O
champion	O
lee	O
sedol	O
,	O
winning	O
4	O
out	O
of	O
a	O
5	O
games	O
in	O
a	O
challenge	O
match	O
,	O
making	O
worldwide	O
head-	O
line	O
news	O
.	O
artiﬁcial	B
intelligence	I
researchers	O
thought	O
that	O
it	O
would	O
be	O
many	O
more	O
years	O
,	O
perhaps	O
decades	O
,	O
before	O
a	O
program	O
reached	O
this	O
level	O
of	O
play	O
.	O
here	O
we	O
describe	O
alphago	O
and	O
a	O
successor	O
program	O
called	O
alphago	O
zero	O
(	O
silver	O
et	O
al	O
.	O
2017a	O
)	O
.	O
where	O
in	O
addition	O
to	O
reinforcement	B
learning	I
,	O
alphago	O
relied	O
on	O
supervised	B
learning	I
from	O
a	O
large	O
database	O
of	O
expert	O
human	O
moves	O
,	O
alphago	O
zero	O
used	O
only	O
rein-	O
forcement	O
learning	O
and	O
no	O
human	O
data	O
or	O
guidance	O
beyond	O
the	O
basic	O
rules	O
of	O
the	O
game	O
(	O
hence	O
the	O
zero	O
in	O
its	O
name	O
)	O
.	O
we	O
ﬁrst	O
describe	O
alphago	O
in	O
some	O
detail	O
in	O
order	O
to	O
highlight	O
the	O
relative	O
simplicity	O
of	O
alphago	O
zero	O
,	O
which	O
is	O
both	O
higher-performing	O
and	O
more	O
of	O
a	O
pure	O
reinforcement	B
learning	I
program	O
.	O
in	O
many	O
ways	O
,	O
both	O
alphago	O
and	O
alphago	O
zero	O
are	O
descendants	O
of	O
tesauo	O
’	O
s	O
td-	O
gammon	O
(	O
section	O
16.1	O
)	O
,	O
itself	O
a	O
descendant	O
of	O
samuel	O
’	O
s	O
checkers	O
player	O
(	O
section	O
16.2	O
)	O
.	O
all	O
these	O
programs	O
included	O
reinforcement	B
learning	I
over	O
simulated	O
games	O
of	O
self-play	O
.	O
alphago	O
and	O
alphago	O
zero	O
also	O
built	O
upon	O
the	O
progress	O
made	O
by	O
deepmind	O
on	O
playing	O
atari	O
games	O
with	O
the	O
program	O
dqn	O
(	O
section	O
16.5	O
)	O
that	O
used	O
deep	O
convolutional	O
anns	O
to	O
approximate	B
optimal	O
value	B
functions	O
.	O
go	O
is	O
a	O
game	O
between	O
two	O
players	O
who	O
al-	O
ternately	O
place	O
black	O
and	O
white	O
‘	O
stones	O
’	O
on	O
un-	O
occupied	O
intersections	O
,	O
or	O
‘	O
points	O
,	O
’	O
on	O
a	O
board	O
with	O
a	O
grid	O
of	O
19	O
horizontal	O
and	O
19	O
vertical	O
lines	O
to	O
produce	O
positions	O
like	O
that	O
shown	O
to	O
the	O
right	O
.	O
the	O
game	O
’	O
s	O
goal	B
is	O
to	O
capture	O
an	O
area	O
of	O
the	O
board	O
larger	O
than	O
that	O
captured	O
by	O
the	O
op-	O
ponent	O
.	O
stones	O
are	O
captured	O
according	O
to	O
sim-	O
ple	O
rules	O
.	O
a	O
player	O
’	O
s	O
stones	O
are	O
captured	O
if	O
they	O
are	O
completely	O
surrounded	O
by	O
the	O
other	O
player	O
’	O
s	O
stones	O
,	O
meaning	O
that	O
there	O
is	O
no	O
horizontally	O
or	O
vertically	O
adjacent	O
point	O
that	O
is	O
unoccupied	O
.	O
for	O
example	O
,	O
figure	O
16.5	O
shows	O
on	O
the	O
left	O
three	O
white	O
stones	O
with	O
an	O
unoccupied	O
adjacent	O
point	O
(	O
labeled	O
x	O
)	O
.	O
if	O
player	O
black	O
places	O
a	O
stone	O
on	O
x	O
,	O
the	O
three	O
white	O
stones	O
are	O
captured	O
and	O
taken	O
a	O
go	O
board	O
conﬁguration	O
16.6.	O
mastering	O
the	O
game	O
of	O
go	O
447	O
oﬀ	O
the	O
board	O
(	O
figure	O
16.5	O
middle	O
)	O
.	O
however	O
,	O
if	O
player	O
white	O
were	O
to	O
place	O
a	O
stone	O
on	O
point	O
x	O
ﬁrst	O
,	O
than	O
the	O
possibility	O
of	O
this	O
capture	O
would	O
be	O
blocked	O
(	O
figure	O
16.5	O
right	O
)	O
.	O
other	O
rules	O
are	O
needed	O
to	O
prevent	O
inﬁnite	O
capturing/re-capturing	O
loops	O
.	O
the	O
game	O
ends	O
when	O
neither	O
player	O
wishes	O
to	O
place	O
another	O
stone	O
.	O
these	O
rules	O
are	O
simple	O
,	O
but	O
they	O
produce	O
a	O
very	O
complex	O
game	O
that	O
has	O
had	O
wide	O
appeal	O
for	O
thousands	O
of	O
years	O
.	O
figure	O
16.5	O
:	O
go	O
capturing	O
rule	O
.	O
left	O
:	O
the	O
three	O
white	O
stones	O
are	O
not	O
surrounded	O
because	O
point	O
x	O
is	O
unoccupied	O
.	O
middle	O
:	O
if	O
black	O
places	O
a	O
stone	O
on	O
x	O
,	O
the	O
three	O
white	O
stones	O
are	O
captured	O
and	O
removed	O
from	O
the	O
board	O
.	O
right	O
:	O
if	O
white	O
places	O
a	O
stone	O
on	O
point	O
x	O
ﬁrst	O
,	O
the	O
capture	O
is	O
blocked	O
.	O
methods	O
that	O
produce	O
strong	O
play	O
for	O
other	O
games	O
,	O
such	O
as	O
chess	O
,	O
have	O
not	O
worked	O
as	O
well	O
for	O
go	O
.	O
the	O
search	O
space	O
for	O
go	O
is	O
signiﬁcantly	O
larger	O
than	O
that	O
of	O
chess	O
because	O
go	O
has	O
a	O
larger	O
number	O
of	O
legal	O
moves	O
per	O
position	O
than	O
chess	B
(	O
≈	O
250	O
versus	O
≈	O
35	O
)	O
and	O
go	O
games	O
tend	O
to	O
involve	O
more	O
moves	O
than	O
chess	B
games	O
(	O
≈	O
150	O
versus	O
≈	O
80	O
)	O
.	O
but	O
the	O
size	O
of	O
the	O
search	O
space	O
is	O
not	O
the	O
major	O
factor	O
that	O
makes	O
go	O
so	O
diﬃcult	O
.	O
exhaustive	O
search	O
is	O
infeasible	O
for	O
both	O
chess	B
and	O
go	O
,	O
and	O
go	O
on	O
smaller	O
boards	O
,	O
e.g.	O
,	O
9	O
×	O
9	O
,	O
has	O
proven	O
to	O
be	O
exceedingly	O
diﬃcult	O
as	O
well	O
.	O
experts	O
agree	O
that	O
the	O
major	O
stumbling	O
block	O
to	O
creating	O
stronger-than-amateur	O
go	O
programs	O
is	O
the	O
diﬃculty	O
of	O
deﬁning	O
an	O
adequate	O
position	O
evaluation	O
function	O
.	O
a	O
good	O
evaluation	O
function	O
allows	O
search	O
to	O
be	O
truncated	B
at	O
a	O
feasible	O
depth	O
by	O
providing	O
relatively	O
easy-to-compute	O
predictions	O
of	O
what	O
deeper	O
search	O
would	O
likely	O
yield	O
.	O
according	O
to	O
m¨uller	O
(	O
2002	O
)	O
:	O
“	O
no	O
simple	O
yet	O
reasonable	O
evaluation	O
function	O
will	O
ever	O
be	O
found	O
for	O
go.	O
”	O
a	O
major	O
step	O
forward	O
was	O
the	O
introduction	O
of	O
mcts	O
to	O
go	O
programs	O
.	O
the	O
strongest	O
programs	O
at	O
the	O
time	O
of	O
alphago	O
’	O
s	O
development	O
all	O
included	O
mcts	O
,	O
but	O
master-level	O
skill	O
remained	O
elusive	O
.	O
recall	O
from	O
section	O
8.11	O
that	O
mcts	O
is	O
a	O
decision-time	O
planning	B
procedure	O
that	O
does	O
not	O
attempt	O
to	O
learn	O
and	O
store	O
a	O
global	O
evaluation	O
function	O
.	O
like	O
a	O
rollout	O
algorithm	O
(	O
section	O
8.10	O
)	O
,	O
it	O
runs	O
many	O
monte	O
carlo	O
simulations	O
of	O
entire	O
episodes	B
(	O
here	O
,	O
entire	O
go	O
games	O
)	O
to	O
select	O
each	O
action	B
(	O
here	O
,	O
each	O
go	O
move	O
:	O
where	O
to	O
place	O
a	O
stone	O
or	O
to	O
resign	O
)	O
.	O
unlike	O
a	O
simple	O
rollout	O
algorithm	O
,	O
however	O
,	O
mcts	O
is	O
an	O
iterative	B
procedure	O
that	O
incrementally	O
extends	O
a	O
search	O
tree	O
whose	O
root	O
node	O
represents	O
the	O
current	O
environment	B
state	O
.	O
as	O
illustrated	O
in	O
figure	O
8.10	O
,	O
each	O
iteration	O
traverses	O
the	O
tree	O
by	O
simulating	O
actions	O
guided	O
by	O
statistics	O
associated	O
with	O
the	O
tree	O
’	O
s	O
edges	O
.	O
in	O
its	O
basic	O
version	O
,	O
when	O
a	O
simulation	O
reaches	O
a	O
leaf	O
node	O
of	O
the	O
search	O
tree	O
,	O
mcts	O
expands	O
the	O
tree	O
by	O
adding	O
some	O
,	O
or	O
all	O
,	O
of	O
the	O
leaf	O
node	O
’	O
s	O
children	O
to	O
the	O
tree	O
.	O
from	O
the	O
leaf	O
node	O
,	O
or	O
one	O
of	O
its	O
newly	O
added	O
child	O
notes	O
,	O
a	O
rollout	O
is	O
executed	O
:	O
a	O
simulation	O
that	O
typically	O
proceeds	O
all	O
the	O
way	O
to	O
a	O
terminal	O
state	B
,	O
with	O
actions	O
selected	O
by	O
a	O
rollout	O
policy	O
.	O
when	O
the	O
rollout	O
completes	O
,	O
the	O
statistics	O
associated	O
with	O
the	O
search	O
tree	O
’	O
s	O
edges	O
that	O
were	O
traversed	O
in	O
this	O
iteration	O
are	O
updated	O
by	O
backing	O
up	O
the	O
return	B
produced	O
by	O
the	O
rollout	O
.	O
mcts	O
continues	O
this	O
process	O
,	O
starting	O
each	O
time	O
at	O
the	O
search	O
tree	O
’	O
s	O
root	O
at	O
the	O
current	O
state	B
,	O
for	O
as	O
many	O
iterations	O
as	O
possible	O
given	O
the	O
time	O
constraints	O
.	O
then	O
,	O
ﬁnally	O
,	O
an	O
action	B
from	O
the	O
root	O
node	O
(	O
which	O
still	O
represents	O
the	O
current	O
environment	B
state	O
)	O
is	O
selected	O
x	O
448	O
chapter	O
16	O
:	O
applications	B
and	I
case	I
studies	I
according	O
to	O
statistics	O
accumulated	O
in	O
the	O
root	O
node	O
’	O
s	O
outgoing	O
edges	O
.	O
this	O
is	O
the	O
action	B
the	O
agent	O
takes	O
.	O
after	O
the	O
environment	B
transitions	O
to	O
its	O
next	O
state	B
,	O
mcts	O
is	O
executed	O
again	O
with	O
the	O
root	O
node	O
set	O
to	O
represent	O
the	O
new	O
current	O
state	B
.	O
the	O
search	O
tree	O
at	O
the	O
start	O
of	O
this	O
next	O
execution	O
might	O
be	O
just	O
this	O
new	O
root	O
node	O
,	O
or	O
it	O
might	O
include	O
descendants	O
of	O
this	O
node	O
left	O
over	O
from	O
mcts	O
’	O
s	O
previous	O
execution	O
.	O
the	O
remainder	O
of	O
the	O
tree	O
is	O
discarded	O
.	O
16.6.1	O
alphago	O
the	O
main	O
innovation	O
that	O
made	O
alphago	O
such	O
a	O
strong	O
player	O
is	O
that	O
it	O
selected	O
moves	O
by	O
a	O
novel	O
version	O
of	O
mcts	O
that	O
was	O
guided	O
by	O
both	O
a	O
policy	B
and	O
a	O
value	B
function	I
learned	O
by	O
reinforcement	B
learning	I
with	O
function	B
approximation	I
provided	O
by	O
deep	O
convo-	O
lutional	O
anns	O
.	O
another	O
key	O
feature	O
is	O
that	O
instead	O
of	O
reinforcement	O
learning	O
starting	O
from	O
random	O
network	O
weights	O
,	O
it	O
started	O
from	O
weights	O
that	O
were	O
the	O
result	O
of	O
previous	O
supervised	B
learning	I
from	O
a	O
large	O
collection	O
of	O
human	O
expert	O
moves	O
.	O
the	O
deepmind	O
team	O
called	O
alphago	O
’	O
s	O
modiﬁcation	O
of	O
basic	O
mcts	O
“	O
asynchronous	O
policy	O
and	O
value	O
mcts	O
,	O
”	O
or	O
apv-mcts	O
.	O
it	O
selected	O
actions	O
via	O
basic	O
mcts	O
as	O
described	O
above	O
but	O
with	O
some	O
twists	O
in	O
how	O
it	O
extended	O
its	O
search	O
tree	O
and	O
how	O
it	O
evaluated	O
action	B
edges	O
.	O
in	O
contrast	O
to	O
basic	O
mcts	O
,	O
which	O
expands	O
its	O
current	O
search	O
tree	O
by	O
using	O
stored	O
action	B
values	O
to	O
select	O
an	O
unexplored	O
edge	O
from	O
a	O
leaf	O
node	O
,	O
apv-mcts	O
,	O
as	O
implemented	O
in	O
alphago	O
,	O
expanded	O
its	O
tree	O
by	O
choosing	O
an	O
edge	O
according	O
to	O
probabilities	O
supplied	O
by	O
a	O
13-layer	O
deep	O
convolutional	O
ann	O
,	O
called	O
the	O
sl-policy	O
network	O
,	O
trained	O
previously	O
by	O
supervised	B
learning	I
to	O
predict	O
moves	O
contained	O
in	O
a	O
database	O
of	O
nearly	O
30	O
million	O
human	O
expert	O
moves	O
.	O
then	O
,	O
also	O
in	O
contrast	O
to	O
basic	O
mcts	O
,	O
which	O
evaluates	O
the	O
newly-added	O
state	B
node	O
solely	O
by	O
the	O
return	B
of	O
a	O
rollout	O
initiated	O
from	O
it	O
,	O
apv-mcts	O
evaluated	O
the	O
node	O
in	O
two	O
ways	O
:	O
by	O
this	O
return	B
of	O
the	O
rollout	O
,	O
but	O
also	O
by	O
a	O
value	B
function	I
,	O
vθ	O
,	O
learned	O
previously	O
by	O
a	O
reinforcement	B
learning	I
method	O
.	O
if	O
s	O
was	O
the	O
newly-added	O
node	O
,	O
its	O
value	B
became	O
v	O
(	O
s	O
)	O
=	O
(	O
1	O
−	O
η	O
)	O
vθ	O
(	O
s	O
)	O
+	O
ηg	O
,	O
(	O
16.4	O
)	O
where	O
g	O
was	O
the	O
return	B
of	O
the	O
rollout	O
and	O
η	O
controlled	O
the	O
mixing	O
of	O
the	O
values	O
resulting	O
from	O
these	O
two	O
evaluation	O
methods	O
.	O
in	O
alphago	O
,	O
these	O
values	O
were	O
supplied	O
by	O
the	O
value	B
network	O
,	O
another	O
13-layer	O
deep	O
convolutional	O
ann	O
that	O
was	O
trained	O
as	O
we	O
describe	O
below	O
to	O
output	O
estimated	O
values	O
of	O
board	O
positions	O
.	O
apv-mcts	O
’	O
s	O
rollouts	O
in	O
alphago	O
were	O
simulated	O
games	O
with	O
both	O
players	O
using	O
a	O
fast	O
rollout	O
policy	O
provided	O
by	O
a	O
simple	O
linear	O
network	O
,	O
also	O
trained	O
by	O
supervised	B
learning	I
before	O
play	O
.	O
throughout	O
its	O
execution	O
,	O
apv-mcts	O
kept	O
track	O
of	O
how	O
many	O
simulations	O
passed	O
through	O
each	O
edge	O
of	O
the	O
search	O
tree	O
,	O
and	O
when	O
its	O
execution	O
completed	O
,	O
the	O
most-visited	O
edge	O
from	O
the	O
root	O
node	O
was	O
selected	O
as	O
the	O
action	B
to	O
take	O
,	O
here	O
the	O
move	O
alphago	O
actually	O
made	O
in	O
a	O
game	O
.	O
the	O
value	B
network	O
had	O
the	O
same	O
structure	O
as	O
the	O
deep	O
convolutional	O
sl	O
policy	B
network	O
except	O
that	O
it	O
had	O
a	O
single	O
output	O
unit	O
that	O
gave	O
estimated	O
values	O
of	O
game	O
positions	O
instead	O
of	O
the	O
sl	O
policy	B
network	O
’	O
s	O
probability	O
distributions	O
over	O
legal	O
actions	O
.	O
ideally	O
,	O
the	O
value	B
network	O
would	O
output	O
optimal	O
state	O
values	O
,	O
and	O
it	O
might	O
have	O
been	O
possible	O
to	O
approximate	B
the	O
optimal	O
value	O
function	O
along	O
the	O
lines	O
of	O
td-gammon	O
described	O
above	O
:	O
16.6.	O
mastering	O
the	O
game	O
of	O
go	O
449	O
self-play	O
with	O
nonlinear	O
td	O
(	O
λ	O
)	O
coupled	O
to	O
a	O
deep	O
convolutional	O
ann	O
.	O
but	O
the	O
deepmind	O
team	O
took	O
a	O
diﬀerent	O
approach	O
that	O
held	O
more	O
promise	O
for	O
a	O
game	O
as	O
complex	O
as	O
go	O
.	O
they	O
divided	O
the	O
process	O
of	O
training	O
the	O
value	B
network	O
into	O
two	O
stages	O
.	O
in	O
the	O
ﬁrst	O
stage	O
,	O
they	O
created	O
the	O
best	O
policy	B
they	O
could	O
by	O
using	O
reinforcement	B
learning	I
to	O
train	O
an	O
rl	O
policy	B
network	O
.	O
this	O
was	O
a	O
deep	O
convolutional	O
ann	O
with	O
the	O
same	O
structure	O
as	O
the	O
sl	O
policy	B
network	O
.	O
it	O
was	O
initialized	O
with	O
the	O
ﬁnal	O
weights	O
of	O
the	O
sl	O
policy	B
network	O
that	O
were	O
learned	O
via	O
supervised	B
learning	I
,	O
and	O
then	O
policy-gradient	O
reinforcement	B
learning	I
was	O
used	O
to	O
improve	O
upon	O
the	O
sl	O
policy	B
.	O
in	O
the	O
second	O
stage	O
of	O
training	O
the	O
value	B
network	O
,	O
the	O
team	O
used	O
monte	O
carlo	O
policy	B
evaluation	I
on	O
data	O
obtained	O
from	O
a	O
large	O
number	O
of	O
simulated	O
self-play	O
games	O
with	O
moves	O
selected	O
by	O
the	O
rl	O
policy	B
network	O
.	O
figure	O
16.6	O
illustrates	O
the	O
networks	O
used	O
by	O
alphago	O
and	O
the	O
steps	O
taken	O
to	O
train	O
them	O
in	O
what	O
the	O
deepmind	O
team	O
called	O
the	O
“	O
alphago	O
pipeline.	O
”	O
all	O
these	O
networks	O
were	O
trained	O
before	O
any	O
live	O
game	O
play	O
took	O
place	O
,	O
and	O
their	O
weights	O
remained	O
ﬁxed	O
throughout	O
live	O
play	O
.	O
figure	O
16.6	O
:	O
alphago	O
pipeline	O
.	O
adapted	O
with	O
permission	O
from	O
macmillan	O
publishers	O
ltd	O
:	O
nature	O
,	O
vol	O
.	O
529	O
(	O
7587	O
)	O
,	O
p.	O
485	O
,	O
copyright	O
(	O
2016	O
)	O
.	O
here	O
is	O
some	O
more	O
detail	O
about	O
alphago	O
’	O
s	O
anns	O
and	O
their	O
training	O
.	O
the	O
identically-	O
structured	O
sl	O
and	O
rl	O
policy	B
networks	O
were	O
similar	O
to	O
dqn	O
’	O
s	O
deep	O
convolutional	O
network	O
described	O
in	O
section	O
16.5	O
for	O
playing	O
atari	O
games	O
,	O
except	O
that	O
they	O
had	O
13	O
convolutional	O
layers	O
with	O
the	O
ﬁnal	O
layer	O
consisting	O
of	O
a	O
soft-max	B
unit	O
for	O
each	O
point	O
on	O
the	O
19	O
×	O
19	O
go	O
board	O
.	O
the	O
networks	O
’	O
input	O
was	O
a	O
19	O
×	O
19	O
×	O
48	O
image	O
stack	O
in	O
which	O
each	O
point	O
on	O
the	O
go	O
board	O
was	O
represented	O
by	O
the	O
values	O
of	O
48	O
binary	O
or	O
integer-valued	O
features	O
.	O
for	O
example	O
,	O
for	O
each	O
point	O
,	O
one	O
feature	O
indicated	O
if	O
the	O
point	O
was	O
occupied	O
by	O
one	O
of	O
alphago	O
’	O
s	O
stones	O
,	O
one	O
of	O
its	O
opponent	O
’	O
s	O
stones	O
,	O
or	O
was	O
unoccupied	O
,	O
thus	O
providing	O
the	O
“	O
raw	O
”	O
representation	O
of	O
the	O
board	O
conﬁguration	O
.	O
other	O
features	O
were	O
based	O
on	O
the	O
rules	O
of	O
go	O
,	O
such	O
as	O
the	O
number	O
of	O
adjacent	O
points	O
that	O
were	O
empty	O
,	O
the	O
number	O
of	O
opponent	O
stones	O
that	O
would	O
be	O
captured	O
by	O
placing	O
a	O
stone	O
there	O
,	O
the	O
number	O
of	O
turns	O
since	O
a	O
stone	O
rollout	O
policy	O
sl	O
policy	B
network	O
rl	O
policy	B
network	O
value	B
networkpolicy	O
gradientsupervised	O
learningmc	O
policy	B
evaluationself	O
playsupervised	O
learning	O
]	O
]	O
networksdata	O
450	O
chapter	O
16	O
:	O
applications	B
and	I
case	I
studies	I
was	O
placed	O
there	O
,	O
and	O
other	O
features	O
that	O
the	O
design	O
team	O
considered	O
to	O
be	O
important	O
.	O
training	O
the	O
sl	O
policy	B
network	O
took	O
approximately	O
3	O
weeks	O
using	O
a	O
distributed	O
im-	O
plementation	O
of	O
stochastic	O
gradient	B
ascent	O
on	O
50	O
processors	O
.	O
the	O
network	O
achieved	O
57	O
%	O
accuracy	O
,	O
where	O
the	O
best	O
accuracy	O
achieved	O
by	O
other	O
groups	O
at	O
the	O
time	O
of	O
publication	O
was	O
44.4	O
%	O
.	O
training	O
the	O
rl	O
policy	B
network	O
was	O
done	O
by	O
policy	O
gradient	O
reinforcement	O
learning	O
over	O
simulated	O
games	O
between	O
the	O
rl	O
policy	B
network	O
’	O
s	O
current	O
policy	B
and	O
op-	O
ponents	O
using	O
policies	O
randomly	O
selected	O
from	O
policies	O
produced	O
by	O
earlier	O
iterations	O
of	O
the	O
learning	O
algorithm	O
.	O
playing	O
against	O
a	O
randomly	O
selected	O
collection	O
of	O
opponents	O
pre-	O
vented	O
overﬁtting	O
to	O
the	O
current	O
policy	B
.	O
the	O
reward	B
signal	I
was	O
+1	O
if	O
the	O
current	O
policy	B
won	O
,	O
−1	O
if	O
it	O
lost	O
,	O
and	O
zero	O
otherwise	O
.	O
these	O
games	O
directly	O
pitted	O
the	O
two	O
policies	O
against	O
one	O
another	O
without	O
involving	O
mcts	O
.	O
by	O
simulating	O
many	O
games	O
in	O
parallel	O
on	O
50	O
processors	O
,	O
the	O
deepmind	O
team	O
trained	O
the	O
rl	O
policy	B
network	O
on	O
a	O
million	O
games	O
in	O
a	O
single	O
day	O
.	O
in	O
testing	O
the	O
ﬁnal	O
rl	O
policy	B
,	O
they	O
found	O
that	O
it	O
won	O
more	O
than	O
80	O
%	O
of	O
games	O
played	O
against	O
the	O
sl	O
policy	B
,	O
and	O
it	O
won	O
85	O
%	O
of	O
games	O
played	O
against	O
a	O
go	O
program	O
using	O
mcts	O
that	O
simulated	O
100,000	O
games	O
per	O
move	O
.	O
the	O
value	B
network	O
,	O
whose	O
structure	O
was	O
similar	O
to	O
that	O
of	O
the	O
sl	O
and	O
rl	O
policy	B
networks	O
except	O
for	O
its	O
single	O
output	O
unit	O
,	O
received	O
the	O
same	O
input	O
as	O
the	O
sl	O
and	O
rl	O
policy	B
networks	O
with	O
the	O
exception	O
that	O
there	O
was	O
an	O
additional	O
binary	O
feature	O
giving	O
the	O
current	O
color	O
to	O
play	O
.	O
monte	O
carlo	O
policy	B
evaluation	I
was	O
used	O
to	O
train	O
the	O
network	O
from	O
data	O
obtained	O
from	O
a	O
large	O
number	O
of	O
self-play	O
games	O
played	O
using	O
the	O
rl	O
policy	B
.	O
to	O
avoid	O
overﬁtting	O
and	O
instability	O
due	O
to	O
the	O
strong	O
correlations	O
between	O
positions	O
encountered	O
in	O
self-play	O
,	O
the	O
deepmind	O
team	O
constructed	O
a	O
data	O
set	O
of	O
30	O
million	O
positions	O
each	O
chosen	O
randomly	O
from	O
a	O
unique	O
self-play	O
game	O
.	O
then	O
training	O
was	O
done	O
using	O
50	O
million	O
mini-batches	O
each	O
of	O
32	O
positions	O
drawn	O
from	O
this	O
data	O
set	O
.	O
training	O
took	O
one	O
week	O
on	O
50	O
gpus	O
.	O
the	O
rollout	O
policy	O
was	O
learned	O
prior	O
to	O
play	O
by	O
a	O
simple	O
linear	O
network	O
trained	O
by	O
supervised	B
learning	I
from	O
a	O
corpus	O
of	O
8	O
million	O
human	O
moves	O
.	O
the	O
rollout	O
policy	O
network	O
had	O
to	O
output	O
actions	O
quickly	O
while	O
still	O
being	O
reasonably	O
accurate	O
.	O
in	O
principle	O
,	O
the	O
sl	O
or	O
rl	O
policy	B
networks	O
could	O
have	O
been	O
used	O
in	O
the	O
rollouts	O
,	O
but	O
the	O
forward	O
propagation	O
through	O
these	O
deep	O
networks	O
took	O
too	O
much	O
time	O
for	O
either	O
of	O
them	O
to	O
be	O
used	O
in	O
rollout	O
simulations	O
,	O
a	O
great	O
many	O
of	O
which	O
had	O
to	O
be	O
carried	O
out	O
for	O
each	O
move	O
decision	O
during	O
live	O
play	O
.	O
for	O
this	O
reason	O
,	O
the	O
rollout	O
policy	O
network	O
was	O
less	O
complex	O
than	O
the	O
other	O
policy	B
networks	O
,	O
and	O
its	O
input	O
features	O
could	O
be	O
computed	O
more	O
quickly	O
than	O
the	O
features	O
used	O
for	O
the	O
policy	B
networks	O
.	O
the	O
rollout	O
policy	O
network	O
allowed	O
approximately	O
1,000	O
complete	O
game	O
simulations	O
per	O
second	O
to	O
be	O
run	O
on	O
each	O
of	O
the	O
processing	O
threads	O
that	O
alphago	O
used	O
.	O
one	O
may	O
wonder	O
why	O
the	O
sl	O
policy	B
was	O
used	O
instead	O
of	O
the	O
better	O
rl	O
policy	B
to	O
select	O
actions	O
in	O
the	O
expansion	O
phase	O
of	O
apv-mcts	O
.	O
these	O
policies	O
took	O
the	O
same	O
amount	O
of	O
time	O
to	O
compute	O
because	O
they	O
used	O
the	O
same	O
network	O
architecture	O
.	O
the	O
team	O
actually	O
found	O
that	O
alphago	O
played	O
better	O
against	O
human	O
opponents	O
when	O
apv-mcts	O
used	O
as	O
the	O
sl	O
policy	B
instead	O
of	O
the	O
rl	O
policy	B
.	O
they	O
conjectured	O
that	O
the	O
reason	O
for	O
this	O
was	O
that	O
the	O
latter	O
was	O
tuned	O
to	O
respond	O
to	O
optimal	O
moves	O
rather	O
than	O
to	O
the	O
broader	O
set	O
of	O
moves	O
characteristic	O
of	O
human	O
play	O
.	O
interestingly	O
,	O
the	O
situation	O
was	O
reversed	O
for	O
the	O
value	B
function	I
used	O
by	O
apv-mcts	O
.	O
they	O
found	O
that	O
when	O
apv-mcts	O
used	O
the	O
value	B
16.6.	O
mastering	O
the	O
game	O
of	O
go	O
451	O
function	O
derived	O
from	O
the	O
rl	O
policy	B
,	O
it	O
performed	O
better	O
than	O
if	O
it	O
used	O
the	O
value	B
function	I
derived	O
from	O
the	O
sl	O
policy	B
.	O
several	O
methods	O
worked	O
together	O
to	O
produce	O
alphago	O
’	O
s	O
impressive	O
playing	O
skill	O
.	O
the	O
deepmind	O
team	O
evaluated	O
diﬀerent	O
versions	O
of	O
alphago	O
in	O
order	O
to	O
assess	O
the	O
contri-	O
butions	O
made	O
by	O
these	O
various	O
components	O
.	O
the	O
parameter	O
η	O
in	O
(	O
16.4	O
)	O
controlled	O
the	O
mixing	O
of	O
game	O
state	B
evaluations	O
produced	O
by	O
the	O
value	B
network	O
and	O
by	O
rollouts	O
.	O
with	O
η	O
=	O
0	O
,	O
alphago	O
used	O
just	O
the	O
value	B
network	O
without	O
rollouts	O
,	O
and	O
with	O
η	O
=	O
1	O
,	O
evaluation	O
relied	O
just	O
on	O
rollouts	O
.	O
they	O
found	O
that	O
alphago	O
using	O
just	O
the	O
value	B
network	O
played	O
better	O
than	O
the	O
rollout-only	O
alphago	O
,	O
and	O
in	O
fact	O
played	O
better	O
than	O
the	O
strongest	O
of	O
all	O
other	O
go	O
programs	O
existing	O
at	O
the	O
time	O
.	O
the	O
best	O
play	O
resulted	O
from	O
setting	O
η	O
=	O
0.5	O
,	O
indicating	O
that	O
combining	O
the	O
value	B
network	O
with	O
rollouts	O
was	O
particularly	O
important	O
to	O
alphago	O
’	O
s	O
success	O
.	O
these	O
evaluation	O
methods	O
complemented	O
one	O
another	O
:	O
the	O
value	B
net-	O
work	O
evaluated	O
the	O
high-performance	O
rl	O
policy	B
that	O
was	O
too	O
slow	O
to	O
be	O
used	O
in	O
live	O
play	O
,	O
while	O
rollouts	O
using	O
the	O
weaker	O
but	O
much	O
faster	O
rollout	O
policy	O
were	O
able	O
to	O
add	O
precision	O
to	O
the	O
value	B
network	O
’	O
s	O
evaluations	O
for	O
speciﬁc	O
states	O
that	O
occurred	O
during	O
games	O
.	O
overall	O
,	O
alphago	O
’	O
s	O
remarkable	O
success	O
fueled	O
a	O
new	O
round	O
of	O
enthusiasm	O
for	O
the	O
promise	O
of	O
artiﬁcial	O
intelligence	O
,	O
speciﬁcally	O
for	O
systems	O
combining	O
reinforcement	B
learning	I
with	O
deep	O
anns	O
,	O
to	O
address	O
problems	O
in	O
other	O
challenging	O
domains	O
.	O
16.6.2	O
alphago	O
zero	O
building	O
upon	O
the	O
experience	O
with	O
alphago	O
,	O
a	O
deepmind	O
team	O
developed	O
alphago	O
zero	O
(	O
silver	O
et	O
al	O
.	O
2017a	O
)	O
.	O
in	O
contrast	O
to	O
alphago	O
,	O
this	O
program	O
used	O
no	O
human	O
data	O
or	O
guidance	O
beyond	O
the	O
basic	O
rules	O
of	O
the	O
game	O
(	O
hence	O
the	O
zero	O
in	O
its	O
name	O
)	O
.	O
it	O
learned	O
exclusively	O
from	O
self-play	O
reinforcement	B
learning	I
,	O
with	O
input	O
giving	O
just	O
“	O
raw	O
”	O
descrip-	O
tions	O
of	O
the	O
placements	O
of	O
stones	O
on	O
the	O
go	O
board	O
.	O
alphago	O
zero	O
implemented	O
a	O
form	O
of	O
policy	O
iteration	O
(	O
section	O
4.3	O
)	O
,	O
interleaving	O
policy	B
evaluation	I
with	O
policy	B
improvement	I
.	O
figure	O
16.7	O
is	O
an	O
overview	O
of	O
alphago	O
zero	O
’	O
s	O
algorithm	O
.	O
a	O
signiﬁcant	O
diﬀerence	O
be-	O
tween	O
alphago	O
zero	O
and	O
alphago	O
is	O
that	O
alphago	O
zero	O
used	O
mcts	O
to	O
select	O
moves	O
throughout	O
self-play	O
reinforcement	B
learning	I
,	O
whereas	O
alphago	O
used	O
mcts	O
for	O
live	O
play	O
after—but	O
not	O
during—learning	O
.	O
other	O
diﬀerences	O
besides	O
not	O
using	O
any	O
human	O
data	O
or	O
human-crafted	O
features	O
are	O
that	O
alphago	O
zero	O
used	O
only	O
one	O
deep	O
convolutional	O
ann	O
and	O
used	O
a	O
simpler	O
version	O
of	O
mcts	O
.	O
alphago	O
zero	O
’	O
s	O
mcts	O
was	O
simpler	O
than	O
the	O
version	O
used	O
by	O
alphago	O
in	O
that	O
it	O
did	O
not	O
include	O
rollouts	O
of	O
complete	O
games	O
,	O
and	O
therefore	O
did	O
not	O
need	O
a	O
rollout	O
policy	O
.	O
each	O
iteration	O
of	O
alphago	O
zero	O
’	O
s	O
mcts	O
ran	O
a	O
simulation	O
that	O
ended	O
at	O
a	O
leaf	O
node	O
of	O
the	O
current	O
search	O
tree	O
instead	O
of	O
at	O
the	O
terminal	O
position	O
of	O
a	O
complete	O
game	O
simulation	O
.	O
but	O
as	O
in	O
alphago	O
,	O
each	O
iteration	O
of	O
mcts	O
in	O
alphago	O
zero	O
was	O
guided	O
by	O
the	O
output	O
of	O
a	O
deep	O
convolutional	O
network	O
,	O
labeled	O
fθ	O
in	O
figure	O
16.7	O
,	O
were	O
θ	O
is	O
the	O
network	O
’	O
s	O
weight	O
vector	B
.	O
the	O
input	O
to	O
the	O
network	O
,	O
whose	O
architecture	O
we	O
describe	O
below	O
,	O
consisted	O
of	O
raw	O
representations	O
of	O
board	O
positions	O
,	O
and	O
its	O
output	O
had	O
two	O
parts	O
:	O
a	O
scalar	O
value	B
,	O
v	O
,	O
an	O
estimate	O
of	O
the	O
probability	O
that	O
the	O
current	O
player	O
will	O
win	O
from	O
from	O
the	O
current	O
board	O
position	O
,	O
and	O
a	O
vector	B
,	O
p	O
,	O
of	O
move	O
probabilities	O
,	O
one	O
for	O
each	O
possible	O
stone	O
placement	O
on	O
the	O
current	O
board	O
,	O
plus	O
the	O
pass	O
,	O
or	O
resign	O
,	O
move	O
.	O
452	O
chapter	O
16	O
:	O
applications	B
and	I
case	I
studies	I
figure	O
16.7	O
:	O
alphago	O
zero	O
self-play	O
reinforcement	B
learning	I
.	O
a	O
)	O
the	O
program	O
played	O
many	O
games	O
against	O
itself	O
,	O
one	O
shown	O
here	O
as	O
a	O
sequence	O
of	O
board	O
positions	O
si	O
,	O
i	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
t	O
,	O
with	O
moves	O
ai	O
,	O
i	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
t	O
,	O
and	O
winner	O
z.	O
each	O
move	O
ai	O
was	O
determined	O
by	O
action	B
probabilities	O
πi	O
returned	O
by	O
mcts	O
executed	O
from	O
root	O
node	O
si	O
and	O
guided	O
by	O
a	O
deep	O
convolutional	O
network	O
,	O
here	O
labeled	O
fθ	O
,	O
with	O
latest	O
weights	O
θ.	O
shown	O
here	O
for	O
just	O
one	O
position	O
s	O
but	O
repeated	O
for	O
all	O
si	O
,	O
the	O
network	O
’	O
s	O
inputs	O
were	O
raw	O
representations	O
of	O
board	O
positions	O
si	O
(	O
together	O
with	O
several	O
past	O
positions	O
,	O
though	O
not	O
shown	O
here	O
)	O
,	O
and	O
its	O
outputs	O
were	O
vectors	O
p	O
of	O
move	O
probabilities	O
that	O
guided	O
mcts	O
’	O
s	O
forward	O
searches	O
,	O
and	O
scalar	O
values	O
v	O
that	O
estimated	O
the	O
probability	O
of	O
the	O
current	O
player	O
winning	O
from	O
each	O
position	O
si	O
.	O
b	O
)	O
deep	O
convolutional	O
network	O
training	O
.	O
training	O
examples	O
were	O
randomly	O
sampled	O
steps	O
from	O
recent	O
self-play	O
games	O
.	O
weights	O
θ	O
were	O
updated	O
to	O
move	O
the	O
policy	B
vector	O
p	O
toward	O
the	O
probabilities	O
π	O
returned	O
by	O
mcts	O
,	O
and	O
to	O
include	O
the	O
winners	O
z	O
in	O
the	O
estimated	O
win	O
probability	O
v.	O
reprinted	O
from	O
draft	O
of	O
silver	O
et	O
al	O
.	O
(	O
2017a	O
)	O
with	O
permission	O
of	O
the	O
authors	O
and	O
deepmind	O
.	O
instead	O
of	O
selecting	O
self-play	O
actions	O
according	O
to	O
the	O
probabilities	O
p	O
,	O
however	O
,	O
al-	O
phago	O
zero	O
used	O
these	O
probabilities	O
,	O
together	O
with	O
the	O
network	O
’	O
s	O
value	B
output	O
,	O
to	O
direct	O
each	O
execution	O
of	O
mcts	O
,	O
which	O
returned	O
new	O
move	O
probabilities	O
,	O
shown	O
in	O
figure	O
16.7	O
as	O
the	O
policies	O
πi	O
.	O
these	O
policies	O
beneﬁtted	O
from	O
the	O
many	O
simulations	O
that	O
mcts	O
conducted	O
each	O
time	O
it	O
executed	O
.	O
the	O
result	O
was	O
that	O
the	O
policy	B
actually	O
followed	O
by	O
alphago	O
zero	O
was	O
an	O
improvement	O
over	O
the	O
policy	B
given	O
by	O
the	O
network	O
’	O
s	O
outputs	O
p.	O
silver	O
et	O
al	O
.	O
(	O
2017a	O
)	O
wrote	O
that	O
“	O
mcts	O
may	O
therefore	O
be	O
viewed	O
as	O
a	O
powerful	O
policy	B
improvement	I
operator.	O
”	O
here	O
is	O
more	O
detail	O
about	O
alphago	O
zero	O
’	O
s	O
ann	O
and	O
how	O
it	O
was	O
trained	O
.	O
the	O
network	O
took	O
as	O
input	O
a	O
19×	O
19×	O
17	O
image	O
stack	O
consisting	O
of	O
17	O
binary	O
feature	O
planes	O
.	O
the	O
ﬁrst	O
8	O
feature	O
planes	O
were	O
raw	O
representations	O
of	O
the	O
positions	O
of	O
the	O
current	O
player	O
’	O
s	O
stones	O
in	O
the	O
current	O
and	O
seven	O
past	O
board	O
conﬁgurations	O
:	O
a	O
feature	O
value	O
was	O
1	O
if	O
a	O
player	O
’	O
s	O
stone	O
was	O
on	O
the	O
corresponding	O
point	O
,	O
and	O
was	O
0	O
otherwise	O
.	O
the	O
next	O
8	O
feature	O
planes	O
figure1	O
:	O
self-playreinforcementlearninginalphagozero.atheprogramplaysagames1	O
,	O
...	O
,	O
stagainstitself.ineachpositionst	O
,	O
amonte-carlotreesearch	O
(	O
mcts	O
)	O
isexecuted	O
(	O
seefigure2	O
)	O
usingthelatestneuralnetworkf✓.movesareselectedaccordingtothesearchprobabil-itiescomputedbythemcts	O
,	O
at⇠⇡⇡⇡t.theterminalpositionstisscoredtocomputethegamewinnerz.bneuralnetworktraininginalphagozero.theneuralnetworktakestherawboardpositionsasitsinput	O
,	O
passesitthroughmanyconvolutionallayerswithparameters✓	O
,	O
andoutputsbothavectorp	O
,	O
representingaprobabilitydistributionovermoves	O
,	O
andascalarvaluev	O
,	O
represent-ingtheprobabilityofthecurrentplayerwinninginpositions.theneuralnetworkistrainedonrandomlysampledstepsfromrecentgamesofself-play	O
,	O
(	O
s	O
,	O
⇡⇡⇡	O
,	O
z	O
)	O
.theparameters✓areupdatedsoastomaximisethesimilarityofthepolicyvectorptothesearchprobabilities⇡⇡⇡	O
,	O
andtominimisetheerrorbetweenthepredictedwinnervandthegamewinnerz	O
(	O
seeequation1	O
)	O
.4	O
16.6.	O
mastering	O
the	O
game	O
of	O
go	O
453	O
similarly	O
coded	O
the	O
positions	O
of	O
the	O
opponent	O
’	O
s	O
stones	O
.	O
a	O
ﬁnal	O
input	O
feature	O
plane	O
had	O
a	O
constant	O
value	B
indicating	O
the	O
color	O
of	O
the	O
current	O
play	O
:	O
1	O
for	O
black	O
;	O
0	O
for	O
white	O
.	O
because	O
repetition	O
is	O
not	O
allowed	O
in	O
go	O
and	O
one	O
player	O
is	O
given	O
some	O
number	O
of	O
“	O
compensation	O
points	O
”	O
for	O
not	O
getting	O
the	O
ﬁrst	O
move	O
,	O
the	O
current	O
board	O
position	O
is	O
not	O
a	O
markov	O
state	B
of	O
go	O
.	O
this	O
is	O
why	O
features	O
describing	O
past	O
board	O
positions	O
and	O
the	O
color	O
feature	O
were	O
needed	O
.	O
the	O
network	O
was	O
“	O
two-headed	O
,	O
”	O
meaning	O
that	O
after	O
a	O
number	O
of	O
initial	O
layers	O
,	O
the	O
network	O
split	O
into	O
two	O
separate	O
“	O
heads	O
”	O
of	O
additional	O
layers	O
that	O
separately	O
fed	O
into	O
two	O
sets	O
of	O
output	O
units	O
.	O
in	O
this	O
case	O
,	O
one	O
head	O
fed	O
362	O
output	O
units	O
producing	O
192	O
+	O
1	O
move	O
probabilities	O
p	O
,	O
one	O
for	O
each	O
possible	O
stone	O
placement	O
plus	O
pass	O
;	O
the	O
other	O
head	O
fed	O
just	O
one	O
output	O
unit	O
producing	O
the	O
scalar	O
v	O
,	O
an	O
estimate	O
of	O
the	O
probability	O
that	O
the	O
current	O
player	O
will	O
win	O
from	O
the	O
current	O
board	O
position	O
.	O
the	O
network	O
before	O
the	O
split	O
consisted	O
of	O
41	O
convolutional	O
layers	O
,	O
each	O
followed	O
by	O
batch	O
normalization	O
,	O
and	O
with	O
skip	O
connections	O
added	O
to	O
implement	O
residual	O
learning	O
by	O
pairs	O
of	O
layers	O
(	O
see	O
section	O
9.7	O
)	O
.	O
overall	O
,	O
move	O
probabilities	O
and	O
values	O
were	O
computed	O
by	O
43	O
and	O
44	O
layers	O
respectively	O
.	O
starting	O
with	O
random	O
weights	O
,	O
the	O
network	O
was	O
trained	O
by	O
stochastic	O
gradient	O
descent	O
(	O
with	O
momentum	O
,	O
regularization	O
,	O
and	O
step-size	O
parameter	O
decreasing	O
as	O
training	O
contin-	O
ues	O
)	O
using	O
batches	O
of	O
examples	O
sampled	O
uniformly	O
at	O
random	O
from	O
all	O
the	O
steps	O
of	O
the	O
most	O
recent	O
500,000	O
games	O
of	O
self-play	O
with	O
the	O
current	O
best	O
policy	B
.	O
extra	O
noise	O
was	O
added	O
to	O
the	O
network	O
’	O
s	O
output	O
p	O
to	O
encourage	O
exploration	O
of	O
all	O
possible	O
moves	O
.	O
at	O
pe-	O
riodic	O
checkpoints	O
during	O
training	O
,	O
which	O
silver	O
et	O
al	O
.	O
(	O
2017a	O
)	O
chose	O
to	O
be	O
at	O
every	O
1,000	O
training	O
steps	O
,	O
the	O
policy	B
output	O
by	O
the	O
ann	O
with	O
the	O
latest	O
weights	O
was	O
evaluated	O
by	O
simulating	O
400	O
games	O
(	O
using	O
mcts	O
with	O
1,600	O
iterations	O
to	O
select	O
each	O
move	O
)	O
against	O
the	O
current	O
best	O
policy	B
.	O
if	O
the	O
new	O
policy	B
won	O
(	O
by	O
a	O
margin	O
set	O
to	O
reduce	O
noise	O
in	O
the	O
outcome	O
)	O
,	O
then	O
it	O
became	O
the	O
best	O
policy	B
to	O
be	O
used	O
in	O
subsequent	O
self-play	O
.	O
the	O
net-	O
work	O
’	O
s	O
weights	O
were	O
updated	O
to	O
make	O
the	O
network	O
’	O
s	O
policy	B
output	O
p	O
more	O
closely	O
match	O
the	O
policy	B
returned	O
by	O
mcts	O
,	O
and	O
to	O
make	O
its	O
value	B
output	O
,	O
v	O
,	O
more	O
closely	O
match	O
the	O
probability	O
that	O
the	O
current	O
best	O
policy	B
wins	O
from	O
the	O
board	O
position	O
represented	O
by	O
the	O
network	O
’	O
s	O
input	O
.	O
the	O
deepmind	O
team	O
trained	O
alphago	O
zero	O
over	O
4.9	O
million	O
games	O
of	O
self-play	O
,	O
which	O
took	O
about	O
3	O
days	O
.	O
each	O
move	O
of	O
each	O
game	O
was	O
selected	O
by	O
running	O
mcts	O
for	O
1,600	O
iterations	O
,	O
taking	O
approximately	O
0.4	O
second	O
per	O
move	O
.	O
network	O
weights	O
were	O
updated	O
over	O
700,000	O
batches	O
each	O
consisting	O
of	O
2,048	O
board	O
conﬁgurations	O
.	O
they	O
then	O
ran	O
tour-	O
naments	O
with	O
the	O
trained	O
alphago	O
zero	O
playing	O
against	O
the	O
version	O
of	O
alphago	O
that	O
defeated	O
fan	O
hui	O
by	O
5	O
games	O
to	O
0	O
,	O
and	O
against	O
the	O
version	O
that	O
defeated	O
lee	O
sedol	O
by	O
4	O
games	O
to	O
1.	O
they	O
used	O
the	O
elo	O
rating	O
system	O
to	O
evaluate	O
the	O
relative	O
performances	O
of	O
the	O
programs	O
.	O
the	O
diﬀerence	O
between	O
two	O
elo	O
ratings	O
is	O
meant	O
to	O
predict	O
the	O
outcome	O
of	O
games	O
between	O
the	O
players	O
.	O
the	O
elo	O
ratings	O
of	O
alphago	O
zero	O
,	O
the	O
version	O
of	O
alphago	O
that	O
played	O
against	O
fan	O
hui	O
,	O
and	O
the	O
version	O
that	O
played	O
against	O
lee	O
sedol	O
were	O
respec-	O
tively	O
4,308	O
,	O
3,144	O
,	O
and	O
3,739.	O
the	O
gaps	O
in	O
these	O
elo	O
ratings	O
translate	O
into	O
predictions	O
that	O
alphago	O
zero	O
would	O
defeat	O
these	O
other	O
programs	O
with	O
probabilities	O
very	O
close	O
to	O
one	O
.	O
in	O
a	O
match	O
of	O
100	O
games	O
between	O
alphago	O
zero	O
,	O
trained	O
as	O
described	O
,	O
and	O
the	O
exact	O
version	O
of	O
alphago	O
that	O
defeated	O
lee	O
sedol	O
held	O
under	O
the	O
same	O
conditions	O
that	O
were	O
used	O
in	O
that	O
match	O
,	O
alphago	O
zero	O
defeated	O
alphago	O
in	O
all	O
100	O
games	O
.	O
454	O
chapter	O
16	O
:	O
applications	B
and	I
case	I
studies	I
the	O
deepmind	O
team	O
also	O
compared	O
alphago	O
zero	O
with	O
a	O
program	O
using	O
an	O
ann	O
with	O
the	O
same	O
architecture	O
but	O
trained	O
by	O
supervised	B
learning	I
to	O
predict	O
human	O
moves	O
in	O
a	O
data	O
set	O
containing	O
nearly	O
30	O
million	O
positions	O
from	O
160,000	O
games	O
.	O
they	O
found	O
that	O
the	O
supervised-learning	O
player	O
initially	O
played	O
better	O
than	O
alphago	O
zero	O
,	O
and	O
was	O
better	O
at	O
predicting	O
human	O
expert	O
moves	O
,	O
but	O
played	O
less	O
well	O
after	O
alphago	O
zero	O
was	O
trained	O
for	O
a	O
day	O
.	O
this	O
suggested	O
that	O
alphago	O
zero	O
had	O
discovered	O
a	O
strategy	O
for	O
playing	O
that	O
was	O
diﬀerent	O
from	O
how	O
humans	O
play	O
.	O
in	O
fact	O
,	O
alphago	O
zero	O
discovered	O
,	O
and	O
came	O
to	O
prefer	O
,	O
some	O
novel	O
variations	O
of	O
classical	O
move	O
sequences	O
.	O
final	O
tests	O
of	O
alphago	O
zero	O
’	O
s	O
algorithm	O
were	O
conducted	O
with	O
a	O
version	O
having	O
a	O
larger	O
ann	O
and	O
trained	O
over	O
29	O
million	O
self-play	O
games	O
,	O
which	O
took	O
about	O
40	O
days	O
,	O
again	O
starting	O
with	O
random	O
weights	O
.	O
this	O
version	O
achieved	O
an	O
elo	O
rating	O
of	O
5,185.	O
the	O
team	O
pitted	O
this	O
version	O
of	O
alphago	O
zero	O
against	O
a	O
program	O
called	O
alphago	O
master	O
,	O
the	O
strongest	O
program	O
at	O
the	O
time	O
,	O
that	O
was	O
identical	O
to	O
alphago	O
zero	O
but	O
,	O
like	O
alphago	O
,	O
used	O
human	O
data	O
and	O
features	O
.	O
alphago	O
master	O
’	O
s	O
elo	O
rating	O
was	O
4,858	O
,	O
and	O
it	O
had	O
defeated	O
the	O
strongest	O
human	O
professional	O
players	O
60	O
to	O
0	O
in	O
online	O
games	O
.	O
in	O
a	O
100	O
game	O
match	O
,	O
alphago	O
zero	O
with	O
the	O
larger	O
network	O
and	O
more	O
extensive	O
learning	O
defeated	O
alphago	O
master	O
89	O
games	O
to	O
11	O
,	O
thus	O
providing	O
a	O
convincing	O
demonstration	O
of	O
the	O
problem-solving	O
power	O
of	O
alphago	O
zero	O
’	O
s	O
algorithm	O
.	O
alphago	O
zero	O
soundly	O
demonstrated	O
that	O
superhuman	O
performance	O
can	O
be	O
achieved	O
by	O
pure	O
reinforcement	B
learning	I
,	O
augmented	O
by	O
a	O
simple	O
version	O
of	O
mcts	O
,	O
and	O
deep	O
anns	O
with	O
very	O
minimal	O
knowledge	O
of	O
the	O
domain	O
and	O
no	O
reliance	O
on	O
human	O
data	O
or	O
guidance	O
.	O
we	O
will	O
surely	O
see	O
systems	O
inspired	O
by	O
the	O
deepmind	O
accomplishments	O
of	O
both	O
alphago	O
and	O
alphago	O
zero	O
applied	O
to	O
challenging	O
problems	O
in	O
other	O
domains	O
.	O
recently	O
,	O
yet	O
a	O
better	O
program	O
,	O
alphazero	O
,	O
was	O
described	O
by	O
silver	O
et	O
al	O
.	O
(	O
2017b	O
)	O
that	O
does	O
not	O
even	O
incorporate	O
knowledge	O
of	O
go	O
.	O
alphazero	O
is	O
a	O
general	O
reinforcement	O
learning	O
algorithm	O
that	O
improves	O
over	O
the	O
world	O
’	O
s	O
hitherto	O
best	O
programs	O
in	O
the	O
diverse	O
games	O
of	O
go	O
,	O
chess	B
,	O
and	O
shogi	O
.	O
16.7	O
personalized	O
web	O
services	O
personalizing	B
web	I
services	I
such	O
as	O
the	O
delivery	O
of	O
news	O
articles	O
or	O
advertisements	O
is	O
one	O
approach	O
to	O
increasing	O
users	O
’	O
satisfaction	O
with	O
a	O
website	O
or	O
to	O
increase	O
the	O
yield	O
of	O
a	O
marketing	O
campaign	O
.	O
a	O
policy	B
can	O
recommend	O
content	O
considered	O
to	O
be	O
the	O
best	O
for	O
each	O
particular	O
user	O
based	O
on	O
a	O
proﬁle	O
of	O
that	O
user	O
’	O
s	O
interests	O
and	O
preferences	O
inferred	O
from	O
their	O
history	B
of	I
online	O
activity	O
.	O
this	O
is	O
a	O
natural	O
domain	O
for	O
machine	O
learning	O
,	O
and	O
in	O
particular	O
,	O
for	O
reinforcement	O
learning	O
.	O
a	O
reinforcement	B
learning	I
system	O
can	O
improve	O
a	O
recommendation	O
policy	B
by	O
making	O
adjustments	O
in	O
response	O
to	O
user	O
feedback	O
.	O
one	O
way	O
to	O
obtain	O
user	O
feedback	O
is	O
by	O
means	O
of	O
website	O
satisfaction	O
surveys	O
,	O
but	O
for	O
acquiring	O
feedback	O
in	O
real	O
time	O
it	O
is	O
common	O
to	O
monitor	O
user	O
clicks	O
as	O
indicators	O
of	O
interest	O
in	O
a	O
link	O
.	O
a	O
method	O
long	O
used	O
in	O
marketing	O
called	O
a/b	O
testing	O
is	O
a	O
simple	O
type	O
of	O
reinforcement	O
learning	O
used	O
to	O
decide	O
which	O
of	O
two	O
versions	O
,	O
a	O
or	O
b	O
,	O
of	O
a	O
website	O
users	O
prefer	O
.	O
because	O
it	O
is	O
non-associative	O
,	O
like	O
a	O
two-armed	O
bandit	O
problem	O
,	O
this	O
approach	O
does	O
not	O
personalize	O
content	O
delivery	O
.	O
adding	O
context	O
consisting	O
of	O
features	O
describing	O
individual	O
users	O
and	O
16.7.	O
personalized	O
web	O
services	O
455	O
the	O
content	O
to	O
be	O
delivered	O
allows	O
personalizing	O
service	O
.	O
this	O
has	O
been	O
formalized	O
as	O
a	O
contextual	O
bandit	O
problem	O
(	O
or	O
an	O
associative	B
reinforcement	I
learning	I
problem	O
,	O
section	O
2.9	O
)	O
with	O
the	O
objective	O
of	O
maximizing	O
the	O
total	O
number	O
of	O
user	O
clicks	O
.	O
li	O
,	O
chu	O
,	O
langford	O
,	O
and	O
schapire	O
(	O
2010	O
)	O
applied	O
a	O
contextual	O
bandit	O
algorithm	O
to	O
the	O
problem	O
of	O
personalizing	O
the	O
yahoo	O
!	O
front	O
page	O
today	O
webpage	O
(	O
one	O
of	O
the	O
most	O
visited	O
pages	O
on	O
the	O
internet	O
at	O
the	O
time	O
of	O
their	O
research	O
)	O
by	O
selecting	O
the	O
news	O
story	O
to	O
feature	O
.	O
their	O
objective	O
was	O
to	O
maximize	O
the	O
click-through	O
rate	O
(	O
ctr	O
)	O
,	O
which	O
is	O
the	O
ratio	B
of	O
the	O
total	O
number	O
of	O
clicks	O
all	O
users	O
make	O
on	O
a	O
webpage	O
to	O
the	O
total	O
number	O
of	O
visits	O
to	O
the	O
page	O
.	O
their	O
contextual	O
bandit	O
algorithm	O
improved	O
over	O
a	O
standard	O
non-associative	O
bandit	B
algorithm	I
by	O
12.5	O
%	O
.	O
theocharous	O
,	O
thomas	O
,	O
and	O
ghavamzadeh	O
(	O
2015	O
)	O
argued	O
that	O
better	O
results	O
are	O
possi-	O
ble	O
by	O
formulating	O
personalized	O
recommendation	O
as	O
a	O
markov	O
decision	O
problem	O
(	O
mdp	O
)	O
with	O
the	O
objective	O
of	O
maximizing	O
the	O
total	O
number	O
of	O
clicks	O
users	O
make	O
over	O
repeated	O
visits	O
to	O
a	O
website	O
.	O
policies	O
derived	O
from	O
the	O
contextual	O
bandit	O
formulation	O
are	O
greedy	O
in	O
the	O
sense	O
that	O
they	O
do	O
not	O
take	O
long-term	O
eﬀects	O
of	O
actions	O
into	O
account	O
.	O
these	O
policies	O
eﬀectively	O
treat	O
each	O
visit	O
to	O
a	O
website	O
as	O
if	O
it	O
were	O
made	O
by	O
a	O
new	O
visitor	O
uniformly	O
sam-	O
pled	O
from	O
the	O
population	O
of	O
the	O
website	O
’	O
s	O
visitors	O
.	O
by	O
not	O
using	O
the	O
fact	O
that	O
many	O
users	O
repeatedly	O
visit	O
the	O
same	O
websites	O
,	O
greedy	O
policies	O
do	O
not	O
take	O
advantage	O
of	O
possibilities	O
provided	O
by	O
long-term	O
interactions	O
with	O
individual	O
users	O
.	O
as	O
an	O
example	O
of	O
how	O
a	O
marketing	O
strategy	O
might	O
take	O
advantage	O
of	O
long-term	O
user	O
interaction	O
,	O
theocharous	O
et	O
al	O
.	O
contrasted	O
a	O
greedy	O
policy	O
with	O
a	O
longer-term	O
policy	B
for	O
displaying	O
ads	O
for	O
buying	O
a	O
product	O
,	O
say	O
a	O
car	O
.	O
the	O
ad	O
displayed	O
by	O
the	O
greedy	O
policy	O
might	O
oﬀer	O
a	O
discount	O
if	O
the	O
user	O
buys	O
the	O
car	O
immediately	O
.	O
a	O
user	O
either	O
takes	O
the	O
oﬀer	O
or	O
leaves	O
the	O
website	O
,	O
and	O
if	O
they	O
ever	O
return	B
to	O
the	O
site	O
,	O
they	O
would	O
likely	O
see	O
the	O
same	O
oﬀer	O
.	O
a	O
longer-term	O
policy	B
,	O
on	O
the	O
other	O
hand	O
,	O
can	O
transition	O
the	O
user	O
“	O
down	O
a	O
sales	O
funnel	O
”	O
before	O
presenting	O
the	O
ﬁnal	O
deal	O
.	O
it	O
might	O
start	O
by	O
describing	O
the	O
availability	O
of	O
favorable	O
ﬁnancing	O
terms	O
,	O
then	O
praise	O
an	O
excellent	O
service	O
department	O
,	O
and	O
then	O
,	O
on	O
the	O
next	O
visit	O
,	O
oﬀer	O
the	O
ﬁnal	O
discount	O
.	O
this	O
type	O
of	O
policy	O
can	O
result	O
in	O
more	O
clicks	O
by	O
a	O
user	O
over	O
repeated	O
visits	O
to	O
the	O
site	O
,	O
and	O
if	O
the	O
policy	B
is	O
suitably	O
designed	O
,	O
more	O
eventual	O
sales	O
.	O
working	O
at	O
adobe	O
systems	O
incorporated	O
,	O
theocharous	O
et	O
al	O
.	O
conducted	O
experiments	O
to	O
see	O
if	O
policies	O
designed	O
to	O
maximize	O
clicks	O
over	O
the	O
long	O
term	O
could	O
in	O
fact	O
improve	O
over	O
short-term	O
greedy	O
policies	O
.	O
the	O
adobe	O
marketing	O
cloud	O
,	O
a	O
set	O
of	O
tools	O
that	O
many	O
com-	O
panies	O
use	O
to	O
run	O
digital	O
marketing	O
campaigns	O
,	O
provides	O
infrastructure	O
for	O
automating	O
user-targed	O
advertising	O
and	O
fund-raising	O
campaigns	O
.	O
actually	O
deploying	O
novel	O
policies	O
using	O
these	O
tools	O
entails	O
signiﬁcant	O
risk	O
because	O
a	O
new	O
policy	B
may	O
end	O
up	O
performing	O
poorly	O
.	O
for	O
this	O
reason	O
,	O
the	O
research	O
team	O
needed	O
to	O
assess	O
what	O
a	O
policy	B
’	O
s	O
performance	O
would	O
be	O
if	O
it	O
were	O
to	O
be	O
actually	O
deployed	O
,	O
but	O
to	O
do	O
so	O
on	O
the	O
basis	O
of	O
data	O
collected	O
under	O
the	O
execution	O
of	O
other	O
policies	O
.	O
a	O
critical	O
aspect	O
of	O
this	O
research	O
,	O
then	O
,	O
was	O
oﬀ-	O
policy	B
evaluation	I
.	O
further	O
,	O
the	O
team	O
wanted	O
to	O
do	O
this	O
with	O
high	O
conﬁdence	O
to	O
reduce	O
the	O
risk	O
of	O
deploying	O
a	O
new	O
policy	B
.	O
although	O
high	O
conﬁdence	O
oﬀ-policy	B
evaluation	O
was	O
a	O
central	O
component	O
of	O
this	O
research	O
(	O
see	O
also	O
thomas	O
,	O
2015	O
;	O
thomas	O
,	O
theocharous	O
,	O
and	O
ghavamzadeh	O
,	O
2015	O
)	O
,	O
here	O
we	O
focus	O
only	O
on	O
the	O
algorithms	O
and	O
their	O
results	O
.	O
theocharous	O
et	O
al	O
.	O
compared	O
the	O
results	O
of	O
two	O
algorithms	O
for	O
learning	O
ad	O
recommen-	O
dation	O
policies	O
.	O
the	O
ﬁrst	O
algorithm	O
,	O
which	O
they	O
called	O
greedy	O
optimization	O
,	O
had	O
the	O
goal	B
456	O
chapter	O
16	O
:	O
applications	B
and	I
case	I
studies	I
of	O
maximizing	O
only	O
the	O
probability	O
of	O
immediate	O
clicks	O
.	O
as	O
in	O
the	O
standard	O
contextual	O
bandit	O
formulation	O
,	O
this	O
algorithm	O
did	O
not	O
take	O
the	O
long-term	O
eﬀects	O
of	O
recommendations	O
into	O
account	O
.	O
the	O
other	O
algorithm	O
,	O
a	O
reinforcement	B
learning	I
algorithm	O
based	O
on	O
an	O
mdp	O
formulation	O
,	O
aimed	O
at	O
improving	O
the	O
number	O
of	O
clicks	O
users	O
made	O
over	O
multiple	O
visits	O
to	O
a	O
website	O
.	O
they	O
called	O
this	O
latter	O
algorithm	O
life-time	O
value	B
(	O
ltv	O
)	O
optimization	O
.	O
both	O
algorithms	O
faced	O
challenging	O
problems	O
because	O
the	O
reward	B
signal	I
in	O
this	O
domain	O
is	O
very	O
sparse	B
because	O
users	O
usually	O
do	O
not	O
click	O
on	O
ads	O
,	O
and	O
user	O
clicking	O
is	O
very	O
random	O
so	O
that	O
returns	O
have	O
high	O
variance	O
.	O
data	O
sets	O
from	O
the	O
banking	O
industry	O
were	O
used	O
for	O
training	O
and	O
testing	O
these	O
algo-	O
rithms	O
.	O
the	O
data	O
sets	O
consisted	O
of	O
many	O
complete	O
trajectories	O
of	O
customer	O
interaction	O
with	O
a	O
bank	O
’	O
s	O
website	O
that	O
showed	O
each	O
customer	O
one	O
out	O
of	O
a	O
collection	O
of	O
possible	O
oﬀers	O
.	O
if	O
a	O
customer	O
clicked	O
,	O
the	O
reward	O
was	O
1	O
,	O
and	O
otherwise	O
it	O
was	O
0.	O
one	O
data	O
set	O
contained	O
approximately	O
200,000	O
interactions	O
from	O
a	O
month	O
of	O
a	O
bank	O
’	O
s	O
campaign	O
that	O
randomly	O
oﬀered	O
one	O
of	O
7	O
oﬀers	O
.	O
the	O
other	O
data	O
set	O
from	O
another	O
bank	O
’	O
s	O
campaign	O
contained	O
4,000,000	O
interactions	O
involving	O
12	O
possible	O
oﬀers	O
.	O
all	O
interactions	O
included	O
customer	O
features	O
such	O
as	O
the	O
time	O
since	O
the	O
customer	O
’	O
s	O
last	O
visit	O
to	O
the	O
website	O
,	O
the	O
number	O
of	O
their	O
visits	O
so	O
far	O
,	O
the	O
last	O
time	O
the	O
customer	O
clicked	O
,	O
geographic	O
location	O
,	O
one	O
of	O
a	O
collection	O
of	O
interests	O
,	O
and	O
features	O
giving	O
demographic	O
information	O
.	O
greedy	O
optimization	O
was	O
based	O
on	O
a	O
mapping	O
estimating	O
the	O
probability	O
of	O
a	O
click	O
as	O
a	O
function	O
of	O
user	O
features	O
.	O
the	O
mapping	O
was	O
learned	O
via	O
supervised	B
learning	I
from	O
one	O
of	O
the	O
data	O
sets	O
by	O
means	O
of	O
a	O
random	O
forest	O
(	O
rf	O
)	O
algorithm	O
(	O
breiman	O
,	O
2001	O
)	O
.	O
rf	O
algorithms	O
have	O
been	O
widely	O
used	O
for	O
large-scale	O
applications	O
in	O
industry	O
because	O
they	O
are	O
eﬀective	O
predictive	O
tools	O
that	O
tend	O
not	O
to	O
overﬁt	O
and	O
are	O
relatively	O
insensitive	O
to	O
outliers	O
and	O
noise	O
.	O
theocharous	O
et	O
al	O
.	O
then	O
used	O
the	O
mapping	O
to	O
deﬁne	O
an	O
ε-greedy	O
policy	O
that	O
selected	O
with	O
probability	O
1-ε	O
the	O
oﬀer	O
predicted	O
by	O
the	O
rf	O
algorithm	O
to	O
have	O
the	O
highest	O
probability	O
of	O
producing	O
a	O
click	O
,	O
and	O
otherwise	O
selected	O
from	O
the	O
other	O
oﬀers	O
uniformly	O
at	O
random	O
.	O
ltv	O
optimization	O
used	O
a	O
batch-mode	O
reinforcement	B
learning	I
algorithm	O
called	O
ﬁtted	O
q	O
iteration	O
(	O
fqi	O
)	O
.	O
it	O
is	O
a	O
variant	O
of	O
ﬁtted	O
value	B
iteration	I
(	O
gordon	O
,	O
1999	O
)	O
adapted	O
to	O
q-learning	O
.	O
batch	O
mode	O
means	O
that	O
the	O
entire	O
data	O
set	O
for	O
learning	O
is	O
available	O
from	O
the	O
start	O
,	O
as	O
opposed	O
to	O
the	O
online	B
mode	O
of	O
the	O
algorithms	O
we	O
focus	O
on	O
in	O
this	O
book	O
in	O
which	O
data	O
are	O
acquired	O
sequentially	O
while	O
the	O
learning	O
algorithm	O
executes	O
.	O
batch-	O
mode	O
reinforcement	B
learning	I
algorithms	O
are	O
sometimes	O
necessary	O
when	O
online	B
learning	O
is	O
not	O
practical	O
,	O
and	O
they	O
can	O
use	O
any	O
batch-mode	O
supervised	B
learning	I
regression	O
algorithm	O
,	O
including	O
algorithms	O
known	O
to	O
scale	O
well	O
to	O
high-dimensional	O
spaces	O
.	O
the	O
convergence	O
of	O
fqi	O
depends	O
on	O
properties	O
of	O
the	O
function	B
approximation	I
algorithm	O
(	O
gordon	O
,	O
1999	O
)	O
.	O
for	O
their	O
application	O
to	O
ltv	O
optimization	O
,	O
theocharous	O
et	O
al	O
.	O
used	O
the	O
same	O
rf	O
algorithm	O
they	O
used	O
for	O
the	O
greedy	O
optimization	O
approach	O
.	O
because	O
in	O
this	O
case	O
fqi	O
convergence	O
is	O
not	O
monotonic	O
,	O
theocharous	O
et	O
al	O
.	O
kept	O
track	O
of	O
the	O
best	O
fqi	O
policy	B
by	O
oﬀ-policy	B
evaluation	O
using	O
a	O
validation	O
training	O
set	O
.	O
the	O
ﬁnal	O
policy	O
for	O
testing	O
the	O
ltv	O
approach	O
was	O
the	O
ε-greedy	O
policy	O
based	O
on	O
the	O
best	O
policy	B
produced	O
by	O
fqi	O
with	O
the	O
initial	O
action-	O
value	B
function	I
set	O
to	O
the	O
mapping	O
produced	O
by	O
the	O
rf	O
for	O
the	O
greedy	O
optimization	O
approach	O
.	O
to	O
measure	O
the	O
performance	O
of	O
the	O
policies	O
produced	O
by	O
the	O
greedy	O
and	O
ltv	O
ap-	O
16.7.	O
personalized	O
web	O
services	O
457	O
proaches	O
,	O
theocharous	O
et	O
al	O
.	O
used	O
the	O
ctr	O
metric	O
and	O
a	O
metric	O
they	O
called	O
the	O
ltv	O
metric	O
.	O
these	O
metrics	O
are	O
similar	O
,	O
except	O
that	O
the	O
ltv	O
metric	O
critically	O
distinguishes	O
between	O
individual	O
website	O
visitors	O
:	O
ctr	O
=	O
total	O
#	O
of	O
clicks	O
total	O
#	O
of	O
visits	O
,	O
ltv	O
=	O
total	O
#	O
of	O
clicks	O
total	O
#	O
of	O
visitors	O
.	O
figure	O
16.8	O
illustrates	O
how	O
these	O
metrics	O
diﬀer	O
.	O
each	O
circle	O
represents	O
a	O
user	O
visit	O
to	O
the	O
site	O
;	O
black	O
circles	O
are	O
visits	O
at	O
which	O
the	O
user	O
clicks	O
.	O
each	O
row	O
represents	O
visits	O
by	O
a	O
par-	O
ticular	O
user	O
.	O
by	O
not	O
distinguishing	O
between	O
visitors	O
,	O
the	O
ctr	O
for	O
these	O
sequences	O
is	O
0.35	O
,	O
whereas	O
the	O
ltv	O
is	O
1.5.	O
because	O
ltv	O
is	O
larger	O
than	O
ctr	O
to	O
the	O
extent	O
that	O
individual	O
users	O
revisit	O
the	O
site	O
,	O
it	O
is	O
an	O
indicator	O
of	O
how	O
successful	O
a	O
policy	B
is	O
in	O
encouraging	O
users	O
to	O
engage	O
in	O
extended	O
interactions	O
with	O
the	O
site	O
.	O
figure	O
16.8	O
:	O
click	O
through	O
rate	O
(	O
ctr	O
)	O
versus	O
life-time	O
value	B
(	O
ltv	O
)	O
.	O
each	O
circle	O
represents	O
a	O
user	O
visit	O
;	O
black	O
circles	O
are	O
visits	O
at	O
which	O
the	O
user	O
clicks	O
.	O
adapted	O
from	O
theocharous	O
et	O
al	O
.	O
(	O
2015	O
)	O
.	O
testing	O
the	O
policies	O
produced	O
by	O
the	O
greedy	O
and	O
ltv	O
approaches	O
was	O
done	O
using	O
a	O
high	O
conﬁdence	O
oﬀ-policy	B
evaluation	O
method	O
on	O
a	O
test	O
data	O
set	O
consisting	O
of	O
real-world	O
interactions	O
with	O
a	O
bank	O
website	O
served	O
by	O
a	O
random	O
policy	O
.	O
as	O
expected	O
,	O
results	O
showed	O
that	O
greedy	O
optimization	O
performed	O
best	O
as	O
measured	O
by	O
the	O
ctr	O
metric	O
,	O
while	O
ltv	O
optimization	O
performed	O
best	O
as	O
measured	O
by	O
the	O
ltv	O
metric	O
.	O
furthermore—although	O
we	O
have	O
omitted	O
its	O
details—the	O
high	O
conﬁdence	O
oﬀ-policy	B
evaluation	O
method	O
provided	O
probabilistic	O
guarantees	O
that	O
the	O
ltv	O
optimization	O
method	O
would	O
,	O
with	O
high	O
probabil-	O
ity	O
,	O
produce	O
policies	O
that	O
improve	O
upon	O
policies	O
currently	O
deployed	O
.	O
assured	O
by	O
these	O
probabilistic	O
guarantees	O
,	O
adobe	O
announced	O
in	O
2016	O
that	O
the	O
new	O
ltv	O
algorithm	O
would	O
be	O
a	O
standard	O
component	O
of	O
the	O
adobe	O
marketing	O
cloud	O
so	O
that	O
a	O
retailer	O
could	O
issue	O
a	O
sequence	O
of	O
oﬀers	O
following	O
a	O
policy	B
likely	O
to	O
yield	O
higher	O
return	B
than	O
a	O
policy	B
that	O
is	O
insensitive	O
to	O
long-term	O
results	O
.	O
458	O
chapter	O
16	O
:	O
applications	B
and	I
case	I
studies	I
16.8	O
thermal	O
soaring	O
birds	O
and	O
gliders	O
take	O
advantage	O
of	O
upward	O
air	O
currents—thermals—to	O
gain	O
altitude	O
in	O
order	O
to	O
maintain	O
ﬂight	O
while	O
expending	O
little	O
,	O
or	O
no	O
,	O
energy	O
.	O
thermal	O
soaring	O
,	O
as	O
this	O
behavior	O
is	O
called	O
,	O
is	O
a	O
complex	O
skill	O
requiring	O
responding	O
to	O
subtle	O
environmental	O
cues	O
to	O
increase	O
altitude	O
by	O
exploiting	O
a	O
rising	O
column	O
of	O
air	O
for	O
as	O
long	O
as	O
possible	O
.	O
reddy	O
,	O
celani	O
,	O
sejnowski	O
,	O
and	O
vergassola	O
(	O
2016	O
)	O
used	O
reinforcement	B
learning	I
to	O
inves-	O
tigate	O
thermal	O
soaring	O
policies	O
that	O
are	O
eﬀective	O
in	O
the	O
strong	O
atmospheric	O
turbulence	O
usually	O
accompanying	O
rising	O
air	O
currents	O
.	O
their	O
primary	O
goal	B
was	O
to	O
provide	O
insight	O
into	O
the	O
cues	O
birds	O
sense	O
and	O
how	O
they	O
use	O
them	O
to	O
achieve	O
their	O
impressive	O
thermal	O
soaring	O
performance	O
,	O
but	O
the	O
results	O
also	O
contribute	O
to	O
technology	O
relevant	O
to	O
autonomous	O
glid-	O
ers	O
.	O
reinforcement	B
learning	I
had	O
previously	O
been	O
applied	O
to	O
the	O
problem	O
of	O
navigating	O
eﬃciently	O
to	O
the	O
vicinity	O
of	O
a	O
thermal	O
updraft	O
(	O
woodbury	O
,	O
dunn	O
,	O
and	O
valasek	O
,	O
2014	O
)	O
but	O
not	O
to	O
the	O
more	O
challenging	O
problem	O
of	O
soaring	O
within	O
the	O
turbulence	O
of	O
the	O
updraft	O
itself	O
.	O
reddy	O
et	O
al	O
.	O
modeled	O
the	O
soaring	O
problem	O
as	O
a	O
continuing	O
mdp	O
with	O
discounting	O
.	O
the	O
agent	O
interacted	O
with	O
a	O
detailed	O
model	O
of	O
a	O
glider	O
ﬂying	O
in	O
turbulent	O
air	O
.	O
they	O
devoted	O
signiﬁcant	O
eﬀort	O
toward	O
making	O
the	O
model	O
generate	O
realistic	O
thermal	O
soaring	O
conditions	O
,	O
including	O
investigating	O
several	O
diﬀerent	O
approaches	O
to	O
atmospheric	O
modeling	O
.	O
for	O
the	O
learning	O
experiments	O
,	O
air	O
ﬂow	O
in	O
a	O
three-dimensional	O
box	O
with	O
one	O
kilometer	O
sides	O
,	O
one	O
of	O
which	O
was	O
at	O
ground	O
level	O
,	O
was	O
modeled	O
by	O
a	O
sophisticated	O
physics-based	O
set	O
of	O
partial	O
diﬀerential	B
equations	O
involving	O
air	O
velocity	O
,	O
temperature	O
,	O
and	O
pressure	O
.	O
in-	O
troducing	O
small	O
random	O
perturbations	O
into	O
the	O
numerical	O
simulation	O
caused	O
the	O
model	O
to	O
produce	O
analogs	O
of	O
thermal	O
updrafts	O
and	O
accompanying	O
turbulence	O
(	O
figure	O
16.9	O
left	O
)	O
glider	O
ﬂight	O
was	O
modeled	O
by	O
aerodynamic	O
equations	O
involving	O
velocity	O
,	O
lift	O
,	O
drag	O
,	O
and	O
figure	O
16.9	O
:	O
thermal	O
soaring	O
model	O
:	O
left	O
:	O
snapshot	O
of	O
the	O
vertical	O
velocity	O
ﬁeld	O
of	O
the	O
simulated	O
cube	O
of	O
air	O
:	O
in	O
red	O
(	O
blue	O
)	O
is	O
a	O
region	O
of	O
large	O
upward	O
(	O
downward	O
)	O
ﬂow	O
.	O
right	O
:	O
diagram	O
of	O
powerless	O
ﬂight	O
showing	O
bank	O
angle	O
µ	O
and	O
angle	O
of	O
attack	O
α.	O
adapted	O
with	O
permission	O
from	O
pnas	O
vol	O
.	O
113	O
(	O
22	O
)	O
,	O
p.	O
e4879	O
,	O
2016	O
,	O
reddy	O
,	O
celani	O
,	O
sejnowski	O
,	O
and	O
vergassola	O
,	O
learning	O
to	O
soar	O
in	O
turbulent	O
environments	O
.	O
contributesignificantlyandmoreexploratorystrategiesarepreferred.thesarsaalgorithmfindstheoptimalpolicybyestimatingforeverystate–actionpairitsqfunctiondefinedastheexpectedsumoffuturerewardsgiventhecurrentstatesandtheactiona.ateachstep	O
,	O
theqfunctionisupdatedasfollows	O
:	O
qðs	O
,	O
aþ→qðs	O
,	O
aþ+ηðr+βqðs′	O
,	O
a′þ−qðs	O
,	O
aþþ	O
,	O
[	O
5	O
]	O
whereristhereceivedrewardandηisthelearningrate.theupdateismadeonlineanddoesnotrequireanypriormodeloftheflowortheflight.thisfeatureisparticularlyrelevantinmodelingdecision-makingprocessesinanimals.whenthealgo-rithmisclosetoconvergence	O
,	O
theqfunctionapproachesthesolutiontobellman	O
’	O
sdynamicprogrammingequations	O
(	O
12	O
)	O
.thepolicyπas	O
,	O
whichencodestheprobabilityofchoosingactionaatstates	O
,	O
approachestheoptimaloneπpandisobtainedfromtheqfunctionviaaboltzmann-likeexpression	O
:	O
πas∝exp −^qðs	O
,	O
aþτtemp	O
,	O
[	O
6	O
]	O
^qðs	O
,	O
aþ=maxa′qðs	O
,	O
a′þ−qðs	O
,	O
aþmaxa′qðs	O
,	O
a′þ−mina′qðs	O
,	O
a′þ	O
.	O
[	O
7	O
]	O
here	O
,	O
τtempisaneffective	O
“	O
temperature	O
”	O
:	O
whenτtemp	O
1	O
,	O
ac-tionsareonlyweaklydependentontheassociatedqfunction	O
;	O
conversely	O
,	O
forτtempsmall	O
,	O
thepolicygreedilychoosestheactionwiththelargestq.thetemperatureparameterisinitiallychosenlargeandloweredastrainingprogressestocreateanannealingeffect	O
,	O
therebypreventingthepolicyfromgettingstuckinlocalextrema.parametersusedinoursimulationscanbefoundintables1.inthesequel	O
,	O
weshallqualifythepolicyidentifiedbysarsaasoptimal.itshouldbeunderstood	O
,	O
however	O
,	O
thatthesarsaalgorithm	O
(	O
asotherreinforcementlearningalgorithms	O
)	O
typicallyidentifiesanapproximatelyoptimalpolicyand	O
“	O
approximately	O
”	O
isskippedonlyforthesakeofconciseness.resultssensorimotorcuesandrewardfunctionforeffectivelearning.keyaspectsofthelearningforthesoaringproblemarethesensori-motorcuesthattheglidercansense	O
(	O
statespace	O
)	O
andthechoiceoftherewardusedtotraintheglidertoascendquickly.asthestateandactionspacesarecontinuousandhigh-dimensional	O
,	O
itisnecessarytodiscretizethem	O
,	O
whichwerealizeherebyastandardlookuptablerepresentation.theheightascendedpertrial	O
,	O
averagedoverdifferentrealizationsoftheflow	O
,	O
servesasourperformancecriterion.thegliderisallowedcontroloveritsangleofattackanditsbankangle	O
(	O
fig.1b	O
)	O
.controlovertheangleofattackfeaturestworegimes	O
:	O
(	O
i	O
)	O
atsmallanglesofattack	O
,	O
thehorizontalspeedislargeandtheclimbrateissmall	O
(	O
theglidersinksquickly	O
)	O
;	O
(	O
ii	O
)	O
atlargeanglesofattackbutbelowthestallangle	O
,	O
thehorizontalspeedissmall	O
,	O
whereastheclimbrateislarge.thebankanglecontrolstheheadingoftheglider	O
,	O
andweallowforarangeofvariationbetween−15°and15°.exploringvariouspossibilities	O
,	O
wefoundthatthreeactionsareminimallysufficient	O
:	O
increasing	O
,	O
decreasing	O
,	O
orpreservingtheangleofattackandthebankangle.theangleofattackandbankanglewereincremented/decre-mentedinstepsof2.5°and5°	O
,	O
respectively.insummary	O
,	O
theglidercanchoose32possibleactionstocontrolitsnavigationinresponsetothesensorimotorcuesdescribedhereafter.ourrationaleinthechoiceofthestatespacewastryingtominimizebiologicalorelectronicsensorydevicesnecessaryforcontrol.wetesteddifferentcombinationsoflocalsensorimotoraczylift	O
lzxlift	O
ldrag	O
dvelocity	O
directionwing	O
directionbank	O
angleglide	O
angleangle	O
of	O
attackbdfig.1.snapshotsoftheverticalvelocity	O
(	O
a	O
)	O
andthetemperaturefields	O
(	O
b	O
)	O
inournumericalsimulationsof3drayleigh–bénardconvection.fortheverticalvelocityfield	O
,	O
theredandbluecolorsindicateregionsoflargeupwardanddownwardflow	O
,	O
respectively.forthetemperaturefield	O
,	O
theredandbluecolorsindicateregionsofhighandlowtemperature	O
,	O
respectively.noticethatthehotandcoldregionsdrivetheupwardanddownwardbranchesoftheconvectivecell	O
,	O
inagreementwiththebasicphysicsofconvection.	O
(	O
c	O
)	O
theforce-bodydiagramofflightwithnothrust	O
,	O
thatis	O
,	O
withoutanyengineorflappingofwings.thefigurealsoshowsthebankangleμ	O
(	O
blue	O
)	O
,	O
theangleofattackα	O
(	O
green	O
)	O
,	O
andtheglideangleγ	O
(	O
red	O
)	O
.	O
(	O
d	O
)	O
therangeofhorizontalspeedsandclimbratesaccessiblebycontrollingtheangleofattack.atsmallanglesofattack	O
,	O
theglidermovesfastbutalsosinksfast	O
,	O
whereasatlargerangles	O
,	O
theglidermovesandsinksmoreslowly.iftheangleofattackistoohigh	O
,	O
atabout16°	O
,	O
thegliderstalls	O
,	O
leadingtoasuddendropinlift.theverticalblackdashedlineshowsthefixedangleofattackformostofthesimulations	O
(	O
results	O
,	O
controlovertheangleofattack	O
)	O
.reddyetal.pnas|publishedonlineaugust1,2016|e4879neurosciencephysicspnaspluscontributesignificantlyandmoreexploratorystrategiesarepreferred.thesarsaalgorithmfindstheoptimalpolicybyestimatingforeverystate–actionpairitsqfunctiondefinedastheexpectedsumoffuturerewardsgiventhecurrentstatesandtheactiona.ateachstep	O
,	O
theqfunctionisupdatedasfollows	O
:	O
qðs	O
,	O
aþ→qðs	O
,	O
aþ+ηðr+βqðs′	O
,	O
a′þ−qðs	O
,	O
aþþ	O
,	O
[	O
5	O
]	O
whereristhereceivedrewardandηisthelearningrate.theupdateismadeonlineanddoesnotrequireanypriormodeloftheflowortheflight.thisfeatureisparticularlyrelevantinmodelingdecision-makingprocessesinanimals.whenthealgo-rithmisclosetoconvergence	O
,	O
theqfunctionapproachesthesolutiontobellman	O
’	O
sdynamicprogrammingequations	O
(	O
12	O
)	O
.thepolicyπas	O
,	O
whichencodestheprobabilityofchoosingactionaatstates	O
,	O
approachestheoptimaloneπpandisobtainedfromtheqfunctionviaaboltzmann-likeexpression	O
:	O
πas∝exp −^qðs	O
,	O
aþτtemp	O
,	O
[	O
6	O
]	O
^qðs	O
,	O
aþ=maxa′qðs	O
,	O
a′þ−qðs	O
,	O
aþmaxa′qðs	O
,	O
a′þ−mina′qðs	O
,	O
a′þ	O
.	O
[	O
7	O
]	O
here	O
,	O
τtempisaneffective	O
“	O
temperature	O
”	O
:	O
whenτtemp	O
1	O
,	O
ac-tionsareonlyweaklydependentontheassociatedqfunction	O
;	O
conversely	O
,	O
forτtempsmall	O
,	O
thepolicygreedilychoosestheactionwiththelargestq.thetemperatureparameterisinitiallychosenlargeandloweredastrainingprogressestocreateanannealingeffect	O
,	O
therebypreventingthepolicyfromgettingstuckinlocalextrema.parametersusedinoursimulationscanbefoundintables1.inthesequel	O
,	O
weshallqualifythepolicyidentifiedbysarsaasoptimal.itshouldbeunderstood	O
,	O
however	O
,	O
thatthesarsaalgorithm	O
(	O
asotherreinforcementlearningalgorithms	O
)	O
typicallyidentifiesanapproximatelyoptimalpolicyand	O
“	O
approximately	O
”	O
isskippedonlyforthesakeofconciseness.resultssensorimotorcuesandrewardfunctionforeffectivelearning.keyaspectsofthelearningforthesoaringproblemarethesensori-motorcuesthattheglidercansense	O
(	O
statespace	O
)	O
andthechoiceoftherewardusedtotraintheglidertoascendquickly.asthestateandactionspacesarecontinuousandhigh-dimensional	O
,	O
itisnecessarytodiscretizethem	O
,	O
whichwerealizeherebyastandardlookuptablerepresentation.theheightascendedpertrial	O
,	O
averagedoverdifferentrealizationsoftheflow	O
,	O
servesasourperformancecriterion.thegliderisallowedcontroloveritsangleofattackanditsbankangle	O
(	O
fig.1b	O
)	O
.controlovertheangleofattackfeaturestworegimes	O
:	O
(	O
i	O
)	O
atsmallanglesofattack	O
,	O
thehorizontalspeedislargeandtheclimbrateissmall	O
(	O
theglidersinksquickly	O
)	O
;	O
(	O
ii	O
)	O
atlargeanglesofattackbutbelowthestallangle	O
,	O
thehorizontalspeedissmall	O
,	O
whereastheclimbrateislarge.thebankanglecontrolstheheadingoftheglider	O
,	O
andweallowforarangeofvariationbetween−15°and15°.exploringvariouspossibilities	O
,	O
wefoundthatthreeactionsareminimallysufficient	O
:	O
increasing	O
,	O
decreasing	O
,	O
orpreservingtheangleofattackandthebankangle.theangleofattackandbankanglewereincremented/decre-mentedinstepsof2.5°and5°	O
,	O
respectively.insummary	O
,	O
theglidercanchoose32possibleactionstocontrolitsnavigationinresponsetothesensorimotorcuesdescribedhereafter.ourrationaleinthechoiceofthestatespacewastryingtominimizebiologicalorelectronicsensorydevicesnecessaryforcontrol.wetesteddifferentcombinationsoflocalsensorimotoraczylift	O
lzxlift	O
ldrag	O
dvelocity	O
directionwing	O
directionbank	O
angleglide	O
angleangle	O
of	O
attackbdfig.1.snapshotsoftheverticalvelocity	O
(	O
a	O
)	O
andthetemperaturefields	O
(	O
b	O
)	O
inournumericalsimulationsof3drayleigh–bénardconvection.fortheverticalvelocityfield	O
,	O
theredandbluecolorsindicateregionsoflargeupwardanddownwardflow	O
,	O
respectively.forthetemperaturefield	O
,	O
theredandbluecolorsindicateregionsofhighandlowtemperature	O
,	O
respectively.noticethatthehotandcoldregionsdrivetheupwardanddownwardbranchesoftheconvectivecell	O
,	O
inagreementwiththebasicphysicsofconvection.	O
(	O
c	O
)	O
theforce-bodydiagramofflightwithnothrust	O
,	O
thatis	O
,	O
withoutanyengineorflappingofwings.thefigurealsoshowsthebankangleμ	O
(	O
blue	O
)	O
,	O
theangleofattackα	O
(	O
green	O
)	O
,	O
andtheglideangleγ	O
(	O
red	O
)	O
.	O
(	O
d	O
)	O
therangeofhorizontalspeedsandclimbratesaccessiblebycontrollingtheangleofattack.atsmallanglesofattack	O
,	O
theglidermovesfastbutalsosinksfast	O
,	O
whereasatlargerangles	O
,	O
theglidermovesandsinksmoreslowly.iftheangleofattackistoohigh	O
,	O
atabout16°	O
,	O
thegliderstalls	O
,	O
leadingtoasuddendropinlift.theverticalblackdashedlineshowsthefixedangleofattackformostofthesimulations	O
(	O
results	O
,	O
controlovertheangleofattack	O
)	O
.reddyetal.pnas|publishedonlineaugust1,2016|e4879neurosciencephysicspnasplus↵	O
16.8.	O
thermal	O
soaring	O
459	O
other	O
factors	O
governing	O
powerless	O
ﬂight	O
of	O
a	O
ﬁxed-wing	O
aircraft	O
.	O
maneuvering	O
the	O
glider	O
involved	O
changing	O
its	O
angle	O
of	O
attack	O
(	O
the	O
angle	O
between	O
the	O
glider	O
’	O
s	O
wing	O
and	O
the	O
direc-	O
tion	B
of	O
air	O
ﬂow	O
)	O
and	O
its	O
bank	O
angle	O
(	O
figure	O
16.9	O
right	O
)	O
.	O
the	O
interface	O
between	O
the	O
agent	O
and	O
the	O
environment	O
required	O
deﬁning	O
the	O
agent	O
’	O
s	O
actions	O
,	O
the	O
state	B
information	O
the	O
agent	O
receives	O
from	O
the	O
environment	B
,	O
and	O
the	O
reward	O
signal	O
.	O
by	O
experimenting	O
with	O
various	O
possibilities	O
,	O
reddy	O
et	O
al	O
.	O
decided	O
that	O
three	O
actions	O
each	O
for	O
the	O
angle	O
of	O
attack	O
and	O
the	O
bank	O
angle	O
were	O
enough	O
for	O
their	O
purposes	O
:	O
increment	O
or	O
decrement	O
the	O
current	O
bank	O
angle	O
and	O
angle	O
of	O
attack	O
by	O
5◦	O
and	O
2.5◦	O
,	O
respectively	O
,	O
or	O
leave	O
them	O
unchanged	O
.	O
this	O
resulted	O
in	O
32	O
possible	O
actions	O
.	O
the	O
bank	O
angle	O
was	O
bounded	O
to	O
remain	O
between	O
−15◦	O
and	O
+15◦	O
.	O
because	O
a	O
goal	B
of	O
their	O
study	O
was	O
to	O
try	O
to	O
determine	O
what	O
minimal	O
set	O
of	O
sensory	O
cues	O
are	O
necessary	O
for	O
eﬀective	O
soaring	O
,	O
both	O
to	O
shed	O
light	O
on	O
the	O
cues	O
birds	O
might	O
use	O
for	O
soaring	O
and	O
to	O
minimize	O
the	O
sensing	O
complexity	O
required	O
for	O
automated	O
glider	O
soaring	O
,	O
the	O
authors	O
tried	O
various	O
sets	O
of	O
signals	O
as	O
input	O
to	O
the	O
reinforcement	B
learning	I
agent	O
.	O
they	O
started	O
by	O
using	O
state	B
aggregation	I
(	O
section	O
9.3	O
)	O
of	O
a	O
four-dimensional	O
state	B
space	O
with	O
dimensions	O
giving	O
local	O
vertical	O
wind	O
speed	O
,	O
local	O
vertical	O
wind	O
acceleration	O
,	O
torque	O
depending	O
on	O
the	O
diﬀerence	O
between	O
the	O
vertical	O
wind	O
velocities	O
at	O
the	O
left	O
and	O
right	O
wing	O
tips	O
,	O
and	O
the	O
local	O
temperature	O
.	O
each	O
dimension	O
was	O
discretized	O
into	O
three	O
bins	O
:	O
positive	O
high	O
,	O
negative	O
high	O
,	O
and	O
small	O
.	O
results	O
,	O
described	O
below	O
,	O
showed	O
that	O
only	O
two	O
of	O
these	O
dimensions	O
were	O
critical	O
for	O
eﬀective	O
soaring	O
behavior	O
.	O
the	O
overall	O
objective	O
of	O
thermal	O
soaring	O
is	O
to	O
gain	O
as	O
much	O
altitude	O
as	O
possible	O
from	O
each	O
rising	O
column	O
of	O
air	O
.	O
reddy	O
et	O
al	O
.	O
tried	O
a	O
straightforward	O
reward	B
signal	I
that	O
rewarded	O
the	O
agent	O
at	O
the	O
end	O
of	O
each	O
episode	O
based	O
on	O
the	O
altitude	O
gained	O
over	O
the	O
episode	O
,	O
a	O
large	O
negative	O
reward	B
signal	I
if	O
the	O
glider	O
touched	O
the	O
ground	O
,	O
and	O
zero	O
otherwise	O
.	O
they	O
found	O
that	O
learning	O
was	O
not	O
successful	O
with	O
this	O
reward	B
signal	I
for	O
episodes	B
of	O
realistic	O
duration	O
and	O
that	O
eligibility	B
traces	I
did	O
not	O
help	O
.	O
by	O
experimenting	O
with	O
various	O
reward	O
signals	O
,	O
they	O
found	O
that	O
learning	O
was	O
best	O
with	O
a	O
reward	B
signal	I
that	O
at	O
each	O
time	O
step	O
linearly	O
combined	O
the	O
vertical	O
wind	O
velocity	O
and	O
vertical	O
wind	O
acceleration	O
observed	O
on	O
the	O
previous	O
time	O
step	O
.	O
learning	O
was	O
by	O
one-step	O
sarsa	O
,	O
with	O
actions	O
selected	O
according	O
to	O
a	O
soft-max	B
dis-	O
tribution	O
based	O
on	O
normalized	O
action	B
values	O
.	O
speciﬁcally	O
,	O
the	O
action	B
probabilities	O
were	O
computed	O
according	O
to	O
(	O
13.2	O
)	O
with	O
action	O
preferences	O
:	O
h	O
(	O
s	O
,	O
a	O
,	O
θ	O
)	O
=	O
ˆq	O
(	O
s	O
,	O
a	O
,	O
θ	O
)	O
−	O
minb	O
ˆq	O
(	O
s	O
,	O
b	O
,	O
θ	O
)	O
τ	O
(	O
cid:0	O
)	O
maxb	O
ˆq	O
(	O
s	O
,	O
b	O
,	O
θ	O
)	O
−	O
minb	O
ˆq	O
(	O
s	O
,	O
b	O
,	O
θ	O
)	O
(	O
cid:1	O
)	O
,	O
where	O
θ	O
is	O
a	O
parameter	O
vector	O
with	O
one	O
component	O
for	O
each	O
action	B
and	O
aggregated	O
group	O
of	O
states	O
,	O
and	O
ˆq	O
(	O
s	O
,	O
a	O
,	O
θ	O
)	O
merely	O
returned	O
the	O
component	O
corresponding	O
to	O
s	O
,	O
a	O
in	O
the	O
usual	O
way	O
for	O
state	O
aggregation	O
methods	O
.	O
the	O
above	O
equation	O
forms	O
the	O
action	B
preferences	I
by	O
normalizing	O
the	O
approximate	O
action	O
values	O
to	O
the	O
interval	O
[	O
0	O
,	O
1	O
]	O
then	O
dividing	O
by	O
τ	O
,	O
a	O
positive	O
“	O
temperature	O
parameter.	O
”	O
3	O
as	O
τ	O
increases	O
,	O
the	O
probability	O
of	O
selecting	O
an	O
action	B
becomes	O
less	O
dependent	O
on	O
its	O
preference	O
;	O
as	O
τ	O
decreases	O
toward	O
zero	O
,	O
the	O
probability	O
of	O
selecting	O
the	O
most	O
highly-preferred	O
action	B
approaches	O
one	O
,	O
making	O
the	O
policy	B
approach	O
the	O
greedy	O
policy	O
.	O
the	O
temperature	O
parameter	O
τ	O
was	O
initialized	O
to	O
2.0	O
and	O
incrementally	O
3reddy	O
et	O
al	O
.	O
described	O
this	O
slightly	O
diﬀerently	O
,	O
but	O
our	O
version	O
is	O
equivalent	O
to	O
theirs	O
.	O
460	O
chapter	O
16	O
:	O
applications	B
and	I
case	I
studies	I
decreased	O
to	O
0.2	O
during	O
learning	O
.	O
action	B
preferences	I
were	O
computed	O
from	O
the	O
current	O
estimates	O
of	O
the	O
action	B
values	O
:	O
the	O
action	B
with	O
the	O
maximum	O
estimated	O
action	B
value	O
was	O
given	O
preference	O
1/τ	O
,	O
the	O
action	B
with	O
the	O
minimum	O
estimated	O
action	B
value	O
was	O
given	O
preference	O
0	O
,	O
and	O
the	O
preferences	O
of	O
the	O
other	O
actions	O
were	O
scaled	O
between	O
these	O
extremes	O
.	O
the	O
step-size	O
and	O
discount-rate	O
parameters	O
were	O
ﬁxed	O
at	O
0.1	O
and	O
0.98	O
respectively	O
.	O
each	O
learning	O
episode	O
took	O
place	O
with	O
the	O
agent	O
controlling	O
simulated	O
ﬂight	O
in	O
an	O
independently	O
generated	O
period	O
of	O
simulated	O
turbulent	O
air	O
currents	O
.	O
each	O
episode	O
lasted	O
2.5	O
minutes	O
simulated	O
with	O
a	O
1	O
second	O
time	O
step	O
.	O
learning	O
eﬀectively	O
converged	O
after	O
a	O
few	O
hundred	O
episodes	B
.	O
the	O
left	O
panel	O
of	O
figure	O
16.10	O
shows	O
a	O
sample	O
trajectory	O
before	O
learning	O
where	O
the	O
agent	O
selects	O
actions	O
randomly	O
.	O
starting	O
at	O
the	O
top	O
of	O
the	O
volume	O
shown	O
,	O
the	O
glider	O
’	O
s	O
trajectory	O
is	O
in	O
the	O
direction	O
indicated	O
by	O
the	O
arrow	O
and	O
quickly	O
loses	O
altitude	O
.	O
figure	O
16.10	O
’	O
s	O
right	O
panel	O
is	O
a	O
trajectory	O
after	O
learning	O
.	O
the	O
glider	O
starts	O
at	O
the	O
same	O
place	O
(	O
here	O
appearing	O
at	O
the	O
bottom	O
of	O
the	O
volume	O
)	O
and	O
gains	O
altitude	O
by	O
spiraling	O
within	O
the	O
rising	O
column	O
of	O
air	O
.	O
although	O
reddy	O
at	O
al	O
.	O
found	O
that	O
performance	O
varied	O
widely	O
over	O
diﬀerent	O
simulated	O
periods	O
of	O
air	O
ﬂow	O
,	O
the	O
number	O
of	O
times	O
the	O
glider	O
touched	O
the	O
ground	O
consistently	O
decreased	O
to	O
nearly	O
zero	O
as	O
learning	O
progressed	O
.	O
after	O
experimenting	O
with	O
diﬀerent	O
sets	O
of	O
features	O
available	O
to	O
the	O
learning	O
agent	O
,	O
it	O
turned	O
out	O
that	O
the	O
combination	O
of	O
just	O
vertical	O
wind	O
acceleration	O
and	O
torques	O
worked	O
best	O
.	O
the	O
authors	O
conjectured	O
that	O
because	O
these	O
features	O
give	O
information	O
about	O
the	O
gradient	B
of	O
vertical	O
wind	O
velocity	O
in	O
two	O
diﬀerent	O
directions	O
,	O
they	O
allow	O
the	O
controller	O
to	O
select	O
between	O
turning	O
by	O
changing	O
the	O
bank	O
angle	O
or	O
continuing	O
along	O
the	O
same	O
course	O
by	O
leaving	O
the	O
bank	O
angle	O
alone	O
.	O
this	O
allows	O
the	O
glider	O
to	O
stay	O
within	O
a	O
rising	O
column	O
of	O
figure	O
16.10	O
:	O
sample	O
thermal	O
soaring	O
trajectories	O
,	O
with	O
arrows	O
showing	O
the	O
direction	O
of	O
ﬂight	O
from	O
the	O
same	O
starting	O
point	O
(	O
note	O
that	O
the	O
altitude	O
scales	O
are	O
shifted	O
)	O
.	O
left	O
:	O
before	O
learning	O
:	O
the	O
agent	O
selects	O
actions	O
randomly	O
and	O
the	O
glider	O
descends	O
.	O
right	O
:	O
after	O
learning	O
:	O
the	O
glider	O
gains	O
altitude	O
by	O
following	O
a	O
spiral	O
trajectory	O
.	O
adapted	O
with	O
permission	O
from	O
pnas	O
vol	O
.	O
113	O
(	O
22	O
)	O
,	O
p.	O
e4879	O
,	O
2016	O
,	O
reddy	O
,	O
celani	O
,	O
sejnowski	O
,	O
and	O
vergassola	O
,	O
learning	O
to	O
soar	O
in	O
turbulent	O
environments	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
16.8.	O
thermal	O
soaring	O
461	O
air	O
.	O
vertical	O
wind	O
velocity	O
is	O
indicative	O
of	O
the	O
strength	O
of	O
the	O
thermal	O
but	O
does	O
not	O
help	O
in	O
staying	O
within	O
the	O
ﬂow	O
.	O
they	O
found	O
that	O
sensitivity	O
to	O
temperature	O
was	O
of	O
little	O
help	O
.	O
they	O
also	O
found	O
that	O
controlling	O
the	O
angle	O
of	O
attack	O
is	O
not	O
helpful	O
in	O
staying	O
within	O
a	O
particular	O
thermal	O
,	O
being	O
useful	O
instead	O
for	O
traveling	O
between	O
thermals	O
when	O
covering	O
large	O
distances	O
,	O
as	O
in	O
cross-country	O
gliding	O
and	O
bird	O
migration	O
.	O
due	O
to	O
the	O
fact	O
that	O
soaring	O
in	O
diﬀerent	O
levels	O
of	O
turbulence	O
requires	O
diﬀerent	O
policies	O
,	O
training	O
was	O
done	O
in	O
conditions	O
ranging	O
from	O
weak	O
to	O
strong	O
turbulence	O
.	O
in	O
strong	O
turbu-	O
lence	O
the	O
rapidly	O
changing	O
wind	O
and	O
glider	O
velocities	O
allowed	O
less	O
time	O
for	O
the	O
controller	O
to	O
react	O
.	O
this	O
reduced	O
the	O
amount	O
of	O
control	O
possible	O
compared	O
to	O
what	O
was	O
possible	O
for	O
maneuvering	O
when	O
ﬂuctuations	O
were	O
weak	O
.	O
reddy	O
at	O
al	O
.	O
examined	O
the	O
policies	O
sarsa	O
learned	O
under	O
these	O
diﬀerent	O
conditions	O
.	O
common	O
to	O
policies	O
learned	O
in	O
all	O
regimes	O
were	O
these	O
features	O
:	O
when	O
sensing	O
negative	O
wind	O
acceleration	O
,	O
bank	O
sharply	O
in	O
the	O
direction	O
of	O
the	O
wing	O
with	O
the	O
higher	O
lift	O
;	O
when	O
sensing	O
large	O
positive	O
wind	O
acceleration	O
and	O
no	O
torque	O
,	O
do	O
nothing	O
.	O
however	O
,	O
diﬀerent	O
levels	O
of	O
turbulence	O
led	O
to	O
policy	B
diﬀerences	O
.	O
policies	O
learned	O
in	O
strong	O
turbulence	O
were	O
more	O
conservative	O
in	O
that	O
they	O
preferred	O
small	O
bank	O
angles	O
,	O
whereas	O
in	O
weak	O
turbulence	O
,	O
the	O
best	O
action	B
was	O
to	O
turn	O
as	O
much	O
as	O
pos-	O
sible	O
by	O
banking	O
sharply	O
.	O
systematic	O
study	O
of	O
the	O
bank	O
angles	O
preferred	O
by	O
the	O
policies	O
learned	O
under	O
the	O
diﬀerent	O
conditions	O
led	O
the	O
authors	O
to	O
suggest	O
that	O
by	O
detecting	O
when	O
vertical	O
wind	O
acceleration	O
crosses	O
a	O
certain	O
threshold	O
the	O
controller	O
can	O
adjust	O
its	O
policy	B
to	O
cope	O
with	O
diﬀerent	O
turbulence	O
regimes	O
.	O
reddy	O
et	O
al	O
.	O
also	O
conducted	O
experiments	O
to	O
investigate	O
the	O
eﬀect	O
of	O
the	O
discount-rate	O
parameter	O
γ	O
on	O
the	O
performance	O
of	O
the	O
learned	O
policies	O
.	O
they	O
found	O
that	O
the	O
altitude	O
gained	O
in	O
an	O
episode	O
increased	O
as	O
γ	O
increased	O
,	O
reaching	O
a	O
maximum	O
for	O
γ	O
=	O
.99	O
,	O
suggesting	O
that	O
eﬀective	O
thermal	O
soaring	O
requires	O
taking	O
into	O
account	O
long-term	O
eﬀects	O
of	O
control	O
decisions	O
.	O
this	O
computational	O
study	O
of	O
thermal	O
soaring	O
illustrates	O
how	O
reinforcement	B
learning	I
can	O
further	O
progress	O
toward	O
diﬀerent	O
kinds	O
of	O
objectives	O
.	O
learning	O
policies	O
having	O
ac-	O
cess	O
to	O
diﬀerent	O
sets	O
of	O
environmental	O
cues	O
and	B
control	I
actions	O
contributes	O
to	O
both	O
the	O
engineering	O
objective	O
of	O
designing	O
autonomous	O
gliders	O
and	O
the	O
scientiﬁc	O
objective	O
of	O
im-	O
proving	O
understanding	O
of	O
the	O
soaring	O
skills	O
of	O
birds	O
.	O
in	O
both	O
cases	O
,	O
hypotheses	O
resulting	O
from	O
the	O
learning	O
experiments	O
can	O
be	O
tested	O
in	O
the	O
ﬁeld	O
by	O
instrumenting	O
real	O
gliders	O
and	O
by	O
comparing	O
predictions	O
with	O
observed	O
bird	O
soaring	O
behavior	O
.	O
chapter	O
17	O
frontiers	O
in	O
this	O
ﬁnal	O
chapter	O
we	O
touch	O
on	O
some	O
topics	O
that	O
are	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
but	O
that	O
we	O
see	O
as	O
particularly	O
important	O
for	O
the	O
future	O
of	O
reinforcement	O
learning	O
.	O
many	O
of	O
these	O
topics	O
bring	O
us	O
beyond	O
what	O
is	O
reliably	O
known	O
,	O
and	O
some	O
bring	O
us	O
beyond	O
the	O
mdp	O
framework	O
.	O
17.1	O
general	O
value	O
functions	O
and	O
auxiliary	B
tasks	I
over	O
the	O
course	O
of	O
this	O
book	O
,	O
our	O
notion	O
of	O
value	O
function	O
has	O
become	O
quite	O
general	O
.	O
with	O
oﬀ-policy	O
learning	O
we	O
allowed	O
a	O
value	B
function	I
to	O
be	O
conditional	O
on	O
an	O
arbitrary	O
target	B
policy	O
.	O
then	O
in	O
section	O
12.8	O
we	O
generalized	O
discounting	O
to	O
a	O
termination	O
function	O
γ	O
:	O
s	O
(	O
cid:55	O
)	O
→	O
[	O
0	O
,	O
1	O
]	O
,	O
so	O
that	O
a	O
diﬀerent	O
discount	O
rate	O
could	O
be	O
applied	O
at	O
each	O
time	O
step	O
in	O
determining	O
the	O
return	B
(	O
12.17	O
)	O
.	O
this	O
allowed	O
us	O
to	O
express	O
predictions	O
about	O
how	O
much	O
reward	O
we	O
will	O
get	O
over	O
an	O
arbitrary	O
,	O
state-dependent	O
horizon	O
.	O
the	O
next	O
,	O
and	O
perhaps	O
ﬁnal	O
,	O
step	O
is	O
to	O
generalize	O
beyond	O
rewards	O
to	O
permit	O
predictions	O
about	O
arbitrary	O
signals	O
.	O
rather	O
than	O
predicting	O
the	O
sum	O
of	O
future	O
rewards	O
,	O
we	O
might	O
predict	O
the	O
sum	O
of	O
the	O
future	O
values	O
of	O
a	O
sound	O
or	O
color	O
sensation	O
,	O
or	O
of	O
an	O
internal	O
,	O
highly	O
processed	O
signal	O
such	O
as	O
another	O
prediction	B
.	O
whatever	O
signal	O
is	O
added	O
up	O
in	O
this	O
way	O
in	O
a	O
value-function-like	O
prediction	B
,	O
we	O
call	O
it	O
the	O
cumulant	O
of	O
that	O
prediction	B
.	O
we	O
formalize	O
it	O
in	O
a	O
cumulant	O
signal	O
ct	O
∈	O
r.	O
using	O
this	O
,	O
a	O
general	O
value	O
function	O
,	O
or	O
gvf	O
,	O
is	O
written	O
vπ	O
,	O
γ	O
,	O
c	O
(	O
s	O
)	O
=	O
e	O
(	O
cid:34	O
)	O
∞	O
(	O
cid:88	O
)	O
k=t	O
ck+1	O
k	O
(	O
cid:89	O
)	O
i=t+1	O
st	O
=	O
s	O
,	O
at	O
:	O
∞∼	O
π	O
(	O
cid:35	O
)	O
.	O
γ	O
(	O
si	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
as	O
with	O
conventional	O
value	B
functions	O
(	O
such	O
as	O
vπ	O
or	O
q∗	O
)	O
this	O
is	O
an	O
ideal	O
function	O
that	O
we	O
seek	O
to	O
approximate	B
with	O
a	O
parameterized	O
form	O
,	O
which	O
we	O
might	O
continue	O
to	O
denote	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
,	O
although	O
of	O
course	O
there	O
would	O
have	O
to	O
be	O
a	O
diﬀerent	O
w	O
for	O
each	O
prediction	B
,	O
that	O
is	O
,	O
for	O
each	O
choice	O
of	O
π	O
,	O
γ	O
,	O
and	O
ct.	O
because	O
a	O
gvf	O
has	O
no	O
necessary	O
connection	O
to	O
reward	O
,	O
it	O
is	O
perhaps	O
a	O
misnomer	O
to	O
call	O
it	O
a	O
value	B
function	I
.	O
one	O
could	O
simply	O
call	O
it	O
a	O
prediction	B
or	O
,	O
to	O
make	O
it	O
more	O
distinctive	O
,	O
a	O
forecast	O
(	O
ring	O
,	O
in	O
preparation	O
)	O
.	O
whatever	O
it	O
is	O
called	O
,	O
463	O
464	O
chapter	O
17	O
:	O
frontiers	O
it	O
is	O
in	O
the	O
form	O
of	O
a	O
value	B
function	I
and	O
thus	O
can	O
be	O
learned	O
in	O
the	O
usual	O
ways	O
using	O
the	O
methods	O
developed	O
in	O
this	O
book	O
for	O
learning	O
approximate	B
value	O
functions	O
.	O
along	O
with	O
the	O
learned	O
predictions	O
,	O
we	O
might	O
also	O
learn	O
policies	O
to	O
maximize	O
the	O
predictions	O
in	O
the	O
usual	O
ways	O
by	O
generalized	O
policy	O
iteration	O
(	O
section	O
4.6	O
)	O
or	O
by	O
actor–critic	B
methods	O
.	O
in	O
this	O
way	O
an	O
agent	O
could	O
learn	O
to	O
predict	O
and	B
control	I
great	O
numbers	O
of	O
signals	O
,	O
not	O
just	O
long-term	O
reward	O
.	O
why	O
might	O
it	O
be	O
useful	O
to	O
predict	O
and	B
control	I
signals	O
other	O
than	O
long-term	O
reward	O
?	O
these	O
are	O
auxiliary	B
tasks	I
in	O
that	O
they	O
are	O
extra	O
,	O
in-addition-to	O
,	O
the	O
main	O
task	O
of	O
maxi-	O
mizing	O
reward	O
.	O
one	O
answer	O
is	O
that	O
the	O
ability	O
to	O
predict	O
and	B
control	I
a	O
diverse	O
multitude	O
of	O
signals	O
can	O
constitute	O
a	O
powerful	O
kind	O
of	O
environmental	O
model	O
.	O
as	O
we	O
saw	O
in	O
chap-	O
ter	O
8	O
,	O
a	O
good	O
model	O
can	O
enable	O
the	O
agent	O
to	O
get	O
reward	O
more	O
eﬃciently	O
.	O
it	O
takes	O
a	O
couple	O
of	O
further	O
concepts	O
to	O
develop	O
this	O
answer	O
clearly	O
,	O
so	O
we	O
postpone	O
it	O
to	O
the	O
next	O
section	O
.	O
first	O
let	O
’	O
s	O
consider	O
two	O
simpler	O
ways	O
in	O
which	O
a	O
multitude	O
of	O
diverse	O
predictions	O
can	O
be	O
helpful	O
to	O
a	O
reinforcement	B
learning	I
agent	O
.	O
one	O
simple	O
way	O
in	O
which	O
auxiliary	B
tasks	I
can	O
help	O
on	O
the	O
main	O
task	O
is	O
that	O
they	O
may	O
require	O
some	O
of	O
the	O
same	O
representations	O
as	O
are	O
needed	O
on	O
the	O
main	O
task	O
.	O
some	O
of	O
the	O
auxiliary	B
tasks	I
may	O
be	O
easier	O
,	O
with	O
less	O
delay	O
and	O
a	O
clearer	O
connection	O
between	O
actions	O
and	O
outcomes	O
.	O
if	O
good	O
features	O
can	O
be	O
found	O
early	O
on	O
easy	O
auxilary	O
tasks	O
,	O
then	O
those	O
features	O
may	O
signiﬁcantly	O
speed	O
learning	O
on	O
the	O
main	O
task	O
.	O
there	O
is	O
no	O
necessary	O
reason	O
why	O
this	O
has	O
to	O
be	O
true	O
,	O
but	O
in	O
many	O
cases	O
it	O
seems	O
plausible	O
.	O
for	O
example	O
,	O
if	O
you	O
learn	O
to	O
predict	O
and	B
control	I
your	O
sensors	O
over	O
short	O
time	O
scales	O
,	O
say	O
seconds	O
,	O
then	O
you	O
might	O
plausibly	O
come	O
up	O
with	O
part	O
of	O
the	O
idea	O
of	O
objects	O
,	O
which	O
would	O
then	O
greatly	O
help	O
with	O
the	O
prediction	B
and	O
control	B
of	O
long-term	O
reward	O
.	O
we	O
might	O
imagine	O
an	O
artiﬁcial	O
neural	O
network	O
in	O
which	O
the	O
last	O
layer	O
is	O
split	O
into	O
multiple	O
parts	O
,	O
or	O
heads	O
,	O
each	O
working	O
on	O
a	O
diﬀerent	O
task	O
.	O
one	O
head	O
might	O
produce	O
the	O
approximate	B
value	O
function	O
for	O
the	O
main	O
task	O
(	O
with	O
reward	O
as	O
its	O
cumulant	O
)	O
whereas	O
the	O
others	O
would	O
produce	O
solutions	O
to	O
various	O
auxilary	O
tasks	O
.	O
all	O
heads	O
could	O
propagate	O
errors	O
by	O
stochastic	O
gradient	O
descent	O
into	O
the	O
same	O
body—the	O
shared	O
preceding	O
part	O
of	O
the	O
network—which	O
would	O
then	O
try	O
to	O
form	O
representations	O
,	O
in	O
its	O
next-to-last	O
layer	O
,	O
to	O
support	O
all	O
the	O
heads	O
.	O
researchers	O
have	O
experimented	O
with	O
auxiliary	O
tasks	O
such	O
as	O
predicting	O
change	O
in	O
pixels	O
,	O
predicting	O
the	O
next	O
time	O
step	O
’	O
s	O
reward	O
,	O
and	O
predicting	O
the	O
distribution	O
of	O
the	O
return	B
.	O
in	O
many	O
cases	O
this	O
approach	O
has	O
been	O
shown	O
to	O
greatly	O
ac-	O
celerate	O
learning	O
on	O
the	O
main	O
task	O
(	O
jaderberg	O
et	O
al.	O
,	O
2017	O
)	O
.	O
multiple	O
predictions	O
have	O
similarly	O
been	O
repeatedly	O
proposed	O
as	O
a	O
way	O
of	O
directing	O
the	O
construction	O
of	O
state	O
esti-	O
mates	O
(	O
see	O
section	O
17.3	O
)	O
.	O
another	O
simple	O
way	O
in	O
which	O
the	O
learning	O
of	O
auxiliary	B
tasks	I
can	O
improve	O
performance	O
is	O
best	O
explained	O
by	O
analogy	O
to	O
the	O
psychological	O
phenomena	O
of	O
classical	O
conditioning	B
(	O
section	O
14.2	O
)	O
.	O
one	O
way	O
of	O
understanding	O
classical	B
conditioning	I
is	O
that	O
evolution	B
has	O
built	O
in	O
a	O
reﬂexive	O
(	O
non-learned	O
)	O
association	O
to	O
a	O
particular	O
action	B
from	O
the	O
prediction	B
of	O
a	O
particular	O
signal	O
.	O
for	O
example	O
,	O
humans	O
and	O
many	O
other	O
animals	O
appear	O
to	O
have	O
a	O
built-in	O
reﬂex	O
to	O
blink	O
whenever	O
their	O
prediction	B
of	O
being	O
poked	O
in	O
the	O
eye	O
exceeds	O
some	O
threshold	O
.	O
the	O
prediction	B
is	O
learned	O
,	O
but	O
the	O
association	O
from	O
prediction	B
to	O
eye	O
closure	O
is	O
built	O
in	O
,	O
and	O
thus	O
the	O
animal	O
is	O
saved	O
many	O
unprotected	O
pokes	O
in	O
its	O
eye	O
.	O
similarly	O
,	O
the	O
association	O
from	O
fear	O
to	O
increased	O
heart	O
rate	O
,	O
or	O
to	O
freezing	O
,	O
can	O
be	O
built	O
in	O
.	O
agent	O
17.2.	O
temporal	B
abstraction	I
via	O
options	B
465	O
designers	O
can	O
do	O
something	O
similar	O
,	O
connecting	O
by	O
design	O
(	O
without	O
learning	O
)	O
predictions	O
of	O
speciﬁc	O
events	O
to	O
predetermined	O
actions	O
.	O
for	O
example	O
,	O
a	O
self-driving	O
car	O
that	O
learns	O
to	O
predict	O
whether	O
going	O
forward	O
will	O
produce	O
a	O
collision	O
could	O
be	O
given	O
a	O
built-in	O
reﬂex	O
to	O
stop	O
,	O
or	O
to	O
turn	O
away	O
,	O
whenever	O
the	O
prediction	B
is	O
above	O
some	O
threshold	O
.	O
or	O
consider	O
a	O
vacuum-cleaning	O
robot	O
that	O
learned	O
to	O
predict	O
whether	O
it	O
might	O
run	O
out	O
of	O
battery	O
power	O
before	O
returning	O
to	O
the	O
charger	O
and	O
that	O
reﬂexively	O
headed	O
back	O
to	O
the	O
charger	O
whenever	O
the	O
prediction	B
became	O
non-zero	O
.	O
the	O
correct	O
prediction	B
would	O
depend	O
on	O
the	O
size	O
of	O
the	O
house	O
,	O
the	O
room	O
the	O
robot	O
was	O
in	O
,	O
and	O
the	O
age	O
of	O
the	O
battery	O
,	O
all	O
of	O
which	O
would	O
be	O
hard	O
for	O
the	O
robot	O
designer	O
to	O
know	O
.	O
it	O
would	O
be	O
diﬃcult	O
for	O
the	O
designer	O
to	O
build	O
in	O
a	O
reliable	O
algorithm	O
for	O
deciding	O
whether	O
to	O
head	O
back	O
to	O
the	O
charger	O
in	O
sensory	O
terms	O
,	O
but	O
it	O
might	O
be	O
easy	O
to	O
do	O
this	O
in	O
terms	O
of	O
the	O
learned	O
prediction	B
.	O
we	O
foresee	O
many	O
possible	O
ways	O
like	O
this	O
in	O
which	O
learned	O
predictions	O
might	O
combine	O
usefully	O
with	O
built-in	O
algorithms	O
for	O
controlling	O
behavior	O
.	O
finally	O
,	O
perhaps	O
the	O
most	O
important	O
role	O
for	O
auxiliary	O
tasks	O
is	O
in	O
moving	O
beyond	O
the	O
assumption	O
we	O
have	O
made	O
throughout	O
this	O
book	O
that	O
the	O
state	B
representation	O
is	O
ﬁxed	O
and	O
given	O
to	O
the	O
agent	O
.	O
to	O
explain	O
this	O
role	O
,	O
we	O
ﬁrst	O
have	O
to	O
take	O
a	O
few	O
steps	O
back	O
to	O
appreciate	O
the	O
magnitude	O
of	O
this	O
assumption	O
and	O
the	O
implications	O
of	O
removing	O
it	O
.	O
we	O
do	O
that	O
in	O
section	O
17.3	O
.	O
17.2	O
temporal	B
abstraction	I
via	O
options	B
an	O
appealing	O
aspect	O
of	O
the	O
mdp	O
formalism	O
is	O
that	O
it	O
can	O
be	O
applied	O
usefully	O
to	O
tasks	O
at	O
many	O
diﬀerent	O
time	O
scales	O
.	O
one	O
can	O
use	O
it	O
to	O
formalize	O
the	O
task	O
of	O
deciding	O
which	O
muscles	O
to	O
twitch	O
to	O
grasp	O
an	O
object	O
,	O
which	O
airplane	O
ﬂight	O
to	O
take	O
to	O
arrive	O
conveniently	O
at	O
a	O
distant	O
city	O
,	O
and	O
which	O
job	O
to	O
take	O
to	O
lead	O
a	O
satisfying	O
life	O
.	O
these	O
tasks	O
diﬀer	O
greatly	O
in	O
their	O
time	O
scales	O
,	O
yet	O
each	O
can	O
be	O
usefully	O
formulated	O
as	O
an	O
mdp	O
that	O
can	O
be	O
solved	O
by	O
planning	B
or	O
learning	O
processes	O
as	O
described	O
in	O
this	O
book	O
.	O
all	O
involve	O
interaction	O
with	O
the	O
world	O
,	O
sequential	O
decision	O
making	O
,	O
and	O
a	O
goal	B
usefully	O
conceived	O
of	O
as	O
accumulating	B
rewards	O
over	O
time	O
,	O
and	O
so	O
all	O
can	O
be	O
formulated	O
as	O
mdps	O
.	O
although	O
all	O
these	O
tasks	O
can	O
be	O
formulated	O
as	O
mdps	O
,	O
one	O
might	O
think	O
that	O
they	O
can	O
not	O
be	O
formulated	O
as	O
a	O
single	O
mdp	O
.	O
they	O
involve	O
such	O
diﬀerent	O
time	O
scales	O
,	O
such	O
diﬀerent	O
notions	O
of	O
choice	O
and	O
action	O
!	O
it	O
would	O
be	O
no	O
good	O
,	O
for	O
example	O
,	O
to	O
plan	O
a	O
ﬂight	O
across	O
a	O
continent	O
at	O
the	O
level	O
of	O
muscle	O
twitches	O
.	O
yet	O
for	O
other	O
tasks	O
,	O
grasping	O
,	O
throwing	O
darts	O
,	O
or	O
hitting	O
a	O
baseball	O
,	O
low-level	O
muscle	O
twitches	O
may	O
be	O
just	O
the	O
right	O
level	O
.	O
people	O
do	O
all	O
these	O
things	O
seamlessly	O
without	O
appearing	O
to	O
switch	O
between	O
levels	O
.	O
can	O
the	O
mdp	O
framework	O
be	O
stretched	O
to	O
cover	O
all	O
the	O
levels	O
simultaneously	O
?	O
perhaps	O
it	O
can	O
.	O
one	O
popular	O
idea	O
is	O
to	O
formalize	O
an	O
mdp	O
at	O
a	O
detailed	O
level	O
,	O
with	O
a	O
small	O
time	O
step	O
,	O
yet	O
enable	O
planning	B
at	O
higher	O
levels	O
using	O
extended	O
courses	O
of	O
action	O
that	O
correspond	O
to	O
many	O
base-level	O
time	O
steps	O
.	O
to	O
do	O
this	O
we	O
need	O
a	O
notion	O
of	O
course	O
of	O
action	O
that	O
extends	O
over	O
many	O
time	O
steps	O
and	O
includes	O
a	O
notion	O
of	O
termination	O
.	O
a	O
general	O
way	O
to	O
formulate	O
these	O
two	O
ideas	O
is	O
as	O
a	O
policy	B
,	O
π	O
,	O
and	O
a	O
state-dependent	O
termination	O
function	O
,	O
γ	O
,	O
as	O
in	O
gvfs	O
.	O
we	O
deﬁne	O
a	O
pair	O
of	O
these	O
as	O
a	O
generalized	O
notion	O
of	O
action	O
termed	O
an	O
option	O
.	O
to	O
execute	O
an	O
option	O
ω	O
=	O
(	O
cid:104	O
)	O
πω	O
,	O
γω	O
(	O
cid:105	O
)	O
at	O
time	O
t	O
is	O
to	O
obtain	O
the	O
action	B
to	O
take	O
,	O
at	O
,	O
from	O
πω	O
(	O
·|st	O
)	O
,	O
then	O
terminate	O
at	O
time	O
t	O
+	O
1	O
with	O
probability	O
γω	O
(	O
st+1	O
)	O
.	O
if	O
the	O
option	O
does	O
466	O
chapter	O
17	O
:	O
frontiers	O
not	O
terminate	O
at	O
t	O
+	O
1	O
,	O
then	O
at+1	O
is	O
selected	O
from	O
πω	O
(	O
·|st+1	O
)	O
,	O
and	O
the	O
option	O
terminates	O
at	O
t	O
+	O
2	O
with	O
probability	O
γω	O
(	O
st+2	O
)	O
,	O
and	O
so	O
on	O
until	O
eventual	O
termination	O
.	O
it	O
is	O
convenient	O
to	O
consider	O
low-level	O
actions	O
to	O
be	O
special	O
cases	O
of	O
options—each	O
action	B
a	O
corresponds	O
to	O
an	O
option	O
(	O
cid:104	O
)	O
πω	O
,	O
γω	O
(	O
cid:105	O
)	O
whose	O
policy	B
picks	O
the	O
action	B
(	O
πω	O
(	O
s	O
)	O
=	O
a	O
for	O
all	O
s	O
∈	O
s	O
)	O
and	O
whose	O
termination	O
function	O
is	O
zero	O
(	O
γω	O
(	O
s	O
)	O
=	O
0	O
for	O
all	O
s	O
∈	O
s+	O
)	O
.	O
options	B
eﬀectively	O
extend	O
the	O
action	B
space	O
.	O
the	O
agent	O
can	O
either	O
select	O
a	O
low-level	O
action/option	O
,	O
terminating	O
after	O
one	O
time	O
step	O
,	O
or	O
select	O
an	O
extended	O
option	O
that	O
might	O
execute	O
for	O
many	O
time	O
steps	O
before	O
terminating	O
.	O
options	B
are	O
designed	O
so	O
that	O
they	O
are	O
interchangable	O
with	O
low-level	O
actions	O
.	O
for	O
example	O
,	O
the	O
notion	O
of	O
an	O
action-value	B
function	I
qπ	O
naturally	O
generalizes	O
to	O
an	O
option-	O
value	B
function	I
that	O
takes	O
a	O
state	B
and	O
option	O
as	O
input	O
and	O
returns	O
the	O
expected	O
return	O
starting	O
from	O
that	O
state	B
,	O
executing	O
that	O
option	O
to	O
termination	O
,	O
and	O
thereafter	O
following	O
the	O
policy	B
,	O
π.	O
we	O
can	O
also	O
generalize	O
the	O
notion	O
of	O
policy	O
to	O
a	O
hierarchical	B
policy	I
that	O
selects	O
from	O
options	B
rather	O
than	O
actions	O
,	O
where	O
options	B
,	O
when	O
selected	O
,	O
execute	O
until	O
termination	O
.	O
with	O
these	O
ideas	O
,	O
many	O
of	O
the	O
algorithms	O
in	O
this	O
book	O
can	O
be	O
generalized	O
to	O
learn	O
approximate	B
option-value	O
functions	O
and	O
hierarchical	O
policies	O
.	O
in	O
the	O
simplest	O
case	O
,	O
the	O
learning	O
process	O
‘	O
jumps	O
’	O
from	O
option	O
initiation	O
to	O
option	O
termination	O
,	O
with	O
an	O
update	O
only	O
occurring	O
when	O
an	O
option	O
terminates	O
.	O
more	O
subtly	O
,	O
updates	O
can	O
be	O
made	O
on	O
each	O
time	O
step	O
,	O
using	O
intra-option	O
learning	O
algorithms	O
,	O
which	O
in	O
general	O
require	O
oﬀ-policy	B
learning	O
.	O
perhaps	O
the	O
most	O
important	O
generalization	O
made	O
possible	O
by	O
option	O
ideas	O
is	O
that	O
of	O
the	O
environmental	O
model	O
as	O
developed	O
in	O
chapters	O
3	O
,	O
4	O
,	O
and	O
8.	O
the	O
conventional	O
model	O
of	O
an	O
action	B
is	O
the	O
state-transition	O
probabilities	O
and	O
the	O
expected	O
immediate	O
reward	O
for	O
taking	O
the	O
action	B
in	O
each	O
state	B
.	O
how	O
do	O
conventional	O
action	B
models	O
generalize	O
to	O
option	B
models	I
?	O
for	B
options	I
,	O
the	O
appropriate	O
model	O
is	O
again	O
of	O
two	O
parts	O
,	O
one	O
corresponding	O
to	O
the	O
state	B
transition	O
resulting	O
from	O
executing	O
the	O
option	O
and	O
one	O
corresponding	O
to	O
the	O
expected	O
cumulative	O
reward	O
along	O
the	O
way	O
.	O
the	O
reward	O
part	O
of	O
an	O
option	O
model	O
,	O
analogous	O
to	O
the	O
expected	O
reward	O
for	O
state–action	O
pairs	O
(	O
3.5	O
)	O
,	O
is	O
.	O
r	O
(	O
s	O
,	O
ω	O
)	O
=	O
e	O
(	O
cid:2	O
)	O
r1	O
+	O
γr2	O
+	O
γ2r3	O
+	O
···	O
+	O
γτ−1rτ	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
s0	O
=	O
s	O
,	O
a0	O
:	O
τ−1∼	O
πω	O
,	O
τ	O
∼	O
γω	O
(	O
cid:3	O
)	O
,	O
(	O
17.1	O
)	O
for	O
all	O
options	B
ω	O
and	O
all	O
states	O
s	O
∈	O
s	O
,	O
where	O
τ	O
is	O
the	O
random	O
time	O
step	O
at	O
which	O
the	O
option	O
terminates	O
according	O
to	O
γω	O
.	O
note	O
the	O
role	O
of	O
the	O
overall	O
discounting	B
parameter	O
γ	O
in	O
this	O
equation—discounting	O
is	O
according	O
to	O
γ	O
,	O
but	O
termination	O
of	O
the	O
option	O
is	O
according	O
to	O
γω	O
.	O
the	O
state-transition	O
part	O
of	O
an	O
option	O
model	O
is	O
a	O
little	O
more	O
subtle	O
.	O
this	O
part	O
of	O
the	O
model	O
characterizes	O
the	O
probability	O
of	O
each	O
possible	O
resulting	O
state	B
(	O
as	O
in	O
(	O
3.4	O
)	O
)	O
,	O
but	O
now	O
this	O
state	B
may	O
result	O
after	O
various	O
numbers	O
of	O
time	O
steps	O
,	O
each	O
of	O
which	O
must	O
be	O
discounted	O
diﬀerently	O
.	O
the	O
model	O
for	O
option	O
ω	O
speciﬁes	O
,	O
for	O
each	O
state	B
s	O
that	O
ω	O
might	O
start	O
executing	O
in	O
,	O
and	O
for	O
each	O
state	B
s	O
(	O
cid:48	O
)	O
that	O
ω	O
might	O
terminate	O
in	O
,	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
ω	O
)	O
.	O
=	O
∞	O
(	O
cid:88	O
)	O
k=1	O
γk	O
pr	O
{	O
sk	O
=	O
s	O
(	O
cid:48	O
)	O
,	O
τ	O
=	O
k	O
|	O
s0	O
=	O
s	O
,	O
a0	O
:	O
k−1∼	O
πω	O
,	O
τ	O
∼	O
γω	O
}	O
.	O
(	O
17.2	O
)	O
note	O
that	O
,	O
because	O
of	O
the	O
factor	O
of	O
γk	O
,	O
this	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
ω	O
)	O
is	O
no	O
longer	O
a	O
transition	O
probability	O
and	O
no	O
longer	O
sums	O
to	O
one	O
over	O
all	O
values	O
of	O
s	O
(	O
cid:48	O
)	O
.	O
(	O
nevertheless	O
,	O
we	O
continue	O
to	O
use	O
the	O
‘	O
|	O
’	O
notation	O
in	O
p.	O
)	O
17.2.	O
temporal	B
abstraction	I
via	O
options	B
467	O
the	O
above	O
deﬁnition	O
of	O
the	O
transition	O
part	O
of	O
an	O
option	O
model	O
allows	O
us	O
to	O
formulate	O
bellman	O
equations	O
and	B
dynamic	I
programming	I
algorithms	O
that	O
apply	O
to	O
all	O
options	B
,	O
in-	O
cluding	O
low-level	O
actions	O
as	O
a	O
special	O
case	O
.	O
for	O
example	O
,	O
the	O
general	O
bellman	O
equation	O
for	O
the	O
state	B
values	O
of	O
a	O
hierarchical	B
policy	I
π	O
is	O
vπ	O
(	O
s	O
)	O
=	O
(	O
cid:88	O
)	O
ω∈ω	O
(	O
s	O
)	O
π	O
(	O
ω|s	O
)	O
(	O
cid:34	O
)	O
r	O
(	O
s	O
,	O
ω	O
)	O
+	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
ω	O
)	O
vπ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:35	O
)	O
,	O
(	O
17.3	O
)	O
where	O
ω	O
(	O
s	O
)	O
denotes	O
the	O
set	O
of	O
options	O
available	O
in	O
state	O
s.	O
if	O
ω	O
(	O
s	O
)	O
includes	O
only	O
the	O
low-level	O
actions	O
,	O
then	O
this	O
equation	O
reduces	O
to	O
a	O
version	O
of	O
the	O
usual	O
bellman	O
equation	O
(	O
3.14	O
)	O
,	O
except	O
of	O
course	O
γ	O
is	O
included	O
in	O
the	O
new	O
p	O
(	O
17.2	O
)	O
and	O
thus	O
does	O
not	O
appear	O
.	O
similarly	O
,	O
the	O
corresponding	O
planning	B
algorithms	O
also	O
have	O
no	O
γ.	O
for	O
example	O
,	O
the	O
value	B
iteration	I
algorithm	O
with	B
options	I
,	O
analogous	O
to	O
(	O
4.10	O
)	O
,	O
is	O
vk+1	O
(	O
s	O
)	O
.	O
=	O
max	O
ω∈ω	O
(	O
s	O
)	O
(	O
cid:34	O
)	O
r	O
(	O
s	O
,	O
ω	O
)	O
+	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
ω	O
)	O
vk	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
cid:35	O
)	O
,	O
for	O
all	O
s	O
∈	O
s.	O
if	O
ω	O
(	O
s	O
)	O
includes	O
all	O
the	O
low-level	O
actions	O
available	O
in	O
each	O
s	O
,	O
then	O
this	O
algorithm	O
converges	O
to	O
the	O
conventional	O
v∗	O
,	O
from	O
which	O
the	O
optimal	O
policy	O
can	O
be	O
computed	O
.	O
however	O
,	O
it	O
is	O
particularly	O
useful	O
to	O
plan	O
with	B
options	I
when	O
only	O
a	O
subset	O
of	O
the	O
possible	O
options	B
are	O
considered	O
(	O
in	O
ω	O
(	O
s	O
)	O
)	O
in	O
each	O
state	B
.	O
value	B
iteration	I
will	O
then	O
converge	O
to	O
the	O
best	O
hierarchical	B
policy	I
limited	O
to	O
the	O
restricted	O
set	O
of	O
options	O
.	O
although	O
this	O
policy	B
may	O
be	O
sub-optimal	O
,	O
convergence	O
can	O
be	O
much	O
faster	O
because	O
fewer	O
options	B
are	O
considered	O
and	O
because	O
each	O
option	O
can	O
jump	O
over	O
many	O
time	O
steps	O
.	O
to	O
plan	O
with	B
options	I
,	O
one	O
must	O
either	O
be	O
given	O
the	O
option	B
models	I
,	O
or	O
learn	O
them	O
.	O
one	O
natural	O
way	O
to	O
learn	O
an	O
option	O
model	O
is	O
to	O
formulate	O
it	O
as	O
a	O
collection	O
of	O
gvfs	O
(	O
as	O
deﬁned	O
in	O
the	O
preceding	O
section	O
)	O
and	O
then	O
learn	O
the	O
gvfs	O
using	O
the	O
methods	O
presented	O
in	O
this	O
book	O
.	O
it	O
is	O
not	O
diﬃcult	O
to	O
see	O
how	O
this	O
could	O
be	O
done	O
for	O
the	O
reward	O
part	O
of	O
the	O
option	O
model	O
.	O
one	O
merely	O
chooses	O
one	O
gvf	O
’	O
s	O
cumulant	O
to	O
be	O
the	O
reward	O
(	O
ct	O
=	O
rt	O
)	O
,	O
its	O
policy	B
to	O
be	O
the	O
the	O
option	O
’	O
s	O
policy	B
(	O
π	O
=	O
πω	O
)	O
,	O
and	O
its	O
termination	O
function	O
to	O
be	O
the	O
discount	O
rate	O
times	O
the	O
option	O
’	O
s	O
termination	O
function	O
(	O
γ	O
(	O
s	O
)	O
=	O
γ	O
·	O
γω	O
(	O
s	O
)	O
)	O
.	O
the	O
true	O
gvf	O
then	O
equals	O
the	O
reward	O
part	O
of	O
the	O
option	O
model	O
,	O
vπ	O
,	O
γ	O
,	O
c	O
(	O
s	O
)	O
=	O
r	O
(	O
s	O
,	O
ω	O
)	O
,	O
and	O
the	O
learning	O
methods	O
described	O
in	O
this	O
book	O
can	O
be	O
used	O
to	O
approximate	B
it	O
.	O
the	O
state-transition	O
part	O
of	O
the	O
option	O
model	O
is	O
only	O
a	O
little	O
more	O
complicated	O
.	O
one	O
needs	O
to	O
allocate	O
one	O
gvf	O
for	O
each	O
state	B
that	O
the	O
option	O
might	O
terminate	O
in	O
.	O
we	O
don	O
’	O
t	O
want	O
these	O
gvfs	O
to	O
accumulate	O
anything	O
except	O
when	O
the	O
option	O
terminates	O
,	O
and	O
then	O
only	O
when	O
the	O
termination	O
is	O
in	O
the	O
appropriate	O
state	B
.	O
this	O
can	O
be	O
achieved	O
by	O
choosing	O
the	O
cumulant	O
of	O
the	O
gvf	O
that	O
predicts	O
transition	O
to	O
state	B
s	O
(	O
cid:48	O
)	O
to	O
be	O
ct	O
=	O
γ	O
(	O
st	O
)	O
·	O
1st=s	O
(	O
cid:48	O
)	O
.	O
the	O
gvf	O
’	O
s	O
policy	B
and	O
termination	O
functions	O
are	O
chosen	O
the	O
same	O
as	O
for	O
the	O
reward	O
part	O
of	O
the	O
option	O
model	O
.	O
the	O
true	O
gvf	O
then	O
equals	O
the	O
s	O
(	O
cid:48	O
)	O
portion	O
of	O
the	O
option	O
’	O
s	O
state-transition	O
model	O
,	O
vπ	O
,	O
γ	O
,	O
c	O
(	O
s	O
)	O
=	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
ω	O
)	O
,	O
and	O
again	O
this	O
book	O
’	O
s	O
methods	O
could	O
be	O
employed	O
to	O
learn	O
it	O
.	O
although	O
each	O
of	O
these	O
steps	O
is	O
seemingly	O
natural	O
,	O
putting	O
them	O
all	O
together	O
(	O
including	O
function	B
approximation	I
and	O
other	O
essential	O
components	O
)	O
is	O
quite	O
challenging	O
and	O
beyond	O
the	O
current	O
state	B
of	O
the	O
art	O
.	O
468	O
chapter	O
17	O
:	O
frontiers	O
exercise	O
17.1	O
this	O
section	O
has	O
presented	O
options	B
for	O
the	O
discounted	O
case	O
,	O
but	O
discounting	B
is	O
arguably	O
inappropriate	O
for	O
control	O
when	O
using	O
function	B
approximation	I
(	O
section	O
10.4	O
)	O
.	O
what	O
is	O
the	O
natural	O
bellman	O
equation	O
for	O
a	O
hierarchical	O
policy	B
,	O
analogous	O
to	O
(	O
17.3	O
)	O
,	O
but	O
for	O
the	O
average	B
reward	I
setting	I
(	O
section	O
10.3	O
)	O
?	O
what	O
are	O
the	O
two	O
parts	O
of	O
the	O
option	O
(	O
cid:3	O
)	O
model	O
,	O
analogous	O
to	O
(	O
17.1	O
)	O
and	O
(	O
17.2	O
)	O
,	O
for	O
the	O
average	B
reward	I
setting	I
?	O
17.3	O
observations	O
and	O
state	O
throughout	O
this	O
book	O
we	O
have	O
written	O
the	O
learned	O
approximate	B
value	O
functions	O
(	O
and	O
the	O
policies	O
in	O
chapter	O
13	O
)	O
as	O
functions	O
of	O
the	O
environment	B
’	O
s	O
state	B
.	O
this	O
is	O
a	O
signiﬁcant	O
limitation	O
of	O
the	O
methods	O
presented	O
in	O
part	O
i	O
,	O
in	O
which	O
the	O
learned	O
value	B
function	I
was	O
implemented	O
as	O
a	O
table	O
such	O
that	O
any	O
value	B
function	I
could	O
be	O
exactly	O
approximated	O
;	O
that	O
case	O
is	O
tantamount	O
to	O
assuming	O
that	O
the	O
state	B
of	O
the	O
environment	B
is	O
completely	O
observed	O
by	O
the	O
agent	O
.	O
but	O
in	O
many	O
cases	O
of	O
interest	O
,	O
and	O
certainly	O
in	O
the	O
lives	O
of	O
all	O
natural	O
intelligences	O
,	O
the	O
sensory	O
input	O
gives	O
only	O
partial	O
information	O
about	O
the	O
state	B
of	O
the	O
world	O
.	O
some	O
objects	O
may	O
be	O
occluded	O
by	O
others	O
,	O
or	O
behind	O
the	O
agent	O
,	O
or	O
miles	O
away	O
.	O
in	O
these	O
cases	O
,	O
potentially	O
important	O
aspects	O
of	O
the	O
environment	B
’	O
s	O
state	B
are	O
not	O
directly	O
observable	O
,	O
and	O
it	O
is	O
a	O
strong	O
,	O
unrealistic	O
,	O
and	O
limiting	O
assumption	O
to	O
assume	O
that	O
the	O
learned	O
value	B
function	I
is	O
implemented	O
as	O
a	O
table	O
over	O
the	O
environment	B
’	O
s	O
state	B
space	O
.	O
the	O
framework	O
of	O
parametric	O
function	B
approximation	I
that	O
we	O
developed	O
in	O
part	O
ii	O
is	O
far	O
less	O
restrictive	O
and	O
,	O
arguably	O
,	O
no	O
limitation	O
at	O
all	O
.	O
in	O
part	O
ii	O
we	O
retained	O
the	O
assump-	O
tion	B
that	O
the	O
learned	O
value	B
functions	O
(	O
and	O
policies	O
)	O
are	O
functions	O
of	O
the	O
environment	B
’	O
s	O
state	B
,	O
but	O
allowed	O
these	O
functions	O
to	O
be	O
arbitrarily	O
restricted	O
by	O
the	O
parameterization	O
.	O
it	O
is	O
somewhat	O
surprising	O
and	O
not	O
widely	O
recognized	O
that	O
function	B
approximation	I
includes	O
important	O
aspects	O
of	O
partial	O
observability	O
.	O
for	O
example	O
,	O
if	O
there	O
is	O
some	O
state	B
variable	O
that	O
is	O
not	O
observable	O
,	O
then	O
the	O
parameterization	O
can	O
be	O
chosen	O
such	O
that	O
the	O
approx-	O
imate	O
value	B
does	O
not	O
depend	O
on	O
that	O
state	B
variable	O
.	O
the	O
eﬀect	O
is	O
just	O
as	O
if	O
that	O
state	B
variable	O
were	O
not	O
observable	O
.	O
because	O
of	O
this	O
,	O
all	O
the	O
results	O
obtained	O
for	O
the	O
param-	O
eterized	O
case	O
apply	O
to	O
partial	O
observability	O
without	O
change	O
.	O
in	O
this	O
sense	O
,	O
the	O
case	O
of	O
parameterized	O
function	B
approximation	I
includes	O
the	O
case	O
of	O
partial	O
observability	O
.	O
nevertheless	O
,	O
there	O
are	O
many	O
issues	O
that	O
can	O
not	O
be	O
investigated	O
without	O
a	O
more	O
explicit	O
treatment	O
of	O
partial	O
observability	O
.	O
although	O
we	O
can	O
not	O
give	O
them	O
a	O
full	O
treatment	O
here	O
,	O
we	O
can	O
outline	O
the	O
changes	O
that	O
would	O
be	O
needed	O
to	O
do	O
so	O
.	O
there	O
are	O
four	O
steps	O
.	O
first	O
,	O
we	O
would	O
change	O
the	O
problem	O
.	O
the	O
environment	B
would	O
emit	O
not	O
its	O
states	O
,	O
but	O
only	O
observations—signals	O
that	O
depend	O
on	O
its	O
state	B
but	O
,	O
like	O
a	O
robot	O
’	O
s	O
sensors	O
,	O
provide	O
only	O
partial	O
information	O
about	O
it	O
.	O
for	O
convenience	O
,	O
without	O
loss	O
of	O
generality	O
,	O
we	O
assume	O
that	O
the	O
reward	O
is	O
a	O
direct	O
,	O
known	O
function	O
of	O
the	O
observation	O
(	O
perhaps	O
the	O
observation	O
is	O
a	O
vector	B
,	O
and	O
the	O
reward	O
is	O
one	O
of	O
is	O
components	O
)	O
.	O
the	O
environmental	O
interaction	O
would	O
then	O
have	O
no	O
explicit	O
states	O
or	O
rewards	O
,	O
but	O
would	O
simply	O
be	O
an	O
alternating	O
sequence	O
of	O
actions	O
at	O
∈	O
a	O
and	B
observations	I
ot	O
∈	O
o	O
:	O
a0	O
,	O
o1	O
,	O
a1	O
,	O
o2	O
,	O
a2	O
,	O
o3	O
,	O
a3	O
,	O
o4	O
,	O
.	O
.	O
.	O
,	O
going	O
on	O
forever	O
(	O
cf	O
.	O
equation	O
3.1	O
)	O
or	O
forming	O
episodes	B
each	O
ending	O
with	O
a	O
special	O
terminal	O
observation	O
.	O
17.3.	O
observations	O
and	O
state	O
469	O
second	O
,	O
we	O
can	O
recover	O
the	O
idea	O
of	O
state	O
as	O
used	O
in	O
this	O
book	O
from	O
the	O
sequence	O
of	O
observations	O
and	O
actions	O
.	O
let	O
us	O
use	O
the	O
word	O
history	O
,	O
and	O
the	O
notation	O
ht	O
,	O
for	O
an	O
initial	O
.	O
portion	O
of	O
the	O
trajectory	O
up	O
to	O
an	O
observation	O
:	O
ht	O
=	O
a0	O
,	O
o1	O
,	O
.	O
.	O
.	O
,	O
at−1	O
,	O
ot	O
.	O
the	O
history	O
represents	O
the	O
most	O
that	O
we	O
can	O
know	O
about	O
the	O
past	O
without	O
looking	O
outside	O
of	O
the	O
data	O
stream	O
(	O
because	O
the	O
history	O
is	O
the	O
whole	O
past	O
data	O
stream	O
)	O
.	O
of	O
course	O
,	O
the	O
history	O
grows	O
with	O
t	O
and	O
can	O
become	O
large	O
and	O
unwieldy	O
.	O
the	O
idea	O
of	O
state	O
is	O
that	O
of	O
some	O
compact	O
summary	O
of	O
the	O
history	O
that	O
is	O
as	O
useful	O
as	O
the	O
actual	O
history	O
for	O
predicting	O
the	O
future	O
.	O
let	O
us	O
be	O
clear	O
about	O
exactly	O
what	O
this	O
means	O
.	O
to	O
be	O
a	O
summary	O
of	O
the	O
history	O
,	O
the	O
state	B
must	O
be	O
a	O
function	O
of	O
history	O
,	O
st	O
=	O
f	O
(	O
ht	O
)	O
,	O
and	O
to	O
be	O
as	O
useful	O
for	O
predicting	O
the	O
future	O
as	O
the	O
whole	O
history	O
,	O
it	O
must	O
have	O
what	O
is	O
known	O
as	O
the	O
markov	O
property	O
.	O
formally	O
,	O
this	O
is	O
a	O
property	O
of	O
the	O
function	O
f	O
.	O
a	O
function	O
f	O
has	O
the	O
markov	O
property	O
if	O
and	O
only	O
if	O
any	O
two	O
histories	O
h	O
and	O
h	O
(	O
cid:48	O
)	O
that	O
are	O
mapped	O
by	O
f	O
to	O
the	O
same	O
state	B
(	O
f	O
(	O
h	O
)	O
=	O
f	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
)	O
also	O
have	O
the	O
same	O
probabilities	O
for	O
their	O
next	O
observation	O
,	O
f	O
(	O
h	O
)	O
=	O
f	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
⇒	O
pr	O
{	O
ot+1	O
=	O
o|ht	O
=	O
h	O
,	O
at	O
=	O
a	O
}	O
=	O
pr	O
{	O
ot+1	O
=	O
o|ht	O
=	O
h	O
(	O
cid:48	O
)	O
,	O
at	O
=	O
a	O
}	O
,	O
(	O
17.4	O
)	O
for	O
all	O
o	O
∈	O
o	O
and	O
a	O
∈	O
a.	O
if	O
f	O
is	O
markov	O
,	O
then	O
st	O
=	O
f	O
(	O
ht	O
)	O
is	O
a	O
state	B
as	O
we	O
have	O
used	O
the	O
term	O
in	O
this	O
book	O
.	O
let	O
us	O
henceforth	O
call	O
it	O
a	O
markov	O
state	B
to	O
distinguish	O
it	O
from	O
states	O
that	O
are	O
summaries	O
of	O
the	O
history	O
but	O
fall	O
short	O
of	O
the	O
markov	O
property	O
(	O
which	O
we	O
will	O
consider	O
shortly	O
)	O
.	O
a	O
markov	O
state	B
is	O
a	O
good	O
basis	O
for	O
predicting	O
the	O
next	O
observation	O
(	O
17.4	O
)	O
but	O
,	O
more	O
importantly	O
,	O
it	O
is	O
also	O
a	O
good	O
basis	O
for	O
predicting	O
or	O
controlling	O
anything	O
.	O
for	O
example	O
,	O
let	O
a	O
test	O
be	O
any	O
speciﬁc	O
sequence	O
of	O
alternating	O
actions	O
and	B
observations	I
that	O
might	O
occur	O
in	O
the	O
future	O
.	O
for	O
example	O
,	O
a	O
three-step	O
test	O
is	O
denoted	O
τ	O
=	O
a1	O
o1	O
a2	O
,	O
o2	O
,	O
a3	O
,	O
o3	O
.	O
the	O
probability	O
of	O
this	O
test	O
given	O
a	O
speciﬁc	O
history	O
h	O
is	O
deﬁned	O
as	O
p	O
(	O
τ|h	O
)	O
.	O
=	O
pr	O
{	O
ot+1	O
=	O
o1	O
,	O
ot+2	O
=	O
o2	O
,	O
ot+3	O
=	O
o3	O
|	O
ht	O
=	O
h	O
,	O
at	O
=	O
a1	O
,	O
at+1	O
=	O
a2	O
,	O
at+2	O
=	O
a3	O
}	O
.	O
(	O
17.5	O
)	O
if	O
f	O
is	O
markov	O
and	O
h	O
and	O
h	O
(	O
cid:48	O
)	O
are	O
any	O
two	O
histories	O
that	O
map	O
to	O
the	O
same	O
state	B
under	O
f	O
,	O
then	O
for	O
any	O
test	O
τ	O
of	O
any	O
length	O
,	O
its	O
probabilities	O
given	O
the	O
two	O
histories	O
must	O
also	O
be	O
the	O
same	O
:	O
f	O
(	O
h	O
)	O
=	O
f	O
(	O
h	O
(	O
cid:48	O
)	O
)	O
⇒	O
p	O
(	O
τ|h	O
)	O
=	O
p	O
(	O
τ|h	O
(	O
cid:48	O
)	O
)	O
.	O
in	O
other	O
words	O
,	O
a	O
markov	O
state	B
summarizes	O
all	O
the	O
information	O
in	O
the	O
history	O
necessary	O
for	O
determining	O
any	O
test	O
’	O
s	O
probability	O
.	O
in	O
fact	O
,	O
it	O
summarizes	O
all	O
that	O
is	O
necessary	O
for	O
making	O
any	O
prediction	B
,	O
including	O
any	O
gvf	O
,	O
and	O
for	O
behaving	O
optimally	O
(	O
if	O
f	O
is	O
markov	O
,	O
.	O
then	O
there	O
is	O
always	O
a	O
deterministic	O
function	O
π	O
such	O
that	O
choosing	O
at	O
=	O
π	O
(	O
f	O
(	O
ht	O
)	O
)	O
is	O
optimal	O
)	O
.	O
the	O
third	O
step	O
in	O
extending	O
reinforcement	B
learning	I
to	O
partial	O
observability	O
is	O
to	O
deal	O
with	O
certain	O
computational	O
considerations	O
.	O
in	O
particular	O
,	O
we	O
want	O
the	O
state	B
to	O
be	O
a	O
compact	O
summary	O
of	O
the	O
history	O
.	O
for	O
example	O
,	O
the	O
identity	O
function	O
completely	O
satis-	O
ﬁes	O
the	O
conditions	O
for	O
a	O
markov	O
f	O
,	O
but	O
would	O
nevertheless	O
be	O
of	O
little	O
use	O
because	O
the	O
corresponding	O
state	B
st	O
=	O
ht	O
would	O
grow	O
with	O
time	O
and	O
become	O
unwieldy	O
,	O
as	O
mentioned	O
earlier	O
,	O
but	O
more	O
fundamentally	O
because	O
it	O
would	O
never	O
recur	O
;	O
the	O
agent	O
would	O
never	O
470	O
chapter	O
17	O
:	O
frontiers	O
encounter	O
the	O
same	O
state	B
twice	O
(	O
in	O
a	O
continuing	O
task	O
)	O
and	O
thus	O
could	O
never	O
beneﬁt	O
from	O
a	O
tabular	O
learning	O
method	O
.	O
we	O
want	O
our	O
states	O
to	O
be	O
compact	O
as	O
well	O
as	O
markov	O
.	O
there	O
is	O
a	O
similar	O
issue	O
regarding	O
how	O
state	B
is	O
obtained	O
and	O
updated	O
.	O
we	O
don	O
’	O
t	O
really	O
want	O
a	O
function	O
f	O
that	O
takes	O
whole	O
histories	O
.	O
instead	O
,	O
for	O
computational	O
reasons	O
we	O
prefer	O
to	O
obtain	O
the	O
same	O
eﬀect	O
as	O
f	O
with	O
an	O
incremental	O
,	O
recursive	O
update	O
that	O
computes	O
st+1	O
from	O
st	O
,	O
incorporating	O
the	O
next	O
increment	O
of	O
data	O
,	O
at	O
and	O
ot+1	O
:	O
st+1	O
=	O
u	O
(	O
st	O
,	O
at	O
,	O
ot+1	O
)	O
,	O
for	O
all	O
t	O
≥	O
0	O
,	O
with	O
the	O
ﬁrst	O
state	B
s0	O
given	O
.	O
the	O
function	O
u	O
is	O
called	O
the	O
state-update	B
function	I
.	O
for	O
example	O
,	O
if	O
f	O
were	O
the	O
identity	O
(	O
st	O
=	O
ht	O
)	O
,	O
then	O
u	O
would	O
merely	O
extend	O
st	O
by	O
appending	O
at	O
and	O
ot+1	O
to	O
it	O
.	O
given	O
f	O
,	O
it	O
is	O
always	O
possible	O
to	O
construct	O
a	O
corresponding	O
u	O
,	O
but	O
it	O
may	O
not	O
be	O
computationally	O
convenient	O
and	O
,	O
as	O
in	O
the	O
identity	O
example	O
,	O
it	O
may	O
not	O
produce	O
a	O
compact	O
state	B
.	O
the	O
state-update	B
function	I
is	O
a	O
central	O
part	O
of	O
any	O
agent	O
architecture	O
that	O
handles	O
partial	O
observability	O
.	O
it	O
must	O
be	O
eﬃciently	O
computatible	O
,	O
as	O
no	O
actions	O
or	O
predictions	O
can	O
be	O
made	O
until	O
the	O
state	B
is	O
available	O
.	O
an	O
example	O
of	O
obtaining	O
markov	O
states	O
through	O
a	O
state-update	B
function	I
is	O
provided	O
by	O
the	O
popular	O
bayesian	O
approach	O
known	O
as	O
partially	O
observable	O
mdps	O
,	O
or	O
pomdps	O
.	O
in	O
this	O
approach	O
the	O
environment	B
is	O
assumed	O
to	O
have	O
a	O
well	O
deﬁned	O
latent	B
state	I
xt	O
that	O
underlies	O
and	O
produces	O
the	O
environment	B
’	O
s	O
observations	O
,	O
but	O
is	O
never	O
available	O
to	O
the	O
agent	O
(	O
and	O
is	O
not	O
to	O
be	O
confused	O
with	O
the	O
state	B
st	O
used	O
by	O
the	O
agent	O
to	O
make	O
predictions	O
and	O
decisions	O
)	O
.	O
the	O
natural	O
markov	O
state	B
st	O
for	O
a	O
pomdp	O
is	O
the	O
distribution	O
over	O
the	O
latent	O
states	O
given	O
the	O
history	O
,	O
called	O
the	O
belief	B
state	I
.	O
for	O
concreteness	O
,	O
assume	O
the	O
usual	O
case	O
in	O
which	O
there	O
are	O
a	O
ﬁnite	O
number	O
of	O
hidden	O
states	O
,	O
xt	O
∈	O
{	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
d	O
}	O
.	O
then	O
the	O
belief	B
state	I
is	O
the	O
vector	B
st	O
.	O
=	O
st	O
∈	O
rd	O
with	O
components	O
st	O
[	O
i	O
]	O
.	O
=	O
pr	O
{	O
xt	O
=	O
i	O
|	O
ht	O
}	O
,	O
for	O
all	O
possible	O
latent	O
states	O
i	O
∈	O
{	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
d	O
}	O
.	O
the	O
belief	B
state	I
remains	O
the	O
same	O
size	O
(	O
same	O
number	O
of	O
components	O
)	O
however	O
t	O
grows	O
.	O
it	O
can	O
also	O
be	O
incrementally	O
updated	O
by	O
bayes	O
’	O
rule	O
,	O
assuming	O
one	O
has	O
complete	O
knowledge	O
of	O
the	O
internal	O
workings	O
of	O
the	O
environment	B
.	O
speciﬁcally	O
,	O
the	O
ith	O
component	O
of	O
the	O
belief-state	O
update	O
function	O
is	O
(	O
cid:80	O
)	O
d	O
x=1	O
(	O
cid:80	O
)	O
d	O
(	O
cid:80	O
)	O
d	O
x=1	O
s	O
[	O
x	O
]	O
p	O
(	O
i	O
,	O
o|x	O
,	O
a	O
)	O
x	O
(	O
cid:48	O
)	O
=1	O
s	O
[	O
x	O
]	O
p	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
o|x	O
,	O
a	O
)	O
,	O
u	O
(	O
s	O
,	O
a	O
,	O
o	O
)	O
[	O
i	O
]	O
=	O
for	O
all	O
a	O
∈	O
a	O
,	O
o	O
∈	O
o	O
,	O
and	O
belief	O
states	O
s	O
∈	O
rd	O
with	O
components	O
s	O
[	O
x	O
]	O
,	O
where	O
the	O
four-	O
argument	O
p	O
function	O
here	O
is	O
not	O
the	O
usual	O
one	O
for	O
mdps	O
(	O
as	O
in	O
chapter	O
3	O
)	O
,	O
but	O
the	O
anal-	O
.	O
ogous	O
one	O
for	O
pomdps	O
,	O
in	O
terms	O
of	O
the	O
latent	B
state	I
:	O
p	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
o|x	O
,	O
a	O
)	O
=	O
pr	O
{	O
xt	O
=	O
x	O
(	O
cid:48	O
)	O
,	O
ot	O
=	O
o	O
|xt−1	O
=	O
x	O
,	O
at−1	O
=	O
a	O
}	O
.	O
this	O
approach	O
is	O
popular	O
in	O
theoretical	O
work	O
and	O
has	O
many	O
signif-	O
icant	O
applications	O
,	O
but	O
its	O
assumptions	O
and	O
computational	O
complexity	O
scale	O
poorly	O
and	O
we	O
do	O
not	O
recommend	O
it	O
as	O
an	O
approach	O
to	O
artiﬁcial	B
intelligence	I
.	O
another	O
example	O
of	O
markov	O
states	O
is	O
provided	O
by	O
predictive	B
state	I
representations	I
,	O
or	O
psrs	O
.	O
psrs	O
address	O
the	O
weakness	O
of	O
the	O
pomdp	O
approach	O
that	O
the	O
semantics	O
of	O
its	O
agent	O
state	B
st	O
are	O
grounded	O
in	O
the	O
environment	O
state	B
,	O
xt	O
,	O
which	O
is	O
never	O
observed	O
and	O
thus	O
is	O
diﬃcult	O
to	O
learn	O
about	O
.	O
in	O
psrs	O
and	O
related	O
approaches	O
,	O
the	O
semantics	O
of	O
17.3.	O
observations	O
and	O
state	O
471	O
the	O
agent	O
state	B
is	O
instead	O
grounded	O
in	O
predictions	O
about	O
future	O
observations	O
and	O
actions	O
,	O
which	O
are	O
readily	O
observable	O
.	O
in	O
psrs	O
,	O
a	O
markov	O
state	B
is	O
deﬁned	O
as	O
a	O
d-vector	O
of	O
the	O
probabilities	O
of	O
d	O
specially	O
chosen	O
“	O
core	O
”	O
tests	O
as	O
deﬁned	O
above	O
(	O
17.5	O
)	O
.	O
the	O
vector	B
is	O
then	O
updated	O
by	O
a	O
state-update	B
function	I
u	O
that	O
is	O
analogous	O
to	O
bayes	O
rule	O
,	O
but	O
with	O
a	O
semantics	O
grounded	O
in	O
observable	O
data	O
,	O
which	O
arguably	O
makes	O
it	O
easier	O
to	O
learn	O
.	O
this	O
approach	O
has	O
been	O
extended	O
in	O
many	O
ways	O
,	O
including	O
end-tests	O
,	O
compositional	O
tests	O
,	O
powerful	O
“	O
spectral	O
”	O
methods	O
,	O
and	O
closed-loop	O
and	O
temporally	O
abstract	O
tests	O
learned	O
by	O
td	O
methods	O
.	O
some	O
of	O
the	O
best	O
theoretical	O
developments	O
are	O
for	O
systems	O
known	O
as	O
observable	O
operator	O
models	O
(	O
ooms	O
)	O
and	O
sequential	O
systems	O
(	O
thon	O
,	O
2017	O
)	O
.	O
the	O
fourth	O
and	O
ﬁnal	O
step	O
in	O
our	O
brief	O
outline	O
of	O
how	O
to	O
handle	O
partial	O
observability	O
in	O
reinforcement	O
learning	O
is	O
to	O
re-introduce	O
approximation	O
.	O
as	O
discussed	O
in	O
the	O
introduction	O
to	O
part	O
ii	O
,	O
to	O
approach	O
artiﬁcial	B
intelligence	I
ambitiously	O
one	O
must	O
embrace	O
approxima-	O
tion	B
.	O
this	O
is	O
just	O
as	O
true	O
for	O
states	O
as	O
it	O
is	O
for	O
value	O
functions	O
.	O
we	O
must	O
accept	O
and	O
work	O
with	O
an	O
approximate	B
notion	O
of	O
state	O
.	O
the	O
approximate	O
state	O
will	O
play	O
the	O
same	O
role	O
in	O
our	O
algorithms	O
as	O
before	O
,	O
so	O
we	O
continue	O
to	O
use	O
the	O
notation	O
st	O
for	O
the	O
state	B
used	O
by	O
the	O
agent	O
,	O
even	O
though	O
it	O
may	O
not	O
be	O
markov	O
.	O
perhaps	O
the	O
simplest	O
example	O
of	O
an	O
approximate	O
state	O
is	O
just	O
the	O
latest	O
observation	O
,	O
.	O
st	O
=	O
ot	O
.	O
of	O
course	O
this	O
approach	O
can	O
not	O
handle	O
any	O
hidden	O
state	B
information	O
.	O
it	O
would	O
.	O
be	O
better	O
to	O
use	O
the	O
last	O
k	O
observations	O
and	O
actions	O
,	O
st	O
=	O
ot	O
,	O
at−1	O
,	O
ot−1	O
,	O
.	O
.	O
.	O
,	O
at−k	O
,	O
for	O
some	O
k	O
≥	O
1	O
,	O
which	O
can	O
be	O
achieved	O
by	O
a	O
state-update	B
function	I
that	O
just	O
shifts	O
the	O
new	O
data	O
in	O
and	O
the	O
oldest	O
data	O
out	O
.	O
this	O
kth-order	B
history	I
approach	I
is	O
still	O
very	O
simple	O
,	O
but	O
can	O
greatly	O
increase	O
the	O
agent	O
’	O
s	O
capabilities	O
compared	O
to	O
trying	O
to	O
use	O
the	O
single	O
immediate	O
observation	O
directly	O
as	O
the	O
state	B
.	O
what	O
happens	O
when	O
the	O
markov	O
property	O
(	O
17.4	O
)	O
is	O
only	O
approximately	O
satisﬁed	O
?	O
un-	O
fortunately	O
,	O
long-term	O
prediction	B
performance	O
can	O
degrade	O
dramatically	O
when	O
the	O
one-	O
step	O
predictions	O
deﬁning	O
the	O
markov	O
property	O
become	O
even	O
slightly	O
inaccurate	O
.	O
longer-	O
term	O
tests	O
,	O
gvfs	O
,	O
and	O
state-update	O
functions	O
may	O
all	O
approximate	B
poorly	O
.	O
the	O
short-	O
term	O
and	O
long-term	O
approximation	O
objectives	O
are	O
just	O
diﬀerent	O
,	O
and	O
there	O
are	O
no	O
useful	O
theoretical	O
guarantees	O
at	O
present	O
.	O
nevertheless	O
,	O
there	O
are	O
still	O
reasons	O
to	O
think	O
that	O
the	O
general	O
idea	O
outlined	O
in	O
this	O
section	O
applies	O
to	O
the	O
approximate	B
case	O
.	O
the	O
general	O
idea	O
is	O
that	O
a	O
state	B
that	O
is	O
good	O
for	O
some	O
predictions	O
is	O
also	O
good	O
for	O
others	O
(	O
in	O
particular	O
,	O
that	O
a	O
markov	O
state	B
,	O
suﬃcient	O
for	O
one-step	O
predictions	O
,	O
is	O
also	O
suﬃcient	O
for	O
all	O
others	O
)	O
.	O
if	O
we	O
step	O
back	O
from	O
that	O
speciﬁc	O
result	O
for	O
the	O
markov	O
case	O
,	O
the	O
general	O
idea	O
is	O
similar	O
to	O
what	O
we	O
discussed	O
in	O
section	O
17.1	O
with	O
multi-headed	O
learning	O
and	O
auxiliary	B
tasks	I
.	O
we	O
discussed	O
how	O
representations	O
that	O
were	O
good	O
for	O
the	O
auxiliary	B
tasks	I
were	O
often	O
also	O
good	O
for	O
the	O
main	O
task	O
.	O
taken	O
together	O
,	O
these	O
suggest	O
an	O
approach	O
to	O
both	O
partial	O
observability	O
and	O
representation	O
learning	O
in	O
which	O
multiple	O
predictions	O
are	O
pursued	O
and	O
used	O
to	O
direct	O
the	O
construction	O
of	O
state	O
features	O
.	O
the	O
guarantee	O
provided	O
by	O
the	O
perfect-but-impractical	O
markov	O
property	O
is	O
replaced	O
by	O
the	O
heuristic	O
that	O
what	O
’	O
s	O
good	O
for	O
some	O
predictions	O
may	O
be	O
good	O
for	O
others	O
.	O
this	O
approach	O
scales	O
well	O
with	O
computational	O
resources	O
.	O
with	O
a	O
large	O
machine	O
one	O
could	O
experiment	O
with	O
large	O
numbers	O
of	O
predictions	O
,	O
perhaps	O
favoring	O
those	O
that	O
are	O
most	O
similar	O
to	O
the	O
ones	O
of	O
ultimate	O
interest	O
or	O
that	O
are	O
easiest	O
to	O
learn	O
reliably	O
,	O
or	O
according	O
to	O
some	O
other	O
criteria	O
.	O
it	O
is	O
important	O
here	O
to	O
move	O
beyond	O
selecting	O
the	O
predictions	O
472	O
chapter	O
17	O
:	O
frontiers	O
manually	O
.	O
the	O
agent	O
should	O
do	O
it	O
.	O
this	O
would	O
require	O
a	O
general	O
language	O
for	O
predictions	O
,	O
so	O
that	O
the	O
agent	O
can	O
systematically	O
explore	O
a	O
large	O
space	O
of	O
possible	O
predictions	O
,	O
sifting	O
through	O
them	O
for	O
the	O
ones	O
that	O
are	O
most	O
useful	O
.	O
in	O
particular	O
,	O
both	O
pomdp	O
and	O
psr	O
approaches	O
can	O
be	O
applied	O
with	O
approximate	O
states	O
.	O
the	O
semantics	O
of	O
the	O
state	B
is	O
useful	O
in	O
forming	O
the	O
state-update	B
function	I
,	O
as	O
it	O
is	O
in	O
these	O
two	O
approaches	O
and	O
in	O
the	O
k-order	O
approach	O
.	O
there	O
is	O
not	O
a	O
strong	O
need	O
for	O
the	O
semantics	O
to	O
be	O
correct	O
in	O
order	O
to	O
retain	O
useful	O
information	O
in	O
the	O
state	O
.	O
some	O
approaches	O
to	O
state	B
augmentation	O
,	O
such	O
as	O
echo	O
state	B
networks	O
(	O
jaeger	O
,	O
2002	O
)	O
,	O
keep	O
almost	O
arbitrary	O
information	O
about	O
the	O
history	O
and	O
can	O
nevertheless	O
perform	O
well	O
.	O
there	O
are	O
many	O
possibilities	O
and	O
we	O
expect	O
more	O
work	O
and	O
ideas	O
in	O
this	O
area	O
.	O
learning	O
the	O
state-update	B
function	I
for	O
an	O
approximate	O
state	O
is	O
a	O
major	O
part	O
of	O
the	O
representation	B
learning	I
problem	O
as	O
it	O
arises	O
in	O
reinforcement	O
learning	O
.	O
17.4	O
designing	O
reward	O
signals	O
a	O
major	O
advantage	O
of	O
reinforcement	O
learning	O
over	O
supervised	B
learning	I
is	O
that	O
reinforce-	O
ment	O
learning	O
does	O
not	O
rely	O
on	O
detailed	O
instructional	O
information	O
:	O
generating	O
a	O
reward	B
signal	I
does	O
not	O
depend	O
on	O
knowledge	O
of	O
what	O
the	O
agent	O
’	O
s	O
correct	O
actions	O
should	O
be	O
.	O
but	O
the	O
success	O
of	O
a	O
reinforcement	B
learning	I
application	O
strongly	O
depends	O
on	O
how	O
well	O
the	O
re-	O
ward	O
signal	O
frames	O
the	O
goal	B
of	O
the	O
application	O
’	O
s	O
designer	O
and	O
how	O
well	O
the	O
signal	O
assesses	O
progress	O
in	O
reaching	O
that	O
goal	B
.	O
for	O
these	O
reasons	O
,	O
designing	O
a	O
reward	B
signal	I
is	O
a	O
critical	O
part	O
of	O
any	O
application	O
of	O
reinforcement	O
learning	O
.	O
by	O
designing	O
a	O
reward	B
signal	I
we	O
mean	O
designing	O
the	O
part	O
of	O
an	O
agent	O
’	O
s	O
environment	B
that	O
is	O
responsible	O
for	O
computing	O
each	O
scalar	O
reward	O
rt	O
and	O
sending	O
it	O
to	O
the	O
agent	O
at	O
each	O
time	O
t.	O
in	O
our	O
discussion	O
of	O
terminology	O
at	O
the	O
end	O
of	O
chapter	O
14	O
,	O
we	O
said	O
that	O
rt	O
is	O
more	O
like	O
a	O
signal	O
generated	O
inside	O
an	O
animal	O
’	O
s	O
brain	O
than	O
it	O
is	O
like	O
an	O
object	O
or	O
event	O
in	O
the	O
animal	O
’	O
s	O
external	O
environment	B
.	O
the	O
parts	O
of	O
our	O
brains	O
that	O
generate	O
these	O
signals	O
for	O
us	O
evolved	O
over	O
millions	O
of	O
years	O
to	O
be	O
well	O
suited	O
to	O
the	O
challenges	O
our	O
ancestors	O
had	O
to	O
face	O
in	O
their	O
struggles	O
to	O
propagate	O
their	O
genes	O
to	O
future	O
generations	O
.	O
we	O
should	O
therefore	O
not	O
think	O
that	O
designing	O
a	O
good	O
reward	B
signal	I
is	O
always	O
an	O
easy	O
thing	O
to	O
do	O
!	O
one	O
challenge	O
is	O
to	O
design	O
a	O
reward	B
signal	I
so	O
that	O
as	O
an	O
agent	O
learns	O
,	O
its	O
behavior	O
approaches	O
,	O
and	O
ideally	O
eventually	O
achieves	O
,	O
what	O
the	O
application	O
’	O
s	O
designer	O
actually	O
desires	O
.	O
this	O
can	O
be	O
easy	O
if	O
the	O
designer	O
’	O
s	O
goal	B
is	O
simple	O
and	O
easy	O
to	O
identify	O
,	O
such	O
as	O
ﬁnding	O
the	O
solution	O
to	O
a	O
well-deﬁned	O
problem	O
or	O
earning	O
a	O
high	O
score	O
in	O
a	O
well-deﬁned	O
game	O
.	O
in	O
cases	O
like	O
these	O
,	O
it	O
is	O
usual	O
to	O
reward	O
the	O
agent	O
according	O
to	O
its	O
success	O
in	O
solving	O
the	O
problem	O
or	O
its	O
success	O
in	O
improving	O
its	O
score	O
.	O
but	O
some	O
problems	O
involve	O
goals	O
that	O
are	O
diﬃcult	O
to	O
translate	O
into	O
reward	O
signals	O
.	O
this	O
is	O
especially	O
true	O
when	O
the	O
problem	O
requires	O
the	O
agent	O
to	O
skillfully	O
perform	O
a	O
complex	O
task	O
or	O
set	O
of	O
tasks	O
,	O
such	O
as	O
would	O
be	O
required	O
of	O
a	O
useful	O
household	O
robotic	O
assistant	O
.	O
further	O
,	O
reinforcement	B
learning	I
agents	O
can	O
discover	O
unexpected	O
ways	O
to	O
make	O
their	O
environments	O
deliver	O
reward	O
,	O
some	O
of	O
which	O
might	O
be	O
undesirable	O
,	O
or	O
even	O
dangerous	O
.	O
this	O
is	O
a	O
longstanding	O
and	O
critical	O
challenge	O
for	O
any	O
method	O
,	O
like	O
reinforcement	B
learning	I
,	O
that	O
is	O
based	O
on	O
optimization	O
.	O
we	O
discuss	O
this	O
issue	O
more	O
in	O
section	O
17.6	O
,	O
the	O
ﬁnal	O
section	O
of	O
this	O
book	O
.	O
even	O
when	O
there	O
is	O
a	O
simple	O
and	O
easily	O
identiﬁable	O
goal	B
,	O
the	O
problem	O
of	O
sparse	O
reward	O
17.4.	O
designing	O
reward	O
signals	O
473	O
often	O
arises	O
.	O
delivering	O
non-zero	O
reward	O
frequently	O
enough	O
to	O
allow	O
the	O
agent	O
to	O
achieve	O
the	O
goal	B
once	O
,	O
let	O
alone	O
to	O
learn	O
to	O
achieve	O
it	O
eﬃciently	O
from	O
multiple	O
initial	O
conditions	O
,	O
can	O
be	O
a	O
daunting	O
challenge	O
.	O
state–action	O
pairs	O
that	O
clearly	O
deserve	O
to	O
trigger	O
reward	O
may	O
be	O
few	O
and	O
far	O
between	O
,	O
and	O
rewards	O
that	O
mark	O
progress	O
toward	O
a	O
goal	B
can	O
be	O
infrequent	O
because	O
progress	O
is	O
diﬃcult	O
or	O
even	O
impossible	O
to	O
detect	O
.	O
the	O
agent	O
may	O
wander	O
aimlessly	O
for	O
long	O
periods	O
of	O
time	O
(	O
what	O
minsky	O
,	O
1961	O
,	O
called	O
the	O
“	O
plateau	O
problem	O
”	O
)	O
.	O
in	O
practice	O
,	O
designing	O
a	O
reward	B
signal	I
is	O
often	O
left	O
to	O
an	O
informal	O
trial-and-error	B
search	O
for	O
a	O
signal	O
that	O
produces	O
acceptable	O
results	O
.	O
if	O
the	O
agent	O
fails	O
to	O
learn	O
,	O
learns	O
too	O
slowly	O
,	O
or	O
learns	O
the	O
wrong	O
thing	O
,	O
then	O
the	O
designer	O
of	O
the	O
application	O
tweaks	O
the	O
reward	B
signal	I
and	O
tries	O
again	O
.	O
to	O
do	O
this	O
,	O
the	O
designer	O
judges	O
the	O
agent	O
’	O
s	O
performance	O
by	O
criteria	O
that	O
he	O
or	O
she	O
is	O
attempting	O
to	O
translate	O
into	O
a	O
reward	B
signal	I
so	O
that	O
the	O
agent	O
’	O
s	O
goal	B
matches	O
his	O
or	O
her	O
own	O
.	O
and	O
if	O
learning	O
is	O
too	O
slow	O
,	O
the	O
designer	O
may	O
try	O
to	O
design	O
a	O
non-sparse	O
reward	B
signal	I
that	O
eﬀectively	O
guides	O
learning	O
throughout	O
the	O
agent	O
’	O
s	O
interaction	O
with	O
its	O
environment	B
.	O
it	O
is	O
tempting	O
to	O
address	O
the	O
sparse	B
reward	O
problem	O
by	O
rewarding	O
the	O
agent	O
for	O
achiev-	O
ing	B
subgoals	O
that	O
the	O
designer	O
thinks	O
are	O
important	O
way	O
stations	O
to	O
the	O
overall	O
goal	B
.	O
but	O
augmenting	O
the	O
reward	B
signal	I
with	O
well-intentioned	O
supplemental	O
rewards	O
may	O
lead	O
the	O
agent	O
to	O
behave	O
very	O
diﬀerently	O
from	O
what	O
is	O
intended	O
;	O
the	O
agent	O
may	O
end	O
up	O
not	O
achiev-	O
ing	B
the	O
overall	O
goal	B
at	O
all	O
.	O
a	O
better	O
way	O
to	O
provide	O
such	O
guidance	O
is	O
to	O
leave	O
the	O
reward	B
signal	I
alone	O
and	O
instead	O
augment	O
the	O
value-function	B
approximation	I
with	O
an	O
initial	O
guess	O
of	O
what	O
it	O
should	O
ultimately	O
be	O
,	O
or	O
augment	O
it	O
with	O
initial	O
guesses	O
as	O
to	O
what	O
certain	O
parts	O
of	O
it	O
should	O
be	O
.	O
for	O
example	O
,	O
suppose	O
one	O
wants	O
to	O
oﬀer	O
v0	O
:	O
s	O
→	O
r	O
as	O
an	O
initial	O
guess	O
at	O
the	O
true	O
optimal	O
value	B
function	I
v∗	O
,	O
and	O
that	O
one	O
is	O
using	O
linear	O
function	O
ap-	O
proximation	O
with	O
features	O
x	O
:	O
s	O
→	O
rd	O
.	O
then	O
one	O
would	O
deﬁne	O
the	O
initial	O
value	B
function	I
approximation	O
to	O
be	O
ˆv	O
(	O
s	O
,	O
w	O
)	O
.	O
=	O
w	O
(	O
cid:62	O
)	O
x	O
(	O
s	O
)	O
+	O
v0	O
(	O
s	O
)	O
,	O
(	O
17.6	O
)	O
and	O
update	O
the	O
weights	O
w	O
as	O
usual	O
.	O
if	O
the	O
initial	O
weight	O
vector	B
is	O
0	O
,	O
then	O
the	O
initial	O
value	B
function	I
will	O
be	O
v0	O
,	O
but	O
the	O
asymptotic	O
solution	O
quality	O
will	O
be	O
determined	O
by	O
the	O
feature	O
vectors	O
as	O
usual	O
.	O
this	O
initialization	O
can	O
be	O
done	O
for	O
arbitrary	O
nonlinear	O
approximators	O
and	O
arbitrary	O
forms	O
of	O
v0	O
,	O
though	O
it	O
is	O
not	O
guaranteed	O
to	O
always	O
accelerate	O
learning	O
.	O
a	O
particularly	O
eﬀective	O
approach	O
to	O
the	O
sparse	B
reward	O
problem	O
is	O
the	O
shaping	B
tech-	O
nique	O
introduced	O
by	O
the	O
psychologist	O
b.	O
f.	O
skinner	O
and	O
described	O
in	O
section	O
14.3.	O
the	O
eﬀectiveness	O
of	O
this	O
technique	O
relies	O
on	O
the	O
fact	O
that	O
sparse	B
reward	O
problems	O
are	O
not	O
just	O
problems	O
with	O
the	O
reward	B
signal	I
;	O
they	O
are	O
also	O
problems	O
with	O
an	O
agent	O
’	O
s	O
policy	B
in	O
preventing	O
the	O
agent	O
from	O
frequently	O
encountering	O
rewarding	O
states	O
.	O
shaping	B
involves	O
changing	O
the	O
reward	B
signal	I
as	O
learning	O
proceeds	O
,	O
starting	O
from	O
a	O
reward	B
signal	I
that	O
is	O
not	O
sparse	B
given	O
the	O
agent	O
’	O
s	O
initial	O
behavior	O
,	O
and	O
gradually	O
modifying	O
it	O
toward	O
the	O
re-	O
ward	O
signal	O
suited	O
to	O
problem	O
of	O
original	O
interest	O
.	O
each	O
modiﬁcation	O
is	O
made	O
so	O
that	O
the	O
agent	O
is	O
frequently	O
rewarded	O
given	O
its	O
current	O
behavior	O
.	O
the	O
agent	O
faces	O
a	O
sequence	O
of	O
increasingly-diﬃcult	O
reinforcement	B
learning	I
problems	O
,	O
where	O
what	O
is	O
learned	O
at	O
each	O
stage	O
makes	O
the	O
next-harder	O
problem	O
relatively	O
easy	O
because	O
the	O
agent	O
now	O
encounters	O
reward	O
more	O
frequently	O
than	O
it	O
would	O
if	O
it	O
did	O
not	O
have	O
prior	O
experience	O
with	O
easier	O
problems	O
.	O
this	O
kind	O
of	O
shaping	O
is	O
an	O
essential	O
technique	O
in	O
training	O
animals	O
,	O
and	O
it	O
is	O
eﬀective	O
in	O
computational	O
reinforcement	B
learning	I
as	O
well	O
.	O
474	O
chapter	O
17	O
:	O
frontiers	O
what	O
if	O
one	O
has	O
no	O
idea	O
what	O
the	O
rewards	O
should	O
be	O
but	O
there	O
is	O
another	O
agent	O
,	O
per-	O
haps	O
a	O
person	O
,	O
who	O
is	O
already	O
expert	O
at	O
the	O
task	O
and	O
whose	O
behavior	O
can	O
be	O
observed	O
?	O
in	O
this	O
case	O
one	O
can	O
use	O
methods	O
known	O
variously	O
as	O
“	O
imitation	O
learning	O
,	O
”	O
“	O
learning	O
from	O
demonstration	O
,	O
”	O
and	O
“	O
apprenticeship	O
learning.	O
”	O
the	O
idea	O
here	O
is	O
to	O
beneﬁt	O
from	O
the	O
ex-	O
pert	O
agent	O
but	O
leave	O
open	O
the	O
possibility	O
of	O
eventually	O
performing	O
better	O
.	O
learning	O
from	O
an	O
expert	O
’	O
s	O
behavior	O
can	O
be	O
done	O
either	O
by	O
learning	O
directly	O
by	O
supervised	B
learning	I
or	O
by	O
extracting	O
a	O
reward	B
signal	I
using	O
what	O
is	O
known	O
as	O
“	O
inverse	B
reinforcement	I
learning	I
”	O
and	O
then	O
using	O
a	O
reinforcement	B
learning	I
algorithm	O
with	O
that	O
reward	B
signal	I
to	O
learn	O
a	O
policy	B
.	O
the	O
task	O
of	O
inverse	O
reinforcement	B
learning	I
as	O
explored	O
by	O
ng	O
and	O
russell	O
(	O
2000	O
)	O
is	O
to	O
try	O
to	O
recover	O
the	O
expert	O
’	O
s	O
reward	B
signal	I
from	O
the	O
expert	O
’	O
s	O
behavior	O
alone	O
.	O
this	O
can	O
not	O
be	O
done	O
exactly	O
because	O
a	O
policy	B
can	O
be	O
optimal	O
with	O
respect	O
to	O
many	O
diﬀer-	O
ent	O
reward	O
signals	O
(	O
for	O
example	O
,	O
any	O
reward	B
signal	I
that	O
gives	O
the	O
same	O
reward	O
for	O
all	O
states	O
and	O
actions	O
)	O
,	O
but	O
it	O
is	O
possible	O
to	O
ﬁnd	O
plausible	O
reward	B
signal	I
candidates	O
.	O
un-	O
fortunately	O
,	O
strong	O
assumptions	O
are	O
required	O
,	O
including	O
knowledge	O
of	O
the	O
environment	B
’	O
s	O
dynamics	O
and	O
of	O
the	O
feature	O
vectors	O
in	O
which	O
the	O
reward	B
signal	I
is	O
linear	O
.	O
the	O
method	O
also	O
requires	O
completely	O
solving	O
the	O
problem	O
(	O
e.g.	O
,	O
by	O
dynamic	B
programming	I
methods	O
)	O
multiple	O
times	O
.	O
these	O
diﬃculties	O
notwithstanding	O
,	O
abbeel	O
and	O
ng	O
(	O
2004	O
)	O
argue	O
that	O
the	O
inverse	B
reinforcement	I
learning	I
approach	O
can	O
sometimes	O
be	O
more	O
eﬀective	O
than	O
supervised	B
learning	I
for	O
beneﬁting	O
from	O
the	O
behavior	O
of	O
an	O
expert	O
.	O
another	O
approach	O
to	O
ﬁnding	O
a	O
good	O
reward	B
signal	I
is	O
to	O
automate	O
the	O
trial-and-error	B
search	O
for	O
a	O
good	O
signal	O
that	O
we	O
mentioned	O
above	O
.	O
from	O
an	O
application	O
perspective	O
,	O
the	O
reward	B
signal	I
is	O
a	O
parameter	O
of	O
the	O
learning	O
algorithm	O
.	O
as	O
is	O
true	O
for	O
other	O
algorithm	O
parameters	O
,	O
the	O
search	O
for	O
a	O
good	O
reward	B
signal	I
can	O
be	O
automated	O
by	O
deﬁning	O
a	O
space	O
of	O
feasible	O
candidates	O
and	O
applying	O
an	O
optimization	O
algorithm	O
.	O
the	O
optimization	O
algorithm	O
evaluates	O
each	O
candidate	O
reward	B
signal	I
by	O
running	O
the	O
reinforcement	B
learning	I
system	O
with	O
that	O
signal	O
for	O
some	O
number	O
of	O
steps	O
,	O
and	O
then	O
scoring	O
the	O
overall	O
result	O
by	O
a	O
“	O
high-level	O
”	O
objective	O
function	O
intended	O
to	O
encode	O
the	O
designer	O
’	O
s	O
true	O
goal	O
,	O
ignoring	O
the	O
limitations	O
of	O
the	O
agent	O
.	O
reward	O
signals	O
can	O
even	O
be	O
improved	O
via	O
online	B
gradient	O
ascent	O
,	O
where	O
the	O
gradient	B
is	O
that	O
of	O
the	O
high-level	O
objective	O
function	O
(	O
sorg	O
,	O
lewis	O
,	O
and	O
singh	O
,	O
2010	O
)	O
.	O
relating	O
this	O
approach	O
to	O
the	O
natural	O
world	O
,	O
the	O
algorithm	O
for	O
optimizing	O
the	O
high-level	O
objective	O
function	O
is	O
analogous	O
to	O
evolution	B
,	O
where	O
the	O
high-level	O
objective	O
function	O
is	O
an	O
animal	O
’	O
s	O
evolutionary	O
ﬁtness	O
determined	O
by	O
the	O
number	O
of	O
its	O
oﬀspring	O
that	O
survive	O
to	O
reproductive	O
age	O
.	O
computational	O
experiments	O
with	O
this	O
bilevel	O
optimization	O
approach—one	O
level	O
anal-	O
ogous	O
to	O
evolution	B
,	O
and	O
the	O
other	O
due	O
to	O
reinforcement	B
learning	I
by	O
individual	O
agents—	O
have	O
conﬁrmed	O
that	O
intuition	O
alone	O
is	O
not	O
always	O
adequate	O
to	O
devise	O
a	O
good	O
reward	B
signal	I
(	O
singh	O
,	O
lewis	O
,	O
and	O
barto	O
,	O
2009	O
)	O
.	O
the	O
performance	O
of	O
a	O
reinforcement	B
learning	I
agent	O
as	O
evaluated	O
by	O
the	O
high-level	O
objective	O
function	O
can	O
be	O
very	O
sensitive	O
to	O
details	O
of	O
the	O
agent	O
’	O
s	O
reward	B
signal	I
in	O
subtle	O
ways	O
determined	O
by	O
the	O
agent	O
’	O
s	O
limitations	O
and	O
the	O
environments	O
in	O
which	O
it	O
acts	O
and	O
learns	O
.	O
these	O
experiments	O
also	O
demonstrated	O
that	O
an	O
agent	O
’	O
s	O
goal	B
should	O
not	O
always	O
be	O
the	O
same	O
as	O
the	O
goal	B
of	O
the	O
agent	O
’	O
s	O
designer	O
.	O
at	O
ﬁrst	O
this	O
seems	O
counterintuitive	O
,	O
but	O
it	O
may	O
be	O
impossible	O
for	O
the	O
agent	O
to	O
achieve	O
the	O
designer	O
’	O
s	O
goal	B
no	O
matter	O
what	O
its	O
reward	B
signal	I
is	O
.	O
the	O
agent	O
has	O
to	O
learn	O
under	O
various	O
kinds	O
of	O
constraints	O
,	O
such	O
as	O
limited	O
computational	O
power	O
,	O
limited	O
access	O
to	O
infor-	O
17.5.	O
remaining	O
issues	O
475	O
mation	O
about	O
its	O
environment	B
,	O
or	O
limited	O
time	O
to	O
learn	O
.	O
when	O
there	O
are	O
constraints	O
like	O
these	O
,	O
learning	O
to	O
achieve	O
a	O
goal	B
that	O
is	O
diﬀerent	O
from	O
the	O
designer	O
’	O
s	O
goal	B
can	O
sometimes	O
end	O
up	O
getting	O
closer	O
to	O
the	O
designer	O
’	O
s	O
goal	B
than	O
if	O
that	O
goal	B
were	O
pursued	O
directly	O
(	O
sorg	O
,	O
singh	O
,	O
and	O
lewis	O
,	O
2010	O
;	O
sorg	O
,	O
2011	O
)	O
.	O
examples	O
of	O
this	O
in	O
the	O
natural	O
world	O
are	O
easy	O
to	O
ﬁnd	O
.	O
because	O
we	O
can	O
not	O
directly	O
assess	O
the	O
nutritional	O
value	B
of	O
most	O
foods	O
,	O
evolution—	O
the	O
designer	O
of	O
our	O
reward	O
signal—gave	O
us	O
a	O
reward	B
signal	I
that	O
makes	O
us	O
seek	O
certain	O
tastes	O
.	O
though	O
certainly	O
not	O
infallible	O
(	O
indeed	O
,	O
possibly	O
detrimental	O
in	O
environments	O
that	O
diﬀer	O
in	O
certain	O
ways	O
from	O
ancestral	O
environments	O
)	O
,	O
this	O
compensates	O
for	O
many	O
of	O
our	O
limitations	O
:	O
our	O
limited	O
sensory	O
abilities	O
,	O
the	O
limited	O
time	O
over	O
which	O
we	O
can	O
learn	O
,	O
and	O
the	O
risks	O
involved	O
in	O
ﬁnding	O
a	O
healthy	O
diet	O
through	O
personal	O
experimentation	O
.	O
similarly	O
,	O
because	O
an	O
animal	O
can	O
not	O
observe	O
its	O
own	O
evolutionary	O
ﬁtness	O
,	O
that	O
objective	O
function	O
does	O
not	O
work	O
as	O
a	O
reward	B
signal	I
for	O
learning	O
.	O
evolution	B
instead	O
provides	O
reward	O
signals	O
that	O
are	O
sensitive	O
to	O
observable	O
predictors	O
of	O
evolutionary	O
ﬁtness	O
.	O
finally	O
,	O
remember	O
that	O
a	O
reinforcement	B
learning	I
agent	O
is	O
not	O
necessarily	O
like	O
a	O
complete	O
organism	O
or	O
robot	O
;	O
it	O
can	O
be	O
a	O
component	O
of	O
a	O
larger	O
behaving	O
system	O
.	O
this	O
means	O
that	O
reward	O
signals	O
may	O
be	O
inﬂuenced	O
by	O
things	O
inside	O
the	O
larger	O
behaving	O
agent	O
,	O
such	O
as	O
motivational	O
states	O
,	O
memories	O
,	O
ideas	O
,	O
or	O
even	O
hallucinations	O
.	O
reward	O
signals	O
may	O
also	O
depend	O
on	O
properties	O
of	O
the	O
learning	O
process	O
itself	O
,	O
such	O
as	O
measures	O
of	O
how	O
much	O
progress	O
learning	O
is	O
making	O
.	O
making	O
reward	O
signals	O
sensitive	O
to	O
information	O
about	O
internal	O
factors	O
such	O
as	O
these	O
makes	O
it	O
possible	O
for	O
an	O
agent	O
to	O
learn	O
how	O
to	O
control	B
the	O
“	O
cognitive	O
architecture	O
”	O
of	O
which	O
it	O
is	O
a	O
part	O
,	O
as	O
well	O
as	O
to	O
acquire	O
knowledge	O
and	O
skills	O
that	O
would	O
be	O
diﬃcult	O
to	O
learn	O
from	O
a	O
reward	B
signal	I
that	O
depended	O
only	O
on	O
external	O
events	O
.	O
possibilities	O
like	O
these	O
led	O
to	O
the	O
idea	O
of	O
“	O
intrinsically-motivated	O
reinforcement	B
learning	I
”	O
that	O
we	O
brieﬂy	O
discuss	O
further	O
at	O
the	O
end	O
of	O
the	O
following	O
section	O
.	O
17.5	O
remaining	O
issues	O
in	O
this	O
book	O
we	O
have	O
presented	O
the	O
foundations	O
of	O
a	O
reinforcement	B
learning	I
approach	O
to	O
artiﬁcial	B
intelligence	I
.	O
roughly	O
speaking	O
,	O
that	O
approach	O
is	O
based	O
on	O
model-free	O
and	O
model-based	O
methods	O
working	O
together	O
,	O
as	O
in	O
the	O
dyna	O
architecture	O
of	O
chapter	O
8	O
,	O
com-	O
bined	O
with	B
function	I
approximation	I
as	O
developed	O
in	O
part	O
ii	O
.	O
the	O
focus	O
has	O
been	O
on	O
online	B
and	O
incremental	O
algorithms	O
,	O
which	O
we	O
see	O
as	O
fundamental	O
even	O
to	O
model-based	O
meth-	O
ods	O
,	O
and	O
on	O
how	O
these	O
can	O
be	O
applied	O
in	O
oﬀ-policy	O
training	O
situations	O
.	O
the	O
full	O
ratio-	O
nale	O
for	O
the	O
latter	O
has	O
been	O
presented	O
only	O
in	O
this	O
last	O
chapter	O
.	O
that	O
is	O
,	O
we	O
have	O
all	O
along	O
presented	O
oﬀ-policy	B
learning	O
as	O
an	O
appealing	O
way	O
to	O
deal	O
with	O
the	O
explore/exploit	B
dilemma	I
,	O
but	O
only	O
in	O
this	O
chapter	O
have	O
we	O
discussed	O
learning	O
about	O
many	O
diverse	O
aux-	O
iliary	O
tasks	O
simultaneously	O
with	O
gvfs	O
and	O
learning	O
about	O
the	O
world	O
hierarchically	O
in	O
terms	O
of	O
temporally-abstract	O
option	B
models	I
,	O
both	O
of	O
which	O
involve	O
oﬀ-policy	B
learning	O
.	O
much	O
remains	O
to	O
be	O
worked	O
out	O
,	O
as	O
we	O
have	O
indicated	O
throughout	O
the	O
book	O
and	O
as	O
evi-	O
denced	O
by	O
the	O
directions	O
for	O
additional	O
research	O
discussed	O
in	O
this	O
chapter	O
.	O
but	O
suppose	O
we	O
are	O
generous	O
and	O
grant	O
the	O
broad	O
outlines	O
of	O
everything	O
that	O
we	O
have	O
done	O
in	O
the	O
book	O
and	O
everything	O
that	O
has	O
been	O
outlined	O
so	O
far	O
in	O
this	O
chapter	O
.	O
what	O
would	O
remain	O
even	O
after	O
that	O
?	O
of	O
course	O
,	O
we	O
can	O
’	O
t	O
know	O
for	O
sure	O
what	O
will	O
be	O
required	O
,	O
but	O
we	O
can	O
make	O
some	O
guesses	O
.	O
in	O
this	O
section	O
we	O
highlight	O
six	O
further	O
issues	O
which	O
it	O
seems	O
to	O
us	O
476	O
chapter	O
17	O
:	O
frontiers	O
will	O
still	O
need	O
to	O
be	O
addressed	O
by	O
future	O
research	O
.	O
first	O
,	O
we	O
still	O
need	O
powerful	O
parametric	O
function	B
approximation	I
methods	O
that	O
work	O
well	O
in	O
fully	O
incremental	O
and	O
online	B
settings	O
.	O
methods	O
based	O
on	O
deep	B
learning	I
and	O
arti-	O
ﬁcial	O
neural	B
networks	I
are	O
a	O
major	O
step	O
in	O
this	O
direction	O
but	O
,	O
still	O
,	O
only	O
work	O
well	O
with	O
batch	O
training	O
on	O
large	O
data	O
sets	O
,	O
with	O
training	O
from	O
extensive	O
oﬀ-line	B
self	O
play	O
,	O
or	O
with	O
learning	O
from	O
the	O
interleaved	O
experience	O
of	O
multiple	O
agents	O
on	O
the	O
same	O
task	O
.	O
these	O
and	O
other	O
settings	O
are	O
ways	O
of	O
working	O
around	O
a	O
basic	O
limitation	O
of	O
today	O
’	O
s	O
deep	B
learning	I
methods	O
,	O
which	O
struggle	O
to	O
learn	O
rapidly	O
in	O
the	O
incremental	O
,	O
online	B
settings	O
that	O
are	O
most	O
natural	O
for	O
the	O
reinforcement	B
learning	I
algorithms	O
emphasized	O
in	O
this	O
book	O
.	O
the	O
problem	O
is	O
sometimes	O
described	O
as	O
one	O
of	O
“	O
catastrophic	B
interference	I
”	O
or	O
“	O
correlated	O
data.	O
”	O
when	O
something	O
new	O
is	O
learned	O
it	O
tends	O
to	O
replace	O
what	O
has	O
previously	O
been	O
learned	O
rather	O
than	O
adding	O
to	O
it	O
,	O
with	O
the	O
result	O
that	O
the	O
beneﬁt	O
of	O
the	O
older	O
learning	O
is	O
lost	O
.	O
techniques	O
such	O
as	O
“	O
replay	O
buﬀers	O
”	O
are	O
often	O
used	O
to	O
retain	O
and	O
replay	O
old	O
data	O
so	O
that	O
its	O
beneﬁts	O
are	O
not	O
permanently	O
lost	O
.	O
an	O
honest	O
assessment	O
has	O
to	O
be	O
that	O
current	O
deep	B
learning	I
methods	O
are	O
not	O
well	O
suited	O
to	O
online	B
learning	O
.	O
we	O
see	O
no	O
reason	O
that	O
this	O
limitation	O
is	O
insurmountable	O
,	O
but	O
algorithms	O
that	O
address	O
it	O
,	O
while	O
at	O
the	O
same	O
time	O
retaining	O
the	O
advantages	B
of	I
deep	O
learning	O
,	O
have	O
not	O
yet	O
been	O
devised	O
.	O
most	O
current	O
deep	B
learning	I
research	O
is	O
directed	O
toward	O
working	O
around	O
this	O
limitation	O
rather	O
than	O
removing	O
it	O
.	O
second	O
(	O
and	O
perhaps	O
closely	O
related	O
)	O
,	O
we	O
still	O
need	O
methods	O
for	O
learning	O
features	O
such	O
that	O
subsequent	O
learning	O
generalizes	O
well	O
.	O
this	O
issue	O
is	O
an	O
instance	O
of	O
a	O
general	O
problem	O
variously	O
called	O
“	O
representation	B
learning	I
,	O
”	O
“	O
constructive	O
induction	O
,	O
”	O
and	O
“	O
meta-	O
learning	O
”	O
—how	O
can	O
we	O
use	O
experience	O
not	O
just	O
to	O
learn	O
a	O
given	O
desired	O
function	O
,	O
but	O
to	O
learn	O
inductive	O
biases	O
such	O
that	O
future	O
learning	O
generalizes	O
better	O
and	O
is	O
thus	O
faster	O
?	O
this	O
is	O
an	O
old	O
problem	O
,	O
dating	O
back	O
to	O
the	O
origins	O
of	O
artiﬁcial	O
intelligence	O
and	O
pattern	O
recognition	O
in	O
the	O
1950s	O
and	O
1960s.1	O
such	O
age	O
should	O
give	O
one	O
pause	O
.	O
perhaps	O
there	O
is	O
no	O
solution	O
.	O
but	O
it	O
is	O
equally	O
likely	O
that	O
the	O
time	O
for	O
ﬁnding	O
a	O
solution	O
and	O
demonstrating	O
its	O
eﬀectiveness	O
has	O
not	O
yet	O
arrived	O
.	O
today	O
machine	O
learning	O
is	O
conducted	O
at	O
a	O
far	O
larger	O
scale	O
than	O
it	O
has	O
been	O
in	O
the	O
past	O
,	O
and	O
the	O
potential	O
beneﬁts	O
of	O
a	O
good	O
representation	B
learning	I
method	O
have	O
become	O
much	O
more	O
apparent	O
.	O
we	O
note	O
that	O
a	O
new	O
annual	O
conference—	O
the	O
international	O
conference	O
on	O
learning	O
representations—has	O
been	O
exploring	O
this	O
and	O
related	O
topics	O
every	O
year	O
since	O
2013.	O
it	O
is	O
also	O
less	O
common	O
to	O
explore	O
representation	B
learning	I
within	O
a	O
reinforcement	B
learning	I
context	O
.	O
reinforcement	B
learning	I
brings	O
some	O
new	O
possibilities	O
to	O
this	O
old	O
issue	O
,	O
such	O
as	O
the	O
auxiliary	B
tasks	I
discussed	O
in	O
section	O
17.1.	O
in	O
reinforcement	O
learning	O
,	O
the	O
problem	O
of	O
representation	O
learning	O
can	O
be	O
identiﬁed	O
with	O
the	O
problem	O
of	O
learning	O
the	O
state-update	B
function	I
discussed	O
in	O
section	O
17.3.	O
third	O
,	O
we	O
still	O
need	O
scalable	O
methods	O
for	O
planning	O
with	O
learned	O
environment	O
models	O
.	O
planning	B
methods	O
have	O
proven	O
extremely	O
eﬀective	O
in	O
applications	O
such	O
as	O
alphago	O
zero	O
and	O
computer	O
chess	B
in	O
which	O
the	O
model	B
of	I
the	I
environment	I
is	O
known	O
from	O
the	O
rules	O
of	O
the	O
game	O
or	O
can	O
otherwise	O
be	O
supplied	O
by	O
human	O
designers	O
.	O
but	O
cases	O
of	O
full	O
model-based	B
reinforcement	I
learning	I
,	O
in	O
which	O
the	O
environment	B
model	O
is	O
learned	O
from	O
data	O
and	O
then	O
used	O
for	O
planning	O
,	O
are	O
rare	O
.	O
the	O
dyna	O
system	O
described	O
in	O
chapter	O
8	O
is	O
one	O
example	O
,	O
but	O
1some	O
would	O
claim	O
that	O
deep	B
learning	I
solves	O
this	O
problem	O
,	O
for	O
example	O
,	O
that	O
dqn	O
as	O
described	O
in	O
section	O
16.5	O
illustrates	O
a	O
solution	O
,	O
but	O
we	O
are	O
unconvinced	O
.	O
there	O
is	O
as	O
yet	O
little	O
evidence	O
that	O
deep	B
learning	I
alone	O
solves	O
the	O
representation	B
learning	I
problem	O
in	O
a	O
general	O
and	O
eﬃcient	O
way	O
.	O
17.5.	O
remaining	O
issues	O
477	O
as	O
described	O
there	O
and	O
in	O
most	O
subsequent	O
work	O
it	O
uses	O
a	O
tabular	O
model	O
without	O
function	B
approximation	I
,	O
which	O
greatly	O
limits	O
its	O
applicability	O
.	O
only	O
a	O
few	O
studies	O
have	O
included	O
learned	O
linear	O
models	O
,	O
and	O
even	O
fewer	O
have	O
also	O
explored	O
including	O
temporally-abstract	O
models	O
using	O
options	B
as	O
discussed	O
in	O
section	O
17.2.	O
more	O
work	O
is	O
needed	O
before	O
planning	O
with	O
learned	O
models	O
can	O
be	O
eﬀective	O
.	O
for	O
exam-	O
ple	O
,	O
the	O
learning	O
of	O
the	O
model	O
needs	O
to	O
be	O
selective	O
because	O
the	O
scope	O
of	O
a	O
model	O
strongly	O
aﬀects	O
planning	B
eﬃciency	O
.	O
if	O
a	O
model	O
focuses	O
on	O
the	O
key	O
consequences	O
of	O
the	O
most	O
impor-	O
tant	O
options	B
,	O
then	O
planning	B
can	O
be	O
eﬃcient	O
and	O
rapid	O
,	O
but	O
if	O
a	O
model	O
includes	O
details	O
of	O
unimportant	O
consequences	O
of	O
options	O
that	O
are	O
unlikely	O
to	O
be	O
selected	O
,	O
then	O
planning	B
may	O
be	O
almost	O
useless	O
.	O
environment	B
models	O
should	O
be	O
constructed	O
judiciously	O
with	O
regard	O
to	O
both	O
their	O
states	O
and	O
dynamics	O
with	O
the	O
goal	B
of	O
optimizing	O
the	O
planning	B
process	O
.	O
the	O
various	O
parts	O
of	O
the	O
model	O
should	O
be	O
continually	O
monitored	O
as	O
to	O
the	O
degree	O
to	O
which	O
they	O
contribute	O
to	O
,	O
or	O
detract	O
from	O
,	O
planning	B
eﬃciency	O
.	O
the	O
ﬁeld	O
has	O
not	O
yet	O
addressed	O
this	O
complex	O
of	O
issues	O
or	O
designed	O
model-learning	O
methods	O
that	O
take	O
into	O
account	O
their	O
implications	O
.	O
a	O
fourth	O
issue	O
that	O
needs	O
to	O
be	O
addressed	O
in	O
future	O
research	O
is	O
that	O
of	O
automating	O
the	O
choice	O
of	O
tasks	O
on	O
which	O
an	O
agent	O
works	O
and	O
uses	O
to	O
structure	O
its	O
developing	O
competence	O
.	O
it	O
is	O
usual	O
in	O
machine	O
learning	O
for	O
human	O
designers	O
to	O
set	O
the	O
tasks	O
that	O
the	O
learning	O
agent	O
is	O
expected	O
to	O
master	O
.	O
because	O
these	O
tasks	O
are	O
known	O
in	O
advance	O
and	O
remain	O
ﬁxed	O
,	O
they	O
can	O
be	O
built	O
into	O
the	O
learning	O
algorithm	O
code	O
.	O
however	O
,	O
looking	O
ahead	O
,	O
we	O
will	O
want	O
the	O
agent	O
to	O
make	O
its	O
own	O
choices	O
about	O
what	O
tasks	O
it	O
should	O
try	O
to	O
master	O
.	O
these	O
might	O
be	O
subtasks	O
of	O
a	O
speciﬁc	O
overall	O
task	O
that	O
is	O
already	O
known	O
,	O
or	O
they	O
might	O
be	O
intended	O
to	O
create	O
building	O
blocks	O
that	O
permit	O
more	O
eﬃcient	O
learning	O
of	O
many	O
diﬀerent	O
tasks	O
that	O
the	O
agent	O
is	O
likely	O
to	O
face	O
in	O
the	O
future	O
but	O
which	O
are	O
currently	O
unknown	O
.	O
these	O
tasks	O
may	O
be	O
like	O
the	O
auxiliary	B
tasks	I
or	O
the	O
gvfs	O
discussed	O
in	O
section	O
17.1	O
,	O
or	O
tasks	O
solved	O
by	O
options	B
as	O
discussed	O
in	O
section	O
17.2.	O
in	O
forming	O
a	O
gvf	O
,	O
for	O
example	O
,	O
what	O
should	O
the	O
cumulant	O
,	O
the	O
policy	B
,	O
and	O
the	O
termination	O
function	O
be	O
?	O
the	O
current	O
state	B
of	O
the	O
art	O
is	O
to	O
select	O
these	O
manually	O
,	O
but	O
far	O
greater	O
power	O
and	O
generality	O
would	O
come	O
from	O
making	O
these	O
task	O
choices	O
automatically	O
,	O
particularly	O
when	O
they	O
derive	O
from	O
what	O
the	O
agent	O
has	O
previously	O
constructed	O
as	O
a	O
result	O
of	O
representation	O
learning	O
or	O
experience	O
with	O
previous	O
subproblems	O
.	O
if	O
gvf	O
design	O
is	O
automated	O
,	O
then	O
the	O
design	O
choices	O
themselves	O
will	O
have	O
to	O
be	O
explicitly	O
represented	O
.	O
rather	O
than	O
the	O
task	O
choices	O
being	O
in	O
the	O
mind	O
of	O
the	O
designer	O
and	O
built	O
into	O
the	O
code	O
,	O
they	O
will	O
have	O
to	O
be	O
in	O
the	O
machine	O
itself	O
in	O
such	O
a	O
way	O
that	O
they	O
can	O
be	O
set	O
and	O
changed	O
,	O
monitored	O
,	O
ﬁltered	O
,	O
and	O
searched	O
among	O
automatically	O
.	O
tasks	O
could	O
then	O
be	O
built	O
hierarchically	O
upon	O
others	O
much	O
like	O
features	O
are	O
in	O
an	O
artiﬁcial	O
neural	O
network	O
.	O
the	O
tasks	O
are	O
the	O
questions	O
,	O
and	O
the	O
contents	O
of	O
the	O
neural	B
network	O
are	O
the	O
answers	O
to	O
those	O
questions	O
.	O
we	O
expect	O
there	O
will	O
need	O
to	O
be	O
a	O
full	O
hierarchy	O
of	O
questions	O
to	O
match	O
the	O
hierarchy	O
of	O
answers	O
provided	O
by	O
modern	O
deep	B
learning	I
methods	O
.	O
the	O
ﬁfth	O
issue	O
that	O
we	O
would	O
like	O
to	O
highlight	O
for	O
future	O
research	O
is	O
that	O
of	O
the	O
interac-	O
tion	B
between	O
behavior	O
and	O
learning	O
via	O
some	O
computational	O
analog	O
of	O
curiosity	O
.	O
in	O
this	O
chapter	O
we	O
have	O
been	O
imagining	O
a	O
setting	O
in	O
which	O
many	O
tasks	O
are	O
being	O
learned	O
simul-	O
taneously	O
,	O
using	O
oﬀ-policy	B
methods	I
,	O
from	O
the	O
same	O
stream	O
of	O
experience	O
.	O
the	O
actions	O
taken	O
will	O
of	O
course	O
inﬂuence	O
this	O
stream	O
of	O
experience	O
,	O
which	O
in	O
turn	O
will	O
determine	O
478	O
chapter	O
17	O
:	O
frontiers	O
how	O
much	O
learning	O
occurs	O
and	O
which	O
tasks	O
are	O
learned	O
.	O
when	O
reward	O
is	O
not	O
available	O
,	O
or	O
not	O
strongly	O
inﬂuenced	O
by	O
behavior	O
,	O
the	O
agent	O
is	O
free	O
to	O
choose	O
actions	O
that	O
maximize	O
in	O
some	O
sense	O
the	O
learning	O
on	O
the	O
tasks	O
,	O
that	O
is	O
,	O
to	O
use	O
some	O
measure	O
of	O
learning	O
progress	O
as	O
an	O
internal	O
or	O
“	O
intrinsic	B
”	O
reward	O
,	O
implementing	O
a	O
computational	O
form	O
of	O
curiosity	O
.	O
in	O
addition	O
to	O
measuring	O
learning	O
progress	O
,	O
intrinsic	B
reward	O
can	O
,	O
among	O
other	O
possibilities	O
,	O
signal	O
the	O
receipt	O
of	O
unexpected	O
,	O
novel	O
,	O
or	O
otherwise	O
interesting	O
input	O
,	O
or	O
can	O
assess	O
the	O
agent	O
’	O
s	O
ability	O
to	O
cause	O
changes	O
in	O
its	O
environment	B
.	O
intrinsic	B
reward	O
signals	O
generated	O
in	O
these	O
ways	O
can	O
be	O
used	O
by	O
an	O
agent	O
to	O
pose	O
tasks	O
for	O
itself	O
by	O
deﬁning	O
auxiliary	B
tasks	I
,	O
gvfs	O
,	O
or	O
options	B
,	O
as	O
discussed	O
above	O
,	O
so	O
that	O
skills	O
learned	O
in	O
this	O
way	O
can	O
contribute	O
to	O
the	O
agent	O
’	O
s	O
ability	O
to	O
master	O
future	O
tasks	O
.	O
the	O
result	O
is	O
a	O
computational	O
analog	O
of	O
something	O
like	O
play	O
.	O
many	O
preliminary	O
studies	O
of	O
such	O
uses	O
of	O
intrinsic	O
reward	O
signals	O
have	O
been	O
conducted	O
,	O
and	O
exciting	O
topics	O
for	O
future	O
research	O
remain	O
in	O
this	O
general	O
area	O
.	O
a	O
ﬁnal	O
issue	O
that	O
demands	O
attention	O
in	O
future	O
research	O
is	O
that	O
of	O
developing	O
methods	O
to	O
make	O
it	O
acceptably	O
safe	O
to	O
embed	O
reinforcement	B
learning	I
agents	O
into	O
physical	O
environ-	O
ments	O
,	O
thereby	O
helping	O
to	O
ensure	O
that	O
the	O
beneﬁts	O
of	O
reinforcement	O
learning	O
outweigh	O
harm	O
it	O
can	O
cause	O
.	O
this	O
is	O
one	O
of	O
the	O
most	O
pressing	O
areas	O
for	O
future	O
research	O
,	O
and	O
we	O
discuss	O
it	O
further	O
in	O
the	O
following	O
section	O
.	O
17.6	O
reinforcement	B
learning	I
and	O
the	O
future	O
of	O
arti-	O
ﬁcial	O
intelligence	O
when	O
we	O
were	O
writing	O
the	O
ﬁrst	O
edition	O
of	O
this	O
book	O
in	O
the	O
mid-1990s	O
,	O
artiﬁcial	O
intelli-	O
gence	O
was	O
making	O
signiﬁcant	O
progress	O
and	O
was	O
having	O
an	O
impact	O
on	O
society	O
,	O
though	O
it	O
was	O
mostly	O
still	O
the	O
promise	O
of	O
artiﬁcial	O
intelligence	O
that	O
was	O
inspiring	O
developments	O
.	O
machine	O
learning	O
was	O
part	O
of	O
that	O
outlook	O
,	O
but	O
it	O
had	O
not	O
yet	O
become	O
indispensable	O
to	O
artiﬁcial	B
intelligence	I
.	O
by	O
today	O
that	O
promise	O
has	O
transitioned	O
to	O
applications	O
that	O
are	O
changing	O
the	O
lives	O
of	O
millions	O
of	O
people	O
,	O
and	O
machine	O
learning	O
has	O
come	O
into	O
its	O
own	O
as	O
a	O
key	O
technology	O
.	O
as	O
we	O
write	O
this	O
second	O
edition	O
,	O
some	O
of	O
the	O
most	O
remarkable	O
develop-	O
ments	O
in	O
artiﬁcial	O
intelligence	O
have	O
involved	O
reinforcement	B
learning	I
,	O
most	O
notably	O
“	O
deep	B
reinforcement	I
learning	I
”	O
—reinforcement	O
learning	O
with	O
function	B
approximation	I
by	O
deep	O
neural	O
networks	O
.	O
we	O
are	O
at	O
the	O
beginning	O
of	O
a	O
wave	O
of	O
real-world	O
applications	O
of	O
artiﬁ-	O
cial	O
intelligence	O
,	O
many	O
of	O
which	O
will	O
include	O
reinforcement	B
learning	I
,	O
deep	O
and	O
otherwise	O
,	O
that	O
will	O
impact	O
our	O
lives	O
in	O
ways	O
that	O
are	O
hard	O
to	O
predict	O
.	O
but	O
an	O
abundance	O
of	O
successful	O
real-world	O
applications	O
does	O
not	O
mean	O
that	O
true	O
ar-	O
tiﬁcial	O
intelligence	O
has	O
arrived	O
.	O
despite	O
great	O
progress	O
in	O
many	O
areas	O
,	O
the	O
gulf	O
between	O
artiﬁcial	B
intelligence	I
and	O
the	O
intelligence	O
of	O
humans	O
,	O
and	O
even	O
of	O
other	O
animals	O
,	O
remains	O
great	O
.	O
superhuman	O
performance	O
can	O
be	O
achieved	O
in	O
some	O
domains	O
,	O
even	O
formidable	O
do-	O
mains	O
like	O
go	O
,	O
but	O
it	O
remains	O
a	O
signiﬁcant	O
challenge	O
to	O
develop	O
systems	O
that	O
are	O
like	O
us	O
in	O
being	O
complete	O
,	O
interactive	O
agents	O
having	O
general	O
adaptability	O
and	O
problem-solving	O
skills	O
,	O
emotional	O
sophistication	O
,	O
creativity	O
,	O
and	O
the	O
ability	O
to	O
learn	O
quickly	O
from	O
experi-	O
ence	O
.	O
with	O
its	O
focus	O
on	O
learning	O
by	O
interacting	O
with	O
dynamic	O
environments	O
,	O
reinforce-	O
ment	O
learning	O
,	O
as	O
it	O
develops	O
over	O
the	O
future	O
,	O
will	O
be	O
a	O
critical	O
component	O
of	O
agents	O
with	O
these	O
abilities	O
.	O
17.6.	O
reinforcement	B
learning	I
and	O
the	O
future	O
of	O
artiﬁcial	O
intelligence	O
479	O
reinforcement	B
learning	I
’	O
s	O
connections	O
to	O
psychology	B
and	O
neuroscience	B
(	O
chapters	O
14	O
and	O
15	O
)	O
underscore	O
its	O
relevance	O
to	O
another	O
longstanding	O
goal	B
of	O
artiﬁcial	B
intelligence	I
:	O
shedding	O
light	O
on	O
fundamental	O
questions	O
about	O
the	O
mind	O
and	O
how	O
it	O
emerges	O
from	O
the	O
brain	O
.	O
reinforcement	B
learning	I
theory	O
is	O
already	O
contributing	O
to	O
our	O
understanding	O
of	O
the	O
brain	O
’	O
s	O
reward	O
,	O
motivation	B
,	O
and	O
decision-making	O
processes	O
,	O
and	O
there	O
is	O
good	O
reason	O
to	O
believe	O
that	O
through	O
its	O
links	O
to	O
computational	O
psychiatry	O
,	O
reinforcement	B
learning	I
theory	O
will	O
contribute	O
to	O
methods	O
for	O
treating	O
mental	O
disorders	O
,	O
including	O
drug	O
abuse	O
and	B
addiction	I
.	O
another	O
contribution	O
that	O
reinforcement	B
learning	I
can	O
make	O
over	O
the	O
future	O
is	O
as	O
an	O
aid	O
to	O
human	O
decision	O
making	O
.	O
policies	O
derived	O
by	O
reinforcement	B
learning	I
in	O
simulated	O
environments	O
can	O
advise	O
human	O
decision	O
makers	O
in	O
such	O
areas	O
as	O
education	O
,	O
healthcare	O
,	O
transportation	O
,	O
energy	O
,	O
and	O
public-sector	O
resource	O
allocation	O
.	O
particularly	O
relevant	O
is	O
the	O
key	O
feature	O
of	O
reinforcement	B
learning	I
that	O
it	O
takes	O
long-term	O
consequences	O
of	O
decisions	O
into	O
account	O
.	O
this	O
is	O
very	O
clear	O
in	O
games	O
like	O
backgammon	B
and	O
go	O
,	O
where	O
some	O
of	O
the	O
most	O
impressive	O
results	O
of	O
reinforcement	O
learning	O
have	O
been	O
demonstrated	O
,	O
but	O
it	O
is	O
also	O
a	O
property	O
of	O
many	O
high-stakes	O
decisions	O
that	O
aﬀect	O
our	O
lives	O
and	O
our	O
planet	O
.	O
rein-	O
forcement	O
learning	O
follows	O
related	O
methods	O
for	O
advising	O
human	O
decision	O
making	O
that	O
have	O
been	O
developed	O
in	O
the	O
past	O
by	O
decision	O
analysts	O
in	O
many	O
disciplines	O
.	O
with	O
advanced	O
func-	O
tion	B
approximation	O
methods	O
and	O
massive	O
computational	O
power	O
,	O
reinforcement	B
learning	I
methods	O
have	O
the	O
potential	O
to	O
overcome	O
some	O
of	O
the	O
diﬃculties	O
of	O
scaling	O
up	O
traditional	O
decision-support	O
methods	O
to	O
larger	O
and	O
more	O
complex	O
problems	O
.	O
the	O
rapid	O
pace	O
of	O
advances	O
in	O
artiﬁcial	O
intelligence	O
has	O
led	O
to	O
warnings	O
that	O
artiﬁcial	B
intelligence	I
poses	O
serious	O
threats	O
to	O
our	O
societies	O
,	O
even	O
to	O
humanity	O
itself	O
.	O
the	O
renowned	O
scientist	O
and	B
artiﬁcial	I
intelligence	I
pioneer	O
herbert	O
simon	O
anticipated	O
the	O
warnings	O
we	O
are	O
hearing	O
today	O
in	O
a	O
presentation	O
at	O
the	O
earthware	O
symposium	O
at	O
cmu	O
in	O
2000	O
(	O
si-	O
mon	O
,	O
2000	O
)	O
.	O
he	O
spoke	O
of	O
the	O
eternal	O
conﬂict	O
between	O
the	O
promise	O
and	O
perils	O
of	O
any	O
new	O
knowledge	O
,	O
reminding	O
us	O
of	O
the	O
greek	O
myths	O
of	O
prometheus	O
,	O
the	O
hero	O
of	O
modern	O
science	O
,	O
who	O
stole	O
ﬁre	O
from	O
the	O
gods	O
for	O
the	O
beneﬁt	O
of	O
mankind	O
,	O
and	O
pandora	O
,	O
whose	O
box	O
could	O
be	O
opened	O
by	O
a	O
small	O
and	O
innocent	O
action	B
to	O
release	O
untold	O
perils	O
on	O
the	O
world	O
.	O
while	O
accepting	O
that	O
this	O
conﬂict	O
is	O
inevitable	O
,	O
simon	O
urged	O
us	O
to	O
recognize	O
that	O
as	O
design-	O
ers	O
of	O
our	O
future	O
and	O
not	O
mere	O
spectators	O
,	O
the	O
decisions	O
we	O
make	O
can	O
tilt	O
the	O
scale	O
in	O
prometheus	O
’	O
favor	O
.	O
this	O
is	O
certainly	O
true	O
for	O
reinforcement	B
learning	I
,	O
which	O
can	O
beneﬁt	O
society	O
but	O
can	O
also	O
produce	O
undesirable	O
outcomes	O
if	O
it	O
is	O
carelessly	O
deployed	O
.	O
thus	O
,	O
the	O
safety	O
of	O
artiﬁcial	O
intelligence	O
applications	O
involving	O
reinforcement	B
learning	I
is	O
a	O
topic	O
that	O
deserves	O
careful	O
attention	O
.	O
a	O
reinforcement	B
learning	I
agent	O
can	O
learn	O
by	O
interacting	O
with	O
either	O
the	O
real	O
world	O
or	O
with	O
a	O
simulation	O
of	O
some	O
piece	O
of	O
the	O
real	O
world	O
,	O
or	O
by	O
a	O
mixture	O
of	O
these	O
two	O
sources	O
of	O
experience	O
.	O
simulators	O
provide	O
safe	O
environments	O
in	O
which	O
an	O
agent	O
can	O
explore	O
and	O
learn	O
without	O
risking	O
real	O
damage	O
to	O
itself	O
or	O
to	O
its	O
environment	B
.	O
in	O
most	O
current	O
applications	O
,	O
policies	O
are	O
learned	O
from	O
simulated	O
experience	O
instead	O
of	O
direct	O
interaction	O
with	O
the	O
real	O
world	O
.	O
in	O
addition	O
to	O
avoiding	O
undesirable	O
real-world	O
consequences	O
,	O
learning	O
from	O
simulated	O
experience	O
can	O
make	O
virtually	O
unlimited	O
data	O
available	O
for	O
learning	O
,	O
generally	O
at	O
less	O
cost	O
than	O
needed	O
to	O
obtain	O
real	O
experience	O
,	O
and	O
because	O
simulations	O
typically	O
run	O
much	O
faster	O
than	O
real	O
time	O
,	O
learning	O
can	O
often	O
occur	O
more	O
quickly	O
than	O
if	O
it	O
relied	O
on	O
real	O
480	O
experience	O
.	O
chapter	O
17	O
:	O
frontiers	O
nevertheless	O
,	O
the	O
full	O
potential	O
of	O
reinforcement	O
learning	O
requires	O
reinforcement	O
learn-	O
ing	B
agents	O
to	O
be	O
embedded	O
into	O
the	O
ﬂow	O
of	O
real-world	O
experience	O
,	O
where	O
they	O
act	O
,	O
explore	O
,	O
and	O
learn	O
in	O
our	O
world	O
,	O
and	O
not	O
just	O
in	O
their	O
worlds	O
.	O
after	O
all	O
,	O
reinforcement	B
learning	I
algorithms—at	O
least	O
those	O
upon	O
which	O
we	O
focus	O
in	O
this	O
book—are	O
designed	O
to	O
learn	O
on-	O
line	O
,	O
and	O
they	O
emulate	O
many	O
aspects	O
of	O
how	O
animals	O
are	O
able	O
to	O
survive	O
in	O
nonstationary	O
and	O
hostile	O
environments	O
.	O
embedding	O
reinforcement	B
learning	I
agents	O
in	O
the	O
real	O
world	O
can	O
be	O
transformative	O
in	O
realizing	O
the	O
promises	O
of	O
artiﬁcial	O
intelligence	O
to	O
amplify	O
and	O
extend	O
human	O
abilities	O
.	O
a	O
major	O
reason	O
for	O
wanting	O
a	O
reinforcement	B
learning	I
agent	O
to	O
act	O
and	O
learn	O
in	O
the	O
real	O
world	O
is	O
that	O
it	O
is	O
often	O
diﬃcult	O
,	O
sometimes	O
impossible	O
,	O
to	O
simulate	O
real-world	O
experience	O
with	O
enough	O
ﬁdelity	O
to	O
make	O
the	O
resulting	O
policies	O
,	O
whether	O
derived	O
by	O
reinforcement	B
learning	I
or	O
by	O
other	O
methods	O
,	O
work	O
well—and	O
safely—when	O
directing	O
real	O
actions	O
.	O
this	O
is	O
especially	O
true	O
for	O
environments	O
whose	O
dynamics	O
depend	O
on	O
the	O
behavior	O
of	O
humans	O
,	O
such	O
as	O
in	O
education	O
,	O
healthcare	O
,	O
transportation	O
,	O
and	O
public	O
policy	B
,	O
domains	O
that	O
can	O
surely	O
beneﬁt	O
from	O
improved	O
decision	O
making	O
.	O
however	O
,	O
it	O
is	O
for	O
real-world	O
embedded	O
agents	O
that	O
warnings	O
about	O
potential	O
dangers	O
of	O
artiﬁcial	O
intelligence	O
need	O
to	O
be	O
heeded	O
.	O
some	O
of	O
these	O
warnings	O
are	O
particularly	O
relevant	O
to	O
reinforcement	B
learning	I
.	O
because	O
reinforcement	B
learning	I
is	O
based	O
on	O
optimization	O
,	O
it	O
inherits	O
the	O
pluses	O
and	O
minuses	O
of	O
all	O
optimization	O
methods	O
.	O
on	O
the	O
minus	O
side	O
is	O
the	O
problem	O
of	O
devising	O
objective	O
functions	O
,	O
or	O
reward	O
signals	O
in	O
the	O
case	O
of	O
reinforcement	O
learning	O
,	O
so	O
that	O
optimization	O
produces	O
the	O
desired	O
results	O
while	O
avoiding	O
undesirable	O
results	O
.	O
we	O
said	O
in	O
section	O
17.4	O
that	O
re-	O
inforcement	O
learning	O
agents	O
can	O
discover	O
unexpected	O
ways	O
to	O
make	O
their	O
environments	O
deliver	O
reward	O
,	O
some	O
of	O
which	O
might	O
be	O
undesirable	O
,	O
or	O
even	O
dangerous	O
.	O
when	O
we	O
specify	O
what	O
we	O
want	O
a	O
system	O
to	O
learn	O
only	O
indirectly	O
,	O
as	O
we	O
do	O
in	O
designing	O
a	O
reinforcement	B
learning	I
system	O
’	O
s	O
reward	B
signal	I
,	O
we	O
will	O
not	O
know	O
how	O
closely	O
the	O
agent	O
will	O
fulﬁll	O
our	O
desire	O
until	O
its	O
learning	O
is	O
complete	O
.	O
this	O
is	O
hardly	O
a	O
new	O
problem	O
with	O
reinforcement	O
learning	O
;	O
recognition	O
of	O
it	O
has	O
a	O
long	O
history	O
in	O
both	O
literature	O
and	O
engineering	O
.	O
for	O
example	O
,	O
in	O
goethe	O
’	O
s	O
poem	O
“	O
the	O
sorcerer	O
’	O
s	O
apprentice	O
”	O
(	O
goethe	O
,	O
1878	O
)	O
,	O
the	O
apprentice	O
uses	O
magic	O
to	O
enchant	O
a	O
broom	O
to	O
do	O
his	O
job	O
of	O
fetching	O
water	O
,	O
but	O
the	O
result	O
is	O
an	O
unin-	O
tended	O
ﬂood	O
due	O
to	O
the	O
apprentice	O
’	O
s	O
inadequate	O
knowledge	O
of	O
magic	O
.	O
in	O
the	O
engineering	O
context	O
,	O
norbert	O
wiener	O
,	O
the	O
founder	O
of	O
cybernetics	O
,	O
warned	O
of	O
this	O
problem	O
more	O
than	O
half	O
a	O
century	O
ago	O
by	O
relating	O
the	O
supernatural	O
story	O
of	O
“	O
the	O
monkey	O
’	O
s	O
paw	O
”	O
(	O
wiener	O
,	O
1964	O
)	O
:	O
“	O
...	O
it	O
grants	O
what	O
you	O
ask	O
for	O
,	O
not	O
what	O
you	O
should	O
have	O
asked	O
for	O
or	O
what	O
you	O
intend	O
”	O
(	O
p.	O
59	O
)	O
.	O
the	O
problem	O
has	O
also	O
been	O
discussed	O
at	O
length	O
in	O
a	O
modern	O
context	O
by	O
nick	O
bostrom	O
(	O
2014	O
)	O
.	O
anyone	O
having	O
experience	O
with	O
reinforcement	B
learning	I
has	O
likely	O
seen	O
their	O
systems	O
discover	O
unexpected	O
ways	O
to	O
obtain	O
a	O
lot	O
of	O
reward	O
.	O
sometimes	O
the	O
unexpected	O
behavior	O
is	O
good	O
:	O
it	O
solves	O
a	O
problem	O
in	O
a	O
nice	O
new	O
way	O
.	O
in	O
other	O
instances	O
,	O
what	O
the	O
agent	O
learns	O
violates	O
considerations	O
that	O
the	O
system	O
designer	O
may	O
never	O
have	O
thought	O
about	O
.	O
careful	O
design	B
of	I
reward	O
signals	O
is	O
essential	O
if	O
an	O
agent	O
is	O
to	O
act	O
in	O
the	O
real	O
world	O
with	O
no	O
opportunity	O
for	O
human	O
vetting	O
of	O
its	O
actions	O
or	O
means	O
to	O
easily	O
interrupt	O
its	O
behavior	O
.	O
despite	O
the	O
possibility	O
of	O
unintended	O
negative	O
consequences	O
,	O
optimization	O
has	O
been	O
used	O
for	O
hundreds	O
of	O
years	O
by	O
engineers	O
,	O
architects	O
,	O
and	O
others	O
whose	O
designs	O
have	O
posi-	O
17.6.	O
reinforcement	B
learning	I
and	O
the	O
future	O
of	O
artiﬁcial	O
intelligence	O
481	O
tively	O
impacted	O
the	O
world	O
.	O
we	O
owe	O
much	O
that	O
is	O
good	O
in	O
our	O
environment	B
to	O
the	O
applica-	O
tion	B
of	O
optimization	O
methods	O
.	O
many	O
approaches	O
have	O
been	O
developed	O
to	O
mitigate	O
the	O
risk	O
of	O
optimization	O
,	O
such	O
as	O
adding	O
hard	O
and	O
soft	O
constraints	O
,	O
restricting	O
optimization	O
to	O
ro-	O
bust	O
and	O
risk-sensitive	O
policies	O
,	O
and	O
optimizing	O
with	O
multiple	O
objective	O
functions	O
.	O
some	O
of	O
these	O
approaches	O
have	O
been	O
adapted	O
to	O
reinforcement	B
learning	I
,	O
and	O
more	O
research	O
is	O
needed	O
to	O
address	O
these	O
concerns	O
.	O
the	O
problem	O
of	O
ensuring	O
that	O
a	O
reinforcement	B
learning	I
agent	O
’	O
s	O
goal	B
is	O
attuned	O
to	O
our	O
own	O
remains	O
a	O
challenge	O
.	O
another	O
challenge	O
if	O
reinforcement	B
learning	I
agents	O
are	O
to	O
act	O
and	O
learn	O
in	O
the	O
real	O
world	O
is	O
not	O
just	O
about	O
what	O
they	O
might	O
learn	O
eventually	O
,	O
but	O
about	O
how	O
they	O
will	O
behave	O
while	O
they	O
are	O
learning	O
.	O
how	O
do	O
you	O
make	O
sure	O
that	O
an	O
agent	O
gets	O
enough	O
ex-	O
perience	O
to	O
learn	O
a	O
high-performing	O
policy	B
,	O
all	O
the	O
while	O
not	O
harming	O
its	O
environment	B
,	O
other	O
agents	O
,	O
or	O
itself	O
(	O
or	O
more	O
realistically	O
,	O
while	O
keeping	O
the	O
probability	O
of	O
harm	O
ac-	O
ceptably	O
low	O
)	O
?	O
this	O
problem	O
is	O
also	O
not	O
novel	O
or	O
unique	O
to	O
reinforcement	B
learning	I
.	O
risk	O
management	O
and	O
mitigation	O
for	O
embedded	O
reinforcement	B
learning	I
is	O
similar	O
to	O
what	O
con-	O
trol	O
engineers	O
have	O
had	O
to	O
confront	O
from	O
the	O
beginning	O
of	O
using	O
automatic	O
control	O
in	O
situations	O
where	O
a	O
controller	O
’	O
s	O
behavior	O
can	O
have	O
unacceptable	O
,	O
possibly	O
catastrophic	O
,	O
consequences	O
,	O
as	O
in	O
the	O
control	B
of	O
an	O
aircraft	O
or	O
a	O
delicate	O
chemical	O
process	O
.	O
control	B
ap-	O
plications	O
rely	O
on	O
careful	O
system	O
modeling	O
,	O
model	O
validation	O
,	O
and	O
extensive	O
testing	O
,	O
and	O
there	O
is	O
a	O
highly-developed	O
body	O
of	O
theory	O
aimed	O
at	O
ensuring	O
convergence	O
and	B
stability	I
of	O
adaptive	O
controllers	O
designed	O
for	O
use	O
when	O
the	O
dynamics	O
of	O
the	O
system	O
to	O
be	O
controlled	O
are	O
not	O
fully	O
known	O
.	O
theoretical	O
guarantees	O
are	O
never	O
iron-clad	O
because	O
they	O
depend	O
on	O
the	O
validity	O
of	O
the	O
assumptions	O
underlying	O
the	O
mathematics	O
,	O
but	O
without	O
this	O
theory	O
,	O
combined	O
with	O
risk-management	O
and	O
mitigation	O
practices	O
,	O
automatic	O
control—adaptive	O
and	O
otherwise—would	O
not	O
be	O
as	O
beneﬁcial	O
as	O
it	O
is	O
today	O
in	O
improving	O
the	O
quality	O
,	O
eﬃ-	O
ciency	O
,	O
and	O
cost-eﬀectiveness	O
of	O
processes	O
on	O
which	O
we	O
have	O
come	O
to	O
rely	O
.	O
one	O
of	O
the	O
most	O
pressing	O
areas	O
for	O
future	O
reinforcement	B
learning	I
research	O
is	O
to	O
adapt	O
and	O
extend	O
methods	O
developed	O
in	O
control	O
engineering	O
with	O
the	O
goal	B
of	O
making	O
it	O
acceptably	O
safe	O
to	O
fully	O
embed	O
reinforcement	B
learning	I
agents	O
into	O
physical	O
environments	O
.	O
in	O
closing	O
,	O
we	O
return	B
to	O
simon	O
’	O
s	O
call	O
for	O
us	O
to	O
recognize	O
that	O
we	O
are	O
designers	O
of	O
our	O
future	O
and	O
not	O
simply	O
spectators	O
.	O
by	O
decisions	O
we	O
make	O
as	O
individuals	O
,	O
and	O
by	O
the	O
inﬂu-	O
ence	O
we	O
can	O
exert	O
on	O
how	O
our	O
societies	O
are	O
governed	O
,	O
we	O
can	O
work	O
toward	O
ensuring	O
that	O
the	O
beneﬁts	O
made	O
possible	O
by	O
a	O
new	O
technology	O
outweigh	O
the	O
harm	O
it	O
can	O
cause	O
.	O
there	O
is	O
ample	O
opportunity	O
to	O
do	O
this	O
in	O
the	O
case	O
of	O
reinforcement	O
learning	O
,	O
which	O
can	O
help	O
improve	O
the	O
quality	O
,	O
fairness	O
,	O
and	O
sustainability	O
of	O
life	O
on	O
our	O
planet	O
,	O
but	O
which	O
can	O
also	O
release	O
new	O
perils	O
.	O
a	O
threat	O
already	O
here	O
is	O
the	O
displacement	O
of	O
jobs	O
caused	O
by	O
applica-	O
tions	O
of	O
artiﬁcial	O
intelligence	O
.	O
still	O
there	O
are	O
good	O
reasons	O
to	O
believe	O
that	O
the	O
beneﬁts	O
of	O
artiﬁcial	O
intelligence	O
can	O
outweigh	O
the	O
disruption	O
it	O
causes	O
.	O
as	O
to	O
safety	O
,	O
hazards	O
possi-	O
ble	O
with	O
reinforcement	O
learning	O
are	O
not	O
completely	O
diﬀerent	O
from	O
those	O
that	O
have	O
been	O
managed	O
successfully	O
for	O
related	O
applications	O
of	O
optimization	O
and	B
control	I
methods	O
.	O
as	O
reinforcement	O
learning	O
moves	O
out	O
into	O
the	O
real	O
world	O
in	O
future	O
applications	O
,	O
developers	O
have	O
an	O
obligation	O
to	O
follow	O
best	O
practices	O
that	O
have	O
evolved	O
for	O
similar	O
technologies	O
,	O
while	O
at	O
the	O
same	O
time	O
extending	O
them	O
to	O
make	O
sure	O
that	O
prometheus	O
keeps	O
the	O
upper	O
hand	O
.	O
482	O
chapter	O
17	O
:	O
frontiers	O
bibliographical	O
and	O
historical	O
remarks	O
17.1	O
general	O
value	O
functions	O
were	O
ﬁrst	O
explicitly	O
identiﬁed	O
by	O
sutton	O
and	O
colleagues	O
(	O
sutton	O
,	O
1995a	O
;	O
sutton	O
et	O
al.	O
,	O
2011	O
;	O
modayil	O
,	O
white	O
and	O
sutton	O
,	O
2013	O
)	O
.	O
ring	O
(	O
in	O
preparation	O
)	O
developed	O
an	O
extensive	O
thought	O
experiment	O
with	O
gvfs	O
(	O
“	O
fore-	O
casts	O
”	O
)	O
that	O
has	O
been	O
inﬂuential	O
despite	O
not	O
yet	O
having	O
been	O
published	O
.	O
17.2	O
the	O
ﬁrst	O
demonstrations	O
of	O
multi-headed	O
learning	O
in	O
reinforcement	B
learning	I
were	O
by	O
jaderberg	O
et	O
al	O
.	O
(	O
2017	O
)	O
.	O
bellemare	O
,	O
dabney	O
and	O
munos	O
(	O
2017	O
)	O
showed	O
that	O
predicting	O
more	O
things	O
about	O
the	O
distribution	O
of	O
reward	O
could	O
signiﬁcantly	O
speed	O
learning	O
to	O
optimize	O
its	O
expectation	O
,	O
an	O
instance	O
of	O
auxiliary	O
tasks	O
.	O
many	O
others	O
have	O
since	O
taken	O
up	O
this	O
line	O
of	O
research	O
.	O
the	O
general	O
theory	O
of	O
classical	O
conditioning	B
as	O
learned	O
predictions	O
together	O
with	O
built-in	O
,	O
reﬂexive	O
reactions	O
to	O
the	O
predictions	O
has	O
not	O
to	O
our	O
knowledge	O
been	O
clearly	O
articulated	O
in	O
the	O
psychological	O
literature	O
.	O
modayil	O
and	O
sutton	O
(	O
2014	O
)	O
describe	O
it	O
as	O
an	O
approach	O
to	O
the	O
engineering	O
of	O
robots	O
and	O
other	O
agents	O
,	O
calling	O
it	O
“	O
pavlovian	O
control	B
”	O
to	O
allude	O
to	O
its	O
roots	O
in	O
classical	O
conditioning	B
.	O
the	O
formalization	O
of	O
temporally	O
abstract	O
courses	O
of	O
action	O
as	O
options	O
was	O
in-	O
troduced	O
by	O
sutton	O
,	O
precup	O
,	O
and	O
singh	O
(	O
1999	O
)	O
,	O
building	O
on	O
prior	O
work	O
by	O
parr	O
(	O
1998	O
)	O
and	O
sutton	O
(	O
1995a	O
)	O
,	O
and	O
on	O
classical	O
work	O
on	O
semi-mdps	O
(	O
e.g.	O
,	O
see	O
put-	O
erman	O
,	O
1994	O
)	O
.	O
precup	O
’	O
s	O
(	O
2000	O
)	O
phd	O
thesis	O
developed	O
option	O
ideas	O
fully	O
.	O
an	O
im-	O
portant	O
limitation	O
of	O
these	O
early	O
works	O
is	O
that	O
they	O
did	O
not	O
treat	O
the	O
oﬀ-policy	B
case	O
with	B
function	I
approximation	I
.	O
intra-option	O
learning	O
in	O
general	O
requires	O
oﬀ-	O
policy	B
learning	O
,	O
which	O
could	O
not	O
be	O
done	O
reliably	O
with	B
function	I
approximation	I
at	O
that	O
time	O
.	O
although	O
now	O
we	O
have	O
a	O
variety	O
of	O
stable	O
oﬀ-policy	B
learning	O
meth-	O
ods	O
using	O
function	B
approximation	I
,	O
their	O
combination	O
with	O
option	O
ideas	O
had	O
not	O
been	O
signiﬁcantly	O
explored	O
at	O
the	O
time	O
of	O
publication	O
of	O
this	O
book	O
.	O
barto	O
and	O
mahadevan	O
(	O
2003	O
)	O
and	O
hengst	O
(	O
2012	O
)	O
review	O
the	O
options	B
formalism	O
and	O
other	O
approaches	O
to	O
temporal	B
abstraction	I
.	O
using	O
gvfs	O
to	O
implement	O
option	B
models	I
has	O
not	O
previously	O
been	O
described	O
.	O
our	O
presentation	O
uses	O
the	O
trick	O
introduced	O
by	O
modayil	O
,	O
white	O
and	O
sutton	O
(	O
2014	O
)	O
for	O
predicting	O
signals	O
at	O
the	O
termination	O
of	O
policies	O
.	O
among	O
the	O
few	O
works	O
that	O
have	O
learned	O
option	B
models	I
with	O
function	O
approxi-	O
mation	O
are	O
those	O
by	O
bacon	O
,	O
harb	O
,	O
and	O
precup	O
(	O
2017	O
)	O
.	O
the	O
extension	O
of	O
options	O
and	O
option	O
models	O
to	O
the	O
average-reward	O
setting	O
has	O
not	O
yet	O
been	O
developed	O
in	O
the	O
literature	O
.	O
17.3	O
a	O
good	O
presentation	O
of	O
the	O
pomdp	O
approach	O
is	O
given	O
by	O
monahan	O
(	O
1982	O
)	O
.	O
psrs	O
and	O
tests	O
were	O
introduced	O
by	O
littman	O
,	O
sutton	O
and	O
singh	O
(	O
2002	O
)	O
.	O
ooms	O
were	O
introduced	O
by	O
jaeger	O
(	O
1997	O
,	O
1998	O
,	O
2000	O
)	O
.	O
sequential	O
systems	O
,	O
which	O
unify	O
psrs	O
,	O
ooms	O
,	O
and	O
many	O
other	O
works	O
,	O
were	O
introduced	O
in	O
the	O
phd	O
thesis	O
of	O
michael	O
thon	O
(	O
2017	O
;	O
thon	O
and	O
jaeger	O
,	O
2015	O
)	O
.	O
the	O
theory	O
of	O
reinforcement	O
learning	O
with	O
a	O
non-markov	O
state	B
representation	O
was	O
developed	O
explicitly	O
by	O
singh	O
,	O
jaakkola	O
,	O
and	O
jordan	O
(	O
1994	O
;	O
jaakkola	O
,	O
singh	O
,	O
and	O
17.6.	O
reinforcement	B
learning	I
and	O
the	O
future	O
of	O
artiﬁcial	O
intelligence	O
483	O
jordan	O
,	O
1995	O
)	O
.	O
early	O
reinforcement	B
learning	I
approaches	O
to	O
partial	O
observability	O
were	O
developed	O
by	O
chrisman	O
(	O
1992	O
)	O
,	O
mccallum	O
(	O
1993	O
,	O
1995	O
)	O
,	O
parr	O
and	O
russell	O
(	O
1995	O
)	O
,	O
littman	O
,	O
cassandra	O
,	O
and	O
kaelbling	O
(	O
1995	O
)	O
,	O
and	O
by	O
lin	O
and	O
mitchell	O
(	O
1992	O
)	O
.	O
17.4	O
early	O
eﬀorts	O
to	O
include	O
advice	O
and	O
teaching	O
in	O
reinforcement	O
learning	O
include	O
those	O
by	O
lin	O
(	O
1992	O
)	O
,	O
maclin	O
and	O
shavlik	O
(	O
1994	O
)	O
,	O
clouse	O
(	O
1996	O
)	O
,	O
and	O
clouse	O
and	O
utgoﬀ	O
(	O
1992	O
)	O
.	O
skinner	O
’	O
s	O
shaping	B
should	O
not	O
be	O
confused	O
with	O
the	O
“	O
potential-based	O
shaping	B
”	O
technique	O
introduced	O
by	O
ng	O
,	O
harada	O
,	O
and	O
russell	O
(	O
1999	O
)	O
.	O
their	O
technique	O
has	O
been	O
shown	O
by	O
wiewiora	O
(	O
2003	O
)	O
to	O
be	O
equivalent	O
to	O
the	O
simpler	O
idea	O
of	O
providing	O
an	O
initial	O
approximation	O
to	O
the	O
value	B
function	I
,	O
as	O
in	O
(	O
17.6	O
)	O
.	O
17.5	O
we	O
recommend	O
the	O
book	O
by	O
goodfellow	O
,	O
bengio	O
,	O
and	O
courville	O
(	O
2016	O
)	O
for	O
dis-	O
cussion	O
of	O
today	O
’	O
s	O
deep	B
learning	I
techniques	O
.	O
the	O
problem	O
of	O
catastrophic	O
in-	O
terference	O
in	O
artiﬁcial	O
neural	B
networks	I
was	O
developed	O
by	O
mccloskey	O
and	O
cohen	O
(	O
1989	O
)	O
,	O
ratcliﬀ	O
(	O
1990	O
)	O
,	O
and	O
french	O
(	O
1999	O
)	O
.	O
the	O
idea	O
of	O
a	O
replay	O
buﬀer	O
was	O
in-	O
troduced	O
by	O
lin	O
(	O
1992	O
)	O
and	O
used	O
prominently	O
in	O
deep	O
learning	O
in	O
the	O
atari	O
game	O
playing	O
system	O
(	O
section	O
16.5	O
,	O
mnih	O
et	O
al.	O
,	O
2013	O
,	O
2015	O
)	O
.	O
minsky	O
(	O
1961	O
)	O
was	O
one	O
of	O
the	O
ﬁrst	O
to	O
identify	O
the	O
problem	O
of	O
representation	O
learning	O
.	O
among	O
the	O
few	O
works	O
to	O
consider	O
planning	O
with	O
learned	O
,	O
approximate	B
models	O
are	O
those	O
by	O
kuvayev	O
and	O
sutton	O
(	O
1996	O
)	O
,	O
sutton	O
,	O
szepesvari	O
,	O
geramifard	O
,	O
and	O
bowling	O
(	O
2008	O
)	O
,	O
nouri	O
and	O
littman	O
(	O
2009	O
)	O
,	O
and	O
hester	O
and	O
stone	O
(	O
2012	O
)	O
.	O
the	O
need	O
to	O
be	O
selective	O
in	O
model	O
construction	O
to	O
avoid	O
slowing	O
planning	B
is	O
well	O
known	O
in	O
artiﬁcial	O
intelligence	O
.	O
some	O
of	O
the	O
classic	O
work	O
is	O
by	O
minton	O
(	O
1990	O
)	O
and	O
tambe	O
,	O
newell	O
,	O
and	O
rosenbloom	O
(	O
1990	O
)	O
.	O
hauskrecht	O
,	O
meuleau	O
,	O
kaelbling	O
,	O
dean	O
,	O
and	O
boutilier	O
(	O
1998	O
)	O
showed	O
this	O
eﬀect	O
in	O
mdps	O
with	O
deterministic	O
options	B
.	O
schmidhuber	O
(	O
1991a	O
,	O
b	O
)	O
proposed	O
how	O
something	O
like	O
curiosity	B
would	O
result	O
if	O
reward	O
signals	O
were	O
a	O
function	O
of	O
how	O
quickly	O
an	O
agent	O
’	O
s	O
environment	B
model	O
is	O
improving	O
.	O
the	O
empowerment	O
function	O
proposed	O
by	O
klyubin	O
,	O
polani	O
,	O
and	O
ne-	O
haniv	O
(	O
2005	O
)	O
is	O
an	O
information-theoretic	O
measure	O
of	O
an	O
agent	O
’	O
s	O
ability	O
to	O
control	B
its	O
environment	B
that	O
can	O
function	O
as	O
an	O
intrinsic	B
reward	O
signal	O
.	O
baldassarre	O
and	O
mirolli	O
(	O
2013	O
)	O
is	O
a	O
collection	O
of	O
contributions	O
by	O
researchers	O
studying	O
intrinsic	B
reward	O
and	O
motivation	O
from	O
both	O
biological	O
and	O
computational	O
perspectives	O
,	O
in-	O
cluding	O
a	O
perspective	O
on	O
“	O
intrinsically-motivated	O
reinforcement	B
learning	I
,	O
”	O
to	O
use	O
the	O
term	O
introduced	O
by	O
singh	O
,	O
barto	O
,	O
and	O
chentenez	O
(	O
2004	O
)	O
.	O
see	O
also	O
oudeyer	O
and	O
kaplan	O
(	O
2007	O
)	O
,	O
oudeyer	O
,	O
kaplan	O
,	O
and	O
hafner	O
(	O
2007	O
)	O
,	O
and	O
barto	O
(	O
2013	O
)	O
.	O
references	O
abbeel	O
,	O
p.	O
,	O
ng	O
,	O
a.	O
y	O
.	O
(	O
2004	O
)	O
.	O
apprenticeship	O
learning	O
via	O
inverse	B
reinforcement	I
learning	I
.	O
in	O
proceedings	O
of	O
the	O
21st	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
2004	O
)	O
.	O
acm	O
,	O
new	O
york	O
.	O
abramson	O
,	O
b	O
.	O
(	O
1990	O
)	O
.	O
expected-outcome	O
:	O
a	O
general	O
model	O
of	O
static	O
evaluation	O
.	O
ieee	O
trans-	O
actions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
12	O
(	O
2	O
)	O
:182–193	O
.	O
adams	O
,	O
c.	O
d.	O
(	O
1982	O
)	O
.	O
variations	O
in	O
the	O
sensitivity	O
of	O
instrumental	O
responding	O
to	O
reinforcer	O
devaluation	O
.	O
the	O
quarterly	O
journal	O
of	O
experimental	O
psychology	B
,	O
34	O
(	O
2	O
)	O
:77–98	O
.	O
adams	O
,	O
c.	O
d.	O
,	O
dickinson	O
,	O
a	O
.	O
(	O
1981	O
)	O
.	O
instrumental	O
responding	O
following	O
reinforcer	O
devaluation	O
.	O
the	O
quarterly	O
journal	O
of	O
experimental	O
psychology	B
,	O
33	O
(	O
2	O
)	O
:109–121	O
.	O
adams	O
,	O
r.	O
a.	O
,	O
huys	O
,	O
q.	O
j.	O
m.	O
,	O
roiser	O
,	O
j.	O
p.	O
(	O
2015	O
)	O
.	O
computational	O
psychiatry	O
:	O
towards	O
a	O
math-	O
ematically	O
informed	O
understanding	O
of	O
mental	O
illness	O
.	O
journal	O
of	O
neurology	O
,	O
neurosurgery	O
&	O
psychiatry	O
.	O
doi:10.1136/jnnp-2015-310737	O
agrawal	O
,	O
r.	O
(	O
1995	O
)	O
.	O
sample	O
mean	O
based	O
index	O
policies	O
with	O
o	O
(	O
logn	O
)	O
regret	O
for	O
the	O
multi-armed	O
bandit	O
problem	O
.	O
advances	O
in	O
applied	O
probability	O
,	O
27	O
(	O
4	O
)	O
:1054–1078	O
.	O
agre	O
,	O
p.	O
e.	O
(	O
1988	O
)	O
.	O
the	O
dynamic	O
structure	O
of	O
everyday	O
life	O
.	O
ph.d.	O
thesis	O
,	O
massachusetts	O
institute	O
of	O
technology	O
,	O
cambridge	O
ma	O
.	O
ai-tr	O
1085	O
,	O
mit	O
artiﬁcial	B
intelligence	I
laboratory	O
.	O
agre	O
,	O
p.	O
e.	O
,	O
chapman	O
,	O
d.	O
(	O
1990	O
)	O
.	O
what	O
are	O
plans	O
for	O
?	O
robotics	O
and	O
autonomous	O
systems	O
,	O
6	O
(	O
1-2	O
)	O
:17–34	O
.	O
aizerman	O
,	O
m.	O
a.	O
,	O
braverman	O
,	O
e	O
.	O
´i.	O
,	O
rozonoer	O
,	O
l.	O
i	O
.	O
(	O
1964	O
)	O
.	O
probability	O
problem	O
of	O
pattern	O
recognition	O
learning	O
and	O
potential	O
functions	O
method	O
.	O
avtomat	O
.	O
i	O
telemekh	O
,	O
25	O
(	O
9	O
)	O
:1307–	O
1323.	O
albus	O
,	O
j.	O
s.	O
(	O
1971	O
)	O
.	O
a	O
theory	O
of	O
cerebellar	O
function	O
.	O
mathematical	O
biosciences	O
,	O
10	O
(	O
1-2	O
)	O
:25–61	O
.	O
albus	O
,	O
j.	O
s.	O
(	O
1981	O
)	O
.	O
brain	O
,	O
behavior	O
,	O
and	O
robotics	O
.	O
byte	O
books	O
,	O
peterborough	O
,	O
nh	O
.	O
aleksandrov	O
,	O
v.	O
m.	O
,	O
sysoev	O
,	O
v.	O
i.	O
,	O
shemeneva	O
,	O
v.	O
v.	O
(	O
1968	O
)	O
.	O
stochastic	O
optimization	O
of	O
systems	O
.	O
izv	O
.	O
akad	O
.	O
nauk	O
sssr	O
,	O
tekh	O
.	O
kibernetika:14–19	O
.	O
amari	O
,	O
s.	O
i	O
.	O
(	O
1998	O
)	O
.	O
natural	O
gradient	B
works	O
eﬃciently	O
in	O
learning	O
.	O
neural	B
computation	O
,	O
10	O
(	O
2	O
)	O
:251–276	O
.	O
an	O
,	O
p.	O
c.	O
e.	O
(	O
1991	O
)	O
.	O
an	O
improved	O
multi-dimensional	O
cmac	O
neural	B
network	O
:	O
receptive	O
field	O
function	O
and	O
placement	O
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
new	O
hampshire	O
,	O
durham	O
.	O
an	O
,	O
p.	O
c.	O
e.	O
,	O
miller	O
,	O
w.	O
t.	O
,	O
parks	O
,	O
p.	O
c.	O
(	O
1991	O
)	O
.	O
design	O
improvements	O
in	O
associative	O
memories	O
for	O
cerebellar	O
model	O
articulation	O
controllers	O
(	O
cmac	O
)	O
.	O
artiﬁcial	B
neural	I
networks	I
,	O
pp	O
.	O
1207–	O
1210	O
,	O
elsevier	O
north-holland	O
.	O
http	O
:	O
//www.incompleteideas.net/papers/anmillerparks1991.pdf	O
anderson	O
,	O
c.	O
w.	O
(	O
1986	O
)	O
.	O
learning	O
and	O
problem	O
solving	O
with	O
multilayer	O
connectionist	O
systems	O
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
massachusetts	O
,	O
amherst	O
.	O
anderson	O
,	O
c.	O
w.	O
(	O
1987	O
)	O
.	O
strategy	O
learning	O
with	O
multilayer	O
connectionist	O
representations	O
.	O
in	O
485	O
486	O
references	O
proceedings	O
of	O
the	O
4th	O
international	O
workshop	O
on	O
machine	O
learning	O
,	O
pp	O
.	O
103–114	O
.	O
morgan	O
kaufmann	O
.	O
anderson	O
,	O
c.	O
w.	O
(	O
1989	O
)	O
.	O
learning	O
to	O
control	B
an	O
inverted	O
pendulum	O
using	O
neural	B
networks	I
.	O
ieee	O
control	B
systems	O
magazine	O
,	O
9	O
(	O
3	O
)	O
:31–37	O
.	O
anderson	O
,	O
j.	O
a.	O
,	O
silverstein	O
,	O
j.	O
w.	O
,	O
ritz	O
,	O
s.	O
a.	O
,	O
jones	O
,	O
r.	O
s.	O
(	O
1977	O
)	O
.	O
distinctive	O
features	O
,	O
categorical	O
perception	O
,	O
and	O
probability	O
learning	O
:	O
some	O
applications	O
of	O
a	O
neural	B
model	O
.	O
psy-	O
chological	O
review	O
,	O
84	O
(	O
5	O
)	O
:413–451	O
.	O
andreae	O
,	O
j.	O
h.	O
(	O
1963	O
)	O
.	O
stella	O
,	O
a	O
scheme	O
for	O
a	O
learning	O
machine	O
.	O
in	O
proceedings	O
of	O
the	O
2nd	O
ifac	O
congress	O
,	O
basle	O
,	O
pp	O
.	O
497–502	O
.	O
butterworths	O
,	O
london	O
.	O
andreae	O
,	O
j.	O
h.	O
(	O
1969a	O
)	O
.	O
a	O
learning	O
machine	O
with	O
monologue	O
.	O
international	O
journal	O
of	O
man–	O
machine	O
studies	O
,	O
1	O
(	O
1	O
)	O
:1–20	O
.	O
andreae	O
,	O
j.	O
h.	O
(	O
1969b	O
)	O
.	O
learning	O
machines—a	O
uniﬁed	O
view	O
.	O
in	O
a.	O
r.	O
meetham	O
and	O
r.	O
a.	O
hud-	O
son	O
(	O
eds	O
.	O
)	O
,	O
encyclopedia	O
of	O
information	O
,	O
linguistics	O
,	O
and	B
control	I
,	O
pp	O
.	O
261–270	O
.	O
pergamon	O
,	O
oxford	O
.	O
andreae	O
,	O
j.	O
h.	O
(	O
1977	O
)	O
.	O
thinking	O
with	O
the	O
teachable	O
machine	O
.	O
academic	O
press	O
,	O
london	O
.	O
arthur	O
,	O
w.	O
b	O
.	O
(	O
1991	O
)	O
.	O
designing	O
economic	O
agents	O
that	O
act	O
like	O
human	O
agents	O
:	O
a	O
behavioral	O
approach	O
to	O
bounded	O
rationality	O
.	O
the	O
american	O
economic	O
review	O
,	O
81	O
(	O
2	O
)	O
:353–359	O
.	O
atkeson	O
,	O
c.	O
g.	O
(	O
1992	O
)	O
.	O
memory-based	O
approaches	O
to	O
approximating	O
continuous	O
functions	O
.	O
in	O
sante	O
fe	O
institute	O
studies	O
in	O
the	O
sciences	O
of	O
complexity	O
,	O
proceedings	O
vol	O
.	O
12	O
,	O
pp	O
.	O
521–521	O
.	O
addison-wesley	O
.	O
atkeson	O
,	O
c.	O
g.	O
,	O
moore	O
,	O
a.	O
w.	O
,	O
schaal	O
,	O
s.	O
(	O
1997	O
)	O
.	O
locally	O
weighted	O
learning	O
.	O
artiﬁcial	O
intelli-	O
gence	O
review	O
,	O
11	O
:11–73	O
.	O
auer	O
,	O
p.	O
,	O
cesa-bianchi	O
,	O
n.	O
,	O
fischer	O
,	O
p.	O
(	O
2002	O
)	O
.	O
finite-time	O
analysis	O
of	O
the	O
multiarmed	O
bandit	O
problem	O
.	O
machine	O
learning	O
,	O
47	O
(	O
2-3	O
)	O
:235–256	O
.	O
bacon	O
,	O
p.	O
l.	O
,	O
harb	O
,	O
j.	O
,	O
precup	O
,	O
d.	O
(	O
2017	O
)	O
.	O
the	O
option-critic	O
architecture	O
.	O
in	O
proceedings	O
of	O
the	O
association	O
for	O
the	O
advancement	O
of	O
artiﬁcial	O
intelligence	O
,	O
pp	O
.	O
1726–1734	O
.	O
baird	O
,	O
l.	O
c.	O
(	O
1995	O
)	O
.	O
residual	O
algorithms	O
:	O
reinforcement	B
learning	I
with	O
function	B
approximation	I
.	O
in	O
proceedings	O
of	O
the	O
12th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
1995	O
)	O
,	O
pp	O
.	O
30–37	O
.	O
morgan	O
kaufmann	O
.	O
baird	O
,	O
l.	O
c.	O
(	O
1999	O
)	O
.	O
reinforcement	B
learning	I
through	O
gradient	B
descent	I
.	O
ph.d.	O
thesis	O
,	O
carnegie	O
mellon	O
university	O
,	O
pittsburgh	O
pa.	O
baird	O
,	O
l.	O
c.	O
,	O
klopf	O
,	O
a.	O
h.	O
(	O
1993	O
)	O
.	O
reinforcement	B
learning	I
with	O
high-dimensional	O
,	O
continuous	O
actions	O
.	O
wright	O
laboratory	O
,	O
wright-patterson	O
air	O
force	O
base	O
,	O
tech	O
.	O
rep.	O
wl-tr-93-1147	O
.	O
baird	O
,	O
l.	O
,	O
moore	O
,	O
a.	O
w.	O
(	O
1999	O
)	O
.	O
gradient	B
descent	I
for	O
general	O
reinforcement	O
learning	O
.	O
in	O
ad-	O
vances	O
in	O
neural	O
information	O
processing	O
systems	O
11	O
(	O
nips	O
1998	O
)	O
,	O
pp	O
.	O
968–974	O
.	O
mit	O
press	O
,	O
cambridge	O
ma	O
.	O
baldassarre	O
,	O
g.	O
,	O
mirolli	O
,	O
m	O
.	O
(	O
eds	O
.	O
)	O
(	O
2013	O
)	O
.	O
intrinsically	O
motivated	O
learning	O
in	O
natural	O
and	O
artiﬁcial	O
systems	O
.	O
springer-verlag	O
,	O
berlin	O
heidelberg	O
.	O
balke	O
,	O
a.	O
,	O
pearl	O
,	O
j	O
.	O
(	O
1994	O
)	O
.	O
counterfactual	O
probabilities	O
:	O
computational	O
methods	O
,	O
bounds	O
and	O
applications	O
.	O
in	O
proceedings	O
of	O
the	O
tenth	O
international	O
conference	O
on	O
uncertainty	O
in	O
artiﬁcial	O
intelligence	O
(	O
uai-1994	O
,	O
pp	O
.	O
46–54	O
.	O
morgan	O
kaufmann	O
.	O
baras	O
,	O
d.	O
,	O
meir	O
,	O
r.	O
(	O
2007	O
)	O
.	O
reinforcement	B
learning	I
,	O
spike-time-dependent	O
plasticity	O
,	O
and	O
the	O
bcm	O
rule	O
.	O
neural	B
computation	O
,	O
19	O
(	O
8	O
)	O
:2245–2279	O
.	O
barnard	O
,	O
e.	O
(	O
1993	O
)	O
.	O
temporal-diﬀerence	O
methods	O
and	O
markov	O
models	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
23	O
(	O
2	O
)	O
:357–365	O
.	O
barreto	O
,	O
a.	O
s.	O
,	O
precup	O
,	O
d.	O
,	O
pineau	O
,	O
j	O
.	O
(	O
2011	O
)	O
.	O
reinforcement	B
learning	I
using	O
kernel-based	O
stochastic	O
factorization	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
24	O
(	O
nips	O
references	O
487	O
2011	O
)	O
,	O
pp	O
.	O
720–728	O
.	O
curran	O
associates	O
,	O
inc.	O
bartlett	O
,	O
p.	O
l.	O
,	O
baxter	O
,	O
j	O
.	O
(	O
1999	O
)	O
.	O
hebbian	O
synaptic	O
modiﬁcations	O
in	O
spiking	O
neurons	O
that	O
learn	O
.	O
technical	O
report	O
,	O
research	O
school	O
of	O
information	O
sciences	O
and	O
engineering	O
,	O
australian	O
na-	O
tional	O
university	O
.	O
bartlett	O
,	O
p.	O
l.	O
,	O
baxter	O
,	O
j	O
.	O
(	O
2000	O
)	O
.	O
a	O
biologically	O
plausible	O
and	O
locally	O
optimal	O
learning	O
algorithm	O
for	O
spiking	O
neurons	O
.	O
rapport	O
technique	O
,	O
australian	O
national	O
university	O
.	O
barto	O
,	O
a.	O
g.	O
(	O
1985	O
)	O
.	O
learning	O
by	O
statistical	O
cooperation	O
of	O
self-interested	O
neuron-like	O
computing	O
elements	O
.	O
human	O
neurobiology	O
,	O
4	O
(	O
4	O
)	O
:229–256	O
.	O
barto	O
,	O
a.	O
g.	O
(	O
1986	O
)	O
.	O
game-theoretic	O
cooperativity	O
in	O
networks	O
of	O
self-interested	O
units	O
.	O
in	O
j.	O
s.	O
denker	O
(	O
ed	O
.	O
)	O
,	O
neural	B
networks	I
for	O
computing	O
,	O
pp	O
.	O
41–46	O
.	O
american	O
institute	O
of	O
physics	O
,	O
new	O
york	O
.	O
barto	O
,	O
a.	O
g.	O
(	O
1989	O
)	O
.	O
from	O
chemotaxis	O
to	O
cooperativity	O
:	O
abstract	O
exercises	O
in	O
neuronal	O
learning	O
strategies	O
.	O
in	O
r.	O
durbin	O
,	O
r.	O
maill	O
and	O
g.	O
mitchison	O
(	O
eds	O
.	O
)	O
,	O
the	O
computing	O
neuron	O
,	O
pp	O
.	O
73–	O
98.	O
addison-wesley	O
,	O
reading	O
,	O
ma	O
.	O
barto	O
,	O
a.	O
g.	O
(	O
1990	O
)	O
.	O
connectionist	O
learning	O
for	O
control	B
:	O
an	O
overview	O
.	O
in	O
t.	O
miller	O
,	O
r.	O
s.	O
sutton	O
,	O
and	O
p.	O
j.	O
werbos	O
(	O
eds	O
.	O
)	O
,	O
neural	B
networks	I
for	O
control	B
,	O
pp	O
.	O
5–58	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
barto	O
,	O
a.	O
g.	O
(	O
1991	O
)	O
.	O
some	O
learning	O
tasks	O
from	O
a	O
control	B
perspective	O
.	O
in	O
l.	O
nadel	O
and	O
d.	O
l.	O
stein	O
(	O
eds	O
.	O
)	O
,	O
1990	O
lectures	O
in	O
complex	O
systems	O
,	O
pp	O
.	O
195–223	O
.	O
addison-wesley	O
,	O
redwood	O
city	O
,	O
ca	O
.	O
barto	O
,	O
a.	O
g.	O
(	O
1992	O
)	O
.	O
reinforcement	B
learning	I
and	O
adaptive	O
critic	B
methods	O
.	O
in	O
d.	O
a.	O
white	O
and	O
d.	O
a.	O
sofge	O
(	O
eds	O
.	O
)	O
,	O
handbook	O
of	O
intelligent	O
control	B
:	O
neural	B
,	O
fuzzy	O
,	O
and	O
adaptive	O
ap-	O
proaches	O
,	O
pp	O
.	O
469–491	O
.	O
van	O
nostrand	O
reinhold	O
,	O
new	O
york	O
.	O
barto	O
,	O
a.	O
g.	O
(	O
1995a	O
)	O
.	O
adaptive	O
critics	O
and	O
the	O
basal	O
ganglia	O
.	O
in	O
j.	O
c.	O
houk	O
,	O
j.	O
l.	O
davis	O
,	O
and	O
d.	O
g.	O
beiser	O
(	O
eds	O
.	O
)	O
,	O
models	O
of	O
information	O
processing	O
in	O
the	O
basal	O
ganglia	O
,	O
pp	O
.	O
215–232	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
barto	O
,	O
a.	O
g.	O
(	O
1995b	O
)	O
.	O
reinforcement	B
learning	I
.	O
in	O
m.	O
a.	O
arbib	O
(	O
ed	O
.	O
)	O
,	O
handbook	O
of	O
brain	O
theory	O
and	O
neural	O
networks	O
,	O
pp	O
.	O
804–809	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
barto	O
,	O
a.	O
g.	O
(	O
2011	O
)	O
.	O
adaptive	O
real-time	B
dynamic	I
programming	I
.	O
in	O
c.	O
sammut	O
and	O
g.	O
i	O
webb	O
(	O
eds	O
.	O
)	O
,	O
encyclopedia	O
of	O
machine	O
learning	O
,	O
pp	O
.	O
19–22	O
.	O
springer	O
science	O
and	O
business	O
media	O
.	O
barto	O
,	O
a.	O
g.	O
(	O
2013	O
)	O
.	O
intrinsic	B
motivation	O
and	B
reinforcement	I
learning	O
.	O
in	O
g.	O
baldassarre	O
and	O
m.	O
mirolli	O
(	O
eds	O
.	O
)	O
,	O
intrinsically	O
motivated	O
learning	O
in	O
natural	O
and	O
artiﬁcial	O
systems	O
,	O
pp	O
.	O
17–47	O
.	O
springer-verlag	O
,	O
berlin	O
heidelberg	O
.	O
barto	O
,	O
a.	O
g.	O
,	O
anandan	O
,	O
p.	O
(	O
1985	O
)	O
.	O
pattern	O
recognizing	O
stochastic	O
learning	O
automata	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
15	O
(	O
3	O
)	O
:360–375	O
.	O
barto	O
,	O
a.	O
g.	O
,	O
anderson	O
,	O
c.	O
w.	O
(	O
1985	O
)	O
.	O
structural	B
learning	O
in	O
connectionist	O
systems	O
.	O
in	O
program	O
of	O
the	O
seventh	O
annual	O
conference	O
of	O
the	O
cognitive	O
science	O
society	O
,	O
pp	O
.	O
43–54	O
.	O
barto	O
,	O
a.	O
g.	O
,	O
anderson	O
,	O
c.	O
w.	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
1982	O
)	O
.	O
synthesis	O
of	O
nonlinear	O
control	B
surfaces	O
by	O
a	O
layered	O
associative	B
search	I
network	O
.	O
biological	O
cybernetics	B
,	O
43	O
(	O
3	O
)	O
:175–185	O
.	O
barto	O
,	O
a.	O
g.	O
,	O
bradtke	O
,	O
s.	O
j.	O
,	O
singh	O
,	O
s.	O
p.	O
(	O
1991	O
)	O
.	O
real-time	O
learning	O
and	B
control	I
using	O
asyn-	O
chronous	O
dynamic	B
programming	I
.	O
technical	O
report	O
91-57.	O
department	O
of	O
computer	O
and	O
information	O
science	O
,	O
university	O
of	O
massachusetts	O
,	O
amherst	O
.	O
barto	O
,	O
a.	O
g.	O
,	O
bradtke	O
,	O
s.	O
j.	O
,	O
singh	O
,	O
s.	O
p.	O
(	O
1995	O
)	O
.	O
learning	O
to	O
act	O
using	O
real-time	B
dynamic	I
programming	I
.	O
artiﬁcial	B
intelligence	I
,	O
72	O
(	O
1-2	O
)	O
:81–138	O
.	O
barto	O
,	O
a.	O
g.	O
,	O
duﬀ	O
,	O
m.	O
(	O
1994	O
)	O
.	O
monte	O
carlo	O
matrix	O
inversion	O
and	B
reinforcement	I
learning	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
6	O
(	O
nips	O
1993	O
)	O
,	O
pp	O
.	O
687–694	O
.	O
morgan	O
kaufmann	O
,	O
san	O
francisco	O
.	O
488	O
references	O
barto	O
,	O
a.	O
g.	O
,	O
jordan	O
,	O
m.	O
i	O
.	O
(	O
1987	O
)	O
.	O
gradient	B
following	O
without	O
back-propagation	O
in	O
layered	O
in	O
m.	O
caudill	O
and	O
c.	O
butler	O
(	O
eds	O
.	O
)	O
,	O
proceedings	O
of	O
the	O
ieee	O
first	O
annual	O
networks	O
.	O
conference	O
on	O
neural	B
networks	I
,	O
pp	O
.	O
ii629–ii636	O
.	O
sos	O
printing	O
,	O
san	O
diego	O
.	O
barto	O
,	O
a.	O
g.	O
,	O
mahadevan	O
,	O
s.	O
(	O
2003	O
)	O
.	O
recent	O
advances	O
in	O
hierarchical	O
reinforcement	B
learning	I
.	O
discrete	O
event	O
dynamic	O
systems	O
,	O
13	O
(	O
4	O
)	O
:341–379	O
.	O
barto	O
,	O
a.	O
g.	O
,	O
singh	O
,	O
s.	O
p.	O
(	O
1990	O
)	O
.	O
on	O
the	O
computational	O
economics	O
of	O
reinforcement	O
learning	O
.	O
in	O
connectionist	O
models	O
:	O
proceedings	O
of	O
the	O
1990	O
summer	O
school	O
.	O
morgan	O
kaufmann	O
.	O
barto	O
,	O
a.	O
g.	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
1981a	O
)	O
.	O
goal	B
seeking	O
components	O
for	O
adaptive	O
intelligence	O
:	O
an	O
initial	O
assessment	O
.	O
technical	O
report	O
afwal-tr-81-1070	O
.	O
air	O
force	O
wright	O
aeronautical	O
laboratories/avionics	O
laboratory	O
,	O
wright-patterson	O
afb	O
,	O
oh	O
.	O
barto	O
,	O
a.	O
g.	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
1981b	O
)	O
.	O
landmark	O
learning	O
:	O
an	O
illustration	O
of	O
associative	O
search	O
.	O
biological	O
cybernetics	B
,	O
42	O
(	O
1	O
)	O
:1–8	O
.	O
barto	O
,	O
a.	O
g.	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
1982	O
)	O
.	O
simulation	O
of	O
anticipatory	O
responses	O
in	O
classical	O
conditioning	B
by	O
a	O
neuron-like	O
adaptive	O
element	O
.	O
behavioural	O
brain	O
research	O
,	O
4	O
(	O
3	O
)	O
:221–235	O
.	O
barto	O
,	O
a.	O
g.	O
,	O
sutton	O
,	O
r.	O
s.	O
,	O
anderson	O
,	O
c.	O
w.	O
(	O
1983	O
)	O
.	O
neuronlike	O
elements	O
that	O
can	O
solve	O
diﬃcult	O
learning	O
control	O
problems	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
13	O
(	O
5	O
)	O
:835–846	O
.	O
reprinted	O
in	O
j.	O
a.	O
anderson	O
and	O
e.	O
rosenfeld	O
(	O
eds	O
.	O
)	O
,	O
neurocomputing	O
:	O
foundations	O
of	O
research	O
,	O
pp	O
.	O
535–549	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
,	O
1988.	O
barto	O
,	O
a.	O
g.	O
,	O
sutton	O
,	O
r.	O
s.	O
,	O
brouwer	O
,	O
p.	O
s.	O
(	O
1981	O
)	O
.	O
associative	B
search	I
network	O
:	O
a	O
reinforcement	B
learning	I
associative	O
memory	O
.	O
biological	O
cybernetics	B
,	O
40	O
(	O
3	O
)	O
:201–211	O
.	O
barto	O
,	O
a.	O
g.	O
,	O
sutton	O
,	O
r.	O
s.	O
,	O
watkins	O
,	O
c.	O
j.	O
c.	O
h.	O
(	O
1990	O
)	O
.	O
learning	O
and	O
sequential	O
decision	O
in	O
m.	O
gabriel	O
and	O
j.	O
moore	O
(	O
eds	O
.	O
)	O
,	O
learning	O
and	O
computational	O
neuroscience	O
:	O
making	O
.	O
foundations	O
of	O
adaptive	O
networks	O
,	O
pp	O
.	O
539–602	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
baxter	O
,	O
j.	O
,	O
bartlett	O
,	O
p.	O
l.	O
(	O
2001	O
)	O
.	O
inﬁnite-horizon	O
policy-gradient	O
estimation	O
.	O
journal	O
of	O
arti-	O
ﬁcial	O
intelligence	O
research	O
,	O
15	O
:319–350	O
.	O
baxter	O
,	O
j.	O
,	O
bartlett	O
,	O
p.	O
l.	O
,	O
weaver	O
,	O
l.	O
(	O
2001	O
)	O
.	O
experiments	O
with	O
inﬁnite-horizon	O
,	O
policy-gradient	O
estimation	O
.	O
journal	O
of	O
artiﬁcial	O
intelligence	O
research	O
,	O
15	O
:351–381	O
.	O
bellemare	O
,	O
m.	O
g.	O
,	O
dabney	O
,	O
w.	O
,	O
munos	O
,	O
r.	O
(	O
2017	O
)	O
.	O
a	O
distributional	O
perspective	O
on	O
reinforcement	B
learning	I
.	O
arxiv	O
preprint	O
arxiv:1707.06887.	O
bellemare	O
,	O
m.	O
g.	O
,	O
naddaf	O
,	O
y.	O
,	O
veness	O
,	O
j.	O
,	O
bowling	O
,	O
m.	O
(	O
2013	O
)	O
.	O
the	O
arcade	O
learning	O
environment	O
:	O
journal	O
of	O
artiﬁcial	O
intelligence	O
research	O
,	O
an	O
evaluation	O
platform	O
for	O
general	O
agents	O
.	O
47:253–279	O
.	O
bellemare	O
,	O
m.	O
g.	O
,	O
veness	O
,	O
j.	O
,	O
bowling	O
,	O
m.	O
(	O
2012	O
)	O
.	O
investigating	O
contingency	O
awareness	O
using	O
in	O
proceedings	O
of	O
the	O
twenty-sixth	O
aaai	O
conference	O
on	O
artiﬁcial	O
atari	O
2600	O
games	O
.	O
intelligence	O
(	O
aaai-12	O
)	O
,	O
pp	O
.	O
864–871	O
.	O
aaai	O
press	O
,	O
menlo	O
park	O
,	O
ca	O
.	O
bellman	O
,	O
r.	O
e.	O
(	O
1956	O
)	O
.	O
a	O
problem	O
in	O
the	O
sequential	O
design	B
of	I
experiments	O
.	O
sankhya	O
,	O
16:221–229	O
.	O
bellman	O
,	O
r.	O
e.	O
(	O
1957a	O
)	O
.	O
dynamic	B
programming	I
.	O
princeton	O
university	O
press	O
,	O
princeton	O
.	O
bellman	O
,	O
r.	O
e.	O
(	O
1957b	O
)	O
.	O
a	O
markov	O
decision	O
process	O
.	O
journal	O
of	O
mathematics	O
and	O
mechanics	O
,	O
6	O
(	O
5	O
)	O
:679–684	O
.	O
bellman	O
,	O
r.	O
e.	O
,	O
dreyfus	O
,	O
s.	O
e.	O
(	O
1959	O
)	O
.	O
functional	O
approximations	O
and	B
dynamic	I
programming	I
.	O
mathematical	O
tables	O
and	O
other	O
aids	O
to	O
computation	O
,	O
13:247–251	O
.	O
bellman	O
,	O
r.	O
e.	O
,	O
kalaba	O
,	O
r.	O
,	O
kotkin	O
,	O
b	O
.	O
(	O
1973	O
)	O
.	O
polynomial	O
approximation—a	O
new	O
computa-	O
tional	O
technique	O
in	O
dynamic	O
programming	O
:	O
allocation	O
processes	O
.	O
mathematical	O
computa-	O
tion	B
,	O
17:155–161	O
.	O
bengio	O
,	O
y	O
.	O
(	O
2009	O
)	O
.	O
learning	O
deep	O
architectures	O
for	O
ai	O
.	O
foundations	O
and	O
trends	O
in	O
machine	O
learning	O
,	O
2	O
(	O
1	O
)	O
:1–27	O
.	O
bengio	O
,	O
y.	O
,	O
courville	O
,	O
a.	O
c.	O
,	O
vincent	O
,	O
p.	O
(	O
2012	O
)	O
.	O
unsupervised	O
feature	O
learning	O
and	O
deep	O
learn-	O
references	O
489	O
ing	B
:	O
a	O
review	O
and	O
new	O
perspectives	O
.	O
corr	O
1	O
,	O
arxiv	O
1206.5538.	O
bentley	O
,	O
j.	O
l.	O
(	O
1975	O
)	O
.	O
multidimensional	O
binary	O
search	O
trees	O
used	O
for	O
associative	O
searching	O
.	O
communications	O
of	O
the	O
acm	O
,	O
18	O
(	O
9	O
)	O
:509–517	O
.	O
berg	O
,	O
h.	O
c.	O
(	O
1975	O
)	O
.	O
chemotaxis	O
in	O
bacteria	O
.	O
annual	O
review	O
of	O
biophysics	O
and	O
bioengineering	O
,	O
4	O
(	O
1	O
)	O
:119–136	O
.	O
berns	O
,	O
g.	O
s.	O
,	O
mcclure	O
,	O
s.	O
m.	O
,	O
pagnoni	O
,	O
g.	O
,	O
montague	O
,	O
p.	O
r.	O
(	O
2001	O
)	O
.	O
predictability	O
modulates	O
human	O
brain	O
response	O
to	O
reward	O
.	O
the	O
journal	O
of	O
neuroscience	O
,	O
21	O
(	O
8	O
)	O
:2793–2798	O
.	O
berridge	O
,	O
k.	O
c.	O
,	O
kringelbach	O
,	O
m.	O
l.	O
(	O
2008	O
)	O
.	O
aﬀective	O
neuroscience	B
of	O
pleasure	O
:	O
reward	O
in	O
humans	O
and	O
animals	O
.	O
psychopharmacology	O
,	O
199	O
(	O
3	O
)	O
:457–480	O
.	O
berridge	O
,	O
k.	O
c.	O
,	O
robinson	O
,	O
t.	O
e.	O
(	O
1998	O
)	O
.	O
what	O
is	O
the	O
role	O
of	O
dopamine	O
in	O
reward	O
:	O
hedonic	O
impact	O
,	O
reward	O
learning	O
,	O
or	O
incentive	O
salience	O
?	O
brain	O
research	O
reviews	O
,	O
28	O
(	O
3	O
)	O
:309–369	O
.	O
berry	O
,	O
d.	O
a.	O
,	O
fristedt	O
,	O
b	O
.	O
(	O
1985	O
)	O
.	O
bandit	B
problems	I
.	O
chapman	O
and	O
hall	O
,	O
london	O
.	O
bertsekas	O
,	O
d.	O
p.	O
(	O
1982	O
)	O
.	O
distributed	O
dynamic	B
programming	I
.	O
ieee	O
transactions	O
on	O
automatic	O
control	O
,	O
27	O
(	O
3	O
)	O
:610–616	O
.	O
bertsekas	O
,	O
d.	O
p.	O
(	O
1983	O
)	O
.	O
distributed	O
asynchronous	O
computation	O
of	O
ﬁxed	O
points	O
.	O
mathematical	O
programming	O
,	O
27	O
(	O
1	O
)	O
:107–120	O
.	O
bertsekas	O
,	O
d.	O
p.	O
(	O
1987	O
)	O
.	O
dynamic	B
programming	I
:	O
deterministic	O
and	O
stochastic	O
models	O
.	O
prentice-	O
hall	O
,	O
englewood	O
cliﬀs	O
,	O
nj	O
.	O
bertsekas	O
,	O
d.	O
p.	O
(	O
2005	O
)	O
.	O
dynamic	B
programming	I
and	O
optimal	B
control	I
,	O
volume	O
1	O
,	O
third	O
edition	O
.	O
athena	O
scientiﬁc	O
,	O
belmont	O
,	O
ma	O
.	O
bertsekas	O
,	O
d.	O
p.	O
(	O
2012	O
)	O
.	O
dynamic	B
programming	I
and	O
optimal	B
control	I
,	O
volume	O
2	O
:	O
approximate	B
dynamic	I
programming	I
,	O
fourth	O
edition	O
.	O
athena	O
scientiﬁc	O
,	O
belmont	O
,	O
ma	O
.	O
bertsekas	O
,	O
d.	O
p.	O
(	O
2013	O
)	O
.	O
rollout	B
algorithms	I
for	O
discrete	O
optimization	O
:	O
a	O
survey	O
.	O
in	O
handbook	O
of	O
combinatorial	O
optimization	O
,	O
pp	O
.	O
2989–3013	O
.	O
springer	O
,	O
new	O
york	O
.	O
bertsekas	O
,	O
d.	O
p.	O
,	O
tsitsiklis	O
,	O
j.	O
n.	O
(	O
1989	O
)	O
.	O
parallel	O
and	O
distributed	O
computation	O
:	O
numerical	O
methods	O
.	O
prentice-hall	O
,	O
englewood	O
cliﬀs	O
,	O
nj	O
.	O
bertsekas	O
,	O
d.	O
p.	O
,	O
tsitsiklis	O
,	O
j.	O
n.	O
(	O
1996	O
)	O
.	O
neuro-dynamic	O
programming	O
.	O
athena	O
scientiﬁc	O
,	O
belmont	O
,	O
ma	O
.	O
bertsekas	O
,	O
d.	O
p.	O
,	O
tsitsiklis	O
,	O
j.	O
n.	O
,	O
wu	O
,	O
c.	O
(	O
1997	O
)	O
.	O
rollout	B
algorithms	I
for	O
combinatorial	O
opti-	O
mization	O
.	O
journal	O
of	O
heuristics	O
,	O
3	O
(	O
3	O
)	O
:245–262	O
.	O
bertsekas	O
,	O
d.	O
p.	O
,	O
yu	O
,	O
h.	O
(	O
2009	O
)	O
.	O
projected	O
equation	O
methods	O
for	O
approximate	O
solution	O
of	O
large	O
linear	O
systems	O
.	O
journal	O
of	O
computational	O
and	O
applied	O
mathematics	O
,	O
227	O
(	O
1	O
)	O
:27–50	O
.	O
bhat	O
,	O
n.	O
,	O
farias	O
,	O
v.	O
,	O
moallemi	O
,	O
c.	O
c.	O
(	O
2012	O
)	O
.	O
non-parametric	O
approximate	O
dynamic	O
program-	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
25	O
ming	O
via	O
the	O
kernel	O
method	O
.	O
(	O
nips	O
2012	O
)	O
,	O
pp	O
.	O
386–394	O
.	O
curran	O
associates	O
,	O
inc.	O
bhatnagar	O
,	O
s.	O
,	O
sutton	O
,	O
r.	O
,	O
ghavamzadeh	O
,	O
m.	O
,	O
lee	O
,	O
m.	O
(	O
2009	O
)	O
.	O
natural	O
actor–critic	B
algorithms	O
.	O
automatica	O
,	O
45	O
(	O
11	O
)	O
.	O
biermann	O
,	O
a.	O
w.	O
,	O
fairﬁeld	O
,	O
j.	O
r.	O
c.	O
,	O
beres	O
,	O
t.	O
r.	O
(	O
1982	O
)	O
.	O
signature	O
table	O
systems	O
and	O
learning	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
12	O
(	O
5	O
)	O
:635–648	O
.	O
bishop	O
,	O
c.	O
m.	O
(	O
1995	O
)	O
.	O
neural	B
networks	I
for	O
pattern	O
recognition	O
.	O
clarendon	O
,	O
oxford	O
.	O
bishop	O
,	O
c.	O
m.	O
(	O
2006	O
)	O
.	O
pattern	O
recognition	O
and	O
machine	O
learning	O
.	O
springer	O
science	O
+	O
business	O
media	O
new	O
york	O
llc	O
.	O
blodgett	O
,	O
h.	O
c.	O
(	O
1929	O
)	O
.	O
the	O
eﬀect	O
of	O
the	O
introduction	O
of	O
reward	O
upon	O
the	O
maze	O
performance	O
of	O
rats	O
.	O
university	O
of	O
california	O
publications	O
in	B
psychology	I
,	O
4:113–134	O
.	O
boakes	O
,	O
r.	O
a.	O
,	O
costa	O
,	O
d.	O
s.	O
j	O
.	O
(	O
2014	O
)	O
.	O
temporal	O
contiguity	O
in	O
associative	O
learning	O
:	O
iinterference	O
journal	O
of	O
experimental	O
psychology	B
:	O
animal	O
and	O
decay	O
from	O
an	O
historical	O
perspective	O
.	O
learning	O
and	O
cognition	O
,	O
40	O
(	O
4	O
)	O
:381–400	O
.	O
490	O
references	O
booker	O
,	O
l.	O
b	O
.	O
(	O
1982	O
)	O
.	O
intelligent	O
behavior	O
as	O
an	O
adaptation	O
to	O
the	O
task	O
environment	B
.	O
ph.d.	O
the-	O
sis	O
,	O
university	O
of	O
michigan	O
,	O
ann	O
arbor	O
.	O
bostrom	O
,	O
n.	O
(	O
2014	O
)	O
.	O
superintelligence	O
:	O
paths	O
,	O
dangers	O
,	O
strategies	O
.	O
oxford	O
university	O
press	O
,	O
oxford	O
.	O
bottou	O
,	O
l.	O
,	O
vapnik	O
,	O
v.	O
(	O
1992	O
)	O
.	O
local	O
learning	O
algorithms	O
.	O
neural	B
computation	O
,	O
4	O
(	O
6	O
)	O
:888–900	O
.	O
boyan	O
,	O
j.	O
a	O
.	O
(	O
1999	O
)	O
.	O
least-squares	O
temporal	O
diﬀerence	O
learning	O
.	O
in	O
proceedings	O
of	O
the	O
16th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
1999	O
)	O
,	O
pp	O
.	O
49–56	O
.	O
boyan	O
,	O
j.	O
a	O
.	O
(	O
2002	O
)	O
.	O
technical	O
update	O
:	O
least-squares	O
temporal	O
diﬀerence	O
learning	O
.	O
machine	O
learning	O
,	O
49	O
(	O
2	O
)	O
:233–246	O
.	O
boyan	O
,	O
j.	O
a.	O
,	O
moore	O
,	O
a.	O
w.	O
(	O
1995	O
)	O
.	O
generalization	O
in	O
reinforcement	O
learning	O
:	O
safely	O
approxi-	O
mating	O
the	O
value	B
function	I
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
7	O
(	O
nips	O
1994	O
)	O
,	O
pp	O
.	O
369–376	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
bradtke	O
,	O
s.	O
j	O
.	O
(	O
1993	O
)	O
.	O
reinforcement	B
learning	I
applied	O
to	O
linear	O
quadratic	O
regulation	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
5	O
(	O
nips	O
1992	O
)	O
,	O
pp	O
.	O
295–302	O
.	O
morgan	O
kaufmann	O
.	O
bradtke	O
,	O
s.	O
j	O
.	O
(	O
1994	O
)	O
.	O
incremental	O
dynamic	O
programming	O
for	O
on-line	O
adaptive	O
optimal	B
control	I
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
massachusetts	O
,	O
amherst	O
.	O
appeared	O
as	O
cmpsci	O
technical	O
report	O
94-62.	O
bradtke	O
,	O
s.	O
j.	O
,	O
barto	O
,	O
a.	O
g.	O
(	O
1996	O
)	O
.	O
linear	O
least–squares	O
algorithms	O
for	O
temporal	O
diﬀerence	O
learning	O
.	O
machine	O
learning	O
,	O
22:33–57	O
.	O
bradtke	O
,	O
s.	O
j.	O
,	O
ydstie	O
,	O
b.	O
e.	O
,	O
barto	O
,	O
a.	O
g.	O
(	O
1994	O
)	O
.	O
adaptive	O
linear	O
quadratic	O
control	B
using	O
policy	B
in	O
proceedings	O
of	O
the	O
american	O
control	B
conference	O
,	O
pp	O
.	O
3475–3479	O
.	O
american	O
iteration	O
.	O
automatic	O
control	O
council	O
,	O
evanston	O
,	O
il	O
.	O
brafman	O
,	O
r.	O
i.	O
,	O
tennenholtz	O
,	O
m.	O
(	O
2003	O
)	O
.	O
r-max	O
–	O
a	O
general	O
polynomial	O
time	O
algorithm	O
for	O
near-optimal	O
reinforcement	B
learning	I
.	O
journal	O
of	O
machine	O
learning	O
research	O
,	O
3	O
:213–231	O
.	O
breiman	O
,	O
l.	O
(	O
2001	O
)	O
.	O
random	O
forests	O
.	O
machine	O
learning	O
,	O
45	O
(	O
1	O
)	O
:5–32	O
.	O
breiter	O
,	O
h.	O
c.	O
,	O
aharon	O
,	O
i.	O
,	O
kahneman	O
,	O
d.	O
,	O
dale	O
,	O
a.	O
,	O
shizgal	O
,	O
p.	O
(	O
2001	O
)	O
.	O
functional	O
imaging	O
of	O
neural	O
responses	O
to	O
expectancy	O
and	O
experience	O
of	O
monetary	O
gains	O
and	O
losses	O
.	O
neuron	O
,	O
30	O
(	O
2	O
)	O
:619–639	O
.	O
breland	O
,	O
k.	O
,	O
breland	O
,	O
m.	O
(	O
1961	O
)	O
.	O
the	O
misbehavior	O
of	O
organisms	O
.	O
american	O
psychologist	O
,	O
16	O
(	O
11	O
)	O
:681–684	O
.	O
bridle	O
,	O
j.	O
s.	O
(	O
1990	O
)	O
.	O
training	O
stochastic	O
model	O
recognition	O
algorithms	O
as	O
networks	O
can	O
lead	O
to	O
maximum	O
mutual	O
information	O
estimates	O
of	O
parameters	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
2	O
(	O
nips	O
1989	O
)	O
,	O
pp	O
.	O
211–217	O
.	O
morgan	O
kaufmann	O
,	O
san	O
mateo	O
,	O
ca	O
.	O
broomhead	O
,	O
d.	O
s.	O
,	O
lowe	O
,	O
d.	O
(	O
1988	O
)	O
.	O
multivariable	O
functional	O
interpolation	O
and	O
adaptive	O
net-	O
works	O
.	O
complex	O
systems	O
,	O
2:321–355	O
.	O
bromberg-martin	O
,	O
e.	O
s.	O
,	O
matsumoto	O
,	O
m.	O
,	O
hong	O
,	O
s.	O
,	O
hikosaka	O
,	O
o	O
.	O
(	O
2010	O
)	O
.	O
a	O
pallidus-habenula-	O
dopamine	B
pathway	O
signals	O
inferred	O
stimulus	O
values	O
.	O
journal	O
of	O
neurophysiology	O
,	O
104	O
(	O
2	O
)	O
:1068–	O
1076.	O
browne	O
,	O
c.b.	O
,	O
powley	O
,	O
e.	O
,	O
whitehouse	O
,	O
d.	O
,	O
lucas	O
,	O
s.m.	O
,	O
cowling	O
,	O
p.i.	O
,	O
rohlfshagen	O
,	O
p.	O
,	O
tavener	O
,	O
s.	O
,	O
perez	O
,	O
d.	O
,	O
samothrakis	O
,	O
s.	O
,	O
colton	O
,	O
s.	O
(	O
2012	O
)	O
.	O
a	O
survey	O
of	O
monte	O
carlo	O
tree	O
search	O
methods	O
.	O
ieee	O
transactions	O
on	O
computational	O
intelligence	O
and	O
ai	O
in	O
games	O
,	O
4	O
(	O
1	O
)	O
:1–43	O
.	O
brown	O
,	O
j.	O
,	O
bullock	O
,	O
d.	O
,	O
grossberg	O
,	O
s.	O
(	O
1999	O
)	O
.	O
how	O
the	O
basal	B
ganglia	I
use	O
parallel	O
excitatory	O
and	O
inhibitory	O
learning	O
pathways	O
to	O
selectively	O
respond	O
to	O
unexpected	O
rewarding	O
cues	O
.	O
the	O
journal	O
of	O
neuroscience	O
,	O
19	O
(	O
23	O
)	O
:10502–10511	O
.	O
bryson	O
,	O
a.	O
e.	O
,	O
jr.	O
(	O
1996	O
)	O
.	O
optimal	O
control—1950	O
to	O
1985.	O
ieee	O
control	B
systems	O
,	O
13	O
(	O
3	O
)	O
:26–33	O
.	O
buchanan	O
,	O
b.	O
g.	O
,	O
mitchell	O
,	O
t.	O
,	O
smith	O
,	O
r.	O
g.	O
,	O
johnson	O
,	O
c.	O
r.	O
,	O
jr.	O
(	O
1978	O
)	O
.	O
models	O
of	O
learning	O
references	O
491	O
systems	O
.	O
encyclopedia	O
of	O
computer	O
science	O
and	O
technology	O
,	O
11.	O
buhusi	O
,	O
c.	O
v.	O
,	O
schmajuk	O
,	O
n.	O
a	O
.	O
(	O
1999	O
)	O
.	O
timing	O
in	O
simple	O
conditioning	B
and	O
occasion	O
setting	O
:	O
a	O
neural	B
network	O
approach	O
.	O
behavioural	O
processes	O
,	O
45	O
(	O
1	O
)	O
:33–57	O
.	O
bu¸soniu	O
,	O
l.	O
,	O
lazaric	O
,	O
a.	O
,	O
ghavamzadeh	O
,	O
m.	O
,	O
munos	O
,	O
r.	O
,	O
babu˘ska	O
,	O
r.	O
,	O
de	O
schutter	O
,	O
b	O
.	O
(	O
2012	O
)	O
.	O
in	O
m.	O
wiering	O
and	O
m.	O
van	O
otterlo	O
(	O
eds	O
.	O
)	O
,	O
least-squares	O
methods	O
for	O
policy	O
iteration	O
.	O
reinforcement	B
learning	I
:	O
state-of-the-art	O
,	O
pp	O
.	O
75–109	O
.	O
springer-verlag	O
berlin	O
heidelberg	O
.	O
bush	O
,	O
r.	O
r.	O
,	O
mosteller	O
,	O
f.	O
(	O
1955	O
)	O
.	O
stochastic	O
models	O
for	O
learning	O
.	O
wiley	O
,	O
new	O
york	O
.	O
byrne	O
,	O
j.	O
h.	O
,	O
gingrich	O
,	O
k.	O
j.	O
,	O
baxter	O
,	O
d.	O
a	O
.	O
(	O
1990	O
)	O
.	O
computational	O
capabilities	O
of	O
single	O
neurons	O
:	O
relationship	O
to	O
simple	O
forms	O
of	O
associative	O
and	O
nonassociative	O
learning	O
in	O
aplysia	O
.	O
in	O
r.	O
d.	O
hawkins	O
and	O
g.	O
h.	O
bower	O
(	O
eds	O
.	O
)	O
,	O
computational	O
models	O
of	O
learning	O
,	O
pp	O
.	O
31–63	O
.	O
academic	O
press	O
,	O
new	O
york	O
.	O
calabresi	O
,	O
p.	O
,	O
picconi	O
,	O
b.	O
,	O
tozzi	O
,	O
a.	O
,	O
filippo	O
,	O
m.	O
d.	O
(	O
2007	O
)	O
.	O
dopamine-mediated	O
regulation	O
of	O
corticostriatal	O
synaptic	B
plasticity	I
.	O
trends	O
in	B
neuroscience	I
,	O
30	O
(	O
5	O
)	O
:211–219	O
.	O
camerer	O
,	O
c.	O
(	O
2011	O
)	O
.	O
behavioral	O
game	B
theory	I
:	O
experiments	O
in	O
strategic	O
interaction	O
.	O
princeton	O
university	O
press	O
.	O
campbell	O
,	O
d.	O
t.	O
(	O
1960	O
)	O
.	O
knowledge-processes	O
.	O
pp	O
.	O
205–231	O
.	O
pergamon	O
,	O
new	O
york	O
.	O
blind	O
variation	O
and	O
selective	O
survival	O
as	O
a	O
general	O
strategy	O
in	O
in	O
m.	O
c.	O
yovits	O
and	O
s.	O
cameron	O
(	O
eds	O
.	O
)	O
,	O
self-organizing	O
systems	O
,	O
cao	O
,	O
x.	O
r.	O
(	O
2009	O
)	O
.	O
stochastic	O
learning	O
and	O
optimization—a	O
sensitivity-based	O
approach	O
.	O
annual	O
reviews	O
in	O
control	O
,	O
33	O
(	O
1	O
)	O
:11–24	O
.	O
cao	O
,	O
x.	O
r.	O
,	O
chen	O
,	O
h.	O
f.	O
(	O
1997	O
)	O
.	O
perturbation	O
realization	O
,	O
potentials	O
,	O
and	O
sensitivity	O
analysis	O
of	O
markov	O
processes	O
.	O
ieee	O
transactions	O
on	O
automatic	O
control	O
,	O
42	O
(	O
10	O
)	O
:1382–1393	O
.	O
carlstr¨om	O
,	O
j.	O
,	O
nordstr¨om	O
,	O
e.	O
(	O
1997	O
)	O
.	O
control	B
of	O
self-similar	O
atm	O
call	O
traﬃc	O
by	O
reinforcement	B
learning	I
.	O
in	O
proceedings	O
of	O
the	O
international	O
workshop	O
on	O
applications	O
of	O
neural	B
networks	I
to	O
telecommunications	O
3	O
,	O
pp	O
.	O
54–62	O
.	O
erlbaum	O
,	O
hillsdale	O
,	O
nj	O
.	O
chapman	O
,	O
d.	O
,	O
kaelbling	O
,	O
l.	O
p.	O
(	O
1991	O
)	O
.	O
input	O
generalization	O
in	O
delayed	O
reinforcement	B
learning	I
:	O
an	O
algorithm	O
and	O
performance	O
comparisons	O
.	O
in	O
proceedings	O
of	O
the	O
twelfth	O
international	O
conference	O
on	O
artiﬁcial	B
intelligence	I
(	O
ijcai-91	O
)	O
,	O
pp	O
.	O
726–731	O
.	O
morgan	O
kaufmann	O
,	O
san	O
mateo	O
,	O
ca	O
.	O
chaslot	O
,	O
g.	O
,	O
bakkes	O
,	O
s.	O
,	O
szita	O
,	O
i.	O
,	O
spronck	O
,	O
p.	O
(	O
2008	O
)	O
.	O
monte-carlo	O
tree	O
search	O
:	O
a	O
new	O
framework	O
for	O
game	O
ai	O
.	O
in	O
proceedings	O
of	O
the	O
fourth	O
aaai	O
conference	O
on	O
artiﬁcial	B
intelligence	I
and	O
interactive	O
digital	O
entertainment	O
(	O
aide-08	O
)	O
,	O
pp	O
.	O
216–217	O
.	O
aaai	O
press	O
,	O
menlo	O
park	O
,	O
ca	O
.	O
chow	O
,	O
c.-s.	O
,	O
tsitsiklis	O
,	O
j.	O
n.	O
(	O
1991	O
)	O
.	O
an	O
optimal	O
one-way	O
multigrid	O
algorithm	O
for	O
discrete-time	O
stochastic	O
control	O
.	O
ieee	O
transactions	O
on	O
automatic	O
control	O
,	O
36	O
(	O
8	O
)	O
:898–914	O
.	O
chrisman	O
,	O
l.	O
(	O
1992	O
)	O
.	O
reinforcement	B
learning	I
with	O
perceptual	O
aliasing	O
:	O
the	O
perceptual	O
distinc-	O
tions	O
approach	O
.	O
in	O
proceedings	O
of	O
the	O
tenth	O
national	O
conference	O
on	O
artiﬁcial	B
intelligence	I
(	O
aaai-92	O
)	O
,	O
pp	O
.	O
183–188	O
.	O
aaai/mit	O
press	O
,	O
menlo	O
park	O
,	O
ca	O
.	O
christensen	O
,	O
j.	O
,	O
korf	O
,	O
r.	O
e.	O
(	O
1986	O
)	O
.	O
a	O
uniﬁed	O
theory	O
of	O
heuristic	O
evaluation	O
functions	O
and	O
in	O
proceedings	O
of	O
the	O
fifth	O
national	O
conference	O
on	O
artiﬁcial	O
its	O
application	O
to	O
learning	O
.	O
intelligence	O
,	O
pp	O
.	O
148–152	O
.	O
morgan	O
kaufmann	O
.	O
cichosz	O
,	O
p.	O
(	O
1995	O
)	O
.	O
truncating	O
temporal	O
diﬀerences	O
:	O
on	O
the	O
eﬃcient	O
implementation	O
of	O
td	O
(	O
λ	O
)	O
for	O
reinforcement	O
learning	O
.	O
journal	O
of	O
artiﬁcial	O
intelligence	O
research	O
,	O
2:287–318	O
.	O
claridge-chang	O
,	O
a.	O
,	O
roorda	O
,	O
r.	O
d.	O
,	O
vrontou	O
,	O
e.	O
,	O
sjulson	O
,	O
l.	O
,	O
li	O
,	O
h.	O
,	O
hirsh	O
,	O
j.	O
,	O
miesenb¨ock	O
,	O
g.	O
(	O
2009	O
)	O
.	O
writing	O
memories	O
with	O
light-addressable	O
reinforcement	O
circuitry	O
.	O
cell	O
,	O
139	O
(	O
2	O
)	O
:405–	O
415.	O
clark	O
,	O
r.	O
e.	O
,	O
squire	O
,	O
l.	O
r.	O
(	O
1998	O
)	O
.	O
classical	B
conditioning	I
and	O
brain	O
systems	O
:	O
the	O
role	O
of	O
awareness	O
.	O
science	O
,	O
280	O
(	O
5360	O
)	O
:77–81	O
.	O
492	O
references	O
clark	O
,	O
w.	O
a.	O
,	O
farley	O
,	O
b.	O
g.	O
(	O
1955	O
)	O
.	O
generalization	O
of	O
pattern	O
recognition	O
in	O
a	O
self-organizing	O
system	O
.	O
in	O
proceedings	O
of	O
the	O
1955	O
western	O
joint	O
computer	O
conference	O
,	O
pp	O
.	O
86–91	O
.	O
clouse	O
,	O
j	O
.	O
(	O
1996	O
)	O
.	O
on	O
integrating	O
apprentice	O
learning	O
and	O
reinforcement	B
learning	I
title2	O
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
massachusetts	O
,	O
amherst	O
.	O
appeared	O
as	O
cmpsci	O
technical	O
report	O
96-026.	O
clouse	O
,	O
j.	O
,	O
utgoﬀ	O
,	O
p.	O
(	O
1992	O
)	O
.	O
a	O
teaching	O
method	O
for	O
reinforcement	O
learning	O
systems	O
.	O
in	O
proceedings	O
of	O
the	O
9th	O
international	O
workshop	O
on	O
machine	O
learning	O
,	O
pp	O
.	O
92–101	O
.	O
morgan	O
kaufmann	O
.	O
cobo	O
,	O
l.	O
c.	O
,	O
zang	O
,	O
p.	O
,	O
isbell	O
,	O
c.	O
l.	O
,	O
thomaz	O
,	O
a.	O
l.	O
(	O
2011	O
)	O
.	O
automatic	O
state	O
abstraction	O
from	O
in	O
proceedings	O
of	O
the	O
twenty-second	O
international	O
joint	O
conference	O
on	O
demonstration	O
.	O
artiﬁcial	B
intelligence	I
(	O
ijcai-11	O
)	O
,	O
pp	O
.	O
1243-1248.	O
aaai	O
press	O
.	O
connell	O
,	O
j	O
.	O
(	O
1989	O
)	O
.	O
a	O
colony	O
architecture	O
for	O
an	O
artiﬁcial	O
creature	O
.	O
technical	O
report	O
ai-tr-	O
1151.	O
mit	O
artiﬁcial	B
intelligence	I
laboratory	O
,	O
cambridge	O
,	O
ma	O
.	O
connell	O
,	O
m.	O
e.	O
,	O
utgoﬀ	O
,	O
p.	O
e.	O
(	O
1987	O
)	O
.	O
learning	O
to	O
control	B
a	O
dynamic	O
physical	O
system	O
.	O
compu-	O
tational	O
intelligence	O
,	O
3	O
(	O
1	O
)	O
:330–337	O
.	O
contreras-vidal	O
,	O
j.	O
l.	O
,	O
schultz	O
,	O
w.	O
(	O
1999	O
)	O
.	O
a	O
predictive	O
reinforcement	O
model	O
of	O
dopamine	O
neurons	O
for	O
learning	O
approach	O
behavior	O
.	O
journal	O
of	O
computational	O
neuroscience	B
,	O
6	O
(	O
3	O
)	O
:191–	O
214.	O
coulom	O
,	O
r.	O
(	O
2006	O
)	O
.	O
eﬃcient	O
selectivity	O
and	O
backup	O
operators	O
in	O
monte-carlo	O
tree	O
search	O
.	O
in	O
proceedings	O
of	O
the	O
5th	O
international	O
conference	O
on	O
computers	O
and	O
games	O
(	O
cg	O
’	O
06	O
)	O
,	O
pp	O
.	O
72–	O
83.	O
springer-verlag	O
berlin	O
,	O
heidelberg	O
.	O
courville	O
,	O
a.	O
c.	O
,	O
daw	O
,	O
n.	O
d.	O
,	O
touretzky	O
,	O
d.	O
s.	O
(	O
2006	O
)	O
.	O
bayesian	O
theories	O
of	O
conditioning	O
in	O
a	O
changing	O
world	O
.	O
trends	O
in	O
cognitive	O
science	O
,	O
10	O
(	O
7	O
)	O
:294–300	O
.	O
craik	O
,	O
k.	O
j.	O
w.	O
(	O
1943	O
)	O
.	O
the	O
nature	O
of	O
explanation	O
.	O
cambridge	O
university	O
press	O
,	O
cambridge	O
.	O
cross	O
,	O
j.	O
g.	O
(	O
1973	O
)	O
.	O
a	O
stochastic	O
learning	O
model	O
of	O
economic	O
behavior	O
.	O
the	O
quarterly	O
journal	O
of	O
economics	O
,	O
87	O
(	O
2	O
)	O
:239–266	O
.	O
crow	O
,	O
t.	O
j	O
.	O
(	O
1968	O
)	O
.	O
cortical	O
synapses	O
and	B
reinforcement	I
:	O
a	O
hypothesis	O
.	O
nature	O
,	O
219	O
(	O
5155	O
)	O
:736–	O
737.	O
curtiss	O
,	O
j.	O
h.	O
(	O
1954	O
)	O
.	O
a	O
theoretical	O
comparison	O
of	O
the	O
eﬃciencies	O
of	O
two	O
classical	O
methods	O
and	O
a	O
monte	O
carlo	O
method	O
for	O
computing	O
one	O
component	O
of	O
the	O
solution	O
of	O
a	O
set	O
of	O
linear	O
algebraic	O
equations	O
.	O
in	O
h.	O
a.	O
meyer	O
(	O
ed	O
.	O
)	O
,	O
symposium	O
on	O
monte	O
carlo	O
methods	O
,	O
pp	O
.	O
191–233	O
.	O
wiley	O
,	O
new	O
york	O
.	O
cybenko	O
,	O
g.	O
(	O
1989	O
)	O
.	O
approximation	O
by	O
superpositions	O
of	O
a	O
sigmoidal	O
function	O
.	O
mathematics	O
of	O
control	O
,	O
signals	O
and	O
systems	O
,	O
2	O
(	O
4	O
)	O
:303–314	O
.	O
cziko	O
,	O
g.	O
(	O
1995	O
)	O
.	O
without	O
miracles	O
:	O
universal	O
selection	O
theory	O
and	O
the	O
second	O
darvinian	O
revolution	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
dabney	O
,	O
w.	O
(	O
2014	O
)	O
.	O
adaptive	O
step-sizes	O
for	O
reinforcement	O
learning	O
.	O
phd	O
thesis	O
,	O
university	O
of	O
massachusetts	O
,	O
amherst	O
.	O
dabney	O
,	O
w.	O
,	O
barto	O
,	O
a.	O
g.	O
(	O
2012	O
)	O
.	O
adaptive	O
step-size	O
for	O
online	B
temporal	O
diﬀerence	O
learning	O
.	O
in	O
proceedings	O
of	O
the	O
annual	O
conference	O
of	O
the	O
association	O
for	O
the	O
advancement	O
of	O
artiﬁcial	O
intelligence	O
(	O
aaai	O
)	O
.	O
daniel	O
,	O
j.	O
w.	O
(	O
1976	O
)	O
.	O
splines	O
and	O
eﬃciency	O
in	O
dynamic	O
programming	O
.	O
journal	O
of	O
mathematical	O
analysis	O
and	O
applications	O
,	O
54:402–407	O
.	O
dann	O
,	O
c.	O
,	O
neumann	O
,	O
g.	O
,	O
peters	O
,	O
j	O
.	O
(	O
2014	O
)	O
.	O
policy	B
evaluation	I
with	O
temporal	O
diﬀerences	O
:	O
a	O
survey	O
and	O
comparison	O
.	O
journal	O
of	O
machine	O
learning	O
research	O
,	O
15	O
:809–883	O
.	O
daw	O
,	O
n.	O
d.	O
,	O
courville	O
,	O
a.	O
c.	O
,	O
touretzky	O
,	O
d.	O
s.	O
(	O
2003	O
)	O
.	O
timing	O
and	O
partial	O
observability	O
in	O
the	O
dopamine	O
system	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
15	O
(	O
nips	O
2002	O
)	O
,	O
references	O
493	O
pp	O
.	O
99–106	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
daw	O
,	O
n.	O
d.	O
,	O
courville	O
,	O
a.	O
c.	O
,	O
touretzky	O
,	O
d.	O
s.	O
(	O
2006	O
)	O
.	O
representation	O
and	O
timing	O
in	O
theories	O
of	O
the	O
dopamine	B
system	O
.	O
neural	B
computation	O
,	O
18	O
(	O
7	O
)	O
:1637–1677	O
.	O
daw	O
,	O
n.	O
d.	O
,	O
niv	O
,	O
y.	O
,	O
dayan	O
,	O
p.	O
(	O
2005	O
)	O
.	O
uncertainty	O
based	O
competition	O
between	O
prefrontal	O
and	O
dorsolateral	O
striatal	O
systems	O
for	O
behavioral	O
control	B
.	O
nature	O
neuroscience	B
,	O
8	O
(	O
12	O
)	O
:1704–1711	O
.	O
daw	O
,	O
n.	O
d.	O
,	O
shohamy	O
,	O
d.	O
(	O
2008	O
)	O
.	O
the	O
cognitive	O
neuroscience	O
of	O
motivation	O
and	O
learning	O
.	O
social	O
cognition	O
,	O
26	O
(	O
5	O
)	O
:593–620	O
.	O
dayan	O
,	O
p.	O
(	O
1991	O
)	O
.	O
reinforcement	O
comparison	O
.	O
in	O
d.	O
s.	O
touretzky	O
,	O
j.	O
l.	O
elman	O
,	O
t.	O
j.	O
sejnowski	O
,	O
and	O
g.	O
e.	O
hinton	O
(	O
eds	O
.	O
)	O
,	O
connectionist	O
models	O
:	O
proceedings	O
of	O
the	O
1990	O
summer	O
school	O
,	O
pp	O
.	O
45–51	O
.	O
morgan	O
kaufmann	O
.	O
dayan	O
,	O
p.	O
(	O
1992	O
)	O
.	O
the	O
convergence	O
of	O
td	O
(	O
λ	O
)	O
for	O
general	O
λ.	O
machine	O
learning	O
,	O
8	O
(	O
3	O
)	O
:341–362	O
.	O
dayan	O
,	O
p.	O
(	O
2002	O
)	O
.	O
matters	O
temporal	O
.	O
trends	O
in	O
cognitive	O
sciences	O
,	O
6	O
(	O
3	O
)	O
:105–106	O
.	O
dayan	O
,	O
p.	O
,	O
abbott	O
,	O
l.	O
f.	O
(	O
2001	O
)	O
.	O
theoretical	O
neuroscience	B
:	O
computational	O
and	O
mathematical	O
modeling	O
of	O
neural	O
systems	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
dayan	O
,	O
p.	O
,	O
berridge	O
,	O
k.	O
c.	O
(	O
2014	O
)	O
.	O
model-based	O
and	O
model-free	O
pavlovian	O
reward	O
learning	O
:	O
revaluation	O
,	O
revision	O
,	O
and	O
revaluation	O
.	O
cognitive	O
,	O
aﬀective	O
,	O
&	O
behavioral	O
neuroscience	B
,	O
14	O
(	O
2	O
)	O
:473–492	O
.	O
dayan	O
,	O
p.	O
,	O
niv	O
,	O
y	O
.	O
(	O
2008	O
)	O
.	O
reinforcement	B
learning	I
:	O
the	O
good	O
,	O
the	O
bad	O
and	O
the	O
ugly	O
.	O
current	O
opinion	O
in	O
neurobiology	O
,	O
18	O
(	O
2	O
)	O
:185–196	O
.	O
dayan	O
,	O
p.	O
,	O
niv	O
,	O
y.	O
,	O
seymour	O
,	O
b.	O
,	O
daw	O
,	O
n.	O
d.	O
(	O
2006	O
)	O
.	O
the	O
misbehavior	O
of	O
value	O
and	O
the	O
discipline	O
of	O
the	O
will	O
.	O
neural	B
networks	I
,	O
19	O
(	O
8	O
)	O
:1153–1160	O
.	O
dayan	O
,	O
p.	O
,	O
sejnowski	O
,	O
t.	O
(	O
1994	O
)	O
.	O
td	O
(	O
λ	O
)	O
converges	O
with	O
probability	O
1.	O
machine	O
learning	O
,	O
14	O
(	O
3	O
)	O
:295–301	O
.	O
de	O
asis	O
,	O
k.	O
,	O
hernandez-garcia	O
,	O
j.	O
f.	O
,	O
holland	O
,	O
g.	O
z.	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
2017	O
)	O
.	O
multi-step	O
rein-	O
forcement	O
learning	O
:	O
a	O
unifying	O
algorithm	O
.	O
arxiv	O
preprint	O
arxiv:1703.01327.	O
dean	O
,	O
t.	O
,	O
lin	O
,	O
s.-h.	O
(	O
1995	O
)	O
.	O
decomposition	O
techniques	O
for	O
planning	O
in	O
stochastic	O
domains	O
.	O
in	O
proceedings	O
of	O
the	O
fourteenth	O
international	O
joint	O
conference	O
on	O
artiﬁcial	B
intelligence	I
(	O
ijcai-95	O
)	O
,	O
pp	O
.	O
1121–1127	O
.	O
morgan	O
kaufmann	O
.	O
see	O
also	O
technical	O
report	O
cs-95-10	O
,	O
brown	O
university	O
,	O
department	O
of	O
computer	O
science	O
,	O
1995.	O
degris	O
,	O
t.	O
,	O
white	O
,	O
m.	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
2012	O
)	O
.	O
oﬀ-policy	B
actor–critic	O
.	O
in	O
proceedings	O
of	O
the	O
29th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
2012	O
)	O
.	O
arxiv	O
preprint	O
arxiv:1205.4839	O
,	O
2012.	O
denardo	O
,	O
e.	O
v.	O
(	O
1967	O
)	O
.	O
contraction	O
mappings	O
in	O
the	O
theory	O
underlying	O
dynamic	B
programming	I
.	O
siam	O
review	O
,	O
9	O
(	O
2	O
)	O
:165–177	O
.	O
dennett	O
,	O
d.	O
c.	O
(	O
1978	O
)	O
.	O
why	O
the	O
law	O
of	O
eﬀect	O
will	O
not	O
go	O
away	O
.	O
brainstorms	O
,	O
pp	O
.	O
71–89	O
.	O
bradford/mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
derthick	O
,	O
m.	O
(	O
1984	O
)	O
.	O
variations	O
on	O
the	O
boltzmann	O
machine	O
learning	O
algorithm	O
.	O
carnegie-mellon	O
university	O
department	O
of	O
computer	O
science	O
technical	O
report	O
no	O
.	O
cmu-cs-84-120	O
.	O
deutsch	O
,	O
j.	O
a	O
.	O
(	O
1953	O
)	O
.	O
a	O
new	O
type	O
of	O
behaviour	O
theory	O
.	O
british	O
journal	O
of	O
psychology	O
.	O
general	O
section	O
,	O
44	O
(	O
4	O
)	O
:304–317	O
.	O
deutsch	O
,	O
j.	O
a	O
.	O
(	O
1954	O
)	O
.	O
a	O
machine	O
with	O
insight	O
.	O
quarterly	O
journal	O
of	O
experimental	O
psychology	B
,	O
6	O
(	O
1	O
)	O
:6–11	O
.	O
dick	O
,	O
t.	O
(	O
2015	O
)	O
.	O
policy	O
gradient	O
reinforcement	O
learning	O
without	O
regret	O
.	O
m.sc	O
.	O
thesis	O
,	O
uni-	O
versity	O
of	O
alberta	O
,	O
edmonton	O
.	O
dickinson	O
,	O
a	O
.	O
(	O
1980	O
)	O
.	O
contemporary	O
animal	O
learning	O
theory	O
.	O
cambridge	O
university	O
press	O
,	O
cambridge	O
.	O
dickinson	O
,	O
a	O
.	O
(	O
1985	O
)	O
.	O
actions	O
and	O
habits	O
:	O
the	O
development	O
of	O
behavioral	O
autonomy	O
.	O
phil	O
.	O
494	O
references	O
trans	O
.	O
r.	O
soc	O
.	O
lond	O
.	O
b	O
,	O
308	O
(	O
1135	O
)	O
:67–78	O
.	O
dickinson	O
,	O
a.	O
,	O
balleine	O
,	O
b.	O
w.	O
(	O
2002	O
)	O
.	O
the	O
role	O
of	O
learning	O
in	O
motivation	O
.	O
in	O
c.	O
r.	O
gallistel	O
(	O
ed	O
.	O
)	O
,	O
stevens	O
handbook	O
of	O
experimental	O
psychology	B
,	O
volume	O
3	O
,	O
pp	O
.	O
497–533	O
.	O
wiley	O
,	O
ny	O
.	O
dietterich	O
,	O
t.	O
g.	O
,	O
buchanan	O
,	O
b.	O
g.	O
(	O
1984	O
)	O
.	O
the	O
role	O
of	O
the	O
critic	B
in	O
learning	O
systems	O
.	O
in	O
o.	O
g.	O
selfridge	O
,	O
e.	O
l.	O
rissland	O
,	O
and	O
m.	O
a.	O
arbib	O
(	O
eds	O
.	O
)	O
,	O
adaptive	O
control	B
of	O
ill-deﬁned	O
systems	O
,	O
pp	O
.	O
127–147	O
.	O
plenum	O
press	O
,	O
ny	O
.	O
proceedings	O
of	O
the	O
nato	O
advanced	O
research	O
institute	O
on	O
adaptive	O
control	B
of	O
ill-deﬁned	O
systems	O
,	O
nato	O
conference	O
series	O
ii	O
,	O
systems	O
science	O
,	O
vol	O
.	O
16.	O
dietterich	O
,	O
t.	O
g.	O
,	O
flann	O
,	O
n.	O
s.	O
(	O
1995	O
)	O
.	O
explanation-based	O
learning	O
and	O
reinforcement	B
learning	I
:	O
a	O
uniﬁed	O
view	O
.	O
in	O
a.	O
prieditis	O
and	O
s.	O
russell	O
(	O
eds	O
.	O
)	O
,	O
proceedings	O
of	O
the	O
126h	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
1995	O
)	O
,	O
pp	O
.	O
176–184	O
.	O
morgan	O
kaufmann	O
.	O
dietterich	O
,	O
t.	O
g.	O
,	O
wang	O
,	O
x	O
.	O
(	O
2002	O
)	O
.	O
batch	O
value	B
function	I
approximation	O
via	O
support	O
vectors	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
14	O
(	O
nips	O
2001	O
)	O
,	O
pp	O
.	O
1491–1498	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
diuk	O
,	O
c.	O
,	O
cohen	O
,	O
a.	O
,	O
littman	O
,	O
m.	O
l.	O
(	O
2008	O
)	O
.	O
an	O
object-oriented	O
representation	O
for	O
eﬃcient	O
in	O
proceedings	O
of	O
the	O
25th	O
international	O
conference	O
on	O
machine	O
reinforcement	B
learning	I
.	O
learning	O
(	O
icml	O
2008	O
)	O
,	O
pp	O
.	O
240–247	O
.	O
acm	O
,	O
new	O
york	O
.	O
dolan	O
,	O
r.	O
j.	O
,	O
dayan	O
,	O
p.	O
(	O
2013	O
)	O
.	O
goals	O
and	O
habits	O
in	O
the	O
brain	O
.	O
neuron	O
,	O
80	O
(	O
2	O
)	O
:312–325	O
.	O
doll	O
,	O
b.	O
b.	O
,	O
simon	O
,	O
d.	O
a.	O
,	O
daw	O
,	O
n.	O
d.	O
(	O
2012	O
)	O
.	O
the	O
ubiquity	O
of	O
model-based	O
reinforcement	B
learning	I
.	O
current	O
opinion	O
in	O
neurobiology	O
,	O
22	O
(	O
6	O
)	O
:1–7	O
.	O
donahoe	O
,	O
j.	O
w.	O
,	O
burgos	O
,	O
j.	O
e.	O
(	O
2000	O
)	O
.	O
behavior	O
analysis	O
and	O
revaluation	O
.	O
journal	O
of	O
the	O
experimental	O
analysis	O
of	O
behavior	O
,	O
74	O
(	O
3	O
)	O
:331–346	O
.	O
dorigo	O
,	O
m.	O
,	O
colombetti	O
,	O
m.	O
(	O
1994	O
)	O
.	O
robot	O
shaping	B
:	O
developing	O
autonomous	O
agents	O
through	O
learning	O
.	O
artiﬁcial	B
intelligence	I
,	O
71	O
(	O
2	O
)	O
:321–370	O
.	O
doya	O
,	O
k.	O
(	O
1996	O
)	O
.	O
temporal	O
diﬀerence	O
learning	O
in	O
continuous	B
time	I
and	O
space	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
8	O
(	O
nips	O
1995	O
)	O
,	O
pp	O
.	O
1073–1079	O
.	O
mit	O
press	O
,	O
cam-	O
bridge	O
,	O
ma	O
.	O
doya	O
,	O
k.	O
,	O
sejnowski	O
,	O
t.	O
j	O
.	O
(	O
1995	O
)	O
.	O
a	O
novel	O
reinforcement	O
model	O
of	O
birdsong	O
vocalization	O
learning	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
7	O
(	O
nips	O
1994	O
)	O
,	O
pp	O
.	O
101–	O
108.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
doya	O
,	O
k.	O
,	O
sejnowski	O
,	O
t.	O
j	O
.	O
(	O
1998	O
)	O
.	O
a	O
computational	O
model	O
of	O
birdsong	O
learning	O
by	O
auditory	O
in	O
p.	O
w.	O
f.	O
poon	O
and	O
j.	O
f.	O
brugge	O
(	O
eds	O
.	O
)	O
,	O
central	O
experience	O
and	O
auditory	O
feedback	O
.	O
auditory	O
processing	O
and	O
neural	O
modeling	O
,	O
pp	O
.	O
77–88	O
.	O
springer	O
,	O
boston	O
,	O
ma	O
.	O
doyle	O
,	O
p.	O
g.	O
,	O
snell	O
,	O
j.	O
l.	O
(	O
1984	O
)	O
.	O
random	O
walks	O
and	O
electric	O
networks	O
.	O
the	O
mathematical	O
association	O
of	O
america	O
.	O
carus	O
mathematical	O
monograph	O
22.	O
dreyfus	O
,	O
s.	O
e.	O
,	O
law	O
,	O
a.	O
m.	O
(	O
1977	O
)	O
.	O
the	O
art	O
and	O
theory	O
of	O
dynamic	O
programming	O
.	O
academic	O
press	O
,	O
new	O
york	O
.	O
duda	O
,	O
r.	O
o.	O
,	O
hart	O
,	O
p.	O
e.	O
(	O
1973	O
)	O
.	O
pattern	O
classiﬁcation	O
and	O
scene	O
analysis	O
.	O
wiley	O
,	O
new	O
york	O
.	O
duﬀ	O
,	O
m.	O
o	O
.	O
(	O
1995	O
)	O
.	O
q-learning	O
for	B
bandit	I
problems	I
.	O
in	O
proceedings	O
of	O
the	O
12th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
1995	O
)	O
,	O
pp	O
.	O
209–217	O
.	O
morgan	O
kaufmann	O
.	O
egger	O
,	O
d.	O
m.	O
,	O
miller	O
,	O
n.	O
e.	O
(	O
1962	O
)	O
.	O
secondary	B
reinforcement	I
in	O
rats	O
as	O
a	O
function	O
of	O
information	O
value	B
and	O
reliability	O
of	O
the	O
stimulus	O
.	O
journal	O
of	O
experimental	O
psychology	B
,	O
64:97–104	O
.	O
eshel	O
,	O
n.	O
,	O
tian	O
,	O
j.	O
,	O
bukwich	O
,	O
m.	O
,	O
uchida	O
,	O
n.	O
(	O
2016	O
)	O
.	O
dopamine	B
neurons	O
share	O
common	O
response	O
function	O
for	O
reward	O
prediction	O
error	O
.	O
nature	O
neuroscience	B
,	O
19	O
(	O
3	O
)	O
:479–486	O
.	O
estes	O
,	O
w.	O
k.	O
(	O
1943	O
)	O
.	O
discriminative	O
conditioning	B
.	O
i.	O
a	O
discriminative	O
property	O
of	O
conditioned	O
anticipation	O
.	O
journal	O
of	O
experimental	O
psychology	B
,	O
32	O
(	O
2	O
)	O
:150–155	O
.	O
estes	O
,	O
w.	O
k.	O
(	O
1948	O
)	O
.	O
discriminative	O
conditioning	B
.	O
ii	O
.	O
eﬀects	O
of	O
a	O
pavlovian	O
conditioned	O
stimulus	O
references	O
495	O
upon	O
a	O
subsequently	O
established	O
operant	O
response	O
.	O
journal	O
of	O
experimental	O
psychology	B
,	O
38	O
(	O
2	O
)	O
:173–177	O
.	O
estes	O
,	O
w.	O
k.	O
(	O
1950	O
)	O
.	O
toward	O
a	O
statistical	O
theory	O
of	O
learning	O
.	O
psychololgical	O
review	O
,	O
57	O
(	O
2	O
)	O
:94–	O
107.	O
farley	O
,	O
b.	O
g.	O
,	O
clark	O
,	O
w.	O
a	O
.	O
(	O
1954	O
)	O
.	O
simulation	O
of	O
self-organizing	O
systems	O
by	O
digital	O
computer	O
.	O
ire	O
transactions	O
on	O
information	O
theory	O
,	O
4	O
(	O
4	O
)	O
:76–84	O
.	O
farries	O
,	O
m.	O
a.	O
,	O
fairhall	O
,	O
a.	O
l.	O
(	O
2007	O
)	O
.	O
reinforcement	B
learning	I
with	O
modulated	O
spike	O
timingde-	O
pendent	O
synaptic	B
plasticity	I
.	O
journal	O
of	O
neurophysiology	O
,	O
98	O
(	O
6	O
)	O
:3648–3665	O
.	O
feldbaum	O
,	O
a.	O
a	O
.	O
(	O
1965	O
)	O
.	O
optimal	B
control	I
systems	O
.	O
academic	O
press	O
,	O
new	O
york	O
.	O
finch	O
,	O
g.	O
,	O
culler	O
,	O
e.	O
(	O
1934	O
)	O
.	O
higher	O
order	O
conditioning	B
with	O
constant	O
motivation	B
.	O
the	O
american	O
journal	O
of	O
psychology:596–602	O
.	O
finnsson	O
,	O
h.	O
,	O
bj¨ornsson	O
,	O
y	O
.	O
(	O
2008	O
)	O
.	O
simulation-based	O
approach	O
to	O
general	O
game	O
playing	O
.	O
in	O
proceedings	O
of	O
the	O
association	O
for	O
the	O
advancement	O
of	O
artiﬁcial	O
intelligence	O
,	O
pp	O
.	O
259–264	O
.	O
fiorillo	O
,	O
c.	O
d.	O
,	O
yun	O
,	O
s.	O
r.	O
,	O
song	O
,	O
m.	O
r.	O
(	O
2013	O
)	O
.	O
diversity	O
and	O
homogeneity	O
in	O
responses	O
of	O
midbrain	O
dopamine	B
neurons	O
.	O
the	O
journal	O
of	O
neuroscience	O
,	O
33	O
(	O
11	O
)	O
:4693–4709	O
.	O
florian	O
,	O
r.	O
v.	O
(	O
2007	O
)	O
.	O
reinforcement	B
learning	I
through	O
modulation	O
of	O
spike-timing-dependent	O
synaptic	B
plasticity	I
.	O
neural	B
computation	O
,	O
19	O
(	O
6	O
)	O
:1468–1502	O
.	O
fogel	O
,	O
l.	O
j.	O
,	O
owens	O
,	O
a.	O
j.	O
,	O
walsh	O
,	O
m.	O
j	O
.	O
(	O
1966	O
)	O
.	O
artiﬁcial	B
intelligence	I
through	O
simulated	O
evo-	O
lution	O
.	O
john	O
wiley	O
and	O
sons	O
.	O
french	O
,	O
r.	O
m.	O
(	O
1999	O
)	O
.	O
catastrophic	O
forgetting	O
in	O
connectionist	O
networks	O
.	O
trends	O
in	O
cognitive	O
sciences	O
,	O
3	O
(	O
4	O
)	O
:128–135	O
.	O
frey	O
,	O
u.	O
,	O
morris	O
,	O
r.	O
g.	O
m.	O
(	O
1997	O
)	O
.	O
synaptic	O
tagging	O
and	O
long-term	O
potentiation	O
.	O
nature	O
,	O
385	O
(	O
6616	O
)	O
:533–536	O
.	O
fr´emaux	O
,	O
n.	O
,	O
sprekeler	O
,	O
h.	O
,	O
gerstner	O
,	O
w.	O
(	O
2010	O
)	O
.	O
functional	O
requirements	O
for	O
reward-modulated	O
spike-timing-dependent	O
plasticity	O
.	O
the	O
journal	O
of	O
neuroscience	O
,	O
30	O
(	O
40	O
)	O
:	O
13326–13337	O
friedman	O
,	O
j.	O
h.	O
,	O
bentley	O
,	O
j.	O
l.	O
,	O
finkel	O
,	O
r.	O
a	O
.	O
(	O
1977	O
)	O
.	O
an	O
algorithm	O
for	O
ﬁnding	O
best	O
matches	O
in	O
logarithmic	O
expected	O
time	O
.	O
acm	O
transactions	O
on	O
mathematical	O
software	O
,	O
3	O
(	O
3	O
)	O
:209–226	O
.	O
friston	O
,	O
k.	O
j.	O
,	O
tononi	O
,	O
g.	O
,	O
reeke	O
,	O
g.	O
n.	O
,	O
sporns	O
,	O
o.	O
,	O
edelman	O
,	O
g.	O
m.	O
(	O
1994	O
)	O
.	O
value-dependent	O
selection	O
in	O
the	O
brain	O
:	O
simulation	O
in	O
a	O
synthetic	O
neural	B
model	O
.	O
neuroscience	B
,	O
59	O
(	O
2	O
)	O
:229–243	O
.	O
ieee	O
transactions	O
on	O
fu	O
,	O
k.	O
s.	O
(	O
1970	O
)	O
.	O
learning	O
control	O
systems—review	O
and	O
outlook	O
.	O
automatic	O
control	O
,	O
15	O
(	O
2	O
)	O
:210–221	O
.	O
galanter	O
,	O
e.	O
,	O
gerstenhaber	O
,	O
m.	O
(	O
1956	O
)	O
.	O
on	O
thought	O
:	O
the	O
extrinsic	O
theory	O
.	O
psychological	O
review	O
,	O
63	O
(	O
4	O
)	O
:218–227	O
.	O
gallistel	O
,	O
c.	O
r.	O
(	O
2005	O
)	O
.	O
deconstructing	O
the	O
law	O
of	O
eﬀect	O
.	O
games	O
and	O
economic	O
behavior	O
,	O
52	O
(	O
2	O
)	O
:410–423	O
.	O
gardner	O
,	O
m.	O
(	O
1973	O
)	O
.	O
mathematical	O
games	O
.	O
scientiﬁc	O
american	O
,	O
228	O
(	O
1	O
)	O
:108–115	O
.	O
geist	O
,	O
m.	O
,	O
scherrer	O
,	O
b	O
.	O
(	O
2014	O
)	O
.	O
oﬀ-policy	B
learning	O
with	B
eligibility	I
traces	I
:	O
a	O
survey	O
.	O
journal	O
of	O
machine	O
learning	O
research	O
,	O
15	O
(	O
1	O
)	O
:289–333	O
.	O
gelly	O
,	O
s.	O
,	O
silver	O
,	O
d.	O
(	O
2007	O
)	O
.	O
combining	O
online	B
and	O
oﬄine	O
knowledge	O
in	O
uct	O
.	O
proceedings	O
of	O
the	O
24th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
2007	O
)	O
,	O
pp	O
.	O
273–280	O
.	O
gelperin	O
,	O
a.	O
,	O
hopﬁeld	O
,	O
j.	O
j.	O
,	O
tank	O
,	O
d.	O
w.	O
(	O
1985	O
)	O
.	O
the	O
logic	O
of	O
limax	O
learning	O
.	O
in	O
a.	O
selverston	O
(	O
ed	O
.	O
)	O
,	O
model	O
neural	O
networks	O
and	O
behavior	O
,	O
pp	O
.	O
247–261	O
.	O
plenum	O
press	O
,	O
new	O
york	O
.	O
genesereth	O
,	O
m.	O
,	O
thielscher	O
,	O
m.	O
(	O
2014	O
)	O
.	O
general	O
game	O
playing	O
.	O
synthesis	O
lectures	O
on	O
artiﬁcial	B
intelligence	I
and	O
machine	O
learning	O
,	O
8	O
(	O
2	O
)	O
:1–229	O
.	O
gershman	O
,	O
s.	O
j.	O
,	O
moustafa	O
,	O
a.	O
a.	O
,	O
ludvig	O
,	O
e.	O
a	O
.	O
(	O
2014	O
)	O
.	O
time	O
representation	O
in	O
reinforcement	B
learning	I
models	O
of	O
the	O
basal	B
ganglia	I
.	O
frontiers	O
in	O
computational	O
neuroscience	B
,	O
7:194.	O
doi=10.3389/fncom.2013.00194	O
.	O
496	O
references	O
gershman	O
,	O
s.	O
j.	O
,	O
pesaran	O
,	O
b.	O
,	O
daw	O
,	O
n.	O
d.	O
(	O
2009	O
)	O
.	O
human	O
reinforcement	B
learning	I
subdivides	O
structured	O
action	B
spaces	O
by	O
learning	O
eﬀector-speciﬁc	O
values	O
.	O
the	O
journal	O
of	O
neuroscience	O
,	O
29	O
(	O
43	O
)	O
:13524–13531	O
.	O
ghiassian	O
,	O
s.	O
,	O
raﬁee	O
,	O
b.	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
2016	O
)	O
.	O
a	O
ﬁrst	O
empirical	O
study	O
of	O
emphatic	O
temporal	O
diﬀerence	O
learning	O
.	O
workshop	O
on	O
continual	O
learning	O
and	O
deep	B
learning	I
at	O
the	O
conference	O
on	O
neural	B
information	O
processing	O
systems	O
(	O
nips	O
2016	O
)	O
.	O
arxiv:1705.04185.	O
gibbs	O
,	O
c.	O
m.	O
,	O
cool	O
,	O
v.	O
,	O
land	O
,	O
t.	O
,	O
kehoe	O
,	O
e.	O
j.	O
,	O
gormezano	O
,	O
i	O
.	O
(	O
1991	O
)	O
.	O
second-order	O
condition-	O
ing	B
of	O
the	O
rabbits	O
nictitating	O
membrane	O
response	O
.	O
integrative	O
physiological	O
and	O
behavioral	O
science	O
,	O
26	O
(	O
4	O
)	O
:282–295	O
.	O
gittins	O
,	O
j.	O
c.	O
,	O
jones	O
,	O
d.	O
m.	O
(	O
1974	O
)	O
.	O
a	O
dynamic	O
allocation	O
index	O
for	O
the	O
sequential	O
design	B
of	I
experiments	O
.	O
in	O
j.	O
gani	O
,	O
k.	O
sarkadi	O
,	O
and	O
i.	O
vincze	O
(	O
eds	O
.	O
)	O
,	O
progress	O
in	O
statistics	O
,	O
pp	O
.	O
241–266	O
.	O
north-holland	O
,	O
amsterdam–london	O
.	O
glimcher	O
,	O
p.	O
w.	O
(	O
2011	O
)	O
.	O
understanding	O
dopamine	B
and	O
reinforcement	B
learning	I
:	O
the	O
dopamine	B
proceedings	O
of	O
the	O
national	O
academy	O
of	O
sciences	O
,	O
reward	B
prediction	I
error	I
hypothesis	I
.	O
108	O
(	O
supplement	O
3	O
)	O
:15647–15654	O
.	O
glimcher	O
,	O
p.	O
w.	O
(	O
2003	O
)	O
.	O
decisions	O
,	O
uncertainty	O
,	O
and	O
the	O
brain	O
:	O
the	O
science	O
of	O
neuroeconomics	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
glimcher	O
,	O
p.	O
w.	O
,	O
fehr	O
,	O
e	O
.	O
(	O
eds	O
.	O
)	O
(	O
2013	O
)	O
.	O
neuroeconomics	B
:	O
decision	O
making	O
and	O
the	O
brain	O
,	O
second	O
edition	O
.	O
academic	O
press	O
.	O
goethe	O
,	O
j.	O
w.	O
v.	O
(	O
1878	O
)	O
.	O
the	O
sorcerers	O
apprentice	O
.	O
in	O
the	O
permanent	O
goethe	O
,	O
p.	O
349.	O
the	O
dial	O
press	O
,	O
inc.	O
,	O
new	O
york	O
.	O
goldstein	O
,	O
h.	O
(	O
1957	O
)	O
.	O
classical	O
mechanics	O
.	O
addison-wesley	O
,	O
reading	O
,	O
ma	O
.	O
goodfellow	O
,	O
i.	O
,	O
bengio	O
,	O
y.	O
,	O
courville	O
,	O
a	O
.	O
(	O
2016	O
)	O
.	O
deep	B
learning	I
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
goodwin	O
,	O
g.	O
c.	O
,	O
sin	O
,	O
k.	O
s.	O
(	O
1984	O
)	O
.	O
adaptive	O
filtering	O
prediction	B
and	O
control	B
.	O
prentice-hall	O
,	O
englewood	O
cliﬀs	O
,	O
nj	O
.	O
gopnik	O
,	O
a.	O
,	O
glymour	O
,	O
c.	O
,	O
sobel	O
,	O
d.	O
,	O
schulz	O
,	O
l.	O
e.	O
,	O
kushnir	O
,	O
t.	O
,	O
danks	O
,	O
d.	O
(	O
2004	O
)	O
.	O
a	O
theory	O
of	O
causal	O
learning	O
in	O
children	O
:	O
causal	O
maps	O
and	O
bayes	O
nets	O
.	O
psychological	O
review	O
,	O
111	O
(	O
1	O
)	O
:3–32	O
.	O
gordon	O
,	O
g.	O
j	O
.	O
(	O
1995	O
)	O
.	O
stable	O
function	B
approximation	I
in	O
dynamic	B
programming	I
.	O
in	O
a.	O
prieditis	O
and	O
s.	O
russell	O
(	O
eds	O
.	O
)	O
,	O
proceedings	O
of	O
the	O
12th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
1995	O
)	O
,	O
pp	O
.	O
261–268	O
.	O
morgan	O
kaufmann	O
.	O
an	O
expanded	O
version	O
was	O
published	O
as	O
technical	O
report	O
cmu-cs-95-103	O
.	O
carnegie	O
mellon	O
university	O
,	O
pittsburgh	O
,	O
pa	O
,	O
1995.	O
gordon	O
,	O
g.	O
j	O
.	O
(	O
1996a	O
)	O
.	O
chattering	O
in	O
sarsa	O
(	O
λ	O
)	O
.	O
cmu	O
learning	O
lab	O
internal	O
report	O
.	O
gordon	O
,	O
g.	O
j	O
.	O
(	O
1996b	O
)	O
.	O
stable	O
ﬁtted	O
reinforcement	B
learning	I
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
8	O
(	O
nips	O
1995	O
)	O
,	O
pp	O
.	O
1052–1058	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
gordon	O
,	O
g.	O
j	O
.	O
(	O
1999	O
)	O
.	O
approximate	B
solutions	O
to	O
markov	O
decision	O
processes	O
.	O
ph.d.	O
thesis	O
,	O
carnegie	O
mellon	O
university	O
,	O
pittsburgh	O
pa.	O
pittsburgh	O
,	O
pa.	O
gordon	O
,	O
g.	O
j	O
.	O
(	O
2001	O
)	O
.	O
reinforcement	B
learning	I
with	O
function	B
approximation	I
converges	O
to	O
a	O
region	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
13	O
(	O
nips	O
2000	O
)	O
,	O
pp	O
.	O
1040–	O
1046.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
graybiel	O
,	O
a.	O
m.	O
(	O
2000	O
)	O
.	O
the	O
basal	B
ganglia	I
.	O
current	O
biology	O
,	O
10	O
(	O
14	O
)	O
:	O
r509–r511	O
.	O
greensmith	O
,	O
e.	O
,	O
bartlett	O
,	O
p.	O
l.	O
,	O
baxter	O
,	O
j	O
.	O
(	O
2002	O
)	O
.	O
variance	O
reduction	O
techniques	O
for	O
gradient	O
estimates	O
in	O
reinforcement	O
learning	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
14	O
(	O
nips	O
2001	O
)	O
,	O
pp	O
.	O
1507–1514	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
greensmith	O
,	O
e.	O
,	O
bartlett	O
,	O
p.	O
l.	O
,	O
baxter	O
,	O
j	O
.	O
(	O
2004	O
)	O
.	O
variance	O
reduction	O
techniques	O
for	O
gradient	O
estimates	O
in	O
reinforcement	O
learning	O
.	O
journal	O
of	O
machine	O
learning	O
research	O
,	O
5	O
(	O
nov	O
)	O
:1471–	O
1530.	O
griﬃth	O
,	O
a.	O
k.	O
(	O
1966	O
)	O
.	O
a	O
new	O
machine	O
learning	O
technique	O
applied	O
to	O
the	O
game	O
of	O
checkers	O
.	O
references	O
497	O
technical	O
report	O
project	O
mac	O
,	O
artiﬁcial	B
intelligence	I
memo	O
94.	O
massachusetts	O
institute	O
of	O
technology	O
,	O
cambridge	O
,	O
ma	O
.	O
griﬃth	O
,	O
a.	O
k.	O
(	O
1974	O
)	O
.	O
a	O
comparison	O
and	O
evaluation	O
of	O
three	O
machine	O
learning	O
procedures	O
as	O
applied	O
to	O
the	O
game	O
of	O
checkers	O
.	O
artiﬁcial	B
intelligence	I
,	O
5	O
(	O
2	O
)	O
:137–148	O
.	O
grondman	O
,	O
i.	O
,	O
busoniu	O
,	O
l.	O
,	O
lopes	O
,	O
g.	O
a.	O
,	O
babuska	O
,	O
r.	O
(	O
2012	O
)	O
.	O
a	O
survey	O
of	O
actor–critic	O
rein-	O
forcement	O
learning	O
:	O
standard	O
and	O
natural	O
policy	B
gradients	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
part	O
c	O
(	O
applications	O
and	O
reviews	O
)	O
,	O
42	O
(	O
6	O
)	O
:1291–1307	O
.	O
grossberg	O
,	O
s.	O
(	O
1975	O
)	O
.	O
a	O
neural	B
model	O
of	O
attention	O
,	O
reinforcement	O
,	O
and	O
discrimination	O
learning	O
.	O
international	O
review	O
of	O
neurobiology	O
,	O
18:263–327	O
.	O
grossberg	O
,	O
s.	O
,	O
schmajuk	O
,	O
n.	O
a	O
.	O
(	O
1989	O
)	O
.	O
neural	B
dynamics	O
of	O
adaptive	O
timing	O
and	O
temporal	O
discrimination	O
during	O
associative	O
learning	O
.	O
neural	B
networks	I
,	O
2	O
(	O
2	O
)	O
:79–102	O
.	O
gullapalli	O
,	O
v.	O
(	O
1990	O
)	O
.	O
a	O
stochastic	O
reinforcement	O
algorithm	O
for	O
learning	O
real-valued	O
functions	O
.	O
neural	B
networks	I
,	O
3	O
(	O
6	O
)	O
:	O
671–692	O
.	O
gullapalli	O
,	O
v.	O
,	O
barto	O
,	O
a.	O
g.	O
(	O
1992	O
)	O
.	O
shaping	B
as	O
a	O
method	O
for	O
accelerating	O
reinforcement	B
learning	I
.	O
in	O
proceedings	O
of	O
the	O
1992	O
ieee	O
international	O
symposium	O
on	O
intelligent	O
control	B
,	O
pp	O
.	O
554–	O
559.	O
ieee	O
.	O
gurvits	O
,	O
l.	O
,	O
lin	O
,	O
l.-j.	O
,	O
hanson	O
,	O
s.	O
j	O
.	O
(	O
1994	O
)	O
.	O
incremental	O
learning	O
of	O
evaluation	O
functions	O
for	O
absorbing	O
markov	O
chains	O
:	O
new	O
methods	O
and	O
theorems	O
.	O
siemans	O
corporate	O
research	O
,	O
princeton	O
,	O
nj	O
.	O
hackman	O
,	O
l.	O
(	O
2012	O
)	O
.	O
faster	O
gradient-td	O
algorithms	O
.	O
m.sc	O
.	O
thesis	O
,	O
university	O
of	O
alberta	O
,	O
edmonton	O
.	O
hallak	O
,	O
a.	O
,	O
tamar	O
,	O
a.	O
,	O
mannor	O
,	O
s.	O
(	O
2015	O
)	O
.	O
emphatic	O
td	O
bellman	O
operator	O
is	O
a	O
contraction	O
.	O
arxiv:1508.03411.	O
hallak	O
,	O
a.	O
,	O
tamar	O
,	O
a.	O
,	O
munos	O
,	O
r.	O
,	O
mannor	O
,	O
s.	O
(	O
2016	O
)	O
.	O
generalized	O
emphatic	O
temporal	O
diﬀer-	O
ence	O
learning	O
:	O
bias-variance	O
analysis	O
.	O
in	O
proceedings	O
of	O
the	O
thirtieth	O
aaai	O
conference	O
on	O
artiﬁcial	B
intelligence	I
(	O
aaai-16	O
)	O
,	O
pp	O
.	O
1631–1637	O
.	O
aaai	O
press	O
,	O
menlo	O
park	O
,	O
ca	O
.	O
hammer	O
,	O
m.	O
(	O
1997	O
)	O
.	O
the	O
neural	B
basis	O
of	O
associative	O
reward	O
learning	O
in	O
honeybees	O
.	O
trends	O
in	B
neuroscience	I
,	O
20	O
(	O
6	O
)	O
:245–252	O
.	O
hammer	O
,	O
m.	O
,	O
menzel	O
,	O
r.	O
(	O
1995	O
)	O
.	O
learning	O
and	O
memory	O
in	O
the	O
honeybee	O
.	O
the	O
journal	O
of	O
neuroscience	O
,	O
15	O
(	O
3	O
)	O
:1617–1630	O
.	O
hampson	O
,	O
s.	O
e.	O
(	O
1983	O
)	O
.	O
a	O
neural	B
model	O
of	O
adaptive	O
behavior	O
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
california	O
,	O
irvine	O
.	O
hampson	O
,	O
s.	O
e.	O
(	O
1989	O
)	O
.	O
connectionist	O
problem	O
solving	O
:	O
computational	O
aspects	O
of	O
biological	O
learning	O
.	O
birkhauser	O
,	O
boston	O
.	O
hare	O
,	O
t.	O
a.	O
,	O
o	O
’	O
doherty	O
,	O
j.	O
,	O
camerer	O
,	O
c.	O
f.	O
,	O
schultz	O
,	O
w.	O
,	O
rangel	O
,	O
a	O
.	O
(	O
2008	O
)	O
.	O
dissociating	O
the	O
role	O
of	O
the	O
orbitofrontal	O
cortex	O
and	O
the	O
striatum	O
in	O
the	O
computation	O
of	O
goal	O
values	O
and	O
prediction	O
errors	O
.	O
the	O
journal	O
of	O
neuroscience	O
,	O
28	O
(	O
22	O
)	O
:5623–5630	O
.	O
harth	O
,	O
e.	O
,	O
tzanakou	O
,	O
e.	O
(	O
1974	O
)	O
.	O
alopex	O
:	O
a	O
stochastic	O
method	O
for	O
determining	O
visual	O
receptive	O
ﬁelds	O
.	O
vision	O
research	O
,	O
14	O
(	O
12	O
)	O
:1475–1482	O
.	O
hassabis	O
,	O
d.	O
,	O
maguire	O
,	O
e.	O
a	O
.	O
(	O
2007	O
)	O
.	O
deconstructing	O
episodic	O
memory	O
with	O
construction	O
.	O
trends	O
in	O
cognitive	O
sciences	O
,	O
11	O
(	O
7	O
)	O
:299–306	O
.	O
hauskrecht	O
,	O
m.	O
,	O
meuleau	O
,	O
n.	O
,	O
kaelbling	O
,	O
l.	O
p.	O
,	O
dean	O
,	O
t.	O
,	O
boutilier	O
,	O
c.	O
(	O
1998	O
)	O
.	O
hierarchical	O
solution	O
of	O
markov	O
decision	O
processes	O
using	O
macro-actions	O
.	O
in	O
proceedings	O
of	O
the	O
fourteenth	O
conference	O
on	O
uncertainty	O
in	O
artiﬁcial	O
intelligence	O
,	O
pp	O
.	O
220–229	O
.	O
morgan	O
kaufmann	O
.	O
hawkins	O
,	O
r.	O
d.	O
,	O
kandel	O
,	O
e.	O
r.	O
(	O
1984	O
)	O
.	O
is	O
there	O
a	O
cell-biological	O
alphabet	O
for	O
simple	O
forms	O
of	O
learning	O
?	O
psychological	O
review	O
,	O
91	O
(	O
3	O
)	O
:375–391	O
.	O
haykin	O
,	O
s.	O
(	O
1994	O
)	O
.	O
neural	B
networks	I
:	O
a	O
comprehensive	O
foundation	O
,	O
macmillan	O
college	O
publish-	O
498	O
references	O
ing	B
company	O
,	O
new	O
york	O
.	O
he	O
,	O
k.	O
,	O
huertas	O
,	O
m.	O
,	O
hong	O
,	O
s.	O
z.	O
,	O
tie	O
,	O
x.	O
,	O
hell	O
,	O
j.	O
w.	O
,	O
shouval	O
,	O
h.	O
,	O
kirkwood	O
,	O
a	O
.	O
(	O
2015	O
)	O
.	O
distinct	O
eligibility	B
traces	I
for	O
ltp	O
and	O
ltd	O
in	O
cortical	O
synapses	O
.	O
neuron	O
,	O
88	O
(	O
3	O
)	O
:528–538	O
.	O
he	O
,	O
k.	O
,	O
zhang	O
,	O
x.	O
,	O
ren	O
,	O
s.	O
,	O
sun	O
,	O
j	O
.	O
(	O
2016	O
)	O
.	O
deep	B
residual	I
learning	I
for	O
image	O
recognition	O
.	O
in	O
proceedings	O
of	O
the	O
1992	O
ieee	O
conference	O
on	O
computer	O
vision	O
and	O
pattern	O
recognition	O
,	O
pp	O
.	O
770–778	O
.	O
hebb	O
,	O
d.	O
o	O
.	O
(	O
1949	O
)	O
.	O
the	O
organization	O
of	O
behavior	O
:	O
a	O
neuropsychological	O
theory	O
.	O
john	O
wiley	O
and	O
sons	O
inc.	O
,	O
new	O
york	O
.	O
reissued	O
by	O
lawrence	O
erlbaum	O
associates	O
inc.	O
,	O
mahwah	O
nj	O
,	O
2002.	O
hengst	O
,	O
b	O
.	O
(	O
2012	O
)	O
.	O
hierarchical	O
approaches	O
.	O
in	O
m.	O
wiering	O
and	O
m.	O
van	O
otterlo	O
(	O
eds	O
.	O
)	O
,	O
rein-	O
forcement	O
learning	O
:	O
state-of-the-art	O
,	O
pp	O
.	O
293–323	O
.	O
springer-verlag	O
berlin	O
heidelberg	O
.	O
herrnstein	O
,	O
r.	O
j	O
.	O
(	O
1970	O
)	O
.	O
on	O
the	O
law	O
of	O
eﬀect	O
.	O
journal	O
of	O
the	O
experimental	O
analysis	O
of	O
behavior	O
,	O
13	O
(	O
2	O
)	O
:243–266	O
.	O
hersh	O
,	O
r.	O
,	O
griego	O
,	O
r.	O
j	O
.	O
(	O
1969	O
)	O
.	O
brownian	O
motion	O
and	O
potential	O
theory	O
.	O
scientiﬁc	O
american	O
,	O
220	O
(	O
3	O
)	O
:66–74	O
.	O
hester	O
,	O
t.	O
,	O
stone	O
,	O
p.	O
(	O
2012	O
)	O
.	O
learning	O
and	O
using	O
models	O
.	O
in	O
m.	O
wiering	O
and	O
m.	O
van	O
ot-	O
terlo	O
(	O
eds	O
.	O
)	O
,	O
reinforcement	B
learning	I
:	O
state-of-the-art	O
,	O
pp	O
.	O
111–141	O
.	O
springer-verlag	O
berlin	O
heidelberg	O
.	O
hesterberg	O
,	O
t.	O
c.	O
(	O
1988	O
)	O
,	O
advances	O
in	O
importance	O
sampling	O
,	O
ph.d.	O
thesis	O
,	O
statistics	O
depart-	O
ment	O
,	O
stanford	O
university	O
.	O
hilgard	O
,	O
e.	O
r.	O
(	O
1956	O
)	O
.	O
theories	O
of	O
learning	O
,	O
second	O
edition	O
.	O
appleton-century-cofts	O
,	O
inc.	O
,	O
new	O
york	O
.	O
hilgard	O
,	O
e.	O
r.	O
,	O
bower	O
,	O
g.	O
h.	O
(	O
1975	O
)	O
.	O
theories	O
of	O
learning	O
.	O
prentice-hall	O
,	O
englewood	O
cliﬀs	O
,	O
nj	O
.	O
hinton	O
,	O
g.	O
e.	O
(	O
1984	O
)	O
.	O
distributed	O
representations	O
.	O
technical	O
report	O
cmu-cs-84-157	O
.	O
depart-	O
ment	O
of	O
computer	O
science	O
,	O
carnegie-mellon	O
university	O
,	O
pittsburgh	O
,	O
pa.	O
hinton	O
,	O
g.	O
e.	O
,	O
osindero	O
,	O
s.	O
,	O
teh	O
,	O
y	O
.	O
(	O
2006	O
)	O
.	O
a	O
fast	O
learning	O
algorithm	O
for	O
deep	O
belief	O
nets	O
.	O
neural	B
computation	O
,	O
18	O
(	O
7	O
)	O
:1527–1554	O
.	O
hochreiter	O
,	O
s.	O
,	O
schmidhuber	O
,	O
j	O
.	O
(	O
1997	O
)	O
.	O
ltsm	O
can	O
solve	O
hard	O
time	O
lag	O
problems	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
9	O
(	O
nips	O
1996	O
)	O
,	O
pp	O
.	O
473–479	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
holland	O
,	O
j.	O
h.	O
(	O
1975	O
)	O
.	O
adaptation	O
in	O
natural	O
and	O
artiﬁcial	O
systems	O
.	O
university	O
of	O
michigan	O
press	O
,	O
ann	O
arbor	O
.	O
holland	O
,	O
j.	O
h.	O
(	O
1976	O
)	O
.	O
adaptation	O
.	O
in	O
r.	O
rosen	O
and	O
f.	O
m.	O
snell	O
(	O
eds	O
.	O
)	O
,	O
progress	O
in	O
theoretical	O
biology	O
,	O
vol	O
.	O
4	O
,	O
pp	O
.	O
263–293	O
.	O
academic	O
press	O
,	O
new	O
york	O
.	O
holland	O
,	O
j.	O
h.	O
(	O
1986	O
)	O
.	O
escaping	O
brittleness	O
:	O
the	O
possibility	O
of	O
general-purpose	O
learning	O
algo-	O
rithms	O
applied	O
to	O
rule-based	O
systems	O
.	O
in	O
r.	O
s.	O
michalski	O
,	O
j.	O
g.	O
carbonell	O
,	O
and	O
t.	O
m.	O
mitchell	O
(	O
eds	O
.	O
)	O
,	O
machine	O
learning	O
:	O
an	O
artiﬁcial	B
intelligence	I
approach	O
,	O
vol	O
.	O
2	O
,	O
pp	O
.	O
593–623	O
.	O
morgan	O
kaufmann	O
.	O
hollerman	O
,	O
j.	O
r.	O
,	O
schultz	O
,	O
w.	O
(	O
1998	O
)	O
.	O
dopmine	O
neurons	O
report	O
an	O
error	O
in	O
the	O
temporal	O
prediction	O
of	O
reward	O
during	O
learning	O
.	O
nature	O
neuroscience	B
,	O
1	O
(	O
4	O
)	O
:304–309	O
.	O
houk	O
,	O
j.	O
c.	O
,	O
adams	O
,	O
j.	O
l.	O
,	O
barto	O
,	O
a.	O
g.	O
(	O
1995	O
)	O
.	O
a	O
model	O
of	O
how	O
the	O
basal	B
ganglia	I
generates	O
and	O
uses	O
neural	B
signals	O
that	O
predict	O
reinforcement	O
.	O
in	O
j.	O
c.	O
houk	O
,	O
j.	O
l.	O
davis	O
,	O
and	O
d.	O
g.	O
beiser	O
(	O
eds	O
.	O
)	O
,	O
models	O
of	O
information	O
processing	O
in	O
the	O
basal	O
ganglia	O
,	O
pp	O
.	O
249–270	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
howard	O
,	O
r.	O
(	O
1960	O
)	O
.	O
dynamic	B
programming	I
and	O
markov	O
processes	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
references	O
499	O
hull	O
,	O
c.	O
l.	O
(	O
1932	O
)	O
.	O
the	O
goal-gradient	O
hypothesis	O
and	O
maze	O
learning	O
.	O
psychological	O
review	O
,	O
39	O
(	O
1	O
)	O
:25–43	O
.	O
hull	O
,	O
c.	O
l.	O
(	O
1943	O
)	O
.	O
principles	O
of	O
behavior	O
.	O
appleton-century	O
,	O
new	O
york	O
.	O
hull	O
,	O
c.	O
l.	O
(	O
1952	O
)	O
.	O
a	O
behavior	O
system	O
.	O
wiley	O
,	O
new	O
york	O
.	O
ioﬀe	O
,	O
s.	O
,	O
szegedy	O
,	O
c.	O
(	O
2015	O
)	O
.	O
batch	O
normalization	O
:	O
accelerating	O
deep	O
network	O
training	O
by	O
reducing	O
internal	O
covariate	O
shift	O
.	O
arxiv:1502.03167	O
.	O
˙ipek	O
,	O
e.	O
,	O
mutlu	O
,	O
o.	O
,	O
mart´ınez	O
,	O
j.	O
f.	O
,	O
caruana	O
,	O
r.	O
(	O
2008	O
)	O
.	O
self-optimizing	O
memory	O
controllers	O
:	O
a	O
reinforcement	B
learning	I
approach	O
.	O
in	O
isca	O
’	O
08	O
:	O
proceedings	O
of	O
the	O
35th	O
annual	O
international	O
symposium	O
on	O
computer	O
architecture	O
,	O
pp	O
.	O
39–50	O
.	O
ieee	O
computer	O
society	O
washington	O
,	O
dc	O
,	O
usa	O
.	O
izhikevich	O
,	O
e.	O
m.	O
(	O
2007	O
)	O
.	O
solving	O
the	O
distal	O
reward	O
problem	O
through	O
linkage	O
of	O
stdp	O
and	O
dopamine	O
signaling	O
.	O
cerebral	O
cortex	O
,	O
17	O
(	O
10	O
)	O
:2443–2452	O
.	O
jaakkola	O
,	O
t.	O
,	O
jordan	O
,	O
m.	O
i.	O
,	O
singh	O
,	O
s.	O
p.	O
(	O
1994	O
)	O
.	O
on	O
the	O
convergence	O
of	O
stochastic	O
iterative	B
dynamic	O
programming	O
algorithms	O
.	O
neural	B
computation	O
,	O
6:1185–1201	O
.	O
jaakkola	O
,	O
t.	O
,	O
singh	O
,	O
s.	O
p.	O
,	O
jordan	O
,	O
m.	O
i	O
.	O
(	O
1995	O
)	O
.	O
reinforcement	B
learning	I
algorithm	O
for	O
par-	O
tially	O
observable	O
markov	O
decision	O
problems	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
7	O
(	O
nips	O
1994	O
)	O
,	O
pp	O
.	O
345–352	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
jacobs	O
,	O
r.	O
a	O
.	O
(	O
1988	O
)	O
.	O
increased	O
rates	O
of	O
convergence	O
through	O
learning	O
rate	O
adaptation	O
.	O
neural	B
networks	I
,	O
1	O
(	O
4	O
)	O
:295–307	O
.	O
jaderberg	O
,	O
m.	O
,	O
mnih	O
,	O
v.	O
,	O
czarnecki	O
,	O
w.	O
m.	O
,	O
schaul	O
,	O
t.	O
,	O
leibo	O
,	O
j.	O
z.	O
,	O
silver	O
,	O
d.	O
,	O
kavukcuoglu	O
,	O
k.	O
(	O
2016	O
)	O
.	O
reinforcement	B
learning	I
with	O
unsupervised	O
auxiliary	O
tasks	O
.	O
arxiv	O
preprint	O
arxiv:1611.05397.	O
jaeger	O
,	O
h.	O
(	O
1997	O
)	O
.	O
observable	O
operator	O
models	O
and	O
conditioned	O
continuation	O
representations	O
.	O
arbeitspapiere	O
der	O
gmd	O
1043	O
,	O
gmd	O
forschungszentrum	O
informationstechnik	O
,	O
sankt	O
au-	O
gustin	O
,	O
germany	O
.	O
jaeger	O
,	O
h.	O
(	O
1998	O
)	O
.	O
discrete	O
time	O
,	O
discrete	O
valued	O
observable	O
operator	O
models	O
:	O
a	O
tutorial	O
.	O
gmd-forschungszentrum	O
informationstechnik	O
.	O
jaeger	O
,	O
h.	O
(	O
2000	O
)	O
.	O
observable	O
operator	O
models	O
for	O
discrete	O
stochastic	O
time	O
series	O
.	O
neural	B
computation	O
,	O
12	O
(	O
6	O
)	O
:1371–1398	O
.	O
jaeger	O
,	O
h.	O
(	O
2002	O
)	O
.	O
tutorial	O
on	O
training	O
recurrent	O
neural	B
networks	I
,	O
covering	O
bppt	O
,	O
rtrl	O
,	O
ekf	O
and	O
the	O
‘	O
echo	O
state	B
network	O
’	O
approach	O
.	O
german	O
national	O
research	O
center	O
for	O
information	O
technology	O
,	O
technical	O
report	O
gmd	O
report	O
159	O
,	O
2002.	O
joel	O
,	O
d.	O
,	O
niv	O
,	O
y.	O
,	O
ruppin	O
,	O
e.	O
(	O
2002	O
)	O
.	O
actor–critic	B
models	O
of	O
the	O
basal	B
ganglia	I
:	O
new	O
anatomical	O
and	O
computational	O
perspectives	O
.	O
neural	B
networks	I
,	O
15	O
(	O
4	O
)	O
:535–547	O
.	O
johnson	O
,	O
a.	O
,	O
redish	O
,	O
a.	O
d.	O
(	O
2007	O
)	O
.	O
neural	B
ensembles	O
in	O
ca3	O
transiently	O
encode	O
paths	O
forward	O
of	O
the	O
animal	O
at	O
a	O
decision	O
point	O
.	O
the	O
journal	O
of	O
neuroscience	O
,	O
27	O
(	O
45	O
)	O
:12176–12189	O
.	O
kaelbling	O
,	O
l.	O
p.	O
(	O
1993a	O
)	O
.	O
hierarchical	O
learning	O
in	O
stochastic	O
domains	O
:	O
preliminary	O
results	O
.	O
in	O
proceedings	O
of	O
the	O
10th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
1993	O
)	O
,	O
pp	O
.	O
167–173	O
.	O
morgan	O
kaufmann	O
.	O
kaelbling	O
,	O
l.	O
p.	O
(	O
1993b	O
)	O
.	O
learning	O
in	O
embedded	O
systems	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
kaelbling	O
,	O
l.	O
p	O
.	O
(	O
ed	O
.	O
)	O
(	O
1996	O
)	O
.	O
special	O
triple	O
issue	O
on	O
reinforcement	B
learning	I
,	O
machine	O
learning	O
,	O
22	O
(	O
1/2/3	O
)	O
.	O
kaelbling	O
,	O
l.	O
p.	O
,	O
littman	O
,	O
m.	O
l.	O
,	O
moore	O
,	O
a.	O
w.	O
(	O
1996	O
)	O
.	O
reinforcement	B
learning	I
:	O
a	O
survey	O
.	O
journal	O
of	O
artiﬁcial	O
intelligence	O
research	O
,	O
4:237–285	O
.	O
kakade	O
,	O
s.	O
m.	O
(	O
2002	O
)	O
.	O
a	O
natural	O
policy	O
gradient	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
14	O
(	O
nips	O
2001	O
)	O
,	O
pp	O
.	O
1531–1538	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
kakade	O
,	O
s.	O
m.	O
(	O
2003	O
)	O
.	O
on	O
the	O
sample	O
complexity	O
of	O
reinforcement	O
learning	O
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
london	O
.	O
500	O
references	O
kakutani	O
,	O
s.	O
(	O
1945	O
)	O
.	O
markov	O
processes	O
and	O
the	O
dirichlet	O
problem	O
.	O
proceedings	O
of	O
the	O
japan	O
academy	O
,	O
21	O
(	O
3-10	O
)	O
:227–233	O
.	O
kalos	O
,	O
m.	O
h.	O
,	O
whitlock	O
,	O
p.	O
a	O
.	O
(	O
1986	O
)	O
.	O
monte	O
carlo	O
methods	O
.	O
wiley	O
,	O
new	O
york	O
.	O
kamin	O
,	O
l.	O
j	O
.	O
(	O
1968	O
)	O
.	O
“	O
attention-like	O
”	O
processes	O
in	O
classical	O
conditioning	B
.	O
in	O
m.	O
r.	O
jones	O
(	O
ed	O
.	O
)	O
,	O
miami	O
symposium	O
on	O
the	O
prediction	B
of	O
behavior	O
,	O
1967	O
:	O
aversive	O
stimulation	O
,	O
pp	O
.	O
9–31	O
.	O
university	O
of	O
miami	O
press	O
,	O
coral	O
gables	O
,	O
florida	O
.	O
kamin	O
,	O
l.	O
j	O
.	O
(	O
1969	O
)	O
.	O
predictability	O
,	O
surprise	O
,	O
attention	O
,	O
and	O
conditioning	O
.	O
in	O
b.	O
a.	O
campbell	O
and	O
r.	O
m.	O
church	O
(	O
eds	O
.	O
)	O
,	O
punishment	O
and	O
aversive	O
behavior	O
,	O
pp	O
.	O
279–296	O
.	O
appleton-	O
century-crofts	O
,	O
new	O
york	O
.	O
kandel	O
,	O
e.	O
r.	O
,	O
schwartz	O
,	O
j.	O
h.	O
,	O
jessell	O
,	O
t.	O
m.	O
,	O
siegelbaum	O
,	O
s.	O
a.	O
,	O
hudspeth	O
,	O
a.	O
j	O
.	O
(	O
eds	O
.	O
)	O
(	O
2013	O
)	O
.	O
principles	O
of	O
neural	O
science	O
,	O
fifth	O
edition	O
.	O
mcgraw-hill	O
companies	O
,	O
inc.	O
karampatziakis	O
,	O
n.	O
,	O
langford	O
,	O
j	O
.	O
(	O
2010	O
)	O
.	O
online	B
importance	O
weight	O
aware	O
updates	O
.	O
arxiv:1011.1576.	O
kashyap	O
,	O
r.	O
l.	O
,	O
blaydon	O
,	O
c.	O
c.	O
,	O
fu	O
,	O
k.	O
s.	O
(	O
1970	O
)	O
.	O
stochastic	O
approximation	O
.	O
in	O
j.	O
m.	O
mendel	O
and	O
k.	O
s.	O
fu	O
(	O
eds	O
.	O
)	O
,	O
adaptive	O
,	O
learning	O
,	O
and	O
pattern	O
recognition	O
systems	O
:	O
theory	O
and	O
applications	O
,	O
pp	O
.	O
329–355	O
.	O
academic	O
press	O
,	O
new	O
york	O
.	O
kearney	O
,	O
a.	O
,	O
veeriah	O
,	O
v	O
,	O
travnik	O
,	O
j	O
,	O
sutton	O
,	O
r.	O
s.	O
,	O
pilarski	O
,	O
p.	O
m.	O
(	O
in	O
preparation	O
)	O
.	O
tidbd	O
:	O
adapting	O
temporal-diﬀerence	O
step-sizes	O
through	O
stochastic	O
meta-descent	O
.	O
kearns	O
,	O
m.	O
,	O
singh	O
,	O
s.	O
(	O
2002	O
)	O
.	O
near-optimal	O
reinforcement	B
learning	I
in	O
polynomial	O
time	O
.	O
machine	O
learning	O
,	O
49	O
(	O
2-3	O
)	O
:209–232	O
.	O
keerthi	O
,	O
s.	O
s.	O
,	O
ravindran	O
,	O
b	O
.	O
(	O
1997	O
)	O
.	O
reinforcement	B
learning	I
.	O
in	O
e.	O
fieslerm	O
and	O
r.	O
beale	O
(	O
eds	O
.	O
)	O
,	O
handbook	O
of	O
neural	O
computation	O
,	O
c3	O
.	O
oxford	O
university	O
press	O
,	O
new	O
york	O
.	O
kehoe	O
,	O
e.	O
j	O
.	O
(	O
1982	O
)	O
.	O
conditioning	B
with	O
serial	O
compound	B
stimuli	O
:	O
theoretical	O
and	O
empirical	O
issues	O
.	O
experimental	O
animal	O
behavior	O
,	O
1:30–65	O
.	O
kehoe	O
,	O
e.	O
j.	O
,	O
schreurs	O
,	O
b.	O
g.	O
,	O
graham	O
,	O
p.	O
(	O
1987	O
)	O
.	O
temporal	O
primacy	O
overrides	O
prior	O
training	O
in	O
serial	O
compound	B
conditioning	O
of	O
the	O
rabbits	O
nictitating	O
membrane	O
response	O
.	O
animal	O
learning	O
&	O
behavior	O
,	O
15	O
(	O
4	O
)	O
:455–464	O
.	O
keiﬂin	O
,	O
r.	O
,	O
janak	O
,	O
p.	O
h.	O
(	O
2015	O
)	O
.	O
dopamine	B
prediction	O
errors	O
in	O
reward	O
learning	O
and	O
addiction	B
:	O
ffrom	O
theory	O
to	O
neural	B
circuitry	O
.	O
neuron	O
,	O
88	O
(	O
2	O
)	O
:247–	O
263.	O
kimble	O
,	O
g.	O
a	O
.	O
(	O
1961	O
)	O
.	O
hilgard	O
and	O
marquis	O
’	O
conditioning	B
and	O
learning	O
.	O
appleton-century-	O
crofts	O
,	O
new	O
york	O
.	O
kimble	O
,	O
g.	O
a	O
.	O
(	O
1967	O
)	O
.	O
foundations	O
of	O
conditioning	O
and	O
learning	O
.	O
appleton-century-crofts	O
,	O
new	O
york	O
.	O
kingma	O
,	O
d.	O
,	O
ba	O
,	O
j	O
.	O
(	O
2014	O
)	O
.	O
adam	O
:	O
a	O
method	O
for	O
stochastic	O
optimization	O
.	O
arxiv:1412.6980.	O
klopf	O
,	O
a.	O
h.	O
(	O
1972	O
)	O
.	O
brain	O
function	O
and	O
adaptive	O
systems—a	O
heterostatic	O
theory	O
.	O
technical	O
report	O
afcrl-72-0164	O
,	O
air	O
force	O
cambridge	O
research	O
laboratories	O
,	O
bedford	O
,	O
ma	O
.	O
a	O
summary	O
appears	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
(	O
1974	O
)	O
.	O
ieee	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
society	O
,	O
dallas	O
,	O
tx	O
.	O
klopf	O
,	O
a.	O
h.	O
(	O
1975	O
)	O
.	O
a	O
comparison	O
of	O
natural	O
and	B
artiﬁcial	I
intelligence	I
.	O
sigart	O
newsletter	O
,	O
53:11–13	O
.	O
klopf	O
,	O
a.	O
h.	O
(	O
1982	O
)	O
.	O
the	O
hedonistic	O
neuron	O
:	O
a	O
theory	O
of	O
memory	O
,	O
learning	O
,	O
and	O
intelligence	O
.	O
hemisphere	O
,	O
washington	O
,	O
dc	O
.	O
klopf	O
,	O
a.	O
h.	O
(	O
1988	O
)	O
.	O
a	O
neuronal	O
model	O
of	O
classical	O
conditioning	B
.	O
psychobiology	O
,	O
16	O
(	O
2	O
)	O
:85–125	O
.	O
klyubin	O
,	O
a.	O
s.	O
,	O
polani	O
,	O
d.	O
,	O
nehaniv	O
,	O
c.	O
l.	O
(	O
2005	O
)	O
.	O
empowerment	O
:	O
a	O
universal	O
agent-centric	O
measure	O
of	O
control	O
.	O
in	O
proceedings	O
of	O
the	O
2005	O
ieee	O
congress	O
on	O
evolutionary	O
computation	O
(	O
vol	O
.	O
1	O
,	O
pp	O
.	O
128–135	O
)	O
.	O
ieee	O
.	O
kober	O
,	O
j.	O
,	O
peters	O
,	O
j	O
.	O
(	O
2012	O
)	O
.	O
reinforcement	B
learning	I
in	O
robotics	O
:	O
a	O
survey	O
.	O
in	O
m.	O
wiering	O
,	O
m.	O
van	O
otterlo	O
(	O
eds	O
.	O
)	O
,	O
reinforcement	B
learning	I
:	O
state-of-the-art	O
,	O
pp	O
.	O
579–610	O
.	O
springer-verlag	O
references	O
berlin	O
heidelberg	O
.	O
501	O
kocsis	O
,	O
l.	O
,	O
szepesv´ari	O
,	O
cs	O
.	O
(	O
2006	O
)	O
.	O
bandit	O
based	O
monte-carlo	O
planning	B
.	O
in	O
proceedings	O
of	O
the	O
european	O
conference	O
on	O
machine	O
learning	O
,	O
pp	O
.	O
282–293	O
.	O
springer-verlag	O
berlin	O
heidelberg	O
.	O
kohonen	O
,	O
t.	O
(	O
1977	O
)	O
.	O
associative	O
memory	O
:	O
a	O
system	O
theoretic	O
approach	O
.	O
springer-verlag	O
,	O
berlin	O
.	O
koller	O
,	O
d.	O
,	O
friedman	O
,	O
n.	O
(	O
2009	O
)	O
.	O
probabilistic	O
graphical	O
models	O
:	O
principles	O
and	O
techniques	O
.	O
mit	O
press	O
.	O
kolodziejski	O
,	O
c.	O
,	O
porr	O
,	O
b.	O
,	O
w¨org¨otter	O
,	O
f.	O
(	O
2009	O
)	O
.	O
on	O
the	O
asymptotic	O
equivalence	O
between	O
diﬀerential	B
hebbian	O
and	O
temporal	O
diﬀerence	O
learning	O
.	O
neural	B
computation	O
,	O
21	O
(	O
4	O
)	O
:1173–	O
1202.	O
kolter	O
,	O
j.	O
z	O
.	O
(	O
2011	O
)	O
.	O
the	O
ﬁxed	O
points	O
of	O
oﬀ-policy	O
td	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
24	O
(	O
nips	O
2011	O
)	O
,	O
pp	O
.	O
2169–2177	O
.	O
curran	O
associates	O
,	O
inc.	O
konda	O
,	O
v.	O
r.	O
,	O
tsitsiklis	O
,	O
j.	O
n.	O
(	O
2000	O
)	O
.	O
actor-critic	O
algorithms	O
.	O
in	O
advances	O
in	O
neural	O
informa-	O
tion	B
processing	O
systems	O
12	O
(	O
nips	O
1999	O
)	O
,	O
pp	O
.	O
1008–1014	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
konda	O
,	O
v.	O
r.	O
,	O
tsitsiklis	O
,	O
j.	O
n.	O
(	O
2003	O
)	O
.	O
on	O
actor-critic	O
algorithms	O
.	O
siam	O
journal	O
on	O
control	O
and	O
optimization	O
,	O
42	O
(	O
4	O
)	O
:1143–1166	O
.	O
konidaris	O
,	O
g.	O
d.	O
,	O
osentoski	O
,	O
s.	O
,	O
thomas	O
,	O
p.	O
s.	O
(	O
2011	O
)	O
.	O
value	B
function	I
approximation	O
in	O
rein-	O
forcement	O
learning	O
using	O
the	O
fourier	O
basis	O
.	O
in	O
proceedings	O
of	O
the	O
twenty-fifth	O
conference	O
of	O
the	O
association	O
for	O
the	O
advancement	O
of	O
artiﬁcial	O
intelligence	O
,	O
pp	O
.	O
380–385	O
.	O
korf	O
,	O
r.	O
e.	O
(	O
1988	O
)	O
.	O
optimal	O
path	O
ﬁnding	O
algorithms	O
.	O
in	O
l.	O
n.	O
kanal	O
and	O
v.	O
kumar	O
(	O
eds	O
.	O
)	O
,	O
search	O
in	O
artiﬁcial	B
intelligence	I
,	O
pp	O
.	O
223–267	O
.	O
springer-verlag	O
,	O
berlin	O
.	O
korf	O
,	O
r.	O
e.	O
(	O
1990	O
)	O
.	O
real-time	O
heuristic	O
search	O
.	O
artiﬁcial	B
intelligence	I
,	O
42	O
(	O
2–3	O
)	O
,	O
189–211	O
.	O
koshland	O
,	O
d.	O
e.	O
(	O
1980	O
)	O
.	O
bacterial	O
chemotaxis	O
as	O
a	O
model	O
behavioral	O
system	O
.	O
raven	O
press	O
,	O
new	O
york	O
.	O
koza	O
,	O
j.	O
r.	O
(	O
1992	O
)	O
.	O
genetic	O
programming	O
:	O
on	O
the	O
programming	O
of	O
computers	O
by	O
means	O
of	O
natural	O
selection	O
(	O
vol	O
.	O
1	O
)	O
.	O
mit	O
press.	O
,	O
cambridge	O
,	O
ma	O
.	O
kraft	O
,	O
l.	O
g.	O
,	O
campagna	O
,	O
d.	O
p.	O
(	O
1990	O
)	O
.	O
a	O
summary	O
comparison	O
of	O
cmac	O
neural	B
network	O
and	O
traditional	O
adaptive	O
control	B
systems	O
.	O
in	O
t.	O
miller	O
,	O
r.	O
s.	O
sutton	O
,	O
and	O
p.	O
j.	O
werbos	O
(	O
eds	O
.	O
)	O
,	O
neural	B
networks	I
for	O
control	B
,	O
pp	O
.	O
143–169	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
kraft	O
,	O
l.	O
g.	O
,	O
miller	O
,	O
w.	O
t.	O
,	O
dietz	O
,	O
d.	O
(	O
1992	O
)	O
.	O
development	O
and	O
application	O
of	O
cmac	O
neural	B
network-based	O
control	B
.	O
in	O
d.	O
a.	O
white	O
and	O
d.	O
a.	O
sofge	O
(	O
eds	O
.	O
)	O
,	O
handbook	O
of	O
intelligent	O
control	B
:	O
neural	B
,	O
fuzzy	O
,	O
and	O
adaptive	O
approaches	O
,	O
pp	O
.	O
215–232	O
.	O
van	O
nostrand	O
reinhold	O
,	O
new	O
york	O
.	O
kumar	O
,	O
p.	O
r.	O
,	O
varaiya	O
,	O
p.	O
(	O
1986	O
)	O
.	O
stochastic	O
systems	O
:	O
estimation	O
,	O
identiﬁcation	O
,	O
and	O
adaptive	O
control	B
.	O
prentice-hall	O
,	O
englewood	O
cliﬀs	O
,	O
nj	O
.	O
kumar	O
,	O
p.	O
r.	O
(	O
1985	O
)	O
.	O
a	O
survey	O
of	O
some	O
results	O
in	O
stochastic	O
adaptive	O
control	B
.	O
siam	O
journal	O
of	O
control	O
and	O
optimization	O
,	O
23	O
(	O
3	O
)	O
:329–380	O
.	O
kumar	O
,	O
v.	O
,	O
kanal	O
,	O
l.	O
n.	O
(	O
1988	O
)	O
.	O
the	O
cdp	O
,	O
a	O
unifying	O
formulation	O
for	O
heuristic	O
search	O
,	O
dynamic	O
in	O
l.	O
n.	O
kanal	O
and	O
v.	O
kumar	O
(	O
eds	O
.	O
)	O
,	O
search	O
in	O
programming	O
,	O
and	O
branch-and-bound	O
.	O
artiﬁcial	B
intelligence	I
,	O
pp	O
.	O
1–37	O
.	O
springer-verlag	O
,	O
berlin	O
.	O
kushner	O
,	O
h.	O
j.	O
,	O
dupuis	O
,	O
p.	O
(	O
1992	O
)	O
.	O
numerical	O
methods	O
for	O
stochastic	O
control	B
problems	O
in	O
continuous	O
time	O
.	O
springer-verlag	O
,	O
new	O
york	O
.	O
kuvayev	O
,	O
l.	O
,	O
sutton	O
,	O
r.s	O
.	O
(	O
1996	O
)	O
.	O
model-based	B
reinforcement	I
learning	I
with	O
an	O
approximate	B
,	O
learned	O
model	O
.	O
proceedings	O
of	O
the	O
ninth	O
yale	O
workshop	O
on	O
adaptive	O
and	O
learning	O
systems	O
,	O
pp	O
.	O
101–105	O
,	O
yale	O
university	O
,	O
new	O
haven	O
,	O
ct.	O
lagoudakis	O
,	O
m.	O
,	O
parr	O
,	O
r.	O
(	O
2003	O
)	O
.	O
least	O
squares	O
policy	B
iteration	I
.	O
journal	O
of	O
machine	O
learning	O
research	O
,	O
4	O
(	O
dec	O
)	O
:1107–1149	O
.	O
502	O
references	O
lai	O
,	O
t.	O
l.	O
,	O
robbins	O
,	O
h.	O
(	O
1985	O
)	O
.	O
asymptotically	O
eﬃcient	O
adaptive	O
allocation	O
rules	O
.	O
advances	O
in	O
applied	O
mathematics	O
,	O
6	O
(	O
1	O
)	O
:4–22	O
.	O
lakshmivarahan	O
,	O
s.	O
,	O
narendra	O
,	O
k.	O
s.	O
(	O
1982	O
)	O
.	O
learning	O
algorithms	O
for	O
two-person	O
zero-sum	O
siam	O
journal	O
of	O
stochastic	O
games	O
with	O
incomplete	O
information	O
:	O
a	O
uniﬁed	O
approach	O
.	O
control	O
and	O
optimization	O
,	O
20	O
(	O
4	O
)	O
:541–552	O
.	O
lammel	O
,	O
s.	O
,	O
lim	O
,	O
b.	O
k.	O
,	O
malenka	O
,	O
r.	O
c.	O
(	O
2014	O
)	O
.	O
reward	O
and	O
aversion	O
in	O
a	O
heterogeneous	O
midbrain	O
dopamine	B
system	O
.	O
neuropharmacology	O
,	O
76:353–359	O
.	O
lane	O
,	O
s.	O
h.	O
,	O
handelman	O
,	O
d.	O
a.	O
,	O
gelfand	O
,	O
j.	O
j	O
.	O
(	O
1992	O
)	O
.	O
theory	O
and	O
development	O
of	O
higher-order	O
cmac	O
neural	B
networks	I
.	O
ieee	O
control	B
systems	O
,	O
12	O
(	O
2	O
)	O
:23–30	O
.	O
lecun	O
,	O
y	O
.	O
(	O
1985	O
)	O
.	O
une	O
procdure	O
d	O
’	O
apprentissage	O
pour	O
rseau	O
a	O
seuil	O
asymmetrique	O
(	O
a	O
learning	O
scheme	O
for	O
asymmetric	O
threshold	O
networks	O
)	O
.	O
in	O
proceedings	O
of	O
cognitiva	O
85	O
,	O
paris	O
,	O
france	O
.	O
lecun	O
,	O
y.	O
,	O
bottou	O
,	O
l.	O
,	O
bengio	O
,	O
y.	O
,	O
haﬀner	O
,	O
p.	O
(	O
1998	O
)	O
.	O
gradient-based	O
learning	O
applied	O
to	O
document	O
recognition	O
.	O
proceedings	O
of	O
the	O
ieee	O
,	O
86	O
(	O
11	O
)	O
:2278–2324	O
.	O
legenstein	O
,	O
r.	O
w.	O
,	O
maass	O
,	O
d.	O
p.	O
(	O
2008	O
)	O
.	O
a	O
learning	O
theory	O
for	O
reward-modulated	O
spike-timing-	O
dependent	O
plasticity	O
with	O
application	O
to	O
biofeedback	O
.	O
plos	O
computational	O
biology	O
,	O
4	O
(	O
10	O
)	O
.	O
levy	O
,	O
w.	O
b.	O
,	O
steward	O
,	O
d.	O
(	O
1983	O
)	O
.	O
temporal	O
contiguity	O
requirements	O
for	O
long-term	O
associative	O
potentiation/depression	O
in	O
the	O
hippocampus	O
.	O
neuroscience	B
,	O
8	O
(	O
4	O
)	O
:791–797	O
.	O
lewis	O
,	O
f.	O
l.	O
,	O
liu	O
,	O
d	O
.	O
(	O
eds	O
.	O
)	O
(	O
2012	O
)	O
.	O
reinforcement	B
learning	I
and	O
approximate	O
dynamic	O
pro-	O
gramming	O
for	O
feedback	O
control	B
.	O
john	O
wiley	O
and	O
sons	O
.	O
lewis	O
,	O
r.	O
l.	O
,	O
howes	O
,	O
a.	O
,	O
singh	O
,	O
s.	O
(	O
2014	O
)	O
.	O
computational	O
rationality	O
:	O
linking	O
mechanism	O
and	O
behavior	O
through	O
utility	O
maximization	O
.	O
topics	O
in	O
cognitive	O
science	O
,	O
6	O
(	O
2	O
)	O
:279–311	O
.	O
li	O
,	O
l.	O
(	O
2012	O
)	O
.	O
sample	O
complexity	O
bounds	O
of	O
exploration	O
.	O
in	O
m.	O
wiering	O
and	O
m.	O
van	O
otterlo	O
(	O
eds	O
.	O
)	O
,	O
reinforcement	B
learning	I
:	O
state-of-the-art	O
,	O
pp	O
.	O
175–204	O
.	O
springer-verlag	O
berlin	O
hei-	O
delberg	O
.	O
li	O
,	O
l.	O
,	O
chu	O
,	O
w.	O
,	O
langford	O
,	O
j.	O
,	O
schapire	O
,	O
r.	O
e.	O
(	O
2010	O
)	O
.	O
a	O
contextual-bandit	O
approach	O
to	O
person-	O
in	O
proceedings	O
of	O
the	O
19th	O
international	O
conference	O
alized	O
news	O
article	O
recommendation	O
.	O
on	O
world	O
wide	O
web	O
,	O
pp	O
.	O
661–670	O
.	O
acm	O
,	O
new	O
york	O
.	O
lin	O
,	O
c.-s.	O
,	O
kim	O
,	O
h.	O
(	O
1991	O
)	O
.	O
cmac-based	O
adaptive	O
critic	B
self-learning	O
control	B
.	O
ieee	O
transac-	O
tions	O
on	O
neural	B
networks	I
,	O
2	O
(	O
5	O
)	O
:530–533	O
.	O
lin	O
,	O
l.-j	O
.	O
(	O
1992	O
)	O
.	O
self-improving	O
reactive	O
agents	O
based	O
on	O
reinforcement	B
learning	I
,	O
planning	B
and	O
teaching	O
.	O
machine	O
learning	O
,	O
8	O
(	O
3-4	O
)	O
:293–321	O
.	O
lin	O
,	O
l.-j.	O
,	O
mitchell	O
,	O
t.	O
(	O
1992	O
)	O
.	O
reinforcement	B
learning	I
with	O
hidden	O
states	O
.	O
in	O
proceedings	O
of	O
the	O
second	O
international	O
conference	O
on	O
simulation	O
of	O
adaptive	O
behavior	O
:	O
from	O
animals	O
to	O
animats	O
,	O
pp	O
.	O
271–280	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
littman	O
,	O
m.	O
l.	O
,	O
cassandra	O
,	O
a.	O
r.	O
,	O
kaelbling	O
,	O
l.	O
p.	O
(	O
1995	O
)	O
.	O
learning	O
policies	O
for	O
partially	O
in	O
proceedings	O
of	O
the	O
12th	O
international	O
conference	O
observable	O
environments	O
:	O
scaling	O
up	O
.	O
on	O
machine	O
learning	O
(	O
icml	O
1995	O
)	O
,	O
pp	O
.	O
362–370	O
.	O
morgan	O
kaufmann	O
.	O
littman	O
,	O
m.	O
l.	O
,	O
dean	O
,	O
t.	O
l.	O
,	O
kaelbling	O
,	O
l.	O
p.	O
(	O
1995	O
)	O
.	O
on	O
the	O
complexity	O
of	O
solving	O
markov	O
in	O
proceedings	O
of	O
the	O
eleventh	O
annual	O
conference	O
on	O
uncertainty	O
in	O
decision	O
problems	O
.	O
artiﬁcial	B
intelligence	I
,	O
pp	O
.	O
394–402	O
.	O
littman	O
,	O
m.	O
l.	O
,	O
sutton	O
,	O
r.	O
s.	O
,	O
singh	O
(	O
2002	O
)	O
.	O
predictive	O
representations	O
of	O
state	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
14	O
(	O
nips	O
2001	O
)	O
,	O
pp	O
.	O
1555-1561.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
liu	O
,	O
j.	O
s.	O
(	O
2001	O
)	O
.	O
monte	O
carlo	O
strategies	O
in	O
scientiﬁc	O
computing	O
.	O
springer-verlag	O
,	O
berlin	O
.	O
ljung	O
,	O
l.	O
(	O
1998	O
)	O
.	O
system	B
identiﬁcation	I
.	O
in	O
a.	O
proch´azka	O
,	O
j.	O
uhl´ıˆr	O
,	O
p.	O
w.	O
j.	O
rayner	O
,	O
and	O
n.	O
g.	O
kingsbury	O
(	O
eds	O
.	O
)	O
,	O
signal	O
analysis	O
and	O
prediction	O
,	O
pp	O
.	O
163–173	O
.	O
springer	O
science	O
+	O
business	O
media	O
new	O
york	O
,	O
llc	O
.	O
references	O
503	O
ljung	O
,	O
l.	O
,	O
s¨oderstrom	O
,	O
t.	O
(	O
1983	O
)	O
.	O
theory	O
and	O
practice	O
of	O
recursive	O
identiﬁcation	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
ljungberg	O
,	O
t.	O
,	O
apicella	O
,	O
p.	O
,	O
schultz	O
,	O
w.	O
(	O
1992	O
)	O
.	O
responses	O
of	O
monkey	O
dopamine	B
neurons	O
during	O
learning	O
of	O
behavioral	O
reactions	O
.	O
journal	O
of	O
neurophysiology	O
,	O
67	O
(	O
1	O
)	O
:145–163	O
.	O
lovejoy	O
,	O
w.	O
s.	O
(	O
1991	O
)	O
.	O
a	O
survey	O
of	O
algorithmic	O
methods	O
for	O
partially	O
observed	O
markov	O
decision	O
processes	O
.	O
annals	O
of	O
operations	O
research	O
,	O
28	O
(	O
1	O
)	O
:47–66	O
.	O
luce	O
,	O
d.	O
(	O
1959	O
)	O
.	O
individual	O
choice	O
behavior	O
.	O
wiley	O
,	O
new	O
york	O
.	O
ludvig	O
,	O
e.	O
a.	O
,	O
bellemare	O
,	O
m.	O
g.	O
,	O
pearson	O
,	O
k.	O
g.	O
(	O
2011	O
)	O
.	O
a	O
primer	O
on	O
reinforcement	B
learning	I
in	O
the	O
brain	O
:	O
psychological	O
,	O
computational	O
,	O
and	O
neural	O
perspectives	O
.	O
in	O
e.	O
alonso	O
and	O
e.	O
mondrag´on	O
(	O
eds	O
.	O
)	O
,	O
computational	O
neuroscience	O
for	O
advancing	O
artiﬁcial	B
intelligence	I
:	O
models	O
,	O
methods	O
and	O
applications	O
,	O
pp	O
.	O
111–44	O
.	O
medical	O
information	O
science	O
reference	O
,	O
hershey	O
pa.	O
ludvig	O
,	O
e.	O
a.	O
,	O
sutton	O
,	O
r.	O
s.	O
,	O
kehoe	O
,	O
e.	O
j	O
.	O
(	O
2008	O
)	O
.	O
stimulus	O
representation	O
and	O
the	O
tim-	O
ing	B
of	O
reward-prediction	O
errors	O
in	O
models	O
of	O
the	O
dopamine	B
system	O
.	O
neural	B
computation	O
,	O
20	O
(	O
12	O
)	O
:3034–3054	O
.	O
ludvig	O
,	O
e.	O
a.	O
,	O
sutton	O
,	O
r.	O
s.	O
,	O
kehoe	O
,	O
e.	O
j	O
.	O
(	O
2012	O
)	O
.	O
evaluating	O
the	O
td	O
model	O
of	O
classical	O
conditioning	B
.	O
learning	O
&	O
behavior	O
,	O
40	O
(	O
3	O
)	O
:305–319	O
.	O
machado	O
,	O
a	O
.	O
(	O
1997	O
)	O
.	O
learning	O
the	O
temporal	O
dynamics	O
of	O
behavior	O
.	O
psychological	O
review	O
,	O
104	O
(	O
2	O
)	O
:241–265	O
.	O
mackintosh	O
,	O
n.	O
j	O
.	O
(	O
1975	O
)	O
.	O
a	O
theory	O
of	O
attention	O
:	O
variations	O
in	O
the	O
associability	O
of	O
stimuli	O
with	O
reinforcement	O
.	O
psychological	O
review	O
,	O
82	O
(	O
4	O
)	O
:276–298	O
.	O
mackintosh	O
,	O
n.	O
j	O
.	O
(	O
1983	O
)	O
.	O
conditioning	B
and	O
associative	O
learning	O
.	O
clarendon	O
press	O
,	O
oxford	O
.	O
maclin	O
,	O
r.	O
,	O
shavlik	O
,	O
j.	O
w.	O
(	O
1994	O
)	O
.	O
incorporating	O
advice	O
into	O
agents	O
that	O
learn	O
from	O
rein-	O
in	O
proceedings	O
of	O
the	O
twelfth	O
national	O
conference	O
on	O
artiﬁcial	B
intelligence	I
forcements	O
.	O
(	O
aaai-94	O
)	O
,	O
pp	O
.	O
694–699	O
.	O
aaai	O
press	O
,	O
menlo	O
park	O
,	O
ca	O
.	O
maei	O
,	O
h.	O
r.	O
(	O
2011	O
)	O
.	O
gradient	B
temporal-diﬀerence	O
learning	O
algorithms	O
.	O
ph.d.	O
thesis	O
,	O
univer-	O
sity	O
of	O
alberta	O
,	O
edmonton	O
.	O
maei	O
,	O
h.	O
r.	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
2010	O
)	O
.	O
gq	O
(	O
λ	O
)	O
:	O
a	O
general	O
gradient	O
algorithm	O
for	O
temporal-diﬀerence	O
prediction	B
learning	O
with	B
eligibility	I
traces	I
.	O
in	O
proceedings	O
of	O
the	O
third	O
conference	O
on	O
artiﬁ-	O
cial	O
general	O
intelligence	O
,	O
pp	O
.	O
91–96	O
.	O
maei	O
,	O
h.	O
r.	O
,	O
szepesv´ari	O
,	O
cs.	O
,	O
bhatnagar	O
,	O
s.	O
,	O
precup	O
,	O
d.	O
,	O
silver	O
,	O
d.	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
2009	O
)	O
.	O
convergent	O
temporal-diﬀerence	B
learning	I
with	O
arbitrary	O
smooth	O
function	B
approximation	I
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
22	O
(	O
nips	O
2009	O
)	O
,	O
pp	O
.	O
1204–1212	O
.	O
curran	O
associates	O
,	O
inc.	O
maei	O
,	O
h.	O
r.	O
,	O
szepesv´ari	O
,	O
cs.	O
,	O
bhatnagar	O
,	O
s.	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
2010	O
)	O
.	O
toward	O
oﬀ-policy	B
learning	O
control	B
with	O
function	B
approximation	I
.	O
in	O
proceedings	O
of	O
the	O
27th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
2010	O
)	O
,	O
pp	O
.	O
719–726	O
)	O
.	O
mahadevan	O
,	O
s.	O
(	O
1996	O
)	O
.	O
average	O
reward	O
reinforcement	O
learning	O
:	O
foundations	O
,	O
algorithms	O
,	O
and	O
empirical	O
results	O
.	O
machine	O
learning	O
,	O
22	O
(	O
1	O
)	O
:159–196	O
.	O
mahadevan	O
,	O
s.	O
,	O
liu	O
,	O
b.	O
,	O
thomas	O
,	O
p.	O
,	O
dabney	O
,	O
w.	O
,	O
giguere	O
,	O
s.	O
,	O
jacek	O
,	O
n.	O
,	O
gemp	O
,	O
i.	O
,	O
liu	O
,	O
j	O
.	O
(	O
2014	O
)	O
.	O
proximal	O
reinforcement	B
learning	I
:	O
a	O
new	O
theory	O
of	O
sequential	O
decision	O
making	O
in	O
primal-dual	O
spaces	O
.	O
arxiv	O
preprint	O
arxiv:1405.6757.	O
mahadevan	O
,	O
s.	O
,	O
connell	O
,	O
j	O
.	O
(	O
1992	O
)	O
.	O
automatic	O
programming	O
of	O
behavior-based	O
robots	O
using	O
reinforcement	B
learning	I
.	O
artiﬁcial	B
intelligence	I
,	O
55	O
(	O
2-3	O
)	O
:311–365	O
.	O
mahmood	O
,	O
a.	O
r.	O
(	O
2017	O
)	O
.	O
incremental	O
oﬀ-policy	O
reinforcement	B
learning	I
algorithms	O
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
alberta	O
,	O
edmonton	O
.	O
mahmood	O
,	O
a.	O
r.	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
2015	O
)	O
.	O
oﬀ-policy	B
learning	O
based	O
on	O
weighted	O
importance	O
504	O
references	O
sampling	O
with	O
linear	O
computational	O
complexity	O
.	O
in	O
proceedings	O
of	O
the	O
31st	O
conference	O
on	O
uncertainty	O
in	O
artiﬁcial	O
intelligence	O
(	O
uai-2015	O
)	O
,	O
pp	O
.	O
552–561	O
.	O
auai	O
press	O
corvallis	O
,	O
ore-	O
gon	O
.	O
mahmood	O
,	O
a.	O
r.	O
,	O
sutton	O
,	O
r.	O
s.	O
,	O
degris	O
,	O
t.	O
,	O
pilarski	O
,	O
p.	O
m.	O
(	O
2012	O
)	O
.	O
tuning-free	O
step-size	O
adap-	O
tation	O
.	O
in	O
2012	O
ieee	O
international	O
conference	O
on	O
acoustics	O
,	O
speech	O
and	O
signal	O
processing	O
(	O
icassp	O
)	O
,	O
proceedings	O
,	O
pp	O
.	O
2121–2124	O
.	O
ieee	O
.	O
mahmood	O
,	O
a.	O
r.	O
,	O
yu	O
,	O
h	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
2017	O
)	O
.	O
multi-step	O
oﬀ-policy	B
learning	O
without	O
importance	B
sampling	I
ratios	O
.	O
arxiv	O
1702.03006.	O
mahmood	O
,	O
a.	O
r.	O
,	O
van	O
hasselt	O
,	O
h.	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
2014	O
)	O
.	O
weighted	O
importance	O
sampling	O
for	O
oﬀ-policy	O
learning	O
with	O
linear	B
function	I
approximation	I
.	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
27	O
(	O
nips	O
2014	O
)	O
,	O
pp	O
.	O
3014–3022	O
.	O
curran	O
associates	O
,	O
inc.	O
marbach	O
,	O
p.	O
,	O
tsitsiklis	O
,	O
j.	O
n.	O
(	O
1998	O
)	O
.	O
simulation-based	O
optimization	O
of	O
markov	O
reward	O
processes	O
.	O
mit	O
technical	O
report	O
lids-p-2411	O
.	O
marbach	O
,	O
p.	O
,	O
tsitsiklis	O
,	O
j.	O
n.	O
(	O
2001	O
)	O
.	O
simulation-based	O
optimization	O
of	O
markov	O
reward	O
processes	O
.	O
ieee	O
transactions	O
on	O
automatic	O
control	O
,	O
46	O
(	O
2	O
)	O
:191–209	O
.	O
markram	O
,	O
h.	O
,	O
l¨ubke	O
,	O
j.	O
,	O
frotscher	O
,	O
m.	O
,	O
sakmann	O
,	O
b	O
.	O
(	O
1997	O
)	O
.	O
regulation	O
of	O
synaptic	O
eﬃcacy	O
by	O
coincidence	O
of	O
postsynaptic	O
aps	O
and	O
epsps	O
.	O
science	O
,	O
275	O
(	O
5297	O
)	O
:213–215	O
.	O
mart´ınez	O
,	O
j.	O
f.	O
,	O
˙ipek	O
,	O
e.	O
(	O
2009	O
)	O
.	O
dynamic	O
multicore	O
resource	O
management	O
:	O
a	O
machine	O
learning	O
approach	O
.	O
micro	O
,	O
ieee	O
,	O
29	O
(	O
5	O
)	O
:8–17	O
.	O
mataric	O
,	O
m.	O
j	O
.	O
(	O
1994	O
)	O
.	O
reward	O
functions	O
for	O
accelerated	O
learning	O
.	O
in	O
proceedings	O
of	O
the	O
11th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
1994	O
)	O
,	O
pp	O
.	O
181–189	O
.	O
morgan	O
kaufmann	O
.	O
matsuda	O
,	O
w.	O
,	O
furuta	O
,	O
t.	O
,	O
nakamura	O
,	O
k.	O
c.	O
,	O
hioki	O
,	O
h.	O
,	O
fujiyama	O
,	O
f.	O
,	O
arai	O
,	O
r.	O
,	O
kaneko	O
,	O
t.	O
single	O
nigrostriatal	O
dopaminergic	O
neurons	O
form	O
widely	O
spread	O
and	O
highly	O
dense	O
(	O
2009	O
)	O
.	O
axonal	O
arborizations	O
in	O
the	O
neostriatum	O
.	O
the	O
journal	O
of	O
neuroscience	O
,	O
29	O
(	O
2	O
)	O
:444–453	O
.	O
mazur	O
,	O
j.	O
e.	O
(	O
1994	O
)	O
.	O
learning	O
and	O
behavior	O
,	O
3rd	O
ed	O
.	O
prentice-hall	O
,	O
englewood	O
cliﬀs	O
,	O
nj	O
.	O
mccallum	O
,	O
a.	O
k.	O
(	O
1993	O
)	O
.	O
overcoming	O
incomplete	O
perception	O
with	O
utile	O
distinction	O
memory	O
.	O
in	O
proceedings	O
of	O
the	O
10th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
1993	O
)	O
,	O
pp	O
.	O
190–196	O
.	O
morgan	O
kaufmann	O
.	O
mccallum	O
,	O
a.	O
k.	O
(	O
1995	O
)	O
.	O
reinforcement	B
learning	I
with	O
selective	O
perception	O
and	O
hidden	O
state	B
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
rochester	O
,	O
rochester	O
ny	O
.	O
mccloskey	O
,	O
m.	O
,	O
cohen	O
,	O
n.	O
j	O
.	O
(	O
1989	O
)	O
.	O
catastrophic	B
interference	I
in	O
connectionist	O
networks	O
:	O
the	O
sequential	O
learning	O
problem	O
.	O
psychology	B
of	O
learning	O
and	O
motivation	B
,	O
24	O
:109–165	O
.	O
mcclure	O
,	O
s.	O
m.	O
,	O
daw	O
,	O
n.	O
d.	O
,	O
montague	O
,	O
p.	O
r.	O
(	O
2003	O
)	O
.	O
a	O
computational	O
substrate	O
for	O
incentive	O
salience	O
.	O
trends	O
in	O
neurosciences	O
,	O
26	O
(	O
8	O
)	O
:423–428	O
.	O
mcculloch	O
,	O
w.	O
s.	O
,	O
pitts	O
,	O
w.	O
(	O
1943	O
)	O
.	O
a	O
logical	O
calculus	O
of	O
the	O
ideas	O
immanent	O
in	O
nervous	O
activity	O
.	O
bulletin	O
of	O
mathematical	O
biophysics	O
,	O
5	O
(	O
4	O
)	O
:115–133	O
.	O
mcmahan	O
,	O
h.	O
b.	O
,	O
gordon	O
,	O
g.	O
j	O
.	O
(	O
2005	O
)	O
.	O
fast	O
exact	O
planning	B
in	O
markov	O
decision	O
processes	O
.	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
automated	O
planning	B
and	O
scheduling	O
,	O
pp	O
.	O
151-	O
160.	O
melo	O
,	O
f.	O
s.	O
,	O
meyn	O
,	O
s.	O
p.	O
,	O
ribeiro	O
,	O
m.	O
i	O
.	O
(	O
2008	O
)	O
.	O
an	O
analysis	O
of	O
reinforcement	O
learning	O
with	O
function	B
approximation	I
.	O
in	O
proceedings	O
of	O
the	O
25th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
2008	O
)	O
,	O
pp	O
.	O
664–671	O
.	O
mendel	O
,	O
j.	O
m.	O
(	O
1966	O
)	O
.	O
a	O
survey	O
of	O
learning	O
control	B
systems	O
.	O
isa	O
transactions	O
,	O
5:297–303	O
.	O
mendel	O
,	O
j.	O
m.	O
,	O
mclaren	O
,	O
r.	O
w.	O
(	O
1970	O
)	O
.	O
reinforcement	B
learning	I
control	O
and	O
pattern	O
recognition	O
systems	O
.	O
in	O
j.	O
m.	O
mendel	O
and	O
k.	O
s.	O
fu	O
(	O
eds	O
.	O
)	O
,	O
adaptive	O
,	O
learning	O
and	O
pattern	O
recognition	O
systems	O
:	O
theory	O
and	O
applications	O
,	O
pp	O
.	O
287–318	O
.	O
academic	O
press	O
,	O
new	O
york	O
.	O
references	O
505	O
michie	O
,	O
d.	O
(	O
1961	O
)	O
.	O
trial	O
and	O
error	O
.	O
in	O
s.	O
a.	O
barnett	O
and	O
a.	O
mclaren	O
(	O
eds	O
.	O
)	O
,	O
science	O
survey	O
,	O
part	O
2	O
,	O
pp	O
.	O
129–145	O
.	O
penguin	O
,	O
harmondsworth	O
.	O
michie	O
,	O
d.	O
(	O
1963	O
)	O
.	O
experiments	O
on	O
the	O
mechanisation	O
of	O
game	O
learning	O
.	O
1.	O
characterization	O
of	O
the	O
model	O
and	O
its	O
parameters	O
.	O
the	O
computer	O
journal	O
,	O
6	O
(	O
3	O
)	O
:232–263	O
.	O
michie	O
,	O
d.	O
(	O
1974	O
)	O
.	O
on	O
machine	O
intelligence	O
.	O
edinburgh	O
university	O
press	O
,	O
edinburgh	O
.	O
michie	O
,	O
d.	O
,	O
chambers	O
,	O
r.	O
a	O
.	O
(	O
1968	O
)	O
.	O
boxes	O
,	O
an	O
experiment	O
in	O
adaptive	O
control	B
.	O
in	O
e.	O
dale	O
and	O
d.	O
michie	O
(	O
eds	O
.	O
)	O
,	O
machine	O
intelligence	O
2	O
,	O
pp	O
.	O
137–152	O
.	O
oliver	O
and	O
boyd	O
,	O
edinburgh	O
.	O
miller	O
,	O
r.	O
(	O
1981	O
)	O
.	O
meaning	O
and	O
purpose	O
in	O
the	O
intact	O
brain	O
:	O
a	O
philosophical	O
,	O
psychological	O
,	O
and	O
biological	O
account	O
of	O
conscious	O
process	O
.	O
clarendon	O
press	O
,	O
oxford	O
.	O
miller	O
,	O
w.	O
t.	O
,	O
an	O
,	O
e.	O
,	O
glanz	O
,	O
f.	O
,	O
carter	O
,	O
m.	O
(	O
1990	O
)	O
.	O
the	O
design	B
of	I
cmac	O
neural	B
networks	I
for	O
control	B
.	O
adaptive	O
and	O
learning	O
systems	O
,	O
1	O
:140–145	O
.	O
miller	O
,	O
w.	O
t.	O
,	O
glanz	O
,	O
f.	O
h.	O
(	O
1996	O
)	O
.	O
unh	O
cmac	O
verison	O
2.1	O
:	O
the	O
university	O
of	O
new	O
hampshire	O
implementation	O
of	O
the	O
cerebellar	O
model	O
arithmetic	O
computer	O
-	O
cmac	O
.	O
robotics	O
laboratory	O
technical	O
report	O
,	O
university	O
of	O
new	O
hampshire	O
,	O
durham	O
.	O
miller	O
,	O
s.	O
,	O
williams	O
,	O
r.	O
j	O
.	O
(	O
1992	O
)	O
.	O
learning	O
to	O
control	B
a	O
bioreactor	O
using	O
a	O
neural	B
net	O
dyna-q	O
system	O
.	O
in	O
proceedings	O
of	O
the	O
seventh	O
yale	O
workshop	O
on	O
adaptive	O
and	O
learning	O
systems	O
,	O
pp	O
.	O
167–172	O
.	O
center	O
for	O
systems	O
science	O
,	O
dunham	O
laboratory	O
,	O
yale	O
university	O
,	O
new	O
haven	O
.	O
miller	O
,	O
w.	O
t.	O
,	O
scalera	O
,	O
s.	O
m.	O
,	O
kim	O
,	O
a	O
.	O
(	O
1994	O
)	O
.	O
neural	B
network	O
control	B
of	O
dynamic	O
balance	O
for	O
a	O
biped	O
walking	O
robot	O
.	O
in	O
proceedings	O
of	O
the	O
eighth	O
yale	O
workshop	O
on	O
adaptive	O
and	O
learning	O
systems	O
,	O
pp	O
.	O
156–161	O
.	O
center	O
for	O
systems	O
science	O
,	O
dunham	O
laboratory	O
,	O
yale	O
university	O
,	O
new	O
haven	O
.	O
minton	O
,	O
s.	O
(	O
1990	O
)	O
.	O
quantitative	O
results	O
concerning	O
the	O
utility	O
of	O
explanation-based	O
learning	O
.	O
artiﬁcial	B
intelligence	I
,	O
42	O
(	O
2-3	O
)	O
:363–391	O
.	O
minsky	O
,	O
m.	O
l.	O
(	O
1954	O
)	O
.	O
theory	O
of	O
neural-analog	O
reinforcement	O
systems	O
and	O
its	O
application	O
to	O
the	O
brain-model	O
problem	O
.	O
ph.d.	O
thesis	O
,	O
princeton	O
university	O
.	O
minsky	O
,	O
m.	O
l.	O
(	O
1961	O
)	O
.	O
steps	O
toward	O
artiﬁcial	B
intelligence	I
.	O
proceedings	O
of	O
the	O
institute	O
of	O
radio	O
engineers	O
,	O
49:8–30	O
.	O
reprinted	O
in	O
e.	O
a.	O
feigenbaum	O
and	O
j.	O
feldman	O
(	O
eds	O
.	O
)	O
,	O
computers	O
and	O
thought	O
,	O
pp	O
.	O
406–450	O
.	O
mcgraw-hill	O
,	O
new	O
york	O
,	O
1963.	O
minsky	O
,	O
m.	O
l.	O
(	O
1967	O
)	O
.	O
computation	O
:	O
finite	O
and	O
inﬁnite	O
machines	O
.	O
prentice-hall	O
,	O
englewood	O
cliﬀs	O
,	O
nj	O
.	O
mnih	O
,	O
v.	O
,	O
kavukcuoglu	O
,	O
k.	O
,	O
silver	O
,	O
d.	O
,	O
graves	O
,	O
a.	O
,	O
antonoglou	O
,	O
i.	O
,	O
wierstra	O
,	O
d.	O
,	O
riedmiller	O
,	O
m.	O
(	O
2013	O
)	O
.	O
playing	O
atari	O
with	O
deep	O
reinforcement	B
learning	I
.	O
arxiv	O
preprint	O
arxiv:1312.5602.	O
mnih	O
,	O
v.	O
,	O
kavukcuoglu	O
,	O
k.	O
,	O
silver	O
,	O
d.	O
,	O
rusu	O
,	O
a.	O
a.	O
,	O
veness	O
,	O
j.	O
,	O
bellemare	O
,	O
m.	O
g.	O
,	O
graves	O
,	O
a.	O
,	O
riedmiller	O
,	O
m.	O
,	O
fidjeland	O
,	O
a.	O
k.	O
,	O
ostrovski	O
,	O
g.	O
,	O
petersen	O
,	O
s.	O
,	O
beattie	O
,	O
c.	O
,	O
sadik	O
,	O
a.	O
,	O
antonoglou	O
,	O
i.	O
,	O
king	O
,	O
h.	O
,	O
kumaran	O
,	O
d.	O
,	O
wierstra	O
,	O
d.	O
,	O
legg	O
,	O
s.	O
,	O
hassabis	O
,	O
d.	O
(	O
2015	O
)	O
.	O
human-	O
level	O
control	B
through	O
deep	B
reinforcement	I
learning	I
.	O
nature	O
,	O
518	O
(	O
7540	O
)	O
:529–533	O
.	O
modayil	O
,	O
j.	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
2014	O
)	O
.	O
prediction	B
driven	O
behavior	O
:	O
learning	O
predictions	O
that	O
drive	O
ﬁxed	O
responses	O
.	O
in	O
aaai-14	O
workshop	O
on	O
artiﬁcial	B
intelligence	I
and	O
robotics	O
,	O
quebec	O
city	O
,	O
canada	O
.	O
modayil	O
,	O
j.	O
,	O
white	O
,	O
a.	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
2014	O
)	O
.	O
multi-timescale	O
nexting	O
in	O
a	O
reinforcement	B
learning	I
robot	O
.	O
adaptive	O
behavior	O
,	O
22	O
(	O
2	O
)	O
:146–160	O
.	O
monahan	O
,	O
g.	O
e.	O
(	O
1982	O
)	O
.	O
state	B
of	O
the	O
art—a	O
survey	O
of	O
partially	O
observable	O
markov	O
decision	O
processes	O
:	O
theory	O
,	O
models	O
,	O
and	O
algorithms	O
.	O
management	O
science	O
,	O
28	O
(	O
1	O
)	O
:1–16	O
.	O
montague	O
,	O
p.	O
r.	O
,	O
dayan	O
,	O
p.	O
,	O
nowlan	O
,	O
s.	O
j.	O
,	O
pouget	O
,	O
a.	O
,	O
sejnowski	O
,	O
t.	O
j	O
.	O
(	O
1993	O
)	O
.	O
using	O
aperiodic	O
in	O
advances	O
in	O
neural	O
reinforcement	O
for	O
directed	O
self-organization	O
during	O
development	O
.	O
information	O
processing	O
systems	O
5	O
(	O
nips	O
1992	O
)	O
,	O
pp	O
.	O
969–976	O
.	O
morgan	O
kaufmann	O
.	O
montague	O
,	O
p.	O
r.	O
,	O
dayan	O
,	O
p.	O
,	O
person	O
,	O
c.	O
,	O
sejnowski	O
,	O
t.	O
j	O
.	O
(	O
1995	O
)	O
.	O
bee	O
foraging	O
in	O
uncertain	O
506	O
references	O
environments	O
using	O
predictive	O
hebbian	O
learning	O
.	O
nature	O
,	O
377	O
(	O
6551	O
)	O
:725–728	O
.	O
montague	O
,	O
p.	O
r.	O
,	O
dayan	O
,	O
p.	O
,	O
sejnowski	O
,	O
t.	O
j	O
.	O
(	O
1996	O
)	O
.	O
a	O
framework	O
for	O
mesencephalic	O
dopamine	B
systems	O
based	O
on	O
predictive	O
hebbian	O
learning	O
.	O
the	O
journal	O
of	O
neuroscience	O
,	O
16	O
(	O
5	O
)	O
:1936–	O
1947.	O
montague	O
,	O
p.	O
r.	O
,	O
dolan	O
,	O
r.	O
j.	O
,	O
friston	O
,	O
k.	O
j.	O
,	O
dayan	O
,	O
p.	O
(	O
2012	O
)	O
.	O
computational	O
psychiatry	O
.	O
trends	O
in	O
cognitive	O
sciences	O
,	O
16	O
(	O
1	O
)	O
:72–80	O
.	O
montague	O
,	O
p.	O
r.	O
,	O
sejnowski	O
,	O
t.	O
j	O
.	O
(	O
1994	O
)	O
.	O
the	O
predictive	O
brain	O
:	O
temporal	O
coincidence	O
and	O
temporal	O
order	O
in	O
synaptic	O
learningmechanisms	O
.	O
learning	O
&	O
memory	O
,	O
1	O
(	O
1	O
)	O
:1–33	O
.	O
moore	O
,	O
a.	O
w.	O
(	O
1990	O
)	O
.	O
eﬃcient	O
memory-based	O
learning	O
for	O
robot	O
control	B
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
cambridge	O
.	O
moore	O
,	O
a.	O
w.	O
,	O
atkeson	O
,	O
c.	O
g.	O
(	O
1993	O
)	O
.	O
prioritized	B
sweeping	I
:	O
reinforcement	B
learning	I
with	O
less	O
data	O
and	O
less	O
real	O
time	O
.	O
machine	O
learning	O
,	O
13	O
(	O
1	O
)	O
:103–130	O
.	O
moore	O
,	O
a.	O
w.	O
,	O
schneider	O
,	O
j.	O
,	O
deng	O
,	O
k.	O
(	O
1997	O
)	O
.	O
eﬃcient	O
locally	O
weighted	O
polynomial	O
regres-	O
sion	O
predictions	O
.	O
in	O
proceedings	O
of	O
the	O
14th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
1997	O
)	O
.	O
morgan	O
kaufmann	O
.	O
moore	O
,	O
j.	O
w.	O
,	O
blazis	O
,	O
d.	O
e.	O
j	O
.	O
(	O
1989	O
)	O
.	O
simulation	O
of	O
a	O
classically	O
conditioned	O
response	O
:	O
a	O
in	O
j.	O
h.	O
byrne	O
and	O
w.	O
o.	O
cerebellar	O
implementation	O
of	O
the	O
sutton-barto-desmond	O
model	O
.	O
berry	O
(	O
eds	O
.	O
)	O
,	O
neural	B
models	O
of	O
plasticity	O
,	O
pp	O
.	O
187–207	O
.	O
academic	O
press	O
,	O
san	O
diego	O
,	O
ca	O
.	O
moore	O
,	O
j.	O
w.	O
,	O
choi	O
,	O
j.-s.	O
,	O
brunzell	O
,	O
d.	O
h.	O
(	O
1998	O
)	O
.	O
predictive	O
timing	O
under	O
temporal	O
uncertainty	O
:	O
in	O
d.	O
a.	O
rosenbaum	O
and	O
c.	O
e.	O
the	O
time	O
derivative	O
model	O
of	O
the	O
conditioned	O
response	O
.	O
collyer	O
(	O
eds	O
.	O
)	O
,	O
timing	O
of	O
behavior	O
,	O
pp	O
.	O
3–34	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
moore	O
,	O
j.	O
w.	O
,	O
desmond	O
,	O
j.	O
e.	O
,	O
berthier	O
,	O
n.	O
e.	O
,	O
blazis	O
,	O
e.	O
j.	O
,	O
sutton	O
,	O
r.	O
s.	O
,	O
barto	O
,	O
a.	O
g.	O
(	O
1986	O
)	O
.	O
simulation	O
of	O
the	O
classically	O
conditioned	O
nictitating	O
membrane	O
response	O
by	O
a	O
neuron-	O
like	O
adaptive	O
element	O
:	O
i.	O
response	O
topography	O
,	O
neuronal	O
ﬁring	O
,	O
and	O
interstimulus	O
intervals	O
.	O
behavioural	O
brain	O
research	O
,	O
21	O
(	O
2	O
)	O
:143–154	O
.	O
moore	O
,	O
j.	O
w.	O
,	O
marks	O
,	O
j.	O
s.	O
,	O
castagna	O
,	O
v.	O
e.	O
,	O
polewan	O
,	O
r.	O
j	O
.	O
(	O
2001	O
)	O
.	O
parameter	O
stability	O
in	O
the	O
td	O
model	O
of	O
complex	O
cr	O
topographies	O
.	O
in	O
society	O
for	O
neuroscience	O
abstracts	O
,	O
27:642.	O
moore	O
,	O
j.	O
w.	O
,	O
schmajuk	O
,	O
n.	O
a	O
.	O
(	O
2008	O
)	O
.	O
kamin	O
blocking	B
.	O
scholarpedia	O
,	O
3	O
(	O
5	O
)	O
:3542.	O
moore	O
,	O
j.	O
w.	O
,	O
stickney	O
,	O
k.	O
j	O
.	O
(	O
1980	O
)	O
.	O
formation	O
of	O
attentional-associative	O
networks	O
in	O
real	O
time	O
:	O
role	O
of	O
the	O
hippocampus	O
and	O
implications	O
for	O
conditioning	O
.	O
physiological	O
psychology	B
,	O
8	O
(	O
2	O
)	O
:207–217	O
.	O
mukundan	O
,	O
j.	O
,	O
mart´ınez	O
,	O
j.	O
f.	O
(	O
2012	O
)	O
.	O
morse	O
,	O
multi-objective	O
reconﬁgurable	O
self-optimizing	O
memory	O
scheduler	O
.	O
in	O
ieee	O
18th	O
international	O
symposium	O
on	O
high	O
performance	O
computer	O
architecture	O
(	O
hpca	O
)	O
,	O
pp	O
.	O
1–12	O
.	O
m¨uller	O
,	O
m.	O
(	O
2002	O
)	O
.	O
computer	O
go	O
.	O
artiﬁcial	B
intelligence	I
,	O
134	O
(	O
1	O
)	O
:145–179	O
.	O
munos	O
,	O
r.	O
,	O
stepleton	O
,	O
t.	O
,	O
harutyunyan	O
,	O
a.	O
,	O
bellemare	O
,	O
m.	O
(	O
2016	O
)	O
.	O
safe	O
and	O
eﬃcient	O
oﬀ-policy	B
reinforcement	O
learning	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
29	O
(	O
nips	O
2016	O
)	O
,	O
pp	O
.	O
1046–1054	O
.	O
curran	O
associates	O
,	O
inc.	O
naddaf	O
,	O
y	O
.	O
(	O
2010	O
)	O
.	O
game-independent	O
ai	O
agents	O
for	O
playing	O
atari	O
2600	O
console	O
games	O
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
alberta	O
,	O
edmonton	O
.	O
narendra	O
,	O
k.	O
s.	O
,	O
thathachar	O
,	O
m.	O
a.	O
l.	O
(	O
1974	O
)	O
.	O
learning	O
automata—a	O
survey	O
.	O
ieee	O
transac-	O
tions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
4:323–334	O
.	O
narendra	O
,	O
k.	O
s.	O
,	O
thathachar	O
,	O
m.	O
a.	O
l.	O
(	O
1989	O
)	O
.	O
learning	B
automata	I
:	O
an	O
introduction	O
.	O
prentice-	O
hall	O
,	O
englewood	O
cliﬀs	O
,	O
nj	O
.	O
narendra	O
,	O
k.	O
s.	O
,	O
wheeler	O
,	O
r.	O
m.	O
(	O
1983	O
)	O
.	O
an	O
n-player	O
sequential	O
stochastic	O
game	O
with	O
identical	O
payoﬀs	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
6:1154–1158	O
.	O
narendra	O
,	O
k.	O
s.	O
,	O
wheeler	O
,	O
r.	O
m.	O
(	O
1986	O
)	O
.	O
decentralized	O
learning	O
in	O
ﬁnite	O
markov	O
chains	O
.	O
ieee	O
references	O
507	O
transactions	O
on	O
automatic	O
control	O
,	O
31	O
(	O
6	O
)	O
:519–526	O
.	O
nedi´c	O
,	O
a.	O
,	O
bertsekas	O
,	O
d.	O
p.	O
(	O
2003	O
)	O
.	O
least	O
squares	O
policy	B
evaluation	I
algorithms	O
with	O
linear	O
function	B
approximation	I
.	O
discrete	O
event	O
dynamic	O
systems	O
,	O
13	O
(	O
1-2	O
)	O
:79–110	O
.	O
shaping	B
and	O
policy	B
search	O
in	O
reinforcement	O
learning	O
.	O
ng	O
,	O
a.	O
y	O
.	O
(	O
2003	O
)	O
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
california	O
,	O
berkeley	O
.	O
ng	O
,	O
a.	O
y.	O
,	O
harada	O
,	O
d.	O
,	O
russell	O
,	O
s.	O
(	O
1999	O
)	O
.	O
policy	B
invariance	O
under	O
reward	O
transformations	O
:	O
theory	O
and	O
application	O
to	O
reward	O
shaping	O
.	O
in	O
i.	O
bratko	O
and	O
s.	O
dzeroski	O
(	O
eds	O
.	O
)	O
,	O
proceedings	O
of	O
the	O
16th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
1999	O
)	O
,	O
pp	O
.	O
278–287	O
.	O
ng	O
,	O
a.	O
y.	O
,	O
russell	O
,	O
s.	O
j	O
.	O
(	O
2000	O
)	O
.	O
algorithms	O
for	O
inverse	O
reinforcement	B
learning	I
.	O
in	O
proceedings	O
of	O
the	O
17th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
2000	O
)	O
,	O
pp	O
.	O
663–670	O
.	O
niv	O
,	O
y	O
.	O
(	O
2009	O
)	O
.	O
reinforcement	B
learning	I
in	O
the	O
brain	O
.	O
journal	O
of	O
mathematical	O
psychology	B
,	O
53	O
(	O
3	O
)	O
:139–154	O
.	O
niv	O
,	O
y.	O
,	O
daw	O
,	O
n.	O
d.	O
,	O
dayan	O
,	O
p.	O
(	O
2006	O
)	O
.	O
how	O
fast	O
to	O
work	O
:	O
response	O
vigor	O
,	O
motivation	B
and	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
18	O
(	O
nips	O
2005	O
)	O
,	O
tonic	O
dopamine	B
.	O
pp	O
.	O
1019–1026	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
niv	O
,	O
y.	O
,	O
daw	O
,	O
n.	O
d.	O
,	O
joel	O
,	O
d.	O
,	O
dayan	O
,	O
p.	O
(	O
2007	O
)	O
.	O
tonic	O
dopamine	B
:	O
opportunity	O
costs	O
and	O
the	O
control	O
of	O
response	O
vigor	O
.	O
psychopharmacology	O
,	O
191	O
(	O
3	O
)	O
:507–520	O
.	O
niv	O
,	O
y.	O
,	O
joel	O
,	O
d.	O
,	O
dayan	O
,	O
p.	O
(	O
2006	O
)	O
.	O
a	O
normative	O
perspective	O
on	O
motivation	B
.	O
trends	O
in	O
cognitive	O
sciences	O
,	O
10	O
(	O
8	O
)	O
:375–381	O
.	O
nouri	O
,	O
a.	O
,	O
littman	O
,	O
m.	O
l.	O
(	O
2009	O
)	O
.	O
multi-resolution	O
exploration	O
in	O
continuous	O
spaces	O
.	O
in	O
ad-	O
vances	O
in	O
neural	O
information	O
processing	O
systems	O
21	O
(	O
nips	O
2008	O
)	O
,	O
pp	O
.	O
1209–1216	O
.	O
curran	O
associates	O
,	O
inc.	O
now´e	O
,	O
a.	O
,	O
vrancx	O
,	O
p.	O
,	O
hauwere	O
,	O
y.-m.	O
d.	O
(	O
2012	O
)	O
.	O
game	B
theory	I
and	O
multi-agent	O
reinforcement	B
learning	I
.	O
in	O
m.	O
wiering	O
and	O
m.	O
van	O
otterlo	O
(	O
eds	O
.	O
)	O
,	O
reinforcement	B
learning	I
:	O
state-of-the-	O
art	O
,	O
pp	O
.	O
441–467	O
.	O
springer-verlag	O
berlin	O
heidelberg	O
.	O
nutt	O
,	O
d.	O
j.	O
,	O
lingford-hughes	O
,	O
a.	O
,	O
erritzoe	O
,	O
d.	O
,	O
stokes	O
,	O
p.	O
r.	O
a	O
.	O
(	O
2015	O
)	O
.	O
the	O
dopamine	B
theory	O
of	O
addiction	O
:	O
40	O
years	O
of	O
highs	O
and	O
lows	O
.	O
nature	O
reviews	O
neuroscience	B
,	O
16	O
(	O
5	O
)	O
:305–312	O
.	O
o	O
’	O
doherty	O
,	O
j.	O
p.	O
,	O
dayan	O
,	O
p.	O
,	O
friston	O
,	O
k.	O
,	O
critchley	O
,	O
h.	O
,	O
dolan	O
,	O
r.	O
j	O
.	O
(	O
2003	O
)	O
.	O
temporal	O
diﬀerence	O
models	O
and	O
reward-related	O
learning	O
in	O
the	O
human	O
brain	O
.	O
neuron	O
,	O
38	O
(	O
2	O
)	O
:329–337	O
.	O
o	O
’	O
doherty	O
,	O
j.	O
p.	O
,	O
dayan	O
,	O
p.	O
,	O
schultz	O
,	O
j.	O
,	O
deichmann	O
,	O
r.	O
,	O
friston	O
,	O
k.	O
,	O
dolan	O
,	O
r.	O
j	O
.	O
(	O
2004	O
)	O
.	O
science	O
,	O
dissociable	O
roles	O
of	O
ventral	O
and	O
dorsal	O
striatum	O
in	O
instrumental	O
conditioning	B
.	O
304	O
(	O
5669	O
)	O
:452–454	O
.	O
´olafsd´ottir	O
,	O
h.	O
f.	O
,	O
barry	O
,	O
c.	O
,	O
saleem	O
,	O
a.	O
b.	O
,	O
hassabis	O
,	O
d.	O
,	O
spiers	O
,	O
h.	O
j	O
.	O
(	O
2015	O
)	O
.	O
hippocampal	O
place	O
cells	O
construct	O
reward	O
related	O
sequences	O
through	O
unexplored	O
space	O
.	O
elife	O
,	O
4	O
:	O
e06063	O
.	O
oh	O
,	O
j.	O
,	O
guo	O
,	O
x.	O
,	O
lee	O
,	O
h.	O
,	O
lewis	O
,	O
r.	O
l.	O
,	O
singh	O
,	O
s.	O
(	O
2015	O
)	O
.	O
action-conditional	O
video	O
prediction	B
using	O
deep	O
networks	O
in	O
atari	O
games	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
28	O
(	O
nips	O
2015	O
)	O
,	O
pp	O
.	O
2845–2853	O
.	O
curran	O
associates	O
,	O
inc.	O
olds	O
,	O
j.	O
,	O
milner	O
,	O
p.	O
(	O
1954	O
)	O
.	O
positive	O
reinforcement	O
produced	O
by	O
electrical	O
stimulation	O
of	O
the	O
sep-	O
tal	O
area	O
and	O
other	O
regions	O
of	O
rat	O
brain	O
.	O
journal	O
of	O
comparative	O
and	O
physiological	O
psychology	B
,	O
47	O
(	O
6	O
)	O
:419–427	O
.	O
o	O
’	O
reilly	O
,	O
r.	O
c.	O
,	O
frank	O
,	O
m.	O
j	O
.	O
(	O
2006	O
)	O
.	O
making	O
working	O
memory	O
work	O
:	O
a	O
computational	O
model	O
of	O
learning	O
in	O
the	O
prefrontal	O
cortex	O
and	O
basal	O
ganglia	O
.	O
neural	B
computation	O
,	O
18	O
(	O
2	O
)	O
:283–328	O
.	O
o	O
’	O
reilly	O
,	O
r.	O
c.	O
,	O
frank	O
,	O
m.	O
j.	O
,	O
hazy	O
,	O
t.	O
e.	O
,	O
watz	O
,	O
b	O
.	O
(	O
2007	O
)	O
.	O
pvlv	O
,	O
the	O
primary	O
value	B
and	O
learned	O
value	B
pavlovian	O
learning	O
algorithm	O
.	O
behavioral	O
neuroscience	B
,	O
121	O
(	O
1	O
)	O
:31–49	O
.	O
omohundro	O
,	O
s.	O
m.	O
(	O
1987	O
)	O
.	O
eﬃcient	O
algorithms	O
with	O
neural	O
network	O
behavior	O
.	O
technical	O
report	O
,	O
department	O
of	O
computer	O
science	O
,	O
university	O
of	O
illinois	O
at	O
urbana-champaign	O
.	O
ormoneit	O
,	O
d.	O
,	O
sen	O
,	O
´s	O
.	O
(	O
2002	O
)	O
.	O
kernel-based	O
reinforcement	O
learning	O
.	O
machine	O
learning	O
,	O
49	O
(	O
2-	O
508	O
3	O
)	O
:161–178	O
.	O
references	O
oudeyer	O
,	O
p.-y.	O
,	O
kaplan	O
,	O
f.	O
(	O
2007	O
)	O
.	O
what	O
is	O
intrinsic	B
motivation	O
?	O
a	O
typology	O
of	O
computational	O
approaches	O
.	O
frontiers	O
in	O
neurorobotics	O
,	O
1:6.	O
oudeyer	O
,	O
p.-y.	O
,	O
kaplan	O
,	O
f.	O
,	O
hafner	O
,	O
v.	O
v.	O
(	O
2007	O
)	O
.	O
intrinsic	B
motivation	O
systems	O
for	O
autonomous	O
mental	O
development	O
.	O
ieee	O
transactions	O
on	O
evolutionary	O
computation	O
,	O
11	O
(	O
2	O
)	O
:265–286	O
.	O
padoa-schioppa	O
,	O
c.	O
,	O
assad	O
,	O
j.	O
a	O
.	O
(	O
2006	O
)	O
.	O
neurons	O
in	O
the	O
orbitofrontal	O
cortex	O
encode	O
economic	O
value	B
.	O
nature	O
,	O
441	O
(	O
7090	O
)	O
:223–226	O
.	O
page	O
,	O
c.	O
v.	O
(	O
1977	O
)	O
.	O
heuristics	O
for	O
signature	O
table	O
analysis	O
as	O
a	O
pattern	O
recognition	O
technique	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
7	O
(	O
2	O
)	O
:77–86	O
.	O
pagnoni	O
,	O
g.	O
,	O
zink	O
,	O
c.	O
f.	O
,	O
montague	O
,	O
p.	O
r.	O
,	O
berns	O
,	O
g.	O
s.	O
(	O
2002	O
)	O
.	O
activity	O
in	O
human	O
ventral	O
striatum	O
locked	O
to	O
errors	O
of	O
reward	O
prediction	B
.	O
nature	O
neuroscience	B
,	O
5	O
(	O
2	O
)	O
:97–98	O
.	O
pan	O
,	O
w.-x.	O
,	O
schmidt	O
,	O
r.	O
,	O
wickens	O
,	O
j.	O
r.	O
,	O
hyland	O
,	O
b.	O
i	O
.	O
(	O
2005	O
)	O
.	O
dopamine	B
cells	O
respond	O
to	O
predicted	O
events	O
during	O
classical	B
conditioning	I
:	O
evidence	O
for	O
eligibility	O
traces	O
in	O
the	O
reward-	O
learning	O
network	O
.	O
the	O
journal	O
of	O
neuroscience	O
,	O
25	O
(	O
26	O
)	O
:6235–6242	O
.	O
park	O
,	O
j.	O
,	O
kim	O
,	O
j.	O
,	O
kang	O
,	O
d.	O
(	O
2005	O
)	O
.	O
an	O
rls-based	O
natural	O
actor–critic	B
algorithm	O
for	O
locomotion	O
of	O
a	O
two-linked	O
robot	O
arm	O
.	O
computational	O
intelligence	O
and	O
security:65–72	O
.	O
parks	O
,	O
p.	O
c.	O
,	O
militzer	O
,	O
j	O
.	O
(	O
1991	O
)	O
.	O
improved	O
allocation	O
of	O
weights	O
for	O
associative	O
memory	O
storage	O
in	O
learning	O
control	B
systems	O
.	O
in	O
ifac	O
design	O
methods	O
of	O
control	O
systems	O
,	O
zurich	O
,	O
switzer-	O
land	O
,	O
pp	O
.	O
507–512	O
.	O
parr	O
,	O
r.	O
(	O
1988	O
)	O
.	O
hierarchical	O
control	O
and	O
learning	O
for	O
markov	O
decision	O
processes	O
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
california	O
,	O
berkeley	O
.	O
parr	O
,	O
r.	O
,	O
russell	O
,	O
s.	O
(	O
1995	O
)	O
.	O
approximating	O
optimal	O
policies	O
for	O
partially	O
observable	O
stochastic	O
in	O
proceedings	O
of	O
the	O
fourteenth	O
international	O
joint	O
conference	O
on	O
artiﬁcial	O
domains	O
.	O
intelligence	O
,	O
pp	O
.	O
1088–1094	O
.	O
morgan	O
kaufmann	O
.	O
pavlov	O
,	O
p.	O
i	O
.	O
(	O
1927	O
)	O
.	O
conditioned	O
reﬂexes	O
.	O
oxford	O
university	O
press	O
,	O
london	O
.	O
pawlak	O
,	O
v.	O
,	O
kerr	O
,	O
j.	O
n.	O
d.	O
(	O
2008	O
)	O
.	O
dopamine	B
receptor	O
activation	O
is	O
required	O
for	O
corticostriatal	O
spike-timing-dependent	O
plasticity	O
.	O
the	O
journal	O
of	O
neuroscience	O
,	O
28	O
(	O
10	O
)	O
:2435–2446	O
.	O
pawlak	O
,	O
v.	O
,	O
wickens	O
,	O
j.	O
r.	O
,	O
kirkwood	O
,	O
a.	O
,	O
kerr	O
,	O
j.	O
n.	O
d.	O
(	O
2010	O
)	O
.	O
timing	O
is	O
not	O
every-	O
thing	O
:	O
neuromodulation	O
opens	O
the	O
stdp	O
gate	O
.	O
frontiers	O
in	O
synaptic	O
neuroscience	B
,	O
2:146.	O
doi:10.3389/fnsyn.2010.00146	O
.	O
pearce	O
,	O
j.	O
m.	O
,	O
hall	O
,	O
g.	O
(	O
1980	O
)	O
.	O
a	O
model	O
for	O
pavlovian	O
learning	O
:	O
variation	O
in	O
the	O
eﬀectiveness	O
of	O
conditioning	O
but	O
not	O
unconditioned	O
stimuli	O
.	O
psychological	O
review	O
,	O
87	O
(	O
6	O
)	O
:532–552	O
.	O
pearl	O
,	O
j	O
.	O
(	O
1984	O
)	O
.	O
heuristics	O
:	O
intelligent	O
search	O
strategies	O
for	O
computer	O
problem	O
solving	O
.	O
addison-wesley	O
,	O
reading	O
,	O
ma	O
.	O
pearl	O
,	O
j	O
.	O
(	O
1995	O
)	O
.	O
causal	O
diagrams	O
for	O
empirical	O
research	O
.	O
biometrika	O
,	O
82	O
(	O
4	O
)	O
:669-688.	O
pecevski	O
,	O
d.	O
,	O
maass	O
,	O
w.	O
,	O
legenstein	O
,	O
r.	O
a	O
.	O
(	O
2008	O
)	O
.	O
theoretical	O
analysis	O
of	O
learning	O
with	O
reward-	O
modulated	O
spike-timing-dependent	O
plasticity	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
20	O
(	O
nips	O
2007	O
)	O
,	O
pp	O
.	O
881–888	O
.	O
curran	O
associates	O
,	O
inc.	O
peng	O
,	O
j	O
.	O
(	O
1993	O
)	O
.	O
eﬃcient	O
dynamic	O
programming-based	O
learning	O
for	O
control	B
.	O
ph.d.	O
thesis	O
,	O
northeastern	O
university	O
,	O
boston	O
ma	O
.	O
peng	O
,	O
j	O
.	O
(	O
1995	O
)	O
.	O
eﬃcient	O
memory-based	O
dynamic	O
programming	O
.	O
in	O
proceedings	O
of	O
the	O
12th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
1995	O
)	O
,	O
pp	O
.	O
438–446	O
.	O
peng	O
,	O
j.	O
,	O
williams	O
,	O
r.	O
j	O
.	O
(	O
1993	O
)	O
.	O
eﬃcient	O
learning	O
and	O
planning	B
within	O
the	O
dyna	O
framework	O
.	O
adaptive	O
behavior	O
,	O
1	O
(	O
4	O
)	O
:437–454	O
.	O
peng	O
,	O
j.	O
,	O
williams	O
,	O
r.	O
j	O
.	O
(	O
1994	O
)	O
.	O
in	O
proceedings	O
of	O
the	O
11th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
1994	O
)	O
,	O
pp	O
.	O
226–232	O
.	O
morgan	O
kaufmann	O
,	O
san	O
francisco	O
.	O
incremental	O
multi-step	O
q-learning	O
.	O
references	O
509	O
peng	O
,	O
j.	O
,	O
williams	O
,	O
r.	O
j	O
.	O
(	O
1996	O
)	O
.	O
incremental	O
multi-step	O
q-learning	O
.	O
machine	O
learning	O
,	O
22	O
(	O
1	O
)	O
:283–290	O
.	O
perkins	O
,	O
t.	O
j.	O
,	O
pendrith	O
,	O
m.	O
d.	O
(	O
2002	O
)	O
.	O
on	O
the	O
existence	O
of	O
ﬁxed	O
points	O
for	O
q-learning	O
and	O
sarsa	O
in	O
partially	O
observable	O
domains	O
.	O
in	O
proceedings	O
of	O
the	O
19th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
2002	O
)	O
,	O
pp	O
.	O
490–497	O
.	O
perkins	O
,	O
t.	O
j.	O
,	O
precup	O
,	O
d.	O
(	O
2003	O
)	O
.	O
a	O
convergent	O
form	O
of	O
approximate	O
policy	B
iteration	I
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
15	O
(	O
nips	O
2002	O
)	O
,	O
pp	O
.	O
1627–1634	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
peters	O
,	O
j.	O
,	O
b¨uchel	O
,	O
c.	O
(	O
2010	O
)	O
.	O
neural	B
representations	O
of	O
subjective	O
reward	O
value	O
.	O
behavioral	O
brain	O
research	O
,	O
213	O
(	O
2	O
)	O
:135–141	O
.	O
peters	O
,	O
j.	O
,	O
schaal	O
,	O
s.	O
(	O
2008	O
)	O
.	O
natural	O
actor–critic	B
.	O
neurocomputing	O
,	O
71	O
(	O
7	O
)	O
:1180–1190	O
.	O
peters	O
,	O
j.	O
,	O
vijayakumar	O
,	O
s.	O
,	O
schaal	O
,	O
s.	O
(	O
2005	O
)	O
.	O
natural	O
actor–critic	B
.	O
in	O
european	O
conference	O
on	O
machine	O
learning	O
,	O
pp	O
.	O
280–291	O
.	O
springer	O
berlin	O
heidelberg	O
.	O
pezzulo	O
,	O
g.	O
,	O
van	O
der	O
meer	O
,	O
m.	O
a.	O
a.	O
,	O
lansink	O
,	O
c.	O
s.	O
,	O
pennartz	O
,	O
c.	O
m.	O
a	O
.	O
(	O
2014	O
)	O
.	O
internally	O
generated	O
sequences	O
in	O
learning	O
and	O
executing	O
goal-directed	O
behavior	O
.	O
trends	O
in	O
cognitive	O
science	O
,	O
18	O
(	O
12	O
)	O
:647–657	O
.	O
pfeiﬀer	O
,	O
b.	O
e.	O
,	O
foster	O
,	O
d.	O
j	O
.	O
(	O
2013	O
)	O
.	O
hippocampal	O
place-cell	O
sequences	O
depict	O
future	O
paths	O
to	O
remembered	O
goals	O
.	O
nature	O
,	O
497	O
(	O
7447	O
)	O
:74–79	O
.	O
phansalkar	O
,	O
v.	O
v.	O
,	O
thathachar	O
,	O
m.	O
a.	O
l.	O
(	O
1995	O
)	O
.	O
local	O
and	O
global	O
optimization	O
algorithms	O
for	O
generalized	O
learning	B
automata	I
.	O
neural	B
computation	O
,	O
7	O
(	O
5	O
)	O
:950–973	O
.	O
poggio	O
,	O
t.	O
,	O
girosi	O
,	O
f.	O
(	O
1989	O
)	O
.	O
a	O
theory	O
of	O
networks	O
for	O
approximation	O
and	O
learning	O
.	O
a.i	O
.	O
memo	O
1140.	O
artiﬁcial	B
intelligence	I
laboratory	O
,	O
massachusetts	O
institute	O
of	O
technology	O
,	O
cambridge	O
,	O
ma	O
.	O
poggio	O
,	O
t.	O
,	O
girosi	O
,	O
f.	O
(	O
1990	O
)	O
.	O
regularization	O
algorithms	O
for	O
learning	O
that	O
are	O
equivalent	O
to	O
multilayer	O
networks	O
.	O
science	O
,	O
247	O
(	O
4945	O
)	O
:978–982	O
.	O
polyak	O
,	O
b.	O
t.	O
(	O
1990	O
)	O
.	O
new	O
stochastic	O
approximation	O
type	O
procedures	O
.	O
automat	O
.	O
i	O
telemekh	O
,	O
7	O
(	O
98-107	O
)	O
:2	O
(	O
in	O
russian	O
)	O
.	O
polyak	O
,	O
b.	O
t.	O
,	O
juditsky	O
,	O
a.	O
b	O
.	O
(	O
1992	O
)	O
.	O
acceleration	O
of	O
stochastic	O
approximation	O
by	O
averaging	O
.	O
siam	O
journal	O
on	O
control	O
and	O
optimization	O
,	O
30	O
(	O
4	O
)	O
:838–855	O
.	O
powell	O
,	O
m.	O
j.	O
d.	O
(	O
1987	O
)	O
.	O
radial	O
basis	O
functions	O
for	O
multivariate	O
interpolation	O
:	O
a	O
review	O
.	O
in	O
j.	O
c.	O
mason	O
and	O
m.	O
g.	O
cox	O
(	O
eds	O
.	O
)	O
,	O
algorithms	O
for	O
approximation	O
,	O
pp	O
.	O
143–167	O
.	O
clarendon	O
press	O
,	O
oxford	O
.	O
powell	O
,	O
w.	O
b	O
.	O
(	O
2011	O
)	O
.	O
approximate	B
dynamic	I
programming	I
:	O
solving	O
the	O
curses	O
of	O
dimension-	O
ality	O
,	O
second	O
edition	O
.	O
john	O
wiley	O
and	O
sons	O
.	O
powers	O
,	O
w.	O
t.	O
(	O
1973	O
)	O
.	O
behavior	O
:	O
the	O
control	B
of	O
perception	O
.	O
aldine	O
de	O
gruyter	O
,	O
chicago	O
.	O
2nd	O
expanded	O
edition	O
2005.	O
precup	O
,	O
d.	O
(	O
2000	O
)	O
.	O
temporal	B
abstraction	I
in	O
reinforcement	B
learning	I
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
massachusetts	O
,	O
amherst	O
.	O
precup	O
,	O
d.	O
,	O
sutton	O
,	O
r.	O
s.	O
,	O
dasgupta	O
,	O
s.	O
(	O
2001	O
)	O
.	O
oﬀ-policy	B
temporal-diﬀerence	O
learning	O
with	O
function	B
approximation	I
.	O
in	O
proceedings	O
of	O
the	O
18th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
2001	O
)	O
,	O
pp	O
.	O
417–424	O
.	O
precup	O
,	O
d.	O
,	O
sutton	O
,	O
r.	O
s.	O
,	O
paduraru	O
,	O
c.	O
,	O
koop	O
,	O
a.	O
,	O
singh	O
,	O
s.	O
(	O
2006	O
)	O
.	O
oﬀ-policy	B
learning	O
with	B
options	I
and	O
recognizers	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
18	O
(	O
nips	O
2005	O
)	O
,	O
pp	O
.	O
1097–1104	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
precup	O
,	O
d.	O
,	O
sutton	O
,	O
r.	O
s.	O
,	O
singh	O
,	O
s.	O
(	O
2000	O
)	O
.	O
eligibility	B
traces	I
for	O
oﬀ-policy	B
policy	O
evaluation	O
.	O
in	O
proceedings	O
of	O
the	O
17th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
2000	O
)	O
,	O
pp	O
.	O
759–766	O
.	O
morgan	O
kaufmann	O
.	O
510	O
references	O
puterman	O
,	O
m.	O
l.	O
(	O
1994	O
)	O
.	O
markov	O
decision	O
problems	O
.	O
wiley	O
,	O
new	O
york	O
.	O
puterman	O
,	O
m.	O
l.	O
,	O
shin	O
,	O
m.	O
c.	O
(	O
1978	O
)	O
.	O
modiﬁed	O
policy	B
iteration	I
algorithms	O
for	O
discounted	O
markov	O
decision	O
problems	O
.	O
management	O
science	O
,	O
24	O
(	O
11	O
)	O
:1127–1137	O
.	O
quartz	O
,	O
s.	O
,	O
dayan	O
,	O
p.	O
,	O
montague	O
,	O
p.	O
r.	O
,	O
sejnowski	O
,	O
t.	O
j	O
.	O
(	O
1992	O
)	O
.	O
expectation	O
learning	O
in	O
the	O
brain	O
using	O
diﬀuse	O
ascending	O
connections	O
.	O
in	O
society	O
for	O
neuroscience	O
abstracts	O
,	O
18:1210.	O
randløv	O
,	O
j.	O
,	O
alstrøm	O
,	O
p.	O
(	O
1998	O
)	O
.	O
learning	O
to	O
drive	O
a	O
bicycle	O
using	O
reinforcement	B
learning	I
and	O
shaping	B
.	O
in	O
proceedings	O
of	O
the	O
15th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
1998	O
)	O
,	O
pp	O
.	O
463–471	O
.	O
rangel	O
,	O
a.	O
,	O
camerer	O
,	O
c.	O
,	O
montague	O
,	O
p.	O
r.	O
(	O
2008	O
)	O
.	O
a	O
framework	O
for	O
studying	O
the	O
neurobiology	O
of	O
value-based	O
decision	O
making	O
.	O
nature	O
reviews	O
neuroscience	B
,	O
9	O
(	O
7	O
)	O
:545–556	O
.	O
rangel	O
,	O
a.	O
,	O
hare	O
,	O
t.	O
(	O
2010	O
)	O
.	O
neural	B
computations	O
associated	O
with	O
goal-directed	O
choice	O
.	O
current	O
opinion	O
in	O
neurobiology	O
,	O
20	O
(	O
2	O
)	O
:262–270	O
.	O
rao	O
,	O
r.	O
p.	O
,	O
sejnowski	O
,	O
t.	O
j	O
.	O
(	O
2001	O
)	O
.	O
spike-timing-dependent	O
hebbian	O
plasticity	O
as	O
temporal	O
diﬀerence	O
learning	O
.	O
neural	B
computation	O
,	O
13	O
(	O
10	O
)	O
:2221–2237	O
.	O
ratcliﬀ	O
,	O
r.	O
(	O
1990	O
)	O
.	O
connectionist	O
models	O
of	O
recognition	O
memory	O
:	O
constraints	O
imposed	O
by	O
learning	O
and	O
forgetting	O
functions	O
.	O
psychological	O
review	O
,	O
97	O
(	O
2	O
)	O
:285–308	O
.	O
reddy	O
,	O
g.	O
,	O
celani	O
,	O
a.	O
,	O
sejnowski	O
,	O
t.	O
j.	O
,	O
vergassola	O
,	O
m.	O
(	O
2016	O
)	O
.	O
learning	O
to	O
soar	O
in	O
turbulent	O
environments	O
.	O
proceedings	O
of	O
the	O
national	O
academy	O
of	O
sciences	O
,	O
113	O
(	O
33	O
)	O
:	O
e4877–e4884	O
.	O
redish	O
,	O
d.	O
a	O
.	O
(	O
2004	O
)	O
.	O
addiction	B
as	O
a	O
computational	O
process	O
gone	O
awry	O
.	O
science	O
,	O
306	O
(	O
5703	O
)	O
:1944–	O
1947.	O
reetz	O
,	O
d.	O
(	O
1977	O
)	O
.	O
approximate	B
solutions	O
of	O
a	O
discounted	O
markovian	O
decision	O
process	O
.	O
bonner	O
mathematische	O
schriften	O
,	O
98:77–92	O
.	O
rescorla	O
,	O
r.	O
a.	O
,	O
wagner	O
,	O
a.	O
r.	O
(	O
1972	O
)	O
.	O
a	O
theory	O
of	O
pavlovian	O
conditioning	B
:	O
variations	O
in	O
the	O
in	O
a.	O
h.	O
black	O
and	O
w.	O
f.	O
prokasy	O
eﬀectiveness	O
of	O
reinforcement	O
and	O
nonreinforcement	O
.	O
(	O
eds	O
.	O
)	O
,	O
classical	B
conditioning	I
ii	O
,	O
pp	O
.	O
64–99	O
.	O
appleton-century-crofts	O
,	O
new	O
york	O
.	O
revusky	O
,	O
s.	O
,	O
garcia	O
,	O
j	O
.	O
(	O
1970	O
)	O
.	O
learned	O
associations	O
over	O
long	O
delays	O
.	O
in	O
g.	O
bower	O
(	O
ed	O
.	O
)	O
,	O
the	O
psychology	B
of	O
learning	O
and	O
motivation	B
,	O
v.	O
4	O
,	O
pp	O
.	O
1–84	O
.	O
academic	O
press	O
,	O
inc.	O
,	O
new	O
york	O
.	O
reynolds	O
,	O
j.	O
n.	O
j.	O
,	O
wickens	O
,	O
j.	O
r.	O
(	O
2002	O
)	O
.	O
dopamine-dependent	O
plasticity	O
of	O
corticostriatal	O
synapses	O
.	O
neural	B
networks	I
,	O
15	O
(	O
4	O
)	O
:507–521	O
.	O
ring	O
,	O
m.	O
b	O
.	O
(	O
in	O
preparation	O
)	O
.	O
representing	O
knowledge	O
as	O
forecasts	O
(	O
and	O
state	O
as	O
knowledge	O
)	O
.	O
ripley	O
,	O
b.	O
d.	O
(	O
2007	O
)	O
.	O
pattern	O
recognition	O
and	O
neural	O
networks	O
.	O
cambridge	O
university	O
press	O
.	O
rixner	O
,	O
s.	O
(	O
2004	O
)	O
.	O
memory	O
controller	O
optimizations	O
for	O
web	O
servers	O
.	O
in	O
proceedings	O
of	O
the	O
37th	O
annual	O
ieee/acm	O
international	O
symposium	O
on	O
microarchitecture	O
,	O
p.	O
355–366	O
.	O
ieee	O
computer	O
society	O
.	O
robbins	O
,	O
h.	O
(	O
1952	O
)	O
.	O
some	O
aspects	O
of	O
the	O
sequential	O
design	B
of	I
experiments	O
.	O
bulletin	O
of	O
the	O
american	O
mathematical	O
society	O
,	O
58:527–535	O
.	O
robertie	O
,	O
b	O
.	O
(	O
1992	O
)	O
.	O
carbon	O
versus	O
silicon	O
:	O
matching	O
wits	O
with	O
td-gammon	O
.	O
inside	O
backgam-	O
mon	O
,	O
2	O
(	O
2	O
)	O
:14–22	O
.	O
romo	O
,	O
r.	O
,	O
schultz	O
,	O
w.	O
(	O
1990	O
)	O
.	O
dopamine	B
neurons	O
of	O
the	O
monkey	O
midbrain	O
:	O
contingencies	O
of	O
responses	O
to	O
active	O
touch	O
during	O
self-initiated	O
arm	O
movements	O
.	O
journal	O
of	O
neurophysiology	O
,	O
63	O
(	O
3	O
)	O
:592–624	O
.	O
rosenblatt	O
,	O
f.	O
(	O
1962	O
)	O
.	O
principles	O
of	O
neurodynamics	O
:	O
perceptrons	O
and	O
the	O
theory	O
of	O
brain	O
mechanisms	O
.	O
spartan	O
books	O
,	O
washington	O
,	O
dc	O
.	O
ross	O
,	O
s.	O
(	O
1983	O
)	O
.	O
introduction	O
to	O
stochastic	O
dynamic	O
programming	O
.	O
academic	O
press	O
,	O
new	O
york	O
.	O
ross	O
,	O
t.	O
(	O
1933	O
)	O
.	O
machines	O
that	O
think	O
.	O
scientiﬁc	O
american	O
,	O
148	O
(	O
4	O
)	O
:206–208	O
.	O
rubinstein	O
,	O
r.	O
y	O
.	O
(	O
1981	O
)	O
.	O
simulation	O
and	O
the	O
monte	O
carlo	O
method	O
.	O
wiley	O
,	O
new	O
york	O
.	O
rumelhart	O
,	O
d.	O
e.	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
williams	O
,	O
r.	O
j	O
.	O
(	O
1986	O
)	O
.	O
learning	O
internal	O
representations	O
references	O
511	O
by	O
error	O
propagation	O
.	O
in	O
d.	O
e.	O
rumelhart	O
and	O
j.	O
l.	O
mcclelland	O
(	O
eds	O
.	O
)	O
,	O
parallel	O
dis-	O
tributed	O
processing	O
:	O
explorations	O
in	O
the	O
microstructure	O
of	O
cognition	O
,	O
vol	O
.	O
i	O
,	O
foundations	O
.	O
bradford/mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
rummery	O
,	O
g.	O
a	O
.	O
(	O
1995	O
)	O
.	O
problem	O
solving	O
with	O
reinforcement	O
learning	O
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
cambridge	O
.	O
rummery	O
,	O
g.	O
a.	O
,	O
niranjan	O
,	O
m.	O
(	O
1994	O
)	O
.	O
on-line	O
q-learning	O
using	O
connectionist	O
systems	O
.	O
tech-	O
nical	O
report	O
cued/f-infeng/tr	O
166.	O
engineering	O
department	O
,	O
cambridge	O
university	O
.	O
ruppert	O
,	O
d.	O
(	O
1988	O
)	O
.	O
eﬃcient	O
estimations	O
from	O
a	O
slowly	O
convergent	O
robbins-monro	O
process	O
.	O
cornell	O
university	O
operations	O
research	O
and	O
industrial	O
engineering	O
technical	O
report	O
no	O
.	O
781.	O
russell	O
,	O
s.	O
,	O
norvig	O
,	O
p.	O
(	O
2009	O
)	O
.	O
artiﬁcial	B
intelligence	I
:	O
a	O
modern	O
approach	O
,	O
3rd	O
edition	O
.	O
prentice-	O
hall	O
,	O
englewood	O
cliﬀs	O
,	O
nj	O
.	O
rust	O
,	O
j	O
.	O
(	O
1996	O
)	O
.	O
numerical	O
dynamic	B
programming	I
in	O
economics	O
.	O
in	O
h.	O
amman	O
,	O
d.	O
kendrick	O
,	O
and	O
j.	O
rust	O
(	O
eds	O
.	O
)	O
,	O
handbook	O
of	O
computational	O
economics	O
,	O
pp	O
.	O
614–722	O
.	O
elsevier	O
,	O
amster-	O
dam	O
.	O
saddoris	O
,	O
m.	O
p.	O
,	O
cacciapaglia	O
,	O
f.	O
,	O
wightmman	O
,	O
r.	O
m.	O
,	O
carelli	O
,	O
r.	O
m.	O
(	O
2015	O
)	O
.	O
diﬀerential	B
dopamine	O
release	O
dynamics	O
in	O
the	O
nucleus	O
accumbens	O
core	O
and	O
shell	O
reveal	O
complemen-	O
tary	O
signals	O
for	O
error	O
prediction	B
and	O
incentive	O
motivation	B
.	O
the	O
journal	O
of	O
neuroscience	O
,	O
35	O
(	O
33	O
)	O
:11572–11582	O
.	O
saksida	O
,	O
l.	O
m.	O
,	O
raymond	O
,	O
s.	O
m.	O
,	O
touretzky	O
,	O
d.	O
s.	O
(	O
1997	O
)	O
.	O
shaping	B
robot	O
behavior	O
using	O
principles	O
from	O
instrumental	B
conditioning	I
.	O
robotics	O
and	O
autonomous	O
systems	O
,	O
22	O
(	O
3	O
)	O
:231–	O
249.	O
samuel	O
,	O
a.	O
l.	O
(	O
1959	O
)	O
.	O
some	O
studies	O
in	O
machine	O
learning	O
using	O
the	O
game	O
of	O
checkers	O
.	O
ibm	O
journal	O
on	O
research	O
and	O
development	O
,	O
3	O
(	O
3	O
)	O
,	O
210–229	O
.	O
reprinted	O
in	O
e.	O
a.	O
feigenbaum	O
and	O
j.	O
feldman	O
(	O
eds	O
.	O
)	O
,	O
computers	O
and	O
thought	O
,	O
pp	O
.	O
71–105	O
.	O
mcgraw-hill	O
,	O
new	O
york	O
,	O
1963.	O
samuel	O
,	O
a.	O
l.	O
(	O
1967	O
)	O
.	O
some	O
studies	O
in	O
machine	O
learning	O
using	O
the	O
game	O
of	O
checkers	O
.	O
ii—recent	O
progress	O
.	O
ibm	O
journal	O
on	O
research	O
and	O
development	O
,	O
11	O
(	O
6	O
)	O
:601–617	O
.	O
schaal	O
,	O
s.	O
,	O
atkeson	O
,	O
c.	O
g.	O
(	O
1994	O
)	O
.	O
robot	O
juggling	O
:	O
implementation	O
of	O
memory-based	O
learning	O
.	O
ieee	O
control	B
systems	O
,	O
14	O
(	O
1	O
)	O
:57–71	O
.	O
schmajuk	O
,	O
n.	O
a	O
.	O
(	O
2008	O
)	O
.	O
computational	O
models	O
of	O
classical	O
conditioning	B
.	O
scholarpedia	O
,	O
3	O
(	O
3	O
)	O
:1664.	O
schmidhuber	O
,	O
j	O
.	O
(	O
1991a	O
)	O
.	O
curious	O
model-building	O
control	B
systems	O
.	O
in	O
proceedings	O
of	O
the	O
ieee	O
international	O
joint	O
conference	O
on	O
neural	B
networks	I
,	O
pp	O
.	O
1458–1463	O
.	O
ieee	O
.	O
schmidhuber	O
,	O
j	O
.	O
(	O
1991b	O
)	O
.	O
a	O
possibility	O
for	O
implementing	O
curiosity	B
and	O
boredom	O
in	O
model-	O
in	O
from	O
animals	O
to	O
animats	O
:	O
proceedings	O
of	O
the	O
first	O
in-	O
222–227	O
.	O
mit	O
press	O
,	O
building	O
neural	B
controllers	O
.	O
ternational	O
conference	O
on	O
simulation	O
of	O
adaptive	O
behavior	O
,	O
pp	O
.	O
cambridge	O
,	O
ma	O
.	O
schmidhuber	O
,	O
j	O
.	O
(	O
2015	O
)	O
.	O
deep	B
learning	I
in	O
neural	B
networks	I
:	O
an	O
overview	O
.	O
neural	B
networks	I
,	O
6	O
:85–117	O
.	O
schmidhuber	O
,	O
j.	O
,	O
storck	O
,	O
j.	O
,	O
hochreiter	O
,	O
s.	O
(	O
1994	O
)	O
.	O
reinforcement	O
driven	O
information	O
acquisition	O
in	O
nondeterministic	O
environments	O
.	O
technical	O
report	O
,	O
fakult¨at	O
f¨ur	O
informatik	O
,	O
technische	O
universit¨at	O
m¨unchen	O
,	O
m¨unchen	O
,	O
germany	O
.	O
schraudolph	O
,	O
n.	O
n.	O
(	O
1999	O
)	O
.	O
local	O
gain	O
adaptation	O
in	O
stochastic	O
gradient	B
descent	I
.	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
artiﬁcial	B
neural	I
networks	I
,	O
pp	O
.	O
569–574	O
.	O
ieee	O
,	O
london	O
.	O
schraudolph	O
,	O
n.	O
n.	O
(	O
2002	O
)	O
.	O
fast	O
curvature	O
matrix-vector	O
products	O
for	O
second-order	O
gradient	B
descent	I
.	O
neural	B
computation	O
,	O
14	O
(	O
7	O
)	O
:1723–1738	O
.	O
schraudolph	O
,	O
n.	O
n.	O
,	O
yu	O
,	O
j.	O
,	O
aberdeen	O
,	O
d.	O
(	O
2006	O
)	O
.	O
fast	O
online	B
policy	O
gradient	B
learning	O
with	O
smd	O
gain	O
vector	B
adaptation	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
pp	O
.	O
1185–	O
512	O
1192.	O
references	O
schultz	O
,	O
d.	O
g.	O
,	O
melsa	O
,	O
j.	O
l.	O
(	O
1967	O
)	O
.	O
state	B
functions	O
and	O
linear	O
control	B
systems	O
.	O
mcgraw-hill	O
,	O
new	O
york	O
.	O
schultz	O
,	O
w.	O
(	O
1998	O
)	O
.	O
predictive	O
reward	O
signal	O
of	O
dopamine	O
neurons	O
.	O
journal	O
of	O
neurophysiology	O
,	O
80	O
(	O
1	O
)	O
:1–27	O
.	O
schultz	O
,	O
w.	O
,	O
apicella	O
,	O
p.	O
,	O
ljungberg	O
,	O
t.	O
(	O
1993	O
)	O
.	O
responses	O
of	O
monkey	O
dopamine	B
neurons	O
to	O
reward	O
and	O
conditioned	O
stimuli	O
during	O
successive	O
steps	O
of	O
learning	O
a	O
delayed	O
response	O
task	O
.	O
the	O
journal	O
of	O
neuroscience	O
,	O
13	O
(	O
3	O
)	O
:900–913	O
.	O
schultz	O
,	O
w.	O
,	O
dayan	O
,	O
p.	O
,	O
montague	O
,	O
p.	O
r.	O
(	O
1997	O
)	O
.	O
a	O
neural	B
substrate	O
of	O
prediction	O
and	O
reward	O
.	O
science	O
,	O
275	O
(	O
5306	O
)	O
:1593–1598	O
.	O
schultz	O
,	O
w.	O
,	O
romo	O
,	O
r.	O
(	O
1990	O
)	O
.	O
dopamine	B
neurons	O
of	O
the	O
monkey	O
midbrain	O
:	O
contingencies	O
of	O
responses	O
to	O
stimuli	O
eliciting	O
immediate	O
behavioral	O
reactions	O
.	O
journal	O
of	O
neurophysiology	O
,	O
63	O
(	O
3	O
)	O
:607–624	O
.	O
schultz	O
,	O
w.	O
,	O
romo	O
,	O
r.	O
,	O
ljungberg	O
,	O
t.	O
,	O
mirenowicz	O
,	O
j.	O
,	O
hollerman	O
,	O
j.	O
r.	O
,	O
dickinson	O
,	O
a	O
.	O
(	O
1995	O
)	O
.	O
reward-related	O
signals	O
carried	O
by	O
dopamine	B
neurons	O
.	O
in	O
j.	O
c.	O
houk	O
,	O
j.	O
l.	O
davis	O
,	O
and	O
d.	O
g.	O
beiser	O
(	O
eds	O
.	O
)	O
,	O
models	O
of	O
information	O
processing	O
in	O
the	O
basal	O
ganglia	O
,	O
pp	O
.	O
233–248	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
schwartz	O
,	O
a	O
.	O
(	O
1993	O
)	O
.	O
a	O
reinforcement	B
learning	I
method	O
for	O
maximizing	O
undiscounted	O
rewards	O
.	O
in	O
proceedings	O
of	O
the	O
10th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
1993	O
)	O
,	O
pp	O
.	O
298–305	O
.	O
morgan	O
kaufmann	O
.	O
schweitzer	O
,	O
p.	O
j.	O
,	O
seidmann	O
,	O
a	O
.	O
(	O
1985	O
)	O
.	O
generalized	O
polynomial	O
approximations	O
in	O
markovian	O
decision	O
processes	O
.	O
journal	O
of	O
mathematical	O
analysis	O
and	O
applications	O
,	O
110	O
(	O
2	O
)	O
:568–582	O
.	O
selfridge	O
,	O
o.	O
g.	O
(	O
1978	O
)	O
.	O
tracking	O
and	O
trailing	O
:	O
adaptation	O
in	O
movement	O
strategies	O
.	O
technical	O
report	O
,	O
bolt	O
beranek	O
and	O
newman	O
,	O
inc.	O
unpublished	O
report	O
.	O
selfridge	O
,	O
o.	O
g.	O
(	O
1984	O
)	O
.	O
some	O
themes	O
and	O
primitives	O
in	O
ill-deﬁned	O
systems	O
.	O
in	O
o.	O
g.	O
selfridge	O
,	O
e.	O
l.	O
rissland	O
,	O
and	O
m.	O
a.	O
arbib	O
(	O
eds	O
.	O
)	O
,	O
adaptive	O
control	B
of	O
ill-deﬁned	O
systems	O
,	O
pp	O
.	O
21–26	O
.	O
plenum	O
press	O
,	O
ny	O
.	O
proceedings	O
of	O
the	O
nato	O
advanced	O
research	O
institute	O
on	O
adaptive	O
control	B
of	O
ill-deﬁned	O
systems	O
,	O
nato	O
conference	O
series	O
ii	O
,	O
systems	O
science	O
,	O
vol	O
.	O
16.	O
selfridge	O
,	O
o.	O
j.	O
,	O
sutton	O
,	O
r.	O
s.	O
,	O
barto	O
,	O
a.	O
g.	O
(	O
1985	O
)	O
.	O
training	O
and	O
tracking	O
in	O
robotics	O
.	O
in	O
a.	O
joshi	O
(	O
ed	O
.	O
)	O
,	O
proceedings	O
of	O
the	O
ninth	O
international	O
joint	O
conference	O
on	O
artiﬁcial	B
intelligence	I
,	O
pp	O
.	O
670–672	O
.	O
morgan	O
kaufmann	O
.	O
seo	O
,	O
h.	O
,	O
barraclough	O
,	O
d.	O
,	O
lee	O
,	O
d.	O
(	O
2007	O
)	O
.	O
dynamic	O
signals	O
related	O
to	O
choices	O
and	O
outcomes	O
in	O
the	O
dorsolateral	O
prefrontal	O
cortex	O
.	O
cerebral	O
cortex	O
,	O
17	O
(	O
suppl	O
1	O
)	O
:110–117	O
.	O
seung	O
,	O
h.	O
s.	O
(	O
2003	O
)	O
.	O
learning	O
in	O
spiking	O
neural	B
networks	I
by	O
reinforcement	O
of	O
stochastic	O
synaptic	O
transmission	O
.	O
neuron	O
,	O
40	O
(	O
6	O
)	O
:1063–1073	O
.	O
shah	O
,	O
a	O
.	O
(	O
2012	O
)	O
.	O
psychological	O
and	O
neuroscientiﬁc	O
connections	O
with	O
reinforcement	O
learning	O
.	O
in	O
m.	O
wiering	O
and	O
m.	O
van	O
otterlo	O
(	O
eds	O
.	O
)	O
,	O
reinforcement	B
learning	I
:	O
state-of-the-art	O
,	O
pp	O
.	O
507–	O
537.	O
springer-verlag	O
berlin	O
heidelberg	O
.	O
shannon	O
,	O
c.	O
e.	O
(	O
1950	O
)	O
.	O
programming	O
a	O
computer	O
for	O
playing	O
chess	B
.	O
philosophical	O
magazine	O
and	O
journal	O
of	O
science	O
,	O
41	O
(	O
314	O
)	O
:256–275	O
.	O
shannon	O
,	O
c.	O
e.	O
(	O
1951	O
)	O
.	O
presentation	O
of	O
a	O
maze-solving	O
machine	O
.	O
in	O
h.	O
v.	O
forester	O
(	O
ed	O
.	O
)	O
,	O
cyber-	O
netics	O
.	O
transactions	O
of	O
the	O
eighth	O
conference	O
,	O
pp	O
.	O
173–180	O
.	O
josiah	O
macy	O
jr.	O
foundation	O
.	O
shannon	O
,	O
c.	O
e.	O
(	O
1952	O
)	O
.	O
“	O
theseus	O
”	O
maze-solving	O
mouse	O
.	O
http	O
:	O
//cyberneticzoo.com/mazesolvers/1952-	O
-theseus-maze-solving-mouse	O
--	O
claude-shannon-american/	O
.	O
shelton	O
,	O
c.	O
r.	O
(	O
2001	O
)	O
.	O
importance	B
sampling	I
for	O
reinforcement	B
learning	I
with	O
multiple	O
objec-	O
tives	O
.	O
ph.d.	O
thesis	O
,	O
massachusetts	O
institute	O
of	O
technology	O
,	O
cambridge	O
ma	O
.	O
shepard	O
,	O
d.	O
(	O
1968	O
)	O
.	O
a	O
two-dimensional	O
interpolation	O
function	O
for	O
irregularly-spaced	O
data	O
.	O
in	O
references	O
513	O
proceedings	O
of	O
the	O
23rd	O
acm	O
national	O
conference	O
,	O
pp	O
.	O
517–524	O
.	O
acm	O
,	O
new	O
york	O
.	O
sherman	O
,	O
j.	O
,	O
morrison	O
,	O
w.	O
j	O
.	O
(	O
1949	O
)	O
.	O
adjustment	O
of	O
an	O
inverse	O
matrix	O
corresponding	O
to	O
changes	O
in	O
the	O
elements	O
of	O
a	O
given	O
column	O
or	O
a	O
given	O
row	O
of	O
the	O
original	O
matrix	O
(	O
abstract	O
)	O
.	O
annals	O
of	O
mathematical	O
statistics	O
,	O
20	O
(	O
4	O
)	O
:621.	O
shewchuk	O
,	O
j.	O
,	O
dean	O
,	O
t.	O
(	O
1990	O
)	O
.	O
towards	O
learning	O
time-varying	O
functions	O
with	O
high	O
input	O
in	O
proceedings	O
of	O
the	O
fifth	O
ieee	O
international	O
symposium	O
on	O
intelligent	O
dimensionality	O
.	O
control	B
,	O
pp	O
.	O
383–388	O
.	O
ieee	O
computer	O
society	O
press	O
,	O
los	O
alamitos	O
,	O
ca	O
.	O
shimansky	O
,	O
y.	O
p.	O
(	O
2009	O
)	O
.	O
biologically	O
plausible	O
learning	O
in	O
neural	B
networks	I
:	O
a	O
lesson	O
from	O
bacterial	O
chemotaxis	O
.	O
biological	O
cybernetics	B
,	O
101	O
(	O
5-6	O
)	O
:379–385	O
.	O
si	O
,	O
j.	O
,	O
barto	O
,	O
a.	O
,	O
powell	O
,	O
w.	O
,	O
wunsch	O
,	O
d	O
.	O
(	O
eds	O
.	O
)	O
(	O
2004	O
)	O
.	O
handbook	O
of	O
learning	O
and	O
approximate	O
dynamic	B
programming	I
.	O
john	O
wiley	O
and	O
sons	O
.	O
silver	O
,	O
d.	O
(	O
2009	O
)	O
.	O
reinforcement	B
learning	I
and	O
simulation	O
based	O
search	O
in	O
the	O
game	O
of	O
go	O
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
alberta	O
,	O
edmonton	O
.	O
silver	O
,	O
d.	O
,	O
huang	O
,	O
a.	O
,	O
maddison	O
,	O
c.	O
j.	O
,	O
guez	O
,	O
a.	O
,	O
sifre	O
,	O
l.	O
,	O
van	O
den	O
driessche	O
,	O
g.	O
,	O
schrittwieser	O
,	O
j.	O
,	O
antonoglou	O
,	O
i.	O
,	O
panneershelvam	O
,	O
v.	O
,	O
lanctot	O
,	O
m.	O
,	O
dieleman	O
,	O
s.	O
,	O
grewe	O
,	O
d.	O
,	O
nham	O
,	O
j.	O
,	O
kalchbrenner	O
,	O
n.	O
,	O
sutskever	O
,	O
i.	O
,	O
lillicrap	O
,	O
t.	O
,	O
leach	O
,	O
m.	O
,	O
kavukcuoglu	O
,	O
k.	O
,	O
graepel	O
,	O
t.	O
,	O
hassabis	O
,	O
d.	O
(	O
2016	O
)	O
.	O
mastering	O
the	O
game	O
of	O
go	O
with	O
deep	O
neural	B
networks	I
and	O
tree	O
search	O
.	O
nature	O
,	O
529	O
(	O
7587	O
)	O
:484–489	O
.	O
silver	O
,	O
d.	O
,	O
lever	O
,	O
g.	O
,	O
heess	O
,	O
n.	O
,	O
degris	O
,	O
t.	O
,	O
wierstra	O
,	O
d.	O
,	O
riedmiller	O
,	O
m.	O
(	O
2014	O
)	O
.	O
deterministic	O
policy	O
gradient	O
algorithms	O
.	O
in	O
proceedings	O
of	O
the	O
31st	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
2014	O
)	O
,	O
pp	O
.	O
387–395	O
.	O
silver	O
,	O
d.	O
,	O
schrittwieser	O
,	O
j.	O
,	O
simonyan	O
,	O
k.	O
,	O
antonoglou	O
,	O
i.	O
,	O
huang	O
,	O
a.	O
,	O
guez	O
,	O
a.	O
,	O
hubert	O
,	O
t.	O
,	O
baker	O
,	O
l.	O
,	O
lai	O
,	O
m.	O
,	O
bolton	O
,	O
a.	O
,	O
chen	O
,	O
y.	O
,	O
lillicrap	O
,	O
l.	O
,	O
hui	O
,	O
f.	O
,	O
sifre	O
,	O
l.	O
,	O
van	O
den	O
driessche	O
,	O
g.	O
,	O
graepel	O
,	O
t.	O
,	O
hassibis	O
,	O
d.	O
(	O
2017a	O
)	O
.	O
mastering	O
the	O
game	O
of	O
go	O
without	O
human	O
knowledge	O
.	O
nature	O
,	O
550	O
(	O
7676	O
)	O
:354–359	O
.	O
silver	O
,	O
d.	O
,	O
hubert	O
,	O
t.	O
,	O
schrittwieser	O
,	O
j.	O
,	O
antonoglou	O
,	O
i.	O
,	O
lai	O
,	O
m.	O
,	O
guez	O
,	O
a.	O
,	O
lanctot	O
,	O
m.	O
,	O
sifre	O
,	O
l.	O
,	O
kumaran	O
,	O
d.	O
,	O
graepel	O
,	O
t.	O
,	O
lillicrap	O
,	O
t.	O
,	O
simoyan	O
,	O
k.	O
,	O
hassibis	O
,	O
d.	O
(	O
2017b	O
)	O
.	O
mastering	O
chess	B
and	O
shogi	O
by	O
self-play	O
with	O
a	O
general	O
reinforcement	O
learning	O
algorithm	O
.	O
arxiv:1712.01815.	O
s¸im¸sek	O
,	O
¨o.	O
,	O
alg´orta	O
,	O
s.	O
,	O
kothiyal	O
,	O
a	O
.	O
(	O
2016	O
)	O
.	O
why	O
most	O
decisions	O
are	O
easy	O
in	O
tetris—and	O
per-	O
haps	O
in	O
other	O
sequential	O
decision	O
problems	O
,	O
as	O
well	O
.	O
in	O
proceedings	O
of	O
the	O
33rd	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
2016	O
)	O
,	O
pp	O
.	O
1757-1765.	O
simon	O
,	O
h.	O
(	O
2000	O
)	O
.	O
lecture	O
at	O
the	O
earthware	O
symposium	O
,	O
carnegie	O
mellon	O
university	O
.	O
https	O
:	O
//www.youtube.com/watch	O
?	O
v=ezhyi-	O
8dbjc	O
.	O
singh	O
,	O
s.	O
p.	O
(	O
1992a	O
)	O
.	O
reinforcement	B
learning	I
with	O
a	O
hierarchy	O
of	O
abstract	O
models	O
.	O
in	O
proceed-	O
ings	O
of	O
the	O
tenth	O
national	O
conference	O
on	O
artiﬁcial	B
intelligence	I
(	O
aaai-92	O
)	O
,	O
pp	O
.	O
202–207	O
.	O
aaai/mit	O
press	O
,	O
menlo	O
park	O
,	O
ca	O
.	O
singh	O
,	O
s.	O
p.	O
(	O
1992b	O
)	O
.	O
scaling	O
reinforcement	B
learning	I
algorithms	O
by	O
learning	O
variable	O
temporal	O
resolution	O
models	O
.	O
in	O
proceedings	O
of	O
the	O
9th	O
international	O
workshop	O
on	O
machine	O
learning	O
,	O
pp	O
.	O
406–415	O
.	O
morgan	O
kaufmann	O
.	O
singh	O
,	O
s.	O
p.	O
(	O
1993	O
)	O
.	O
learning	O
to	O
solve	O
markovian	O
decision	O
processes	O
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
massachusetts	O
,	O
amherst	O
.	O
singh	O
,	O
s.	O
p	O
.	O
(	O
ed	O
.	O
)	O
(	O
2002	O
)	O
.	O
special	O
double	B
issue	O
on	O
reinforcement	B
learning	I
,	O
machine	O
learning	O
,	O
49	O
(	O
2-3	O
)	O
.	O
singh	O
,	O
s.	O
,	O
barto	O
,	O
a.	O
g.	O
,	O
chentanez	O
,	O
n.	O
(	O
2005	O
)	O
.	O
intrinsically	O
motivated	O
reinforcement	B
learning	I
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
17	O
(	O
nips	O
2004	O
)	O
,	O
pp	O
.	O
1281–1288	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
singh	O
,	O
s.	O
p.	O
,	O
bertsekas	O
,	O
d.	O
(	O
1997	O
)	O
.	O
reinforcement	B
learning	I
for	O
dynamic	O
channel	O
allocation	O
in	O
514	O
references	O
cellular	O
telephone	O
systems	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
9	O
(	O
nips	O
1996	O
)	O
,	O
pp	O
.	O
974–980	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
singh	O
,	O
s.	O
p.	O
,	O
jaakkola	O
,	O
t.	O
,	O
jordan	O
,	O
m.	O
i	O
.	O
(	O
1994	O
)	O
.	O
learning	O
without	O
state-estimation	O
in	O
par-	O
in	O
proceedings	O
of	O
the	O
11th	O
international	O
tially	O
observable	O
markovian	O
decision	O
problems	O
.	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
1994	O
)	O
,	O
pp	O
.	O
284–292	O
.	O
morgan	O
kaufmann	O
.	O
singh	O
,	O
s.	O
,	O
jaakkola	O
,	O
t.	O
,	O
littman	O
,	O
m.	O
l.	O
,	O
szepesvri	O
,	O
c.	O
(	O
2000	O
)	O
.	O
convergence	O
results	O
for	O
single-step	O
on-policy	O
reinforcement-learning	O
algorithms	O
.	O
machine	O
learning	O
,	O
38	O
(	O
3	O
)	O
:287–308	O
.	O
singh	O
,	O
s.	O
p.	O
,	O
jaakkola	O
,	O
t.	O
,	O
jordan	O
,	O
m.	O
i	O
.	O
(	O
1995	O
)	O
.	O
reinforcement	B
learning	I
with	O
soft	O
state	O
aggre-	O
gation	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
7	O
(	O
nips	O
1994	O
)	O
,	O
pp	O
.	O
359–368	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
singh	O
,	O
s.	O
,	O
lewis	O
,	O
r.	O
l.	O
,	O
barto	O
,	O
a.	O
g.	O
(	O
2009	O
)	O
.	O
where	O
do	O
rewards	O
come	O
from	O
?	O
in	O
n.	O
taatgen	O
and	O
h.	O
van	O
rijn	O
(	O
eds	O
.	O
)	O
,	O
proceedings	O
of	O
the	O
31st	O
annual	O
conference	O
of	O
the	O
cognitive	O
science	O
society	O
,	O
pp	O
.	O
2601–2606	O
.	O
cognitive	O
science	O
society	O
.	O
singh	O
,	O
s.	O
,	O
lewis	O
,	O
r.	O
l.	O
,	O
barto	O
,	O
a.	O
g.	O
,	O
sorg	O
,	O
j	O
.	O
(	O
2010	O
)	O
.	O
intrinsically	O
motivated	O
reinforcement	B
learning	I
:	O
an	O
evolutionary	O
perspective	O
.	O
ieee	O
transactions	O
on	O
autonomous	O
mental	O
develop-	O
ment	O
,	O
2	O
(	O
2	O
)	O
:70–82	O
.	O
special	O
issue	O
on	O
active	O
learning	O
and	O
intrinsically	O
motivated	O
exploration	O
in	O
robots	O
:	O
advances	O
and	O
challenges	O
.	O
singh	O
,	O
s.	O
p.	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
1996	O
)	O
.	O
reinforcement	B
learning	I
with	O
replacing	B
eligibility	O
traces	O
.	O
machine	O
learning	O
,	O
22	O
(	O
1-3	O
)	O
:123–158	O
.	O
skinner	O
,	O
b.	O
f.	O
(	O
1938	O
)	O
.	O
the	O
behavior	O
of	O
organisms	O
:	O
an	O
experimental	O
analysis	O
.	O
appleton-	O
century	O
,	O
new	O
york	O
.	O
skinner	O
,	O
b.	O
f.	O
(	O
1958	O
)	O
.	O
reinforcement	O
today	O
.	O
american	O
psychologist	O
,	O
13	O
(	O
3	O
)	O
:94–99	O
.	O
skinner	O
,	O
b.	O
f.	O
(	O
1963	O
)	O
.	O
operant	O
behavior	O
.	O
american	O
psychologist	O
,	O
18	O
(	O
8	O
)	O
:503–515	O
.	O
sofge	O
,	O
d.	O
a.	O
,	O
white	O
,	O
d.	O
a	O
.	O
(	O
1992	O
)	O
.	O
applied	O
learning	O
:	O
optimal	B
control	I
for	O
manufacturing	O
.	O
in	O
d.	O
a.	O
white	O
and	O
d.	O
a.	O
sofge	O
(	O
eds	O
.	O
)	O
,	O
handbook	O
of	O
intelligent	O
control	B
:	O
neural	B
,	O
fuzzy	O
,	O
and	O
adaptive	O
approaches	O
,	O
pp	O
.	O
259–281	O
.	O
van	O
nostrand	O
reinhold	O
,	O
new	O
york	O
.	O
sorg	O
,	O
j.	O
d.	O
(	O
2011	O
)	O
.	O
the	O
optimal	O
reward	O
problem	O
:	O
designing	O
eﬀective	O
reward	O
for	O
bounded	O
agents	O
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
michigan	O
,	O
ann	O
arbor	O
.	O
sorg	O
,	O
j.	O
,	O
lewis	O
,	O
r.	O
l.	O
,	O
singh	O
,	O
s.	O
p.	O
(	O
2010	O
)	O
.	O
reward	O
design	O
via	O
online	B
gradient	O
ascent	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
23	O
(	O
nips	O
2010	O
)	O
,	O
pp	O
.	O
2190–2198	O
.	O
curran	O
associates	O
,	O
inc.	O
sorg	O
,	O
j.	O
,	O
singh	O
,	O
s.	O
,	O
lewis	O
,	O
r.	O
(	O
2010	O
)	O
.	O
internal	O
rewards	O
mitigate	O
agent	O
boundedness	O
.	O
in	O
proceed-	O
ings	O
of	O
the	O
27th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
2010	O
)	O
,	O
pp	O
.	O
1007–1014	O
.	O
spence	O
,	O
k.	O
w.	O
(	O
1947	O
)	O
.	O
the	O
role	O
of	O
secondary	O
reinforcement	O
in	O
delayed	B
reward	I
learning	O
.	O
psy-	O
chological	O
review	O
,	O
54	O
(	O
1	O
)	O
:1–8	O
.	O
srivastava	O
,	O
n.	O
,	O
hinton	O
,	O
g.	O
,	O
krizhevsky	O
,	O
a.	O
,	O
sutskever	O
,	O
i.	O
,	O
salakhutdinov	O
,	O
r.	O
(	O
2014	O
)	O
.	O
dropout	O
:	O
a	O
simple	O
way	O
to	O
prevent	O
neural	B
networks	I
from	O
overﬁtting	O
.	O
journal	O
of	O
machine	O
learning	O
research	O
,	O
15	O
(	O
1	O
)	O
:1929–1958	O
.	O
staddon	O
,	O
j.	O
e.	O
r.	O
(	O
1983	O
)	O
.	O
adaptive	O
behavior	O
and	O
learning	O
.	O
cambridge	O
university	O
press	O
.	O
stanﬁll	O
,	O
c.	O
,	O
waltz	O
,	O
d.	O
(	O
1986	O
)	O
.	O
toward	O
memory-based	O
reasoning	O
.	O
communications	O
of	O
the	O
acm	O
,	O
29	O
(	O
12	O
)	O
:1213–1228	O
.	O
steinberg	O
,	O
e.	O
e.	O
,	O
keiﬂin	O
,	O
r.	O
,	O
boivin	O
,	O
j.	O
r.	O
,	O
witten	O
,	O
i.	O
b.	O
,	O
deisseroth	O
,	O
k.	O
,	O
janak	O
,	O
p.	O
h.	O
(	O
2013	O
)	O
.	O
a	O
causal	O
link	O
between	O
prediction	B
errors	O
,	O
dopamine	B
neurons	O
and	O
learning	O
.	O
nature	O
neuroscience	B
,	O
16	O
(	O
7	O
)	O
:966–973	O
.	O
sterling	O
,	O
p.	O
,	O
laughlin	O
,	O
s.	O
(	O
2015	O
)	O
.	O
principles	O
of	O
neural	O
design	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
sugiyama	O
,	O
m.	O
,	O
hachiya	O
,	O
h.	O
,	O
morimura	O
,	O
t.	O
(	O
2013	O
)	O
.	O
statistical	O
reinforcement	B
learning	I
:	O
modern	O
machine	O
learning	O
approaches	O
.	O
chapman	O
&	O
hall/crc	O
.	O
references	O
515	O
suri	O
,	O
r.	O
e.	O
,	O
bargas	O
,	O
j.	O
,	O
arbib	O
,	O
m.	O
a	O
.	O
(	O
2001	O
)	O
.	O
modeling	O
functions	O
of	O
striatal	O
dopamine	B
modulation	O
in	O
learning	O
and	O
planning	O
.	O
neuroscience	B
,	O
103	O
(	O
1	O
)	O
:65–85	O
.	O
suri	O
,	O
r.	O
e.	O
,	O
schultz	O
,	O
w.	O
(	O
1998	O
)	O
.	O
learning	O
of	O
sequential	O
movements	O
by	O
neural	B
network	O
model	O
with	O
dopamine-like	O
reinforcement	B
signal	I
.	O
experimental	O
brain	O
research	O
,	O
121	O
(	O
3	O
)	O
:350–354	O
.	O
suri	O
,	O
r.	O
e.	O
,	O
schultz	O
,	O
w.	O
(	O
1999	O
)	O
.	O
a	O
neural	B
network	O
model	O
with	O
dopamine-like	O
reinforcement	B
signal	I
that	O
learns	O
a	O
spatial	O
delayed	O
response	O
task	O
.	O
neuroscience	B
,	O
91	O
(	O
3	O
)	O
:871–890	O
.	O
sutton	O
,	O
r.	O
s.	O
(	O
1978a	O
)	O
.	O
unpublished	O
report	O
.	O
learning	O
theory	O
support	O
for	O
a	O
single	O
channel	O
theory	O
of	O
the	O
brain	O
.	O
sutton	O
,	O
r.	O
s.	O
(	O
1978b	O
)	O
.	O
single	O
channel	O
theory	O
:	O
a	O
neuronal	O
theory	O
of	O
learning	O
.	O
brain	O
the-	O
ory	O
newsletter	O
,	O
4:72–75	O
.	O
center	O
for	O
systems	O
neuroscience	B
,	O
university	O
of	O
massachusetts	O
,	O
amherst	O
,	O
ma	O
.	O
sutton	O
,	O
r.	O
s.	O
(	O
1978c	O
)	O
.	O
a	O
uniﬁed	O
theory	O
of	O
expectation	O
in	O
classical	O
and	O
instrumental	O
conditioning	B
.	O
bachelors	O
thesis	O
,	O
stanford	O
university	O
.	O
sutton	O
,	O
r.	O
s.	O
(	O
1984	O
)	O
.	O
temporal	O
credit	O
assignment	O
in	O
reinforcement	O
learning	O
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
massachusetts	O
,	O
amherst	O
.	O
sutton	O
,	O
r.	O
s.	O
(	O
1988	O
)	O
.	O
learning	O
to	O
predict	O
by	O
the	O
method	O
of	O
temporal	O
diﬀerences	O
.	O
machine	O
learning	O
,	O
3	O
(	O
1	O
)	O
:9–44	O
(	O
important	O
erratum	O
p.	O
377	O
)	O
.	O
sutton	O
,	O
r.	O
s.	O
(	O
1990	O
)	O
.	O
integrated	O
architectures	O
for	O
learning	O
,	O
planning	B
,	O
and	O
reacting	O
based	O
on	O
approximating	O
dynamic	B
programming	I
.	O
in	O
proceedings	O
of	O
the	O
7th	O
international	O
workshop	O
on	O
machine	O
learning	O
,	O
pp	O
.	O
216–224	O
.	O
morgan	O
kaufmann	O
.	O
sutton	O
,	O
r.	O
s.	O
(	O
1991a	O
)	O
.	O
dyna	O
,	O
an	O
integrated	O
architecture	O
for	O
learning	O
,	O
planning	B
,	O
and	O
reacting	O
.	O
sigart	O
bulletin	O
,	O
2	O
(	O
4	O
)	O
:160–163	O
.	O
acm	O
,	O
new	O
york	O
.	O
sutton	O
,	O
r.	O
s.	O
(	O
1991b	O
)	O
.	O
planning	B
by	O
incremental	O
dynamic	O
programming	O
.	O
in	O
proceedings	O
of	O
the	O
8th	O
international	O
workshop	O
on	O
machine	O
learning	O
,	O
pp	O
.	O
353–357	O
.	O
morgan	O
kaufmann	O
.	O
sutton	O
,	O
r.	O
s	O
.	O
(	O
ed	O
.	O
)	O
(	O
1992a	O
)	O
.	O
reinforcement	B
learning	I
.	O
kluwer	O
academic	O
press	O
.	O
reprinting	O
of	O
a	O
special	O
double	B
issue	O
on	O
reinforcement	B
learning	I
,	O
machine	O
learning	O
,	O
8	O
(	O
3-4	O
)	O
.	O
sutton	O
,	O
r.s	O
.	O
(	O
1992b	O
)	O
.	O
adapting	O
bias	O
by	O
gradient	B
descent	I
:	O
an	O
incremental	O
version	O
of	O
delta-bar-	O
delta	O
.	O
proceedings	O
of	O
the	O
tenth	O
national	O
conference	O
on	O
artiﬁcial	B
intelligence	I
,	O
pp	O
.	O
171–176	O
,	O
mit	O
press	O
.	O
sutton	O
,	O
r.s	O
.	O
(	O
1992c	O
)	O
.	O
gain	O
adaptation	O
beats	O
least	O
squares	O
?	O
proceedings	O
of	O
the	O
seventh	O
yale	O
workshop	O
on	O
adaptive	O
and	O
learning	O
systems	O
,	O
pp	O
.	O
161–166	O
,	O
yale	O
university	O
,	O
new	O
haven	O
,	O
ct.	O
sutton	O
,	O
r.	O
s.	O
(	O
1995a	O
)	O
.	O
td	O
models	O
:	O
modeling	O
the	O
world	O
at	O
a	O
mixture	O
of	O
time	O
scales	O
.	O
in	O
proceed-	O
ings	O
of	O
the	O
12th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
1995	O
)	O
,	O
pp	O
.	O
531–539	O
.	O
morgan	O
kaufmann	O
.	O
sutton	O
,	O
r.	O
s.	O
(	O
1995b	O
)	O
.	O
on	O
the	O
virtues	O
of	O
linear	O
learning	O
and	O
trajectory	O
distributions	O
.	O
in	O
pro-	O
ceedings	O
of	O
the	O
workshop	O
on	O
value	B
function	I
approximation	O
at	O
the	O
12th	O
international	O
con-	O
ference	O
on	O
machine	O
learning	O
(	O
icml	O
1995	O
)	O
.	O
sutton	O
,	O
r.	O
s.	O
(	O
1996	O
)	O
.	O
generalization	O
in	O
reinforcement	O
learning	O
:	O
successful	O
examples	O
using	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
8	O
(	O
nips	O
sparse	B
coarse	O
coding	O
.	O
1995	O
)	O
,	O
pp	O
.	O
1038–1044	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
sutton	O
,	O
r.	O
s.	O
(	O
2009	O
)	O
.	O
the	O
grand	O
challenge	O
of	O
predictive	O
empirical	O
abstract	O
knowledge	O
.	O
working	O
notes	O
of	O
the	O
ijcai-09	O
workshop	O
on	O
grand	O
challenges	O
for	O
reasoning	O
from	O
experiences	O
.	O
sutton	O
,	O
r.	O
s.	O
(	O
2015a	O
)	O
introduction	O
to	O
reinforcement	B
learning	I
with	O
function	B
approximation	I
.	O
tu-	O
torial	O
at	O
the	O
conference	O
on	O
neural	B
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
montreal	O
,	O
de-	O
cember	O
7	O
,	O
2015.	O
sutton	O
,	O
r.	O
s.	O
(	O
2015b	O
)	O
true	B
online	I
emphatic	O
td	O
(	O
λ	O
)	O
:	O
quick	O
reference	O
and	O
implementation	O
guide	O
.	O
516	O
references	O
arxiv:1507.07147.	O
code	O
is	O
available	O
in	O
python	O
and	O
c++	O
by	O
downloading	O
the	O
source	O
ﬁles	O
of	O
this	O
arxiv	O
paper	O
as	O
a	O
zip	O
archive	O
.	O
sutton	O
,	O
r.	O
s.	O
,	O
barto	O
,	O
a.	O
g.	O
(	O
1981a	O
)	O
.	O
toward	O
a	O
modern	O
theory	O
of	O
adaptive	O
networks	O
:	O
expectation	O
and	O
prediction	O
.	O
psychological	O
review	O
,	O
88	O
(	O
2	O
)	O
:135–170	O
.	O
sutton	O
,	O
r.	O
s.	O
,	O
barto	O
,	O
a.	O
g.	O
(	O
1981b	O
)	O
.	O
an	O
adaptive	O
network	O
that	O
constructs	O
and	O
uses	O
an	O
internal	O
model	O
of	O
its	O
world	O
.	O
cognition	O
and	O
brain	O
theory	O
,	O
3:217–246	O
.	O
sutton	O
,	O
r.	O
s.	O
,	O
barto	O
,	O
a.	O
g.	O
(	O
1987	O
)	O
.	O
a	O
temporal-diﬀerence	O
model	O
of	O
classical	O
conditioning	B
.	O
in	O
proceedings	O
of	O
the	O
ninth	O
annual	O
conference	O
of	O
the	O
cognitive	O
science	O
society	O
,	O
pp	O
.	O
355-378.	O
erlbaum	O
,	O
hillsdale	O
,	O
nj	O
.	O
sutton	O
,	O
r.	O
s.	O
,	O
barto	O
,	O
a.	O
g.	O
(	O
1990	O
)	O
.	O
time-derivative	O
models	O
of	O
pavlovian	O
reinforcement	O
.	O
in	O
m.	O
gabriel	O
and	O
j.	O
moore	O
(	O
eds	O
.	O
)	O
,	O
learning	O
and	O
computational	O
neuroscience	O
:	O
foundations	O
of	O
adaptive	O
networks	O
,	O
pp	O
.	O
497–537	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
sutton	O
,	O
r.	O
s.	O
,	O
maei	O
,	O
h.	O
r.	O
,	O
precup	O
,	O
d.	O
,	O
bhatnagar	O
,	O
s.	O
,	O
silver	O
,	O
d.	O
,	O
szepesv´ari	O
,	O
cs.	O
,	O
wiewiora	O
,	O
e.	O
(	O
2009a	O
)	O
.	O
fast	O
gradient-descent	O
methods	O
for	O
temporal-diﬀerence	O
learning	O
with	O
linear	B
function	I
approximation	I
.	O
in	O
proceedings	O
of	O
the	O
26th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
2009	O
)	O
,	O
pp	O
.	O
993–1000	O
.	O
acm	O
,	O
new	O
york	O
.	O
sutton	O
,	O
r.	O
s.	O
,	O
szepesv´ari	O
,	O
cs.	O
,	O
maei	O
,	O
h.	O
r.	O
(	O
2009b	O
)	O
.	O
a	O
convergent	O
o	O
(	O
d2	O
)	O
temporal-diﬀerence	O
algorithm	O
for	O
oﬀ-policy	O
learning	O
with	O
linear	B
function	I
approximation	I
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
21	O
(	O
nips	O
2008	O
)	O
,	O
pp	O
.	O
1609–1616	O
.	O
curran	O
associates	O
,	O
inc.	O
sutton	O
,	O
r.	O
s.	O
,	O
mahmood	O
,	O
a.	O
r.	O
,	O
precup	O
,	O
d.	O
,	O
van	O
hasselt	O
,	O
h.	O
(	O
2014	O
)	O
.	O
a	O
new	O
q	O
(	O
λ	O
)	O
with	O
interim	O
forward	O
view	O
and	O
monte	O
carlo	O
equivalence	O
.	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
machine	O
learning	O
,	O
31.	O
jmlr	O
w	O
&	O
cp	O
32	O
(	O
2	O
)	O
.	O
sutton	O
,	O
r.	O
s.	O
,	O
mahmood	O
,	O
a.	O
r.	O
,	O
white	O
,	O
m.	O
(	O
2016	O
)	O
.	O
an	O
emphatic	O
approach	O
to	O
the	O
problem	O
of	O
oﬀ-policy	O
temporal-diﬀerence	B
learning	I
.	O
journal	O
of	O
machine	O
learning	O
research	O
,	O
17	O
(	O
73	O
)	O
:1–29	O
.	O
sutton	O
,	O
r.	O
s.	O
,	O
mcallester	O
,	O
d.	O
a.	O
,	O
singh	O
,	O
s.	O
p.	O
,	O
mansour	O
,	O
y	O
.	O
(	O
2000	O
)	O
.	O
policy	B
gradient	I
methods	I
for	O
reinforcement	B
learning	I
with	O
function	B
approximation	I
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
12	O
(	O
nips	O
1999	O
)	O
,	O
pp	O
.	O
1057–1063	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
sutton	O
,	O
r.	O
s.	O
,	O
modayil	O
,	O
j.	O
,	O
delp	O
,	O
m.	O
,	O
degris	O
,	O
t.	O
,	O
pilarski	O
,	O
p.	O
m.	O
,	O
white	O
,	O
a.	O
,	O
precup	O
,	O
d.	O
(	O
2011	O
)	O
.	O
horde	O
:	O
a	O
scalable	O
real-time	O
architecture	O
for	O
learning	O
knowledge	O
from	O
unsupervised	O
senso-	O
rimotor	O
interaction	O
.	O
in	O
proceedings	O
of	O
the	O
tenth	O
international	O
conference	O
on	O
autonomous	O
agents	O
and	O
multiagent	O
systems	O
,	O
pp	O
.	O
761–768	O
,	O
taipei	O
,	O
taiwan	O
.	O
sutton	O
,	O
r.	O
s.	O
,	O
pinette	O
,	O
b	O
.	O
(	O
1985	O
)	O
.	O
the	O
learning	O
of	O
world	O
models	O
by	O
connectionist	O
networks	O
.	O
in	O
proceedings	O
of	O
the	O
seventh	O
annual	O
conference	O
of	O
the	O
cognitive	O
science	O
society	O
,	O
pp	O
.	O
54–64	O
.	O
sutton	O
,	O
r.	O
s.	O
,	O
precup	O
,	O
d.	O
,	O
singh	O
,	O
s.	O
(	O
1999	O
)	O
.	O
between	O
mdps	O
and	O
semi-mdps	O
:	O
a	O
framework	O
for	O
temporal	O
abstraction	O
in	O
reinforcement	O
learning	O
.	O
artiﬁcial	B
intelligence	I
,	O
112	O
(	O
1-2	O
)	O
:181–211	O
.	O
sutton	O
,	O
r.	O
s.	O
,	O
singh	O
,	O
s.	O
p.	O
,	O
mcallester	O
,	O
d.	O
a	O
.	O
(	O
2000	O
)	O
.	O
comparing	O
policy-gradient	O
algorithms	O
.	O
unpublished	O
manuscript	O
.	O
sutton	O
,	O
r.	O
s.	O
,	O
szepesv´ari	O
,	O
cs.	O
,	O
geramifard	O
,	O
a.	O
,	O
bowling	O
,	O
m.	O
,	O
(	O
2008	O
)	O
.	O
dyna-style	O
planning	B
in	O
proceedings	O
of	O
the	O
24th	O
with	O
linear	O
function	B
approximation	I
and	O
prioritized	B
sweeping	I
.	O
conference	O
on	O
uncertainty	O
in	O
artiﬁcial	O
intelligence	O
,	O
pp	O
.	O
528–536	O
.	O
szepesv´ari	O
,	O
cs	O
.	O
(	O
2010	O
)	O
.	O
algorithms	O
for	O
reinforcement	O
learning	O
.	O
in	O
synthesis	O
lectures	O
on	O
artiﬁ-	O
cial	O
intelligence	O
and	O
machine	O
learning	O
,	O
4	O
(	O
1	O
)	O
:1–103	O
.	O
morgan	O
and	O
claypool	O
.	O
szita	O
,	O
i	O
.	O
(	O
2012	O
)	O
.	O
reinforcement	B
learning	I
in	O
games	O
.	O
in	O
m.	O
wiering	O
and	O
m.	O
van	O
otterlo	O
(	O
eds	O
.	O
)	O
,	O
reinforcement	B
learning	I
:	O
state-of-the-art	O
,	O
pp	O
.	O
539–577	O
.	O
springer-verlag	O
berlin	O
heidelberg	O
.	O
tadepalli	O
,	O
p.	O
,	O
ok	O
,	O
d.	O
(	O
1994	O
)	O
.	O
h-learning	O
:	O
a	O
reinforcement	B
learning	I
method	O
to	O
optimize	O
undis-	O
counted	O
average	O
reward	O
.	O
technical	O
report	O
94-30-01.	O
oregon	O
state	B
university	O
,	O
computer	O
science	O
department	O
,	O
corvallis	O
.	O
references	O
517	O
tadepalli	O
,	O
p.	O
,	O
ok	O
,	O
d.	O
(	O
1996	O
)	O
.	O
scaling	O
up	O
average	O
reward	O
reinforcement	O
learning	O
by	O
approxi-	O
mating	O
the	O
domain	O
models	O
and	O
the	O
value	O
function	O
.	O
in	O
proceedings	O
of	O
the	O
13th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
1996	O
)	O
,	O
pp	O
.	O
471–479	O
.	O
takahashi	O
,	O
y.	O
,	O
schoenbaum	O
,	O
g.	O
,	O
and	O
niv	O
,	O
y	O
.	O
(	O
2008	O
)	O
.	O
silencing	O
the	O
critics	O
:	O
understanding	O
the	O
eﬀects	O
of	O
cocaine	O
sensitization	O
on	O
dorsolateral	O
and	O
ventral	O
striatum	O
in	O
the	O
context	O
of	O
an	O
actor/critic	O
model	O
.	O
frontiers	O
in	B
neuroscience	I
,	O
2	O
(	O
1	O
)	O
:86–99	O
.	O
tambe	O
,	O
m.	O
,	O
newell	O
,	O
a.	O
,	O
rosenbloom	O
,	O
p.	O
s.	O
(	O
1990	O
)	O
.	O
the	O
problem	O
of	O
expensive	O
chunks	O
and	O
its	O
solution	O
by	O
restricting	O
expressiveness	O
.	O
machine	O
learning	O
,	O
5	O
(	O
3	O
)	O
:299–348	O
.	O
tan	O
,	O
m.	O
(	O
1991	O
)	O
.	O
learning	O
a	O
cost-sensitive	O
internal	O
representation	O
for	O
reinforcement	B
learning	I
.	O
in	O
l.	O
a.	O
birnbaum	O
and	O
g.	O
c.	O
collins	O
(	O
eds	O
.	O
)	O
,	O
proceedings	O
of	O
the	O
8th	O
international	O
workshop	O
on	O
machine	O
learning	O
,	O
pp	O
.	O
358–362	O
.	O
morgan	O
kaufmann	O
.	O
taylor	O
,	O
g.	O
,	O
parr	O
,	O
r.	O
(	O
2009	O
)	O
.	O
kernelized	O
value	B
function	I
approximation	O
for	O
reinforcement	O
learning	O
.	O
in	O
proceedings	O
of	O
the	O
26th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
2009	O
)	O
,	O
pp	O
.	O
1017–1024	O
.	O
acm	O
,	O
new	O
york	O
.	O
taylor	O
,	O
m.	O
e.	O
,	O
stone	O
,	O
p.	O
(	O
2009	O
)	O
.	O
transfer	O
learning	O
for	O
reinforcement	B
learning	I
domains	O
:	O
a	O
survey	O
.	O
journal	O
of	O
machine	O
learning	O
research	O
,	O
10	O
:1633–1685	O
.	O
tesauro	O
,	O
g.	O
(	O
1986	O
)	O
.	O
simple	O
neural	B
models	O
of	O
classical	O
conditioning	B
.	O
biological	O
cybernetics	B
,	O
55	O
(	O
2-3	O
)	O
:187–200	O
.	O
tesauro	O
,	O
g.	O
(	O
1992	O
)	O
.	O
practical	O
issues	O
in	O
temporal	O
diﬀerence	O
learning	O
.	O
machine	O
learning	O
,	O
8	O
(	O
3-	O
4	O
)	O
:257–277	O
.	O
tesauro	O
,	O
g.	O
(	O
1994	O
)	O
.	O
td-gammon	O
,	O
a	O
self-teaching	O
backgammon	B
program	O
,	O
achieves	O
master-level	O
play	O
.	O
neural	B
computation	O
,	O
6	O
(	O
2	O
)	O
:215–219	O
.	O
tesauro	O
,	O
g.	O
(	O
1995	O
)	O
.	O
temporal	O
diﬀerence	O
learning	O
and	O
td-gammon	O
.	O
communications	O
of	O
the	O
acm	O
,	O
38	O
(	O
3	O
)	O
:58–68	O
.	O
tesauro	O
,	O
g.	O
(	O
2002	O
)	O
.	O
programming	O
backgammon	B
using	O
self-teaching	O
neural	B
nets	O
.	O
artiﬁcial	B
intelligence	I
,	O
134	O
(	O
1-2	O
)	O
:181–199	O
.	O
tesauro	O
,	O
g.	O
,	O
galperin	O
,	O
g.	O
r.	O
(	O
1997	O
)	O
.	O
on-line	O
policy	B
improvement	I
using	O
monte-carlo	O
search	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
9	O
(	O
nips	O
1996	O
)	O
,	O
pp	O
.	O
1068–1074	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
tesauro	O
,	O
g.	O
,	O
gondek	O
,	O
d.	O
c.	O
,	O
lechner	O
,	O
j.	O
,	O
fan	O
,	O
j.	O
,	O
prager	O
,	O
j.	O
m.	O
(	O
2012	O
)	O
.	O
simulation	O
,	O
learning	O
,	O
ibm	O
journal	O
of	O
research	O
and	O
and	O
optimization	O
techniques	O
in	O
watson	O
’	O
s	O
game	O
strategies	O
.	O
development	O
,	O
56	O
(	O
3-4	O
)	O
:16–1–16–11	O
.	O
tesauro	O
,	O
g.	O
,	O
gondek	O
,	O
d.	O
c.	O
,	O
lenchner	O
,	O
j.	O
,	O
fan	O
,	O
j.	O
,	O
prager	O
,	O
j.	O
m.	O
(	O
2013	O
)	O
.	O
analysis	O
of	O
watson	O
’	O
s	O
strategies	O
for	O
playing	O
jeopardy	O
!	O
journal	O
of	O
artiﬁcial	O
intelligence	O
research	O
,	O
47:205–251	O
.	O
tham	O
,	O
c.	O
k.	O
(	O
1994	O
)	O
.	O
modular	O
on-line	O
function	B
approximation	I
for	O
scaling	O
up	O
reinforcement	B
learning	I
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
cambridge	O
.	O
thathachar	O
,	O
m.	O
a.	O
l.	O
,	O
sastry	O
,	O
p.	O
s.	O
(	O
1985	O
)	O
.	O
a	O
new	O
approach	O
to	O
the	O
design	B
of	I
reinforcement	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
schemes	O
for	O
learning	O
automata	O
.	O
15	O
(	O
1	O
)	O
:168–175	O
.	O
thathachar	O
,	O
m.	O
,	O
sastry	O
,	O
p.	O
s.	O
(	O
2002	O
)	O
.	O
varieties	O
of	O
learning	O
automata	O
:	O
an	O
overview	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
part	O
b	O
:	O
cybernetics	B
,	O
36	O
(	O
6	O
)	O
:711–722	O
.	O
thathachar	O
,	O
m.	O
,	O
sastry	O
,	O
p.	O
s.	O
(	O
2011	O
)	O
.	O
networks	O
of	O
learning	O
automata	O
:	O
techniques	O
for	O
online	O
stochastic	O
optimization	O
.	O
springer	O
science	O
&	O
business	O
media	O
.	O
theocharous	O
,	O
g.	O
,	O
thomas	O
,	O
p.	O
s.	O
,	O
ghavamzadeh	O
,	O
m.	O
(	O
2015	O
)	O
.	O
personalized	O
ad	O
recommendation	O
for	O
life-time	O
value	B
optimization	O
guarantees	O
.	O
in	O
proceedings	O
of	O
the	O
twenty-fourth	O
international	O
joint	O
conference	O
on	O
artiﬁcial	B
intelligence	I
(	O
ijcai-15	O
)	O
.	O
aaai	O
press	O
,	O
palo	O
alto	O
,	O
ca	O
.	O
thistlethwaite	O
,	O
d.	O
(	O
1951	O
)	O
.	O
a	O
critical	O
review	O
of	O
latent	O
learning	O
and	O
related	O
experiments	O
.	O
psy-	O
518	O
references	O
chological	O
bulletin	O
,	O
48	O
(	O
2	O
)	O
:97–129	O
.	O
thomas	O
,	O
p.	O
s.	O
(	O
2014	O
)	O
.	O
bias	O
in	O
natural	O
actor–critic	B
algorithms	O
.	O
in	O
proceedings	O
of	O
the	O
31st	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
2014	O
)	O
,	O
jmlr	O
w	O
&	O
cp	O
32	O
(	O
1	O
)	O
,	O
pp	O
.	O
441–	O
448.	O
thomas	O
,	O
p.	O
s.	O
(	O
2015	O
)	O
.	O
safe	O
reinforcement	B
learning	I
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
massachusetts	O
,	O
amherst	O
.	O
thomas	O
,	O
p.	O
s.	O
,	O
theocharous	O
,	O
g.	O
,	O
ghavamzadeh	O
,	O
m.	O
(	O
2015	O
)	O
.	O
high-conﬁdence	O
oﬀ-policy	B
eval-	O
in	O
proceedings	O
of	O
the	O
twenty-ninth	O
aaai	O
conference	O
on	O
artiﬁcial	B
intelligence	I
uation	O
.	O
(	O
aaai-15	O
)	O
,	O
pp	O
.	O
3000–3006	O
.	O
aaai	O
press	O
,	O
menlo	O
park	O
,	O
ca	O
.	O
thompson	O
,	O
w.	O
r.	O
(	O
1933	O
)	O
.	O
on	O
the	O
likelihood	O
that	O
one	O
unknown	O
probability	O
exceeds	O
another	O
in	O
view	O
of	O
the	O
evidence	O
of	O
two	O
samples	O
.	O
biometrika	O
,	O
25	O
(	O
3-4	O
)	O
:285–294	O
.	O
thompson	O
,	O
w.	O
r.	O
(	O
1934	O
)	O
.	O
on	O
the	O
theory	O
of	O
apportionment	O
.	O
american	O
journal	O
of	O
mathematics	O
,	O
57	O
:	O
450–457	O
.	O
thon	O
,	O
m.	O
(	O
2017	O
)	O
.	O
spectral	O
learning	O
of	O
sequential	O
systems	O
.	O
ph.d.	O
thesis	O
,	O
jacobs	O
university	O
bremen	O
.	O
thon	O
,	O
m.	O
,	O
jaeger	O
,	O
h.	O
(	O
2015	O
)	O
.	O
links	O
between	O
multiplicity	O
automata	O
,	O
observable	O
operator	O
models	O
and	O
predictive	B
state	I
representations	I
:	O
a	O
uniﬁed	O
learning	O
framework	O
.	O
the	O
journal	O
of	O
machine	O
learning	O
research	O
,	O
16	O
(	O
1	O
)	O
:103–147	O
.	O
thorndike	O
,	O
e.	O
l.	O
(	O
1898	O
)	O
.	O
animal	O
intelligence	O
:	O
an	O
experimental	O
study	O
of	O
the	O
associative	O
processes	O
in	O
animals	O
.	O
the	O
psychological	O
review	O
,	O
series	O
of	O
monograph	O
supplements	O
,	O
ii	O
(	O
4	O
)	O
.	O
thorndike	O
,	O
e.	O
l.	O
(	O
1911	O
)	O
.	O
animal	O
intelligence	O
.	O
hafner	O
,	O
darien	O
,	O
ct.	O
thorp	O
,	O
e.	O
o	O
.	O
(	O
1966	O
)	O
.	O
beat	O
the	O
dealer	O
:	O
a	O
winning	O
strategy	O
for	O
the	O
game	O
of	O
twenty-one	O
.	O
random	O
house	O
,	O
new	O
york	O
.	O
tian	O
,	O
t.	O
(	O
in	O
preparation	O
)	O
an	O
empirical	O
study	O
of	O
sliding-step	O
methods	O
in	O
temporal	O
diﬀerence	O
learning	O
.	O
m.sc	O
thesis	O
,	O
university	O
of	O
alberta	O
,	O
edmonton	O
.	O
tieleman	O
,	O
t.	O
,	O
hinton	O
,	O
g.	O
(	O
2012	O
)	O
.	O
lecture	O
6.5–rmsprop	O
.	O
coursera	O
:	O
neural	B
networks	I
for	O
machine	O
learning	O
4.2:26–31	O
.	O
tolman	O
,	O
e.	O
c.	O
(	O
1932	O
)	O
.	O
purposive	O
behavior	O
in	O
animals	O
and	O
men	O
.	O
century	O
,	O
new	O
york	O
.	O
tolman	O
,	O
e.	O
c.	O
(	O
1948	O
)	O
.	O
cognitive	B
maps	I
in	O
rats	O
and	O
men	O
.	O
psychological	O
review	O
,	O
55	O
(	O
4	O
)	O
:189–208	O
.	O
tsai	O
,	O
h.-s.	O
,	O
zhang	O
,	O
f.	O
,	O
adamantidis	O
,	O
a.	O
,	O
stuber	O
,	O
g.	O
d.	O
,	O
bonci	O
,	O
a.	O
,	O
de	O
lecea	O
,	O
l.	O
,	O
deisseroth	O
,	O
k.	O
(	O
2009	O
)	O
.	O
phasic	O
ﬁring	O
in	O
dopaminergic	O
neurons	O
is	O
suﬃcient	O
for	O
behavioral	O
conditioning	B
.	O
science	O
,	O
324	O
(	O
5930	O
)	O
:1080–1084	O
.	O
tsetlin	O
,	O
m.	O
l.	O
(	O
1973	O
)	O
.	O
automaton	O
theory	O
and	O
modeling	O
of	O
biological	O
systems	O
.	O
academic	O
press	O
,	O
new	O
york	O
.	O
tsitsiklis	O
,	O
j.	O
n.	O
(	O
1994	O
)	O
.	O
asynchronous	O
stochastic	O
approximation	O
and	O
q-learning	O
.	O
machine	O
learning	O
,	O
16	O
(	O
3	O
)	O
:185–202	O
.	O
tsitsiklis	O
,	O
j.	O
n.	O
(	O
2002	O
)	O
.	O
on	O
the	O
convergence	O
of	O
optimistic	O
policy	B
iteration	I
.	O
journal	O
of	O
machine	O
learning	O
research	O
,	O
3:59–72	O
.	O
tsitsiklis	O
,	O
j.	O
n.	O
,	O
van	O
roy	O
,	O
b	O
.	O
(	O
1996	O
)	O
.	O
feature-based	O
methods	O
for	O
large	O
scale	O
dynamic	O
program-	O
ming	O
.	O
machine	O
learning	O
,	O
22	O
(	O
1-3	O
)	O
:59–94	O
.	O
tsitsiklis	O
,	O
j.	O
n.	O
,	O
van	O
roy	O
,	O
b	O
.	O
(	O
1997	O
)	O
.	O
an	O
analysis	O
of	O
temporal-diﬀerence	O
learning	O
with	O
function	B
approximation	I
.	O
ieee	O
transactions	O
on	O
automatic	O
control	O
,	O
42	O
(	O
5	O
)	O
:674–690	O
.	O
tsitsiklis	O
,	O
j.	O
n.	O
,	O
van	O
roy	O
,	O
b	O
.	O
(	O
1999	O
)	O
.	O
average	O
cost	O
temporal-diﬀerence	B
learning	I
.	O
automatica	O
,	O
35	O
(	O
11	O
)	O
:1799–1808	O
.	O
turing	O
,	O
a.	O
m.	O
(	O
1948	O
)	O
.	O
intelligent	O
machinery	O
,	O
a	O
heretical	O
theory	O
.	O
the	O
turing	O
test	O
:	O
verbal	O
behavior	O
as	O
the	O
hallmark	O
of	O
intelligence	O
,	O
p.	O
105.	O
ungar	O
,	O
l.	O
h.	O
(	O
1990	O
)	O
.	O
a	O
bioreactor	O
benchmark	O
for	O
adaptive	O
network-based	O
process	O
control	B
.	O
in	O
references	O
519	O
w.	O
t.	O
miller	O
,	O
r.	O
s.	O
sutton	O
,	O
and	O
p.	O
j.	O
werbos	O
(	O
eds	O
.	O
)	O
,	O
neural	B
networks	I
for	O
control	B
,	O
pp	O
.	O
387–	O
402.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
unnikrishnan	O
,	O
k.	O
p.	O
,	O
venugopal	O
,	O
k.	O
p.	O
(	O
1994	O
)	O
.	O
alopex	O
:	O
a	O
correlation-based	O
learning	O
algorithm	O
for	O
feedforward	O
and	O
recurrent	O
neural	B
networks	I
.	O
n	O
eural	O
computation	O
,	O
6	O
(	O
3	O
)	O
:	O
469–490	O
.	O
urbanczik	O
,	O
r.	O
,	O
senn	O
,	O
w.	O
(	O
2009	O
)	O
.	O
reinforcement	B
learning	I
in	O
populations	O
of	O
spiking	O
neurons	O
.	O
nature	O
neuroscience	B
,	O
12	O
(	O
3	O
)	O
:250–252	O
.	O
urbanowicz	O
,	O
r.	O
j.	O
,	O
moore	O
,	O
j.	O
h.	O
(	O
2009	O
)	O
.	O
learning	O
classiﬁer	O
systems	O
:	O
a	O
complete	O
introduction	O
,	O
re-	O
view	O
,	O
and	O
roadmap	O
.	O
journal	O
of	O
artiﬁcial	O
evolution	B
and	O
applications	O
.	O
10.1155/2009/736398	O
.	O
valentin	O
,	O
v.	O
v.	O
,	O
dickinson	O
,	O
a.	O
,	O
o	O
’	O
doherty	O
,	O
j.	O
p.	O
(	O
2007	O
)	O
.	O
determining	O
the	O
neural	B
substrates	O
of	O
goal-directed	O
learning	O
in	O
the	O
human	O
brain	O
.	O
the	O
journal	O
of	O
neuroscience	O
,	O
27	O
(	O
15	O
)	O
:4019–4026	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
van	O
hasselt	O
,	O
h.	O
(	O
2010	O
)	O
.	O
double	B
q-learning	O
.	O
systems	O
23	O
(	O
nips	O
2010	O
)	O
,	O
pp	O
.	O
2613–2621	O
.	O
curran	O
associates	O
,	O
inc.	O
van	O
hasselt	O
,	O
h.	O
(	O
2011	O
)	O
.	O
insights	O
in	O
reinforcement	O
learning	O
:	O
formal	O
analysis	O
and	O
empirical	O
evaluation	O
of	O
temporal-diﬀerence	O
learning	O
.	O
siks	O
dissertation	O
series	O
number	O
2011-04.	O
van	O
hasselt	O
,	O
h.	O
(	O
2012	O
)	O
.	O
reinforcement	B
learning	I
in	O
continuous	B
state	I
and	O
action	B
spaces	O
.	O
in	O
m.	O
wiering	O
and	O
m.	O
van	O
otterlo	O
(	O
eds	O
.	O
)	O
,	O
reinforcement	B
learning	I
:	O
state-of-the-art	O
,	O
pp	O
.	O
207–251	O
.	O
springer-verlag	O
berlin	O
heidelberg	O
.	O
van	O
hasselt	O
,	O
h.	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
2015	O
)	O
.	O
learning	O
to	O
predict	O
independent	O
of	O
span	O
.	O
arxiv	O
1508.04582.	O
van	O
roy	O
,	O
b.	O
,	O
bertsekas	O
,	O
d.	O
p.	O
,	O
lee	O
,	O
y.	O
,	O
tsitsiklis	O
,	O
j.	O
n.	O
(	O
1997	O
)	O
.	O
a	O
neuro-dynamic	O
programming	O
approach	O
to	O
retailer	O
inventory	O
management	O
.	O
in	O
proceedings	O
of	O
the	O
36th	O
ieee	O
conference	O
on	O
decision	O
and	B
control	I
,	O
vol	O
.	O
4	O
,	O
pp	O
.	O
4052–4057	O
.	O
van	O
seijen	O
,	O
h.	O
(	O
2016	O
)	O
.	O
eﬀective	O
multi-step	O
temporal-diﬀerence	B
learning	I
for	O
non-linear	O
function	B
approximation	I
.	O
arxiv	O
preprint	O
arxiv:1608.05151.	O
van	O
seijen	O
,	O
h.	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
2013	O
)	O
.	O
eﬃcient	O
planning	B
in	O
mdps	O
by	O
small	O
backups	O
.	O
in	O
:	O
proceed-	O
ings	O
of	O
the	O
30th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
2013	O
)	O
,	O
pp	O
.	O
361–369	O
.	O
van	O
seijen	O
,	O
h.	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
2014	O
)	O
.	O
true	B
online	I
td	O
(	O
λ	O
)	O
.	O
in	O
proceedings	O
of	O
the	O
31st	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
2014	O
)	O
,	O
pp	O
.	O
692–700	O
.	O
jmlr	O
w	O
&	O
cp	O
32	O
(	O
1	O
)	O
,	O
van	O
seijen	O
,	O
h.	O
,	O
mahmood	O
,	O
a.	O
r.	O
,	O
pilarski	O
,	O
p.	O
m.	O
,	O
machado	O
,	O
m.	O
c.	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
2016	O
)	O
.	O
true	B
online	I
temporal-diﬀerence	O
learning	O
.	O
journal	O
of	O
machine	O
learning	O
research	O
,	O
17	O
(	O
145	O
)	O
:1–40	O
.	O
van	O
seijen	O
,	O
h.	O
,	O
van	O
hasselt	O
,	O
h.	O
,	O
whiteson	O
,	O
s.	O
,	O
wiering	O
,	O
m.	O
(	O
2009	O
)	O
.	O
a	O
theoretical	O
and	O
empirical	O
analysis	O
of	O
expected	O
sarsa	O
.	O
in	O
ieee	O
symposium	O
on	O
adaptive	O
dynamic	B
programming	I
and	O
reinforcement	B
learning	I
,	O
pp	O
.	O
177–184	O
.	O
varga	O
,	O
r.	O
s.	O
(	O
1962	O
)	O
.	O
matrix	O
iterative	B
analysis	O
.	O
englewood	O
cliﬀs	O
,	O
nj	O
:	O
prentice-hall	O
.	O
vasilaki	O
,	O
e.	O
,	O
fr´emaux	O
,	O
n.	O
,	O
urbanczik	O
,	O
r.	O
,	O
senn	O
,	O
w.	O
,	O
gerstner	O
,	O
w.	O
(	O
2009	O
)	O
.	O
spike-based	O
rein-	O
forcement	O
learning	O
in	O
continuous	B
state	I
and	O
action	B
space	O
:	O
when	O
policy	B
gradient	I
methods	I
fail	O
.	O
plos	O
computational	O
biology	O
,	O
5	O
(	O
12	O
)	O
.	O
viswanathan	O
,	O
r.	O
,	O
narendra	O
,	O
k.	O
s.	O
(	O
1974	O
)	O
.	O
games	O
of	O
stochastic	O
automata	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
4	O
(	O
1	O
)	O
:131–135	O
.	O
walter	O
,	O
w.	O
g.	O
(	O
1950	O
)	O
.	O
an	O
imitation	O
of	O
life	O
.	O
scientiﬁc	O
american	O
,	O
182	O
(	O
5	O
)	O
:42–45	O
.	O
walter	O
,	O
w.	O
g.	O
(	O
1951	O
)	O
.	O
a	O
machine	O
that	O
learns	O
.	O
scientiﬁc	O
american	O
,	O
185	O
(	O
2	O
)	O
:60–63	O
.	O
waltz	O
,	O
m.	O
d.	O
,	O
fu	O
,	O
k.	O
s.	O
(	O
1965	O
)	O
.	O
a	O
heuristic	O
approach	O
to	O
reinforcement	B
learning	I
control	O
systems	O
.	O
ieee	O
transactions	O
on	O
automatic	O
control	O
,	O
10	O
(	O
4	O
)	O
:390–398	O
.	O
watkins	O
,	O
c.	O
j.	O
c.	O
h.	O
(	O
1989	O
)	O
.	O
learning	O
from	O
delayed	O
rewards	O
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
cambridge	O
.	O
watkins	O
,	O
c.	O
j.	O
c.	O
h.	O
,	O
dayan	O
,	O
p.	O
(	O
1992	O
)	O
.	O
q-learning	O
.	O
machine	O
learning	O
,	O
8	O
(	O
3-4	O
)	O
:279–292	O
.	O
werbos	O
,	O
p.	O
j	O
.	O
(	O
1977	O
)	O
.	O
advanced	O
forecasting	O
methods	O
for	O
global	O
crisis	O
warning	O
and	O
models	O
of	O
520	O
references	O
intelligence	O
.	O
general	O
systems	O
yearbook	O
,	O
22	O
(	O
12	O
)	O
:25–38	O
.	O
werbos	O
,	O
p.	O
j	O
.	O
(	O
1982	O
)	O
.	O
applications	O
of	O
advances	O
in	O
nonlinear	O
sensitivity	O
analysis	O
.	O
in	O
r.	O
f.	O
drenick	O
and	O
f.	O
kozin	O
(	O
eds	O
.	O
)	O
,	O
system	O
modeling	O
and	O
optimization	O
,	O
pp	O
.	O
762–770	O
.	O
springer-verlag	O
,	O
berlin	O
.	O
werbos	O
,	O
p.	O
j	O
.	O
(	O
1987	O
)	O
.	O
building	O
and	O
understanding	O
adaptive	O
systems	O
:	O
a	O
statistical/numerical	O
approach	O
to	O
factory	O
automation	O
and	O
brain	O
research	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
17	O
(	O
1	O
)	O
:7–20	O
.	O
werbos	O
,	O
p.	O
j	O
.	O
(	O
1988	O
)	O
.	O
generalization	O
of	O
back	O
propagation	O
with	O
applications	O
to	O
a	O
recurrent	O
gas	O
market	O
model	O
.	O
neural	B
networks	I
,	O
1	O
(	O
4	O
)	O
:339–356	O
.	O
werbos	O
,	O
p.	O
j	O
.	O
(	O
1989	O
)	O
.	O
neural	B
networks	I
for	O
control	O
and	O
system	O
identiﬁcation	O
.	O
in	O
proceedings	O
of	O
the	O
28th	O
conference	O
on	O
decision	O
and	B
control	I
,	O
pp	O
.	O
260–265	O
.	O
ieee	O
control	B
systems	O
society	O
.	O
werbos	O
,	O
p.	O
j	O
.	O
(	O
1992	O
)	O
.	O
approximate	B
dynamic	I
programming	I
for	O
real-time	O
control	O
and	O
neural	O
modeling	O
.	O
in	O
d.	O
a.	O
white	O
and	O
d.	O
a.	O
sofge	O
(	O
eds	O
.	O
)	O
,	O
handbook	O
of	O
intelligent	O
control	B
:	O
neural	B
,	O
fuzzy	O
,	O
and	O
adaptive	O
approaches	O
,	O
pp	O
.	O
493–525	O
.	O
van	O
nostrand	O
reinhold	O
,	O
new	O
york	O
.	O
werbos	O
,	O
p.	O
j	O
.	O
(	O
1994	O
)	O
.	O
the	O
roots	O
of	O
backpropagation	O
:	O
from	O
ordered	O
derivatives	O
to	O
neural	B
networks	I
and	O
political	O
forecasting	O
(	O
vol	O
.	O
1	O
)	O
.	O
john	O
wiley	O
and	O
sons	O
.	O
wiering	O
,	O
m.	O
,	O
van	O
otterlo	O
,	O
m.	O
(	O
2012	O
)	O
.	O
reinforcement	B
learning	I
:	O
state-of-the-art	O
.	O
springer-	O
verlag	O
berlin	O
heidelberg	O
.	O
white	O
,	O
a	O
.	O
(	O
2015	O
)	O
.	O
developing	O
a	O
predictive	O
approach	O
to	O
knowledge	O
.	O
ph.d.	O
thesis	O
,	O
university	O
of	O
alberta	O
,	O
edmonton	O
.	O
white	O
,	O
d.	O
j	O
.	O
(	O
1969	O
)	O
.	O
dynamic	B
programming	I
.	O
holden-day	O
,	O
san	O
francisco	O
.	O
white	O
,	O
d.	O
j	O
.	O
(	O
1985	O
)	O
.	O
real	O
applications	O
of	O
markov	O
decision	O
processes	O
.	O
interfaces	O
,	O
15	O
(	O
6	O
)	O
:73–83	O
.	O
white	O
,	O
d.	O
j	O
.	O
(	O
1988	O
)	O
.	O
interfaces	O
,	O
further	O
real	O
applications	O
of	O
markov	O
decision	O
processes	O
.	O
18	O
(	O
5	O
)	O
:55–61	O
.	O
white	O
,	O
d.	O
j	O
.	O
(	O
1993	O
)	O
.	O
a	O
survey	O
of	O
applications	O
of	O
markov	O
decision	O
processes	O
.	O
journal	O
of	O
the	O
operational	O
research	O
society	O
,	O
44	O
(	O
11	O
)	O
:1073–1096	O
.	O
white	O
,	O
a.	O
,	O
white	O
,	O
m.	O
(	O
2016	O
)	O
.	O
investigating	O
practical	O
linear	O
temporal	O
diﬀerence	O
learning	O
.	O
in	O
proceedings	O
of	O
the	O
2016	O
international	O
conference	O
on	O
autonomous	O
agents	O
and	O
multiagent	O
systems	O
,	O
pp	O
.	O
494–502	O
.	O
whitehead	O
,	O
s.	O
d.	O
,	O
ballard	O
,	O
d.	O
h.	O
(	O
1991	O
)	O
.	O
learning	O
to	O
perceive	O
and	O
act	O
by	O
trial	O
and	O
error	O
.	O
machine	O
learning	O
,	O
7	O
(	O
1	O
)	O
:45–83	O
.	O
whitt	O
,	O
w.	O
(	O
1978	O
)	O
.	O
approximations	O
of	O
dynamic	O
programs	O
i.	O
mathematics	O
of	O
operations	O
re-	O
search	O
,	O
3	O
(	O
3	O
)	O
:231–243	O
.	O
whittle	O
,	O
p.	O
(	O
1982	O
)	O
.	O
optimization	O
over	O
time	O
,	O
vol	O
.	O
1.	O
wiley	O
,	O
new	O
york	O
.	O
whittle	O
,	O
p.	O
(	O
1983	O
)	O
.	O
optimization	O
over	O
time	O
,	O
vol	O
.	O
2.	O
wiley	O
,	O
new	O
york	O
.	O
wickens	O
,	O
j.	O
,	O
k¨otter	O
,	O
r.	O
(	O
1995	O
)	O
.	O
cellular	O
models	O
of	O
reinforcement	O
.	O
in	O
j.	O
c.	O
houk	O
,	O
j.	O
l.	O
davis	O
and	O
d.	O
g.	O
beiser	O
(	O
eds	O
.	O
)	O
,	O
models	O
of	O
information	O
processing	O
in	O
the	O
basal	O
ganglia	O
,	O
pp	O
.	O
187–214	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
widrow	O
,	O
b.	O
,	O
gupta	O
,	O
n.	O
k.	O
,	O
maitra	O
,	O
s.	O
(	O
1973	O
)	O
.	O
punish/reward	O
:	O
learning	O
with	O
a	O
critic	B
in	O
adaptive	O
threshold	O
systems	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
3	O
(	O
5	O
)	O
:455–465	O
.	O
widrow	O
,	O
b.	O
,	O
hoﬀ	O
,	O
m.	O
e.	O
(	O
1960	O
)	O
.	O
adaptive	O
switching	O
circuits	O
.	O
in	O
1960	O
wescon	O
convention	O
record	O
part	O
iv	O
,	O
pp	O
.	O
96–104	O
.	O
institute	O
of	O
radio	O
engineers	O
,	O
new	O
york	O
.	O
reprinted	O
in	O
j.	O
a.	O
anderson	O
and	O
e.	O
rosenfeld	O
,	O
neurocomputing	O
:	O
foundations	O
of	O
research	O
,	O
pp	O
.	O
126–134	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
,	O
1988.	O
widrow	O
,	O
b.	O
,	O
smith	O
,	O
f.	O
w.	O
(	O
1964	O
)	O
.	O
pattern-recognizing	O
control	B
systems	O
.	O
in	O
j.	O
t.	O
tou	O
and	O
r.	O
h.	O
wilcox	O
(	O
eds	O
.	O
)	O
,	O
computer	O
and	O
information	O
sciences	O
,	O
pp	O
.	O
288–317	O
.	O
spartan	O
,	O
washing-	O
ton	O
,	O
dc	O
.	O
references	O
521	O
widrow	O
,	O
b.	O
,	O
stearns	O
,	O
s.	O
d.	O
(	O
1985	O
)	O
.	O
adaptive	O
signal	O
processing	O
.	O
prentice-hall	O
,	O
englewood	O
cliﬀs	O
,	O
nj	O
.	O
wiener	O
,	O
n.	O
(	O
1964	O
)	O
.	O
god	O
and	O
golem	O
,	O
inc	O
:	O
a	O
comment	O
on	O
certain	O
points	O
where	O
cybernetics	B
impinges	O
on	O
religion	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
wiewiora	O
,	O
e.	O
(	O
2003	O
)	O
.	O
potential-based	O
shaping	B
and	O
q-value	O
initialization	O
are	O
equivalent	O
.	O
journal	O
of	O
artiﬁcial	O
intelligence	O
research	O
,	O
19	O
:205–208	O
.	O
williams	O
,	O
r.	O
j	O
.	O
(	O
1986	O
)	O
.	O
reinforcement	B
learning	I
in	O
connectionist	O
networks	O
:	O
a	O
mathematical	O
analysis	O
.	O
technical	O
report	O
ics	O
8605.	O
institute	O
for	O
cognitive	O
science	O
,	O
university	O
of	O
california	O
at	O
san	O
diego	O
,	O
la	O
jolla	O
.	O
williams	O
,	O
r.	O
j	O
.	O
(	O
1987	O
)	O
.	O
reinforcement-learning	O
connectionist	O
systems	O
.	O
technical	O
report	O
nu-	O
ccs-87-3	O
.	O
college	O
of	O
computer	O
science	O
,	O
northeastern	O
university	O
,	O
boston	O
.	O
williams	O
,	O
r.	O
j	O
.	O
(	O
1988	O
)	O
.	O
on	O
the	O
use	O
of	O
backpropagation	O
in	O
associative	O
reinforcement	B
learning	I
.	O
in	O
proceedings	O
of	O
the	O
ieee	O
international	O
conference	O
on	O
neural	B
networks	I
,	O
pp	O
.	O
i263–i270	O
.	O
ieee	O
san	O
diego	O
section	O
and	O
ieee	O
tab	O
neural	B
network	O
committee	O
.	O
williams	O
,	O
r.	O
j	O
.	O
(	O
1992	O
)	O
.	O
simple	O
statistical	O
gradient-following	O
algorithms	O
for	O
connectionist	O
rein-	O
forcement	O
learning	O
.	O
machine	O
learning	O
,	O
8	O
(	O
3-4	O
)	O
:229–256	O
.	O
williams	O
,	O
r.	O
j.	O
,	O
baird	O
,	O
l.	O
c.	O
(	O
1990	O
)	O
.	O
a	O
mathematical	O
analysis	O
of	O
actor–critic	O
architectures	O
for	O
learning	O
optimal	O
controls	O
through	O
incremental	O
dynamic	O
programming	O
.	O
in	O
proceedings	O
of	O
the	O
sixth	O
yale	O
workshop	O
on	O
adaptive	O
and	O
learning	O
systems	O
,	O
pp	O
.	O
96–101	O
.	O
center	O
for	O
systems	O
science	O
,	O
dunham	O
laboratory	O
,	O
yale	O
university	O
,	O
new	O
haven	O
.	O
wilson	O
,	O
r.	O
c.	O
,	O
takahashi	O
,	O
y.	O
k.	O
,	O
schoenbaum	O
,	O
g.	O
,	O
niv	O
,	O
y	O
.	O
(	O
2014	O
)	O
.	O
orbitofrontal	O
cortex	O
as	O
a	O
cognitive	O
map	O
of	O
task	O
space	O
.	O
neuron	O
,	O
81	O
(	O
2	O
)	O
:267–279	O
.	O
wilson	O
,	O
s.	O
w.	O
(	O
1994	O
)	O
.	O
zcs	O
,	O
a	O
zeroth	O
order	O
classiﬁer	O
system	O
.	O
evolutionary	O
computation	O
,	O
2	O
(	O
1	O
)	O
:1–18	O
.	O
wise	O
,	O
r.	O
a	O
.	O
(	O
2004	O
)	O
.	O
dopamine	B
,	O
learning	O
,	O
and	O
motivation	O
.	O
nature	O
reviews	O
neuroscience	B
,	O
5	O
(	O
6	O
)	O
:1–12	O
.	O
witten	O
,	O
i.	O
h.	O
(	O
1976	O
)	O
.	O
the	O
apparent	O
conﬂict	O
between	O
estimation	O
and	O
control—a	O
survey	O
of	O
the	O
two-armed	O
problem	O
.	O
journal	O
of	O
the	O
franklin	O
institute	O
,	O
301	O
(	O
1-2	O
)	O
:161–189	O
.	O
witten	O
,	O
i.	O
h.	O
(	O
1977	O
)	O
.	O
an	O
adaptive	O
optimal	O
controller	O
for	O
discrete-time	O
markov	O
environments	O
.	O
information	O
and	B
control	I
,	O
34	O
(	O
4	O
)	O
:286–295	O
.	O
witten	O
,	O
i.	O
h.	O
,	O
corbin	O
,	O
m.	O
j	O
.	O
(	O
1973	O
)	O
.	O
human	O
operators	O
and	O
automatic	O
adaptive	O
controllers	O
:	O
a	O
comparative	O
study	O
on	O
a	O
particular	O
control	B
task	O
.	O
international	O
journal	O
of	O
man–machine	O
studies	O
,	O
5	O
(	O
1	O
)	O
:75–104	O
.	O
woodbury	O
,	O
t.	O
,	O
dunn	O
,	O
c.	O
,	O
and	O
valasek	O
,	O
j	O
.	O
(	O
2014	O
)	O
.	O
autonomous	O
soaring	O
using	O
reinforcement	B
learning	I
for	O
trajectory	O
generation	O
.	O
in	O
52nd	O
aerospace	O
sciences	O
meeting	O
,	O
p.	O
0990.	O
woodworth	O
,	O
r.	O
s.	O
(	O
1938	O
)	O
.	O
experimental	O
psychology	B
.	O
new	O
york	O
:	O
henry	O
holt	O
and	O
company	O
.	O
xie	O
,	O
x.	O
,	O
seung	O
,	O
h.	O
s.	O
(	O
2004	O
)	O
.	O
learning	O
in	O
neural	B
networks	I
by	O
reinforcement	O
of	O
irregular	O
spiking	O
.	O
physical	O
review	O
e	O
,	O
69	O
(	O
4	O
)	O
:041909.	O
xu	O
,	O
x.	O
,	O
xie	O
,	O
t.	O
,	O
hu	O
,	O
d.	O
,	O
lu	O
,	O
x	O
.	O
(	O
2005	O
)	O
.	O
kernel	O
least-squares	O
temporal	O
diﬀerence	O
learning	O
.	O
international	O
journal	O
of	O
information	O
technology	O
,	O
11	O
(	O
9	O
)	O
:54–63	O
.	O
yagishita	O
,	O
s.	O
,	O
hayashi-takagi	O
,	O
a.	O
,	O
ellis-davies	O
,	O
g.	O
c.	O
r.	O
,	O
urakubo	O
,	O
h.	O
,	O
ishii	O
,	O
s.	O
,	O
kasai	O
,	O
h.	O
(	O
2014	O
)	O
.	O
a	O
critical	O
time	O
window	O
for	O
dopamine	O
actions	O
on	O
the	O
structural	B
plasticity	O
of	O
dendritic	O
spines	O
.	O
science	O
,	O
345	O
(	O
6204	O
)	O
:1616–1619	O
.	O
yee	O
,	O
r.	O
c.	O
,	O
saxena	O
,	O
s.	O
,	O
utgoﬀ	O
,	O
p.	O
e.	O
,	O
barto	O
,	O
a.	O
g.	O
(	O
1990	O
)	O
.	O
explaining	O
temporal	O
diﬀerences	O
to	O
create	O
useful	O
concepts	O
for	O
evaluating	O
states	O
.	O
in	O
proceedings	O
of	O
the	O
eighth	O
national	O
conference	O
on	O
artiﬁcial	B
intelligence	I
(	O
aaai-90	O
)	O
,	O
pp	O
.	O
882–888	O
.	O
aaai	O
press	O
,	O
menlo	O
park	O
,	O
ca	O
.	O
yin	O
,	O
h.	O
h.	O
,	O
knowlton	O
,	O
b.	O
j	O
.	O
(	O
2006	O
)	O
.	O
the	O
role	O
of	O
the	O
basal	B
ganglia	I
in	O
habit	O
formation	O
.	O
nature	O
522	O
references	O
reviews	O
neuroscience	B
,	O
7	O
(	O
6	O
)	O
:464–476	O
.	O
young	O
,	O
p.	O
(	O
1984	O
)	O
.	O
recursive	O
estimation	O
and	O
time-series	O
analysis	O
.	O
springer-verlag	O
,	O
berlin	O
.	O
yu	O
,	O
h.	O
(	O
2010	O
)	O
.	O
convergence	O
of	O
least	O
squares	O
temporal	O
diﬀerence	O
methods	O
under	O
general	O
condi-	O
tions	O
.	O
international	O
conference	O
on	O
machine	O
learning	O
27	O
,	O
pp	O
.	O
1207–1214	O
.	O
yu	O
,	O
h.	O
(	O
2012	O
)	O
.	O
least	O
squares	O
temporal	O
diﬀerence	O
methods	O
:	O
an	O
analysis	O
under	O
general	O
conditions	O
.	O
siam	O
journal	O
on	O
control	O
and	O
optimization	O
,	O
50	O
(	O
6	O
)	O
:3310–3343	O
.	O
yu	O
,	O
h.	O
(	O
2015	O
)	O
.	O
on	O
convergence	O
of	O
emphatic	O
temporal-diﬀerence	B
learning	I
.	O
in	O
proceedings	O
of	O
the	O
28th	O
annual	O
conference	O
on	O
learning	O
theory	O
,	O
jmlr	O
w	O
&	O
cp	O
40.	O
also	O
arxiv:1506.02582.	O
yu	O
,	O
h.	O
(	O
2016	O
)	O
.	O
weak	O
convergence	O
properties	O
of	O
constrained	O
emphatic	O
temporal-diﬀerence	O
learn-	O
ing	B
with	O
constant	O
and	O
slowly	O
diminishing	O
stepsize	O
.	O
journal	O
of	O
machine	O
learning	O
research	O
,	O
17	O
(	O
220	O
)	O
:1–58	O
.	O
yu	O
,	O
h.	O
(	O
2017	O
)	O
.	O
on	O
convergence	O
of	O
some	O
gradient-based	O
temporal-diﬀerences	O
algorithms	O
for	O
oﬀ-	O
policy	B
learning	O
.	O
arxiv:1712.09652.	O
yu	O
,	O
h.	O
,	O
mahmood	O
,	O
a.	O
r.	O
,	O
sutton	O
,	O
r.	O
s.	O
(	O
2017	O
)	O
.	O
on	O
generalized	O
bellman	O
equations	O
and	O
temporal-	O
diﬀerence	O
learning	O
.	O
arxiv:17041.04463.	O
a	O
summary	O
appeared	O
in	O
proceedings	O
of	O
the	O
cana-	O
dian	O
conference	O
on	O
artiﬁcial	B
intelligence	I
,	O
pp	O
.	O
3–14	O
.	O
springer	O
.	O
zhang	O
,	O
m.	O
,	O
yum	O
,	O
t.	O
p.	O
(	O
1989	O
)	O
.	O
comparisons	O
of	O
channel-assignment	O
strategies	O
in	O
cellular	O
mobile	O
telephone	O
systems	O
.	O
ieee	O
transactions	O
on	O
vehicular	O
technology	O
,	O
38:211-215.	O
zhang	O
,	O
w.	O
(	O
1996	O
)	O
.	O
reinforcement	B
learning	I
for	O
job-shop	O
scheduling	O
.	O
ph.d.	O
thesis	O
,	O
oregon	O
state	B
university	O
,	O
corvallis	O
.	O
technical	O
report	O
cs-96-30-1	O
.	O
zhang	O
,	O
w.	O
,	O
dietterich	O
,	O
t.	O
g.	O
(	O
1995	O
)	O
.	O
a	O
reinforcement	B
learning	I
approach	O
to	O
job-shop	O
scheduling	O
.	O
in	O
proceedings	O
of	O
the	O
fourteenth	O
international	O
joint	O
conference	O
on	O
artiﬁcial	B
intelligence	I
(	O
ijcai-95	O
)	O
,	O
pp	O
.	O
1114–1120	O
.	O
morgan	O
kaufmann	O
.	O
zhang	O
,	O
w.	O
,	O
dietterich	O
,	O
t.	O
g.	O
(	O
1996	O
)	O
.	O
high-performance	O
job-shop	O
scheduling	O
with	O
a	O
time–delay	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
8	O
(	O
nips	O
1995	O
)	O
,	O
td	O
(	O
λ	O
)	O
network	O
.	O
pp	O
.	O
1024–1030	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
zweben	O
,	O
m.	O
,	O
daun	O
,	O
b.	O
,	O
deale	O
,	O
m.	O
(	O
1994	O
)	O
.	O
scheduling	O
and	O
rescheduling	O
with	O
iterative	O
repair	O
.	O
in	O
m.	O
zweben	O
and	O
m.	O
s.	O
fox	O
(	O
eds	O
.	O
)	O
,	O
intelligent	O
scheduling	O
,	O
pp	O
.	O
241–255	O
.	O
morgan	O
kaufmann	O
.	O