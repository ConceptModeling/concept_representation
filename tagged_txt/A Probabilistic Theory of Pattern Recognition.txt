stochastic	O
mechanics	O
applications	O
of	O
random	O
media	O
mathematics	O
signal	O
processing	O
stochastic	O
modelling	O
and	O
image	O
synthesis	O
and	O
applied	O
probability	O
applications	O
of	O
mathematics	O
1	O
fleming/rishel	O
,	O
deterministic	O
and	O
stochastic	O
optimal	O
control	O
(	O
1975	O
)	O
2	O
marchuk	O
,	O
methods	O
of	O
numerical	O
mathematics	O
,	O
second	O
ed	O
.	O
(	O
1982	O
)	O
3	O
balakrishnan	O
,	O
applied	O
functional	O
analysis	O
,	O
second	O
ed	O
.	O
(	O
1981	O
)	O
4	O
borovkov	O
,	O
stochastic	O
processes	O
in	O
queueing	O
theory	O
(	O
1976	O
)	O
5	O
liptserlshiryayev	O
,	O
statistics	B
of	O
random	O
processes	O
i	O
:	O
general	O
theory	O
(	O
1977	O
)	O
6	O
liptserlshiryayev	O
,	O
statistics	B
of	O
random	O
processes	O
ii	O
:	O
applications	O
(	O
1978	O
)	O
7	O
vorob'ev	O
,	O
game	O
theory	O
:	O
lectures	O
for	O
economists	O
and	O
systems	O
scientists	O
(	O
1977	O
)	O
8	O
shiryayev	O
,	O
optimal	O
stopping	O
rules	O
(	O
1978	O
)	O
9	O
ibragimov/rozanov	O
,	O
gaussian	B
random	O
processes	O
(	O
1978	O
)	O
10	O
wonham	O
,	O
linear	O
multivariable	O
control	O
:	O
a	O
geometric	O
approach	O
,	O
third	O
ed	O
.	O
(	O
1985	O
)	O
11	O
hida	O
,	O
brownian	O
motion	O
(	O
1980	O
)	O
12	O
hestenes	O
,	O
conjugate	O
direction	O
methods	O
in	O
optimization	O
(	O
1980	O
)	O
13	O
kallianpur	O
,	O
stochastic	O
filtering	O
theory	O
(	O
1980	O
)	O
14	O
krylov	O
,	O
controlled	O
diffusion	O
processes	O
(	O
1980	O
)	O
15	O
prabhu	O
,	O
stochastic	O
storage	O
processes	O
:	O
queues	O
,	O
insurance	O
risk	O
,	O
and	O
dams	O
(	O
1980	O
)	O
16	O
ibragimov/has'minskii	O
,	O
statistical	O
estimation	B
:	O
asymptotic	O
theory	O
(	O
1981	O
)	O
17	O
cesari	O
,	O
optimization	O
:	O
theory	O
and	O
applications	O
(	O
1982	O
)	O
18	O
elliott	O
,	O
stochastic	O
calculus	O
and	O
applications	O
(	O
1982	O
)	O
19	O
marchuk/shaidourov	O
,	O
difference	O
methods	O
and	O
their	O
extrapolations	O
(	O
1983	O
)	O
20	O
hijab	O
,	O
stabilization	O
of	O
control	O
systems	O
(	O
1986	O
)	O
21	O
protter	O
,	O
stochastic	O
integration	O
and	O
differential	O
equations	O
(	O
1990	O
)	O
22	O
benveniste/metivier/priouret	O
,	O
adaptive	O
algorithms	O
and	O
stochastic	O
approximations	O
(	O
1990	O
)	O
23	O
kloedeniplaten	O
,	O
numerical	O
solution	O
of	O
stochastic	O
differential	O
equations	O
(	O
1992	O
)	O
24	O
kushner/dupuis	O
,	O
numerical	O
methods	O
for	O
stochastic	O
control	O
problems	O
in	O
continuous	O
time	O
(	O
1992	O
)	O
25	O
fleming/soner	O
,	O
controlled	O
markov	O
processes	O
and	O
viscosity	O
solutions	O
(	O
1993	O
)	O
26	O
baccellilbremaud	O
,	O
elements	O
of	O
queueing	O
theory	O
(	O
1994	O
)	O
27	O
winkler	O
,	O
image	O
analysis	O
,	O
random	O
fields	O
,	O
and	O
dynamic	O
monte	O
carlo	O
methods	O
:	O
an	O
introduction	O
to	O
mathematical	O
aspects	O
(	O
1994	O
)	O
28	O
kalpazidou	O
,	O
cycle	O
representations	O
of	O
markov	O
processes	O
(	O
1995	O
)	O
29	O
elliott	O
!	O
aggounimoore	O
,	O
hidden	O
markov	O
models	O
:	O
estimation	B
and	O
control	O
(	O
1995	O
)	O
30	O
hernandez-lermailasserre	O
,	O
discrete-time	O
markov	O
control	O
processes	O
:	O
basic	O
optimality	O
criteria	O
(	O
1996	O
)	O
31	O
devroye/gyorfl/lugosi	O
,	O
a	O
probabilistic	O
theory	O
of	O
pattern	O
recognition	O
(	O
1996	O
)	O
32	O
maitraisudderth	O
,	O
discrete	O
gambling	O
and	O
stochastic	O
games	O
(	O
1996	O
)	O
luc	O
devroye	O
laszlo	O
gyorfi	O
gabor	O
lugosi	O
a	O
probabilistic	O
theory	O
of	O
pattern	O
recognition	O
with	O
99	O
figures	O
springer	O
lasz16	O
gyorfi	O
gabor	O
lugosi	O
department	O
of	O
mathematics	O
and	O
computer	O
science	O
technical	O
university	O
of	O
budapest	O
budapest	O
hungary	O
luc	O
devroye	O
school	O
of	O
computer	O
science	O
mcgill	O
university	O
montreal	O
,	O
quebec	O
,	O
h3a	O
2a	O
7	O
canada	O
managing	O
editors	O
i.	O
karatzas	O
department	O
of	O
statistics	O
columbia	O
university	O
new	O
york	O
,	O
ny	O
10027	O
,	O
usa	O
m.	O
yor	O
cnrs	O
,	O
laboratoire	O
de	O
probabilites	O
universite	O
pierre	O
et	O
marie	O
curie	O
4	O
,	O
place	O
jussieu	O
,	O
tour	O
56	O
f-75252	O
paris	O
cedex	O
os	O
,	O
france	O
mathematics	O
subject	O
classification	O
(	O
1991	O
)	O
:	O
68t1o	O
,	O
68t05	O
,	O
62g07	O
,	O
62h30	O
library	O
of	O
congress	O
cataloging-in-publication	O
data	O
devroye	O
,	O
luc	O
.	O
a	O
probabilistic	O
theory	O
of	O
pattern	O
recognition/luc	O
devroye	O
,	O
laszlo	O
gyorfi	O
,	O
gabor	O
lugosi	O
.	O
p.	O
cm	O
.	O
includes	O
bibliographical	O
references	O
and	O
index	O
.	O
isbn	O
0-387-94618-7	O
(	O
hardcover	O
)	O
1.	O
pattern	O
perception	O
.	O
2.	O
probabilities	O
.	O
1.	O
gyorfi	O
,	O
laszlo	O
.	O
ii	O
.	O
lugosi	O
,	O
gabor	O
.	O
q327.d5	O
1996	O
003	O
'	O
.52'015192-dc20	O
iii	O
.	O
title	O
.	O
95-44633	O
printed	O
on	O
acid-free	O
paper	O
.	O
©	O
1996	O
springer-verlag	O
new	O
york	O
,	O
inc.	O
all	O
rights	O
reserved	O
.	O
this	O
work	O
may	O
not	O
be	O
translated	O
or	O
copied	O
in	O
whole	O
or	O
in	O
part	O
without	O
the	O
written	O
permission	O
of	O
the	O
publisher	O
(	O
springer-verlag	O
new	O
york	O
,	O
inc.	O
,	O
175	O
fifth	O
avenue	O
,	O
new	O
york	O
,	O
ny	O
10010	O
,	O
usa	O
)	O
,	O
except	O
for	O
brief	O
excerpts	O
in	O
connection	O
with	O
reviews	O
or	O
scholarly	O
analysis	O
.	O
use	O
in	O
connection	O
with	O
any	O
form	O
of	O
information	O
storage	O
and	O
retrieval	O
,	O
electronic	O
adaptation	O
,	O
computer	O
software	O
,	O
or	O
by	O
similar	O
or	O
dissimilar	O
methodology	O
now	O
known	O
or	O
here	O
(	O
cid:173	O
)	O
after	O
developed	O
is	O
forbidden	O
.	O
the	O
use	O
of	O
general	O
descriptive	O
names	O
,	O
trade	O
names	O
,	O
trademarks	O
,	O
etc.	O
,	O
in	O
this	O
publication	O
,	O
even	O
if	O
the	O
former	O
are	O
not	O
especially	O
identified	O
,	O
is	O
not	O
to	O
be	O
taken	O
as	O
a	O
sign	O
that	O
such	O
names	O
,	O
as	O
understood	O
by	O
the	O
trade	O
marks	O
and	O
merchandise	O
marks	O
act	O
,	O
may	O
accordingly	O
be	O
used	O
freely	O
by	O
anyone	O
.	O
production	O
managed	O
by	O
francine	O
mcneill	O
;	O
manufacturing	O
supervised	B
by	O
jeffrey	O
taub	O
.	O
photocomposed	O
copy	O
prepared	O
using	O
springer	O
's	O
svsing.sty	O
macro	O
.	O
printed	O
and	O
bound	O
by	O
braun-brumfield	O
,	O
inc.	O
,	O
ann	O
arbor	O
,	O
mi	O
.	O
printed	O
in	O
the	O
united	O
states	O
of	O
america	O
.	O
9	O
8	O
7	O
6	O
5	O
4	O
3	O
2	O
(	O
corrected	O
second	O
printing	O
.	O
1997	O
)	O
isbn	O
0-387-94618-7	O
springer-verlag	O
new	O
york	O
berlin	O
heidelberg	O
spin	O
10565785	O
preface	O
life	O
is	O
just	O
a	O
long	O
random	O
walk	O
.	O
things	O
are	O
created	O
because	O
the	O
circumstances	O
happen	O
to	O
be	O
right	O
.	O
more	O
often	O
than	O
not	O
,	O
creations	O
,	O
such	O
as	O
this	O
book	O
,	O
are	O
acci	O
(	O
cid:173	O
)	O
dental	O
.	O
nonparametric	O
estimation	B
came	O
to	O
life	O
in	O
the	O
fifties	O
and	O
sixties	O
and	O
started	O
developing	O
at	O
a	O
frenzied	O
pace	O
in	O
the	O
late	O
sixties	O
,	O
engulfing	O
pattern	O
recognition	O
in	O
its	O
growth	O
.	O
in	O
the	O
mid-sixties	O
,	O
two	O
young	O
men	O
,	O
tom	O
cover	O
and	O
peter	O
hart	O
,	O
showed	O
the	O
world	O
that	O
the	O
nearest	B
neighbor	I
rule	I
in	O
all	O
its	O
simplicity	O
was	O
guaranteed	O
to	O
err	O
at	O
most	O
twice	O
as	O
often	O
as	O
the	O
best	O
possible	O
discrimination	O
method	O
.	O
tom	O
's	O
results	O
had	O
a	O
profound	O
influence	O
on	O
terry	O
wagner	O
,	O
who	O
became	O
a	O
professor	O
at	O
the	O
university	O
of	O
texas	O
at	O
austin	O
and	O
brought	O
probabilistic	O
rigor	O
to	O
the	O
young	O
field	O
of	O
nonpara	O
(	O
cid:173	O
)	O
metric	B
estimation	O
.	O
around	O
1971	O
,	O
vapnik	O
and	O
chervonenkis	O
started	O
publishing	O
a	O
revolutionary	O
series	O
of	O
papers	O
with	O
deep	O
implications	O
in	O
pattern	O
recognition	O
,	O
but	O
their	O
work	O
was	O
not	O
well	O
known	O
at	O
the	O
time	O
.	O
however	O
,	O
tom	O
and	O
terry	O
had	O
noticed	O
the	O
potential	O
of	O
the	O
work	O
,	O
and	O
terry	O
asked	O
luc	O
devroye	O
to	O
read	O
that	O
work	O
in	O
prepa	O
(	O
cid:173	O
)	O
ration	O
for	O
his	O
ph.d.	O
dissertation	O
at	O
the	O
university	O
of	O
texas	O
.	O
the	O
year	O
was	O
1974.	O
luc	O
ended	O
up	O
in	O
texas	O
quite	O
by	O
accident	O
thanks	O
to	O
a	O
tip	O
by	O
his	O
friend	O
and	O
fellow	O
belgian	O
willy	O
wouters	O
,	O
who	O
matched	O
him	O
up	O
with	O
terry	O
.	O
by	O
the	O
time	O
luc	O
's	O
dissertation	O
was	O
published	O
in	O
1976	O
,	O
pattern	O
recognition	O
had	O
taken	O
off	O
in	O
earnest	O
.	O
on	O
the	O
theoret	O
(	O
cid:173	O
)	O
ical	O
side	O
,	O
important	O
properties	O
were	O
still	O
being	O
discovered	O
.	O
in	O
1977	O
,	O
stone	O
stunned	O
the	O
nonparametric	O
community	O
by	O
showing	O
that	O
there	O
are	O
iionparametric	O
rules	O
that	O
are	O
convergent	O
for	O
all	O
distributions	O
of	O
the	O
data	O
.	O
this	O
is	O
called	O
distribution-free	O
or	O
universal	B
consistency	I
,	O
and	O
it	O
is	O
what	O
makes	O
nonparametric	O
methods	O
so	O
attractive	O
.	O
yet	O
,	O
very	O
few	O
researchers	O
were	O
concerned	O
with	O
universal	O
consistency-one	O
notable	O
exception	O
was	O
laci	O
gyorfi	O
,	O
who	O
at	O
that	O
time	O
worked	O
in	O
budapest	O
amid	O
an	O
energetic	O
group	O
of	O
nonparametric	O
specialists	O
that	O
included	O
sandor	O
csibi	O
,	O
j6zsef	O
fritz	O
,	O
and	O
pal	O
revesz	O
.	O
vi	O
preface	O
so	O
,	O
linked	O
by	O
a	O
common	O
vision	O
,	O
luc	O
and	O
laci	O
decided	O
to	O
join	O
forces	O
in	O
the	O
early	O
eighties	O
.	O
in	O
1982	O
,	O
they	O
wrote	O
six	O
chapters	O
of	O
a	O
book	O
on	O
nonparametric	O
regression	B
function	I
estimation	O
,	O
but	O
these	O
were	O
never	O
published	O
.	O
in	O
fact	O
,	O
the	O
notes	O
are	O
still	O
in	O
drawers	O
in	O
their	O
offices	O
today	O
.	O
they	O
felt	O
that	O
the	O
subject	O
had	O
not	O
matured	O
yet	O
.	O
a	O
book	O
on	O
nonparametric	O
density	B
estimation	I
saw	O
the	O
light	O
in	O
1985.	O
unfortunately	O
,	O
as	O
true	O
baby-boomers	O
,	O
neither	O
luc	O
nor	O
laci	O
had	O
the	O
time	O
after	O
1985	O
to	O
write	O
a	O
text	O
on	O
nonpararnetric	O
pattern	O
recognition	O
.	O
enter	O
gabor	O
lugosi	O
,	O
who	O
obtained	O
his	O
doctoral	O
degree	O
under	O
laci	O
's	O
supervision	O
in	O
1991.	O
gabor	O
had	O
prepared	O
a	O
set	O
of	O
rough	O
course	O
notes	O
on	O
the	O
subject	O
around	O
1992	O
and	O
proposed	O
to	O
coordinate	O
the	O
project-this	O
book-in	O
1993.	O
with	O
renewed	O
energy	O
,	O
we	O
set	O
out	O
to	O
write	O
the	O
book	O
that	O
we	O
should	O
have	O
written	O
at	O
least	O
ten	O
years	O
ago	O
.	O
discussions	O
and	O
work	O
sessions	O
were	O
held	O
in	O
budapest	O
,	O
montreal	O
,	O
leuven	O
,	O
and	O
louvain-la-neuve	O
.	O
in	O
leuven	O
,	O
our	O
gracious	O
hosts	O
were	O
ed	O
van	O
der	O
meulen	O
and	O
jan	O
beirlant	O
,	O
and	O
in	O
louvain	O
(	O
cid:173	O
)	O
la-neuve	O
,	O
we	O
were	O
gastronomically	O
and	O
spiritually	O
supported	O
by	O
leopold	O
simar	O
and	O
irene	O
gijbels	O
.	O
we	O
thank	O
all	O
of	O
them	O
.	O
new	O
results	O
accumulated	O
,	O
and	O
we	O
had	O
to	O
resist	O
the	O
temptation	O
to	O
publish	O
these	O
in	O
journals	O
.	O
finally	O
,	O
in	O
may	O
1995	O
,	O
the	O
manuscript	O
had	O
bloated	O
to	O
such	O
extent	O
that	O
it	O
had	O
to	O
be	O
sent	O
to	O
the	O
publisher	O
,	O
for	O
otherwise	O
it	O
would	O
have	O
become	O
an	O
encyclopedia	O
.	O
some	O
important	O
unanswered	O
questions	O
were	O
quickly	O
turned	O
into	O
masochistic	O
exercises	O
or	O
wild	O
conjectures	O
.	O
we	O
will	O
explain	O
subject	O
selection	B
,	O
classroom	O
use	O
,	O
chapter	O
dependence	O
,	O
and	O
personal	O
viewpoints	O
in	O
the	O
introduction	O
.	O
we	O
do	O
apologize	O
,	O
of	O
course	O
,	O
for	O
all	O
remaining	O
errors	O
.	O
we	O
were	O
touched	O
,	O
influenced	O
,	O
guided	O
,	O
and	O
taught	O
by	O
many	O
people	O
.	O
terry	O
wag	O
(	O
cid:173	O
)	O
ner	O
's	O
rigor	O
and	O
taste	O
for	O
beautiful	O
nonparametric	O
problems	O
have	O
infected	O
us	O
for	O
life	O
.	O
we	O
thank	O
our	O
past	O
and	O
present	O
coauthors	O
on	O
nonpararnetric	O
papers	O
,	O
alain	O
berlinet	O
,	O
michel	O
broniatowski	O
,	O
ricardo	O
cao	O
,	O
paul	O
deheuvels	O
,	O
andras	O
farago	O
,	O
adam	O
krzyzak	O
,	O
tamas	O
linder	O
,	O
andrew	O
nobel	O
,	O
mirek	O
pawlak	O
,	O
igor	O
vajda	O
,	O
harro	O
walk	O
,	O
and	O
ken	O
zeger	O
.	O
tamas	O
linder	O
read	O
most	O
of	O
the	O
book	O
and	O
provided	O
invalu	O
(	O
cid:173	O
)	O
able	O
feedback	O
.	O
his	O
help	O
is	O
especially	O
appreciated	O
.	O
several	O
chapters	O
were	O
critically	O
read	O
by	O
students	O
in	O
budapest	O
.	O
we	O
thank	O
all	O
of	O
them	O
,	O
especially	O
andras	O
antos	O
,	O
miklos	O
csuros	O
,	O
balazs	O
kegl	O
,	O
istvan	O
pali	O
,	O
and	O
marti	O
pinter	O
.	O
finally	O
,	O
here	O
is	O
an	O
al	O
(	O
cid:173	O
)	O
phabetically	O
ordered	B
list	O
of	O
friends	O
who	O
directly	O
or	O
indirectly	O
contributed	O
to	O
our	O
knowledge	O
and	O
love	O
of	O
nonparametrics	O
:	O
andrew	O
and	O
roger	O
barron	O
,	O
denis	O
bosq	O
,	O
prabhir	O
burman	O
,	O
tom	O
cover	O
,	O
antonio	O
cuevas	O
,	O
pierre	O
devijver	O
,	O
ricardo	O
fraiman	O
,	O
ned	O
glick	O
,	O
wenceslao	O
gonzalez-manteiga	O
,	O
peter	O
hall	O
,	O
eiichi	O
isogai	O
,	O
ed	O
mack	O
,	O
arthur	O
nadas	O
,	O
georg	O
pflug	O
,	O
george	O
roussas	O
,	O
winfried	O
stute	O
,	O
tamas	O
szabados	O
,	O
godfried	O
toussaint	O
,	O
sid	O
yakowitz	O
,	O
and	O
yannis	O
yatracos	O
.	O
gabor	O
diligently	O
typed	O
the	O
entire	O
manuscript	O
and	O
coordinated	O
all	O
contributions	O
.	O
he	O
became	O
quite	O
a	O
texpert	O
in	O
the	O
process	O
.	O
several	O
figures	O
were	O
made	O
by	O
idraw	O
and	O
xi	O
ig	O
by	O
gabor	O
and	O
luc	O
.	O
most	O
of	O
the	O
drawings	O
were	O
directly	O
programmed	O
in	O
postscript	O
by	O
luc	O
and	O
an	O
undergraduate	O
student	O
at	O
mcgill	O
university	O
,	O
hisham	O
petry	O
,	O
to	O
whom	O
we	O
are	O
grateful	O
.	O
for	O
gabor	O
,	O
this	O
book	O
comes	O
at	O
the	O
beginning	O
of	O
his	O
career	O
.	O
unfortunately	O
,	O
the	O
other	O
two	O
authors	O
are	O
not	O
so	O
lucky	O
.	O
as	O
both	O
luc	O
and	O
laci	O
felt	O
that	O
they	O
would	O
probably	O
not	O
write	O
another	O
book	O
on	O
nonparametric	O
pattern	O
recognition-the	O
random	O
walk	O
must	O
go	O
on-they	O
decided	O
to	O
put	O
their	O
general	O
preface	O
vii	O
view	O
of	O
the	O
subject	O
area	O
on	O
paper	O
while	O
trying	O
to	O
separate	O
the	O
important	O
from	O
the	O
irrelevant	O
.	O
surely	O
,	O
this	O
has	O
contributed	O
to	O
the	O
length	O
of	O
the	O
text	O
.	O
so	O
far	O
,	O
our	O
random	O
excursions	O
have	O
been	O
happy	O
ones	O
.	O
coincidentally	O
,	O
luc	O
is	O
married	O
to	O
bea	O
,	O
the	O
most	O
understanding	O
woman	O
in	O
the	O
world	O
,	O
and	O
happens	O
to	O
have	O
two	O
great	O
daughters	O
,	O
natasha	O
and	O
birgit	O
,	O
who	O
do	O
not	O
stray	O
off	O
their	O
random	O
courses	O
.	O
similarly	O
,	O
laci	O
has	O
an	O
equally	O
wonderful	O
wife	O
,	O
kati	O
,	O
and	O
two	O
children	O
with	O
steady	O
compasses	O
,	O
kati	O
and	O
janos	O
.	O
during	O
the	O
preparations	O
of	O
this	O
book	O
,	O
gabor	O
met	O
a	O
wonderful	O
girl	O
,	O
arrate	O
.	O
they	O
have	O
recently	O
decided	O
to	O
tie	O
their	O
lives	O
together	O
.	O
on	O
the	O
less	O
amorous	O
and	O
glamorous	O
side	O
,	O
we	O
gratefully	O
acknowledge	O
the	O
research	O
support	B
of	O
nserc	O
canada	O
,	O
fcar	O
quebec	O
,	O
otka	O
hungary	O
,	O
and	O
the	O
exchange	O
program	O
between	O
the	O
hungarian	O
academy	O
of	O
sciences	O
and	O
the	O
royal	O
belgian	O
academy	O
of	O
sciences	O
.	O
early	O
versions	O
of	O
this	O
text	O
were	O
tried	O
out	O
in	O
some	O
classes	O
at	O
the	O
technical	O
university	O
of	O
budapest	O
,	O
katholieke	O
universiteit	O
leuven	O
,	O
universitat	O
stuttgart	O
,	O
and	O
universite	O
montpellier	O
ii	O
.	O
we	O
would	O
like	O
to	O
thank	O
those	O
students	O
for	O
their	O
help	O
in	O
making	O
this	O
a	O
better	O
book	O
.	O
montreal	O
,	O
quebec	O
,	O
canada	O
budapest	O
,	O
hungary	O
budapest	O
,	O
hungary	O
luc	O
devroye	O
laci	O
gyorfi	O
gabor	O
lugosi	O
contents	O
preface	O
1	O
introduction	O
2	O
the	O
bayes	O
error	O
another	O
simple	O
example	O
2.1	O
the	O
bayes	O
problem	O
2.2	O
a	O
simple	O
example	O
2.3	O
2.4	O
other	O
formulas	O
for	O
the	O
bayes	O
risk	O
2.5	O
2.6	O
problems	O
and	O
exercises	O
plug-in	O
decisions	O
bayes	O
error	O
versus	O
dimension	B
3	O
inequalities	O
and	O
alternate	O
distance	B
measures	O
3.1	O
measuring	O
discriminatory	O
information	O
3.2	O
the	O
kolmogorov	O
variational	O
distance	B
3.3	O
the	O
nearest	B
neighbor	I
error	I
3.4	O
the	O
bhattacharyya	O
affinity	O
3.5	O
entropy	B
3.6	O
jeffreys'divergence	O
3.7	O
f-errors	O
3.8	O
the	O
mahalanobis	O
distance	B
3.9	O
i-divergences	O
problems	O
and	O
exercises	O
v	O
1	O
9	O
9	O
11	O
12	O
14	O
15	O
17	O
18	O
21	O
21	O
22	O
22	O
23	O
25	O
27	O
28	O
30	O
31	O
35	O
x	O
contents	O
4	O
linear	O
discrimination	O
univariate	O
discrimination	O
and	O
stoller	O
splits	O
linear	O
discriminants	O
the	O
fisher	O
linear	B
discriminant	I
the	O
normal	B
distribution	I
empirical	O
risk	O
minimization	O
4.1	O
4.2	O
4.3	O
4.4	O
4.5	O
4.6	O
minimizing	O
other	O
criteria	O
problems	O
and	O
exercises	O
5	O
nearest	B
neighbor	I
rules	I
introduction	O
5.1	O
5.2	O
notation	O
and	O
simple	O
asymptotics	O
5.3	O
5.4	O
5.5	O
proof	O
of	O
stone	O
's	O
lemma	O
the	O
asymptotic	O
probability	O
of	O
error	O
the	O
asymptotic	O
error	O
probability	O
of	O
weighted	O
nearest	B
neighbor	I
rules	I
k-nearest	O
neighbor	O
rules	O
:	O
even	O
k	O
inequalities	O
for	O
the	O
probability	O
of	O
error	O
behavior	O
when	O
l	O
*	O
is	O
small	O
5.6	O
5.7	O
5.8	O
5.9	O
nearest	B
neighbor	I
rules	I
when	O
l	O
*	O
=	O
0	O
5.10	O
admissibility	O
of	O
the	O
nearest	O
neighbor	B
rule	I
5.11	O
the	O
(	O
k	O
,	O
i	O
)	O
-nearest	O
neighbor	B
rule	I
problems	O
and	O
exercises	O
6	O
consistency	B
universal	O
consistency	B
classification	O
and	O
regression	O
estimation	O
partitioning	B
rules	I
the	O
histogram	B
rule	I
stone	O
's	O
theorem	B
the	O
k-nearest	O
neighbor	B
rule	I
classification	O
is	O
easier	O
than	O
regression	B
function	I
estimation	O
smart	O
rules	O
6.1	O
6.2	O
6.3	O
6.4	O
6.5	O
6.6	O
6.7	O
6.8	O
problems	O
and	O
exercises	O
7	O
slow	O
rates	O
of	O
convergence	O
finite	O
training	O
sequence	O
slow	O
rates	O
7.1	O
7.2	O
problems	O
and	O
exercises	O
8	O
error	B
estimation	I
error	O
counting	O
8.1	O
8.2	O
hoeffding	O
's	O
inequality	B
8.3	O
8.4	O
error	B
estimation	I
without	O
testing	O
data	O
selecting	O
classifiers	O
39	O
40	O
44	O
46	O
47	O
49	O
54	O
56	O
61	O
61	O
63	O
66	O
69	O
71	O
74	O
75	O
78	O
80	O
81	O
81	O
83	O
91	O
91	O
92	O
94	O
95	O
97	O
100	O
101	O
106	O
107	O
111	O
111	O
113	O
118	O
121	O
121	O
122	O
124	O
125	O
contents	O
xi	O
estimating	O
the	O
bayes	O
error	O
8.5	O
problems	O
and	O
exercises	O
9	O
the	O
regular	B
histogram	O
rule	B
the	O
method	B
of	I
bounded	I
differences	I
strong	O
universal	B
consistency	I
9.1	O
9.2	O
problems	O
and	O
exercises	O
10	O
kernel	B
rules	I
10.1	O
consistency	B
10.2	O
proof	O
of	O
the	O
consistency	B
theorem	O
10.3	O
potential	O
function	O
rules	O
problems	O
and	O
exercises	O
11	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	B
rule	I
11.1	O
strong	B
consistency	I
11.2	O
breaking	O
distance	B
ties	O
11.3	O
recursive	B
methods	O
11.4	O
scale-invariant	O
rules	O
11.5	O
weighted	B
nearest	O
neighbor	O
rules	O
11.6	O
rotation-invariant	B
rules	O
11.7	O
relabeling	B
rules	O
problems	O
and	O
exercises	O
12	O
vapnik-chervonenkis	O
theory	O
12.1	O
empirical	B
error	I
minimization	O
12.2	O
fingering	B
12.3	O
the	O
glivenko-cantelli	O
theorem	B
12.4	O
uniform	B
deviations	I
of	O
relative	O
frequencies	O
from	O
probabilities	O
12.5	O
classifier	B
selection	I
12.6	O
sample	B
complexity	I
12.7	O
the	O
zero-error	O
case	O
12.8	O
extensions	O
problems	O
and	O
exercises	O
13	O
combinatorial	O
aspects	O
of	O
vapnik-chervonenkis	O
theory	O
13.1	O
shatter	O
coefficients	O
and	O
vc	B
dimension	I
13.2	O
shatter	O
coefficients	O
of	O
some	O
classes	O
13.3	O
linear	O
and	O
generalized	O
linear	O
discrimination	O
rules	O
13.4	O
convex	O
sets	O
and	O
monotone	O
layers	O
problems	O
and	O
exercises	O
14	O
lower	B
bounds	I
for	I
empirical	O
classifier	B
selection	I
14.1	O
minimax	O
lower	O
bounds	O
14.2	O
the	O
case	O
lc	O
=	O
0	O
14.3	O
classes	O
with	O
infinite	O
vc	B
dimension	I
128	O
129	O
133	O
133	O
138	O
142	O
147	O
149	O
153	O
159	O
161	O
169	O
170	O
174	O
176	O
177	O
178	O
179	O
180	O
182	O
187	O
187	O
191	O
192	O
196	O
199	O
201	O
202	O
206	O
208	O
215	O
215	O
219	O
224	O
226	O
229	O
233	O
234	O
234	O
238	O
xii	O
contents	O
14.4	O
the	O
case	O
lc	O
>	O
0	O
14.5	O
sample	B
complexity	I
problems	O
and	O
exercises	O
15	O
the	O
maximum	B
likelihood	I
principle	O
15.1	O
maximum	B
likelihood	I
:	O
the	O
formats	O
15.2	O
the	O
maximum	B
likelihood	I
method	O
:	O
regression	B
format	I
15.3	O
consistency	B
15.4	O
examples	O
15.5	O
classical	O
maximum	B
likelihood	I
:	O
distribution	B
format	I
problems	O
and	O
exercises	O
16	O
parametric	B
classification	I
16.1	O
example	O
:	O
exponential	B
families	O
16.2	O
standard	B
plug-in	O
rules	O
16.3	O
minimum	O
distance	O
estimates	O
16.4	O
empirical	B
error	I
minimization	O
problems	O
and	O
exercises	O
17	O
generalized	O
linear	O
discrimination	O
17.1	O
fourier	O
series	O
classification	O
17.2	O
generalized	O
linear	O
classification	O
problems	O
and	O
exercises	O
18	O
complexity	B
regularization	I
18.1	O
structural	B
risk	I
minimization	I
18.2	O
poor	O
approximation	O
properties	O
of	O
vc	O
classes	O
18.3	O
simple	B
empirical	I
covering	O
problems	O
and	O
exercises	O
19	O
condensed	B
and	O
edited	B
nearest	O
neighbor	O
rules	O
19.1	O
condensed	B
nearest	O
neighbor	O
rules	O
19.2	O
edited	B
nearest	O
neighbor	O
rules	O
19.3	O
sieves	O
and	O
prototypes	O
problems	O
and	O
exercises	O
20	O
tree	B
classifiers	O
invariance	B
20.1	O
20.2	O
trees	O
with	O
the	O
x-property	O
20.3	O
balanced	B
search	O
trees	O
20.4	O
binary	B
search	O
trees	O
20.5	O
the	O
chronological	B
k-d	O
tree	B
20.6	O
the	O
deep	B
k-d	O
tree	B
20.7	O
quadtrees	O
20.8	O
best	O
possible	O
perpendicular	O
splits	O
20.9	O
splitting	O
criteria	O
based	O
on	O
impurity	O
functions	O
239	O
245	O
247	O
249	O
249	O
250	O
253	O
256	O
260	O
261	O
263	O
266	O
267	O
270	O
275	O
276	O
279	O
280	O
285	O
287	O
289	O
290	O
297	O
297	O
300	O
303	O
303	O
309	O
309	O
312	O
315	O
318	O
319	O
322	O
326	O
328	O
332	O
333	O
334	O
336	O
20.10	O
a	O
consistent	O
splitting	B
criterion	I
20.11	O
bsp	O
trees	O
20.12	O
primitive	O
selection	B
20.13	O
constructing	O
consistent	O
tree	O
classifiers	O
20.14	O
a	O
greedy	O
classifier	B
problems	O
and	O
exercises	O
21	O
data-dependent	B
partitioning	O
introduction	O
21.1	O
21.2	O
a	O
vapnik-chervonenkis	O
inequality	B
for	O
partitions	O
21.3	O
consistency	B
21.4	O
statistically	B
equivalent	I
blocks	I
21.5	O
partitioning	B
rules	I
based	O
on	O
clustering	B
21.6	O
data-based	B
scaling	O
21.7	O
classification	O
trees	O
problems	O
and	O
exercises	O
22	O
splitting	B
the	I
data	I
22.1	O
the	O
holdout	B
estimate	O
22.2	O
consistency	B
and	O
asymptotic	B
optimality	I
22.3	O
nearest	B
neighbor	I
rules	I
with	O
automatic	B
scaling	O
22.4	O
classification	O
based	O
on	O
clustering	O
22.5	O
statistically	B
equivalent	I
blocks	I
22.6	O
binary	B
tree	O
classifiers	O
problems	O
and	O
exercises	O
23	O
the	O
resubstitution	B
estimate	O
23.1	O
the	O
resubstitution	B
estimate	O
23.2	O
histogram	O
rules	O
23.3	O
data-based	B
histograms	O
and	O
rule	B
selection	O
problems	O
and	O
exercises	O
24	O
deleted	B
estimates	O
of	O
the	O
error	O
probability	O
24.1	O
a	O
general	O
lower	O
bound	O
24.2	O
a	O
general	O
upper	O
bound	O
for	O
deleted	B
estimates	O
24.3	O
nearest	B
neighbor	I
rules	I
24.4	O
kernel	B
rules	I
24.5	O
histogram	O
rules	O
problems	O
and	O
exercises	O
25	O
automatic	B
kernel	O
rules	O
25.1	O
consistency	B
25.2	O
data	O
splitting	O
25.3	O
kernel	B
complexity	I
25.4	O
multiparameter	B
kernel	O
rules	O
contents	O
xiii	O
340	O
341	O
343	O
346	O
348	O
357	O
363	O
363	O
364	O
368	O
372	O
377	O
381	O
383	O
383	O
387	O
387	O
389	O
391	O
392	O
393	O
394	O
395	O
397	O
397	O
399	O
403	O
405	O
407	O
408	O
411	O
413	O
415	O
417	O
419	O
423	O
424	O
428	O
431	O
435	O
xiv	O
contents	O
25.5	O
kernels	O
of	O
infinite	O
complexity	O
25.6	O
on	O
minimizing	O
the	O
apparent	B
error	I
rate	I
25.7	O
minimizing	O
the	O
deleted	B
estimate	O
25.8	O
sieve	O
methods	O
25.9	O
squared	B
error	I
minimization	I
problems	O
and	O
exercises	O
26	O
automatic	B
nearest	O
neighbor	O
rules	O
26.1	O
consistency	B
26.2	O
data	O
splitting	O
26.3	O
data	O
splitting	O
for	O
weighted	B
nn	O
rules	O
26.4	O
reference	O
data	O
and	O
data	O
splitting	O
26.5	O
variable	B
metric	I
nn	O
rules	O
26.6	O
selection	B
of	O
k	O
based	O
on	O
the	O
deleted	B
estimate	O
problems	O
and	O
exercises	O
independent	O
components	O
27	O
hypercubes	O
and	O
discrete	O
spaces	O
27.1	O
multinomial	B
discrimination	I
27.2	O
quantization	B
27.3	O
27.4	O
boolean	O
classifiers	O
27.5	O
series	O
methods	O
for	O
the	O
hypercube	O
27.6	O
maximum	B
likelihood	I
27.7	O
kernel	B
methods	O
problems	O
and	O
exercises	O
28	O
epsilon	O
entropy	B
and	O
totally	O
bounded	O
sets	O
28.1	O
definitions	O
28.2	O
examples	O
:	O
totally	O
bounded	O
classes	O
28.3	O
skeleton	B
estimates	I
28.4	O
rate	B
of	I
convergence	I
problems	O
and	O
exercises	O
29	O
uniform	B
laws	I
of	I
large	I
numbers	I
29.1	O
minimizing	O
the	O
empirical	O
squared	O
error	O
29.2	O
uniform	B
deviations	I
of	O
averages	O
from	O
expectations	O
29.3	O
empirical	O
squared	O
error	O
minimization	O
29.4	O
proof	O
of	O
theorem	O
29.1	O
29.5	O
covering	B
numbers	O
and	O
shatter	O
coefficients	O
29.6	O
generalized	O
linear	O
classification	O
problems	O
and	O
exercises	O
30	O
neural	O
networks	O
30.1	O
multilayer	B
perceptrons	O
30.2	O
arrangements	O
436	O
439	O
441	O
444	O
445	O
446	O
451	O
451	O
452	O
453	O
454	O
455	O
457	O
458	O
461	O
461	O
464	O
466	O
468	O
470	O
472	O
474	O
474	O
479	O
479	O
480	O
482	O
485	O
486	O
489	O
489	O
490	O
493	O
494	O
496	O
501	O
505	O
507	O
507	O
511	O
30.3	O
approximation	O
by	O
neural	O
networks	O
30.4	O
vc	B
dimension	I
30.5	O
l1	O
error	O
minimization	O
30.6	O
the	O
adaline	O
and	O
padaline	O
30.7	O
polynomial	B
networks	O
30.8	O
kolmogorov-lorentz	O
networks	O
and	O
additive	O
models	O
30.9	O
projection	B
pursuit	I
30.10	O
radial	B
basis	I
function	I
networks	O
problems	O
and	O
exercises	O
31	O
other	O
error	O
estimates	O
31.1	O
smoothing	O
the	O
error	O
count	O
31.2	O
posterior	B
probability	I
estimates	O
31.3	O
rotation	B
estimate	O
31.4	O
bootstrap	B
problems	O
and	O
exercises	O
32	O
feature	B
extraction	I
32.1	O
dimensionality	O
reduction	O
32.2	O
transformations	O
with	O
small	O
distortion	O
32.3	O
admissible	O
and	O
sufficient	O
transformations	O
problems	O
and	O
exercises	O
appendix	O
probability	O
inequalities	O
a.l	O
basics	O
of	O
measure	O
theory	O
a.2	O
the	O
lebesgue	O
integral	O
a.3	O
denseness	B
results	O
a.4	O
a.5	O
a.6	O
convergence	O
of	O
random	O
variables	O
a.7	O
conditional	O
expectation	O
a.8	O
the	O
binomial	B
distribution	I
a.9	O
the	O
hypergeometric	B
distribution	I
a.i0	O
the	O
multinomial	B
distribution	I
a.ll	O
the	O
exponential	B
and	O
gamma	B
distributions	O
a.12	O
the	O
multivariate	B
normal	I
distribution	I
notation	O
references	O
author	O
index	O
subject	O
index	O
contents	O
xv	O
517	O
521	O
526	O
531	O
532	O
534	O
538	O
540	O
542	O
549	O
549	O
554	O
556	O
556	O
559	O
561	O
561	O
567	O
569	O
572	O
575	O
575	O
576	O
579	O
581	O
582	O
584	O
585	O
586	O
589	O
589	O
590	O
590	O
591	O
593	O
619	O
627	O
1	O
introduction	O
pattern	O
recognition	O
or	O
discrimination	O
is	O
about	O
guessing	O
or	O
predicting	O
the	O
unknown	O
nature	O
of	O
an	O
observation	O
,	O
a	O
discrete	O
quantity	O
such	O
as	O
black	O
or	O
white	O
,	O
one	O
or	O
zero	O
,	O
sick	O
or	O
healthy	O
,	O
real	O
or	O
fake	O
.	O
an	O
observation	O
is	O
a	O
collection	O
of	O
numerical	O
measurements	O
such	O
as	O
an	O
image	O
(	O
which	O
is	O
a	O
sequence	O
of	O
bits	O
,	O
one	O
per	O
pixel	O
)	O
,	O
a	O
vector	O
of	O
weather	O
data	O
,	O
an	O
electrocardiogram	O
,	O
or	O
a	O
signature	O
on	O
a	O
check	O
suitably	O
digitized	O
.	O
more	O
formally	O
,	O
an	O
observation	O
is	O
a	O
d-dimensional	O
vector	O
x.	O
the	O
unknown	O
nature	O
of	O
the	O
observation	O
is	O
called	O
a	O
class	O
.	O
it	O
is	O
denoted	O
by	O
y	O
and	O
takes	O
values	O
in	O
a	O
finite	O
set	O
{	O
i	O
,	O
2	O
,	O
...	O
,	O
m	O
}	O
.	O
in	O
pattern	O
recognition	O
,	O
one	O
creates	O
a	O
function	O
g	O
(	O
x	O
)	O
:	O
nd	O
-*	O
{	O
i	O
,	O
...	O
,	O
m	O
}	O
which	O
represents	O
one	O
's	O
guess	O
of	O
y	O
given	O
x.	O
the	O
mapping	O
g	O
is	O
called	O
a	O
classifier	O
.	O
our	O
classifier	B
errs	O
on	O
x	O
if	O
g	O
(	O
x	O
)	O
i	O
y.	O
how	O
one	O
creates	O
a	O
rule	O
g	O
depends	O
upon	O
the	O
problem	O
at	O
hand	O
.	O
experts	O
can	O
be	O
called	O
for	O
medical	O
diagnoses	O
or	O
earthquake	O
predictions-they	O
try	O
to	O
mold	O
g	O
to	O
their	O
own	O
knowledge	O
and	O
experience	O
,	O
often	O
by	O
trial	O
and	O
error	O
.	O
theoretically	O
,	O
each	O
expert	O
operates	O
with	O
a	O
built-in	O
classifier	B
g	O
,	O
but	O
describing	O
this	O
g	O
explicitly	O
in	O
mathematical	O
form	O
is	O
not	O
a	O
sinecure	O
.	O
the	O
sheer	O
magnitude	O
and	O
richness	O
of	O
the	O
space	O
of	O
x	O
may	O
defeat	O
even	O
the	O
best	O
expert-it	O
is	O
simply	O
impossible	O
to	O
specify	O
g	O
for	O
all	O
possible	O
x	O
's	O
one	O
is	O
likely	O
to	O
see	O
in	O
the	O
future	O
.	O
we	O
have	O
to	O
be	O
prepared	O
to	O
live	O
with	O
imperfect	O
classifiers	O
.	O
in	O
fact	O
,	O
how	O
should	O
we	O
measure	B
the	O
quality	O
of	O
a	O
classifier	O
?	O
we	O
ca	O
n't	O
just	O
dismiss	O
a	O
classifier	O
just	O
because	O
it	O
misclassifies	O
a	O
particular	O
x.	O
for	O
one	O
thing	O
,	O
if	O
the	O
observation	O
does	O
not	O
fully	O
describe	O
the	O
underlying	O
process	O
(	O
that	O
is	O
,	O
if	O
y	O
is	O
not	O
a	O
deterministic	O
function	O
of	O
x	O
)	O
,	O
it	O
is	O
possible	O
that	O
the	O
same	O
x	O
may	O
give	O
rise	O
to	O
two	O
different	O
y	O
's	O
on	O
different	O
occasions	O
.	O
for	O
example	O
,	O
if	O
we	O
just	O
measure	B
water	O
content	O
of	O
a	O
person	O
's	O
body	O
,	O
and	O
we	O
find	O
that	O
the	O
person	O
is	O
dehydrated	O
,	O
then	O
the	O
cause	O
(	O
the	O
class	O
)	O
may	O
range	O
from	O
a	O
low	O
water	O
intake	O
in	O
hot	O
weather	O
to	O
severe	O
diarrhea	O
.	O
thus	O
,	O
we	O
introduce	O
a	O
probabilistic	O
setting	O
,	O
and	O
let	O
(	O
x	O
,	O
y	O
)	O
be	O
an	O
n	O
d	O
x	O
{	O
i	O
,	O
...	O
,	O
m	O
}	O
-valued	O
2	O
1.	O
introduction	O
random	O
pair	O
.	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
describes	O
the	O
frequency	O
of	O
encountering	O
particular	O
pairs	O
in	O
practice	O
.	O
an	O
error	O
occurs	O
if	O
g	O
(	O
x	O
)	O
i	O
y	O
,	O
and	O
the	O
probability	O
of	O
error	O
for	O
a	O
classifier	O
g	O
is	O
l	O
(	O
g	O
)	O
=	O
p	O
{	O
g	O
(	O
x	O
)	O
i	O
y	O
}	O
.	O
there	O
is	O
a	O
best	O
possible	O
classifier	B
,	O
g*	O
,	O
which	O
is	O
defined	O
by	O
g*	O
=	O
arg	O
min	O
p	O
{	O
g	O
(	O
x	O
)	O
i	O
y	O
}	O
.	O
g	O
:	O
rd-+	O
{	O
i	O
,	O
...	O
,	O
m	O
}	O
note	O
that	O
g*	O
depends	O
upon	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
.	O
if	O
this	O
distribution	B
is	O
known	O
,	O
g*	O
may	O
be	O
computed	O
.	O
the	O
problem	O
of	O
finding	O
g*	O
is	O
bayes	O
j	O
problem	O
,	O
and	O
the	O
clas	O
(	O
cid:173	O
)	O
sifier	O
g*	O
is	O
called	O
the	O
bayes	O
classifier	B
(	O
or	O
the	O
bayes	O
rule	B
)	O
.	O
the	O
minimal	O
probability	O
of	O
error	O
is	O
called	O
the	O
bayes	O
error	O
and	O
is	O
denoted	O
by	O
l	O
*	O
=	O
l	O
(	O
g*	O
)	O
.	O
mostly	O
,	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
is	O
unknown	O
,	O
so	O
that	O
g*	O
is	O
unknown	O
too	O
.	O
we	O
do	O
not	O
consult	O
an	O
expert	O
to	O
try	O
to	O
reconstruct	O
g*	O
,	O
but	O
have	O
access	O
to	O
a	O
good	O
database	O
of	O
pairs	O
(	O
xi	O
,	O
yi	O
)	O
,	O
1	O
:	O
:	O
:	O
:	O
i	O
:	O
:	O
:	O
;	O
n	O
,	O
observed	O
in	O
the	O
past	O
.	O
this	O
database	O
may	O
be	O
the	O
result	O
of	O
experimental	O
observation	O
(	O
as	O
for	O
meteorological	O
data	O
,	O
fingerprint	O
data	O
,	O
ecg	O
data	O
,	O
or	O
handwritten	O
characters	O
)	O
.	O
it	O
could	O
also	O
be	O
obtained	O
through	O
an	O
expert	O
or	O
a	O
teacher	O
who	O
filled	O
in	O
the	O
yi	O
's	O
after	O
having	O
seen	O
the	O
x/so	O
to	O
find	O
a	O
classifier	O
g	O
with	O
a	O
small	O
probability	O
of	O
error	O
is	O
hopeless	O
unless	O
there	O
is	O
some	O
assurance	O
that	O
the	O
(	O
xi	O
,	O
yi	O
)	O
'sjointly	O
are	O
somehow	O
representative	O
of	O
the	O
unknown	O
distribution	B
.	O
we	O
shall	O
assume	O
in	O
this	O
book	O
that	O
(	O
xl	O
,	O
y1	O
)	O
,	O
.•	O
.	O
,	O
(	O
xn	O
,	O
yn	O
)	O
,	O
the	O
data	O
,	O
is	O
a	O
sequence	O
of	O
independent	O
identically	O
distributed	O
(	O
i.i.d	O
.	O
)	O
random	O
pairs	O
with	O
the	O
same	O
distribution	B
as	O
that	O
of	O
(	O
x	O
,	O
y	O
)	O
.	O
this	O
is	O
a	O
very	O
strong	B
assumption	O
indeed	O
.	O
however	O
,	O
some	O
theo	O
(	O
cid:173	O
)	O
retical	O
results	O
are	O
emerging	O
that	O
show	O
that	O
classifiers	O
based	O
on	O
slightly	O
dependent	O
data	O
pairs	O
and	O
on	O
i.i.d	O
.	O
data	O
pairs	O
behave	O
roughly	O
the	O
same	O
.	O
also	O
,	O
simple	O
models	O
are	O
easier	O
to	O
understand	O
are	O
more	O
amenable	O
to	O
interpretation	O
.	O
a	O
classifier	O
is	O
constructed	O
on	O
the	O
basis	O
of	O
xl	O
,	O
y1	O
,	O
•••	O
,	O
x	O
n	O
,	O
yn	O
and	O
is	O
denoted	O
by	O
gn~	O
y	O
is	O
guessed	O
by	O
gn	O
(	O
x~	O
xl	O
,	O
yi	O
,	O
.	O
,	O
.	O
,	O
xn	O
,	O
yn	O
)	O
.	O
the	O
process	O
of	O
constructing	O
gn	O
is	O
called	O
learning	B
,	O
supervised	B
learning	O
,	O
or	O
learning	B
with	O
a	O
teacher	O
.	O
the	O
performance	O
of	O
gn	O
is	O
measured	O
by	O
the	O
conditional	O
probability	O
of	O
error	O
this	O
is	O
a	O
random	O
variable	B
because	O
it	O
depends	O
upon	O
the	O
data	O
.	O
so	O
,	O
ln	O
averages	O
over	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
but	O
the	O
data	O
is	O
held	O
fixed	O
.	O
averaging	O
over	O
the	O
data	O
as	O
well	O
would	O
be	O
unnatural	O
,	O
because	O
in	O
a	O
given	O
application	O
,	O
one	O
has	O
to	O
live	O
with	O
the	O
data	O
at	O
hand	O
.	O
it	O
would	O
be	O
marginally	O
useful	O
to	O
know	O
the	O
number	O
eln	O
as	O
this	O
number	O
would	O
indicate	O
the	O
quality	O
of	O
an	O
average	O
data	O
sequence	O
,	O
not	O
your	O
data	O
sequence	O
.	O
this	O
text	O
is	O
thus	O
about	O
l	O
n	O
,	O
the	O
conditional	O
probability	O
of	O
error	O
.	O
an	O
individual	O
mapping	O
gn	O
:	O
nd	O
x	O
{	O
nd	O
x	O
{	O
l	O
,	O
...	O
,	O
m	O
}	O
}	O
n	O
--	O
-+	O
{	O
i	O
,	O
...	O
,	O
m	O
}	O
is	O
still	O
called	O
a	O
classifier	O
.	O
a	O
sequence	O
{	O
gn	O
,	O
n	O
:	O
:	O
:	O
:	O
i	O
}	O
is	O
called	O
a	O
(	O
discrimination	O
)	O
rule	B
.	O
thus	O
,	O
classifiers	O
are	O
functions	O
,	O
and	O
rules	O
are	O
sequences	O
of	O
functions	O
.	O
a	O
novice	O
might	O
ask	O
simple	O
questions	O
like	O
this	O
:	O
how	O
does	O
one	O
construct	O
a	O
good	O
classifier	B
?	O
how	O
good	O
can	O
a	O
classifier	O
be	O
?	O
is	O
classifier	B
a	O
better	O
than	O
classifier	B
b	O
?	O
can	O
we	O
estimate	B
how	O
good	O
a	O
classifier	O
is	O
?	O
what	O
is	O
the	O
best	O
classifier	B
?	O
this	O
book	O
partially	O
1.	O
introduction	O
3	O
answers	O
such	O
simple	O
questions	O
.	O
a	O
good	O
deal	O
of	O
energy	O
is	O
spent	O
on	O
the	O
mathematical	O
formulations	O
of	O
the	O
novice	O
's	O
questions	O
.	O
for	O
us	O
,	O
a	O
rule-not	O
a	O
classifier-is	O
good	O
if	O
it	O
is	O
consistent	O
,	O
that	O
is	O
,	O
if	O
lim	O
eln	O
=	O
l*	O
n-+oo	O
or	O
equivalently	O
,	O
if	O
ln	O
-7	O
l	O
*	O
in	O
probability	O
as	O
n	O
-7	O
00.	O
we	O
assume	O
that	O
the	O
reader	O
has	O
a	O
good	O
grasp	O
of	O
the	O
basic	O
elements	O
of	O
probability	O
,	O
including	O
notions	O
such	O
as	O
convergence	O
in	O
probability	O
,	O
strong	B
laws	O
of	O
large	O
numbers	O
for	O
averages	O
,	O
and	O
conditional	O
probability	O
.	O
a	O
selection	O
of	O
results	O
and	O
definitions	O
that	O
may	O
be	O
useful	O
for	O
this	O
text	O
is	O
given	O
in	O
the	O
appendix	O
.	O
a	O
consistent	O
rule	B
guarantees	O
us	O
that	O
taking	O
more	O
samples	O
essentially	O
suffices	O
to	O
roughly	O
reconstruct	O
the	O
unknown	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
because	O
ln	O
can	O
be	O
pushed	O
as	O
close	O
as	O
desired	O
to	O
l	O
*	O
.	O
in	O
other	O
words	O
,	O
infinite	O
amounts	O
of	O
information	O
can	O
be	O
gleaned	O
from	O
finite	O
samples	O
.	O
without	O
this	O
guarantee	O
,	O
we	O
would	O
not	O
be	O
motivated	O
to	O
take	O
more	O
samples	O
.	O
we	O
should	O
be	O
careful	O
and	O
not	O
impose	O
conditions	O
on	O
(	O
x	O
,	O
y	O
)	O
for	O
the	O
consistency	B
of	O
a	O
rule	O
,	O
because	O
such	O
conditions	O
may	O
not	O
be	O
verifiable	O
.	O
if	O
a	O
rule	O
is	O
consistent	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
,	O
itis	O
said	O
to	O
be	O
universally	O
consistent	O
.	O
interestingly	O
,	O
until	O
1977	O
,	O
it	O
was	O
not	O
known	O
if	O
a	O
universally	O
consistent	B
rule	I
existed	O
.	O
all	O
pre-1977	O
consistency	B
results	O
came	O
with	O
restrictions	O
on	O
(	O
x	O
,	O
y	O
)	O
.	O
in	O
1977	O
,	O
stone	O
showed	O
that	O
one	O
could	O
just	O
take	O
any	O
k-nearest	O
neighbor	B
rule	I
with	O
k	O
=	O
k	O
(	O
n	O
)	O
-7	O
00	O
and	O
kin	O
-7	O
0.	O
the	O
k-nearest	O
neighbor	O
classifier	O
gn	O
(	O
x	O
)	O
takes	O
a	O
majority	O
vote	O
over	O
the	O
y/s	O
in	O
the	O
subset	O
of	O
k	O
pairs	O
(	O
xi	O
,	O
yi	O
)	O
from	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
that	O
have	O
the	O
smallest	O
values	O
for	O
ii	O
xi	O
-	O
x	O
ii	O
(	O
i.e.	O
,	O
for	O
which	O
xi	O
is	O
closest	O
to	O
x	O
)	O
.	O
since	O
stone	O
's	O
proof	O
of	O
the	O
universal	B
consistency	I
of	O
the	O
k-nearest	O
neighbor	B
rule	I
,	O
several	O
other	O
rules	O
have	O
been	O
shown	O
to	O
be	O
universally	O
consistent	O
as	O
well	O
.	O
this	O
book	O
stresses	O
universality	O
and	O
hopefully	O
gives	O
a	O
reasonable	O
account	O
of	O
the	O
developments	O
in	O
this	O
direction	O
.	O
probabilists	O
may	O
wonder	O
why	O
we	O
did	O
not	O
use	O
convergence	O
with	O
probability	O
one	O
in	O
our	O
definition	B
of	I
consistency	O
.	O
indeed	O
,	O
strong	B
consistency-convergence	O
of	O
ln	O
to	O
l	O
*	O
with	O
probability	O
one-implies	O
convergence	O
for	O
almost	O
every	O
sample	O
as	O
it	O
grows	O
.	O
fortunately	O
,	O
for	O
most	O
well-behaved	O
rules	O
,	O
consistency	B
and	O
strong	B
consistency	I
are	O
equivalent	O
.	O
for	O
example	O
,	O
for	O
the	O
k-nearest	O
neighbor	B
rule	I
,	O
k	O
-7	O
00	O
and	O
kin	O
-7	O
°	O
together	O
imply	O
ln	O
-7	O
l	O
*	O
with	O
probability	O
one	O
.	O
the	O
equivalence	O
will	O
be	O
dealt	O
with	O
,	O
but	O
it	O
will	O
not	O
be	O
a	O
major	O
focus	O
of	O
attention	O
.	O
most	O
,	O
if	O
not	O
all	O
,	O
equivalence	O
results	O
are	O
based	O
upon	O
some	O
powerful	O
concentration	O
inequalities	O
such	O
as	O
mcdiarmid	O
's	O
.	O
for	O
example	O
,	O
we	O
will	O
be	O
able	O
to	O
show	O
that	O
for	O
the	O
k-nearest	O
neighbor	B
rule	I
,	O
there	O
exists	O
a	O
number	O
c	O
>	O
0	O
,	O
such	O
that	O
for	O
all	O
e	O
>	O
0	O
,	O
there	O
exists	O
n	O
(	O
e	O
)	O
>	O
°	O
depending	O
upon	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
such	O
that	O
p	O
{	O
ln	O
-	O
l	O
*	O
>	O
e	O
}	O
:	O
:	O
:	O
:	O
:	O
e-cne2	O
,	O
n	O
~	O
n	O
(	O
e	O
)	O
.	O
this	O
illustrates	O
yet	O
another	O
focus	O
of	O
the	O
book-inequalities	O
.	O
whenever	O
possible	O
,	O
we	O
make	O
a	O
case	O
or	O
conclude	O
a	O
proof	O
via	O
explicit	O
inequalities	O
.	O
various	O
parameters	O
can	O
be	O
substituted	O
in	O
these	O
inequalities	O
to	O
allow	O
the	O
user	O
to	O
draw	O
conclusions	O
regarding	O
sample	O
size	O
or	O
to	O
permit	O
identification	O
of	O
the	O
most	O
important	O
parameters	O
.	O
the	O
material	O
in	O
the	O
book	O
is	O
often	O
technical	O
and	O
dry	O
.	O
so	O
,	O
to	O
stay	O
focused	O
on	O
the	O
main	O
issues	O
,	O
we	O
keep	O
the	O
problem	O
simple	O
:	O
4	O
1.	O
introduction	O
a.	O
we	O
only	O
deal	O
with	O
binary	O
classification	O
(	O
m	O
==	O
2	O
)	O
.	O
the	O
class	O
y	O
takes	O
values	O
in	O
{	O
a	O
,	O
i	O
}	O
,	O
and	O
a	O
classifier	O
gn	O
is	O
a	O
mapping	O
:	O
rd	O
x	O
{	O
rd	O
x	O
{	O
a	O
,	O
l	O
}	O
}	O
n	O
~	O
{	O
a	O
,	O
i	O
}	O
.	O
b.	O
we	O
only	O
consider	O
i.i.d	O
.	O
data	O
sequences	O
.	O
we	O
also	O
disallow	O
active	O
learning	B
,	O
a	O
set-up	O
in	O
which	O
the	O
user	O
can	O
select	O
the	O
xi	O
's	O
deterministically	O
.	O
c.	O
we	O
do	O
not	O
consider	O
infinite	O
spaces	O
.	O
for	O
example	O
,	O
x	O
can	O
not	O
be	O
a	O
random	O
function	O
such	O
as	O
a	O
cardiogram	O
.	O
x	O
must	O
be	O
a	O
rd-valued	O
random	O
vector	O
.	O
the	O
reader	O
should	O
be	O
aware	O
that	O
many	O
results	O
given	O
here	O
may	O
be	O
painlessly	O
extended	O
to	O
certain	O
metric	B
spaces	O
of	O
infinite	O
dimension	B
.	O
let	O
us	O
return	O
to	O
our	O
novice	O
's	O
questions	O
.	O
we	O
know	O
that	O
there	O
are	O
good	O
rules	O
,	O
but	O
just	O
how	O
good	O
can	O
a	O
classifier	O
be	O
?	O
obviously	O
,	O
ln	O
:	O
:	O
:	O
:	O
l	O
*	O
in	O
all	O
cases	O
.	O
it	O
is	O
thus	O
important	O
to	O
know	O
l	O
*	O
or	O
to	O
estimate	B
it	O
,	O
for	O
if	O
l	O
*	O
is	O
large	O
,	O
any	O
classifier	B
,	O
including	O
yours	O
,	O
will	O
perform	O
poorly	O
.	O
but	O
even	O
if	O
l	O
*	O
were	O
zero	O
,	O
ln	O
could	O
still	O
be	O
large	O
.	O
thus	O
,	O
it	O
would	O
be	O
nice	O
to	O
have	O
explicit	O
inequalities	O
for	O
probabilities	O
such	O
as	O
p	O
{	O
ln	O
:	O
:	O
:	O
:	O
l*	O
+e	O
}	O
.	O
however	O
,	O
such	O
inequalities	O
must	O
necessarily	O
depend	O
upon	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
.	O
that	O
is	O
,	O
for	O
any	O
rule	B
,	O
lim	O
inf	O
n-+oo	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
with	O
l*+e	O
<	O
1/2	O
sup	O
p	O
{	O
ln	O
:	O
:	O
:	O
:	O
l	O
*	O
+	O
e	O
}	O
>	O
°	O
.	O
universal	B
rate	O
of	O
convergence	O
guarantees	O
do	O
not	O
exist	O
.	O
rate	B
of	I
convergence	I
studies	O
must	O
involve	O
certain	O
subclasses	O
of	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
.	O
for	O
this	O
reason	O
,	O
with	O
few	O
exceptions	O
,	O
we	O
will	O
steer	O
clear	O
of	O
the	O
rate	B
of	I
convergence	I
quicksand	O
.	O
even	O
if	O
there	O
are	O
no	O
universal	B
performance	O
guarantees	O
,	O
we	O
might	O
still	O
be	O
able	O
to	O
satisfy	O
our	O
novice	O
's	O
curiosity	O
if	O
we	O
could	O
satisfactorily	O
estimate	B
ln	O
for	O
the	O
rule	B
at	O
hand	O
by	O
a	O
function	O
ln	O
of	O
the	O
data	O
.	O
such	O
functions	O
are	O
called	O
error	O
estimates	O
.	O
for	O
example	O
,	O
for	O
the	O
k-nearest	O
neighbor	O
classifier	O
,	O
we	O
could	O
use	O
the	O
deleted	B
estimate	O
where	O
gni	O
(	O
xi	O
)	O
classifies	O
xi	O
by	O
the	O
k-nearest	O
neighbor	O
method	O
based	O
upon	O
the	O
data	O
(	O
x	O
1	O
,	O
yd	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
with	O
(	O
xi	O
,	O
yi	O
)	O
deleted	B
.	O
if	O
this	O
is	O
done	O
,	O
we	O
have	O
a	O
distribution	O
(	O
cid:173	O
)	O
free	O
inequality	B
--	O
-	O
6k	O
+	O
i	O
p	O
{	O
iln	O
-	O
ln	O
i	O
>	O
e	O
}	O
:	O
s	O
-	O
-	O
2	O
-	O
nf	O
(	O
the	O
rogers-wagner	O
inequality	B
)	O
,	O
provided	O
that	O
distance	B
ties	O
are	O
broken	O
in	O
an	O
ap	O
(	O
cid:173	O
)	O
propriate	O
manner	O
.	O
in	O
other	O
words	O
,	O
without	O
knowing	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
we	O
can	O
state	O
with	O
a	O
certain	O
confidence	B
that	O
ln	O
is	O
contained	O
in	O
[	O
ln	O
-	O
f	O
,	O
~1	O
+	O
f	O
]	O
.	O
thus	O
,	O
for	O
many	O
classifiers	O
,	O
it	O
is	O
indeed	O
possible	O
to	O
estimate	B
ln	O
from	O
the	O
data	O
at	O
hand	O
.	O
however	O
,	O
it	O
is	O
impossible	O
to	O
estimate	B
l	O
*	O
universally	O
well	O
:	O
for	O
any	O
n	O
,	O
and	O
any	O
es	O
(	O
cid:173	O
)	O
timate	O
of	O
l	O
*	O
based	O
upon	O
the	O
data	O
sequence	O
,	O
there	O
always	O
exists	O
a	O
distribution	O
of	O
(	O
x	O
,	O
y	O
)	O
for	O
which	O
the	O
estimate	B
is	O
arbitrarily	O
poor	O
.	O
can	O
we	O
compare	O
rules	O
{	O
gn	O
}	O
and	O
{	O
g~	O
}	O
?	O
again	O
,	O
the	O
answer	O
is	O
negative	O
:	O
there	O
exists	O
1.	O
introduction	O
5	O
no	O
``	O
best	O
''	O
classifier	B
(	O
or	O
superclassifier	B
)	O
,	O
as	O
for	O
any	O
rule	B
{	O
gn	O
}	O
,	O
there	O
exists	O
a	O
distri	O
(	O
cid:173	O
)	O
bution	O
of	O
(	O
x	O
,	O
y	O
)	O
and	O
another	O
rule	B
{	O
g~	O
}	O
such	O
that	O
for	O
all	O
n	O
,	O
e	O
{	O
l	O
(	O
g~	O
)	O
}	O
<	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
.	O
if	O
there	O
had	O
been	O
a	O
universally	O
best	O
classifier	B
,	O
this	O
book	O
would	O
have	O
been	O
unnec	O
(	O
cid:173	O
)	O
essary	O
:	O
we	O
would	O
all	O
have	O
to	O
use	O
it	O
all	O
the	O
time	O
.	O
this	O
nonexistence	O
implies	O
that	O
the	O
debate	O
between	O
practicing	O
pattern	O
recognizers	O
will	O
never	O
end	O
and	O
that	O
simulations	O
on	O
particular	O
examples	O
should	O
never	O
be	O
used	O
to	O
compare	O
classifiers	O
.	O
as	O
an	O
exam	O
(	O
cid:173	O
)	O
ple	O
,	O
consider	O
the	O
i-nearest	O
neighbor	B
rule	I
,	O
a	O
simple	O
but	O
not	O
universally	O
consistent	B
rule	I
.	O
yet	O
,	O
among	O
all	O
k-nearest	O
neighbor	O
classifiers	O
,	O
the	O
i-nearest	O
neighbor	O
classifier	O
is	O
admissible-there	O
are	O
distributions	O
for	O
which	O
its	O
expected	O
probability	O
of	O
error	O
is	O
better	O
than	O
for	O
any	O
k-nearest	O
neighbor	O
classifier	O
with	O
k	O
>	O
1.	O
so	O
,	O
it	O
can	O
never	O
be	O
totally	O
dismissed	O
.	O
thus	O
,	O
we	O
must	O
study	O
all	O
simple	O
rules	O
,	O
and	O
we	O
will	O
reserve	O
many	O
pages	O
for	O
the	O
nearest	B
neighbor	I
rule	I
and	O
its	O
derivatives	O
.	O
we	O
will	O
for	O
example	O
prove	O
the	O
cover-hart	O
inequality	B
(	O
cover	O
and	O
hart	O
,	O
1967	O
)	O
which	O
states	O
that	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
,	O
lim	O
sup	O
eln	O
:	O
s	O
2l	O
*	O
n-+oo	O
where	O
ln	O
is	O
the	O
probability	O
of	O
error	O
with	O
the	O
i-nearest	O
neighbor	B
rule	I
.	O
as	O
l	O
*	O
is	O
usually	O
small	O
(	O
for	O
otherwise	O
,	O
you	O
would	O
not	O
want	O
to	O
do	O
discrimination	O
)	O
,	O
2l	O
*	O
is	O
small	O
too	O
,	O
and	O
the	O
i-nearest	O
neighbor	B
rule	I
will	O
do	O
just	O
fine	O
.	O
the	O
nonexistence	O
of	O
a	O
best	O
classifier	B
may	O
disappoint	O
our	O
novice	O
.	O
however	O
,	O
we	O
may	O
change	O
the	O
setting	O
somewhat	O
and	O
limit	O
the	O
classifiers	O
to	O
a	O
certain	O
class	O
c	O
,	O
such	O
as	O
all	O
k-nearest	O
neighbor	O
classifiers	O
with	O
all	O
possible	O
values	O
for	O
k.	O
is	O
it	O
possible	O
to	O
select	O
the	O
best	O
classifier	B
from	O
this	O
class	O
?	O
phrased	O
in	O
this	O
manner	O
,	O
we	O
can	O
not	O
possibly	O
do	O
better	O
than	O
def	O
.	O
l	O
=	O
mf	O
p	O
{	O
gn	O
(	O
x	O
)	O
-r	O
y	O
.	O
...	O
./	O
}	O
gn	O
ec	O
typically	O
,	O
l	O
>	O
l	O
*	O
.	O
interestingly	O
,	O
there	O
is	O
a	O
general	O
paradigm	O
for	O
picking	O
classifiers	O
from	O
c	O
and	O
to	O
obtain	O
universal	B
performance	O
guarantees	O
.	O
it	O
uses	O
empirical	B
risk	I
min	O
(	O
cid:173	O
)	O
imization	O
,	O
a	O
method	O
studied	O
in	O
great	O
detail	O
in	O
the	O
work	O
of	O
vapnik	O
and	O
chervonenkis	O
(	O
1971	O
)	O
.	O
for	O
example	O
,	O
if	O
we	O
select	O
gn	O
from	O
c	O
by	O
minimizing	O
then	O
the	O
corresponding	O
probability	O
of	O
error	O
ln	O
satisfies	O
the	O
following	O
inequality	B
for	O
all	O
e	O
>	O
0	O
:	O
p	O
{	O
ln	O
>	O
l+e	O
}	O
:	O
:s	O
8	O
(	O
n	O
v	O
+i	O
)	O
e-ne	O
2	O
/128	O
.	O
here	O
v	O
>	O
0	O
is	O
an	O
integer	O
depending	O
upon	O
the	O
massiveness	O
of	O
c	O
only	O
.	O
v	O
is	O
called	O
the	O
vc	B
dimension	I
of	O
c	O
and	O
may	O
be	O
infinite	O
for	O
large	O
classes	O
c.	O
for	O
sufficiently	O
restricted	O
classes	O
c	O
,	O
v	O
is	O
finite	O
and	O
the	O
explicit	O
universal	B
bound	O
given	O
above	O
can	O
be	O
used	O
to	O
obtain	O
performance	O
guarantees	O
for	O
the	O
selected	O
g	O
n	O
(	O
relative	O
to	O
l	O
,	O
not	O
l	O
*	O
)	O
.	O
the	O
bound	O
above	O
is	O
only	O
valid	O
if	O
c	O
is	O
independent	O
of	O
the	O
data	O
pairs	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
.	O
fixed	O
classes	O
such	O
as	O
all	O
classifiers	O
that	O
decide	O
1	O
on	O
a	O
halfspace	O
and	O
0	O
on	O
its	O
complement	O
are	O
fine	O
.	O
we	O
may	O
also	O
sample	O
m	O
more	O
pairs	O
(	O
in	O
addition	O
to	O
the	O
n	O
pairs	O
6	O
1.	O
introduction	O
already	O
present	O
)	O
,	O
and	O
use	O
the	O
n	O
pairs	O
as	O
above	O
to	O
select	O
the	O
best	O
k	O
for	O
use	O
in	O
the	O
k-nearest	O
neighbor	O
classifier	O
based	O
on	O
the	O
m	O
pairs	O
.	O
as	O
we	O
will	O
see	O
,	O
the	O
selected	O
rule	B
is	O
universally	O
consistent	O
if	O
both	O
m	O
and	O
n	O
diverge	O
and	O
n	O
flog	O
m	O
--	O
+	O
00.	O
and	O
we	O
have	O
automatically	O
solved	O
the	O
problem	O
of	O
picking	O
k.	O
recall	O
that	O
stone	O
's	O
universal	B
consistency	I
theorem	O
only	O
told	O
us	O
to	O
pick	O
k	O
=	O
o	O
(	O
m	O
)	O
and	O
to	O
let	O
k	O
--	O
+	O
00	O
,	O
but	O
it	O
does	O
not	O
tell	O
us	O
whether	O
k	O
~	O
m	O
0.01	O
is	O
preferable	O
over	O
k	O
~	O
m	O
0.99.	O
empirical	B
risk	I
minimization	I
produces	O
a	O
random	O
data-dependent	B
k	O
that	O
is	O
not	O
even	O
guaranteed	O
to	O
tend	O
to	O
infinity	O
or	O
to	O
be	O
oem	O
)	O
,	O
yet	O
the	O
selected	O
rule	B
is	O
universally	O
consistent	O
.	O
we	O
offer	O
virtually	O
no	O
help	O
with	O
algorithms	O
as	O
in	O
standard	O
texts	O
,	O
with	O
two	O
notable	O
exceptions	O
.	O
ease	O
of	O
computation	O
,	O
storage	O
,	O
and	O
interpretation	O
has	O
spurred	O
the	O
de	O
(	O
cid:173	O
)	O
velopment	O
of	O
certain	O
rules	O
.	O
for	O
example	O
,	O
tree	B
classifiers	O
construct	O
a	O
tree	O
for	O
storing	O
the	O
data	O
,	O
and	O
partition	B
r/	O
by	O
certain	O
cuts	O
that	O
are	O
typically	O
perpendicular	O
to	O
a	O
co	O
(	O
cid:173	O
)	O
ordinate	O
axis	O
.	O
we	O
say	O
that	O
a	O
coordinate	O
axis	O
is	O
``	O
cut	O
.	O
''	O
such	O
classifiers	O
have	O
obvious	O
computational	O
advantages	O
,	O
and	O
are	O
amenable	O
to	O
interpretation-the	O
components	O
of	O
the	O
vector	O
x	O
that	O
are	O
cut	O
at	O
the	O
early	O
stages	O
of	O
the	O
tree	B
are	O
most	O
crucial	O
in	O
reaching	O
a	O
decision	O
.	O
expert	O
systems	O
,	O
automated	O
medical	O
diagnosis	O
,	O
and	O
a	O
host	O
of	O
other	O
recog	O
(	O
cid:173	O
)	O
nition	O
rules	O
use	O
tree	B
classification	O
.	O
for	O
example	O
,	O
in	O
automated	O
medical	O
diagnosis	O
,	O
one	O
may	O
first	O
check	O
a	O
patient	O
's	O
pulse	O
(	O
component	O
#	O
1	O
)	O
.	O
if	O
this	O
is	O
zero	O
,	O
the	O
patient	O
is	O
dead	O
.	O
if	O
it	O
is	O
below	O
40	O
,	O
the	O
patient	O
is	O
weak	B
.	O
the	O
first	O
component	O
is	O
cut	O
twice	O
.	O
in	O
each	O
case	O
,	O
we	O
may	O
then	O
consider	O
another	O
component	O
,	O
and	O
continue	O
the	O
breakdown	O
into	O
more	O
and	O
more	O
specific	O
cases	O
.	O
several	O
interesting	O
new	O
universally	O
consistent	O
tree	O
classifiers	O
are	O
described	O
in	O
chapter	O
20.	O
the	O
second	O
group	O
of	O
classifiers	O
whose	O
development	O
was	O
partially	O
based	O
upon	O
easy	O
implementations	O
is	O
the	O
class	O
of	O
neural	B
network	I
classifiers	I
,	O
descendants	O
of	O
rosenblatt	O
's	O
perceptron	B
(	O
rosenblatt	O
,	O
1956	O
)	O
.	O
these	O
classifiers	O
have	O
unknown	O
pa	O
(	O
cid:173	O
)	O
rameters	O
that	O
must	O
be	O
trained	O
or	O
selected	O
by	O
the	O
data	O
,	O
in	O
the	O
way	O
we	O
let	O
the	O
data	O
pick	O
k	O
in	O
the	O
k-nearest	O
neighbor	O
classifier	O
.	O
most	O
research	O
papers	O
on	O
neural	O
networks	O
deal	O
with	O
the	O
training	O
aspect	O
,	O
but	O
we	O
will	O
not	O
.	O
when	O
we	O
say	O
``	O
pick	O
the	O
parameters	O
by	O
empirical	B
risk	I
minimization	I
,	O
''	O
we	O
will	O
leave	O
the	O
important	O
algorithmic	O
complexity	O
questions	O
unanswered	O
.	O
perceptrons	O
divide	O
the	O
space	O
by	O
one	O
hyperplane	B
and	O
attach	O
decisions	O
1	O
and	O
0	O
to	O
the	O
two	O
halfspaces	O
.	O
such	O
simple	O
classifiers	O
are	O
not	O
consistent	O
except	O
for	O
a	O
few	O
distributions	O
.	O
this	O
is	O
the	O
case	O
,	O
for	O
example	O
,	O
when	O
x	O
takes	O
val	O
(	O
cid:173	O
)	O
ues	O
on	O
{	O
o	O
,	O
1	O
}	O
d	O
(	O
the	O
hypercube	O
)	O
and	O
the	O
components	O
of	O
x	O
are	O
independent	O
.	O
neural	O
networks	O
with	B
one	I
hidden	I
layer	I
are	O
universally	O
consistent	O
if	O
the	O
parameters	O
are	O
well-chosen	O
.	O
we	O
will	O
see	O
that	O
there	O
is	O
also	O
some	O
gain	O
in	O
considering	O
two	O
hidden	O
layers	O
,	O
but	O
that	O
it	O
is	O
not	O
really	O
necessary	O
to	O
go	O
beyond	O
two	O
.	O
complexity	O
of	O
the	O
training	O
algorithm-the	O
phase	O
in	O
which	O
a	O
classifier	O
gn	O
is	O
selected	O
from	O
c-is	O
of	O
course	O
important	O
.	O
sometimes	O
,	O
one	O
would	O
like	O
to	O
obtain	O
classifiers	O
that	O
are	O
invariant	O
under	O
certain	O
transformations	O
.	O
for	O
example	O
,	O
the	O
k	O
(	O
cid:173	O
)	O
nearest	B
neighbor	I
classifier	O
is	O
not	O
invariant	O
under	O
nonlinear	O
transformations	O
of	O
the	O
coordinate	O
axes	O
.	O
this	O
is	O
a	O
drawback	O
as	O
components	O
are	O
often	O
measurements	O
in	O
an	O
arbitrary	O
scale	O
.	O
switching	O
to	O
a	O
logarithmic	O
scale	O
or	O
stretching	O
a	O
scale	O
out	O
by	O
using	O
fahrenheit	O
instead	O
of	O
celsius	O
should	O
not	O
affect	O
good	O
discrimination	O
rules	O
.	O
there	O
exist	O
variants	O
of	O
the	O
k-nearest	O
neighbor	B
rule	I
that	O
have	O
the	O
given	O
invariance	B
.	O
in	O
character	O
recognition	O
,	O
sometimes	O
all	O
components	O
of	O
a	O
vector	O
x	O
that	O
represents	O
1.	O
introduction	O
7	O
a	O
character	O
are	O
true	O
measurements	O
involving	O
only	O
vector	O
differences	O
between	O
se	O
(	O
cid:173	O
)	O
lected	O
points	O
such	O
as	O
the	O
leftmost	O
and	O
rightmost	O
points	O
,	O
the	O
geometric	B
center	O
,	O
the	O
weighted	B
center	O
of	O
all	O
black	O
pixels	O
,	O
the	O
topmost	O
and	O
bottommost	O
points	O
.	O
in	O
this	O
case	O
,	O
the	O
scale	O
has	O
essential	O
information	O
,	O
and	O
invariance	B
with	O
respect	O
to	O
changes	O
of	O
scale	O
would	O
be	O
detrimental	O
.	O
here	O
however	O
,	O
some	O
invariance	B
with	O
respect	O
to	O
orthonormal	O
rotations	O
is	O
healthy	O
.	O
we	O
follow	O
the	O
standard	B
notation	O
from	O
textbooks	O
on	O
probability	O
.	O
thus	O
,	O
random	O
variables	O
are	O
uppercase	O
characters	O
such	O
as	O
x	O
,	O
y	O
,	O
and	O
z.	O
probability	O
measures	O
are	O
denoted	O
by	O
greek	O
letters	O
such	O
as	O
jl	O
and	O
1	O
)	O
.	O
numbers	O
and	O
vectors	O
are	O
denoted	O
by	O
lowercase	O
letters	O
such	O
as	O
a	O
,	O
b	O
,	O
c	O
,	O
x	O
,	O
and	O
y.	O
sets	O
are	O
also	O
denoted	O
by	O
roman	O
capitals	O
,	O
but	O
there	O
are	O
obvious	O
mnemonics	O
:	O
s	O
denotes	O
a	O
sphere	O
,	O
b	O
denotes	O
a	O
borel	O
set	O
,	O
and	O
so	O
forth	O
.	O
if	O
we	O
need	O
many	O
kinds	O
of	O
sets	O
,	O
we	O
will	O
typically	O
use	O
the	O
beginning	O
of	O
the	O
alphabet	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
.	O
most	O
functions	O
are	O
denoted	O
by	O
j	O
,	O
g	O
,	O
¢	O
,	O
and	O
0/	O
.	O
calligraphic	O
letters	O
such	O
as	O
a	O
,	O
c	O
,	O
and	O
:	O
f	O
are	O
used	O
to	O
denote	O
classes	O
of	O
functions	O
or	O
sets	O
.	O
a	O
short	O
list	O
of	O
frequently	O
used	O
symbols	O
is	O
found	O
at	O
the	O
end	O
of	O
the	O
book	O
.	O
at	O
the	O
end	O
of	O
this	O
chapter	O
,	O
you	O
will	O
find	O
a	O
directed	O
acyclic	O
graph	O
that	O
describes	O
the	O
dependence	O
between	O
chapters	O
.	O
clearly	O
,	O
prospective	O
teachers	O
will	O
have	O
to	O
select	O
small	O
subsets	O
of	O
chapters	O
.	O
all	O
chapters	O
,	O
without	O
exception	O
,	O
are	O
unashamedly	O
theo	O
(	O
cid:173	O
)	O
retical	O
.	O
we	O
did	O
not	O
scar	O
the	O
pages	O
with	O
backbreaking	O
simulations	O
or	O
quick	O
-and-dirty	O
engineering	O
solutions	O
.	O
the	O
methods	O
gleaned	O
from	O
this	O
text	O
must	O
be	O
supplemented	O
with	O
a	O
healthy	O
dose	O
of	O
engineering	O
savvy	O
.	O
ideally	O
,	O
students	O
should	O
have	O
a	O
com	O
(	O
cid:173	O
)	O
panion	O
text	O
filled	O
with	O
beautiful	O
applications	O
such	O
as	O
automated	O
virus	O
recognition	O
,	O
telephone	O
eavesdropping	O
language	O
recognition	O
,	O
voice	O
recognition	O
in	O
security	O
sys	O
(	O
cid:173	O
)	O
tems	O
,	O
fingerprint	O
recognition	O
,	O
or	O
handwritten	O
character	O
recognition	O
.	O
to	O
run	O
a	O
real	O
pattern	O
recognition	O
project	O
from	O
scratch	O
,	O
several	O
classical	O
texts	O
on	O
statistical	O
pat	O
(	O
cid:173	O
)	O
tern	O
recognition	O
could	O
and	O
should	O
be	O
consulted	O
,	O
as	O
our	O
work	O
is	O
limited	O
to	O
general	O
probability-theoretical	O
aspects	O
of	O
pattern	O
recognition	O
.	O
we	O
have	O
over	O
430	O
exercises	O
to	O
help	O
the	O
scholars	O
.	O
these	O
include	O
skill	O
honing	O
exercises	O
,	O
brainteasers	O
,	O
cute	O
puz	O
(	O
cid:173	O
)	O
zles	O
,	O
open	O
problems	O
,	O
and	O
serious	O
mathematical	O
challenges	O
.	O
there	O
is	O
no	O
solution	O
manual	O
.	O
this	O
book	O
is	O
only	O
a	O
start	O
.	O
use	O
it	O
as	O
a	O
toy-read	O
some	O
proofs	O
,	O
enjoy	O
some	O
inequalities	O
,	O
learn	O
new	O
tricks	O
,	O
and	O
study	O
the	O
art	O
of	O
camouflaging	O
one	O
problem	O
to	O
look	O
like	O
another	O
.	O
learn	O
for	O
the	O
sake	O
of	O
learning	O
.	O
8	O
1.	O
introduction	O
introduction	O
inequalities	O
and	O
alternate	O
distance	B
measures	O
1	O
2	O
the	O
bayes	O
error	O
3	O
4	O
linear	O
discrimination	O
5	O
nearest	B
neighbor	I
rules	I
6	O
consistency	B
7	O
slow	O
rates	O
of	O
convergence	O
8	O
error	B
estimation	I
9	O
the	O
regular	B
histogram	O
rule	B
10	O
kernel	B
rules	I
11	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	B
rule	I
12	O
vapnik-chervonenkis	O
theoi	O
)	O
'	O
13	O
combinatorial	O
aspects	O
of	O
v	O
apnik	O
-chervonenkis	O
theory	O
14	O
lower	B
bounds	I
for	I
empirical	O
classifier	B
selection	I
15	O
the	O
maximum	B
likelihood	I
principle	O
16	O
parametric	B
classification	I
17	O
generalized	O
linear	O
discrimination	O
18	O
complexity	B
regularization	I
19	O
condensed	B
and	O
edited	B
nearest	O
neighbor	O
rules	O
20	O
tree	B
classifiers	O
21	O
data-dependent	B
partitioning	O
22	O
splitting	B
the	I
data	I
23	O
the	O
resubstitution	B
estimate	O
24	O
deleted	B
estimates	O
of	O
the	O
error	O
probability	O
25	O
automatic	B
kernel	O
rules	O
26	O
automatic	B
nearest	O
neighbor	O
rules	O
27	O
hypercubes	O
and	O
discrete	O
spaces	O
28	O
epsilon	O
entropy	B
and	O
totally	O
bounded	O
sets	O
29	O
uniform	B
laws	I
of	I
large	I
numbers	I
30	O
neural	O
networks	O
31	O
other	O
error	O
estimates	O
32	O
feature	B
extraction	I
figure	O
1.1	O
.	O
2	O
the	O
bayes	O
error	O
2.1	O
the	O
bayes	O
problem	O
in	O
this	O
section	O
,	O
we	O
define	O
the	O
mathematical	O
model	O
and	O
introduce	O
the	O
notation	O
we	O
will	O
use	O
for	O
the	O
entire	O
book	O
.	O
let	O
(	O
x	O
,	O
y	O
)	O
be	O
a	O
pair	O
of	O
random	O
variables	O
taking	O
their	O
respective	O
values	O
from	O
rd	O
and	O
{	O
o	O
,	O
i	O
}	O
.	O
the	O
random	O
pair	O
(	O
x	O
,	O
y	O
)	O
may	O
be	O
described	O
in	O
a	O
variety	O
of	O
ways	O
:	O
for	O
example	O
,	O
it	O
is	O
defined	O
by	O
the	O
pair	O
(	O
il	O
,	O
1	O
]	O
)	O
,	O
where	O
il	O
is	O
the	O
probability	O
measure	B
for	O
x	O
and	O
1	O
]	O
is	O
the	O
regression	O
of	O
y	O
on	O
x.	O
more	O
precisely	O
,	O
for	O
a	O
borel-measurable	O
set	O
a	O
s	O
;	O
r	O
d	O
,	O
il	O
(	O
a	O
)	O
=	O
p	O
{	O
x	O
e	O
a	O
}	O
,	O
and	O
for	O
any	O
x	O
e	O
r	O
d	O
,	O
1	O
]	O
(	O
x	O
)	O
=	O
ply	O
=	O
llx	O
=	O
x	O
}	O
=	O
e	O
{	O
yix	O
=	O
x	O
}	O
.	O
thus	O
,	O
1	O
]	O
(	O
x	O
)	O
is	O
the	O
conditional	O
probability	O
that	O
y	O
is	O
1	O
given	O
x	O
=	O
x.	O
to	O
see	O
that	O
this	O
suffices	O
to	O
describe	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
observe	O
that	O
for	O
any	O
c	O
s	O
;	O
rd	O
x	O
{	O
o	O
,	O
i	O
}	O
,	O
we	O
have	O
c	O
=	O
(	O
c	O
n	O
(	O
rd	O
x	O
{	O
oj	O
)	O
)	O
u	O
(	O
c	O
n	O
(	O
rd	O
x	O
{	O
ij	O
)	O
)	O
d	O
;	O
t	O
co	O
x	O
{	O
oj	O
u	O
c1	O
x	O
{	O
i	O
}	O
,	O
and	O
p	O
{	O
(	O
x	O
,	O
y	O
)	O
e	O
c	O
}	O
p	O
{	O
x	O
e	O
co	O
,	O
y	O
=	O
o	O
}	O
+	O
p	O
{	O
x	O
e	O
c1	O
,	O
y	O
=	O
i	O
}	O
[	O
(	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
)	O
jl	O
(	O
dx	O
)	O
+	O
[	O
1	O
]	O
(	O
x	O
)	O
jl	O
(	O
dx	O
)	O
.	O
lco	O
lci	O
10	O
2.	O
the	O
bayes	O
error	O
as	O
this	O
is	O
valid	O
for	O
any	O
borel-measurable	O
set	O
c	O
,	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
is	O
deter	O
(	O
cid:173	O
)	O
mined	O
by	O
(	O
il	O
,	O
ry	O
)	O
.	O
the	O
function	O
ry	O
is	O
sometimes	O
called	O
the	O
a	B
posteriori	I
probability	I
.	O
any	O
function	O
g	O
:	O
n	O
d	O
-+	O
{	O
a	O
,	O
1	O
}	O
defines	O
a	O
classifier	O
or	O
a	O
decision	O
function	O
.	O
the	O
error	O
probability	O
of	O
g	O
is	O
l	O
(	O
g	O
)	O
=	O
p	O
{	O
g	O
(	O
x	O
)	O
=i	O
y	O
}	O
.	O
of	O
particular	O
interest	O
is	O
the	O
bayes	O
decision	O
function	O
*	O
(	O
x	O
)	O
=	O
{	O
1	O
g	O
if	O
ry	O
(	O
x	O
)	O
:	O
-	O
1/2	O
°	O
otherwise	O
.	O
this	O
decision	O
function	O
minimizes	O
the	O
error	O
probability	O
.	O
theorem	B
2.1.	O
for	O
any	O
decision	O
function	O
g	O
:	O
nd	O
-+	O
{	O
a	O
,	O
1	O
}	O
,	O
p	O
{	O
g*	O
(	O
x	O
)	O
=i	O
y	O
}	O
:	O
:	O
:	O
p	O
{	O
g	O
(	O
x	O
)	O
=i	O
y	O
}	O
,	O
that	O
is	O
,	O
g*	O
is	O
the	O
optimal	O
decision	O
.	O
proof	O
.	O
given	O
x	O
=	O
x	O
,	O
the	O
conditional	O
error	O
probability	O
of	O
any	O
decision	O
g	O
may	O
be	O
expressed	O
as	O
p	O
{	O
g	O
(	O
x	O
)	O
=i	O
yix	O
=	O
x	O
}	O
=	O
1	O
-	O
p	O
{	O
y	O
=	O
g	O
(	O
x	O
)	O
ix	O
=	O
x	O
}	O
=	O
1	O
-	O
=	O
1	O
-	O
=	O
1	O
-	O
(	O
p	O
{	O
y	O
=	O
1	O
,	O
g	O
(	O
x	O
)	O
=	O
11x	O
=	O
x	O
}	O
+p	O
{	O
y	O
=	O
0	O
,	O
g	O
(	O
x	O
)	O
=	O
0ix	O
=	O
xd	O
(	O
!	O
{	O
g	O
(	O
x	O
)	O
=l	O
}	O
p	O
{	O
y	O
=	O
iix	O
=	O
x	O
}	O
+	O
i	O
{	O
g	O
(	O
x	O
)	O
=o	O
}	O
p	O
{	O
y	O
=	O
oix	O
=	O
x	O
}	O
)	O
(	O
!	O
{	O
g	O
(	O
x	O
)	O
=l	O
}	O
ry	O
(	O
x	O
)	O
+	O
f	O
{	O
g	O
(	O
x	O
)	O
=o	O
}	O
(	O
1	O
-	O
ry	O
(	O
x	O
»	O
)	O
,	O
where	O
fa	O
denotes	O
the	O
indicator	O
of	O
the	O
set	O
a.	O
thus	O
,	O
for	O
every	O
x	O
e	O
n	O
d	O
,	O
p	O
{	O
g	O
(	O
x	O
)	O
=i	O
yix	O
=	O
x	O
}	O
-	O
p	O
{	O
g*	O
(	O
x	O
)	O
=i	O
yix	O
=	O
x	O
}	O
=	O
ry	O
(	O
x	O
)	O
(	O
!	O
{	O
g*	O
(	O
x	O
)	O
=l	O
)	O
-	O
f	O
{	O
gex	O
)	O
=l	O
}	O
)	O
+	O
(	O
l	O
-	O
ry	O
(	O
x	O
)	O
)	O
(	O
i	O
{	O
g*ex	O
)	O
=o	O
)	O
-	O
i	O
{	O
g	O
(	O
x	O
)	O
=o	O
}	O
)	O
(	O
2ry	O
(	O
x	O
)	O
-	O
1	O
)	O
(	O
i	O
{	O
g*ex	O
)	O
=l	O
)	O
-	O
f	O
{	O
g	O
(	O
x	O
)	O
=1l	O
)	O
=	O
>	O
°	O
by	O
the	O
definition	B
of	I
g*	O
.	O
the	O
statement	O
now	O
follows	O
by	O
integrating	O
both	O
sides	O
with	O
respect	O
to	O
il-	O
(	O
dx	O
)	O
.	O
0	O
decide	O
class	O
0	O
112	O
________________	O
_	O
figure	O
2.1.	O
the	O
bayes	O
decision	O
in	O
the	O
example	O
on	O
the	O
left	O
is	O
1	O
if	O
x	O
>	O
a	O
,	O
and	O
°	O
otherwise	O
.	O
2.2	O
a	O
simple	O
example	O
11	O
remark	O
.	O
g*	O
is	O
called	O
the	O
bayes	O
decision	O
and	O
l	O
*	O
=	O
p	O
{	O
g*	O
(	O
x	O
)	O
=i	O
y	O
}	O
is	O
referred	O
to	O
as	O
the	O
bayes	O
probability	O
of	O
error	O
,	O
bayes	O
error	O
,	O
or	O
bayes	O
risk	O
.	O
the	O
proof	O
given	O
above	O
reveals	O
that	O
l	O
(	O
g	O
)	O
=	O
1	O
-	O
e	O
{	O
i	O
{	O
g	O
(	O
x	O
)	O
=i	O
}	O
ry	O
(	O
x	O
)	O
+	O
i	O
{	O
g	O
(	O
x	O
)	O
=o	O
}	O
(	O
l	O
-	O
ry	O
(	O
x	O
»	O
}	O
,	O
and	O
in	O
particular	O
,	O
l	O
*	O
=	O
1	O
-	O
e	O
{	O
i	O
{	O
1j	O
(	O
x	O
»	O
1/2	O
}	O
ry	O
(	O
x	O
)	O
+	O
i	O
{	O
1j	O
(	O
x	O
)	O
:	O
s1/2	O
}	O
(	O
l	O
-	O
ry	O
(	O
x	O
»	O
}	O
.	O
0	O
we	O
observe	O
that	O
the	O
a	B
posteriori	I
probability	I
ry	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
l/x	O
=	O
x	O
}	O
=	O
e	O
{	O
y/x	O
=	O
x	O
}	O
minimizes	O
the	O
squared	B
error	I
when	O
y	O
is	O
to	O
be	O
predicted	O
by	O
f	O
(	O
x	O
)	O
for	O
some	O
function	O
f	O
:	O
rd	O
--	O
+	O
r	O
:	O
e	O
{	O
(	O
ry	O
(	O
x	O
)	O
-	O
y	O
)	O
2	O
}	O
:	O
:	O
:	O
e	O
{	O
(	O
f	O
(	O
x	O
)	O
_	O
y	O
)	O
2	O
}	O
.	O
to	O
see	O
why	O
the	O
above	O
inequality	B
is	O
true	O
,	O
observe	O
that	O
for	O
each	O
x	O
e	O
r	O
d	O
,	O
e	O
{	O
(	O
f	O
(	O
x	O
)	O
-	O
y	O
)	O
21x	O
=	O
x	O
}	O
=	O
e	O
{	O
(	O
f	O
(	O
x	O
)	O
-	O
=	O
(	O
f	O
(	O
x	O
)	O
-	O
ry	O
(	O
x	O
»	O
2	O
+	O
2	O
(	O
f	O
(	O
x	O
)	O
-	O
+e	O
{	O
(	O
ry	O
(	O
x	O
)	O
-	O
y	O
)	O
21x	O
=	O
x	O
}	O
ry	O
(	O
x	O
)	O
+	O
ry	O
(	O
x	O
)	O
-	O
y	O
)	O
21x	O
=	O
x	O
}	O
ry	O
(	O
x	O
»	O
e	O
{	O
ry	O
(	O
x	O
)	O
-	O
yix	O
=	O
x	O
}	O
=	O
(	O
f	O
(	O
x	O
)	O
-	O
ry	O
(	O
x	O
»	O
2	O
+	O
e	O
{	O
(	O
ry	O
(	O
x	O
)	O
-	O
y	O
)	O
21x	O
=	O
x	O
}	O
.	O
the	O
conditional	O
median	B
,	O
i.e.	O
,	O
the	O
function	O
minimizing	O
the	O
absolute	B
error	I
e	O
{	O
i	O
f	O
(	O
x	O
)	O
-	O
y	O
i	O
}	O
is	O
even	O
more	O
closely	O
related	O
to	O
the	O
bayes	O
rule	B
(	O
see	O
problem	O
2.12	O
)	O
.	O
2.2	O
a	O
simple	O
example	O
let	O
us	O
consider	O
the	O
prediction	O
of	O
a	O
student	O
's	O
performance	O
in	O
a	O
course	O
(	O
pass/fail	O
)	O
when	O
given	O
a	O
number	O
of	O
important	O
factors	O
.	O
first	O
,	O
let	O
y	O
=	O
1	O
denote	O
a	O
pass	O
and	O
let	O
y	O
=	O
°	O
stand	O
for	O
failure	O
.	O
the	O
sole	O
observation	O
x	O
is	O
the	O
number	O
of	O
hours	O
of	O
study	O
per	O
week	O
.	O
this	O
,	O
in	O
itself	O
,	O
is	O
not	O
a	O
foolproof	O
predictor	O
of	O
a	O
student	O
's	O
performance	O
,	O
because	O
for	O
that	O
we	O
would	O
need	O
more	O
information	O
about	O
the	O
student	O
's	O
quickness	O
of	O
mind	O
,	O
health	O
,	O
and	O
social	O
habits	O
.	O
the	O
regression	B
function	I
ry	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
11x	O
=	O
x	O
}	O
is	O
probably	O
monotonically	O
increasing	O
in	O
x.	O
if	O
it	O
were	O
known	O
to	O
be	O
ry	O
(	O
x	O
)	O
=	O
x	O
/	O
(	O
c	O
+	O
x	O
)	O
,	O
c	O
>	O
0	O
,	O
say	O
,	O
our	O
problem	O
would	O
be	O
solved	O
because	O
the	O
bayes	O
decision	O
is	O
g*	O
(	O
x	O
)	O
=	O
{	O
01	O
if	O
ry	O
(	O
x	O
)	O
>	O
1/2	O
(	O
i.e.	O
,	O
x	O
>	O
c	O
)	O
otherwise	O
.	O
12	O
2.	O
the	O
bayes	O
error	O
the	O
corresponding	O
bayes	O
error	O
is	O
l	O
*	O
=	O
l	O
(	O
g*	O
)	O
=	O
e	O
{	O
min	O
(	O
71	O
(	O
x	O
)	O
,	O
1	O
-	O
71	O
(	O
x	O
»	O
}	O
=	O
e	O
{	O
min	O
(	O
e	O
,	O
x	O
)	O
}	O
.	O
e+x	O
while	O
we	O
could	O
deduce	O
the	O
bayes	O
decision	O
from	O
71	O
alone	O
,	O
the	O
same	O
can	O
not	O
be	O
said	O
for	O
the	O
bayes	O
error	O
l	O
*	O
-it	O
requires	O
knowledge	O
of	O
the	O
distribution	B
of	O
x.	O
if	O
x	O
=	O
e	O
with	O
probability	O
one	O
(	O
as	O
in	O
an	O
army	O
school	O
,	O
where	O
all	O
students	O
are	O
forced	O
to	O
study	O
e	O
hours	O
per	O
week	O
)	O
,	O
then	O
l	O
*	O
=	O
1/2	O
.	O
if	O
we	O
have	O
a	O
population	O
that	O
is	O
nicely	O
spread	O
out	O
,	O
say	O
,	O
x	O
is	O
uniform	B
on	O
[	O
0	O
,	O
4e	O
]	O
,	O
then	O
the	O
situation	O
improves	O
:	O
1	O
14c	O
mince	O
,	O
x	O
)	O
4e	O
0	O
e	O
+	O
x	O
l*	O
=	O
-	O
dx	O
=	O
-log	O
-	O
~	O
0.305785	O
.	O
1	O
4	O
5e	O
4	O
far	O
away	O
from	O
x	O
=	O
e	O
,	O
discrimination	O
is	O
really	O
simple	O
.	O
in	O
general	O
,	O
discrimination	O
is	O
much	O
easier	O
than	O
estimation	B
because	O
of	O
this	O
phenomenon	O
.	O
2.3	O
another	O
simple	O
example	O
let	O
us	O
work	O
out	O
a	O
second	O
simple	O
example	O
in	O
which	O
y	O
=	O
°	O
or	O
y	O
=	O
1	O
according	O
to	O
whether	O
a	O
student	O
fails	O
or	O
passes	O
a	O
course	O
.	O
x	O
represents	O
one	O
or	O
more	O
observations	O
regarding	O
the	O
student	O
.	O
the	O
components	O
of	O
x	O
in	O
our	O
example	O
will	O
be	O
denoted	O
by	O
t	O
,	O
b	O
,	O
and	O
e	O
respectively	O
,	O
where	O
t	O
is	O
the	O
average	O
number	O
of	O
hours	O
the	O
students	O
watches	O
tv	O
,	O
b	O
is	O
the	O
average	O
number	O
of	O
beers	O
downed	O
each	O
day	O
,	O
and	O
e	O
is	O
an	O
intangible	O
quantity	O
measuring	O
extra	O
negative	O
factors	O
such	O
as	O
laziness	O
and	O
learning	B
difficulties	O
.	O
in	O
our	O
cooked-up	O
example	O
,	O
we	O
have	O
y=	O
{	O
l	O
ift+b+e	O
<	O
7	O
°	O
otherwise	O
.	O
thus	O
,	O
if	O
t	O
,	O
b	O
,	O
and	O
e	O
are	O
known	O
,	O
y	O
is	O
known	O
as	O
well	O
.	O
the	O
bayes	O
classifier	B
decides	O
1	O
if	O
t	O
+	O
b	O
+	O
e	O
<	O
7	O
and	O
°	O
otherwise	O
.	O
the	O
corresponding	O
bayes	O
probability	O
of	O
error	O
is	O
zero	O
.	O
unfortunately	O
,	O
e	O
is	O
intangible	O
,	O
and	O
not	O
available	O
to	O
the	O
observer	O
.	O
we	O
only	O
have	O
access	O
to	O
t	O
and	O
b.	O
given	O
t	O
and	O
b	O
,	O
when	O
should	O
we	O
guess	O
that	O
y	O
=	O
i	O
?	O
to	O
answer	O
this	O
question	O
,	O
one	O
must	O
know	O
the	O
joint	O
distribution	B
of	O
(	O
t	O
,	O
b	O
,	O
e	O
)	O
,	O
or	O
,	O
equivalently	O
,	O
the	O
joint	O
distribution	B
of	O
(	O
t	O
,	O
b	O
,	O
y	O
)	O
.	O
so	O
,	O
let	O
us	O
assume	O
that	O
t	O
,	O
b	O
,	O
and	O
e	O
are	O
i.i.d	O
.	O
exponential	B
random	O
variables	O
(	O
thus	O
,	O
they	O
have	O
density	O
e-u	O
on	O
[	O
0	O
,	O
(	O
0	O
»	O
.	O
the	O
bayes	O
rule	B
compares	O
p	O
{	O
y	O
=	O
lit	O
,	O
b	O
}	O
with	O
p	O
{	O
y	O
=	O
0it	O
,	O
b	O
}	O
and	O
makes	O
a	O
decision	O
consistent	O
with	O
the	O
maximum	O
of	O
these	O
two	O
values	O
.	O
a	O
simple	O
calculation	O
shows	O
that	O
p	O
{	O
y	O
=	O
lit	O
,	O
b	O
}	O
=	O
p	O
{	O
t	O
+	O
b	O
+	O
e	O
<	O
71t	O
,	O
b	O
}	O
=	O
pre	O
<	O
7	O
-	O
t	O
-	O
bit	O
,	O
b	O
}	O
max	O
(	O
0	O
,	O
1	O
-	O
e-	O
(	O
7-t-b	O
)	O
)	O
.	O
the	O
crossover	O
between	O
two	O
decisions	O
occurs	O
when	O
this	O
value	O
equals	O
1	O
/2	O
.	O
thus	O
,	O
the	O
bayes	O
classifier	B
is	O
as	O
follows	O
:	O
g*	O
(	O
t	O
,	O
b	O
)	O
=	O
{	O
i	O
if	O
t	O
+	O
~	O
<	O
7	O
-log2	O
=	O
6.306852819	O
...	O
o	O
otherwise	O
.	O
2.3	O
another	O
simple	O
example	O
13	O
of	O
course	O
,	O
this	O
classifier	B
is	O
not	O
perfect	O
.	O
the	O
probability	O
of	O
error	O
is	O
p	O
{	O
g*	O
(	O
t	O
,	O
b	O
)	O
=i	O
y	O
}	O
=	O
p	O
{	O
t	O
+b	O
<	O
7	O
-log2	O
,	O
t	O
+b	O
+	O
e	O
2	O
:	O
7	O
}	O
+p	O
{	O
t	O
+	O
b	O
2	O
:	O
7	O
-log2	O
,	O
t	O
+	O
b	O
+	O
e	O
<	O
7	O
}	O
=	O
e	O
{	O
e-	O
(	O
7-t-b	O
)	O
i	O
{	O
t+b	O
<	O
7-10g2	O
)	O
}	O
p	O
{	O
(	O
i	O
+	O
-	O
e	O
-	O
(	O
7-t-b	O
)	O
)	O
i	O
{	O
7	O
>	O
t+bo	O
:	O
:7-log2	O
)	O
}	O
=	O
xe-x	O
(	O
1	O
-	O
e-	O
(	O
7-x	O
)	O
)	O
dx	O
/,7	O
xe-x	O
e-	O
(	O
7-x	O
)	O
dx	O
+	O
17-10g2	O
o	O
(	O
since	O
the	O
density	O
of	O
t	O
+	O
b	O
is	O
ue-u	O
on	O
[	O
0	O
,	O
(	O
0	O
»	O
-7	O
(	O
(	O
7	O
-log2	O
)	O
2	O
72	O
+	O
2	O
(	O
8	O
-	O
log	O
2	O
)	O
-	O
8	O
-	O
2	O
+	O
7-log2	O
2	O
=	O
e	O
(	O
7	O
-log2	O
)	O
2	O
)	O
2	O
(	O
as	O
jxoo	O
ue-udu	O
=	O
(	O
1	O
+	O
x	O
)	O
e-	O
x	O
)	O
=	O
0.0199611	O
e	O
_	O
•	O
•	O
if	O
we	O
have	O
only	O
access	O
to	O
t	O
,	O
then	O
the	O
bayes	O
classifier	B
is	O
allowed	O
to	O
use	O
t	O
only	O
.	O
first	O
,	O
we	O
find	O
p	O
{	O
y=iit	O
}	O
=	O
p	O
{	O
e+b	O
<	O
7-tit	O
}	O
max	O
(	O
0	O
,	O
1	O
-	O
(	O
1	O
+	O
7	O
-	O
t	O
)	O
e-	O
(	O
7-t	O
)	O
)	O
the	O
crossover	O
at	O
1/2	O
occurs	O
at	O
t	O
=	O
c	O
=	O
5.321653009	O
...	O
,	O
so	O
that	O
the	O
bayes	O
classifier	B
is	O
given	O
by	O
def	O
g*	O
(	O
t	O
)	O
=	O
{	O
i	O
if	O
t	O
<	O
.c	O
°	O
otherwise	O
.	O
the	O
probability	O
of	O
error	O
is	O
p	O
{	O
g*	O
(	O
t	O
)	O
=i	O
y	O
}	O
p	O
{	O
t	O
<	O
c	O
,	O
t	O
+	O
b	O
+	O
e	O
:	O
:	O
:	O
:	O
7	O
}	O
+	O
p	O
{	O
t	O
:	O
:	O
:	O
:	O
c	O
,	O
t	O
+	O
b	O
+	O
e	O
<	O
7	O
}	O
=	O
e	O
{	O
(	O
l	O
+	O
7	O
-	O
t	O
)	O
e-	O
(	O
7-t	O
)	O
i	O
{	O
t	O
<	O
cd	O
+	O
p	O
{	O
(	O
1	O
-	O
(	O
1	O
+	O
7	O
-	O
t	O
)	O
e-	O
(	O
7-t	O
)	O
)	O
i	O
{	O
7	O
>	O
to	O
:	O
:cd	O
1c	O
e-x	O
(	O
l	O
+	O
7	O
-	O
x	O
)	O
e-	O
(	O
7-x	O
)	O
dx	O
+	O
17	O
e-x	O
(	O
1	O
-	O
(	O
1	O
+	O
7	O
-	O
x	O
)	O
e-	O
(	O
7-x	O
»	O
)	O
dx	O
-7	O
(	O
82	O
(	O
8	O
-	O
c	O
)	O
2	O
-	O
(	O
c-7	O
)	O
+e	O
1-	O
(	O
8	O
-	O
c	O
)	O
2	O
1	O
)	O
+-	O
2	O
2	O
=	O
e	O
-	O
-	O
2	O
2	O
=	O
0.02235309002	O
...	O
.	O
the	O
bayes	O
error	O
has	O
increased	O
slightly	O
,	O
but	O
not	O
by	O
much	O
.	O
finally	O
,	O
if	O
we	O
do	O
not	O
have	O
access	O
to	O
any	O
of	O
the	O
three	O
variables	O
,	O
t	O
,	O
b	O
,	O
and	O
e	O
,	O
the	O
best	O
we	O
can	O
do	O
is	O
see	O
which	O
14	O
2.	O
the	O
bayes	O
error	O
class	O
is	O
most	O
likely	O
.	O
to	O
this	O
end	O
,	O
we	O
compute	O
pry	O
=	O
o	O
}	O
=	O
p	O
{	O
t	O
+	O
b	O
+	O
e	O
~	O
7	O
}	O
=	O
(	O
1	O
+	O
7	O
+	O
72/2	O
)	O
e-7	O
=	O
.02963616388	O
...	O
if	O
we	O
set	O
g	O
==	O
1	O
all	O
the	O
time	O
,	O
we	O
make	O
an	O
error	O
with	O
probability	O
0.02963616388	O
...	O
.	O
in	O
practice	O
,	O
bayes	O
classifiers	O
are	O
unknown	O
simply	O
because	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
is	O
unknown	O
.	O
consider	O
a	O
classifier	O
based	O
upon	O
(	O
t	O
,	O
b	O
)	O
.	O
rosenblatt	O
's	O
percep	O
(	O
cid:173	O
)	O
tron	O
(	O
see	O
chapter	O
4	O
)	O
looks	O
for	O
the	O
best	O
linear	B
classifier	I
based	O
upon	O
the	O
data	O
.	O
that	O
is	O
,	O
the	O
decision	O
is	O
of	O
the	O
form	O
i	O
get	O
,	O
b	O
)	O
=	O
0	O
{	O
if	O
at	O
+	O
bb	O
<	O
c	O
otherwise	O
for	O
some	O
data-based	B
choices	O
for	O
a	O
,	O
band	O
c.	O
if	O
we	O
have	O
lots	O
of	O
data	O
at	O
our	O
disposal	O
,	O
then	O
it	O
is	O
possible	O
to	O
pick	O
out	O
a	O
linear	O
classifier	B
that	O
is	O
nearly	O
optimal	O
.	O
as	O
we	O
have	O
seen	O
above	O
,	O
the	O
bayes	O
classifier	B
happens	O
to	O
be	O
linear	O
.	O
that	O
is	O
a	O
sheer	O
coincidence	O
,	O
of	O
course	O
.	O
if	O
the	O
bayes	O
classifier	B
had	O
not	O
been	O
linear-for	O
example	O
,	O
if	O
we	O
had	O
y	O
=	O
i	O
{	O
t+b2+e	O
<	O
7	O
}	O
-then	O
even	O
the	O
best	O
perceptron	B
would	O
be	O
suboptimal	O
,	O
regardless	O
of	O
how	O
many	O
data	O
pairs	O
one	O
would	O
have	O
.	O
if	O
we	O
use	O
the	O
3-nearest	O
neighbor	B
rule	I
(	O
chapter	O
5	O
)	O
,	O
the	O
asymptotic	O
probability	O
of	O
error	O
is	O
not	O
more	O
than	O
1.3155	O
times	O
the	O
bayes	O
error	O
,	O
which	O
in	O
our	O
example	O
is	O
about	O
0.02625882705.	O
the	O
example	O
above	O
also	O
shows	O
the	O
need	O
to	O
look	O
at	O
individual	O
components	O
,	O
and	O
to	O
evaluate	O
how	O
many	O
and	O
which	O
components	O
would	O
be	O
most	O
useful	O
for	O
discrimination	O
.	O
this	O
subject	O
is	O
covered	O
in	O
the	O
chapter	O
on	O
feature	B
extraction	I
(	O
chapter	O
32	O
)	O
.	O
2.4	O
other	O
formulas	O
for	O
the	O
bayes	O
risk	O
the	O
following	O
forms	O
of	O
the	O
bayes	O
error	O
are	O
often	O
convenient	O
:	O
l	O
*	O
=	O
inf	O
p	O
{	O
g	O
(	O
x	O
)	O
=i	O
y	O
}	O
g	O
:	O
rc~	O
{	O
o	O
,	O
l	O
}	O
=	O
e	O
{	O
min	O
{	O
1	O
]	O
(	O
x	O
)	O
,	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
}	O
}	O
1	O
1	O
2	O
:	O
-	O
2	O
:	O
e	O
{	O
121	O
]	O
(	O
x	O
)	O
-	O
=	O
ii	O
}	O
·	O
figure	O
2.2.	O
the	O
bayes	O
decision	O
when	O
class	O
(	O
cid:173	O
)	O
conditional	O
densities	O
exist	O
.	O
in	O
the	O
figure	O
on	O
the	O
left	O
,	O
the	O
decision	O
is	O
0	O
on	O
[	O
a	O
,	O
b	O
]	O
and	O
1	O
elsewhere	O
.	O
pi	O
,	O
class	O
i	O
class	O
0	O
class	O
1	O
2.5	O
plug-in	O
decisions	O
15	O
in	O
special	O
cases	O
,	O
we	O
may	O
obtain	O
other	O
helpful	O
forms	O
.	O
for	O
example	O
,	O
if	O
x	O
has	O
a	O
density	O
f	O
,	O
then	O
l	O
*	O
f	O
min	O
(	O
1	O
]	O
(	O
x	O
)	O
,	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
)	O
f	O
(	O
x	O
)	O
dx	O
f	O
min	O
(	O
(	O
l	O
-	O
p	O
)	O
fo	O
(	O
x	O
)	O
,	O
pli	O
(	O
x	O
)	O
)	O
dx	O
,	O
where	O
p	O
=	O
p	O
{	O
y	O
=	O
i	O
}	O
,	O
and	O
fi	O
(	O
x	O
)	O
is	O
the	O
density	O
of	O
x	O
given	O
that	O
y	O
=	O
i.	O
p	O
and	O
1	O
-	O
p	O
are	O
called	O
the	O
class	O
probabilities	O
,	O
and	O
fa	O
,	O
fl	O
are	O
the	O
class-conditional	B
densities	O
.	O
if	O
fa	O
and	O
ii	O
are	O
nonoverlapping	O
,	O
that	O
is	O
,	O
f	O
fofl	O
=	O
0	O
,	O
then	O
obviously	O
l	O
*	O
=	O
0.	O
assume	O
moreover	O
that	O
p	O
=	O
1/2	O
.	O
then	O
l*	O
=	O
~	O
f	O
min	O
(	O
fo	O
(	O
x	O
)	O
,	O
fl	O
(	O
x	O
)	O
)	O
dx	O
~	O
f	O
ii	O
(	O
x	O
)	O
-	O
(	O
ii	O
(	O
x	O
)	O
-	O
=	O
~	O
-	O
~	O
f	O
ifl	O
(	O
x	O
)	O
-	O
fo	O
(	O
x	O
)	O
ldx	O
.	O
fo	O
(	O
x	O
)	O
)	O
+	O
dx	O
here	O
g+	O
denotes	O
the	O
positive	O
part	O
of	O
a	O
function	O
g.	O
thus	O
,	O
the	O
bayes	O
error	O
is	O
directly	O
related	O
to	O
the	O
l	O
1	O
distance	B
between	O
the	O
class	O
densities	O
.	O
figure	O
2.3.	O
the	O
shaded	O
area	O
is	O
the	O
l	O
1	O
distance	B
between	O
the	O
class-conditional	B
densities	O
.	O
2.5	O
plug-in	O
decisions	O
the	O
best	O
guess	O
of	O
y	O
from	O
the	O
observation	O
x	O
is	O
the	O
bayes	O
decision	O
*	O
(	O
x	O
)	O
=	O
{	O
o	O
if	O
1	O
]	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
;	O
1/2	O
=	O
{	O
o	O
if	O
1	O
]	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
;	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
1	O
otherwise	O
.	O
g	O
1	O
otherwise	O
the	O
function	O
1	O
]	O
is	O
typically	O
unknown	O
.	O
assume	O
that	O
we	O
have	O
access	O
to	O
nonnegative	O
functions	O
i	O
]	O
(	O
x	O
)	O
,	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
respectively	O
.	O
in	O
this	O
case	O
it	O
seems	O
natural	O
to	O
use	O
the	O
plug-in	B
decision	I
function	O
i	O
]	O
(	O
x	O
)	O
that	O
approximate	O
1	O
]	O
(	O
x	O
)	O
and	O
1	O
-	O
g	O
(	O
x	O
)	O
=	O
{	O
01	O
if	O
i	O
]	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
;	O
1/2	O
otherwise	O
,	O
16	O
2.	O
the	O
bayes	O
error	O
to	O
approximate	O
the	O
bayes	O
decision	O
.	O
the	O
next	O
well-known	O
theorem	B
(	O
see	O
,	O
e.g.	O
,	O
van	O
ryzin	O
(	O
1966	O
)	O
,	O
wolverton	O
and	O
wagner	O
(	O
1969a	O
)	O
,	O
glick	O
(	O
1973	O
)	O
,	O
csibi	O
(	O
1971	O
)	O
,	O
gyorfi	O
(	O
1975	O
)	O
,	O
(	O
1978	O
)	O
,	O
devroye	O
and	O
wagner	O
(	O
1976b	O
)	O
,	O
devroye	O
(	O
1982b	O
)	O
,	O
and	O
devroye	O
and	O
gyorfi	O
(	O
1985	O
)	O
)	O
states	O
that	O
if	O
i1	O
(	O
x	O
)	O
is	O
close	O
to	O
the	O
real	O
a	B
posteriori	I
probability	I
in	O
l	O
1-sense	O
,	O
then	O
the	O
error	O
probability	O
of	O
decision	O
g	O
is	O
near	O
the	O
optimal	O
decision	O
g*	O
.	O
theorem	B
2.2.	O
for	O
the	O
error	O
probability	O
of	O
the	O
plug-in	B
decision	I
g	O
defined	O
above	O
,	O
we	O
have	O
p	O
{	O
g	O
(	O
x	O
)	O
=i	O
y	O
}	O
-	O
l	O
*	O
=	O
2	O
[	O
11j	O
(	O
x	O
)	O
-	O
1/2ii	O
{	O
g	O
(	O
x	O
)	O
:	O
fg*	O
(	O
x	O
)	O
}	O
{	O
l	O
(	O
dx	O
)	O
,	O
lrd	O
and	O
p	O
{	O
g	O
(	O
x	O
)	O
=i	O
y	O
}	O
-	O
l*	O
s	O
2	O
r	O
11j	O
(	O
x	O
)	O
-	O
lrd	O
i1	O
(	O
x	O
)	O
ijl	O
(	O
dx	O
)	O
=	O
2ei1j	O
(	O
x	O
)	O
-	O
i1	O
(	O
x	O
)	O
i.	O
proof	O
.	O
if	O
for	O
some	O
x	O
e	O
nd	O
,	O
g	O
(	O
x	O
)	O
=	O
g*	O
(	O
x	O
)	O
,	O
then	O
clearly	O
the	O
difference	O
between	O
the	O
conditional	O
error	O
probabilities	O
of	O
g	O
and	O
g*	O
is	O
zero	O
:	O
p	O
{	O
g	O
(	O
x	O
)	O
=i	O
yix	O
=	O
x	O
}	O
-	O
p	O
{	O
g*	O
(	O
x	O
)	O
=i	O
yix	O
=	O
x	O
}	O
=	O
o.	O
otherwise	O
,	O
if	O
g	O
(	O
x	O
)	O
=i	O
g*	O
(	O
x	O
)	O
,	O
then	O
as	O
seen	O
in	O
the	O
proof	O
of	O
theorem	O
2.1	O
,	O
the	O
difference	O
may	O
be	O
written	O
as	O
p	O
{	O
g	O
(	O
x	O
)	O
=i	O
yix	O
=	O
x	O
}	O
p	O
{	O
g*	O
(	O
x	O
)	O
=i	O
yix	O
=	O
x	O
}	O
=	O
=	O
(	O
21j	O
(	O
x	O
)	O
-	O
1	O
)	O
(	O
i	O
{	O
g*	O
(	O
x	O
)	O
:	O
:	O
:	O
:l	O
}	O
-	O
i	O
{	O
g	O
(	O
x	O
)	O
=l	O
}	O
)	O
12	O
17	O
(	O
x	O
)	O
-	O
lii	O
{	O
g	O
(	O
x	O
)	O
¥g*	O
(	O
x	O
)	O
}	O
.	O
thus	O
,	O
p	O
{	O
g	O
(	O
x	O
)	O
=i	O
y	O
}	O
-	O
l	O
*	O
=	O
[	O
211j	O
(	O
x	O
)	O
-	O
1/2ii	O
{	O
g	O
(	O
x	O
)	O
:	O
fg*	O
(	O
x	O
)	O
}	O
{	O
l	O
(	O
dx	O
)	O
jrd	O
<	O
since	O
g	O
(	O
x	O
)	O
=i	O
g*	O
(	O
x	O
)	O
implies	O
11j	O
(	O
x	O
)	O
-	O
[	O
211j	O
(	O
x	O
)	O
-17	O
(	O
x	O
)	O
ijl	O
(	O
dx	O
)	O
,	O
jrf	O
i1	O
(	O
x	O
)	O
1	O
:	O
:	O
:	O
:	O
11j	O
(	O
x	O
)	O
-	O
1/21	O
.	O
0	O
when	O
the	O
classifier	B
g	O
(	O
x	O
)	O
can	O
be	O
put	O
in	O
the	O
form	O
(	O
x	O
)	O
=	O
{	O
o	O
ifi11	O
(	O
x~	O
s	O
i1o	O
(	O
x	O
)	O
g	O
1	O
otherwise	O
,	O
where	O
i11	O
(	O
x	O
)	O
,	O
i1o	O
(	O
x	O
)	O
are	O
some	O
approximations	O
of	O
1j	O
(	O
x	O
)	O
and	O
1	O
-	O
7j	O
(	O
x	O
)	O
,	O
respectively	O
,	O
the	O
situation	O
differs	O
from	O
that	O
discussed	O
in	O
theorem	O
2.2	O
if	O
i1o	O
(	O
x	O
)	O
+	O
171	O
(	O
x	O
)	O
does	O
not	O
necessarily	O
equal	O
to	O
one	O
.	O
however	O
,	O
an	O
inequality	B
analogous	O
to	O
that	O
of	O
theorem	O
2.2	O
remains	O
true	O
:	O
2.6	O
bayes	O
error	O
versus	O
dimension	B
17	O
theorem	B
2.3.	O
the	O
error	O
probability	O
of	O
the	O
decision	O
defined	O
above	O
is	O
bounded	O
from	O
above	O
by	O
p	O
{	O
g	O
(	O
x	O
)	O
=i	O
y	O
}	O
-	O
l	O
*	O
:	O
s	O
[	O
10	O
-	O
lj	O
(	O
x	O
»	O
-	O
jrd	O
ilo	O
(	O
x	O
)	O
ll-l	O
(	O
dx	O
)	O
+	O
[	O
.	O
.	O
jrd	O
ilj	O
(	O
x	O
)	O
-	O
ill	O
(	O
x	O
)	O
ii-l	O
(	O
dx	O
)	O
.	O
the	O
proof	O
is	O
left	O
to	O
the	O
reader	O
(	O
problem	O
2.9	O
)	O
.	O
remark	O
.	O
assume	O
that	O
the	O
class-conditional	B
densities	O
fo	O
,	O
ii	O
exist	O
and	O
are	O
approxi	O
(	O
cid:173	O
)	O
mated	O
by	O
the	O
densities	O
10	O
(	O
x	O
)	O
,	O
ir	O
(	O
x	O
)	O
.	O
assume	O
furthermore	O
that	O
the	O
class	O
probabilities	O
p	O
==	O
pry	O
==	O
1	O
}	O
and	O
1	O
-	O
p	O
==	O
p	O
{	O
y	O
=	O
o	O
}	O
are	O
approximated	O
by	O
pi	O
and	O
po	O
,	O
respectively	O
.	O
then	O
for	O
the	O
error	O
probability	O
of	O
the	O
plug-in	B
decision	I
function	O
g	O
(	O
x	O
)	O
=	O
{	O
o	O
if	O
piji	O
(	O
x	O
)	O
:	O
s	O
pojo	O
(	O
x	O
)	O
1	O
otherwise	O
,	O
p	O
{	O
g	O
(	O
x	O
)	O
=i	O
y	O
}	O
-	O
l	O
*	O
:	O
s	O
[	O
10	O
-	O
p	O
)	O
foex	O
)	O
-	O
pojoex	O
)	O
ldx	O
+	O
[	O
jrd	O
jrd	O
ipfiex	O
)	O
-	O
piirex	O
)	O
ldx	O
.	O
see	O
problem	O
2.10.0	O
2.6	O
bayes	O
error	O
versus	O
dimension	B
the	O
components	O
of	O
x	O
that	O
matter	O
in	O
the	O
bayes	O
classifier	B
are	O
those	O
that	O
explicitly	O
appear	O
in	O
lj	O
(	O
x	O
)	O
.	O
in	O
fact	O
,	O
then	O
,	O
all	O
discrimination	O
problems	O
are	O
one-dimensional	O
,	O
as	O
we	O
could	O
equally	O
well	O
replace	O
x	O
by	O
ljex	O
)	O
or	O
by	O
any	O
strictly	O
monotone	O
function	O
of	O
1	O
]	O
(	O
x	O
)	O
,	O
such	O
as	O
lj	O
7	O
ex	O
)	O
+	O
5lj3	O
(	O
x	O
)	O
+	O
lj	O
(	O
x	O
)	O
.	O
unfortunately	O
,	O
lj	O
is	O
unknown	O
in	O
general	O
.	O
in	O
the	O
example	O
in	O
section	O
2.3	O
,	O
we	O
had	O
in	O
one	O
case	O
lj	O
(	O
t	O
,	O
b	O
)	O
=	O
max	O
(	O
0	O
,	O
1	O
-	O
e-c7	O
-	O
t-	O
b	O
»	O
)	O
and	O
in	O
another	O
case	O
ljet	O
)	O
=	O
max	O
(	O
0	O
,	O
1	O
-	O
0	O
+	O
7	O
-	O
t	O
)	O
e-	O
(	O
7-t	O
»	O
)	O
.	O
the	O
former	O
format	O
suggests	O
that	O
we	O
could	O
base	O
all	O
decisions	O
on	O
t	O
+	O
b.	O
this	O
means	O
that	O
if	O
we	O
had	O
no	O
access	O
to	O
t	O
and	O
b	O
individually	O
,	O
but	O
to	O
t	O
+	O
b	O
jointly	O
,	O
we	O
would	O
be	O
able	O
to	O
achieve	O
the	O
same	O
results	O
!	O
since	O
lj	O
is	O
unknown	O
,	O
all	O
of	O
this	O
is	O
really	O
irrelevant	O
.	O
in	O
general	O
,	O
the	O
bayes	O
risk	O
increases	O
if	O
we	O
replace	O
x	O
by	O
tex	O
)	O
for	O
any	O
trans	O
(	O
cid:173	O
)	O
formation	O
t	O
(	O
see	O
problem	O
2.1	O
)	O
,	O
as	O
this	O
destroys	O
information	O
.	O
on	O
the	O
other	O
hand	O
,	O
there	O
exist	O
transformations	O
(	O
such	O
as	O
lj	O
(	O
x	O
»	O
that	O
leave	O
the	O
bayes	O
error	O
untouched	O
.	O
for	O
more	O
on	O
the	O
relationship	O
between	O
the	O
bayes	O
error	O
and	O
the	O
dimension	B
,	O
refer	O
to	O
chapter	O
32	O
.	O
18	O
2.	O
the	O
bayes	O
error	O
problems	O
and	O
exercises	O
problem	O
2.1.	O
let	O
t	O
:	O
x	O
-+	O
x	O
'	O
be	O
an	O
arbitrary	O
measurable	O
function	O
.	O
if	O
l	O
~	O
and	O
l~	O
(	O
x	O
)	O
denote	O
the	O
bayes	O
error	O
probabilities	O
for	O
(	O
x	O
,	O
y	O
)	O
and	O
(	O
t	O
(	O
x	O
)	O
,	O
y	O
)	O
,	O
respectively	O
,	O
then	O
prove	O
that	O
(	O
this	O
shows	O
that	O
transformations	O
of	O
x	O
destroy	O
information	O
,	O
because	O
the	O
bayes	O
risk	O
in	O
(	O
cid:173	O
)	O
creases	O
.	O
)	O
problem	O
2.2.	O
let	O
x	O
'	O
be	O
independent	O
of	O
(	O
x	O
,	O
y	O
)	O
.	O
prove	O
that	O
lex	O
,	O
xf	O
)	O
=	O
l~	O
.	O
problem	O
2.3.	O
show	O
that	O
l*	O
:	O
:	O
:	O
:	O
min	O
(	O
p	O
,	O
1	O
-	O
p	O
)	O
,	O
where	O
p	O
,	O
1	O
-	O
p	O
are	O
the	O
class	O
probabilities	O
.	O
show	O
that	O
equality	O
holds	O
if	O
x	O
and	O
y	O
are	O
independent	O
.	O
exhibit	O
a	O
distribution	O
where	O
x	O
is	O
not	O
independent	O
of	O
y	O
,	O
but	O
l	O
*	O
=	O
rnin	O
(	O
p	O
,	O
1	O
-	O
p	O
)	O
.	O
problem	O
2.4.	O
neyman-pearson	O
lemma	O
.	O
consider	O
again	O
the	O
decision	O
problem	O
,	O
but	O
with	O
a	O
decision	O
g	O
,	O
we	O
now	O
assign	O
two	O
error	O
probabilities	O
,	O
l	O
(	O
o	O
)	O
(	O
g	O
)	O
=	O
p	O
{	O
g	O
(	O
x	O
)	O
=	O
11y	O
=	O
o	O
}	O
and	O
l	O
(	O
1	O
)	O
(	O
g	O
)	O
=	O
p	O
{	O
g	O
(	O
x	O
)	O
=	O
oiy	O
=	O
i	O
}	O
.	O
assume	O
that	O
the	O
class-conditional	B
densities	O
fo	O
,	O
fl	O
exist	O
.	O
for	O
c	O
>	O
0	O
,	O
define	O
the	O
decision	O
if	O
cfl	O
(	O
x	O
)	O
>	O
!	O
o	O
(	O
x	O
)	O
otherwise	O
.	O
prove	O
that	O
for	O
any	O
decision	O
g	O
,	O
if	O
l	O
(	O
o	O
)	O
(	O
g	O
)	O
:	O
:	O
:	O
:	O
:	O
l	O
(	O
o	O
)	O
(	O
gc	O
)	O
,	O
then	O
l	O
(	O
1\g	O
)	O
:	O
:	O
:	O
:	O
l	O
(	O
l	O
)	O
(	O
gc	O
)	O
'	O
in	O
other	O
words	O
,	O
if	O
l	O
(	O
0	O
)	O
is	O
required	O
to	O
be	O
kept	O
under	O
a	O
certain	O
level	O
,	O
then	O
the	O
decision	O
minimizing	O
l	O
(	O
1	O
)	O
has	O
the	O
form	O
of	O
gc	O
for	O
some	O
c.	O
note	O
that	O
g*	O
is	O
like	O
that	O
.	O
problem	O
2.5.	O
decisions	O
with	O
rejection	O
.	O
sometimes	O
in	O
decision	O
problems	O
,	O
one	O
is	O
allowed	O
to	O
say	O
``	O
i	O
do	O
n't	O
know	O
,	O
''	O
if	O
this	O
does	O
not	O
happen	O
frequently	O
.	O
these	O
decisions	O
are	O
called	O
decisions	O
with	O
a	O
reject	O
option	O
(	O
see	O
,	O
e.g.	O
,	O
forney	O
(	O
1968	O
)	O
,	O
chow	O
(	O
1970	O
»	O
.	O
formally	O
,	O
a	O
decision	O
g	O
(	O
x	O
)	O
can	O
have	O
three	O
values	O
:	O
0	O
,	O
1	O
,	O
and	O
``	O
reject	O
.	O
''	O
there	O
are	O
two	O
performance	O
measures	O
:	O
the	O
probability	O
of	O
rejection	O
p	O
{	O
g	O
(	O
x	O
)	O
=	O
``	O
reject	O
''	O
}	O
,	O
and	O
the	O
error	O
probability	O
p	O
{	O
g	O
(	O
x	O
)	O
i	O
ylg	O
(	O
x	O
)	O
i	O
``	O
reject	O
''	O
}	O
.	O
for	O
a	O
°	O
<	O
c	O
<	O
1/2	O
,	O
define	O
the	O
decision	O
gc	O
(	O
x	O
)	O
=	O
{	O
~	O
if	O
ry	O
(	O
x	O
)	O
>	O
1/2	O
+	O
c	O
ifry	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
1/2	O
-	O
c	O
otherwise	O
.	O
``	O
reject	O
''	O
show	O
that	O
for	O
any	O
decision	O
g	O
,	O
if	O
p	O
{	O
g	O
(	O
x	O
)	O
=	O
``	O
reject	O
''	O
}	O
:	O
:	O
:	O
:	O
p	O
{	O
gc	O
(	O
x	O
)	O
=	O
``	O
reject	O
''	O
}	O
,	O
then	O
p	O
{	O
g	O
(	O
x	O
)	O
i	O
y	O
!	O
g	O
(	O
x	O
)	O
i	O
``	O
reject	O
''	O
}	O
:	O
:	O
:	O
:	O
p	O
{	O
gc	O
(	O
x	O
)	O
i	O
ylgc	O
(	O
x	O
)	O
i	O
``	O
reject	O
''	O
}	O
.	O
thus	O
,	O
to	O
keep	O
the	O
probability	O
of	O
rejection	O
under	O
a	O
certain	O
level	O
,	O
decisions	O
of	O
the	O
form	O
of	O
gc	O
are	O
optimal	O
(	O
gyorfi	O
,	O
gyorfi	O
,	O
and	O
vajda	O
(	O
1978	O
»	O
.	O
problems	O
and	O
exercises	O
19	O
problem	O
2.6.	O
consider	O
the	O
prediction	O
of	O
a	O
student	O
's	O
failure	O
based	O
upon	O
variables	O
t	O
and	O
b	O
,	O
where	O
y	O
=	O
i	O
{	O
t+b+e	O
<	O
7	O
}	O
and	O
e	O
is	O
an	O
inaccessible	O
variable	B
(	O
see	O
section	O
2.3	O
)	O
.	O
(	O
1	O
)	O
let	O
t	O
,	O
b	O
,	O
and	O
e	O
be	O
independent	O
.	O
merely	O
by	O
changing	O
the	O
distribution	B
of	O
e	O
,	O
show	O
that	O
the	O
bayes	O
error	O
for	O
classification	O
based	O
upon	O
(	O
t	O
,	O
b	O
)	O
can	O
be	O
made	O
as	O
close	O
as	O
desired	O
to	O
1/2	O
.	O
(	O
2	O
)	O
let	O
t	O
and	O
b	O
be	O
independent	O
and	O
exponentially	O
distributed	O
.	O
find	O
ajoint	O
distribution	B
of	O
(	O
t	O
,	O
b	O
,	O
e	O
)	O
such	O
that	O
the	O
bayes	O
classifier	B
is	O
not	O
a	O
linear	O
classifier	B
.	O
(	O
3	O
)	O
let	O
t	O
and	O
b	O
be	O
independent	O
and	O
exponentially	O
distributed	O
.	O
find	O
a	O
joint	O
distribution	B
of	O
(	O
t	O
,	O
b	O
,	O
e	O
)	O
such	O
that	O
the	O
bayes	O
classifier	B
is	O
given	O
by	O
g*	O
(	O
t	O
,	O
b	O
)	O
=	O
{	O
i	O
if	O
t2	O
+.b2	O
<	O
10	O
,	O
°	O
otherwise	O
.	O
(	O
4	O
)	O
find	O
the	O
bayes	O
classifier	B
and	O
bayes	O
error	O
for	O
classification	O
based	O
on	O
(	O
t	O
,	O
b	O
)	O
(	O
with	O
y	O
as	O
above	O
)	O
if	O
(	O
t	O
,	O
b	O
,	O
e	O
)	O
is	O
uniformly	O
distributed	O
on	O
[	O
0	O
,	O
4j3	O
.	O
problem	O
2.7.	O
assume	O
that	O
t	O
,	O
b	O
,	O
and	O
e	O
are	O
independent	O
uniform	B
[	O
0,4	O
]	O
random	O
variables	O
with	O
interpretations	O
as	O
in	O
section	O
2.3.	O
let	O
y	O
=	O
1	O
(	O
0	O
)	O
denote	O
whether	O
a	O
student	O
passes	O
(	O
fails	O
)	O
a	O
course	O
.	O
assume	O
that	O
y	O
=	O
1	O
if	O
and	O
only	O
if	O
t	O
b	O
e	O
:	O
s	O
8	O
.	O
(	O
1	O
)	O
find	O
the	O
bayes	O
decision	O
if	O
no	O
variable	B
is	O
available	O
,	O
if	O
only	O
t	O
is	O
available	O
,	O
and	O
if	O
only	O
t	O
and	O
b	O
are	O
available	O
.	O
(	O
2	O
)	O
determine	O
in	O
all	O
three	O
cases	O
the	O
bayes	O
error	O
.	O
(	O
3	O
)	O
determine	O
the	O
best	O
linear	B
classifier	I
based	O
upon	O
t	O
and	O
b	O
only	O
.	O
problem	O
2.8.	O
let	O
1	O
]	O
'	O
,	O
1	O
]	O
''	O
:	O
nd	O
-+	O
[	O
0	O
,	O
1	O
]	O
be	O
arbitrary	O
measurable	O
functions	O
,	O
and	O
define	O
the	O
corresponding	O
decisions	O
by	O
g	O
'	O
(	O
x	O
)	O
=	O
i	O
{	O
t/	O
'	O
(	O
x	O
»	O
1/2j	O
and	O
g	O
''	O
(	O
x	O
)	O
=	O
i	O
(	O
t/i/	O
(	O
x	O
»	O
1/2j	O
.	O
prove	O
that	O
/l	O
(	O
g	O
'	O
)	O
-	O
l	O
(	O
gff	O
)	O
/	O
:	O
s	O
p	O
{	O
g	O
'	O
(	O
x	O
)	O
i	O
gff	O
(	O
x	O
)	O
}	O
and	O
/l	O
(	O
g	O
'	O
)	O
-	O
l	O
(	O
gff	O
)	O
1	O
:	O
s	O
e	O
{	O
121	O
]	O
(	O
x	O
)	O
-	O
1ii	O
{	O
gl	O
(	O
x	O
)	O
igl/	O
(	O
x	O
)	O
d	O
.	O
problem	O
2.9.	O
prove	O
theorem	B
2.3.	O
problem	O
2.10.	O
assume	O
that	O
the	O
class-conditional	B
densities	O
fo	O
and	O
fi	O
exist	O
and	O
are	O
ap	O
(	O
cid:173	O
)	O
proximated	O
by	O
the	O
densities	O
fo	O
and	O
a	O
,	O
respectively	O
.	O
assume	O
furthermore	O
that	O
the	O
class	O
probabilities	O
p	O
=	O
pry	O
=	O
i	O
}	O
and	O
1	O
-	O
p	O
=	O
pry	O
=	O
o	O
}	O
are	O
approximated	O
by	O
pi	O
and	O
po	O
.	O
prove	O
that	O
for	O
the	O
error	O
probability	O
of	O
the	O
plug-in	B
decision	I
function	O
if	O
pi	O
a	O
(	O
x	O
)	O
:	O
s	O
po	O
10	O
ex	O
)	O
otherwise	O
,	O
g	O
(	O
x	O
)	O
=	O
{	O
~	O
we	O
have	O
p	O
{	O
g	O
(	O
x	O
)	O
i	O
y	O
}	O
-	O
l	O
*	O
:	O
s	O
llpfl	O
(	O
x	O
)	O
-	O
pia	O
ex	O
)	O
ldx	O
+	O
1	O
10	O
-	O
p	O
)	O
fo	O
(	O
x	O
)	O
-	O
polo	O
(	O
x	O
)	O
ldx	O
.	O
nd	O
problem	O
2.11.	O
using	O
the	O
notation	O
of	O
problem	O
2.10	O
,	O
show	O
that	O
if	O
for	O
a	O
sequence	O
of	O
fm	O
,	O
n	O
(	O
x	O
)	O
and	O
pm	O
,	O
n	O
(	O
m	O
=	O
0	O
,	O
1	O
)	O
,	O
nd	O
20	O
2.	O
the	O
bayes	O
error	O
then	O
for	O
the	O
corresponding	O
sequence	O
of	O
plug-in	O
decisions	O
limn	O
--	O
-+oo	O
p	O
{	O
gn	O
(	O
x	O
)	O
=i	O
yj	O
=	O
l	O
*	O
(	O
wolverton	O
and	O
wagner	O
(	O
l969a	O
»	O
.	O
hint	O
:	O
according	O
to	O
problem	O
2.10	O
,	O
it	O
suffices	O
to	O
show	O
that	O
if	O
we	O
are	O
given	O
a	O
deterministic	O
sequence	O
of	O
density	O
functions	O
f	O
,	O
!	O
i	O
,	O
12	O
,	O
h	O
,	O
...	O
,	O
then	O
implies	O
n	O
--	O
-+oo	O
lim	O
f	O
(	O
fn	O
(	O
x	O
)	O
-	O
lim	O
f	O
ifn	O
(	O
x	O
)	O
-	O
11	O
--	O
-+00	O
f	O
(	O
x	O
»	O
2	O
dx	O
=	O
0	O
f	O
(	O
x	O
)	O
ldx	O
=	O
o	O
.	O
(	O
a	O
function	O
f	O
is	O
called	O
a	O
density	O
function	O
if	O
it	O
is	O
nonnegative	O
and	O
f	O
f	O
(	O
x	O
)	O
dx	O
=	O
1	O
.	O
)	O
to	O
see	O
this	O
,	O
observe	O
that	O
where	O
ai	O
,	O
a	O
2	O
,	O
•..	O
is	O
a	O
partition	O
of	O
n	O
d	O
into	O
unit	O
cubes	O
,	O
and	O
f+	O
denotes	O
the	O
positive	O
part	O
of	O
a	O
function	O
f.	O
the	O
key	O
observation	O
is	O
that	O
convergence	O
to	O
zero	O
of	O
each	O
term	O
of	O
the	O
infinite	O
sum	O
implies	O
convergence	O
of	O
the	O
whole	O
integral	O
by	O
the	O
dominated	B
convergence	I
theorem	I
,	O
since	O
f	O
(	O
fn	O
(	O
x	O
)	O
-	O
f	O
(	O
x	O
»	O
+	O
dx	O
:	O
s	O
f	O
fn	O
(	O
x	O
)	O
dx	O
=	O
1.	O
handle	O
the	O
right-hand	O
side	O
by	O
the	O
cauchy-schwarz	O
inequality	B
.	O
problem	O
2.12.	O
define	O
the	O
ll	O
error	O
of	O
a	O
function	O
f	O
:	O
n	O
d	O
--	O
'j	O
>	O
n	O
by	O
j	O
(	O
f	O
)	O
=	O
e	O
{	O
lf	O
(	O
x	O
)	O
-	O
yi	O
}	O
.	O
show	O
that	O
a	O
function	O
minimizing	O
j	O
(	O
f	O
)	O
is	O
the	O
bayes	O
rule	B
g*	O
,	O
that	O
is	O
,	O
j*	O
=	O
inf	O
f	O
j	O
(	O
f	O
)	O
=	O
j	O
(	O
g*	O
)	O
.	O
thus	O
,	O
j*	O
=	O
l	O
*	O
.	O
define	O
a	O
decision	O
by	O
g	O
(	O
x	O
)	O
=	O
{	O
if	O
f	O
(	O
x	O
)	O
:	O
s	O
1/2	O
0	O
1	O
otherwise	O
,	O
prove	O
that	O
its	O
error	O
probability	O
l	O
(	O
g	O
)	O
=	O
p	O
{	O
g	O
(	O
x	O
)	O
=i	O
y	O
}	O
satisfies	O
the	O
inequality	B
l	O
(	O
g	O
)	O
-	O
l*	O
:	O
s	O
j	O
(	O
f	O
)	O
-	O
j*	O
.	O
3	O
inequalities	O
and	O
alternate	O
distance	B
measures	O
3.1	O
measuring	O
discriminatory	O
information	O
in	O
our	O
two-class	O
discrimination	O
problem	O
,	O
the	O
best	O
rule	B
has	O
(	O
bayes	O
)	O
probability	O
of	O
error	O
l	O
*	O
=	O
e	O
{	O
min	O
(	O
1j	O
(	O
x	O
)	O
,	O
1	O
-	O
1j	O
(	O
x	O
)	O
)	O
}	O
.	O
this	O
quantity	O
measures	O
how	O
difficult	O
the	O
discrimination	O
problem	O
is	O
.	O
it	O
also	O
serves	O
as	O
a	O
gauge	O
of	O
the	O
quality	O
of	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
for	O
pattern	O
recognition	O
.	O
put	O
differently	O
,	O
if	O
1/11	O
and	O
1/12	O
are	O
certain	O
many-to-one	O
mappings	O
,	O
l	O
*	O
may	O
be	O
used	O
to	O
compare	O
discrimination	O
based	O
on	O
(	O
1/11	O
(	O
x	O
)	O
,	O
y	O
)	O
with	O
that	O
based	O
on	O
(	O
1/12	O
(	O
x	O
)	O
,	O
y	O
)	O
.	O
when	O
1/11	O
projects	O
n	O
d	O
to	O
n	O
d	O
]	O
by	O
taking	O
the	O
first	O
d	O
1	O
coordinates	O
,	O
and	O
1/12	O
takes	O
the	O
last	O
d2	O
coordinates	O
,	O
the	O
corresponding	O
bayes	O
errors	O
will	O
help	O
us	O
decide	O
which	O
projection	O
is	O
better	O
.	O
in	O
this	O
sense	O
,	O
l	O
*	O
is	O
the	O
fundamental	O
quantity	O
in	O
feature	O
extraction	O
.	O
other	O
quantities	O
have	O
been	O
suggested	O
over	O
the	O
years	O
that	O
measure	B
the	O
discrimi	O
(	O
cid:173	O
)	O
natory	O
power	O
hidden	O
in	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
.	O
these	O
may	O
be	O
helpful	O
in	O
some	O
settings	O
.	O
for	O
example	O
,	O
in	O
theoretical	O
studies	O
or	O
in	O
certain	O
proofs	O
,	O
the	O
relationship	O
be	O
(	O
cid:173	O
)	O
tween	O
l	O
*	O
and	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
may	O
become	O
clearer	O
via	O
certain	O
inequalities	O
that	O
link	O
l	O
*	O
with	O
other	O
functionals	O
of	O
the	O
distribution	B
.	O
we	O
all	O
understand	O
moments	O
and	O
variances	O
,	O
but	O
how	O
do	O
these	O
simple	O
functionals	O
relate	O
to	O
l	O
*	O
?	O
perhaps	O
we	O
may	O
even	O
learn	O
a	O
thing	O
or	O
two	O
about	O
what	O
it	O
is	O
that	O
makes	O
l	O
*	O
small	O
.	O
in	O
feature	O
selection	B
,	O
some	O
explicit	O
inequalities	O
involving	O
l	O
*	O
may	O
provide	O
just	O
the	O
kind	O
of	O
numerical	O
in	O
(	O
cid:173	O
)	O
formation	O
that	O
will	O
allow	O
one	O
to	O
make	O
certain	O
judgements	O
on	O
what	O
kind	O
of	O
feature	O
is	O
preferable	O
in	O
practice	O
.	O
in	O
short	O
,	O
we	O
will	O
obtain	O
more	O
information	O
about	O
l	O
*	O
with	O
a	O
variety	O
of	O
uses	O
in	O
pattern	O
recognition	O
.	O
22	O
3.	O
inequalities	O
and	O
alternate	O
distance	B
measures	O
in	O
the	O
next	O
few	O
sections	O
,	O
we	O
avoid	O
putting	O
any	O
conditions	O
on	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
.	O
3.2	O
the	O
kolmogorov	O
variational	O
distance	B
inspired	O
by	O
the	O
total	B
variation	I
distance	O
between	O
distributions	O
,	O
the	O
kolmogorov	O
variational	O
distance	B
bko	O
=	O
~e	O
{	O
ip	O
{	O
y	O
=	O
iix	O
}	O
-	O
pry	O
=	O
dix	O
}	O
i	O
}	O
2	O
1	O
=	O
``	O
2	O
e	O
{	O
/21	O
]	O
(	O
x	O
)	O
-	O
11	O
}	O
captures	O
the	O
distance	B
between	O
the	O
two	O
classes	O
.	O
we	O
will	O
not	O
need	O
anything	O
special	O
to	O
deal	O
with	O
bko	O
as	O
l*	O
=	O
e	O
g	O
-~12~	O
(	O
x	O
)	O
-	O
ii	O
}	O
1	O
i	O
''	O
2	O
-	O
``	O
2e	O
{	O
12ry	O
(	O
x	O
)	O
-	O
11	O
}	O
i	O
''	O
2	O
-	O
okq	O
'	O
=	O
=	O
3.3	O
the	O
nearest	B
neighbor	I
error	I
the	O
asymptotic	O
error	O
of	O
the	O
nearest	B
neighbor	I
rule	I
is	O
lnn	O
=	O
e	O
{	O
2ry	O
(	O
x	O
)	O
(	O
1	O
-	O
ry	O
(	O
x	O
)	O
)	O
}	O
(	O
see	O
chapter	O
5	O
)	O
.	O
clearly	O
,	O
2ryo	O
-ry	O
)	O
2	O
:	O
min	O
(	O
1	O
]	O
,	O
1	O
-ry	O
)	O
as	O
2	O
max	O
(	O
1	O
]	O
,	O
1	O
-ry	O
)	O
2	O
:	O
1.	O
also	O
,	O
using	O
the	O
notation	O
a	O
=	O
min	O
(	O
1	O
]	O
(	O
x	O
)	O
,	O
1	O
-	O
ry	O
(	O
x	O
)	O
)	O
,	O
we	O
have	O
l	O
*	O
:	O
:	O
:	O
lnn	O
=	O
2e	O
{	O
ao	O
-	O
a	O
)	O
}	O
<	O
2e	O
{	O
a	O
}	O
·	O
e	O
{	O
1	O
-	O
a	O
}	O
(	O
by	O
the	O
second	O
association	B
inequality	I
of	O
theorem	B
a.19	O
)	O
=	O
2l*	O
(	O
1-l*	O
)	O
:	O
:	O
:	O
;	O
2l*	O
,	O
(	O
3.1	O
)	O
which	O
are	O
well-known	O
inequalities	O
of	O
cover	O
and	O
hart	O
(	O
1967	O
)	O
.	O
lnn	O
provides	O
us	O
with	O
quite	O
a	O
bit	O
of	O
information	O
about	O
l	O
*	O
.	O
the	O
measure	B
lnn	O
has	O
been	O
rediscovered	O
under	O
other	O
guises	O
:	O
devijver	O
and	O
kittler	O
(	O
1982	O
,	O
p.263	O
)	O
and	O
vajda	O
(	O
1968	O
)	O
call	O
it	O
the	O
quadratic	O
entropy	O
,	O
and	O
mathai	O
and	O
rathie	O
(	O
975	O
)	O
refer	O
to	O
it	O
as	O
the	O
harmonic	O
mean	O
coefficient	O
.	O
3.4	O
the	O
bhattacharyya	O
affinity	O
23	O
figure	O
3.1.	O
relationship	O
between	O
the	O
bayes	O
error	O
and	O
the	O
asymptotic	O
near	O
(	O
cid:173	O
)	O
est	O
neighbor	O
error	O
.	O
every	O
point	O
in	O
the	O
unshaded	O
region	O
is	O
possible	O
.	O
112	O
l*~nn	O
~n~2l*	O
(	O
1-l*	O
)	O
o	O
112	O
3.4	O
the	O
bhattacharyya	O
affinity	O
the	O
bhattacharyya	O
measure	B
of	O
affinity	O
(	O
bhattacharyya	O
,	O
(	O
1946	O
)	O
)	O
is	O
-	O
log	O
p	O
,	O
where	O
p	O
=	O
e	O
{	O
)	O
7j	O
(	O
x	O
)	O
(	O
1	O
-	O
7j	O
(	O
x	O
)	O
)	O
}	O
will	O
be	O
referred	O
to	O
as	O
the	O
matushita	O
error	O
.	O
it	O
does	O
not	O
occur	O
naturally	O
as	O
the	O
limit	O
of	O
any	O
standard	B
discrimination	O
rule	B
(	O
see	O
,	O
however	O
,	O
problem	O
6.11	O
)	O
.	O
p	O
was	O
suggested	O
as	O
a	O
distance	O
measure	B
for	O
pattern	O
recognition	O
by	O
matushita	O
(	O
1956	O
)	O
.	O
it	O
also	O
occurs	O
under	O
other	O
guises	O
in	O
mathematical	O
statistics-see	O
,	O
for	O
example	O
,	O
the	O
hellinger	O
distance	B
literature	O
(	O
le	O
cam	O
(	O
1970	O
)	O
,	O
beran	O
(	O
1977	O
)	O
)	O
.	O
clearly	O
,	O
p	O
=	O
0	O
if	O
and	O
only	O
if	O
7j	O
(	O
x	O
)	O
e	O
{	O
o	O
,	O
i	O
}	O
with	O
probability	O
one	O
,	O
that	O
is	O
,	O
if	O
l	O
*	O
=	O
o.	O
furthermore	O
,	O
p	O
takes	O
its	O
maximal	O
value	O
1/2	O
if	O
and	O
only	O
if	O
7j	O
(	O
x	O
)	O
=	O
1/2	O
with	O
probability	O
one	O
.	O
the	O
relationship	O
between	O
p	O
and	O
l	O
*	O
is	O
not	O
linear	O
though	O
.	O
we	O
will	O
show	O
that	O
for	O
all	O
distributions	O
,	O
lnn	O
is	O
more	O
useful	O
than	O
p	O
if	O
it	O
is	O
to	O
be	O
used	O
as	O
an	O
approximation	O
of	O
l	O
*	O
.	O
theorem	B
3.1.	O
for	O
all	O
distributions	O
,	O
we	O
have	O
1	O
-	O
-	O
-j1-	O
4p2	O
<	O
2	O
1	O
2	O
1	O
-	O
-	O
-	O
)	O
1-	O
2l	O
t	O
2	O
1	O
2	O
nn	O
<	O
l*	O
<	O
p.	O
proof	O
.	O
first	O
of	O
all	O
,	O
p2	O
e2	O
{	O
j	O
7j	O
(	O
x	O
)	O
(	O
1	O
-	O
7j	O
(	O
x	O
)	O
)	O
}	O
<	O
e	O
{	O
7j	O
(	O
x	O
)	O
(	O
l	O
-	O
7j	O
(	O
x	O
)	O
)	O
}	O
(	O
by	O
jensen	O
's	O
inequality	B
)	O
24	O
3.	O
inequalities	O
and	O
alternate	O
distance	B
measures	O
=	O
<	O
l	O
*	O
(	O
1	O
-	O
l	O
*	O
)	O
(	O
by	O
the	O
cover-hart	O
inequality	B
(	O
3.1	O
»	O
.	O
second	O
,	O
as	O
j11	O
(	O
i	O
-	O
finally	O
,	O
by	O
the	O
cover-hart	O
inequality	B
,	O
11	O
)	O
~	O
211	O
(	O
1	O
-17	O
)	O
for	O
all	O
11	O
e	O
[	O
0,1	O
]	O
,	O
we	O
see	O
that	O
p	O
~	O
lnn	O
~	O
l*	O
.	O
)	O
1-	O
2lnn	O
~	O
)	O
1-	O
4l*	O
(	O
1-	O
l*	O
)	O
=	O
1	O
-	O
2l*	O
.	O
putting	O
all	O
these	O
things	O
together	O
establishes	O
the	O
chain	O
of	O
inequalities	O
.	O
0	O
112	O
figure	O
3.2.	O
the	O
inequalities	O
linking	O
p	O
to	O
l	O
*	O
are	O
illustrated	O
.	O
note	O
that	O
the	O
re	O
(	O
cid:173	O
)	O
gion	O
is	O
larger	O
than	O
that	O
cut	O
out	O
in	O
the	O
(	O
l	O
*	O
,	O
lnn	O
)	O
plane	O
in	O
figure	O
3.1	O
.	O
4p2~1-	O
(	O
1-2l*	O
)	O
2	O
p	O
112	O
the	O
inequality	B
lnn	O
:	O
s	O
p	O
is	O
due	O
to	O
ito	O
(	O
1972	O
)	O
.	O
the	O
inequality	B
lnn	O
~	O
2p2	O
is	O
due	O
to	O
horibe	O
(	O
1970	O
)	O
.	O
the	O
inequality	B
1/2	O
-	O
)	O
1	O
-	O
4p2/2	O
:	O
s	O
l	O
*	O
:	O
s	O
p	O
can	O
be	O
found	O
in	O
kailath	O
(	O
1967	O
)	O
.	O
the	O
left-hand	O
side	O
of	O
the	O
last	O
inequality	B
was	O
shown	O
by	O
hudimoto	O
(	O
1957	O
)	O
.	O
all	O
these	O
inequalities	O
are	O
tight	O
(	O
see	O
problem	O
3.2	O
)	O
.	O
the	O
appeal	O
of	O
quantities	O
like	O
lnn	O
and	O
p	O
is	O
that	O
they	O
involve	O
polynomials	O
of	O
11	O
,	O
whereas	O
l	O
*	O
=	O
e	O
{	O
min	O
(	O
11	O
(	O
x	O
)	O
,	O
1	O
-	O
11	O
(	O
x	O
»	O
}	O
is	O
nonpolynomial	O
.	O
for	O
certain	O
discrimination	O
problems	O
in	O
which	O
x	O
has	O
a	O
distribution	O
that	O
is	O
known	O
up	O
to	O
certain	O
parameters	O
,	O
one	O
may	O
be	O
able	O
to	O
compute	O
lnn	O
and	O
p	O
explicitly	O
as	O
a	O
function	O
of	O
these	O
parameters	O
.	O
via	O
inequalities	O
,	O
this	O
may	O
then	O
be	O
used	O
to	O
obtain	O
performance	O
guarantees	O
for	O
parametric	O
discrimination	O
rules	O
of	O
the	O
plug-in	O
type	O
(	O
see	O
chapter	O
16	O
)	O
.	O
for	O
completeness	O
,	O
we	O
mention	O
a	O
generalization	O
of	O
bhattacharyya	O
's	O
measure	B
of	O
affinity	O
,	O
first	O
suggested	O
by	O
chernoff	O
(	O
1952	O
)	O
:	O
where	O
ex	O
e	O
(	O
0	O
,	O
1	O
)	O
is	O
fixed	O
.	O
for	O
ex	O
=	O
1/2	O
,	O
be	O
=	O
-log	O
p.	O
the	O
asymmetry	O
introduced	O
by	O
taking	O
ex	O
=/1/2	O
has	O
no	O
practical	O
interpretation	O
,	O
however	O
.	O
3.5	O
entropy	B
the	O
entropy	B
of	O
a	O
discrete	O
probability	O
distribution	B
(	O
pi	O
,	O
p2	O
,	O
...	O
)	O
is	O
defined	O
by	O
3.5	O
entropy	B
25	O
1	O
{	O
=	O
1	O
{	O
(	O
pi	O
,	O
p2	O
,	O
...	O
)	O
=	O
-	O
lpi	O
log	O
pi	O
,	O
00	O
i=1	O
where	O
,	O
by	O
definition	O
,	O
0	O
log	O
0	O
=	O
0	O
(	O
shannon	O
(	O
1948	O
)	O
)	O
.	O
the	O
key	O
quantity	O
in	O
information	O
theory	O
(	O
see	O
cover	O
and	O
thomas	O
(	O
1991	O
)	O
)	O
,	O
it	O
has	O
countless	O
applications	O
in	O
many	O
branches	O
of	O
computer	O
science	O
,	O
mathematical	O
statistics	B
,	O
and	O
physics	O
.	O
the	O
entropy	B
's	O
main	O
properties	O
may	O
be	O
summarized	O
as	O
follows	O
.	O
a	O
.	O
1	O
{	O
:	O
:	O
:	O
:	O
0	O
with	O
equality	O
if	O
and	O
only	O
if	O
pi	O
=	O
1	O
for	O
some	O
i.	O
proof	O
:	O
log	O
pi	O
:	O
:	O
:	O
:	O
0	O
for	O
all	O
i	O
with	O
equality	O
if	O
and	O
only	O
if	O
pi	O
=	O
1	O
for	O
some	O
i.	O
thus	O
,	O
entropy	B
is	O
minimal	O
for	O
a	O
degenerate	O
distribution	B
,	O
i.e.	O
,	O
a	O
distribution	O
with	O
the	O
least	O
amount	O
of	O
''	O
spread	O
.	O
''	O
b	O
.	O
1	O
{	O
(	O
pi	O
,	O
...	O
,	O
pk	O
)	O
:	O
:	O
:	O
:	O
log	O
k	O
with	O
equality	O
if	O
and	O
only	O
if	O
pi	O
=	O
p2	O
=	O
...	O
=	O
pk	O
=	O
1/	O
k.	O
in	O
other	O
words	O
,	O
the	O
entropy	B
is	O
maximal	O
when	O
the	O
distribution	B
is	O
maxi	O
(	O
cid:173	O
)	O
mally	O
smeared	O
out	O
.	O
proof	O
:	O
by	O
the	O
inequality	B
logx	O
:	O
:	O
:	O
:	O
x	O
1	O
,	O
x	O
>	O
o.	O
c.	O
for	O
a	O
bernoulli	O
distribution	B
(	O
p	O
,	O
1	O
-	O
p	O
)	O
,	O
the	O
binary	B
entropy	O
1	O
{	O
(	O
p	O
,	O
1	O
-	O
p	O
)	O
=	O
-	O
p	O
log	O
p	O
-	O
(	O
1	O
-	O
p	O
)	O
log	O
(	O
1	O
-	O
p	O
)	O
is	O
concave	O
in	O
p.	O
assume	O
that	O
x	O
is	O
a	O
discrete	O
random	O
variable	B
that	O
must	O
be	O
guessed	O
by	O
asking	O
questions	O
of	O
the	O
type	O
``	O
is	O
x	O
e	O
a	O
?	O
,	O
''	O
for	O
some	O
sets	O
a.	O
let	O
n	O
be	O
the	O
minimum	O
expected	O
number	O
of	O
questions	O
required	O
to	O
determine	O
x	O
with	O
certainty	O
.	O
it	O
is	O
well	O
known	O
that	O
1	O
{	O
--	O
<	O
n	O
<	O
--	O
+1	O
log2	O
-	O
1	O
{	O
log	O
2	O
(	O
e.g.	O
,	O
cover	O
and	O
thomas	O
(	O
1991	O
)	O
)	O
.	O
thus	O
,	O
1	O
{	O
not	O
only	O
measures	O
how	O
spread	O
out	O
the	O
mass	O
of	O
x	O
is	O
,	O
but	O
also	O
provides	O
us	O
with	O
concrete	O
computational	O
bounds	O
for	O
certain	O
algorithms	O
.	O
in	O
the	O
simple	O
example	O
above	O
,	O
1	O
{	O
is	O
in	O
fact	O
proportional	O
to	O
the	O
expected	O
computational	O
time	O
of	O
the	O
best	O
algorithm	B
.	O
we	O
are	O
not	O
interested	O
in	O
information	O
theory	O
per	O
se	O
,	O
but	O
rather	O
in	O
its	O
usefulness	O
in	O
pattern	O
recognition	O
.	O
for	O
our	O
discussion	O
,	O
if	O
we	O
fix	O
x	O
=	O
x	O
,	O
then	O
y	O
is	O
bernoulli	O
(	O
rj	O
(	O
x	O
)	O
)	O
.	O
hence	O
,	O
the	O
conditional	O
entropy	B
of	O
y	O
given	O
x	O
=	O
x	O
is	O
1	O
{	O
(	O
r	O
]	O
(	O
x	O
)	O
,	O
1-	O
r	O
]	O
(	O
x	O
)	O
)	O
=	O
-rj	O
(	O
x	O
)	O
logrj	O
(	O
x	O
)	O
-	O
(	O
1-	O
rj	O
(	O
x	O
)	O
)	O
log	O
(	O
1-	O
r	O
]	O
(	O
x	O
)	O
)	O
.	O
it	O
measures	O
the	O
amount	O
of	O
uncertainty	O
or	O
chaos	O
in	O
y	O
given	O
x	O
=	O
x.	O
as	O
we	O
know	O
,	O
it	O
takes	O
values	O
between	O
0	O
(	O
when	O
rj	O
(	O
x	O
)	O
e	O
{	O
o	O
,	O
i	O
}	O
)	O
and	O
log	O
2	O
(	O
when	O
r	O
]	O
(	O
x	O
)	O
=	O
1/2	O
)	O
,	O
and	O
is	O
26	O
3.	O
inequalities	O
and	O
alternate	O
distance	B
measures	O
concave	O
in	O
1j	O
(	O
x	O
)	O
.	O
we	O
define	O
the	O
expected	O
conditional	O
entropy	B
by	O
e	O
=	O
e	O
{	O
11	O
(	O
1j	O
(	O
x	O
)	O
,	O
1	O
-	O
1j	O
(	O
x	O
)	O
)	O
}	O
=	O
-e	O
{	O
1j	O
(	O
x	O
)	O
log	O
1j	O
(	O
x	O
)	O
+	O
(	O
1	O
-	O
1l	O
(	O
x	O
)	O
log	O
(	O
1	O
-	O
1j	O
(	O
x	O
»	O
}	O
.	O
for	O
brevity	O
,	O
we	O
will	O
refer	O
to	O
e	O
as	O
the	O
entropy	B
.	O
as	O
pointed	O
out	O
above	O
,	O
e	O
=	O
0	O
if	O
and	O
only	O
if	O
1j	O
(	O
x	O
)	O
e	O
{	O
o	O
,	O
l	O
}	O
with	O
probability	O
one	O
.	O
thus	O
,	O
e	O
and	O
l	O
*	O
are	O
related	O
to	O
each	O
other	O
.	O
theorem	B
3.2.	O
a.	O
e	O
:	O
:	O
:	O
:	O
11	O
(	O
l*	O
,	O
1	O
-	O
l*	O
)	O
=	O
-l*logl*	O
-	O
(	O
1	O
-	O
l*	O
)	O
log	O
(	O
1	O
-	O
l*	O
)	O
(	O
fano	O
's	O
in	O
(	O
cid:173	O
)	O
equality	O
;	O
fano	O
(	O
1952	O
)	O
,	O
see	O
cover	O
and	O
thomas	O
(	O
1991	O
,	O
p.	O
39	O
)	O
)	O
.	O
b.	O
e	O
:	O
:	O
:	O
-log	O
(	O
1	O
-	O
l	O
nn	O
)	O
:	O
:	O
:	O
-log	O
(	O
1	O
-	O
l	O
*	O
)	O
.	O
c.	O
e	O
:	O
:	O
:	O
:	O
log	O
2	O
-	O
~	O
(	O
l	O
-	O
2l	O
nn	O
)	O
:	O
:	O
:	O
:	O
log	O
2	O
-	O
~	O
(	O
l-	O
2l*	O
)	O
2.	O
l*	O
1/2	O
figure	O
3.3.	O
inequalities	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
of	O
theorem	O
3.2	O
are	O
illus	O
(	O
cid:173	O
)	O
trated	O
here	O
.	O
£	O
''	O
?	O
-log	O
(	O
i-l*	O
)	O
£	O
~	O
11	O
(	O
l*	O
,	O
i-l*	O
)	O
o	O
o	O
log	O
2	O
proof	O
.	O
part	O
a.	O
define	O
a	O
=	O
min	O
(	O
1j	O
(	O
x	O
)	O
,	O
1	O
-	O
1j	O
(	O
x	O
»	O
.	O
then	O
e	O
=	O
e	O
{	O
11	O
(	O
a	O
,	O
1	O
-	O
a	O
)	O
}	O
:	O
:	O
:	O
:	O
11	O
(	O
ea	O
,	O
1	O
-	O
ea	O
)	O
(	O
because	O
11	O
is	O
concave	O
,	O
by	O
jensen	O
's	O
inequality	B
)	O
=	O
11	O
(	O
l	O
*	O
,	O
1	O
-	O
l	O
*	O
)	O
.	O
partb	O
.	O
e	O
=	O
:	O
:	O
:	O
-e	O
{	O
a	O
log	O
a	O
+	O
(	O
l	O
-	O
a	O
)	O
log	O
(	O
l	O
-	O
a	O
)	O
}	O
-e	O
{	O
log	O
(	O
a2	O
+	O
(	O
l	O
-	O
a	O
)	O
2	O
)	O
}	O
-e	O
{	O
log	O
(	O
l	O
-	O
2a	O
(	O
l	O
-	O
a	O
»	O
)	O
}	O
(	O
by	O
jensen	O
's	O
inequality	B
)	O
>	O
-log	O
(	O
1	O
-	O
e	O
{	O
2a	O
(	O
1	O
-	O
a	O
)	O
}	O
)	O
(	O
by	O
jensen	O
's	O
inequality	B
)	O
3.6	O
jeffreys	O
'	O
divergence	B
27	O
=	O
>	O
-log	O
(	O
1	O
-	O
l	O
nn	O
)	O
-log	O
(	O
1	O
-	O
l	O
*	O
)	O
.	O
part	O
c.	O
by	O
the	O
concavity	O
of	O
h	O
(	O
a	O
,	O
1	O
-	O
a	O
)	O
as	O
a	O
function	O
of	O
a	O
,	O
and	O
taylor	O
series	O
expansion	O
,	O
h	O
(	O
a,1	O
a	O
)	O
:	O
:	O
:	O
;	O
log	O
2	O
-	O
2	O
(	O
2a	O
-	O
1	O
)	O
2	O
.	O
1	O
therefore	O
,	O
by	O
part	O
a	O
,	O
=	O
log2	O
1	O
-	O
(	O
1	O
2	O
<	O
=	O
*	O
log	O
2	O
-	O
2	O
+	O
2l	O
(	O
1	O
-	O
l	O
)	O
*	O
1	O
(	O
by	O
the	O
cover-hart	O
inequality	B
(	O
3.1	O
)	O
)	O
1	O
,	O
2	O
log	O
2	O
-	O
2	O
(	O
1	O
-	O
2l'1	O
'	O
)	O
.	O
0	O
remark	O
.	O
the	O
nearly	O
monotone	O
relationship	O
between	O
£	O
and	O
l	O
*	O
will	O
see	O
lots	O
of	O
uses	O
.	O
we	O
warn	O
the	O
reader	O
that	O
near	O
the	O
origin	O
,	O
l	O
*	O
may	O
decrease	O
linearly	O
in	O
£	O
,	O
but	O
it	O
may	O
also	O
decrease	O
much	O
faster	O
than	O
£209	O
.	O
such	O
wide	O
variation	B
was	O
not	O
observed	O
in	O
the	O
relationship	O
between	O
l	O
*	O
and	O
lnn	O
(	O
where	O
it	O
is	O
linear	O
)	O
or	O
l	O
*	O
and	O
p	O
(	O
where	O
it	O
is	O
between	O
linear	O
and	O
quadratic	O
)	O
.	O
0	O
3.6	O
jeffreys	O
'	O
divergence	B
jeffreys	O
'	O
divergence	B
(	O
1948	O
)	O
is	O
a	O
symmetric	O
form	O
of	O
the	O
kullback-leibler	O
(	O
1951	O
)	O
divergence	B
okl	O
=	O
e	O
{	O
rj	O
(	O
x	O
)	O
log	O
rj	O
(	O
x	O
)	O
1	O
-	O
rj	O
(	O
x	O
)	O
}	O
.	O
it	O
will	O
be	O
denoted	O
by	O
j	O
=	O
e	O
(	O
2rj	O
(	O
x	O
)	O
-	O
1	O
)	O
log	O
{	O
rj	O
(	O
x	O
)	O
1	O
-	O
rj	O
(	O
x	O
)	O
}	O
.	O
to	O
understand	O
j	O
,	O
note	O
that	O
the	O
function	O
(	O
2rj	O
-	O
1	O
)	O
log	O
1~t/	O
is	O
symmetric	O
about	O
1/2	O
,	O
convex	O
,	O
and	O
has	O
minimum	O
(	O
0	O
)	O
at	O
rj	O
=	O
1/2	O
.	O
as	O
rj	O
+	O
0	O
,	O
rj	O
t	O
1	O
,	O
the	O
function	O
becomes	O
unbounded	O
.	O
therefore	O
,	O
j	O
=	O
00	O
if	O
p	O
{	O
rj	O
(	O
x	O
)	O
e	O
{	O
o	O
,	O
i	O
}	O
}	O
>	O
0.	O
for	O
this	O
reason	O
,	O
its	O
28	O
3.	O
inequalities	O
and	O
alternate	O
distance	B
measures	O
use	O
in	O
discrimination	O
is	O
necessarily	O
limited	O
.	O
for	O
generalizations	O
of	O
j	O
,	O
see	O
renyi	O
(	O
1961	O
)	O
,	O
burbea	O
and	O
rao	O
(	O
1982	O
)	O
,	O
taneja	O
(	O
1983	O
;	O
1987	O
)	O
,	O
and	O
burbea	O
(	O
1984	O
)	O
.	O
it	O
is	O
thus	O
impossible	O
to	O
bound	O
j	O
from	O
above	O
by	O
a	O
function	O
of	O
lnn	O
and/or	O
l	O
*	O
.	O
however	O
,	O
lower	O
bounds	O
are	O
easy	O
to	O
obtain	O
.	O
as	O
x	O
log	O
(	O
(	O
1	O
+	O
x	O
)	O
/	O
(	O
1	O
-	O
x	O
)	O
)	O
is	O
convex	O
in	O
x	O
and	O
(	O
21	O
]	O
-	O
1	O
)	O
log	O
-	O
-	O
=	O
121	O
]	O
-	O
1	O
]	O
1-1	O
]	O
11	O
log	O
(	O
1	O
+	O
121	O
]	O
-	O
11	O
)	O
1-121	O
]	O
-11	O
,	O
we	O
note	O
that	O
by	O
jensen	O
's	O
inequality	B
,	O
j	O
2	O
:	O
e	O
{	O
121	O
]	O
(	O
x	O
)	O
-	O
11	O
}	O
log	O
1	O
_	O
e	O
{	O
121	O
]	O
(	O
x	O
)	O
_	O
11	O
}	O
(	O
1	O
+	O
e	O
{	O
121	O
]	O
(	O
x	O
)	O
-	O
ii	O
}	O
)	O
*	O
(	O
1	O
+	O
(	O
1	O
-	O
2l	O
*	O
)	O
)	O
(	O
1	O
-	O
2l	O
)	O
log	O
1	O
_	O
(	O
1	O
2l*	O
)	O
=	O
(	O
1-	O
2l*	O
)	O
log	O
~	O
1	O
-	O
l*	O
)	O
(	O
>	O
2	O
(	O
1	O
-	O
2l	O
*	O
)	O
2.	O
the	O
first	O
bound	O
can	O
not	O
be	O
universally	O
bettered	O
(	O
it	O
is	O
achieved	O
when	O
1	O
]	O
(	O
x	O
)	O
is	O
constant	O
over	O
the	O
space	O
)	O
.	O
also	O
,	O
for	O
fixed	O
l	O
*	O
,	O
any	O
value	O
of	O
j	O
above	O
the	O
lower	O
bound	O
is	O
possible	O
for	O
some	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
.	O
from	O
the	O
definition	B
of	I
j	O
,	O
we	O
see	O
that	O
j	O
=	O
°	O
if	O
and	O
only	O
if	O
1	O
]	O
==	O
1/2	O
with	O
probability	O
one	O
,	O
or	O
l	O
*	O
=	O
1/2	O
.	O
figure	O
3.4.	O
this	O
figure	O
illustrates	O
the	O
above	O
lower	O
bound	O
on	O
jeffreys	O
'	O
divergence	B
in	O
terms	O
of	O
the	O
bayes	O
er	O
(	O
cid:173	O
)	O
ror	O
.	O
~	O
--	O
j	O
?	O
:	O
.	O
(	O
l-2l*	O
)	O
log	O
(	O
(	O
l-c	O
)	O
/l*	O
)	O
l*	O
112	O
related	O
bounds	O
were	O
obtained	O
by	O
toussaint	O
(	O
1974b	O
)	O
:	O
(	O
1	O
+	O
jl	O
-	O
2lnn	O
)	O
j	O
2	O
:	O
v	O
1	O
-	O
2lnn	O
log	O
1	O
_	O
jl	O
_	O
2lnn	O
/	O
2	O
:	O
2	O
(	O
1	O
-	O
2lnn	O
)	O
.	O
the	O
last	O
bound	O
is	O
strictly	O
better	O
than	O
our	O
l	O
*	O
bound	O
given	O
above	O
.	O
see	O
problem	O
3.7	O
.	O
3.7	O
f-errors	O
the	O
error	O
measures	O
discussed	O
so	O
far	O
are	O
all	O
related	O
to	O
expected	O
values	O
of	O
concave	O
functions	O
of	O
1	O
]	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
lix	O
}	O
.	O
in	O
general	O
,	O
if	O
f	O
is	O
a	O
concave	O
function	O
on	O
[	O
0	O
,	O
1	O
]	O
,	O
3.7	O
f-errors	O
29	O
we	O
define	O
the	O
f	O
-error	O
corresponding	O
to	O
(	O
x	O
,	O
y	O
)	O
by	O
df	O
(	O
x	O
,	O
y	O
)	O
=	O
e	O
{	O
f	O
(	O
1	O
]	O
(	O
x	O
»	O
}	O
.	O
examples	O
of	O
f	O
-errors	O
are	O
(	O
a	O
)	O
the	O
bayes	O
error	O
l	O
*	O
:	O
f	O
(	O
x	O
)	O
=	O
min	O
(	O
x	O
,	O
1	O
-	O
x	O
)	O
,	O
(	O
b	O
)	O
the	O
asymptotic	O
nearest	O
neighbor	O
error	O
lnn	O
:	O
f	O
(	O
x	O
)	O
=	O
2x	O
(	O
1	O
-	O
x	O
)	O
,	O
(	O
c	O
)	O
the	O
matushita	O
error	O
p	O
:	O
f	O
(	O
x	O
)	O
=	O
y'x	O
(	O
1	O
-	O
x	O
)	O
,	O
(	O
d	O
)	O
the	O
expected	O
conditional	O
entropy	B
[	O
:	O
f	O
(	O
x	O
)	O
=	O
-x	O
log	O
x	O
-	O
(	O
1	O
-	O
x	O
)	O
log	O
(	O
1	O
-	O
x	O
)	O
,	O
(	O
e	O
)	O
the	O
negated	O
jeffreys	O
'	O
divergence	B
-j	O
:	O
f	O
(	O
x	O
)	O
=	O
-	O
(	O
2x	O
-	O
1	O
)	O
log	O
l~x	O
'	O
hashlamoun	O
,	O
varshney	O
,	O
and	O
samarasooriya	O
(	O
1994	O
)	O
point	O
out	O
that	O
if	O
f	O
(	O
x	O
)	O
>	O
min	O
(	O
x	O
,	O
l	O
-	O
x	O
)	O
for	O
each	O
x	O
e	O
[	O
0,1	O
]	O
,	O
then	O
the	O
corresponding	O
f-error	O
is	O
an	O
upper	O
bound	O
on	O
the	O
bayes	O
error	O
.	O
the	O
closer	O
f	O
(	O
x	O
)	O
is	O
to	O
min	O
(	O
x	O
,	O
1	O
-	O
x	O
)	O
,	O
the	O
tighter	O
the	O
upper	O
bound	O
is	O
.	O
for	O
example	O
,	O
f	O
(	O
x	O
)	O
=	O
(	O
1/2	O
)	O
sin	O
(	O
jtx	O
)	O
:	O
:	O
:	O
2x	O
(	O
1	O
x	O
)	O
yields	O
an	O
upper	O
bound	O
tighter	O
than	O
l	O
nn	O
•	O
all	O
these	O
errors	O
share	O
the	O
property	O
that	O
the	O
error	O
increases	O
if	O
x	O
is	O
transformed	O
by	O
an	O
arbitrary	O
function	O
.	O
theorem	B
3.3.	O
let	O
t	O
:	O
nd	O
-+	O
nk	O
be	O
an	O
arbitrary	O
measurable	O
function	O
.	O
thenfor	O
any	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
proof	O
.	O
define	O
1	O
]	O
[	O
:	O
nk	O
-+	O
[	O
0	O
,	O
1	O
]	O
by	O
1	O
]	O
t	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
1it	O
(	O
x	O
)	O
=	O
x	O
}	O
,	O
and	O
observe	O
that	O
1	O
]	O
t	O
(	O
t	O
(	O
x	O
»	O
=	O
e	O
{	O
1	O
]	O
(	O
x	O
)	O
lt	O
(	O
x	O
)	O
}	O
.	O
thus	O
,	O
e	O
{	O
f	O
(	O
1	O
]	O
t	O
(	O
t	O
(	O
x	O
»	O
)	O
}	O
e	O
{	O
f	O
(	O
e	O
{	O
1	O
]	O
(	O
x	O
)	O
lt	O
(	O
x	O
)	O
}	O
)	O
}	O
>	O
e	O
{	O
e	O
{	O
f	O
(	O
1	O
]	O
(	O
x	O
»	O
lt	O
(	O
x	O
)	O
}	O
}	O
(	O
by	O
jensen	O
's	O
inequality	B
)	O
e	O
{	O
f	O
(	O
1	O
]	O
(	O
x	O
»	O
}	O
=	O
df	O
(	O
x	O
,	O
y	O
)	O
.	O
0	O
remark	O
.	O
we	O
also	O
see	O
from	O
the	O
proof	O
that	O
the	O
f	O
-error	O
remains	O
unchanged	O
if	O
the	O
transformation	O
t	O
is	O
invertible	O
.	O
theorem	B
3.3	O
states	O
that	O
f	O
-errors	O
are	O
a	O
bit	O
like	O
bayes	O
errors-when	O
information	O
is	O
lost	O
(	O
by	O
replacing	O
x	O
by	O
t	O
(	O
x	O
»	O
,	O
f	O
-errors	O
increase	O
.	O
0	O
30	O
3.	O
inequalities	O
and	O
alternate	O
distance	B
measures	O
3.8	O
the	O
mahalanobis	O
distance	B
two	O
conditional	O
distributions	O
with	O
about	O
the	O
same	O
covariance	O
matrices	O
and	O
means	O
that	O
are	O
far	O
away	O
from	O
each	O
other	O
are	O
probably	O
so	O
well	O
separated	O
that	O
l	O
*	O
is	O
small	O
.	O
an	O
interesting	O
measure	B
of	O
the	O
visual	O
distance	B
between	O
two	O
random	O
variables	O
xo	O
and	O
xl	O
is	O
the	O
so-called	O
mahalanobis	O
distance	B
(	O
mahalanobis	O
,	O
(	O
1936	O
)	O
)	O
given	O
by	O
whereml	O
=	O
ex1	O
,	O
mo	O
=	O
exo	O
,	O
are	O
the	O
means	O
,	O
~l	O
=	O
e	O
{	O
(	O
xl	O
-	O
md	O
(	O
xl	O
-	O
mil	O
}	O
and	O
~o	O
=	O
e	O
{	O
(	O
xo	O
-	O
mo	O
)	O
(	O
xo	O
-	O
mol	O
}	O
are	O
the	O
covariance	O
matrices	O
,	O
~	O
=	O
p	O
~l	O
+	O
(	O
1	O
-	O
p	O
)	O
~o	O
,	O
(	O
·l	O
is	O
the	O
transpose	O
of	O
a	O
vector	O
,	O
and	O
p	O
=	O
1	O
-	O
p	O
is	O
a	O
mixture	O
parameter	O
.	O
if	O
~l	O
=	O
~o	O
=	O
(	O
j2	O
i	O
,	O
where	O
i	O
is	O
the	O
identity	O
matrix	O
,	O
then	O
~	O
=	O
_ii	O
m_l	O
_-_m_o_11	O
is	O
a	O
scaled	O
version	O
of	O
the	O
distance	B
between	O
the	O
means	O
.	O
if	O
~l	O
=	O
(	O
jf	O
i	O
,	O
~o	O
=	O
(	O
j5	O
i	O
,	O
then	O
iiml	O
-	O
moll	O
~	O
=	O
--	O
-	O
;	O
======	O
j	O
p	O
(	O
jf	O
+	O
(	O
1	O
-	O
p	O
)	O
(	O
j5	O
varies	O
between	O
ilml	O
-moll/	O
(	O
jl	O
and	O
ilml	O
-moll/	O
(	O
jo	O
as	O
p	O
changes	O
from	O
1	O
too	O
.	O
assume	O
that	O
we	O
have	O
a	O
discrimination	O
problem	O
in	O
which	O
given	O
y	O
=	O
1	O
,	O
x	O
is	O
distributed	O
as	O
xl	O
,	O
given	O
y	O
=	O
0	O
,	O
x	O
is	O
distributed	O
as	O
xo	O
,	O
and	O
p	O
=	O
p	O
{	O
y	O
=	O
1	O
}	O
,	O
1	O
-	O
p	O
are	O
the	O
class	O
probabilities	O
.	O
then	O
,	O
interestingly	O
,	O
~	O
is	O
related	O
to	O
the	O
bayes	O
error	O
in	O
a	O
general	O
sense	O
.	O
if	O
the	O
mahalanobis	O
distance	B
between	O
the	O
class-conditional	B
distributions	O
is	O
large	O
,	O
then	O
l	O
*	O
is	O
small	O
.	O
theorem	B
3.4	O
.	O
(	O
devijver	O
and	O
kittler	O
(	O
1982	O
,	O
p.	O
166	O
)	O
)	O
.	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
for	O
which	O
e	O
{	O
iixi12	O
}	O
<	O
00	O
,	O
we	O
have	O
l*	O
<	O
l	O
<	O
-	O
nn	O
-	O
2p	O
(	O
1	O
p	O
)	O
1	O
+	O
p	O
(	O
1	O
_	O
p	O
)	O
~2	O
remark	O
.	O
for	O
a	O
distribution	O
with	O
mean	O
m	O
and	O
covariance	O
matrix	O
~	O
,	O
the	O
mahalanobis	O
distance	B
from	O
a	O
point	O
x	O
e	O
n	O
d	O
to	O
m	O
is	O
in	O
one	O
dimension	B
,	O
this	O
is	O
simply	O
interpreted	O
as	O
distance	B
from	O
the	O
mean	O
as	O
measured	O
in	O
units	O
of	O
standard	O
deviation	O
.	O
the	O
use	O
of	O
mahalanobis	O
distance	B
in	O
discrimination	O
is	O
based	O
upon	O
the	O
intuitive	O
notion	O
that	O
we	O
should	O
classify	O
according	O
to	O
the	O
class	O
for	O
which	O
we	O
are	O
within	O
the	O
least	O
units	O
of	O
standard	O
deviations	O
.	O
at	O
least	O
,	O
for	O
distributions	O
that	O
look	O
like	O
nice	O
globular	O
clouds	O
,	O
such	O
a	O
recommendation	O
may	O
make	O
sense	O
.	O
0	O
3.9	O
f-divergences	O
31	O
proof	O
.	O
first	O
assume	O
that	O
d	O
=	O
1	O
,	O
that	O
is	O
,	O
x	O
is	O
real	O
valued	O
.	O
let	O
u	O
and	O
c	O
be	O
real	O
numbers	O
,	O
and	O
consider	O
the	O
quantity	O
e	O
{	O
(	O
u	O
(	O
x	O
-	O
c	O
)	O
-	O
(	O
217	O
(	O
x	O
)	O
-	O
1	O
)	O
)	O
2	O
}	O
.	O
we	O
will	O
show	O
that	O
if	O
u	O
and	O
c	O
are	O
chosen	O
to	O
minimize	O
this	O
number	O
,	O
then	O
it	O
satisfies	O
o	O
:	O
s	O
e	O
{	O
(	O
u	O
(	O
x	O
-	O
c	O
)	O
-	O
(	O
217	O
(	O
x	O
)	O
-	O
1	O
)	O
)	O
2	O
}	O
=	O
2	O
2p	O
(	O
1-p	O
)	O
1	O
+	O
p	O
(	O
1	O
-	O
p	O
)	O
.6.	O
)	O
2	O
-	O
lnn	O
(	O
,	O
(	O
3.2	O
)	O
which	O
proves	O
the	O
theorem	B
for	O
d	O
=	O
1.	O
to	O
see	O
this	O
,	O
note	O
that	O
the	O
expression	B
(	O
217	O
(	O
x	O
)	O
-	O
1	O
)	O
)	O
2	O
}	O
is	O
minimized	O
for	O
c	O
=	O
ex	O
-	O
e	O
{	O
217	O
(	O
x	O
)	O
-	O
1	O
}	O
/u	O
.	O
e	O
{	O
(	O
u	O
(	O
x	O
-	O
c	O
)	O
then	O
e	O
{	O
(	O
u	O
(	O
x	O
-	O
c	O
)	O
-	O
(	O
217	O
(	O
x	O
)	O
-	O
1	O
)	O
)	O
2	O
}	O
=	O
var	O
{	O
217	O
(	O
x	O
)	O
-	O
i	O
}	O
+	O
u2	O
var	O
{	O
x	O
}	O
-	O
2u	O
cov	O
{	O
x	O
,	O
217	O
(	O
x	O
)	O
-	O
i	O
}	O
,	O
-where	O
cov	O
{	O
x	O
,	O
z	O
}	O
=	O
e	O
{	O
(	O
x	O
-	O
ex	O
)	O
(	O
z	O
-	O
ez	O
)	O
}	O
-which	O
is	O
,	O
in	O
turn	O
,	O
minimized	O
for	O
u	O
=	O
cov	O
{	O
x	O
,	O
217	O
(	O
x	O
)	O
-	O
1	O
}	O
/var	O
{	O
x	O
}	O
.	O
straightforward	O
calculation	O
shows	O
that	O
(	O
3.2	O
)	O
indeed	O
holds	O
.	O
to	O
extend	O
the	O
inequality	B
(	O
3.2	O
)	O
to	O
multidimensional	O
problems	O
,	O
apply	O
it	O
to	O
the	O
one-dimensional	O
decision	O
problem	O
(	O
z	O
,	O
y	O
)	O
,	O
where	O
z	O
=	O
xtb-l	O
(	O
ml	O
mo	O
)	O
.	O
then	O
the	O
theorem	B
follows	O
by	O
noting	O
that	O
by	O
theorem	B
3.3	O
,	O
where	O
lnn	O
(	O
x	O
,	O
y	O
)	O
denotes	O
the	O
nearest-neighbor	O
error	O
corresponding	O
to	O
(	O
x	O
,	O
y	O
)	O
.	O
0	O
in	O
case	O
x	O
i	O
and	O
xo	O
are	O
both	O
normal	B
with	O
the	O
same	O
covariance	O
matrices	O
,	O
we	O
have	O
theorem	B
3.5	O
.	O
(	O
matushita	O
(	O
1973	O
)	O
;	O
see	O
problem	O
3.11	O
)	O
.	O
when	O
xl	O
and	O
xo	O
are	O
multivariate	O
normal	O
random	O
variables	O
with	O
bl	O
=	O
bo	O
=	O
b	O
,	O
then	O
if	O
the	O
class-conditional	B
densities	O
!	O
l	O
and	O
10	O
may	O
be	O
written	O
as	O
functions	O
of	O
(	O
x	O
-	O
ml	O
)	O
tb11	O
(	O
x	O
-	O
md	O
and	O
(	O
x	O
molbol	O
(	O
x	O
-	O
mo	O
)	O
respectively	O
,	O
then.6	O
.	O
remains	O
relatively	O
tightly	O
linked	O
with	O
l	O
*	O
(	O
mitchell	O
and	O
krzanowski	O
(	O
1985	O
)	O
)	O
,	O
but	O
such	O
dis	O
(	O
cid:173	O
)	O
tributions	O
are	O
the	O
exception	O
rather	O
than	O
the	O
rule	B
.	O
in	O
general	O
,	O
when	O
.6.	O
is	O
small	O
,	O
it	O
is	O
impossible	O
to	O
deduce	O
whether	O
l	O
*	O
is	O
small	O
or	O
not	O
(	O
see	O
problem	O
3.12	O
)	O
.	O
3.9	O
f-divergences	O
we	O
have	O
defined	O
error	O
measures	O
as	O
the	O
expected	O
value	O
of	O
a	O
concave	O
function	O
of	O
17	O
(	O
x	O
)	O
.	O
this	O
makes	O
it	O
easier	O
to	O
relate	O
these	O
measures	O
to	O
the	O
bayes	O
error	O
l	O
*	O
and	O
other	O
error	O
probabilities	O
.	O
in	O
this	O
section	O
we	O
briefly	O
make	O
the	O
connection	O
to	O
the	O
more	O
classical	O
statistical	O
theory	O
of	O
distances	O
between	O
probability	O
measures	O
.	O
a	O
general	O
concept	O
of	O
these	O
distance	B
measures	O
,	O
called	O
i-divergences	O
,	O
was	O
introduced	O
by	O
csiszar	O
(	O
1967	O
)	O
.	O
32	O
3.	O
inequalities	O
and	O
alternate	O
distance	B
measures	O
the	O
corresponding	O
theory	O
is	O
summarized	O
in	O
vajda	O
(	O
1989	O
)	O
.	O
f	O
-errors	O
defined	O
earlier	O
may	O
be	O
calculated	O
if	O
one	O
knows	O
the	O
class	O
probabilities	O
p	O
,	O
1	O
-	O
p	O
,	O
and	O
the	O
conditional	O
distributions	O
flo	O
,	O
fl	O
1	O
of	O
x	O
,	O
given	O
{	O
y	O
:	O
:	O
:	O
o	O
}	O
and	O
{	O
y	O
:	O
:	O
:	O
i	O
}	O
,	O
that	O
is	O
,	O
fli	O
(	O
a	O
)	O
:	O
:	O
:	O
p	O
{	O
x	O
e	O
aiy	O
:	O
:	O
:	O
i	O
}	O
i	O
:	O
:	O
:	O
0	O
,	O
1.	O
for	O
fixed	O
class	O
probabilities	O
,	O
an	O
f	O
-error	O
is	O
small	O
if	O
the	O
two	O
conditional	O
distributions	O
are	O
``	O
far	O
away	O
''	O
from	O
each	O
other	O
.	O
a	O
metric	O
quantifying	O
this	O
distance	B
may	O
be	O
defined	O
as	O
follows	O
.	O
let	O
f	O
:	O
[	O
0	O
,	O
(	O
0	O
)	O
~	O
r	O
u	O
{	O
-oo	O
,	O
oo	O
}	O
be	O
a	O
convex	O
function	O
with	O
f	O
(	O
l	O
)	O
:	O
:	O
:	O
0.	O
the	O
f	O
-divergence	O
between	O
two	O
probability	O
measures	O
fl	O
and	O
v	O
on	O
rd	O
is	O
defined	O
by	O
di	O
(	O
fl	O
,	O
v	O
)	O
:	O
:	O
:	O
sup	O
l	O
v	O
(	O
aj	O
)	O
f	O
(	O
fl	O
(	O
a	O
j	O
)	O
)	O
,	O
a=	O
{	O
a	O
j	O
}	O
j	O
v	O
(	O
aj	O
)	O
where	O
the	O
supremum	O
is	O
taken	O
over	O
all	O
finite	O
measurable	O
partitions	O
a	O
of	O
rd	O
.	O
if	O
a	O
is	O
a	O
measure	O
dominating	O
fl	O
and	O
v-that	O
is	O
,	O
both	O
fl	O
and	O
v	O
are	O
absolutely	O
continuous	O
with	O
respect	O
to	O
a-and	O
p	O
:	O
:	O
:	O
dfl/da	O
and	O
q	O
:	O
:	O
:	O
dv/da	O
are	O
the	O
corresponding	O
densities	O
,	O
then	O
the	O
f	O
-divergence	O
may	O
be	O
put	O
in	O
the	O
form	O
di	O
(	O
fl	O
,	O
v	O
)	O
:	O
:	O
:	O
f	O
q	O
(	O
x	O
)	O
f	O
(	O
p	O
(	O
x	O
)	O
)	O
a	O
(	O
dx	O
)	O
.	O
q	O
(	O
x	O
)	O
clearly	O
,	O
this	O
quantity	O
is	O
independent	O
of	O
the	O
choice	O
of	O
a.	O
for	O
example	O
,	O
we	O
may	O
take	O
a	O
:	O
:	O
:	O
fl	O
+	O
v.	O
if	O
fl	O
and	O
v	O
are	O
absolutely	O
continuous	O
with	O
respect	O
to	O
the	O
lebesgue	O
measure	B
,	O
then	O
a	O
may	O
be	O
chosen	O
to	O
be	O
the	O
lebesgue	O
measure	B
.	O
by	O
jensen	O
's	O
inequality	B
,	O
di	O
(	O
fl	O
,	O
v	O
)	O
?	O
:	O
0	O
,	O
and	O
di	O
(	O
fl	O
,	O
fl	O
)	O
:	O
:	O
:	O
0.	O
an	O
important	O
example	O
of	O
f	O
-divergences	O
is	O
the	O
total	B
variation	I
,	O
or	O
variational	O
distance	B
obtained	O
by	O
choosing	O
f	O
(	O
x	O
)	O
:	O
:	O
:	O
ix	O
-	O
11	O
,	O
yielding	O
v	O
(	O
fl	O
,	O
v	O
)	O
:	O
:	O
:	O
sup	O
l	O
ifl	O
(	O
a	O
j	O
)	O
-	O
v	O
(	O
aj	O
)	O
l·	O
a=	O
{	O
a	O
j	O
}	O
j	O
for	O
this	O
divergence	B
,	O
the	O
equivalence	O
of	O
the	O
two	O
definitions	O
is	O
stated	O
by	O
scheffe	O
's	O
theorem	B
(	O
see	O
problem	O
12.13	O
)	O
.	O
theorem	B
3.6	O
.	O
(	O
scheffe	O
(	O
1947	O
)	O
)	O
.	O
v	O
(	O
fl	O
,	O
v	O
)	O
:	O
:	O
:	O
2	O
s~p	O
ifl	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
i	O
:	O
:	O
:	O
f	O
ip	O
(	O
x	O
)	O
-	O
q	O
(	O
x	O
)	O
ia	O
(	O
dx	O
)	O
,	O
where	O
the	O
supremum	O
is	O
taken	O
over	O
all	O
borel	O
subsets	O
of	O
rd	O
.	O
another	O
important	O
example	O
is	O
the	O
hellinger	O
distance	B
,	O
given	O
by	O
f	O
(	O
x	O
)	O
:	O
:	O
:	O
(	O
1	O
-	O
,	O
jx	O
)	O
2	O
:	O
h	O
2	O
(	O
fl	O
,	O
v	O
)	O
:	O
:	O
:	O
sup	O
2	O
(	O
1	O
-	O
l	O
j	O
fl	O
(	O
a	O
j	O
)	O
v	O
(	O
a	O
j	O
)	O
)	O
a=	O
{	O
a	O
j	O
}	O
j	O
=	O
2	O
(	O
1	O
-f	O
v	O
'	O
p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
a	O
(	O
dx	O
»	O
)	O
3.9	O
i-divergences	O
33	O
the	O
quantity	O
12	O
(	O
j.l	O
,	O
v	O
)	O
=	O
f	O
,	O
jp	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
j	O
...	O
(	O
dx	O
)	O
is	O
often	O
called	O
the	O
hellinger	O
integral	O
.	O
we	O
mention	O
two	O
useful	O
inequalities	O
in	O
this	O
respect	O
.	O
for	O
the	O
sake	O
of	O
simplicity	O
,	O
we	O
state	O
their	O
discrete	O
form	O
.	O
(	O
the	O
integral	O
forms	O
are	O
analogous	O
,	O
see	O
problem	O
3.21	O
.	O
)	O
lemma	O
3.1	O
.	O
(	O
lecam	O
(	O
1973	O
»	O
.	O
for	O
positive	O
sequences	O
ai	O
and	O
bi	O
,	O
both	O
summing	O
to	O
one	O
,	O
proof	O
.	O
by	O
the	O
cauchy-schwarz	O
inequality	B
,	O
l..t	O
yuiui_	O
''	O
r	O
;	O
;	O
e	O
;	O
<	O
i	O
:	O
ai	O
<	O
bi	O
this	O
,	O
together	O
with	O
the	O
inequality	B
(	O
x	O
+	O
y	O
)	O
2	O
:	O
:	O
:	O
2x2	O
+	O
2	O
y2	O
,	O
and	O
symmetry	O
,	O
implies	O
(	O
~ja	O
;	O
b	O
)	O
2	O
c~b	O
,	O
jaibi+	O
;	O
~	O
,	O
jaibi	O
)	O
2	O
<	O
2c~/a	O
;	O
by	O
+2c~/aibir	O
<	O
2	O
c~b	O
'	O
ai	O
+	O
;	O
~	O
,	O
bi	O
)	O
2	O
lmin	O
(	O
ai	O
'	O
bi	O
)	O
.	O
0	O
lemma	O
3.2	O
.	O
(	O
devroye	O
and	O
gyorfi	O
(	O
1985	O
,	O
p.	O
225	O
»	O
)	O
.	O
let	O
al	O
,	O
...	O
,	O
ak	O
>	O
bt	O
,	O
...	O
bk	O
be	O
nonnegative	O
numbers	O
such	O
that	O
l	O
:	O
=i	O
ai	O
=	O
l	O
:	O
=i	O
bi	O
=	O
1.	O
then	O
proof	O
.	O
(	O
by	O
the	O
cauchy-schwarz	O
inequality	B
)	O
34	O
3.	O
inequalities	O
and	O
alternate	O
distance	B
measures	O
k	O
2	O
i=l	O
<	O
4	O
l	O
(	O
y'ai	O
-	O
~	O
)	O
=	O
s	O
(	O
l-tja'h	O
}	O
which	O
proves	O
the	O
lemma	O
.	O
0	O
information	B
divergence	I
is	O
obtained	O
by	O
taking	O
f	O
(	O
x	O
)	O
=	O
x	O
log	O
x	O
:	O
i	O
(	O
il	O
,	O
v	O
)	O
is	O
also	O
called	O
the	O
kullback-leibler	O
number	O
.	O
our	O
last	O
example	O
is	O
the	O
x2-divergence	B
,	O
defined	O
by	O
f	O
(	O
x	O
)	O
=	O
(	O
x	O
-	O
1	O
)	O
2	O
:	O
'	O
``	O
(	O
il	O
(	O
a·	O
)	O
-	O
v	O
(	O
a	O
.	O
)	O
)	O
2	O
}	O
sup	O
~	O
}	O
a=	O
{	O
a	O
j	O
}	O
j	O
v	O
(	O
aj	O
)	O
f	O
p2	O
(	O
x	O
)	O
a	O
(	O
dx	O
)	O
-	O
1.	O
q	O
(	O
x	O
)	O
next	O
,	O
we	O
highlight	O
the	O
connection	O
between	O
f	O
-errors	O
and	O
f	O
-divergences	O
.	O
let	O
ilo	O
and	O
ill	O
denote	O
the	O
conditional	O
distributions	O
of	O
x	O
given	O
{	O
y	O
=	O
o	O
}	O
and	O
{	O
y	O
=	O
i	O
}	O
.	O
assume	O
that	O
the	O
class	O
probabilities	O
are	O
equal	O
:	O
p	O
=	O
1/2	O
.	O
if	O
f	O
is	O
a	O
concave	O
function	O
,	O
then	O
the	O
f	O
-error	O
df	O
(	O
x	O
,	O
y	O
)	O
may	O
be	O
written	O
as	O
where	O
f	O
(	O
x	O
)	O
=	O
-~f	O
(	O
_x_	O
)	O
(	O
1	O
+x	O
)	O
+	O
f	O
(	O
~	O
)	O
,	O
2	O
l+x	O
2	O
and	O
d	O
f	O
is	O
the	O
corresponding	O
f	O
-divergence	O
.	O
it	O
is	O
easy	O
to	O
see	O
that	O
f	O
is	O
convex	O
,	O
whenever	O
f	O
is	O
concave	O
.	O
a	O
special	O
case	O
of	O
this	O
correspondence	O
is	O
)	O
*	O
1	O
(	O
1	O
l	O
=	O
''	O
2	O
1	O
-	O
2	O
v	O
(	O
ilo	O
,	O
il	O
1	O
)	O
if	O
p	O
=	O
1/2	O
.	O
also	O
,	O
it	O
is	O
easy	O
to	O
verify	O
,	O
that	O
,	O
where	O
p	O
is	O
the	O
matushita	O
error	O
.	O
for	O
further	O
connections	O
,	O
we	O
refer	O
the	O
reader	O
to	O
the	O
exercises	O
.	O
problems	O
and	O
exercises	O
35	O
problems	O
and	O
exercises	O
problem3.1	O
.	O
showthatforevery	O
(	O
i	O
1	O
,	O
i*	O
)	O
witho	O
:	O
s	O
i*	O
:	O
s	O
i	O
1	O
:	O
s	O
21*	O
(	O
1-1*	O
)	O
:	O
s	O
1/2	O
,	O
there	O
exists	O
a	O
distribution	O
of	O
(	O
x	O
,	O
y	O
)	O
with	O
lnn	O
=	O
ii	O
andl	O
*	O
=	O
[	O
*	O
.	O
therefore	O
,	O
the	O
cover-hart	O
inequalities	O
are	O
not	O
universally	O
improvable	O
.	O
problem	O
3.2.	O
tightness	O
of	O
the	O
bounds	O
.	O
theorem	B
3.1	O
can	O
not	O
be	O
improved	O
.	O
(	O
1	O
)	O
show	O
that	O
for	O
all	O
a	O
e	O
[	O
0	O
,	O
1/2	O
]	O
,	O
there	O
exists	O
a	O
distribution	O
of	O
(	O
x	O
,	O
y	O
)	O
such	O
that	O
lnn	O
=	O
l	O
*	O
=	O
a	O
.	O
(	O
2	O
)	O
show	O
that	O
for	O
all	O
a	O
e	O
[	O
0	O
,	O
1/2	O
]	O
,	O
there	O
exists	O
a	O
distribution	O
of	O
(	O
x	O
,	O
y	O
)	O
such	O
that	O
lnn	O
=	O
a	O
,	O
l*	O
=	O
~	O
-	O
~	O
,	O
ji	O
-	O
2a	O
.	O
(	O
3	O
)	O
show	O
that	O
for	O
all	O
a	O
e	O
[	O
0	O
,	O
1/2	O
]	O
,	O
there	O
exists	O
a	O
distribution	O
of	O
(	O
x	O
,	O
y	O
)	O
such	O
that	O
l*=p=a	O
.	O
(	O
4	O
)	O
show	O
that	O
for	O
all	O
a	O
e	O
[	O
0	O
,	O
1/2	O
]	O
,	O
there	O
exists	O
a	O
distribution	O
of	O
(	O
x	O
,	O
y	O
)	O
such	O
that	O
l	O
nn-a	O
,	O
-	O
l*-	O
1	O
-2	O
''	O
-2	O
''	O
y.l-'tp-·	O
1	O
~4	O
2	O
problem	O
3.3.	O
show	O
that	O
e	O
:	O
:	O
:	O
l	O
*	O
.	O
problem	O
3.4.	O
for	O
any	O
a	O
:	O
s	O
1	O
,	O
find	O
a	O
sequence	O
of	O
distributions	O
of	O
(	O
xn	O
'	O
yn	O
)	O
having	O
expected	O
conditional	O
entropies	O
en	O
and	O
bayes	O
errors	O
l~	O
such	O
that	O
l~	O
~	O
°	O
as	O
n	O
~	O
00	O
,	O
and	O
en	O
decreases	O
to	O
zero	O
at	O
the	O
same	O
rate	O
as	O
(	O
l	O
~	O
)	O
a	O
.	O
problem	O
3.5	O
.	O
concavity	O
of	O
error	O
measures	O
.	O
let	O
y	O
denote	O
the	O
mixture	O
random	O
variable	B
taking	O
the	O
value	O
y1	O
with	O
probability	O
p	O
and	O
the	O
value	O
y2	O
with	O
probability	O
1-p.	O
let	O
x	O
be	O
a	O
fixed	O
rd-valuedrandom	O
variable	B
,	O
and	O
define	O
1	O
]	O
1	O
(	O
x	O
)	O
=	O
p	O
{	O
y1	O
=	O
llx	O
=	O
x	O
}	O
,	O
1	O
]	O
2	O
(	O
x	O
)	O
=	O
p	O
{	O
y2	O
=	O
11x	O
=	O
x	O
}	O
,	O
where	O
y1	O
,	O
y2	O
are	O
bernoulli	O
random	O
variables	O
.	O
clearly	O
,	O
1	O
]	O
(	O
x	O
)	O
=	O
prjl	O
(	O
x	O
)	O
+	O
(	O
1-	O
p	O
)	O
rj2	O
(	O
x	O
)	O
.	O
which	O
of	O
the	O
error	O
measures	O
l	O
*	O
,	O
p	O
,	O
lnn	O
,	O
e	O
are	O
concave	O
in	O
p	O
for	O
fixed	O
joint	O
distribution	B
of	O
x	O
,	O
y1	O
,	O
y2	O
?	O
can	O
every	O
discrimination	O
problem	O
(	O
x	O
,	O
y	O
)	O
be	O
decomposed	O
this	O
way	O
for	O
some	O
y1	O
,	O
p	O
,	O
y2	O
,	O
where	O
171	O
(	O
x	O
)	O
,	O
rj2	O
(	O
x	O
)	O
e	O
{	O
o	O
,	O
i	O
}	O
for	O
all	O
x	O
?	O
if	O
not	O
,	O
will	O
the	O
condition	O
1	O
]	O
1	O
(	O
x	O
)	O
,	O
rj2	O
(	O
x	O
)	O
e	O
{	O
o	O
,	O
1/2	O
,	O
i	O
}	O
for	O
all	O
x	O
do	O
?	O
problem	O
3.6.	O
show	O
that	O
for	O
every	O
1*	O
e	O
[	O
0	O
,	O
1/2	O
]	O
,	O
there	O
exists	O
a	O
distribution	O
of	O
(	O
x	O
,	O
y	O
)	O
with	O
l	O
*	O
=	O
l*	O
and	O
e	O
=	O
h	O
(	O
l	O
*	O
,	O
1	O
-	O
l	O
*	O
)	O
.	O
thus	O
,	O
fano	O
's	O
inequality	B
is	O
tight	O
.	O
problem	O
3.7.	O
toussaint	O
's	O
inequalities	O
(	O
1974b	O
)	O
.	O
mimic	O
a	O
proof	O
in	O
the	O
text	O
to	O
show	O
that	O
j	O
>	O
1	O
-	O
2lnn	O
log	O
j	O
-	O
(	O
1	O
+	O
,	O
jl	O
-	O
2lnn	O
)	O
,	O
j	O
1	O
-	O
2lnn	O
i	O
-	O
>	O
2	O
(	O
1	O
-	O
2lnn	O
)	O
.	O
-	O
problem	O
3.8.	O
show	O
that	O
l	O
*	O
:	O
s	O
e-oc	O
,	O
where	O
dc	O
is	O
chernoff	O
's	O
measure	B
of	O
affinity	O
with	O
parameter	O
a	O
e	O
(	O
0	O
,	O
1	O
)	O
.	O
problem	O
3.9.	O
prove	O
that	O
l*	O
=	O
p	O
-	O
e	O
{	O
(	O
2rj	O
(	O
x	O
)	O
-	O
1	O
)	O
+	O
}	O
,	O
where	O
p	O
=	O
pry	O
=	O
i	O
}	O
,	O
and	O
(	O
x	O
)	O
+	O
=	O
max	O
(	O
x	O
,	O
0	O
)	O
.	O
problem	O
3.10.	O
show	O
that	O
j	O
:	O
:	O
:	O
-210gp	O
-	O
2h	O
(	O
p	O
,	O
1	O
-	O
p	O
)	O
,	O
where	O
p	O
=	O
pry	O
=	O
i	O
}	O
(	O
toussaint	O
(	O
1974b	O
»	O
.	O
problem	O
3	O
.11.	O
let	O
fl	O
and	O
fo	O
be	O
two	O
multivariate	O
normal	O
densities	O
with	O
means	O
rno	O
,	O
rn	O
1	O
and	O
common	O
covariance	O
matrix	O
l	O
ifp	O
{	O
y	O
=	O
i	O
}	O
=	O
p	O
,	O
and	O
ii	O
,	O
fo	O
are	O
the	O
conditional	O
densities	O
of	O
36	O
3.	O
inequalities	O
and	O
alternate	O
distance	B
measures	O
x	O
given	O
y	O
=	O
1	O
and	O
y	O
=	O
°	O
respectively	O
,	O
then	O
show	O
that	O
p	O
=	O
jp	O
(	O
1-	O
p	O
)	O
e-6.2	O
/8	O
,	O
where	O
p	O
is	O
the	O
matushita	O
error	O
and	O
~	O
is	O
the	O
mahalanobis	O
distance	B
.	O
problem	O
3.12.	O
for	O
every	O
0	O
e	O
[	O
0	O
,	O
(	O
0	O
)	O
and	O
[	O
*	O
e	O
[	O
0	O
,	O
1/2	O
]	O
with	O
[	O
*	O
:	O
:	O
:	O
:	O
2/	O
(	O
4	O
+	O
02	O
)	O
,	O
find	O
distribu	O
(	O
cid:173	O
)	O
tions	O
ilo	O
and	O
ill	O
for	O
x	O
given	O
y	O
=	O
1	O
,	O
y	O
=	O
0	O
,	O
such	O
that	O
the	O
maha1anobis	O
distance	B
tl	O
=	O
0	O
,	O
yet	O
l	O
*	O
=	O
[	O
*	O
.	O
therefore	O
,	O
the	O
maha1anobis	O
distance	B
is	O
not	O
universally	O
related	O
to	O
the	O
bayes	O
risk	O
.	O
problem	O
3.13.	O
show	O
that	O
the	O
mahalanobis	O
distance	B
~	O
is	O
invariant	O
under	O
linear	O
invertible	O
transformations	O
of	O
x.	O
problem	O
3.14.	O
lissack	O
and	O
fu	O
(	O
1976	O
)	O
have	O
suggested	O
the	O
measures	O
for	O
ex	O
=	O
1	O
,	O
this	O
is	O
twice	O
the	O
ko1mogorov	O
distance	B
0ko	O
.	O
show	O
the	O
following	O
:	O
(	O
1	O
)	O
(	O
2	O
)	O
ifo	O
<	O
ex	O
:	O
:	O
:	O
:	O
1	O
,	O
then	O
~	O
(	O
1	O
-	O
olf	O
)	O
:	O
:	O
:	O
:	O
l*	O
:	O
:	O
:	O
:	O
(	O
1	O
-	O
o~~a	O
)	O
.	O
if	O
1	O
:	O
:	O
:	O
:	O
ex	O
<	O
00	O
,	O
then	O
~	O
(	O
1	O
-	O
j~~a	O
)	O
:	O
:	O
:	O
:	O
l	O
*	O
:	O
:	O
:	O
:	O
(	O
1	O
-	O
olf	O
)	O
'	O
problem	O
3.15.	O
hashlamoun	O
,	O
varshney	O
,	O
and	O
samarasooriya	O
(	O
1994	O
)	O
suggest	O
using	O
the	O
f	O
(	O
cid:173	O
)	O
error	O
with	O
the	O
function	O
)	O
-1.8063	O
(	O
x-i	O
)	O
2	O
f	O
(	O
)	O
1	O
.	O
(	O
x	O
=	O
2	O
''	O
sill	O
trx	O
e	O
to	O
obtain	O
tight	O
upper	O
bounds	O
on	O
l	O
*	O
.	O
show	O
that	O
f	O
(	O
x	O
)	O
2	O
:	O
min	O
(	O
x	O
,	O
1	O
-	O
x	O
)	O
,	O
so	O
that	O
the	O
corre	O
(	O
cid:173	O
)	O
sponding	O
f	O
-error	O
is	O
indeed	O
an	O
upper	O
bound	O
on	O
the	O
bayes	O
risk	O
.	O
problem	O
3.16.	O
prove	O
that	O
l*	O
:	O
:	O
:	O
:	O
max	O
(	O
p	O
(	O
1-	O
p	O
»	O
(	O
1	O
-	O
~	O
v	O
(	O
ilo	O
,	O
ild	O
)	O
.	O
problem	O
3.17.	O
prove	O
that	O
l*	O
:	O
:	O
:	O
:	O
jp	O
(	O
1-	O
p	O
)	O
h	O
(	O
ilolll	O
)	O
.	O
hint	O
:	O
min	O
(	O
a	O
,	O
b	O
)	O
:	O
:	O
:	O
:.jqb	O
.	O
problem	O
3.18.	O
assume	O
that	O
the	O
components	O
of	O
x	O
=	O
(	O
x	O
(	O
1	O
)	O
,	O
...	O
,	O
xed	O
)	O
~	O
are	O
conditionally	O
independent	O
(	O
given	O
y	O
)	O
,	O
and	O
identically	O
distributed	O
,	O
that	O
is	O
,	O
p	O
{	O
xu	O
)	O
e	O
aiy	O
=	O
j	O
}	O
=	O
vj	O
(	O
a	O
)	O
for	O
i	O
=	O
1	O
,	O
...	O
,	O
d	O
and	O
j	O
=	O
0	O
,	O
1.	O
use	O
the	O
previous	O
exercise	O
to	O
show	O
that	O
problem	O
3.19.	O
show	O
that	O
x2	O
(	O
ili	O
'	O
il2	O
)	O
2	O
:	O
f	O
(	O
ili	O
,	O
il2	O
)	O
'	O
hint	O
:	O
x-i	O
2	O
:	O
10gx	O
.	O
problem	O
3.20.	O
show	O
the	O
following	O
analog	O
of	O
theorem	O
3.3.	O
let	O
t	O
:	O
n	O
d	O
--	O
+	O
n	O
k	O
be	O
a	O
mea	O
(	O
cid:173	O
)	O
surable	O
function	O
,	O
and	O
il	O
,	O
v	O
probability	O
measures	O
on	O
nd	O
.	O
define	O
the	O
measures	O
ilt	O
and	O
vt	O
on	O
n	O
k	O
by	O
ilt	O
(	O
a	O
)	O
=	O
il	O
(	O
t-\a	O
»	O
and	O
vt	O
(	O
a	O
)	O
=	O
vet-lea	O
»	O
~	O
.	O
show	O
that	O
for	O
any	O
convex	O
function	O
j	O
,	O
dj	O
(	O
il	O
,	O
v	O
)	O
2	O
:	O
dj	O
(	O
ilt	O
,	O
vt	O
)	O
.	O
problem	O
3.21.	O
prove	O
the	O
following	O
connections	O
between	O
the	O
hellinger	O
integral	O
and	O
the	O
total	B
variation	I
:	O
and	O
hint	O
:	O
proceed	O
analogously	O
to	O
lemmas	O
3.1	O
and	O
3.2	O
.	O
(	O
v	O
(	O
il	O
,	O
v	O
»	O
2	O
:	O
:	O
:	O
:	O
8	O
(	O
1	O
-	O
h	O
(	O
il	O
,	O
v	O
»	O
.	O
problems	O
and	O
exercises	O
37	O
problem	O
3.22.	O
pinsker	O
's	O
inequality	B
.	O
show	O
that	O
(	O
v	O
(	O
/1	O
,	O
v	O
)	O
)	O
2	O
.	O
:	O
:	O
:	O
:	O
21	O
(	O
/1	O
,	O
v	O
)	O
(	O
csiszar	O
(	O
1967	O
)	O
,	O
kullback	O
(	O
1967	O
)	O
,	O
and	O
kemperman	O
(	O
1969	O
)	O
)	O
.	O
hint	O
:	O
first	O
prove	O
the	O
inequality	B
if	O
j1	O
and	O
v	O
are	O
concentrated	O
on	O
the	O
same	O
two	O
atoms	O
.	O
then	O
define	O
a	O
=	O
{	O
x	O
:	O
p	O
(	O
x	O
)	O
2	O
:	O
:	O
q	O
(	O
x	O
)	O
}	O
,	O
and	O
v*	O
(	O
1	O
)	O
=	O
the	O
measures	O
/1*	O
,	O
v*	O
on	O
the	O
set	O
{	O
o	O
,	O
i	O
}	O
by	O
/1*	O
(	O
0	O
)	O
=	O
1	O
-	O
v	O
(	O
a	O
)	O
,	O
and	O
apply	O
the	O
previous	O
result	O
.	O
conclude	O
by	O
pointing	O
out	O
that	O
scheffe	O
's	O
theorem	B
states	O
v	O
(	O
j1*	O
,	O
v*	O
)	O
=	O
v	O
(	O
/1	O
,	O
v	O
)	O
,	O
and	O
that	O
1	O
(	O
j1*	O
,	O
v*	O
)	O
.	O
:	O
:	O
:	O
:	O
1	O
(	O
/1	O
,	O
v	O
)	O
.	O
/1*	O
(	O
1	O
)	O
=	O
/1	O
(	O
a	O
)	O
and	O
v*	O
(	O
o	O
)	O
=	O
1	O
4	O
linear	O
discrimination	O
in	O
this	O
chapter	O
,	O
we	O
split	O
the	O
space	O
by	O
a	O
hyperplane	O
and	O
assign	O
a	O
different	O
class	O
to	O
each	O
halfspace	O
.	O
such	O
rules	O
offer	O
tremendous	O
advantages-they	O
are	O
easy	O
to	O
interpret	O
as	O
each	O
decision	O
is	O
based	O
upon	O
the	O
sign	O
of	O
l~=l	O
aix	O
(	O
i	O
)	O
+	O
ao	O
,	O
where	O
x	O
=	O
(	O
x	O
(	O
1	O
)	O
,	O
...	O
,	O
xed	O
)	O
)	O
and	O
the	O
ai	O
's	O
are	O
weights	O
.	O
the	O
weight	O
vector	O
determines	O
the	O
relative	O
importance	O
of	O
the	O
components	O
.	O
the	O
decision	O
is	O
also	O
easily	O
implemented-in	O
a	O
standard	O
software	O
solution	O
,	O
the	O
time	O
of	O
a	O
decision	O
is	O
proportional	O
to	O
d-and	O
the	O
prospect	O
that	O
a	O
small	O
chip	O
can	O
be	O
built	O
to	O
make	O
a	O
virtually	O
instantaneous	O
decision	O
is	O
particularly	O
exciting	O
.	O
rosenblatt	O
(	O
1962	O
)	O
realized	O
the	O
tremendous	O
potential	O
of	O
such	O
linear	O
rules	O
and	O
called	O
themperceptrons	O
.	O
changing	O
one	O
or	O
more	O
weights	O
as	O
new	O
data	O
arrive	O
allows	O
us	O
to	O
quickly	O
and	O
easily	O
adapt	O
the	O
weights	O
to	O
new	O
situations	O
.	O
training	O
or	O
learn	O
(	O
cid:173	O
)	O
ing	O
patterned	O
after	O
the	O
human	O
brain	O
thus	O
became	O
a	O
reality	O
.	O
this	O
chapter	O
merely	O
looks	O
at	O
some	O
theoretical	B
properties	O
of	O
perceptrons	O
.	O
we	O
begin	O
with	O
the	O
simple	O
one	O
(	O
cid:173	O
)	O
dimensional	O
situation	O
,	O
and	O
deal	O
with	O
the	O
choice	O
of	O
weights	O
in	O
nd	O
further	O
on	O
.	O
unless	O
one	O
is	O
terribly	O
lucky	O
,	O
linear	O
discrimination	O
rules	O
can	O
not	O
provide	O
error	O
probabilities	O
close	O
to	O
the	O
bayes	O
risk	O
,	O
but	O
that	O
should	O
not	O
diminish	O
the	O
value	O
of	O
this	O
chapter	O
.	O
linear	O
discrimination	O
is	O
at	O
the	O
heart	O
of	O
nearly	O
every	O
successful	O
pattern	O
recognition	O
method	O
,	O
including	O
tree	B
classifiers	O
(	O
chapters	O
20	O
and	O
21	O
)	O
,	O
generalized	O
linear	O
classi	O
(	O
cid:173	O
)	O
fiers	O
(	O
chapter	O
17	O
)	O
,	O
and	O
neural	O
networks	O
(	O
chapter	O
30	O
)	O
.	O
we	O
also	O
encounter	O
for	O
the	O
first	O
time	O
rules	O
in	O
which	O
the	O
parameters	O
(	O
weights	O
)	O
are	O
dependent	O
upon	O
the	O
data	O
.	O
40	O
4.	O
linear	O
discrimination	O
input	O
•	O
oarl	O
weights	O
figure	O
4.1.	O
rosenblatt	O
's	O
perceptron	B
.	O
the	O
decision	O
is	O
based	O
upon	O
a	O
linear	O
combination	O
of	O
the	O
components	O
of	O
the	O
input	O
vector	O
.	O
4.1	O
univariate	O
discrimination	O
and	O
stoller	O
splits	O
as	O
an	O
introductory	O
example	O
,	O
let	O
x	O
be	O
univariate	O
.	O
the	O
crudest	O
and	O
simplest	O
possible	O
rule	B
is	O
the	O
linear	O
discrimination	O
rule	B
g	O
(	O
x	O
)	O
=	O
{	O
y	O
'	O
1	O
-	O
y	O
'	O
otherwise	O
,	O
if	O
x	O
~	O
x	O
'	O
where	O
x	O
'	O
is	O
a	O
split	O
point	O
and	O
y	O
'	O
e	O
{	O
o	O
,	O
1	O
}	O
is	O
aclass	O
.	O
ingeneral	O
,	O
x	O
'	O
and	O
y	O
'	O
are	O
measurable	O
functions	O
of	O
the	O
data	O
dn	O
.	O
within	O
this	O
class	O
of	O
simple	O
rules	O
,	O
there	O
is	O
of	O
course	O
a	O
best	O
possible	O
rule	B
that	O
can	O
be	O
determined	O
if	O
we	O
know	O
the	O
distribution	B
.	O
assume	O
for	O
example	O
that	O
(	O
x	O
,	O
y	O
)	O
is	O
described	O
in	O
the	O
standard	B
manner	O
:	O
let	O
p	O
{	O
y	O
=	O
1	O
}	O
=	O
p.	O
given	O
y	O
=	O
1	O
,	O
x	O
has	O
a	O
distribution	O
function	O
fl	O
(	O
x	O
)	O
=	O
p	O
{	O
x	O
~	O
xly	O
=	O
1	O
}	O
,	O
and	O
given	O
y	O
=	O
0	O
,	O
x	O
has	O
a	O
distribution	O
function	O
fo	O
(	O
x	O
)	O
=	O
p	O
{	O
x	O
~	O
xly	O
=	O
o	O
}	O
,	O
where	O
fo	O
and	O
fl	O
are	O
the	O
class-conditional	B
distribution	I
functions	O
.	O
then	O
a	O
theoretically	O
optimal	O
rule	B
is	O
determined	O
by	O
the	O
split	O
point	O
x*	O
and	O
class	O
y*	O
given	O
by	O
(	O
x*	O
,	O
y*	O
)	O
=	O
arg	O
min	O
p	O
{	O
g	O
(	O
x	O
)	O
=i	O
y	O
}	O
(	O
x	O
'	O
,	O
y	O
'	O
)	O
(	O
the	O
minimum	O
is	O
always	O
reached	O
if	O
we	O
allow	O
the	O
values	O
x	O
'	O
=	O
00	O
and	O
x	O
'	O
=	O
-	O
(	O
0	O
)	O
.	O
we	O
call	O
the	O
corresponding	O
minimal	O
probability	O
of	O
error	O
l	O
and	O
note	O
that	O
l	O
=	O
inf	O
{	O
i	O
{	O
y'=o	O
}	O
(	O
p	O
fl	O
(	O
x	O
'	O
)	O
+	O
(	O
1	O
-	O
p	O
)	O
(	O
1	O
(	O
x	O
'	O
,	O
y	O
'	O
)	O
fo	O
(	O
x	O
'	O
)	O
)	O
)	O
+	O
iv=l	O
}	O
(	O
p	O
(	O
1	O
-	O
fl	O
(	O
x	O
'	O
)	O
)	O
+	O
(	O
1	O
-	O
p	O
)	O
fo	O
(	O
x	O
'	O
)	O
)	O
}	O
.	O
a	O
split	O
defined	O
by	O
(	O
x*	O
,	O
y*	O
)	O
will	O
be	O
called	O
a	O
theoretical	O
stoller	O
split	O
(	O
stoller	O
(	O
1954	O
)	O
)	O
.	O
4.1	O
univariate	O
discrimination	O
and	O
stoller	O
splits	O
41	O
lemma	O
4.1.	O
l	O
:	O
:	O
:	O
:	O
;	O
1/2	O
with	O
equality	O
if	O
and	O
only	O
if	O
l	O
*	O
=	O
1/2	O
.	O
proof	O
.	O
take	O
(	O
x	O
'	O
,	O
y	O
'	O
)	O
=	O
(	O
-00,0	O
)	O
.	O
then	O
the	O
probability	O
of	O
erroris	O
1-	O
p	O
=	O
p	O
{	O
y	O
=	O
oj	O
.	O
take	O
(	O
x	O
'	O
,	O
y	O
'	O
)	O
=	O
(	O
-00	O
,	O
1	O
)	O
.	O
then	O
the	O
probability	O
of	O
error	O
is	O
p.	O
clearly	O
,	O
l	O
*	O
:	O
:	O
:	O
:	O
;	O
l	O
:	O
:	O
:	O
:	O
;	O
min	O
(	O
p	O
,	O
1	O
-	O
p	O
)	O
.	O
this	O
proves	O
the	O
first	O
part	O
of	O
the	O
lemma	O
.	O
for	O
the	O
second	O
part	O
,	O
if	O
l	O
=	O
1/2	O
,	O
then	O
p	O
=	O
1/2	O
,	O
and	O
for	O
every	O
x	O
,	O
pfl	O
(	O
x	O
)	O
+	O
(	O
1-	O
p	O
)	O
(	O
1	O
-	O
fo	O
(	O
x	O
)	O
)	O
:	O
:	O
:	O
:	O
1/2	O
and	O
p	O
(	O
1	O
fi	O
(	O
x	O
)	O
)	O
+	O
(	O
1-	O
p	O
)	O
fo	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
1/2	O
.	O
the	O
first	O
inequality	B
implies	O
p	O
fi	O
(	O
x	O
)	O
-	O
(	O
1-	O
p	O
)	O
fo	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
p	O
1/2	O
,	O
(	O
1	O
-	O
p	O
)	O
fo	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
;	O
p	O
-	O
1/2	O
.	O
therefore	O
,	O
l	O
=	O
1/2	O
while	O
the	O
second	O
implies	O
p	O
fi	O
(	O
x	O
)	O
-	O
(	O
1	O
-	O
p	O
)	O
fo	O
(	O
x	O
)	O
=	O
p	O
-	O
1/2	O
.	O
thus	O
,	O
for	O
all	O
x	O
,	O
means	O
that	O
for	O
every	O
x	O
,	O
pfi	O
(	O
x	O
)	O
-	O
fi	O
(	O
x	O
)	O
=	O
fo	O
(	O
x	O
)	O
,	O
and	O
therefore	O
l	O
*	O
=	O
1/2	O
.	O
d	O
lemma	O
4.2.	O
l	O
=	O
~	O
-	O
sup	O
ipfi	O
(	O
x	O
)	O
-	O
2	O
x	O
(	O
1	O
-	O
p	O
)	O
fo	O
(	O
x	O
)	O
-	O
p	O
+	O
~i	O
.	O
2	O
in	O
particular	O
,	O
if	O
p	O
=	O
1/2	O
,	O
then	O
1	O
l	O
=	O
-	O
-	O
2	O
1	O
-	O
sup	O
ifi	O
(	O
x	O
)	O
-	O
fo	O
(	O
x	O
)	O
l.	O
2	O
x	O
proof	O
.	O
set	O
p	O
(	O
x	O
)	O
=	O
p	O
fi	O
(	O
x	O
)	O
-	O
(	O
l	O
-	O
p	O
)	O
fo	O
(	O
x	O
)	O
.	O
then	O
,	O
by	O
definition	O
,	O
l	O
=	O
inf	O
min	O
{	O
p	O
(	O
x	O
)	O
+	O
1	O
-	O
p	O
,	O
p	O
-	O
p	O
(	O
x	O
)	O
}	O
x	O
~	O
-	O
sup	O
i	O
p	O
(	O
x	O
)	O
-	O
p	O
+	O
~	O
i	O
2	O
2	O
(	O
since	O
min	O
{	O
a	O
,	O
b	O
}	O
=	O
(	O
a	O
+	O
b	O
-	O
x	O
la	O
-	O
bl	O
)	O
/2	O
)	O
.	O
d	O
the	O
last	O
property	O
relates	O
the	O
quality	O
of	O
theoretical	O
stoller	O
splits	O
to	O
the	O
kol	O
(	O
cid:173	O
)	O
mogorov-smirnov	O
distance	B
supx	O
i	O
fi	O
(	O
x	O
)	O
-	O
fo	O
(	O
x	O
)	O
i	O
between	O
the	O
class-conditional	B
distribution	I
functions	O
.	O
as	O
a	O
fun	O
exercise	O
,	O
consider	O
two	O
classes	O
with	O
means	O
mo	O
=	O
e	O
{	O
xiy	O
=	O
oj	O
,	O
ml	O
=	O
e	O
{	O
xiy	O
=	O
l	O
}	O
,	O
and	O
variances	O
a5	O
=	O
var	O
{	O
xiy	O
=	O
o	O
}	O
and	O
af	O
=	O
var	O
{	O
x	O
i	O
y	O
=	O
i	O
}	O
.	O
then	O
the	O
following	O
inequality	B
holds	O
.	O
theorem	B
4.1.	O
remark	O
.	O
when	O
p	O
=	O
1/2	O
,	O
chernoff	O
(	O
1971	O
)	O
proved	O
1	O
l	O
<	O
-	O
2	O
+	O
2	O
(	O
mo-ml	O
)	O
2	O
(	O
ao+al	O
)	O
2	O
.	O
42	O
4.	O
linear	O
discrimination	O
moreover	O
,	O
becker	O
(	O
1968	O
)	O
pointed	O
out	O
that	O
this	O
is	O
the	O
best	O
possible	O
bound	O
(	O
see	O
problem	O
4.2	O
)	O
.	O
0	O
proof	O
.	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
mo	O
<	O
mi	O
.	O
clearly	O
,	O
l	O
is	O
smaller	O
than	O
the	O
probability	O
of	O
error	O
for	O
the	O
rule	B
that	O
decides	O
0	O
when	O
x	O
:	O
:	O
:	O
mo	O
+	O
.6.0	O
,	O
and	O
1	O
otherwise	O
,	O
where	O
ml	O
-	O
mo	O
=	O
.6.0	O
+	O
.6	O
.	O
1	O
,	O
.6.0	O
,	O
.6	O
.	O
1	O
>	O
o.	O
decide	O
class	O
0	O
decide	O
class	O
1	O
figure	O
4.2.	O
the	O
split	O
providing	O
the	O
bound	O
of	O
theorem	O
4.1	O
.	O
6	O
.	O
1	O
the	O
probability	O
of	O
error	O
of	O
the	O
latter	O
rule	B
is	O
=	O
<	O
pp	O
{	O
x	O
:	O
:	O
:	O
ml	O
-	O
.6	O
.	O
11y	O
=	O
i	O
}	O
+	O
(	O
1	O
-	O
p	O
)	O
p	O
{	O
x	O
>	O
mo	O
+	O
.6.oly	O
=	O
o	O
}	O
0-	O
2	O
(	O
j2	O
p	O
21.6.2	O
+	O
(	O
1-p	O
)	O
20.6.2	O
0	O
0'1	O
+	O
1	O
0'0	O
+	O
(	O
by	O
the	O
chebyshev-cantelli	O
inequality	B
;	O
see	O
appendix	O
,	O
theorem	B
a.17	O
)	O
p	O
1-	O
p	O
-	O
-+	O
-	O
-	O
1	O
+	O
lli	O
1	O
+	O
ll5	O
(	O
jj	O
a	O
}	O
(	O
take	O
.6	O
.	O
1	O
=	O
(	O
ar/o-o	O
)	O
.6.	O
o	O
,	O
and	O
.6.0	O
=	O
im1	O
-	O
molao/	O
(	O
ao	O
+	O
0-1	O
)	O
)	O
1	O
we	O
have	O
yet	O
another	O
example	O
of	O
the	O
principle	O
that	O
well-separated	O
classes	O
yield	O
small	O
values	O
for	O
l	O
and	O
thus	O
l	O
*	O
.	O
separation	O
is	O
now	O
measured	O
in	O
terms	O
of	O
the	O
largeness	O
of	O
1m	O
1	O
-	O
mo	O
i	O
with	O
respect	O
to	O
0-0	O
+	O
0-1.	O
another	O
inequality	B
in	O
the	O
same	O
spirit	O
is	O
given	O
in	O
problem	O
4.1.	O
the	O
limitations	O
of	O
theoretical	O
stoller	O
splits	O
are	O
best	O
shown	O
in	O
a	O
simple	O
example	O
.	O
consider	O
a	O
uniform	O
[	O
0	O
,	O
1	O
]	O
random	O
variable	B
x	O
,	O
and	O
define	O
y=	O
{	O
if	O
0	O
:	O
:	O
:	O
x	O
:	O
:	O
:	O
~	O
+	O
e	O
1	O
0	O
ifi+e	O
<	O
x	O
:	O
:	O
:	O
~-e	O
1	O
if	O
3	O
-e	O
:	O
:	O
:	O
x	O
:	O
:	O
:	O
l	O
for	O
some	O
small	O
e	O
>	O
o.	O
as	O
y	O
is	O
a	O
function	O
of	O
x	O
,	O
we	O
have	O
l	O
*	O
=	O
o.	O
if	O
we	O
are	O
forced	O
to	O
make	O
a	O
trivial	O
x	O
-independent	O
decision	O
,	O
then	O
the	O
best	O
we	O
can	O
do	O
is	O
to	O
set	O
g	O
(	O
x	O
)	O
==	O
1	O
.	O
4.1	O
univariate	O
discrimination	O
and	O
stoller	O
splits	O
43	O
the	O
probability	O
of	O
error	O
is	O
p	O
{	O
1	O
/3	O
+	O
e	O
<	O
x	O
<	O
2/3	O
-	O
e	O
}	O
=	O
1/3	O
-	O
2e	O
.	O
consider	O
next	O
a	O
theoretical	O
stoller	O
split	O
.	O
one	O
sees	O
quickly	O
that	O
the	O
best	O
split	O
occurs	O
at	O
x	O
'	O
=	O
°	O
or	O
x	O
'	O
=	O
1	O
,	O
and	O
thus	O
that	O
l	O
=	O
1/3	O
-	O
2e	O
.	O
in	O
other	O
words	O
,	O
even	O
the	O
best	O
theoretical	B
split	O
is	O
superfluous	O
.	O
note	O
also	O
that	O
in	O
the	O
above	O
example	O
,	O
mo	O
=	O
ml	O
=	O
1/2	O
so	O
that	O
the	O
inequality	B
of	O
theorem	B
4.1	O
says	O
l	O
:	O
:	O
:	O
:	O
:	O
i-it	O
degenerates	O
.	O
we	O
now	O
consider	O
what	O
to	O
do	O
when	O
a	O
split	O
must	O
be	O
data-based	B
.	O
stoller	O
(	O
l954	O
)	O
suggests	O
taking·	O
(	O
x	O
'	O
,	O
y	O
'	O
)	O
such	O
that	O
the	O
empirical	B
error	I
is	O
minimal	O
.	O
he	O
finds	O
(	O
x	O
'	O
,	O
y	O
'	O
)	O
such	O
that	O
,	O
.	O
(	O
x	O
,	O
y	O
)	O
=	O
argrrun	O
,	O
1	O
~	O
(	O
)	O
-	O
l	O
...	O
i	O
{	O
x	O
;	O
sx	O
,	O
ydy	O
}	O
+	O
i	O
{	O
x	O
;	O
>	O
x	O
,	O
ydl-y	O
}	O
.	O
(	O
x	O
,	O
y	O
)	O
erx	O
{	O
o	O
,	O
i	O
}	O
n	O
i=l	O
(	O
x	O
'	O
and	O
y	O
'	O
are	O
now	O
random	O
variables	O
,	O
but	O
in	O
spite	O
of	O
our	O
convention	O
,	O
we	O
keep	O
the	O
lowercase	O
notation	O
for	O
now	O
.	O
)	O
we	O
will	O
call	O
this	O
stoller	O
's	O
rule	B
.	O
the	O
split	O
is	O
referred	O
to	O
as	O
an	O
empirical	B
stoller	O
split	O
.	O
denote	O
the	O
set	O
{	O
(	O
-00	O
,	O
x	O
]	O
x	O
{	O
y	O
}	O
}	O
u	O
{	O
(	O
x	O
,	O
(	O
0	O
)	O
x	O
{	O
i	O
-	O
y	O
}	O
}	O
by	O
c	O
(	O
x	O
,	O
y	O
)	O
.	O
then	O
(	O
x	O
'	O
,	O
y	O
'	O
)	O
=	O
argmin	O
vn	O
(	O
c	O
(	O
x	O
,	O
y	O
)	O
,	O
(	O
x	O
,	O
y	O
)	O
where	O
vn	O
is	O
the	O
empirical	B
measure	I
for	O
the	O
data	O
dn	O
=	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
,	O
that	O
is	O
,	O
for	O
every	O
measurable	O
set	O
a	O
e	O
r	O
x	O
{	O
o	O
,	O
i	O
}	O
,	O
vn	O
(	O
a	O
)	O
=	O
(	O
l/n	O
)	O
'l7=1	O
i	O
{	O
(	O
xi	O
,	O
yi	O
)	O
ea	O
}	O
.	O
denoting	O
the	O
measure	B
of	O
(	O
x	O
,	O
y	O
)	O
in	O
r	O
x	O
{	O
o	O
,	O
i	O
}	O
by	O
v	O
,	O
it	O
is	O
clear	O
that	O
e	O
{	O
vn	O
(	O
c	O
)	O
}	O
=	O
v	O
(	O
c	O
)	O
=	O
p	O
{	O
x	O
:	O
:	O
:	O
:	O
:	O
x	O
,	O
y	O
=i	O
y	O
}	O
+	O
p	O
{	O
x	O
>	O
x	O
,	O
y	O
=11	O
-	O
y	O
}	O
.	O
let	O
ln	O
=	O
p	O
{	O
gn	O
(	O
x	O
)	O
=i	O
yidn	O
}	O
be	O
the	O
error	O
probability	O
of	O
the	O
splitting	O
rule	O
gn	O
with	O
the	O
data-dependent	B
choice	O
(	O
x	O
'	O
,	O
y	O
'	O
)	O
given	O
above	O
,	O
conditioned	O
on	O
the	O
data	O
.	O
then	O
ln	O
v	O
(	O
c	O
(	O
x	O
'	O
,	O
y	O
'	O
)	O
=	O
v	O
(	O
c	O
(	O
x	O
'	O
,	O
y	O
'	O
»	O
)	O
-	O
vn	O
(	O
c	O
(	O
x	O
'	O
,	O
y	O
'	O
)	O
+	O
vn	O
(	O
c	O
(	O
x	O
'	O
,	O
y	O
'	O
)	O
<	O
sup	O
(	O
v	O
(	O
c	O
(	O
x	O
,	O
y	O
»	O
-	O
vn	O
(	O
c	O
(	O
x	O
,	O
y	O
»	O
)	O
+	O
vn	O
(	O
c	O
(	O
x*	O
,	O
y*	O
»	O
(	O
x	O
,	O
y	O
)	O
(	O
where	O
(	O
x*	O
,	O
y*	O
)	O
minimizes	O
v	O
(	O
c	O
(	O
x	O
,	O
y	O
)	O
)	O
over	O
all	O
(	O
x	O
,	O
y	O
)	O
<	O
2	O
sup	O
iv	O
(	O
c	O
(	O
x	O
,	O
y	O
»	O
)	O
-	O
vn	O
(	O
c	O
(	O
x	O
,	O
y	O
)	O
)	O
1	O
+	O
v	O
(	O
c	O
(	O
x*	O
,	O
y*	O
)	O
(	O
x	O
,	O
y	O
)	O
2	O
sup	O
iv	O
(	O
c	O
(	O
x	O
,	O
y	O
»	O
)	O
-	O
vn	O
(	O
c	O
(	O
x	O
,	O
y	O
»	O
)	O
1	O
+	O
l.	O
(	O
x	O
,	O
y	O
)	O
from	O
the	O
next	O
theorem	B
we	O
see	O
that	O
the	O
supremum	O
above	O
is	O
small	O
even	O
for	O
moder	O
(	O
cid:173	O
)	O
ately	O
large	O
n	O
,	O
and	O
therefore	O
,	O
stoller	O
's	O
rule	B
performs	O
closely	O
to	O
the	O
best	O
split	O
regard	O
(	O
cid:173	O
)	O
less	O
of	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
.	O
theorem	B
4.2.	O
for	O
stoller	O
's	O
rule	B
,	O
and	O
e	O
>	O
0	O
,	O
and	O
p	O
{	O
ln	O
-	O
l	O
:	O
:	O
:	O
e	O
}	O
:	O
:	O
:	O
:	O
:	O
4e-ne2/2	O
,	O
e	O
{	O
ln	O
-	O
l	O
}	O
:	O
:	O
:	O
:	O
:	O
2iog	O
(	O
4e	O
)	O
n	O
44	O
4.	O
linear	O
discrimination	O
proof	O
.	O
by	O
the	O
inequality	B
given	O
just	O
above	O
the	O
theorem	B
,	O
(	O
x.y	O
)	O
<	O
p	O
{	O
sup	O
iv	O
(	O
c	O
(	O
x	O
,	O
y	O
»	O
-	O
vn	O
(	O
c	O
(	O
x	O
,	O
y	O
»	O
1	O
:	O
:	O
:	O
~	O
}	O
<	O
p	O
{	O
s~p	O
[	O
v	O
(	O
c	O
(	O
x	O
,	O
0	O
)	O
)	O
-	O
vn	O
(	O
c	O
(	O
x	O
,	O
0	O
»	O
[	O
'	O
''	O
~	O
i	O
+	O
p	O
{	O
s~p	O
[	O
v	O
(	O
c	O
(	O
x	O
,	O
1	O
»	O
-	O
vn	O
(	O
c	O
(	O
x	O
,	O
1	O
»	O
[	O
'	O
''	O
h	O
2	O
<	O
4e-2n	O
(	O
e/2	O
)	O
2	O
by	O
a	O
double	O
application	O
of	O
massart	O
's	O
(	O
1990	O
)	O
tightened	O
version	O
of	O
the	O
dvoretzky	O
(	O
cid:173	O
)	O
kiefer-wolfowitz	O
inequality	B
(	O
1956	O
)	O
(	O
theorem	B
12.9	O
)	O
.	O
see	O
problem	O
4.5.	O
we	O
do	O
not	O
prove	O
this	O
inequality	B
here	O
,	O
but	O
we	O
will	O
thoroughly	O
discuss	O
several	O
such	O
inequalities	O
in	O
chapter	O
12	O
in	O
a	O
greater	O
generality	O
.	O
the	O
second	O
inequality	B
follows	O
from	O
the	O
first	O
via	O
problem	O
12.1	O
.	O
0	O
the	O
probability	O
of	O
error	O
of	O
stoller	O
's	O
rule	B
is	O
uniformly	O
close	O
to	O
l	O
over	O
all	O
possible	O
distributions	O
.	O
this	O
is	O
just	O
a	O
preview	O
of	O
things	O
to	O
come	O
,	O
as	O
we	O
may	O
be	O
able	O
to	O
obtain	O
good	O
performance	O
guarantees	O
within	O
a	O
limited	O
class	O
of	O
rules	O
.	O
4.2	O
linear	O
discriminants	O
rosenblatt	O
's	O
perceptron	B
(	O
rosenblatt	O
(	O
1962	O
)	O
;	O
see	O
nilsson	O
(	O
1965	O
)	O
for	O
a	O
good	O
discus	O
(	O
cid:173	O
)	O
sion	O
)	O
is	O
based	O
upon	O
a	O
dichotomy	O
of	O
rd	O
into	O
two	O
parts	O
by	O
a	O
hyperplane	O
.	O
the	O
linear	O
discrimination	O
rule	B
with	O
weights	O
ao	O
,	O
ai	O
,	O
...	O
,	O
ad	O
is	O
given	O
by	O
figure	O
4.3.	O
a	O
linear	O
discriminant	O
in	O
r2	O
that	O
cor	O
(	O
cid:173	O
)	O
rectly	O
classifies	O
all	O
but	O
four	O
data	O
points	O
.	O
if	O
laix	O
(	O
i	O
)	O
+ao	O
>	O
0	O
i=l	O
otherwise	O
,	O
g	O
(	O
x	O
)	O
=	O
l	O
d	O
where	O
x	O
=	O
(	O
x	O
o	O
)	O
,	O
...	O
,	O
xed	O
)	O
~	O
.	O
•	O
•	O
•	O
•	O
•	O
•	O
.e	O
•	O
•	O
0	O
0	O
0	O
.	O
.	O
.	O
0	O
4.2	O
linear	O
discriminants	O
45	O
its	O
probability	O
of	O
error	O
is	O
for	O
now	O
denoted	O
by	O
l	O
(	O
a	O
,	O
ao	O
)	O
,	O
where	O
a	O
=	O
(	O
ai	O
,	O
...	O
,	O
ad	O
)	O
.	O
again	O
,	O
we	O
set	O
l	O
=	O
inf	O
aerd	O
,	O
aoer	O
l	O
(	O
a	O
,	O
ao	O
)	O
for	O
the	O
best	O
possible	O
probability	O
of	O
error	O
within	O
this	O
class	O
.	O
let	O
the	O
class-conditional	B
distribution	I
functions	O
of	O
a	O
i	O
x	O
(	O
l	O
)	O
+	O
..	O
'+ad	O
xed	O
)	O
be	O
denoted	O
by	O
fo	O
,	O
a	O
and	O
fi	O
,	O
a	O
,	O
depending	O
upon	O
whether	O
y	O
=	O
0	O
or	O
y	O
=	O
1.	O
for	O
l	O
(	O
a	O
,	O
ao	O
)	O
,	O
we	O
may	O
use	O
the	O
bounds	O
of	O
lemma	O
4.2	O
,	O
and	O
apply	O
them	O
to	O
fo	O
,	O
a	O
and	O
fi	O
,	O
a	O
.	O
thus	O
,	O
l	O
=	O
~	O
-	O
s~ps~p	O
ipfi	O
,	O
a	O
(	O
x	O
)	O
-	O
(	O
l	O
-	O
p	O
)	O
fo	O
,	O
a	O
(	O
x	O
)	O
-	O
p	O
+	O
~i	O
,	O
which	O
,	O
for	O
p	O
=	O
1/2	O
,	O
reduces	O
to	O
1	O
2	O
a	O
1	O
2	O
l	O
=	O
-	O
-	O
-	O
sup	O
sup	O
i	O
f1	O
,	O
a	O
(	O
x	O
)	O
fo	O
,	O
a	O
(	O
x	O
)	O
i.	O
x	O
therefore	O
,	O
l	O
=	O
1/2	O
if	O
and	O
only	O
if	O
p	O
=	O
1/2	O
and	O
for	O
all	O
a	O
,	O
fi	O
,	O
a	O
==	O
fo	O
,	O
a	O
.	O
then	O
apply	O
the	O
following	O
simple	O
lemma	O
.	O
lemma	O
4.3	O
.	O
(	O
cramer	O
and	O
wold	O
(	O
1936	O
)	O
)	O
.	O
xl	O
and	O
x	O
2	O
,	O
random	O
variables	O
taking	O
values	O
in	O
r	O
d	O
,	O
are	O
identically	O
distributed	O
if	O
and	O
only	O
if	O
at	O
xl	O
and	O
at	O
x2	O
have	O
the	O
same	O
distribution	B
for	O
all	O
vectors	O
a	O
e	O
rd	O
.	O
proof	O
.	O
two	O
random	O
variables	O
have	O
identical	O
distributions	O
if	O
and	O
only	O
if	O
they	O
have	O
the	O
same	O
characteristic	O
function-see	O
,	O
for	O
example	O
,	O
lukacs	O
and	O
laha	O
(	O
1964	O
)	O
.	O
now	O
,	O
the	O
characteristic	B
function	I
of	O
x	O
i	O
=	O
(	O
x~l	O
)	O
,	O
...	O
,	O
xid	O
»	O
)	O
is	O
=	O
e	O
{	O
ei	O
(	O
aix\l	O
)	O
+	O
..	O
+adx\d	O
)	O
}	O
=	O
e	O
{	O
ei	O
(	O
aix~i	O
)	O
+	O
..	O
+adx~d	O
)	O
}	O
(	O
by	O
assumption	O
)	O
the	O
characteristic	B
function	I
of	O
x	O
2	O
•	O
0	O
thus	O
,	O
we	O
have	O
proved	O
the	O
following	O
:	O
theorem	B
4.3.	O
l	O
:	O
:	O
:	O
;	O
1/2	O
with	O
equality	O
if	O
and	O
only	O
if	O
l	O
*	O
=	O
1/2	O
.	O
thus	O
,	O
as	O
in	O
the	O
one-dimensional	O
case	O
,	O
whenever	O
l	O
*	O
<	O
1/2	O
,	O
a	O
meaningful	O
(	O
l	O
<	O
1/2	O
)	O
cut	O
by	O
a	O
hyperplane	O
is	O
possible	O
.	O
there	O
are	O
also	O
examples	O
in	O
which	O
no	O
cut	O
improves	O
over	O
a	O
rule	O
in	O
which	O
g	O
(	O
x	O
)	O
==	O
y	O
for	O
some	O
y	O
and	O
all	O
x	O
,	O
yet	O
l	O
*	O
=	O
0	O
and	O
l	O
>	O
1/4	O
(	O
say	O
)	O
.	O
to	O
generalize	O
theorem	B
4.1	O
,	O
we	O
offer	O
the	O
following	O
result	O
.	O
a	O
related	O
inequality	B
is	O
shown	O
in	O
problem	O
4.7.	O
the	O
idea	O
of	O
using	O
chebyshev	O
's	O
inequality	B
to	O
obtain	O
such	O
bounds	O
is	O
due	O
to	O
yau	O
and	O
lin	O
(	O
1968	O
)	O
(	O
see	O
also	O
devijver	O
and	O
kittler	O
(	O
1982	O
,	O
p.162	O
)	O
)	O
.	O
46	O
4.	O
linear	O
discrimination	O
theorem	B
4.4.	O
let	O
xo	O
and	O
x	O
i	O
be	O
random	O
variables	O
distributed	O
as	O
x	O
given	O
y	O
=	O
0	O
,	O
and	O
y	O
=	O
1	O
respectively	O
.	O
set	O
mo	O
=	O
e	O
{	O
xo	O
}	O
,	O
ml	O
=	O
e	O
{	O
xd	O
.	O
define	O
also	O
the	O
covariance	O
matrices	O
~l	O
=	O
e	O
{	O
(	O
xl	O
-	O
ml	O
)	O
(	O
x	O
i	O
-	O
mil	O
}	O
and	O
~o	O
=	O
e	O
{	O
(	O
xo	O
-	O
mo	O
)	O
(	O
xo	O
-	O
mol	O
}	O
.	O
then	O
l*	O
<	O
l	O
<	O
inf	O
-	O
-	O
ae'rd	O
1	O
1	O
+	O
(	O
at	O
(	O
ml-mo	O
)	O
i	O
2	O
(	O
at	O
:	O
eoa	O
)	O
1/2+	O
(	O
at	O
:	O
ela	O
)	O
i/2	O
)	O
proof	O
.	O
for	O
any	O
a	O
end	O
we	O
may	O
apply	O
theorem	B
4.1	O
to	O
at	O
xo	O
and	O
at	O
xl	O
.	O
theorem	B
4.4	O
follows	O
by	O
noting	O
that	O
e	O
{	O
at	O
xo	O
}	O
=	O
ate	O
{	O
xo	O
}	O
=	O
at	O
mo	O
,	O
e	O
{	O
atxi	O
}	O
=atml	O
,	O
and	O
that	O
var	O
{	O
at	O
xo	O
}	O
=	O
e	O
{	O
at	O
(	O
xo	O
-	O
mo	O
)	O
(	O
xo	O
-	O
mo	O
)	O
t	O
a	O
}	O
=	O
at~oa	O
,	O
var	O
{	O
atxi	O
}	O
=at~la	O
.	O
0	O
we	O
may	O
obtain	O
explicit	O
inequalities	O
by	O
different	O
choices	O
of	O
a.	O
a	O
=	O
ml	O
-	O
mo	O
yields	O
a	O
convenient	O
formula	O
.	O
we	O
see	O
from	O
the	O
next	O
section	O
that	O
a	O
=	O
~	O
(	O
ml	O
-	O
mo	O
)	O
with	O
:	O
e	O
=	O
p	O
:	O
e1	O
+	O
(	O
1	O
-	O
p	O
)	O
~o	O
is	O
also	O
a	O
meaningful	O
choice	O
(	O
see	O
also	O
problem	O
4.7	O
)	O
.	O
4.3	O
the	O
fisher	O
linear	B
discriminant	I
data-based	O
values	O
for	O
a	O
may	O
be	O
found	O
by	O
various	O
criteria	O
.	O
one	O
of	O
the	O
first	O
methods	O
was	O
suggested	O
by	O
fisher	O
(	O
1936	O
)	O
.	O
let	O
ml	O
and	O
mo	O
be	O
the	O
sample	O
means	O
for	O
the	O
two	O
classes	O
(	O
e.g.	O
,	O
ml	O
=	O
li	O
:	O
yi=1	O
xdl	O
{	O
i	O
:	O
yi	O
=	O
1	O
}	O
1-	O
)	O
picture	O
projecting	O
xl	O
,	O
...	O
,	O
xn	O
to	O
a	O
line	O
in	O
the	O
direction	O
of	O
a.	O
note	O
that	O
this	O
is	O
perpendicular	O
to	O
the	O
hyperplane	B
given	O
by	O
a	O
t	O
x	O
+	O
ao	O
=	O
o.	O
the	O
projected	O
values	O
are	O
a	O
t	O
xl	O
,	O
...	O
,	O
a	O
t	O
x	O
n.	O
these	O
are	O
all	O
equal	O
to	O
o	O
for	O
those	O
xi	O
on	O
the	O
hyperplane	B
at	O
x	O
=	O
0	O
through	O
the	O
origin	O
,	O
and	O
grow	O
in	O
absolute	O
value	O
as	O
we	O
flee	O
that	O
hyperplane	B
.	O
let	O
~2	O
and	O
85	O
be	O
the	O
sample	O
scatters	O
for	O
classes	O
1	O
and	O
0	O
,	O
respectively	O
,	O
that	O
is	O
,	O
--	O
-2	O
``	O
(	O
tx	O
0'1	O
=	O
~	O
a	O
t	O
--	O
-	O
)	O
2	O
i-a	O
ml	O
=	O
asia	O
t	O
and	O
similarly	O
for	O
ag	O
,	O
where	O
i	O
:	O
yi=l	O
sl	O
=	O
l	O
(	O
xi	O
-	O
md	O
(	O
xi	O
-	O
mdt	O
i	O
:	O
yi=1	O
is	O
the	O
scatter	B
matrix	I
for	O
class	O
1.	O
the	O
fisher	O
linear	B
discriminant	I
is	O
that	O
linear	O
function	O
a	O
t	O
x	O
for	O
which	O
the	O
criterion	O
4.4	O
the	O
normal	B
distribution	I
47	O
j	O
(	O
a	O
)	O
=	O
(	O
t	O
--	O
a	O
ml	O
-	O
a	O
mo	O
t	O
--	O
)	O
2	O
--	O
2	O
--	O
2	O
(	O
)	O
l	O
+	O
(	O
70	O
__	O
)	O
2	O
(	O
t	O
(	O
--	O
a	O
ml	O
-	O
mo	O
)	O
.	O
at	O
(	O
sl	O
+	O
so	O
)	O
a	O
is	O
maximum	O
.	O
this	O
corresponds	O
to	O
finding	O
a	O
direction	O
a	O
that	O
best	O
separates	O
at	O
rn	O
1	O
from	O
at	O
rno	O
relative	O
to	O
the	O
sample	B
scatter	I
.	O
luckily	O
,	O
to	O
find	O
that	O
a	O
,	O
we	O
need	O
not	O
resort	O
to	O
numerical	O
iteration-the	O
solution	O
is	O
given	O
by	O
fisher	O
's	O
suggestion	O
is	O
to	O
replace	O
(	O
xl	O
,	O
y1	O
)	O
,	O
.••	O
,	O
(	O
xn	O
,	O
yn	O
)	O
by	O
(	O
at	O
xl	O
,	O
y1	O
)	O
,	O
.••	O
,	O
(	O
at	O
x	O
n	O
,	O
yn	O
)	O
and	O
to	O
perform	O
one-dimensional	O
discrimination	O
.	O
usually	O
,	O
the	O
rule	B
uses	O
a	O
simple	O
split	O
(	O
4.1	O
)	O
gao	O
(	O
x	O
)	O
=	O
{	O
if	O
at	O
x	O
+	O
ao	O
>	O
0	O
i	O
0	O
otherwise	O
for	O
some	O
constant	O
ao	O
.	O
unfortunately	O
,	O
fisher	O
discriminants	O
can	O
be	O
arbitrarily	O
bad	O
:	O
there	O
are	O
distributions	O
such	O
that	O
even	O
though	O
the	O
two	O
classes	O
are	O
linearly	O
separable	O
(	O
i.e.	O
,	O
l	O
=	O
0	O
)	O
,	O
the	O
fisher	O
linear	B
discriminant	I
has	O
an	O
error	O
probability	O
close	O
to	O
1	O
(	O
see	O
problem	O
4.9	O
)	O
.	O
4.4	O
the	O
normal	B
distribution	I
there	O
are	O
a	O
few	O
situations	O
in	O
which	O
,	O
by	O
sheer	O
accident	O
,	O
the	O
bayes	O
rule	B
is	O
a	O
lin	O
(	O
cid:173	O
)	O
ear	O
discriminant	O
.	O
while	O
this	O
is	O
not	O
a	O
major	O
issue	O
,	O
it	O
is	O
interesting	O
to	O
identify	O
the	O
most	O
important	O
case	O
,	O
i.e.	O
,	O
that	O
of	O
the	O
multivariate	B
normal	I
distribution	I
.	O
the	O
general	O
multivariate	O
normal	O
density	O
is	O
written	O
as	O
where	O
m	O
is	O
the	O
mean	O
(	O
both	O
x	O
and	O
m	O
are	O
d-component	O
column	O
vectors	O
)	O
,	O
l	O
;	O
is	O
the	O
d	O
x	O
d	O
covariance	O
matrix	O
,	O
l	O
;	O
-l	O
is	O
the	O
inverse	O
of	O
l	O
;	O
,	O
and	O
det	O
(	O
l	O
;	O
)	O
is	O
its	O
determinant.	O
'	O
''	O
n	O
(	O
m	O
,	O
l	O
;	O
)	O
.	O
clearly	O
,	O
if	O
x	O
has	O
density	O
j	O
,	O
then	O
m	O
=	O
ex	O
and	O
l	O
;	O
=	O
we	O
write	O
j	O
e	O
{	O
(	O
x	O
-	O
m	O
)	O
(	O
x	O
-	O
ml	O
}	O
.	O
the	O
multivariate	O
normal	O
density	O
is	O
completely	O
specified	O
by	O
d	O
+	O
e	O
)	O
formal	O
pa	O
(	O
cid:173	O
)	O
rameters	O
(	O
m	O
and	O
l	O
;	O
)	O
.	O
a	O
sample	O
from	O
the	O
density	O
is	O
clustered	O
in	O
an	O
elliptical	O
cloud	O
.	O
the	O
loci	O
of	O
points	O
of	O
constant	O
density	O
are	O
ellipsoids	O
described	O
by	O
for	O
some	O
constant	O
r	O
~	O
o.	O
the	O
number	O
''	O
r	O
is	O
the	O
mahalanobis	O
distance	B
from	O
x	O
to	O
m	O
,	O
and	O
is	O
in	O
fact	O
useful	O
even	O
when	O
the	O
underlying	O
distribution	B
is	O
not	O
normal	B
.	O
it	O
takes	O
into	O
account	O
the	O
directional	O
stretch	O
of	O
the	O
space	O
determined	O
by	O
l	O
;	O
.	O
48	O
4.	O
linear	O
discrimination	O
figure	O
4.4.	O
points	O
at	O
equal	O
mahalanobis	O
distance	B
from	O
m.	O
given	O
a	O
two-class	O
problem	O
in	O
which	O
x	O
has	O
a	O
density	O
(	O
1	O
-	O
p	O
)	O
!	O
o	O
(	O
x	O
)	O
+	O
p	O
!	O
i	O
(	O
x	O
)	O
and	O
!	O
o	O
and	O
!	O
i	O
are	O
both	O
multivariate	O
normal	O
with	O
parameters	O
mi	O
,	O
l	O
:	O
i	O
,	O
i	O
=	O
0	O
,	O
1	O
,	O
the	O
bayes	O
rule	B
is	O
easily	O
described	O
by	O
g*	O
(	O
x	O
)	O
=	O
{	O
01	O
if	O
p	O
!	O
l	O
(	O
x	O
)	O
>	O
(	O
1	O
-	O
p	O
)	O
!	O
o	O
(	O
x	O
)	O
otherwise	O
.	O
take	O
logarithms	O
and	O
note	O
that	O
g*	O
(	O
x	O
)	O
=	O
1	O
if	O
and	O
only	O
if	O
(	O
x	O
-	O
mdtl:11	O
(	O
x	O
-	O
ml	O
)	O
-	O
2logp	O
+log	O
(	O
det	O
(	O
l	O
:	O
l	O
»	O
<	O
(	O
x	O
-	O
mo	O
)	O
tl	O
:	O
ol	O
(	O
x	O
-	O
mo	O
)	O
-	O
2log	O
(	O
1	O
-	O
p	O
)	O
+	O
log	O
(	O
det	O
(	O
l	O
:	O
o	O
»	O
.	O
in	O
practice	O
,	O
one	O
might	O
wish	O
to	O
estimate	B
m	O
1	O
,	O
mo	O
,	O
l	O
:	O
1	O
,	O
l	O
:	O
o	O
and	O
p	O
from	O
the	O
data	O
and	O
use	O
these	O
estimates	O
in	O
the	O
formula	O
for	O
g*	O
.	O
interestingly	O
,	O
as	O
(	O
x	O
mi	O
f	O
l	O
:	O
i	O
1	O
(	O
x	O
-	O
mi	O
)	O
is	O
the	O
squared	O
mahalanobis	O
distance	B
from	O
x	O
to	O
mi	O
in	O
class	O
i	O
(	O
called	O
rl	O
)	O
,	O
the	O
bayes	O
rule	B
is	O
simply	O
*	O
(	O
x	O
)	O
=	O
{	O
1	O
g	O
0	O
otherwise	O
.	O
ifrl	O
<	O
.r5	O
-	O
2log	O
(	O
(	O
1-	O
p	O
)	O
/p	O
)	O
+log	O
(	O
det	O
(	O
l	O
:	O
o	O
)	O
/det	O
(	O
l	O
:	O
d	O
)	O
in	O
particular	O
,	O
when	O
p	O
=	O
1/2	O
,	O
l	O
:	O
o	O
=	O
l	O
:	O
l	O
=	O
l	O
:	O
,	O
we	O
have	O
*	O
x	O
=	O
{	O
g	O
(	O
)	O
·f	O
2	O
2	O
1	O
1	O
r	O
1	O
<	O
ro	O
0	O
otherwise	O
;	O
just	O
classify	O
according	O
to	O
the	O
class	O
whose	O
mean	O
is	O
at	O
the	O
nearest	O
mahalanobis	O
distance	B
from	O
x.	O
when	O
l	O
:	O
o	O
=	O
l	O
:	O
1	O
=	O
l	O
:	O
,	O
the	O
bayes	O
rule	B
becomes	O
linear	O
:	O
*	O
(	O
x	O
)	O
=	O
{	O
i	O
if	O
a	O
t	O
x	O
:	O
-	O
ao	O
>	O
0	O
0	O
otherwise	O
,	O
g	O
where	O
a	O
=	O
(	O
ml	O
-	O
mo	O
)	O
l	O
:	O
-l	O
,	O
and	O
ao	O
=	O
2log	O
(	O
p	O
/	O
(	O
1	O
-	O
p	O
»	O
+	O
mt	O
;	O
l	O
:	O
-lmo	O
mfl	O
:	O
-lml	O
.	O
thus	O
,	O
linear	O
discrimination	O
rules	O
occur	O
as	O
special	O
cases	O
of	O
bayes	O
rules	O
for	O
multi	O
(	O
cid:173	O
)	O
variate	O
normal	B
distributions	O
.	O
our	O
intuition	O
that	O
a	O
should	O
be	O
in	O
the	O
direction	O
m	O
1	O
-	O
mo	O
to	O
best	O
separate	O
the	O
classes	O
is	O
almost	O
right	O
.	O
note	O
nevertheless	O
that	O
a	O
is	O
not	O
perpendicular	O
in	O
general	O
to	O
the	O
hyperplane	B
of	O
loci	O
at	O
equal	O
distance	B
from	O
mo	O
and	O
mi	O
.	O
when	O
l	O
:	O
is	O
replaced	O
by	O
4.5	O
empirical	B
risk	I
minimization	I
49	O
the	O
standard	B
data-based	O
estimate	B
,	O
we	O
obtain	O
in	O
fact	O
the	O
fisher	O
linear	B
discriminant	I
.	O
furthermore	O
,	O
when	O
~l	O
i	O
~o	O
,	O
the	O
decision	O
boundary	O
is	O
usually	O
not	O
linear	O
,	O
and	O
fisher	O
's	O
linear	B
discriminant	I
must	O
therefore	O
be	O
suboptimal	O
.	O
in	O
early	O
.	O
statistical	O
work	O
on	O
discrimination	O
,	O
the	O
normal	B
historical	O
remarks	O
.	O
distribution	B
plays	O
a	O
central	O
role	O
(	O
anderson	O
(	O
1958	O
»	O
.	O
for	O
a	O
simple	O
introduction	O
,	O
we	O
refer	O
to	O
duda	O
and	O
hart	O
(	O
1973	O
)	O
.	O
mclachlan	O
(	O
1992	O
)	O
has	O
more	O
details	O
,	O
and	O
raudys	O
(	O
1972	O
;	O
1976	O
)	O
relates	O
the	O
error	O
,	O
dimensionality	O
,	O
and	O
sample	O
size	O
for	O
normal	B
and	O
nearly	O
normal	B
models	O
.	O
see	O
also	O
raudys	O
and	O
pikelis	O
(	O
1980	O
;	O
1982	O
)	O
.	O
0	O
4.5	O
empirical	B
risk	I
minimization	I
in	O
this	O
section	O
we	O
present	O
an	O
algorithm	B
that	O
yields	O
a	O
classifier	O
whose	O
error	O
probabil	O
(	O
cid:173	O
)	O
ity	O
is	O
very	O
close	O
to	O
the	O
minimal	O
error	O
probability	O
l	O
achievable	O
by	O
linear	O
classifiers	O
,	O
provided	O
that	O
x	O
has	O
a	O
density	O
.	O
the	O
algorithm	B
selects	O
a	O
classifier	O
by	O
minimizing	O
the	O
empirical	B
error	I
over	O
finitely	O
many-2c	O
)	O
-linear	O
classifiers	O
.	O
for	O
a	O
rule	O
<	O
/	O
>	O
(	O
x	O
)	O
=	O
{	O
~	O
if	O
at	O
x	O
+	O
ao	O
>	O
0	O
otherwise	O
,	O
the	O
probability	O
of	O
error	O
is	O
l	O
(	O
¢	O
)	O
=	O
p	O
{	O
¢	O
(	O
x	O
)	O
i	O
y	O
}	O
.	O
l	O
(	O
¢	O
)	O
may	O
be	O
estimated	O
by	O
the	O
empirical	B
risk	I
that	O
is	O
,	O
the	O
number	O
of	O
errors	O
made	O
by	O
the	O
classifier	B
¢	O
is	O
counted	O
and	O
normalized	O
.	O
assume	O
that	O
x	O
has	O
a	O
density	O
,	O
and	O
consider	O
d	O
arbitrary	O
data	O
points	O
xii	O
'	O
x	O
i2	O
,	O
...	O
,	O
xid	O
among	O
{	O
xl	O
,	O
''	O
''	O
xn	O
}	O
,	O
and	O
let	O
at	O
x	O
+	O
ao	O
=	O
0	O
be	O
a	O
hyperplane	O
containing	O
these	O
points	O
.	O
because	O
of	O
the	O
density	O
assumption	O
,	O
the	O
d	O
points	O
are	O
in	O
general	O
position	O
with	O
probability	O
one	O
,	O
and	O
this	O
hyperplane	B
is	O
unique	O
.	O
this	O
hyperplane	B
determines	O
two	O
classifiers	O
:	O
if	O
at	O
x	O
+	O
ao	O
>	O
0	O
0	O
otherwise	O
,	O
and	O
<	O
/	O
>	O
1	O
(	O
x	O
)	O
=	O
{	O
1	O
<	O
/	O
>	O
,	O
(	O
x	O
)	O
=	O
i	O
1	O
if	O
at	O
x	O
+	O
ao	O
<	O
0	O
0	O
otherwise	O
,	O
whose	O
empirical	B
errors	O
ln	O
(	O
¢l	O
)	O
and	O
l	O
n	O
(	O
¢2	O
)	O
may	O
be	O
calculated	O
.	O
to	O
each	O
d-tuple	O
xii	O
'	O
xi2	O
,	O
...	O
,	O
xid	O
of	O
data	O
points	O
,	O
we	O
may	O
assign	O
two	O
classifiers	O
in	O
this	O
manner	O
,	O
yielding	O
altogether	O
2c	O
)	O
classifiers	O
.	O
denote	O
these	O
classifiers	O
by	O
¢l	O
,	O
...	O
,	O
¢2	O
(	O
1l	O
)	O
.	O
let	O
~	O
¢	O
be	O
a	O
linear	O
classifier	B
that	O
minimizes	O
ln	O
(	O
¢i	O
)	O
over	O
all	O
i	O
=	O
1	O
,	O
...	O
,	O
2	O
(	O
~	O
)	O
.	O
~	O
d	O
50	O
4.	O
linear	O
discrimination	O
we	O
denote	O
the	O
best	O
possible	O
error	O
probability	O
by	O
l	O
=	O
inf	O
l	O
(	O
fjj	O
)	O
¢	O
over	O
the	O
class	O
of	O
all	O
linear	O
rules	O
,	O
and	O
define	O
fjj*	O
=	O
arg	O
min¢	O
l	O
(	O
fjj	O
)	O
as	O
the	O
best	O
linear	O
rule	O
.	O
if	O
there	O
are	O
several	O
classifiers	O
with	O
l	O
(	O
fjj	O
)	O
=	O
l	O
,	O
then	O
we	O
choose	O
fjj*	O
among	O
these	O
in	O
an	O
arbitrary	O
fixed	O
manner	O
.	O
next	O
we	O
show	O
that	O
the	O
classifier	B
corresponding	O
to	O
1	O
>	O
is	O
really	O
very	O
good	O
.	O
•	O
0	O
•	O
0	O
•	O
0	O
0	O
•	O
•	O
•	O
•	O
•	O
•	O
•	O
figure	O
4.5.	O
if	O
the	O
data	O
points	O
are	O
in	O
general	O
position	O
,	O
thenfor	O
each	O
linear	O
rule	O
there	O
exists	O
a	O
linear	O
split	O
defined	O
by	O
a	O
hyperplane	O
crossing	O
d	O
points	O
such	O
that	O
the	O
difference	O
between	O
the	O
empirical	B
errors	O
is	O
at	O
most	O
din	O
.	O
first	O
note	O
that	O
there	O
is	O
no	O
linear	B
classifier	I
fjj	O
whose	O
empirical	B
error	I
[	O
;	O
n	O
(	O
fjj	O
)	O
is	O
smaller	O
than	O
[	O
;	O
(	O
1	O
)	O
)	O
-	O
din	O
.	O
this	O
follows	O
from	O
the	O
fact	O
that	O
since	O
the	O
data	O
points	O
are	O
in	O
general	O
position	O
(	O
recall	O
the	O
density	O
assumption	O
)	O
,	O
then	O
for	O
each	O
linear	B
classifier	I
we	O
may	O
find	O
one	O
whose	O
defining	O
hyperplane	B
contains	O
exactly	O
d	O
data	O
points	O
such	O
that	O
the	O
two	O
decisions	O
agree	O
on	O
all	O
data	O
points	O
except	O
possibly	O
for	O
these	O
d	O
points	O
(	O
cid:173	O
)	O
see	O
figure	O
4.5.	O
thus	O
,	O
we	O
may	O
view	O
minimization	O
of	O
the	O
empirical	B
error	I
over	O
the	O
finite	O
set	O
{	O
fjjl	O
,	O
...	O
,	O
fjj2c	O
)	O
}	O
as	O
approximate	O
minimization	O
over	O
the	O
infinite	O
set	O
of	O
linear	O
classifiers	O
.	O
in	O
chapters	O
12	O
and	O
13	O
we	O
will	O
develop	O
the	O
full	O
theory	O
for	O
rules	O
that	O
are	O
found	O
by	O
empirical	B
risk	I
minimization	I
.	O
theorem	B
4.5	O
just	O
gives	O
you	O
a	O
taste	O
of	O
things	O
to	O
come	O
.	O
other-more	O
involved	O
,	O
but	O
also	O
more	O
general-proofs	O
go	O
back	O
to	O
vapnik	O
and	O
chervonenkis	O
(	O
1971	O
;	O
197	O
4c	O
)	O
.	O
theorem	B
4.5.	O
assume	O
that	O
x	O
has	O
a	O
density	O
.	O
ij1	O
>	O
isfound	O
by	O
empirical	B
error	I
mini	O
(	O
cid:173	O
)	O
mization	O
as	O
described	O
above	O
,	O
then	O
,	O
jor	O
all	O
possible	O
distributions	O
oj	O
(	O
x	O
,	O
y	O
)	O
,	O
ifn	O
~	O
d	O
and	O
2dln	O
~	O
e	O
~	O
1	O
,	O
we	O
have	O
moreover	O
,	O
ifn	O
~	O
d	O
,	O
then	O
e	O
{	O
l	O
(	O
¢	O
)	O
-	O
l	O
}	O
:	O
:	O
:	O
2	O
-	O
(	O
cd	O
+	O
1	O
)	O
log	O
n	O
+	O
(	O
2d	O
+	O
2	O
»	O
.	O
n	O
remark	O
.	O
with	O
some	O
care	O
theorem	B
4.5	O
and	O
theorem	B
4.6	O
below	O
can	O
be	O
extended	O
so	O
that	O
the	O
density	O
assumption	O
may	O
be	O
dropped	O
.	O
one	O
needs	O
to	O
ensure	O
that	O
the	O
selected	O
4.5	O
empirical	B
risk	I
minimization	I
51	O
linear	O
rule	O
has	O
empirical	B
error	I
close	O
to	O
that	O
of	O
the	O
best	O
possible	O
linear	O
rule	O
.	O
with	O
the	O
classifier	B
suggested	O
above	O
this	O
property	O
may	O
fail	O
to	O
hold	O
if	O
the	O
data	O
points	O
are	O
not	O
necessarily	O
of	O
general	O
position	O
.	O
the	O
ideas	O
presented	O
here	O
are	O
generalized	B
in	O
chapter	O
12	O
(	O
see	O
theorem	B
12.2	O
)	O
.	O
d	O
proof	O
.	O
we	O
begin	O
with	O
the	O
following	O
simple	O
inequality	O
:	O
l	O
(	O
j	O
;	O
)	O
-	O
l	O
=	O
l	O
(	O
j	O
;	O
)	O
ln	O
(	O
¢	O
)	O
+	O
ln	O
(	O
$	O
)	O
-	O
l	O
(	O
¢*	O
)	O
(	O
since	O
ln	O
(	O
¢	O
)	O
:	O
:	O
:	O
ln	O
(	O
¢	O
)	O
+	O
din	O
for	O
any	O
¢	O
)	O
<	O
max	O
i=1	O
,	O
...	O
,2g	O
)	O
(	O
l	O
(	O
¢i	O
)	O
ln	O
(	O
¢j	O
)	O
+	O
ln	O
(	O
¢*	O
)	O
-	O
l	O
(	O
¢*	O
)	O
+	O
-	O
.	O
--	O
-	O
--	O
-	O
d	O
n	O
therefore	O
,	O
by	O
the	O
union-of-events	O
bound	O
,	O
we	O
have	O
p	O
{	O
l	O
(	O
¢	O
)	O
-	O
l	O
>	O
e	O
}	O
:	O
:	O
:	O
lp	O
l	O
(	O
¢i	O
)	O
-ln	O
(	O
¢j	O
>	O
-	O
+p	O
ln	O
(	O
¢*	O
)	O
-l	O
(	O
¢*	O
)	O
+-	O
>	O
-	O
de	O
}	O
.	O
n	O
2	O
e	O
}	O
2	O
{	O
i=l	O
2g	O
)	O
{	O
to	O
bound	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
,	O
observe	O
that	O
nln	O
(	O
¢*	O
)	O
is	O
bi	O
(	O
cid:173	O
)	O
nomially	O
distributed	O
with	O
parameters	O
nand	O
l	O
(	O
¢*	O
)	O
.	O
by	O
an	O
inequality	B
due	O
to	O
chernoff	O
(	O
1952	O
)	O
and	O
okamoto	O
(	O
1958	O
)	O
for	O
the	O
tail	O
of	O
the	O
binomial	B
distribution	I
,	O
--	O
-	O
p	O
ln	O
(	O
¢*	O
)	O
-	O
l	O
(	O
¢*	O
)	O
>	O
2	O
:	O
e	O
{	O
we	O
prove	O
this	O
inequality	B
later	O
(	O
see	O
theorem	B
8.1	O
)	O
.	O
next	O
we	O
bound	O
one	O
term	O
of	O
the	O
sum	O
on	O
the	O
right-hand	O
side	O
.	O
note	O
that	O
by	O
symmetry	O
all	O
2	O
(	O
~	O
)	O
terms	O
are	O
equal	O
.	O
assume	O
that	O
the	O
classifier	B
¢1	O
is	O
determined	O
by	O
the	O
d-tuple	O
of	O
the	O
first	O
d	O
data	O
points	O
xl	O
,	O
.	O
'	O
''	O
xd	O
.	O
we	O
write	O
p	O
{	O
l	O
(	O
¢l	O
)	O
l	O
n	O
(	O
¢l	O
)	O
>	O
~	O
}	O
=	O
e	O
{	O
p	O
{	O
l	O
(	O
¢l	O
)	O
ln	O
(	O
¢d	O
>	O
~	O
i	O
xl	O
,	O
...	O
,	O
xd	O
}	O
}	O
,	O
and	O
bound	O
the	O
conditional	O
probability	O
inside	O
.	O
let	O
(	O
x~	O
,	O
y	O
{	O
'	O
)	O
,	O
...	O
,	O
(	O
xj	O
,	O
yj	O
)	O
be	O
inde-	O
pendent	O
of	O
the	O
data	O
and	O
be	O
distributed	O
as	O
the	O
data	O
(	O
xl	O
,	O
yd	O
,	O
...	O
,	O
(	O
xd	O
,	O
yd	O
)	O
.	O
define	O
(	O
x~	O
y	O
!	O
)	O
=	O
{	O
(	O
x	O
;	O
'	O
,	O
y/	O
,	O
)	O
(	O
xi	O
,	O
yi	O
)	O
l	O
'	O
i	O
if	O
i	O
:	O
:	O
:	O
d	O
ifi	O
>	O
d.	O
then	O
p	O
{	O
l	O
(	O
¢l	O
)	O
-	O
l	O
n	O
(	O
¢l	O
)	O
>	O
~	O
i	O
xl	O
,	O
...	O
,	O
xd	O
}	O
:	O
:	O
:	O
p	O
{	O
l	O
(	O
¢d	O
-	O
~	O
t	O
i	O
{	O
¢l	O
(	O
xi	O
)	O
=/yd	O
>	O
:	O
.1	O
xl	O
,	O
...	O
,	O
xd	O
}	O
n	O
i=d+l	O
2	O
52	O
4.	O
linear	O
discrimination	O
<	O
p	O
l	O
(	O
¢l	O
)	O
-	O
{	O
=	O
p	O
l	O
(	O
¢l	O
)	O
-	O
{	O
-	O
l	O
i	O
{	O
(	O
/ji	O
(	O
x	O
'	O
)	O
=jyi	O
}	O
+	O
-	O
>	O
-	O
xl	O
,	O
·	O
..	O
,	O
xd	O
}	O
1	O
n	O
n	O
i=l	O
ei	O
2	O
d	O
n	O
i	O
i	O
;	O
:	O
;	O
binomlal	O
(	O
n	O
,	O
l	O
(	O
¢l	O
)	O
)	O
>	O
'2	O
-	O
;	O
:	O
;	O
x	O
i	O
,	O
...	O
,	O
xd	O
1	O
.	O
e	O
dj	O
}	O
(	O
as	O
l	O
(	O
¢l	O
)	O
depends	O
upon	O
xl	O
,	O
...	O
,	O
xd	O
only	O
and	O
(	O
x~	O
,	O
y	O
{	O
)	O
.	O
..	O
,	O
(	O
x~	O
,	O
y~	O
)	O
are	O
independent	O
of	O
xl	O
,	O
...	O
,	O
xd	O
)	O
_2n	O
(	O
``	O
-_	O
!	O
l	O
)	O
2	O
e	O
2	O
ii	O
<	O
(	O
by	O
theorem	B
8.1	O
;	O
use	O
the	O
fact	O
that	O
e	O
2	O
:	O
2d/n	O
)	O
the	O
inequality	B
for	O
the	O
expected	O
value	O
follows	O
from	O
the	O
probability	O
inequality	B
by	O
the	O
following	O
simple	O
argument	O
:	O
by	O
the	O
cauchy-schwarz	O
inequality	B
,	O
(	O
e	O
{	O
l	O
(	O
¢	O
)	O
-	O
l	O
}	O
)	O
2	O
:	O
:	O
:	O
e	O
{	O
(	O
l	O
(	O
¢	O
)	O
_	O
l	O
)	O
2	O
}	O
.	O
denoting	O
z	O
=	O
(	O
l	O
(	O
¢	O
)	O
-	O
l	O
)	O
2	O
,	O
for	O
any	O
u	O
>	O
0	O
,	O
e	O
{	O
z	O
}	O
e	O
{	O
ziz	O
>	O
u	O
}	O
p	O
{	O
z	O
>	O
u	O
}	O
+e	O
{	O
ziz	O
:	O
:	O
:	O
u	O
}	O
p	O
{	O
z	O
:	O
:	O
:	O
u	O
}	O
<	O
p	O
{	O
z	O
>	O
u	O
}	O
+	O
u	O
<	O
e2d	O
(	O
2nd	O
+	O
1	O
)	O
e-nu	O
/2	O
+	O
u	O
if	O
u	O
2	O
:	O
(	O
2d	O
)	O
2	O
-	O
;	O
;	O
by	O
the	O
probability	O
inequality	B
,	O
and	O
since	O
g	O
)	O
:	O
:	O
:	O
n	O
d	O
.	O
choosing	O
u	O
to	O
minimize	O
the	O
obtained	O
expression	B
yields	O
the	O
desired	O
inequality	B
:	O
first	O
verify	O
that	O
the	O
minimum	O
occurs	O
for	O
ne	O
u	O
=	O
-log-	O
2	O
'	O
2	O
n	O
where	O
e	O
=	O
e2d	O
(	O
2nd	O
+	O
1	O
)	O
.	O
check	O
that	O
if	O
n	O
2	O
:	O
d	O
,	O
then	O
u	O
?	O
:	O
(	O
2d	O
/	O
n	O
)	O
2.	O
then	O
note	O
that	O
the	O
bound	O
ee-nu	O
/	O
2	O
+	O
u	O
equals	O
2	O
-log	O
-	O
n	O
nee	O
2	O
2	O
n	O
2	O
n	O
:	O
:	O
:	O
-log	O
(	O
e2d+2nd+l	O
)	O
=	O
-	O
(	O
cd	O
+	O
1	O
)	O
log	O
n	O
+	O
(	O
2d	O
+	O
2	O
)	O
)	O
.	O
0	O
observe	O
for	O
now	O
that	O
the	O
bound	O
on	O
p	O
{	O
l	O
(	O
¢	O
)	O
>	O
l	O
+	O
e	O
}	O
decreases	O
rapidly	O
with	O
n.	O
to	O
have	O
an	O
impact	O
,	O
it	O
must	O
become	O
less	O
than	O
0	O
for	O
smallo	O
.	O
this	O
happens	O
,	O
roughly	O
speaking	O
,	O
when	O
n	O
?	O
:	O
e	O
.	O
~	O
(	O
log	O
~	O
+	O
log	O
~	O
)	O
e2	O
e2	O
0	O
for	O
some	O
constant	O
e.	O
doubling	O
d	O
,	O
the	O
dimension	B
,	O
causes	O
this	O
minimal	O
sample	O
size	O
to	O
roughly	O
double	O
as	O
well	O
.	O
4.5	O
empiiical	O
risk	O
minimization	O
53	O
an	O
important	O
special	O
case	O
is	O
when	O
the	O
distribution	B
is	O
linearly	O
separable	O
,	O
that	O
is	O
,	O
l	O
=	O
o.	O
in	O
such	O
cases	O
the	O
empirical	B
risk	I
minimization	I
above	O
performs	O
even	O
better	O
as	O
the	O
size	O
of	O
the	O
error	O
improves	O
to	O
0	O
(	O
d	O
log	O
n	O
in	O
)	O
from	O
0	O
(	O
j	O
d	O
log	O
n	O
in	O
)	O
.	O
clearly	O
,	O
the	O
data	O
points	O
are	O
linearly	O
separable	O
as	O
well	O
,	O
that	O
is	O
,	O
l17	O
(	O
¢*	O
)	O
=	O
0	O
with	O
probability	O
one	O
,	O
and	O
therefore	O
l17	O
(	O
;	O
;	O
;	O
)	O
:	O
:	O
:	O
din	O
with	O
probability	O
one	O
.	O
theorem	B
4.6.	O
assume	O
that	O
x	O
has	O
a	O
density	O
,	O
and	O
that	O
the	O
best	O
linear	B
classifier	I
has	O
zero	O
probability	O
of	O
error	O
(	O
l	O
=	O
0	O
)	O
.	O
then	O
for	O
the	O
empirical	B
risk	I
minimization	I
algorithm	O
of	O
theorem	O
4.5	O
,	O
for	O
all	O
n	O
>	O
d	O
and	O
e	O
:	O
:	O
:	O
1	O
,	O
p	O
(	O
l	O
(	O
~	O
)	O
>	O
<	O
j	O
:	O
:0	O
2	O
(	O
:	O
)	O
e-i'-d	O
)	O
'	O
,	O
and	O
.-	O
...	O
}	O
e	O
l	O
(	O
¢	O
)	O
:	O
:	O
:	O
{	O
dlogn	O
+	O
2	O
.	O
n-d	O
proof	O
.	O
by	O
the	O
union	O
bound	O
,	O
p	O
{	O
l	O
(	O
¢	O
)	O
>	O
<	O
}	O
:	O
:0	O
p	O
t=1'2	O
''	O
.	O
,	O
mff	O
''	O
(	O
¢	O
;	O
)	O
~~	O
l	O
(	O
<	O
/	O
>	O
i	O
)	O
>	O
<	O
}	O
<	O
~	O
?	O
{	O
ln	O
(	O
<	O
/	O
>	O
i	O
)	O
:	O
:0	O
;	O
;	O
'	O
l	O
(	O
<	O
/	O
>	O
i	O
)	O
>	O
<	O
}	O
,	O
2g	O
)	O
.-	O
...	O
d	O
by	O
symmetry	O
,	O
this	O
sum	O
equals	O
2	O
(	O
:	O
)	O
p	O
{	O
£n	O
(	O
<	O
/	O
>	O
i	O
)	O
:	O
:0	O
~	O
,	O
l	O
(	O
<	O
/	O
>	O
i	O
)	O
>	O
<	O
}	O
=	O
2	O
(	O
:	O
)	O
e	O
{	O
p	O
{	O
£n	O
(	O
<	O
/	O
>	O
'	O
)	O
:	O
:0	O
~	O
,	O
l	O
(	O
<	O
/	O
>	O
i	O
)	O
>	O
<	O
i	O
xi	O
''	O
''	O
xd	O
}	O
}	O
,	O
where	O
,	O
as	O
in	O
theorem	O
4.5	O
,	O
¢l	O
is	O
determined	O
by	O
the	O
data	O
points	O
xl	O
,	O
...	O
,	O
x	O
d	O
.	O
how	O
(	O
cid:173	O
)	O
ever	O
,	O
p	O
{	O
£n	O
(	O
<	O
/	O
>	O
,	O
)	O
:	O
:o	O
~	O
,	O
l	O
(	O
<	O
/	O
>	O
i	O
)	O
>	O
<	O
i	O
xi	O
,	O
''	O
''	O
xd	O
}	O
<	O
p	O
{	O
¢i	O
(	O
xd+l	O
)	O
=	O
yd+l	O
,	O
...	O
,	O
¢1	O
(	O
x17	O
)	O
=	O
y17	O
,	O
l	O
(	O
¢l	O
)	O
>	O
ei	O
xl	O
,	O
...	O
,	O
xd	O
}	O
(	O
since	O
all	O
of	O
the	O
(	O
at	O
most	O
d	O
)	O
errors	O
committed	O
by	O
¢l	O
occur	O
for	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xd	O
,	O
yd	O
»	O
(	O
1	O
-	O
e	O
)	O
17-d	O
,	O
<	O
since	O
the	O
probability	O
that	O
no	O
(	O
xi	O
,	O
yi	O
)	O
,	O
pair	O
i	O
=	O
d	O
+	O
1	O
,	O
...	O
,	O
n	O
falls	O
in	O
the	O
set	O
{	O
(	O
x	O
,	O
y	O
)	O
:	O
<	O
pi	O
(	O
x	O
)	O
=i	O
y	O
}	O
is	O
less	O
than	O
(	O
1	O
-	O
e	O
)	O
17-d	O
if	O
the	O
probability	O
of	O
the	O
set	O
is	O
larger	O
than	O
e.	O
the	O
x	O
:	O
:	O
:	O
e-x	O
.	O
proof	O
of	O
the	O
probability	O
inequality	B
may	O
be	O
completed	O
by	O
noting	O
that	O
1	O
54	O
4.	O
linear	O
discrimination	O
for	O
the	O
expected	O
error	O
probability	O
,	O
note	O
that	O
for	O
any	O
u	O
>	O
0	O
,	O
p	O
{	O
l	O
(	O
¢	O
)	O
>	O
t	O
}	O
dt	O
e	O
{	O
l	O
(	O
¢	O
)	O
}	O
100	O
<	O
u	O
+	O
rx	O
u	O
+	O
2n	O
d	O
100	O
(	O
by	O
the	O
probability	O
inequality	B
and	O
g	O
)	O
:	O
:	O
:	O
:	O
:	O
:	O
nd	O
u	O
+	O
_e-	O
(	O
n-d	O
)	O
u	O
,	O
p	O
{	O
l	O
(	O
¢	O
)	O
>	O
t	O
}	O
dt	O
)	O
<	O
e-	O
(	O
n-d	O
)	O
t	O
dt	O
2nd	O
n	O
we	O
choose	O
u	O
to	O
minimize	O
the	O
obtained	O
bound	O
,	O
which	O
yields	O
the	O
desired	O
inequality	B
.	O
o	O
4.6	O
minimizing	O
other	O
criteria	O
empirical	B
risk	I
minimization	I
uses	O
extensive	O
computations	O
,	O
because	O
ln	O
(	O
4	O
)	O
)	O
is	O
not	O
a	O
unimodal	O
function	O
in	O
general	O
(	O
see	O
problems	O
4.10	O
and	O
4.11	O
)	O
.	O
also	O
,	O
gradient	O
op	O
(	O
cid:173	O
)	O
timization	O
is	O
difficult	O
because	O
the	O
gradients	O
are	O
zero	O
almost	O
everywhere	O
.	O
in	O
fact	O
,	O
given	O
n	O
labeled	O
points	O
in	O
n	O
d	O
,	O
finding	O
the	O
best	O
linear	O
dichotomy	O
is	O
np	O
hard	O
(	O
see	O
johnson	O
and	O
preparata	O
(	O
1978	O
)	O
)	O
.	O
to	O
aid	O
in	O
the	O
optimization	O
,	O
some	O
have	O
suggested	O
minimizing	O
a	O
modified	O
empirical	B
error	I
,	O
such	O
as	O
or	O
--	O
l	O
n	O
(	O
4	O
)	O
)	O
=	O
-	O
~	O
\ii	O
(	O
2yi	O
-	O
1	O
)	O
-	O
a	O
xi	O
-	O
ao	O
t	O
)	O
,	O
1~	O
(	O
n	O
i=l	O
where	O
\ii	O
is	O
a	O
positive	O
convex	O
function	O
.	O
of	O
particular	O
importance	O
here	O
is	O
the	O
mean	O
square	O
error	O
criterion	O
\ii	O
(	O
u	O
)	O
=	O
u2	O
(	O
see	O
,	O
e.g.	O
,	O
widrow	O
and	O
hoff	O
(	O
1960	O
)	O
)	O
.	O
one	O
can	O
easily	O
verify	O
that	O
ln	O
(	O
4	O
)	O
)	O
has	O
a	O
gradient	O
(	O
with	O
respect	O
to	O
(	O
a	O
,	O
ao	O
)	O
)	O
that	O
may	O
aid	O
in	O
locating	O
a	O
local	O
minimum	O
.	O
let	O
-	O
;	O
;	O
;	O
denote	O
the	O
linear	O
discrimination	O
rule	B
minimizing	O
over	O
all	O
a	O
and	O
ao	O
.	O
a	O
description	O
of	O
the	O
solution	O
is	O
given	O
in	O
problem	O
4.14.	O
even	O
in	O
a	O
one-dimensional	O
situation	O
,	O
the	O
mean	O
square	O
error	O
criterion	O
muddles	O
the	O
issue	O
and	O
does	O
not	O
give	O
any	O
performance	O
guarantees	O
:	O
4.6	O
minimizing	O
other	O
criteria	O
55	O
theorem	B
4.7.	O
if	O
sup	O
(	O
x	O
,	O
y	O
)	O
denotes	O
the	O
supremum	O
with	O
respect	O
to	O
all	O
distributions	O
on	O
r	O
x	O
{	O
o	O
,	O
i	O
}	O
,	O
then	O
sup	O
(	O
l	O
(	O
¢	O
)	O
-	O
l	O
)	O
=	O
1	O
,	O
where	O
1	O
)	O
is	O
a	O
linear	O
discriminant	O
obtained	O
by	O
minimizing	O
(	O
x	O
,	O
y	O
)	O
over	O
all	O
al	O
and	O
ao	O
.	O
remark	O
.	O
this	O
theorem	B
establishes	O
the	O
existence	O
of	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
for	O
which	O
l	O
(	O
1	O
)	O
>	O
1	O
-	O
e	O
and	O
l	O
<	O
e	O
simultaneously	O
for	O
arbitrarily	O
small	O
e	O
>	O
0.	O
therefore	O
,	O
minimizing	O
the	O
mean	O
square	O
error	O
criterion	O
is	O
not	O
recommended	O
unless	O
one	O
has	O
additional	O
information	O
regarding	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
.	O
0	O
proof	O
.	O
let	O
e	O
>	O
°	O
and	O
8	O
>	O
0.	O
consider	O
a	O
triatomic	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
:	O
p	O
{	O
(	O
x	O
,	O
y	O
)	O
=	O
(	O
-8	O
,	O
i	O
)	O
}	O
=	O
p	O
{	O
(	O
x	O
,	O
y	O
)	O
=	O
(	O
1	O
,	O
i	O
)	O
}	O
=	O
e/2	O
,	O
p	O
{	O
(	O
x	O
,	O
y	O
)	O
=	O
(	O
0	O
,	O
o	O
)	O
}	O
=	O
1	O
-	O
e.	O
-8	O
~.~	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
40	O
0	O
1	O
•	O
figure	O
4.6.	O
a	O
distribution	O
for	O
which	O
squared	B
error	I
minimization	I
fails	O
.	O
probability	O
£/2	O
probability	O
1-£	O
probability	O
£/2	O
for	O
e	O
<	O
1/2	O
,	O
the	O
best	O
linear	O
rule	O
decides	O
class	O
°	O
on	O
[	O
-8/2	O
,	O
(	O
0	O
)	O
and	O
1	O
elsewhere	O
,	O
for	O
a	O
probability	O
of	O
error	O
l	O
=	O
e	O
/2	O
.	O
the	O
mean	O
square	O
error	O
criterion	O
asks	O
that	O
we	O
minimize	O
--	O
--	O
l	O
(	O
¢	O
)	O
=	O
(	O
l-e	O
)	O
(	O
-l-v	O
)	O
+	O
''	O
2	O
(	O
1-u-v	O
)	O
+	O
''	O
2	O
(	O
1+u8-v	O
)	O
2	O
e2	O
e	O
{	O
2	O
}	O
with	O
respect	O
to	O
ao	O
=	O
v	O
and	O
a	O
1	O
=	O
u.	O
setting	O
the	O
derivatives	O
with	O
respect	O
to	O
u	O
and	O
v	O
equal	O
to	O
zero	O
yields	O
(	O
v	O
-	O
1	O
)	O
8	O
-	O
v	O
u=	O
and	O
v	O
=	O
2e	O
-	O
1	O
+	O
``	O
2	O
u	O
(	O
8	O
-	O
1	O
)	O
,	O
e	O
for	O
~8	O
(	O
8	O
-	O
1	O
)	O
v	O
=	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-=	O
-	O
-	O
-	O
-	O
-	O
-	O
1	O
+82	O
-	O
~	O
(	O
1	O
8	O
)	O
2	O
if	O
we	O
let	O
e	O
-	O
!	O
-	O
°	O
and	O
let	O
8	O
t	O
00	O
,	O
then	O
v	O
'	O
''	O
''	O
3e	O
/2	O
.	O
thus	O
,	O
for	O
e	O
small	O
enough	O
and	O
8	O
large	O
enough	O
,	O
considering	O
the	O
decision	O
at	O
°	O
only	O
,	O
l	O
(	O
1	O
)	O
2	O
:	O
:	O
:	O
1	O
-	O
e	O
,	O
because	O
at	O
x	O
=	O
0	O
,	O
ux	O
+	O
v	O
=	O
v	O
>	O
0.	O
thus	O
,	O
l	O
(	O
1	O
)	O
-	O
l	O
2	O
:	O
:	O
:	O
1	O
-	O
3e	O
/2	O
for	O
e	O
small	O
enough	O
and	O
8	O
large	O
enough	O
.	O
0	O
56	O
4.	O
linear	O
discrimination	O
others	O
have	O
suggested	O
minimizing	O
n	O
l	O
(	O
a	O
(	O
a	O
t	O
xi	O
+	O
ao	O
)	O
-	O
yi	O
)	O
2	O
,	O
i==l	O
where	O
a	O
(	O
u	O
)	O
is	O
a	O
sigmoid	O
,	O
that	O
is	O
,	O
an	O
increasing	O
function	O
from	O
0	O
to	O
1	O
such	O
as	O
1/	O
(	O
1	O
+	O
)	O
,	O
see	O
,	O
for	O
example	O
,	O
wassel	O
and	O
sklansky	O
(	O
1972	O
)	O
,	O
do	O
tu	O
and	O
installe	O
(	O
1975	O
)	O
,	O
e-	O
u	O
fritz	O
and	O
gyorfi	O
(	O
1976	O
)	O
,	O
and	O
sklansky	O
and	O
wassel	O
(	O
1979	O
)	O
.	O
clearly	O
,	O
a	O
(	O
u	O
)	O
=	O
i	O
{	O
uc	O
:	O
:o	O
}	O
provides	O
the	O
empirical	B
en-or	O
probability	O
.	O
however	O
,	O
the	O
point	O
here	O
is	O
to	O
use	O
smooth	O
sigmoids	O
so	O
that	O
gradient	O
algorithms	O
may	O
be	O
used	O
to	O
find	O
the	O
optimum	O
.	O
this	O
may	O
be	O
viewed	O
as	O
a	O
compromise	O
between	O
the	O
mean	O
squared	O
en-or	O
criteria	O
and	O
empirical	B
en-or	O
minimization	O
.	O
here	O
,	O
too	O
,	O
anomalies	O
can	O
occur	O
,	O
and	O
the	O
en-or	O
space	O
is	O
not	O
well	O
behaved	O
,	O
displaying	O
many	O
local	O
minima	O
(	O
hertz	O
,	O
krogh	O
,	O
and	O
palmer	O
(	O
1991	O
,	O
p.1	O
08	O
»	O
.	O
see	O
,	O
however	O
,	O
problems	O
4.16	O
and	O
4.17.	O
problems	O
and	O
exercises	O
problem	O
4.1.	O
with	O
the	O
notation	O
of	O
theorem	O
4.1	O
,	O
show	O
that	O
the	O
error	O
probability	O
l	O
of	O
a	O
one-dimensional	O
theoretical	B
stoller	O
split	O
satisfies	O
4p	O
(	O
1	O
-	O
p	O
)	O
l~	O
--	O
--	O
--	O
--	O
--	O
-	O
)	O
(	O
mo-mj	O
)	O
2	O
-	O
p	O
(	O
l-p	O
)	O
(	O
j6+p°-f	O
1	O
+	O
(	O
1	O
p	O
(	O
gyorfi	O
and	O
vajda	O
(	O
1980	O
»	O
.	O
is	O
this	O
bound	O
better	O
than	O
that	O
of	O
theorem	O
4.1	O
?	O
hint	O
:	O
for	O
any	O
threshold	B
rule	O
gc	O
(	O
x	O
)	O
=	O
i	O
{	O
x	O
:	O
:.c	O
}	O
and	O
u	O
>	O
0	O
,	O
write	O
l	O
(	O
gj	O
pix	O
-	O
c	O
:	O
:	O
:	O
0	O
,	O
2y	O
-	O
1	O
=	O
-i	O
}	O
+	O
pix	O
-	O
c	O
<	O
0	O
,	O
2y	O
-	O
1	O
=	O
i	O
}	O
<	O
p	O
{	O
lu	O
(	O
x	O
-	O
c	O
)	O
-	O
(	O
2y	O
-	O
1	O
)	O
1	O
:	O
:	O
:	O
1	O
}	O
<	O
e	O
{	O
(	O
u	O
(	O
x	O
-	O
c	O
)	O
-	O
(	O
2y	O
-	O
i	O
»	O
2	O
}	O
by	O
chebyshev	O
's	O
inequality	B
.	O
choose	O
u	O
and	O
c	O
to	O
minimize	O
the	O
upper	O
bound	O
.	O
problem	O
4.2.	O
let	O
p	O
=	O
1/2	O
.	O
if	O
l	O
is	O
the	O
error	O
probability	O
of	O
the	O
one-dimensional	O
theoretical	B
stoller	O
split	O
,	O
show	O
that	O
1	O
l	O
<	O
--	O
--	O
-	O
;	O
:	O
(	O
cid:173	O
)	O
-	O
2	O
+	O
2	O
(	O
mo-mj	O
)	O
2	O
(	O
(	O
jo+	O
(	O
jj	O
}	O
2	O
show	O
that	O
the	O
bound	O
is	O
achieved	O
for	O
some	O
distribution	B
when	O
the	O
class-conditional	B
distribu	O
(	O
cid:173	O
)	O
tions	O
of	O
x	O
(	O
that	O
is	O
,	O
given	O
y	O
=	O
°	O
and	O
y	O
=	O
1	O
)	O
are	O
concentrated	O
on	O
two	O
points	O
each	O
,	O
one	O
of	O
which	O
is	O
shared	O
by	O
both	O
classes	O
(	O
chernoff	O
(	O
1971	O
)	O
,	O
becker	O
(	O
1968	O
»	O
.	O
problem	O
4.3.	O
let	O
x	O
be	O
a	O
univariate	O
random	O
variable	B
.	O
the	O
distribution	B
functions	O
for	O
x	O
given	O
y	O
=	O
1	O
and	O
y	O
=	O
°	O
are	O
f1	O
and	O
fo	O
respectively	O
.	O
assume	O
that	O
the	O
moment	O
generating	O
functions	O
for	O
x	O
exist	O
,	O
that	O
is	O
,	O
e	O
{	O
etxiy	O
=	O
1	O
}	O
=	O
o/l	O
(	O
t	O
)	O
,	O
e	O
{	O
etxiy	O
=	O
o	O
}	O
=	O
%	O
(	O
t	O
)	O
,	O
ten	O
,	O
where	O
0/1	O
,	O
0/0	O
are	O
finite	O
for	O
all	O
t.	O
in	O
the	O
spirit	O
of	O
theorem	O
4.1	O
,	O
derive	O
an	O
upper	O
bound	O
for	O
l	O
in	O
function	O
of	O
0/1	O
,	O
0/0	O
'	O
apply	O
your	O
bound	O
to	O
the	O
case	O
that	O
f1	O
and	O
fo	O
are	O
both	O
normal	B
with	O
possibly	O
different	O
means	O
and	O
variances	O
.	O
problems	O
and	O
exercises	O
57	O
problem	O
4.4.	O
signals	O
in	O
additne	O
gaussian	B
noise	I
.	O
let	O
so	O
,	O
sl	O
e	O
rd	O
be	O
fixed	O
,	O
and	O
let	O
n	O
be	O
a	O
multivariate	O
gaussian	B
random	O
variable	B
with	O
zero	O
mean	O
and	O
covariance	O
matrix	O
2	O
:	O
.	O
let	O
pry	O
=	O
o	O
}	O
=	O
pry	O
=	O
i	O
}	O
=	O
1/2	O
,	O
and	O
define	O
x	O
=	O
{	O
..	O
so	O
+n	O
si	O
+n	O
if	O
y	O
=	O
0	O
if	O
y	O
=	O
1.	O
construct	O
the	O
bayes	O
decision	O
and	O
calculate	O
l	O
*	O
.	O
prove	O
that	O
if	O
2	O
:	O
is	O
the	O
identity	O
matrix	O
,	O
and	O
so	O
and	O
si	O
have	O
constant	O
components	O
,	O
then	O
l	O
*	O
-+	O
0	O
exponentially	O
rapidly	O
as	O
d	O
-+	O
00.	O
problem	O
4.5.	O
in	O
the	O
last	O
step	O
of	O
the	O
proof	O
of	O
theorem	O
4.2	O
,	O
we	O
used	O
the	O
dvoretzky-kiefer	O
(	O
cid:173	O
)	O
wolfowitz-massart	O
inequality	B
(	O
theorem	B
12.9	O
)	O
.	O
this	O
result	O
states	O
that	O
if	O
zi	O
,	O
...	O
,	O
zn	O
are	O
i.i.d	O
.	O
random	O
variables	O
on	O
the	O
real	O
line	O
with	O
distribution	O
function	O
f	O
(	O
z	O
)	O
=	O
p	O
{	O
zi	O
:	O
:	O
:	O
z	O
}	O
and	O
empirical	B
distribution	O
function	O
fn	O
(	O
z	O
)	O
=	O
(	O
l/n	O
)	O
l	O
:	O
;	O
z=1	O
i	O
{	O
zi	O
:	O
:ozj	O
,	O
then	O
use	O
this	O
inequality	B
to	O
conclude	O
that	O
p	O
{	O
s~p	O
iv	O
(	O
c	O
(	O
x	O
,	O
1	O
»	O
-	O
vn	O
(	O
c	O
(	O
x	O
,	O
1	O
)	O
1	O
2	O
:	O
i	O
}	O
:	O
:	O
:	O
2e-	O
2n	O
(	O
e/2	O
)	O
2.	O
hint	O
:	O
map	O
(	O
x	O
,	O
y	O
)	O
on	O
the	O
real	O
line	O
by	O
a	O
one-to-one	O
function	O
1jf	O
:	O
(	O
r	O
x	O
{	O
o	O
,	O
i	O
}	O
)	O
-+	O
r	O
such	O
that	O
z	O
=	O
1jf	O
(	O
(	O
x	O
,	O
y	O
»	O
<	O
0	O
if	O
and	O
only	O
if	O
y	O
=	O
o.	O
use	O
the	O
dvoretzky-kiefer-wolfowitz-massart	O
inequality	B
for	O
z.	O
problem	O
4.6.	O
let	O
l	O
be	O
the	O
probability	O
of	O
error	O
for	O
the	O
best	O
sphere	B
rule	O
,	O
that	O
is	O
,	O
for	O
the	O
rule	B
that	O
associates	O
a	O
class	O
with	O
the	O
inside	O
of	O
a	O
sphere	O
sx	O
,	O
n	O
and	O
the	O
other	O
class	O
with	O
the	O
outside	O
.	O
here	O
the	O
center	O
x	O
,	O
and	O
radius	O
r	O
are	O
both	O
variable	B
.	O
show	O
that	O
l	O
=	O
1/2	O
if	O
and	O
only	O
if	O
l	O
*	O
=	O
1/2	O
,	O
and	O
that	O
l	O
:	O
:	O
:	O
1/2	O
.	O
problem	O
4.7.	O
with	O
the	O
notation	O
of	O
theorem	O
4.4	O
,	O
show	O
that	O
the	O
probability	O
of	O
error	O
l	O
of	O
the	O
best	O
linear	B
discriminant	I
satisfies	O
4p	O
(	O
l	O
-	O
p	O
)	O
l	O
:	O
:	O
:	O
--	O
-=	O
--	O
--	O
--	O
=~­	O
l+p	O
(	O
1-p	O
)	O
fl2	O
'	O
where	O
fl	O
=	O
j	O
(	O
rnl	O
-	O
rno	O
)	O
t2	O
:	O
-i	O
(	O
rnl	O
-	O
rno	O
)	O
,	O
is	O
the	O
mahalanobis	O
distance	B
(	O
chapter	O
3	O
)	O
with	O
2	O
:	O
=	O
p2:1	O
+	O
(	O
l-	O
p	O
)	O
2	O
:	O
o	O
(	O
gyorfi	O
and	O
vajda	O
(	O
1980	O
»	O
.	O
interestingly	O
,	O
the	O
upper	O
bound	O
is	O
just	O
twice	O
the	O
bound	O
of	O
theorem	O
3.4	O
for	O
the	O
asymptotic	O
nearest	O
neighbor	O
error	O
.	O
thus	O
,	O
a	O
large	O
mahalanobis	O
distance	B
does	O
not	O
only	O
imply	O
that	O
the	O
bayes	O
error	O
is	O
small	O
,	O
but	O
also	O
,	O
small	O
error	O
probabilities	O
may	O
be	O
achieved	O
by	O
simple	O
linear	O
classifiers	O
.	O
hint	O
:	O
apply	O
the	O
inequality	B
of	O
problem	O
4.1	O
for	O
the	O
univariate	O
random	O
variable	B
x	O
'	O
=	O
at	O
x	O
a	O
=	O
2	O
:	O
-i	O
(	O
rnl	O
-	O
rno	O
)	O
.	O
problem	O
4.8.	O
if	O
rni	O
and	O
a	O
?	O
are	O
the	O
mean	O
and	O
variance	B
of	I
at	O
x	O
,	O
given	O
that	O
y	O
=	O
i	O
,	O
i	O
=	O
0	O
,	O
1	O
,	O
where	O
a	O
is	O
a	O
column	O
vector	O
of	O
weights	O
,	O
then	O
show	O
that	O
the	O
criterion	O
58	O
4.	O
linear	O
discrimination	O
is	O
minimized	O
for	O
a	O
=	O
(	O
~j	O
+	O
~o	O
)	O
-l	O
(	O
mj	O
-	O
mo	O
)	O
,	O
where	O
mi	O
and	O
~i	O
are	O
the	O
mean	O
vector	O
and	O
covariance	O
matrix	O
of	O
x	O
,	O
given	O
y	O
=	O
i.	O
also	O
,	O
show	O
that	O
is	O
minimized	O
for	O
a	O
=	O
(	O
p~j	O
+	O
(	O
1	O
-	O
p	O
)	O
~o	O
)	O
-j	O
(	O
mj	O
-	O
mo	O
)	O
,	O
where	O
p	O
,	O
1	O
-	O
p	O
are	O
the	O
class	O
probabilities	O
.	O
this	O
exercise	O
shows	O
that	O
if	O
discrimination	O
is	O
attempted	O
in	O
one	O
dimension	B
,	O
we	O
might	O
consider	O
projections	O
a	O
t	O
x	O
where	O
a	O
maximizes	O
the	O
weighted	B
distance	O
between	O
the	O
projected	O
means	O
.	O
problem	O
4.9.	O
in	O
the	O
fisher	O
linear	B
discriminant	I
rule	O
(	O
4.1	O
)	O
with	O
free	O
parameter	O
ao	O
,	O
show	O
that	O
for	O
any	O
e	O
>	O
0	O
,	O
there	O
exists	O
a	O
distribution	O
for	O
(	O
x	O
,	O
y	O
)	O
,	O
x	O
e	O
n	O
2	O
}	O
<	O
00	O
such	O
that	O
infao	O
e	O
{	O
l	O
(	O
gao	O
)	O
}	O
>	O
1/2	O
-	O
e.	O
moreover	O
,	O
if	O
ao	O
is	O
chosen	O
to	O
minimize	O
the	O
squared	B
error	I
,	O
with	O
l	O
=	O
°	O
and	O
e	O
{	O
iixii	O
2	O
then	O
e	O
{	O
l	O
(	O
gao	O
)	O
}	O
>	O
1	O
-	O
e.	O
problem	O
4.10.	O
find	O
a	O
distribution	O
of	O
(	O
x	O
,	O
y	O
)	O
with	O
x	O
e	O
n	O
2	O
such	O
that	O
with	O
probability	O
at	O
least	O
one	O
half	O
,	O
ln	O
(	O
<	O
/	O
»	O
is	O
not	O
unimodal	O
with	O
respect	O
to	O
the	O
weight	O
vector	O
(	O
a	O
,	O
ao	O
)	O
.	O
problem	O
4.11.	O
the	O
following	O
observation	O
may	O
help	O
in	O
developing	O
a	O
fast	O
algorithm	B
to	O
find	O
the	O
best	O
linear	B
classifier	I
in	O
certain	O
cases	O
.	O
assume	O
that	O
the	O
bayes	O
rule	B
is	O
a	O
linear	O
split	O
cutting	O
through	O
the	O
origin	O
,	O
that	O
is	O
,	O
l	O
*	O
=	O
l	O
(	O
a*	O
)	O
for	O
some	O
coefficient	O
vector	O
a*	O
e	O
n	O
d	O
,	O
where	O
l	O
(	O
a	O
)	O
denotes	O
the	O
error	O
probability	O
of	O
the	O
classifier	B
if	O
2	O
:	O
.1=j	O
aix	O
(	O
i	O
)	O
2	O
:	O
°	O
otherwise	O
,	O
and	O
a	O
=	O
(	O
aj	O
,	O
...	O
,	O
ad	O
)	O
'	O
show	O
that	O
l	O
(	O
a	O
)	O
is	O
unimodal	O
as	O
a	O
function	O
of	O
a	O
end	O
,	O
and	O
l	O
(	O
a	O
)	O
is	O
monotone	O
increasing	O
along	O
rays	O
pointing	O
from	O
a*	O
,	O
that	O
is	O
,	O
for	O
any	O
'a	O
e	O
(	O
0	O
,	O
1	O
)	O
and	O
a	O
e	O
n	O
d	O
,	O
l	O
(	O
a	O
)	O
-	O
l	O
(	O
'aa	O
+	O
(	O
1	O
-	O
'a	O
)	O
a*	O
)	O
2	O
:	O
°	O
(	O
fritz	O
and	O
gyorfi	O
(	O
1976	O
»	O
.	O
hint	O
:	O
use	O
the	O
expression	B
l	O
(	O
a	O
)	O
=	O
1/2	O
-	O
j	O
(	O
~	O
(	O
x	O
)	O
-	O
1/2	O
)	O
sign	O
(	O
t	O
aixu	O
)	O
)	O
i	O
''	O
(	O
dx	O
)	O
to	O
show	O
that	O
l	O
(	O
a	O
)	O
-	O
lc'aa	O
+	O
(	O
1	O
-	O
'a	O
)	O
a*	O
)	O
=	O
fa	O
11	O
]	O
(	O
x	O
)	O
-	O
1j21	O
,	O
u	O
(	O
dx	O
)	O
for	O
some	O
set	O
a	O
end	O
.	O
problem	O
4.12.	O
let	O
a	O
=	O
(	O
ao	O
,	O
aj	O
)	O
and	O
a=	O
arg	O
min	O
e	O
{	O
c	O
(	O
2y	O
-	O
1	O
)	O
-	O
ajx	O
-	O
ao	O
)	O
2	O
i	O
(	O
ydga	O
(	O
xi	O
)	O
d	O
,	O
a	O
and	O
ga	O
(	O
x	O
)	O
=	O
i	O
(	O
alx+ao	O
>	O
o	O
}	O
'	O
show	O
that	O
for	O
every	O
e	O
>	O
0	O
,	O
there	O
exists	O
a	O
distribution	O
of	O
(	O
x	O
,	O
y	O
)	O
on	O
n	O
x	O
{	O
o	O
,	O
i	O
}	O
such	O
that	O
leii	O
'	O
)	O
-	O
l	O
2	O
:	O
1	O
-	O
e	O
,	O
where	O
l	O
(	O
a	O
)	O
is	O
the	O
error	O
probability	O
for	O
gao	O
hint	O
:	O
argue	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
4.7.	O
a	O
distribution	O
with	O
four	O
atoms	O
suffices	O
.	O
problem	O
4.13.	O
repeat	O
the	O
previous	O
exercise	O
for	O
a	O
=	O
argmine	O
{	O
i	O
(	O
2y	O
-	O
1	O
)	O
-	O
ajx	O
-	O
aol	O
}	O
.	O
a	O
problems	O
and	O
exercises	O
59	O
problem	O
4.14.	O
let	O
¢*	O
denote	O
the	O
linear	O
discrimination	O
rule	B
that	O
minimizes	O
the	O
mean	O
square	O
error	O
e	O
{	O
(	O
2y	O
-	O
i	O
-	O
at	O
x	O
-	O
ao	O
)	O
2	O
}	O
over	O
all	O
a	O
and	O
ao	O
.	O
as	O
this	O
criterion	O
is	O
quadratic	O
in	O
(	O
a	O
,	O
ao	O
)	O
,	O
it	O
is	O
unimodal	O
.	O
one	O
usually	O
approximates	O
¢*	O
by	O
¢	O
by	O
minimizing	O
li	O
(	O
2yi	O
-	O
i	O
-	O
at	O
xi	O
-	O
ao	O
)	O
2	O
over	O
all	O
a	O
and	O
ao	O
.	O
show	O
that	O
the	O
minimal	O
column	O
vector	O
(	O
a	O
,	O
ao	O
)	O
is	O
given	O
by	O
(	O
~x	O
;	O
x	O
;	O
t	O
)	O
-1	O
(	O
~	O
(	O
2yi	O
~	O
l	O
)	O
x	O
;	O
)	O
,	O
where	O
x	O
;	O
=	O
(	O
xi	O
,	O
1	O
)	O
is	O
a	O
(	O
d	O
+	O
i	O
)	O
-dimensional	O
column	O
vector	O
.	O
problem	O
4.15.	O
the	O
perceptron	B
criterion	I
is	O
j	O
=	O
l	O
i:2yi-1isign	O
(	O
at	O
xi+ao	O
)	O
latxi	O
+aol·	O
find	O
a	O
distribution	O
for	O
which	O
l*	O
=	O
0	O
,	O
l	O
:	O
:	O
:	O
1/4	O
,	O
yet	O
liminfn	O
--	O
-	O
,	O
>	O
ooe	O
{	O
ln	O
(	O
¢	O
)	O
}	O
~	O
1/2	O
,	O
where	O
¢	O
is	O
the	O
linear	O
discrimination	O
rule	B
obtained	O
by	O
using	O
the	O
a	O
and	O
ao	O
that	O
minimize	O
j.	O
problem	O
4.16.	O
let	O
0	O
'	O
be	O
a	O
monotone	O
nondecreasing	O
function	O
on	O
n	O
satisfying	O
limu	O
--	O
-	O
,	O
>	O
_oo	O
0	O
'	O
(	O
u	O
)	O
=	O
0	O
and	O
limu	O
--	O
-	O
,	O
>	O
oo	O
o	O
'	O
(	O
u	O
)	O
=	O
1.	O
for	O
h	O
>	O
0	O
,	O
define	O
o'h	O
(	O
u	O
)	O
=	O
o	O
'	O
(	O
hu	O
)	O
.	O
consider	O
the	O
linear	O
discrimi	O
(	O
cid:173	O
)	O
nation	O
rule	B
¢	O
with	O
a	O
and	O
ao	O
chosen	O
to	O
minimize	O
t	O
(	O
o'h	O
(	O
a	O
t	O
i=l	O
xi	O
+	O
ao	O
)	O
-	O
yi	O
)	O
2	O
.	O
for	O
every	O
fixed	O
h	O
>	O
0	O
and	O
0	O
<	O
e	O
<	O
1	O
,	O
exhibit	O
a	O
distribution	O
with	O
l	O
<	O
e	O
and	O
liminfe	O
{	O
ln	O
(	O
¢	O
)	O
}	O
>	O
1	O
-	O
e.	O
n-+oo	O
on	O
the	O
other	O
hand	O
,	O
show	O
that	O
if	O
h	O
depends	O
on	O
the	O
sample	O
size	O
n	O
such	O
that	O
h	O
-+	O
00	O
as	O
n	O
-+	O
00	O
,	O
then	O
for	O
all	O
distributions	O
,	O
e	O
{	O
ln	O
(	O
¢	O
)	O
}	O
-+	O
l.	O
problem	O
4.17.	O
given	O
y	O
=	O
i	O
,	O
let	O
x	O
be	O
normal	B
with	O
mean	O
mi	O
and	O
covariance	O
matrix	O
li	O
,	O
i	O
=	O
0	O
,	O
1.	O
consider	O
discrimination	O
based	O
upon	O
the	O
minimization	O
of	O
the	O
criterion	O
with	O
respect	O
to	O
a	O
,	O
w	O
,	O
and	O
c	O
,	O
a	O
d	O
x	O
d	O
matrix	O
,	O
d	O
x	O
1	O
vector	O
and	O
constant	O
respectively	O
,	O
where	O
o	O
'	O
(	O
u	O
)	O
=	O
1/	O
(	O
1	O
+	O
e-u	O
)	O
is	O
the	O
standard	B
sigmoid	O
function	O
.	O
show	O
that	O
this	O
is	O
minimized	O
for	O
the	O
same	O
a	O
,	O
w	O
,	O
and	O
c	O
that	O
minimize	O
the	O
probability	O
of	O
error	O
and	O
conclude	O
that	O
in	O
this	O
particular	O
case	O
,	O
the	O
squared	B
error	I
criterion	O
may	O
be	O
used	O
to	O
obtain	O
a	O
bayes-optimal	O
classifier	B
(	O
horne	O
and	O
hush	O
(	O
1990	O
)	O
)	O
.	O
5	O
nearest	B
neighbor	I
rules	I
5.1	O
introduction	O
simple	O
rules	O
survive	O
.	O
the	O
k-nearest	O
neighbor	B
rule	I
,	O
since	O
its	O
conception	O
in	O
1951	O
and	O
1952	O
(	O
fix	O
and	O
hodges	O
(	O
1951	O
;	O
1952	O
;	O
1991a	O
;	O
1991b	O
»	O
,	O
has	O
thus	O
attracted	O
many	O
followers	O
and	O
continues	O
to	O
be	O
studied	O
by	O
many	O
researchers	O
.	O
formally	O
,	O
we	O
define	O
the	O
k	O
-nn	O
rule	B
by	O
gn	O
(	O
x	O
)	O
=	O
{	O
~	O
if	O
l7=1	O
wni	O
i	O
{	O
yi=ll	O
>	O
l7=1	O
wni	O
i	O
{	O
yi=ol	O
otherwise	O
,	O
where	O
wni	O
=	O
1/	O
k	O
if	O
xi	O
is	O
among	O
the	O
k	O
nearest	O
neighbors	O
of	O
x	O
,	O
and	O
wni	O
=	O
0	O
elsewhere	O
.	O
xi	O
is	O
said	O
to	O
be	O
the	O
k-th	O
nearest	B
neighbor	I
of	O
x	O
if	O
the	O
distance	B
iix	O
-	O
xi	O
ii	O
is	O
the	O
k-th	O
smallest	O
among	O
ilx	O
-	O
xiii	O
,	O
...	O
,	O
ilx	O
-	O
xn	O
ii	O
.	O
in	O
case	O
of	O
a	O
distance	O
tie	O
,	O
the	O
candidate	O
with	O
the	O
smaller	O
index	O
is	O
said	O
to	O
be	O
closer	O
to	O
x.	O
the	O
decision	O
is	O
based	O
upon	O
a	O
majority	O
vote	O
.	O
it	O
is	O
convenient	O
to	O
let	O
k	O
be	O
odd	O
,	O
to	O
avoid	O
voting	O
ties	O
.	O
several	O
issues	O
are	O
worth	O
considering	O
:	O
(	O
a	O
)	O
universal	B
consistency	I
.	O
establish	O
convergence	O
to	O
the	O
bayes	O
rule	B
if	O
k	O
-+	O
00	O
and	O
k	O
/	O
n	O
-+	O
0	O
as	O
n	O
-+	O
00.	O
this	O
is	O
dealt	O
with	O
in	O
chapter	O
11	O
.	O
(	O
b	O
)	O
finite	O
k	O
performance	O
.	O
what	O
happens	O
if	O
we	O
hold	O
k	O
fixed	O
and	O
let	O
n	O
tend	O
to	O
infinity	O
?	O
(	O
c	O
)	O
the	O
choice	O
of	O
the	O
weight	O
vector	O
(	O
wn	O
l	O
,	O
...	O
,	O
wnn	O
)	O
.	O
are	O
equal	O
weights	O
for	O
the	O
k	O
nearest	O
neighbors	O
better	O
than	O
unequal	O
weights	O
in	O
some	O
sense	O
?	O
62	O
5.	O
nearest	B
neighbor	I
rules	I
(	O
d	O
)	O
the	O
choice	O
of	O
a	O
distance	O
metric	B
.	O
achieve	O
invariance	B
with	O
respect	O
to	O
a	O
certain	O
family	B
of	I
transformations	O
.	O
(	O
e	O
)	O
the	O
reduction	B
of	I
the	O
data	O
size	O
.	O
can	O
we	O
obtain	O
good	O
performance	O
when	O
the	O
data	O
set	O
is	O
edited	B
and/or	O
reduced	O
in	O
size	O
to	O
lessen	O
the	O
storage	O
load	O
?	O
figure	O
5.1.	O
at	O
every	O
point	O
the	O
decision	O
is	O
the	O
label	O
of	O
the	O
closest	O
data	O
point	O
.	O
the	O
set	O
of	O
points	O
whose	O
nearest	B
neighbor	I
is	O
xi	O
is	O
called	O
the	O
voronoi	O
cell	O
of	O
xi	O
.	O
the	O
partition	B
induced	O
by	O
the	O
voronoi	O
cells	O
is	O
a	O
varona	O
!	O
partition	B
.	O
a	O
voronoi	O
partition	B
of	O
15	O
random	O
points	O
is	O
shown	O
here	O
.	O
in	O
the	O
first	O
couple	O
of	O
sections	O
,	O
we	O
will	O
be	O
concerned	O
with	O
convergence	O
issues	O
for	O
k	O
nearest	B
neighbor	I
rules	I
when	O
k	O
does	O
not	O
change	O
with	O
n.	O
in	O
particular	O
,	O
we	O
will	O
see	O
that	O
for	O
all	O
distributions	O
,	O
the	O
expected	O
error	O
probability	O
e	O
{	O
l	O
n	O
}	O
tends	O
to	O
a	O
limit	O
lknn	O
that	O
is	O
in	O
general	O
close	O
to	O
but	O
larger	O
than	O
l	O
*	O
.	O
the	O
methodology	O
for	O
obtaining	O
this	O
result	O
is	O
interesting	O
in	O
its	O
own	O
right	O
.	O
the	O
expression	B
for	O
lknn	O
is	O
then	O
studied	O
,	O
and	O
several	O
key	O
inequalities	O
such	O
as	O
lnn	O
:	O
:	O
:	O
:	O
:	O
2l	O
*	O
(	O
cover	O
and	O
hart	O
(	O
1967	O
»	O
and	O
lknn	O
:	O
:	O
:	O
:	O
:	O
l	O
*	O
(	O
1	O
+	O
.j2j	O
k	O
)	O
are	O
proved	O
and	O
applied	O
.	O
the	O
other	O
issues	O
mentioned	O
above	O
are	O
dealt	O
with	O
in	O
the	O
remaining	O
sections	O
.	O
for	O
surveys	O
of	O
various	O
aspects	O
of	O
the	O
nearest	B
neighbor	I
or	O
related	O
methods	O
,	O
see	O
dasarathy	O
(	O
1991	O
)	O
,	O
devijver	O
(	O
1980	O
)	O
,	O
or	O
devroye	O
and	O
wagner	O
(	O
1982	O
)	O
.	O
remark	O
.	O
computational	O
concerns	O
.	O
storing	O
the	O
n	O
data	O
pairs	O
in	O
an	O
array	O
and	O
searching	O
for	O
the	O
k	O
nearest	O
neighbors	O
may	O
take	O
time	O
proportional	O
to	O
nkd	O
if	O
done	O
in	O
a	O
naive	B
manner-the	O
``	O
d	O
''	O
accounts	O
for	O
the	O
cost	O
of	O
one	O
distance	B
computation	O
.	O
this	O
complexity	O
may	O
be	O
reduced	O
in	O
terms	O
of	O
one	O
or	O
more	O
of	O
the	O
three	O
factors	O
involved	O
.	O
typically	O
,	O
with	O
k	O
and	O
d	O
fixed	O
,	O
o	O
(	O
n	O
lid	O
)	O
worst-case	O
time	O
(	O
papadimitriou	O
and	O
bentley	O
(	O
1980	O
»	O
and	O
0	O
(	O
log	O
n	O
)	O
expected	O
time	O
(	O
friedman	O
,	O
bentley	O
,	O
and	O
finkel	O
(	O
1977	O
»	O
may	O
be	O
achieved	O
.	O
multidimensional	O
search	O
trees	O
that	O
partition	B
the	O
space	O
and	O
guide	O
the	O
search	O
are	O
invaluable-for	O
this	O
approach	O
,	O
see	O
fukunaga	O
and	O
n	O
arendra	O
(	O
1975	O
)	O
,	O
friedman	O
,	O
bentley	O
,	O
and	O
finkel	O
(	O
1977	O
)	O
,	O
niemann	O
and	O
goppert	O
(	O
1988	O
)	O
,	O
kim	O
and	O
park	O
(	O
1986	O
)	O
,	O
and	O
broder	O
(	O
1990	O
)	O
.	O
we	O
refer	O
to	O
a	O
survey	O
in	O
dasarathy	O
(	O
1991	O
)	O
for	O
more	O
references	O
.	O
other	O
approaches	O
are	O
described	O
by	O
yunck	O
(	O
1976	O
)	O
,	O
friedman	O
,	O
baskett	O
,	O
and	O
shustek	O
(	O
1975	O
)	O
,	O
vidal	O
(	O
1986	O
)	O
,	O
sethi	O
(	O
1981	O
)	O
,	O
and	O
farag6	O
,	O
linder	O
,	O
and	O
lugosi	O
(	O
1993	O
)	O
.	O
generally	O
,	O
with	O
preprocessing	O
,	O
one	O
may	O
considerably	O
reduce	O
the	O
overall	O
complexity	O
in	O
terms	O
of	O
nand	O
d.	O
0	O
5.2	O
notation	O
and	O
simple	O
asymptotics	O
63	O
5.2	O
notation	O
and	O
simple	O
asymptotics	O
we	O
fix	O
x	O
e	O
rd	O
,	O
andreorderthedata	O
(	O
x	O
1	O
,	O
yd	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
accordingtoincreasing	O
values	O
of	O
ii	O
xi	O
-	O
x	O
ii	O
.	O
the	O
reordered	O
data	O
sequence	O
is	O
denoted	O
by	O
if	O
no	O
confusion	O
is	O
possible	O
.	O
x	O
(	O
k	O
)	O
(	O
x	O
)	O
is	O
the	O
k-th	O
nearest	B
neighbor	I
of	O
x.	O
remark	O
.	O
we	O
note	O
here	O
that	O
,	O
rather	O
arbitrarily	O
,	O
we	O
defined	O
neighbors	O
in	O
terms	O
of	O
y	O
ii	O
.	O
surprisingly	O
,	O
the	O
asymptotic	O
properties	O
derived	O
in	O
the	O
euclidean	O
distance	B
ilx	O
this	O
chapter	O
remain	O
valid	O
to	O
a	O
wide	O
variety	O
of	O
metrics-the	O
asymptotic	O
probability	O
of	O
error	O
is	O
independent	O
of	O
the	O
distance	B
measure	O
(	O
see	O
problem	O
5.1	O
)	O
.	O
0	O
denote	O
the	O
probability	O
measure	B
for	O
x	O
by	O
fj	O
.	O
''	O
and	O
let	O
sx	O
,	O
e	O
be	O
the	O
closed	O
ball	O
centered	O
at	O
x	O
of	O
radius	O
e	O
>	O
o.	O
the	O
collection	O
of	O
all	O
x	O
with	O
fj.	O
,	O
(	O
sx	O
,	O
e	O
)	O
>	O
0	O
for	O
all	O
e	O
>	O
0	O
is	O
called	O
the	O
support	B
of	O
x	O
or	O
fj.	O
,	O
.	O
this	O
set	O
plays	O
a	O
key	O
role	O
because	O
of	O
the	O
following	O
property	O
.	O
lemma	O
5.1.	O
if	O
x	O
e	O
support	B
(	O
fj.	O
,	O
)	O
and	O
limn-+oo	O
kin	O
=	O
0	O
,	O
then	O
iix	O
(	O
k	O
)	O
(	O
x	O
)	O
-	O
x	O
ii	O
~	O
0	O
with	O
probability	O
one	O
.	O
if	O
x	O
is	O
independent	O
of	O
the	O
data	O
and	O
has	O
probability	O
measure	B
/-l	O
,	O
then	O
iix	O
(	O
k	O
)	O
(	O
x	O
)	O
-	O
xii	O
~	O
0	O
with	O
probability	O
one	O
whenever	O
kin	O
~	O
o.	O
proof	O
.	O
take	O
e	O
>	O
o.	O
by	O
definition	O
,	O
x	O
e	O
support	B
(	O
fj.	O
,	O
)	O
implies	O
that	O
fj.	O
,	O
(	O
sx	O
,	O
e	O
)	O
>	O
o.	O
observe	O
that	O
iix	O
(	O
k	O
)	O
(	O
x	O
)	O
-	O
x	O
ii	O
>	O
e	O
if	O
and	O
only	O
if	O
1	O
n	O
-	O
li	O
{	O
xes	O
}	O
<	O
-	O
.	O
n	O
i=l	O
k	O
n	O
i	O
x	O
,	O
e	O
by	O
the	O
strong	B
law	O
of	O
large	O
numbers	O
,	O
the	O
left-hand	O
side	O
converges	O
to	O
fj.	O
,	O
(	O
sx	O
,	O
e	O
)	O
>	O
0	O
with	O
probability	O
one	O
,	O
while	O
,	O
by	O
assumption	O
,	O
the	O
right-hand	O
side	O
tends	O
to	O
zero	O
.	O
therefore	O
,	O
iix	O
(	O
k	O
)	O
(	O
x	O
)	O
-	O
xii	O
~	O
0	O
with	O
probability	O
one	O
.	O
the	O
second	O
statement	O
follows	O
from	O
the	O
previous	O
argument	O
as	O
well	O
.	O
first	O
note	O
that	O
by	O
lemma	O
a.1	O
in	O
the	O
appendix	O
,	O
p	O
{	O
x	O
e	O
support	B
(	O
fj.	O
,	O
)	O
}	O
=	O
1	O
,	O
therefore	O
for	O
every	O
e	O
>	O
0	O
,	O
p	O
{	O
iix	O
(	O
k	O
)	O
(	O
x	O
)	O
-	O
xii	O
>	O
e	O
}	O
=	O
e	O
{	O
i	O
{	O
xesupport	O
(	O
il	O
)	O
}	O
p	O
{	O
iix	O
(	O
k	O
)	O
(	O
x	O
)	O
-	O
xii	O
>	O
eix	O
e	O
support	B
(	O
fj.	O
,	O
)	O
}	O
}	O
,	O
which	O
converges	O
to	O
zero	O
by	O
the	O
dominated	B
convergence	I
theorem	I
,	O
proving	O
conver	O
(	O
cid:173	O
)	O
gence	O
in	O
probability	O
.	O
if	O
k	O
does	O
not	O
change	O
with	O
n	O
,	O
then	O
ii	O
x	O
(	O
k	O
)	O
(	O
x	O
)	O
-	O
x	O
ii	O
is	O
monotone	O
decreasing	O
for	O
n	O
~	O
k	O
;	O
therefore	O
,	O
it	O
converges	O
with	O
probability	O
one	O
as	O
well	O
.	O
if	O
k	O
=	O
kn	O
is	O
allowed	O
to	O
grow	O
with	O
n	O
such	O
that	O
kin	O
~	O
0	O
,	O
then	O
using	O
the	O
notation	O
x	O
(	O
kn	O
,	O
n	O
)	O
(	O
x	O
)	O
=	O
x	O
(	O
klx	O
)	O
,	O
we	O
see	O
by	O
a	O
similar	O
argument	O
that	O
the	O
sequence	O
of	O
monotone	O
decreasing	O
random	O
variables	O
sup	O
iix	O
(	O
km	O
,	O
m	O
)	O
(	O
x	O
)	O
-	O
xii	O
~	O
iix	O
(	O
kn	O
,	O
n	O
)	O
(	O
x	O
)	O
-	O
xii	O
m~n	O
64	O
5.	O
nearest	B
neighbor	I
rules	I
converges	O
to	O
zero	O
in	O
probability	O
,	O
and	O
therefore	O
,	O
with	O
probability	O
one	O
as	O
well	O
.	O
this	O
completes	O
the	O
proof	O
.	O
0	O
because	O
1	O
]	O
is	O
measurable	O
(	O
and	O
thus	O
well-behaved	O
in	O
a	O
general	O
sense	O
)	O
and	O
iix	O
(	O
k	O
)	O
(	O
x	O
)	O
-	O
xii	O
is	O
small	O
,	O
the	O
values	O
1	O
]	O
(	O
x	O
(	O
i	O
)	O
(	O
x	O
»	O
should	O
be	O
close	O
to	O
1	O
]	O
(	O
x	O
)	O
for	O
all	O
i	O
small	O
enough	O
.	O
we	O
now	O
introduce	O
a	O
proof	O
method	O
that	O
exploits	O
this	O
fact	O
,	O
and	O
will	O
make	O
subsequent	O
analyses	O
very	O
simple-it	O
suffices	O
to	O
look	O
at	O
data	O
samples	O
in	O
a	O
new	O
way	O
via	O
embedding	B
.	O
the	O
basic	O
idea	O
is	O
to	O
define	O
an	O
auxiliary	O
rule	B
g	O
:	O
l	O
(	O
x	O
)	O
in	O
which	O
the	O
y	O
(	O
i	O
)	O
(	O
x	O
)	O
's	O
are	O
replaced	O
by	O
k	O
i.i.d	O
.	O
bernoulli	O
random	O
variables	O
with	O
parameter	O
1	O
]	O
(	O
x	O
)	O
-locauy	O
,	O
the	O
y	O
(	O
i	O
)	O
(	O
x	O
)	O
's	O
behave	O
in	O
such	O
a	O
way	O
.	O
it	O
is	O
easy	O
to	O
show	O
that	O
the	O
error	O
probabilities	O
of	O
the	O
two	O
rules	O
are	O
close	O
,	O
and	O
analyzing	O
the	O
behavior	O
of	O
the	O
auxiliary	O
rule	B
is	O
much	O
more	O
convenient	O
.	O
to	O
make	O
things	O
more	O
precise	O
,	O
we	O
assume	O
that	O
we	O
are	O
given	O
i.i.d	O
.	O
data	O
pairs	O
(	O
xl	O
,	O
ud	O
,	O
...	O
,	O
(	O
xii	O
'	O
un	O
)	O
,	O
all	O
distributed	O
as	O
(	O
x	O
,	O
u	O
)	O
,	O
where	O
x	O
is	O
as	O
before	O
(	O
and	O
has	O
probability	O
measure	B
f.l	O
on	O
the	O
borel	O
sets	O
of	O
r	O
d	O
)	O
,	O
and	O
u	O
is	O
uniformly	O
distributed	O
on	O
[	O
0,1	O
]	O
and	O
independent	O
of	O
x.	O
if	O
we	O
set	O
y	O
i	O
=	O
l	O
{	O
ui~tj	O
(	O
xi	O
)	O
}	O
'	O
then	O
(	O
xl	O
,	O
y	O
i	O
)	O
,	O
••	O
.	O
,	O
(	O
xn	O
,	O
y	O
n	O
)	O
are	O
i.i.d	O
.	O
and	O
distributed	O
as	O
the	O
prototype	B
pair	O
(	O
x	O
,	O
y	O
)	O
.	O
so	O
why	O
bother	O
with	O
the	O
ui	O
's	O
?	O
in	O
embedding	O
arguments	O
,	O
we	O
will	O
use	O
the	O
same	O
ui	O
's	O
to	O
construct	O
a	O
second	O
data	O
sequence	O
that	O
is	O
heavily	O
correlated	O
(	O
coupled	O
)	O
with	O
the	O
original	O
data	O
sequence	O
,	O
and	O
is	O
more	O
convenient	O
to	O
analyze	O
.	O
for	O
example	O
,	O
for	O
fixed	O
x	O
e	O
r	O
d	O
,	O
we	O
may	O
define	O
y/	O
(	O
x	O
)	O
=	O
l	O
{	O
ui~ry	O
(	O
x	O
)	O
l	O
'	O
we	O
now	O
have	O
an	O
i.i.d	O
.	O
sequence	O
with	O
i-th	O
vector	O
given	O
by	O
xi	O
,	O
yi	O
,	O
y	O
:	O
(	O
x	O
)	O
,	O
ui	O
.	O
re	O
(	O
cid:173	O
)	O
ordering	O
the	O
data	O
sequence	O
according	O
to	O
increasing	O
values	O
of	O
iixi	O
-	O
x	O
ii	O
yields	O
a	O
new	O
sequence	O
with	O
the	O
i-th	O
vector	O
denoted	O
by	O
x	O
(	O
i	O
)	O
(	O
x	O
)	O
,	O
y	O
(	O
i	O
)	O
(	O
x	O
)	O
,	O
y	O
(	O
i	O
)	O
(	O
x	O
)	O
,	O
u	O
(	O
i	O
)	O
(	O
x	O
)	O
.	O
if	O
no	O
confusion	O
is	O
possible	O
,	O
the	O
argument	O
x	O
will	O
be	O
dropped	O
.	O
a	O
rule	O
is	O
called	O
k-local	O
if	O
for	O
n	O
:	O
:	O
:	O
:	O
k	O
,	O
gn	O
is	O
of	O
the	O
form	O
x	O
)	O
=	O
{	O
i	O
if	O
ljr	O
(	O
x	O
,	O
y	O
(	O
l	O
)	O
(	O
x	O
)	O
,	O
...	O
,	O
y	O
(	O
k	O
)	O
(	O
x	O
»	O
>	O
0	O
,	O
°	O
otherwise	O
,	O
(	O
gil	O
(	O
5.1	O
)	O
for	O
some	O
function	O
1/1	O
.	O
for	O
the	O
k-nn	O
rule	B
,	O
we	O
have	O
,	O
for	O
example	O
,	O
in	O
other	O
words	O
,	O
gn	O
takes	O
a	O
majority	O
vote	O
over	O
the	O
k	O
nearest	O
neighbors	O
of	O
x	O
and	O
breaks	O
ties	O
in	O
favor	O
of	O
class	O
0.	O
to	O
study	O
gn	O
turns	O
out	O
to	O
be	O
almost	O
equivalent	O
to	O
studying	O
the	O
approximate	O
rule	B
g	O
''	O
n	O
'	O
ifljr	O
(	O
x	O
,	O
y	O
(	O
l	O
)	O
(	O
x	O
)	O
,	O
...	O
,	O
y	O
(	O
k	O
)	O
(	O
x	O
»	O
>	O
°	O
'	O
(	O
x	O
)	O
=	O
{	O
i	O
gil	O
°	O
otherwise	O
.	O
the	O
latter	O
rule	B
is	O
of	O
no	O
practical	O
value	O
because	O
it	O
requires	O
the	O
knowledge	O
of	O
1	O
]	O
(	O
x	O
)	O
.	O
interestingly	O
however	O
,	O
it	O
is	O
easier	O
to	O
study	O
,	O
as	O
y	O
(	O
l	O
)	O
(	O
x	O
)	O
,	O
...	O
,	O
y	O
(	O
k	O
)	O
(	O
x	O
)	O
are	O
i.i.d.	O
,	O
whereas	O
y	O
(	O
l	O
)	O
(	O
x	O
)	O
,	O
...	O
,	O
y	O
(	O
k	O
)	O
(	O
x	O
)	O
are	O
not	O
.	O
note	O
,	O
in	O
particular	O
,	O
the	O
following	O
:	O
lemma	O
5.2.	O
for	O
all	O
x	O
,	O
n	O
~	O
k	O
,	O
5.2	O
notation	O
and	O
simple	O
asymptotics	O
65	O
p	O
{	O
ljj	O
(	O
x	O
,	O
y	O
(	O
l	O
)	O
(	O
x	O
)	O
,	O
...	O
,	O
y	O
(	O
k	O
)	O
(	O
x	O
)	O
)	O
=/ljj	O
(	O
x	O
,	O
ycl	O
)	O
(	O
x	O
)	O
,	O
...	O
,	O
yck	O
)	O
(	O
x	O
)	O
)	O
}	O
:	O
s	O
;	O
le	O
{	O
11j	O
(	O
x	O
)	O
-	O
1j	O
(	O
x	O
(	O
i	O
)	O
(	O
x	O
)	O
)	O
i	O
}	O
k	O
i=l	O
and	O
p	O
{	O
gn	O
(	O
x	O
)	O
=/	O
g~	O
(	O
x	O
)	O
}	O
:	O
s	O
;	O
le	O
{	O
11j	O
(	O
x	O
)	O
-1j	O
(	O
x	O
(	O
i	O
)	O
(	O
x	O
)	O
)	O
i	O
}	O
.	O
k	O
i=l	O
proof	O
.	O
both	O
statements	O
follow	O
directly	O
from	O
the	O
observation	O
that	O
{	O
ljj	O
(	O
x	O
,	O
ycl	O
)	O
(	O
x	O
)	O
''	O
''	O
,	O
y	O
(	O
k	O
)	O
(	O
x	O
)	O
)	O
=/ljj	O
(	O
x	O
,	O
ycl	O
)	O
(	O
x	O
)	O
''	O
''	O
,	O
yck	O
)	O
(	O
x	O
)	O
)	O
}	O
{	O
(	O
y	O
(	O
1	O
)	O
(	O
x	O
)	O
''	O
..	O
,	O
y	O
(	O
k	O
)	O
(	O
x	O
)	O
)	O
=/	O
(	O
ycl	O
)	O
(	O
x	O
)	O
,	O
...	O
,	O
yck	O
)	O
(	O
x	O
)	O
)	O
}	O
c	O
c	O
u	O
{	O
1j	O
(	O
x	O
(	O
i	O
)	O
(	O
x	O
)	O
)	O
:	O
s	O
;	O
u	O
(	O
i	O
)	O
(	O
x	O
)	O
:	O
s	O
;	O
1j	O
(	O
x	O
)	O
}	O
u	O
u	O
h	O
(	O
x	O
)	O
:	O
s	O
;	O
uci	O
)	O
(	O
x	O
)	O
:	O
s	O
;	O
1j	O
(	O
x	O
(	O
i	O
)	O
(	O
x	O
)	O
)	O
}	O
,	O
k	O
k	O
i=l	O
i=l	O
and	O
using	O
the	O
union	O
bound	O
and	O
the	O
fact	O
that	O
the	O
u	O
(	O
i	O
)	O
(	O
x	O
)	O
's	O
are	O
uniform	B
[	O
0	O
,	O
1	O
]	O
.	O
0	O
we	O
need	O
the	O
following	O
result	O
,	O
in	O
which	O
x	O
is	O
distributed	O
as	O
xl	O
,	O
but	O
independent	O
of	O
the	O
data	O
sequence	O
:	O
lemma	O
5.3	O
.	O
(	O
stone	O
(	O
1977	O
)	O
)	O
.	O
for	O
any	O
integrable	O
function	O
f	O
,	O
any	O
n	O
,	O
and	O
any	O
k	O
:	O
s	O
;	O
n	O
,	O
k	O
le	O
{	O
if	O
(	O
x	O
(	O
i	O
)	O
(	O
x	O
)	O
)	O
i	O
}	O
:	O
s	O
;	O
kyde	O
{	O
lf	O
(	O
x	O
)	O
i	O
}	O
'	O
i=l	O
,	O
j3	O
)	O
d	O
-	O
1	O
depends	O
upon	O
the	O
dimension	B
only	O
.	O
where	O
yd	O
:	O
s	O
;	O
(	O
1	O
+	O
2//2	O
-	O
the	O
proof	O
of	O
this	O
lemma	O
is	O
beautiful	O
but	O
a	O
bit	O
technical-it	O
is	O
given	O
in	O
a	O
separate	O
section	O
.	O
here	O
is	O
how	O
it	O
is	O
applied	O
,	O
and	O
why	O
,	O
for	O
fixed	O
k	O
,	O
we	O
may	O
think	O
of	O
f	O
(	O
x	O
(	O
k	O
)	O
(	O
x	O
)	O
)	O
as	O
f	O
(	O
x	O
)	O
for	O
all	O
practical	O
purposes	O
.	O
lemma	O
5.4.	O
for	O
any	O
integrable	O
function	O
f	O
,	O
1	O
k	O
-le	O
{	O
if	O
(	O
x	O
)	O
-	O
k	O
i=l	O
f	O
(	O
x	O
(	O
i	O
)	O
(	O
x	O
)	O
)	O
i	O
}	O
-+	O
0	O
as	O
n	O
-+	O
00	O
whenever	O
kin	O
-+	O
o.	O
proof	O
.	O
given	O
e	O
>	O
0	O
,	O
find	O
a	O
uniformly	O
continuous	O
function	O
g	O
vanishing	O
off	O
a	O
boundedseta	O
,	O
suchthate	O
{	O
lg	O
(	O
x	O
)	O
-	O
f	O
(	O
x	O
)	O
i	O
}	O
<	O
e	O
(	O
see	O
theorem	B
a.8	O
in	O
the	O
appendix	O
)	O
.	O
66	O
5.	O
nearest	B
neighbor	I
rules	I
thenforeache	O
>	O
o	O
,	O
thereisao	O
>	O
°	O
such	O
that	O
ilx-zll	O
<	O
o	O
implies	O
ig	O
(	O
x	O
)	O
-g	O
(	O
z	O
)	O
1	O
<	O
e.	O
thus	O
,	O
f	O
(	O
x	O
(	O
i	O
)	O
(	O
x	O
»	O
i	O
}	O
1	O
k	O
-le	O
{	O
if	O
(	O
x	O
)	O
-	O
k	O
i=l	O
:	O
:	O
:	O
e	O
{	O
if	O
(	O
x	O
)	O
-	O
g	O
(	O
x	O
)	O
1	O
}	O
+	O
k	O
l	O
e	O
{	O
lg	O
(	O
x	O
)	O
-	O
g	O
(	O
x	O
(	O
i	O
)	O
(	O
x	O
»	O
i	O
}	O
1	O
k	O
i=l	O
1	O
k	O
k	O
i=l	O
f	O
(	O
x	O
(	O
i	O
)	O
(	O
x	O
»	O
i	O
}	O
+	O
-	O
l	O
e	O
{	O
lg	O
(	O
x	O
(	O
i	O
)	O
(	O
x	O
»	O
-	O
(	O
1	O
+	O
yd	O
)	O
e	O
{	O
if	O
(	O
x	O
)	O
g	O
(	O
x	O
)	O
1	O
}	O
+	O
e	O
+	O
iigiioop	O
{	O
iix	O
-	O
x	O
(	O
k	O
)	O
(	O
x	O
)	O
ii	O
>	O
o	O
}	O
(	O
by	O
lemma	O
5.3	O
,	O
where	O
0	O
depends	O
on	O
e	O
only	O
)	O
(	O
2	O
+	O
yd	O
)	O
e	O
+	O
0	O
(	O
1	O
)	O
(	O
by	O
lemma	O
5.1	O
)	O
.	O
0	O
:	O
:	O
:	O
:	O
:	O
:	O
5.3	O
proof	O
of	O
stone	O
's	O
lemma	O
in	O
this	O
section	O
we	O
prove	O
lemma	O
5.3.	O
for	O
e	O
e	O
(	O
0	O
,	O
ni2	O
)	O
,	O
a	O
cone	O
c	O
(	O
x	O
,	O
e	O
)	O
is	O
the	O
col	O
(	O
cid:173	O
)	O
lection	O
of	O
all	O
y	O
e	O
nd	O
for	O
which	O
angle	O
(	O
x	O
,	O
y	O
)	O
:	O
:	O
:	O
e.	O
equivalently	O
,	O
in	O
vector	O
notation	O
,	O
x	O
t	O
yi/ix/illyi/	O
2	O
:	O
cose	O
.	O
the	O
set	O
z	O
+	O
c	O
(	O
x	O
,	O
e	O
)	O
is	O
the	O
translation	O
of	O
c	O
(	O
x	O
,	O
e	O
)	O
by	O
z.	O
figure	O
5.2.	O
a	O
cone	O
of	O
angle	O
e.	O
c	O
(	O
x.e	O
)	O
ify	O
,	O
z	O
e	O
c	O
(	O
x	O
,	O
ni6	O
)	O
,	O
andllyll	O
<	O
iizll	O
,	O
thenily-zll	O
<	O
ilzll	O
,	O
aswewillnowshow	O
.	O
indeed	O
,	O
:	O
:	O
:	O
=	O
ii	O
y	O
112	O
+	O
liz	O
112	O
-	O
211	O
y	O
ililz	O
ii	O
cos	O
(	O
n	O
/3	O
)	O
112	O
(	O
1	O
+	O
iiyll2	O
-~	O
)	O
ii	O
z	O
iizl12	O
ilzll	O
(	O
see	O
figure	O
5.3	O
)	O
.	O
5.3	O
proof	O
of	O
stone	O
's	O
lemma	O
67	O
figure	O
5.3.	O
the	O
key	O
geometrical	O
prop	O
(	O
cid:173	O
)	O
erty	O
of	O
cones	O
of	O
angle	O
<	O
n	O
/2	O
.	O
the	O
following	O
covering	B
lemma	I
is	O
needed	O
in	O
what	O
follows	O
:	O
lemma	O
5.5.	O
lete	O
e	O
(	O
o	O
,	O
n/2	O
)	O
bejixed	O
.	O
then	O
there	O
exists	O
a	O
set	O
{	O
xl	O
,	O
...	O
,	O
xyj	O
end	O
such	O
that	O
yd	O
nd	O
=	O
u	O
c	O
(	O
xi	O
,	O
e	O
)	O
.	O
i=l	O
furthermore	O
,	O
it	O
is	O
always	O
possible	O
to	O
take	O
l	O
)	O
d	O
(	O
yd	O
:	O
:	O
:	O
:	O
1	O
+	O
since	O
/2	O
)	O
-	O
1.	O
for	O
e	O
=	O
n	O
/6	O
,	O
we	O
have	O
yd	O
:	O
:	O
:	O
:	O
(	O
1	O
+	O
2	O
)	O
d	O
_	O
1	O
.	O
)	O
2	O
-	O
v'3	O
proof	O
.	O
we	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
iixi	O
ii	O
=	O
1	O
for	O
all	O
i.	O
each	O
xi	O
is	O
the	O
center	O
of	O
a	O
sphere	O
si	O
of	O
radius	O
r	O
=	O
2	O
since	O
/2	O
)	O
.	O
si	O
has	O
the	O
property	O
that	O
{	O
x	O
:	O
iixll	O
=	O
l	O
}	O
nsi	O
=	O
{	O
x	O
:	O
iixll	O
=	O
l	O
}	O
nc	O
(	O
xi	O
,	O
e	O
)	O
.	O
let	O
us	O
only	O
look	O
at	O
xi	O
's	O
such	O
that	O
iixi	O
-x	O
j	O
''	O
:	O
:	O
:	O
r	O
for	O
all	O
j	O
i	O
i.	O
in	O
that	O
case	O
,	O
u	O
c	O
(	O
xi	O
'	O
e	O
)	O
covers	O
nd	O
if	O
and	O
only	O
if	O
u	O
si	O
covers	O
{	O
x	O
:	O
iix	O
ii	O
=	O
i	O
}	O
.	O
then	O
the	O
spheres	O
s	O
;	O
of	O
radius	O
r/2	O
centered	O
at	O
the	O
xi	O
's	O
are	O
disjoint	O
and	O
u	O
s	O
;	O
s	O
;	O
so,1+rj2	O
-	O
so	O
,	O
rj2	O
(	O
see	O
figure	O
5.4	O
)	O
.	O
68	O
5.	O
nearest	B
neighbor	I
rules	I
figure	O
5.4.	O
bounding	O
yd	O
.	O
thus	O
,	O
if	O
vd	O
=	O
volume	O
(	O
so	O
,	O
l	O
)	O
,	O
or	O
ydvd	O
(	O
~	O
)	O
d	O
~	O
vd	O
(	O
l+~	O
)	O
d	O
_vd	O
(	O
~	O
)	O
d	O
yd	O
~	O
(	O
1	O
+	O
~	O
)	O
d	O
_	O
1	O
=	O
(	O
1	O
+	O
.	O
1	O
)	O
d	O
_	O
l.	O
sm	O
(	O
e	O
/2	O
)	O
r	O
the	O
last	O
inequality	B
follows	O
from	O
the	O
fact	O
that	O
sin	O
~	O
=	O
/1-	O
cos	O
(	O
,	O
,/6	O
)	O
=	O
)	O
2	O
-	O
v'3	O
.	O
d	O
12	O
v	O
2	O
4	O
figure	O
5.5.	O
covering	B
the	O
space	O
by	O
cones	O
.	O
with	O
the	O
preliminary	O
results	O
out	O
of	O
the	O
way	O
,	O
we	O
cover	O
nd	O
by	O
yd	O
cones	O
x	O
+	O
c	O
(	O
xj,1	O
[	O
/6	O
)	O
,	O
1	O
~	O
j	O
~	O
yd	O
,	O
and	O
mark	O
in	O
each	O
cone	O
the	O
xi	O
that	O
is	O
nearest	O
to	O
5.4	O
the	O
asymptotic	O
probability	O
of	O
error	O
69	O
x	O
,	O
if	O
such	O
an	O
xi	O
exists	O
.	O
if	O
xi	O
belongs	O
to	O
x	O
+	O
c	O
(	O
xj	O
,	O
7tj6	O
)	O
and	O
is	O
not	O
marked	O
,	O
then	O
x	O
can	O
not	O
be	O
the	O
nearest	B
neighbor	I
of	O
xi	O
in	O
{	O
xl	O
,	O
...	O
,	O
xi-i	O
,	O
x	O
,	O
xi+l	O
,	O
...	O
,	O
x	O
n	O
}	O
.	O
similarly	O
,	O
we	O
might	O
mark	O
all	O
k	O
nearest	O
neighbors	O
of	O
x	O
in	O
each	O
cone	O
(	O
if	O
there	O
are	O
less	O
than	O
k	O
points	O
in	O
a	O
cone	O
,	O
mark	O
all	O
of	O
them	O
)	O
.	O
by	O
a	O
similar	O
argument	O
,	O
if	O
xi	O
e	O
x	O
+	O
c	O
(	O
x	O
j	O
,	O
7t	O
j	O
6	O
)	O
is	O
not	O
marked	O
,	O
then	O
x	O
can	O
not	O
be	O
among	O
the	O
k	O
nearest	O
neigh	O
(	O
cid:173	O
)	O
bors	O
of	O
xi	O
in	O
{	O
x	O
1	O
,	O
...	O
,	O
xi	O
-1	O
,	O
x	O
,	O
xi	O
+	O
1	O
,	O
...	O
,	O
x	O
n	O
}	O
.	O
(	O
the	O
order	O
of	O
this	O
set	O
of	O
points	O
is	O
important	O
if	O
distance	B
ties	O
occur	O
with	O
positive	O
probability	O
,	O
and	O
they	O
are	O
broken	O
by	O
comparing	O
indices	O
.	O
)	O
therefore	O
,	O
if	O
!	O
is	O
a	O
nonnegative	O
function	O
,	O
k	O
le	O
{	O
!	O
(	O
x	O
(	O
i	O
)	O
(	O
x	O
)	O
)	O
}	O
i=l	O
1=1	O
e	O
{	O
t	O
i	O
{	O
xi	O
is	O
among	O
the	O
k	O
nearest	O
neighbors	O
of	O
xin	O
{	O
xj	O
,	O
...	O
,	O
xn	O
}	O
}	O
!	O
(	O
xi	O
)	O
}	O
e	O
{	O
f	O
(	O
x	O
)	O
t	O
<	O
e	O
{	O
f	O
(	O
x	O
)	O
t	O
i	O
{	O
k	O
,	O
i	O
,	O
``	O
''	O
,	O
ked	O
}	O
}	O
i	O
{	O
x	O
is	O
among	O
the	O
knearest	O
neighbors	O
of	O
xi	O
in	O
{	O
xj	O
''	O
''	O
,	O
xi-j	O
,	O
x	O
,	O
xi+j	O
,	O
...	O
,	O
xn	O
}	O
}	O
}	O
(	O
by	O
exchanging	O
x	O
and	O
xi	O
)	O
<	O
kyde	O
{	O
!	O
(	O
x	O
)	O
}	O
,	O
as	O
we	O
can	O
mark	O
at	O
most	O
k	O
nodes	O
in	O
each	O
cone	O
,	O
and	O
the	O
number	O
of	O
cones	O
is	O
at	O
most	O
yd-see	O
lemma	O
5.5.	O
this	O
concludes	O
the	O
proof	O
of	O
stone	O
's	O
lemma	O
.	O
0	O
5.4	O
the	O
asymptotic	O
probability	O
of	O
error	O
we	O
return	O
to	O
k-iocal	O
rules	O
(	O
and	O
in	O
particular	O
,	O
to	O
k-nearest	O
neighbor	O
rules	O
)	O
.	O
let	O
d~	O
=	O
(	O
(	O
x	O
i	O
,	O
yi	O
,	O
vi	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
,	O
vn	O
)	O
)	O
be	O
the	O
i.i.d	O
.	O
data	O
augmented	O
by	O
the	O
uniform	B
random	O
variables	O
vi	O
,	O
...	O
,	O
vn	O
as	O
described	O
earlier	O
.	O
for	O
a	O
decision	O
gn	O
based	O
on	O
dn	O
,	O
we	O
have	O
the	O
probability	O
of	O
error	O
ln	O
=	O
p	O
{	O
gn	O
(	O
x	O
)	O
i	O
yid~	O
}	O
=	O
p	O
{	O
sign	O
(	O
1/i	O
(	O
x	O
,	O
y	O
(	O
l	O
)	O
(	O
x	O
)	O
,	O
...	O
,	O
y	O
(	O
k	O
)	O
(	O
x	O
)	O
)	O
)	O
i	O
sign	O
(	O
2y	O
-	O
1	O
)	O
id	O
;	O
i	O
}	O
'	O
where	O
1/1	O
is	O
the	O
function	O
whose	O
sign	O
determines	O
gn~	O
see	O
(	O
5.1	O
)	O
.	O
define	O
the	O
random	O
variables	O
ycl	O
)	O
(	O
x	O
)	O
,	O
...	O
,	O
yck	O
)	O
(	O
x	O
)	O
as	O
we	O
did	O
earlier	O
,	O
and	O
set	O
l~	O
=	O
p	O
{	O
sign	O
(	O
1/i	O
(	O
x	O
,	O
ycl	O
)	O
(	O
x	O
)	O
''	O
''	O
,	O
yck	O
)	O
(	O
x	O
)	O
)	O
)	O
isign	O
(	O
2y	O
-1	O
)	O
id~	O
}	O
.	O
70	O
5.	O
nearest	B
neighbor	I
rules	I
by	O
lemmas	O
5.2	O
and	O
5.4	O
,	O
e	O
{	O
iln	O
-	O
l	O
;	O
zl	O
}	O
<	O
p	O
{	O
1jj	O
(	O
x	O
,	O
y	O
(	O
l	O
)	O
(	O
x	O
)	O
,	O
...	O
,	O
y	O
(	O
k	O
)	O
(	O
x	O
)	O
)	O
f	O
1jj	O
(	O
x	O
,	O
ycl	O
)	O
(	O
x	O
)	O
,	O
...	O
,	O
yck	O
)	O
(	O
x	O
)	O
)	O
}	O
<	O
l	O
e	O
{	O
11	O
]	O
(	O
x	O
)	O
-	O
1	O
]	O
(	O
x	O
(	O
i	O
)	O
(	O
x	O
)	O
)	O
i	O
}	O
k	O
i=l	O
0	O
(	O
1	O
)	O
.	O
because	O
limn	O
--	O
-+oo	O
(	O
el	O
;	O
z	O
-	O
eln	O
)	O
=	O
0	O
,	O
we	O
need	O
only	O
study	O
the	O
rule	B
g	O
:	O
z	O
'	O
(	O
x	O
)	O
=	O
{	O
1	O
gn	O
0	O
otherwise	O
if1jj	O
(	O
x	O
,	O
zl	O
,	O
...	O
,	O
zk	O
»	O
o	O
(	O
z	O
}	O
,	O
...	O
,	O
zk	O
are	O
i.i.d	O
.	O
bernoulli	O
(	O
1	O
]	O
(	O
x	O
)	O
)	O
)	O
unless	O
we	O
are	O
concerned	O
with	O
the	O
closeness	O
of	O
ln	O
to	O
eln	O
as	O
well	O
.	O
we	O
now	O
illustrate	O
this	O
important	O
time-saving	O
device	O
on	O
the	O
i-nearest	O
neighbor	B
rule	I
.	O
clearly	O
,	O
1jj	O
(	O
x	O
,	O
zi	O
)	O
=	O
2z1	O
-	O
1	O
,	O
and	O
therefore	O
e	O
{	O
l~	O
}	O
=	O
p	O
{	O
zj	O
f	O
y	O
}	O
=	O
e	O
{	O
21	O
]	O
(	O
x	O
)	O
(	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
)	O
}	O
.	O
we	O
have	O
,	O
without	O
further	O
work	O
:	O
theorem	B
5.1.	O
for	O
the	O
nearest	B
neighbor	I
rule	I
,	O
jor	O
any	O
distribution	B
oj	O
(	O
x	O
,	O
y	O
)	O
,	O
lim	O
e	O
{	O
l	O
n	O
}	O
=	O
e	O
{	O
21	O
]	O
(	O
x	O
)	O
(	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
)	O
}	O
=	O
l	O
nn	O
.	O
n	O
--	O
-+oo	O
under	O
various	O
continuity	O
conditions	O
(	O
x	O
has	O
a	O
density	O
j	O
and	O
both	O
j	O
and	O
1	O
]	O
are	O
almost	O
everywhere	O
continuous	O
)	O
;	O
this	O
result	O
is	O
due	O
to	O
cover	O
and	O
hart	O
(	O
1967	O
)	O
.	O
in	O
the	O
present	O
generality	O
,	O
it	O
essentially	O
appears	O
in	O
stone	O
(	O
1977	O
)	O
.	O
see	O
also	O
devroye	O
(	O
198ic	O
)	O
.	O
elsewhere	O
(	O
chapter	O
3	O
)	O
,	O
we	O
show	O
that	O
l*	O
:5	O
lnn	O
:	O
:	O
:	O
:	O
2l*	O
(	O
1-	O
l*	O
)	O
:	O
:	O
:	O
:	O
2l*	O
.	O
hence	O
,	O
the	O
previous	O
result	O
says	O
that	O
the	O
nearest	B
neighbor	I
rule	I
is	O
asymptotically	O
at	O
most	O
twice	O
as	O
bad	O
as	O
the	O
bayes	O
rule-especially	O
for	O
small	O
l	O
*	O
,	O
this	O
property	O
should	O
be	O
useful	O
.	O
we	O
formally	O
define	O
the	O
quantity	O
,	O
when	O
k	O
is	O
odd	O
,	O
lknn	O
we	O
have	O
the	O
following	O
result	O
:	O
theorem	B
5.2.	O
let	O
k	O
be	O
odd	O
andfixed	O
.	O
then	O
,	O
jor	O
the	O
k-nn	O
rule	B
,	O
lim	O
e	O
{	O
l	O
n	O
}	O
=	O
l	O
knn	O
.	O
n	O
--	O
-+oo	O
5.5	O
the	O
asymptotic	O
error	O
probability	O
of	O
weighted	O
nearest	B
neighbor	I
rules	I
71	O
proof	O
.	O
we	O
note	O
that	O
it	O
suffices	O
to	O
show	O
that	O
limn~oo	O
e	O
{	O
l~	O
}	O
=	O
lknn	O
(	O
in	O
the	O
previ	O
(	O
cid:173	O
)	O
ously	O
introduced	O
notation	O
)	O
.	O
but	O
for	O
every	O
n	O
,	O
e	O
{	O
l~	O
}	O
~	O
p	O
{	O
z1	O
+	O
...	O
+	O
zk	O
>	O
~	O
,	O
y	O
~	O
o	O
}	O
+	O
p	O
{	O
z1	O
+	O
...	O
+	O
zk	O
<	O
~	O
,	O
y	O
~	O
i	O
}	O
~	O
p	O
{	O
z1	O
+	O
...	O
+	O
zk	O
>	O
~	O
,	O
zo	O
~	O
o	O
}	O
+	O
p	O
{	O
z1	O
+	O
...	O
+	O
zk	O
<	O
~	O
,	O
zo	O
=	O
i	O
}	O
(	O
where	O
zo	O
,	O
...	O
,	O
zk	O
are	O
i.i.d	O
.	O
bernoulli	O
(	O
1j	O
(	O
x	O
»	O
random	O
variables	O
)	O
,	O
which	O
leads	O
directly	O
to	O
the	O
sought	O
result	O
.	O
0	O
several	O
representations	O
of	O
lknn	O
will	O
be	O
useful	O
for	O
later	O
analysis	O
.	O
for	O
example	O
,	O
we	O
have	O
lknn	O
=	O
e	O
{	O
ry	O
(	O
x	O
)	O
p	O
{	O
binomial	B
(	O
k	O
,	O
~	O
(	O
x	O
»	O
<	O
~	O
i	O
x	O
}	O
}	O
+	O
e	O
{	O
(	O
l	O
-	O
ry	O
(	O
x	O
)	O
)	O
p	O
{	O
binomial	B
(	O
k	O
,	O
ry	O
(	O
x	O
»	O
>	O
~	O
i	O
x	O
}	O
}	O
e	O
{	O
min	O
(	O
1j	O
(	O
x	O
)	O
,	O
1	O
-	O
1j	O
(	O
x	O
»	O
}	O
+	O
e	O
{	O
(	O
1	O
-	O
2rnin	O
(	O
~	O
(	O
x	O
)	O
,	O
1	O
-	O
ry	O
(	O
x	O
»	O
)	O
p	O
{	O
binomial	B
(	O
k	O
,	O
ry	O
(	O
x	O
»	O
>	O
~	O
i	O
x	O
}	O
}	O
.	O
it	O
should	O
be	O
stressed	O
that	O
the	O
limit	O
result	O
in	O
theorem	O
5.2	O
is	O
distribution-free	O
.	O
the	O
limit	O
lknn	O
depends	O
upon	O
1j	O
(	O
x	O
)	O
(	O
or	O
min	O
(	O
1j	O
(	O
x	O
)	O
,	O
1	O
-	O
1j	O
(	O
x	O
»	O
)	O
only	O
.	O
the	O
continuity	O
or	O
lack	O
of	O
smoothness	O
of	O
1j	O
is	O
immaterial-it	O
only	O
matters	O
for	O
the	O
speed	O
with	O
which	O
e	O
{	O
l	O
n	O
}	O
approaches	O
the	O
limit	O
l	O
knn	O
.	O
5.5	O
the	O
asymptotic	O
error	O
probability	O
of	O
weighted	O
nearest	B
neighbor	I
rules	I
following	O
royall	O
(	O
1966	O
)	O
,	O
a	O
weighted	O
nearest	B
neighbor	I
rule	I
with	O
weights	O
wi	O
,	O
...	O
,	O
wk	O
makes	O
a	O
decision	O
according	O
to	O
gn	O
(	O
x	O
)	O
=	O
{	O
if	O
~~	O
1	O
o	O
otherwise	O
.	O
l	O
...	O
-l	O
:	O
~ci	O
)	O
(	O
x	O
)	O
=i	O
w·	O
>	O
~~	O
l	O
l	O
...	O
-l	O
:	O
yci	O
)	O
(	O
x	O
)	O
=o	O
w·	O
l	O
in	O
case	O
of	O
a	O
voting	O
tie	O
,	O
this	O
rule	B
is	O
not	O
symnletric	O
.	O
we	O
may	O
modify	O
it	O
so	O
that	O
gn	O
(	O
x	O
)	O
d	O
;	O
t	O
-1	O
if	O
we	O
have	O
a	O
voting	O
tie	O
.	O
the	O
``	O
-1	O
''	O
should	O
be	O
considered	O
as	O
an	O
indecision	O
.	O
by	O
72	O
5.	O
nearest	B
neighbor	I
rules	I
previous	O
arguments	O
the	O
asymptotic	O
probability	O
of	O
error	O
is	O
a	O
function	O
of	O
wi	O
,	O
...	O
,	O
wk	O
given	O
by	O
l	O
(	O
wl	O
,	O
..	O
``	O
wk	O
)	O
=	O
e	O
{	O
a	O
(	O
1j	O
(	O
x	O
»	O
}	O
,	O
where	O
a	O
(	O
p	O
)	O
=	O
p	O
{	O
t	O
wiy/	O
>	O
t	O
w	O
;	O
(	O
l	O
-	O
y	O
;	O
)	O
}	O
(	O
1-	O
p	O
)	O
+p	O
{	O
t	O
wi	O
y/	O
:	O
:	O
:	O
t	O
w	O
;	O
(	O
l-	O
y/	O
)	O
}	O
p	O
,	O
where	O
now	O
y	O
{	O
,	O
...	O
,	O
y~	O
are	O
i.i.d	O
.	O
bernoulli	O
(	O
p	O
)	O
.	O
equivalently	O
,	O
with	O
zi	O
=	O
2y	O
!	O
-	O
1	O
e	O
{	O
-1,1	O
}	O
,	O
a	O
(	O
p	O
)	O
=	O
(	O
1	O
-	O
p	O
)	O
p	O
{	O
t	O
wizi	O
>	O
o	O
}	O
+	O
pp	O
{	O
t	O
wizi	O
:	O
:	O
:	O
o	O
}	O
.	O
assume	O
that	O
p	O
{	O
l~=l	O
wi	O
zi	O
=	O
o	O
}	O
=	O
0	O
for	O
now	O
.	O
then	O
,	O
if	O
p	O
<	O
1/2	O
,	O
a	O
(	O
p	O
)	O
=	O
p	O
+	O
(	O
l	O
-	O
2p	O
)	O
p	O
{	O
t	O
wizi	O
>	O
o	O
}	O
.	O
and	O
an	O
antisymmetric	O
expression	B
is	O
valid	O
when	O
p	O
>	O
1/2	O
.	O
note	O
next	O
the	O
following	O
.	O
ifweletnz	O
be	O
the	O
number	O
of	O
vectors	O
z	O
=	O
(	O
zl	O
,	O
''	O
''	O
zk	O
)	O
e	O
{	O
-i	O
,	O
l	O
}	O
k	O
with	O
l	O
i	O
{	O
zi=l	O
}	O
=	O
i	O
and	O
l	O
wizi	O
>	O
0	O
,	O
then	O
nz	O
+	O
nk-z	O
=	O
e	O
)	O
.	O
thus	O
,	O
k	O
=	O
l	O
nz/	O
(	O
1	O
-	O
p	O
)	O
k-z	O
=	O
l	O
(	O
k	O
)	O
pk-z	O
(	O
1_	O
pi	O
+	O
l	O
nz	O
(	O
/	O
(	O
1	O
-	O
pl-l	O
-	O
pk-i	O
(	O
1_	O
pi	O
)	O
z=o	O
l	O
<	O
kj2	O
i	O
l	O
<	O
kj2	O
1	O
(	O
k	O
)	O
kj2	O
+	O
2	O
k/2	O
p	O
i	O
+	O
i	O
i	O
+	O
i	O
i	O
i	O
.	O
(	O
1	O
-	O
p	O
)	O
kj2	O
i	O
{	O
k	O
even	O
}	O
=	O
note	O
that	O
i	O
+	O
i	O
i	O
i	O
does	O
not	O
depend	O
on	O
the	O
vector	O
of	O
weights	O
,	O
and	O
represents	O
p	O
{	O
binomial	B
(	O
k	O
,	O
l	O
-	O
p	O
)	O
:	O
's	O
k/2	O
}	O
=	O
p	O
{	O
binomial	B
(	O
k	O
,	O
p	O
)	O
~	O
k/2	O
}	O
.	O
finally	O
,	O
since	O
p	O
:	O
's	O
1/2	O
,	O
i	O
i	O
=	O
l	O
nz	O
(	O
/	O
(	O
1	O
-	O
pl-z	O
-	O
pk-l	O
(	O
1	O
pi	O
)	O
l	O
nzpl	O
(	O
1	O
-	O
pi	O
(	O
(	O
1-	O
pl-21	O
-	O
pk-21	O
)	O
l	O
<	O
kj2	O
l	O
<	O
kj2	O
~	O
o	O
.	O
5.5	O
the	O
asymptotic	O
error	O
probability	O
of	O
weighted	O
nearest	B
neighbor	I
rules	I
73	O
this	O
term	O
is	O
zero	O
if	O
and	O
only	O
if	O
nz	O
=	O
0	O
for	O
alii	O
<	O
k12	O
.	O
in	O
other	O
words	O
,	O
it	O
vanishes	O
if	O
and	O
only	O
if	O
no	O
numerical	O
minority	O
of	O
wi	O
's	O
can	O
sum	O
to	O
a	O
majority	O
(	O
as	O
in	O
the	O
case	O
(	O
0.7,0.2,0.1	O
)	O
,	O
where	O
0.7	O
alone	O
,	O
a	O
numerical	O
minority	O
,	O
outweighs	O
the	O
others	O
)	O
.	O
but	O
such	O
cases	O
are	O
equivalent	O
to	O
ordinaryk-nearest	O
neighbor	O
rules	O
if	O
k	O
is	O
odd	O
.	O
when	O
k	O
is	O
even	O
,	O
and	O
we	O
add	O
a	O
tiny	O
weight	O
to	O
one	O
wi	O
,	O
as	O
in	O
1	O
+	O
e	O
,	O
1	O
-	O
e/	O
(	O
k	O
-	O
1	O
)	O
,	O
...	O
,	O
1	O
-	O
e/	O
(	O
k	O
-	O
1	O
»	O
)	O
,	O
k	O
k	O
k	O
(	O
for	O
small	O
e	O
>	O
0	O
,	O
then	O
no	O
numerical	O
minority	O
can	O
win	O
either	O
,	O
and	O
we	O
have	O
an	O
optimal	O
rule	B
(	O
l	O
i	O
=	O
0	O
)	O
.	O
we	O
have	O
thus	O
shown	O
the	O
following	O
:	O
theorem	B
5.3	O
.	O
(	O
bailey	O
and	O
jain	O
(	O
1978	O
»	O
.	O
let	O
l	O
(	O
wi	O
,	O
...	O
,	O
wk	O
)	O
be	O
the	O
asymptotic	O
probability	O
of	O
error	O
of	O
the	O
weighted	B
k-nn	O
rule	B
with	O
weights	O
wi	O
,	O
...	O
,	O
wk	O
.	O
let	O
the	O
k-nn	O
rule	B
be	O
defined	O
by	O
(	O
11	O
k	O
,	O
ii	O
k	O
,	O
...	O
,	O
ii	O
k	O
)	O
if	O
k	O
is	O
odd	O
,	O
and	O
by	O
(	O
11	O
k	O
,	O
ii	O
k	O
,	O
...	O
,	O
ii	O
k	O
)	O
+	O
e	O
(	O
1	O
,	O
-1/	O
(	O
k	O
-	O
1	O
)	O
,	O
-1/	O
(	O
k	O
-	O
1	O
)	O
,	O
...	O
,	O
-1/	O
(	O
k	O
-	O
1	O
»	O
for	O
0	O
<	O
e	O
<	O
1	O
i	O
k	O
when	O
k	O
is	O
even	O
.	O
denoting	O
the	O
asymptotic	O
probability	O
of	O
error	O
by	O
lknn	O
for	O
the	O
latter	O
rule	B
,	O
we	O
have	O
if	O
p	O
{	O
17	O
(	O
x	O
)	O
=	O
1/2	O
}	O
<	O
1	O
,	O
then	O
equality	O
occurs	O
ifand	O
only	O
if	O
every	O
numerical	O
minority	O
of	O
the	O
wi	O
's	O
carries	O
less	O
than	O
half	O
of	O
the	O
total	O
weight	O
.	O
the	O
result	O
states	O
that	O
standard	B
k-nearest	O
neighbor	O
rules	O
are	O
to	O
be	O
preferred	O
in	O
an	O
asymptotic	O
sense	O
.	O
this	O
does	O
not	O
mean	O
that	O
for	O
a	O
particular	O
sample	O
size	O
,	O
one	O
should	O
steer	O
clear	O
of	O
nonuniform	O
weights	O
.	O
in	O
fact	O
,	O
if	O
k	O
is	O
allowed	O
to	O
vary	O
with	O
n	O
,	O
then	O
nonuniform	O
weights	O
are	O
advantageous	O
(	O
royall	O
(	O
1966	O
»	O
.	O
consider	O
the	O
space	O
wofall	O
weight	O
vectors	O
(	O
wi	O
,	O
...	O
,	O
wk	O
)	O
with	O
wi	O
~	O
0	O
,	O
l~=l	O
wi	O
=	O
1.	O
is	O
it	O
totally	O
ordered	B
with	O
respect	O
to	O
l	O
(	O
wi	O
,	O
...	O
,	O
wk	O
)	O
or	O
not	O
?	O
to	O
answer	O
this	O
ques	O
(	O
cid:173	O
)	O
tion	O
,	O
we	O
must	O
return	O
to	O
a	O
(	O
p	O
)	O
once	O
again	O
.	O
the	O
weight	O
vector	O
only	O
influences	O
the	O
term	O
i	O
i	O
given	O
there	O
.	O
consider	O
,	O
for	O
example	O
,	O
the	O
weight	O
vectors	O
(	O
0.3,0.22,0.13,0.12,0.071,0.071,0.071,0.017	O
)	O
and	O
(	O
0.26,0.26,0.13,0.12,0.071,0.071,0.071,0.017	O
)	O
.	O
numerical	O
minorities	O
are	O
made	O
up	O
of	O
one	O
,	O
two	O
,	O
or	O
three	O
components	O
.	O
for	O
both	O
weight	O
vectors	O
,	O
ni	O
=	O
0	O
,	O
n2	O
=	O
1.	O
however	O
,	O
n3	O
=	O
6	O
+	O
4	O
in	O
the	O
former	O
case	O
,	O
and	O
n3	O
=	O
6	O
+	O
2	O
in	O
the	O
latter	O
.	O
thus	O
,	O
the	O
``	O
i	O
i	O
''	O
term	O
is	O
uniformly	O
smaller	O
over	O
all	O
p	O
<	O
1/2	O
in	O
the	O
latter	O
case	O
,	O
and	O
we	O
see	O
that	O
for	O
all	O
distributions	O
,	O
the	O
second	O
weight	O
vector	O
is	O
better	O
.	O
when	O
the	O
nt	O
's	O
are	O
not	O
strictly	O
nested	O
,	O
such	O
a	O
universal	O
comparison	O
becomes	O
impossible	O
,	O
as	O
in	O
the	O
example	O
of	O
problem	O
5.8.	O
hence	O
,	O
w	O
is	O
only	O
partially	O
ordered	B
.	O
unwittingly	O
,	O
we	O
have	O
also	O
shown	O
the	O
following	O
theorem	B
:	O
theorem	B
5.4.	O
for	O
all	O
distributions	O
,	O
l	O
*	O
:	O
:	O
:	O
:	O
;	O
...	O
:	O
:	O
:	O
:	O
;	O
l	O
c2k+i	O
)	O
nn	O
:	O
:	O
:	O
:	O
;	O
l	O
c2k-l	O
)	O
nn	O
:	O
:	O
:	O
:	O
;	O
...	O
:	O
:	O
:	O
:	O
;	O
l3nn	O
:	O
:	O
:	O
:	O
;	O
lnn	O
:	O
:	O
:	O
:	O
;	O
2l	O
*	O
.	O
74	O
5.	O
nearest	B
neighbor	I
rules	I
proof	O
.	O
it	O
suffices	O
once	O
again	O
to	O
look	O
at	O
a	O
(	O
p	O
)	O
.	O
consider	O
the	O
weight	O
vector	O
wi	O
==	O
...	O
=	O
w2k+l	O
=	O
1	O
(	O
ignoring	O
normalization	O
)	O
as	O
for	O
the	O
(	O
2k	O
+	O
i	O
)	O
-nn	O
rule	B
.	O
the	O
term	O
i	O
i	O
is	O
zero	O
,	O
as	O
no	O
=	O
nl	O
=	O
..	O
,	O
=	O
nk	O
==	O
0.	O
however	O
,	O
the	O
(	O
2k	O
-	O
l	O
)	O
-nn	O
rule	B
with	O
vector	O
wi	O
=	O
...	O
=	O
w2k-l	O
=	O
1	O
,	O
w2k	O
=	O
w2k+l	O
=	O
0	O
,	O
has	O
a	O
nonzero	O
term	O
i	O
i	O
,	O
because	O
no	O
=	O
...	O
=	O
nk-	O
1	O
=	O
0	O
,	O
yet	O
nk	O
=	O
ek	O
;	O
l	O
)	O
>	O
0.	O
hence	O
,	O
l	O
c2k+1	O
)	O
nn	O
~	O
l	O
c2k-i	O
)	O
nn	O
'	O
0	O
remark	O
.	O
we	O
have	O
strict	O
inequality	B
l	O
(	O
2k+l	O
)	O
nn	O
<	O
l	O
(	O
2k-l	O
)	O
nn	O
whenever	O
p	O
{	O
1j	O
(	O
x	O
)	O
rj	O
.	O
{	O
a	O
,	O
1	O
,	O
1/2	O
}	O
}	O
>	O
0.	O
when	O
l	O
*	O
=	O
0	O
,	O
we	O
have	O
lnn	O
=	O
l3nn	O
=	O
lsnn	O
=	O
...	O
=	O
°	O
as	O
well	O
.	O
0	O
5.6	O
k-nearest	O
neighbor	O
rules	O
:	O
even	O
k	O
until	O
now	O
we	O
assumed	O
throughout	O
that	O
k	O
was	O
odd	O
,	O
so	O
that	O
voting	O
ties	O
were	O
avoided	O
.	O
the	O
tie-breaking	O
procedure	O
we	O
follow	O
forthe	O
2k-nearest	O
neighbor	B
rule	I
is	O
as	O
follows	O
:	O
gn	O
(	O
x	O
)	O
=	O
°	O
i	O
{	O
ycl	O
)	O
(	O
x	O
)	O
if	O
:	O
l	O
;	O
:	O
l	O
yci	O
)	O
(	O
x	O
)	O
>	O
k	O
if	O
:	O
l	O
;	O
:1	O
y	O
(	O
i	O
)	O
(	O
x	O
)	O
<	O
k	O
if	O
:	O
l	O
;	O
:1	O
yci	O
)	O
(	O
x	O
)	O
=	O
k.	O
formally	O
,	O
this	O
is	O
equivalent	O
to	O
a	O
weighted	O
2k-nearest	O
neighbor	B
rule	I
with	O
weight	O
vector	O
(	O
3	O
,	O
2	O
,	O
2	O
,	O
2	O
,	O
...	O
,	O
2	O
,	O
2	O
)	O
.	O
it	O
is	O
easy	O
to	O
check	O
from	O
theorem	B
5.3	O
that	O
this	O
is	O
the	O
asymptotically	O
best	O
weight	O
vector	O
.	O
even	O
values	O
do	O
not	O
decrease	O
the	O
probability	O
of	O
error	O
.	O
in	O
particular	O
,	O
we	O
have	O
the	O
following	O
:	O
theorem	B
5.5	O
.	O
(	O
devijver	O
(	O
1978	O
)	O
)	O
.	O
for	O
all	O
distributions	O
,	O
and	O
all	O
integers	O
k	O
,	O
l	O
(	O
2k-l	O
)	O
nn	O
=	O
l	O
c2k	O
)	O
nn	O
'	O
proof	O
.	O
recall	O
that	O
lknn	O
may	O
be	O
written	O
in	O
the	O
form	O
lknn	O
=	O
e	O
{	O
a	O
(	O
1j	O
(	O
x	O
)	O
)	O
}	O
,	O
where	O
a	O
(	O
1j	O
(	O
x	O
)	O
)	O
=	O
lim	O
p	O
{	O
g~k	O
)	O
(	O
x	O
)	O
=i	O
yix	O
=	O
x	O
}	O
n	O
--	O
-+oo	O
is	O
the	O
pointwise	O
asymptotic	O
error	O
probability	O
of	O
the	O
k-nn	O
rule	B
g~k	O
)	O
.	O
it	O
is	O
convenient	O
to	O
consider	O
zl	O
,	O
...	O
,	O
z2k	O
i.i.d	O
.	O
{	O
-i	O
,	O
l	O
}	O
-valuedrandom	O
variables	O
withp	O
{	O
zi	O
=	O
i	O
}	O
=	O
p	O
=	O
1j	O
(	O
x	O
)	O
,	O
and	O
to	O
base	O
the	O
decision	O
upon	O
the	O
sign	O
of	O
:	O
l	O
;	O
:1	O
zi	O
.	O
from	O
the	O
general	O
formula	O
for	O
weighted	B
nearest	O
neighbor	O
rules	O
,	O
the	O
pointwise	O
asymptotic	O
error	O
probability	O
of	O
the	O
(	O
2k	O
)	O
-nn	O
rule	B
is	O
n	O
--	O
-+oo	O
n	O
lim	O
p	O
{	O
gc2k	O
)	O
(	O
x	O
)	O
=i	O
yix	O
=	O
x	O
}	O
~	O
pp	O
{	O
tzi	O
<	O
o	O
}	O
+pp	O
{	O
tzi=o	O
,	O
zi	O
<	O
o	O
}	O
+	O
(	O
1-	O
p	O
)	O
p	O
~zi	O
>	O
°	O
+	O
(	O
1-	O
p	O
)	O
p	O
~zi	O
=0	O
,	O
zi	O
>	O
°	O
{	O
2k	O
2k	O
}	O
}	O
{	O
5.7	O
inequalities	O
for	O
the	O
probability	O
of	O
error	O
75	O
=	O
pp	O
{	O
t	O
,	O
zi	O
<	O
o	O
}	O
+	O
(	O
1-	O
p	O
)	O
p	O
{	O
t	O
,	O
zi	O
>	O
o	O
}	O
lim	O
p	O
{	O
g	O
(	O
2k-l	O
)	O
(	O
x	O
)	O
=i	O
yjx=	O
x	O
}	O
.	O
n	O
--	O
--	O
+oo	O
n	O
~	O
therefore	O
,	O
l	O
(	O
2k	O
)	O
nn	O
=	O
l	O
(	O
2k-l	O
)	O
nn	O
'	O
0	O
5.7	O
inequalities	O
for	O
the	O
probability	O
of	O
error	O
we	O
return	O
to	O
the	O
case	O
when	O
k	O
is	O
odd	O
.	O
recall	O
that	O
where	O
''	O
'k	O
(	O
p	O
)	O
=	O
rnin	O
(	O
p	O
,	O
1	O
-	O
p	O
)	O
+	O
12p	O
-	O
lip	O
{	O
binornia1	O
(	O
k	O
,	O
rnin	O
(	O
p	O
,	O
1	O
-	O
p	O
)	O
)	O
>	O
~	O
}	O
,	O
since	O
l	O
*	O
=	O
e	O
{	O
min	O
(	O
1j	O
(	O
x	O
)	O
,	O
1	O
-	O
1j	O
(	O
x	O
)	O
)	O
}	O
,	O
we	O
may	O
exploit	O
this	O
representation	O
to	O
obtain	O
a	O
variety	O
of	O
inequalities	O
on	O
lknn	O
-	O
l	O
*	O
.	O
we	O
begin	O
with	O
one	O
that	O
is	O
very	O
easy	O
to	O
prove	O
but	O
perhaps	O
not	O
the	O
strongest	O
.	O
theorem	B
5.6.	O
for	O
all	O
odd	O
k	O
and	O
all	O
distributions	O
,	O
lknn	O
:	O
s	O
l	O
+	O
~	O
.	O
*	O
1	O
'\ike	O
proof	O
.	O
by	O
the	O
above	O
representation	O
,	O
<	O
:	O
s	O
=	O
=	O
sup	O
(	O
l	O
-	O
2	O
p	O
)	O
p	O
{	O
b	O
>	O
~	O
}	O
2	O
05p51p	O
(	O
b	O
is	O
binomial	B
(	O
k	O
,	O
p	O
)	O
)	O
sup	O
(	O
l	O
_	O
2p	O
)	O
p	O
{	O
b	O
-	O
kp	O
>	O
~	O
_	O
p	O
}	O
05p51/2	O
k	O
2	O
sup	O
(	O
l	O
-	O
2p	O
)	O
e-2k	O
(	O
l/2-	O
p	O
)	O
2	O
05p5	O
1/2	O
(	O
by	O
the	O
okamoto-hoeffding	O
inequality-theorem	O
8.1	O
)	O
sup	O
ue-	O
ku2	O
05u51	O
/	O
2	O
1	O
o	O
~	O
.	O
76	O
5.	O
nearest	B
neighbor	I
rules	I
theorem	O
5.7	O
.	O
(	O
gyorfi	O
and	O
gyorfi	O
(	O
1978	O
)	O
)	O
.	O
for	O
all	O
distributions	O
and	O
all	O
odd	O
k	O
,	O
proof	O
.	O
we	O
note	O
that	O
for	O
p	O
:	O
:	O
:	O
:	O
;	O
1/2	O
,	O
with	O
b	O
binomial	B
(	O
k	O
,	O
p	O
)	O
,	O
<	O
e	O
{	O
ib	O
-	O
kpl	O
}	O
k	O
(	O
1/2	O
-	O
p	O
)	O
jvar	O
{	O
b	O
}	O
k	O
(	O
1/2	O
-	O
p	O
)	O
(	O
markov	O
's	O
inequality	B
)	O
(	O
cauchy-schwarz	O
inequality	B
)	O
=	O
2jp	O
(	O
1-	O
p	O
)	O
jk	O
(	O
1	O
-	O
2p	O
)	O
'	O
hence	O
,	O
lknn	O
-	O
l	O
'	O
:	O
:	O
:	O
e	O
{	O
~v'ry	O
(	O
x	O
)	O
(	O
l	O
-	O
ry	O
(	O
x	O
)	O
)	O
}	O
2	O
jkje	O
{	O
17	O
(	O
x	O
)	O
(	O
1	O
-	O
17	O
(	O
x	O
)	O
)	O
}	O
<	O
(	O
jensen	O
's	O
inequality	B
)	O
=	O
~jl	O
;	O
=	O
j2l	O
nn	O
0	O
k	O
.	O
remark	O
.	O
for	O
large	O
k	O
,	O
b	O
is	O
approximately	O
normal	B
(	O
k	O
,	O
p	O
(	O
1	O
-	O
p	O
)	O
)	O
,	O
and	O
thus	O
e	O
{	O
ib	O
-	O
kpl	O
}	O
~	O
jkp	O
(	O
1	O
-	O
p	O
)	O
j2/n	O
,	O
as	O
the	O
first	O
absolute	O
moment	O
of	O
a	O
normal	O
random	O
variable	B
is	O
,	O
j2tii	O
(	O
see	O
problem	O
5.11	O
)	O
.	O
working	O
this	O
through	O
yields	O
an	O
approximate	O
bound	O
of	O
j	O
lnn	O
/	O
(	O
n	O
k	O
)	O
.	O
the	O
bound	O
is	O
proportional	O
to	O
-jl	O
;	O
;	O
.	O
this	O
can	O
be	O
improved	O
to	O
l	O
*	O
if	O
,	O
instead	O
of	O
bounding	O
it	O
from	O
above	O
by	O
markov	O
's	O
inequality	B
,	O
we	O
directly	O
approximate	O
p	O
{	O
b	O
-	O
kp	O
>	O
k	O
(	O
1/2	O
-	O
p	O
)	O
}	O
as	O
shown	O
below	O
.	O
0	O
theorem	B
5.8	O
.	O
(	O
devroye	O
(	O
l981b	O
)	O
)	O
.	O
for	O
all	O
distributions	O
and	O
k	O
:	O
:	O
:	O
:	O
3	O
odd	O
,	O
where	O
y	O
=	O
supr	O
>	O
o	O
2rp	O
{	O
n	O
>	O
r	O
}	O
=	O
0.33994241	O
...	O
,	O
n	O
is	O
normal	B
(	O
0	O
,	O
1	O
)	O
,	O
and	O
0	O
(	O
-	O
)	O
refers	O
to	O
k	O
...	O
..-+	O
00	O
.	O
(	O
explicit	O
constants	O
are	O
given	O
in	O
the	O
proof	O
.	O
)	O
5.7	O
inequalities	O
for	O
the	O
probability	O
of	O
error	O
77	O
the	O
constant	O
y	O
in	O
the	O
proof	O
can	O
not	O
be	O
improved	O
.	O
a	O
slightly	O
weaker	O
bound	O
was	O
obtained	O
by	O
devijver	O
(	O
1979	O
)	O
:	O
lknn	O
:	O
:	O
:	O
s	O
lnn	O
(	O
where	O
e	O
=	O
rk/2l	O
)	O
l	O
*	O
+	O
22k	O
'	O
k	O
'	O
1	O
(	O
2k	O
'	O
)	O
.j	O
!	O
;	O
,	O
nk	O
'	O
l	O
'	O
''	O
+	O
lnn	O
-	O
(	O
1	O
+	O
0	O
(	O
1	O
)	O
)	O
(	O
as	O
k	O
--	O
+	O
00	O
,	O
see	O
lemma	O
a.3	O
)	O
.	O
see	O
also	O
devijver	O
and	O
kittler	O
(	O
1982	O
,	O
p.l02	O
)	O
.	O
lemma	O
5.6	O
.	O
(	O
devroye	O
(	O
1981b	O
)	O
)	O
.	O
for	O
p	O
:	O
:	O
:	O
s	O
1/2	O
and	O
with	O
k	O
>	O
3	O
odd	O
,	O
p	O
{	O
binomial	B
(	O
k	O
,	O
p	O
)	O
>	O
~	O
}	O
k	O
!	O
{	O
p	O
(	O
k-l	O
)	O
/2	O
(	O
k	O
;	O
l	O
)	O
!	O
(	O
k	O
;	O
l	O
)	O
!	O
jo	O
(	O
x	O
(	O
1-x	O
)	O
)	O
dx	O
<	O
a	O
(	O
~	O
e-z2	O
/	O
2dz	O
j	O
(	O
l-2p	O
)	O
~	O
,	O
proof	O
.	O
consider	O
k	O
i.i.d	O
.	O
uniform	B
random	O
variables	O
on	O
[	O
0	O
,	O
1	O
]	O
.	O
the	O
number	O
of	O
values	O
in	O
[	O
0	O
,	O
p	O
]	O
is	O
binomial	B
(	O
k	O
,	O
p	O
)	O
.	O
the	O
number	O
exceeds	O
k/2	O
if	O
and	O
only	O
ifthe	O
(	O
k	O
+	O
l	O
)	O
/2-th	O
order	O
statistic	O
of	O
the	O
uniformc1oudis	O
at	O
most	O
p.	O
the	O
latter	O
is	O
beta	B
(	O
(	O
k+	O
1	O
)	O
/2	O
,	O
(	O
k+	O
1	O
)	O
/2	O
)	O
distributed	O
,	O
explaining	O
the	O
first	O
equality	O
(	O
problem	O
5.32	O
)	O
.	O
note	O
that	O
we	O
have	O
written	O
a	O
discrete	O
sum	O
as	O
an	O
integral-in	O
some	O
cases	O
,	O
such	O
tricks	O
payoff	O
handsome	O
rewards	O
.	O
to	O
show	O
the	O
inequality	B
,	O
replace	O
x	O
by	O
~	O
(	O
1	O
-	O
a=r	O
)	O
and	O
use	O
the	O
inequality	B
1	O
u	O
:	O
:	O
:	O
s	O
e-u	O
to	O
obtain	O
a	O
bound	O
as	O
shown	O
with	O
finally	O
,	O
a	O
p	O
{	O
b	O
__	O
k+l	O
}	O
k+l	O
2	O
2~	O
(	O
b	O
is	O
binomial	B
(	O
k	O
,	O
1/2	O
)	O
)	O
<	O
<	O
k	O
k+	O
1	O
2n	O
k+l	O
k-l	O
2	O
'k=l	O
y	O
l	O
(	O
,	O
-1	O
2	O
2	O
(	O
problem	O
5.17	O
)	O
(	O
problem	O
5.18	O
)	O
.	O
0	O
78	O
5.	O
nearest	B
neighbor	I
rules	I
proof	O
of	O
theorem	O
5.8.	O
from	O
earlier	O
remarks	O
,	O
lknn	O
-	O
l	O
*	O
=	O
e	O
{	O
ak	O
(	O
17	O
(	O
x	O
)	O
)	O
-	O
min	O
(	O
17	O
(	O
x	O
)	O
,	O
1	O
-	O
17	O
(	O
x	O
)	O
)	O
}	O
=	O
e	O
{	O
cnin	O
(	O
ry~	O
;	O
~~\x~	O
ry	O
(	O
x	O
)	O
)	O
-	O
1	O
)	O
min	O
(	O
ry	O
(	O
x	O
)	O
,	O
1	O
-	O
ry	O
(	O
x	O
)	O
)	O
}	O
1-2p	O
{	O
k	O
}	O
)	O
sup	O
-	O
-p	O
b	O
>	O
-	O
2	O
(	O
o	O
<	O
p	O
<	O
l/2	O
p	O
l	O
*	O
(	O
b	O
is	O
binomial	B
(	O
k	O
,	O
p	O
)	O
)	O
.	O
we	O
merely	O
bound	O
the	O
factor	O
in	O
brackets	O
.	O
clearly	O
,	O
by	O
lemma	O
5.6	O
,	O
lknn	O
-	O
l	O
*	O
:	O
:	O
:	O
;	O
l	O
*	O
(	O
sup	O
1	O
-	O
2p	O
a	O
i-jk-l	O
o	O
<	O
p	O
<	O
1/2	O
p	O
(	O
1-2p	O
)	O
-jk-l	O
e-z2	O
/	O
2dz	O
)	O
.	O
take	O
a	O
<	O
1	O
as	O
the	O
solution	O
of	O
(	O
3j	O
(	O
ea2	O
)	O
)	O
3/2	O
~	O
=	O
y	O
,	O
which	O
is	O
possible	O
if	O
k	O
-	O
1	O
>	O
2~	O
?	O
n	O
)	O
6	O
=	O
2.4886858	O
...	O
.	O
setting	O
v	O
=	O
(	O
l	O
-	O
2	O
p	O
)	O
vt=1	O
,	O
we	O
have	O
1	O
-	O
2p	O
i-jk-l	O
sup	O
-	O
-	O
o	O
<	O
p	O
<	O
l/2	O
p	O
2	O
e-z2	O
--	O
dz	O
/	O
:	O
:	O
:	O
;	O
max	O
(	O
l-2p	O
)	O
v'k=t	O
.j2ii	O
sup	O
2vjvt=1	O
100	O
e-z2	O
2	O
--	O
dz	O
,	O
.j2ii	O
2	O
)	O
-	O
-	O
a-jk-l	O
sv	O
<	O
v'k=t	O
1	O
-	O
v	O
j	O
vt=1	O
.j2ii	O
o	O
<	O
vsa-jk-l	O
1	O
-	O
v	O
j	O
vt=1	O
v	O
(	O
2pvt=1	O
e-	O
v2	O
sup	O
/	O
/	O
(	O
1	O
-	O
a	O
)	O
vt=1	O
:	O
:	O
:	O
;	O
max	O
(	O
:	O
:	O
:	O
;	O
max	O
(	O
y	O
y	O
vt=1	O
-a	O
2	O
(	O
k-l	O
)	O
/2	O
)	O
,	O
--	O
-e	O
.j2ii	O
(	O
3	O
)	O
3/2	O
1	O
)	O
l	O
)	O
.j2ii	O
(	O
1	O
-	O
a	O
)	O
vt=1	O
'	O
ea2	O
(	O
k	O
-	O
(	O
use	O
u3/2e-cu	O
:	O
:	O
:	O
;	O
(	O
3j	O
(	O
2ce	O
)	O
)	O
3/2	O
for	O
all	O
u	O
>	O
0	O
)	O
=	O
y	O
(	O
1-	O
a	O
)	O
vt=1	O
'	O
collect	O
all	O
bounds	O
and	O
note	O
that	O
a	O
=	O
0	O
(	O
k-l/6	O
)	O
.	O
0	O
5.8	O
behavior	O
when	O
l	O
*	O
is	O
small	O
in	O
this	O
section	O
,	O
we	O
look	O
more	O
closely	O
at	O
lknn	O
when	O
l	O
*	O
is	O
small	O
.	O
recalling	O
that	O
lknn	O
=	O
e	O
{	O
ak	O
(	O
17	O
(	O
x	O
)	O
)	O
}	O
with	O
''	O
,	O
,	O
(	O
p	O
)	O
=	O
min	O
(	O
p	O
,	O
1-p	O
)	O
+11-2min	O
(	O
p	O
,	O
1-p	O
)	O
ip	O
{	O
binomial	B
(	O
k	O
,	O
min	O
(	O
p	O
,	O
1-	O
p	O
)	O
)	O
>	O
~	O
}	O
for	O
odd	O
k	O
,	O
it	O
is	O
easily	O
seen	O
that	O
lknn	O
=	O
e	O
{	O
~k	O
(	O
min	O
(	O
17	O
(	O
x	O
)	O
,	O
1	O
function	O
~k	O
.	O
because	O
17	O
(	O
x	O
)	O
)	O
)	O
}	O
for	O
some	O
5.8	O
behavior	O
when	O
l	O
*	O
is	O
small	O
79	O
.	O
mlll	O
(	O
p	O
,	O
1	O
-	O
p	O
)	O
=	O
...	O
.	O
1	O
-	O
,	O
jl	O
-	O
4p	O
(	O
l	O
-	O
p	O
)	O
'	O
2	O
we	O
also	O
have	O
lknn	O
=	O
e	O
{	O
lj	O
!	O
k	O
(	O
17	O
(	O
x	O
)	O
(	O
l	O
-	O
17	O
(	O
x	O
)	O
)	O
)	O
}	O
for	O
some	O
other	O
function	O
ljfk	O
.	O
worked	O
(	O
cid:173	O
)	O
out	O
forms	O
of	O
l	O
knn	O
include	O
j	O
>	O
kj2	O
+	O
l	O
e	O
)	O
~	O
(	O
x	O
)	O
j	O
(	O
l-	O
~	O
(	O
x	O
»	O
k-j+l	O
}	O
l	O
e	O
)	O
e	O
{	O
(	O
17	O
(	O
x	O
)	O
(	O
l	O
-	O
17	O
(	O
x	O
)	O
)	O
)	O
j+1	O
(	O
(	O
1	O
-	O
17	O
(	O
x	O
)	O
)	O
k-2j	O
-1	O
+	O
17	O
(	O
x/-2j-1	O
)	O
}	O
.	O
]	O
j	O
<	O
kj2	O
]	O
as	O
pa	O
+	O
(	O
1	O
-	O
p	O
)	O
a	O
is	O
a	O
function	O
of	O
p	O
(	O
1	O
-	O
p	O
)	O
for	O
integer	O
a	O
,	O
this	O
may	O
be	O
further	O
reduced	O
to	O
simplified	O
forms	O
such	O
as	O
lnn	O
e	O
{	O
217	O
(	O
x	O
)	O
(	O
l	O
-	O
17	O
(	O
x	O
)	O
)	O
}	O
,	O
l3nn	O
=	O
e	O
{	O
17	O
(	O
x	O
)	O
(	O
1	O
-	O
17	O
(	O
x	O
)	O
)	O
}	O
+	O
4e	O
{	O
(	O
17	O
(	O
x	O
)	O
(	O
1	O
-	O
17	O
(	O
x	O
)	O
)	O
)	O
2	O
}	O
,	O
lsnn	O
=	O
e	O
{	O
17	O
(	O
x	O
)	O
(	O
l	O
-	O
17	O
(	O
x	O
)	O
)	O
}	O
+	O
e	O
{	O
(	O
17	O
(	O
x	O
)	O
(	O
1	O
-	O
17	O
(	O
x	O
)	O
)	O
)	O
2	O
}	O
+	O
12e	O
{	O
(	O
17	O
(	O
x	O
)	O
(	O
l	O
-	O
17	O
(	O
x	O
)	O
)	O
)	O
3	O
}	O
.	O
the	O
behavior	O
of	O
ak	O
near	O
zero	O
is	O
very	O
informative	O
.	O
as	O
p	O
+	O
0	O
,	O
we	O
have	O
a1	O
(	O
p	O
)	O
=	O
2p	O
(	O
l	O
-	O
p	O
)	O
'	O
''	O
'-	O
'	O
2p	O
,	O
a3	O
(	O
p	O
)	O
as	O
(	O
p	O
)	O
p	O
(	O
l	O
-	O
p	O
)	O
(	O
1	O
+	O
4p	O
)	O
'	O
''	O
'-	O
'	O
p	O
+	O
3p2	O
,	O
p+l0p3	O
,	O
while	O
for	O
the	O
bayes	O
error	O
,	O
l	O
*	O
=	O
e	O
{	O
rnin	O
(	O
17	O
(	O
x	O
)	O
,	O
1	O
-	O
17	O
(	O
x	O
)	O
)	O
}	O
=	O
e	O
{	O
aoo	O
(	O
17	O
(	O
x	O
)	O
)	O
}	O
,	O
where	O
a	O
oo	O
=	O
rnin	O
(	O
p	O
,	O
1	O
-	O
p	O
)	O
aj	O
pas	O
p	O
+	O
o.	O
assume	O
that	O
17	O
(	O
x	O
)	O
=	O
p	O
at	O
all	O
x.	O
then	O
,	O
as	O
p	O
+	O
0	O
,	O
lnn	O
aj	O
2l	O
*	O
and	O
l3nn	O
aj	O
l	O
*	O
.	O
moreover	O
,	O
lnn	O
-	O
l	O
*	O
aj	O
l	O
*	O
,	O
l3nn	O
-	O
l	O
*	O
aj	O
3l	O
*2.	O
assume	O
that	O
l	O
*	O
=	O
p	O
=	O
0.01.	O
then	O
l1	O
~	O
0.02	O
,	O
whereas	O
l3nn	O
-	O
l	O
*	O
~	O
0.0003.	O
for	O
all	O
practical	O
purposes	O
,	O
the	O
3-nn	O
rule	B
is	O
virtually	O
perfect	O
.	O
for	O
this	O
reason	O
,	O
the	O
3-nn	O
rule	B
is	O
highly	O
recommended	O
.	O
little	O
is	O
gained	O
by	O
considering	O
the	O
5-nn	O
rule	B
when	O
p	O
is	O
small	O
,	O
as	O
lsnn	O
-	O
l	O
*	O
~	O
0.00001.	O
let	O
ak	O
be	O
the	O
smallest	O
number	O
such	O
that	O
ak	O
(	O
p	O
)	O
:	O
:s	O
ak	O
min	O
(	O
p	O
,	O
1	O
p	O
)	O
for	O
all	O
p	O
(	O
the	O
tangents	O
in	O
figure	O
5.6	O
)	O
.	O
then	O
80	O
5.	O
nearest	B
neighbor	I
rules	I
this	O
is	O
precisely	O
at	O
the	O
basis	O
of	O
the	O
inequalities	O
of	O
theorems	O
5.6	O
through	O
5.8	O
,	O
where	O
it	O
was	O
shown	O
that	O
ak	O
=	O
i	O
+	O
0	O
(	O
1/	O
-jk	O
)	O
.	O
figure	O
5.6.	O
ak	O
(	O
p	O
)	O
as	O
a	O
function	O
ofp·	O
1/2	O
1/2	O
and	O
thus	O
,	O
the	O
classes	O
are	O
separated	O
.	O
this	O
does	O
not	O
imply	O
that	O
the	O
support	B
of	O
x	O
given	O
for	O
every	O
fixed	O
k	O
,	O
the	O
k-nearest	O
neighbor	B
rule	I
is	O
consistent	O
.	O
cover	O
has	O
a	O
beautiful	O
5.9	O
nearest	B
neighbor	I
rules	I
when	O
l	O
*	O
=	O
0	O
from	O
theorem	B
5.4	O
we	O
retain	O
that	O
if	O
l	O
*	O
=	O
0	O
,	O
then	O
lknn	O
=	O
°	O
for	O
all	O
k.	O
in	O
fact	O
,	O
then	O
,	O
example	O
to	O
illustrate	O
this	O
remarkable	O
fact	O
.	O
l	O
*	O
=	O
°	O
implies	O
that	O
1j	O
(	O
x	O
)	O
e	O
{	O
o	O
,	O
i	O
}	O
for	O
all	O
x	O
,	O
y	O
=	O
°	O
is	O
different	O
from	O
the	O
support	B
of	O
x	O
given	O
y	O
=	O
1.	O
take	O
for	O
example	O
a	O
random	O
rational	O
number	O
from	O
[	O
0	O
,	O
1	O
]	O
(	O
e.g.	O
,	O
generate	O
i	O
,	O
j	O
independently	O
and	O
at	O
random	O
from	O
the	O
geometric	B
distribution	I
on	O
{	O
i	O
,	O
2	O
,	O
3	O
,	O
...	O
}	O
,	O
and	O
set	O
x	O
=	O
min	O
(	O
i	O
,	O
j	O
)	O
/	O
max	O
(	O
i	O
,	O
j	O
)	O
)	O
.	O
every	O
rational	O
number	O
on	O
[	O
0	O
,	O
1	O
]	O
has	O
positive	O
probability	O
.	O
given	O
y	O
=	O
1	O
,	O
x	O
is	O
as	O
above	O
,	O
and	O
given	O
y	O
=	O
0	O
,	O
x	O
is	O
uniform	B
on	O
[	O
0	O
,	O
1	O
]	O
.	O
let	O
p	O
{	O
y	O
=	O
i	O
}	O
=	O
p	O
{	O
y	O
=	O
o	O
}	O
=	O
1/2	O
.	O
the	O
support	B
of	O
x	O
is	O
identical	O
in	O
both	O
cases	O
.	O
as	O
1j	O
(	O
x	O
)	O
=	O
°	O
if	O
x	O
is	O
irrational	O
,	O
if	O
x	O
is	O
rational	O
i	O
{	O
we	O
see	O
that	O
l	O
*	O
=	O
°	O
and	O
that	O
the	O
nearest	B
neighbor	I
rule	I
is	O
consistent	O
.	O
if	O
someone	O
shows	O
us	O
a	O
number	O
x	O
drawn	O
from	O
the	O
same	O
distribution	B
as	O
the	O
data	O
,	O
then	O
we	O
may	O
decide	O
the	O
rationality	O
of	O
x	O
merely	O
by	O
looking	O
at	O
the	O
rationality	O
of	O
the	O
nearest	B
neighbor	I
of	O
x.	O
although	O
we	O
did	O
not	O
show	O
this	O
,	O
the	O
same	O
is	O
true	O
if	O
we	O
are	O
given	O
any	O
x	O
e	O
[	O
0	O
,	O
1	O
]	O
:	O
lim	O
p	O
{	O
x	O
is	O
rational	O
y	O
(	O
l	O
)	O
(	O
x	O
)	O
=	O
°	O
(	O
x	O
(	O
l	O
)	O
(	O
x	O
)	O
is	O
not	O
rational	O
)	O
}	O
n	O
--	O
-+oo	O
=	O
lim	O
p	O
{	O
x	O
is	O
not	O
rational	O
y	O
(	O
l	O
)	O
(	O
x	O
)	O
=	O
1	O
(	O
x	O
(	O
l	O
)	O
(	O
x	O
)	O
is	O
rational	O
)	O
}	O
n	O
--	O
-+oo	O
°	O
(	O
see	O
problem	O
5.38	O
)	O
.	O
5.11	O
the	O
(	O
k	O
,	O
i	O
)	O
-nearest	O
neighbor	B
rule	I
81	O
5.10	O
admissibility	O
of	O
the	O
nearest	O
neighbor	B
rule	I
the	O
consistency	B
theorems	O
of	O
chapter	O
11	O
show	O
us	O
that	O
we	O
should	O
take	O
k	O
=	O
k	O
n	O
~	O
00	O
in	O
the	O
k-nn	O
rule	B
.	O
the	O
decreasing	O
nature	O
ofl	O
knn	O
corroborates	O
this	O
.	O
yet	O
,	O
there	O
exist	O
distributions	O
for	O
which	O
for	O
all	O
n	O
,	O
the	O
i-nn	O
rule	B
is	O
better	O
than	O
the	O
k-nn	O
rule	B
for	O
any	O
k	O
2	O
:	O
3.	O
this	O
observation	O
,	O
due	O
to	O
cover	O
and	O
hart	O
(	O
1967	O
)	O
,	O
rests	O
on	O
the	O
following	O
class	O
of	O
examples	O
.	O
let	O
so	O
and	O
sl	O
be	O
two	O
spheres	O
of	O
radius	O
1	O
centered	O
at	O
a	O
and	O
b	O
,	O
where	O
iia	O
-	O
bll	O
>	O
2.	O
given	O
y	O
=	O
1	O
,	O
x	O
is	O
uniform	B
on	O
sl	O
,	O
while	O
given	O
y	O
=	O
0	O
,	O
x	O
is	O
uniform	B
on	O
so	O
,	O
whereas	O
p	O
{	O
y	O
=	O
i	O
}	O
=	O
p	O
{	O
y	O
=	O
o	O
}	O
=	O
1/2	O
.	O
we	O
note	O
that	O
given	O
n	O
observations	O
,	O
with	O
the	O
i-nn	O
rule	B
,	O
e	O
{	O
ln	O
}	O
=	O
p	O
{	O
y	O
=	O
0	O
,	O
y1	O
=	O
...	O
=	O
yn	O
=	O
i	O
}	O
+	O
p	O
{	O
y	O
=	O
1	O
,	O
y1	O
=	O
...	O
=	O
yn	O
=	O
o	O
}	O
=	O
-	O
.	O
1	O
2n	O
for	O
the	O
k-nn	O
rule	B
,	O
k	O
being	O
odd	O
,	O
we	O
have	O
e	O
(	O
l	O
n	O
}	O
=	O
p	O
{	O
y	O
=	O
0	O
,	O
t	O
'	O
[	O
y	O
,	O
=il	O
}	O
:	O
'0	O
lk/2j	O
}	O
+	O
p	O
{	O
y	O
=	O
1	O
,	O
t	O
'	O
[	O
y	O
,	O
=i	O
}	O
:	O
'0	O
lk/2j	O
}	O
p	O
{	O
binomial	B
(	O
n	O
,	O
1/2	O
)	O
s	O
lk/2j	O
}	O
=	O
1	O
lk/2j	O
(	O
)	O
1	O
-	O
~	O
n	O
>	O
_	O
when	O
k	O
>	O
3	O
.	O
2n	O
~	O
.	O
2n	O
-	O
j=o	O
]	O
hence	O
,	O
the	O
k-nn	O
rule	B
is	O
worse	O
than	O
the	O
i-nn	O
rule	B
for	O
every	O
n	O
when	O
the	O
distribution	B
is	O
given	O
above	O
.	O
we	O
refer	O
to	O
the	O
exercises	O
regarding	O
some	O
interesting	O
admissibility	O
questions	O
for	O
k-nn	O
rules	O
.	O
5.11	O
the	O
(	O
k	O
,	O
i	O
)	O
-nearest	O
neighbor	B
rule	I
in	O
1970	O
,	O
hellman	O
(	O
1970	O
)	O
proposed	O
the	O
(	O
k	O
,	O
i	O
)	O
-nearest	O
neighbor	B
rule	I
,	O
which	O
is	O
iden	O
(	O
cid:173	O
)	O
tical	O
to	O
the	O
k-nearest	O
neighbor	B
rule	I
,	O
but	O
refuses	O
to	O
make	O
a	O
decision	O
unless	O
at	O
least	O
i	O
>	O
k	O
/2	O
observations	O
are	O
from	O
the	O
same	O
class	O
.	O
formally	O
,	O
we	O
set	O
gn	O
(	O
x	O
)	O
=	O
°	O
if	O
l~=l	O
y	O
(	O
i	O
)	O
(	O
x	O
)	O
ski	O
if	O
l~=l	O
y	O
(	O
i	O
)	O
(	O
x	O
)	O
2	O
:	O
i	O
i	O
-1	O
otherwise	O
(	O
no	O
decision	O
)	O
.	O
{	O
define	O
the	O
pseudoprobability	O
of	O
error	O
by	O
ln	O
=	O
p	O
{	O
gn	O
(	O
x	O
)	O
~	O
{	O
-i	O
,	O
y	O
}	O
id	O
,	O
j	O
,	O
that	O
is	O
,	O
the	O
probability	O
that	O
we	O
reach	O
a	O
decision	O
and	O
correctly	O
classify	O
x.	O
clearly	O
,	O
ln	O
s	O
p	O
{	O
gn	O
(	O
x	O
)	O
=i	O
yidn	O
}	O
,	O
our	O
standard	B
probability	O
of	O
error	O
.	O
the	O
latter	O
inequality	B
82	O
5.	O
nearest	B
neighbor	I
rules	I
is	O
only	O
superficially	O
interesting	O
,	O
as	O
the	O
probability	O
of	O
not	O
reaching	O
a	O
decision	O
is	O
not	O
taken	O
into	O
account	O
in	O
ln	O
.	O
we	O
may	O
extend	O
theorem	B
5.2	O
to	O
show	O
the	O
following	O
(	O
problem	O
5.35	O
)	O
:	O
theorem	B
5.9.	O
for	O
the	O
(	O
k	O
,	O
i	O
)	O
-nearest	O
neighbor	B
rule	I
,	O
the	O
pseudoprobability	O
of	O
error	O
ln	O
satisfies	O
n-+oo	O
lim	O
e	O
{	O
l	O
n	O
}	O
=	O
e	O
{	O
17	O
(	O
x	O
)	O
p	O
{	O
binomial	B
(	O
k	O
,	O
17	O
(	O
x	O
»	O
:	O
s	O
k	O
-iix	O
}	O
+	O
(	O
1	O
-	O
17	O
(	O
x	O
»	O
p	O
{	O
binomial	B
(	O
k	O
,	O
17	O
(	O
x	O
»	O
?	O
:	O
iix	O
}	O
}	O
def	O
lk	O
,	O
l	O
'	O
the	O
above	O
result	O
is	O
distribution-free	O
.	O
note	O
that	O
the	O
k-nearest	O
neighbor	B
rule	I
for	O
odd	O
k	O
corresponds	O
to	O
lk	O
,	O
(	O
k+l	O
)	O
/2	O
.	O
the	O
limit	O
lk	O
,	O
l	O
by	O
itself	O
is	O
not	O
interesting	O
,	O
but	O
it	O
was	O
shown	O
by	O
devijver	O
(	O
1979	O
)	O
that	O
lk	O
,	O
l	O
holds	O
information	O
regarding	O
the	O
bayes	O
error	O
l*	O
.	O
theorem	B
5.10	O
.	O
(	O
devijver	O
(	O
1979	O
»	O
.	O
for	O
all	O
distributions	O
and	O
with	O
k	O
odd	O
,	O
also	O
,	O
lknn	O
+	O
l	O
k	O
,	O
fk/21+1	O
<	O
l	O
*	O
<	O
l	O
2	O
-	O
-	O
knn	O
'	O
this	O
theorem	B
(	O
for	O
which	O
we	O
refer	O
to	O
problem	O
5.34	O
)	O
shows	O
that	O
l	O
*	O
is	O
tightly	O
sandwiched	O
between	O
lknn	O
,	O
the	O
asymptotic	O
probability	O
of	O
error	O
of	O
the	O
k-nearest	O
neighbor	B
rule	I
,	O
and	O
the	O
``	O
tennis	O
''	O
rule	B
which	O
requires	O
that	O
the	O
difference	O
of	O
votes	O
between	O
the	O
two	O
classes	O
among	O
the	O
k	O
nearest	O
neighbors	O
be	O
at	O
least	O
two	O
.	O
if	O
ln	O
is	O
close	O
to	O
its	O
limit	O
,	O
and	O
if	O
we	O
can	O
estimate	B
ln	O
(	O
see	O
the	O
chapters	O
on	O
error	B
estimation	I
)	O
,	O
then	O
we	O
may	O
be	O
able	O
to	O
use	O
devijver	O
's	O
inequalities	O
to	O
obtain	O
estimates	O
of	O
the	O
bayes	O
error	O
l	O
*	O
.	O
for	O
additional	O
results	O
,	O
see	O
loizou	O
and	O
maybank	O
(	O
1987	O
)	O
.	O
as	O
a	O
corollary	O
of	O
devijver	O
's	O
inequalities	O
,	O
we	O
note	O
that	O
l	O
knn	O
we	O
have	O
_	O
l	O
*	O
<	O
lknn	O
-	O
lk	O
,	O
rk/21+1	O
-	O
2	O
lk	O
,	O
l	O
=	O
l*	O
+e	O
{	O
l1-	O
2min	O
(	O
17	O
(	O
x	O
)	O
,	O
1-17	O
(	O
x	O
»	O
1	O
x	O
p	O
{	O
binomial	B
(	O
k	O
,	O
min	O
(	O
17	O
(	O
x	O
)	O
,	O
1	O
-	O
17	O
(	O
x	O
»	O
)	O
?	O
:	O
ilx	O
}	O
}	O
,	O
and	O
therefore	O
e	O
{	O
/1-2min	O
(	O
17	O
(	O
x	O
)	O
,1-17	O
(	O
x	O
»	O
i	O
x	O
p	O
{	O
binomial	B
(	O
k	O
,	O
min	O
(	O
17	O
(	O
x	O
)	O
,	O
1	O
-	O
17	O
(	O
x	O
»	O
)	O
=	O
l	O
ix	O
}	O
}	O
problems	O
and	O
exercises	O
83	O
e	O
{	O
11	O
-	O
2	O
min	O
(	O
1	O
]	O
(	O
x	O
)	O
,	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
)	O
1	O
x	O
g	O
)	O
min	O
(	O
ry	O
(	O
x	O
)	O
,	O
1	O
-	O
ry	O
(	O
x	O
)	O
j	O
'	O
(	O
l	O
min	O
(	O
ry	O
(	O
x	O
)	O
,	O
1	O
-	O
ry	O
(	O
x	O
)	O
)	O
)	O
k-l	O
}	O
<	O
(	O
k	O
)	O
ll	O
(	O
k	O
-	O
l	O
)	O
k-l	O
kk	O
l	O
(	O
because	O
ul	O
(	O
1	O
-	O
u	O
)	O
k-l	O
reaches	O
its	O
maximum	O
on	O
[	O
0,1	O
]	O
at	O
u	O
=	O
lj	O
k	O
)	O
<	O
k	O
12nl	O
(	O
k	O
-i	O
)	O
(	O
use	O
(	O
~	O
)	O
:	O
:	O
:	O
:	O
;	O
ll	O
(	O
k	O
'	O
!	O
.z	O
)	O
k-l	O
kj	O
l	O
(	O
k~l	O
)	O
'	O
by	O
stirling	O
's	O
formula	O
)	O
.	O
with	O
i	O
=	O
rkj2l	O
,	O
we	O
thus	O
obtain	O
lknn	O
-	O
l*	O
<	O
1	O
k	O
rkj2l	O
lkj2j	O
2v2jt	O
~	O
~0.398942	O
v~	O
v'k	O
'	O
improving	O
on	O
theorem	B
5.6.	O
various	O
other	O
inequalities	O
may	O
be	O
derived	O
in	O
this	O
man	O
(	O
cid:173	O
)	O
ner	O
as	O
well	O
.	O
problems	O
and	O
exercises	O
problem	O
5.1.	O
let	O
ii	O
.	O
ii	O
be	O
an	O
arbitrary	O
norm	O
on	O
n	O
d	O
,	O
and	O
define	O
the	O
k-nearest	O
neighbor	B
rule	I
in	O
terms	O
of	O
the	O
distance	B
p	O
(	O
x	O
,	O
z	O
)	O
=	O
iix	O
-	O
z	O
ii	O
.	O
show	O
that	O
theorems	O
5.1	O
and	O
5.2	O
remain	O
valid	O
.	O
hint	O
:	O
only	O
stone	O
's	O
lemma	O
needs	O
adjusting	O
.	O
the	O
role	O
of	O
cones	O
c	O
(	O
x	O
,	O
jt	O
/6	O
)	O
used	O
in	O
the	O
proof	O
are	O
now	O
played	O
by	O
sets	O
with	O
the	O
following	O
property	O
:	O
x	O
and	O
z	O
belong	O
to	O
the	O
same	O
set	O
if	O
and	O
only	O
if	O
problem	O
5.2.	O
does	O
there	O
exist	O
a	O
distribution	O
for	O
which	O
supn	O
:	O
:	O
:	O
i	O
e	O
{	O
ln	O
}	O
>	O
1/2	O
for	O
the	O
nearest	B
neighbor	I
rule	I
?	O
problem	O
5.3.	O
show	O
that	O
l3nn	O
:	O
s	O
1.32l	O
*	O
and	O
that	O
lsnn	O
:	O
s	O
1.22l	O
*	O
.	O
problem	O
5.4.	O
show	O
that	O
if	O
c*	O
is	O
a	O
compact	O
subset	O
of	O
n	O
d	O
and	O
c	O
is	O
the	O
support	B
set	O
for	O
the	O
probability	O
measure	B
fl	O
,	O
iix	O
(	O
l	O
)	O
-	O
xii	O
-+	O
0	O
sup	O
xecnc*	O
with	O
probability	O
one	O
,	O
where	O
x	O
(	O
l	O
)	O
is	O
the	O
nearest	B
neighbor	I
of	O
x	O
among	O
xi	O
,	O
...	O
,	O
xn	O
(	O
wagner	O
(	O
1971	O
)	O
)	O
.	O
84	O
5.	O
nearest	B
neighbor	I
rules	I
problem	O
5.5.	O
let	O
fl	O
be	O
the	O
probability	O
measure	B
of	O
x	O
given	O
y	O
=	O
0	O
,	O
and	O
let	O
v	O
be	O
the	O
probability	O
measure	B
of	O
x	O
given	O
y	O
=	O
1.	O
assume	O
that	O
x	O
is	O
real-valued	O
and	O
that	O
p	O
{	O
y	O
=	O
o	O
}	O
=	O
p	O
{	O
y	O
=	O
i	O
}	O
=	O
1/2	O
.	O
find	O
a	O
pair	O
(	O
v	O
,	O
fl	O
)	O
such	O
that	O
support	B
(	O
fl	O
)	O
=	O
support	B
(	O
v	O
)	O
;	O
(	O
1	O
)	O
(	O
2	O
)	O
l*	O
=	O
0.	O
conclude	O
that	O
l	O
*	O
=	O
°	O
does	O
not	O
tell	O
us	O
a	O
lot	O
about	O
the	O
support	B
sets	O
of	O
fl	O
and	O
v.	O
problem	O
5.6.	O
consider	O
the	O
(	O
2k	O
+	O
i	O
)	O
-nearest	O
neighbor	B
rule	I
for	O
distributions	O
(	O
x	O
,	O
y	O
)	O
with	O
1	O
(	O
x	O
)	O
==	O
p	O
constant	O
,	O
and	O
y	O
independent	O
of	O
x.	O
this	O
exercise	O
explores	O
the	O
behavior	O
of	O
l	O
(	O
2k+l	O
)	O
nn	O
as	O
p	O
to	O
.	O
(	O
1	O
)	O
for	O
fixed	O
integer	O
1	O
>	O
0	O
,	O
as	O
p	O
to	O
,	O
show	O
that	O
(	O
2	O
)	O
use	O
a	O
convenient	O
representation	O
of	O
l	O
(	O
2k+l	O
)	O
nn	O
to	O
conclude	O
that	O
as	O
p	O
t	O
0	O
,	O
l	O
(	O
2k+l	O
)	O
nn	O
=	O
p	O
+	O
(	O
(	O
2k	O
)	O
(	O
2k	O
)	O
)	O
k+l	O
k	O
+	O
k	O
+	O
1	O
p	O
+	O
o	O
(	O
p	O
k+l	O
)	O
.	O
problem	O
5.7.	O
das	O
gupta	O
and	O
lin	O
(	O
1980	O
)	O
proposed	O
the	O
following	O
rule	B
for	O
data	O
with	O
x	O
e	O
r.	O
assume	O
x	O
is	O
nonatomic	O
.	O
first	O
,	O
reorder	O
xi	O
,	O
...	O
,	O
x	O
n	O
,	O
x	O
according	O
to	O
increasing	O
values	O
,	O
and	O
denote	O
the	O
ordered	B
set	O
by	O
x	O
(	O
1	O
)	O
<	O
x	O
(	O
2	O
)	O
<	O
...	O
<	O
xci	O
)	O
<	O
x	O
<	O
x	O
(	O
i+l	O
)	O
<	O
'	O
''	O
<	O
x	O
(	O
n	O
)	O
.theyi'sare	O
permuted	O
so	O
that	O
y	O
(	O
j	O
)	O
is	O
the	O
label	O
of	O
x	O
(	O
j	O
)	O
.	O
take	O
votes	O
among	O
{	O
y	O
(	O
i	O
)	O
,	O
y	O
(	O
i+l	O
)	O
}	O
,	O
{	O
y	O
(	O
i-l	O
)	O
,	O
y	O
(	O
i+2	O
)	O
}	O
,	O
...	O
until	O
for	O
the	O
first	O
time	O
there	O
is	O
agreement	B
(	O
yu-j	O
)	O
=	O
y	O
(	O
i+j+l	O
»	O
)	O
,	O
at	O
which	O
time	O
we	O
decide	O
that	O
class	O
,	O
that	O
is	O
,	O
gn	O
(	O
x	O
)	O
=	O
y	O
(	O
i	O
_	O
j	O
)	O
=	O
y	O
(	O
i+	O
j+	O
1	O
)	O
.	O
this	O
rule	B
is	O
invariant	O
under	O
monotone	O
transformations	O
of	O
the	O
x-axis	O
.	O
(	O
1	O
)	O
if	O
l	O
denotes	O
the	O
asymptotic	O
expected	O
probability	O
of	O
error	O
,	O
show	O
that	O
for	O
all	O
non	O
(	O
cid:173	O
)	O
atomic	O
x	O
,	O
l	O
=	O
e	O
{	O
1	O
(	O
x	O
)	O
(	O
1	O
-	O
1	O
(	O
x	O
)	O
i	O
1	O
-	O
21	O
(	O
x	O
)	O
(	O
1	O
-	O
1	O
(	O
x	O
)	O
.	O
(	O
2	O
)	O
show	O
that	O
l	O
is	O
the	O
same	O
as	O
for	O
the	O
rule	B
in	O
which	O
x	O
e	O
rd	O
and	O
we	O
consider	O
2-nn	O
,	O
4-nn	O
,	O
6-nn	O
,	O
etc	O
.	O
rules	O
in	O
turn	O
,	O
stopping	O
at	O
the	O
first	O
2k-nn	O
rule	B
for	O
which	O
there	O
is	O
no	O
voting	O
tie	O
.	O
assume	O
for	O
simplicity	O
that	O
x	O
has	O
a	O
density	O
(	O
with	O
a	O
good	O
distance-tie	O
breaking	O
rule	B
,	O
this	O
may	O
be	O
dropped	O
)	O
.	O
(	O
3	O
)	O
show	O
that	O
l	O
-	O
lnn	O
~	O
(	O
l/2	O
)	O
(	O
l3nn	O
-	O
l	O
nn	O
)	O
,	O
and	O
thus	O
that	O
l	O
~	O
(	O
lnn	O
+	O
l	O
3nn	O
)	O
/2	O
.	O
(	O
4	O
)	O
show	O
that	O
l	O
:	O
:	O
:	O
:	O
:	O
l	O
nn	O
.	O
hence	O
,	O
the	O
rule	B
performs	O
somewhere	O
in	O
between	O
the	O
i-nn	O
and	O
3-nn	O
rules	O
.	O
problem	O
5.8.	O
let	O
y	O
be	O
independent	O
of	O
x	O
,	O
and	O
1	O
(	O
x	O
)	O
==	O
p	O
constant	O
.	O
consider	O
a	O
weighted	O
(	O
2k	O
+	O
i	O
)	O
-nearest	O
neighbor	B
rule	I
with	O
weights	O
(	O
2m	O
+	O
1	O
,	O
1	O
,	O
1	O
,	O
...	O
,1	O
)	O
(	O
there	O
are	O
2k	O
``	O
ones	O
''	O
)	O
,	O
where	O
k	O
-	O
1	O
~	O
m	O
~	O
0.	O
for	O
m	O
=	O
0	O
,	O
we	O
obtain	O
the	O
(	O
2k	O
+	O
1	O
)	O
-nn	O
rule	B
.	O
let	O
l	O
(	O
k	O
,	O
m	O
)	O
be	O
the	O
asymptotic	O
probability	O
of	O
error	O
.	O
(	O
1	O
)	O
using	O
results	O
from	O
problem	O
5.6	O
,	O
show	O
that	O
l	O
(	O
k	O
,	O
m	O
)	O
=	O
p	O
+	O
(	O
2k	O
)	O
pk-m+l	O
+	O
(	O
2k	O
)	O
pk+m+l	O
+	O
o	O
(	O
pk-m+l	O
)	O
k-m	O
k+m+l	O
as	O
p	O
t	O
0.	O
conclude	O
that	O
within	O
this	O
class	O
of	O
rules	O
,	O
for	O
small	O
p	O
,	O
the	O
goodness	O
of	O
a	O
rule	O
is	O
measured	O
by	O
k	O
-	O
m.	O
(	O
2	O
)	O
let	O
0	O
>	O
°	O
be	O
small	O
,	O
and	O
set	O
p	O
=	O
1/2	O
-	O
o.	O
show	O
that	O
if	O
x	O
is	O
binomial	B
(	O
2k	O
,	O
1/2	O
-	O
0	O
)	O
and	O
z	O
is	O
binomial	B
(	O
2k	O
,	O
1/2	O
)	O
,	O
then	O
for	O
fixed	O
i	O
,	O
as	O
8	O
+	O
0	O
,	O
problems	O
and	O
exercises	O
85	O
pix	O
2	O
:	O
l	O
}	O
=	O
p	O
{	O
z	O
2	O
:	O
l	O
}	O
-	O
2kop	O
{	O
z	O
=	O
l	O
}	O
+	O
0	O
(	O
02	O
)	O
,	O
and	O
pix	O
:	O
:s	O
i	O
}	O
=	O
p	O
{	O
z	O
:	O
:s	O
l	O
}	O
+	O
2k8p	O
{	O
z	O
=	O
1	O
+	O
i	O
}	O
+	O
0	O
(	O
82	O
)	O
.	O
(	O
3	O
)	O
conclude	O
that	O
for	O
fixed	O
k	O
,	O
m	O
as	O
8	O
.	O
}	O
0	O
,	O
l	O
(	O
k	O
,	O
m	O
)	O
1	O
``	O
2	O
-	O
282	O
(	O
kp	O
{	O
z	O
=	O
k	O
+	O
m	O
}	O
+	O
kp	O
{	O
z	O
=	O
k	O
+	O
m	O
+	O
i	O
}	O
+	O
p	O
{	O
z	O
:	O
:s	O
k	O
+	O
m	O
}	O
-	O
p	O
{	O
z	O
2	O
:	O
k	O
+	O
m	O
+	O
i	O
}	O
)	O
+	O
0	O
(	O
0	O
2	O
)	O
.	O
(	O
4	O
)	O
take	O
weight	O
vector	O
w	O
with	O
k	O
fixed	O
and	O
m	O
=	O
l	O
1	O
0	O
,	O
jk	O
j	O
,	O
and	O
compare	O
it	O
with	O
weight	O
vector	O
w	O
'	O
with	O
k/2	O
components	O
and	O
m	O
=	O
l.jkl2j	O
as	O
p	O
.	O
}	O
°	O
and	O
p	O
t	O
1/2	O
.	O
assume	O
that	O
k	O
is	O
very	O
large	O
but	O
fixed	O
.	O
in	O
particular	O
,	O
show	O
that	O
w	O
is	O
better	O
as	O
p	O
+	O
0	O
,	O
and	O
w	O
'	O
is	O
better	O
as	O
p	O
t	O
1/2	O
.	O
for	O
the	O
last	O
example	O
,	O
note	O
that	O
for	O
fixed	O
c	O
>	O
0	O
,	O
kp	O
{	O
z=k+m	O
}	O
+kp	O
{	O
z=k+m+l	O
}	O
''	O
-'8-jk	O
~e-2c2	O
as	O
k	O
...	O
...	O
,	O
..oo	O
y2jt	O
by	O
the	O
central	B
limit	I
theorem	I
.	O
(	O
5	O
)	O
conclude	O
that	O
there	O
exist	O
different	O
weight	O
vectors	O
w	O
,	O
wi	O
for	O
which	O
there	O
exists	O
a	O
pair	O
of	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
such	O
that	O
their	O
asymptotic	O
error	O
probabilities	O
are	O
differently	O
ordered	B
.	O
thus	O
,	O
w	O
is	O
not	O
totally	O
ordered	B
with	O
respect	O
to	O
the	O
probability	O
of	O
error	O
.	O
problem	O
5.9.	O
patrick	O
and	O
fisher	O
(	O
1970	O
)	O
find	O
the	O
k-th	O
nearest	B
neighbor	I
in	O
each	O
of	O
the	O
two	O
classes	O
and	O
classify	O
according	O
to	O
which	O
is	O
nearest	O
.	O
show	O
that	O
their	O
rule	B
is	O
equivalent	O
to	O
a	O
(	O
2k	O
-	O
i	O
)	O
-nearest	O
neighbor	B
rule	I
.	O
problem	O
5.10.	O
rabiner	O
et	O
al	O
(	O
1979	O
)	O
generalize	O
the	O
rule	B
of	O
problem	O
5.9	O
so	O
as	O
to	O
classify	O
according	O
to	O
the	O
average	O
distance	B
to	O
the	O
k-th	O
nearest	B
neighbor	I
within	O
each	O
class	O
.	O
assume	O
that	O
x	O
has	O
a	O
density	O
.	O
for	O
fixed	O
k	O
,	O
find	O
the	O
asymptotic	O
probability	O
of	O
error	O
.	O
problem	O
5.11.	O
if	O
n	O
is	O
normal	B
(	O
0	O
,	O
1	O
)	O
,	O
then	O
e	O
{	O
ini	O
}	O
=	O
,	O
j2	O
!	O
ii	O
.	O
prove	O
this	O
.	O
problem	O
5.12.	O
show	O
that	O
if	O
l9nn	O
=	O
l	O
(	O
11	O
)	O
nn	O
,	O
then	O
l	O
(	O
99	O
)	O
nn	O
=	O
l	O
(	O
lll	O
)	O
nn	O
'	O
problem	O
5.13.	O
show	O
that	O
l3nn	O
:	O
:s	O
(	O
7v7	O
+	O
17	O
27v'3	O
+	O
1	O
l	O
~	O
1.3155	O
...	O
l	O
*	O
)	O
*	O
for	O
all	O
distributions	O
(	O
devroye	O
(	O
1981b	O
»	O
.	O
hint	O
:	O
find	O
the	O
smallest	O
constanta	O
such	O
that	O
l3nn	O
:	O
:s	O
l	O
*	O
(	O
1	O
+	O
a	O
)	O
using	O
the	O
representation	O
of	O
l3nn	O
in	O
terms	O
of	O
the	O
binomial	B
tail	O
problem	O
5.14.	O
show	O
that	O
if	O
x	O
has	O
a	O
density	O
j	O
,	O
then	O
for	O
all	O
u	O
>	O
0	O
,	O
lim	O
p	O
{	O
nl/dilx	O
(	O
l	O
)	O
(	O
x	O
)	O
-	O
xii	O
>	O
u\x	O
}	O
=	O
e-f	O
(	O
x	O
)	O
vu	O
n	O
--	O
+oo	O
d	O
with	O
probability	O
one	O
,	O
where	O
v	O
=	O
is	O
dx	O
is	O
the	O
volume	O
ofthe	O
unit	O
ball	O
in	O
n	O
d	O
(	O
gyorfi	O
(	O
1978	O
»	O
.	O
0,1	O
86	O
5.	O
nearest	B
neighbor	I
rules	I
problem	O
5.15.	O
consider	O
a	O
rule	O
that	O
takes	O
a	O
majority	O
vote	O
over	O
all	O
yi	O
's	O
for	O
which	O
iixi	O
-xii	O
:	O
:s	O
(	O
c	O
/	O
v	O
n	O
)	O
1	O
j	O
d	O
,	O
where	O
v	O
=	O
is	O
dx	O
is	O
the	O
volume	O
of	O
the	O
unit	O
ball	O
,	O
and	O
c	O
>	O
0	O
is	O
fixed	O
.	O
in	O
case	O
of	O
a	O
tie	O
decide	O
gn	O
(	O
x	O
)	O
=	O
o	O
.	O
0,1	O
if	O
x	O
has	O
a	O
density	O
j	O
,	O
show	O
thatliminfn	O
--	O
+ooe	O
{	O
ln	O
}	O
:	O
:	O
:	O
:	O
e	O
{	O
1j	O
(	O
x	O
)	O
e-cf	O
(	O
x	O
)	O
}	O
.	O
hint	O
:	O
use	O
the	O
obvious	O
inequality	B
e	O
{	O
l	O
n	O
}	O
:	O
:	O
:	O
:	O
p	O
{	O
y	O
=	O
1	O
,	O
fj	O
--	O
n	O
(	O
sx	O
,	O
cjvn	O
)	O
=	O
oj	O
.	O
(	O
1	O
)	O
(	O
2	O
)	O
if	O
y	O
is	O
independent	O
of	O
x	O
and	O
1j	O
==	O
p	O
>	O
1/2	O
,	O
then	O
e	O
{	O
(	O
x	O
)	O
e-cf	O
(	O
x	O
)	O
}	O
1j	O
l*	O
=	O
e	O
{	O
e-cf	O
(	O
x	O
)	O
}	O
_p-	O
too	O
1-	O
p	O
as	O
p	O
t	O
1.	O
show	O
this	O
.	O
(	O
3	O
)	O
conclude	O
that	O
sup	O
(	O
x	O
,	O
y	O
)	O
:	O
l*	O
>	O
o	O
lim	O
infn	O
--	O
+oo	O
e	O
{	O
l	O
n	O
}	O
l*	O
=	O
00	O
,	O
and	O
thus	O
that	O
distribution-free	O
bounds	O
of	O
the	O
form	O
limn	O
--	O
+	O
oo	O
e	O
{	O
ln	O
}	O
:	O
:	O
:	O
:	O
c	O
'	O
l	O
*	O
obtained	O
for	O
k-nearest	O
neighbor	O
estimates	O
do	O
not	O
exist	O
for	O
these	O
simple	O
rules	O
(	O
devroye	O
(	O
198ia	O
»	O
.	O
problem	O
5.16.	O
take	O
an	O
example	O
with	O
1j	O
(	O
x	O
)	O
==	O
1/2	O
-	O
i/	O
(	O
2./k	O
)	O
,	O
and	O
show	O
that	O
the	O
bound	O
lknn	O
-	O
l	O
*	O
:	O
:	O
:	O
:	O
i/	O
''	O
jke	O
can	O
not	O
be	O
essentially	O
bettered	O
for	O
large	O
values	O
of	O
k	O
,	O
that	O
is	O
,	O
there	O
exists	O
a	O
sequence	O
of	O
distributions	O
(	O
indexed	O
by	O
k	O
)	O
for	O
which	O
as	O
k	O
-+	O
00	O
,	O
where	O
n	O
is	O
a	O
normal	O
(	O
0	O
,	O
1	O
)	O
random	O
variable	B
.	O
problem	O
5.17.	O
if	O
b	O
is	O
binomial	B
(	O
n	O
,	O
p	O
)	O
,	O
then	O
sup	O
p	O
{	O
b	O
=	O
i	O
}	O
:	O
:	O
:	O
:	O
i	O
.	O
n	O
v	O
27il	O
(	O
n	O
-	O
1	O
)	O
p	O
.	O
,	O
0	O
<	O
i	O
<	O
n.	O
problem	O
5.18.	O
show	O
that	O
for	O
k	O
:	O
:	O
:	O
:	O
3	O
,	O
,	O
jk	O
(	O
k+l	O
)	O
<	O
(	O
1	O
+	O
~	O
)	O
(	O
1	O
+	O
~	O
)	O
=	O
1	O
+	O
~	O
+	O
~	O
.	O
k	O
-	O
1	O
4k2	O
2k	O
2k	O
k	O
-	O
problem	O
5.19.	O
show	O
that	O
there	O
exists	O
a	O
sequence	O
of	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
(	O
indexed	O
by	O
k	O
)	O
in	O
which	O
y	O
is	O
independent	O
of	O
x	O
and	O
1j	O
(	O
x	O
)	O
==	O
p	O
(	O
with	O
p	O
depending	O
on	O
k	O
only	O
)	O
such	O
that	O
lim	O
inf	O
n	O
--	O
+oo	O
(	O
lknn	O
-	O
l	O
*	O
)	O
l*	O
''	O
v	O
k	O
:	O
:	O
:	O
:	O
y	O
=	O
0.339942	O
...	O
,	O
where	O
y	O
is	O
the	O
constant	O
of	O
theorem	O
5.8	O
(	O
devroye	O
(	O
1981b	O
»	O
.	O
hint	O
:	O
verify	O
the	O
proof	O
of	O
the	O
(	O
cid:173	O
)	O
orem	O
5.8	O
but	O
bound	O
things	O
from	O
below	O
.	O
slud	O
's	O
inequality	B
(	O
see	O
lemma	O
a.6	O
in	O
the	O
appendix	O
)	O
may	O
be	O
of	O
use	O
here	O
.	O
problem	O
5.20.	O
consider	O
a	O
weighted	O
nearest	B
neighbor	I
rule	I
with	O
weights	O
1	O
,	O
p	O
,	O
p2	O
,	O
p3	O
,	O
'	O
''	O
for	O
p	O
<	O
1.	O
show	O
that	O
the	O
expected	O
probability	O
of	O
error	O
tends	O
for	O
all	O
distributions	O
to	O
a	O
limit	O
l	O
(	O
p	O
)	O
.	O
hint	O
:	O
truncate	O
at	O
k	O
fixed	O
but	O
large	O
,	O
and	O
argue	O
that	O
the	O
tail	O
has	O
asymptotically	O
negligible	O
weight	O
.	O
problems	O
and	O
exercises	O
87	O
problem	O
5.21.	O
continued	O
.	O
with	O
l	O
(	O
p	O
)	O
as	O
in	O
the	O
previous	O
exercise	O
,	O
show	O
that	O
l	O
(	O
p	O
)	O
=	O
lnn	O
whenever	O
p	O
<	O
1/2	O
.	O
problem	O
5.22.	O
continued	O
.	O
prove	O
or	O
disprove	O
:	O
as	O
p	O
increases	O
from	O
1/2	O
to	O
1	O
,	O
l	O
(	O
p	O
)	O
decreases	O
monotonically	O
from	O
lnn	O
to	O
l	O
*	O
.	O
(	O
this	O
question	O
is	O
difficult	O
.	O
)	O
problem	O
5.23.	O
show	O
that	O
in	O
the	O
weighted	B
nn	O
rule	B
with	O
weights	O
(	O
1	O
,	O
p	O
,	O
p2	O
)	O
,	O
0	O
<	O
p	O
<	O
1	O
,	O
the	O
asymptotic	O
probability	O
of	O
error	O
is	O
lnn	O
if	O
p	O
<	O
(	O
.j5	O
-	O
1	O
)	O
/2	O
and	O
is	O
l3nn	O
if	O
p	O
>	O
(	O
.j5	O
-	O
1	O
)	O
/2	O
.	O
problem	O
5.24.	O
is	O
there	O
any	O
k	O
,	O
other	O
than	O
one	O
,	O
for	O
which	O
the	O
k-nn	O
rule	B
is	O
admissible	O
,	O
that	O
is	O
,	O
for	O
which	O
there	O
exists	O
a	O
distribution	O
of	O
(	O
x	O
,	O
y	O
)	O
such	O
that	O
e	O
{	O
l	O
n	O
}	O
for	O
the	O
k-nn	O
rule	B
is	O
smaller	O
than	O
e	O
{	O
l	O
n	O
}	O
for	O
any	O
k'-nn	O
rule	B
with	O
k	O
'	O
=i	O
k	O
,	O
for	O
all	O
n	O
?	O
hint	O
:	O
this	O
is	O
difficult	O
.	O
note	O
that	O
if	O
this	O
is	O
to	O
hold	O
for	O
all	O
n	O
,	O
then	O
it	O
must	O
hold	O
for	O
the	O
limits	O
.	O
from	O
this	O
,	O
deduce	O
that	O
with	O
probability	O
one	O
,	O
7j	O
(	O
x	O
)	O
e	O
{	O
o	O
,	O
1/2	O
,	O
i	O
}	O
for	O
any	O
such	O
distribution	B
.	O
problem	O
5.25.	O
for	O
every	O
fixed	O
n	O
and	O
odd	O
k	O
with	O
n	O
>	O
loook	O
,	O
find	O
a	O
distribution	O
of	O
(	O
x	O
,	O
y	O
)	O
such	O
that	O
e	O
{	O
ln	O
}	O
for	O
the	O
k-nn	O
rule	B
is	O
smaller	O
than	O
e	O
{	O
ln	O
}	O
for	O
any	O
k'-nn	O
rule	B
with	O
k	O
'	O
=i	O
k	O
,	O
k	O
'	O
odd	O
.	O
thus	O
,	O
for	O
a	O
given	O
n	O
,	O
no	O
k	O
can	O
be	O
a	O
priori	O
discarded	O
from	O
consideration	O
.	O
problem	O
5.26.	O
let	O
x	O
be	O
uniform	B
on	O
[	O
0	O
,	O
1	O
]	O
,	O
7j	O
(	O
x	O
)	O
==	O
x	O
,	O
and	O
pry	O
=	O
o	O
}	O
=	O
pry	O
=	O
i	O
}	O
=	O
1/2	O
.	O
show	O
that	O
for	O
the	O
nearest	B
neighbor	I
rule	I
,	O
e	O
{	O
ln	O
}	O
=	O
-	O
+	O
-	O
-	O
-	O
-	O
-	O
-	O
2	O
(	O
n	O
+	O
1	O
)	O
(	O
n	O
+	O
2	O
)	O
(	O
n	O
+	O
3	O
)	O
3n	O
+	O
5	O
1	O
3	O
(	O
cover	O
and	O
hart	O
(	O
1967	O
)	O
;	O
peterson	O
(	O
1970	O
)	O
)	O
.	O
problem	O
5.27.	O
for	O
the	O
nearest	B
neighbor	I
rule	I
,	O
if	O
x	O
has	O
a	O
density	O
,	O
then	O
ie	O
{	O
ln	O
}	O
-	O
e	O
{	O
ln+dl	O
:	O
:s	O
-	O
1	O
n+l	O
(	O
cover	O
(	O
1968a	O
)	O
)	O
.	O
problem	O
5.28.	O
let	O
x	O
have	O
a	O
density	O
f	O
~	O
c	O
>	O
0	O
on	O
[	O
0	O
,	O
1	O
]	O
,	O
and	O
assume	O
that	O
f~	O
''	O
and	O
f	O
{	O
``	O
exist	O
and	O
are	O
uniformly	O
bounded	O
.	O
show	O
that	O
for	O
the	O
nearest	B
neighbor	I
rule	I
,	O
e	O
{	O
ln	O
}	O
=	O
lnn	O
+	O
0	O
(	O
1/	O
n2	O
)	O
(	O
cover	O
(	O
1968a	O
»	O
)	O
.	O
for	O
d-dimensional	O
problems	O
this	O
result	O
was	O
generalized	B
by	O
psaltis	O
,	O
snapp	O
,	O
and	O
venkatesh	O
(	O
1994	O
)	O
.	O
problem	O
5.29.	O
show	O
that	O
lknn	O
:	O
:s	O
(	O
1	O
+	O
,	O
j2jk	O
)	O
l	O
*	O
is	O
the	O
best	O
possible	O
bound	O
of	O
the	O
form	O
lknn	O
:	O
:s	O
(	O
1	O
+	O
a/	O
``	O
jk	O
)	O
l	O
*	O
valid	O
simultaneously	O
for	O
all	O
k	O
~	O
1	O
(	O
devroye	O
(	O
1981b	O
)	O
)	O
.	O
problem	O
5.30.	O
show	O
that	O
lknn	O
:	O
:s	O
(	O
1	O
+	O
-jllk	O
)	O
l*	O
for	O
all	O
k	O
~	O
3	O
(	O
devroye	O
(	O
1981b	O
)	O
)	O
.	O
problem	O
5.31.	O
let	O
x	O
=	O
(	O
x	O
(	O
1	O
)	O
,	O
x	O
(	O
2	O
)	O
)	O
e	O
n2	O
.	O
consider	O
the	O
nearest	B
neighbor	I
rule	I
based	O
upon	O
vectors	O
with	O
components	O
(	O
x	O
3	O
(	O
1	O
)	O
,	O
x	O
7	O
(	O
2	O
)	O
,	O
x	O
(	O
1	O
)	O
x	O
(	O
2	O
)	O
)	O
.	O
show	O
that	O
this	O
is	O
asymptotically	O
not	O
better	O
than	O
if	O
we	O
had	O
used	O
(	O
x	O
(	O
1	O
)	O
,	O
x	O
(	O
2	O
»	O
)	O
.	O
show	O
by	O
example	O
that	O
(	O
x2	O
(	O
1	O
)	O
,	O
x	O
3	O
(	O
2	O
)	O
,	O
x	O
6	O
(	O
1	O
)	O
x	O
(	O
2	O
)	O
)	O
may	O
yield	O
a	O
worse	O
asymptotic	O
error	O
probability	O
than	O
(	O
x	O
(	O
1	O
)	O
,	O
x	O
(	O
2	O
)	O
)	O
.	O
problem	O
5.32.	O
uniform	B
order	O
statistics	B
.	O
let	O
u	O
(	O
1	O
)	O
<	O
...	O
<	O
u	O
(	O
n	O
)	O
be	O
order	B
statistics	I
of	O
n	O
i.i.d	O
.	O
uniform	B
[	O
0	O
,	O
1	O
]	O
random	O
variables	O
.	O
show	O
the	O
following	O
:	O
(	O
1	O
)	O
u	O
(	O
k	O
)	O
is	O
beta	B
(	O
k	O
,	O
n	O
+	O
1	O
-	O
k	O
)	O
,	O
that	O
is	O
,	O
u	O
(	O
k	O
)	O
has	O
density	O
f	O
(	O
x	O
)	O
-	O
-	O
n	O
!	O
x	O
k-	O
1	O
(	O
1	O
-	O
x	O
)	O
n-k	O
0	O
_	O
<	O
x	O
_	O
<	O
1	O
.	O
(	O
k	O
-	O
1	O
)	O
!	O
(	O
n	O
-	O
k	O
)	O
!	O
'	O
88	O
5.	O
nearest	B
neighbor	I
rules	I
(	O
2	O
)	O
(	O
3	O
)	O
a	O
e	O
{	O
u	O
(	O
k	O
)	O
}	O
=	O
r	O
(	O
k	O
+	O
a	O
)	O
r	O
(	O
n	O
+	O
1	O
)	O
,	O
r	O
(	O
k	O
)	O
r	O
(	O
n	O
+	O
1	O
+	O
a	O
)	O
for	O
any	O
a	O
>	O
0	O
.	O
l/	O
!	O
(	O
a	O
)	O
1	O
-	O
-	O
<	O
-	O
-	O
-	O
<	O
1+	O
-	O
-	O
a	O
e	O
{	O
u	O
&	O
)	O
}	O
(	O
kln	O
)	O
a	O
-	O
n	O
-	O
k	O
for	O
a	O
:	O
:	O
:	O
:	O
1	O
,	O
where	O
l/	O
!	O
(	O
a	O
)	O
is	O
a	O
function	O
of	O
a	O
only	O
(	O
royall	O
(	O
1966	O
)	O
)	O
.	O
problem	O
5.33.	O
dudani	O
's	O
rule	B
.	O
dudani	O
(	O
1976	O
)	O
proposes	O
a	O
weighted	O
k-nn	O
rule	B
where	O
y	O
(	O
i	O
)	O
(	O
x	O
)	O
receives	O
weight	O
iix	O
(	O
k	O
)	O
(	O
x	O
)	O
-	O
xii	O
-	O
iix	O
(	O
i	O
)	O
(	O
x	O
)	O
-	O
xii	O
,	O
1	O
:	O
:	O
:	O
;	O
i	O
:	O
:	O
:	O
;	O
k.	O
why	O
is	O
this	O
roughly	O
speaking	O
equivalent	O
to	O
attaching	O
weight	O
1	O
-	O
(	O
i	O
i	O
k	O
)	O
l/d	O
to	O
the	O
i-th	O
nearest	B
neighbor	I
if	O
x	O
has	O
a	O
density	O
?	O
hint	O
:	O
if	O
~	O
is	O
the	O
probability	O
measure	B
of	O
x	O
,	O
then	O
are	O
distributed	O
like	O
u	O
(	O
1	O
)	O
,	O
...	O
,	O
u	O
(	O
k	O
)	O
where	O
u	O
(	O
l	O
)	O
<	O
...	O
<	O
u	O
(	O
n	O
)	O
are	O
the	O
order	B
statistics	I
of	O
n	O
i.i.d	O
.	O
uniform	B
[	O
0	O
,	O
1	O
]	O
random	O
variables	O
.	O
replace	O
~	O
by	O
a	O
good	O
local	O
approximation	O
,	O
and	O
use	O
results	O
from	O
the	O
previous	O
exercise	O
.	O
problem	O
5.34.	O
show	O
devijver	O
's	O
theorem	B
(	O
theorem	B
5.10	O
)	O
in	O
two	O
parts	O
:	O
first	O
establish	O
the	O
inequality	B
l	O
*	O
:	O
:	O
:	O
:	O
lk	O
,	O
lk/21-1	O
for	O
the	O
tennis	B
rule	I
,	O
and	O
then	O
establish	O
the	O
monotonicity	O
.	O
problem	O
5.35.	O
show	O
theorem	B
5.9	O
for	O
the	O
(	O
k	O
,	O
i	O
)	O
nearest	B
neighbor	I
rule	I
.	O
problem	O
5.36.	O
let	O
r	O
be	O
the	O
asymptotic	O
error	O
probability	O
of	O
the	O
(	O
2	O
,	O
2	O
)	O
-nearest	O
neighbor	B
rule	I
.	O
prove	O
that	O
r	O
=	O
e	O
{	O
21j	O
(	O
x	O
)	O
(	O
1	O
-	O
1j	O
(	O
x	O
»	O
)	O
}	O
=	O
l	O
nn	O
•	O
problem	O
5.37.	O
for	O
the	O
nearest	B
neighbor	I
rule	I
,	O
show	O
that	O
for	O
all	O
distributions	O
,	O
lim	O
p	O
{	O
gn	O
(	O
x	O
)	O
=	O
0	O
,	O
y	O
=	O
i	O
}	O
n-+oo	O
lim	O
p	O
{	O
gn	O
(	O
x	O
)	O
=	O
1	O
,	O
y	O
=	O
o	O
}	O
n-+oo	O
e	O
{	O
1j	O
(	O
x	O
)	O
(	O
1	O
-	O
1j	O
(	O
x	O
»	O
)	O
}	O
(	O
devijver	O
and	O
kittler	O
(	O
1982	O
»	O
)	O
.	O
thus	O
,	O
errors	O
of	O
both	O
kinds	O
are	O
equally	O
likely	O
.	O
problem	O
5.38.	O
let	O
p	O
{	O
y	O
=	O
i	O
}	O
=	O
p	O
{	O
y	O
=	O
o	O
}	O
=	O
1/2	O
and	O
let	O
x	O
be	O
a	O
random	O
rational	O
if	O
y	O
=	O
1	O
(	O
as	O
defined	O
in	O
section	O
5.9	O
)	O
such	O
that	O
every	O
rational	O
number	O
has	O
positive	O
probability	O
,	O
and	O
let	O
x	O
be	O
uniform	B
[	O
0	O
,	O
1	O
]	O
if	O
y	O
=	O
0.	O
show	O
that	O
for	O
every	O
x	O
e	O
[	O
0	O
,	O
1	O
]	O
not	O
rational	O
,	O
p	O
{	O
y	O
(	O
l	O
)	O
(	O
x	O
)	O
=	O
i	O
}	O
--	O
-+	O
°	O
as	O
n	O
--	O
-+	O
00	O
,	O
while	O
for	O
every	O
x	O
e	O
[	O
0,1	O
]	O
rational	O
,	O
p	O
{	O
y	O
(	O
l	O
)	O
(	O
x	O
)	O
=	O
o	O
}	O
--	O
-+	O
°	O
as	O
n	O
--	O
-+	O
00.	O
problem	O
5.39.	O
let	O
xl	O
,	O
...	O
,	O
xn	O
be	O
i.i.d	O
.	O
and	O
have	O
a	O
common	O
density	O
.	O
show	O
that	O
for	O
fixed	O
k	O
>	O
0	O
,	O
np	O
{	O
x3	O
is	O
among	O
the	O
k	O
nearest	O
neighbors	O
of	O
xl	O
and	O
x2	O
in	O
{	O
x3	O
,	O
...	O
,	O
xn	O
}	O
}	O
--	O
-+	O
0.	O
show	O
that	O
the	O
same	O
result	O
remains	O
valid	O
whenever	O
k	O
varies	O
with	O
n	O
such	O
that	O
kl	O
in	O
--	O
-+	O
0.	O
problems	O
and	O
exercises	O
89.	O
problem	O
5.40.	O
imperfect	B
training	I
.	O
let	O
(	O
x	O
,	O
y	O
,	O
z	O
)	O
,	O
(	O
xl	O
,	O
yi	O
,	O
zd	O
,	O
..	O
·	O
,	O
(	O
xn	O
'	O
~1	O
'	O
zn	O
)	O
bease	O
(	O
cid:173	O
)	O
quence	O
of	O
i.i.d	O
.	O
triples	O
in	O
rd	O
x	O
{	O
a	O
,	O
i	O
}	O
x	O
{	O
a	O
,	O
i	O
}	O
with	O
pry	O
=	O
llx	O
=	O
x	O
}	O
=	O
17	O
(	O
x	O
)	O
and	O
p	O
{	O
z	O
=	O
llx	O
=	O
x	O
}	O
=	O
17	O
'	O
(	O
x	O
)	O
.	O
let	O
z	O
(	O
l	O
)	O
(	O
x	O
)	O
be	O
z	O
j	O
if	O
x	O
j	O
is	O
the	O
nearest	B
neighbor	I
of	O
x	O
among	O
x	O
i	O
,	O
...	O
,	O
x	O
n	O
•	O
show	O
that	O
lim	O
p	O
{	O
z	O
(	O
l	O
)	O
(	O
x	O
)	O
=i	O
y	O
}	O
=	O
e	O
{	O
17	O
(	O
x	O
)	O
+	O
17	O
'	O
(	O
x	O
)	O
-	O
217	O
(	O
x	O
)	O
17	O
'	O
(	O
x	O
)	O
}	O
n-+oo	O
(	O
lugosi	O
(	O
1992	O
»	O
.	O
problem	O
5.41.	O
improve	O
the	O
bound	O
in	O
lemma	O
5.3	O
to	O
yd	O
:	O
s	O
3d	O
-	O
1.	O
,	O
then	O
yd	O
2	O
:	O
2d	O
.	O
problem	O
5.42.	O
show	O
that	O
if	O
{	O
c	O
(	O
xi	O
'	O
]	O
[	O
/6	O
)	O
,	O
...	O
,	O
c	O
(	O
xyd	O
,	O
]	O
[	O
/6	O
)	O
}	O
is	O
a	O
collection	O
of	O
cones	O
cover	O
(	O
cid:173	O
)	O
ing	O
r	O
d	O
problem	O
5.43.	O
recalling	O
that	O
lknn	O
=	O
e	O
{	O
ak	O
(	O
17	O
(	O
x	O
»	O
)	O
}	O
,	O
where	O
ak	O
(	O
p	O
)	O
=	O
min	O
(	O
p	O
,	O
1	O
-	O
p	O
)	O
+	O
11	O
-	O
2min	O
(	O
p	O
,	O
1	O
-	O
p	O
)	O
ip	O
{	O
binomial	B
(	O
k	O
,	O
min	O
(	O
p	O
,	O
1	O
-	O
p	O
»	O
>	O
~	O
}	O
,	O
show	O
that	O
for	O
every	O
fixed	O
p	O
,	O
p	O
{	O
binomial	B
(	O
k	O
,	O
min	O
(	O
p	O
,	O
1	O
-	O
p	O
»	O
>	O
~	O
}	O
,	O
}	O
°	O
as	O
k	O
~	O
00	O
(	O
it	O
is	O
the	O
mono	O
tonicity	O
that	O
is	O
harder	O
to	O
show	O
)	O
.	O
how	O
would	O
you	O
then	O
prove	O
that	O
limk	O
--	O
-+oo	O
lknn	O
=	O
l	O
*	O
?	O
problem	O
5.44.	O
show	O
that	O
the	O
asymptotic	O
error	O
probability	O
of	O
the	O
rule	O
that	O
decides	O
gn	O
(	O
x	O
)	O
=	O
y	O
(	O
8	O
)	O
(	O
x	O
)	O
is	O
identical	O
to	O
that	O
of	O
the	O
rule	B
in	O
which	O
gn	O
(	O
x	O
)	O
=	O
y	O
(	O
3	O
)	O
(	O
x	O
)	O
.	O
problem	O
5.45.	O
show	O
that	O
for	O
all	O
distributions	O
lsnn	O
=	O
e	O
{	O
vrs	O
(	O
17	O
(	O
x	O
)	O
(	O
i-17	O
(	O
x	O
»	O
}	O
,	O
where	O
vrs	O
(	O
u	O
)	O
=	O
u	O
+	O
u2	O
+	O
12u3	O
.	O
problem	O
5.46.	O
show	O
that	O
for	O
all	O
distributions	O
,	O
and	O
that	O
problem	O
5.47.	O
let	O
x	O
(	O
l	O
)	O
be	O
the	O
nearest	B
neighbor	I
of	O
x	O
among	O
xl	O
,	O
...	O
,	O
x	O
n	O
.	O
construct	O
an	O
example	O
for	O
which	O
e	O
{	O
ii	O
x	O
(	O
l	O
)	O
-	O
x	O
ii	O
}	O
=	O
00	O
for	O
all	O
x	O
e	O
rd	O
.	O
(	O
therefore	O
,	O
we	O
have	O
to	O
steer	O
clear	O
of	O
convergence	O
in	O
the	O
mean	O
in	O
lemma	O
5.1	O
.	O
)	O
let	O
x	O
(	O
1	O
)	O
be	O
the	O
nearest	B
neighbor	I
of	O
x	O
i	O
among	O
x	O
2	O
,	O
•..	O
,	O
x	O
n	O
•	O
construct	O
a	O
distribution	O
such	O
that	O
e	O
{	O
iix	O
(	O
l	O
)	O
-	O
xiii	O
}	O
=	O
00	O
for	O
all	O
n.	O
problem	O
5.48.	O
consider	O
the	O
weighted	B
nearest	O
neighbor	B
rule	I
with	O
weights	O
(	O
wi	O
,	O
...	O
,	O
wk	O
)	O
.	O
define	O
a	O
new	O
weight	O
vector	O
(	O
wi	O
,	O
w2	O
,	O
...	O
,	O
wk-l	O
,	O
vi	O
,	O
...	O
,	O
vz	O
)	O
,	O
where	O
2	O
:	O
:	O
:	O
;	O
=1	O
vi	O
=	O
wk	O
.	O
thus	O
,	O
the	O
weight	O
vectors	O
are	O
partially	O
ordered	B
by	O
the	O
operation	O
``	O
partition	B
.	O
''	O
assume	O
that	O
all	O
weights	O
are	O
nonnegative	O
.	O
let	O
the	O
asymptotic	O
expected	O
probability	O
of	O
errors	O
be	O
land	O
l	O
'	O
,	O
respectively	O
.	O
true	O
or	O
false	O
:	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
,	O
l	O
'	O
:	O
s	O
l.	O
problem	O
5.49.	O
gabriel	O
neighbors	O
.	O
given	O
xl	O
,	O
'	O
.	O
''	O
xn	O
e	O
r	O
d	O
,	O
we	O
say	O
that	O
xi	O
and	O
xj	O
are	O
gabriel	O
neighbors	O
if	O
the	O
ball	O
centered	O
at	O
(	O
xi	O
+	O
x	O
j	O
)	O
/2	O
of	O
radius	O
iixi	O
-	O
xj	O
11/2	O
contains	O
no	O
x	O
k	O
,	O
k	O
=i	O
i	O
,	O
j	O
(	O
gabriel	O
and	O
sokal	O
(	O
1969	O
)	O
;	O
matula	O
and	O
sokal	O
(	O
1980	O
»	O
.	O
clearly	O
,	O
if	O
xj	O
is	O
the	O
nearest	B
neighbor	I
of	O
xi	O
,	O
then	O
xi	O
and	O
xj	O
are	O
gabriel	O
neighbors	O
.	O
show	O
that	O
if	O
x	O
has	O
a	O
density	O
and	O
x	O
i	O
,	O
...	O
,	O
xn	O
are	O
i.i.d	O
.	O
and	O
drawn	O
from	O
the	O
distribution	B
of	O
x	O
,	O
then	O
the	O
expected	O
number	O
of	O
gabriel	O
neighbors	O
of	O
xl	O
tends	O
to	O
2d	O
as	O
n	O
~	O
00	O
(	O
devroye	O
(	O
1988c	O
»	O
.	O
90	O
5.	O
nearest	B
neighbor	I
rules	I
figure	O
5.7.	O
the	O
gabriel	O
graph	O
of	O
20	O
points	O
on	O
the	O
plane	O
is	O
shown	O
:	O
gabriel	O
neighbors	O
are	O
connected	O
by	O
an	O
edge	O
.	O
note	O
that	O
all	O
circles	O
strad	O
(	O
cid:173	O
)	O
dling	O
these	O
edges	O
have	O
no	O
data	O
point	O
in	O
their	O
interior	O
.	O
problem	O
5.50.	O
gabriel	O
neighbor	B
rule	I
.	O
define	O
the	O
gabriel	O
neighbor	B
rule	I
simply	O
as	O
the	O
rule	B
that	O
takes	O
a	O
majority	O
vote	O
over	O
all	O
yi	O
's	O
for	O
the	O
gabriel	O
neighbors	O
of	O
x	O
among	O
xl	O
,	O
...	O
,	O
x	O
n	O
•	O
ties	O
are	O
broken	O
by	O
flipping	O
a	O
coin	O
.	O
let	O
ln	O
be	O
the	O
conditional	O
probability	O
of	O
error	O
for	O
the	O
gabriel	O
rule	B
.	O
using	O
the	O
result	O
of	O
the	O
previous	O
exercise	O
,	O
show	O
that	O
if	O
l	O
*	O
is	O
the	O
bayes	O
error	O
,	O
then	O
for	O
(	O
3	O
)	O
,	O
determine	O
the	O
best	O
possible	O
value	O
of	O
c.	O
hint	O
:	O
use	O
theorem	B
5.8	O
and	O
try	O
obtaining	O
,	O
for	O
d	O
=	O
2	O
,	O
a	O
lower	O
bound	O
for	O
p	O
{	O
n	O
x	O
2	O
:	O
3	O
}	O
,	O
where	O
n	O
x	O
is	O
the	O
number	O
of	O
gabriel	O
neighbors	O
of	O
x	O
among	O
xl	O
,	O
...	O
,	O
xn	O
•	O
(	O
1	O
)	O
(	O
2	O
)	O
(	O
3	O
)	O
lim	O
e	O
{	O
l	O
n	O
}	O
=	O
0	O
if	O
l*	O
=	O
0	O
;	O
n	O
--	O
-+oo	O
limsupe	O
{	O
l	O
n	O
}	O
<	O
lnn	O
if	O
l*	O
>	O
0	O
,	O
d	O
>	O
1	O
;	O
n	O
--	O
+oo	O
lim	O
sup	O
e	O
{	O
ln	O
}	O
.	O
:	O
:	O
:	O
:	O
cl	O
*	O
for	O
some	O
c	O
<	O
2	O
,	O
if	O
d	O
>	O
1.	O
n	O
--	O
+oo	O
6	O
consistency	B
6.1	O
universal	B
consistency	I
if	O
we	O
are	O
given	O
a	O
sequence	O
dn	O
=	O
(	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
)	O
of	O
training	O
data	O
,	O
the	O
best	O
we	O
can	O
expect	O
from	O
a	O
classification	O
function	O
is	O
to	O
achieve	O
the	O
bayes	O
error	O
probability	O
l	O
*	O
.	O
generally	O
,	O
we	O
can	O
not	O
hope	O
to	O
obtain	O
a	O
function	O
that	O
exactly	O
achieves	O
the	O
bayes	O
error	O
probability	O
,	O
but	O
it	O
is	O
possible	O
to	O
construct	O
a	O
sequence	O
of	O
classification	O
functions	O
{	O
gn	O
}	O
,	O
that	O
is	O
,	O
a	O
classification	O
rule	B
,	O
such	O
that	O
the	O
error	O
probability	O
gets	O
arbitrarily	O
close	O
to	O
l	O
*	O
with	O
large	O
probability	O
(	O
that	O
is	O
,	O
for	O
``	O
most	O
''	O
dn	O
)	O
.	O
this	O
idea	O
is	O
formulated	O
in	O
the	O
definitions	O
of	O
consistency	O
:	O
definition	O
6.1	O
.	O
(	O
weak	B
and	O
strong	B
consistency	I
)	O
.	O
a	O
classification	O
rule	B
is	O
con	O
(	O
cid:173	O
)	O
sistent	O
(	O
or	O
asymptotically	O
bayes-risk	O
efficient	O
)	O
for	O
a	O
certain	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
if	O
and	O
strongly	O
consistent	O
if	O
lim	O
ln	O
=	O
l	O
*	O
with	O
probability	O
1.	O
n-+oo	O
remark	O
.	O
consistency	B
is	O
defined	O
as	O
the	O
convergence	O
of	O
the	O
expected	O
value	O
of	O
ln	O
to	O
l	O
*	O
.	O
since	O
ln	O
is	O
a	O
random	O
variable	B
bounded	O
between	O
l	O
*	O
and	O
1	O
,	O
this	O
convergence	O
92	O
6.	O
consistency	B
every	O
e	O
>	O
°	O
is	O
equivalent	O
to	O
the	O
convergence	O
of	O
ln	O
to	O
l	O
*	O
in	O
probability	O
,	O
which	O
means	O
that	O
for	O
lim	O
p	O
{	O
ln	O
-	O
l	O
*	O
>	O
e	O
}	O
=	O
0.	O
n	O
--	O
-+oo	O
obviously	O
,	O
since	O
almost	O
sure	O
convergence	O
always	O
implies	O
convergence	O
in	O
proba	O
(	O
cid:173	O
)	O
bility	O
,	O
strong	B
consistency	I
implies	O
consistency	B
.	O
d	O
a	O
consistent	O
rule	B
guarantees	O
that	O
by	O
increasing	O
the	O
amount	O
of	O
data	O
the	O
probability	O
that	O
the	O
error	O
probability	O
is	O
within	O
a	O
very	O
small	O
distance	B
of	O
the	O
optimal	O
achievable	O
gets	O
arbitrarily	O
close	O
to	O
one	O
.	O
intuitively	O
,	O
the	O
rule	B
can	O
eventually	O
learn	O
the	O
optimal	O
decision	O
from	O
a	O
large	O
amount	O
of	O
training	O
data	O
with	O
high	O
probability	O
.	O
strong	B
con	O
(	O
cid:173	O
)	O
sistency	O
means	O
that	O
by	O
using	O
more	O
data	O
the	O
error	O
probability	O
gets	O
arbitrarily	O
close	O
to	O
the	O
optimum	O
for	O
every	O
training	O
sequence	O
except	O
for	O
a	O
set	O
of	O
sequences	O
that	O
has	O
zero	O
probability	O
altogether	O
.	O
a	O
decision	O
rule	B
can	O
be	O
consistent	O
for	O
a	O
certain	O
class	O
of	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
,	O
but	O
may	O
not	O
be	O
consistent	O
for	O
others	O
.	O
it	O
is	O
clearly	O
desirable	O
to	O
have	O
a	O
rule	O
that	O
is	O
consistent	O
for	O
a	O
large	O
class	O
of	O
distributions	O
.	O
since	O
in	O
many	O
situations	O
we	O
do	O
not	O
have	O
any	O
prior	O
information	O
about	O
the	O
distribution	B
,	O
it	O
is	O
essential	O
to	O
have	O
a	O
rule	O
that	O
gives	O
good	O
performance	O
for	O
all	O
distributions	O
.	O
this	O
very	O
strong	B
requirement	O
of	O
universal	O
goodness	O
is	O
formulated	O
as	O
follows	O
:	O
definition	O
6.2	O
.	O
(	O
universal	B
consistency	I
)	O
.	O
a	O
sequence	O
of	O
decision	O
rules	O
is	O
called	O
universally	O
(	O
strongly	O
)	O
consistent	O
if	O
it	O
is	O
(	O
strongly	O
)	O
consistent	O
for	O
any	O
distribution	B
of	O
the	O
pair	O
(	O
x	O
,	O
y	O
)	O
.	O
in	O
this	O
chapter	O
we	O
show	O
that	O
such	O
universally	O
consistent	O
classification	O
rules	O
exist	O
.	O
at	O
first	O
,	O
this	O
may	O
seem	O
very	O
surprising	O
,	O
for	O
some	O
distributions	O
are	O
very	O
``	O
strange	O
,	O
''	O
and	O
seem	O
hard	O
to	O
learn	O
.	O
for	O
example	O
,	O
let	O
x	O
be	O
uniformly	O
distributed	O
on	O
[	O
0	O
,	O
1	O
]	O
with	O
probability	O
1/2	O
,	O
and	O
let	O
x	O
be	O
atomic	O
on	O
the	O
rationals	O
with	O
probability	O
1/2	O
.	O
for	O
example	O
,	O
if	O
the	O
rationals	O
are	O
enumerated	O
rl	O
,	O
r2	O
,	O
r3	O
,	O
...	O
,	O
then	O
p	O
{	O
x	O
=	O
rd	O
=	O
1/2i+l	O
.	O
let	O
y	O
=	O
1	O
if	O
x	O
is	O
rational	O
and	O
y	O
=	O
°	O
if	O
x	O
is	O
irrational	O
.	O
obviously	O
,	O
l	O
*	O
=	O
0.	O
if	O
a	O
classification	O
rule	B
gn	O
is	O
consistent	O
,	O
then	O
the	O
probability	O
of	O
incorrectly	O
guessing	O
the	O
rationality	O
of	O
x	O
tends	O
to	O
zero	O
.	O
note	O
here	O
that	O
we	O
can	O
not	O
``	O
check	O
''	O
whether	O
x	O
is	O
rational	O
or	O
not	O
,	O
but	O
we	O
should	O
base	O
our	O
decision	O
solely	O
on	O
the	O
data	O
dn	O
given	O
to	O
us	O
.	O
one	O
consistent	B
rule	I
is	O
the	O
following	O
:	O
gn	O
(	O
x	O
,	O
dn	O
)	O
=	O
yk	O
if	O
x	O
k	O
is	O
the	O
closest	O
point	O
to	O
x	O
among	O
xl	O
,	O
''	O
''	O
x	O
n	O
•	O
the	O
fact	O
that	O
the	O
rationals	O
are	O
dense	O
in	O
[	O
0	O
,	O
1	O
]	O
makes	O
the	O
statement	O
even	O
more	O
surprising	O
.	O
see	O
problem	O
6.3	O
.	O
6.2	O
classification	O
and	O
regression	O
estimation	O
in	O
this	O
section	O
we	O
show	O
how	O
consistency	B
of	O
classification	O
rules	O
can	O
be	O
deduced	O
from	O
consistent	O
regression	O
estimation	B
.	O
in	O
many	O
cases	O
the	O
a	B
posteriori	I
probability	I
rj	O
(	O
x	O
)	O
is	O
estimated	O
from	O
the	O
training	O
data	O
dn	O
by	O
some	O
function	O
rjn	O
(	O
x	O
)	O
=	O
rjn	O
(	O
x	O
,	O
dn	O
)	O
.	O
6.2	O
classification	O
and	O
regression	O
estimation	O
93	O
in	O
this	O
case	O
,	O
the	O
error	O
probability	O
l	O
(	O
gn	O
)	O
=	O
p	O
{	O
gn	O
(	O
x	O
)	O
=i	O
yidn	O
}	O
of	O
the	O
plug-in	O
rule	O
0	O
if	O
rjn	O
(	O
x	O
)	O
:	O
s	O
1/2	O
gn	O
(	O
x	O
)	O
=	O
1	O
otherwise	O
{	O
is	O
a	O
random	O
variable	B
.	O
then	O
a	O
simple	O
corollary	O
of	O
theorem	O
2.2	O
is	O
as	O
follows	O
:	O
corollary	O
6.1.	O
the	O
error	O
probability	O
of	O
the	O
classifier	B
gn	O
(	O
x	O
)	O
defined	O
above	O
satis	O
(	O
cid:173	O
)	O
fies	O
the	O
inequality	B
the	O
next	O
corollary	O
follows	O
from	O
the	O
cauchy-schwarz	O
inequality	B
.	O
corollary	O
6.2.	O
if	O
{	O
°	O
if	O
rjn	O
(	O
x	O
)	O
:	O
s	O
1/2	O
h	O
ot	O
erwise	O
,	O
gn	O
(	O
x	O
)	O
=	O
then	O
its	O
error	O
probability	O
satisfies	O
clearly	O
,	O
rj	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
iix	O
=	O
x	O
}	O
=	O
e	O
{	O
yix	O
=	O
x	O
}	O
is	O
just	O
the	O
regression	B
function	I
of	O
y	O
on	O
x.	O
therefore	O
,	O
the	O
most	O
interesting	O
consequence	O
of	O
theorem	O
2.2	O
is	O
that	O
the	O
mere	O
existence	O
of	O
a	O
regression	O
function	O
estimate	B
rjn	O
(	O
x	O
)	O
for	O
which	O
in	O
probability	O
or	O
with	O
probability	O
one	O
implies	O
that	O
the	O
plug-in	B
decision	I
rule	O
gn	O
is	O
consistent	O
or	O
strongly	O
consistent	O
,	O
respectively	O
.	O
clearly	O
,	O
from	O
theorem	B
2.3	O
,	O
one	O
can	O
arrive	O
at	O
a	O
conclusion	O
analogous	O
to	O
corollary	O
6.1	O
when	O
the	O
probabilities	O
rjo	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
0ix	O
=	O
x	O
}	O
and	O
rjl	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
iix	O
=	O
x	O
}	O
are	O
estimated	O
from	O
data	O
separately	O
by	O
some	O
rjo	O
,	O
n	O
and	O
rjl	O
,	O
n	O
,	O
respectively	O
.	O
usually	O
,	O
a	O
key	O
part	O
of	O
proving	O
consistency	B
of	O
classification	O
rules	O
is	O
writing	O
the	O
rules	O
in	O
one	O
of	O
the	O
plug-in	O
forms	O
,	O
and	O
showing	O
l1-convergence	O
of	O
the	O
approximating	O
functions	O
to	O
the	O
a	O
posteriori	O
probabilities	O
.	O
here	O
we	O
have	O
some	O
freedom	O
,	O
as	O
for	O
any	O
positive	O
function	O
in	O
(	O
x	O
)	O
,	O
we	O
have	O
,	O
for	O
example	O
,	O
g	O
(	O
x	O
)	O
_	O
{	O
o	O
n	O
-	O
if	O
rjl	O
,	O
n	O
(	O
x	O
)	O
:	O
s	O
rjo	O
,	O
n	O
(	O
x	O
)	O
=	O
{	O
0	O
1	O
otherwise	O
,	O
if	O
l	O
)	O
l	O
,	O
n	O
(	O
x	O
)	O
<	O
r	O
]	O
o	O
,	O
n	O
(	O
x	O
)	O
tn	O
(	O
x	O
)	O
tn	O
(	O
x	O
)	O
-	O
otherwise	O
.	O
94	O
6.	O
consistency	B
6.3	O
partitioning	B
rules	I
many	O
important	O
classification	O
rules	O
partition	B
nd	O
into	O
disjoint	O
cells	O
ai	O
,	O
a	O
2	O
,	O
...	O
and	O
classify	O
in	O
each	O
cell	O
according	O
to	O
the	O
majority	B
vote	I
among	O
the	O
labels	O
of	O
the	O
xi	O
's	O
falling	O
in	O
the	O
same	O
cell	O
.	O
more	O
precisely	O
,	O
(	O
x	O
)	O
=	O
{	O
o	O
gn	O
1	O
otherwise	O
,	O
if	O
:	O
l7=1	O
i	O
{	O
y	O
;	O
=l	O
}	O
i	O
{	O
x	O
;	O
ea	O
(	O
x	O
)	O
}	O
~	O
:	O
l7=1	O
i	O
{	O
y	O
;	O
=o	O
}	O
i	O
{	O
x	O
;	O
ea	O
(	O
x	O
)	O
}	O
where	O
a	O
(	O
x	O
)	O
denotes	O
the	O
cell	O
containing	O
x.	O
the	O
decision	O
is	O
zero	O
if	O
the	O
number	O
of	O
ones	O
does	O
not	O
exceed	O
the	O
number	O
of	O
zeros	O
in	O
the	O
cell	O
where	O
x	O
falls	O
,	O
and	O
vice	O
versa	O
.	O
the	O
partitions	O
we	O
consider	O
in	O
this	O
section	O
may	O
change	O
with	O
n	O
,	O
and	O
they	O
may	O
also	O
depend	O
on	O
the	O
points	O
xi	O
,	O
...	O
,	O
x	O
n	O
,	O
but	O
we	O
assume	O
that	O
the	O
labels	O
do	O
not	O
playa	O
role	O
in	O
constructing	O
the	O
partition	B
.	O
the	O
next	O
theorem	B
is	O
a	O
general	O
consistency	B
result	O
for	O
such	O
partitioning	B
rules	I
.	O
it	O
requires	O
two	O
properties	O
of	O
the	O
partition	B
:	O
first	O
,	O
cells	O
should	O
be	O
small	O
enough	O
so	O
that	O
local	O
changes	O
of	O
the	O
distribution	B
can	O
be	O
detected	O
.	O
on	O
the	O
other	O
hand	O
,	O
cells	O
should	O
be	O
large	O
enough	O
to	O
contain	O
a	O
large	O
number	O
of	O
points	O
so	O
that	O
averaging	O
among	O
the	O
labels	O
is	O
effective	O
.	O
diam	O
(	O
a	O
)	O
denotes	O
the	O
diameter	O
of	O
a	O
set	O
a	O
,	O
that	O
is	O
,	O
diam	O
(	O
a	O
)	O
=	O
sup	O
iix	O
-	O
yii	O
.	O
x	O
,	O
yea	O
let	O
n	O
(	O
x	O
)	O
=	O
n/ln	O
(	O
a	O
(	O
x	O
»	O
=	O
l	O
i	O
{	O
x	O
;	O
ea	O
(	O
x	O
)	O
}	O
n	O
i=1	O
denote	O
the	O
number	O
of	O
xi	O
's	O
falling	O
in	O
the	O
same	O
cell	O
as	O
x.	O
the	O
conditions	O
of	O
the	O
theorem	B
below	O
require	O
that	O
a	O
random	O
cell-selected	O
according	O
to	O
the	O
distribution	B
of	O
x	O
-has	O
a	O
small	O
diameter	O
,	O
and	O
contains	O
many	O
points	O
with	O
large	O
probability	O
.	O
theorem	B
6.1.	O
consider	O
a	O
partitioning	O
classification	O
rule	B
as	O
defined	O
above	O
.	O
then	O
e	O
{	O
l	O
n	O
}	O
-+	O
l	O
*	O
if	O
(	O
1	O
)	O
diam	O
(	O
a	O
(	O
x	O
»	O
-+	O
0	O
in	O
probability	O
,	O
(	O
2	O
)	O
n	O
(	O
x	O
)	O
-+	O
00	O
in	O
probability	O
.	O
proof	O
.	O
define	O
1j	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
llx	O
=	O
x	O
}	O
.	O
from	O
corollary	O
6.1	O
we	O
recall	O
that	O
we	O
need	O
only	O
show	O
eh1jn	O
(	O
x	O
)	O
-	O
1j	O
(	O
x	O
)	O
1	O
}	O
-+	O
0	O
,	O
where	O
__	O
1jn	O
(	O
x	O
)	O
=	O
-	O
1	O
'	O
''	O
l-	O
n	O
(	O
x	O
)	O
.	O
ex	O
;	O
ea	O
(	O
x	O
)	O
yi	O
•	O
introduce	O
ij	O
(	O
x	O
)	O
=	O
e	O
{	O
1j	O
(	O
x	O
)	O
ix	O
e	O
a	O
(	O
x	O
)	O
}	O
.	O
by	O
the	O
triangle	O
inequality	B
,	O
e	O
{	O
lryn	O
(	O
x	O
)	O
-	O
1j	O
(	O
x	O
)	O
1	O
}	O
~	O
e	O
{	O
lryn	O
(	O
x	O
)	O
-	O
ij	O
(	O
x	O
)	O
1	O
}	O
+	O
e	O
{	O
lij	O
(	O
x	O
)	O
-	O
1j	O
(	O
x	O
)	O
i	O
}	O
·	O
by	O
conditioning	O
on	O
the	O
random	O
variable	B
n	O
(	O
x	O
)	O
,	O
it	O
is	O
easy	O
to	O
see	O
that	O
n	O
(	O
x	O
)	O
ryn	O
(	O
x	O
)	O
is	O
distributed	O
as	O
b	O
(	O
n	O
(	O
x	O
)	O
,	O
ij	O
(	O
x	O
»	O
,	O
a	O
binomial	O
random	O
variable	B
with	O
parameters	O
n	O
(	O
x	O
)	O
and	O
ij	O
(	O
x	O
)	O
.	O
thus	O
,	O
6.4	O
the	O
histogram	B
rule	I
95	O
e	O
{	O
i71n	O
(	O
x	O
)	O
-	O
ij	O
(	O
x	O
)	O
iix	O
,	O
i	O
{	O
xl	O
ea	O
(	O
x	O
)	O
}	O
,	O
•••	O
,	O
i	O
{	O
xilea	O
(	O
x	O
)	O
}	O
}	O
i	O
}	O
:	O
:	O
:	O
:	O
e	O
{	O
i	O
b	O
(	O
n	O
(	O
x	O
)	O
,	O
ij	O
(	O
x	O
»	O
n	O
(	O
x	O
)	O
+	O
i	O
{	O
n	O
(	O
x	O
)	O
=o	O
}	O
i	O
-	O
-	O
11	O
(	O
x	O
)	O
i	O
{	O
n	O
(	O
x	O
»	O
o	O
}	O
x	O
,	O
i	O
{	O
x	O
1ea	O
(	O
x	O
)	O
}	O
,	O
•••	O
,	O
i	O
{	O
xilea	O
(	O
x	O
)	O
}	O
ij	O
(	O
x	O
)	O
(	O
l	O
-	O
ij	O
(	O
x	O
»	O
n	O
(	O
x	O
)	O
i	O
i	O
{	O
n	O
(	O
x	O
»	O
o	O
}	O
x	O
,	O
i	O
{	O
x	O
]	O
ea	O
(	O
x	O
)	O
}	O
•	O
.••	O
,	O
i	O
{	O
xilea	O
(	O
x	O
)	O
}	O
}	O
:	O
:	O
:	O
:	O
e	O
{	O
+	O
i	O
{	O
n	O
(	O
x	O
)	O
=o	O
}	O
by	O
the	O
cauchy-schwarz	O
inequality	B
.	O
taking	O
expectations	O
,	O
we	O
see	O
that	O
e	O
{	O
i71n	O
(	O
x	O
)	O
-	O
ij	O
(	O
x	O
)	O
1	O
}	O
:	O
:	O
:	O
:	O
e	O
{	O
~iin	O
(	O
x	O
»	O
oi	O
}	O
+	O
p	O
{	O
n	O
(	O
x	O
)	O
=	O
o	O
}	O
2	O
n	O
(	O
x	O
)	O
1	O
ip	O
{	O
n	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
k	O
}	O
+	O
2.jk	O
+	O
p	O
{	O
n	O
(	O
x	O
)	O
=	O
o	O
}	O
1	O
:	O
:	O
:	O
:	O
for	O
any	O
k	O
,	O
and	O
this	O
can	O
be	O
made	O
small	O
,	O
first	O
by	O
choosing	O
k	O
large	O
enough	O
and	O
then	O
by	O
using	O
condition	O
(	O
2	O
)	O
.	O
for	O
e	O
>	O
0	O
,	O
find	O
a	O
uniformly	O
continuous	O
[	O
0	O
,	O
i	O
]	O
-valued	O
function	O
11e	O
on	O
a	O
bounded	O
set	O
c	O
and	O
vanishing	O
off	O
c	O
so	O
that	O
e	O
{	O
111e	O
(	O
x	O
)	O
-	O
11	O
(	O
x	O
)	O
1	O
}	O
<	O
e.	O
next	O
,	O
we	O
employ	O
the	O
triangle	O
inequality	B
:	O
e	O
{	O
lij	O
(	O
x	O
)	O
-	O
11	O
(	O
x	O
)	O
l	O
}	O
:	O
:	O
:	O
:	O
e	O
{	O
lij	O
(	O
x	O
)	O
-	O
ije	O
(	O
x	O
)	O
1	O
}	O
+e	O
{	O
lije	O
(	O
x	O
)	O
-11e	O
(	O
x	O
)	O
1	O
}	O
+	O
e	O
{	O
111e	O
(	O
x	O
)	O
-	O
11	O
(	O
x	O
)	O
\	O
}	O
i	O
+	O
i	O
i	O
+	O
i	O
i	O
i	O
,	O
where	O
ije	O
(	O
x	O
)	O
=	O
e	O
{	O
11ax	O
)	O
ix	O
e	O
a	O
(	O
x	O
)	O
}	O
.	O
clearly	O
,	O
i	O
i	O
i	O
<	O
e	O
by	O
choice	O
of	O
11e	O
'	O
since	O
11e	O
is	O
uniformly	O
continuous	O
,	O
we	O
can	O
find	O
a	O
8	O
=	O
8	O
(	O
e	O
)	O
>	O
°	O
such	O
that	O
i	O
i	O
:	O
:	O
:	O
:	O
e	O
+	O
p	O
{	O
diam	O
(	O
a	O
(	O
x	O
»	O
>	O
8	O
}	O
.	O
therefore	O
,	O
i	O
i	O
<	O
2e	O
for	O
n	O
large	O
enough	O
,	O
by	O
condition	O
(	O
l	O
)	O
.	O
finally	O
,	O
i	O
taken	O
together	O
these	O
steps	O
prove	O
the	O
theorem	B
.	O
0	O
:	O
:	O
:	O
:	O
i	O
i	O
i	O
<	O
e.	O
6.4	O
the	O
histogram	B
rule	I
in	O
this	O
section	O
we	O
describe	O
the	O
cubic	B
histogram	O
rule	B
and	O
show	O
its	O
universal	B
con	O
(	O
cid:173	O
)	O
sistency	O
by	O
checking	O
the	O
conditions	O
of	O
theorem	O
6.1.	O
the	O
rule	B
partitions	O
rd	O
into	O
96	O
6.	O
consistency	B
cubes	O
of	O
the	O
same	O
size	O
,	O
and	O
makes	O
a	O
decision	O
according	O
to	O
the	O
majority	B
vote	I
among	O
the	O
y/s	O
such	O
that	O
the	O
corresponding	O
xi	O
falls	O
in	O
the	O
same	O
cube	O
as	O
x.	O
formally	O
,	O
let	O
p	O
n	O
=	O
{	O
ani	O
,	O
a	O
n2	O
,	O
...	O
}	O
beapartitionofrd	O
into	O
cubes	O
ofsizeh	O
n	O
>	O
0	O
,	O
that	O
is	O
,	O
into	O
sets	O
of	O
the	O
type	O
nf=1	O
[	O
kih	O
n	O
,	O
(	O
ki	O
+	O
1	O
)	O
hn	O
)	O
,	O
where	O
the	O
k/s	O
are	O
integers	O
.	O
for	O
every	O
x	O
e	O
rd	O
let	O
an	O
(	O
x	O
)	O
=	O
ani	O
if	O
x	O
e	O
ani	O
.	O
the	O
histogram	B
rule	I
is	O
defined	O
by	O
(	O
)	O
{	O
gn	O
x	O
=	O
0	O
if	O
i:7=1	O
i	O
{	O
yi=l	O
}	O
i	O
{	O
xiean	O
(	O
x	O
)	O
}	O
:	O
:	O
:	O
:	O
;	O
i:7=1	O
i	O
{	O
yi=o	O
}	O
i	O
{	O
xiean	O
(	O
x	O
)	O
}	O
1	O
otherwise	O
.	O
figure	O
6.1.	O
a	O
cubic	O
histogram	B
rule	I
:	O
the	O
decision	O
is	O
1	O
in	O
the	O
shaded	O
area	O
.	O
oe	O
0	O
e	O
o	O
consistency	B
of	O
the	O
histogram	B
rule	I
was	O
established	O
under	O
some	O
additional	O
con	O
(	O
cid:173	O
)	O
ditions	O
by	O
glick	O
(	O
1973	O
)	O
.	O
universal	B
consistency	I
follows	O
from	O
the	O
results	O
of	O
gordon	O
and	O
olshen	O
(	O
1978	O
)	O
,	O
(	O
1980	O
)	O
.	O
a	O
direct	O
proof	O
of	O
strong	O
universal	B
consistency	I
is	O
given	O
in	O
chapter	O
9.	O
the	O
next	O
theorem	B
establishes	O
universal	B
consistency	I
of	O
certain	O
cubic	B
histogram	O
rules	O
.	O
theorem	B
6.2.	O
if	O
hn	O
rule	B
is	O
universally	O
consistent	O
.	O
--	O
-7	O
>	O
-	O
0	O
and	O
nh~	O
--	O
-7	O
>	O
-	O
00	O
as	O
n	O
--	O
-7	O
>	O
-	O
00	O
,	O
then	O
the	O
cubic	B
histogram	O
proof	O
.	O
we	O
check	O
the	O
two	O
simple	O
conditions	O
of	O
theorem	O
6.1.	O
clearly	O
,	O
the	O
diameter	O
of	O
each	O
cell	O
is	O
-y'dhd	O
.	O
therefore	O
condition	O
(	O
1	O
)	O
follows	O
trivially	O
.	O
to	O
show	O
condition	O
(	O
2	O
)	O
,	O
we	O
need	O
to	O
prove	O
that	O
for	O
any	O
m	O
<	O
00	O
,	O
p	O
{	O
n	O
(	O
x	O
)	O
:	O
:	O
:	O
m	O
}	O
--	O
-7	O
>	O
-	O
o.	O
let	O
s	O
be	O
an	O
arbitrary	O
ball	O
centered	O
at	O
the	O
origin	O
.	O
then	O
the	O
number	O
of	O
cells	O
intersecting	O
s	O
is	O
not	O
more	O
than	O
ci	O
+	O
c21	O
hd	O
for	O
some	O
positive	O
constants	O
ci	O
,	O
c2	O
.	O
then	O
p	O
{	O
n	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
;	O
m	O
}	O
<	O
l	O
p	O
{	O
x	O
e	O
a	O
nj	O
,	O
n	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
;	O
m	O
}	O
+	O
p	O
{	O
x	O
esc	O
}	O
j	O
:	O
a	O
nj	O
ns=j0	O
<	O
j	O
:	O
a	O
nj	O
ns10	O
fj.	O
,	O
(	O
a	O
nj	O
)	O
:	O
s2mjn	O
j	O
:	O
a	O
nj	O
ns10	O
fj.	O
,	O
(	O
a	O
nj	O
»	O
2mjn	O
6.5	O
stone	O
's	O
theorem	B
97	O
j	O
:	O
a	O
,	O
,/lsi	O
0	O
fl	O
,	O
(	O
a	O
nj	O
»	O
2m/n	O
(	O
by	O
chebyshev	O
's	O
inequality	B
)	O
j	O
:	O
a	O
nj	O
nsi	O
0	O
fl	O
,	O
(	O
a	O
nj	O
»	O
2m/n	O
c2	O
)	O
2m	O
+	O
4	O
(	O
--	O
n-	O
cl	O
+	O
hd	O
+	O
!	O
l	O
(	O
s	O
)	O
!	O
l	O
(	O
sc	O
)	O
,	O
c	O
<	O
<	O
because	O
nh	O
d	O
--	O
+	O
00.	O
since	O
s	O
is	O
arbitrary	O
,	O
the	O
proof	O
of	O
the	O
theorem	B
is	O
complete	O
.	O
0	O
6.5	O
stone	O
's	O
theorem	B
a	O
general	O
theorem	B
by	O
stone	O
(	O
1977	O
)	O
allows	O
us	O
to	O
deduce	O
universal	B
consistency	I
of	O
several	O
classification	O
rules	O
.	O
consider	O
a	O
rule	O
based	O
on	O
an	O
estimate	B
of	O
the	O
a	B
posteriori	I
probability	I
1	O
]	O
of	O
the	O
form	O
n	O
1	O
]	O
n	O
(	O
x	O
)	O
=	O
l	O
i	O
{	O
yi=l	O
}	O
wni	O
(	O
x	O
)	O
=	O
lyi	O
wni	O
(	O
x	O
)	O
,	O
n	O
where	O
the	O
weights	O
wni	O
(	O
x	O
)	O
=	O
wni	O
(	O
x	O
,	O
xl	O
,	O
...	O
,	O
xn	O
)	O
are	O
nonnegative	O
and	O
sum	O
to	O
one	O
:	O
i=l	O
i=l	O
i=l	O
the	O
classification	O
rule	B
is	O
defined	O
as	O
gn	O
x	O
(	O
)	O
0	O
{	O
=	O
=	O
{	O
o	O
if	O
l7	O
:	O
:	O
:	O
:1	O
i	O
{	O
yi=l	O
}	O
wni	O
(	O
x	O
)	O
:	O
:	O
:	O
l7=1	O
i	O
{	O
yi=o	O
}	O
wni	O
(	O
x	O
)	O
otherwise	O
,	O
if	O
l7=1	O
yi	O
wni	O
(	O
x	O
)	O
:	O
:	O
:	O
1/2	O
otherwise	O
.	O
98	O
6.	O
consistency	B
17n	O
is	O
a	O
weighted	O
average	O
estimator	O
of	O
17.	O
it	O
is	O
intuitively	O
clear	O
that	O
pairs	O
(	O
xi	O
,	O
yi	O
)	O
such	O
that	O
xi	O
is	O
close	O
to	O
x	O
should	O
provide	O
more	O
information	O
about	O
17	O
(	O
x	O
)	O
than	O
those	O
far	O
from	O
x.	O
thus	O
,	O
the	O
weights	O
are	O
typically	O
much	O
larger	O
in	O
the	O
neighborhood	O
of	O
x	O
,	O
so	O
17n	O
is	O
roughly	O
a	O
(	O
weighted	B
)	O
relative	O
frequency	O
of	O
the	O
xi	O
's	O
that	O
have	O
label	O
1	O
among	O
points	O
in	O
the	O
neighborhood	O
of	O
x.	O
thus	O
,	O
17n	O
might	O
be	O
viewed	O
as	O
a	O
local	O
average	O
estimator	O
,	O
and	O
gn	O
a	O
local	O
(	O
weighted	B
)	O
majority	B
vote	I
.	O
examples	O
of	O
such	O
rules	O
include	O
the	O
histogram	O
,	O
kernel	B
,	O
and	O
nearest	B
neighbor	I
rules	I
.	O
these	O
rules	O
will	O
be	O
studied	O
in	O
depth	O
later	O
.	O
theorem	B
6.3.	O
satisfy	O
the	O
following	O
three	O
conditions	O
:	O
(	O
stone	O
(	O
1977	O
»	O
.	O
assume	O
that	O
for	O
any	O
distribution	B
of	O
x	O
,	O
the	O
weights	O
(	O
i	O
)	O
there	O
is	O
a	O
constant	O
c	O
such	O
that	O
,	O
for	O
every	O
nonnegative	O
measurable	O
function	O
f	O
satisfying	O
ef	O
(	O
x	O
)	O
<	O
00	O
,	O
e	O
{	O
t	O
wni	O
(	O
x	O
)	O
f	O
(	O
x	O
,	O
)	O
}	O
s	O
cef	O
(	O
x	O
)	O
.	O
(	O
ii	O
)	O
for	O
all	O
a	O
>	O
0	O
,	O
(	O
iii	O
)	O
lim	O
e	O
i	O
m~x	O
wni	O
(	O
x	O
)	O
}	O
=	O
0	O
.	O
1:9	O
:	O
:	O
:	O
n	O
n	O
--	O
-+oo	O
then	O
gn	O
is	O
universally	O
consistent	O
.	O
remark	O
.	O
condition	O
(	O
ii	O
)	O
requires	O
that	O
the	O
overall	O
weight	O
of	O
x/s	O
outside	O
of	O
any	O
ball	O
of	O
a	O
fixed	O
radius	O
centered	O
at	O
x	O
must	O
go	O
to	O
zero	O
.	O
in	O
other	O
words	O
,	O
only	O
points	O
in	O
a	O
shrinking	O
neighborhood	O
of	O
x	O
should	O
be	O
taken	O
into	O
account	O
in	O
the	O
averaging	O
.	O
condition	O
(	O
iii	O
)	O
requires	O
that	O
no	O
single	O
xi	O
has	O
too	O
large	O
a	O
contribution	O
to	O
the	O
estimate	B
.	O
hence	O
,	O
the	O
number	O
of	O
points	O
encountered	O
in	O
the	O
averaging	O
must	O
tend	O
to	O
infinity	O
.	O
condition	O
(	O
i	O
)	O
is	O
technical	O
.	O
0	O
proof	O
.	O
by	O
corollary	O
6.2	O
it	O
suffices	O
to	O
show	O
that	O
for	O
every	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
introduce	O
the	O
notation	O
lim	O
e	O
{	O
(	O
17	O
(	O
x	O
)	O
-	O
17n	O
(	O
x	O
»	O
2	O
}	O
=	O
0.	O
n	O
--	O
-+oo	O
n	O
r	O
]	O
n	O
(	O
x	O
)	O
=	O
l	O
17	O
(	O
xi	O
)	O
wni	O
(	O
x	O
)	O
.	O
i=l	O
6.5	O
stone	O
's	O
theorem	B
99	O
then	O
by	O
the	O
simple	O
inequality	O
(	O
a	O
+	O
b	O
)	O
2	O
:	O
:	O
:	O
2	O
(	O
a2	O
+	O
b2	O
)	O
we	O
have	O
e	O
{	O
(	O
ry	O
(	O
x	O
)	O
-	O
ryn	O
(	O
x	O
»	O
2	O
}	O
=	O
e	O
{	O
(	O
(	O
ry	O
(	O
x	O
)	O
-	O
iln	O
(	O
x	O
»	O
+	O
(	O
li'n	O
(	O
x	O
)	O
-	O
ryn	O
(	O
x	O
»	O
)	O
2	O
}	O
:	O
:	O
:	O
2	O
(	O
e	O
{	O
(	O
ry	O
(	O
x	O
)	O
-	O
tfn	O
(	O
x	O
»	O
2	O
}	O
+	O
e	O
{	O
(	O
tfn	O
(	O
x	O
)	O
-	O
ryn	O
(	O
x	O
»	O
2	O
}	O
)	O
.	O
(	O
6.1	O
)	O
therefore	O
,	O
it	O
is	O
enough	O
to	O
show	O
that	O
both	O
terms	O
on	O
the	O
right-hand	O
side	O
tend	O
to	O
zero	O
.	O
since	O
the	O
wni	O
's	O
are	O
nonnegative	O
and	O
sum	O
to	O
one	O
,	O
by	O
jensen	O
's	O
inequality	B
,	O
the	O
first	O
term	O
is	O
e	O
{	O
(	O
t	O
wni	O
(	O
x	O
)	O
(	O
~	O
(	O
x	O
)	O
-	O
~	O
(	O
xi	O
)	O
)	O
r	O
}	O
:	O
s	O
:	O
e	O
{	O
t	O
wni	O
(	O
x	O
)	O
(	O
~	O
(	O
x	O
)	O
-	O
~	O
(	O
xi	O
)	O
l	O
''	O
}	O
.	O
if	O
the	O
function	O
0	O
:	O
:	O
:	O
ry*	O
:	O
:	O
:	O
1	O
is	O
continuous	O
with	O
bounded	O
support	B
,	O
then	O
it	O
is	O
uniformly	O
continuous	O
as	O
well	O
:	O
for	O
every	O
e	O
>	O
0	O
,	O
there	O
is	O
an	O
a	O
>	O
0	O
such	O
that	O
for	O
ilxi	O
-	O
x	O
ii	O
<	O
a	O
,	O
iry*	O
(	O
xd	O
_ry*	O
(	O
x	O
)	O
1	O
2	O
<	O
e.	O
recall	O
here	O
that	O
ilxli	O
denotes	O
the	O
euc1idean	O
norm	O
ofa	O
vector	O
x	O
e	O
rd	O
.	O
thus	O
,	O
since	O
iry*	O
(	O
xd	O
-	O
ry*	O
(	O
x	O
)	O
/	O
:	O
:	O
:	O
1	O
,	O
e	O
{	O
t	O
wni	O
(	O
x	O
)	O
(	O
~	O
'	O
(	O
x	O
)	O
-	O
ry	O
'	O
(	O
xi	O
»	O
'	O
}	O
:	O
s	O
:	O
e	O
{	O
t	O
wn	O
;	O
(	O
x	O
)	O
j	O
(	O
iix-x	O
;	O
li~a	O
)	O
}	O
+	O
e	O
{	O
t	O
wni	O
(	O
x	O
)	O
e	O
}	O
--	O
+	O
e	O
,	O
by	O
(	O
ii	O
)	O
.	O
since	O
the	O
set	O
of	O
continuous	O
functions	O
with	O
bounded	O
support	B
is	O
dense	O
in	O
l2	O
(	O
/l	O
)	O
,	O
for	O
every	O
e	O
>	O
0	O
we	O
can	O
choose	O
ry*	O
such	O
that	O
e	O
{	O
(	O
ry	O
(	O
x	O
)	O
-	O
ry*	O
(	O
x	O
»	O
2	O
}	O
<	O
e.	O
by	O
this	O
choice	O
,	O
using	O
the	O
inequality	B
(	O
a	O
+	O
b	O
+	O
c	O
?	O
:	O
:	O
:	O
3	O
(	O
a2	O
+	O
b2	O
+	O
c2	O
)	O
(	O
which	O
follows	O
from	O
the	O
cauchy-schwarz	O
inequality	B
)	O
,	O
e	O
{	O
(	O
ry	O
(	O
x	O
)	O
-	O
tfn	O
(	O
x	O
)	O
/	O
}	O
:	O
s	O
:	O
e	O
{	O
t	O
wni	O
(	O
x	O
)	O
(	O
ry	O
(	O
x	O
)	O
-	O
:	O
s	O
:	O
3e	O
{	O
t	O
wni	O
(	O
x	O
)	O
(	O
~	O
(	O
x	O
)	O
-	O
~	O
'	O
(	O
x	O
»	O
2	O
+	O
(	O
ry	O
'	O
(	O
x	O
)	O
-	O
~	O
'	O
(	O
xi	O
»	O
2	O
ry	O
(	O
xi	O
)	O
)	O
2	O
}	O
+	O
(	O
ry*	O
(	O
xi	O
)	O
-	O
ry	O
(	O
xd	O
)	O
2	O
)	O
}	O
:	O
:	O
:	O
3e	O
{	O
(	O
ry	O
(	O
x	O
)	O
-	O
ry*	O
(	O
x	O
»	O
2	O
}	O
+3e	O
{	O
t	O
wni	O
(	O
x	O
)	O
(	O
~	O
'	O
(	O
x	O
)	O
-	O
ry	O
'	O
(	O
xi	O
»	O
2	O
}	O
+	O
3ce	O
{	O
(	O
~	O
(	O
x	O
)	O
-	O
~*	O
(	O
x	O
»	O
2	O
}	O
.	O
100	O
6.	O
consistency	B
where	O
we	O
used	O
(	O
i	O
)	O
.	O
therefore	O
,	O
lim	O
sup	O
e	O
{	O
(	O
1j	O
(	O
x	O
)	O
-1h7	O
(	O
x	O
)	O
)	O
2	O
}	O
:	O
:	O
:	O
:	O
3e	O
(	O
1	O
+	O
1	O
+	O
c	O
)	O
.	O
n-+oo	O
to	O
handle	O
the	O
second	O
term	O
of	O
the	O
right	O
-hand	O
side	O
of	O
(	O
6.1	O
)	O
,	O
observe	O
that	O
by	O
independence	O
.	O
therefore	O
,	O
e	O
{	O
(	O
1h7	O
(	O
x	O
)	O
-	O
1jn	O
(	O
x	O
)	O
)	O
2	O
}	O
=	O
e	O
{	O
(	O
t	O
wn	O
;	O
(	O
x	O
)	O
(	O
ry	O
(	O
xi	O
)	O
-	O
yi	O
)	O
r	O
}	O
n	O
l	O
le	O
{	O
wni	O
(	O
x	O
)	O
(	O
1j	O
(	O
xi	O
)	O
-	O
yi	O
)	O
wnj	O
(	O
x	O
)	O
(	O
1j	O
(	O
x	O
j	O
)	O
-	O
yj	O
)	O
}	O
n	O
i=l	O
j=l	O
n	O
i=l	O
=	O
le	O
{	O
w	O
;	O
i	O
(	O
x	O
)	O
(	O
1j	O
(	O
xi	O
)	O
-	O
yi	O
)	O
2	O
}	O
<	O
e	O
{	O
t	O
w.	O
;	O
,	O
ex	O
)	O
}	O
:	O
:0	O
e	O
{	O
1~i	O
'	O
:	O
;	O
,	O
wni	O
(	O
x	O
)	O
~	O
wnj	O
(	O
x	O
)	O
}	O
e	O
{	O
1~~	O
;	O
''	O
wn	O
,	O
(	O
x	O
)	O
}	O
-+	O
0	O
by	O
(	O
iii	O
)	O
,	O
and	O
the	O
theorem	B
is	O
proved	O
.	O
0	O
6.6	O
the	O
k-nearest	O
neighbor	B
rule	I
in	O
chapter	O
5	O
we	O
discussed	O
asymptotic	O
properties	O
of	O
the	O
k-nearest	O
neighbor	B
rule	I
when	O
k	O
remains	O
fixed	O
as	O
the	O
sample	O
size	O
n	O
grows	O
.	O
in	O
such	O
cases	O
the	O
expected	O
probability	O
of	O
error	O
converges	O
to	O
a	O
number	O
between	O
l	O
*	O
and	O
2l	O
*	O
.	O
in	O
this	O
section	O
we	O
show	O
that	O
if	O
k	O
is	O
allowed	O
to	O
grow	O
with	O
n	O
such	O
that	O
kin	O
--	O
+	O
0	O
,	O
the	O
rule	B
is	O
weakly	O
universally	O
consistent	O
.	O
the	O
proof	O
is	O
a	O
very	O
simple	O
application	O
of	O
stone	O
's	O
theorem	B
.	O
this	O
result	O
,	O
appearing	O
in	O
stone	O
's	O
paper	O
(	O
1977	O
)	O
,	O
was	O
the	O
first	O
universal	B
consistency	I
result	O
for	O
any	O
rule	B
.	O
strong	B
consistency	I
,	O
and	O
many	O
other	O
different	O
aspects	O
of	O
the	O
k-nn	O
rule	B
are	O
studied	O
in	O
chapters	O
11	O
and	O
26.	O
recall	O
the	O
definition	B
of	I
the	O
k-nearest	O
neighbor	B
rule	I
:	O
first	O
the	O
data	O
are	O
ordered	B
according	O
to	O
increasing	O
euclidean	O
distances	O
of	O
the	O
x	O
j	O
,	O
s	O
to	O
x	O
:	O
6.7	O
classification	O
is	O
easier	O
than	O
regression	B
function	I
estimation	O
101.	O
that	O
is	O
,	O
x	O
(	O
i	O
)	O
(	O
x	O
)	O
is	O
the	O
i-th	O
nearest	B
neighbor	I
of	O
x	O
among	O
the	O
points	O
xl	O
,	O
...	O
,	O
x	O
n	O
.	O
distance	B
ties	O
are	O
broken	O
by	O
comparing	O
indices	O
,	O
that	O
is	O
,	O
in	O
case	O
of	O
ii	O
xi	O
-	O
x	O
ii	O
''	O
x	O
j	O
-	O
x	O
ii	O
,	O
xi	O
is	O
considered	O
to	O
be	O
``	O
closer	O
''	O
to	O
x	O
if	O
i	O
<	O
j.	O
the	O
k-nn	O
classification	O
rule	B
is	O
defined	O
as	O
gn	O
(	O
x	O
)	O
=	O
{	O
o	O
if	O
l~=l	O
.	O
i	O
{	O
y	O
(	O
i	O
)	O
(	O
x	O
)	O
=l	O
}	O
:	O
:	O
:	O
;	O
l~=l	O
i	O
{	O
y	O
(	O
i	O
)	O
(	O
x	O
)	O
=o	O
}	O
1	O
otherwise	O
.	O
in	O
other	O
words	O
,	O
gn	O
(	O
x	O
)	O
is	O
a	O
majority	O
vote	O
among	O
the	O
labels	O
of	O
the	O
k	O
nearest	O
neighbors	O
of	O
x.	O
theorem	B
6.4	O
.	O
(	O
stone	O
(	O
1977	O
»	O
.	O
ifk	O
-+	O
00	O
and	O
k/n	O
-+	O
0	O
,	O
thenforall	O
distributions	O
eln-+	O
l*	O
.	O
proof	O
.	O
we	O
proceed	O
by	O
checking	O
the	O
conditions	O
of	O
stone	O
's	O
weak	B
convergence	O
theo	O
(	O
cid:173	O
)	O
rem	O
(	O
theorem	B
6.3	O
)	O
.	O
the	O
weight	O
wni	O
(	O
x	O
)	O
in	O
theorem	O
6.3	O
equals	O
1/	O
k	O
iff	O
xi	O
is	O
among	O
the	O
k	O
nearest	O
neighbors	O
of	O
x	O
,	O
and	O
equals	O
°	O
otherwise	O
.	O
condition	O
(	O
iii	O
)	O
is	O
obvious	O
since	O
k	O
-+	O
00.	O
for	O
condition	O
(	O
ii	O
)	O
observe	O
that	O
holds	O
whenever	O
p	O
{	O
iix	O
(	O
k	O
)	O
(	O
x	O
)	O
-	O
xii	O
>	O
e	O
}	O
-+	O
0	O
,	O
where	O
x	O
(	O
k	O
)	O
(	O
x	O
)	O
denotes	O
the	O
k-th	O
nearest	B
neighbor	I
of	O
x	O
among	O
xl	O
,	O
...	O
,	O
x	O
n	O
•	O
but	O
we	O
know	O
from	O
lemma	O
5.1	O
that	O
this	O
is	O
true	O
for	O
all	O
e	O
>	O
°	O
whenever	O
k/n	O
-+	O
0.	O
finally	O
,	O
we	O
consider	O
condition	O
(	O
i	O
)	O
.	O
we	O
have	O
to	O
show	O
that	O
for	O
any	O
nonnegative	O
measurable	O
function	O
f	O
with	O
e	O
{	O
f	O
(	O
x	O
)	O
}	O
<	O
00	O
,	O
e	O
{	O
t	O
~i	O
(	O
xi	O
is	O
among	O
the	O
k	O
nearest	O
neighbors	O
of	O
x	O
}	O
f	O
(	O
xd	O
}	O
:	O
:	O
:	O
;	O
e	O
{	O
cf	O
(	O
x	O
)	O
}	O
1=1	O
for	O
some	O
constant	O
c.	O
but	O
we	O
have	O
shown	O
in	O
lemma	O
5.3	O
that	O
this	O
inequality	B
always	O
holds	O
with	O
c	O
=	O
yd	O
.	O
thus	O
,	O
condition	O
(	O
i	O
)	O
is	O
verified	O
.	O
0	O
6.7	O
classification	O
is	O
easier	O
than	O
regression	B
function	I
estimation	O
once	O
again	O
assume	O
that	O
our	O
decision	O
is	O
based	O
on	O
some	O
estimate	B
17n	O
of	O
the	O
a	B
posteriori	I
probability	I
function	O
17	O
,	O
that	O
is	O
,	O
(	O
x	O
)	O
=	O
{	O
°	O
if17n	O
(	O
x	O
)	O
:	O
:	O
:	O
;	O
1/2	O
1	O
otherwise	O
.	O
gn	O
102	O
6.	O
consistency	B
the	O
bounds	O
of	O
theorems	O
2.2	O
,	O
2.3	O
,	O
and	O
corollary	O
6.2	O
point	O
out	O
that	O
if	O
1	O
}	O
n	O
is	O
a	O
consistent	O
estimate	B
of	O
1	O
}	O
,	O
then	O
the	O
resulting	O
rule	B
is	O
also	O
consistent	O
.	O
for	O
example	O
,	O
writing	O
ln	O
=	O
p	O
{	O
gn	O
(	O
x	O
)	O
i	O
yidn	O
}	O
,	O
we	O
have	O
that	O
is	O
,	O
l	O
2-consistent	O
estimation	B
of	I
the	O
regression	B
function	I
1	O
}	O
leads	O
to	O
consistent	O
classification	O
,	O
and	O
in	O
fact	O
,	O
this	O
is	O
the	O
main	O
tool	O
used	O
in	O
the	O
proof	O
of	O
theorem	O
6.3.	O
while	O
the	O
said	O
bounds	O
are	O
useful	O
for	O
proving	O
consistency	B
,	O
they	O
are	O
almost	O
useless	O
when	O
it	O
comes	O
to	O
studying	O
rates	O
of	O
convergence	O
.	O
as	O
theorem	B
6.5	O
below	O
shows	O
,	O
for	O
consistent	O
rules	O
rates	O
of	O
convergence	O
of	O
p	O
{	O
gn	O
(	O
x	O
)	O
i	O
y	O
}	O
to	O
l	O
*	O
are	O
always	O
orders	O
of	O
magnitude	O
better	O
than	O
rates	O
of	O
convergence	O
of	O
je	O
{	O
(	O
1	O
}	O
(	O
x	O
)	O
-	O
1	O
}	O
n	O
(	O
x	O
»	O
2	O
}	O
to	O
zero	O
.	O
112	O
___________________	O
...	O
o========~	O
__________________________________________	O
_	O
x	O
figure	O
6.2.	O
the	O
difference	O
between	O
the	O
error	O
probabilities	O
grows	O
roughly	O
in	O
proportion	O
to	O
the	O
shaded	O
area	O
.	O
elsewhere	O
1	O
}	O
n	O
(	O
x	O
)	O
does	O
not	O
need	O
to	O
be	O
close	O
1	O
}	O
(	O
x	O
)	O
.	O
pattern	O
recognition	O
is	O
thus	O
easier	O
than	O
regression	B
function	I
estimation	O
.	O
this	O
will	O
be	O
a	O
recurring	O
theme-to	O
achieve	O
acceptable	O
results	O
in	O
pattern	O
recognition	O
,	O
we	O
can	O
do	O
more	O
with	O
smaller	O
sample	O
sizes	O
than	O
in	O
regression	O
function	O
estimation	B
.	O
this	O
is	O
really	O
just	O
a	O
consequence	O
of	O
the	O
fact	O
that	O
less	O
is	O
required	O
in	O
pattern	O
recognition	O
.	O
it	O
also	O
corroborates	O
our	O
belief	O
that	O
pattern	O
recognition	O
is	O
dramatically	O
different	O
from	O
regression	B
function	I
estimation	O
,	O
and	O
that	O
it	O
deserves	O
separate	O
treatment	O
in	O
the	O
statistical	O
community	O
.	O
theorem	B
6.5.	O
let	O
1	O
}	O
n	O
be	O
a	O
weakly	O
consistent	O
regression	O
estimate	B
,	O
that	O
is	O
,	O
define	O
lim	O
e	O
{	O
(	O
1jn	O
(	O
x	O
)	O
n-+oo	O
1	O
}	O
(	O
x	O
»	O
2	O
}	O
=	O
o.	O
gn	O
(	O
x	O
)	O
=	O
{	O
0	O
if1	O
}	O
n	O
(	O
x	O
)	O
:	O
:	O
:	O
:1/2	O
otherwise	O
.	O
6.7	O
classification	O
is	O
easier	O
than	O
regression	B
function	I
estimation	O
103	O
then	O
el	O
-l*	O
lim	O
n-+oo	O
)	O
e	O
{	O
(	O
fln	O
(	O
x	O
)	O
-	O
n	O
=	O
0	O
,	O
fl	O
(	O
x	O
)	O
?	O
}	O
that	O
is	O
,	O
eln	O
-	O
l	O
*	O
converges	O
to	O
zero	O
faster	O
than	O
the	O
l	O
2-error	O
of	O
the	O
regression	O
estimate	O
.	O
proof	O
.	O
we	O
start	O
with	O
the	O
equality	O
of	O
theorem	O
2.2	O
:	O
eln	O
-	O
l	O
*	O
=	O
2e	O
{	O
lfl	O
(	O
x	O
)	O
-	O
1/2ii	O
{	O
gn	O
(	O
x	O
)	O
¥g*	O
(	O
x	O
)	O
}	O
}	O
.	O
fix	O
e	O
>	O
o.	O
we	O
may	O
bound	O
the	O
last	O
factor	O
by	O
e	O
{	O
lfl	O
(	O
x	O
)	O
-	O
1/2i	O
i	O
{	O
gn	O
(	O
x	O
)	O
¥g*	O
(	O
x	O
)	O
d	O
:	O
:	O
:	O
e	O
{	O
i	O
{	O
1j	O
(	O
x	O
)	O
i1/2	O
}	O
lfl	O
(	O
x	O
)	O
-	O
fln	O
(	O
x	O
)	O
ii	O
{	O
g	O
,	O
,	O
(	O
x	O
)	O
¥g*	O
(	O
x	O
)	O
}	O
}	O
e	O
{	O
lfl	O
(	O
x	O
)	O
-	O
+	O
e	O
{	O
lfl	O
(	O
x	O
)	O
-	O
<	O
)	O
e	O
{	O
(	O
rjn	O
(	O
x	O
)	O
-	O
fln	O
(	O
x	O
)	O
1	O
i	O
{	O
gn	O
(	O
x	O
)	O
¥g*	O
(	O
x	O
)	O
}	O
i	O
{	O
i1j	O
(	O
x	O
)	O
-1/21	O
:	O
:	O
:	O
:e	O
}	O
i	O
{	O
1j	O
(	O
x	O
)	O
il/2	O
}	O
}	O
fln	O
(	O
x	O
)	O
ii	O
{	O
g	O
,	O
,	O
(	O
x	O
)	O
¥g*	O
(	O
x	O
)	O
}	O
i	O
{	O
i1j	O
(	O
x	O
)	O
-1/21	O
>	O
ed	O
fl	O
(	O
x	O
)	O
?	O
}	O
x	O
(	O
jp	O
{	O
lfl	O
(	O
x	O
)	O
-	O
1/21	O
:	O
:	O
:	O
e	O
,	O
flex	O
)	O
=jl/2	O
}	O
+	O
jp	O
{	O
gn	O
(	O
x	O
)	O
=j	O
g*	O
(	O
x	O
)	O
,	O
ifl	O
(	O
x	O
)	O
-1/21	O
>	O
e	O
}	O
)	O
(	O
by	O
the	O
cauchy-schwarz	O
inequality	B
)	O
.	O
since	O
gn	O
(	O
x	O
)	O
=j	O
g*	O
(	O
x	O
)	O
and	O
ifl	O
(	O
x	O
)	O
-	O
1/21	O
>	O
e	O
imply	O
that	O
irjn	O
(	O
x	O
)	O
-	O
consistency	B
of	O
the	O
regression	O
estimate	O
implies	O
that	O
for	O
any	O
fixed	O
e	O
>	O
0	O
,	O
fl	O
(	O
x	O
)	O
1	O
>	O
e	O
,	O
lim	O
p	O
{	O
gn	O
(	O
x	O
)	O
=j	O
g*	O
(	O
x	O
)	O
,	O
ifl	O
(	O
x	O
)	O
-	O
1/21	O
>	O
e	O
}	O
=	O
o.	O
n-+oo	O
on	O
the	O
other	O
hand	O
,	O
p	O
{	O
lfl	O
(	O
x	O
)	O
-	O
1/21	O
:	O
:	O
:	O
e	O
,	O
flex	O
)	O
=j1/2	O
}	O
--	O
-+	O
0	O
as	O
e	O
--	O
-+	O
0	O
,	O
which	O
completes	O
the	O
proof	O
.	O
0	O
the	O
actual	O
value	O
of	O
the	O
ratio	O
eln	O
-	O
l*	O
pn	O
=	O
--	O
,	O
========	O
fl	O
(	O
x	O
)	O
?	O
}	O
)	O
e	O
{	O
(	O
rjn	O
(	O
x	O
)	O
-	O
can	O
not	O
be	O
universally	O
bounded	O
.	O
in	O
fact	O
,	O
pn	O
may	O
tend	O
to	O
zero	O
arbitrarily	O
slowly	O
(	O
see	O
problem	O
6.5	O
)	O
.	O
on	O
the	O
other	O
hand	O
,	O
pn	O
may	O
tend	O
to	O
zero	O
extremely	O
quickly	O
.	O
in	O
problems	O
6.6	O
and	O
6.7	O
and	O
in	O
the	O
theorem	B
below	O
,	O
upper	O
bounds	O
on	O
pn	O
are	O
given	O
that	O
104	O
6.	O
consistency	B
may	O
be	O
used	O
in	O
deducing	O
rate-of-convergence	O
results	O
.	O
theorem	B
6.6	O
,	O
in	O
particular	O
,	O
states	O
that	O
eln	O
-	O
l	O
*	O
tends	O
to	O
zero	O
as	O
fast	O
as	O
the	O
square	O
of	O
the	O
l2	O
error	O
of	O
the	O
regression	O
estimate	O
,	O
i.e.	O
,	O
e	O
{	O
(	O
1	O
]	O
n	O
(	O
x	O
)	O
-	O
1	O
]	O
(	O
x	O
)	O
)	O
2	O
}	O
,	O
whenever	O
l*	O
=	O
o.	O
just	O
how	O
slowly	O
pn	O
tends	O
to	O
zero	O
depends	O
upon	O
two	O
things	O
,	O
basically	O
:	O
(	O
1	O
)	O
the	O
rate	B
of	I
convergence	I
of	O
1	O
]	O
n	O
to	O
1	O
]	O
,	O
and	O
(	O
2	O
)	O
the	O
behavior	O
of	O
p	O
{	O
i1	O
]	O
(	O
x	O
)	O
-	O
1/21	O
:	O
:	O
:	O
:	O
e	O
,	O
1	O
]	O
(	O
x	O
)	O
:	O
:jl/2	O
}	O
as	O
a	O
function	O
of	O
e	O
when	O
e	O
{	O
-	O
0	O
(	O
i.e.	O
,	O
the	O
behavior	O
of	O
1	O
]	O
(	O
x	O
)	O
at	O
those	O
x	O
's	O
where	O
1	O
]	O
(	O
x	O
)	O
~	O
1/2	O
)	O
.	O
theorem	B
6.6.	O
assume	O
that	O
l	O
*	O
=	O
0	O
,	O
and	O
consider	O
the	O
decision	O
(	O
x	O
)	O
=	O
{	O
o	O
gn	O
if1	O
]	O
n	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
1/2	O
1	O
otherwise	O
.	O
then	O
p	O
{	O
gn	O
(	O
x	O
)	O
:	O
:j	O
y	O
}	O
:	O
:	O
:	O
:	O
4	O
e	O
{	O
(	O
1	O
]	O
n	O
(	O
x	O
)	O
-	O
1	O
]	O
(	O
x	O
)	O
)	O
2	O
}	O
.	O
proof	O
.	O
by	O
theorem	B
2.2	O
,	O
p	O
{	O
gn	O
(	O
x	O
)	O
:	O
:j	O
y	O
}	O
=	O
2e	O
{	O
11	O
]	O
(	O
x	O
)	O
1/2ii	O
{	O
gn	O
(	O
x	O
)	O
=tg*	O
(	O
x	O
)	O
d	O
=	O
2e	O
{	O
11	O
]	O
(	O
x	O
)	O
-	O
1/2/i	O
{	O
gn	O
(	O
x	O
)	O
=ty	O
}	O
}	O
(	O
since	O
g*	O
(	O
x	O
)	O
=	O
y	O
by	O
the	O
assumption	O
l	O
*	O
=	O
0	O
)	O
<	O
2je	O
{	O
(	O
1	O
]	O
n	O
(	O
x	O
)	O
-	O
1	O
]	O
(	O
x	O
)	O
)	O
2	O
h/p	O
{	O
gn	O
(	O
x	O
)	O
:	O
:j	O
y	O
}	O
(	O
by	O
the	O
cauchy-schwarz	O
inequality	B
)	O
.	O
dividing	O
both	O
sides	O
by	O
,	O
jp	O
{	O
gn	O
(	O
x	O
)	O
:	O
:j	O
y	O
}	O
yields	O
the	O
result	O
.	O
0	O
the	O
results	O
above	O
show	O
that	O
the	O
bounds	O
of	O
theorems	O
2.2	O
,	O
2.3	O
,	O
and	O
corollary	O
6.2	O
may	O
be	O
arbitrarily	O
loose	O
,	O
and	O
the	O
error	O
probability	O
converges	O
to	O
l	O
*	O
faster	O
than	O
the	O
l	O
2-error	O
of	O
the	O
regression	O
estimate	O
converges	O
to	O
zero	O
.	O
in	O
some	O
cases	O
,	O
consistency	B
may	O
even	O
occur	O
without	O
convergence	O
of	O
ei1	O
]	O
n	O
(	O
x	O
)	O
-	O
1	O
]	O
(	O
x	O
)	O
1	O
to	O
zero	O
.	O
consider	O
for	O
example	O
a	O
strictly	O
separable	O
distribution	B
,	O
that	O
is	O
,	O
a	O
distribution	O
such	O
that	O
there	O
exist	O
two	O
sets	O
a	O
,	O
bend	O
with	O
inf	O
xea	O
,	O
yeb	O
/ix	O
y	O
ii	O
~	O
8	O
>	O
0	O
for	O
some	O
8	O
>	O
0	O
,	O
and	O
having	O
the	O
property	O
that	O
p	O
{	O
x	O
e	O
aiy	O
=	O
i	O
}	O
=p	O
{	O
x	O
e	O
biy	O
=o	O
}	O
=	O
1.	O
in	O
such	O
cases	O
,	O
there	O
is	O
a	O
version	O
of	O
1	O
]	O
that	O
has	O
1	O
]	O
(	O
x	O
)	O
=	O
1	O
on	O
a	O
and	O
1	O
]	O
(	O
x	O
)	O
=	O
0	O
on	O
b.	O
we	O
say	O
version	O
because	O
1	O
]	O
is	O
not	O
defined	O
on	O
sets	O
of	O
measure	O
zero	O
.	O
for	O
such	O
strictly	B
separable	I
distributions	O
,	O
l	O
*	O
=	O
o.	O
let	O
ij	O
be	O
1/2	O
-	O
e	O
on	O
band	O
1/2	O
+	O
e	O
on	O
a.	O
then	O
,	O
with	O
g	O
(	O
x	O
)	O
=	O
{	O
if	O
ij	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
1/2	O
0	O
1	O
otherwise	O
,	O
{	O
o	O
1	O
=	O
if	O
x	O
e	O
b	O
if	O
x	O
e	O
a	O
,	O
6.7	O
classification	O
is	O
easier	O
than	O
regression	B
function	I
estimation	O
105	O
we	O
have	O
p	O
{	O
g	O
(	O
x	O
)	O
-=i	O
y	O
}	O
=	O
l	O
*	O
=	O
o.	O
yet	O
,	O
2elry	O
(	O
x	O
)	O
-ry	O
(	O
x	O
)	O
1	O
=	O
1	O
-	O
2e	O
is	O
arbitrarily	O
close	O
to	O
one	O
.	O
in	O
a	O
more	O
realistic	O
example	O
,	O
we	O
consider	O
the	O
kernel	B
rule	I
(	O
see	O
chapter	O
10	O
)	O
,	O
gn	O
(	O
x	O
)	O
=	O
{	O
ifryn	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
1/2	O
0	O
1	O
otherwise	O
,	O
in	O
which	O
where	O
k	O
is	O
the	O
standard	B
normal	O
density	O
in	O
nd	O
:	O
k	O
(	O
u	O
)	O
=	O
__	O
i_	O
e-	O
iiu	O
\\2/2	O
.	O
(	O
2n	O
)	O
d/2	O
assume	O
that	O
a	O
and	O
b	O
consist	O
of	O
one	O
point	O
each	O
,	O
at	O
distance	B
0	O
from	O
each	O
other-that	O
is	O
,	O
the	O
distribution	B
of	O
x	O
is	O
concentrated	O
on	O
two	O
points	O
.	O
if	O
p	O
{	O
y	O
=	O
o	O
}	O
=	O
p	O
{	O
y	O
=	O
i	O
}	O
=	O
1/2	O
,	O
we	O
see	O
that	O
lim	O
-	O
l	O
k	O
(	O
x	O
-	O
xi	O
)	O
=	O
1	O
n	O
11	O
--	O
+00	O
n	O
i	O
:	O
::1	O
k	O
(	O
o	O
)	O
+	O
k	O
(	O
o	O
)	O
2	O
with	O
probability	O
one	O
at	O
x	O
e	O
a	O
u	O
b	O
,	O
by	O
the	O
law	O
of	O
large	O
numbers	O
.	O
also	O
,	O
k	O
(	O
0	O
)	O
/2	O
k	O
(	O
o	O
)	O
/2	O
if	O
x	O
e	O
a	O
if	O
x	O
e	O
b	O
with	O
probability	O
one	O
.	O
thus	O
,	O
lim	O
ryn	O
(	O
x	O
)	O
=	O
if	O
x	O
e	O
a	O
if	O
x	O
e	O
b	O
hence	O
,	O
as	O
ry	O
(	O
x	O
)	O
=	O
ion	O
a	O
and	O
ry	O
(	O
x	O
)	O
=	O
0	O
on	O
b	O
,	O
n	O
--	O
+oo	O
k	O
(	O
o	O
)	O
k	O
(	O
<	O
jt	O
(	O
8~	O
(	O
8	O
)	O
k	O
(	O
o	O
)	O
+k	O
(	O
8	O
)	O
{	O
with	O
probability	O
one	O
.	O
lim	O
2elry	O
(	O
x	O
)	O
-	O
n	O
--	O
+oo	O
ryn	O
(	O
x	O
)	O
1	O
=	O
k	O
(	O
o	O
)	O
+	O
k	O
(	O
8	O
)	O
yet	O
,	O
l	O
*	O
=	O
0	O
and	O
p	O
{	O
gn	O
(	O
x	O
)	O
-=i	O
y	O
}	O
~	O
o.	O
in	O
fact	O
,	O
if	O
dn	O
denotes	O
the	O
training	O
data	O
,	O
lim	O
p	O
{	O
gn	O
(	O
x	O
)	O
-=i	O
yidn	O
}	O
=	O
l	O
*	O
with	O
probability	O
one	O
,	O
n	O
--	O
+oo	O
106	O
6.	O
consistency	B
and	O
}	O
l~	O
2e	O
iljn	O
(	O
x	O
)	O
-	O
{	O
lj	O
(	O
x	O
)	O
1	O
dn	O
=	O
k	O
(	O
o	O
)	O
+	O
k	O
(	O
8	O
)	O
with	O
probability	O
one	O
.	O
i	O
}	O
2k	O
(	O
8	O
)	O
this	O
shows	O
very	O
strongly	O
that	O
for	O
any	O
8	O
>	O
0	O
,	O
for	O
many	O
practical	O
classification	O
rules	O
,	O
we	O
do	O
not	O
need	O
convergence	O
of	O
ljn	O
to	O
lj	O
at	O
all	O
!	O
as	O
all	O
the	O
consistency	B
proofs	O
in	O
chapters	O
6	O
through	O
11	O
rely	O
on	O
the	O
convergence	O
of	O
ljn	O
to	O
lj	O
,	O
we	O
will	O
create	O
unnecessary	O
conditions	O
for	O
some	O
distributions	O
,	O
although	O
it	O
will	O
always	O
be	O
possible	O
to	O
find	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
for	O
which	O
the	O
conditions	O
are	O
needed-in	O
the	O
latter	O
sense	O
,	O
the	O
conditions	O
of	O
these	O
universal	B
consistency	I
results	O
are	O
not	O
improvable	O
.	O
6.8	O
smart	O
rules	O
a	O
rule	O
is	O
a	O
sequence	O
of	O
mappings	O
gn	O
:	O
rd	O
x	O
(	O
rd	O
x	O
to	O
,	O
l	O
}	O
f	O
-+	O
to	O
,	O
i	O
}	O
.	O
most	O
rules	O
are	O
expected	O
to	O
perform	O
better	O
when	O
n	O
increases	O
.	O
so	O
,	O
we	O
say	O
that	O
a	O
rule	O
is	O
smart	O
if	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
,	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
is	O
nonincreasing	O
,	O
where	O
some	O
dumb	O
rules	O
are	O
smart	O
,	O
such	O
as	O
the	O
(	O
useless	O
)	O
rule	B
that	O
,	O
for	O
each	O
n	O
,	O
takes	O
a	O
majority	O
over	O
all	O
yi	O
's	O
,	O
ignoring	O
the	O
xi	O
's	O
.	O
this	O
follows	O
from	O
the	O
fact	O
that	O
p	O
{	O
t	O
(	O
2yi	O
-	O
1	O
)	O
>	O
0	O
,	O
y	O
=	O
0	O
or	O
t	O
(	O
2yi	O
-	O
1	O
)	O
:	O
'0	O
0	O
,	O
y	O
=	O
1	O
}	O
is	O
monotone	O
in	O
n.	O
this	O
is	O
a	O
property	O
of	O
the	O
binomial	B
distribution	I
(	O
see	O
problem	O
6.12	O
)	O
.	O
a	O
histogram	O
rule	B
with	O
a	O
fixed	O
partition	B
is	O
smart	O
(	O
problem	O
6.13	O
)	O
.	O
the	O
1-	O
nearest	B
neighbor	I
rule	I
is	O
not	O
smart	O
.	O
to	O
see	O
this	O
,	O
let	O
(	O
x	O
,	O
y	O
)	O
be	O
(	O
0	O
,	O
1	O
)	O
and	O
(	O
z	O
,	O
0	O
)	O
with	O
probabilities	O
p	O
and	O
1	O
-	O
p	O
,	O
respectively	O
,	O
where	O
z	O
is	O
uniform	B
on	O
[	O
-1000	O
,	O
1000	O
]	O
.	O
verify	O
that	O
for	O
n	O
=	O
1	O
,	O
eln	O
=	O
2p	O
(	O
1	O
-	O
p	O
)	O
,	O
while	O
for	O
n	O
=	O
2	O
,	O
2p	O
(	O
1-	O
p	O
)	O
2	O
(	O
~+	O
eizi	O
)	O
+	O
p2	O
(	O
1-	O
p	O
)	O
+	O
(	O
1-	O
p	O
)	O
2p	O
2	O
4000	O
=	O
2p	O
(	O
1	O
-	O
p	O
)	O
(	O
5	O
(	O
1	O
-	O
p	O
)	O
1	O
)	O
8	O
+	O
''	O
2	O
'	O
which	O
is	O
larger	O
than	O
2p	O
(	O
l	O
-	O
p	O
)	O
whenever	O
p	O
e	O
(	O
0	O
,	O
1/5	O
)	O
.	O
this	O
shows	O
that	O
in	O
all	O
these	O
cases	O
it	O
is	O
better	O
to	O
have	O
n	O
=	O
1	O
than	O
n	O
=	O
2.	O
similarly	O
,	O
the	O
standard	B
kernel	O
rule-discussed	O
in	O
chapter	O
10-with	O
fixed	O
h	O
is	O
not	O
smart	O
(	O
see	O
problems	O
6.14	O
,	O
6.15	O
)	O
.	O
the	O
error	O
probabilities	O
of	O
the	O
above	O
examples	O
of	O
smart	O
rules	O
do	O
not	O
change	O
dramatically	O
with	O
n.	O
however	O
,	O
change	O
is	O
necessary	O
to	O
guarantee	O
bayes	O
risk	O
consis	O
(	O
cid:173	O
)	O
tency	O
.	O
at	O
the	O
places	O
of	O
change-for	O
example	O
when	O
hn	O
jumps	O
to	O
a	O
new	O
value	O
in	O
the	O
histogram	O
rule-the	O
monotonicity	O
may	O
be	O
lost	O
.	O
this	O
leads	O
to	O
the	O
conjecture	O
that	O
no	O
universally	O
consistent	B
rule	I
can	O
be	O
smart	O
.	O
problems	O
and	O
exercises	O
107	O
problems	O
and	O
exercises	O
problem	O
6.1.	O
let	O
the	O
i.i.d	O
.	O
random	O
variables	O
xl	O
,	O
...	O
,	O
xn	O
be	O
distributed	O
on	O
rd	O
according	O
to	O
the	O
density	O
f.	O
estimate	B
f	O
by	O
fn	O
'	O
a·	O
function	O
of	O
x	O
and	O
xl	O
,	O
...	O
,	O
x	O
n	O
,	O
and	O
assume	O
that	O
f	O
i	O
fn	O
(	O
x	O
)	O
-	O
f	O
(	O
x	O
)	O
idx	O
-+	O
0	O
in	O
probability	O
(	O
or	O
with	O
probability	O
one	O
)	O
.	O
then	O
show	O
that	O
there	O
exists	O
a	O
consistent	O
(	O
or	O
strongly	O
consistent	O
)	O
classification	O
rule	B
whenever	O
the	O
conditional	O
densities	O
fa	O
and	O
fl	O
exist	O
.	O
problem	O
6.2.	O
histogram	B
density	I
estimation	I
.	O
let	O
xl	O
,	O
...	O
,	O
xn	O
be	O
i.i.d	O
.	O
random	O
variables	O
in	O
rd	O
with	O
density	O
f.	O
let	O
p	O
n	O
be	O
a	O
partition	O
of	O
rd	O
into	O
cubes	O
of	O
size	O
hn	O
'	O
and	O
define	O
the	O
histogram	O
density	O
estimate	O
by	O
where	O
an	O
(	O
x	O
)	O
is	O
the	O
set	O
in	O
p	O
n	O
that	O
contains	O
x.	O
prove	O
that	O
the	O
estimate	B
is	O
universally	O
consistent	O
in	O
ll	O
if	O
hn	O
-+	O
0	O
and	O
nh~	O
-+	O
00	O
as	O
n	O
-+	O
00	O
,	O
that	O
is	O
,	O
for	O
any	O
f	O
the	O
ll	O
error	O
of	O
the	O
estimate	B
f	O
ifn	O
(	O
x	O
)	O
-	O
f	O
(	O
x	O
)	O
ldx	O
converges	O
to	O
zero	O
in	O
probability	O
,	O
or	O
equivalently	O
,	O
!	O
l~	O
e	O
{	O
f	O
ifn	O
(	O
x	O
)	O
-	O
f	O
(	O
x	O
)	O
ldx	O
}	O
=	O
o.	O
hint	O
:	O
the	O
following	O
suggestions	O
may	O
be	O
helpful	O
fl	O
}	O
:	O
s	O
e	O
{	O
f	O
ifn	O
-	O
efnl	O
}	O
+	O
f	O
iefn	O
-	O
fl	O
.	O
(	O
1	O
)	O
e	O
{	O
f	O
ifn	O
-	O
(	O
2	O
)	O
e	O
{	O
f	O
itl	O
-	O
efn	O
i	O
}	O
=	O
lj	O
1f-i	O
(	O
anj	O
)	O
-	O
(	O
3	O
)	O
first	O
show	O
f	O
iefn	O
-	O
arbitrary	O
densities	O
.	O
f-in	O
(	O
anj	O
)	O
l.	O
fl	O
-+	O
0	O
for	O
uniformly	O
continuous	O
f	O
,	O
and	O
then	O
extend	O
it	O
to	O
problem	O
6.3.	O
let	O
x	O
be	O
uniformly	O
distributed	O
on	O
[	O
0	O
,	O
1	O
]	O
with	O
probability	O
1/2	O
,	O
and	O
let	O
x	O
be	O
atomic	O
on	O
the	O
rationals	O
with	O
probability	O
1/2	O
(	O
e.g.	O
,	O
if	O
the	O
rationals	O
are	O
enumerated	O
rl	O
,	O
r2	O
,	O
r3	O
,	O
''	O
''	O
then	O
p	O
{	O
x	O
=	O
rd	O
=	O
i/2i	O
+1	O
)	O
.	O
let	O
y	O
=	O
1	O
if	O
x	O
is	O
rational	O
and	O
y	O
=	O
0	O
if	O
x	O
is	O
irrational	O
.	O
give	O
a	O
direct	O
proof	O
of	O
consistency	O
of	O
the	O
i-nearest	O
neighbor	B
rule	I
.	O
hint	O
:	O
given	O
y	O
=	O
1	O
,	O
the	O
conditional	O
distribution	B
of	O
x	O
is	O
discrete	O
.	O
thus	O
,	O
for	O
every	O
e	O
>	O
0	O
,	O
there	O
is	O
an	O
integer	O
k	O
such	O
that	O
given	O
y	O
=	O
1	O
,	O
x	O
equals	O
one	O
of	O
k	O
rationals	O
with	O
probability	O
at	O
least	O
i-e.	O
now	O
,	O
if	O
n	O
is	O
large	O
enough	O
,	O
every	O
point	O
in	O
this	O
set	O
captures	O
data	O
points	O
with	O
label	O
i	O
with	O
large	O
probability	O
.	O
also	O
,	O
for	O
large	O
n	O
,	O
the	O
space	O
between	O
these	O
points	O
is	O
filled	O
with	O
data	O
points	O
labeled	O
with	O
zeros	O
.	O
problem	O
6.4.	O
prove	O
the	O
consistency	B
of	O
the	O
cubic	B
histogram	O
rule	B
by	O
checking	O
the	O
conditions	O
of	O
stone	O
's	O
theorem	B
.	O
hint	O
:	O
to	O
check	O
(	O
i	O
)	O
,	O
first	O
bound	O
wni	O
(	O
x	O
)	O
by	O
since	O
n	O
j=l	O
i	O
{	O
xiean	O
(	O
x	O
)	O
}	O
/	O
l	O
i	O
{	O
xjean	O
(	O
x	O
)	O
}	O
+	O
lin	O
.	O
e	O
{	O
t	O
~f	O
(	O
xi	O
)	O
}	O
~	O
ef	O
(	O
x	O
)	O
,	O
it	O
suffices	O
to	O
show	O
that	O
there	O
is	O
a	O
constant	O
c	O
'	O
>	O
0	O
such	O
that	O
for	O
any	O
nonnegative	O
function	O
f	O
with	O
ef	O
(	O
x	O
)	O
<	O
00	O
,	O
108	O
6.	O
consistency	B
in	O
doing	O
so	O
,	O
you	O
may	O
need	O
to	O
use	O
lemma	O
a.2	O
(	O
i	O
)	O
.	O
to	O
prove	O
that	O
condition	O
(	O
iii	O
)	O
holds	O
,	O
write	O
and	O
use	O
lemma	O
a.2	O
(	O
ii	O
)	O
.	O
problem	O
6.s	O
.	O
let	O
{	O
an	O
}	O
be	O
a	O
sequence	O
of	O
positive	O
numbers	O
converging	O
to	O
zero	O
.	O
give	O
an	O
exam	O
(	O
cid:173	O
)	O
ple	O
of	O
an	O
a	B
posteriori	I
probability	I
function	O
1	O
]	O
,	O
and	O
a	O
sequence	O
of	O
functions	O
{	O
1	O
]	O
n	O
}	O
approximating	O
1	O
]	O
such	O
that	O
p	O
{	O
gn	O
(	O
x	O
)	O
=i	O
y	O
}	O
-	O
l*	O
.	O
f	O
·	O
1m	O
in	O
1	O
11-+00	O
anje	O
{	O
(	O
1	O
]	O
n	O
(	O
x	O
)	O
-	O
0	O
,	O
>	O
1	O
]	O
(	O
x	O
)	O
)	O
2	O
}	O
where	O
gn	O
(	O
x	O
)	O
=	O
{	O
~	O
if	O
1	O
]	O
n	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
1/2	O
otherwise	O
.	O
thus	O
,	O
the	O
rate	B
of	I
convergence	I
in	O
theorem	B
6.s	O
may	O
be	O
arbitrarily	O
slow	O
.	O
hint	O
:	O
define	O
1	O
]	O
=	O
1/2	O
+	O
hex	O
)	O
,	O
where	O
hex	O
)	O
is	O
a	O
very	O
slowly	O
increasing	O
nonnegative	O
function	O
.	O
problem	O
6.6.	O
let	O
0	O
>	O
0	O
,	O
and	O
assume	O
that	O
11	O
]	O
(	O
x	O
)	O
-	O
1/21	O
:	O
:	O
:	O
0	O
for	O
all	O
x.	O
consider	O
the	O
decision	O
if	O
1	O
]	O
n	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
1/2	O
otherwise	O
.	O
prove	O
that	O
p	O
{	O
gn	O
(	O
x	O
)	O
=i	O
y	O
}	O
_	O
l	O
*	O
:	O
:	O
:	O
:	O
2e	O
{	O
(	O
1	O
]	O
n	O
(	O
x~	O
-	O
1	O
]	O
(	O
x	O
)	O
)	O
2	O
}	O
.	O
this	O
shows	O
that	O
the	O
rate	B
of	I
convergence	I
implied	O
by	O
the	O
inequality	B
of	O
theorem	B
6.6	O
may	O
be	O
preserved	O
for	O
very	O
general	O
classes	O
of	O
distributions	O
.	O
problem	O
6.7.	O
assume	O
that	O
l	O
*	O
=	O
0	O
,	O
and	O
consider	O
the	O
decision	O
show	O
that	O
for	O
all	O
1	O
:	O
:	O
:	O
:	O
p	O
<	O
00	O
,	O
if	O
11n	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
1/2	O
otherwise	O
.	O
hint	O
:	O
proceed	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
6.6	O
,	O
but	O
use	O
holder	O
's	O
inequality	B
.	O
problem	O
6.8.	O
theorem	B
6.s	O
can	O
not	O
be	O
generalized	B
to	O
the	O
ll	O
error	O
.	O
in	O
particular	O
,	O
show	O
by	O
example	O
that	O
it	O
is	O
not	O
always	O
true	O
that	O
el	O
-l*	O
lim	O
n	O
--	O
-+oo	O
e	O
{	O
111n	O
(	O
x	O
)	O
-	O
11	O
(	O
x	O
)	O
i	O
}	O
n	O
=	O
0	O
when	O
e	O
{	O
/1	O
]	O
n	O
(	O
x	O
)	O
-	O
1	O
]	O
(	O
x	O
)	O
j	O
}	O
-+	O
0	O
as	O
11	O
-+	O
00	O
for	O
some	O
regression	B
function	I
estimate	O
11n	O
.	O
thus	O
,	O
the	O
inequality	B
(	O
corollary	O
6.2	O
)	O
eln	O
-	O
l	O
*	O
:	O
:	O
:	O
:	O
2e	O
{	O
11	O
]	O
n	O
(	O
x	O
)	O
-	O
11	O
(	O
x	O
)	O
i	O
}	O
can	O
not	O
be	O
universally	O
improved	O
.	O
problems	O
and	O
exercises	O
109	O
problem	O
6.9.	O
let	O
1	O
]	O
'	O
:	O
rd	O
-+	O
[	O
0	O
,	O
1	O
]	O
,	O
and	O
define	O
g	O
(	O
x	O
)	O
=	O
i	O
{	O
rl	O
'	O
(	O
x	O
»	O
1/2j	O
.	O
assume	O
that	O
the	O
random	O
that	O
e	O
{	O
j1l	O
(	O
x	O
)	O
-	O
1	O
]	O
n	O
(	O
x	O
)	O
i	O
}	O
-+	O
°	O
as	O
n	O
-+	O
00.	O
prove	O
that	O
l	O
(	O
gn	O
)	O
-+	O
l	O
(	O
g	O
)	O
for	O
all	O
distributions	O
of	O
variable	O
x	O
satisfies	O
that	O
p	O
{	O
1	O
]	O
'	O
(	O
x	O
)	O
=	O
1/2	O
}	O
=	O
0.	O
let	O
1	O
]	O
1	O
,	O
1	O
]	O
2	O
,	O
...	O
be	O
a	O
sequence	O
of	O
functions	O
such	O
(	O
x	O
,	O
y	O
)	O
satisfying	O
the	O
condition	O
on	O
x	O
above	O
,	O
where	O
gn	O
(	O
x	O
)	O
=	O
i	O
{	O
lln	O
(	O
x	O
»	O
lj2	O
}	O
(	O
lugosi	O
(	O
1992	O
)	O
)	O
.	O
problem	O
6.10.	O
a	O
lying	O
teacher	O
.	O
sometimes	O
the	O
training	O
labels	O
yj	O
,	O
...	O
,	O
yn	O
are	O
not	O
avail	O
(	O
cid:173	O
)	O
able	O
,	O
but	O
can	O
only	O
be	O
observed	O
through	O
a	O
noisy	O
binary	B
channel	O
.	O
still	O
,	O
we	O
want	O
to	O
decide	O
on	O
y.	O
consider	O
the	O
following	O
model	O
.	O
assume	O
that	O
the	O
yi	O
'	O
s	O
in	O
the	O
training	O
data	O
are	O
replaced	O
by	O
the	O
i.i.d	O
.	O
binary-valued	O
random	O
variables	O
zi	O
,	O
whose	O
distribution	B
is	O
given	O
by	O
p	O
{	O
zi	O
=	O
llyi	O
=	O
o	O
}	O
=	O
p	O
<	O
1/2	O
,	O
p	O
{	O
zi	O
=	O
0iyi	O
=	O
i	O
}	O
=	O
q	O
<	O
1/2	O
.	O
p	O
{	O
zi	O
=	O
llyi	O
=	O
0	O
,	O
xi	O
=	O
x	O
}	O
p	O
{	O
zi	O
=	O
oiyi	O
=	O
1	O
,	O
xi	O
=	O
x	O
}	O
consider	O
the	O
decision	O
g	O
(	O
x	O
)	O
=	O
{	O
~	O
where	O
1	O
]	O
'	O
(	O
x	O
)	O
=	O
p	O
{	O
zi	O
=	O
i\xi	O
=	O
x	O
}	O
.	O
show	O
that	O
p	O
{	O
g	O
(	O
x	O
)	O
=i	O
y	O
}	O
:	O
s	O
l	O
*	O
(	O
1	O
+	O
if	O
1	O
]	O
'	O
(	O
x	O
)	O
:	O
s	O
1/2	O
otherwise	O
,	O
21p	O
-	O
q	O
i	O
)	O
.	O
1	O
-	O
2max	O
(	O
p	O
,	O
q	O
)	O
use	O
problem	O
6.9	O
to	O
conclude	O
that	O
if	O
the	O
binary	B
channel	O
is	O
symmetric	O
(	O
i.e.	O
,	O
p	O
=	O
q	O
<	O
1/2	O
)	O
,	O
and	O
p	O
{	O
1	O
]	O
'	O
(	O
x	O
)	O
=	O
1/2	O
}	O
=	O
0	O
,	O
then	O
l	O
i-consistent	O
estimation	B
leads	O
to	O
a	O
consistent	O
rule	B
,	O
in	O
spite	O
of	O
the	O
fact	O
that	O
the	O
labels	O
yi	O
were	O
not	O
available	O
in	O
the	O
training	O
sequence	O
(	O
lugosi	O
(	O
1992	O
)	O
)	O
.	O
problem	O
6.11.	O
develop	O
a	O
discrimination	O
rule	B
which	O
has	O
the	O
property	O
lim	O
eln	O
=	O
p	O
=	O
e	O
{	O
j1	O
]	O
(	O
x	O
)	O
(	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
)	O
}	O
,	O
n-+oo	O
for	O
all	O
distributions	O
such	O
that	O
x	O
has	O
a	O
density	O
.	O
note	O
:	O
clearly	O
,	O
since	O
p	O
:	O
:	O
:	O
:	O
:	O
l	O
*	O
,	O
this	O
rule	B
is	O
not	O
universally	O
consistent	O
,	O
but	O
it	O
will	O
aid	O
you	O
in	O
``	O
visualizing	O
''	O
the	O
matushita	O
error	O
!	O
problem	O
6.12.	O
if	O
zn	O
is	O
binomial	B
(	O
n	O
,	O
p	O
)	O
and	O
z	O
is	O
bernoulli	O
(	O
p	O
)	O
,	O
independent	O
of	O
zn	O
,	O
then	O
show	O
that	O
p	O
{	O
zn	O
>	O
n12	O
,	O
z	O
=	O
o	O
}	O
+	O
p	O
{	O
zn	O
:	O
s	O
n12	O
,	O
z	O
=	O
1	O
}	O
is	O
nonincreasing	O
in	O
n.	O
problem	O
6.13.	O
let	O
gn	O
be	O
the	O
histogram	B
rule	I
based	O
on	O
a	O
fixed	O
partition	B
p.	O
show	O
that	O
gn	O
is	O
smart	O
.	O
problem	O
6.14.	O
show	O
that	O
the	O
kernel	B
rule	I
with	O
gaussian	B
kernel	O
and	O
h	O
=	O
1	O
,	O
d	O
=	O
1	O
,	O
is	O
not	O
smart	O
(	O
kernel	B
rules	I
are	O
discussed	O
in	O
chapter	O
10	O
)	O
.	O
hint	O
:	O
consider	O
n	O
=	O
1	O
and	O
n	O
=	O
2	O
only	O
.	O
problem	O
6.15.	O
show	O
that	O
the	O
kernel	B
rule	I
on	O
r	O
,	O
with	O
k	O
(	O
x	O
)	O
=	O
i	O
[	O
-l	O
,	O
lj	O
(	O
x	O
)	O
,	O
and	O
h	O
t	O
0	O
,	O
such	O
that	O
nh	O
-+	O
00	O
,	O
is	O
not	O
smart	O
.	O
problem	O
6.16.	O
conjecture	O
:	O
no	O
universally	O
consistent	B
rule	I
is	O
smart	O
.	O
:	O
rd	O
x	O
(	O
rd	O
x	O
{	O
o	O
,	O
l	O
}	O
r	O
is	O
called	O
symmetric	O
if	O
gn	O
(	O
x	O
,	O
dn	O
)	O
problem	O
6.17.	O
a	O
rule	O
gn	O
gn	O
(	O
x	O
,	O
d~	O
)	O
for	O
every	O
x	O
,	O
and	O
every	O
training	O
sequence	O
dn	O
,	O
where	O
d~	O
is	O
an	O
arbitrary	O
permutation	O
of	O
the	O
pairs	O
(	O
xi	O
,	O
yi	O
)	O
in	O
dn	O
.	O
any	O
nonsymmetric	O
rule	B
gn	O
may	O
be	O
symmetrized	O
by	O
taking	O
a	O
ma	O
(	O
cid:173	O
)	O
jority	O
vote	O
at	O
every	O
x	O
e	O
rd	O
over	O
all	O
gn	O
(	O
x	O
,	O
d~	O
)	O
,	O
obtained	O
by	O
then	O
!	O
permutations	O
of	O
dn	O
.	O
it	O
may	O
intuitively	O
be	O
expected	O
that	O
symmetrized	O
rules	O
perform	O
better	O
.	O
prove	O
that	O
this	O
is	O
false	O
,	O
that	O
is	O
,	O
exhibit	O
a	O
distribution	O
and	O
a	O
nonsymmetric	O
classifier	B
gn	O
such	O
that	O
its	O
expected	O
probability	O
of	O
error	O
is	O
smaller	O
than	O
that	O
of	O
the	O
symmetrized	O
version	O
of	O
gn	O
'	O
hint	O
:	O
take	O
g3	O
(	O
x	O
,	O
d3	O
)	O
=	O
1	O
-	O
y1	O
.	O
7	O
slow	O
rates	O
of	O
convergence	O
in	O
this	O
chapter	O
we	O
consider	O
the	O
general	O
pattern	O
recognition	O
problem	O
:	O
given	O
the	O
observation	O
x	O
and	O
the	O
training	O
data	O
dn	O
=	O
(	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
»	O
of	O
indepen	O
(	O
cid:173	O
)	O
dent	O
identically	O
distributed	O
random	O
variable	B
pairs	O
,	O
we	O
estimate	B
the	O
label	O
y	O
by	O
the	O
decision	O
the	O
error	O
probability	O
is	O
obviously	O
,	O
the	O
average	O
error	O
probability	O
eln	O
=	O
p	O
{	O
y	O
=i	O
gn	O
(	O
x	O
)	O
}	O
is	O
completely	O
determined	O
by	O
the	O
distribution	B
of	O
the	O
pair	O
(	O
x	O
,	O
y	O
)	O
,	O
and	O
the	O
classifier	B
gn	O
'	O
we	O
have	O
seen	O
in	O
chapter	O
6	O
that	O
there	O
exist	O
classification	O
rules	O
such	O
as	O
the	O
cubic	B
histogram	O
rule	B
with	O
properly	O
chosen	O
cube	O
sizes	O
such	O
that	O
limn	O
--	O
-+	O
oo	O
eln	O
=	O
l	O
*	O
for	O
all	O
possible	O
distributions	O
.	O
the	O
next	O
question	O
is	O
whether	O
there	O
are	O
classification	O
rules	O
with	O
eln	O
tending	O
to	O
the	O
bayes	O
risk	O
at	O
a	O
specified	O
rate	O
for	O
all	O
distributions	O
.	O
disappointingly	O
,	O
such	O
rules	O
do	O
not	O
exist	O
.	O
7.1	O
finite	O
training	O
sequence	O
the	O
first	O
negative	O
result	O
shows	O
that	O
for	O
any	O
classification	O
rule	B
and	O
for	O
any	O
fixed	O
n	O
,	O
there	O
exists	O
a	O
distribution	O
such	O
that	O
the	O
difference	O
between	O
the	O
error	O
probability	O
of	O
the	O
rule	B
and	O
l	O
*	O
is	O
larger	O
than	O
1/4	O
.	O
to	O
explain	O
this	O
,	O
note	O
that	O
for	O
fixed	O
n	O
,	O
we	O
can	O
find	O
a	O
sufficiently	O
complex	O
distribution	B
for	O
which	O
the	O
sample	O
size	O
n	O
is	O
hopelessly	O
small	O
.	O
112	O
7.	O
slow	O
rates	O
of	O
convergence	O
theorem	B
7.1	O
.	O
(	O
devroye	O
(	O
1982b	O
)	O
)	O
.	O
let	O
e	O
>	O
°	O
be	O
an	O
arbitrarily	O
small	O
number	O
.	O
with	O
bayes	O
risk	O
l	O
*	O
=	O
°	O
such	O
that	O
for	O
any	O
integer	O
n	O
and	O
classification	O
rule	B
gn	O
,	O
there	O
exists	O
a	O
distribution	O
of	O
(	O
x	O
,	O
y	O
)	O
eln	O
:	O
:	O
:	O
:	O
1/2	O
-	O
e.	O
proof	O
.	O
first	O
we	O
construct	O
a	O
family	O
of	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
.	O
then	O
we	O
show	O
that	O
the	O
error	O
probability	O
of	O
any	O
classifier	B
is	O
large	O
for	O
at	O
least	O
one	O
member	O
of	O
the	O
family	O
.	O
for	O
every	O
member	O
of	O
the	O
family	O
,	O
x	O
is	O
uniformly	O
distributed	O
on	O
the	O
set	O
{	O
i	O
,	O
2	O
,	O
...	O
,	O
k	O
}	O
of	O
positive	O
integers	O
.	O
=	O
p	O
{	O
x	O
=	O
i	O
}	O
=	O
{	O
1/	O
k	O
pi	O
°	O
otherwise	O
,	O
if	O
i	O
e	O
{	O
.1	O
,	O
...	O
,	O
k	O
}	O
where	O
k	O
is	O
a	O
large	O
integer	O
specified	O
later	O
.	O
now	O
,	O
the	O
family	B
of	I
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
is	O
parameterized	O
by	O
a	O
number	O
b	O
e	O
[	O
0	O
,	O
1	O
)	O
,	O
that	O
is	O
,	O
every	O
b	O
determines	O
a	O
distribution	O
as	O
follows	O
.	O
let	O
b	O
e	O
[	O
0	O
,	O
1	O
)	O
have	O
binary	B
expansion	O
b	O
=	O
0.bob1b2	O
...	O
,	O
and	O
define	O
y	O
=	O
bx	O
.	O
as	O
the	O
label	O
y	O
is	O
a	O
function	O
of	O
x	O
,	O
there	O
exists	O
a	O
perfect	O
decision	O
,	O
and	O
thus	O
l	O
*	O
=	O
0.	O
we	O
show	O
that	O
for	O
any	O
decision	O
rule	O
gn	O
there	O
is	O
a	O
b	O
such	O
that	O
if	O
y	O
=	O
bx	O
,	O
then	O
gn	O
has	O
very	O
poor	O
performance	O
.	O
denote	O
the	O
average	O
error	O
probability	O
corresponding	O
to	O
the	O
distribution	B
determined	O
by	O
b	O
,	O
by	O
rn	O
(	O
b	O
)	O
=	O
eln	O
.	O
the	O
proof	O
of	O
the	O
existence	O
of	O
a	O
bad	O
distribution	B
is	O
based	O
on	O
the	O
so-called	O
prob	O
(	O
cid:173	O
)	O
abilistic	O
method	O
.	O
here	O
the	O
key	O
trick	O
is	O
the	O
randomization	O
of	O
b.	O
define	O
a	O
random	O
variable	B
b	O
which	O
is	O
uniformly	O
distributed	O
in	O
[	O
0	O
,	O
1	O
)	O
and	O
independent	O
of	O
x	O
and	O
xl	O
,	O
...	O
,	O
x	O
n	O
.	O
then	O
we	O
may	O
compute	O
the	O
expected	O
value	O
of	O
the	O
random	O
variable	B
rn	O
(	O
b	O
)	O
.	O
since	O
for	O
any	O
decision	O
rule	O
gn	O
,	O
sup	O
rn	O
(	O
b	O
)	O
:	O
:	O
:	O
:	O
e	O
{	O
rn	O
(	O
b	O
)	O
}	O
,	O
be	O
[	O
o	O
,	O
i	O
)	O
a	O
lower	O
bound	O
for	O
e	O
{	O
rn	O
(	O
b	O
)	O
}	O
proves	O
the	O
existence	O
of	O
abe	O
[	O
0	O
,	O
1	O
)	O
whose	O
corre	O
(	O
cid:173	O
)	O
sponding	O
error	O
probability	O
exceeds	O
the	O
lower	O
bound	O
.	O
since	O
b	O
is	O
uniformly	O
distributed	O
in	O
[	O
0	O
,	O
1	O
)	O
,	O
its	O
binary	B
extension	O
b	O
=	O
0.bib2	O
'	O
''	O
is	O
a	O
sequence	O
of	O
independent	O
binary	B
random	O
variables	O
with	O
p	O
{	O
bi	O
=	O
o	O
}	O
=	O
p	O
{	O
bi	O
=	O
i	O
}	O
=	O
1/2	O
.	O
but	O
e	O
{	O
rn	O
(	O
b	O
)	O
}	O
=	O
p	O
{	O
gn	O
(	O
x	O
,	O
dn	O
)	O
=i	O
bx	O
}	O
p	O
{	O
gn	O
(	O
x	O
,	O
xl	O
,	O
bxl	O
,	O
..	O
,	O
,	O
xn	O
,	O
bx	O
,	O
j	O
=i	O
bx	O
}	O
=	O
e	O
{	O
p	O
{	O
gn	O
(	O
x	O
,	O
xl	O
,	O
bx	O
!	O
'	O
•..	O
,	O
x	O
n	O
,	O
bx	O
,	O
j	O
=i	O
bx	O
\	O
x	O
,	O
xl	O
,	O
''	O
''	O
xn	O
}	O
}	O
>	O
2p	O
{	O
x	O
=i	O
xl	O
,	O
x	O
=i	O
x	O
2	O
,	O
.·.	O
,	O
x	O
=i	O
xn	O
}	O
,	O
1	O
since	O
if	O
x	O
=i	O
xi	O
for	O
all	O
i	O
=	O
1,2	O
,	O
...	O
,	O
n	O
,	O
then	O
given	O
x	O
,	O
xl	O
,	O
``	O
''	O
x	O
n	O
,	O
y	O
=	O
bx	O
is	O
conditionally	O
independent	O
of	O
gn	O
(	O
x	O
,	O
dn	O
)	O
and	O
y	O
takes	O
values	O
°	O
and	O
1	O
with	O
probability	O
7.2	O
slow	O
rates	O
113	O
1/2	O
.	O
but	O
clearly	O
,	O
p	O
{	O
x	O
i	O
xl	O
,	O
x	O
i	O
x	O
2	O
,	O
..	O
·	O
,	O
x	O
i	O
xnl	O
x	O
}	O
=	O
p	O
{	O
x	O
i	O
xiix	O
}	O
n	O
=	O
(	O
1-	O
i/kt	O
.	O
in	O
summary	O
,	O
sup	O
rn	O
(	O
b	O
)	O
:	O
:	O
:	O
:	O
-	O
(	O
1	O
-	O
1/	O
kt	O
.	O
he	O
[	O
o	O
,	O
l	O
)	O
1	O
2	O
the	O
lower	O
bound	O
tends	O
to	O
1/2	O
as	O
k	O
-+	O
00.	O
d	O
theorem	B
7.1	O
states	O
that	O
even	O
though	O
we	O
have	O
rules	O
that	O
are	O
universally	O
consistent	O
,	O
that	O
is	O
,	O
they	O
asymptotically	O
provide	O
the	O
optimal	O
performance	O
for	O
any	O
distribution	B
,	O
their	O
finite	O
sample	O
performance	O
is	O
always	O
extremely	O
bad	O
for	O
some	O
distributions	O
.	O
this	O
means	O
that	O
no	O
classifier	B
guarantees	O
that	O
with	O
a	O
sample	O
size	O
of	O
(	O
say	O
)	O
n	O
=	O
108	O
we	O
get	O
within	O
1/4	O
of	O
the	O
bayes	O
error	O
probability	O
for	O
all	O
distributions	O
.	O
however	O
,	O
as	O
the	O
bad	O
distribution	B
depends	O
upon	O
n	O
,	O
theorem	B
7.1	O
does	O
not	O
allow	O
us	O
to	O
conclude	O
that	O
there	O
is	O
one	O
distribution	B
for	O
which	O
the	O
error	O
probability	O
is	O
more	O
than	O
l	O
*	O
+	O
1/4	O
for	O
all	O
n.	O
indeed	O
,	O
that	O
would	O
contradict	O
the	O
very	O
existence	O
of	O
universally	O
consistent	O
rules	O
.	O
7.2	O
slow	O
rates	O
the	O
next	O
question	O
is	O
whether	O
a	O
certain	O
universal	B
rate	O
of	O
convergence	O
to	O
l	O
*	O
is	O
achievable	O
for	O
some	O
classifier	B
.	O
for	O
example	O
,	O
theorem	B
7.1	O
does	O
not	O
exclude	O
the	O
existence	O
of	O
a	O
classifier	O
such	O
that	O
for	O
every	O
n	O
,	O
eln	O
-	O
l	O
*	O
:	O
:	O
:	O
:	O
c	O
/	O
n	O
for	O
all	O
distributions	O
,	O
for	O
some	O
constant	O
c	O
depending	O
upon	O
the	O
actual	O
distribution	B
.	O
the	O
next	O
negative	O
result	O
is	O
that	O
this	O
can	O
not	O
be	O
the	O
case	O
.	O
theorem	B
7.2	O
below	O
states	O
that	O
the	O
error	O
probability	O
eln	O
of	O
any	O
classifier	B
is	O
larger	O
than	O
(	O
say	O
)	O
l	O
*	O
+	O
c/	O
(	O
log	O
log	O
log	O
n	O
)	O
for	O
every	O
n	O
for	O
some	O
distribution	B
,	O
even	O
if	O
c	O
depends	O
on	O
the	O
distribution	B
.	O
(	O
this	O
can	O
be	O
seen	O
by	O
considering	O
that	O
by	O
theorem	B
7.2	O
,	O
there	O
exists	O
a	O
distribution	O
of	O
(	O
x	O
,	O
y	O
)	O
such	O
that	O
eln	O
:	O
:	O
:	O
:	O
l	O
*	O
+	O
1/	O
-/log	O
log	O
log	O
n	O
for	O
every	O
n.	O
)	O
moreover	O
,	O
there	O
is	O
no	O
sequence	O
of	O
numbers	O
an	O
converging	O
to	O
zero	O
such	O
that	O
there	O
is	O
a	O
classification	O
rule	B
with	O
error	O
probability	O
below	O
l	O
*	O
plus	O
an	O
for	O
all	O
distributions	O
.	O
thus	O
,	O
in	O
practice	O
,	O
no	O
classifier	B
assures	O
us	O
that	O
its	O
error	O
probability	O
is	O
close	O
to	O
l	O
*	O
,	O
unless	O
the	O
actual	O
distribution	B
is	O
known	O
to	O
be	O
a	O
member	O
of	O
a	O
restricted	O
class	O
of	O
distributions	O
.	O
now	O
,	O
it	O
is	O
easily	O
seen	O
that	O
in	O
the	O
proof	O
of	O
both	O
theorems	O
we	O
could	O
take	O
x	O
to	O
have	O
uniform	B
distribution	O
on	O
[	O
0	O
,	O
1	O
]	O
,	O
or	O
any	O
other	O
density	O
(	O
see	O
problem	O
7.2	O
)	O
.	O
therefore	O
,	O
putting	O
restrictions	O
on	O
the	O
distribution	B
of	O
x	O
alone	O
does	O
not	O
suffice	O
to	O
obtain	O
rate-of-convergence	O
results	O
.	O
for	O
such	O
results	O
,	O
one	O
needs	O
conditions	O
on	O
the	O
a	B
posteriori	I
probability	I
17	O
(	O
x	O
)	O
as	O
well	O
.	O
however	O
,	O
if	O
only	O
training	O
data	O
give	O
information	O
about	O
the	O
joint	O
distribution	B
,	O
then	O
theorems	O
with	O
extra	O
conditions	O
on	O
the	O
distribution	B
have	O
little	O
practical	O
value	O
,	O
as	O
it	O
is	O
impossible	O
to	O
detect	O
whether	O
,	O
for	O
example	O
,	O
the	O
a	B
posteriori	I
probability	I
1j	O
(	O
x	O
)	O
is	O
twice	O
differentiable	O
or	O
not	O
.	O
now	O
,	O
the	O
situation	O
may	O
look	O
hopeless	O
,	O
but	O
this	O
is	O
not	O
so	O
.	O
simply	O
put	O
,	O
the	O
bayes	O
error	O
is	O
too	O
difficult	O
a	O
target	O
to	O
shoot	O
at	O
.	O
114	O
7.	O
slow	O
rates	O
of	O
convergence	O
weaker	O
versions	O
of	O
theorem	O
7.2	O
appeared	O
earlier	O
in	O
the	O
literature	O
.	O
first	O
cover	O
(	O
1968b	O
)	O
showed	O
that	O
for	O
any	O
sequence	O
of	O
classification	O
rules	O
,	O
for	O
sequences	O
{	O
an	O
}	O
converging	O
to	O
zero	O
at	O
arbitrarily	O
slow	O
algebraic	O
rates	O
(	O
i.e.	O
,	O
as	O
11no	O
for	O
arbitrarily	O
small	O
8	O
>	O
0	O
)	O
,	O
there	O
exists	O
a	O
distribution	O
such	O
that	O
eln	O
:	O
:	O
:	O
l	O
*	O
+	O
an	O
infinitely	O
often	O
.	O
devroye	O
(	O
1982b	O
)	O
strengthened	O
cover	O
's	O
result	O
allowing	O
sequences	O
tending	O
to	O
zero	O
arbitrarily	O
slowly	O
.	O
the	O
next	O
result	O
asserts	O
that	O
eln	O
>	O
l	O
*	O
+	O
an	O
for	O
every	O
n.	O
theorem	B
7.2.	O
let	O
{	O
an	O
}	O
be	O
a	O
sequence	O
of	O
positive	O
numbers	O
converging	O
to	O
zero	O
with	O
1	O
116	O
:	O
:	O
:	O
a	O
i	O
:	O
:	O
:	O
a2	O
:	O
:	O
:	O
...	O
.	O
for	O
every	O
sequence	O
of	O
classification	O
rules	O
,	O
there	O
exists	O
a	O
distribution	O
of	O
(	O
x	O
,	O
y	O
)	O
with	O
l	O
*	O
=	O
0	O
,	O
such	O
that	O
for	O
all	O
n.	O
this	O
result	O
shows	O
that	O
universally	O
good	O
classification	O
rules	O
do	O
not	O
exist	O
.	O
rate	B
of	I
convergence	I
studies	O
for	O
particular	O
rules	O
must	O
necessarily	O
be	O
accompanied	O
by	O
conditions	O
on	O
(	O
x	O
,	O
y	O
)	O
.	O
that	O
these	O
conditions	O
too	O
are	O
necessarily	O
restrictive	O
follows	O
from	O
examples	O
suggested	O
in	O
problem	O
7.2.	O
under	O
certain	O
regularity	O
conditions	O
it	O
is	O
possible	O
to	O
obtain	O
upper	O
bounds	O
for	O
the	O
rates	O
of	O
convergence	O
for	O
the	O
probability	O
of	O
error	O
of	O
certain	O
rules	O
to	O
l	O
*	O
.	O
then	O
it	O
is	O
natural	O
to	O
ask	O
what	O
the	O
fastest	O
achievable	O
rate	O
is	O
for	O
the	O
given	O
class	O
of	O
distributions	O
.	O
a	O
theory	O
for	O
regression	B
function	I
estimation	O
was	O
worked	O
out	O
by	O
stone	O
(	O
1982	O
)	O
.	O
related	O
results	O
for	O
classification	O
were	O
obtained	O
by	O
marron	O
(	O
1983	O
)	O
.	O
in	O
the	O
proof	O
of	O
theorem	O
7.2	O
we	O
will	O
need	O
the	O
following	O
simple	O
lemma	O
:	O
lemma	O
7.1.	O
for	O
any	O
monotone	O
decreasing	O
sequence	O
{	O
an	O
}	O
of	O
positive	O
numbers	O
converging	O
to	O
zero	O
with	O
al	O
~	O
1/16	O
,	O
a	O
probability	O
distribution	B
(	O
pi	O
,	O
p2	O
,	O
...	O
)	O
may	O
be	O
found	O
such	O
that	O
pi	O
:	O
:	O
:	O
p2	O
:	O
:	O
:	O
...	O
,	O
and	O
for	O
all	O
n	O
00	O
l	O
pi	O
:	O
:	O
:	O
max	O
(	O
san	O
,	O
32npn+i	O
)	O
.	O
i=n+1	O
proof	O
.	O
it	O
suffices	O
to	O
look	O
for	O
pi	O
's	O
such	O
that	O
00	O
l	O
pi	O
:	O
:	O
:	O
max	O
(	O
8an	O
,	O
32npn	O
)	O
.	O
i=n+1	O
these	O
conditions	O
are	O
easily	O
satisfied	O
.	O
for	O
positive	O
integers	O
u	O
<	O
v	O
,	O
define	O
the	O
func	O
(	O
cid:173	O
)	O
tion	O
h	O
(	O
v	O
,	O
u	O
)	O
=	O
l~	O
:	O
l	O
1	O
ii	O
.	O
first	O
we	O
find	O
a	O
sequence	O
1	O
=	O
n	O
1	O
<	O
n2	O
<	O
...	O
of	O
integers	O
with	O
the	O
following	O
properties	O
:	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
h	O
(	O
nk+l	O
,	O
nk	O
)	O
is	O
monotonically	O
increasing	O
,	O
h	O
(	O
n2	O
,	O
nr	O
)	O
:	O
:	O
:	O
32	O
,	O
8ank	O
~	O
1/2k	O
for	O
all	O
k	O
:	O
:	O
:	O
1.	O
note	O
that	O
(	O
c	O
)	O
may	O
only	O
be	O
satisfied	O
if	O
anl	O
=	O
al	O
:	O
:s	O
1/16	O
.	O
to	O
this	O
end	O
,	O
define	O
constants	O
cl	O
,	O
c2	O
,	O
...	O
by	O
7.2	O
slow	O
rates	O
115	O
so	O
that	O
the	O
ck	O
'	O
s	O
are	O
decreasing	O
in	O
k	O
,	O
and	O
for	O
n	O
e	O
[	O
nk	O
,	O
nk+l	O
)	O
,	O
we	O
define	O
pn	O
=	O
ck/	O
(	O
32n	O
)	O
.	O
we	O
claim	O
that	O
these	O
numbers	O
have	O
the	O
required	O
properties	O
.	O
indeed	O
,	O
{	O
pn	O
}	O
is	O
decreasing	O
,	O
and	O
finally	O
,	O
if	O
n	O
e	O
[	O
nk	O
,	O
nk+l	O
)	O
,	O
then	O
1	O
~	O
p	O
'	O
>	O
~	O
~h	O
(	O
n	O
'	O
1	O
n·	O
)	O
=	O
~	O
-	O
=	O
-	O
2k	O
.	O
~	O
i	O
i=n+l	O
1	O
j+	O
'	O
j	O
~	O
2j	O
00	O
c.	O
-	O
~	O
32	O
j=k+l	O
00	O
00	O
j=k+l	O
clearly	O
,	O
on	O
the	O
one	O
hand	O
,	O
by	O
the	O
monotonicity	O
of	O
h	O
(	O
nk+l	O
,	O
nk	O
)	O
,	O
1/2k	O
:	O
:	O
:	O
:	O
ck	O
=	O
32npn	O
.	O
on	O
the	O
other	O
hand	O
,	O
1/2k	O
:	O
:	O
:	O
:	O
8ank	O
:	O
:	O
:	O
:	O
8an	O
.	O
this	O
concludes	O
the	O
proof	O
.	O
0	O
proof	O
of	O
theorem	O
7.2.	O
we	O
introduce	O
some	O
notation	O
.	O
let	O
b	O
=	O
o.b	O
l	O
b2b3	O
..•	O
be	O
a	O
real	O
number	O
on	O
[	O
0	O
,	O
1	O
]	O
with	O
the	O
shown	O
binary	B
expansion	O
,	O
and	O
let	O
b	O
be	O
a	O
random	O
variable	B
uniformly	O
distributed	O
on	O
[	O
0	O
,	O
1	O
]	O
with	O
expansion	O
b	O
=	O
o.bi	O
b2b3	O
...	O
.	O
let	O
us	O
restrict	O
ourselves	O
to	O
a	O
random	O
variable	B
x	O
with	O
p	O
{	O
x	O
=	O
i	O
}	O
=	O
pi	O
,	O
i	O
:	O
:	O
:	O
:	O
1	O
,	O
where	O
pi	O
:	O
:	O
:	O
:	O
p2	O
:	O
:	O
:	O
:	O
...	O
>	O
0	O
,	O
and	O
l~n+l	O
pi	O
:	O
:	O
:	O
:	O
max	O
(	O
8an	O
,	O
32npn+l	O
)	O
for	O
every	O
n.	O
that	O
such	O
pi	O
's	O
exist	O
follows	O
from	O
lemma	O
7.1.	O
set	O
y	O
=	O
bx	O
.	O
as	O
y	O
is	O
a	O
function	O
of	O
x	O
,	O
we	O
see	O
that	O
l	O
*	O
=	O
0.	O
each	O
b	O
e	O
[	O
0	O
,	O
1	O
)	O
however	O
describes	O
a	O
different	O
distribution	B
.	O
with	O
b	O
replaced	O
by	O
b	O
we	O
have	O
a	O
random	O
distribution	B
.	O
introduce	O
the	O
short	O
notation	O
~n	O
=	O
(	O
(	O
xl	O
,	O
bxj	O
,	O
...	O
,	O
(	O
xn	O
,	O
bxj	O
)	O
,	O
and	O
define	O
g	O
ni	O
=	O
gn	O
(	O
i	O
,	O
~n	O
)	O
.ifln	O
(	O
b	O
)	O
denotesthe	O
probabilityoferrorp	O
{	O
gn	O
(	O
x	O
,	O
~n	O
)	O
=i	O
yib	O
,	O
xl	O
,	O
...	O
,	O
xn	O
}	O
for	O
the	O
random	O
distribution	B
,	O
then	O
we	O
note	O
that	O
we	O
may	O
write	O
00	O
ln	O
(	O
b	O
)	O
=	O
l	O
pj	O
{	O
gni¥bd	O
'	O
i=l	O
116	O
7.	O
slow	O
rates	O
of	O
convergence	O
if	O
ln	O
(	O
b	O
)	O
is	O
the	O
probability	O
of	O
error	O
for	O
a	O
distribution	O
parametrized	O
by	O
b	O
,	O
then	O
.	O
ln	O
(	O
b	O
)	O
supmfe	O
--	O
2an	O
b	O
n	O
we	O
consider	O
only	O
the	O
conditional	O
expectation	O
for	O
now	O
.	O
we	O
have	O
e	O
{	O
inf	O
ln	O
(	O
b	O
)	O
i	O
xl	O
,	O
x	O
2	O
,	O
•.•	O
}	O
n	O
2an	O
>	O
p	O
{	O
o	O
{	O
ln	O
(	O
b	O
)	O
:	O
:	O
:	O
2anll	O
xl	O
,	O
x2	O
,	O
...	O
}	O
00	O
>	O
1-	O
lp	O
{	O
ln	O
(	O
b	O
)	O
<	O
2anl	O
xl	O
,	O
x	O
2	O
,	O
•..	O
}	O
n=1	O
00	O
=	O
1	O
-	O
lp	O
{	O
ln	O
(	O
b	O
)	O
<	O
2anl	O
xl	O
,	O
x	O
2	O
,	O
•.•	O
,	O
xn	O
}	O
n=1	O
00	O
1	O
-	O
l	O
e	O
{	O
p	O
{	O
ln	O
(	O
b	O
)	O
<	O
2an	O
i	O
~n	O
}	O
i	O
x	O
i	O
,	O
x	O
2	O
,	O
...	O
,	O
x	O
n	O
}	O
.	O
n	O
:	O
::1	O
we	O
bound	O
the	O
conditional	O
probabilities	O
inside	O
the	O
sum	O
:	O
p	O
{	O
ln	O
(	O
b	O
)	O
<	O
2anl	O
~n	O
}	O
<	O
p	O
{	O
l	O
i¢	O
{	O
xl	O
''	O
''	O
,	O
xn	O
)	O
pj	O
{	O
glti=jb	O
;	O
)	O
<	O
2an	O
i	O
~n	O
}	O
(	O
and	O
,	O
noting	O
that	O
g	O
ni	O
,	O
xl	O
,	O
...	O
,	O
xn	O
are	O
all	O
functions	O
of	O
~n	O
,	O
we	O
have	O
:	O
)	O
pii	O
{	O
bi=l	O
}	O
<	O
2anl	O
~n	O
}	O
i¢	O
{	O
xl	O
,	O
...	O
,	O
xn	O
}	O
=	O
p	O
{	O
l	O
<	O
p	O
{	O
.f	O
pi	O
i	O
{	O
bi=l	O
)	O
<	O
2an	O
}	O
=	O
p	O
{	O
.f	O
pibi	O
<	O
2an	O
}	O
.	O
l=n+1	O
l=n+1	O
(	O
since	O
the	O
pi	O
's	O
are	O
decreasing	O
,	O
by	O
stochastic	O
dominance	O
)	O
now	O
everything	O
boils	O
down	O
to	O
bounding	O
these	O
probabilities	O
from	O
above	O
.	O
we	O
pro	O
(	O
cid:173	O
)	O
ceed	O
by	O
chernoff	O
's	O
bounding	O
technique	O
.	O
the	O
idea	O
is	O
the	O
following	O
:	O
for	O
any	O
random	O
variable	B
x	O
,	O
and	O
s	O
>	O
0	O
,	O
by	O
markov	O
's	O
inequality	B
,	O
7.2	O
slow	O
rates	O
117	O
by	O
cleverly	O
choosing	O
s	O
one	O
can	O
often	O
obtain	O
very	O
sharp	O
bounds	O
.	O
for	O
more	O
discussion	O
and	O
examples	O
of	O
chernoff	O
's	O
method	O
,	O
refer	O
to	O
chapter	O
8.	O
in	O
our	O
case	O
,	O
(	O
since	O
e-x	O
:	O
s	O
1	O
-	O
x	O
+	O
x	O
2/2	O
for	O
x	O
2	O
:	O
0	O
)	O
<	O
<	O
=	O
<	O
s2	O
pn+1b	O
)	O
(	O
since	O
1	O
-	O
x	O
:	O
s	O
e-	O
x	O
sb	O
)	O
(	O
exp	O
2san	O
-	O
2	O
+	O
4	O
(	O
where	O
b	O
=	O
l~n+l	O
pj	O
1_	O
(	O
4_an_-_b_	O
)	O
2	O
)	O
exp	O
--	O
4	O
bpn+l	O
(	O
(	O
by	O
taking	O
s	O
=	O
l	O
:	O
-4~	O
,	O
and	O
the	O
fact	O
that	O
b	O
>	O
4an	O
)	O
exp	O
(	O
-~~	O
)	O
(	O
since	O
b	O
2	O
:	O
8an	O
)	O
pn+ll	O
...	O
16	O
pn+l	O
<	O
e-2n	O
(	O
since	O
b	O
2	O
:	O
32pn+ln	O
)	O
.	O
thus	O
,	O
we	O
conclude	O
that	O
supinfe	O
--	O
>	O
1	O
-	O
le-2n	O
=	O
-	O
-	O
>	O
-	O
e2	O
-	O
2	O
e2	O
-	O
1	O
lncb	O
)	O
2an	O
-	O
1	O
2	O
'	O
b	O
n	O
00	O
n=l	O
so	O
that	O
there	O
exists	O
a	O
b	O
for	O
which	O
eln	O
(	O
b	O
)	O
;	O
:	O
:	O
:	O
an	O
for	O
all	O
n.	O
0	O
118	O
7.	O
slow	O
rates	O
of	O
convergence	O
problems	O
and	O
exercises	O
problem	O
7.1.	O
extend	O
theorem	B
7.2	O
for	O
distributions	O
with	O
°	O
<	O
l	O
*	O
<	O
1/2	O
:	O
show	O
that	O
if	O
an	O
is	O
a	O
sequence	O
of	O
positive	O
numbers	O
as	O
in	O
theorem	O
7.2	O
,	O
then	O
for	O
any	O
classification	O
rule	B
there	O
is	O
a	O
distribution	O
such	O
that	O
eln	O
-	O
l	O
*	O
:	O
:	O
:	O
:	O
an	O
for	O
every	O
n	O
for	O
which	O
l	O
*	O
+	O
an	O
<	O
1/2	O
.	O
problem	O
7.2.	O
prove	O
theorems	O
7.1	O
and	O
7.2	O
,	O
under	O
one	O
of	O
the	O
following	O
additional	O
assump	O
(	O
cid:173	O
)	O
tions	O
,	O
which	O
make	O
the	O
case	O
that	O
one	O
will	O
need	O
very	O
restrictive	O
conditions	O
indeed	O
to	O
study	O
rates	O
of	O
convergence	O
.	O
(	O
1	O
)	O
x	O
has	O
a	O
uniform	O
density	O
on	O
(	O
0	O
,	O
1	O
)	O
.	O
(	O
2	O
)	O
x	O
has	O
a	O
uniform	O
density	O
on	O
[	O
0	O
,	O
1	O
)	O
and	O
7j	O
is	O
infinitely	O
many	O
times	O
continuously	O
(	O
3	O
)	O
(	O
4	O
)	O
7j	O
is	O
unimodal	O
in	O
x	O
e	O
n	O
2	O
,	O
that	O
is	O
,	O
7j	O
(	O
ax	O
)	O
decreases	O
as	O
a	O
>	O
°	O
increases	O
for	O
any	O
differentiable	O
on	O
[	O
0	O
,	O
1	O
)	O
.	O
x	O
e	O
n2	O
.	O
7j	O
is	O
{	O
o	O
,	O
l	O
}	O
-valued	O
,	O
x	O
is	O
n	O
2-valued	O
,	O
and	O
the	O
set	O
{	O
x	O
:	O
7j	O
(	O
x	O
)	O
=	O
i	O
}	O
is	O
a	O
compact	O
convex	O
set	O
containing	O
the	O
origin	O
.	O
problem	O
7.3.	O
there	O
is	O
no	O
super-classifier	O
.	O
show	O
that	O
for	O
every	O
sequence	O
of	O
classifica	O
(	O
cid:173	O
)	O
tion	O
rules	O
{	O
gn	O
}	O
there	O
is	O
a	O
universally	O
consistent	O
sequence	O
of	O
rules	O
{	O
g	O
;	O
l	O
}	O
'	O
such	O
that	O
for	O
some	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
p	O
{	O
gn	O
(	O
x	O
)	O
=i	O
y	O
}	O
>	O
p	O
{	O
g~	O
(	O
x	O
)	O
=i	O
y	O
}	O
for	O
all	O
n.	O
problem	O
7.4.	O
the	O
next	O
two	O
exercises	O
are	O
intended	O
to	O
demonstrate	O
that	O
the	O
weaponry	O
of	O
pattern	O
recognition	O
can	O
often	O
be	O
successfully	O
used	O
for	O
attacking	O
other	O
statistical	O
problems	O
.	O
for	O
example	O
,	O
a	O
consequence	O
of	O
theorem	O
7.2	O
is	O
that	O
estimating	O
infinite	O
discrete	O
distributions	O
is	O
hard	O
.	O
consider	O
the	O
problem	O
of	O
estimating	O
a	O
distribution	O
(	O
pi	O
,	O
p2	O
,	O
...	O
)	O
on	O
the	O
positive	O
integers	O
{	O
l	O
,	O
2	O
,	O
3	O
,	O
...	O
}	O
from	O
a	O
sample	O
xl	O
,	O
...	O
,	O
xn	O
of	O
i.i.d	O
.	O
random	O
variables	O
with	O
p	O
{	O
x1	O
=	O
i	O
}	O
=	O
pi	O
,	O
i	O
:	O
:	O
:	O
:	O
1.	O
show	O
that	O
for	O
any	O
decreasing	O
sequence	O
{	O
an	O
}	O
of	O
positive	O
numbers	O
converging	O
to	O
zero	O
with	O
al	O
:	O
s	O
1/16	O
,	O
and	O
any	O
estimate	B
{	O
pi	O
,	O
n	O
}	O
,	O
there	O
exists	O
a	O
distribution	O
such	O
that	O
e	O
{	O
f	O
ipi	O
-	O
pi	O
,	O
nl	O
}	O
:	O
:	O
:	O
:	O
an	O
'	O
1=1	O
hint	O
:	O
consider	O
a	O
classification	O
problem	O
with	O
l	O
*	O
=	O
0	O
,	O
pry	O
=	O
o	O
}	O
=	O
1/2	O
,	O
and	O
x	O
concentrated	O
on	O
{	O
i	O
,	O
2	O
,	O
...	O
}	O
.	O
assume	O
that	O
the	O
class-conditional	B
probabilities	O
pia	O
)	O
=	O
p	O
{	O
x	O
=	O
ily	O
=	O
o	O
}	O
and	O
pil	O
)	O
=	O
p	O
{	O
x	O
=	O
ily	O
=	O
i	O
}	O
are	O
estimated	O
from	O
two	O
i.i.d	O
.	O
samples	O
xiol	O
,	O
...	O
,	O
x~o	O
)	O
and	O
xill	O
,	O
...	O
,	O
x~l	O
)	O
,	O
distributed	O
according	O
to	O
{	O
piol	O
}	O
and	O
{	O
pil	O
)	O
}	O
,	O
respectively	O
.	O
use	O
theorem	B
2.3	O
to	O
show	O
that	O
for	O
the	O
classification	O
rule	B
obtained	O
from	O
these	O
estimates	O
in	O
a	O
natural	O
way	O
,	O
therefore	O
the	O
lower	O
bound	O
of	O
theorem	O
7.2	O
can	O
be	O
applied	O
.	O
problem	O
7.5.	O
a	O
similar	O
slow-rate	O
result	O
appears	O
in	O
density	O
estimation	B
.	O
consider	O
the	O
prob	O
(	O
cid:173	O
)	O
lem	O
of	O
estimating	O
a	O
density	O
1	O
on	O
n	O
,	O
from	O
an	O
i.i.d	O
.	O
sample	O
x	O
i	O
,	O
...	O
,	O
xn	O
having	O
density	O
1.	O
show	O
that	O
for	O
any	O
decreasing	O
sequence	O
{	O
an	O
}	O
of	O
positive	O
numbers	O
converging	O
to	O
zero	O
with	O
a	O
i	O
:	O
s	O
1/16	O
,	O
and	O
any	O
density	O
estimate	O
in	O
,	O
there	O
exists	O
a	O
distribution	O
such	O
that	O
problems	O
and	O
exercises	O
119	O
e	O
{	O
f	O
if	O
(	O
x	O
)	O
-	O
fn	O
(	O
x	O
)	O
ldx	O
}	O
:	O
:	O
:	O
:	O
an·	O
this	O
result	O
was	O
proved	O
by	O
birge	O
(	O
1986	O
)	O
using	O
a	O
different-and	O
in	O
our	O
view	O
much	O
more	O
d	O
comp	O
lcate	O
-argument	O
.	O
hint	O
:	O
put	O
pi	O
=ji	O
fn	O
(	O
x	O
)	O
dx	O
and	O
apply	O
problem	O
7.4	O
.	O
(	O
x	O
)	O
dx	O
and	O
pi	O
,	O
n	O
=	O
ji	O
1·	O
ri+l	O
f	O
r	O
i+1	O
8	O
error	B
estimation	I
8.1	O
error	B
counting	I
estimating	O
the	O
error	O
probability	O
ln	O
=	O
p	O
{	O
gn	O
(	O
x	O
)	O
=i	O
y	O
i	O
dn	O
}	O
of	O
a	O
classification	O
function	O
gn	O
is	O
of	O
essential	O
importance	O
.	O
the	O
designer	O
always	O
wants	O
to	O
know	O
what	O
performance	O
can	O
be	O
expected	O
from	O
a	O
classifier	O
.	O
as	O
the	O
designer	O
does	O
not	O
know	O
the	O
distribution	B
of	O
the	O
data-otherwise	O
there	O
would	O
not	O
be	O
any	O
need	O
to	O
design	O
a	O
classifier-it	O
is	O
important	O
to	O
find	O
error	B
estimation	I
methods	O
that	O
work	O
well	O
without	O
any	O
condition	O
on	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
.	O
this	O
motivates	O
us	O
to	O
search	O
for	O
distribution-free	O
performance	O
bounds	O
for	O
error	B
estimation	I
methods	O
.	O
suppose	O
that	O
we	O
want	O
to	O
estimate	B
the	O
error	O
probability	O
of	O
a	O
classifier	O
gn	O
designed	O
from	O
the	O
training	O
sequence	O
dn	O
=	O
(	O
(	O
xl	O
,	O
yl	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
»	O
)	O
.	O
assume	O
first	O
that	O
a	O
testing	O
sequence	O
tm	O
=	O
(	O
(	O
xn+l	O
,	O
yn+l	O
)	O
,	O
...	O
,	O
(	O
xn+m	O
,	O
yn+m	O
)	O
is	O
available	O
,	O
which	O
is	O
a	O
sequence	O
of	O
i.i.d	O
.	O
pairs	O
that	O
are	O
independent	O
of	O
(	O
x	O
,	O
y	O
)	O
and	O
dn	O
,	O
and	O
that	O
are	O
distributed	O
as	O
(	O
x	O
,	O
y	O
)	O
.	O
an	O
obvious	O
way	O
to	O
estimate	B
ln	O
is	O
to	O
count	O
the	O
number	O
of	O
errors	O
that	O
gn	O
commits	O
on	O
tm	O
.	O
the	O
error-counting	O
estimator	O
ln	O
,	O
m	O
is	O
defined	O
by	O
the	O
relative	O
frequency	O
the	O
estimator	O
is	O
clearly	O
unbiased	O
in	O
the	O
sense	O
that	O
122	O
8.	O
error	B
estimation	I
and	O
the	O
conditional	O
distribution	B
of	O
mln	O
,	O
m	O
,	O
given	O
the	O
training	O
data	O
dn	O
,	O
is	O
binomial	B
with	O
parameters	O
m	O
and	O
ln	O
.	O
this	O
makes	O
analysis	O
easy	O
,	O
for	O
properties	O
of	O
the	O
binomial	B
distribution	I
are	O
well	O
known	O
.	O
one	O
main	O
tool	O
in	O
the	O
analysis	O
is	O
hoeffding	O
's	O
inequality	B
,	O
which	O
we	O
will	O
use	O
many	O
many	O
times	O
throughout	O
this	O
book	O
.	O
8.2	O
hoeffding	O
's	O
inequality	B
the	O
following	O
inequality	B
was	O
proved	O
for	O
binomial	B
random	O
variables	O
by	O
chernoff	O
(	O
1952	O
)	O
and	O
okamoto	O
(	O
1958	O
)	O
.	O
the	O
general	O
format	O
is	O
due	O
to	O
hoeffding	O
(	O
1963	O
)	O
:	O
theorem	B
8.1	O
.	O
(	O
hoeffding	O
(	O
1963	O
)	O
)	O
.	O
let	O
xl	O
,	O
...	O
,	O
xn	O
be	O
independent	O
bounded	O
random	O
variables	O
such	O
that	O
xi	O
falls	O
in	O
the	O
interval	O
[	O
ai	O
,	O
bd	O
with	O
probability	O
one	O
.	O
denote	O
their	O
sum	O
by	O
sn	O
=	O
l7=1	O
xi	O
.	O
thenfor	O
any	O
e	O
>	O
°	O
we	O
have	O
p	O
{	O
sn	O
-	O
esn	O
:	O
:	O
:	O
:	O
e	O
}	O
:	O
s	O
e-2e2/l7=1	O
(	O
bi-ad	O
and	O
the	O
proof	O
uses	O
a	O
simple	O
auxiliary	O
inequality	B
:	O
lemma	O
8.1.	O
let	O
x	O
be	O
a	O
random	O
variable	B
with	O
ex	O
=	O
0	O
,	O
a	O
:	O
s	O
x	O
:	O
s	O
b.	O
then	O
for	O
s	O
>	O
0	O
,	O
proof	O
.	O
note	O
that	O
by	O
convexity	O
of	O
the	O
exponential	B
function	O
esx	O
<	O
__	O
esb	O
+	O
__	O
esa	O
x-a	O
-	O
b-a	O
b-x	O
b-a	O
for	O
a	O
:	O
s	O
x	O
:	O
s	O
b.	O
exploiting	O
ex	O
=	O
0	O
,	O
and	O
introducing	O
the	O
notation	O
p	O
=	O
-a/	O
(	O
b	O
-	O
a	O
)	O
we	O
get	O
b	O
a	O
__	O
esa	O
_	O
__	O
esb	O
b-a	O
(	O
1	O
-	O
p	O
+	O
pes	O
(	O
b-a	O
»	O
)	O
e-ps	O
(	O
b-a	O
)	O
b-a	O
=	O
def	O
e¢	O
(	O
u	O
)	O
,	O
where	O
u	O
=	O
s	O
(	O
b	O
-	O
a	O
)	O
,	O
and	O
¢	O
(	O
u	O
)	O
=	O
-	O
pu	O
+	O
10g	O
(	O
1	O
-	O
p	O
+	O
pe	O
u	O
calculation	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
derivative	O
of	O
¢	O
is	O
)	O
.	O
but	O
by	O
straightforward	O
,	O
!	O
.	O
'	O
(	O
u	O
)	O
-	O
'p	O
-	O
-	O
+	O
p	O
p	O
+	O
(	O
1	O
p	O
-	O
p	O
e-	O
)	O
u	O
'	O
therefore	O
¢	O
(	O
o	O
)	O
=	O
¢	O
'	O
(	O
0	O
)	O
=	O
0.	O
moreover	O
,	O
¢//	O
(	O
u	O
)	O
=	O
p	O
(	O
1	O
)	O
-u	O
1	O
<	O
_	O
(	O
p	O
+	O
(	O
1	O
-	O
p	O
)	O
e-u	O
)	O
2	O
-	O
4	O
-	O
p	O
e	O
thus	O
,	O
by	O
taylor	O
series	O
expansion	O
with	O
remainder	O
,	O
for	O
some	O
e	O
e	O
[	O
0	O
,	O
u	O
]	O
,	O
8.2	O
hoeffding	O
's	O
inequality	B
123	O
proof	O
of	O
theorem	O
8.1.	O
the	O
proof	O
is	O
based	O
on	O
chernoff	O
's	O
bounding	O
method	O
(	O
chernoff	O
(	O
1952	O
)	O
)	O
:	O
by	O
markov	O
's	O
inequality	B
,	O
for	O
any	O
nonnegative	O
random	O
variable	B
x	O
,	O
and	O
any	O
e	O
>	O
0	O
,	O
ex	O
p	O
{	O
x	O
:	O
:	O
:	O
:e	O
}	O
:	O
s-	O
.	O
e	O
therefore	O
,	O
if	O
s	O
is	O
an	O
arbitrary	O
positive	O
number	O
,	O
then	O
for	O
any	O
random	O
variable	B
x	O
,	O
in	O
chernoff	O
's	O
method	O
,	O
we	O
find	O
an	O
s	O
>	O
°	O
that	O
minimizes	O
the	O
upper	O
bound	O
or	O
makes	O
the	O
upper	O
bound	O
small	O
.	O
in	O
our	O
case	O
,	O
we	O
have	O
=	O
e-se	O
n	O
e	O
{	O
es	O
(	O
xi-exi	O
)	O
}	O
<	O
e-se	O
n	O
es2	O
(	O
bi	O
-ai	O
)	O
2j8	O
i=l	O
n	O
n	O
(	O
by	O
independence	O
)	O
(	O
by	O
lemma	O
8.1	O
)	O
i=l	O
=	O
=	O
the	O
second	O
inequality	B
is	O
proved	O
analogouslo/	O
.	O
0	O
the	O
two	O
inequalities	O
in	O
theorem	O
8.1	O
may	O
be	O
combined	O
to	O
get	O
now	O
,	O
we	O
can	O
apply	O
this	O
inequality	B
to	O
get	O
a	O
distribution-free	O
performance	O
bound	O
for	O
the	O
counting	O
error	O
estimate	O
:	O
corollary	O
8.1.	O
for	O
every	O
e	O
>	O
0	O
,	O
124	O
8.	O
error	B
estimation	I
the	O
variance	B
of	I
the	O
estimate	B
can	O
easily	O
be	O
computed	O
using	O
the	O
fact	O
that	O
,	O
condi	O
(	O
cid:173	O
)	O
tioned	O
on	O
the	O
data	O
dn	O
,	O
mln	O
,	O
m	O
is	O
binomially	O
distributed	O
:	O
these	O
are	O
just	O
the	O
types	O
of	O
inequalities	O
we	O
want	O
,	O
for	O
these	O
are	O
valid	O
for	O
any	O
distri	O
(	O
cid:173	O
)	O
bution	O
and	O
data	O
size	O
,	O
and	O
the	O
bounds	O
do	O
not	O
even	O
depend	O
on	O
gn	O
'	O
consider	O
a	O
special	O
case	O
in	O
which	O
all	O
the	O
x/s	O
take	O
values	O
on	O
[	O
-c	O
,	O
c	O
]	O
and	O
have	O
zero	O
mean	O
.	O
then	O
hoeffding	O
's	O
inequality	B
states	O
that	O
p	O
{	O
snl	O
n	O
>	O
e	O
}	O
s	O
e-nr	O
:	O
2	O
jc2c	O
2	O
)	O
.	O
this	O
bound	O
,	O
while	O
useful	O
for	O
e	O
larger	O
than	O
c	O
1	O
-vii	O
,	O
ignores	O
variance	O
information	O
.	O
when	O
var	O
{	O
xi	O
}	O
«	O
c2	O
,	O
it	O
is	O
indeed	O
possible	O
to	O
outperform	O
hoeffding	O
's	O
inequality	B
.	O
in	O
particular	O
,	O
we	O
have	O
:	O
theorem	B
8.2	O
.	O
(	O
bennett	O
(	O
1962	O
)	O
and	O
bernstein	O
(	O
1946	O
)	O
)	O
.	O
let	O
xl	O
,	O
...	O
,	O
xn	O
be	O
independent	O
real-valued	O
random	O
variables	O
with	O
zero	O
mean	O
,	O
and	O
assume	O
that	O
xi	O
:	O
:	O
;	O
c	O
with	O
probability	O
one	O
.	O
let	O
(	O
}	O
2	O
=	O
-	O
lvar	O
{	O
xd	O
.	O
1	O
n	O
n	O
i=l	O
then	O
,	O
for	O
any	O
e	O
>	O
0	O
,	O
(	O
bennett	O
(	O
1962	O
)	O
)	O
,	O
and	O
p	O
{	O
~	O
t	O
xi	O
>	O
e	O
}	O
:	O
:	O
;	O
exp	O
(	O
__	O
2_ne_2	O
-	O
)	O
2	O
(	O
}	O
+	O
2ce	O
13	O
n	O
i	O
:	O
:	O
:	O
l	O
(	O
bernstein	O
(	O
1946	O
)	O
)	O
.	O
the	O
proofs	O
are	O
left	O
as	O
exercises	O
(	O
problem	O
8.2	O
)	O
.	O
we	O
note	O
that	O
bernstein	O
's	O
inequality	B
kicks	O
in	O
when	O
e	O
is	O
larger	O
than	O
about	O
max	O
(	O
(	O
)	O
1	O
-vii	O
,	O
cl	O
-vii	O
)	O
.	O
it	O
is	O
typically	O
better	O
than	O
hoeffding	O
's	O
inequality	B
when	O
(	O
)	O
«	O
c.	O
8.3	O
error	B
estimation	I
without	O
testing	O
data	O
a	O
serious	O
problem	O
concerning	O
the	O
practical	O
applicability	O
of	O
the	O
estimate	B
introduced	O
above	O
is	O
that	O
it	O
requires	O
a	O
large	O
,	O
independent	O
testing	B
sequence	I
.	O
in	O
practice	O
,	O
how	O
(	O
cid:173	O
)	O
ever	O
,	O
an	O
additional	O
sample	O
is	O
rarely	O
available	O
.	O
one	O
usually	O
wants	O
to	O
incorporate	O
all	O
available	O
(	O
xi	O
,	O
yi	O
)	O
pairs	O
in	O
the	O
decision	O
function	O
.	O
in	O
such	O
cases	O
,	O
to	O
estimate	B
l	O
n	O
,	O
we	O
have	O
to	O
rely	O
on	O
the	O
training	O
data	O
only_	O
there	O
are	O
well-known	O
methods	O
that	O
we	O
8.4	O
selecting	O
classifiers	O
125	O
will	O
discuss	O
later	O
that	O
are	O
based	O
on	O
cross-validation	O
(	O
or	O
leave-one-out	B
)	O
(	O
lunts	O
and	O
brailovsky	O
(	O
1967	O
)	O
;	O
stone	O
(	O
1974	O
)	O
)	O
;	O
and	O
holdout	B
,	O
resubstitution	B
,	O
rotation	B
,	O
smoothing	O
,	O
and	O
bootstrapping	O
(	O
efron	O
(	O
1979	O
)	O
,	O
(	O
1983	O
)	O
)	O
,	O
which	O
may	O
be	O
employed	O
to	O
construct	O
an	O
empirical	B
risk	I
from	O
the	O
training	O
sequence	O
,	O
thus	O
obviating	O
the	O
need	O
for	O
a	O
testing	O
sequence	O
.	O
(	O
see	O
kanal	O
(	O
1974	O
)	O
,	O
cover	O
and	O
wagner	O
(	O
1975	O
)	O
,	O
toussaint	O
(	O
1974a	O
)	O
,	O
glick	O
(	O
1978	O
)	O
,	O
hand	O
(	O
1986	O
)	O
,	O
jain	O
,	O
dubes	O
,	O
and	O
chen	O
(	O
1987	O
)	O
,	O
and	O
mclachlan	O
(	O
1992	O
)	O
for	O
surveys	O
,	O
discussion	O
,	O
and	O
empirical	B
comparison	O
.	O
)	O
analysis	O
of	O
these	O
methods	O
,	O
in	O
general	O
,	O
is	O
clearly	O
a	O
much	O
harder	O
problem	O
,	O
as	O
~n	O
can	O
depend	O
on	O
dn	O
in	O
a	O
rather	O
complicated	O
way	O
.	O
if	O
we	O
construct	O
some	O
estimator	O
ln	O
from	O
d	O
n	O
,	O
then	O
it	O
would	O
be	O
desirable	O
to	O
obtain	O
distribution-free	O
bounds	O
on	O
or	O
on	O
e	O
{	O
iln	O
-	O
lnl	O
q	O
}	O
for	O
some	O
q	O
:	O
:	O
:	O
:	O
1.	O
conditional	O
probabilities	O
and	O
expectations	O
given	O
dn	O
are	O
mean	O
(	O
cid:173	O
)	O
ingless	O
,	O
since	O
everything	O
is	O
a	O
funs	O
,	O
eion	O
of	O
dn	O
.	O
here	O
,	O
however	O
,	O
we	O
have	O
to	O
be	O
much	O
more	O
careful	O
as	O
we	O
do	O
not	O
want	O
ln	O
to	O
be	O
optimistically	O
biased	O
because	O
the	O
same	O
data	O
are	O
used	O
both	O
for	O
training	O
and	O
testing	O
.	O
distribution-free	O
bounds	O
for	O
the	O
above	O
quantities	O
would	O
be	O
extremely	O
helpful	O
,	O
as	O
we	O
usually	O
do	O
not	O
know	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
.	O
while	O
for	O
some	O
rules	O
such	O
estimates	O
exist-we	O
will	O
exhibit	O
several	O
avenues	O
in	O
chapters	O
22	O
,	O
23	O
,	O
24	O
,	O
25	O
,	O
26	O
,	O
and	O
3	O
i-it	O
is	O
disappointing	O
that	O
a	O
single	O
error	B
estimation	I
method	O
can	O
not	O
possibly	O
work	O
for	O
all	O
discrimination	O
rules	O
.	O
it	O
is	O
therefore	O
important	O
to	O
point	O
out	O
that	O
we	O
have	O
to	O
consider	O
(	O
gn	O
,	O
ln	O
)	O
pairs-for	O
every	O
rule	B
one	O
or	O
more	O
error	O
estimates	O
must	O
be	O
found	O
if	O
possible	O
,	O
and	O
vice	O
versa	O
,	O
for	O
every	O
error	O
estimate	O
,	O
its	O
limitations	O
have	O
to	O
be	O
stated	O
.	O
secondly	O
,	O
rules	O
for	O
which	O
no	O
good	O
error	O
estimates	O
are	O
known	O
should	O
be	O
avoided	O
.	O
luckily	O
,	O
most	O
popular	O
rules	O
do	O
not	O
fall	O
into	O
this	O
category	O
.	O
on	O
the	O
other	O
hand	O
,	O
proven	O
distribution-free	O
performance	O
guarantees	O
are	O
rarely	O
available-see	O
chapters	O
23	O
and	O
24	O
for	O
examples	O
.	O
8.4	O
selecting	O
classifiers	O
probably	O
the	O
most	O
important	O
application	O
of	O
error	O
estimation	B
is	O
in	O
the	O
selection	B
of	O
a	O
classification	O
function	O
from	O
a	O
class	O
c	O
of	O
functions	O
.	O
if	O
a	O
class	O
c	O
of	O
classifiers	O
is	O
given	O
,	O
then	O
it	O
is	O
tempting	O
to	O
pick	O
the	O
one	O
that	O
minimizes	O
an	O
estimate	B
of	O
the	O
error	O
probability	O
over	O
the	O
class	O
.	O
a	O
good	O
method	O
should	O
pick	O
a	O
classifier	O
with	O
an	O
error	O
probability	O
that	O
is	O
close	O
to	O
the	O
minimal	O
error	O
probability	O
in	O
the	O
class	O
.	O
here	O
we	O
require	O
much	O
more	O
than	O
distribution-free	O
performance	O
bounds	O
of	O
the	O
error	O
estimator	O
for	O
each	O
of	O
the	O
classifiers	O
in	O
the	O
class	O
.	O
problem	O
8.8	O
demonstrates	O
that	O
it	O
is	O
not	O
sufficient	O
to	O
be	O
able	O
to	O
estimate	B
the	O
error	O
probability	O
of	O
all	O
classifiers	O
in	O
the	O
class	O
.	O
intuitively	O
,	O
if	O
we	O
can	O
estimate	B
the	O
error	O
probability	O
for	O
the	O
classifiers	O
in	O
c	O
uniformly	O
well	O
,	O
then	O
the	O
classification	O
function	O
that	O
minimizes	O
the	O
estimated	O
error	O
probability	O
is	O
likely	O
to	O
have	O
an	O
error	O
probability	O
that	O
is	O
close	O
to	O
the	O
best	O
in	O
the	O
class	O
.	O
to	O
certify	O
this	O
126	O
8.	O
error	B
estimation	I
intuition	O
,	O
consider	O
the	O
following	O
situation	O
:	O
let	O
c	O
be	O
a	O
class	O
of	O
classifiers	O
,	O
that	O
is	O
,	O
a	O
class	O
of	O
mappings	O
of	O
the	O
form	O
¢	O
:	O
n	O
d	O
-+	O
{	O
o	O
,	O
i	O
}	O
.	O
assume	O
that	O
the	O
error	O
count	O
/	O
'	O
<	O
0-	O
1	O
n	O
ln	O
(	O
¢	O
)	O
=	O
-	O
``	O
n~	O
j	O
i	O
{	O
<	O
p	O
(	O
x	O
)	O
¥y	O
}	O
j	O
j=l	O
is	O
used	O
to	O
estimate	B
the	O
error	O
probability	O
l	O
(	O
¢	O
)	O
=	O
p	O
{	O
¢	O
(	O
x	O
)	O
i	O
y	O
}	O
of	O
each	O
classifier	B
¢	O
e	O
c.	O
denote	O
by	O
¢	O
:	O
the	O
classifier	B
that	O
minimizes	O
the	O
estimated	O
error	O
probability	O
over	O
the	O
class	O
:	O
ln	O
(	O
¢~	O
)	O
:	O
:	O
;	O
ln	O
(	O
¢	O
)	O
for	O
all	O
¢	O
e	O
c.	O
then	O
for	O
the	O
error	O
probability	O
of	O
the	O
selected	O
rule	B
we	O
have	O
:	O
lemma	O
8.2	O
.	O
(	O
vapnik	O
and	O
chervonenkis	O
(	O
1974c	O
)	O
;	O
see	O
also	O
devroye	O
(	O
1988b	O
)	O
.	O
l	O
(	O
¢~	O
)	O
-	O
inf	O
l	O
(	O
¢	O
)	O
:	O
:	O
;	O
2	O
sup	O
iln	O
(	O
¢	O
)	O
-	O
l	O
(	O
¢	O
)	O
i	O
,	O
<	O
pec	O
<	O
pec	O
iln	O
(	O
¢~	O
)	O
-	O
l	O
(	O
¢	O
:	O
)	O
i	O
:	O
:	O
;	O
sup	O
iln	O
(	O
¢	O
)	O
-	O
l	O
(	O
¢	O
)	O
i·	O
<	O
pec	O
proof	O
.	O
l	O
(	O
¢~	O
)	O
-	O
l	O
n	O
(	O
¢	O
,	O
:	O
)	O
+	O
ln	O
(	O
¢~	O
)	O
-	O
inf	O
l	O
(	O
¢	O
)	O
<	O
pec	O
<	O
l	O
(	O
¢~	O
)	O
-	O
ln	O
(	O
¢~	O
)	O
+	O
sup	O
iln	O
(	O
¢	O
)	O
-	O
l	O
(	O
¢	O
)	O
i	O
<	O
pec	O
<	O
2	O
sup	O
iln	O
(	O
¢	O
)	O
-	O
l	O
(	O
¢	O
)	O
i	O
.	O
<	O
pec	O
the	O
second	O
inequality	B
is	O
trivially	O
true	O
.	O
0	O
we	O
see	O
that	O
upper	O
bounds	O
for	O
sup	O
<	O
pec	O
iln	O
(	O
¢	O
)	O
-	O
l	O
(	O
¢	O
)	O
i	O
provide	O
us	O
with	O
upper	O
bounds	O
for	O
two	O
things	O
simultaneously	O
:	O
(	O
1	O
)	O
an	O
upper	O
bound	O
for	O
the	O
suboptimality	O
of	O
¢	O
:	O
within	O
c	O
,	O
that	O
is	O
,	O
a	O
bound	O
for	O
l	O
(	O
¢	O
,	O
~	O
)	O
-	O
inf	O
<	O
pec	O
l	O
(	O
¢	O
)	O
.	O
(	O
2	O
)	O
an	O
upper	O
bound	O
for	O
the	O
error	O
iln	O
(	O
¢	O
:	O
)	O
-	O
l	O
(	O
¢	O
:	O
)	O
i	O
committed	O
when	O
l	O
n	O
(	O
¢	O
:	O
)	O
is	O
used	O
to	O
estimate	B
the	O
probability	O
of	O
error	O
l	O
(	O
¢	O
:	O
)	O
of	O
the	O
selected	O
rule	B
.	O
in	O
other	O
words	O
,	O
by	O
bounding	O
sup	O
<	O
pec	O
iln	O
(	O
¢	O
)	O
-	O
l	O
(	O
¢	O
)	O
i	O
,	O
we	O
kill	O
two	O
flies	O
at	O
once	O
.	O
it	O
is	O
particularly	O
useful	O
to	O
know	O
that	O
even	O
though	O
ln	O
(	O
¢	O
:	O
)	O
is	O
usually	O
optimistically	O
biased	O
,	O
it	O
is	O
within	O
given	O
bounds	O
of	O
the	O
unknown	O
probability	O
of	O
error	O
with	O
¢	O
:	O
'	O
and	O
that	O
no	O
other	O
test	O
sample	O
is	O
needed	O
to	O
estimate	B
this	O
probability	O
of	O
error	O
.	O
whenever	O
8.4	O
selecting	O
classifiers	O
127.	O
our	O
bounds	O
indicate	O
that	O
we	O
are	O
close	O
to	O
the	O
optimum	O
in	O
c	O
,	O
we	O
must	O
at	O
the	O
same	O
time	O
have	O
a	O
good	O
estimate	B
of	O
the	O
probability	O
of	O
error	O
,	O
and	O
vice	O
versa	O
.	O
as	O
a	O
simple	O
,	O
but	O
interesting	O
application	O
of	O
lemma	O
8.2	O
we	O
consider	O
the	O
case	O
when	O
the	O
class	O
c	O
contains	O
finitely	O
many	O
classifiers	O
.	O
theorem	B
8.3.	O
assume	O
that	O
the	O
cardinality	O
oj	O
c	O
is	O
bounded	O
by	O
n.	O
then	O
we	O
have	O
for	O
all	O
e	O
>	O
0	O
,	O
proof	O
.	O
p	O
{	O
sup	O
iln	O
(	O
¢	O
)	O
-	O
l	O
(	O
¢	O
)	O
i	O
>	O
e	O
}	O
<	O
pec	O
<	O
lp	O
{	O
iln	O
(	O
¢	O
)	O
-	O
l	O
(	O
¢	O
)	O
i	O
>	O
e	O
}	O
<	O
pec	O
<	O
2ne-	O
2ne2	O
,	O
where	O
we	O
used	O
hoeffding	O
'	O
s	O
inequality	B
,	O
and	O
the	O
fact	O
that	O
the	O
random	O
variable	B
nln	O
(	O
¢	O
)	O
is	O
binomially	O
distributed	O
with	O
parameters	O
nand	O
l	O
(	O
¢	O
)	O
.	O
0	O
remark	O
.	O
distribution-free	O
properties	O
.	O
theorem	B
8.3	O
shows	O
that	O
the	O
problem	O
studied	O
here	O
is	O
purely	O
combinatorial	O
.	O
the	O
actual	O
distribution	B
of	O
the	O
data	O
does	O
not	O
playa	O
role	O
at	O
all	O
in	O
the	O
upper	O
bounds	O
.	O
0	O
remark	O
.	O
without	O
testing	O
data	O
.	O
very	O
often	O
,	O
a	O
class	O
of	O
rules	O
c	O
of	O
the	O
form	O
¢n	O
(	O
x	O
)	O
=	O
¢n	O
(	O
x	O
,	O
dn	O
)	O
is	O
given	O
,	O
and	O
the	O
same	O
data	O
dn	O
are	O
used	O
to	O
select	O
a	O
rule	O
by	O
minimizing	O
some	O
estimates	O
ln	O
(	O
¢n	O
)	O
of	O
the	O
error	O
probabilities	O
l	O
(	O
¢n	O
)	O
=	O
p	O
{	O
¢n	O
(	O
x	O
)	O
=i	O
yidn	O
}	O
.	O
a	O
similar	O
analysis	O
can	O
be	O
carried	O
out	O
in	O
this	O
case	O
.	O
in	O
particular	O
,	O
if	O
¢~	O
denotes	O
the	O
selected	O
rule	B
,	O
then	O
we	O
have	O
similar	O
to	O
lemma	O
8.2	O
:	O
theorem	B
8.4.	O
and	O
iln	O
(	O
¢~	O
)	O
-	O
l	O
(	O
¢~	O
)	O
i	O
~	O
sup	O
iln	O
(	O
¢n	O
)	O
-	O
l	O
(	O
¢n	O
)	O
l.	O
<	O
pn	O
ec	O
if	O
c	O
is	O
finite	O
,	O
then	O
again	O
,	O
similar	O
to	O
theorem	B
8.3	O
,	O
we	O
have	O
for	O
example	O
128	O
8.	O
error	B
estimation	I
8.5	O
estimating	O
the	O
bayes	O
error	O
it	O
is	O
also	O
important	O
to	O
have	O
a	O
good	O
estimate	B
of	O
the	O
optimal	O
error	O
probability	O
l	O
*	O
.	O
first	O
of	O
all	O
,	O
if	O
l	O
*	O
is	O
large	O
,	O
we	O
would	O
know	O
beforehand	O
that	O
any	O
rule	B
is	O
going	O
to	O
perform	O
poorly	O
.	O
then	O
perhaps	O
the	O
information	O
might	O
be	O
used	O
to	O
return	O
to	O
the	O
feature	O
selection	O
stage	O
.	O
also	O
,	O
a	O
comparison	O
of	O
estimates	O
of	O
ln	O
and	O
l	O
*	O
gives	O
us	O
an	O
idea	O
how	O
much	O
room	O
is	O
left	O
for	O
improvement	O
.	O
typically	O
,	O
l	O
*	O
is	O
estimated	O
by	O
an	O
estimate	B
of	O
the	O
error	O
probability	O
of	O
some	O
consistent	O
classification	O
rule	B
(	O
see	O
fukunaga	O
and	O
kessel	O
(	O
1971	O
)	O
,	O
chen	O
and	O
fu	O
(	O
1973	O
)	O
,	O
fukunaga	O
and	O
hummels	O
(	O
1987	O
)	O
,	O
and	O
garnett	O
and	O
yau	O
(	O
1977	O
»	O
.	O
clearly	O
,	O
if	O
the	O
estimate	B
t1	O
we	O
use	O
is	O
consistent	O
in	O
the	O
sense	O
that	O
in	O
-	O
ln	O
-+	O
°	O
with	O
probability	O
one	O
as	O
n	O
-+	O
00	O
,	O
and	O
the	O
rule	B
is	O
strongly	O
consistent	O
,	O
then	O
in	O
-+	O
l*	O
with	O
probability	O
one	O
.	O
in	O
other	O
words	O
,	O
we	O
have	O
a	O
consistent	O
estimate	B
of	O
the	O
bayes	O
error	O
probability	O
.	O
there	O
are	O
two	O
problems	O
with	O
this	O
approach	O
.	O
the	O
first	O
problem	O
is	O
that	O
if	O
our	O
purpose	O
is	O
comparing	O
l	O
*	O
with	O
l	O
n	O
,	O
then	O
using	O
the	O
same	O
estimate	B
for	O
both	O
of	O
them	O
does	O
not	O
~ive	O
any	O
information	O
.	O
the	O
other	O
problem	O
is	O
that	O
even	O
though	O
for	O
many	O
classifiers	O
,	O
ln	O
-	O
ln	O
can	O
be	O
guaranteed	O
to	O
converge	O
to	O
zero	O
rapidly	O
,	O
regardless	O
what	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
is	O
(	O
see	O
ch~ters	O
23	O
and	O
24	O
)	O
,	O
in	O
view	O
of	O
the	O
results	O
of	O
chapter	O
7	O
,	O
the	O
rate	B
of	I
convergence	I
of	O
ln	O
to	O
l	O
*	O
using	O
such	O
a	O
method	O
may	O
be	O
arbitrarily	O
slow	O
.	O
thus	O
,	O
we	O
can	O
not	O
expect	O
good	O
performance	O
for	O
all	O
distributions	O
from	O
such	O
a	O
method	O
.	O
the	O
question	O
is	O
whether	O
it	O
is	O
possible	O
to	O
come	O
up	O
with	O
a	O
method	O
of	O
estimating	O
l	O
*	O
such	O
that	O
the	O
difference	O
in	O
-	O
l	O
*	O
converges	O
to	O
zero	O
rapidly	O
for	O
all	O
distributions	O
.	O
unfortunately	O
,	O
there	O
is	O
no	O
method	O
that	O
guarantees	O
a	O
certain	O
finite	O
sample	O
performance	O
for	O
all	O
distributions	O
.	O
this	O
disappointing	O
fact	O
is	O
reflected	O
in	O
the	O
following	O
negative	O
result	O
:	O
theorem	B
8.5.	O
for	O
every	O
n	O
,	O
for	O
any	O
estimate	B
in	O
of	O
the	O
bayes	O
error	O
probability	O
l	O
*	O
,	O
and	O
for	O
every	O
e	O
>	O
0	O
,	O
there	O
exists	O
a	O
distribution	O
of	O
(	O
x	O
,	O
y	O
)	O
,	O
such	O
that	O
--	O
--	O
e	O
{	O
iln	O
-	O
l*i	O
}	O
:	O
:	O
:	O
4	O
-	O
e.	O
1	O
proof	O
.	O
for	O
a	O
fixed	O
n	O
,	O
we	O
construct	O
a	O
family	O
f	O
of	O
distributions	O
,	O
and	O
show	O
that	O
for	O
at	O
least	O
one	O
member	O
of	O
the	O
family	O
,	O
e	O
{	O
i	O
in	O
-	O
l	O
*	O
i	O
}	O
:	O
:	O
:	O
~	O
-	O
e.	O
the	O
family	O
contains	O
2m	O
+	O
1	O
distributions	O
,	O
where	O
m	O
is	O
a	O
large	O
integer	O
specified	O
later	O
.	O
in	O
all	O
cases	O
,	O
xl	O
,	O
...	O
,	O
xn	O
are	O
drawn	O
independently	O
by	O
a	O
uniform	O
distribution	B
from	O
the	O
set	O
{	O
i	O
,	O
...	O
,	O
m	O
}	O
.	O
let	O
bo	O
,	O
bi	O
,	O
b2	O
,	O
...	O
,	O
bn	O
be	O
i.i.d	O
.	O
bernoulli	O
random	O
variables	O
,	O
independent	O
of	O
the	O
xi	O
's	O
,	O
with	O
p	O
{	O
bi	O
=	O
o	O
}	O
=	O
p	O
{	O
bi	O
=	O
i	O
}	O
=	O
1/2	O
.	O
for	O
the	O
first	O
member	O
of	O
the	O
family	O
f	O
,	O
let	O
yi	O
=	O
bi	O
for	O
i	O
=	O
1	O
,	O
...	O
,	O
n.	O
thus	O
,	O
for	O
this	O
distribution	B
,	O
l	O
*	O
=	O
1/2	O
.	O
the	O
bayes	O
error	O
for	O
the	O
other	O
2m	O
members	O
of	O
the	O
family	O
is	O
zero	O
.	O
these	O
distributions	O
are	O
determined	O
by	O
m	O
binary	B
parameters	O
ai	O
,	O
a2	O
,	O
'	O
''	O
,	O
am	O
e	O
{	O
o	O
,	O
i	O
}	O
as	O
follows	O
:	O
r	O
;	O
(	O
i	O
)	O
=	O
p	O
{	O
y	O
=	O
11x	O
=	O
i	O
}	O
=	O
ai	O
.	O
in	O
other	O
words	O
,	O
yi	O
=	O
a	O
xi	O
for	O
every	O
i	O
=	O
1	O
,	O
...	O
,	O
n.	O
clearly	O
,	O
l	O
*	O
=	O
°	O
for	O
these	O
distribu	O
(	O
cid:173	O
)	O
tions	O
.	O
note	O
also	O
that	O
all	O
distributions	O
with	O
x	O
distributed	O
uniformly	O
on	O
{	O
i	O
,	O
...	O
,	O
m	O
}	O
problems	O
and	O
exercises	O
129	O
and	O
l	O
*	O
=	O
0	O
are	O
members	O
of	O
the	O
family	O
.	O
just	O
as	O
in	O
the	O
proofs	O
of	O
theorems	O
7.1	O
and	O
7.2	O
,	O
we	O
randomize	O
over	O
the	O
family	O
f	O
of	O
distributions	O
.	O
however	O
,	O
the	O
way	O
of	O
random	O
(	O
cid:173	O
)	O
ization	O
is	O
different	O
here	O
.	O
the	O
trick	O
is	O
to	O
use	O
bo	O
,	O
b	O
i	O
,	O
b2	O
,	O
...	O
,	O
bn	O
in	O
randomly	O
picking	O
a	O
distribution	O
.	O
(	O
recall	O
that	O
these	O
random	O
variables	O
are	O
just	O
the	O
labels	O
yi	O
,	O
...	O
,	O
yn	O
in	O
the	O
training	O
sequence	O
for	O
the	O
first	O
distribution	B
in	O
the	O
family	O
.	O
)	O
we	O
choose	O
a	O
distribu	O
(	O
cid:173	O
)	O
tion	O
randomly	O
,	O
as	O
follows	O
:	O
if	O
bo	O
=	O
0	O
,	O
then	O
we	O
choose	O
the	O
first	O
member	O
of	O
f	O
(	O
the	O
one	O
with	O
l	O
*	O
=	O
1/2	O
)	O
.	O
if	O
bo	O
=	O
1	O
,	O
then	O
the	O
labels	O
of	O
the	O
training	O
sequence	O
are	O
given	O
by	O
if	O
xi	O
=i	O
xl	O
,	O
xi	O
=i	O
x	O
2	O
,	O
...	O
,	O
xi	O
=i	O
xi	O
-	O
l	O
if	O
j	O
<	O
i	O
is	O
the	O
smallest	O
index	O
such	O
that	O
xi	O
=	O
x	O
j	O
.	O
note	O
that	O
in	O
case	O
of	O
bo	O
=	O
1	O
,	O
for	O
any	O
fixed	O
realization	O
hi	O
,	O
...	O
,	O
hn	O
e	O
{	O
o	O
,	O
i	O
}	O
of	O
b	O
i	O
,	O
...	O
,	O
bn	O
,	O
the	O
bayes	O
risk	O
is	O
zero	O
.	O
therefore	O
,	O
the	O
distribution	B
is	O
in	O
the	O
family	O
f.	O
now	O
,	O
let	O
a	O
be	O
the	O
event	O
that	O
all	O
the	O
xi	O
's	O
are	O
different	O
.	O
observe	O
that	O
under	O
a	O
,	O
ln	O
is	O
a	O
function	O
of	O
xl	O
,	O
...	O
,	O
x	O
n	O
,	O
b	O
i	O
,	O
...	O
,	O
bn	O
only	O
,	O
but	O
not	O
bo	O
.	O
therefore	O
,	O
supe	O
{	O
iln	O
-	O
l*i	O
}	O
>	O
e	O
{	O
iln	O
-	O
l*i	O
}	O
:	O
f	O
(	O
with	O
bo	O
,	O
b	O
i	O
,	O
...	O
,	O
bn	O
random	O
)	O
>	O
e	O
{	O
i	O
a	O
i	O
ln	O
-	O
l	O
*	O
i	O
}	O
=	O
e	O
{	O
fa	O
(	O
f	O
[	O
m	O
)	O
ii	O
.	O
-~	O
1+	O
i	O
[	O
b	O
,	O
''	O
!	O
)	O
ii	O
.	O
-	O
01	O
)	O
}	O
e	O
{	O
lahlin	O
~i	O
+	O
lin	O
-ol	O
)	O
}	O
>	O
e	O
{	O
fa	O
~	O
}	O
~p	O
{	O
a	O
}	O
.	O
now	O
,	O
if	O
we	O
pick	O
m	O
large	O
enough	O
,	O
p	O
{	O
a	O
}	O
can	O
be	O
as	O
close	O
to	O
1	O
as	O
desired	O
.	O
hence	O
,	O
1	O
sup	O
e	O
{	O
i	O
ln	O
-	O
l	O
*	O
i	O
}	O
:	O
:	O
:	O
:	O
4	O
'	O
--	O
where	O
the	O
supremum	O
is	O
taken	O
over	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
.	O
0	O
problems	O
and	O
exercises	O
..problem	O
8.1.	O
let	O
b	O
be	O
a	O
binomial	O
random	O
variable	B
with	O
parameters	O
nand	O
p.	O
show	O
that	O
and	O
p	O
{	O
b	O
>	O
e	O
}	O
:	O
s	O
ee-np-elog	O
(	O
ejnp	O
)	O
(	O
e	O
>	O
np	O
)	O
p	O
{	O
b	O
<	O
e	O
}	O
:	O
s	O
ee-np-elog	O
(	O
ejnp	O
)	O
(	O
e	O
<	O
np	O
)	O
.	O
(	O
chernoff	O
(	O
1952	O
)	O
.	O
)	O
hint	O
:	O
proceed	O
by	O
chernoff	O
's	O
bounding	O
method	O
.	O
130	O
8.	O
error	B
estimation	I
problem	O
8.2.	O
prove	O
the	O
inequalities	O
of	O
bennett	O
and	O
bernstein	O
given	O
in	O
theorem	O
8.2.	O
to	O
help	O
you	O
,	O
we	O
will	O
guide	O
you	O
through	O
different	O
stages	O
:	O
(	O
1	O
)	O
show	O
that	O
for	O
any	O
s	O
>	O
0	O
,	O
and	O
any	O
random	O
variable	B
x	O
with	O
ex	O
=	O
0	O
,	O
ex2	O
=	O
(	O
52	O
,	O
x	O
:	O
:sc	O
,	O
where	O
(	O
2	O
)	O
show	O
that	O
ffl	O
(	O
u	O
)	O
:	O
:s	O
°	O
for	O
u	O
~	O
0	O
.	O
(	O
3	O
)	O
by	O
chernoff	O
's	O
bounding	O
method	O
,	O
show	O
that	O
feu	O
)	O
=	O
log	O
(	O
_l_	O
e	O
-	O
csu	O
+	O
_u_	O
ecs	O
)	O
.	O
l+u	O
l+u	O
(	O
4	O
)	O
show	O
that	O
feu	O
)	O
:	O
:s	O
f	O
(	O
o	O
)	O
+	O
uf	O
'	O
(	O
o	O
)	O
=	O
(	O
e	O
cs	O
(	O
5	O
)	O
using	O
the	O
bound	O
of	O
(	O
4	O
)	O
,	O
find	O
the	O
optimal	O
value	O
of	O
s	O
and	O
derive	O
bennett	O
's	O
inequality	B
.	O
-	O
1	O
-	O
cs	O
)	O
u.	O
problem	O
8.3.	O
use	O
bernstein	O
's	O
inequality	B
to	O
show	O
that	O
if	O
b	O
is	O
a	O
binomial	O
(	O
n	O
,	O
p	O
)	O
random	O
variable	B
,	O
then	O
for	O
e	O
>	O
0	O
,	O
and	O
problem	O
8.4.	O
letx	O
1	O
,	O
•••	O
,	O
xn	O
be	O
independent	O
binary-valued	O
random	O
variables	O
withp	O
{	O
xi	O
=	O
i	O
}	O
=	O
1	O
-	O
p	O
{	O
xi	O
=	O
o	O
}	O
=	O
pi	O
.	O
set	O
p	O
=	O
(	O
1ln	O
)	O
l~=l	O
pi	O
and	O
sn	O
=	O
l~=l	O
xi	O
'	O
prove	O
that	O
(	O
angluin	O
and	O
valiant	O
(	O
1979	O
)	O
,	O
see	O
also	O
hagerup	O
and	O
rub	O
(	O
1990	O
»	O
.	O
compare	O
the	O
results	O
with	O
bernstein	O
's	O
inequality	B
for	O
this	O
case	O
.	O
hint	O
:	O
put	O
s	O
=	O
log	O
(	O
1	O
+	O
e	O
)	O
and	O
s	O
=	O
-log	O
(	O
1	O
-	O
e	O
)	O
in	O
the	O
chernoff	O
bounding	O
argument	O
.	O
prove	O
and	O
exploit	O
the	O
elementary	O
inequalities	O
and	O
e2	O
-2	O
~e-	O
(	O
1+e	O
)	O
log	O
(	O
1+e	O
)	O
,	O
e	O
e	O
(	O
-l	O
,	O
o	O
]	O
.	O
problem	O
8.5.	O
let	O
b	O
be	O
a	O
binomial	O
(	O
n	O
,	O
p	O
)	O
random	O
variable	B
.	O
show	O
that	O
for	O
p	O
:	O
:s	O
a	O
<	O
1	O
,	O
show	O
that	O
for	O
°	O
<	O
a	O
<	O
p	O
the	O
same	O
upper	O
bounds	O
hold	O
for	O
p	O
{	O
b	O
:	O
:s	O
an	O
}	O
(	O
karp	O
(	O
1988	O
)	O
,	O
see	O
also	O
hagerup	O
and	O
rub	O
(	O
1990	O
»	O
)	O
.	O
hint	O
:	O
use	O
chernoff	O
's	O
method	O
with	O
parameter	O
s	O
and	O
set	O
s	O
=	O
log	O
(	O
a	O
(	O
1	O
p	O
)	O
/	O
(	O
p	O
(	O
1	O
-	O
a	O
»	O
)	O
.	O
problem	O
8.6.	O
let	O
b	O
be	O
a	O
binomial	O
(	O
n	O
,	O
p	O
)	O
random	O
variable	B
.	O
show	O
that	O
if	O
p	O
2	O
:	O
1/2	O
,	O
problems	O
and	O
exercises	O
131	O
and	O
if	O
p	O
:	O
s	O
1/2	O
,	O
(	O
okamoto	O
(	O
1958	O
)	O
)	O
.	O
hint	O
:	O
use	O
chernoff	O
's	O
method	O
,	O
and	O
the	O
inequality	B
p	O
{	O
b	O
-	O
np	O
:	O
s	O
-ne	O
}	O
<	O
e-	O
2p	O
(	O
l-p	O
)	O
n	O
<	O
2	O
1	O
-	O
x	O
(	O
x	O
_	O
p	O
)	O
2	O
x	O
log	O
-	O
+	O
(	O
1	O
-	O
x	O
)	O
log	O
-	O
-	O
>	O
-	O
-	O
-	O
-	O
1	O
-	O
p	O
-	O
2p	O
(	O
1	O
-	O
p	O
)	O
x	O
p	O
for	O
1/2	O
:	O
s	O
p	O
:	O
s	O
x	O
:	O
s	O
1	O
,	O
and	O
for	O
0	O
:	O
s	O
x	O
:	O
s	O
p	O
:	O
s	O
1/2	O
.	O
problem	O
8.7.	O
let	O
b	O
be	O
a	O
binomial	O
(	O
n	O
,	O
p	O
)	O
random	O
variable	B
.	O
prove	O
that	O
and	O
p	O
{	O
-jb	O
-	O
fp	O
2	O
:	O
ey'n	O
}	O
<	O
e-2ne2	O
,	O
p	O
{	O
-jb	O
-	O
fp	O
:	O
s	O
-ey'n	O
}	O
<	O
e-ne2	O
(	O
okamoto	O
(	O
1958	O
)	O
)	O
.	O
hint	O
:	O
use	O
chernoff	O
's	O
method	O
,	O
and	O
the	O
inequalities	O
x	O
log	O
-	O
+	O
(	O
1	O
-	O
x	O
)	O
log	O
-	O
-	O
2	O
:	O
2	O
(	O
v'x	O
-	O
y'p	O
)	O
2	O
x	O
e	O
[	O
p	O
,	O
1	O
]	O
,	O
and	O
x	O
log	O
-	O
+	O
(	O
1	O
-	O
x	O
)	O
log	O
-	O
-	O
2	O
:	O
(	O
v'x	O
-	O
y'p	O
)	O
x	O
e	O
[	O
0	O
,	O
p	O
]	O
,	O
2	O
x	O
p	O
x	O
p	O
1	O
-x	O
1-	O
p	O
1-	O
x	O
1-	O
p	O
problem	O
8.8.	O
give	O
a	O
class	O
c	O
of	O
decision	O
functions	O
of	O
the	O
form	O
¢	O
:	O
n	O
d	O
--	O
+	O
{	O
o	O
,	O
i	O
}	O
(	O
i.e.	O
,	O
the	O
training	O
data	O
do	O
not	O
play	O
any	O
role	O
in	O
the	O
decision	O
)	O
such	O
that	O
for	O
every	O
e	O
>	O
0	O
supp	O
(	O
l	O
''	O
ln	O
(	O
¢	O
)	O
-	O
l	O
(	O
¢	O
)	O
i	O
>	O
e	O
}	O
:	O
s	O
2e-	O
<	O
pec	O
2ne2	O
for	O
every	O
distribution	B
,	O
where	O
ln	O
(	O
¢	O
)	O
is	O
the	O
error-counting	O
estimate	B
of	O
the	O
error	O
probability	O
l	O
(	O
¢	O
)	O
=	O
p	O
{	O
¢	O
(	O
x	O
)	O
=i	O
y	O
}	O
of	O
decision	O
¢	O
,	O
and	O
at	O
the	O
same	O
time	O
,	O
if	O
fn	O
is	O
the	O
class	O
of	O
mappings	O
¢~	O
minimizing	O
the	O
error	O
count	O
ln	O
(	O
¢	O
)	O
over	O
the	O
class	O
c	O
,	O
then	O
there	O
exists	O
one	O
distribution	B
such	O
that	O
p	O
{	O
sup	O
l	O
(	O
¢	O
)	O
-	O
<	O
pefn	O
for	O
all	O
n.	O
inf	O
l	O
(	O
¢	O
)	O
=	O
1	O
}	O
=	O
1	O
<	O
pec	O
problem	O
8.9.	O
let	O
c	O
be	O
a	O
class	O
of	O
classifiers	O
,	O
that	O
is	O
,	O
a	O
class	O
of	O
mappings	O
of	O
the	O
form	O
¢n	O
(	O
x	O
,	O
dn	O
)	O
=	O
¢n	O
(	O
x	O
)	O
.	O
assume	O
that	O
an	O
independent	O
testing	B
sequence	I
tm	O
is	O
given	O
,	O
and	O
that	O
the	O
error	O
count	O
____	O
1	O
m	O
ln	O
,	O
m	O
(	O
¢n	O
)	O
=	O
-	O
l	O
i	O
(	O
¢n	O
(	O
xn+j	O
)	O
=	O
!	O
yi1+j	O
}	O
m	O
j=	O
!	O
is	O
used	O
to	O
estimate	B
the	O
error	O
probability	O
l	O
(	O
¢n	O
)	O
=	O
p	O
{	O
¢n	O
(	O
x	O
)	O
=i	O
yidn	O
}	O
of	O
each	O
classifier	B
¢n	O
e	O
c.	O
denote	O
by	O
¢~	O
,	O
m	O
the	O
classifier	B
that	O
minimizes	O
the	O
estimated	O
error	O
probability	O
over	O
the	O
class	O
.	O
prove	O
that	O
for	O
the	O
error	O
probability	O
132	O
8.	O
error	O
~stimation	O
of	O
the	O
selected	O
rule	B
we	O
have	O
also	O
,	O
if	O
c	O
is	O
of	O
finite	O
cardinality	O
with	O
lei	O
=	O
n	O
,	O
then	O
problem	O
8.10.	O
show	O
that	O
if	O
a	O
rule	O
gn	O
is	O
consistent	O
,	O
then	O
we	O
can	O
always	O
find	O
an	O
estimate	B
of	O
the	O
error	O
such	O
that	O
e	O
{	O
lin	O
-	O
ln	O
iq	O
}	O
-+	O
0	O
for	O
all	O
q	O
>	O
0.	O
hint	O
:	O
split	O
the	O
data	O
sequence	O
dn	O
and	O
use	O
the	O
second	O
half	O
to	O
estimate	B
the	O
error	O
probability	O
of	O
grn/21	O
'	O
problem	O
8.11.	O
open-ended	O
problem	O
.	O
is	O
there	O
a	O
rule	O
for	O
which	O
no	O
error	O
estimate	O
works	O
for	O
all	O
distributions	O
?	O
more	O
specifically	O
,	O
is	O
there	O
a	O
sequence	O
of	O
classification	O
rules	O
gn	O
such	O
that	O
for	O
all	O
n	O
large	O
enough	O
,	O
infsupe	O
{	O
(	O
in	O
-	O
lni	O
}	O
:	O
:	O
:	O
:	O
c	O
in	O
x	O
,	O
y	O
for	O
some	O
constant	O
c	O
>	O
0	O
,	O
where	O
the	O
infimum	O
is	O
taken	O
over	O
all	O
possible	O
error	O
estimates	O
?	O
are	O
such	O
rules	O
necessarily	O
inconsistent	O
?	O
problem	O
8.12.	O
consider	O
the	O
problem	O
of	O
estimating	O
the	O
asymptotic	O
probability	O
of	O
error	O
of	O
the	O
nearest	B
neighbor	I
rule	I
lnn	O
=	O
2e	O
{	O
1	O
]	O
(	O
x	O
)	O
(	O
1	O
-	O
1	O
]	O
(	O
x	O
»	O
}	O
.	O
show	O
that	O
for	O
every	O
n	O
,	O
for	O
any	O
estimate	B
ln	O
of	O
l	O
nn	O
,	O
and	O
for	O
every	O
e	O
>	O
0	O
,	O
there	O
exists	O
a	O
distribution	O
of	O
ex	O
,	O
y	O
)	O
,	O
such	O
that	O
9	O
the	O
regular	B
histogram	O
rule	B
in	O
this	O
chapter	O
we	O
study	O
the	O
cubic	B
histogram	O
rule	B
.	O
recall	O
that	O
this	O
rule	B
partitions	O
rd	O
into	O
cubes	O
of	O
the	O
same	O
size	O
,	O
and	O
gives	O
the	O
decision	O
according	O
to	O
the	O
number	O
of	O
zeros	O
and	O
ones	O
among	O
the	O
yi	O
's	O
such	O
that	O
the	O
corresponding	O
xi	O
falls	O
in	O
the	O
same	O
cube	O
as	O
x.	O
pn	O
=	O
{	O
ani	O
,	O
a	O
n2	O
,	O
...	O
}	O
denotes	O
a	O
partition	O
of	O
rd	O
into	O
cubes	O
of	O
size	O
h	O
n	O
>	O
0	O
,	O
that	O
is	O
,	O
into	O
sets	O
of	O
the	O
type	O
n1=1	O
[	O
kihn	O
,	O
(	O
ki	O
+	O
l	O
)	O
hn	O
)	O
,	O
where	O
the	O
ki	O
's	O
are	O
integers	O
,	O
and	O
the	O
histogram	B
rule	I
is	O
defined	O
by	O
where	O
for	O
every	O
x	O
e	O
rd	O
,	O
an	O
(	O
x	O
)	O
=	O
ani	O
if	O
x	O
e	O
ani	O
.	O
that	O
is	O
,	O
the	O
decision	O
is	O
zero	O
if	O
the	O
number	O
of	O
ones	O
does	O
not	O
exceed	O
the	O
number	O
of	O
zeros	O
in	O
the	O
cell	O
in	O
which	O
x	O
falls	O
.	O
weak	B
universal	O
consistency	B
of	O
this	O
rule	B
was	O
shown	O
in	O
chapter	O
6	O
under	O
the	O
conditions	O
hn	O
--	O
+	O
0	O
and	O
nh~	O
--	O
+	O
00	O
as	O
n	O
--	O
+	O
00.	O
the	O
purpose	O
of	O
this	O
chapter	O
is	O
to	O
introduce	O
some	O
techniques	O
by	O
proving	O
strong	B
universal	I
consistency	I
of	O
this	O
rule	B
.	O
these	O
techniques	O
will	O
prove	O
very	O
useful	O
in	O
handling	O
other	O
problems	O
as	O
well	O
.	O
first	O
we	O
introduce	O
the	O
method	B
of	I
bounded	I
differences	I
.	O
9.1	O
the	O
method	B
of	I
bounded	I
differences	I
in	O
this	O
section	O
we	O
present	O
a	O
generalization	O
of	O
hoeffding	O
's	O
inequality	B
,	O
due	O
to	O
mcdiarmid	O
(	O
1989	O
)	O
.	O
the	O
result	O
will	O
equip	O
us	O
with	O
a	O
powerful	O
tool	O
to	O
handle	O
com	O
(	O
cid:173	O
)	O
plicated	O
functions	O
of	O
independent	O
random	O
variables	O
.	O
this	O
inequality	B
follows	O
by	O
results	O
of	O
hoeffding	O
(	O
1963	O
)	O
and	O
azuma	O
(	O
1967	O
)	O
who	O
observed	O
that	O
theorem	B
8.1	O
134	O
9.	O
the	O
regular	B
histogram	O
rule	B
can	O
be	O
generalized	B
to	O
bounded	O
martingale	O
difference	O
sequences	O
.	O
the	O
inequality	B
has	O
found	O
many	O
applications	O
in	O
combinatorics	O
,	O
as	O
well	O
as	O
in	O
nonparametric	O
statistics	B
(	O
see	O
mcdiarmid	O
(	O
1989	O
)	O
and	O
devroye	O
(	O
1991a	O
)	O
for	O
surveys	O
)	O
.	O
let	O
us	O
first	O
recall	O
the	O
notion	O
of	O
martingales	O
.	O
consider	O
a	O
probability	O
space	O
(	O
q	O
,	O
f	O
,	O
p	O
)	O
.	O
definition	O
9.1.	O
a	O
sequence	O
of	O
random	O
variables	O
z	O
1	O
,	O
z2	O
,	O
...	O
is	O
called	O
a	O
martingale	O
if	O
e	O
{	O
zi+i1zi	O
,	O
...	O
,	O
zd	O
=	O
zi	O
with	O
probability	O
one	O
for	O
each	O
i	O
>	O
0.	O
let	O
xl	O
,	O
x	O
2	O
,	O
...	O
be	O
an	O
arbitrary	O
sequence	O
of	O
random	O
variables	O
.	O
zl	O
,	O
z2	O
,	O
...	O
is	O
called	O
a	O
martingale	O
with	O
respect	O
to	O
the	O
sequence	O
xl	O
,	O
x	O
2	O
,	O
...	O
iffor	O
every	O
i	O
>	O
0	O
,	O
zi	O
is	O
afunction	O
of	O
xl	O
,	O
...	O
,	O
xi	O
,	O
and	O
e	O
{	O
zi+iixi	O
,	O
''	O
''	O
xd	O
=	O
zi	O
with	O
probability	O
one	O
.	O
obviously	O
,	O
if	O
zl	O
,	O
z2	O
,	O
...	O
is	O
a	O
martingale	O
with	O
respect	O
to	O
xl	O
,	O
x2	O
,	O
...	O
,	O
then	O
zl	O
,	O
z2	O
,	O
...	O
is	O
a	O
martingale	O
,	O
since	O
e	O
{	O
zi+liz1	O
,	O
...	O
,	O
zd	O
=	O
e	O
{	O
e	O
{	O
zi+lixl	O
'	O
...	O
,	O
xd	O
zi	O
,	O
''	O
''	O
zd	O
=	O
e	O
{	O
ziizl	O
,	O
...	O
,	O
zi	O
}	O
the	O
most	O
important	O
examples	O
of	O
martingales	O
are	O
sums	O
of	O
independent	O
zero	O
(	O
cid:173	O
)	O
mean	O
random	O
variables	O
.	O
let	O
u1	O
,	O
u2	O
,	O
...	O
be	O
independent	O
random	O
variables	O
with	O
zero	O
mean	O
.	O
then	O
the	O
random	O
variables	O
i	O
si	O
=	O
l	O
uj	O
,	O
j=l	O
i	O
>	O
0	O
,	O
form	O
a	O
martingale	O
(	O
see	O
problem	O
9.1	O
)	O
.	O
martingales	O
share	O
many	O
properties	O
of	O
sums	O
of	O
independent	O
variables	O
.	O
our	O
purpose	O
here	O
is	O
to	O
extend	O
hoeffding	O
's	O
inequality	B
to	O
martingales	O
.	O
the	O
role	O
of	O
the	O
independent	O
random	O
variables	O
is	O
played	O
here	O
by	O
a	O
so-called	O
martingale	B
difference	I
sequence	I
.	O
definition	O
9.2.	O
a	O
sequence	O
of	O
random	O
variables	O
vi	O
,	O
v2	O
,	O
.•	O
.	O
is	O
a	O
martingale	O
dif	O
(	O
cid:173	O
)	O
ference	O
sequence	O
if	O
e	O
{	O
vi+ii	O
vi	O
,	O
...	O
,	O
vi	O
}	O
=	O
°	O
with	O
probability	O
one	O
for	O
every	O
i	O
>	O
0.	O
a	O
sequence	O
of	O
random	O
variables	O
vi	O
,	O
v2	O
,	O
.	O
•	O
.	O
is	O
called	O
a	O
martingale	O
difference	O
sequence	O
with	O
respect	O
to	O
a	O
sequence	O
of	O
random	O
variables	O
xl	O
,	O
x	O
2	O
,	O
...	O
iffor	O
every	O
i	O
>	O
°	O
vi	O
is	O
a	O
function	O
of	O
xl	O
,	O
...	O
,	O
xi	O
,	O
and	O
e	O
{	O
vi+i	O
ix	O
1	O
,	O
•••	O
,	O
xd	O
=	O
°	O
with	O
probability	O
one	O
.	O
9.1	O
the	O
method	B
of	I
bounded	I
differences	I
135	O
again	O
,	O
it	O
is	O
easily	O
seen	O
that	O
if	O
vi	O
,	O
v2	O
,	O
...	O
is	O
a	O
martingale	O
difference	O
sequence	O
with	O
respect	O
to	O
a	O
sequence	O
xl	O
,	O
x	O
2	O
,	O
••	O
.	O
of	O
random	O
variables	O
,	O
then	O
it	O
is	O
a	O
martingale	O
dif	O
(	O
cid:173	O
)	O
ference	O
sequence	O
.	O
also	O
,	O
any	O
martingale	B
zl	O
,	O
z2	O
,	O
...	O
leads	O
naturally	O
to	O
a	O
martingale	O
difference	O
sequence	O
by	O
defining	O
for	O
i	O
>	O
o.	O
the	O
key	O
result	O
in	O
the	O
method	B
of	I
bounded	I
differences	I
is	O
the	O
following	O
inequality	B
that	O
relaxes	O
the	O
independence	O
assumption	O
in	O
theorem	O
8.1	O
,	O
allowing	O
martingale	O
difference	O
sequences	O
:	O
theorem	B
9.1	O
.	O
(	O
hoeffding	O
(	O
1963	O
)	O
,	O
azuma	O
(	O
1967	O
»	O
.	O
let	O
xl	O
,	O
x	O
2	O
,	O
.••	O
be	O
a	O
sequence	O
of	O
random	O
variables	O
,	O
and	O
assume	O
that	O
vi	O
,	O
v2	O
,	O
.	O
•	O
.	O
is	O
a	O
martingale	O
difference	O
se	O
(	O
cid:173	O
)	O
quence	O
with	O
respect	O
to	O
x	O
i	O
,	O
x	O
2	O
,	O
...	O
.	O
assume	O
furthermore	O
that	O
there	O
exist	O
random	O
variables	O
z	O
1	O
,	O
z2	O
,	O
...	O
and	O
nonnegative	O
constants	O
c1	O
,	O
c2	O
,	O
...	O
such	O
that	O
for	O
every	O
i	O
>	O
0	O
zi	O
is	O
afunction	O
of	O
xl	O
,	O
...	O
,	O
xi-i	O
,	O
and	O
then	O
for	O
any	O
e	O
>	O
0	O
and	O
n	O
and	O
the	O
proof	O
is	O
a	O
rather	O
straightforward	O
extension	O
of	O
that	O
of	O
hoeffding	O
'	O
s	O
inequality	B
.	O
first	O
we	O
need	O
an	O
analog	O
of	O
lemma	O
8.1	O
:	O
lemma	O
9.1.	O
assume	O
that	O
the	O
random	O
variables	O
v	O
and	O
z	O
satisfy	O
with	O
probability	O
one	O
that	O
e	O
{	O
v	O
i	O
z	O
}	O
=	O
0	O
,	O
and	O
for	O
some	O
function	O
f	O
and	O
constant	O
c	O
:	O
:	O
:	O
:	O
0	O
fez	O
)	O
:	O
:s	O
v	O
:	O
:s	O
fez	O
)	O
+	O
c.	O
then	O
for	O
every	O
s	O
>	O
0	O
the	O
proof	O
of	O
the	O
lemma	O
is	O
left	O
as	O
an	O
exercise	O
(	O
problem	O
9.2	O
)	O
.	O
proof	O
of	O
theorem	O
9.1.	O
as	O
in	O
the	O
proof	O
of	O
hoeffding	O
'	O
s	O
inequality	B
,	O
we	O
proceed	O
by	O
chernoff	O
's	O
bounding	O
method	O
.	O
set	O
sk	O
=	O
i	O
:	O
:=1	O
~	O
.	O
then	O
for	O
any	O
s	O
>	O
0	O
p	O
{	O
s	O
>	O
n_e	O
}	O
<	O
e-see	O
{	O
e	O
ssn	O
}	O
136	O
9.	O
the	O
rygular	O
histogram	B
rule	I
<	O
=	O
e-se	O
es2	O
l:7=1	O
ct	O
/8	O
-2	O
2/	O
``	O
n	O
e	O
e	O
l-i=l	O
ci	O
2	O
(	O
iterate	O
previous	O
argument	O
)	O
2	O
(	O
choose	O
s	O
=	O
4e/	O
'\	O
'	O
.	O
c.	O
)	O
lz=l	O
z	O
'	O
n	O
the	O
second	O
inequality	B
is	O
proved	O
analogously	O
.	O
0	O
now	O
,	O
we	O
are	O
ready	O
to	O
state	O
the	O
main	O
inequality	B
of	O
this	O
section	O
.	O
it	O
is	O
a	O
large	O
deviation-type	O
inequality	B
for	O
functions	O
of	O
independent	O
random	O
variables	O
such	O
that	O
the	O
function	O
is	O
relatively	O
robust	O
to	O
individual	O
changes	O
in	O
the	O
values	O
of	O
the	O
random	O
variables	O
.	O
the	O
condition	O
of	O
the	O
function	O
requires	O
that	O
by	O
changing	O
the	O
value	O
of	O
its	O
i	O
-th	O
variable	B
,	O
the	O
value	O
of	O
the	O
function	O
can	O
not	O
change	O
by	O
more	O
than	O
a	O
constant	O
ci	O
.	O
theorem	B
9.2	O
.	O
(	O
mcdiarmid	O
(	O
1989	O
»	O
.	O
let	O
xl	O
,	O
''	O
''	O
xn	O
be	O
independent	O
random	O
va	O
(	O
cid:173	O
)	O
riables	O
taking	O
values	O
in	O
a	O
set	O
a	O
,	O
and	O
assume	O
that	O
f	O
:	O
an	O
-+	O
r	O
satisfies	O
sup	O
if	O
(	O
xl	O
,	O
...	O
,	O
xn	O
)	O
-	O
f	O
(	O
xi	O
,	O
.	O
,	O
.	O
,	O
xi-i	O
,	O
x	O
;	O
,	O
xi+1	O
,	O
...	O
,	O
xn	O
)	O
1	O
:	O
s	O
ci	O
,	O
1	O
:	O
s	O
i	O
:	O
s	O
n	O
.	O
xl	O
,	O
·	O
..	O
,	O
xn	O
,	O
x	O
;	O
ea	O
then	O
for	O
all	O
e	O
>	O
0	O
p	O
{	O
f	O
(	O
x1	O
,	O
.•	O
.	O
,	O
xn	O
)	O
-	O
ef	O
(	O
x	O
1	O
,	O
.•	O
.	O
,	O
xn	O
)	O
:	O
:	O
:	O
:	O
e	O
}	O
:	O
s	O
e-	O
2e2/l:7=lcr	O
,	O
and	O
p	O
{	O
ef	O
(	O
x	O
1	O
,	O
...	O
,	O
x	O
)	O
-	O
n	O
f	O
(	O
x	O
1	O
,	O
...	O
,	O
x	O
)	O
>	O
n	O
_	O
e	O
_	O
e	O
}	O
<	O
-2e	O
2	O
/	O
l:7=1	O
cf	O
.	O
proof	O
.	O
define	O
v	O
=	O
f	O
(	O
x	O
1	O
,	O
...	O
,	O
xn	O
)	O
-ef	O
(	O
xi	O
,	O
...	O
,	O
xn	O
)	O
.	O
introduce	O
vi	O
=	O
e	O
{	O
vixd	O
(	O
cid:173	O
)	O
ev	O
,	O
and	O
for	O
k	O
>	O
1	O
,	O
so	O
that	O
v	O
=	O
l~=l	O
vk	O
.	O
clearly	O
,	O
vi	O
,	O
...	O
,	O
vn	O
form	O
a	O
martingale	O
difference	O
sequence	O
with	O
respect	O
to	O
xl	O
,	O
...	O
,	O
x	O
n	O
•	O
define	O
the	O
random	O
variables	O
and	O
vk	O
=	O
hk	O
(	O
x	O
i	O
,	O
...	O
,	O
x	O
k	O
)	O
-	O
f	O
hk	O
(	O
x	O
1	O
,	O
.·.	O
,	O
x	O
k-	O
i	O
,	O
x	O
)	O
fk	O
(	O
dx	O
)	O
,	O
where	O
the	O
integration	O
is	O
with	O
respect	O
to	O
fk	O
,	O
the	O
probability	O
measure	B
of	O
x	O
k.	O
introduce	O
the	O
random	O
variables	O
wk	O
~	O
s~p	O
(	O
hk	O
(	O
x	O
i	O
,	O
•..	O
,	O
xk-i	O
,	O
u	O
)	O
-	O
f	O
hk	O
(	O
x	O
1	O
,	O
...	O
,	O
xk-j	O
,	O
x	O
)	O
fk	O
(	O
dx	O
»	O
)	O
,	O
and	O
9.1	O
the	O
method	B
of	I
bounded	I
differences	I
137	O
clearly	O
,	O
zk	O
:	O
:s	O
vk	O
:	O
:s	O
wk	O
with	O
probability	O
one	O
.	O
since	O
for	O
every	O
k	O
zk	O
is	O
a	O
function	O
of	O
xl	O
,	O
...	O
,	O
xk-l	O
,	O
we	O
can	O
apply	O
theorem	B
9.1	O
directly	O
to	O
v	O
=	O
l	O
:	O
~=1	O
vk	O
,	O
if	O
we	O
can	O
show	O
that	O
wk	O
-	O
zk	O
:	O
:s	O
ck	O
.	O
but	O
this	O
follows	O
from	O
wk	O
-	O
zk	O
=	O
sup	O
sup	O
(	O
hk	O
(	O
xi	O
,	O
...	O
,	O
xk-i	O
,	O
u	O
)	O
-	O
hk	O
(	O
x	O
i	O
,	O
.··	O
,	O
xk-l	O
,	O
v	O
)	O
)	O
u	O
by	O
the	O
condition	O
of	O
the	O
theorem	B
.	O
0	O
clearly	O
,	O
if	O
the	O
xi	O
's	O
are	O
bounded	O
,	O
then	O
the	O
choice	O
i	O
(	O
xl	O
,	O
...	O
,	O
xn	O
)	O
=	O
l:7=1	O
xi	O
yields	O
hoeffding	O
's	O
inequality	B
.	O
many	O
times	O
the	O
inequality	B
can	O
be	O
used	O
to	O
handle	O
very	O
complicated	O
functions	O
of	O
independent	O
random	O
variables	O
with	O
great	O
elegance	O
.	O
for	O
examples	O
in	O
nonparametric	O
statistics	B
,	O
see	O
problems	O
9.3	O
,	O
9.6	O
,	O
10.3.	O
similar	O
methods	O
to	O
those	O
used	O
in	O
the	O
proof	O
of	O
theorem	O
9.2	O
may	O
be	O
used	O
to	O
bound	O
the	O
variance	O
var	O
{	O
f	O
(	O
x1	O
,	O
...	O
,	O
xn	O
)	O
}	O
.	O
other	O
inequalities	O
for	O
the	O
variance	B
of	I
general	O
functions	O
of	O
independent	O
random	O
variables	O
were	O
derived	O
by	O
efron	O
and	O
stein	O
(	O
1981	O
)	O
and	O
steele	O
(	O
1986	O
)	O
.	O
theorem	B
9.3	O
.	O
(	O
devroye	O
(	O
1991a	O
)	O
)	O
.	O
assume	O
that	O
the	O
conditions	O
of	O
theorem	O
9.2	O
hold	O
.	O
then	O
proof	O
.	O
using	O
the	O
notations	O
of	O
the	O
proof	O
of	O
theorem	O
9.2	O
,	O
we	O
have	O
to	O
show	O
that	O
var	O
{	O
v	O
}	O
:	O
:s	O
-	O
z=	O
cf-	O
1	O
n	O
4	O
i=l	O
observe	O
that	O
var	O
{	O
v	O
}	O
=	O
ev2	O
=	O
e	O
{	O
tv	O
?	O
}	O
'	O
1=1	O
where	O
in	O
the	O
last	O
step	O
we	O
used	O
the	O
martingale	B
property	O
in	O
the	O
following	O
way	O
:	O
for	O
i	O
<	O
j	O
we	O
have	O
=	O
0	O
with	O
probability	O
one	O
.	O
138	O
9.	O
the	O
re	O
(	O
gular	O
histogram	B
rule	I
thus	O
,	O
the	O
theorem	B
follows	O
if	O
we	O
can	O
show	O
that	O
introducing	O
wi	O
and	O
zi	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
9.2	O
,	O
we	O
see	O
that	O
with	O
proba	O
(	O
cid:173	O
)	O
bility	O
one	O
zi	O
s	O
vi	O
s	O
zi	O
+	O
ci	O
.	O
since	O
zi	O
is	O
a	O
function	O
of	O
xl	O
,	O
...	O
,	O
xi	O
-1	O
,	O
therefore	O
,	O
conditioned	O
on	O
xl	O
,	O
...	O
,	O
xi-i	O
,	O
~	O
is	O
a	O
zero	O
mean	O
random	O
variable	B
taking	O
values	O
in	O
the	O
interval	O
[	O
zi	O
,	O
zi	O
+	O
cd	O
.	O
but	O
an	O
arbitrary	O
random	O
variable	B
u	O
taking	O
values	O
in	O
an	O
interval	O
[	O
a	O
,	O
b	O
]	O
has	O
variance	O
not	O
exceeding	O
so	O
that	O
cf	O
e	O
{	O
vi	O
ixi	O
,	O
·	O
..	O
,	O
xi-d	O
s	O
4	O
'	O
2	O
which	O
concludes	O
the	O
proof	O
.	O
0	O
9.2	O
strong	B
universal	I
consistency	I
the	O
purpose	O
of	O
this	O
section	O
is	O
to	O
prove	O
strong	B
universal	I
consistency	I
of	O
the	O
histogram	B
rule	I
.	O
this	O
is	O
the	O
first	O
such	O
result	O
that	O
we	O
mention	O
.	O
later	O
we	O
will	O
prove	O
the	O
same	O
property	O
for	O
other	O
rules	O
,	O
too	O
.	O
the	O
theorem	B
,	O
stated	O
here	O
for	O
cubic	B
partitions	O
,	O
is	O
essentially	O
due	O
to	O
devroye	O
and	O
gyorfi	O
(	O
1983	O
)	O
.	O
for	O
more	O
general	O
sequences	O
of	O
partitions	O
,	O
see	O
problem	O
9.7.	O
an	O
alternative	O
proof	O
of	O
the	O
theorem	B
based	O
on	O
the	O
vapnik-chervonenkis	O
inequality	B
will	O
be	O
given	O
later-see	O
the	O
remark	O
following	O
theorem	B
17.2.	O
theorem	B
9.4.	O
assume	O
that	O
the	O
sequence	O
of	O
partitions	O
{	O
pn	O
}	O
satisfies	O
the	O
following	O
two	O
conditions	O
as	O
n	O
--	O
-+	O
00	O
:	O
and	O
nh~	O
--	O
-+	O
00.	O
for	O
any	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
andfor	O
every	O
e	O
>	O
0	O
there	O
is	O
an	O
integer	O
no	O
such	O
that	O
for	O
n	O
>	O
no	O
,	O
for	O
the	O
error	O
probability	O
ln	O
of	O
the	O
histogram	B
rule	I
p	O
{	O
ln	O
-	O
l	O
*	O
>	O
e	O
}	O
s	O
2e-ne2	O
/32	O
.	O
thus	O
,	O
the	O
cubic	B
histogram	O
rule	B
is	O
strongly	O
universally	O
consistent	O
.	O
proof	O
.	O
define	O
,	O
,~	O
y	O
.	O
]	O
*	O
(	O
)	O
lc=l	O
c	O
{	O
xiean	O
(	O
x	O
)	O
}	O
ry	O
x	O
=	O
n	O
nfl	O
(	O
an	O
(	O
x	O
»	O
.	O
clearly	O
,	O
the	O
decision	O
based	O
on	O
1j~	O
,	O
9.2	O
strong	B
universal	I
consistency	I
139	O
is	O
just	O
the	O
histogram	B
rule	I
.	O
therefore	O
,	O
by	O
theorem	B
2.3	O
,	O
it	O
suffices	O
to	O
prove	O
that	O
for	O
n	O
large	O
enough	O
,	O
p	O
{	O
f	O
iry	O
(	O
x	O
)	O
-	O
ry	O
:	O
(	O
x	O
)	O
ijl	O
(	O
dx	O
)	O
>	O
~	O
}	O
:	O
s	O
e-n	O
''	O
132	O
,	O
decompose	O
the	O
difference	O
as	O
11j	O
(	O
x	O
)	O
-1j~	O
(	O
x	O
)	O
1	O
=	O
ei1j	O
(	O
x	O
)	O
-	O
1j~	O
(	O
x	O
)	O
1	O
+	O
(	O
11j	O
(	O
x	O
)	O
-	O
1j~	O
(	O
x	O
)	O
1	O
-	O
ei1j	O
(	O
x	O
)	O
-	O
1j~	O
(	O
x	O
)	O
i	O
)	O
.	O
(	O
9.1	O
)	O
the	O
convergence	O
of	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
implies	O
weak	B
consistency	O
of	O
the	O
histogram	B
rule	I
.	O
the	O
technique	O
we	O
use	O
to	O
bound	O
this	O
term	O
is	O
similar	O
to	O
that	O
which	O
we	O
already	O
saw	O
in	O
the	O
proof	O
of	O
theorem	O
6.3.	O
for	O
completeness	O
,	O
we	O
give	O
the	O
details	O
here	O
.	O
however	O
,	O
new	O
ideas	O
have	O
to	O
appear	O
in	O
our	O
handling	O
of	O
the	O
second	O
term	O
.	O
we	O
begin	O
with	O
the	O
first	O
term	O
.	O
since	O
the	O
set	O
of	O
continuous	O
functions	O
with	O
bounded	O
support	B
is	O
dense	O
in	O
l	O
1	O
(	O
f.l	O
)	O
,	O
it	O
is	O
possible	O
to	O
find	O
a	O
continuous	O
function	O
of	O
bounded	O
support	B
r	O
(	O
x	O
)	O
such	O
that	O
f	O
11j	O
(	O
x	O
)	O
-	O
r	O
(	O
x	O
)	O
if.l	O
(	O
dx	O
)	O
<	O
e/16	O
.	O
note	O
that	O
r	O
(	O
x	O
)	O
is	O
uniformly	O
continuous	O
.	O
introduce	O
the	O
function	O
r*	O
(	O
x	O
)	O
=	O
e	O
{	O
r	O
(	O
x	O
)	O
i	O
{	O
xean	O
(	O
x	O
)	O
}	O
}	O
.	O
n	O
f.l	O
(	O
an	O
(	O
x	O
»	O
then	O
we	O
can	O
further	O
decompose	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
9.1	O
)	O
as	O
ei1j	O
(	O
x	O
)	O
-	O
1j~	O
(	O
x	O
)	O
1	O
<	O
r	O
(	O
x	O
)	O
1	O
+	O
ir	O
(	O
x	O
)	O
-	O
r	O
,	O
;	O
(	O
x	O
)	O
i	O
11j	O
(	O
x	O
)	O
-	O
+	O
ir	O
;	O
(	O
x	O
)	O
-	O
e1j~	O
(	O
x	O
)	O
1	O
+	O
eie1j~	O
(	O
x	O
)	O
-	O
1j~	O
(	O
x	O
)	O
l.	O
(	O
9.2	O
)	O
we	O
proceed	O
term	O
by	O
term	O
:	O
first	O
term	O
:	O
the	O
integral	O
of	O
11j	O
(	O
x	O
)	O
-	O
by	O
the	O
definition	B
of	I
r	O
(	O
x	O
)	O
.	O
r	O
(	O
x	O
)	O
1	O
(	O
with	O
respect	O
to	O
f.l	O
)	O
is	O
smaller	O
than	O
e/16	O
second	O
term	O
:	O
using	O
fubini	O
's	O
theorem	B
we	O
have	O
[	O
ir	O
(	O
x	O
)	O
-	O
e	O
{	O
r	O
(	O
x	O
)	O
~	O
{	O
xeajd	O
i	O
f.l	O
(	O
dx	O
)	O
f.l	O
(	O
j	O
)	O
r	O
;	O
(	O
x	O
)	O
if.l	O
(	O
dx	O
)	O
f	O
ir	O
(	O
x	O
)	O
-	O
=	O
l	O
ahi	O
(	O
aj	O
)	O
¥o	O
j	O
aj	O
140	O
9.	O
the	O
regular	B
histogram	O
rule	B
as	O
rex	O
)	O
is	O
uniformly	O
continuous	O
,	O
if	O
hn	O
is	O
small	O
enough	O
,	O
then	O
!	O
r	O
(	O
x	O
)	O
-	O
r	O
(	O
y	O
)	O
!	O
<	O
e/16	O
for	O
every	O
x	O
,	O
yea	O
for	O
any	O
cell	O
a	O
e	O
pn	O
.	O
then	O
the	O
double	O
integral	O
in	O
the	O
above	O
expression	B
can	O
be	O
bounded	O
from	O
above	O
as	O
follows	O
:	O
l.	O
l	O
ir	O
(	O
x	O
)	O
-	O
}	O
}	O
r	O
(	O
y	O
)	O
lll	O
(	O
dx	O
)	O
ll	O
(	O
dy	O
)	O
:	O
:	O
:	O
eil2	O
(	O
a	O
j	O
)	O
/16	O
.	O
note	O
that	O
we	O
used	O
the	O
condition	O
hn	O
--	O
--	O
-+	O
0	O
here	O
.	O
summing	O
over	O
the	O
cells	O
we	O
get	O
f	O
ir	O
(	O
x	O
)	O
-	O
rz	O
(	O
x	O
)	O
lll	O
(	O
dx	O
)	O
:	O
:	O
:	O
e/16	O
.	O
third	O
term	O
:	O
we	O
have	O
f	O
!	O
rz	O
(	O
x	O
)	O
-	O
e1	O
]	O
~	O
(	O
x	O
)	O
lll	O
(	O
dx	O
)	O
=	O
l	O
ie	O
{	O
r	O
(	O
x	O
)	O
i	O
{	O
xea	O
j	O
}	O
-	O
y	O
i	O
{	O
xea	O
j	O
}	O
}	O
i	O
ajepn	O
a~	O
''	O
ili	O
r	O
(	O
x	O
)	O
j1jdx	O
)	O
-	O
li	O
1	O
)	O
(	O
x	O
)	O
/l	O
(	O
dx	O
)	O
i	O
:	O
:	O
:	O
f	O
ir	O
(	O
x	O
)	O
-	O
1	O
]	O
(	O
x	O
)	O
lll	O
(	O
dx	O
)	O
<	O
e/16	O
.	O
fourth	O
term	O
:	O
our	O
aim	O
is	O
to	O
show	O
that	O
for	O
n	O
large	O
enough	O
,	O
e	O
f	O
ie1	O
]	O
~	O
(	O
x	O
)	O
-	O
1	O
]	O
~	O
(	O
x	O
)	O
lll	O
(	O
dx	O
)	O
<	O
e/16	O
.	O
to	O
this	O
end	O
,	O
let	O
s	O
be	O
an	O
arbitrary	O
large	O
ball	O
centered	O
at	O
the	O
origin	O
.	O
denote	O
by	O
mn	O
the	O
number	O
of	O
cells	O
of	O
the	O
partition	B
pn	O
that	O
intersect	O
s.	O
clearly	O
,	O
mn	O
is	O
proportional	O
to	O
1/	O
h~	O
as	O
hn	O
--	O
--	O
-+	O
o.	O
using	O
the	O
notation	O
vn	O
(	O
a	O
)	O
=	O
~	O
l7==1	O
i	O
{	O
yi=l	O
,	O
x	O
;	O
ea	O
}	O
,	O
it	O
is	O
clear	O
that	O
vn	O
(	O
a	O
)	O
=	O
fa	O
1	O
]	O
~	O
(	O
x	O
)	O
ll	O
(	O
dx	O
)	O
.	O
now	O
,	O
we	O
can	O
write	O
e	O
f	O
ie1	O
]	O
~	O
(	O
x	O
)	O
-	O
1	O
]	O
~	O
(	O
x	O
)	O
lll	O
(	O
dx	O
)	O
e	O
~	O
l	O
,	O
ie1	O
)	O
z	O
(	O
x	O
)	O
-	O
1	O
)	O
;	O
(	O
x	O
)	O
i/l	O
(	O
dx	O
)	O
=	O
e	O
l	O
ievn	O
(	O
an	O
,	O
j	O
)	O
-	O
vn	O
(	O
an	O
,	O
j	O
)	O
1	O
j	O
9.2	O
strong	B
universal	I
consistency	I
141	O
<	O
e	O
l	O
ievn	O
(	O
an	O
,	O
j	O
)	O
-	O
vn	O
(	O
an	O
,	O
j	O
)	O
1	O
+	O
2tl	O
(	O
sc	O
)	O
j	O
:	O
a	O
n	O
,	O
jns-	O
:	O
j0	O
(	O
where	O
sc	O
denotes	O
the	O
complement	O
of	O
s	O
)	O
<	O
l	O
je	O
ievn	O
(	O
an	O
,	O
j	O
)	O
-	O
vn	O
(	O
an	O
,	O
j	O
)	O
1	O
j	O
:	O
a	O
n	O
,	O
jns-	O
:	O
j0	O
2	O
+	O
2tl	O
(	O
sc	O
)	O
(	O
by	O
the	O
cauchy-schwarz	O
inequality	B
)	O
<	O
mn	O
_1	O
l	O
j	O
/l	O
(	O
an	O
,	O
j	O
)	O
+	O
2/l	O
(	O
s	O
'	O
)	O
mn	O
j	O
:	O
a	O
n	O
,	O
jns-	O
:	O
j0	O
<	O
mn	O
..1..	O
l	O
''	O
mn	O
j.a	O
n	O
,	O
jns-	O
:	O
j0	O
n	O
n	O
tl	O
(	O
an	O
.	O
)	O
,	O
j	O
+	O
2tl	O
(	O
sc	O
)	O
(	O
by	O
jensen	O
's	O
inequality	B
)	O
<	O
/	O
'	O
!	O
f-	O
+	O
2/l	O
(	O
s	O
'	O
)	O
<	O
e/l6	O
if	O
n	O
and	O
the	O
radius	O
of	O
s	O
are	O
large	O
enough	O
,	O
since	O
mn/n	O
converges	O
to	O
zero	O
by	O
the	O
condition	O
nh~	O
-+	O
00	O
,	O
and	O
tl	O
(	O
sc	O
)	O
can	O
be	O
made	O
arbitrarily	O
small	O
by	O
choice	O
of	O
s.	O
we	O
have	O
proved	O
for	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
9.1	O
)	O
that	O
for	O
n	O
large	O
enough	O
,	O
e	O
f	O
11	O
]	O
(	O
x	O
)	O
-	O
1	O
]	O
~	O
(	O
x	O
)	O
ltl	O
(	O
dx	O
)	O
<	O
e/4	O
.	O
finally	O
,	O
we	O
handle	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
9.1	O
)	O
by	O
obtaining	O
an	O
exponential	B
bound	O
for	O
f	O
11	O
]	O
(	O
x	O
)	O
-	O
1	O
]	O
~	O
(	O
x	O
)	O
ltl	O
(	O
dx	O
)	O
-	O
e	O
f	O
11	O
]	O
(	O
x	O
)	O
-	O
1	O
]	O
~	O
(	O
x	O
)	O
ltl	O
(	O
dx	O
)	O
using	O
theorem	B
9.2.	O
fix	O
the	O
training	O
data	O
(	O
xl	O
,	O
yl	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
e	O
rd	O
x	O
{	O
a	O
,	O
1	O
}	O
,	O
and	O
replace	O
(	O
xi	O
,	O
yi	O
)	O
by	O
(	O
x	O
;	O
,	O
yi	O
)	O
changing	O
the	O
value	O
of	O
1	O
]	O
~	O
(	O
x	O
)	O
to	O
1	O
]	O
~i	O
(	O
x	O
)	O
,	O
then	O
1j~	O
(	O
x	O
)	O
-1j~i	O
(	O
x	O
)	O
differs	O
from	O
zero	O
only	O
on	O
an	O
(	O
xi	O
)	O
and	O
an	O
(	O
x	O
;	O
)	O
,	O
and	O
thus	O
f	O
11	O
]	O
(	O
x	O
)	O
-	O
1	O
]	O
~	O
(	O
x	O
)	O
ltl	O
(	O
dx	O
)	O
-	O
f	O
11	O
]	O
(	O
x	O
)	O
-	O
1	O
]	O
~i	O
(	O
x	O
)	O
ltl	O
(	O
dx	O
)	O
<	O
f	O
11	O
]	O
~	O
(	O
x	O
)	O
-	O
l	O
]	O
~/x	O
)	O
ltl	O
(	O
dx	O
)	O
<	O
142	O
9.	O
the	O
regular	B
histogram	O
rule	B
so	O
by	O
theorem	B
9.2	O
,	O
for	O
sufficiently	O
large	O
n	O
,	O
p	O
{	O
f	O
iry	O
(	O
x	O
)	O
-	O
ry~	O
(	O
x	O
)	O
ijl	O
(	O
dx	O
)	O
>	O
~	O
}	O
<	O
p	O
{	O
f	O
iry	O
(	O
x	O
)	O
-	O
ry	O
:	O
(	O
x	O
)	O
ijl	O
(	O
dx	O
)	O
-	O
e	O
f	O
iry	O
(	O
x	O
)	O
-	O
<	O
e-n	O
£2	O
/32	O
.	O
0	O
ry	O
:	O
(	O
x	O
)	O
ijl	O
(	O
dx	O
)	O
>	O
~	O
}	O
remark	O
.	O
strong	B
universal	I
consistency	I
follows	O
from	O
the	O
exponential	B
bound	O
on	O
the	O
probability	O
p	O
{	O
ln	O
-	O
l	O
*	O
>	O
e	O
}	O
via	O
the	O
borel-cantelli	O
lemma	O
.	O
the	O
inequality	B
in	O
theorem	B
9.4	O
may	O
seem	O
universal	B
in	O
nature	O
.	O
however	O
,	O
it	O
is	O
distribution-dependent	O
in	O
a	O
surreptitious	O
way	O
because	O
its	O
range	O
of	O
validity	O
,	O
n	O
:	O
:	O
:	O
:	O
:	O
no	O
,	O
depends	O
heavily	O
on	O
e	O
,	O
h	O
n	O
,	O
and	O
the	O
distribution	B
.	O
we	O
know	O
that	O
distribution-free	O
upper	O
bounds	O
could	O
not	O
exist	O
anyway	O
,	O
in	O
view	O
of	O
theorem	O
7.2	O
.	O
0	O
problems	O
and	O
exercises	O
problem	O
9.1.	O
let	O
vi	O
,	O
v2	O
,	O
•••	O
be	O
independent	O
random	O
variables	O
with	O
zero	O
mean	O
.	O
show	O
that	O
the	O
random	O
variables	O
si	O
=	O
2	O
:	O
:~=1	O
vj	O
i	O
>	O
0	O
form	O
a	O
martingale	O
.	O
problem	O
9.2.	O
prove	O
lemma	O
9.1.	O
problem	O
9.3.	O
let	O
xl	O
,	O
...	O
,	O
x	O
n	O
be	O
real	O
valued	O
i.i.d	O
.	O
random	O
variables	O
with	O
distribution	O
func	O
(	O
cid:173	O
)	O
tion	O
f	O
(	O
x	O
)	O
,	O
and	O
corresponding	O
empirical	B
distribution	O
function	O
fn	O
(	O
x	O
)	O
=	O
~	O
2	O
:	O
::1	O
f	O
{	O
x	O
;	O
:9	O
)	O
'	O
denote	O
the	O
kolmogorov-smirnov	O
statistic	O
by	O
vn	O
=	O
sup	O
ifn	O
(	O
x	O
)	O
-	O
f	O
(	O
x	O
)	O
l·	O
xer	O
use	O
theorem	B
9.2	O
to	O
show	O
that	O
compare	O
this	O
result	O
with	O
theorem	O
12.9	O
.	O
(	O
none	O
of	O
them	O
implies	O
the	O
other	O
.	O
)	O
also	O
,	O
consider	O
a	O
class	O
a	O
of	O
subsets	O
of	O
nd	O
.	O
let	O
zl	O
,	O
...	O
,	O
zn	O
be	O
i.i.d	O
.	O
random	O
variables	O
in	O
n	O
d	O
with	O
common	O
distribution	B
p	O
{	O
zl	O
e	O
a	O
}	O
=	O
v	O
(	O
a	O
)	O
,	O
and	O
consider	O
the	O
random	O
variable	B
wn	O
=	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
i	O
,	O
aea	O
where	O
vnca	O
)	O
=	O
~	O
2	O
:	O
:7=1	O
f	O
{	O
2	O
;	O
ea	O
)	O
denotes	O
the	O
standard	B
empirical	I
measure	I
of	O
a.	O
prove	O
that	O
compare	O
this	O
result	O
with	O
theorem	O
12.5	O
,	O
and	O
note	O
that	O
this	O
result	O
is	O
true	O
even	O
if	O
sea	O
,	O
n	O
)	O
=	O
211	O
for	O
all	O
n.	O
problems	O
and	O
exercises	O
143	O
problem	O
9.4.	O
the	O
lazy	B
histogram	O
rule	B
.	O
let	O
pn	O
=	O
{	O
ani	O
,	O
an2	O
'	O
...	O
}	O
be	O
a	O
sequence	O
of	O
parti	O
(	O
cid:173	O
)	O
tions	O
satisfying	O
the	O
conditions	O
of	O
the	O
convergence	O
theorem	B
9.4.	O
define	O
the	O
lazy	B
histogram	O
rule	B
as	O
follows	O
:	O
gn	O
(	O
x	O
)	O
=	O
yj	O
,	O
x	O
e	O
ani	O
,	O
where	O
xj	O
is	O
the	O
minimum-index	O
point	O
among	O
xl	O
,	O
...	O
,	O
xn	O
for	O
which	O
xj	O
e	O
ani	O
.	O
in	O
other	O
words	O
,	O
we	O
ignore	O
all	O
but	O
one	O
point	O
in	O
each	O
set	O
of	O
the	O
partition	B
.	O
if	O
ln	O
is	O
the	O
conditional	O
probability	O
of	O
error	O
for	O
the	O
lazy	B
histogram	O
rule	B
,	O
then	O
show	O
that	O
for	O
any	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
lim	O
sup	O
eln	O
:	O
s	O
2l	O
*	O
.	O
n-+oo	O
problem	O
9.5.	O
assume	O
that	O
pn	O
=	O
p	O
=	O
{	O
ai	O
,	O
...	O
,	O
a	O
k	O
}	O
is	O
a	O
fixed	O
partition	B
into	O
k	O
sets	O
.	O
consider	O
the	O
lazy	B
histogram	O
rule	B
defined	O
in	O
problem	O
9.4	O
based	O
on	O
p.	O
show	O
that	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
,	O
limn-+	O
oo	O
eln	O
exists	O
and	O
satisfies	O
lim	O
eln	O
=	O
~	O
2pi	O
(	O
1	O
-	O
pi	O
)	O
p	O
,	O
(	O
aj	O
,	O
n-+oo	O
k	O
l	O
...	O
.	O
i=i	O
where	O
fl	O
is	O
the	O
probability	O
measure	B
for	O
x	O
,	O
and	O
pi	O
=	O
fa	O
;	O
'f/	O
(	O
x	O
)	O
fl	O
(	O
dx	O
)	O
/fl	O
(	O
ai	O
)	O
.	O
show	O
that	O
the	O
limit	O
of	O
the	O
probability	O
of	O
error	O
el~	O
for	O
the	O
ordinary	B
histogram	O
rule	B
is	O
lim	O
el~	O
=	O
~	O
min	O
(	O
pi	O
,	O
i	O
-	O
pi	O
)	O
fl	O
(	O
ai	O
)	O
,	O
n-+oo	O
k	O
l	O
...	O
.	O
i=i	O
and	O
show	O
that	O
lim	O
eln	O
:	O
s	O
2	O
lim	O
el~	O
.	O
n-+oo	O
n-+oo	O
problem	O
9.6.	O
histogram	B
density	I
estimation	I
.	O
let	O
xl	O
,	O
...	O
,	O
xn	O
be	O
i.i.d	O
.	O
random	O
variables	O
in	O
n	O
d	O
with	O
density	O
f.	O
let	O
p	O
be	O
a	O
partition	O
of	O
n	O
d	O
,	O
and	O
define	O
the	O
histogram	O
density	O
estimate	O
by	O
1	O
n	O
fn	O
(	O
x	O
)	O
=	O
na	O
(	O
a	O
(	O
x	O
»	O
~	O
i	O
{	O
x	O
;	O
ea	O
(	O
x	O
»	O
)	O
,	O
where	O
a	O
(	O
x	O
)	O
is	O
the	O
set	O
in	O
p	O
that	O
contains	O
x	O
and	O
a	O
is	O
the	O
lebesgue	O
measure	B
.	O
prove	O
for	O
the	O
l	O
1	O
-error	O
of	O
the	O
estimate	B
that	O
p	O
{	O
if	O
if	O
,	O
l	O
(	O
x	O
)	O
-	O
f	O
(	O
x	O
)	O
ldx	O
-	O
e	O
f	O
ifn	O
(	O
x	O
)	O
-	O
f	O
(	O
x	O
)	O
'dxi	O
>	O
e	O
}	O
:	O
s	O
2e-	O
ne2	O
/2	O
.	O
(	O
devroye	O
(	O
1991a	O
)	O
.	O
)	O
conclude	O
that	O
weak	B
l1-consistency	O
of	O
the	O
estimate	B
implies	O
strong	B
con	O
(	O
cid:173	O
)	O
sistency	O
(	O
abou-jaoude	O
(	O
1976a	O
;	O
1976c	O
)	O
,	O
see	O
also	O
problem	O
6.2	O
)	O
.	O
problem	O
9.7.	O
general	O
partitions	O
.	O
extend	O
the	O
consistency	B
result	O
of	O
theorem	O
9.4	O
for	O
se	O
(	O
cid:173	O
)	O
quences	O
of	O
general	O
,	O
not	O
necessarily	O
cubic	B
partitions	O
.	O
actually	O
,	O
cells	O
of	O
the	O
partitions	O
need	O
not	O
even	O
be	O
hyperrectangles	O
.	O
assume	O
that	O
the	O
sequence	O
of	O
partitions	O
{	O
pn	O
}	O
satisfies	O
the	O
following	O
two	O
conditions	O
.	O
for	O
every	O
ball	O
s	O
centered	O
at	O
the	O
origin	O
and	O
lim	O
.	O
max	O
(	O
sup	O
ilx	O
-	O
yii	O
)	O
=	O
0	O
n-+oo	O
i	O
:	O
an	O
;	O
nsi0	O
x	O
,	O
yean	O
;	O
lim	O
~i	O
{	O
i	O
:	O
ani	O
n	O
s	O
=/0	O
}	O
1	O
=	O
o.	O
n-+oo	O
n	O
prove	O
that	O
the	O
corresponding	O
histogram	O
classification	O
rule	B
is	O
strongly	O
universally	O
consistent	O
.	O
144	O
9.	O
the	O
regular	B
histogram	O
rule	B
problem	O
9.8.	O
show	O
that	O
for	O
cubic	B
histograms	O
the	O
conditions	O
of	O
problem	O
9.7	O
on	O
the	O
partition	B
are	O
equivalent	O
to	O
the	O
conditions	O
hn	O
--	O
-+	O
°	O
and	O
nh~	O
--	O
-+	O
00	O
,	O
respectively	O
.	O
problem	O
9.9.	O
linear	O
scaling	O
.	O
partition	B
n	O
d	O
into	O
congruent	O
rectangles	O
of	O
the	O
form	O
where	O
kl	O
'	O
...	O
,	O
kd	O
are	O
integers	O
,	O
and	O
hi	O
,	O
...	O
,	O
hd	O
>	O
°	O
denote	O
the	O
size	O
of	O
the	O
edges	O
of	O
the	O
if	O
hi	O
--	O
-+	O
°	O
for	O
every	O
i	O
=	O
1	O
,	O
...	O
,	O
d	O
,	O
and	O
nh	O
1h2	O
•••	O
hd	O
--	O
-+	O
00	O
as	O
n	O
--	O
-+	O
00.	O
hint	O
:	O
this	O
is	O
a	O
corollary	O
of	O
problem	O
9.7.	O
problem	O
9.10.	O
nonlinear	O
scaling	O
.	O
let	O
fi	O
,	O
...	O
,	O
fd	O
:	O
n	O
--	O
-+	O
n	O
be	O
invertible	O
,	O
strictly	O
mono	O
(	O
cid:173	O
)	O
tone	O
increasing	O
functions	O
.	O
consider	O
the	O
partition	B
of	O
n	O
d	O
,	O
whose	O
cells	O
are	O
rectangles	O
of	O
the	O
form	O
rectangles	O
.	O
prove	O
that	O
the	O
corresponding	O
histogram	B
rule	I
is	O
strongly	O
universally	O
consistent	O
[	O
fi-i	O
(	O
klhd	O
,	O
fi-i	O
(	O
(	O
kl	O
+	O
1	O
)	O
h	O
l	O
)	O
x	O
...	O
x	O
[	O
fd-l	O
(	O
kdhd	O
)	O
,	O
f	O
;	O
;	O
\	O
(	O
kd	O
+	O
l	O
)	O
hd	O
»	O
.	O
(	O
see	O
problem	O
9.9	O
.	O
)	O
prove	O
that	O
the	O
histogram	B
rule	I
corresponding	O
to	O
this	O
partition	B
is	O
strongly	O
universally	O
consistent	O
under	O
the	O
conditions	O
of	O
problem	O
9.9.	O
hint	O
:	O
use	O
problem	O
9.7.	O
problem	O
9.11.	O
necessary	O
and	O
sufficient	O
conditions	O
for	O
the	O
bias	B
.	O
a	O
sequence	O
of	O
par	O
(	O
cid:173	O
)	O
titions	O
{	O
pn	O
}	O
is	O
called	O
{	O
-	O
[	O
-approximating	O
if	O
for	O
every	O
measurable	O
set	O
a	O
,	O
for	O
every	O
e	O
>	O
0	O
,	O
and	O
for	O
all	O
sufficiently	O
large	O
n	O
there	O
is	O
a	O
set	O
an	O
e	O
a	O
(	O
pn	O
)	O
,	O
(	O
a	O
(	O
p	O
)	O
denotes	O
the	O
a-algebra	O
generated	O
by	O
cells	O
of	O
the	O
partition	B
p	O
)	O
such	O
that	O
{	O
-	O
[	O
(	O
anl	O
:	O
.a	O
)	O
<	O
e.	O
prove	O
that	O
the	O
bias	B
term	O
f	O
17j	O
(	O
x	O
)	O
-	O
e7j~	O
(	O
x	O
)	O
i	O
{	O
-	O
[	O
(	O
dx	O
)	O
converges	O
to	O
zero	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
if	O
and	O
only	O
if	O
the	O
sequence	O
of	O
partitions	O
{	O
pn	O
}	O
is	O
{	O
-	O
[	O
-approximating	O
for	O
every	O
probability	O
measure	B
{	O
-	O
[	O
on	O
n	O
d	O
(	O
abou-jaoude	O
(	O
l976a	O
»	O
.	O
conclude	O
that	O
the	O
first	O
condition	O
of	O
problem	O
9.7	O
implies	O
that	O
{	O
pn	O
}	O
is	O
{	O
-	O
[	O
-approximating	O
for	O
every	O
probability	O
measure	B
{	O
-	O
[	O
(	O
csiszar	O
(	O
1973	O
»	O
.	O
problem	O
9.12.	O
necessary	O
and	O
sufficient	O
conditions	O
for	O
the	O
variation	B
.	O
assume	O
that	O
for	O
every	O
probability	O
measure	B
fj-	O
on	O
n	O
d	O
,	O
every	O
measurable	O
set	O
a	O
,	O
every	O
c	O
>	O
0	O
,	O
and	O
every	O
e	O
>	O
0	O
,	O
there	O
is	O
an	O
n	O
(	O
e	O
,	O
c	O
,	O
a	O
,	O
{	O
-	O
[	O
)	O
,	O
such	O
that	O
for	O
all	O
n	O
>	O
n	O
(	O
e	O
,	O
c	O
,	O
a	O
,	O
{	O
-	O
[	O
)	O
,	O
l	O
{	O
-	O
[	O
(	O
an	O
,	O
}	O
n	O
a	O
)	O
<	O
e.	O
}	O
:	O
p.	O
,	O
(	O
an	O
,	O
j	O
na	O
)	O
:	O
sc	O
/	O
n	O
prove	O
that	O
the	O
variation	B
term	O
f	O
ie7j~	O
(	O
x	O
)	O
-7j~	O
(	O
x	O
)	O
i	O
{	O
-	O
[	O
(	O
dx	O
)	O
converges	O
to	O
zero	O
for	O
all	O
distributions	O
of	O
ex	O
,	O
y	O
)	O
if	O
and	O
only	O
if	O
the	O
sequence	O
of	O
partitions	O
{	O
pn	O
}	O
satisfies	O
the	O
condition	O
above	O
(	O
abou	O
(	O
cid:173	O
)	O
jaoude	O
(	O
l976a	O
»	O
.	O
problem	O
9.13.	O
the	O
e-effective	O
cardinality	O
m	O
(	O
p	O
,	O
ilv	O
,	O
a	O
,	O
e	O
)	O
of	O
a	O
partition	O
with	O
respect	O
to	O
the	O
probability	O
measure	B
{	O
-	O
[	O
,	O
restricted	O
to	O
a	O
set	O
a	O
is	O
the	O
minimum	O
number	O
of	O
sets	O
in	O
p	O
such	O
that	O
the	O
union	O
of	O
the	O
remaining	O
sets	O
intersected	O
with	O
a	O
has	O
{	O
-	O
[	O
-measure	O
less	O
than	O
e.	O
prove	O
that	O
the	O
sequence	O
of	O
partitions	O
{	O
pn	O
}	O
satisfies	O
the	O
condition	O
of	O
problem	O
9.12	O
if	O
and	O
only	O
if	O
for	O
every	O
e	O
>	O
0	O
,	O
1·	O
m	O
(	O
pn	O
,	O
{	O
-	O
[	O
,	O
a	O
,	O
e	O
)	O
°	O
=	O
1m	O
}	O
1	O
--	O
hxj	O
n	O
(	O
barron	O
,	O
gyorfi	O
,	O
and	O
van	O
der	O
meulen	O
(	O
1992	O
»	O
.	O
problem	O
9.14.	O
in	O
n	O
2	O
,	O
partition	B
the	O
plane	O
by	O
taking	O
three	O
fixed	O
points	O
not	O
on	O
a	O
line	O
,	O
x	O
,	O
y	O
and	O
z.	O
at	O
each	O
of	O
these	O
points	O
,	O
partition	B
n	O
2	O
by	O
considering	O
k	O
equal	O
sectors	O
of	O
angle	O
2	O
]	O
'	O
[	O
/	O
k	O
each	O
.	O
sets	O
in	O
the	O
histogram	O
partition	O
are	O
obtained	O
as	O
intersections	O
of	O
cones	O
.	O
is	O
the	O
induced	O
histogram	B
rule	I
strongly	O
universally	O
consistent	O
?	O
if	O
yes	O
,	O
state	O
the	O
conditions	O
on	O
k	O
,	O
and	O
if	O
no	O
,	O
provide	O
a	O
counterexample	O
.	O
problems	O
and	O
exercises	O
145	O
problem	O
9.15.	O
partition	B
n	O
2	O
into	O
shells	O
of	O
size	O
h	O
each	O
.	O
the	O
i-th	O
shell	O
contains	O
all	O
points	O
at	O
l	O
)	O
h	O
,	O
ih	O
)	O
from	O
the	O
origin	O
.	O
let	O
h	O
-+	O
0	O
and	O
nh	O
-+	O
00	O
as	O
n	O
-+	O
00.	O
consider	O
distance	B
d	O
e	O
[	O
(	O
i	O
-	O
the	O
histogram	B
rule	I
.	O
as	O
n	O
-+	O
00	O
,	O
to	O
what	O
does	O
eln	O
converge	O
?	O
10	O
kernel	B
rules	I
histogram	O
rules	O
have	O
the	O
somewhat	O
undesirable	O
property	O
that	O
the	O
rule	B
is	O
less	O
ac	O
(	O
cid:173	O
)	O
curate	O
at	O
borders	O
of	O
cells	O
of	O
the	O
partition	B
than	O
in	O
the	O
middle	O
of	O
cells	O
.	O
looked	O
at	O
intuitively	O
,	O
this	O
is	O
because	O
points	O
near	O
the	O
border	O
of	O
a	O
cell	O
should	O
have	O
less	O
weight	O
in	O
a	O
decision	O
regarding	O
the	O
cell	O
's	O
center	O
.	O
to	O
remedy	O
this	O
problem	O
,	O
one	O
might	O
in	O
(	O
cid:173	O
)	O
troduce	O
the	O
moving	B
window	I
rule	I
,	O
which	O
is	O
smoother	O
than	O
the	O
histogram	B
rule	I
.	O
this	O
classifier	B
simply	O
takes	O
the	O
data	O
points	O
within	O
a	O
certain	O
distance	B
of	O
the	O
point	O
to	O
be	O
classified	O
,	O
and	O
decides	O
according	O
to	O
majority	B
vote	I
.	O
working	O
formally	O
,	O
let	O
h	O
be	O
a	O
positive	O
number	O
.	O
then	O
the	O
moving	B
window	I
rule	I
is	O
defined	O
as	O
gn	O
(	O
x	O
)	O
=	O
{	O
0	O
if	O
2:7=1	O
i	O
{	O
yi=o	O
,	O
xiesx	O
,	O
h	O
}	O
~	O
2:7=1	O
i	O
{	O
yi	O
=l	O
,	O
xi	O
esx	O
,	O
h	O
}	O
otherwise	O
,	O
where	O
sx	O
,	O
h	O
denotes	O
the	O
closed	O
ball	O
of	O
radius	O
h	O
centered	O
at	O
x.	O
it	O
is	O
possible	O
to	O
make	O
the	O
decision	O
even	O
smoother	O
by	O
giving	O
more	O
weight	O
to	O
closer	O
points	O
than	O
to	O
more	O
distant	O
ones	O
.	O
let	O
k	O
:	O
rd	O
~	O
r	O
be	O
a	O
kernel	O
junction	O
,	O
which	O
is	O
usually	O
nonnegative	O
and	O
monotone	O
decreasing	O
along	O
rays	O
starting	O
from	O
the	O
origin	O
.	O
the	O
kernel	B
classification	O
rule	B
is	O
given	O
by	O
gn	O
(	O
x	O
)	O
=	O
{	O
0	O
'f~n	O
i	O
1	O
l..	O
,	O
i=l	O
otherwise	O
.	O
{	O
yi=o	O
}	O
k	O
(	O
x-xi	O
)	O
~n	O
i	O
-h-	O
~	O
l..	O
,	O
i=l	O
{	O
yi=l	O
}	O
k	O
(	O
x-xi	O
)	O
-h-	O
148	O
10.	O
kernel	B
rules	I
figure	O
10.1.	O
the	O
moving	O
win	O
(	O
cid:173	O
)	O
dow	O
rule	B
in	O
n2	O
.	O
the	O
decision	O
is	O
1	O
in	O
the	O
shaded	O
area	O
.	O
g	O
o	O
class	O
0	O
•	O
class	O
1	O
the	O
number	O
h	O
is	O
called	O
the	O
smoothing	B
factor	I
,	O
or	O
bandwidth	B
.	O
it	O
provides	O
some	O
form	O
of	O
distance	O
weighting	O
.	O
figure	O
10.2.	O
kernel	B
rule	I
on	O
the	O
real	O
line	O
.	O
the	O
figure	O
shows	O
:	O
l7=1	O
(	O
2yi	O
-l	O
)	O
k	O
(	O
(	O
x	O
-	O
xi	O
)	O
!	O
h	O
)	O
for	O
n	O
=	O
20	O
,	O
k	O
(	O
u	O
)	O
=	O
(	O
1	O
-	O
u	O
2	O
)	O
i	O
{	O
lul	O
:	O
:	O
:	O
:l	O
}	O
(	O
the	O
epanechnikov	O
kernel	B
)	O
,	O
and	O
three	O
smoothing	O
factors	O
h.	O
one	O
definitely	O
undersmooths	O
and	O
one	O
oversmooths	O
.	O
we	O
took	O
p	O
=	O
1/2	O
,	O
and	O
the	O
class-cconditional	O
densi	O
(	O
cid:173	O
)	O
ties	O
are	O
fo	O
(	O
x	O
)	O
=	O
2	O
(	O
1	O
-	O
x	O
)	O
and	O
flex	O
)	O
=	O
2x	O
on	O
[	O
0,1	O
]	O
.	O
clearly	O
,	O
the	O
kernel	B
rule	I
is	O
a	O
generalization	O
of	O
the	O
moving	B
window	I
rule	I
,	O
since	O
taking	O
the	O
special	O
kernel	B
k	O
(	O
x	O
)	O
=	O
i	O
{	O
xeso	O
,	O
d	O
yields	O
the	O
moving	B
window	I
rule	I
.	O
this	O
kernel	B
is	O
sometimes	O
called	O
the	O
naive	B
kernel	O
.	O
other	O
popular	O
kernels	O
include	O
the	O
gaus	O
(	O
cid:173	O
)	O
sian	O
kernel	B
,	O
k	O
(	O
x	O
)	O
=	O
e-	O
lixil2	O
;	O
the	O
cauchy	O
kernel	B
,	O
k	O
(	O
x	O
)	O
=	O
1/	O
(	O
1	O
+	O
iixll	O
d+l	O
)	O
;	O
and	O
the	O
epanechnikov	O
kernel	B
k	O
(	O
x	O
)	O
=	O
(	O
1	O
-	O
distance	B
.	O
ilxi12	O
)	O
i	O
{	O
lixll	O
:	O
::1	O
}	O
.	O
where	O
ii	O
.	O
ii	O
denotes	O
euclidean	O
10.1	O
consistency	B
149	O
gaussian	B
kernel	O
cauchy	O
kernel	B
epanechnikov	O
kernel	B
uniform	O
kernel	B
/\0	O
'	O
/\	O
,	O
xld	O
1	O
1	O
-1	O
0	O
-1	O
0	O
o	O
0	O
figure	O
10.3.	O
various	O
kernels	O
on	O
r.	O
kernel-based	O
rules	O
are	O
derived	O
from	O
the	O
kernel	B
estimate	O
in	O
density	O
estimation	B
originally	O
studied	O
by	O
parzen	O
(	O
1962	O
)	O
,	O
rosenblatt	O
(	O
1956	O
)	O
,	O
akaike	O
(	O
1954	O
)	O
,	O
and	O
ca	O
(	O
cid:173	O
)	O
coullos	O
(	O
1965	O
)	O
(	O
see	O
problems	O
10.2	O
and	O
10.3	O
)	O
;	O
and	O
in	O
regression	O
estimation	B
,	O
in	O
(	O
cid:173	O
)	O
troduced	O
by	O
nadaraya	O
(	O
1964	O
;	O
1970	O
)	O
,	O
and	O
watson	O
(	O
1964	O
)	O
.	O
for	O
particular	O
choices	O
of	O
k	O
,	O
rules	O
of	O
this	O
sort	O
have	O
been	O
proposed	O
by	O
fix	O
and	O
hodges	O
(	O
1951	O
;	O
1952	O
)	O
,	O
sebestyen	O
(	O
1962	O
)	O
,	O
van	O
ryzin	O
(	O
1966	O
)	O
,	O
and	O
meisel	O
(	O
1969	O
)	O
.	O
statistical	O
analysis	O
of	O
these	O
rules	O
and/or	O
the	O
corresponding	O
regression	B
function	I
estimate	O
can	O
be	O
found	O
in	O
nadaraya	O
(	O
1964	O
;	O
1970	O
)	O
,	O
rejto	O
and	O
revesz	O
(	O
1973	O
)	O
,	O
devroye	O
and	O
wagner	O
(	O
1976b	O
;	O
1980a	O
;	O
1980b	O
)	O
,	O
greblicki	O
(	O
1974	O
;	O
1978b	O
;	O
1978a	O
)	O
,	O
krzyzak	O
and	O
pawlak	O
(	O
1984b	O
)	O
,	O
and	O
devroye	O
and	O
krzyzak	O
(	O
1989	O
)	O
.	O
usage	O
of	O
cauchy	O
kernels	O
in	O
discrimination	O
is	O
investigated	O
by	O
arkadjew	O
and	O
braverman	O
(	O
1966	O
)	O
,	O
hand	O
(	O
1981	O
)	O
,	O
and	O
coomans	O
and	O
broeckaert	O
(	O
1986	O
)	O
.	O
10.1	O
consistency	B
in	O
this	O
section	O
we	O
demonstrate	O
strong	B
universal	I
consistency	I
of	O
kernel-based	O
rules	O
under	O
general	O
conditions	O
on	O
hand	O
k.	O
let	O
h	O
>	O
0	O
be	O
a	O
smoothing	O
factor	O
depending	O
only	O
on	O
n	O
,	O
and	O
let	O
k	O
be	O
a	O
kernel	O
function	O
.	O
if	O
the	O
conditional	O
densities	O
fo	O
,	O
!	O
i	O
exist	O
,	O
then	O
weak	B
and	O
strong	B
consistency	I
follow	O
from	O
problems	O
10.2	O
and	O
10.3	O
,	O
respec	O
(	O
cid:173	O
)	O
tively	O
,	O
via	O
problem	O
2.11.	O
we	O
state	O
the	O
universal	B
consistency	I
theorem	O
for	O
a	O
large	O
class	O
of	O
kernel	B
functions	O
,	O
namely	O
,	O
for	O
all	O
regular	B
kernels	O
.	O
definition	O
10.1.	O
the	O
kernel	B
k	O
is	O
called	O
regular	B
if	O
it	O
is	O
nonnegative	O
,	O
and	O
there	O
is	O
a	O
ball	O
so	O
,	O
r	O
of	O
radius	O
r	O
>	O
0	O
centered	O
at	O
the	O
origin	O
,	O
and	O
constant	O
b	O
>	O
0	O
such	O
that	O
k	O
(	O
x	O
)	O
~	O
biso	O
,	O
r	O
and	O
f	O
supyex+so	O
,	O
r	O
k	O
(	O
y	O
)	O
dx	O
<	O
00.	O
we	O
provide	O
three	O
informative	O
exercises	O
on	O
regular	B
kernels	O
(	O
problems	O
10.18	O
,	O
10.19	O
,	O
10.20	O
)	O
.	O
in	O
all	O
cases	O
,	O
regular	B
kernels	O
are	O
bounded	O
and	O
integrable	O
.	O
the	O
last	O
condition	O
holds	O
whenever	O
k	O
is	O
integrable	O
and	O
uniformly	O
continuous	O
.	O
introduce	O
the	O
short	O
notation	O
kh	O
(	O
x	O
)	O
=	O
kk	O
(	O
v.	O
the	O
next	O
theorem	B
states	O
strong	B
universal	I
con	O
(	O
cid:173	O
)	O
sistency	O
of	B
kernel	I
rules	I
.	O
the	O
theorem	B
is	O
essentially	O
due	O
to	O
devroye	O
and	O
krzyzak	O
150	O
10.	O
kernel	B
rules	I
(	O
1989	O
)	O
.	O
under	O
the	O
assumption	O
that	O
x	O
has	O
a	O
density	O
,	O
it	O
was	O
proven	O
by	O
devroye	O
and	O
gyorfi	O
(	O
1985	O
)	O
and	O
zhao	O
(	O
1989	O
)	O
.	O
theorem	B
10.1	O
.	O
(	O
devroye	O
and	O
krzyzak	O
(	O
1989	O
)	O
)	O
.	O
assume	O
that	O
k	O
is	O
a	O
regular	O
kernel.lf	O
h	O
~	O
0	O
and	O
nhd	O
~	O
00	O
as	O
n	O
~	O
00	O
,	O
thenfor	O
any	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
andfor	O
every	O
e	O
>	O
0	O
there	O
is	O
an	O
integer	O
no	O
such	O
that	O
for	O
n	O
>	O
no	O
for	O
the	O
error	O
probability	O
ln	O
of	O
the	O
kernel	B
rule	I
p	O
{	O
ln	O
-	O
l	O
*	O
>	O
e	O
}	O
.	O
:	O
:s	O
2e-ne2/	O
(	O
32p2	O
)	O
,	O
where	O
the	O
constant	O
p	O
depends	O
on	O
the	O
kernel	B
k	O
and	O
the	O
dimension	B
only	O
.	O
thus	O
,	O
the	O
kernel	B
rule	I
is	O
strongly	O
universally	O
consistent	O
.	O
clearly	O
,	O
naive	B
kernels	O
are	O
regular	B
,	O
and	O
moving	O
window	O
rules	O
are	O
thus	O
strongly	O
universally	O
consistent	O
.	O
for	O
the	O
sake	O
of	O
readability	O
,	O
we	O
give	O
the	O
proof	O
for	O
this	O
special	O
case	O
only	O
,	O
and	O
leave	O
the	O
extension	O
to	O
regular	B
kernels	O
to	O
the	O
reader-see	O
problems	O
10.14	O
,	O
10.15	O
,	O
and	O
10.16.	O
before	O
we	O
embark	O
on	O
the	O
proof	O
in	O
the	O
next	O
section	O
,	O
we	O
should	O
warn	O
the	O
reader	O
that	O
theorem	B
10.1	O
is	O
of	O
no	O
help	O
whatsoever	O
regarding	O
the	O
choice	O
of	O
k	O
or	O
h.	O
one	O
possible	O
solution	O
is	O
to	O
derive	O
explicit	O
upper	O
bounds	O
for	O
the	O
probability	O
of	O
error	O
as	O
a	O
function	O
of	O
descriptors	O
of	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
and	O
of	O
k	O
,	O
nand	O
h.	O
minimizing	O
such	O
bounds	O
with	O
respect	O
to	O
k	O
and	O
h	O
will	O
lead	O
to	O
some	O
expedient	O
choices	O
.	O
typically	O
,	O
such	O
bounds	O
would	O
be	O
based	O
upon	O
the	O
inequality	B
eln	O
-	O
l*	O
:	O
''	O
e	O
{	O
f	O
1	O
(	O
1	O
-	O
p	O
)	O
!	O
o	O
(	O
x	O
)	O
-	O
pno	O
!	O
no	O
(	O
x	O
)	O
ldx	O
+	O
f	O
ip	O
!	O
l	O
(	O
x	O
)	O
-	O
pnjinl	O
(	O
x	O
)	O
ldx	O
}	O
(	O
see	O
chapter	O
6	O
)	O
,	O
where	O
fo	O
,	O
f1	O
are	O
the	O
class	O
densities	O
,	O
fno	O
,	O
fn1	O
are	O
their	O
kernel	B
esti	O
(	O
cid:173	O
)	O
mates	O
(	O
see	O
problem	O
10.2	O
)	O
(	O
1	O
-	O
p	O
)	O
and	O
p	O
are	O
the	O
class	O
probabilities	O
,	O
and	O
pno	O
,	O
pnl	O
are	O
their	O
relative-frequency	O
estimates	O
.	O
bounds	O
for	O
the	O
expected	O
l1-error	O
in	O
den	O
(	O
cid:173	O
)	O
sity	O
estimation	B
may	O
be	O
found	O
in	O
devroye	O
(	O
1987	O
)	O
for	O
d	O
=	O
1	O
and	O
holmstrom	O
and	O
klemehi	O
(	O
1992	O
)	O
for	O
d	O
>	O
1.	O
under	O
regularity	O
conditions	O
on	O
the	O
distribution	B
,	O
the	O
choice	O
h	O
=	O
cn-d	O
/	O
(	O
d+4	O
)	O
for	O
some	O
constant	O
is	O
asymptotically	O
optimal	O
in	O
density	O
es	O
(	O
cid:173	O
)	O
timation	O
.	O
however	O
,	O
c	O
depends	O
upon	O
unknown	O
distributional	O
parameters	O
.	O
rather	O
than	O
following	O
this	O
roundabout	O
process	O
,	O
we	O
ask	O
the	O
reader	O
to	O
be	O
patient	O
and	O
to	O
wait	O
until	O
chapter	O
25	O
,	O
where	O
we	O
study	O
automatic	B
kernel	O
rules	O
,	O
i.e.	O
,	O
rules	O
in	O
which	O
h	O
,	O
and	O
sometimes	O
k	O
as	O
well	O
,	O
is	O
picked	O
by	O
the	O
data	O
without	O
intervention	O
from	O
the	O
statistician	O
.	O
it	O
is	O
still	O
too	O
early	O
to	O
say	O
meaningful	O
things	O
about	O
the	O
choice	O
of	O
a	O
kernel	O
.	O
the	O
kernel	O
density	O
estimate	O
fn	O
(	O
x	O
)	O
=	O
n~d	O
~	O
?	O
c	O
~	O
xi	O
)	O
based	O
upon	O
an	O
i.i.d	O
.	O
sample	O
xl	O
,	O
...	O
,	O
xn	O
drawn	O
from	O
an	O
unknown	O
density	O
f	O
is	O
clearly	O
a	O
density	O
in	O
its	O
own	O
right	O
if	O
k	O
:	O
:	O
:	O
:	O
0	O
and	O
j	O
k	O
=	O
1.	O
also	O
,	O
there	O
are	O
certain	O
10.1	O
consistency	B
151	O
popular	O
choices	O
of	O
k	O
that	O
are	O
based	O
upon	O
various	O
optimality	O
criteria	O
.	O
in	O
pattern	O
recognition	O
,	O
the	O
story	O
is	O
much	O
more	O
confused	O
,	O
as	O
there	O
is	O
no	O
compelling	O
a	O
priori	O
reason	O
to	O
pick	O
a	O
function	O
k	O
that	O
is	O
nonnegative	O
or	O
integrable	O
.	O
let	O
us	O
make	O
a	O
few	O
points	O
with	O
the	O
trivial	O
case	O
n	O
=	O
1.	O
t	O
$	O
ing	O
h	O
=	O
1	O
,	O
the	O
kernel	B
rule	I
is	O
given	O
by	O
(	O
x	O
)	O
=	O
{	O
0	O
ifyi	O
=o	O
,	O
k	O
(	O
x-xi	O
)	O
:	O
:	O
:	O
:oorifyi	O
=i	O
,	O
k	O
(	O
x-xd	O
:	O
:	O
:	O
:	O
;	O
o	O
gi	O
1	O
otherwise	O
.	O
if	O
k	O
:	O
:	O
:	O
:	O
0	O
,	O
then	O
gn	O
(	O
x	O
)	O
=	O
0	O
if	O
yi	O
=	O
0	O
,	O
or	O
if	O
yi	O
=	O
1	O
and	O
k	O
(	O
x	O
-	O
xd	O
=	O
o.	O
as	O
we	O
would	O
obviously	O
like	O
gn	O
(	O
x	O
)	O
=	O
0	O
if	O
and	O
only	O
if	O
y1	O
=	O
0	O
,	O
it	O
seems	O
necessary	O
to	O
insist	O
on	O
k	O
>	O
0	O
everywhere	O
.	O
however	O
,	O
this	O
restriction	O
makes	O
the	O
kernel	B
estimate	O
nonlocal	O
in	O
nature	O
.	O
for	O
n	O
=	O
1	O
and	O
d	O
=	O
1	O
,	O
consider	O
next	O
a	O
negative-valued	O
kernel	B
such	O
as	O
the	O
hermite	O
kernel	B
figure	O
10.4.	O
hermite	O
kernel	B
.	O
it	O
is	O
easy	O
to	O
verify	O
that	O
k	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
0	O
if	O
and	O
only	O
if	O
i	O
x	O
i	O
:	O
:	O
:	O
:	O
;	O
1.	O
also	O
,	O
j	O
k	O
=	O
o.	O
nevertheless	O
,	O
we	O
note	O
that	O
it	O
yields	O
a	O
simple	O
rule	B
:	O
(	O
x	O
)	O
=	O
{	O
o	O
gl	O
1	O
otherwise	O
.	O
if	O
yi	O
=	O
0	O
,	O
ix	O
-	O
xii	O
:	O
:	O
:	O
:	O
;	O
1	O
or	O
if	O
yi	O
=	O
i	O
,	O
ix	O
-	O
xii	O
:	O
:	O
:	O
:	O
1	O
if	O
we	O
have	O
a	O
biatomic	O
distribution	B
for	O
x	O
,	O
with	O
equally	O
likely	O
atoms	O
at	O
0	O
and	O
2	O
,	O
and	O
1	O
]	O
(	O
0	O
)	O
=	O
0	O
and	O
1	O
]	O
(	O
2	O
)	O
=	O
1	O
(	O
i.e.	O
,	O
y	O
=	O
0	O
if	O
x	O
=	O
0	O
and	O
y	O
=	O
1	O
if	O
x	O
=	O
2	O
)	O
,	O
then	O
l	O
*	O
=	O
0	O
and	O
the	O
probability	O
of	O
error	O
for	O
this	O
kernel	B
rule	I
(	O
l	O
i	O
)	O
is	O
0	O
as	O
well	O
.	O
note	O
also	O
that	O
for	O
all	O
n	O
,	O
gn	O
=	O
gi	O
if	O
we	O
keep	O
the	O
same	O
k.	O
consider	O
now	O
any	O
positive	O
kernel	B
in	O
the	O
same	O
example	O
.	O
if	O
xl	O
,	O
...	O
,	O
xn	O
are	O
all	O
zero	O
,	O
then	O
the	O
decision	O
is	O
gn	O
(	O
x	O
)	O
=	O
0	O
for	O
all	O
x.	O
hence	O
ln	O
:	O
:	O
:	O
:	O
ip	O
{	O
xl	O
=	O
...	O
=	O
xn	O
}	O
=	O
ij2n+l	O
>	O
o.	O
our	O
negative	O
zero-integral	O
kernel	B
is	O
strictly	O
better	O
for	O
all	O
n	O
than	O
any	O
positive	O
kernel	B
!	O
such	O
kernels	O
should	O
not	O
be	O
discarded	O
without	O
further	O
thought	O
.	O
in	O
density	O
estimation	B
,	O
negative-valued	O
kernels	O
are	O
used	O
to	O
reduce	O
the	O
bias	B
under	O
some	O
smoothness	O
conditions	O
.	O
here	O
,	O
as	O
shown	O
above	O
,	O
there	O
is	O
an	O
additional	O
reason-negative	O
weights	O
given	O
to	O
points	O
far	O
away	O
from	O
the	O
xi	O
's	O
may	O
actually	O
be	O
beneficial	O
.	O
staying	O
with	O
the	O
same	O
example	O
,	O
if	O
k	O
>	O
0	O
everywhere	O
,	O
then	O
eli	O
=	O
p	O
{	O
y1	O
=	O
0	O
,	O
y	O
=	O
i	O
}	O
+	O
p	O
{	O
y1	O
=	O
1	O
,	O
y	O
=	O
o	O
}	O
=	O
2e1	O
]	O
(	O
x	O
)	O
e	O
{	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
}	O
,	O
which	O
maybe	O
ij2	O
(	O
ife1	O
]	O
(	O
x	O
)	O
=	O
ij2	O
)	O
evenifl*	O
=	O
o	O
(	O
which	O
happens	O
when	O
1	O
]	O
e	O
{	O
o	O
,	O
i	O
}	O
everywhere	O
)	O
.	O
for	O
this	O
particular	O
example	O
,	O
we	O
would	O
have	O
obtained	O
the	O
same	O
result	O
152	O
10.	O
kernel	B
rules	I
even	O
if	O
k	O
==	O
1	O
everywhere	O
.	O
with	O
k	O
==	O
1	O
,	O
we	O
simply	O
ignore	O
the	O
xi	O
's	O
and	O
take	O
a	O
majority	O
vote	O
among	O
the	O
yi	O
's	O
(	O
with	O
k	O
==	O
-1	O
,	O
it	O
would	O
be	O
a	O
minority	O
vote	O
!	O
)	O
:	O
(	O
x	O
)	O
=	O
{	O
o	O
if	O
l:7==1	O
i	O
{	O
yi==o	O
}	O
~	O
l:7==1	O
i	O
{	O
yi==l	O
}	O
1	O
otherwise	O
.	O
gn	O
let	O
nn	O
be	O
the	O
number	O
of	O
yi	O
's	O
equal	O
to	O
zero	O
.	O
as	O
nn	O
is	O
binomial	B
(	O
n	O
,	O
1	O
-	O
p	O
)	O
with	O
p	O
=	O
e17	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
i	O
}	O
,	O
we	O
see	O
that	O
eln	O
=	O
pp	O
{	O
nn	O
~	O
~	O
}	O
+	O
(	O
1	O
-	O
p	O
)	O
p	O
{	O
nn	O
<	O
~	O
}	O
-+	O
min	O
(	O
p	O
,	O
1	O
-	O
p	O
)	O
,	O
simply	O
by	O
invoking	O
the	O
law	O
of	O
large	O
numbers	O
.	O
thus	O
,	O
eln	O
-+	O
min	O
(	O
p	O
,	O
1	O
-	O
p	O
)	O
.	O
as	O
in	O
the	O
case	O
with	O
n	O
=	O
1	O
,	O
the	O
limit	O
is	O
1/2	O
when	O
p	O
=	O
1/2	O
,	O
even	O
though	O
l	O
*	O
=	O
°	O
when	O
1	O
]	O
e	O
{	O
a	O
,	O
i	O
}	O
everywhere	O
.	O
it	O
is	O
interesting	O
to	O
note	O
the	O
following	O
though	O
:	O
eli	O
=	O
2p	O
(	O
1	O
-	O
p	O
)	O
=	O
2	O
min	O
(	O
p	O
,	O
1	O
-	O
p	O
)	O
(	O
1	O
-	O
min	O
(	O
p	O
,	O
1	O
-	O
p	O
»	O
<	O
2min	O
(	O
p	O
,	O
1	O
-	O
p	O
)	O
=	O
2	O
lim	O
elno	O
n-+oo	O
the	O
expected	O
error	O
with	O
one	O
observation	O
is	O
at	O
most	O
twice	O
as	O
bad	O
as	O
the	O
expected	O
error	O
with	O
an	O
infinite	O
sequence	O
.	O
we	O
have	O
seen	O
various	O
versions	O
of	O
this	O
inequality	B
at	O
work	O
in	O
many	O
instances	O
such	O
as	O
the	O
nearest	B
neighbor	I
rule	I
.	O
let	O
us	O
apply	O
the	O
inequality	B
for	O
el	O
1	O
to	O
each	O
part	O
in	O
a	O
fixed	O
partition	B
p	O
of	O
rd	O
.	O
on	O
each	O
of	O
the	O
k	O
sets	O
ai	O
,	O
...	O
,	O
ak	O
ofp	O
,	O
we	O
apply	O
a	O
simple	O
majority	B
vote	I
among	O
the	O
yi	O
's	O
,	O
as	O
in	O
the	O
histogram	B
rule	I
.	O
if	O
we	O
define	O
the	O
lazy	B
histogram	O
rule	B
as	O
the	O
one	O
in	O
which	O
in	O
each	O
set	O
ai	O
,	O
we	O
assign	O
the	O
class	O
according	O
to	O
the	O
yj	O
for	O
which	O
x	O
j	O
e	O
ai	O
and	O
j	O
is	O
the	O
lowest	O
such	O
index	O
(	O
``	O
the	O
first	O
point	O
to	O
fall	O
in	O
a/	O
'	O
)	O
.	O
it	O
is	O
clear	O
(	O
see	O
problems	O
9.4	O
and	O
9.5	O
)	O
that	O
lim	O
ellazy	O
,	O
n	O
<	O
2	O
lim	O
eln	O
n-+oo	O
n-+oo	O
=	O
2	O
t	O
/l	O
(	O
~i	O
)	O
l	O
ry	O
(	O
x	O
)	O
/l	O
(	O
dx	O
)	O
1,0	O
-ry	O
(	O
x	O
)	O
)	O
/l	O
(	O
dx	O
)	O
,	O
where	O
ln	O
is	O
the	O
probability	O
of	O
error	O
for	O
the	O
ordinary	B
histogram	O
rule	B
.	O
again	O
,	O
the	O
vast	O
majority	O
of	O
observations	O
is	O
barely	O
needed	O
to	O
reach	O
a	O
good	O
decision	O
.	O
just	O
for	O
fun	O
,	O
let	O
us	O
return	O
to	O
a	O
majority	O
vote	O
rule	B
,	O
now	O
applied	O
to	O
the	O
first	O
three	O
observations	O
only	O
.	O
with	O
p	O
=	O
p	O
{	O
y	O
=	O
i	O
}	O
,	O
we	O
see	O
that	O
el3	O
=	O
p	O
(	O
(	O
1-	O
p	O
)	O
3	O
+	O
3	O
(	O
1-	O
pfp	O
)	O
+	O
(	O
1-	O
p	O
)	O
(	O
3	O
(	O
1-	O
p	O
)	O
p2	O
+	O
p3	O
)	O
by	O
just	O
writing	O
down	O
binomial	B
probabilities	O
.	O
observe	O
that	O
el3	O
=	O
p	O
(	O
1	O
-	O
p	O
)	O
(	O
1	O
+	O
4p	O
(	O
1	O
-	O
p	O
»	O
<	O
min	O
(	O
p	O
,	O
1-	O
p	O
)	O
(	O
l	O
+4min	O
(	O
p	O
,	O
1	O
lim	O
eln	O
(	O
1	O
+	O
4	O
lim	O
4eln	O
)	O
.	O
=	O
n-+oo	O
n-+oo	O
p	O
»	O
10.2	O
proof	O
of	O
the	O
consistency	B
theorem	O
153	O
if	O
limn	O
--	O
+oo	O
eln	O
is	O
small	O
to	O
start	O
with	O
,	O
e.g.	O
,	O
limn	O
--	O
+oo	O
eln	O
=	O
0.01	O
,	O
then	O
el3	O
:	O
:	O
:	O
:	O
0.01	O
x	O
1.04	O
=	O
0.0104.	O
in	O
such	O
cases	O
,	O
it	O
just	O
does	O
not	O
pay	O
to	O
take	O
more	O
than	O
three	O
observations	O
.	O
kernels	O
with	O
fixed	O
smoothing	O
factors	O
have	O
no	O
local	O
sensitivity	O
and	O
,	O
except	O
in	O
some	O
circumstances	O
,	O
have	O
probabilities	O
of	O
error	O
that	O
do	O
not	O
converge	O
to	O
l	O
*	O
.	O
the	O
uni	O
(	O
cid:173	O
)	O
versal	O
consistency	B
theorem	O
makes	O
a	O
strong	O
case	O
for	O
decreasing	O
smoothing	O
factors	O
(	O
cid:173	O
)	O
there	O
is	O
no	O
hope	O
in	O
general	O
of	O
approaching	O
l	O
*	O
unless	O
decisions	O
are	O
asymptotically	O
local	O
.	O
the	O
consistency	B
theorem	O
describes	O
kernel	B
rules	I
with	O
h	O
-+	O
0	O
:	O
these	O
rules	O
become	O
more	O
and	O
more	O
local	O
in	O
nature	O
as	O
n	O
-+	O
00.	O
the	O
necessity	O
oflocal	O
rules	O
is	O
not	O
appar	O
(	O
cid:173	O
)	O
ent	O
from	O
the	O
previous	O
biatomic	O
example	O
.	O
however	O
,	O
it	O
is	O
clear	O
that	O
if	O
we	O
consider	O
a	O
distribution	O
in	O
which	O
given	O
y	O
=	O
0	O
,	O
x	O
is	O
uniform	B
on	O
{	O
8	O
,	O
38	O
,	O
...	O
,	O
(	O
2k	O
+	O
1	O
)	O
8	O
}	O
,	O
and	O
given	O
y	O
=	O
1	O
,	O
x	O
is	O
uniform	B
on	O
{	O
28	O
,	O
48	O
,	O
...	O
,	O
2k8	O
}	O
,	O
that	O
is	O
,	O
with	O
the	O
two	O
classes	O
intimately	O
interwoven	O
,	O
a	O
kernel	O
rule	B
with	O
k	O
:	O
:	O
:	O
0	O
of	O
compact	O
support	B
[	O
-1	O
,	O
1	O
]	O
,	O
and	O
h	O
<	O
8	O
<	O
1	O
will	O
have	O
ln	O
:	O
:	O
:	O
:	O
i	O
{	O
u~	O
!	O
dni=o	O
}	O
}	O
,	O
where	O
ni	O
is	O
the	O
number	O
of	O
x/s	O
at	O
the	O
i-th	O
atom	O
.	O
hence	O
eln	O
goes	O
to	O
zero	O
expo	O
(	O
cid:173	O
)	O
nentially	O
fast	O
.	O
if	O
in	O
the	O
above	O
example	O
we	O
assign	O
x	O
by	O
a	O
geometric	O
distribution	B
on	O
8,83	O
,85	O
,	O
...	O
when	O
y	O
=	O
0	O
,	O
and	O
by	O
a	O
geometric	O
distribution	B
on	O
82	O
,84	O
,86	O
,	O
...	O
when	O
y	O
=	O
1	O
,	O
then	O
to	O
obtain	O
eln	O
-+	O
0	O
,	O
it	O
is	O
necessary	O
that	O
h	O
-+	O
0	O
(	O
see	O
problem	O
10.1	O
)	O
.	O
remark	O
.	O
it	O
is	O
worthwhile	O
to	O
investigate	O
what	O
happens	O
for	O
negative-valued	O
kernels	O
k	O
when	O
h	O
-+	O
0	O
,	O
nhd	O
-+	O
00	O
and	O
k	O
has	O
compact	O
support	B
.	O
every	O
decision	O
becomes	O
an	O
average	O
over	O
many	O
local	O
decisions	O
.	O
if	O
fj	O
has	O
a	O
density	O
f	O
,	O
then	O
at	O
almost	O
all	O
points	O
x	O
,	O
f	O
may	O
be	O
approximated	O
very	O
nicely	O
by	O
f	O
sx8	O
f/'a	O
(	O
sx	O
,	O
o	O
)	O
for	O
small	O
8	O
>	O
0	O
,	O
where	O
s	O
x	O
,	O
o	O
is	O
the	O
closed	O
ball	O
of	O
radius	O
8	O
about	O
x.	O
this	O
implies	O
,	O
roughly	O
speaking	O
,	O
that	O
the	O
number	O
of	O
weighted	O
votes	O
from	O
class	O
0	O
observations	O
in	O
a	O
neighborhood	O
of	O
x	O
is	O
about	O
1	O
]	O
(	O
x	O
»	O
f	O
(	O
x	O
)	O
nhd	O
f	O
k	O
,	O
while	O
for	O
class	O
1	O
the	O
weight	O
is	O
about	O
1	O
]	O
(	O
x	O
)	O
f	O
(	O
x	O
)	O
nhd	O
f	O
k.	O
(	O
1	O
the	O
correct	O
decision	O
is	O
nearly	O
always	O
made	O
for	O
nh	O
d	O
large	O
enough	O
provided	O
that	O
j	O
k	O
>	O
o.	O
see	O
problem	O
10.4	O
on	O
why	O
kernels	O
with	O
j	O
k	O
<	O
0	O
should	O
be	O
avoided	O
.	O
0	O
10.2	O
proof	O
of	O
the	O
consistency	B
theorem	O
in	O
the	O
proof	O
we	O
can	O
proceed	O
as	O
for	O
the	O
histogram	O
.	O
the	O
crucial	O
difference	O
is	O
captured	O
in	O
the	O
following	O
covering	B
lemmas	O
.	O
let	O
f3d	O
denote	O
the	O
minimum	O
number	O
of	O
balls	O
of	O
radius	O
1/2	O
that	O
cover	O
the	O
ball	O
so	O
,	O
1.	O
if	O
k	O
is	O
the	O
naive	B
kernel	O
,	O
then	O
p	O
=	O
f3d	O
in	O
theorem	O
10.1.	O
lemma	O
10.1	O
.	O
(	O
covering	B
lemma	I
)	O
./f	O
k	O
(	O
x	O
)	O
=	O
i	O
{	O
xeso	O
,	O
il	O
,	O
thenforany	O
y	O
end	O
,	O
h	O
>	O
0	O
,	O
and	O
probability	O
measure	B
fj	O
,	O
f	O
kh	O
(	O
x	O
-	O
y	O
)	O
f	O
kh	O
(	O
x	O
-	O
z	O
)	O
fj	O
(	O
dz	O
)	O
fj	O
(	O
dx	O
)	O
:	O
:	O
:	O
:	O
f3d	O
.	O
154	O
10.	O
kernel	B
rules	I
proof	O
.	O
cover	O
the	O
ball	O
sy	O
,	O
h	O
by	O
fjd	O
balls	O
of	O
radius	O
h12	O
.	O
denote	O
their	O
centers	O
by	O
xl	O
,	O
...	O
,	O
xf3d	O
'	O
then	O
x	O
e	O
sxi	O
,	O
h/2	O
implies	O
sxi	O
,	O
h/2	O
c	O
sx	O
,	O
h	O
and	O
thus	O
we	O
may	O
write	O
jl	O
(	O
sx	O
,	O
h	O
)	O
f	O
i	O
{	O
xesy	O
,	O
h	O
}	O
jl	O
(	O
dx	O
)	O
<	O
t	O
f	O
i	O
{	O
xe	O
;	O
xi	O
,	O
hi2	O
}	O
jl	O
(	O
dx	O
)	O
<	O
t	O
f	O
i	O
{	O
xesxi	O
,	O
hf2	O
}	O
jl	O
(	O
dx	O
)	O
jl	O
(	O
x	O
,	O
h	O
)	O
i==l	O
i==l	O
jl	O
(	O
sxi	O
,	O
hj2	O
)	O
=	O
f3d	O
.	O
0	O
lemma	O
10.2.	O
let	O
0	O
<	O
h	O
:	O
:	O
:	O
:	O
r	O
<	O
00	O
,	O
and	O
let	O
s	O
c	O
rd	O
be	O
a	O
ball	O
of	O
radius	O
r.	O
then	O
for	O
every	O
probability	O
measure	B
jl	O
,	O
where	O
cd	O
depends	O
upon	O
the	O
dimension	B
d	O
only	O
.	O
proof	O
.	O
cover	O
s	O
with	O
balls	O
of	O
radius	O
h	O
12	O
,	O
centered	O
at	O
center	O
points	O
of	O
a	O
regular	O
grid	O
of	O
dimension	B
hl	O
(	O
2-j	O
(	O
i	O
)	O
x	O
...	O
x	O
hl	O
(	O
2-j	O
(	O
i	O
)	O
.	O
denote	O
these	O
centers	O
by	O
xl	O
,	O
..•	O
,	O
xm	O
,	O
where	O
m	O
is	O
the	O
number	O
of	O
balls	O
that	O
cover	O
s.	O
clearly	O
,	O
m	O
<	O
=	O
volume	O
(	O
so	O
,	O
r+h	O
)	O
volume	O
(	O
grid	O
cell	O
)	O
vd	O
(	O
r	O
+	O
h	O
)	O
d	O
(	O
vd	O
is	O
the	O
volume	O
of	O
the	O
unit	O
ball	O
in	O
r	O
d	O
)	O
(	O
hl	O
(	O
2-j	O
(	O
i	O
)	O
)	O
d	O
<	O
(	O
l+~r	O
c~	O
,	O
where	O
the	O
constant	O
c~	O
depends	O
upon	O
the	O
dimension	B
only	O
.	O
every	O
x	O
gets	O
covered	O
at	O
most	O
kl	O
times	O
where	O
kl	O
depends	O
upon	O
d	O
only	O
.	O
then	O
we	O
have	O
10.2	O
proof	O
of	O
the	O
consistency	B
theorem	O
155	O
m	O
f	O
i	O
<	O
l	O
i	O
:	O
:	O
:	O
l	O
{	O
xesxi	O
,	O
hj2	O
}	O
j	O
tt	O
(	O
sxi	O
,	O
h/2	O
)	O
tt	O
(	O
dx	O
)	O
(	O
by	O
the	O
same	O
argument	O
as	O
in	O
lemma	O
10.1	O
)	O
m	O
<	O
l	O
j	O
tt	O
(	O
sxi	O
,	O
h/2	O
)	O
i	O
:	O
:	O
:	O
l	O
m	O
<	O
m	O
l	O
tt	O
(	O
sxi	O
,	O
h/2	O
)	O
i	O
:	O
:	O
:	O
l	O
(	O
by	O
the	O
cauchy-schwarz	O
inequality	B
)	O
<	O
jklm	O
<	O
he	O
d	O
'	O
where	O
cd	O
depends	O
upon	O
the	O
dimension	B
only	O
.	O
0	O
proof	O
of	O
theorem	O
10.1.	O
define	O
2	O
:	O
}	O
:	O
::1	O
yjkh	O
(	O
x	O
-	O
x	O
j	O
)	O
.	O
nekh	O
(	O
x	O
-	O
x	O
)	O
l1n	O
(	O
x	O
)	O
=	O
since	O
the	O
decision	O
rule	O
can	O
be	O
written	O
as	O
gn	O
(	O
x	O
)	O
=	O
{	O
0	O
if	O
2	O
:	O
}	O
:	O
::1	O
yjkh	O
(	O
x	O
-	O
x	O
j	O
)	O
<	O
2	O
:	O
}	O
:	O
::1	O
(	O
1-	O
yj	O
)	O
kh	O
(	O
x	O
-	O
x	O
j	O
)	O
nekh	O
(	O
x	O
-	O
x	O
)	O
-	O
nekh	O
(	O
x	O
-	O
x	O
)	O
1	O
otherwise	O
,	O
by	O
theorem	B
2.3	O
,	O
what	O
we	O
have	O
to	O
prove	O
is	O
that	O
for	O
n	O
large	O
enough	O
p	O
{	O
f	O
iry	O
(	O
x	O
)	O
-	O
ryn	O
(	O
x	O
)	O
iil	O
(	O
dx	O
)	O
>	O
~	O
}	O
:	O
''	O
e-ne'/	O
(	O
32p	O
'	O
)	O
we	O
use	O
a	O
decomposition	O
as	O
in	O
the	O
proof	O
of	O
strong	O
consistency	B
of	O
the	O
histogram	B
rule	I
:	O
111	O
(	O
x	O
)	O
-	O
l1n	O
(	O
x	O
)	O
1	O
=	O
ell1	O
(	O
x	O
)	O
-	O
l1n	O
(	O
x	O
)	O
1	O
+	O
(	O
ll1	O
(	O
x	O
)	O
-	O
l1n	O
(	O
x	O
)	O
1	O
-	O
ell1	O
(	O
x	O
)	O
-	O
l1n	O
(	O
x	O
)	O
i	O
)	O
.	O
(	O
10.1	O
)	O
to	O
handle	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
,	O
fix	O
e/	O
>	O
0	O
,	O
and	O
let	O
r	O
:	O
rd	O
-+	O
r	O
be	O
a	O
continuous	O
function	O
of	O
bounded	O
support	B
satisfying	O
f	O
111	O
(	O
x	O
)	O
-	O
r	O
(	O
x	O
)	O
ltt	O
(	O
dx	O
)	O
<	O
e/	O
.	O
156	O
10.	O
kernel	B
rules	I
obviously	O
,	O
we	O
can	O
choose	O
the	O
function	O
r	O
such	O
that	O
0	O
s	O
r	O
(	O
x	O
)	O
s	O
1	O
for	O
all	O
x	O
e	O
rd	O
.	O
then	O
we	O
have	O
the	O
following	O
simple	O
upper	O
bound	O
:	O
ei1	O
]	O
(	O
x	O
)	O
-	O
1	O
]	O
n	O
(	O
x	O
)	O
!	O
<	O
11	O
]	O
(	O
x	O
)	O
-	O
e	O
{	O
r	O
(	O
x	O
)	O
kh	O
(	O
x	O
-	O
x	O
)	O
}	O
i	O
r	O
(	O
x	O
)	O
1	O
+	O
rex	O
)	O
-	O
-	O
-	O
-	O
-	O
-	O
(	O
cid:173	O
)	O
ekh	O
(	O
x	O
-	O
x	O
)	O
i	O
+	O
e	O
{	O
r	O
(	O
x	O
)	O
kh	O
(	O
x	O
-	O
x	O
)	O
}	O
i	O
ekh	O
(	O
x	O
-	O
x	O
)	O
i	O
-	O
e1	O
]	O
n	O
(	O
x	O
)	O
+	O
eie1	O
]	O
n	O
(	O
x	O
)	O
-	O
1	O
]	O
n	O
(	O
x	O
)	O
l·	O
(	O
10.2	O
)	O
next	O
we	O
bound	O
the	O
integral	O
of	O
each	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
the	O
inequality	B
above	O
.	O
first	O
term	O
:	O
by	O
the	O
definition	B
of	I
r	O
,	O
f	O
11	O
]	O
(	O
x	O
)	O
-	O
r	O
(	O
x	O
)	O
ltl	O
(	O
dx	O
)	O
<	O
fl	O
.	O
second	O
term	O
:	O
since	O
r	O
(	O
x	O
)	O
is	O
continuous	O
and	O
zero	O
outside	O
of	O
a	O
bounded	O
set	O
,	O
it	O
is	O
also	O
uniformly	O
continuous	O
,	O
that	O
is	O
,	O
there	O
exists	O
a	O
8	O
>	O
0	O
such	O
that	O
ilx	O
-	O
y	O
\i	O
<	O
8	O
implies	O
ir	O
(	O
x	O
)	O
-	O
r	O
(	O
y	O
)	O
1	O
<	O
ft.	O
also	O
,	O
rex	O
)	O
is	O
bounded	O
.	O
thus	O
,	O
rex	O
)	O
-	O
tl	O
(	O
dx	O
)	O
ekh	O
(	O
x	O
_	O
x	O
)	O
e	O
{	O
r	O
(	O
x	O
)	O
kh	O
(	O
x	O
-	O
x	O
)	O
}	O
i	O
f	O
i	O
=	O
f	O
ir	O
(	O
x	O
)	O
-	O
f	O
r	O
(	O
y	O
)	O
kh	O
(	O
x	O
-	O
y	O
)	O
tl	O
(	O
dy	O
)	O
i	O
jl	O
(	O
dx	O
)	O
s	O
f	O
(	O
kh	O
(	O
x	O
-	O
y	O
)	O
ekh	O
(	O
x	O
-	O
x	O
)	O
]	O
sx	O
,	O
s	O
ekh	O
(	O
x	O
-	O
x	O
)	O
!	O
r	O
(	O
x	O
)	O
-	O
r	O
(	O
y	O
)	O
!	O
tl	O
(	O
dy	O
)	O
tl	O
(	O
dx	O
)	O
r	O
(	O
y	O
)	O
1	O
s	O
1.	O
clearly	O
,	O
we	O
have	O
in	O
the	O
last	O
step	O
we	O
used	O
the	O
fact	O
that	O
supx	O
,	O
y	O
!	O
r	O
(	O
x	O
)	O
-	O
fsx	O
,	O
ii	O
e~h~	O
:	O
?	O
l	O
)	O
tl	O
(	O
dy	O
)	O
s	O
1	O
,	O
and	O
by	O
the	O
uniform	B
continuity	O
of	O
rex	O
)	O
,	O
supzesx	O
,	O
o	O
ir	O
(	O
x	O
)	O
-	O
r	O
(	O
z	O
)	O
1	O
<	O
ft.	O
thus	O
,	O
the	O
first	O
term	O
at	O
the	O
end	O
of	O
the	O
chain	O
of	O
inequalities	O
above	O
is	O
bounded	O
by	O
ft.	O
the	O
second	O
term	O
converges	O
to	O
zero	O
since	O
h	O
<	O
8	O
for	O
all	O
n	O
large	O
enough	O
,	O
which	O
in	O
tum	O
implies	O
fs~	O
/j	O
e~	O
,	O
(	O
z¥	O
?	O
l	O
)	O
tl	O
(	O
dy	O
)	O
=	O
o	O
.	O
(	O
this	O
is	O
obvious	O
for	O
the	O
naive	B
kernel	O
.	O
for	O
regular	B
kernels	O
,	O
convergence	O
to	O
zero	O
follows	O
from	O
problem	O
10.15	O
.	O
)	O
the	O
convergence	O
of	O
the	O
integral	O
(	O
with	O
respect	O
to	O
tl	O
(	O
dx	O
»	O
follows	O
from	O
the	O
dominated	O
10.2	O
proof	O
of	O
the	O
consistency	B
theorem	O
157	O
convergence	O
theorem	B
.	O
in	O
summary	O
,	O
we	O
have	O
shown	O
that	O
e	O
{	O
r	O
(	O
x	O
)	O
kh	O
(	O
x	O
-	O
x	O
)	O
}	O
i	O
lim	O
sup	O
n	O
--	O
+oo	O
r	O
(	O
x	O
)	O
-	O
..	O
ekh	O
(	O
x	O
-	O
x	O
)	O
i	O
i	O
p	O
,	O
(	O
dx	O
)	O
:	O
s	O
e	O
'	O
.	O
third	O
term	O
:	O
e	O
{	O
r	O
(	O
x	O
)	O
kh	O
(	O
x	O
-	O
x	O
)	O
}	O
ekh	O
(	O
x	O
_	O
x	O
)	O
i	O
-	O
el	O
]	O
n	O
(	O
x	O
)	O
p	O
,	O
(	O
dx	O
)	O
i	O
i	O
ir	O
(	O
y	O
)	O
-	O
l	O
]	O
(	O
y	O
)	O
i	O
!	O
kh	O
(	O
x	O
_	O
z	O
)	O
p	O
,	O
(	O
dz	O
)	O
p	O
,	O
(	O
dy	O
)	O
p	O
,	O
(	O
dx	O
)	O
p	O
,	O
(	O
dx	O
)	O
:	O
s	O
!	O
(	O
r	O
(	O
y	O
)	O
-	O
kh	O
(	O
x	O
-	O
y	O
)	O
l	O
]	O
(	O
y	O
)	O
)	O
kh	O
(	O
x	O
-	O
y	O
)	O
p	O
,	O
(	O
dy	O
)	O
i	O
ekh	O
(	O
x	O
-	O
x	O
)	O
i	O
i	O
i	O
i	O
=	O
i	O
(	O
f	O
f	O
k~	O
:	O
(	O
~	O
~	O
)	O
~	O
(	O
dz/	O
''	O
(	O
dx	O
)	O
)	O
ir	O
(	O
y	O
)	O
-	O
:	O
s	O
i	O
plr	O
(	O
y	O
)	O
-	O
l	O
]	O
(	O
y	O
)	O
ip	O
,	O
(	O
dy	O
)	O
:	O
s	O
pe	O
'	O
,	O
(	O
by	O
fubini	O
's	O
theorem	B
)	O
ry	O
(	O
y	O
)	O
ll	O
''	O
(	O
dy	O
)	O
where	O
in	O
the	O
last	O
two	O
steps	O
we	O
used	O
the	O
covering	B
lemma	I
(	O
see	O
lemma	O
10.1	O
for	O
the	O
naive	B
kernel	O
,	O
and	O
problem	O
10.14	O
for	O
general	O
kernels	O
)	O
,	O
and	O
the	O
definition	B
of	I
rex	O
)	O
.	O
p	O
is	O
the	O
constant	O
appearing	O
in	O
the	O
covering	B
lemma	I
:	O
fourth	O
term	O
:	O
we	O
show	O
that	O
e	O
{	O
i	O
ieryn	O
(	O
x	O
)	O
-	O
for	O
the	O
naive	B
kernel	O
,	O
we	O
have	O
ryn	O
(	O
x	O
)	O
i	O
i	O
''	O
(	O
dx	O
)	O
}	O
-+	O
o.	O
e	O
{	O
iel	O
]	O
n	O
(	O
x	O
)	O
-	O
l	O
]	O
n	O
(	O
x	O
)	O
1	O
}	O
:	O
s	O
je	O
{	O
iel	O
]	O
n	O
(	O
x	O
)	O
-	O
l	O
]	O
n	O
(	O
x	O
)	O
1	O
2	O
}	O
e	O
{	O
(	O
l	O
:	O
j.	O
,	O
(	O
yjkh	O
(	O
x	O
-	O
x	O
j	O
)	O
-	O
ely	O
kh	O
(	O
x	O
_	O
xl	O
}	O
)	O
)	O
2	O
}	O
n	O
2	O
(	O
ekh	O
(	O
x	O
-	O
x	O
)	O
)	O
2	O
e	O
{	O
(	O
ykh	O
(	O
x	O
-	O
x	O
)	O
-	O
e	O
{	O
ykh	O
(	O
x	O
-	O
x	O
)	O
}	O
)	O
2	O
}	O
n	O
(	O
ekh	O
(	O
x	O
-	O
x	O
)	O
)	O
2	O
158	O
10.	O
kernel	B
rules	I
where	O
we	O
used	O
the	O
cauchy-schwarz	O
inequality	B
,	O
and	O
properties	O
of	O
the	O
naive	B
kernel	O
.	O
extension	O
to	O
regular	B
kernels	O
is	O
straightforward	O
.	O
next	O
we	O
use	O
the	O
inequality	B
above	O
to	O
show	O
that	O
the	O
integral	O
converges	O
to	O
zero	O
.	O
divide	O
the	O
integral	O
over	O
nd	O
into	O
two	O
terms	O
,	O
namely	O
an	O
integral	O
over	O
a	O
large	O
ball	O
s	O
centered	O
at	O
the	O
origin	O
,	O
of	O
radius	O
r	O
>	O
0	O
,	O
and	O
an	O
integral	O
over	O
sc	O
.	O
for	O
the	O
integral	O
outside	O
of	O
the	O
ball	O
we	O
have	O
1	O
e	O
{	O
ie1	O
]	O
n	O
(	O
x	O
)	O
-	O
1	O
]	O
n	O
(	O
x	O
)	O
1	O
}	O
fl	O
(	O
dx	O
)	O
:	O
:	O
:	O
;	O
21	O
e1	O
]	O
n	O
(	O
x	O
)	O
fl	O
(	O
dx	O
)	O
--	O
+	O
211	O
]	O
(	O
x	O
)	O
fl	O
(	O
dx	O
)	O
with	O
probability	O
one	O
as	O
n	O
--	O
+	O
00	O
,	O
which	O
can	O
be	O
shown	O
in	O
the	O
same	O
way	O
we	O
proved	O
~	O
~	O
~	O
[	O
e1	O
]	O
n	O
(	O
x	O
)	O
fl	O
(	O
dx	O
)	O
--	O
+	O
jrd	O
[	O
1	O
]	O
(	O
x	O
)	O
fl	O
(	O
dx	O
)	O
jrd	O
(	O
see	O
the	O
first	O
,	O
second	O
,	O
and	O
third	O
terms	O
of	O
(	O
10.2	O
)	O
)	O
.	O
clearly	O
,	O
the	O
radius	O
r	O
of	O
the	O
ball	O
s	O
can	O
be	O
chosen	O
such	O
that	O
2isc	O
1	O
]	O
(	O
x	O
)	O
fl	O
(	O
dx	O
)	O
<	O
e/4	O
.	O
to	O
bound	O
the	O
integral	O
over	O
s	O
we	O
employ	O
lemma	O
10.2	O
:	O
f.	O
e	O
{	O
ie1	O
]	O
n	O
(	O
x	O
)	O
-	O
1	O
]	O
n	O
(	O
x	O
)	O
1	O
}	O
fl	O
(	O
dx	O
)	O
<	O
-1-1	O
1	O
fl	O
(	O
dx	O
)	O
..	O
;	O
n	O
s	O
j	O
fl	O
(	O
sx	O
,	O
h	O
)	O
(	O
by	O
the	O
inequality	B
obtained	O
above	O
)	O
r	O
)	O
d/2	O
1+	O
-	O
h	O
cd	O
1	O
(	O
-	O
..	O
;	O
n	O
--	O
+	O
0	O
(	O
since	O
by	O
assumption	O
nhd	O
--	O
+	O
(	O
0	O
)	O
.	O
therefore	O
,	O
if	O
n	O
is	O
sufficiently	O
large	O
,	O
then	O
for	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
10.1	O
)	O
we	O
have	O
e	O
{	O
i	O
iry	O
(	O
x	O
)	O
-	O
if	O
we	O
take	O
e	O
!	O
=	O
e/	O
(	O
4p	O
+	O
12	O
)	O
.	O
ryn	O
(	O
x	O
)	O
ii-	O
'	O
(	O
dx	O
)	O
}	O
<	O
e	O
'	O
(	O
p	O
+	O
3	O
)	O
=	O
e/4	O
it	O
remains	O
to	O
show	O
that	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
10.1	O
)	O
is	O
small	O
with	O
large	O
probability	O
.	O
to	O
do	O
this	O
,	O
we	O
use	O
mcdiarmid	O
's	O
inequality	B
(	O
theorem	B
9.2	O
)	O
for	O
i	O
iry	O
(	O
x	O
)	O
-	O
ryn	O
(	O
x	O
)	O
ll-	O
'	O
(	O
dx	O
)	O
e	O
{	O
i	O
iry	O
(	O
x	O
)	O
-	O
ryn	O
(	O
x	O
)	O
ii-	O
'	O
(	O
dx	O
)	O
}	O
.	O
fix	O
the	O
training	O
data	O
at	O
(	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
)	O
and	O
replace	O
the	O
i-th	O
pair	O
(	O
xi	O
,	O
yi	O
)	O
by	O
(	O
xi	O
,	O
)	O
it	O
)	O
,	O
changing	O
the	O
value	O
of	O
1	O
]	O
n	O
(	O
x	O
)	O
to	O
1	O
]	O
~i	O
(	O
x	O
)	O
,	O
clearly	O
,	O
by	O
the	O
covering	B
lemma	I
(	O
lemma	O
10.1	O
)	O
,	O
i	O
i1	O
]	O
(	O
x	O
)	O
-1	O
]	O
n	O
(	O
x	O
)	O
ifl	O
(	O
dx	O
)	O
-	O
i	O
i1	O
]	O
(	O
x	O
)	O
-	O
1	O
]	O
~i	O
(	O
x	O
)	O
lfl	O
(	O
dx	O
)	O
<	O
i	O
i1	O
]	O
n	O
(	O
x	O
)	O
-	O
1	O
]	O
~jx	O
)	O
ifl	O
(	O
dx	O
)	O
:	O
:	O
:	O
sup	O
i	O
2kh	O
(	O
x	O
-	O
y	O
)	O
fl	O
(	O
dx	O
)	O
yerd	O
nekh	O
(	O
x	O
-	O
x	O
)	O
<	O
2p	O
n	O
10.3	O
potential	O
function	O
rules	O
159	O
so	O
by	O
theorem	B
9.2	O
,	O
p	O
{	O
f	O
i~	O
(	O
x	O
)	O
-	O
~n	O
(	O
x	O
)	O
i	O
''	O
,	O
(	O
dx	O
)	O
>	O
i	O
}	O
<	O
p	O
{	O
f	O
i~	O
(	O
x	O
)	O
-	O
ryn	O
(	O
x	O
)	O
i	O
''	O
,	O
(	O
dx	O
)	O
-	O
e	O
{	O
f	O
iry	O
(	O
x	O
)	O
-	O
~n	O
(	O
x	O
)	O
i	O
''	O
'	O
(	O
dx	O
)	O
}	O
>	O
:	O
}	O
the	O
proof	O
is	O
now	O
completed	O
.	O
0	O
10.3	O
potential	O
function	O
rules	O
kernel	B
classification	O
rules	O
may	O
be	O
formulated	O
in	O
terms	O
of	O
the	O
so-called	O
potential	O
function	O
rules	O
.	O
these	O
rules	O
were	O
originally	O
introduced	O
and	O
studied	O
by	O
bashkirov	O
,	O
braverman	O
and	O
muchnik	O
(	O
1964	O
)	O
,	O
aizerman	O
,	O
braverman	O
and	O
rozonoer	O
(	O
1964c	O
;	O
1964b	O
;	O
1964a	O
;	O
1970	O
)	O
,	O
braverman	O
(	O
1965	O
)	O
,	O
and	O
braverman	O
and	O
pyatniskii	O
(	O
1966	O
)	O
.	O
the	O
original	O
idea	O
was	O
the	O
following	O
:	O
put	O
a	O
unit	O
of	O
positive	O
electrical	O
charge	O
at	O
every	O
data	O
point	O
xi	O
,	O
where	O
yi	O
=	O
1	O
,	O
and	O
a	O
unit	O
of	O
negative	O
charge	O
,	O
at	O
data	O
points	O
xi	O
where	O
yi	O
=	O
o.	O
the	O
resulting	O
potential	O
field	O
defines	O
an	O
intuitively	O
appealing	O
rule	B
:	O
the	O
decision	O
at	O
a	O
point	O
x	O
is	O
one	O
if	O
the	O
potential	O
at	O
that	O
point	O
is	O
positive	O
,	O
and	O
zero	O
if	O
it	O
is	O
negative	O
.	O
this	O
idea	O
leads	O
to	O
a	O
rule	O
that	O
can	O
be	O
generalized	B
to	O
obtain	O
rules	O
of	O
the	O
form	O
gn	O
(	O
x	O
)	O
=	O
{	O
if	O
!	O
n	O
(	O
x	O
)	O
:	O
:	O
:	O
;	O
0	O
0	O
i	O
otherwise	O
,	O
where	O
n	O
fn	O
(	O
x	O
)	O
=	O
l	O
rn	O
,	O
i	O
(	O
dn	O
)	O
kn	O
,	O
i	O
(	O
x	O
,	O
xi	O
)	O
,	O
i=l	O
where	O
the	O
kn/s	O
describe	O
the	O
potential	O
field	O
around	O
xi	O
,	O
and	O
the	O
rn/s	O
are	O
their	O
weights	O
.	O
rules	O
that	O
can	O
be	O
put	O
into	O
this	O
form	O
are	O
often	O
called	O
potential	O
function	O
rules	O
.	O
here	O
we	O
give	O
a	O
brief	O
survey	O
of	O
these	O
rules	O
.	O
kernel	B
rules	I
.	O
clearly	O
,	O
kernel	B
rules	I
studied	O
in	O
the	O
previous	O
section	O
are	O
potential	O
function	O
rules	O
with	O
kn	O
,	O
i	O
(	O
x	O
,	O
y	O
)	O
=	O
k	O
t	O
(	O
x	O
-	O
y	O
)	O
'	O
here	O
k	O
is	O
a	O
fixed	O
kernel	B
function	O
,	O
and	O
hi	O
,	O
h2	O
,	O
...	O
is	O
a	O
sequence	O
of	O
positive	O
numbers	O
.	O
histogram	O
rules	O
.	O
similarly	O
,	O
histogram	O
rules	O
(	O
see	O
chapters	O
6	O
and	O
9	O
)	O
can	O
be	O
put	O
in	O
this	O
form	O
,	O
by	O
choosing	O
160	O
10.	O
kernel	B
rules	I
and	O
rn	O
,	O
i	O
(	O
dn	O
)	O
=	O
2yi	O
-	O
1.	O
recall	O
that	O
an	O
(	O
x	O
)	O
denotes	O
the	O
cell	O
of	O
the	O
partition	B
in	O
which	O
x	O
falls	O
.	O
kn	O
,	O
jx	O
,	O
y	O
)	O
=	O
i	O
{	O
yean	O
(	O
x	O
)	O
}	O
,	O
polynomial	O
discriminant	O
functions	O
.	O
specht	O
(	O
1967	O
)	O
suggested	O
applying	O
a	O
poly	O
(	O
cid:173	O
)	O
nomial	O
expansion	O
to	O
the	O
kernel	B
k	O
c~y	O
)	O
.	O
this	O
led	O
to	O
the	O
choice	O
kn	O
,	O
i	O
(	O
x	O
,	O
y	O
)	O
=	O
l	O
v-r/x	O
)	O
'ljfj	O
(	O
y	O
)	O
,	O
k	O
j=l	O
and	O
rn	O
,	O
i	O
(	O
dn	O
)	O
=	O
2yi	O
-	O
1	O
,	O
where	O
v-r1	O
,	O
...	O
,	O
v-rk	O
are	O
fixed	O
real-valued	O
functions	O
on	O
nd	O
.	O
when	O
these	O
functions	O
are	O
polynomials	O
,	O
the	O
corresponding	O
classifier	B
gn	O
is	O
called	O
a	O
polynomial	O
discriminant	O
function	O
.	O
the	O
potential	B
function	I
rule	I
obtained	O
this	O
way	O
is	O
a	O
generalized	O
linear	O
rule	O
(	O
see	O
chapter	O
17	O
)	O
with	O
k	O
fn	O
(	O
x	O
)	O
=	O
l	O
an	O
,	O
j'ljlj	O
(	O
x	O
)	O
,	O
j=l	O
where	O
the	O
coefficients	O
an	O
,	O
}	O
depend	O
on	O
the	O
data	O
dn	O
only	O
,	O
through	O
n	O
an	O
,	O
j	O
=	O
i	O
)	O
2yi	O
-	O
i=l	O
l	O
)	O
v-rj	O
(	O
xi	O
)	O
.	O
this	O
choice	O
of	O
the	O
coefficients	O
does	O
not	O
necessarily	O
lead	O
to	O
a	O
consistent	O
rule	B
,	O
unless	O
the	O
functions	O
v-r1	O
,	O
...	O
,	O
v-rk	O
are	O
allowed	O
to	O
change	O
with	O
n	O
,	O
or	O
k	O
is	O
allowed	O
to	O
vary	O
with	O
n.	O
nevertheless	O
,	O
the	O
rule	B
has	O
some	O
computational	O
advantages	O
over	O
kernel	B
rules	I
.	O
in	O
many	O
practical	O
situations	O
there	O
is	O
enough	O
time	O
to	O
preprocess	O
the	O
data	O
dn	O
,	O
but	O
once	O
the	O
observation	O
x	O
becomes	O
known	O
,	O
the	O
decision	O
has	O
to	O
be	O
made	O
very	O
quickly	O
.	O
clearly	O
,	O
the	O
coefficients	O
an	O
,	O
1	O
,	O
...	O
,	O
an	O
,	O
n	O
can	O
be	O
computed	O
by	O
knowing	O
the	O
training	O
data	O
dn	O
only	O
,	O
and	O
if	O
the	O
values	O
v-r1	O
(	O
x	O
)	O
,	O
...	O
,	O
v-rk	O
(	O
x	O
)	O
are	O
easily	O
computable	O
,	O
then	O
fn	O
(	O
x	O
)	O
can	O
be	O
computed	O
much	O
more	O
quickly	O
than	O
in	O
a	O
kernel-based	O
decision	O
,	O
where	O
all	O
n	O
terms	O
of	O
the	O
sum	O
have	O
to	O
be	O
computed	O
in	O
real	O
time	O
,	O
if	O
no	O
preprocessing	O
is	O
done	O
.	O
however	O
,	O
using	O
preprocessing	O
of	O
the	O
data	O
may	O
also	O
help	O
with	O
kernel	O
rules	O
,	O
espe	O
(	O
cid:173	O
)	O
cially	O
when	O
d	O
=	O
1.	O
for	O
a	O
survey	O
of	O
computational	O
speed-up	O
with	O
kernel	O
methods	O
,	O
see	O
devroye	O
and	O
machell	O
(	O
1985	O
)	O
.	O
recursive	O
kernel	O
rules	O
.	O
consider	O
the	O
choice	O
kn	O
,	O
i	O
(	O
x	O
,	O
y	O
)	O
=k	O
~	O
,	O
(	O
x	O
-	O
y	O
)	O
(	O
10.3	O
)	O
observe	O
that	O
the	O
only	O
difference	O
between	O
this	O
and	O
the	O
ordinary	B
kernel	O
rule	B
is	O
that	O
in	O
the	O
expression	B
of	O
kn	O
,	O
i	O
,	O
the	O
smoothing	O
parameter	O
hn	O
is	O
replaced	O
with	O
hi	O
.	O
with	O
this	O
change	O
,	O
we	O
can	O
compute	O
the	O
rule	B
recursively	O
by	O
observing	O
that	O
fn+l	O
(	O
x	O
)	O
=	O
fn	O
(	O
x	O
)	O
+	O
(	O
2yn+1	O
-	O
l	O
)	O
k	O
(	O
x	O
-	O
xn+l	O
)	O
hn+l	O
.	O
problems	O
and	O
exercises	O
161	O
the	O
computational	O
advantage	O
of	O
this	O
rule	B
is	O
that	O
if	O
one	O
collects	O
additional	O
data	O
,	O
then	O
the	O
rule	B
does	O
not	O
have	O
to	O
be	O
entirely	O
recomputed	O
.	O
it	O
can	O
be	O
adjusted	O
using	O
the	O
formula	O
above	O
.	O
consistency	B
properties	O
of	O
this	O
rule	B
were	O
studied	O
by	O
devroye	O
and	O
wagner	O
(	O
1980b	O
)	O
,	O
krzyzak	O
and	O
pawlak	O
(	O
1984a	O
)	O
,	O
krzyzak	O
(	O
1986	O
)	O
,	O
and	O
greblicki	O
and	O
pawlak	O
(	O
1987	O
)	O
.	O
several	O
similar	O
recursive	O
kernel	O
rules	O
have	O
been	O
studied	O
in	O
the	O
literature	O
.	O
wolverton	O
and	O
wagner	O
(	O
1969b	O
)	O
,	O
greblicki	O
(	O
1974	O
)	O
,	O
and	O
krzyzak	O
and	O
pawlak	O
(	O
1983	O
)	O
,	O
studied	O
the	O
situation	O
when	O
(	O
x	O
-	O
y	O
)	O
kn	O
,	O
i	O
(	O
x	O
,	O
y	O
)	O
=	O
h1	O
k	O
t	O
(	O
10.4	O
)	O
1	O
the	O
corresponding	O
rule	B
can	O
be	O
computed	O
recursively	O
by	O
fn+l	O
(	O
x	O
)	O
=	O
fn	O
(	O
x	O
)	O
+	O
(	O
2yn+l	O
-	O
1	O
)	O
-d-	O
k	O
1	O
hn+l	O
(	O
x	O
-	O
xn+l	O
)	O
hn+l	O
.	O
motivated	O
by	O
stochastic	B
approximation	I
methods	O
(	O
see	O
chapter	O
17	O
)	O
,	O
revesz	O
(	O
1973	O
)	O
suggested	O
and	O
studied	O
the	O
rule	B
obtained	O
from	O
fn+l	O
(	O
x	O
)	O
=	O
fn	O
(	O
x	O
)	O
+	O
--	O
(	O
2yn+l	O
-	O
1	O
-	O
1	O
n	O
+	O
1	O
fn	O
(	O
x	O
)	O
)	O
-d-k	O
1	O
hn+l	O
(	O
x	O
-	O
xn+l	O
)	O
hn+l	O
.	O
a	O
similar	O
rule	B
was	O
studied	O
by	O
gyorfi	O
(	O
1981	O
)	O
:	O
fn+l	O
(	O
x	O
)	O
=	O
fn	O
(	O
x	O
)	O
+	O
--	O
(	O
2yn+l	O
-	O
1	O
-	O
1	O
n	O
+	O
1	O
fn	O
(	O
x	O
)	O
)	O
k	O
(	O
x	O
-	O
xn+l	O
)	O
hn+l	O
.	O
problem	O
10.1.	O
let	O
k	O
be	O
a	O
nonnegative	O
kernel	B
with	O
compact	O
support	B
on	O
[	O
-1	O
,	O
1	O
]	O
.	O
show	O
problems	O
and	O
exercises	O
that	O
for	O
some	O
distribution	B
,	O
h	O
-+	O
°	O
is	O
necessary	O
for	O
consistency	B
of	O
the	O
kernel	B
rule	I
.	O
to	O
this	O
end	O
,	O
consider	O
the	O
following	O
example	O
.	O
given	O
y	O
=	O
0	O
,	O
x	O
has	O
a	O
geometric	O
distribution	B
on	O
8,83,8	O
5	O
,	O
...	O
,	O
and	O
given	O
y	O
=	O
1	O
,	O
x	O
has	O
a	O
geometric	O
distribution	B
on	O
82,84	O
,	O
86	O
,	O
...	O
.	O
then	O
show	O
that	O
to	O
obtain	O
eln	O
-+	O
l	O
*	O
=	O
0	O
,	O
it	O
is	O
necessary	O
that	O
h	O
-+	O
0.	O
nd	O
with	O
density	O
f.	O
let	O
k	O
be	O
a	O
kernel	O
function	O
integrating	O
to	O
one	O
,	O
and	O
let	O
hn	O
>	O
°	O
be	O
a	O
problem	O
10.2.	O
kernel	B
density	I
estimation	I
.	O
let	O
xl	O
,	O
...	O
,	O
xn	O
be	O
i.i.d	O
.	O
random	O
variables	O
in	O
smoothing	O
factor	O
.	O
the	O
kernel	O
density	O
estimate	O
is	O
defined	O
by	O
1	O
~	O
(	O
x-x	O
.	O
)	O
fn	O
(	O
x	O
)	O
=	O
nh~	O
f	O
:	O
t	O
k	O
t	O
(	O
rosenblatt	O
(	O
1956	O
)	O
,	O
parzen	O
(	O
1962	O
)	O
)	O
.	O
prove	O
that	O
the	O
estimate	B
is	O
weakly	O
universally	O
consistent	O
in	O
ll	O
if	O
h	O
n	O
-+	O
°	O
and	O
nh~	O
-+	O
00	O
as	O
n	O
-+	O
00.	O
hint	O
:	O
proceed	O
as	O
in	O
problem	O
6.2	O
.	O
162	O
10.	O
kernel	B
rules	I
problem	O
10.3.	O
strong	B
consistency	I
of	O
kernel	B
density	I
estimation	I
.	O
let	O
xl	O
,	O
...	O
,	O
xn	O
be	O
i.i.d	O
.	O
random	O
variables	O
in	O
nd	O
with	O
density	O
f.	O
let	O
k	O
be	O
a	O
nonnegative	O
function	O
integrating	O
to	O
one	O
(	O
a	O
kernel	O
)	O
and	O
h	O
>	O
0	O
a	O
smoothing	O
factor	O
.	O
as	O
in	O
the	O
previous	O
exercise	O
,	O
the	O
kernel	O
density	O
estimate	O
is	O
defined	O
by	O
fn	O
(	O
x	O
)	O
=	O
/	O
;	O
d	O
~	O
k	O
(	O
-h	O
_	O
i	O
)	O
.	O
1	O
~	O
x-x	O
(	O
cid:173	O
)	O
n	O
i=	O
!	O
prove	O
for	O
the	O
ll-error	O
of	O
the	O
estimate	B
that	O
p	O
{	O
if	O
l.hl	O
(	O
x	O
)	O
-	O
f	O
(	O
x	O
)	O
ldx	O
-	O
e	O
f	O
ifn	O
(	O
x	O
)	O
-	O
f	O
(	O
x	O
)	O
'dxi	O
>	O
e	O
}	O
s	O
2e-ne2j2	O
(	O
devroye	O
(	O
1991a	O
)	O
)	O
.	O
conclude	O
that	O
weak	B
ll-consistency	O
of	O
the	O
estimate	B
implies	O
strong	B
con	O
(	O
cid:173	O
)	O
sistency	O
(	O
see	O
problem	O
10.2	O
)	O
.	O
this	O
is	O
a	O
way	O
to	O
show	O
that	O
weak	B
and	O
strong	B
ll-consistencies	O
of	O
the	O
kernel	O
density	O
estimate	O
are	O
equivalent	O
(	O
devroye	O
(	O
1983	O
)	O
.	O
)	O
also	O
,	O
for	O
d	O
=	O
1	O
,	O
since	O
if	O
k	O
is	O
nonnegative	O
,	O
then	O
e	O
f	O
ifn	O
(	O
x	O
)	O
-	O
f	O
(	O
x	O
)	O
ldx	O
can	O
not	O
converge	O
to	O
zero	O
faster	O
than	O
n-2	O
(	O
5	O
for	O
any	O
density	O
(	O
see	O
devroye	O
and	O
gyorfi	O
(	O
1985	O
)	O
)	O
,	O
therefore	O
,	O
the	O
inequality	B
above	O
implies	O
that	O
for	O
any	O
density	O
lim	O
f	O
ifn	O
(	O
x	O
)	O
-	O
n-+co	O
e	O
f	O
ifn	O
(	O
x	O
)	O
-	O
f	O
(	O
x	O
)	O
ldx	O
=	O
0	O
f	O
(	O
x	O
)	O
ldx	O
with	O
probability	O
one	O
(	O
devroye	O
(	O
1988d	O
)	O
)	O
.	O
this	O
property	O
is	O
called	O
the	O
relative	B
stability	I
of	O
the	O
l	O
i	O
error	O
.	O
it	O
means	O
that	O
the	O
asymptotic	O
behavior	O
of	O
the	O
l	O
i	O
-error	O
is	O
the	O
same	O
as	O
that	O
of	O
its	O
expected	O
value	O
.	O
hint	O
:	O
use	O
mcdiarmid	O
's	O
inequality	B
.	O
problem	O
10.4.	O
if	O
f	O
k	O
<	O
0	O
,	O
show	O
that	O
under	O
the	O
assumption	O
that	O
fj.	O
,	O
has	O
a	O
density	O
f	O
,	O
and	O
that	O
h	O
--	O
-	O
:	O
>	O
-	O
0	O
,	O
nhd	O
~	O
00	O
,	O
the	O
kernel	B
rule	I
has	O
lim	O
eln	O
=	O
e	O
{	O
max	O
(	O
1j	O
(	O
x	O
)	O
,	O
i	O
-1j	O
(	O
x	O
)	O
)	O
}	O
=	O
1	O
-	O
l	O
*	O
.	O
n-+co	O
thus	O
,	O
the	O
rule	B
makes	O
the	O
wrong	O
decisions	O
,	O
and	O
such	O
kernels	O
should	O
be	O
avoided	O
.	O
hint	O
:	O
you	O
may	O
use	O
the	O
fact	O
that	O
for	O
the	O
kernel	O
density	O
estimate	O
with	O
kernel	O
l	O
satisfying	O
f	O
l	O
=	O
1	O
,	O
f	O
ifn	O
(	O
x	O
)	O
-	O
f	O
(	O
x	O
)	O
ldx	O
--	O
-	O
:	O
>	O
-	O
0	O
with	O
probability	O
one	O
,	O
if	O
h	O
~	O
0	O
and	O
nhd	O
~	O
00	O
(	O
see	O
problems	O
10.2	O
and	O
10.3	O
)	O
.	O
problem	O
10.5.	O
consider	O
a	O
devilish	O
kernel	B
that	O
attaches	O
counterproductive	O
weight	O
to	O
the	O
origin	O
:	O
k	O
(	O
x	O
)	O
=	O
{	O
-i	O
~	O
if	O
iixll	O
s	O
1/3	O
if	O
1/3	O
<	O
ilxll	O
s	O
1	O
if	O
ilxll	O
>	O
1	O
.	O
--	O
-	O
:	O
>	O
-	O
00.	O
assume	O
that	O
l	O
*	O
=	O
o.	O
show	O
that	O
ln	O
--	O
-	O
:	O
>	O
-	O
0	O
with	O
assume	O
that	O
h	O
--	O
-	O
:	O
>	O
-	O
0	O
,	O
yet	O
nhd	O
probability	O
one	O
.	O
concession	O
:	O
if	O
you	O
find	O
that	O
you	O
ca	O
n't	O
handle	O
the	O
universality	O
,	O
try	O
first	O
proving	O
the	O
statement	O
for	O
strictly	B
separable	I
distributions	O
.	O
problem	O
10.6.	O
show	O
that	O
for	O
the	O
distribution	B
depicted	O
in	O
figure	O
10.2	O
,	O
the	O
kernel	B
rule	I
with	O
kernel	B
k	O
(	O
u	O
)	O
=	O
(	O
1-	O
u2	O
)	O
i	O
{	O
lul~l	O
)	O
is	O
consistent	O
whenever	O
h	O
,	O
the	O
smoothing	B
factor	I
,	O
remains	O
fixed	O
and	O
0	O
<	O
h	O
s	O
1/2	O
.	O
problem	O
10.7.	O
the	O
limit	O
for	O
fixed	O
h.	O
consider	O
a	O
kernel	O
rule	B
with	O
fixed	O
h	O
==	O
1	O
,	O
and	O
fixed	O
kernel	B
k.	O
find	O
a	O
simple	O
argument	O
that	O
proves	O
problems	O
and	O
exercises	O
163	O
lim	O
eln	O
=	O
looi	O
n-	O
'	O
>	O
oo	O
where	O
loo	O
is	O
the	O
probability	O
of	O
error	O
for	O
the	O
decision	O
goo	O
defined	O
by	O
if	O
e	O
{	O
k	O
(	O
x	O
-	O
x	O
)	O
(	O
21	O
]	O
(	O
x	O
)	O
-	O
if	O
e	O
{	O
k	O
(	O
x	O
-	O
x	O
)	O
(	O
21	O
]	O
(	O
x	O
)	O
-	O
l	O
)	O
}	O
:	O
s	O
°	O
i	O
)	O
}	O
>	O
0.	O
find	O
a	O
distribution	O
such	O
that	O
for	O
the	O
window	B
kernel	O
,	O
loo	O
=	O
1/2	O
,	O
yet	O
l	O
*	O
=	O
0.	O
is	O
there	O
such	O
a	O
distribution	O
for	O
any	O
kernel	B
?	O
hint	O
:	O
try	O
proving	O
a	O
convergence	O
result	O
at	O
each	O
x	O
by	O
invoking	O
the	O
law	O
of	O
large	O
numbers	O
,	O
and	O
then	O
replace	O
x	O
by	O
x.	O
problem	O
10.8.	O
show	O
that	O
the	O
conditions	O
h	O
n	O
-+	O
°	O
and	O
nhd	O
-+	O
00	O
of	O
theorem	O
10.1	O
are	O
not	O
necessary	O
for	O
consistency	B
,	O
that	O
is	O
,	O
exhibit	O
a	O
distribution	O
such	O
that	O
the	O
kernel	B
rule	I
is	O
consistent	O
with	O
hn	O
=	O
1	O
,	O
and	O
exhibit	O
another	O
distribution	B
for	O
which	O
the	O
kernel	B
rule	I
is	O
consistent	O
with	O
hn	O
rv	O
l/nljd	O
.	O
problem	O
10.9.	O
prove	O
that	O
the	O
conditions	O
hn	O
-+	O
0	O
and	O
nhd	O
-+	O
00	O
of	O
theorem	O
10.1	O
are	O
necessary	O
for	O
universal	B
consistency	I
,	O
that	O
is	O
,	O
show	O
that	O
if	O
one	O
of	O
these	O
conditions	O
are	O
violated	O
then	O
there	O
is	O
a	O
distribution	O
for	O
which	O
the	O
kernel	B
rule	I
is	O
not	O
consistent	O
(	O
krzyzak	O
(	O
1991	O
)	O
.	O
problem	O
10.10.	O
this	O
exercise	O
provides	O
an	O
argument	O
in	O
favor	O
of	O
monotonicity	O
of	O
the	O
kernel	B
k.	O
in	O
'r..2	O
,	O
find	O
a	O
nonatornic	O
distribution	B
for	O
(	O
x	O
,	O
y	O
)	O
,	O
and	O
a	O
positive	O
kernel	B
with	O
f	O
k	O
>	O
0	O
,	O
k	O
vanishing	O
off	O
so.o	O
for	O
some	O
8	O
>	O
0	O
,	O
such	O
that	O
for	O
all	O
h	O
>	O
0	O
,	O
and	O
all	O
n	O
,	O
the	O
kernel	B
rule	I
b	O
>	O
°	O
in	O
the	O
universal	B
consistency	I
theorem	O
can	O
not	O
be	O
abolished	O
altogether	O
.	O
has	O
eln	O
=	O
1/2	O
,	O
while	O
l*	O
=	O
o.	O
this	O
result	O
says	O
that	O
the	O
condition	O
k	O
(	O
x	O
)	O
~	O
bi	O
{	O
so	O
,	O
ol	O
for	O
some	O
problem	O
10.11.	O
with	O
k	O
as	O
in	O
the	O
previous	O
problem	O
,	O
and	O
taking	O
h	O
=	O
1	O
,	O
show	O
that	O
lim	O
eln	O
=	O
l	O
*	O
n-	O
'	O
>	O
oo	O
under	O
the	O
following	O
conditions	O
:	O
(	O
1	O
)	O
k	O
has	O
compact	O
support	B
vanishing	O
off	O
so	O
,	O
o	O
,	O
k	O
~	O
0	O
,	O
and	O
k	O
~	O
bi	O
{	O
so	O
''	O
l	O
for	O
some	O
e	O
>	O
o	O
.	O
(	O
2	O
)	O
we	O
say	O
that	O
we	O
have	O
agreement	B
on	O
sx	O
,	O
o	O
when	O
for	O
all	O
z	O
e	O
sx	O
,	O
o	O
,	O
either	O
1j	O
(	O
z	O
)	O
:	O
s	O
1/2	O
,	O
or	O
1	O
]	O
(	O
z	O
)	O
~	O
1/2	O
.	O
we	O
ask	O
that	O
p	O
{	O
agreement	B
on	O
sx	O
,	O
o	O
}	O
=	O
1.	O
problem	O
10.12.	O
the	O
previous	O
exercise	O
shows	O
that	O
at	O
points	O
where	O
there	O
is	O
agreement	B
,	O
we	O
make	O
asymptotically	O
the	O
correct	O
decision	O
with	O
kernels	O
with	O
fixed	O
smoothing	B
factor	I
.	O
let	O
d	O
be	O
the	O
set	O
{	O
x	O
:	O
1j	O
(	O
x	O
)	O
=	O
1/2	O
}	O
,	O
and	O
let	O
the	O
8-neighborhood	O
of	O
d	O
be	O
defined	O
by	O
do	O
=	O
{	O
y	O
:	O
ii	O
y	O
-	O
x	O
ii	O
:	O
s	O
8	O
for	O
some	O
xed	O
}	O
.	O
let	O
fl	O
be	O
the	O
probability	O
measure	B
for	O
x.	O
take	O
k	O
,	O
h	O
as	O
in	O
the	O
previous	O
exercise	O
.	O
noting	O
that	O
x	O
f	O
{	O
,	O
do	O
means	O
that	O
we	O
have	O
agreement	B
on	O
sx	O
,	O
o	O
,	O
show	O
that	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
,	O
lim	O
sup	O
eln	O
:	O
s	O
l	O
*	O
+	O
fl	O
(	O
do	O
)	O
.	O
n-+oo	O
164	O
10.	O
kernel	B
rules	I
figure	O
10.5	O
.	O
8-neighborhood	O
of	O
a	O
set	O
d.	O
problem	O
10.13.	O
continuation	O
.	O
clearly	O
,	O
fj.	O
,	O
(	O
d8	O
)	O
~	O
0	O
as	O
8	O
~	O
0	O
when	O
fj.	O
,	O
(	O
d	O
)	O
=	O
o.	O
convince	O
yourself	O
that	O
fj.	O
,	O
(	O
d	O
)	O
=	O
0	O
for	O
most	O
problems	O
.	O
if	O
you	O
knew	O
how	O
fast	O
fj.	O
,	O
(	O
d8	O
)	O
tended	O
to	O
zero	O
,	O
then	O
the	O
previous	O
exercise	O
would	O
enable	O
you	O
to	O
pick	O
h	O
as	O
a	O
function	O
of	O
n	O
such	O
that	O
h	O
~	O
0	O
and	O
such	O
that	O
the	O
upper	O
bound	O
for	O
eln	O
obtained	O
by	O
analogy	O
from	O
the	O
previous	O
exercise	O
is	O
approximately	O
minimal	O
.	O
if	O
in	O
n	O
d	O
,	O
d	O
is	O
the	O
surface	O
of	O
the	O
unit	O
ball	O
,	O
x	O
has	O
a	O
bounded	O
density	O
f	O
,	O
and	O
'f	O
}	O
is	O
lipschitz	O
,	O
determine	O
a	O
bound	O
for	O
fj.	O
,	O
(	O
d8	O
)	O
.	O
by	O
considering	O
the	O
proof	O
ofthe	O
universal	B
consistency	I
theorem	O
,	O
show	O
how	O
to	O
choose	O
h	O
such	O
that	O
problem	O
10.14.	O
extension	O
of	O
the	O
covering	B
lemma	I
(	O
lemma	O
10.1	O
)	O
to	O
regular	B
kernels	O
.	O
let	O
k	O
be	O
a	O
regular	O
kernel	B
,	O
and	O
let	O
fj.	O
,	O
be	O
an	O
arbitrary	O
probability	O
measure	B
.	O
prove	O
that	O
there	O
exists	O
a	O
finite	O
constant	O
p	O
=	O
p	O
(	O
k	O
)	O
only	O
depending	O
upon	O
k	O
such	O
that	O
for	O
any	O
y	O
and	O
h	O
(	O
devroye	O
and	O
krzyzak	O
(	O
1989	O
)	O
)	O
.	O
hint	O
:	O
prove	O
this	O
by	O
checking	O
the	O
following	O
:	O
(	O
1	O
)	O
first	O
take	O
a	O
bounded	O
overlap	O
cover	O
of	O
nd	O
with	O
translates	O
of	O
so	O
,	O
ri2	O
,	O
where	O
r	O
>	O
0	O
is	O
the	O
constant	O
appearing	O
in	O
the	O
definition	B
of	I
a	O
regular	B
kernel	O
.	O
this	O
cover	O
has	O
an	O
infinite	O
number	O
of	O
member	O
balls	O
,	O
but	O
every	O
x	O
gets	O
covered	O
at	O
most	O
k	O
1	O
times	O
where	O
k	O
[	O
depends	O
upon	O
d	O
only	O
.	O
(	O
2	O
)	O
the	O
centers	O
of	O
the	O
balls	O
are	O
called	O
xi	O
,	O
i	O
=	O
1	O
,	O
2	O
,	O
...	O
.	O
the	O
integral	O
condition	O
on	O
k	O
implies	O
that	O
l	O
sup	O
k	O
(	O
x	O
)	O
:	O
s	O
00	O
i=l	O
xexi+so	O
,	O
rj2	O
for	O
another	O
finite	O
constant	O
k2	O
•	O
(	O
3	O
)	O
show	O
that	O
k	O
j	O
iso	O
,	O
r/2	O
dx	O
1	O
sup	O
k	O
(	O
y	O
)	O
dx	O
:	O
s	O
k2	O
yex+so	O
,	O
r/2	O
kh	O
(	O
x	O
-	O
y	O
)	O
:	O
s	O
l	O
00	O
sup	O
kh	O
(	O
x	O
-	O
y	O
)	O
i	O
[	O
xey+hxi+	O
so	O
,	O
rh/2	O
]	O
,	O
i=l	O
xey+hxi+	O
so	O
,	O
rh/2	O
and	O
from	O
(	O
c	O
)	O
conclude	O
problems	O
and	O
exercises	O
165	O
<	O
l.	O
supzehxi+so	O
rh/2	O
kh	O
(	O
z	O
)	O
i=l	O
xey+hxi+so	O
,	O
rh/2	O
bfl	O
(	O
y	O
+	O
hxi	O
+	O
so	O
,	O
rh/2	O
)	O
~	O
1	O
f	O
fl	O
(	O
y	O
+	O
hxi	O
+	O
so	O
,	O
rh/2	O
)	O
supzehxi+so	O
,	O
rh/2	O
kh	O
(	O
z	O
)	O
'	O
fl	O
(	O
dx	O
)	O
bfl	O
(	O
y	O
+	O
hxi	O
+	O
so	O
,	O
rhj2	O
)	O
i=l	O
1	O
00	O
-l	O
sup	O
b	O
i=l	O
zehxi+so	O
,	O
rh/2	O
~	O
kh	O
(	O
z	O
)	O
:	O
:	O
:	O
:	O
-	O
,	O
b	O
where	O
k2	O
depends	O
on	O
k	O
and	O
d	O
only	O
.	O
problem	O
10.15.	O
let	O
k	O
be	O
a	O
regular	O
kernel	B
,	O
and	O
let	O
fl	O
be	O
an	O
arbitrary	O
probability	O
measure	B
.	O
prove	O
that	O
for	O
any	O
8	O
>	O
0	O
r	O
h~	O
s~p	O
f	O
kh	O
(	O
x	O
-	O
y	O
)	O
i	O
{	O
llx-yll	O
>	O
o	O
)	O
(	O
d	O
)	O
0	O
f	O
kh	O
(	O
x	O
_	O
z	O
)	O
fl	O
(	O
d	O
;	O
)	O
fl	O
x	O
=	O
.	O
hint	O
:	O
substitute	O
kh	O
(	O
z	O
)	O
in	O
the	O
proof	O
of	O
problem	O
10.15	O
by	O
kh	O
(	O
z	O
)	O
i	O
(	O
llzil	O
2	O
:	O
8	O
)	O
and	O
notice	O
that	O
f	O
kh	O
(	O
x	O
-	O
y	O
)	O
i	O
{	O
llx-yll	O
>	O
o	O
)	O
kh	O
(	O
x	O
-	O
z	O
)	O
fl	O
(	O
dz	O
)	O
f	O
sup	O
y	O
~	O
-	O
fl	O
(	O
dx	O
)	O
:	O
:	O
:	O
:	O
l.	O
i=l	O
zehxi+so	O
,	O
rh/2	O
sup	O
kh	O
(	O
z	O
)	O
i	O
(	O
lizll	O
2	O
:	O
8	O
)	O
-+	O
0	O
as	O
h	O
-+	O
o.	O
problem	O
10.16.	O
use	O
problems	O
10.14	O
and	O
10.15	O
to	O
extend	O
the	O
proof	O
of	O
theorem	O
10.1	O
for	O
arbitrary	O
regular	B
kernels	O
.	O
problem	O
10.17.	O
show	O
that	O
the	O
constant	O
f3d	O
in	O
lemma	O
10.1	O
is	O
never	O
more	O
than	O
4d	O
•	O
problem	O
10.18.	O
show	O
that	O
if	O
l	O
2	O
:	O
0	O
is	O
a	O
bounded	O
function	O
that	O
is	O
monotonically	O
decreasing	O
on	O
[	O
0	O
,	O
(	O
0	O
)	O
with	O
the	O
property	O
that	O
f	O
u	O
-	O
1	O
l	O
(	O
u	O
)	O
du	O
<	O
00	O
,	O
and	O
if	O
k	O
:	O
n	O
d	O
-+	O
[	O
0	O
,	O
(	O
0	O
)	O
is	O
a	O
function	O
with	O
k	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
l	O
(	O
lix	O
ii	O
)	O
,	O
then	O
k	O
is	O
regular	B
.	O
d	O
problem	O
10.19.	O
find	O
a	O
kernel	O
k	O
2	O
:	O
0	O
that	O
is	O
monotonically	O
decreasing	O
along	O
rays	O
(	O
i.e.	O
,	O
k	O
(	O
rx	O
)	O
:	O
:	O
:	O
:	O
k	O
(	O
x	O
)	O
for	O
all	O
x	O
e	O
n	O
d	O
and	O
all	O
r	O
2	O
:	O
1	O
)	O
such	O
that	O
k	O
is	O
not	O
regular	B
.	O
(	O
this	O
exercise	O
is	O
intended	O
to	O
convince	O
you	O
that	O
it	O
is	O
very	O
difficult	O
to	O
find	O
well-behaved	O
kernels	O
that	O
are	O
not	O
regular	B
.	O
)	O
problem	O
10.20.	O
let	O
k	O
(	O
x	O
)	O
=	O
l	O
(	O
llxll	O
)	O
for	O
some	O
bounded	O
function	O
l	O
2	O
:	O
o.	O
show	O
that	O
k	O
is	O
regular	B
if	O
l	O
is	O
decreasing	O
on	O
[	O
0	O
,	O
(	O
0	O
)	O
and	O
f	O
k	O
(	O
x	O
)	O
dx	O
<	O
00.	O
conclude	O
that	O
the	O
gaussian	B
and	O
cauchy	O
kernels	O
are	O
regular	B
.	O
problem	O
10.21.	O
regularity	O
of	O
the	O
kernel	B
is	O
not	O
necessary	O
for	O
universal	B
consistency	I
.	O
investi	O
(	O
cid:173	O
)	O
gate	O
universal	B
consistency	I
with	O
a	O
nonintegrable	O
kernel-that	O
is	O
,	O
for	O
which	O
f	O
k	O
(	O
x	O
)	O
dx	O
=	O
00-	O
such	O
as	O
k	O
(	O
x	O
)	O
=	O
1/	O
(	O
1	O
+	O
ixl	O
)	O
.	O
greblicki	O
,	O
krzyzak	O
,	O
and	O
pawlak	O
(	O
1984	O
)	O
proved	O
consistency	B
of	O
the	O
kernel	B
rule	I
with	O
smoothing	B
factor	I
hn	O
satisfying	O
hn	O
-+	O
0	O
and	O
nh~	O
-+	O
00	O
if	O
the	O
ker	O
(	O
cid:173	O
)	O
nel	O
k	O
satisfies	O
the	O
following	O
conditions	O
:	O
k	O
(	O
x	O
)	O
2	O
:	O
ci	O
{	O
lixli	O
:	O
:olj	O
for	O
some	O
c	O
>	O
0	O
and	O
for	O
some	O
cl	O
,	O
c2	O
>	O
0	O
,	O
c1h	O
(	O
llx	O
ii	O
)	O
:	O
:	O
:	O
:	O
k	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
c2h	O
(	O
llxll	O
)	O
,	O
where	O
h	O
is	O
a	O
nonincreasing	O
function	O
on	O
[	O
0	O
,	O
(	O
0	O
)	O
with	O
ud	O
h	O
(	O
u	O
)	O
-+	O
0	O
as	O
u	O
-+	O
00	O
.	O
166	O
10.	O
kernel	B
rules	I
problem	O
10.22.	O
consider	O
the	O
kernel	B
rule	I
with	O
kernel	B
k	O
(	O
x	O
)	O
=	O
l/llx	O
ilr	O
,	O
r	O
>	O
0.	O
such	O
kernels	O
are	O
useless	O
for	O
atomic	O
distributions	O
unless	O
we	O
take	O
limits	O
and	O
define	O
gn	O
as	O
usual	O
when	O
x	O
tj	O
.	O
s	O
,	O
the	O
collection	O
of	O
points	O
z	O
with	O
xi	O
=	O
x	O
)	O
=	O
z	O
for	O
some	O
pair	O
(	O
i	O
=i	O
j	O
)	O
.	O
for	O
xes	O
,	O
we	O
take	O
a	O
majority	O
vote	O
over	O
the	O
yi	O
's	O
for	O
which	O
xi	O
=	O
x.	O
discuss	O
the	O
weak	B
universal	O
consistency	B
of	O
this	O
rule	B
,	O
which	O
has	O
the	O
curious	O
property	O
that	O
gn	O
is	O
invariant	O
to	O
the	O
smoothing	B
factor	I
h-so	O
,	O
we	O
might	O
as	O
well	O
set	O
h	O
=	O
1	O
without	O
loss	O
of	O
generality	O
.	O
note	O
also	O
that	O
for	O
r	O
:	O
:	O
:	O
:	O
d	O
,	O
is	O
k	O
(	O
x	O
)	O
dx	O
=	O
00	O
,	O
and	O
for	O
r	O
:	O
:	O
:	O
;	O
d	O
,	O
isc	O
k	O
(	O
x	O
)	O
dx	O
=	O
00	O
,	O
where	O
so	O
,	O
i	O
is	O
the	O
unit	O
ball	O
of	O
nd	O
centered	O
at	O
the	O
origin	O
.	O
in	O
particular	O
,	O
if	O
r	O
:	O
:	O
:	O
;	O
d	O
,	O
by	O
considering	O
x	O
uniform	B
on	O
so	O
,	O
i	O
if	O
y	O
=	O
1	O
and	O
x	O
uniform	B
on	O
the	O
surface	O
of	O
so	O
,	O
i	O
if	O
y	O
=	O
0	O
,	O
show	O
that	O
even	O
though	O
l	O
*	O
=	O
0	O
,	O
the	O
probability	O
of	O
error	O
of	O
the	O
rule	B
may	O
tend	O
to	O
a	O
nonzero	O
limit	O
for	O
certain	O
values	O
of	O
p	O
{	O
y	O
=	O
i	O
}	O
.	O
hence	O
,	O
the	O
rule	B
is	O
not	O
universally	O
consistent	O
.	O
for	O
r	O
:	O
:	O
:	O
:	O
d	O
,	O
prove	O
or	O
disprove	O
the	O
weak	B
universal	O
consistency	B
,	O
noting	O
that	O
the	O
rules	O
'	O
decisions	O
are	O
by-and-iarge	O
based	O
on	O
the	O
few	O
nearest	O
neighbors	O
.	O
prove	O
the	O
rule	B
is	O
weakly	O
consistent	O
for	O
all	O
r	O
:	O
:	O
:	O
;	O
d	O
whenever	O
x	O
has	O
a	O
density	O
.	O
o	O
,	O
!	O
0,1	O
problem	O
10.23.	O
assume	O
that	O
the	O
class	O
densities	O
coincide	O
,	O
that	O
is	O
,	O
1o	O
(	O
x	O
)	O
=	O
ii	O
(	O
x	O
)	O
for	O
every	O
x	O
e	O
n	O
,	O
and	O
assume	O
p	O
=	O
p	O
{	O
y	O
=	O
1	O
}	O
>	O
1/2	O
.	O
show	O
that	O
the	O
expected	O
probability	O
of	O
error	O
of	O
the	O
kernel	B
rule	I
with	O
k	O
==	O
1	O
is	O
smaller	O
than	O
that	O
with	O
any	O
unimodal	O
regular	B
kernel	O
for	O
every	O
n	O
and	O
h	O
small	O
enough	O
.	O
exhibit	O
a	O
distribution	O
such	O
that	O
the	O
kernel	B
rule	I
with	O
a	O
symmetric	O
kernel	B
such	O
that	O
k	O
(	O
ix	O
i	O
)	O
is	O
monotone	O
increasing	O
has	O
smaller	O
expected	O
error	O
probability	O
than	O
that	O
with	O
any	O
unimodal	O
regular	B
kernel	O
.	O
problem	O
10.24.	O
scaling	O
.	O
assume	O
that	O
the	O
kernel	B
k	O
can	O
be	O
written	O
into	O
the	O
following	O
product	B
form	O
of	O
one-dimensional	O
kernels	O
:	O
k	O
(	O
x	O
)	O
=	O
k	O
(	O
x	O
(	O
j	O
)	O
,	O
...	O
,	O
xed	O
»	O
)	O
=	O
n	O
ki	O
(	O
x	O
(	O
i	O
»	O
)	O
.	O
d	O
i-i	O
assume	O
also	O
that	O
k	O
is	O
regular	B
.	O
one	O
can	O
use	O
different	O
smoothing	O
factors	O
along	O
the	O
different	O
coordinate	O
axes	O
to	O
define	O
a	O
kernel	O
rule	B
by	O
n	O
if	O
l	O
(	O
2	O
yi	O
-	O
1	O
)	O
t1	O
k	O
)	O
i-i	O
d	O
)	O
-1	O
(	O
x	O
(	O
j	O
)	O
-	O
x	O
(	O
j	O
)	O
)	O
,	O
i	O
:	O
:	O
:	O
;	O
°	O
h	O
in	O
otherwise	O
,	O
where	O
x~j	O
)	O
denotes	O
the	O
j	O
-th	O
component	O
of	O
xi	O
'	O
prove	O
that	O
gn	O
is	O
strongly	O
universally	O
consistent	O
if	O
hin	O
-+	O
°	O
for	O
all	O
i	O
=	O
1	O
,	O
...	O
,	O
d	O
,	O
and	O
nhlnh2n	O
'	O
''	O
hdn	O
-+	O
00.	O
problem	O
10.25.	O
let	O
k	O
:	O
(	O
0	O
,	O
(	O
0	O
)	O
--	O
-+	O
[	O
0	O
,	O
(	O
0	O
)	O
be	O
a	O
function	O
,	O
and	O
l	O
a	O
symmetric	O
positive	O
definite	O
d	O
x	O
d	O
matrix	O
.	O
for	O
x	O
e	O
n	O
d	O
define	O
k	O
'	O
(	O
x	O
)	O
=	O
k	O
(	O
xtlx	O
)	O
.	O
find	O
conditions	O
on	O
k	O
such	O
that	O
the	O
kernel	B
rule	I
with	O
kernel	B
k	O
'	O
is	O
universally	O
consistent	O
.	O
problem	O
10.26.	O
prove	O
that	O
the	O
recursive	B
kernel	I
rule	I
defined	O
by	O
(	O
10.4	O
)	O
is	O
strongly	O
universally	O
consistent	O
if	O
k	O
is	O
a	O
regular	O
kernel	B
,	O
hn	O
--	O
-+	O
0	O
,	O
and	O
nh~	O
--	O
-+	O
00	O
as	O
n	O
--	O
-+	O
00	O
(	O
krzyzak	O
and	O
pawlak	O
(	O
1983	O
»	O
.	O
problem	O
10.27.	O
show	O
that	O
the	O
recursive	B
kernel	I
rule	I
of	O
(	O
10.3	O
)	O
is	O
strongly	O
universally	O
con	O
(	O
cid:173	O
)	O
sistent	O
whenever	O
k	O
is	O
a	O
regular	O
kernel	B
.	O
limn	O
-	O
hxl	O
hn	O
=	O
0	O
,	O
and	O
l	O
:	O
:l	O
h~	O
=	O
00	O
(	O
greblicki	O
and	O
weaker	O
assumptions	O
on	O
the	O
kernel	B
.	O
they	O
assume	O
that	O
k	O
(	O
x	O
)	O
:	O
:	O
:	O
c	O
!	O
lllxll	O
:	O
:	O
:	O
:lj	O
for	O
some	O
c	O
>	O
°	O
and	O
pawlak	O
(	O
1987	O
»	O
.	O
note	O
:	O
greblicki	O
and	O
pawlak	O
(	O
1987	O
)	O
showed	O
convergence	O
under	O
significantly	O
tion	O
on	O
[	O
0	O
,	O
(	O
0	O
)	O
with	O
ud	O
h	O
(	O
u	O
)	O
--	O
-+	O
°	O
as	O
u	O
--	O
-+	O
00.	O
they	O
also	O
showed	O
that	O
under	O
the	O
additional	O
that	O
for	O
some	O
ci	O
,	O
c2	O
>	O
0	O
,	O
ci	O
h	O
(	O
lixll	O
)	O
:	O
:	O
:	O
;	O
k	O
(	O
x	O
)	O
:	O
:	O
:	O
;	O
c2h	O
(	O
llxll	O
)	O
,	O
where	O
h	O
is	O
a	O
nonincreasing	O
func	O
(	O
cid:173	O
)	O
assumption	O
f	O
k	O
(	O
x	O
)	O
dx	O
<	O
00	O
the	O
following	O
conditions	O
on	O
hn	O
are	O
necessary	O
and	O
sufficient	O
for	O
universal	B
consistency	I
:	O
problems	O
and	O
exercises	O
167	O
problem	O
10.28.	O
open-ended	O
problem	O
.	O
let	O
p	O
{	O
y	O
=	O
i	O
}	O
=	O
1/2	O
.	O
given	O
y	O
=	O
1	O
,	O
let	O
x	O
be	O
uni	O
(	O
cid:173	O
)	O
formly	O
distributed	O
on	O
[	O
0	O
,	O
1	O
]	O
.	O
given	O
y	O
=	O
0	O
,	O
let	O
x	O
be	O
atomic	O
on	O
the	O
rationals	O
with	O
the	O
following	O
distribution	B
:	O
let	O
x	O
=	O
v	O
/	O
w	O
,	O
where	O
v	O
and	O
ware	O
independent	O
identically	O
distributed	O
,	O
and	O
p	O
{	O
v	O
=	O
i	O
}	O
=	O
1/2i	O
,	O
i	O
:	O
:	O
:	O
:	O
1.	O
consider	O
the	O
kernel	B
rule	I
with	O
the	O
window	B
kernel	O
.	O
what	O
is	O
the	O
behavior	O
of	O
the	O
smoothing	B
factor	I
h~	O
that	O
minimizes	O
the	O
expected	O
probability	O
of	O
error	O
eln	O
?	O
11	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	B
rule	I
in	O
chapter	O
5	O
we	O
discuss	O
results	O
about	O
the	O
asymptotic	O
behavior	O
of	O
k-nearest	O
neighbor	O
classification	O
rules	O
,	O
where	O
the	O
value	O
of	O
k-the	O
number	O
of	O
neighbors	O
taken	O
into	O
account	O
at	O
the	O
decision-is	O
kept	O
at	O
a	O
fixed	O
number	O
as	O
the	O
size	O
of	O
the	O
training	O
data	O
n	O
increases	O
.	O
this	O
choice	O
leads	O
to	O
asymptotic	O
error	O
probabilities	O
smaller	O
than	O
2l	O
*	O
,	O
but	O
no	O
universal	B
consistency	I
.	O
in	O
chapter	O
6	O
we	O
showed	O
that	O
if	O
we	O
let	O
k	O
grow	O
to	O
infinity	O
as	O
n	O
--	O
+	O
00	O
such	O
that	O
k	O
/	O
n	O
--	O
+	O
0	O
,	O
then	O
the	O
resulting	O
rule	B
is	O
weakly	O
consistent	O
.	O
the	O
main	O
purpose	O
of	O
this	O
chapter	O
is	O
to	O
demonstrate	O
strong	B
consistency	I
,	O
and	O
to	O
discuss	O
various	O
versions	O
of	O
the	O
rule	B
.	O
we	O
are	O
not	O
concerned	O
here	O
with	O
the	O
data-based	B
choice	O
of	O
k-that	O
subject	O
deserves	O
a	O
chapter	O
of	O
its	O
own	O
(	O
chapter	O
26	O
)	O
.	O
we	O
are	O
also	O
not	O
tackling	O
the	O
problem	O
of	O
the	O
selection	B
of	O
a	O
suitable-even	O
data-based-metric	O
.	O
at	O
the	O
end	O
of	O
this	O
chapter	O
and	O
in	O
the	O
exercises	O
,	O
we	O
draw	O
the	O
attention	O
to	O
i-nearest	O
neighbor	O
relabeling	O
rules	O
,	O
which	O
combine	O
the	O
computational	O
comfort	O
of	O
the	O
i-nearest	O
neighbor	B
rule	I
with	O
the	O
asymptotic	O
performance	O
of	O
variable-k	O
nearest	B
neighbor	I
rules	I
.	O
consistency	B
of	O
k-nearest	O
neighbor	O
classification	O
,	O
and	O
corresponding	O
regression	O
and	O
density	B
estimation	I
has	O
been	O
studied	O
by	O
many	O
researchers	O
.	O
see	O
fix	O
and	O
hodges	O
(	O
1951	O
;	O
1952	O
)	O
,	O
cover	O
(	O
1968a	O
)	O
,	O
stone	O
(	O
1977	O
)	O
,	O
beck	O
(	O
1979	O
)	O
,	O
gyorfi	O
and	O
gyorfi	O
(	O
1975	O
)	O
,	O
devroye	O
(	O
1981a	O
;	O
1982b	O
)	O
,	O
collomb	O
(	O
1979	O
;	O
1980	O
;	O
1981	O
)	O
,	O
bickel	O
and	O
breiman	O
(	O
1983	O
)	O
,	O
mack	O
(	O
1981	O
)	O
,	O
stute	O
(	O
1984	O
)	O
,	O
devroye	O
and	O
gyorfi	O
(	O
1985	O
)	O
,	O
bhattacharya	O
and	O
mack	O
(	O
1987	O
)	O
,	O
zhao	O
(	O
1987	O
)	O
,	O
and	O
devroye	O
,	O
gyorfi	O
,	O
krzyzak	O
,	O
and	O
lugosi	O
(	O
1994	O
)	O
.	O
recall	O
the	O
definition	B
of	I
the	O
k-nearest	O
neighbor	B
rule	I
:	O
first	O
reorder	O
the	O
data	O
(	O
x	O
(	O
l	O
)	O
(	O
x	O
)	O
,	O
y	O
(	O
l	O
)	O
(	O
x	O
»	O
,	O
...	O
,	O
(	O
x	O
(	O
n	O
)	O
(	O
x	O
)	O
,	O
y	O
(	O
'	O
!	O
)	O
(	O
x	O
»	O
)	O
according	O
to	O
increasing	O
euclidean	O
distances	O
of	O
the	O
x	O
j	O
'	O
s	O
to	O
x.	O
in	O
other	O
words	O
,	O
x	O
co	O
(	O
x	O
)	O
is	O
the	O
i	O
-th	O
nearest	B
neighbor	I
of	O
x	O
among	O
the	O
points	O
xl	O
,	O
...	O
,	O
x	O
n	O
'	O
if	O
distance	B
ties	O
occur	O
,	O
170	O
11.	O
cons~stency	O
of	O
the	O
k-nearest	O
neighbor	B
rule	I
a	O
tie-breaking	O
strategy	O
must	O
be	O
defined	O
.	O
if	O
fjv	O
is	O
absolutely	O
continuous	O
with	O
respect	O
to	O
the	O
lebesgue	O
measure	B
,	O
that	O
is	O
,	O
it	O
has	O
a	O
density	O
,	O
then	O
no	O
ties	O
occur	O
with	O
probability	O
one	O
,	O
so	O
formally	O
we	O
break	O
ties	O
by	O
comparing	O
indices	O
.	O
however	O
,	O
for	O
general	O
fjv	O
,	O
the	O
problem	O
of	O
distance	O
ties	O
turns	O
out	O
to	O
be	O
important	O
,	O
and	O
its	O
solution	O
is	O
messy	O
.	O
the	O
issue	O
of	O
tie	O
breaking	O
becomes	O
important	O
when	O
one	O
is	O
concerned	O
with	O
convergence	O
of	O
ln	O
with	O
probability	O
one	O
.	O
for	O
weak	B
universal	O
consistency	B
,	O
it	O
suffices	O
to	O
break	O
ties	O
by	O
comparing	O
indices	O
.	O
the	O
k-nn	O
classification	O
rule	B
is	O
defined	O
as	O
gn	O
(	O
x	O
)	O
=	O
{	O
o	O
if	O
2:7=1	O
.	O
i	O
{	O
yu	O
)	O
(	O
x	O
)	O
=l	O
}	O
:	O
:	O
:	O
.	O
:	O
2:7=1	O
i	O
{	O
yci	O
)	O
(	O
x	O
)	O
=o	O
}	O
1	O
otherwise	O
.	O
in	O
other	O
words	O
,	O
gn	O
(	O
x	O
)	O
is	O
a	O
majority	O
vote	O
among	O
the	O
labels	O
of	O
the	O
k	O
nearest	O
neighbors	O
ofx	O
.	O
11.1	O
strong	B
consistency	I
in	O
this	O
section	O
we	O
prove	O
theorem	B
11.1.	O
we	O
assume	O
the	O
existence	O
of	O
a	O
density	O
for	O
fjv	O
,	O
so	O
that	O
we	O
can	O
avoid	O
messy	O
technicalities	O
necessary	O
to	O
handle	O
distance	B
ties	O
.	O
we	O
discuss	O
this	O
issue	O
briefly	O
in	O
the	O
next	O
section	O
.	O
the	O
following	O
result	O
implies	O
strong	B
consistency	I
whenever	O
x	O
has	O
an	O
absolutely	O
continuous	O
distribution	B
.	O
the	O
result	O
was	O
proved	O
by	O
devroye	O
and	O
gyorfi	O
(	O
1985	O
)	O
,	O
and	O
zhao	O
(	O
1987	O
)	O
.	O
the	O
proof	O
presented	O
here	O
basically	O
appears	O
in	O
devroye	O
,	O
gyorfi	O
,	O
krzyzak	O
,	O
and	O
lugosi	O
(	O
1994	O
)	O
,	O
where	O
strong	B
universal	I
consistency	I
is	O
proved	O
under	O
an	O
appropriate	O
tie-breaking	O
strategy	O
(	O
see	O
discussion	O
later	O
)	O
.	O
some	O
of	O
the	O
main	O
ideas	O
appeared	O
in	O
the	O
proof	O
of	O
the	O
strong	B
universal	I
consistency	I
of	O
the	O
regular	B
histogram	O
rule	B
(	O
theorem	B
9.4	O
)	O
.	O
theorem	B
11.1	O
.	O
(	O
devroye	O
and	O
gyorfi	O
(	O
1985	O
)	O
,	O
zhao	O
(	O
1987	O
)	O
)	O
.	O
assume	O
that	O
fjv	O
has	O
a	O
density	O
.	O
if	O
k	O
--	O
+	O
00	O
and	O
k	O
/	O
n	O
--	O
+	O
0	O
then	O
for	O
every	O
e	O
>	O
0	O
there	O
is	O
an	O
no	O
such	O
that	O
forn	O
>	O
no	O
p	O
{	O
ln	O
-	O
l*	O
>	O
e	O
}	O
:	O
:	O
:	O
.	O
:	O
2e-m:2	O
/	O
(	O
72yl	O
)	O
,	O
where	O
yd	O
is	O
the	O
minimal	O
number	O
of	O
cones	O
centered	O
at	O
the	O
origin	O
of	O
angle	O
jr	O
/6	O
that	O
cover	O
rd	O
.	O
(	O
for	O
the	O
definition	B
of	I
a	O
cone	O
,	O
see	O
chapter	O
5	O
.	O
)	O
thus	O
,	O
the	O
k-nn	O
rule	B
is	O
strongly	O
consistent	O
.	O
remark	O
.	O
at	O
first	O
glance	O
the	O
upper	O
bound	O
in	O
the	O
theorem	B
does	O
not	O
seem	O
to	O
depend	O
on	O
k.	O
it	O
is	O
no	O
that	O
depends	O
on	O
the	O
sequence	O
of	O
k	O
's	O
.	O
what	O
we	O
really	O
prove	O
is	O
the	O
following	O
:	O
for	O
every	O
e	O
>	O
0	O
there	O
exists	O
a	O
/30	O
e	O
(	O
0	O
,	O
1	O
)	O
such	O
that	O
for	O
any	O
/3	O
<	O
/30	O
there	O
is	O
an	O
no	O
such	O
that	O
if	O
n	O
>	O
no	O
,	O
k	O
>	O
1//3	O
,	O
and	O
kin	O
<	O
/3	O
,	O
the	O
exponential	B
inequality	O
holds	O
.	O
0	O
for	O
the	O
proof	O
we	O
need	O
a	O
generalization	O
of	O
lemma	O
5.3.	O
the	O
role	O
of	O
this	O
covering	B
lemma	I
is	O
analogous	O
to	O
that	O
of	O
lemma	O
10.1	O
in	O
the	O
proof	O
of	O
consistency	O
of	B
kernel	I
rules	I
.	O
11.1	O
strong	B
consistency	I
171	O
lemma	O
11.1	O
.	O
(	O
devroye	O
and	O
gyorfi	O
(	O
1985	O
)	O
)	O
.	O
let	O
ba	O
(	O
x	O
'	O
)	O
=	O
{	O
x	O
:	O
tl	O
(	O
sx	O
,	O
l	O
!	O
x-x	O
'll	O
)	O
:	O
:	O
:	O
;	O
a	O
}	O
.	O
then	O
for	O
all	O
x	O
'	O
e	O
rd	O
proof	O
.	O
for	O
x	O
e	O
rd	O
let	O
c	O
(	O
x	O
,	O
s	O
)	O
c	O
rd	O
be	O
a	O
cone	O
of	O
angle	O
jr	O
/6	O
centered	O
at	O
x.	O
the	O
cone	O
consists	O
of	O
all	O
y	O
with	O
the	O
property	O
that	O
either	O
y	O
=	O
x	O
or	O
angle	O
(	O
y	O
-	O
x	O
,	O
s	O
)	O
:	O
:	O
:	O
;	O
jr	O
/	O
6	O
,	O
where	O
s	O
is	O
a	O
fixed	O
direction	O
.	O
if	O
y	O
,	O
y	O
'	O
e	O
c	O
(	O
x	O
,	O
s	O
)	O
,	O
and	O
iix	O
-	O
yll	O
<	O
iix	O
-	O
y'li	O
,	O
then	O
\i	O
y	O
-	O
y	O
'	O
\i	O
<	O
\ix	O
-	O
y	O
'	O
ii·	O
this	O
follows	O
from	O
a	O
simple	O
geometric	B
argument	O
in	O
the	O
vector	O
space	O
spanned	O
by	O
x	O
,	O
y	O
and	O
y	O
'	O
(	O
see	O
the	O
proof	O
of	O
lemma	O
5.3	O
)	O
.	O
now	O
,	O
let	O
c	O
i	O
,	O
...	O
,	O
cyd	O
be	O
a	O
collection	O
of	O
cones	O
centered	O
at	O
x	O
with	O
different	O
central	O
direction	O
covering	B
rd	O
.	O
then	O
yd	O
tl	O
(	O
ba	O
(	O
x	O
'	O
)	O
:	O
:	O
:	O
;	O
l	O
tl	O
(	O
ci	O
n	O
ba	O
(	O
x	O
'	O
)	O
)	O
.	O
i=l	O
let	O
x*	O
e	O
ci	O
n	O
ba	O
(	O
x	O
'	O
)	O
.	O
then	O
by	O
the	O
property	O
of	O
the	O
cones	O
mentioned	O
above	O
we	O
have	O
fj..	O
,	O
(	O
ci	O
n	O
sx	O
l	O
,	O
iix'-x*	O
ii	O
n	O
ba	O
(	O
x	O
'	O
»	O
:	O
:	O
:	O
;	O
/-t	O
(	O
sx*	O
,	O
lix	O
'	O
-x*li	O
)	O
:	O
:	O
:	O
;	O
a	O
,	O
where	O
we	O
use	O
the	O
fact	O
that	O
x*	O
e	O
ba	O
(	O
x	O
'	O
)	O
.	O
since	O
x*	O
is	O
arbitrary	O
,	O
tl	O
(	O
ci	O
n	O
ba	O
(	O
x	O
'	O
)	O
:	O
s	O
a	O
,	O
which	O
completes	O
the	O
proof	O
of	O
the	O
lemma	O
.	O
0	O
an	O
immediate	O
consequence	O
of	O
the	O
lemma	O
is	O
that	O
the	O
number	O
of	O
points	O
among	O
x	O
i	O
,	O
...	O
,	O
xn	O
such	O
that	O
x	O
is	O
one	O
of	O
their	O
k	O
nearest	O
neighbors	O
is	O
not	O
more	O
than	O
a	O
constant	O
times	O
k.	O
corollary	O
11.1.	O
n	O
l1	O
{	O
xisamongtheknn'sof	O
xi	O
in	O
{	O
x1	O
,	O
...	O
,	O
xn	O
,	O
x	O
}	O
-	O
{	O
xd	O
}	O
:	O
:	O
:	O
;	O
kyd	O
.	O
i=l	O
proof	O
.	O
apply	O
lemma	O
11.1	O
with	O
a	O
=	O
k	O
/	O
n	O
and	O
let	O
tl	O
be	O
the	O
empirical	B
measure	I
tln	O
of	O
xl	O
,	O
...	O
,	O
xn	O
,	O
that	O
is	O
,	O
for	O
each	O
borel	O
set	O
a	O
~	O
r	O
d	O
,	O
tln	O
(	O
a	O
)	O
=	O
(	O
l/n	O
)	O
l7=1	O
i	O
{	O
xi	O
ea	O
}	O
.	O
0	O
proof	O
of	O
theorem	O
11.1.	O
since	O
the	O
decision	O
rule	O
gn	O
may	O
be	O
rewritten	O
as	O
(	O
x	O
)	O
=	O
{	O
o	O
gn	O
if1jn	O
(	O
x	O
)	O
:	O
:	O
:	O
;	O
1/2	O
1	O
otherwise	O
,	O
where	O
1jn	O
is	O
the	O
corresponding	O
regression	B
function	I
estimate	O
1	O
k	O
1	O
]	O
n	O
(	O
x	O
)	O
=	O
k	O
ly	O
(	O
i	O
)	O
(	O
x	O
)	O
,	O
i=l	O
172	O
11.	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	B
rule	I
the	O
statement	O
follows	O
from	O
theorem	B
2.2	O
if	O
we	O
show	O
that	O
for	O
sufficiently	O
large	O
n	O
p	O
{	O
f	O
i~	O
(	O
x	O
)	O
-	O
~n	O
(	O
x	O
)	O
lfl	O
(	O
dx	O
)	O
>	O
~	O
}	O
s	O
2e-n	O
''	O
/	O
(	O
72yj	O
)	O
define	O
pn	O
(	O
x	O
)	O
as	O
the	O
solution	O
of	O
the	O
equation	O
note	O
that	O
the	O
absolute	O
continuity	O
of	O
j-l	O
implies	O
that	O
the	O
solution	O
always	O
exists	O
.	O
(	O
this	O
is	O
the	O
only	O
point	O
in	O
the	O
proof	O
where	O
we	O
use	O
this	O
assumption	O
.	O
)	O
also	O
define	O
the	O
basis	O
of	O
the	O
proof	O
is	O
the	O
following	O
decomposition	O
:	O
11j	O
(	O
x	O
)	O
-	O
1jn	O
(	O
x	O
)	O
1	O
:	O
:	O
:	O
;	O
irjn	O
(	O
x	O
)	O
-	O
1j~	O
(	O
x	O
)	O
1	O
+	O
irj~	O
(	O
x	O
)	O
-	O
rj	O
(	O
x	O
)	O
l·	O
for	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
,	O
observe	O
that	O
denoting	O
rn	O
(	O
x	O
)	O
=	O
ii	O
x	O
(	O
k	O
)	O
(	O
x	O
)	O
xii	O
,	O
where	O
fi	O
;	O
;	O
is	O
defined	O
as	O
1j~	O
with	O
y	O
replaced	O
by	O
the	O
constant	O
random	O
variable	B
y	O
=	O
1	O
,	O
and	O
17	O
==	O
1	O
is	O
the	O
corresponding	O
regression	B
function	I
.	O
thus	O
,	O
11j	O
(	O
x	O
)	O
-	O
1jn	O
(	O
x	O
)	O
1	O
:	O
:	O
:	O
;	O
r~	O
(	O
x	O
)	O
-17	O
(	O
x	O
)	O
1	O
+	O
11j~	O
(	O
x	O
)	O
-	O
1j	O
(	O
x	O
)	O
l·	O
(	O
11.1	O
)	O
first	O
we	O
show	O
that	O
the	O
expected	O
values	O
of	O
the	O
integrals	O
of	O
both	O
terms	O
on	O
the	O
right	O
(	O
cid:173	O
)	O
hand	O
side	O
converge	O
to	O
zero	O
.	O
then	O
we	O
use	O
mcdiarmid	O
's	O
inequality	B
to	O
prove	O
that	O
both	O
terms	O
are	O
very	O
close	O
to	O
their	O
expected	O
values	O
with	O
large	O
probability	O
.	O
for	O
the	O
expected	O
value	O
of	O
the	O
first	O
term	O
on	O
the	O
right	O
-hand	O
side	O
of	O
(	O
11.1	O
)	O
,	O
using	O
the	O
cauchy-schwarz	O
inequality	B
,	O
we	O
have	O
e	O
f	O
11j~	O
(	O
x	O
)	O
-	O
1jn	O
(	O
x	O
)	O
ij-l	O
(	O
dx	O
)	O
<	O
e	O
f	O
iry	O
;	O
:	O
(	O
x	O
)	O
-17	O
(	O
x	O
)	O
ij-l	O
(	O
dx	O
)	O
<	O
f	O
je	O
{	O
117~	O
(	O
x	O
)	O
-17	O
(	O
x	O
)	O
1	O
2	O
}	O
j-l	O
(	O
dx	O
)	O
11.1	O
strong	B
consistency	I
173	O
''	O
2	O
n	O
var	O
{	O
l	O
{	O
xes	O
k	O
x	O
,	O
pn	O
x	O
(	O
)	O
}	O
}	O
fl	O
(	O
dx	O
)	O
k2	O
n/-i	O
(	O
sx	O
,	O
pn	O
(	O
x	O
)	O
/-i	O
(	O
dx	O
)	O
=	O
i	O
1	O
s	O
i	O
1	O
=	O
i	O
/	O
n	O
~fl	O
(	O
dx	O
)	O
=	O
~	O
'	O
k2	O
n	O
1	O
which	O
converges	O
to	O
zero	O
.	O
for	O
the	O
expected	O
value	O
of	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
11.1	O
)	O
,	O
note	O
that	O
in	O
the	O
proof	O
of	O
theorems	O
6.3	O
and	O
6.4	O
we	O
already	O
showed	O
that	O
lim	O
ei	O
17j	O
(	O
x	O
)	O
-	O
7jn	O
(	O
x	O
)	O
lfl	O
(	O
dx	O
)	O
=	O
o.	O
n-+oo	O
therefore	O
,	O
e	O
i	O
17j~	O
(	O
x	O
)	O
-	O
7j	O
(	O
x	O
)	O
i/-i	O
(	O
dx	O
)	O
s	O
e	O
i	O
17j~	O
(	O
x	O
)	O
-	O
7jn	O
(	O
x	O
)	O
lfl	O
(	O
dx	O
)	O
+	O
e	O
i	O
17j	O
(	O
x	O
)	O
-	O
77n	O
(	O
x	O
)	O
i/-i	O
(	O
dx	O
)	O
-+	O
o.	O
assume	O
now	O
that	O
n	O
is	O
so	O
large	O
that	O
e	O
i	O
i~	O
(	O
x	O
)	O
-17	O
(	O
x	O
)	O
i/-i	O
(	O
dx	O
)	O
+	O
e	O
i	O
17j~	O
(	O
x	O
)	O
-	O
7j	O
(	O
x	O
)	O
lfl	O
(	O
dx	O
)	O
<	O
~	O
.	O
then	O
,	O
by	O
(	O
11.1	O
)	O
,	O
we	O
have	O
(	O
11.2	O
)	O
:	O
:s	O
p	O
{	O
i	O
iry	O
;	O
(	O
x	O
)	O
-	O
+p	O
u	O
i~	O
(	O
x	O
)	O
-	O
ry	O
(	O
x	O
)	O
lfl	O
(	O
dx	O
)	O
-	O
e	O
i	O
i~~	O
(	O
x	O
)	O
-	O
i	O
)	O
(	O
x	O
)	O
lfl	O
(	O
dx	O
)	O
-	O
e	O
i	O
1ti	O
;	O
:	O
(	O
x	O
)	O
-	O
ry	O
(	O
x	O
)	O
lfl	O
(	O
dx	O
)	O
>	O
~	O
}	O
i	O
)	O
(	O
x	O
)	O
lfl	O
(	O
dx	O
)	O
>	O
~	O
}	O
.	O
next	O
we	O
get	O
an	O
exponential	B
bound	O
for	O
the	O
first	O
probability	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
11.2	O
)	O
by	O
mcdiarmid	O
's	O
inequality	B
(	O
theorem	B
9.2	O
)	O
.	O
fix	O
an	O
arbitrary	O
realization	O
of	O
the	O
data	O
dn	O
=	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
,	O
and	O
replace	O
(	O
xi	O
,	O
yj	O
by	O
(	O
xi	O
,	O
yi	O
)	O
,	O
changing	O
the	O
value	O
of	O
7j~	O
(	O
x	O
)	O
to	O
7j~i	O
(	O
x	O
)	O
,	O
then	O
174	O
11.	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	B
rule	I
but	O
11j~	O
(	O
x	O
)	O
-	O
1j~/x	O
)	O
1	O
is	O
bounded	O
by	O
2/	O
k	O
and	O
can	O
differ	O
from	O
zero	O
only	O
if	O
iix	O
-	O
xi	O
ii	O
<	O
pn	O
(	O
x	O
)	O
or	O
iix	O
-	O
xi	O
ii	O
<	O
pn	O
(	O
x	O
)	O
.	O
observe	O
that	O
iix	O
-	O
xiii	O
<	O
pn	O
(	O
x	O
)	O
if	O
and	O
only	O
if	O
f.l	O
(	O
sx	O
,	O
lix-x	O
;	O
lj	O
)	O
<	O
kin	O
.	O
but	O
the	O
measure	B
of	O
such	O
x	O
's	O
is	O
bounded	O
by	O
ydkln	O
by	O
lemma	O
11.1.	O
therefore	O
and	O
by	O
theorem	B
9.2	O
finally	O
,	O
we	O
need	O
a	O
bound	O
for	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
11.2	O
)	O
.	O
this	O
probability	O
may	O
be	O
bounded	O
by	O
mcdiarmid	O
's	O
inequality	B
exactly	O
the	O
same	O
way	O
as	O
for	O
the	O
first	O
term	O
,	O
obtaining	O
and	O
the	O
proof	O
is	O
completed	O
.	O
0	O
remark	O
.	O
the	O
conditions	O
k	O
-+	O
00	O
and	O
kin	O
-+	O
0	O
are	O
optimal	O
in	O
the	O
sense	O
that	O
they	O
are	O
also	O
necessary	O
for	O
consistency	B
for	O
some	O
distributions	O
with	O
a	O
density	O
.	O
however	O
,	O
for	O
some	O
distributions	O
they	O
are	O
not	O
necessary	O
for	O
consistency	B
,	O
and	O
in	O
fact	O
,	O
keeping	O
k	O
=	O
1	O
for	O
all	O
n	O
may	O
be	O
a	O
better	O
choice	O
.	O
this	O
latter	O
property	O
,	O
dealt	O
with	O
in	O
problem	O
11.1	O
,	O
shows	O
that	O
the	O
i-nearest	O
neighbor	B
rule	I
is	O
admissible	O
.	O
0	O
11.2	O
breaking	O
distance	B
ties	O
theorem	B
11.1	O
provides	O
strong	B
consistency	I
under	O
the	O
assumption	O
that	O
x	O
has	O
a	O
density	O
.	O
this	O
assumption	O
was	O
needed	O
to	O
avoid	O
problems	O
caused	O
by	O
equal	O
distances	O
.	O
turning	O
to	O
the	O
general	O
case	O
,	O
we	O
see	O
that	O
if	O
f.l	O
does	O
not	O
have	O
a	O
density	O
then	O
distance	B
ties	O
can	O
occur	O
with	O
nonzero	O
probability	O
,	O
so	O
we	O
have	O
to	O
deal	O
with	O
the	O
problem	O
of	O
breaking	O
them	O
.	O
to	O
see	O
that	O
the	O
density	O
assumption	O
can	O
not	O
be	O
relaxed	O
to	O
the	O
condition	O
that	O
f.l	O
is	O
merely	O
nonatomic	O
without	O
facing	O
frequent	O
distance	B
ties	O
,	O
consider	O
the	O
following	O
distribution	B
on	O
n	O
d	O
x	O
n	O
d	O
'	O
with	O
d	O
,	O
d	O
'	O
:	O
:	O
:	O
:	O
2	O
:	O
f.l	O
=	O
-	O
(	O
td	O
x	O
ad	O
'	O
)	O
+	O
-	O
(	O
ad	O
x	O
td	O
'	O
)	O
,	O
1	O
2	O
1	O
2	O
where	O
td	O
denotes	O
the	O
uniform	B
distribution	O
on	O
the	O
surface	O
of	O
the	O
unit	O
sphere	B
of	O
n	O
d	O
and	O
ad	O
denotes	O
the	O
unit	O
point	O
mass	O
at	O
the	O
origin	O
of	O
nd	O
.	O
observe	O
that	O
if	O
x	O
has	O
distribution	B
td	O
x	O
ad	O
'	O
and	O
x	O
'	O
has	O
distribution	B
ad	O
x	O
td	O
'	O
,	O
then	O
ilx	O
x'ii	O
=	O
ji	O
.	O
11.2	O
breaking	O
distance	B
ties	O
175	O
hence	O
,	O
if	O
xl	O
,	O
x	O
2	O
,	O
x	O
3	O
,	O
x4	O
are	O
independent	O
with	O
distribution	O
j-l	O
,	O
thenp	O
{	O
iix	O
i	O
-	O
x2	O
11	O
=	O
iix3	O
-	O
x311	O
}	O
=	O
1/4	O
.	O
next	O
we	O
list	O
some	O
methods	O
of	O
breaking	O
distance	B
ties	O
.	O
•	O
ne-breaking	O
by	O
indices	O
:	O
if	O
xi	O
and	O
xj	O
are	O
equidistant	O
from	O
x	O
,	O
then	O
xi	O
is	O
declared	O
closer	O
if	O
i	O
<	O
j.	O
this	O
method	O
has	O
some	O
undesirable	O
properties	O
.	O
for	O
example	O
,	O
if	O
x	O
is	O
monoatomic	O
,	O
with	O
'f	O
}	O
<	O
1/2	O
,	O
then	O
x	O
1	O
is	O
the	O
nearest	B
neighbor	I
of	O
all	O
x	O
j	O
's	O
,	O
j	O
>	O
1	O
,	O
but	O
x	O
j	O
is	O
only	O
the	O
j	O
-	O
i-st	O
nearest	B
neighbor	I
of	O
x	O
1.	O
the	O
influence	O
of	O
x	O
1	O
in	O
such	O
a	O
situation	O
is	O
too	O
large	O
,	O
making	O
the	O
estimate	B
very	O
unstable	O
and	O
thus	O
undesirable	O
.	O
in	O
fact	O
,	O
in	O
this	O
monoatomic	O
case	O
,	O
if	O
l*	O
+	O
e	O
<	O
1/2	O
,	O
p	O
{	O
ln	O
-	O
l	O
*	O
>	O
e	O
}	O
~	O
e	O
-ck	O
for	O
some	O
c	O
>	O
0	O
(	O
see	O
problem	O
11.2	O
)	O
.	O
thus	O
,	O
we	O
can	O
not	O
expect	O
a	O
distribution	O
(	O
cid:173	O
)	O
free	O
version	O
of	O
theorem	O
11.1	O
with	O
this	O
tie-breaking	O
method	O
.	O
•	O
stone	O
's	O
tie-breaking	O
:	O
stone	O
(	O
1977	O
)	O
introduced	O
a	O
version	O
of	O
the	O
nearest	B
neighbor	I
rule	I
,	O
where	O
the	O
labels	O
of	O
the	O
points	O
having	O
the	O
same	O
distance	B
from	O
x	O
as	O
the	O
k-th	O
nearest	B
neighbor	I
are	O
averaged	O
.	O
if	O
we	O
denote	O
the	O
distance	B
of	O
the	O
k-th	O
nearest	B
neighbor	I
to	O
x	O
by	O
rn	O
(	O
x	O
)	O
,	O
then	O
stone	O
's	O
rule	B
is	O
the	O
following	O
:	O
o	O
'f	O
l	O
i	O
{	O
i	O
:	O
lix	O
-	O
xi	O
ii	O
<	O
rn	O
(	O
x	O
)	O
}	O
1	O
'	O
``	O
~	O
i	O
{	O
yi=o	O
}	O
+	O
--	O
.	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
i	O
{	O
l	O
:	O
ilx	O
-	O
xi	O
ii	O
=	O
rn	O
(	O
x	O
)	O
}	O
1	O
''	O
11	O
-	O
·11	O
r	O
(	O
)	O
1.	O
x	O
xl	O
<	O
n	O
x	O
k	O
-	O
gn	O
(	O
x	O
)	O
=	O
k	O
x	O
~	O
{	O
yi=o	O
}	O
~	O
2	O
'	O
``	O
i	O
i	O
:	O
llx-xdl=r	O
,	O
,	O
(	O
x	O
)	O
1	O
otherwise	O
.	O
this	O
is	O
not	O
a	O
k-nearest	O
neighbor	B
rule	I
in	O
a	O
strict	O
sense	O
,	O
since	O
this	O
estimate	B
,	O
in	O
general	O
,	O
uses	O
more	O
than	O
k	O
neighbors	O
.	O
stone	O
(	O
1977	O
)	O
proved	O
weak	B
universal	O
consistency	B
of	O
this	O
rule	B
.	O
•	O
adding	O
a	O
random	O
component	O
:	O
to	O
circumvent	O
the	O
aforementioned	O
difficul	O
(	O
cid:173	O
)	O
ties	O
,	O
we	O
may	O
artificially	O
increase	O
the	O
dimension	B
of	O
the	O
feature	O
vector	O
by	O
one	O
.	O
define	O
the	O
the	O
d	O
+	O
i-dimensional	O
random	O
vectors	O
where	O
the	O
randomizing	O
variables	O
v	O
,	O
vi	O
,	O
...	O
,	O
vn	O
are	O
real-valued	O
i.i.d	O
.	O
ran	O
(	O
cid:173	O
)	O
dom	O
variables	O
independent	O
of	O
x	O
,	O
y	O
,	O
and	O
dn	O
,	O
and	O
their	O
common	O
distribution	B
has	O
a	O
density	O
.	O
clearly	O
,	O
because	O
of	O
the	O
independence	O
of	O
v	O
,	O
the	O
bayes	O
error	O
corresponding	O
to	O
the	O
pair	O
(	O
x	O
'	O
,	O
y	O
)	O
is	O
the	O
same	O
as	O
that	O
of	O
(	O
x	O
,	O
y	O
)	O
.	O
the	O
algorithm	B
performs	O
the	O
k-nearest	O
neighbor	B
rule	I
on	O
the	O
modified	O
data	O
set	O
d~	O
=	O
«	O
x~	O
,	O
yd	O
,	O
...	O
,	O
(	O
x~	O
,	O
yn	O
)	O
)	O
.	O
it	O
finds	O
the	O
k	O
nearest	O
neighbors	O
of	O
x	O
'	O
,	O
and	O
uses	O
a	O
majority	O
vote	O
among	O
these	O
labels	O
to	O
guess	O
y.	O
since	O
v	O
has	O
a	O
density	O
and	O
is	O
independent	O
of	O
x	O
,	O
distance	B
176	O
11.	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	B
rule	I
ties	O
occur	O
with	O
zero	O
probability	O
.	O
strong	B
universal	I
consistency	I
of	O
this	O
rule	B
can	O
be	O
seen	O
by	O
observing	O
that	O
the	O
proof	O
of	O
theorem	O
11.1	O
used	O
the	O
existence	O
of	O
the	O
density	O
in	O
the	O
definition	B
of	I
pn	O
(	O
x	O
)	O
only	O
.	O
with	O
our	O
randomization	O
,	O
pn	O
(	O
x	O
)	O
is	O
well-defined	O
,	O
and	O
the	O
same	O
proof	O
yields	O
strong	B
universal	I
consistency	I
.	O
inter	O
(	O
cid:173	O
)	O
estingly	O
,	O
this	O
rule	B
is	O
consistent	O
whenever	O
u	O
has	O
a	O
density	O
and	O
is	O
independent	O
of	O
(	O
x	O
,	O
y	O
)	O
.	O
if	O
,	O
for	O
example	O
,	O
the	O
magnitude	O
of	O
u	O
is	O
much	O
larger	O
than	O
that	O
of	O
ii	O
x	O
ii	O
,	O
then	O
the	O
rule	B
defined	O
this	O
way	O
will	O
significantly	O
differ	O
from	O
the	O
k	O
(	O
cid:173	O
)	O
nearest	B
neighbor	I
rule	I
,	O
though	O
it	O
still	O
preserves	O
universal	B
consistency	I
.	O
one	O
should	O
expect	O
however	O
a	O
dramatic	O
decrease	O
in	O
the	O
performance	O
.	O
of	O
course	O
,	O
if	O
u	O
is	O
very	O
small	O
,	O
then	O
the	O
rule	B
remains	O
intuitively	O
appealing	O
.	O
•	O
tie-breaking	O
by	O
randomization	O
:	O
there	O
is	O
another	O
,	O
perhaps	O
more	O
natural	O
way	O
of	O
breaking	O
ties	O
via	O
randomization	O
.	O
we	O
assume	O
that	O
(	O
x	O
,	O
u	O
)	O
is	O
a	O
random	O
vector	O
independent	O
of	O
the	O
data	O
,	O
where	O
u	O
is	O
independent	O
of	O
x	O
and	O
uniformly	O
distributed	O
on	O
[	O
0	O
,	O
1	O
]	O
.	O
we	O
also	O
artificially	O
enlarge	O
the	O
data	O
by	O
introducing	O
u1	O
,	O
u2	O
,	O
.	O
'	O
''	O
un	O
,	O
where	O
the	O
u/s	O
are	O
i.i.d	O
.	O
uniform	B
[	O
0,1	O
]	O
as	O
well	O
.	O
thus	O
,	O
each	O
(	O
xi	O
,	O
ui	O
)	O
is	O
distributed	O
as	O
(	O
x	O
,	O
u	O
)	O
.	O
let	O
(	O
x	O
(	O
1	O
)	O
(	O
x	O
,	O
u	O
)	O
,	O
ycl	O
)	O
(	O
x	O
,	O
u	O
)	O
)	O
,	O
``	O
.	O
,	O
(	O
xcn	O
)	O
(	O
x	O
,	O
u	O
)	O
,	O
ycn	O
)	O
(	O
x	O
,	O
u	O
)	O
)	O
be	O
a	O
reordering	O
of	O
the	O
data	O
according	O
to	O
increasing	O
values	O
of	O
ilx	O
-	O
xi	O
ii	O
.	O
in	O
case	O
of	O
distance	O
ties	O
,	O
we	O
declare	O
(	O
xi	O
,	O
ui	O
)	O
closer	O
to	O
(	O
x	O
,	O
u	O
)	O
than	O
(	O
x	O
j	O
,	O
uj	O
)	O
provided	O
that	O
lui	O
-	O
ul	O
:	O
:	O
:	O
juj	O
-	O
uj	O
.	O
define	O
the	O
k-nn	O
classification	O
rule	B
as	O
gn	O
(	O
x	O
)	O
=	O
{	O
o	O
if	O
l~=l	O
.	O
i	O
{	O
yci	O
)	O
cx	O
,	O
u	O
)	O
=l	O
}	O
:	O
:	O
:	O
;	O
l~=l	O
i	O
{	O
y	O
(	O
i	O
)	O
cx	O
,	O
u	O
)	O
=o	O
}	O
1	O
otherwise	O
,	O
(	O
11.3	O
)	O
and	O
denote	O
the	O
error	O
probability	O
of	O
gn	O
by	O
devroye	O
,	O
gyorfi	O
,	O
krzyzak	O
,	O
and	O
lugosi	O
(	O
1994	O
)	O
proved	O
that	O
ln	O
--	O
-+	O
l	O
*	O
with	O
probability	O
one	O
for	O
all	O
distributions	O
if	O
k	O
--	O
-+	O
00	O
and	O
k	O
/	O
n	O
--	O
-+	O
0.	O
the	O
basic	O
argument	O
in	O
(	O
1994	O
)	O
is	O
the	O
same	O
as	O
that	O
of	O
theorem	O
11.1	O
,	O
except	O
that	O
the	O
covering	B
lemma	I
(	O
lemma	O
11.1	O
)	O
has	O
to	O
be	O
appropriately	O
modified	O
.	O
it	O
should	O
be	O
stressed	O
again	O
that	O
if	O
fl	O
has	O
a	O
density	O
,	O
or	O
just	O
has	O
an	O
absolutely	O
con	O
(	O
cid:173	O
)	O
tinuous	O
component	O
,	O
then	O
tie-breaking	O
is	O
needed	O
with	O
zero	O
probability	O
,	O
and	O
becomes	O
therefore	O
irrelevant	O
.	O
11.3	O
recursive	B
methods	O
to	O
find	O
the	O
nearest	B
neighbor	I
of	O
a	O
point	O
x	O
among	O
xl	O
,	O
...	O
,	O
x	O
n	O
,	O
we	O
may	O
preprocess	O
the	O
data	O
in	O
0	O
(	O
n	O
log	O
n	O
)	O
time	O
,	O
such	O
that	O
each	O
query	O
may	O
be	O
answered	O
in	O
0	O
(	O
log	O
n	O
)	O
11.4	O
scale-invariant	O
rules	O
177	O
worst-case	O
time-see	O
,	O
for	O
example	O
,	O
preparata	O
and	O
shamos	O
(	O
1985	O
)	O
.	O
other	O
recent	O
developments	O
in	O
computational	O
geometry	O
have	O
made	O
the	O
nearest	B
neighbor	I
rules	I
computationally	O
feasible	O
even	O
when	O
n	O
is	O
formidable	O
.	O
without	O
preprocessing	O
how	O
(	O
cid:173	O
)	O
ever	O
,	O
one	O
must	O
resort	O
to	O
slow	O
methods	O
.	O
if	O
we	O
need	O
to	O
find	O
a	O
decision	O
at	O
x	O
and	O
want	O
to	O
process	O
the	O
data	O
file	O
once	O
when	O
doing	O
so	O
,	O
a	O
simple	O
rule	B
was	O
proposed	O
by	O
de	O
(	O
cid:173	O
)	O
vroye	O
and	O
wise	O
(	O
1980	O
)	O
.	O
it	O
is	O
a	O
fully	O
recursive	B
rule	O
that	O
may	O
be	O
updated	O
as	O
more	O
observations	O
become	O
available	O
.	O
split	O
the	O
data	O
sequence	O
dn	O
into	O
disjoint	O
blocks	O
of	O
length	O
ii	O
,	O
...	O
,	O
in	O
,	O
where	O
ii	O
,	O
...	O
,	O
in	O
are	O
positive	O
integers	O
satisfying	O
l~1	O
ii	O
=	O
n.	O
in	O
each	O
block	O
find	O
the	O
nearest	B
neighbor	I
of	O
x	O
,	O
and	O
denote	O
the	O
nearest	B
neighbor	I
of	O
x	O
from	O
the	O
i-th	O
block	O
by	O
x7	O
(	O
x	O
)	O
.	O
let	O
yt	O
(	O
x	O
)	O
be	O
the	O
corresponding	O
label	O
.	O
ties	O
are	O
broken	O
by	O
comparing	O
indices	O
.	O
the	O
classification	O
rule	B
is	O
defined	O
as	O
a	O
majority	O
vote	O
among	O
the	O
nearest	O
neighbors	O
from	O
each	O
block	O
:	O
note	O
that	O
we	O
have	O
only	O
defined	O
the	O
rule	B
gn	O
for	O
n	O
satisfying	O
l~l	O
ii	O
=	O
n	O
for	O
some	O
n.	O
a	O
possible	O
extension	O
for	O
all	O
n	O
's	O
is	O
given	O
by	O
gn	O
(	O
x	O
)	O
=	O
gm	O
(	O
x	O
)	O
,	O
where	O
m	O
is	O
the	O
largest	O
integer	O
not	O
exceeding	O
n	O
that	O
can	O
be	O
written	O
as	O
l~l	O
ii	O
for	O
some	O
n.	O
the	O
rule	B
is	O
weakly	O
universally	O
consistent	O
if	O
lim	O
in	O
=	O
00	O
n	O
--	O
-+oo	O
(	O
devroye	O
and	O
wise	O
(	O
1980	O
)	O
,	O
see	O
problem	O
11.3	O
)	O
.	O
11.4	O
scale-invariant	O
rules	O
a	O
scale-invariant	O
rule	B
is	O
a	O
rule	O
that	O
is	O
invariant	O
under	O
rescalings	O
of	O
the	O
components	O
.	O
it	O
is	O
motivated	O
by	O
the	O
lack	O
of	O
a	O
universal	O
yardstick	O
when	O
components	O
of	O
a	O
vec	O
(	O
cid:173	O
)	O
tor	O
represent	O
physically	O
different	O
quantities	O
,	O
such	O
as	O
temperature	O
,	O
blood	O
pressure	O
,	O
alcohol	O
,	O
and	O
the	O
number	O
of	O
lost	O
teeth	O
.	O
more	O
formally	O
,	O
let	O
x	O
(	O
i	O
)	O
,	O
...	O
,	O
xed	O
)	O
be	O
the	O
d	O
components	O
of	O
a	O
vector	O
x.	O
if	O
1/1	O
1	O
,	O
...	O
,	O
1/1	O
d	O
are	O
strictly	O
monotone	O
mappings	O
:	O
n	O
-+	O
n	O
,	O
and	O
if	O
we	O
define	O
1/1	O
(	O
x	O
)	O
=	O
(	O
1/11	O
(	O
x	O
(	O
l	O
)	O
,	O
...	O
,	O
1/1	O
d	O
(	O
x	O
(	O
d	O
)	O
)	O
,	O
then	O
gn	O
is	O
scale-invariant	O
if	O
gn	O
(	O
x	O
,	O
dn	O
)	O
=	O
gn	O
(	O
1/i	O
(	O
x	O
)	O
,	O
d	O
;	O
l	O
)	O
'	O
where	O
d:1	O
=	O
(	O
(	O
1/i	O
(	O
x	O
1	O
)	O
,	O
y1	O
)	O
,	O
...	O
,	O
(	O
1j	O
;	O
'	O
(	O
xn	O
)	O
,	O
yn	O
»	O
.	O
in	O
other	O
words	O
,	O
if	O
all	O
the	O
xi	O
's	O
and	O
x	O
are	O
transformed	O
in	O
the	O
same	O
manner	O
,	O
the	O
decision	O
does	O
not	O
change	O
.	O
some	O
rules	O
based	O
on	O
statistically	O
equivalent	O
blocks	O
(	O
discussed	O
in	O
chapters	O
21	O
and	O
22	O
)	O
are	O
scale-invariant	O
,	O
while	O
the	O
k-nearest	O
neighbor	B
rule	I
clearly	O
is	O
not	O
.	O
here	O
we	O
describe	O
a	O
scale-invariant	O
modification	O
of	O
the	O
k-nearest	O
neighbor	B
rule	I
,	O
suggested	O
by	O
olshen	O
(	O
1977	O
)	O
and	O
devroye	O
(	O
1978	O
)	O
.	O
178	O
11.	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	B
rule	I
the	O
scale-invariant	O
k-nearest	O
neighbor	B
rule	I
is	O
based	O
upon	O
empirical	B
distances	O
that	O
are	O
defined	O
in	O
terms	O
of	O
the	O
order	B
statistics	I
along	O
the	O
d	O
coordinate	O
axes	O
.	O
first	O
order	O
the	O
points	O
x	O
,	O
xl	O
,	O
...	O
,	O
xn	O
according	O
to	O
increasing	O
values	O
of	O
their	O
first	O
com	O
(	O
cid:173	O
)	O
ponents	O
x	O
(	O
i	O
)	O
,	O
xii	O
)	O
,	O
...	O
,	O
x~l	O
)	O
,	O
breaking	O
ties	O
via	O
randomization	O
.	O
denote	O
the	O
rank	O
of	O
xii	O
)	O
by	O
r	O
?	O
)	O
,	O
and	O
the	O
rank	O
of	O
x	O
(	O
l	O
)	O
by	O
r	O
(	O
l	O
)	O
.	O
repeating	O
the	O
same	O
procedure	O
for	O
the	O
other	O
coordinates	O
,	O
we	O
obtain	O
the	O
ranks	O
ri	O
,	O
r	O
,	O
]	O
-	O
(	O
j	O
)	O
(	O
j	O
)	O
.	O
-	O
1	O
d	O
'	O
-	O
1	O
,	O
...	O
,	O
,	O
l	O
-	O
,	O
...	O
,	O
n.	O
define	O
the	O
empirical	B
distance	O
between	O
x	O
and	O
xi	O
by	O
p	O
(	O
x	O
,	O
xi	O
)	O
=	O
max	O
ir~j	O
)	O
-	O
i~j~d	O
i	O
/j	O
)	O
i.	O
a	O
k-nn	O
rule	B
can	O
be	O
defined	O
based	O
on	O
these	O
distances	O
,	O
by	O
a	O
majority	O
vote	O
among	O
the	O
yi	O
's	O
with	O
the	O
corresponding	O
xi	O
's	O
whose	O
empirical	B
distance	O
from	O
x	O
are	O
among	O
the	O
k	O
smallest	O
.	O
since	O
these	O
distances	O
are	O
integer-valued	O
,	O
ties	O
frequently	O
occur	O
.	O
these	O
ties	O
should	O
be	O
broken	O
by	O
randomization	O
.	O
devroye	O
(	O
1978	O
)	O
proved	O
that	O
this	O
rule	B
(	O
with	O
randomized	O
tie-breaking	O
)	O
is	O
weakly	O
universally	O
consistent	O
when	O
k	O
--	O
-+	O
(	O
xl	O
and	O
kj	O
n	O
--	O
-+	O
0	O
(	O
see	O
problem	O
11.5	O
)	O
.	O
for	O
another	O
consistent	O
scale-invariant	O
nearest	B
neighbor	I
rule	I
we	O
refer	O
to	O
problem	O
11.6	O
.	O
12	O
13	O
ii	O
ji	O
10	O
8	O
9	O
5	O
7	O
7	O
3	O
2	O
5	O
9	O
10	O
1	O
figure	O
11.1.	O
scale-invariant	O
distances	O
of	O
15	O
points	O
from	O
a	O
fixed	O
point	O
are	O
shown	O
here	O
.	O
11.5	O
weighted	B
nearest	O
neighbor	O
rules	O
in	O
the	O
k-nn	O
rule	B
,	O
each	O
of	O
the	O
k	O
nearest	O
neighbors	O
of	O
a	O
point	O
x	O
plays	O
an	O
equally	O
impor	O
(	O
cid:173	O
)	O
tant	O
role	O
in	O
the	O
decision	O
.	O
however	O
,	O
intuitively	O
speaking	O
,	O
nearer	O
neighbors	O
should	O
11.6	O
rotation-invariant	B
rules	O
179	O
provide	O
more	O
information	O
than	O
more	O
distant	O
ones	O
.	O
royall	O
(	O
1966	O
)	O
first	O
suggested	O
using	O
rules	O
in	O
which	O
the	O
labels	O
yi	O
are	O
given	O
unequal	O
voting	O
powers	O
in	O
the	O
decision	O
according	O
to	O
the	O
distances	O
of	O
the	O
xi	O
's	O
from	O
x	O
:	O
the	O
i	O
-th	O
nearest	B
neighbor	I
receives	O
weight	O
wni	O
,	O
where	O
usually	O
wnl	O
:	O
:	O
:	O
:	O
:	O
w	O
n2	O
:	O
:	O
:	O
:	O
:	O
...	O
:	O
:	O
:	O
:	O
:	O
whn	O
:	O
:	O
:	O
:	O
:	O
°	O
and	O
'l7	O
:	O
::1	O
wni	O
=	O
1.	O
the	O
rule	B
is	O
defined	O
as	O
(	O
x	O
)	O
=-	O
{	O
o	O
if'l7=l	O
wnj	O
{	O
y	O
(	O
i	O
)	O
(	O
x	O
)	O
=l	O
}	O
.	O
:	O
:	O
:	O
:	O
:	O
'l7=1	O
wni	O
i	O
{	O
y	O
(	O
i	O
)	O
(	O
x	O
)	O
=ol	O
gn	O
1	O
otherwise	O
.	O
we	O
get	O
the	O
ordinary	B
k-nearest	O
neighbor	B
rule	I
back	O
by	O
the	O
choice	O
wni	O
=	O
°	O
otherwise	O
.	O
if	O
i	O
.	O
:	O
:	O
:	O
:	O
:	O
k	O
1/	O
k	O
{	O
the	O
following	O
conditions	O
for	O
consistency	B
were	O
established	O
by	O
stone	O
(	O
1977	O
)	O
:	O
and	O
lim	O
max	O
wni	O
=	O
0	O
,	O
n	O
--	O
hx	O
)	O
l	O
:	O
:	O
:	O
i	O
:	O
:	O
:	O
n	O
'	O
''	O
''	O
''	O
wni	O
=	O
°	O
lim	O
n-+oo	O
6	O
for	O
some	O
k	O
with	O
kin	O
-+	O
°	O
(	O
see	O
problem	O
11.7	O
)	O
.	O
weighted	B
versions	O
of	O
the	O
recursive	B
and	O
scale-invariant	O
methods	O
described	O
above	O
can	O
also	O
be	O
defined	O
similarly	O
.	O
k	O
:	O
:	O
:	O
i	O
:	O
:	O
:	O
n	O
11.6	O
rotation-invariant	B
rules	O
assume	O
that	O
an	O
affine	O
transformation	O
t	O
is	O
applied	O
to	O
x	O
and	O
xl	O
,	O
...	O
,	O
xn	O
(	O
i.e.	O
,	O
any	O
number	O
of	O
combinations	O
of	O
rotations	O
,	O
translations	O
,	O
and	O
linear	O
rescalings	O
)	O
,	O
and	O
that	O
for	O
any	O
such	O
linear	O
transformation	O
t	O
,	O
gn	O
(	O
x	O
,	O
dn	O
)	O
=	O
gn	O
(	O
t	O
(	O
x	O
)	O
,	O
d~	O
)	O
,	O
where	O
d~	O
=	O
«	O
t	O
(	O
xr	O
)	O
,	O
yr	O
)	O
,	O
...	O
,	O
(	O
t	O
(	O
xn	O
)	O
,	O
~1	O
)	O
)	O
'	O
then	O
we	O
call	O
gn	O
rotation-invariant	B
.	O
rotation-invariance	O
is	O
indeed	O
a	O
very	O
strong	B
property	O
.	O
in	O
r	O
d	O
,	O
in	O
the	O
context	O
of	O
k-nn	O
estimates	O
,	O
it	O
suffices	O
to	O
be	O
able	O
to	O
define	O
a	O
rotation-invariant	O
distance	B
measure	O
.	O
these	O
are	O
necessarily	O
data-dependent	B
.	O
an	O
example	O
of	O
this	O
goes	O
as	O
follows	O
.	O
any	O
collection	O
of	O
d	O
points	O
in	O
general	O
position	O
defines	O
a	O
polyhedron	O
in	O
a	O
hyperplane	B
of	O
rd	O
.	O
for	O
points	O
(	O
xii	O
'	O
...	O
,	O
xij	O
,	O
we	O
denote	O
this	O
polyhedron	O
by	O
pul	O
,	O
...	O
,	O
id	O
)	O
.	O
then	O
we	O
define	O
the	O
distance	B
p	O
(	O
xi	O
,	O
x	O
)	O
=	O
l	O
i	O
{	O
segment	O
(	O
xi.x	O
)	O
intersectsp	O
(	O
il	O
•	O
...	O
,	O
id	O
)	O
}	O
·	O
il	O
,	O
...	O
,	O
id	O
,	O
i~hil	O
...	O
..	O
idl	O
near	O
points	O
have	O
few	O
intersections	O
.	O
using	O
p	O
(	O
.	O
,	O
.	O
)	O
in	O
a	O
k-nn	O
rule	B
with	O
k	O
-+	O
00	O
and	O
k	O
/	O
n	O
-+	O
0	O
,	O
we	O
expect	O
weak	B
universal	O
consistency	B
under	O
an	O
appropriate	O
scheme	O
of	O
tie-breaking	O
.	O
the	O
answer	O
to	O
this	O
is	O
left	O
as	O
an	O
open	O
problem	O
for	O
the	O
scholars	O
.	O
180	O
11.	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	B
rule	I
3	O
figure	O
11.2.	O
rotation-invariant	B
dis-	O
4	O
tancesfrom	O
x	O
.	O
2	O
11.7	O
relabeling	B
rules	O
the	O
i-nn	O
rule	B
appeals	O
to	O
the	O
masses	O
who	O
crave	O
simplicity	O
and	O
attracts	O
the	O
pro	O
(	O
cid:173	O
)	O
grammers	O
who	O
want	O
to	O
write	O
short	O
understandable	O
code	O
.	O
nearest	O
neighbors	O
may	O
be	O
found	O
efficiently	O
if	O
the	O
data	O
are	O
preprocessed	O
(	O
see	O
chapter	O
5	O
for	O
references	O
)	O
.	O
can	O
we	O
make	O
the	O
i-nn	O
rule	B
universally	O
consistent	O
as	O
well	O
?	O
in	O
this	O
section	O
we	O
introduce	O
a	O
tool	O
called	O
relabeling	B
,	O
which	O
works	O
as	O
follows	O
.	O
assume	O
that	O
we	O
have	O
a	O
classifica	O
(	O
cid:173	O
)	O
tion	O
rule	B
{	O
gn	O
(	O
x	O
,	O
dn	O
)	O
,	O
x	O
e	O
n	O
d	O
,	O
n	O
~	O
i	O
}	O
,	O
where	O
dn	O
is	O
the	O
data	O
(	O
xl	O
,	O
yl	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
.	O
this	O
rule	B
will	O
be	O
called	O
the	O
ancestral	B
rule	I
.	O
define	O
the	O
labels	O
these	O
are	O
the	O
decisions	O
for	O
the	O
xi	O
's	O
themselves	O
obtained	O
by	O
mere	O
resubstitution	B
.	O
in	O
the	O
relabeling	B
method	O
,	O
we	O
apply	O
the	O
i-nn	O
rule	B
to	O
the	O
new	O
data	O
(	O
xl	O
,	O
zl	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
zn	O
)	O
.	O
if	O
all	O
goes	O
well	O
,	O
when	O
the	O
ancestral	B
rule	I
gn	O
is	O
universally	O
consistent	O
,	O
so	O
should	O
the	O
relabeling	B
rule	O
.	O
we	O
will	O
show	O
this	O
by	O
example	O
,	O
starting	O
from	O
a	O
consistent	O
k-nn	O
rule	B
as	O
ancestral	B
rule	I
(	O
with	O
k	O
--	O
+	O
00	O
,	O
kin	O
--	O
+	O
0	O
)	O
.	O
unfortunately	O
,	O
relabeling	B
rules	O
do	O
not	O
always	O
inherit	O
consistency	B
from	O
their	O
ancestral	O
rules	O
,	O
so	O
that	O
a	O
more	O
general	O
theorem	B
is	O
more	O
difficult	O
to	O
obtain	O
,	O
unless	O
one	O
adds	O
in	O
a	O
lot	O
of	O
regularity	O
conditions-this	O
does	O
not	O
seem	O
to	O
be	O
the	O
right	O
time	O
for	O
that	O
sort	O
of	O
effort	O
.	O
to	O
see	O
that	O
universal	B
consistency	I
of	O
the	O
ancestral	B
rule	I
does	O
not	O
imply	O
consistency	B
of	O
the	O
relabeling	B
rule	O
,	O
consider	O
the	O
following	O
rule	B
hn	O
:	O
if	O
x	O
=	O
xi	O
and	O
x	O
=i	O
x	O
j	O
,	O
all	O
j	O
=i	O
i	O
otherwise	O
,	O
where	O
gn	O
is	O
a	O
weakly	O
universally	O
consistent	B
rule	I
.	O
it	O
is	O
easy	O
to	O
show	O
(	O
see	O
problem	O
11.15	O
)	O
that	O
hn	O
is	O
universally	O
consistent	O
as	O
well	O
.	O
changing	O
a	O
rule	O
on	O
a	O
set	O
of	O
measure	O
zero	O
indeed	O
does	O
not	O
affect	O
ln	O
.	O
also	O
,	O
if	O
x	O
=	O
xi	O
is	O
at	O
an	O
atom	O
of	O
the	O
distribution	B
of	O
x	O
,	O
we	O
only	O
change	O
gn	O
to	O
1	O
-	O
yi	O
if	O
xi	O
is	O
the	O
sole	O
occurrence	O
of	O
that	O
atom	O
in	O
the	O
data	O
.	O
this	O
has	O
asymptotically	O
no	O
impact	O
on	O
ln	O
.	O
however	O
,	O
if	O
hn	O
is	O
used	O
as	O
an	O
ancestral	B
rule	I
,	O
and	O
x	O
is	O
nonatomic	O
,	O
then	O
hn	O
(	O
xi	O
,	O
dn	O
)	O
=	O
1	O
yi	O
for	O
all	O
i	O
,	O
and	O
therefore	O
,	O
the	O
relabeling	B
rule	O
is	O
a	O
i-nn	O
rule	B
based	O
on	O
the	O
data	O
(	O
x	O
1	O
,	O
1	O
-	O
yl	O
)	O
,	O
•••	O
,	O
(	O
xn	O
,	O
1	O
-	O
yn	O
)	O
.	O
if	O
l	O
*	O
=	O
0	O
for	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
then	O
the	O
relabeling	B
rule	O
has	O
probability	O
of	O
error	O
converging	O
to	O
one	O
!	O
for	O
most	O
nonpathological	O
ancestral	O
rules	O
,	O
relabeling	B
does	O
indeed	O
preserve	O
uni	O
(	O
cid:173	O
)	O
versal	O
consistency	B
.	O
we	O
offer	O
a	O
prototype	O
proof	O
for	O
the	O
k-nn	O
rule	B
.	O
11.7	O
relabeling	B
rules	O
181	O
theorem	B
11.2.	O
let	O
gn	O
be	O
the	O
k-nn	O
rule	B
in	O
which	O
tie-breaking	O
is	O
done	O
by	O
random	O
(	O
cid:173	O
)	O
ization	O
as	O
in	O
(	O
11.3	O
)	O
.	O
assume	O
that	O
k	O
-+	O
00	O
and	O
k	O
/	O
n	O
-+	O
°	O
(	O
so	O
that	O
gn	O
is	O
weakly	O
universally	O
consistent	O
)	O
.	O
then	O
the	O
relabeling	B
rule	O
based	O
upon	O
gn	O
is	O
weakly	O
univer	O
(	O
cid:173	O
)	O
sally	O
consistent	O
as	O
well	O
.	O
proof	O
.	O
we	O
verify	O
the	O
conditions	O
of	O
stone	O
's	O
weak	B
convergence	O
theorem	B
(	O
see	O
theo	O
(	O
cid:173	O
)	O
rem	O
6.3	O
)	O
.	O
to	O
keep	O
things	O
simple	O
,	O
we	O
assume	O
that	O
the	O
distribution	B
of	O
x	O
has	O
a	O
density	O
so	O
that	O
distance	B
ties	O
happen	O
with	O
probability	O
zero	O
.	O
in	O
our	O
case	O
,	O
the	O
weight	O
wni	O
(	O
x	O
)	O
of	O
theorem	O
6.3	O
equals	O
1/	O
k	O
iff	O
xi	O
is	O
among	O
the	O
k	O
nearest	O
neighbors	O
of	O
x	O
(	O
l	O
)	O
(	O
x	O
)	O
,	O
where	O
x	O
(	O
l	O
)	O
(	O
x	O
)	O
is	O
the	O
nearest	B
neighbor	I
of	O
x.	O
it	O
is	O
zero	O
otherwise	O
.	O
condition	O
6.3	O
(	O
iii	O
)	O
is	O
trivially	O
satisfied	O
since	O
k	O
-+	O
00.	O
for	O
condition	O
6.3	O
(	O
ii	O
)	O
,	O
we	O
note	O
that	O
if	O
x	O
(	O
i	O
)	O
(	O
x	O
)	O
denotes	O
the	O
i-th	O
nearest	B
neighbor	I
of	O
x	O
among	O
xl	O
,	O
...	O
,	O
x	O
n	O
,	O
then	O
just	O
note	O
that	O
sx	O
(	O
1	O
)	O
(	O
x	O
)	O
.2i1x-x	O
(	O
k	O
)	O
1i	O
;	O
2	O
sx.llx-xck	O
)	O
11	O
and	O
that	O
the	O
latter	O
sphere	B
contains	O
k	O
data	O
points	O
.	O
but	O
we	O
already	O
know	O
from	O
the	O
proof	O
of	O
weak	O
consistency	B
of	O
the	O
k-nn	O
rule	B
that	O
if	O
k	O
/	O
n	O
-+	O
0	O
,	O
then	O
for	O
all	O
e	O
>	O
0	O
,	O
p	O
{	O
iix	O
(	O
k	O
)	O
(	O
x	O
)	O
-	O
xii	O
>	O
e	O
}	O
-+	O
0.	O
finally	O
,	O
we	O
consider	O
condition	O
6.3	O
(	O
i	O
)	O
.	O
here	O
we	O
have	O
,	O
arguing	O
partially	O
as	O
in	O
stone	O
(	O
1977	O
)	O
,	O
=	O
e	O
{	O
~t	O
i	O
{	O
xisamongtheknn'sofxcj	O
)	O
(	O
xi	O
)	O
in	O
(	O
xl	O
,	O
...	O
,	O
xn.x	O
}	O
-ixd	O
}	O
!	O
(	O
x	O
)	O
}	O
1=1	O
(	O
reverse	O
the	O
roles	O
of	O
xi	O
and	O
x	O
)	O
1	O
n	O
n	O
=	O
e	O
{	O
k	O
lli	O
{	O
xj	O
is	O
the	O
nn	O
of	O
xi	O
in	O
{	O
xl	O
...	O
..	O
xn.x	O
}	O
-	O
{	O
xd	O
}	O
i=l	O
j=l	O
x	O
i	O
{	O
x	O
is	O
among	O
the	O
k	O
nn	O
's	O
of	O
xj	O
in	O
{	O
xl	O
,	O
...	O
,	O
xn	O
,	O
x	O
}	O
-	O
{	O
xd	O
}	O
!	O
(	O
x	O
)	O
}	O
,	O
however	O
,	O
by	O
lemma	O
11.1	O
,	O
n	O
l	O
i=l	O
i	O
{	O
xj	O
is	O
thenn	O
of	O
xi	O
in	O
{	O
xl	O
...	O
.	O
,	O
x	O
''	O
.x	O
}	O
-	O
{	O
xdj	O
:	O
:	O
:	O
yd·	O
182	O
11.	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	B
rule	I
also	O
,	O
n	O
l	O
i	O
{	O
x	O
is	O
among	O
thek	O
nn	O
's	O
of	O
xj	O
in	O
{	O
xl	O
''	O
''	O
,	O
xn	O
,	O
x	O
}	O
-	O
{	O
xd	O
}	O
:	O
:	O
:	O
s	O
kyd	O
'	O
j=l	O
therefore	O
,	O
by	O
a	O
double	O
application	O
of	O
lemma	O
11.1	O
,	O
e	O
{	O
t	O
~	O
irx	O
,	O
j	O
,	O
,	O
''	O
''	O
ong	O
thok	O
nn	O
'	O
,	O
of	O
xo	O
)	O
(	O
xjj	O
!	O
(	O
x	O
,	O
)	O
}	O
:	O
'0	O
yj	O
ef	O
(	O
x	O
)	O
,	O
and	O
condition	O
6.3	O
(	O
i	O
)	O
is	O
verified	O
.	O
0	O
problems	O
and	O
exercises	O
problem	O
11.1.	O
show	O
that	O
the	O
conditions	O
k	O
-+	O
00	O
and	O
k	O
/	O
n	O
-+	O
°	O
are	O
necessary	O
for	O
universal	B
bounded	O
,	O
lim	O
inf	O
n-hx	O
)	O
eln	O
>	O
l	O
*	O
.	O
exhibit	O
a	O
second	O
distribution	B
such	O
that	O
if	O
k	O
/	O
n	O
:	O
:	O
:	O
:	O
e	O
>	O
°	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	B
rule	I
.	O
that	O
is	O
,	O
exhibit	O
a	O
distribution	O
such	O
that	O
if	O
k	O
remains	O
for	O
all	O
n	O
,	O
and	O
some	O
e	O
,	O
then	O
lim	O
infn-	O
hx	O
)	O
eln	O
>	O
l	O
*	O
.	O
problem	O
11.2.	O
let	O
x	O
be	O
monoatomic	O
,	O
with	O
1	O
]	O
<	O
1/2	O
.	O
show	O
that	O
for	O
e	O
<	O
1/2	O
-1	O
]	O
,	O
for	O
some	O
c	O
>	O
0	O
,	O
where	O
ln	O
is	O
the	O
error	O
probability	O
of	O
the	O
k-nn	O
rule	B
with	O
tie-breaking	O
by	O
indices	O
.	O
problem	O
11.3.	O
prove	O
that	O
the	O
recursive	B
nearest	O
neighbor	B
rule	I
is	O
universally	O
consistent	O
pro	O
(	O
cid:173	O
)	O
vided	O
that	O
limn-+oo	O
in	O
=	O
00	O
(	O
devroye	O
and	O
wise	O
(	O
1980	O
)	O
)	O
.	O
hint	O
:	O
check	O
the	O
conditions	O
of	O
theorem	O
6.3.	O
problem	O
11.4.	O
prove	O
that	O
the	O
nearest	B
neighbor	I
rule	I
defined	O
by	O
any	O
l	O
p-distance	O
measure	B
°	O
<	O
p	O
:	O
:	O
:	O
:	O
:	O
00	O
is	O
universally	O
consistent	O
under	O
the	O
usual	O
conditions	O
on	O
k.	O
the	O
l	O
p	O
-distance	O
between	O
x	O
,	O
y	O
end	O
is	O
defined	O
by	O
(	O
e1=1ix	O
(	O
i	O
)	O
-	O
y	O
(	O
i	O
)	O
ip	O
rip	O
foro	O
<	O
p	O
<	O
oo	O
,	O
andbysupi	O
ix	O
(	O
i	O
)	O
(	O
cid:173	O
)	O
y	O
(	O
i	O
)	O
i	O
for	O
p	O
=	O
00	O
,	O
where	O
x	O
=	O
(	O
x	O
(	O
1	O
)	O
,	O
...	O
,	O
xed	O
»	O
)	O
.	O
hint	O
:	O
check	O
the	O
conditions	O
of	O
theorem	O
6.3.	O
problem	O
11.5.	O
let	O
o-	O
(	O
x	O
,	O
z	O
,	O
xi	O
,	O
zd	O
=	O
p	O
(	O
x	O
,	O
xi	O
)	O
+	O
izi	O
-	O
zl	O
be	O
a	O
generalized	O
distance	B
between	O
(	O
x	O
,	O
z	O
)	O
and	O
(	O
xi	O
,	O
zi	O
)	O
,	O
where	O
x	O
,	O
xl	O
,	O
...	O
,	O
xn	O
are	O
as	O
in	O
the	O
description	O
of	O
the	O
scale-invariantk-nn	O
rule	B
,	O
p	O
(	O
x	O
,	O
xi	O
)	O
is	O
the	O
empirical	B
distance	O
defined	O
there	O
,	O
and	O
z	O
,	O
zi	O
e	O
[	O
0,1	O
]	O
are	O
real	O
numbers	O
added	O
to	O
break	O
ties	O
at	O
random	O
.	O
the	O
sequence	O
zl	O
,	O
...	O
,	O
zn	O
is	O
i.i.d	O
.	O
uniform	B
[	O
0	O
,	O
1	O
]	O
and	O
is	O
independent	O
of	O
the	O
data	O
dn	O
.	O
with	O
the	O
k-nn	O
rule	B
based	O
on	O
the	O
artificial	O
distances	O
0-	O
,	O
show	O
that	O
the	O
rule	B
is	O
universally	O
consistent	O
by	O
verifying	O
the	O
conditions	O
of	O
theorem	O
6.3	O
,	O
when	O
k	O
-+	O
00	O
and	O
kin	O
-+	O
0.	O
in	O
particular	O
,	O
show	O
first	O
that	O
if	O
z	O
is	O
uniform	B
[	O
0	O
,	O
1	O
]	O
and	O
independent	O
of	O
x	O
,	O
y	O
,	O
dn	O
and	O
zi	O
,	O
...	O
,	O
zn	O
,	O
and	O
if	O
wni	O
(	O
x	O
,	O
z	O
)	O
is	O
the	O
weight	O
of	O
(	O
xi	O
,	O
zi	O
)	O
in	O
this	O
k-nn	O
rule	B
(	O
i.e.	O
,	O
it	O
is	O
1/	O
k	O
iff	O
(	O
xi	O
,	O
zi	O
)	O
is	O
among	O
the	O
k	O
nearest	O
neighbors	O
of	O
(	O
x	O
,	O
z	O
)	O
according	O
to	O
0-	O
)	O
,	O
then	O
(	O
1	O
)	O
e	O
{	O
i:7=1	O
wni	O
(	O
x	O
,	O
z	O
)	O
f	O
(	O
xd	O
}	O
:	O
:	O
:	O
:	O
:	O
2d	O
ef	O
(	O
x	O
)	O
for	O
all	O
nonnegative	O
measurable	O
f	O
with	O
ef	O
(	O
x	O
)	O
<	O
00.	O
problems	O
and	O
exercises	O
183	O
(	O
2	O
)	O
if	O
kin	O
-+	O
0	O
,	O
then	O
for	O
all	O
a	O
>	O
o.	O
hint	O
:	O
check	O
the	O
conditions	O
of	O
theorem	O
6.3	O
(	O
devroye	O
(	O
1978	O
)	O
)	O
.	O
problem	O
11.6.	O
the	O
layered	B
nearest	O
neighbor	B
rule	I
partitions	O
the	O
space	O
at	O
x	O
into	O
2d	O
quad	O
(	O
cid:173	O
)	O
rants	O
.	O
in	O
each	O
quadrant	O
,	O
the	O
outer-layer	O
points	O
are	O
marked	O
,	O
that	O
is	O
,	O
those	O
xi	O
for	O
which	O
the	O
hyperrectangle	O
defined	O
by	O
x	O
and	O
xi	O
contains	O
no	O
other	O
data	O
point	O
.	O
then	O
it	O
takes	O
a	O
majority	O
vote	O
over	O
the	O
yi	O
's	O
for	O
the	O
marked	O
points	O
.	O
observe	O
that	O
this	O
rule	B
is	O
scale-invariant	O
.	O
show	O
that	O
whenever	O
x	O
has	O
nonatornic	O
marginals	O
(	O
to	O
avoid	O
ties	O
)	O
,	O
e	O
{	O
ln	O
}	O
-+	O
l	O
*	O
in	O
probability	O
.	O
•	O
•	O
•	O
@	O
i	O
@	O
~	O
•	O
figure	O
11.3.	O
the	O
layered	B
nearest	O
neighbor	B
rule	I
takes	O
a	O
majority	O
vote	O
over	O
the	O
marked	O
points	O
.	O
--	O
--	O
--	O
--	O
--	O
-_.	O
--	O
--	O
--	O
--	O
-.	O
--	O
--	O
--	O
--	O
--	O
.	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
--	O
@	O
@	O
xl	O
i	O
empty	O
i	O
•	O
®	O
'	O
--	O
-	O
:	O
@	O
xi	O
@	O
•	O
hint	O
:	O
it	O
suffices	O
to	O
show	O
that	O
the	O
number	O
of	O
marked	O
points	O
increases	O
unboundedly	O
in	O
prob	O
(	O
cid:173	O
)	O
ability	O
,	O
and	O
that	O
its	O
proportion	O
to	O
unmarked	O
points	O
tends	O
to	O
zero	O
in	O
probability	O
.	O
problem	O
11.7.	O
prove	O
weak	B
universal	O
consistency	B
of	O
the	O
weighted	B
nearest	O
neighbor	B
rule	I
if	O
the	O
weights	O
satisfy	O
lim	O
max	O
wni	O
=	O
0	O
,	O
n	O
--	O
'	O
)	O
-oo	O
lsisn	O
and	O
lim	O
``	O
n	O
--	O
'	O
)	O
-oo	O
~	O
ksisn	O
wni	O
=	O
0	O
for	O
some	O
k	O
with	O
kin	O
-+	O
0	O
(	O
stone	O
(	O
1977	O
)	O
.	O
hint	O
:	O
check	O
the	O
conditions	O
of	O
theorem	O
6.3.	O
problem	O
11.8.	O
if	O
(	O
wnl	O
'	O
...	O
,	O
wnn	O
)	O
is	O
a	O
probability	O
vector	O
,	O
then	O
limn	O
--	O
,	O
)	O
-oo	O
li	O
>	O
no	O
wni	O
=	O
0	O
for	O
all	O
8	O
>	O
0	O
if	O
and	O
only	O
if	O
there	O
exists	O
a	O
sequence	O
of	O
integers	O
k	O
=	O
kn	O
such	O
that	O
k	O
=	O
o	O
(	O
n	O
)	O
,	O
k	O
-+	O
00	O
,	O
and	O
li	O
>	O
k	O
wni	O
=	O
0	O
(	O
1	O
)	O
.	O
show	O
this	O
.	O
conclude	O
that	O
the	O
conditions	O
of	O
problem	O
11.7	O
are	O
equivalent	O
to	O
(	O
i	O
)	O
(	O
ii	O
)	O
lim	O
max	O
wni	O
=	O
0	O
,	O
n	O
--	O
'	O
)	O
-oo	O
lsisn	O
lim	O
~	O
wni	O
=	O
0	O
for	O
all	O
8	O
>	O
o.	O
n	O
--	O
'	O
)	O
-oo~	O
i	O
>	O
no	O
problem	O
11.9.	O
verify	O
the	O
conditions	O
of	O
problems	O
11.7	O
and	O
11.8	O
for	O
weight	O
vectors	O
of	O
the	O
form	O
wni	O
=	O
cn	O
i	O
i	O
ci	O
,	O
where	O
a	O
>	O
0	O
is	O
a	O
constant	O
and	O
cn	O
is	O
a	O
normalizing	O
constant	O
.	O
in	O
particular	O
,	O
check	O
that	O
they	O
do	O
not	O
hold	O
for	O
a	O
>	O
1	O
but	O
that	O
they	O
do	O
hold	O
for	O
0	O
<	O
a	O
:	O
s	O
1.	O
problem	O
l1.lo	O
.	O
consider	O
as	O
weight	O
vector	O
.	O
show	O
that	O
the	O
conditions	O
of	O
problem	O
11.8	O
hold	O
if	O
pn	O
-+	O
°	O
and	O
npn	O
-+	O
00.	O
iii	O
-	O
1	O
:	O
s	O
i	O
:	O
s	O
n	O
,	O
w·-	O
pn	O
(	O
1	O
+	O
pny	O
1	O
x	O
-	O
-	O
-	O
-	O
-	O
1	O
-	O
(	O
1	O
+	O
pn	O
)	O
-n	O
'	O
184	O
11.	O
consistency	B
of	O
the	O
k-nearest	O
neighbor	B
rule	I
problem	O
11.11.	O
let	O
wni	O
=	O
p	O
{	O
z	O
=	O
i	O
}	O
/p	O
{	O
z	O
:	O
s	O
n	O
}	O
,	O
1	O
:	O
s	O
i	O
:	O
s	O
n	O
,	O
where	O
z	O
is	O
a	O
poisson	O
(	O
)	O
''	O
n	O
)	O
random	O
variable	B
.	O
show	O
that	O
there	O
is	O
no	O
choice	O
of	O
an	O
such	O
that	O
{	O
wni	O
,	O
1	O
:	O
s	O
i	O
:	O
s	O
n	O
}	O
is	O
a	O
consistent	O
weight	O
sequence	O
following	O
the	O
conditions	O
of	O
problem	O
11.8.	O
problem	O
11.12.	O
let	O
wni	O
=	O
p	O
{	O
binomial	B
(	O
n	O
,	O
pn	O
)	O
=	O
i	O
}	O
=	O
g	O
)	O
p	O
;	O
7	O
(	O
1-	O
pn	O
)	O
n-i	O
.	O
derive	O
conditions	O
on	O
pn	O
for	O
this	O
choice	O
of	O
weight	O
sequence	O
to	O
be	O
consistent	O
in	O
the	O
sense	O
of	O
problem	O
11.8.	O
problem	O
11.13.	O
k-nn	O
density	B
estimation	I
.	O
we	O
recall	O
from	O
problem	O
2.11	O
that	O
if	O
the	O
con	O
(	O
cid:173	O
)	O
ditional	O
densities	O
/0	O
,	O
/1	O
exist	O
,	O
then	O
l	O
1-consistent	O
density	B
estimation	I
leads	O
to	O
a	O
consistent	O
classification	O
rule	B
.	O
consider	O
now	O
the	O
k-nearest	O
neighbor	O
density	O
estimate	B
introduced	O
by	O
lofts	O
gaarden	O
and	O
quesenberry	O
c	O
1965	O
)	O
.	O
let	O
xi	O
,	O
...	O
,	O
x	O
n	O
be	O
independent	O
,	O
identically	O
dis	O
(	O
cid:173	O
)	O
tributed	O
random	O
variables	O
in	O
r	O
d	O
by	O
,	O
with	O
common	O
density	O
/	O
.	O
the	O
k-nn	O
estimate	B
of	O
/	O
is	O
defined	O
fncx	O
)	O
=	O
(	O
a	O
sx	O
,	O
lix-	O
xck	O
)	O
(	O
xlll	O
k	O
)	O
'	O
where	O
x	O
(	O
klcx	O
)	O
is	O
the	O
k-th	O
nearest	B
neighbor	I
of	O
x	O
among	O
xl	O
,	O
...	O
,	O
x	O
n	O
.	O
show	O
that	O
for	O
every	O
n	O
,	O
f	O
if	O
(	O
x	O
)	O
-	O
fn	O
(	O
x	O
)	O
ldx	O
=	O
00	O
,	O
so	O
that	O
the	O
density	O
estimate	O
is	O
never	O
consistent	O
in	O
l	O
1•	O
on	O
the	O
other	O
hand	O
,	O
according	O
to	O
problem	O
11.14	O
,	O
the	O
corresponding	O
classification	O
rule	B
is	O
consistent	O
,	O
so	O
read	O
on	O
.	O
problem	O
11.14.	O
assume	O
that	O
the	O
conditional	O
densities	O
fo	O
and	O
f1	O
exist	O
.	O
then	O
we	O
can	O
use	O
a	O
rule	O
suggested	O
by	O
patrick	O
and	O
fischer	O
(	O
1970	O
)	O
:	O
let	O
no	O
=	O
l7=1	O
i	O
{	O
yi=o	O
}	O
and	O
n1	O
=	O
n	O
-	O
no	O
be	O
the	O
number	O
of	O
zeros	O
and	O
ones	O
in	O
the	O
training	O
data	O
.	O
denote	O
by	O
x6k\x	O
)	O
the	O
k-th	O
nearest	B
neighbor	I
of	O
x	O
among	O
the	O
x/s	O
with	O
yi	O
=	O
0.	O
define	O
xiklcx	O
)	O
similarly	O
.	O
if	O
aca	O
)	O
denotes	O
the	O
volume	O
of	O
a	O
set	O
a	O
c	O
rd	O
,	O
then	O
the	O
rule	B
is	O
defined	O
as	O
if	O
no/a	O
(	O
sx	O
,	O
iix-x6k	O
)	O
(	O
xlll	O
)	O
2	O
:	O
nda	O
(	O
sx	O
,	O
llx-xik	O
)	O
(	O
xlli	O
)	O
otherwise	O
.	O
this	O
estimate	B
is	O
based	O
on	O
the	O
k-nearest	O
neighbor	O
density	O
estimate	B
introduced	O
by	O
loftsgaarden	O
and	O
quesenberry	O
(	O
1965	O
)	O
.	O
their	O
estimate	B
of	O
fo	O
is	O
then	O
the	O
rule	B
gn	O
can	O
be	O
re-written	O
as	O
if	O
po	O
,	O
nlo	O
,	O
n	O
2	O
:	O
pl	O
,	O
nlr	O
,	O
n	O
otherwise	O
,	O
where	O
po	O
,	O
n	O
=	O
no/	O
nand	O
pl	O
,	O
n	O
=	O
nd	O
n	O
are	O
the	O
obvious	O
estimates	O
of	O
the	O
class	O
probabilities	O
.	O
show	O
that	O
gn	O
is	O
weakly	O
consistent	O
if	O
k	O
--	O
+	O
00	O
and	O
kin	O
--	O
+	O
0	O
,	O
whenever	O
the	O
conditional	O
densities	O
exist	O
.	O
problem	O
11.15.	O
consider	O
a	O
weakly	O
universally	O
consistent	B
rule	I
gn	O
,	O
and	O
define	O
the	O
rule	B
if	O
x	O
=	O
xi	O
and	O
x	O
=i	O
x	O
j	O
,	O
all	O
j	O
=i	O
i	O
otherwise	O
.	O
show	O
that	O
h	O
n	O
too	O
is	O
weakly	O
universally	O
consistent	O
.	O
note	O
:	O
it	O
is	O
the	O
atomic	O
cor	O
partially	O
atomic	O
)	O
distributions	O
of	O
x	O
that	O
make	O
this	O
exercise	O
interesting	O
.	O
hint	O
:	O
the	O
next	O
exercise	O
may	O
help	O
.	O
problems	O
and	O
exercises	O
185	O
problem	O
11.16.	O
let	O
x	O
have	O
an	O
atomic	O
distribution	B
which	O
puts	O
probability	O
pi	O
at	O
atom	O
i.	O
let	O
xl	O
,	O
...	O
,	O
xn	O
be	O
an	O
u.d	O
.	O
sample	O
drawn	O
from	O
this	O
distribution	B
.	O
then	O
show	O
the	O
following	O
.	O
(	O
1	O
)	O
p	O
{	O
\n	O
-	O
en\	O
>	O
e	O
}	O
s	O
2e-2e2in	O
,	O
where	O
e	O
>	O
0	O
,	O
and	O
n	O
is	O
the	O
number	O
of	O
``	O
occupied	O
''	O
atoms	O
(	O
the	O
number	O
of	O
different	O
values	O
in	O
the	O
data	O
sequence	O
)	O
.	O
(	O
2	O
)	O
en	O
in	O
-+	O
o	O
.	O
(	O
3	O
)	O
n	O
i	O
n	O
-+	O
0	O
almost	O
surely	O
.	O
(	O
4	O
)	O
li	O
:	O
x	O
rli	O
for	O
all	O
j	O
sn	O
pi	O
-+	O
0	O
almost	O
surely	O
.	O
problem	O
11.17.	O
royall	O
's	O
rule	B
.	O
royall	O
(	O
1966	O
)	O
proposes	O
the	O
regression	B
function	I
estimate	O
1	O
lnhnj	O
(	O
i	O
)	O
l	O
]	O
n	O
(	O
x	O
)	O
=	O
-	O
,	O
-	O
l	O
1	O
-h	O
y	O
(	O
i	O
)	O
(	O
x	O
)	O
,	O
n	O
111	O
i=i	O
n	O
n	O
where	O
leu	O
)	O
is	O
a	O
smooth	O
kernel-like	O
function	O
on	O
[	O
0	O
,	O
1	O
]	O
with	O
f~	O
l	O
(	O
u	O
)	O
du	O
=	O
1	O
and	O
h	O
n	O
>	O
0	O
is	O
a	O
smoothing	O
factor	O
.	O
suggestions	O
included	O
(	O
i	O
)	O
(	O
ii	O
)	O
leu	O
)	O
==	O
1	O
;	O
(	O
d	O
+	O
2	O
?	O
(	O
l	O
(	O
u	O
)	O
=-4-	O
1-	O
d+2u	O
d	O
+	O
4	O
21d	O
)	O
define	O
(	O
note	O
that	O
this	O
function	O
becomes	O
negative	O
)	O
.	O
if	O
l	O
]	O
11	O
(	O
x	O
)	O
>	O
1/2	O
otherwise	O
.	O
assume	O
that	O
hn	O
-+	O
0	O
,	O
nhn	O
-+	O
00.	O
derive	O
sufficient	O
conditions	O
on	O
1	O
that	O
guarantee	O
the	O
weak	B
universal	O
consistency	B
of	O
royall	O
's	O
rule	B
.	O
in	O
particular	O
,	O
insure	O
that	O
choice	O
(	O
ii	O
)	O
is	O
weakly	O
universally	O
consistent	O
.	O
hint	O
:	O
try	O
adding	O
an	O
appropriate	O
smoothness	O
condition	O
to	O
l.	O
problem	O
11.18.	O
let	O
k	O
be	O
a	O
kernel	O
and	O
let	O
rn	O
(	O
x	O
)	O
denote	O
the	O
distance	B
between	O
x	O
and	O
its	O
k-th	O
nearest	B
neighbor	I
,	O
x	O
(	O
k	O
)	O
(	O
x	O
)	O
,	O
among	O
x	O
i	O
,	O
...	O
,	O
x	O
n	O
.	O
the	O
discrimination	O
rule	B
that	O
corresponds	O
to	O
a	O
kernel-type	O
nearest	B
neighbor	I
regression	O
function	O
estimate	B
of	O
mack	O
(	O
1981	O
)	O
is	O
if	O
``	O
'~l	O
(	O
2y	O
.	O
-	O
otherwise	O
.	O
l.a=1	O
i	O
i	O
)	O
k	O
(	O
x-xi	O
)	O
<	O
0	O
rn	O
(	O
x	O
)	O
-	O
(	O
the	O
idea	O
of	O
replacing	O
the	O
smoothing	B
factor	I
in	O
the	O
kernel	B
estimate	O
by	O
a	O
local	O
rank-based	O
value	O
such	O
as	O
rn	O
(	O
x	O
)	O
is	O
due	O
to	O
breiman	O
,	O
meisel	O
,	O
and	O
purcell	O
(	O
1977	O
)	O
.	O
)	O
for	O
the	O
kernel	B
k	O
=	O
iso	O
.	O
!	O
,	O
this	O
rule	B
coincides	O
with	O
the	O
k-nn	O
rule	B
.	O
for	O
regular	B
kernels	O
(	O
see	O
chapter	O
10	O
)	O
,	O
show	O
that	O
the	O
rule	B
remains	O
weakly	O
universally	O
consistent	O
whenever	O
k	O
-+	O
00	O
and	O
kin	O
-+	O
0	O
by	O
verifying	O
stone	O
's	O
conditions	O
of	O
theorem	O
6.3.	O
problem	O
11.19.	O
let	O
{	O
gn	O
}	O
be	O
a	O
weakly	O
universally	O
consistent	O
sequence	O
of	O
classifiers	O
.	O
split	O
the	O
data	O
sequence	O
dn	O
into	O
two	O
parts	O
:	O
dm	O
=	O
«	O
x1	O
'	O
yi	O
)	O
,	O
...	O
,	O
(	O
xnn	O
ym	O
»	O
and	O
tn-	O
m	O
=	O
«	O
xm+i	O
,	O
ym+1	O
)	O
,	O
•••	O
,	O
(	O
xn	O
'	O
yn	O
»	O
.	O
use	O
gn-m	O
and	O
the	O
second	O
part	O
tn	O
-	O
m	O
to	O
relabel	O
the	O
first	O
part	O
,	O
i.e.	O
,	O
define	O
y	O
:	O
=	O
gn-m	O
(	O
xi	O
,	O
tn-	O
m	O
)	O
for	O
i	O
=	O
1	O
,	O
...	O
,	O
m.	O
prove	O
that	O
the	O
i-nn	O
rule	B
based	O
on	O
the	O
data	O
(	O
xl	O
,	O
y	O
{	O
)	O
,	O
...	O
,	O
(	O
xm	O
,	O
y~	O
)	O
is	O
weakly	O
universally	O
consistent	O
whenever	O
m	O
-+	O
00	O
and	O
n	O
-	O
m	O
-+	O
00.	O
hint	O
:	O
use	O
problem	O
5.40.	O
problem	O
11.20.	O
consider	O
the	O
k-nn	O
rule	B
with	O
a	O
fixed	O
k	O
as	O
the	O
ancestral	B
rule	I
,	O
and	O
apply	O
the	O
1-nn	O
rule	B
using	O
the	O
relabeled	O
data	O
.	O
investigate	O
the	O
convergence	O
of	O
eln	O
.	O
is	O
the	O
limit	O
lknn	O
or	O
something	O
else	O
?	O
12	O
vapnik-chervonenkis	O
theory	O
12.1	O
empirical	B
error	I
minimization	O
in	O
this	O
chapter	O
we	O
select	O
a	O
decision	O
rule	B
from	O
a	O
class	O
of	O
rules	O
with	O
the	O
help	O
of	O
training	O
data	O
.	O
working	O
formally	O
,	O
let	O
c	O
be	O
a	O
class	O
of	O
functions	O
¢	O
:	O
nd	O
-+	O
{	O
a	O
,	O
1	O
}	O
.	O
one	O
wishes	O
to	O
select	O
a	O
function	O
from	O
c	O
with	O
small	O
error	O
probability	O
.	O
assume	O
that	O
the	O
training	O
data	O
dn	O
=	O
(	O
(	O
xl	O
,	O
y1	O
)	O
,	O
...	O
,	O
(	O
x	O
n	O
,	O
yn	O
)	O
)	O
are	O
given	O
to	O
pick	O
one	O
of	O
the	O
functions	O
from	O
c	O
to	O
be	O
used	O
as	O
a	O
classifier	O
.	O
perhaps	O
the	O
most	O
natural	O
way	O
of	O
selecting	O
a	O
function	O
is	O
to	O
minimize	O
the	O
empirical	B
error	I
probability	O
1	O
n	O
ln	O
(	O
¢	O
)	O
=	O
-	O
l	O
i	O
{	O
<	O
/l	O
(	O
xi	O
)	O
=	O
!	O
yd	O
..-	O
...	O
n	O
i=l	O
over	O
the	O
class	O
c.	O
denote	O
the	O
empirically	O
optimal	O
rule	O
by	O
¢~	O
:	O
¢~	O
=	O
argmin	O
ln	O
(	O
¢	O
)	O
.	O
<	O
/lec	O
thus	O
,	O
¢~	O
is	O
the	O
classifier	B
that	O
,	O
according	O
to	O
the	O
data	O
d	O
n	O
,	O
``	O
looks	O
best	O
''	O
among	O
the	O
classifiers	O
in	O
c.	O
this	O
idea	O
of	O
minimizing	O
the	O
empirical	B
risk	I
in	O
the	O
construction	O
of	O
a	O
rule	O
was	O
developed	O
to	O
great	O
extent	O
by	O
vapnik	O
and	O
chervonenkis	O
(	O
1971	O
;	O
1974c	O
;	O
1974a	O
;	O
1974b	O
)	O
.	O
intuitively	O
,	O
the	O
selected	O
classifier	B
¢~	O
should	O
be	O
good	O
in	O
the	O
sense	O
that	O
its	O
true	O
error	O
probability	O
l	O
(	O
¢~	O
)	O
=	O
p	O
{	O
¢~	O
(	O
x	O
)	O
=i	O
yidn	O
}	O
is	O
expected	O
to	O
be	O
close	O
to	O
the	O
optimal	O
error	O
probability	O
within	O
the	O
class	O
.	O
their	O
difference	O
is	O
the	O
quantity	O
that	O
primarily	O
interests	O
us	O
in	O
this	O
chapter	O
:	O
l	O
(	O
¢~	O
)	O
-	O
inf	O
l	O
(	O
¢	O
)	O
.	O
<	O
/lec	O
188	O
12.	O
vapnik-chervonenkis	O
theory	O
the	O
latter	O
difference	O
may	O
be	O
bounded	O
in	O
a	O
distribution-free	O
manner	O
,	O
and	O
a	O
rate	O
of	O
convergence	O
results	O
that	O
only	O
depends	O
on	O
the	O
structure	O
of	O
c.	O
while	O
this	O
is	O
very	O
exciting	O
,	O
we	O
must	O
add	O
that	O
l	O
(	O
¢~	O
)	O
may	O
be	O
far	O
away	O
from	O
the	O
bayes	O
error	O
l	O
*	O
.	O
note	O
that	O
l	O
(	O
¢~	O
)	O
-	O
l	O
*	O
=	O
(	O
l	O
(	O
¢~	O
)	O
-	O
inf	O
l	O
(	O
¢	O
»	O
)	O
+	O
(	O
inf	O
l	O
(	O
¢	O
)	O
-	O
l	O
*	O
)	O
.	O
¢ec	O
¢ec	O
the	O
size	O
of	O
c	O
is	O
a	O
compromise	O
:	O
when	O
c	O
is	O
large	O
,	O
inf¢ec	O
l	O
(	O
¢	O
)	O
may	O
be	O
close	O
to	O
l	O
*	O
,	O
but	O
the	O
former	O
error	O
,	O
the	O
estimation	B
error	I
,	O
is	O
probably	O
large	O
as	O
well	O
.	O
if	O
c	O
is	O
too	O
small	O
,	O
there	O
is	O
no	O
hope	O
to	O
make	O
the	O
approximation	B
error	I
inf¢ec	O
l	O
(	O
¢	O
)	O
-	O
l	O
*	O
small	O
.	O
for	O
example	O
,	O
if	O
c	O
is	O
the	O
class	O
of	O
all	O
(	O
measurable	O
)	O
decision	O
functions	O
,	O
then	O
we	O
can	O
always	O
find	O
a	O
classifier	O
in	O
c	O
with	O
zero	O
empirical	B
error	I
,	O
but	O
it	O
may	O
have	O
arbitrary	O
values	O
outside	O
of	O
the	O
points	O
xl	O
,	O
...	O
,	O
x	O
n	O
'	O
for	O
example	O
,	O
an	O
empirically	B
optimal	I
classifier	I
is	O
if	O
x	O
=	O
xi	O
,	O
i	O
=	O
1	O
,	O
...	O
,	O
n	O
otherwise	O
.	O
this	O
is	O
clearly	O
not	O
what	O
we	O
are	O
looking	O
for	O
.	O
this	O
phenomenon	O
is	O
called	O
overjitting	O
,	O
as	O
the	O
overly	O
large	O
class	O
c	O
overfits	O
the	O
data	O
.	O
we	O
will	O
give	O
precise	O
conditions	O
on	O
c	O
that	O
allow	O
us	O
to	O
avoid	O
this	O
anomaly	O
.	O
the	O
choice	O
of	O
c	O
such	O
that	O
inf¢ec	O
l	O
(	O
¢	O
)	O
is	O
close	O
to	O
l	O
*	O
has	O
been	O
the	O
subject	O
of	O
various	O
chapters	O
on	O
consistency-just	O
assume	O
that	O
c	O
is	O
allowed	O
to	O
grow	O
with	O
n	O
in	O
some	O
manner	O
.	O
here	O
we	O
take	O
the	O
point	O
of	O
view	O
that	O
c	O
is	O
fixed	O
,	O
and	O
that	O
we	O
have	O
to	O
live	O
with	O
the	O
functions	O
in	O
c.	O
the	O
best	O
we	O
may	O
then	O
hope	O
for	O
is	O
to	O
minimize	O
l	O
(	O
¢~	O
)	O
inf¢ec	O
l	O
(	O
¢	O
)	O
.	O
a	O
typical	O
situation	O
is	O
shown	O
in	O
figure	O
12.1.	O
picked	O
rule	B
best	O
rule	B
in	O
class	O
inf	O
<	O
pee	O
stimation	O
error	O
(	O
can	O
be	O
controlled	O
)	O
(	O
small	O
)	O
approximation	B
error	I
(	O
not	O
controllable	O
)	O
(	O
usually	O
larger	O
than	O
estimation	B
error	I
)	O
figure	O
12.1.	O
various	O
errors	O
in	O
empirical	O
classifier	B
selection	I
.	O
12.1	O
empirical	B
error	I
minimization	O
189	O
consider	O
first	O
a	O
finite	O
collection	O
c	O
,	O
and	O
assume	O
that	O
one	O
of	O
the	O
classifiers	O
in	O
c	O
has	O
zero	O
error	O
probability	O
,	O
that	O
is	O
,	O
min¢ec	O
l	O
(	O
¢	O
)	O
=	O
o.	O
then	O
clearly	O
,	O
ln	O
(	O
¢~	O
)	O
=	O
0	O
with	O
probability	O
one	O
.	O
we	O
then	O
have	O
the	O
following	O
performance	O
bound	O
:	O
theorem	B
12.1	O
.	O
(	O
vapnik	O
and	O
chervonenkis	O
(	O
l974c	O
»	O
.	O
assume	O
ici	O
<	O
00	O
and	O
min¢ec	O
l	O
(	O
¢	O
)	O
=	O
o.	O
then	O
for	O
every	O
nand	O
e	O
>	O
0	O
,	O
and	O
proof	O
.	O
clearly	O
,	O
e	O
{	O
l	O
(	O
¢~	O
)	O
}	O
:	O
:	O
:	O
;	O
1	O
+	O
log	O
ici	O
.	O
n	O
p	O
{	O
l	O
(	O
¢~	O
)	O
>	O
e	O
}	O
<	O
p	O
{	O
illax	O
¢ec	O
:	O
l	O
,	O
,	O
(	O
¢	O
)	O
=o	O
l	O
(	O
¢	O
)	O
>	O
e	O
i	O
e	O
{	O
i	O
{	O
max1	O
>	O
ec	O
:	O
ln	O
(	O
4	O
)	O
)	O
;	O
{	O
)	O
l	O
(	O
¢	O
»	O
e	O
}	O
}	O
~	O
e	O
{	O
<	O
g	O
:	O
t	O
1	O
{	O
i	O
;	O
''	O
(	O
¢	O
)	O
=fj	O
)	O
i	O
{	O
l	O
>	O
(	O
¢	O
»	O
,	O
)	O
i	O
¢ec	O
:	O
l	O
(	O
¢	O
»	O
e	O
<	O
l	O
p	O
{	O
ln	O
(	O
¢	O
)	O
=o	O
}	O
since	O
the	O
probability	O
that	O
no	O
(	O
xi	O
,	O
yi	O
)	O
pair	O
falls	O
in	O
the	O
set	O
{	O
(	O
x	O
,	O
y	O
)	O
:	O
¢	O
(	O
x	O
)	O
=i	O
y	O
}	O
is	O
less	O
than	O
(	O
1	O
-	O
e	O
)	O
n	O
if	O
the	O
probability	O
of	O
the	O
set	O
is	O
larger	O
than	O
e.	O
the	O
probability	O
inequality	B
of	O
the	O
theorem	B
follows	O
from	O
the	O
simple	O
inequality	O
1	O
-	O
x	O
:	O
:	O
:	O
;	O
e-x	O
.	O
to	O
bound	O
the	O
expected	O
error	O
probability	O
,	O
note	O
that	O
for	O
any	O
u	O
>	O
0	O
,	O
e	O
{	O
l	O
(	O
¢i~	O
)	O
}	O
=	O
f	O
''	O
p	O
{	O
l	O
(	O
<	O
/	O
>	O
~	O
)	O
>	O
t	O
}	O
dt	O
<	O
u	O
+	O
1	O
''	O
0	O
p	O
{	O
l	O
(	O
<	O
/	O
>	O
~	O
)	O
>	O
t	O
}	O
dt	O
<	O
u	O
+	O
ici/	O
.	O
oo	O
e-	O
nt	O
dt	O
=	O
u+-e	O
ici	O
-nu	O
.	O
n	O
since	O
u	O
was	O
arbitrary	O
,	O
we	O
may	O
choose	O
it	O
to	O
minimize	O
the	O
obtained	O
upper	O
bound	O
.	O
the	O
optimal	O
choice	O
is	O
u	O
=	O
log	O
ici/n	O
,	O
which	O
yields	O
the	O
desired	O
inequality	B
.	O
0	O
theorem	B
12.1	O
shows	O
that	O
empirical	B
selection	O
works	O
very	O
well	O
if	O
the	O
sample	O
size	O
n	O
is	O
much	O
larger	O
than	O
the	O
logarithm	O
of	O
the	O
size	O
of	O
the	O
family	O
c.	O
unfortunately	O
,	O
the	O
190	O
12.	O
vapnik-chervonenkis	O
theory	O
assumption	O
on	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
that	O
is	O
,	O
that	O
min¢ec	O
l	O
(	O
¢	O
)	O
=	O
0	O
,	O
is	O
very	O
restrictive	O
.	O
in	O
the	O
sequel	O
we	O
drop	O
this	O
assumption	O
,	O
and	O
deal	O
with	O
the	O
distribution	B
(	O
cid:173	O
)	O
free	O
problem	O
.	O
one	O
of	O
our	O
main	O
tools	O
is	O
taken	O
from	O
lemma	O
8.2	O
:	O
this	O
leads	O
to	O
the	O
study	O
of	O
uniform	O
deviations	O
of	O
relative	O
frequencies	O
from	O
their	O
probabilities	O
by	O
the	O
following	O
simple	O
observation	O
:	O
let	O
v	O
be	O
a	O
probability	O
measure	B
of	O
(	O
x	O
,	O
y	O
)	O
on	O
nd	O
x	O
{	O
o	O
,	O
i	O
}	O
,	O
and	O
let	O
vn	O
be	O
the	O
empirical	B
measure	I
based	O
upon	O
dn	O
.	O
that	O
is	O
,	O
for	O
any	O
fixed	O
measurable	O
set	O
a	O
c	O
nd	O
x	O
{	O
o	O
,	O
i	O
}	O
,	O
v	O
(	O
a	O
)	O
=	O
p	O
{	O
(	O
x	O
,	O
y	O
)	O
e	O
a	O
}	O
,	O
and	O
vn	O
(	O
a	O
)	O
=	O
~	O
l7=1	O
i	O
{	O
(	O
xi	O
,	O
yi	O
)	O
eaj	O
.	O
then	O
l	O
(	O
¢	O
)	O
=	O
v	O
(	O
{	O
(	O
x	O
,	O
y	O
)	O
:	O
¢	O
(	O
x	O
)	O
i	O
y	O
}	O
)	O
is	O
just	O
the	O
v-measure	O
of	O
the	O
set	O
of	O
pairs	O
(	O
x	O
,	O
y	O
)	O
e	O
n	O
d	O
x	O
{	O
o	O
,	O
i	O
}	O
,	O
where	O
¢	O
(	O
x	O
)	O
i	O
y.	O
formally	O
,	O
l	O
(	O
¢	O
)	O
is	O
the	O
v-measure	O
of	O
the	O
set	O
{	O
{	O
x	O
:	O
¢	O
(	O
x	O
)	O
=	O
i	O
}	O
x	O
{	O
o	O
}	O
}	O
u	O
{	O
{	O
x	O
:	O
¢	O
(	O
x	O
)	O
=	O
o	O
}	O
x	O
{	O
i	O
}	O
}	O
.	O
similarly	O
,	O
ln	O
(	O
¢	O
)	O
=	O
vn	O
(	O
{	O
(	O
x	O
,	O
y	O
)	O
:	O
¢	O
(	O
x	O
)	O
i	O
y	O
}	O
)	O
.	O
thus	O
,	O
sup	O
iln	O
(	O
¢	O
)	O
-	O
l	O
(	O
¢	O
)	O
i	O
=	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
i	O
,	O
¢ec	O
aea	O
where	O
a	O
is	O
the	O
collection	O
of	O
all	O
sets	O
{	O
{	O
x	O
:	O
¢	O
(	O
x	O
)	O
=	O
i	O
}	O
x	O
{	O
o	O
}	O
}	O
u	O
{	O
{	O
x	O
:	O
¢	O
(	O
x	O
)	O
=	O
o	O
}	O
x	O
{	O
i	O
}	O
}	O
,	O
¢	O
e	O
c.	O
for	O
a	O
fixed	O
set	O
a	O
,	O
for	O
any	O
probability	O
measure	B
v	O
,	O
by	O
the	O
law	O
of	O
large	O
numbers	O
vn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
--	O
-+	O
°	O
almost	O
surely	O
as	O
n	O
--	O
-+	O
00.	O
moreover	O
,	O
by	O
hoeffding	O
's	O
inequality	B
(	O
theorem	B
8.1	O
)	O
,	O
p	O
{	O
lvn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
1	O
>	O
e	O
}	O
:	O
:	O
:	O
:	O
:	O
2e-2ne2	O
.	O
however	O
,	O
it	O
is	O
a	O
much	O
harder	O
problem	O
to	O
obtain	O
such	O
results	O
for	O
supaea	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
i.	O
if	O
the	O
class	O
of	O
sets	O
a	O
(	O
or	O
,	O
analogously	O
,	O
in	O
the	O
pattern	O
recognition	O
context	O
,	O
c	O
)	O
is	O
of	O
finite	O
cardinality	O
,	O
then	O
the	O
union	O
bound	O
trivially	O
gives	O
however	O
,	O
if	O
a	O
contains	O
infinitely	O
many	O
sets	O
(	O
as	O
in	O
many	O
of	O
the	O
interesting	O
cases	O
)	O
then	O
the	O
problem	O
becomes	O
nontrivial	O
,	O
spawning	O
a	O
vast	O
literature	O
.	O
the	O
most	O
pow	O
(	O
cid:173	O
)	O
erful	O
weapons	O
to	O
attack	O
these	O
problems	O
are	O
distribution-free	O
large	O
deviation-type	O
inequalities	O
first	O
proved	O
by	O
vapnik	O
and	O
chervonenkis	O
(	O
1971	O
)	O
in	O
their	O
pigneering	O
work	O
.	O
however	O
,	O
in	O
some	O
situations	O
,	O
we	O
can	O
handle	O
the	O
problem	O
in	O
a	O
much	O
simpler	O
way	O
.	O
we	O
have	O
already	O
seen	O
such	O
an	O
example	O
in	O
section	O
4.5	O
.	O
12.2	O
fingering	B
191	O
12.2	O
fingering	B
recall	O
that	O
in	O
section	O
4.5	O
we	O
studied	O
a	O
specific	O
rule	B
that	O
selects	O
a	O
linear	O
classifier	B
by	O
minimizing	O
the	O
empirical	B
error	I
.	O
the	O
performance	O
bounds	O
provided	O
by	O
theorems	O
4.5	O
and	O
4.6	O
show	O
that	O
the	O
selected	O
rule	B
performs	O
very	O
closely	O
to	O
the	O
best	O
possible	O
linear	O
rule	O
.	O
these	O
bounds	O
apply	O
only	O
to	O
the	O
specific	O
algorithm	B
used	O
to	O
find	O
the	O
em	O
(	O
cid:173	O
)	O
pirical	O
minima-we	O
have	O
not	O
showed	O
that	O
any	O
classifier	B
minimizing	O
the	O
empirical	B
error	I
performs	O
well	O
.	O
this	O
matter	O
will	O
be	O
dealt	O
with	O
in	O
later	O
sections	O
.	O
in	O
this	O
section	O
we	O
extend	O
theorems	O
4.5	O
and	O
4.6	O
to	O
classes	O
other	O
than	O
linear	O
classifiers	O
.	O
let	O
c	O
be	O
the	O
class	O
of	O
classifiers	O
assigning	O
1	O
to	O
those	O
x	O
contained	O
in	O
a	O
closed	O
hy	O
(	O
cid:173	O
)	O
perrectangle	O
,	O
and	O
0	O
to	O
all	O
other	O
points	O
.	O
then	O
a	O
classifier	O
minimizing	O
the	O
empirical	B
error	I
ln	O
(	O
¢	O
)	O
over	O
all	O
¢	O
e	O
c	O
may	O
be	O
obtained	O
by	O
the	O
following	O
algorithm	B
:	O
to	O
each	O
2d-tuple	O
(	O
xii	O
'	O
...	O
,	O
x	O
i2d	O
)	O
of	O
points	O
from	O
xl	O
,	O
...	O
,	O
x	O
n	O
,	O
assign	O
the	O
smallest	O
hyperrect	O
(	O
cid:173	O
)	O
angle	O
containing	O
these	O
points	O
.	O
if	O
we	O
assume	O
that	O
x	O
has	O
a	O
density	O
,	O
then	O
the	O
points	O
x	O
1	O
,	O
...	O
,	O
xn	O
are	O
in	O
general	O
position	O
with	O
probability	O
one	O
.	O
this	O
way	O
we	O
obtain	O
at	O
most	O
(	O
;	O
d	O
)	O
sets	O
.	O
let	O
¢i	O
be	O
the	O
classifier	B
corresponding	O
to	O
the	O
i-th	O
such	O
hyperrectan	O
(	O
cid:173	O
)	O
gle	O
,	O
that	O
is	O
,	O
the	O
one	O
assigning	O
1	O
to	O
those	O
x	O
contained	O
in	O
the	O
hyperrectangle	O
,	O
and	O
0	O
to	O
other	O
points	O
.	O
clearly	O
,	O
for	O
each	O
¢	O
e	O
c	O
,	O
there	O
exists	O
a	O
¢i	O
,	O
i	O
=	O
1	O
,	O
...	O
,	O
(	O
2~	O
)	O
'	O
such	O
that	O
for	O
all	O
x	O
j	O
,	O
except	O
possibly	O
for	O
those	O
on	O
the	O
boundary	O
of	O
the	O
hyperrectangle	O
.	O
since	O
the	O
points	O
are	O
in	O
general	O
position	O
,	O
there	O
are	O
at	O
most	O
2d	O
such	O
exceptional	O
points	O
.	O
therefore	O
,	O
if	O
we	O
select	O
a	O
classifier	O
;	O
;	O
;	O
among	O
¢1	O
,	O
...	O
,	O
¢g~	O
)	O
to	O
minimize	O
the	O
empirical	B
error	I
,	O
then	O
it	O
approximately	O
minimizes	O
the	O
empirical	B
error	I
over	O
the	O
whole	O
class	O
c	O
as	O
well	O
.	O
a	O
quick	O
scan	O
through	O
the	O
proof	O
of	O
theorem	O
4.5	O
reveals	O
that	O
by	O
similar	O
arguments	O
we	O
may	O
obtain	O
the	O
performance	O
bound	O
for	O
n	O
:	O
:	O
:	O
2d	O
and	O
e	O
:	O
:	O
:	O
4d/n	O
.	O
the	O
idea	O
may	O
be	O
generalized	B
.	O
it	O
always	O
works	O
,	O
if	O
for	O
some	O
k	O
,	O
k-tuples	O
of	O
points	O
determine	O
classifiers	O
from	O
c	O
such	O
that	O
no	O
matter	O
where	O
the	O
other	O
data	O
points	O
fall	O
,	O
the	O
minimal	O
empirical	B
error	I
over	O
these	O
sets	O
coincides	O
with	O
the	O
overall	O
minimum	O
.	O
then	O
we	O
may	O
fix	O
these	O
sets-	O
''	O
put	O
our	O
finger	O
on	O
them	O
''	O
-and	O
look	O
for	O
the	O
empirical	B
minima	O
over	O
this	O
finite	O
collection	O
.	O
the	O
next	O
theorems	O
,	O
whose	O
proofs	O
are	O
left	O
as	O
an	O
exercise	O
(	O
problem	O
12.2	O
)	O
,	O
show	O
that	O
if	O
c	O
has	O
this	O
property	O
,	O
then	O
``	O
fingering	B
''	O
works	O
extremely	O
well	O
whenever	O
n	O
»	O
k.	O
theorem	B
12.2.	O
assume	O
that	O
the	O
class	O
c	O
of	O
classifiers	O
has	O
the	O
following	O
property	O
:	O
:	O
{	O
rd	O
)	O
k	O
~	O
c	O
such	O
that	O
for	O
all	O
for	O
some	O
integer	O
k	O
there	O
exists	O
a	O
function	O
\11	O
xl	O
,	O
•••	O
,	O
xn	O
e	O
rd	O
and	O
all	O
¢	O
e	O
c	O
,	O
there	O
exists	O
a	O
k-tuple	O
ii	O
,	O
...	O
,	O
ik	O
e	O
{	O
i	O
,	O
...	O
,	O
n	O
}	O
of	O
different	O
indices	O
such	O
that	O
w	O
(	O
xi	O
''	O
...	O
,	O
xh	O
)	O
(	O
x	O
)	O
=	O
¢	O
(	O
xj	O
)	O
for	O
all	O
j	O
=	O
1	O
,	O
...	O
,	O
n	O
with	O
j	O
=	O
!	O
ii	O
,	O
1=	O
1	O
,	O
...	O
,	O
k	O
192	O
12.	O
vapnik-chervonenkis	O
theory	O
with	O
probability	O
one	O
.	O
let	O
¢	O
be	O
found	O
by	O
fingering	B
,	O
that	O
is	O
,	O
by	O
empirical	B
error	I
mini	O
(	O
cid:173	O
)	O
mization	O
over	O
the	O
collection	O
of	O
n	O
!	O
/	O
(	O
n	O
-	O
k	O
)	O
!	O
classifiers	O
of	O
the	O
form	O
ii	O
,	O
...	O
,	O
ik	O
e	O
{	O
1	O
,	O
...	O
,	O
n	O
}	O
,	O
different	O
.	O
then	O
for	O
n	O
2	O
:	O
k	O
and	O
2k	O
/	O
n	O
.	O
:	O
s	O
e	O
.	O
:	O
s	O
1	O
,	O
p	O
{	O
l	O
(	O
4j	O
)	O
-	O
inf	O
l	O
(	O
¢	O
)	O
>	O
e	O
}	O
.	O
:	O
s	O
e2ke	O
(	O
n	O
k	O
+	O
1	O
)	O
e-	O
epec	O
ne2	O
/2	O
.	O
moreover	O
,	O
if	O
n	O
2	O
:	O
k	O
,	O
then	O
e	O
{	O
l	O
(	O
¢	O
)	O
-	O
inf	O
l	O
(	O
¢	O
)	O
}	O
.	O
:	O
s	O
epec	O
2	O
(	O
k	O
+	O
1	O
)	O
log	O
n	O
+	O
(	O
2k	O
+	O
2	O
)	O
n	O
the	O
smallest	O
k	O
for	O
which	O
c	O
has	O
the	O
property	O
described	O
in	O
the	O
theorem	B
may	O
be	O
called	O
the	O
fingering	B
dimension	I
of	O
c.	O
in	O
most	O
interesting	O
cases	O
,	O
it	O
is	O
independent	O
of	O
n.	O
problem	O
12.3	O
offers	O
a	O
few	O
such	O
classes	O
.	O
we	O
will	O
see	O
later	O
in	O
this	O
chapter	O
that	O
the	O
fingering	B
dimension	I
is	O
closely	O
related	O
the	O
so-called	O
vc	B
dimension	I
of	O
c	O
(	O
see	O
also	O
problem	O
12.4	O
)	O
.	O
again	O
,	O
we	O
get	O
much	O
smaller	O
errors	O
if	O
infepec	O
l	O
(	O
¢	O
)	O
=	O
o.	O
the	O
next	O
inequality	B
generalizes	O
theorem	B
4.6.	O
theorem	B
12.3.	O
assume	O
that	O
c	O
has	O
the	O
property	O
described	O
in	O
theorem	O
12.2	O
with	O
fingering	O
dimension	B
k.	O
assume	O
,	O
in	O
addition	O
,	O
that	O
infepec	O
l	O
(	O
¢	O
)	O
=	O
o.	O
then	O
for	O
all	O
n	O
and	O
e	O
,	O
and	O
--	O
--	O
}	O
e	O
l	O
(	O
¢	O
)	O
{	O
.	O
:	O
s	O
k	O
logn	O
+	O
2	O
.	O
n-k	O
remark	O
.	O
even	O
though	O
the	O
results	O
in	O
the	O
next	O
few	O
sections	O
based	O
on	O
the	O
vapnik	O
(	O
cid:173	O
)	O
chervonenkis	O
theory	O
supersede	O
those	O
of	O
this	O
section	O
(	O
by	O
requiring	O
less	O
from	O
the	O
class	O
c	O
and	O
being	O
able	O
to	O
bound	O
the	O
error	O
of	O
any	O
classifier	B
minimizing	O
the	O
empirical	B
risk	I
)	O
,	O
we	O
must	O
remark	O
that	O
the	O
exponents	O
in	O
the	O
above	O
probability	O
inequalities	O
are	O
the	O
best	O
possible	O
,	O
and	O
bounds	O
of	O
the	O
same	O
type	O
for	O
the	O
general	O
case	O
can	O
only	O
be	O
obtained	O
with	O
significantly	O
more	O
effort	O
.	O
d	O
12.3	O
the	O
glivenko-cantelli	O
theorem	B
in	O
the	O
next	O
two	O
sections	O
,	O
we	O
prove	O
the	O
vapnik-chervonenkis	O
inequality	B
,	O
a	O
powerful	O
generalization	O
of	O
the	O
classical	O
glivenko-cantelli	O
theorem	B
.	O
it	O
provides	O
upper	O
bounds	O
on	O
random	O
variables	O
of	O
the	O
type	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
i.	O
aea	O
12.3	O
the	O
glivenko-cantelli	O
theorem	B
193	O
as	O
we	O
noted	O
in	O
section	O
12.1	O
,	O
such	O
bounds	O
yield	O
performance	O
bounds	O
for	O
any	O
classifier	B
selected	O
by	O
minimizing	O
the	O
empirical	B
error	I
.	O
to	O
make	O
the	O
material	O
more	O
digestible	O
,	O
we	O
first	O
present	O
the	O
main	O
ideas	O
in	O
a	O
simple	O
one-dimensional	O
setting	O
,	O
and	O
then	O
prove	O
the	O
general	O
theorem	B
in	O
the	O
next	O
section	O
.	O
we	O
drop	O
the	O
pattern	O
recognition	O
setting	O
momentarily	O
,	O
and	O
return	O
to	O
probability	O
theory	O
.	O
the	O
following	O
theorem	B
is	O
sometimes	O
referred	O
to	O
as	O
the	O
fundamental	B
theorem	I
of	I
mathematical	I
statistics	O
,	O
stating	O
uniform	B
almost	O
sure	O
convergence	O
of	O
the	O
empirical	B
distribution	O
function	O
to	O
the	O
true	O
one	O
:	O
theorem	B
12.4	O
.	O
(	O
glivenko-cantelli	O
theorem	B
)	O
.	O
let	O
zl	O
,	O
...	O
,	O
zn	O
be	O
i.i.d	O
.	O
real	O
(	O
cid:173	O
)	O
valued	O
random	O
variables	O
with	O
distribution	O
function	O
f	O
(	O
z	O
)	O
=	O
p	O
{	O
zl	O
:	O
:	O
:	O
:	O
:	O
:	O
z	O
}	O
.	O
denote	O
the	O
standard	O
empirical	O
distribution	O
function	O
by	O
then	O
p	O
{	O
sup	O
if	O
(	O
z	O
)	O
-	O
fncz	O
)	O
1	O
>	O
e	O
)	O
:	O
:	O
:	O
:	O
:	O
:	O
8cn	O
+	O
1	O
)	O
e-ne2/32	O
,	O
zer	O
and	O
,	O
in	O
particular	O
,	O
by	O
the	O
borel-cantelli	O
lemma	O
,	O
lim	O
sup	O
if	O
(	O
z	O
)	O
-	O
fn	O
(	O
z	O
)	O
1	O
=	O
0	O
with	O
probability	O
one	O
.	O
n-foo	O
zer	O
proof	O
.	O
the	O
proof	O
presented	O
here	O
is	O
not	O
the	O
simplest	O
possible	O
,	O
but	O
it	O
contains	O
the	O
main	O
ideas	O
leading	O
to	O
a	O
powerful	O
generalization	O
.	O
introduce	O
the	O
notation	O
v	O
(	O
a	O
)	O
=	O
p	O
{	O
zl	O
e	O
a	O
}	O
and	O
vnca	O
)	O
=	O
(	O
lin	O
)	O
2	O
:	O
;	O
:	O
::1	O
i	O
{	O
zjea	O
}	O
for	O
all	O
measurable	O
sets	O
a	O
c	O
r.	O
let	O
a	O
denote	O
the	O
class	O
of	O
sets	O
of	O
form	O
(	O
-00	O
,	O
z	O
]	O
for	O
z	O
e	O
r.	O
with	O
these	O
notations	O
,	O
sup	O
if	O
(	O
z	O
)	O
-	O
fn	O
(	O
z	O
)	O
1	O
=	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
i.	O
zer	O
aea	O
we	O
prove	O
the	O
theorem	B
in	O
several	O
steps	O
,	O
following	O
symmetrization	B
ideas	O
of	O
dudley	O
(	O
1978	O
)	O
,	O
and	O
pollard	O
(	O
l984	O
)	O
.	O
we	O
assume	O
that	O
ne	O
2	O
:	O
:	O
:	O
2	O
,	O
since	O
otherwise	O
the	O
bound	O
is	O
trivial	O
.	O
in	O
the	O
first	O
step	O
we	O
introduce	O
a	O
symmetrization	O
.	O
step	O
1.	O
first	O
symmetrization	B
by	O
a	O
ghost	O
sample	O
.	O
define	O
the	O
random	O
variables	O
zi	O
,	O
...	O
,	O
z~	O
e	O
r	O
such	O
that	O
zl	O
,	O
''	O
''	O
zn	O
'	O
zi	O
,	O
...	O
,	O
z:1	O
are	O
all	O
independent	O
and	O
iden	O
(	O
cid:173	O
)	O
tically	O
distributed	O
.	O
denote	O
by	O
v~	O
the	O
empirical	B
measure	I
corresponding	O
to	O
the	O
new	O
sample	O
:	O
then	O
for	O
ne	O
2	O
:	O
:	O
:	O
2	O
we	O
have	O
p	O
{	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
1	O
>	O
e	O
)	O
:	O
:	O
:	O
:	O
:	O
:	O
2p	O
{	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
;	O
/a	O
)	O
i	O
>	O
~	O
)	O
.	O
aea	O
aea	O
2	O
194	O
12.	O
vapnik-chervonenkis	O
theory	O
to	O
see	O
this	O
,	O
let	O
a	O
*	O
e	O
a	O
be	O
a	O
set	O
for	O
which	O
ivn	O
(	O
a	O
*	O
)	O
-	O
v	O
(	O
a	O
*	O
)	O
1	O
>	O
e	O
if	O
such	O
a	O
set	O
exists	O
,	O
and	O
let	O
a	O
*	O
be	O
a	O
fixed	O
set	O
in	O
a	O
otherwise	O
.	O
then	O
p	O
{	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v~	O
(	O
a	O
)	O
1	O
>	O
e	O
12	O
}	O
aea	O
>	O
p	O
{	O
lvn	O
(	O
a	O
*	O
)	O
-	O
v~	O
(	O
a	O
*	O
)	O
1	O
>	O
e	O
12	O
}	O
>	O
p	O
{	O
lvn	O
(	O
a	O
*	O
)	O
-	O
v	O
(	O
a	O
*	O
)	O
1	O
>	O
e	O
,	O
iv~	O
(	O
a	O
*	O
)	O
-	O
v	O
(	O
a	O
*	O
)	O
1	O
<	O
~	O
}	O
=	O
e	O
{	O
i	O
{	O
lvn	O
(	O
a*	O
)	O
-v	O
(	O
a*	O
)	O
i	O
>	O
e	O
}	O
p	O
{	O
iv~	O
(	O
a	O
*	O
)	O
v	O
(	O
a	O
*	O
)	O
1	O
<	O
~	O
j	O
zl	O
,	O
...	O
,	O
zn	O
}	O
}	O
.	O
the	O
conditional	O
probability	O
inside	O
may	O
be	O
bounded	O
by	O
chebyshev	O
's	O
inequality	B
as	O
follows	O
:	O
p	O
{	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
1	O
<	O
2	O
zl	O
,	O
...	O
,	O
zn	O
}	O
ej	O
*	O
*	O
f	O
>	O
1	O
_	O
v	O
(	O
a	O
*	O
)	O
(	O
1	O
-	O
v	O
(	O
a	O
*	O
)	O
)	O
ne2/4	O
1	O
>	O
1	O
-	O
-	O
>	O
-	O
ne	O
2	O
-	O
2	O
1	O
whenever	O
ne	O
2	O
:	O
:	O
:	O
:	O
2.	O
in	O
summary	O
,	O
p	O
{	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v~	O
(	O
a	O
)	O
1	O
>	O
e	O
12	O
}	O
aea	O
1	O
:	O
:	O
:	O
:	O
2	O
p	O
{	O
lvn	O
(	O
a	O
*	O
)	O
-	O
v	O
(	O
a	O
*	O
)	O
1	O
>	O
e	O
}	O
>	O
~	O
p	O
{	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
1	O
>	O
e	O
}	O
.	O
2	O
aea	O
step	O
2.	O
second	O
symmetrization	B
by	O
random	O
signs	O
.	O
let	O
(	O
)	O
1	O
,	O
...	O
,	O
(	O
)	O
n	O
be	O
i.i.d	O
.	O
sign	O
variables	O
,	O
independent	O
of	O
zl	O
,	O
...	O
,	O
zn	O
and	O
z~	O
,	O
...	O
,	O
z~	O
,	O
with	O
p	O
{	O
(	O
)	O
i	O
=	O
-i	O
}	O
=	O
p	O
{	O
(	O
)	O
i	O
=	O
i	O
}	O
=	O
1/2	O
.	O
clearly	O
,	O
because	O
zl	O
,	O
zi	O
,	O
...	O
,	O
zn	O
,	O
z~	O
are	O
all	O
independent	O
and	O
identically	O
distributed	O
,	O
the	O
distribution	B
of	O
is	O
the	O
same	O
as	O
the	O
distribution	B
of	O
!	O
~~	O
itua	O
(	O
zi	O
)	O
-	O
ia	O
(	O
z	O
;	O
)	O
)	O
i	O
sup	O
it	O
(	O
)	O
iua	O
(	O
zi	O
)	O
-	O
ia	O
(	O
z	O
;	O
)	O
)	O
i·	O
aea	O
i=l	O
thus	O
,	O
by	O
step	O
1	O
,	O
p	O
{	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
1	O
>	O
e	O
}	O
aea	O
:	O
s	O
2p	O
{	O
sup	O
1	O
itua	O
(	O
zi	O
)	O
-	O
aea	O
n	O
i=l	O
ia	O
(	O
z	O
)	O
)	O
i	O
>	O
:	O
.	O
}	O
2	O
12.3	O
the	O
glivenko-cantelli	O
theorem	B
195	O
simply	O
applying	O
the	O
union	O
bound	O
,	O
we	O
can	O
remove	O
the	O
auxiliary	O
random	O
variables	O
zi	O
,	O
...	O
,	O
z~	O
:	O
step	O
3.	O
conditioning	O
.	O
to	O
bound	O
the	O
probability	O
we	O
condition	O
on	O
z	O
1	O
,	O
...	O
,	O
zn	O
.	O
fix	O
z	O
1	O
,	O
...	O
,	O
zn	O
e	O
n	O
d	O
,	O
and	O
note	O
that	O
as	O
z	O
ranges	O
over	O
n	O
,	O
the	O
number	O
of	O
different	O
vectors	O
(	O
i	O
{	O
zl	O
:	O
:	O
:	O
z	O
}	O
,	O
...	O
,	O
i	O
{	O
zn	O
:	O
:	O
:	O
z	O
}	O
)	O
is	O
at	O
most	O
n	O
+	O
1.	O
thus	O
,	O
conditional	O
on	O
zl	O
,	O
...	O
,	O
zn	O
,	O
the	O
supremum	O
in	O
the	O
probability	O
above	O
is	O
just	O
a	O
maximum	O
taken	O
over	O
at	O
most	O
n	O
+	O
1	O
random	O
variables	O
.	O
thus	O
,	O
applying	O
the	O
union	O
bound	O
gives	O
with	O
the	O
supremum	O
now	O
outside	O
the	O
probability	O
,	O
it	O
suffices	O
to	O
find	O
an	O
exponential	B
bound	O
on	O
the	O
conditional	O
probability	O
step	O
4.	O
hoeffdino	O
's	O
inequality	B
.	O
with	O
zl	O
,	O
...	O
,	O
zn	O
fixed	O
,	O
l7=1	O
aja	O
(	O
zi	O
)	O
is	O
the	O
sum	O
of	O
n	O
independent	O
zero	O
mean	O
random	O
variables	O
bounded	O
between	O
-1	O
and	O
1.	O
therefore	O
,	O
theorem	B
8.1	O
applies	O
in	O
a	O
straightforward	O
manner	O
:	O
thus	O
,	O
p	O
{	O
1	O
i	O
~	O
i	O
e	O
i	O
4	O
sup	O
-	O
~aja	O
(	O
zi	O
)	O
>	O
-	O
zl	O
,	O
...	O
,	O
zn	O
aea	O
n	O
i=l	O
}	O
:	O
s	O
2	O
(	O
n	O
+	O
l	O
)	O
e	O
-ne	O
2	O
.	O
/32	O
196	O
12.	O
vapnik-chervonenkis	O
theory	O
taking	O
the	O
expected	O
value	O
on	O
both	O
sides	O
we	O
have	O
in	O
summary	O
,	O
p	O
{	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
1	O
>	O
e	O
}	O
:	O
:	O
:	O
:	O
:	O
8	O
(	O
n	O
+	O
l	O
)	O
e-ne2/32	O
.	O
d	O
aea	O
12.4	O
uniform	B
deviations	I
of	O
relative	O
frequencies	O
from	O
probabilities	O
in	O
this	O
section	O
we	O
prove	O
the	O
vapnik-chervonenkis	O
inequality	B
,	O
a	O
mighty	O
general	O
(	O
cid:173	O
)	O
ization	O
of	O
theorem	O
12.4.	O
in	O
the	O
proof	O
we	O
need	O
only	O
a	O
slight	O
adjustment	O
of	O
the	O
proof	O
above	O
.	O
in	O
the	O
general	O
setting	O
,	O
let	O
the	O
independent	O
identically	O
distributed	O
random	O
variables	O
zl	O
,	O
...	O
,	O
zn	O
take	O
their	O
values	O
from	O
rd	O
.	O
again	O
,	O
we	O
use	O
the	O
no	O
(	O
cid:173	O
)	O
tation	O
v	O
(	O
a	O
)	O
=	O
p	O
{	O
zi	O
e	O
a	O
}	O
and	O
vn	O
(	O
a	O
)	O
=	O
(	O
lin	O
)	O
l~=l	O
i	O
{	O
zjea	O
}	O
for	O
all	O
measurable	O
sets	O
a	O
c	O
rd	O
.	O
the	O
vapnik-chervonenkis	O
theory	O
begins	O
with	O
the	O
concepts	O
of	O
shatter	O
coefficient	O
and	O
vapnik-chervonenkis	O
(	O
or	O
vc	O
)	O
dimension	B
:	O
definition	O
12.1.	O
let	O
a	O
be	O
a	O
collection	O
of	O
measurable	O
sets	O
.	O
for	O
(	O
zl	O
,	O
...	O
,	O
zn	O
)	O
e	O
{	O
rd	O
}	O
n	O
,	O
let	O
n	O
a	O
(	O
zl	O
,	O
...	O
,	O
zn	O
)	O
be	O
the	O
number	O
of	O
different	O
sets	O
in	O
{	O
{	O
z	O
1	O
,	O
...	O
,	O
zn	O
}	O
n	O
a	O
;	O
a	O
e	O
a	O
}	O
.	O
the	O
n-th	O
shatter	B
coefficient	I
of	O
a	O
is	O
that	O
is	O
,	O
the	O
shatter	B
coefficient	I
is	O
the	O
maximal	O
number	O
of	O
different	O
subsets	O
of	O
n	O
points	O
that	O
can	O
be	O
picked	O
out	O
by	O
the	O
class	O
of	O
sets	O
a.	O
the	O
shatter	O
coefficients	O
measure	B
the	O
richness	O
of	O
the	O
class	O
a.	O
clearly	O
,	O
sea	O
,	O
n	O
)	O
:	O
:	O
:	O
:	O
:	O
2n	O
,	O
as	O
there	O
are	O
2n	O
subsets	O
of	O
a	O
set	O
with	O
n	O
elements	O
.	O
if	O
n	O
a	O
(	O
zl	O
,	O
...	O
,	O
zn	O
)	O
=	O
2n	O
for	O
some	O
(	O
zl	O
,	O
...	O
,	O
zn	O
)	O
,	O
then	O
we	O
say	O
that	O
a	O
shatters	O
{	O
zl	O
,	O
...	O
,	O
zn	O
}	O
.	O
if	O
sea	O
,	O
n	O
)	O
<	O
2n	O
,	O
then	O
any	O
set	O
of	O
n	O
points	O
has	O
a	O
subset	O
such	O
that	O
there	O
is	O
no	O
set	O
in	O
a	O
that	O
contains	O
exactly	O
that	O
subset	O
of	O
the	O
n	O
points	O
.	O
clearly	O
,	O
if	O
sea	O
,	O
k	O
)	O
<	O
2k	O
for	O
some	O
integer	O
k	O
,	O
then	O
sea	O
,	O
n	O
)	O
<	O
2n	O
for	O
all	O
n	O
>	O
k.	O
the	O
first	O
time	O
when	O
this	O
happens	O
is	O
important	O
:	O
definition	O
12.2.	O
let	O
a	O
be	O
a	O
collection	O
of	O
sets	O
with	O
iai	O
:	O
:	O
:	O
:	O
:	O
2.	O
the	O
largest	O
integer	O
k	O
:	O
:	O
:	O
:	O
:	O
1	O
for	O
which	O
sea	O
,	O
k	O
)	O
=	O
2k	O
is	O
denoted	O
by	O
va	O
,	O
and	O
it	O
is	O
called	O
the	O
vapnik	O
(	O
cid:173	O
)	O
chervonenkis	O
dimension	B
(	O
or	O
vc	B
dimension	I
)	O
of	O
the	O
class	O
a.	O
if	O
sea	O
,	O
n	O
)	O
=	O
2n	O
for	O
all	O
n	O
,	O
then	O
by	O
definition	O
,	O
va	O
=	O
00	O
.	O
12.4	O
uniform	B
deviations	I
of	O
relative	O
frequencies	O
from	O
probabilities	O
197	O
for	O
example	O
,	O
if	O
a	O
contains	O
allhalfiines	O
of	O
form	O
(	O
-00	O
,	O
x	O
]	O
,	O
x	O
e	O
r	O
,	O
thens	O
(	O
a	O
,	O
2	O
)	O
=	O
3	O
<	O
22	O
,	O
and	O
va	O
=	O
1.	O
this	O
is	O
easily	O
seen	O
by	O
observing	O
that	O
for	O
any	O
two	O
different	O
points	O
zl	O
<	O
z2	O
there	O
is	O
no	O
set	O
of	O
the	O
form	O
(	O
-00	O
,	O
x	O
]	O
that	O
contains	O
z2	O
,	O
but	O
not	O
zl	O
.	O
a	O
class	O
of	O
sets	O
a	O
for	O
which	O
va	O
<	O
00	O
is	O
called	O
a	O
vapnik-chervonenkis	O
(	O
orve	O
)	O
class	O
.	O
in	O
a	O
sense	O
,	O
v	O
a	O
may	O
be	O
considered	O
as	O
the	O
complexity	O
,	O
or	O
size	O
,	O
of	O
a.	O
several	O
properties	O
of	O
the	O
shatter	O
coefficients	O
and	O
the	O
ve	O
dimension	B
will	O
be	O
shown	O
in	O
chapter	O
13.	O
the	O
main	O
purpose	O
of	O
this	O
section	O
is	O
to	O
prove	O
the	O
following	O
important	O
result	O
by	O
vapnik	O
and	O
chervonenkis	O
(	O
1971	O
)	O
:	O
theorem	B
12.5	O
.	O
(	O
vapnik	O
and	O
chervonenkis	O
(	O
1971	O
)	O
)	O
.	O
for	O
any	O
probability	O
measure	B
v	O
and	O
class	O
of	O
sets	O
a	O
,	O
and	O
for	O
any	O
nand	O
e	O
>	O
0	O
,	O
p	O
{	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
1	O
>	O
e	O
}	O
:	O
:	O
:	O
:	O
:	O
8s	O
(	O
a	O
,	O
n	O
)	O
e-ne2	O
aea	O
32	O
/	O
.	O
proof	O
.	O
the	O
proof	O
parallels	O
that	O
of	O
theorem	O
12.4.	O
we	O
may	O
again	O
assume	O
that	O
ne2	O
:	O
:	O
:	O
:	O
2.	O
in	O
the	O
first	O
two	O
steps	O
we	O
prove	O
that	O
this	O
may	O
be	O
done	O
exactly	O
the	O
same	O
way	O
as	O
in	O
theorem	O
12.4	O
;	O
we	O
do	O
not	O
repeat	O
the	O
argument	O
.	O
the	O
only	O
difference	O
appears	O
in	O
step	O
3	O
:	O
step	O
3.	O
conditioning	O
.	O
to	O
bound	O
the	O
probability	O
again	O
we	O
condition	O
on	O
z	O
1	O
,	O
...	O
,	O
zn	O
.	O
fix	O
z	O
1	O
,	O
...	O
,	O
zn	O
e	O
r	O
d	O
,	O
and	O
observe	O
that	O
as	O
a	O
ranges	O
over	O
a	O
,	O
the	O
number	O
of	O
different	O
vectors	O
(	O
la	O
(	O
zl	O
)	O
,	O
...	O
,	O
ia	O
(	O
zn	O
)	O
)	O
is	O
just	O
the	O
number	O
of	O
different	O
subsets	O
of	O
{	O
z	O
1	O
,	O
...	O
,	O
zn	O
}	O
produced	O
by	O
intersecting	O
it	O
with	O
sets	O
in	O
a	O
,	O
which	O
,	O
by	O
definition	O
,	O
can	O
not	O
exceed	O
sea	O
,	O
n	O
)	O
.	O
therefore	O
,	O
with	O
zl	O
,	O
...	O
,	O
zn	O
fixed	O
,	O
the	O
supremum	O
in	O
the	O
above	O
probability	O
is	O
a	O
maximum	O
of	O
at	O
most	O
n	O
a	O
(	O
z	O
1	O
,	O
...	O
,	O
zn	O
)	O
random	O
variables	O
.	O
this	O
number	O
,	O
by	O
definition	O
,	O
is	O
bounded	O
from	O
above	O
by	O
sea	O
,	O
n	O
)	O
.	O
by	O
the	O
union	O
bound	O
we	O
get	O
i	O
therefore	O
,	O
as	O
before	O
,	O
it	O
suffices	O
to	O
bound	O
the	O
conditional	O
probability	O
198	O
12.	O
vapnik-chervonenkis	O
theory	O
this	O
may	O
be	O
done	O
by	O
hoeffding	O
's	O
inequality	B
exactly	O
as	O
in	O
step	O
4	O
of	O
the	O
proof	O
of	O
theorem	O
12.4.	O
finally	O
,	O
we	O
obtain	O
p	O
{	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
1	O
>	O
e	O
}	O
s	O
8s	O
(	O
a	O
,	O
n	O
)	O
e-ne2/32	O
.	O
0	O
aea	O
the	O
bound	O
of	O
theorem	O
12.5	O
is	O
useful	O
when	O
the	O
shatter	O
coefficients	O
do	O
not	O
increase	O
too	O
quickly	O
with	O
n.	O
for	O
example	O
,	O
if	O
a	O
contains	O
all	O
borel	O
sets	O
of	O
nd	O
,	O
then	O
we	O
can	O
shatter	O
any	O
collection	O
of	O
n	O
different	O
points	O
at	O
will	O
,	O
and	O
obtain	O
sea	O
,	O
n	O
)	O
=	O
2n	O
.	O
this	O
would	O
be	O
useless	O
,	O
of	O
course	O
.	O
the	O
smaller	O
a	O
,	O
the	O
smaller	O
the	O
shatter	B
coefficient	I
is	O
.	O
to	O
apply	O
the	O
vc	O
bound	O
,	O
it	O
suffices	O
to	O
compute	O
shatter	O
coefficients	O
for	O
certain	O
families	O
of	O
sets	O
.	O
examples	O
may	O
be	O
found	O
in	O
cover	O
(	O
1965	O
)	O
,	O
vapnik	O
and	O
chervonenkis	O
(	O
1971	O
)	O
,	O
devroye	O
and	O
wagner	O
(	O
1979a	O
)	O
,	O
feinholz	O
(	O
1979	O
)	O
,	O
devroye	O
(	O
1982a	O
)	O
,	O
massart	O
(	O
1983	O
)	O
,	O
dudley	O
(	O
1984	O
)	O
,	O
simon	O
(	O
1991	O
)	O
,	O
and	O
stengle	O
and	O
yukich	O
(	O
1989	O
)	O
.	O
this	O
list	O
of	O
references	O
is	O
far	O
from	O
exhaustive	O
.	O
more	O
information	O
about	O
shatter	O
coefficients	O
is	O
given	O
in	O
chapter	O
13.	O
remark	O
.	O
measurability	O
.	O
the	O
supremum	O
in	O
theorem	O
12.5	O
is	O
not	O
always	O
mea	O
(	O
cid:173	O
)	O
surable	O
.	O
measurability	O
must	O
be	O
verified	O
for	O
every	O
family	O
a.	O
for	O
all	O
our	O
examples	O
,	O
the	O
quantities	O
are	O
indeed	O
measurable	O
.	O
for	O
more	O
on	O
the	O
measurability	O
question	O
,	O
see	O
dudley	O
(	O
1978	O
;	O
1984	O
)	O
,	O
massart	O
(	O
1983	O
)	O
,	O
and	O
gaenssler	O
(	O
1983	O
)	O
.	O
gine	O
andzinn	O
(	O
1984	O
)	O
and	O
yukich	O
(	O
1985	O
)	O
provide	O
further	O
work	O
on	O
suprema	O
of	O
the	O
type	O
shown	O
in	O
theorem	O
12.5.0	O
remark	O
.	O
optimal	O
exponent	O
.	O
for	O
the	O
sake	O
of	O
readability	O
we	O
followed	O
the	O
line	O
of	O
pollard	O
's	O
proof	O
(	O
1984	O
)	O
instead	O
of	O
the	O
original	O
by	O
vapnik	O
and	O
chervonenkis	O
.	O
in	O
particular	O
,	O
the	O
exponent	O
-ne	O
2/32	O
in	O
theorem	O
12.5	O
is	O
worse	O
than	O
the	O
-ne2/8	O
established	O
in	O
the	O
original	O
paper	O
.	O
the	O
best	O
known	O
exponents	O
together	O
with	O
some	O
other	O
related	O
results	O
are	O
mentioned	O
in	O
section	O
12.8.	O
the	O
basic	O
ideas	O
of	O
the	O
original	O
proof	O
by	O
vapnik	O
and	O
chervonenkis	O
appear	O
in	O
the	O
proof	O
of	O
theorem	O
12.7	O
below	O
.	O
0	O
remark	O
.	O
necessary	O
and	O
sufficient	O
conditions	O
.	O
the	O
theorem	B
that	O
it	O
can	O
be	O
strengthened	O
to	O
it	O
is	O
clear	O
from	O
the	O
proof	O
of	O
p	O
{	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
1	O
>	O
e	O
}	O
s	O
8e	O
{	O
n	O
a	O
(	O
zl	O
,	O
...	O
,	O
zn	O
)	O
}	O
e-	O
aea	O
ne2	O
/32	O
,	O
where	O
z	O
1	O
,	O
...	O
,	O
zn	O
are	O
i.i.d	O
.	O
random	O
variables	O
with	O
probability	O
measure	B
v.	O
although	O
this	O
upper	O
bound	O
is	O
tighter	O
than	O
that	O
in	O
the	O
stated	O
inequality	B
,	O
it	O
is	O
usually	O
more	O
difficult	O
to	O
handle	O
,	O
since	O
the	O
coefficient	O
in	O
front	O
of	O
the	O
exponential	B
term	O
depends	O
on	O
the	O
distribution	B
of	O
zl	O
,	O
while	O
sea	O
,	O
n	O
)	O
is	O
purely	O
combinatorial	O
in	O
nature	O
.	O
however	O
,	O
this	O
form	O
is	O
important	O
in	O
a	O
different	O
setting	O
:	O
we	O
say	O
that	O
the	O
uniform	B
law	O
of	O
large	O
numbers	O
holds	O
if	O
sup	O
ivn	O
(	O
a	O
)	O
aea	O
v	O
(	O
a	O
)	O
1	O
-+	O
0	O
in	O
probability	O
.	O
12.5	O
classifier	B
selection	I
199	O
it	O
follows	O
from	O
this	O
form	O
of	O
theorem	O
12.5	O
that	O
the	O
uniform	B
law	O
of	O
large	O
numbers	O
holds	O
if	O
e	O
{	O
log	O
(	O
na	O
(	O
zl	O
,	O
...	O
,	O
zn	O
)	O
)	O
}	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-+	O
o.	O
n	O
vapnik	O
and	O
chervonenkis	O
showed	O
(	O
1971	O
;	O
1981	O
)	O
that	O
this	O
condition	O
is	O
also	O
necessary	O
for	O
the	O
uniform	B
law	O
of	O
large	O
numbers	O
.	O
another	O
characterization	O
of	O
the	O
uniform	B
law	O
of	O
large	O
numbers	O
is	O
given	O
by	O
talagrand	O
(	O
1987	O
)	O
,	O
who	O
showed	O
that	O
the	O
uniform	B
law	O
of	O
large	O
numbers	O
holds	O
if	O
and	O
only	O
if	O
there	O
does	O
not	O
exist	O
a	O
set	O
a	O
c	O
rd	O
with	O
v	O
(	O
a	O
)	O
>	O
0	O
such	O
that	O
,	O
with	O
probability	O
one	O
,	O
the	O
set	O
{	O
zl	O
,	O
...	O
,	O
zn	O
}	O
n	O
a	O
is	O
shattered	O
by	O
a.d	O
12.5	O
classifier	B
selection	I
the	O
following	O
theorem	B
relates	O
the	O
results	O
of	O
the	O
previous	O
sections	O
to	O
empirical	B
classifier	I
selection	I
,	O
that	O
is	O
,	O
when	O
the	O
empirical	B
error	I
probability	O
ln	O
(	O
cj	O
>	O
)	O
is	O
minimized	O
over	O
a	O
class	O
of	O
classifiers	O
c.	O
we	O
emphasize	O
that	O
unlike	O
in	O
section	O
12.2	O
,	O
here	O
we	O
allow	O
any	O
classifier	B
with	O
the	O
property	O
that	O
has	O
minimal	O
empirical	B
error	I
l	O
n	O
(	O
cj	O
>	O
)	O
in	O
c.	O
first	O
introduce	O
the	O
shatter	O
coefficients	O
and	O
vc	B
dimension	I
of	O
c	O
:	O
definition	O
12.3.	O
let	O
c	O
be	O
a	O
class	O
of	O
decision	O
functions	O
of	O
the	O
form	O
cj	O
>	O
:	O
rd	O
-+	O
{	O
o	O
,	O
i	O
}	O
.	O
define	O
a	O
as	O
the	O
collection	O
of	O
all	O
sets	O
{	O
{	O
x	O
:	O
cj	O
>	O
(	O
x	O
)	O
=	O
i	O
}	O
x	O
{	O
on	O
u	O
{	O
{	O
x	O
:	O
cj	O
>	O
(	O
x	O
)	O
=	O
o	O
}	O
x	O
{	O
in	O
,	O
cj	O
>	O
e	O
c.	O
define	O
the	O
n-th	O
shatter	B
coefficient	I
s	O
(	O
c	O
,	O
n	O
)	O
of	O
the	O
class	O
of	O
classifiers	O
cas	O
furthermore	O
,	O
define	O
the	O
vc	B
dimension	I
vc	O
of	O
c	O
as	O
s	O
(	O
c	O
,	O
n	O
)	O
=	O
sea	O
,	O
n	O
)	O
.	O
for	O
the	O
performance	O
of	O
the	O
empirically	O
selected	O
decision	O
cj	O
>	O
,	O
~	O
,	O
we	O
have	O
the	O
fol	O
(	O
cid:173	O
)	O
lowing	O
:	O
theorem	B
12.6.	O
letc	O
be	O
a	O
class	O
of	O
decision	O
functions	O
oftheformcj	O
>	O
:	O
rd	O
-+	O
{	O
o	O
,	O
i	O
}	O
.	O
then	O
using	O
the	O
notation	O
ln	O
(	O
cj	O
»	O
=	O
~	O
'l7=1i	O
{	O
¢	O
(	O
x	O
i	O
)	O
=/yd	O
and	O
l	O
(	O
cj	O
»	O
=	O
p	O
{	O
cj	O
>	O
(	O
x	O
)	O
=i	O
y	O
}	O
,	O
we	O
have	O
and	O
therefore	O
p	O
{	O
l	O
(	O
cj	O
>	O
~	O
)	O
-	O
inf	O
l	O
(	O
cj	O
»	O
>	O
e	O
}	O
:	O
:	O
:	O
:	O
;	O
8s	O
(	O
c	O
,	O
n	O
)	O
e-ne2/128	O
,	O
¢ec	O
where	O
cj	O
>	O
~	O
denotes	O
the	O
classifier	B
minimizing	O
ln	O
(	O
cj	O
»	O
over	O
the	O
class	O
c.	O
200	O
12.	O
vapnik-chervonenkis	O
theory	O
proof	O
.	O
the	O
statements	O
are	O
immediate	O
consequences	O
of	O
theorem	O
12.5	O
and	O
lemma	O
8.2.0	O
the	O
next	O
corollary	O
,	O
an	O
easy	O
application	O
of	O
theorem	O
12.6	O
(	O
see	O
problem	O
12.1	O
)	O
,	O
makes	O
things	O
a	O
little	O
more	O
transparent	O
.	O
corollary	O
12.1.	O
in	O
the	O
notation	O
of	O
theorem	O
12.6	O
,	O
e	O
{	O
l	O
(	O
¢*	O
)	O
}	O
-	O
n	O
inf	O
l	O
(	O
¢	O
)	O
:	O
s	O
16	O
epec	O
log	O
(	O
8es	O
(	O
c	O
,	O
n	O
)	O
)	O
.	O
2n	O
if	O
s	O
(	O
c	O
,	O
n	O
)	O
increases	O
polynomially	O
with	O
n	O
,	O
then	O
the	O
average	O
error	O
probability	O
of	O
the	O
selected	O
classifier	B
is	O
within	O
0	O
(	O
jlog	O
n	O
/	O
n	O
)	O
of	O
the	O
error	O
of	O
the	O
best	O
rule	B
in	O
the	O
class	O
.	O
we	O
point	O
out	O
that	O
this	O
result	O
is	O
completely	O
distribution-free	O
.	O
furthermore	O
,	O
note	O
the	O
nonasymptotic	O
nature	O
of	O
these	O
inequalities	O
:	O
they	O
hold	O
for	O
every	O
n.	O
from	O
here	O
on	O
the	O
problem	O
is	O
purely	O
combinatorial-one	O
has	O
to	O
estimate	B
the	O
shatter	O
coefficients	O
.	O
many	O
properties	O
are	O
given	O
in	O
chapter	O
13.	O
in	O
particular	O
,	O
if	O
vc	O
>	O
2	O
,	O
then	O
s	O
(	O
c	O
,	O
n	O
)	O
:	O
s	O
n	O
vc	O
,	O
that	O
is	O
,	O
if	O
the	O
class	O
c	O
has	O
finite	O
vc	B
dimension	I
,	O
then	O
s	O
(	O
c	O
,	O
n	O
)	O
increases	O
at	O
a	O
polynomial	O
rate	O
,	O
and	O
e	O
{	O
l	O
(	O
¢~	O
)	O
}	O
-	O
inf	O
l	O
(	O
¢	O
)	O
:	O
s	O
16	O
epec	O
vc	O
logn	O
+	O
4	O
2n	O
remark	O
.	O
theorem	B
12.6	O
provides	O
a	O
bound	O
for	O
the	O
behavior	O
of	O
the	O
empirically	O
opti	O
(	O
cid:173	O
)	O
mal	O
classifier	B
.	O
in	O
practice	O
,	O
finding	O
an	O
empirically	B
optimal	I
classifier	I
is	O
often	O
compu	O
(	O
cid:173	O
)	O
tationally	O
very	O
expensive	O
.	O
in	O
such	O
cases	O
,	O
the	O
designer	O
is	O
often	O
forced	O
to	O
put	O
up	O
with	O
algorithms	O
yielding	O
suboptimal	O
classifiers	O
.	O
assume	O
for	O
example	O
,	O
that	O
we	O
have	O
an	O
algorithm	B
which	O
selects	O
a	O
classifier	O
gn	O
such	O
that	O
its	O
empirical	B
error	I
is	O
not	O
too	O
far	O
from	O
the	O
optimum	O
with	O
large	O
probability	O
:	O
where	O
{	O
en	O
}	O
and	O
{	O
6n	O
}	O
are	O
sequences	O
of	O
positive	O
numbers	O
converging	O
to	O
zero	O
.	O
then	O
it	O
is	O
easy	O
to	O
see	O
(	O
see	O
problem	O
12.6	O
)	O
that	O
p	O
{	O
l	O
(	O
gn	O
)	O
-	O
inf	O
l	O
(	O
¢	O
)	O
>	O
e	O
}	O
:	O
s	O
6n	O
+p	O
{	O
2sup	O
iln	O
(	O
¢	O
)	O
-	O
l	O
(	O
¢	O
)	O
i	O
>	O
e	O
-	O
en	O
}	O
'	O
epec	O
epec	O
and	O
theorem	B
12.6	O
may	O
be	O
used	O
to	O
obtain	O
bounds	O
for	O
the	O
error	O
probability	O
of	O
gn	O
.	O
0	O
interestingly	O
,	O
the	O
empirical	B
error	I
probability	O
of	O
the	O
empirically	B
optimal	I
classifier	I
is	O
always	O
close	O
to	O
its	O
expected	O
value	O
,	O
as	O
may	O
be	O
seen	O
from	O
the	O
following	O
example	O
:	O
corollary	O
12.2.	O
let	O
c	O
be	O
an	O
arbitrary	O
class	O
of	O
classification	O
rules	O
(	O
that	O
is	O
,	O
func	O
(	O
cid:173	O
)	O
tions	O
of	O
the	O
form	O
¢	O
:	O
n	O
d	O
-+	O
{	O
a	O
,	O
i	O
}	O
)	O
.	O
let	O
¢	O
:	O
e	O
c	O
be	O
the	O
rule	B
that	O
minimizes	O
the	O
number	O
of	O
errors	O
committed	O
on	O
the	O
training	O
sequence	O
dn	O
,	O
among	O
the	O
classifiers	O
in	O
c.	O
in	O
other	O
words	O
,	O
12.6	O
sample	B
complexity	I
201	O
where	O
ln	O
(	O
¢	O
)	O
=	O
n	O
li=	O
:	O
l	O
i	O
{	O
¢	O
(	O
xi	O
)	O
=/yd	O
'	O
thenfor	O
every	O
nand	O
e	O
>	O
0	O
,	O
--	O
n	O
1	O
the	O
corollary	O
follows	O
immediately	O
from	O
theorem	B
9.1	O
by	O
observing	O
that	O
chang	O
(	O
cid:173	O
)	O
ing	O
the	O
value	O
of	O
one	O
(	O
xi	O
,	O
yi	O
)	O
pair	O
in	O
the	O
training	O
sequence	O
results	O
in	O
a	O
change	O
of	O
at	O
most	O
lin	O
in	O
the	O
value	O
of	O
ln	O
(	O
¢~	O
)	O
'	O
the	O
corollary	O
is	O
true	O
even	O
if	O
the	O
vc	B
dimension	I
of	O
c	O
is	O
infinite	O
(	O
i	O
)	O
.	O
the	O
result	O
shows	O
that	O
ln	O
(	O
¢~	O
)	O
is	O
always	O
very	O
close	O
to	O
its	O
expected	O
value	O
with	O
large	O
probability	O
,	O
even	O
if	O
e	O
{	O
ln	O
(	O
¢~	O
)	O
}	O
is	O
far	O
from	O
inf¢ec	O
l	O
(	O
¢	O
)	O
(	O
see	O
also	O
problem	O
12.12	O
)	O
.	O
12.6	O
sample	B
complexity	I
in	O
his	O
theory	O
of	O
learning	O
,	O
valiant	O
(	O
1984	O
)	O
rephrases	O
the	O
empirical	B
classifier	I
selection	I
problem	O
as	O
follows	O
.	O
for	O
e	O
,	O
8	O
>	O
0	O
,	O
define	O
an	O
(	O
e	O
,	O
8	O
)	O
learning	B
algorithm	O
as	O
a	O
method	O
that	O
selects	O
a	O
classifier	O
gn	O
from	O
c	O
using	O
the	O
data	O
dn	O
such	O
that	O
for	O
the	O
selected	O
rule	B
sup	O
p	O
{	O
l	O
(	O
gn	O
)	O
-	O
(	O
x	O
,	O
y	O
)	O
inf	O
l	O
(	O
¢	O
)	O
>	O
e	O
}	O
~	O
8	O
,	O
¢ec	O
whenever	O
n	O
~	O
n	O
(	O
e	O
,	O
8	O
)	O
.	O
here	O
n	O
(	O
e	O
,	O
8	O
)	O
is	O
the	O
sample	B
complexity	I
of	O
the	O
algorithm	B
,	O
defined	O
as	O
the	O
smallest	O
integer	O
with	O
the	O
above	O
property	O
.	O
since	O
the	O
supremum	O
is	O
taken	O
over	O
all	O
possible	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
,	O
the	O
integer	O
n	O
(	O
e	O
,	O
8	O
)	O
is	O
the	O
number	O
of	O
data	O
pairs	O
that	O
guarantees	O
e	O
accuracy	B
with	O
8	O
confidence	B
for	O
any	O
distribution	B
.	O
note	O
that	O
we	O
use	O
the	O
notation	O
gn	O
and	O
not	O
¢i~	O
in	O
the	O
definition	O
above	O
,	O
as	O
the	O
definition	O
does	O
not	O
force	O
us	O
to	O
take	O
the	O
empirical	B
risk	I
minimizer	O
¢i~	O
.	O
we	O
may	O
use	O
theorem	B
12.6	O
to	O
get	O
an	O
upper	O
bound	O
on	O
the	O
sample	B
complexity	I
of	O
the	O
selection	B
algorithm	O
based	O
on	O
empirical	O
error	O
minimization	O
(	O
i.e.	O
,	O
the	O
classifier	B
¢~	O
)	O
.	O
corollary	O
12.3.	O
the	O
sample	B
complexity	I
of	O
the	O
method	O
based	O
on	O
empirical	B
error	I
minimization	O
is	O
bounded	O
from	O
above	O
by	O
8	O
)	O
.	O
n	O
(	O
e	O
,	O
8	O
)	O
~	O
max	O
--	O
2	O
-log	O
--	O
2-	O
'	O
-2	O
log	O
-	O
8	O
e	O
e	O
e	O
(	O
512vc	O
256vc	O
256	O
the	O
corollary	O
is	O
a	O
direct	O
consequence	O
of	O
theorem	O
12.6.	O
the	O
details	O
are	O
left	O
as	O
an	O
exercise	O
(	O
problem	O
12.5	O
)	O
.	O
the	O
constants	O
may	O
be	O
improved	O
by	O
using	O
refined	O
versions	O
of	O
theorem	O
12.5	O
(	O
see	O
,	O
e.g.	O
,	O
theorem	B
12.8	O
)	O
.	O
the	O
sample	O
size	O
that	O
guarantees	O
the	O
prescribed	O
accuracy	B
and	O
confidence	B
is	O
proportional	O
to	O
the	O
maximum	O
of	O
vc	O
-	O
e2	O
log	O
-	O
vc	O
e2	O
and	O
1	O
-	O
e2	O
1	O
log	O
-	O
.	O
8	O
202	O
12.	O
vapnik-chervonenkis	O
theory	O
here	O
we	O
have	O
our	O
first	O
practical	O
interpretation	O
of	O
the	O
vc	B
dimension	I
.	O
doubling	O
the	O
vc	B
dimension	I
requires	O
that	O
we	O
basically	O
double	O
the	O
sample	O
size	O
to	O
obtain	O
the	O
same	O
accuracy	B
and	O
confidence	B
.	O
doubling	O
the	O
accuracy	B
however	O
,	O
forces	O
us	O
to	O
quadruple	O
the	O
sample	O
size	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
confidence	B
level	O
has	O
little	O
influence	O
on	O
the	O
sample	O
size	O
,	O
as	O
it	O
is	O
hidden	O
behind	O
a	O
logarithmic	O
term	O
,	O
thanks	O
to	O
the	O
exponential	B
nature	O
of	O
the	O
vapnik-chervonenkis	O
inequality	B
.	O
the	O
vapnik-chervonenkis	O
bound	O
and	O
the	O
sample	B
complexity	I
n	O
(	O
e	O
,	O
8	O
)	O
also	O
allow	O
us	O
to	O
compare	O
different	O
classes	O
in	O
a	O
unified	O
manner	O
.	O
for	O
example	O
,	O
if	O
we	O
pick	O
¢	O
,	O
~	O
by	O
minimizing	O
the	O
empirical	B
error	I
over	O
all	O
hyperrectangles	O
of	O
n	O
18	O
,	O
will	O
we	O
need	O
a	O
sample	O
size	O
that	O
exceeds	O
that	O
of	O
the	O
rule	B
that	O
minimizes	O
the	O
empirical	B
error	I
over	O
all	O
linear	O
halfspaces	O
of	O
n29	O
?	O
with	O
the	O
sample	B
complexity	I
in	O
hand	O
,	O
it	O
is	O
just	O
a	O
matter	O
of	O
comparing	O
vc	O
dimensions	O
.	O
as	O
a	O
function	O
of	O
e	O
,	O
the	O
above	O
bound	O
grows	O
as	O
0	O
(	O
(	O
1/e2	O
)	O
log	O
(	O
1/e2	O
»	O
)	O
.	O
itis	O
possible	O
,	O
interestingly	O
,	O
to	O
get	O
rid	O
of	O
the	O
``	O
log	O
''	O
term	O
,	O
at	O
the	O
expense	O
of	O
increasing	O
the	O
linearity	O
in	O
the	O
vc	B
dimension	I
(	O
see	O
problem	O
12.11	O
)	O
.	O
12.7	O
the	O
zero-error	O
case	O
theorem	B
12.5	O
is	O
completely	O
general	O
,	O
as	O
it	O
applies	O
to	O
any	O
class	O
of	O
classifiers	O
and	O
all	O
distributions	O
.	O
in	O
some	O
cases	O
,	O
however	O
,	O
when	O
we	O
have	O
some	O
additional	O
information	O
about	O
the	O
distribution	B
,	O
it	O
is	O
possible	O
to	O
obtain	O
even	O
better	O
bounds	O
.	O
for	O
example	O
,	O
in	O
the	O
theory	O
of	O
concept	O
learning	B
one	O
commonly	O
assumes	O
that	O
l	O
*	O
=	O
0	O
,	O
and	O
that	O
the	O
bayes	O
decision	O
is	O
contained	O
in	O
c	O
(	O
see	O
,	O
e.g.	O
,	O
valiant	O
(	O
1984	O
)	O
,	O
blumer	O
,	O
ehrenfeucht	O
,	O
haussler	O
,	O
and	O
warmuth	O
(	O
1989	O
)	O
,	O
natarajan	O
(	O
1991	O
»	O
.	O
the	O
following	O
theorem	B
provides	O
significant	O
improvement	O
.	O
its	O
various	O
forms	O
have	O
been	O
proved	O
by	O
devroye	O
and	O
wagner	O
(	O
1976b	O
)	O
,	O
vapnik	O
(	O
1982	O
)	O
,	O
and	O
blumer	O
,	O
ehrenfeucht	O
,	O
haussler	O
,	O
and	O
warmuth	O
(	O
1989	O
)	O
.	O
for	O
a	O
sharper	O
result	O
,	O
see	O
problem	O
12.9.	O
theorem	B
12.7.	O
let	O
c	O
be	O
a	O
class	O
of	O
decision	O
functions	O
mapping	O
n	O
d	O
to	O
{	O
o	O
,	O
i	O
}	O
,	O
and	O
let	O
¢i~	O
be	O
a	O
function	O
in	O
c	O
that	O
minimizes	O
the	O
empirical	B
error	I
based	O
on	O
the	O
training	O
sample	O
dn	O
.	O
suppose	O
that	O
inf	O
<	O
jjec	O
l	O
(	O
¢	O
)	O
=	O
0	O
,	O
i.e.	O
,	O
the	O
bayes	O
decision	O
is	O
contained	O
in	O
c	O
,	O
and	O
l*	O
=	O
0.	O
then	O
to	O
contrast	O
this	O
with	O
theorem	O
12.6	O
,	O
observe	O
that	O
the	O
exponent	O
in	O
the	O
upper	O
bound	O
for	O
the	O
empirically	O
optimal	O
rule	O
is	O
proportional	O
to	O
-ne	O
instead	O
of	O
-ne2	O
.	O
to	O
see	O
the	O
significance	O
of	O
this	O
difference	O
,	O
note	O
that	O
theorem	B
12.7	O
implies	O
that	O
the	O
error	O
proba	O
(	O
cid:173	O
)	O
bility	O
of	O
the	O
selected	O
classifier	B
is	O
within	O
0	O
(	O
log	O
n	O
/	O
n	O
)	O
of	O
the	O
optimal	O
rule	B
in	O
the	O
class	O
(	O
which	O
equals	O
zero	O
in	O
this	O
case	O
,	O
see	O
problem	O
12.8	O
)	O
,	O
as	O
opposed	O
to	O
o	O
(	O
y'logn/n	O
)	O
from	O
theorem	B
12.6.	O
we	O
show	O
in	O
chapter	O
14	O
that	O
this	O
is	O
not	O
a	O
technical	O
coincidence	O
,	O
but	O
since	O
both	O
bounds	O
are	O
essentially	O
tight	O
,	O
it	O
is	O
a	O
mathematical	O
witness	O
to	O
the	O
fact	O
that	O
it	O
is	O
remarkably	O
easier	O
to	O
select	O
a	O
good	O
classifier	B
when	O
l	O
*	O
=	O
0.	O
the	O
proof	O
is	O
12.7	O
the	O
zero-error	O
case	O
203	O
based	O
on	O
the	O
random	O
permutation	O
argument	O
developed	O
in	O
the	O
original	O
proof	O
of	O
the	O
vapnik	O
-chervonenkis	O
inequality	B
(	O
1971	O
)	O
.	O
proof	O
.	O
for	O
ne	O
:	O
s	O
2	O
,	O
the	O
inequality	B
is	O
clearly	O
true	O
.	O
so	O
,	O
we	O
assume	O
that	O
ne	O
>	O
2.	O
first	O
observe	O
that	O
since	O
infq	O
'	O
>	O
ec	O
l	O
(	O
¢	O
)	O
=	O
o	O
,	O
ln	O
(	O
¢~	O
)	O
=	O
o	O
with	O
probability	O
one	O
.	O
it	O
is	O
easily	O
seen	O
that	O
l	O
(	O
¢i	O
:	O
)	O
:	O
s	O
sup	O
il	O
(	O
¢	O
)	O
-	O
ln	O
(	O
¢	O
)	O
1.	O
q	O
'	O
>	O
:	O
ln	O
(	O
q	O
'	O
»	O
=o	O
now	O
,	O
we	O
return	O
to	O
the	O
notation	O
of	O
the	O
previous	O
sections	O
,	O
that	O
is	O
,	O
zi	O
denotes	O
the	O
pair	O
(	O
xi	O
,	O
yi	O
)	O
,	O
v	O
denotes	O
its	O
measure	B
,	O
and	O
vn	O
is	O
the	O
empirical	B
measure	I
based	O
on	O
zl	O
,	O
...	O
,	O
zn	O
.	O
also	O
,	O
a	O
consists	O
of	O
all	O
sets	O
of	O
the	O
form	O
a	O
=	O
{	O
(	O
x	O
,	O
y	O
)	O
:	O
¢	O
(	O
x	O
)	O
=i	O
y	O
}	O
for	O
¢	O
e	O
c.	O
with	O
these	O
notations	O
,	O
sup	O
q	O
'	O
>	O
:	O
ln	O
(	O
q	O
'	O
»	O
=o	O
il	O
(	O
¢	O
)	O
-	O
ln	O
(	O
¢	O
)	O
1	O
=	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
i.	O
a	O
:	O
vn	O
(	O
a	O
)	O
=o	O
step	O
1.	O
symmetrization	B
by	O
a	O
ghost	O
sample	O
.	O
the	O
first	O
step	O
ofthe	O
proof	O
is	O
similar	O
to	O
that	O
of	O
theorem	O
12.5.	O
introduce	O
the	O
auxiliary	O
sample	O
z~	O
,	O
...	O
,	O
z~	O
such	O
that	O
the	O
random	O
variables	O
z	O
1	O
,	O
...	O
,	O
zn	O
,	O
zi	O
,	O
...	O
,	O
z~	O
are	O
i.i.d.	O
,	O
and	O
let	O
v~	O
be	O
the	O
empirical	B
measure	I
for	O
zi	O
,	O
...	O
,	O
z~	O
.	O
then	O
for	O
ne	O
>	O
2	O
p	O
{	O
a	O
:	O
'~£	O
)	O
dllvn	O
(	O
al	O
v	O
(	O
all	O
>	O
e	O
}	O
:	O
s	O
2p	O
{	O
a	O
:	O
'~£	O
)	O
dllvn	O
(	O
al	O
-	O
v~	O
(	O
all	O
>	O
~	O
}	O
.	O
the	O
proof	O
of	O
this	O
inequality	B
parallels	O
that	O
of	O
the	O
corresponding	O
one	O
in	O
the	O
proof	O
of	O
theorem	O
12.5.	O
however	O
,	O
an	O
important	O
difference	O
is	O
that	O
the	O
condition	O
ne2	O
>	O
2	O
there	O
is	O
more	O
restrictive	O
than	O
the	O
condition	O
ne	O
>	O
2	O
here	O
.	O
the	O
details	O
of	O
the	O
proof	O
are	O
left	O
to	O
the	O
reader	O
(	O
see	O
problem	O
12.7	O
)	O
.	O
step	O
2.	O
symmetrization	B
by	O
permuting	O
.	O
note	O
that	O
the	O
distribution	B
of	O
i	O
a.vn	O
(	O
a	O
)	O
-o	O
is	O
the	O
same	O
as	O
the	O
distribution	B
of	O
1	O
n	O
.	O
sup	O
_	O
ivn	O
(	O
a	O
)	O
-	O
v~	O
(	O
a	O
)	O
1	O
=	O
.	O
n	O
sup	O
_	O
;	O
;	O
:	O
?	O
=	O
ia	O
(	O
zi	O
)	O
-	O
;	O
;	O
l	O
ia	O
(	O
z	O
;	O
)	O
11	O
sup	O
_i	O
~	O
t	O
i	O
a	O
(	O
i1	O
(	O
zi	O
)	O
)	O
-	O
~	O
t	O
ia	O
(	O
i1	O
(	O
z	O
;	O
)	O
)	O
1	O
'	O
,	O
b	O
(	O
i1	O
)	O
d	O
;	O
f	O
.	O
a·li=iia	O
(	O
zi	O
)	O
-o	O
1	O
1=1	O
1	O
n	O
1=1	O
a·li=iia	O
(	O
i1	O
(	O
zi	O
»	O
-o	O
1=1	O
1=1	O
where	O
i1	O
(	O
zd	O
,	O
...	O
,	O
i1	O
(	O
zn	O
)	O
,	O
i1	O
(	O
zd	O
,	O
...	O
,	O
i1	O
(	O
z~	O
)	O
is	O
an	O
arbitrary	O
permutation	O
of	O
the	O
random	O
variables	O
zl	O
,	O
...	O
,	O
zn	O
,	O
zi	O
,	O
...	O
,	O
z~	O
.	O
the	O
(	O
2n	O
)	O
!	O
possible	O
permutations	O
are	O
denoted	O
by	O
therefore	O
,	O
p	O
{	O
sup	O
a	O
:	O
vll	O
(	O
a	O
)	O
=o	O
ivn	O
(	O
a	O
)	O
-	O
v~	O
(	O
a	O
)	O
1	O
>	O
~	O
}	O
2	O
=	O
e	O
{	O
(	O
2n	O
)	O
!	O
i	O
(	O
2n	O
)	O
!	O
~	O
i	O
{	O
,	O
b	O
(	O
i1	O
j	O
»	O
h	O
}	O
204	O
12.	O
vapnik-chervonenkis	O
theory	O
i	O
=	O
e-l	O
(	O
2n	O
)	O
!	O
(	O
2n	O
)	O
!	O
j=l	O
{	O
sup	O
a:2	O
:	O
:	O
;	O
~1	O
ia	O
(	O
[	O
1j	O
(	O
zd	O
)	O
=o	O
i	O
{	O
11	O
``	O
''	O
i	O
(	O
[	O
1	O
(	O
z·	O
»	O
_l	O
``	O
''	O
i	O
(	O
[	O
1	O
(	O
zi	O
»	O
i	O
>	O
s	O
}	O
}	O
.	O
``	O
l..	O
,	O
=1	O
a	O
j	O
,	O
''	O
l..	O
,	O
=l	O
a	O
2	O
)	O
,	O
step	O
3.	O
conditioning	O
.	O
next	O
we	O
fix	O
z	O
1	O
,	O
...	O
,	O
zn	O
,	O
z~	O
,	O
...	O
,	O
z~	O
and	O
bound	O
the	O
value	O
of	O
the	O
random	O
variable	B
above	O
.	O
let	O
a	O
c	O
a	O
be	O
a	O
collection	O
of	O
sets	O
such	O
that	O
any	O
two	O
sets	O
in	O
a	O
pick	O
different	O
subsets	O
of	O
{	O
z	O
1	O
,	O
...	O
,	O
zn	O
,	O
z~	O
,	O
...	O
,	O
z~	O
}	O
,	O
and	O
its	O
cardinality	O
is	O
n	O
a	O
(	O
zi	O
,	O
...	O
,	O
zn	O
,	O
z~	O
,	O
...	O
,	O
z~	O
)	O
,	O
that	O
is	O
,	O
all	O
possible	O
subsets	O
are	O
represented	O
exactly	O
once	O
.	O
then	O
it	O
suffices	O
to	O
take	O
the	O
supremum	O
over	O
a	O
instead	O
of	O
a	O
:	O
(	O
2n	O
)	O
!	O
1	O
(	O
2n	O
)	O
!	O
~	O
.	O
``	O
sup	O
]	O
=1	O
a·2	O
:	O
:i=iia	O
(	O
i1/zj	O
)	O
-o	O
_	O
i	O
{	O
i~	O
2	O
:	O
:	O
;	O
~1	O
ia	O
(	O
tij	O
(	O
zd	O
)	O
-~	O
2	O
:	O
:7=ja	O
(	O
i1j	O
(	O
z	O
;	O
»	O
i	O
>	O
0	O
=	O
<	O
=	O
(	O
2n	O
)	O
!	O
1	O
(	O
2n	O
)	O
!	O
~	O
-	O
.	O
''	O
sup	O
j=1	O
aea2	O
:	O
:i=lia	O
(	O
ti	O
j	O
(	O
zi	O
)	O
)	O
-o	O
_	O
iu	O
2	O
:	O
:	O
;	O
'=1	O
ia	O
(	O
i1j	O
(	O
z	O
;	O
»	O
>	O
0	O
1	O
(	O
2n	O
)	O
!	O
(	O
2n	O
)	O
!	O
~	O
2	O
:	O
.	O
i	O
{	O
2	O
:	O
:	O
;	O
'=1	O
ia	O
(	O
i1j	O
(	O
zi	O
)	O
)	O
=o	O
}	O
iu	O
2	O
:	O
:	O
;	O
'=j	O
ia	O
(	O
tij	O
(	O
z	O
;	O
»	O
>	O
0	O
2	O
:	O
.	O
(	O
2n	O
)	O
!	O
~	O
i	O
{	O
2	O
:	O
:	O
;	O
'=j	O
ia	O
(	O
[	O
1j	O
(	O
zi	O
)	O
)	O
=o	O
}	O
iu	O
2	O
:	O
:	O
;	O
~1	O
ia	O
(	O
i1j	O
(	O
z	O
;	O
)	O
)	O
>	O
o	O
'	O
]	O
=1	O
aea	O
1	O
(	O
2n	O
)	O
!	O
aea	O
]	O
=1	O
step	O
4.	O
counting	O
.	O
clearly	O
,	O
the	O
expression	B
behind	O
the	O
first	O
summation	O
sign	O
is	O
just	O
the	O
number	O
of	O
permutations	O
of	O
the	O
2n	O
points	O
z	O
1	O
,	O
z~	O
,	O
...	O
,	O
zn	O
,	O
z~	O
with	O
the	O
property	O
divided	O
by	O
(	O
2n	O
)	O
!	O
,	O
the	O
total	O
number	O
of	O
permutations	O
.	O
observe	O
that	O
if	O
l	O
=	O
l:7=1	O
(	O
l	O
a	O
(	O
zi	O
)	O
+	O
ia	O
(	O
zd	O
)	O
is	O
the	O
total	O
number	O
of	O
points	O
in	O
a	O
among	O
zi	O
,	O
z~	O
,	O
...	O
,	O
zn	O
,	O
z~	O
for	O
a	O
fixed	O
set	O
a	O
,	O
then	O
the	O
number	O
of	O
permutations	O
such	O
that	O
is	O
zero	O
if	O
l	O
.	O
:	O
s	O
ne	O
/2	O
.	O
if	O
l	O
>	O
ne	O
/2	O
,	O
then	O
the	O
fraction	O
of	O
the	O
number	O
of	O
permutations	O
with	O
the	O
above	O
property	O
and	O
the	O
number	O
of	O
all	O
permutations	O
can	O
not	O
exceed	O
to	O
see	O
this	O
,	O
note	O
that	O
for	O
the	O
above	O
product	B
of	O
indicators	O
to	O
be	O
1	O
,	O
all	O
the	O
points	O
falling	O
in	O
a	O
have	O
to	O
be	O
in	O
the	O
second	O
half	O
of	O
the	O
permuted	O
sample	O
.	O
now	O
clearly	O
,	O
12.7	O
the	O
zero-error	O
case	O
205	O
_n_	O
(	O
_n_-_1_	O
)	O
_	O
.	O
.	O
...	O
=.	O
<	O
'-	O
(	O
n	O
--	O
'	O
--	O
_l_+_l_	O
)	O
_	O
.	O
<	O
2-l	O
<	O
2-ne	O
/2	O
.	O
2n	O
(	O
2n	O
-	O
1	O
)	O
...	O
(	O
2n	O
-	O
l	O
+	O
1	O
)	O
-	O
-	O
summarizing	O
,	O
we	O
have	O
p	O
{	O
a.	O
:	O
'~~=oi~n	O
(	O
a	O
)	O
-	O
v~	O
(	O
a	O
)	O
1	O
>	O
~	O
}	O
<	O
e	O
{	O
~~	O
)	O
!	O
~	O
n	O
sup	O
•	O
a·2	O
:	O
i=lia	O
(	O
it/zi	O
»	O
-o	O
_	O
i	O
{	O
i~	O
2	O
:	O
;	O
'=lia	O
(	O
itj	O
(	O
zi	O
»	O
-~	O
2:7=1	O
ia	O
(	O
itj	O
(	O
zml	O
>	O
0	O
}	O
<	O
e	O
{	O
2:2-ne	O
/2	O
}	O
aea	O
=	O
e	O
{	O
na	O
(	O
zl	O
,	O
...	O
,	O
zn	O
,	O
z~	O
,	O
...	O
,	O
z~	O
)	O
2-ne/2	O
}	O
<	O
sea	O
,	O
2n	O
)	O
2-ne/2	O
,	O
and	O
the	O
theorem	B
is	O
proved	O
.	O
0	O
again	O
,	O
we	O
can	O
bound	O
the	O
sample	B
complexity	I
n	O
(	O
e	O
,	O
8	O
)	O
restricted	O
to	O
the	O
class	O
of	O
distributions	O
with	O
inf¢ec	O
l	O
(	O
¢	O
)	O
=	O
o.	O
just	O
as	O
theorem	B
12.6	O
implies	O
corollary	O
12.3	O
,	O
theorem	B
12.7	O
yields	O
corollary	O
12.4.	O
the	O
sample	B
complexity	I
n	O
(	O
e	O
,	O
8	O
)	O
that	O
guarantees	O
inf	O
l	O
(	O
¢	O
)	O
>	O
e	O
}	O
:	O
s	O
8	O
p	O
{	O
l	O
(	O
gn	O
)	O
-	O
sup	O
(	O
x	O
,	O
y	O
)	O
:	O
inf	O
<	O
/jec	O
l	O
(	O
¢	O
)	O
=o	O
¢ec	O
for	O
n	O
2	O
:	O
n	O
(	O
e	O
,	O
8	O
)	O
,	O
is	O
bounded	O
by	O
13	O
4	O
2	O
)	O
n	O
(	O
e	O
,	O
8	O
)	O
:	O
s	O
max	O
-log2	O
-	O
,	O
-log2	O
-	O
8	O
8vc	O
e	O
e	O
e	O
(	O
.	O
a	O
quick	O
comparison	O
with	O
corollary	O
12.3	O
shows	O
that	O
the	O
e2	O
factors	O
in	O
the	O
denom	O
(	O
cid:173	O
)	O
inators	O
there	O
are	O
now	O
replaced	O
by	O
e.	O
for	O
the	O
same	O
accuracy	B
,	O
much	O
smaller	O
samples	O
suffice	O
if	O
we	O
know	O
that	O
inf¢ec	O
l	O
(	O
¢	O
)	O
=	O
o.	O
interestingly	O
,	O
the	O
sample	B
complexity	I
is	O
still	O
roughly	O
linear	O
in	O
the	O
vc	B
dimension	I
.	O
remark	O
.	O
we	O
also	O
note	O
that	O
under	O
the	O
condition	O
of	O
theorem	O
12.7	O
,	O
p	O
{	O
l	O
(	O
¢~	O
)	O
>	O
e	O
}	O
<	O
2e	O
{	O
na	O
(	O
zl	O
,	O
...	O
,	O
zn	O
,	O
z~	O
,	O
...	O
,	O
z~	O
)	O
}	O
2-ne	O
/	O
2	O
206	O
12.	O
vapnik-chervonenkis	O
theory	O
where	O
the	O
class	O
a	O
of	O
sets	O
is	O
defined	O
in	O
the	O
proof	O
of	O
theorem	O
12.7	O
.	O
0	O
remark	O
.	O
as	O
theorem	B
12.7	O
shows	O
,	O
the	O
difference	O
between	O
the	O
error	O
probability	O
of	O
an	O
empirically	B
optimal	I
classifier	I
and	O
that	O
of	O
the	O
optimal	O
in	O
the	O
class	O
is	O
much	O
smaller	O
if	O
the	O
latter	O
quantity	O
is	O
known	O
to	O
be	O
zero	O
than	O
if	O
no	O
restriction	O
is	O
imposed	O
on	O
the	O
distribution	B
.	O
to	O
bridge	O
the	O
gap	O
between	O
the	O
two	O
bounds	O
,	O
one	O
may	O
put	O
the	O
restriction	O
inicpec	O
l	O
(	O
t/	O
»	O
~	O
l	O
on	O
the	O
distribution	B
,	O
where	O
l	O
e	O
(	O
0	O
,	O
1/2	O
)	O
is	O
a	O
fixed	O
number	O
.	O
devroye	O
and	O
wagner	O
(	O
1976b	O
)	O
and	O
vapnik	O
(	O
1982	O
)	O
obtained	O
such	O
bounds	O
.	O
for	O
example	O
,	O
it	O
follows	O
from	O
a	O
result	O
by	O
vapnik	O
(	O
1982	O
)	O
that	O
as	O
expected	O
,	O
the	O
bound	O
becomes	O
smaller	O
as	O
l	O
decreases	O
.	O
we	O
face	O
the	O
same	O
phe	O
(	O
cid:173	O
)	O
nomenon	O
in	O
chapter	O
14	O
,	O
where	O
lower	O
bounds	O
are	O
obtained	O
for	O
the	O
probability	O
above	O
.	O
o	O
12.8	O
extensions	O
we	O
mentioned	O
earlier	O
that	O
the	O
constant	O
in	O
the	O
exponent	O
in	O
theorem	O
12.5	O
can	O
be	O
improved	O
at	O
the	O
expense	O
of	O
a	O
more	O
complicated	O
argument	O
.	O
the	O
best	O
possible	O
ex	O
(	O
cid:173	O
)	O
ponent	O
appears	O
in	O
the	O
following	O
result	O
,	O
whose	O
proof	O
is	O
left	O
as	O
an	O
exercise	O
(	O
problem	O
12.15	O
)	O
:	O
theorem	B
12.8	O
.	O
(	O
devroye	O
(	O
1982a	O
)	O
.	O
p	O
{	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
1	O
>	O
e	O
}	O
~	O
cs	O
(	O
a	O
,	O
n2	O
)	O
e-2ne2	O
,	O
aea	O
where	O
the	O
constant	O
c	O
does	O
not	O
exceed	O
4e4e+	O
4e2	O
:5	O
4e8	O
,	O
e	O
~	O
1.	O
even	O
though	O
the	O
coefficient	O
in	O
front	O
is	O
larger	O
than	O
in	O
theorem	O
12.5	O
,	O
it	O
becomes	O
very	O
quickly	O
absorbed	O
by	O
the	O
exponential	B
term	O
.	O
we	O
will	O
see	O
in	O
chapter	O
13	O
that	O
for	O
va	O
>	O
2	O
,	O
sea	O
,	O
n	O
)	O
~	O
n	O
va	O
,	O
so	O
sea	O
,	O
n2	O
)	O
~	O
n2va	O
.	O
this	O
difference	O
is	O
negligible	O
compared	O
to	O
the	O
difference	O
between	O
the	O
exponential	B
terms	O
,	O
even	O
for	O
moderately	O
large	O
values	O
of	O
ne2	O
.	O
both	O
theorem	B
12.5	O
and	O
theorem	B
12.8	O
imply	O
that	O
e	O
{	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
i	O
}	O
=	O
o	O
(	O
y'logn/n	O
)	O
aea	O
12.8	O
extensions	O
207	O
(	O
see	O
problem	O
12.1	O
)	O
.	O
however	O
,	O
it	O
is	O
possible	O
to	O
get	O
rid	O
of	O
the	O
logarithmic	O
term	O
to	O
obtain	O
0	O
(	O
1/	O
fn	O
)	O
.	O
for	O
example	O
,	O
for	O
the	O
kolmogorov-srnirnov	O
statistic	O
we	O
have	O
the	O
following	O
result	O
by	O
dvoretzky	O
,	O
kiefer	O
and	O
wolfowitz	O
(	O
1956	O
)	O
,	O
sharpened	O
by	O
massart	O
(	O
1990	O
)	O
:	O
theorem	B
12.9	O
.	O
(	O
dvoretzky	O
,	O
kiefer	O
,	O
and	O
wolfowitz	O
(	O
1956	O
)	O
;	O
massart	O
(	O
1990	O
)	O
)	O
.	O
using	O
the	O
notation	O
of	O
theorem	O
12.4	O
,	O
we	O
have	O
for	O
every	O
nand	O
e	O
>	O
0	O
,	O
p	O
{	O
sup	O
if	O
(	O
z	O
)	O
-	O
fn	O
(	O
z	O
)	O
1	O
>	O
e	O
}	O
:	O
:s	O
2e-2ne2	O
zer	O
.	O
for	O
the	O
general	O
case	O
,	O
we	O
also	O
have	O
alexander	O
's	O
bound	O
:	O
theorem	B
12.10	O
.	O
(	O
alexander	O
(	O
1984	O
)	O
)	O
.	O
for	O
ne	O
2	O
:	O
:	O
:	O
:	O
64	O
,	O
p	O
{	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
1	O
>	O
e	O
aea	O
}	O
:	O
:s	O
16	O
v	O
ne	O
(	O
r	O
:	O
:	O
)	O
4096va	O
2	O
2	O
e-	O
ne	O
•	O
the	O
theorem	B
implies	O
the	O
following	O
(	O
problem	O
12.10	O
)	O
:	O
corollary	O
12.5.	O
e	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
1	O
{	O
aea	O
:	O
:s	O
8+j2048valog	O
(	O
4096va	O
)	O
.	O
fn	O
n	O
}	O
the	O
bound	O
in	O
theorem	O
12.10	O
is	O
theoretically	O
interesting	O
,	O
since	O
it	O
implies	O
(	O
see	O
problem	O
12.10	O
)	O
that	O
for	O
fixed	O
v	O
a	O
the	O
expected	O
value	O
of	O
the	O
supremum	O
decreases	O
as	O
a	O
/	O
fn	O
instead	O
of	O
jlog	O
n	O
/	O
n.	O
however	O
,	O
a	O
quick	O
comparison	O
reveals	O
that	O
alexander	O
's	O
bound	O
is	O
larger	O
than	O
that	O
of	O
theorem	O
12.8	O
,	O
unless	O
n	O
>	O
26144	O
,	O
an	O
astronomically	O
large	O
value	O
.	O
recently	O
,	O
talagrand	O
(	O
1994	O
)	O
obtained	O
a	O
very	O
strong	B
result	O
.	O
he	O
proved	O
that	O
there	O
exists	O
a	O
universal	O
constant	O
c	O
such	O
that	O
for	O
more	O
information	O
about	O
these	O
inequalities	O
,	O
see	O
also	O
vapnik	O
(	O
1982	O
)	O
,	O
gaenssler	O
(	O
1983	O
)	O
,	O
gaenssler	O
and	O
stute	O
(	O
1979	O
)	O
,	O
and	O
massart	O
(	O
1983	O
)	O
.	O
it	O
is	O
only	O
natural	O
to	O
ask	O
whether	O
the	O
uniform	B
law	O
of	O
large	O
numbers	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
1	O
-+	O
0	O
aea	O
holds	O
if	O
we	O
allow	O
a	O
to	O
be	O
the	O
class	O
of	O
all	O
measurable	O
subsets	O
of	O
nd	O
.	O
in	O
this	O
case	O
the	O
supremum	O
above	O
is	O
called	O
the	O
total	B
variation	I
between	O
the	O
measures	O
vn	O
and	O
v.	O
the	O
convergence	O
clearly	O
can	O
not	O
hold	O
if	O
vn	O
is	O
the	O
standard	B
empirical	I
measure	I
208	O
12.	O
vapnik-chevonenkis	O
theory	O
but	O
is	O
there	O
another	O
empirical	B
measure	I
such	O
that	O
the	O
convergence	O
holds	O
?	O
the	O
somewhat	O
amusing	O
answer	O
is	O
no	O
.	O
as	O
devroye	O
and	O
gyorfi	O
(	O
1992	O
)	O
proved	O
,	O
for	O
any	O
empirical	B
measure	I
vn-that	O
is	O
,	O
a	O
function	O
depending	O
on	O
zl	O
,	O
...	O
,	O
zn	O
assigning	O
a	O
nonnegative	O
number	O
to	O
any	O
measurable	O
set-there	O
exists	O
a	O
distribution	O
of	O
z	O
such	O
that	O
for	O
all	O
n	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
1	O
>	O
1/4	O
aea	O
almost	O
surely	O
.	O
thus	O
,	O
in	O
this	O
generality	O
,	O
the	O
problem	O
is	O
hopeless	O
.	O
for	O
meaningful	O
results	O
,	O
either	O
a	O
or	O
v	O
must	O
be	O
restricted	O
.	O
for	O
example	O
,	O
if	O
we	O
assume	O
that	O
v	O
is	O
absolutely	O
continuous	O
with	O
density	O
f	O
,	O
and	O
that	O
vn	O
is	O
absolutely	O
continuous	O
too	O
(	O
with	O
density	O
fn	O
)	O
,	O
then	O
by	O
scheffe	O
's	O
theorem	B
(	O
1947	O
)	O
,	O
-	O
11	O
2	O
rd	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
1	O
=	O
-	O
aea	O
i	O
fn	O
(	O
x	O
)	O
-	O
f	O
(	O
x	O
)	O
idx	O
(	O
see	O
problem	O
12.13	O
)	O
.	O
but	O
as	O
we	O
see	O
from	O
problems	O
6.2	O
,	O
10.2,9.6	O
,	O
and	O
10.3	O
,	O
there	O
exist	O
density	O
estimators	O
(	O
such	O
as	O
histogram	O
and	O
kernel	B
estimates	O
)	O
such	O
that	O
the	O
l	O
1-	O
error	O
converges	O
to	O
zero	O
almost	O
surely	O
for	O
all	O
possible	O
densities	O
.	O
therefore	O
,	O
the	O
total	B
variation	I
between	O
the	O
empirical	B
measures	O
derived	O
from	O
these	O
density	O
estimates	O
and	O
the	O
true	O
measure	B
converges	O
to	O
zero	O
almost	O
surely	O
for	O
all	O
distributions	O
with	O
a	O
density	O
.	O
for	O
other	O
large	O
classes	O
of	O
distributions	O
that	O
can	O
be	O
estimated	O
consistently	O
in	O
total	O
variation	B
,	O
we	O
refer	O
to	O
barron	O
,	O
gyorfi	O
,	O
and	O
van	O
der	O
meulen	O
(	O
1992	O
)	O
.	O
problems	O
and	O
exercises	O
for	O
all	O
t	O
>	O
°	O
and	O
some	O
c	O
>	O
0	O
,	O
then	O
,	O
problem	O
12.1.	O
prove	O
that	O
if	O
a	O
nonnegative	O
random	O
variable	B
z	O
satisfies	O
p	O
{	O
z	O
>	O
t	O
}	O
:	O
s	O
ce-2nt2	O
furthermore	O
,	O
ez	O
:	O
s	O
je	O
{	O
z2	O
}	O
:	O
s	O
jlo~~e	O
)	O
.	O
hint	O
:	O
use	O
the	O
identity	O
e	O
{	O
z2	O
}	O
=	O
1000	O
p	O
{	O
z2	O
>	O
t	O
}	O
dt	O
,	O
and	O
set	O
1000	O
=	O
iou	O
+	O
luoo	O
.	O
bound	O
the	O
first	O
integral	O
by	O
u	O
,	O
and	O
the	O
second	O
by	O
the	O
exponential	B
inequality	O
.	O
find	O
the	O
value	O
of	O
u	O
that	O
minimizes	O
the	O
obtained	O
upper	O
bound	O
.	O
problem	O
12.2.	O
generalize	O
the	O
arguments	O
of	O
theorems	O
4.5	O
and	O
4.6	O
to	O
prove	O
theorems	O
12.2	O
and	O
12.3.	O
problem	O
12.3.	O
determine	O
the	O
fingering	B
dimension	I
of	O
classes	O
of	O
classifiers	O
c	O
=	O
{	O
¢	O
:	O
¢	O
(	O
x	O
)	O
=	O
i	O
{	O
xea	O
)	O
;	O
a	O
e	O
a	O
}	O
if	O
the	O
class	O
ais	O
(	O
1	O
)	O
(	O
2	O
)	O
(	O
3	O
)	O
(	O
4	O
)	O
the	O
class	O
of	O
all	O
closed	O
intervals	O
in	O
n	O
,	O
the	O
class	O
of	O
all	O
sets	O
obtained	O
as	O
the	O
union	O
of	O
m	O
closed	O
intervals	O
in	O
n	O
,	O
the	O
class	O
of	O
balls	O
in	O
n	O
d	O
centered	O
at	O
the	O
origin	O
,	O
the	O
class	O
of	O
all	O
balls	O
in	O
n	O
d	O
,	O
problems	O
and	O
exercises	O
209	O
(	O
5	O
)	O
(	O
6	O
)	O
the	O
class	O
of	O
sets	O
of	O
the	O
form	O
(	O
-00	O
,	O
xd	O
x	O
...	O
x	O
(	O
-00	O
,	O
xd	O
]	O
in	O
n	O
d	O
,	O
and	O
the	O
class	O
of	O
all	O
convex	O
polygons	O
in	O
n2	O
.	O
problem	O
12.4.	O
let	O
c	O
be	O
a	O
class	O
of	O
classifiers	O
with	O
fingering	O
dimension	B
k	O
>	O
4	O
(	O
independently	O
of	O
n	O
)	O
.	O
show	O
that	O
vc	O
.	O
:	O
;	O
k	O
log~	O
k.	O
problem	O
12.5.	O
prove	O
that	O
theorem	B
12.6	O
implies	O
corollary	O
12.3.	O
hint	O
:	O
find	O
n	O
(	O
e	O
,	O
0	O
)	O
such	O
that	O
8n	O
vc	O
e-ne2	O
/128	O
.	O
:	O
;	O
0	O
whenever	O
n	O
>	O
n	O
(	O
e	O
,	O
0	O
)	O
.	O
to	O
see	O
this	O
,	O
first	O
show	O
that	O
n	O
vc	O
.	O
:	O
;	O
ene2/256	O
is	O
satisfied	O
for	O
n	O
:	O
:	O
:	O
:	O
51:2vc	O
log	O
25:2vc	O
,	O
which	O
follows	O
from	O
the	O
fact	O
that	O
2	O
log	O
x	O
.	O
:	O
;	O
x	O
if	O
x	O
:	O
:	O
:	O
:	O
e2	O
.	O
but	O
in	O
this	O
case	O
8n	O
vc	O
e-ne2	O
/128	O
<	O
8e-ne2	O
/256	O
.	O
the	O
upper	O
bound	O
does	O
not	O
exceed	O
0	O
if	O
n	O
>	O
256	O
log	O
£	O
.	O
8	O
problem	O
12.6.	O
let	O
c	O
be	O
a	O
class	O
of	O
classifiers	O
¢	O
:	O
n	O
d	O
--	O
-+	O
{	O
o	O
,	O
1	O
}	O
,	O
and	O
let	O
¢	O
,	O
:	O
be	O
a	O
classi	O
(	O
cid:173	O
)	O
fier	O
minimizing	O
the	O
empirical	B
error	I
probability	O
measured	O
on	O
dn	O
.	O
assume	O
that	O
we	O
have	O
an	O
algorithm	B
which	O
selects	O
a	O
classifier	O
gn	O
such	O
that	O
e2	O
-	O
-	O
where	O
{	O
en	O
}	O
and	O
{	O
on	O
}	O
are	O
sequences	O
of	O
positive	O
numbers	O
converging	O
to	O
zero	O
.	O
show	O
that	O
p	O
{	O
l	O
(	O
gn	O
)	O
-	O
inf	O
l	O
(	O
¢	O
)	O
>	O
e	O
}	O
.	O
:	O
;	O
on	O
+p	O
!	O
2sup	O
iln	O
(	O
¢	O
)	O
-	O
l	O
(	O
¢	O
)	O
i	O
>	O
e	O
-	O
en	O
}	O
.	O
¢ec	O
¢ec	O
find	O
conditions	O
on	O
{	O
en	O
}	O
and	O
{	O
on	O
}	O
so	O
that	O
el	O
(	O
gn	O
)	O
-	O
inf	O
l	O
(	O
¢	O
)	O
=	O
0	O
-	O
,	O
¢ec	O
(	O
fi¥ogn	O
)	O
n	O
that	O
is	O
,	O
el	O
(	O
gn	O
)	O
converges	O
to	O
the	O
optimum	O
at	O
the	O
same	O
order	O
as	O
the	O
error	O
probability	O
of	O
¢	O
,	O
:	O
.	O
problem	O
12.7.	O
prove	O
that	O
p	O
{	O
sup	O
a	O
:	O
vn	O
(	O
a	O
)	O
=o	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
1	O
>	O
e	O
}	O
.	O
:	O
;	O
2p	O
{	O
sup	O
a	O
:	O
vn	O
(	O
a	O
)	O
=o	O
ivn	O
(	O
a	O
)	O
-	O
v~	O
(	O
a	O
)	O
1	O
>	O
~	O
}	O
2	O
holds	O
if	O
ne	O
>	O
2.	O
this	O
inequality	B
is	O
needed	O
to	O
complete	O
the	O
proof	O
of	O
theorem	O
12.7.	O
hint	O
:	O
proceed	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
12.5.	O
introduce	O
a	O
*	O
with	O
v/1	O
(	O
a	O
*	O
)	O
=	O
°	O
and	O
justify	O
the	O
validity	O
of	O
the	O
steps	O
of	O
the	O
following	O
chain	O
of	O
inequalities	O
:	O
p	O
l	O
:	O
:	O
'	O
,	O
~f	O
)	O
=o	O
ivn	O
(	O
a	O
)	O
-	O
v~	O
(	O
a	O
)	O
1	O
>	O
e/2	O
}	O
>	O
e	O
{	O
i	O
{	O
v	O
(	O
a*	O
»	O
ejp	O
{	O
v~	O
(	O
a*	O
)	O
:	O
:	O
:	O
:	O
~	O
i	O
zi	O
,	O
...	O
,	O
zn	O
}	O
}	O
>	O
p	O
{	O
b	O
(	O
n	O
,	O
e	O
)	O
>	O
n2e	O
}	O
p	O
{	O
lvn	O
(	O
a*	O
)	O
-	O
v	O
(	O
a*	O
)	O
1	O
>	O
e	O
}	O
,	O
where	O
b	O
(	O
n	O
,	O
e	O
)	O
is	O
a	O
binomial	O
random	O
variable	B
with	O
parameters	O
nand	O
e.	O
finish	O
the	O
proof	O
by	O
showing	O
that	O
the	O
probability	O
on	O
the	O
right-hand	O
side	O
is	O
greater	O
than	O
or	O
equal	O
to	O
1/2	O
if	O
ne	O
>	O
2	O
.	O
(	O
under	O
the	O
slightly	O
more	O
restrictive	O
condition	O
ne	O
>	O
8	O
,	O
this	O
follows	O
from	O
chebyshev	O
's	O
inequality	B
.	O
)	O
210	O
12.	O
vapq.ik-chervonenkis	O
theory	O
problem	O
12.8.	O
prove	O
that	O
theorem	B
12.7	O
implies	O
that	O
if	O
inf	O
<	O
/.	O
>	O
ec	O
l	O
(	O
4	O
)	O
)	O
=	O
0	O
,	O
then	O
*	O
el	O
(	O
4	O
)	O
n	O
)	O
:	O
s	O
:	O
2vc	O
10g	O
(	O
2n	O
)	O
+	O
4	O
.	O
i	O
2	O
n	O
og	O
we	O
note	O
here	O
that	O
haussler	O
,	O
littlestone	O
,	O
and	O
warmuth	O
(	O
1988	O
)	O
demonstrated	O
the	O
existence	O
of	O
a	O
classifier	O
4	O
>	O
;	O
with	O
el	O
(	O
4	O
)	O
;	O
)	O
:	O
s	O
:	O
2vcln	O
when	O
inf	O
<	O
/.	O
>	O
ec	O
l	O
(	O
4	O
)	O
)	O
=	O
o.	O
hint	O
:	O
use	O
the	O
identity	O
fooo	O
p	O
{	O
x	O
>	O
t	O
}	O
dt	O
for	O
nonnegative	O
random	O
variables	O
x	O
,	O
and	O
employ	O
the	O
fact	O
that	O
ex	O
=	O
s	O
(	O
c	O
,	O
n	O
)	O
:	O
s	O
:	O
nvc	O
(	O
see	O
theorem	B
13.3	O
)	O
.	O
problem	O
12.9.	O
prove	O
the	O
following	O
version	O
of	O
theorem	O
12.7.	O
let	O
4	O
>	O
;	O
be	O
a	O
function	O
that	O
minimizes	O
the	O
empirical	B
error	I
over	O
a	O
class	O
in	O
c.	O
assume	O
that	O
inf¢ec	O
l	O
(	O
4	O
)	O
)	O
=	O
o.	O
then	O
p	O
{	O
l	O
(	O
4	O
)	O
j~	O
)	O
>	O
e	O
}	O
:	O
s	O
:	O
2s	O
(	O
c	O
,	O
n2	O
)	O
e-ne	O
(	O
shawe-taylor	O
,	O
anthony	O
,	O
and	O
biggs	O
(	O
1993	O
»	O
.	O
hint	O
:	O
modify	O
the	O
proof	O
of	O
theorem	O
12.7	O
by	O
introducing	O
a	O
ghost	O
sample	O
z	O
;	O
,	O
...	O
,	O
z	O
:	O
j1	O
with	O
size	O
m	O
(	O
to	O
be	O
specified	O
after	O
optimization	O
)	O
.	O
only	O
the	O
first	O
symmetrization	B
step	O
(	O
problem	O
12.7	O
)	O
needs	O
adjusting	O
:	O
show	O
that	O
for	O
any	O
a	O
e	O
(	O
0	O
,	O
1	O
)	O
,	O
p	O
{	O
sup	O
v	O
(	O
a	O
)	O
>	O
e	O
}	O
:	O
s	O
:	O
_	O
i_mete	O
p	O
{	O
sup	O
v	O
:	O
n	O
(	O
a	O
)	O
>	O
(	O
1	O
-	O
a	O
)	O
e	O
}	O
,	O
a	O
:	O
i'i/	O
(	O
a	O
)	O
-o	O
1	O
e	O
a	O
:	O
vl/	O
(	O
a	O
)	O
-o	O
where	O
v~i	O
is	O
the	O
empirical	B
measure	I
based	O
on	O
the	O
ghost	B
sample	I
(	O
use	O
bernstein	O
's	O
inequality	B
(	O
cid:173	O
)	O
theorem	B
8.2	O
)	O
.	O
the	O
rest	O
of	O
the	O
proof	O
is	O
similar	O
to	O
that	O
of	O
theorem	O
12.7.	O
choose	O
m	O
=	O
n2	O
-	O
n	O
and	O
a	O
=	O
n/	O
(	O
n	O
+	O
m	O
)	O
.	O
problem	O
12.10.	O
prove	O
that	O
alexander	O
's	O
bound	O
(	O
theorem	B
12.10	O
)	O
implies	O
that	O
if	O
a	O
is	O
a	O
vapnik-chervonenkis	O
class	O
with	O
vc	B
dimension	I
v	O
=	O
va	O
,	O
then	O
e	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
1	O
{	O
aea	O
:	O
s	O
:	O
8	O
+	O
)	O
2048	O
v	O
loge	O
4096	O
v	O
)	O
.	O
r.	O
;	O
v	O
n	O
}	O
hint	O
:	O
justify	O
the	O
following	O
steps	O
:	O
(	O
1	O
)	O
if	O
1/1	O
is	O
a	O
negative	O
decreasing	O
concave	O
function	O
,	O
then	O
e	O
''	O
'	O
(	O
ii	O
)	O
-1/1	O
(	O
u	O
)	O
hint	O
:	O
bound	O
l/i	O
by	O
using	O
its	O
taylor	O
series	O
expansion	O
.	O
e	O
''	O
'	O
(	O
t	O
)	O
dt	O
:	O
s	O
:	O
--	O
,	O
(	O
cid:173	O
)	O
f	O
oo	O
ii	O
(	O
2	O
)	O
let	O
b	O
>	O
0	O
be	O
fixed	O
.	O
then	O
for	O
u	O
2	O
:	O
.jb72	O
,	O
hint	O
:	O
use	O
the	O
previous	O
step	O
.	O
(	O
3	O
)	O
let	O
x	O
be	O
a	O
positive	O
random	O
variable	B
for	O
which	O
p	O
{	O
x	O
>	O
u	O
}	O
:	O
s	O
:	O
au	O
h	O
e-	O
2112	O
,	O
u	O
2	O
:	O
..jc	O
,	O
where	O
a	O
,	O
b	O
,	O
c	O
are	O
positive	O
constants	O
.	O
then	O
,	O
if	O
b	O
2	O
:	O
2c	O
2	O
:	O
e	O
,	O
ex	O
<	O
-+	O
-	O
-	O
.	O
jblogb	O
a	O
-	O
2	O
2	O
hint	O
:	O
use	O
ex	O
=	O
fooo	O
p	O
{	O
x	O
>	O
u	O
}	O
du	O
,	O
and	O
bound	O
the	O
probability	O
either	O
by	O
one	O
,	O
or	O
by	O
the	O
bound	O
of	O
the	O
previous	O
step	O
.	O
problem	O
12.11.	O
use	O
alexander	O
's	O
inequality	B
to	O
obtain	O
the	O
following	O
sample	O
size	O
bound	O
for	O
empirical	B
error	I
minimization	O
:	O
problems	O
and	O
exercises	O
211	O
n	O
(	O
e	O
,	O
8	O
)	O
s	O
max	O
(	O
214vciog	O
(	O
214vc	O
)	O
4	O
.	O
'	O
zlog	O
-	O
2	O
e	O
e	O
16	O
)	O
8	O
.	O
for	O
what	O
values	O
of	O
e	O
does	O
this	O
bound	O
beat	O
corollary	O
12.3	O
?	O
problem	O
12.12.	O
let	O
zi	O
,	O
...	O
,	O
zn	O
be	O
i.i.d	O
.	O
random	O
variables	O
in	O
rd	O
,	O
with	O
measure	O
v	O
,	O
and	O
standard	B
empirical	I
measure	I
vn	O
•	O
let	O
a	O
be	O
an	O
arbitrary	O
class	O
of	O
subsets	O
of	O
rd	O
.	O
show	O
that	O
as	O
n	O
~	O
00	O
,	O
if	O
and	O
only	O
if	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
1	O
~	O
0	O
aea	O
in	O
probability	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
1	O
~	O
0	O
with	O
probability	O
one	O
.	O
aea	O
hint	O
:	O
use	O
mcdiarmid	O
's	O
inequality	B
.	O
problem	O
12.13.	O
prove	O
scheffe	O
's	O
theorem	B
(	O
1947	O
)	O
:	O
let	O
p	O
,	O
and	O
v	O
be	O
absolute	O
continuous	O
prob	O
(	O
cid:173	O
)	O
ability	O
measures	O
on	O
nd	O
with	O
densities	O
i	O
and	O
g	O
,	O
respectively	O
.	O
prove	O
that	O
sup	O
1p	O
,	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
1	O
=	O
~	O
f	O
i/	O
(	O
x	O
)	O
-	O
g	O
(	O
x	O
)	O
ldx	O
,	O
aea	O
2	O
where	O
a	O
is	O
the	O
class	O
of	O
all	O
borel-measurable	O
sets	O
.	O
hint	O
:	O
show	O
that	O
the	O
supremum	O
is	O
achieved	O
for	O
the	O
set	O
{	O
x	O
:	O
i	O
(	O
x	O
)	O
>	O
g	O
(	O
x	O
)	O
}	O
.	O
problem	O
12.14.	O
learning	B
based	O
on	O
empirical	B
covering	I
.	O
this	O
problem	O
demonstrates	O
an	O
alternative	O
method	O
of	O
picking	O
a	O
classifier	O
which	O
works	O
as	O
well	O
as	O
empirical	B
error	I
minimiza	O
(	O
cid:173	O
)	O
tion	O
.	O
the	O
method	O
,	O
based	O
on	O
empirical	O
covering	B
of	O
the	O
class	O
of	O
classifiers	O
,	O
was	O
introduced	O
by	O
buescher	O
and	O
kumar	O
(	O
1996a	O
)	O
.	O
the	O
idea	O
of	O
covering	O
the	O
class	O
goes	O
back	O
to	O
vapnik	O
(	O
1982	O
)	O
.	O
see	O
also	O
benedek	O
and	O
itai	O
(	O
1988	O
)	O
,	O
kulkarni	O
(	O
1991	O
)	O
,	O
and	O
dudley	O
,	O
kulkarni	O
,	O
richardson	O
,	O
and	O
zeitouni	O
(	O
1994	O
)	O
.	O
letc	O
be	O
a	O
class	O
ofclassifierscp	O
:	O
rd	O
~	O
{	O
o	O
,	O
i	O
}	O
.	O
thedatasetdn	O
is	O
split	O
into	O
two	O
parts	O
,	O
dm	O
=	O
«	O
xi	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xm	O
'	O
ym	O
»	O
,	O
and	O
tt	O
=	O
«	O
xm+i	O
'	O
ym+i	O
)	O
,	O
...	O
,	O
(	O
xn	O
'	O
yn	O
»	O
,	O
where	O
n	O
=	O
m	O
+	O
i.	O
we	O
use	O
the	O
first	O
part	O
dm	O
to	O
cover	O
c	O
as	O
follows	O
.	O
define	O
the	O
random	O
variable	B
n	O
as	O
the	O
number	O
of	O
different	O
values	O
the	O
binary	B
vector	O
bm	O
(	O
cp	O
)	O
=	O
(	O
cp	O
(	O
x	O
i	O
)	O
,	O
...	O
,	O
cp	O
(	O
xm	O
»	O
takes	O
as	O
cp	O
is	O
varied	O
over	O
c.	O
clearly	O
,	O
n	O
s	O
s	O
(	O
c	O
,	O
m	O
)	O
.	O
take	O
n	O
classifiers	O
from	O
c	O
,	O
such	O
that	O
all	O
n	O
possible	O
values	O
of	O
the	O
binary	B
vector	O
b	O
m	O
(	O
cp	O
)	O
are	O
represented	O
exactly	O
once	O
.	O
denote	O
these	O
classifiers	O
by	O
4	O
>	O
1	O
,	O
...	O
,	O
4	O
>	O
n.	O
among	O
these	O
functions	O
,	O
pick	O
one	O
that	O
minimizes	O
the	O
empirical	B
error	I
on	O
the	O
second	O
part	O
of	O
the	O
data	O
set	O
tt	O
:	O
denote	O
the	O
selected	O
classifier	B
by	O
;	O
p	O
,	O
l	O
'	O
show	O
that	O
for	O
every	O
n	O
,	O
m	O
and	O
e	O
>	O
0	O
,	O
the	O
difference	O
between	O
the	O
error	O
probability	O
l	O
(	O
;	O
p	O
,	O
l	O
)	O
=	O
p	O
{	O
¢	O
,	O
;	O
(	O
x	O
)	O
``	O
i	O
yidn	O
}	O
and	O
the	O
minimal	O
error	O
probability	O
in	O
the	O
class	O
satisfies	O
212	O
12.	O
vapnik-chervonenkis	O
theory	O
(	O
buescher	O
and	O
kumar	O
(	O
1996a	O
»	O
.	O
for	O
example	O
,	O
by	O
taking	O
m	O
'	O
''	O
,	O
jii.	O
,	O
we	O
get	O
{	O
...	O
...	O
e	O
l	O
(	O
<	O
/jn	O
)	O
-	O
}	O
f¥clogn	O
inf	O
l	O
(	O
<	O
/j	O
)	O
~	O
c	O
-	O
-	O
,	O
~ec	O
n	O
where	O
c	O
is	O
a	O
universal	O
constant	O
.	O
the	O
fact	O
that	O
the	O
number	O
of	O
samples	O
m	O
used	O
for	O
covering	B
c	O
is	O
very	O
small	O
compared	O
to	O
n	O
,	O
may	O
make	O
the	O
algorithm	B
computationally	O
more	O
attractive	O
than	O
the	O
method	O
of	O
empirical	O
error	O
minimization	O
.	O
hint	O
:	O
use	O
the	O
decomposition	O
p	O
{	O
l	O
(	O
¢n	O
)	O
-	O
inf	O
l	O
(	O
<	O
/j	O
)	O
>	O
e	O
}	O
~ec	O
bound	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
by	O
using	O
lemma	O
8.2	O
and	O
hoeffding	O
's	O
inequality	B
:	O
to	O
bound	O
the	O
second	O
term	O
of	O
the	O
decomposition	O
,	O
observe	O
that	O
inf	O
l	O
(	O
¢i	O
)	O
-	O
i-i	O
,	O
...	O
,	O
n	O
inf	O
l	O
(	O
<	O
/j	O
)	O
~ec	O
<	O
<	O
sup	O
il	O
(	O
<	O
/j	O
)	O
-	O
l	O
(	O
¢	O
'	O
)	O
i	O
~	O
,	O
¢	O
>	O
/ec	O
:	O
bm	O
(	O
~	O
)	O
-bmw	O
)	O
sup	O
p	O
{	O
<	O
/j	O
(	O
x	O
)	O
i¢	O
'	O
(	O
x	O
)	O
}	O
~	O
,	O
~/ec	O
:	O
bm	O
(	O
~	O
)	O
-bm	O
(	O
~/	O
)	O
sup	O
v	O
(	O
a	O
)	O
,	O
aea	O
:	O
vm	O
(	O
a	O
)	O
=o	O
where	O
a	O
=	O
{	O
{	O
x	O
:	O
<	O
/j	O
(	O
x	O
)	O
=	O
i	O
}	O
:	O
¢	O
(	O
x	O
)	O
=	O
1¢i	O
(	O
x	O
)	O
-	O
¢2	O
(	O
x	O
)	O
l	O
,	O
¢i	O
,	O
¢2	O
e	O
c	O
}	O
,	O
and	O
vm	O
(	O
a	O
)	O
=	O
~	O
.e	O
:	O
i	O
l	O
{	O
x	O
;	O
ea	O
)	O
.	O
bound	O
the	O
latter	O
quantity	O
by	O
applying	O
theorem	B
12.7.	O
to	O
do	O
this	O
,	O
you	O
will	O
need	O
to	O
bound	O
the	O
shatter	O
coefficients	O
sea	O
,	O
2m	O
)	O
.	O
in	O
chapter	O
13	O
we	O
introduce	O
simple	O
tools	O
for	O
this	O
.	O
for	O
example	O
,	O
it	O
is	O
easy	O
to	O
deduce	O
from	O
parts	O
(	O
ii	O
)	O
,	O
(	O
iii	O
)	O
,	O
and	O
(	O
iv	O
)	O
of	O
theorem	O
13.5	O
,	O
that	O
sea	O
,	O
2m	O
)	O
~	O
s4	O
(	O
c	O
,	O
2m	O
)	O
.	O
problem	O
12.15.	O
prove	O
that	O
for	O
all	O
e	O
e	O
(	O
0	O
,	O
1	O
)	O
,	O
p	O
{	O
sup	O
/vn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
/	O
>	O
e	O
}	O
~	O
cs	O
(	O
a	O
,	O
n2	O
)	O
e-2ne2	O
,	O
aea	O
where	O
c	O
~	O
4e4€+4€2	O
(	O
1	O
)	O
(	O
devroye	O
(	O
1982a	O
»	O
.	O
hint	O
:	O
proceed	O
as	O
indicated	O
by	O
the	O
following	O
steps	O
:	O
introduce	O
an	O
i.i.d	O
.	O
ghost	B
sample	I
zi	O
,	O
...	O
,	O
z~	O
of	O
size	O
m	O
=	O
n2	O
-	O
n	O
,	O
where	O
zi	O
is	O
distributed	O
as	O
z	O
1.	O
denote	O
the	O
corresponding	O
empirical	B
measure	I
by	O
v~	O
.	O
as	O
in	O
the	O
proof	O
of	O
the	O
first	O
step	O
of	O
theorem	O
12.4	O
,	O
prove	O
that	O
for	O
a	O
,	O
e	O
e	O
(	O
0	O
,	O
1	O
)	O
,	O
p	O
{	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v~	O
(	O
a	O
)	O
1	O
>	O
(	O
1	O
-	O
a	O
)	O
e	O
}	O
aea	O
(	O
1	O
-	O
+	O
,	O
)	O
p	O
{	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
/	O
>	O
e	O
}	O
.	O
4a	O
e	O
m	O
aea	O
(	O
2	O
)	O
introduce	O
n2	O
!	O
permutations	O
it	O
r	O
,	O
...	O
,	O
itn+m	O
of	O
the	O
n	O
+	O
m	O
random	O
variables	O
as	O
in	O
step	O
2	O
of	O
the	O
proof	O
of	O
theorem	O
12.5.	O
show	O
that	O
problems	O
and	O
exercises	O
213	O
/1	O
2	O
!	O
1	O
n2	O
!	O
~	O
~~~	O
i	O
{	O
i*	O
l	O
:	O
:'=l	O
ia	O
(	O
tijczi	O
)	O
)	O
-	O
;	O
)	O
,	O
l	O
:	O
r=l	O
jactijcz	O
;	O
)	O
)	O
i	O
>	O
ci-a	O
)	O
e	O
}	O
(	O
3	O
)	O
show	O
that	O
for	O
each	O
a	O
e	O
a	O
,	O
by	O
using	O
hoeffding	O
's	O
inequality	B
for	O
sampling	B
without	I
replacement	I
from	O
n2	O
binary	B
(	O
cid:173	O
)	O
valued	O
elements	O
(	O
see	O
theorem	B
a.25	O
)	O
.	O
choose	O
a	O
=	O
i/	O
(	O
ne	O
)	O
.	O
13	O
combinatorial	O
aspects	O
of	O
vapnik	O
-chervonenkis	O
theory	O
13.1	O
shatter	O
coefficients	O
and	O
vc	B
dimension	I
in	O
this	O
section	O
we	O
list	O
a	O
few	O
interesting	O
properties	O
of	O
shatter	O
coefficients	O
sea	O
,	O
n	O
)	O
and	O
of	O
the	O
vc	B
dimension	I
v	O
a	O
of	O
a	O
class	O
of	O
sets	O
a.	O
we	O
begin	O
with	O
a	O
property	O
that	O
makes	O
things	O
easier	O
.	O
in	O
chapter	O
12	O
we	O
noted	O
the	O
importance	O
of	O
classes	O
of	O
the	O
form	O
a	O
=	O
{	O
a	O
x	O
{	O
oj	O
u	O
a	O
c	O
x	O
{	O
l	O
}	O
;	O
a	O
e	O
a	O
}	O
.	O
(	O
the	O
sets	O
a	O
are	O
of	O
the	O
form	O
{	O
x	O
:	O
¢	O
(	O
x	O
)	O
=	O
i	O
}	O
,	O
and	O
the	O
sets	O
in	O
a	O
are	O
sets	O
of	O
pairs	O
(	O
x	O
,	O
y	O
)	O
for	O
which	O
¢	O
(	O
x	O
)	O
=i	O
y	O
.	O
)	O
recall	O
that	O
if	O
c	O
is	O
a	O
class	O
of	O
classifiers	O
¢	O
:	O
rd	O
--	O
-+	O
{	O
o	O
,	O
i	O
}	O
,	O
then	O
by	O
definition	O
,	O
s	O
(	O
c	O
,	O
n	O
)	O
=	O
sea	O
,	O
n	O
)	O
and	O
vc	O
=	O
va.	O
the	O
first	O
result	O
states	O
that	O
s	O
(	O
c	O
,	O
n	O
)	O
=	O
sea	O
,	O
n	O
)	O
,	O
so	O
it	O
suffices	O
to	O
investigate	O
properties	O
of	O
a	O
,	O
a	O
class	O
of	O
subsets	O
of	O
rd	O
.	O
theorem	B
13.1.	O
for	O
every	O
n	O
we	O
have	O
sea	O
,	O
n	O
)	O
=	O
sea	O
,	O
n	O
)	O
,	O
and	O
therefore	O
va	O
=	O
va.	O
proof	O
.	O
let	O
n	O
be	O
a	O
positive	O
integer	O
.	O
we	O
show	O
that	O
for	O
any	O
n	O
pairs	O
from	O
rd	O
x	O
{	O
o	O
,	O
i	O
}	O
,	O
if	O
n	O
sets	O
from	O
a	O
pick	O
n	O
different	O
subsets	O
of	O
the	O
n	O
pairs	O
,	O
then	O
there	O
are	O
n	O
corresponding	O
sets	O
in	O
a	O
that	O
pick	O
n	O
different	O
subsets	O
of	O
n	O
points	O
in	O
r	O
d	O
,	O
and	O
vice	O
versa	O
.	O
fix	O
n	O
pairs	O
(	O
xl	O
,	O
0	O
)	O
,	O
...	O
,	O
(	O
xm	O
,	O
0	O
)	O
,	O
(	O
xm+l	O
,	O
1	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
1	O
)	O
.	O
note	O
that	O
since	O
ordering	O
does	O
not	O
matter	O
,	O
we	O
may	O
arrange	O
any	O
n	O
pairs	O
in	O
this	O
manner	O
.	O
assume	O
that	O
for	O
a	O
certain	O
set	O
a	O
e	O
a	O
,	O
the	O
corresponding	O
set	O
a	O
=	O
a	O
x	O
{	O
oj	O
u	O
ac	O
x	O
{	O
i	O
}	O
e	O
a	O
picks	O
out	O
the	O
pairs	O
(	O
xl	O
,	O
0	O
)	O
,	O
...	O
,	O
(	O
xk	O
,	O
0	O
)	O
,	O
(	O
xm+l	O
,	O
1	O
)	O
,	O
...	O
,	O
(	O
xm+z	O
,	O
1	O
)	O
,	O
that	O
is	O
,	O
the	O
set	O
of	O
these	O
pairs	O
is	O
the	O
intersection	O
of	O
a	O
and	O
the	O
n	O
pairs	O
.	O
again	O
,	O
we	O
can	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
the	O
pairs	O
are	O
ordered	B
in	O
this	O
way	O
.	O
this	O
means	O
that	O
a	O
picks	O
from	O
the	O
set	O
216	O
13.	O
combinatorial	O
aspects	O
of	O
vapnik-chervonenkis	O
theory	O
{	O
xl	O
,	O
...	O
,	O
xn	O
}	O
the	O
subset	O
{	O
xl	O
,	O
...	O
,	O
xb	O
xm+l+l	O
,	O
...	O
,	O
x	O
,	O
,	O
}	O
,	O
and	O
the	O
two	O
subsets	O
uniquely	O
determine	O
each	O
other	O
.	O
this	O
proves	O
sea	O
,	O
n	O
)	O
:	O
:	O
:	O
:	O
sea	O
,	O
n	O
)	O
.	O
to	O
prove	O
the	O
other	O
direction	O
,	O
notice	O
that	O
if	O
a	O
picks	O
a	O
subset	O
of	O
k	O
points	O
xl	O
,	O
...	O
,	O
xb	O
then	O
the	O
corresponding	O
set	O
a	O
e	O
a	O
picks	O
the	O
pairs	O
with	O
the	O
same	O
indices	O
from	O
{	O
(	O
xl	O
,	O
0	O
)	O
,	O
``	O
''	O
(	O
xb	O
o	O
)	O
}	O
.	O
equality	O
of	O
the	O
vc	O
dimensions	O
follows	O
from	O
the	O
equality	O
of	O
the	O
shatter	O
coefficients	O
.	O
0	O
the	O
following	O
theorem	B
,	O
attributed	O
to	O
vapnik	O
and	O
chervonenkis	O
(	O
1971	O
)	O
and	O
sauer	O
(	O
1972	O
)	O
,	O
describes	O
the	O
relationship	O
between	O
the	O
vc	B
dimension	I
and	O
shatter	O
coeffi	O
(	O
cid:173	O
)	O
cients	O
of	O
a	O
class	O
of	O
sets	O
.	O
this	O
is	O
the	O
most	O
important	O
tool	O
for	O
obtaining	O
useful	O
upper	O
bounds	O
on	O
the	O
shatter	O
coefficients	O
in	O
terms	O
of	O
the	O
vc	B
dimension	I
.	O
theorem	B
13.2.	O
if	O
a	O
is	O
a	O
class	O
of	O
sets	O
with	O
vc	O
dimension	B
v	O
a	O
,	O
then	O
for	O
every	O
n	O
sea	O
,	O
n	O
)	O
:	O
:	O
:	O
:	O
t	O
(	O
~	O
)	O
.	O
i=o	O
1	O
proof	O
.	O
recall	O
the	O
definition	B
of	I
the	O
shatter	O
coefficients	O
sea	O
,	O
n	O
)	O
=	O
max	O
na	O
(	O
xl	O
,	O
''	O
''	O
xn	O
)	O
,	O
(	O
xj	O
,	O
...	O
,	O
xn	O
)	O
where	O
na	O
(	O
xi	O
,	O
...	O
,	O
xn	O
)	O
=	O
i	O
{	O
{	O
xi	O
,	O
...	O
,	O
xn	O
}	O
na	O
;	O
a	O
e	O
all·	O
clearly	O
,	O
it	O
suffices	O
to	O
prove	O
that	O
for	O
every	O
xl	O
,	O
...	O
,	O
x	O
n	O
,	O
but	O
since	O
n	O
a	O
(	O
xi	O
,	O
...	O
,	O
xn	O
)	O
is	O
just	O
the	O
shatter	B
coefficient	I
of	O
the	O
class	O
of	O
finite	O
sets	O
we	O
need	O
only	O
to	O
prove	O
the	O
theorem	B
for	O
finite	O
sets	O
.	O
we	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
a	O
is	O
a	O
class	O
of	O
subsets	O
of	O
{	O
xl	O
,	O
...	O
,	O
xn	O
}	O
with	O
vc	O
dimension	B
va.	O
note	O
that	O
in	O
this	O
case	O
sea	O
,	O
n	O
)	O
=	O
iai	O
.	O
we	O
prove	O
the	O
theorem	B
by	O
induction	O
with	O
respect	O
to	O
n	O
and	O
va.	O
the	O
statement	O
is	O
obviously	O
true	O
for	O
n	O
=	O
1	O
for	O
any	O
class	O
with	O
v	O
a	O
~	O
1.	O
it	O
is	O
also	O
true	O
for	O
any	O
n	O
~	O
1	O
if	O
va	O
=	O
0	O
,	O
since	O
sea	O
,	O
n	O
)	O
=	O
1	O
for	O
all	O
n	O
in	O
this	O
case	O
.	O
thus	O
,	O
we	O
assume	O
va	O
2	O
:	O
:	O
1.	O
assume	O
that	O
the	O
statement	O
is	O
true	O
for	O
all	O
k	O
<	O
n	O
for	O
all	O
classes	O
of	O
subsets	O
of	O
{	O
xl	O
,	O
...	O
,	O
xd	O
with	O
vc	O
dimension	B
not	O
exceeding	O
va	O
,	O
and	O
for	O
n	O
for	O
all	O
classes	O
with	O
vc	O
dimension	B
smaller	O
than	O
va.	O
define	O
the	O
following	O
two	O
classes	O
of	O
subsets	O
of	O
{	O
xl	O
,	O
...	O
,	O
xn	O
}	O
:	O
a	O
'	O
=	O
{	O
a	O
-	O
{	O
xn	O
}	O
;	O
a	O
e	O
a	O
}	O
,	O
and	O
13.1	O
shatter	O
coefficients	O
and	O
vc	B
dimension	I
217	O
note	O
that	O
both	O
a	O
'	O
and	O
a	O
contain	O
subsets	O
of	O
{	O
xl	O
,	O
...	O
,	O
xn-d.	O
a	O
contains	O
all	O
sets	O
a	O
that	O
are	O
members	O
of	O
a	O
such	O
that	O
au	O
{	O
xn	O
}	O
is	O
also	O
in	O
a	O
,	O
but	O
xn	O
tj	O
.	O
a.	O
then	O
iai	O
=	O
ia'i	O
+	O
iai	O
.	O
to	O
see	O
this	O
,	O
write	O
a	O
'	O
=	O
{	O
a	O
-	O
{	O
xn	O
}	O
:	O
xn	O
e	O
a	O
,	O
a	O
e	O
aru	O
{	O
a	O
-	O
{	O
xn	O
}	O
:	O
xn	O
tj	O
.	O
a	O
,	O
a	O
e	O
a	O
}	O
=	O
8	O
1	O
u	O
8	O
2.	O
thus	O
,	O
ia'i	O
1811	O
+	O
182	O
1	O
-	O
181	O
n	O
8	O
2	O
1	O
i	O
{	O
a	O
-	O
{	O
xn	O
}	O
:	O
xn	O
e	O
a	O
,	O
a	O
e	O
a	O
}	O
i	O
+	O
i	O
{	O
a	O
-	O
{	O
xn	O
}	O
:	O
xn	O
tj	O
.	O
a	O
,	O
a	O
e	O
a	O
}	O
i	O
-	O
i	O
{	O
a	O
:	O
xn	O
e	O
a	O
,	O
a	O
e	O
a	O
}	O
i	O
+	O
i	O
{	O
a	O
:	O
xn	O
tj	O
.	O
a	O
,	O
a	O
e	O
a	O
}	O
i	O
-	O
iai	O
=	O
iai	O
iai-iai	O
.	O
since	O
ia'i	O
:	O
s	O
iai	O
,	O
and	O
a	O
'	O
is	O
a	O
class	O
of	O
subsets	O
of	O
{	O
xl	O
,	O
...	O
,	O
xn-d	O
,	O
the	O
induction	O
hypothesis	O
implies	O
that	O
ia'i	O
=	O
sea	O
'	O
,	O
n	O
~	O
(	O
n	O
-1	O
)	O
.	O
1	O
)	O
.	O
:	O
s	O
~	O
.	O
i=o	O
1	O
next	O
we	O
show	O
that	O
va.	O
:	O
s	O
va	O
-	O
1	O
,	O
which	O
will	O
imply	O
by	O
the	O
induction	O
hypothesis	O
.	O
to	O
see	O
this	O
,	O
consider	O
a	O
set	O
s	O
c	O
{	O
xl	O
,	O
...	O
,	O
xn-d	O
that	O
is	O
shattered	O
by	O
a.	O
then	O
s	O
u	O
{	O
xn	O
}	O
is	O
shattered	O
by	O
a.	O
to	O
prove	O
this	O
we	O
have	O
to	O
show	O
that	O
any	O
set	O
s	O
'	O
c	O
sand	O
s	O
'	O
u	O
{	O
xn	O
}	O
is	O
the	O
intersection	O
of	O
s	O
u	O
{	O
xn	O
}	O
and	O
a	O
set	O
from	O
a.	O
since	O
s	O
is	O
shattered	O
by	O
a	O
,	O
if	O
s	O
'	O
c	O
s	O
,	O
then	O
there	O
exists	O
a	O
set	O
a	O
e	O
a	O
such	O
that	O
s	O
'	O
=	O
s	O
n	O
a.	O
but	O
since	O
by	O
definition	O
xn	O
tj	O
.	O
a	O
,	O
we	O
must	O
have	O
s	O
'	O
=	O
(	O
s	O
u	O
{	O
xnd	O
n	O
a	O
and	O
sf	O
u	O
{	O
xn	O
}	O
=	O
(	O
s	O
u	O
{	O
xnd	O
n	O
(	O
au	O
{	O
xn	O
}	O
)	O
.	O
since	O
by	O
the	O
definition	B
of	I
a	O
both	O
a	O
and	O
au	O
{	O
xn	O
}	O
are	O
in	O
a	O
,	O
we	O
see	O
that	O
s	O
u	O
{	O
xn	O
}	O
is	O
indeed	O
shattered	O
by	O
a.	O
but	O
any	O
set	O
that	O
is	O
shattered	O
by	O
a	O
must	O
have	O
cardinality	O
not	O
exceeding	O
va	O
,	O
therefore	O
lsi	O
.	O
:	O
s	O
va	O
-	O
l.	O
but	O
s	O
was	O
an	O
arbitrary	O
set	O
shattered	O
by	O
a	O
,	O
which	O
means	O
va	O
.	O
:	O
s	O
va	O
-	O
1.	O
thus	O
,	O
we	O
have	O
shown	O
that	O
1	O
)	O
va-l	O
(	O
1	O
)	O
sea	O
,	O
n	O
)	O
=	O
iai	O
=	O
ia'i	O
+	O
iai	O
:	O
s	O
l	O
n	O
~	O
+	O
l	O
n	O
~	O
.	O
va	O
(	O
i=o	O
1	O
i=o	O
1	O
straightforward	O
application	O
of	O
the	O
identity	O
(	O
7	O
)	O
=	O
c~l	O
)	O
+	O
(	O
7=	O
:	O
)	O
shows	O
that	O
218	O
13.	O
comhinatorial	O
aspects	O
of	O
vapnik-chervonenkis	O
theory	O
theorem	B
13.2	O
has	O
some	O
very	O
surprising	O
implications	O
.	O
for	O
example	O
,	O
it	O
follows	O
immediately	O
from	O
the	O
binomial	B
theorem	I
that	O
sea	O
,	O
n	O
)	O
:	O
:	O
:	O
(	O
n	O
+	O
1	O
)	O
va.	O
this	O
means	O
that	O
a	O
shatter	O
coefficient	O
falls	O
in	O
one	O
of	O
two	O
categories	O
:	O
either	O
sea	O
,	O
n	O
)	O
=	O
2n	O
for	O
all	O
n	O
,	O
or	O
sea	O
,	O
n	O
)	O
:	O
:	O
:	O
(	O
n	O
+	O
l	O
)	O
va	O
,	O
which	O
happens	O
if	O
the	O
vc	B
dimension	I
of	O
a	O
is	O
finite	O
.	O
we	O
can	O
not	O
have	O
sea	O
,	O
n	O
)	O
~	O
2jn	O
,	O
for	O
example	O
.	O
if	O
va	O
<	O
00	O
,	O
the	O
upper	O
bound	O
in	O
theorem	O
12.5	O
decreases	O
exponentially	O
quickly	O
with	O
n.	O
other	O
sharper	O
bounds	O
are	O
given	O
below	O
.	O
theorem	B
13.3.	O
for	O
all	O
n	O
>	O
2	O
v	O
,	O
sea	O
,	O
n	O
)	O
:	O
:	O
:	O
l	O
.	O
:	O
:	O
:	O
-	O
va	O
(	O
n	O
)	O
(	O
en	O
)	O
va	O
i=o	O
l	O
va	O
theorem	B
13.3	O
follows	O
from	O
theorem	B
13.4	O
below	O
.	O
we	O
leave	O
the	O
details	O
as	O
an	O
exercise	O
(	O
see	O
problem	O
13.2	O
)	O
.	O
theorem	B
13.4.	O
for	O
all	O
n	O
2	O
:	O
1	O
and	O
va	O
<	O
n	O
/2	O
,	O
h	O
(	O
~	O
)	O
sea	O
,	O
n	O
)	O
:	O
:	O
:	O
en	O
n	O
,	O
where	O
h	O
(	O
x	O
)	O
=	O
-x	O
logx	O
-	O
0	O
-	O
x	O
)	O
logo	O
-	O
x	O
)	O
for	O
x	O
e	O
(	O
0	O
,	O
1	O
)	O
,	O
and	O
h	O
(	O
o	O
)	O
=	O
ho	O
)	O
=	O
0	O
is	O
the	O
binary	B
entropy	O
function	O
.	O
theorem	B
13.4	O
is	O
a	O
consequence	O
of	O
theorem	O
13.2	O
,	O
and	O
the	O
inequality	B
below	O
.	O
a	O
different	O
,	O
probabilistic	O
proof	O
is	O
sketched	O
in	O
problem	O
13.3	O
(	O
see	O
also	O
problem	O
13.4	O
)	O
.	O
lemma	O
13.1.	O
for	O
k	O
<	O
n	O
/2	O
,	O
proof	O
.	O
introduce	O
)	O
.	O
=	O
kin	O
:	O
:	O
:	O
1/2	O
.	O
by	O
the	O
binomial	B
theorem	I
,	O
(	O
since	O
)	O
./0	O
-	O
)	O
.	O
)	O
:	O
:	O
:	O
1	O
)	O
e	O
-nh	O
(	O
``	O
a	O
)	O
t	O
(	O
~	O
)	O
,	O
i==o	O
l	O
=	O
the	O
desired	O
inequality	B
.	O
0	O
13.2	O
shatter	O
coefficients	O
of	O
some	O
classes	O
219	O
remark	O
.	O
the	O
binary	B
entropy	O
function	O
h	O
(	O
x	O
)	O
plays	O
a	O
central	O
role	O
in	O
information	O
theory	O
(	O
see	O
,	O
e.g.	O
,	O
csiszar	O
and	O
korner	O
(	O
1981	O
)	O
,	O
cover	O
and	O
thomas	O
(	O
1991	O
»	O
.	O
its	O
main	O
properties	O
are	O
the	O
following	O
:	O
h	O
(	O
x	O
)	O
is	O
symmetric	O
around	O
1/2	O
,	O
where	O
it	O
takes	O
its	O
maximum	O
.	O
it	O
is	O
continuous	O
,	O
concave	O
,	O
strictly	O
monotone	O
increasing	O
in	O
[	O
0	O
,	O
1/2	O
]	O
,	O
decreasing	O
in	O
[	O
1/2	O
,	O
1	O
]	O
,	O
and	O
equals	O
zero	O
for	O
x	O
=	O
0	O
,	O
and	O
x	O
=	O
1	O
.	O
0	O
next	O
we	O
present	O
some	O
simple	O
results	O
about	O
shatter	O
coefficients	O
of	O
classes	O
that	O
are	O
obtained	O
by	O
combinations	O
of	O
classes	O
of	O
sets	O
.	O
theorem	B
13.5	O
.	O
(	O
i	O
)	O
if	O
a	O
=	O
al	O
u	O
a	O
2	O
,	O
then	O
sea	O
,	O
n	O
)	O
:	O
:s	O
s	O
(	O
ai	O
,	O
n	O
)	O
+	O
s	O
(	O
a2	O
,	O
n	O
)	O
.	O
(	O
ii	O
)	O
given	O
a	O
class	O
a	O
define	O
ac	O
=	O
{	O
a	O
c	O
(	O
iii	O
)	O
for	O
a	O
=	O
{	O
a	O
n	O
a	O
;	O
a	O
e	O
a	O
,	O
a	O
e	O
a	O
}	O
sea	O
,	O
n	O
)	O
:	O
:s	O
sea	O
,	O
n	O
)	O
s	O
(	O
a	O
,	O
n	O
)	O
.	O
(	O
iv	O
)	O
for	O
a	O
=	O
{	O
au	O
a	O
;	O
a	O
e	O
a	O
,	O
a	O
e	O
a	O
}	O
,	O
sea	O
,	O
n	O
)	O
:	O
:s	O
sea	O
,	O
n	O
)	O
s	O
(	O
a	O
,	O
n	O
)	O
.	O
(	O
v	O
)	O
for	O
a	O
=	O
{	O
a	O
x	O
a	O
;	O
a	O
e	O
a	O
,	O
a	O
e	O
a	O
}	O
,	O
sea	O
,	O
n	O
)	O
:	O
:s	O
sea	O
,	O
n	O
)	O
s	O
(	O
a	O
,	O
n	O
)	O
.	O
;	O
a	O
e	O
a	O
}	O
.	O
then	O
s	O
(	O
ac	O
,	O
n	O
)	O
=	O
sea	O
,	O
n	O
)	O
.	O
proof	O
.	O
(	O
i	O
)	O
,	O
(	O
ii	O
)	O
,	O
and	O
(	O
v	O
)	O
are	O
trivial	O
.	O
to	O
prove	O
(	O
iii	O
)	O
,	O
fix	O
n	O
points	O
xl	O
,	O
...	O
,	O
x	O
n	O
,	O
and	O
assume	O
that	O
a	O
picks	O
n	O
:	O
:s	O
sea	O
,	O
n	O
)	O
subsets	O
c	O
i	O
,	O
...	O
,	O
cn	O
.	O
then	O
a	O
picks	O
from	O
c	O
at	O
most	O
sea	O
,	O
icil	O
)	O
subsets	O
.	O
therefore	O
,	O
sets	O
of	O
the	O
form	O
ana	O
pick	O
at	O
most	O
n	O
l	O
sea	O
,	O
i	O
ci	O
i	O
)	O
:	O
:s	O
sea	O
,	O
n	O
)	O
s	O
(	O
a	O
,	O
n	O
)	O
i=l	O
subsets	O
.	O
here	O
we	O
used	O
the	O
obvious	O
monotonicity	O
property	O
sea	O
,	O
n	O
)	O
:	O
:s	O
sea	O
,	O
n	O
+	O
m	O
)	O
.	O
to	O
prove	O
(	O
iv	O
)	O
,	O
observe	O
that	O
{	O
a	O
u	O
a	O
;	O
a	O
e	O
a	O
,	O
a	O
e	O
a	O
}	O
=	O
{	O
(	O
ac	O
n	O
a	O
c	O
)	O
c	O
;	O
a	O
e	O
a	O
,	O
a	O
e	O
a	O
}	O
.	O
the	O
statement	O
now	O
follows	O
from	O
(	O
ii	O
)	O
and	O
(	O
iii	O
)	O
.	O
0	O
13.2	O
shatter	O
coefficients	O
of	O
some	O
classes	O
here	O
we	O
calculate	O
shatter	O
coefficients	O
of	O
some	O
simple	O
but	O
important	O
examples	O
of	O
classes	O
of	O
subsets	O
of	O
nd	O
.	O
we	O
begin	O
with	O
a	O
simple	O
observation	O
.	O
theorem	B
13.6.	O
if	O
a	O
contains	O
finitely	O
many	O
sets	O
,	O
then	O
va	O
<	O
sea	O
,	O
n	O
)	O
:	O
:s	O
iai	O
for	O
every	O
n.	O
log2	O
lai	O
,	O
and	O
proof	O
.	O
the	O
first	O
inequality	B
follows	O
from	O
the	O
fact	O
that	O
at	O
least	O
2n	O
sets	O
are	O
necessary	O
to	O
shatter	O
n	O
points	O
.	O
the	O
second	O
inequality	B
is	O
trivial	O
.	O
0	O
220	O
13.	O
combinatorial	O
aspects	O
of	O
vapnik	O
-chervonenkis	O
theory	O
in	O
the	O
next	O
example	O
,	O
it	O
is	O
interesting	O
to	O
observe	O
that	O
the	O
bound	O
of	O
theorem	O
13.2	O
is	O
tight	O
.	O
theorem	B
13.7	O
.	O
(	O
i	O
)	O
if	O
a	O
is	O
the	O
class	O
of	O
all	O
half	O
lines	O
:	O
a	O
==	O
{	O
(	O
-00	O
,	O
x	O
]	O
;	O
x	O
e	O
r	O
}	O
,	O
then	O
va	O
==	O
1	O
,	O
and	O
s	O
(	O
a	O
,	O
n	O
)	O
=	O
n	O
+	O
1	O
=	O
(	O
~	O
)	O
+	O
(	O
:	O
)	O
,	O
(	O
ii	O
)	O
if	O
a	O
is	O
the	O
class	O
of	O
all	O
intervals	O
in	O
r	O
,	O
then	O
va	O
==	O
2	O
,	O
and	O
sea	O
,	O
n	O
)	O
==	O
n	O
(	O
n	O
+	O
1	O
)	O
2	O
+	O
1	O
==	O
°	O
+	O
1	O
+	O
2	O
(	O
n	O
)	O
(	O
n	O
)	O
(	O
n	O
)	O
.	O
proof	O
.	O
(	O
i	O
)	O
is	O
easy	O
.	O
to	O
see	O
that	O
va	O
==	O
2	O
in	O
(	O
ii	O
)	O
,	O
observe	O
that	O
if	O
we	O
fix	O
three	O
different	O
points	O
in	O
r	O
,	O
then	O
there	O
is	O
no	O
interval	O
that	O
does	O
not	O
contain	O
the	O
middle	O
point	O
,	O
but	O
does	O
contain	O
the	O
other	O
two	O
.	O
the	O
shatter	B
coefficient	I
can	O
be	O
calculated	O
by	O
counting	O
that	O
there	O
are	O
at	O
most	O
n	O
-	O
k	O
+	O
1	O
sets	O
in	O
{	O
a	O
n	O
{	O
xl	O
,	O
...	O
,	O
x	O
n	O
}	O
;	O
a	O
e	O
a	O
}	O
such	O
that	O
ia	O
n	O
{	O
xl	O
,	O
...	O
,	O
xn	O
}	O
1	O
==	O
k	O
for	O
k	O
==	O
1	O
,	O
...	O
,	O
n	O
,	O
and	O
one	O
set	O
(	O
namely	O
0	O
)	O
such	O
that	O
ia	O
n	O
{	O
x	O
1	O
,	O
...	O
,	O
xn	O
}	O
i	O
==	O
0.	O
this	O
gives	O
altogether	O
o	O
1	O
+	O
l..-	O
(	O
n	O
-	O
k	O
+	O
1	O
)	O
=	O
k=l	O
n	O
(	O
n	O
+	O
1	O
)	O
2	O
+	O
1	O
.	O
0	O
now	O
we	O
can	O
generalize	O
the	O
result	O
above	O
for	O
classes	O
of	O
intervals	O
and	O
rectangles	O
in	O
rd	O
:	O
theorem	B
13.8.	O
if	O
a	O
=	O
{	O
(	O
-00	O
,	O
xd	O
x	O
...	O
x	O
(	O
-00	O
,	O
xd	O
]	O
}	O
,	O
then	O
va	O
=	O
d.	O
(	O
i	O
)	O
(	O
ii	O
)	O
if	O
a	O
is	O
the	O
class	O
of	O
all	O
rectangles	O
in	O
r	O
d	O
,	O
then	O
va	O
=	O
2d	O
.	O
proof	O
.	O
we	O
prove	O
(	O
ii	O
)	O
.	O
the	O
first	O
part	O
is	O
left	O
as	O
an	O
exercise	O
(	O
problem	O
13.5	O
)	O
.	O
we	O
have	O
to	O
show	O
that	O
there	O
are	O
2d	O
points	O
that	O
can	O
be	O
shattered	O
by	O
a	O
,	O
but	O
for	O
any	O
set	O
of	O
2d	O
+	O
1	O
points	O
there	O
is	O
a	O
subset	O
of	O
it	O
that	O
can	O
not	O
be	O
picked	O
by	O
sets	O
in	O
a.	O
to	O
see	O
the	O
first	O
part	O
just	O
consider	O
the	O
following	O
2d	O
points	O
:	O
(	O
1,0,0	O
,	O
...	O
,0	O
)	O
,	O
(	O
0	O
,	O
1,0,0	O
,	O
...	O
,0	O
)	O
,	O
...	O
,	O
(	O
0,0	O
,	O
...	O
,0	O
,	O
1	O
)	O
,	O
(	O
-1,0,0	O
,	O
...	O
,0	O
)	O
,	O
(	O
0	O
,	O
-1,0,0	O
,	O
...	O
,0	O
)	O
,	O
...	O
,	O
(	O
0,0	O
,	O
...	O
,0	O
,	O
-1	O
)	O
,	O
(	O
see	O
figure	O
13.1	O
)	O
.	O
13.2	O
shatter	O
coefficients	O
of	O
some	O
classes	O
221	O
figure	O
13.1	O
.	O
24	O
=	O
16	O
rectangles	O
shatter	O
4	O
points	O
in	O
n2	O
.	O
[	O
!	O
]	O
[	O
!	O
]	O
0	O
[	O
!	O
]	O
gj	O
on	O
the	O
other	O
hand	O
,	O
for	O
any	O
given	O
set	O
of	O
2d	O
+	O
1	O
points	O
we	O
can	O
choose	O
a	O
subset	O
of	O
at	O
most	O
2d	O
points	O
with	O
the	O
property	O
that	O
it	O
contains	O
a	O
point	O
with	O
largest	O
first	O
coordinate	O
,	O
a	O
point	O
with	O
smallest	O
first	O
coordinate	O
,	O
a	O
point	O
with	O
largest	O
second	O
coordinate	O
,	O
and	O
so	O
forth	O
.	O
clearly	O
,	O
there	O
is	O
no	O
set	O
in	O
a	O
that	O
contains	O
these	O
points	O
,	O
but	O
does	O
not	O
contain	O
the	O
others	O
(	O
figure	O
13.2	O
)	O
.	O
d	O
max	O
y	O
figure	O
13.2.	O
no	O
5	O
points	O
can	O
be	O
shattered	O
by	O
rectangles	O
in	O
n2	O
.	O
minx	O
o	O
z	O
max	O
x	O
miny	O
theorem	B
13.9	O
.	O
(	O
steele	O
(	O
1975	O
)	O
,	O
dudley	O
(	O
1978	O
)	O
)	O
.	O
let	O
9	O
be	O
afinite-dimensional	O
vector	O
space	O
of	O
real	O
functions	O
on	O
nd	O
.	O
the	O
class	O
of	O
sets	O
a	O
=	O
{	O
{	O
x	O
:	O
g	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
o	O
}	O
:	O
g	O
e	O
g	O
}	O
has	O
vc	B
dimension	I
va	O
:	O
:	O
:	O
r	O
,	O
where	O
r	O
=	O
dimension	B
(	O
g	O
)	O
.	O
proof	O
.	O
it	O
suffices	O
to	O
show	O
that	O
no	O
set	O
of	O
size	O
m	O
=	O
1	O
+	O
r	O
can	O
be	O
shattered	O
by	O
sets	O
of	O
the	O
form	O
{	O
x	O
:	O
g	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
o	O
}	O
.	O
fix	O
m	O
arbitrary	O
points	O
xl	O
,	O
...	O
,	O
x	O
m	O
,	O
and	O
define	O
the	O
linear	O
mapping	O
l	O
:	O
9	O
-+	O
nm	O
as	O
222	O
13.	O
combinatorial	O
aspects	O
of	O
vapnik	O
-chervonenkis	O
theory	O
then	O
the	O
image	O
of	O
q	O
,	O
l	O
(	O
q	O
)	O
,	O
is	O
a	O
linear	O
subspace	O
of	O
nm	O
of	O
dimension	O
not	O
ex	O
(	O
cid:173	O
)	O
ceeding	O
the	O
dimension	B
of	O
q	O
,	O
that	O
is	O
,	O
m	O
-	O
1.	O
then	O
there	O
exists	O
a	O
nonzero	O
vector	O
y	O
=	O
(	O
yl	O
,	O
...	O
,	O
ym	O
)	O
e	O
n	O
m	O
,	O
that	O
is	O
orthogonal	O
to	O
l	O
(	O
q	O
)	O
,	O
that	O
is	O
,	O
for	O
every	O
g	O
e	O
q	O
we	O
can	O
assume	O
that	O
at	O
least	O
one	O
of	O
the	O
yi	O
's	O
is	O
negative	O
.	O
rearrange	O
this	O
equality	O
so	O
that	O
terms	O
with	O
nonnegative	O
yi	O
stay	O
on	O
the	O
left-hand	O
side	O
:	O
l	O
yig	O
(	O
xi	O
)	O
=	O
l	O
-yig	O
(	O
xi	O
)	O
'	O
i	O
:	O
yi	O
:	O
:	O
:	O
o	O
i	O
:	O
yi	O
<	O
o	O
now	O
,	O
suppose	O
that	O
there	O
exists	O
age	O
q	O
such	O
that	O
the	O
set	O
{	O
x	O
:	O
g	O
(	O
x	O
)	O
~	O
o	O
}	O
picks	O
exactly	O
the	O
xi	O
's	O
on	O
the	O
left-hand	O
side	O
.	O
then	O
all	O
terms	O
on	O
the	O
left-hand	O
side	O
are	O
nonnegative	O
,	O
while	O
the	O
terms	O
on	O
the	O
right-hand	O
side	O
must	O
be	O
negative	O
,	O
which	O
is	O
a	O
contradiction	O
,	O
so	O
xl	O
,	O
...	O
,	O
xm	O
can	O
not	O
be	O
shattered	O
,	O
and	O
the	O
proof	O
is	O
completed	O
.	O
0	O
remark	O
.	O
theorem	B
13.2	O
implies	O
that	O
the	O
shatter	O
coefficients	O
of	O
the	O
class	O
of	O
sets	O
in	O
theorem	O
13.9	O
are	O
bounded	O
as	O
follows	O
:	O
sea	O
,	O
n	O
)	O
:	O
s	O
t	O
(	O
~	O
)	O
.	O
i=o	O
1	O
in	O
many	O
cases	O
it	O
is	O
possible	O
to	O
get	O
sharper	O
estimates	O
.	O
let	O
be	O
the	O
linear	O
space	O
of	O
functions	O
spanned	O
by	O
some	O
fixed	O
functions	O
1/fi	O
,	O
...	O
,	O
1jf	O
r	O
n	O
d	O
-+	O
n	O
,	O
and	O
define	O
\li	O
(	O
x	O
)	O
=	O
(	O
o/l	O
(	O
x	O
)	O
,	O
...	O
,	O
o/r	O
(	O
x	O
»	O
.	O
cover	O
(	O
1965	O
)	O
showed	O
that	O
if	O
for	O
some	O
xl	O
,	O
...	O
,	O
xn	O
e	O
nd	O
,	O
every	O
r-element	O
subset	O
of	O
\li	O
(	O
xl	O
)	O
,	O
...	O
,	O
\li	O
(	O
xn	O
)	O
is	O
linearly	O
independent	O
,	O
then	O
the	O
n-th	O
shatter	B
coefficient	I
of	O
the	O
class	O
of	O
sets	O
a	O
=	O
{	O
{	O
x	O
:	O
g	O
(	O
x	O
)	O
~	O
o	O
}	O
:	O
g	O
e	O
q	O
}	O
actually	O
equals	O
1	O
)	O
sea	O
,	O
n	O
)	O
=	O
2	O
l	O
n	O
~	O
1'-1	O
(	O
i=o	O
1	O
(	O
see	O
problem	O
13.6	O
)	O
.	O
by	O
using	O
the	O
last	O
identity	O
in	O
the	O
proof	O
of	O
theorem	O
13.2	O
,	O
it	O
is	O
easily	O
seen	O
that	O
the	O
difference	O
between	O
the	O
bound	O
obtained	O
from	O
theorem	B
13.9	O
and	O
the	O
true	O
value	O
is	O
(	O
n~l	O
)	O
.	O
using	O
cover	O
's	O
result	O
for	O
the	O
shatter	O
coefficients	O
we	O
can	O
actually	O
improve	O
theorem	B
13.9	O
,	O
in	O
that	O
the	O
vc	B
dimension	I
of	O
the	O
class	O
a	O
equals	O
r.	O
to	O
see	O
this	O
,	O
note	O
that	O
13.2	O
shatter	O
coefficients	O
of	O
some	O
classes	O
223	O
while	O
sea	O
,	O
r	O
+	O
1	O
)	O
=	O
2	O
~	O
(	O
~	O
)	O
=	O
2	O
t	O
(	O
~	O
)	O
-2	O
(	O
r	O
)	O
=	O
2·	O
2r	O
-	O
2	O
<	O
2r+l	O
.	O
i=o	O
l	O
i=o	O
l	O
.	O
r	O
therefore	O
no	O
r	O
+	O
1	O
points	O
are	O
shattered	O
.	O
it	O
is	O
interesting	O
that	O
theorem	B
13.9	O
above	O
combined	O
with	O
theorem	O
13.2	O
and	O
theorem	B
13.3	O
gives	O
the	O
bound	O
s	O
(	O
a	O
,	O
n	O
)	O
s	O
nr	O
+	O
1	O
when	O
r	O
>	O
2.	O
cover	O
's	O
result	O
,	O
however	O
,	O
improves	O
it	O
to	O
sea	O
,	O
n	O
)	O
s	O
2	O
(	O
n	O
-	O
1y-1	O
+	O
2	O
.	O
0	O
perhaps	O
the	O
most	O
important	O
class	O
of	O
sets	O
is	O
the	O
class	O
of	O
halfspaces	O
in	O
rd	O
,	O
that	O
is	O
,	O
sets	O
containing	O
points	O
falling	O
on	O
one	O
side	O
of	O
a	O
hyperplane	O
.	O
the	O
shatter	O
coefficients	O
of	O
this	O
class	O
can	O
be	O
obtained	O
from	O
the	O
results	O
above	O
:	O
corollary	O
13.1.	O
let	O
a	O
be	O
the	O
class	O
of	O
half	O
spaces	O
,	O
that	O
is	O
,	O
subsets	O
ofrd	O
of	O
the	O
form	O
{	O
x	O
:	O
ax	O
:	O
:	O
:	O
:	O
b	O
}	O
,	O
where	O
a	O
e	O
rd	O
,	O
be	O
rtakeallpossiblevalues	O
.	O
then	O
va	O
=	O
d+1	O
,	O
and	O
proof	O
.	O
this	O
is	O
an	O
immediate	O
consequence	O
of	O
the	O
remark	O
above	O
if	O
we	O
take	O
9	O
to	O
be	O
the	O
linear	O
space	O
spanned	O
by	O
the	O
functions	O
(	O
h	O
(	O
x	O
)	O
=	O
x	O
(	O
l	O
)	O
,	O
(	O
h	O
(	O
x	O
)	O
=	O
x	O
(	O
2	O
)	O
,	O
...	O
,	O
<	O
pd	O
(	O
x	O
)	O
=	O
xed	O
)	O
,	O
and	O
<	O
pd+i	O
(	O
x	O
)	O
=	O
1	O
,	O
where	O
x	O
(	O
l	O
)	O
,	O
...	O
,	O
xed	O
)	O
denote	O
the	O
d	O
components	O
of	O
the	O
vector	O
x	O
.	O
0	O
it	O
is	O
equally	O
simple	O
now	O
to	O
obtain	O
an	O
upper	O
bound	O
on	O
the	O
vc	B
dimension	I
of	O
the	O
class	O
of	O
all	O
closed	O
balls	O
in	O
rd	O
(	O
see	O
cover	O
(	O
1965	O
)	O
,	O
devroye	O
(	O
1978	O
)	O
,	O
or	O
dudley	O
(	O
1979	O
)	O
for	O
more	O
information	O
)	O
.	O
corollary	O
13.2.	O
let	O
a	O
be	O
the	O
class	O
of	O
all	O
closed	O
balls	O
in	O
r	O
d	O
,	O
that	O
is	O
,	O
subsets	O
of	O
rd	O
of	O
the	O
form	O
where	O
ai	O
,	O
...	O
,	O
ad	O
,	O
b	O
e	O
r	O
take	O
all	O
possible	O
values	O
.	O
then	O
va	O
s	O
d	O
+	O
2.	O
proof	O
.	O
if	O
we	O
write	O
d	O
l	O
i=l	O
ix	O
(	O
i	O
)	O
-	O
ail	O
2	O
d	O
-	O
b	O
=	O
l	O
i=l	O
ix	O
(	O
i	O
)	O
1	O
2	O
d	O
d	O
-	O
2	O
lx	O
(	O
i	O
)	O
ai	O
+	O
lal-	O
b	O
,	O
i=1	O
i=l	O
224	O
13.	O
combinatorial	O
aspects	O
of	O
vapnik-chervonenkis	O
theory	O
then	O
it	O
is	O
clear	O
that	O
theorem	B
13.9	O
yields	O
the	O
result	O
by	O
setting	O
9	O
to	O
be	O
the	O
linear	O
space	O
spanned	O
by	O
d	O
(	O
h	O
(	O
x	O
)	O
=	O
l	O
ixu	O
)	O
i	O
z	O
,	O
(	O
h	O
(	O
x	O
)	O
=	O
x	O
(	O
l	O
)	O
,	O
...	O
,	O
¢d+l	O
(	O
x	O
)	O
=	O
xed	O
)	O
,	O
and	O
¢d+z	O
(	O
x	O
)	O
=	O
1	O
.	O
0	O
i=l	O
it	O
follows	O
from	O
theorems	O
13.9	O
and	O
13.5	O
(	O
iii	O
)	O
that	O
the	O
class	O
of	O
all	O
polytopes	O
with	O
a	O
bounded	O
number	O
of	O
faces	O
has	O
finite	O
vc	B
dimension	I
.	O
the	O
next	O
negative	O
example	O
demonstrates	O
that	O
this	O
boundedness	O
is	O
necessary	O
.	O
theorem	B
13.10.	O
if	O
a	O
is	O
the	O
class	O
of	O
all	O
convex	O
polygons	O
in	O
r	O
z	O
,	O
then	O
va	O
=	O
00.	O
figure	O
13.3.	O
any	O
subset	O
of	O
n	O
points	O
on	O
the	O
unit	O
circle	O
can	O
be	O
picked	O
by	O
a	O
convex	O
polygon	O
.	O
proof	O
.	O
let	O
xl	O
,	O
...	O
,	O
xn	O
e	O
r2	O
lie	O
on	O
the	O
unit	O
circle	O
.	O
then	O
it	O
is	O
easy	O
to	O
see	O
that	O
for	O
any	O
subset	O
of	O
these	O
(	O
different	O
)	O
points	O
there	O
is	O
a	O
polygon	O
that	O
picks	O
that	O
subset	O
.	O
0	O
13.3	O
linear	O
and	O
generalized	O
linear	O
discrimination	O
rules	O
recall	O
from	O
chapter	O
4	O
that	O
a	O
linear	O
classification	O
rule	B
classifies	O
x	O
into	O
one	O
of	O
the	O
two	O
classes	O
according	O
to	O
whether	O
d	O
ao	O
+	O
laix	O
(	O
i	O
)	O
i=l	O
is	O
positive	O
or	O
negative	O
,	O
where	O
xci	O
)	O
,	O
...	O
,	O
x	O
(	O
d	O
)	O
denote	O
the	O
components	O
of	O
x	O
e	O
rd	O
.	O
the	O
coefficients	O
ai	O
are	O
determined	O
by	O
the	O
training	O
sequence	O
.	O
these	O
decisions	O
di	O
(	O
cid:173	O
)	O
chotomize	O
the	O
space	O
r	O
d	O
by	O
virtue	O
of	O
a	O
halfspace	O
,	O
and	O
assign	O
class	O
1	O
to	O
one	O
halfspace	O
,	O
and	O
class	O
0	O
to	O
the	O
other	O
.	O
points	O
on	O
the	O
border	O
are	O
treated	O
as	O
belonging	O
to	O
class	O
o.	O
consider	O
a	O
classifier	O
that	O
adjusts	O
the	O
coefficients	O
by	O
minimizing	O
the	O
number	O
of	O
13.3	O
linear	O
and	O
generalized	O
linear	O
discrimination	O
rules	O
225	O
errors	O
committed	O
on	O
dn	O
.	O
in	O
the	O
terminology	O
of	O
chapter	O
12	O
,	O
c	O
is	O
the	O
collection	O
of	O
all	O
linear	O
classifiers	O
.	O
o	O
x7	O
figure	O
13.4.	O
an	O
empirically	O
opti	O
(	O
cid:173	O
)	O
mal	O
linear	B
classifier	I
.	O
o	O
x	O
,	O
.	O
xs	O
o	O
x6	O
decide	O
class	O
0	O
o	O
x8	O
decide	O
class	O
1	O
.	O
xii	O
glick	O
(	O
1976	O
)	O
pointed	O
out	O
that	O
for	O
the	O
error	O
probability	O
l	O
(	O
¢l~	O
)	O
of	O
this	O
classifier	B
,	O
l	O
(	O
¢l~	O
)	O
-	O
inf¢ec	O
l	O
(	O
¢	O
)	O
-+	O
°	O
almost	O
surely	O
.	O
however	O
,	O
from	O
theorems	O
12.6	O
,	O
13.1	O
and	O
corollary	O
13.1	O
,	O
we	O
can	O
now	O
provide	O
more	O
details	O
:	O
theorem	B
13.11.	O
for	O
all	O
nand	O
e	O
>	O
0	O
,	O
the	O
error	O
probability	O
l	O
(	O
¢~	O
)	O
of	O
the	O
empirically	O
optimal	O
linear	O
classifier	B
satisfies	O
p	O
{	O
l	O
(	O
¢*	O
)	O
-	O
n	O
inf	O
l	O
(	O
¢	O
)	O
>	O
e	O
}	O
:	O
s	O
8nd+le-ne2/128	O
.	O
¢ec	O
comparing	O
the	O
above	O
inequality	B
with	O
theorem	B
4.5	O
,	O
note	O
that	O
there	O
we	O
selected	O
¢~	O
by	O
a	O
specific	O
algorithm	B
,	O
while	O
this	O
result	O
holds	O
for	O
any	O
linear	B
classifier	I
whose	O
empirical	B
error	I
is	O
minimal	O
.	O
generalized	O
linear	O
classification	O
rules	O
(	O
see	O
duda	O
and	O
hart	O
(	O
1973	O
»	O
are	O
defined	O
by	O
where	O
d*	O
is	O
a	O
positive	O
integer	O
,	O
the	O
functions	O
1/ii	O
,	O
...	O
,	O
0/	O
d*	O
are	O
fixed	O
,	O
and	O
the	O
coef	O
(	O
cid:173	O
)	O
ficients	O
ao	O
,	O
...	O
,	O
ad*	O
are	O
functions	O
of	O
the	O
data	O
dn	O
.	O
these	O
include	O
for	O
example	O
all	O
quadratic	O
discrimination	O
rules	O
in	O
n	O
d	O
when	O
we	O
choose	O
all	O
functions	O
that	O
are	O
either	O
components	O
of	O
x	O
,	O
or	O
squares	O
of	O
components	O
of	O
x	O
,	O
or	O
products	O
of	O
two	O
components	O
of	O
x.	O
that	O
is	O
,	O
the	O
functions	O
o/i	O
(	O
x	O
)	O
are	O
of	O
the	O
form	O
either	O
x	O
(	O
j	O
)	O
,	O
or	O
x	O
(	O
j	O
)	O
x	O
(	O
k	O
)	O
.	O
in	O
all	O
,	O
d*	O
=	O
2d	O
+	O
d	O
(	O
d	O
-	O
1	O
)	O
/2	O
.	O
the	O
argument	O
used	O
for	O
linear	O
discriminants	O
remain	O
valid	O
,	O
and	O
we	O
obtain	O
theorem	B
13.12.	O
let	O
c	O
be	O
the	O
class	O
of	O
generalized	O
linear	O
discriminants	O
(	O
i.e.	O
,	O
the	O
coefficients	O
vary	O
,	O
the	O
basis	O
functions	O
o/i	O
are	O
fixed	O
)	O
.	O
for	O
the	O
error	O
probability	O
l	O
(	O
¢l~	O
)	O
of	O
the	O
empirically	B
optimal	I
classifier	I
,	O
for	O
all	O
d*	O
>	O
1	O
,	O
n	O
and	O
e	O
>	O
0	O
,	O
we	O
have	O
226	O
13.	O
combinatorial	O
aspects	O
of	O
vapnik	O
-chervonenkis	O
theory	O
also	O
,	O
for	O
n	O
>	O
2d*	O
+	O
1	O
,	O
the	O
second	O
inequality	B
is	O
obtained	O
by	O
using	O
the	O
bound	O
of	O
theorem	O
13.4	O
for	O
the	O
shatter	O
coefficients	O
.	O
note	O
nevertheless	O
that	O
unless	O
d*	O
(	O
and	O
therefore	O
c	O
)	O
is	O
allowed	O
to	O
increase	O
with	O
n	O
,	O
there	O
is	O
no	O
hope	O
of	O
obtaining	O
universal	B
consistency	I
.	O
the	O
question	O
of	O
universal	O
consistency	B
will	O
be	O
addressed	O
in	O
chapters	O
17	O
and	O
18	O
.	O
13.4	O
convex	O
sets	O
and	O
monotone	O
layers	O
classes	O
of	O
infinite	O
vc	B
dimension	I
are	O
not	O
hopeless	O
by	O
any	O
means	O
.	O
in	O
this	O
section	O
,	O
we	O
offer	O
examples	O
that	O
will	O
show	O
how	O
they	O
may	O
be	O
useful	O
in	O
pattern	O
recognition	O
.	O
the	O
classes	O
of	O
interest	O
to	O
us	O
for	O
now	O
are	O
c	O
=	O
{	O
all	O
convex	O
sets	O
of	O
n	O
2	O
}	O
c	O
=	O
{	O
all	O
monotone	O
layers	O
of	O
n	O
2	O
,	O
i.e.	O
,	O
all	O
sets	O
of	O
the	O
form	O
{	O
(	O
xl	O
,	O
x2	O
)	O
:	O
x2	O
~	O
1jf	O
(	O
xl	O
)	O
}	O
for	O
some	O
nonincreasing	O
function	O
1jf	O
}	O
.	O
in	O
discrimination	O
,	O
this	O
corresponds	O
to	O
making	O
decisions	O
of	O
the	O
form	O
¢	O
(	O
x	O
)	O
=	O
i	O
{	O
xec	O
}	O
,	O
c	O
e	O
c	O
,	O
or	O
¢	O
(	O
x	O
)	O
=	O
i	O
{	O
x1-c	O
}	O
,	O
c	O
e	O
c	O
,	O
and	O
similarly	O
for	O
c.	O
decisions	O
of	O
these	O
forms	O
are	O
important	O
in	O
many	O
situations	O
.	O
for	O
example	O
,	O
if	O
1j	O
(	O
x	O
)	O
is	O
monotone	O
decreasing	O
in	O
both	O
components	O
of	O
x	O
e	O
n2	O
,	O
then	O
the	O
bayes	O
rule	B
is	O
of	O
the	O
form	O
g*	O
(	O
x	O
)	O
=	O
hx	O
:	O
el	O
}	O
for	O
some	O
l	O
e	O
c.	O
we	O
have	O
pointed	O
out	O
elsewhere	O
(	O
theorem	B
13.10	O
,	O
and	O
problem	O
13.19	O
)	O
that	O
vc	O
=	O
v.c	O
=	O
ex	O
)	O
.	O
to	O
see	O
this	O
,	O
note	O
that	O
any	O
set	O
of	O
n	O
points	O
on	O
the	O
unit	O
circle	O
is	O
shattered	O
by	O
c	O
,	O
while	O
any	O
set	O
of	O
n	O
points	O
on	O
the	O
antidiagonal	O
x2	O
=	O
-xl	O
is	O
shattered	O
by	O
c.	O
nevertheless	O
,	O
shattering	B
becomes	O
unlikely	O
if	O
x	O
has	O
a	O
density	O
.	O
our	O
starting	O
point	O
here	O
is	O
the	O
bound	O
obtained	O
while	O
proving	O
theorem	B
12.5	O
:	O
where	O
na	O
(	O
x	O
1	O
,	O
•••	O
,	O
xn	O
)	O
is	O
the	O
number	O
of	O
sets	O
in	O
{	O
a	O
n	O
{	O
xl	O
,	O
...	O
,	O
xn	O
}	O
:	O
a	O
e	O
a	O
}	O
.	O
the	O
following	O
theorem	B
is	O
essential	O
:	O
theorem	B
13.13.	O
if	O
x	O
has	O
a	O
density	O
f	O
on	O
n	O
2	O
,	O
then	O
when	O
a	O
is	O
either	O
c	O
or	O
£	O
.	O
this	O
theorem	B
,	O
a	O
proof	O
of	O
which	O
is	O
a	O
must	O
for	O
the	O
reader	O
,	O
implies	O
the	O
following	O
:	O
13.4	O
convex	O
sets	O
and	O
monotone	O
layers	O
227	O
corollary	O
13.3.	O
let	O
x	O
have	O
a	O
density	O
f	O
on	O
n2	O
.	O
let	O
¢~	O
be	O
picked	O
by	O
minimizing	O
the	O
empirical	B
error	I
over	O
all	O
classifiers	O
of	O
the	O
form	O
¢	O
(	O
x	O
)	O
=	O
{	O
01	O
if	O
x	O
e	O
a	O
if	O
x	O
tf	O
:	O
a	O
,	O
where	O
a	O
or	O
a	O
c	O
is	O
in	O
c	O
(	O
or	O
£	O
)	O
.	O
then	O
l	O
(	O
¢~	O
)	O
--	O
-+	O
inf	O
l	O
(	O
¢	O
)	O
¢=ic	O
for	O
c	O
or	O
cc	O
in	O
c	O
with	O
probability	O
one	O
(	O
and	O
similarly	O
for	O
£	O
)	O
.	O
proof	O
.	O
this	O
follows	O
from	O
the	O
inequality	B
of	O
lemma	O
8.2	O
,	O
theorem	B
13.13	O
,	O
and	O
the	O
borel-cantelli	O
lemma	O
.	O
d	O
remark	O
.	O
theorem	B
13	O
.13	O
and	O
the	O
corollary	O
may	O
be	O
extended	O
to	O
n	O
d	O
,	O
but	O
this	O
gen	O
(	O
cid:173	O
)	O
eralization	O
holds	O
nothing	O
new	O
and	O
will	O
only	O
result	O
in	O
tedious	O
notations	O
.	O
d	O
proof	O
of	O
theorem	O
13.13.	O
we	O
show	O
the	O
theorem	B
for	O
£	O
,	O
and	O
indicate	O
the	O
proof	O
for	O
c.	O
take	O
two	O
sequences	O
of	O
integers	O
,	O
m	O
and	O
r	O
,	O
where	O
m	O
``	O
''	O
..jii	O
and	O
r	O
``	O
''	O
m	O
1/3	O
,	O
so	O
that	O
m	O
--	O
-+	O
00	O
,	O
yet	O
r21m	O
--	O
-+	O
0	O
,	O
as	O
n	O
--	O
-+	O
00.	O
consider	O
the	O
set	O
[	O
-r	O
,	O
r	O
]	O
2	O
and	O
partition	B
each	O
side	O
into	O
m	O
equal	O
intervals	O
,	O
thus	O
obtaining	O
an	O
m	O
x	O
m	O
grid	O
of	O
square	O
cells	O
cl	O
,	O
...	O
,	O
cm	O
2.	O
denote	O
co	O
=	O
n2	O
-	O
[	O
-r	O
,	O
r	O
]	O
2.	O
let	O
no	O
,	O
nl	O
,	O
...	O
,	O
nm	O
2	O
be	O
the	O
number	O
of	O
points	O
among	O
xl	O
,	O
...	O
,	O
xn	O
that	O
belong	O
to	O
these	O
cells	O
.	O
the	O
vector	O
(	O
no	O
,	O
n	O
l	O
,	O
...	O
,	O
n	O
m2	O
)	O
is	O
clearly	O
multinomially	O
distributed	O
.	O
let	O
1/1	O
be	O
a	O
nonincreasing	O
function	O
n	O
--	O
-+	O
n	O
defining	O
a	O
set	O
in	O
£	O
by	O
l	O
=	O
{	O
(	O
xl	O
,	O
x2	O
)	O
:	O
x2	O
:	O
:	O
:	O
;	O
1/i	O
(	O
xl	O
)	O
}	O
.	O
let	O
c	O
(	O
1/i	O
)	O
be	O
the	O
collection	O
of	O
all	O
cell	O
sets	O
cut	O
by	O
1/1	O
,	O
that	O
is	O
,	O
all	O
cells	O
with	O
a	O
nonempty	O
intersection	O
with	O
both	O
land	O
l	O
c.	O
the	O
collection	O
c	O
(	O
1/1	O
)	O
is	O
shaded	O
in	O
figure	O
13.5.	O
figure	O
13.5.	O
a	O
monotone	O
layer	O
and	O
bordering	O
cells	O
.	O
228	O
13.	O
combinatorial	O
aspects	O
of	O
vapnik-chervonenkis	O
theory	O
we	O
bound	O
n	O
dx	O
1	O
,	O
...	O
,	O
xn	O
)	O
from	O
above	O
,	O
conservatively	O
,	O
as	O
follows	O
:	O
(	O
13.1	O
)	O
the	O
number	O
of	O
different	O
collections	O
c	O
(	O
1/1	O
)	O
can	O
not	O
exceed	O
22m	O
because	O
each	O
cell	O
in	O
c	O
(	O
1/1	O
)	O
may	O
be	O
obtained	O
from	O
its	O
predecessor	O
cell	O
by	O
either	O
moving	O
right	O
on	O
the	O
same	O
row	O
or	O
moving	O
down	O
one	O
cell	O
in	O
the	O
same	O
column	O
.	O
for	O
a	O
particular	O
collection	O
,	O
denoting	O
pi	O
=	O
p	O
{	O
x	O
e	O
c	O
}	O
,	O
we	O
have	O
=	O
(	O
~	O
2	O
pi	O
+	O
2	O
po	O
+	O
1	O
-	O
~	O
pi	O
_	O
po	O
)	O
n	O
ci	O
ec	O
(	O
1/	O
!	O
)	O
ci	O
ec	O
(	O
1/	O
!	O
)	O
(	O
by	O
applying	O
lemma	O
a.7	O
)	O
<	O
exp	O
(	O
n	O
c~o/	O
)	O
p	O
;	O
+	O
po	O
)	O
)	O
<	O
exp	O
c	O
en	O
sets	O
a	O
wit~uf	O
(	O
a	O
)	O
:	O
s	O
:	O
8r'im	O
1	O
f	O
+	O
l	O
f	O
)	O
)	O
(	O
since	O
a	O
(	O
,	O
'	O
u	O
ci	O
)	O
s	O
2m	O
(	O
2rlm	O
)	O
2	O
=	O
8r21m	O
)	O
t.	O
ci	O
ec	O
(	O
1/	O
!	O
)	O
=	O
because	O
r2	O
i	O
m	O
-+	O
0	O
and	O
r	O
-+	O
00	O
and	O
by	O
the	O
absolute	O
continuity	O
of	O
x.	O
as	O
this	O
estimate	B
is	O
uniform	B
over	O
all	O
collections	O
c	O
(	O
1/1	O
)	O
,	O
we	O
see	O
that	O
the	O
expected	O
value	O
of	O
(	O
13.1	O
)	O
is	O
not	O
more	O
than	O
22m	O
eo	O
(	O
n	O
)	O
=	O
eo	O
(	O
n	O
)	O
.	O
the	O
argument	O
for	O
the	O
collection	O
c	O
of	O
convex	O
sets	O
is	O
analogous	O
.	O
0	O
remark	O
.	O
the	O
theorem	B
implies	O
that	O
if	O
a	O
is	O
the	O
class	O
of	O
all	O
convex	O
sets	O
,	O
then	O
fl	O
(	O
a	O
)	O
1	O
-+	O
0	O
with	O
probability	O
one	O
whenever	O
fl	O
has	O
a	O
density	O
.	O
this	O
supaea	O
ifln	O
(	O
a	O
)	O
-	O
is	O
a	O
special	O
case	O
of	O
a	O
result	O
of	O
ranga	O
rao	O
(	O
1962	O
)	O
.	O
assume	O
now	O
that	O
the	O
density	O
f	O
of	O
x	O
is	O
bounded	O
and	O
of	O
bounded	O
support	B
.	O
then	O
in	O
the	O
proof	O
above	O
we	O
may	O
take	O
r	O
fixed	O
so	O
that	O
[	O
-r	O
,	O
r	O
]	O
2	O
contains	O
the	O
support	B
of	O
i.	O
then	O
the	O
estimate	B
of	O
theorem	B
13.13	O
is	O
e	O
{	O
n	O
(	O
x	O
ai	O
,	O
..	O
·	O
,	O
n	O
x	O
)	O
}	O
<	O
22m	O
.	O
e8nllfllex	O
,	O
r2/m	O
(	O
where	O
11/1100	O
denotes	O
the	O
bound	O
on	O
i	O
)	O
problems	O
and	O
exercises	O
229	O
=	O
24r	O
.jn	O
ii	O
flloo	O
e4r.jnllflloo	O
(	O
if	O
we	O
take	O
m	O
=	O
2rjnllflloo	O
)	O
for	O
a	O
constant	O
a.	O
this	O
implies	O
by	O
corollary	O
12.1	O
that	O
e	O
{	O
l	O
(	O
¢~	O
)	O
-	O
inf	O
r/j=lc	O
for	O
coree	O
in	O
c	O
l	O
(	O
¢	O
)	O
}	O
=	O
0	O
(	O
n-i/4	O
)	O
.	O
this	O
latter	O
inequality	B
was	O
proved	O
by	O
steele	O
(	O
1975	O
)	O
.	O
to	O
see	O
that	O
it	O
can	O
not	O
be	O
extended	O
to	O
arbitrary	O
densities	O
,	O
observe	O
that	O
the	O
data	O
points	O
falling	O
on	O
the	O
convex	B
hull	I
(	O
or	O
upper	O
layer	O
)	O
of	O
the	O
points	O
x	O
i	O
,	O
...	O
,	O
x	O
n	O
can	O
always	O
be	O
shattered	O
by	O
convex	O
sets	O
(	O
or	O
monotone	O
layers	O
,	O
respectively	O
)	O
.	O
thus	O
,	O
na	O
(	O
x	O
i	O
,	O
...	O
,	O
xn	O
)	O
is	O
at	O
least	O
2m	O
''	O
,	O
where	O
mn	O
is	O
the	O
number	O
of	O
points	O
among	O
xl	O
,	O
...	O
,	O
x	O
n	O
,	O
falling	O
on	O
the	O
convex	B
hull	I
(	O
or	O
upper	O
layer	O
)	O
of	O
x	O
i	O
,	O
...	O
,	O
x	O
n	O
.	O
thus	O
,	O
but	O
it	O
follows	O
from	O
results	O
of	O
carnal	O
(	O
1970	O
)	O
and	O
devroye	O
(	O
1991b	O
)	O
that	O
for	O
each	O
a	O
<	O
1	O
,	O
there	O
exists	O
a	O
density	O
such	O
that	O
e	O
{	O
mn	O
}	O
.	O
hmsup	O
-	O
-	O
-	O
>	O
1	O
.	O
0	O
n-+oo	O
na	O
the	O
important	O
point	O
of	O
this	O
interlude	O
is	O
that	O
with	O
infinite	O
vc	B
dimension	I
,	O
we	O
may	O
under	O
some	O
circumstances	O
get	O
expected	O
error	O
rates	O
that	O
are	O
0	O
(	O
1	O
)	O
but	O
larger	O
than	O
1/	O
,	O
jri	O
.	O
however	O
,	O
the	O
bounds	O
are	O
sometimes	O
rather	O
loose	O
.	O
the	O
reason	O
is	O
the	O
looseness	O
of	O
the	O
vapnik	O
-chervonenkis	O
inequality	B
when	O
the	O
collections	O
a	O
become	O
very	O
big	O
.	O
to	O
get	O
such	O
results	O
for	O
classes	O
with	O
infinite	O
vc	B
dimension	I
it	O
is	O
necessary	O
to	O
impose	O
some	O
conditions	O
on	O
the	O
distribution	B
.	O
we	O
will	O
prove	O
this	O
in	O
chapter	O
14.	O
problems	O
and	O
exercises	O
problem	O
13.1.	O
show	O
that	O
the	O
inequality	B
of	O
theorem	B
13.2	O
is	O
tight	O
,	O
that	O
is	O
,	O
exhibit	O
a	O
class	O
a	O
of	O
sets	O
such	O
that	O
for	O
each	O
n	O
,	O
sea	O
,	O
n	O
)	O
=	O
'l:1	O
g	O
)	O
.	O
problem	O
13.2.	O
show	O
that	O
for	O
all	O
n	O
>	O
2va	O
and	O
that	O
230	O
13.	O
combinatorial	O
aspects	O
of	O
vapnik	O
-chervonenkis	O
theory	O
if	O
va	O
>	O
2.	O
hint	O
:	O
there	O
are	O
several	O
ways	O
to	O
prove	O
the	O
statements	O
.	O
one	O
can	O
proceed	O
di	O
(	O
cid:173	O
)	O
rectly	O
by	O
using	O
the	O
recurrence	O
l~	O
(	O
;	O
)	O
=	O
l~fo	O
e~l	O
)	O
+	O
l~fo-l	O
e~	O
}	O
a	O
simpler	O
way	O
to	O
prove	O
l~1	O
g	O
)	O
:	O
:	O
:	O
:	O
(	O
~	O
:	O
)	O
va	O
is	O
by	O
using	O
theorem	B
13.4.	O
the	O
third	O
inequality	B
is	O
an	O
immediate	O
consequence	O
of	O
the	O
first	O
two	O
.	O
problem	O
13.3.	O
give	O
an	O
alternative	O
proof	O
of	O
lemma	O
13.1	O
by	O
completing	O
the	O
following	O
probabilistic	O
argument	O
.	O
observe	O
that	O
for	O
k	O
'	O
=	O
n	O
-	O
k	O
,	O
where	O
bn	O
is	O
a	O
binomial	O
random	O
variable	B
with	O
parameters	O
nand	O
1/2	O
.	O
then	O
chernoff	O
's	O
bounding	O
technique	O
(	O
see	O
the	O
proof	O
of	O
theorem	O
8.1	O
)	O
may	O
be	O
used	O
to	O
bound	O
this	O
probability	O
:	O
for	O
all	O
s	O
>	O
0	O
,	O
p	O
{	O
bn	O
:	O
:	O
:	O
:	O
k	O
'	O
}	O
:	O
:	O
:	O
:	O
e-sk'e	O
{	O
e	O
sell	O
}	O
=	O
exp	O
(	O
-n	O
(	O
sk'in	O
-log	O
(	O
e	O
s	O
;	O
1	O
)	O
)	O
)	O
.	O
take	O
the	O
derivative	O
of	O
the	O
exponent	O
with	O
respect	O
to	O
s	O
to	O
minimize	O
the	O
upper	O
bound	O
.	O
substitute	O
the	O
obtained	O
value	O
into	O
the	O
bound	O
to	O
get	O
the	O
desired	O
inequality	B
.	O
problem	O
13.4.	O
let	O
b	O
be	O
a	O
binomial	O
random	O
variable	B
with	O
parameters	O
nand	O
p.	O
prove	O
that	O
for	O
k	O
>	O
np	O
p	O
{	O
b	O
:	O
:	O
:	O
:	O
k	O
}	O
:	O
:	O
:	O
:	O
exp	O
(	O
-n	O
(	O
h	O
(	O
kln	O
)	O
+	O
~logp+	O
n	O
:	O
k	O
10g	O
(	O
1-	O
p	O
)	O
)	O
)	O
.	O
hint	O
:	O
use	O
chernoff	O
's	O
bounding	O
technique	O
as	O
in	O
the	O
previous	O
problem	O
.	O
problem	O
13.5.	O
prove	O
part	O
(	O
i	O
)	O
of	O
theorem	O
13.8.	O
problem	O
13.6.	O
prove	O
that	O
for	O
the	O
class	O
of	O
sets	O
defined	O
in	O
the	O
remark	O
following	O
theorem	B
13.9	O
,	O
1	O
)	O
sea	O
,	O
n	O
)	O
=	O
2	O
l	O
n	O
~	O
r-1	O
(	O
i=o	O
l	O
(	O
cover	O
(	O
1965	O
»	O
.	O
hint	O
:	O
proceed	O
by	O
induction	O
with	O
respect	O
to	O
nand	O
r.	O
in	O
particular	O
,	O
show	O
that	O
the	O
recurrence	O
scar	O
,	O
n	O
)	O
=	O
scan	O
n	O
-	O
1	O
)	O
+	O
s	O
(	O
ar-	O
1	O
,	O
n	O
-	O
1	O
)	O
holds	O
,	O
where	O
ar	O
denotes	O
the	O
class	O
of	O
sets	O
defined	O
as	O
{	O
x	O
:	O
g	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
o	O
}	O
where	O
g	O
runs	O
through	O
a	O
vector	O
space	O
spanned	O
by	O
the	O
first	O
r	O
of	O
the	O
sequence	O
of	O
functions	O
1h	O
,	O
7f2	O
,	O
...	O
.	O
problem	O
13.7.	O
let	O
a	O
and	O
b	O
be	O
two	O
families	O
of	O
subsets	O
of	O
nd	O
•	O
assume	O
that	O
for	O
some	O
r	O
:	O
:	O
:	O
:	O
2	O
,	O
sea	O
,	O
n	O
)	O
:	O
:	O
:	O
:	O
nrs	O
(	O
b	O
,	O
n	O
)	O
for	O
all	O
n	O
:	O
:	O
:	O
:	O
1.	O
show	O
that	O
if	O
vb	O
>	O
2	O
,	O
then	O
va	O
:	O
:	O
:	O
:	O
2	O
(	O
vb	O
+	O
r	O
-	O
1	O
)	O
log	O
(	O
vb	O
+	O
r	O
-	O
1	O
)	O
.	O
hint	O
:	O
by	O
theorem	B
13.3	O
,	O
sea	O
,	O
n	O
)	O
:	O
:	O
:	O
:	O
n	O
vs+r-l.	O
clearly	O
,	O
va	O
is	O
not	O
larger	O
than	O
any	O
k	O
for	O
which	O
kvs+r-l	O
<	O
2k	O
.	O
problem	O
13.8.	O
determine	O
the	O
vc	B
dimension	I
of	O
the	O
class	O
of	O
subsets	O
of	O
the	O
real	O
line	O
such	O
that	O
each	O
set	O
in	O
the	O
class	O
can	O
be	O
written	O
as	O
a	O
union	O
of	O
k	O
intervals	O
.	O
problems	O
and	O
exercises	O
231	O
problem	O
13.9.	O
determine	O
the	O
vc	B
dimension	I
of	O
the	O
collection	O
of	O
all	O
polygons	O
with	O
k	O
vertices	O
in	O
the	O
plane	O
.	O
problem	O
13.10.	O
what	O
is	O
the	O
vc	B
dimension	I
of	O
the	O
collection	O
of	O
all	O
ellipsoids	O
ofnd	O
?	O
problem	O
13	O
.11.	O
determine	O
the	O
vc	B
dimension	I
of	O
the	O
collection	O
of	O
all	O
subsets	O
of	O
{	O
1	O
,	O
...	O
,	O
k	O
}	O
d	O
,	O
where	O
k	O
and	O
d	O
are	O
fixed	O
.	O
how	O
does	O
the	O
answer	O
change	O
if	O
we	O
restrict	O
the	O
subsets	O
to	O
those	O
of	O
cardinality	O
l	O
:	O
's	O
kd	O
?	O
problem	O
13.12.	O
let	O
a	O
consist	O
of	O
all	O
simplices	O
ofnd	O
,	O
that	O
is	O
,	O
all	O
sets	O
of	O
the	O
form	O
where	O
xl	O
,	O
...	O
,	O
xd+l	O
are	O
fixed	O
points	O
of	O
nd	O
.	O
determine	O
the	O
vc	B
dimension	I
of	O
a.	O
problem	O
13.13.	O
let	O
a	O
(	O
xi	O
'	O
...	O
,	O
xk	O
)	O
be	O
the	O
set	O
of	O
all	O
x	O
en	O
that	O
are	O
of	O
the	O
form	O
where	O
xl	O
,	O
...	O
,	O
xk	O
are	O
fixed	O
numbers	O
and	O
'tfi	O
,	O
...	O
,	O
'tfk	O
are	O
fixed	O
functions	O
on	O
the	O
integers	O
.	O
let	O
a	O
=	O
{	O
a	O
(	O
xi	O
,	O
...	O
,	O
xd	O
:	O
xl	O
,	O
...	O
,	O
xk	O
en	O
}	O
.	O
determine	O
the	O
vc	B
dimension	I
of	O
a.	O
problem	O
13.14.	O
in	O
some	O
sense	O
,	O
vc	B
dimension	I
measures	O
the	O
``	O
size	O
''	O
of	O
a	O
class	O
of	O
sets	O
.	O
how	O
(	O
cid:173	O
)	O
ever	O
it	O
has	O
little	O
to	O
do	O
with	O
cardinalities	O
,	O
as	O
this	O
exercise	O
demonstrates	O
.	O
exhibit	O
a	O
class	O
of	O
subsets	O
of	O
the	O
integers	O
with	O
uncountably	O
many	O
sets	O
,	O
yet	O
vc	B
dimension	I
1	O
.	O
(	O
this	O
property	O
was	O
pointed	O
out	O
to	O
us	O
by	O
andras	O
farago	O
.	O
)	O
note	O
:	O
on	O
the	O
other	O
hand	O
,	O
the	O
class	O
of	O
all	O
subsets	O
of	O
integers	O
can	O
not	O
be	O
written	O
as	O
a	O
countable	O
union	O
of	O
classes	O
with	O
finite	O
vc	B
dimension	I
(	O
theo	O
(	O
cid:173	O
)	O
rem	O
18.6	O
)	O
.	O
hint	O
:	O
find	O
a	O
class	O
of	O
subsets	O
of	O
the	O
reals	O
with	O
the	O
desired	O
properties	O
,	O
and	O
make	O
a	O
proper	O
correspondence	O
between	O
sets	O
of	O
integers	O
and	O
sets	O
in	O
the	O
class	O
.	O
problem	O
13.15.	O
show	O
that	O
if	O
a	O
class	O
of	O
sets	O
a	O
is	O
linearly	O
ordered	B
by	O
inclusion	O
,	O
that	O
is	O
,	O
for	O
any	O
pair	O
of	O
sets	O
a	O
,	O
b	O
e	O
a	O
either	O
a	O
c	O
b	O
or	O
b	O
c	O
a	O
and	O
iai	O
2	O
:	O
:	O
2	O
,	O
then	O
va	O
=	O
1.	O
conversely	O
,	O
assume	O
that	O
va	O
=	O
1	O
and	O
for	O
every	O
set	O
b	O
with	O
i	O
b	O
i	O
=	O
2	O
,	O
0	O
,	O
b	O
e	O
{	O
a	O
n	O
b	O
:	O
a	O
e	O
a	O
}	O
.	O
prove	O
that	O
then	O
a	O
is	O
linearly	O
ordered	B
by	O
inclusion	O
(	O
dudley	O
(	O
1984	O
)	O
)	O
.	O
problem	O
13.16.	O
we	O
say	O
that	O
four	O
sets	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
form	O
a	O
diamond	O
if	O
a	O
c	O
b	O
c	O
c	O
,	O
a	O
c	O
dec	O
,	O
but	O
b	O
ct	O
d	O
and	O
d	O
ct	O
c.	O
let	O
a	O
be	O
a	O
class	O
of	O
sets	O
.	O
show	O
that	O
va	O
2	O
:	O
:	O
2	O
if	O
and	O
only	O
if	O
for	O
some	O
set	O
r	O
,	O
the	O
class	O
{	O
a	O
n	O
r	O
:	O
a	O
e	O
a	O
}	O
includes	O
a	O
diamond	O
(	O
dudley	O
(	O
1984	O
)	O
)	O
.	O
problem	O
13	O
.17.	O
let	O
a	O
be	O
a	O
class	O
of	O
sets	O
,	O
and	O
define	O
its	O
density	O
by	O
da	O
=	O
inf	O
{	O
r	O
>	O
0	O
:	O
sup	O
sea	O
,	O
n	O
)	O
<	O
oo	O
}	O
.	O
n	O
:	O
:	O
:	O
:	O
:	O
l	O
nr	O
verify	O
the	O
following	O
properties	O
:	O
(	O
1	O
)	O
da	O
:	O
's	O
va	O
;	O
(	O
2	O
)	O
for	O
each	O
positive	O
integer	O
k	O
,	O
there	O
exists	O
a	O
class	O
a	O
of	O
sets	O
such	O
that	O
va	O
=	O
k	O
,	O
yet	O
da	O
=	O
0	O
;	O
and	O
232	O
13.	O
combinatorial	O
aspects	O
of	O
vapnik-chervonenkis	O
theory	O
(	O
3	O
)	O
da	O
<	O
00	O
if	O
and	O
only	O
if	O
va	O
<	O
00	O
(	O
assouad	O
(	O
1983a	O
)	O
)	O
.	O
problem	O
13.18.	O
continued	O
.	O
let	O
a	O
and	O
ai	O
be	O
classes	O
of	O
sets	O
,	O
and	O
define	O
b	O
=	O
a	O
u	O
a	O
'	O
.	O
show	O
that	O
(	O
1	O
)	O
db	O
=	O
max	O
(	O
da	O
'	O
da	O
'	O
)	O
;	O
(	O
2	O
)	O
vb	O
:	O
:s	O
va	O
+	O
va	O
'	O
+	O
1	O
;	O
and	O
(	O
3	O
)	O
for	O
every	O
pair	O
of	O
positive	O
integers	O
k	O
,	O
m	O
there	O
exist	O
classes	O
a	O
and	O
ai	O
such	O
that	O
va	O
=k	O
,	O
va	O
'	O
=m	O
,	O
and	O
vb	O
=k+m+	O
1	O
(	O
assouad	O
(	O
l983a	O
)	O
)	O
.	O
problem	O
13.19.	O
a	O
set	O
a	O
c	O
n	O
d	O
is	O
called	O
a	O
monotone	O
layer	O
if	O
x	O
e	O
a	O
implies	O
that	O
yea	O
for	O
all	O
y	O
:	O
:s	O
x	O
(	O
i.e.	O
,	O
each	O
component	O
of	O
x	O
is	O
not	O
larger	O
than	O
the	O
corresponding	O
component	O
of	O
y	O
)	O
.	O
show	O
that	O
the	O
class	O
of	O
all	O
monotone	O
layers	O
has	O
infinite	O
vc	B
dimension	I
.	O
problem	O
13.20.	O
let	O
(	O
x	O
,	O
y	O
)	O
beapairofrandomvariablesinn2	O
x	O
{	O
0	O
,	O
l	O
}	O
suchthaty	O
=	O
i	O
{	O
xea	O
}	O
,	O
where	O
a	O
is	O
a	O
convex	O
set	O
.	O
let	O
dn	O
=	O
«	O
xl	O
,	O
yd	O
,	O
...	O
,	O
(	O
xn	O
'	O
yn	O
)	O
)	O
be	O
an	O
i.i.d	O
.	O
training	O
sequence	O
,	O
and	O
consider	O
the	O
classifier	B
if	O
x	O
is	O
in	O
the	O
convex	B
hull	I
of	O
the	O
xi	O
's	O
with	O
y	O
=	O
1	O
otherwise	O
.	O
find	O
a	O
distribution	O
for	O
which	O
gil	O
is	O
not	O
consistent	O
,	O
and	O
find	O
conditions	O
for	O
consistency	B
.	O
hint	O
:	O
recall	O
theorem	B
13.10	O
and	O
its	O
proof	O
.	O
14	O
lower	B
bounds	I
for	I
empirical	O
classifier	B
selection	I
in	O
chapter	O
12	O
a	O
classifier	O
was	O
selected	O
by	O
minimizing	O
the	O
empirical	B
error	I
over	O
a	O
class	O
of	O
classifiers	O
c.	O
with	O
the	O
help	O
of	O
the	O
vapnik	O
-chervonenkis	O
theory	O
we	O
have	O
been	O
able	O
to	O
obtain	O
distribution-free	O
performance	O
guarantees	O
for	O
the	O
selected	O
rule	B
.	O
for	O
example	O
,	O
it	O
was	O
shown	O
that	O
the	O
difference	O
between	O
the	O
expected	O
error	O
proba	O
(	O
cid:173	O
)	O
bility	O
of	O
the	O
selected	O
rule	B
and	O
the	O
best	O
error	O
probability	O
in	O
the	O
class	O
behaves	O
at	O
least	O
as	O
well	O
as	O
o	O
(	O
.jvc	O
logn/n	O
)	O
,	O
where	O
vc	O
is	O
the	O
vapnik-chervonenkis	O
dimension	B
of	O
c	O
,	O
and	O
n	O
is	O
the	O
size	O
of	O
the	O
training	O
data	O
dn	O
.	O
(	O
this	O
upper	O
bound	O
is	O
obtained	O
from	O
theorem	B
12.5.	O
corollary	O
12.5	O
may	O
be	O
used	O
to	O
replace	O
the	O
log	O
n	O
term	O
with	O
log	O
vc	O
.	O
)	O
two	O
questions	O
arise	O
immediately	O
:	O
are	O
these	O
upper	O
bounds	O
(	O
at	O
least	O
up	O
to	O
the	O
order	O
of	O
magnitude	O
)	O
tight	O
?	O
is	O
there	O
a	O
much	O
better	O
way	O
of	O
selecting	O
a	O
classifier	O
than	O
mini	O
(	O
cid:173	O
)	O
mizing	O
the	O
empirical	B
error	I
?	O
this	O
chapter	O
attempts	O
to	O
answer	O
these	O
questions	O
.	O
as	O
it	O
turns	O
out	O
,	O
the	O
answer	O
is	O
essentially	O
affirmative	O
for	O
the	O
first	O
question	O
,	O
and	O
negative	O
for	O
the	O
second	O
.	O
these	O
questions	O
were	O
also	O
asked	O
in	O
the	O
learning	B
theory	O
setup	O
,	O
where	O
it	O
is	O
usually	O
assumed	O
that	O
the	O
error	O
probability	O
of	O
the	O
best	O
classifier	B
in	O
the	O
class	O
is	O
zero	O
(	O
see	O
blumer	O
,	O
ehrenfeucht	O
,	O
haussler	O
,	O
and	O
warmuth	O
(	O
1989	O
)	O
,	O
haussler	O
,	O
littlestone	O
,	O
and	O
warmuth	O
(	O
1988	O
)	O
,	O
and	O
ehrenfeucht	O
,	O
haussler	O
,	O
kearns	O
,	O
and	O
valiant	O
(	O
1989	O
)	O
)	O
.	O
in	O
this	O
case	O
,	O
as	O
the	O
bound	O
of	O
theorem	O
12.7	O
implies	O
,	O
the	O
error	O
of	O
the	O
rule	B
selected	O
by	O
minimizing	O
the	O
empirical	B
error	I
is	O
within	O
0	O
(	O
vc	O
log	O
n	O
/	O
n	O
)	O
of	O
that	O
of	O
the	O
best	O
in	O
the	O
class	O
(	O
which	O
equals	O
zero	O
,	O
by	O
assumption	O
)	O
.	O
we	O
will	O
see	O
that	O
essentially	O
there	O
is	O
no	O
way	O
to	O
beat	O
this	O
upper	O
bound	O
either	O
.	O
234	O
14.	O
lo~er	O
bounds	O
for	O
empirical	B
classifier	I
selection	I
14.1	O
minimax	O
lower	O
bounds	O
let	O
us	O
formulate	O
exactly	O
what	O
we	O
are	O
interested	O
in	O
.	O
let	O
c	O
be	O
a	O
class	O
of	O
decision	O
functions	O
¢	O
:	O
nd	O
--	O
-+	O
{	O
o	O
,	O
i	O
}	O
.	O
the	O
training	O
sequence	O
dn	O
=	O
(	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
)	O
is	O
used	O
to	O
select	O
the	O
classifier	B
gn	O
(	O
x	O
)	O
=	O
gn	O
(	O
x	O
,	O
dn	O
)	O
from	O
c	O
,	O
where	O
the	O
selection	B
is	O
based	O
on	O
the	O
data	O
dn	O
.	O
we	O
emphasize	O
here	O
that	O
gn	O
can	O
be	O
an	O
arbitrary	O
function	O
of	O
the	O
data	O
,	O
we	O
do	O
not	O
restrict	O
our	O
attention	O
to	O
empirical	B
error	I
minimization	O
,	O
where	O
gn	O
is	O
a	O
classifier	O
in	O
c	O
that	O
minimizes	O
the	O
number	O
errors	O
committed	O
on	O
the	O
data	O
dn	O
.	O
as	O
before	O
,	O
we	O
measure	B
the	O
performance	O
of	O
the	O
selected	O
classifier	B
by	O
the	O
dif	O
(	O
cid:173	O
)	O
ference	O
between	O
the	O
error	O
probability	O
l	O
(	O
gn	O
)	O
=	O
p	O
{	O
gn	O
(	O
x	O
)	O
i	O
yidn	O
}	O
of	O
the	O
selected	O
classifier	B
and	O
that	O
of	O
the	O
best	O
in	O
the	O
class	O
.	O
to	O
save	O
space	O
further	O
on	O
,	O
denote	O
this	O
optimum	O
by	O
ij	O
>	O
ec	O
in	O
particular	O
,	O
we	O
seek	O
lower	B
bounds	I
for	I
lc	O
d	O
;	O
f	O
inf	O
p	O
{	O
¢	O
(	O
x	O
)	O
i	O
y	O
}	O
.	O
and	O
supp	O
{	O
l	O
(	O
gn	O
)	O
-	O
lc	O
>	O
e	O
}	O
,	O
supel	O
(	O
gn	O
)	O
-	O
lc	O
,	O
where	O
the	O
supremum	O
is	O
taken	O
over	O
all	O
possible	O
distributions	O
of	O
the	O
pair	O
(	O
x	O
,	O
y	O
)	O
.	O
a	O
lower	O
bound	O
for	O
one	O
of	O
these	O
quantities	O
means	O
that	O
no	O
matter	O
what	O
our	O
method	O
of	O
picking	O
a	O
rule	O
from	O
c	O
is	O
,	O
we	O
may	O
face	O
a	O
distribution	O
such	O
that	O
our	O
method	O
performs	O
worse	O
than	O
the	O
bound	O
.	O
this	O
view	O
may	O
be	O
criticized	O
as	O
too	O
pessimistic	O
.	O
however	O
,	O
it	O
is	O
clearly	O
a	O
perfectly	O
meaningful	O
question	O
to	O
pursue	O
,	O
as	O
typically	O
we	O
have	O
no	O
other	O
information	O
available	O
than	O
the	O
training	O
data	O
,	O
so	O
we	O
have	O
to	O
be	O
prepared	O
for	O
the	O
worst	O
situation	O
.	O
actually	O
,	O
we	O
investigate	O
a	O
stronger	O
problem	O
,	O
in	O
that	O
the	O
supremum	O
is	O
taken	O
over	O
all	O
distributions	O
with	O
lc	O
kept	O
at	O
a	O
fixed	O
value	O
between	O
zero	O
and	O
1/2	O
.	O
we	O
will	O
see	O
that	O
the	O
bounds	O
depend	O
on	O
n	O
,	O
vc	O
,	O
and	O
lc	O
jointly	O
.	O
as	O
it	O
turns	O
out	O
,	O
the	O
situations	O
for	O
lc	O
>	O
0	O
and	O
lc	O
=	O
0	O
are	O
quite	O
different	O
.	O
because	O
of	O
its	O
relative	O
simplicity	O
,	O
we	O
first	O
treat	O
the	O
case	O
lc	O
=	O
o.	O
all	O
the	O
proofs	O
are	O
based	O
on	O
a	O
technique	O
called	O
''	O
the	O
probabilistic	B
method	I
.	O
''	O
the	O
basic	O
idea	O
here	O
is	O
that	O
the	O
existence	O
of	O
a	O
``	O
bad	O
''	O
distribution	B
is	O
proved	O
by	O
considering	O
a	O
large	O
class	O
of	O
distributions	O
,	O
and	O
bounding	O
the	O
average	O
behavior	O
over	O
the	O
class	O
.	O
lower	O
bounds	O
on	O
the	O
probabilities	O
p	O
{	O
l	O
(	O
gn	O
)	O
-	O
lc	O
>	O
e	O
}	O
may	O
be	O
translated	O
into	O
lower	O
bounds	O
on	O
the	O
sample	B
complexity	I
n	O
(	O
e	O
,	O
8	O
)	O
.	O
we	O
obtain	O
lower	B
bounds	I
for	I
the	O
size	O
ofthe	O
training	O
sequence	O
such	O
that	O
for	O
any	O
classifier	B
,	O
p	O
{	O
l	O
(	O
gn	O
)	O
-	O
lc	O
>	O
e	O
}	O
can	O
not	O
be	O
smaller	O
than	O
8	O
for	O
all	O
distributions	O
if	O
n	O
is	O
smaller	O
than	O
this	O
bound	O
.	O
14.2	O
the	O
case	O
lc	O
=	O
0	O
in	O
this	O
section	O
we	O
obtain	O
lower	O
bounds	O
under	O
the	O
assumption	O
that	O
the	O
best	O
classifier	B
in	O
the	O
class	O
has	O
zero	O
error	O
probability	O
.	O
in	O
view	O
of	O
theorem	O
12.7	O
we	O
see	O
that	O
the	O
14.2	O
the	O
case	O
lc	O
=	O
0	O
235	O
situation	O
here	O
is	O
different	O
from	O
when	O
lc	O
>	O
0	O
;	O
there	O
exist	O
methods	O
of	O
picking	O
a	O
classifier	O
from	O
c	O
(	O
e.g.	O
,	O
minimization	O
of	O
the	O
empirical	B
error	I
)	O
such	O
that	O
the	O
error	O
probability	O
decreases	O
to	O
zero	O
at	O
a	O
rate	O
of	O
0	O
(	O
vc	O
log	O
n	O
in	O
)	O
.	O
we	O
obtain	O
minimax	O
lower	O
bounds	O
close	O
to	O
the	O
upper	O
bounds	O
obtained	O
for	O
empirical	B
error	I
minimization	O
.	O
for	O
example	O
,	O
theorem	B
14.1	O
shows	O
that	O
if	O
lc	O
=	O
0	O
,	O
then	O
the	O
expected	O
error	O
probability	O
can	O
not	O
decrease	O
faster	O
than	O
a	O
sequence	O
proportional	O
to	O
vc	O
i	O
n	O
for	O
some	O
distributions	O
.	O
theorem	B
14.1	O
.	O
(	O
vapnik	O
and	O
chervonenkis	O
(	O
1974c	O
)	O
;	O
haussler	O
,	O
littlestone	O
,	O
and	O
warmuth	O
(	O
1988	O
»	O
.	O
let	O
c	O
be	O
a	O
class	O
of	O
discrimination	O
functions	O
with	O
vc	O
di	O
(	O
cid:173	O
)	O
mension	O
v.	O
let	O
x	O
be	O
the	O
set	O
of	O
all	O
random	O
variables	O
(	O
x	O
,	O
y	O
)	O
for	O
which	O
lc	O
=	O
0.	O
then	O
,	O
for	O
every	O
discrimination	O
rule	B
gn	O
based	O
upon	O
xl	O
,	O
yi	O
,	O
...	O
,	O
x	O
n	O
,	O
yn	O
,	O
and	O
n	O
:	O
:	O
:	O
:	O
v	O
-	O
1	O
,	O
sup	O
eln	O
:	O
:	O
:	O
v	O
-	O
1	O
(	O
1	O
_~	O
)	O
.	O
(	O
x	O
,	O
y	O
)	O
ex	O
2en	O
n	O
proof	O
.	O
the	O
idea	O
is	O
to	O
construct	O
a	O
family	O
f	O
of	O
2	O
v-i	O
distributions	O
within	O
the	O
distri	O
(	O
cid:173	O
)	O
butions	O
with	O
lc	O
=	O
°	O
as	O
follows	O
:	O
first	O
find	O
points	O
x	O
i	O
,	O
...	O
,	O
xv	O
that	O
are	O
shattered	O
by	O
c.	O
each	O
distribution	B
in	O
f	O
is	O
concentrated	O
on	O
the	O
set	O
of	O
these	O
points	O
.	O
a	O
member	O
in	O
f	O
is	O
described	O
by	O
v	O
-	O
1	O
bits	O
,	O
bi	O
,	O
...	O
,	O
bv	O
-1.	O
for	O
convenience	O
,	O
this	O
is	O
represented	O
as	O
a	O
bit	O
vector	O
b.	O
assume	O
v	O
-	O
1	O
~	O
n.	O
for	O
a	O
particular	O
bit	O
vector	O
,	O
we	O
let	O
x	O
=	O
xi	O
(	O
i	O
<	O
v	O
)	O
with	O
probability	O
lin	O
each	O
,	O
while	O
x	O
=	O
xv	O
with	O
probability	O
1	O
-	O
l	O
)	O
/n	O
.	O
then	O
set	O
y	O
=	O
fb	O
(	O
x	O
)	O
,	O
where	O
fb	O
is	O
defined	O
as	O
follows	O
:	O
(	O
v	O
-	O
-r	O
j	O
h	O
(	O
x	O
)	O
=	O
°	O
if	O
x	O
=	O
xv	O
.	O
if	O
x	O
=	O
xi	O
,	O
i	O
<	O
v	O
{	O
bi	O
note	O
that	O
since	O
y	O
is	O
a	O
function	O
of	O
x	O
,	O
we	O
must	O
have	O
l	O
*	O
=	O
0.	O
also	O
,	O
lc	O
=	O
0	O
,	O
as	O
the	O
set	O
{	O
xl	O
,	O
...	O
,	O
xv	O
}	O
is	O
shattered	O
by	O
c	O
,	O
i.e.	O
,	O
there	O
is	O
age	O
c	O
with	O
g	O
(	O
xi	O
)	O
=	O
fb	O
(	O
xi	O
)	O
for	O
1	O
~	O
i	O
~	O
v.	O
clearly	O
,	O
sup	O
e	O
{	O
l	O
n	O
-	O
lc	O
}	O
(	O
x	O
,	O
y	O
)	O
:	O
lc=o	O
>	O
sup	O
e	O
{	O
l	O
n	O
-	O
lc	O
}	O
(	O
x	O
,	O
y	O
)	O
ef	O
supe	O
{	O
ln	O
-	O
lc	O
}	O
b	O
>	O
e	O
{	O
l	O
n	O
-	O
lc	O
}	O
(	O
where	O
b	O
is	O
replaced	O
by	O
b	O
,	O
uniformly	O
distributed	O
over	O
{	O
a	O
,	O
1	O
}	O
v-i	O
)	O
e	O
{	O
l	O
n	O
}	O
,	O
p	O
{	O
gn	O
(	O
x	O
,	O
xl	O
,	O
yi	O
,	O
...	O
,	O
x	O
n	O
,	O
yn	O
)	O
=i	O
fb	O
(	O
x	O
)	O
}	O
.	O
the	O
last	O
probability	O
may	O
be	O
viewed	O
as	O
the	O
error	O
probability	O
of	O
the	O
decision	O
function	O
gn	O
:	O
n	O
d	O
x	O
(	O
nd	O
x	O
{	O
a	O
,	O
l	O
}	O
)	O
n	O
--	O
-+	O
{	O
a	O
,	O
1	O
}	O
in	O
predicting	O
the	O
value	O
of	O
the	O
random	O
variable	B
fb	O
(	O
x	O
)	O
based	O
on	O
the	O
observation	O
zn	O
=	O
(	O
x	O
,	O
xl	O
,	O
yi	O
,	O
...	O
,	O
x	O
n	O
,	O
yn	O
)	O
.	O
naturally	O
,	O
this	O
probability	O
is	O
bounded	O
from	O
below	O
by	O
the	O
bayes	O
probability	O
of	O
error	O
l	O
*	O
(	O
zn	O
,	O
fb	O
(	O
x	O
»	O
=	O
inf	O
p	O
{	O
gn	O
(	O
zn	O
)	O
=i	O
fb	O
(	O
x	O
)	O
}	O
gil	O
236	O
14.	O
lm.yer	O
bounds	O
for	O
empirical	B
classifier	I
selection	I
corresponding	O
to	O
the	O
decision	O
problem	O
(	O
zn	O
,	O
fb	O
(	O
x	O
»	O
,	O
by	O
the	O
results	O
of	O
chapter	O
2	O
,	O
where	O
17*	O
(	O
zn	O
)	O
=	O
p	O
{	O
fb	O
(	O
x	O
)	O
=	O
lizn	O
}	O
.	O
observe	O
that	O
*	O
z	O
)	O
_1112	O
17	O
(	O
n	O
-	O
0	O
or	O
1	O
otherwise	O
.	O
if	O
x	O
=i	O
xl	O
,	O
...	O
,	O
x	O
=i	O
xn	O
,	O
x	O
=i	O
xv	O
thus	O
,	O
we	O
see	O
that	O
sup	O
e	O
{	O
l	O
n	O
-	O
lc	O
}	O
>	O
l	O
*	O
(	O
z11	O
,	O
fb	O
(	O
x	O
»	O
(	O
x	O
,	O
y	O
)	O
:	O
lc=o	O
1	O
2	O
:	O
p	O
{	O
x	O
=i	O
xl	O
...	O
.	O
,	O
x	O
=i	O
x	O
n	O
,	O
x	O
=i	O
xv	O
}	O
1	O
v-i	O
-i	O
:	O
pix	O
=	O
xd	O
(	O
1	O
-	O
pix	O
=	O
xd	O
)	O
11	O
2	O
i=l	O
v-i	O
--	O
(	O
1-1int	O
2n	O
>	O
v-i	O
(	O
1	O
__	O
1	O
)	O
2en	O
n	O
(	O
since	O
(	O
1	O
-	O
1in	O
)	O
n-l	O
-l-	O
lie	O
)	O
.	O
this	O
concludes	O
the	O
proof	O
.	O
0	O
minimax	O
lower	O
bounds	O
on	O
the	O
probability	O
p	O
{	O
ln	O
:	O
:	O
:	O
:	O
e	O
}	O
can	O
also	O
be	O
obtained	O
.	O
these	O
bounds	O
have	O
evolved	O
through	O
several	O
papers	O
:	O
see	O
ehrenfeucht	O
,	O
haussler	O
,	O
kearns	O
and	O
valiant	O
(	O
1989	O
)	O
;	O
and	O
blumer	O
,	O
ehrenfeucht	O
,	O
haussler	O
and	O
warmuth	O
(	O
1989	O
)	O
.	O
the	O
tightest	O
bounds	O
we	O
are	O
aware	O
of	O
thus	O
far	O
are	O
given	O
by	O
the	O
next	O
theorem	B
.	O
theorem	B
14.2	O
.	O
(	O
devroye	O
and	O
lugosi	O
(	O
1995	O
»	O
.	O
let	O
c	O
be	O
a	O
class	O
of	O
discrimination	O
functions	O
with	O
vc	O
dimension	B
v	O
:	O
:	O
:	O
:	O
2.	O
let	O
x	O
be	O
the	O
set	O
of	O
all	O
random	O
variables	O
(	O
x	O
,	O
y	O
)	O
for	O
which	O
lc	O
=	O
o.	O
assume	O
e	O
:	O
:	O
:	O
:	O
1/4	O
.	O
assume	O
n	O
:	O
:	O
:	O
:	O
v-i	O
.	O
then	O
for	O
every	O
discrimination	O
rule	B
gn	O
based	O
upon	O
xl	O
,	O
yi	O
,	O
•••	O
,	O
x	O
n	O
'	O
yn	O
,	O
sup	O
p	O
{	O
ln	O
:	O
:	O
:	O
:	O
e	O
}	O
:	O
:	O
:	O
:	O
-	O
-	O
-	O
-	O
e-jrrv	O
v-i	O
(	O
x	O
,	O
y	O
)	O
ex	O
1	O
(	O
2nee	O
)	O
(	O
v-i	O
)	O
!	O
2	O
e-4ne	O
!	O
(	O
l-4e	O
)	O
•	O
if	O
on	O
the	O
other	O
hand	O
n	O
:	O
:	O
:	O
:	O
15	O
and	O
n	O
:	O
:	O
:	O
:	O
(	O
v	O
-	O
1	O
)	O
/	O
(	O
12e	O
)	O
,	O
then	O
sup	O
p	O
{	O
ln	O
:	O
:	O
:	O
:	O
e	O
}	O
:	O
:	O
:	O
:	O
-	O
.	O
(	O
x	O
,	O
y	O
)	O
ex	O
1	O
10	O
proof	O
.	O
we	O
randomize	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
14.1.	O
the	O
difference	O
now	O
is	O
that	O
we	O
pick	O
xi	O
,	O
``	O
''	O
xv-l	O
with	O
probability	O
peach	O
.	O
thus	O
,	O
p	O
{	O
x	O
=xv	O
}	O
=	O
1-	O
p	O
(	O
v	O
-1	O
)	O
.	O
we	O
inherit	O
the	O
notation	O
from	O
the	O
proof	O
of	O
theorem	O
14.1.	O
for	O
a	O
fixed	O
b	O
,	O
denote	O
the	O
error	O
probability	O
by	O
14.2	O
the	O
case	O
lc	O
=	O
0	O
237	O
we	O
now	O
randomize	O
and	O
replace	O
b	O
by	O
b.	O
clearly	O
,	O
sup	O
p	O
{	O
ln	O
:	O
:	O
:	O
:	O
e	O
}	O
:	O
:	O
:	O
:	O
(	O
x	O
,	O
y	O
)	O
:	O
lc=o	O
sup	O
p	O
{	O
ln	O
(	O
b	O
)	O
:	O
:	O
:	O
:	O
e	O
}	O
b	O
:	O
:	O
:	O
:	O
e	O
{	O
p	O
{	O
ln	O
(	O
b	O
)	O
:	O
:	O
:	O
:	O
eib	O
)	O
}	O
p	O
{	O
ln	O
(	O
b	O
)	O
:	O
:	O
:	O
:	O
e	O
}	O
.	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
14.1	O
,	O
observe	O
that	O
ln	O
(	O
b	O
)	O
can	O
not	O
be	O
smaller	O
than	O
the	O
bayes	O
risk	O
corresponding	O
to	O
the	O
decision	O
problem	O
(	O
zn	O
,	O
ibex	O
)	O
)	O
,	O
where	O
thus	O
,	O
ln	O
(	O
b	O
)	O
:	O
:	O
:	O
:	O
e	O
{	O
min	O
(	O
1	O
]	O
*	O
(	O
zn	O
)	O
,	O
1	O
-1	O
]	O
*	O
(	O
zn	O
)	O
)	O
ix	O
i	O
,	O
...	O
,	O
xn	O
}	O
.	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
14.1	O
,	O
we	O
see	O
that	O
e	O
{	O
min	O
(	O
1	O
]	O
*	O
(	O
zn	O
)	O
,	O
1	O
-	O
1	O
]	O
*	O
(	O
zn	O
)	O
)	O
ix	O
i	O
,	O
...	O
,	O
xn	O
}	O
1	O
2p	O
{	O
x	O
f	O
x	O
i	O
.	O
···	O
,	O
x	O
f	O
xn	O
,	O
x	O
f	O
xvix	O
i	O
,	O
···	O
,	O
xn	O
}	O
1	O
v-i	O
:	O
:	O
:	O
:	O
:	O
2	O
p	O
l	O
i	O
{	O
x	O
;	O
=jx1	O
,	O
...	O
,	O
xifxil	O
}	O
,	O
i=i	O
for	O
fixed	O
xl	O
,	O
...	O
,	O
xn	O
,	O
we	O
denote	O
by	O
1	O
the	O
collection	O
{	O
j	O
:	O
1	O
s	O
j	O
s	O
v-i	O
,	O
n7=1	O
{	O
xi	O
=i	O
x	O
j	O
}	O
}	O
.	O
this	O
is	O
the	O
collection	O
of	O
empty	O
cells	O
xi	O
.	O
we	O
summarize	O
:	O
we	O
consider	O
two	O
choices	O
for	O
p	O
:	O
choice	O
a.	O
take	O
p	O
=	O
lin	O
,	O
and	O
assume	O
12ne	O
s	O
v-i	O
,	O
e	O
<	O
1/2	O
.	O
note	O
that	O
for	O
n	O
:	O
:	O
:	O
:	O
:	O
15	O
,	O
eill	O
=	O
(	O
v	O
-	O
1	O
)	O
(	O
1	O
-	O
p	O
)	O
n	O
:	O
:	O
:	O
:	O
(	O
v	O
-	O
1	O
)	O
/3	O
.	O
also	O
,	O
since	O
0	O
sill	O
s	O
v-i	O
,	O
we	O
have	O
var	O
iii	O
s	O
(	O
v	O
-	O
1	O
)	O
2/4	O
.	O
by	O
the	O
chebyshev-cantelli	O
inequality	B
(	O
theorem	B
a.l7	O
)	O
,	O
p	O
{	O
lll	O
:	O
:	O
:	O
:	O
:	O
2ne	O
}	O
1	O
-	O
p	O
{	O
lll	O
<	O
2ne	O
}	O
:	O
:	O
:	O
:	O
1	O
-	O
p	O
{	O
lll	O
<	O
(	O
v	O
-	O
1	O
)	O
/6	O
}	O
1	O
-	O
p	O
{	O
lll	O
-	O
eill	O
s	O
-	O
(	O
v	O
-	O
1	O
)	O
/6	O
}	O
238	O
14.	O
low~r	O
bounds	O
for	O
empirical	B
classifier	I
selection	I
>	O
1	O
____	O
v	O
:	O
_a_r_l_l_1	O
__	O
var	O
iii	O
+	O
(	O
v	O
-	O
1	O
)	O
2/36	O
(	O
v	O
-	O
1	O
)	O
2/4	O
(	O
v	O
-	O
1	O
)	O
2/4	O
+	O
(	O
v	O
-	O
1	O
)	O
2/36	O
1	O
_	O
1	O
10	O
this	O
proves	O
the	O
second	O
inequality	B
for	O
supp	O
{	O
lil	O
2	O
:	O
e	O
}	O
.	O
choice	O
b.	O
assume	O
that	O
e	O
:5	O
1/4	O
.	O
by	O
the	O
pigeonhole	B
principle	I
,	O
iii	O
~	O
2e	O
/	O
p	O
if	O
the	O
number	O
of	O
points	O
xi	O
,	O
1	O
:5	O
i	O
:5	O
n	O
,	O
that	O
are	O
not	O
equal	O
to	O
xv	O
does	O
not	O
exceed	O
v-i	O
-	O
2e	O
/	O
p.	O
therefore	O
,	O
we	O
have	O
a	O
further	O
lower	O
bound	O
:	O
p	O
{	O
lll	O
~	O
2e/p	O
}	O
~	O
p	O
{	O
binomial	B
(	O
n	O
,	O
(	O
v	O
-1	O
)	O
p	O
)	O
:5	O
v-i	O
-	O
2e/p	O
}	O
.	O
define	O
v	O
=	O
r	O
(	O
v	O
-	O
1	O
)	O
/21	O
.	O
take	O
p	O
=	O
2e/v	O
.	O
by	O
assumption	O
,	O
n	O
~	O
2v	O
-	O
1.	O
then	O
the	O
lower	O
bound	O
is	O
p	O
{	O
binomial	B
(	O
n	O
,	O
4e	O
)	O
:5	O
v	O
}	O
>	O
(	O
:	O
)	O
<	O
4	O
<	O
)	O
''	O
(	O
1	O
_	O
4	O
<	O
yh	O
>	O
_1_	O
(	O
4ee	O
(	O
n	O
-	O
v	O
+	O
1	O
»	O
)	O
v	O
(	O
'i	O
_	O
4e	O
)	O
1l	O
v	O
(	O
1	O
-	O
4e	O
)	O
e-j2n	O
v	O
(	O
since	O
(	O
:	O
)	O
2	O
:	O
(	O
n-~+l	O
)	O
e	O
)	O
v	O
ek	O
by	O
stirling	O
's	O
formula	O
)	O
1	O
(	O
4ee	O
(	O
n	O
-	O
v	O
+	O
1	O
»	O
)	O
v	O
11	O
(	O
1-4e	O
)	O
>	O
-	O
-	O
e-j2nv	O
v	O
>	O
_1_	O
(	O
4ene	O
)	O
v	O
(	O
1-	O
4e	O
)	O
1l	O
(	O
1-	O
v	O
-	O
e-j2nv	O
v	O
l	O
)	O
v	O
n	O
>	O
__	O
1_	O
(	O
2ene	O
)	O
v	O
e-4m=/	O
(	O
l-4€	O
)	O
e-j2nv	O
(	O
use	O
1	O
-	O
x	O
2	O
:	O
exp	O
(	O
-x/	O
(	O
1-	O
x	O
»	O
)	O
.	O
v	O
(	O
since	O
n	O
2	O
:	O
2	O
(	O
v	O
_	O
1	O
»	O
this	O
concludes	O
the	O
proof	O
.	O
0	O
14.3	O
classes	O
with	O
infinite	O
vc	B
dimension	I
the	O
results	O
presented	O
in	O
the	O
previous	O
section	O
may	O
also	O
be	O
applied	O
to	O
classes	O
with	O
infinite	O
vc	B
dimension	I
.	O
for	O
example	O
,	O
it	O
is	O
not	O
hard	O
to	O
derive	O
the	O
following	O
result	O
from	O
theorem	B
14.1	O
:	O
theorem	B
14.3.	O
assume	O
that	O
vc	O
=	O
00.	O
for	O
every	O
n	O
,	O
8	O
>	O
0	O
and	O
classification	O
rule	B
gn	O
,	O
there	O
is	O
a	O
distribution	O
with	O
lc	O
=	O
0	O
such	O
that	O
14.4	O
the	O
case	O
lc	O
>	O
0	O
239	O
el	O
(	O
gn	O
)	O
>	O
-	O
1	O
2e	O
j.	O
for	O
the	O
proof	O
,	O
see	O
problem	O
14.2.	O
thus	O
,	O
when	O
vc	O
=	O
00	O
,	O
distribution-free	O
nontrivial	O
performance	O
guarantees	O
for	O
l	O
(	O
g	O
n	O
)	O
-	O
lc	O
do	O
not	O
exist	O
.	O
this	O
generalizes	O
theorem	B
7.1	O
,	O
where	O
a	O
similar	O
result	O
is	O
shown	O
if	O
c	O
is	O
the	O
class	O
of	O
all	O
measurable	O
discrimination	O
functions	O
.	O
we	O
have	O
also	O
seen	O
in	O
theorem	O
7.2	O
,	O
that	O
if	O
c	O
is	O
the	O
class	O
of	O
all	O
measurable	O
classifiers	O
,	O
then	O
no	O
universal	B
rate	O
of	O
convergence	O
exists	O
.	O
however	O
,	O
we	O
will	O
see	O
in	O
chapter	O
18	O
that	O
for	O
some	O
classes	O
with	O
infinite	O
vc	B
dimension	I
,	O
it	O
is	O
possible	O
to	O
find	O
a	O
classification	O
rule	B
such	O
that	O
l	O
(	O
gn	O
)	O
-	O
l	O
*	O
:	O
:	O
:	O
cjlog	O
n/n	O
for	O
any	O
distribution	B
such	O
that	O
the	O
bayes	O
classifier	B
is	O
in	O
c.	O
the	O
constant	O
c	O
,	O
however	O
,	O
necessarily	O
depends	O
on	O
the	O
distribution	B
,	O
as	O
is	O
apparent	O
from	O
theorem	B
14.3.	O
infinite	O
vc	B
dimension	I
means	O
that	O
the	O
class	O
c	O
shatters	O
finite	O
sets	O
of	O
any	O
size	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
c	O
shatters	O
infinitely	O
many	O
points	O
,	O
then	O
no	O
universal	B
rate	O
of	O
convergence	O
exists	O
.	O
this	O
may	O
be	O
seen	O
by	O
an	O
appropriate	O
modification	O
of	O
theorem	O
7.2	O
,	O
as	O
follows	O
.	O
see	O
problem	O
14.3	O
for	O
the	O
proof	O
.	O
theorem	B
14.4.	O
let	O
{	O
an	O
}	O
be	O
a	O
sequence	O
of	O
positive	O
numbers	O
converging	O
to	O
zero	O
with	O
1/16	O
~	O
al	O
~	O
a2	O
~	O
...	O
.	O
let	O
c	O
be	O
a	O
class	O
of	O
classifiers	O
with	O
the	O
property	O
that	O
there	O
exists	O
a	O
set	O
a	O
c	O
nd	O
of	O
infinite	O
cardinality	O
such	O
that	O
for	O
any	O
subset	O
b	O
of	O
a	O
,	O
there	O
exists	O
¢	O
e	O
c	O
such	O
that	O
¢	O
(	O
x	O
)	O
=	O
1	O
if	O
x	O
e	O
band	O
¢	O
(	O
x	O
)	O
=	O
0	O
if	O
x	O
e	O
a-b	O
.	O
then	O
for	O
every	O
sequence	O
of	O
classification	O
rules	O
,	O
there	O
exists	O
a	O
distribution	O
of	O
(	O
x	O
,	O
y	O
)	O
with	O
le	O
=	O
0	O
,	O
such	O
that	O
for	O
all	O
n.	O
note	O
that	O
the	O
basic	O
difference	O
between	O
this	O
result	O
and	O
all	O
others	O
in	O
this	O
chapter	O
is	O
that	O
in	O
theorem	O
14.4	O
the	O
``	O
bad	O
''	O
distribution	B
does	O
not	O
vary	O
with	O
n.	O
this	O
theorem	B
shows	O
that	O
selecting	O
a	O
classifier	O
from	O
a	O
class	O
shattering	B
infinitely	O
many	O
points	O
is	O
essentially	O
as	O
hard	O
as	O
selecting	O
one	O
from	O
the	O
class	O
of	O
all	O
classifiers	O
.	O
14.4	O
the	O
case	O
lc	O
>	O
0	O
in	O
the	O
more	O
general	O
case	O
,	O
when	O
the	O
best	O
decision	O
in	O
the	O
class	O
c	O
has	O
positive	O
error	O
probability	O
,	O
the	O
upper	O
bounds	O
derived	O
in	O
chapter	O
12	O
for	O
the	O
expected	O
error	O
proba	O
(	O
cid:173	O
)	O
bility	O
of	O
the	O
classifier	B
obtained	O
by	O
minimizing	O
the	O
empirical	B
risk	I
are	O
much	O
larger	O
than	O
when	O
le	O
=	O
o.	O
in	O
this	O
section	O
we	O
show	O
that	O
these	O
upper	O
bounds	O
are	O
necessarily	O
large	O
,	O
and	O
they	O
may	O
be	O
tight	O
for	O
some	O
distributions	O
.	O
moreover	O
,	O
there	O
is	O
no	O
other	O
classifier	B
that	O
performs	O
significantly	O
better	O
than	O
empirical	B
error	I
minimization	O
.	O
theorem	B
14.5	O
below	O
gives	O
a	O
lower	O
bound	O
for	O
supex	O
,	O
y	O
)	O
:	O
lc	O
fixed	O
el	O
(	O
gn	O
)	O
-	O
le	O
.	O
as	O
a	O
function	O
of	O
nand	O
vc	O
,	O
the	O
bound	O
decreases	O
basically	O
as	O
in	O
the	O
upper	O
bound	O
ob	O
(	O
cid:173	O
)	O
tained	O
from	O
theorem	B
12.6.	O
interestingly	O
,	O
the	O
lower	O
bound	O
becomes	O
smaller	O
as	O
le	O
240	O
14.	O
lower	B
bounds	I
for	I
empirical	O
classifier	B
selection	I
decreases	O
,	O
as	O
should	O
be	O
expected	O
.	O
the	O
bound	O
is	O
largest	O
when	O
lc	O
is	O
close	O
to	O
1/2	O
.	O
the	O
constants	O
in	O
the	O
bound	O
may	O
be	O
tightened	O
at	O
the	O
expense	O
of	O
more	O
complicated	O
expressions	O
.	O
the	O
theorem	B
is	O
essentially	O
due	O
to	O
devroye	O
and	O
lugosi	O
(	O
1995	O
)	O
,	O
though	O
the	O
proof	O
given	O
here	O
is	O
different	O
.	O
similar	O
bounds-without	O
making	O
the	O
dependence	O
on	O
lc	O
explicit-have	O
been	O
proved	O
by	O
vapnik	O
and	O
chervonenkis	O
(	O
197	O
4c	O
)	O
and	O
simon	O
(	O
1993	O
)	O
.	O
theorem	B
14.5.	O
let	O
c	O
be	O
a	O
class	O
of	O
discrimination	O
functions	O
with	O
vc	O
dimension	B
:	O
:	O
:	O
:	O
2.	O
let	O
x	O
be	O
the	O
set	O
of	O
all	O
random	O
variables	O
(	O
x	O
,	O
y	O
)	O
for	O
which	O
for	O
fixed	O
v	O
l	O
e	O
(	O
0	O
,	O
1/2	O
)	O
,	O
l	O
=	O
inf	O
p	O
{	O
g	O
(	O
x	O
)	O
i	O
y	O
}	O
.	O
gee	O
then	O
,	O
for	O
every	O
discrimination	O
rule	B
gn	O
based	O
upon	O
xl	O
,	O
y1	O
,	O
.•	O
.	O
,	O
x	O
n	O
,	O
yn	O
,	O
sup	O
e	O
(	O
l	O
n	O
-	O
l	O
)	O
:	O
:	O
:	O
:	O
(	O
x	O
,	O
y	O
)	O
ea	O
'	O
/l	O
(	O
v	O
-1	O
)	O
24n	O
e-	O
8	O
ifn	O
:	O
:	O
:	O
:	O
i~l	O
max	O
(	O
9	O
,	O
1/	O
(	O
1	O
-	O
2l	O
)	O
2	O
)	O
.	O
(	O
v	O
-	O
proof	O
.	O
again	O
we	O
consider	O
the	O
finite	O
family	O
f	O
from	O
the	O
previous	O
section	O
.	O
the	O
notation	O
band	O
b	O
is	O
also	O
as	O
above	O
.	O
x	O
now	O
puts	O
mass	O
p	O
at	O
xi	O
,	O
i	O
<	O
v	O
,	O
and	O
mass	O
l	O
)	O
p	O
:	O
s	O
1	O
,	O
which	O
will	O
be	O
1	O
-	O
satisfied	O
.	O
next	O
introduce	O
the	O
constant	O
c	O
e	O
(	O
0	O
,	O
1/2	O
)	O
.	O
we	O
no	O
longer	O
have	O
y	O
as	O
a	O
function	O
of	O
x.	O
instead	O
,	O
we	O
have	O
a	O
uniform	O
[	O
0	O
,	O
1	O
]	O
random	O
variable	B
u	O
independent	O
of	O
x	O
and	O
define	O
l	O
)	O
p	O
at	O
xv	O
.	O
this	O
imposes	O
the	O
condition	O
(	O
v	O
-	O
y	O
=	O
{	O
i	O
if	O
u	O
:	O
s	O
4	O
-	O
c	O
+	O
2cbi	O
,	O
x	O
=	O
xi	O
,	O
i	O
<	O
v	O
°	O
otherwise	O
.	O
thus	O
,	O
when	O
x	O
=	O
xi	O
,	O
i	O
<	O
v	O
,	O
y	O
is	O
1	O
with	O
probability	O
1/2	O
-	O
cor	O
1/2	O
+	O
c.	O
a	O
simple	O
argument	O
shows	O
that	O
the	O
best	O
rule	B
for	O
b	O
is	O
the	O
one	O
which	O
sets	O
f	O
,	O
(	O
x	O
)	O
=	O
{	O
i	O
if	O
x	O
=	O
~i	O
,	O
i	O
<	O
v	O
,	O
bi	O
=	O
1	O
°	O
otherwise	O
.	O
b	O
also	O
,	O
observe	O
that	O
noting	O
that	O
127j	O
(	O
xj	O
-	O
2.2	O
,	O
we	O
may	O
write	O
l	O
=	O
(	O
v	O
-	O
l	O
)	O
p	O
(	O
1j2	O
-	O
c	O
)	O
.	O
11	O
=	O
c	O
for	O
i	O
<	O
v	O
,	O
for	O
fixed	O
b	O
,	O
by	O
the	O
equality	O
in	O
theorem	O
v-i	O
ln	O
-	O
l	O
:	O
:	O
:	O
:	O
l	O
2pc	O
!	O
(	O
gll	O
(	O
xi	O
,	O
x	O
1	O
,	O
y1	O
,	O
•••	O
,	O
xil	O
,	O
l	O
'	O
;	O
,	O
)	O
=1-	O
fb	O
(	O
xdl	O
.	O
i=l	O
it	O
is	O
sometimes	O
convenient	O
to	O
make	O
the	O
dependence	O
of	O
gn	O
upon	O
b	O
explicit	O
by	O
con	O
(	O
cid:173	O
)	O
sidering	O
gn	O
(	O
xi	O
)	O
as	O
a	O
function	O
of	O
xi	O
,	O
xl	O
,	O
...	O
,	O
x	O
n	O
,	O
u	O
1	O
,	O
...	O
,	O
un	O
(	O
an	O
i.i.d	O
.	O
sequence	O
of	O
uniform	O
[	O
0	O
,	O
1	O
]	O
random	O
variables	O
)	O
,	O
and	O
bi	O
•	O
the	O
proof	O
given	O
here	O
is	O
based	O
on	O
14.4	O
the	O
case	O
lc	O
>	O
0	O
241	O
the	O
ideas	O
used	O
in	O
the	O
proofs	O
of	O
theorems	O
14.1	O
and	O
14.2.	O
we	O
replace	O
b	O
by	O
a	O
uni	O
(	O
cid:173	O
)	O
formly	O
distributed	O
random	O
b	O
over	O
{	O
o	O
,	O
i	O
}	O
v-i	O
.	O
after	O
this	O
randomization	O
,	O
denote	O
zn	O
=	O
(	O
x	O
,	O
xl	O
,	O
yi	O
,	O
·	O
..	O
,	O
x	O
n	O
,	O
yn	O
)	O
.	O
thus	O
,	O
sup	O
e	O
{	O
ln	O
-	O
l	O
}	O
(	O
x	O
,	O
y	O
)	O
ef	O
supe	O
{	O
ln	O
-	O
l	O
}	O
b	O
v-i	O
i=i	O
(	O
with	O
random	O
b	O
)	O
>	O
e	O
{	O
ln	O
-	O
l	O
}	O
>	O
l	O
2pcei	O
{	O
gn	O
(	O
x	O
;	O
,	O
x	O
j	O
,	O
•••	O
,	O
yn	O
)	O
=i-	O
ibex	O
;	O
)	O
}	O
=	O
2cp	O
{	O
gn	O
(	O
zn	O
)	O
=i	O
ibex	O
)	O
}	O
>	O
2cl	O
*	O
(	O
zn	O
,	O
ibex	O
)	O
)	O
,	O
where	O
,	O
as	O
before	O
,	O
l	O
*	O
(	O
zn	O
,	O
ibex	O
)	O
)	O
denotes	O
the	O
bayes	O
probability	O
of	O
error	O
of	O
pre	O
(	O
cid:173	O
)	O
dicting	O
the	O
value	O
of	O
ibex	O
)	O
based	O
on	O
observing	O
zn	O
'	O
all	O
we	O
have	O
to	O
do	O
is	O
to	O
find	O
a	O
suitable	O
lower	O
bound	O
for	O
where	O
1	O
]	O
*	O
(	O
zn	O
)	O
=	O
p	O
{	O
ib	O
(	O
x	O
)	O
=	O
lizn	O
}	O
.	O
observe	O
that	O
next	O
we	O
compute	O
p	O
{	O
bi	O
=	O
llyij	O
=	O
yi	O
,	O
...	O
,	O
yik	O
=	O
yk	O
}	O
for	O
yi	O
,	O
...	O
,	O
yk	O
e	O
{	O
o	O
,	O
i	O
}	O
.	O
denoting	O
the	O
numbers	O
of	O
zeros	O
and	O
ones	O
by	O
ko	O
=	O
i	O
{	O
j	O
s	O
k	O
:	O
y	O
j	O
=	O
o	O
}	O
i	O
and	O
k	O
1	O
=	O
i	O
{	O
j	O
s	O
k	O
:	O
yj	O
=	O
1	O
}	O
1	O
,	O
we	O
see	O
that	O
p	O
{	O
bi	O
=	O
llyij	O
=	O
yi	O
,	O
...	O
,	O
yik	O
=	O
yd	O
=	O
(	O
1	O
-	O
2c	O
)	O
kj	O
(	O
1	O
+	O
2c	O
)	O
ko	O
(	O
1	O
-	O
2c	O
)	O
kl	O
(	O
1	O
+	O
2c	O
)	O
ko	O
+	O
(	O
1	O
+	O
2c	O
)	O
kl	O
(	O
1	O
-	O
2c	O
)	O
ko	O
.	O
therefore	O
,	O
if	O
x	O
=	O
xij	O
=	O
...	O
=	O
x	O
ik	O
=	O
xi	O
,	O
i	O
<	O
v	O
,	O
then	O
min	O
(	O
17*	O
(	O
zn	O
)	O
,	O
1	O
-	O
1	O
]	O
*	O
(	O
z/i	O
)	O
)	O
min	O
(	O
(	O
1	O
-	O
2c	O
)	O
kj	O
(	O
1	O
+	O
2c	O
)	O
ko	O
,	O
(	O
1	O
+	O
2c	O
)	O
kj	O
(	O
1	O
-	O
2c	O
)	O
ko	O
)	O
(	O
1	O
-	O
2c	O
)	O
kj	O
(	O
1	O
+	O
2c	O
)	O
ko	O
+	O
(	O
1	O
+	O
2c	O
)	O
kj	O
(	O
1	O
-	O
2c	O
)	O
ko	O
min	O
(	O
1	O
,	O
(	O
f*	O
)	O
kj	O
-ko	O
)	O
1	O
+	O
(	O
i+2c	O
)	O
k1-ko	O
i-2c	O
1	O
+	O
(	O
i+2c	O
)	O
lkj-koi	O
'	O
i-2c	O
242	O
14.	O
lower	B
bounds	I
for	I
empirical	O
classifier	B
selection	I
in	O
summary	O
,	O
denoting	O
a	O
=	O
(	O
1	O
+	O
2c	O
)	O
/	O
(	O
l	O
-	O
2c	O
)	O
,	O
we	O
have	O
l	O
*	O
(	O
zn	O
'	O
ibex	O
)	O
)	O
e	O
{	O
1	O
1	O
+	O
a/lj	O
:	O
xj=x	O
(	O
2yj-1	O
)	O
/	O
}	O
>	O
e	O
{	O
2all	O
:	O
jxj~x	O
(	O
2yj-i	O
)	O
1	O
}	O
>	O
>	O
1	O
i	O
:	O
p	O
{	O
x	O
=	O
xde	O
{	O
a	O
-ilj	O
:	O
xj=xi	O
(	O
2yj-l	O
)	O
i	O
}	O
2	O
i=l	O
~	O
(	O
v	O
-	O
2	O
(	O
by	O
jensen	O
's	O
inequality	B
)	O
.	O
l	O
)	O
pa	O
-e	O
{	O
]	O
lj	O
:	O
xrxi	O
(	O
2yj	O
-l	O
)	O
1	O
}	O
next	O
we	O
bound	O
e	O
{	O
i	O
lj	O
:	O
xj=xi	O
(	O
2yj	O
-	O
1	O
)	O
\	O
}	O
.	O
clearly	O
,	O
if	O
b	O
(	O
k	O
,	O
q	O
)	O
denotes	O
a	O
binomial	O
random	O
variable	B
with	O
parameters	O
k	O
and	O
q	O
,	O
e	O
{	O
ij~	O
;	O
(	O
2yj	O
-ljl	O
}	O
=	O
~	O
g	O
)	O
pk	O
(	O
l-	O
p	O
)	O
n-ke	O
{	O
i2b	O
(	O
k	O
,	O
1/2	O
-	O
c	O
)	O
-	O
klj·	O
however	O
,	O
by	O
straightforward	O
calculation	O
we	O
see	O
that	O
e	O
{	O
12b	O
(	O
k	O
,	O
1/2	O
-	O
c	O
)	O
-	O
kl	O
}	O
<	O
je	O
{	O
(	O
2b	O
(	O
k	O
,	O
1/2	O
-	O
c	O
)	O
-	O
k	O
)	O
2	O
}	O
jk	O
(	O
l	O
-	O
4c2	O
)	O
+	O
4k2c2	O
<	O
2kc	O
+	O
,	O
jk	O
.	O
therefore	O
,	O
applying	O
jensen	O
's	O
inequality	B
once	O
again	O
,	O
we	O
get	O
t	O
(	O
n	O
)	O
pk	O
(	O
1	O
-	O
pt-ke	O
{	O
12b	O
(	O
k	O
,	O
1/2	O
-	O
c	O
)	O
-	O
kl	O
}	O
:	O
:	O
:	O
2npc	O
+-jrlp	O
.	O
k=o	O
k	O
summarizing	O
what	O
we	O
have	O
obtained	O
so	O
far	O
,	O
we	O
have	O
supe	O
{	O
ln	O
-	O
l	O
}	O
>	O
b	O
2cl	O
*	O
(	O
zn	O
,	O
ibex	O
)	O
)	O
>	O
2c-	O
(	O
v	O
-	O
l	O
)	O
pa-2npc	O
-y	O
?	O
£p	O
>	O
l	O
)	O
pe-2npc	O
(	O
a-l	O
)	O
-	O
(	O
a-1	O
)	O
y	O
?	O
£p	O
1	O
2	O
c	O
(	O
v	O
-	O
(	O
by	O
the	O
inequality	B
1	O
+	O
x	O
:	O
:	O
:	O
ex	O
)	O
c	O
(	O
v	O
-	O
l	O
)	O
pe-8npc2/	O
(	O
l-2c	O
)	O
-4cy	O
?	O
£p/	O
(	O
l-2c	O
)	O
.	O
a	O
rough	O
asymptotic	O
analysis	O
shows	O
that	O
the	O
best	O
asymptotic	O
choice	O
for	O
c	O
is	O
given	O
by	O
1	O
c=	O
-	O
-	O
,	O
j4np	O
'	O
14.4	O
the	O
case	O
lc	O
>	O
0	O
243	O
then	O
the	O
constraint	O
l	O
=	O
(	O
v	O
-	O
1	O
)	O
p	O
(	O
1	O
/2	O
-	O
c	O
)	O
leaves	O
us	O
with	O
a	O
quadratic	O
equation	O
in	O
c.	O
instead	O
of	O
solving	O
this	O
equation	O
,	O
it	O
is	O
more	O
convenient	O
to	O
take	O
c	O
=	O
j	O
(	O
v	O
-	O
1	O
)	O
/	O
(	O
8nl	O
)	O
.	O
if2nl/	O
(	O
v-1	O
)	O
:	O
:	O
:	O
:	O
:	O
9	O
,	O
thenc	O
:	O
:	O
:	O
:	O
1/6	O
.	O
with	O
this	O
choiceforc	O
,	O
using	O
l	O
=	O
(	O
v-1	O
)	O
p	O
(	O
1/2-	O
c	O
)	O
,	O
straightforward	O
calculation	O
provides	O
sup	O
e	O
(	O
l	O
n	O
-	O
l	O
)	O
:	O
:	O
:	O
:	O
:	O
(	O
x	O
,	O
y	O
)	O
ef	O
j	O
(	O
v	O
-	O
l	O
)	O
l	O
24n	O
e-8	O
•	O
the	O
condition	O
p	O
(	O
v-i	O
)	O
:	O
:	O
:	O
:	O
1	O
implies	O
that	O
we	O
need	O
to	O
ask	O
that	O
n	O
:	O
:	O
:	O
:	O
:	O
(	O
v	O
-	O
1	O
)	O
/	O
(	O
2l	O
(	O
1	O
2l	O
)	O
2	O
)	O
.	O
this	O
concludes	O
the	O
proof	O
of	O
theorem	O
14.5.	O
d	O
next	O
we	O
obtain	O
a	O
probabilistic	O
bound	O
.	O
its	O
proof	O
below	O
is	O
based	O
upon	O
hellinger	O
distances	O
,	O
and	O
its	O
methodology	O
is	O
essentially	O
due	O
to	O
assouad	O
(	O
1983b	O
)	O
.	O
for	O
refine	O
(	O
cid:173	O
)	O
ments	O
and	O
applications	O
,	O
we	O
refer	O
to	O
birge	O
(	O
1983	O
;	O
1986	O
)	O
and	O
devroye	O
(	O
1987	O
)	O
.	O
theorem	B
14.6	O
.	O
(	O
devroye	O
and	O
lugosi	O
(	O
1995	O
)	O
)	O
.	O
let	O
c	O
be	O
a	O
class	O
of	O
discrimination	O
functions	O
with	O
vc	O
dimension	B
v	O
:	O
:	O
:	O
:	O
:	O
2.	O
let	O
x	O
be	O
the	O
set	O
of	O
all	O
random	O
variables	O
(	O
x	O
,	O
y	O
)	O
for	O
which	O
for	O
fixed	O
l	O
e	O
(	O
0	O
,	O
1/4	O
]	O
,	O
l	O
=	O
inf	O
p	O
{	O
g	O
(	O
x	O
)	O
=i	O
y	O
}	O
.	O
gee	O
then	O
,	O
for	O
every	O
discrimination	O
rule	B
gn	O
based	O
upon	O
xl	O
,	O
y1	O
,	O
•••	O
,	O
x	O
n	O
,	O
yn	O
,	O
and	O
any	O
e	O
~	O
l	O
,	O
sup	O
p	O
{	O
ln	O
(	O
x	O
,	O
y	O
)	O
ex	O
proof	O
.	O
the	O
method	O
of	O
randomization	O
here	O
is	O
similar	O
to	O
that	O
in	O
the	O
proof	O
of	O
theorem	O
14.5.	O
using	O
the	O
same	O
notation	O
as	O
there	O
,	O
it	O
is	O
clear	O
that	O
sup	O
p	O
{	O
ln	O
l	O
:	O
:	O
:	O
:	O
:	O
e	O
}	O
(	O
x	O
,	O
y	O
)	O
ex	O
>	O
ei	O
``	O
v-l	O
{	O
l.	O
,	O
i=l	O
2pci	O
{	O
gn	O
(	O
xi	O
,	O
x	O
i	O
,	O
..	O
``	O
yn	O
)	O
=l-	O
fb	O
(	O
xi	O
)	O
}	O
:	O
:	O
:	O
e	O
}	O
=	O
2-	O
(	O
v-l	O
)	O
(	O
xi	O
,	O
..	O
``	O
xi	O
''	O
yl	O
``	O
..	O
,	O
yn	O
)	O
e	O
(	O
{	O
xl	O
''	O
''	O
,	O
xv	O
}	O
x	O
{	O
o	O
,	O
i	O
}	O
)	O
''	O
first	O
observe	O
that	O
if	O
then	O
e/	O
(	O
2pc	O
)	O
~	O
(	O
v	O
-	O
1	O
)	O
/2	O
,	O
(	O
14.1	O
)	O
244	O
14.	O
lower	B
bounds	I
for	I
empirical	O
classifier	B
selection	I
where	O
be	O
denotes	O
the	O
binary	B
vector	O
(	O
1	O
-	O
h	O
,	O
...	O
,	O
1	O
-	O
bv	O
-1	O
)	O
,	O
that	O
is	O
,	O
the	O
complement	O
of	O
b.	O
therefore	O
,	O
for	O
e	O
:	O
:s	O
pc	O
(	O
v	O
-	O
1	O
)	O
,	O
the	O
last	O
expression	B
in	O
the	O
lower	O
bound	O
above	O
is	O
bounded	O
from	O
below	O
by	O
(	O
by	O
lecam	O
's	O
inequality	B
,	O
lemma	O
3.1	O
)	O
2n	O
2	O
;	O
+1	O
l	O
l	O
j	O
pb	O
(	O
x	O
,	O
y	O
)	O
pbc	O
(	O
x	O
,	O
y	O
)	O
)	O
(	O
(	O
x	O
,	O
y	O
)	O
b	O
it	O
is	O
easy	O
to	O
see	O
that	O
for	O
x	O
=	O
xv	O
,	O
pb	O
(	O
x	O
,	O
y	O
)	O
=pbc	O
(	O
x	O
,	O
y	O
)	O
=	O
1	O
-	O
(	O
v	O
-	O
1	O
)	O
p	O
2	O
'	O
and	O
for	O
x	O
=	O
xi	O
,	O
i	O
<	O
v	O
,	O
pb	O
(	O
x	O
,	O
y	O
)	O
pbc	O
(	O
x	O
,	O
y	O
)	O
=	O
p	O
4	O
-	O
c	O
2	O
(	O
1	O
2	O
)	O
thus	O
,	O
we	O
have	O
the	O
equality	O
summarizing	O
,	O
since	O
l	O
=	O
p	O
(	O
v	O
-	O
1	O
)	O
(	O
1/2	O
-	O
c	O
)	O
,	O
we	O
have	O
sup	O
p	O
{	O
ln	O
-	O
l	O
:	O
:	O
:	O
:	O
e	O
}	O
>	O
(	O
x	O
,	O
y	O
)	O
ex	O
>	O
>	O
~	O
(	O
1	O
_	O
~4c2	O
)	O
2n	O
4	O
''	O
2-	O
c	O
~	O
exp	O
(	O
_	O
16nlc	O
1	O
-	O
2c	O
4	O
2	O
/	O
(	O
1	O
_	O
8lc	O
2	O
1	O
-	O
2c	O
)	O
.	O
)	O
,	O
where	O
we	O
used	O
the	O
inequality	B
1	O
-	O
x	O
:	O
:	O
:	O
:	O
e-x/	O
(	O
l-x	O
)	O
again	O
.	O
we	O
may	O
choose	O
c	O
as	O
2l:2e	O
'	O
it	O
is	O
easy	O
to	O
verify	O
that	O
condition	O
(	O
14.1	O
)	O
holds	O
.	O
also	O
,	O
p	O
(	O
v	O
-	O
1	O
)	O
:	O
:s	O
1.	O
from	O
the	O
condition	O
l	O
2	O
:	O
e	O
we	O
deduce	O
that	O
c	O
:	O
's	O
1/4	O
.	O
the	O
exponent	O
in	O
the	O
expression	B
above	O
may	O
be	O
bounded	O
as	O
14.5	O
sample	B
complexity	I
245	O
1	O
-	O
2c	O
-	O
8lc2	O
(	O
by	O
substituting	O
c	O
=	O
e	O
/	O
(	O
2l	O
+	O
2e	O
»	O
16nlc2	O
1	O
''	O
=2c	O
1	O
_	O
8lc2	O
1-2c	O
=	O
=	O
<	O
thus	O
,	O
(	O
x	O
,	O
y	O
)	O
ex	O
as	O
desired	O
.	O
0	O
sup	O
p	O
{	O
ln	O
-	O
l	O
2	O
:	O
e	O
}	O
2	O
:	O
-	O
exp	O
(	O
-4ne2	O
/	O
l	O
)	O
,	O
1	O
4	O
14.5	O
sample	B
complexity	I
we	O
may	O
rephrase	O
the	O
probability	O
bounds	O
above	O
in	O
terms	O
of	O
the	O
sample	B
complexity	I
of	O
algorithms	O
for	O
selecting	O
a	O
classifier	O
from	O
a	O
class	O
.	O
recall	O
that	O
for	O
given	O
e	O
,	O
8	O
>	O
0	O
,	O
the	O
sample	B
complexity	I
of	O
a	O
selection	O
rule	B
gn	O
is	O
the	O
smallest	O
integer	O
n	O
(	O
e	O
,	O
8	O
)	O
such	O
that	O
sup	O
p	O
{	O
l	O
(	O
gn	O
)	O
-	O
lc	O
2	O
:	O
e	O
}	O
:	O
's	O
8	O
for	O
all	O
n	O
2	O
:	O
n	O
(	O
e	O
,	O
8	O
)	O
.	O
the	O
supremum	O
is	O
taken	O
over	O
a	O
class	O
of	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
.	O
here	O
we	O
are	O
interested	O
in	O
distributions	O
such	O
that	O
lc	O
is	O
fixed	O
.	O
we	O
start	O
with	O
the	O
case	O
lc	O
=	O
0	O
,	O
by	O
checking	O
the	O
implications	O
of	O
theorem	O
14.2	O
for	O
the	O
sample	B
complexity	I
n	O
(	O
e	O
,	O
8	O
)	O
.	O
first	O
blumer	O
,	O
ehrenfeucht	O
,	O
haussler	O
,	O
and	O
warmuth	O
(	O
1989	O
)	O
showed	O
that	O
for	O
any	O
algorithm	B
,	O
n	O
(	O
e	O
,	O
o	O
)	O
,	O
,=cgiogg	O
)	O
+vc	O
)	O
,	O
where	O
c	O
is	O
a	O
universal	O
constant	O
.	O
in	O
ehrenfeucht	O
,	O
haussler	O
,	O
kearns	O
,	O
and	O
valiant	O
(	O
1989	O
)	O
,	O
the	O
lower	O
bound	O
was	O
partially	O
improved	O
to	O
vc	O
-1	O
n	O
(	O
e,8	O
)	O
2	O
:	O
~	O
:	O
's	O
1/8	O
and	O
8	O
:	O
's	O
1/100	O
.	O
it	O
may	O
be	O
combined	O
with	O
the	O
previous	O
bound	O
.	O
when	O
e	O
theorem	B
14.2	O
provides	O
the	O
following	O
bounds	O
:	O
corollary	O
14.1.	O
let	O
c	O
be	O
a	O
class	O
of	O
discrimination	O
functions	O
with	O
vc	O
dimension	B
v	O
2	O
:	O
2.	O
let	O
x	O
be	O
the	O
set	O
of	O
all	O
random	O
variables	O
(	O
x	O
,	O
y	O
)	O
for	O
which	O
lc	O
=	O
0.	O
assume	O
246	O
14.	O
lower	B
bounds	I
for	I
empirical	O
classifier	B
selection	I
e	O
:	O
's	O
1/4	O
,	O
and	O
denote	O
v	O
=	O
rev	O
-l	O
)	O
/2l	O
thenfor	O
every	O
discrimination	O
rule	B
gn	O
based	O
upon	O
xl	O
,	O
f	O
l	O
,	O
...	O
,	O
x	O
n	O
,	O
fn	O
,	O
when	O
e	O
:	O
:	O
:	O
1/8	O
and	O
then	O
1	O
n	O
(	O
.	O
,	O
8	O
)	O
~	O
8	O
•	O
log	O
g	O
)	O
finally	O
,	O
jor	O
8	O
:	O
's	O
1/10	O
,	O
and	O
e	O
<	O
1/2	O
,	O
v-i	O
n	O
(	O
e	O
,	O
8	O
)	O
:	O
:	O
:	O
--	O
u	O
;	O
-	O
.	O
proof	O
.	O
the	O
second	O
bound	O
follows	O
trivially	O
from	O
the	O
second	O
inequality	B
of	O
theorem	B
14.2.	O
by	O
the	O
first	O
inequality	B
there	O
,	O
sup	O
p	O
{	O
ln	O
:	O
:	O
:	O
e	O
}	O
>	O
1	O
(	O
2nee	O
)	O
(	O
v-i	O
)	O
/2	O
e-4ne/	O
(	O
l-4e	O
)	O
(	O
x	O
,	O
y	O
)	O
ex	O
ev'2nv	O
v-i	O
>	O
>	O
__	O
1_	O
(	O
2ene	O
)	O
v	O
e-81le	O
ev'2nv	O
(	O
se	O
)	O
v	O
v	O
-8ne	O
v	O
-	O
-	O
-	O
n	O
e	O
logv	O
(	O
1/8	O
)	O
(	O
sincee	O
:	O
:	O
:	O
l/s	O
)	O
(	O
since	O
we	O
assume	O
log	O
g	O
)	O
:	O
:	O
:	O
(	O
~	O
)	O
(	O
ev'27r	O
v	O
y/v	O
)	O
.	O
the	O
function	O
nv	O
e-	O
sile	O
varies	O
unimodally	O
in	O
n	O
,	O
and	O
achieves	O
a	O
peak	O
at	O
n	O
=	O
v	O
i	O
(	O
se	O
)	O
.	O
for	O
n	O
below	O
this	O
threshold	B
,	O
by	O
monotonicity	O
,	O
we	O
apply	O
the	O
bound	O
at	O
n	O
=	O
vi	O
(	O
se	O
)	O
.	O
it	O
is	O
easy	O
to	O
verify	O
that	O
the	O
value	O
of	O
the	O
bound	O
at	O
v	O
/	O
(	O
se	O
)	O
is	O
always	O
at	O
least	O
8.	O
if	O
on	O
the	O
other	O
hand	O
,	O
(	O
1	O
/se	O
)	O
10g	O
(	O
1	O
/8	O
)	O
:	O
:	O
:	O
n	O
:	O
:	O
:	O
:	O
v	O
/	O
(	O
se	O
)	O
,	O
the	O
lower	O
bound	O
achieves	O
its	O
minimal	O
value	O
at	O
(	O
1	O
ise	O
)	O
10g	O
(	O
1	O
18	O
)	O
,	O
and	O
the	O
value	O
there	O
is	O
8.	O
this	O
proves	O
the	O
first	O
bound	O
.	O
0	O
corollary	O
14.1	O
shows	O
that	O
for	O
any	O
classifier	B
,	O
at	O
least	O
(	O
1	O
(	O
1	O
)	O
vc	O
-	O
1	O
)	O
''	O
8	O
'~	O
max	O
se	O
log	O
training	O
samples	O
are	O
necessary	O
to	O
achieve	O
e	O
accuracy	B
with	O
8	O
confidence	B
for	O
all	O
distributions	O
.	O
apart	O
from	O
a	O
log	O
(	O
~	O
)	O
factor	O
,	O
the	O
order	O
of	O
magnitude	O
of	O
this	O
expression	B
is	O
the	O
same	O
as	O
that	O
of	O
the	O
upper	O
bound	O
for	O
empirical	B
error	I
minimization	O
,	O
obtained	O
in	O
corollary	O
12.4.	O
that	O
the	O
upper	O
and	O
lower	O
bounds	O
are	O
very	O
close	O
,	O
has	O
two	O
important	O
messages	O
.	O
on	O
the	O
one	O
hand	O
it	O
gives	O
a	O
very	O
good	O
estimate	B
for	O
the	O
number	O
of	O
training	O
samples	O
needed	O
to	O
achieve	O
a	O
certain	O
performance	O
.	O
on	O
the	O
other	O
hand	O
it	O
shows	O
that	O
there	O
is	O
essentially	O
no	O
better	O
method	O
than	O
minimizing	O
the	O
empirical	B
error	I
probability	O
.	O
in	O
the	O
case	O
lc	O
>	O
0	O
,	O
we	O
may	O
derive	O
lower	B
bounds	I
for	I
n	O
(	O
e	O
,	O
8	O
)	O
from	O
theorems	O
14.5	O
and	O
14.6	O
:	O
problems	O
and	O
exercises	O
247	O
corollary	O
14.2.	O
let	O
9	O
be	O
a	O
class	O
of	O
discrimination	O
functions	O
with	O
vc	O
dimension	B
v	O
2	O
:	O
2.	O
let	O
x	O
be	O
the	O
set	O
of	O
all	O
random	O
variables	O
(	O
x	O
,	O
y	O
)	O
for	O
which	O
for	O
fixed	O
l	O
e	O
(	O
0	O
,	O
1/2	O
)	O
,	O
l	O
=	O
infp	O
{	O
g	O
(	O
x	O
)	O
;	O
ly	O
}	O
.	O
gee	O
;	O
;	O
then	O
,	O
for	O
every	O
discrimination	O
rule	B
gn	O
based	O
upon	O
xl	O
,	O
y1	O
,	O
••	O
.	O
,	O
x	O
n	O
,	O
yn	O
,	O
n	O
(	O
e,8	O
)	O
2	O
:	O
l	O
(	O
v	O
-	O
l	O
)	O
e-	O
lo	O
32	O
also	O
,	O
and	O
in	O
particular	O
,	O
for	O
e	O
:	O
:	O
:	O
:	O
;	O
l	O
:	O
:	O
:	O
:	O
;	O
1	O
14	O
,	O
(	O
1	O
1	O
)	O
xmin	O
82	O
'e	O
2	O
.	O
l	O
1	O
n	O
(	O
e	O
,	O
8	O
)	O
2	O
:	O
4e2	O
log	O
48	O
.	O
proof	O
.	O
the	O
first	O
bound	O
may	O
be	O
obtained	O
easily	O
from	O
the	O
expectation-bound	O
of	O
theorem	O
14.5	O
(	O
see	O
problem	O
14.1	O
)	O
.	O
setting	O
the	O
bound	O
of	O
theorem	O
14.6	O
equal	O
to	O
8	O
provides	O
the	O
second	O
bound	O
on	O
n	O
(	O
e	O
,	O
8	O
)	O
.	O
0	O
these	O
bounds	O
may	O
of	O
course	O
be	O
combined	O
.	O
they	O
show	O
that	O
n	O
(	O
e	O
,	O
8	O
)	O
is	O
bounded	O
from	O
below	O
by	O
terms	O
like	O
(	O
l/e2	O
)	O
log	O
(	O
1/8	O
)	O
(	O
independent	O
of	O
vc	O
)	O
and	O
(	O
ve	O
-	O
l	O
)	O
/e2	O
,	O
as	O
i	O
)	O
is	O
typically	O
much	O
smaller	O
than	O
e.	O
by	O
comparing	O
these	O
bounds	O
with	O
the	O
upper	O
bounds	O
of	O
corollary	O
12.3	O
,	O
we	O
see	O
that	O
the	O
only	O
difference	O
between	O
the	O
orders	O
of	O
magnitude	O
is	O
a	O
log	O
(	O
1	O
ie	O
)	O
-factor	O
,	O
so	O
all	O
remarks	O
made	O
for	O
the	O
case	O
le	O
=	O
°	O
remain	O
valid	O
.	O
interestingly	O
,	O
all	O
bounds	O
depend	O
on	O
the	O
class	O
c	O
only	O
through	O
its	O
vc	B
dimension	I
.	O
this	O
fact	O
suggests	O
that	O
when	O
studying	O
distribution-free	O
properties	O
of	O
l	O
(	O
gn	O
)	O
-	O
le	O
,	O
the	O
vc	B
dimension	I
is	O
the	O
most	O
important	O
characteristic	O
of	O
the	O
class	O
.	O
also	O
,	O
all	O
bounds	O
are	O
linear	O
in	O
the	O
vc	B
dimension	I
,	O
which	O
links	O
it	O
conveniently	O
to	O
sample	O
size	O
.	O
remark	O
.	O
it	O
is	O
easy	O
to	O
see	O
from	O
the	O
proofs	O
that	O
all	O
results	O
remain	O
valid	O
if	O
we	O
allow	O
randomization	O
in	O
the	O
rules	O
gn	O
.	O
0	O
problems	O
and	O
exercises	O
problem	O
14.1.	O
show	O
that	O
theorem	B
14.5	O
implies	O
that	O
for	O
every	O
discrimination	O
rule	B
gn	O
based	O
upon	O
d	O
''	O
,	O
l	O
(	O
v	O
-	O
l	O
)	O
e-	O
io	O
n	O
(	O
e	O
,	O
8	O
)	O
:	O
:	O
:	O
:	O
32	O
•	O
(	O
1	O
1	O
)	O
x	O
mill	O
82	O
'	O
e2	O
.	O
hint	O
:	O
assume	O
that	O
p	O
{	O
ln	O
-	O
l	O
>	O
e	O
}	O
<	O
8.	O
then	O
clearly	O
,	O
e	O
{	O
ln	O
-	O
l	O
}	O
:	O
:	O
:	O
:	O
e	O
+	O
8.	O
problem	O
14.2.	O
prove	O
theorem	B
14.3	O
:	O
first	O
apply	O
the	O
proof	O
method	O
of	O
theorem	O
14.1	O
to	O
show	O
that	O
for	O
every	O
n	O
,	O
andg	O
''	O
,	O
there	O
is	O
a	O
distribution	O
with	O
lc	O
=	O
0	O
such	O
thatel	O
(	O
gn	O
)	O
:	O
:	O
:	O
:	O
(	O
1-11	O
n	O
)	O
/	O
(	O
2e	O
)	O
.	O
use	O
a	O
monotonicity	O
argument	O
to	O
finish	O
the	O
proof	O
.	O
problem	O
14.3.	O
prove	O
theorem	B
14.4	O
by	O
modifying	O
the	O
proof	O
of	O
theorem	O
7.2	O
.	O
15	O
the	O
maximum	B
likelihood	I
principle	O
in	O
this	O
chapter	O
we	O
explore	O
the	O
various	O
uses	O
of	O
the	O
maximum	B
likelihood	I
principle	O
in	O
discrimination	O
.	O
in	O
general	O
,	O
the	O
principle	O
is	O
only	O
applicable	O
if	O
we	O
have	O
some	O
a	O
priori	O
knowledge	O
of	O
the	O
problem	O
at	O
hand	O
.	O
we	O
offer	O
definitions	O
,	O
consistency	B
results	O
,	O
and	O
examples	O
that	O
highlight	O
the	O
advantages	O
and	O
shortcomings	O
.	O
15.1	O
maximum	B
likelihood	I
:	O
the	O
formats	O
sometimes	O
,	O
advance	O
information	O
takes	O
a	O
very	O
specific	O
form	O
(	O
e.g.	O
,	O
``	O
if	O
y	O
=	O
1	O
,	O
x	O
is	O
normal	B
(	O
fl	O
,	O
a	O
2	O
)	O
''	O
)	O
.	O
often	O
,	O
it	O
is	O
rather	O
vague	O
(	O
e.g.	O
,	O
``	O
we	O
believe	O
that	O
x	O
has	O
a	O
density	O
,	O
''	O
or	O
``	O
17	O
(	O
x	O
)	O
is	O
thought	O
to	O
be	O
a	O
monotone	O
function	O
of	O
x	O
e	O
r	O
''	O
)	O
.	O
if	O
we	O
have	O
information	O
in	O
set	B
format	I
,	O
the	O
maximum	B
likelihood	I
principle	O
is	O
less	O
appropriate	O
.	O
here	O
we	O
know	O
that	O
the	O
bayes	O
rule	B
g*	O
(	O
x	O
)	O
is	O
of	O
the	O
form	O
g	O
(	O
x	O
)	O
=	O
i	O
{	O
xea	O
}	O
'	O
where	O
a	O
e	O
a	O
and	O
a	O
is	O
a	O
class	O
of	O
sets	O
of	O
rd	O
.	O
we	O
refer	O
to	O
the	O
chapters	O
on	O
empirical	B
risk	I
minimization	I
(	O
see	O
chapter	O
12	O
and	O
also	O
chapter	O
18	O
)	O
for	O
this	O
situation	O
.	O
if	O
we	O
know	O
that	O
the	O
true	O
(	O
unknown	O
)	O
17	O
belongs	O
to	O
a	O
class	O
:	O
.f	O
of	O
functions	O
that	O
map	O
n	O
d	O
to	O
[	O
0	O
,	O
1	O
]	O
,	O
then	O
we	O
say	O
that	O
we	O
are	O
given	O
information	O
in	O
regressionformat	O
.	O
with	O
each	O
17	O
'	O
e	O
:	O
.f	O
we	O
associate	O
a	O
set	O
a	O
=	O
{	O
x	O
:	O
17	O
'	O
(	O
x	O
)	O
>	O
1/2	O
}	O
and	O
a	O
discrimination	O
rule	B
g	O
(	O
x	O
)	O
=	O
i	O
{	O
xea	O
}	O
.	O
the	O
class	O
of	O
these	O
rules	O
is	O
denoted	O
by	O
c.	O
assume	O
that	O
we	O
somehow	O
could	O
estimate	B
17	O
by	O
17n	O
.	O
then	O
it	O
makes	O
sense	O
to	O
use	O
the	O
associated	O
rule	B
gn	O
(	O
x	O
)	O
=	O
i/2	O
}	O
.	O
the	O
maximum	B
likelihood	I
method	O
suggests	O
a	O
way	O
of	O
picking	O
the	O
17n	O
from	O
i	O
{	O
1	O
)	O
il	O
(	O
x	O
»	O
:	O
.f	O
that	O
in	O
some	O
sense	O
is	O
most	O
likely	O
given	O
the	O
data	O
.	O
it	O
is	O
fully	O
automatic-the	O
user	O
does	O
not	O
have	O
to	O
pick	O
any	O
parameters-but	O
it	O
does	O
require	O
a	O
serious	O
implementation	O
effort	O
in	O
many	O
cases	O
.	O
in	O
a	O
sense	O
,	O
the	O
regression	B
format	I
is	O
more	O
powerful	O
than	O
the	O
250	O
15.	O
the	O
maximum	B
likelihood	I
principle	O
set	B
format	I
,	O
as	O
there	O
is	O
more	O
information	O
in	O
knowing	O
a	O
function	O
1	O
]	O
than	O
in	O
knowing	O
the	O
indicator	O
function	O
i	O
{	O
7	O
]	O
>	O
1/2	O
}	O
.	O
still	O
,	O
no	O
structure	O
is	O
assumed	O
on	O
the	O
part	O
of	O
x	O
,	O
and	O
none	O
is	O
needed	O
to	O
obtain	O
consistency	B
results	O
.	O
a	O
third	O
format	O
,	O
even	O
more	O
detailed	O
,	O
is	O
that	O
in	O
which	O
we	O
know	O
that	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
belongs	O
to	O
a	O
class	O
v	O
of	O
distributions	O
on	O
nd	O
x	O
{	O
o	O
,	O
i	O
}	O
.	O
for	O
a	O
given	O
distribu	O
(	O
cid:173	O
)	O
tion	O
,	O
we	O
know	O
1	O
]	O
,	O
so	O
we	O
may	O
once	O
again	O
deduce	O
a	O
rule	O
g	O
by	O
setting	O
g	O
(	O
x	O
)	O
=	O
i	O
{	O
17	O
(	O
x	O
»	O
1/2	O
}	O
.	O
this	O
distributionformat	O
is	O
even	O
more	O
powerful	O
,	O
as	O
the	O
positions	O
x	O
i	O
,	O
...	O
,	O
xn	O
alone	O
in	O
some	O
cases	O
may	O
determine	O
the	O
unknown	O
parameters	O
in	O
the	O
model	O
.	O
this	O
situa	O
(	O
cid:173	O
)	O
tion	O
fits	O
in	O
squarely	O
with	O
classical	O
parameter	B
estimation	I
in	O
mathematical	O
statistics	B
.	O
once	O
again	O
,	O
we	O
may	O
apply	O
the	O
maximum	B
likelihood	I
principle	O
to	O
select	O
a	O
distribu	O
(	O
cid:173	O
)	O
tion	O
from	O
v.	O
unfortunately	O
,	O
as	O
we	O
move	O
to	O
more	O
restrictive	O
and	O
stronger	O
formats	O
,	O
the	O
number	O
of	O
conditions	O
under	O
which	O
the	O
maximum	B
likelihood	I
principle	O
is	O
consis	O
(	O
cid:173	O
)	O
tent	O
increases	O
as	O
well	O
.	O
we	O
will	O
only	O
superficially	O
deal	O
with	O
the	O
distribution	B
format	I
(	O
see	O
chapter	O
16	O
for	O
more	O
detail	O
)	O
.	O
15.2	O
the	O
maximum	B
likelihood	I
method	O
:	O
regression	B
format	I
given	O
xl	O
,	O
...	O
,	O
x	O
n	O
,	O
the	O
probability	O
of	O
observing	O
y1	O
=	O
yi	O
,	O
...	O
,	O
yn	O
=	O
yn	O
is	O
n	O
il1	O
]	O
(	O
xi	O
)	O
yi	O
(	O
l	O
-	O
1	O
]	O
(	O
xi	O
»	O
l-	O
yi	O
.	O
i=l	O
if	O
1	O
]	O
is	O
unknown	O
but	O
belongs	O
to	O
a	O
family	O
f	O
of	O
functions	O
,	O
we	O
may	O
wish	O
to	O
select	O
that	O
1	O
]	O
'	O
from	O
f	O
(	O
if	O
it	O
exists	O
)	O
for	O
which	O
that	O
likelihood	B
product	I
is	O
maximal	O
.	O
more	O
formally	O
,	O
we	O
select	O
1	O
]	O
'	O
so	O
that	O
the	O
logarithm	O
is	O
maximal	O
.	O
if	O
the	O
family	O
f	O
is	O
too	O
rich	O
,	O
this	O
will	O
overfit	O
,	O
and	O
consequently	O
,	O
the	O
selected	O
function	O
1	O
]	O
n	O
has	O
a	O
probability	O
of	O
error	O
that	O
does	O
not	O
tend	O
to	O
l	O
*	O
.	O
for	O
convenience	O
,	O
we	O
assume	O
here	O
that	O
there	O
exists	O
an	O
element	O
of	O
f	O
maximizing	O
.en	O
.	O
we	O
do	O
not	O
assume	O
here	O
that	O
the	O
class	O
f	O
is	O
very	O
small	O
.	O
classes	O
in	O
which	O
each	O
1	O
]	O
'	O
in	O
f	O
is	O
known	O
up	O
to	O
one	O
or	O
a	O
few	O
parameters	O
are	O
loosely	O
called	O
parametric	O
.	O
sometimes	O
f	O
is	O
defined	O
via	O
a	O
generic	O
description	O
such	O
as	O
:	O
f	O
is	O
the	O
class	O
of	O
all	O
1	O
]	O
'	O
:	O
nd	O
-+	O
[	O
0	O
,	O
1	O
]	O
that	O
are	O
lipschitz	O
with	O
constant	O
c.	O
such	O
classes	O
are	O
called	O
nonparametric	O
.	O
in	O
certain	O
cases	O
,	O
the	O
boundary	O
between	O
parametric	O
and	O
nonparametric	O
is	O
unclear	O
.	O
we	O
will	O
be	O
occupied	O
with	O
the	O
consistency	B
question	O
:	O
does	O
l	O
(	O
1	O
]	O
n	O
)	O
-+	O
inf17'ef	O
l	O
(	O
1	O
]	O
'	O
)	O
with	O
probability	O
one	O
for	O
all	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
?	O
(	O
here	O
l	O
(	O
1	O
]	O
'	O
)	O
=	O
p	O
{	O
i	O
{	O
17	O
'	O
(	O
x	O
»	O
1/2	O
}	O
i	O
y	O
}	O
is	O
15.2	O
the	O
maximum	B
likelihood	I
method	O
:	O
regression	B
format	I
251	O
the	O
probability	O
of	O
error	O
of	O
the	O
natural	O
rule	O
that	O
corresponds	O
to	O
rj	O
'	O
.	O
)	O
if	O
,	O
additionally	O
,	O
f	O
is	O
rich	O
enough	O
or	O
our	O
prior	O
information	O
is	O
good	O
enough	O
,	O
we	O
may	O
have	O
inf	O
11	O
'	O
ef	O
l	O
(	O
rj	O
'	O
)	O
=	O
l	O
*	O
,	O
but	O
that	O
is	O
not	O
our	O
concern	O
here	O
,	O
as	O
f	O
is	O
given	O
to	O
us	O
.	O
we	O
will	O
not	O
be	O
concerned	O
with	O
the	O
computational	O
problems	O
related	O
to	O
the	O
maxi	O
(	O
cid:173	O
)	O
mization	O
of	O
ln	O
(	O
rj	O
'	O
)	O
over	O
f.	O
gradient	O
methods	O
or	O
variations	O
of	O
them	O
are	O
sometimes	O
used-refer	O
to	O
mclachlan	O
(	O
1992	O
)	O
for	O
a	O
bibliography	O
.	O
it	O
suffices	O
to	O
say	O
that	O
in	O
simple	O
cases	O
,	O
an	O
explicit	O
form	O
for	O
rjn	O
may	O
be	O
available	O
.	O
an	O
example	O
follows	O
.	O
our	O
first	O
lemma	O
illustrates	O
that	O
the	O
maximum	B
likelihood	I
method	O
should	O
only	O
be	O
used	O
when	O
the	O
true	O
(	O
but	O
unknown	O
)	O
rj	O
indeed	O
belongs	O
to	O
f.	O
recall	O
that	O
the	O
same	O
was	O
not	O
true	O
for	O
empirical	B
risk	I
minimization	I
over	O
vc	O
classes	O
(	O
see	O
chapter	O
12	O
)	O
.	O
lemma	O
15.1.	O
consider	O
the	O
class	O
f	O
with	O
two	O
functions	O
rja	O
==	O
0.45	O
,	O
rjb	O
==	O
0.95.	O
let	O
rjn	O
be	O
the	O
function	O
picked	O
by	O
the	O
maximum	B
likelihood	I
method	O
.	O
there	O
exists	O
a	O
distribution	O
for	O
(	O
x	O
,	O
y	O
)	O
with	O
rj	O
tj	O
.	O
f	O
such	O
that	O
with	O
probability	O
one	O
,	O
as	O
n	O
--	O
-+	O
00	O
,	O
l	O
(	O
rjn	O
)	O
--	O
-+	O
max	O
l	O
(	O
rj	O
'	O
)	O
>	O
min	O
l	O
(	O
rj	O
'	O
)	O
.	O
l1'ef	O
l1'ef	O
thus	O
,	O
maximum	B
likelihood	I
picks	O
the	O
wrong	O
classifier	B
.	O
proof	O
.	O
define	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
on	O
{	O
o	O
,	O
1	O
}	O
x	O
{	O
o	O
,	O
1	O
}	O
by	O
p	O
{	O
x	O
=	O
0	O
,	O
y	O
=	O
o	O
}	O
=	O
p	O
{	O
x	O
=	O
1	O
,	O
y	O
=	O
o	O
}	O
=	O
2/9	O
,	O
p	O
{	O
x	O
=	O
0	O
,	O
y	O
=	O
1	O
}	O
=	O
1/9	O
,	O
and	O
p	O
{	O
x	O
=	O
1	O
,	O
y	O
=	O
1	O
}	O
=	O
4/9	O
.	O
then	O
one	O
may	O
quickly	O
verify	O
that	O
(	O
note	O
that	O
l	O
*	O
=	O
1/3	O
,	O
but	O
this	O
is	O
irrelevant	O
here	O
.	O
)	O
within	O
f	O
,	O
rjb	O
is	O
the	O
better	O
for	O
our	O
distribution	B
.	O
by	O
the	O
strong	B
law	O
of	O
large	O
numbers	O
,	O
we	O
have	O
with	O
probability	O
one	O
(	O
and	O
similarly	O
for	O
rjb	O
)	O
.	O
if	O
one	O
works	O
out	O
the	O
values	O
,	O
it	O
is	O
seen	O
that	O
with	O
probability	O
one	O
,	O
rjn	O
==	O
rja	O
for	O
all	O
n	O
large	O
enough	O
.	O
hence	O
,	O
l	O
(	O
rjn	O
)	O
--	O
-+	O
maxl1'ef	O
l	O
(	O
rj	O
'	O
)	O
with	O
probability	O
one	O
.	O
0	O
remark	O
.	O
besides	O
the	O
clear	O
theoretical	B
hazard	O
of	O
not	O
capturing	O
rj	O
in	O
f	O
,	O
the	O
maximum	B
likelihood	I
method	O
runs	O
into	O
a	O
practical	O
problem	O
with	O
``	O
infinity	O
.	O
''	O
for	O
example	O
,	O
take	O
:	O
f	O
=	O
{	O
rja	O
==	O
0	O
,	O
rjb	O
==	O
1	O
}	O
,	O
and	O
assume	O
rj	O
==	O
1/3	O
.	O
for	O
all	O
n	O
large	O
enough	O
,	O
both	O
classes	O
are	O
represented	O
in	O
the	O
data	O
sample	O
with	O
probability	O
one	O
.	O
this	O
implies	O
that	O
ln	O
(	O
rja	O
)	O
=	O
ln	O
(	O
rjb	O
)	O
=	O
-00.	O
the	O
maximum	B
likelihood	I
estimate	O
rjn	O
is	O
ill-defined	O
,	O
while	O
any	O
reasonable	O
rule	B
should	O
quickly	O
be	O
able	O
to	O
pick	O
rja	O
over	O
rjb	O
.	O
0	O
the	O
lemma	O
shows	O
that	O
when	O
rj	O
tj	O
.	O
f	O
,	O
the	O
maximum	B
likelihood	I
method	O
is	O
not	O
even	O
capable	O
of	O
selecting	O
one	O
of	O
two	O
choices	O
.	O
the	O
situation	O
changes	O
dramatically	O
when	O
rj	O
e	O
f.	O
for	O
finite	O
classes	O
f	O
,	O
nothing	O
can	O
go	O
wrong	O
.	O
noting	O
that	O
whenever	O
rj	O
e	O
f	O
,	O
we	O
have	O
l	O
*	O
=	O
infl1'ef	O
l	O
(	O
rj	O
'	O
)	O
,	O
we	O
may	O
now	O
expect	O
that	O
l	O
(	O
rjn	O
)	O
--	O
-+	O
l	O
*	O
in	O
probability	O
,	O
or	O
with	O
probability	O
one	O
.	O
252	O
15.	O
the	O
maximum	B
likelihood	I
principle	O
theorem	B
15.1.	O
lflfi	O
=	O
k	O
<	O
00	O
,	O
and	O
1	O
]	O
e	O
f	O
,	O
then	O
the	O
maximum	B
likelihood	I
method	O
is	O
consistent	O
,	O
that	O
is	O
,	O
l	O
(	O
1	O
]	O
n	O
)	O
-+	O
l	O
*	O
in	O
probability	O
.	O
proof	O
.	O
for	O
a	O
fixed	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
we	O
rank	O
the	O
members	O
1	O
]	O
(	O
1	O
)	O
,	O
...	O
,	O
1	O
]	O
(	O
k	O
)	O
of	O
f	O
by	O
increasing	O
values	O
of	O
l	O
(	O
1	O
]	O
(	O
i	O
»	O
)	O
.	O
put	O
1	O
]	O
(	O
1	O
)	O
==	O
1	O
]	O
.	O
let	O
io	O
be	O
the	O
largest	O
index	O
for	O
which	O
l	O
(	O
1	O
]	O
(	O
i	O
»	O
)	O
=	O
l	O
*	O
.	O
let	O
1	O
]	O
n	O
be	O
the	O
maximum	B
likelihood	I
choice	O
from	O
f.	O
for	O
any	O
a	O
,	O
we	O
have	O
p	O
{	O
l	O
(	O
1	O
]	O
n	O
)	O
=i	O
l	O
*	O
}	O
<	O
p	O
{	O
ln	O
(	O
1	O
]	O
(	O
l	O
»	O
)	O
:	O
:	O
:	O
rp.~x	O
ln	O
(	O
1j	O
(	O
i	O
»	O
)	O
}	O
1	O
>	O
10	O
<	O
p	O
{	O
ln	O
(	O
1	O
]	O
(	O
l	O
»	O
)	O
:	O
:	O
:	O
a	O
}	O
+	O
lp	O
{	O
ln	O
(	O
1	O
]	O
(	O
i	O
»	O
)	O
~	O
a	O
}	O
.	O
i	O
>	O
io	O
define	O
the	O
entropy	B
of	O
(	O
x	O
,	O
y	O
)	O
by	O
£	O
=	O
£	O
(	O
1	O
]	O
)	O
=	O
-e	O
{	O
1	O
]	O
(	O
x	O
)	O
log	O
1	O
]	O
(	O
x	O
)	O
+	O
(	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
)	O
log	O
(	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
)	O
}	O
(	O
see	O
chapter	O
3	O
)	O
.	O
recall	O
that	O
°	O
:	O
:	O
:	O
£	O
:	O
:	O
:	O
log	O
2.	O
we	O
also	O
need	O
the	O
negative	O
divergences	O
di	O
=	O
e	O
'fj	O
(	O
x	O
)	O
log	O
-	O
-	O
+	O
(	O
1	O
-	O
{	O
1	O
]	O
(	O
i	O
)	O
(	O
x	O
)	O
1	O
]	O
(	O
x	O
)	O
'fj	O
(	O
x	O
)	O
)	O
log	O
1	O
-	O
1	O
]	O
(	O
i	O
)	O
(	O
x	O
)	O
i	O
,	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
which	O
are	O
easily	O
seen	O
to	O
be	O
nonpositive	O
for	O
all	O
i	O
(	O
by	O
jensen	O
's	O
inequality	B
)	O
.	O
further	O
(	O
cid:173	O
)	O
more	O
,	O
di	O
=	O
°	O
if	O
and	O
only	O
if	O
1	O
]	O
(	O
i	O
)	O
(	O
x	O
)	O
=	O
1	O
]	O
(	O
x	O
)	O
with	O
probability	O
one	O
.	O
observe	O
that	O
for	O
i	O
>	O
io	O
,	O
we	O
can	O
not	O
have	O
this	O
.	O
let	O
e	O
=	O
maxi	O
>	O
io	O
di	O
.	O
(	O
if	O
io	O
=	O
k	O
,	O
this	O
set	O
is	O
empty	O
,	O
but	O
then	O
the	O
theorem	B
is	O
trivially	O
true	O
.	O
)	O
it	O
is	O
advantageous	O
to	O
take	O
a	O
=	O
-£	O
+	O
e	O
/2	O
.	O
observe	O
that	O
e	O
ln	O
(	O
1	O
]	O
{	O
(	O
i	O
)	O
}	O
_	O
)	O
-	O
-£	O
+	O
di	O
{	O
=	O
-£	O
c	O
e	O
·f·	O
if	O
i	O
=	O
1	O
.	O
1	O
1	O
>	O
lo	O
.	O
:	O
:	O
:	O
-0	O
+	O
thus	O
,	O
p	O
{	O
l	O
(	O
ryn	O
)	O
i	O
l	O
*	O
)	O
:	O
'0	O
p	O
{	O
ln	O
(	O
ry	O
(	O
i	O
)	O
)	O
:	O
'0	O
-e	O
+	O
~	O
}	O
+	O
l	O
p	O
{	O
ln	O
(	O
ry	O
(	O
i	O
)	O
)	O
:	O
:	O
:	O
-e	O
+	O
n	O
.	O
1	O
>	O
10	O
by	O
the	O
law	O
of	O
large	O
numbers	O
,	O
we	O
see	O
that	O
both	O
terms	O
converge	O
to	O
zero	O
.	O
note	O
,	O
in	O
particular	O
,	O
that	O
it	O
is	O
true	O
even	O
if	O
for	O
some	O
i	O
>	O
io	O
,	O
di	O
=	O
-00.	O
d	O
for	O
infinite	O
classes	O
,	O
many	O
things	O
can	O
go	O
wrong	O
.	O
assume	O
that	O
f	O
is	O
the	O
class	O
of	O
all	O
1	O
]	O
'	O
:	O
r2	O
-+	O
[	O
0	O
,	O
1	O
]	O
with	O
11	O
'	O
=	O
fa	O
and	O
a	O
is	O
a	O
convex	O
set	O
containing	O
the	O
origin	O
.	O
pick	O
x	O
uniformly	O
on	O
the	O
perimeter	O
of	O
the	O
unit	O
circle	O
.	O
then	O
the	O
maximum	B
likelihood	I
estimate	O
'fjn	O
matches	O
the	O
data	O
,	O
as	O
we	O
may	O
always	O
find	O
a	O
closed	O
polygon	O
pn	O
with	O
15.3	O
consistency	B
253	O
vertices	O
at	O
the	O
xi	O
's	O
with	O
yi	O
=	O
1.	O
for	O
r	O
;	O
n	O
=	O
lpn	O
'	O
we	O
have	O
ln	O
(	O
r	O
;	O
n	O
)	O
=	O
°	O
(	O
its	O
maximal	O
value	O
)	O
,	O
yet	O
l	O
(	O
r	O
;	O
n	O
)	O
=	O
p	O
{	O
y	O
=	O
i	O
}	O
=	O
p	O
and	O
l*	O
=	O
0.	O
the	O
class	O
f	O
is	O
plainly	O
too	O
rich	O
.	O
for	O
distributions	O
of	O
x	O
on	O
the	O
positive	O
integers	O
,	O
maximum	B
likelihood	I
does	O
not	O
behave	O
as	O
poorly	O
,	O
even	O
though	O
it	O
must	O
pick	O
among	O
infinitely	O
many	O
possibilities	O
.	O
assume	O
f	O
is	O
the	O
class	O
of	O
all	O
r	O
;	O
'	O
,	O
but	O
we	O
know	O
that	O
x	O
puts	O
all	O
its	O
mass	O
on	O
the	O
positive	O
integers	O
.	O
then	O
maximum	B
likelihood	I
tries	O
to	O
maximize	O
(	O
x	O
)	O
n	O
(	O
r	O
;	O
'	O
(	O
i	O
)	O
)	O
nl	O
,	O
i	O
(	O
1	O
-	O
i=l	O
r	O
;	O
'	O
(	O
i	O
)	O
)	O
no	O
,	O
i	O
over	O
r	O
;	O
'	O
e	O
f	O
,	O
where	O
nl	O
,	O
i	O
=	O
l~=l	O
i	O
{	O
xj=i	O
,	O
yj=l	O
)	O
and	O
no	O
,	O
i	O
=	O
l~=l	O
i	O
{	O
xj=i'yj=o	O
}	O
.	O
the	O
maximization	O
is	O
to	O
be	O
done	O
over	O
all	O
(	O
r	O
;	O
'	O
(	O
i	O
)	O
,	O
r	O
;	O
'	O
(	O
2	O
)	O
,	O
...	O
)	O
from	O
®~l	O
[	O
0	O
,	O
1	O
]	O
.	O
fortunately	O
,	O
thus	O
,	O
if	O
no	O
,	O
i	O
+	O
n1	O
,	O
i	O
=	O
0	O
,	O
we	O
set	O
r	O
;	O
n	O
(	O
i	O
)	O
=	O
°	O
(	O
arbitrarily	O
)	O
,	O
while	O
if	O
no	O
,	O
i	O
+	O
nl	O
,	O
i	O
>	O
0	O
,	O
we	O
this	O
is	O
turned	O
into	O
a	O
maximization	O
for	O
each	O
individual	O
i	O
-usually	O
we	O
are	O
not	O
so	O
lucky	O
.	O
pick	O
r	O
;	O
n	O
(	O
i	O
)	O
as	O
the	O
value	O
u	O
that	O
maximizes	O
nl	O
,	O
i	O
log	O
u	O
+	O
no	O
,	O
i	O
10g	O
(	O
1	O
-	O
u	O
)	O
.	O
setting	O
the	O
derivative	O
with	O
respect	O
to	O
u	O
equal	O
to	O
zero	O
shows	O
that	O
(	O
.	O
)	O
nl	O
,	O
i	O
r	O
;	O
n	O
l	O
=	O
-	O
-	O
-	O
-	O
nl	O
,	O
i	O
+	O
no	O
,	O
i	O
in	O
other	O
words	O
,	O
r	O
;	O
n	O
is	O
the	O
familiar	O
histogram	O
estimate	O
with	O
bin	O
width	O
less	O
than	O
one	O
half	O
.	O
it	O
is	O
known	O
to	O
be	O
universally	O
consistent	O
(	O
see	O
theorem	B
6.2	O
)	O
.	O
thus	O
,	O
maximum	B
likelihood	I
may	O
work	O
for	O
large	O
f	O
if	O
we	O
restrict	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
a	O
bit	O
.	O
15.3	O
consistency	B
finally	O
,	O
we	O
are	O
ready	O
for	O
the	O
main	O
consistency	B
result	O
for	O
r	O
;	O
n	O
when	O
f	O
may	O
have	O
infinitely	O
many	O
elements	O
.	O
the	O
conditions	O
of	O
the	O
theorem	B
involve	O
the	O
bracketing	B
metric	I
entropy	I
of	O
f	O
,	O
defined	O
as	O
follows	O
:	O
for	O
every	O
distribution	B
of	O
x	O
,	O
and	O
e	O
>	O
0	O
,	O
let	O
fx	O
,	O
e	O
be	O
a	O
set	O
of	O
functions	O
such	O
that	O
for	O
each	O
r	O
;	O
'	O
e	O
f	O
,	O
there	O
exist	O
functions	O
r	O
;	O
~	O
,	O
r	O
;	O
~	O
e	O
fx	O
,	O
e	O
such	O
that	O
for	O
all	O
x	O
end	O
and	O
r	O
;	O
~	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
,	O
l	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
r	O
;	O
~	O
(	O
x	O
)	O
,	O
e	O
{	O
r	O
;	O
~	O
(	O
x	O
)	O
-	O
r	O
;	O
~	O
(	O
x	O
)	O
}	O
:	O
:	O
:	O
e.	O
that	O
is	O
,	O
every	O
r	O
;	O
'	O
e	O
f	O
is	O
``	O
bracketed	O
''	O
between	O
two	O
members	O
of	O
fx	O
,	O
e	O
whose	O
l1	O
(	O
il	O
)	O
distance	B
is	O
not	O
larger	O
than	O
e.	O
let	O
n	O
(	O
x	O
,	O
e	O
)	O
denote	O
the	O
cardinality	O
of	O
the	O
smallest	O
such	O
fx	O
,	O
e	O
'	O
if	O
n	O
(	O
x	O
,	O
e	O
)	O
<	O
00	O
,	O
log	O
n	O
(	O
x	O
,	O
e	O
)	O
is	O
called	O
the	O
bracketing	O
e	O
-entropy	O
of	O
f	O
,	O
corresponding	O
to	O
x	O
.	O
254	O
15.	O
the	O
maximum	B
likelihood	I
principle	O
theorem	B
15.2.	O
let	O
f	O
be	O
a	O
class	O
of	O
regression	O
functions	O
n	O
d	O
~	O
[	O
0	O
,	O
1	O
]	O
.	O
assume	O
that	O
for	O
every	O
distribution	B
of	O
x	O
and	O
e	O
>	O
0	O
,	O
n	O
(	O
x	O
,	O
e	O
)	O
<	O
00.	O
then	O
the	O
maximum	B
likelihood	I
choice	O
rjn	O
satisfies	O
lim	O
l	O
(	O
1	O
]	O
n	O
)	O
=	O
l	O
*	O
n	O
--	O
+oo	O
in	O
probability	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
with	O
rj	O
e	O
f.	O
thus	O
,	O
consistency	B
is	O
guaranteed	O
if	O
f	O
has	O
a	O
finite	O
bracketing	O
e	O
-entropy	O
for	O
every	O
x	O
and	O
e.	O
we	O
provide	O
examples	O
in	O
the	O
next	O
section	O
.	O
for	O
the	O
proof	O
,	O
first	O
we	O
need	O
a	O
simple	O
corollary	O
of	O
lemma	O
3.2	O
:	O
corollary	O
15.1.	O
letrj	O
,	O
rj	O
'	O
:	O
nd	O
~	O
[	O
0	O
,	O
1	O
]	O
,	O
and	O
let	O
(	O
x	O
,	O
y	O
)	O
beanndx	O
{	O
o	O
,	O
1	O
}	O
-valued	O
random	O
variable	B
pair	O
with	O
p	O
{	O
y	O
=	O
llx	O
=	O
x	O
}	O
=	O
rj	O
(	O
x	O
)	O
.	O
define	O
l	O
*	O
=	O
e	O
{	O
min	O
(	O
rj	O
(	O
x	O
)	O
,	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
)	O
}	O
,	O
l	O
(	O
rj	O
'	O
)	O
=	O
p	O
{	O
i	O
{	O
ryl	O
(	O
x	O
»	O
1/2l	O
=i	O
y	O
}	O
.	O
then	O
e	O
{	O
)	O
rj	O
'	O
(	O
x	O
)	O
rj	O
(	O
x	O
)	O
+	O
)	O
(	O
1	O
-	O
rj	O
'	O
(	O
x	O
)	O
)	O
(	O
1	O
-	O
rj	O
(	O
x	O
)	O
)	O
}	O
<	O
(	O
2elrj	O
(	O
x	O
)	O
-	O
rj	O
'	O
(	O
x	O
)	O
i	O
)	O
2	O
1	O
-	O
--	O
'	O
--	O
--	O
--	O
--	O
--	O
'	O
--	O
-	O
8	O
<	O
1	O
-	O
~	O
(	O
l	O
(	O
rj	O
'	O
)	O
-	O
l	O
*	O
)	O
2	O
.	O
8	O
proof	O
.	O
the	O
first	O
inequality	B
follows	O
by	O
lemma	O
3.2	O
,	O
and	O
the	O
second	O
by	O
theorem	B
2.2.0	O
now	O
,	O
we	O
are	O
ready	O
to	O
prove	O
the	O
theorem	B
.	O
some	O
of	O
the	O
ideas	O
used	O
here	O
appear	O
in	O
wong	O
and	O
shen	O
(	O
1992	O
)	O
.	O
proof	O
of	O
theorem	O
15.2.	O
look	O
again	O
at	O
the	O
proof	O
for	O
the	O
case	O
i	O
fi	O
<	O
00	O
(	O
theorem	B
15.1	O
)	O
.	O
define	O
e	O
as	O
there	O
.	O
let	O
f	O
(	O
e	O
)	O
be	O
the	O
collection	O
of	O
those	O
rj	O
'	O
e	O
f	O
with	O
l	O
(	O
rj	O
'	O
)	O
>	O
l	O
*	O
+	O
e	O
,	O
recalling	O
that	O
l	O
(	O
rj	O
'	O
)	O
=	O
p	O
{	O
i	O
{	O
ryl	O
(	O
x	O
»	O
1/2l	O
=i	O
y	O
}	O
.	O
for	O
every	O
a	O
,	O
for	O
reasons	O
that	O
will	O
be	O
obvious	O
later	O
,	O
we	O
take	O
a	O
=	O
-e	O
-	O
e2/	O
16.	O
thus	O
,	O
15.3	O
consistency	B
255	O
noting	O
that	O
e	O
{	O
ln	O
(	O
ry	O
)	O
}	O
=	O
-£	O
,	O
the	O
law	O
of	O
large	O
numbers	O
implies	O
that	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
converges	O
to	O
zero	O
for	O
every	O
e.	O
(	O
see	O
problem	O
15.4	O
for	O
more	O
information	O
.	O
)	O
next	O
we	O
bound	O
the	O
second	O
term	O
.	O
for	O
a	O
fixed	O
distribution	B
,	O
let	O
fx,8	O
be	O
the	O
smallest	O
set	O
of	O
functions	O
such	O
that	O
for	O
each	O
ry	O
'	O
e	O
f	O
,	O
there	O
exist	O
ry~	O
,	O
ry~	O
e	O
fx,8	O
with	O
ry~	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
ry	O
'	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
ry~	O
(	O
x	O
)	O
,	O
x	O
end	O
and	O
e	O
{	O
ry~	O
(	O
x	O
)	O
-	O
ry~	O
(	O
x	O
)	O
}	O
:	O
:	O
:	O
:	O
8	O
,	O
where	O
8	O
>	O
0	O
will	O
be	O
specified	O
later	O
.	O
by	O
assumption	O
,	O
n	O
(	O
x	O
,	O
8	O
)	O
=	O
ifx,81	O
<	O
00.	O
we	O
have	O
,	O
ry~	O
(	O
xi	O
)	O
)	O
l-y	O
;	O
,	O
l-y	O
~	O
e	O
ry	O
(	O
xi	O
)	O
,	O
(	O
1	O
-	O
ry	O
(	O
xi	O
)	O
)	O
-ne2/8	O
}	O
y	O
:	O
:	O
:	O
:	O
p	O
sup	O
ry'ef	O
(	O
e	O
)	O
i=l	O
{	O
nn	O
ry~	O
(	O
xjy	O
;	O
(	O
1	O
:	O
:	O
:	O
:	O
n	O
2	O
(	O
x,8	O
)	O
sup	O
p	O
n	O
ryu	O
ry'ef	O
(	O
e	O
)	O
{	O
i=l	O
(	O
where	O
ry~	O
,	O
ry~	O
e	O
f	O
x	O
,8	O
,	O
ry~	O
:	O
:	O
:	O
:	O
ry	O
'	O
:	O
:	O
:	O
:	O
ry~	O
and	O
e	O
{	O
ry~	O
(	O
x	O
)	O
-	O
ry~	O
(	O
x	O
)	O
}	O
<	O
8	O
)	O
n	O
'	O
(	O
x	O
)	O
y	O
(	O
1	O
i	O
y	O
'	O
-	O
'	O
(	O
x	O
)	O
)	O
l-y	O
ryl	O
}	O
i	O
l-y	O
,	O
~	O
e-ne2/8	O
ry	O
(	O
xi	O
)	O
i	O
(	O
1-ry	O
(	O
xi	O
)	O
)	O
i	O
(	O
by	O
the	O
union	O
bound	O
)	O
n	O
'	O
(	O
x	O
,	O
8	O
)	O
``	O
~u	O
:	O
'	O
)	O
p	O
{	O
o	O
x	O
sup	O
(	O
e	O
{	O
jry~	O
(	O
x	O
)	O
ry	O
(	O
x	O
)	O
+	O
j	O
(	O
1	O
-	O
ry~	O
(	O
x	O
)	O
)	O
(	O
1	O
_	O
ry	O
(	O
x	O
)	O
)	O
}	O
)	O
n	O
ene2/16	O
,	O
:	O
:	O
:	O
:	O
n2	O
(	O
x	O
,	O
8	O
)	O
ry'ef	O
(	O
e	O
)	O
256	O
15.	O
the	O
maximum	B
likelihood	I
principle	O
where	O
in	O
the	O
last	O
step	O
we	O
used	O
markov	O
's	O
inequality	B
and	O
independence	O
.	O
we	O
now	O
find	O
a	O
good	O
bound	O
for	O
e	O
{	O
j	O
ry~	O
(	O
x	O
)	O
ry	O
(	O
x	O
)	O
+	O
j	O
(	O
l	O
-	O
ry~	O
(	O
x	O
»	O
(	O
1	O
-	O
ry	O
(	O
x	O
»	O
}	O
for	O
each	O
1	O
]	O
'	O
e	O
fee	O
)	O
as	O
follows	O
:	O
e	O
{	O
j	O
ry	O
;	O
/x	O
)	O
ry	O
(	O
x	O
)	O
+	O
j	O
(	O
l	O
-	O
ry~	O
(	O
x	O
»	O
(	O
l	O
-	O
ry	O
(	O
x	O
»	O
}	O
:	O
s	O
e	O
{	O
j1	O
]	O
'	O
(	O
x	O
)	O
1	O
]	O
(	O
x	O
)	O
+	O
.j	O
(	O
l-	O
1	O
]	O
'	O
(	O
x	O
»	O
(	O
l	O
-	O
1	O
]	O
(	O
x	O
»	O
+	O
j	O
ry	O
(	O
x	O
)	O
(	O
ry~	O
(	O
x	O
)	O
-	O
ry	O
'	O
(	O
x	O
»	O
)	O
+	O
j	O
(	O
1	O
-	O
ry	O
(	O
x	O
»	O
(	O
ry	O
'	O
(	O
x	O
)	O
-	O
ry~	O
(	O
x	O
»	O
)	O
}	O
(	O
here	O
we	O
used	O
ra+b	O
:	O
s	O
fa	O
+	O
jb	O
)	O
:	O
s	O
e	O
{	O
j	O
1	O
]	O
'	O
(	O
x	O
)	O
1	O
]	O
(	O
x	O
)	O
+	O
.j	O
(	O
l	O
-	O
1	O
]	O
'	O
(	O
x	O
»	O
(	O
l	O
-	O
1	O
]	O
(	O
x	O
»	O
}	O
+	O
.je	O
{	O
1	O
]	O
(	O
x	O
)	O
}	O
je	O
{	O
1	O
]	O
~	O
(	O
x	O
)	O
-	O
1	O
]	O
'	O
(	O
x	O
)	O
}	O
+.je	O
{	O
1	O
-1	O
]	O
(	O
x	O
)	O
}	O
je	O
{	O
1	O
]	O
'	O
(	O
x	O
)	O
-1	O
]	O
~	O
(	O
x	O
)	O
}	O
(	O
by	O
the	O
cauchy-schwarz	O
inequality	B
)	O
:	O
s	O
e	O
{	O
j1	O
]	O
'	O
(	O
x	O
)	O
1	O
]	O
(	O
x	O
)	O
+	O
.j	O
(	O
l	O
-	O
1	O
]	O
'	O
(	O
x	O
»	O
(	O
l	O
-	O
1	O
]	O
(	O
x	O
»	O
}	O
+	O
2.ji	O
:	O
s	O
1	O
-	O
e2	O
-	O
+	O
2.ji	O
8	O
(	O
by	O
corollary	O
15.1	O
)	O
.	O
summarizing	O
,	O
we	O
obtain	O
ry'ef	O
(	O
e	O
)	O
8	O
p	O
{	O
sup	O
ln	O
(	O
1	O
]	O
'	O
)	O
-	O
ln	O
(	O
1	O
]	O
)	O
:	O
:	O
:	O
:	O
_	O
e2	O
}	O
:	O
s	O
:	O
n	O
'	O
(	O
x	O
,	O
0	O
)	O
(	O
1	O
-	O
~	O
+	O
2j8	O
r	O
e	O
''	O
''	O
/16	O
by	O
taking	O
8	O
<	O
(	O
~	O
)	O
2	O
,	O
we	O
see	O
that	O
the	O
probability	O
converges	O
to	O
zero	O
exponentially	O
rapidly	O
,	O
which	O
concludes	O
the	O
proof	O
.	O
0	O
15.4	O
examples	O
in	O
this	O
section	O
we	O
show	O
by	O
example	O
how	O
to	O
apply	O
the	O
previous	O
consistency	B
results	O
.	O
in	O
all	O
cases	O
,	O
we	O
assume	O
that	O
1	O
]	O
e	O
f	O
and	O
we	O
are	O
concerned	O
with	O
the	O
weak	B
convergence	O
15.4	O
examples	O
257	O
of	O
lcr	O
}	O
,	O
j	O
to	O
l	O
*	O
for	O
all	O
such	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
.	O
the	O
classes	O
are	O
as	O
follows	O
:	O
{	O
ry	O
=	O
/	O
[	O
a	O
,	O
b	O
]	O
,	O
-00	O
:	O
:	O
:	O
:	O
a	O
:	O
:	O
:	O
:	O
b	O
:	O
:	O
:	O
:	O
oo	O
}	O
{	O
ry	O
=	O
c/	O
[	O
a	O
,	O
b	O
]	O
'	O
c	O
e	O
[	O
0	O
,	O
1	O
]	O
,	O
-00	O
~	O
a	O
:	O
:	O
:	O
:	O
b	O
:	O
:	O
:	O
:	O
oo	O
}	O
{	O
ry	O
=	O
/	O
[	O
aj	O
,	O
bilx	O
''	O
,	O
x	O
[	O
ad	O
,	O
bd	O
]	O
,	O
-00	O
:	O
:	O
:	O
:	O
ai	O
:	O
:	O
:	O
:	O
bi	O
:	O
:	O
:	O
:	O
00	O
,	O
1	O
:	O
:	O
:	O
:	O
i	O
:	O
:	O
:	O
:	O
d	O
}	O
fl	O
=	O
f	O
{	O
f2	O
f3	O
=	O
{	O
ry	O
:	O
ry	O
(	O
x	O
)	O
=	O
cx/	O
(	O
l	O
+cx	O
)	O
:	O
x	O
:	O
:	O
:	O
:	O
0	O
,	O
c	O
:	O
:	O
:	O
:	O
o	O
}	O
{	O
ry	O
is	O
monotone	O
decreasing	O
on	O
[	O
0	O
,	O
i	O
]	O
}	O
f5	O
=	O
{	O
ry	O
:	O
ry	O
(	O
x	O
)	O
=	O
{	O
ry	O
:	O
ry	O
(	O
x	O
)	O
=	O
sin2	O
(	O
ex	O
)	O
,	O
e	O
e	O
r	O
}	O
1	O
+	O
ilx	O
-	O
m	O
1	O
,	O
m	O
e	O
r	O
d	O
}	O
ry	O
:	O
ry	O
(	O
x	O
)	O
=	O
eao+o	O
?	O
x	O
t	O
1	O
+	O
eao+a	O
x	O
{	O
}	O
,	O
ao	O
e	O
r	O
,	O
x	O
,	O
a	O
e	O
rd	O
.	O
these	O
are	O
all	O
rather	O
simple	O
,	O
yet	O
they	O
will	O
illustrate	O
various	O
points	O
.	O
of	O
these	O
classes	O
,	O
f4	O
is	O
nonparametric	O
;	O
yet	O
,	O
it	O
behaves	O
``	O
better	O
''	O
than	O
the	O
one-parameter	O
class	O
f6	O
,	O
for	O
example	O
.	O
we	O
emphasize	O
again	O
that	O
we	O
are	O
interested	O
in	O
consistency	O
for	O
all	O
distributions	O
of	O
x.	O
for	O
f	O
1	O
,	O
ryn	O
will	O
agree	O
with	O
the	O
samples	O
,	O
that	O
is	O
,	O
ryn	O
(	O
x	O
i	O
)	O
=	O
yi	O
for	O
all	O
i	O
,	O
and	O
therefore	O
,	O
ryn	O
e	O
fl	O
is	O
any	O
function	O
of	O
the	O
form	O
/	O
[	O
a	O
,	O
b	O
]	O
with	O
x	O
(	O
l-i	O
)	O
<	O
a	O
:	O
:	O
:	O
:	O
x	O
(	O
l	O
)	O
,	O
x	O
(	O
r	O
)	O
:	O
:	O
:	O
:	O
b	O
<	O
x	O
(	O
r+l	O
)	O
,	O
where	O
x	O
(	O
1	O
)	O
:	O
:	O
:	O
:	O
...	O
:	O
:	O
:	O
:	O
x	O
(	O
n	O
)	O
are	O
the	O
order	B
statistics	I
for	O
xi	O
,	O
...	O
,	O
xn	O
(	O
with	O
x	O
(	O
o	O
)	O
=	O
-00	O
,	O
x	O
(	O
n+l	O
)	O
=	O
(	O
0	O
)	O
,	O
x	O
(	O
l	O
)	O
is	O
the	O
smallest	O
data	O
point	O
with	O
y	O
(	O
l	O
)	O
=	O
1	O
,	O
and	O
x	O
(	O
r	O
)	O
is	O
the	O
largest	O
data	O
point	O
with	O
y	O
(	O
r	O
)	O
=	O
1.	O
as	O
l	O
*	O
=	O
0	O
,	O
we	O
claim	O
that	O
e	O
{	O
l	O
(	O
ryn	O
)	O
}	O
<	O
p	O
{	O
x	O
(	O
l-l	O
)	O
<	O
x	O
<	O
x	O
(	O
l	O
)	O
}	O
+p	O
{	O
x	O
(	O
r	O
)	O
<	O
x	O
<	O
x	O
(	O
r+l	O
)	O
}	O
<	O
4	O
n+1	O
the	O
rule	B
is	O
simply	O
excellent	O
,	O
and	O
has	O
universal	B
performance	O
guarantees	O
.	O
remark	O
.	O
note	O
that	O
in	O
this	O
case	O
,	O
maximum	B
likelihood	I
minimizes	O
the	O
empirical	B
risk	I
over	O
the	O
class	O
of	O
classifiers	O
c	O
=	O
{	O
¢	O
=	O
/	O
[	O
a	O
,	O
b	O
]	O
,	O
a	O
:	O
:	O
:	O
:	O
b	O
}	O
.	O
as	O
c	O
has	O
vc	B
dimension	I
vc	O
=	O
2	O
(	O
theorem	B
13.7	O
)	O
,	O
and	O
inf¢ec	O
l	O
(	O
¢	O
)	O
=	O
0	O
,	O
theorem	B
12.7	O
implies	O
that	O
for	O
all	O
e	O
>	O
0	O
,	O
p	O
{	O
l	O
(	O
ryn	O
)	O
>	O
e	O
}	O
:	O
:	O
:	O
:	O
8n	O
22-ne/2	O
,	O
and	O
that	O
e	O
{	O
l	O
(	O
ryn	O
)	O
}	O
:	O
:	O
:	O
:	O
-	O
-	O
(	O
cid:173	O
)	O
4logn	O
+4	O
nlog2	O
(	O
see	O
problem	O
12.8	O
)	O
.	O
with	O
the	O
analysis	O
given	O
here	O
,	O
we	O
have	O
gotten	O
rid	O
of	O
the	O
log	O
n	O
factor	O
.	O
0	O
258	O
15.	O
the	O
maximum	B
likelihood	I
principle	O
the	O
class	O
f	O
{	O
is	O
much	O
more	O
interesting	O
.	O
here	O
you	O
will	O
observe	O
a	O
dramatic	O
dif	O
(	O
cid:173	O
)	O
ference	O
with	O
empirical	O
risk	O
minimization	O
,	O
as	O
the	O
parameter	O
c	O
plays	O
a	O
key	O
role	O
that	O
will	O
aid	O
a	O
lot	O
in	O
the	O
selection	B
.	O
note	O
that	O
the	O
likelihood	B
product	I
is	O
zero	O
if	O
ni	O
=	O
0	O
or	O
if	O
yi	O
=	O
1	O
for	O
some	O
i	O
with	O
xi	O
tj	O
[	O
a	O
,	O
b	O
]	O
,	O
and	O
it	O
is	O
cn1	O
(	O
1	O
-	O
c	O
)	O
no	O
otherwise	O
,	O
where	O
no	O
is	O
the	O
number	O
of	O
(	O
xi	O
,	O
yi	O
)	O
pairs	O
with	O
a	O
:	O
:	O
:	O
:	O
xi	O
:	O
:	O
:	O
:	O
b	O
,	O
yi	O
=	O
0	O
,	O
and	O
ni	O
is	O
the	O
number	O
of	O
(	O
xi	O
,	O
yi	O
)	O
pairs	O
with	O
a	O
:	O
:	O
:	O
:	O
xi	O
~	O
b	O
,	O
yi	O
=	O
1.	O
for	O
fixed	O
no	O
,	O
n	O
i	O
,	O
this	O
is	O
maximal	O
when	O
c=	O
--	O
-no+ni	O
resubstitution	B
yields	O
that	O
the	O
likelihood	B
product	I
is	O
zero	O
if	O
ni	O
=	O
0	O
or	O
if	O
yi	O
=	O
1	O
for	O
some	O
i	O
with	O
xi	O
tj	O
[	O
a	O
,	O
b	O
]	O
,	O
and	O
equals	O
exp	O
{	O
niiog	O
ni	O
no	O
+	O
nl	O
+	O
no	O
log	O
no	O
no	O
+	O
ni	O
}	O
if	O
otherwise	O
.	O
thus	O
,	O
we	O
should	O
pick	O
'fln	O
=	O
ci	O
[	O
a	O
,	O
b	O
]	O
such	O
thatc	O
=	O
ni/	O
(	O
no+ni	O
)	O
'	O
and	O
[	O
a	O
,	O
b	O
]	O
maximizes	O
the	O
divergence	B
nilog	O
see	O
problem	O
15.5.	O
ni	O
no+ni	O
no	O
+	O
no	O
log	O
-	O
-	O
-	O
no+ni	O
for	O
.1'2	O
,	O
by	O
a	O
similar	O
argument	O
as	O
for	O
.1'1	O
,	O
let	O
[	O
ai	O
,	O
bd	O
x	O
``	O
.	O
x	O
[	O
ad	O
,	O
bd	O
]	O
be	O
the	O
smallest	O
rectangle	O
of	O
r	O
d	O
that	O
encloses	O
all	O
xi	O
's	O
for	O
which	O
yi	O
=	O
1.	O
then	O
,	O
we	O
know	O
that	O
'fln	O
=	O
i	O
[	O
al	O
,	O
bi1x	O
...	O
x	O
[	O
ad	O
,	O
bd	O
]	O
agrees	O
with	O
the	O
data	O
.	O
furthermore	O
,	O
l	O
*	O
=	O
0	O
,	O
and	O
e	O
{	O
l	O
(	O
7	O
]	O
n	O
)	O
}	O
~	O
-	O
4d	O
n+1	O
(	O
see	O
problem	O
15.6	O
)	O
.	O
the	O
logarithm	O
of	O
the	O
likelihood	B
product	I
for	O
.1'3	O
is	O
l	O
log	O
(	O
cxi	O
)	O
-	O
llog	O
(	O
1	O
+	O
cxi	O
)	O
·	O
n	O
i	O
:	O
yi=l	O
i=l	O
setting	O
the	O
derivative	O
with	O
respect	O
to	O
c	O
equal	O
to	O
zero	O
in	O
the	O
hope	O
of	O
obtaining	O
an	O
equation	O
that	O
must	O
be	O
satisfied	O
by	O
the	O
maximum	O
,	O
we	O
see	O
that	O
c	O
must	O
satisfy	O
ni=t~=t	O
xi	O
i=l	O
1	O
+	O
cxi	O
i=1	O
1/c	O
+	O
xi	O
unless	O
xl	O
=	O
...	O
=	O
xn	O
=	O
0	O
,	O
where	O
nl	O
=	O
l7=1	O
i	O
{	O
yi=i	O
}	O
'	O
this	O
equation	O
has	O
a	O
unique	O
solution	O
for	O
c.	O
the	O
rule	B
corresponding	O
to	O
'fln	O
is	O
of	O
the	O
form	O
g	O
(	O
x	O
)	O
=	O
i	O
{	O
cx	O
>	O
i	O
}	O
=	O
i	O
{	O
x	O
>	O
l/c	O
}	O
'	O
equivalently	O
,	O
(	O
x	O
)	O
=	O
{	O
1	O
g	O
if	O
nl	O
>	O
l7=1	O
x	O
:	O
xi	O
0	O
otherwise	O
.	O
15.4	O
examples	O
259	O
this	O
surprising	O
example	O
shows	O
that	O
we	O
do	O
not	O
even	O
have	O
to	O
know	O
the	O
parameter	O
c	O
in	O
order	O
to	O
describe	O
the	O
maximum	B
likelihood	I
rule	O
.	O
in	O
quite	O
a	O
few	O
cases	O
,	O
this	O
shortcut	O
makes	O
such	O
rules	O
very	O
appealing	O
indeed	O
.	O
furthermore	O
,	O
as	O
the	O
condition	O
of	O
theorem	O
15.2	O
is	O
fulfilled	O
,	O
1	O
]	O
e	O
:	O
f3	O
implies	O
that	O
the	O
rule	B
is	O
consistent	O
as	O
well	O
.	O
for	O
:	O
f4	O
,	O
it	O
is	O
convenient	O
to	O
order	O
the	O
xi	O
's	O
from	O
small	O
to	O
large	O
and	O
to	O
identify	O
k	O
consecutive	O
groups	O
,	O
each	O
group	O
consisting	O
of	O
any	O
number	O
of	O
xi	O
's	O
with	O
yi	O
=	O
0	O
,	O
and	O
one	O
xi	O
with	O
yi	O
=	O
1.	O
thus	O
,	O
k	O
=	O
l:7=1	O
i	O
{	O
yi=i	O
}	O
'	O
then	O
,	O
a	O
moment	O
's	O
thought	O
shows	O
that	O
l	O
]	O
n	O
must	O
be	O
piecewise	O
constant	O
,	O
taking	O
values	O
1	O
:	O
:	O
:	O
:	O
al	O
:	O
:	O
:	O
:	O
...	O
:	O
:	O
:	O
:	O
ak	O
:	O
:	O
:	O
:	O
°	O
on	O
the	O
consecutive	O
groups	O
,	O
and	O
the	O
value	O
zero	O
past	O
the	O
k-th	O
group	O
(	O
which	O
can	O
only	O
consist	O
of	O
xi	O
's	O
with	O
yi	O
=	O
0.	O
the	O
likelihood	B
product	I
thus	O
is	O
of	O
the	O
form	O
al	O
(	O
1	O
-	O
adn1a2	O
(	O
1	O
-	O
a2	O
)	O
n2	O
•••	O
ak	O
(	O
1	O
ak	O
)	O
nk	O
,	O
where	O
n	O
i	O
,	O
...	O
,	O
n	O
k	O
are	O
the	O
cardinalities	O
of	O
the	O
groups	O
minus	O
one	O
(	O
i.e.	O
,	O
the	O
number	O
of	O
yj	O
=	O
o-elements	O
in	O
the	O
i	O
-th	O
group	O
)	O
.	O
finally	O
,	O
we	O
have	O
(	O
ai	O
,	O
...	O
,	O
ak	O
)	O
=	O
argmax	O
it	O
aj	O
(	O
1	O
-	O
aj	O
)	O
n	O
j	O
k	O
l	O
:	O
:	O
:	O
:al	O
:	O
:	O
:	O
:···	O
:	O
:	O
:	O
:ak	O
:	O
:	O
:	O
:o	O
j=l	O
•	O
to	O
check	O
consistency	B
,	O
we	O
see	O
that	O
for	O
every	O
x	O
and	O
e	O
>	O
0	O
,	O
n	O
(	O
x	O
,	O
e	O
)	O
:	O
:	O
:	O
:	O
41~1	O
(	O
see	O
problem	O
15.7	O
)	O
.	O
thus	O
,	O
the	O
condition	O
of	O
theorem	O
15.2	O
is	O
satisfied	O
,	O
and	O
l	O
(	O
l	O
]	O
n	O
)	O
--	O
-+	O
l	O
*	O
in	O
probability	O
,	O
whenever	O
1	O
]	O
e	O
:	O
f4	O
.	O
in	O
:	O
fs	O
,	O
the	O
maximum	B
likelihood	I
method	O
will	O
attempt	O
to	O
place	O
m	O
at	O
the	O
center	O
of	O
the	O
highest	O
concentration	O
in	O
n	O
d	O
of	O
xi	O
's	O
with	O
yi	O
=	O
1	O
,	O
while	O
staying	O
away	O
from	O
xi	O
's	O
with	O
yi	O
=	O
0.	O
certainly	O
,	O
there	O
are	O
computational	O
problems	O
,	O
but	O
it	O
takes	O
little	O
thought	O
to	O
verify	O
that	O
the	O
conditions	O
of	O
theorem	O
15.2	O
are	O
satisfied	O
.	O
explicit	O
description	O
of	O
the	O
rule	B
is	O
not	O
necessary	O
for	O
some	O
theoretical	B
analysis	O
!	O
class	O
:	O
f6	O
is	O
a	O
simple	O
one-parameter	O
class	O
that	O
does	O
not	O
satisfy	O
the	O
condition	O
of	O
theorem	O
15.2.	O
in	O
fact	O
,	O
maximum	B
likelihood	I
fails	O
here	O
for	O
the	O
following	O
reason	O
:	O
let	O
x	O
be	O
uniform	B
on	O
[	O
0	O
,	O
1	O
]	O
.	O
then	O
the	O
likelihood	B
product	I
is	O
it	O
sin2	O
(	O
8xi	O
)	O
x	O
it	O
cos2	O
(	O
8xi	O
)	O
.	O
i	O
:	O
yi=o	O
this	O
product	B
reaches	O
a	O
degenerate	O
global	O
maximum	O
(	O
1	O
)	O
as	O
8	O
--	O
-+	O
00	O
,	O
regardless	O
of	O
the	O
true	O
(	O
unknown	O
)	O
value	O
of	O
8	O
that	O
gave	O
rise	O
to	O
the	O
data	O
.	O
see	O
problem	O
15.9.	O
class	O
:	O
f7	O
is	O
used	O
in	O
the	O
popular	O
logistic	B
discrimination	I
problem	O
,	O
reviewed	O
and	O
studied	O
by	O
anderson	O
(	O
1982	O
)	O
,	O
see	O
also	O
mclachlan	O
(	O
1992	O
,	O
chapter	O
8	O
)	O
.	O
it	O
is	O
particu	O
(	O
cid:173	O
)	O
larly	O
important	O
to	O
observe	O
that	O
with	O
this	O
model	O
,	O
260	O
15.	O
the	O
maximum	B
likelihood	I
principle	O
where	O
f3	O
=	O
-cio	O
-	O
log	O
2.	O
thus	O
,	O
:	O
f7	O
subsumes	O
linear	O
discriminants	O
.	O
it	O
does	O
also	O
force	O
a	O
bit	O
more	O
structure	O
on	O
the	O
problem	O
,	O
making	O
rj	O
approach	O
zero	O
or	O
one	O
as	O
we	O
move	O
away	O
from	O
the	O
separating	O
hyperplane	B
.	O
day	O
and	O
kerridge	O
(	O
1967	O
)	O
point	O
out	O
that	O
model	O
:	O
f7	O
is	O
appropriate	O
if	O
the	O
class-conditional	B
densities	O
take	O
the	O
form	O
cf	O
(	O
x	O
)	O
exp	O
{	O
-~	O
(	O
x	O
-	O
m	O
)	O
te-	O
'	O
(	O
x	O
-	O
m	O
)	O
}	O
,	O
where	O
c	O
is	O
a	O
normalizing	O
constant	O
,	O
f	O
is	O
a	O
density	O
,	O
m	O
is	O
a	O
vector	O
,	O
and	O
b	O
is	O
a	O
positive	O
definite	O
matrix	O
;	O
f	O
and	O
:	O
e	O
must	O
be	O
the	O
same	O
for	O
the	O
two	O
densities	O
,	O
but	O
c	O
and	O
m	O
may	O
be	O
different	O
.	O
unfortunately	O
,	O
obtaining	O
the	O
best	O
values	O
for	O
cio	O
and	O
ci	O
by	O
maximum	B
likelihood	I
takes	O
a	O
serious	O
computational	O
effort	O
.	O
had	O
we	O
tried	O
to	O
estimate	B
f	O
,	O
m	O
,	O
and	O
:	O
e	O
in	O
the	O
last	O
example	O
,	O
we	O
would	O
have	O
done	O
more	O
than	O
what	O
is	O
needed	O
,	O
as	O
both	O
f	O
and	O
b	O
drop	O
out	O
of	O
the	O
picture	O
.	O
in	O
this	O
respect	O
,	O
the	O
regression	B
format	I
is	O
both	O
parsimonious	O
and	O
lightweight	O
.	O
15.5	O
classical	O
maximum	B
likelihood	I
:	O
distribution	B
format	I
in	O
a	O
more	O
classical	O
approach	O
,	O
we	O
assume	O
that	O
given	O
y	O
=	O
1	O
,	O
x	O
has	O
a	O
density	O
ii	O
,	O
and	O
given	O
y	O
=	O
0	O
,	O
x	O
has	O
a	O
density	O
fo	O
,	O
where	O
both	O
fo	O
and	O
ii	O
belong	O
to	O
a	O
given	O
family	O
:	O
f	O
of	O
densities	O
.	O
a	O
similar	O
setup	O
may	O
be	O
used	O
for	O
atomic	O
distributions	O
,	O
but	O
this	O
will	O
not	O
add	O
anything	O
new	O
here	O
,	O
and	O
is	O
rather	O
routine	O
.	O
the	O
likelihood	B
product	I
for	O
the	O
data	O
(	O
xl	O
,	O
yd	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
is	O
n	O
n	O
(	O
pfl	O
(	O
xi	O
)	O
)	O
y	O
;	O
(	O
(	O
1	O
-	O
p	O
)	O
fo	O
(	O
xi	O
)	O
)	O
l-yi	O
,	O
i=l	O
where	O
p	O
=	O
p	O
{	O
y	O
=	O
i	O
}	O
is	O
assumed	O
to	O
be	O
unknown	O
as	O
well	O
.	O
the	O
maximum	B
likelihood	I
choices	O
for	O
p	O
,	O
fo	O
,	O
fl	O
are	O
given	O
by	O
(	O
p*	O
,	O
f	O
;	O
,	O
ft	O
)	O
=	O
arg	O
max	O
ln	O
(	O
p	O
,	O
fo	O
,	O
ii	O
)	O
,	O
pe	O
[	O
o	O
,	O
1	O
]	O
,	O
fo	O
,	O
ii	O
ef	O
where	O
having	O
determined	O
p*	O
,	O
fo*	O
'	O
ft	O
(	O
recall	O
that	O
the	O
solution	O
is	O
not	O
necessarily	O
unique	O
)	O
,	O
the	O
maximum	B
likelihood	I
rule	O
is	O
(	O
x	O
)	O
=	O
i	O
°	O
if	O
(	O
1	O
-	O
p*	O
)	O
fo*	O
(	O
x	O
)	O
2	O
:	O
p*	O
ft	O
(	O
x	O
)	O
1	O
otherwise	O
.	O
gn	O
generally	O
speaking	O
,	O
the	O
distribution	B
format	I
is	O
more	O
sensitive	O
than	O
the	O
regression	B
format	I
.	O
it	O
may	O
work	O
better	O
under	O
the	O
correct	O
circumstances	O
.	O
however	O
,	O
we	O
give	O
problems	O
and	O
exercises	O
261	O
up	O
our	O
universality	O
,	O
as	O
the	O
nature	O
of	O
the	O
distribution	B
of	O
x	O
must	O
be	O
known	O
.	O
for	O
example	O
,	O
if	O
x	O
is	O
distributed	O
on	O
a	O
lower	O
dimensional	O
nonlinear	O
manifold	O
of	O
n	O
d	O
,	O
the	O
distribution	B
format	I
is	O
particularly	O
inconvenient	O
.	O
consistency	B
results	O
and	O
examples	O
are	O
provided	O
in	O
a	O
few	O
exercises	O
.	O
problems	O
and	O
exercises	O
problem	O
15.1.	O
show	O
that	O
if	O
ifi	O
=	O
k	O
<	O
00	O
and	O
1	O
]	O
e	O
f	O
,	O
then	O
l	O
(	O
1	O
]	O
n	O
)	O
-+	O
l	O
*	O
with	O
probability	O
one	O
,	O
where	O
1	O
]	O
n	O
is	O
obtained	O
by	O
the	O
maximum	B
likelihood	I
method	O
.	O
also	O
,	O
prove	O
that	O
convergence	O
with	O
probability	O
one	O
holds	O
in	O
theorem	O
15.2.	O
problem	O
15.2.	O
prove	O
that	O
if	O
1	O
]	O
,	O
1	O
]	O
'	O
are	O
[	O
0	O
,	O
1	O
]	O
-valued	O
regression	O
functions	O
,	O
then	O
the	O
divergence	B
1	O
-	O
1	O
]	O
'	O
(	O
x	O
)	O
}	O
d	O
(	O
1	O
]	O
)	O
=	O
e	O
1	O
]	O
(	O
x	O
)	O
log	O
-	O
-	O
+	O
(	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
log	O
-	O
-	O
-	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
'	O
{	O
1	O
]	O
'	O
(	O
x	O
)	O
1	O
]	O
(	O
x	O
)	O
is	O
nonpositive	O
,	O
and	O
that	O
d	O
=	O
0	O
if	O
and	O
only	O
if	O
1	O
]	O
(	O
x	O
)	O
=	O
1	O
]	O
'	O
(	O
x	O
)	O
with	O
probability	O
one	O
.	O
in	O
this	O
sense	O
,	O
d	O
measures	O
the	O
distance	B
between	O
1	O
]	O
and	O
1	O
]	O
'	O
.	O
problem	O
15.3.	O
let	O
l	O
*	O
=	O
e	O
{	O
min	O
(	O
1	O
]	O
(	O
x	O
)	O
,	O
1	O
-	O
1	O
]	O
(	O
x	O
»	O
}	O
,	O
l	O
(	O
1	O
]	O
'	O
)	O
=	O
p	O
{	O
i	O
{	O
i	O
)	O
'	O
(	O
x	O
»	O
lj2j	O
=i	O
y	O
}	O
,	O
and	O
d	O
(	O
1	O
]	O
'	O
)	O
=	O
e	O
{	O
1	O
]	O
(	O
x	O
)	O
log	O
1	O
]	O
'	O
(	O
x	O
)	O
+	O
(	O
1-1	O
]	O
(	O
x	O
)	O
)	O
log	O
1	O
-1	O
]	O
'	O
(	O
x	O
)	O
}	O
,	O
1	O
]	O
(	O
x	O
)	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
where	O
1	O
]	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
llx	O
=	O
x	O
}	O
,	O
and	O
1	O
]	O
'	O
:	O
nd	O
-+	O
[	O
0,1	O
]	O
is	O
arbitrary	O
.	O
prove	O
that	O
(	O
see	O
also	O
problem	O
3.22	O
)	O
.	O
hint	O
:	O
first	O
prove	O
that	O
for	O
p	O
,	O
q	O
e	O
[	O
0	O
,	O
1	O
]	O
,	O
plog-	O
+	O
(	O
1-	O
p	O
)	O
log	O
q	O
p	O
1	O
-	O
1-	O
p	O
(	O
p	O
_	O
q	O
)	O
2	O
:	O
:	O
:	O
:	O
-	O
-	O
-	O
-	O
2	O
(	O
use	O
taylor	O
's	O
series	O
with	O
remainder	O
term	O
for	O
h	O
(	O
·	O
)	O
)	O
.	O
problem	O
15.4.	O
show	O
that	O
for	O
each	O
1	O
]	O
there	O
exists	O
an	O
eo	O
>	O
0	O
such	O
that	O
for	O
all	O
e	O
e	O
(	O
0	O
,	O
eo	O
)	O
,	O
hint	O
:	O
proceed	O
by	O
chernoff	O
's	O
bounding	O
technique	O
.	O
problem	O
15.5.	O
consider	O
1	O
]	O
e	O
f	O
{	O
and	O
let	O
1	O
]	O
n	O
be	O
a	O
maximum	O
likelihood	O
estimate	O
over	O
f	O
{	O
.	O
let	O
p	O
=	O
p	O
{	O
y	O
=	O
i	O
}	O
.	O
show	O
that	O
1	O
-	O
c	O
)	O
l*	O
=	O
pmin	O
1	O
,	O
-c-	O
.	O
(	O
derive	O
an	O
upper	O
bound	O
for	O
e	O
{	O
l	O
(	O
17n	O
)	O
-	O
l	O
*	O
}	O
,	O
262	O
15.	O
t	O
,	O
he	O
maximum	B
likelihood	I
principle	O
where	O
in	O
case	O
of	O
multiple	O
choices	O
for	O
1	O
]	O
n	O
,	O
you	O
take	O
the	O
smallest	O
1	O
]	O
n	O
in	O
the	O
equivalence	O
class	O
.	O
this	O
is	O
an	O
important	O
problem	O
,	O
as	O
f	O
{	O
picks	O
a	O
histogram	O
cell	O
in	O
a	O
data-based	B
manner	O
.	O
f	O
{	O
may	O
be	O
generalized	B
to	O
the	O
automatic	B
selection	O
of	O
the	O
best	O
k-cell	O
histogram	O
:	O
let	O
f	O
be	O
the	O
collection	O
of	O
all1	O
]	O
's	O
that	O
are	O
constant	O
on	O
the	O
k	O
intervals	O
determined	O
by	O
breakpoints	O
-00	O
<	O
al	O
<	O
...	O
<	O
ak-l	O
<	O
00.	O
problem	O
15.6.	O
show	O
that	O
for	O
the	O
class	O
f2	O
,	O
e	O
{	O
l	O
(	O
1	O
]	O
,	O
j	O
}	O
s	O
(	O
cid:173	O
)	O
4d	O
n+l	O
when	O
yj	O
e	O
f2	O
and	O
yjn	O
is	O
the	O
maximum	B
likelihood	I
estimate	O
.	O
problem	O
15.7.	O
show	O
that	O
for	O
the	O
class	O
f4	O
,	O
holds	O
for	O
all	O
x	O
and	O
e	O
>	O
0	O
,	O
that	O
is	O
,	O
the	O
bracketing	O
e-entropy	O
of	O
f4	O
is	O
bounded	O
by	O
f3/el	O
.	O
hint	O
:	O
cover	O
the	O
class	O
f4	O
by	O
a	O
class	O
of	O
monotone	O
decreasing	O
,	O
piecewise	O
constant	O
functions	O
,	O
whose	O
values	O
are	O
multiples	O
of	O
e	O
/3	O
,	O
and	O
whose	O
breakpoints	O
are	O
at	O
the	O
ke	O
/3-quantiles	O
of	O
the	O
distribution	B
of	O
x	O
(	O
k	O
=	O
1	O
,	O
2	O
,	O
...	O
,	O
f3	O
/	O
e	O
l	O
)	O
.	O
problem	O
15.8.	O
discuss	O
the	O
maximum	B
likelihood	I
method	O
for	O
the	O
class	O
if	O
at	O
x	O
>	O
f3	O
otherwise	O
;	O
e	O
,	O
e	O
'	O
e	O
[	O
0	O
,	O
1	O
]	O
,	O
a	O
e	O
rd	O
,	O
j3	O
e	O
r	O
}	O
.	O
what	O
do	O
the	O
discrimination	O
rules	O
look	O
like	O
?	O
if	O
yj	O
e	O
f	O
,	O
is	O
the	O
rule	B
consistent	O
?	O
can	O
you	O
guarantee	O
a	O
certain	O
rate	B
of	I
convergence	I
for	O
e	O
{	O
l	O
(	O
yjn	O
)	O
}	O
?	O
if	O
yj	O
¢	O
.	O
f	O
,	O
can	O
you	O
prove	O
that	O
l	O
(	O
1	O
]	O
n	O
)	O
does	O
not	O
converge	O
in	O
probability	O
to	O
inf1j'ef	O
l	O
(	O
yj	O
'	O
)	O
for	O
some	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
with	O
1	O
]	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
11	O
x	O
=	O
x	O
}	O
?	O
how	O
would	O
you	O
obtain	O
the	O
values	O
of	O
e	O
,	O
e	O
'	O
,	O
a	O
,	O
j3	O
for	O
the	O
maximum	B
likelihood	I
choice	O
1	O
]	O
/1	O
?	O
problem	O
15.9.	O
let	O
xl	O
,	O
...	O
,	O
x/1	O
be	O
i.i.d	O
.	O
uniform	B
[	O
0	O
,	O
1	O
]	O
random	O
variables	O
.	O
let	O
yi	O
,	O
...	O
,	O
yn	O
be	O
arbitrary	O
{	O
o	O
,	O
1	O
}	O
-valued	O
numbers	O
.	O
show	O
that	O
with	O
probability	O
one	O
,	O
lim	O
sup	O
n	O
sin2	O
(	O
exi	O
)	O
x	O
n	O
cos	O
e-+oo	O
i	O
:	O
yi=l	O
i	O
:	O
yi=o	O
2	O
(	O
exi	O
)	O
=	O
1	O
,	O
while	O
for	O
any	O
t	O
<	O
00	O
,	O
with	O
probability	O
one	O
,	O
sup	O
n	O
sin\exi	O
)	O
x	O
n	O
cos	O
o	O
:	O
ses	O
:	O
t	O
i	O
:	O
yi=	O
]	O
i	O
:	O
yi=o	O
2	O
(	O
exi	O
)	O
<	O
l.	O
16	O
parametric	B
classification	I
what	O
do	O
you	O
do	O
if	O
you	O
believe	O
(	O
or	O
someone	O
tells	O
you	O
)	O
that	O
the	O
conditional	O
distribu	O
(	O
cid:173	O
)	O
tions	O
of	O
x	O
given	O
y	O
=	O
0	O
and	O
y	O
=	O
1	O
are	O
members	O
of	O
a	O
given	O
family	B
of	I
distributions	O
,	O
described	O
by	O
finitely	O
many	O
real-valued	O
parameters	O
?	O
of	O
course	O
,	O
it	O
does	O
not	O
make	O
sense	O
to	O
say	O
that	O
there	O
are	O
,	O
say	O
,	O
six	O
parameters	O
.	O
by	O
interleaving	O
the	O
bits	O
of	O
binary	O
expansions	O
,	O
we	O
can	O
always	O
make	O
one	O
parameter	O
out	O
of	O
six	O
,	O
and	O
by	O
splitting	O
binary	O
expressions	O
,	O
we	O
may	O
make	O
a	O
countable	O
number	O
of	O
parameters	O
out	O
of	O
one	O
parameter	O
(	O
by	O
writing	O
the	O
bits	O
down	O
in	O
triangular	O
fashion	O
as	O
shown	O
below	O
)	O
.	O
b7	O
b4	O
bs	O
b2	O
bs	O
b9	O
bi	O
b3	O
b6	O
ho	O
thus	O
,	O
we	O
must	O
proceed	O
with	O
care	O
.	O
the	O
number	O
of	O
parameters	O
of	O
a	O
family	O
really	O
is	O
measured	O
more	O
by	O
the	O
sheer	O
size	O
or	O
vastness	O
of	O
the	O
family	O
than	O
by	O
mere	O
repre	O
(	O
cid:173	O
)	O
sentation	O
of	O
numbers	O
.	O
if	O
the	O
family	O
is	O
relatively	O
small	O
,	O
we	O
will	O
call	O
it	O
parametric	O
but	O
we	O
will	O
not	O
give	O
you	O
a	O
formal	O
definition	B
of	I
``	O
parametric	O
.	O
''	O
for	O
now	O
,	O
we	O
let	O
8	O
,	O
the	O
set	O
of	O
all	O
possible	O
values	O
of	O
the	O
parameter	O
e	O
,	O
be	O
a	O
subset	O
of	O
a	O
finite-dimensional	O
euclidean	O
space	O
.	O
formally	O
,	O
let	O
pe	O
=	O
{	O
pe	O
:	O
e	O
e	O
8	O
}	O
,	O
be	O
a	O
class	O
of	O
probability	O
distributions	O
on	O
the	O
borel	O
sets	O
of	O
rd	O
.	O
typically	O
,	O
the	O
family	O
pe	O
is	O
parametrized	O
in	O
a	O
smooth	O
way	O
.	O
that	O
is	O
,	O
two	O
distributions	O
,	O
corresponding	O
to	O
two	O
parameter	O
vectors	O
close	O
to	O
each	O
other	O
are	O
in	O
some	O
sense	O
close	O
to	O
each	O
other	O
,	O
264	O
16.	O
parametric	B
classification	I
as	O
well	O
.	O
assume	O
that	O
the	O
class-conditional	B
densities	O
fo	O
and	O
fi	O
exist	O
,	O
and	O
that	O
both	O
belong	O
to	O
the	O
class	O
of	O
densities	O
fe	O
=	O
{	O
fe	O
:	O
e	O
e	O
e	O
}	O
.	O
discrete	O
examples	O
may	O
be	O
handled	O
similarly	O
.	O
take	O
for	O
example	O
all	O
gaussian	B
distri	O
(	O
cid:173	O
)	O
butions	O
on	O
n	O
d	O
,	O
in	O
which	O
where	O
mend	O
is	O
the	O
vector	O
of	O
means	O
,	O
and	O
l	O
:	O
is	O
the	O
covariance	O
matrix	O
.	O
recall	O
that	O
x	O
t	O
denotes	O
the	O
transposition	O
ofthe	O
column	O
vector	O
x	O
,	O
and	O
det	O
(	O
l	O
:	O
)	O
is	O
the	O
determinant	O
of	O
l	O
:	O
.	O
this	O
class	O
is	O
conveniently	O
parametrized	O
bye	O
=	O
(	O
m	O
,	O
l	O
:	O
)	O
,	O
that	O
is	O
,	O
a	O
vector	O
of	O
d	O
+	O
d	O
(	O
d	O
+	O
1	O
)	O
/2	O
real	O
numbers	O
.	O
knowing	O
that	O
the	O
class-conditional	B
distributions	O
are	O
in	O
pe	O
makes	O
discrimination	O
so	O
much	O
easier-rates	O
of	O
convergence	O
to	O
l	O
*	O
are	O
excellent	O
.	O
take	O
fe	O
as	O
the	O
class	O
of	O
uniform	B
densities	O
on	O
hyperrectangles	O
of	O
n	O
d	O
:	O
this	O
has	O
2d	O
natural	O
parameters	O
,	O
the	O
coordinates	O
of	O
the	O
lower	O
left	O
and	O
upper	O
right	O
vertices	O
.	O
figure	O
16.1.	O
the	O
class-conditional	B
densities	O
are	O
uni	O
(	O
cid:173	O
)	O
form	O
on	O
hyperrectangles	O
.	O
class	O
0	O
given	O
(	O
xl	O
,	O
yd	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
,	O
a	O
child	O
could	O
not	O
do	O
things	O
wrong-for	O
class	O
1	O
,	O
estimate	B
the	O
upper	O
right	O
vertex	O
by	O
and	O
similarly	O
for	O
the	O
upper	O
right	O
vertex	O
of	O
the	O
class	O
0	O
density	O
.	O
lower	O
left	O
vertices	O
are	O
estimated	O
by	O
considering	O
minima	O
.	O
if	O
ad	O
,	O
a	O
1	O
are	O
the	O
two	O
unknown	O
hyperrectangles	O
and	O
p	O
=	O
p	O
{	O
y	O
=	O
1	O
}	O
,	O
the	O
bayes	O
rule	B
is	O
simply	O
1	O
if	O
x	O
e	O
al	O
-	O
ad	O
o	O
if	O
x	O
e	O
ad	O
-	O
al	O
1	O
1-	O
p	O
if	O
x	O
e	O
al	O
n	O
ad	O
,	O
-	O
-	O
>	O
-	O
-	O
a	O
(	O
ao	O
)	O
a	O
(	O
ai	O
)	O
p	O
g*	O
(	O
x	O
)	O
=	O
o	O
if	O
x	O
e	O
al	O
nao	O
-	O
-	O
<	O
-	O
-	O
.	O
1-	O
p	O
,	O
a	O
(	O
ad	O
-	O
a	O
(	O
ao	O
)	O
p	O
16.	O
parametric	B
classification	I
265	O
in	O
reality	O
,	O
replace	O
ao	O
,	O
ai	O
,	O
and	O
p	O
by	O
the	O
sample	O
estimates	O
ao	O
,	O
al	O
(	O
described	O
above	O
)	O
and	O
p	O
=	O
(	O
1/	O
n	O
)	O
2:7=1	O
f	O
{	O
yi=lj	O
.	O
this	O
way	O
of	O
doing	O
things	O
works	O
very	O
well	O
,	O
and	O
we	O
will	O
pick	O
up	O
the	O
example	O
a	O
bit	O
further	O
on	O
.	O
however	O
,	O
it	O
is	O
a	O
bit	O
ad	O
hoc	O
.	O
there	O
are	O
indeed	O
a	O
few	O
main	O
principles	O
that	O
may	O
be	O
used	O
in	O
the	O
design	O
of	O
classifiers	O
under	O
the	O
additional	O
information	O
given	O
here	O
.	O
in	O
no	O
particular	O
order	O
,	O
here	O
are	O
a	O
few	O
methodologies	O
:	O
(	O
a	O
)	O
as	O
the	O
bayes	O
classifiers	O
belong	O
to	O
the	O
class	O
c	O
=	O
{	O
¢	O
=	O
f	O
{	O
p	O
!	O
el	O
>	O
(	O
l-p	O
)	O
!	O
eo	O
)	O
:	O
p	O
e	O
[	O
0	O
,	O
1	O
]	O
,80,81	O
e	O
e	O
}	O
,	O
it	O
suffices	O
to	O
consider	O
classifiers	O
in	O
c.	O
for	O
example	O
,	O
if	O
fe	O
is	O
the	O
normal	B
family	O
,	O
then	O
c	O
coincides	O
with	O
indicators	O
of	O
functions	O
in	O
the	O
set	O
{	O
{	O
x	O
:	O
x	O
t	O
ax	O
+	O
bt	O
x	O
+	O
c	O
>	O
0	O
:	O
a	O
is	O
a	O
d	O
x	O
d	O
matrix	O
,	O
b	O
e	O
r	O
d	O
,	O
c	O
e	O
r	O
}	O
}	O
,	O
that	O
is	O
,	O
the	O
family	B
of	I
quadratic	O
decisions	O
.	O
in	O
the	O
hyperrectangular	O
example	O
above	O
,	O
every	O
¢	O
is	O
of	O
the	O
form	O
f	O
al	O
-	O
a2	O
where	O
al	O
and	O
a2	O
are	O
hyperrectangles	O
of	O
rd	O
.	O
finding	O
the	O
best	O
classifier	B
ofthe	O
form	O
¢	O
=	O
fa	O
where	O
a	O
e	O
a	O
is	O
something	O
we	O
can	O
do	O
in	O
a	O
variety	O
of	O
ways	O
:	O
one	O
such	O
way	O
,	O
empirical	B
risk	I
minimization	I
,	O
is	O
dealt	O
with	O
in	O
chapter	O
12	O
for	O
example	O
.	O
(	O
b	O
)	O
plug-in	O
rules	O
estimate	B
(	O
80	O
,	O
8d	O
by	O
(	O
80	O
,	O
ih	O
)	O
and	O
p	O
=	O
pry	O
=	O
i	O
}	O
by	O
pfrom	O
the	O
data	O
,	O
and	O
form	O
the	O
rule	B
gn	O
(	O
x	O
)	O
=	O
°	O
otherwise	O
.	O
i	O
if	O
pfejx	O
)	O
>	O
(	O
1	O
-	O
[	O
i	O
)	O
f	O
?	O
o	O
(	O
x	O
)	O
{	O
the	O
rule	B
here	O
is	O
within	O
the	O
class	O
c	O
described	O
in	O
the	O
previous	O
paragraph	O
.	O
we	O
are	O
hopeful	O
that	O
the	O
performance	O
with	O
gn	O
is	O
close	O
to	O
the	O
performance	O
with	O
the	O
bayes	O
rule	B
g*	O
when	O
(	O
p	O
,	O
8o	O
,	O
~	O
)	O
is	O
close	O
to	O
(	O
p	O
,	O
80	O
,	O
8d	O
.	O
for	O
this	O
strategy	O
to	O
work	O
it	O
is	O
absolutely	O
essential	O
that	O
l	O
(	O
p	O
,	O
80	O
,	O
81	O
)	O
(	O
the	O
probability	O
of	O
error	O
when	O
p	O
,	O
80	O
,	O
81	O
are	O
the	O
parameters	O
)	O
be	O
continuous	O
in	O
(	O
p	O
,	O
80	O
,	O
81	O
)	O
.	O
robustness	O
is	O
a	O
key	O
ingredient	O
.	O
if	O
the	O
coj	O
.	O
?	O
tinuity	O
can	O
be	O
captured	O
in	O
an	O
inequality	B
,	O
then	O
we	O
may	O
get	O
performance	O
guarantees	O
for	O
e	O
{	O
ln	O
}	O
-	O
l	O
*	O
in	O
terms	O
of	O
the	O
distance	B
between	O
(	O
ii	O
,	O
8o	O
,	O
~	O
)	O
and	O
(	O
p	O
,	O
80	O
,	O
81	O
)	O
.	O
methods	O
of	O
estimating	O
the	O
parameters	O
include	O
maximum	B
likelihood	I
.	O
this	O
methodology	O
is	O
dealt	O
with	O
in	O
chapter	O
15.	O
this	O
method	O
is	O
rather	O
sensitive	O
to	O
incorrect	O
hypotheses	O
(	O
what	O
if	O
we	O
were	O
wrong	O
about	O
our	O
assumption	O
that	O
the	O
class-conditional	B
distributions	O
were	O
in	O
p	O
e	O
?	O
)	O
.	O
another	O
strategy	O
,	O
minimum	B
distance	I
estimation	I
,	O
picks	O
that	O
member	O
from	O
p	O
e	O
that	O
is	O
closest	O
in	O
some	O
sense	O
to	O
the	O
raw	B
empirical	O
measure	B
that	O
puts	O
mass	O
1/	O
n	O
at	O
each	O
if	O
the	O
n	O
data	O
points	O
.	O
see	O
section	O
16.3	O
this	O
approach	O
does	O
not	O
care	O
about	O
continuity	O
of	O
l	O
(	O
p	O
,	O
80	O
,	O
81	O
)	O
,	O
as	O
it	O
judges	O
members	O
of	O
pe	O
by	O
closeness	O
in	O
some	O
space	O
under	O
a	O
metric	O
that	O
is	O
directly	O
related	O
to	O
the	O
probability	O
of	O
error	O
.	O
robustness	O
will	O
drop	O
out	O
naturally	O
.	O
a	O
general	O
approach	O
should	O
not	O
have	O
to	O
know	O
whether	O
e	O
can	O
be	O
described	O
by	O
a	O
fi	O
(	O
cid:173	O
)	O
nite	O
number	O
of	O
parameters	O
.	O
for	O
example	O
,	O
it	O
should	O
equally	O
well	O
handle	O
descriptions	O
266	O
16.	O
parametric	B
classification	I
as	O
pe	O
is	O
the	O
class	O
of	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
in	O
which	O
1j	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
llx	O
=	O
x	O
}	O
is	O
monotonically	O
increasing	O
in	O
all	O
the	O
components	O
of	O
x.	O
universal	B
paradigms	O
such	O
as	O
maximum	B
likelihood	I
,	O
minimum	B
distance	I
estimation	I
,	O
and	O
empirical	B
error	I
min	O
(	O
cid:173	O
)	O
imization	O
are	O
all	O
applicable	O
here	O
.	O
this	O
particular	O
pe	O
is	O
dealt	O
with	O
in	O
chapter	O
15	O
,	O
just	O
to	O
show	O
you	O
that	O
the	O
description	O
of	O
the	O
class	O
does	O
invalidate	O
the	O
underlying	O
principles	O
.	O
16.1	O
example	O
:	O
exponential	B
families	O
a	O
class	O
pe	O
is	O
exponential	B
,	O
if	O
every	O
class-conditional	B
density	I
fe	O
can	O
be	O
written	O
in	O
the	O
form	O
where	O
/3	O
,	O
0/1	O
,	O
...	O
,	O
o/k	O
:	O
rd	O
-+	O
r	O
,	O
/3	O
:	O
:	O
:	O
:	O
0	O
,	O
a	O
,	O
jt1	O
,	O
...	O
,	O
jtk	O
:	O
e	O
-+	O
r	O
are	O
fixed	O
func	O
(	O
cid:173	O
)	O
tions	O
,	O
and	O
c	O
is	O
a	O
normalizing	O
constant	O
.	O
examples	O
of	O
exponential	O
families	O
include	O
the	O
gaussian	B
,	O
gamma	B
,	O
beta	B
,	O
rayleigh	O
,	O
and	O
maxwell	O
densities	O
(	O
see	O
problem	O
16.4	O
)	O
.	O
the	O
bayes-rule	O
can	O
be	O
rewritten	O
as	O
g*	O
(	O
x	O
)	O
~	O
{	O
~	O
this	O
is	O
equivalent	O
to	O
p	O
!	O
e*	O
(	O
x	O
)	O
)	O
(	O
l-p	O
)	O
l	O
!	O
eo	O
(	O
x	O
)	O
>	O
0	O
(	O
if	O
log	O
otherwise	O
.	O
g*	O
(	O
x	O
)	O
=	O
{	O
01	O
ifl7=10/i	O
(	O
x	O
)	O
(	O
jti	O
(	O
e	O
;	O
)	O
-	O
jti	O
(	O
e	O
:	O
)	O
)	O
<	O
log	O
(	O
l~~~~~~o	O
)	O
)	O
otherwise	O
.	O
the	O
bayes-rule	O
is	O
a	O
so-called	O
generalized	O
linear	O
rule	O
with	O
1	O
,	O
0/1	O
,	O
...	O
,	O
o/k	O
as	O
basis	O
functions	O
.	O
such	O
rules	O
are	O
easily	O
dealt	O
with	O
by	O
empirical	B
risk	I
minimization	I
and	O
related	O
methods	O
such	O
as	O
complexity	B
regularization	I
(	O
chapters	O
12	O
,	O
17	O
,	O
18,22	O
)	O
.	O
another	O
important	O
point	O
is	O
that	O
g*	O
does	O
not	O
involve	O
the	O
function	O
/3	O
.	O
for	O
all	O
we	O
know	O
,	O
f3	O
may	O
be	O
some	O
esoteric	O
ill-behaved	O
function	O
that	O
would	O
make	O
estimating	O
fe	O
(	O
x	O
)	O
all	O
but	O
impossible	O
if	O
/3	O
were	O
unknown	O
.	O
even	O
if	O
pe	O
is	O
the	O
huge	O
family	O
in	O
which	O
f3	O
:	O
:	O
:	O
:	O
0	O
is	O
left	O
undetermined	O
,	O
but	O
it	O
is	O
known	O
to	O
be	O
identical	O
for	O
the	O
two	O
class	O
(	O
cid:173	O
)	O
conditional	O
densities	O
(	O
and	O
ace	O
)	O
is	O
just	O
a	O
normalization	O
factor	O
)	O
,	O
we	O
would	O
still	O
only	O
have	O
to	O
look	O
at	O
the	O
same	O
small	O
class	O
of	O
generalized	O
linear	O
discrimination	O
rules	O
!	O
so	O
,	O
densities	O
do	O
not	O
matter-ratios	O
of	O
densities	O
do	O
.	O
pattern	O
recognition	O
should	O
be	O
,	O
and	O
is	O
,	O
easier	O
than	O
density	B
estimation	I
.	O
those	O
who	O
first	O
estimate	B
(	O
fo	O
,	O
!	O
i	O
)	O
by	O
(	O
10	O
,	O
it	O
)	O
and	O
then	O
construct	O
rules	O
based	O
on	O
the	O
sign	O
of	O
fij-	O
;	O
(	O
x	O
)	O
fj	O
)	O
1o	O
(	O
x	O
)	O
do	O
themselves	O
a	O
disservice	O
.	O
(	O
1	O
-	O
16.2	O
standard	B
plug-in	O
rules	O
267	O
16.2	O
standard	B
plug-in	O
rules	O
in	O
standard	O
plug-in	O
methods	O
,	O
we	O
construct	O
estimates	O
80	O
,	O
bt	O
and	O
pfrom	O
the	O
data	O
and	O
use	O
them	O
to	O
form	O
a	O
classifier	O
(	O
x	O
)	O
=	O
{	O
1	O
ifpf8t	O
(	O
x	O
)	O
>	O
(	O
1-	O
fi	O
)	O
f80	O
(	O
x	O
)	O
gn	O
0	O
otherwise	O
.	O
it	O
is	O
generally	O
not	O
true	O
that	O
if	O
p	O
-+	O
p	O
,	O
80	O
-+	O
80	O
,	O
and	O
bt	O
-+	O
81	O
in	O
probability	O
,	O
then	O
l	O
(	O
gn	O
)	O
-+	O
l	O
*	O
in	O
probability	O
,	O
where	O
l	O
(	O
gn	O
)	O
is	O
the	O
probability	O
of	O
error	O
with	O
gn	O
'	O
consider	O
the	O
following	O
simple	O
example	O
:	O
example	O
.	O
let	O
:	O
fe	O
be	O
the	O
class	O
of	O
all	O
uniform	B
densities	O
on	O
[	O
-8,0	O
]	O
if	O
8	O
=/1	O
and	O
on	O
[	O
0,8	O
]	O
if	O
8	O
=	O
1.	O
let	O
80	O
=	O
1,81	O
=	O
2	O
,	O
p	O
=	O
1/2	O
.	O
then	O
a	O
reasonable	O
estimate	B
of	O
8i	O
would	O
be	O
fh	O
=	O
maxi	O
:	O
yi=l	O
ixi	O
i·	O
clearly	O
,	O
fh	O
-+	O
8i	O
in	O
probability	O
.	O
however	O
,	O
as	O
fh	O
=/1	O
with	O
probability	O
one	O
,	O
we	O
note	O
that	O
gn	O
(	O
x	O
)	O
=	O
0	O
for	O
x	O
>	O
0	O
and	O
thus	O
,	O
even	O
though	O
l	O
*	O
=	O
0	O
(	O
as	O
the	O
supports	O
of	O
fed	O
and	O
fel	O
are	O
not	O
overlapping	O
)	O
,	O
l	O
(	O
gn	O
)	O
~	O
p	O
{	O
y	O
=	O
i	O
}	O
=	O
p	O
=	O
1/2	O
.	O
the	O
problem	O
with	O
this	O
is	O
that	O
there	O
is	O
no	O
continuity	O
with	O
respect	O
to	O
8	O
in	O
:	O
fe	O
.	O
0	O
basic	O
consistency	B
based	O
upon	O
continuity	O
considerations	O
is	O
indeed	O
easy	O
to	O
estab	O
(	O
cid:173	O
)	O
lish	O
.	O
as	O
ratios	O
of	O
densities	O
matter	O
,	O
it	O
helps	O
to	O
introduce	O
l7e	O
(	O
x	O
)	O
=	O
pfel	O
(	O
x	O
)	O
(	O
1	O
-	O
p	O
)	O
fed	O
(	O
x	O
)	O
+	O
p	O
fel	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
llx	O
=	O
x	O
}	O
,	O
where	O
8	O
=	O
(	O
p	O
,	O
80	O
,	O
8d	O
or	O
(	O
80	O
,	O
8d	O
as	O
the	O
case	O
may	O
be	O
.	O
we	O
recall	O
that	O
if	O
gn	O
(	O
x	O
)	O
=	O
i	O
(	O
1	O
)	O
(	O
f	O
(	O
x	O
»	O
1/2	O
}	O
where	O
ois	O
an	O
estimate	B
of	O
8	O
,	O
then	O
l	O
(	O
gn	O
)	O
-	O
l	O
*	O
:	O
:	O
:	O
2e	O
{	O
117e	O
(	O
x	O
)	O
-	O
l7e	O
(	O
x	O
)	O
11	O
dn	O
}	O
,	O
where	O
dn	O
is	O
the	O
data	O
sequence	O
.	O
thus	O
,	O
e	O
{	O
l	O
(	O
gn	O
)	O
-	O
l*	O
}	O
:	O
:	O
:	O
2e	O
{	O
ll7e	O
(	O
x	O
)	O
-l7e	O
(	O
x	O
)	O
i	O
}	O
.	O
by	O
the	O
lebesgue	O
dominated	B
convergence	I
theorem	I
,	O
we	O
have	O
,	O
without	O
further	O
ado	O
:	O
theorem	B
16.1.	O
if	O
l7e	O
is	O
continuous	O
in	O
8	O
in	O
the	O
l	O
1	O
(	O
m	O
)	O
sense	O
,	O
where	O
m	O
is	O
the	O
measure	B
of	O
x	O
,	O
and	O
0	O
-+	O
8	O
in	O
probability	O
,	O
then	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
-+	O
l	O
*	O
for	O
the	O
standard	B
plug-in	O
rule	B
.	O
in	O
some	O
cases	O
,	O
we	O
can	O
do	O
better	O
and	O
derive	O
rates	O
of	O
convergence	O
by	O
examining	O
the	O
local	O
behavior	O
of	O
l7e	O
(	O
x	O
)	O
.	O
for	O
example	O
,	O
if	O
l7e	O
(	O
x	O
)	O
=	O
e-e/x/	O
,	O
x	O
e	O
r	O
,	O
then	O
117e	O
(	O
x	O
)	O
l7b	O
'	O
(	O
x	O
)	O
1	O
:	O
:	O
:	O
ixl18	O
-	O
8'1	O
,	O
and	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
-	O
l*	O
<	O
2e	O
{	O
ix118	O
-	O
8	O
]	O
}	O
<	O
2/e	O
{	O
x	O
2	O
)	O
je	O
{	O
(	O
8	O
-en	O
268	O
16.	O
parametric	B
classification	I
yielding	O
an	O
explicit	O
bound	O
.	O
in	O
general	O
,	O
8	O
is	O
multivariate	O
,	O
consisting	O
at	O
least	O
of	O
the	O
triple	O
(	O
p	O
,	O
80	O
,81	O
)	O
,	O
but	O
the	O
above	O
example	O
shows	O
the	O
way	O
to	O
happy	O
analysis	O
.	O
for	O
the	O
simple	O
example	O
given	O
here	O
,	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
-+	O
l	O
*	O
if	O
e	O
{	O
(	O
8	O
-	O
e	O
?	O
}	O
-+	O
o.	O
in	O
fact	O
,	O
this	O
seems	O
to	O
suggest	O
that	O
for	O
this	O
family	O
,	O
e	O
should	O
be	O
found	O
to	O
minimize	O
e	O
{	O
(	O
8	O
-	O
8	O
)	O
2	O
}	O
.	O
this	O
is	O
false	O
.	O
one	O
is	O
always	O
best	O
off	O
minimizing	O
the	O
probability	O
of	O
error	O
.	O
other	O
criteria	O
may	O
be	O
relevant	O
via	O
continuity	O
,	O
but	O
should	O
be	O
considered	O
with	O
care	O
.	O
how	O
certain	O
parameters	O
are	O
estimated	O
for	O
given	O
families	O
of	O
distributions	O
is	O
what	O
mathematical	O
statistics	B
is	O
all	O
about	O
.	O
the	O
maximum	B
likelihood	I
principle	O
looms	O
large	O
:	O
80	O
is	O
estimated	O
for	O
densities	O
ie	O
by	O
80	O
=	O
argmax	O
n	O
le	O
(	O
xj	O
e	O
i	O
:	O
yi=o	O
for	O
example	O
.	O
if	O
you	O
work	O
out	O
this	O
(	O
likelihood	O
)	O
product	B
,	O
you	O
will	O
often	O
discover	O
a	O
simple	O
form	O
for	O
the	O
estimate	B
of	O
the	O
data	O
.	O
in	O
discrimination	O
,	O
only	O
1	O
]	O
matters	O
,	O
not	O
the	O
class-conditional	B
densities	O
.	O
maximum	B
likelihood	I
in	O
function	O
of	O
the	O
1	O
]	O
's	O
was	O
studied	O
in	O
chapter	O
15.	O
we	O
saw	O
there	O
that	O
this	O
is	O
often	O
consistent	O
,	O
but	O
that	O
maximum	B
likelihood	I
behaves	O
poorly	O
when	O
the	O
true	O
distribution	B
is	O
not	O
in	O
pe	O
.	O
we	O
will	O
work	O
out	O
two	O
simple	O
examples	O
:	O
as	O
an	O
example	O
of	O
the	O
maximum	B
likelihood	I
method	O
in	O
discrimination	O
,	O
we	O
assume	O
that	O
fe	O
=	O
{	O
/e	O
(	O
x	O
)	O
=	O
x	O
a	O
l	O
x	O
f3	O
e-	O
/	O
-	O
f3af	O
(	O
a	O
)	O
i	O
{	O
x	O
>	O
o	O
}	O
:	O
a	O
,	O
f3	O
>	O
oj	O
is	O
the	O
class	O
of	O
all	O
gamma	B
densities	O
with	O
8	O
=	O
(	O
a	O
,	O
f3	O
)	O
.	O
the	O
likelihood	B
product	I
given	O
(	O
x	O
1	O
,	O
yl	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
,	O
is	O
,	O
if	O
80,81	O
are	O
the	O
unknown	O
parameters	O
,	O
n	O
n	O
(	O
plej	O
(	O
xi	O
)	O
(	O
i	O
(	O
(	O
1	O
-	O
p	O
)	O
/eo	O
(	O
xi	O
)	O
)	O
l-yi	O
•	O
i=i	O
this	O
is	O
the	O
probability	O
of	O
observing	O
the	O
data	O
sequence	O
if	O
the	O
ie	O
's	O
were	O
in	O
fact	O
discrete	O
probabilities	O
.	O
this	O
product	B
is	O
simply	O
where	O
80	O
=	O
(	O
ao	O
,	O
f30	O
)	O
,	O
81	O
=	O
(	O
ai	O
,	O
f31	O
)	O
'	O
the	O
first	O
thing	O
we	O
notice	O
is	O
that	O
this	O
expression	B
depends	O
only	O
on	O
certain	O
functions	O
of	O
the	O
data	O
,	O
notably	O
l	O
yi	O
xi	O
,	O
l	O
(	O
l	O
-	O
yi	O
)	O
xi	O
'	O
l	O
yi	O
log	O
xi	O
,	O
l	O
(	O
l	O
-	O
yi	O
)	O
log	O
xi	O
,	O
and	O
l	O
yi·	O
these	O
are	O
called	O
the	O
sufficient	B
statistics	I
for	O
the	O
problem	O
at	O
hand	O
.	O
we	O
may	O
in	O
fact	O
throwaway	O
the	O
data	O
and	O
just	O
store	O
the	O
sufficient	B
statistics	I
.	O
the	O
likelihood	B
product	I
has	O
to	O
be	O
maximized	O
.	O
even	O
in	O
this	O
rather	O
simple	O
univariate	O
example	O
,	O
this	O
is	O
a	O
nontrivial	O
task	O
.	O
luckily	O
,	O
we	O
immediately	O
note	O
that	O
p	O
occurs	O
in	O
the	O
factor	O
pn	O
(	O
1	O
-	O
p	O
)	O
n-n	O
,	O
where	O
n	O
=	O
l	O
yi	O
•	O
this	O
is	O
maximal	O
at	O
p	O
=	O
n	O
in	O
,	O
a	O
well-known	O
result	O
.	O
for	O
fixed	O
ao	O
,	O
ai	O
,	O
we	O
can	O
also	O
get	O
f30	O
,	O
f3l	O
;	O
but	O
for	O
16.2	O
standard	B
plug-in	O
rules	O
269	O
variable	B
ao	O
,	O
ai	O
,	O
f30	O
,	O
f31	O
,	O
the	O
optimization	O
is	O
difficult	O
.	O
in	O
d-dimensional	O
cases	O
,	O
one	O
has	O
nearly	O
always	O
to	O
resort	O
to	O
specialized	O
algorithms	O
.	O
as	O
a	O
last	O
example	O
,	O
we	O
return	O
to	O
fe	O
=	O
{	O
unifonn	O
densities	O
on	O
rectangles	O
of	O
rd	O
}	O
.	O
here	O
the	O
likelihood	B
product	I
once	O
again	O
has	O
pn	O
(	O
l-=	O
--	O
p	O
)	O
n-n	O
as	O
a	O
factor	O
,	O
leading	O
to	O
p	O
=	O
n	O
in	O
.	O
the	O
other	O
factor	O
(	O
if	O
(	O
ai	O
,	O
bl	O
)	O
,	O
(	O
ao	O
,	O
bo	O
)	O
are	O
the	O
lower	O
left	O
,	O
upper	O
right	O
vertices	O
of	O
the	O
rectangles	O
for	O
81	O
,	O
(	O
0	O
)	O
is	O
zero	O
if	O
for	O
some	O
i	O
,	O
yi	O
=	O
1	O
,	O
xi	O
tf	O
:	O
-	O
rectangle	O
(	O
al	O
,	O
bd	O
,	O
or	O
yi	O
=	O
0	O
,	O
xi	O
tf	O
:	O
-	O
rectangle	O
(	O
ao	O
,	O
bo	O
)	O
·	O
otherwise	O
,	O
it	O
is	O
1	O
where	O
ilbl	O
-	O
al	O
ii	O
=	O
ti~=l	O
(	O
bij	O
)	O
-	O
aij	O
)	O
)	O
denotes	O
the	O
volume	O
of	O
the	O
rectangle	O
(	O
ai	O
,	O
h	O
)	O
,	O
and	O
similarly	O
for	O
iibo	O
-ao	O
ii	O
.	O
this	O
is	O
maximal	O
if	O
iibl	O
-al	O
ii	O
and	O
iibo	O
-ao	O
ii	O
are	O
minimal	O
.	O
thus	O
,	O
the	O
maximum	B
likelihood	I
estimates	O
are	O
~k	O
)	O
=	O
min	O
x~k	O
)	O
,	O
1	O
:	O
:	O
:	O
;	O
k	O
:	O
:	O
:	O
;	O
d	O
,	O
i	O
=	O
0,1	O
,	O
ai	O
j	O
:	O
yj=i	O
1	O
j	O
}	O
k	O
)	O
1	O
max	O
x~k	O
)	O
,	O
j	O
:	O
yfi	O
1	O
1	O
:	O
:	O
:	O
;	O
k	O
:	O
:	O
:	O
;	O
d	O
,	O
i	O
=	O
0,1	O
,	O
where	O
ai	O
=	O
(	O
ai	O
--	O
~l	O
)	O
,	O
...	O
,	O
ai	O
~d	O
)	O
--	O
)	O
,	O
bi	O
=	O
(	O
bi	O
,	O
...	O
,	O
bi	O
'2	O
(	O
1	O
)	O
'2	O
(	O
d	O
)	O
)	O
,	O
and	O
xi	O
=	O
(	O
xi	O
,	O
...	O
,	O
xi	O
)	O
.	O
(	O
d	O
)	O
(	O
1	O
)	O
rates	O
of	O
convergence	O
may	O
be	O
obtained	O
via	O
some	O
of	O
the	O
(	O
in	O
)	O
equalities	O
of	O
chapter	O
6	O
,	O
such	O
as	O
(	O
1	O
)	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
-	O
l	O
*	O
<	O
2e	O
{	O
i17n	O
(	O
x	O
)	O
-	O
17	O
(	O
x	O
)	O
i	O
}	O
(	O
where	O
1717	O
=	O
pf~1	O
(	O
pf~	O
+	O
(	O
1	O
-	O
p	O
>	O
feo	O
)	O
)	O
,	O
(	O
2	O
)	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
-	O
l	O
*	O
<	O
2je	O
{	O
(	O
17n	O
(	O
x	O
)	O
-	O
17	O
(	O
x	O
)	O
)	O
2	O
}	O
,	O
(	O
3	O
)	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
-	O
l	O
*	O
2e	O
{	O
i	O
ry	O
(	O
x	O
)	O
-	O
~	O
i	O
l	O
(	O
g	O
''	O
(	O
x	O
)	O
1g	O
'	O
(	O
xll	O
}	O
.	O
the	O
rate	O
with	O
which	O
e	O
approaches	O
8	O
in	O
e	O
(	O
measured	O
with	O
some	O
metric	B
)	O
may	O
be	O
very	O
different	O
from	O
that	O
with	O
which	O
l	O
(	O
gn	O
)	O
approaches	O
l	O
*	O
.	O
as	O
shown	O
in	O
theorem	O
6.5	O
,	O
the	O
inequality	B
(	O
2	O
)	O
is	O
always	O
loose	O
,	O
yet	O
it	O
is	O
this	O
inequality	B
that	O
is	O
often	O
used	O
to	O
derive	O
rates	O
of	O
convergence	O
by	O
authors	O
and	O
researchers	O
.	O
let	O
us	O
take	O
a	O
simple	O
example	O
to	O
illustrate	O
this	O
point	O
.	O
assume	O
fe	O
is	O
the	O
family	B
of	I
nonnal	O
densities	O
on	O
the	O
real	O
line	O
.	O
if	O
80	O
=	O
(	O
mo	O
,	O
ao	O
)	O
,	O
81	O
=	O
(	O
ml	O
'	O
al	O
)	O
are	O
the	O
unknown	O
means	O
and	O
standard	B
deviations	O
of	O
the	O
class-conditional	B
densities	O
,	O
and	O
p	O
=	O
p	O
{	O
y	O
=	O
1	O
}	O
is	O
also	O
unknown	O
,	O
then	O
we	O
may	O
estimate	B
p	O
by	O
p	O
=	O
l	O
]	O
=l	O
yj	O
in	O
,	O
and	O
mi	O
and	O
ai	O
by	O
respectively	O
,	O
when	O
denominators	O
are	O
positive	O
.	O
if	O
a	O
denominator	O
is	O
0	O
,	O
set	O
mi	O
=	O
0	O
,	O
~2	O
=	O
1.	O
from	O
chebyshev	O
's	O
inequality	B
,	O
we	O
can	O
verify	O
that	O
for	O
fixed	O
p	O
e	O
(	O
0	O
,	O
1	O
)	O
,	O
i	O
=	O
0,1	O
,	O
270	O
16.	O
parametric	B
classification	I
e	O
{	O
lp	O
-	O
ph	O
=	O
0	O
(	O
l/-fo	O
)	O
,	O
e	O
{	O
lmi	O
-	O
mil	O
}	O
=	O
0	O
(	O
l/-fo	O
)	O
,	O
e	O
{	O
lai	O
-	O
~i	O
}	O
=	O
0	O
(	O
l/-fo	O
)	O
(	O
problem	O
16.3	O
)	O
.	O
if	O
we	O
compute	O
e	O
{	O
117n	O
(	O
x	O
)	O
-	O
17	O
(	O
x	O
)	O
i	O
}	O
'	O
we	O
will	O
discover	O
that	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
-	O
l	O
*	O
=	O
o	O
(	O
l/-fo	O
)	O
.	O
however	O
,	O
if	O
we	O
compute	O
e	O
{	O
\17	O
(	O
x	O
)	O
-	O
~i	O
i	O
(	O
gn	O
(	O
x	O
)	O
:	O
;	O
t'g*	O
(	O
x	O
)	O
d	O
,	O
we	O
will	O
find	O
that	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
-	O
l	O
*	O
=	O
0	O
(	O
lin	O
)	O
.	O
thus	O
,	O
while	O
the	O
parameters	O
converge	O
at	O
a	O
rate	O
o	O
(	O
1	O
i	O
-fo	O
)	O
dictated	O
by	O
the	O
central	B
limit	I
theorem	I
,	O
and	O
while	O
1711	O
converges	O
to	O
17	O
in	O
l	O
1	O
(	O
fl	O
)	O
with	O
the	O
same	O
rate	O
,	O
the	O
error	O
rate	O
in	O
discrimination	O
is	O
much	O
smaller	O
.	O
see	O
problems	O
16.7	O
to	O
16.9	O
for	O
some	O
practice	O
in	O
this	O
respect	O
.	O
bibliographic	O
remarks	O
.	O
mclachlan	O
(	O
1992	O
)	O
has	O
a	O
comprehensive	O
treatment	O
on	O
parametric	B
classification	I
.	O
duda	O
and	O
hart	O
(	O
1973	O
)	O
have	O
many	O
good	O
introductory	O
examples	O
and	O
a	O
nice	O
discussion	O
on	O
sufficient	B
statistics	I
,	O
a	O
topic	O
we	O
do	O
not	O
deal	O
with	O
in	O
this	O
text	O
.	O
for	O
maximum	B
likelihood	I
estimation	O
,	O
see	O
hjort	O
(	O
1986a	O
;	O
1986b	O
)	O
.	O
0	O
16.3	O
minimum	O
distance	O
estimates	O
here	O
we	O
describe	O
a	O
general	O
parameter	B
estimation	I
principle	O
that	O
appears	O
to	O
be	O
more	O
suitable	O
for	O
plug-in	O
classification	O
rules	O
than	O
the	O
maximum	B
likelihood	I
method	O
.	O
the	O
estimated	O
parameter	O
is	O
obtained	O
by	O
the	O
projection	O
of	O
the	O
empirical	B
measure	I
on	O
the	O
parametric	O
family	O
.	O
the	O
principle	O
of	O
minimum	O
distance	B
estimation	O
may	O
be	O
described	O
as	O
follows	O
.	O
let	O
'pe	O
=	O
{	O
pe	O
:	O
(	O
)	O
e	O
8	O
}	O
be	O
a	O
parametric	O
family	B
of	I
distributions	O
,	O
and	O
assume	O
that	O
pe*	O
is	O
the	O
unknown	O
distribution	B
of	O
the	O
i.i.d	O
.	O
observations	O
zl	O
,	O
...	O
,	O
zn	O
.	O
denote	O
by	O
vn	O
the	O
empirical	B
measure	I
let	O
d	O
(	O
·	O
,	O
.	O
)	O
be	O
a	O
metric	O
on	O
the	O
set	O
of	O
all	O
probability	O
distributions	O
on	O
nd	O
.	O
the	O
minimum	O
distance	O
estimate	O
of	O
(	O
)	O
*	O
is	O
defined	O
as	O
(	O
)	O
n	O
=	O
argmin	O
d	O
(	O
vn	O
,	O
pe	O
)	O
,	O
eee	O
if	O
it	O
exists	O
and	O
is	O
unique	O
.	O
if	O
it	O
is	O
not	O
unique	O
,	O
select	O
one	O
candidate	O
for	O
which	O
the	O
minimum	O
is	O
attained	O
.	O
consider	O
for	O
example	O
the	O
kolmogorov-smirnov	O
distance	B
dks	O
(	O
p	O
,	O
q	O
)	O
=	O
sup	O
if	O
(	O
z	O
)	O
-	O
g	O
(	O
z	O
)	O
!	O
,	O
zend	O
16.3	O
minimum	O
distance	O
estimates	O
271	O
pen	O
--	O
-	O
--	O
-	O
--	O
-	O
--	O
``	O
''	O
.	O
vn	O
figure	O
16.2.	O
the	O
member	O
oips	O
closest	O
to	O
the	O
empirical	B
measure	I
vn	O
is	O
chosen	O
by	O
the	O
minimum	O
distance	O
estimate	O
.	O
where	O
f	O
and	O
g	O
are	O
the	O
distribution	B
functions	O
of	O
the	O
measures	O
p	O
and	O
q	O
on	O
n	O
d	O
,	O
respectively	O
.	O
it	O
is	O
easy	O
to	O
see	O
that	O
d	O
ks	O
is	O
a	O
metric	O
.	O
note	O
that	O
sup	O
if	O
(	O
z	O
)	O
-	O
g	O
(	O
z	O
)	O
1	O
=	O
sup	O
lp	O
(	O
a	O
)	O
-	O
q	O
(	O
a	O
)	O
i	O
,	O
zerd	O
aea	O
where	O
a	O
is	O
the	O
class	O
of	O
sets	O
of	O
the	O
form	O
(	O
-00	O
,	O
z	O
(	O
1	O
»	O
x	O
...	O
x	O
(	O
-00	O
,	O
zed	O
»	O
~	O
,	O
for	O
z	O
=	O
(	O
zo	O
)	O
,	O
...	O
,	O
zed	O
»	O
~	O
e	O
nd	O
.	O
for	O
the	O
kolmogorov-smirnov	O
distance	B
between	O
the	O
estimated	O
and	O
the	O
true	O
distributions	O
,	O
by	O
the	O
triangle	O
inequality	B
,	O
we	O
have	O
dks	O
(	O
pen	O
,	O
pe*	O
)	O
<	O
dks	O
(	O
pell	O
,	O
vn	O
)	O
+	O
dks	O
(	O
vn	O
,	O
pe*	O
)	O
<	O
2dks	O
(	O
vn	O
,	O
pe*	O
)	O
,	O
where	O
in	O
the	O
second	O
inequality	B
we	O
used	O
the	O
definition	B
of	I
en	O
.	O
now	O
notice	O
that	O
the	O
upper	O
bound	O
is	O
just	O
twice	O
the	O
kolmogorov-smirnov	O
distance	B
between	O
an	O
empirical	B
distribution	O
and	O
the	O
true	O
distribution	B
.	O
by	O
a	O
straightforward	O
application	O
of	O
theorem	O
12.5	O
,	O
for	O
every	O
nand	O
e	O
>	O
0	O
,	O
since	O
the	O
n-th	O
shatter	B
coefficient	I
sea	O
,	O
n	O
)	O
of	O
the	O
class	O
of	O
sets	O
can	O
not	O
exceed	O
(	O
ne	O
/	O
d	O
)	O
d.	O
this	O
can	O
easily	O
be	O
seen	O
by	O
an	O
argument	O
similar	O
to	O
that	O
in	O
the	O
proof	O
of	O
theorem	O
13.8.	O
from	O
the	O
inequalities	O
above	O
,	O
we	O
see	O
that	O
the	O
kolmogorov	O
(	O
cid:173	O
)	O
smirnov	O
distance	B
between	O
the	O
estimated	O
and	O
the	O
true	O
distributions	O
is	O
always	O
o	O
(	O
jlog	O
n	O
/	O
n	O
)	O
.	O
the	O
only	O
condition	O
we	O
require	O
is	O
that	O
en	O
be	O
well	O
defined	O
.	O
272	O
16.	O
parametric	B
classification	I
of	O
course	O
,	O
rather	O
than	O
the	O
kolmogorov-smirnov	O
distance	B
,	O
it	O
is	O
the	O
error	O
prob	O
(	O
cid:173	O
)	O
ability	O
of	O
the	O
plug-in	O
classification	O
rule	B
in	O
which	O
we	O
are	O
primarily	O
interested	O
.	O
in	O
order	O
to	O
make	O
the	O
connection	O
,	O
we	O
adapt	O
the	O
notions	O
of	O
minimum	O
distance	B
estima	O
(	O
cid:173	O
)	O
tion	O
and	O
kolmogorov-smirnov	O
distance	B
to	O
better	O
suit	O
the	O
classification	O
problem	O
we	O
are	O
after	O
.	O
every	O
parametric	O
family	O
of	O
distributions	O
defines	O
a	O
class	O
of	O
sets	O
in	O
nd	O
as	O
the	O
collection	O
a	O
of	O
sets	O
of	O
the	O
form	O
{	O
x	O
:	O
¢	O
(	O
x	O
)	O
=	O
i	O
}	O
,	O
where	O
the	O
classifiers	O
¢	O
are	O
the	O
possible	O
plug-in	O
rules	O
defined	O
by	O
the	O
parametric	O
family	O
.	O
the	O
idea	O
here	O
is	O
to	O
perform	O
minimum	B
distance	I
estimation	I
with	O
the	O
generalized	B
kolmogorov-smirnov	O
distance	B
with	O
respect	O
to	O
a	O
class	O
of	O
sets	O
closely	O
related	O
to	O
a.	O
assume	O
that	O
both	O
class-conditional	B
distributions	O
pea	O
'	O
pel	O
belong	O
to	O
a	O
parametric	O
family	O
pe	O
,	O
and	O
the	O
class-conditional	B
densities	O
feo	O
,	O
fe	O
]	O
exist	O
.	O
then	O
the	O
bayes	O
rule	B
may	O
be	O
written	O
as	O
*	O
(	O
x	O
)	O
=	O
{	O
i	O
ifae	O
(	O
x	O
!	O
>	O
0	O
g	O
where	O
ae	O
(	O
x	O
)	O
=	O
pfel	O
(	O
x	O
)	O
-	O
(	O
1	O
-	O
p	O
)	O
fea	O
(	O
x	O
)	O
and	O
p	O
=	O
p	O
{	O
y	O
=	O
i	O
}	O
.	O
we	O
use	O
the	O
short	O
notation	O
8	O
=	O
(	O
p	O
,	O
80	O
,	O
8d	O
.	O
the	O
function	O
ae	O
may	O
be	O
thought	O
of	O
as	O
the	O
radon-nikodym	O
derivative	O
of	O
the	O
signed	B
measure	I
qe	O
=	O
ppe1	O
-	O
(	O
1	O
-	O
p	O
)	O
peo	O
'	O
in	O
other	O
words	O
,	O
to	O
each	O
borel	O
set	O
a	O
c	O
n	O
d	O
,	O
qe	O
assigns	O
the	O
real	O
number	O
qe	O
(	O
a	O
)	O
=	O
ppe1	O
(	O
a	O
)	O
-	O
(	O
1-	O
p	O
)	O
peo	O
(	O
a	O
)	O
.	O
given	O
the	O
data	O
dn	O
=	O
«	O
xl	O
,	O
yd	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
)	O
,	O
we	O
define	O
the	O
empirical	B
counterpart	O
of	O
qe	O
by	O
0	O
otherwise	O
,	O
the	O
minimum	O
distance	O
classification	O
rule	B
we	O
propose	O
projects	O
the	O
empirical	B
signed	O
measure	B
1	O
}	O
n	O
on	O
the	O
set	O
of	O
measures	O
qe	O
.	O
the	O
metric	B
we	O
use	O
is	O
also	O
specifically	O
fitted	O
to	O
the	O
given	O
pattern	O
recognition	O
problem	O
:	O
define	O
the	O
class	O
of	O
sets	O
a	O
=	O
{	O
{	O
x	O
e	O
nd	O
:	O
ae	O
(	O
x	O
)	O
>	O
o	O
}	O
:	O
p	O
e	O
[	O
0	O
,	O
1	O
]	O
,80,81	O
e	O
e	O
}	O
.	O
a	O
is	O
just	O
the	O
class	O
of	O
sets	O
a	O
c	O
nd	O
such	O
that	O
i	O
{	O
xea	O
}	O
is	O
the	O
bayes	O
rule	B
for	O
some	O
8	O
=	O
(	O
p	O
,	O
80,81	O
)	O
.	O
also	O
introduce	O
13	O
=	O
{	O
a	O
n	O
be	O
:	O
a	O
,	O
b	O
e	O
a	O
}	O
.	O
given	O
two	O
signed	O
measures	O
q	O
,	O
q	O
'	O
,	O
we	O
define	O
their	O
generalized	B
kolmogorov	O
(	O
cid:173	O
)	O
smirnov	O
distance	B
by	O
db	O
(	O
q	O
,	O
q	O
'	O
)	O
=	O
sup	O
iq	O
(	O
a	O
)	O
-	O
q	O
'	O
(	O
a	O
)	O
i	O
,	O
aeb	O
that	O
is	O
,	O
instead	O
of	O
the	O
class	O
of	O
half-infinite	O
intervals	O
as	O
in	O
the	O
definition	B
of	I
the	O
ordinary	B
kolmogorov-smirnov	O
distance	B
,	O
here	O
we	O
take	O
the	O
supremum	O
over	O
13	O
,	O
a	O
class	O
tailored	O
to	O
our	O
discrimination	O
problem	O
.	O
now	O
,	O
we	O
are	O
ready	O
to	O
define	O
our	O
minimum	O
distance	O
estimate	O
e	O
=	O
argmin	O
db	O
(	O
qe	O
,	O
1	O
}	O
n	O
)	O
,	O
e	O
16.3	O
minimum	O
distance	O
estimates	O
273	O
where	O
the	O
minimum	O
is	O
taken	O
over	O
all	O
triples	O
e	O
=	O
(	O
p	O
,	O
eo	O
,	O
(	O
1	O
)	O
with	O
p	O
e	O
[	O
0	O
,	O
1	O
]	O
,	O
(	O
)	O
o	O
,	O
(	O
)	O
l	O
e	O
e.	O
the	O
corresponding	O
classification	O
rule	B
is	O
if	O
ae	O
(	O
x	O
)	O
>	O
0	O
gg	O
(	O
x	O
)	O
=	O
.	O
·0	O
otherwise	O
.	O
i	O
{	O
the	O
next	O
theorem	B
shows	O
that	O
if	O
the	O
parametric	O
assumption	O
is	O
valid	O
,	O
then	O
ge	O
performs	O
extremely	O
well	O
.	O
the	O
theorem	B
shows	O
that	O
if	O
a	O
has	O
finite	O
vc	B
dimension	I
v	O
a	O
,	O
then	O
with	O
(	O
cid:173	O
)	O
out	O
any	O
additional	O
conditions	O
imposed	O
on	O
the	O
parametric	O
class	O
,	O
the	O
corresponding	O
error	O
probability	O
l	O
(	O
ge	O
)	O
is	O
not	O
large	O
:	O
l	O
(	O
ge	O
)	O
-	O
l	O
*	O
=	O
0	O
(	O
iva	O
log	O
n/n	O
)	O
.	O
theorem	B
16.2.	O
assume	O
that	O
both	O
conditional	O
densities	O
feo	O
,	O
fe	O
l	O
are	O
in	O
the	O
para	O
(	O
cid:173	O
)	O
metric	B
class	O
fe	O
.	O
then	O
for	O
the	O
classification	O
rule	B
defined	O
above	O
,	O
we	O
have	O
for	O
every	O
nand	O
e	O
>	O
0	O
that	O
p	O
{	O
l	O
(	O
ge	O
)	O
-	O
l*	O
>	O
e	O
}	O
:	O
:s	O
8s	O
(	O
8	O
,	O
n	O
)	O
e-ne2js12	O
,	O
where	O
s	O
(	O
8	O
,	O
n	O
)	O
is	O
the	O
n-th	O
shatter	B
coefficient	I
of	O
8.	O
furthermore	O
,	O
e	O
{	O
l	O
(	O
ge	O
)	O
-	O
l	O
*	O
}	O
:	O
:s	O
32	O
109	O
(	O
8e	O
;	O
~8	O
,	O
n	O
)	O
)	O
.	O
remark	O
.	O
recalling	O
from	O
chapter	O
13	O
that	O
s	O
(	O
8	O
,	O
n	O
)	O
:	O
:	O
:	O
:	O
:	O
s2	O
(	O
a	O
,	O
n	O
)	O
and	O
that	O
sea	O
,	O
n	O
)	O
:	O
:s	O
(	O
n	O
+	O
l	O
)	O
va	O
,	O
where	O
va	O
is	O
the	O
vc	B
dimension	I
of	O
a	O
,	O
we	O
obtain	O
the	O
bound	O
p	O
{	O
l	O
(	O
ge	O
)	O
-	O
l*	O
>	O
e	O
}	O
:	O
:	O
:	O
:	O
:	O
8	O
(	O
n	O
+	O
1	O
)	O
2vae-ne2js12	O
.	O
0	O
the	O
proof	O
of	O
the	O
theorem	B
is	O
based	O
upon	O
two	O
key	O
observations	O
.	O
the	O
first	O
lemma	O
provides	O
a	O
bound	O
on	O
l	O
(	O
ge	O
)	O
-	O
l	O
*	O
in	O
terms	O
of	O
the	O
generalized	B
kolmogorov-smirnov	O
distance	B
between	O
the	O
estimated	O
and	O
the	O
true	O
parameters	O
.	O
the	O
second	O
lemma	O
is	O
a	O
straightforward	O
extension	O
of	O
the	O
vapnik-chervonenkis	O
inequality	B
to	O
signed	O
mea	O
(	O
cid:173	O
)	O
sures	O
.	O
lemma	O
16.1.	O
proof	O
.	O
denote	O
ae	O
=	O
{	O
x	O
:	O
ae	O
(	O
x	O
)	O
>	O
o	O
}	O
and	O
ae	O
=	O
{	O
x	O
:	O
ae	O
(	O
x	O
)	O
>	O
a	O
}	O
,	O
that	O
is	O
,	O
gg	O
(	O
x	O
)	O
=	O
i	O
{	O
ag	O
(	O
x	O
»	O
o	O
}	O
,	O
g*	O
(	O
x	O
)	O
=	O
i	O
{	O
ae	O
(	O
x	O
»	O
o	O
}	O
,	O
and	O
ae	O
,	O
ae	O
e	O
a.	O
at	O
the	O
first	O
crucial	O
step	O
,	O
we	O
use	O
the	O
equality	O
of	O
theorem	O
2.2	O
:	O
l	O
(	O
ge	O
)	O
-	O
l	O
*	O
f	O
i	O
{	O
ge	O
(	O
x	O
)	O
;	O
,	O
fge	O
(	O
x	O
)	O
}	O
lae	O
(	O
x	O
)	O
ldx	O
-	O
r	O
c	O
ae	O
(	O
x	O
)	O
dx	O
+	O
[	O
c	O
j	O
aenae	O
j	O
aenae	O
ae	O
(	O
x	O
)	O
dx	O
274	O
16.	O
parametric	B
classification	I
<	O
r	O
ae	O
(	O
x	O
)	O
dx	O
-	O
r	O
ae	O
(	O
x	O
)	O
dx	O
j	O
aena~	O
+	O
r	O
ae	O
(	O
x	O
)	O
dx	O
-	O
r	O
ae	O
(	O
x	O
)	O
dx	O
ja~nae	O
ja~nae	O
j	O
aena~	O
qe	O
(	O
ae	O
n	O
a~	O
)	O
-	O
qe	O
(	O
ae	O
n	O
a~	O
)	O
+	O
qe	O
(	O
a~	O
n	O
ae	O
)	O
-	O
qe	O
(	O
a~	O
n	O
ae	O
)	O
<	O
2ds	O
(	O
qe	O
,	O
qe	O
)	O
,	O
since	O
both	O
ae	O
n	O
a~	O
and	O
a~	O
n	O
ae	O
are	O
members	O
of	O
be	O
.	O
0	O
the	O
next	O
lemma	O
is	O
an	O
extension	O
of	O
the	O
vapnik	O
-chervonenkis	O
inequality	B
.	O
the	O
proof	O
is	O
left	O
to	O
the	O
reader	O
(	O
problem	O
16.11	O
)	O
.	O
lemma	O
16.2.	O
for	O
every	O
nand	O
e	O
>	O
0	O
,	O
p	O
{	O
ds	O
(	O
qe	O
,	O
1	O
)	O
n	O
)	O
>	O
e	O
}	O
:	O
:s	O
8s	O
(	O
b	O
,	O
n	O
)	O
e-ne2/32	O
.	O
the	O
rest	O
of	O
the	O
proof	O
of	O
theorem	O
16.2	O
shows	O
that	O
the	O
generalized	B
kolmogorov	O
(	O
cid:173	O
)	O
smimov	O
distance	B
with	O
respect	O
to	O
b	O
between	O
the	O
estimated	O
and	O
true	O
distributions	O
is	O
small	O
with	O
large	O
probability	O
.	O
this	O
can	O
be	O
done	O
as	O
we	O
proved	O
for	O
the	O
kolmogorov	O
(	O
cid:173	O
)	O
smimov	O
distance	B
at	O
the	O
beginning	O
of	O
the	O
section	O
.	O
proof	O
of	O
theorem	O
16.2.	O
by	O
lemma	O
16.1	O
,	O
l	O
(	O
ge	O
)	O
-	O
l	O
*	O
<	O
2ds	O
(	O
qe	O
,	O
qe	O
)	O
<	O
2ds	O
(	O
qe	O
,	O
1	O
)	O
n	O
)	O
+	O
2ds	O
(	O
qe	O
,	O
1	O
)	O
n	O
)	O
<	O
4ds	O
(	O
qe,1	O
)	O
n	O
)	O
(	O
by	O
the	O
definition	B
of	I
e	O
)	O
.	O
(	O
by	O
the	O
triangle	O
inequality	B
)	O
the	O
theorem	B
now	O
follows	O
by	O
lemma	O
16.2	O
.	O
0	O
finally	O
,	O
we	O
examine	O
robustness	O
of	O
the	O
minimum	O
distance	O
rule	O
against	O
modeling	O
errors	O
,	O
that	O
is	O
,	O
what	O
happens	O
if	O
the	O
distributions	O
are	O
not	O
in	O
pe	O
.	O
a	O
good	O
rule	B
should	O
still	O
work	O
reasonably	O
well	O
if	O
the	O
distributions	O
are	O
in	O
some	O
sense	O
close	O
to	O
the	O
modeled	O
parametric	O
class	O
pe	O
.	O
observe	O
that	O
if	O
for	O
some	O
e	O
=	O
(	O
p	O
,	O
eo	O
,	O
(	O
1	O
)	O
the	O
bayes	O
rule	B
can	O
be	O
written	O
as	O
{	O
1	O
if	O
ae	O
(	O
x	O
)	O
>	O
°	O
g	O
(	O
x	O
)	O
=	O
°	O
otherwise	O
,	O
*	O
then	O
lemma	O
16.1	O
remains	O
valid	O
even	O
when	O
the	O
class-conditional	B
distributions	O
are	O
not	O
in	O
p	O
e.	O
denote	O
the	O
true	O
class-conditional	B
distributions	O
by	O
p	O
;	O
,	O
pt	O
,	O
let	O
p	O
*	O
=	O
pry	O
=	O
1	O
}	O
,	O
and	O
introduce	O
q*	O
=	O
p*	O
pt	O
-	O
(	O
l	O
-	O
p	O
)	O
p	O
;	O
.	O
thus	O
,	O
l	O
(	O
ge	O
)	O
-	O
l	O
*	O
(	O
by	O
lemma	O
16.1	O
)	O
<	O
2ds	O
(	O
q*	O
,	O
qe	O
)	O
<	O
2ds	O
(	O
q*	O
,	O
1	O
)	O
n	O
)	O
+	O
2ds	O
(	O
1	O
)	O
n	O
,	O
qe	O
)	O
<	O
2ds	O
(	O
q*	O
,	O
1	O
)	O
n	O
)	O
+	O
2ds	O
(	O
1	O
)	O
n	O
,	O
qe	O
)	O
<	O
4ds	O
(	O
q*	O
,	O
vn	O
)	O
+	O
2ds	O
(	O
q*	O
,	O
qe	O
)	O
(	O
by	O
the	O
triangle	O
inequality	B
)	O
(	O
by	O
the	O
definition	B
of	I
qe	O
)	O
(	O
again	O
by	O
the	O
triangle	O
inequality	B
)	O
.	O
16.4	O
empirical	B
error	I
minimization	O
275	O
lemma	O
16.2	O
now	O
applies	O
to	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
.	O
thus	O
,	O
we	O
conclude	O
that	O
if	O
the	O
bayes	O
rule	B
is	O
in	O
a	O
,	O
then	O
for	O
all	O
e	O
:	O
:	O
:	O
:	O
4	O
inf	O
qe	O
ds	O
(	O
q*	O
,	O
qe	O
)	O
,	O
p	O
{	O
l	O
(	O
ge	O
)	O
-	O
l	O
*	O
>	O
ej	O
:	O
:s	O
8	O
(	O
n	O
+1	O
)	O
2vae-ne2j211	O
.	O
the	O
constant	O
in	O
the	O
exponent	O
may	O
be	O
improved	O
significantly	O
by	O
more	O
careful	O
anal	O
(	O
cid:173	O
)	O
ysis	O
.	O
in	O
other	O
words	O
,	O
if	O
the	O
bayes	O
rule	B
is	O
in	O
a	O
and	O
the	O
true	O
distribution	B
is	O
close	O
to	O
the	O
parametric	O
family	O
in	O
the	O
generalized	B
kolmogorov-smimov	O
distance	B
specified	O
above	O
,	O
then	O
the	O
minimum	O
distance	O
rule	O
still	O
performs	O
close	O
to	O
the	O
bayes	O
error	O
.	O
unfortunately	O
,	O
we	O
can	O
not	O
say	O
the	O
same	O
if	O
a	O
does	O
not	O
contain	O
the	O
bayes	O
rule	B
.	O
em	O
(	O
cid:173	O
)	O
pirical	O
error	O
minimization	O
,	O
discussed	O
in	O
the	O
next	O
section	O
,	O
is	O
however	O
very	O
robust	O
in	O
all	O
situations	O
.	O
16.4	O
empirical	B
error	I
minimization	O
in	O
this	O
section	O
we	O
explore	O
the	O
connection	O
between	O
parametric	B
classification	I
and	O
rule	B
selection	O
by	O
minimizing	O
the	O
empirical	B
error	I
,	O
studied	O
in	O
chapter	O
12.	O
consider	O
the	O
class	O
c	O
of	O
classifiers	O
of	O
the	O
form	O
¢	O
(	O
x	O
)	O
=	O
{	O
o	O
if	O
(	O
1	O
-	O
.p	O
)	O
feo	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
pfe/x	O
)	O
1	O
otherwise	O
,	O
where	O
p	O
e	O
[	O
0	O
,	O
1	O
]	O
,	O
and	O
eo	O
,	O
el	O
e	O
8.	O
the	O
parametric	O
assumption	O
means	O
that	O
the	O
bayes	O
rule	B
is	O
contained	O
in	O
c.	O
then	O
it	O
is	O
a	O
very	O
natural	O
approach	O
to	O
minimize	O
the	O
empirical	B
error	I
probability	O
measured	O
on	O
the	O
training	O
data	O
dn	O
over	O
classifiers	O
¢	O
in	O
the	O
class	O
c.	O
denote	O
the	O
empirically	O
selected	O
rule	B
(	O
i.e.	O
,	O
the	O
one	O
minimizing	O
ln	O
(	O
¢	O
)	O
)	O
by	O
¢~	O
.	O
for	O
most	O
typical	O
parametric	O
classes	O
8	O
,	O
the	O
vc	B
dimension	I
vc	O
is	O
finite	O
.	O
therefore	O
,	O
as	O
a	O
straightforward	O
consequence	O
of	O
theorem	O
12.6	O
,	O
we	O
have	O
corollary	O
16.1.	O
if	O
both	O
conditional	O
distributions	O
are	O
contained	O
in	O
the	O
paramet	O
(	O
cid:173	O
)	O
ric	O
family	O
pes	O
,	O
then	O
for	O
the	O
error	O
probability	O
l	O
(	O
¢~	O
)	O
=	O
p	O
{	O
¢~	O
(	O
x	O
)	O
=i	O
yidn	O
}	O
of	O
the	O
empirically	O
optimal	O
rule	O
¢~	O
,	O
we	O
have	O
for	O
every	O
nand	O
e	O
>	O
°	O
p	O
{	O
l	O
(	O
¢~	O
)	O
-	O
l*	O
>	O
e	O
}	O
:	O
:s	O
8s	O
(	O
c	O
,	O
n	O
)	O
e-ne2j128	O
.	O
the	O
result	O
above	O
means	O
that	O
0	O
(	O
-/log	O
n	O
/	O
n	O
)	O
rate	B
of	I
convergence	I
to	O
the	O
bayes	O
rule	B
is	O
guaranteed	O
for	O
the	O
empirically	O
optimal	O
rule	O
,	O
whenever	O
the	O
vc	B
dimension	I
vc	O
is	O
finite	O
.	O
this	O
is	O
the	O
case	O
,	O
for	O
example	O
,	O
for	O
exponential	B
families	O
.	O
if	O
pes	O
is	O
an	O
exponential	B
family	I
,	O
with	O
densities	O
of	O
the	O
form	O
je	O
(	O
x	O
)	O
~	O
ca	O
(	O
e	O
)	O
j	O
:	O
j	O
(	O
x	O
)	O
exp	O
{	O
tjri	O
(	O
e	O
)	O
<	O
/	O
!	O
i	O
(	O
x	O
)	O
}	O
,	O
276	O
16.	O
parametric	B
classification	I
then	O
by	O
results	O
from	O
chapter	O
13	O
,	O
s	O
(	O
c	O
,	O
n	O
)	O
:	O
:	O
:	O
(	O
n+	O
1t+l	O
.	O
observe	O
that	O
in	O
this	O
approach	O
,	O
nothing	O
but	O
properties	O
of	O
the	O
class	O
c	O
are	O
used	O
to	O
derive	O
the	O
a	O
(	O
jlog	O
n	O
in	O
)	O
rate	B
of	I
convergence	I
.	O
remark	O
.	O
robustness	O
.	O
the	O
method	O
of	O
empirical	O
minimization	O
is	O
clearly	O
extremely	O
robust	O
against	O
errors	O
in	O
the	O
parametric	O
model	O
.	O
obviously	O
,	O
(	O
see	O
theorem	B
12.6	O
)	O
if	O
the	O
true	O
conditional	O
distributions	O
are	O
not	O
contained	O
in	O
the	O
class	O
p	O
e	O
,	O
then	O
l	O
*	O
can	O
be	O
replaced	O
in	O
corollary	O
16.1	O
by	O
inf¢	O
>	O
ec	O
p	O
{	O
¢	O
(	O
x	O
)	O
¥	O
y	O
}	O
.	O
if	O
the	O
model	O
is	O
fairly	O
good	O
,	O
then	O
this	O
number	O
should	O
be	O
very	O
close	O
to	O
the	O
bayes	O
risk	O
l	O
*	O
.	O
d	O
remark	O
.	O
lower	O
bounds	O
.	O
the	O
results	O
of	O
chapter	O
14	O
imply	O
that	O
for	O
some	O
distri	O
(	O
cid:173	O
)	O
butions	O
the	O
error	O
probability	O
of	O
the	O
selected	O
rule	B
is	O
about	O
a	O
(	O
1	O
i	O
vfn	O
,	O
)	O
away	O
from	O
the	O
bayes	O
risk	O
.	O
in	O
the	O
parametric	O
case	O
,	O
however	O
,	O
since	O
the	O
class	O
of	O
distributions	O
is	O
restricted	O
by	O
the	O
parametric	O
model	O
,	O
this	O
is	O
not	O
necessarily	O
true	O
.	O
in	O
some	O
cases	O
,	O
a	O
much	O
faster	O
rate	B
of	I
convergence	I
is	O
possible	O
than	O
the	O
a	O
(	O
jlog	O
n	O
in	O
)	O
rate	O
guaranteed	O
by	O
corollary	O
16.1.	O
see	O
,	O
for	O
example	O
,	O
problem	O
16.3	O
,	O
where	O
an	O
example	O
is	O
given	O
in	O
which	O
the	O
error	O
rate	O
is	O
a	O
(	O
1in	O
)	O
.	O
d	O
problems	O
and	O
exercises	O
problem	O
16.1.	O
show	O
that	O
if	O
both	O
conditional	O
distributions	O
of	O
x	O
,	O
given	O
y	O
=	O
0	O
and	O
y	O
=	O
1	O
,	O
are	O
gaussian	B
,	O
then	O
the	O
bayes	O
decision	O
is	O
quadratic	O
,	O
that	O
is	O
,	O
it	O
can	O
be	O
written	O
as	O
g*	O
(	O
x	O
)	O
=	O
{	O
~	O
if'l.~	O
:	O
(	O
d+i	O
)	O
/2	O
a	O
(	O
t/ji	O
(	O
x	O
)	O
+	O
ao	O
2	O
:	O
0	O
otherwise	O
,	O
where	O
the	O
functions	O
l/ji	O
(	O
x	O
)	O
are	O
either	O
of	O
the	O
form	O
x	O
(	O
i	O
)	O
(	O
the	O
i-th	O
component	O
of	O
the	O
vector	O
x	O
)	O
,	O
or	O
x	O
(	O
i	O
)	O
x	O
(	O
j	O
)	O
,	O
and	O
ao	O
,	O
...	O
,	O
ad+d	O
(	O
d+j	O
)	O
/2	O
e	O
r.	O
problem	O
16.2.	O
let	O
1	O
be	O
the	O
normal	B
density	O
on	O
the	O
real	O
line	O
with	O
mean	O
m	O
and	O
standard	B
deviation	O
0-	O
,	O
and	O
we	O
draw	O
an	O
i.i.d	O
.	O
sample	O
x	O
i	O
,	O
...	O
,	O
xn	O
from	O
1	O
,	O
and	O
set	O
2	O
and	O
--	O
-2	O
0-	O
=	O
-	O
l	O
(	O
xi	O
-	O
m	O
)	O
.	O
'	O
``	O
2	O
1	O
11	O
n	O
i=1	O
show	O
that	O
e	O
{	O
1m	O
-	O
mil	O
=	O
0	O
(	O
1/	O
in	O
)	O
and	O
e	O
{	O
io-	O
-	O
cti	O
}	O
=	O
0	O
(	O
1/	O
in	O
)	O
by	O
using	O
chebyshev	O
's	O
inequality	B
.	O
show	O
that	O
this	O
rate	O
is	O
in	O
fact	O
tight	O
.	O
prove	O
also	O
that	O
the	O
result	O
remains	O
true	O
if	O
n	O
is	O
replaced	O
by	O
n	O
,	O
a	O
binomial	O
(	O
n	O
,	O
p	O
)	O
random	O
variable	B
independent	O
of	O
xi	O
,	O
...	O
,	O
x	O
n	O
,	O
where	O
p	O
e	O
(	O
0	O
,	O
1	O
)	O
.	O
that	O
is	O
,	O
m	O
becomes	O
0/	O
n	O
)	O
2=	O
:	O
:1	O
xi	O
if	O
n	O
>	O
0	O
and	O
0	O
otherwise	O
,	O
and	O
similarly	O
for	O
ct.	O
problem	O
16.3.	O
assume	O
that	O
p	O
=	O
1/2	O
,	O
and	O
that	O
both	O
class-conditional	B
densities	O
10	O
and	O
11	O
are	O
gaussian	B
on	O
r	O
with	O
unit	O
variance	O
,	O
but	O
different	O
means	O
.	O
we	O
use	O
the	O
maximum	B
likelihood	I
estimates	O
mo	O
,	O
iii	O
]	O
of	O
the	O
conditional	O
means	O
mo	O
=	O
e	O
{	O
xiy	O
=	O
o	O
}	O
and	O
m	O
]	O
=	O
e	O
{	O
xiy	O
=	O
i	O
}	O
to	O
obtain	O
the	O
plug-in	O
classifier	O
ge	O
.	O
show	O
that	O
e	O
{	O
(	O
mo	O
-	O
mo	O
)	O
2	O
}	O
=	O
00/	O
n	O
)	O
.	O
then	O
go	O
on	O
to	O
show	O
thate	O
{	O
l	O
(	O
ge	O
)	O
}	O
-	O
l*	O
:	O
:	O
:	O
oo/n	O
)	O
.	O
problems	O
and	O
exercises	O
277	O
problem	O
16.4.	O
show	O
that	O
the	O
following	O
classes	O
of	O
densities	O
on	O
r	O
constitute	O
exponential	B
families	O
:	O
(	O
1	O
)	O
gaussian	B
family	O
:	O
(	O
2	O
)	O
gamma	B
family	O
:	O
(	O
3	O
)	O
beta	B
family	O
:	O
{	O
(	O
4	O
)	O
rayleigh	O
family	O
:	O
rea	O
+	O
f3	O
)	O
r	O
(	O
a	O
)	O
r	O
(	O
f3	O
)	O
x	O
0'-1	O
tj-i	O
.	O
(	O
1	O
-	O
x	O
)	O
i	O
(	O
xeo.i	O
)	O
.	O
a	O
,	O
f3	O
>	O
0	O
}	O
.	O
,	O
e	O
;	O
af3	O
1-	O
ae-tj	O
/	O
(	O
2e	O
)	O
x	O
''	O
e-ex	O
/2	O
,	O
,	O
''	O
''	O
,	O
2	O
2	O
{	O
1	O
00	O
1	O
:	O
tj	O
!	O
r	O
(	O
j+	O
,	O
a	O
)	O
2	O
(	O
x	O
)	O
2j+a-i	O
-	O
:	O
e	O
f3	O
>	O
0	O
,	O
a	O
>	O
0	O
}	O
.	O
''	O
-	O
problem	O
16.5.	O
this	O
exercise	O
shows	O
that	O
one-parameter	O
classes	O
may	O
be	O
incredibly	O
rich	O
.	O
let	O
c	O
be	O
the	O
class	O
of	O
rules	O
of	O
the	O
form	O
ge	O
(	O
x	O
)	O
=	O
{	O
~	O
if	O
x	O
e	O
a+e	O
if	O
x	O
'/	O
:	O
a+e	O
,	O
where	O
x	O
e	O
r	O
,	O
8	O
e	O
r	O
is	O
a	O
parameter	O
,	O
and	O
a	O
is	O
a	O
union	O
of	O
intervals	O
on	O
the	O
real	O
line	O
.	O
equivalently	O
,	O
ge	O
=	O
ia+e	O
.	O
let	O
a	O
=	O
u	O
[	O
i	O
-	O
~	O
,	O
i	O
+	O
~	O
)	O
,	O
i	O
:	O
h	O
;	O
=1	O
2	O
2	O
where	O
b	O
i	O
,	O
b2	O
,	O
.••	O
are	O
bits	O
,	O
obtained	O
as	O
follows	O
:	O
first	O
list	O
all	O
sequences	O
of	O
length	O
l	O
,	O
then	O
those	O
of	O
length	O
2	O
,	O
et	O
cetera	O
,	O
so	O
that	O
(	O
b	O
i	O
,	O
b2	O
,	O
..•	O
)	O
is	O
a	O
concatenation	O
of	O
(	O
1,0	O
)	O
,	O
(	O
1	O
,	O
1	O
,	O
1,0,0	O
,	O
1,0,0	O
)	O
,	O
et	O
cetera	O
.	O
(	O
1	O
)	O
show	O
that	O
for	O
all	O
n	O
,	O
there	O
exists	O
a	O
set	O
{	O
x	O
1	O
,	O
...	O
,	O
x	O
n	O
}	O
c	O
r	O
that	O
can	O
be	O
shattered	O
by	O
a	O
set	O
from	O
{	O
a	O
+	O
e	O
:	O
e	O
en	O
}	O
.	O
conclude	O
that	O
the	O
vc	B
dimension	I
of	O
c	O
is	O
infinite	O
.	O
if	O
we	O
use	O
c	O
for	O
empirical	B
error	I
minimization	O
and	O
l	O
*	O
=	O
0	O
,	O
what	O
can	O
you	O
say	O
about	O
e	O
{	O
l	O
n	O
}	O
,	O
the	O
probability	O
of	O
error	O
of	O
the	O
selected	O
rule	B
?	O
(	O
2	O
)	O
problem	O
16.6.	O
continuation	O
.	O
let	O
x	O
be	O
uniform	B
on	O
8	O
+	O
[	O
i	O
-	O
1/2	O
,	O
i	O
+	O
1/2	O
)	O
with	O
probability	O
1/2i	O
,	O
i	O
2	O
:	O
1.	O
set	O
y	O
=	O
bi	O
if	O
x	O
e	O
8	O
+	O
[	O
i	O
-1/2	O
,	O
i	O
+	O
1/2	O
)	O
,	O
so	O
that	O
l*	O
=	O
o	O
.	O
(	O
1	O
)	O
derive	O
the	O
class	O
of	O
bayes	O
rules	O
.	O
(	O
2	O
)	O
work	O
out	O
the	O
details	O
of	O
the	O
generalized	B
kolmogorov-smirnov	O
distance	B
minimizer	O
based	O
on	O
the	O
class	O
of	O
(	O
1	O
)	O
.	O
(	O
3	O
)	O
provide	O
the	O
best	O
upper	O
bound	O
on	O
e	O
{	O
l	O
n	O
}	O
you	O
can	O
get	O
with	O
any	O
method	O
.	O
(	O
4	O
)	O
regardless	O
of	O
discrimination	O
,	O
how	O
would	O
you	O
estimate	B
e	O
?	O
derive	O
the	O
asymptotic	O
behavior	O
of	O
e	O
{	O
ln	O
}	O
for	O
the	O
plug-in	O
rule	O
based	O
on	O
your	O
estimate	B
of	O
e.	O
problem	O
16.7.	O
if	O
fe	O
=	O
{	O
all	O
uniform	B
densities	O
on	O
rectangles	O
of	O
rd	O
}	O
and	O
if	O
we	O
use	O
the	O
maximum	B
likelihood	I
estimates	O
of	O
p	O
,	O
80	O
,	O
(	O
h	O
derived	O
in	O
the	O
text	O
,	O
show	O
that	O
e	O
{	O
ln	O
}	O
-	O
l	O
*	O
=	O
o	O
(	O
1/n	O
)	O
.	O
278	O
16.	O
parametric	B
classification	I
problem	O
16.8.	O
continued	O
.	O
assume	O
that	O
the	O
true	O
class-conditional	B
densities	O
are	O
fa	O
,	O
f1	O
,	O
and	O
fa	O
,	O
ii	O
¢	O
.	O
:	O
fe·	O
with	O
the	O
same	O
maximum	B
likelihood	I
estimates	O
given	O
above	O
,	O
find	O
fa	O
,	O
f1	O
for	O
which	O
e	O
{	O
l	O
n	O
}	O
~	O
1	O
,	O
yet	O
l	O
*	O
=	O
o.	O
problem	O
16.9.	O
continued	O
.	O
show	O
that	O
the	O
o	O
(	O
1/n	O
)	O
rate	O
above	O
can	O
,	O
in	O
general	O
,	O
not	O
be	O
im	O
(	O
cid:173	O
)	O
proved	O
.	O
problem	O
16.10.	O
show	O
that	O
by	O
noting	O
that	O
the	O
supremum	O
in	O
the	O
definition	B
of	I
d	O
ks	O
may	O
be	O
replaced	O
by	O
a	O
maximum	O
over	O
nd	O
carefully	O
selected	O
points	O
of	O
nd	O
.	O
hint	O
:	O
use	O
the	O
idea	O
of	O
fingering	O
from	O
chapters	O
4	O
and	O
12.	O
problem	O
16.11.	O
prove	O
lemma	O
16.2	O
by	O
following	O
the	O
line	O
of	O
the	O
proof	O
of	O
the	O
vapnik	O
(	O
cid:173	O
)	O
chervonenkis	O
inequality	B
(	O
theorem	B
12.5	O
)	O
.	O
hint	O
:	O
in	O
the	O
second	O
symmetrization	B
step	O
observe	O
that	O
has	O
the	O
same	O
distribution	B
as	O
that	O
of	O
sup	O
1	O
~	O
t	O
ai	O
i	O
{	O
xi	O
ea	O
}	O
i·	O
aeb	O
n	O
i=l	O
problem	O
16.12.	O
minimum	O
distance	O
density	O
estimation	B
.	O
let	O
:	O
fe	O
=	O
{	O
fe	O
:	O
e	O
e	O
8	O
}	O
be	O
a	O
parametric	O
class	O
of	O
densities	O
on	O
rd	O
,	O
and	O
assume	O
that	O
the	O
i.i.d	O
.	O
sample	O
xl	O
,	O
...	O
,	O
xn	O
is	O
drawn	O
from	O
the	O
density	O
fe	O
e	O
:	O
fe	O
.	O
define	O
the	O
class	O
of	O
sets	O
and	O
define	O
the	O
minimum	O
distance	O
estimate	O
of	O
e	O
by	O
(	O
j	O
=	O
arg	O
min	O
d	O
a	O
(	O
pe	O
''	O
fin	O
)	O
,	O
e'ee	O
where	O
pe	O
,	O
is	O
the	O
distribution	B
belonging	O
to	O
the	O
density	O
fe	O
'	O
,	O
fin	O
is	O
the	O
empirical	B
distribution	O
defined	O
by	O
xl	O
,	O
...	O
,	O
x	O
n	O
,	O
and	O
dais	O
the	O
generalized	B
kolmogorov-smimov	O
distance	B
defined	O
by	O
aea	O
prove	O
that	O
if	O
a	O
has	O
finite	O
vc	B
dimension	I
,	O
then	O
da	O
(	O
p	O
,	O
q	O
)	O
=	O
sup	O
ip	O
(	O
a	O
)	O
-	O
q	O
(	O
a	O
)	O
i.	O
e	O
{	O
!	O
ifg-	O
(	O
x	O
)	O
-	O
fe	O
(	O
x	O
)	O
ldx	O
}	O
=	O
0	O
(	O
l/jn	O
)	O
.	O
for	O
more	O
information	O
on	O
minimum	O
distance	O
density	O
estimation	B
,	O
see	O
yatracos	O
(	O
1985	O
)	O
and	O
devroye	O
(	O
1987	O
)	O
.	O
hint	O
:	O
follow	O
the	O
steps	O
listed	O
below	O
:	O
(	O
1	O
)	O
f	O
ifg-	O
-	O
fe	O
i	O
:	O
s	O
2da	O
(	O
pg-	O
,	O
pe	O
)	O
(	O
use	O
scheff6	O
's	O
theorem	B
)	O
.	O
(	O
2	O
)	O
da	O
(	O
pg-	O
,	O
pe	O
)	O
:	O
s	O
2da	O
(	O
pe	O
,	O
fin	O
)	O
(	O
proceed	O
as	O
in	O
the	O
text	O
)	O
.	O
(	O
3	O
)	O
finish	O
the	O
proof	O
by	O
applying	O
alexander	O
's	O
improvement	O
of	O
the	O
vapnik-chervonen	O
(	O
cid:173	O
)	O
kis	O
inequality	B
(	O
theorem	B
12.10	O
)	O
.	O
17	O
generalized	O
linear	O
discrimination	O
the	O
classifiers	O
we	O
study	O
here	O
have	O
their	O
roots	O
in	O
the	O
fourier	O
series	O
estimate	B
or	O
other	O
series	O
estimates	O
of	O
an	O
unknown	O
density	O
,	O
potential	O
function	O
methods	O
(	O
see	O
chapter	O
10	O
)	O
,	O
and	O
generalized	O
linear	O
classifiers	O
.	O
all	O
these	O
estimators	O
can	O
be	O
put	O
into	O
the	O
following	O
form	O
:	O
classify	O
x	O
as	O
belonging	O
to	O
class	O
0	O
if	O
k	O
l	O
an	O
,	O
jo/j	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
0	O
,	O
j=l	O
where	O
the	O
0/	O
j	O
's	O
are	O
fixed	O
functions	O
,	O
forming	O
a	O
base	O
for	O
the	O
series	O
estimate	B
,	O
an	O
,	O
j	O
is	O
a	O
fixed	O
function	O
of	O
the	O
training	O
data	O
,	O
and	O
k	O
controls	O
the	O
amount	O
of	O
smoothing	O
.	O
when	O
the	O
o//s	O
are	O
the	O
usual	O
trigonometric	B
basis	O
,	O
then	O
this	O
leads	O
to	O
the	O
fourier	O
series	O
classifier	B
studied	O
by	O
greblicki	O
and	O
pawlak	O
(	O
1981	O
;	O
1982	O
)	O
.	O
when	O
the	O
o//s	O
form	O
an	O
orthonormal	O
system	O
based	O
upon	O
hermite	O
polynomials	O
,	O
we	O
obtain	O
the	O
classifiers	O
studied	O
by	O
greblicki	O
(	O
1981	O
)	O
,	O
and	O
greblicki	O
and	O
pawlak	O
(	O
1983	O
;	O
1985	O
)	O
.	O
when	O
{	O
o/	O
j	O
(	O
x	O
)	O
}	O
is	O
the	O
collection	O
of	O
all	O
products	O
of	O
components	O
of	O
x	O
(	O
such	O
as	O
1	O
,	O
(	O
x	O
(	O
i	O
)	O
k	O
,	O
(	O
x	O
(	O
i	O
)	O
k	O
(	O
x	O
(	O
j	O
)	O
y	O
,	O
etcetera	O
)	O
,	O
we	O
obtain	O
the	O
polynomial	B
method	O
of	O
specht	O
(	O
1971	O
)	O
.	O
we	O
start	O
with	O
classification	O
based	O
on	O
fourier	O
series	O
expansion	O
,	O
which	O
has	O
its	O
origins	O
in	O
fourier	O
series	O
density	B
estimation	I
,	O
which	O
,	O
in	O
tum	O
,	O
was	O
developed	O
by	O
cencov	O
(	O
1962	O
)	O
,	O
schwartz	O
(	O
1967	O
)	O
,	O
kronmal	O
and	O
tarter	O
(	O
1968	O
)	O
,	O
tarter	O
and	O
kronmal	O
(	O
1970	O
)	O
,	O
and	O
specht	O
(	O
1971	O
)	O
.	O
its	O
use	O
in	O
classification	O
was	O
considered	O
by	O
greblicki	O
(	O
1981	O
)	O
,	O
specht	O
(	O
1971	O
)	O
,	O
greblicki	O
and	O
pawlak	O
(	O
1981	O
;	O
1982	O
;	O
1983	O
)	O
,	O
and	O
others	O
.	O
280	O
17.	O
generalized	O
linear	O
discrimination	O
17.1	O
fourier	O
series	O
classification	O
let	O
f	O
be	O
the	O
density	O
of	O
x	O
,	O
and	O
assume	O
that	O
f	O
e	O
l2	O
(	O
a	O
)	O
,	O
that	O
is	O
,	O
f	O
:	O
:	O
:	O
:	O
0	O
,	O
f	O
f	O
(	O
x	O
)	O
dx	O
=	O
1	O
and	O
f	O
f2	O
(	O
x	O
)	O
dx	O
<	O
00	O
,	O
and	O
recall	O
that	O
a	O
denotes	O
the	O
lebesgue	O
measure	B
on	O
nd	O
.	O
let	O
,	O
,/ii	O
,	O
0/2	O
,	O
...	O
be	O
a	O
complete	O
orthonormal	O
set	O
of	O
bounded	O
functions	O
in	O
l	O
2	O
(	O
a	O
)	O
with	O
sup	O
j	O
,	O
x	O
100j	O
(	O
x	O
)	O
1	O
:	O
:	O
:	O
:	O
b	O
<	O
00.	O
an	O
orthonormal	O
set	O
of	O
functions	O
{	O
o/j	O
}	O
in	O
l	O
2	O
(	O
tl	O
)	O
is	O
such	O
that	O
f	O
l/jjo/i	O
=	O
iu==j	O
}	O
for	O
all	O
i	O
,	O
j.	O
it	O
is	O
complete	O
if	O
f	O
al/ji	O
=	O
0	O
for	O
all	O
i	O
for	O
some	O
function	O
a	O
e	O
l	O
2	O
(	O
a	O
)	O
implies	O
that	O
a	O
'=	O
0	O
almost	O
everywhere	O
(	O
with	O
respect	O
to	O
a	O
)	O
.	O
if	O
f	O
e	O
l	O
2	O
(	O
a	O
)	O
,	O
then	O
the	O
class-conditional	B
densities	O
fo	O
and	O
fl	O
exist	O
,	O
and	O
are	O
in	O
l2	O
(	O
a	O
)	O
.	O
then	O
the	O
function	O
a	O
(	O
x	O
)	O
=	O
p/1	O
(	O
x	O
)	O
-	O
(	O
1	O
-	O
p	O
)	O
fo	O
(	O
x	O
)	O
=	O
(	O
217	O
(	O
x	O
)	O
-	O
l	O
)	O
f	O
(	O
x	O
)	O
is	O
in	O
l	O
2	O
(	O
a	O
)	O
as	O
well	O
.	O
recall	O
that	O
the	O
bayes	O
decision	O
is	O
given	O
by	O
*	O
g	O
(	O
x	O
)	O
=	O
{	O
o	O
if	O
a	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
0	O
1	O
otherwise	O
.	O
let	O
the	O
bounded	O
functions	O
l/jl	O
,	O
l/j2	O
,	O
...	O
form	O
a	O
complete	O
orthonormal	O
basis	O
,	O
and	O
let	O
the	O
fourier	O
representation	O
of	O
a	O
be	O
given	O
by	O
00	O
a	O
=	O
lajl/jj	O
.	O
j=l	O
the	O
above	O
convergence	O
is	O
understood	O
in	O
l	O
2	O
(	O
a	O
)	O
,	O
that	O
is	O
,	O
the	O
infinite	O
sum	O
means	O
that	O
the	O
coefficients	O
are	O
given	O
by	O
we	O
approximate	O
a	O
(	O
x	O
)	O
by	O
a	O
truncation	O
of	O
its	O
fourier	O
representation	O
to	O
finitely	O
many	O
terms	O
,	O
and	O
use	O
the	O
data	O
dn	O
to	O
estimate	B
the	O
coefficients	O
appearing	O
in	O
this	O
sum	O
.	O
formally	O
,	O
consider	O
the	O
classification	O
rule	B
(	O
17.1	O
)	O
where	O
the	O
estimates	O
an	O
,	O
j	O
of	O
a	O
j	O
are	O
very	O
easy	O
to	O
compute	O
:	O
before	O
discussing	O
the	O
properties	O
of	O
these	O
rules	O
,	O
we	O
list	O
a	O
few	O
examples	O
of	O
complete	O
orthonormal	O
systems	O
on	O
the	O
real	O
line	O
.	O
some	O
of	O
these	O
systems	O
contain	O
functions	O
17.1	O
fourier	O
series	O
classification	O
281	O
on	O
a	O
bounded	O
interval	O
.	O
these	O
,	O
of	O
course	O
,	O
can	O
only	O
be	O
used	O
if	O
the	O
distribution	B
of	O
x	O
is	O
concentrated	O
on	O
an	O
interval	O
.	O
the	O
completeness	O
of	O
these	O
systems	O
may	O
typically	O
be	O
checked	O
via	O
the	O
stone-weierstrass	O
theorem	B
(	O
theorem	B
a.9	O
)	O
.	O
a	O
general	O
way	O
of	O
producing	O
complete	B
orthonormal	I
systems	O
on	O
n	O
d	O
is	O
taking	O
products	O
of	O
one	O
(	O
cid:173	O
)	O
dimensional	O
functions	O
of	O
the	O
d	O
components	O
,	O
as	O
sketched	O
in	O
problem	O
17.1.	O
for	O
more	O
information	O
on	O
orthogonal	O
series	O
,	O
we	O
refer	O
to	O
sansone	O
(	O
1969	O
)	O
,	O
szeg6	O
(	O
1959	O
)	O
,	O
and	O
zygmund	O
(	O
1959	O
)	O
.	O
(	O
1	O
)	O
the	O
standard	B
trigonometric	O
basis	O
on	O
the	O
interval	O
[	O
-jr	O
,	O
jr	O
]	O
is	O
formed	O
by	O
the	O
functions	O
%	O
(	O
x	O
)	O
=	O
~	O
'	O
o/2i-l	O
(	O
x	O
)	O
=	O
1	O
v~	O
cos	O
(	O
ix	O
)	O
r	O
:	O
;	O
;	O
'	O
o/2i	O
(	O
x	O
)	O
=	O
vjr	O
r	O
:	O
;	O
;	O
'	O
i	O
=	O
1,2	O
,	O
...	O
.	O
vjr	O
sin	O
(	O
ix	O
)	O
this	O
is	O
a	O
complete	O
orthonormal	O
system	O
in	O
l	O
2	O
(	O
[	O
-jr	O
,	O
n	O
]	O
)	O
.	O
(	O
2	O
)	O
the	O
legendre	O
polynomials	O
form	O
a	O
complete	O
orthonormal	O
basis	O
over	O
the	O
in	O
(	O
cid:173	O
)	O
terval	O
[	O
-1	O
,	O
1	O
]	O
:	O
i	O
)	O
·x	O
=	O
-	O
-	O
-	O
-	O
x	O
-	O
i	O
)	O
,	O
i	O
=0	O
,	O
1,2	O
,	O
...	O
.	O
f	O
!	O
{	O
ii	O
+	O
1	O
1	O
d	O
i	O
(	O
2	O
2	O
2ii	O
!	O
dxi	O
(	O
o/l	O
(	O
)	O
(	O
3	O
)	O
the	O
set	O
of	O
hermite	O
functions	O
is	O
complete	O
and	O
orthonormal	O
over	O
the	O
whole	O
real	O
line	O
:	O
(	O
4	O
)	O
functions	O
of	O
the	O
laguerre	O
basis	O
are	O
defined	O
on	O
[	O
0	O
,	O
(	O
0	O
)	O
by	O
rei	O
+	O
1	O
)	O
o/i	O
(	O
x	O
)	O
=	O
.	O
(	O
r	O
(	O
z	O
+a	O
+	O
1	O
)	O
-ex	O
x	O
)	O
1/2	O
1	O
d	O
i	O
-	O
:	O
--	O
-	O
.	O
x	O
x	O
e	O
z	O
!	O
dx	O
1	O
(	O
i+ex	O
-x	O
)	O
e	O
.	O
,	O
z=0,1,2	O
,	O
...	O
,	O
where	O
a	O
>	O
-1	O
is	O
a	O
parameter	O
of	O
the	O
system	O
.	O
a	O
complete	O
orthonormal	O
system	O
over	O
the	O
whole	O
real	O
line	O
may	O
be	O
obtained	O
by	O
(	O
5	O
)	O
the	O
haar	O
basis	O
contains	O
functions	O
on	O
[	O
0	O
,	O
1	O
]	O
that	O
take	O
three	O
different	O
values	O
.	O
it	O
is	O
orthonormal	O
and	O
complete	O
.	O
functions	O
with	O
finitely	O
many	O
values	O
have	O
computational	O
advantages	O
in	O
pattern	O
recognition	O
.	O
write	O
the	O
integer	O
i	O
:	O
:	O
:	O
:	O
0	O
as	O
i	O
=	O
2k	O
+	O
j	O
,	O
where	O
k	O
=	O
llog2	O
i	O
j	O
(	O
i.e.	O
,	O
0	O
:	O
:	O
:	O
:	O
j	O
<	O
2k	O
)	O
.	O
then	O
o/i	O
(	O
x	O
)	O
=	O
{	O
2k/2	O
-	O
k/2	O
2	O
0	O
if	O
x	O
e	O
(	O
(	O
j	O
-	O
1	O
)	O
j2k	O
,	O
(	O
j	O
-	O
if	O
x	O
e	O
(	O
(	O
j	O
-	O
otherwise	O
.	O
ij2	O
)	O
j2k	O
,	O
jj2k	O
)	O
ij2	O
)	O
j2k	O
)	O
282	O
17.	O
generalized	O
linear	O
discrimination	O
(	O
6	O
)	O
functions	O
on	O
[	O
0	O
,	O
1	O
]	O
of	O
the	O
rademacher	O
basis	O
take	O
only	O
two	O
values	O
,	O
-1	O
and	O
1.	O
it	O
is	O
an	O
orthonormal	O
system	O
,	O
but	O
it	O
is	O
not	O
complete	O
:	O
1/io	O
(	O
x	O
)	O
==	O
1	O
,	O
1/ii	O
(	O
x	O
)	O
=	O
(	O
_l	O
)	O
lixj	O
,	O
i	O
=	O
1,2	O
,	O
...	O
.	O
the	O
basis	O
may	O
be	O
completed	O
as	O
follows	O
:	O
write	O
i	O
=	O
l~=1	O
2ij	O
such	O
that	O
0	O
:	O
.s	O
il	O
<	O
i2	O
<	O
...	O
<	O
ik	O
.	O
this	O
form	O
is	O
unique	O
.	O
define	O
where	O
the	O
1/1	O
j	O
's	O
are	O
the	O
rademacher	O
functions	O
.	O
the	O
resulting	O
basis	O
1/1~	O
,	O
1/1	O
{	O
,	O
...	O
is	O
orthonormal	O
and	O
complete	O
,	O
and	O
is	O
called	O
the	O
walsh	O
basis	O
.	O
there	O
is	O
no	O
system	O
of	O
basis	O
functions	O
that	O
is	O
better	O
than	O
another	O
for	O
all	O
distribu	O
(	O
cid:173	O
)	O
tions	O
.	O
in	O
selecting	O
the	O
basis	O
of	O
a	O
fourier	O
series	O
rule	B
,	O
the	O
designer	O
must	O
use	O
a	O
mixture	O
of	O
intuition	O
,	O
error	B
estimation	I
,	O
and	O
computational	O
concerns	O
.	O
we	O
have	O
the	O
following	O
consistency	B
theorem	O
:	O
theorem	B
17.1.	O
let	O
{	O
1/io	O
,	O
1/11	O
,	O
...	O
}	O
be	O
a	O
complete	O
orthonormal	O
basis	O
on	O
rd	O
such	O
that	O
for	O
some	O
b	O
<	O
00	O
,	O
!	O
1/ii	O
(	O
x	O
)	O
!	O
:	O
:s	O
b	O
for	O
each	O
i	O
and	O
x.	O
assume	O
that	O
the	O
class-conditional	B
densities	O
fo	O
and	O
fl	O
exist	O
and	O
are	O
in	O
l2ca	O
)	O
.	O
if	O
kn	O
~	O
00	O
and	O
-	O
~	O
0	O
as	O
n	O
~	O
00	O
,	O
kn	O
n	O
then	O
the	O
fourier	O
series	O
classification	O
rule	B
defined	O
in	O
(	O
17.1	O
)	O
is	O
consistent	O
:	O
lim	O
el	O
(	O
gn	O
)	O
=	O
l	O
*	O
.	O
n	O
--	O
-+oo	O
if	O
then	O
kn	O
~	O
00	O
and	O
-	O
-	O
-	O
~	O
0	O
as	O
n	O
~	O
00	O
,	O
kn	O
logn	O
n	O
lim	O
l	O
(	O
gn	O
)	O
=	O
l	O
*	O
n	O
--	O
-+oo	O
with	O
probability	O
one	O
,	O
that	O
is	O
,	O
the	O
rule	B
is	O
strongly	O
consistent	O
.	O
proof	O
.	O
first	O
observe	O
that	O
the	O
an	O
,	O
)	O
's	O
are	O
unbiased	O
estimates	O
of	O
the	O
a	O
j	O
's	O
:	O
e	O
{	O
an	O
,	O
j	O
}	O
e	O
{	O
(	O
2y	O
-	O
1	O
)	O
1/ij	O
(	O
x	O
)	O
}	O
=	O
e	O
{	O
e	O
{	O
(	O
2y	O
-	O
1	O
)	O
1/ij	O
(	O
x	O
)	O
!	O
x	O
}	O
}	O
e	O
{	O
1/ij	O
(	O
x	O
)	O
e	O
{	O
(	O
2y	O
-	O
l	O
)	O
ix	O
}	O
}	O
=	O
e	O
{	O
(	O
217	O
(	O
x	O
)	O
-	O
1	O
)	O
1/ij	O
(	O
x	O
)	O
}	O
=	O
j	O
(	O
217	O
(	O
x	O
)	O
-	O
1	O
)	O
1/ij	O
(	O
x	O
)	O
f	O
(	O
x	O
)	O
dx	O
=	O
j	O
1/i/x	O
)	O
a	O
(	O
x	O
)	O
dx	O
and	O
that	O
their	O
variance	O
can	O
be	O
bounded	O
from	O
above	O
as	O
follows	O
:	O
17.1	O
fourier	O
series	O
classification	O
283	O
n	O
!	O
1/i	O
]	O
(	O
x	O
)	O
(	O
2ry	O
(	O
x	O
)	O
-	O
1	O
)	O
2	O
f	O
(	O
x	O
)	O
dx	O
-	O
ct	O
;	O
n	O
<	O
j	O
<	O
_	O
,	O
b2	O
n	O
b2	O
-	O
ct~	O
n	O
where	O
we	O
used	O
the	O
boundedness	O
of	O
the	O
l/j/s	O
.	O
by	O
parseval	O
's	O
identity	O
,	O
therefore	O
,	O
exploiting	O
orthonormality	O
of	O
the	O
1/1	O
j	O
's	O
,	O
we	O
have	O
j	O
(	O
a	O
(	O
x	O
)	O
-	O
t	O
a	O
n	O
,	O
j1jfm	O
)	O
2	O
dx	O
=	O
j	O
a	O
(	O
x	O
)	O
dx+	O
j	O
(	O
tanamr	O
dx	O
2	O
thus	O
,	O
the	O
expected	O
l	O
2-error	O
is	O
bounded	O
as	O
follows	O
:	O
k	O
''	O
lvar	O
{	O
ctn	O
,	O
j	O
}	O
+	O
l	O
j=l	O
(	O
x	O
)	O
j=kn+l	O
ctj	O
<	O
284	O
17.	O
generalized	O
linear	O
discrimination	O
since	O
l~l	O
a	O
;	O
<	O
00	O
,	O
the	O
second	O
term	O
tends	O
to	O
zero	O
if	O
kn	O
--	O
-+	O
00.	O
if	O
at	O
the	O
same	O
time	O
kn	O
/	O
n	O
--	O
-+	O
0	O
,	O
then	O
the	O
expected	O
l2-error	O
converges	O
to	O
zero	O
,	O
that	O
is	O
,	O
the	O
estimate	B
is	O
consistent	O
in	O
l	O
2•	O
now	O
,	O
convergence	O
of	O
the	O
error	O
probability	O
follows	O
from	O
problem	O
2.11.	O
to	O
prove	O
strong	B
consistency	I
(	O
i.e.	O
,	O
convergence	O
with	O
probability	O
one	O
)	O
,	O
fix	O
e	O
>	O
0	O
,	O
and	O
assume	O
that	O
n	O
is	O
so	O
large	O
that	O
00	O
l	O
a	O
;	O
<	O
e/2	O
.	O
j=kn+l	O
then	O
kn	O
<	O
l	O
p	O
{	O
(	O
an	O
,	O
j	O
-	O
aj	O
)	O
2	O
>	O
e/	O
(	O
2kn	O
)	O
}	O
j=l	O
(	O
by	O
the	O
union	O
bound	O
)	O
<	O
where	O
we	O
used	O
hoeffding	O
's	O
inequality	B
(	O
theorem	B
8.1	O
)	O
.	O
because	O
k	O
n	O
log	O
n	O
/	O
n	O
--	O
-+	O
0	O
,	O
the	O
upper	O
bound	O
is	O
eventually	O
smaller	O
than	O
e-21ogn	O
=	O
n-2	O
,	O
which	O
is	O
summable	O
.	O
the	O
borel-cantelli	O
lemma	O
yields	O
strong	B
l2	O
consistency	B
.	O
strong	B
consistency	I
of	O
the	O
classifier	B
then	O
follows	O
from	O
problem	O
2.11	O
.	O
0	O
remark	O
.	O
it	O
is	O
clear	O
from	O
the	O
inequality	B
that	O
the	O
rate	B
of	I
convergence	I
is	O
determined	O
by	O
the	O
choice	O
of	O
kn	O
.	O
if	O
kn	O
is	O
small	O
,	O
then	O
the	O
first	O
term	O
,	O
which	O
corresponds	O
to	O
the	O
estimation	B
error	I
,	O
is	O
small	O
,	O
but	O
the	O
approximation	B
error	I
,	O
expressed	O
by	O
the	O
second	O
term	O
,	O
is	O
larger	O
.	O
for	O
large	O
kn	O
'	O
the	O
17.2	O
generalized	O
linear	O
classification	O
285	O
situation	O
is	O
reversed	O
.	O
the	O
optimal	O
choice	O
of	O
kn	O
depends	O
on	O
how	O
fast	O
the	O
second	O
term	O
goes	O
to	O
zero	O
as	O
kn	O
grows	O
.	O
this	O
depends	O
on	O
other	O
properties	O
of	O
f	O
,	O
such	O
as	O
the	O
size	O
of	O
its	O
tail	O
and	O
its	O
smoothness	O
.	O
0	O
remark	O
.	O
consistency	B
in	O
theorem	B
17.1	O
is	O
not	O
universal	B
,	O
since	O
we	O
needed	O
to	O
as	O
(	O
cid:173	O
)	O
sume	O
the	O
existence	O
of	O
square	O
integrable	O
conditional	O
densities	O
.	O
this	O
,	O
however	O
,	O
is	O
not	O
a	O
restrictive	O
assumption	O
in	O
some	O
practical	O
situations	O
.	O
for	O
example	O
,	O
if	O
the	O
observa	O
(	O
cid:173	O
)	O
tions	O
are	O
corrupted	O
by	O
additive	O
gaussian	O
noise	O
,	O
then	O
the	O
conditions	O
of	O
the	O
theorem	B
hold	O
(	O
problem	O
17.2	O
)	O
.	O
however	O
,	O
if	O
f1	O
,	O
does	O
not	O
have	O
a	O
density	O
,	O
the	O
method	O
may	O
be	O
inconsistent	O
(	O
see	O
problem	O
17.4	O
)	O
.	O
0	O
fourier	O
series	O
classifiers	O
have	O
two	O
rather	O
unattractive	O
features	O
in	O
general	O
:	O
(	O
i	O
)	O
they	O
are	O
not	O
invariant	O
under	O
translations	O
of	O
the	O
coordinate	O
space	O
.	O
(	O
ii	O
)	O
most	O
series	O
classifiers	O
are	O
not	O
local	O
in	O
nature-points	O
at	O
arbitrary	O
distances	O
from	O
x	O
affect	O
the	O
decision	O
at	O
x.	O
in	O
kernel	O
and	O
nearest	B
neighbor	I
rules	I
,	O
the	O
locality	O
is	O
easily	O
controlled	O
.	O
17.2	O
generalized	O
linear	O
classification	O
when	O
x	O
is	O
purely	O
atomic	O
or	O
singular	O
continuous	O
,	O
theorem	B
17.1	O
is	O
not	O
applicable	O
.	O
a	O
theme	O
of	O
this	O
book	O
is	O
that	O
pattern	O
recognition	O
can	O
be	O
developed	O
in	O
a	O
distribution	B
(	O
cid:173	O
)	O
free	O
manner	O
since	O
,	O
after	O
all	O
,	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
is	O
not	O
known	O
.	O
besides	O
,	O
even	O
if	O
we	O
had	O
an	O
i.i.d	O
.	O
sample	O
(	O
x	O
1	O
,	O
y1	O
)	O
,	O
.••	O
,	O
(	O
xn	O
,	O
yn	O
)	O
at	O
our	O
disposal	O
,	O
we	O
do	O
not	O
know	O
of	O
any	O
test	O
for	O
verifying	O
whether	O
x	O
has	O
a	O
square	O
integrable	O
density	O
.	O
so	O
,	O
we	O
proceed	O
a	O
bit	O
differently	O
to	O
develop	O
universally	O
consistent	O
rules	O
.	O
to	O
generalize	O
fourier	O
series	O
classifiers	O
,	O
let	O
1/11	O
,	O
1/12	O
,	O
...	O
be	O
bounded	O
functions	O
on	O
nd	O
.	O
these	O
functions	O
do	O
not	O
necessarily	O
form	O
an	O
orthonormal	O
basis	O
of	O
l2	O
.	O
consider	O
the	O
classifier	B
where	O
the	O
coefficients	O
an	O
,	O
)	O
are	O
some	O
functions	O
of	O
the	O
data	O
dn	O
.	O
this	O
may	O
be	O
viewed	O
as	O
a	O
generalization	O
of	O
fourier	O
series	O
rules	O
,	O
where	O
the	O
coefficients	O
were	O
unbiased	O
estimates	O
ofthe	O
fourier	O
coefficients	O
of	O
a	O
(	O
x	O
)	O
=	O
(	O
2rj	O
(	O
x	O
)	O
-1	O
)	O
f	O
(	O
x	O
)	O
.	O
here	O
we	O
will	O
con	O
(	O
cid:173	O
)	O
sider	O
some	O
other	O
choices	O
of	O
the	O
coefficients	O
.	O
observe	O
that	O
gn	O
is	O
a	O
generalized	O
linear	B
classifier	I
,	O
as	O
defined	O
in	O
chapter	O
13.	O
an	O
intuitively	O
appealing	O
way	O
to	O
determine	O
the	O
coefficients	O
is	O
to	O
pick	O
them	O
to	O
minimize	O
the	O
empirical	B
error	I
committed	O
on	O
dn	O
,	O
as	O
in	O
empirical	O
risk	O
minimization	O
.	O
the	O
critical	O
parameter	O
here	O
is	O
kn	O
,	O
the	O
number	O
of	O
basis	O
functions	O
used	O
in	O
the	O
linear	O
combination	O
.	O
if	O
we	O
keep	O
k	O
n	O
fixed	O
,	O
then	O
as	O
we	O
saw	O
in	O
chapter	O
13	O
,	O
the	O
error	O
probability	O
of	O
the	O
selected	O
rule	B
converges	O
quickly	O
to	O
that	O
of	O
the	O
best	O
classifier	B
of	O
the	O
above	O
form	O
.	O
however	O
,	O
for	O
some	O
distributions	O
,	O
this	O
infimum	O
may	O
be	O
far	O
from	O
the	O
bayes	O
risk	O
,	O
so	O
it	O
is	O
useful	O
to	O
let	O
kn	O
grow	O
as	O
n	O
becomes	O
286	O
17.	O
generalized	O
linear	O
discrimination	O
larger	O
.	O
however	O
,	O
choosing	O
kn	O
too	O
large	O
may	O
result	O
in	O
overfitting	O
the	O
data	O
.	O
using	O
the	O
terminology	O
introduced	O
in	O
chapter	O
12	O
,	O
let	O
c	O
(	O
kn	O
)	O
be	O
the	O
class	O
of	O
classifiers	O
of	O
the	O
form	O
where	O
the	O
a	O
j	O
's	O
range	O
through	O
n.	O
choose	O
the	O
coefficients	O
to	O
minimize	O
the	O
empirical	B
error	I
let	O
gn	O
=	O
¢	O
:	O
be	O
the	O
corresponding	O
classifier	B
.	O
we	O
recall	O
from	O
chapter	O
13	O
that	O
the	O
)	O
is	O
not	O
more	O
than	O
kn	O
.	O
therefore	O
,	O
by	O
theorem	B
13.12	O
,	O
for	O
every	O
vc	B
dimension	I
of	O
c	O
(	O
kll	O
n	O
>	O
2kn	O
+	O
1	O
,	O
where	O
h	O
is	O
the	O
binary	B
entropy	O
function	O
.	O
the	O
right-hand	O
side	O
is	O
e-ne2	O
j128+o	O
(	O
n	O
)	O
if	O
kn	O
=	O
o	O
(	O
n	O
)	O
.	O
however	O
,	O
to	O
obtain	O
consistency	B
,	O
we	O
need	O
to	O
know	O
how	O
close	O
inf	O
<	O
/	O
>	O
ec	O
(	O
knj	O
l	O
(	O
¢	O
)	O
is	O
to	O
l	O
*	O
.	O
this	O
obviously	O
depends	O
on	O
the	O
choice	O
of	O
the	O
1/1/	O
s	O
,	O
as	O
well	O
as	O
on	O
the	O
distribution	B
.	O
if	O
kn	O
is	O
not	O
allowed	O
to	O
grow	O
with	O
n	O
,	O
and	O
is	O
bounded	O
by	O
a	O
number	O
k	O
,	O
then	O
it	O
follows	O
from	O
theorem	B
2.2	O
that	O
for	O
every	O
b	O
>	O
°	O
universal	B
consistency	I
eludes	O
us	O
,	O
as	O
for	O
some	O
distribution	B
inf	O
<	O
/	O
>	O
ec	O
(	O
k	O
)	O
l	O
(	O
¢	O
)	O
-	O
l	O
*	O
>	O
0.	O
inf	O
l	O
(	O
¢	O
)	O
-	O
l	O
*	O
<	O
/	O
>	O
ec	O
(	O
kn	O
)	O
<	O
inf	O
[	O
al	O
,	O
···	O
,	O
akn	O
1i	O
!	O
x	O
,	O
,	O
:	O
sb	O
1	O
(	O
21j	O
(	O
x	O
)	O
-	O
1	O
)	O
-	O
taj1/lj	O
(	O
x	O
)	O
1	O
p.	O
,	O
(	O
dx	O
)	O
+	O
[	O
j=l	O
1i	O
!	O
x	O
''	O
>	O
b	O
p.	O
,	O
(	O
dx	O
)	O
.	O
the	O
limit	O
supremum	O
of	O
the	O
right-hand	O
side	O
can	O
be	O
arbitrarily	O
close	O
to	O
zero	O
for	O
all	O
distributions	O
if	O
kn	O
~	O
00	O
as	O
n	O
~	O
00	O
and	O
the	O
set	O
of	O
functions	O
is	O
dense	O
in	O
ll	O
(	O
p.	O
,	O
)	O
on	O
balls	O
of	O
the	O
form	O
{	O
x	O
:	O
ilx	O
ii	O
:	O
:	O
:	O
b	O
}	O
for	O
all	O
p.	O
,	O
.	O
this	O
means	O
that	O
given	O
any	O
probability	O
measure	B
p.	O
''	O
and	O
function	O
f	O
with	O
j	O
ifldp.	O
,	O
<	O
00	O
,	O
for	O
every	O
e	O
>	O
0	O
,	O
there	O
exists	O
an	O
integer	O
k	O
and	O
coefficients	O
ai	O
,	O
...	O
,	O
ak	O
such	O
that	O
thus	O
,	O
we	O
have	O
obtained	O
the	O
following	O
consistency	B
result	O
:	O
problems	O
and	O
exercises	O
287	O
theorem	B
17.2.	O
let	O
0/1	O
,	O
0/2	O
,	O
...	O
be	O
a	O
sequence	O
of	O
functions	O
such	O
that	O
the	O
set	O
of	O
all	O
finite	O
linear	O
combinations	O
of	O
the	O
0/	O
j	O
's	O
is	O
dense	O
in	O
l	O
1	O
(	O
jl	O
)	O
on	O
balls	O
of	O
the	O
form	O
{	O
x	O
:	O
ilx	O
ii	O
s	O
b	O
}	O
for	O
any	O
probability	O
measure	B
jl	O
.	O
then	O
then	O
gn	O
is	O
strongly	O
universally	O
consistent	O
when	O
kn	O
--	O
-+	O
00	O
and	O
-	O
--	O
-+	O
°	O
as	O
n	O
--	O
-+	O
00.	O
kn	O
n	O
remark	O
.	O
to	O
see	O
that	O
the	O
statement	O
of	O
theorem	O
17.2	O
is	O
not	O
vacuous	O
,	O
note	O
that	O
dense	O
(	O
cid:173	O
)	O
ness	O
in	O
l	O
1	O
(	O
jl	O
)	O
on	O
balls	O
follows	O
from	O
denseness	B
with	O
respect	O
to	O
the	O
supremum	O
norm	O
on	O
balls	O
.	O
denseness	B
in	O
the	O
loo	O
sense	O
may	O
be	O
checked	O
by	O
the	O
stone-weierstrass	O
the	O
(	O
cid:173	O
)	O
orem	O
(	O
theorem	B
a.9	O
)	O
.	O
for	O
example	O
,	O
the	O
class	O
of	O
all	O
polynomial	B
classifiers	O
satisfies	O
the	O
conditions	O
of	O
the	O
theorem	B
.	O
this	O
class	O
can	O
be	O
obtained	O
by	O
choosing	O
the	O
func	O
(	O
cid:173	O
)	O
tions	O
0/1	O
,	O
0/2	O
,	O
...	O
as	O
the	O
simple	O
polynomials	O
x	O
(	O
l	O
)	O
,	O
...	O
,	O
x	O
cd	O
)	O
,	O
xci	O
)	O
x	O
(	O
2	O
)	O
,	O
...	O
.	O
similarly	O
,	O
the	O
functions	O
o/j	O
can	O
be	O
chosen	O
as	O
bases	O
of	O
a	O
trigonometric	O
series	O
.	O
0	O
remark	O
.	O
the	O
histogram	B
rule	I
.	O
it	O
is	O
clear	O
that	O
theorem	B
17.2	O
can	O
be	O
modified	O
in	O
the	O
following	O
way	O
:	O
let	O
0/	O
j	O
,	O
b	O
j	O
=	O
1	O
,	O
...	O
,	O
k	O
;	O
k	O
=	O
1	O
,	O
2	O
,	O
...	O
,	O
be	O
functions	O
such	O
that	O
for	O
every	O
fell	O
(	O
f-l	O
)	O
(	O
with	O
f-l	O
concentrated	O
on	O
a	O
bounded	O
ball	O
)	O
and	O
e	O
>	O
°	O
there	O
is	O
a	O
ko	O
such	O
that	O
for	O
all	O
k	O
>	O
ko	O
there	O
is	O
a	O
function	O
of	O
the	O
form	O
l~=l	O
a	O
j	O
0/	O
j	O
,	O
k	O
whose	O
distance	B
from	O
f	O
in	O
l	O
1	O
(	O
f-l	O
)	O
is	O
smaller	O
than	O
e.	O
let	O
c	O
(	O
kl1	O
)	O
be	O
the	O
class	O
of	O
generalized	O
linear	O
classifiers	O
based	O
on	O
the	O
functions	O
o/l	O
,	O
kn	O
,	O
•••	O
,	O
o/k	O
''	O
,	O
kn	O
•	O
if	O
k	O
n	O
--	O
-+	O
00	O
and	O
kn	O
/	O
n	O
--	O
-+	O
0	O
,	O
then	O
the	O
classifier	B
gn	O
that	O
minimizes	O
the	O
empirical	B
error	I
over	O
c	O
(	O
kn	O
)	O
is	O
strongly	O
universally	O
consistent	O
.	O
this	O
modification	O
has	O
an	O
interesting	O
implication	O
.	O
consider	O
for	O
example	O
functions	O
o/l	O
,	O
kn	O
,	O
•••	O
,	O
o/k	O
''	O
,	O
k	O
''	O
that	O
are	O
indicators	O
of	O
sets	O
of	O
a	O
partition	O
of	O
nd	O
.	O
then	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
classifier	B
that	O
minimizes	O
the	O
empirical	B
error	I
is	O
just	O
the	O
histogram	O
classifier	O
based	O
on	O
the	O
same	O
partition	B
.	O
the	O
denseness	B
assumption	O
requires	O
that	O
the	O
partition	B
becomes	O
infinitesimally	O
fine	O
as	O
n	O
--	O
-+	O
00.	O
in	O
fact	O
,	O
we	O
have	O
obtained	O
an	O
alternative	O
proof	O
for	O
the	O
strong	B
universal	I
consistency	I
of	O
the	O
regular	B
histogram	O
rule	B
(	O
theorem	B
9.4	O
)	O
.	O
the	O
details	O
are	O
left	O
as	O
an	O
exercise	O
(	O
problem	O
17.3	O
)	O
.	O
0	O
problem	O
17.l	O
.	O
let	O
yrl	O
,	O
yr2	O
,	O
...	O
be	O
a	O
sequence	O
of	O
real-valued	O
functions	O
on	O
a	O
bounded	O
in	O
(	O
cid:173	O
)	O
problems	O
and	O
exercises	O
terval	O
[	O
a	O
,	O
b	O
]	O
such	O
that	O
f	O
:	O
yri	O
(	O
x	O
)	O
yr	O
}	O
(	O
x	O
)	O
dx	O
=	O
i	O
{	O
i=	O
}	O
}	O
,	O
and	O
the	O
set	O
of	O
finite	O
linear	O
combinations	O
2	O
:	O
=	O
;	O
=1	O
ai	O
yri	O
(	O
x	O
)	O
is	O
dense	O
in	O
the	O
class	O
of	O
continuous	O
functions	O
on	O
[	O
a	O
,	O
b	O
]	O
with	B
respect	I
to	I
the	I
supremum	I
norm	I
.	O
define	O
the	O
functions	O
\	O
{	O
iil	O
,	O
..	O
``	O
id	O
:	O
[	O
a	O
,	O
b	O
]	O
d	O
-+	O
n	O
by	O
show	O
that	O
these	O
functions	O
form	O
a	O
complete	O
orthonormal	O
set	O
of	O
functions	O
on	O
[	O
a	O
,	O
b	O
]	O
d.	O
problem	O
17.2.	O
let	O
z	O
be	O
an	O
arbitrary	O
random	O
variable	B
on	O
n	O
,	O
and	O
v	O
be	O
a	O
real	O
random	O
variable	B
,	O
independent	O
of	O
z	O
,	O
that	O
has	O
a	O
density	O
h	O
e	O
l2	O
(	O
'a	O
)	O
.	O
show	O
that	O
the	O
density	O
f	O
of	O
the	O
288	O
17.	O
generalized	O
linear	O
discrimination	O
random	O
variable	B
x	O
=	O
z	O
+	O
v	O
exists	O
,	O
and	O
f	O
f2	O
(	O
x	O
)	O
dx	O
<	O
00.	O
hint	O
:	O
use	O
jensen	O
's	O
inequality	B
to	O
prove	O
that	O
f	O
f2	O
(	O
x	O
)	O
dx	O
:	O
s	O
f	O
h2	O
(	O
x	O
)	O
dx	O
.	O
problem	O
17.3.	O
derive	O
the	O
strong	B
universal	I
consistency	I
of	O
the	O
regular	B
histogram	O
rule	B
from	O
theorem	B
17.2	O
,	O
as	O
indicated	O
in	O
the	O
remark	O
following	O
it	O
.	O
problem	O
17.4.	O
let	O
{	O
1fo	O
,	O
1f1	O
'	O
1f2	O
'	O
...	O
}	O
be	O
the	O
standard	B
trigonometric	O
basis	O
,	O
and	O
consider	O
the	O
classification	O
rule	B
defined	O
in	O
(	O
17.1	O
)	O
.	O
show	O
that	O
the	O
rule	B
is	O
not	O
consistent	O
for	O
the	O
following	O
distribution	B
:	O
given	O
y	O
=	O
l	O
,	O
let	O
x	O
be	O
0	O
,	O
and	O
given	O
y	O
=	O
0	O
,	O
let	O
x	O
be	O
uniformly	O
distributed	O
on	O
[	O
-jt	O
,	O
jt	O
]	O
.	O
assume	O
furthermore	O
that	O
pry	O
=	O
1	O
}	O
=	O
1/2	O
.	O
problem	O
17.5.	O
the	O
haar	O
basis	O
is	O
not	O
bounded	O
.	O
determine	O
whether	O
or	O
not	O
the	O
laguerre	O
,	O
hermite	O
,	O
and	O
legendre	O
bases	O
are	O
bounded	O
.	O
18	O
complexity	B
regularization	I
this	O
chapter	O
offers	O
key	O
theoretical	B
results	O
that	O
confirm	O
the	O
existence	O
of	O
certain	O
''	O
good	O
''	O
rules	O
.	O
although	O
the	O
proofs	O
are	O
constructive-we	O
do	O
tell	O
you	O
how	O
you	O
may	O
design	O
such	O
rules-the	O
computational	O
requirements	O
are	O
often	O
prohibitive	O
.	O
many	O
of	O
these	O
rules	O
are	O
thus	O
not	O
likely	O
to	O
filter	O
down	O
to	O
the	O
software	O
packages	O
and	O
pattern	O
recognition	O
implementations	O
.	O
an	O
attempt	O
at	O
reducing	O
the	O
computational	O
complex	O
(	O
cid:173	O
)	O
ity	O
somewhat	O
is	O
described	O
in	O
the	O
section	O
entitled	O
``	O
simple	B
empirical	I
covering	O
.	O
''	O
nevertheless	O
,	O
we	O
feel	O
that	O
much	O
more	O
serious	O
work	O
on	O
discovering	O
practical	O
algo	O
(	O
cid:173	O
)	O
rithms	O
for	O
empirical	B
risk	I
minimization	I
is	O
sorely	O
needed	O
.	O
in	O
chapter	O
12	O
,	O
the	O
empirical	B
error	I
probability	O
was	O
minimized	O
over	O
a	O
class	O
e	O
of	O
decision	O
rules	O
¢	O
:	O
n	O
d	O
~	O
{	O
o	O
,	O
i	O
}	O
.	O
we	O
saw	O
that	O
if	O
the	O
vc	B
dimension	I
vc	O
of	O
the	O
class	O
is	O
finite	O
,	O
then	O
the	O
error	O
probability	O
of	O
the	O
selected	O
rule	B
is	O
within	O
constant	O
times	O
-/vc	O
log	O
n	O
/	O
n	O
of	O
that	O
of	O
the	O
best	O
rule	B
in	O
e.	O
unfortunately	O
,	O
classes	O
with	O
finite	O
vc	B
dimension	I
are	O
nearly	O
always	O
too	O
small	O
,	O
and	O
thus	O
the	O
error	O
probability	O
of	O
the	O
best	O
rule	B
in	O
e	O
is	O
typically	O
far	O
from	O
the	O
bayes	O
risk	O
l	O
*	O
for	O
some	O
distribution	B
(	O
see	O
theorem	B
18.4	O
)	O
.	O
in	O
chapter	O
17	O
we	O
investigated	O
the	O
special	O
classes	O
of	B
generalized	I
linear	I
rules	I
.	O
theorem	B
17.2	O
,	O
for	O
example	O
,	O
shows	O
that	O
if	O
we	O
increase	O
the	O
size	O
of	O
the	O
class	O
in	O
a	O
controlled	O
fashion	O
as	O
the	O
sample	O
size	O
n	O
increases	O
,	O
the	O
error	O
probability	O
of	O
the	O
selected	O
rule	B
approaches	O
l	O
*	O
for	O
any	O
distribution	B
.	O
thus	O
,	O
vc	O
increases	O
with	O
n	O
!	O
theorem	B
17.2	O
may	O
be	O
generalized	B
straightforwardly	O
for	O
other	O
types	O
of	O
classifiers	O
.	O
consider	O
a	O
sequence	O
of	O
classes	O
e	O
(	O
l	O
)	O
,	O
e	O
(	O
2	O
)	O
,	O
...	O
,	O
containing	O
classifiers	O
of	O
the	O
form	O
¢	O
:	O
n	O
d	O
~	O
{	O
o	O
,	O
i	O
}	O
.	O
the	O
training	O
data	O
dn	O
=	O
(	O
(	O
xl	O
,	O
yd	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
)	O
are	O
used	O
to	O
select	O
a	O
classifier	O
¢~	O
by	O
minimizing	O
the	O
empirical	B
error	I
probability	O
ln	O
(	O
¢	O
)	O
over	O
the	O
class	O
e	O
(	O
kn	O
)	O
,	O
where	O
the	O
integer	O
kn	O
depends	O
on	O
n	O
in	O
a	O
specified	O
way	O
.	O
the	O
following	O
generalization	O
is	O
based	O
on	O
the	O
proof	O
of	O
theorem	O
17.2	O
(	O
see	O
problem	O
18.1	O
)	O
:	O
290	O
18.	O
complexity	B
regularization	I
theorem	O
18.1.	O
assume	O
thate	O
(	O
l	O
)	O
,	O
e	O
(	O
2	O
)	O
,	O
...	O
is	O
a	O
sequence	O
of	O
classes	O
of	O
decision	O
rules	O
such	O
that	O
for	O
any	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
lim	O
inf	O
l	O
(	O
¢	O
)	O
=	O
l	O
*	O
,	O
i	O
--	O
+oo	O
<	O
jjecci	O
)	O
and	O
that	O
the	O
vc	O
dimensions	O
vcc	O
]	O
)	O
,	O
vc	O
(	O
2	O
)	O
,	O
•••	O
are	O
all	O
finite	O
.	O
if	O
kn	O
-+	O
00	O
and	O
vc	O
(	O
kn	O
)	O
log	O
n	O
-	O
-	O
-	O
-	O
-+	O
0	O
as	O
n	O
-+	O
00	O
,	O
n	O
then	O
the	O
classifier	B
¢~	O
that	O
minimizes	O
the	O
empirical	B
error	I
over	O
the	O
class	O
e	O
ckn	O
)	O
is	O
strongly	O
universally	O
consistent	O
.	O
theorem	B
18.1	O
is	O
the	O
missing	O
link-we	O
are	O
now	O
ready	O
to	O
apply	O
the	O
rich	O
theory	O
of	O
vapnik	O
-chervonenkis	O
classes	O
in	O
the	O
construction	O
of	O
universally	O
consistent	O
rules	O
.	O
the	O
theorem	B
does	O
not	O
help	O
us	O
,	O
however	O
,	O
with	O
the	O
choice	O
of	O
the	O
classes	O
eco	O
,	O
or	O
the	O
choice	O
of	O
the	O
sequence	O
{	O
kn	O
}	O
.	O
examples	O
for	O
sequences	O
of	O
classes	O
satisfying	O
the	O
condition	O
of	O
theorem	O
18.1	O
include	O
generalized	O
linear	O
decisions	O
with	O
properly	O
chosen	O
basis	O
functions	O
(	O
chapter	O
17	O
)	O
,	O
or	O
neural	O
networks	O
(	O
chapter	O
30	O
)	O
.	O
a	O
word	O
of	O
warning	O
here	O
.	O
the	O
universally	O
consistent	O
rules	O
obtained	O
via	O
theorem	B
18.1	O
may	O
come	O
at	O
a	O
tremendous	O
computational	O
price	O
.	O
as	O
we	O
will	O
see	O
further	O
on	O
,	O
we	O
will	O
often	O
have	O
exponential	B
complexities	O
in	O
n	O
instead	O
of	O
polynomial	O
time	O
that	O
would	O
be	O
obtained	O
if	O
we	O
just	O
minimized	O
the	O
empirical	B
risk	I
over	O
a	O
fixed	O
vc	B
class	I
.	O
the	O
computational	O
complexity	O
of	O
these	O
rules	O
are	O
often	O
incomparably	O
larger	O
than	O
that	O
of	O
some	O
simple	O
universally	O
consistent	O
rules	O
as	O
k-nearest	O
neighbor	O
,	O
kernel	B
,	O
or	O
partitioning	B
rules	I
.	O
18.1	O
structural	B
risk	I
minimization	I
let	O
ecl	O
)	O
,	O
e	O
(	O
2	O
)	O
,	O
•..	O
be	O
a	O
sequence	O
of	O
classes	O
of	O
classifiers	O
,	O
from	O
which	O
we	O
wish	O
to	O
select	O
a	O
sequence	O
of	O
classifiers	O
with	O
the	O
help	O
of	O
the	O
training	O
data	O
dn	O
.	O
previously	O
,	O
we	O
picked	O
¢l	O
:	O
from	O
the	O
kn	O
-th	O
class	O
eckn	O
)	O
,	O
where	O
the	O
integer	O
kn	O
is	O
some	O
prespecified	O
func	O
(	O
cid:173	O
)	O
tion	O
of	O
the	O
sample	O
size	O
n	O
only	O
.	O
the	O
integer	O
kn	O
basically	O
determines	O
the	O
complexity	O
of	O
the	O
class	O
from	O
which	O
the	O
decision	O
rule	O
is	O
selected	O
.	O
theorem	B
18.1	O
shows	O
that	O
under	O
mild	O
conditions	O
on	O
the	O
sequence	O
of	O
classes	O
,	O
it	O
is	O
possible	O
to	O
find	O
sequences	O
{	O
kn	O
}	O
such	O
that	O
the	O
rule	B
is	O
universally	O
consistent	O
.	O
typically	O
,	O
kn	O
should	O
grow	O
with	O
n	O
in	O
order	O
to	O
assure	O
convergence	O
of	O
the	O
approximation	B
error	I
inf	O
<	O
jjec	O
(	O
knj	O
l	O
(	O
¢	O
)	O
-	O
l	O
*	O
,	O
but	O
it	O
can	O
not	O
grow	O
too	O
rapidly	O
,	O
for	O
otherwise	O
the	O
estimation	B
error	I
l	O
(	O
¢~	O
)	O
-	O
inf	O
<	O
jjec	O
(	O
kn	O
j	O
l	O
(	O
¢	O
)	O
might	O
fail	O
to	O
converge	O
to	O
zero	O
.	O
ideally	O
,	O
to	O
get	O
best	O
performance	O
,	O
the	O
two	O
types	O
of	O
error	O
should	O
be	O
about	O
the	O
same	O
order	O
of	O
magnitude	O
.	O
clearly	O
,	O
a	O
pre	O
specified	O
choice	O
of	O
the	O
complexity	O
kn	O
can	O
not	O
balance	O
the	O
two	O
sides	O
of	O
the	O
trade-off	O
for	O
all	O
distribu	O
(	O
cid:173	O
)	O
tions	O
.	O
therefore	O
,	O
it	O
is	O
important	O
to	O
find	O
methods	O
such	O
that	O
the	O
classifier	B
is	O
selected	O
from	O
a	O
class	O
whose	O
index	O
is	O
automatically	O
determined	O
by	O
the	O
data	O
dn	O
.	O
this	O
section	O
deals	O
with	O
such	O
methods	O
.	O
18.1	O
structural	B
risk	I
minimization	I
291	O
the	O
most	O
obvious	O
method	O
would	O
be	O
based	O
on	O
selecting	O
a	O
candidate	O
decision	O
rule	O
¢n	O
,	O
)	O
from	O
every	O
class	O
c	O
(	O
j	O
)	O
(	O
for	O
example	O
,	O
by	O
minimizing	O
the	O
empirical	B
error	I
over	O
c	O
(	O
j	O
)	O
)	O
,	O
and	O
then	O
minimizing	O
the	O
empirical	B
error	I
over	O
these	O
rules	O
.	O
however	O
,	O
typically	O
,	O
the	O
vc	B
dimension	I
of	O
00	O
c*	O
=	O
uc	O
(	O
j	O
)	O
equals	O
infinity	O
,	O
which	O
,	O
in	O
view	O
of	O
results	O
in	O
chapter	O
14	O
,	O
implies	O
that	O
this	O
approach	O
will	O
not	O
work	O
.	O
in	O
fact	O
,	O
in	O
order	O
to	O
guarantee	O
)	O
=1	O
it	O
is	O
necessary	O
that	O
inf	O
inf	O
l	O
(	O
¢	O
)	O
=	O
l*	O
,	O
)	O
~1	O
<	O
jjec	O
(	O
j	O
)	O
vc*	O
=	O
00	O
(	O
see	O
theorems	O
18.4	O
and	O
18.5	O
)	O
.	O
a	O
possible	O
solution	O
for	O
the	O
problem	O
is	O
a	O
method	O
proposed	O
by	O
vapnik	O
and	O
cher	O
(	O
cid:173	O
)	O
vonenkis	O
(	O
1974c	O
)	O
and	O
vapnik	O
(	O
1982	O
)	O
,	O
called	O
structural	B
risk	I
minimization	I
.	O
first	O
we	O
select	O
a	O
classifier	O
1n	O
,	O
)	O
from	O
every	O
class	O
c	O
(	O
j	O
)	O
which	O
minimizes	O
the	O
empirical	B
error	I
over	O
the	O
class	O
.	O
then	O
we	O
know	O
from	O
theorem	B
12.5	O
that	O
for	O
every	O
j	O
,	O
with	O
very	O
large	O
probability	O
,	O
vcu	O
)	O
~ogn	O
)	O
.	O
now	O
,	O
pick	O
a	O
classifier	O
that	O
minimizes	O
the	O
upper	O
bound	O
over	O
j	O
:	O
:	O
:	O
:	O
1.	O
to	O
make	O
things	O
more	O
precise	O
,	O
for	O
every	O
nand	O
j	O
,	O
we	O
introduce	O
the	O
complexity	B
penalty	I
r	O
(	O
j	O
,	O
n	O
)	O
=	O
32	O
-	O
n	O
vc	O
(	O
j	O
)	O
log	O
(	O
en	O
)	O
.	O
let	O
¢n	O
,	O
l	O
,	O
1n,2	O
,	O
.	O
,	O
.	O
be	O
classifiers	O
minimizing	O
the	O
empirical	B
error	I
ln	O
(	O
¢	O
)	O
over	O
the	O
classes	O
c	O
(	O
l	O
)	O
,	O
c	O
(	O
2	O
)	O
,	O
...	O
,	O
respectively	O
.	O
for	O
¢	O
e	O
c	O
(	O
j	O
)	O
,	O
define	O
the	O
complexity-penalized	O
error	O
estimate	O
-	O
ln	O
(	O
¢	O
)	O
=	O
ln	O
(	O
¢	O
)	O
+	O
r	O
(	O
j	O
,	O
n	O
)	O
.	O
--	O
--	O
finally	O
,	O
select	O
the	O
classifier	B
¢i~	O
minimizing	O
the	O
complexity	B
penalized	I
error	O
estimate	B
in	O
(	O
¢n	O
,	O
)	O
)	O
over	O
j	O
:	O
:	O
:	O
:	O
1.	O
the	O
nexttheorem	O
states	O
thatthis	O
method	O
avoids	O
overfitting	B
the	O
data	O
.	O
the	O
only	O
condition	O
is	O
that	O
each	O
class	O
in	O
the	O
sequence	O
has	O
finite	O
vc	B
dimension	I
.	O
theorem	B
18.2.	O
let	O
c	O
(	O
l	O
)	O
,	O
c	O
(	O
2	O
)	O
,	O
...	O
be	O
a	O
sequence	O
of	O
classes	O
of	O
classifiers	O
such	O
that	O
for	O
any	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
lim	O
)	O
--	O
-	O
'	O
?	O
oo	O
<	O
jjec	O
(	O
j	O
)	O
inf	O
l	O
(	O
¢	O
)	O
=	O
l	O
*	O
.	O
assume	O
also	O
that	O
the	O
vc	O
dimensions	O
vc	O
(	O
!	O
)	O
,	O
vc	O
(	O
2	O
)	O
,	O
•••	O
are	O
finite	O
and	O
satisfy	O
00	O
.6	O
.	O
=	O
l	O
e-vc	O
(	O
j	O
)	O
<	O
00	O
.	O
)	O
=1	O
292	O
18	O
.	O
~omplexity	O
regularization	B
then	O
the	O
classification	O
rule	B
cp~	O
based	O
on	O
structural	O
risk	O
minimization	O
,	O
as	O
defined	O
above	O
,	O
is	O
strongly	O
universally	O
consistent	O
.	O
remark	O
.	O
note	O
that	O
the	O
condition	O
on	O
the	O
vc	O
dimensions	O
is	O
satisfied	O
if	O
we	O
insist	O
that	O
vc	O
(	O
j+i	O
)	O
>	O
vc	O
(	O
j	O
)	O
for	O
all	O
j	O
,	O
a	O
natural	O
assumption	O
.	O
see	O
also	O
problem	O
18.3	O
.	O
0	O
n	O
instead	O
of	O
minimizing	O
the	O
empirical	B
error	I
ln	O
(	O
cp	O
)	O
over	O
the	O
set	O
of	O
candidates	O
c*	O
,	O
the	O
method	O
of	O
structural	O
risk	O
minimization	O
minimizes	O
in	O
(	O
cp	O
)	O
,	O
the	O
sum	O
of	O
the	O
empirical	B
error	I
,	O
and	O
a	O
term	O
~	O
vc	O
(	O
j	O
)	O
log	O
(	O
en	O
)	O
,	O
which	O
increases	O
as	O
the	O
vc	B
dimension	I
of	O
the	O
class	O
c	O
(	O
j	O
)	O
containing	O
cp	O
increases	O
.	O
since	O
classes	O
with	O
larger	O
vc	B
dimension	I
can	O
be	O
considered	O
as	O
more	O
complex	O
than	O
those	O
with	O
smaller	O
vc	B
dimension	I
,	O
the	O
term	O
added	O
to	O
the	O
empirical	B
error	I
may	O
be	O
considered	O
as	O
a	O
penalty	O
for	O
complexity	O
.	O
the	O
idea	O
of	O
minimizing	O
the	O
sum	O
of	O
the	O
empirical	B
error	I
and	O
a	O
term	O
penalizing	O
the	O
complexity	O
has	O
been	O
investigated	O
in	O
various	O
statistical	O
problems	O
by	O
,	O
for	O
example	O
,	O
rissanen	O
(	O
1983	O
)	O
,	O
akaike	O
(	O
1974	O
)	O
,	O
barron	O
(	O
1985	O
;	O
1991	O
)	O
,	O
barron	O
and	O
cover	O
(	O
1991	O
)	O
,	O
and	O
lugosi	O
and	O
zeger	O
(	O
1996	O
)	O
.	O
barron	O
(	O
1991	O
)	O
minimizes	O
the	O
penalized	O
empirical	B
risk	I
over	O
a	O
suitably	O
chosen	O
countably	O
infinite	O
list	O
of	O
candidates	O
.	O
this	O
approach	O
is	O
close	O
in	O
spirit	O
to	O
the	O
skeleton	B
estimates	I
discussed	O
in	O
chapter	O
28.	O
he	O
makes	O
the	O
connection	O
between	O
structural	B
risk	I
minimization	I
and	O
the	O
minimum	B
description	I
length	I
principle	I
,	O
and	O
obtains	O
results	O
similar	O
to	O
those	O
discussed	O
in	O
this	O
section	O
.	O
the	O
theorems	O
presented	O
here	O
were	O
essentially	O
developed	O
in	O
lugosi	O
and	O
zeger	O
(	O
1996	O
)	O
.	O
proof	O
of	O
theorem	O
18.2.	O
we	O
show	O
that	O
both	O
terms	O
on	O
the	O
right-hand	O
side	O
of	O
the	O
following	O
decomposition	O
converge	O
to	O
zero	O
with	O
probability	O
one	O
:	O
for	O
the	O
first	O
term	O
we	O
have	O
p	O
{	O
l	O
(	O
¢z	O
)	O
-	O
%	O
\	O
i	O
.	O
(	O
<	O
p	O
''	O
,	O
j	O
)	O
>	O
e	O
}	O
p	O
{	O
l	O
(	O
cp~	O
)	O
-	O
ln	O
(	O
cp	O
,	O
~	O
)	O
>	O
e	O
}	O
<	O
p	O
{	O
~up	O
(	O
l	O
(	O
¢n	O
,	O
j	O
)	O
-	O
in	O
(	O
¢ll.j	O
»	O
)	O
>	O
e	O
}	O
}	O
~l	O
00	O
<	O
l	O
p	O
{	O
il	O
(	O
¢n	O
,	O
j	O
)	O
-	O
ln	O
(	O
¢n.j	O
)	O
1	O
>	O
e	O
+	O
r	O
(	O
j	O
,	O
n	O
)	O
}	O
<	O
l	O
8n	O
vc	O
(	O
j	O
)	O
e-n	O
(	O
hru.1l	O
)	O
f/32	O
(	O
by	O
theorem	B
12.5	O
)	O
j=l	O
00	O
j=l	O
18.1	O
structural	B
risk	I
minimization	I
293	O
<	O
l	O
8n	O
vc	O
(	O
j	O
)	O
e-nr2	O
(	O
j	O
,	O
n	O
)	O
/32e-ne2/32	O
00	O
j=l	O
00	O
2	O
/32	O
l	O
e	O
-vc	O
(	O
j	O
)	O
=	O
f	O
:	O
:..e	O
~ne2	O
/32	O
8e	O
-ne	O
j=l	O
using	O
the	O
defining	O
expression	B
for	O
r	O
(	O
j	O
,	O
n	O
)	O
,	O
where	O
f	O
:	O
:..	O
=	O
8	O
l~l	O
e-	O
vc	O
(	O
j	O
)	O
<	O
00	O
,	O
by	O
assumption	O
.	O
thus	O
,	O
it	O
follows	O
from	O
the	O
borel-cantelli	O
lemma	O
that	O
l	O
(	O
<	O
/	O
>	O
~	O
)	O
-	O
~nfl	O
in	O
(	O
¢n	O
,	O
j	O
)	O
~	O
°	O
j	O
?	O
:	O
.	O
with	O
probability	O
one	O
as	O
n	O
~	O
00.	O
now	O
,	O
we	O
can	O
tum	O
to	O
investigating	O
the	O
second	O
term	O
inff	O
:	O
:	O
:	O
l	O
inc¢n	O
,	O
j	O
)	O
-	O
l	O
*	O
.	O
by	O
our	O
assumptions	O
,	O
for	O
every	O
e	O
>	O
0	O
,	O
there	O
exists	O
an	O
integer	O
k	O
such	O
that	O
inf	O
lc	O
<	O
/	O
»	O
-	O
l	O
*	O
:	O
:s	O
e.	O
<	O
pec	O
(	O
k	O
)	O
fix	O
such	O
a	O
k.	O
thus	O
,	O
it	O
suffices	O
to	O
prove	O
that	O
lim	O
sup	O
inf	O
inc¢n	O
,	O
j	O
)	O
-	O
n-+oo	O
j	O
?	O
:	O
.l	O
<	O
pec	O
(	O
k	O
)	O
inf	O
lc	O
<	O
/	O
»	O
:	O
:s	O
°	O
with	O
probability	O
one	O
.	O
clearly	O
,	O
for	O
any	O
fixed	O
k	O
,	O
if	O
n	O
is	O
large	O
enough	O
,	O
then	O
rck	O
,	O
n	O
)	O
=	O
32	O
e	O
-	O
;	O
;	O
vc	O
(	O
k	O
)	O
logc	O
en	O
)	O
:	O
:s	O
2	O
'	O
thus	O
,	O
for	O
such	O
large	O
n	O
,	O
p	O
{	O
i.nf	O
inc¢n	O
,	O
j	O
)	O
-	O
j	O
?	O
:	O
.i	O
inf	O
lc	O
<	O
/	O
»	O
>	O
e	O
}	O
<	O
pecw	O
inf	O
lc	O
<	O
/	O
»	O
>	O
e	O
}	O
<	O
p	O
{	O
inc¢n	O
,	O
k	O
)	O
-	O
=	O
p	O
{	O
l	O
n	O
c	O
¢n	O
,	O
k	O
)	O
+	O
r	O
c	O
k	O
,	O
n	O
)	O
-	O
<	O
pec	O
(	O
k	O
)	O
inf	O
l	O
c	O
<	O
/	O
»	O
>	O
e	O
}	O
<	O
pec	O
(	O
k	O
)	O
<	O
p	O
{	O
lnc¢n	O
,	O
k	O
)	O
-	O
inf	O
lc	O
<	O
/	O
»	O
>	O
~	O
}	O
2	O
<	O
p	O
{	O
sup	O
iln	O
(	O
¢	O
)	O
-lc¢	O
)	O
1	O
>	O
~	O
}	O
<	O
pec	O
(	O
k	O
)	O
<	O
pec	O
(	O
k	O
)	O
2	O
<	O
8n	O
vc	O
(	O
k	O
)	O
e-	O
ne2	O
/128	O
by	O
theorem	B
12.5.	O
therefore	O
,	O
since	O
vc	O
(	O
k	O
)	O
<	O
00	O
,	O
the	O
proof	O
is	O
completed	O
.	O
0	O
theorem	B
18.2	O
shows	O
that	O
the	O
method	O
of	O
structural	O
risk	O
minimization	O
is	O
univer	O
(	O
cid:173	O
)	O
sally	O
consistent	O
under	O
very	O
mild	O
conditions	O
on	O
the	O
sequence	O
of	O
classes	O
e	O
(	O
l	O
)	O
,	O
e	O
(	O
2	O
)	O
,	O
...	O
.	O
294	O
18.	O
complexity	B
regularization	I
this	O
property	O
,	O
however	O
,	O
is	O
shared	O
with	O
the	O
minimizer	O
of	O
the	O
empirical	B
error	I
over	O
the	O
)	O
,	O
where	O
kn	O
is	O
a	O
properly	O
chosen	O
function	O
of	O
the	O
sample	O
size	O
n	O
(	O
theorem	B
class	O
c	O
(	O
kn	O
18.1	O
)	O
.	O
the	O
real	O
strength	O
,	O
then	O
,	O
of	O
structural	O
risk	O
minimization	O
is	O
seen	O
from	O
the	O
next	O
result	O
.	O
theorem	B
18.3.	O
let	O
c	O
(	O
l	O
)	O
,	O
c	O
(	O
2	O
)	O
,	O
...	O
be	O
a	O
sequence	O
of	O
classes	O
of	O
classifiers	O
such	O
that	O
the	O
vc	O
dimensions	O
vc	O
(	O
l	O
)	O
,	O
v	O
c	O
(	O
2	O
)	O
,	O
•••	O
are	O
all	O
finite	O
.	O
assume	O
further	O
that	O
the	O
bayes	O
rule	B
00	O
g*	O
e	O
c*	O
=	O
uc	O
(	O
j	O
)	O
,	O
j=l	O
that	O
is	O
,	O
a	O
bayes	O
rule	B
is	O
contained	O
in	O
one	O
of	O
the	O
classes	O
.	O
let	O
k	O
be	O
the	O
smallest	O
integer	O
such	O
that	O
g*	O
e	O
c	O
(	O
k	O
)	O
.	O
thenfor	O
every	O
nand	O
e	O
>	O
°	O
satisfying	O
the	O
error	O
probability	O
of	O
the	O
classification	O
rule	B
based	O
on	O
structural	B
risk	I
minimization	I
¢~	O
satisfies	O
proof	O
.	O
theorem	B
18.3	O
follows	O
by	O
examining	O
the	O
proof	O
of	O
theorem	O
18.2.	O
the	O
only	O
difference	O
is	O
that	O
by	O
assumption	O
,	O
there	O
is	O
an	O
integer	O
k	O
such	O
that	O
inf	O
rjjec	O
(	O
k	O
)	O
l	O
(	O
¢	O
)	O
=	O
l	O
*	O
.	O
therefore	O
,	O
for	O
this	O
k	O
,	O
l	O
(	O
¢~	O
)	O
-	O
l*	O
=	O
(	O
l	O
(	O
¢~	O
)	O
-	O
~nf	O
in	O
(	O
¢n	O
,	O
j	O
)	O
)	O
+	O
(	O
~nf	O
in	O
(	O
¢n	O
,	O
j	O
)	O
-	O
inf	O
l	O
(	O
¢	O
)	O
)	O
,	O
12.1	O
12.1	O
rjjec	O
(	O
k	O
)	O
and	O
the	O
two	O
terms	O
on	O
the	O
right-hand	O
side	O
may	O
be	O
bounded	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
18.2.0	O
theorem	B
18.3	O
implies	O
that	O
if	O
g*	O
e	O
c*	O
,	O
there	O
is	O
a	O
universal	O
constant	O
co	O
and	O
a	O
finite	O
k	O
such	O
that	O
vc	O
(	O
k	O
)	O
logn	O
n	O
that	O
is	O
,	O
the	O
rate	B
of	I
convergence	I
is	O
always	O
of	O
the	O
order	O
of	O
jlog	O
n	O
/	O
n	O
,	O
and	O
the	O
constant	O
factor	O
vc	O
(	O
k	O
)	O
depends	O
on	O
the	O
distribution	B
.	O
the	O
number	O
vc	O
(	O
k	O
)	O
may	O
be	O
viewed	O
as	O
the	O
inherent	O
complexity	O
of	O
the	O
bayes	O
rule	B
for	O
the	O
distribution	B
.	O
the	O
intuition	O
is	O
that	O
the	O
simplest	O
rules	O
are	O
contained	O
in	O
c	O
(	O
l	O
)	O
,	O
and	O
more	O
and	O
more	O
complex	O
rules	O
are	O
added	O
to	O
the	O
class	O
as	O
the	O
index	O
of	O
the	O
class	O
increases	O
.	O
the	O
size	O
of	O
the	O
error	O
is	O
about	O
the	O
same	O
as	O
if	O
we	O
had	O
known	O
k	O
beforehand	O
,	O
and	O
minimized	O
the	O
empirical	B
error	I
over	O
c	O
(	O
k	O
)	O
.	O
in	O
view	O
of	O
the	O
results	O
of	O
chapter	O
14	O
it	O
is	O
clear	O
that	O
the	O
classifier	B
described	O
in	O
theorem	O
18.1	O
does	O
not	O
share	O
this	O
property	O
,	O
since	O
if	O
l	O
*	O
>	O
0	O
,	O
then	O
the	O
error	O
probability	O
of	O
the	O
is.1	O
structural	B
risk	I
minimization	I
295	O
rule	B
selected	O
from	O
c	O
(	O
kn	O
)	O
is	O
larger	O
than	O
a	O
constant	O
times	O
jvc	O
(	O
kn	O
)	O
log	O
n/	O
n	O
for	O
some	O
distribution-even	O
if	O
g*	O
e	O
c	O
(	O
k	O
)	O
for	O
some	O
fixed	O
k.	O
just	O
as	O
in	O
designs	O
based	O
upon	O
minimum	O
description	O
length	O
,	O
automatic	B
model	O
selection	B
,	O
and	O
other	O
complexity	B
regularization	I
methods	O
(	O
rissanen	O
(	O
1983	O
)	O
,	O
akaike	O
(	O
1974	O
)	O
,	O
barron	O
(	O
1985	O
;	O
1991	O
)	O
,	O
and	O
barron	O
and	O
cover	O
(	O
1991	O
)	O
)	O
,	O
structural	B
risk	I
minimization	I
automatically	O
finds	O
where	O
to	O
look	O
for	O
the	O
optimal	O
classifier	B
.	O
the	O
constants	O
appearing	O
in	O
theorem	O
18.3	O
may	O
be	O
improved	O
by	O
using	O
refined	O
versions	O
of	O
the	O
vapnik	O
-chervonenkis	O
inequality	B
.	O
the	O
condition	O
g*	O
e	O
c*	O
in	O
theorem	O
18.3	O
is	O
not	O
very	O
severe	O
,	O
as	O
c*	O
can	O
be	O
a	O
large	O
class	O
with	O
infinite	O
vc	B
dimension	I
.	O
the	O
only	O
requirement	O
is	O
that	O
it	O
should	O
be	O
written	O
as	O
a	O
countable	O
union	O
of	O
classes	O
of	O
finite	O
vc	B
dimension	I
.	O
note	O
however	O
that	O
the	O
class	O
of	O
all	O
decision	O
rules	O
can	O
not	O
be	O
decomposed	O
as	O
such	O
(	O
see	O
theorem	B
18.6	O
)	O
.	O
we	O
also	O
emphasize	O
that	O
in	O
order	O
to	O
achieve	O
the	O
0	O
(	O
jlog	O
n	O
/	O
n	O
)	O
rate	B
of	I
convergence	I
,	O
we	O
do	O
not	O
have	O
to	O
assume	O
that	O
the	O
distribution	B
is	O
a	O
member	O
of	O
a	O
known	O
finite-dimensional	O
parametric	O
family	O
(	O
see	O
chapter	O
16	O
)	O
.	O
the	O
condition	O
is	O
imposed	O
solely	O
on	O
the	O
form	O
of	O
the	O
bayes	O
classifier	B
g*	O
.	O
by	O
inspecting	O
the	O
proof	O
of	O
theorem	O
18.2	O
we	O
see	O
that	O
for	O
every	O
k	O
,	O
el	O
(	O
¢~	O
)	O
-	O
l*	O
:	O
:	O
;	O
co	O
vc	O
(	O
k	O
)	O
log	O
n	O
+	O
(	O
inf	O
l	O
(	O
¢	O
)	O
_	O
l	O
*	O
)	O
.	O
n	O
¢ec	O
(	O
k	O
)	O
in	O
fact	O
,	O
theorem	B
18.3	O
is	O
the	O
consequence	O
of	O
this	O
inequality	B
.	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
,	O
which	O
may	O
be	O
called	O
estimation	B
error	I
,	O
usually	O
increases	O
with	O
growing	O
k	O
,	O
while	O
the	O
second	O
,	O
the	O
approximation	B
error	I
,	O
usually	O
decreases	O
with	O
it	O
.	O
importantly	O
,	O
the	O
above	O
inequality	B
is	O
true	O
for	O
every	O
k	O
,	O
so	O
that	O
vc	O
(	O
k	O
)	O
logn	O
+	O
(	O
inf	O
l	O
(	O
¢	O
)	O
-	O
l	O
*	O
)	O
)	O
.	O
¢ec	O
(	O
k	O
)	O
n	O
thus	O
structural	B
risk	I
minimization	I
finds	O
a	O
nearly	O
optimal	O
balance	O
between	O
the	O
two	O
terms	O
.	O
see	O
also	O
problem	O
18.6.	O
remark	O
.	O
it	O
is	O
worthwhile	O
mentioning	O
that	O
under	O
the	O
conditions	O
of	O
theorem	O
18.3	O
,	O
an	O
even	O
faster	O
,	O
0	O
(	O
-ji711	O
)	O
,	O
rate	B
of	I
convergence	I
is	O
achievable	O
at	O
the	O
expense	O
of	O
mag	O
(	O
cid:173	O
)	O
nifying	O
the	O
constant	O
factor	O
.	O
more	O
precisely	O
,	O
it	O
is	O
possible	O
to	O
define	O
the	O
complexity	O
penalties	O
r	O
(	O
j	O
,	O
n	O
)	O
such	O
that	O
the	O
resulting	O
classifier	B
satisfies	O
el	O
(	O
a	O
.	O
*	O
)	O
-	O
l*	O
<	O
~	O
-.jn	O
'	O
'f'n	O
where	O
the	O
constant	O
cl	O
depends	O
on	O
the	O
distribution	B
.	O
these	O
penalties	O
may	O
be	O
defined	O
by	O
exploiting	O
alexander	O
's	O
inequality	B
(	O
theorem	B
12.10	O
)	O
,	O
and	O
the	O
inequality	B
above	O
can	O
be	O
proved	O
by	O
using	O
the	O
bound	O
in	O
problem	O
12.10	O
,	O
see	O
problem	O
18.6	O
.	O
0	O
remark	O
.	O
we	O
see	O
from	O
the	O
proof	O
of	O
theorem	O
18.2	O
that	O
p	O
{	O
l	O
(	O
¢~	O
)	O
>	O
in	O
(	O
¢~	O
)	O
+	O
e	O
}	O
:	O
:	O
:	O
:	O
6.e-ne2	O
/32	O
.	O
296	O
18.	O
complexity	B
regularization	I
this	O
means	O
that	O
in	O
(	O
¢~	O
)	O
does	O
not	O
underestimate	O
the	O
true	O
error	O
probability	O
of	O
¢~	O
by	O
much	O
.	O
this	O
is	O
a	O
very	O
attractive	O
feature	O
of	O
in	O
as	O
an	O
error	O
estimate	O
,	O
as	O
the	O
designer	O
can	O
be	O
confident	O
about	O
the	O
performance	O
of	O
the	O
rule	B
¢~	O
.	O
0	O
the	O
initial	O
excitement	O
over	O
a	O
consistent	O
rule	B
with	O
a	O
guaranteed	O
0	O
(	O
.jlog	O
n	O
/	O
n	O
)	O
rate	B
of	I
convergence	I
to	O
the	O
bayes	O
error	O
is	O
tempered	O
by	O
a	O
few	O
sobering	O
facts	O
:	O
(	O
1	O
)	O
the	O
user	O
needs	O
to	O
choose	O
the	O
sequence	O
cm	O
and	O
has	O
to	O
know	O
vcw	O
(	O
see	O
,	O
however	O
,	O
problems	O
18.3	O
,	O
18.4	O
,	O
and	O
the	O
method	O
of	O
simple	O
empirical	B
covering	I
below	O
)	O
.	O
(	O
2	O
)	O
it	O
is	O
difficult	O
to	O
find	O
the	O
structural	O
risk	O
minimizer	O
¢/	O
:	O
efficiently	O
.	O
after	O
all	O
,	O
the	O
minimization	O
is	O
done	O
over	O
an	O
infinite	O
sequence	O
of	O
infinite	O
sets	O
.	O
the	O
second	O
concern	O
above	O
deserves	O
more	O
attention	O
.	O
consider	O
the	O
following	O
simple	O
example	O
:	O
let	O
d	O
=	O
1	O
,	O
and	O
let	O
c	O
(	O
j	O
)	O
be	O
the	O
class	O
of	O
classifiers	O
¢	O
for	O
which	O
{	O
x	O
:	O
¢	O
(	O
x	O
)	O
=	O
1	O
}	O
=	O
ua	O
i	O
,	O
j	O
i=l	O
where	O
each	O
ai	O
is	O
an	O
interval	O
ocr	O
.	O
then	O
,	O
from	O
theorem	B
l3.7	O
,	O
vcw	O
=	O
2j	O
,	O
and	O
we	O
may	O
take	O
r	O
(	O
j	O
,	O
n	O
)	O
=	O
64j	O
-log	O
(	O
en	O
)	O
.	O
n	O
in	O
structural	O
risk	O
minimization	O
,	O
we	O
find	O
those	O
j	O
(	O
possibly	O
empty	O
)	O
intervals	O
that	O
minimize	O
ln	O
(	O
¢	O
)	O
+	O
r	O
(	O
j	O
,	O
n	O
)	O
,	O
and	O
call	O
the	O
corresponding	O
classifier	B
¢n	O
,	O
j	O
'	O
for	O
j	O
=	O
1	O
we	O
have	O
r	O
(	O
j	O
,	O
n	O
)	O
=	O
~	O
loge	O
en	O
)	O
.	O
as	O
r	O
(	O
n	O
,	O
n	O
)	O
>	O
1	O
+	O
r	O
(	O
l	O
,	O
n	O
)	O
,	O
and	O
r	O
(	O
j	O
,	O
n	O
)	O
is	O
monotone	O
in	O
j	O
,	O
it	O
is	O
easily	O
seen	O
that	O
to	O
pick	O
the	O
best	O
j	O
as	O
well	O
,	O
we	O
need	O
only	O
consider	O
1	O
:	O
:	O
:	O
:	O
j	O
:	O
:	O
:	O
:	O
n.	O
still	O
,	O
this	O
is	O
a	O
formidable	O
exercise	O
.	O
for	O
fixed	O
j	O
,	O
the	O
best	O
j	O
intervals	O
may	O
be	O
found	O
by	O
considering	O
all	O
possible	O
insertions	O
of	O
2j	O
interval	O
boundaries	O
among	O
the	O
n	O
x/so	O
this	O
brute	O
force	O
method	O
takes	O
computation	O
time	O
bounded	O
from	O
below	O
by	O
(	O
n	O
;	O
~j	O
)	O
.	O
if	O
we	O
let	O
j	O
run	O
up	O
to	O
n	O
,	O
then	O
we	O
have	O
as	O
a	O
lower	O
bound	O
just	O
the	O
last	O
term	O
alone	O
,	O
g~	O
)	O
,	O
grows	O
as	O
a	O
ul	O
r	O
,	O
and	O
is	O
prohibitively	O
large	O
for	O
n	O
:	O
:	O
:	O
:	O
20.	O
fortunately	O
,	O
in	O
this	O
particular	O
case	O
,	O
there	O
is	O
an	O
algorithm	B
which	O
finds	O
a	O
classifier	O
minimizing	O
ln	O
(	O
¢	O
)	O
+	O
r	O
(	O
j	O
,	O
n	O
)	O
over	O
c*	O
=	O
u~l	O
c	O
(	O
j	O
)	O
in	O
computational	O
time	O
polynomial	B
in	O
n	O
,	O
see	O
problem	O
18.5.	O
another	O
example	O
when	O
the	O
structural	O
risk	O
minimizer	O
¢~	O
is	O
easy	O
to	O
find	O
is	O
described	O
in	O
problem	O
18.6.	O
however	O
,	O
such	O
fast	O
algorithms	O
are	O
not	O
always	O
available	O
,	O
and	O
exponential	B
running	O
time	O
prohibits	O
the	O
use	O
of	O
structural	O
risk	O
minimization	O
even	O
for	O
relatively	O
small	O
values	O
of	O
n.	O
t	O
(	O
n	O
+~j	O
)	O
.	O
j=l	O
2	O
]	O
18.3	O
simple	B
empirical	I
covering	O
297	O
18.2	O
poor	O
approximation	O
properties	O
of	O
vc	O
classes	O
we	O
pause	O
here	O
for	O
a	O
moment	O
to	O
summarize	O
some	O
interesting	O
by-products	O
that	O
readily	O
follow	O
from	O
theorem	B
18.3	O
and	O
the	O
slow	O
convergence	O
results	O
of	O
chapter	O
7.	O
theorem	B
18.3	O
states	O
that	O
for	O
a	O
large	O
class	O
of	O
distributions	O
an	O
0	O
(	O
/log	O
n	O
/	O
n	O
)	O
rate	B
of	I
convergence	I
to	O
the	O
bayes	O
error	O
l	O
*	O
is	O
achievable	O
.	O
on	O
the	O
other	O
hand	O
,	O
theorem	B
7.2	O
asserts	O
that	O
no	O
universal	B
rates	O
of	O
convergence	O
to	O
l	O
*	O
exist	O
.	O
therefore	O
,	O
the	O
class	O
of	O
distributions	O
for	O
which	O
theorem	B
18.3	O
is	O
valid	O
can	O
not	O
be	O
that	O
large	O
,	O
after	O
all	O
.	O
the	O
combination	O
of	O
these	O
facts	O
results	O
in	O
the	O
following	O
three	O
theorems	O
,	O
which	O
say	O
that	O
vc	O
classes-classes	O
of	O
subsets	O
of	O
n	O
d	O
with	O
finite	O
vc	O
dimension-have	O
necessarily	O
very	O
poor	O
approximation	O
properties	O
.	O
the	O
proofs	O
are	O
left	O
to	O
the	O
reader	O
as	O
easy	O
exercises	O
(	O
see	O
problems	O
18.7	O
to	O
18.9	O
)	O
.	O
for	O
direct	O
proofs	O
,	O
see	O
,	O
for	O
example	O
,	O
benedek	O
and	O
itai	O
(	O
1994	O
)	O
.	O
theorem	B
18.4.	O
letc	O
be	O
any	O
class	O
of	O
classifiers	O
of	O
the	O
fo	O
rm	O
¢	O
:	O
nd	O
--	O
+	O
{	O
a	O
,	O
i	O
}	O
,	O
with	O
vc	O
dimension	B
vc	O
<	O
00.	O
then	O
for	O
every	O
e	O
>	O
°	O
there	O
exists	O
a	O
distribution	O
such	O
that	O
.	O
1	O
mf	O
l	O
(	O
¢	O
)	O
-	O
l	O
>	O
-	O
2	O
¢ec	O
*	O
-	O
e.	O
theorem	B
18.5.	O
let	O
c	O
(	O
1	O
)	O
,	O
c	O
(	O
2	O
)	O
,	O
...	O
be	O
a	O
sequence	O
of	O
classifiers	O
such	O
that	O
the	O
vc	O
dimensions	O
vc	O
(	O
l	O
)	O
,	O
vc	O
(	O
2	O
)	O
,	O
...	O
are	O
all	O
finite	O
.	O
then	O
for	O
any	O
sequence	O
{	O
ak	O
}	O
of	O
positive	O
numbers	O
converging	O
to	O
zero	O
arbitrarily	O
slowly	O
,	O
there	O
exists	O
a	O
distribution	O
such	O
that	O
for	O
every	O
k	O
large	O
enough	O
,	O
inf	O
l	O
(	O
¢	O
)	O
-	O
l	O
*	O
>	O
ak	O
.	O
¢ec	O
(	O
k	O
)	O
theorem	B
18.6.	O
the	O
class	O
c*	O
of	O
all	O
(	O
borel	O
measurable	O
)	O
decision	O
rules	O
of	O
form	O
¢	O
:	O
rd	O
--	O
+	O
{	O
a	O
,	O
i	O
}	O
can	O
not	O
be	O
written	O
as	O
00	O
c*	O
=	O
u	O
cen	O
,	O
j=l	O
where	O
the	O
vc	B
dimension	I
of	O
each	O
class	O
c	O
(	O
j	O
)	O
is	O
finite	O
.	O
in	O
other	O
words	O
,	O
the	O
class	O
b	O
of	O
all	O
borel	O
subsets	O
ofnd	O
can	O
not	O
be	O
written	O
as	O
a	O
union	O
of	O
countably	O
many	O
vc	O
classes	O
.	O
in	O
fact	O
,	O
the	O
same	O
is	O
true	O
for	O
the	O
class	O
of	O
all	O
subsets	O
of	O
the	O
set	O
of	O
positive	O
integers	O
.	O
18.3	O
simple	B
empirical	I
covering	O
as	O
theorem	B
18.2	O
shows	O
,	O
the	O
method	O
of	O
structural	O
risk	O
minimization	O
provides	O
au	O
(	O
cid:173	O
)	O
tomatic	O
protection	O
against	O
the	O
danger	O
of	O
overfitting	O
the	O
data	O
,	O
by	O
penalizing	O
com	O
(	O
cid:173	O
)	O
plex	O
candidate	O
classifiers	O
.	O
one	O
of	O
the	O
disadvantages	O
of	O
the	O
method	O
is	O
that	O
the	O
penalty	O
terms	O
r	O
(	O
j	O
,	O
n	O
)	O
require	O
knowledge	O
of	O
the	O
vc	O
dimensions	O
of	O
the	O
classes	O
c	O
(	O
j	O
)	O
or	O
upper	O
bounds	O
of	O
these	O
dimensions	O
.	O
next	O
we	O
discuss	O
a	O
method	O
proposed	O
by	O
298	O
18.	O
complexity	B
regularization	I
buescher	O
and	O
kumar	O
(	O
1996b	O
)	O
which	O
is	O
applicable	O
even	O
if	O
the	O
vc	O
dimensions	O
of	O
the	O
classes	O
are	O
completely	O
unknown	O
.	O
the	O
method	O
,	O
called	O
simple	B
empirical	I
cov~	O
ering	O
,	O
is	O
closely	O
related	O
to	O
the	O
method	O
based	O
on	O
empirical	B
covering	I
studied	O
in	O
problem	O
12.14.	O
in	O
simple	O
empirical	B
covering	I
,	O
we	O
first	O
split	O
the	O
data	O
sequence	O
dn	O
into	O
two	O
parts	O
.	O
the	O
first	O
part	O
is	O
dm	O
=	O
(	O
(	O
xl	O
,	O
yd	O
,	O
...	O
,	O
(	O
xm	O
,	O
ym	O
»	O
and	O
the	O
second	O
part	O
is	O
'it	O
=	O
(	O
xm+l	O
'	O
ym+1	O
)	O
,	O
•••	O
,	O
(	O
xn	O
,	O
yn	O
»	O
.	O
the	O
positive	O
integers	O
m	O
and	O
i	O
=	O
n	O
-	O
m	O
will	O
be	O
specified	O
later	O
.	O
the	O
first	O
part	O
dm	O
is	O
used	O
to	O
cover	O
c*	O
=	O
u7=1	O
cen	O
as	O
follows	O
.	O
for	O
every	O
¢	O
e	O
c*	O
,	O
define	O
the	O
binary	B
m-vector	O
bm	O
(	O
¢	O
)	O
by	O
there	O
are	O
n	O
:	O
:	O
:	O
:	O
2m	O
different	O
values	O
of	O
bm	O
(	O
¢	O
)	O
.	O
usually	O
,	O
as	O
ve*	O
=	O
00	O
,	O
n	O
=	O
2m	O
,	O
that	O
is	O
,	O
all	O
possible	O
values	O
of	O
bm	O
(	O
¢	O
)	O
occur	O
as	O
¢	O
is	O
varied	O
over	O
c*	O
,	O
but	O
of	O
course	O
,	O
n	O
depends	O
on	O
the	O
values	O
of	O
x	O
i	O
,	O
...	O
,	O
x	O
m.	O
we	O
call	O
a	O
classifier	O
¢	O
simpler	O
than	O
another	O
classifier	B
¢/	O
,	O
if	O
the	O
smallest	O
index	O
i	O
such	O
that	O
¢	O
e	O
c	O
(	O
i	O
)	O
is	O
smaller	O
than	O
or	O
equal	O
to	O
the	O
-smallest	O
index	O
j	O
such	O
that	O
¢	O
'	O
e	O
c	O
(	O
j	O
)	O
.	O
for	O
every	O
binary	B
m-vector	O
b	O
e	O
{	O
o	O
,	O
l	O
}	O
m	O
that	O
can	O
be	O
written	O
as	O
b	O
=	O
bm	O
(	O
¢	O
)	O
for	O
some	O
¢	O
e	O
c*	O
,	O
we	O
pick	O
a	O
candidate	O
classifier	B
;	O
pm.k	O
with	O
k	O
e	O
{	O
l	O
,	O
...	O
,	O
n	O
}	O
such	O
that	O
bm	O
(	O
;	O
pm	O
,	O
k	O
)	O
=	O
b	O
,	O
and	O
it	O
is	O
the	O
simplest	O
such	O
classifier	B
,	O
that	O
is	O
,	O
there	O
is	O
no	O
¢	O
e	O
c*	O
such	O
that	O
simultaneously	O
bm	O
(	O
;	O
pm	O
,	O
k	O
)	O
=	O
b	O
(	O
¢	O
)	O
and	O
¢	O
is	O
simpler	O
than	O
¢m.k	O
.	O
this	O
yields	O
n	O
:	O
:	O
:	O
:	O
2m	O
candidates	O
;	O
pm	O
,	O
l	O
,	O
...	O
,	O
;	O
pm.n	O
'	O
among	O
these	O
,	O
we	O
select	O
one	O
that	O
minimizes	O
the	O
empirical	B
error	I
measured	O
on	O
the	O
independent	O
testing	B
sequence	I
'it	O
.	O
denote	O
the	O
selected	O
classifier	B
by	O
¢	O
:	O
.	O
the	O
next	O
theorem	B
asserts	O
that	O
the	O
method	O
works	O
under	O
circumstances	O
similar	O
to	O
structural	B
risk	I
minimization	I
.	O
theorem	B
18.7	O
.	O
(	O
buescher	O
and	O
kumar	O
(	O
l996b	O
»	O
.	O
let	O
c	O
(	O
l	O
)	O
~	O
c	O
(	O
2	O
)	O
~	O
'	O
''	O
be	O
a	O
nested	O
sequence	O
of	O
classes	O
of	O
classifiers	O
such	O
that	O
for	O
any	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
lim	O
j~oo¢ee	O
(	O
j	O
)	O
inf	O
l	O
(	O
¢	O
)	O
=	O
l	O
*	O
.	O
assume	O
also	O
that	O
the	O
vc	O
dimensions	O
ve	O
(	O
l	O
)	O
,	O
ve	O
(	O
2	O
)	O
,	O
...	O
are	O
all	O
finite	O
.	O
if	O
m	O
i	O
log	O
n	O
-+	O
00	O
and	O
min	O
-+	O
0	O
,	O
then	O
the	O
classification	O
rule	B
¢	O
:	O
based	O
on	O
simple	O
empirical	B
covering	I
as	O
defined	O
above	O
is	O
strongly	O
universally	O
consistent	O
.	O
proof	O
.	O
we	O
decompose	O
the	O
difference	O
between	O
the	O
error	O
probability	O
of	O
the	O
selected	O
rule	B
¢	O
,	O
~	O
and	O
the	O
bayes	O
risk	O
as	O
follows	O
:	O
the	O
first	O
term	O
can	O
be	O
handled	O
by	O
lemma	O
8.2	O
and	O
hoeffding	O
's	O
inequality	B
:	O
18.3	O
simple	B
empirical	I
covering	O
299	O
p	O
{	O
l	O
(	O
ep	O
:	O
)	O
-	O
inf	O
l	O
(	O
¢m	O
,	O
k	O
)	O
>	O
e	O
}	O
l-sk-sn	O
<	O
p	O
{	O
2	O
sup	O
il	O
(	O
¢m	O
,	O
k	O
)	O
e	O
{	O
p	O
{	O
2	O
1~~	O
;	O
jl	O
(	O
1m'k	O
)	O
-	O
[	O
,	O
(	O
1m	O
,	O
k	O
)	O
i	O
>	O
,	O
i	O
dm	O
}	O
}	O
lz	O
(	O
¢m	O
,	O
k	O
)	O
1	O
>	O
e	O
}	O
l-sk-sn	O
<	O
e	O
{	O
2n	O
e	O
-ze2	O
/2	O
}	O
<	O
2m+le-	O
ze2	O
/	O
2	O
=	O
2m+1e-	O
(	O
n-m	O
)	O
e	O
2	O
/2	O
.	O
because	O
m	O
=	O
o	O
(	O
n	O
)	O
,	O
the	O
latter	O
expression	B
converges	O
to	O
zero	O
exponentially	O
rapidly	O
.	O
thus	O
,	O
it	O
remains	O
to	O
show	O
that	O
inf	O
l	O
(	O
¢m	O
,	O
k	O
)	O
-	O
l	O
*	O
-+	O
°	O
l-sk-sn	O
with	O
probability	O
one	O
.	O
by	O
our	O
assumptions	O
,	O
for	O
every	O
e	O
>	O
0	O
,	O
there	O
is	O
an	O
integer	O
k	O
such	O
that	O
inf	O
l	O
(	O
ep	O
)	O
-	O
l*	O
<	O
e.	O
¢ec	O
(	O
k	O
)	O
then	O
there	O
exists	O
a	O
classifier	O
epee	O
)	O
e	O
e	O
(	O
k	O
)	O
with	O
l	O
(	O
ep	O
(	O
e	O
)	O
)	O
-	O
l	O
*	O
:	O
:	O
:	O
s	O
e.	O
therefore	O
,	O
we	O
are	O
done	O
if	O
we	O
prove	O
that	O
lim	O
sup	O
inf	O
l	O
(	O
¢m	O
,	O
k	O
)	O
-	O
l	O
(	O
ep	O
(	O
e	O
)	O
)	O
:	O
:	O
:	O
:	O
°	O
with	O
probability	O
one	O
.	O
n-hx	O
!	O
l-sk-sn	O
clearly	O
,	O
there	O
is	O
a	O
classifier	O
¢m	O
,	O
j	O
among	O
the	O
candidates	O
¢m	O
,	O
l	O
,	O
...	O
,	O
¢m	O
,	O
n	O
,	O
such	O
that	O
since	O
by	O
definition	O
,	O
¢m	O
,	O
j	O
is	O
simpler	O
than	O
ep	O
(	O
e	O
)	O
,	O
and	O
the	O
classes	O
e	O
(	O
l	O
)	O
,	O
e	O
(	O
2	O
)	O
,	O
...	O
are	O
nested	O
,	O
it	O
follows	O
that	O
¢m	O
,	O
j	O
e	O
e	O
(	O
k	O
)	O
.	O
therefore	O
,	O
<	O
sup	O
il	O
(	O
ep	O
)	O
l	O
(	O
ep	O
'	O
)	O
i	O
,	O
¢	O
,	O
¢'ec	O
(	O
k	O
)	O
:	O
bm	O
(	O
¢	O
)	O
=bm	O
(	O
¢	O
'	O
)	O
where	O
the	O
last	O
supremum	O
is	O
taken	O
over	O
all	O
pairs	O
of	O
classifiers	O
such	O
that	O
their	O
corre	O
(	O
cid:173	O
)	O
sponding	O
binary	B
vectors	O
bm	O
(	O
ep	O
)	O
and	O
bm	O
(	O
ep	O
'	O
)	O
are	O
equal	O
.	O
but	O
from	O
problem	O
12.14	O
,	O
300	O
18.	O
g	O
:	O
omplexity	O
regularization	B
which	O
is	O
summable	O
if	O
m/	O
log	O
n	O
-+	O
00.0	O
remark	O
.	O
as	O
in	O
theorem	O
18.3	O
,	O
we	O
may	O
assume	O
again	O
that	O
there	O
is	O
an	O
integer	O
k	O
such	O
that	O
inf	O
<	O
/	O
>	O
eok	O
)	O
l	O
(	O
¢	O
)	O
=	O
l	O
*	O
.	O
then	O
from	O
the	O
proof	O
of	O
the	O
theorem	B
above	O
we	O
see	O
that	O
the	O
error	O
probability	O
of	O
the	O
classifier	B
¢~	O
,	O
obtained	O
by	O
the	O
method	O
of	O
simple	O
empirical	B
covering	I
,	O
satisfies	O
p	O
{	O
l	O
(	O
¢/	O
:	O
)	O
-	O
l	O
*	O
>	O
e	O
}	O
:	O
:s	O
2	O
m+1	O
e-	O
(	O
n-m	O
)	O
e	O
2	O
/8	O
+	O
2s4	O
(	O
c	O
(	O
k	O
)	O
,	O
2m	O
)	O
e-mdog2/8	O
.	O
unfortunately	O
,	O
for	O
any	O
choice	O
of	O
m	O
,	O
this	O
bound	O
is	O
much	O
larger	O
than	O
the	O
analogous	O
bound	O
obtained	O
for	O
structural	B
risk	I
minimization	I
.	O
in	O
particular	O
,	O
it	O
does	O
not	O
yield	O
an	O
ocjlogn/n	O
)	O
rate	B
of	I
convergence	I
.	O
thus	O
,	O
it	O
appears	O
that	O
the	O
price	O
paid	O
for	O
the	O
advantages	O
of	O
simple	O
empirical	B
covering-no	O
knowledge	O
of	O
the	O
vc	O
dimensions	O
is	O
required	O
,	O
and	O
the	O
implementation	O
may	O
require	O
significantly	O
less	O
computational	O
time	O
in	O
general-is	O
a	O
slower	O
rate	B
of	I
convergence	I
.	O
see	O
problem	O
18.10	O
.	O
0	O
problems	O
and	O
exercises	O
problem	O
18.1.	O
prove	O
theorem	B
18.1.	O
problem	O
18.2.	O
define	O
the	O
complexity	O
penalties	O
r	O
(	O
j	O
,	O
n	O
)	O
so	O
that	O
under	O
the	O
conditions	O
of	O
theorem	O
18.3	O
,	O
the	O
classification	O
rule	B
¢1~	O
based	O
upon	O
structural	B
risk	I
minimization	I
satisfies	O
where	O
the	O
constant	O
cl	O
depends	O
on	O
the	O
distribution	B
.	O
hint	O
:	O
use	O
alexander	O
's	O
bound	O
(	O
theorem	B
12.10	O
)	O
,	O
and	O
the	O
inequality	B
of	O
problem	O
12.10.	O
problem	O
18.3.	O
let	O
c	O
(	O
l	O
)	O
,	O
c	O
(	O
2	O
)	O
,	O
...	O
be	O
a	O
sequence	O
of	O
classes	O
of	O
decision	O
rules	O
with	O
finite	O
vc	O
:	O
:	O
:	O
veuj	O
on	O
the	O
vc	O
dimensions	O
are	O
known	O
.	O
dimensions	O
.	O
assume	O
that	O
only	O
upper	O
bounds	O
a	O
j	O
define	O
the	O
complexity	O
penalties	O
by	O
r	O
(	O
j	O
,	O
n	O
)	O
=	O
32	O
-	O
(	O
xj	O
log	O
(	O
en	O
)	O
.	O
n	O
show	O
that	O
if	O
l~i	O
e-cxj	O
<	O
00	O
,	O
then	O
theorems	O
18.2	O
and	O
18.3	O
carryover	O
to	O
the	O
classifier	B
based	O
on	O
structural	B
risk	I
minimization	I
defined	O
by	O
these	O
penalties	O
.	O
this	O
points	O
out	O
that	O
knowledge	O
of	O
relatively	O
rough	O
estimates	O
of	O
the	O
vc	O
dimensions	O
suffice	O
.	O
problem	O
18.4.	O
let	O
cci	O
)	O
,	O
c	O
(	O
2	O
)	O
,	O
...	O
be	O
a	O
sequence	O
of	O
classes	O
of	O
classifiers	O
such	O
that	O
the	O
vc	O
dimensions	O
ve	O
(	O
l	O
)	O
,	O
ve	O
(	O
2	O
)	O
,	O
•••	O
are	O
all	O
finite	O
.	O
assume	O
furthermore	O
that	O
the	O
bayes	O
rule	B
is	O
con	O
(	O
cid:173	O
)	O
tained	O
in	O
one	O
of	O
the	O
classes	O
and	O
that	O
l	O
*	O
=	O
o.	O
let	O
¢1	O
:	O
be	O
the	O
rule	B
obtained	O
by	O
structural	B
risk	I
minimization	I
using	O
the	O
positive	O
penalties	O
r	O
(	O
j	O
,	O
n	O
)	O
,	O
satisfying	O
problems	O
and	O
exercises	O
301	O
(	O
l	O
)	O
for	O
each	O
n	O
,	O
r	O
(	O
j	O
,	O
n	O
)	O
is	O
strictly	O
monotone	O
increasing	O
in	O
j	O
.	O
(	O
2	O
)	O
for	O
each	O
j	O
,	O
limn-+oo	O
r	O
(	O
j	O
,	O
n	O
)	O
=	O
0.	O
show	O
that	O
e	O
{	O
l	O
(	O
¢	O
:	O
)	O
}	O
=	O
o	O
(	O
logn/n	O
)	O
(	O
lugosi	O
and	O
zeger	O
(	O
1996	O
»	O
.	O
for	O
related	O
work	O
,	O
see	O
benedek	O
and	O
itai	O
(	O
1994	O
)	O
.	O
problem	O
18.5.	O
let	O
c	O
(	O
j	O
)	O
be	O
the	O
class	O
of	O
classifiers	O
¢	O
:	O
n	O
d	O
~	O
to	O
,	O
i	O
}	O
satisfying	O
{	O
x	O
:	O
¢	O
(	O
x	O
)	O
=	O
i	O
}	O
=	O
u	O
ai	O
,	O
j	O
i=l	O
where	O
the	O
ai	O
's	O
are	O
bounded	O
intervals	O
in	O
n.	O
the	O
purpose	O
of	O
this	O
exercise	O
is	O
to	O
point	O
out	O
that	O
there	O
is	O
a	O
fast	O
algorithm	B
to	O
find	O
the	O
structural	O
risk	O
minimizer	O
¢	O
:	O
over	O
c*	O
=	O
u~l	O
cen	O
,	O
that	O
is	O
,	O
which	O
minimizes	O
in	O
=	O
ln	O
+	O
r	O
(	O
j	O
,	O
n	O
)	O
over	O
c*	O
,	O
where	O
the	O
penalties	O
r	O
(	O
j	O
,	O
n	O
)	O
are	O
defined	O
as	O
in	O
theorems	O
18.2	O
and	O
18.3.	O
the	O
property	O
below	O
was	O
pointed	O
out	O
to	O
us	O
by	O
miklos	O
csuros	O
and	O
r6ka	O
szabo	O
.	O
(	O
1	O
)	O
let	O
al	O
,	O
...	O
,	O
aj	O
,	O
}	O
be	O
the	O
j	O
intervals	O
defining	O
the	O
classifier	B
(	O
fin	O
,	O
)	O
minimizing	O
ln	O
over	O
c	O
(	O
j	O
)	O
.	O
show	O
that	O
the	O
optimal	O
intervals	O
for	O
c	O
(	O
j+l	O
)	O
,	O
ar	O
,	O
}	O
+l	O
'	O
...	O
,	O
a	O
j+l	O
,	O
}	O
+l	O
satisfy	O
the	O
following	O
property	O
:	O
either	O
j	O
of	O
the	O
intervals	O
coincide	O
with	O
a	O
l	O
,	O
...	O
,	O
a	O
j	O
,	O
}	O
,	O
or	O
1	O
of	O
them	O
are	O
among	O
a	O
r	O
,	O
}	O
,	O
...	O
,	O
a	O
j	O
,	O
}	O
,	O
and	O
the	O
remaining	O
two	O
intervals	O
are	O
j	O
subsets	O
of	O
the	O
j	O
-th	O
interval	O
.	O
(	O
2	O
)	O
use	O
property	O
(	O
1	O
)	O
to	O
define	O
an	O
algorithm	B
that	O
finds	O
¢	O
:	O
in	O
running	O
time	O
polynomial	B
in	O
the	O
sample	O
size	O
n.	O
problem	O
18.6.	O
assume	O
that	O
the	O
distribution	B
of	O
x	O
is	O
concentrated	O
on	O
the	O
unit	O
cube	O
,	O
that	O
is	O
,	O
p	O
{	O
x	O
e	O
[	O
0	O
,	O
l	O
]	O
d	O
}	O
=	O
1.	O
let	O
p	O
}	O
be	O
a	O
partition	O
of	O
[	O
0	O
,	O
l	O
]	O
d	O
into	O
cubes	O
of	O
size	O
1/	O
j	O
,	O
that	O
is	O
,	O
p	O
}	O
contains	O
/	O
cubic	B
cells	O
.	O
let	O
c	O
(	O
j	O
)	O
be	O
the	O
class	O
of	O
all	O
histogram	O
classifiers	O
¢	O
:	O
[	O
0	O
,	O
l	O
]	O
d	O
~	O
to	O
,	O
i	O
}	O
based	O
on	O
p	O
}	O
.	O
in	O
other	O
words	O
,	O
p	O
}	O
contains	O
all	O
2	O
}	O
d	O
classifiers	O
which	O
assign	O
the	O
same	O
label	O
to	O
points	O
falling	O
in	O
the	O
same	O
cell	O
of	O
p	O
}	O
.	O
what	O
is	O
the	O
vc	B
dimension	I
ve	O
(	O
j	O
)	O
of	O
cen	O
?	O
point	O
out	O
that	O
the	O
classifier	B
minimizing	O
ln	O
over	O
c	O
(	O
j	O
)	O
is	O
just	O
the	O
regular	B
histogram	O
rule	B
based	O
on	O
p	O
}	O
.	O
(	O
see	O
chapter	O
9	O
.	O
)	O
thus	O
,	O
we	O
have	O
another	O
example	O
in	O
which	O
the	O
empirically	B
optimal	I
classifier	I
is	O
computationally	O
inexpensive	O
.	O
the	O
structural	O
risk	O
minimizer	O
¢	O
;	O
based	O
on	O
c*	O
=	O
u~l	O
c	O
(	O
j	O
)	O
is	O
also	O
easy	O
to	O
find	O
.	O
assume	O
that	O
the	O
a	B
posteriori	I
probability	I
'fl	O
(	O
x	O
)	O
is	O
uniformly	O
lipschitz	O
,	O
that	O
is	O
,	O
for	O
any	O
x	O
,	O
y	O
e	O
[	O
0	O
,	O
l	O
]	O
d	O
,	O
!	O
'fl	O
(	O
x	O
)	O
-	O
'fl	O
(	O
y	O
)	O
!	O
:	O
:	O
:	O
:	O
cllx	O
-	O
y	O
!	O
1	O
,	O
where	O
c	O
is	O
some	O
constant	O
.	O
find	O
upper	O
bounds	O
for	O
the	O
rate	B
of	I
convergence	I
of	O
el	O
(	O
¢	O
;	O
)	O
to	O
l	O
*	O
.	O
problem	O
18.7.	O
prove	O
theorem	B
18.4.	O
hint	O
:	O
use	O
theorem	B
7.1.	O
problem	O
18.8.	O
prove	O
theorem	B
18.5.	O
hint	O
:	O
use	O
theorem	B
7.2.	O
problem	O
18.9.	O
prove	O
theorem	B
18.6.	O
hint	O
:	O
use	O
theorems	O
7.2	O
and	O
18.3.	O
problem	O
18.10.	O
assume	O
that	O
the	O
bayes	O
rule	B
g*	O
is	O
contained	O
in	O
c*	O
=	O
u~l	O
c	O
(	O
j	O
)	O
.	O
let	O
¢	O
;	O
be	O
the	O
classifier	B
obtained	O
by	O
simple	B
empirical	I
covering	O
.	O
determine	O
the	O
value	O
of	O
the	O
design	O
parameter	O
m	O
that	O
minimizes	O
the	O
bounds	O
obtained	O
in	O
the	O
proof	O
of	O
theorem	O
18.7.	O
obtain	O
a	O
tight	O
upper	O
bound	O
for	O
el	O
(	O
¢	O
:	O
)	O
-	O
l	O
*	O
.	O
compare	O
your	O
results	O
with	O
theorem	O
18.3	O
.	O
19	O
condensed	B
and	O
edited	B
nearest	O
neighbor	O
rules	O
19.1	O
condensed	B
nearest	O
neighbor	O
rules	O
condensing	O
is	O
the	O
process	O
by	O
which	O
we	O
eliminate	O
data	O
points	O
,	O
yet	O
keep	O
the	O
same	O
behavior	O
.	O
for	O
example	O
,	O
in	O
the	O
nearest	B
neighbor	I
rule	I
,	O
by	O
condensing	O
we	O
might	O
mean	O
the	O
reduction	B
of	I
(	O
x	O
1	O
,	O
y1	O
)	O
,	O
•••	O
,	O
(	O
xn	O
,	O
yn	O
)	O
to	O
(	O
x~	O
,	O
y	O
{	O
)	O
,	O
...	O
,	O
(	O
x~	O
,	O
y~l	O
)	O
such	O
that	O
for	O
all	O
x	O
e	O
n	O
d	O
,	O
the	O
l-nn	O
rule	B
is	O
identical	O
based	O
on	O
the	O
two	O
samples	O
.	O
this	O
will	O
be	O
called	O
pure	O
condensing	O
.	O
this	O
operation	O
has	O
no	O
effect	O
on	O
l	O
n	O
,	O
and	O
therefore	O
is	O
recommended	O
whenever	O
space	O
is	O
at	O
a	O
premium	O
.	O
the	O
space	O
savings	O
should	O
be	O
substantial	O
whenever	O
the	O
classes	O
are	O
separated	O
.	O
unfortunately	O
,	O
pure	O
condensing	O
is	O
computationally	O
expensive	O
,	O
and	O
offers	O
no	O
hope	O
of	O
improving	O
upon	O
the	O
performance	O
of	O
the	O
ordinary	B
l-nn	O
rule	B
.	O
o	O
•	O
o	O
•	O
class	O
1	O
under	O
i-nn	O
rule	B
figure	O
19.1.	O
pure	O
condensing	O
:	O
eliminating	O
the	O
marked	O
points	O
does	O
not	O
change	O
the	O
decision	O
.	O
class	O
0	O
under	O
1-nn	O
rule	B
•	O
•	O
o	O
304	O
19.	O
g	O
:	O
ondensed	O
and	O
edited	B
nearest	O
neighbor	O
rules	O
hart	O
(	O
1968	O
)	O
has	O
the	O
first	O
simple	O
algorithm	O
for	O
condensing	O
.	O
he	O
picks	O
a	O
subset	O
that	O
correctly	O
classifies	O
the	O
remaining	O
data	O
by	O
the	O
i-nn	O
rule	B
.	O
finding	O
a	O
minimal	O
such	O
subset	O
is	O
computationally	O
difficult	O
,	O
but	O
heuristics	O
may	O
do	O
the	O
job	O
.	O
hart	O
's	O
heuristic	O
is	O
also	O
discussed	O
in	O
devijver	O
and	O
kittler	O
(	O
1982	O
,	O
p.120	O
)	O
.	O
for	O
probabilistic	O
analysis	O
,	O
we	O
take	O
a	O
more	O
abstract	O
setting	O
.	O
let	O
(	O
x~	O
,	O
y	O
{	O
)	O
,	O
...	O
,	O
(	O
x	O
:	O
n	O
,	O
y~1	O
)	O
be	O
a	O
sequence	O
that	O
depends	O
in	O
an	O
arbitrary	O
fashion	O
on	O
the	O
data	O
dn	O
,	O
and	O
let	O
gn	O
be	O
the	O
i-nearest	O
neighbor	B
rule	I
with	O
(	O
x~	O
,	O
y	O
{	O
)	O
,	O
...	O
,	O
(	O
x	O
;	O
n	O
'	O
y	O
'	O
;	O
i	O
)	O
'	O
where	O
for	O
simplicity	O
,	O
m	O
is	O
fixed	O
beforehand	O
.	O
the	O
data	O
might	O
,	O
for	O
example	O
,	O
be	O
obtained	O
by	O
finding	O
the	O
subset	O
of	O
the	O
data	O
of	O
size	O
m	O
for	O
which	O
the	O
error	O
with	O
the	O
i-nn	O
rule	B
committed	O
on	O
the	O
remaining	O
n	O
-	O
m	O
data	O
is	O
minimal	O
(	O
this	O
will	O
be	O
called	O
hart	O
's	O
rule	B
)	O
.	O
regardless	O
,	O
if	O
in	O
=	O
(	O
1/	O
n	O
)	O
l~1=1	O
i	O
{	O
gn	O
(	O
x	O
,	O
)	O
/y	O
;	O
}	O
and	O
ln	O
=	O
p	O
{	O
gn	O
(	O
x	O
)	O
=i	O
yidnl	O
then	O
we	O
have	O
the	O
following	O
:	O
theorem	B
19.1	O
.	O
(	O
devroye	O
and	O
wagner	O
(	O
i979c	O
»	O
.	O
for	O
all	O
e	O
>	O
0	O
and	O
all	O
distri	O
(	O
cid:173	O
)	O
butions	O
,	O
____	O
p	O
{	O
iln	O
-	O
lnl	O
:	O
:	O
:	O
:	O
e	O
}	O
:	O
s	O
8	O
d+	O
1	O
(	O
ne	O
)	O
(	O
d+l	O
)	O
m	O
<	O
m-l	O
)	O
0	O
e-ncj32	O
.	O
remark	O
.	O
the	O
estimate	B
in	O
is	O
called	O
the	O
resubstitution	B
estimate	O
of	O
the	O
error	O
prob	O
(	O
cid:173	O
)	O
ability	O
.	O
it	O
is	O
thoroughly	O
studied	O
in	O
chapter	O
23	O
,	O
where	O
several	O
results	O
of	O
the	O
afore	O
(	O
cid:173	O
)	O
mentioned	O
kind	O
are	O
stated	O
.	O
0	O
proof	O
.	O
observe	O
that	O
where	O
bi	O
is	O
the	O
voronoi	O
cell	O
of	O
x	O
;	O
in	O
the	O
voronoi	O
partition	B
corresponding	O
to	O
xi	O
,	O
...	O
,	O
x~l	O
'	O
that	O
is	O
,	O
bi	O
is	O
the	O
set	O
of	O
points	O
of	O
nd	O
closer	O
to	O
x	O
;	O
than	O
to	O
any	O
other	O
x~	O
(	O
with	O
appropriate	O
distance-tie	O
breaking	O
)	O
.	O
similarly	O
,	O
we	O
use	O
the	O
simple	O
upper	O
bound	O
iln	O
-	O
inl	O
:	O
s	O
sup	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
i	O
,	O
aeam	O
where	O
v	O
denotes	O
the	O
measure	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
vn	O
is	O
the	O
corresponding	O
empirical	B
measure	I
,	O
and	O
am	O
is	O
the	O
family	B
of	I
all	O
subsets	O
ofnd	O
x	O
{	O
o	O
,	O
i	O
}	O
of	O
the	O
form	O
u	O
;	O
:	O
l	O
bi	O
x	O
{	O
yd	O
,	O
where	O
b	O
l	O
,	O
.••	O
,	O
bm	O
are	O
voronoi	O
cells	O
corresponding	O
to	O
xl	O
,	O
...	O
,	O
x	O
m	O
,	O
xi	O
end	O
,	O
yi	O
-'e	O
{	O
o	O
,	O
i	O
}	O
.	O
we	O
use	O
the	O
vapnik-chervonenkis	O
inequality	B
to	O
bound	O
the	O
above	O
supremum	O
.	O
by	O
theorem	B
13.5	O
(	O
iv	O
)	O
,	O
19.1	O
condensed	B
nearest	O
neighbor	O
rules	O
305	O
where	O
a	O
is	O
the	O
class	O
of	O
sets	O
b1	O
x	O
{	O
yd	O
.	O
but	O
each	O
set	O
in	O
a	O
is	O
an	O
intersection	O
of	O
at	O
most	O
m	O
-	O
1	O
hyperplanes	O
.	O
therefore	O
,	O
by	O
theorems	O
13.5	O
(	O
iii	O
)	O
,	O
13.9	O
,	O
and	O
13.3	O
,	O
sea	O
,	O
n	O
)	O
:	O
:	O
:	O
;	O
sup	O
110.111:110+111=11	O
(	O
01	O
-	O
-	O
)	O
=0	O
d	O
+	O
1	O
..	O
(	O
n	O
)	O
.e	O
)	O
(	O
d+1	O
)	O
(	O
k-1	O
)	O
)	O
(	O
ne	O
)	O
(	O
d+1	O
)	O
(	O
k-1	O
)	O
:	O
:	O
:	O
;	O
-	O
-	O
d	O
+	O
1	O
,	O
where	O
n	O
)	O
denotes	O
the	O
number	O
of	O
points	O
in	O
r	O
d	O
x	O
{	O
j	O
}	O
.	O
the	O
result	O
now	O
follows	O
from	O
theorem	B
12.5.0	O
remark	O
.	O
with	O
hart	O
's	O
rule	B
,	O
at	O
least	O
m	O
data	O
points	O
are	O
correctly	O
classified	O
by	O
the	O
1-nn	O
rule	B
(	O
if	O
we	O
handle	O
distance	B
ties	O
satisfactorily	O
)	O
.	O
therefore	O
,	O
ln	O
:	O
:	O
:	O
;	O
1	O
-	O
min	O
.	O
0	O
the	O
following	O
is	O
a	O
special	O
case	O
:	O
let	O
m	O
<	O
n	O
be	O
fixed	O
and	O
let	O
dm	O
be	O
an	O
arbitrary	O
(	O
possibly	O
random	O
)	O
subsetofm	O
pairs	O
from	O
(	O
xl	O
,	O
y1	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
,	O
usedingn	O
•	O
let	O
the	O
remaining	O
n	O
-	O
m	O
points	O
be	O
denoted	O
by	O
tn-	O
m	O
.	O
we	O
write	O
ln	O
(	O
dm	O
)	O
for	O
the	O
probability	O
of	O
error	O
with	O
the	O
1-nn	O
based	O
upon	O
dm	O
,	O
and	O
we	O
define	O
in	O
hart	O
's	O
rule	B
,	O
ln	O
.m	O
would	O
be	O
zero	O
,	O
for	O
example	O
.	O
then	O
we	O
have	O
:	O
theorem	B
19.2.	O
for	O
all	O
e	O
>	O
0	O
,	O
where	O
ln	O
is	O
the	O
probability	O
of	O
error	O
with	O
gn	O
(	O
note	O
that	O
dm	O
depends	O
in	O
an	O
arbitrary	O
fashion	O
upon	O
dn	O
)	O
,	O
and	O
l	O
n.m	O
is	O
ln.m	O
(	O
dm	O
,	O
tn-	O
m	O
)	O
with	O
the	O
data	O
set	O
dm	O
.	O
proof	O
.	O
list	O
the	O
m-element	O
subsets	O
{	O
i	O
1	O
,	O
...	O
,	O
im	O
}	O
of	O
{	O
i	O
,	O
2	O
,	O
...	O
,	O
n	O
}	O
,	O
and	O
define	O
d~~	O
)	O
as	O
the	O
sequence	O
of	O
m	O
pairs	O
from	O
dn	O
indexed	O
by	O
i	O
=	O
ii	O
1	O
,	O
...	O
,	O
i	O
in	O
}	O
,	O
1	O
:	O
:s	O
i	O
:	O
:s	O
(	O
,	O
~	O
)	O
.	O
a	O
ccor	O
mg	O
y	O
,	O
enote	O
n-m	O
-	O
d·	O
1	O
d	O
en	O
d	O
(	O
i	O
)	O
th	O
in	O
'	O
t	O
(	O
i	O
)	O
-	O
d	O
n	O
-	O
306	O
19.	O
c	O
:	O
:ondensed	O
and	O
edited	B
nearest	O
neighbor	O
rules	O
(	O
by	O
hoeffding	O
's	O
inequality	B
,	O
because	O
given	O
d~	O
)	O
,	O
(	O
n	O
-	O
m	O
)	O
ln	O
,	O
m	O
(	O
d~i	O
)	O
,	O
t~~m	O
)	O
is	O
binomial	B
(	O
(	O
n	O
-	O
m	O
)	O
,	O
ln	O
(	O
d~	O
)	O
)	O
)	O
)	O
~	O
2	O
(	O
:	O
)	O
e~2	O
(	O
n~m	O
)	O
c	O
'	O
.	O
0	O
by	O
checking	O
the	O
proof	O
,	O
we	O
also	O
see	O
that	O
if	O
dm	O
is	O
selected	O
to	O
minimize	O
the	O
error	O
estimate	O
ln	O
,	O
m	O
(	O
dm	O
,	O
t	O
,	O
~-m	O
)	O
,	O
then	O
the	O
error	O
probability	O
ln	O
of	O
the	O
obtained	O
rule	B
satisfies	O
p	O
{	O
iln	O
-	O
inf	O
ln	O
(	O
dm	O
)	O
1	O
>	O
e	O
}	O
d111	O
<	O
p	O
{	O
2	O
sup	O
iln	O
(	O
dnj	O
-	O
ln	O
,	O
m	O
(	O
dm	O
,	O
t,1-m	O
)	O
1	O
>	O
e	O
}	O
d	O
''	O
,	O
(	O
by	O
theorem	B
8.4	O
)	O
<	O
2	O
(	O
:	O
)	O
e~	O
(	O
``	O
~m	O
)	O
''	O
/2	O
(	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
19.2	O
)	O
.	O
(	O
19.1	O
)	O
thus	O
,	O
for	O
the	O
particular	O
rule	B
that	O
mimics	O
hart	O
's	O
rule	B
(	O
with	O
the	O
exception	O
that	O
m	O
is	O
fixed	O
)	O
,	O
if	O
m	O
is	O
not	O
too	O
large-it	O
must	O
be	O
much	O
smaller	O
than	O
n	O
/	O
log	O
n-ln	O
is	O
likely	O
to	O
be	O
close	O
to	O
the	O
best	O
possible	O
we	O
can	O
hope	O
for	O
with	O
a	O
1-nn	O
rule	B
based	O
upon	O
a	O
subsample	O
of	O
size	O
m.	O
with	O
some	O
work	O
(	O
see	O
problem	O
12.1	O
)	O
,	O
we	O
see	O
that	O
by	O
theorem	B
5.1	O
,	O
where	O
lm	O
is	O
the	O
probability	O
of	O
error	O
with	O
the	O
1-nn	O
rule	B
based	O
upon	O
a	O
sample	O
of	O
m	O
data	O
pairs	O
.	O
hence	O
,	O
if	O
m	O
=	O
o	O
(	O
n/	O
log	O
n	O
)	O
,	O
m	O
-+	O
00	O
,	O
lim	O
sup	O
e	O
{	O
l	O
n	O
}	O
:	O
:	O
:	O
:	O
lnn	O
n	O
--	O
7oo	O
for	O
the	O
i-nn	O
rule	B
based	O
on	O
m	O
data	O
pairs	O
dm	O
selected	O
to	O
minimize	O
the	O
error	O
estimate	O
ln	O
,	O
m	O
(	O
dm	O
,	O
tn-m	O
)	O
.	O
however	O
,	O
this	O
is	O
very	O
pessimistic	O
indeed	O
.	O
it	O
reassures	O
us	O
that	O
with	O
only	O
a	O
small	O
fraction	O
of	O
the	O
original	O
data	O
,	O
we	O
obtain	O
at	O
least	O
as	O
good	O
a	O
performance	O
as	O
with	O
the	O
full	O
data	O
set-so	O
,	O
this	O
method	O
of	O
condensing	O
is	O
worthwhile	O
.	O
this	O
is	O
not	O
very	O
surprising	O
.	O
interestingly	O
,	O
however	O
,	O
the	O
following	O
much	O
stronger	O
result	O
is	O
true	O
.	O
19.1	O
condensed	B
nearest	O
neighbor	O
rules	O
307	O
theorem	B
19.3.	O
if	O
m	O
=	O
o	O
(	O
n	O
/	O
log	O
n	O
)	O
and	O
m	O
-+	O
00	O
,	O
and	O
if	O
ln	O
}	O
s	O
the	O
probability	O
of	O
error	O
for	O
the	O
condensed	B
nearest	O
neighbor	B
rule	I
in	O
which	O
ln	O
,	O
m	O
(	O
dm	O
,	O
tn-	O
m	O
)	O
is	O
minimized	O
over	O
all	O
data	O
sets	O
dm	O
,	O
then	O
lim	O
e	O
{	O
ln	O
}	O
=	O
l*	O
.	O
n	O
--	O
*oo	O
proof	O
.	O
by	O
(	O
19.1	O
)	O
,	O
it	O
suffices	O
to	O
establish	O
that	O
as	O
m	O
-+	O
00	O
such	O
that	O
m	O
=	O
o	O
(	O
n	O
)	O
,	O
where	O
ln	O
(	O
dm	O
)	O
is	O
the	O
probability	O
of	O
error	O
of	O
the	O
i-nn	O
rule	B
with	O
dm	O
.	O
as	O
this	O
is	O
one	O
of	O
the	O
fundamental	O
properties	O
of	B
nearest	I
neighbor	I
rules	I
not	O
previously	O
found	O
in	O
texts	O
,	O
we	O
offer	O
a	O
thorough	O
analysis	O
and	O
proof	O
of	O
this	O
result	O
in	O
the	O
remainder	O
of	O
this	O
section	O
,	O
culminating	O
in	O
theorem	O
19.4	O
.	O
0	O
the	O
distribution-free	O
result	O
of	O
theorem	O
19.3	O
sets	O
the	O
stage	O
for	O
many	O
consistency	B
proofs	O
for	O
rules	O
that	O
use	O
condensing	O
(	O
or	O
editing	O
,	O
or	O
proto	O
typing	O
,	O
as	O
defined	O
in	O
the	O
next	O
two	O
sections	O
)	O
.	O
it	O
states	O
that	O
inherently	O
,	O
partitions	O
of	O
the	O
space	O
by	O
i-nn	O
rules	O
are	O
rich	O
.	O
historical	O
remark	O
.	O
other	O
condensed	B
nearest	O
neighbor	O
rules	O
are	O
presented	O
by	O
gates	O
(	O
1972	O
)	O
,	O
ullmann	O
(	O
1974	O
)	O
,	O
ritter	O
,	O
woodruff	O
,	O
lowry	O
,	O
and	O
isenhour	O
(	O
1975	O
)	O
,	O
tomek	O
(	O
1976b	O
)	O
,	O
swonger	O
(	O
1972	O
)	O
,	O
gowda	O
and	O
krishna	O
(	O
1979	O
)	O
,	O
and	O
fukunaga	O
and	O
mantock	O
(	O
1984	O
)	O
.	O
0	O
define	O
z	O
=	O
i	O
{	O
1	O
)	O
(	O
x	O
»	O
1/2	O
)	O
'	O
let	O
(	O
x	O
;	O
,	O
yi	O
,	O
zi	O
)	O
,	O
i	O
=	O
1,2	O
,	O
...	O
,	O
n	O
,	O
be	O
i.i.d	O
.	O
tuples	O
,	O
inde	O
(	O
cid:173	O
)	O
pendent	O
of	O
(	O
x	O
,	O
y	O
,	O
z	O
)	O
,	O
where	O
x	O
;	O
may	O
have	O
a	O
distribution	O
different	O
from	O
x	O
,	O
but	O
the	O
support	B
set	O
of	O
the	O
distribution	B
j-	O
[	O
'	O
of	O
x	O
;	O
is	O
identical	O
to	O
that	O
of	O
j-	O
[	O
,	O
the	O
distribution	B
of	O
x.	O
furthermore	O
,	O
p	O
{	O
yi	O
=	O
l1x	O
;	O
=	O
x	O
}	O
=	O
1j	O
(	O
x	O
)	O
,	O
and	O
zi	O
=	O
i	O
{	O
1	O
)	O
(	O
xd	O
>	O
1/2	O
)	O
is	O
the	O
bayes	O
decision	O
at	O
x	O
;	O
.	O
lemma	O
19.1.	O
let	O
ln	O
=	O
p	O
{	O
z	O
(	O
l	O
)	O
(	O
x	O
)	O
=i	O
zixi	O
,	O
zi	O
,	O
...	O
,	O
x~	O
,	O
zn	O
}	O
be	O
the	O
probability	O
of	O
error	O
for	O
the	O
j-nn	O
rule	B
based	O
on	O
(	O
x	O
;	O
,	O
zi	O
)	O
,	O
1	O
:	O
:	O
:	O
i	O
:	O
:	O
:	O
n	O
,	O
that	O
is	O
,	O
z	O
(	O
l	O
)	O
(	O
x	O
)	O
=	O
zi	O
if	O
x	O
;	O
is	O
the	O
nearest	B
neighbor	I
of	O
x	O
among	O
xi	O
,	O
...	O
,	O
x~	O
.	O
then	O
lim	O
e	O
{	O
l	O
n	O
}	O
=	O
o.	O
n	O
--	O
*oo	O
proof	O
.	O
denote	O
by	O
x	O
(	O
l	O
)	O
(	O
x	O
)	O
the	O
nearest	B
neighbor	I
of	O
x	O
among	O
xi	O
,	O
...	O
,	O
x~	O
.	O
notice	O
that	O
the	O
proof	O
of	O
lemma	O
5.1	O
may	O
be	O
extended	O
in	O
a	O
straightforward	O
way	O
to	O
show	O
that	O
iix	O
(	O
l	O
)	O
(	O
x	O
)	O
-	O
xii	O
-+	O
0	O
with	O
probability	O
one	O
.	O
since	O
this	O
is	O
the	O
only	O
property	O
of	O
the	O
nearest	B
neighbor	I
of	O
x	O
that	O
we	O
used	O
in	O
deriving	O
the	O
asymptotic	O
formula	O
for	O
the	O
ordinary	B
i-nn	O
error	O
,	O
limn	O
--	O
*oo	O
e	O
{	O
ln	O
}	O
equals	O
lnn	O
corresponding	O
to	O
the	O
pair	O
(	O
x	O
,	O
z	O
)	O
.	O
but	O
we	O
have	O
p	O
{	O
z	O
=	O
11	O
x	O
=	O
x	O
}	O
=	O
i	O
{	O
1	O
)	O
(	O
x	O
»	O
1/2	O
)	O
.	O
thus	O
,	O
the	O
bayes	O
probability	O
of	O
error	O
l	O
*	O
308	O
19.	O
condensed	B
and	O
edited	B
nearest	O
neighbor	O
rules	O
for	O
the	O
pattern	O
recognition	O
problem	O
with	O
(	O
x	O
,	O
z	O
)	O
is	O
zero	O
.	O
hence	O
,	O
for	O
this	O
distribution	B
,	O
the	O
i-nn	O
rule	B
is	O
consistent	O
as	O
lnn	O
=	O
l	O
*	O
=	O
o	O
.	O
0	O
lemma	O
19.2.	O
let	O
z	O
(	O
i	O
)	O
(	O
x	O
)	O
be	O
as	O
in	O
the	O
previous	O
lemma	O
.	O
let	O
ln	O
=	O
p	O
{	O
z	O
(	O
l	O
)	O
(	O
x	O
)	O
i	O
yix~	O
,	O
y1	O
,	O
•••	O
,	O
x~	O
,	O
yn	O
}	O
be	O
the	O
probability	O
of	O
error	O
for	O
the	O
discrimination	O
problem	O
for	O
(	O
x	O
,	O
y	O
)	O
(	O
not	O
(	O
x	O
,	O
z	O
)	O
)	O
.	O
then	O
lim	O
e	O
{	O
ln	O
}	O
=	O
l	O
*	O
,	O
11-+00	O
where	O
l	O
*	O
is	O
the	O
bayes	O
error	O
corresponding	O
to	O
(	O
x	O
,	O
y	O
)	O
.	O
proof	O
.	O
p	O
{	O
z	O
(	O
l	O
)	O
(	O
x	O
)	O
i	O
y	O
}	O
<	O
p	O
{	O
z	O
(	O
l	O
)	O
(	O
x	O
)	O
i	O
z	O
}	O
+p	O
{	O
y	O
i	O
z	O
}	O
0	O
(	O
1	O
)	O
+	O
l	O
*	O
by	O
lemma	O
19.1	O
.	O
0	O
theorem	B
19.4.	O
let	O
dm	O
be	O
a	O
subset	O
of	O
size	O
m	O
drawn	O
from	O
dn	O
.	O
if	O
m	O
-+	O
00	O
and	O
min	O
-+	O
0	O
as	O
n	O
-+	O
00	O
,	O
then	O
lim	O
p	O
{	O
infln	O
(	O
dm	O
)	O
>	O
l*+e	O
}	O
=0	O
n-+oo	O
di11	O
for	O
all	O
e	O
>	O
0	O
,	O
where	O
ln	O
(	O
dm	O
)	O
denotes	O
the	O
conditional	O
probability	O
of	O
error	O
of	O
the	O
nearest	B
neighbor	I
rule	I
with	O
dnu	O
and	O
the	O
infimum	O
ranges	O
over	O
all	O
(	O
;	O
j	O
subsets	O
.	O
proof	O
.	O
let	O
d	O
be	O
the	O
subset	O
of	O
dn	O
consisting	O
of	O
those	O
pairs	O
(	O
xi	O
,	O
yi	O
)	O
for	O
which	O
yi	O
=	O
i	O
{	O
rj	O
(	O
xi	O
»	O
lj2	O
}	O
=	O
zi	O
.	O
if	O
jdj	O
~	O
m	O
,	O
let	O
d*	O
be	O
the	O
first	O
m	O
pairs	O
of	O
d	O
,	O
and	O
if	O
jdi	O
<	O
m	O
,	O
let	O
d*	O
=	O
{	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xm	O
,	O
ym	O
)	O
}	O
.	O
then	O
if	O
n	O
=	O
jdi	O
~	O
m	O
,	O
then	O
we	O
know	O
that	O
the	O
pairs	O
in	O
d*	O
are	O
i.i.d	O
.	O
and	O
drawn	O
from	O
the	O
distribution	B
of	O
(	O
x	O
'	O
,	O
z	O
)	O
,	O
where	O
x	O
'	O
has	O
the	O
same	O
support	B
set	O
as	O
x	O
;	O
see	O
problem	O
19.2	O
for	O
properties	O
of	O
x	O
'	O
.	O
in	O
particular	O
,	O
s	O
p	O
{	O
n	O
<	O
m	O
}	O
+p	O
{	O
n	O
~	O
m	O
,	O
ln	O
(	O
d*	O
)	O
>	O
l*	O
+e	O
}	O
19.3	O
sieves	O
and	O
prototypes	O
309	O
<	O
p	O
{	O
n	O
<	O
m	O
}	O
+p	O
{	O
ln	O
(	O
d*	O
»	O
l*+e	O
}	O
<	O
p	O
{	O
binomial	B
(	O
n	O
,	O
p	O
)	O
<	O
m	O
}	O
+	O
__	O
n	O
___	O
_	O
e	O
{	O
l	O
(	O
d*	O
)	O
}	O
-	O
l	O
*	O
(	O
where	O
p	O
=	O
p	O
{	O
y	O
=	O
z	O
}	O
=	O
1	O
-	O
l	O
*	O
~	O
1/2	O
,	O
and	O
by	O
markov	O
's	O
inequality	B
)	O
e	O
=	O
0	O
(	O
1	O
)	O
,	O
by	O
the	O
law	O
of	O
large	O
numbers	O
(	O
here	O
we	O
use	O
m/n	O
~	O
0	O
)	O
,	O
and	O
by	O
lemma	O
19.2	O
(	O
here	O
we	O
use	O
m	O
~	O
(	O
0	O
)	O
.	O
0	O
19.2	O
edited	B
nearest	O
neighbor	O
rules	O
edited	B
nearest	O
neighbor	O
rules	O
are	O
i-nn	O
rules	O
that	O
are	O
based	O
upon	O
carefully	O
selected	O
subsets	O
(	O
x~	O
,	O
y	O
{	O
)	O
,	O
...	O
,	O
(	O
x~	O
,	O
yi~	O
)	O
'	O
this	O
situation	O
is	O
partially	O
dealt	O
with	O
in	O
the	O
previous	O
section	O
,	O
as	O
the	O
frontier	O
between	O
condensed	B
and	O
edited	B
nearest	O
neighbor	O
rules	O
is	O
ill	O
(	O
cid:173	O
)	O
defined	O
.	O
the	O
idea	O
of	O
editing	O
based	O
upon	O
the	O
k-nn	O
rule	B
was	O
first	O
suggested	O
by	O
wilson	O
(	O
1972	O
)	O
and	O
later	O
studied	O
by	O
wagner	O
(	O
1973	O
)	O
and	O
penrod	O
and	O
wagner	O
(	O
1977	O
)	O
.	O
wilson	O
suggests	O
the	O
following	O
scheme	O
:	O
compute	O
(	O
xi	O
,	O
yi	O
,	O
zi	O
)	O
,	O
where	O
zi	O
is	O
the	O
k-nn	O
decision	O
at	O
xi	O
based	O
on	O
the	O
full	O
data	O
set	O
with	O
(	O
xi	O
,	O
yi	O
)	O
deleted	B
.	O
then	O
eliminate	O
all	O
data	O
pairs	O
for	O
which	O
yi	O
=i	O
zi	O
.	O
the	O
remaining	O
data	O
pairs	O
are	O
used	O
with	O
the	O
i-nn	O
rule	B
(	O
not	O
the	O
k-nn	O
rule	B
)	O
.	O
another	O
rule	B
,	O
based	O
upon	O
data	O
splitting	O
is	O
dealt	O
with	O
by	O
devijver	O
and	O
kittler	O
(	O
1982	O
)	O
.	O
a	O
survey	O
is	O
given	O
by	O
dasarathy	O
(	O
1991	O
)	O
,	O
devijver	O
(	O
1980	O
)	O
,	O
and	O
devijver	O
and	O
kittler	O
(	O
1980	O
)	O
.	O
repeated	O
editing	O
was	O
investigated	O
by	O
tomek	O
(	O
1976a	O
)	O
.	O
devijver	O
and	O
kittler	O
(	O
1982	O
)	O
propose	O
a	O
modification	O
of	O
wilson	O
's	O
leave-one-out	B
method	O
of	O
editing	O
based	O
upon	O
data	O
splitting	O
.	O
19.3	O
sieves	O
and	O
prototypes	O
let	O
gn	O
be	O
a	O
rule	O
that	O
uses	O
the	O
i-nn	O
classification	O
based	O
upon	O
prototype	B
data	O
pairs	O
(	O
x~	O
,	O
y	O
{	O
)	O
,	O
...	O
,	O
(	O
x~	O
,	O
y~	O
)	O
thatdependinsomefashionontheoriginaldata.ifthepairs	O
form	O
a	O
subset	O
of	O
the	O
data	O
pairs	O
(	O
and	O
thus	O
,	O
m	O
:	O
:	O
:	O
:	O
;	O
n	O
)	O
,	O
we	O
have	O
edited	B
or	O
condensed	B
nearest	O
neighbor	O
rules	O
.	O
however	O
,	O
the	O
(	O
x	O
;	O
,	O
yf	O
)	O
pairs	O
may	O
be	O
strategically	O
picked	O
outside	O
the	O
original	O
data	O
set	O
.	O
for	O
example	O
,	O
in	O
relabeling	O
(	O
see	O
section	O
11.7	O
)	O
,	O
m	O
=	O
n	O
,	O
x	O
;	O
=	O
xi	O
and	O
y/	O
=	O
g~	O
(	O
xi	O
)	O
'	O
where	O
g~	O
(	O
xi	O
)	O
is	O
the	O
k-nn	O
decision	O
at	O
xi	O
.	O
under	O
some	O
conditions	O
,	O
the	O
relabeling	B
rule	O
is	O
consistent	O
(	O
see	O
theorem	B
11.2	O
)	O
.	O
the	O
true	O
objective	O
of	O
proto	O
typing	O
is	O
to	O
extract	O
information	O
from	O
the	O
data	O
by	O
insisting	O
that	O
m	O
be	O
much	O
smaller	O
than	O
n.	O
310	O
19.	O
condensed	B
and	O
edited	B
nearest	O
neighbor	O
rules	O
...	O
.	O
..	O
•	O
•	O
figure	O
19.2.	O
a	O
1-nn	O
rule	B
based	O
upon	O
4	O
prototypes	O
.	O
in	O
this	O
example	O
,	O
all	O
the	O
data	O
points	O
are	O
correctly	O
classified	O
based	O
upon	O
the	O
prototype	B
1-nn	O
rule	B
.	O
:	O
chang	O
(	O
1974	O
)	O
describes	O
a	O
rule	O
in	O
which	O
we	O
iterate	O
the	O
following	O
step	O
until	O
a	O
given	O
stopping	O
rule	B
is	O
satisfied	O
:	O
merge	O
the	O
two	O
closest	O
nearest	O
neighbors	O
of	O
the	O
same	O
class	O
and	O
replace	O
both	O
pairs	O
by	O
a	O
new	O
average	O
prototype	B
pair	O
.	O
kohonen	O
(	O
1988	O
;	O
1990	O
)	O
recognizes	O
the	O
advantages	O
of	O
such	O
prototyping	O
in	O
general	O
as	O
a	O
device	O
for	O
partitioning	O
rd-he	O
calls	O
this	O
learning	B
vector	I
quantization	I
.	O
this	O
theme	O
was	O
picked	O
up	O
again	O
by	O
geva	O
and	O
sitte	O
(	O
1991	O
)	O
,	O
who	O
pick	O
x~	O
,	O
...	O
,	O
x~	O
as	O
a	O
random	O
subset	O
of	O
x	O
i	O
,	O
...	O
,	O
xn	O
and	O
allow	O
y	O
(	O
to	O
be	O
different	O
from	O
yi	O
.	O
diverging	O
a	O
bit	O
from	O
geva	O
and	O
sitte	O
,	O
we	O
might	O
minimize	O
the	O
empirical	B
error	I
with	O
the	O
prototyped	O
1-nn	O
rule	B
over	O
all	O
y	O
{	O
,	O
...	O
,	O
y~	O
,	O
where	O
the	O
empirical	B
error	I
is	O
that	O
committed	O
on	O
the	O
remaining	O
data	O
.	O
we	O
show	O
that	O
this	O
simple	O
strategy	O
leads	O
to	O
a	O
bayes-risk	O
consistent	B
rule	I
whenever	O
m	O
--	O
-+	O
00	O
and	O
min	O
--	O
-+	O
o.	O
note	O
,	O
in	O
particular	O
,	O
that	O
we	O
may	O
take	O
(	O
x~	O
,	O
...	O
,	O
x~	O
)	O
=	O
(	O
xl	O
,	O
...	O
,	O
xm	O
)	O
,	O
and	O
that	O
we	O
``	O
throwaway	O
''	O
yi	O
,	O
...	O
,	O
y	O
m	O
,	O
as	O
these	O
are	O
not	O
used	O
.	O
we	O
may	O
,	O
in	O
fact	O
,	O
use	O
additional	O
data	O
with	O
missing	O
yi-values	O
for	O
this	O
purpose-the	O
unclassified	O
data	O
are	O
thus	O
efficiently	O
used	O
to	O
partition	B
the	O
space	O
.	O
let	O
be	O
the	O
empirical	B
risk	I
on	O
the	O
remaining	O
data	O
,	O
where	O
gn	O
is	O
the	O
1-nn	O
rule	B
based	O
upon	O
(	O
x	O
1	O
,	O
yd	O
,	O
...	O
,	O
(	O
xm	O
,	O
y~	O
)	O
.	O
let	O
g~	O
be	O
the	O
i-nn	O
rule	B
with	O
the	O
choice	O
of	O
y	O
{	O
,	O
...	O
,	O
y~	O
that	O
minimizes	O
ln	O
(	O
gn	O
)	O
.	O
let	O
l	O
(	O
g~	O
)	O
denote	O
its	O
probability	O
of	O
error	O
.	O
theorem	B
19.5.	O
l	O
(	O
g	O
,	O
:	O
)	O
--	O
-+	O
l	O
*	O
in	O
probability	O
for	O
all	O
distributions	O
whenever	O
m	O
--	O
-+	O
00	O
and	O
min	O
--	O
-+	O
o.	O
proof	O
.	O
there	O
are	O
2m	O
different	O
possible	O
functions	O
gn	O
'	O
thus	O
,	O
p	O
{	O
s~	O
''	O
p	O
iln	O
(	O
gn	O
)	O
-	O
l	O
(	O
gn	O
)	O
1	O
>	O
€	O
i	O
xl	O
,	O
...	O
,	O
xn	O
}	O
<	O
2m	O
supp	O
{	O
iln	O
(	O
gn	O
)	O
-	O
l	O
(	O
gn	O
)	O
1	O
>	O
ei	O
xl	O
,	O
...	O
,	O
xm	O
}	O
gn	O
by	O
hoeffding	O
's	O
inequality	B
.	O
also	O
,	O
p	O
{	O
l	O
(	O
g	O
]	O
:	O
)	O
>	O
l	O
*	O
+	O
3e	O
}	O
<	O
p	O
{	O
l	O
(	O
g~	O
)	O
-	O
ln	O
(	O
g~	O
)	O
>	O
e	O
}	O
+	O
p	O
{	O
ln	O
(	O
gn	O
)	O
-	O
l	O
(	O
gn	O
)	O
>	O
e	O
}	O
+	O
p	O
{	O
l	O
(	O
gn	O
)	O
>	O
l	O
*	O
+	O
e	O
}	O
19.3	O
sieves	O
and	O
prototypes	O
311	O
(	O
gn	O
minimizes	O
l	O
(	O
gn	O
»	O
<	O
2e	O
{	O
p	O
{	O
s~	O
''	O
p	O
iln	O
(	O
gn	O
)	O
-	O
l	O
(	O
gn	O
)	O
1	O
>	O
,	O
i	O
xl	O
,	O
...	O
,	O
xn	O
}	O
}	O
(	O
here	O
we	O
used	O
ln	O
(	O
gn	O
)	O
~	O
ln	O
(	O
g	O
]	O
~	O
»	O
+	O
p	O
{	O
l	O
(	O
g	O
;	O
)	O
>	O
l	O
*	O
+	O
e	O
}	O
(	O
where	O
g	O
;	O
is	O
the	O
1-nn	O
rule	B
based	O
on	O
(	O
xl	O
,	O
zl	O
)	O
,	O
...	O
,	O
(	O
xm	O
,	O
zm	O
)	O
with	O
zi	O
=	O
i	O
(	O
y/	O
(	O
x	O
;	O
»	O
lj2	O
}	O
as	O
in	O
lemma	O
19.1	O
)	O
2m+2e-	O
(	O
n-m	O
)	O
e	O
2	O
+	O
0	O
(	O
1	O
)	O
<	O
(	O
if	O
m	O
-+	O
00	O
,	O
by	O
lemma	O
19.1	O
)	O
=	O
0	O
(	O
1	O
)	O
if	O
m	O
-+	O
00	O
and	O
m	O
/	O
n	O
-+	O
0	O
.	O
0	O
if	O
we	O
let	O
xi	O
,	O
...	O
,	O
x	O
;	O
n	O
have	O
arbitrary	O
values-not	O
only	O
among	O
those	O
taken	O
by	O
xl	O
,	O
...	O
,	O
xn-then	O
we	O
get	O
a	O
much	O
larger	O
,	O
more	O
flexible	O
class	O
of	O
classifiers	O
.	O
for	O
ex	O
(	O
cid:173	O
)	O
ample	O
,	O
every	O
linear	B
discriminant	I
is	O
nothing	O
but	O
a	O
prototype	O
i-nn	O
rule	B
with	O
m	O
=	O
2-	O
just	O
take	O
(	O
x~	O
,	O
1	O
)	O
,	O
(	O
x	O
;	O
,	O
0	O
)	O
and	O
place	O
x~	O
and	O
x	O
;	O
in	O
the	O
right	O
places	O
.	O
in	O
this	O
sense	O
,	O
prototype	B
1-nn	O
rules	O
generalize	O
a	O
vast	O
class	O
of	O
rules	O
.	O
the	O
most	O
promising	O
strategy	O
of	O
choosing	O
prototypes	O
is	O
to	O
minimize	O
the	O
empirical	B
error	I
committed	O
on	O
the	O
train	O
(	O
cid:173	O
)	O
ing	O
sequence	O
dn	O
.	O
finding	O
this	O
optimum	O
may	O
be	O
computationally	O
very	O
expensive	O
.	O
nevertheless	O
,	O
the	O
theoretical	B
properties	O
provided	O
in	O
the	O
next	O
result	O
may	O
provide	O
useful	O
guidance	O
.	O
theorem	B
19.6.	O
let	O
c	O
be	O
the	O
class	O
of	O
nearest	B
neighbor	I
rules	I
based	O
on	O
prototype	B
pairs	O
(	O
xl	O
,	O
yd	O
,	O
...	O
,	O
(	O
xm	O
,	O
ym	O
)	O
,	O
m	O
~	O
3	O
,	O
where	O
the	O
(	O
xi	O
,	O
yi	O
)	O
's	O
range	O
through	O
nd	O
x	O
{	O
o	O
,	O
i	O
}	O
.	O
given	O
the	O
training	O
data	O
dn	O
=	O
(	O
x	O
i	O
,	O
yd	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
,	O
let	O
gn	O
be	O
the	O
nearest	B
neighbor	I
rule	I
from	O
c	O
minimizing	O
the	O
error	O
estimate	O
then	O
for	O
each	O
e	O
>	O
0	O
,	O
the	O
rule	B
is	O
consistent	O
if	O
m	O
-+	O
00	O
such	O
that	O
m	O
2	O
log	O
m	O
=	O
o	O
(	O
n	O
)	O
.	O
for	O
d	O
=	O
1	O
and	O
d	O
=	O
2	O
312	O
19.	O
condensed	B
and	O
edited	B
nearest	O
neighbor	O
rules	O
the	O
probability	O
bound	O
may	O
be	O
improved	O
significantly	O
.	O
for	O
d	O
=	O
1	O
,	O
andford	O
=	O
2	O
,	O
in	O
both	O
cases	O
,	O
the	O
rule	B
is	O
consistent	O
if	O
m	O
--	O
-+	O
00	O
and	O
m	O
log	O
m	O
=	O
o	O
(	O
n	O
)	O
.	O
proof	O
.	O
it	O
follows	O
from	O
theorem	B
12.6	O
that	O
where	O
s	O
(	O
c	O
,	O
n	O
)	O
is	O
the	O
n-th	O
shatter	B
coefficient	I
of	O
the	O
class	O
of	O
sets	O
{	O
x	O
:	O
¢	O
(	O
x	O
)	O
=	O
i	O
}	O
,	O
¢	O
e	O
c.	O
all	O
we	O
need	O
is	O
to	O
find	O
suitable	O
upper	O
bounds	O
for	O
s	O
(	O
c	O
,	O
n	O
)	O
.	O
each	O
classifier	B
¢	O
is	O
a	O
partitioning	O
rule	B
based	O
on	O
the	O
m	O
voronoi	O
cells	O
defined	O
by	O
xl	O
,	O
...	O
,	O
x	O
m	O
.	O
therefore	O
,	O
s	O
(	O
c	O
,	O
n	O
)	O
is	O
not	O
more	O
than	O
2m	O
times	O
the	O
number	O
of	O
different	O
ways	O
n	O
points	O
in	O
nd	O
can	O
be	O
partitioned	O
by	O
voronoi	O
partitions	O
defined	O
by	O
m	O
points	O
.	O
in	O
each	O
partition	B
,	O
there	O
are	O
at	O
most	O
m	O
(	O
m	O
-	O
1	O
)	O
/2	O
cell	O
boundaries	O
that	O
are	O
subsets	O
of	O
d	O
-	O
i-dimensional	O
hyperplanes	O
.	O
thus	O
,	O
the	O
sought	O
number	O
is	O
not	O
greater	O
than	O
the	O
number	O
of	O
different	O
ways	O
m	O
(	O
m	O
-	O
1	O
)	O
/2	O
hyperplanes	O
can	O
partition	B
n	O
points	O
.	O
by	O
results	O
of	O
chapter	O
13	O
,	O
this	O
is	O
at	O
most	O
n	O
(	O
d+l	O
)	O
m	O
(	O
m-l	O
)	O
/2	O
,	O
proving	O
the	O
first	O
inequality	B
.	O
the	O
other	O
two	O
inequalities	O
follow	O
by	O
sharper	O
bounds	O
on	O
the	O
number	O
of	O
cell	O
boundaries	O
.	O
for	O
d	O
=	O
1	O
,	O
this	O
is	O
clearly	O
at	O
most	O
m.	O
to	O
prove	O
the	O
third	O
inequality	B
,	O
for	O
each	O
voronoi	O
partition	B
construct	O
a	O
graph	O
whose	O
vertices	O
are	O
xl	O
,	O
.•	O
.	O
,	O
x	O
m	O
,	O
and	O
two	O
vertices	O
are	O
connected	O
with	O
an	O
edge	O
if	O
and	O
only	O
if	O
their	O
corresponding	O
voronoi	O
cells	O
are	O
connected	O
.	O
it	O
is	O
easy	O
to	O
see	O
that	O
this	O
graph	O
is	O
planar	O
.	O
but	O
the	O
number	O
of	O
edges	O
of	O
a	O
planar	O
graph	O
with	O
m	O
vertices	O
can	O
not	O
exceed	O
3m	O
-	O
6	O
(	O
see	O
nishizeki	O
and	O
chiba	O
(	O
1988	O
,	O
p.10	O
)	O
)	O
which	O
proves	O
the	O
inequality	B
.	O
the	O
consistency	B
results	O
follow	O
from	O
the	O
stated	O
inequalities	O
and	O
from	O
the	O
fact	O
that	O
inf¢ec	O
l	O
(	O
¢	O
)	O
tends	O
to	O
l	O
*	O
as	O
m	O
--	O
-+	O
00	O
(	O
check	O
the	O
proof	O
of	O
theorem	O
5.15	O
again	O
)	O
.	O
0	O
problems	O
and	O
exercises	O
lim	O
infn-+cxj	O
e	O
{	O
n	O
}	O
/n	O
>	O
°	O
whenever	O
l	O
*	O
>	O
0.	O
true	O
or	O
false	O
:	O
if	O
l	O
*	O
=	O
0	O
,	O
then	O
e	O
{	O
n	O
}	O
=	O
o	O
(	O
n	O
)	O
.	O
problem	O
19.1.	O
let	O
n	O
:	O
s	O
n	O
be	O
the	O
size	O
of	O
the	O
data	O
set	O
after	O
pure	O
condensing	O
.	O
show	O
that	O
hint	O
:	O
consider	O
the	O
real	O
line	O
,	O
and	O
note	O
that	O
all	O
points	O
whose	O
right	O
and	O
left	O
neighbors	O
are	O
of	O
the	O
same	O
class	O
are	O
eliminated	O
.	O
problem	O
19.2.	O
let	O
(	O
xl	O
,	O
yd	O
,	O
(	O
x2	O
'	O
y2	O
)	O
,	O
...	O
be	O
an	O
i.i.d	O
.	O
sequence	O
of	O
pairs	O
of	O
random	O
variables	O
in	O
n	O
d	O
x	O
{	O
a	O
,	O
l	O
}	O
with	O
pry	O
]	O
=	O
llx	O
]	O
=	O
x	O
}	O
=	O
ry	O
(	O
x	O
)	O
.	O
let	O
(	O
x	O
'	O
,	O
z	O
)	O
be	O
the	O
first	O
pair	O
(	O
xi	O
,	O
yi	O
)	O
in	O
problems	O
and	O
exercises	O
313	O
the	O
sequence	O
such	O
that	O
yi	O
=	O
i	O
{	O
i	O
)	O
(	O
x	O
;	O
»	O
jj2	O
}	O
.	O
show	O
that	O
the	O
distribution	B
~	O
'	O
of	O
x	O
'	O
is	O
absolutely	O
continuous	O
with	O
respect	O
to	O
the	O
common	O
distribution	B
~	O
of	O
the	O
xi	O
's	O
,	O
with	O
density	O
(	O
i.e.	O
,	O
radon	O
(	O
cid:173	O
)	O
nikodym	O
derivative	O
)	O
d~	O
'	O
d	O
~	O
(	O
x	O
)	O
=	O
1	O
-	O
min	O
(	O
1j	O
(	O
x	O
)	O
,	O
1	O
-	O
17	O
(	O
x	O
)	O
)	O
1	O
-	O
l	O
*	O
'	O
where	O
l*	O
is	O
the	O
bayes	O
error	O
corresponding	O
to	O
(	O
xl	O
,	O
yl	O
)	O
.	O
let	O
y	O
be	O
a	O
{	O
a	O
,	O
l	O
}	O
-valued	O
random	O
variable	B
with	O
p	O
{	O
y	O
=	O
11x	O
'	O
=	O
x	O
}	O
=	O
1j	O
(	O
x	O
)	O
.	O
if	O
l	O
'	O
denotes	O
the	O
bayes	O
error	O
corresponding	O
to	O
(	O
x	O
'	O
,	O
y	O
)	O
,	O
then	O
show	O
that	O
l	O
'	O
:	O
:	O
:	O
:	O
l	O
*	O
.	O
problem	O
19.3.	O
consider	O
the	O
following	O
edited	B
nn	O
rule	B
.	O
the	O
pair	O
(	O
xi	O
,	O
yz	O
)	O
is	O
eliminated	O
from	O
the	O
training	O
sequence	O
if	O
the	O
k-nn	O
rule	B
(	O
based	O
on	O
the	O
remaining	O
n	O
-	O
1	O
pairs	O
)	O
incorrectly	O
classifies	O
xi	O
'	O
the	O
i-nn	O
rule	B
is	O
used	O
with	O
the	O
edited	B
data	O
.	O
show	O
that	O
this	O
rule	B
is	O
consistent	O
whenever	O
x	O
has	O
a	O
density	O
if	O
k	O
--	O
-+	O
00	O
and	O
k	O
/	O
n	O
--	O
-+	O
o.	O
related	O
papers	O
:	O
wilson	O
(	O
1972	O
)	O
,	O
wagner	O
(	O
1973	O
)	O
,	O
penrod	O
and	O
wagner	O
(	O
1977	O
)	O
,	O
and	O
devijver	O
and	O
kittler	O
(	O
1980	O
)	O
.	O
20	O
tree	B
classifiers	O
classification	O
trees	O
partition	B
nd	O
into	O
regions	O
,	O
often	O
hyperrectangles	O
parallel	O
to	O
the	O
axes	O
.	O
among	O
these	O
,	O
the	O
most	O
important	O
are	O
the	O
binary	B
classification	O
trees	O
,	O
since	O
they	O
have	O
just	O
two	O
children	O
per	O
node	O
and	O
are	O
thus	O
easiest	O
to	O
manipulate	O
and	O
update	O
.	O
we	O
recall	O
the	O
simple	O
terminology	O
of	O
books	O
on	O
data	O
structures	O
.	O
the	O
top	O
of	O
a	O
binary	O
tree	B
is	O
called	O
the	O
root	O
.	O
each	O
node	O
has	O
either	O
no	O
child	O
(	O
in	O
that	O
case	O
it	O
is	O
called	O
a	O
terminal	O
node	O
or	O
leaf	O
)	O
,	O
a	O
left	O
child	O
,	O
a	O
right	O
child	O
,	O
or	O
a	O
left	O
child	O
and	O
a	O
right	O
child	O
.	O
each	O
node	O
is	O
the	O
root	O
of	O
a	O
tree	O
itself	O
.	O
the	O
trees	O
rooted	O
at	O
the	O
children	O
of	O
a	O
node	O
are	O
called	O
the	O
left	O
and	O
right	O
subtrees	O
of	O
that	O
node	O
.	O
the	O
depth	B
of	I
a	O
node	O
is	O
the	O
length	O
of	O
the	O
path	O
from	O
the	O
node	O
to	O
the	O
root	O
.	O
the	O
height	B
of	I
a	O
tree	B
is	O
the	O
maximal	O
depth	B
of	I
any	O
node	O
.	O
trees	O
with	O
more	O
than	O
two	O
children	O
per	O
node	O
can	O
be	O
reduced	O
to	O
binary	B
trees	O
by	O
root	O
figure	O
20.1.	O
a	O
binary	O
tree	B
.	O
right	O
child	O
...	O
...	O
.	O
.	O
leaf	O
~	O
...	O
...	O
...	O
.	O
,	O
,	O
,	O
316	O
20.	O
tree	B
classifiers	O
a	O
simple	O
device-just	O
associate	O
a	O
left	O
child	O
with	O
each	O
node	O
by	O
selecting	O
the	O
oldest	O
child	O
in	O
the	O
list	O
of	O
children	O
.	O
call	O
the	O
right	O
child	O
of	O
a	O
node	O
its	O
next	O
sibling	O
(	O
see	O
figures	O
20.2	O
and	O
20.3	O
)	O
.	O
the	O
new	O
binary	B
tree	O
is	O
called	O
the	O
oldest-childlnext-sibling	O
binary	B
tree	O
(	O
see	O
,	O
e.g.	O
,	O
cormen	O
,	O
leiserson	O
,	O
and	O
rivest	O
(	O
1990	O
)	O
for	O
a	O
general	O
introduction	O
)	O
.	O
we	O
only	O
mention	O
this	O
particular	O
mapping	O
because	O
it	O
enables	O
us	O
to	O
only	O
consider	O
binary	B
trees	O
for	O
simplicity	O
.	O
a	O
a	O
b	O
f	O
g	O
h	O
h	O
figure	O
20.2.	O
ordered	B
tree	O
:	O
the	O
chil	O
(	O
cid:173	O
)	O
dren	O
are	O
ordered	B
from	O
oldest	O
to	O
youngest	O
.	O
figure	O
20.3.	O
the	O
corresponding	O
bi	O
(	O
cid:173	O
)	O
nary	O
tree	B
.	O
in	O
a	O
classification	O
tree	B
,	O
each	O
node	O
represents	O
a	O
set	O
in	O
the	O
space	O
nd	O
.	O
also	O
,	O
each	O
node	O
has	O
exactly	O
two	O
or	O
zero	O
children	O
.	O
if	O
a	O
node	O
u	O
represents	O
the	O
set	O
a	O
and	O
its	O
children	O
u	O
'	O
,	O
u	O
''	O
represent	O
a	O
'	O
and	O
a	O
''	O
,	O
then	O
we	O
require	O
that	O
a	O
=	O
a	O
'	O
u	O
a	O
''	O
and	O
a	O
'	O
n	O
a	O
''	O
=	O
0.	O
the	O
root	O
represents	O
n	O
d	O
,	O
and	O
the	O
leaves	O
,	O
taken	O
together	O
,	O
form	O
a	O
partition	O
of	O
nd	O
.	O
assume	O
that	O
we	O
know	O
x	O
e	O
a.	O
then	O
the	O
question	O
``	O
is	O
x	O
e	O
a	O
?	O
''	O
should	O
be	O
answered	O
in	O
a	O
computationally	O
simple	O
manner	O
so	O
as	O
to	O
conserve	O
time	O
.	O
therefore	O
,	O
if	O
x	O
=	O
(	O
x	O
(	O
l	O
)	O
,	O
...	O
,	O
xed	O
»	O
~	O
,	O
we	O
may	O
just	O
limit	O
ourselves	O
to	O
questions	O
of	O
the	O
following	O
forms	O
:	O
(	O
i	O
)	O
is	O
xu	O
)	O
:	O
:	O
:	O
ex	O
?	O
this	O
leads	O
to	O
ordinary	B
binary	O
classification	O
trees	O
with	O
partitions	O
into	O
hyperrectangles	O
.	O
(	O
ii	O
)	O
is	O
alx	O
(	O
l	O
)	O
+	O
...	O
+	O
adx	O
(	O
d	O
)	O
:	O
:	O
:	O
ex	O
?	O
this	O
leads	O
to	O
bsp	O
trees	O
(	O
binary	O
space	O
partition	O
trees	O
)	O
.	O
each	O
decision	O
is	O
more	O
time	O
consuming	O
,	O
but	O
the	O
space	O
is	O
more	O
flexibly	O
cut	O
up	O
into	O
convex	O
polyhedral	O
cells	O
.	O
(	O
iii	O
)	O
is	O
ilx	O
-	O
z	O
\i	O
:	O
:	O
:	O
ex	O
?	O
(	O
here	O
z	O
is	O
a	O
point	O
of	O
n	O
d	O
,	O
to	O
be	O
picked	O
for	O
each	O
node	O
.	O
)	O
this	O
induces	O
a	O
partition	O
into	O
pieces	O
of	O
spheres	O
.	O
such	O
trees	O
are	O
called	O
sphere	B
trees	O
.	O
(	O
iv	O
)	O
is	O
ljj	O
(	O
x	O
)	O
2	O
:	O
o	O
?	O
here	O
,	O
ljj	O
is	O
a	O
nonlinear	O
function	O
,	O
different	O
for	O
each	O
node	O
.	O
every	O
classifier	B
can	O
be	O
thought	O
of	O
as	O
being	O
described	O
in	O
this	O
format-decide	O
class	O
one	O
if	O
ljj	O
(	O
x	O
)	O
2	O
:	O
o.	O
however	O
,	O
this	O
misses	O
the	O
point	O
,	O
as	O
tree	B
classifiers	O
should	O
really	O
be	O
built	O
up	O
from	O
fundamental	O
atomic	O
operations	O
and	O
queries	O
such	O
as	O
those	O
listed	O
in	O
(	O
i	O
)	O
-	O
(	O
iii	O
)	O
.	O
we	O
will	O
not	O
consider	O
such	O
trees	O
any	O
further	O
.	O
20.	O
tree	B
classifiers	O
317	O
-	O
i	O
-	O
-	O
-	O
i	O
--	O
-	O
figure	O
20.4.	O
partition	B
induced	O
by	O
an	O
ordinary	B
binary	O
tree	B
.	O
figure	O
20.5.	O
corresponding	O
tree	B
.	O
figure	O
20.6.	O
partition	B
induced	O
by	O
a	O
bsp	O
tree	B
.	O
figure	O
20.7.	O
partition	B
induced	O
by	O
a	O
sphere	O
tree	B
.	O
we	O
associate	O
a	O
class	O
in	O
some	O
manner	O
with	O
each	O
leaf	O
in	O
a	O
classification	O
tree	B
.	O
the	O
tree	B
structure	O
is	O
usually	O
data	O
dependent	O
,	O
as	O
well	O
,	O
and	O
indeed	O
,	O
it	O
is	O
in	O
the	O
construction	O
itself	O
where	O
methods	O
differ	O
.	O
if	O
a	O
leaf	O
represents	O
region	O
a	O
,	O
then	O
we	O
say	O
that	O
the	O
classifier	B
gn	O
is	O
natural	O
if	O
if	O
l	O
yi	O
>	O
l	O
(	O
1	O
-	O
yi	O
)	O
,	O
x	O
e	O
a	O
i	O
:	O
xiea	O
i	O
:	O
xiea	O
otherwise	O
.	O
that	O
is	O
,	O
in	O
every	O
leaf	O
region	O
,	O
we	O
take	O
a	O
majority	O
vote	O
over	O
all	O
(	O
xi	O
,	O
yi	O
)	O
's	O
with	O
xi	O
in	O
the	O
same	O
region	O
.	O
ties	O
are	O
broken	O
,	O
as	O
usual	O
,	O
in	O
favor	O
of	O
class	O
o.	O
in	O
this	O
set-up	O
,	O
natural	O
tree	O
classifiers	O
are	O
but	O
special	O
cases	O
of	O
data-dependent	O
partitioning	B
rules	I
.	O
the	O
latter	O
are	O
further	O
described	O
in	O
detail	O
in	O
chapter	O
21	O
.	O
318	O
20.	O
tree	B
classifiers	O
figure	O
20.8.	O
a	O
natural	O
classifier	B
based	O
on	O
an	O
ordinary	B
binary	O
tree	B
.	O
the	O
deci	O
(	O
cid:173	O
)	O
sion	O
is	O
1	O
in	O
regions	O
where	O
points	O
with	O
label	O
1	O
form	O
a	O
majority	O
.	O
these	O
areas	O
are	O
shaded	O
.	O
regular	B
histograms	O
can	O
also	O
be	O
thought	O
of	O
as	O
natural	O
binary	O
tree	B
classifiers-the	O
construction	O
and	O
relationship	O
is	O
obvious	O
.	O
however	O
,	O
as	O
n	O
--	O
-+	O
00	O
,	O
histograms	O
change	O
size	O
,	O
and	O
usually	O
,	O
histogram	O
partitions	O
are	O
not	O
nested	O
as	O
n	O
grows	O
.	O
trees	O
offer	O
the	O
exciting	O
perspective	O
of	O
fully	O
dynamic	O
classification-as	O
data	O
are	O
added	O
,	O
we	O
may	O
update	O
the	O
tree	B
slightly	O
,	O
say	O
,	O
by	O
splitting	O
a	O
leaf	O
or	O
so	O
,	O
to	O
obtain	O
an	O
updated	O
classifier	B
.	O
the	O
most	O
compelling	O
reason	O
for	O
using	O
binary	B
tree	O
classifiers	O
is	O
to	O
explain	O
com	O
(	O
cid:173	O
)	O
plicated	O
data	O
and	O
to	O
have	O
a	O
classifier	O
that	O
is	O
easy	O
to	O
analyze	O
and	O
understand	O
.	O
in	O
fact	O
,	O
expert	O
system	O
design	O
is	O
based	O
nearly	O
exclusively	O
upon	O
decisions	O
obtained	O
by	O
going	O
down	O
a	O
binary	O
classification	O
tree	B
.	O
some	O
argue	O
that	O
binary	B
classification	O
trees	O
are	O
preferable	O
over	O
bsp	O
trees	O
for	O
this	O
simple	O
reason	O
.	O
as	O
argued	O
in	O
breiman	O
,	O
fried	O
(	O
cid:173	O
)	O
man	O
,	O
olshen	O
,	O
and	O
stone	O
(	O
1984	O
)	O
,	O
trees	O
allow	O
mixing	O
component	O
variables	O
that	O
are	O
heterogeneous-some	O
components	O
may	O
be	O
of	O
a	O
nonnumerical	O
nature	O
,	O
others	O
may	O
represent	O
integers	O
,	O
and	O
still	O
others	O
may	O
be	O
real	O
numbers	O
.	O
20.1	O
invariance	B
nearly	O
all	O
rules	O
in	O
this	O
chapter	O
and	O
in	O
chapters	O
21	O
and	O
30	O
show	O
some	O
sort	O
of	O
invariance	O
with	O
respect	O
to	O
certain	O
transformations	O
of	O
the	O
input	O
.	O
this	O
is	O
often	O
a	O
major	O
asset	O
in	O
pattern	O
recognition	O
methods	O
.	O
we	O
say	O
a	O
rule	O
gn	O
is	O
invariant	O
under	O
transformation	O
t	O
if	O
for	O
all	O
values	O
of	O
the	O
arguments	O
.	O
in	O
this	O
sense	O
,	O
we	O
may	O
require	O
translation	O
invariance	B
,	O
rotation	B
invariance	I
,	O
linear	O
translation	O
invariance	B
,	O
and	O
monotone	O
transformation	O
invariance	B
(	O
t	O
(	O
·	O
)	O
maps	O
each	O
coordinate	O
separately	O
by	O
a	O
strictly	O
increasing	O
but	O
pos	O
(	O
cid:173	O
)	O
sibly	O
nonlinear	O
function	O
)	O
.	O
monotone	O
transformation	O
invariance	B
frees	O
us	O
from	O
worries	O
about	O
the	O
kind	O
of	O
measuring	O
unit	O
.	O
for	O
example	O
,	O
it	O
would	O
not	O
matter	O
whether	O
earthquakes	O
were	O
mea	O
(	O
cid:173	O
)	O
sured	O
on	O
a	O
logarithmic	O
(	O
richter	O
)	O
scale	O
or	O
a	O
linear	O
scale	O
.	O
rotation	B
invariance	I
matters	O
of	O
course	O
in	O
situations	O
in	O
which	O
input	O
data	O
have	O
no	O
natural	O
coordinate	O
axis	O
system	O
.	O
in	O
many	O
cases	O
,	O
data	O
are	O
of	O
the	O
ordinal	O
form-colors	O
and	O
names	O
spring	O
to	O
mind-and	O
20.2	O
trees	O
with	O
the	O
x-property	O
319	O
ordinal	O
values	O
may	O
be	O
translated	O
into	O
numeric	O
values	O
by	O
creating	O
bit	O
vectors	O
.	O
here	O
,	O
distance	B
loses	O
its	O
physical	O
meaning	O
,	O
and	O
any	O
rule	B
that	O
uses	O
ordinal	O
data	O
perhaps	O
mixed	O
in	O
with	O
numerical	O
data	O
should	O
be	O
monotone	O
transformation	O
invariant	O
.	O
tree	B
methods	O
that	O
are	O
based	O
upon	O
perpendicular	O
splits	O
are	O
usually	O
(	O
but	O
not	O
al	O
(	O
cid:173	O
)	O
ways	O
)	O
monotone	O
transformation	O
invariant	O
and	O
translation	O
invariant	O
.	O
tree	B
methods	O
based	O
upon	O
linear	O
hyperplane	O
splits	O
are	O
sometimes	O
linear	O
transformation	O
invariant	O
.	O
the	O
partitions	O
of	O
space	O
cause	O
some	O
problems	O
if	O
the	O
data	O
points	O
can	O
line	O
up	O
along	O
hyperplanes	O
.	O
this	O
is	O
just	O
a	O
matter	O
of	O
housekeeping	O
,	O
of	O
course	O
,	O
but	O
the	O
fact	O
that	O
some	O
projections	O
of	O
x	O
to	O
a	O
line	O
have	O
atoms	O
or	O
some	O
components	O
of	O
x	O
have	O
atoms	O
will	O
make	O
the	O
proofs	O
heavier	O
to	O
digest	O
.	O
for	O
this	O
reason	O
only	O
,	O
we	O
assume	O
throughout	O
this	O
chapter	O
that	O
x	O
has	O
a	O
density	O
f.	O
as	O
typically	O
no	O
conditions	O
are	O
put	O
on	O
f	O
in	O
our	O
consistency	B
theorems	O
,	O
it	O
will	O
be	O
relatively	O
easy	O
to	O
generalize	O
them	O
to	O
all	O
distributions	O
.	O
the	O
density	O
assumption	O
affords	O
us	O
the	O
luxury	O
of	O
being	O
able	O
to	O
say	O
that	O
,	O
with	O
probability	O
one	O
,	O
no	O
d	O
+	O
1	O
points	O
fall	O
in	O
a	O
hyperplane	B
,	O
no	O
d	O
points	O
fall	O
iii	O
a	O
hyperplane	O
perpendicular	O
to	O
one	O
axis	O
,	O
no	O
d	O
-	O
1	O
points	O
fall	O
in	O
a	O
hyperplane	B
perpendicular	O
to	O
two	O
axes	O
,	O
etcetera	O
.	O
if	O
a	O
rule	O
is	O
monotone	O
transformation	O
invariant	O
,	O
we	O
can	O
without	O
harm	O
transform	O
all	O
the	O
data	O
as	O
follows	O
for	O
the	O
purpose	O
of	O
analysis	O
only	O
.	O
let	O
ii	O
,	O
...	O
,	O
fd	O
be	O
the	O
marginal	O
densities	O
of	O
x	O
(	O
see	O
problem	O
20.1	O
)	O
,	O
with	O
corresponding	O
distribution	B
functions	O
f	O
i	O
,	O
...	O
,	O
fd	O
.	O
then	O
replace	O
in	O
the	O
data	O
each	O
xi	O
by	O
t	O
(	O
xi	O
)	O
,	O
where	O
each	O
component	O
of	O
t	O
(	O
xi	O
)	O
is	O
now	O
uniformly	O
distributed	O
on	O
[	O
0	O
,	O
1	O
]	O
.	O
of	O
course	O
,	O
as	O
we	O
do	O
not	O
know	O
t	O
beforehand	O
,	O
this	O
device	O
could	O
only	O
be	O
used	O
in	O
the	O
analysis	O
.	O
the	O
transformation	O
t	O
will	O
be	O
called	O
the	O
uniform	B
marginal	I
transformation	I
.	O
observe	O
that	O
the	O
original	O
density	O
is	O
now	O
transformed	O
into	O
another	O
density	O
.	O
20.2	O
trees	O
with	O
the	O
x	O
-property	O
it	O
is	O
possible	O
to	O
prove	O
the	O
convergence	O
of	O
many	O
tree	B
classifiers	O
all	O
at	O
once	O
.	O
what	O
is	O
needed	O
,	O
clearly	O
,	O
is	O
a	O
partition	O
into	O
small	O
regions	O
,	O
yet	O
,	O
most	O
majority	O
votes	O
should	O
be	O
over	O
sufficiently	O
large	O
sample	O
.	O
in	O
many	O
of	O
the	O
cases	O
considered	O
,	O
the	O
form	O
of	O
the	O
tree	B
is	O
determined	O
by	O
the	O
x/s	O
only	O
,	O
that	O
is	O
,	O
the	O
labels	O
yi	O
do	O
not	O
playa	O
role	O
in	O
constructing	O
the	O
partition	B
,	O
but	O
they	O
are	O
used	O
in	O
voting	O
.	O
this	O
is	O
of	O
course	O
rather	O
simplistic	O
,	O
but	O
as	O
a	O
start	O
,	O
it	O
is	O
very	O
convenient	O
.	O
we	O
will	O
say	O
that	O
the	O
classification	O
tree	B
has	O
the	O
x	O
-property	O
,	O
for	O
lack	O
of	O
a	O
better	O
mnemonic	O
.	O
let	O
the	O
leaf	O
regions	O
be	O
{	O
ai	O
,	O
...	O
,	O
an	O
}	O
(	O
with	O
n	O
possibly	O
random	O
)	O
.	O
define	O
n	O
j	O
as	O
the	O
number	O
of	O
xi	O
's	O
falling	O
in	O
a	O
j.	O
as	O
the	O
leaf	O
regions	O
form	O
a	O
partition	O
,	O
we	O
have	O
l~=i	O
n	O
j	O
=	O
n.	O
by	O
diam	O
(	O
a	O
j	O
)	O
we	O
mean	O
the	O
diameter	O
of	O
the	O
cell	O
a	O
j	O
,	O
that	O
is	O
,	O
the	O
maximal	O
distance	B
between	O
two	O
points	O
of	O
a	O
j.	O
finally	O
,	O
decisions	O
are	O
taken	O
by	O
majority	B
vote	I
,	O
so	O
for	O
x	O
e	O
a	O
j	O
,	O
1	O
s	O
j	O
s	O
n	O
,	O
320	O
20.	O
tree	B
classifiers	O
the	O
rule	B
is	O
i	O
0	O
gn	O
(	O
x	O
)	O
=	O
{	O
if	O
l	O
yi	O
>	O
l	O
(	O
1	O
-	O
yi	O
)	O
,	O
x	O
e	O
a	O
j	O
i	O
:	O
x	O
;	O
eaj	O
i	O
:	O
x	O
;	O
eaj	O
otherwise	O
.	O
a	O
(	O
x	O
)	O
denotes	O
the	O
set	O
ofthe	O
partition	B
{	O
ai	O
,	O
...	O
,	O
an	O
}	O
into	O
which	O
x	O
falls	O
,	O
and	O
n	O
(	O
x	O
)	O
is	O
the	O
number	O
of	O
data	O
points	O
falling	O
in	O
this	O
set	O
.	O
recall	O
that	O
the	O
general	O
consistency	B
result	O
given	O
in	O
theorem	O
6.1	O
is	O
applicable	O
in	O
such	O
cases	O
.	O
consider	O
a	O
natural	O
classification	O
tree	B
as	O
defined	O
above	O
and	O
assume	O
the	O
x	O
-property	O
.	O
theorem	B
6.1	O
states	O
that	O
then	O
e	O
{	O
l	O
n	O
}	O
~	O
l*	O
if	O
(	O
1	O
)	O
diam	O
(	O
a	O
(	O
x	O
»	O
~	O
0	O
in	O
probability	O
,	O
(	O
2	O
)	O
n	O
(	O
x	O
)	O
~	O
00	O
in	O
probability	O
.	O
a	O
more	O
general	O
,	O
but	O
also	O
more	O
complicated	O
consistency	B
theorem	O
is	O
proved	O
in	O
chap	O
(	O
cid:173	O
)	O
ter	O
21.	O
let	O
us	O
start	O
with	O
the	O
simplest	O
possible	O
example	O
.	O
we	O
verify	O
the	O
conditions	O
of	O
theorem	O
6.1	O
for	O
the	O
k-spacing	B
rule	I
in	O
one	O
dimension	B
.	O
this	O
rule	B
partitions	O
the	O
real	O
line	O
by	O
using	O
the	O
k-th	O
,	O
2k-th	O
(	O
and	O
so	O
on	O
)	O
order	B
statistics	I
(	O
mahalanobis	O
,	O
(	O
1961	O
)	O
;	O
see	O
also	O
parthasarathy	O
and	O
bhattacharya	O
(	O
1961	O
»	O
.	O
o	O
0	O
figure	O
20.9.	O
a	O
3-spacing	O
classifier	B
.	O
formally	O
,	O
let	O
k	O
<	O
n	O
be	O
a	O
positive	O
integer	O
,	O
and	O
let	O
x	O
(	O
l	O
)	O
,	O
x	O
(	O
2	O
)	O
,	O
.•	O
.	O
,	O
x	O
(	O
n	O
)	O
be	O
the	O
order	B
statistics	I
of	O
the	O
data	O
points	O
.	O
recall	O
that	O
x	O
(	O
l	O
)	O
,	O
x	O
(	O
2	O
)	O
,	O
...	O
,	O
x	O
(	O
n	O
)	O
are	O
obtained	O
by	O
permuting	O
xl	O
,	O
...	O
,	O
xn	O
in	O
such	O
a	O
way	O
that	O
x	O
(	O
l	O
)	O
:	O
s	O
x	O
(	O
2	O
)	O
:	O
s	O
...	O
:	O
s	O
x	O
(	O
n	O
)	O
.	O
note	O
that	O
this	O
ordering	O
is	O
unique	O
with	O
probability	O
one	O
as	O
x	O
has	O
a	O
density	O
.	O
we	O
partition	B
r	O
into	O
n	O
intervals	O
ai	O
,	O
...	O
,	O
an	O
,	O
where	O
n	O
=	O
r	O
n	O
i	O
k	O
l	O
,	O
such	O
that	O
for	O
j	O
=	O
1	O
,	O
...	O
,	O
n	O
-	O
1	O
,	O
a	O
j	O
satisfies	O
x	O
(	O
ku-l	O
)	O
+i	O
)	O
,	O
.	O
'	O
.	O
'	O
x	O
(	O
kj	O
)	O
e	O
a	O
j	O
,	O
and	O
the	O
rightmost	O
cell	O
an	O
satisfies	O
we	O
have	O
not	O
specified	O
the	O
endpoints	O
of	O
each	O
cell	O
of	O
the	O
partition	B
.	O
for	O
simplicity	O
,	O
let	O
the	O
borders	O
between	O
a	O
j	O
and	O
a	O
j+i	O
be	O
put	O
halfway	O
between	O
the	O
rightmost	O
data	O
point	O
in	O
a	O
j	O
and	O
leftmost	O
data	O
point	O
in	O
a	O
j+i	O
,	O
j	O
=	O
1	O
,	O
...	O
,	O
n	O
-	O
1.	O
the	O
classification	O
rule	B
gn	O
is	O
defined	O
in	O
the	O
usual	O
way	O
:	O
theorem	B
20.1.	O
let	O
gn	O
be	O
the	O
k-spacing	O
classifier	O
given	O
above	O
.	O
assume	O
that	O
the	O
distribution	B
of	O
x	O
has	O
a	O
density	O
f	O
on	O
r.	O
then	O
the	O
classification	O
rule	B
gn	O
is	O
consistent	O
,	O
that	O
is	O
,	O
limn-	O
,	O
>	O
-oo	O
e	O
{	O
l	O
n	O
}	O
=	O
l	O
*	O
,	O
if	O
k	O
~	O
00	O
and	O
kin	O
~	O
0	O
as	O
n	O
tends	O
to	O
infinity	O
.	O
20.2	O
trees	O
with	O
the	O
x	O
-property	O
321	O
remark	O
.	O
¥le	O
discuss	O
various	O
generalizations	O
of	O
this	O
rule	B
in	O
chapter	O
21	O
.	O
0	O
proof	O
.	O
we	O
check	O
the	O
conditions	O
of	O
theorem	O
6.1	O
,	O
as	O
the	O
partition	B
has	O
the	O
x	O
-property	O
.	O
condition	O
(	O
2	O
)	O
is	O
obvious	O
from	O
k	O
-+	O
00.	O
to	O
establish	O
condition	O
(	O
1	O
)	O
,	O
fix	O
e	O
>	O
o.	O
note	O
that	O
by	O
the	O
invariance	B
of	O
the	O
rule	B
under	O
monotone	O
transformations	O
,	O
we	O
may	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
f	O
is	O
the	O
uniform	B
density	O
on	O
[	O
0,1	O
]	O
.	O
among	O
the	O
intervals	O
ai	O
,	O
...	O
,	O
an	O
,	O
there	O
are	O
at	O
most	O
lie	O
disjoint	O
intervals	O
of	O
length	O
greater	O
than	O
e	O
in	O
[	O
0	O
,	O
1	O
]	O
.	O
thus	O
,	O
p	O
{	O
diam	O
(	O
a	O
(	O
x	O
)	O
)	O
>	O
e	O
}	O
<	O
~e	O
{	O
max	O
fl	O
(	O
a	O
j	O
)	O
}	O
e	O
l	O
'	O
,	O
,	O
:	O
:j~n	O
e	O
<	O
~e	O
{	O
(	O
m~x	O
fln	O
(	O
a	O
j	O
)	O
+	O
m.ax	O
ifl	O
(	O
a	O
j	O
)	O
-	O
<	O
h~	O
+	O
e	O
hp	O
ifl	O
(	O
a	O
)	O
-	O
fln	O
(	O
a	O
)	O
i	O
}	O
)	O
,	O
l~j~n	O
l~j~n	O
fln	O
(	O
a	O
j	O
)	O
i	O
)	O
}	O
where	O
the	O
supremum	O
is	O
taken	O
over	O
all	O
intervals	O
in	O
r.	O
the	O
first	O
term	O
within	O
the	O
parentheses	O
converges	O
to	O
zero	O
by	O
the	O
second	O
condition	O
of	O
the	O
theorem	B
,	O
while	O
the	O
second	O
term	O
goes	O
to	O
zero	O
by	O
an	O
obvious	O
extension	O
of	O
the	O
classical	O
glivenko-cantelli	O
theorem	B
(	O
theorem	B
12.4	O
)	O
.	O
this	O
completes	O
the	O
proof	O
.	O
0	O
we	O
will	O
encounter	O
several	O
trees	O
in	O
which	O
the	O
partition	B
is	O
determined	O
by	O
a	O
small	O
fraction	O
of	O
the	O
data	O
,	O
such	O
as	O
binary	B
k	O
-d	O
trees	O
and	O
quadtrees	O
.	O
in	O
these	O
cases	O
,	O
condition	O
(	O
2	O
)	O
of	O
theorem	O
6.1	O
may	O
be	O
verified	O
with	O
the	O
help	O
of	O
the	O
following	O
lemma	O
:	O
lemma	O
20.1.	O
let	O
pi	O
,	O
...	O
,	O
pk	O
be	O
a	O
probability	O
vector	O
.	O
let	O
n	O
l	O
,	O
...	O
,	O
nk	O
be	O
multino	O
(	O
cid:173	O
)	O
mially	O
distributed	O
random	O
variables	O
with	O
parameters	O
n	O
and	O
pi	O
,	O
...	O
,	O
pk·	O
then	O
if	O
the	O
random	O
variable	B
x	O
is	O
independent	O
oj	O
n	O
l	O
,	O
...	O
,	O
nb	O
andp	O
{	O
x	O
=	O
i	O
}	O
=	O
pi	O
,	O
we	O
have	O
for	O
any	O
m	O
,	O
p	O
{	O
nx	O
:	O
s	O
m	O
}	O
:	O
s	O
(	O
2m	O
+	O
4	O
)	O
k	O
.	O
n	O
(	O
note	O
:	O
this	O
probability	O
goes	O
to	O
0	O
if	O
kin	O
-+	O
0	O
uniformly	O
over	O
all	O
probability	O
vectors	O
with	O
k	O
components	O
!	O
)	O
proof	O
.	O
let	O
zi	O
be	O
binomial	B
with	O
parameters	O
n	O
and	O
pi	O
.	O
then	O
p	O
{	O
nx	O
:	O
s	O
m	O
}	O
<	O
i	O
:	O
npi~2m	O
i	O
:	O
npi	O
>	O
2m	O
<	O
<	O
2mk	O
--	O
+	O
n	O
i	O
:	O
npi	O
>	O
2m	O
l	O
pip	O
{	O
zi	O
-	O
ezi	O
:	O
s	O
m	O
-	O
npi	O
}	O
2mk	O
-	O
-	O
+	O
n	O
''	O
.	O
~	O
c	O
l	O
:	O
npi	O
>	O
2m	O
e~	O
}	O
p.p	O
z·	O
-ez·	O
<	O
-	O
-	O
2	O
l_	O
{	O
i	O
322	O
20.	O
tree	B
classifiers	O
<	O
<	O
<	O
_2m_k	O
+	O
'	O
''	O
4	O
p-	O
var	O
{	O
zd	O
i	O
:	O
n~m	O
i	O
(	O
e	O
{	O
zd	O
)	O
2	O
n	O
(	O
by	O
chebyshev	O
's	O
inequality	B
)	O
2mk	O
-+	O
l	O
4pi-	O
1	O
n	O
i	O
:	O
npi	O
>	O
2m	O
npi	O
(	O
2m	O
+4	O
)	O
k	O
o	O
n	O
the	O
previous	O
lemma	O
implies	O
that	O
for	O
any	O
binary	B
tree	O
classifier	B
constructed	O
on	O
the	O
basis	O
of	O
xl	O
,	O
...	O
,	O
x	O
k	O
with	O
k	O
+	O
1	O
regions	O
,	O
n	O
(	O
x	O
)	O
-+	O
00	O
in	O
probability	O
whenever	O
kl	O
(	O
n	O
-	O
k	O
)	O
-+	O
0	O
(	O
i.e.	O
,	O
kin	O
-+	O
0	O
)	O
.	O
it	O
suffices	O
to	O
note	O
that	O
we	O
may	O
take	O
m	O
arbitrarily	O
large	O
but	O
fixed	O
in	O
lemma	O
20.1.	O
this	O
remark	O
saves	O
us	O
the	O
trouble	O
of	O
having	O
to	O
verify	O
just	O
how	O
large	O
or	O
small	O
the	O
probability	O
mass	O
of	O
the	O
region	O
is	O
.	O
in	O
fact	O
,	O
it	O
also	O
implies	O
that	O
we	O
should	O
not	O
worry	O
so	O
much	O
about	O
regions	O
with	O
few	O
data	O
points	O
.	O
what	O
matters	O
more	O
than	O
anything	O
else	O
is	O
the	O
number	O
of	O
regions	O
.	O
stopping	O
rules	O
based	O
upon	O
cardinalities	O
of	O
regions	O
can	O
effectively	O
be	O
dropped	O
in	O
many	O
cases	O
!	O
20.3	O
balanced	B
search	O
trees	O
balanced	B
multidimensional	O
search	O
trees	O
are	O
computationally	O
attractive	O
.	O
binary	B
trees	O
with	O
n	O
leaves	O
have	O
0	O
(	O
log	O
n	O
)	O
height	O
,	O
for	O
example	O
,	O
when	O
at	O
each	O
node	O
,	O
the	O
size	O
of	O
every	O
subtree	O
is	O
at	O
least	O
ex	O
times	O
the	O
size	O
of	O
the	O
other	O
subtree	O
rooted	O
at	O
the	O
parent	O
,	O
for	O
some	O
constant	O
ex	O
>	O
o.	O
it	O
is	O
thus	O
important	O
to	O
verify	O
the	O
consistency	B
of	O
balanced	B
search	O
trees	O
used	O
in	O
classification	O
.	O
we	O
again	O
consider	O
binary	B
classification	O
trees	O
with	O
the	O
x	O
-property	O
and	O
majority	O
votes	O
over	O
the	O
leaf	O
regions	O
.	O
take	O
for	O
example	O
a	O
tree	O
in	O
which	O
we	O
split	O
every	O
node	O
perfectly	O
,	O
that	O
is	O
,	O
if	O
there	O
are	O
n	O
points	O
,	O
we	O
find	O
the	O
median	B
according	O
to	O
one	O
coordinate	O
,	O
and	O
create	O
two	O
subtrees	O
of	O
sizes	O
l	O
(	O
n	O
-	O
1	O
)	O
/2j	O
and	O
r	O
(	O
n	O
-1	O
)	O
/2l	O
.	O
the	O
median	B
itself	O
stays	O
behind	O
and	O
is	O
not	O
sent	O
down	O
to	O
the	O
subtrees	O
.	O
repeat	O
this	O
for	O
k	O
levels	O
of	O
nodes	O
,	O
at	O
each	O
level	O
cutting	O
along	O
the	O
next	O
coordinate	O
axe	O
in	O
a	O
rotational	O
manner	O
.	O
this	O
leads	O
to	O
2k	O
leaf	O
regions	O
,	O
each	O
having	O
at	O
least	O
n	O
12k	O
-	O
k	O
points	O
and	O
at	O
most	O
nl2k	O
points	O
.	O
such	O
a	O
tree	O
will	O
be	O
called	O
a	O
median	O
tree	B
.	O
figure	O
20.10.	O
median	B
tree	I
withfour	O
leaf	O
regions	O
in	O
1	O
?	O
}	O
.	O
25	O
%	O
25	O
%	O
25	O
%	O
25	O
%	O
setting	O
up	O
such	O
a	O
tree	O
is	O
very	O
easy	O
,	O
and	O
hence	O
such	O
trees	O
may	O
appeal	O
to	O
certain	O
pro	O
(	O
cid:173	O
)	O
grammers	O
.	O
in	O
hypothesis	O
testing	O
,	O
median	B
trees	O
were	O
studied	O
by	O
anderson	O
(	O
1966	O
)	O
.	O
theorem	B
20.2.	O
natural	O
classifiers	O
based	O
upon	O
median	B
trees	O
with	O
k	O
levels	O
(	O
2k	O
leaf	O
regions	O
)	O
are	O
consistent	O
(	O
e	O
{	O
ln	O
}	O
-+	O
l	O
*	O
)	O
whenever	O
x	O
has	O
a	O
density	O
,	O
if	O
20.3	O
balanced	B
search	O
trees	O
323	O
n	O
-k	O
-+	O
00	O
and	O
k-+oo	O
.	O
k2	O
...	O
.	O
.	O
(	O
note	O
:	O
the	O
conditions	O
of	O
k	O
are	O
fulfilled	O
if	O
k	O
:	O
:	O
:	O
s	O
log2	O
n	O
-	O
2log2log2	O
n	O
,	O
k	O
-+	O
00	O
.	O
)	O
we	O
may	O
prove	O
the	O
theorem	B
by	O
checking	O
the	O
conditions	O
of	O
theorem	O
6.1.	O
condition	O
(	O
2	O
)	O
follows	O
trivially	O
by	O
the	O
fact	O
that	O
each	O
leaf	O
region	O
contains	O
at	O
least	O
n	O
12k	O
-	O
k	O
points	O
and	O
the	O
condition	O
nl	O
(	O
k2k	O
)	O
-+	O
00.	O
thus	O
,	O
we	O
need	O
only	O
verify	O
the	O
first	O
condition	O
of	O
theorem	O
6.1.	O
to	O
make	O
the	O
proof	O
more	O
transparent	O
,	O
we	O
first	O
analyze	O
a	O
closely	O
related	O
hypothetical	O
tree	B
,	O
the	O
theoretical	B
median	O
tree	B
.	O
also	O
,	O
we	O
restrict	O
the	O
analysis	O
to	O
d	O
==	O
2.	O
the	O
multidimensional	O
extension	O
is	O
straightforward	O
.	O
the	O
theoretical	B
median	O
tree	B
rotates	O
through	O
the	O
coordinates	O
and	O
cuts	O
each	O
hyperrectangle	O
precisely	O
so	O
that	O
the	O
two	O
new	O
hyperrectangles	O
have	O
equal	O
jl-measure	O
.	O
figure	O
20.11.	O
theoretical	B
median	O
tree	B
with	O
three	O
levels	O
of	O
cuts	O
.	O
118	O
1/8	O
118	O
!	O
-	O
-	O
-	O
118	O
~	O
118	O
118	O
118	O
i	O
observe	O
that	O
the	O
rule	B
is	O
invariant	O
under	O
monotone	O
transformations	O
of	O
the	O
coordinate	O
axes	O
.	O
recall	O
that	O
in	O
such	O
cases	O
there	O
is	O
no	O
harm	O
in	O
assuming	O
that	O
the	O
marginal	O
distributions	O
are	O
all	O
uniform	B
on	O
[	O
0	O
,	O
1	O
]	O
.	O
we	O
let	O
{	O
hi	O
,	O
vd	O
denote	O
the	O
horizontal	O
and	O
vertical	O
sizes	O
of	O
the	O
rectangles	O
after	O
k	O
levels	O
of	O
cuts	O
.	O
of	O
course	O
,	O
we	O
begin	O
with	O
hi	O
=	O
vi	O
=	O
1	O
when	O
k	O
=	O
o.	O
we	O
now	O
show	O
that	O
,	O
for	O
the	O
theoretical	B
median	O
tree	B
,	O
diam	O
(	O
a	O
(	O
x	O
)	O
)	O
-+	O
0	O
in	O
probability	O
,	O
as	O
k	O
-+	O
00.	O
note	O
that	O
diam	O
(	O
a	O
(	O
x	O
)	O
)	O
:	O
:	O
:	O
s	O
h	O
(	O
x	O
)	O
+	O
vex	O
)	O
,	O
where	O
h	O
(	O
x	O
)	O
and	O
vex	O
)	O
are	O
the	O
horizontal	O
and	O
vertical	O
sizes	O
of	O
the	O
rectangle	O
a	O
(	O
x	O
)	O
.	O
we	O
show	O
that	O
if	O
k	O
is	O
even	O
,	O
e	O
{	O
h	O
(	O
x	O
)	O
+	O
vex	O
)	O
}	O
=	O
2k	O
/	O
2	O
'	O
2	O
from	O
which	O
the	O
claim	O
follows	O
.	O
after	O
the	O
k-th	O
round	O
of	O
splits	O
,	O
since	O
a1l2k	O
rectangles	O
have	O
equal	O
probability	O
measure	B
,	O
we	O
have	O
apply	O
another	O
round	O
of	O
splits	O
,	O
all	O
vertical	O
.	O
then	O
each	O
term	O
-1f	O
(	O
hi	O
+	O
vj	O
spawns	O
,	O
so	O
to	O
speak	O
,	O
two	O
new	O
rectangles	O
with	O
horizontal	O
and	O
vertical	O
sizes	O
(	O
h	O
(	O
,	O
vi	O
)	O
and	O
324	O
20.	O
tree	B
classifiers	O
(	O
h	O
(	O
``	O
,	O
vi	O
)	O
with	O
hi	O
=	O
h	O
(	O
+	O
h	O
(	O
``	O
that	O
contribute	O
the	O
next	O
round	O
yields	O
horizontal	O
splits	O
,	O
with	O
total	O
contribution	O
now	O
(	O
see	O
figure	O
20.12	O
)	O
_1_	O
(	O
h	O
:	O
+	O
v.	O
'	O
+	O
h	O
:	O
+	O
v.	O
''	O
+	O
h	O
'	O
''	O
+	O
v.	O
'	O
''	O
+	O
h	O
:	O
''	O
+	O
v.	O
''	O
''	O
)	O
2k+2	O
i	O
i	O
i	O
i	O
i	O
i	O
i	O
i	O
1	O
2k+2	O
(	O
2hi	O
+	O
2	O
vi	O
)	O
1	O
2k+1	O
(	O
hi	O
+	O
~	O
)	O
.	O
thus	O
,	O
over	O
two	O
iterations	O
of	O
splits	O
,	O
we	O
see	O
that	O
e	O
{	O
h	O
(	O
x	O
)	O
+	O
v	O
(	O
x	O
)	O
}	O
is	O
halved	O
,	O
and	O
the	O
claim	O
follows	O
by	O
simple	O
induction	O
.	O
we	O
show	O
now	O
what	O
happens	O
in	O
the	O
real	O
median	B
tree	I
when	O
cuts	O
are	O
based	O
upon	O
a	O
random	O
sample	O
.	O
we	O
deviate	O
of	O
course	O
from	O
the	O
theoretical	B
median	O
tree	B
,	O
but	O
consistency	B
is	O
preserved	O
.	O
the	O
reason	O
,	O
seen	O
intuitively	O
,	O
is	O
that	O
if	O
the	O
number	O
of	O
points	O
in	O
a	O
cell	O
is	O
large	O
,	O
then	O
the	O
sample	O
median	O
will	O
be	O
close	O
to	O
the	O
theoretical	B
median	O
,	O
so	O
that	O
the	O
shrinking-diameter	O
property	O
is	O
preserved	O
.	O
the	O
methodology	O
followed	O
here	O
shows	O
how	O
one	O
may	O
approach	O
the	O
analysis	O
in	O
general	O
by	O
separating	O
the	O
theoretical	B
model	O
from	O
the	O
sample-based	O
model	O
.	O
proof	O
of	O
theorem	O
20.2.	O
as	O
we	O
noted	O
before	O
,	O
all	O
we	O
have	O
to	O
show	O
is	O
that	O
diam	O
(	O
a	O
(	O
x	O
»	O
--	O
+	O
0	O
in	O
probability	O
.	O
again	O
,	O
we	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
the	O
marginals	O
of	O
x	O
are	O
uniform	B
[	O
0	O
,	O
1	O
]	O
,	O
and	O
that	O
d	O
=	O
2.	O
again	O
,	O
we	O
show	O
that	O
e	O
{	O
h	O
(	O
x	O
)	O
+	O
vex	O
)	O
}	O
--	O
+	O
o.	O
figure	O
20.12.	O
a	O
rectangle	O
after	O
two	O
rounds	O
a/splits	O
.	O
h·	O
i	O
p/	O
!	O
/	O
i	O
v/	O
!	O
/	O
i	O
h	O
!	O
''	O
i	O
p	O
''	O
''	O
i	O
hi	O
'	O
''	O
tvt/	O
,	O
v·	O
i	O
i	O
p	O
:	O
/	O
tvy	O
i	O
h	O
(	O
'	O
+	O
p~	O
v'·	O
i	O
i	O
h	O
(	O
i	O
if	O
a	O
rectangle	O
of	O
probability	O
mass	O
pi	O
and	O
sizes	O
hi	O
,	O
vi	O
is	O
split	O
into	O
four	O
rectangles	O
as	O
in	O
figure	O
20.12	O
,	O
with	O
probability	O
masses	O
p	O
;	O
,	O
p	O
;	O
'	O
,	O
p	O
;	O
''	O
,	O
pt	O
'	O
,	O
then	O
the	O
contribution	O
pichi	O
+	O
vi	O
)	O
to	O
e	O
{	O
h	O
(	O
x	O
)	O
+	O
vex	O
)	O
}	O
becomes	O
after	O
two	O
levels	O
of	O
cuts	O
.	O
this	O
does	O
not	O
exceed	O
if	O
and	O
20.3	O
balanced	B
search	O
trees	O
325	O
1	O
;	O
-	O
:	O
;	O
-	O
:	O
-1	O
:	O
:	O
:	O
:	O
''	O
2	O
v	O
1	O
+	O
e	O
pi	O
,	O
(	O
i	O
max	O
pi	O
+	O
pi	O
,	O
pi	O
+	O
pi	O
ii	O
iii	O
1111	O
)	O
(	O
i	O
max	O
pi	O
'	O
pi	O
/1	O
)	O
<	O
~	O
;	O
-	O
:	O
;	O
-	O
:	O
-1	O
(	O
i	O
-	O
2	O
v	O
1	O
+	O
e	O
pi	O
+	O
pi	O
''	O
)	O
'	O
max	O
(	O
p	O
:	O
''	O
p	O
:	O
lli	O
)	O
<	O
_~	O
(	O
:	O
''	O
+	O
:111	O
)	O
'	O
z	O
'	O
z	O
pz	O
pz	O
1	O
-	O
2	O
that	O
is	O
,	O
when	O
all	O
three	O
cuts	O
are	O
within	O
(	O
lj2	O
)	O
~	O
of	O
the	O
true	O
median	B
.	O
we	O
call	O
such	O
''	O
(	O
lj2	O
)	O
~	O
''	O
cuts	O
good	O
.	O
if	O
all	O
cuts	O
are	O
good	O
,	O
we	O
thus	O
note	O
that	O
in	O
two	O
levels	O
of	O
cuts	O
,	O
e	O
{	O
h	O
(	O
x	O
)	O
+	O
vex	O
)	O
}	O
is	O
reduced	O
by	O
(	O
1	O
+e	O
)	O
j2	O
.	O
also	O
,	O
all	O
pi	O
's	O
decrease	O
at	O
a	O
controlled	O
rate	O
.	O
let	O
g	O
be	O
the	O
event	O
that	O
all	O
1	O
+	O
2	O
+	O
...	O
+	O
2k-	O
1	O
cuts	O
in	O
a	O
median	B
tree	I
with	O
k	O
levels	O
are	O
good	O
.	O
then	O
,	O
at	O
level	O
k	O
,	O
all	O
p/s	O
are	O
at	O
most	O
(	O
~j2	O
)	O
k.	O
thus	O
,	O
2k	O
lpi	O
(	O
hi	O
+	O
vi	O
)	O
<	O
i=l	O
since	O
l~~l	O
(	O
hi	O
+	O
vj	O
:	O
:	O
:	O
:	O
4	O
+	O
2	O
+	O
4	O
+	O
8	O
+	O
...	O
+	O
2k/2	O
:	O
:	O
:	O
:	O
2k/2+1	O
+	O
3	O
if	O
k	O
is	O
even	O
.	O
hence	O
,	O
after	O
k	O
levels	O
of	O
cuts	O
,	O
the	O
last	O
term	O
tends	O
to	O
zero	O
if	O
e	O
is	O
small	O
enough	O
.	O
we	O
bound	O
p	O
{	O
g	O
e	O
}	O
by	O
2k	O
times	O
the	O
probability	O
that	O
one	O
cut	O
is	O
bad	O
.	O
let	O
us	O
cut	O
a	O
cell	O
with	O
n	O
points	O
and	O
probability	O
content	O
p	O
in	O
a	O
given	O
direction	O
.	O
a	O
quick	O
check	O
of	O
the	O
median	B
tree	I
shows	O
that	O
given	O
the	O
position	O
and	O
size	O
of	O
the	O
cell	O
,	O
the	O
n	O
points	O
inside	O
the	O
cell	O
are	O
distributed	O
in	O
an	O
i.i.d	O
.	O
manner	O
according	O
to	O
the	O
restriction	O
of	O
fl	O
to	O
the	O
cell	O
.	O
after	O
the	O
cut	O
,	O
we	O
have	O
l	O
)	O
j2j	O
and	O
r	O
(	O
n	O
-	O
1	O
)	O
j2l	O
points	O
in	O
the	O
new	O
cells	O
,	O
and	O
probability	O
contents	O
l	O
(	O
n	O
-	O
pi	O
and	O
p	O
''	O
.	O
it	O
is	O
clear	O
that	O
we	O
may	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
p	O
=	O
1.	O
thus	O
,	O
if	O
all	O
points	O
are	O
projected	O
down	O
in	O
the	O
direction	O
of	O
the	O
cut	O
,	O
and	O
f	O
and	O
f	O
n	O
denote	O
the	O
distribution	B
function	I
and	O
empirical	B
distribution	O
function	O
of	O
the	O
obtained	O
one-dimensional	O
data	O
,	O
then	O
p	O
{	O
cut	O
is	O
not	O
goodl	O
n	O
}	O
<	O
p	O
p	O
>	O
--	O
2-orp	O
>	O
--	O
2-	O
n	O
i	O
~	O
ii	O
~i	O
}	O
{	O
326	O
20.	O
tree	B
classifiers	O
<	O
php	O
(	O
f	O
(	O
x	O
)	O
-fn	O
(	O
x	O
)	O
)	O
>	O
~	O
(	O
~-l	O
)	O
in	O
}	O
<	O
2exp	O
(	O
-~n	O
(	O
~-ln	O
(	O
by	O
theorem	B
12.9	O
)	O
<	O
2exp	O
(	O
-	O
(	O
2	O
:	O
+1	O
-d	O
(	O
~-ln	O
(	O
as	O
n	O
:	O
:	O
:	O
:	O
nj	O
(	O
2k	O
)	O
-	O
k	O
)	O
.	O
hence	O
,	O
for	O
n	O
large	O
enough	O
,	O
20.4	O
binary	B
search	O
trees	O
the	O
simplest	O
trees	O
to	O
analyze	O
are	O
those	O
whose	O
structure	O
depends	O
in	O
a	O
straightforward	O
way	O
on	O
the	O
data	O
.	O
to	O
make	O
this	O
point	O
,	O
we	O
begin	O
with	O
the	O
binary	B
search	O
tree	B
and	O
its	O
multivariate	O
extension	O
,	O
the	O
k-d	B
tree	I
(	O
see	O
cormen	O
,	O
leiserson	O
,	O
and	O
rivest	O
(	O
1990	O
)	O
for	O
the	O
binary	B
search	O
tree	B
;	O
for	O
multivariate	O
binary	O
trees	O
,	O
we	O
refer	O
to	O
samet	O
(	O
1990b	O
)	O
)	O
.	O
a	O
full-fledged	O
k-d	B
tree	I
is	O
defined	O
as	O
follows	O
:	O
we	O
promote	O
the	O
first	O
data	O
point	O
xl	O
to	O
the	O
root	O
and	O
partition	B
{	O
x2	O
,	O
...	O
,	O
xn	O
}	O
into	O
two	O
sets	O
:	O
those	O
whose	O
first	O
coordinate	O
exceeds	O
that	O
of	O
xl	O
,	O
and	O
the	O
remaining	O
points	O
.	O
within	O
each	O
set	O
,	O
points	O
are	O
ordered	B
by	O
original	O
index	O
.	O
the	O
former	O
set	O
is	O
used	O
to	O
build	O
the	O
right	O
subtree	O
of	O
xl	O
and	O
the	O
latter	O
to	O
construct	O
the	O
left	O
subtree	O
of	O
xl	O
'	O
for	O
each	O
subtree	O
,	O
the	O
same	O
construction	O
is	O
applied	O
recursively	O
with	O
only	O
one	O
variant	O
:	O
at	O
depth	O
l	O
in	O
the	O
tree	B
,	O
the	O
(	O
l	O
mod	O
d	O
+	O
l	O
)	O
-st	O
coordinate	O
is	O
used	O
to	O
split	O
the	O
data	O
.	O
in	O
this	O
manner	O
,	O
we	O
rotate	O
through	O
the	O
coordinate	O
axes	O
periodically	O
.	O
attach	O
to	O
each	O
leaf	O
two	O
new	O
nodes	O
,	O
and	O
to	O
each	O
node	O
with	O
one	O
child	O
a	O
sec	O
(	O
cid:173	O
)	O
ond	O
child	O
.	O
call	O
these	O
new	O
nodes	O
external	O
nodes	O
.	O
each	O
of	O
the	O
n	O
+	O
1	O
external	O
nodes	O
correspond	O
to	O
a	O
region	O
of	O
r	O
d	O
,	O
and	O
collectively	O
the	O
external	O
nodes	O
define	O
a	O
parti	O
(	O
cid:173	O
)	O
tion	O
ofrd	O
(	O
if	O
we	O
define	O
exactly	O
what	O
happens	O
on	O
the	O
boundaries	O
between	O
regions	O
)	O
.	O
20.4	O
binary	B
search	O
trees	O
327	O
figure	O
20.13.	O
a	O
k-d	O
tree	B
oi15	O
random	O
points	O
on	O
the	O
plane	O
and	O
the	O
induced	O
partition	B
.	O
put	O
differently	O
,	O
we	O
may	O
look	O
at	O
the	O
external	O
nodes	O
as	O
the	O
leaves	O
of	O
a	O
new	O
tree	B
with	O
2n	O
+	O
1	O
nodes	O
and	O
declare	O
this	O
new	O
tree	B
to	O
be	O
our	O
new	O
binary	B
classification	O
tree	B
.	O
as	O
there	O
are	O
n	O
+	O
1	O
leaf	O
regions	O
and	O
n	O
data	O
points	O
,	O
the	O
natural	O
binary	O
tree	B
classifier	O
it	O
induces	O
is	O
degenerate-indeed	O
,	O
all	O
external	O
regions	O
contain	O
very	O
few	O
points	O
.	O
clearly	O
,	O
we	O
must	O
have	O
a	O
mechanism	O
for	O
trimming	O
the	O
tree	B
to	O
insure	O
better	O
populated	O
leaves	O
.	O
let	O
us	O
look	O
at	O
just	O
three	O
naive	B
strategies	O
.	O
for	O
convenience	O
,	O
we	O
assume	O
that	O
the	O
data	O
points	O
determining	O
the	O
tree	B
are	O
not	O
counted	O
when	O
taking	O
a	O
majority	O
vote	O
over	O
the	O
cells	O
.	O
as	O
the	O
number	O
of	O
these	O
points	O
is	O
typically	O
much	O
smaller	O
than	O
n	O
,	O
this	O
restriction	O
does	O
not	O
make	O
a	O
significant	O
difference	O
.	O
(	O
1	O
)	O
fix	O
k	O
<	O
n	O
,	O
and	O
construct	O
a	O
k	O
-d	O
tree	B
with	O
k	O
internal	O
nodes	O
and	O
k	O
+	O
1	O
external	O
nodes	O
,	O
based	O
on	O
the	O
first	O
k	O
data	O
points	O
xl	O
,	O
...	O
,	O
xk	O
.	O
classify	O
by	O
majority	B
vote	I
over	O
all	O
k	O
+	O
1	O
regions	O
as	O
in	O
natural	O
classification	O
tees	O
(	O
taking	O
the	O
data	O
pairs	O
(	O
xk+l	O
,	O
yk+d	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
into	O
account	O
)	O
.	O
call	O
this	O
the	O
chronological	B
k-d	O
tree	B
.	O
(	O
2	O
)	O
fix	O
k	O
and	O
truncate	O
the	O
k-d	B
tree	I
to	O
k	O
levels	O
.	O
all	O
nodes	O
at	O
level	O
k	O
are	O
declared	O
leaves	O
and	O
classification	O
is	O
again	O
by	O
majority	B
vote	I
over	O
the	O
leaf	O
regions	O
.	O
call	O
this	O
the	O
deep	B
k-d	O
tree	B
.	O
(	O
3	O
)	O
fix	O
k	O
and	O
trim	O
the	O
tree	B
so	O
that	O
each	O
node	O
represents	O
at	O
least	O
k	O
points	O
in	O
the	O
original	O
construction	O
.	O
consider	O
the	O
maximal	O
such	O
tree	B
.	O
the	O
number	O
of	O
regions	O
here	O
is	O
random	O
,	O
with	O
between	O
1	O
and	O
nj	O
k	O
regions	O
.	O
call	O
this	O
the	O
well	O
(	O
cid:173	O
)	O
populated	O
k-d	B
tree	I
.	O
328	O
20.	O
tree	B
classifiers	O
let	O
the	O
leaf	O
regions	O
be	O
{	O
ai	O
,	O
...	O
,	O
an	O
}	O
(	O
with	O
n	O
possibly	O
random	O
)	O
,	O
and	O
denote	O
the	O
leaf	O
nodes	O
by	O
ui	O
,	O
...	O
,	O
un	O
.	O
the	O
strict	O
descendants	O
of	O
ui	O
in	O
the	O
full	O
k-d	B
tree	I
have	O
indices	O
that	O
we	O
will	O
collect	O
in	O
an	O
index	O
set	O
ii	O
.	O
define	O
i	O
ii	O
i	O
=	O
ni	O
.	O
as	O
the	O
leaf	O
regions	O
form	O
a	O
partition	O
,	O
we	O
have	O
n	O
lni=n-n	O
,	O
i=l	O
because	O
the	O
leaf	O
nodes	O
themselves	O
are	O
not	O
counted	O
in	O
ii	O
.	O
voting	O
is	O
by	O
majority	B
vote	I
,	O
so	O
the	O
rule	B
is	O
20.5	O
the	O
chronological	B
k-d	O
tree	B
here	O
we	O
have	O
n	O
=	O
k	O
+	O
1.	O
also	O
,	O
(	O
/l	O
(	O
a	O
l	O
)	O
,	O
...	O
,	O
/l	O
(	O
ak+i	O
)	O
)	O
are	O
distributed	O
as	O
uniform	B
spacings	O
.	O
that	O
is	O
,	O
if	O
u	O
i	O
,	O
...	O
,	O
uk	O
are	O
i.i.d	O
.	O
uniform	B
[	O
0	O
,	O
1	O
]	O
random	O
variables	O
defin	O
(	O
cid:173	O
)	O
ing	O
k	O
+	O
1	O
spacings	O
u	O
(	O
l	O
)	O
,	O
u	O
(	O
2	O
)	O
-	O
u	O
(	O
l	O
)	O
,	O
...	O
,	O
u	O
(	O
k	O
)	O
-	O
u	O
(	O
k-i	O
)	O
,	O
1	O
-	O
u	O
(	O
k	O
)	O
by	O
their	O
order	B
statistics	I
u	O
(	O
l	O
)	O
.	O
:	O
:	O
:	O
u	O
(	O
2	O
)	O
.	O
:	O
:	O
:	O
...	O
.	O
:	O
:	O
:	O
u	O
(	O
k	O
)	O
,	O
then	O
these	O
spacings	O
are	O
jointly	O
distributed	O
as	O
(	O
{	O
l	O
(	O
ad	O
,	O
...	O
,	O
/l	O
(	O
ak+d	O
)	O
.	O
this	O
can	O
be	O
shown	O
by	O
induction	O
.	O
when	O
uk+1	O
is	O
added	O
,	O
uk+1	O
first	O
picks	O
a	O
spacing	O
with	O
probability	O
equal	O
to	O
the	O
size	O
of	O
the	O
spacing	B
.	O
then	O
it	O
cuts	O
that	O
spacing	B
in	O
a	O
uniform	O
manner	O
.	O
as	O
the	O
same	O
is	O
true	O
when	O
the	O
chronological	B
k-d	O
tree	B
grows	O
by	O
one	O
leaf	O
,	O
the	O
property	O
follows	O
by	O
induction	O
on	O
k.	O
theorem	B
20.3.	O
we	O
have	O
e	O
{	O
ln	O
}	O
--	O
-+	O
l	O
*	O
for	O
all	O
distributions	O
of	O
x	O
with	O
a	O
density	O
for	O
the	O
chronological	B
k-d	O
tree	B
classifier	O
whenever	O
k	O
--	O
-+	O
00	O
,	O
and	O
kin	O
--	O
-+	O
o.	O
proof	O
.	O
we	O
verify	O
the	O
conditions	O
of	O
theorem	O
6.1.	O
as	O
the	O
number	O
of	O
regions	O
is	O
k+	O
1	O
,	O
and	O
the	O
partition	B
is	O
determined	O
by	O
the	O
first	O
k	O
data	O
points	O
,	O
condition	O
(	O
2	O
)	O
immediately	O
follows	O
from	O
lemma	O
20.1	O
and	O
the	O
remark	O
following	O
it	O
.	O
condition	O
(	O
1	O
)	O
of	O
theorem	O
6.1	O
requires	O
significantly	O
more	O
work	O
.	O
we	O
verify	O
con	O
(	O
cid:173	O
)	O
dition	O
(	O
1	O
)	O
for	O
d	O
=	O
2	O
,	O
leaving	O
the	O
straightforward	O
extension	O
to	O
r	O
d	O
,	O
d	O
>	O
2	O
,	O
to	O
the	O
reader	O
.	O
throughout	O
,	O
we	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
the	O
marginal	O
distri	O
(	O
cid:173	O
)	O
butions	O
are	O
uniform	B
[	O
0	O
,	O
1	O
]	O
.	O
we	O
may	O
do	O
so	O
by	O
the	O
invariance	B
properties	O
discussed	O
earlier	O
.	O
fix	O
a	O
point	O
x	O
e	O
rd	O
.	O
we	O
insert	O
points	O
xl	O
,	O
...	O
,	O
xk	O
into	O
an	O
initially	O
empty	O
k	O
-d	O
tree	B
and	O
let	O
r	O
1	O
,	O
...	O
,	O
rk	O
be	O
the	O
rectangles	O
containing	O
x	O
just	O
after	O
these	O
points	O
were	O
inserted	O
.	O
note	O
that	O
rl	O
;	O
2	O
r2	O
;	O
2	O
...	O
;	O
2	O
rk	O
.	O
assume	O
for	O
simplicity	O
that	O
the	O
integer	O
k	O
is	O
a	O
perfect	O
cube	O
and	O
set	O
l	O
=	O
k	O
1	O
/	O
3	O
.	O
define	O
the	O
distances	O
from	O
x	O
to	O
the	O
sides	O
of	O
ri	O
by	O
hi	O
,	O
h	O
:	O
,	O
vi	O
,	O
v/	O
(	O
see	O
figure	O
20.14	O
)	O
.	O
,	O
m	O
=	O
k	O
2	O
3	O
/	O
20.5	O
the	O
chronological	B
k-d	O
tree	B
329	O
figure	O
20.14.	O
a	O
rectangle	O
ri	O
containing	O
x	O
with	O
its	O
distances	O
to	O
the	O
sides	O
.	O
hi	O
v.	O
'	O
i	O
x	O
h	O
;	O
vi	O
we	O
construct	O
a	O
sequence	O
of	O
events	O
that	O
forces	O
the	O
diameter	O
of	O
rk	O
to	O
be	O
small	O
with	O
high	O
probability	O
.	O
let	O
e	O
be	O
a	O
small	O
positive	O
number	O
to	O
be	O
specified	O
later	O
.	O
denote	O
the	O
four	O
squares	O
with	O
opposite	O
vertices	O
x	O
,	O
x	O
+	O
(	O
±e	O
,	O
±e	O
)	O
by	O
c1	O
,	O
c2	O
,	O
c3	O
,	O
c4	O
'	O
then	O
define	O
the	O
following	O
events	O
:	O
el	O
n	O
{	O
ci	O
n	O
{	O
xl	O
,	O
...	O
,	O
xd	O
7'0	O
}	O
,	O
4	O
i=l	O
e2	O
e3	O
e4	O
e5	O
e6	O
{	O
max	O
(	O
~	O
,	O
~	O
'	O
,	O
h/	O
,	O
h	O
(	O
)	O
:	O
s	O
e	O
}	O
,	O
{	O
min	O
(	O
max	O
(	O
v/	O
,	O
v/	O
)	O
,	O
max	O
(	O
h/	O
,	O
h	O
(	O
»	O
)	O
:	O
s	O
e	O
<	O
max	O
(	O
v/	O
,	O
v/	O
'	O
h/	O
,	O
h	O
(	O
)	O
}	O
,	O
{	O
at	O
least	O
three	O
of	O
vm	O
,	O
v~	O
,	O
hm	O
,	O
h~l	O
are	O
:	O
s	O
e	O
}	O
,	O
{	O
max	O
(	O
vm	O
,	O
v~	O
,	O
hm	O
,	O
h~	O
)	O
:	O
s	O
e	O
}	O
,	O
{	O
max	O
(	O
vk	O
,	O
v	O
;	O
,	O
hk	O
,	O
hd	O
~	O
e	O
}	O
.	O
if	O
e2	O
,	O
e5	O
,	O
or	O
e6	O
hold	O
,	O
then	O
diam	O
(	O
r	O
k	O
)	O
:	O
s	O
e.j8	O
.	O
assume	O
that	O
we	O
find	O
a	O
set	O
bend	O
such	O
that	O
p	O
{	O
x	O
e	O
b	O
}	O
=	O
1	O
,	O
and	O
for	O
all	O
x	O
e	O
b	O
,	O
and	O
sufficiently	O
small	O
e	O
>	O
0	O
,	O
p	O
{	O
e2	O
u	O
e5	O
u	O
e6	O
}	O
-+	O
1	O
as	O
k	O
-+	O
00.	O
then	O
,	O
by	O
the	O
lebesgue	O
dominated	B
convergence	I
theorem	I
,	O
diam	O
(	O
a	O
(	O
x	O
»	O
-+	O
0	O
in	O
probability	O
,	O
and	O
condition	O
(	O
1	O
)	O
of	O
theorem	O
6.1	O
would	O
follow	O
.	O
in	O
the	O
remaining	O
part	O
of	O
the	O
proof	O
we	O
define	O
such	O
a	O
set	O
b	O
,	O
and	O
show	O
that	O
p	O
{	O
e2	O
u	O
e5	O
u	O
e6	O
}	O
-+	O
1.	O
this	O
will	O
require	O
some	O
work	O
.	O
we	O
define	O
the	O
set	O
b	O
in	O
terms	O
of	O
the	O
density	O
f	O
of	O
x.	O
x	O
e	O
b	O
if	O
and	O
only	O
if	O
(	O
1	O
)	O
minl~i	O
:	O
~4	O
fe	O
i	O
f	O
(	O
z	O
)	O
dz	O
>	O
0	O
for	O
all	O
e	O
>	O
0	O
small	O
enough	O
;	O
f	O
f	O
(	O
z	O
)	O
dz	O
r	O
a	O
(	O
r	O
)	O
f	O
(	O
x	O
)	O
>	O
__	O
for	O
all	O
e	O
>	O
0	O
small	O
enough	O
;	O
-	O
2	O
(	O
2	O
)	O
inf	O
rectangles	O
r	O
containing	O
x	O
,	O
of	O
diameter	O
~	O
e.j8	O
(	O
3	O
)	O
f	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
o.	O
that	O
p	O
{	O
x	O
e	O
b	O
}	O
=	O
1	O
follows	O
from	O
a	O
property	O
of	O
the	O
support	B
(	O
problem	O
20.6	O
)	O
,	O
a	O
corollary	O
of	O
the	O
lessen-marcinkiewicz-zygmund	O
theorem	B
(	O
corollary	O
a.2	O
;	O
this	O
implies	O
(	O
2	O
)	O
for	O
almost	O
all	O
x	O
)	O
,	O
and	O
the	O
fact	O
that	O
for	O
j.i-almost	O
all	O
x	O
,	O
f	O
(	O
x	O
)	O
>	O
o.	O
it	O
is	O
easy	O
to	O
verify	O
the	O
following	O
:	O
p	O
{	O
e~	O
}	O
=	O
p	O
{	O
ed	O
+p	O
{	O
e1	O
n	O
e~	O
n	O
e~	O
}	O
330	O
20.	O
tree	B
classifiers	O
+p	O
{	O
e1	O
n	O
e~	O
n	O
e3	O
n	O
e~	O
}	O
+p	O
{	O
e1	O
n	O
e~	O
n	O
e3	O
n	O
e4	O
n	O
e~	O
n	O
e6	O
}	O
.	O
we	O
show	O
that	O
each	O
term	O
tends	O
to	O
0	O
at	O
x	O
e	O
b.	O
term	O
1.	O
by	O
the	O
union	O
bound	O
,	O
4	O
p	O
{	O
ef	O
}	O
<	O
~p	O
{	O
xi	O
ti	O
:	O
ci	O
,	O
...	O
,	O
xi	O
ti	O
:	O
cd	O
i=l	O
4	O
<	O
~	O
(	O
1	O
-	O
i=l	O
/l	O
(	O
ci	O
)	O
i	O
<	O
exp	O
(	O
-l	O
min	O
/l	O
(	O
ei	O
»	O
)	O
l	O
:	O
:si	O
:	O
:	O
:	O
a	O
-+	O
0	O
,	O
by	O
part	O
(	O
1	O
)	O
of	O
the	O
definition	B
of	I
b.	O
term	O
2.	O
e1	O
~	O
e3	O
by	O
a	O
simple	O
geometric	B
argument	O
.	O
hence	O
p	O
{	O
ei	O
n	O
en	O
=	O
o.	O
term	O
3.	O
to	O
show	O
that	O
p	O
{	O
e1	O
n	O
e~	O
n	O
e3	O
n	O
e~	O
}	O
-+	O
0	O
,	O
we	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
max	O
(	O
vz	O
,	O
v/	O
)	O
:	O
:	O
:	O
:	O
e	O
while	O
max	O
(	O
hz	O
,	O
hi	O
)	O
=	O
a	O
>	O
e.	O
let	O
{	O
xd	O
be	O
a	O
subset	O
of	O
{	O
xi	O
,	O
i	O
<	O
i	O
:	O
:	O
:	O
:	O
m	O
}	O
consisting	O
of	O
those	O
xi	O
's	O
that	O
fall	O
in	O
re•	O
we	O
introduce	O
three	O
notions	O
in	O
this	O
sequence	O
:	O
first	O
,	O
zi	O
is	O
the	O
absolute	O
value	O
of	O
the	O
difference	O
of	O
the	O
x	O
(	O
2	O
)	O
-coordinates	O
of	O
x	O
and	O
x	O
;	O
.	O
let	O
wi	O
be	O
the	O
absolute	O
value	O
of	O
the	O
difference	O
of	O
the	O
x	O
(	O
l	O
)	O
-coordinates	O
of	O
x	O
and	O
x	O
;	O
.	O
we	O
re-index	O
the	O
sequence	O
x	O
;	O
(	O
and	O
wi	O
and	O
zi	O
)	O
so	O
that	O
i	O
runs	O
from	O
1	O
to	O
n	O
,	O
where	O
n	O
=	O
~	O
i	O
{	O
x	O
;	O
ertl	O
.	O
m	O
i=z+l	O
to	O
avoid	O
trivialities	O
,	O
assume	O
that	O
n	O
~	O
1	O
(	O
this	O
will	O
be	O
shown	O
to	O
happen	O
with	O
prob	O
(	O
cid:173	O
)	O
ability	O
tending	O
to	O
one	O
)	O
.	O
call	O
x	O
;	O
a	O
record	O
if	O
zi	O
=	O
min	O
(	O
zl	O
,	O
...	O
,	O
zi	O
)	O
.	O
call	O
x	O
;	O
a	O
good	O
point	O
if	O
wi	O
:	O
:	O
:	O
:	O
e.	O
an	O
x	O
;	O
causes	O
min	O
(	O
hm	O
,	O
h'~l	O
)	O
:	O
:	O
:	O
:	O
e	O
if	O
that	O
x	O
;	O
is	O
a	O
good	O
point	O
and	O
a	O
record	O
,	O
and	O
if	O
it	O
defines	O
a	O
vertical	O
cut	O
.	O
the	O
alternating	O
nature	O
of	O
the	O
cuts	O
makes	O
our	O
analysis	O
a	O
bit	O
heavier	O
than	O
needed	O
.	O
we	O
show	O
here	O
what	O
happens	O
when	O
all	O
directions	O
are	O
picked	O
independently	O
of	O
each	O
other	O
,	O
leaving	O
the	O
rotating-cuts	O
case	O
to	O
the	O
reader	O
(	O
problem	O
20.8	O
)	O
.	O
thus	O
,	O
if	O
we	O
set	O
si	O
=	O
i	O
{	O
x	O
;	O
is	O
a	O
record	O
,	O
x	O
;	O
defines	O
a	O
vertical	O
cut	O
}	O
,	O
we	O
have	O
,	O
20.5	O
the	O
chronological	B
k-d	O
tree	B
331	O
re-index	O
again	O
and	O
let	O
x~	O
,	O
x~	O
,	O
...	O
all	O
be	O
records	O
.	O
note	O
that	O
given	O
x	O
;	O
,	O
x	O
;	O
+l	O
is	O
distributed	O
according	O
to	O
f	O
restricted	O
to	O
the	O
rectangle	O
r	O
'	O
of	O
height	O
min	O
(	O
v	O
[	O
,	O
zi	O
)	O
above	O
x	O
,	O
height	O
min	O
(	O
``	O
l	O
'	O
,	O
z	O
i	O
)	O
below	O
x	O
,	O
width	O
hz	O
to	O
the	O
left	O
of	O
x	O
,	O
width	O
h	O
!	O
to	O
the	O
right	O
of	O
x.	O
call	O
these	O
four	O
quantities	O
v	O
,	O
v	O
'	O
,	O
h	O
,	O
h	O
'	O
,	O
respectively	O
.	O
then	O
p	O
{	O
w.	O
<	O
eix~	O
}	O
>	O
(	O
v	O
+	O
v	O
'	O
)	O
ef	O
(	O
x	O
)	O
/2	O
=	O
ef	O
(	O
x	O
)	O
1+1	O
-	O
1	O
-	O
v	O
+	O
v	O
'	O
2	O
because	O
the	O
marginal	O
distribution	B
of	O
an	O
independent	O
x	O
1	O
is	O
uniform	B
and	O
thus	O
,	O
p	O
{	O
x	O
1	O
e	O
r'ir	O
'	O
}	O
:	O
:	O
:	O
v	O
+	O
v	O
'	O
,	O
while	O
,	O
by	O
property	O
(	O
2	O
)	O
of	O
b	O
,	O
p	O
{	O
x1	O
e	O
r	O
'	O
,	O
wi	O
:	O
:	O
:	O
eir	O
'	O
}	O
2	O
:	O
:	O
(	O
v	O
+	O
v	O
'	O
)	O
ef	O
(	O
x	O
)	O
/2	O
.	O
recall	O
the	O
re-indexing	O
.	O
let	O
m	O
be	O
the	O
number	O
of	O
records	O
(	O
thus	O
,	O
m	O
is	O
the	O
length	O
of	O
our	O
sequence	O
x	O
)	O
.	O
then	O
e	O
!	O
n	O
e2	O
n	O
e3	O
n	O
e~	O
n	O
(	O
n	O
>	O
oj	O
~	O
{	O
t	O
i	O
{	O
w	O
,	O
s	O
,	O
)	O
i	O
{	O
x	O
;	O
``	O
''	O
erticrumt	O
)	O
=	O
o	O
}	O
.	O
but	O
as	O
cuts	O
have	O
independently	O
picked	O
directions	O
,	O
and	O
since	O
p	O
{	O
wi+1	O
:	O
:	O
:	O
eix	O
;	O
j	O
2	O
:	O
:	O
ef	O
(	O
x	O
)	O
/2	O
,	O
we	O
see	O
that	O
p	O
{	O
e1	O
n	O
e2	O
n	O
e3	O
n	O
e4	O
,	O
n	O
>	O
o	O
}	O
:	O
:	O
:	O
e	O
c	O
c	O
ef	O
(	O
x	O
)	O
1	O
-	O
-4-	O
)	O
{	O
(	O
m	O
f	O
{	O
n	O
>	O
o	O
}	O
,	O
}	O
we	O
rewrite	O
m	O
=	O
l~1	O
fix	O
;	O
is	O
a	O
record	O
}	O
and	O
recall	O
that	O
the	O
indicator	O
variables	O
in	O
this	O
sum	O
are	O
independent	O
and	O
are	O
of	O
mean	O
1	O
i	O
i	O
(	O
problem	O
20.9	O
)	O
.	O
hence	O
,	O
for	O
c	O
>	O
0	O
,	O
<	O
e	O
{	O
e-	O
(	O
l-c	O
)	O
l~l	O
iii	O
}	O
(	O
use	O
1	O
-	O
x	O
:	O
:s	O
e-x	O
)	O
<	O
e	O
{	O
1	O
(	O
n	O
+	O
1	O
)	O
i-c	O
}	O
(	O
use	O
``	O
1:1	O
ii	O
i	O
2	O
:	O
:	O
log	O
(	O
n	O
+	O
1	O
)	O
)	O
.	O
l	O
the	O
latter	O
formula	O
remains	O
valid	O
even	O
if	O
n	O
=	O
o.	O
thus	O
,	O
with	O
c	O
=	O
1	O
-	O
ef	O
(	O
x	O
)	O
/4	O
,	O
pie	O
!	O
n	O
e2	O
n	O
e3	O
n	O
e~j	O
<	O
e	O
{	O
(	O
i	O
-	O
ef~x	O
)	O
r	O
}	O
<	O
e	O
{	O
(	O
n+1l	O
)	O
!	O
_e	O
}	O
.	O
332	O
20.	O
tree	B
classifiers	O
l	O
,	O
fl	O
(	O
rz	O
)	O
)	O
.	O
we	O
know	O
from	O
the	O
introduction	O
n	O
is	O
binomial	B
with	O
parameters	O
(	O
m	O
-	O
of	O
this	O
section	O
that	O
fl	O
(	O
rt	O
)	O
is	O
distributed	O
as	O
the	O
minimum	O
of	O
l	O
i.i.d	O
.	O
uniform	B
[	O
0	O
,	O
1	O
]	O
random	O
variables	O
.	O
thus	O
,	O
for	O
8	O
>	O
0	O
,	O
e	O
{	O
fl	O
(	O
rz	O
)	O
}	O
=	O
1/	O
(	O
l	O
+	O
1	O
)	O
,	O
and	O
p	O
{	O
fl	O
(	O
rz	O
)	O
<	O
81	O
l	O
}	O
:	O
:	O
:	O
:	O
8.	O
hence	O
,	O
e	O
{	O
(	O
n	O
+\	O
)	O
1-	O
'	O
}	O
<	O
p	O
{	O
fl	O
(	O
rz	O
)	O
<	O
81	O
i	O
}	O
+	O
e	O
{	O
}	O
(	O
binomial	B
(	O
m	O
-1,81	O
i	O
)	O
+	O
l	O
)	O
l-c	O
1	O
<	O
8	O
+	O
(	O
2	O
(	O
m	O
-1	O
)	O
8	O
l	O
)	O
l-c	O
{	O
+	O
p	O
binomial	B
(	O
m	O
-	O
l	O
,	O
8	O
i	O
l	O
)	O
<	O
(	O
m	O
-1	O
)	O
8	O
}	O
.	O
2l	O
the	O
first	O
term	O
is	O
small	O
by	O
choice	O
of	O
8.	O
the	O
second	O
one	O
is	O
0	O
(	O
k-	O
(	O
l-c	O
)	O
/3	O
)	O
.	O
the	O
third	O
one	O
is	O
bounded	O
from	O
above	O
,	O
by	O
chebyshev	O
's	O
inequality	B
,	O
by	O
(	O
m	O
-l	O
)	O
(	O
81	O
l	O
)	O
(	O
1	O
-	O
81	O
i	O
)	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
<	O
-	O
(	O
(	O
m	O
-l	O
)	O
812l	O
)	O
2	O
41	O
(	O
m	O
-1	O
)	O
8	O
-'7	O
0.	O
term	O
4.	O
this	O
term	O
is	O
handled	O
exactly	O
as	O
term	O
3.	O
note	O
,	O
however	O
,	O
that	O
i	O
and	O
m	O
now	O
become	O
m	O
and	O
k	O
respectively	O
.	O
the	O
convergence	O
to	O
°	O
requires	O
now	O
m	O
i	O
(	O
k	O
-	O
m	O
)	O
-'7	O
0	O
,	O
which	O
is	O
still	O
the	O
case	O
.	O
this	O
concludes	O
the	O
proof	O
of	O
theorem	O
20.3	O
.	O
0	O
20.6	O
the	O
deep	B
k-d	O
tree	B
theorem	O
20.4.	O
the	O
deep	B
k-d	O
tree	B
classifier	O
is	O
consistent	O
(	O
i.e.	O
,	O
e	O
{	O
l	O
n	O
}	O
-'7	O
l	O
*	O
)	O
for	O
all	O
distributions	O
such	O
that	O
x	O
has	O
a	O
density	O
,	O
whenever	O
lim	O
k	O
=	O
00	O
and	O
n	O
--	O
--	O
+oo	O
k	O
.	O
hmsup	O
--	O
<	O
2.	O
n	O
--	O
--	O
+oo	O
log	O
n	O
proof	O
.	O
in	O
problem	O
20.10	O
,	O
you	O
are	O
asked	O
to	O
show	O
that	O
k	O
-'7	O
00	O
implies	O
diam	O
(	O
a	O
(	O
x	O
)	O
)	O
-'7	O
°	O
in	O
probability	O
.	O
theorem	B
20.3	O
may	O
be	O
invoked	O
here	O
.	O
we	O
now	O
show	O
that	O
the	O
assumption	O
lim	O
supn	O
--	O
--	O
+oo	O
kl	O
log	O
n	O
<	O
2	O
implies	O
that	O
n	O
(	O
x	O
)	O
-'7	O
00	O
in	O
probability	O
.	O
let	O
d	O
be	O
the	O
depth	O
(	O
distance	B
from	O
the	O
root	O
)	O
of	O
x	O
when	O
x	O
is	O
inserted	O
into	O
a	O
k-d	O
tree	B
having	O
n	O
elements	O
.	O
clearly	O
,	O
n	O
(	O
x	O
)	O
~	O
d	O
-	O
k	O
,	O
so	O
it	O
suffices	O
to	O
show	O
that	O
d	O
-	O
k	O
-'7	O
00	O
in	O
probability	O
.	O
we	O
know	O
that	O
d	O
1	O
(	O
2	O
log	O
n	O
)	O
-'7	O
1	O
in	O
probability	O
(	O
see	O
,	O
e.g.	O
,	O
devroye	O
(	O
1988a	O
)	O
and	O
problem	O
20.10	O
)	O
.	O
this	O
concludes	O
the	O
proof	O
of	O
the	O
theorem	B
.	O
0	O
20.7	O
quadtrees	O
333	O
20.7	O
quadtrees	O
quadtrees	O
or	O
hyperquadtrees	O
are	O
unquestionably	O
the	O
most	O
prominent	O
trees	O
in	O
com	O
(	O
cid:173	O
)	O
puter	O
graphics	O
.	O
easy	O
to	O
manipulate	O
and	O
compact	O
to	O
store	O
,	O
they	O
have	O
found	O
their	O
way	O
into	O
mainstream	O
computer	O
science	O
.	O
discovered	O
by	O
finkel	O
and	O
bentley	O
(	O
1974	O
)	O
and	O
surveyed	O
by	O
samet	O
(	O
1984	O
)	O
,	O
(	O
1990a	O
)	O
,	O
they	O
take	O
several	O
forms	O
.	O
we	O
are	O
given	O
d	O
(	O
cid:173	O
)	O
dimensional	O
data	O
x	O
i	O
,	O
...	O
,	O
x	O
n	O
.	O
the	O
tree	B
is	O
constructed	O
as	O
the	O
k	O
-d	O
tree	B
.	O
in	O
particular	O
,	O
xl	O
becomes	O
the	O
root	O
of	O
the	O
tree	B
.	O
it	O
partitions	O
x	O
2	O
,	O
..•	O
,	O
xn	O
into	O
2d	O
(	O
possibly	O
empty	O
)	O
sets	O
according	O
to	O
membership	O
in	O
one	O
of	O
the	O
2d	O
(	O
hyper-	O
)	O
quadrants	O
centered	O
at	O
xl	O
(	O
see	O
figure	O
20.15	O
)	O
.	O
r	O
--	O
-	O
1	O
.	O
--	O
-	O
1	O
i	O
:	O
--	O
-11	O
--	O
-	O
~	O
~	O
f	O
--	O
--	O
--	O
-1	O
[	O
1	O
t	O
1	O
figure	O
20.15.	O
quadtree	B
and	O
the	O
induced	O
partition	B
of	O
r2	O
,	O
the	O
points	O
on	O
the	O
right	O
are	O
shown	O
in	O
the	O
position	O
in	O
space	O
.	O
the	O
root	O
is	O
specially	O
marked	O
.	O
the	O
partitioning	O
process	O
is	O
repeated	O
at	O
the	O
2d	O
child	O
nodes	O
until	O
a	O
certain	O
stopping	O
rule	B
is	O
satisfied	O
.	O
in	O
analogy	O
with	O
the	O
k-d	B
tree	I
,	O
we	O
may	O
define	O
the	O
chronological	B
quadtree	O
(	O
only	O
k	O
splits	O
are	O
allowed	O
,	O
defined	O
by	O
the	O
first	O
k	O
points	O
x	O
i	O
,	O
...	O
,	O
x	O
k	O
)	O
,	O
and	O
the	O
deep	B
quadtree	O
(	O
k	O
levels	O
of	O
splits	O
are	O
allowed	O
)	O
.	O
other	O
,	O
more	O
balanced	B
versions	O
may	O
also	O
be	O
introduced	O
.	O
classification	O
is	O
by	O
majority	B
vote	I
over	O
all	O
(	O
xi	O
,	O
yi	O
)	O
,	O
s-with	O
k	O
<	O
i	O
:	O
:	O
:	O
n	O
in	O
the	O
chronological	B
quadtree-that	O
fall	O
in	O
the	O
same	O
region	O
as	O
x.	O
ties	O
are	O
broken	O
in	O
favor	O
of	O
class	O
o.	O
we	O
will	O
refer	O
to	O
this	O
as	O
the	O
(	O
chronological	B
,	O
deep	B
)	O
quadtree	B
classifier	O
.	O
theorem	B
20.5.	O
whenever	O
x	O
has	O
a	O
density	O
,	O
the	O
chronological	B
quadtree	O
classifier	B
is	O
consistent	O
(	O
e	O
{	O
ln	O
}	O
--	O
-+	O
l	O
*	O
)	O
provided	O
that	O
k	O
--	O
-+	O
00	O
and	O
kin	O
--	O
-+	O
o.	O
proof	O
.	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
all	O
marginal	O
distributions	O
are	O
uni	O
(	O
cid:173	O
)	O
form	O
[	O
0,1	O
]	O
.	O
as	O
the	O
x-property	O
holds	O
,	O
theorem	B
6.1	O
applies	O
.	O
by	O
lemma	O
20.1	O
,	O
since	O
we	O
have	O
k	O
(	O
2d	O
-	O
1	O
)	O
+	O
1	O
external	O
regions	O
,	O
p	O
{	O
n	O
(	O
x	O
)	O
:	O
:	O
:	O
m	O
}	O
:	O
s	O
(	O
2m	O
+	O
4	O
)	O
(	O
k	O
(	O
2d	O
-	O
1	O
)	O
+	O
1	O
)	O
n	O
_	O
k	O
--	O
-+	O
0	O
for	O
all	O
m	O
>	O
0	O
,	O
provided	O
that	O
kin	O
--	O
-+	O
o.	O
hence	O
,	O
we	O
need	O
only	O
verify	O
the	O
condition	O
334	O
20.	O
tree	B
classifiers	O
diam	O
(	O
a	O
(	O
x	O
)	O
~	O
0	O
in	O
probability	O
.	O
this	O
is	O
a	O
bit	O
easier	O
than	O
in	O
the	O
proof	O
of	O
theorem	O
20.3	O
for	O
the	O
k-d	B
tree	I
and	O
is	O
thus	O
left	O
to	O
the	O
reader	O
(	O
problem	O
20.18	O
)	O
.	O
0	O
remark	O
.	O
full-fledged	O
random	O
quadtrees	O
with	O
n	O
nodes	O
have	O
expected	O
height	O
o	O
(	O
log	O
n	O
)	O
whenever	O
x	O
has	O
a	O
density	O
(	O
see	O
,	O
e.g.	O
,	O
devroye	O
and	O
laforest	O
(	O
1990	O
)	O
)	O
.	O
with	O
k	O
nodes	O
,	O
every	O
region	O
is	O
thus	O
reached	O
in	O
only	O
o	O
(	O
log	O
k	O
)	O
steps	O
on	O
the	O
average	O
.	O
fur	O
(	O
cid:173	O
)	O
thermore	O
,	O
quadtrees	O
enjoy	O
the	O
same	O
monotone	O
transformation	O
invariance	B
that	O
we	O
observed	O
for	O
k-d	B
trees	O
and	O
median	B
trees	O
.	O
0	O
20.8	O
best	O
possible	O
perpendicular	O
splits	O
for	O
computational	O
reasons	O
,	O
classification	O
trees	O
are	O
most	O
often	O
produced	O
by	O
deter	O
(	O
cid:173	O
)	O
mining	O
the	O
splits	O
recursively	O
.	O
at	O
a	O
given	O
stage	O
of	O
the	O
tree-growing	O
algorithm	B
,	O
some	O
criterion	O
is	O
used	O
to	O
determine	O
which	O
node	O
of	O
the	O
tree	B
should	O
be	O
split	O
next	O
,	O
and	O
where	O
the	O
split	O
should	O
be	O
made	O
.	O
as	O
these	O
criteria	O
typically	O
use	O
all	O
the	O
data	O
,	O
the	O
resulting	O
trees	O
no	O
longer	O
have	O
the	O
x	O
-property	O
.	O
in	O
this	O
section	O
we	O
examine	O
perhaps	O
the	O
most	O
natural	O
criterion	O
.	O
in	O
the	O
following	O
sections	O
we	O
introduce	O
some	O
alternative	O
splitting	O
criteria	O
.	O
a	O
binary	O
classification	O
tree	B
can	O
be	O
obtained	O
by	O
associating	O
with	O
each	O
node	O
a	O
splitting	O
function	O
¢	O
(	O
x	O
)	O
obtained	O
in	O
a	O
top-down	O
fashion	O
from	O
the	O
data	O
.	O
for	O
example	O
,	O
at	O
the	O
root	O
,	O
we	O
may	O
select	O
the	O
function	O
¢	O
(	O
x	O
)	O
=	O
sx	O
(	O
i	O
)	O
-	O
ex	O
,	O
where	O
i	O
,	O
the	O
component	O
cut	O
,	O
ex	O
e	O
n	O
,	O
the	O
threshold	B
,	O
and	O
s	O
e	O
{	O
-i	O
,	O
+	O
i	O
}	O
,	O
a	O
po	O
(	O
cid:173	O
)	O
larization	O
,	O
are	O
all	O
dependent	O
upon	O
the	O
data	O
.	O
the	O
root	O
then	O
splits	O
the	O
data	O
dn	O
=	O
{	O
(	O
xl	O
,	O
yd	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
}	O
into	O
two	O
sets	O
,	O
d~	O
,	O
d~	O
,	O
withd	O
;	O
1ud~	O
=	O
dn	O
,	O
id~i+id~i	O
=	O
n	O
,	O
such	O
that	O
d~	O
=	O
{	O
(	O
x	O
,	O
y	O
)	O
e	O
dn	O
:	O
¢	O
(	O
x	O
)	O
~	O
oj	O
,	O
d	O
;	O
:	O
{	O
(	O
x	O
,	O
y	O
)	O
e	O
dn	O
:	O
¢	O
(	O
x	O
)	O
<	O
o	O
}	O
.	O
a	O
decision	O
is	O
made	O
whether	O
to	O
split	O
a	O
node	O
or	O
not	O
,	O
and	O
the	O
procedure	O
is	O
applied	O
recursively	O
to	O
the	O
subtrees	O
.	O
natural	O
majority	O
vote	O
decisions	O
are	O
taken	O
at	O
the	O
leaf	O
level	O
.	O
all	O
such	O
trees	O
will	O
be	O
called	O
perpendicular	B
splitting	I
trees	O
.	O
in	O
chapter	O
4	O
,	O
we	O
introduced	O
univariate	O
stoller	O
splits	O
,	O
that	O
is	O
,	O
splits	O
that	O
minimize	O
the	O
empirical	B
error	I
.	O
this	O
could	O
be	O
at	O
the	O
basis	O
of	O
a	O
perpendicular	O
splitting	O
tree	O
.	O
one	O
realizes	O
immediately	O
that	O
the	O
number	O
of	O
possibilities	O
for	O
stopping	O
is	O
endless	O
.	O
to	O
name	O
two	O
,	O
we	O
could	O
stop	O
after	O
k	O
splitting	O
nodes	O
have	O
been	O
defined	O
,	O
or	O
we	O
could	O
make	O
a	O
tree	O
with	O
k	O
full	O
levels	O
of	O
splits	O
(	O
so	O
that	O
all	O
leaves	O
are	O
at	O
distance	B
k	O
from	O
the	O
root	O
)	O
.	O
we	O
first	O
show	O
that	O
for	O
d	O
>	O
1	O
,	O
any	O
such	O
strategy	O
is	O
virtually	O
doomed	O
to	O
fail	O
.	O
to	O
make	O
this	O
case	O
,	O
we	O
will	O
argue	O
on	O
the	O
basis	O
of	O
distribution	B
functions	O
only	O
.	O
for	O
convenience	O
,	O
we	O
consider	O
a	O
two-dimensional	O
problem	O
.	O
given	O
a	O
rectangle	O
r	O
now	O
20.8	O
best	O
possible	O
perpendicular	O
splits	O
335	O
assigned	O
to	O
one	O
class	O
y	O
e	O
{	O
o	O
,	O
i	O
}	O
,	O
we	O
see	O
that	O
the	O
current	O
probability	O
of	O
error	O
in	O
r	O
,	O
before	O
splitting	O
is	O
p	O
{	O
x	O
e	O
r	O
,	O
y	O
=i	O
y	O
}	O
.	O
let	O
r	O
'	O
range	O
over	O
all	O
rectangles	O
of	O
the	O
form	O
r	O
n	O
(	O
(	O
-oo	O
,	O
a	O
]	O
x	O
r	O
)	O
,	O
r	O
n	O
(	O
[	O
a	O
,	O
(	O
0	O
)	O
x	O
r	O
)	O
,	O
r	O
n	O
(	O
r	O
x	O
(	O
-00	O
,	O
ad	O
,	O
or	O
r	O
n	O
(	O
r	O
x	O
[	O
a	O
,	O
(	O
0	O
)	O
)	O
,	O
and	O
let	O
r	O
''	O
=	O
r	O
-	O
r	O
'	O
.	O
then	O
after	O
asplit	O
based	O
upon	O
(	O
r	O
'	O
,	O
r	O
''	O
)	O
,	O
the	O
probability	O
of	O
error	O
over	O
the	O
rectangle	O
r	O
is	O
p	O
{	O
x	O
e	O
r	O
'	O
,	O
y	O
=	O
i	O
}	O
+	O
p	O
{	O
x	O
e	O
r	O
''	O
,	O
y	O
=	O
o	O
}	O
as	O
we	O
assign	O
class	O
°	O
to	O
r	O
'	O
and	O
class	O
1	O
to	O
r	O
''	O
.	O
call1	O
:	O
:j	O
.	O
(	O
r	O
)	O
the	O
decrease	O
in	O
probability	O
of	O
error	O
if	O
we	O
minimize	O
over	O
all	O
r	O
'	O
.	O
computel	O
:	O
:j	O
.	O
(	O
r	O
)	O
for	O
all	O
leaf	O
rectangles	O
and	O
then	O
proceed	O
to	O
split	O
that	O
rectangle	O
(	O
or	O
leaf	O
)	O
for	O
which	O
i	O
:	O
:j	O
.	O
(	O
r	O
)	O
is	O
maximal	O
.	O
the	O
data-based	B
rule	O
based	O
upon	O
this	O
would	O
proceed	O
similarly	O
,	O
if	O
p	O
{	O
a	O
}	O
is	O
replaced	O
everywhere	O
by	O
the	O
empirical	B
estimate	O
(	O
l/n	O
)	O
l.:7=1	O
i	O
{	O
(	O
xi'yi	O
)	O
eaj	O
,	O
where	O
a	O
is	O
of	O
the	O
form	O
r	O
'	O
x	O
{	O
i	O
}	O
,	O
r	O
''	O
x	O
{	O
o	O
}	O
,	O
or	O
r	O
x	O
{	O
i	O
-	O
y	O
}	O
,	O
as	O
the	O
case	O
may	O
be	O
.	O
let	O
us	O
denote	O
by	O
l	O
o	O
,	O
l	O
1	O
,	O
l	O
2	O
,	O
•..	O
the	O
sequence	O
of	O
the	O
overall	O
probabilities	O
of	O
error	O
for	O
the	O
theoretically	O
optimal	O
sequence	O
of	O
cuts	O
described	O
above	O
.	O
here	O
we	O
start	O
with	O
r	O
=	O
r2	O
and	O
y	O
=	O
0	O
,	O
for	O
example	O
.	O
for	O
fixed	O
e	O
>	O
0	O
,	O
we	O
now	O
construct	O
a	O
simple	O
example	O
in	O
which	O
l*	O
=	O
0	O
,	O
and	O
lk	O
,	O
j..-	O
-	O
-	O
as	O
k	O
--	O
-+	O
00.	O
l-e	O
2	O
thus	O
,	O
applying	O
the	O
best	O
split	O
incrementally	O
,	O
even	O
if	O
we	O
use	O
the	O
true	O
probability	O
of	O
error	O
as	O
our	O
criterion	O
for	O
splitting	O
,	O
is	O
not	O
advisable	O
.	O
the	O
example	O
is	O
very	O
simple	O
:	O
x	O
has	O
uniform	B
distribution	O
on	O
[	O
0	O
,	O
1	O
]	O
2	O
with	O
proba	O
(	O
cid:173	O
)	O
bility	O
e	O
and	O
on	O
[	O
1,2	O
]	O
2	O
with	O
probability	O
1	O
-	O
e.	O
also	O
,	O
y	O
is	O
a	O
deterministic	O
function	O
of	O
x	O
,	O
so	O
that	O
l	O
*	O
=	O
0.	O
figure	O
20.16.	O
repeated	O
stoller	O
splits	O
are	O
not	O
consistent	O
in	O
this	O
two-dimensional	O
example	O
.	O
cuts	O
will	O
always	O
be	O
made	O
in	O
the	O
leftmost	O
square	O
.	O
probability	O
e	O
o	O
the	O
way	O
y	O
depends	O
on	O
x	O
is	O
shown	O
in	O
figure	O
20.16	O
:	O
y	O
=	O
1	O
if	O
x	O
e	O
[	O
1,3/2	O
]	O
2	O
u	O
[	O
3/2,2	O
]	O
2	O
u	O
(	O
a2	O
u	O
a4	O
u	O
a6	O
``	O
.	O
)	O
x	O
[	O
0	O
,	O
1	O
]	O
,	O
336	O
20.	O
tree	B
classifiers	O
where	O
al	O
=	O
a2	O
=	O
[	O
0	O
,	O
1/4	O
)	O
,	O
[	O
1/4	O
,	O
1/4	O
+	O
3/8	O
)	O
,	O
a3	O
=	O
[	O
1/4	O
+	O
3/8	O
,	O
1/4	O
+	O
3/8	O
+	O
3/16	O
)	O
,	O
ak	O
[	O
1/4	O
+	O
3/8	O
+	O
...	O
+	O
3/2k	O
,	O
1/4	O
+	O
3/8	O
+	O
...	O
+	O
3/2k+i	O
)	O
,	O
and	O
so	O
forth	O
.	O
we	O
verify	O
easily	O
that	O
p	O
{	O
y	O
=	O
i	O
}	O
=	O
1/2	O
.	O
also	O
,	O
the	O
error	O
probability	O
before	O
any	O
cut	O
is	O
made	O
is	O
lo	O
=	O
1/2	O
.	O
the	O
best	O
split	O
has	O
r	O
'	O
=	O
(	O
-00	O
,	O
1/4	O
)	O
x	O
r	O
so	O
that	O
al	O
is	O
cut	O
off	O
.	O
therefore	O
,	O
li	O
=	O
e/4	O
+	O
(	O
l	O
-	O
e	O
)	O
/2	O
.	O
we	O
continue	O
and	O
split	O
off	O
a2	O
and	O
so	O
forth	O
,	O
leading	O
to	O
the	O
tree	B
of	O
figure	O
20.17.	O
figure	O
20.17.	O
the	O
tree	B
obtained	O
by	O
repeated	O
stoller	O
splits	O
.	O
yes	O
o	O
verify	O
that	O
l2	O
=	O
e/8	O
+	O
(	O
l	O
-	O
e	O
)	O
/2	O
and	O
in	O
general	O
that	O
1-e	O
1-e	O
lk	O
-	O
-+	O
-	O
-	O
1_	O
-	O
'	O
2	O
''	O
0/	O
2	O
e	O
2k+1	O
-	O
as	O
claimed	O
.	O
20.9	O
splitting	O
criteria	O
based	O
on	O
impurity	O
functions	O
in	O
1984	O
,	O
breiman	O
,	O
friedman	O
,	O
olshen	O
,	O
and	O
stone	O
presented	O
their	O
cart	O
program	O
for	O
constructing	O
classification	O
trees	O
with	O
perpendicular	O
splits	O
.	O
one	O
of	O
the	O
key	O
ideas	O
in	O
their	O
approach	O
is	O
the	O
notion	O
that	O
trees	O
should	O
be	O
constructed	O
from	O
the	O
bottom	O
up	O
,	O
by	O
combining	O
small	O
subtrees	O
.	O
the	O
starting	O
point	O
is	O
a	O
tree	O
with	O
n	O
+	O
1	O
leaf	O
regions	O
c	O
defined	O
by	O
a	O
partition	O
of	O
the	O
space	O
based	O
on	O
the	O
n	O
data	O
points	O
.	O
such	O
a	O
tree	O
is	O
much	O
'	O
too	O
large	O
and	O
is	O
pruned	O
by	O
some	O
methods	O
that	O
will	O
not	O
be	O
explored	O
here	O
.	O
when	O
20.9	O
splitting	O
criteria	O
based	O
on	O
impurity	O
functions	O
337	O
constructing	O
a	O
starting	O
tree	B
,	O
a	O
certain	O
splitting	B
criterion	I
is	O
applied	O
recursively	O
.	O
the	O
criterion	O
determines	O
which	O
rectangle	O
should	O
be	O
split	O
,	O
and	O
where	O
the	O
cut	O
should	O
be	O
made	O
.	O
to	O
keep	O
the	O
classifier	B
invariant	O
under	O
monotone	O
transformation	O
of	O
the	O
coordinate	O
axes	O
,	O
the	O
criterion	O
should	O
only	O
depend	O
on	O
the	O
coordinatewise	O
ranks	O
of	O
the	O
points	O
,	O
and	O
their	O
labels	O
.	O
typically	O
the	O
criterion	O
is	O
a	O
function	O
of	O
the	O
numbers	O
of	O
points	O
labeled	O
by	O
°	O
and	O
1	O
in	O
the	O
rectangles	O
after	O
the	O
cut	O
is	O
made	O
.	O
one	O
such	O
class	O
of	O
let	O
a	O
e	O
n	O
,	O
and	O
let	O
i	O
be	O
a	O
given	O
coordinate	O
(	O
1	O
:	O
:s	O
i	O
:	O
:s	O
d	O
)	O
.	O
let	O
r	O
be	O
a	O
hyperrectangle	O
to	O
be	O
cut	O
.	O
define	O
the	O
following	O
quantities	O
for	O
a	O
split	O
at	O
a	O
,	O
perpendicular	O
to	O
the	O
i	O
-th	O
coordinate	O
:	O
criteria	O
is	O
described	O
here	O
.	O
xt	O
(	O
r	O
,	O
a	O
)	O
=	O
{	O
(	O
x	O
j	O
,	O
yj	O
)	O
:	O
xj	O
e	O
r	O
,	O
xji	O
)	O
>	O
a	O
}	O
are	O
the	O
sets	O
of	O
pairs	O
falling	O
to	O
the	O
left	O
and	O
to	O
the	O
right	O
of	O
the	O
cut	O
,	O
respectively	O
.	O
are	O
the	O
numbers	O
of	O
such	O
pairs	O
.	O
finally	O
,	O
the	O
numbers	O
of	O
points	O
with	O
label	O
°	O
and	O
label	O
1	O
in	O
these	O
sets	O
are	O
denoted	O
,	O
respectively	O
,	O
by	O
ni	O
,	O
ocr	O
,	O
a	O
)	O
=	O
ixi	O
(	O
r	O
,	O
a	O
)	O
n	O
{	O
cx	O
j	O
,	O
yj	O
)	O
:	O
yj	O
=	O
o	O
}	O
i	O
,	O
ixicr	O
,	O
a	O
)	O
n	O
{	O
cx	O
j	O
,	O
yj	O
)	O
:	O
yj	O
=	O
1	O
}	O
1	O
,	O
ni	O
,	O
lcr	O
,	O
a	O
)	O
!	O
xiccr	O
,	O
a	O
)	O
n	O
{	O
cxj	O
,	O
yj	O
)	O
:	O
yj=o	O
}	O
!	O
,	O
n	O
:	O
'ocr	O
,	O
a	O
)	O
=	O
n	O
(	O
,	O
l	O
(	O
r	O
,	O
a	O
)	O
=	O
i	O
xt	O
(	O
r	O
,	O
a	O
)	O
n	O
{	O
(	O
xj	O
,	O
yj	O
)	O
:	O
yj	O
=l	O
}	O
l·	O
following	O
breiman	O
,	O
friedman	O
,	O
olshen	O
,	O
and	O
stone	O
(	O
1984	O
)	O
,	O
we	O
define	O
an	O
impurity	O
junction	O
for	O
a	O
possible	O
split	O
(	O
i	O
,	O
a	O
)	O
by	O
ii	O
(	O
r	O
,	O
a	O
)	O
=	O
ljj	O
(	O
nio	O
,	O
nil	O
)	O
ni	O
+	O
ljj	O
nio	O
+	O
nil	O
nio	O
+	O
nil	O
(	O
n	O
!	O
o	O
n	O
:	O
l	O
)	O
i	O
i	O
'	O
i	O
i	O
ni	O
'	O
i	O
nio	O
+	O
nil	O
nio	O
+	O
nil	O
where	O
we	O
dropped	O
the	O
argument	O
(	O
r	O
,	O
a	O
)	O
throughout	O
.	O
here	O
ljj	O
is	O
a	O
nonnegative	O
func	O
(	O
cid:173	O
)	O
tion	O
with	O
the	O
following	O
properties	O
:	O
(	O
1	O
)	O
ljj	O
g	O
,	O
d	O
2	O
'	O
:	O
ljj	O
(	O
p	O
,	O
1	O
-	O
p	O
)	O
for	O
any	O
p	O
e	O
[	O
0	O
,	O
1	O
]	O
;	O
(	O
2	O
)	O
ljj	O
(	O
o	O
,	O
1	O
)	O
=	O
ljj	O
(	O
1	O
,	O
0	O
)	O
=	O
0	O
;	O
(	O
3	O
)	O
1f	O
;	O
'	O
(	O
p	O
,	O
1	O
-	O
p	O
)	O
increases	O
in	O
p	O
on	O
[	O
0	O
,	O
1/2	O
]	O
and	O
decreases	O
in	O
p	O
on	O
[	O
1/2,1	O
]	O
.	O
a	O
rectangle	O
r	O
is	O
split	O
at	O
a	O
along	O
the	O
i-th	O
coordinate	O
if	O
ii	O
(	O
r	O
,	O
a	O
)	O
is	O
minimal	O
.	O
ii	O
penalizes	O
splits	O
in	O
which	O
the	O
subregions	O
have	O
about	O
equal	O
proportions	O
from	O
both	O
classes	O
.	O
examples	O
of	O
such	O
functions	O
ljj	O
include	O
(	O
1	O
)	O
the	O
entropy	B
junction	O
1f	O
;	O
'	O
(	O
p	O
,	O
1-	O
p	O
)	O
=	O
-	O
p	O
log	O
p	O
-	O
(	O
1-	O
p	O
)	O
10g	O
(	O
1-	O
p	O
)	O
(	O
breiman	O
et	O
al	O
.	O
(	O
1984	O
,	O
pp.25,103	O
)	O
)	O
.	O
338	O
20.	O
tree	B
classifiers	O
(	O
2	O
)	O
the	O
gini	O
function	O
ljf	O
(	O
p	O
,	O
1	O
-	O
p	O
)	O
=	O
2p	O
(	O
1	O
-	O
p	O
)	O
,	O
leading	O
to	O
the	O
gini	O
index	O
of	O
diversity	O
f	O
i	O
•	O
(	O
breiman	O
et	O
al	O
.	O
(	O
1984	O
,	O
pp.38	O
,	O
l03	O
»	O
.	O
(	O
3	O
)	O
the	O
probability	O
ofmisclassification	O
t	O
(	O
p	O
,	O
1	O
-	O
p	O
)	O
=	O
min	O
(	O
p	O
,	O
1	O
-	O
p	O
)	O
.	O
in	O
this	O
case	O
the	O
splitting	B
criterion	I
leads	O
to	O
the	O
empirical	B
stoller	O
splits	O
studied	O
in	O
the	O
previous	O
section	O
.	O
we	O
have	O
two	O
kinds	O
of	O
splits	O
:	O
(	O
a	O
)	O
theforced	O
split	O
:	O
force	O
a	O
split	O
along	O
the	O
i-th	O
coordinate	O
,	O
but	O
minimize	O
fica	O
,	O
r	O
)	O
over	O
all	O
a	O
and	O
rectangles	O
r.	O
(	O
b	O
)	O
the	O
free	O
split	O
:	O
choose	O
the	O
most	O
advantageous	O
coordinate	O
for	O
splitting	O
,	O
that	O
is	O
,	O
minimize	O
fica	O
,	O
r	O
)	O
over	O
all	O
a	O
,	O
i	O
,	O
and	O
r.	O
unfortunately	O
,	O
regardless	O
of	O
which	O
kind	O
of	O
policy	O
we	O
choose	O
,	O
there	O
are	O
distributions	O
for	O
which	O
no	O
splitting	O
based	O
on	O
an	O
impurity	B
function	I
leads	O
to	O
a	O
consistent	O
classifier	B
.	O
to	O
see	O
this	O
,	O
note	O
that	O
the	O
two-dimensional	O
example	O
of	O
the	O
previous	O
section	O
applies	O
to	O
all	O
impurity	O
functions	O
.	O
assume	O
that	O
we	O
had	O
infinite	O
sample	O
size	O
.	O
then	O
fica	O
,	O
r	O
)	O
would	O
approach	O
aljf	O
(	O
p	O
,	O
1-	O
p	O
)	O
+	O
bt	O
(	O
q	O
,	O
1-	O
q	O
)	O
,	O
where	O
p	O
is	O
the	O
probability	O
content	O
of	O
r	O
'	O
(	O
one	O
of	O
the	O
rectangles	O
obtained	O
after	O
the	O
cut	O
is	O
made	O
)	O
,	O
b	O
is	O
that	O
of	O
r	O
''	O
(	O
the	O
other	O
rectangle	O
)	O
,	O
p	O
=	O
p	O
{	O
y	O
=	O
11x	O
e	O
r	O
'	O
}	O
and	O
q	O
=	O
p	O
{	O
y	O
=	O
llx	O
e	O
r	O
''	O
}	O
.	O
if	O
x	O
is	O
uniformly	O
distributed	O
on	O
the	O
checkerboard	O
shown	O
in	O
figure	O
20.18	O
,	O
regardless	O
where	O
we	O
try	O
to	O
cut	O
,	O
p	O
=	O
q	O
=	O
1/2	O
,	O
and	O
every	O
cut	O
seems	O
to	O
be	O
undesirable	O
.	O
figure	O
20.18.	O
no	O
cut	O
decreases	O
the	O
value	O
of	O
the	O
impurity	B
function	I
.	O
this	O
simple	O
example	O
may	O
be	O
made	O
more	O
interesting	O
by	O
mixing	O
it	O
with	O
a	O
distribu	O
(	O
cid:173	O
)	O
tion	O
with	O
much	O
less	O
weight	O
in	O
which	O
x	O
(	O
l	O
)	O
-	O
and	O
x	O
(	O
2	O
)	O
-direction	O
splits	O
are	O
alternatingly	O
encouraged	O
all	O
the	O
time	O
.	O
therefore	O
,	O
impurity	O
functions	O
should	O
be	O
avoided	O
in	O
their	O
raw	B
form	O
for	O
splitting	O
.	O
this	O
may	O
explain	O
partially	O
why	O
in	O
cart	O
,	O
the	O
original	O
tree	B
is	O
undesirable	O
and	O
must	O
be	O
pruned	O
from	O
the	O
bottom	O
up	O
.	O
see	O
problems	O
20.22	O
to	O
20.24	O
for	O
more	O
information	O
.	O
in	O
the	O
next	O
section	O
and	O
in	O
the	O
last	O
section	O
of	O
this	O
chapter	O
we	O
propose	O
other	O
ways	O
of	O
growing	O
trees	O
with	O
much	O
more	O
desirable	O
properties	O
.	O
the	O
derivation	O
shown	O
above	O
does	O
not	O
indicate	O
that	O
the	O
empirical	B
version	O
will	O
not	O
work	O
properly	O
,	O
but	O
simple	O
versions	O
of	O
it	O
will	O
certainly	O
not	O
.	O
see	O
problem	O
20.22.	O
remark	O
.	O
malicious	O
splitting	O
.	O
the	O
impurity	O
functions	O
suggested	O
above	O
all	O
avoid	O
leaving	O
the	O
proportions	O
of	O
zeros	O
and	O
ones	O
intact	O
through	O
splitting	O
.	O
they	O
push	O
to	O
(	O
cid:173	O
)	O
wards	O
more	O
homogeneous	O
regions	O
.	O
assume	O
now	O
that	O
we	O
do	O
the	O
opposite	O
.	O
through	O
such	O
splits	O
,	O
we	O
can	O
in	O
fact	O
create	O
hyperplane	B
classification	O
trees	O
that	O
are	O
globally	O
poor	O
,	O
that	O
is	O
,	O
that	O
are	O
such	O
that	O
every	O
trimmed	O
version	O
of	O
the	O
tree	B
is	O
also	O
a	O
poor	O
classifier	B
.	O
such	O
splitting	O
methods	O
must	O
of	O
course	O
use	O
the	O
yi	O
'so	O
our	O
example	O
shows	O
20.9	O
splitting	O
criteria	O
based	O
on	O
impurity	O
functions	O
339	O
that	O
any	O
general	O
consistency	B
theorem	O
for	O
hyperplane	B
classification	O
trees	O
must	O
come	O
with	O
certain	O
restrictions	O
on	O
the	O
splitting	O
process-the	O
x	O
property	O
is	O
good	O
;	O
some	O
(	O
cid:173	O
)	O
times	O
it	O
is	O
necessary	O
to	O
force	O
cells	O
to	O
shrink	O
;	O
sometimes	O
the	O
position	O
of	O
the	O
split	O
is	O
restricted	O
by	O
empirical	B
error	I
minimization	O
or	O
some	O
other	O
criterion	O
.	O
the	O
ham-sandwich	B
theorem	I
(	O
see	O
edelsbrunner	O
(	O
1987	O
»	O
states	O
that	O
given	O
2n	O
class-	O
o	O
points	O
and	O
2m	O
class-l	O
points	O
in	O
nd	O
,	O
d	O
>	O
1	O
,	O
there	O
exists	O
a	O
hyperplane	O
cut	O
that	O
leaves	O
n	O
class-o	O
points	O
and	O
m	O
class-l	O
points	O
in	O
each	O
halfspace	O
.	O
so	O
,	O
assume	O
that	O
x	O
has	O
a	O
density	O
and	O
that	O
p	O
=	O
p	O
{	O
y	O
=	O
i	O
}	O
=	O
1/2	O
.	O
in	O
a	O
sample	O
of	O
size	O
n	O
,	O
let	O
y	O
be	O
the	O
majority	O
class	O
(	O
ties	O
are	O
broken	O
in	O
favor	O
of	O
class	O
0	O
)	O
.	O
figure	O
20.19.	O
ham-sandwich	O
cut	O
:	O
each	O
halfspace	O
contains	O
exactly	O
half	O
the	O
points	O
from	O
each	O
class	O
.	O
o	O
•	O
o	O
•	O
regardless	O
of	O
the	O
sample	O
make-up	O
,	O
if	O
y	O
=	O
0	O
,	O
we	O
may	O
construct	O
a	O
hyperplane-tree	O
classifier	B
in	O
which	O
,	O
during	O
the	O
construction	O
,	O
every	O
node	O
represents	O
a	O
region	O
in	O
which	O
the	O
majority	B
vote	I
would	O
be	O
o.	O
this	O
property	O
has	O
nothing	O
to	O
do	O
with	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
and	O
therefore	O
,	O
for	O
any	O
trimmed	O
version	O
of	O
the	O
tree	B
classifier	O
,	O
and	O
p	O
{	O
ln	O
:	O
:	O
:	O
:	O
p	O
}	O
:	O
:	O
:	O
:	O
p	O
{	O
y	O
=	O
o	O
}	O
:	O
:	O
:	O
:	O
1/2	O
if	O
p	O
=	O
1/2	O
.	O
obviously	O
,	O
as	O
we	O
may	O
take	O
l	O
*	O
=	O
0	O
,	O
these	O
classifiers	O
are	O
hopeless	O
.	O
0	O
bibliographic	O
remarks	O
.	O
empirical	B
stoller	O
splits	O
without	O
forced	O
rotation	B
were	O
rec	O
(	O
cid:173	O
)	O
ommended	O
by	O
payne	O
and	O
meisel	O
(	O
1977	O
)	O
and	O
rounds	O
(	O
1980	O
)	O
,	O
but	O
their	O
failure	O
to	O
be	O
universally	O
good	O
was	O
noted	O
by	O
gordon	O
and	O
olshen	O
(	O
1978	O
)	O
.	O
the	O
last	O
two	O
au	O
(	O
cid:173	O
)	O
thors	O
recommended	O
a	O
splitting	O
scheme	O
that	O
combined	O
several	O
ideas	O
,	O
but	O
roughly	O
speaking	O
,	O
they	O
perform	O
empirical	B
stoller	O
splits	O
with	O
forced	O
rotation	B
through	O
the	O
co	O
(	O
cid:173	O
)	O
ordinate	O
axes	O
(	O
gordon	O
and	O
olshen	O
(	O
1978	O
)	O
,	O
(	O
1980	O
»	O
.	O
other	O
splitting	O
criteria	O
include	O
the	O
aid	O
criterion	O
of	O
morgan	O
and	O
sonquist	O
(	O
1963	O
)	O
,	O
which	O
is	O
a	O
predecessor	O
of	O
the	O
gini	O
index	O
of	O
diversity	O
used	O
in	O
cart	O
(	O
breiman	O
,	O
friedman	O
,	O
olshen	O
,	O
and	O
stone	O
(	O
1984	O
)	O
,	O
see	O
also	O
gelfand	O
and	O
delp	O
(	O
1991	O
)	O
,	O
guo	O
and	O
gelfand	O
(	O
1992	O
)	O
gelfand	O
,	O
ravishankar	O
,	O
and	O
delp	O
(	O
1989	O
)	O
,	O
(	O
1991	O
)	O
,	O
and	O
ciampi	O
(	O
1991	O
»	O
.	O
michel-briand	O
and	O
milhaud	O
(	O
1994	O
)	O
also	O
observed	O
the	O
failure	O
of	O
multivariate	O
classification	O
trees	O
based	O
on	O
the	O
aid	O
criterion	O
.	O
the	O
shannon	O
entropy	B
or	O
modifications	O
of	O
it	O
are	O
recommended	O
by	O
talmon	O
(	O
1986	O
)	O
,	O
sethi	O
and	O
sarvarayudu	O
(	O
1982	O
)	O
,	O
wang	O
and	O
suen	O
(	O
1984	O
)	O
,	O
goodman	O
and	O
smyth	O
(	O
1988	O
)	O
,	O
and	O
chou	O
(	O
1991	O
)	O
.	O
permutation	O
statistics	B
are	O
used	O
in	O
li	O
and	O
dubes	O
(	O
1986	O
)	O
,	O
still	O
without	O
forced	O
rotations	O
through	O
the	O
coordinate	O
axes	O
.	O
quinlan	O
(	O
1993	O
)	O
has	O
a	O
more	O
involved	O
splitting	B
criterion	I
.	O
a	O
general	O
discussion	O
on	O
tree	B
splitting	O
may	O
be	O
340	O
20.	O
tree	B
classifiers	O
found	O
in	O
the	O
paper	O
by	O
sethi	O
(	O
1991	O
)	O
.	O
a	O
class	O
of	O
impurity	O
functions	O
is	O
studied	O
in	O
burshtein	O
,	O
della	O
pietra	O
,	O
kanevsky	O
,	O
and	O
nadas	O
(	O
1992	O
)	O
.	O
among	O
the	O
pioneers	O
of	O
tree	O
splitting	O
(	O
with	O
perpendicular	O
cuts	O
)	O
are	O
sebestyen	O
(	O
1962	O
)	O
,	O
and	O
henrichon	O
and	O
fu	O
(	O
1969	O
)	O
.	O
for	O
related	O
work	O
,	O
we	O
refer	O
to	O
stoffel	O
(	O
1974	O
)	O
,	O
sethi	O
and	O
chatterjee	O
(	O
1977	O
)	O
,	O
argentiero	O
,	O
chin	O
and	O
beaudet	O
(	O
1982	O
)	O
,	O
you	O
andfu	O
(	O
1976	O
)	O
,	O
anderson	O
andfu	O
(	O
1979	O
)	O
,	O
qing-yun	O
and	O
fu	O
(	O
1983	O
)	O
,	O
hartmann	O
,	O
varshney	O
,	O
mehrotra	O
,	O
and	O
gerberich	O
(	O
1982	O
)	O
,	O
and	O
casey	O
and	O
nagy	O
(	O
1984	O
)	O
.	O
references	O
on	O
nonperpendicular	O
splitting	O
methods	O
are	O
given	O
below	O
in	O
the	O
section	O
on	O
esp	O
trees	O
.	O
20.10	O
a	O
consistent	O
splitting	B
criterion	I
there	O
is	O
no	O
reason	O
for	O
pessimism	O
after	O
the	O
previous	O
sections	O
.	O
rest	O
assured	O
,	O
there	O
are	O
several	O
consistent	O
splitting	O
strategies	O
that	O
are	O
fully	O
automatic	B
and	O
depend	O
only	O
upon	O
the	O
populations	O
of	O
the	O
regions	O
.	O
in	O
this	O
section	O
we	O
provide	O
a	O
solution	O
for	O
the	O
simple	O
case	O
when	O
x	O
is	O
univariate	O
and	O
nonatomic	O
.	O
it	O
is	O
possible	O
to	O
generalize	O
the	O
method	O
for	O
d	O
>	O
1	O
if	O
we	O
force	O
cuts	O
to	O
alternate	O
directions	O
.	O
we	O
omit	O
here	O
the	O
detailed	O
analysis	O
for	O
multidimensional	O
cases	O
for	O
two	O
reasons	O
.	O
first	O
,	O
it	O
is	O
significantly	O
heavier	O
than	O
for	O
d	O
=	O
1.	O
secondly	O
,	O
in	O
the	O
last	O
section	O
of	O
this	O
chapter	O
we	O
introduce	O
a	O
fully	O
automatic	B
way	O
of	O
building	O
up	O
consistent	O
trees	O
,	O
that	O
is	O
,	O
without	O
forcing	O
the	O
directions	O
of	O
the	O
splits	O
.	O
to	O
a	O
partition	O
ai	O
,	O
...	O
,	O
an	O
of	O
r	O
,	O
we	O
assign	O
the	O
quantity	O
n	O
q	O
=	O
l	O
no	O
(	O
ai	O
)	O
ni	O
(	O
ai	O
)	O
,	O
i=1	O
where	O
n	O
no	O
(	O
a	O
)	O
=	O
l	O
i	O
{	O
xjea	O
,	O
yj=oj	O
,	O
j=l	O
n	O
ni	O
(	O
a	O
)	O
=	O
l	O
i	O
{	O
xjea	O
,	O
yj=l	O
}	O
j=l	O
are	O
the	O
respective	O
numbers	O
of	O
points	O
labeled	O
with	O
0	O
and	O
1	O
falling	O
in	O
the	O
region	O
a.	O
the	O
tree-growing	O
algorithm	B
starts	O
with	O
the	O
trivial	O
partition	B
{	O
r	O
}	O
,	O
and	O
at	O
each	O
step	O
it	O
makes	O
a	O
cut	O
that	O
yields	O
the	O
minimal	O
value	O
of	O
q.	O
it	O
proceeds	O
recursively	O
until	O
the	O
improvement	O
in	O
the	O
value	O
of	O
q	O
falls	O
below	O
a	O
threshold	O
~n	O
.	O
remark	O
.	O
notice	O
that	O
this	O
criterion	O
always	O
splits	O
a	O
cell	O
that	O
has	O
many	O
points	O
from	O
both	O
classes	O
(	O
see	O
the	O
proof	O
of	O
the	O
theorem	B
below	O
)	O
.	O
thus	O
,	O
it	O
avoids	O
the	O
anomalies	O
of	O
impurity-function	O
criteria	O
described	O
in	O
the	O
previous	O
section	O
.	O
on	O
the	O
other	O
hand	O
,	O
it	O
does	O
not	O
necessarily	O
split	O
large	O
cells	O
,	O
if	O
they	O
are	O
almost	O
homogeneous	O
.	O
for	O
a	O
comparison	O
,	O
recall	O
that	O
the	O
gini	O
criterion	O
minimizes	O
the	O
quantity	O
,	O
~	O
no	O
(	O
ai	O
)	O
nl	O
(	O
ai	O
)	O
o	O
=~	O
i=1	O
no	O
(	O
ai	O
)	O
+	O
ni	O
(	O
ai	O
)	O
'	O
-	O
thus	O
favoring	O
cutting	O
cells	O
with	O
very	O
few	O
points	O
.	O
we	O
realize	O
that	O
the	O
criterion	O
q	O
introduced	O
here	O
is	O
just	O
one	O
of	O
many	O
with	O
similarly	O
good	O
properties	O
,	O
and	O
albeit	O
probably	O
imperfect	O
,	O
it	O
is	O
certainly	O
one	O
of	O
the	O
simplest	O
.	O
0	O
20.11	O
bsp	O
trees	O
341	O
theorem	B
20.6.	O
let	O
x	O
have	O
a	O
nonatomic	O
distribution	B
on	O
the	O
real	O
line	O
,	O
and	O
consider	O
the	O
tree	B
classifier	O
obtained	O
by	O
the	O
algorithm	B
described	O
above	O
.	O
if	O
the	O
threshold	B
satisfies	O
-+	O
0	O
,	O
~n	O
-	O
n	O
~n	O
-+	O
(	O
xl	O
and	O
2	O
n	O
then	O
the	O
classification	O
rule	B
is	O
strongly	O
consistent	O
.	O
proof	O
.	O
there	O
are	O
two	O
key	O
properties	O
of	O
the	O
algorithm	B
that	O
we	O
exploit	O
:	O
property	O
1.	O
if	O
min	O
(	O
no	O
(	O
a	O
)	O
,	O
ni	O
(	O
a	O
)	O
)	O
>	O
v2~n	O
for	O
a	O
cell	O
a	O
,	O
then	O
a	O
gets	O
cut	O
by	O
the	O
algorithm	B
.	O
to	O
see	O
this	O
,	O
observe	O
that	O
(	O
dropping	O
the	O
argument	O
a	O
from	O
the	O
notation	O
)	O
if	O
no	O
:	O
:	O
:	O
n	O
1	O
,	O
and	O
we	O
cut	O
a	O
so	O
that	O
the	O
number	O
of	O
o-labeled	O
points	O
in	O
the	O
two	O
child	O
regions	O
differ	O
by	O
at	O
most	O
one	O
,	O
then	O
the	O
contribution	O
of	O
these	O
two	O
new	O
regions	O
to	O
q	O
is	O
r	O
no/2l	O
n	O
{	O
+	O
lno/2j	O
n~	O
'	O
:	O
s	O
r	O
nond2l	O
,	O
where	O
n	O
{	O
and	O
n	O
{	O
'	O
are	O
the	O
numbers	O
of	O
class-1	O
points	O
in	O
the	O
two	O
child	O
regions	O
.	O
thus	O
,	O
the	O
decrease	O
of	O
q	O
if	O
a	O
is	O
split	O
is	O
at	O
least	O
lnoni/2j	O
.	O
if	O
min	O
(	O
no	O
,	O
nd	O
>	O
j2~n	O
'	O
then	O
d.n	O
:	O
s	O
lnoni/2j	O
,	O
and	O
a	O
cut	O
is	O
made	O
.	O
property	O
2.	O
no	O
leaf	O
node	O
has	O
less	O
than	O
~nl	O
n	O
points	O
.	O
assume	O
that	O
after	O
a	O
region	O
is	O
cut	O
,	O
in	O
one	O
of	O
the	O
child	O
regions	O
,	O
the	O
total	O
number	O
of	O
points	O
is	O
n~	O
+	O
n	O
{	O
:	O
s	O
k.	O
then	O
the	O
improvement	O
in	O
q	O
caused	O
by	O
the	O
split	O
is	O
bounded	O
by	O
nonl	O
-	O
(	O
nbn~	O
+	O
n~	O
n~	O
'	O
)	O
s	O
nonl	O
-	O
(	O
no	O
-	O
k	O
)	O
(	O
nl	O
-	O
k	O
)	O
s	O
(	O
no	O
+	O
n1	O
)	O
k	O
s	O
nk	O
.	O
therefore	O
,	O
if	O
k	O
<	O
~n	O
in	O
,	O
then	O
the	O
improvement	O
is	O
smaller	O
than	O
~n	O
'	O
thus	O
,	O
no	O
cut	O
is	O
made	O
that	O
leaves	O
behind	O
a	O
child	O
region	O
with	O
fewer	O
than	O
~n	O
i	O
n	O
points	O
.	O
it	O
follows	O
from	O
property	O
1	O
that	O
if	O
a	O
leaf	O
region	O
has	O
more	O
than	O
4v2~n	O
points	O
,	O
then	O
since	O
the	O
class	O
in	O
minority	O
has	O
less	O
than	O
j2~n	O
points	O
in	O
it	O
,	O
it	O
may	O
be	O
cut	O
into	O
intervals	O
containing	O
between	O
2j2~n	O
and	O
4j2~n	O
points	O
without	O
altering	O
the	O
decision	O
,	O
since	O
the	O
majority	B
vote	I
within	O
each	O
region	O
remains	O
the	O
same	O
.	O
summarizing	O
,	O
we	O
see	O
that	O
the	O
tree	B
classifier	O
is	O
equivalent	O
to	O
a	O
classifier	O
that	O
partitions	O
the	O
real	O
line	O
into	O
intervals	O
,	O
each	O
containing	O
at	O
least	O
~nln	O
,	O
and	O
at	O
most	O
4j2~n	O
data	O
points	O
.	O
thus	O
,	O
in	O
this	O
partition	B
,	O
each	O
interval	O
has	O
a	O
number	O
of	O
points	O
growing	O
to	O
infinity	O
as	O
o	O
(	O
n	O
)	O
.	O
we	O
emphasize	O
that	O
the	O
number	O
of	O
points	O
in	O
a	O
region	O
of	O
the	O
studied	O
tree	B
classifier	O
may	O
be	O
large	O
,	O
but	O
such	O
regions	O
are	O
almost	O
homogeneous	O
,	O
and	O
therefore	O
the	O
classifier	B
is	O
equivalent	O
to	O
another	O
classifier	B
which	O
has	O
o	O
(	O
n	O
)	O
points	O
in	O
each	O
region	O
.	O
consistency	B
of	O
such	O
partitioning	O
classifiers	O
is	O
proved	O
in	O
the	O
next	O
chapter-see	O
theorem	B
21.3.	O
d	O
20.11	O
bsp	O
trees	O
binary	O
space	O
partition	O
trees	O
(	O
or	O
bsp	O
trees	O
)	O
partition	B
rd	O
by	O
hyperplanes	O
.	O
trees	O
of	O
this	O
nature	O
have	O
evolved	O
in	O
the	O
computer	O
graphics	O
literature	O
via	O
the	O
work	O
of	O
fuchs	O
,	O
342	O
20.	O
tree	B
classifiers	O
kedem	O
,	O
and	O
naylor	O
(	O
1980	O
)	O
and	O
fuchs	O
,	O
abram	O
,	O
and	O
grant	O
(	O
1983	O
)	O
(	O
see	O
also	O
samet	O
(	O
1990b	O
)	O
,	O
kaplan	O
(	O
1985	O
)	O
,	O
and	O
sung	O
and	O
shirley	O
(	O
1992	O
»	O
.	O
in	O
discrimination	O
they	O
are	O
at	O
the	O
same	O
time	O
generalizations	O
of	O
linear	O
discriminants	O
,	O
of	O
histograms	O
,	O
and	O
of	O
binary	O
tree	B
classifiers	O
.	O
bsp	O
trees	O
were	O
recommended	O
for	O
use	O
in	O
discrimination	O
by	O
henrichon	O
and	O
fu	O
(	O
1969	O
)	O
,	O
mizoguchi	O
,	O
kizawa	O
,	O
and	O
shimura	O
(	O
1977	O
)	O
and	O
friedman	O
(	O
1977	O
)	O
.	O
further	O
studies	O
include	O
sklansky	O
and	O
michelotti	O
(	O
1980	O
)	O
,	O
argentiero	O
,	O
chin	O
,	O
and	O
beaudet	O
(	O
1982	O
)	O
,	O
qing-yun	O
and	O
fu	O
(	O
1983	O
)	O
,	O
breiman	O
,	O
friedman	O
,	O
olshen	O
,	O
and	O
stone	O
(	O
1984	O
)	O
,	O
loh	O
and	O
vanichsetakul	O
(	O
1988	O
)	O
,	O
and	O
park	O
and	O
sklansky	O
(	O
1990	O
)	O
.	O
there	O
are	O
numerous	O
ways	O
of	O
constructing	O
bsp	O
trees	O
.	O
most	O
methods	O
of	O
course	O
use	O
the	O
y	O
-values	O
to	O
determine	O
good	O
splits	O
.	O
nevertheless	O
,	O
we	O
should	O
mention	O
first	O
simple	O
splits	O
with	O
the	O
x	O
-property	O
,	O
if	O
only	O
to	O
better	O
understand	O
the	O
bsp	O
trees	O
.	O
1,2	O
figure	O
20.20.	O
a	O
raw	O
bsp	O
tree	B
and	O
its	O
induced	O
partition	B
in	O
the	O
plane	O
.	O
every	O
region	O
is	O
split	O
by	O
a	O
line	O
determined	O
by	O
the	O
two	O
data	O
points	O
with	O
smallest	O
index	O
in	O
the	O
region	O
.	O
we	O
call	O
the	O
raw	B
bsp	O
tree	B
the	O
tree	B
obtained	O
by	O
letting	O
xl	O
,	O
...	O
,	O
xd	O
determine	O
the	O
first	O
hyperplane	B
.	O
the	O
d	O
data	O
points	O
remain	O
associated	O
with	O
the	O
root	O
,	O
and	O
the	O
others	O
(	O
xd+l	O
,	O
...	O
,	O
x	O
k	O
)	O
are	O
sent	O
down	O
to	O
the	O
subtrees	O
,	O
where	O
the	O
process	O
is	O
repeated	O
as	O
far	O
as	O
possible	O
.	O
the	O
remaining	O
points	O
xk+l	O
,	O
...	O
,	O
xn	O
are	O
used	O
in	O
a	O
majority	B
vote	I
in	O
the	O
external	O
regions	O
.	O
note	O
that	O
the	O
number	O
of	O
regions	O
is	O
not	O
more	O
than	O
kid	O
.	O
thus	O
,	O
by	O
lemma	O
20.1	O
,	O
if	O
kin	O
-+	O
0	O
,	O
we	O
have	O
n	O
(	O
x	O
)	O
-+	O
00	O
in	O
probability	O
.	O
combining	O
this	O
with	O
problem	O
20.25	O
,	O
we	O
have	O
our	O
first	O
result	O
:	O
theorem	B
20.7.	O
the	O
natural	O
binary	O
tree	B
classifier	O
based	O
upon	O
a	O
raw	O
bsp	O
tree	B
with	O
k	O
-+	O
00	O
and	O
kin	O
-+	O
0	O
is	O
consistent	O
whenever	O
x	O
has	O
a	O
density	O
.	O
hyperplanes	O
may	O
also	O
be	O
selected	O
by	O
optimization	O
of	O
a	O
criterion	O
.	O
typically	O
,	O
this	O
would	O
involve	O
separating	O
the	O
classes	O
in	O
some	O
way	O
.	O
all	O
that	O
was	O
said	O
for	O
perpen~	O
dicular	O
splitting	O
remains	O
valid	O
here	O
.	O
it	O
is	O
worthwhile	O
recalling	O
therefore	O
that	O
there	O
20.12	O
primitive	O
selection	B
343	O
are	O
distributions	O
for	O
which	O
the	O
best	O
empirical	B
stoller	O
split	O
does	O
not	O
improve	O
the	O
probabilit~	O
of	O
error	O
.	O
take	O
for	O
example	O
the	O
uniform	B
distribution	O
in	O
the	O
unit	O
ball	O
of	O
nd	O
in	O
which	O
y	O
=	O
{	O
1	O
if	O
iixii	O
2	O
:	O
1/2	O
o	O
if	O
iixii	O
<	O
1/2	O
.	O
figure	O
20.21.	O
no	O
single	O
split	O
improves	O
on	O
the	O
error	O
prob	O
(	O
cid:173	O
)	O
ability	O
for	O
this	O
distribution	B
.	O
here	O
,	O
no	O
linear	O
split	O
would	O
be	O
helpful	O
as	O
the	O
l	O
'	O
s	O
would	O
always	O
hold	O
a	O
strong	O
majority	O
.	O
minimizing	O
other	O
impurity	O
functions	O
such	O
as	O
the	O
gini	O
criterion	O
may	O
be	O
helpful	O
,	O
however	O
(	O
problem	O
20.27	O
)	O
.	O
bibliographic	O
remarks	O
.	O
hyperplane	B
splits	O
may	O
be	O
generalized	B
to	O
include	O
quadratic	O
splits	O
(	O
henrichon	O
and	O
fu	O
(	O
1969	O
)	O
)	O
.	O
for	O
example	O
,	O
mui	O
and	O
fu	O
(	O
1980	O
)	O
suggest	O
taking	O
d	O
'	O
<	O
d	O
and	O
forming	O
quadratic	O
classifiers	O
as	O
in	O
normal	O
discrimination	O
(	O
see	O
chapter	O
4	O
)	O
based	O
upon	O
vectors	O
in	O
r	O
d	O
'	O
.	O
the	O
cuts	O
are	O
thus	O
perpendicular	O
to	O
d	O
-	O
d	O
'	O
axes	O
but	O
quadratic	O
in	O
the	O
subspace	O
r	O
d	O
tance	O
or	O
2-means	O
clustering	B
for	O
determining	O
splits	O
.	O
as	O
a	O
novelty	O
,	O
within	O
each	O
leaf	O
region	O
,	O
the	O
decision	O
is	O
not	O
by	O
majority	B
vote	I
,	O
but	O
rather	O
by	O
a	O
slightly	O
more	O
sophisti	O
(	O
cid:173	O
)	O
cated	O
rule	B
such	O
as	O
the	O
k-nn	O
rule	B
or	O
linear	O
discrimination	O
.	O
here	O
,	O
no	O
optimization	O
is	O
required	O
along	O
the	O
way	O
.	O
loh	O
and	O
vanichsetakul	O
(	O
1988	O
)	O
allow	O
linear	O
splits	O
but	O
use	O
f	O
ratios	O
to	O
select	O
desirable	O
hyperplanes	O
.	O
'	O
.	O
lin	O
and	O
fu	O
(	O
1983	O
)	O
employ	O
the	O
bhattacharyya	O
dis	O
(	O
cid:173	O
)	O
20.12	O
primitive	O
selection	B
there	O
are	O
two	O
reasons	O
for	O
optimizing	O
a	O
tree	O
configuration	O
.	O
first	O
of	O
all	O
,	O
it	O
just	O
does	O
not	O
make	O
sense	O
to	O
ignore	O
the	O
class	O
labels	O
when	O
constructing	O
a	O
tree	O
classifier	B
,	O
so	O
the	O
yi	O
's	O
must	O
be	O
used	O
to	O
help	O
in	O
the	O
selection	B
of	O
a	O
best	O
tree	B
.	O
secondly	O
,	O
some	O
trees	O
may	O
not	O
be	O
consistent	O
(	O
or	O
provably	O
consistent	O
)	O
,	O
yet	O
when	O
optimized	O
over	O
a	O
family	O
of	O
trees	O
,	O
consistency	B
drops	O
out	O
.	O
we	O
take	O
the	O
following	O
example	O
:	O
let	O
(	O
;	O
h	O
be	O
a	O
class	O
of	O
binary	O
tree	B
classifiers	O
with	O
the	O
x	O
-property	O
,	O
with	O
the	O
space	O
partitioned	O
into	O
k	O
+	O
1	O
regions	O
determined	O
by	O
xl	O
,	O
...	O
,	O
xk	O
only	O
.	O
examples	O
include	O
the	O
chronological	B
k-d	O
tree	B
and	O
some	O
kinds	O
of	O
bsp	O
trees	O
.	O
we	O
estimate	B
the	O
error	O
for	O
g	O
e	O
9k	O
by	O
realizing	O
the	O
danger	O
of	O
using	O
the	O
same	O
data	O
that	O
were	O
used	O
to	O
obtain	O
majority	O
votes	O
to	O
estimate	B
the	O
error	O
.	O
an	O
optimistic	O
bias	B
will	O
be	O
introduced	O
.	O
(	O
for	O
more	O
on	O
such	O
error	O
estimates	O
,	O
see	O
chapter	O
23	O
.	O
)	O
let	O
g~	O
be	O
the	O
classifier	B
(	O
or	O
one	O
of	O
the	O
classifiers	O
)	O
in	O
(	O
h	O
for	O
which	O
ln	O
is	O
minimum	O
.	O
assume	O
that	O
19k1	O
<	O
oo-for	O
example	O
,	O
9k	O
could	O
344	O
20.	O
tree	B
classifiers	O
be	O
all	O
k	O
!	O
chronological	B
k-d	O
trees	O
obtained	O
by	O
permuting	O
xl	O
,	O
...	O
,	O
xk	O
.	O
we	O
call	O
g~	O
then	O
the	O
permutation-optimized	B
chronological	I
k-d	O
classifier	B
.	O
when	O
k	O
=	O
l	O
jiognj	O
,	O
one	O
can	O
verify	O
that	O
k	O
!	O
=	O
o	O
(	O
ne	O
)	O
for	O
any	O
e	O
>	O
0	O
,	O
so	O
that	O
the	O
computational	O
burden-at	O
least	O
on	O
paper-is	O
not	O
out	O
of	O
sight	O
.	O
we	O
assume	O
that	O
9k	O
has	O
a	O
consistent	O
classifier	B
sequence	O
,	O
that	O
is	O
,	O
as	O
n	O
-+	O
00	O
,	O
k	O
usually	O
grows	O
unbounded	O
,	O
and	O
e	O
{	O
ln	O
(	O
gk	O
)	O
}	O
-+	O
l*	O
for	O
a	O
sequence	O
gk	O
e	O
9k	O
.	O
example	O
.	O
among	O
the	O
chronological	B
k-d	O
trees	O
modulo	O
permutations	O
,	O
the	O
first	O
one	O
(	O
i.e.	O
,	O
the	O
one	O
corresponding	O
to	O
the	O
identity	O
permutation	O
)	O
was	O
shown	O
to	O
be	O
consistent	O
in	O
theorem	B
20.3	O
,	O
if	O
x	O
has	O
a	O
density	O
,	O
k	O
-+	O
00	O
,	O
and	O
kin	O
-+	O
o	O
.	O
0	O
example	O
.	O
let	O
9k	O
be	O
the	O
class	O
of	O
bsp	O
trees	O
in	O
which	O
we	O
take	O
as	O
possible	O
hyperplanes	O
for	O
splitting	O
the	O
root	O
:	O
(	O
1	O
)	O
(	O
2	O
)	O
(	O
3	O
)	O
the	O
hyperplane	B
through	O
x	O
i	O
,	O
...	O
,	O
x	O
d	O
;	O
the	O
d	O
hyperplanes	O
through	O
x	O
i	O
,	O
...	O
,	O
x	O
d	O
-	O
l	O
that	O
are	O
parallel	O
to	O
one	O
axis	O
;	O
the	O
(	O
~	O
)	O
hyperplanes	O
through	O
x	O
i	O
,	O
...	O
,	O
xd-2	O
that	O
are	O
parallel	O
to	O
two	O
axes	O
;	O
(	O
d	O
)	O
the	O
c~l	O
)	O
=	O
d	O
hyperplanes	O
through	O
xl	O
that	O
are	O
parallel	O
to	O
d	O
-	O
1	O
axes	O
.	O
thus	O
,	O
conservatively	O
estimated	O
,	O
19k	O
i	O
:	O
:	O
:	O
k	O
!	O
2d	O
because	O
there	O
are	O
at	O
most	O
2d	O
possible	O
choices	O
at	O
each	O
node	O
and	O
there	O
are	O
k	O
!	O
permutations	O
of	O
x	O
i	O
,	O
...	O
,	O
x	O
k.	O
granted	O
,	O
the	O
number	O
of	O
external	O
regions	O
is	O
very	O
variable	B
,	O
but	O
it	O
remains	O
bounded	O
by	O
k	O
+	O
1	O
in	O
any	O
case	O
.	O
as	O
9k	O
contains	O
the	O
chronological	B
k-d	O
tree	B
,	O
it	O
has	O
a	O
consistent	O
sequence	O
of	O
classifiers	O
when	O
k	O
-+	O
00	O
and	O
nl	O
(	O
k	O
log	O
k	O
)	O
-+	O
00	O
.	O
0	O
theorem	B
20.8.	O
let	O
gl~	O
=	O
argminge9k	O
ln	O
(	O
g	O
)	O
.	O
then	O
,	O
if9k	O
has	O
a	O
consistent	O
sequence	O
of	O
classifiers	O
,	O
if	O
the	O
number	O
of	O
regions	O
in	O
the	O
partitions	O
for	O
all	O
g	O
e	O
9k	O
are	O
at	O
most	O
k	O
+	O
1	O
,	O
andifk	O
-+	O
00	O
and	O
nl	O
log	O
19k1-+	O
00	O
,	O
then	O
e	O
{	O
ln	O
(	O
g~	O
)	O
}	O
-+	O
l*	O
,	O
where	O
ln	O
(	O
g~	O
)	O
is	O
the	O
conditional	O
probability	O
of	O
error	O
of	O
g~	O
.	O
in	O
the	O
examples	O
cited	O
above	O
,	O
we	O
must	O
take	O
k	O
-+	O
00	O
,	O
n	O
i	O
k	O
-+	O
00.	O
furthermore	O
,	O
log	O
19k	O
i	O
=	O
o	O
(	O
log	O
kl	O
)	O
=	O
o	O
(	O
k	O
log	O
k	O
)	O
in	O
both	O
cases	O
.	O
thus	O
,	O
gl~	O
is	O
consistent	O
whenever	O
x	O
has	O
a	O
density	O
,	O
k	O
-+	O
00	O
and	O
k	O
=	O
o	O
(	O
nl	O
log	O
n	O
)	O
.	O
this	O
is	O
a	O
simple	O
way	O
of	O
constructing	O
a	O
basically	O
universally	O
consistent	O
bsp	O
tree	B
.	O
proof	O
.	O
let	O
g+	O
=	O
arg	O
minge9k	O
ln	O
(	O
g	O
)	O
.	O
ln	O
(	O
g/	O
:	O
)	O
-	O
l	O
*	O
=	O
ln	O
(	O
g~	O
)	O
-	O
ln	O
(	O
g~	O
)	O
+	O
ln	O
(	O
g~	O
)	O
-	O
~l	O
(	O
g+	O
)	O
+	O
ln	O
(	O
g+	O
)	O
-	O
ln	O
(	O
g+	O
)	O
+ln	O
(	O
g+	O
)	O
-	O
l*	O
.	O
clearly	O
,	O
def	O
i	O
+	O
i	O
i.	O
obviously	O
,	O
i	O
-+	O
0	O
in	O
the	O
mean	O
by	O
our	O
assumption	O
,	O
and	O
for	O
e	O
>	O
0	O
,	O
20.12	O
primitive	O
selection	B
345	O
next	O
we	O
bound	O
the	O
probabilities	O
on	O
the	O
right-hand	O
side	O
.	O
let	O
pw	O
,	O
pil	O
denote	O
p	O
{	O
x	O
e	O
region	O
i	O
,	O
y	O
=	O
o	O
}	O
and	O
p	O
{	O
x	O
e	O
region	O
i	O
,	O
y	O
=	O
1	O
}	O
respectively	O
,	O
with	O
regions	O
determined	O
by	O
g	O
,	O
1	O
:	O
:	O
:	O
:	O
i	O
:	O
:	O
:	O
:	O
k	O
+	O
1.	O
let	O
n	O
nw	O
=	O
l	O
i	O
{	O
xjefegion	O
i	O
,	O
yj=o	O
}	O
j=k+l	O
and	O
nil	O
=	O
l	O
i	O
{	O
xjeregion	O
i	O
,	O
yj=i	O
}	O
'	O
n	O
j=k+l	O
then	O
and	O
thus	O
,	O
ln	O
(	O
g	O
)	O
=	O
l	O
pili	O
{	O
nio~nid	O
+	O
l	O
pwi	O
{	O
nio	O
<	O
nid	O
,	O
k+l	O
k+1	O
i=1	O
i=1	O
iln	O
(	O
g	O
)	O
-	O
ln	O
(	O
g	O
)	O
1	O
:	O
:	O
:	O
:	O
l	O
pil	O
-	O
_i_l	O
+	O
l	O
pig	O
__	O
io_	O
i	O
i	O
.	O
n	O
n	O
-	O
k	O
k+1	O
i	O
i=l	O
k+1	O
i	O
i=l	O
n	O
n	O
-	O
k	O
introduce	O
the	O
notation	O
z	O
for	O
the	O
random	O
variable	B
on	O
the	O
right-hand	O
side	O
of	O
the	O
above	O
inequality	B
.	O
by	O
the	O
cauchy-schwarz	O
inequality	B
,	O
k+l	O
<	O
l	O
i=1	O
k+1	O
+l	O
i=l	O
e	O
{	O
(	O
pio	O
-	O
n~okr	O
ix	O
i	O
,	O
.	O
,	O
xc	O
}	O
(	O
as	O
given	O
xl	O
,	O
''	O
''	O
xb	O
nil	O
is	O
binomial	B
(	O
n	O
-	O
k	O
,	O
pil	O
)	O
)	O
<	O
~	O
(	O
j	O
np~\	O
+	O
j	O
np~ok	O
)	O
<	O
2k+2	O
rn=k	O
(	O
by	O
another	O
use	O
of	O
the	O
cauchy-schwarz	O
inequality	B
)	O
j2k+2	O
n-k	O
346	O
20.	O
tree	B
classifiers	O
thus	O
,	O
e	O
{	O
zixl	O
,	O
...	O
,	O
x	O
k	O
}	O
-+	O
0	O
as	O
kin	O
-+	O
o.	O
note	O
that	O
p	O
{	O
z	O
>	O
eix1	O
,	O
...	O
,	O
xd	O
<	O
p	O
{	O
z	O
-	O
e	O
{	O
zixi	O
,	O
...	O
,	O
xk	O
}	O
>	O
~	O
i	O
xl	O
,	O
...	O
,	O
xk	O
}	O
e	O
(	O
if	O
e	O
{	O
zixi	O
'	O
...	O
,	O
xk	O
}	O
<	O
e/2	O
,	O
which	O
happens	O
when	O
-	O
-	O
:	O
:	O
:	O
-	O
)	O
2	O
j2k+2	O
n	O
-k	O
<	O
exp	O
(	O
-	O
(	O
~	O
)	O
2	O
/2	O
(	O
n	O
_	O
k	O
)	O
2	O
4	O
2	O
)	O
(	O
n	O
-k	O
)	O
(	O
by	O
mcdiarmid	O
's	O
inequality	B
,	O
as	O
changing	O
the	O
value	O
of	O
an	O
x	O
j	O
,	O
j	O
:	O
:	O
:	O
:	O
:	O
k	O
,	O
changes	O
z	O
by	O
at	O
most	O
2/	O
(	O
n	O
-	O
k	O
)	O
;	O
see	O
theorem	B
9.2	O
)	O
=	O
exp	O
(	O
(	O
n	O
-	O
k	O
)	O
e2	O
)	O
32	O
.	O
thus	O
,	O
taking	O
expected	O
values	O
,	O
we	O
see	O
that	O
if	O
j	O
(	O
2k	O
+	O
2	O
)	O
/	O
(	O
n	O
-	O
k	O
)	O
<	O
e/2	O
.	O
this	O
tends	O
to	O
0	O
for	O
all	O
e	O
>	O
0	O
if	O
kin	O
-+	O
0	O
and	O
nllog	O
19k1	O
-+	O
00.0	O
20.13	O
constructing	O
consistent	O
tree	O
classifiers	O
thus	O
far	O
,	O
we	O
have	O
taken	O
you	O
through	O
a	O
forest	O
of	O
beautiful	O
trees	O
and	O
we	O
have	O
shown	O
you	O
a	O
few	O
tricks	O
of	O
the	O
trade	O
.	O
when	O
you	O
read	O
(	O
or	O
write	O
)	O
a	O
research	O
paper	O
on	O
tree	B
classifiers	O
,	O
and	O
try	O
to	O
directly	O
apply	O
a	O
consistency	O
theorem	B
,	O
you	O
will	O
get	O
frustrated	O
however-most	O
real-life	O
tree	B
classifiers	O
use	O
the	O
data	O
in	O
intricate	O
ways	O
to	O
suit	O
a	O
certain	O
application	O
.	O
it	O
really	O
helps	O
to	O
have	O
a	O
few	O
truly	O
general	O
results	O
that	O
have	O
universal	B
impact	O
.	O
in	O
this	O
section	O
we	O
will	O
point	O
you	O
to	O
three	O
different	O
places	O
in	O
the	O
book	O
where	O
you	O
may	O
find	O
useful	O
results	O
in	O
this	O
respect	O
.	O
first	O
of	O
all	O
,	O
there	O
is	O
a	O
consistency	O
theorem-theorem	O
21.2-that	O
applies	O
to	O
rules	O
that	O
partition	B
the	O
space	O
and	O
decide	O
by	O
majority	B
vote	I
.	O
the	O
partition	B
is	O
arbitrary	O
and	O
may	O
thus	O
be	O
generated	O
by	O
using	O
some	O
or	O
all	O
of	O
xl	O
,	O
y1	O
,	O
•••	O
,	O
x	O
n	O
,	O
yn	O
.	O
if	O
a	O
rule	O
satisfies	O
the	O
two	O
(	O
weak	B
)	O
conditions	O
of	O
theorem	O
21.2	O
,	O
it	O
must	O
be	O
universally	O
consistent	O
.	O
to	O
put	O
it	O
differently	O
,	O
even	O
the	O
worst	O
rule	B
within	O
the	O
boundaries	O
of	O
the	O
theorem	B
's	O
conditions	O
must	O
perform	O
well	O
asymptotically	O
.	O
second	O
,	O
we	O
will	O
briefly	O
discuss	O
the	O
design	O
of	O
tree	O
classifiers	O
obtained	O
by	O
mini	O
(	O
cid:173	O
)	O
mizing	O
the	O
empirical	B
error	I
estimate	O
(	O
in	O
)	O
over	O
possibly	O
infinite	O
classes	O
of	O
classifiers	O
.	O
such	O
classifiers	O
,	O
however	O
hard	O
to	O
find	O
by	O
an	O
algorithm	B
,	O
have	O
asymptotic	O
properties	O
that	O
are	O
related	O
to	O
the	O
vc	B
dimension	I
of	O
the	O
class	O
of	O
rules	O
.	O
consistency	B
follows	O
al	O
(	O
cid:173	O
)	O
most	O
without	O
work	O
if	O
one	O
can	O
calculate	O
or	O
bound	O
the	O
vc	B
dimension	I
appropriately	O
.	O
20.13	O
constructing	O
consistent	O
tree	O
classifiers	O
347	O
while	O
chapters	O
13	O
and	O
21	O
deal	O
in	O
more	O
detail	O
with	O
the	O
vc	B
dimension	I
,	O
it	O
is	O
necessary	O
to	O
give	O
a	O
few	O
examples	O
here	O
.	O
third	O
,	O
we	O
point	O
the	O
reader	O
to	O
chapter	O
22	O
on	O
data	O
splitting	O
,	O
where	O
the	O
previous	O
approach	O
is	O
applied	O
to	O
the	O
minimization	O
.	O
of	O
the	O
holdout	B
estimate	O
,	O
obtained	O
by	O
trees	O
based	O
upon	O
part	O
of	O
the	O
sample	O
,	O
and	O
using	O
another	O
part	O
to	O
select	O
the	O
best	O
tree	B
in	O
the	O
bunch	O
.	O
here	O
too	O
the	O
vc	B
dimension	I
plays	O
a	O
crucial	O
role	O
.	O
theorem	B
21.2	O
allows	O
space	O
partitions	O
that	O
depend	O
quite	O
arbitrarily	O
on	O
all	O
the	O
data	O
and	O
extends	O
earlier	O
universally	O
applicable	O
results	O
of	O
gordon	O
and	O
olshen	O
(	O
1978	O
)	O
,	O
(	O
1980	O
)	O
,	O
(	O
1984	O
)	O
,	O
and	O
breiman	O
,	O
friedman	O
,	O
olshen	O
,	O
and	O
stone	O
(	O
1984	O
)	O
.	O
a	O
particularly	O
useful	O
format	O
is	O
given	O
in	O
theorem	O
21.8.	O
if	O
the	O
partition	B
is	O
by	O
recursive	B
hyperplane	O
splits	O
as	O
in	O
bsp	O
trees	O
and	O
the	O
number	O
of	O
splits	O
is	O
at	O
most	O
m	O
n	O
,	O
if	O
mn	O
log	O
n	O
i	O
n	O
-+	O
0	O
,	O
and	O
if	O
diam	O
(	O
a	O
(	O
x	O
)	O
n	O
sb	O
)	O
-+	O
0	O
with	O
probability	O
one	O
for	O
all	O
sb	O
(	O
where	O
sb	O
is	O
the	O
ball	O
of	O
radius	O
b	O
centered	O
at	O
the	O
origin	O
)	O
,	O
then	O
the	O
classification	O
rule	B
is	O
strongly	O
consistent	O
.	O
the	O
last	O
condition	O
forces	O
a	O
randomly	O
picked	O
region	O
in	O
the	O
partition	B
to	O
be	O
small	O
.	O
however	O
,	O
mn	O
log	O
n	O
i	O
n	O
-+	O
0	O
guarantees	O
that	O
no	O
devilish	B
partition	O
can	O
be	O
inconsistent	O
.	O
the	O
latter	O
condition	O
is	O
certainly	O
satisfied	O
if	O
each	O
region	O
contains	O
at	O
least	O
kn	O
points	O
,	O
where	O
knl	O
log	O
n	O
-+	O
00.	O
next	O
we	O
take	O
a	O
look	O
at	O
full-fledged	O
minimization	O
of	O
ln	O
,	O
the	O
empirical	B
error	I
,	O
over	O
certain	O
classes	O
of	O
tree	O
classifiers	O
.	O
here	O
we	O
are	O
not	O
concerned	O
with	O
the	O
(	O
often	O
unacceptable	O
)	O
computational	O
effort	O
.	O
for	O
example	O
,	O
let	O
qk	O
be	O
the	O
class	O
of	O
all	O
binary	B
tree	O
classifiers	O
based	O
upon	O
a	O
tree	O
consisting	O
of	O
k	O
internal	O
nodes	O
,	O
each	O
representing	O
a	O
hyperplane	O
cut	O
(	O
as	O
in	O
the	O
bsp	O
tree	B
)	O
,	O
and	O
all	O
possible	O
2k+l	O
labelings	O
of	O
the	O
k	O
+	O
1	O
leaf	O
regions	O
.	O
pick	O
such	O
a	O
classifier	O
for	O
which	O
is	O
minimal	O
.	O
observe	O
that	O
the	O
chosen	O
tree	B
is	O
always	O
natural	O
;	O
that	O
is	O
,	O
it	O
takes	O
majority	O
votes	O
over	O
the	O
leaf	O
regions	O
.	O
thus	O
,	O
the	O
minimization	O
is	O
equivalent	O
to	O
the	O
minimization	O
of	O
the	O
resubstitution	B
error	O
estimate	B
(	O
defined	O
in	O
chapter	O
23	O
)	O
over	O
the	O
corresponding	O
class	O
of	O
natural	O
tree	O
classifiers	O
.	O
we	O
say	O
that	O
a	O
sequence	O
{	O
qk	O
}	O
of	O
classes	O
is	O
rich	O
if	O
we	O
can	O
find	O
a	O
sequence	O
gk	O
e	O
qk	O
such	O
that	O
l	O
(	O
gk	O
)	O
-+	O
l	O
*	O
.	O
for	O
hyperplanes	O
,	O
this	O
is	O
the	O
case	O
if	O
k	O
-+	O
00	O
as	O
n	O
-+	O
oo-just	O
make	O
the	O
hyperplane	B
cuts	O
form	O
a	O
regular	O
histogram	O
grid	O
and	O
recall	O
theorem	B
9.4.	O
let	O
s	O
(	O
qb	O
n	O
)	O
be	O
the	O
shatter	B
coefficient	I
of	O
qk	O
(	O
for	O
a	O
definition	O
,	O
see	O
chapter	O
12	O
,	O
definition	O
12.3	O
)	O
.	O
for	O
example	O
,	O
for	O
the	O
hyperplane	B
family	O
,	O
s	O
(	O
qk	O
,	O
n	O
)	O
.	O
:	O
s	O
nk	O
(	O
d+l	O
)	O
.	O
then	O
by	O
corollary	O
12.1	O
we	O
have	O
e	O
{	O
ln	O
(	O
g~	O
)	O
}	O
-+	O
l	O
*	O
for	O
the	O
selected	O
classifier	B
g~	O
when	O
qk	O
is	O
rich	O
(	O
k	O
-+	O
00	O
here	O
)	O
and	O
log	O
s	O
(	O
qk	O
,	O
n	O
)	O
=	O
o	O
(	O
n	O
)	O
,	O
that	O
is	O
,	O
k=o	O
-	O
-	O
)	O
logn	O
n	O
(	O
.	O
348	O
20.	O
tree	B
classifiers	O
observe	O
that	O
no	O
conditions	O
are	O
placed	O
on	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
here	O
!	O
consis	O
(	O
cid:173	O
)	O
tency	O
follows	O
from	O
basic	O
notions-one	O
combinatorial	O
to	O
keep	O
us	O
from	O
overfitting	B
,	O
and	O
one	O
approximation-theoretical	O
(	O
the	O
richness	O
)	O
.	O
the	O
above	O
result	O
remains	O
valid	O
under	O
the	O
same	O
conditions	O
on	O
k	O
in	O
the	O
following	O
classes	O
:	O
(	O
1	O
)	O
all	O
trees	O
based	O
upon	O
k	O
internal	O
nodes	O
each	O
representing	O
a	O
perpendicular	O
split	O
.	O
(	O
note	O
:	O
s	O
(	O
~h	O
,	O
n	O
)	O
:	O
:	O
:	O
:	O
(	O
(	O
n	O
+	O
l	O
)	O
d	O
)	O
k.	O
)	O
(	O
2	O
)	O
all	O
trees	O
based	O
upon	O
k	O
internal	O
nodes	O
,	O
each	O
representing	O
a	O
quadtree	O
split	O
.	O
(	O
note	O
:	O
s	O
(	O
(	O
;	O
h	O
,	O
n	O
)	O
:	O
:	O
:	O
:	O
(	O
n	O
d	O
+	O
ll	O
.	O
)	O
20.14	O
a	O
greedy	O
classifier	B
in	O
this	O
section	O
,	O
we	O
define	O
simply	O
a	O
binary	O
tree	B
classifier	O
that	O
is	O
grown	O
via	O
optimiza	O
(	O
cid:173	O
)	O
tion	O
of	O
a	O
simple	O
criterion	O
.	O
it	O
has	O
the	O
remarkable	O
property	O
that	O
it	O
does	O
not	O
require	O
a	O
forced	O
rotation	B
through	O
the	O
coordinate	O
axes	O
or	O
special	O
safeguards	O
against	O
small	O
or	O
large	O
regions	O
or	O
the	O
like	O
.	O
it	O
remains	O
entirely	O
parameter-free	O
(	O
nothing	O
is	O
picked	O
by	O
the	O
user	O
)	O
,	O
is	O
monotone	O
transformation	O
invariant	O
,	O
and	O
fully	O
automatic	B
.	O
we	O
show	O
that	O
in	O
nd	O
it	O
is	O
always	O
consistent	O
.	O
it	O
serves	O
as	O
a	O
prototype	O
for	O
teaching	O
about	O
such	O
rules	O
and	O
should	O
not	O
be	O
considered	O
as	O
more	O
than	O
that	O
.	O
for	O
fully	O
practical	O
methods	O
,	O
we	O
believe	O
,	O
one	O
will	O
have	O
to	O
tinker	O
with	O
the	O
approach	O
.	O
the	O
space	O
is	O
partitioned	O
into	O
rectangles	O
as	O
shown	O
below	O
:	O
-	O
cj	O
3	O
6rn	O
5	O
figure	O
20.22.	O
a	O
tree	O
based	O
on	O
partitioning	O
the	O
plane	O
into	O
rectangles	O
.	O
the	O
right	O
subtree	O
of	O
each	O
internal	O
node	O
belongs	O
to	O
the	O
inside	O
of	O
a	O
rectangle	O
,	O
and	O
the	O
left	O
subtree	O
belongs	O
to	O
the	O
complement	O
of	O
the	O
same	O
rectangle	O
(	O
ic	O
denotes	O
the	O
complement	O
of	O
i	O
)	O
.	O
rectangles	O
are	O
not	O
allowed	O
to	O
overlap	O
.	O
a	O
hyperrectangle	O
defines	O
a	O
split	O
in	O
a	O
natural	O
way	O
.	O
the	O
theory	O
presented	O
here	O
applies	O
for	O
many	O
other	O
types	O
of	O
cuts	O
.	O
these	O
will	O
be	O
discussed	O
after	O
the	O
main	O
con	O
(	O
cid:173	O
)	O
sistency	O
theorem	B
is	O
stated	O
.	O
20.14	O
a	O
greedy	O
classifier	B
349	O
a	O
partition	O
is	O
denoted	O
by	O
p	O
,	O
and	O
a	O
decision	O
on	O
a	O
set	O
a	O
e	O
p	O
is	O
by	O
majority	B
vote	I
.	O
we	O
write	O
gp	O
for	O
such	O
a	O
rule	O
:	O
gp	O
(	O
x	O
)	O
=	O
{	O
1	O
0	O
if	O
l	O
yi	O
>	O
l	O
(	O
1	O
-	O
yi	O
)	O
,	O
x	O
e	O
a	O
i	O
:	O
x	O
;	O
ea	O
otherwise	O
.	O
i	O
:	O
x	O
;	O
ea	O
given	O
a	O
partition	O
p	O
,	O
a	O
legal	O
rectangle	O
a	O
is	O
one	O
for	O
which	O
a	O
n	O
b	O
=	O
0	O
or	O
a	O
s	O
;	O
b	O
for	O
all	O
sets	O
b	O
e	O
p.	O
if	O
we	O
refine	O
p	O
by	O
adding	O
a	O
legal	O
rectangle	O
t	O
somewhere	O
,	O
then	O
we	O
obtain	O
the	O
partition	B
t.	O
the	O
decision	O
gy	O
agrees	O
with	O
gp	O
except	O
on	O
the	O
set	O
b	O
e	O
p	O
that	O
contains	O
t.	O
we	O
introduce	O
the	O
convenient	O
notation	O
=	O
p	O
{	O
x	O
e	O
a	O
,	O
y	O
=	O
}	O
}	O
,	O
j	O
e	O
{	O
a	O
,	O
i	O
}	O
,	O
an	O
estimate	B
of	O
the	O
quality	O
of	O
gp	O
is	O
where	O
1	O
n	O
-	O
l	O
i	O
{	O
x	O
;	O
er	O
,	O
gp	O
(	O
xj=	O
!	O
y	O
;	O
}	O
=	O
min	O
(	O
vo	O
,	O
n	O
(	O
r	O
)	O
,	O
vi	O
,	O
n	O
(	O
r	O
»	O
.	O
n	O
i=i	O
here	O
we	O
use	O
two	O
different	O
arguments	O
for	O
ln	O
(	O
r	O
and	O
p	O
)	O
,	O
but	O
the	O
distinction	O
should	O
be	O
clear	O
.	O
we	O
may	O
similarly	O
define	O
ln	O
(	O
t	O
)	O
.	O
given	O
a	O
partition	O
p	O
,	O
the	O
greedy	B
classifier	O
selects	O
that	O
legal	O
rectangle	O
t	O
for	O
which	O
ln	O
(	O
t	O
)	O
is	O
minimal	O
(	O
with	O
any	O
appropriate	O
policy	O
for	O
breaking	O
ties	O
)	O
.	O
let	O
r	O
be	O
the	O
set	O
of	O
p	O
containing	O
t.	O
then	O
the	O
greedy	B
classifier	O
picks	O
that	O
t	O
for	O
which	O
is	O
minimal	O
.	O
starting	O
with	O
the	O
trivial	O
partition	B
po	O
=	O
{	O
n	O
d	O
}	O
,	O
we	O
repeat	O
the	O
previous	O
step	O
k	O
times	O
,	O
leading	O
thus	O
to	O
k	O
+	O
1	O
regions	O
.	O
the	O
sequence	O
of	O
partitions	O
is	O
denoted	O
by	O
po	O
,	O
pi	O
,	O
...	O
,	O
pk	O
.	O
we	O
put	O
no	O
safeguards	O
in	O
place-the	O
rectangles	O
are	O
not	O
forced	O
to	O
shrink	O
.	O
and	O
in	O
fact	O
,	O
it	O
is	O
easy	O
to	O
construct	O
examples	O
in	O
which	O
most	O
rectangles	O
do	O
not	O
shrink	O
.	O
the	O
main	O
result	O
of	O
the	O
section	O
,	O
and	O
indeed	O
of	O
this	O
chapter	O
,	O
is	O
that	O
the	O
obtained	O
classifier	B
is	O
consistent	O
:	O
theorem	B
20.9.	O
for	O
the	O
greedy	B
classifier	O
with	O
k	O
-+	O
00	O
and	O
k	O
=	O
0	O
(	O
j	O
n	O
/	O
log	O
n	O
)	O
,	O
assuming	O
that	O
x	O
has	O
nonatomic	O
marginals	O
,	O
we	O
have	O
ln	O
-+	O
l	O
*	O
with	O
probability	O
one	O
.	O
350	O
20.	O
tree	B
classifiers	O
remark	O
.	O
we	O
note	O
that	O
with	O
techniques	O
presented	O
in	O
the	O
next	O
chapter	O
,	O
it	O
is	O
possible	O
to	O
improve	O
the	O
second	O
condition	O
on	O
k	O
to	O
k	O
==	O
0	O
(	O
vi	O
n	O
/	O
log	O
n	O
)	O
(	O
see	O
problem	O
20.31	O
)	O
.	O
o	O
before	O
proving	O
the	O
theorem	B
,	O
we	O
mention	O
that	O
the	O
same	O
argument	O
may	O
be	O
used	O
to	O
establish	O
consistency	B
of	O
greedily	O
grown	O
trees	O
with	O
many	O
other	O
types	O
of	O
cuts	O
.	O
we	O
have	O
seen	O
in	O
section	O
20.8	O
that	O
repeated	O
stoller	O
splits	O
do	O
not	O
result	O
in	O
good	O
classifiers	O
.	O
the	O
reason	O
is	O
that	O
optimization	O
is	O
over	O
a	O
collection	O
of	O
sets	O
(	O
halfspaces	O
)	O
that	O
is	O
not	O
guaranteed	O
to	O
improve	O
matters-witness	O
the	O
examples	O
provided	O
in	O
previous	O
sections	O
.	O
a	O
good	O
cutting	O
method	O
is	O
one	O
that	O
includes	O
somehow	O
many	O
(	O
but	O
not	O
too	O
many	O
)	O
small	O
sets	O
.	O
for	O
example	O
,	O
let	O
us	O
split	O
at	O
the	O
root	O
making	O
d	O
+	O
1	O
hyperplane	B
cuts	O
at	O
once	O
,	O
that	O
is	O
,	O
by	O
finding	O
the	O
d	O
+	O
1	O
cuts	O
that	O
together	O
produce	O
the	O
largest	O
decrease	O
in	O
the	O
empirical	B
error	I
probability	O
.	O
then	O
repeat	O
this	O
step	O
recursively	O
in	O
each	O
region	O
k	O
times	O
.	O
the	O
procedure	O
is	O
consistent	O
under	O
the	O
same	O
conditions	O
on	O
k	O
as	O
in	O
theorem	O
6.1	O
,	O
whenever	O
x	O
has	O
a	O
density	O
(	O
see	O
problem	O
20.30	O
)	O
.	O
the	O
d	O
+	O
1	O
hyperplane	B
cuts	O
may	O
be	O
considered	O
as	O
an	O
elementary	B
cut	I
which	O
is	O
repeated	O
in	O
a	O
greedy	B
manner	O
.	O
in	O
figures	O
20.23	O
to	O
20.25	O
we	O
show	O
a	O
few	O
elementary	O
cuts	O
that	O
may	O
be	O
repeated	O
greedily	O
for	O
a	O
consistent	O
classifier	B
.	O
the	O
straightforward	O
proofs	O
of	O
consistency	O
are	O
left	O
to	O
the	O
reader	O
in	O
problem	O
20.30.	O
proof	O
of	O
theorem	O
20.9.	O
we	O
restrict	O
ourselves	O
to	O
n	O
2	O
,	O
but	O
the	O
proof	O
remains	O
similar	O
in	O
n	O
d	O
(	O
problem	O
20.29	O
)	O
.	O
the	O
notation	O
ln	O
(	O
·	O
)	O
was	O
introduced	O
above	O
,	O
where	O
the	O
argument	O
is	O
allowed	O
to	O
be	O
a	O
partition	O
or	O
a	O
set	O
in	O
a	O
partition	B
.	O
we	O
similarly	O
define	O
l	O
(	O
r	O
)	O
min	O
p	O
{	O
x	O
e	O
r	O
,	O
y	O
iy	O
}	O
ye	O
{	O
o.i	O
}	O
l	O
(	O
p	O
)	O
=	O
min	O
(	O
vo	O
(	O
r	O
)	O
,	O
vi	O
(	O
r	O
)	O
)	O
,	O
l	O
l	O
(	O
r	O
)	O
.	O
rep	O
figure	O
20.23.	O
an	O
elementary	B
cut	I
here	O
is	O
composed	O
of	O
d	O
+	O
1	O
hyperplane	B
cuts	O
.	O
they	O
are	O
jointly	O
optimized	O
.	O
figure	O
20.24	O
.	O
2d	O
rectangular	O
cuts	O
de-	O
termine	O
an	O
elementary	B
cut	I
.	O
all	O
2d	O
cuts	O
,	O
have	O
arbitrary	O
directions	O
;	O
there	O
are	O
no	O
forced	O
directions	O
.	O
20.14	O
a	O
greedy	O
classifier	B
351	O
2	O
1	O
figure	O
20.25.	O
simplex	O
cuts	O
.	O
a	O
cut	O
is	O
determined	O
by	O
a	O
polyhedron	O
with	O
d	O
+	O
1	O
vertices	O
.	O
the	O
simplices	O
are	O
not	O
allowed	O
to	O
overlap	O
,	O
just	O
as	O
for	O
rectangular	O
cuts	O
in	O
the	O
greedy	B
classifier	O
.	O
three	O
consecu-	O
tive	O
simplex	O
cuts	O
in	O
r2	O
are	O
shown	O
here	O
.	O
mal	O
.	O
two	O
consecutive	O
rectangular	O
grid	O
figure	O
20.26.	O
at	O
every	O
iteration	O
we	O
re	O
(	O
cid:173	O
)	O
fine	O
the	O
partition	B
by	O
selecting	O
that	O
rect	O
(	O
cid:173	O
)	O
angle	O
r	O
in	O
the	O
partition	B
and	O
that	O
3	O
x	O
3	O
x	O
...	O
x	O
3	O
rectangular	O
grid	O
cut	O
of	O
r	O
for	O
which	O
the	O
empirical	B
error	I
is	O
mini	O
(	O
cid:173	O
)	O
cuts	O
are	O
shown	O
.	O
for	O
example	O
,	O
let	O
(	O
;	O
h	O
denote	O
a	O
partition	O
of	O
r	O
2	O
into	O
a	O
rectangular	O
l	O
x	O
i	O
grid	O
.	O
figure	O
20.27	O
.	O
91	O
:	O
an	O
i	O
x	O
i	O
grid	O
(	O
with	O
i	O
=	O
7	O
here	O
)	O
.	O
it	O
is	O
clear	O
that	O
for	O
all	O
e	O
>	O
0	O
,	O
there	O
exists	O
an	O
i	O
=	O
i	O
(	O
e	O
)	O
and	O
an	O
i	O
x	O
i	O
grid	O
91	O
such	O
that	O
if	O
q	O
is	O
another	O
finer	O
partition	B
into	O
rectangles	O
(	O
i.e.	O
,	O
each	O
set	O
of	O
q	O
is	O
a	O
rectangle	O
and	O
intersects	O
at	O
most	O
one	O
rectangle	O
of	O
91	O
)	O
,	O
then	O
necessarily	O
l	O
(	O
q	O
)	O
:	O
:	O
:	O
:	O
l	O
(	O
91	O
)	O
:	O
:	O
:	O
:	O
l*	O
+e	O
.	O
we	O
will	O
call	O
q	O
a	O
refinement	O
of	O
91.	O
the	O
next	O
lemma	O
is	O
a	O
key	O
property	O
of	O
partitioning	O
classifiers	O
.	O
in	O
our	O
eyes	O
,	O
it	O
is	O
the	O
main	O
technical	O
property	O
of	O
this	O
entire	O
chapter	O
.	O
we	O
say	O
that	O
the	O
partition	B
t	O
is	O
an	O
extension	O
of	O
p	O
by	O
a	O
set	O
q-where	O
q	O
~	O
rep-if	O
t	O
contains	O
all	O
cells	O
of	O
p	O
other	O
than	O
r	O
,	O
plus	O
q	O
and	O
r	O
-	O
q	O
.	O
352	O
20..	O
tree	B
classifiers	O
lemma	O
20.2.	O
let	O
oz	O
be	O
a	O
finite	O
partition	B
with	O
l	O
(	O
oz	O
)	O
:	O
:	O
:	O
:	O
l	O
*	O
+	O
e.	O
let	O
p	O
be	O
a	O
finite	O
partition	B
oird	O
,	O
and	O
let	O
q	O
be	O
a	O
refinement	O
oiboth	O
p	O
and	O
oz	O
.	O
then	O
there	O
exists	O
a	O
set	O
q	O
e	O
q	O
(	O
note	O
:	O
q	O
is	O
contained	O
in	O
one	O
set	O
oip	O
only	O
)	O
and	O
an	O
extension	O
oip	O
by	O
q	O
to	O
tq	O
such	O
that	O
,	O
if	O
l	O
(	O
p	O
)	O
:	O
:	O
:	O
l	O
*	O
+	O
e	O
,	O
proof	O
.	O
first	O
fix	O
rep	O
and	O
let	O
q	O
1	O
,	O
...	O
,	O
q	O
n	O
be	O
the	O
sets	O
of	O
q	O
contained	O
in	O
r.	O
define	O
i=l	O
l	O
(	O
r	O
)	O
=	O
min	O
(	O
p	O
,	O
q	O
)	O
,	O
first	O
we	O
show	O
that	O
there	O
exists	O
an	O
integer	O
i	O
such	O
that	O
l	O
(	O
r	O
)	O
-	O
l	O
(	O
q	O
,	O
)	O
-	O
l	O
(	O
r	O
-	O
q	O
,	O
)	O
``	O
:	O
l	O
(	O
r	O
)	O
-	O
''	O
7	O
;	O
'	O
:	O
[	O
l	O
(	O
q	O
,	O
)	O
,	O
or	O
equivalently	O
,	O
where	O
il	O
i	O
=	O
min	O
(	O
p	O
,	O
q	O
)	O
-	O
min	O
(	O
pi	O
,	O
qi	O
)	O
-	O
min	O
(	O
p	O
-	O
pi	O
,	O
q	O
-	O
qi	O
)	O
.	O
to	O
see	O
this	O
,	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
p	O
:	O
:	O
:	O
:	O
q.	O
if	O
pi	O
:	O
:	O
:	O
:	O
qi	O
for	O
all	O
i	O
,	O
then	O
min	O
(	O
p	O
,	O
q	O
)	O
-	O
i	O
:	O
min	O
(	O
pi	O
,	O
qi	O
)	O
=	O
p	O
-	O
i	O
:	O
pi	O
=	O
0	O
,	O
n	O
n	O
l=l	O
i=l	O
so	O
we	O
are	O
done	O
.	O
assume	O
therefore	O
that	O
pi	O
>	O
qi	O
for	O
i	O
e	O
a	O
,	O
where	O
a	O
is	O
a	O
set	O
of	O
indices	O
with	O
i	O
a	O
i	O
:	O
:	O
:	O
1.	O
for	O
such	O
i	O
,	O
and	O
thus	O
,	O
i	O
:	O
ili	O
=	O
i	O
:	O
(	O
pi	O
-	O
qj	O
=	O
p	O
-	O
i	O
:	O
pi	O
-	O
i	O
:	O
qi	O
=	O
p	O
-	O
lmin	O
(	O
pi	O
'	O
qj	O
.	O
n	O
iea	O
lea	O
i¢.a	O
iea	O
i=l	O
but	O
then	O
,	O
ail	O
a	O
iai	O
iea	O
i	O
max	O
u	O
'	O
>	O
-	O
lsisn	O
-	O
u	O
'	O
>	O
i	O
-	O
p	O
-	O
l~l	O
min	O
(	O
pi	O
,	O
qi	O
)	O
iai	O
p	O
-	O
l~l	O
min	O
(	O
pi	O
'	O
qi	O
)	O
>	O
--	O
--	O
'-	O
...	O
:	O
...	O
--	O
--	O
-	O
-	O
'	O
n	O
and	O
the	O
claim	O
follows	O
.	O
to	O
prove	O
the	O
lemma	O
,	O
notice	O
that	O
since	O
l	O
(	O
q	O
)	O
:	O
:	O
:	O
l	O
*	O
+	O
e	O
,	O
it	O
suffices	O
to	O
show	O
that	O
20.14	O
a	O
greedy	O
classifier	B
353	O
max	O
(	O
l	O
(	O
p	O
)	O
-	O
l	O
(	O
tq	O
»	O
:	O
:	O
:	O
qeq	O
l	O
(	O
p	O
)	O
-	O
l	O
(	O
q	O
)	O
'	O
q	O
i	O
i	O
or	O
equivalently	O
,	O
that	O
max	O
(	O
l	O
(	O
r	O
q	O
)	O
-	O
l	O
(	O
q	O
)	O
-	O
l	O
(	O
r	O
q	O
-	O
q	O
»	O
qeq	O
:	O
:	O
:	O
l	O
(	O
p	O
)	O
-	O
l	O
(	O
q	O
)	O
'	O
q	O
i	O
i	O
where	O
rq	O
is	O
the	O
unique	O
cell	O
of	O
p	O
containing	O
q.	O
however	O
,	O
max	O
(	O
l	O
(	O
r	O
q	O
)	O
-	O
l	O
(	O
q	O
)	O
-	O
l	O
(	O
r	O
q	O
-	O
q	O
»	O
qeq	O
>	O
max	O
(	O
l	O
(	O
r	O
)	O
-	O
l	O
(	O
q	O
)	O
-	O
l	O
(	O
r	O
-	O
q	O
»	O
rep	O
,	O
q	O
<	O
:	O
;	O
r	O
>	O
>	O
=	O
l	O
(	O
r	O
)	O
-	O
lqcr	O
l	O
(	O
q	O
)	O
-	O
iri	O
max	O
rep	O
(	O
by	O
the	O
inequality	B
shown	O
above	O
,	O
where	O
i	O
r	O
i	O
denotes	O
the	O
number	O
of	O
sets	O
of	O
q	O
contained	O
in	O
r	O
)	O
lrep	O
(	O
l	O
(	O
r	O
)	O
-	O
lq	O
<	O
:	O
;	O
r	O
l	O
(	O
q	O
»	O
)	O
lrep	O
iri	O
lrep	O
l	O
(	O
r	O
)	O
-	O
lqeq	O
l	O
(	O
q	O
)	O
iqi	O
l	O
(	O
p	O
)	O
-	O
l	O
(	O
q	O
)	O
iqi	O
and	O
the	O
lemma	O
is	O
proved	O
.	O
0	O
let	O
us	O
return	O
to	O
the	O
proof	O
of	O
theorem	O
20.9.	O
the	O
previous	O
lemma	O
applied	O
to	O
our	O
situation	O
(	O
p	O
=	O
pi	O
)	O
shows	O
that	O
we	O
may	O
extend	O
pi	O
by	O
a	O
rectangle	O
q	O
e	O
q	O
(	O
as	O
q	O
will	O
(	O
l	O
*	O
+	O
e	O
)	O
is	O
be	O
a	O
collection	O
of	O
rectangles	O
refining	O
both	O
qz	O
and	O
pi	O
)	O
such	O
that	O
l	O
(	O
tq	O
)	O
-	O
smaller	O
by	O
a	O
guaranteed	O
amount	O
than	O
l	O
(	O
pi	O
)	O
-	O
(	O
l	O
*	O
+	O
e	O
)	O
.	O
(	O
tq	O
is	O
the	O
partition	B
obtained	O
by	O
extending	O
pi	O
.	O
)	O
to	O
describe	O
q	O
,	O
we	O
do	O
the	O
following	O
(	O
note	O
that	O
q	O
must	O
consist	O
entirely	O
of	O
rectangles	O
for	O
otherwise	O
the	O
lemma	O
is	O
useless	O
)	O
:	O
take	O
the	O
rectangles	O
of	O
pi	O
and	O
extend	O
all	O
four	O
sides	O
(	O
in	O
the	O
order	O
of	O
birth	O
of	O
the	O
rectangles	O
)	O
until	O
they	O
hit	O
a	O
side	O
of	O
another	O
rectangle	O
or	O
an	O
extended	O
border	O
of	O
a	O
previous	O
rectangle	O
if	O
they	O
hit	O
anything	O
at	O
all	O
.	O
figure	O
20.28	O
illustrates	O
the	O
partition	B
into	O
rectangles	O
.	O
note	O
that	O
this	O
partition	B
consists	O
of	O
4i	O
line	O
segments	O
,	O
and	O
at	O
most	O
9i	O
rectangles	O
(	O
this	O
can	O
be	O
seen	O
by	O
noting	O
that	O
we	O
can	O
write	O
in	O
each	O
non	O
original	O
rectangle	O
of	O
pi	O
the	O
number	O
of	O
an	O
original	O
neighboring	O
rectangle	O
,	O
each	O
number	O
appearing	O
at	O
most	O
8	O
times	O
)	O
.	O
354	O
20.	O
tree	B
classifiers	O
figure	O
20.28.	O
extensions	O
of	O
rectangles	O
to	O
get	O
a	O
rectangular	O
grid	O
.	O
the	O
rectangular	O
grid	O
partition	O
thus	O
obtained	O
is	O
intersected	O
with	O
91	O
to	O
yield	O
q.	O
to	O
apply	O
lemma	O
20.2	O
,	O
we	O
only	O
need	O
a	O
bound	O
on	O
i	O
qi	O
.	O
to	O
the	O
rectangular	O
partition	B
just	O
created	O
,	O
we	O
add	O
each	O
of	O
the	O
2l	O
-	O
2	O
lines	O
of	O
the	O
grid	O
91	O
one	O
by	O
one	O
.	O
start	O
with	O
the	O
horizontal	O
lines	O
.	O
each	O
time	O
one	O
is	O
added	O
,	O
it	O
creates	O
at	O
most	O
9i	O
new	O
rectangles	O
.	O
then	O
,	O
each	O
vertical	O
line	O
adds	O
at	O
most	O
9i	O
+	O
1	O
new	O
rectangles	O
for	O
a	O
total	O
of	O
not	O
more	O
than	O
9i	O
+	O
9i	O
(	O
l-	O
1	O
)	O
+	O
(	O
l	O
-	O
1	O
)	O
(	O
9i	O
+	O
l	O
)	O
:	O
s	O
l2	O
+	O
18li	O
(	O
assuming	O
1	O
:	O
:	O
:	O
:	O
1	O
)	O
.	O
hence	O
,	O
i	O
qi	O
:	O
s	O
l2	O
+	O
18li	O
.	O
apply	O
the	O
lemma	O
,	O
and	O
observe	O
that	O
there	O
is	O
a	O
rectangle	O
q	O
e	O
q	O
such	O
that	O
the	O
extension	O
of	O
pi	O
by	O
q	O
to	O
p	O
;	O
would	O
yield	O
at	O
this	O
point	O
in	O
the	O
proof	O
,	O
the	O
reader	O
can	O
safely	O
forget	O
q.	O
it	O
has	O
done	O
what	O
it	O
was	O
supposed	O
to	O
do	O
.	O
of	O
course	O
,	O
we	O
can	O
not	O
hope	O
to	O
find	O
p	O
[	O
because	O
we	O
do	O
not	O
know	O
the	O
distribution	B
.	O
let	O
us	O
denote	O
the	O
actual	O
new	O
partition	B
by	O
pi+1	O
and	O
let	O
ri+l	O
be	O
the	O
selected	O
rectangle	O
by	O
the	O
empirical	B
minimization	O
.	O
observe	O
that	O
l	O
(	O
pi+d	O
-	O
l	O
(	O
p	O
;	O
)	O
(	O
l	O
(	O
pi+d	O
-	O
ln	O
(	O
pi+d	O
)	O
+	O
(	O
ln	O
(	O
pi+1	O
)	O
-	O
ln	O
(	O
p	O
;	O
)	O
)	O
+	O
(	O
ln	O
(	O
p	O
;	O
)	O
-	O
l	O
(	O
p	O
;	O
)	O
)	O
(	O
l	O
(	O
pi+l	O
)	O
-	O
ln	O
(	O
pi+1	O
)	O
)	O
+	O
(	O
ln	O
(	O
p	O
[	O
)	O
-	O
l	O
(	O
p	O
;	O
)	O
)	O
.	O
<	O
as	O
pi	O
+1	O
and	O
n	O
;	O
have	O
most	O
sets	O
in	O
comm~	O
,	O
many	O
terms	O
cancel	O
in	O
the	O
last	O
dou	O
(	O
cid:173	O
)	O
ble	O
sum	O
.	O
we	O
are	O
left	O
with	O
just	O
l	O
(	O
·	O
)	O
and	O
ln	O
(	O
.	O
)	O
terms	O
for	O
sets	O
of	O
the	O
form	O
r	O
-	O
ri+1	O
,	O
ri+l	O
,	O
r	O
-	O
q	O
,	O
q	O
with	O
r	O
e	O
pi	O
.	O
thus	O
,	O
l	O
(	O
pi+1	O
)	O
-	O
l	O
(	O
p	O
;	O
)	O
:	O
s	O
2	O
sup	O
iln	O
(	O
r	O
'	O
)	O
-	O
l	O
(	O
r	O
'	O
)	O
i	O
,	O
r	O
'	O
where	O
the	O
supremum	O
is	O
taken	O
over	O
all	O
rectangles	O
of	O
the	O
above	O
described	O
form	O
.	O
these	O
are	O
sets	O
of	O
the	O
form	O
obtainable	O
in	O
pi	O
+1•	O
every	O
such	O
set	O
can	O
be	O
written	O
as	O
the	O
difference	O
of	O
a	O
(	O
possibly	O
infinite	O
)	O
rectangle	O
and	O
at	O
most	O
i	O
+	O
1	O
nonoverlapping	O
contained	O
rectangles	O
.	O
as	O
i	O
+	O
1	O
:	O
s	O
k	O
in	O
our	O
analysis	O
,	O
we	O
call	O
zk	O
the	O
class	O
of	O
all	O
sets	O
20.14	O
a	O
greedy	O
classifier	B
355	O
...	O
-	O
zk	O
,	O
where	O
zo	O
,	O
zl	O
,	O
...	O
,	O
zk	O
that	O
may	O
be	O
described	O
in	O
this	O
form	O
:	O
zo	O
-	O
zl	O
-	O
are	O
rectangles	O
,	O
each	O
zi	O
~	O
zo	O
,	O
zl	O
,	O
...	O
,	O
zk	O
are	O
mutually	O
disjoint	O
.	O
rectangles	O
may	O
be	O
infinite	O
or	O
half	O
infinite	O
.	O
hence	O
,	O
for	O
i	O
<	O
k	O
,	O
l	O
(	O
pi+1	O
)	O
-	O
l	O
(	O
f	O
)	O
:	O
:	O
:	O
;	O
2	O
sup	O
iln	O
(	O
z	O
)	O
-	O
l	O
(	O
z	O
)	O
i.	O
zezk	O
fof	O
a	O
fixed	O
z	O
e	O
zk	O
,	O
l£n	O
(	O
z	O
)	O
-	O
l	O
(	O
z	O
)	O
i	O
1	O
min	O
(	O
vo	O
,	O
n	O
(	O
z	O
)	O
,	O
v1	O
,	O
n	O
(	O
z	O
»	O
min	O
(	O
vo	O
(	O
z	O
)	O
,	O
v1	O
(	O
z	O
»	O
1	O
<	O
ivo	O
,	O
n	O
(	O
z	O
)	O
-	O
vo	O
(	O
z	O
)	O
1	O
+	O
iv1	O
,	O
n	O
(	O
z	O
)	O
-	O
v1	O
(	O
z	O
)	O
i·	O
thus	O
,	O
l	O
(	O
pi+1	O
)	O
-	O
l	O
(	O
j	O
:	O
:	O
:	O
;	O
vn	O
=	O
2	O
sup	O
p	O
'	O
def	O
(	O
ivo	O
,	O
n	O
(	O
z	O
)	O
-	O
vo	O
(	O
z	O
)	O
1	O
+	O
iv1	O
,	O
n	O
(	O
z	O
)	O
-	O
v1	O
(	O
z	O
)	O
1	O
)	O
.	O
define	O
the	O
(	O
good	O
)	O
event	O
zezk	O
g	O
=	O
{	O
vn	O
<	O
8	O
}	O
,	O
where	O
8	O
>	O
0	O
will	O
be	O
picked	O
later	O
.	O
on	O
g	O
,	O
we	O
see	O
that	O
for	O
all	O
i	O
<	O
k	O
,	O
l	O
(	O
pi+1	O
)	O
-	O
(	O
l	O
*	O
+	O
e	O
)	O
<	O
l	O
(	O
p	O
;	O
)	O
-	O
(	O
l	O
*	O
+	O
e	O
)	O
+	O
8	O
(	O
l	O
(	O
pi	O
)	O
-	O
(	O
l	O
*	O
+	O
e	O
»	O
)	O
(	O
1	O
<	O
2	O
1	O
)	O
+	O
8	O
l	O
+	O
18h	O
(	O
lemma	O
20.2	O
)	O
.	O
we	O
now	O
introduce	O
a	O
convenient	O
lemma	O
.	O
lemma	O
20.3.	O
let	O
an	O
,	O
bn	O
be	O
sequences	O
of	O
positive	O
numbers	O
with	O
bn	O
t	O
0	O
,	O
bo	O
<	O
1	O
,	O
and	O
let	O
8	O
>	O
0	O
be	O
fixed	O
.	O
if	O
an+1	O
:	O
:	O
:	O
;	O
an	O
(	O
1	O
-	O
bn	O
)	O
+	O
8	O
,	O
then	O
proof	O
.	O
we	O
have	O
an+	O
!	O
:	O
'0	O
an+1	O
:	O
:	O
:	O
;	O
aoe-ej=ob	O
j	O
+8	O
(	O
n	O
+	O
1	O
)	O
.	O
ao	O
0	O
(	O
1-	O
bj	O
)	O
+	O
8	O
(	O
~d	O
(	O
l	O
-bj	O
)	O
)	O
+	O
8	O
i	O
+	O
i	O
i	O
+	O
i	O
i	O
i.	O
clearly	O
,	O
i	O
:	O
:	O
:	O
;	O
ao	O
exp	O
(	O
-	O
l~=o	O
b	O
j	O
)	O
.	O
a	O
trivial	O
bound	O
for	O
i	O
i	O
+	O
i	O
i	O
i	O
is	O
8	O
(	O
n	O
+	O
1	O
)	O
.	O
0	O
conclude	O
that	O
on	O
g	O
,	O
e	O
1/	O
(	O
l2+1slu	O
)	O
du	O
+	O
8k	O
<	O
<	O
e-	O
fs71og	O
(	O
l2+1sl	O
(	O
k-1	O
»	O
/	O
(	O
l2	O
»	O
+	O
8k	O
1	O
--	O
--	O
--	O
:	O
-	O
:	O
-	O
:	O
-	O
:	O
-	O
:	O
:-	O
:	O
-	O
:	O
-	O
+	O
8k	O
.	O
1+-l-	O
ls	O
(	O
k-1	O
»	O
)	O
l/	O
(	O
lsl	O
)	O
(	O
356	O
20.	O
tree	B
classifiers	O
thus	O
,	O
p	O
{	O
l	O
(	O
pk	O
)	O
>	O
l	O
*	O
+	O
2e	O
}	O
:	O
:	O
:	O
s	O
p	O
{	O
ge	O
}	O
==	O
p	O
{	O
v,1	O
:	O
:	O
:	O
o	O
}	O
when	O
ok	O
<	O
e/2	O
and	O
1	O
e	O
--	O
--	O
--	O
-	O
:	O
-	O
:	O
-	O
:	O
:-	O
:	O
-	O
:	O
:	O
:	O
-	O
:	O
:-	O
<	O
-	O
.	O
2	O
l8	O
(	O
k_l	O
)	O
)	O
1/	O
(	O
l8/	O
)	O
1+-/-	O
(	O
finally	O
,	O
introduce	O
the	O
notation	O
ln	O
(	O
r	O
)	O
ln	O
(	O
p	O
)	O
==	O
p	O
{	O
x	O
e	O
r	O
,	O
y	O
i	O
gp	O
(	O
x	O
)	O
}	O
,	O
l	O
ln	O
(	O
r	O
)	O
.	O
rep	O
as	O
l	O
(	O
pk	O
)	O
takes	O
the	O
partition	B
into	O
account	O
,	O
but	O
not	O
the	O
majority	B
vote	I
,	O
and	O
ln	O
(	O
pk	O
)	O
does	O
,	O
we	O
need	O
a	O
small	O
additional	O
argument	O
:	O
ln	O
(	O
pk	O
)	O
(	O
ln	O
(	O
pk	O
)	O
-	O
ln	O
(	O
pk	O
»	O
)	O
+	O
(	O
ln	O
(	O
pk	O
)	O
-	O
l	O
(	O
pk	O
»	O
)	O
+	O
l	O
(	O
pk	O
)	O
<	O
2	O
l	O
(	O
ivo	O
(	O
r	O
)	O
vo	O
,	O
n	O
(	O
r	O
)	O
1	O
+	O
ivi	O
(	O
r	O
)	O
-	O
vl	O
,	O
n	O
(	O
r	O
)	O
i	O
)	O
+	O
l	O
(	O
pk	O
)	O
repk	O
putting	O
things	O
together	O
,	O
p	O
{	O
ln	O
(	O
pk	O
)	O
>	O
l	O
*	O
+	O
4e	O
}	O
<	O
p	O
{	O
wn	O
>	O
e	O
}	O
+	O
p	O
{	O
l	O
(	O
pk	O
)	O
>	O
l	O
*	O
+	O
2e	O
}	O
<	O
p	O
{	O
wn	O
>	O
e	O
}	O
+	O
p	O
{	O
vn	O
:	O
:	O
:	O
o	O
}	O
under	O
the	O
given	O
conditions	O
on	O
0	O
and	O
k.	O
observe	O
that	O
if	O
for	O
a	O
set	O
z	O
,	O
def	O
un	O
(	O
z	O
)	O
==	O
ivo	O
(	O
z	O
)	O
-	O
vo	O
,	O
n	O
(	O
z	O
)	O
1	O
+	O
ivl	O
(	O
z	O
)	O
-	O
vl	O
,	O
n	O
(	O
z	O
)	O
i	O
,	O
then	O
vn	O
<	O
sup	O
un	O
(	O
z	O
)	O
+	O
sup	O
rectangles	O
z	O
disjoint	O
sets	O
of	O
k	O
rectangles	O
zi	O
,	O
...	O
,	O
zk	O
k	O
lun	O
(	O
zi	O
)	O
i=l	O
<	O
(	O
k	O
+	O
1	O
)	O
sup	O
u/1	O
(	O
z	O
)	O
,	O
rectangles	O
z	O
wn	O
<	O
(	O
k	O
+	O
1	O
)	O
sup	O
un	O
(	O
z	O
)	O
.	O
rectangles	O
z	O
we	O
may	O
now	O
use	O
the	O
vapnik-chervonenkis	O
inequality	B
(	O
theorems	O
12.5	O
and	O
13.8	O
)	O
to	O
conclude	O
that	O
for	O
all	O
e	O
>	O
0	O
,	O
p	O
{	O
sup	O
un	O
(	O
z	O
)	O
>	O
e	O
}	O
:	O
:	O
:	O
s	O
16n2de-ne2/32	O
.	O
rectangles	O
z	O
problems	O
and	O
exercises	O
357	O
armed	O
with	O
this	O
,	O
we	O
have	O
p	O
{	O
wn	O
>	O
e	O
}	O
<	O
16n2de	O
-n	O
(	O
2	O
(	O
k~1	O
)	O
//32	O
,	O
p	O
{	O
vn	O
:	O
:	O
:	O
:	O
o	O
}	O
<	O
16n2d	O
e	O
--	O
n	O
(	O
2	O
(	O
li	O
)	O
/	O
/32	O
.	O
both	O
terms	O
tend	O
to	O
0	O
with	O
n	O
if	O
nl	O
(	O
k2log	O
n	O
)	O
~	O
00	O
and	O
no2	O
l	O
(	O
k2log	O
n	O
)	O
~	O
00.	O
take	O
0	O
=	O
e	O
i	O
(	O
3k	O
)	O
to	O
satisfy	O
our	O
earlier	O
side	O
condition	O
,	O
and	O
note	O
that	O
we	O
need	O
l	O
*	O
by	O
n/	O
(	O
k3	O
log	O
n	O
)	O
~	O
00.	O
we	O
,	O
in	O
fact	O
,	O
have	O
strong	B
convergence	O
to	O
0	O
for	O
ln	O
(	O
pk	O
)	O
the	O
borel-cantelli	O
lemma	O
.	O
0	O
the	O
crucial	O
element	O
in	O
the	O
proof	O
of	O
theorem	O
20.9	O
is	O
the	O
fact	O
that	O
the	O
number	O
of	O
sets	O
in	O
the	O
partition	B
q	O
grows	O
at	O
most	O
linearly	O
in	O
i	O
,	O
the	O
iteration	O
number	O
.	O
had	O
it	O
grown	O
quadratically	O
,	O
say	O
,	O
it	O
would	O
not	O
have	O
been	O
good	O
enough-we	O
would	O
not	O
have	O
had	O
guaranteed	O
improvements	O
of	O
large	O
enough	O
sizes	O
to	O
push	O
the	O
error	O
probability	O
towards	O
l	O
*	O
.	O
in	O
the	O
multidimensional	O
version	O
and	O
in	O
extension	O
to	O
other	O
types	O
of	O
cuts	O
(	O
problems	O
20.29	O
,	O
20.30	O
)	O
this	O
is	O
virtually	O
the	O
only	O
thing	O
that	O
must	O
be	O
verified	O
.	O
for	O
hyperplane	B
cuts	O
,	O
an	O
additional	O
inequality	B
of	O
the	O
vc	O
type	O
is	O
needed	O
,	O
extending	O
that	O
for	O
classes	O
of	O
hyperplanes	O
.	O
our	O
proof	O
is	O
entirely	O
combinatorial	O
and	O
geometric	B
and	O
comes	O
with	O
explicit	O
bounds	O
.	O
the	O
only	O
reference	O
to	O
the	O
bayes	O
error	O
we	O
need	O
is	O
the	O
quantity	O
i	O
(	O
e	O
)	O
,	O
which	O
is	O
the	O
smallest	O
value	O
l	O
for	O
which	O
inf	O
alllxl	O
grids	O
9z	O
l	O
(	O
yl	O
)	O
:	O
:	O
:	O
:	O
;	O
l	O
*	O
+	O
e.	O
it	O
depends	O
heavily	O
on	O
the	O
distribution	B
of	O
course	O
.	O
calli	O
(	O
e	O
)	O
the	O
grid	B
complexity	I
of	O
the	O
distribution	B
,	O
for	O
lack	O
of	O
a	O
better	O
term	O
.	O
for	O
example	O
,	O
if	O
x	O
is	O
discrete	O
and	O
takes	O
values	O
on	O
the	O
hypercube	O
{	O
o	O
,	O
l	O
}	O
d	O
,	O
then	O
i	O
(	O
e	O
)	O
:	O
:	O
:	O
:	O
;	O
2d	O
.	O
if	O
x	O
takes	O
values	O
on	O
{	O
ai	O
,	O
...	O
,	O
am	O
}	O
d	O
,	O
then	O
i	O
(	O
e	O
)	O
:	O
:	O
:	O
:	O
;	O
(	O
m	O
+	O
l	O
)	O
d.	O
if	O
x	O
has	O
an	O
arbitrary	O
distribution	B
on	O
the	O
real	O
line	O
and	O
1	O
]	O
(	O
x	O
)	O
is	O
monotone	O
,	O
then	O
l	O
(	O
e	O
)	O
:	O
:	O
:	O
:	O
;	O
2.	O
however	O
,	O
if	O
1	O
]	O
is	O
unimodal	O
,	O
l	O
(	O
e	O
)	O
:	O
:	O
:	O
:	O
;	O
3.	O
in	O
one	O
dimension	B
,	O
i	O
(	O
e	O
)	O
is	O
sensitive	O
to	O
the	O
number	O
of	O
places	O
where	O
the	O
bayes	O
decision	O
changes	O
.	O
in	O
two	O
dimensions	O
,	O
however	O
,	O
l	O
(	O
e	O
)	O
measures	O
the	O
complexity	O
of	O
the	O
distribution	B
,	O
especially	O
near	O
regions	O
with	O
1	O
]	O
(	O
x	O
)	O
~	O
1/2	O
.	O
when	O
x	O
is	O
uniform	B
on	O
[	O
0	O
,	O
1	O
]	O
2	O
and	O
y	O
=	O
1	O
if	O
and	O
only	O
if	O
the	O
components	O
of	O
x	O
sum	O
to	O
less	O
than	O
one	O
,	O
then	O
i	O
(	O
e	O
)	O
=	O
8	O
(	O
1	O
i	O
-je	O
)	O
as	O
e	O
{	O
-	O
0	O
(	O
problem	O
20.32	O
)	O
,	O
for	O
example	O
.	O
the	O
grid	B
complexity	I
is	O
eminently	O
suited	O
to	O
study	O
rates	O
of	O
convergence	O
,	O
as	O
it	O
is	O
explicitly	O
featured	O
in	O
the	O
inequalities	O
of	O
our	O
proof	O
.	O
there	O
is	O
no	O
room	O
in	O
this	O
book	O
for	O
following	O
this	O
thread	O
,	O
however	O
.	O
problems	O
and	O
exercises	O
problem	O
20.1.	O
let	O
x	O
be	O
a	O
random	O
vector	O
with	O
density	O
f	O
on	O
nd	O
.	O
show	O
that	O
each	O
component	O
has	O
a	O
density	O
as	O
well	O
.	O
problem	O
20.2.	O
show	O
that	O
both	O
condition	O
(	O
1	O
)	O
and	O
condition	O
(	O
2	O
)	O
of	O
theorem	O
6.1	O
are	O
necessary	O
for	O
consistency	B
of	O
trees	O
with	O
the	O
x	O
-property	O
.	O
358	O
20.	O
tree	B
classifiers	O
problem	O
20.3.	O
for	O
a	O
theoretical	O
median	B
tree	I
in	O
n	O
d	O
with	O
uniform	O
marginals	O
and	O
k	O
levels	O
of	O
splitting	O
,	O
show	O
that	O
e	O
{	O
h	O
(	O
x	O
)	O
+	O
vex	O
)	O
}	O
=	O
1/	O
(	O
2k	O
)	O
when	O
k	O
is	O
a	O
multiple	O
of	O
d.	O
d	O
/	O
problem	O
20.4.	O
a-balanced	B
trees	O
.	O
consider	O
the	O
following	O
generalization	O
of	O
the	O
median	B
tree	I
:	O
at	O
a	O
node	O
in	O
the	O
tree	B
that	O
represents	O
n	O
points	O
,	O
if	O
a	O
split	O
occurs	O
,	O
both	O
subtrees	O
must	O
be	O
of	O
size	O
at	O
least	O
a	O
(	O
n	O
-	O
1	O
)	O
for	O
some	O
fixed	O
a	O
>	O
o.	O
repeat	O
the	O
splits	O
for	O
k	O
levels	O
,	O
resulting	O
in	O
2k	O
leaf	O
regions	O
.	O
the	O
tree	B
has	O
the	O
x	O
-property	O
and	O
the	O
classifier	B
is	O
natural	O
.	O
however	O
,	O
the	O
points	O
at	O
which	O
cuts	O
are	O
made	O
are	O
picked	O
among	O
the	O
data	O
points	O
in	O
an	O
arbitrary	O
fashion	O
.	O
the	O
splits	O
rotate	O
through	O
the	O
coordinate	O
axes	O
.	O
generalize	O
the	O
consistency	B
theorem	O
for	O
median	B
trees	O
.	O
problem	O
20.5.	O
consider	O
the	O
following	O
tree	B
classifier	O
.	O
first	O
we	O
find	O
the	O
median	B
according	O
to	O
one	O
coordinate	O
,	O
and	O
create	O
two	O
subtrees	O
of	O
sizes	O
len	O
-	O
1	O
)	O
/2j	O
and	O
r	O
(	O
n	O
-	O
1	O
)	O
/21-	O
repeat	O
this	O
at	O
each	O
level	O
,	O
cutting	O
the	O
next	O
coordinate	O
axis	O
in	O
a	O
rotational	O
manner	O
.	O
do	O
not	O
cut	O
a	O
node	O
any	O
further	O
if	O
either	O
all	O
data	O
points	O
in	O
the	O
corresponding	O
region	O
have	O
identical	O
yi	O
values	O
or	O
the	O
region	O
contains	O
less	O
than	O
k	O
points	O
.	O
prove	O
that	O
the	O
obtained	O
natural	B
classifier	I
is	O
consistent	O
whenever	O
x	O
has	O
a	O
density	O
if	O
k	O
-+	O
00	O
and	O
log	O
n/	O
k	O
-+	O
o.	O
problem	O
20.6.	O
let	O
m	O
be	O
a	O
probability	O
measure	B
with	O
a	O
density	O
f	O
on	O
n	O
d	O
,	O
and	O
define	O
c	O
=	O
{	O
x	O
=	O
(	O
xci	O
)	O
,	O
...	O
,	O
x	O
cd	O
»	O
:	O
m	O
(	O
[	O
xci	O
)	O
,	O
xci	O
)	O
+	O
e	O
]	O
x	O
...	O
x	O
[	O
x	O
(	O
d	O
)	O
,	O
xed	O
)	O
+	O
en	O
>	O
0	O
all	O
e	O
>	O
o	O
}	O
.	O
show	O
that	O
m	O
(	O
c	O
)	O
=	O
1.	O
hint	O
:	O
proceed	O
as	O
in	O
the	O
proof	O
of	O
lemma	O
a.l	O
in	O
the	O
appendix	O
.	O
problem	O
20.7.	O
let	O
u	O
(	O
l	O
)	O
,	O
...	O
,	O
u	O
cn	O
)	O
be	O
uniform	B
order	O
statistics	B
defining	O
spacings	O
sl	O
,	O
...	O
,	O
sn+l	O
with	O
si	O
=	O
u	O
(	O
i	O
)	O
-	O
uu-l	O
)	O
,	O
if	O
u	O
(	O
n+l	O
)	O
=	O
1	O
and	O
u	O
(	O
o	O
)	O
=	O
o.	O
show	O
that	O
(	O
1	O
)	O
sl	O
,	O
...	O
,	O
sn+l	O
are	O
identically	O
distributed	O
;	O
(	O
2	O
)	O
p	O
{	O
sl	O
>	O
x	O
}	O
=	O
(	O
1	O
-	O
xy\	O
x	O
e	O
[	O
0	O
,	O
1	O
]	O
;	O
(	O
3	O
)	O
e	O
{	O
sil	O
=	O
l/	O
(	O
n	O
+	O
1	O
)	O
.	O
problem	O
20.8.	O
in	O
the	O
proof	O
of	O
theorem	O
20.3	O
,	O
we	O
assumed	O
in	O
the	O
second	O
part	O
that	O
horizontal	O
and	O
vertical	O
cuts	O
were	O
meted	O
out	O
independently	O
.	O
return	O
to	O
the	O
proof	O
and	O
see	O
how	O
you	O
can	O
modify	O
it	O
to	O
take	O
care	O
of	O
the	O
forced	O
alternating	O
cuts	O
.	O
problem	O
20.9.	O
let	O
xl	O
,	O
x	O
2	O
,	O
•..	O
,	O
xn	O
be	O
i.i.d	O
.	O
with	O
common	O
density	O
f	O
on	O
the	O
real	O
line	O
.	O
call	O
xi	O
a	O
record	O
if	O
xi	O
=	O
min	O
(	O
x	O
i	O
,	O
...	O
,	O
xj	O
.	O
let	O
k	O
=	O
f	O
{	O
x	O
;	O
is	O
a	O
record	O
)	O
·	O
show	O
that	O
r	O
l	O
,	O
..•	O
,	O
rn	O
are	O
independent	O
and	O
that	O
p	O
{	O
ri	O
=	O
i	O
}	O
=	O
11	O
i.	O
conclude	O
that	O
the	O
expected	O
number	O
of	O
records	O
is	O
''	O
-	O
'	O
logn	O
.	O
problem	O
20.10.	O
the	O
deep	B
k-d	O
tree	B
.	O
assume	O
that	O
x	O
has	O
a	O
density	O
f	O
and	O
that	O
all	O
marginal	O
densities	O
are	O
uniform	B
.	O
(	O
1	O
)	O
show	O
that	O
k	O
-+	O
00	O
implies	O
that	O
diam	O
(	O
a	O
(	O
x	O
»	O
-+	O
0	O
in	O
probability	O
.	O
do	O
this	O
by	O
arguing	O
that	O
diam	O
(	O
a	O
(	O
x	O
»	O
-+	O
0	O
in	O
probability	O
for	O
the	O
chronological	B
k-d	O
tree	B
with	O
the	O
same	O
parameter	O
k.	O
in	O
a	O
random	O
k	O
-d	O
tree	B
with	O
n	O
elements	O
,	O
show	O
that	O
the	O
depth	O
d	O
of	O
the	O
last	O
inserted	O
node	O
satisfies	O
d	O
i	O
(	O
2	O
log	O
n	O
)	O
-+	O
i	O
in	O
probability	O
.	O
argue	O
first	O
that	O
you	O
may	O
restrict	O
yourself	O
to	O
d	O
=	O
1.	O
then	O
write	O
d	O
as	O
a	O
sum	O
of	O
indicators	O
of	O
records	O
.	O
conclude	O
by	O
computing	O
e	O
{	O
d	O
}	O
and	O
var	O
{	O
d	O
}	O
or	O
at	O
least	O
bounding	O
these	O
quantities	O
in	O
an	O
appropriate	O
manner	O
.	O
improve	O
the	O
second	O
condition	O
of	O
theorem	O
20.4	O
to	O
(	O
k	O
-	O
2	O
log	O
n	O
)	O
ijlog	O
n	O
-+	O
-00	O
.	O
(	O
note	O
that	O
it	O
is	O
possible	O
that	O
lim	O
supn-hxl	O
k	O
i	O
log	O
n	O
=	O
2	O
.	O
)	O
hint	O
:	O
show	O
that	O
i	O
d	O
-	O
2	O
log	O
n	O
i	O
=	O
0	O
(	O
jlog	O
n	O
)	O
in	O
probability	O
by	O
referring	O
to	O
the	O
previous	O
part	O
of	O
this	O
(	O
2	O
)	O
(	O
3	O
)	O
exercise	O
.	O
problems	O
and	O
exercises	O
359	O
problem	O
20.11.	O
consider	O
a	O
full-fledged	O
k-d	B
tree	I
with	O
n+	O
1	O
external	O
node	O
regions	O
.	O
let	O
ln	O
be	O
the	O
probability	O
of	O
error	O
if	O
classification	O
is	O
based	O
upon	O
this	O
tree	B
and	O
if	O
external	O
node	O
regions	O
are	O
assigned	O
the	O
labels	O
(	O
classes	O
)	O
of	O
their	O
immediate	O
parents	O
(	O
see	O
figure	O
20.29	O
)	O
.	O
figure	O
20.29.	O
the	O
external	O
nodes	O
are	O
as	O
(	O
cid:173	O
)	O
signed	O
the	O
classes	O
of	O
their	O
parents	O
.	O
leaf	O
region	O
show	O
that	O
whenever	O
x	O
has	O
a	O
density	O
,	O
e	O
{	O
ln	O
}	O
~	O
lnn	O
:	O
:	O
:	O
2l	O
*	O
just	O
as	O
for	O
the	O
i-nearest	O
neighbor	B
rule	I
.	O
problem	O
20.12.	O
continuation	O
.	O
show	O
that	O
ln	O
~	O
lnn	O
in	O
probability	O
.	O
problem	O
20.13.	O
meisel	O
and	O
micha1opoulos	O
(	O
1973	O
)	O
propose	O
a	O
binary	O
tree	B
classifier	O
with	O
perpendicular	O
cuts	O
in	O
which	O
all	O
leaf	O
regions	O
are	O
homogeneous	O
,	O
that	O
is	O
,	O
all	O
yi	O
's	O
for	O
the	O
xi	O
's	O
in	O
the	O
same	O
region	O
are	O
identical	O
.	O
•	O
•	O
•	O
•	O
•	O
0	O
0	O
0	O
-.-	O
0	O
0	O
•	O
•	O
0	O
0	O
0	O
0	O
figure	O
20.30.	O
example	O
of	O
a	O
tree	O
partition	B
into	O
homogeneous	O
regions	O
.	O
(	O
1	O
)	O
(	O
2	O
)	O
(	O
3	O
)	O
(	O
4	O
)	O
if	O
l	O
*	O
=	O
0	O
,	O
give	O
a	O
stopping	O
rule	B
for	O
constructing	O
a	O
consistent	O
rule	B
whenever	O
x	O
has	O
a	O
density	O
.	O
if	O
l	O
*	O
=	O
0	O
,	O
show	O
that	O
there	O
exists	O
a	O
consistent	O
homogeneous	O
partition	B
classifier	O
with	O
o	O
(	O
n	O
)	O
expected	O
number	O
of	O
cells	O
.	O
if	O
l	O
*	O
>	O
0	O
,	O
show	O
a	O
(	O
more	O
complicated	O
)	O
way	O
of	O
constructing	O
a	O
homogeneous	O
par	O
(	O
cid:173	O
)	O
tition	O
that	O
yields	O
a	O
tree	O
classifier	B
with	O
e	O
{	O
ln	O
}	O
~	O
l	O
*	O
whenever	O
x	O
has	O
a	O
density	O
.	O
hint	O
:	O
first	O
make	O
a	O
consistent	O
nonhomogeneous	O
binary	B
tree	O
classifier	B
,	O
and	O
refine	O
it	O
to	O
make	O
it	O
homogeneous	O
.	O
if	O
l	O
*	O
>	O
0	O
,	O
then	O
show	O
that	O
the	O
expected	O
number	O
of	O
homogeneous	O
regions	O
is	O
at	O
least	O
en	O
for	O
some	O
e	O
>	O
0.	O
such	O
rules	O
are	O
therefore	O
not	O
practical	O
,	O
unless	O
l	O
*	O
=	O
0.	O
overfitting	B
will	O
occur	O
.	O
problem	O
20.14.	O
let	O
x	O
be	O
uniform	B
on	O
[	O
0	O
,	O
1	O
]	O
and	O
let	O
y	O
be	O
independent	O
of	O
x	O
with	O
p	O
{	O
y	O
=	O
i	O
}	O
=	O
p.	O
find	O
a	O
tree	O
classifier	B
based	O
upon	O
simple	O
interval	O
splitting	O
for	O
which	O
each	O
region	O
has	O
one	O
data	O
point	O
,	O
and	O
.	O
.	O
hmmf	O
p-+o	O
lim	O
infn-+oo	O
eln	O
p	O
2	O
:	O
3	O
.	O
360	O
20.	O
tree	B
classifiers	O
we	O
know	O
that	O
for	O
the	O
nearest	B
neighbor	I
rule	I
,	O
.	O
hm	O
p-+o	O
lim	O
supn-+cxl	O
eln	O
p	O
=2	O
,	O
so	O
that	O
the	O
given	O
interval	O
splitting	O
method	O
is	O
worse	O
than	O
the	O
nearest	B
neighbor	I
method	O
.	O
(	O
this	O
provides	O
a	O
counterexample	O
to	O
a	O
conjecture	O
of	O
breiman	O
,	O
friedman	O
,	O
olshen	O
,	O
and	O
stone	O
(	O
1984	O
,	O
pp.89-90	O
)	O
.	O
)	O
hint	O
:	O
sort	O
the	O
points	O
,	O
and	O
adjust	O
the	O
sizes	O
of	O
the	O
intervals	O
given	O
to	O
the	O
class	O
0	O
and	O
class	O
1	O
points	O
in	O
favor	O
of	O
the	O
1	O
'so	O
by	O
doing	O
so	O
,	O
the	O
asymptotic	O
probability	O
of	O
error	O
can	O
be	O
made	O
as	O
close	O
as	O
desired	O
to	O
(	O
p2	O
+	O
3	O
p	O
(	O
1	O
-	O
p	O
)	O
)	O
(	O
1	O
-	O
p	O
)	O
.	O
problem	O
20.15.	O
continuation	O
.	O
let	O
x	O
be	O
uniform	B
on	O
[	O
0	O
,	O
1	O
)	O
2	O
and	O
let	O
y	O
be	O
independent	O
of	O
x	O
with	O
p	O
{	O
y	O
=	O
i	O
}	O
=	O
p.	O
find	O
a	O
data-based	O
tree	B
classifier	O
based	O
upon	O
perpendicular	O
cuts	O
for	O
which	O
each	O
region	O
has	O
one	O
data	O
point	O
,	O
diam	O
(	O
a	O
(	O
x	O
)	O
)	O
-+	O
0	O
in	O
probability	O
(	O
recall	O
theorems	O
6.1	O
and	O
21.2	O
)	O
,	O
and	O
.	O
.	O
hmmf	O
p-+o	O
lim	O
infn-+	O
cxl	O
eln	O
p	O
=	O
00.	O
conclude	O
that	O
in	O
n	O
d	O
,	O
we	O
can	O
construct	O
tree	B
classifiers	O
with	O
one	O
point	O
per	O
cell	O
that	O
are	O
much	O
worse	O
than	O
the	O
nearest	B
neighbor	I
rule	I
.	O
hint	O
:	O
the	O
next	O
two	O
problems	O
may	O
help	O
you	O
with	O
the	O
construction	O
and	O
analysis	O
.	O
problem	O
20.16.	O
continuation	O
.	O
draw	O
ani.i.d	O
.	O
sample	O
xl	O
,	O
...	O
,	O
xn	O
from	O
the	O
uniformdistn	O
(	O
cid:173	O
)	O
butionon	O
[	O
0,1	O
)	O
2	O
,	O
and	O
let	O
yi	O
,	O
•••	O
,	O
yn	O
bei.i.d	O
.	O
and	O
independent	O
of	O
the	O
xi	O
's	O
withp	O
{	O
y	O
=	O
i	O
}	O
=	O
p.	O
construct	O
a	O
binary	O
tree	B
partition	O
with	O
perpendicular	O
cuts	O
for	O
{	O
xi	O
:	O
yi	O
=	O
i	O
}	O
such	O
that	O
every	O
leaf	O
region	O
has	O
one	O
and	O
only	O
one	O
point	O
and	O
diam	O
(	O
a	O
(	O
x	O
)	O
)	O
-+	O
0	O
in	O
probability	O
.	O
(	O
1	O
)	O
how	O
would	O
you	O
proceed	O
,	O
avoiding	O
putting	O
xi	O
's	O
on	O
borders	O
of	O
regions	O
?	O
(	O
2	O
)	O
prove	O
diam	O
(	O
a	O
(	O
x	O
)	O
)	O
-+	O
0	O
in	O
probability	O
.	O
(	O
3	O
)	O
add	O
the	O
(	O
xi	O
,	O
yi	O
)	O
pairs	O
with	O
yi	O
=	O
0	O
to	O
the	O
leaf	O
regions	O
,	O
so	O
that	O
every	O
region	O
has	O
one	O
class-1	O
observation	O
and	O
zero	O
or	O
more	O
class-o	O
observations	O
.	O
give	O
the	O
class-1	O
observation	O
the	O
largest	O
area	O
containing	O
no	O
class-o	O
points	O
,	O
as	O
shown	O
in	O
figure	O
20.31.	O
show	O
that	O
this	O
can	O
always	O
be	O
done	O
by	O
adding	O
perpendicular	O
cuts	O
and	O
keeping	O
at	O
least	O
one	O
observation	O
per	O
region	O
.	O
·0	O
0	O
figure	O
20.31.	O
cutting	O
a	O
rectangle	O
by	O
giving	O
a	O
large	O
area	O
to	O
the	O
single	O
class-1	O
point	O
.	O
...	O
0	O
•	O
1	O
0	O
.0	O
·0	O
(	O
4	O
)	O
partition	B
all	O
rectangles	O
with	O
more	O
than	O
one	O
point	O
further	O
to	O
finally	O
obtain	O
a	O
one	O
(	O
cid:173	O
)	O
point-per-leaf-region	O
partition	B
.	O
if	O
there	O
are	O
n	O
points	O
in	O
a	O
region	O
of	O
the	O
tree	B
before	O
the	O
class-l	O
and	O
class-o	O
points	O
are	O
separated	O
(	O
thus	O
,	O
n	O
-	O
1	O
class-o	O
points	O
and	O
one	O
class-1	O
point	O
)	O
,	O
then	O
show	O
that	O
the	O
expected	O
proportion	O
of	O
the	O
region	O
's	O
area	O
given	O
to	O
class	O
1	O
,	O
given	O
n	O
,	O
times	O
n	O
tends	O
to	O
00	O
.	O
(	O
an	O
explicit	O
lower	O
bound	O
will	O
be	O
helpful	O
.	O
)	O
hint	O
:	O
use	O
the	O
next	O
problem	O
.	O
(	O
5	O
)	O
write	O
the	O
probability	O
of	O
error	O
for	O
the	O
rule	B
in	O
terms	O
of	O
areas	O
of	O
rectangles	O
,	O
and	O
use	O
part	O
(	O
4	O
)	O
to	O
get	O
an	O
asymptotic	O
lower	O
bound	O
.	O
problems	O
and	O
exercises	O
361	O
(	O
6	O
)	O
now	O
let	O
p	O
tend	O
to	O
zero	O
and	O
get	O
an	O
asymptotic	O
expression	O
for	O
your	O
lower	O
bound	O
in	O
terms	O
of	O
p.	O
compare	O
this	O
with	O
2	O
p	O
(	O
1	O
-	O
p	O
)	O
,	O
the	O
asymptotic	O
error	O
probability	O
of	O
the	O
i-nearest	O
neighbor	B
rule	I
.	O
problem	O
20.17.	O
continuation	O
.	O
draw	O
a	O
sample	O
xl	O
,	O
...	O
,	O
xn	O
of	O
n	O
i.i.d	O
.	O
observations	O
uni	O
(	O
cid:173	O
)	O
formly	O
distributed	O
on	O
[	O
0	O
,	O
i	O
]	O
d.	O
the	O
rectangles	O
defined	O
by	O
the	O
origin	O
and	O
x	O
i	O
,	O
...	O
,	O
xn	O
as	O
op	O
(	O
cid:173	O
)	O
posite	O
vertices	O
are	O
denoted	O
by	O
r	O
i	O
,	O
...	O
,	O
rn	O
,	O
respectively	O
.	O
the	O
probability	O
content	O
of	O
ri	O
is	O
clearly	O
fl	O
(	O
ri	O
)	O
=	O
n	O
:	O
=l	O
xij	O
)	O
·	O
study	O
mn	O
=	O
max	O
{	O
fl	O
(	O
ri	O
)	O
:	O
1	O
:	O
s	O
i	O
:	O
s	O
n	O
,	O
ri	O
does	O
not	O
contain	O
r	O
j	O
for	O
any	O
j	O
=i	O
i	O
}	O
,	O
the	O
probability	O
content	O
of	O
the	O
largest	O
empty	O
rectangle	O
with	O
a	O
vertex	O
at	O
the	O
origin	O
.	O
for	O
d	O
=	O
1	O
,	O
mil	O
is	O
just	O
the	O
minimum	O
of	O
the	O
x/s	O
,	O
and	O
thus	O
nmn	O
~	O
e	O
,	O
where	O
e	O
is	O
the	O
exponential	B
distribution	I
.	O
also	O
e	O
{	O
mn	O
}	O
=	O
l/	O
(	O
n	O
+	O
1	O
)	O
.	O
for	O
d	O
>	O
1	O
,	O
mn	O
is	O
larger	O
.	O
show	O
that	O
nmn	O
-+	O
00	O
in	O
probability	O
and	O
try	O
obtaining	O
the	O
first	O
term	O
in	O
the	O
rate	O
of	O
increase	O
.	O
problem	O
20.18.	O
show	O
that	O
diam	O
(	O
a	O
(	O
x	O
»	O
-+	O
0	O
in	O
probability	O
for	O
the	O
chronological	B
quadtree	O
whenever	O
k	O
-+	O
0	O
and	O
x	O
has	O
a	O
density	O
.	O
hint	O
:	O
mimic	O
the	O
proof	O
for	O
the	O
chronological	B
quadtree	O
.	O
problem	O
20.19.	O
show	O
that	O
the	O
deep	B
quadtree	O
is	O
consistent	O
if	O
x	O
has	O
a	O
density	O
and	O
k	O
levels	O
of	O
splits	O
are	O
applied	O
,	O
where	O
k	O
-+	O
00	O
and	O
kl	O
log	O
n	O
-+	O
o	O
.	O
-	O
1	O
)	O
+	O
1	O
leaf	O
problem	O
20.20.	O
consider	O
a	O
full-fledged	O
quadtree	B
with	O
n	O
nodes	O
(	O
and	O
thus	O
n	O
(	O
2d	O
regions	O
)	O
.	O
assign	O
to	O
each	O
region	O
the	O
y-iabel	O
(	O
class	O
)	O
of	O
its	O
parent	O
in	O
the	O
quadtree	B
.	O
with	O
this	O
simple	O
classifier	O
,	O
show	O
that	O
whenever	O
x	O
has	O
a	O
density	O
,	O
e	O
{	O
ln	O
}	O
-+	O
2e	O
{	O
1	O
]	O
(	O
x	O
)	O
(	O
1-1	O
]	O
(	O
x	O
»	O
}	O
=	O
l	O
nn	O
•	O
in	O
particular	O
,	O
lim	O
supn-	O
;	O
.	O
do	O
e	O
{	O
ln	O
}	O
:	O
s	O
2l	O
*	O
and	O
the	O
classifier	B
is	O
consistent	O
when	O
l	O
*	O
=	O
o.	O
problem	O
20.21.	O
in	O
r	O
}	O
,	O
partition	B
the	O
space	O
as	O
follows	O
:	O
xl	O
,	O
x2	O
define	O
nine	O
regions	O
by	O
vertical	O
and	O
horizontal	O
lines	O
through	O
them	O
.	O
x	O
3	O
,	O
•..	O
,	O
x	O
k	O
are	O
sent	O
down	O
to	O
the	O
appropriate	O
subtrees	O
in	O
the	O
9-ary	O
tree	B
,	O
and	O
within	O
each	O
subtree	O
with	O
at	O
least	O
two	O
points	O
,	O
the	O
process	O
is	O
repeated	O
recursively	O
.	O
a	O
decision	O
at	O
x	O
is	O
by	O
a	O
majority	O
vote	O
(	O
among	O
(	O
xk+l	O
,	O
yk+i	O
)	O
,	O
.••	O
,	O
(	O
xn	O
'	O
yn	O
»	O
among	O
those	O
xi	O
's	O
in	O
the	O
same	O
rectangle	O
of	O
the	O
partition	B
as	O
x	O
show	O
that	O
if	O
k	O
-+	O
00	O
,	O
kin	O
-+	O
0	O
,	O
the	O
rule	B
is	O
consistent	O
whenever	O
x	O
has	O
a	O
density	O
.	O
problem	O
20.22.	O
on	O
the	O
two-dimensional	O
counterexample	O
shown	O
in	O
the	O
text	O
for	O
multivariate	O
stoller	O
splits	O
,	O
prove	O
that	O
if	O
splits	O
are	O
performed	O
based	O
upon	O
a	O
sample	O
drawn	O
from	O
the	O
distri	O
(	O
cid:173	O
)	O
bution	O
,	O
and	O
if	O
we	O
stop	O
after	O
k	O
splits	O
with	O
k	O
depending	O
on	O
n	O
in	O
such	O
a	O
way	O
that	O
k	O
i	O
log	O
n	O
-+	O
0	O
,	O
then	O
l	O
n	O
,	O
the	O
conditional	O
probability	O
of	O
error	O
,	O
satisfies	O
lim	O
inf'hdo	O
e	O
{	O
l	O
,	O
j	O
2	O
:	O
(	O
1-	O
e	O
)	O
/2	O
.	O
hint	O
:	O
bound	O
the	O
probability	O
of	O
ever	O
splitting	O
[	O
1	O
,	O
2f	O
anywhere	O
by	O
noting	O
that	O
the	O
maximal	O
differ	O
(	O
cid:173	O
)	O
ence	O
between	O
the	O
empirical	B
distribution	O
functions	O
for	O
the	O
first	O
coordinate	O
of	O
x	O
given	O
y	O
=	O
0	O
and	O
y	O
=	O
1	O
is	O
0	O
(	O
1	O
i	O
in	O
)	O
when	O
restricted	O
to	O
[	O
1	O
,	O
2f	O
.	O
problem	O
20.23.	O
let	O
x	O
have	O
the	O
uniform	B
distribution	O
on	O
[	O
0,5	O
]	O
and	O
let	O
y	O
=	O
i	O
{	O
2	O
<	O
xdj	O
,	O
so	O
that	O
l	O
*	O
=	O
o.	O
construct	O
a	O
binary	O
classification	O
tree	B
by	O
selecting	O
at	O
each	O
iteration	O
the	O
split	O
that	O
minimizes	O
the	O
impurity	B
function	I
i	O
,	O
where	O
1jj	O
is	O
the	O
gini	O
criterion	O
.	O
consider	O
just	O
the	O
first	O
three	O
splits	O
made	O
in	O
this	O
manner	O
.	O
let	O
ln	O
be	O
the	O
probability	O
of	O
error	O
with	O
the	O
given	O
rule	B
(	O
use	O
a	O
majority	O
vote	O
over	O
the	O
leaf	O
regions	O
)	O
.	O
show	O
that	O
ln	O
-+	O
0	O
in	O
probability	O
.	O
analyze	O
the	O
algorithm	B
when	O
the	O
gini	O
criterion	O
is	O
replaced	O
by	O
the	O
probability-of-error	O
criterion	O
.	O
problem	O
20.24.	O
let	O
x	O
be	O
uniform	B
on	O
[	O
0	O
,	O
1	O
]	O
and	O
let	O
y	O
be	O
independent	O
of	O
x	O
,	O
with	O
ply	O
=	O
i	O
}	O
=	O
2/3	O
.	O
draw	O
a	O
sample	O
of	O
size	O
n	O
from	O
this	O
distribution	B
.	O
investigate	O
where	O
the	O
first	O
cut	O
362	O
20.	O
tree	B
classifiers	O
might	O
take	O
place	O
,	O
based	O
upon	O
minimizing	O
the	O
impurity	B
function	I
with	O
'i/f	O
(	O
p	O
,	O
1-	O
p	O
)	O
(	O
the	O
gini	O
criterion	O
)	O
.	O
(	O
once	O
this	O
is	O
established	O
,	O
you	O
will	O
have	O
discovered	O
the	O
nature	O
of	O
the	O
classification	O
tree	B
,	O
roughly	O
speaking	O
.	O
)	O
problem	O
20.25.	O
complete	O
the	O
consistency	B
proof	O
of	O
theorem	O
20.7	O
for	O
the	O
raw	B
bsp	O
tree	B
for	O
n	O
2	O
by	O
showing	O
that	O
diam	O
(	O
a	O
(	O
x	O
)	O
)	O
~	O
0	O
in	O
probability	O
.	O
problem	O
20.26.	O
balanced	B
bsp	O
trees	O
.	O
we	O
generalize	O
median	B
trees	O
to	O
allow	O
splitting	O
the	O
space	O
along	O
hyperplanes	O
.	O
50	O
%	O
/	O
/50	O
%	O
figure	O
20.32.	O
a	O
balanced	O
bsp	O
tree	B
:	O
each	O
hyperplane	B
cut	O
splits	O
a	O
region	O
into	O
two	O
cells	O
of	O
the	O
same	O
cardinality	O
.	O
assume	O
that	O
x	O
has	O
a	O
density	O
and	O
that	O
the	O
tree	B
possesses	O
the	O
x	O
-property	O
.	O
keep	O
splitting	O
until	O
there	O
are	O
2k	O
leaf	O
regions	O
,	O
as	O
with	O
median	O
trees	O
.	O
call	O
the	O
trees	O
balanced	B
bsp	O
trees	O
.	O
(	O
1	O
)	O
show	O
that	O
there	O
are	O
ways	O
of	O
splitting	O
in	O
n	O
2	O
that	O
lead	O
to	O
nonconsistent	O
rules	O
,	O
re	O
(	O
cid:173	O
)	O
(	O
2	O
)	O
gardless	O
how	O
k	O
varies	O
with	O
n.	O
if	O
every	O
splitting	O
hyperplane	O
is	O
forced	O
to	O
contain	O
d	O
data	O
points	O
(	O
in	O
nd	O
)	O
and	O
these	O
data	O
points	O
stay	O
with	O
the	O
splitting	O
node	O
(	O
they	O
are	O
not	O
sent	O
down	O
to	O
subtrees	O
)	O
,	O
then	O
show	O
that	O
once	O
again	O
,	O
there	O
exists	O
a	O
splitting	O
method	O
that	O
leads	O
to	O
nonconsistent	O
rules	O
,	O
regardless	O
of	O
how	O
k	O
varies	O
with	O
n.	O
problem	O
20.27.	O
letx	O
have	O
a	O
uniform	O
distribution	B
on	O
the	O
unit	O
ball	O
of	O
rd	O
.	O
let	O
y	O
=	O
i	O
{	O
ilxil~1/2	O
)	O
'	O
so	O
that	O
l	O
*	O
=	O
o.	O
assume	O
that	O
we	O
split	O
the	O
space	O
by	O
a	O
hyperplane	O
by	O
minimizing	O
an	O
impurity	B
function	I
based	O
upon	O
the	O
gini	O
criterion	O
.	O
if	O
11	O
is	O
very	O
large	O
,	O
where	O
approximately	O
would	O
the	O
cut	O
take	O
place	O
(	O
modulo	O
a	O
rotation	O
,	O
of	O
course	O
)	O
?	O
problem	O
20.28.	O
there	O
exist	O
singular	O
continuous	O
distributions	O
that	O
admit	O
uniform	B
[	O
0	O
,	O
1	O
]	O
marginals	O
in	O
n	O
d.	O
show	O
,	O
for	O
example	O
,	O
that	O
if	O
x	O
is	O
uniformly	O
distributed	O
on	O
the	O
surface	O
of	O
the	O
unit	O
sphere	B
of	O
n3	O
,	O
then	O
its	O
three	O
components	O
are	O
all	O
uniformly	O
distributed	O
on	O
[	O
-	O
1	O
,	O
1	O
]	O
.	O
problem	O
20.29.	O
verify	O
that	O
theorem	B
20.9	O
remains	O
valid	O
in	O
rd	O
.	O
problem	O
20.30.	O
prove	O
that	O
theorem	B
20.9	O
remains	O
valid	O
if	O
rectangular	O
cuts	O
are	O
replaced	O
by	O
any	O
of	O
the	O
elementary	O
cuts	O
shown	O
on	O
figures	O
20.23	O
to	O
20.25	O
,	O
and	O
such	O
cuts	O
are	O
performed	O
recursively	O
k	O
times	O
,	O
always	O
by	O
maximizing	O
the	O
decrease	O
of	O
the	O
empirical	B
error	I
.	O
problem	O
20.31.	O
show	O
that	O
theorem	B
20.11	O
remains	O
valid	O
if	O
k	O
-+	O
ex	O
)	O
and	O
k	O
=	O
0	O
(	O
j	O
n	O
/	O
log	O
n	O
)	O
.	O
hint	O
:	O
in	O
the	O
proof	O
of	O
the	O
theorem	B
,	O
the	O
bound	O
on	O
'01	O
and	O
w	O
''	O
is	O
loose	O
.	O
you	O
may	O
get	O
more	O
efficient	O
bounds	O
by	O
applying	O
theorem	B
21.1	O
from	O
the	O
next	O
chapter	O
.	O
problem	O
20.32.	O
study	O
the	O
behavior	O
of	O
the	O
grid	B
complexity	I
as	O
e	O
t	O
0	O
for	O
the	O
following	O
cases	O
:	O
(	O
1	O
)	O
x	O
is	O
uniform	B
on	O
the	O
perimeter	O
of	O
the	O
unit	O
circle	O
of	O
r	O
2	O
with	O
probability	O
1/2	O
and	O
x	O
is	O
uniform	B
on	O
[	O
0	O
,	O
1	O
]	O
2	O
otherwise	O
.	O
let	O
y	O
=	O
1	O
if	O
and	O
only	O
if	O
x	O
is	O
on	O
the	O
perimeter	O
of	O
that	O
circle	O
(	O
so	O
that	O
l	O
*	O
=	O
0	O
)	O
.	O
(	O
2	O
)	O
x	O
=	O
(	O
x	O
(	O
l	O
)	O
,	O
x	O
(	O
2	O
)	O
)	O
is	O
uniform	B
on	O
(	O
0	O
,	O
1	O
f	O
and	O
y	O
=	O
1	O
if	O
and	O
only	O
if	O
x	O
(	O
l	O
)	O
+	O
x	O
(	O
2	O
)	O
:	O
:	O
:	O
:	O
1	O
.	O
21	O
data-dependent	B
partitioning	O
21.1	O
introduction	O
in	O
chapter	O
9	O
we	O
investigated	O
properties	O
of	O
the	O
regular	B
histogram	O
rule	B
.	O
histogram	O
classifiers	O
partition	B
the	O
observation	O
space	O
nd	O
and	O
classify	O
the	O
input	O
vector	O
x	O
ac	O
(	O
cid:173	O
)	O
cording	O
to	O
a	O
majority	O
vote	O
among	O
the	O
y/s	O
whose	O
corresponding	O
x/s	O
fall	O
in	O
the	O
same	O
cell	O
of	O
the	O
partition	B
as	O
x.	O
partitions	O
discussed	O
in	O
chapter	O
9	O
could	O
depend	O
on	O
the	O
sample	O
size	O
n	O
,	O
but	O
were	O
not	O
allowed	O
to	O
depend	O
on	O
the	O
data	O
dn	O
itself	O
.	O
we	O
dealt	O
mostly	O
with	O
grid	O
partitions	O
,	O
but	O
will	O
now	O
allow	O
other	O
partitions	O
as	O
well	O
.	O
just	O
consider	O
clustered	O
training	O
observations	O
xl	O
,	O
...	O
,	O
x	O
n	O
.	O
near	O
the	O
cluster	O
's	O
center	O
finer	O
partitions	O
are	O
called	O
for	O
.	O
similarly	O
,	O
when	O
the	O
components	O
have	O
different	O
physical	O
dimensions	O
,	O
the	O
scale	O
of	O
one	O
coordinate	O
axis	O
is	O
not	O
related	O
at	O
all	O
to	O
the	O
other	O
scales	O
,	O
and	O
some	O
data-adaptive	O
stretching	O
is	O
necessary	O
.	O
sometimes	O
the	O
data	O
are	O
concen	O
(	O
cid:173	O
)	O
trated	O
on	O
or	O
around	O
a	O
hyperplane	O
.	O
in	O
all	O
these	O
cases	O
,	O
although	O
consistent	O
,	O
the	O
regular	B
histogram	O
method	O
behaves	O
rather	O
poorly	O
,	O
especially	O
if	O
the	O
dimension	B
of	O
the	O
space	O
is	O
large	O
.	O
therefore	O
,	O
it	O
is	O
useful	O
to	O
allow	O
data-dependent	B
partitions	O
,	O
while	O
keeping	O
a	O
majority	O
voting	O
scheme	O
within	O
each	O
cell	O
.	O
the	O
simplest	O
data-dependent	B
partitioning	O
methods	O
are	O
based	O
on	O
statistically	O
equivalent	O
blocks	O
in	O
which	O
each	O
cell	O
contains	O
the	O
same	O
number	O
of	O
points	O
.	O
in	O
one	O
(	O
cid:173	O
)	O
dimensional	O
problems	O
statistically	B
equivalent	I
blocks	I
reduce	O
to	O
k-spacing	O
estimates	O
where	O
the	O
k-th	O
,	O
2k-th	O
,	O
etc	O
.	O
order	B
statistics	I
determine	O
the	O
partition	B
of	O
the	O
real	O
line	O
.	O
sometimes	O
,	O
it	O
makes	O
sense	O
to	O
cluster	O
the	O
data	O
points	O
into	O
groups	O
such	O
that	O
points	O
in	O
a	O
group	O
are	O
close	O
to	O
each	O
other	O
,	O
and	O
define	O
the	O
partition	B
so	O
that	O
each	O
group	O
is	O
in	O
a	O
different	O
cell	O
.	O
many	O
other	O
data-dependent	B
partitioning	O
schemes	O
have	O
been	O
introduced	O
in	O
the	O
lit-	O
364	O
21	O
:	O
data-dependent	B
partitioning	O
erature	O
.	O
in	O
most	O
of	O
these	O
algorithms	O
the	O
cells	O
of	O
the	O
partition	B
correspond	O
to	O
the	O
leaves	O
of	O
a	O
binary	O
tree	B
,	O
which	O
makes	O
computation	O
of	O
the	O
corresponding	O
classification	O
rule	B
fast	O
and	O
convenient	O
.	O
tree	B
classifiers	O
were	O
dealt	O
with	O
in	O
chapter	O
20.	O
analysis	O
of	O
uni	O
(	O
cid:173	O
)	O
versal	O
consistency	B
for	O
these	O
algorithms	O
and	O
corresponding	O
density	O
estimates	O
was	O
begun	O
by	O
abou-jaoude	O
(	O
1976b	O
)	O
and	O
gordon	O
and	O
olshen	O
(	O
1984	O
)	O
,	O
(	O
1978	O
)	O
,	O
(	O
1980	O
)	O
in	O
a	O
general	O
framework	O
,	O
and	O
was	O
extended	O
,	O
for	O
example	O
,	O
by	O
breiman	O
,	O
friedman	O
,	O
olshen	O
,	O
and	O
stone	O
(	O
1984	O
)	O
,	O
chen	O
and	O
zhao	O
(	O
1987	O
)	O
,	O
zhao	O
,	O
krishnaiah	O
,	O
and	O
chen	O
(	O
1990	O
)	O
,	O
lugosi	O
and	O
nobel	O
(	O
1996	O
)	O
,	O
and	O
nobel	O
(	O
1994	O
)	O
.	O
this	O
chapter	O
is	O
more	O
general	O
than	O
the	O
chapter	O
on	O
tree	B
classifiers	O
,	O
as	O
every	O
partition	B
induced	O
by	O
a	O
tree	O
classifier	B
is	O
a	O
valid	O
partition	B
of	O
space	O
,	O
but	O
not	O
vice	O
versa	O
.	O
the	O
example	O
below	O
shows	O
a	O
rectangular	O
partition	B
of	O
the	O
plane	O
that	O
can	O
not	O
be	O
obtained	O
by	O
consecutive	O
perpendicular	O
cuts	O
in	O
a	O
binary	B
classification	O
tree	B
.	O
figure	O
21.1.	O
a	O
rectangular	O
partition	B
of	O
[	O
0	O
,	O
1	O
j2	O
that	O
can	O
not	O
be	O
achieved	O
by	O
a	O
tree	O
of	O
consecutive	O
cuts	O
.	O
in	O
this	O
chapter	O
we	O
first	O
establish	O
general	O
sufficient	O
conditions	O
for	O
the	O
consistency	B
of	O
data-dependent	B
histogram	O
classifiers	O
.	O
because	O
of	O
the	O
complicated	O
dependence	O
of	O
the	O
partition	B
on	O
the	O
data	O
,	O
methods	O
useful	O
for	O
handling	O
regular	B
histograms	O
have	O
to	O
be	O
significantly	O
refined	O
.	O
the	O
main	O
tool	O
is	O
a	O
large	O
deviation	O
inequality	B
for	O
families	O
of	O
partitions	O
that	O
is	O
related	O
to	O
the	O
vapnik	O
-chervonenkis	O
inequality	B
for	O
families	O
of	O
sets	O
.	O
the	O
reader	O
is	O
asked	O
for	O
forgiveness-we	O
want	O
to	O
present	O
a	O
very	O
generally	O
applicable	O
theorem	B
and	O
have	O
to	O
sacrifice	O
(	O
temporarily	O
)	O
by	O
increasing	O
the	O
density	O
of	O
the	O
text	O
.	O
however	O
,	O
as	O
you	O
will	O
discover	O
,	O
the	O
rewards	O
will	O
be	O
sweet	O
.	O
21.2	O
a	O
vapnik	O
-chervonenkis	O
inequality	B
for	O
partitions	O
this	O
is	O
a	O
technical	O
section	O
.	O
we	O
will	O
use	O
its	O
results	O
in	O
the	O
next	O
section	O
to	O
establish	O
a	O
general	O
consistency	B
theorem	O
for	O
histogram	O
rules	O
with	O
data-dependent	O
partitions	O
.	O
the	O
main	O
goal	O
of	O
this	O
section	O
is	O
to	O
extend	O
the	O
basic	O
vapnik-chervonenkis	O
inequality	B
(	O
theorem	B
12.5	O
)	O
to	O
families	O
of	O
partitions	O
from	O
families	O
of	O
sets	O
.	O
the	O
line	O
of	O
thought	O
followed	O
here	O
essentially	O
appears	O
in	O
zhao	O
,	O
krishnaiah	O
,	O
and	O
chen	O
(	O
1990	O
)	O
for	O
rect	O
(	O
cid:173	O
)	O
angular	O
partitions	O
,	O
and	O
more	O
generally	O
in	O
lugosi	O
and	O
nobel	O
(	O
1996	O
)	O
.	O
a	O
substantial	O
simplification	O
in	O
the	O
proof	O
was	O
pointed	O
out	O
to	O
us	O
by	O
andrew	O
barron	O
.	O
by	O
a	O
partition	O
of	O
rd	O
we	O
mean	O
a	O
countable	O
collection	O
p	O
=	O
{	O
ai	O
,	O
a2	O
,	O
..	O
'	O
}	O
of	O
subsets	O
of	O
rd	O
such	O
that	O
uj	O
:	O
l	O
a	O
j	O
=	O
rd	O
and	O
ai	O
n	O
a	O
j	O
=	O
0	O
if	O
i	O
=i	O
j.	O
each	O
set	O
a	O
j	O
is	O
called	O
a	O
cell	O
of	O
the	O
partition	B
p.	O
21.2	O
a	O
vapnik-chervonenkis	O
inequality	B
for	O
partitions	O
365	O
let	O
m	O
be	O
a	O
positive	O
number	O
.	O
for	O
each	O
partition	B
p	O
,	O
we	O
define	O
p	O
(	O
m	O
)	O
as	O
the	O
restric	O
(	O
cid:173	O
)	O
tion	O
ofp	O
to	O
the	O
ball	O
sm	O
(	O
recall	O
that	O
sm	O
c	O
rd	O
denotes	O
the	O
closed	O
ball	O
of	O
radius	O
m	O
centered	O
at	O
the	O
origin	O
)	O
.	O
in	O
other	O
words	O
,	O
p	O
(	O
m	O
)	O
is	O
a	O
partition	O
of	O
sm	O
,	O
whose	O
cells	O
are	O
obtained	O
by	O
intersecting	O
the	O
cells	O
oip	O
with	O
sm	O
.	O
we	O
assume	O
throughout	O
that	O
pis	O
such	O
that	O
ip	O
(	O
m	O
)	O
i	O
<	O
00	O
for	O
each	O
m	O
<	O
00.	O
we	O
denote	O
by	O
b	O
(	O
p	O
(	O
m	O
)	O
the	O
collection	O
of	O
all	O
2ip	O
(	O
m	O
)	O
1	O
sets	O
obtained	O
by	O
unions	O
of	O
cells	O
of	O
p	O
(	O
m	O
)	O
.	O
just	O
as	O
we	O
dealt	O
with	O
classes	O
of	O
sets	O
in	O
chapter	O
12	O
,	O
here	O
we	O
introduce	O
fami	O
(	O
cid:173	O
)	O
lies	O
of	O
partitions	O
.	O
let	O
f	O
be	O
a	O
(	O
possibly	O
infinite	O
)	O
collection	O
of	O
partitions	O
of	O
rd	O
.	O
f	O
(	O
m	O
)	O
=	O
{	O
p	O
(	O
m	O
)	O
:	O
p	O
e	O
f	O
}	O
denotes	O
the	O
family	B
of	I
partitions	O
of	O
sm	O
obtained	O
by	O
re	O
(	O
cid:173	O
)	O
stricting	O
members	O
of	O
f	O
to	O
sm	O
.	O
for	O
each	O
m.	O
we	O
will	O
measure	B
the	O
complexity	B
of	I
a	I
family	I
of	I
partitions	O
f	O
(	O
m	O
)	O
by	O
the	O
shatter	O
coefficients	O
of	O
the	O
class	O
of	O
sets	O
obtained	O
as	O
unions	O
of	O
cells	O
of	O
partitions	O
taken	O
from	O
the	O
family	O
pm	O
)	O
.	O
formally	O
,	O
we	O
define	O
the	O
combinatorial	O
quantity	O
fl.n	O
(	O
f	O
(	O
m	O
)	O
as	O
follows	O
:	O
introduce	O
the	O
class	O
a	O
(	O
m	O
)	O
of	O
subsets	O
ofrd	O
by	O
a	O
(	O
m	O
)	O
=	O
{	O
a	O
e	O
b	O
(	O
p	O
(	O
m	O
)	O
for	O
some	O
p	O
(	O
m	O
)	O
e	O
pm	O
)	O
}	O
,	O
and	O
define	O
fl.n	O
(	O
pm	O
)	O
=	O
s	O
(	O
a	O
(	O
m	O
)	O
,	O
n	O
)	O
,	O
the	O
shatter	B
coefficient	I
of	O
a	O
(	O
m	O
)	O
•	O
a	O
(	O
m	O
)	O
is	O
thus	O
the	O
class	O
of	O
all	O
sets	O
that	O
can	O
be	O
obtained	O
as	O
unions	O
of	O
cells	O
of	O
some	O
partition	B
of	O
s	O
m	O
in	O
the	O
collection	O
pm	O
)	O
.	O
for	O
example	O
,	O
if	O
all	O
members	O
of	O
f	O
(	O
m	O
)	O
partition	B
sm	O
into	O
two	O
sets	O
,	O
then	O
fl.n	O
(	O
pm	O
)	O
is	O
just	O
the	O
shatter	B
coefficient	I
of	O
all	O
sets	O
in	O
these	O
partitions	O
(	O
with	O
0	O
and	O
sm	O
included	O
in	O
the	O
collection	O
of	O
sets	O
)	O
.	O
let	O
jl	O
be	O
a	O
probability	O
measure	B
on	O
rd	O
and	O
let	O
xl	O
,	O
x	O
2	O
,	O
...	O
be	O
i.i.d	O
.	O
random	O
vectors	O
in	O
rd	O
with	O
distribution	O
jl	O
.	O
for	O
n	O
=	O
1	O
,	O
2	O
,	O
...	O
let	O
jln	O
denote	O
the	O
empirical	B
distribution	O
of	O
xl	O
,	O
''	O
''	O
x	O
n	O
,	O
which	O
places	O
mass	O
lin	O
at	O
each	O
of	O
xl	O
,	O
''	O
''	O
x	O
n	O
•	O
to	O
establish	O
the	O
consistency	B
of	O
data-driven	O
histogram	O
methods	O
,	O
we	O
require	O
information	O
about	O
the	O
large	O
deviations	O
of	O
random	O
variables	O
of	O
the	O
form	O
/lca	O
)	O
\	O
,	O
sup	O
l	O
\jln	O
(	O
a	O
)	O
-	O
p	O
(	O
m	O
)	O
e	O
:	O
f	O
(	O
m	O
)	O
aep	O
(	O
m	O
)	O
where	O
f	O
is	O
an	O
appropriate	O
family	B
of	I
partitions	O
.	O
remark	O
.	O
just	O
as	O
in	O
chapter	O
12	O
,	O
the	O
supremum	O
above	O
is	O
not	O
guaranteed	O
to	O
be	O
measurable	O
.	O
in	O
order	O
to	O
insure	O
measurability	O
,	O
it	O
is	O
necessary	O
to	O
impose	O
regularity	O
conditions	O
on	O
uncountable	O
collections	O
of	O
partitions	O
.	O
it	O
suffices	O
to	O
mention	O
that	O
in	O
all	O
our	O
applications	O
,	O
the	O
measurability	O
can	O
be	O
verified	O
by	O
checking	O
conditions	O
given	O
,	O
e.g.	O
,	O
in	O
dudley	O
(	O
1978	O
)	O
,	O
or	O
pollard	O
(	O
1984	O
)	O
.	O
0	O
the	O
following	O
theorem	B
is	O
a	O
consequence	O
of	O
the	O
vapnik	O
-chervonenkis	O
inequality	B
:	O
theorem	B
21.1	O
.	O
(	O
lugosi	O
and	O
nobel	O
(	O
1996	O
»	O
.	O
let	O
xl	O
,	O
''	O
''	O
xn	O
be	O
i.i.d	O
.	O
random	O
vectors	O
in	O
rd	O
with	O
measure	O
jl	O
and	O
empirical	B
measure	I
jln	O
.	O
let	O
f	O
be	O
a	O
collection	O
0/	O
partitions	O
a/rd	O
.	O
then/or	O
each	O
m	O
<	O
00	O
and	O
e	O
>	O
0	O
,	O
p	O
{	O
sup	O
l	O
ijln	O
(	O
a	O
)	O
-	O
jl	O
(	O
a	O
)	O
i	O
>	O
e	O
}	O
:	O
:	O
:	O
:	O
8fl.ncpm	O
)	O
e-ne2/512	O
+	O
e-ne	O
2	O
/2	O
.	O
p	O
(	O
m	O
)	O
e	O
:	O
pm	O
)	O
aep	O
(	O
m	O
)	O
366	O
21.	O
data-dependent	B
partitioning	O
proof	O
.	O
for	O
a	O
fixed	O
partition	B
p	O
,	O
define	O
a	O
'	O
as	O
the	O
set	O
a'=	O
u	O
a.	O
then	O
l	O
ifln	O
(	O
a	O
)	O
-	O
aep	O
(	O
m	O
)	O
fl	O
(	O
a	O
)	O
1	O
=	O
(	O
fln	O
(	O
a	O
)	O
-	O
fl	O
(	O
a	O
)	O
)	O
+	O
=	O
2	O
(	O
fln	O
(	O
a	O
'	O
)	O
-	O
fl	O
(	O
a	O
'	O
)	O
)	O
+	O
fl	O
(	O
sm	O
)	O
fln	O
(	O
sm	O
)	O
<	O
2	O
sup	O
aeb	O
(	O
p	O
(	O
m	O
)	O
ifln	O
(	O
a	O
)	O
-	O
fl	O
(	O
a	O
)	O
1	O
+	O
fl	O
(	O
sm	O
)	O
-	O
fln	O
(	O
sm	O
)	O
·	O
recall	O
that	O
the	O
class	O
of	O
sets	O
b	O
(	O
p	O
(	O
m	O
»	O
)	O
contains	O
all2ip	O
(	O
m	O
)	O
1	O
sets	O
obtained	O
by	O
unions	O
of	O
cells	O
of	O
p	O
(	O
m	O
)	O
.	O
therefore	O
,	O
<	O
2	O
sup	O
p	O
(	O
m	O
)	O
e	O
:	O
fw	O
)	O
aeb	O
(	O
p	O
(	O
m	O
)	O
u	O
{	O
sm	O
}	O
sup	O
ifln	O
(	O
a	O
)	O
-	O
fl	O
(	O
a	O
)	O
1	O
+	O
fl	O
(	O
sm	O
)	O
-	O
fln	O
(	O
sm	O
)	O
·	O
observe	O
that	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
the	O
inequality	B
is	O
a	O
uniform	O
deviation	O
of	O
the	O
empirical	B
measure	I
fln	O
from	O
fl	O
over	O
a	O
specific	O
class	O
of	O
sets	O
.	O
the	O
class	O
contains	O
all	O
sets	O
that	O
can	O
be	O
written	O
as	O
unions	O
of	O
cells	O
of	O
a	O
partition	O
p	O
(	O
m	O
)	O
in	O
the	O
class	O
of	O
partitions	O
:	O
f	O
(	O
m	O
)	O
.	O
this	O
class	O
of	O
sets	O
is	O
just	O
a	O
(	O
m	O
)	O
,	O
defined	O
above	O
.	O
the	O
theorem	B
now	O
follows	O
from	O
the	O
vapnik-chervonenkis	O
inequality	B
(	O
theorem	B
12.5	O
)	O
,	O
the	O
definition	B
of	I
/	O
:	O
}	O
.n	O
(	O
:	O
f	O
(	O
m	O
»	O
)	O
,	O
and	O
hoeffding	O
's	O
inequality	B
(	O
theorem	B
8.1	O
)	O
.	O
0	O
we	O
will	O
use	O
a	O
special	O
application	O
of	O
theorem	O
21.1	O
,	O
summarized	O
in	O
the	O
following	O
corollary	O
:	O
corollary	O
21.1.	O
let	O
(	O
xl	O
,	O
yl	O
)	O
,	O
(	O
x	O
2	O
,	O
y2	O
)	O
...	O
be	O
a	O
sequence	O
ofi.i.d	O
.	O
random	O
pairs	O
in	O
rd	O
x	O
{	O
o	O
,	O
i	O
}	O
and	O
let	O
:	O
fl	O
,	O
:	O
f2	O
,	O
.••	O
be	O
a	O
sequence	O
offamilies	O
of	O
partitions	O
of	O
rd	O
.	O
if	O
form	O
<	O
00	O
lim	O
log	O
(	O
/	O
:	O
}	O
.n	O
(	O
:	O
f~m	O
»	O
)	O
)	O
=	O
0	O
,	O
n-i	O
>	O
oo	O
n	O
then	O
sup	O
l	O
l~tyj	O
{	O
xiea	O
}	O
-e	O
{	O
yi	O
{	O
xea	O
}	O
}	O
i~o	O
p	O
(	O
m	O
)	O
ef	O
,	O
;	O
m	O
)	O
aep	O
(	O
m	O
)	O
n	O
i=l	O
with	O
probability	O
one	O
as	O
n	O
tends	O
to	O
infinity	O
.	O
21.2	O
a	O
vapnik-chervonenkis	O
inequality	B
for	O
partitions	O
367	O
proof	O
.	O
let	O
v	O
be	O
the	O
measure	B
of	O
(	O
x	O
,	O
y	O
)	O
on	O
rd	O
x	O
{	O
o	O
,	O
i	O
}	O
,	O
and	O
let	O
vn	O
be	O
the	O
empirical	B
measure	I
corresponding	O
to	O
the	O
sequence	O
(	O
xl	O
,	O
yd	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
.	O
using	O
f~m	O
)	O
we	O
define	O
a	O
family	O
g~m	O
)	O
of	O
partitions	O
ofrd	O
x	O
{	O
o	O
,	O
i	O
}	O
by	O
g~m	O
)	O
=	O
{	O
p	O
(	O
m	O
)	O
x	O
{	O
oj	O
:	O
p	O
(	O
m	O
)	O
e	O
f~m	O
)	O
}	O
u	O
{	O
p	O
(	O
m	O
)	O
x	O
{	O
i	O
}	O
:	O
p	O
(	O
m	O
)	O
e	O
f~m	O
)	O
}	O
,	O
where	O
p	O
x	O
{	O
o	O
}	O
=	O
{	O
ai	O
,	O
a	O
2	O
,	O
...	O
}	O
x	O
{	O
oj	O
=	O
{	O
ai	O
x	O
{	O
o	O
}	O
,	O
a2	O
x	O
{	O
o	O
}	O
,	O
...	O
}	O
.	O
we	O
apply	O
theorem	B
21.1	O
for	O
these	O
families	O
of	O
partitions	O
.	O
def	O
sup	O
l	O
i~	O
tyj	O
{	O
xi	O
ea	O
}	O
-e	O
{	O
yi	O
{	O
xeadl	O
sup	O
l	O
ivn	O
(	O
a	O
x	O
{	O
i	O
}	O
)	O
-	O
v	O
(	O
a	O
x	O
{	O
i	O
}	O
)	O
i	O
sup	O
l	O
ivn	O
(	O
a	O
)	O
-	O
v	O
(	O
a	O
)	O
i·	O
pcm	O
)	O
e	O
:	O
fj	O
,	O
m	O
)	O
aepcm	O
)	O
pcm	O
)	O
e	O
:	O
fj	O
,	O
m	O
)	O
aepcm	O
)	O
n	O
i=l	O
<	O
pcm	O
)	O
ey	O
,	O
;	O
m	O
)	O
aepcm	O
)	O
it	O
is	O
easy	O
to	O
see	O
that	O
~n	O
(	O
g~m	O
»	O
)	O
=	O
~n	O
(	O
f~m	O
»	O
)	O
.	O
therefore	O
the	O
stated	O
convergence	O
follows	O
from	O
theorem	B
21.1	O
and	O
the	O
borel-cantelli	O
lemma.d	O
lemma	O
21.1.	O
assume	O
that	O
the	O
family	O
f	O
(	O
m	O
)	O
is	O
such	O
that	O
the	O
number	O
of	O
cells	O
of	O
the	O
partitions	O
in	O
the	O
family	O
are	O
uniformly	O
bounded	O
,	O
that	O
is	O
,	O
there	O
is	O
a	O
constant	O
n	O
such	O
that	O
ip	O
(	O
m	O
)	O
i	O
:	O
:	O
:	O
;	O
n	O
for	O
each	O
p	O
(	O
m	O
)	O
e	O
f	O
(	O
m	O
)	O
.	O
then	O
~n	O
(	O
f	O
(	O
m	O
»	O
)	O
:	O
:	O
:	O
;	O
2n	O
~~	O
(	O
f	O
(	O
m	O
»	O
)	O
,	O
where	O
~~	O
(	O
f	O
(	O
m	O
»	O
)	O
is	O
maximal	O
number	O
of	O
different	O
ways	O
n	O
points	O
can	O
be	O
partitioned	O
by	O
members	O
of	O
f	O
(	O
m	O
)	O
.	O
example	O
.	O
flexible	O
grids	O
.	O
as	O
a	O
first	O
simple	O
example	O
,	O
let	O
us	O
take	O
in	O
fn	O
all	O
partitions	O
into	O
d-dimensional	O
grids	O
(	O
called	O
flexible	O
grids	O
as	O
they	O
may	O
be	O
visualized	O
as	O
chicken	O
(	O
cid:173	O
)	O
wire	O
fences	O
with	O
unequally	O
spaced	O
vertical	O
and	O
horizontal	O
wires	O
)	O
in	O
which	O
cells	O
are	O
made	O
up	O
as	O
cartesian	O
products	O
of	O
d	O
intervals	O
,	O
and	O
each	O
coordinate	O
axis	O
contributes	O
one	O
of	O
mn	O
intervals	O
to	O
these	O
products	O
.	O
clearly	O
,	O
if	O
p	O
is	O
a	O
member	O
partition	B
of	O
fn	O
,	O
then	O
ipi	O
=	O
m~	O
.	O
nevertheless	O
,	O
there	O
are	O
uncountably	O
many	O
p	O
's	O
,	O
as	O
there	O
are	O
uncountably	O
many	O
intervals	O
of	O
the	O
real	O
line	O
.	O
this	O
is	O
why	O
the	O
finite	O
quantity	O
~n	O
comes	O
in	O
so	O
handy	O
.	O
we	O
will	O
verify	O
later	O
that	O
for	O
each	O
m	O
,	O
!	O
>	O
'n	O
(	O
pn	O
»	O
)	O
s	O
2m~	O
(	O
n	O
+nmn	O
)	O
d	O
,	O
so	O
that	O
the	O
condition	O
of	O
corollary	O
21.1	O
is	O
fulfilled	O
when	O
m	O
d	O
lim	O
--	O
...	O
!	O
!	O
:	O
.	O
=	O
o	O
.	O
0	O
n-+oo	O
n	O
368	O
21.	O
data-dependent	B
partitioning	O
figure	O
21.2.	O
afiexible	O
grid	O
partition	O
.	O
21.3	O
consistency	B
in	O
this	O
section	O
we	O
establish	O
a	O
general	O
consistency	B
theorem	O
for	O
a	O
large	O
class	O
of	O
data-based	B
partitioning	O
rules	O
.	O
using	O
the	O
training	O
data	O
dn	O
we	O
produce	O
a	O
partition	O
pn	O
=	O
jrn	O
(	O
x	O
r	O
,	O
yr	O
,	O
...	O
,	O
x	O
n	O
,	O
yn	O
)	O
according	O
to	O
a	O
prescribed	O
rule	B
jrn	O
.	O
we	O
then	O
use	O
the	O
partition	B
p	O
n	O
in	O
conjunction	O
with	O
dn	O
to	O
produce	O
a	O
classification	O
rule	B
based	O
on	O
a	O
majority	O
vote	O
within	O
the	O
cells	O
of	O
the	O
(	O
random	O
)	O
partition	B
.	O
that	O
is	O
,	O
the	O
training	O
set	O
is	O
used	O
twice	O
and	O
it	O
is	O
this	O
feature	O
of	O
data-dependent	B
histogram	O
methods	O
that	O
distinguishes	O
them	O
from	O
regular	B
histogram	O
methods	O
.	O
formally	O
,	O
an	O
n-sample	O
partitioning	B
rule	I
for	O
nd	O
is	O
a	O
function	O
jrn	O
that	O
associates	O
every	O
n-tuple	O
of	O
pairs	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
e	O
n	O
d	O
x	O
{	O
a	O
,	O
i	O
}	O
with	O
a	O
measurable	O
partition	B
of	O
nd	O
.	O
associated	O
with	O
every	O
partitioning	B
rule	I
jrn	O
there	O
is	O
a	O
fixed	O
,	O
non	O
(	O
cid:173	O
)	O
random	O
family	B
of	I
partitions	O
;	O
:	O
:	O
;	O
1	O
=	O
{	O
jrn	O
(	O
xi	O
,	O
yi	O
,	O
···	O
,	O
xn	O
,	O
yn	O
)	O
:	O
(	O
xl	O
,	O
yi	O
)	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
e	O
nd	O
x	O
{	O
a	O
,	O
i	O
}	O
}	O
.	O
;	O
:	O
:	O
;	O
1	O
is	O
the	O
family	B
of	I
partitions	O
produced	O
by	O
the	O
partitioning	B
rule	I
jrn	O
for	O
all	O
possible	O
realizations	O
of	O
the	O
training	O
sequence	O
dn	O
.	O
when	O
a	O
partitioning	O
rule	B
jrn	O
is	O
applied	O
to	O
the	O
sequence	O
dn	O
=	O
(	O
xl	O
,	O
yr	O
)	O
,	O
•••	O
,	O
(	O
xn	O
,	O
yn	O
)	O
,	O
it	O
produces	O
a	O
random	O
partition	B
pn	O
=	O
jrncdn	O
)	O
e	O
;	O
:	O
:	O
;	O
1	O
'	O
in	O
what	O
follows	O
we	O
suppress	O
the	O
dependence	O
of	O
pn	O
on	O
dn	O
for	O
notational	O
simplicity	O
.	O
for	O
every	O
x	O
e	O
nd	O
let	O
an	O
(	O
x	O
)	O
be	O
the	O
unique	O
cell	O
of	O
pn	O
that	O
contains	O
the	O
point	O
x.	O
now	O
let	O
{	O
jri	O
,	O
jr2	O
,	O
...	O
}	O
be	O
a	O
fixed	O
sequence	O
of	B
partitioning	I
rules	I
.	O
the	O
classification	O
rule	B
gn	O
(	O
'	O
)	O
=	O
gnc	O
,	O
dn	O
)	O
is	O
defined	O
by	O
taking	O
a	O
majority	O
vote	O
among	O
the	O
classes	O
appearing	O
in	O
a	O
given	O
cell	O
of	O
pn	O
,	O
that	O
is	O
,	O
we	O
emphasize	O
here	O
that	O
the	O
partition	B
pn	O
can	O
depend	O
on	O
the	O
vectors	O
xi	O
and	O
the	O
labels	O
yi	O
•	O
first	O
we	O
establish	O
the	O
strong	B
consistency	I
of	O
the	O
rules	O
{	O
gn	O
}	O
for	O
a	O
wide	O
class	O
of	O
partitioning	B
rules	I
.	O
as	O
always	O
,	O
diam	O
(	O
a	O
)	O
denotes	O
the	O
diameter	O
of	O
a	O
set	O
a	O
,	O
that	O
is	O
,	O
the	O
maximum	O
euclidean	O
distance	B
between	O
any	O
two	O
points	O
of	O
a	O
:	O
diam	O
(	O
a	O
)	O
=	O
sup	O
/ix	O
-	O
y/i	O
.	O
x	O
,	O
yea	O
21.3	O
consistency	B
369	O
theorem	B
21.2	O
.	O
(	O
lugosi	O
and	O
nobel	O
(	O
1996	O
»	O
.	O
let	O
{	O
7rl	O
'	O
7r2	O
,	O
•••	O
}	O
be	O
afixed	O
sequence	O
of	B
partitioning	I
rules	I
,	O
and	O
for	O
each	O
n	O
let	O
fn	O
be	O
the	O
collection	O
of	O
partitions	O
associated	O
with	O
the	O
n-sample	O
partitioning	B
rule	I
7rn·	O
if	O
(	O
i	O
)	O
for	O
each	O
m	O
<	O
00	O
.	O
hm	O
n	O
--	O
+oo	O
log	O
(	O
~n	O
(	O
~m	O
)	O
)	O
=0	O
,	O
n	O
(	O
ii	O
)	O
and	O
for	O
all	O
balls	O
sb	O
and	O
all	O
y	O
>	O
0	O
,	O
lim	O
jj.-	O
(	O
{	O
x	O
:	O
diam	O
(	O
an	O
(	O
x	O
)	O
n	O
sm	O
)	O
>	O
y	O
}	O
)	O
=	O
°	O
n	O
--	O
+oo	O
with	O
probability	O
one	O
,	O
then	O
the	O
classification	O
rule	B
{	O
gn	O
}	O
corresponding	O
to	O
7r1l	O
satisfies	O
with	O
probability	O
one	O
.	O
in	O
other	O
words	O
,	O
the	O
rule	B
{	O
gil	O
}	O
is	O
strongly	O
consistent	O
.	O
remark	O
.	O
in	O
some	O
applications	O
we	O
need	O
a	O
weaker	O
condition	O
to	O
replace	O
(	O
ii	O
)	O
in	O
the	O
theorem	B
.	O
the	O
following	O
condition	O
will	O
do	O
:	O
for	O
every	O
y	O
>	O
°	O
and	O
8	O
e	O
(	O
0	O
,	O
1	O
)	O
jj.-	O
(	O
{	O
x	O
:	O
diam	O
(	O
ail	O
(	O
x	O
)	O
n	O
t	O
)	O
>	O
y	O
}	O
)	O
=	O
°	O
with	O
probability	O
one	O
.	O
lim	O
n-+oo	O
tcr	O
,	O
d	O
:	O
jl	O
(	O
t	O
)	O
~l-o	O
inf	O
the	O
verification	O
of	O
this	O
is	O
left	O
to	O
the	O
reader	O
(	O
problem	O
21.2	O
)	O
.	O
0	O
the	O
proof	O
,	O
given	O
below	O
,	O
requires	O
quite	O
some	O
effort	O
.	O
the	O
utility	O
of	O
the	O
theorem	B
is	O
not	O
immediately	O
apparent	O
.	O
the	O
length	O
of	O
the	O
proof	O
is	O
indicative	O
of	O
the	O
generality	O
of	O
the	O
conditions	O
in	O
the	O
theorem	B
.	O
given	O
a	O
data-dependent	O
partitioning	B
rule	I
,	O
we	O
must	O
verify	O
two	O
things	O
:	O
condition	O
(	O
i	O
)	O
merely	O
relates	O
to	O
the	O
richness	O
of	O
the	O
class	O
of	O
partitions	O
of	O
rd	O
that	O
may	O
possibly	O
occur	O
,	O
such	O
as	O
flexible	O
grids	O
.	O
condition	O
(	O
ii	O
)	O
tells	O
us	O
that	O
the	O
rule	B
should	O
eventually	O
make	O
local	O
decisions	O
.	O
from	O
examples	O
in	O
earlier	O
chapters	O
,	O
it	O
should	O
be	O
obvious	O
that	O
(	O
ii	O
)	O
is	O
not	O
necessary	O
.	O
finite	O
partitions	O
of	O
rd	O
necessarily	O
have	O
component	O
sets	O
with	O
infinite	O
diameter	O
,	O
hence	O
we	O
need	O
a	O
condition	O
that	O
states	O
that	O
such	O
sets	O
have	O
small	O
jj.	O
--	O
measure	B
.	O
condition	O
(	O
ii	O
)	O
requires	O
that	O
a	O
randomly	O
chosen	O
cell	O
have	O
a	O
small	O
diameter	O
.	O
thus	O
,	O
it	O
may	O
be	O
viewed	O
as	O
the	O
``	O
with-probability-one	O
''	O
version	O
of	O
condition	O
(	O
1	O
)	O
of	O
theorem	O
6.1.	O
however	O
,	O
the	O
weaker	O
version	O
of	O
condition	O
(	O
ii	O
)	O
stated	O
in	O
the	O
above	O
remark	O
is	O
more	O
subtle	O
.	O
by	O
considering	O
examples	O
in	O
which	O
jj.-	O
has	O
bounded	O
support	B
,	O
more	O
than	O
just	O
balls	O
s	O
m	O
are	O
needed	O
,	O
as	O
the	O
sets	O
of	O
the	O
partition	B
near	O
the	O
boundary	O
of	O
the	O
support	B
may	O
all	O
have	O
infinite	O
diameter	O
as	O
well	O
.	O
hence	O
we	O
introduced	O
an	O
infimum	O
with	O
respect	O
to	O
sets	O
t	O
over	O
all	O
t	O
with	O
jj.-	O
(	O
t	O
)	O
:	O
:	O
:	O
1	O
-	O
o.	O
it	O
suffices	O
to	O
mention	O
that	O
(	O
ii	O
)	O
is	O
satisfied	O
for	O
all	O
distributions	O
in	O
some	O
of	O
the	O
examples	O
that	O
follow	O
.	O
theorem	B
21.2	O
then	O
allows	O
us	O
to	O
conclude	O
that	O
such	O
rules	O
are	O
strongly	O
universally	O
consistent	O
.	O
the	O
theorem	B
has	O
done	O
most	O
of	O
the	O
digestion	O
of	O
370	O
21.	O
data-dependent	B
partitioning	O
such	O
proofs	O
,	O
so	O
we	O
are	O
left	O
with	O
virtually	O
no	O
work	O
at	O
all	O
.	O
consistency	B
results	O
win	O
follow	O
like	O
dominos	O
falling	O
.	O
proof	O
of	O
theorem	O
21.2.	O
observe	O
that	O
the	O
partitioning	O
classifier	O
gn	O
can	O
be	O
rewritten	O
in	O
the	O
form	O
if	O
l	O
:	O
'~l	O
i	O
{	O
yi~l	O
)	O
i	O
{	O
x	O
;	O
ean	O
(	O
x	O
)	O
}	O
<	O
l	O
:	O
'~l	O
i	O
{	O
y	O
;	O
~o	O
}	O
i	O
{	O
xiea	O
!	O
l	O
(	O
x	O
)	O
}	O
fl	O
(	O
an	O
(	O
x	O
»	O
otherwise	O
.	O
-	O
fl	O
(	O
an	O
(	O
x	O
»	O
introduce	O
the	O
notation	O
17n	O
x	O
=	O
(	O
)	O
,	O
,~	O
yi	O
l	O
...	O
.-l=l	O
i	O
{	O
x	O
;	O
ean	O
(	O
x	O
»	O
)	O
nm	O
(	O
an	O
(	O
x	O
)	O
)	O
.	O
for	O
any	O
e	O
>	O
0	O
,	O
there	O
is	O
an	O
m	O
e	O
(	O
0	O
,	O
(	O
0	O
)	O
such	O
that	O
p	O
{	O
x	O
tf	O
:	O
.	O
sm	O
}	O
<	O
e.	O
thus	O
,	O
by	O
an	O
application	O
of	O
theorem	O
2.3	O
we	O
see	O
that	O
l	O
(	O
gn	O
)	O
-	O
l*	O
<	O
p	O
{	O
gn	O
(	O
x	O
)	O
=i	O
y	O
,	O
x	O
e	O
smidn	O
}	O
-	O
p	O
{	O
g*	O
(	O
x	O
)	O
=i	O
y	O
,	O
x	O
e	O
sm	O
}	O
+	O
e	O
<	O
[	O
117	O
(	O
x	O
)	O
-	O
17n	O
(	O
x	O
)	O
im	O
(	O
dx	O
)	O
+	O
[	O
jsm	O
jsm	O
\	O
(	O
1	O
-	O
17	O
(	O
x	O
)	O
)	O
-	O
17~o\x	O
)	O
im	O
(	O
dx	O
)	O
+	O
e	O
,	O
where	O
17	O
(	O
o	O
)	O
(	O
x	O
)	O
=	O
l7=1	O
(	O
l	O
-	O
ydi	O
{	O
x	O
;	O
ean	O
(	O
x	O
)	O
}	O
•	O
n	O
nm	O
(	O
an	O
(	O
x	O
)	O
)	O
by	O
symmetry	O
,	O
since	O
e	O
>	O
°	O
is	O
arbitrary	O
,	O
it	O
suffices	O
to	O
show	O
that	O
for	O
each	O
m	O
<	O
00	O
,	O
117	O
(	O
x	O
)	O
-	O
17n	O
(	O
x	O
)	O
im	O
(	O
dx	O
)	O
-+	O
°	O
with	O
probability	O
one	O
.	O
[	O
js	O
m	O
fix	O
e	O
>	O
°	O
and	O
let	O
r	O
:	O
rd	O
-+	O
r	O
be	O
a	O
continuous	O
function	O
with	O
bounded	O
support	B
such	O
that	O
ism	O
117	O
(	O
x	O
)	O
-	O
r	O
(	O
x	O
)	O
im	O
(	O
dx	O
)	O
<	O
e.	O
such	O
r	O
exists	O
by	O
theorem	B
a.8	O
.	O
now	O
define	O
the	O
auxiliary	O
functions	O
and	O
note	O
that	O
both	O
ryn	O
and	O
fn	O
are	O
piecewise	O
constant	O
on	O
the	O
cells	O
of	O
the	O
random	O
partition	B
pn	O
.	O
we	O
may	O
decompose	O
the	O
error	O
as	O
follows	O
:	O
i	O
17	O
(	O
x	O
)	O
-	O
17n	O
(	O
x	O
)	O
1	O
(	O
21.1	O
)	O
21.3	O
consistency	B
371	O
the	O
integral	O
of	O
the	O
first	O
term	O
on	O
the	O
right	O
-hand	O
side	O
is	O
smaller	O
than	O
e	O
by	O
the	O
definition	B
of	I
r.	O
for	O
the	O
third	O
term	O
we	O
have	O
r	O
ifn	O
(	O
x	O
)	O
-	O
17n	O
(	O
x	O
)	O
ip	O
,	O
(	O
dx	O
)	O
1sm	O
l	O
ie	O
{	O
y	O
i	O
{	O
xea	O
}	O
i	O
dn	O
}	O
-	O
e	O
{	O
r	O
(	O
x	O
)	O
i	O
{	O
xea	O
}	O
i	O
dn	O
}	O
i	O
a	O
ep	O
,	O
;	O
m	O
)	O
:	O
:	O
:	O
:	O
r	O
11	O
]	O
(	O
x	O
)	O
-	O
1sm	O
r	O
(	O
x	O
)	O
ip	O
,	O
(	O
dx	O
)	O
<	O
e.	O
consider	O
the	O
fourth	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
21.1	O
)	O
.	O
clearly	O
,	O
it	O
follows	O
from	O
the	O
first	O
condition	O
of	O
the	O
theorem	B
and	O
corollary	O
21.1	O
of	O
theorem	O
21.1	O
that	O
r	O
l17n	O
(	O
x	O
)	O
-	O
17n	O
(	O
x	O
)	O
ip	O
,	O
(	O
dx	O
)	O
--	O
-	O
?	O
>	O
0	O
with	O
probability	O
one	O
.	O
1sm	O
finally	O
,	O
we	O
consider	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
21.1	O
)	O
.	O
using	O
fubini	O
's	O
theorem	B
we	O
have	O
372	O
21.	O
data-dependent	B
partitioning	O
fix	O
6	O
e	O
(	O
0	O
,	O
1	O
)	O
.	O
as	O
r	O
is	O
uniformly	O
continuous	O
,	O
there	O
exists	O
a	O
number	O
y	O
>	O
osuch	O
that	O
if	O
diam	O
(	O
a	O
)	O
<	O
y	O
,	O
then	O
ir	O
(	O
x	O
)	O
-	O
r	O
(	O
y	O
)	O
1	O
<	O
6	O
for	O
every	O
x	O
,	O
yea	O
.	O
in	O
addition	O
,	O
there	O
is	O
a	O
constant	O
m	O
<	O
00	O
such	O
that	O
i	O
r	O
(	O
x	O
)	O
i	O
~	O
m	O
for	O
every	O
x	O
e	O
rd	O
.	O
fix	O
n	O
now	O
and	O
consider	O
the	O
integrals	O
_1_	O
(	O
fl	O
(	O
a	O
)	O
ja	O
ja	O
[	O
ir	O
(	O
x	O
)	O
-	O
r	O
(	O
y	O
)	O
im	O
(	O
dx	O
)	O
m	O
(	O
dy	O
)	O
appearing	O
in	O
the	O
sum	O
above	O
.	O
we	O
always	O
have	O
the	O
upper	O
bound	O
_1_	O
[	O
m	O
(	O
a	O
)	O
jaja	O
[	O
ir	O
(	O
x	O
)	O
-	O
r	O
(	O
y	O
)	O
im	O
(	O
dx	O
)	O
m	O
(	O
dy	O
)	O
~	O
2mfl	O
(	O
a	O
)	O
.	O
assume	O
now	O
that	O
a	O
e	O
p~m	O
)	O
has	O
diam	O
(	O
a	O
)	O
<	O
y.	O
then	O
we	O
can	O
write	O
summing	O
over	O
the	O
cells	O
a	O
e	O
pi~m	O
)	O
with	O
m	O
(	O
a	O
)	O
>	O
0	O
,	O
these	O
bounds	O
give	O
ir	O
(	O
x	O
)	O
-	O
(	O
jsm	O
<	O
rn	O
(	O
x	O
)	O
im	O
(	O
dx	O
)	O
l	O
2mm	O
(	O
a	O
)	O
+	O
<	O
2mm	O
(	O
{	O
x	O
:	O
diam	O
(	O
an	O
(	O
x	O
)	O
}	O
)	O
2	O
:	O
y	O
)	O
+	O
6.	O
letting	O
n	O
tend	O
to	O
infinity	O
gives	O
lim	O
sup	O
[	O
n	O
--	O
+oo	O
1sm	O
!	O
r	O
(	O
x	O
)	O
-	O
rn	O
(	O
x	O
)	O
im	O
(	O
dx	O
)	O
~	O
6	O
with	O
probability	O
one	O
by	O
the	O
second	O
condition	O
of	O
the	O
theorem	B
.	O
summarizing	O
,	O
lim	O
sup	O
[	O
11	O
]	O
(	O
x	O
)	O
-	O
17n	O
(	O
x	O
)	O
im	O
(	O
dx	O
)	O
~	O
2e	O
+	O
6	O
with	O
probability	O
one	O
.	O
11	O
--	O
+00	O
j	O
sm	O
since	O
e	O
and	O
6	O
are	O
arbitrary	O
,	O
the	O
theorem	B
is	O
proved	O
.	O
0	O
21.4	O
statistically	B
equivalent	I
blocks	I
in	O
this	O
section	O
we	O
apply	O
theorem	B
21.2	O
both	O
to	O
classifiers	O
based	O
on	O
uniform	O
spacings	O
in	O
one	O
dimension	B
,	O
and	O
to	O
their	O
extension	O
to	O
multidimensional	O
problems	O
.	O
we	O
refer	O
to	O
these	O
as	O
rules	O
based	O
on	O
statistically	O
equivalent	O
blocks	O
.	O
the	O
order	B
statistics	I
of	O
the	O
components	O
of	O
the	O
training	O
data	O
are	O
used	O
to	O
construct	O
a	O
partition	O
into	O
rectangles	O
.	O
all	O
21.4	O
statistically	B
equivalent	I
blocks	I
373	O
such	O
classifiers	O
are	O
invariant	O
with	O
respect	O
to	O
all	O
strictly	O
monotone	O
transformations	O
of	O
the	O
coordinate	O
axes	O
.	O
the	O
simplest	O
such	O
rule	B
is	O
the	O
k-spacing	B
rule	I
studied	O
in	O
chapter	O
20	O
(	O
see	O
theorem	B
20.1	O
)	O
.	O
generalizations	O
are	O
possible	O
in	O
several	O
ways	O
.	O
theorem	B
21.2	O
allows	O
us	O
to	O
handle	O
partitions	O
depending	O
on	O
the	O
whole	O
data	O
sequence-and	O
not	O
only	O
on	O
the	O
xi	O
's	O
.	O
the	O
next	O
simple	O
result	O
is	O
sometimes	O
useful	O
.	O
theorem	B
21.3.	O
consider	O
a	O
data-dependent	O
partitioning	O
classifier	O
on	O
the	O
real	O
line	O
that	O
partitions	O
r	O
into	O
intervals	O
in	O
such	O
a	O
way	O
that	O
each	O
interval	O
contains	O
at	O
least	O
an	O
and	O
at	O
most	O
bn	O
points	O
.	O
assume	O
that	O
x	O
has	O
a	O
nonatomic	O
distribution	B
.	O
then	O
the	O
classifier	B
is	O
strongly	O
consistent	O
whenever	O
an	O
-+	O
00	O
and	O
bnln	O
-+	O
0	O
as	O
n	O
-+	O
00.	O
proof	O
.	O
we	O
check	O
the	O
conditions	O
of	O
theorem	O
21.2.	O
let	O
:01	O
contain	O
all	O
partitions	O
of	O
n	O
into	O
m	O
=	O
r	O
n	O
i	O
an	O
l	O
intervals	O
.	O
since	O
for	O
each	O
m	O
,	O
all	O
partitions	O
in	O
f~m	O
)	O
have	O
at	O
most	O
m	O
cells	O
,	O
we	O
can	O
bound	O
.6.n	O
(	O
f~m	O
)	O
)	O
according	O
to	O
the	O
lemma	O
21.1.	O
by	O
the	O
lemma	O
,	O
d.n	O
(	O
:	O
f~m	O
)	O
)	O
does	O
not	O
exceed	O
2m	O
times	O
the	O
number	O
of	O
different	O
ways	O
n	O
points	O
can	O
be	O
partitioned	O
into	O
m	O
intervals	O
.	O
a	O
little	O
thought	O
confirms	O
that	O
this	O
number	O
is	O
and	O
therefore	O
,	O
let'h	O
denote	O
the	O
binary	B
entropy	O
function	O
,	O
h	O
(	O
x	O
)	O
=	O
-x	O
log	O
(	O
x	O
)	O
-	O
(	O
1	O
-	O
x	O
)	O
loge	O
1	O
-	O
x	O
)	O
for	O
x	O
e	O
(	O
0	O
,	O
1	O
)	O
.	O
note	O
that	O
h	O
is	O
symmetric	O
about	O
1/2	O
and	O
that	O
h	O
is	O
increasing	O
for	O
0	O
<	O
x	O
:	O
:	O
:	O
:	O
1/2	O
.	O
by	O
the	O
inequality	B
of	O
theorem	B
13.4	O
,	O
log	O
g	O
)	O
:	O
:	O
:	O
:	O
sh	O
(	O
tls	O
)	O
.	O
therefore	O
,	O
it	O
is	O
easy	O
to	O
see	O
that	O
<	O
m+	O
(	O
n+m	O
)	O
h	O
(	O
~	O
)	O
n+m	O
<	O
nlan	O
+	O
2nh	O
(	O
1lan	O
)	O
+	O
1.	O
as	O
'h	O
(	O
x	O
)	O
--	O
-+	O
0	O
when	O
x	O
--	O
-+	O
0	O
,	O
the	O
condition	O
an	O
-+	O
00	O
implies	O
that	O
which	O
establishes	O
condition	O
(	O
i	O
)	O
.	O
to	O
establish	O
condition	O
(	O
ii	O
)	O
of	O
theorem	O
21.2	O
,	O
we	O
proceed	O
similarly	O
to	O
the	O
proof	O
of	O
theorem	O
20.1.	O
fix	O
y	O
,	O
e	O
>	O
o.	O
there	O
exists	O
an	O
interval	O
[	O
-m	O
,	O
m	O
]	O
such	O
that	O
jl	O
(	O
[	O
-	O
m	O
,	O
my	O
)	O
<	O
e	O
,	O
and	O
consequently	O
jl	O
(	O
{	O
x	O
:	O
diam	O
(	O
an	O
(	O
x	O
)	O
)	O
>	O
y	O
}	O
)	O
:	O
:	O
:	O
:	O
e	O
+	O
fl	O
,	O
(	O
{	O
x	O
:	O
diam	O
(	O
an	O
(	O
x	O
)	O
)	O
>	O
y	O
}	O
n	O
[	O
-m	O
,	O
md	O
,	O
where	O
an	O
(	O
x	O
)	O
denotes	O
the	O
cell	O
of	O
the	O
partition	B
p	O
n	O
containing	O
x.	O
among	O
the	O
intervals	O
of	O
pn	O
,	O
there	O
can	O
be	O
at	O
most	O
2	O
+	O
2m	O
i	O
y	O
disjoint	O
intervals	O
of	O
length	O
greater	O
than	O
y	O
in	O
374	O
21.	O
data-dependent	B
partitioning	O
[	O
-m	O
,	O
mj	O
.	O
thus	O
we	O
may	O
bound	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
above	O
by	O
fl	O
(	O
{	O
x	O
:	O
diam	O
(	O
an	O
(	O
x	O
)	O
)	O
>	O
y	O
}	O
n	O
[	O
-m	O
,	O
m	O
]	O
)	O
<	O
<	O
<	O
aepn	O
(	O
2	O
+	O
2m	O
)	O
max	O
fl	O
(	O
a	O
)	O
(	O
2	O
+	O
2m	O
)	O
(	O
max	O
fln	O
(	O
a	O
)	O
+	O
max	O
ifl	O
(	O
a	O
)	O
-	O
(	O
2	O
+	O
2m	O
)	O
(	O
bn	O
+	O
sup	O
ifl	O
(	O
a	O
)	O
-	O
aepi1	O
aepi1	O
y	O
y	O
y	O
fln	O
(	O
a	O
)	O
i	O
)	O
n	O
aea	O
fln	O
(	O
a	O
)	O
i	O
)	O
,	O
where	O
a	O
is	O
the	O
set	O
of	O
all	O
intervals	O
in	O
r.	O
the	O
first	O
term	O
in	O
the	O
parenthesis	O
converges	O
to	O
zero	O
by	O
the	O
second	O
condition	O
of	O
the	O
theorem	B
,	O
while	O
the	O
second	O
term	O
goes	O
to	O
zero	O
with	O
probability	O
one	O
by	O
an	O
obvious	O
extension	O
of	O
the	O
classical	O
glivenko-cantelli	O
theorem	B
(	O
theorem	B
12.4	O
)	O
.	O
summarizing	O
,	O
we	O
have	O
shown	O
that	O
for	O
any	O
y	O
,	O
e	O
>	O
0	O
lim	O
sup	O
fl	O
(	O
{	O
x	O
:	O
diam	O
(	O
an	O
(	O
x	O
)	O
)	O
>	O
y	O
}	O
)	O
:	O
:	O
:	O
;	O
e	O
with	O
probability	O
one	O
.	O
n	O
--	O
-+cxj	O
this	O
completes	O
the	O
proof	O
.	O
0	O
the	O
d-dimensional	O
generalizations	O
of	O
k-spacing	O
rules	O
include	O
rules	O
based	O
upon	O
statistically	B
equivalent	I
blocks	I
,	O
that	O
is	O
,	O
partitions	O
with	O
sets	O
that	O
contain	O
k	O
points	O
each	O
.	O
it	O
is	O
obvious	O
that	O
one	O
can	O
proceed	O
in	O
many	O
ways	O
,	O
see	O
,	O
for	O
example	O
,	O
anderson	O
(	O
1966	O
)	O
,	O
patrick	O
(	O
1966	O
)	O
,	O
patrick	O
and	O
fisher	O
(	O
1967	O
)	O
,	O
quesenberry	O
and	O
gessaman	O
(	O
1968	O
)	O
and	O
gessaman	O
and	O
gessaman	O
(	O
1972	O
)	O
.	O
as	O
a	O
first	O
example	O
,	O
consider	O
the	O
following	O
algorithm	B
:	O
the	O
k-th	O
smallest	O
x	O
(	O
lt	O
coordinate	O
among	O
the	O
training	O
data	O
defines	O
the	O
first	O
cut	O
.	O
the	O
(	O
infinite	O
)	O
rectangle	O
with	O
n	O
-	O
k	O
points	O
is	O
cut	O
according	O
to	O
the	O
x	O
(	O
2	O
)	O
-axis	O
,	O
isolating	O
another	O
k	O
points	O
.	O
this	O
can	O
be	O
repeated	O
on	O
a	O
rotational	O
basis	O
for	O
all	O
coordinate	O
axes	O
.	O
unfortunately	O
,	O
the	O
classifier	B
obtained	O
this	O
way	O
is	O
not	O
consistent	O
.	O
to	O
see	O
this	O
,	O
observe	O
that	O
if	O
k	O
is	O
much	O
smaller	O
than	O
n-a	O
clearly	O
necessary	O
requirement	O
for	O
consistency-then	O
almost	O
all	O
cells	O
produced	O
by	O
the	O
cuts	O
are	O
long	O
and	O
thin	O
.	O
we	O
sketched	O
a	O
distribution	O
in	O
figure	O
21.3	O
for	O
which	O
the	O
error	O
probability	O
of	O
this	O
classifier	B
fails	O
to	O
converge	O
to	O
l	O
*	O
.	O
the	O
details	O
are	O
left	O
to	O
the	O
reader	O
(	O
problem	O
21.3	O
)	O
.	O
this	O
example	O
highlights	O
the	O
importance	O
of	O
condition	O
(	O
ii	O
)	O
of	O
theorem	O
21.2	O
,	O
that	O
is	O
,	O
that	O
the	O
diameters	O
of	O
the	O
cells	O
should	O
shrink	O
in	O
some	O
sense	O
as	O
n	O
--	O
-+	O
00	O
.	O
21.4	O
statistically	B
equivalent	I
blocks	I
375	O
figure	O
21.3.	O
a	O
nonconsistent	O
k-block	O
algo	O
(	O
cid:173	O
)	O
rithm	O
(	O
with	O
k	O
=	O
2	O
in	O
the	O
picture	O
)	O
.	O
1j	O
(	O
x	O
)	O
=	O
1	O
in	O
the	O
shaded	O
area	O
and	O
1j	O
(	O
x	O
)	O
=	O
0	O
in	O
the	O
white	O
area	O
.	O
o	O
o	O
o	O
rules	O
have	O
been	O
developed	O
in	O
which	O
the	O
rectangular	O
partition	B
depends	O
not	O
only	O
upon	O
the	O
xi	O
's	O
in	O
the	O
training	O
sequence	O
,	O
but	O
also	O
upon	O
the	O
yi	O
's	O
(	O
see	O
,	O
e.g.	O
,	O
henrichon	O
and	O
fu	O
(	O
1969	O
)	O
,	O
meisel	O
and	O
michalopoulos	O
(	O
1973	O
)	O
and	O
friedman	O
(	O
1977	O
»	O
.	O
for	O
ex	O
(	O
cid:173	O
)	O
ample	O
,	O
friedman	O
cuts	O
the	O
axes	O
at	O
the	O
places	O
where	O
the	O
absolute	O
differences	O
between	O
the	O
marginal	O
empirical	B
distribution	O
functions	O
are	O
largest	O
,	O
to	O
insure	O
minimal	O
empir	O
(	O
cid:173	O
)	O
ical	O
error	O
after	O
the	O
cut	O
.	O
his	O
procedure	O
is	O
based	O
upon	O
stoller	O
splits	O
(	O
see	O
chapters	O
4	O
and	O
20	O
)	O
.	O
rules	O
depending	O
on	O
the	O
coordinatewise	O
ranks	O
of	O
data	O
points	O
are	O
interesting	O
be	O
(	O
cid:173	O
)	O
cause	O
they	O
are	O
invariant	O
under	O
monotone	O
transformations	O
of	O
the	O
coordinate	O
axes	O
.	O
this	O
is	O
particularly	O
important	O
in	O
practice	O
when	O
the	O
components	O
are	O
not	O
physically	O
comparable	O
.	O
``	O
distribution-free	O
''	O
is	O
an	O
adjective	O
often	O
used	O
to	O
point	O
out	O
a	O
property	O
that	O
is	O
universally	O
valid	O
.	O
for	O
such	O
methods	O
in	O
statistics	O
,	O
see	O
the	O
survey	O
of	O
das	O
gupta	O
(	O
1964	O
)	O
.	O
statistically	O
equivalent	O
sets	O
in	O
partitions	O
are	O
called	O
``	O
distribution	B
(	O
cid:173	O
)	O
free	O
''	O
because	O
the	O
measure	B
m	O
(	O
a	O
)	O
of	O
a	O
set	O
in	O
the	O
partition	B
does	O
not	O
depend	O
upon	O
the	O
distribution	B
of	O
x.	O
we	O
already	O
noted	O
a	O
similar	O
distribution-free	O
behavior	O
for	O
k	O
-d	O
trees	O
and	O
median	B
trees	O
(	O
chapter	O
20	O
)	O
.	O
there	O
is	O
no	O
reason	O
to	O
stay	O
with	O
rectangular-shaped	O
sets	O
(	O
anderson	O
and	O
benning	O
(	O
1970	O
)	O
,	O
beakley	O
and	O
tuteur	O
(	O
1972	O
»	O
but	O
doing	O
so	O
greatly	O
simplifies	O
the	O
interpretation	O
of	O
a	O
classifier	O
.	O
in	O
this	O
book	O
,	O
to	O
avoid	O
confusion	O
,	O
we	O
reserve	O
the	O
term	O
``	O
distribution-free	O
''	O
for	O
consistency	B
results	O
or	O
other	O
theoretical	B
properties	O
that	O
hold	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
.	O
it	O
is	O
possible	O
to	O
define	O
consistent	O
partitions	O
that	O
have	O
statistically	O
equivalent	O
sets	O
.	O
to	O
fix	O
the	O
ideas	O
,	O
we	O
take	O
gessaman	O
's	O
rule	B
(	O
1970	O
)	O
as	O
our	O
prototype	B
rule	O
for	O
further	O
study	O
(	O
note	O
:	O
for	O
hypothesis	O
testing	O
,	O
this	O
partition	B
was	O
already	O
noted	O
by	O
anderson	O
(	O
1966	O
»	O
.	O
for	O
each	O
n	O
,	O
let	O
m	O
=	O
i	O
(	O
njkn	O
)	O
l/dl	O
project	O
the	O
vectors	O
xl	O
,	O
...	O
,	O
xn	O
onto	O
the	O
first	O
coordinate	O
axis	O
,	O
and	O
then	O
partition	B
the	O
data	O
into	O
m	O
sets	O
using	O
hyperplanes	O
figure	O
21.4.	O
gessaman	O
's	O
partition	B
with	O
m	O
=	O
3	O
.	O
376	O
21.	O
data-dependent	B
partitioning	O
o	O
0	O
•	O
o	O
•	O
•	O
•	O
•	O
o	O
o	O
°l	O
--	O
--	O
--	O
~·~	O
--	O
=·	O
--	O
--	O
-­	O
•	O
0	O
•	O
0	O
0	O
0	O
0	O
po	O
•	O
0	O
0	O
perpendicular	O
to	O
that	O
axis	O
,	O
in	O
such	O
a	O
way	O
that	O
each	O
set	O
contains	O
an	O
equal	O
number	O
of	O
points	O
(	O
except	O
,	O
possibly	O
,	O
the	O
rightmost	O
set	O
,	O
where	O
fewer	O
points	O
may	O
fall	O
if	O
n	O
is	O
not	O
a	O
multiple	O
of	O
m	O
)	O
.	O
we	O
obtain	O
m	O
cylindrical	O
sets	O
.	O
in	O
the	O
same	O
fashion	O
,	O
cut	O
each	O
of	O
these	O
cylindrical	O
sets	O
,	O
along	O
the	O
second	O
axis	O
,	O
into	O
m	O
boxes	O
such	O
that	O
each	O
box	O
contains	O
the	O
same	O
number	O
of	O
data	O
points	O
.	O
continuing	O
in	O
the	O
same	O
way	O
along	O
the	O
remaining	O
coordinate	O
axes	O
,	O
we	O
obtain	O
m	O
d	O
rectangular	O
cells	O
,	O
each	O
of	O
which	O
(	O
with	O
the	O
exception	O
ofthose	O
on	O
the	O
boundary	O
)	O
contains	O
about	O
kn	O
points	O
.	O
the	O
classification	O
rule	B
gn	O
uses	O
a	O
majority	O
vote	O
among	O
those	O
yi	O
's	O
for	O
which	O
xi	O
lies	O
within	O
a	O
given	O
cell	O
.	O
consistency	B
of	O
this	O
classification	O
rule	B
can	O
be	O
established	O
by	O
an	O
argument	O
similar	O
to	O
that	O
used	O
for	O
the	O
kn	O
-spacing	O
rule	B
above	O
.	O
one	O
needs	O
to	O
check	O
that	O
the	O
conditions	O
of	O
theorem	O
21.2	O
are	O
satisfied	O
.	O
the	O
only	O
minor	O
difference	O
appears	O
in	O
the	O
computation	O
of	O
lln	O
,	O
which	O
in	O
this	O
case	O
is	O
bounded	O
from	O
above	O
by	O
2md	O
(	O
n	O
:	O
m/	O
.	O
the	O
following	O
theorem	B
summarizes	O
the	O
result	O
:	O
theorem	B
21.4.	O
assume	O
that	O
the	O
marginal	O
distributions	O
of	O
x	O
in	O
nd	O
are	O
nonatomic	O
.	O
then	O
the	O
partitioning	O
classification	O
rule	B
based	O
on	O
gessaman	O
's	O
rule	B
is	O
strongly	O
consistent	O
if	O
kn	O
-	O
»	O
-	O
(	O
x	O
)	O
and	O
kn	O
/	O
n	O
-	O
»	O
-	O
0	O
as	O
n	O
tends	O
to	O
infinity	O
.	O
to	O
consider	O
distributions	O
with	O
possibly	O
atomic	O
marginals	O
,	O
the	O
partitioning	O
algo	O
(	O
cid:173	O
)	O
rithm	O
must	O
be	O
modified	O
,	O
since	O
every	O
atom	O
has	O
more	O
than	O
kn	O
points	O
falling	O
on	O
it	O
for·	O
large	O
n.	O
with	O
a	O
proper	O
modification	O
,	O
a	O
strongly	O
universally	O
consistent	B
rule	I
can	O
be	O
obtained	O
.	O
we	O
leave	O
the	O
details	O
to	O
the	O
reader	O
(	O
problem	O
21.4	O
)	O
.	O
remark	O
.	O
consistency	B
of	O
gessaman	O
's	O
classification	O
scheme	O
can	O
also	O
be	O
derived	O
from	O
the	O
results	O
of	O
gordon	O
and	O
olshen	O
(	O
1978	O
)	O
under	O
the	O
additional	O
condition	O
kn	O
/	O
fo	O
-	O
»	O
-	O
00.	O
results	O
of	O
breiman	O
,	O
friedman	O
,	O
olshen	O
,	O
and	O
stone	O
(	O
1984	O
)	O
can	O
be	O
21.5	O
partitioning	B
rules	I
based	O
on	O
clustering	B
377	O
used	O
to	O
improve	O
this	O
condition	O
to	O
kn	O
/	O
log	O
n	O
-+	O
00.	O
theorem	B
21.4	O
guarantees	O
con	O
(	O
cid:173	O
)	O
sistency	O
under	O
the	O
weakest	O
possible	O
condition	O
kn	O
-+	O
00	O
.	O
0	O
21.5	O
partitioning	B
rules	I
based	O
on	O
clustering	B
clustering	O
is	O
one	O
of	O
the	O
most	O
widely	O
used	O
methods	O
in	O
statistical	O
data	O
analysis	O
.	O
typical	O
clustering	B
schemes	O
divide	O
the	O
data	O
into	O
a	O
finite	O
number	O
of	O
disjoint	O
groups	O
by	O
minimizing	O
some	O
empirical	B
error	I
measure	O
,	O
such	O
as	O
the	O
average	O
squared	O
distance	O
from	O
cluster	O
centers	O
(	O
see	O
hartigan	O
(	O
1975	O
»	O
.	O
in	O
this	O
section	O
we	O
outline	O
the	O
application	O
of	O
our	O
results	O
to	O
classification	O
rules	O
based	O
on	O
k-means	O
clustering	B
of	O
unlabeled	O
observations	O
.	O
as	O
a	O
first	O
step	O
,	O
we	O
divide	O
xl	O
,	O
...	O
,	O
xn	O
into	O
kn	O
disjoint	O
groups	O
having	O
cluster	O
centers	O
ai	O
,	O
...	O
,	O
akn	O
e	O
rd	O
.	O
the	O
vectors	O
ai	O
,	O
...	O
,	O
akn	O
are	O
chosen	O
to	O
minimize	O
the	O
empirical	O
squared	O
euclidean	O
distance	B
error	O
,	O
over	O
all	O
the	O
nearest-neighbor	O
clustering	B
rules	O
having	O
kn	O
representatives	O
bi	O
,	O
...	O
,	O
e	O
rd	O
,	O
where	O
ii	O
.	O
ii	O
denotes	O
the	O
usual	O
euclidean	O
norm	O
.	O
note	O
that	O
the	O
choice	O
of	O
bkn	O
cluster	O
centers	O
depends	O
only	O
on	O
the	O
vectors	O
xi	O
,	O
not	O
on	O
their	O
labels	O
.	O
for	O
the	O
behavior	O
of	O
en	O
(	O
ai	O
,	O
...	O
,	O
akj	O
,	O
see	O
problem	O
29.4.	O
the	O
vectors	O
ai	O
,	O
...	O
,	O
akll	O
give	O
rise	O
to	O
a	O
voronoi	O
partition	B
p	O
n	O
=	O
{	O
a	O
1	O
,	O
...	O
,	O
akn	O
}	O
in	O
a	O
natural	O
way	O
:	O
for	O
each	O
j	O
e	O
{	O
i	O
,	O
...	O
,	O
kn	O
}	O
,	O
let	O
ties	O
are	O
broken	O
by	O
assigning	O
points	O
on	O
the	O
boundaries	O
to	O
the	O
vector	O
that	O
has	O
the	O
smallest	O
index	O
.	O
the	O
classification	O
rule	B
gn	O
is	O
defined	O
in	O
the	O
usual	O
way	O
:	O
gl1	O
(	O
x	O
)	O
is	O
a	O
majority	O
vote	O
among	O
those	O
yj	O
's	O
such	O
that	O
x	O
j	O
falls	O
in	O
an	O
(	O
x	O
)	O
.	O
if	O
the	O
measure	B
fl	O
of	O
x	O
has	O
a	O
bounded	O
support	B
,	O
theorem	B
21.2	O
shows	O
that	O
the	O
classification	O
rule	B
{	O
gl1	O
}	O
is	O
strongly	O
consistent	O
if	O
kn	O
grows	O
with	O
n	O
at	O
an	O
appropriate	O
rate	O
.	O
note	O
that	O
this	O
rule	B
is	O
just	O
another	O
of	O
the	O
prototype	B
nearest	O
neighbor	O
rules	O
that	O
we	O
discussed	O
in	O
chapter	O
19	O
.	O
378	O
21.	O
data-dependent	B
partitioning	O
figure	O
21.5.	O
example	O
of	O
partition_	O
ing	O
based	O
on	O
clustering	O
with	O
kn	O
==	O
3.	O
the	O
criterion	O
we	O
minimize	O
is	O
the	O
sum	O
of	O
the	O
squares	O
of	O
the	O
distances	O
of	O
the	O
x/s	O
to	O
the	O
a/so	O
theorem	B
21.5	O
.	O
(	O
lugosi	O
and	O
nobel	O
(	O
1996	O
)	O
)	O
.	O
assume	O
that	O
there	O
is	O
a	O
bounded	O
set	O
a	O
c	O
rd	O
such	O
that	O
pix	O
e	O
a	O
}	O
=	O
1.	O
let	O
{	O
kn	O
}	O
be	O
a	O
sequence	O
of	O
integers	O
for	O
which	O
kn	O
-+	O
00	O
and	O
-	O
-	O
-	O
-+	O
0	O
as	O
n	O
-+	O
00.	O
k~	O
logn	O
n	O
let	O
gn	O
(	O
'	O
,	O
dn	O
)	O
be	O
the	O
histogram	O
classification	O
rule	B
based	O
on	O
the	O
voronoi	O
partition	B
of	O
kn	O
cluster	O
centers	O
minimizing	O
the	O
empirical	O
squared	O
euclidean	O
distance	B
error	O
.	O
then	O
with	O
probability	O
one	O
as	O
n	O
tends	O
to	O
infinity	O
.	O
if	O
d	O
=	O
1	O
or	O
2	O
,	O
then	O
the	O
second	O
condition	O
on	O
kn	O
can	O
be	O
relaxed	O
to	O
kn	O
logn	O
-	O
-	O
-	O
-+0	O
.	O
n	O
proof	O
.	O
again	O
,	O
we	O
check	O
the	O
conditions	O
of	O
theorem	O
21.2.	O
let	O
fn	O
consist	O
of	O
au	O
voronoi	O
partitions	O
of	O
kn	O
points	O
in	O
rd	O
.	O
as	O
each	O
partition	B
consists	O
of	O
kn	O
cells	O
,	O
we	O
may	O
use	O
lemma	O
21.1	O
to	O
bound	O
/	O
)	O
..n	O
(	O
:	O
p	O
:	O
zm	O
»	O
)	O
.	O
clearly	O
,	O
boundaries	O
between	O
cells	O
are	O
subsets	O
of	O
hyperplanes	O
.	O
since	O
there	O
are	O
at	O
most	O
knckn	O
-	O
1	O
)	O
/2	O
boundaries	O
between	O
the	O
kn	O
voronoi	O
cells	O
,	O
each	O
cell	O
of	O
a	O
partition	O
in	O
fn	O
is	O
a	O
polytope	O
with	O
at	O
most	O
knckn	O
-	O
1	O
)	O
/2	O
<	O
k~	O
faces	O
.	O
by	O
theorem	B
13.9	O
,	O
n	O
fixed	O
points	O
in	O
r	O
d	O
,	O
d	O
:	O
:	O
:	O
:	O
2	O
,	O
can	O
be	O
split	O
by	O
hyperplanes	O
in	O
at	O
most	O
nd+1	O
different	O
ways	O
.	O
it	O
follows	O
that	O
for	O
each	O
21.5	O
partitioning	B
rules	I
based	O
on	O
clustering	B
379	O
m	O
<	O
00	O
,	O
f	O
:	O
:j.n	O
(	O
f~m	O
)	O
:	O
:	O
:	O
:	O
:	O
2klln	O
(	O
d+l	O
)	O
k~	O
,	O
and	O
consequently	O
1	O
-log	O
f	O
:	O
:j.n	O
(	O
fn	O
n	O
(	O
m	O
)	O
)	O
:	O
:	O
:	O
:	O
:	O
-	O
+	O
kn	O
n	O
(	O
d	O
+	O
l	O
)	O
k~	O
log	O
n	O
n	O
~	O
0	O
by	O
the	O
second	O
condition	O
on	O
the	O
sequence	O
{	O
kn	O
}	O
.	O
thus	O
condition	O
(	O
i	O
)	O
of	O
theorem	O
21.2	O
is	O
satisfied	O
.	O
it	O
remains	O
to	O
establish	O
condition	O
(	O
ii	O
)	O
of	O
theorem	O
21.2.	O
this	O
time	O
we	O
need	O
the	O
weaker	O
condition	O
mentioned	O
in	O
the	O
remark	O
after	O
the	O
theorem	B
,	O
that	O
is	O
,	O
that	O
for	O
every	O
y	O
>	O
0	O
and	O
£5	O
e	O
(	O
0	O
,	O
1	O
)	O
inf	O
t	O
:	O
m	O
(	O
t	O
)	O
:	O
:	O
:	O
.1-8	O
p	O
,	O
(	O
{	O
x	O
:	O
diam	O
(	O
an	O
(	O
x	O
)	O
n	O
t	O
)	O
>	O
y	O
}	O
)	O
~	O
0	O
with	O
probability	O
one	O
.	O
clearly	O
,	O
we	O
are	O
done	O
if	O
we	O
can	O
prove	O
that	O
there	O
is	O
a	O
sequence	O
of	O
subsets	O
tn	O
of	O
nd	O
(	O
possibly	O
depending	O
on	O
the	O
data	O
dn	O
)	O
such	O
that	O
fl	O
(	O
tn	O
)	O
~	O
1	O
with	O
probability	O
one	O
,	O
and	O
fl	O
(	O
{	O
x	O
:	O
diam	O
(	O
an	O
(	O
x	O
)	O
n	O
tn	O
)	O
>	O
y	O
}	O
)	O
=	O
o.	O
to	O
this	O
end	O
,	O
let	O
a	O
i	O
,	O
...	O
,	O
akn	O
denote	O
the	O
optimal	O
cluster	O
centers	O
corresponding	O
to	O
d	O
n	O
,	O
and	O
define	O
kn	O
'lz	O
=	O
u	O
saj	O
,	O
y/2	O
n	O
a	O
j	O
,	O
j=l	O
'lz	O
implies	O
where	O
sx	O
,	O
r	O
is	O
the	O
ball	O
of	O
radius	O
r	O
around	O
the	O
vector	O
x.	O
clearly	O
,	O
x	O
e	O
that	O
ilx	O
-	O
a	O
(	O
x	O
)	O
ii	O
<	O
y	O
/2	O
,	O
where	O
a	O
(	O
x	O
)	O
denotes	O
the	O
closest	O
cluster	O
center	O
to	O
x	O
among	O
al	O
,	O
''	O
''	O
akn	O
•	O
but	O
since	O
it	O
follows	O
that	O
and	O
fl	O
(	O
{	O
x	O
:	O
diam	O
(	O
an	O
(	O
x	O
)	O
n	O
tn	O
)	O
>	O
y	O
}	O
)	O
=	O
o.	O
it	O
remains	O
to	O
show	O
that	O
fl	O
(	O
tn	O
)	O
~	O
1	O
with	O
probability	O
one	O
as	O
n	O
~	O
00.	O
using	O
markov	O
's	O
inequality	B
,	O
we	O
may	O
write	O
<	O
e	O
{	O
minl	O
:	O
:j	O
:	O
:kn	O
iix	O
-	O
ajl121	O
dn	O
}	O
(	O
y	O
/2	O
)	O
2	O
using	O
a	O
large-deviation	O
inequality	B
for	O
the	O
empirical	O
squared	O
error	O
of	O
nearest	O
(	O
cid:173	O
)	O
neighbor	O
clustering	O
schemes	O
,	O
it	O
can	O
be	O
shown	O
(	O
see	O
problem	O
29.4	O
)	O
that	O
if	O
x	O
has	O
bounded	O
support	B
,	O
then	O
e	O
{	O
~in	O
iix	O
-	O
ajfl	O
dn	O
}	O
-	O
i	O
:	O
:	O
;	O
:	O
j	O
:	O
:	O
;	O
:	O
kn	O
min	O
b1	O
,	O
...	O
,	O
bkn	O
erd	O
e	O
{	O
~in	O
iix	O
-	O
bj112	O
}	O
~	O
0	O
l	O
:	O
:	O
;	O
:	O
j	O
:	O
:	O
;	O
:	O
kn	O
380	O
21.	O
data-dependent	B
partitioning	O
with	O
probability	O
one	O
if	O
kl1	O
log	O
n/	O
n	O
-+	O
0	O
as	O
n	O
-+	O
00.	O
moreover	O
,	O
it	O
is	O
easy	O
to	O
see	O
that	O
b	O
''	O
~~~er	O
'	O
e	O
{	O
i~~~	O
''	O
iix	O
-	O
bj	O
112	O
}	O
-	O
>	O
0	O
as	O
kn	O
-+	O
00.	O
it	O
follows	O
that	O
fl	O
(	O
tn	O
)	O
-+	O
1	O
,	O
as	O
desired	O
.	O
if	O
d	O
=	O
1	O
,	O
then	O
the	O
cells	O
of	O
the	O
voronoi	O
partition	B
are	O
intervals	O
on	O
the	O
real	O
line	O
,	O
and	O
therefore	O
,	O
~n	O
(	O
~~m	O
»	O
)	O
:	O
:	O
:	O
:	O
2kl1	O
nkn	O
.	O
similarly	O
,	O
if	O
d	O
=	O
2	O
,	O
then	O
the	O
number	O
of	O
hyperplanes	O
defining	O
the	O
voronoi	O
partition	B
increases	O
linearly	O
with	O
kn	O
.	O
to	O
see	O
this	O
,	O
observe	O
that	O
if	O
we	O
connect	O
centers	O
of	O
neighboring	O
clusters	O
by	O
edges	O
,	O
then	O
we	O
obtain	O
a	O
planar	O
graph	O
.	O
from	O
euler	O
's	O
theorem	B
(	O
see	O
,	O
e.g.	O
,	O
edelsbrunner	O
(	O
1987	O
,	O
p.242	O
)	O
)	O
,	O
the	O
number	O
of	O
edges	O
in	O
a	O
planar	B
graph	I
is	O
bounded	O
by	O
3n	O
-	O
6	O
,	O
n	O
:	O
:	O
:	O
:	O
3	O
,	O
where	O
n	O
is	O
the	O
number	O
of	O
vertices	O
.	O
thus	O
,	O
in	O
order	O
to	O
satisfy	O
condition	O
(	O
i	O
)	O
,	O
it	O
suffices	O
that	O
kn	O
log	O
n	O
/	O
n	O
--	O
+	O
0	O
in	O
both	O
cases	O
.	O
0	O
remark	O
.	O
in	O
the	O
theorem	B
above	O
we	O
assume	O
that	O
the	O
cluster	O
centers	O
a	O
]	O
,	O
...	O
,	O
ak	O
are	O
empirically	O
optimal	O
in	O
the	O
sense	O
that	O
they	O
are	O
chosen	O
to	O
minimize	O
the	O
empirical	O
squared	O
euclidean	O
distance	B
error	O
practically	O
speaking	O
,	O
it	O
is	O
hard	O
to	O
determine	O
minimum	O
.	O
to	O
get	O
around	O
this	O
difficulty	O
,	O
several	O
fast	O
algorithms	O
have	O
been	O
proposed	O
that	O
approximate	O
the	O
optimum	O
(	O
see	O
hartigan	O
(	O
1975	O
)	O
for	O
a	O
survey	O
)	O
.	O
perhaps	O
the	O
most	O
popular	O
algorithm	B
is	O
the	O
so-called	O
k-means	B
clustering	I
method	O
,	O
also	O
known	O
in	O
the	O
theory	O
of	O
quantization	O
as	O
the	O
lloyd	O
(	O
cid:173	O
)	O
max	O
algorithm	B
(	O
lloyd	O
(	O
1982	O
)	O
,	O
max	O
(	O
1960	O
)	O
,	O
linde	O
,	O
buzo	O
,	O
and	O
gray	O
(	O
1980	O
)	O
)	O
.	O
the	O
iterative	O
method	O
works	O
as	O
follows	O
:	O
step	O
1.	O
s	O
tep	O
2	O
.	O
(	O
0	O
)	O
x	O
(	O
0	O
)	O
.	O
x	O
t	O
k	O
k	O
·	O
..	O
1	O
-nd	O
llutla	O
centers	O
a	O
1	O
,	O
...	O
,	O
ak	O
e	O
1'-	O
'	O
a	O
e	O
(	O
i	O
)	O
cl	O
h	O
d	O
uster	O
t	O
e	O
ata	O
pomts	O
1	O
,	O
'	O
..	O
,	O
,	O
...	O
,	O
ak	O
into	O
k	O
sets	O
such	O
that	O
the	O
m-th	O
set	O
ci~	O
)	O
contains	O
the	O
x	O
j	O
's	O
that	O
are	O
closer	O
to	O
a~	O
)	O
than	O
to	O
any	O
other	O
center	O
.	O
ties	O
are	O
broken	O
in	O
favor	O
of	O
smaller	O
indices	O
.	O
.	O
d	O
.	O
0	O
,	O
an	O
set	O
l	O
=	O
.	O
d	O
h	O
(	O
i	O
)	O
t	O
e	O
centers	O
a	O
1	O
n	O
aroun	O
h	O
as	O
t	O
e	O
averages	O
0	O
f	O
h	O
t	O
e	O
(	O
i+l	O
)	O
'	O
...	O
,	O
ak	O
(	O
i+l	O
)	O
step	O
3.	O
determme	O
t	O
e	O
new	O
centers	O
a	O
1	O
data	O
points	O
within	O
the	O
clusters	O
:	O
h	O
a	O
(	O
i+l	O
)	O
=	O
m	O
l	O
''	O
(	O
i	O
)	O
x	O
,	O
j	O
j.xjec	O
,	O
n	O
ici~	O
)	O
i	O
step	O
4.	O
increase	O
i	O
by	O
one	O
,	O
and	O
repeat	O
steps	O
1	O
and	O
2	O
until	O
there	O
are	O
no	O
changes	O
in	O
the	O
cluster	O
centers	O
.	O
it	O
is	O
easy	O
to	O
see	O
that	O
each	O
step	O
of	O
the	O
algorithm	B
decreases	O
the	O
empirical	O
squared	O
euclidean	O
distance	B
error	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
empirical	O
squared	O
error	O
can	O
take	O
finitely	O
many	O
different	O
values	O
during	O
the	O
execution	O
of	O
the	O
algorithm	B
.	O
therefore	O
21.6	O
data-based	B
scaling	O
381	O
the	O
algorithm	B
halts	O
in	O
finite	O
time	O
.	O
by	O
inspecting	O
the	O
proof	O
of	O
theorem	O
21.5	O
,	O
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
consistency	B
can	O
also	O
be	O
proved	O
for	O
partitions	O
given	O
by	O
the	O
suboptimal	O
cluster	O
centers	O
obtained	O
by	O
the	O
k-means	O
method	O
,	O
provided	O
that	O
it	O
is	O
initialized	O
appropriately	O
(	O
see	O
problem	O
21.6	O
)	O
.	O
0	O
21.6	O
data-based	B
scaling	O
we	O
now	O
choose	O
the	O
grid	O
size	O
h	O
in	O
a	O
cubic	B
histogram	O
rule	B
in	O
a	O
data-dependent	O
manner	O
and	O
denote	O
its	O
value	O
by	O
hn	O
.	O
theorem	B
21.2	O
implies	O
the	O
following	O
general	O
result	O
:	O
theorem	B
21.6.	O
let	O
gn	O
be	O
the	O
cubic	B
histogram	O
classifier	B
based	O
on	O
a	O
partition	O
into	O
cubes	O
of	O
size	O
hn	O
.	O
if	O
(	O
a	O
)	O
(	O
b	O
)	O
lim	O
hn	O
=	O
0	O
and	O
/1	O
--	O
fcxj	O
lim	O
nh~	O
=	O
00	O
with	O
probability	O
one	O
,	O
/1	O
--	O
fcxj	O
then	O
the	O
partitioning	B
rule	I
is	O
strongly	O
universally	O
consistent	O
.	O
to	O
prove	O
the	O
theorem	B
,	O
we	O
need	O
the	O
following	O
auxiliary	O
result	O
:	O
lemma	O
21.2.	O
let	O
z	O
1	O
,	O
z2	O
,	O
...	O
be	O
a	O
sequence	O
of	O
nonnegative	O
random	O
variables	O
.	O
if	O
limn	O
--	O
fcxj	O
zn	O
=	O
0	O
with	O
probability	O
one	O
,	O
then	O
there	O
exists	O
a	O
sequence	O
an	O
.	O
}	O
0	O
of	O
positive	O
numbers	O
such	O
that	O
limn	O
--	O
fcxj	O
i	O
{	O
zn	O
?	O
:	O
.an	O
}	O
=	O
0	O
with	O
probability	O
one	O
.	O
proof	O
.	O
define	O
vn	O
=	O
supm	O
?	O
:	O
.n	O
zm	O
.	O
then	O
clearly	O
,	O
vn	O
can	O
find	O
a	O
subsequence	O
nl	O
,	O
n2	O
,	O
...	O
of	O
positive	O
integers	O
such	O
that	O
for	O
each	O
k	O
,	O
.	O
}	O
0	O
with	O
probability	O
one	O
.	O
we	O
then	O
the	O
borel-cantelli	O
lemma	O
implies	O
that	O
lim	O
i	O
{	O
vllk	O
?	O
:	O
.lj	O
k	O
}	O
=	O
0	O
with	O
probability	O
one	O
.	O
k	O
--	O
f	O
cxj	O
(	O
21.2	O
)	O
the	O
fact	O
that	O
vn	O
2	O
:	O
zn	O
and	O
(	O
21.2	O
)	O
imply	O
the	O
statement	O
.	O
0	O
proof	O
of	O
theorem	O
21.6.	O
let	O
{	O
an	O
}	O
and	O
{	O
bn	O
}	O
be	O
sequences	O
of	O
positive	O
numbers	O
with	O
an	O
<	O
bn	O
.	O
then	O
382	O
21.	O
data-dependent	B
partitioning	O
it	O
follows	O
from	O
lemma	O
21.2	O
that	O
there	O
exist	O
sequences	O
of	O
positive	O
numbers	O
{	O
an	O
}	O
and	O
{	O
bn	O
}	O
satisfying	O
an	O
<	O
bn	O
,	O
bn	O
~	O
0	O
,	O
and	O
na~	O
~	O
00	O
as	O
n	O
~	O
00	O
such	O
that	O
lim	O
i	O
{	O
hil~	O
[	O
an	O
,	O
b	O
,	O
zl	O
}	O
=	O
0	O
with	O
probability	O
one	O
.	O
n-+oo	O
therefore	O
we	O
may	O
assume	O
that	O
for	O
each	O
n	O
,	O
p	O
{	O
hn	O
e	O
[	O
an	O
,	O
bn	O
]	O
}	O
=	O
1.	O
since	O
bn	O
~	O
0	O
,	O
condition	O
(	O
ii	O
)	O
of	O
theorem	O
21.2	O
holds	O
trivially	O
,	O
as	O
all	O
diameters	O
of	O
all	O
cells	O
are	O
inferior	O
to	O
bn	O
,	O
j	O
(	O
i.	O
it	O
remains	O
to	O
check	O
condition	O
(	O
i	O
)	O
.	O
clearly	O
,	O
for	O
each	O
m	O
<	O
00	O
each	O
partition	B
in	O
:	O
f~m	O
)	O
contains	O
less	O
than	O
co	O
(	O
1/an	O
)	O
d	O
cells	O
,	O
where	O
the	O
constant	O
c~	O
depends	O
on	O
m	O
and	O
d	O
only	O
.	O
on	O
the	O
other	O
hand	O
,	O
it	O
is	O
easy	O
to	O
see	O
that	O
n	O
points	O
can	O
not	O
be	O
partitioned	O
more	O
than	O
c1n	O
(	O
1/an	O
)	O
d	O
different	O
ways	O
by	O
cubic-grid	O
partitions	O
with	O
cube	O
size	O
h	O
2	O
:	O
an	O
for	O
some	O
other	O
constant	O
c1	O
.	O
therefore	O
,	O
for	O
each	O
m	O
<	O
00	O
,	O
.6	O
.	O
(	O
:	O
f	O
(	O
m	O
)	O
)	O
<	O
2co	O
(	O
l/an	O
)	O
d	O
c1	O
n	O
a~	O
,	O
n	O
-	O
and	O
condition	O
(	O
i	O
)	O
is	O
satisfied	O
.	O
0	O
in	O
many	O
applications	O
,	O
different	O
components	O
of	O
the	O
feature	O
vector	O
x	O
correspond	O
to	O
different	O
physical	O
measurements	O
.	O
for	O
example	O
,	O
in	O
a	O
medical	O
application	O
,	O
the	O
first	O
coordinate	O
could	O
represent	O
blood	O
pressure	O
,	O
the	O
second	O
cholesterol	O
level	O
,	O
and	O
the	O
third	O
the	O
weight	O
of	O
the	O
patient	O
.	O
in	O
such	O
cases	O
there	O
is	O
no	O
reason	O
to	O
use	O
cubic	B
histograms	O
,	O
because	O
the	O
resolution	O
of	O
the	O
partition	B
hn	O
along	O
the	O
coordinate	O
axes	O
de	O
(	O
cid:173	O
)	O
pends	O
on	O
the	O
apparent	O
scaling	O
of	O
the	O
measurements	O
,	O
which	O
is	O
rather	O
arbitrary	O
.	O
then	O
one	O
can	O
use	O
scale-independent	O
partitions	O
such	O
as	O
methods	O
based	O
on	O
order	O
statistics	B
described	O
earlier	O
.	O
alternatively	O
,	O
one	O
might	O
use	O
rectangular	O
partitions	O
instead	O
of	O
cubic	O
ones	O
,	O
and	O
let	O
the	O
data	O
decide	O
the	O
scaling	O
along	O
the	O
different	O
coordinate	O
axes	O
.	O
again	O
,	O
theorem	B
21.2	O
can	O
be	O
used	O
to	O
establish	O
conditions	O
of	O
universal	O
consistency	B
of	O
the	O
classification	O
rule	B
corresponding	O
to	O
data-based	B
rectangular	O
partitions	O
:	O
theorem	B
21.7.	O
consider	O
a	O
data-dependent	O
histogram	B
rule	I
when	O
the	O
cells	O
of	O
the	O
partition	B
are	O
all	O
rectangles	O
of	O
the	O
form	O
[	O
k	O
1hn1	O
,	O
(	O
k1	O
+	O
1	O
)	O
hn1	O
)	O
x	O
..	O
,	O
x	O
[	O
kdhnd	O
,	O
(	O
kd	O
+	O
l	O
)	O
hnd	O
)	O
,	O
where	O
k	O
1	O
,	O
...	O
,	O
kd	O
run	O
through	O
the	O
set	O
of	O
integers	O
,	O
and	O
the	O
edge	O
sizes	O
of	O
the	O
rectangles	O
hn1	O
,	O
...	O
,	O
hnd	O
are	O
determinedfrom	O
the	O
data	O
dn.1fas	O
n	O
~	O
00	O
hni	O
~	O
0	O
for	O
each	O
1	O
:	O
:	O
;	O
i	O
:	O
:	O
;	O
d	O
,	O
and	O
nhnl	O
...	O
hnd	O
~	O
00	O
with	O
probability	O
one	O
,	O
then	O
the	O
data-dependent	B
rectangular	O
partitioning	B
rule	I
is	O
strongly	O
universally	O
con	O
(	O
cid:173	O
)	O
sistent	O
.	O
to	O
prove	O
this	O
,	O
just	O
check	O
the	O
conditions	O
of	O
theorem	O
21.2	O
(	O
problem	O
21.7	O
)	O
.	O
we	O
may	O
pick	O
,	O
for	O
example	O
,	O
hn1	O
,	O
...	O
,	O
hnd	O
to	O
minimize	O
the	O
resubstitution	B
estimate	O
n	O
l	O
i	O
{	O
gn	O
(	O
xi	O
)	O
:	O
:jyi	O
)	O
i==l	O
subject	O
of	O
course	O
to	O
certain	O
conditions	O
,	O
so	O
that	O
'l.1=1	O
hni	O
~	O
0	O
with	O
probability	O
one	O
,	O
yet	O
n	O
[	O
11=:1	O
hni	O
~	O
00	O
with	O
probability	O
one	O
.	O
see	O
problem	O
21.8.	O
problems	O
and	O
exercises	O
383	O
21.7	O
classification	O
trees	O
consider	O
a	O
partition	O
of	O
the	O
space	O
obtained	O
by	O
a	O
binary	O
classification	O
tree	B
in	O
which	O
each	O
node	O
dichotomizes	O
its	O
set	O
by	O
ahyperplane	O
(	O
see	O
chapter	O
20	O
for	O
more	O
on	O
classifi	O
(	O
cid:173	O
)	O
cation	O
trees	O
)	O
.	O
the	O
construction	O
of	O
the	O
tree	B
is	O
stopped	O
according	O
to	O
some	O
unspecified	O
rule	B
,	O
and	O
classification	O
is	O
by	O
majority	B
vote	I
over	O
the	O
convex	O
polytopes	O
of	O
the	O
parti-	O
tion	O
.	O
the	O
following	O
corollary	O
of	O
theorem	O
21.2	O
generalizes	O
(	O
somewhat	O
)	O
the	O
consis-	O
tency	O
results	O
in	O
the	O
book	O
by	O
breiman	O
,	O
friedman	O
,	O
olshen	O
,	O
and	O
stone	O
(	O
1984	O
)	O
:	O
theorem	B
21.8	O
.	O
(	O
lugosi	O
and	O
nobel	O
(	O
1996	O
»	O
.	O
let	O
gn	O
be	O
a	O
binary	O
tree	B
classifier	O
based	O
upon	O
at	O
most	O
mn	O
-	O
1	O
hyperplane	B
splits	O
,	O
where	O
mn	O
=	O
o	O
(	O
n/logn	O
)	O
.	O
if	O
,	O
in	O
addition	O
,	O
condition	O
(	O
ii	O
)	O
of	O
theorem	O
21.2	O
is	O
satisfied	O
,	O
then	O
gn	O
is	O
strongly	O
consistent	O
.	O
in	O
particular	O
,	O
the	O
rule	B
is	O
strongly	O
consistent	O
if	O
condition	O
(	O
ii	O
)	O
o/theorem	O
21.2	O
holds	O
and	O
every	O
cell	O
of	O
the	O
partition	B
contains	O
at	O
least	O
kn	O
points	O
,	O
where	O
kn	O
/	O
log	O
n	O
-+	O
00.	O
proof	O
.	O
to	O
check	O
condition	O
(	O
i	O
)	O
of	O
theorem	O
21.2	O
,	O
recall	O
theorem	B
13.9	O
which	O
implies	O
that	O
n	O
:	O
:	O
:	O
:	O
2	O
points	O
in	O
a	O
d-dimensional	O
euclidean	O
space	O
can	O
be	O
dichotomized	O
by	O
hyperplanes	O
in	O
at	O
most	O
nd+1	O
different	O
ways	O
.	O
from	O
this	O
,	O
we	O
see	O
that	O
the	O
number	O
of	O
different	O
ways	O
n	O
points	O
of	O
r	O
,	O
d	O
can	O
be	O
partitioned	O
by	O
the	O
rule	B
gil	O
can	O
be	O
bounded	O
by	O
as	O
there	O
are	O
not	O
more	O
than	O
mn	O
cells	O
in	O
the	O
partition	B
.	O
thus	O
,	O
by	O
the	O
assumption	O
that	O
mn	O
log	O
n	O
/	O
n	O
-+	O
°	O
we	O
have	O
11	O
-	O
og~n	O
(	O
jn	O
)	O
~	O
-	O
n	O
'l	O
mn	O
mn	O
(	O
d	O
+	O
1	O
)	O
logn	O
n	O
+	O
n	O
-+	O
0	O
,	O
so	O
that	O
condition	O
(	O
i	O
)	O
of	O
theorem	O
21.2	O
is	O
satisfied	O
.	O
for	O
the	O
second	O
part	O
of	O
the	O
statement	O
,	O
observe	O
that	O
there	O
are	O
no	O
more	O
than	O
n	O
/	O
k	O
n	O
cells	O
in	O
any	O
partition	B
,	O
and	O
that	O
the	O
tree-structured	O
nature	O
of	O
the	O
partitions	O
assures	O
that	O
gn	O
is	O
based	O
on	O
at	O
most	O
n	O
/	O
kn	O
hyperplane	B
splits	O
.	O
this	O
completes	O
the	O
proof	O
.	O
d	O
problems	O
and	O
exercises	O
problem	O
21.1.	O
let	O
p	O
be	O
a	O
partition	O
of	O
n	O
d.	O
prove	O
that	O
l	O
iiln	O
(	O
a	O
)	O
-	O
aep	O
il	O
(	O
a	O
)	O
i	O
=	O
2	O
sup	O
iiln	O
(	O
b	O
)	O
-	O
il	O
(	O
b	O
)	O
i	O
,	O
bes	O
(	O
p	O
)	O
where	O
the	O
class	O
of	O
sets	O
b	O
(	O
p	O
)	O
contains	O
all	O
sets	O
obtained	O
by	O
unions	O
of	O
cells	O
of	O
p.	O
this	O
is	O
scheffe	O
's	O
(	O
1947	O
)	O
theorem	B
for	O
partitions	O
.	O
see	O
also	O
problem	O
12.13	O
.	O
384	O
21.	O
data-dependent	B
partitioning	O
problem	O
21.2.	O
show	O
that	O
condition	O
(	O
ii	O
)	O
of	O
theorem	O
21.2	O
may	O
be	O
replaced	O
by	O
the	O
following	O
:	O
for	O
every	O
y	O
>	O
°	O
and	O
8	O
e	O
(	O
0	O
,	O
1	O
)	O
lim	O
/1-hx	O
)	O
tcrd	O
:	O
/1-	O
(	O
t	O
)	O
?	O
:	O
:	O
1-0	O
inf	O
(	O
t	O
(	O
{	O
x	O
:	O
diam	O
(	O
a	O
i1	O
(	O
x	O
)	O
n	O
t	O
)	O
>	O
y	O
}	O
)	O
=	O
°	O
with	O
probability	O
one	O
.	O
problem	O
21.3.	O
let	O
x	O
be	O
uniformly	O
distributed	O
on	O
the	O
unit	O
square	O
[	O
0	O
,	O
1	O
]	O
2.	O
let	O
77	O
(	O
x	O
)	O
=	O
1	O
if	O
x	O
(	O
l	O
)	O
:	O
s	O
2/3	O
,	O
and	O
77	O
(	O
x	O
)	O
=	O
°	O
otherwise	O
(	O
see	O
figure	O
21.3	O
)	O
.	O
consider	O
the	O
algorithm	B
when	O
first	O
the	O
x	O
(	O
l	O
)	O
-coordinate	O
is	O
cut	O
at	O
the	O
k-th	O
smallest	O
x	O
(	O
l	O
)	O
-value	O
among	O
the	O
training	O
data	O
.	O
next	O
the	O
rectangle	O
with	O
n	O
-	O
k	O
points	O
is	O
cut	O
according	O
to	O
the	O
x	O
(	O
2l-axis	O
,	O
isolating	O
another	O
k	O
points	O
.	O
this	O
is	O
repeated	O
on	O
a	O
rotational	O
basis	O
for	O
the	O
two	O
coordinate	O
axes	O
.	O
show	O
that	O
the	O
error	O
probability	O
of	O
the	O
obtained	O
partitioning	O
classifier	O
does	O
not	O
tend	O
to	O
l	O
*	O
=	O
0.	O
can	O
you	O
determine	O
the	O
asymptotic	O
error	O
probability	O
?	O
problem	O
21.4.	O
modify	O
gessaman	O
's	O
rule	B
based	O
on	O
statistically	B
equivalent	I
blocks	I
so	O
that	O
the	O
rule	B
is	O
strongly	O
universally	O
consistent	O
.	O
problem	O
21.5.	O
cut	O
each	O
axis	O
independently	O
into	O
intervals	O
containing	O
exactly	O
k	O
of	O
the	O
(	O
pro	O
(	O
cid:173	O
)	O
jected	O
)	O
data	O
points	O
.	O
the	O
i	O
-th	O
axis	O
has	O
intervals	O
a	O
l	O
,	O
i	O
,	O
a2	O
,	O
i	O
,	O
...	O
.	O
form	O
a	O
histogram	O
rule	B
that	O
takes	O
a	O
majority	O
vote	O
over	O
the	O
product	B
sets	O
a	O
il	O
,1	O
x	O
...	O
x	O
aid	O
,	O
d	O
.	O
figure	O
21.6.	O
a	O
partition	O
based	O
upon	O
the	O
method	O
obtained	O
above	O
with	O
k	O
=	O
6	O
.	O
0	O
•	O
•	O
0	O
\i	O
\i	O
\i	O
cii	O
\i	O
\i	O
0	O
•	O
0	O
0	O
0	O
\i	O
0	O
\i	O
this	O
rule	B
does	O
not	O
guarantee	O
a	O
minimal	O
number	O
of	O
points	O
in	O
every	O
cell	O
.	O
nevertheless	O
,	O
if	O
k	O
d	O
=	O
o	O
(	O
n	O
)	O
,	O
k	O
-+	O
00	O
,	O
show	O
that	O
this	O
decision	O
rule	O
is	O
consistent	O
,	O
i.e.	O
,	O
that	O
e	O
{	O
la	O
}	O
-+	O
l	O
*	O
in	O
probability	O
.	O
problem	O
2l.6	O
.	O
show	O
that	O
each	O
step	O
of	O
the	O
k-means	B
clustering	I
algorithm	O
decreases	O
the	O
empirical	O
squared	O
error	O
.	O
conclude	O
that	O
theorem	B
21.5	O
is	O
also	O
true	O
if	O
the	O
clusters	O
are	O
given	O
by	O
the	O
k-means	O
algorithm	O
.	O
hint	O
:	O
observe	O
that	O
the	O
only	O
property	O
of	O
the	O
clusters	O
used	O
in	O
the	O
proof	O
of	O
theorem	O
21.5	O
is	O
that	O
e	O
{	O
1	O
;	O
n~~11	O
iix	O
-	O
aji121	O
dn	O
}	O
-+	O
°	O
with	O
probability	O
one	O
.	O
this	O
can	O
be	O
proven	O
for	O
clusters	O
given	O
by	O
the	O
k-means	O
algorithm	O
if	O
it	O
is	O
appropriately	O
initialized	O
.	O
to	O
this	O
end	O
,	O
use	O
the	O
techniques	O
of	O
problem	O
29.4.	O
problem	O
21.7.	O
prove	O
theorem	B
21.7.	O
problem	O
21.8.	O
consider	O
the	O
hni	O
's	O
in	O
theorem	O
21.7	O
,	O
the	O
interval	O
sizes	O
for	O
cubic	B
histograms	O
.	O
let	O
the	O
hni	O
's	O
be	O
found	O
by	O
minimizing	O
n	O
l	O
i	O
{	O
gn	O
(	O
x	O
;	O
ljyil	O
i=	O
]	O
problems	O
and	O
exercises	O
385	O
subject	O
to	O
the	O
condition	O
that	O
each	O
marginal	O
interval	O
contain	O
at	O
least	O
k	O
data	O
points	O
(	O
that	O
is	O
,	O
at	O
least	O
k	O
data	O
points	O
have	O
that	O
one	O
coordinate	O
in	O
the	O
interval	O
)	O
.	O
under	O
which	O
condition	O
on	O
k	O
is	O
the	O
rule	B
consistent	O
?	O
problem	O
21.9.	O
take	O
a	O
histogram	O
rule	B
with	O
data-dependent	B
sizes	O
hnl	O
,	O
..•	O
,	O
hnd	O
as	O
in	O
theo	O
(	O
cid:173	O
)	O
rem	O
21.7	O
,	O
defined	O
as	O
follows	O
:	O
ni	O
-	O
max	O
l~j~n	O
(	O
1	O
)	O
(	O
2	O
)	O
hni	O
=	O
(	O
wni	O
-	O
~li	O
)	O
/nl/	O
(	O
2d	O
)	O
,	O
1	O
:	O
s	O
i	O
:	O
s	O
d	O
,	O
where	O
~li	O
and	O
wni	O
are	O
25	O
and	O
75	O
percentiles	O
-	O
mm	O
l~j~n	O
j	O
'	O
...	O
,	O
y	O
n	O
,	O
:	O
s	O
l	O
:	O
s	O
were	O
d	O
(	O
h	O
x	O
-	O
(	O
x	O
(	O
1	O
)	O
j	O
-	O
.	O
.	O
x	O
(	O
i	O
)	O
)	O
/	O
r	O
:	O
:	O
1	O
x	O
(	O
d	O
)	O
)	O
.	O
,	O
j	O
j	O
h	O
-	O
(	O
xu	O
)	O
j	O
of	O
xii	O
)	O
,	O
...	O
,	O
x~i	O
)	O
.	O
assume	O
for	O
convenience	O
that	O
x	O
has	O
nonatomic	O
marginals	O
.	O
show	O
that	O
(	O
1	O
)	O
leads	O
sometimes	O
to	O
an	O
inconsistent	O
rule	B
,	O
even	O
if	O
d	O
=	O
1.	O
show	O
that	O
(	O
2	O
)	O
always	O
yields	O
a	O
scale-invariant	O
consistent	B
rule	I
.	O
22	O
splitting	B
the	I
data	I
22.1	O
the	O
holdout	B
estimate	O
universal	B
consistency	I
gives	O
us	O
a	O
partial	O
satisfaction-without	O
knowing	O
the	O
under	O
(	O
cid:173	O
)	O
lying	O
distribution	O
,	O
taking	O
more	O
samples	O
is	O
guaranteed	O
to	O
push	O
us	O
close	O
to	O
the	O
bayes	O
rule	B
in	O
the	O
long	O
run	O
.	O
unfortunately	O
,	O
we	O
will	O
never	O
know	O
just	O
how	O
close	O
we	O
are	O
to	O
the	O
bayes	O
rule	B
unless	O
we	O
are	O
given	O
more	O
information	O
about	O
the	O
unknown	O
distribution	B
(	O
see	O
chapter	O
7	O
)	O
.	O
a	O
more	O
modest	O
goal	O
is	O
to	O
do	O
as	O
well	O
as	O
possible	O
within	O
a	O
given	O
class	O
of	O
rules	O
.	O
to	O
fix	O
the	O
ideas	O
,	O
consider	O
all	O
nearest	B
neighbor	I
rules	I
based	O
upon	O
metrics	O
of	O
the	O
form	O
d	O
iixl12	O
=	O
laix	O
(	O
i	O
)	O
2	O
,	O
i=l	O
where	O
ai	O
:	O
:	O
:	O
:	O
0	O
for	O
all	O
i	O
and	O
x	O
=	O
(	O
x	O
(	O
l	O
)	O
,	O
...	O
,	O
x	O
(	O
d	O
»	O
)	O
.	O
here	O
the	O
ai	O
's	O
are	O
variable	B
scale	O
fac	O
(	O
cid:173	O
)	O
tors	O
.	O
let	O
<	O
pn	O
be	O
a	O
particular	O
nearest	B
neighbor	I
rule	I
for	O
a	O
given	O
choice	O
of	O
(	O
al	O
,	O
...	O
,	O
ad	O
)	O
,	O
and	O
let	O
gn	O
be	O
a	O
data-based	O
rule	B
chosen	O
from	O
this	O
class	O
.	O
the	O
best	O
we	O
can	O
hope	O
for	O
now	O
is	O
something	O
like	O
l	O
(	O
gn	O
)	O
--	O
--	O
+	O
1	O
inf	O
<	O
pn	O
l	O
(	O
<	O
pn	O
)	O
in	O
probability	O
for	O
all	O
distributions	O
,	O
where	O
l	O
(	O
gn	O
)	O
=	O
p	O
{	O
gn	O
(	O
x	O
)	O
=i	O
yidn	O
}	O
is	O
the	O
conditional	O
probabil	O
(	O
cid:173	O
)	O
ity	O
of	O
error	O
for	O
gn	O
.	O
this	O
sort	O
of	O
optimality-within-a-class	O
is	O
definitely	O
achievable	O
.	O
however	O
,	O
proving	O
such	O
optimality	O
is	O
generally	O
not	O
easy	O
as	O
gn	O
depends	O
on	O
the	O
data	O
.	O
in	O
this	O
chapter	O
we	O
present	O
one	O
possible	O
methodology	O
for	O
selecting	O
provably	O
good	O
rules	O
from	O
restricted	O
classes	O
.	O
this	O
is	O
achieved	O
by	O
splitting	B
the	I
data	I
into	O
a	O
training	O
388	O
22.	O
splitting	B
the	I
data	I
sequence	O
and	O
a	O
testing	O
sequence	O
.	O
this	O
idea	O
was	O
explored	O
and	O
analyzed	O
in	O
depth	O
in	O
devroye	O
(	O
1988b	O
)	O
and	O
is	O
now	O
formalized	O
.	O
the	O
data	O
sequence	O
dn	O
is	O
split	O
into	O
a	O
training	O
sequence	O
dm	O
=	O
(	O
xl	O
,	O
yd	O
,	O
...	O
(	O
xm	O
,	O
ym	O
)	O
and	O
a	O
testing	O
sequence	O
tt	O
=	O
(	O
xnhl	O
,	O
ym+r	O
)	O
,	O
...	O
,	O
(	O
xm+l	O
,	O
ym+l	O
)	O
,	O
where	O
l	O
~	O
m	O
=	O
n.	O
the	O
sequence	O
dm	O
defines	O
a	O
class	O
of	O
classifiers	O
en	O
whose	O
members	O
are	O
denoted	O
by	O
¢m	O
(	O
'	O
)	O
=	O
¢m	O
(	O
'	O
,	O
dm	O
)	O
.	O
the	O
testing	B
sequence	I
is	O
used	O
to	O
select	O
a	O
classifier	O
from	O
cm	O
that	O
minimizes	O
the	O
error	O
count	O
this	O
error	O
estimate	O
is	O
called	O
the	O
holdout	B
estimate	O
,	O
as	O
the	O
testing	B
sequence	I
is	O
``	O
held	O
out	O
''	O
of	O
the	O
design	O
of	O
¢m	O
.	O
thus	O
,	O
the	O
selected	O
classifier	B
gn	O
e	O
cm	O
satisfies	O
lm	O
,	O
l	O
(	O
gn	O
)	O
:	O
:	O
:	O
;	O
lm	O
,	O
l	O
(	O
¢m	O
)	O
for	O
all	O
¢m	O
e	O
cm	O
.	O
the	O
subscript	O
n	O
in	O
gn	O
may	O
be	O
a	O
little	O
confusing	O
,	O
since	O
gn	O
is	O
in	O
cm	O
,	O
·	O
a	O
class	O
of	O
classifiers	O
depending	O
on	O
the	O
first	O
m	O
pairs	O
dm	O
only	O
.	O
however	O
,	O
gil	O
depends	O
on	O
the	O
entire	O
data	O
dn	O
,	O
as	O
the	O
rest	O
of	O
the	O
data	O
is	O
used	O
for	O
testing	O
the	O
classifiers	O
in	O
cm	O
.	O
we	O
are	O
interested	O
in	O
the	O
difference	O
between	O
the	O
error	O
probability	O
and	O
that	O
of	O
the	O
best	O
classifier	B
in	O
cm	O
,	O
inf	O
<	O
pill	O
ecn	O
l	O
(	O
¢m	O
)	O
.	O
note	O
that	O
l	O
(	O
¢m	O
)	O
=	O
p	O
{	O
¢m	O
(	O
x	O
)	O
=j	O
y	O
i	O
dm	O
}	O
denotes	O
the	O
error	O
probability	O
conditioned	O
on	O
dm	O
.	O
the	O
conditional	O
proba	O
(	O
cid:173	O
)	O
bility	O
is	O
small	O
when	O
most	O
testing	O
sequences	O
tt	O
pick	O
a	O
rule	O
gn	O
whose	O
error	O
probability	O
is	O
within	O
e	O
of	O
the	O
best	O
classifier	B
in	O
cm	O
.	O
we	O
have	O
already	O
addressed	O
similar	O
questions	O
in	O
chapters	O
8	O
and	O
12.	O
there	O
we	O
have	O
seen	O
(	O
lemma	O
8.2	O
)	O
,	O
that	O
l	O
(	O
gn	O
)	O
-	O
inf	O
l	O
(	O
¢m	O
)	O
:	O
:	O
:	O
;	O
2	O
sup	O
ilm	O
,	O
l	O
(	O
¢m	O
)	O
-	O
l	O
(	O
¢m	O
)	O
i	O
.	O
<	O
pmecm	O
<	O
pmecm	O
if	O
cm	O
contains	O
finitely	O
many	O
rules	O
,	O
then	O
the	O
bound	O
of	O
theorem	O
8.3	O
may	O
be	O
useful	O
:	O
if	O
we	O
take	O
m	O
=	O
1	O
=	O
n12	O
,	O
then	O
theorem	B
8.3	O
shows	O
(	O
see	O
problem	O
12.1	O
)	O
that	O
on	O
the	O
average	O
we	O
are	O
within	O
-/log	O
(	O
2e	O
icm	O
i	O
)	O
i	O
n	O
of	O
the	O
best	O
possible	O
error	O
rate	O
,	O
whatever	O
it	O
is	O
.	O
if	O
cm	O
is	O
of	O
infinite	O
cardinality	O
,	O
then	O
we	O
can	O
use	O
the	O
vapnik-chervonenkis	O
theory	O
to	O
get	O
similar	O
inequalities	O
.	O
for	O
example	O
,	O
from	O
theorem	B
12.8	O
we	O
get	O
22.2	O
consistency	B
and	O
asymptotic	B
optimality	I
389	O
and	O
consequently	O
,	O
p	O
{	O
l	O
(	O
gn	O
)	O
-	O
inf	O
l	O
(	O
¢m	O
)	O
>	O
ei	O
dm	O
}	O
s	O
4e8s	O
(	O
cm	O
,	O
l2	O
)	O
e-le2j2	O
,	O
<	O
pmecm	O
_	O
(	O
22.2	O
)	O
where	O
s	O
(	O
cm	O
,	O
i	O
)	O
is	O
the	O
l-th	O
shatter	B
coefficient	I
corresponding	O
to	O
the	O
class	O
of	O
classifiers	O
em	O
(	O
see	O
theorem	B
12.6	O
for	O
the	O
definition	O
)	O
.	O
since	O
cm	O
depends	O
on	O
the	O
training	O
data	O
dm	O
,	O
the	O
shatter	O
coefficients	O
s	O
(	O
cm	O
,	O
l	O
)	O
may	O
depend	O
on	O
d	O
m	O
,	O
too	O
.	O
however	O
,	O
usually	O
itis	O
possible	O
to	O
find	O
upper	O
bounds	O
on	O
the	O
random	O
variable	B
s	O
(	O
cm	O
,	O
i	O
)	O
that	O
depend	O
on	O
m	O
and	O
i	O
only	O
,	O
but	O
not	O
on	O
the	O
actual	O
values	O
of	O
the	O
random	O
variables	O
xl	O
,	O
y1	O
,	O
..•	O
,	O
x	O
m	O
,	O
y	O
m	O
'	O
both	O
upper	O
bounds	O
above	O
are	O
distribution-free	O
,	O
and	O
the	O
problem	O
now	O
is	O
purely	O
combinatorial	O
:	O
count	O
icm	O
i	O
(	O
this	O
is	O
usually	O
trivial	O
)	O
,	O
or	O
compute	O
s	O
(	O
cm	O
,	O
l	O
)	O
.	O
remark	O
.	O
with	O
much	O
more	O
effort	O
it	O
is	O
possible	O
to	O
obtain	O
performance	O
bounds	O
of	O
the	O
holdout	B
estimate	O
in	O
the	O
form	O
of	O
bounds	O
for	O
for	O
some	O
special	O
rules	O
where	O
¢m	O
and	O
¢n	O
are	O
carefully	O
defined	O
.	O
for	O
example	O
,	O
devroye	O
and	O
wagner	O
(	O
1979a	O
)	O
give	O
upper	O
bounds	O
when	O
both	O
¢m	O
and	O
¢n	O
are	O
k-nearest	O
neighbor	O
classifiers	O
with	O
the	O
same	O
k	O
(	O
but	O
working	O
with	O
different	O
sample	O
size	O
)	O
.	O
d	O
remark	O
.	O
minimizing	O
the	O
holdout	B
estimate	O
is	O
not	O
the	O
only	O
possibility	O
.	O
other	O
error	O
estimates	O
that	O
do	O
not	O
split	O
the	O
data	O
may	O
be	O
used	O
in	O
classifier	O
selection	B
as	O
well	O
.	O
such	O
estimates	O
are	O
discussed	O
in	O
chapters	O
23	O
,	O
24	O
,	O
and	O
31.	O
however	O
,	O
these	O
estimates	O
are	O
usually	O
tailored	O
to	O
work	O
well	O
for	O
specific	O
discrimination	O
rules	O
.	O
the	O
most	O
general	O
and	O
robust	O
method	O
is	O
certainly	O
the	O
data	O
splitting	O
described	O
here	O
.	O
d	O
22.2	O
consistency	B
and	O
asymptotic	B
optimality	I
typically	O
,	O
cm	O
becomes	O
richer	O
as	O
m	O
grows	O
,	O
and	O
it	O
is	O
natural	O
to	O
ask	O
whether	O
the	O
empirically	O
best	O
classifier	B
in	O
cm	O
is	O
consistent	O
.	O
theorem	B
22.1.	O
assume	O
that	O
from	O
each	O
cm	O
we	O
can	O
pick	O
one	O
¢m	O
such	O
that	O
the	O
sequence	O
of	O
¢m	O
's	O
is	O
consistent	O
for	O
a	O
certain	O
class	O
of	O
distributions	O
.	O
then	O
the	O
auto	O
(	O
cid:173	O
)	O
matic	O
rule	B
gn	O
defined	O
above	O
is	O
consistent	O
for	O
the	O
same	O
class	O
of	O
distributions	O
(	O
i.e.	O
,	O
el	O
(	O
gn	O
)	O
-+	O
l	O
*	O
as	O
n	O
-+	O
(	O
0	O
)	O
if	O
.	O
!	O
~~	O
10g	O
(	O
e	O
{	O
s	O
(	O
cm	O
,	O
l	O
)	O
}	O
)	O
l	O
=	O
o.	O
proof	O
.	O
decompose	O
the	O
difference	O
between	O
the	O
actual	O
error	O
probability	O
and	O
the	O
bayes	O
error	O
as	O
390	O
22.	O
splitting	B
the	I
data	I
the	O
convergence	O
of	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
is	O
a	O
direct	O
corollary	O
of	O
(	O
22.2	O
)	O
.	O
the	O
second	O
term	O
converges	O
to	O
zero	O
by	O
assumption	O
.	O
0	O
theorem	B
22.1	O
shows	O
that	O
a	O
consistent	O
rule	B
is	O
picked	O
if	O
the	O
sequence	O
of	O
cm	O
's	O
contains	O
a	O
consistent	O
rule	B
,	O
even	O
if	O
we	O
do	O
not	O
know	O
which	O
functions	O
from	O
em	O
lead	O
to	O
consistency	B
.	O
if	O
we	O
are	O
just	O
worried	O
about	O
consistency	B
,	O
theorem	B
22.1	O
reassures	O
us	O
that	O
nothing	O
is	O
lost	O
as	O
long	O
as	O
we	O
take	O
l	O
much	O
larger	O
than	O
log	O
(	O
e	O
{	O
s	O
(	O
cm	O
,	O
l	O
)	O
}	O
)	O
.	O
often	O
this	O
reduces	O
to	O
a	O
very	O
weak	B
condition	O
on	O
the	O
size	O
l	O
of	O
the	O
testing	O
set.	O
'	O
let	O
us	O
now	O
introduce	O
the	O
notion	O
of	O
asymptotic	O
optimality	O
.	O
a	O
sequence	O
of	O
rules	O
gn	O
is	O
said	O
to	O
be	O
asymptotically	O
optimal	O
for	O
a	O
given	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
when	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
-	O
l	O
*	O
.	O
hm	O
17-+00	O
e	O
{	O
inf¢mecm	O
l	O
(	O
¢m	O
)	O
}	O
-	O
l*	O
=	O
l.	O
our	O
definition	O
is	O
not	O
entirely	O
fair	O
,	O
because	O
gn	O
uses	O
n	O
observations	O
,	O
whereas	O
the	O
family	B
of	I
rules	O
in	O
the	O
denominator	O
is	O
restricted	O
to	O
using	O
m	O
observations	O
.	O
if	O
gn	O
is	O
not	O
taken	O
from	O
the	O
same	O
em	O
,	O
then	O
it	O
is	O
possible	O
to	O
have	O
a	O
ratio	O
which	O
is	O
smaller	O
than	O
one	O
.	O
but	O
if	O
gn	O
e	O
cm	O
,	O
then	O
the	O
ratio	O
always	O
is	O
at	O
least	O
one	O
.	O
that	O
is	O
why	O
the	O
definition	O
makes	O
sense	O
in	O
our	O
setup	O
.	O
when	O
our	O
selected	O
rule	B
is	O
asymptotically	O
optimal	O
,	O
we	O
have	O
achieved	O
something	O
very	O
strong	B
:	O
we	O
have	O
in	O
effect	O
picked	O
a	O
rule	O
(	O
or	O
better	O
,	O
a	O
sequence	O
of	O
rules	O
)	O
which	O
has	O
a	O
probability	O
of	O
error	O
converging	O
at	O
the	O
optimal	O
rate	O
attainable	O
within	O
the	O
sequence	O
of	O
em	O
's	O
.	O
and	O
we	O
do	O
not	O
even	O
have	O
to	O
know	O
what	O
the	O
optimal	O
rate	B
of	I
convergence	I
is	O
.	O
this	O
is	O
especially	O
important	O
in	O
nonparametric	O
rules	O
,	O
where	O
some	O
researchers	O
choose	O
smoothing	O
factors	O
based	O
on	O
theoretical	O
results	O
about	O
the	O
optimal	O
attainable	O
rate	B
of	I
convergence	I
for	O
certain	O
classes	O
of	O
distributions	O
.	O
we	O
are	O
constantly	O
faced	O
with	O
the	O
problem	O
of	O
choosing	O
between	O
parametric	O
and~	O
nonparametric	O
discriminators	O
.	O
parametric	O
discriminators	O
are	O
based	O
upon	O
an	O
under	O
(	O
cid:173	O
)	O
lying	O
model	O
in	O
which	O
a	O
finite	O
number	O
of	O
unknown	O
parameters	O
is	O
estimated	O
from	O
the	O
data	O
.	O
a	O
case	O
in	O
point	O
is	O
the	O
multivariate	B
normal	I
distribution	I
,	O
which	O
leads	O
to	O
linear	O
or	O
quadratic	O
discriminators	O
.	O
if	O
the	O
model	O
is	O
wrong	O
,	O
parametric	O
methods	O
can	O
perform	O
very	O
poorly	O
;	O
when	O
the	O
model	O
is	O
right	O
,	O
their	O
performance	O
is	O
difficult	O
to	O
beat	O
.	O
the	O
method	O
based	O
on	O
splitting	B
the	I
data	I
chooses	O
among	O
the	O
best	O
discriminator	O
depending	O
upon	O
which	O
happens	O
to	O
be	O
best	O
for	O
the	O
given	O
data	O
.	O
we	O
can	O
throw	O
in	O
em	O
a	O
variety	O
of	O
rules	O
,	O
including	O
nearest	B
neighbor	I
rules	I
,	O
a	O
few	O
linear	O
discriminators	O
,	O
a	O
couple	O
of	O
tree	O
classifiers	O
and	O
perhaps	O
a	O
kernel	O
rule	B
.	O
the	O
probability	O
bounds	O
above	O
can	O
be	O
used	O
when	O
the	O
complexity	O
of	O
em	O
(	O
measured	O
by	O
its	O
shatter	O
coefficients	O
)	O
does	O
not	O
get	O
out	O
of	O
hand	O
.	O
the	O
notion	O
of	O
asymptotic	O
optimality	O
can	O
be	O
too	O
strong	B
in	O
many	O
cases	O
.	O
the	O
reason	O
for	O
this	O
is	O
that	O
in	O
some	O
rare	O
lucky	O
situations	O
e	O
{	O
inf¢mecm	O
l	O
(	O
¢m	O
)	O
}	O
-	O
l	O
*	O
may	O
be	O
very	O
small	O
.	O
in	O
these	O
cases	O
it	O
is	O
impossible	O
to	O
achieve	O
asymptotic	B
optimality	I
.	O
we	O
can	O
fix	O
this	O
problem	O
by	O
introducing	O
the	O
notion	O
of	O
em	O
-optimality	O
,	O
where	O
em	O
is	O
a	O
positive	O
sequence	O
decreasing	O
to	O
0	O
with	O
m.	O
a	O
rule	O
is	O
said	O
to	O
be	O
em-optimal	O
when	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
-	O
l	O
*	O
.	O
hm	O
n-+oo	O
e	O
{	O
inf¢mecm	O
l	O
(	O
¢m	O
)	O
}	O
-	O
l*	O
=	O
l.	O
22.3	O
nearest	B
neighbor	I
rules	I
with	O
automatic	B
scaling	O
391	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
for	O
which	O
lim	O
e	O
{	O
infrpmecm	O
l	O
(	O
¢m	O
)	O
}	O
-	O
l	O
*	O
=	O
00.	O
m	O
--	O
7cxj	O
in	O
what	O
follows	O
we	O
apply	O
the	O
idea	O
of	O
data	O
splitting	O
to	O
scaled	O
nearest	B
neighbor	I
rules	I
and	O
to	O
rules	O
closely	O
related	O
to	O
the	O
data-dependent	B
partitioning	O
classifiers	O
studied	O
in	O
chapter	O
21.	O
many	O
more	O
examples	O
are	O
presented	O
in	O
chapters	O
25	O
and	O
26	O
.	O
22.3	O
nearest	B
neighbor	I
rules	I
with	O
automatic	B
scaling	O
let	O
us	O
work	O
out	O
the	O
simple	O
example	O
introduced	O
above	O
.	O
the	O
(	O
ai	O
,	O
...	O
,	O
ad	O
)	O
-nn	O
rule	B
is	O
the	O
nearest	B
neighbor	I
rule	I
for	O
the	O
metric	B
ilxll	O
=	O
where	O
x	O
=	O
(	O
x	O
(	O
l	O
)	O
,	O
...	O
,	O
xed	O
»	O
)	O
.	O
the	O
class	O
em	O
is	O
the	O
class	O
of	O
all	O
(	O
al	O
,	O
...	O
,	O
ad	O
)	O
-nn	O
rules	O
for	O
dm	O
=	O
(	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xm	O
,	O
ym	O
)	O
)	O
.	O
the	O
testing	B
sequence	I
tz	O
is	O
used	O
to	O
choose	O
(	O
ai	O
,	O
...	O
,	O
ad	O
)	O
so	O
as	O
to	O
minimize	O
the	O
holdout	B
error	O
estimate	B
.	O
in	O
order	O
to	O
have	O
it	O
suffices	O
that	O
log	O
e	O
{	O
s	O
(	O
em	O
,	O
l	O
)	O
}	O
.	O
}	O
~~	O
l	O
=0	O
.	O
this	O
puts	O
a	O
lower	O
bound	O
on	O
l.	O
to	O
get	O
this	O
lower	O
bound	O
,	O
one	O
must	O
compute	O
s	O
(	O
em	O
,	O
1	O
)	O
.	O
clearly	O
,	O
seem	O
,	O
l	O
)	O
is	O
bounded	O
by	O
the	O
number	O
of	O
ways	O
of	O
classifying	O
xm+l	O
,	O
...	O
,	O
xm+z	O
using	O
rules	O
picked	O
from	O
em	O
,	O
that	O
is	O
,	O
the	O
total	O
number	O
of	O
different	O
values	O
for	O
we	O
now	O
show	O
that	O
regardless	O
of	O
dm	O
and	O
xm+l	O
,	O
...	O
,	O
xm+z	O
,	O
for	O
n	O
:	O
:	O
:	O
:	O
4	O
we	O
have	O
this	O
sort	O
of	O
result	O
is	O
typical-the	O
bound	O
does	O
not	O
depend	O
upon	O
dm	O
or	O
tz	O
.	O
when	O
plugged	O
back	O
into	O
the	O
condition	O
of	O
convergence	O
,	O
it	O
yields	O
the	O
simple	O
condition	O
logm	O
-	O
-	O
-+	O
o.	O
l	O
in	O
fact	O
,	O
we	O
may	O
thus	O
take	O
1	O
slightly	O
larger	O
than	O
log	O
m.	O
it	O
would	O
plainly	O
be	O
silly.to	O
take	O
1	O
=	O
n12	O
,	O
as	O
we	O
would	O
thereby	O
in	O
fact	O
throwaway	O
most	O
of	O
the	O
data	O
.	O
392	O
22.	O
splitting	B
the	I
data	I
set	O
am	O
=	O
{	O
(	O
¢m	O
(	O
xm+d	O
,	O
``	O
.	O
,	O
¢m	O
(	O
xm+z	O
»	O
,	O
¢m	O
e	O
cm	O
}	O
.	O
to	O
count	O
the	O
number	O
of	O
values	O
in	O
am	O
,	O
note	O
that	O
all	O
squared	O
distances	O
can	O
be	O
written	O
as	O
''	O
xi	O
-	O
x	O
j	O
/l	O
2	O
=	O
laipk	O
,	O
i	O
,	O
j	O
,	O
d	O
k=l	O
where	O
pk	O
,	O
i	O
,	O
j	O
is	O
a	O
nonnegative	O
number	O
only	O
depending	O
upon	O
xi	O
and	O
x	O
j.	O
note	O
that	O
each	O
squared	O
distance	O
is	O
linear	O
in	O
(	O
ar	O
,	O
...	O
,	O
aj	O
)	O
.	O
now	O
consider	O
the	O
space	O
of	O
(	O
ar	O
,	O
...	O
,	O
a	O
;	O
)	O
.	O
observe	O
that	O
,	O
in	O
this	O
space	O
,	O
¢m	O
(	O
xm+d	O
is	O
constant	O
within	O
each	O
cell	O
of	O
the	O
partition	B
determined	O
by	O
the	O
(	O
;	O
)	O
hyperplanes	O
laipkj	O
,	O
m+l	O
=	O
laipk	O
,	O
il	O
,	O
m+l	O
,	O
1	O
s	O
i	O
<	O
if	O
sm	O
.	O
d	O
k=l	O
d	O
k=l	O
to	O
see	O
this	O
,	O
note	O
that	O
within	O
each	O
set	O
in	O
the	O
partition	B
,	O
¢m	O
(	O
xm+1	O
)	O
keeps	O
the	O
same	O
nearest	B
neighbor	I
among	O
xl	O
,	O
...	O
,	O
x	O
m	O
'	O
it	O
is	O
known	O
that	O
k	O
>	O
2	O
hyperplanes	O
in	O
rd	O
create	O
partitions	O
of	O
cardinality	O
not	O
exceeding	O
k	O
d	O
(	O
see	O
problem	O
22.1	O
)	O
.	O
now	O
,	O
overlay	O
the	O
i	O
partitions	O
obtained	O
for	O
¢m	O
(	O
xm+d	O
,	O
...	O
,	O
¢m	O
(	O
xnhz	O
)	O
respectively	O
.	O
this	O
yields	O
at	O
most	O
sets	O
,	O
as	O
the	O
overlays	O
are	O
determined	O
by	O
i	O
g	O
)	O
hyperplanes	O
.	O
but	O
clearly	O
,	O
on	O
each	O
of	O
these	O
sets	O
,	O
(	O
¢m	O
(	O
xm+1	O
)	O
,	O
...	O
,	O
¢m	O
(	O
xm+z	O
»	O
is	O
constant	O
.	O
therefore	O
,	O
s	O
(	O
cm	O
,	O
/	O
)	O
:	O
:0	O
iaml	O
:	O
:0	O
«	O
;	O
)	O
)	O
d	O
22.4	O
classification	O
based	O
on	O
clustering	O
recall	O
the	O
classification	O
rule	B
based	O
on	O
clustering	B
that	O
was	O
introduced	O
in	O
chapter	O
21.	O
the	O
data	O
points	O
xl	O
,	O
...	O
,	O
xn	O
are	O
grouped	O
into	O
k	O
clusters	O
,	O
where	O
k	O
is	O
a	O
predetermined	O
integer	O
,	O
and	O
a	O
majority	O
vote	O
decides	O
within	O
the	O
k	O
clusters	O
.	O
if	O
k	O
is	O
chosen	O
such	O
that	O
k	O
--	O
+	O
00	O
and	O
k	O
2	O
log	O
n	O
/	O
n	O
--	O
+	O
0	O
,	O
then	O
the	O
rule	B
is	O
consistent	O
.	O
for	O
a	O
given	O
finite	O
n	O
,	O
however	O
,	O
these	O
conditions	O
give	O
little	O
guidance	O
.	O
also	O
,	O
the	O
choice	O
of	O
k	O
could	O
dramatically	O
affect	O
the	O
performance	O
of	O
the	O
rule	B
,	O
as	O
there	O
may	O
be	O
a	O
mismatch	O
between	O
k	O
and	O
some	O
unknown	O
natural	O
number	O
of	O
clusters	O
.	O
for	O
example	O
,	O
one	O
may	O
construct	O
distributions	O
in	O
which	O
the	O
optimal	O
number	O
of	O
clusters	O
does	O
not	O
increase	O
with	O
n.	O
let	O
us	O
split	O
the	O
data	O
and	O
let	O
the	O
testing	B
sequence	I
decide	O
the	O
value	O
of	O
k.	O
in	O
the	O
framework	O
of	O
this	O
chapter	O
,	O
em	O
contains	O
the	O
classifiers	O
based	O
on	O
the	O
first	O
m	O
pairs	O
dm	O
of	O
the	O
data	O
with	O
all	O
possible	O
values	O
of	O
k.	O
clearly	O
,	O
en	O
is	O
a	O
finite	O
family	O
with	O
icml	O
=	O
m.	O
in	O
this	O
case	O
,	O
by	O
problem	O
12.1	O
,	O
we	O
have	O
2iog	O
(	O
2m	O
)	O
i	O
the	O
consistency	B
result	O
theorem	B
21.5	O
implies	O
that	O
22.5	O
statistically	B
equivalent	I
blocks	I
393	O
for	O
all	O
distributions	O
,	O
ifthe	O
xi	O
's	O
have	O
bounded	O
support	B
.	O
thus	O
,	O
we	O
see	O
that	O
our	O
strategy	O
leads	O
to	O
a	O
universally	O
consistent	B
rule	I
whenever	O
(	O
log	O
m	O
)	O
j	O
1	O
~	O
o.	O
this	O
is	O
a	O
very	O
mild	O
condition	O
,	O
since	O
we	O
can	O
take	O
l	O
equal	O
to	O
a	O
small	O
fraction	O
of	O
n	O
,	O
without	O
sacrificing	O
consistency	B
.	O
if	O
l	O
is	O
small	O
compared	O
to	O
n	O
,	O
then	O
m	O
is	O
close	O
to	O
n	O
,	O
so	O
inf¢mecm	O
l	O
(	O
4	O
)	O
m	O
)	O
is	O
likely	O
to	O
be	O
close	O
to	O
inf¢ii	O
ecli	O
l	O
(	O
4	O
)	O
n	O
)	O
.	O
thus	O
,	O
we	O
do	O
not	O
lose	O
much	O
by	O
sacrificing	O
a	O
part	O
of	O
the	O
data	O
for	O
testing	O
purposes	O
,	O
but	O
the	O
gain	O
can	O
be	O
tremendous	O
,	O
as	O
we	O
are	O
guaranteed	O
to	O
be	O
within	O
-	O
!	O
(	O
log	O
m	O
)	O
j	O
1	O
of	O
the	O
optimum	O
in	O
the	O
class	O
em	O
.	O
that	O
we	O
can	O
not	O
take	O
i	O
=	O
1	O
and	O
hope	O
to	O
obtain	O
consistency	B
should	O
be	O
obvious	O
.	O
it	O
should	O
also	O
be	O
noted	O
that	O
for	O
i	O
=	O
m	O
,	O
we	O
are	O
roughly	O
within	O
-	O
!	O
log	O
(	O
m	O
)	O
jm	O
of	O
the	O
best	O
possible	O
probability	O
of	O
error	O
within	O
the	O
family	O
.	O
also	O
the	O
empirical	B
selection	O
rule	B
is	O
jfog	O
(	O
m	O
)	O
j	O
i-optimal	O
.	O
22.5	O
statistically	B
equivalent	I
blocks	I
recall	O
from	O
chapter	O
21	O
that	O
classifiers	O
based	O
on	O
statistically	O
equivalent	O
blocks	O
typically	O
partition	B
the	O
feature	O
space	O
r	O
,	O
d	O
into	O
rectangles	O
such	O
that	O
each	O
rectangle	O
contains	O
k	O
points	O
,	O
where	O
k	O
is	O
a	O
certain	O
positive	O
integer	O
,	O
the	O
parameter	O
of	O
the	O
rule	B
.	O
this	O
may	O
be	O
done	O
in	O
several	O
different	O
ways	O
.	O
one	O
of	O
many	O
such	O
rules-the	O
rule	B
introduced	O
by	O
gessaman	O
(	O
1970	O
)	O
-	O
is	O
consistent	O
if	O
k	O
~	O
00	O
and	O
k	O
j	O
n	O
~	O
0	O
(	O
theorem	B
21.4	O
)	O
.	O
again	O
,	O
we	O
can	O
let	O
the	O
data	O
pick	O
the	O
value	O
of	O
k	O
by	O
minimizing	O
the	O
holdout	B
estimate	O
.	O
just	O
as	O
in	O
the	O
previous	O
section	O
,	O
lem	O
i	O
=	O
m	O
,	O
and	O
every	O
remark	O
mentioned	O
there	O
about	O
consistency	B
and	O
asymptotic	B
optimality	I
remains	O
true	O
for	O
this	O
case	O
as	O
well	O
.	O
we	O
can	O
enlarge	O
the	O
family	O
em	O
by	O
allowing	O
partitions	O
without	O
restrictions	O
on	O
cardinalities	O
of	O
cells	O
.	O
this	O
leads	O
very	O
quickly	O
to	O
oversized	O
families	O
of	O
rules	O
,	O
and	O
we	O
have	O
to	O
impose	O
reasonable	O
restrictions	O
.	O
consider	O
cuts	O
into	O
at	O
most	O
k	O
rectangles	O
,	O
where	O
k	O
is	O
a	O
number	O
picked	O
beforehand	O
.	O
recall	O
that	O
for	O
a	O
fixed	O
partition	B
,	O
the	O
class	O
assigned	O
to	O
every	O
rectangle	O
is	O
decided	O
upon	O
by	O
a	O
majority	O
vote	O
among	O
the	O
training	O
points	O
.	O
on	O
the	O
real	O
line	O
,	O
choosing	O
a	O
partition	O
into	O
at	O
most	O
k	O
sets	O
is	O
equivalent	O
to	O
choosing	O
k	O
-	O
1	O
cut	O
positions	O
from	O
m	O
+	O
i	O
+	O
1	O
=	O
n	O
+	O
1	O
spacings	O
between	O
all	O
test	O
and	O
training	O
points	O
.	O
hence	O
,	O
s	O
(	O
em	O
,	O
l	O
)	O
:	O
:	O
:	O
;	O
k-1	O
:	O
s	O
(	O
n+1	O
)	O
k-l.	O
n	O
+	O
1	O
)	O
(	O
for	O
consistency	B
,	O
k	O
has	O
to	O
grow	O
as	O
n	O
grows	O
.	O
it	O
is	O
easily	O
seen	O
that	O
394	O
22.	O
splitting	B
the	I
data	I
if	O
k	O
-+	O
00	O
and	O
m	O
-+	O
00.	O
to	O
achieve	O
consistency	B
of	O
the	O
selected	O
rule	B
,	O
however	O
,	O
we	O
also	O
need	O
log	O
s	O
(	O
cm	O
,	O
l	O
)	O
-	O
-	O
-	O
-	O
<	O
(	O
k	O
-	O
l	O
-	O
l	O
)	O
log	O
(	O
n	O
+	O
1	O
)	O
1	O
-+0	O
.	O
now	O
consider	O
d-dimensional	O
partitions	O
defined	O
by	O
at	O
most	O
k	O
-	O
1	O
consecutive	O
orthogonal	O
cuts	O
,	O
that	O
is	O
,	O
the	O
first	O
cut	O
divides	O
rd	O
into	O
two	O
halfspaces	O
along	O
a	O
hyper~	O
plane	O
perpendicular	O
to	O
one	O
of	O
the	O
coordinate	O
axes	O
.	O
the	O
second	O
cut	O
splits	O
one	O
of	O
the	O
halfspaces	O
into	O
two	O
parts	O
along	O
another	O
orthogonal	O
hyperplane	B
,	O
and	O
so	O
forth	O
.	O
this	O
procedure	O
yields	O
a	O
partition	O
of	O
the	O
space	O
into	O
k	O
rectangles	O
.	O
we	O
see	O
that	O
for	O
the	O
first	O
cut	O
,	O
there	O
are	O
at	O
most	O
1	O
+	O
dn	O
possible	O
combinations	O
to	O
choose	O
from	O
.	O
this	O
yields	O
the	O
loose	O
upper	O
bound	O
this	O
bound	O
is	O
also	O
valid	O
for	O
all	O
grids	O
defined	O
by	O
at	O
most	O
k	O
-	O
1	O
cuts	O
.	O
the	O
main	O
difference	O
here	O
is	O
that	O
every	O
cut	O
defines	O
two	O
halfspaces	O
of	O
r	O
d	O
,	O
and	O
not	O
two	O
hyper	O
(	O
cid:173	O
)	O
rectangles	O
of	O
a	O
cells	O
,	O
so	O
that	O
we	O
usually	O
end	O
up	O
with	O
2k	O
rectangles	O
in	O
the	O
partition	B
.	O
assume	O
that	O
cm	O
contains	O
all	O
histograms	O
with	O
partitions	O
into	O
at	O
most	O
k	O
(	O
possibly	O
infinite	O
)	O
rectangles	O
.	O
then	O
,	O
considering	O
that	O
a	O
rectangle	O
in	O
rd	O
requires	O
choosing	O
2d	O
.	O
spacings	O
between	O
all	O
test	O
and	O
training	O
points	O
,	O
two	O
per	O
coordinate	O
axis	O
,	O
see	O
feinholz	O
(	O
1979	O
)	O
for	O
more	O
work	O
on	O
such	O
partitions	O
.	O
22.6	O
binary	B
tree	O
classifiers	O
we	O
can	O
analyze	O
binary	B
tree	O
classifiers	O
from	O
the	O
same	O
viewpoint	O
.	O
recall	O
that	O
such	O
classifiers	O
are	O
represented	O
by	O
binary	B
trees	O
,	O
where	O
each	O
internal	O
node	O
corresponds	O
to	O
a	O
split	O
of	O
a	O
cell	O
by	O
a	O
hyperplane	O
,	O
and	O
the	O
terminal	O
nodes	O
represent	O
the	O
cells	O
of	O
the	O
partition	B
.	O
assume	O
that	O
there	O
are	O
k	O
cells	O
(	O
and	O
therefore	O
k	O
-	O
1	O
splits	O
leading	O
to	O
the	O
partition	B
)	O
.	O
if	O
every	O
split	O
is	O
perpendicular	O
to	O
one	O
of	O
the	O
axes	O
,	O
then	O
,	O
the	O
situation	O
is	O
the	O
same	O
as	O
in	O
the	O
previous	O
section	O
,	O
for	O
smaller	O
families	O
of	O
rules	O
whose	O
cuts	O
depend	O
upon	O
the	O
training	O
sequence	O
only	O
''	O
the	O
bound	O
is	O
pessimistic	O
.	O
others	O
have	O
proposed	O
generalizing	O
orthogonal	O
cuts	O
by	O
using	O
general	O
hyperplane	B
cuts	O
.	O
recall	O
that	O
there	O
are	O
at	O
most	O
ways	O
of	O
dichotomizing	O
n	O
points	O
in	O
nd	O
by	O
hyperplanes	O
(	O
see	O
theorem	B
13.9	O
)	O
.	O
thus	O
,	O
if	O
we	O
allow	O
up	O
to	O
k	O
-	O
1	O
internal	O
nodes	O
(	O
or	O
hyperplane	B
cuts	O
)	O
,	O
problems	O
and	O
exercises	O
395	O
s	O
(	O
em	O
,	O
l	O
)	O
:	O
:	O
:	O
:	O
;	O
(	O
1	O
+	O
nd+1	O
)	O
k-l	O
.	O
the	O
number	O
of	O
internal	O
nodes	O
has	O
to	O
be	O
restricted	O
in	O
order	O
to	O
obtain	O
consistency	B
from	O
this	O
bound	O
:	O
refer	O
to	O
chapter	O
20	O
for	O
more	O
details	O
.	O
problems	O
and	O
exercises	O
problem	O
22.1.	O
prove	O
that	O
n	O
hyperplanes	O
partition	B
n	O
d	O
into	O
at	O
most	O
2..	O
:	O
.1=0	O
(	O
7	O
)	O
contiguous	O
regions	O
when	O
d	O
:	O
:	O
:	O
n	O
(	O
schlaffii	O
(	O
1950	O
)	O
,	O
cover	O
(	O
1965	O
)	O
)	O
.	O
hint	O
:	O
proceed	O
by	O
induction	O
.	O
problem	O
22.2.	O
assume	O
that	O
gn	O
is	O
selected	O
so	O
as	O
to	O
minimize	O
the	O
holdout	B
error	O
estimate	B
lt	O
,	O
m	O
(	O
¢m	O
)	O
over	O
cm	O
,	O
the	O
class	O
of	O
rules	O
based	O
upon	O
the	O
first	O
m	O
data	O
points	O
.	O
assume	O
furthermore	O
that	O
we	O
vary	O
lover	O
[	O
10g2	O
n	O
,	O
ni2	O
]	O
,	O
and	O
that	O
we	O
pick	O
the	O
best	O
i	O
(	O
and	O
m	O
=	O
n	O
-i	O
)	O
by	O
minimizing	O
the	O
holdout	B
error	O
estimate	B
again	O
.	O
show	O
that	O
if	O
s	O
(	O
cm	O
,	O
i	O
)	O
=	O
o	O
(	O
ny	O
)	O
for	O
some	O
y	O
>	O
0	O
,	O
then	O
the	O
obtained	O
rule	B
is	O
strongly	O
universally	O
consistent	O
.	O
problem	O
22.3.	O
let	O
cm	O
be	O
the	O
class	O
of	O
(	O
ai	O
,	O
...	O
,	O
ad	O
)	O
-nn	O
rules	O
based	O
upon	O
dm	O
.	O
show	O
that	O
if	O
min	O
--	O
+	O
1	O
,	O
then	O
inf	O
l	O
(	O
¢m	O
)	O
-	O
<	O
pmecm	O
inf	O
l	O
(	O
¢n	O
)	O
--	O
+	O
0	O
<	O
pnecn	O
in	O
probability	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
for	O
which	O
x	O
has	O
a	O
density	O
.	O
conclude	O
that	O
if	O
ljlogn	O
--	O
+	O
00	O
,	O
m	O
=	O
n	O
-i	O
,	O
1	O
=	O
o	O
(	O
n	O
)	O
,	O
then	O
l	O
(	O
gn	O
)	O
-	O
inf	O
l	O
(	O
¢n	O
)	O
--	O
+	O
0	O
<	O
pnecn	O
in	O
probability	O
.	O
problem	O
22.4.	O
finding	O
the	O
best	O
split	O
.	O
this	O
exercise	O
is	O
concerned	O
with	O
the	O
automatic	B
selection	O
of	O
m	O
and	O
i	O
=	O
n	O
-	O
m.	O
if	O
gn	O
is	O
the	O
selected	O
rule	B
minimizing	O
the	O
holdout	B
estimate	O
,	O
then	O
2log	O
(	O
4s	O
(	O
cm	O
,	O
[	O
2	O
)	O
)	O
+16	O
(	O
.	O
--	O
'	O
--	O
--	O
--	O
'	O
--	O
-	O
+	O
mf	O
l	O
(	O
¢m	O
)	O
-	O
l	O
.	O
(	O
22.3	O
)	O
*	O
)	O
i	O
<	O
pmecm	O
since	O
in	O
most	O
interesting	O
cases	O
s	O
(	O
cm	O
,	O
i	O
)	O
is	O
bounded	O
from	O
above	O
as	O
a	O
polynomial	O
ofm	O
and	O
i	O
,	O
the	O
estimation	B
error	I
typically	O
decreases	O
as	O
i	O
increases	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
approximation	B
error	I
inf	O
<	O
pmecm	O
l	O
(	O
¢m	O
)	O
-	O
l	O
*	O
typically	O
decreases	O
as	O
m	O
increases	O
,	O
as	O
the	O
class	O
cm	O
gets	O
richer	O
.	O
some	O
kind	O
of	O
balance	O
between	O
the	O
two	O
terms	O
is	O
required	O
to	O
get	O
optimum	O
performance	O
.	O
we	O
may	O
use	O
the	O
empirical	B
estimates	O
lm	O
,	O
l	O
(	O
¢m	O
)	O
again	O
to	O
decide	O
which	O
value	O
of	O
m	O
we	O
wish	O
to	O
choose	O
(	O
problem	O
22.2	O
)	O
.	O
however	O
,	O
as	O
m	O
gets	O
large-and	O
therefore	O
i	O
small-the	O
class	O
cm	O
will	O
tend	O
to	O
overfit	O
the	O
data	O
tz	O
,	O
providing	O
strongly	O
optimistically	O
biased	O
estimates	O
for	O
inf	O
<	O
pmecm	O
l	O
(	O
¢m	O
)	O
.	O
to	O
prevent	O
overfitting	B
,	O
we	O
may	O
apply	O
the	O
method	O
of	O
complexity	O
regularization	B
(	O
chapter	O
18	O
)	O
.	O
by	O
(	O
22.1	O
)	O
,	O
we	O
may	O
define	O
the	O
penalty	O
term	O
by	O
rem	O
,	O
i	O
)	O
=	O
2log	O
(	O
4e	O
8e	O
{	O
s	O
(	O
cm	O
,	O
[	O
2	O
)	O
l	O
)	O
+	O
logn	O
i	O
396	O
22.	O
splitting	B
the	I
data	I
and	O
minimize	O
the	O
penalized	O
error	O
estimate	O
lm	O
,	O
l	O
(	O
¢	O
)	O
=	O
lm	O
,	O
l	O
(	O
¢	O
)	O
+	O
rem	O
,	O
l	O
)	O
over	O
all	O
¢	O
e	O
u~l=lcm	O
.	O
denote	O
the	O
selected	O
rule	B
by	O
¢	O
:	O
.	O
prove	O
that	O
for	O
every	O
n	O
and	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
,	O
.	O
(	O
mm	O
3	O
m	O
,	O
!	O
21og	O
(	O
4e	O
{	O
s	O
(	O
ci111	O
f	O
)	O
}	O
)	O
+logn+16	O
l	O
+	O
(	O
e	O
{	O
inf	O
l	O
(	O
¢m	O
)	O
-	O
l	O
*	O
}	O
)	O
)	O
+	O
~	O
.	O
y	O
n	O
<	O
pmecm	O
hint	O
:	O
proceed	O
similarly	O
to	O
the	O
proof	O
of	O
theorem	O
18.2	O
.	O
23	O
the	O
resubstitution	B
estimate	O
estimating	O
the	O
error	O
probability	O
is	O
of	O
primordial	O
importance	O
for	O
classifier	B
selection	I
.	O
the	O
method	O
explored	O
in	O
the	O
previous	O
chapter	O
attempts	O
to	O
solve	O
this	O
problem	O
by	O
using	O
a	O
testing	O
sequence	O
to	O
obtain	O
a	O
reliable	O
holdout	B
estimate	O
.	O
the	O
independence	O
of	O
testing	O
and	O
training	O
sequences	O
leads	O
to	O
a	O
rather	O
straightforward	O
analysis	O
.	O
for	O
a	O
good	O
performance	O
,	O
the	O
testing	B
sequence	I
has	O
to	O
be	O
sufficiently	O
large	O
(	O
although	O
we	O
often	O
get	O
away	O
with	O
testing	O
sequences	O
as	O
small	O
as	O
about	O
log	O
n	O
)	O
.	O
when	O
data	O
are	O
expensive	O
,	O
this	O
constitutes	O
a	O
waste	O
.	O
assume	O
that	O
we	O
do	O
not	O
split	O
the	O
data	O
and	O
use	O
the	O
same	O
sequence	O
for	O
testing	O
and	O
training	O
.	O
often	O
dangerous	O
,	O
this	O
strategy	O
nevertheless	O
works	O
if	O
the	O
class	O
of	O
rules	O
from	O
which	O
we	O
select	O
is	O
sufficiently	O
restricted	O
.	O
the	O
error	O
estimate	O
in	O
this	O
case	O
is	O
appropriately	O
called	O
the	O
resubstitution	B
estimate	O
and	O
it	O
will	O
be	O
denoted	O
by	O
l~r	O
)	O
.	O
this	O
chapter	O
explores	O
its	O
virtues	O
and	O
pitfalls	O
.	O
a	O
third	O
error	O
estimate	O
,	O
the	O
deleted	B
estimate	O
,	O
is	O
discussed	O
in	O
the	O
next	O
chapter	O
.	O
estimates	O
based	O
upon	O
other	O
paradigms	O
are	O
treated	O
briefly	O
in	O
chapter	O
31	O
.	O
23.1	O
the	O
resubstitution	B
estimate	O
the	O
resubstitution	B
estimate	O
l~r	O
)	O
counts	O
the	O
number	O
of	O
errors	O
committed	O
on	O
the	O
training	O
sequence	O
by	O
the	O
classification	O
rule	B
.	O
expressed	O
formally	O
,	O
sometimes	O
l~r	O
)	O
is	O
called	O
the	O
apparent	B
error	I
rate	I
.	O
it	O
is	O
usually	O
strongly	O
optimisti-	O
398	O
23.	O
the	O
resubstitution	B
estimate	O
cally	O
biased	O
.	O
since	O
the	O
classifier	B
gn	O
is	O
tuned	O
by	O
dn	O
,	O
it	O
is	O
intuitively	O
clear	O
that	O
gn	O
may	O
behave	O
better	O
on	O
dn	O
than	O
on	O
independent	O
data	O
.	O
the	O
best	O
way	O
to	O
demonstrate	O
this	O
biasedness	O
is	O
to	O
consider	O
the	O
i-nearest	O
neigh_	O
bor	O
rule	B
.	O
if	O
x	O
has	O
a	O
density	O
,	O
then	O
the	O
nearest	B
neighbor	I
of	O
xi	O
among	O
xl	O
,	O
...	O
,	O
xn	O
is	O
xi	O
itself	O
with	O
probability	O
one	O
.	O
therefore	O
,	O
l~r	O
)	O
==	O
0	O
,	O
regardless	O
of	O
the	O
value	O
of	O
ln	O
=	O
l	O
(	O
gn	O
)	O
.	O
in	O
this	O
case	O
the	O
resubstitution	B
estimate	O
is	O
useless	O
.	O
for	O
k-nearest	O
neigh_	O
bor	O
rules	O
with	O
large	O
k	O
,	O
l~r	O
)	O
is	O
close	O
to	O
ln	O
.	O
this	O
was	O
demonstrated	O
by	O
devroye	O
and	O
wagner	O
(	O
1979a	O
)	O
,	O
who	O
obtained	O
upper	O
bounds	O
on	O
the	O
performance	O
of	O
the	O
resubsti_	O
tution	O
estimate	B
for	O
the	O
k-nearest	O
neighbor	B
rule	I
without	O
posing	O
any	O
assumption	O
on	O
the	O
distribution	B
.	O
also	O
,	O
if	O
the	O
classifier	B
whose	O
error	O
probability	O
is	O
to	O
be	O
estimated	O
is	O
a	O
member	O
of	B
a	I
class	I
of	I
classifiers	I
with	O
finite	O
vapnik-chervonenkis	O
dimension	B
(	O
see	O
the	O
definitions	O
in	O
chapter	O
12	O
)	O
,	O
then	O
we	O
can	O
get	O
good	O
performance	O
bounds	O
for	O
the	O
resubstitution	B
estimate	O
.	O
to	O
see	O
this	O
,	O
consider	O
any	O
generalized	O
linear	O
classification	O
rule	B
,	O
that	O
is	O
,	O
any	O
rule	B
that	O
can	O
be	O
put	O
into	O
the	O
following	O
form	O
:	O
n	O
(	O
x	O
)	O
=	O
{	O
o	O
g	O
if	O
ao.n	O
-	O
:	O
l1~1	O
ai	O
,	O
n1/	O
!	O
i	O
(	O
x	O
)	O
~	O
°	O
1	O
otherwise	O
,	O
where	O
the	O
1/	O
!	O
i	O
's	O
are	O
fixed	O
functions	O
,	O
and	O
the	O
coefficients	O
ai	O
,	O
n	O
depend	O
on	O
the	O
data	O
dn	O
in	O
an	O
arbitrary	O
but	O
measurable	O
way	O
.	O
we	O
have	O
the	O
following	O
estimate	B
for	O
the	O
performance	O
of	O
the	O
resubstitution	B
estimate	O
l~	O
)	O
.	O
theorem	B
23.1	O
.	O
(	O
devroye	O
and	O
wagner	O
(	O
1976a	O
)	O
)	O
.	O
for	O
all	O
nand	O
e	O
>	O
0	O
,	O
the	O
re	O
(	O
cid:173	O
)	O
substitution	O
estimate	B
l~	O
)	O
of	O
the	O
error	O
probability	O
ln	O
of	O
a	O
generalized	O
linear	O
rule	O
satisfies	O
proof	O
.	O
define	O
the	O
set	O
an	O
c	O
rd	O
x	O
{	O
o	O
,	O
i	O
}	O
as	O
the	O
set	O
of	O
all	O
pairs	O
(	O
x	O
,	O
y	O
)	O
e	O
rd	O
x	O
{	O
o	O
,	O
1	O
}	O
,	O
on	O
which	O
gn	O
errs	O
:	O
an	O
=	O
{	O
(	O
x	O
,	O
y	O
)	O
:	O
gn	O
(	O
x	O
)	O
=i	O
y	O
}	O
.	O
observe	O
that	O
and	O
or	O
denoting	O
the	O
measure	B
of	O
(	O
x	O
,	O
y	O
)	O
by	O
1	O
)	O
,	O
and	O
the	O
corresponding	O
empirical	B
measure	I
by	O
l	O
)	O
n	O
,	O
and	O
23.2	O
histogram	O
rules	O
399	O
the	O
set	O
an	O
depends	O
on	O
the	O
data	O
dn	O
,	O
so	O
that	O
,	O
for	O
example	O
,	O
e	O
{	O
vn	O
(	O
an	O
)	O
}	O
=t	O
e	O
{	O
v	O
(	O
an	O
)	O
}	O
.	O
fortunately	O
,	O
the	O
powerful	O
vapnik-chervonenkis	O
theory	O
comes	O
to	O
the	O
rescue	O
via	O
the	O
inequality	B
iln	O
-	O
l~r	O
)	O
i	O
:	O
:	O
:	O
ssup	O
ivn	O
(	O
c	O
)	O
v	O
(	O
c	O
)	O
i	O
,	O
cec	O
where	O
c	O
is	O
the	O
family	B
of	I
all	O
sets	O
of	O
the	O
form	O
{	O
(	O
x	O
,	O
y	O
)	O
:	O
¢	O
(	O
x	O
)	O
=t	O
y	O
}	O
,	O
where	O
¢	O
:	O
rd	O
--	O
-+	O
{	O
a	O
,	O
1	O
}	O
is	O
a	O
generalized	O
linear	B
classifier	I
based	O
on	O
the	O
functions	O
0/1	O
,	O
...	O
,	O
0/	O
d*	O
.	O
by	O
theorems	O
12.6	O
,	O
13.1	O
,	O
and	O
13.7	O
we	O
have	O
p	O
{	O
sup	O
ivn	O
(	O
c	O
)	O
-	O
v	O
(	O
c	O
)	O
1	O
~	O
e	O
}	O
:	O
s	O
;	O
8nd	O
*	O
e-ne2j3z	O
.	O
0	O
cec	O
similar	O
inequalities	O
can	O
be	O
obtained	O
for	O
other	O
classifiers	O
.	O
for	O
example	O
,	O
for	O
par	O
(	O
cid:173	O
)	O
titioning	O
rules	O
with	O
fixed	O
partitions	O
we	O
have	O
the	O
following	O
:	O
theorem	B
23.2.	O
let	O
gn	O
be	O
a	O
classifier	O
whose	O
value	O
is	O
constant	O
over	O
cells	O
of	O
a	O
fixed	O
partition	B
ofrd	O
into	O
k	O
cells	O
.	O
then	O
p	O
{	O
il~r	O
)	O
-	O
lnl	O
~	O
e	O
}	O
:	O
s	O
;	O
8	O
.	O
2ke-nt2j3z	O
.	O
the	O
proof	O
is	O
left	O
as	O
an	O
exercise	O
(	O
problem	O
23.1	O
)	O
.	O
from	O
theorems	O
23.1	O
and	O
23.2	O
we	O
get	O
bounds	O
for	O
the	O
expected	O
difference	O
between	O
the	O
resubstitution	B
estimate	O
and	O
the	O
actual	O
error	O
probability	O
ln	O
.	O
for	O
example	O
,	O
theorem	B
23.1	O
implies	O
(	O
via	O
problem	O
12.1	O
)	O
that	O
(	O
~	O
)	O
e	O
{	O
il~r	O
)	O
-	O
lnl	O
}	O
=	O
0	O
v	O
-	O
;	O
;	O
-	O
.	O
in	O
some	O
special	O
cases	O
the	O
expected	O
behavior	O
of	O
the	O
resubstitution	B
estimate	O
can	O
be	O
analyzed	O
in	O
more	O
detail	O
.	O
for	O
example	O
,	O
mclachlan	O
(	O
1976	O
)	O
proved	O
that	O
if	O
the	O
conditional	O
distributions	O
of	O
x	O
given	O
y	O
=	O
0	O
and	O
y	O
=	O
1	O
are	O
both	O
normal	B
with	O
the	O
same	O
covariance	O
matrices	O
,	O
and	O
the	O
rule	B
is	O
linear	O
and	O
based	O
on	O
the	O
estimated	O
parameters	O
,	O
then	O
the	O
bias	B
of	I
the	O
estimate	B
is	O
of	O
the	O
order	O
1/	O
n	O
:	O
mclachlan	O
also	O
showed	O
for	O
this	O
case	O
that	O
for	O
large	O
n	O
the	O
expected	O
value	O
of	O
the	O
re	O
(	O
cid:173	O
)	O
substitution	O
estimate	B
is	O
smaller	O
than	O
that	O
of	O
l	O
n	O
,	O
that	O
is	O
,	O
the	O
estimate	B
is	O
optimistically	O
biased	O
,	O
as	O
expected	O
.	O
23.2	O
histogram	O
rules	O
in	O
this	O
section	O
,	O
we	O
explore	O
the	O
properties	O
of	O
the	O
resubstitution	B
estimate	O
for	O
his	O
(	O
cid:173	O
)	O
togram	O
rules	O
.	O
let	O
p	O
=	O
{	O
ai	O
,	O
az	O
,	O
...	O
}	O
be	O
a	O
fixed	O
partition	B
of	O
rd	O
,	O
and	O
let	O
gn	O
be	O
the	O
400	O
23.	O
the	O
resubstitution	B
estimate	O
corresponding	O
histogram	O
classifier	O
(	O
see	O
chapters	O
6	O
and	O
9	O
)	O
.	O
introduce	O
the	O
notation	O
the	O
analysis	O
is	O
simplified	O
if	O
we	O
rewrite	O
the	O
estimate	B
l~r	O
)	O
in	O
the	O
following	O
form	O
(	O
see	O
problem	O
23.2	O
)	O
:	O
(	O
23.1	O
)	O
it	O
is	O
also	O
interesting	O
to	O
observe	O
that	O
l~r	O
)	O
can	O
be	O
put	O
in	O
the	O
following	O
form	O
:	O
l~r	O
)	O
=	O
f	O
(	O
i	O
{	O
1jl	O
,	O
n	O
(	O
x	O
)	O
:	O
:	O
:	O
:1jo.n	O
(	O
x	O
)	O
}	O
171,11	O
(	O
x	O
)	O
+	O
i	O
{	O
1jl	O
,	O
n	O
(	O
x	O
»	O
1jo	O
,	O
n	O
(	O
x	O
)	O
}	O
170,11	O
(	O
x	O
»	O
)	O
f.1.	O
,	O
(	O
dx	O
)	O
,	O
where	O
170	O
,	O
i1	O
(	O
x	O
)	O
=	O
--	O
--	O
.	O
:	O
:.	O
--	O
--	O
--	O
(	O
cid:173	O
)	O
11	O
lj=l	O
{	O
yj-o	O
,	O
xjea	O
(	O
x	O
)	O
}	O
.1	O
``	O
'1	O
:	O
i	O
_	O
f.1.	O
,	O
(	O
a	O
(	O
x	O
»	O
and	O
71i	O
,	O
n	O
(	O
x	O
)	O
=	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
(	O
cid:173	O
)	O
.1	O
``	O
'~	O
i	O
_	O
11	O
lj=l	O
{	O
yj-l	O
,	O
xjea	O
(	O
x	O
)	O
}	O
f.1.	O
,	O
(	O
a	O
(	O
x	O
»	O
we	O
can	O
compare	O
this	O
form	O
with	O
the	O
following	O
expression	B
of	O
ln	O
:	O
ln	O
=	O
f	O
(	O
i	O
{	O
1j	O
l	O
,	O
n	O
(	O
x	O
)	O
:	O
:	O
:	O
:1jo.n	O
(	O
x	O
)	O
}	O
11	O
(	O
x	O
)	O
+	O
i	O
{	O
1jl	O
,	O
n	O
(	O
x	O
»	O
1jo	O
,	O
n	O
(	O
x	O
)	O
}	O
(	O
l	O
-	O
71	O
(	O
x	O
»	O
)	O
f.1.	O
,	O
(	O
dx	O
)	O
(	O
see	O
problem	O
23.3	O
)	O
.	O
we	O
begin	O
the	O
analysis	O
of	O
performance	O
of	O
the	O
estimate	B
by	O
showing	O
that	O
its	O
mean	O
squared	B
error	I
is	O
not	O
larger	O
than	O
a	O
constant	O
times	O
the	O
number	O
.	O
of	O
cells	O
in	O
the	O
partition	B
over	O
n.	O
theorem	B
23.3.	O
for	O
any	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
andfor	O
all	O
n	O
,	O
the	O
estimate	B
l~r	O
)	O
of	O
the	O
error	O
probability	O
of	O
any	O
histogram	B
rule	I
satisfies	O
also	O
,	O
the	O
estimate	B
is	O
optimistically	O
biased	O
,	O
that	O
is	O
,	O
if	O
,	O
in	O
addition	O
,	O
the	O
histogram	B
rule	I
is	O
based	O
on	O
a	O
partition	B
p	O
ofrd	O
into	O
at	O
most	O
k	O
cells	O
,	O
then	O
proof	O
.	O
the	O
first	O
inequality	B
is	O
an	O
immediate	O
consequence	O
of	O
theorem	O
9.3	O
and	O
(	O
23.1	O
)	O
.	O
introduce	O
the	O
auxiliary	O
quantity	O
r*	O
=	O
lmin	O
{	O
vo	O
(	O
aj	O
,	O
vi	O
(	O
ai	O
)	O
}	O
,	O
where	O
vo	O
(	O
a	O
)	O
=	O
p	O
{	O
y	O
=	O
0	O
,	O
x	O
e	O
a	O
}	O
,	O
and	O
vi	O
(	O
a	O
)	O
=	O
p	O
{	O
y	O
=	O
1	O
,	O
x	O
e	O
a	O
}	O
.	O
we	O
use	O
the	O
decomposition	O
23.2	O
histogram	O
rules	O
401	O
(	O
23.2	O
)	O
first	O
we	O
bound	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
23.2	O
)	O
.	O
observe	O
that	O
r*	O
is	O
just	O
the	O
bayes	O
error	O
corresponding	O
to	O
the	O
pair	O
of	O
random	O
variables	O
(	O
t	O
(	O
x	O
)	O
,	O
y	O
)	O
,	O
where	O
the	O
function	O
t	O
transforms	O
x	O
according	O
to	O
t	O
(	O
x	O
)	O
=	O
i	O
if	O
x	O
e	O
ai	O
.	O
since	O
the	O
histogram	O
classification	O
rule	B
gn	O
(	O
x	O
)	O
can	O
be	O
written	O
as	O
a	O
function	O
of	O
t	O
(	O
x	O
)	O
,	O
its	O
error	O
probability	O
ln	O
can	O
not	O
be	O
smaller	O
than	O
r*	O
.	O
furthermore	O
,	O
by	O
the	O
first	O
identity	O
of	O
theorem	O
2.2	O
,	O
we	O
have	O
0	O
:	O
:	O
:	O
:	O
;	O
ln	O
-	O
r*	O
=	O
l	O
i	O
{	O
sign	O
(	O
vl	O
,	O
n	O
(	O
ai	O
)	O
-vo	O
,	O
n	O
(	O
ai	O
»	O
¥sign	O
(	O
vl	O
(	O
ai	O
)	O
-vo	O
(	O
ai	O
»	O
}	O
lvl	O
(	O
ai	O
)	O
-	O
vo	O
(	O
ai	O
)	O
1	O
if	O
the	O
partition	B
has	O
at	O
most	O
k	O
cells	O
,	O
then	O
by	O
the	O
cauchy-schwarz	O
inequality	B
,	O
e	O
{	O
cln	O
-	O
r*	O
)	O
2	O
}	O
<	O
e	O
{	O
(	O
~	O
iv1	O
(	O
a	O
;	O
)	O
-	O
vo	O
(	O
a	O
;	O
)	O
_	O
(	O
vi	O
,	O
n	O
(	O
a	O
,	O
)	O
_	O
vo	O
,	O
n	O
(	O
a	O
,	O
)	O
)	O
i	O
)	O
2	O
}	O
<	O
k	O
lvar	O
{	O
vl	O
,	O
n	O
(	O
ai	O
)	O
-vo	O
,	O
n	O
(	O
ai	O
)	O
}	O
we	O
bound	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
23.2	O
)	O
:	O
as	O
we	O
have	O
seen	O
earlier	O
,	O
var	O
{	O
l~	O
)	O
}	O
:	O
:	O
:	O
:	O
;	O
lin	O
,	O
so	O
it	O
suffices	O
to	O
bound	O
ir*	O
-	O
e	O
{	O
l~r	O
)	O
}	O
i.	O
by	O
jensen	O
's	O
inequality	B
,	O
<	O
i	O
:	O
min	O
(	O
e	O
{	O
vo	O
,	O
n	O
(	O
a	O
i	O
)	O
}	O
,	O
e	O
{	O
vl.n	O
(	O
ai	O
)	O
}	O
)	O
i	O
=	O
r*	O
.	O
402	O
23.	O
the	O
resubstitution	B
estimate	O
so	O
we	O
bound	O
r*	O
-	O
e	O
{	O
l~r	O
)	O
}	O
from	O
above	O
.	O
by	O
the	O
inequality	B
imin	O
(	O
a	O
,	O
b	O
)	O
-	O
mince	O
,	O
d	O
)	O
1	O
:	O
:	O
:	O
la	O
-	O
el	O
+	O
ib	O
-	O
dl	O
,	O
we	O
have	O
r*	O
-	O
e	O
{	O
l~	O
)	O
}	O
<	O
le	O
{	O
lvo	O
(	O
ad	O
-	O
vo	O
,	O
n	O
(	O
ai	O
)	O
1	O
+	O
ivl	O
(	O
a	O
i	O
)	O
-	O
vi	O
,	O
n	O
(	O
adl	O
}	O
<	O
l	O
(	O
)	O
var	O
{	O
vo	O
,	O
n	O
(	O
ai	O
)	O
}	O
+	O
jvar	O
{	O
v1	O
,	O
n	O
(	O
aj	O
}	O
)	O
i	O
(	O
by	O
the	O
cauchy-schwarz	O
inequality	B
)	O
=	O
~	O
(	O
vo	O
(	O
ai	O
)	O
(	O
i	O
:	O
vo	O
(	O
ai	O
)	O
)	O
+	O
vi	O
(	O
a	O
,	O
)	O
(	O
i	O
:	O
vi	O
(	O
a	O
,	O
)	O
)	O
)	O
<	O
~	O
(	O
2	O
(	O
vo	O
(	O
a	O
,	O
)	O
(	O
i-	O
vo	O
(	O
ai	O
)	O
)	O
n+	O
vi	O
(	O
a	O
,	O
)	O
(	O
i-	O
vi	O
(	O
a	O
,	O
)	O
)	O
)	O
)	O
<	O
i	O
:	O
.til	O
:	O
a	O
,	O
)	O
i	O
where	O
we	O
used	O
the	O
elementary	O
inequality	O
fa	O
+	O
-jb	O
:	O
:	O
:	O
j2	O
(	O
a	O
+	O
b	O
)	O
.	O
therefore	O
,	O
(	O
r	O
'	O
_	O
e	O
{	O
l~r	O
»	O
)	O
)	O
2	O
:	O
''	O
(	O
~til	O
:	O
ai	O
)	O
)	O
2	O
to	O
complete	O
the	O
proof	O
of	O
the	O
third	O
inequality	B
,	O
observe	O
that	O
if	O
there	O
are	O
at	O
most	O
k	O
cells	O
,	O
then	O
<	O
k	O
~	O
'	O
``	O
2fh	O
(	O
ai	O
)	O
~	O
k.	O
l	O
n	O
(	O
by	O
jensen	O
's	O
inequality	B
)	O
therefore	O
,	O
(	O
r*	O
-	O
e	O
{	O
l	O
;	O
:	O
)	O
}	O
)	O
2	O
:	O
:	O
:	O
2k/n	O
.	O
finally	O
,	O
e	O
{	O
ln	O
}	O
-	O
e	O
{	O
l~r	O
)	O
}	O
=	O
(	O
e	O
{	O
l	O
n	O
}	O
-	O
r*	O
)	O
+	O
(	O
r*	O
-	O
e	O
{	O
l~r	O
)	O
}	O
)	O
:	O
:	O
:	O
:	O
o	O
.	O
0	O
we	O
see	O
that	O
if	O
the	O
partition	B
contains	O
a	O
small	O
number	O
of	O
cells	O
,	O
then	O
the	O
resubsti	O
(	O
cid:173	O
)	O
tution	O
estimate	B
performs	O
very	O
nicely	O
.	O
however	O
,	O
if	O
the	O
partition	B
has	O
a	O
large	O
number	O
of	O
cells	O
,	O
then	O
the	O
resubstitution	B
estimate	O
of	O
ln	O
can	O
be	O
very	O
misleading	O
,	O
as	O
the	O
next	O
result	O
indicates	O
:	O
23.3	O
data-based	B
histograms	O
and	O
rule	B
selection	O
403	O
theorem	B
23.4	O
.	O
(	O
devroye	O
and	O
wagner	O
(	O
1976b	O
)	O
)	O
.	O
for	O
every	O
n	O
there	O
exists	O
a	O
par	O
(	O
cid:173	O
)	O
titioning	O
rule	B
and	O
a	O
distribution	O
such	O
that	O
proof	O
.	O
let	O
ai	O
,	O
...	O
,	O
a2n	O
be	O
2n	O
cells	O
of	O
the	O
partition	B
such	O
that	O
fl	O
,	O
(	O
aj	O
=	O
1/	O
(	O
2n	O
)	O
,	O
i	O
==	O
1	O
,	O
...	O
,	O
2n	O
.	O
assume	O
further	O
that	O
1j	O
(	O
x	O
)	O
=	O
1	O
for	O
every	O
x	O
e	O
n	O
d	O
,	O
that	O
is	O
,	O
y	O
=	O
1	O
with	O
probability	O
one	O
.	O
then	O
clearly	O
l~r	O
)	O
=	O
o.	O
on	O
the	O
other	O
hand	O
,	O
if	O
a	O
cell	O
does	O
not	O
contain	O
any	O
of	O
the	O
data	O
points	O
xl	O
,	O
...	O
,	O
x	O
n	O
,	O
then	O
gn	O
(	O
x	O
)	O
=	O
0	O
in	O
that	O
cell	O
.	O
but	O
since	O
the	O
number	O
of	O
points	O
is	O
only	O
half	O
of	O
the	O
number	O
of	O
cells	O
,	O
at	O
least	O
half	O
of	O
the	O
cells	O
are	O
empty	O
.	O
therefore	O
ln	O
:	O
:	O
:	O
:	O
1/2	O
,	O
and	O
il~r	O
)	O
-	O
ln	O
i	O
:	O
:	O
:	O
:	O
1/2	O
.	O
this	O
concludes	O
the	O
proof	O
.	O
o	O
23.3	O
data-based	B
histograms	O
and	O
rule	B
selection	O
theorem	B
23.3	O
demonstrates	O
the	O
usefulness	O
of	O
l~r	O
)	O
for	O
histogram	O
rules	O
with	O
a	O
fixed	O
partition	B
,	O
provided	O
that	O
the	O
number	O
of	O
cells	O
in	O
the	O
partition	B
is	O
not	O
too	O
large	O
.	O
if	O
we	O
want	O
to	O
use	O
l~r	O
)	O
to	O
select	O
a	O
good	O
classifier	B
,	O
the	O
estimate	B
should	O
work	O
uniformly	O
well	O
over	O
the	O
class	O
from	O
which	O
we	O
select	O
a	O
rule	O
.	O
in	O
this	O
section	O
we	O
explore	O
such	O
data-based	B
histogram	O
rules	O
.	O
let	O
f	O
be	O
a	O
class	O
of	O
partitions	O
of	O
nd	O
.	O
we	O
will	O
assume	O
that	O
each	O
member	O
of	O
f	O
par	O
(	O
cid:173	O
)	O
titions	O
n	O
d	O
into	O
at	O
most	O
k	O
cells	O
.	O
for	O
each	O
partition	B
p	O
e	O
f	O
,	O
define	O
the	O
corresponding	O
histogram	B
rule	I
by	O
if	O
2	O
:	O
:7=1	O
i	O
{	O
y	O
;	O
=l	O
,	O
x	O
;	O
ea	O
(	O
x	O
)	O
}	O
:	O
:	O
:	O
;	O
2	O
:	O
:7=1	O
i	O
{	O
y	O
;	O
=o	O
,	O
x	O
;	O
ea	O
(	O
x	O
)	O
}	O
otherwise	O
,	O
where	O
a	O
(	O
x	O
)	O
is	O
the	O
cell	O
of	O
p	O
that	O
contains	O
x.	O
denote	O
the	O
error	O
probability	O
of	O
g~p	O
)	O
(	O
x	O
)	O
by	O
the	O
corresponding	O
error	O
estimate	O
is	O
denoted	O
by	O
l	O
?	O
\p	O
)	O
=	O
l	O
min	O
{	O
vo	O
,	O
n	O
(	O
a	O
)	O
,	O
vl	O
,	O
n	O
(	O
a	O
)	O
}	O
.	O
aep	O
by	O
analogy	O
with	O
theorems	O
23.1	O
and	O
23.2	O
,	O
we	O
can	O
derive	O
the	O
following	O
result	O
,	O
which	O
gives	O
a	O
useful	O
bound	O
for	O
the	O
largest	O
difference	O
between	O
the	O
estimate	B
and	O
the	O
error	O
probability	O
within	O
the	O
class	O
of	O
histogram	O
classifiers	O
defined	O
by	O
f.	O
the	O
combinatorial	O
coefficient	O
/	O
)	O
'~	O
(	O
f	O
)	O
defined	O
in	O
chapter	O
21	O
appears	O
as	O
a	O
coefficient	O
in	O
the	O
upper	O
bound	O
.	O
the	O
computation	O
of	O
/	O
)	O
'~	O
(	O
f	O
)	O
for	O
several	O
different	O
classes	O
of	O
partitions	O
is	O
illustrated	O
in	O
chapter	O
21	O
.	O
404	O
23.	O
the	O
resubstitution	B
estimate	O
theorem	B
23.5	O
.	O
(	O
feinholz	O
(	O
1979	O
)	O
)	O
.	O
assume	O
that	O
each	O
member	O
of	O
f	O
partitions	O
nd	O
into	O
at	O
most	O
k	O
cells	O
.	O
for	O
every	O
nand	O
e	O
>	O
0	O
,	O
p	O
{	O
sup	O
il~r	O
)	O
(	O
p	O
)	O
-	O
ln	O
(	O
p	O
)	O
1	O
>	O
e	O
}	O
:	O
:	O
:	O
8	O
.	O
2k~~	O
(	O
f	O
)	O
e-ne2/32	O
.	O
pef	O
proof	O
.	O
we	O
can	O
proceed	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
23.1.	O
the	O
shatter	B
coefficient	I
s	O
(	O
c	O
,	O
n	O
)	O
corresponding	O
to	O
the	O
class	O
of	O
histogram	O
classifiers	O
defined	O
by	O
partitions	O
in	O
f	O
can	O
clearly	O
be	O
bounded	O
from	O
above	O
by	O
the	O
number	O
of	O
different	O
ways	O
in	O
which	O
n	O
points	O
can	O
be	O
partitioned	O
by	O
members	O
of	O
f	O
,	O
times	O
2k	O
,	O
as	O
there	O
are	O
at	O
most	O
2k	O
different	O
ways	O
to	O
assign	O
labels	O
to	O
cells	O
of	O
a	O
partition	O
of	O
at	O
most	O
k	O
cells	O
.	O
0	O
the	O
theorem	B
has	O
two	O
interesting	O
implications	O
.	O
the	O
error	O
estimate	O
l~r	O
)	O
can	O
also	O
be	O
used	O
to	O
estimate	B
the	O
perfolmance	O
of	O
histogram	O
rules	O
based	O
on	O
data-dependent	O
partitions	O
(	O
see	O
chapter	O
21	O
)	O
.	O
the	O
argument	O
ofthe	O
proof	O
of	O
theorem	O
23.3	O
is	O
not	O
valid	O
for	O
these	O
rules	O
.	O
however	O
,	O
theorem	B
23.5	O
provides	O
performance	O
guarantees	O
for	O
these	O
rules	O
in	O
the	O
following	O
corollaries	O
:	O
corollary	O
23.1.	O
let	O
gn	O
(	O
x	O
)	O
be	O
a	O
histogram	O
classifier	B
based	O
on	O
a	O
random	O
partition	B
pn	O
into	O
at	O
most	O
k	O
cells	O
,	O
which	O
is	O
determined	O
by	O
the	O
data	O
dn	O
.	O
assume	O
that	O
for	O
any	O
possible	O
realization	O
ot	O
the	O
training	O
data	O
dn	O
,	O
the	O
partition	B
p	O
n	O
is	O
a	O
member	O
of	O
a	O
class	O
of	O
partitions	O
f.	O
if	O
ln	O
is	O
the	O
error	O
probability	O
of	O
gn	O
,	O
then	O
p	O
{	O
i	O
l	O
(	O
r	O
)	O
-	O
l	O
i	O
>	O
e	O
}	O
<	O
8	O
.	O
2k	O
tj	O
,	O
,*	O
(	O
f	O
)	O
e	O
-ne	O
2	O
n	O
n	O
-	O
n	O
/32	O
'	O
and	O
(	O
r	O
)	O
_	O
e	O
ln	O
{	O
(	O
)	O
2	O
}	O
<	O
32k	O
+	O
3210g	O
(	O
~~	O
(	O
f	O
)	O
)	O
+	O
128	O
.	O
_	O
ln	O
n	O
proof	O
.	O
the	O
first	O
inequality	B
follows	O
from	O
theorem	B
23.5	O
by	O
the	O
obvious	O
inequality	B
p	O
{	O
il~r	O
)	O
-	O
ln	O
i	O
>	O
e	O
}	O
:	O
:	O
:	O
p	O
1	O
sup	O
il~r	O
)	O
(	O
p	O
)	O
-	O
ln	O
(	O
p	O
)	O
1	O
>	O
e	O
}	O
.	O
pef	O
the	O
second	O
inequality	B
follows	O
from	O
the	O
first	O
one	O
via	O
problem	O
12.1	O
.	O
0	O
perhaps	O
the	O
most	O
important	O
application	O
of	O
theorem	O
23.5	O
is	O
in	O
classifier	O
selection	B
.	O
let	O
cn	O
be	O
a	O
class	O
of	O
(	O
possibly	O
data-dependent	B
)	O
histogram	O
rules	O
.	O
we	O
may	O
use	O
the	O
error	O
estimate	O
l~r	O
)	O
to	O
select	O
a	O
classifier	O
that	O
minimizes	O
the	O
estimated	O
error	O
probability	O
.	O
denote	O
the	O
selected	O
histogram	B
rule	I
by	O
¢/	O
:	O
'	O
that	O
is	O
,	O
l~r	O
)	O
(	O
¢	O
,	O
:	O
)	O
:	O
:	O
:	O
l~r	O
)	O
(	O
¢n	O
)	O
for	O
all	O
¢n	O
e	O
cn	O
.	O
here	O
l~r	O
)	O
(	O
¢n	O
)	O
denotes	O
the	O
estimated	O
error	O
probability	O
of	O
the	O
classification	O
rule	B
¢in	O
.	O
the	O
question	O
is	O
how	O
well	O
the	O
selection	B
method	O
works	O
,	O
in	O
other	O
words	O
,	O
how	O
close	O
the	O
error	O
probability	O
of	O
the	O
selected	O
classifier	B
l	O
(	O
¢	O
,	O
:	O
)	O
is	O
to	O
the	O
error	O
probability	O
of	O
the	O
best	O
rule	B
in	O
the	O
class	O
,	O
infcpnecn	O
l	O
(	O
¢n	O
)	O
.	O
it	O
turns	O
out	O
that	O
if	O
the	O
possible	O
partitions	O
are	O
not	O
too	O
complex	O
,	O
then	O
the	O
method	O
works	O
very	O
well	O
:	O
problems	O
and	O
exercises	O
405	O
corollary	O
23.2.	O
assume	O
that	O
jor	O
any	O
realization	O
oj	O
the	O
data	O
dn	O
,	O
the	O
possible	O
partitions	O
that	O
define	O
the	O
histogram	O
classifiers	O
in	O
en	O
belong	O
to	O
a	O
class	O
ojpartitions	O
f	O
,	O
whose	O
members	O
partition	B
nd	O
into	O
at	O
most	O
k	O
cells	O
.	O
then	O
p	O
{	O
l	O
(	O
¢z	O
)	O
-	O
¢	O
:	O
~t	O
l	O
(	O
¢n	O
)	O
>	O
e	O
}	O
:	O
:s	O
8	O
.	O
2	O
'	O
/	O
'	O
,	O
,~	O
(	O
:	O
f	O
)	O
e-n	O
''	O
/128	O
proof	O
.	O
by	O
lemma	O
8.2	O
,	O
l	O
(	O
¢	O
,	O
:	O
)	O
-	O
inf	O
l	O
(	O
¢n	O
)	O
~	O
2	O
sup	O
il~r	O
)	O
(	O
¢n	O
)	O
-	O
l	O
(	O
¢n	O
)	O
1	O
~	O
2	O
sup	O
il~r	O
)	O
(	O
p	O
)	O
-	O
l	O
(	O
p	O
)	O
i	O
.	O
~~	O
~~	O
p6	O
therefore	O
,	O
the	O
statement	O
follows	O
from	O
theorem	B
23.5	O
.	O
0	O
problems	O
and	O
exercises	O
problem	O
23.1.	O
prove	O
theorem	B
23.2.	O
hint	O
:	O
proceed	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
23.1.	O
problem	O
23.2.	O
show	O
that	O
for	O
histogram	O
rules	O
,	O
the	O
resubstitution	B
estimate	O
may	O
be	O
written	O
as	O
problem	O
23.3.	O
consider	O
the	O
resubstitution	B
estimate	O
l~r	O
)	O
of	O
the	O
error	O
probability	O
ln	O
of	O
a	O
histogram	O
rule	B
based	O
on	O
a	O
fixed	O
sequence	O
of	O
partitions	O
pn	O
.	O
show	O
that	O
if	O
the	O
regression	B
function	I
estimate	O
7jn	O
(	O
x	O
)	O
=	O
~	O
l	O
;	O
=~~~j	O
(	O
:	O
)	O
;	O
jea	O
(	O
x	O
)	O
}	O
is	O
consistent	O
,	O
that	O
is	O
,	O
it	O
satisfies	O
e	O
{	O
j	O
17j	O
(	O
x	O
)	O
-	O
7jn	O
(	O
x	O
)	O
lfl	O
(	O
dx	O
)	O
}	O
~	O
°	O
as	O
n	O
~	O
00	O
,	O
then	O
lim	O
e	O
{	O
il	O
;	O
:	O
)	O
-	O
lnll	O
=	O
0	O
,	O
n-+oo	O
andalsoe	O
{	O
l~r	O
)	O
}	O
~	O
l*	O
.	O
problem	O
23.4.	O
let	O
(	O
x~	O
,	O
y	O
{	O
)	O
,	O
...	O
,	O
(	O
x~	O
,	O
y~l	O
)	O
be	O
a	O
sequence	O
that	O
depends	O
in	O
an	O
arbitrary	O
fash	O
(	O
cid:173	O
)	O
ion	O
on	O
the	O
data	O
din	O
and	O
let	O
gn	O
be	O
the	O
nearest	B
neighbor	I
rule	I
with	O
(	O
x~	O
,	O
y	O
{	O
)	O
,	O
...	O
,	O
(	O
x~l	O
'	O
y/	O
;	O
)	O
,	O
where	O
m	O
is	O
fixed	O
.	O
let	O
l~r	O
)	O
denote	O
the	O
resubstitution	B
estimate	O
of	O
ln	O
=	O
l	O
(	O
gn	O
)	O
.	O
show	O
that	O
for	O
all	O
e	O
>	O
°	O
and	O
all	O
distributions	O
,	O
p	O
{	O
iln	O
-	O
l	O
;	O
:	O
)	O
i	O
:	O
:	O
:	O
:	O
e	O
}	O
:	O
:	O
:	O
:	O
8ndm	O
(	O
m-l	O
)	O
e-ne2/32	O
.	O
problem	O
23.5.	O
find	O
a	O
rule	O
for	O
(	O
x	O
,	O
y	O
)	O
e	O
r	O
x	O
{	O
o	O
,	O
i	O
}	O
such	O
that	O
for	O
all	O
nonatomic	O
distributions	O
with	O
l	O
*	O
=	O
°	O
we	O
have	O
e	O
{	O
ln	O
}	O
~	O
0	O
,	O
yete	O
{	O
l~r	O
)	O
}	O
:	O
:	O
:	O
:	O
e	O
{	O
ln	O
}	O
.	O
(	O
thus	O
,	O
l~r	O
)	O
maybe	O
pessimistically	O
biased	O
even	O
for	O
a	O
consistent	O
rule	B
.	O
)	O
problem	O
23.6.	O
for	O
histogram	O
rules	O
on	O
fixed	O
partitions	O
(	O
pattitions	O
that	O
do	O
not	O
change	O
with	O
n	O
and	O
are	O
independent	O
of	O
the	O
data	O
)	O
,	O
show	O
that	O
e	O
{	O
l~r	O
)	O
}	O
is	O
monotonically	O
nonincreasing	O
.	O
problem	O
23.7.	O
assume	O
that	O
x	O
has	O
a	O
density	O
,	O
and	O
investigate	O
the	O
resubstitution	B
estimate	O
of	O
the	O
3-nn	O
rule	B
.	O
what	O
is	O
the	O
limit	O
of	O
e	O
{	O
l~r	O
)	O
}	O
as	O
n	O
~	O
oo	O
?	O
24	O
deleted	B
estimates	O
of	O
the	O
error	O
probability	O
the	O
deleted	B
estimate	O
(	O
also	O
called	O
cross-validation	B
,	O
leave-one-out	B
,	O
or	O
u-method	O
)	O
attempts	O
to	O
avoid	O
the	O
bias	B
present	O
in	O
the	O
resubstitution	B
estimate	O
.	O
proposed	O
and	O
developed	O
by	O
lunts	O
and	O
brailovsky	O
(	O
1967	O
)	O
,	O
lachenbruch	O
(	O
1967	O
)	O
,	O
cover	O
(	O
1969	O
)	O
,	O
and	O
stone	O
(	O
1974	O
)	O
,	O
the	O
method	O
deletes	O
the	O
first	O
pair	O
(	O
xl	O
,	O
yl	O
)	O
from	O
the	O
training	O
data	O
and	O
makes	O
a	O
decision	O
gn-l	O
using	O
the	O
remaining	O
n	O
-	O
1	O
pairs	O
.	O
it	O
tests	O
for	O
an	O
an	O
error	O
on	O
(	O
xl	O
,	O
yl	O
)	O
,	O
and	O
repeats	O
this	O
procedure	O
for	O
all	O
n	O
pairs	O
of	O
the	O
training	O
data	O
dn	O
.	O
the	O
estimate	B
l~d	O
)	O
is	O
the	O
average	O
the	O
number	O
of	O
errors	O
.	O
we	O
formally	O
denote	O
the	O
training	O
set	O
with	O
(	O
xi	O
,	O
yj	O
deleted	B
by	O
then	O
we	O
define	O
clearly	O
,	O
the	O
deleted	B
estimate	O
is	O
almost	O
unbiased	O
in	O
the	O
sense	O
that	O
thus	O
,	O
l~d	O
)	O
should	O
be	O
viewed	O
as	O
an	O
estimator	O
of	O
ln-	O
l	O
,	O
rather	O
than	O
of	O
ln	O
.	O
in	O
most	O
of	O
the	O
interesting	O
cases	O
ln	O
converges	O
with	O
probability	O
one	O
so	O
that	O
the	O
difference	O
between	O
ln-	O
l	O
and	O
ln	O
becomes	O
negligible	O
for	O
large	O
n.	O
the	O
designer	O
has	O
the	O
luxury	O
of	O
being	O
able	O
to	O
pick	O
the	O
most	O
convenient	O
gn-l.	O
in	O
some	O
cases	O
the	O
choice	O
is	O
very	O
natural	O
,	O
in	O
other	O
cases	O
it	O
is	O
not	O
.	O
for	O
example	O
,	O
if	O
gn	O
is	O
the	O
i-nearest	O
neighbor	B
rule	I
,	O
then	O
letting	O
gn-l	O
be	O
the	O
i-nearest	O
neighbor	B
rule	I
based	O
on	O
n	O
-	O
1	O
data	O
pairs	O
seems	O
to	O
be	O
an	O
obvious	O
choice	O
.	O
we	O
will	O
see	O
later	O
408	O
24.	O
deleted	B
estimates	O
of	O
the	O
error	O
probability	O
that	O
this	O
indeed	O
yields	O
an	O
extremely	O
good	O
estimator	O
.	O
but	O
what	O
should	O
gn-l	O
be	O
if	O
gn	O
is	O
,	O
for	O
example	O
,	O
a	O
k-nn	O
rule	B
?	O
should	O
it	O
be	O
the	O
k-nearest	O
neighbor	O
classifier	O
,	O
the	O
i-nearest	O
neighbor	O
classifier	O
,	O
or	O
maybe	O
something	O
else	O
?	O
well	O
,	O
the	O
choice	O
is	O
k	O
-	O
typically	O
nontrivial	O
and	O
needs	O
careful	O
attention	O
if	O
the	O
designer	O
wants	O
a	O
distribution_	O
free	O
performance	O
guarantee	O
for	O
the	O
resulting	O
estimate	B
.	O
because	O
of	O
the	O
variety	O
of	O
choices	O
for	O
gn-l	O
,	O
we	O
should	O
not	O
speak	O
of	O
the	O
deleted	B
estimate	O
,	O
but	O
rather	O
of	O
a	O
deleted	O
estimate	B
.	O
in	O
this	O
chapter	O
we	O
analyze	O
the	O
performance	O
of	O
deleted	O
estimates	O
for	O
a	O
few	O
proto	O
(	O
cid:173	O
)	O
type	O
classifiers	O
,	O
such	O
as	O
the	O
kernel	B
,	O
nearest	B
neighbor	I
,	O
and	O
histogram	O
rules	O
.	O
in	O
most	O
cases	O
studied	O
here	O
,	O
deleted	B
estimates	O
have	O
good	O
distribution-free	O
properties	O
.	O
24.1	O
a	O
general	O
lower	O
bound	O
nonparametric	O
rules	O
.	O
an	O
error	O
estimate	O
ln	O
is	O
merely	O
a	O
function	O
(	O
rd	O
x	O
{	O
o	O
,	O
l	O
}	O
r	O
-+	O
we	O
begin	O
by	O
exploring	O
general	O
limitations	O
of	O
error	O
estimates	O
for	O
some	O
important	O
[	O
0	O
,	O
1	O
]	O
,	O
which	O
is	O
applied	O
to	O
the	O
data	O
dn	O
=	O
(	O
(	O
x	O
i	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
»	O
.	O
theorem	B
24.1.	O
let	O
gn	O
be	O
one	O
of	O
the	O
following	O
classifiers	O
:	O
(	O
a	O
)	O
the	O
kernel	B
rule	I
gn	O
(	O
x	O
)	O
=	O
{	O
~	O
if	O
``	O
n	O
i	O
i	O
l.d=l	O
{	O
yi=o	O
}	O
otherwise	O
k	O
(	O
x-xi	O
)	O
~n	O
i	O
-h-	O
:	O
:	O
:	O
:	O
l	O
...	O
.i=l	O
{	O
yi=l	O
}	O
k	O
(	O
x-xi	O
)	O
-h-	O
with	O
a	O
nonnegative	O
kernel	B
k	O
of	O
compact	O
support	B
and	O
smoothing	B
factor	I
h	O
>	O
0	O
.	O
(	O
b	O
)	O
the	O
histogram	B
rule	I
(	O
x	O
)	O
=	O
{	O
o	O
if	O
''	O
l7=1	O
i	O
{	O
yi=l	O
}	O
i	O
{	O
xiea	O
(	O
x	O
)	O
}	O
:	O
s	O
l:7=1	O
i	O
{	O
yi=o	O
}	O
i	O
{	O
xiea	O
(	O
x	O
)	O
}	O
gil	O
1	O
otherwise	O
based	O
on	O
a	O
fixed	O
partition	B
p	O
=	O
{	O
ai	O
,	O
a	O
2	O
,	O
...	O
}	O
containing	O
at	O
least	O
n	O
cells	O
.	O
(	O
c	O
)	O
the	O
lazy	B
histogram	O
rule	B
where	O
xj	O
is	O
the	O
minimum-index	O
point	O
among	O
xl	O
,	O
...	O
,	O
xnforwhich	O
xj	O
e	O
ai	O
,	O
wherep	O
=	O
{	O
ai	O
,	O
a	O
2	O
,	O
...	O
}	O
isajixedpartition	O
containing	O
atleastn	O
cells	O
.	O
denote	O
the	O
probability	O
of	O
error	O
for	O
gil	O
by	O
ln	O
=	O
p	O
{	O
gn	O
(	O
x	O
,	O
dn	O
)	O
=i	O
y/dn	O
}	O
.	O
then/or	O
with	O
l	O
*	O
=	O
°	O
such	O
that	O
every	O
n	O
:	O
:	O
:	O
:	O
10	O
,	O
and/or	O
every	O
error	O
estimatorln	O
,	O
there	O
exists	O
a	O
distribution	O
o/	O
(	O
x	O
,	O
y	O
)	O
the	O
theorem	B
says	O
that	O
for	O
any	O
estimate	B
ln	O
,	O
there	O
exists	O
a	O
distribution	O
with	O
the	O
property	O
thate	O
{	O
iln	O
(	O
dn	O
)	O
-	O
ln	O
i	O
}	O
:	O
:	O
:	O
:	O
1/	O
,	O
j32n	O
.	O
for	O
the	O
rules	O
gn	O
given	O
in	O
the	O
theorem	B
,	O
24.1	O
a	O
general	O
lower	O
bound	O
409	O
no	O
error	O
estimate	O
can	O
possibly	O
give	O
better	O
distribution-free	O
performance	O
guarantees	O
.	O
error	O
estimates	O
are	O
necessarily	O
going	O
to	O
perform	O
at	O
least	O
this	O
poorly	O
for	O
some	O
distributions	O
.	O
proof	O
of	O
theorem	O
24.1.	O
let	O
ln	O
be	O
an	O
arbitrary	O
fixed	O
error	O
estimate	O
.	O
the	O
proof	O
relies	O
on	O
randomization	O
ideas	O
similar	O
to	O
those	O
of	O
the	O
proofs	O
of	O
theorems	O
7.1	O
and	O
7.2.	O
we	O
construct	O
a	O
family	O
of	O
distributions	O
forcx	O
,	O
y	O
)	O
.forb	O
e	O
[	O
0	O
,	O
1	O
)	O
letb	O
=	O
0.b	O
1b2b3	O
•..	O
be	O
its	O
binary	B
expansion	O
.	O
in	O
all	O
cases	O
the	O
distribution	B
of	O
x	O
is	O
uniform	B
on	O
n	O
points	O
{	O
xl	O
,	O
...	O
,	O
x	O
n	O
}	O
.	O
for	O
the	O
histogram	O
and	O
lazy	B
histogram	O
rules	O
choose	O
xl	O
,	O
...	O
,	O
xn	O
such	O
that	O
they	O
fall	O
into	O
different	O
cells	O
.	O
for	O
the	O
kernel	B
rule	I
choose	O
xl	O
,	O
...	O
,	O
xn	O
so	O
that	O
they	O
are	O
isolated	O
from	O
each	O
other	O
,	O
that	O
is	O
,	O
k	O
cxi~xj	O
)	O
=	O
°	O
for	O
all	O
i	O
,	O
j	O
e	O
{	O
1	O
,	O
...	O
,	O
n	O
}	O
,	O
i	O
=i	O
j.	O
to	O
simplify	O
the	O
notation	O
we	O
will	O
refer	O
to	O
these	O
points	O
by	O
their	O
indices	O
,	O
that	O
is	O
,	O
we	O
will	O
write	O
x	O
=	O
i	O
instead	O
of	O
x	O
=	O
xi	O
.	O
for	O
a	O
fixed	O
b	O
define	O
y	O
=bx	O
.	O
we	O
may	O
create	O
infinitely	O
many	O
samples	O
cone	O
for	O
each	O
b	O
e	O
[	O
0	O
,	O
1	O
»	O
drawn	O
from	O
the	O
distribution	B
of	O
cx	O
,	O
y	O
)	O
as	O
follows	O
.	O
let	O
x	O
i	O
,	O
...	O
,	O
xn	O
be	O
i.i.d	O
.	O
and	O
uniformly	O
distributed	O
on	O
{	O
1	O
,	O
...	O
,	O
n	O
}	O
.	O
all	O
the	O
samples	O
share	O
the	O
same	O
xl	O
,	O
...	O
,	O
x	O
n	O
,	O
but	O
differ	O
in	O
their	O
y/s	O
.	O
for	O
given	O
xi	O
,	O
define	O
yi	O
=	O
b	O
xi	O
•	O
write	O
zn	O
=	O
cx	O
1	O
,	O
...	O
,	O
xn	O
)	O
and	O
ni	O
=	O
:	O
l	O
)	O
:	O
=l	O
i	O
{	O
xj	O
:	O
=i	O
}	O
'	O
observe	O
that	O
dn	O
is	O
a	O
function	O
of	O
zn	O
and	O
b.	O
it	O
is	O
clear	O
that	O
for	O
all	O
classifiers	O
covered	O
by	O
our	O
assumptions	O
,	O
for	O
a	O
fixed	O
b	O
,	O
where	O
s	O
=	O
{	O
i	O
:	O
1	O
:	O
:	O
:	O
;	O
i	O
:	O
:	O
:	O
;	O
n	O
,	O
ni	O
=	O
o	O
}	O
is	O
the	O
set	O
of	O
empty	O
bins	O
.	O
we	O
randomize	O
the	O
distribution	B
as	O
follows	O
.	O
let	O
b	O
=	O
0.bib2	O
...	O
be	O
a	O
uniform	O
[	O
0	O
,	O
1	O
]	O
random	O
variable	B
independent	O
of	O
cx	O
,	O
y	O
)	O
.	O
then	O
clearly	O
b	O
i	O
,	O
b	O
2	O
,	O
•..	O
are	O
independent	O
uniform	B
binary	O
random	O
variables	O
.	O
note	O
that	O
supe	O
{	O
ilncdn	O
)	O
-lnl	O
}	O
>	O
e	O
{	O
ilncdn	O
)	O
-lnl	O
}	O
b	O
cwhere	O
b	O
is	O
replaced	O
by	O
b	O
)	O
e	O
{	O
e	O
{	O
ilncdn	O
)	O
-	O
ln	O
ii	O
zn	O
}	O
}	O
.	O
in	O
what	O
follows	O
we	O
bound	O
the	O
conditional	O
expectation	O
within	O
the	O
brackets	O
.	O
to	O
make	O
the	O
dependence	O
upon	O
b	O
explicit	O
we	O
write	O
lnczn	O
,	O
b	O
)	O
=	O
lncdn	O
)	O
and	O
lncb	O
)	O
=	O
ln	O
.	O
thus	O
,	O
e	O
{	O
iln	O
(	O
zn	O
,	O
b	O
)	O
-	O
ln	O
(	O
b	O
)	O
11	O
zn	O
}	O
=	O
--	O
1	O
2e	O
{	O
iln	O
(	O
zn	O
,	O
b	O
)	O
-	O
lncb	O
)	O
1	O
+	O
ilnczn	O
,	O
b*	O
)	O
-	O
lncb*	O
)	O
11	O
zn	O
}	O
(	O
where	O
b	O
;	O
=	O
bi	O
for	O
i	O
with	O
ni	O
>	O
°	O
and	O
b	O
;	O
=	O
1	O
-	O
bi	O
for	O
i	O
with	O
ni	O
=	O
0	O
)	O
--	O
:	O
:	O
:	O
~e	O
{	O
llncb	O
)	O
-	O
ln	O
(	O
b*	O
)	O
11	O
zn	O
}	O
(	O
since	O
ln	O
(	O
zn	O
,	O
b*	O
)	O
=	O
lnczn	O
,	O
b	O
»	O
410	O
2:4.	O
deleted	B
estimates	O
of	O
the	O
error	O
probability	O
(	O
recall	O
the	O
expression	B
for	O
ln	O
given	O
above	O
)	O
=	O
~e	O
{	O
12b	O
(	O
isi	O
,	O
1/2	O
)	O
-111	O
lsi	O
}	O
2n	O
(	O
where	O
b	O
(	O
isi	O
,	O
1/2	O
)	O
is	O
a	O
binomial	O
(	O
lsi	O
,	O
1/2	O
)	O
random	O
variable	B
)	O
:	O
:	O
:	O
:	O
~	O
fist	O
2nvt	O
(	O
by	O
khintchine	O
's	O
inequality	B
,	O
see	O
lemma	O
a.s	O
)	O
.	O
in	O
summary	O
,	O
we	O
have	O
we	O
have	O
only	O
to	O
bound	O
the	O
right-hand	O
side	O
.	O
we	O
apply	O
lemma	O
aa	O
to	O
the	O
random	O
variable	B
jist	O
=	O
jj	O
:	O
.7==11	O
{	O
ni=oj	O
.	O
clearly	O
,	O
eisi	O
=	O
no	O
-	O
l/n	O
)	O
n	O
,	O
and	O
}	O
=	O
e	O
{	O
t	O
l	O
{	O
ni==oj	O
+	O
l	O
l	O
{	O
ni==o	O
,	O
ni==oj	O
}	O
.	O
i==l	O
i=/j	O
=	O
eisi	O
+	O
n	O
(	O
n	O
-	O
1	O
)	O
1	O
-	O
(	O
2	O
)	O
11	O
-	O
;	O
;	O
e	O
{	O
isi	O
2	O
so	O
that	O
in	O
(	O
1-	O
~r	O
+	O
n	O
(	O
n	O
-	O
1	O
)	O
(	O
1-	O
~r	O
(	O
1-	O
~r	O
:	O
:	O
:	O
:	O
v'n	O
--	O
--	O
;	O
:	O
:=====	O
i	O
+	O
(	O
1-2/n	O
)	O
''	O
11	O
(	O
1-l/n	O
)	O
''	O
(	O
use	O
1	O
-	O
''	O
ii	O
(	O
1	O
)	O
11	O
1	O
:	O
:	O
:	O
:	O
;	O
)	O
the	O
proof	O
is	O
now	O
complete	O
.	O
0	O
24.2	O
a	O
general	O
upper	O
bound	O
for	O
deleted	B
estimates	O
411	O
24.2	O
a	O
general	O
upper	O
bound	O
for	O
deleted	B
estimates	O
the	O
following	O
inequality	B
is	O
a	O
general	O
tool	O
for	O
obtaining	O
distribution-free	O
upper	O
bounds	O
for	O
the	O
difference	O
between	O
the	O
deleted	B
estimate	O
and	O
the	O
true	O
error	O
probability	O
ln	O
:	O
theorem	B
24.2	O
.	O
(	O
rogers	O
and	O
wagner	O
(	O
1978	O
)	O
;	O
devroye	O
and	O
wagner	O
(	O
1976b	O
)	O
)	O
.	O
assume	O
that	O
gn	O
is	O
a	O
symmetric	O
classifier	B
,	O
that	O
is	O
,	O
gn	O
(	O
x	O
,	O
dn	O
)	O
==	O
gn	O
(	O
x	O
,	O
d~	O
)	O
,	O
where	O
d~	O
is	O
obtained	O
by	O
permuting	O
the	O
pairs	O
of	O
dn	O
arbitrarily	O
.	O
then	O
proof	O
.	O
first	O
we	O
express	O
the	O
three	O
terms	O
on	O
the	O
right-hand	O
side	O
of	O
the	O
first	O
term	O
can	O
be	O
bounded	O
,	O
by	O
using	O
symmetry	O
of	O
gn	O
,	O
by	O
e	O
{	O
l~d	O
)	O
2	O
}	O
=	O
e	O
{	O
(	O
~	O
~	O
j	O
{	O
g	O
''	O
,	O
(	O
x	O
,	O
d	O
''	O
,	O
)	O
¥y	O
;	O
)	O
r	O
}	O
e	O
{	O
-	O
;	O
.	O
t	O
i	O
{	O
gn_l	O
(	O
xi	O
,	O
dn	O
,	O
i	O
)	O
¥yd	O
}	O
n	O
i=l	O
the	O
second	O
term	O
is	O
written	O
as	O
e	O
{	O
l~d	O
)	O
ln	O
}	O
e	O
{	O
ln~	O
t	O
i	O
{	O
gn-l	O
(	O
xi	O
,	O
dil	O
,	O
i	O
)	O
¥yd	O
}	O
n	O
i=l	O
412	O
24.	O
deleted	B
estimates	O
of	O
the	O
error	O
probability	O
1	O
n	O
-	O
le	O
{	O
p	O
{	O
gn	O
(	O
x	O
,	O
dn	O
)	O
¥	O
y	O
,	O
gn-l	O
(	O
xi	O
,	O
dn	O
,	O
j	O
¥	O
yiidn	O
}	O
}	O
n	O
i=l	O
p	O
{	O
gn	O
(	O
x	O
,	O
dn	O
)	O
¥	O
y	O
,	O
gn-i	O
(	O
xl	O
,	O
dn	O
,	O
l	O
)	O
¥	O
yd	O
.	O
for	O
the	O
third	O
term	O
,	O
we	O
introduce	O
the	O
pair	O
(	O
xi	O
,	O
yl	O
)	O
,	O
independent	O
of	O
x	O
,	O
y	O
,	O
and	O
d	O
n	O
,	O
having	O
the	O
same	O
distribution	B
as	O
(	O
x	O
,	O
y	O
)	O
.	O
then	O
e	O
{	O
l~	O
}	O
=	O
e	O
{	O
p	O
{	O
gn	O
(	O
x	O
,	O
dn	O
)	O
¥yidnf	O
}	O
=	O
e	O
{	O
p	O
{	O
gn	O
(	O
x	O
,	O
dn	O
)	O
¥	O
yidn	O
}	O
p	O
{	O
gn	O
(	O
xi	O
,	O
dn	O
)	O
¥	O
yiidn	O
}	O
}	O
=	O
e	O
{	O
p	O
{	O
gn	O
(	O
x	O
,	O
dn	O
)	O
¥	O
y	O
,	O
gn	O
(	O
xi	O
,	O
dn	O
)	O
¥	O
yiidn	O
}	O
}	O
where	O
we	O
used	O
independence	O
of	O
(	O
xi	O
,	O
yl	O
)	O
.	O
we	O
introduce	O
the	O
notation	O
d~	O
=	O
ak	O
,	O
i	O
,	O
j	O
(	O
x3	O
,	O
y3	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
,	O
{	O
gn	O
(	O
xk	O
;	O
(	O
xi	O
,	O
yi	O
)	O
,	O
(	O
xj	O
,	O
yj	O
)	O
,	O
d~	O
)	O
¥	O
yd	O
'	O
bk	O
,	O
i	O
=	O
{	O
gn-l	O
(	O
xk	O
;	O
(	O
xi'yi	O
)	O
,	O
d~	O
)	O
¥yd	O
,	O
and	O
we	O
formally	O
replace	O
(	O
x	O
,	O
y	O
)	O
and	O
(	O
xi	O
,	O
yl	O
)	O
by	O
(	O
xa	O
,	O
ya	O
)	O
and	O
(	O
xf3	O
,	O
y	O
(	O
3	O
)	O
so	O
that	O
we	O
may	O
work	O
with	O
the	O
indices	O
a	O
and	O
f3	O
.	O
with	O
this	O
notation	O
,	O
we	O
have	O
shown	O
thus	O
far	O
the	O
following	O
:	O
e	O
{	O
(	O
l	O
(	O
d	O
)	O
-	O
l	O
)	O
2	O
}	O
<	O
2	O
.	O
+	O
p	O
{	O
aa,1,2	O
,	O
af3,1,2	O
}	O
-	O
p	O
{	O
aa,1,2	O
,	O
bi,2	O
}	O
n	O
n	O
-	O
n	O
note	O
that	O
also	O
,	O
p	O
{	O
aa,1,2	O
,	O
a	O
,	O
b,1,2	O
}	O
-	O
p	O
{	O
aa,1,2	O
,	O
b1,2	O
}	O
=	O
p	O
{	O
a	O
a	O
,1,2	O
,	O
af3	O
,1,2	O
}	O
-	O
p	O
{	O
a	O
a	O
,	O
t3,2	O
,	O
b	O
{	O
3,2	O
}	O
(	O
by	O
symmetry	O
)	O
p	O
{	O
aa	O
,	O
l,2	O
,	O
a	O
,	O
b,1,2	O
}	O
-	O
p	O
{	O
aa	O
,	O
,b,2	O
,	O
a	O
,	O
b,1,2	O
}	O
+	O
p	O
{	O
aa	O
,	O
,b,2	O
,	O
af3,1,2	O
}	O
-	O
p	O
{	O
aa	O
,	O
,b,2	O
,	O
b	O
,	O
b,2	O
}	O
i	O
+	O
ii	O
.	O
=	O
p	O
{	O
b1,2	O
,	O
b2	O
,	O
d	O
-	O
p	O
{	O
aa,1,2	O
,	O
bl,2	O
}	O
=	O
p	O
{	O
ba	O
,	O
{	O
3	O
,	O
b	O
{	O
3	O
,	O
a	O
}	O
-	O
p	O
{	O
aa	O
,	O
,b,2	O
,	O
b	O
{	O
3,2	O
}	O
(	O
by	O
symmetry	O
)	O
24.3	O
nearest	B
neighbor	I
rules	I
413	O
p	O
{	O
ba	O
,	O
tl	O
,	O
btl.a	O
}	O
-	O
p	O
{	O
aa	O
,	O
tl,2	O
,	O
btl	O
,	O
a	O
}	O
+	O
p	O
{	O
aa	O
,	O
tl,2	O
,	O
btl	O
,	O
a	O
}	O
-	O
p	O
{	O
aa	O
,	O
tl,2	O
,	O
btl,2	O
}	O
iii+iv	O
.	O
using	O
the	O
fact	O
that	O
for	O
events	O
{	O
c	O
}	O
'	O
ip	O
{	O
ci	O
,	O
c	O
j	O
}	O
-	O
p	O
{	O
ci	O
,	O
ck	O
}	O
1	O
:	O
:	O
:	O
:	O
p	O
{	O
cj	O
6ck	O
}	O
,	O
we	O
bound	O
i	O
<	O
p	O
{	O
aa	O
,	O
i,26aa	O
,	O
tl,2	O
}	O
'	O
def	O
ii	O
<	O
p	O
{	O
atl	O
,	O
i,26btl,2	O
}	O
=	O
v	O
,	O
iii	O
<	O
p	O
{	O
ba	O
,	O
tl6aa	O
,	O
tl,2	O
}	O
=	O
v	O
,	O
iv	O
<	O
p	O
{	O
btl	O
,	O
a	O
6btl,2	O
}	O
'	O
the	O
upper	O
bounds	O
for	O
ii	O
and	O
iii	O
are	O
identical	O
by	O
symmetry	O
.	O
also	O
,	O
and	O
iv	O
:	O
:	O
:	O
:	O
p	O
{	O
btl	O
,	O
a6atl	O
,	O
a,2	O
}	O
+	O
p	O
{	O
atl	O
,	O
a,26btl,2	O
}	O
=	O
2v	O
,	O
for	O
a	O
grand	O
total	O
of	O
6	O
v.	O
this	O
concludes	O
the	O
proof	O
.	O
d	O
24.3	O
nearest	B
neighbor	I
rules	I
theorem	O
24.2	O
can	O
be	O
used	O
to	O
obtain	O
distribution-free	O
upper	O
bounds	O
for	O
specific	O
rules	O
.	O
here	O
is	O
the	O
most	O
important	O
example	O
.	O
theorem	B
24.3	O
.	O
(	O
rogers	O
and	O
wagner	O
(	O
1978	O
»	O
.	O
let	O
gn	O
be	O
the	O
k-nearest	O
neighbor	B
rule	I
with	O
randomized	B
tie-breaking	O
.	O
if	O
l~d	O
)	O
is	O
the	O
deleted	B
estimate	O
with	O
gn-l	O
chosen	O
as	O
the	O
k-nn	O
rule	B
(	O
with	O
the	O
same	O
k	O
and	O
with	O
the	O
same	O
randomizing	O
random	O
variables	O
)	O
,	O
then	O
e	O
ln	O
-ln	O
{	O
(	O
(	O
d	O
)	O
)	O
2	O
}	O
6k+l	O
:	O
:	O
:	O
:	O
-	O
-	O
.	O
n	O
proof	O
.	O
because	O
of	O
the	O
randomized	B
tie-breaking	O
,	O
the	O
k-nn	O
rule	B
is	O
symmetric	O
,	O
and	O
theorem	B
24.2	O
is	O
applicable	O
.	O
we	O
only	O
have	O
to	O
show	O
that	O
clearly	O
,	O
gn	O
(	O
x	O
,	O
dn	O
)	O
=i	O
gn-l	O
(	O
x	O
,	O
dn-	O
l	O
)	O
can	O
happen	O
only	O
if	O
xn	O
is	O
among	O
the	O
k	O
nearest	O
neighbors	O
of	O
x.	O
but	O
the	O
probability	O
of	O
this	O
event	O
is	O
just	O
kin	O
,	O
since	O
by	O
symmetry	O
,	O
all	O
points	O
are	O
equally	O
likely	O
to	O
be	O
among	O
the	O
k	O
nearest	O
neighbors	O
.	O
d	O
414	O
24.	O
deleted	B
estimates	O
ofthe	O
error	O
probability	O
remark	O
.	O
if	O
gn	O
is	O
the	O
k-nn	O
rule	B
such	O
that	O
distance	B
ties	O
are	O
broken	O
by	O
compaling	O
indices	O
,	O
then	O
gn	O
is	O
not	O
symmetric	O
,	O
and	O
theorem	B
24.2	O
is	O
no	O
longer	O
applicable	O
(	O
unless	O
e.g.	O
,	O
x	O
has	O
a	O
density	O
)	O
.	O
another	O
non	O
symmetric	O
classifier	O
is	O
the	O
lazy	B
histogram	O
rule	B
:	O
o	O
remark	O
.	O
applying	O
theorem	B
24.3	O
to	O
the	O
i-nn	O
rule	B
,	O
the	O
cauchy-schwarz	O
inequality	B
implies	O
e	O
i	O
l~d	O
)	O
-	O
ln	O
i	O
:	O
:	O
;	O
,	O
j7/	O
n	O
for	O
all	O
distributions	O
.	O
0	O
remark	O
.	O
clearly	O
,	O
the	O
inequality	B
of	O
theorem	B
24.3	O
holds	O
for	O
any	O
rule	B
that	O
is	O
some	O
function	O
of	O
the	O
k	O
nearest	O
points	O
.	O
for	O
the	O
k-nearest	O
neighbor	B
rule	I
,	O
with	O
a	O
more	O
careful	O
analysis	O
,	O
devroye	O
and	O
wagner	O
(	O
1979a	O
)	O
improved	O
theorem	B
24.3	O
to	O
(	O
d	O
)	O
e	O
l	O
{	O
(	O
n	O
(	O
see	O
problem	O
24.8	O
)	O
.	O
0	O
)	O
2	O
}	O
n	O
24	O
,	O
j1	O
(	O
-l	O
<	O
-+	O
-	O
-	O
-	O
n	O
nhii	O
1	O
probability	O
inequalities	O
for	O
il~d	O
)	O
-	O
ln	O
i	O
can	O
also	O
be	O
obtained	O
with	O
further	O
work	O
.	O
by	O
chebyshev	O
's	O
inequality	B
we	O
immediately	O
get	O
so	O
that	O
the	O
above	O
bounds	O
on	O
the	O
expected	O
squared	B
error	I
can	O
be	O
used	O
.	O
sharper	O
distribution-free	O
inequalities	O
were	O
obtained	O
by	O
devroye	O
and	O
wagner	O
(	O
1979a	O
;	O
1979b	O
)	O
for	O
several	O
nonparametric	O
rules	O
.	O
here	O
we	O
present	O
a	O
result	O
that	O
follows	O
immediately	O
from	O
what	O
we	O
have	O
already	O
seen	O
:	O
theorem	B
24.4.	O
consider	O
the	O
k-nearest	O
neighbor	B
rule	I
with	O
randomized	B
tie-break	O
(	O
cid:173	O
)	O
ing	O
.	O
if	O
l~d	O
)	O
is	O
the	O
deleted	B
estimate	O
with	O
gn-l	O
chosen	O
as	O
the	O
k-nn	O
rule	B
with	O
the	O
same	O
tie-breaking	O
,	O
then	O
proof	O
.	O
the	O
result	O
follows	O
immediately	O
from	O
mcdiarmid	O
's	O
inequality	B
by	O
the	O
fol	O
(	O
cid:173	O
)	O
lowing	O
argument	O
:	O
from	O
lemma	O
11.1	O
,	O
given	O
n	O
points	O
in	O
nd	O
,	O
a	O
particular	O
point	O
can	O
be	O
among	O
the	O
k	O
nearest	O
neighbors	O
of	O
at	O
most	O
kyd	O
points	O
.	O
to	O
see	O
this	O
,	O
just	O
set	O
fjv	O
equal	O
to	O
the	O
empirical	B
measure	I
of	O
the	O
n	O
points	O
in	O
lemma	O
11.1.	O
therefore	O
,	O
changing	O
the	O
value	O
of	O
one	O
pair	O
from	O
the	O
training	O
data	O
can	O
change	O
the	O
value	O
of	O
the	O
estimate	B
by	O
at	O
most	O
2kyd	O
.	O
now	O
,	O
since	O
el~d	O
)	O
=	O
eln-l	O
,	O
theorem	B
9.1	O
yields	O
the	O
result	O
.	O
0	O
exponential	B
upper	O
bounds	O
for	O
the	O
probability	O
p	O
{	O
il~d	O
)	O
-	O
ln	O
i	O
>	O
e	O
}	O
are	O
typically	O
much	O
harder	O
to	O
obtain	O
.	O
we	O
mention	O
one	O
result	O
without	O
proof	O
.	O
24.4	O
kernel	B
rules	I
415	O
theorem	B
24.5	O
.	O
(	O
devroye	O
and	O
wagner	O
(	O
1979a	O
»	O
.	O
for	O
the	O
k-nearest	O
neighbor	B
rule	I
,	O
p	O
{	O
il~d	O
)	O
-	O
lnl	O
>	O
e	O
}	O
:	O
s	O
2e-ne2	O
/	O
18	O
+	O
6e-ne3	O
/	O
(	O
108k	O
(	O
yd+2	O
»	O
.	O
one	O
of	O
the	O
drawbacks	O
of	O
the	O
deleted	B
estimate	O
is	O
that	O
it	O
requires	O
much	O
more	O
computation	O
than	O
the	O
resubstitution	B
estimate	O
.	O
if	O
conditional	O
on	O
y	O
=	O
0	O
and	O
y	O
=	O
1	O
,	O
x	O
is	O
gaussian	B
,	O
and	O
the	O
classification	O
rule	B
is	O
the	O
appropriate	O
parametric	O
rule	O
,	O
then	O
the	O
estimate	B
can	O
be	O
computed	O
quickly	O
.	O
see	O
lachenbruch	O
and	O
mickey	O
(	O
1968	O
)	O
,	O
fukunaga	O
and	O
kessel	O
(	O
1971	O
)	O
,	O
and	O
mclachlan	O
(	O
1992	O
)	O
for	O
further	O
references	O
.	O
another	O
,	O
and	O
probably	O
more	O
serious	O
,	O
disadvantage	O
of	O
the	O
deleted	B
estimate	O
is	O
its	O
large	O
variance	O
.	O
this	O
fact	O
can	O
be	O
illustrated	O
by	O
the	O
following	O
example	O
from	O
devroye	O
and	O
wagner	O
(	O
1979b	O
)	O
:	O
let	O
n	O
be	O
even	O
,	O
and	O
let	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
be	O
such	O
that	O
y	O
is	O
independent	O
of	O
x	O
with	O
p	O
{	O
y	O
=	O
o	O
}	O
=	O
p	O
{	O
y	O
=	O
i	O
}	O
=	O
1/2	O
.	O
consider	O
the	O
k-nearest	O
neighbor	B
rule	I
with	O
k	O
=	O
n	O
1.	O
then	O
obviously	O
,	O
ln	O
=	O
1/2	O
.	O
clearly	O
,	O
if	O
the	O
number	O
of	O
zeros	O
and	O
ones	O
among	O
the	O
labels	O
y1	O
,	O
•••	O
,	O
yn	O
are	O
equal	O
,	O
then	O
l~d	O
)	O
=	O
1.	O
thus	O
,	O
for	O
0	O
<	O
e	O
<	O
1/2	O
,	O
p	O
{	O
il~	O
{	O
)	O
)	O
-	O
lnl	O
>	O
e	O
}	O
2	O
:	O
p	O
{	O
t	O
i	O
{	O
y	O
;	O
=ll	O
}	O
=	O
~	O
}	O
=	O
;	O
n	O
c~2	O
)	O
.	O
by	O
stirling	O
's	O
formula	O
(	O
lemma	O
a.3	O
)	O
,	O
we	O
have	O
(	O
d	O
)	O
p	O
{	O
iln	O
-	O
lnl	O
>	O
e	O
}	O
~	O
~	O
1/12	O
.	O
1	O
1	O
v	O
2nn	O
e	O
therefore	O
,	O
for	O
this	O
simple	O
rule	O
and	O
certain	O
distributions	O
,	O
the	O
probability	O
above	O
can	O
not	O
decrease	O
to	O
zero	O
faster	O
than	O
1/	O
-jii	O
.	O
note	O
that	O
in	O
the	O
example	O
above	O
,	O
el~d	O
)	O
=	O
eln-1	O
=	O
1/2	O
,	O
so	O
the	O
lower	O
bound	O
holds	O
for	O
p	O
{	O
il~d	O
)	O
el~d	O
)	O
i	O
>	O
e	O
}	O
as	O
well	O
.	O
also	O
,	O
in	O
this	O
example	O
,	O
we	O
have	O
e	O
{	O
(	O
l~d	O
)	O
-	O
el~	O
»	O
)	O
2	O
}	O
~	O
~p	O
{	O
t	O
l	O
(	O
y	O
;	O
=o	O
}	O
=	O
!	O
2	O
.	O
}	O
2	O
:	O
~	O
1:12	O
.	O
2	O
4	O
2nn	O
e	O
4	O
i=l	O
in	O
chapter	O
31	O
we	O
describe	O
other	O
estimates	O
with	O
much	O
smaller	O
variances	O
.	O
24.4	O
kernel	B
rules	I
theorem	O
24.2	O
may	O
also	O
be	O
used	O
to	O
obtain	O
tight	O
distribution-free	O
upper	O
bounds	O
for	O
the	O
performance	O
of	O
the	O
deleted	B
estimate	O
of	O
the	O
error	O
probability	O
of	B
kernel	I
rules	I
.	O
we	O
have	O
the	O
following	O
bound	O
:	O
theorem	B
24.6.	O
assume	O
that	O
k	O
~	O
0	O
is	O
a	O
regular	O
kernel	B
of	O
bounded	O
support	B
,	O
that	O
is	O
,	O
it	O
is	O
a	O
function	O
satisfying	O
(	O
i	O
)	O
(	O
ii	O
)	O
(	O
iii	O
)	O
k	O
(	O
x	O
)	O
~	O
[	O
3	O
,	O
k	O
(	O
x	O
)	O
:	O
s	O
b	O
,	O
k	O
(	O
x	O
)	O
=	O
0	O
,	O
iixll	O
:	O
s	O
p	O
,	O
ilxll	O
>	O
r	O
,	O
416	O
24	O
,	O
deleted	B
estimates	O
of	O
the	O
error	O
probability	O
for	O
some	O
positive	O
finite	O
constants	O
f3	O
,	O
p	O
,	O
band	O
r.	O
let	O
the	O
kernel	B
rule	I
be	O
defined	O
by	O
(	O
x	O
)	O
=	O
{	O
o	O
ifl~1=1	O
i	O
{	O
yi=o	O
}	O
k	O
(	O
x	O
-	O
xi	O
)	O
:	O
:	O
l~1=1	O
i	O
{	O
yi=l	O
}	O
k	O
(	O
x	O
-	O
xi	O
)	O
gn	O
1	O
otherwise	O
,	O
and	O
define	O
gn-1	O
similarly	O
.	O
then	O
there	O
exist	O
constants	O
c1	O
(	O
d	O
)	O
depending	O
upon	O
d	O
only	O
and	O
c2	O
(	O
k	O
)	O
depending	O
upon	O
k	O
only	O
such	O
that	O
for	O
all	O
n	O
,	O
one	O
may	O
take	O
c2	O
(	O
k	O
)	O
=	O
6	O
(	O
1	O
+	O
rlp	O
)	O
d	O
/2	O
min	O
(	O
2	O
,	O
b	O
i	O
f3	O
)	O
.	O
remark	O
.	O
since	O
c2	O
(	O
k	O
)	O
is	O
a	O
scale-invariant	O
factor	O
,	O
the	O
theorem	B
applies	O
to	O
the	O
rule	B
with	O
k	O
(	O
u	O
)	O
replaced	O
by	O
kh	O
(	O
u	O
)	O
=	O
tk	O
(	O
*	O
)	O
for	O
any	O
smoothing	B
factor	I
.	O
as	O
itis	O
,	O
c2	O
(	O
k	O
)	O
is	O
minimal	O
and	O
equal	O
to	O
12	O
if	O
we	O
let	O
k	O
be	O
the	O
uniform	B
kernel	O
on	O
the	O
unit	O
ball	O
(	O
r	O
=	O
p	O
,	O
b	O
=	O
f3	O
)	O
.	O
the	O
assumptions	O
of	O
the	O
theorem	B
require	O
that	O
gn-l	O
is	O
defined	O
with	O
the	O
same	O
kernel	B
and	O
smoothing	B
factor	I
as	O
gn	O
'	O
0	O
remark	O
.	O
the	O
theorem	B
applies	O
to	O
virtually	O
any	O
kernel	B
of	O
compact	O
support	B
that	O
is	O
of	O
interest	O
to	O
the	O
practitioners	O
.	O
note	O
,	O
however	O
,	O
that	O
the	O
gaussian	B
kernel	O
is	O
not	O
covered	O
by	O
the	O
result	O
.	O
the	O
theorem	B
generalizes	O
an	O
earlier	O
result	O
of	O
devroye	O
and	O
wagner	O
(	O
1979b	O
)	O
,	O
in	O
which	O
a	O
more	O
restricted	O
class	O
of	O
kernels	O
was	O
considered	O
.	O
they	O
showed	O
that	O
if	O
k	O
is	O
the	O
uniform	B
kernel	O
then	O
cd	O
)	O
e	O
ln	O
{	O
(	O
)	O
2	O
}	O
-	O
ln	O
1	O
24	O
:	O
:	O
:	O
2n	O
+	O
jfi	O
'	O
see	O
problem	O
24.4	O
.	O
0	O
we	O
need	O
the	O
following	O
auxiliary	O
inequality	B
,	O
which	O
we	O
quote	O
without	O
proof	O
:	O
lemma	O
24.1	O
.	O
(	O
petrov	O
(	O
1975	O
)	O
,	O
p.44	O
)	O
.	O
let	O
zl	O
,	O
...	O
,	O
zn	O
be	O
real-valuedi.i.d	O
.	O
random	O
variables	O
.	O
for	O
e	O
>	O
0	O
,	O
where	O
c	O
is	O
a	O
universal	O
constant	O
.	O
corollary	O
24.1.	O
let	O
zl	O
,	O
...	O
,	O
zn	O
be	O
real-valued	O
i.i.d	O
.	O
random	O
variables	O
.	O
fore	O
2	O
:	O
a	O
>	O
0	O
,	O
i	O
e	O
}	O
ce	O
z·	O
<	O
-	O
<	O
-	O
-	O
p	O
{	O
i	O
n	O
~	O
i	O
2	O
-	O
jfi	O
a	O
-jp	O
{	O
i	O
z	O
11	O
:	O
:	O
a/2	O
}	O
'	O
1	O
where	O
c	O
is	O
a	O
universal	O
constant	O
.	O
proof	O
of	O
theorem	O
24.6.	O
we	O
apply	O
theorem	B
24.2	O
,	O
by	O
finding	O
an	O
upper	O
bound	O
for	O
24.5	O
histogram	O
rules	O
417	O
for	O
the	O
kernel	B
rule	I
with	O
kernel	B
k	O
:	O
::0	O
,	O
in	O
whichh	O
is	O
absorbed	O
,	O
p	O
{	O
gn	O
(	O
x	O
,	O
dn	O
)	O
=i	O
gn-l	O
(	O
x	O
,	O
dn-	O
l	O
)	O
}	O
:	O
s	O
p	O
{	O
i	O
~	O
(	O
2yi	O
-	O
l	O
)	O
k	O
(	O
x	O
-	O
xill	O
:	O
s	O
k	O
(	O
x	O
-	O
xn	O
)	O
,	O
k	O
(	O
x	O
-	O
xn	O
)	O
>	O
o	O
}	O
.	O
define	O
b	O
'	O
=	O
max	O
(	O
2fj	O
,	O
b	O
)	O
.	O
we	O
have	O
p	O
{	O
1~	O
(	O
2yi	O
-	O
l	O
)	O
k	O
(	O
x	O
-	O
x	O
;	O
)	O
i	O
:	O
s	O
k	O
(	O
x	O
-	O
xn	O
)	O
,	O
k	O
(	O
x	O
-	O
xn	O
)	O
>	O
o	O
}	O
<	O
p	O
{	O
1~	O
(	O
2yi	O
-	O
i	O
)	O
k	O
(	O
x	O
-	O
xi	O
)	O
1	O
:	O
s	O
b	O
'	O
,	O
xn	O
e	O
sx.r	O
}	O
(	O
where	O
sx	O
,	O
r	O
is	O
the	O
ball	O
of	O
radius	O
r	O
centered	O
at	O
x	O
)	O
=	O
e	O
{	O
i	O
{	O
xnesx	O
,	O
r	O
}	O
2fj.jn	O
''	O
,	O
jp	O
{	O
i	O
(	O
2yi	O
-	O
2cb	O
'	O
l	O
)	O
k	O
(	O
x	O
-	O
xdl	O
~	O
fjix	O
}	O
}	O
(	O
by	O
corollary	O
24.1	O
,	O
since	O
2fj	O
:	O
:	O
:	O
:	O
b	O
'	O
)	O
<	O
e	O
{	O
i	O
{	O
xnesx	O
,	O
r	O
}	O
fjjnfl	O
(	O
sx	O
,	O
p	O
)	O
cb	O
'	O
j	O
(	O
recall	O
that	O
k	O
(	O
u	O
)	O
~	O
fj	O
for	O
liull	O
:	O
:	O
:	O
:	O
p	O
,	O
so	O
that	O
p	O
{	O
i	O
(	O
2yi	O
-	O
i	O
)	O
k	O
(	O
x	O
-	O
xi	O
)	O
i	O
~	O
fj	O
}	O
~	O
p	O
{	O
x	O
i	O
~	O
sx	O
,	O
p	O
}	O
)	O
cb	O
'	O
f	O
is	O
-	O
fj.jn	O
''	O
=	O
fl	O
(	O
dy	O
)	O
fl	O
(	O
dx	O
)	O
sx	O
,	O
r	O
j	O
fl	O
(	O
sx	O
,	O
p	O
)	O
ccd	O
b	O
'	O
(	O
r	O
)	O
d/2	O
-	O
-	O
1+-	O
p	O
fj.jn	O
''	O
where	O
we	O
used	O
lemma	O
10.2.	O
the	O
constant	O
cd	O
depends	O
upon	O
the	O
dimension	B
only	O
.	O
o	O
24.5	O
histogram	O
rules	O
in	O
this	O
section	O
we	O
discuss	O
properties	O
of	O
the	O
deleted	B
estimate	O
of	O
the	O
error	O
probability	O
of	O
histogram	O
rules	O
.	O
let	O
p	O
=	O
{	O
ai	O
,	O
a	O
2	O
,	O
...	O
}	O
be	O
a	O
partition	O
of	O
r	O
d	O
,	O
and	O
let	O
gn	O
be	O
the	O
corresponding	O
histogram	O
classifier	O
(	O
see	O
chapters	O
6	O
and	O
9	O
)	O
.	O
to	O
get	O
a	O
performance	O
bound	O
for	O
the	O
deleted	B
estimate	O
,	O
we	O
can	O
simply	O
apply	O
theorem	B
24.2	O
.	O
418	O
24.	O
deleted	B
estimates	O
of	O
the	O
error	O
probability	O
theorem	B
24.7.	O
for	O
the	O
histogram	B
rule	I
gn	O
corresponding	O
to	O
any	O
partition	B
p	O
,	O
and	O
for	O
all	O
n	O
,	O
e	O
{	O
(	O
l	O
(	O
d	O
)	O
_	O
l	O
)	O
2	O
}	O
<	O
_	O
+	O
6	O
''	O
f-l	O
1	O
n	O
n	O
-	O
l	O
j	O
(	O
_	O
1	O
)	O
lr	O
i	O
+	O
6	O
''	O
1/2	O
(	O
a	O
.	O
)	O
e-n/l	O
(	O
a	O
i	O
)	O
,	O
(	O
a	O
)	O
3/2	O
i	O
7t	O
n	O
n	O
i	O
i	O
and	O
in	O
particular	O
,	O
d	O
)	O
e	O
ln	O
{	O
(	O
)	O
2	O
}	O
-	O
ln	O
1	O
+	O
6/	O
e	O
<	O
-	O
-	O
+	O
-	O
n	O
6	O
.	O
j7t	O
(	O
n	O
-	O
1	O
)	O
proof	O
.	O
the	O
first	O
inequality	B
follows	O
from	O
theorem	B
24.2	O
if	O
we	O
can	O
find	O
an	O
upper	O
bound	O
for	O
p	O
{	O
gn	O
(	O
x	O
)	O
=i	O
gn-l	O
(	O
x	O
)	O
}	O
.	O
we	O
introduce	O
the	O
notation	O
clearly	O
,	O
gn-l	O
(	O
x	O
)	O
can	O
differ	O
from	O
gn	O
(	O
x	O
)	O
only	O
if	O
both	O
xn	O
and	O
x	O
fall	O
in	O
the	O
same	O
cell	O
of	O
the	O
partition	B
,	O
and	O
if	O
the	O
number	O
of	O
zeros	O
in	O
the	O
cell	O
is	O
either	O
equal	O
,	O
or	O
less	O
by	O
one	O
than	O
the	O
number	O
of	O
ones	O
.	O
therefore	O
,	O
by	O
independence	O
,	O
we	O
have	O
p	O
{	O
gn	O
(	O
x	O
)	O
=i	O
gn-l	O
(	O
x	O
)	O
}	O
l	O
p	O
{	O
gn	O
(	O
x	O
)	O
=i	O
gn-l	O
(	O
x	O
)	O
1	O
x	O
e	O
ai	O
,	O
xn	O
e	O
ad	O
f-l	O
(	O
ai	O
)	O
2	O
''	O
{	O
if-ln-i	O
(	O
ajji	O
<	O
7	O
p	O
vo	O
,	O
n-l	O
(	O
ai	O
)	O
=	O
xeai	O
,	O
xneai	O
2	O
}	O
f-l	O
(	O
aj	O
.	O
2	O
the	O
terms	O
in	O
the	O
sum	O
above	O
may	O
be	O
bounded	O
as	O
follows	O
:	O
p	O
vo	O
,	O
n-1	O
(	O
a	O
)	O
=	O
{	O
i	O
f-ln-1	O
(	O
a	O
)	O
ji	O
2	O
xea	O
,	O
xnea	O
}	O
<	O
p	O
{	O
f-ln-1	O
(	O
a	O
)	O
=	O
o	O
}	O
2	O
p	O
vo	O
,	O
n-1	O
(	O
a	O
)	O
=	O
(	O
by	O
independence	O
)	O
if-ln-1	O
(	O
a	O
)	O
j	O
}	O
{	O
+	O
e	O
{	O
p	O
{	O
vo	O
,	O
''	O
-i	O
(	O
al	O
=	O
if	O
.	O
'n-~	O
(	O
a	O
)	O
j	O
i	O
xi	O
,	O
``	O
,	O
,	O
xn	O
}	O
j	O
[	O
~	O
''	O
_	O
,	O
(	O
a	O
»	O
o	O
)	O
}	O
(	O
l	O
-	O
f.	O
'	O
(	O
a	O
l	O
)	O
''	O
+	O
e	O
{	O
j2rr	O
(	O
n	O
_	O
l1	O
)	O
f.'n_1	O
(	O
a	O
1	O
i	O
{	O
~	O
''	O
,	O
(	O
a	O
»	O
o	O
)	O
}	O
:0	O
:	O
(	O
by	O
lemma	O
a.3	O
)	O
:	O
:	O
:	O
(	O
1	O
-	O
f-l	O
(	O
a	O
)	O
r	O
+	O
e	O
{	O
i	O
{	O
/ln_l	O
(	O
a	O
»	O
o	O
}	O
}	O
(	O
by	O
jensen	O
's	O
inequality	B
)	O
27t	O
(	O
n	O
-	O
1	O
)	O
mn-1	O
(	O
a	O
)	O
<	O
e-n/l	O
(	O
a	O
)	O
+	O
problems	O
and	O
exercises	O
419	O
where	O
in	O
the	O
last	O
step	O
we	O
use	O
lemma	O
a.2	O
.	O
this	O
concludes	O
the	O
proof	O
of	O
the	O
first	O
inequality	B
.	O
the	O
second	O
one	O
follows	O
trivially	O
by	O
noting	O
that	O
xe-x	O
:	O
:	O
:	O
:	O
;	O
i/e	O
for	O
all	O
x.	O
o	O
remark	O
.	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
inequalities	O
of	O
theorem	O
24.7	O
are	O
tight	O
,	O
up	O
to	O
a	O
constant	O
factor	O
,	O
in	O
the	O
sense	O
that	O
for	O
any	O
partition	B
p	O
,	O
there	O
exists	O
a	O
distribution	O
such	O
that	O
d	O
)	O
e	O
ln	O
{	O
(	O
-	O
ln	O
)	O
2	O
}	O
1	O
1	O
~	O
~	O
1/12	O
4	O
2nn	O
e	O
(	O
see	O
problem	O
24.5	O
)	O
.	O
0	O
remark	O
.	O
the	O
second	O
inequality	B
in	O
theorem	B
23.3	O
points	O
out	O
an	O
important	O
difference	O
between	O
the	O
behavior	O
of	O
the	O
resubstitution	B
and	O
the	O
deleted	B
estimates	O
for	O
histogram	O
rules	O
.	O
as	O
mentioned	O
above	O
,	O
for	O
some	O
distributions	O
the	O
variance	B
of	I
l~	O
)	O
can	O
be	O
of	O
the	O
order	O
i/.jli	O
.	O
this	O
should	O
be	O
contrasted	O
with	O
the	O
much	O
smaller	O
variance	B
of	I
the	O
resubstitution	B
estimate	O
.	O
the	O
small	O
variance	B
of	I
l~r	O
)	O
comes	O
often	O
with	O
a	O
larger	O
bias	B
.	O
other	O
types	O
of	O
error	O
estimates	O
with	O
small	O
variance	O
are	O
discussed	O
in	O
chapter	O
31	O
.	O
0	O
remark	O
.	O
theorem	B
24.3	O
shows	O
that	O
for	O
any	O
partition	B
,	O
sup	O
e	O
{	O
(	O
l~d	O
)	O
-	O
ln	O
)	O
2	O
}	O
=	O
0	O
(	O
~	O
)	O
.	O
~n	O
(	O
x	O
,	O
y	O
)	O
on	O
the	O
other	O
hand	O
,	O
if	O
k	O
=	O
o	O
(	O
.jli	O
)	O
,	O
where	O
k	O
is	O
the	O
number	O
of	O
cells	O
in	O
the	O
partition	B
,	O
then	O
for	O
the	O
resubstitution	B
estimate	O
we	O
have	O
a	O
better	O
guaranteed	O
distribution-free	O
performance	O
:	O
sup	O
e	O
{	O
(	O
l~r	O
)	O
-	O
ln	O
)	O
2	O
}	O
=	O
0	O
(	O
~	O
)	O
.	O
~n	O
(	O
x	O
,	O
y	O
)	O
at	O
first	O
sight	O
,	O
the	O
resubstitution	B
estimate	O
seems	O
preferable	O
to	O
the	O
deleted	B
estimate	O
.	O
however	O
,	O
if	O
the	O
partition	B
has	O
a	O
large	O
number	O
of	O
cells	O
,	O
l~r	O
)	O
may	O
be	O
off	O
the	O
mark	O
;	O
see	O
theorem	B
23.4	O
.	O
0	O
problems	O
and	O
exercises	O
problem	O
24.1.	O
show	O
the	O
following	O
variant	O
of	O
theorem	O
24.2	O
:	O
for	O
all	O
symmetric	O
classifiers	O
e	O
{	O
(	O
l	O
:	O
fl	O
-	O
ln	O
)	O
2	O
}	O
:	O
s	O
~	O
+	O
2p	O
{	O
gn	O
(	O
x	O
,	O
dn	O
)	O
=i	O
gn-l	O
(	O
x	O
,	O
dn-	O
1	O
)	O
}	O
+	O
p	O
{	O
gn	O
(	O
x	O
,	O
dn	O
)	O
=i	O
gn	O
(	O
x	O
,	O
d~	O
)	O
}	O
+	O
p	O
{	O
gn-l	O
(	O
x	O
,	O
dn-	O
1	O
)	O
=i	O
gn-l	O
(	O
x	O
,	O
d~_j	O
}	O
,	O
420	O
24.	O
deleted	B
estimates	O
of	O
the	O
error	O
probability	O
where	O
d	O
,	O
:	O
and	O
dz-	O
i	O
are	O
just	O
dn	O
and	O
dn-	O
l	O
with	O
(	O
xl	O
,	O
yj	O
)	O
replaced	O
by	O
an	O
independent	O
copy	O
(	O
xo	O
,	O
yo	O
)	O
.	O
problem	O
24.2.	O
let	O
gn	O
be	O
the	O
relabeling	B
nn	O
rule	B
with	O
the	O
k-nn	O
classifier	B
as	O
ancestral	B
rule	I
as	O
defined	O
in	O
chapter	O
11.	O
provide	O
an	O
upper	O
bound	O
for	O
the	O
squared	B
error	I
e	O
{	O
(	O
l~d	O
)	O
-	O
ln	O
)	O
2	O
}	O
of	O
the	O
deleted	B
estimate	O
.	O
problem	O
24.3.	O
let	O
gil	O
be	O
the	O
rule	B
obtained	O
by	O
choosing	O
the	O
best	O
k	O
:	O
:	O
:	O
:	O
ko	O
in	O
the	O
k-nn	O
rule	B
(	O
ko	O
is	O
a	O
constant	O
)	O
by	O
minimizing	O
the	O
standard	B
deleted	O
estimate	B
l	O
~d	O
)	O
with	O
respect	O
to	O
k.	O
how	O
would	O
you	O
estimate	B
the	O
probability	O
of	O
error	O
for	O
this	O
rule	B
?	O
give	O
the	O
best	O
possible	O
distribution-free	O
performance	O
guarantees	O
you	O
can	O
find	O
.	O
problem	O
24.4.	O
consider	O
the	O
kernel	B
rule	I
with	O
the	O
window	B
kernel	O
k	O
=	O
so	O
,	O
i.	O
show	O
that	O
e	O
{	O
(	O
l	O
(	O
d	O
)	O
-l	O
)	O
2	O
}	O
<	O
__	O
+	O
n	O
n	O
-	O
1	O
+	O
6je	O
n	O
6	O
.	O
in	O
(	O
n	O
-	O
1	O
)	O
hint	O
:	O
follow	O
the	O
line	O
of	O
the	O
proof	O
of	O
theorem	O
24.7.	O
problem	O
24.5.	O
show	O
that	O
for	O
any	O
partition	B
p	O
,	O
there	O
exists	O
a	O
distribution	O
such	O
that	O
for	O
the	O
deleted	B
estimate	O
of	O
the	O
error	O
probability	O
of	O
the	O
corresponding	O
histogram	B
rule	I
,	O
e	O
{	O
(	O
(	O
d	O
)	O
ln	O
)	O
2	O
}	O
-	O
ln	O
1	O
1	O
2	O
:	O
4j2nn	O
e	O
l	O
/	O
12	O
.	O
hint	O
:	O
proceed	O
as	O
in	O
the	O
proof	O
of	O
the	O
similar	O
inequality	B
for	O
k-nearest	O
neighbor	O
rules	O
.	O
problem	O
24.6.	O
consider	O
the	O
k-spacings	O
method	O
(	O
see	O
chapter	O
21	O
)	O
.	O
we	O
estimate	B
the	O
proba~	O
bility	O
of	O
error	O
(	O
ln	O
)	O
by	O
a	O
modified	O
deleted	B
estimate	O
ln	O
as	O
follows	O
:	O
where	O
gn	O
,	O
i	O
is	O
a	O
histogram	O
rule	B
based	O
upon	O
the	O
same	O
k-spacings	O
partition	B
used	O
for	O
gn-that	O
is	O
,	O
the	O
partition	B
determined	O
by	O
k-spacings	O
of	O
the	O
data	O
points	O
x	O
i	O
,	O
...	O
,	O
xn-but	O
in	O
which	O
a	O
majority	O
vote	O
is	O
based	O
upon	O
the	O
yj	O
's	O
in	O
the	O
same	O
cell	O
of	O
the	O
partition	B
with	O
yi	O
deleted	B
.	O
show	O
that	O
e	O
{	O
(	O
ln	O
-ln	O
)	O
}	O
:	O
:	O
:	O
:	O
6k+l	O
.	O
n	O
hint	O
:	O
condition	O
on	O
the	O
xi	O
's	O
,	O
and	O
verify	O
that	O
the	O
inequality	B
of	O
theorem	B
24.2	O
remains	O
valid	O
.	O
problem	O
24.7.	O
consider	O
a	O
rule	O
in	O
which	O
we	O
rank	O
the	O
real-valued	O
observations	O
xi	O
,	O
...	O
,	O
xn	O
from	O
small	O
to	O
large	O
,	O
to	O
obtain	O
xc	O
!	O
)	O
,	O
...	O
,	O
x	O
(	O
n	O
)	O
'	O
assume	O
that	O
xi	O
has	O
a	O
density	O
.	O
derive	O
an	O
inequality	B
for	O
the	O
error	O
il~d	O
)	O
-	O
lili	O
for	O
some	O
deleted	B
estimate	O
l~d	O
)	O
(	O
of	O
your	O
choice	O
)	O
,	O
when	O
the	O
rule	B
is	O
defined	O
by	O
a	O
majority	O
vote	O
over	O
the	O
data-dependent	B
partition	O
defined	O
by	O
1,2,3,4	O
,	O
...	O
points	O
,	O
respectively	O
.	O
problems	O
and	O
exercises	O
421	O
problem	O
24.8.	O
prove	O
that	O
for	O
the	O
k-nearest	O
neighbor	B
rule	I
,	O
e	O
{	O
(	O
l~d	O
)	O
-	O
ln	O
)	O
2	O
}	O
:	O
s	O
.	O
!	O
.	O
+	O
24-jk	O
n.	O
n-jiii	O
(	O
devroye	O
and	O
wagner	O
(	O
1979a	O
)	O
)	O
.	O
hint	O
:	O
obtain	O
a	O
refined	O
upper	O
bound	O
for	O
using	O
techniques	O
not	O
unlike	O
those	O
of	O
the	O
proof	O
of	O
theorem	O
24.7.	O
problem	O
24.9.	O
open-ended	O
problem	O
.	O
investigate	O
if	O
theorem	B
24.6	O
can	O
be	O
extended	O
to	O
ker	O
(	O
cid:173	O
)	O
nels	O
with	O
unbounded	O
support	B
such	O
as	O
the	O
gaussian	B
kernel	O
.	O
25	O
automatic	B
kernel	O
rules	O
we	O
saw	O
in	O
chapter	O
10	O
that	O
for	O
a	O
large	O
class	O
of	O
kernels	O
,	O
if	O
the	O
smoothing	O
parameter	O
h	O
converges	O
to	O
zero	O
such	O
that	O
nh	O
d	O
goes	O
to	O
infinity	O
as	O
n	O
-+	O
00	O
,	O
then	O
the	O
kernel	B
classifi	O
(	O
cid:173	O
)	O
cation	O
rule	B
is	O
universally	O
consistent	O
.	O
for	O
a	O
particular	O
n	O
,	O
asymptotic	O
results	O
provide	O
little	O
guidance	O
in	O
the	O
selection	B
of	O
h.	O
on	O
the	O
other	O
hand	O
,	O
selecting	O
the	O
wrong	O
value	O
of	O
h	O
may	O
lead	O
to	O
catastrophic	O
error	O
rates-in	O
fact	O
,	O
the	O
crux	O
of	O
every	O
nonparametric	O
estimation	B
problem	O
is	O
the	O
choice	O
of	O
an	O
appropriate	O
smoothing	B
factor	I
.	O
it	O
tells	O
us	O
how	O
far	O
we	O
generalize	O
each	O
data	O
point	O
xi	O
in	O
the	O
space	O
.	O
purely	O
atomic	O
distributions	O
require	O
little	O
smoothing	O
(	O
h	O
=	O
0	O
will	O
generally	O
be	O
fine	O
)	O
,	O
while	O
distributions	O
with	O
densities	O
require	O
a	O
lot	O
of	O
smoothing	O
.	O
as	O
there	O
are	O
no	O
simple	O
tests	O
for	O
verifying	O
whether	O
the	O
data	O
are	O
drawn	O
from	O
an	O
absolutely	O
continuous	O
distribution-let	O
alone	O
a	O
distribution	O
with	O
a	O
lipschitz	O
density-it	O
is	O
important	O
to	O
let	O
the	O
data	O
dn	O
deter	O
(	O
cid:173	O
)	O
mine	O
h.	O
a	O
data-dependent	O
smoothing	B
factor	I
is	O
merely	O
a	O
mathematical	O
function	O
hn	O
:	O
(	O
rd	O
x	O
{	O
o	O
,	O
l	O
}	O
f	O
-+	O
[	O
0	O
,	O
00	O
)	O
.	O
for	O
brevity	O
,	O
we	O
will	O
simply	O
write	O
hn	O
to	O
denote	O
the	O
random	O
variable	B
hn	O
(	O
dn	O
)	O
.	O
this	O
chapter	O
develops	O
results	O
regarding	O
such	O
functions	O
hn	O
.	O
this	O
chapter	O
is	O
not	O
a	O
luxury	O
but	O
a	O
necessity	O
.	O
anybody	O
developing	O
software	O
for	O
pattern	O
recognition	O
must	O
necessarily	O
let	O
the	O
data	O
do	O
the	O
talking-in	O
fact	O
,	O
good	O
universally	O
applicable	O
programs	O
can	O
have	O
only	O
data-dependent	B
parameters	O
.	O
consider	O
the	O
family	B
of	I
kernel	O
decision	O
rules	O
gn	O
and	O
let	O
the	O
smoothing	B
factor	I
h	O
play	O
the	O
role	O
of	O
parameter	O
.	O
the	O
best	O
parameter	O
(	O
hopt	O
)	O
is	O
the	O
one	O
that	O
minimizes	O
ln	O
.	O
unfortunately	O
,	O
it	O
is	O
unknown	O
,	O
as	O
is	O
l	O
opt	O
,	O
the	O
corresponding	O
minimal	O
probability	O
of	O
error	O
.	O
the	O
first	O
goal	O
of	O
any	O
data-dependent	B
smoothing	O
factor	O
hn	O
should	O
be	O
to	O
approach	O
the	O
performance	O
of	O
hopt	O
'	O
we	O
are	O
careful	O
here	O
to	O
avoid	O
saying	O
that	O
hn	O
should	O
be	O
close	O
to	O
hopt	O
>	O
as	O
closeness	O
of	O
smoothing	O
factors	O
does	O
not	O
necessarily	O
imply	O
closeness	O
of	O
error	O
probabilities	O
and	O
vice	O
versa	O
.	O
guarantees	O
one	O
might	O
want	O
424	O
2~	O
.	O
automatic	B
kernel	O
rules	O
in	O
this	O
respect	O
are	O
e	O
{	O
ln	O
-	O
l	O
opt	O
}	O
~	O
an	O
for	O
some	O
suitable	O
sequence	O
an	O
~	O
0	O
,	O
or	O
better	O
still	O
,	O
e	O
{	O
ln	O
-	O
l	O
*	O
}	O
~	O
(	O
1	O
+	O
.bn	O
)	O
e	O
{	O
lopt	O
-	O
l	O
*	O
}	O
,	O
for	O
another	O
sequence.bn	O
~	O
o.	O
but	O
before	O
one	O
even	O
attempts	O
to	O
develop	O
such	O
data	O
(	O
cid:173	O
)	O
dependent	O
smoothing	O
factors	O
,	O
one	O
's	O
first	O
concern	O
should	O
be	O
with	O
consistency	O
:	O
is	O
it	O
true	O
that	O
with	O
the	O
given	O
hn	O
,	O
ln	O
~	O
l	O
*	O
in	O
probability	O
or	O
with	O
probability	O
one	O
?	O
this	O
question	O
is	O
dealt	O
with	O
in	O
the	O
next	O
section	O
.	O
in	O
subsequent	O
sections	O
,	O
we	O
give	O
various	O
examples	O
of	O
data-dependent	O
smoothing	O
factors	O
.	O
25.1	O
consistency	B
we	O
start	O
with	O
consistency	O
results	O
that	O
generalize	O
theorem	B
10.1.	O
the	O
first	O
result	O
assumes	O
that	O
the	O
value	O
of	O
the	O
smoothing	O
parameter	O
is	O
picked	O
from	O
a	O
discrete	O
set	O
.	O
theorem	B
25.1.	O
assume	O
that	O
the	O
random	O
variable	B
hn	O
takes	O
its	O
values	O
from	O
the	O
set	O
of	O
real	O
numbers	O
of	O
the	O
form	O
(	O
1+lk	O
'	O
where	O
k	O
is	O
a	O
nonnegative	O
integer	O
and	O
8n	O
>	O
o.	O
let	O
k	O
be	O
a	O
regular	O
kernel	B
function	O
.	O
(	O
recall	O
definition	O
10.1	O
.	O
)	O
define	O
the	O
kernel	B
classification	O
rule	B
corresponding	O
to	O
the	O
random	O
smoothing	O
parameter	O
hn	O
by	O
gn	O
(	O
x	O
)	O
=	O
k	O
(	O
x-xi	O
)	O
~	O
:	O
:	O
:	O
:	O
li=l	O
{	O
yi=l	O
}	O
~	O
''	O
,	O
n	O
i	O
k	O
(	O
x-xi	O
)	O
{	O
°	O
if	O
''	O
,	O
n	O
1	O
otherwise	O
.	O
i	O
1	O
li=l	O
{	O
yi=o	O
}	O
if	O
and	O
hn	O
~	O
°	O
and	O
nhi~	O
~	O
00	O
with	O
probability	O
one	O
as	O
n	O
~	O
00	O
,	O
then	O
l	O
(	O
gn	O
)	O
~	O
l	O
*	O
with	O
probability	O
one	O
,	O
that	O
is	O
,	O
gn	O
is	O
strongly	O
universally	O
consistent	O
.	O
proof	O
.	O
the	O
theorem	B
is	O
a	O
straightforward	O
extension	O
of	O
theorem	O
10.1.	O
clearly	O
,	O
l	O
(	O
gn	O
)	O
~	O
l*	O
with	O
probability	O
one	O
if	O
and	O
only	O
if	O
for	O
every	O
e	O
>	O
0	O
,	O
i	O
{	O
l	O
(	O
gn	O
)	O
-l*	O
>	O
e	O
}	O
-+	O
0	O
with	O
probability	O
one	O
.	O
now	O
,	O
for	O
any	O
.b	O
>	O
0	O
,	O
i	O
{	O
l	O
(	O
gn	O
)	O
-l*	O
>	O
e	O
}	O
~	O
i	O
{	O
l/hn	O
>	O
f3	O
,	O
nh	O
,	O
~	O
>	O
f3	O
,	O
l	O
(	O
gn	O
)	O
-l*	O
>	O
e	O
}	O
+	O
i	O
{	O
l/hnsf3	O
}	O
+	O
i	O
{	O
nh	O
,	O
~sf3	O
}	O
'	O
we	O
have	O
to	O
show	O
that	O
the	O
random	O
variables	O
on	O
the	O
right-hand	O
side	O
converge	O
to	O
zero	O
with	O
probability	O
one	O
.	O
the	O
convergence	O
of	O
the	O
second	O
and	O
third	O
terms	O
follows	O
from	O
10.1	O
,	O
since	O
it	O
states	O
that	O
for	O
any	O
e	O
>	O
0	O
,	O
there	O
exist.b	O
>	O
°	O
and	O
no	O
such	O
that	O
for	O
the	O
the	O
conditions	O
on	O
hn	O
.	O
the	O
convergence	O
of	O
the	O
first	O
term	O
follows	O
from	O
theorem	B
error	O
probability	O
ln	O
,	O
k	O
of	O
the	O
kernel	B
rule	I
with	O
smoothing	O
parameter	O
h	O
=	O
(	O
l	O
+t	O
)	O
k	O
,	O
p	O
{	O
l	O
n	O
,	O
k	O
-	O
l	O
*	O
>	O
e	O
}	O
<	O
4e-cne2	O
_	O
25.1	O
consistency	B
425	O
for	O
some	O
constant	O
c	O
depending	O
on	O
the	O
dimension	B
only	O
,	O
provided	O
that	O
n	O
>	O
no	O
,	O
h	O
<	O
1/	O
fj	O
and	O
nh	O
d	O
>	O
fj·	O
now	O
clearly	O
,	O
p	O
{	O
ln	O
-	O
l*	O
>	O
e	O
,	O
l/hn	O
>	O
fj	O
,	O
nh	O
:	O
>	O
fj	O
}	O
<	O
p	O
{	O
sup	O
k	O
:	O
(	O
l	O
+8n	O
)	O
k	O
>	O
jj	O
,	O
n/	O
(	O
l	O
+8	O
,	O
z	O
)	O
kd	O
>	O
jj	O
ln	O
,	O
k	O
-	O
l	O
*	O
>	O
e	O
}	O
<	O
cn	O
sup	O
p	O
{	O
ln	O
,	O
k	O
-	O
l*	O
>	O
e	O
}	O
,	O
k	O
:	O
(	O
l	O
+8	O
)	O
k	O
>	O
{	O
j	O
,	O
n/	O
(	O
l	O
+8	O
)	O
kd	O
>	O
{	O
j	O
by	O
the	O
union	O
bound	O
,	O
where	O
cn	O
is	O
the	O
number	O
of	O
possible	O
values	O
of	O
hn	O
in	O
the	O
given	O
range	O
.	O
as	O
we	O
note	O
that	O
cn	O
<	O
2	O
+	O
-	O
~	O
log	O
(	O
~	O
)	O
-	O
log	O
fj	O
log	O
(	O
l	O
+	O
on	O
)	O
=	O
0	O
(	O
logn/8	O
2	O
)	O
=	O
eo	O
(	O
n	O
)	O
n	O
by	O
the	O
condition	O
on	O
the	O
sequence	O
{	O
on	O
}	O
.	O
combining	O
this	O
with	O
theorem	O
10.1	O
,	O
for	O
n	O
>	O
no	O
,	O
we	O
get	O
p	O
{	O
ln	O
-	O
l	O
*	O
>	O
e	O
,	O
1/	O
hn	O
>	O
fj	O
,	O
n	O
h~	O
>	O
fj	O
}	O
:	O
:	O
:	O
;	O
4cn	O
e	O
-cne	O
2	O
,	O
which	O
is	O
summable	O
in	O
n.	O
the	O
borel-cantelli	O
lemma	O
implies	O
that	O
with	O
probability	O
one	O
,	O
and	O
the	O
theorem	B
is	O
proved	O
.	O
0	O
for	O
weak	B
consistency	O
,	O
it	O
suffices	O
to	O
require	O
convergence	O
of	O
hi1	O
and	O
nh	O
:	O
in	O
probability	O
(	O
problem	O
25.1	O
)	O
:	O
theorem	B
25.2.	O
assume	O
that	O
the	O
random	O
variable	B
hn	O
takes	O
its	O
values	O
from	O
the	O
set	O
of	O
real	O
numbers	O
of	O
the	O
form	O
(	O
l+~n	O
)	O
k	O
'	O
where	O
k	O
is	O
a	O
nonnegative	O
integer	O
and	O
on	O
>	O
o.	O
let	O
k	O
be	O
a	O
regular	O
kernel	B
.	O
if	O
and	O
hi1	O
--	O
--	O
+	O
0	O
and	O
nhl~	O
--	O
--	O
+	O
00	O
in	O
probability	O
as	O
n	O
--	O
--	O
+	O
00	O
,	O
then	O
the	O
kernel	B
classification	O
rule	B
corresponding	O
to	O
the	O
random	O
smoothing	O
param	O
(	O
cid:173	O
)	O
eter	O
hn	O
is	O
universally	O
consistent	O
,	O
that	O
is	O
,	O
l	O
(	O
gn	O
)	O
--	O
--	O
+	O
l	O
*	O
in	O
probability	O
.	O
we	O
are	O
now	O
prepared	O
to	O
prove	O
a	O
result	O
similar	O
to	O
theorem	B
25.1	O
without	O
restricting	O
the	O
possible	O
values	O
of	O
the	O
random	O
smoothing	O
parameter	O
hn	O
.	O
for	O
technical	O
reasons	O
,	O
426	O
25.	O
automatic	B
kernel	O
rules	O
we	O
need	O
to	O
assume	O
some	O
additional	O
regularity	O
conditions	O
on	O
the	O
kernel	B
function	O
:	O
k	O
must	O
be	O
decreasing	O
along	O
rays	O
starting	O
from	O
the	O
origin	O
,	O
.	O
but	O
it	O
should	O
not	O
decrease	O
too	O
rapidly	O
.	O
rapidly	O
decreasing	O
functions	O
such	O
as	O
the	O
gaussian	B
kernel	O
,	O
or	O
functions	O
of	O
bounded	O
support	B
,	O
such	O
as	O
the	O
window	B
kernel	O
are	O
excluded	O
.	O
theorem	B
25.3.	O
let	O
k	O
be	O
a	O
regular	O
kernel	B
that	O
is	O
monotone	O
decreasing	O
along	O
rays	O
,	O
that	O
is	O
,	O
for	O
any	O
x	O
e	O
nd	O
and	O
a	O
>	O
1	O
,	O
k	O
(	O
ax	O
)	O
:	O
:	O
:	O
:	O
k	O
(	O
x	O
)	O
.	O
assume	O
in	O
addition	O
that	O
there	O
exists	O
a	O
constant	O
c	O
>	O
0	O
such	O
that	O
for	O
every	O
sufficiently	O
small	O
0	O
>	O
0	O
,	O
and	O
x	O
e	O
rd	O
,	O
k	O
(	O
(	O
l	O
+o	O
)	O
x	O
)	O
2	O
:	O
(	O
l-co	O
)	O
k	O
(	O
x	O
)	O
.	O
let	O
{	O
hn	O
}	O
be	O
a	O
sequence	O
of	O
random	O
variables	O
satisfying	O
hn	O
-7	O
0	O
and	O
nh	O
:	O
-7	O
00	O
with	O
probability	O
one	O
,	O
as	O
n	O
-7	O
00.	O
then	O
the	O
error	O
probability	O
l	O
(	O
gn	O
)	O
of	O
the	O
kernel	B
classification	O
rule	B
with	O
kernel	B
k	O
and	O
smoothing	O
parameter	O
hn	O
converges	O
to	O
l	O
*	O
with	O
probability	O
one	O
,	O
that	O
is	O
,	O
the	O
rule	B
is	O
strongly	O
universally	O
consistent	O
.	O
remark	O
.	O
the	O
technical	O
condition	O
on	O
k	O
is	O
needed	O
to	O
ensure	O
that	O
small	O
changes	O
in	O
h	O
do	O
not	O
cause	O
dramatic	O
changes	O
in	O
l	O
(	O
gn	O
)	O
.	O
we	O
expect	O
some	O
smooth	O
behavior	O
of	O
l	O
(	O
gn	O
)	O
as	O
a	O
function	O
of	O
h.	O
the	O
conditions	O
are	O
rather	O
restrictive	O
,	O
as	O
the	O
kernels	O
must	O
have	O
infinite	O
support	B
and	O
decrease	O
slower	O
than	O
at	O
a	O
polynomial	O
rate	O
.	O
an	O
example	O
satisfying	O
the	O
conditions	O
is	O
k	O
(	O
x	O
)	O
=	O
{	O
;	O
/iixii	O
'	O
if	O
iixll	O
:	O
:	O
:	O
;	O
1	O
otherwise	O
,	O
where	O
r	O
>	O
0	O
(	O
see	O
problem	O
25.2	O
)	O
.	O
the	O
conditions	O
on	O
hn	O
are	O
by	O
no	O
means	O
necessary	O
.	O
we	O
have	O
already	O
seen	O
that	O
consistency	B
occurs	O
for	O
atomic	O
distributions	O
if	O
k	O
(	O
0	O
)	O
>	O
0	O
and	O
hn	O
==	O
0	O
,	O
or	O
for	O
distributions	O
with	O
l	O
*	O
=	O
1/2	O
when	O
hn	O
takes	O
any	O
value	O
.	O
however	O
,	O
theorem	B
25.3	O
provides	O
us	O
with	O
a	O
simple	O
collection	O
of	O
sufficient	O
conditions	O
.	O
0	O
proof	O
of	O
theorem	O
25.3.	O
first	O
we	O
discretize	O
hn	O
.	O
define	O
a	O
sequence	O
on	O
-+	O
0	O
satisfying	O
the	O
condition	O
in	O
theorem	O
25.1	O
,	O
and	O
introduce	O
the	O
random	O
variables	O
h	O
n	O
and	O
hn	O
as	O
follows	O
:	O
h	O
n	O
=	O
(	O
l+8~	O
,	O
)	O
kll	O
'	O
where	O
kn	O
is	O
the	O
smallest	O
integer	O
such	O
that	O
hn	O
>	O
(	O
l+ol1	O
)	O
ki1	O
'	O
and	O
let	O
h	O
n	O
=	O
(	O
l	O
+	O
on	O
)	O
h	O
n	O
'	O
thus	O
,	O
h	O
n	O
<	O
hn	O
:	O
:	O
:	O
;	O
h	O
n.	O
note	O
that	O
bothh.n	O
and	O
h	O
n	O
satisfy	O
the	O
conditions	O
of	O
theorem	O
25.1.	O
as	O
usual	O
,	O
the	O
consistency	B
proof	O
is	O
based	O
on	O
theorem	O
2.3.	O
here	O
,	O
however	O
,	O
we	O
need	O
a	O
somewhat	O
tricky	O
choice	O
of	O
the	O
denominator	O
of	O
the	O
functions	O
that	O
approximate	O
1j	O
(	O
x	O
)	O
.	O
introduce	O
1	O
-	O
-	O
1717	O
,	O
hll	O
(	O
x	O
)	O
=	O
1	O
,	O
,17	O
~	O
l..	O
,	O
i=l	O
j	O
k	O
k	O
(	O
x-xi	O
)	O
i	O
(	O
yi=l	O
}	O
~	O
(	O
)	O
h	O
,	O
~	O
~	O
(	O
dz	O
)	O
clearly	O
,	O
the	O
value	O
of	O
the	O
classification	O
rule	B
gn	O
(	O
x	O
)	O
equals	O
one	O
if	O
and	O
only	O
if17n	O
,	O
hl1	O
(	O
xj	O
is	O
greater	O
than	O
the	O
function	O
defined	O
similarly	O
,	O
with	O
the	O
i	O
{	O
yi=ll	O
's	O
replaced	O
with	O
i	O
{	O
yi=oi	O
>	O
then	O
by	O
theorem	B
2.3	O
it	O
suffices	O
to	O
show	O
that	O
f	O
rj7	O
;	O
t	O
,	O
h	O
,	O
jx	O
)	O
-	O
1j	O
(	O
x	O
)	O
i~	O
(	O
dx	O
)	O
-7	O
0	O
25.1	O
consistency	B
427.	O
with	O
probability	O
one	O
.	O
we	O
use	O
the	O
following	O
decomposition	O
:	O
f	O
l17n	O
,	O
hn	O
(	O
x	O
)	O
-	O
1	O
]	O
(	O
x	O
)	O
ijl	O
(	O
dx	O
)	O
:	O
:	O
:	O
f	O
l1	O
]	O
;	O
l	O
,	O
h	O
''	O
(	O
x	O
)	O
-ifn,81l	O
(	O
x	O
)	O
lfl	O
(	O
dx	O
)	O
+	O
f	O
l17n,8n	O
(	O
x	O
)	O
-	O
1	O
]	O
(	O
x	O
)	O
lfl	O
(	O
dx	O
)	O
.	O
(	O
25.1	O
)	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
converges	O
to	O
zero	O
with	O
probability	O
one	O
,	O
which	O
can	O
be	O
seen	O
by	O
repeating	O
the	O
argument	O
of	O
the	O
proof	O
of	O
theorem	O
25.1	O
,	O
using	O
the	O
observation	O
that	O
in	O
the	O
proof	O
of	O
theorem	O
10.1	O
we	O
proved	O
consistency	B
via	O
an	O
exponential	B
probability	O
inequality	B
for	O
f	O
l1	O
]	O
n	O
,	O
h	O
ll	O
(	O
x	O
)	O
-1	O
]	O
(	O
x	O
)	O
lfl	O
(	O
dx	O
)	O
.	O
the	O
first	O
term	O
may	O
be	O
bounded	O
as	O
the	O
following	O
simple	O
chain	O
of	O
inequalities	O
indi	O
(	O
cid:173	O
)	O
cates	O
:	O
(	O
from	O
h	O
n	O
=	O
(	O
1	O
+	O
8n	O
)	O
h	O
n	O
'	O
and	O
the	O
condition	O
on	O
k	O
,	O
if	O
n	O
is	O
large	O
enough	O
)	O
c8n	O
f	O
(	O
)	O
``	O
j1	O
(	O
dx	O
)	O
,	O
~	O
l:7=1	O
k	O
(	O
xj/i	O
)	O
j	O
k	O
~z	O
fl	O
(	O
dz	O
)	O
h	O
''	O
=	O
since	O
h	O
n	O
satisfies	O
the	O
conditions	O
of	O
theorem	O
25.1	O
,	O
the	O
integral	O
on	O
the	O
right-hand	O
side	O
converges	O
to	O
one	O
with	O
probability	O
one	O
,	O
just	O
as	O
we	O
argued	O
for	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
25.1	O
)	O
.	O
but	O
8n	O
converges	O
to	O
zero	O
.	O
therefore	O
,	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
25.1	O
)	O
tends	O
to	O
zero	O
with	O
probability	O
one	O
.	O
0	O
remark	O
.	O
a	O
quick	O
inspection	O
of	O
the	O
proof	O
above	O
shows	O
that	O
if	O
{	O
an	O
}	O
and	O
{	O
bn	O
}	O
are	O
deterministic	O
sequences	O
with	O
the	O
property	O
that	O
an	O
<	O
bn	O
,	O
bn	O
-+	O
0	O
,	O
and	O
na~	O
-+	O
00	O
,	O
428	O
25.	O
automatic	B
kernel	O
rules	O
then	O
for	O
the	O
kernel	B
estimate	O
with	O
kernel	O
as	O
in	O
theorem	O
25.3	O
,	O
we	O
have	O
sup	O
ln	O
(	O
h	O
)	O
~	O
l	O
*	O
with	O
probability	O
one	O
allshsbll	O
for	O
all	O
distributions	O
.	O
one	O
would	O
never	O
use	O
the	O
worst	O
smoothing	B
factor	I
over	O
the	O
range	O
[	O
an	O
,	O
bn	O
]	O
,	O
but	O
this	O
corollary	O
points	O
out	O
just	O
how	O
powerful	O
theorem	B
25.3	O
is	O
.	O
0	O
25.2	O
data	O
splitting	O
our	O
first	O
example	O
of	O
a	O
data-dependent	O
hn	O
is	O
based	O
upon	O
the	O
minimization	O
of	O
a	O
suitable	O
error	O
estimate	O
.	O
you	O
should	O
have	O
read	O
chapter	O
22	O
on	O
data	O
splitting	O
if	O
you	O
want	O
to	O
understand	O
the	O
remainder	O
of	O
this	O
section	O
.	O
the	O
data	O
sequence	O
dn	O
=	O
(	O
xl	O
,	O
y1	O
)	O
,	O
•••	O
,	O
(	O
xn	O
'	O
yn	O
)	O
is	O
divided	O
into	O
two	O
parts	O
.	O
the	O
first	O
part	O
dm	O
=	O
(	O
xl	O
,	O
yl	O
)	O
,	O
...	O
,	O
(	O
x	O
m	O
,	O
ynj	O
is	O
used	O
for	O
training	O
,	O
while	O
the	O
remaining	O
l	O
=	O
n	O
-	O
m	O
pairs	O
constitute	O
the	O
testing	B
sequence	I
:	O
tz	O
=	O
(	O
xm+l	O
,	O
~n+d	O
,	O
...	O
,	O
(	O
xm+z	O
,	O
ym+z	O
)	O
.	O
the	O
training	O
sequence	O
dm	O
is	O
used	O
to	O
design	O
a	O
class	O
of	O
classifiers	O
em	O
,	O
which	O
,	O
in	O
our	O
case	O
is	O
the	O
class	O
of	O
kernel	B
rules	I
based	O
on	O
dm	O
,	O
with	O
all	O
possible	O
values	O
of	O
h	O
>	O
0	O
,	O
for	O
fixed	O
kernel	B
k.	O
note	O
that	O
the	O
value	O
of	O
the	O
kernel	B
rule	I
gm	O
(	O
x	O
)	O
with	O
smoothing	O
parameter	O
h	O
is	O
zero	O
if	O
and	O
only	O
if	O
(	O
x	O
-	O
x	O
.	O
)	O
h	O
jmex	O
)	O
=	O
l	O
(	O
yi	O
-	O
1/2	O
)	O
k	O
__	O
i	O
so	O
.	O
m	O
i=l	O
classifiers	O
in	O
em	O
are	O
denoted	O
by	O
1m	O
.	O
a	O
classifier	O
is	O
selected	O
from	O
em	O
that	O
minimizes	O
the	O
holdout	B
estimate	O
of	O
the	O
error	O
probability	O
:	O
____	O
1	O
lm	O
,	O
l	O
(	O
¢m	O
)	O
=	O
7	O
l	O
i	O
{	O
rpm	O
(	O
xm+i	O
)	O
=	O
!	O
ym+d	O
'	O
l	O
i=l	O
the	O
particular	O
rule	B
selected	O
in	O
this	O
manner	O
is	O
called	O
gil	O
'	O
the	O
question	O
is	O
how	O
far	O
the	O
error	O
probability	O
l	O
(	O
gn	O
)	O
of	O
the	O
obtained	O
rule	B
is	O
from	O
that	O
of	O
the	O
optimal	O
rule	B
in	O
cm	O
.	O
finite	O
collections	O
.	O
it	O
is	O
computationally	O
attractive	O
to	O
restrict	O
the	O
possible	O
values	O
of	O
h	O
to	O
a	O
finite	O
set	O
of	O
real	O
numbers	O
.	O
for	O
example	O
,	O
em	O
could	O
consist	O
of	O
all	O
kernel	B
rules	I
with	O
h	O
e	O
{	O
2-km	O
,	O
2-km+l	O
,	O
...	O
,	O
1/2	O
,	O
1,2	O
,	O
...	O
,	O
2km	O
}	O
,	O
for	O
some	O
positive	O
integer	O
km	O
'	O
the	O
advantage	O
of	O
this	O
choice	O
of	O
cm	O
is	O
that	O
the	O
best	O
h	O
in	O
this	O
class	O
is	O
within	O
a	O
factor	O
of	O
two	O
of	O
the	O
best	O
h	O
among	O
all	O
possible	O
real	O
smoothing	O
factors	O
,	O
unless	O
the	O
best	O
h	O
is	O
smaller	O
than	O
2-kl11	O
-	O
1	O
or	O
larger	O
than	O
2km+l	O
.	O
clearly	O
,	O
lem	O
i	O
=	O
2km	O
+	O
1	O
,	O
and	O
as	O
pointed	O
out	O
in	O
chapter	O
22	O
,	O
for	O
the	O
selected	O
rule	B
gn	O
hoeffding	O
's	O
inequality	B
and	O
the	O
union	O
bound	O
imply	O
that	O
p	O
{	O
l	O
(	O
gn	O
)	O
-	O
inf	O
l	O
(	O
¢m	O
)	O
>	O
ej	O
dm	O
}	O
:	O
s	O
(	O
4km	O
+	O
2	O
)	O
e-	O
ze2	O
rpmecm	O
2	O
/	O
.	O
if	O
km	O
=	O
eo	O
(	O
l	O
)	O
,	O
then	O
the	O
upper	O
bound	O
decreases	O
exponentially	O
in	O
i	O
,	O
and	O
,	O
in	O
fact	O
,	O
25.2	O
data	O
splitting	O
429	O
{	O
e	O
l	O
(	O
gn	O
)	O
-	O
inf	O
l	O
(	O
cpm	O
)	O
=	O
0	O
-	O
-	O
.	O
¢mecm	O
}	O
(	O
fiflog	O
(	O
i	O
»	O
)	O
i	O
by	O
theorem	B
10.1	O
,	O
em	O
contains	O
a	O
subsequence	O
of	O
consistent	O
rules	O
if	O
m	O
--	O
--	O
+	O
00	O
,	O
and	O
km	O
--	O
--	O
+	O
00	O
as	O
n	O
--	O
--	O
+	O
00.	O
to	O
make	O
sure	O
that	O
gn	O
is	O
strongly	O
universally	O
consistent	O
as	O
well	O
,	O
we	O
only	O
need	O
that	O
limn	O
--	O
+oo	O
i	O
=	O
00	O
,	O
and	O
km	O
=	O
eo	O
(	O
l	O
)	O
(	O
see	O
theorem	B
22.1	O
)	O
.	O
under	O
these	O
conditions	O
,	O
the	O
rule	B
is	O
-/log	O
(	O
km	O
)	O
/	O
i-optimal	O
(	O
see	O
chapter	O
22	O
)	O
.	O
the	O
discussion	O
above	O
does	O
little	O
to	O
help	O
us	O
with	O
the	O
selection	B
of	O
m	O
,	O
i	O
,	O
and	O
km	O
.	O
safe	O
,	O
but	O
possibly	O
suboptimal	O
,	O
choices	O
might	O
be	O
i	O
=	O
n	O
/10	O
,	O
m	O
=	O
n	O
-i	O
,	O
km	O
=	O
210g2	O
n.	O
note	O
that	O
the	O
argument	O
above	O
is	O
valid	O
for	O
any	O
regular	B
kernel	O
k.	O
infinite	O
collections	O
.	O
if	O
we	O
do	O
not	O
want	O
to	O
exclude	O
any	O
value	O
of	O
the	O
smoothing	O
parameter	O
,	O
and	O
pick	O
h	O
from	O
[	O
0	O
,	O
00	O
)	O
,	O
then	O
em	O
is	O
of	O
infinite	O
cardinality	O
.	O
here	O
,	O
we	O
need	O
something	O
stronger	O
,	O
like	O
the	O
vapnik	O
-chervonenkis	O
theory	O
.	O
for	O
example	O
,	O
from	O
chapter	O
22	O
,	O
we	O
have	O
where	O
s	O
(	O
em	O
,	O
i	O
)	O
is	O
the	O
l-th	O
shatter	B
coefficient	I
corresponding	O
to	O
the	O
class	O
of	O
classifiers	O
em	O
.	O
we	O
now	O
obtain	O
upper	O
bounds	O
for	O
seem	O
,	O
i	O
)	O
for	O
different	O
choices	O
of	O
k.	O
define	O
the	O
function	O
recall	O
that	O
for	O
the	O
kernel	B
rule	I
based	O
on	O
d	O
m	O
,	O
(	O
x	O
)	O
=	O
{	O
o	O
if	O
fm	O
(	O
x	O
,	O
dm	O
)	O
:	O
:	O
:	O
:	O
°	O
1	O
otherwise	O
.	O
gm	O
we	O
introduce	O
the	O
kernel	B
complexity	I
km	O
:	O
km	O
sup	O
{	O
number	O
of	O
sign	O
changes	O
of	O
j	O
;	O
n	O
(	O
x	O
,	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xm	O
'	O
ym	O
»	O
as	O
h	O
varies	O
from	O
°	O
to	O
infinity	O
}	O
.	O
suppose	O
we	O
have	O
a	O
kernel	O
with	O
kernel	O
complexity	O
km	O
.	O
then	O
,	O
as	O
h	O
varies	O
from	O
°	O
to	O
infinity	O
,	O
the	O
binary	B
i-vector	O
changes	O
at	O
most	O
ikm	O
times	O
.	O
it	O
can	O
thus	O
take	O
at	O
most	O
ikm	O
+	O
1	O
different	O
values	O
.	O
there	O
(	O
cid:173	O
)	O
fore	O
,	O
430	O
25.	O
automatic	B
kernel	O
rules	O
we	O
postpone	O
the	O
issue	O
of	O
computing	O
kernel	B
complexities	O
until	O
the	O
next	O
section	O
.	O
it	O
suffices	O
to	O
note	O
that	O
if	O
gn	O
is	O
obtained	O
by	O
minimizing	O
the	O
holdout	B
error	O
estimate	B
lm	O
,	O
l	O
(	O
¢m	O
)	O
by	O
varying	O
h	O
,	O
then	O
<	O
<	O
16	O
log	O
(	O
8es	O
(	O
cm	O
,	O
i	O
)	O
)	O
2l	O
16	O
log	O
(	O
8e	O
(	O
ikm	O
+	O
1	O
)	O
)	O
21	O
(	O
corollary	O
12.1	O
)	O
(	O
25.2	O
)	O
(	O
25.3	O
)	O
various	O
probability	O
bounds	O
may	O
also	O
be	O
derived	O
from	O
the	O
results	O
of	O
chapter	O
12.	O
for	O
example	O
,	O
we	O
have	O
<	O
4e	O
8	O
(	O
12km	O
+	O
l	O
)	O
e-le2	O
/	O
2	O
.	O
(	O
25.4	O
)	O
theorem	B
25.4.	O
assume	O
that	O
gn	O
minimizes	O
the	O
holdout	B
estimate	O
lm	O
,	O
l	O
(	O
¢m	O
)	O
over	O
all	O
kernel	B
rules	I
withfixed	O
kernel	B
k	O
of	O
kernel	O
complexity	O
km	O
,	O
and	O
overall	O
(	O
unrestricted	O
)	O
smoothing	O
factors	O
h	O
>	O
o.	O
then	O
gn	O
is	O
strongly	O
universally	O
consistent	O
if	O
(	O
0	O
(	O
u	O
)	O
(	O
iii	O
)	O
(	O
iv	O
)	O
limn-+oo	O
m	O
=	O
00	O
;	O
lim	O
log	O
km	O
=	O
0	O
;	O
n-+oo	O
.	O
hm	O
-	O
-	O
=00	O
;	O
n-+oo	O
log	O
n	O
k	O
is	O
a	O
regular	O
kernel	B
.	O
1	O
i	O
for	O
weak	B
universal	O
consistency	B
,	O
(	O
iii	O
)	O
may	O
be	O
replaced	O
by	O
(	O
v	O
)	O
:	O
limn-+oo	O
i	O
=	O
00.	O
proof	O
.	O
note	O
that	O
cm	O
contains	O
a	O
strongly	O
universally	O
consistent	O
subsequence	O
(	O
cid:173	O
)	O
take	O
h	O
=	O
m-	O
1/	O
(	O
2d	O
)	O
for	O
example	O
,	O
and	O
apply	O
theorem	B
10.1	O
,	O
noting	O
that	O
h	O
--	O
-+	O
0	O
,	O
yet	O
mhd	O
--	O
-+	O
00.	O
thus	O
,	O
lim	O
n-+oo	O
¢mecm	O
inf	O
l	O
(	O
¢m	O
)	O
=	O
l	O
*	O
with	O
probability	O
one	O
.	O
it	O
suffices	O
to	O
apply	O
theorem	B
22.1	O
and	O
to	O
note	O
that	O
the	O
bound	O
in	O
(	O
25.4	O
)	O
is	O
summable	O
in	O
n	O
when	O
lilog	O
n	O
--	O
-+	O
00	O
and	O
log	O
km	O
=	O
o	O
(	O
l	O
)	O
.	O
for	O
weak	B
universal	O
consistency	B
,	O
a	O
simple	O
application	O
of	O
(	O
25.2	O
)	O
suffices	O
to	O
note	O
that	O
we	O
only	O
need	O
1	O
--	O
-+	O
00	O
instead	O
of	O
lilogn	O
--	O
-+	O
00.0	O
approximation	O
errors	O
decrease	O
with	O
m.	O
for	O
example	O
,	O
if	O
class	O
densities	O
exist	O
,	O
we	O
may	O
combine	O
the	O
inequality	B
of	O
problem	O
2.10	O
with	O
bounds	O
from	O
devroye	O
and	O
gyorfi	O
(	O
1985	O
)	O
and	O
holmstrom	O
and	O
klemehi	O
(	O
1992	O
)	O
to	O
conclude	O
thate	O
{	O
inf¢mecm	O
l	O
(	O
¢m	O
)	O
}	O
(	O
cid:173	O
)	O
l	O
*	O
is	O
of	O
the	O
order	O
of	O
m-2a/	O
(	O
4+d	O
)	O
,	O
with	O
ex	O
e	O
[	O
1,2	O
]	O
,	O
under	O
suitable	O
conditions	O
on	O
k	O
25.3	O
kernel	B
complexity	I
431	O
and	O
the	O
densities	O
.	O
by	O
(	O
25.2	O
)	O
,	O
the	O
estimation	B
error	I
is	O
0	O
(	O
jlog	O
(	O
ikm	O
)	O
/	O
i	O
)	O
,	O
requiring	O
instead	O
large	O
values	O
for	O
i.	O
clearly	O
,	O
some	O
sort	O
of	O
balance	O
is	O
called	O
for	O
.	O
ignoring	O
the	O
logarithmic	O
term	O
for	O
now	O
,	O
we	O
see	O
that	O
i	O
should	O
be	O
roughly	O
m	O
4a	O
/	O
(	O
4+d	O
)	O
if	O
we	O
are	O
to	O
balance	O
errors	O
of	O
both	O
kinds	O
.	O
unfortunately	O
,	O
all	O
of	O
this	O
is	O
ad	O
hoc	O
and	O
based	O
upon	O
unverifiable	O
distributional	O
conditions	O
.	O
ideally	O
,	O
one	O
should	O
let	O
the	O
data	O
select	O
i	O
and	O
m.	O
see	O
problem	O
25.3	O
,	O
and	O
problem	O
22.4	O
on	O
optimal	O
data	O
splitting	O
.	O
for	O
some	O
distributions	O
,	O
the	O
estimation	B
error	I
is	O
just	O
too	O
large	O
to	O
obtain	O
asymp	O
(	O
cid:173	O
)	O
totic	O
optimality	O
as	O
defined	O
in	O
chapter	O
22.	O
for	O
example	O
,	O
the	O
best	O
bound	O
on	O
the	O
estimation	B
error	I
is	O
0	O
(	O
jlog	O
n	O
/	O
n	O
)	O
,	O
attained	O
when	O
km	O
=	O
i	O
,	O
i	O
=	O
n.	O
if	O
the	O
distribu	O
(	O
cid:173	O
)	O
tion	O
of	O
x	O
is	O
atomic	O
with	O
finitely	O
many	O
atoms	O
,	O
then	O
the	O
expected	O
approximation	B
error	I
is	O
0	O
(	O
1/	O
jiii	O
)	O
.	O
hence	O
the	O
error	O
introduced	O
by	O
the	O
selection	B
process	O
smothers	O
the	O
approximation	B
error	I
when	O
m	O
is	O
linear	O
in	O
n.	O
similar	O
conclusions	O
may	O
even	O
be	O
with	O
1j	O
(	O
x	O
)	O
=	O
1	O
if	O
x	O
e	O
[	O
0,1	O
]	O
and	O
1j	O
(	O
x	O
)	O
=	O
°	O
if	O
x	O
e	O
[	O
3,4	O
]	O
.	O
for	O
the	O
kernel	B
rule	I
with	O
drawn	O
when	O
x	O
has	O
a	O
density	O
:	O
consider	O
the	O
uniform	B
distribution	O
on	O
[	O
0	O
,	O
1	O
]	O
u	O
[	O
3,4	O
]	O
h	O
=	O
1	O
,	O
k	O
=	O
1	O
[	O
-1,1	O
]	O
,	O
the	O
expected	O
approximation	B
error	I
tends	O
to	O
l	O
*	O
=	O
°	O
exponentially	O
quickly	O
in	O
m	O
,	O
and	O
this	O
is	O
always	O
far	O
better	O
than	O
the	O
estimation	B
error	I
which	O
at	O
best	O
is	O
0	O
(	O
jlog	O
n	O
/	O
n	O
)	O
.	O
25.3	O
kernel	B
complexity	I
we	O
now	O
tum	O
to	O
the	O
kernel	B
complexity	I
km	O
.	O
the	O
following	O
lemmas	O
are	O
useful	O
in	O
our	O
computations	O
.	O
if	O
l/	O
log	O
n	O
--	O
-+	O
00	O
,	O
we	O
note	O
that	O
for	O
strong	B
consistency	I
it	O
suffices	O
that	O
km	O
=	O
0	O
(	O
my	O
)	O
for	O
some	O
finite	O
y	O
gust	O
verify	O
the	O
proof	O
again	O
)	O
.	O
this	O
,	O
as	O
it	O
turns	O
out	O
,	O
is	O
satisfied	O
for	O
nearly	O
all	O
practical	O
kernels	O
.	O
lemma	O
25.1.	O
let	O
°	O
:	O
s	O
b	O
1	O
<	O
...	O
<	O
bm	O
be	O
fixed	O
numbers	O
,	O
and	O
let	O
ai	O
e	O
r	O
,	O
1	O
:	O
s	O
i	O
:	O
s	O
m	O
,	O
befixed	O
,	O
with	O
the	O
restriction	O
that	O
am	O
i	O
0.	O
then	O
the	O
function	O
f	O
(	O
x	O
)	O
=	O
l~:1	O
aixh	O
;	O
has	O
at	O
most	O
m	O
-	O
1	O
nonzero	O
positive	O
roots	O
.	O
proof	O
.	O
note	O
first	O
that	O
f	O
can	O
not	O
be	O
identically	O
zero	O
on	O
any	O
interval	O
of	O
nonzero	O
length	O
.	O
let	O
z	O
(	O
g	O
)	O
denote	O
the	O
number	O
of	O
nonzero	O
positive	O
roots	O
of	O
a	O
function	O
g.	O
we	O
have	O
z	O
(	O
taixhi	O
)	O
1=1	O
z	O
(	O
t	O
aixci	O
)	O
1=1	O
(	O
where	O
ci	O
=	O
bi	O
-	O
bi	O
,	O
all	O
i	O
;	O
thus	O
,	O
cl	O
=	O
0	O
)	O
432	O
25.	O
automatic	B
kernel	O
rules	O
(	O
for	O
a	O
continuously	O
differentiable	O
g	O
,	O
we	O
have	O
z	O
(	O
g	O
)	O
:	O
:	O
:	O
;	O
1	O
+	O
z	O
(	O
gl	O
)	O
)	O
=	O
z	O
(	O
ta	O
;	O
xb	O
;	O
)	O
+	O
1	O
l=2	O
note	O
that	O
the	O
b	O
;	O
are	O
increasing	O
,	O
b~	O
=	O
0	O
,	O
and	O
a	O
;	O
=i	O
0.	O
as	O
z	O
(	O
amx	O
bm	O
we	O
derive	O
our	O
claim	O
by	O
simple	O
induction	O
on	O
m.	O
0	O
)	O
=	O
°	O
for	O
am	O
-	O
:	O
j	O
0	O
,	O
lemma	O
25.2.	O
let	O
a	O
i	O
,	O
...	O
,	O
am	O
be	O
fixed	O
real	O
numbers	O
,	O
and	O
let	O
bi	O
,	O
...	O
,	O
bm	O
be	O
different	O
nonnegative	O
reals	O
.	O
then	O
if	O
ex	O
=i	O
0	O
,	O
the	O
function	O
f	O
(	O
x	O
)	O
=	O
l	O
ai	O
e-	O
bix	O
''	O
,	O
x	O
?	O
:	O
0	O
,	O
m	O
i=1	O
is	O
either	O
identically	O
zero	O
,	O
or	O
takes	O
the	O
value	O
0	O
at	O
most	O
m	O
times	O
.	O
proof	O
.	O
define	O
y	O
=	O
e-xct	O
.	O
if	O
ex	O
=i	O
0	O
,	O
y	O
ranges	O
from	O
0	O
to	O
1.	O
by	O
lemma	O
25.1	O
,	O
g	O
(	O
y	O
)	O
=	O
l	O
;	O
:1	O
ai	O
ybi	O
takes	O
the	O
value	O
0	O
at	O
at	O
most	O
m	O
-1	O
positive	O
y-values	O
,	O
unless	O
it	O
is	O
identically	O
zero	O
everywhere	O
.	O
this	O
concludes	O
the	O
proof	O
of	O
the	O
lemma	O
.	O
0	O
a	O
star-shaped	O
kernel	B
is	O
one	O
of	O
the	O
form	O
k	O
(	O
x	O
)	O
=	O
l	O
{	O
xea	O
}	O
,	O
where	O
a	O
is	O
a	O
star-shaped	O
set	O
of	O
unit	O
lebesgue	O
measure	B
,	O
that	O
is	O
,	O
x	O
f	O
:	O
a	O
implies	O
cx	O
f	O
:	O
a	O
for	O
all	O
c	O
?	O
:	O
1.	O
it	O
is	O
clear	O
that	O
km	O
=	O
m	O
-	O
1	O
by	O
a	O
simple	O
thresholding	O
argument	O
.	O
on	O
the	O
real	O
line	O
,	O
the	O
kernel	B
k	O
(	O
x	O
)	O
=	O
l~-oo	O
aj	O
{	O
xe	O
[	O
2i,2i+lj	O
}	O
for	O
ai	O
>	O
0	O
oscillates	O
infinitely	O
often	O
,	O
and	O
has	O
km	O
=	O
00	O
for	O
all	O
values	O
of	O
m	O
?	O
:	O
2.	O
we	O
must	O
therefore	O
disallow	O
such	O
kernels	O
.	O
for	O
the	O
same	O
reason	O
,	O
kernels	O
such	O
as	O
k	O
(	O
x	O
)	O
=	O
(	O
sin	O
x/x	O
y	O
on	O
the	O
real	O
line	O
are	O
not	O
good	O
(	O
see	O
problem	O
25.4	O
)	O
.	O
if	O
k	O
=	O
l~=l	O
ai	O
hi	O
for	O
some	O
finite	O
k	O
,	O
some	O
numbers	O
ai	O
and	O
some	O
star-shaped	B
sets	O
ai	O
,	O
then	O
km	O
:	O
:	O
:	O
;	O
k	O
(	O
m	O
-	O
1	O
)	O
.	O
consider	O
next	O
kernels	O
of	O
the	O
form	O
where	O
a	O
is	O
star-shaped	B
,	O
and	O
r	O
?	O
:	O
0	O
is	O
a	O
constant	O
(	O
see	O
sebestyen	O
(	O
1962	O
)	O
)	O
.	O
we	O
se~	O
that	O
(	O
x	O
-x	O
.	O
)	O
h	O
1	O
2	O
1	O
m	O
i=1	O
m	O
i=1	O
=	O
l	O
(	O
yi	O
-	O
-	O
)	O
k	O
__	O
i	O
=	O
hr	O
i	O
)	O
yi	O
-	O
2	O
)	O
!	O
!	O
x	O
-	O
xill-	O
r	O
1	O
{	O
(	O
x-xi	O
)	O
/hea	O
}	O
,	O
25.3	O
kernel	B
complexity	I
433	O
which	O
changes	O
sign	O
at	O
most	O
as	O
often	O
as	O
f	O
m	O
(	O
x	O
)	O
/	O
hr	O
.	O
from	O
our	O
earlier	O
remarks	O
,	O
it	O
is	O
easy	O
to	O
see	O
that	O
km	O
=	O
m	O
-	O
1	O
,	O
as	O
km	O
is	O
the	O
same	O
as	O
for	O
the	O
kernel	B
k	O
=	O
i	O
a.	O
if	O
a	O
is	O
replaced	O
by	O
nd	O
,	O
then	O
the	O
kernel	B
is	O
not	O
integrable	O
,	O
but	O
clearly	O
,	O
km	O
=	O
o.	O
assume	O
next	O
that	O
we	O
have	O
k	O
(	O
x	O
)	O
=	O
{	O
;	O
/iixii	O
'	O
if	O
ilx	O
ii	O
:	O
:	O
:	O
1	O
otherwise	O
,	O
where	O
r	O
>	O
o.	O
for	O
r	O
>	O
d	O
,	O
these	O
kernels	O
are	O
integrable	O
and	O
thus	O
regular	B
.	O
note	O
that	O
x	O
)	O
hr	O
k	O
h	O
=	O
i	O
{	O
llxiish	O
}	O
+	O
i	O
{	O
llxll	O
>	O
h	O
}	O
iixll	O
r	O
(	O
'	O
as	O
h	O
increases	O
,	O
f	O
m	O
(	O
x	O
)	O
,	O
which	O
is	O
of	O
the	O
form	O
''	O
~	O
i	O
i	O
:	O
llx-xilish	O
(	O
y	O
.	O
_~	O
)	O
+hf	O
2	O
(	O
y	O
.	O
_~	O
)	O
_1_	O
ilx	O
-	O
xl·llr	O
'	O
2	O
''	O
~	O
i	O
i	O
:	O
llx-xill	O
>	O
h	O
transfers	O
an	O
xi	O
from	O
one	O
sum	O
to	O
the	O
other	O
at	O
most	O
m	O
times	O
.	O
on	O
an	O
interval	O
on	O
which	O
no	O
such	O
transfer	O
occurs	O
,	O
fm	O
varies	O
as	O
a	O
+	O
j3h	O
r	O
and	O
has	O
at	O
most	O
one	O
sign	O
change	O
.	O
therefore	O
,	O
km	O
can	O
not	O
be	O
more	O
than	O
m	O
+	O
1	O
(	O
one	O
for	O
each	O
h-interval	O
)	O
plus	O
m	O
(	O
one	O
for	O
each	O
transfer	O
)	O
,	O
so	O
that	O
km	O
:	O
:	O
:	O
2m	O
+	O
l.	O
for	O
more	O
practice	O
with	O
such	O
computations	O
,	O
we	O
refer	O
to	O
the	O
exercises	O
.	O
we	O
now	O
continue	O
with	O
a	O
few	O
important	O
classes	O
of	O
kernels	O
.	O
consider	O
next	O
exponential	B
kernels	O
such	O
as	O
for	O
some	O
a	O
>	O
0	O
,	O
where	O
ii	O
.	O
ii	O
is	O
any	O
norm	O
on	O
nd	O
.	O
these	O
kernels	O
include	O
the	O
popular	O
gaussian	B
kernels	O
.	O
as	O
the	O
decision	O
rule	O
based	O
on	O
dm	O
is	O
of	O
the	O
form	O
a	O
simple	O
application	O
of	O
lemma	O
25.2	O
shows	O
that	O
km	O
:	O
:	O
:	O
m.	O
the	O
entire	O
class	O
of	O
kernels	O
behaves	O
nicely	O
.	O
among	O
compact	O
support	B
kernels	O
,	O
kernels	O
of	O
the	O
form	O
k	O
(	O
x	O
)	O
=	O
(	O
t	O
ai	O
iixll	O
b	O
,	O
)	O
i	O
{	O
ljxjjsli	O
for	O
real	O
numbers	O
ai	O
,	O
and	O
hi	O
:	O
:	O
:	O
:	O
:	O
1	O
,	O
are	O
important	O
.	O
a	O
particularly	O
popular	O
kernel	B
in	O
d-dimensional	O
density	B
estimation	I
is	O
deheuvels	O
'	O
(	O
1977	O
)	O
kernel	B
if	O
the	O
kernel	B
was	O
k	O
(	O
x	O
)	O
=	O
(	O
i:7=1	O
ai	O
ilx	O
ilhi	O
)	O
,	O
without	O
the	O
indicator	O
function	O
,	O
then	O
lemma	O
25.1	O
would	O
immediately	O
yield	O
the	O
estimate	B
km	O
:	O
:	O
:	O
k	O
,	O
uniformly	O
over	O
all	O
m.	O
such	O
kernels	O
would	O
be	O
particularly	O
interesting	O
.	O
with	O
the	O
indicator	O
function	O
434	O
25.	O
automatic	B
kernel	O
rules	O
multiplied	O
in	O
,	O
we	O
have	O
km	O
:	O
:	O
:	O
:	O
;	O
km	O
,	O
simply	O
because	O
fm	O
(	O
x	O
)	O
at	O
each	O
h	O
is	O
based	O
upon	O
a	O
subset	O
of	O
the	O
xj	O
's	O
,	O
1	O
:	O
:	O
:	O
:	O
;	O
j	O
:	O
:	O
:	O
:	O
;	O
m	O
,	O
with	O
the	O
subset	O
growing	O
monotonically	O
with	O
h.	O
for	O
each	O
subset	O
,	O
the	O
function	O
fm	O
(	O
x	O
)	O
is	O
a	O
polynomial	O
in	O
iix	O
ii	O
with	O
powers	O
bi	O
,	O
...	O
,	O
bk	O
,	O
and	O
changes	O
sign	O
at	O
most	O
k	O
times	O
.	O
therefore	O
,	O
polynomial	B
kernels	O
of	O
compact	O
support	B
also	O
have	O
small	O
complexities	O
.	O
observe	O
that	O
the	O
``	O
k	O
''	O
in	O
the	O
bound	O
km	O
:	O
:	O
:	O
:	O
;	O
km	O
refers	O
to	O
the	O
number	O
of	O
terms	O
in	O
the	O
polynomial	B
and	O
not	O
the	O
maximal	O
power	O
.	O
a	O
large	O
class	O
of	O
kernels	O
of	O
finite	O
complexity	O
may	O
be	O
obtained	O
by	O
applying	O
the	O
rich	O
theory	O
of	O
total	O
positivity	O
.	O
see	O
karlin	O
(	O
1968	O
)	O
for	O
a	O
thorough	O
treatment	O
.	O
a	O
real	O
(	O
cid:173	O
)	O
valued	O
function	O
l	O
of	O
two	O
real	O
variables	O
is	O
said	O
to	O
be	O
totally	O
positive	O
on	O
a	O
x	O
b	O
c	O
'r	O
}	O
if	O
for	O
all	O
n	O
,	O
and	O
all	O
s1	O
<	O
...	O
<	O
sn	O
,	O
si	O
e	O
a	O
,	O
tl	O
<	O
...	O
<	O
tn	O
,	O
ti	O
e	O
b	O
,	O
the	O
determinant	O
of	O
the	O
matrix	O
with	O
elements	O
l	O
(	O
si	O
,	O
tj	O
)	O
is	O
nonnegative	O
.	O
a	O
key	O
property	O
of	O
such	O
functions	O
is	O
the	O
following	O
result	O
,	O
which	O
we	O
cite	O
without	O
proof	O
:	O
theorem	B
25.5	O
.	O
(	O
schoenberg	O
(	O
1950	O
»	O
.	O
let	O
l	O
be	O
a	O
totally	O
positive	O
function	O
on	O
a	O
x	O
b	O
e	O
r	O
2	O
,	O
and	O
let	O
a	O
:	O
b	O
--	O
+	O
r	O
be	O
a	O
bounded	O
function	O
.	O
define	O
the	O
function	O
f3	O
(	O
s	O
)	O
=	O
1.	O
l	O
(	O
s	O
,	O
t	O
)	O
a	O
(	O
t	O
)	O
a	O
(	O
dt	O
)	O
,	O
on	O
a	O
,	O
where	O
a	O
is	O
a	O
finite	O
measure	B
on	O
b.	O
then	O
f3	O
(	O
s	O
)	O
changes	O
sign	O
at	O
most	O
as	O
many	O
times	O
as	O
a	O
(	O
s	O
)	O
does	O
.	O
(	O
the	O
number	O
of	O
sign	O
changes	O
of	O
a	O
function	O
f3	O
is	O
defined	O
as	O
the	O
supremum	O
of	O
sign	O
changes	O
of	O
sequences	O
of	O
the	O
form	O
f3	O
(	O
sl	O
)	O
,	O
...	O
,	O
f3	O
(	O
sn	O
)	O
,	O
where	O
n	O
is	O
arbitrary	O
,	O
and	O
s1	O
<	O
...	O
<	O
sn	O
.	O
)	O
corollary	O
25.1.	O
assume	O
that	O
the	O
kernel	B
k	O
is	O
such	O
that	O
the	O
function	O
l	O
(	O
s	O
,	O
t	O
)	O
=	O
k	O
(	O
st	O
)	O
is	O
totally	O
positive	O
for	O
s	O
>	O
°	O
and	O
t	O
e	O
rd	O
.	O
then	O
the	O
kernel	B
complexity	I
of	O
k	O
satisfies	O
km	O
:	O
:	O
:	O
:	O
;	O
m	O
-	O
1.	O
proof	O
.	O
we	O
apply	O
theorem	B
25.5.	O
we	O
are	O
interested	O
in	O
the	O
number	O
of	O
sign	O
changes	O
of	O
the	O
function	O
m	O
f3	O
(	O
s	O
)	O
=	O
l	O
(	O
2yi	O
-	O
i	O
)	O
k	O
(	O
(	O
xi	O
-	O
x	O
)	O
s	O
)	O
i=l	O
on	O
s	O
e	O
(	O
0	O
,	O
(	O
0	O
)	O
(	O
s	O
plays	O
the	O
role	O
of	O
1/	O
h	O
)	O
.	O
but	O
f3	O
(	O
s	O
)	O
may	O
be	O
written	O
as	O
f3	O
(	O
s	O
)	O
=	O
1.	O
l	O
(	O
s	O
,	O
t	O
)	O
a	O
(	O
t	O
)	O
a	O
(	O
dt	O
)	O
,	O
where	O
l	O
(	O
s	O
,	O
t	O
)	O
=	O
k	O
(	O
st	O
)	O
,	O
the	O
measure	B
a	O
puts	O
mass	O
ion	O
each	O
t	O
=	O
xi	O
-x	O
,	O
i	O
=	O
1	O
,	O
...	O
,	O
m	O
,	O
and	O
act	O
)	O
is	O
defined	O
at	O
these	O
points	O
as	O
a	O
(	O
t	O
)	O
=	O
2	O
yi	O
-	O
1	O
if	O
t	O
=	O
xi	O
-	O
x.	O
other	O
values	O
of	O
act	O
)	O
are	O
irrelevant	O
for	O
the	O
integral	O
above	O
.	O
clearly	O
,	O
act	O
)	O
can	O
be	O
defined	O
such	O
that	O
it	O
changes	O
sign	O
at	O
most	O
m	O
-	O
1	O
times	O
.	O
then	O
theorem	B
25.5	O
implies	O
that	O
f3	O
(	O
s	O
)	O
changes	O
sign	O
at	O
most	O
m	O
-	O
1	O
times	O
,	O
as	O
desired	O
.	O
0	O
this	O
corollary	O
equips	O
us	O
with	O
a	O
whole	O
army	O
of	O
kernels	O
with	O
small	O
complexity	O
.	O
for	O
examples	O
,	O
refer	O
to	O
the	O
monograph	O
of	O
karlin	O
(	O
1968	O
)	O
.	O
25.4	O
multiparameter	B
kernel	O
rules	O
25.4	O
multiparameter	B
kernel	O
rules	O
435	O
assume	O
that	O
in	O
the	O
kernel	B
rules	I
considered	O
in	O
em	O
,	O
we	O
perform	O
an	O
optimization	O
with	O
respect	O
to	O
more	O
than	O
one	O
parameter	O
.	O
collect	O
these	O
parameters	O
in	O
e	O
,	O
and	O
write	O
the	O
discrimination	O
function	O
as	O
fm	O
(	O
x	O
)	O
=	O
t	O
(	O
yi	O
-	O
~	O
)	O
ke	O
(	O
x	O
xi	O
)	O
'	O
i=l	O
2	O
examples	O
:	O
(	O
i	O
)	O
product	B
kernels	O
.	O
take	O
ke	O
(	O
x	O
)	O
=tik	O
(	O
~j	O
)	O
)	O
'	O
h	O
d	O
j=l	O
where	O
e	O
=	O
(	O
h	O
(	O
l	O
)	O
,	O
...	O
,	O
h	O
(	O
d	O
)	O
)	O
is	O
a	O
vector	O
of	O
smoothing	O
factors-one	O
per	O
dimen	O
(	O
cid:173	O
)	O
sion-and	O
k	O
is	O
a	O
fixed	O
one-dimensional	O
kernel	B
.	O
(	O
ii	O
)	O
kernels	O
of	O
variable	O
form	O
.	O
define	O
ke	O
(	O
x	O
)	O
=	O
e-uxll	O
''	O
lh	O
''	O
,	O
where	O
a	O
>	O
0	O
is	O
a	O
shape	O
parameter	O
,	O
and	O
h	O
>	O
0	O
is	O
the	O
standard	B
smoothing	O
parameter	O
.	O
here	O
e	O
=	O
(	O
a	O
,	O
h	O
)	O
is	O
two-dimensional	O
.	O
(	O
iii	O
)	O
define	O
i	O
1	O
ke	O
(	O
x	O
)	O
=	O
1	O
+xt	O
rrtx	O
1	O
+	O
iirxii	O
2	O
'	O
where	O
x	O
t	O
is	O
the	O
transpose	O
of	O
x	O
,	O
and	O
r	O
is	O
an	O
orthogonal	O
transformation	O
matrix	O
,	O
all	O
of	O
whose	O
free	O
components	O
taken	O
together	O
are	O
collected	O
in	O
e.	O
kernels	O
of	O
this	O
kind	O
may	O
be	O
used	O
to	O
adjust	O
automatically	O
to	O
a	O
certain	O
variance-covariance	O
structure	O
in	O
the	O
data	O
.	O
we	O
will	O
not	O
spend	O
a	O
lot	O
of	O
time	O
on	O
these	O
cases	O
.	O
clearly	O
,	O
one	O
route	O
is	O
to	O
properly	O
generalize	O
the	O
definition	B
of	I
kernel	O
complexity	O
.	O
in	O
some	O
cases	O
,	O
it	O
is	O
more	O
convenient	O
to	O
directly	O
find	O
upper	O
bounds	O
for	O
seem	O
,	O
l	O
)	O
.	O
in	O
the	O
product	B
kernel	O
case	O
,	O
with	O
one	O
(	O
cid:173	O
)	O
dimensional	O
kernel	B
1	O
[	O
-1,1	O
]	O
,	O
we	O
claim	O
for	O
example	O
that	O
seem	O
,	O
i	O
)	O
:	O
s	O
(	O
lm	O
)	O
d	O
+	O
1.	O
the	O
corresponding	O
rule	B
takes	O
a	O
majority	O
vote	O
over	O
centered	O
rectangles	O
with	O
sides	O
equal	O
to	O
2h	O
(	O
l	O
)	O
,	O
2h	O
(	O
2	O
)	O
,	O
...	O
,	O
2h	O
(	O
d	O
)	O
.	O
to	O
see	O
why	O
the	O
inequality	B
is	O
true	O
,	O
consider	O
the	O
d	O
(	O
cid:173	O
)	O
dimensional	O
quadrant	O
of	O
1m	O
points	O
obtained	O
by	O
taking	O
the	O
absolute	O
values	O
of	O
the	O
vectors	O
xj	O
-	O
xi	O
,	O
m	O
<	O
j	O
:	O
s	O
m	O
+	O
i	O
=	O
n	O
,	O
1	O
:	O
s	O
i	O
:	O
s	O
m	O
,	O
where	O
the	O
absolute	O
value	O
of	O
a	O
vector	O
is	O
a	O
vector	O
whose	O
components	O
are	O
the	O
absolute	O
values	O
of	O
the	O
components	O
ofthe	O
vector	O
.	O
to	O
compute	O
seem	O
,	O
l	O
)	O
,	O
it	O
suffices	O
to	O
count	O
how	O
many	O
different	O
subsets	O
can	O
be	O
obtained	O
from	O
these	O
lm	O
points	O
by	O
considering	O
all	O
possible	O
rectangles	O
with	O
one	O
vertex	O
at	O
the	O
origin	O
,	O
and	O
the	O
diagonally	O
opposite	O
vertex	O
in	O
the	O
quadrant	O
.	O
this	O
is	O
1	O
+	O
(	O
lm	O
)	O
d.	O
the	O
strong	B
universal	I
consistency	I
of	O
the	O
latter	O
family	O
em	O
is	O
insured	O
when	O
m	O
-+	O
00	O
and	O
i	O
!	O
log	O
n	O
~	O
00	O
.	O
436	O
25.	O
automatic	B
kernel	O
rules	O
25.5	O
kernels	O
of	O
infinite	O
complexity	O
in	O
this	O
section	O
we	O
demonstrate	O
that	O
not	O
every	O
kernel	B
function	O
supports	O
smoothing	B
factor	I
selection	O
based	O
on	O
data	O
splitting	O
.	O
these	O
kernels	O
have	O
infinite	O
kernel	B
com	O
(	O
cid:173	O
)	O
plexity	O
,	O
or	O
even	O
worse	O
,	O
infinite	O
vc	B
dimension	I
.	O
some	O
of	O
the	O
examples	O
may	O
appear	O
rather	O
artificial	O
,	O
but	O
some	O
``	O
nice	O
''	O
kernels	O
will	O
surprise	O
us	O
by	O
misbehaving	O
.	O
we	O
begin	O
with	O
a	O
kernel	O
k	O
having	O
the	O
property	O
that	O
the	O
class	O
of	O
sets	O
has	O
vc	B
dimension	I
va	O
:	O
:	O
:	O
:	O
00	O
for	O
any	O
fixed	O
value	O
of	O
xl	O
(	O
le.	O
,	O
using	O
the	O
notation	O
of	O
the	O
previous	O
sections	O
,	O
vel	O
:	O
:	O
:	O
:	O
(	O
0	O
)	O
.	O
hence	O
,	O
with	O
one	O
sample	O
point	O
,	O
all	O
hope	O
is	O
lost	O
to	O
use	O
the	O
vapnik-chervonenkis	O
inequality	B
in	O
any	O
meaningful	O
way	O
.	O
unfortunately	O
,	O
the	O
kernel	B
k	O
takes	O
alternately	O
positive	O
and	O
negative	O
values	O
.	O
in	O
the	O
second	O
part	O
of	O
this	O
section	O
,	O
a	O
kernel	O
is	O
constructed	O
that	O
is	O
unimodal	O
and	O
symmetric	O
and	O
has	O
vem	O
:	O
:	O
:	O
:	O
00	O
for	O
m	O
:	O
:	O
:	O
:	O
4	O
when	O
d4	O
:	O
:	O
:	O
:	O
«	O
xl	O
,	O
y1	O
)	O
,	O
(	O
x2	O
,	O
y2	O
)	O
,	O
(	O
x3	O
,	O
y3	O
)	O
,	O
(	O
x4	O
,	O
y4	O
»	O
takes	O
certain	O
values	O
.	O
finally	O
,	O
in	O
the	O
last	O
part	O
,	O
we	O
construct	O
a	O
positive	O
kernel	B
with	O
the	O
property	O
that	O
for	O
any	O
m	O
and	O
any	O
nondegenerate	O
non	O
atomic	O
distribution	B
,	O
limm~oo	O
p	O
{	O
vem	O
:	O
:	O
:	O
:	O
oo	O
}	O
:	O
:	O
:	O
:	O
1.	O
we	O
return	O
to	O
a.	O
our	O
function	O
is	O
picked	O
as	O
follows	O
:	O
k	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
a	O
(	O
x	O
)	O
g	O
(	O
i	O
)	O
,	O
2i	O
s	O
x	O
<	O
i+l	O
,	O
i	O
:	O
:	O
:	O
1	O
,	O
where	O
g	O
(	O
i	O
)	O
e	O
{	O
-i	O
,	O
i	O
}	O
for	O
all	O
i	O
,	O
a	O
(	O
x	O
)	O
>	O
°	O
for	O
all	O
x	O
,	O
a	O
(	O
x	O
)	O
,	O
}	O
°	O
as	O
x	O
t	O
00	O
,	O
x	O
>	O
0	O
,	O
and	O
a	O
(	O
x	O
)	O
,	O
}	O
°	O
as	O
x	O
,	O
}	O
-00	O
,	O
x	O
<	O
°	O
(	O
the	O
monotonicity	O
is	O
not	O
essential	O
and	O
may	O
be	O
dropped	O
but	O
the	O
resulting	O
class	O
of	O
kernels	O
will	O
be	O
even	O
less	O
inter	O
(	O
cid:173	O
)	O
esting	O
)	O
.	O
we	O
enumerate	O
all	O
binary	B
strings	O
in	O
lexicographical	O
order	O
,	O
replace	O
all	O
o	O
's	O
by	O
-1	O
's	O
,	O
and	O
map	O
the	O
bit	O
sequence	O
to	O
g	O
(	O
l	O
)	O
,	O
g	O
(	O
2	O
)	O
,	O
...	O
.	O
hence	O
,	O
the	O
bit	O
sequence	O
0,1,00,01,10,11,000,001,010	O
,	O
...	O
becomes	O
-1,1	O
,	O
-1	O
,	O
-1	O
,	O
-1,1,1	O
,	O
-1	O
,	O
1	O
,	O
1	O
,	O
-1	O
,	O
-1	O
,	O
-1	O
,	O
-1	O
,	O
-1	O
,	O
1	O
,	O
-1,1	O
,	O
-1	O
,	O
...	O
.	O
call	O
this	O
sequence	O
s.	O
for	O
every	O
n	O
,	O
we	O
can	O
find	O
a	O
set	O
{	O
x	O
i	O
,	O
...	O
,	O
x	O
n	O
}	O
that	O
can	O
be	O
shattered	O
by	O
sets	O
from	O
a.	O
take	O
{	O
xl	O
,	O
..	O
,	O
,	O
xn	O
}	O
:	O
:	O
:	O
:	O
xl	O
+	O
{	O
2i	O
,	O
...	O
,	O
2n	O
}	O
.	O
a	O
subset	O
of	O
this	O
may	O
be	O
characterized	O
by	O
a	O
string	O
sn	O
from	O
{	O
-i	O
,	O
1	O
}	O
n	O
,	O
1	O
's	O
denoting	O
membership	O
,	O
and	O
-l	O
's	O
denoting	O
absence	O
.	O
we	O
find	O
the	O
first	O
occurrence	O
of	O
sn	O
in	O
s	O
and	O
let	O
the	O
starting	O
point	O
be	O
g	O
(	O
k	O
)	O
.	O
take	O
h	O
:	O
:	O
:	O
:	O
21-k.	O
observe	O
that	O
(	O
k	O
(	O
xl	O
~	O
x	O
i	O
)	O
,	O
...	O
,	O
k	O
(	O
xn	O
~	O
x	O
i	O
)	O
)	O
:	O
:	O
:	O
:	O
:	O
:	O
:	O
:	O
(	O
k	O
(	O
2k	O
)	O
,	O
...	O
,	O
k	O
(	O
2k+n-l	O
)	O
)	O
(	O
a	O
(	O
2k	O
)	O
g	O
(	O
k	O
)	O
,	O
a	O
(	O
2k+i	O
)	O
g	O
(	O
k	O
+	O
1	O
)	O
,	O
...	O
,	O
a	O
(	O
2k+n-l	O
)	O
g	O
(	O
k	O
+	O
n	O
-	O
1	O
)	O
)	O
,	O
which	O
agrees	O
in	O
sign	O
with	O
sn	O
as	O
desired	O
.	O
hence	O
,	O
the	O
vc	B
dimension	I
is	O
infinite	O
.	O
the	O
next	O
kernel	B
is	O
symmetric	O
,	O
unimodal	O
,	O
and	O
piecewise	O
quadratic	O
.	O
the	O
intervals	O
into	O
which	O
[	O
0	O
,	O
(	O
0	O
)	O
is	O
divided	O
are	O
denoted	O
by	O
ao	O
,	O
ai	O
,	O
a2	O
,	O
...	O
,	O
from	O
left	O
to	O
right~	O
25.5	O
kernels	O
of	O
infinite	O
complexity	O
437	O
where	O
i	O
:	O
:	O
:	O
:	O
1	O
i	O
=	O
o.	O
on	O
each	O
ai	O
,	O
k	O
is	O
of	O
the	O
form	O
ax	O
2	O
+	O
bx	O
+	O
c.	O
observe	O
that	O
kf	O
!	O
=	O
2a	O
takes	O
the	O
sign	O
of	O
a.	O
also	O
,	O
any	O
finite	O
symmetric	O
difference	O
of	O
order	O
2	O
has	O
the	O
sign	O
of	O
a	O
,	O
as	O
k	O
(	O
x	O
+	O
8	O
)	O
+	O
k	O
(	O
x	O
-	O
8	O
)	O
-	O
2k	O
(	O
x	O
)	O
=	O
2a8	O
2	O
,	O
x	O
-	O
8	O
,	O
x	O
,	O
x	O
+	O
8	O
e	O
ai	O
.	O
we	O
take	O
four	O
points	O
and	O
construct	O
the	O
class	O
a	O
.	O
=	O
{	O
{	O
x	O
:	O
t	O
k	O
(	O
x	O
~	O
x	O
,	O
)	O
(	O
2y	O
,	O
-	O
1	O
)	O
>	O
o	O
}	O
:	O
h	O
>	O
o	O
}	O
,	O
where	O
xl	O
=	O
-8	O
,	O
x2	O
=	O
8	O
,	O
x3	O
=	O
x4	O
=	O
0	O
,	O
yi	O
=	O
y2	O
=	O
0	O
,	O
y3	O
=	O
y4	O
=	O
1	O
are	O
fixed	O
for	O
now	O
.	O
on	O
ai	O
,	O
we	O
let	O
the	O
``	O
a	O
''	O
coefficient	O
have	O
the	O
same	O
sign	O
as	O
g	O
(	O
i	O
)	O
,	O
known	O
from	O
the	O
previous	O
example	O
.	O
all	O
three	O
quadratic	O
coefficients	O
are	O
picked	O
so	O
that	O
k	O
:	O
:	O
:	O
:	O
0	O
and	O
k	O
is	O
unimodal	O
.	O
for	O
each	O
n	O
,	O
we	O
show	O
that	O
the	O
set	O
{	O
21	O
,	O
...	O
,	O
2n	O
}	O
can	O
be	O
shattered	O
by	O
intersecting	O
with	O
sets	O
from	O
~	O
.	O
a	O
subset	O
is	O
again	O
identified	O
by	O
a	O
{	O
-i	O
,	O
l	O
}	O
n-valued	O
string	O
sn	O
,	O
and	O
its	O
first	O
match	O
in	O
sis	O
g	O
(	O
k	O
)	O
,	O
...	O
,	O
g	O
(	O
k	O
+	O
n	O
-	O
1	O
)	O
.	O
we	O
take	O
h	O
=	O
21-k.	O
note	O
that	O
for	O
1	O
.	O
:	O
:	O
:	O
i	O
.	O
:	O
:	O
:	O
n	O
,	O
sign	O
(	O
k	O
(	O
2	O
'	O
~	O
8	O
)	O
+	O
k	O
(	O
2	O
'	O
:	O
8	O
)	O
_	O
2k	O
(	O
~	O
)	O
)	O
sign	O
(	O
k	O
(	O
2k+i	O
-	O
1	O
-	O
82k	O
-	O
1	O
)	O
+	O
k	O
(	O
2k+i	O
-	O
1	O
+	O
82k	O
-	O
l	O
)	O
2k	O
(	O
2k+i	O
-	O
1	O
)	O
)	O
sign	O
(	O
kf	O
!	O
(	O
2k+i-l	O
)	O
)	O
(	O
if	O
8	O
.	O
:	O
:	O
:	O
1/4	O
,	O
by	O
the	O
finite-difference	O
property	O
of	O
quadratics	O
)	O
g	O
(	O
k+i	O
-1	O
)	O
,	O
as	O
desired	O
.	O
hence	O
,	O
any	O
subset	O
of	O
{	O
2	O
1	O
,	O
...	O
,	O
2n	O
}	O
can	O
be	O
picked	O
out	O
.	O
the	O
previous	O
example	O
works	O
whenever	O
8	O
.	O
:	O
:	O
:	O
1/4	O
.	O
it	O
takes	O
just	O
a	O
little	O
thought	O
to	O
see	O
that	O
if	O
(	O
x	O
1	O
,	O
yr	O
)	O
,	O
...	O
,	O
(	O
x4	O
,	O
y4	O
)	O
are	O
i.i.d	O
.	O
and	O
drawn	O
from	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
then	O
p	O
ivai	O
=	O
oo	O
}	O
:	O
:	O
:	O
:	O
p	O
{	O
xi	O
=	O
-x2	O
,	O
x3	O
=	O
x4	O
=	O
0	O
,	O
yl	O
=	O
yz	O
=	O
0	O
,	O
y3	O
=	O
y4	O
=	O
i	O
}	O
,	O
and	O
this	O
is	O
positive	O
if	O
given	O
y	O
=	O
0	O
,	O
x	O
has	O
atoms	O
at	O
8	O
and	O
-8	O
for	O
some	O
8	O
>	O
0	O
;	O
and	O
given	O
y	O
=	O
1	O
,	O
x	O
has	O
an	O
atom	O
at	O
o.	O
however	O
,	O
with	O
some	O
work	O
,	O
we	O
may	O
even	O
remove	O
these	O
restrictions	O
(	O
see	O
problem	O
25.12	O
)	O
.	O
we	O
also	O
draw	O
the	O
reader	O
's	O
attention	O
to	O
an	O
8-point	O
example	O
in	O
which	O
k	O
is	O
sym	O
(	O
cid:173	O
)	O
metric	B
,	O
unimodal	O
,	O
and	O
convex	O
on	O
[	O
0	O
,	O
(	O
0	O
)	O
,	O
yet	O
ves	O
=	O
00	O
.	O
(	O
see	O
problem	O
25.11	O
.	O
)	O
next	O
we	O
turn	O
to	O
our	O
general	O
m-point	O
example	O
.	O
let	O
the	O
class	O
of	O
rules	O
be	O
given	O
by	O
g	O
m	O
,	O
h	O
(	O
x	O
)	O
-	O
-	O
{	O
if	O
``	O
~1~	O
(	O
2y	O
.	O
-	O
i	O
0	O
otherwise	O
,	O
.l	O
...	O
.l-l	O
i	O
l	O
)	O
k	O
(	O
x-xi	O
)	O
>	O
0	O
h	O
438	O
25	O
..	O
automatic	B
kernel	O
rules	O
h	O
>	O
0	O
,	O
where	O
the	O
data	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xm	O
,	O
ym	O
)	O
are	O
fixed	O
for	O
now	O
.	O
we	O
exhibit	O
a	O
nonnegative	O
kernel	B
such	O
that	O
if	O
the	O
xi	O
's	O
are	O
different	O
and	O
not	O
all	O
the	O
yi	O
's	O
are	O
the	O
same	O
,	O
vcm	O
=	O
00.	O
this	O
situation	O
occurs	O
with	O
probability	O
tending	O
to	O
one	O
whenever	O
x	O
is	O
nonatomic	O
and	O
p	O
{	O
y	O
=	O
i	O
}	O
e	O
(	O
0	O
,	O
1	O
)	O
.	O
it	O
is	O
stressed	O
here	O
that	O
the	O
same	O
kernel	B
k	O
is	O
used	O
regardless	O
of	O
the	O
data	O
.	O
the	O
kernel	B
is	O
of	O
the	O
form	O
k	O
(	O
x	O
)	O
=	O
ko	O
(	O
x	O
)	O
ia	O
(	O
x	O
)	O
,	O
where	O
ko	O
(	O
x	O
)	O
=	O
e-x2	O
,	O
and	O
a	O
c	O
r	O
will	O
be	O
specially	O
picked	O
.	O
order	O
x	O
i	O
,	O
...	O
,	O
xm	O
into	O
x	O
(	O
1	O
)	O
<	O
'	O
''	O
<	O
x	O
cm	O
)	O
and	O
find	O
the	O
first	O
pair	O
x	O
(	O
i	O
)	O
,	O
xu+i	O
)	O
with	O
opposite	O
values	O
for	O
y	O
(	O
i	O
)	O
.	O
without	O
loss	O
of	O
generality	O
,	O
assume	O
that	O
xci	O
)	O
=	O
-1	O
,	O
xu+1	O
)	O
=	O
1	O
,	O
2y	O
(	O
i	O
)	O
-	O
1	O
=	O
-1	O
,	O
and	O
2yu+i	O
)	O
-	O
1	O
=	O
1	O
(	O
yci	O
)	O
is	O
the	O
y-value	O
that	O
corresponds	O
to	O
xci	O
)	O
'	O
let	O
the	O
smallest	O
value	O
of	O
ix	O
(	O
j	O
)	O
1	O
,	O
j	O
=i	O
i	O
,	O
j	O
=i	O
i	O
+	O
1	O
be	O
denoted	O
by	O
8	O
>	O
1.	O
the	O
contribution	O
of	O
all	O
such	O
x	O
(	O
j	O
)	O
's	O
for	O
x	O
e	O
[	O
-8/3,8/3	O
]	O
is	O
not	O
more	O
than	O
(	O
m	O
-	O
2	O
)	O
ko	O
(	O
(	O
l	O
+	O
28/3	O
)	O
/	O
h	O
)	O
~	O
mko	O
(	O
(	O
l	O
+	O
28/3	O
)	O
/	O
h	O
)	O
.	O
the	O
contribution	O
of	O
either	O
xci	O
)	O
or	O
x	O
(	O
i+l	O
)	O
is	O
at	O
least	O
ko	O
(	O
(	O
1	O
+8/3	O
)	O
/	O
h	O
)	O
.	O
for	O
fixed	O
8	O
and	O
m	O
(	O
after	O
all	O
,	O
they	O
are	O
given	O
)	O
,	O
we	O
first	O
find	O
h*	O
such	O
that	O
h	O
~	O
h*	O
implies	O
1	O
+	O
8/3	O
)	O
ko	O
-	O
-	O
-	O
>	O
mko	O
(	O
h	O
(	O
1	O
+	O
28/3	O
)	O
h	O
.	O
for	O
h	O
~	O
h*	O
,	O
the	O
rule	B
,	O
for	O
x	O
e	O
[	O
-8/3,8/3	O
]	O
,	O
is	O
equivalent	O
to	O
a	O
2-point	O
rule	B
based	O
on	O
if	O
-	O
k	O
ezi	O
)	O
+	O
k	O
e-	O
;	O
;	O
i	O
)	O
>	O
0	O
gm	O
,	O
h	O
(	O
x	O
)	O
=	O
0	O
otherwise	O
.	O
{	O
we	O
define	O
the	O
set	O
a	O
by	O
00	O
2k-1	O
a=u	O
u	O
3	O
k==l	O
1==0	O
2k	O
1a	O
k	O
,	O
l	O
,	O
+	O
where	O
the	O
sets	O
ak	O
,	O
l	O
are	O
specified	O
later	O
.	O
(	O
for	O
c	O
=i	O
0	O
and	O
b	O
c	O
r	O
,	O
c	O
b	O
=	O
{	O
x	O
e	O
r	O
:	O
x	O
/	O
c	O
e	O
b	O
}	O
.	O
)	O
here	O
k	O
represents	O
the	O
length	O
of	O
a	O
bit	O
string	O
,	O
and	O
i	O
cycles	O
through	O
all	O
2k	O
bit	O
strings	O
of	O
length	O
k.	O
for	O
a	O
particular	O
such	O
bit	O
string	O
,	O
bi	O
,	O
...	O
,	O
bk	O
,	O
represented	O
by	O
l	O
,	O
we	O
define	O
ak	O
,	O
l	O
as	O
follows	O
.	O
first	O
of	O
all	O
,	O
ak	O
,	O
l	O
~	O
[	O
1/2	O
,	O
3/2	O
]	O
,	O
so	O
that	O
all	O
sets	O
32k	O
+1	O
ak	O
,	O
l	O
are	O
nonoverlapping	O
.	O
ak	O
,	O
l	O
consists	O
of	O
the	O
sets	O
(	O
~	O
[	O
1	O
-	O
(	O
i	O
+	O
l1	O
)	O
2k	O
'	O
1	O
-	O
(	O
i	O
+	O
~	O
)	O
2k	O
)	O
)	O
u	O
(	O
~1	O
(	O
1	O
+	O
(	O
i	O
+	O
~	O
)	O
2k	O
'	O
1	O
+	O
(	O
i	O
+	O
11	O
)	O
2k	O
j	O
)	O
ak	O
,	O
l	O
is	O
completed	O
by	O
symmetry	O
.	O
we	O
now	O
exhibit	O
for	O
each	O
integer	O
k	O
a	O
set	O
of	O
that	O
size	O
that	O
can	O
be	O
shattered	O
.	O
these	O
sets	O
will	O
be	O
positioned	O
in	O
[	O
-8/3,0	O
]	O
at	O
coordinate	O
values	O
-1/	O
(	O
2·	O
2p	O
)	O
,	O
-1/	O
(	O
3·	O
2	O
p	O
)	O
,	O
•••	O
,	O
-1/	O
(	O
(	O
k	O
+	O
1	O
)	O
.	O
2p	O
)	O
,	O
where	O
p	O
is	O
a	O
suitable	O
large	O
integer	O
such	O
that	O
1/	O
(	O
2p+l	O
)	O
<	O
8/3	O
.	O
assume	O
that	O
we	O
wish	O
to	O
extract	O
the	O
set	O
indexed	O
by	O
the	O
bit	O
vector	O
(	O
bl	O
,	O
...	O
,	O
bk	O
)	O
(	O
bi	O
=	O
1	O
means	O
that	O
-1/	O
(	O
i	O
+	O
1	O
)	O
2p	O
must	O
be	O
extracted	O
)	O
.	O
to	O
do	O
so	O
,	O
we	O
are	O
only	O
allowed	O
to	O
vary	O
h.	O
first	O
,	O
we	O
find	O
a	O
pair	O
(	O
k	O
'	O
,	O
i	O
)	O
,	O
where	O
k	O
'	O
is	O
at	O
least	O
k	O
and	O
1/	O
(	O
2k'+i	O
)	O
<	O
8/3	O
.	O
25.6	O
on	O
minimizing	O
the	O
apparent	B
error	I
rate	I
439	O
3	O
_2k	O
'	O
s	O
h	O
*	O
is	O
needed	O
too	O
.	O
also	O
,	O
1	O
is	O
such	O
that	O
it	O
matches	O
bi	O
,	O
...	O
,	O
h	O
in	O
its	O
first	O
k	O
s	O
k	O
'	O
bits	O
.	O
take	O
h	O
=	O
3-2k	O
'	O
-l	O
and	O
p	O
=	O
k	O
'	O
and	O
observe	O
that	O
gn	O
,	O
h	O
(	O
=	O
0	O
>	O
k	O
(	O
i+l	O
)	O
~kl	O
32k	O
+1	O
if	O
k	O
1	O
(	O
i+l	O
)	O
~kl	O
32k	O
+1	O
(	O
i	O
+	O
11	O
)	O
2k	O
'	O
)	O
0	O
otherwise	O
{	O
c-	O
'	O
-	O
'	O
)	O
c-	O
'	O
+1	O
)	O
{	O
1	O
{	O
1	O
if	O
1	O
+	O
(	O
i+i	O
)	O
2k	O
'	O
e	O
ak	O
,	O
l	O
,	O
1	O
-	O
if	O
1	O
+	O
(	O
i+	O
i	O
)	O
2k	O
'	O
1.	O
ak	O
,	O
l	O
,	O
1	O
-	O
0	O
unimportant	O
otherwise	O
if	O
bi	O
=	O
1	O
if	O
bi	O
=	O
0	O
,	O
(	O
i+i	O
)	O
2k	O
'	O
¢	O
ak	O
,	O
l	O
(	O
i+i	O
)	O
2k	O
'	O
e	O
ak	O
,	O
l	O
1	O
.	O
:	O
:	O
:	O
;	O
i	O
s	O
k.	O
thus	O
,	O
we	O
pick	O
the	O
desired	O
set	O
.	O
this	O
construction	O
may	O
be	O
repeated	O
for	O
all	O
values	O
of	O
1	O
of	O
course	O
.	O
we	O
have	O
shown	O
the	O
following	O
:	O
theorem	B
25.6.	O
if	O
x	O
is	O
nonatomic	O
,	O
p	O
{	O
y	O
=	O
i	O
}	O
e	O
(	O
0	O
,	O
1	O
)	O
,	O
and	O
if	O
vcm	O
denotes	O
the	O
vc	B
dimension	I
of	O
the	O
class	O
of	O
kernel	B
rules	I
based	O
on	O
m	O
i.i.d	O
.	O
data	O
drawn	O
from	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
and	O
with	O
the	O
kernel	B
specified	O
above	O
,	O
then	O
lim	O
p	O
{	O
vcm	O
=	O
oo	O
}	O
=	O
1.	O
m-+oo	O
25.6	O
on	O
minimizing	O
the	O
apparent	B
error	I
rate	I
in	O
this	O
section	O
,	O
we	O
look	O
more	O
closely	O
at	O
kernel	B
rules	I
that	O
are	O
picked	O
by	O
minimizing	O
the	O
resubstitution	B
estimate	O
l~r	O
)	O
over	O
kernel	B
rules	I
with	O
smoothing	B
factor	I
h	O
>	O
o.	O
we	O
make	O
two	O
remarks	O
in	O
this	O
respect	O
:	O
(	O
i	O
)	O
the	O
procedure	O
is	O
generally	O
inconsistent	O
if	O
x	O
is	O
nonatomic	O
.	O
(	O
ii	O
)	O
the	O
method	O
is	O
consistent	O
if	O
x	O
is	O
purely	O
atomic	O
.	O
to	O
see	O
(	O
i	O
)	O
,	O
take	O
k	O
=	O
l	O
{	O
so	O
,	O
d	O
'	O
we	O
note	O
that	O
l~r	O
)	O
=	O
0	O
if	O
h	O
<	O
miniij	O
iixi	O
-	O
xj	O
ii	O
as	O
gn	O
(	O
xi	O
)	O
=	O
yi	O
for	O
such	O
h.	O
in	O
fact	O
,	O
if	O
hn	O
is	O
the	O
minimizing	O
h	O
,	O
it	O
may	O
take	O
any	O
value	O
on	O
if	O
x	O
is	O
independent	O
of	O
y	O
,	O
p	O
{	O
y	O
=	O
i	O
}	O
=	O
2/3	O
,	O
and	O
x	O
has	O
a	O
density	O
,	O
then	O
,	O
lim	O
lim	O
inf	O
p	O
{	O
.	O
'	O
.	O
~in	O
_	O
ii	O
xi	O
-	O
x	O
j	O
ii	O
d.	O
:	O
:	O
:	O
;	O
c2	O
}	O
=	O
1	O
c	O
--	O
+oo	O
n	O
--	O
+oo	O
l	O
,	O
j.y	O
;	O
-l	O
,	O
yj-o	O
n	O
(	O
problem	O
25.13	O
)	O
.	O
thus	O
,	O
nhd	O
-+	O
0	O
in	O
probability	O
,	O
and	O
therefore	O
,	O
in	O
this	O
case	O
,	O
e	O
{	O
l	O
(	O
g	O
,	O
j	O
}	O
-+	O
2/3	O
(	O
since	O
ties	O
are	O
broken	O
by	O
favoring	O
the	O
``	O
0	O
''	O
class	O
)	O
.	O
hence	O
the	O
inconsistency	O
.	O
440	O
25.	O
automatic	B
kernel	O
rules	O
consider	O
next	O
x	O
purely	O
atomic	O
.	O
fix	O
(	O
x	O
1	O
,	O
yd	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
,	O
the	O
data	O
;	O
and	O
con	O
(	O
cid:173	O
)	O
sider	O
the	O
class	O
en	O
of	B
kernel	I
rules	I
in	O
which	O
h	O
>	O
0	O
is	O
a	O
free	O
parameter	O
.	O
if	O
a	O
typical	O
rule	B
is	O
gn	O
,	O
h	O
,	O
let	O
an	O
be	O
the	O
class	O
of	O
sets	O
{	O
x	O
:	O
gn	O
,	O
h	O
(	O
x	O
)	O
=	O
i	O
}	O
,	O
h	O
>	O
o.	O
if	O
gn	O
is	O
the	O
rule	B
that	O
minimizes	O
l~r	O
)	O
(	O
gn	O
,	O
h	O
)	O
'	O
we	O
have	O
<	O
2	O
sup	O
'l~r\gn	O
,	O
h	O
)	O
-	O
l	O
(	O
gn	O
,	O
h	O
)	O
1	O
(	O
theorem	B
8.4	O
)	O
g	O
''	O
,	O
izec	O
''	O
<	O
2	O
sup	O
!	O
!	O
tn	O
(	O
a	O
)	O
-	O
!	O
t	O
(	O
a	O
)	O
!	O
+	O
2	O
sup	O
!	O
!	O
tn	O
(	O
a	O
)	O
-	O
!	O
t	O
(	O
a	O
)	O
!	O
aean	O
(	O
a~	O
denotes	O
the	O
collection	O
of	O
sets	O
{	O
x	O
:	O
gn	O
,	O
h	O
(	O
x	O
)	O
=	O
od	O
aea~	O
<	O
4	O
sup	O
!	O
!	O
tn	O
(	O
b	O
)	O
-	O
beb	O
!	O
t	O
(	O
b	O
)	O
!	O
,	O
where	O
b	O
is	O
the	O
collection	O
of	O
all	O
borel	O
sets	O
.	O
however	O
,	O
the	O
latter	O
quantity	O
tends	O
to	O
zero	O
with	O
probability	O
one-as	O
denoting	O
the	O
set	O
of	O
the	O
atoms	O
of	O
x	O
by	O
t	O
,	O
we	O
have	O
sup	O
!	O
fln	O
(	O
b	O
)	O
-	O
beb	O
/j	O
,	O
(	O
b	O
)	O
!	O
1	O
2	O
l	O
l	O
!	O
tn	O
(	O
{	O
x	O
d	O
-	O
!	O
t	O
(	O
{	O
x	O
d	O
!	O
xet	O
1	O
<	O
2	O
l	O
!	O
fln	O
(	O
{	O
xd	O
-	O
xea	O
fl	O
(	O
{	O
xdi	O
+	O
!	O
tn	O
(	O
a	O
c	O
)	O
+	O
fl	O
(	O
ac	O
)	O
(	O
where	O
a	O
is	O
an	O
arbitrary	O
finite	O
subset	O
of	O
t	O
)	O
<	O
2	O
!	O
t	O
(	O
ac	O
)	O
+	O
2	O
l	O
!	O
!	O
tn	O
(	O
{	O
xd	O
-	O
1	O
xea	O
!	O
t	O
(	O
{	O
xdi	O
+	O
!	O
!	O
tn	O
(	O
ac	O
)	O
-	O
!	O
t	O
(	O
ac	O
)	O
i.	O
the	O
first	O
term	O
is	O
small	O
by	O
choice	O
of	O
a	O
and	O
the	O
last	O
two	O
terms	O
are	O
small	O
by	O
applying	O
hoeffding	O
's	O
inequality	B
to	O
each	O
of	O
the	O
!	O
ai	O
+	O
1	O
terms	O
.	O
for	O
yet	O
a	O
different	O
proof	O
,	O
the	O
reader	O
is	O
referred	O
to	O
problems	O
25.16	O
and	O
25.17.	O
note	O
next	O
that	O
but	O
if	O
k	O
(	O
o	O
)	O
>	O
0	O
and	O
k	O
(	O
x	O
)	O
-+	O
0	O
as	O
iixll	O
-+	O
00	O
along	O
any	O
ray	O
,	O
then	O
we	O
have	O
infgn	O
,	O
hecn	O
l	O
(	O
gn	O
,	O
h	O
)	O
:	O
:	O
:	O
l	O
(	O
g	O
;	O
l	O
)	O
'	O
where	O
g	O
;	O
z	O
is	O
the	O
fundamental	B
rule	I
discussed	O
in	O
chapter	O
27	O
(	O
in	O
which	O
h	O
=	O
0	O
)	O
.	O
theorem	B
27.1	O
shows	O
that	O
l	O
(	O
g:1	O
)	O
-+	O
l	O
*	O
with	O
probability	O
one	O
,	O
and	O
therefore	O
,	O
l	O
(	O
gn	O
)	O
--	O
+	O
0	O
with	O
probability	O
one	O
,	O
proving	O
theorem	B
25.7.	O
take	O
any	O
kernel	B
k	O
with	O
k	O
(	O
o	O
)	O
>	O
0	O
and	O
k	O
(	O
x	O
)	O
-+	O
0	O
as	O
i	O
!	O
xll	O
--	O
+	O
00	O
along	O
any	O
ray	O
,	O
and	O
let	O
gn	O
be	O
selected	O
by	O
minimizing	O
l~r	O
)	O
over	O
all	O
h	O
:	O
:	O
:	O
:	O
o.	O
then	O
l	O
(	O
gn	O
)	O
--	O
+	O
l	O
*	O
almost	O
surely	O
whenever	O
the	O
distribution	B
of	O
x	O
is	O
purely	O
atomic	O
.	O
25.7	O
minimizing	O
the	O
deleted	B
estimate	O
441	O
remark	O
.	O
the	O
above	O
theorem	B
is	O
all	O
the	O
more	O
surprising	O
since	O
we	O
may	O
take	O
just	O
about	O
any	O
kernel	B
,	O
such	O
as	O
k	O
(	O
x	O
)	O
=	O
e-	O
lix1e	O
,	O
k	O
(	O
x	O
)	O
=	O
sin	O
(	O
iix	O
1i	O
)	O
/iix	O
ii	O
,	O
or	O
k	O
(	O
x	O
)	O
=	O
(	O
1	O
+	O
iixll	O
)	O
-1/3	O
.	O
also	O
,	O
if	O
x	O
puts	O
its	O
mass	O
on	O
a	O
dense	O
subset	O
of	O
n	O
d	O
,	O
the	O
data	O
will	O
look	O
and	O
feel	O
like	O
data	O
from	O
an	O
absolutely	O
continuous	O
distribution	B
(	O
well	O
,	O
very	O
roughly	O
speaking	O
)	O
;	O
yet	O
there	O
is	O
a	O
dramatic	O
difference	O
with	O
rules	O
that	O
minimize	O
l~r	O
)	O
when	O
the	O
x/s	O
are	O
indeed	O
drawn	O
from	O
a	O
distribution	O
with	O
a	O
density	O
!	O
0	O
25.7	O
minimizing	O
the	O
deleted	B
estimate	O
we	O
have	O
seen	O
in	O
chapter	O
24	O
that	O
the	O
deleted	B
estimate	O
of	O
the	O
error	O
probability	O
of	O
a	O
kernel	O
rule	B
is	O
generally	O
very	O
reliable	O
.	O
this	O
suggests	O
using	O
the	O
estimate	B
as	O
a	O
basis	O
of	O
selecting	O
a	O
smoothing	O
factor	O
for	O
the	O
kernel	B
rule	I
.	O
let	O
l~d	O
)	O
(	O
gn	O
,	O
h	O
)	O
be	O
the	O
deleted	B
estimate	O
of	O
l	O
(	O
gn	O
,	O
h	O
)	O
,	O
obtained	O
by	O
using	O
in	O
gn-l	O
,	O
h	O
the	O
same	O
kernel	B
k	O
and	O
smoothing	B
factor	I
h.	O
define	O
the	O
set	O
of	O
h	O
'	O
s	O
for	O
which	O
l	O
(	O
d	O
)	O
(	O
)	O
-	O
n	O
gn	O
,	O
h	O
-	O
h'e	O
[	O
o	O
,	O
oo	O
)	O
.	O
f	O
l	O
(	O
d	O
)	O
(	O
in	O
11	O
)	O
gn	O
,	O
h	O
'	O
by	O
a.	O
set	O
hn	O
=	O
inf	O
{	O
h	O
:	O
h	O
e	O
a	O
}	O
.	O
two	O
fundamental	O
questions	O
regarding	O
hn	O
must	O
be	O
asked	O
:	O
(	O
a	O
)	O
if	O
we	O
use	O
hn	O
in	O
the	O
kernel	B
estimate	O
,	O
is	O
the	O
rule	B
universally	O
consistent	O
?	O
note	O
in	O
this	O
respect	O
that	O
theorem	B
25.3	O
can	O
not	O
be	O
used	O
because	O
it	O
is	O
not	O
true	O
that	O
hn	O
~	O
0	O
in	O
probability	O
in	O
all	O
cases	O
.	O
(	O
b	O
)	O
if	O
hn	O
is	O
used	O
as	O
smoothing	B
factor	I
,	O
how	O
does	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
-	O
l	O
*	O
compare	O
to	O
e	O
{	O
infh	O
l	O
(	O
gn	O
,	O
lj	O
}	O
-	O
l	O
*	O
?	O
to	O
our	O
knowledge	O
,	O
both	O
questions	O
have	O
been	O
unanswered	O
so	O
far	O
.	O
we	O
believe	O
that	O
this	O
way	O
of	O
selecting	O
h	O
is	O
very	O
effective	O
.	O
below	O
,	O
generalizing	O
a	O
result	O
by	O
tutz	O
(	O
1986	O
)	O
(	O
theorem	B
27.6	O
)	O
,	O
we	O
show	O
that	O
the	O
kernel	B
rule	I
obtained	O
by	O
minimizing	O
the	O
deleted	B
estimate	O
is	O
consistent	O
when	O
the	O
distribution	B
of	O
x	O
is	O
atomic	O
with	O
finitely	O
many	O
atoms	O
.	O
remark	O
.	O
if	O
1	O
]	O
==	O
i	O
everywhere	O
,	O
so	O
that	O
yi	O
==	O
i	O
for	O
all	O
i	O
,	O
and	O
if	O
k	O
has	O
support	B
equal	O
to	O
the	O
unit	O
ball	O
in	O
n	O
d	O
,	O
and	O
k	O
>	O
0	O
on	O
this	O
ball	O
,	O
then	O
l~d	O
)	O
(	O
gn	O
,	O
h	O
)	O
is	O
minimal	O
and	O
zero	O
if	O
where	O
x	O
f	O
n	O
denotes	O
the	O
nearest	B
neighbor	I
of	O
x	O
j	O
among	O
the	O
data	O
points	O
xl	O
,	O
...	O
,	O
xj-l	O
,	O
xj+l	O
'	O
...	O
,	O
x	O
n	O
.	O
but	O
this	O
shows	O
that	O
hn	O
=	O
m~x	O
iixj	O
-	O
xfnii	O
.	O
l-	O
:	O
s	O
;	O
-	O
:	O
sn	O
442	O
25.	O
automatic	B
kernel	O
rules	O
we	O
leave	O
it	O
as	O
an	O
exercise	O
to	O
show	O
that	O
for	O
any	O
nonatomic	O
distribution	B
of	O
x	O
,	O
n	O
hl~	O
~	O
00	O
with	O
probability	O
one	O
.	O
however	O
,	O
it	O
is	O
also	O
true	O
that	O
for	O
some	O
distributions	O
,	O
hn	O
~	O
00	O
with	O
probability	O
one	O
(	O
problem	O
25.19	O
)	O
.	O
nevertheless	O
,	O
for	O
1	O
]	O
==	O
1	O
,	O
the	O
rule	B
is	O
consistent	O
!	O
if	O
1	O
]	O
==	O
0	O
everywhere	O
,	O
then	O
yi	O
=	O
0	O
for	O
all	O
i.	O
under	O
the	O
same	O
condition	O
on	O
k	O
as	O
above	O
,	O
with	O
tie	O
breaking	O
in	O
favor	O
of	O
class	O
``	O
0	O
''	O
(	O
as	O
in	O
the	O
entire	O
book	O
)	O
,	O
it	O
is	O
clear	O
that	O
hn	O
==	O
o.	O
interestingly	O
,	O
here	O
too	O
the	O
rule	B
is	O
consistent	O
despite	O
the	O
strange	O
value	O
of	O
hn	O
.	O
o	O
we	O
state	O
the	O
following	O
theorem	B
for	O
window	B
kernels	O
,	O
though	O
it	O
can	O
be	O
extended	O
to	O
include	O
kernels	O
taking	O
finitely	O
many	O
different	O
values	O
(	O
see	O
problem	O
25.18	O
)	O
:	O
theorem	B
25.8.	O
let	O
gn	O
be	O
the	O
kernel	B
rule	I
with	O
kernel	B
k	O
=	O
iafor	O
some	O
bounded	O
set	O
a	O
c	O
nd	O
containing	O
the	O
origin	O
,	O
and	O
with	O
smoothing	O
factor	O
chosen	O
by	O
minimizing	O
the	O
deleted	B
estimate	O
as	O
described	O
above	O
.	O
then	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
-+	O
l	O
*	O
whenever	O
x	O
has	O
a	O
discrete	O
distribution	B
with	O
finitely	O
many	O
atoms	O
.	O
proof	O
.	O
let	O
ui	O
,	O
...	O
,	O
un	O
be	O
independent	O
,	O
uniform	B
[	O
0	O
,	O
1	O
]	O
random	O
variables	O
,	O
indepen	O
(	O
cid:173	O
)	O
dent	O
of	O
the	O
data	O
d	O
n	O
,	O
and	O
consider	O
the	O
augmented	O
sample	O
where	O
x	O
;	O
=	O
(	O
xi	O
,	O
vi	O
)	O
.	O
for	O
each	O
h	O
~	O
0	O
,	O
introduce	O
the	O
rule	B
g~	O
,	O
h	O
(	O
x	O
!	O
)	O
=	O
{	O
if	O
l~i=l	O
k	O
(	O
x~	O
;	O
i	O
)	O
(	O
2yi	O
-	O
1	O
)	O
-	O
l~i=l	O
k	O
(	O
0	O
)	O
(	O
2yi	O
-	O
o	O
otherwise	O
,	O
l	O
)	O
i	O
{	O
u=ui	O
}	O
>	O
0	O
where	O
xl	O
=	O
(	O
x	O
,	O
u	O
)	O
e	O
nd	O
x	O
[	O
0,1	O
]	O
.	O
observe	O
that	O
minimizing	O
the	O
resubstitution	B
estimate	O
l~r	O
)	O
(	O
g~	O
,	O
h	O
)	O
over	O
these	O
rules	O
yields	O
the	O
same	O
h	O
as	O
minimizing	O
l~d	O
)	O
(	O
gn	O
,	O
h	O
)	O
over	O
the	O
original	O
kernel	B
rules	I
.	O
furthermore	O
,	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
=	O
e	O
{	O
l	O
(	O
g~	O
)	O
}	O
if	O
g~	O
is	O
obtained	O
by	O
minimizing	O
l~r	O
)	O
(	O
g	O
;	O
l	O
,	O
h	O
)	O
'	O
it	O
follows	O
from	O
the	O
universal	B
consistency	I
of	O
kernel	B
rules	I
(	O
theorem	B
1	O
0.1	O
)	O
that	O
lim	O
inf	O
l	O
(	O
g~	O
h	O
)	O
-+	O
l	O
*	O
with	O
probability	O
one	O
.	O
n	O
--	O
+oo	O
h	O
'	O
thus	O
,	O
it	O
suffices	O
to	O
investigate	O
l	O
(	O
g~	O
)	O
-	O
subsets	O
a	O
;	O
l	O
c	O
nd	O
x	O
[	O
0	O
,	O
1	O
]	O
by	O
infh	O
l	O
(	O
g~	O
,	O
h	O
)	O
'	O
for	O
a	O
given	O
d~	O
,	O
define	O
the	O
a~	O
=	O
{	O
xl	O
:	O
g~	O
,	O
h	O
(	O
xi	O
)	O
=	O
i	O
}	O
,	O
h	O
e	O
[	O
0	O
,	O
(	O
0	O
)	O
.	O
arguing	O
as	O
in	O
the	O
previous	O
section	O
,	O
we	O
see	O
that	O
25.7	O
minimizing	O
the	O
deleted	B
estimate	O
443	O
where	O
v	O
is	O
the	O
measure	B
of	O
x	O
'	O
on	O
nd	O
x	O
[	O
0	O
,	O
1	O
]	O
,	O
and	O
vn	O
is	O
the	O
empirical	B
measure	I
determined	O
by	O
d~	O
.	O
observe	O
that	O
each	O
a~	O
can	O
be	O
written	O
as	O
ah	O
=	O
{	O
x	O
e	O
rd	O
:	O
t	O
k	O
e	O
~	O
xi	O
)	O
(	O
2yi	O
-	O
1	O
)	O
>	O
o	O
}	O
,	O
{	O
(	O
x	O
,	O
u	O
)	O
end	O
x	O
{	O
ui	O
,	O
...	O
,	O
un	O
}	O
:	O
t	O
k	O
e	O
~	O
xi	O
)	O
(	O
2yi	O
-	O
1	O
)	O
-	O
k	O
(	O
o	O
)	O
t	O
(	O
2yi	O
-	O
l	O
)	O
i	O
{	O
u~u	O
;	O
}	O
>	O
0	O
)	O
.	O
where	O
and	O
b	O
;	O
1	O
clearly	O
,	O
as	O
it	O
suffices	O
to	O
prove	O
that	O
suph	O
ivn	O
(	O
b	O
!	O
1	O
)	O
-	O
m	O
(	O
ah	O
)	O
1	O
--	O
+	O
°	O
in	O
probability	O
,	O
as	O
n	O
--	O
+	O
00.	O
then	O
sup	O
ivn	O
(	O
b~	O
)	O
-	O
h	O
{	O
l	O
(	O
ah	O
)	O
1	O
<	O
<	O
sup	O
ivn	O
(	O
b~	O
)	O
-	O
v	O
(	O
b~	O
)	O
1	O
+	O
sup	O
ivcb~	O
)	O
-	O
m	O
(	O
ah	O
)	O
1	O
h	O
h	O
sup	O
ivncb	O
'	O
)	O
-	O
vcb	O
'	O
)	O
1	O
+	O
sup	O
iv	O
(	O
b~	O
)	O
-	O
m	O
(	O
ah	O
)	O
l.	O
b'crdx	O
{	O
u1	O
,	O
...	O
,	O
un	O
}	O
h	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
tends	O
to	O
zero	O
in	O
probability	O
,	O
which	O
may	O
be	O
seen	O
by	O
problem	O
25.17	O
and	O
the	O
independence	O
of	O
the	O
ui	O
'	O
s	O
of	O
dn	O
.	O
to	O
bound	O
the	O
second	O
term	O
,	O
observe	O
that	O
for	O
each	O
h	O
,	O
iv	O
(	O
b	O
;	O
)	O
-	O
fl	O
(	O
ah	O
)	O
1	O
<	O
fl	O
(	O
{	O
x	O
:	O
it	O
,	O
k	O
e	O
~	O
xi	O
)	O
(	O
2y	O
,	O
-	O
o	O
!	O
-	O
<	O
:	O
k	O
(	O
o	O
)	O
}	O
)	O
=	O
t	O
;	O
p	O
{	O
x	O
=	O
xj	O
}	O
i	O
{	O
!	O
i	O
:	O
~=1	O
k	O
(	O
xj~xi	O
)	O
c2yi-l	O
)	O
!	O
:	O
:	O
:	O
k	O
(	O
o	O
)	O
}	O
'	O
n	O
where	O
x	O
i	O
,	O
...	O
,	O
x	O
n	O
are	O
the	O
atoms	O
of	O
x.	O
thus	O
,	O
the	O
proof	O
is	O
finished	O
if	O
we	O
show	O
that	O
for	O
each	O
xj	O
,	O
444	O
25.	O
automatic	B
kernel	O
rules	O
now	O
,	O
we	O
exploit	O
the	O
special	O
form	O
k	O
=	O
fa	O
of	O
the	O
kernel	B
.	O
observe	O
that	O
=	O
<	O
ii	O
i	O
-	O
h	O
l..l~l	O
a	O
su.p	O
f	O
{	O
i	O
''	O
,	O
n	O
i	O
(	O
xj-xi	O
)	O
c2y-l	O
)	O
i	O
<	O
i	O
}	O
n	O
l	O
f	O
{	O
ivi+	O
,	O
,+vk	O
i	O
:	O
si	O
}	O
,	O
where	O
k=l	O
vk	O
=	O
l	O
(	O
2yi	O
-	O
1	O
)	O
i	O
:	O
xi=x	O
(	O
k	O
)	O
and	O
x	O
(	O
l	O
)	O
,	O
x	O
(	O
2	O
)	O
,	O
...	O
,	O
xcn	O
)	O
is	O
an	O
ordering	O
of	O
the	O
atoms	O
of	O
x	O
such	O
that	O
x	O
(	O
1	O
)	O
=	O
x	O
j	O
and	O
(	O
xj	O
-	O
x	O
(	O
k	O
)	O
)	O
/	O
h	O
e	O
a	O
implies	O
(	O
xj	O
-	O
x	O
(	O
k-l	O
)	O
)	O
/	O
h	O
e	O
a	O
for	O
1	O
<	O
k	O
~	O
n.	O
by	O
properties	O
of	O
the	O
binomial	B
distribution	I
,	O
as	O
n	O
~	O
00	O
,	O
this	O
completes	O
the	O
proof	O
.	O
0	O
historical	O
remark	O
.	O
in	O
an	O
early	O
paper	O
by	O
habbema	O
,	O
hermans	O
,	O
and	O
van	O
den	O
broek	O
(	O
1974	O
)	O
,	O
the	O
deleted	B
estimate	O
is	O
used	O
to	O
select	O
an	O
appropriate	O
subspace	O
for	O
the	O
kernel	B
rule	I
.	O
the	O
kernel	B
rules	I
in	O
tum	O
have	O
smoothing	O
factors	O
that	O
are	O
selected	O
by	O
maximum	B
likelihood	I
.	O
0	O
25.8	O
sieve	O
methods	O
sieve	O
methods	O
pick	O
a	O
best	O
estimate	B
or	O
rule	B
from	O
a	O
limited	O
class	O
of	O
rules	O
.	O
for	O
example	O
,	O
our	O
sieve	O
ck	O
might	O
consist	O
of	O
rules	O
of	O
the	O
form	O
where	O
the	O
ai	O
's	O
are	O
real	O
numbers	O
,	O
the	O
xi	O
's	O
are	O
points	O
from	O
n	O
d	O
,	O
and	O
the	O
hi	O
's	O
are	O
positive	O
numbers	O
.	O
there	O
are	O
formally	O
(	O
d	O
+	O
2	O
)	O
k	O
free	O
scalar	O
parameters	O
.	O
if	O
we	O
pick	O
a	O
rule	O
¢~	O
that	O
minimizes	O
the	O
empirical	B
error	I
on	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
,	O
we	O
are	O
governed	O
by	O
the	O
theorems	O
of	O
chapters	O
12	O
and	O
18	O
,	O
and	O
we	O
will	O
need	O
to	O
find	O
the	O
vc	B
dimension	I
of	O
ck	O
.	O
for	O
this	O
,	O
conditions	O
on	O
k	O
will	O
be	O
needed	O
.	O
we	O
will	O
return	O
to	O
this	O
question	O
in	O
chapter	O
30	O
on	O
neural	O
networks	O
,	O
as	O
they	O
are	O
closely	O
related	O
to	O
the	O
sieves	O
described	O
here	O
.	O
25.9	O
squared	O
eltor	O
minimization	O
445	O
25.9	O
squared	B
error	I
minimization	I
many	O
researchers	O
have	O
considered	O
the	O
problem	O
of	O
selecting	O
h	O
in	O
order	O
to	O
minimize	O
the	O
l2	O
error	O
(	O
integrated	O
squared	B
error	I
)	O
of	O
the	O
kernel	B
estimate	O
--	O
-	O
1	O
]	O
11	O
,	O
h	O
=	O
1	O
.	O
~~l	O
y.	O
k	O
(	O
xi-x	O
)	O
n	O
l..l=l	O
1	O
.	O
~~	O
k	O
(	O
xi-x	O
)	O
n	O
l..l=l	O
h	O
1	O
h	O
of	O
the	O
regression	B
function	I
1	O
]	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
11	O
x	O
=	O
x	O
}	O
,	O
for	O
example	O
,	O
hardie	O
and	O
marron	O
(	O
1985	O
)	O
proposed	O
and	O
studied	O
a	O
cross-validation	O
method	O
for	O
choosing	O
the	O
optimal	O
h	O
for	O
the	O
kernel	B
regression	O
estimate	B
.	O
they	O
obtain	O
asymptotic	B
optimality	I
for	O
the	O
integrated	O
squared	B
error	I
.	O
although	O
their	O
method	O
gives	O
us	O
a	O
choice	O
for	O
h	O
if	O
we	O
consider	O
1	O
]	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
llx	O
=	O
x	O
}	O
as	O
the	O
regression	B
function	I
,	O
it	O
is	O
not	O
clear	O
that	O
the	O
h	O
thus	O
obtained	O
is	O
optimal	O
for	O
the	O
probability	O
of	O
error	O
.	O
in	O
fact	O
,	O
as	O
the	O
following	O
theorem	B
illustrates	O
,	O
for	O
some	O
distributions	O
,	O
the	O
smoothing	O
parameter	O
that	O
minimizes	O
the	O
l2	O
error	O
yields	O
a	O
rather	O
poor	O
error	O
probability	O
compared	O
to	O
that	O
corresponding	O
to	O
the	O
optimal	O
h.	O
theorem	B
25.9.	O
let	O
d	O
=	O
l.	O
consider	O
the	O
kernel	B
classification	O
rule	B
with	O
the	O
window	B
kernel	O
k	O
=	O
/	O
[	O
-1,1	O
]	O
,	O
and	O
smoothing	O
parameter	O
h	O
>	O
o.	O
denote	O
its	O
error	O
probability	O
by	O
ln	O
(	O
h	O
)	O
.	O
let	O
h*	O
be	O
the	O
smoothing	O
parameter	O
that	O
minimizes	O
the	O
mean	O
integrated	O
squared	B
error	I
then	O
for	O
some	O
distributions	O
e	O
{	O
ln	O
(	O
h*	O
)	O
}	O
.	O
hm	O
n-+oo	O
infh	O
e	O
{	O
ln	O
(	O
h	O
)	O
}	O
=	O
00	O
,	O
and	O
the	O
convergence	O
is	O
exponentially	O
fast	O
.	O
we	O
leave	O
the	O
details	O
of	O
the	O
proof	O
to	O
the	O
reader	O
(	O
problem	O
25.20	O
)	O
.	O
only	O
a	O
rough	O
sketch	O
is	O
given	O
here	O
.	O
consider	O
x	O
uniform	B
on	O
[	O
0	O
,	O
1	O
]	O
u	O
[	O
3,4	O
]	O
,	O
and	O
define	O
if	O
x	O
e	O
[	O
0	O
,	O
1	O
]	O
1	O
]	O
(	O
x	O
)	O
=	O
2	O
-	O
x	O
/2	O
otherwise	O
.	O
i	O
-	O
x	O
/2	O
{	O
the	O
optimal	O
value	O
of	O
h	O
(	O
i.e.	O
,	O
the	O
value	O
minimizing	O
the	O
error	O
probability	O
)	O
is	O
one	O
.	O
it	O
is	O
constant	O
,	O
independent	O
of	O
n.	O
this	O
shows	O
that	O
we	O
should	O
not	O
a	O
priori	O
exclude	O
any	O
values	O
of	O
h	O
,	O
as	O
is	O
commonly	O
done	O
in	O
studies	O
on	O
regression	O
and	O
density	B
estimation	I
.	O
the	O
minimal	O
error	O
probability	O
can	O
be	O
bounded	O
from	O
above	O
(	O
using	O
hoeffding	O
's	O
inequality	B
)	O
by	O
e-	O
c1n	O
for	O
some	O
constant	O
cl	O
>	O
0.	O
on	O
the	O
other	O
hand	O
,	O
straightforward	O
calculations	O
show	O
that	O
the	O
smoothing	B
factor	I
h*	O
that	O
minimizes	O
the	O
mean	O
integrated	O
446	O
25.	O
,	O
automatic	B
kernel	O
rules	O
squared	B
error	I
goes	O
to	O
zero	O
as	O
n	O
-+	O
(	O
xl	O
as	O
0	O
(	O
n-	O
i	O
/	O
4	O
)	O
.	O
the	O
corresponding	O
error	O
probability	O
is	O
larger	O
than	O
e-c2h*n	O
for	O
some	O
constant	O
c2	O
.	O
the	O
order-of-magnitude	O
difference	O
between	O
the	O
exponents	O
explains	O
the	O
exponential	B
'	O
speed	O
of	O
convergence	O
to	O
infinity	O
.	O
problems	O
and	O
exercises	O
problem	O
25.1.	O
prove	O
theorem	B
25.2.	O
problem	O
25.2.	O
show	O
that	O
for	O
any	O
r	O
>	O
0	O
,	O
the	O
kernel	B
k	O
(	O
x	O
)	O
=	O
{	O
~/iixi	O
(	O
if	O
ilxll	O
s	O
1	O
otherwise	O
,	O
satisfies	O
the	O
conditions	O
of	O
theorem	O
25.3.	O
problem	O
25.3.	O
assume	O
that	O
k	O
is	O
a	O
regular	O
kernel	B
with	O
kernel	B
complexity	I
km	O
s	O
my	O
for	O
some	O
constant	O
y	O
>	O
0.	O
let	O
gn	O
be	O
selected	O
so	O
as	O
to	O
minimize	O
the	O
holdout	B
error	O
estimate	B
l1	O
,	O
m	O
(	O
¢	O
>	O
m	O
)	O
over	O
cm	O
,	O
the	O
class	O
of	O
kernel	B
rules	I
based	O
upon	O
the	O
first	O
m	O
data	O
points	O
,	O
with	O
smoothing	O
factor	O
h	O
>	O
0.	O
assume	O
furthermore	O
that	O
we	O
vary	O
lover	O
[	O
log2	O
n	O
,	O
n12	O
]	O
,	O
and	O
that	O
we	O
pick	O
the	O
best	O
l	O
(	O
and	O
m	O
=	O
n	O
-	O
1	O
)	O
by	O
minimizing	O
the	O
holdout	B
error	O
estimate	B
again	O
.	O
show	O
that	O
the	O
obtained	O
rule	B
is	O
strongly	O
universally	O
consistent	O
.	O
problem	O
25.4.	O
prove	O
that	O
the	O
kernel	B
complexity	I
km	O
of	O
the	O
de	O
la	O
vallee-poussin	O
kernel	B
k	O
(	O
x	O
)	O
=	O
(	O
sinxlxf	O
,	O
x	O
e	O
r	O
,	O
is	O
infinite	O
when	O
m	O
:	O
:	O
:	O
:	O
2.	O
problem	O
25.5.	O
show	O
that	O
km	O
=	O
00	O
for	O
m	O
:	O
:	O
:	O
:	O
2	O
when	O
k	O
(	O
x	O
)	O
=	O
cos2	O
(	O
x	O
)	O
,	O
x	O
e	O
r.	O
problem	O
25.6.	O
compute	O
an	O
upper	O
bound	O
for	O
the	O
kernel	B
complexity	I
km	O
for	O
the	O
following	O
kernels	O
,	O
where	O
x	O
=	O
(	O
x	O
(	O
1	O
)	O
,	O
...	O
,	O
xed	O
)	O
~	O
e	O
rd	O
:	O
a.	O
b.	O
c.	O
d.	O
k	O
(	O
x	O
)	O
=	O
d	O
ti	O
(	O
i=l	O
i-x	O
(	O
i	O
)	O
2	O
)	O
iux	O
(	O
i	O
)	O
i	O
:	O
:	O
:	O
l	O
}	O
'	O
1	O
d	O
k	O
(	O
x	O
)	O
=	O
(	O
1	O
+	O
ilxl1	O
2t	O
'	O
ex	O
>	O
0.	O
k	O
(	O
x	O
)	O
=	O
ti	O
-1	O
(	O
i	O
)	O
2	O
'	O
k	O
(	O
x	O
)	O
=	O
ti	O
cos	O
(	O
x	O
(	O
i	O
)	O
iux	O
(	O
i	O
)	O
i	O
:	O
:	O
:	O
n/2j	O
'	O
i=l	O
+x	O
1	O
d	O
i=l	O
problem	O
25.7.	O
can	O
you	O
construct	O
a	O
kernel	O
on	O
r	O
with	O
the	O
property	O
that	O
its	O
complexity	O
satisfies	O
2m	O
s	O
km	O
<	O
00	O
for	O
all	O
m	O
?	O
prove	O
your	O
claim	O
.	O
problem	O
25.8.	O
show	O
that	O
for	O
kernel	B
classes	O
cm	O
with	O
kernel	O
rules	O
having	O
a	O
fixed	O
training	O
sequence	O
dm	O
but	O
variable	B
h	O
>	O
0	O
,	O
we	O
have	O
vcm	O
s	O
10g2	O
km·	O
problems	O
and	O
exercises	O
447	O
problem	O
25.9.	O
calculate	O
upper	O
bounds	O
for	O
s	O
(	O
em	O
,	O
/	O
)	O
when	O
em	O
is	O
the	O
class	O
of	O
kernel	B
rules	I
based	O
on	O
fixed	O
training	O
data	O
but	O
with	O
variable	O
parameter	O
e	O
in	O
the	O
following	O
cases	O
(	O
for	O
definitions	O
,	O
see	O
section	O
25.4	O
)	O
:	O
(	O
1	O
)	O
kg	O
is	O
a	O
product	O
kernel	B
of	O
r	O
d	O
,	O
where	O
the	O
unidimensional	O
kernel	B
k	O
has	O
kernel	B
complexity	I
km	O
.	O
(	O
2	O
)	O
kg	O
=	O
l+lixl	O
:	O
a/ha	O
,	O
where	O
e	O
=	O
(	O
h	O
,	O
a	O
)	O
,	O
h	O
>	O
0	O
,	O
a	O
>	O
°	O
are	O
two	O
parameters	O
.	O
(	O
3	O
)	O
kg	O
(	O
x	O
)	O
=	O
l	O
{	O
xea	O
}	O
,	O
where	O
a	O
is	O
any	O
ellipsoid	O
ofrd	O
centered	O
at	O
the	O
origin	O
.	O
problem	O
25.lo	O
.	O
prove	O
or	O
disprove	O
:	O
if	O
dm	O
is	O
fixed	O
and	O
em	O
is	O
the	O
class	O
of	O
all	O
kernel	B
rules	I
based	O
on	O
dm	O
with	O
k	O
=	O
la	O
,	O
a	O
being	O
any	O
convex	O
set	O
ofrd	O
containing	O
the	O
origin	O
,	O
is	O
it	O
possible	O
that	O
vcm	O
=	O
00	O
,	O
or	O
is	O
vcm	O
<	O
00	O
for	O
all	O
possible	O
configurations	O
of	O
dm	O
?	O
problem	O
25.11.	O
let	O
as	O
~	O
{	O
{	O
x	O
.	O
t	O
k	O
(	O
x	O
~	O
x	O
,	O
)	O
(	O
2y	O
,	O
-	O
i	O
)	O
>	O
o	O
}	O
,	O
h	O
>	O
o	O
}	O
with	O
xl	O
=	O
-38	O
,	O
x2	O
=	O
x3	O
=	O
x	O
4	O
=	O
-8	O
,	O
xs	O
=	O
x6	O
=	O
x7	O
=	O
8	O
,	O
x8	O
=	O
38	O
,	O
yi	O
=	O
1	O
,	O
y2	O
=	O
y3	O
=	O
y4	O
=	O
-1	O
,	O
ys	O
=	O
y6	O
=	O
y7	O
=	O
1	O
,	O
y8	O
=	O
-1	O
,	O
and	O
let	O
k	O
be	O
piecewise	O
cubic	O
.	O
extending	O
the	O
quadratic	O
example	O
in	O
the	O
text	O
,	O
show	O
that	O
the	O
vc	B
dimension	I
of	O
a8	O
is	O
infinite	O
for	O
some	O
k	O
in	O
this	O
class	O
that	O
is	O
symmetric	O
,	O
unimodal	O
,	O
positive	O
,	O
and	O
convex	O
on	O
[	O
0	O
,	O
(	O
0	O
)	O
.	O
problem	O
25.12.	O
draw	O
(	O
xl	O
,	O
yd	O
,	O
...	O
,	O
(	O
x4	O
'	O
y4	O
)	O
from	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
on	O
rd	O
x	O
{	O
o	O
,	O
i	O
}	O
,	O
where	O
x	O
has	O
a	O
density	O
and	O
1j	O
(	O
x	O
)	O
e	O
(	O
0	O
,	O
1	O
)	O
at	O
all	O
x.	O
find	O
a	O
symmetric	O
unimodal	O
k	O
such	O
that	O
as	O
~	O
{	O
{	O
x	O
•	O
t	O
k	O
(	O
x	O
~	O
x	O
,	O
)	O
(	O
2y	O
,	O
-	O
i	O
)	O
>	O
0	O
}	O
,	O
h	O
>	O
0	O
}	O
has	O
vc	B
dimension	I
satisfying	O
p	O
{	O
v	O
~	O
=	O
oo	O
}	O
>	O
0.	O
can	O
you	O
find	O
such	O
a	O
kernel	O
k	O
with	O
a	O
bounded	O
support	B
?	O
problem	O
25.13.	O
let	O
x	O
have	O
a	O
density	O
f	O
on	O
rd	O
and	O
let	O
xl	O
,	O
...	O
,	O
xn	O
bei.i.d.	O
,	O
drawn	O
from	O
f.	O
show	O
that	O
lim	O
liminfp	O
{	O
min	O
iixi	O
-	O
xjlld	O
:	O
s	O
c2	O
n	O
c	O
--	O
700	O
n	O
--	O
7oo	O
11	O
]	O
}	O
=	O
1.	O
apply	O
this	O
result	O
in	O
the	O
following	O
situation	O
:	O
define	O
hn	O
=	O
mini	O
,	O
j	O
:	O
y	O
;	O
=l	O
,	O
yj'=o	O
iixi	O
-	O
xi	O
ii	O
,	O
where	O
(	O
xl	O
,	O
yd	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
are	O
i.i.d	O
.	O
rd	O
x	O
{	O
o	O
,	O
1	O
}	O
-valued	O
random	O
variables	O
distributed	O
as	O
(	O
x	O
,	O
y	O
)	O
,	O
with	O
x	O
absolutely	O
continuous	O
,	O
y	O
independent	O
of	O
x	O
,	O
and	O
p	O
{	O
y	O
=	O
i	O
}	O
e	O
(	O
0	O
,	O
1	O
)	O
.	O
show	O
that	O
lim	O
lim	O
inf	O
p	O
{	O
h	O
:	O
:s	O
c	O
n	O
2	O
}	O
=	O
1.	O
conclude	O
that	O
nhd	O
--	O
-+	O
°	O
in	O
probability	O
.	O
if	O
you	O
have	O
a	O
kernel	O
rule	B
with	O
kernel	B
so,1	O
on	O
r	O
d	O
and	O
if	O
the	O
smoothing	B
factor	I
hn	O
is	O
random	O
but	O
satisfies	O
nh~	O
--	O
-+	O
°	O
in	O
probability	O
,	O
then	O
c	O
--	O
700	O
n	O
--	O
7oo	O
,	O
lim	O
e	O
{	O
l	O
n	O
}	O
=	O
pry	O
=	O
1	O
}	O
n	O
--	O
7oo	O
whenever	O
x	O
has	O
a	O
density	O
.	O
show	O
this	O
.	O
problem	O
25.14.	O
consider	O
the	O
variable	B
kernel	O
rule	B
based	O
upon	O
the	O
variable	B
kernel	O
density	O
estimate	O
of	O
breiman	O
,	O
meisel	O
,	O
and	O
purcell	O
(	O
1977	O
)	O
and	O
studied	O
by	O
krzyzak	O
(	O
1983	O
)	O
gn	O
(	O
x	O
)	O
=	O
{	O
i	O
°	O
otherwise	O
.	O
if	O
li	O
:	O
y	O
;	O
.=1	O
kh	O
;	O
(	O
x	O
-	O
xi	O
)	O
>	O
li	O
:	O
y	O
;	O
=o	O
kh	O
;	O
(	O
x	O
-	O
xi	O
)	O
448	O
25.	O
automatic	B
kernel	O
rules	O
here	O
k	O
is	O
a	O
positive-valued	O
kernel	B
,	O
ku	O
=	O
(	O
l/ud	O
)	O
k	O
(	O
x/u	O
)	O
for	O
u	O
>	O
0	O
,	O
and	O
hi	O
is	O
the	O
distance	B
between	O
xi	O
and	O
the	O
k-th	O
nearest	B
neighbor	I
of	O
xi	O
among	O
xj	O
,	O
j	O
ii	O
,	O
1	O
:	O
:	O
:	O
:	O
j	O
:	O
:	O
:	O
:	O
n.	O
investigate	O
the	O
consistency	B
of	O
this	O
rule	B
when	O
k	O
/	O
n	O
--	O
+	O
0	O
and	O
k	O
--	O
+	O
00	O
and	O
x	O
1	O
has	O
a	O
density	O
.	O
problem	O
25.15.	O
continuation	O
.	O
fix	O
k	O
=	O
1	O
,	O
and	O
let	O
k	O
be	O
the	O
normal	B
density	O
in	O
nd	O
.	O
if	O
xl	O
has	O
a	O
density	O
,	O
what	O
can	O
you	O
say	O
about	O
the	O
asymptotic	O
probability	O
of	O
error	O
of	O
the	O
variable	B
kernel	O
rule	B
?	O
is	O
the	O
inequality	B
of	O
cover	O
and	O
hart	O
still	O
valid	O
?	O
repeat	O
the	O
exercise	O
for	O
the	O
uniform	B
kernel	O
on	O
the	O
unit	O
ball	O
of	O
nd	O
.	O
problem	O
25.16.	O
if	O
x	O
1	O
,	O
...	O
,	O
xl	O
)	O
are	O
discrete	O
i.i.d	O
.	O
random	O
variables	O
and	O
n	O
denotes	O
the	O
num	O
(	O
cid:173	O
)	O
ber	O
of	O
different	O
values	O
taken	O
by	O
x	O
i	O
,	O
...	O
,	O
x	O
n	O
,	O
then	O
.	O
e	O
{	O
n	O
}	O
hm	O
-	O
-	O
=0	O
,	O
17	O
--	O
+00	O
n	O
and	O
n	O
/	O
n	O
--	O
+	O
0	O
with	O
probability	O
one	O
.	O
hint	O
:	O
for	O
the	O
weak	B
convergence	O
,	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
the	O
probabilities	O
are	O
monotone	O
.	O
for	O
the	O
strong	B
convergence	O
,	O
use	O
mcdiarmid	O
's	O
inequality	B
.	O
problem	O
25.17.	O
let	O
b	O
be	O
the	O
class	O
of	O
all	O
borel	O
subsets	O
of	O
nd	O
.	O
using	O
the	O
previous	O
exercise	O
,	O
show	O
that	O
for	O
any	O
discrete	O
distribution	B
,	O
sup	O
ijln	O
(	O
a	O
)	O
-	O
jl	O
(	O
a	O
)	O
i	O
--	O
+	O
0	O
with	O
probability	O
one	O
.	O
aeb	O
hint	O
:	O
recall	O
the	O
necessary	O
and	O
sufficient	O
condition	O
e	O
{	O
log	O
n	O
a	O
(	O
x	O
1	O
,	O
...	O
,	O
xn	O
)	O
}	O
/	O
n	O
--	O
+	O
0	O
from	O
chapter	O
12.	O
problem	O
25.18.	O
prove	O
theorem	B
25.8	O
allowing	O
kernel	B
functions	O
taking	O
finitely	O
many	O
dif	O
(	O
cid:173	O
)	O
ferent	O
values	O
.	O
problem	O
25.19.	O
let	O
x	O
i	O
,	O
...	O
,	O
xn	O
be	O
an	O
li.d	O
.	O
sample	O
drawn	O
from	O
the	O
distribution	B
of	O
x.	O
let	O
xfn	O
denote	O
the	O
nearest	B
neighbor	I
of	O
xj	O
among	O
xl	O
,	O
...	O
,	O
x	O
j	O
-	O
1	O
,	O
x	O
j	O
+1	O
,	O
•••	O
,	O
x	O
n	O
.	O
define	O
bn	O
=	O
max	O
iix	O
}	O
-	O
xfnii·	O
j	O
.	O
(	O
1	O
)	O
show	O
that	O
for	O
all	O
nonatomic	O
distributions	O
of	O
x	O
,	O
nb~	O
--	O
+	O
00	O
with	O
probability	O
one	O
.	O
is	O
it	O
true	O
that	O
for	O
every	O
x	O
with	O
a	O
density	O
,	O
there	O
exists	O
a	O
constant	O
c	O
>	O
0	O
such	O
that	O
(	O
2	O
)	O
with	O
probability	O
one	O
,	O
nb	O
,	O
~	O
:	O
:	O
:	O
clog	O
n	O
for	O
all	O
n	O
large	O
enough	O
?	O
(	O
3	O
)	O
exhibit	O
distributions	O
on	O
the	O
real	O
line	O
for	O
which	O
bn	O
--	O
+	O
00	O
with	O
probability	O
one	O
.	O
hint	O
:	O
look	O
at	O
the	O
difference	O
between	O
the	O
first	O
and	O
second	O
order	B
statistics	I
.	O
problem	O
25.20.	O
prove	O
theorem	B
25.9.	O
hint	O
:	O
use	O
the	O
example	O
given	O
in	O
the	O
text	O
.	O
get	O
an	O
upper	O
bound	O
for	O
the	O
error	O
probability	O
corresponding	O
to	O
h	O
=	O
1	O
by	O
hoeffding	O
's	O
inequality	B
.	O
the	O
mean	O
integrated	O
squared	B
error	I
can	O
be	O
computed	O
for	O
every	O
h	O
in	O
a	O
straightforward	O
way	O
by	O
observing	O
that	O
split	O
the	O
integral	O
between	O
0	O
and	O
1	O
in	O
three	O
parts	O
,	O
0	O
to	O
h	O
,	O
h	O
to	O
1	O
-	O
h	O
,	O
and	O
1	O
-	O
h	O
to	O
1.	O
setting	O
the	O
derivative	O
of	O
the	O
obtained	O
expression	B
with	O
respect	O
to	O
h	O
equal	O
to	O
zero	O
leads	O
to	O
a	O
third-order	O
problems	O
and	O
exercises	O
449	O
4	O
/	O
)	O
.	O
to	O
get	O
a	O
lower	O
bound	O
for	O
the	O
corresponding	O
error	O
equation	O
in	O
h	O
,	O
whose	O
roots	O
are	O
o	O
(	O
n-	O
i	O
probability	O
,	O
use	O
the	O
crude	O
bound	O
iii	O
i-h	O
e	O
{	O
ln	O
(	O
h	O
)	O
}	O
2	O
:	O
-	O
2	O
p	O
{	O
g/1	O
(	O
x	O
)	O
i	O
yix	O
=	O
x	O
}	O
dx	O
.	O
now	O
,	O
estimate	B
the	O
tail	O
of	O
a	O
binomial	O
distribution	B
from	O
below	O
;	O
and	O
use	O
stirling	O
's	O
formula	O
to	O
show	O
that	O
,	O
modulo	O
polynomial	B
factors	O
,	O
the	O
error	O
probability	O
is	O
larger	O
than	O
2-/13/4	O
.	O
26	O
automatic	B
nearest	O
neighbor	O
rules	O
the	O
error	O
probability	O
of	O
the	O
k-nearest	O
neighbor	B
rule	I
converges	O
to	O
the	O
bayes	O
risk	O
for	O
all	O
distributions	O
when	O
k	O
--	O
--	O
'7	O
00	O
,	O
and	O
kin	O
--	O
-+	O
0	O
as	O
n	O
--	O
--	O
'7	O
00.	O
the	O
convergence	O
result	O
is	O
extended	O
here	O
to	O
include	O
data-dependent	B
choices	O
of	O
k.	O
we	O
also	O
look	O
at	O
the	O
data-based	B
selection	O
of	O
a	O
metric	O
and	O
of	O
weights	O
in	O
weighted	O
nearest	B
neighbor	I
rules	I
.	O
to	O
keep	O
the	O
notation	O
consistent	O
with	O
that	O
of	O
earlier	O
chapters	O
,	O
random	O
(	O
data-based	B
)	O
values	O
of	O
k	O
are	O
denoted	O
by	O
kn	O
.	O
in	O
most	O
instances	O
,	O
kn	O
is	O
merely	O
a	O
function	O
of	O
dn	O
,	O
the	O
data	O
sequence	O
(	O
xl	O
,	O
yd	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
.	O
the	O
reader	O
should	O
not	O
confuse	O
kn	O
with	O
the	O
kernel	B
k	O
in	O
other	O
chapters	O
.	O
26.1	O
consistency	B
we	O
start	O
with	O
a	O
general	O
theorem	B
assessing	O
strong	B
consistency	I
of	O
the	O
k-nearest	O
neighbor	B
rule	I
with	O
data-dependent	B
choices	O
of	O
k.	O
for	O
the	O
sake	O
of	O
simplicity	O
,	O
we	O
assume	O
the	O
existence	O
of	O
the	O
density	O
of	O
x.	O
the	O
general	O
case	O
can	O
be	O
taken	O
care	O
of	O
by	O
introducing	O
an	O
appropriate	O
tie-breaking	O
method	O
as	O
in	O
chapter	O
11.	O
theorem	B
26.1.	O
let	O
k	O
1	O
,	O
k	O
2	O
,	O
...	O
be	O
integer	O
valued	O
random	O
variables	O
,	O
and	O
let	O
gn	O
be	O
the	O
kn-nearest	O
neighbor	B
rule	I
.	O
if	O
x	O
has	O
a	O
density	O
,	O
and	O
kn	O
--	O
--	O
'7	O
00	O
and	O
kn/n	O
--	O
--	O
'7	O
0	O
with	O
probability	O
one	O
as	O
n	O
--	O
--	O
'7	O
00	O
,	O
then	O
l	O
(	O
gn	O
)	O
--	O
--	O
'7	O
l	O
*	O
with	O
probability	O
one	O
.	O
452	O
26.	O
automatic	B
nearest	O
neighbor	O
rules	O
proof	O
.	O
l	O
(	O
g	O
,	O
j	O
~	O
l	O
*	O
with	O
probability	O
one	O
if	O
and	O
only	O
if	O
for	O
every	O
e	O
>	O
o	O
,	O
i	O
{	O
l	O
(	O
gn	O
)	O
-u	O
>	O
e	O
)	O
~	O
0	O
with	O
probability	O
one	O
.	O
clearly	O
,	O
for	O
any	O
f3	O
>	O
0	O
,	O
we	O
are	O
done	O
if	O
both	O
random	O
variables	O
on	O
the	O
right-hand	O
side	O
converge	O
to	O
zero	O
with	O
probability	O
one	O
.	O
the	O
convergence	O
of	O
the	O
second	O
term	O
for	O
all	O
f3	O
>	O
0	O
follows	O
trivially	O
from	O
the	O
conditions	O
of	O
the	O
theorem	B
.	O
the	O
convergence	O
of	O
the	O
first	O
term	O
follows	O
from	O
the	O
remark	O
following	O
theorem	B
11.1	O
,	O
which	O
states	O
that	O
for	O
any	O
e	O
>	O
0	O
,	O
there	O
exist	O
f3	O
>	O
0	O
and	O
no	O
such	O
that	O
for	O
the	O
error	O
probability	O
ln	O
,	O
k	O
of	O
the	O
k-nearest	O
neighbor	B
rule	I
,	O
p	O
{	O
l	O
n	O
,	O
k	O
-	O
l	O
*	O
>	O
e	O
}	O
<	O
4e-cne2	O
_	O
for	O
some	O
constant	O
c	O
depending	O
on	O
the	O
dimension	B
only	O
,	O
provided	O
that	O
n	O
>	O
no	O
,	O
k	O
>	O
i/f3	O
and	O
kin	O
<	O
f3	O
.	O
now	O
clearly	O
,	O
p	O
{	O
l	O
(	O
gn	O
)	O
-	O
l	O
*	O
>	O
e	O
,	O
ii	O
kn	O
+	O
knln	O
<	O
2f3	O
}	O
<	O
p	O
{	O
sup	O
ln	O
,	O
k	O
-	O
l	O
*	O
>	O
e	O
}	O
l/f3~k~nf3	O
<	O
n	O
sup	O
p	O
{	O
ln	O
,	O
k	O
-	O
l*	O
>	O
e	O
}	O
,	O
l/f3~k~nf3	O
by	O
the	O
union	O
bound	O
.	O
combining	O
this	O
with	O
theorem	O
11.1	O
,	O
we	O
get	O
p	O
{	O
l	O
(	O
gn	O
)	O
-	O
l	O
*	O
>	O
e	O
,	O
ii	O
kn	O
+	O
knln	O
<	O
2f3	O
}	O
:	O
:	O
:	O
:	O
4ne-cne2	O
,	O
n	O
~	O
no	O
.	O
the	O
borel-cantelli	O
lemma	O
implies	O
that	O
with	O
probability	O
one	O
,	O
and	O
the	O
theorem	B
is	O
proved	O
.	O
0	O
sometimes	O
we	O
only	O
know	O
that	O
kn	O
~	O
00	O
and	O
knln	O
~	O
0	O
in	O
probability	O
.	O
in	O
such	O
cases	O
weak	B
consistency	O
is	O
guaranteed	O
.	O
the	O
proof	O
is	O
left	O
as	O
an	O
exercise	O
(	O
problem	O
26.1	O
)	O
.	O
theorem	B
26.2.	O
let	O
ki	O
,	O
k2	O
,	O
...	O
be	O
integer	O
valued	O
random	O
variables	O
,	O
and	O
let	O
gn	O
be	O
the	O
kn	O
-nearest	O
neighbor	B
rule	I
.	O
if	O
x	O
has	O
a	O
density	O
,	O
and	O
kn	O
~	O
00	O
and	O
knln	O
~	O
0	O
in	O
probability	O
as	O
n	O
~	O
00	O
,	O
then	O
limn	O
--	O
+	O
oo	O
el	O
(	O
gn	O
)	O
=	O
l	O
*	O
,	O
that	O
is	O
,	O
gn	O
is	O
weakly	O
consistent	O
.	O
26.2	O
data	O
splitting	O
consistency	O
by	O
itself	O
may	O
be	O
obtained	O
by	O
choosing	O
k	O
=	O
l	O
fn	O
j	O
,	O
but	O
few-if	O
any	O
(	O
cid:173	O
)	O
users	O
will	O
want	O
to	O
blindly	O
use	O
such	O
recipes	O
.	O
instead	O
,	O
a	O
healthy	O
dose	O
of	O
feed	O
(	O
cid:173	O
)	O
back	O
from	O
the	O
data	O
is	O
preferable	O
.	O
if	O
we	O
proceed	O
as	O
in	O
chapter	O
22	O
,	O
we	O
may	O
split	O
26.3	O
data	O
splitting	O
for	O
weighted	B
nn	O
rules	O
453	O
the	O
data	O
sequence	O
dn	O
=	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
into	O
a	O
training	O
sequence	O
dm	O
=	O
(	O
xl	O
,	O
yd	O
,	O
...	O
,	O
(	O
xm	O
,	O
ym	O
)	O
,	O
and	O
a	O
testing	O
sequence	O
tz	O
=	O
(	O
xm+l	O
,	O
ym+l	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
,	O
where	O
m	O
+	O
i	O
=	O
n.	O
the	O
training	O
sequence	O
dm	O
is	O
used	O
to	O
design	O
a	O
class	O
of	O
classifiers	O
em	O
.	O
the	O
testing	B
sequence	I
is	O
used	O
to	O
select	O
a	O
classifier	O
from	O
em	O
that	O
minimizes	O
the	O
holdout	B
estimate	O
of	O
the	O
error	O
probability	O
,	O
if	O
em	O
contains	O
all	O
k-nearest	O
neighbor	O
rules	O
with	O
i	O
~	O
k	O
~	O
m	O
,	O
then	O
,	O
em	O
'	O
=	O
m.	O
therefore	O
,	O
we	O
have	O
where	O
gn	O
is	O
the	O
selected	O
k-nearest	O
neighbor	B
rule	I
.	O
by	O
combining	O
theorems	O
11.1	O
and	O
22.1	O
,	O
we	O
immediately	O
deduce	O
that	O
gn	O
is	O
universally	O
consistent	O
if	O
lim	O
m	O
=	O
00	O
,	O
n	O
--	O
-+oo	O
i	O
.	O
1	O
1m	O
-	O
-	O
=00	O
.	O
n	O
--	O
-+oo	O
log	O
m	O
it	O
is	O
strongly	O
universally	O
consistent	O
if	O
limn	O
--	O
-+oo	O
1/	O
log	O
n	O
=	O
00	O
also	O
.	O
note	O
too	O
that	O
e	O
{	O
il	O
(	O
gn	O
)	O
-	O
inf	O
l	O
(	O
¢m	O
)	O
11	O
~	O
2	O
<	O
p	O
''	O
,	O
ecm	O
log	O
(	O
2m	O
)	O
+	O
1	O
21	O
(	O
by	O
problem	O
12.1	O
)	O
,	O
so	O
that	O
it	O
is	O
indeed	O
important	O
to	O
pick	O
i	O
much	O
larger	O
than	O
log	O
m.	O
26.3	O
data	O
splitting	O
for	O
weighted	B
nn	O
rules	O
royall	O
(	O
1966	O
)	O
introduced	O
the	O
weighted	B
nn	O
rule	B
in	O
which	O
the	O
i-th	O
nearest	B
neighbor	I
receives	O
weight	O
wi	O
,	O
where	O
wi	O
~	O
w2	O
~	O
.•	O
.	O
~	O
wk	O
~	O
0	O
and	O
the	O
wi	O
's	O
sum	O
to	O
one	O
.	O
we	O
assume	O
that	O
wk+l	O
=	O
...	O
=	O
wn	O
=	O
0	O
if	O
there	O
are	O
n	O
data	O
points	O
.	O
besides	O
the	O
natural	O
appeal	O
of	O
attaching	O
more	O
weight	O
to	O
nearer	O
neighbors	O
,	O
there	O
is	O
also	O
a	O
practical	O
by	O
(	O
cid:173	O
)	O
product	B
:	O
if	O
the	O
wi	O
's	O
are	O
all	O
of	O
the	O
form	O
1/	O
zi	O
,	O
where	O
zi	O
is	O
a	O
prime	O
integer	O
,	O
then	O
no	O
two	O
subsums	O
of	O
wi	O
's	O
are	O
equal	O
,	O
and	O
therefore	O
voting	O
ties	O
are	O
avoided	O
altogether	O
.	O
consider	O
now	O
data	O
splitting	O
in	O
which	O
em	O
consists	O
of	O
all	O
weighted	B
k-nn	O
rules	O
as	O
described	O
above-clearly	O
,	O
k	O
~	O
m	O
now	O
.	O
as	O
lem	O
i	O
=	O
00	O
,	O
we	O
compute	O
the	O
shatter	O
coefficients	O
seem	O
,	O
i	O
)	O
.	O
we	O
claim	O
that	O
if	O
i	O
:	O
:	O
:	O
k	O
if	O
i	O
<	O
k.	O
(	O
26.1	O
)	O
this	O
result	O
is	O
true	O
even	O
if	O
we	O
do	O
not	O
insist	O
that	O
wi	O
~	O
w2	O
~	O
...	O
~	O
wk	O
~	O
o.	O
proof	O
of	O
(	O
26.1	O
)	O
.	O
each	O
xj	O
in	O
the	O
testing	B
sequence	I
is	O
classified	O
based	O
upon	O
the	O
sign	O
of	O
l~=l	O
aij	O
wi	O
,	O
where	O
aij	O
e	O
{	O
-i	O
,	O
i	O
}	O
depends	O
upon	O
the	O
class	O
of	O
the	O
i	O
-th	O
nearest	O
454	O
26.	O
,	O
automatic	B
nearest	O
neighbor	O
rules	O
neighbor	O
of	O
xj	O
among	O
xl	O
,	O
...	O
,	O
xm	O
(	O
and	O
does	O
not	O
depend	O
upon	O
the	O
wi	O
's	O
)	O
.	O
consider	O
the	O
i-vector	O
of	O
signs	O
of	O
2	O
:	O
=	O
:	O
=1	O
aijwi	O
,	O
m	O
<	O
j	O
s	O
n.	O
in	O
the	O
computation	O
of	O
seem	O
,	O
1	O
)	O
,	O
we	O
consider	O
the	O
aij	O
's	O
as	O
fixed	O
numbers	O
,	O
and	O
vary	O
the	O
wi	O
's	O
subject	O
to	O
the	O
condition	O
laid	O
out	O
above	O
.	O
here	O
is	O
the	O
crucial	O
step	O
in	O
the	O
argument	O
:	O
the	O
collection	O
of	O
all	O
vectors	O
(	O
wi	O
,	O
...	O
,	O
wk	O
)	O
for	O
which	O
xj	O
is	O
assigned	O
to	O
class	O
1	O
is	O
a	O
linear	O
halfspace	O
of	O
rk	O
.	O
therefore	O
,	O
seem	O
,	O
l	O
)	O
is	O
bounded	O
from	O
above	O
by	O
the	O
number	O
of	O
cells	O
in	O
the	O
partition	B
of	O
rk	O
defined	O
by	O
llinear	O
halfspaces	O
.	O
this	O
is	O
bounded	O
by	O
2	O
:	O
=	O
:	O
=1	O
g	O
)	O
(	O
see	O
problem	O
22.1	O
)	O
if	O
k	O
s	O
l.	O
0	O
let	O
gn	O
be	O
the	O
rule	B
in	O
lem	O
i	O
that	O
minimizes	O
the	O
empirical	B
error	I
committed	O
on	O
the	O
test	O
sequence	O
(	O
xm+1	O
,	O
ym+1	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
.	O
then	O
by	O
(	O
22.1	O
)	O
,	O
if	O
[	O
=	O
n	O
-	O
m	O
2	O
:	O
k	O
,	O
we	O
have	O
--	O
-+	O
(	O
x	O
)	O
(	O
which	O
implies	O
m	O
--	O
-+	O
(	O
0	O
)	O
,	O
we	O
have	O
universal	B
consistency	I
when	O
if	O
k	O
k	O
log	O
(	O
l	O
)	O
/	O
l	O
--	O
-+	O
o.	O
the	O
estimation	B
error	I
is	O
of	O
the	O
order	O
of	O
jk	O
log	O
1/	O
[	O
-in	O
the	O
termi	O
(	O
cid:173	O
)	O
nology	O
of	O
chapter	O
22	O
,	O
the	O
rule	B
is	O
j	O
k	O
log	O
l/	O
i-optimal	O
.	O
this	O
error	O
must	O
be	O
weighed	O
against	O
the	O
unknown	O
approximation	B
error	I
.	O
let	O
us	O
present	O
a	O
quick	O
heuristic	O
argu	O
(	O
cid:173	O
)	O
ment	O
.	O
on	O
the	O
real	O
line	O
,	O
there	O
is	O
compelling	O
evidence	O
to	O
suggest	O
that	O
when	O
x	O
has	O
a	O
smooth	O
density	O
,	O
k	O
=	O
cm4	O
/	O
5	O
is	O
nearly	O
optimal	O
.	O
with	O
this	O
choice	O
,	O
if	O
both	O
1	O
and	O
m	O
grow	O
linearly	O
in	O
n	O
,	O
the	O
estimation	B
error	I
is	O
of	O
the	O
order	O
of	O
jlog	O
n/	O
n	O
1/1o	O
.	O
this	O
is	O
painfully	O
large-to	O
reduce	O
this	O
error	O
by	O
a	O
factor	O
of	O
two	O
,	O
sample	O
sizes	O
must	O
rise	O
by	O
a	O
factor	O
of	O
about	O
1000.	O
the	O
reason	O
for	O
this	O
disappointing	O
result	O
is	O
that	O
em	O
is	O
just	O
too	O
rich	O
for	O
the	O
values	O
of	O
k	O
that	O
interest	O
us	O
.	O
automatic	B
selection	O
may	O
lead	O
to	O
rules	O
that	O
overfit	O
the	O
data	O
.	O
if	O
we	O
restrict	O
em	O
by	O
making	O
m	O
very	O
small	O
,	O
the	O
following	O
rough	O
argument	O
may	O
be	O
used	O
to	O
glean	O
information	O
about	O
the	O
size	O
of	O
m	O
and	O
i	O
=	O
n	O
-	O
m.	O
we	O
will	O
take	O
k	O
=	O
m	O
«	O
n.	O
for	O
smooth	O
regression	B
function	I
1	O
]	O
,	O
the	O
estimation	B
error	I
may	O
be	O
anywhere	O
between	O
m	O
-2/5	O
and	O
m	O
-4/5	O
on	O
the	O
real	O
line	O
.	O
as	O
the	O
estimation	B
error	I
is	O
of	O
the	O
order	O
of	O
jmlogn/n	O
,	O
equating	O
the	O
errors	O
leads	O
to	O
the	O
rough	O
recipe	O
that	O
m	O
~	O
(	O
n/logn	O
)	O
5/9	O
,	O
and	O
m	O
~	O
(	O
n/logn	O
)	O
5/13	O
,	O
respectively	O
.	O
both	O
errors	O
are	O
then	O
about	O
(	O
n/logn	O
)	O
-2/9	O
and	O
(	O
n/logn	O
)	O
-4/9	O
,	O
respectively	O
.	O
this	O
is	O
better	O
than	O
with	O
the	O
previous	O
example	O
with	O
m	O
linear	O
in	O
n.	O
unfortunately	O
,	O
it	O
is	O
difficult	O
to	O
test	O
whether	O
the	O
conditions	O
on	O
1	O
]	O
that	O
guarantee	O
certain	O
errors	O
are	O
satisfied	O
.	O
the	O
above	O
procedure	O
is	O
thus	O
doomed	O
to	O
remain	O
heuristic	O
.	O
26.4	O
reference	O
data	O
and	O
data	O
splitting	O
split	O
the	O
data	O
into	O
dm	O
and	O
tz	O
as	O
is	O
done	O
in	O
the	O
previous	O
section	O
.	O
let	O
em	O
contain	O
alli-nn	O
rules	O
that	O
are	O
based	O
upon	O
the	O
data	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xb	O
yk	O
)	O
,	O
where	O
k	O
s	O
m	O
is	O
to	O
be	O
picked	O
,	O
{	O
xl	O
,	O
...	O
,	O
xk	O
}	O
c	O
{	O
xl	O
,	O
'	O
''	O
xm	O
}	O
,	O
and	O
{	O
yi	O
,	O
...	O
,	O
yk	O
}	O
e	O
{	O
o	O
,	O
i	O
}	O
k.	O
note	O
that	O
because	O
the	O
y/s	O
are	O
free	O
parameters	O
,	O
{	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xb	O
yk	O
)	O
}	O
is	O
not	O
necessarily	O
a	O
subset	O
of	O
{	O
(	O
x	O
i	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xm	O
,	O
ym	O
)	O
}	O
-this	O
allows	O
us	O
to	O
flip	O
certain	O
y-values	O
at	O
some	O
data	O
points	O
.	O
trivially	O
,	O
lem	O
i	O
=	O
g	O
)	O
2k	O
.	O
hence	O
,	O
26.5	O
variable	B
metric	I
nn	O
rules	O
455	O
where	O
i	O
=	O
n	O
-	O
m	O
,	O
and	O
gn	O
e	O
em	O
minimizes	O
the	O
empirical	B
error	I
on	O
the	O
test	O
sequence	O
tz	O
.	O
the	O
best	O
rule	B
in	O
em	O
is	O
universally	O
consistent	O
when	O
k	O
-+	O
00	O
(	O
see	O
theorem	B
19.4	O
)	O
.	O
therefore	O
,	O
gn	O
is	O
universally	O
consistent	O
when	O
the	O
above	O
bound	O
converges	O
to	O
zero	O
.	O
sufficient	O
conditions	O
are	O
11-+00	O
lim	O
i	O
=	O
00	O
;	O
klogm	O
lim	O
-	O
-	O
=	O
o.	O
n-+oo	O
i	O
(	O
i	O
)	O
(	O
ii	O
)	O
as	O
the	O
estimation	B
error	I
is	O
0	O
(	O
j	O
k	O
log	O
m	O
/	O
i	O
)	O
,	O
it	O
is	O
important	O
to	O
make	O
i	O
large	O
,	O
while	O
keeping	O
k	O
small	O
.	O
the	O
selected	O
sequence	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xb	O
yk	O
)	O
may	O
be	O
called	O
reference	O
data	O
,	O
as	O
it	O
captures	O
the	O
information	O
in	O
the	O
larger	O
data	O
set	O
.	O
if	O
k	O
is	O
sufficiently	O
small	O
,	O
the	O
computation	O
of	O
gn	O
(	O
x	O
)	O
is	O
extremely	O
fast	O
.	O
the	O
idea	O
of	O
selecting	O
reference	O
data	O
or	O
throwing	O
out	O
useless	O
or	O
``	O
bad	O
''	O
data	O
points	O
has	O
been	O
proposed	O
and	O
studied	O
by	O
many	O
researchers	O
under	O
names	O
such	O
as	O
condensed	B
nn	O
rules	O
,	O
edited	B
nn	O
rules	O
,	O
and	O
selective	B
nn	O
rules	O
.	O
see	O
hart	O
(	O
1968	O
)	O
,	O
gates	O
(	O
1972	O
)	O
,	O
wilson	O
(	O
1972	O
)	O
,	O
wagner	O
(	O
1973	O
)	O
,	O
ullmann	O
(	O
1974	O
)	O
,	O
ritter	O
et	O
al	O
.	O
(	O
1975	O
)	O
,	O
tomek	O
(	O
1976b	O
)	O
,	O
and	O
devijver	O
and	O
kittler	O
(	O
1980	O
)	O
.	O
see	O
also	O
section	O
19.1	O
.	O
26.5	O
variable	B
metric	I
nn	O
rules	O
the	O
data	O
may	O
also	O
be	O
used	O
to	O
select	O
a	O
suitable	O
metric	B
for	O
use	O
with	O
the	O
k-nn	O
rule	B
.	O
the	O
metric	B
adapts	O
itself	O
for	O
certain	O
scale	O
information	O
gleaned	O
from	O
the	O
data	O
.	O
for	O
example	O
,	O
we	O
may	O
compute	O
the	O
distance	B
between	O
xl	O
and	O
x2	O
by	O
the	O
formula	O
''	O
at	O
(	O
xl	O
-	O
x2	O
)	O
11	O
(	O
(	O
xl	O
-	O
x2l	O
aat	O
(	O
xl	O
-	O
x2	O
)	O
)	O
1/2	O
=	O
(	O
(	O
xl	O
-	O
x2	O
)	O
t	O
l	O
:	O
(	O
xi	O
-	O
x2	O
»	O
)	O
1/2	O
,	O
where	O
(	O
xl	O
-	O
x2	O
)	O
is	O
a	O
column	O
vector	O
,	O
(	O
·l	O
denotes	O
its	O
transpose	O
,	O
a	O
is	O
a	O
d	O
x	O
d	O
transformation	O
matrix	O
,	O
and	O
l	O
:	O
=	O
aa	O
t	O
is	O
a	O
positive	O
definite	O
matrix	O
.	O
the	O
elements	O
of	O
a	O
or	O
l	O
:	O
may	O
be	O
determined	O
from	O
the	O
data	O
according	O
to	O
some	O
heuristic	O
formulas	O
.	O
we	O
refer	O
to	O
fukunaga	O
and	O
hostetler	O
(	O
1973	O
)	O
,	O
short	O
and	O
fukunaga	O
(	O
1981	O
)	O
,	O
fukunaga	O
and	O
flick	O
(	O
1984	O
)	O
,	O
and	O
myles	O
and	O
hand	O
(	O
1990	O
)	O
for	O
more	O
information	O
.	O
for	O
example	O
,	O
the	O
object	O
of	O
principal	O
component	O
analysis	O
is	O
to	O
find	O
a	O
transforma	O
(	O
cid:173	O
)	O
tion	O
matrix	O
a	O
such	O
that	O
the	O
components	O
of	O
the	O
vector	O
a	O
t	O
x	O
have	O
unit	O
variance	O
and	O
are	O
uncorrelated	O
.	O
these	O
methods	O
are	O
typically	O
based	O
on	O
estimating	O
the	O
eigenvalues	O
of	O
the	O
covariance	O
matrix	O
of	O
x.	O
for	O
such	O
situations	O
,	O
we	O
prove	O
the	O
following	O
general	O
consistency	B
result	O
:	O
456	O
26.	O
automatic	B
nearest	O
neighbor	O
rules	O
theorem	B
26.3.	O
let	O
the	O
random	O
metric	B
pn	O
be	O
of	O
the	O
form	O
pn	O
(	O
x	O
,	O
y	O
)	O
=	O
iia~	O
(	O
x	O
-	O
y	O
)	O
li	O
,	O
where	O
the	O
matrix	O
an	O
is	O
a	O
function	O
of	O
xl	O
,	O
...	O
,	O
x	O
n	O
.	O
assume	O
that	O
distance	B
ties	O
occur	O
with	O
zero	O
probability	O
,	O
and	O
there	O
are	O
two	O
sequences	O
of	O
nonnegative	O
random	O
variables	O
{	O
mn	O
}	O
and	O
{	O
mn	O
}	O
such	O
thatfor	O
any	O
n	O
and	O
x	O
,	O
y	O
end	O
,	O
and	O
if	O
p	O
{	O
liminf	O
mn	O
=	O
o	O
}	O
=	O
o.	O
n-+oo	O
mn	O
lim	O
kn	O
=	O
00	O
and	O
n-+oo	O
kn	O
.	O
hm	O
-	O
=0	O
,	O
n-+oo	O
n	O
then	O
the	O
kn	O
-nearest	O
neighbor	B
rule	I
based	O
on	O
the	O
metric	B
pn	O
is	O
consistent	O
.	O
proof	O
.	O
we	O
verify	O
the	O
three	O
conditions	O
of	O
theorem	O
6.3.	O
in	O
this	O
case	O
,	O
wnjx	O
)	O
=	O
1/	O
kn	O
if	O
xi	O
is	O
one	O
of	O
the	O
kn	O
nearest	O
neighbors	O
of	O
x	O
(	O
according	O
to	O
pn	O
)	O
,	O
and	O
zero	O
otherwise	O
.	O
condition	O
(	O
iii	O
)	O
holds	O
trivially	O
.	O
just	O
as	O
in	O
the	O
proof	O
of	O
consistency	O
of	O
the	O
ordinary	B
kn-	O
nearest	B
neighbor	I
rule	I
,	O
for	O
condition	O
(	O
i	O
)	O
we	O
need	O
the	O
property	O
that	O
the	O
number	O
of	O
data	O
points	O
that	O
can	O
be	O
among	O
the	O
kn	O
nearest	O
neighbors	O
of	O
a	O
particular	O
point	O
is	O
at	O
most	O
kn	O
yd	O
,	O
where	O
the	O
constant	O
yd	O
depends	O
on	O
the	O
dimension	B
only	O
.	O
this	O
is	O
a	O
deterministic	O
property	O
,	O
and	O
it	O
can	O
be	O
proven	O
exactly	O
the	O
same	O
way	O
as	O
for	O
the	O
standard	B
nearest	O
neighbor	B
rule	I
.	O
the	O
only	O
condition	O
of	O
theorem	O
6.3	O
whose	O
justification	O
needs	O
extra	O
work	O
is	O
condition	O
(	O
ii	O
)	O
:	O
we	O
need	O
to	O
show	O
that	O
for	O
any	O
a	O
>	O
0	O
,	O
lim	O
e	O
{	O
~wni	O
(	O
x	O
)	O
!	O
{	O
iix-xill	O
>	O
a	O
}	O
}	O
=	O
o.	O
n-+oo	O
~	O
i=l	O
denote	O
the	O
k-th	O
nearest	B
neighbor	I
of	O
x	O
according	O
to	O
pn	O
by	O
x	O
(	O
k	O
)	O
,	O
and	O
the	O
k-th	O
nearest	B
neighbor	I
of	O
x	O
according	O
to	O
the	O
euclidean	O
metric	B
by	O
x	O
(	O
k	O
)	O
.	O
then	O
e	O
{	O
t	O
wni	O
(	O
x	O
)	O
i	O
{	O
iix-xdl	O
>	O
aj	O
}	O
(	O
since	O
mn	O
iix	O
-	O
yll	O
:	O
:	O
:	O
pn	O
(	O
x	O
,	O
y	O
)	O
)	O
26.6	O
selection	B
of	O
k	O
based	O
on	O
the	O
deleted	B
estimate	O
457	O
<	O
p	O
{	O
mna	O
:	O
:	O
:	O
;	O
pn	O
(	O
x	O
,	O
x	O
(	O
kn	O
)	O
}	O
p	O
{	O
t	O
i	O
{	O
pn	O
(	O
x	O
,	O
x	O
;	O
)	O
:	O
:	O
:	O
'mna	O
}	O
<	O
kn	O
}	O
l=1	O
(	O
since	O
mnllx	O
-	O
y	O
ii	O
:	O
:	O
:	O
:	O
pn	O
(	O
x	O
,	O
y	O
»	O
p	O
{	O
iix-x	O
(	O
dl	O
>	O
a	O
:	O
:	O
}	O
.	O
but	O
we	O
know	O
from	O
the	O
consistency	B
proof	O
of	O
the	O
ordinary	B
kl1	O
-nearest	O
neighbor	B
rule	I
in	O
chapter	O
11	O
that	O
for	O
each	O
a	O
>	O
0	O
,	O
lim	O
p	O
{	O
iix	O
-	O
x	O
(	O
kn	O
)	O
ii	O
>	O
a	O
}	O
=	O
o.	O
l1	O
--	O
hx	O
)	O
it	O
follows	O
from	O
the	O
condition	O
on	O
mn/	O
mn	O
that	O
the	O
probability	O
above	O
converges	O
to	O
zero	O
as	O
well	O
.	O
0	O
the	O
conditions	O
of	O
the	O
theorem	B
hold	O
if	O
,	O
for	O
example	O
,	O
the	O
elements	O
of	O
the	O
matrix	O
an	O
converge	O
to	O
the	O
elements	O
of	O
an	O
invertible	O
matrix	O
a	O
in	O
probability	O
.	O
in	O
that	O
case	O
,	O
we	O
may	O
take	O
mn	O
as	O
the	O
smallest	O
,	O
and	O
ml1	O
as	O
the	O
largest	O
eigenvalues	O
of	O
a~	O
an	O
.	O
then	O
mn/	O
ml1	O
converges	O
to	O
the	O
ratio	O
of	O
the	O
smallest	O
and	O
largest	O
eigenvalues	O
of	O
at	O
a	O
,	O
a	O
positive	O
number	O
.	O
if	O
we	O
pick	O
the	O
elements	O
of	O
a	O
by	O
minimizing	O
the	O
empirical	B
error	I
of	O
a	O
test	O
sequence	O
tz	O
over	O
c'n	O
,	O
where	O
cm	O
contains	O
all	O
k-nn	O
rules	O
based	O
upon	O
a	O
training	O
sequence	O
dm	O
(	O
thus	O
,	O
the	O
elements	O
of	O
a	O
are	O
the	O
free	O
parameters	O
)	O
,	O
the	O
value	O
of	O
s	O
(	O
cm	O
,	O
l	O
)	O
is	O
too	O
large	O
to	O
be	O
useful-see	O
problem	O
26.3.	O
furthermore	O
,	O
such	O
minimization	O
is	O
not	O
computationally	O
feasible	O
.	O
26.6	O
selection	B
of	O
k	O
based	O
on	O
the	O
deleted	B
estimate	O
if	O
you	O
wish	O
to	O
use	O
all	O
the	O
available	O
data	O
in	O
the	O
training	O
sequence	O
,	O
without	O
splitting	O
,	O
then	O
empirical	B
selection	O
based	O
on	O
minimization	O
of	O
other	O
estimates	O
of	O
the	O
error	O
probability	O
may	O
be	O
your	O
solution	O
.	O
unfortunately	O
,	O
performance	O
guarantees	O
for	O
the	O
selected	O
rule	B
are	O
rarely	O
available	O
.	O
if	O
the	O
class	O
of	O
rules	O
is	O
finite	O
,	O
as	O
when	O
k	O
is	O
selected	O
from	O
{	O
i	O
,	O
...	O
,	O
n	O
}	O
,	O
there	O
are	O
useful	O
inequalities	O
.	O
we	O
will	O
show	O
you	O
how	O
this	O
works	O
.	O
458	O
26.	O
automatic	B
nearest	O
neighbor	O
rules	O
let	O
en	O
be	O
the	O
class	O
of	O
all	O
k-nearest	O
neighbor	O
rules	O
based	O
on	O
a	O
fixed	O
training	O
sequence	O
dn	O
,	O
but	O
with	O
k	O
variable	B
.	O
clearly	O
,	O
i	O
en	O
i	O
=	O
n.	O
assume	O
that	O
the	O
deleted	B
estimate	O
is	O
used	O
to	O
pick	O
a	O
classifier	O
gn	O
from	O
en	O
:	O
we	O
can	O
derive	O
performance	O
bounds	O
for	O
gn	O
from	O
theorem	B
24.5.	O
since	O
the	O
result	O
gives	O
poor	O
bounds	O
for	O
large	O
values	O
of	O
k	O
,	O
the	O
range	O
of	O
k	O
's	O
has	O
to	O
be	O
restricted-see	O
the	O
discussion	O
following	O
theorem	B
24.5.	O
let	O
ko	O
denote	O
the	O
value	O
of	O
the	O
largest	O
k	O
allowed	O
,	O
that	O
is	O
,	O
en	O
now	O
contains	O
all	O
k-nearest	O
neighbor	O
rules	O
with	O
k	O
ranging	O
from	O
1	O
to	O
ko	O
.	O
theorem	B
26.4.	O
let	O
gn	O
be	O
the	O
classijierminimizing	O
the	O
deleted	B
estimate	O
of	O
the	O
error	O
probability	O
over	O
en	O
,	O
the	O
class	O
of	O
k-nearest	O
neighbor	O
rules	O
with	O
1	O
:	O
:	O
:	O
:	O
k	O
:	O
:	O
:	O
:	O
ko	O
.	O
then	O
where	O
c	O
is	O
a	O
constant	O
depending	O
on	O
the	O
dimension	B
only	O
.	O
if	O
ko	O
ko	O
log	O
nj	O
n	O
--	O
-7	O
>	O
-	O
0	O
,	O
then	O
gn	O
is	O
strongly	O
universally	O
consistent	O
.	O
--	O
-7	O
>	O
-	O
00	O
and	O
proof	O
.	O
from	O
theorem	B
8.4	O
we	O
recall	O
the	O
inequality	B
now	O
follows	O
from	O
theorem	B
24.5	O
via	O
the	O
union	O
bound	O
.	O
universal	B
consistency	I
follows	O
from	O
the	O
previous	O
inequality	B
,	O
and	O
the	O
fact	O
that	O
the	O
ko-nn	O
rule	B
is	O
strongly	O
universally	O
consistent	O
(	O
see	O
theorem	B
11.1	O
)	O
.	O
0	O
problems	O
and	O
exercises	O
problem	O
26.1.	O
let	O
k	O
i	O
,	O
k	O
2	O
,	O
...	O
be	O
integer	O
valued	O
random	O
variables	O
,	O
and	O
let	O
gn	O
be	O
the	O
kn	O
(	O
cid:173	O
)	O
nearest	B
neighbor	I
rule	I
.	O
show	O
that	O
if	O
x	O
has	O
a	O
density	O
,	O
and	O
kn	O
-+	O
00	O
and	O
kill	O
n	O
-+	O
0	O
in	O
probability	O
as	O
n	O
-+	O
00	O
,	O
then	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
-+	O
l*	O
.	O
problem	O
26.2.	O
let	O
c	O
be	O
the	O
class	O
of	O
all	O
l-nn	O
rules	O
based	O
upon	O
pairs	O
(	O
xi	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xb	O
yk	O
)	O
,	O
where	O
k	O
is	O
a	O
fixed	O
parameter	O
(	O
possibly	O
varying	O
with	O
n	O
)	O
,	O
and	O
the	O
(	O
xi	O
,	O
yi	O
)	O
's	O
are	O
variable	B
pairs	O
from	O
r	O
d	O
x	O
{	O
o	O
,	O
l	O
}	O
.	O
let	O
gn	O
be	O
the	O
rule	B
that	O
minimizes	O
the	O
empirical	B
error	I
over	O
c	O
,	O
or	O
,	O
equivalently	O
,	O
let	O
gn	O
be	O
the	O
rule	B
that	O
minimizes	O
the	O
resubstitution	B
estimate	O
l~r	O
)	O
over	O
c.	O
(	O
1	O
)	O
compute	O
a	O
suitable	O
upper	O
bound	O
for	O
s	O
(	O
c	O
,	O
n	O
)	O
.	O
(	O
2	O
)	O
compute	O
a	O
good	O
upper	O
bound	O
for	O
vc	O
as	O
a	O
function	O
of	O
k	O
and	O
d.	O
(	O
3	O
)	O
if	O
k	O
-+	O
00	O
with	O
n	O
,	O
show	O
that	O
the	O
sequence	O
of	O
classes	O
c	O
contains	O
a	O
strongly	O
um.	O
,	O
versally	O
consistent	O
subsequence	O
of	O
rules	O
(	O
you	O
may	O
assume	O
for	O
convenience	O
that	O
x	O
:	O
has	O
a	O
density	O
to	O
avoid	O
distance	B
ties	O
)	O
.	O
(	O
4	O
)	O
under	O
what	O
condition	O
on	O
k	O
can	O
you	O
guarantee	O
strong	B
universal	I
consistency	I
of	O
gn	O
!	O
(	O
5	O
)	O
give	O
an	O
upper	O
bound	O
for	O
problems	O
and	O
exercises	O
459	O
problem	O
26.3.	O
let	O
cm	O
contain	O
all	O
k-nn	O
rules	O
based	O
upon	O
data	O
pairs	O
(	O
xl	O
,	O
yl	O
)	O
,	O
..•	O
,	O
(	O
xm	O
'	O
ym	O
)	O
.	O
the	O
metric	B
used	O
in	O
computing	O
the	O
neighbors	O
is	O
derived	O
from	O
the	O
norm	O
d	O
/ix	O
ii	O
~	O
=	O
l	O
l	O
(	O
jij	O
x	O
(	O
i	O
)	O
xu	O
)	O
,	O
x	O
=	O
(	O
x	O
o	O
)	O
,	O
...	O
,	O
xed	O
»	O
)	O
,	O
d	O
i=l	O
j=l	O
where	O
{	O
(	O
jij	O
}	O
forms	O
a	O
positive	O
definite	O
matrix	O
2	O
:	O
.	O
the	O
elements	O
of	O
2	O
:	O
are	O
the	O
free	O
parameters	O
in	O
cm	O
.	O
compute	O
upper	O
and	O
lower	B
bounds	I
for	I
seem	O
,	O
i	O
)	O
as	O
a	O
function	O
of	O
m	O
,	O
i	O
,	O
k	O
,	O
and	O
d.	O
problem	O
26.4.	O
let	O
gn	O
be	O
the	O
rule	B
obtained	O
by	O
minimizing	O
l~d	O
)	O
over	O
all	O
k-nn	O
rules	O
with	O
1	O
:	O
:	O
:	O
:	O
k	O
:	O
:	O
:	O
:	O
n.	O
prove	O
or	O
disprove	O
:	O
gn	O
is	O
strongly	O
universally	O
consistent	O
.	O
note	O
that	O
in	O
view	O
of	O
theorem	O
26.4	O
,	O
it	O
suffices	O
to	O
consider	O
en/	O
log	O
n	O
:	O
:	O
:	O
:	O
k	O
:	O
:	O
:	O
:	O
n	O
-	O
1	O
for	O
all	O
e	O
>	O
o	O
.	O
27	O
hypercubes	O
and	O
discrete	O
spaces	O
in	O
many	O
situations	O
,	O
the	O
pair	O
(	O
x	O
,	O
y	O
)	O
is	O
purely	O
binary	B
,	O
taking	O
values	O
in	O
{	O
a	O
,	O
i	O
}	O
d	O
x	O
{	O
a	O
,	O
i	O
}	O
.	O
examples	O
include	O
boolean	O
settings	O
(	O
each	O
component	O
of	O
x	O
represents	O
``	O
on	O
''	O
or	O
``	O
off	O
''	O
)	O
,	O
representations	O
of	O
continuous	O
variables	O
through	O
quantization	B
(	O
continuous	O
variables	O
are	O
always	O
represented	O
by	O
bit	O
strings	O
in	O
computers	O
)	O
,	O
and	O
ordinal	O
data	O
(	O
a	O
component	O
of	O
x	O
is	O
i	O
if	O
and	O
only	O
if	O
a	O
certain	O
item	O
is	O
present	O
)	O
.	O
the	O
components	O
of	O
x	O
are	O
denoted	O
by	O
x	O
(	O
1	O
)	O
,	O
.•	O
.	O
,	O
x	O
(	O
d	O
)	O
.	O
in	O
this	O
chapter	O
,	O
we	O
review	O
pattern	O
recognition	O
briefly	O
in	O
this	O
setup	O
.	O
without	O
any	O
particular	O
structure	O
in	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
or	O
the	O
function	O
1j	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
iix	O
=	O
x	O
}	O
,	O
x	O
e	O
{	O
a	O
,	O
i	O
}	O
d	O
,	O
the	O
pattern	O
recognition	O
problem	O
might	O
as	O
well	O
be	O
cast	O
in	O
the	O
space	O
of	O
the	O
first	O
2d	O
positive	O
integers	O
:	O
(	O
x	O
,	O
y	O
)	O
e	O
{	O
i	O
,	O
...	O
,	O
2d	O
}	O
x	O
{	O
a	O
,	O
i	O
}	O
.	O
this	O
is	O
dealt	O
with	O
in	O
the	O
first	O
section	O
.	O
however	O
,	O
things	O
become	O
more	O
interesting	O
under	O
certain	O
structural	O
assumptions	O
,	O
such	O
as	O
the	O
assumption	O
that	O
the	O
components	O
of	O
x	O
be	O
independent	O
.	O
this	O
is	O
dealt	O
with	O
in	O
the	O
third	O
section	O
.	O
general	O
discrimination	O
rules	O
on	O
hypercubes	O
are	O
treated	O
in	O
the	O
rest	O
of	O
the	O
chapter	O
.	O
27.1	O
multinomial	B
discrimination	I
at	O
first	O
sight	O
,	O
discrimination	O
on	O
a	O
finite	O
set	O
{	O
i	O
,	O
...	O
,	O
k	O
}	O
-called	O
multinomial	B
discri	O
(	O
cid:173	O
)	O
mination-may	O
seem	O
utterly	O
trivial	O
.	O
let	O
us	O
call	O
the	O
following	O
rule	B
the	O
fundamental	B
rule	I
,	O
as	O
it	O
captures	O
what	O
most	O
of	O
us	O
would	O
do	O
in	O
the	O
absence	O
of	O
any	O
additional	O
information	O
.	O
462	O
27.	O
,	O
hypercubes	O
and	O
discrete	O
spaces	O
the	O
fundamental	B
rule	I
coincides	O
with	O
the	O
standard	B
kernel	O
and	O
histogram	O
rules	O
if	O
the	O
smoothing	B
factor	I
or	O
bin	O
width	O
are	O
taken	O
small	O
enough	O
.	O
if	O
pi	O
=	O
p	O
{	O
x	O
=	O
i	O
}	O
and	O
yj	O
is	O
as	O
usual	O
,	O
then	O
it	O
takes	O
just	O
a	O
second	O
to	O
see	O
that	O
yj	O
(	O
x	O
)	O
px	O
and	O
eln	O
;	O
:	O
:	O
:	O
l	O
yj	O
(	O
x	O
)	O
pxcl	O
-	O
px	O
)	O
n	O
,	O
x	O
where	O
ln	O
=	O
l	O
(	O
g~	O
)	O
.	O
assume	O
yj	O
(	O
x	O
)	O
=	O
1	O
at	O
all	O
x.	O
then	O
l	O
*	O
=	O
0	O
and	O
eln	O
;	O
:	O
:	O
:	O
l	O
px	O
(	O
l	O
-	O
px	O
)	O
n.	O
x	O
if	O
px	O
=	O
ii	O
k	O
for	O
all	O
x	O
,	O
we	O
have	O
eln	O
;	O
:	O
:	O
:	O
(	O
l	O
-	O
;	O
:	O
:	O
:	O
1/2	O
if	O
k	O
;	O
:	O
:	O
:	O
2n	O
.	O
this	O
simple	O
calculation	O
shows	O
that	O
we	O
can	O
not	O
say	O
anything	O
useful	O
about	O
fundamental	O
rules	O
unless	O
k	O
:	O
:s	O
2n	O
at	O
the	O
very	O
least	O
.	O
on	O
the	O
positive	O
side	O
,	O
the	O
following	O
universal	B
bound	O
is	O
useful	O
.	O
ii	O
k	O
)	O
n	O
theorem	B
27.1.	O
for	O
the	O
fundamental	B
rule	I
,	O
we	O
have	O
ln	O
--	O
-+	O
l	O
*	O
with	O
probability	O
one	O
as	O
n	O
--	O
-+	O
00	O
,	O
and	O
,	O
in	O
fact	O
,	O
for	O
all	O
distributions	O
,	O
eln	O
<	O
l*	O
+	O
-	O
~	O
k	O
2	O
(	O
n	O
+	O
1	O
)	O
+-	O
en	O
and	O
eln	O
:0	O
:	O
l	O
*	O
+	O
1.07s	O
!	O
'	O
;	O
;	O
.	O
proof	O
.	O
the	O
first	O
statement	O
follows	O
trivially	O
from	O
the	O
strong	B
universal	I
consistency	I
of	O
histogram	O
rules	O
(	O
see	O
theorem	B
9.4	O
)	O
.	O
it	O
is	O
the	O
universal	B
inequality	O
that	O
is	O
of	O
interest	O
here	O
.	O
if	O
b	O
(	O
n	O
,	O
p	O
)	O
denotes	O
a	O
binomial	O
random	O
variable	B
with	O
parameters	O
nand	O
p	O
,	O
then	O
we	O
have	O
k	O
eln	O
=	O
lpx	O
(	O
yj	O
(	O
x	O
)	O
+	O
(	O
l	O
-	O
2yj	O
(	O
x	O
»	O
p	O
{	O
b	O
(	O
n	O
(	O
x	O
)	O
,	O
yj	O
(	O
x	O
»	O
>	O
n	O
(	O
x	O
)	O
/2	O
}	O
)	O
x=l	O
(	O
here	O
n	O
(	O
x	O
)	O
is	O
a	O
binomial	O
(	O
n	O
,	O
p	O
(	O
x	O
»	O
random	O
variable	B
)	O
k	O
=	O
lpx	O
(	O
i-yj	O
(	O
x	O
)	O
+	O
(	O
2yj	O
(	O
x	O
)	O
-i	O
)	O
p	O
{	O
b	O
(	O
n	O
(	O
x	O
)	O
,	O
yj	O
(	O
x	O
»	O
:	O
:s	O
n	O
(	O
x	O
)	O
/2	O
}	O
)	O
.	O
x	O
:	O
:	O
:	O
1	O
from	O
this	O
,	O
if	O
sex	O
)	O
=	O
min	O
(	O
17	O
(	O
x	O
)	O
,	O
1	O
-	O
17	O
(	O
x	O
)	O
)	O
,	O
27.1	O
multinomial	B
discrimination	I
463	O
n	O
(	O
x	O
)	O
s	O
(	O
x	O
)	O
(	O
1	O
-	O
sex	O
)	O
)	O
n	O
(	O
x	O
)	O
s	O
(	O
x	O
)	O
(	O
l	O
-	O
2s	O
(	O
x	O
)	O
)	O
+	O
g	O
-	O
sex	O
)	O
)	O
n	O
(	O
x	O
)	O
2	O
2	O
}	O
k	O
x=l	O
x=l	O
(	O
since	O
s	O
(	O
x	O
)	O
(	O
1	O
-	O
sex	O
)	O
)	O
:	O
s	O
1/4	O
)	O
(	O
by	O
the	O
chebyshev-cantelli	O
inequality-theorem	O
a.17	O
)	O
<	O
l	O
px	O
(	O
s	O
(	O
x	O
)	O
+	O
(	O
1	O
-	O
2s	O
(	O
x	O
)	O
)	O
p	O
{	O
b	O
(	O
n	O
(	O
x	O
)	O
,	O
sex	O
)	O
)	O
~	O
n	O
(	O
x	O
)	O
/2	O
}	O
)	O
<	O
l	O
*	O
+	O
t	O
px	O
(	O
1	O
-	O
2s	O
(	O
x	O
)	O
)	O
e	O
{	O
:0	O
:	O
l	O
'	O
+	O
t	O
pal	O
-	O
2~	O
(	O
x	O
)	O
)	O
e	O
l	O
+	O
(	O
1	O
-	O
2~	O
(	O
x	O
)	O
)	O
2	O
n	O
(	O
x	O
)	O
}	O
:	O
s	O
l	O
*	O
+	O
t	O
pxe	O
{	O
~i	O
{	O
n	O
(	O
x	O
»	O
o	O
)	O
+	O
(	O
1	O
-	O
~	O
(	O
x	O
)	O
)	O
i	O
{	O
n	O
(	O
x	O
)	O
''	O
,	O
,	O
)	O
}	O
:0	O
:	O
l	O
'	O
+	O
t	O
px	O
(	O
1	O
-	O
px	O
)	O
n	O
+	O
~	O
t	O
px	O
e	O
{	O
n	O
;	O
x/	O
{	O
n	O
(	O
x	O
)	O
=o	O
)	O
}	O
:	O
s	O
l	O
*	O
+	O
(	O
1	O
-~	O
)	O
n	O
+	O
~	O
t	O
p	O
{	O
2	O
(	O
since	O
the	O
function	O
u	O
/	O
(	O
1	O
+	O
n	O
u2	O
(	O
by	O
jensen	O
's	O
inequality	B
)	O
2	O
n	O
(	O
x	O
)	O
k	O
2x=1	O
xv~	O
x=l	O
)	O
:	O
s	O
1/	O
(	O
2.jn	O
)	O
for	O
0	O
:	O
s	O
u	O
:	O
s	O
1	O
)	O
(	O
by	O
lemma	O
a.2	O
and	O
the	O
fact	O
that	O
the	O
worst	O
distribution	B
has	O
px	O
=	O
1/	O
k	O
for	O
all	O
x	O
)	O
:	O
s	O
l*	O
+e-n1k	O
+	O
1	O
k	O
ljp	O
;	O
.j2	O
(	O
n	O
+	O
1	O
)	O
x=l	O
:	O
s	O
l	O
*	O
+	O
~	O
+	O
fk	O
(	O
since	O
e-u	O
:	O
s	O
1/	O
(	O
eu	O
)	O
for	O
u	O
~	O
0	O
,	O
en	O
v	O
2	O
(	O
;	O
+1	O
)	O
and	O
by	O
the	O
cauchy-schwarz	O
inequality	B
)	O
.	O
this	O
concludes	O
the	O
proof	O
,	O
as	O
.jij2	O
+	O
1/	O
e	O
:	O
s	O
1.075	O
.	O
0	O
for	O
several	O
key	O
properties	O
of	O
the	O
fundamental	B
rule	I
,	O
the	O
reader	O
is	O
referred	O
to	O
glick	O
(	O
1973	O
)	O
and	O
to	O
problem	O
27.1.	O
other	O
references	O
include	O
krzanowski	O
(	O
1987	O
)	O
,	O
gold	O
(	O
cid:173	O
)	O
stein	O
(	O
1977	O
)	O
,	O
and	O
goldstein	O
and	O
dillon	O
(	O
1978	O
)	O
.	O
note	O
also	O
the	O
following	O
extension	O
of	O
theorem	O
27.1	O
,	O
which	O
shows	O
that	O
the	O
fundamental	B
rule	I
can	O
handle	O
all	O
discrete	O
distributions	O
.	O
464	O
27.	O
hypercubes	O
and	O
discrete	O
spaces	O
theorem	B
27.2.	O
if	O
x	O
is	O
purely	O
atomic	O
(	O
with	O
possibly	O
infinitely	O
many	O
atoms	O
)	O
,	O
then	O
ln	O
~	O
l	O
*	O
with	O
probability	O
one	O
for	O
the	O
fundamental	B
rule	I
.	O
proof	O
.	O
number	O
the	O
atoms	O
1,2	O
,	O
...	O
.	O
define	O
xi	O
=	O
min	O
(	O
x	O
,	O
k	O
)	O
and	O
replace	O
(	O
xi	O
,	O
yt	O
)	O
by	O
(	O
x	O
;	O
,	O
yi	O
)	O
,	O
where	O
x	O
;	O
=	O
min	O
(	O
xi	O
,	O
k	O
)	O
.	O
apply	O
the	O
fundamental	B
rule	I
to	O
the	O
new	O
problem	O
and	O
note	O
that	O
by	O
theorem	B
27.1	O
,	O
if	O
k	O
is	O
fixed	O
,	O
ln	O
~	O
l	O
*	O
with	O
probability	O
one	O
for	O
the	O
new	O
rule	B
(	O
and	O
new	O
distribution	B
)	O
.	O
however	O
,	O
the	O
difference	O
in	O
ln	O
's	O
and	O
in	O
l	O
*	O
's	O
between	O
the	O
truncated	O
and	O
nontruncated	O
versions	O
can	O
not	O
be	O
more	O
than	O
p	O
{	O
x	O
~	O
k	O
}	O
,	O
which	O
may	O
be	O
made	O
as	O
small	O
as	O
desired	O
by	O
choice	O
of	O
k.	O
0	O
27.2	O
quantization	B
consider	O
a	O
fixed	O
partition	B
of	O
space	O
nd	O
into	O
k	O
sets	O
{	O
ai	O
,	O
...	O
,	O
ad	O
,	O
and	O
let	O
gn	O
be	O
the	O
standard	B
partitioning	O
rule	B
based	O
upon	O
majority	O
votes	O
in	O
the	O
ai	O
's	O
(	O
ties	O
are	O
broken	O
by	O
favoring	O
the	O
response	O
``	O
0	O
,	O
''	O
as	O
elsewhere	O
in	O
the	O
book	O
)	O
.	O
we	O
consider	O
two	O
rules	O
:	O
(	O
1	O
)	O
the	O
rule	B
gn	O
considered	O
above	O
;	O
the	O
data	O
are	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
,	O
with	O
(	O
x	O
,	O
y	O
)	O
e	O
nd	O
x	O
to	O
,	O
i	O
}	O
.	O
the	O
probability	O
of	O
error	O
is	O
denoted	O
by	O
l	O
n	O
,	O
and	O
the	O
bayes	O
probability	O
of	O
error	O
is	O
l*	O
=	O
e	O
{	O
min	O
(	O
1j	O
(	O
x	O
)	O
,	O
1	O
-	O
1j	O
(	O
x	O
)	O
)	O
}	O
with	O
1j	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
llx	O
=x	O
}	O
,	O
x	O
end	O
.	O
(	O
2	O
)	O
the	O
fundamental	B
rule	I
g:1	O
operating	O
on	O
the	O
quantized	O
data	O
(	O
x~	O
,	O
y1	O
)	O
,	O
•••	O
,	O
(	O
x	O
;	O
!	O
'	O
yn	O
)	O
,	O
with	O
(	O
xi	O
,	O
y	O
)	O
e	O
{	O
l	O
,	O
...	O
,	O
k	O
}	O
x	O
to	O
,	O
i	O
}	O
,	O
x	O
;	O
=	O
j	O
if	O
x	O
e	O
a	O
j	O
.	O
the	O
bayes	O
probability	O
of	O
error	O
is	O
l'*	O
=	O
e	O
{	O
min	O
(	O
1j	O
'	O
(	O
x	O
'	O
)	O
,	O
1	O
-1j	O
'	O
(	O
x	O
'	O
)	O
)	O
}	O
with	O
1j	O
'	O
(	O
x	O
'	O
)	O
=	O
p	O
{	O
y	O
=	O
11	O
xi	O
=	O
xl	O
}	O
,	O
xl	O
e	O
{	O
i	O
,	O
...	O
,	O
k	O
}	O
.	O
the	O
probability	O
of	O
error	O
is	O
denoted	O
by	O
l	O
;	O
1	O
'	O
clearly	O
,	O
g~	O
is	O
nothing	O
but	O
the	O
fundamental	B
rule	I
for	O
the	O
quantized	O
data	O
.	O
as	O
gn	O
(	O
x	O
)	O
=	O
g:1	O
(	O
xl	O
)	O
,	O
where	O
xl	O
=	O
j	O
if	O
x	O
e	O
a	O
j	O
'	O
we	O
see	O
that	O
however	O
,	O
the	O
bayes	O
error	O
probabilities	O
are	O
different	O
in	O
the	O
two	O
situations	O
.	O
we	O
claim	O
,	O
however	O
,	O
the	O
following	O
:	O
theorem	B
27.3.	O
for	O
the	O
standard	B
partitioning	O
rule	B
,	O
e	O
{	O
ln	O
}	O
:	O
s	O
l*	O
+	O
l.075~	O
+	O
8	O
,	O
where	O
0	O
=	O
e	O
{	O
11j	O
(	O
x	O
)	O
-	O
1j	O
'	O
(	O
x	O
'	O
)	O
i	O
}	O
.	O
furthermore	O
,	O
l	O
*	O
:	O
.	O
:	O
:	O
:	O
l'*	O
:	O
.	O
:	O
:	O
:	O
l	O
*	O
+	O
o	O
.	O
27.2	O
quantization	B
465	O
proof	O
.	O
clearly	O
,	O
l	O
*	O
:	O
:s	O
l'*	O
(	O
see	O
problem	O
2.1	O
)	O
.	O
also	O
,	O
l'*	O
=	O
e	O
{	O
min	O
(	O
n	O
'	O
(	O
x	O
'	O
)	O
,	O
1	O
-	O
n	O
'	O
(	O
x	O
'	O
)	O
)	O
}	O
<	O
e	O
{	O
min	O
(	O
1	O
]	O
(	O
x	O
)	O
+	O
11	O
]	O
(	O
x	O
)	O
-	O
1	O
]	O
'	O
(	O
x	O
'	O
)	O
i	O
'	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
+	O
11	O
]	O
(	O
x	O
)	O
-	O
1	O
]	O
'	O
(	O
x	O
'	O
)	O
i	O
)	O
}	O
<	O
8	O
+	O
e	O
{	O
min	O
(	O
n	O
(	O
x	O
)	O
,	O
1	O
-	O
n	O
(	O
x	O
)	O
)	O
}	O
=	O
8+l*	O
.	O
furthermore	O
,	O
e	O
(	O
ln	O
)	O
s	O
l	O
''	O
+	O
1.075/f	O
;	O
s	O
l*	O
+	O
s	O
+	O
1.075/f	O
;	O
by	O
theorem	B
27.1.0	O
as	O
an	O
immediate	O
corollary	O
,	O
we	O
show	O
how	O
to	O
use	O
the	O
last	O
bound	O
to	O
get	O
useful	O
distribution-free	O
performance	O
bounds	O
.	O
theorem	B
27.4.	O
let	O
f	O
be	O
a	O
class	O
of	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
for	O
which	O
x	O
e	O
[	O
0	O
,	O
l	O
]	O
d	O
with	O
probability	O
one	O
,	O
and	O
for	O
some	O
constants	O
c	O
>	O
0	O
,	O
1	O
:	O
:	O
:	O
:	O
ci	O
>	O
0	O
,	O
11	O
]	O
(	O
x	O
)	O
-	O
1	O
]	O
(	O
z	O
)	O
i	O
:	O
:s	O
cllx	O
-	O
zw\	O
x	O
,	O
z	O
e	O
rd	O
.	O
then	O
,	O
ifwe	O
consider	O
all	O
cubic	B
histogram	O
rules	O
gn	O
(	O
see	O
chapter	O
6	O
for	O
a	O
definition	O
)	O
,	O
we	O
have	O
inf	O
cubic	B
histogram	O
rule	B
gn	O
e	O
{	O
l	O
(	O
gn	O
)	O
-	O
l	O
}	O
:	O
s	O
-	O
*	O
a	O
fn	O
b	O
+	O
-01	O
,	O
n	O
d+201	O
where	O
a	O
and	O
b	O
are	O
constants	O
depending	O
upon	O
c	O
,	O
ci	O
,	O
and	O
d	O
only	O
(	O
see	O
the	O
prooffor	O
explicit	O
expressions	O
)	O
.	O
theorem	B
27.4	O
establishes	O
the	O
existence	O
of	O
rules	O
that	O
perform	O
uniformly	O
at	O
rate	O
o	O
(	O
n-	O
a	O
/	O
(	O
d+2a	O
)	O
)	O
over	O
f.	O
results	O
like	O
this	O
have	O
an	O
impact	O
on	O
the	O
number	O
of	O
data	O
points	O
required	O
to	O
guarantee	O
a	O
given	O
performance	O
for	O
any	O
(	O
x	O
,	O
y	O
)	O
e	O
f.	O
proof	O
.	O
consider	O
a	O
cubic	O
grid	O
with	O
cells	O
of	O
volume	O
hd	O
covering	B
[	O
0	O
,	O
l	O
]	O
d	O
does	O
not	O
exceed	O
(	O
1/	O
h	O
+	O
2	O
)	O
d	O
,	O
we	O
apply	O
theorem	B
27.3	O
to	O
obtain	O
•	O
as	O
the	O
number	O
of	O
cells	O
e	O
(	O
l	O
(	O
gn	O
)	O
)	O
s	O
l	O
*	O
+	O
1.075	O
(	O
2	O
+	O
*	O
)	O
''	O
/2	O
+	O
s	O
,	O
where	O
8	O
:	O
:s	O
sup	O
11	O
]	O
(	O
x	O
)	O
-1	O
]	O
'	O
(	O
x	O
'	O
)	O
1	O
:	O
:s	O
c	O
sup	O
sup	O
liz	O
-	O
xlla	O
:	O
:s	O
c	O
(	O
hfd	O
)	O
a	O
x	O
ai	O
x	O
,	O
zeai	O
the	O
right-hand	O
side	O
is	O
approximately	O
maximal	O
when	O
1.075d	O
)	O
h	O
=	O
2	O
''	O
(	O
c.jd	O
)	O
''	O
in	O
(	O
def	O
c	O
'	O
-1	O
-	O
'	O
n	O
d+201	O
resubstitution	B
yields	O
the	O
result	O
.	O
0	O
466	O
27.	O
hypercubes	O
and	O
discrete	O
spaces	O
27.3	O
independent	O
components	O
let	O
x	O
=	O
(	O
x	O
(	O
l	O
)	O
,	O
...	O
,	O
xcd	O
)	O
)	O
have	O
components	O
that	O
are	O
conditionally	O
independent	O
,	O
given	O
{	O
y	O
=	O
1	O
}	O
,	O
and	O
also	O
,	O
given	O
{	O
y	O
=	O
oj	O
.	O
introduce	O
the	O
notation	O
p	O
{	O
xci	O
)	O
=	O
11y	O
=	O
1	O
}	O
,	O
p	O
(	O
i	O
)	O
q	O
(	O
i	O
)	O
=	O
p	O
{	O
x	O
(	O
i	O
)	O
=	O
11y	O
=	O
o	O
}	O
,	O
p	O
=	O
p	O
{	O
y	O
=	O
1	O
}	O
.	O
with	O
x	O
=	O
(	O
x	O
(	O
l	O
)	O
,	O
...	O
,	O
xed	O
)	O
)	O
e	O
{	O
o	O
,	O
l	O
}	O
d	O
,	O
we	O
see	O
that	O
1j	O
(	O
x	O
)	O
=	O
p	O
{	O
y=1	O
,	O
x=x	O
}	O
p	O
{	O
x	O
=	O
x	O
}	O
=	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
pp	O
{	O
x	O
=	O
xly	O
=	O
1	O
}	O
+	O
(	O
1	O
-	O
p	O
)	O
p	O
{	O
x	O
=	O
xly	O
=	O
o	O
}	O
'	O
pp	O
{	O
x=xiy=1	O
}	O
and	O
p	O
{	O
x	O
=	O
xly	O
=	O
1	O
}	O
=	O
it	O
p	O
(	O
it	O
(	O
i	O
)	O
(	O
1	O
-	O
p	O
(	O
i	O
)	O
)	O
l-x	O
(	O
i	O
)	O
,	O
d	O
i=l	O
p	O
{	O
x	O
=	O
xly	O
=	O
o	O
}	O
=	O
it	O
q	O
(	O
it	O
(	O
i	O
)	O
(	O
1	O
-	O
q	O
(	O
i	O
)	O
)	O
l-xu	O
d	O
)	O
.	O
simple	O
consideration	O
shows	O
that	O
the	O
bayes	O
rule	B
is	O
given	O
by	O
i=l	O
1	O
if	O
p	O
n~=l	O
p	O
(	O
i	O
)	O
x	O
(	O
i	O
)	O
(	O
1	O
-	O
p	O
(	O
i	O
)	O
)	O
l-x	O
(	O
i	O
)	O
g*	O
(	O
x	O
)	O
=	O
{	O
>	O
(	O
1	O
-	O
p	O
)	O
n~=l	O
q	O
(	O
iy	O
(	O
i	O
)	O
(	O
1	O
-	O
q	O
(	O
i	O
)	O
)	O
l-x	O
(	O
i	O
)	O
o	O
otherwise	O
.	O
taking	O
logarithms	O
,	O
it	O
is	O
easy	O
to	O
see	O
that	O
this	O
is	O
equivalent	O
to	O
the	O
following	O
rule	B
:	O
*	O
(	O
x	O
)	O
=	O
{	O
g	O
1	O
'f	O
,	O
\	O
,	O
d	O
1	O
0	O
otherwise	O
,	O
(	O
yo	O
+	O
l	O
...	O
.	O
,	O
i=l	O
(	O
yi	O
x	O
0	O
ci	O
)	O
>	O
where	O
(	O
yo	O
=	O
log	O
(	O
_p_	O
)	O
+	O
~	O
log	O
(	O
1	O
-	O
p	O
(	O
i	O
)	O
)	O
,	O
f	O
:	O
t	O
1	O
-	O
p	O
1	O
-	O
q	O
(	O
i	O
)	O
log	O
-	O
.	O
p	O
(	O
i	O
)	O
1	O
-	O
q	O
(	O
i	O
)	O
)	O
q	O
(	O
i	O
)	O
1	O
-	O
p	O
(	O
i	O
)	O
(	O
.	O
l	O
=	O
1	O
,	O
...	O
,	O
d.	O
in	O
other	O
words	O
,	O
the	O
bayes	O
classifier	B
is	O
linear	O
!	O
this	O
beautiful	O
fact	O
was	O
noted	O
by	O
minsky	O
(	O
1961	O
)	O
,	O
winder	O
(	O
1963	O
)	O
,	O
and	O
chow	O
(	O
1965	O
)	O
.	O
having	O
identified	O
the	O
bayes	O
rule	B
as	O
a	O
linear	O
discrimination	O
rule	B
,	O
we	O
may	O
apply	O
the	O
full	O
force	O
of	O
the	O
vapnik-chervonenkis	O
theory	O
.	O
let	O
gn	O
be	O
the	O
rule	B
that	O
minimizes	O
the	O
empirical	B
error	I
over	O
the	O
class	O
of	O
all	O
linear	O
discrimination	O
rules	O
.	O
as	O
the	O
class	O
27.3	O
independent	O
components	O
467.	O
of	O
linear	O
halfspaces	O
of	O
rd	O
has	O
vc	B
dimension	I
d	O
+	O
1	O
(	O
see	O
corollary	O
13.1	O
)	O
,	O
we	O
recall	O
from	O
theorem	B
13.11	O
that	O
for	O
gn	O
,	O
p	O
{	O
l	O
(	O
gn	O
)	O
-	O
l*	O
>	O
e	O
}	O
<	O
8nd+1	O
e-ne2	O
/128	O
,	O
e	O
{	O
l	O
(	O
gn	O
)	O
-	O
l	O
*	O
}	O
<	O
16	O
(	O
d+1	O
)	O
logn+4	O
2n	O
(	O
by	O
corollary	O
12.1	O
)	O
,	O
p	O
{	O
l	O
(	O
gn	O
)	O
-	O
l	O
*	O
>	O
e	O
}	O
<	O
16	O
(	O
jile	O
)	O
4096	O
(	O
d+1	O
)	O
e-ne2	O
/2	O
,	O
ne	O
2	O
:	O
:	O
:	O
:	O
64	O
,	O
and	O
where	O
e	O
{	O
l	O
(	O
gn	O
)	O
-	O
l*	O
}	O
:	O
s	O
yfi	O
'	O
c	O
c	O
=	O
16	O
+	O
-/i013	O
(	O
d	O
+	O
1	O
)	O
log	O
(	O
l012	O
(	O
d	O
+	O
1	O
)	O
)	O
(	O
problem	O
12.10	O
)	O
.	O
these	O
bounds	O
are	O
useful	O
(	O
i.e.	O
,	O
they	O
tend	O
to	O
zero	O
with	O
n	O
)	O
if	O
d	O
=	O
o	O
(	O
n/logn	O
)	O
.	O
in	O
contrast	O
,	O
without	O
the	O
independence	O
assumption	O
,	O
we	O
have	O
pointed	O
out	O
that	O
no	O
non	O
(	O
cid:173	O
)	O
trivial	O
guarantee	O
can	O
be	O
given	O
about	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
unless	O
2d	O
<	O
n.	O
the	O
independence	O
assumption	O
has	O
led	O
us	O
out	O
of	O
the	O
high-dimensional	O
quagmire	O
.	O
one	O
may	O
wish	O
to	O
attempt	O
to	O
estimate	B
the	O
p	O
(	O
i	O
)	O
's	O
and	O
q	O
(	O
i	O
)	O
's	O
by	O
p	O
(	O
i	O
)	O
and	O
q	O
(	O
i	O
)	O
and	O
to	O
use	O
these	O
in	O
the	O
plug-in	O
rule	O
gn	O
(	O
x	O
)	O
that	O
decides	O
1	O
if	O
d	O
p	O
n	O
putu	O
)	O
(	O
l	O
-	O
p	O
(	O
i	O
)	O
)	O
l-x	O
(	O
i	O
)	O
>	O
(	O
l	O
-	O
fi	O
)	O
n	O
q	O
(	O
i	O
)	O
x	O
(	O
i	O
)	O
(	O
l	O
-	O
qu	O
)	O
)	O
l-x	O
(	O
i	O
)	O
,	O
d	O
i=l	O
i=l	O
where	O
p	O
is	O
the	O
standard	B
sample-based	O
estimate	B
of	O
p.	O
the	O
maximum-likelihood	O
estimate	B
of	O
p	O
(	O
i	O
)	O
is	O
given	O
by	O
while	O
q-	O
(	O
i	O
)	O
=	O
l~=l	O
i	O
{	O
x	O
(	O
i	O
)	O
=l	O
y=o	O
}	O
n	O
}	O
lj=l	O
l	O
{	O
yfo	O
}	O
'	O
j	O
,	O
with	O
%	O
equal	O
to	O
0	O
(	O
see	O
problem	O
27.6	O
)	O
.	O
note	O
that	O
the	O
plug-in	O
rule	O
too	O
is	O
linear	O
,	O
with	O
n	O
(	O
x	O
)	O
=	O
g	O
{	O
'f	O
1	O
ao	O
+	O
.	O
l	O
...	O
i=l	O
ai	O
x	O
l	O
,	O
\	O
:	O
,d	O
0	O
otherwise	O
,	O
0	O
(	O
i	O
)	O
>	O
where	O
ao	O
=	O
p	O
)	O
~	O
(	O
log	O
-	O
-	O
+	O
~	O
log	O
(	O
1	O
-	O
p	O
i=l	O
no	O
!	O
(	O
i	O
)	O
no	O
!	O
(	O
i	O
)	O
+	O
nll	O
(	O
i	O
)	O
noo	O
(	O
i	O
)	O
+	O
nlo	O
(	O
i	O
)	O
)	O
.	O
-	O
-	O
-	O
-	O
-	O
noo	O
(	O
i	O
)	O
,	O
ai	O
=	O
log	O
nll	O
(	O
i	O
)	O
.	O
noo	O
(	O
i	O
)	O
)	O
nol	O
(	O
i	O
)	O
nlo	O
(	O
i	O
)	O
(	O
._	O
l	O
-	O
1	O
,	O
...	O
,	O
d	O
,	O
468	O
27.	O
hypercubes	O
and	O
discrete	O
spaces	O
and	O
n	O
noo	O
(	O
i	O
)	O
=	O
l	O
i	O
{	O
xj	O
)	O
=o	O
,	O
yj=o	O
}	O
'	O
j=l	O
n	O
n	O
lo	O
(	O
i	O
)	O
=	O
l	O
i	O
{	O
xj	O
)	O
=l	O
,	O
yj=o	O
}	O
'	O
j=l	O
n	O
nol	O
(	O
i	O
)	O
=	O
l	O
i	O
{	O
xj	O
)	O
=o	O
,	O
yj=l	O
}	O
,	O
j=l	O
n	O
nll	O
(	O
i	O
)	O
=	O
l	O
i	O
{	O
xji	O
)	O
=l	O
,	O
yj=l	O
}	O
,	O
j=l	O
d	O
p	O
=	O
l	O
(	O
nol	O
(	O
i	O
)	O
+	O
nll	O
(	O
i	O
)	O
)	O
.	O
i=l	O
for	O
all	O
this	O
,	O
we	O
refer	O
to	O
warner	O
,	O
toronto	O
,	O
veasey	O
,	O
and	O
stephenson	O
(	O
1961	O
)	O
(	O
see	O
also	O
mclachlan	O
(	O
1992	O
)	O
)	O
.	O
use	O
the	O
inequality	B
e	O
{	O
l	O
(	O
gn	O
)	O
-	O
l	O
*	O
}	O
:	O
:s	O
2e	O
{	O
117n	O
(	O
x	O
)	O
-	O
17	O
(	O
x	O
)	O
)	O
}	O
,	O
where	O
17	O
is	O
as	O
given	O
in	O
the	O
text	O
,	O
and	O
17n	O
is	O
as	O
17	O
but	O
with	O
p	O
,	O
p	O
(	O
i	O
)	O
,	O
q	O
(	O
i	O
)	O
replaced	O
by	O
p	O
,	O
p	O
(	O
i	O
)	O
,	O
q	O
(	O
i	O
)	O
,	O
to	O
establish	O
consistency	B
:	O
theorem	B
27.5.	O
for	O
the	O
plug-in	O
rule	O
,	O
l	O
(	O
gn	O
)	O
-+	O
l	O
*	O
with	O
probability	O
one	O
and	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
-	O
l	O
*	O
-+	O
0	O
,	O
whenever	O
the	O
components	O
are	O
independent	O
.	O
we	O
refer	O
to	O
the	O
problem	O
section	O
for	O
an	O
evaluation	O
of	O
an	O
upper	O
bound	O
for	O
e	O
{	O
l	O
(	O
gn	O
)	O
(	O
cid:173	O
)	O
l	O
*	O
}	O
(	O
see	O
problem	O
27.7	O
)	O
.	O
linear	O
discrimination	O
on	O
the	O
hypercube	O
has	O
of	O
course	O
limited	O
value	O
.	O
the	O
world	O
is	O
full	O
of	O
examples	O
that	O
are	O
not	O
linearly	O
separable	O
.	O
for	O
example	O
,	O
on	O
{	O
o	O
,	O
1	O
}	O
2	O
,	O
if	O
y	O
=	O
xcl	O
)	O
(	O
1	O
-	O
x	O
(	O
2	O
)	O
)	O
+	O
x	O
(	O
2	O
)	O
(	O
1	O
-	O
xo	O
)	O
)	O
(	O
so	O
that	O
y	O
implements	O
the	O
boolean	O
``	O
xor	O
''	O
or	O
''	O
exclusive	O
or	O
''	O
function	O
)	O
,	O
the	O
problem	O
is	O
not	O
linearly	O
separable	O
if	O
all	O
four	O
possible	O
values	O
of	O
x	O
have	O
positive	O
probability	O
.	O
however	O
,	O
the	O
exclusive	O
or	O
function	O
may	O
be	O
dealt	O
with	O
very	O
nicely	O
if	O
one	O
considers	O
quadratic	O
discriminants	O
dealt	O
with	O
in	O
a	O
later	O
section	O
(	O
27.5	O
)	O
on	O
series	O
methods	O
.	O
27.4	O
boolean	O
classifiers	O
by	O
a	O
boolean	O
classification	O
problem	O
,	O
we	O
mean	O
a	O
pattern	O
recognition	O
problem	O
on	O
the	O
hypercube	O
for	O
which	O
l	O
*	O
=	O
0.	O
this	O
setup	O
relates	O
to	O
the	O
fact	O
that	O
if	O
we	O
consider	O
y	O
=	O
i	O
as	O
a	O
circuit	O
failure	O
and	O
y	O
=	O
°	O
as	O
an	O
operable	O
circuit	O
,	O
then	O
y	O
is	O
a	O
deterministic	O
function	O
of	O
the	O
x	O
(	O
i	O
)	O
's	O
,	O
which	O
may	O
be	O
considered	O
as	O
gates	O
or	O
switches	O
.	O
in	O
that	O
case	O
,	O
y	O
may	O
be	O
written	O
as	O
a	O
boolean	O
function	O
of	O
x	O
(	O
1	O
)	O
,	O
.•	O
.	O
,	O
xcd	O
)	O
.	O
we	O
may	O
limit	O
boolean	O
classifiers	O
in	O
various	O
ways	O
by	O
partially	O
specifying	O
this	O
function	O
.	O
for	O
example	O
,	O
fol	O
(	O
cid:173	O
)	O
lowing	O
natarajan	O
(	O
1991	O
)	O
,	O
we	O
first	O
consider	O
all	O
monomials	O
,	O
that	O
is	O
,	O
all	O
functions	O
g	O
:	O
{	O
o	O
,	O
l	O
}	O
d	O
-+	O
{	O
o	O
,	O
i	O
}	O
of	O
the	O
form	O
27.4	O
boolean	O
classifiers	O
469	O
for	O
some	O
k	O
.	O
:	O
s	O
d	O
and	O
some	O
indices	O
1	O
.	O
:	O
s	O
il	O
<	O
...	O
<	O
ik	O
.	O
:	O
s	O
d	O
(	O
note	O
that	O
algebraic	O
mul	O
(	O
cid:173	O
)	O
tiplication	O
corresponds	O
to	O
a	O
boolean	O
``	O
and	O
''	O
)	O
.	O
in	O
such	O
situations	O
,	O
one	O
might	O
attempt	O
to	O
minimize	O
the	O
empirical	B
error	I
.	O
as	O
we	O
know	O
that	O
g*	O
is	O
also	O
a	O
monomial	O
,	O
it	O
is	O
clear	O
that	O
the	O
minimal	O
empirical	B
error	I
is	O
zero	O
.	O
one	O
such	O
minimizing	O
monomial	B
is	O
given	O
by	O
where	O
d	O
an	O
mm	O
l	O
:	O
:j	O
:	O
:	O
;	O
on	O
.	O
xu	O
)	O
0	O
i	O
=	O
.	O
d	O
{	O
'	O
.	O
}	O
,	O
]	O
'f	O
ll	O
,	O
···	O
,	O
lk	O
.	O
thus	O
,	O
gn	O
picks	O
those	O
components	O
for	O
which	O
every	O
data	O
point	O
has	O
a	O
``	O
1	O
''	O
.	O
clearly	O
,	O
the	O
empirical	B
error	I
is	O
zero	O
.	O
the	O
number	O
of	O
possible	O
functions	O
is	O
2d	O
.	O
therefore	O
,	O
by	O
theorem	B
12.1	O
,	O
and	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
.	O
:	O
s	O
-	O
.	O
d+1	O
n	O
here	O
again	O
,	O
we	O
have	O
avoided	O
the	O
curse	B
of	I
dimensionality	I
.	O
for	O
good	O
performance	O
,	O
it	O
suffices	O
that	O
n	O
be	O
a	O
bit	O
larger	O
than	O
d	O
,	O
regardless	O
of	O
the	O
distribution	B
of	O
the	O
data	O
!	O
assume	O
that	O
we	O
limit	O
the	O
complexity	O
of	O
a	O
boolean	O
classifier	B
g	O
by	O
requiring	O
that	O
g	O
must	O
be	O
written	O
as	O
an	O
expression	B
having	O
at	O
most	O
k	O
operations	O
``	O
not	O
,	O
''	O
``	O
and	O
,	O
''	O
or	O
``	O
or	O
,	O
''	O
with	O
the	O
x	O
(	O
i	O
)	O
,	O
s	O
as	O
inputs	O
.	O
to	O
avoid	O
problems	O
with	O
precedence	O
rules	O
,	O
we	O
assume	O
that	O
any	O
number	O
of	O
parentheses	O
is	O
allowed	O
in	O
the	O
expression	B
.	O
one	O
may	O
visualize	O
each	O
expression	B
as	O
an	O
expression	B
tree	O
,	O
that	O
is	O
,	O
a	O
tree	O
in	O
which	O
internal	O
nodes	O
represent	O
operations	O
and	O
leaves	O
represent	O
operands	O
(	O
inputs	O
)	O
.	O
the	O
number	O
of	O
such	O
binary	B
trees	O
with	O
k	O
internal	O
nodes	O
(	O
and	O
thus	O
k	O
+	O
1	O
leaves	O
)	O
is	O
given	O
by	O
the	O
catalan	O
number	O
1	O
(	O
2k	O
)	O
k	O
+	O
1	O
k	O
(	O
see	O
,	O
e.g.	O
,	O
kemp	O
(	O
1984	O
)	O
)	O
.	O
furthermore	O
,	O
we	O
may	O
associate	O
any	O
of	O
the	O
x	O
(	O
i	O
)	O
,	O
s	O
with	O
the	O
leaves	O
(	O
possibly	O
preceded	O
by	O
``	O
not	O
''	O
)	O
and	O
``	O
and	O
''	O
or	O
``	O
or	O
''	O
with	O
each	O
binary	B
internal	O
node	O
,	O
thus	O
obtaining	O
a	O
total	O
of	O
not	O
more	O
than	O
2k	O
(	O
2d	O
)	O
k+	O
1	O
_1_	O
(	O
2k	O
)	O
k	O
k+1	O
possible	O
boolean	O
functions	O
of	O
this	O
kind	O
.	O
as	O
k	O
-	O
?	O
00	O
,	O
this	O
bound	O
is	O
not	O
more	O
than	O
for	O
k	O
large	O
enough	O
.	O
again	O
,	O
for	O
k	O
large	O
enough	O
,	O
470	O
27.	O
hypercubes	O
and	O
discrete	O
spaces	O
and	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
:	O
:	O
:	O
:	O
k	O
10g	O
(	O
16d	O
)	O
+	O
1	O
.	O
n	O
note	O
that	O
k	O
is	O
much	O
more	O
important	O
than	O
d	O
in	O
determining	O
the	O
sample	O
size	O
.	O
for	O
historic	O
reasons	O
,	O
we	O
mention	O
that	O
if	O
g	O
is	O
any	O
boolean	O
expression	O
consisting	O
of	O
at	O
most	O
k	O
``	O
not	O
,	O
''	O
``	O
and	O
,	O
''	O
or	O
``	O
or	O
''	O
operations	O
,	O
then	O
the	O
number	O
of	O
such	O
functions	O
was	O
shown	O
by	O
pippenger	O
(	O
1977	O
)	O
not	O
to	O
exceed	O
c6	O
(	O
d	O
k	O
+kf	O
y	O
pearl	O
(	O
1979	O
)	O
used	O
pippenger	O
's	O
estimate	B
to	O
obtain	O
performance	O
bounds	O
such	O
as	O
the	O
ones	O
given	O
above	O
.	O
27.5	O
series	O
methods	O
for	O
the	O
hypercube	O
it	O
is	O
interesting	O
to	O
note	O
that	O
we	O
may	O
write	O
any	O
function	O
rt	O
on	O
the	O
hypercube	O
as	O
a	O
linear	O
combination	O
of	O
rademacher-walsh	O
polynomials	O
2x	O
(	O
l	O
)	O
-	O
1	O
2x	O
(	O
d	O
)	O
-	O
1	O
(	O
2x	O
(	O
l	O
)	O
-	O
1	O
)	O
(	O
2x	O
(	O
2	O
)	O
-	O
1	O
)	O
i=o	O
i	O
=	O
1	O
i=d	O
i=d+1	O
(	O
2x	O
(	O
d-l	O
)	O
-	O
1	O
)	O
(	O
2x	O
(	O
d	O
)	O
-	O
1	O
)	O
(	O
2x	O
(	O
l	O
)	O
-	O
1	O
)	O
(	O
2x	O
(	O
2	O
)	O
-	O
1	O
)	O
(	O
2x	O
(	O
3	O
)	O
-	O
1	O
)	O
i	O
=	O
d	O
+	O
1	O
+	O
(	O
~	O
)	O
i	O
=	O
d	O
+	O
2	O
+	O
(	O
~	O
)	O
(	O
2x	O
(	O
l	O
)	O
-	O
1	O
)	O
...	O
(	O
2x	O
(	O
d	O
)	O
-	O
1	O
)	O
i	O
=	O
2d	O
-	O
1.	O
we	O
verify	O
easily	O
that	O
so	O
that	O
the	O
1/ji	O
's	O
form	O
an	O
orthogonal	O
system	O
.	O
therefore	O
,	O
we	O
may	O
write	O
fl-	O
(	O
{	O
x	O
}	O
)	O
rt	O
(	O
x	O
)	O
=	O
l	O
ai	O
1/ji	O
(	O
x	O
)	O
,	O
2d	O
-l	O
i=o	O
where	O
27.5	O
series	O
methods	O
for	O
the	O
hypercube	O
471-	O
and	O
f1	O
(	O
{	O
x	O
}	O
)	O
=	O
p	O
{	O
x	O
=	O
x	O
}	O
.	O
also	O
,	O
f1	O
(	O
{	O
x	O
}	O
)	O
(	O
l	O
-	O
17	O
(	O
x	O
)	O
)	O
=	O
l	O
bil/ji	O
(	O
x	O
)	O
,	O
2d	O
-1	O
i=o	O
with	O
bi	O
=	O
;	O
d	O
~	O
l/rt	O
(	O
x	O
)	O
(	O
l	O
-	O
ry	O
(	O
x	O
»	O
jl	O
(	O
{	O
x	O
)	O
)	O
=	O
e	O
{	O
1/f~sx	O
)	O
liy~oi	O
}	O
'	O
sample-based	O
estimates	O
of	O
ai	O
and	O
bi	O
are	O
=	O
=	O
the	O
bayes	O
rule	B
is	O
given	O
by	O
replacing	O
ai	O
-	O
bi	O
formally	O
by	O
~	O
-	O
b	O
;	O
yields	O
the	O
plug-in	O
rule	O
.	O
observe	O
that	O
this	O
is	O
just	O
a	O
discrete	O
version	O
of	O
the	O
fourier	O
series	O
rules	O
discussed	O
in	O
chapter	O
17.	O
this	O
rule	B
requires	O
the	O
estimation	B
of	I
2d	O
differences	O
ai	O
-	O
bi	O
.	O
therefore	O
,	O
we	O
might	O
as	O
well	O
have	O
used	O
the	O
fundamental	B
rule	I
.	O
when	O
our	O
hand	O
is	O
forced	O
by	O
the	O
dimension	B
,	O
we	O
may	O
wish	O
to	O
consider	O
only	O
rules	O
in	O
the	O
class	O
c	O
given	O
by	O
g	O
(	O
x	O
)	O
=	O
{	O
i	O
0	O
if	O
``	O
~~	O
)	O
+	O
(	O
~	O
)	O
+	O
''	O
+	O
(	O
:	O
)	O
(	O
a~	O
-	O
b	O
(	O
»	O
lr·	O
(	O
x	O
)	O
>	O
0	O
otherwise	O
,	O
.l	O
...	O
t1=o	O
'f'l	O
i	O
i	O
where	O
k	O
:	O
:	O
:	O
d	O
is	O
a	O
positive	O
integer	O
,	O
and	O
ab	O
,	O
bb	O
,	O
ai	O
'	O
b~	O
,	O
...	O
are	O
arbitrary	O
constants	O
.	O
we	O
have	O
seen	O
that	O
the	O
vc	B
dimension	I
of	O
this	O
class	O
is	O
not	O
more	O
than	O
(	O
~	O
)	O
+	O
(	O
~	O
)	O
+	O
...	O
+	O
(	O
~	O
)	O
:	O
:	O
:	O
d	O
k	O
+	O
1	O
(	O
theorem	B
13.9	O
)	O
.	O
within	O
this	O
class	O
,	O
estimation	B
errors	O
of	O
the	O
order	O
of	O
0	O
(	O
.j	O
dk	O
log	O
n	O
/	O
n	O
)	O
are	O
thus	O
possible	O
if	O
we	O
minimize	O
the	O
empirical	B
error	I
(	O
theorem	B
13.12	O
)	O
.	O
this	O
,	O
in	O
effect	O
,	O
forces	O
us	O
to	O
take	O
k	O
«	O
logd	O
n.	O
for	O
larger	O
k	O
,	O
pattern	O
recognition	O
is	O
all	O
but	O
impossible	O
.	O
as	O
an	O
interesting	O
side	O
note	O
,	O
observe	O
that	O
for	O
a	O
given	O
parameter	O
k	O
,	O
each	O
member	O
of	O
c	O
is	O
a	O
k-th	O
degree	O
polynomial	B
in	O
x.	O
remark	O
.	O
performance	O
of	O
the	O
plug-in	O
rule	O
.	O
define	O
the	O
plug-in	O
rule	O
by	O
gn	O
(	O
x	O
)	O
=	O
{	O
i	O
o	O
otherwise	O
.	O
.l	O
...	O
t1=o	O
if	O
,	O
,~~	O
)	O
+	O
(	O
~	O
)	O
+···+	O
(	O
t	O
)	O
(	O
a	O
.	O
-	O
t	O
;	O
.	O
»	O
lr·	O
(	O
x	O
)	O
>	O
0	O
i	O
i	O
'f'l	O
472	O
27.	O
hypercubes	O
and	O
discrete	O
spaces	O
as	O
n	O
--	O
--	O
+	O
00	O
,	O
by	O
the	O
law	O
oflarge	O
numbers	O
,	O
gn	O
approaches	O
goo	O
:	O
goo	O
(	O
x	O
)	O
=	O
{	O
'f	O
~	O
(	O
~	O
)	O
+	O
(	O
~	O
)	O
+	O
...	O
+	O
(	O
~	O
)	O
(	O
1	O
ai	O
-	O
1	O
o	O
otherwise	O
,	O
li==	O
,	O
o	O
b	O
)	O
,	O
/r	O
(	O
)	O
0	O
i	O
'pi	O
x	O
>	O
where	O
ai	O
,	O
bi	O
are	O
as	O
defined	O
above	O
.	O
interestingly	O
,	O
goo	O
may	O
be	O
much	O
worse	O
than	O
the	O
best	O
rule	B
in	O
c	O
!	O
consider	O
the	O
simple	O
example	O
for	O
d	O
=	O
2	O
,	O
k	O
=	O
1	O
:	O
ii	O
x	O
(	O
l	O
)	O
=	O
0	O
x	O
(	O
l	O
)	O
=	O
1	O
table	O
of	O
17	O
(	O
x	O
)	O
:	O
x	O
(	O
2	O
;	O
=	O
1	O
x	O
(	O
2	O
=	O
0	O
118	O
5/8	O
6/8	O
1/8	O
ii	O
ii	O
x	O
(	O
l	O
)	O
=	O
0	O
x	O
(	O
i	O
)	O
=	O
1	O
table	O
of	O
i'l	O
(	O
{	O
x	O
}	O
)	O
=	O
p	O
{	O
x	O
=	O
x	O
}	O
:	O
x	O
(	O
2	O
)	O
=	O
1	O
x	O
(	O
2	O
)	O
=	O
0	O
p	O
4	O
8p	O
-9-	O
ii	O
s-lop	O
~	O
p	O
a	O
simple	O
calculation	O
shows	O
that	O
for	O
any	O
choice	O
p	O
e	O
[	O
0	O
,	O
1/2	O
]	O
,	O
either	O
goo	O
==	O
1	O
or	O
goo	O
==	O
o.	O
we	O
have	O
goo	O
==	O
1	O
if	O
p	O
<	O
10/47	O
,	O
and	O
goo	O
==	O
0	O
otherwise	O
.	O
however	O
,	O
in	O
obvious	O
notation	O
,	O
l	O
(	O
g	O
==	O
1	O
)	O
=	O
(	O
41p	O
+	O
11	O
)	O
/36	O
,	O
l	O
(	O
g	O
==	O
0	O
)	O
=	O
(	O
50	O
-	O
82p	O
)	O
/72	O
.	O
the	O
best	O
constant	O
rule	B
is	O
g	O
==	O
1	O
when	O
p	O
<	O
7/41	O
and	O
g	O
==	O
0	O
otherwise	O
.	O
for	O
7/41	O
<	O
p	O
<	O
10/47	O
,	O
the	O
plug-in	O
rule	O
does	O
not	O
even	O
pick	O
the	O
best	O
constant	O
rule	B
,	O
let	O
alone	O
the	O
best	O
rule	B
in	O
c	O
with	O
k	O
=	O
1	O
,	O
which	O
it	O
was	O
intended	O
to	O
pick	O
.	O
this	O
example	O
highlights	O
the	O
danger	O
of	O
parametric	O
rules	O
or	O
plug-in	O
rules	O
when	O
applied	O
to	O
incorrect	O
or	O
incomplete	O
models	O
.	O
0	O
remark	O
.	O
historical	O
notes	O
.	O
the	O
rademacher-walsh	O
expansion	O
occurs	O
frequently	O
in	O
switching	O
theory	O
,	O
and	O
was	O
given	O
in	O
duda	O
and	O
hart	O
(	O
1973	O
)	O
.	O
the	O
bahadur-lazars	O
(	O
cid:173	O
)	O
feid	O
expansion	O
(	O
bahadur	O
(	O
1961	O
»	O
is	O
similar	O
in	O
nature	O
.	O
ito	O
(	O
1969	O
)	O
presents	O
error	O
bounds	O
for	O
discrimination	O
based	O
upon	O
a	O
k-term	O
truncation	O
of	O
the	O
series	O
.	O
ott	O
and	O
kronmal	O
(	O
1976	O
)	O
provide	O
further	O
statistical	O
properties	O
.	O
the	O
rules	O
described	O
here	O
with	O
k	O
defining	O
the	O
number	O
of	O
interactions	O
are	O
also	O
obtained	O
if	O
we	O
model	O
p	O
{	O
x	O
=	O
x	O
i	O
y	O
=	O
i	O
}	O
and	O
p	O
{	O
x	O
=	O
x	O
i	O
y	O
=	O
1	O
}	O
by	O
functions	O
of	O
the	O
form	O
)	O
and	O
exp	O
~	O
bi	O
o/i	O
(	O
x	O
)	O
(	O
(	O
~	O
)	O
+	O
(	O
~	O
)	O
+	O
...	O
+	O
(	O
~	O
)	O
.	O
the	O
latter	O
model	O
is	O
called	O
the	O
log-linear	B
model	I
(	O
see	O
mclachlan	O
(	O
1992	O
,	O
section	O
7.3	O
»	O
.0	O
27.6	O
maximum	B
likelihood	I
the	O
maximum	B
likelihood	I
method	O
(	O
see	O
chapter	O
15	O
)	O
should	O
not	O
be	O
used	O
for	O
picking	O
the	O
best	O
rule	B
from	O
a	O
class	O
c	O
that	O
is	O
not	O
guaranteed	O
to	O
include	O
the	O
bayes	O
rule	B
.	O
perhaps	O
a	O
simple	O
example	O
will	O
suffice	O
to	O
make	O
the	O
point	O
.	O
consider	O
the	O
following	O
hypercube	O
setting	O
with	O
d	O
=	O
2	O
:	O
27.6	O
maximum	B
likelihood	I
473	O
.	O
1	O
]	O
(	O
x	O
)	O
:	O
p	O
,	O
(	O
{	O
x	O
}	O
)	O
:	O
ii	O
x	O
(	O
l	O
)	O
=	O
0	O
i	O
x	O
(	O
l	O
)	O
=	O
1	O
ii	O
x	O
(	O
l	O
)	O
=	O
0	O
p	O
r	O
q	O
s	O
in	O
this	O
example	O
,	O
l	O
*	O
=	O
o.	O
apply	O
maximum	B
likelihood	I
to	O
a	O
class	O
f	O
with	O
two	O
members	O
{	O
1	O
]	O
a	O
,	O
1	O
]	O
b	O
}	O
,	O
where	O
1	O
]	O
a	O
(	O
x	O
)	O
1	O
]	O
b	O
(	O
x	O
)	O
=	O
{	O
4/5	O
lis	O
{	O
0	O
1	O
if	O
x	O
(	O
l	O
)	O
=	O
0	O
if	O
x	O
(	O
l	O
)	O
=	O
1	O
,	O
if	O
x	O
(	O
l	O
)	O
=	O
0	O
if	O
x	O
(	O
l	O
)	O
=	O
1.	O
then	O
maximum	B
likelihood	I
wo	O
n't	O
even	O
pick	O
the	O
best	O
member	O
from	O
f	O
!	O
to	O
verify	O
this	O
,	O
with	O
ga	O
(	O
x	O
)	O
=	O
1	O
{	O
1ja	O
(	O
x	O
»	O
lj2j	O
,	O
gb	O
(	O
x	O
)	O
=	O
1	O
{	O
1jb	O
(	O
x	O
»	O
lj2j	O
.	O
we	O
see	O
that	O
l	O
(	O
ga	O
)	O
=	O
p	O
+	O
q	O
,	O
l	O
(	O
g	O
b	O
)	O
=	O
r	O
+	O
s.	O
however	O
,	O
if	O
1	O
]	O
ml	O
is	O
given	O
by	O
and	O
if	O
we	O
write	O
nij	O
for	O
the	O
number	O
of	O
data	O
pairs	O
(	O
x	O
k	O
,	O
yk	O
)	O
having	O
x	O
k	O
=	O
i	O
,	O
yk	O
=	O
j	O
,	O
then	O
1	O
]	O
ml	O
=	O
1	O
]	O
a	O
if	O
o	O
if	O
nol	O
+	O
nlo	O
>	O
0	O
if	O
n	O
01	O
+	O
n	O
10	O
=	O
0	O
1	O
and	O
1	O
]	O
ml	O
=	O
1	O
]	O
b	O
otherwise	O
.	O
equivalently	O
,	O
1	O
]	O
ml	O
=	O
1	O
]	O
a	O
if	O
and	O
only	O
if	O
n01	O
+	O
nlo	O
>	O
o.	O
apply	O
the	O
strong	B
law	O
of	O
large	O
numbers	O
to	O
note	O
that	O
nolin	O
--	O
+	O
r	O
,	O
nloln	O
--	O
+	O
s	O
,	O
nooln	O
--	O
+	O
p	O
,	O
and	O
nll/n	O
--	O
+	O
q	O
with	O
probability	O
one	O
,	O
as	O
n	O
--	O
+	O
00.	O
thus	O
,	O
lim	O
p	O
{	O
n-+oo	O
1	O
]	O
ml	O
=	O
}	O
=	O
{	O
i	O
if	O
r	O
+	O
s	O
>	O
0	O
0	O
otherwise	O
.	O
1	O
]	O
a	O
take	O
r	O
+	O
s	O
=	O
e	O
very	O
small	O
,	O
p	O
+	O
q	O
=	O
1	O
-	O
e.	O
then	O
,	O
for	O
the	O
maximum	B
likelihood	I
rule	O
,	O
however	O
,	O
when	O
f	O
contains	O
the	O
bayes	O
rule	B
,	O
maximum	B
likelihood	I
is	O
consistent	O
(	O
see	O
theorem	B
15.1	O
)	O
.	O
474	O
27.	O
hypercubes	O
and	O
discrete	O
spaces	O
27.7	O
kernel	B
methods	O
sometimes	O
,	O
d	O
is	O
so	O
large	O
with	O
respect	O
to	O
n	O
that	O
the	O
atoms	O
in	O
the	O
hypercube	O
are	O
sparsely	O
populated	O
.	O
some	O
amount	O
of	O
smoothing	O
may	O
help	O
under	O
some	O
circum	O
(	O
cid:173	O
)	O
stances	O
.	O
consider	O
a	O
kernel	O
k	O
,	O
and	O
define	O
the	O
kernel	B
rule	I
gn	O
(	O
x	O
)	O
=	O
{	O
if	O
1	O
``	O
'~1_	O
(	O
2yi	O
-l	O
)	O
k	O
(	O
lix	O
-	O
xill	O
)	O
>	O
0	O
h	O
i	O
o	O
otherwise	O
,	O
n	O
l	O
...	O
.-i-l	O
2	O
where	O
ilx	O
-	O
z	O
ii	O
is	O
just	O
the	O
hamming	O
distance	B
(	O
i.e.	O
,	O
the	O
number	O
of	O
disagreements	O
between	O
components	O
of	O
x	O
and	O
z	O
)	O
.	O
with	O
k	O
(	O
u	O
)	O
=	O
e	O
_u	O
,	O
the	O
rule	B
above	O
reduces	O
to	O
a	O
rule	O
given	O
in	O
aitchison	O
and	O
aitken	O
(	O
1976	O
)	O
.	O
in	O
their	O
paper	O
,	O
different	O
h	O
'	O
s	O
are	O
considered	O
for	O
the	O
two	O
classes	O
,	O
but	O
we	O
wo	O
n't	O
consider	O
that	O
distinction	O
here	O
.	O
observe	O
that	O
at	O
h	O
=	O
0	O
,	O
we	O
obtain	O
the	O
fundamental	B
rule	I
.	O
as	O
h	O
~	O
00	O
,	O
we	O
obtain	O
a	O
(	O
degenerate	O
)	O
majority	O
rule	O
over	O
the	O
entire	O
sample	O
.	O
the	O
weight	O
given	O
to	O
an	O
observation	O
xi	O
decreases	O
exponentially	O
in	O
ilx	O
-	O
xi	O
112.	O
for	O
consistency	B
,	O
we	O
merely	O
need	O
h	O
~	O
0	O
(	O
the	O
condition	O
nh	O
d	O
~	O
00	O
of	O
theorem	O
10.1	O
is	O
no	O
longer	O
needed	O
)	O
.	O
and	O
in	O
fact	O
,	O
we	O
even	O
have	O
consistency	B
with	O
h	O
==	O
0	O
as	O
this	O
yields	O
the	O
fundamental	B
rule	I
.	O
the	O
data-based	B
choice	O
of	O
h	O
has	O
been	O
the	O
object	O
of	O
several	O
papers	O
,	O
including	O
hall	O
(	O
1981	O
)	O
and	O
hall	O
and	O
wand	O
(	O
1988	O
)	O
.	O
in	O
the	O
latter	O
paper	O
,	O
a	O
mean	O
squared	B
error	I
criterion	O
is	O
minimized	O
.	O
we	O
only	O
mention	O
the	O
work	O
of	O
tutz	O
(	O
1986	O
;	O
1988	O
;	O
1989	O
)	O
,	O
who	O
picks	O
h	O
so	O
as	O
to	O
minimize	O
the	O
deleted	B
estimate	O
l~d	O
)	O
.	O
theorem	B
27.6	O
.	O
(	O
tutz	O
(	O
1986	O
»	O
.	O
let	O
hn	O
be	O
the	O
smoothing	B
factor	I
in	O
the	O
aitchison	O
(	O
cid:173	O
)	O
aitken	O
rule	B
that	O
minimizes	O
l~	O
)	O
.	O
then	O
the	O
rule	B
is	O
weakly	O
consistent	O
for	O
all	O
distri	O
(	O
cid:173	O
)	O
butions	O
of	O
(	O
x	O
,	O
y	O
)	O
on	O
the	O
hypercube	O
.	O
proof	O
.	O
see	O
theorem	B
25.8	O
.	O
0	O
problems	O
and	O
exercises	O
problem	O
27.1.	O
the	O
fundamental	B
rule	I
.	O
let	O
g	O
,	O
:	O
be	O
the	O
fundamental	B
rule	I
on	O
a	O
finite	O
set	O
{	O
i	O
,	O
...	O
,	O
k	O
}	O
,	O
and	O
define	O
l/1	O
=	O
l	O
(	O
g	O
,	O
:	O
)	O
.	O
let	O
g*	O
be	O
the	O
bayes	O
rule	B
(	O
with	O
error	O
probability	O
l	O
*	O
)	O
,	O
and	O
let	O
~	O
=	O
inf	O
(	O
~-	O
min	O
(	O
r	O
]	O
(	O
x	O
)	O
,	O
1	O
-	O
x:1	O
)	O
(	O
x	O
}	O
fij2	O
2	O
r	O
]	O
(	O
x	O
»	O
)	O
.	O
let	O
l~r	O
)	O
be	O
the	O
resubstitution	B
error	O
estimate	B
(	O
or	O
apparent	B
error	I
rate	I
)	O
.	O
show	O
the	O
following	O
:	O
(	O
1	O
)	O
e	O
{	O
l~r	O
)	O
!	O
:	O
:	O
:	O
eln	O
(	O
the	O
apparent	B
error	I
rate	I
is	O
always	O
optimistic	O
;	O
hills	O
(	O
1966	O
»	O
.	O
(	O
2	O
)	O
e	O
{	O
l~r	O
)	O
:	O
:	O
:	O
l	O
*	O
(	O
the	O
apparent	B
error	I
rate	I
is	O
in	O
fact	O
very	O
optimistic	O
;	O
glick	O
(	O
1973	O
»	O
.	O
(	O
3	O
)	O
eln	O
:	O
:	O
:	O
l	O
*	O
+	O
e-2n	O
/	O
:	O
:,2	O
(	O
for	O
a	O
similar	O
inequality	B
related	O
to	O
hellinger	O
distances	O
,	O
see	O
glick	O
(	O
1973	O
»	O
.	O
this	O
is	O
an	O
exponential	B
but	O
distribution-dependent	O
error	O
rate	O
.	O
problem	O
27.2.	O
discrete	O
lipschitz	O
classes	O
.	O
consider	O
the	O
class	O
of	O
regression	O
functions	O
r	O
]	O
e	O
[	O
0	O
,	O
1	O
]	O
with	O
i	O
r	O
]	O
(	O
x	O
)	O
-	O
r	O
]	O
(	O
z	O
)	O
i	O
:	O
:	O
:	O
cp	O
(	O
x	O
,	O
zt	O
,	O
where	O
x	O
,	O
z	O
e	O
{	O
o	O
,	O
l	O
}	O
d	O
,	O
p	O
(	O
.	O
,	O
.	O
)	O
denotes	O
the	O
hamming	O
distance	B
,	O
and	O
c	O
>	O
°	O
and	O
a	O
>	O
°	O
are	O
constants	O
(	O
note	O
that	O
a	O
is	O
not	O
bounded	O
from	O
above	O
)	O
.	O
the	O
purpose	O
is	O
to	O
design	O
a	O
discrimination	O
rule	B
for	O
which	O
uniformly	O
over	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
on	O
{	O
o	O
,	O
1	O
}	O
d	O
x	O
{	O
o	O
,	O
i	O
}	O
with	O
such	O
1	O
]	O
(	O
x	O
)	O
=	O
pry	O
=	O
11x	O
=	O
x	O
}	O
,	O
we	O
have	O
problems	O
and	O
exercises	O
475	O
e	O
{	O
l	O
_	O
l	O
*	O
}	O
<	O
\ii	O
(	O
e	O
,	O
a	O
,	O
d	O
)	O
'	O
in	O
n	O
-	O
where	O
the	O
function	O
\ii	O
(	O
e	O
,	O
a	O
,	O
d	O
)	O
is	O
as	O
small	O
as	O
possible	O
.	O
note	O
:	O
for	O
e	O
=	O
1	O
,	O
the	O
class	O
contains	O
all	O
regression	O
functions	O
on	O
the	O
hypercube	O
,	O
and	O
thus	O
\11	O
(	O
1	O
,	O
a	O
,	O
d	O
)	O
=	O
1.075	O
.	O
2d/2	O
(	O
theorem	B
27.1	O
)	O
.	O
how	O
small	O
should	O
e	O
be	O
to	O
make	O
\ii	O
polynomial	B
in	O
d	O
?	O
problem	O
27.3.	O
with	O
a	O
cubic	O
histogram	O
partition	O
of	O
[	O
0	O
,	O
1jd	O
into	O
kd	O
cells	O
(	O
of	O
volume	O
1/	O
kd	O
each	O
)	O
,	O
we	O
have	O
,	O
for	O
the	O
lipschitz	O
(	O
e	O
,	O
a	O
)	O
class	O
:	O
f	O
of	O
theorem	O
27.4	O
,	O
sup	O
8	O
=	O
e	O
cx	O
,	O
y	O
)	O
ef	O
(	O
~	O
)	O
a	O
k	O
-	O
this	O
grows	O
as	O
d	O
a	O
hint	O
:	O
consult	O
conway	O
and	O
sloane	O
(	O
1993	O
)	O
.	O
2	O
/	O
.	O
can	O
you	O
define	O
a	O
partition	O
into	O
k	O
d	O
cells	O
for	O
which	O
supcx	O
,	O
y	O
)	O
ef	O
8	O
is	O
smaller	O
?	O
problem	O
27.4.	O
consider	O
the	O
following	O
randomized	B
histogram	O
rule	B
:	O
xl	O
,	O
...	O
,	O
x	O
k	O
partition	B
[	O
0	O
,	O
1	O
]	O
d	O
into	O
polyhedra	O
based	O
on	O
the	O
nearest	B
neighbor	I
rule	I
.	O
within	O
each	O
cell	O
,	O
we	O
employ	O
a	O
majority	O
rule	B
based	O
upon	O
x	O
k+l	O
,	O
•.•	O
,	O
x	O
n	O
.	O
if	O
x	O
is	O
uniform	B
on	O
[	O
0	O
,	O
1	O
]	O
d	O
and	O
1	O
]	O
is	O
lipschitz	O
(	O
e	O
,	O
a	O
)	O
(	O
as	O
in	O
theorem	O
27.4	O
)	O
,	O
then	O
can	O
you	O
derive	O
an	O
upper	O
bound	O
for	O
e	O
{	O
l	O
n	O
-	O
l	O
*	O
}	O
as	O
a	O
function	O
of	O
k	O
,	O
n	O
,	O
e	O
,	O
a	O
,	O
and	O
d	O
?	O
how	O
does	O
your	O
bound	O
compare	O
with	O
the	O
cubic	B
histogram	O
rule	B
that	O
uses	O
the	O
same	O
number	O
(	O
k	O
)	O
of	O
cells	O
?	O
problem	O
27.5.	O
let	O
:	O
f	O
be	O
the	O
class	O
of	O
all	O
lipschitz	O
(	O
e	O
,	O
a	O
)	O
functions	O
1	O
]	O
'	O
e	O
nd	O
-+	O
[	O
0	O
,	O
1j	O
.	O
let	O
(	O
x	O
,	O
y	O
)	O
e	O
:	O
f	O
denote	O
the	O
fact	O
that	O
(	O
x	O
,	O
y	O
)	O
has	O
regression	B
function	I
1	O
]	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
11x	O
=	O
x	O
}	O
in	O
f.	O
then	O
,	O
for	O
any	O
cubic	B
histogram	O
rule	B
,	O
show	O
that	O
sup	O
e	O
{	O
cx	O
,	O
y	O
)	O
ef	O
*	O
1	O
ln	O
-	O
l	O
}	O
2	O
:	O
-	O
2	O
*	O
-	O
l	O
.	O
thus	O
,	O
the	O
compactness	O
condition	O
on	O
the	O
space	O
is	O
essential	O
for	O
the	O
distribution-free	O
error	O
bound	O
given	O
in	O
theorem	O
27.4.	O
problem	O
27.6.	O
independent	O
model	O
.	O
show	O
that	O
in	O
the	O
independent	O
model	O
,	O
the	O
maximum	B
likelihood	I
estimate	O
p	O
(	O
i	O
)	O
of	O
p	O
(	O
i	O
)	O
is	O
given	O
by	O
r	O
:	O
~=l	O
i	O
{	O
xjil=i	O
,	O
yj=l	O
}	O
r	O
:	O
~=l	O
i	O
{	O
yj=l	O
}	O
problem	O
27.7.	O
independent	O
model	O
.	O
for	O
the	O
plug-in	O
rule	O
in	O
the	O
independent	O
model	O
,	O
is	O
it	O
true	O
that	O
e	O
{	O
ln	O
-	O
l	O
*	O
}	O
=	O
0	O
(	O
1/	O
in	O
)	O
uniformly	O
over	O
all	O
pairs	O
(	O
x	O
,	O
y	O
)	O
on	O
{	O
o	O
,	O
i	O
}	O
d	O
x	O
{	O
o	O
,	O
i	O
}	O
?	O
if	O
so	O
,	O
find	O
a	O
constant	O
e	O
depending	O
upon	O
d	O
only	O
,	O
such	O
that	O
e	O
{	O
l	O
n	O
-	O
l	O
*	O
}	O
:	O
s	O
e/	O
-jri	O
.	O
if	O
not	O
,	O
provide	O
a	O
counterexample	O
.	O
problem	O
27.8.	O
consider	O
a	O
hypercube	O
problem	O
in	O
which	O
x	O
=	O
(	O
x	O
(	O
1	O
)	O
,	O
...	O
,	O
xcd	O
)	O
and	O
each	O
xci	O
)	O
e	O
{	O
-1	O
,	O
0	O
,	O
1	O
}	O
(	O
a	O
ternary	O
generalization	O
)	O
.	O
assume	O
that	O
the	O
xu	O
)	O
's	O
are	O
independent	O
but	O
not	O
476	O
27.	O
hypercubes	O
and	O
discrete	O
spaces	O
necessarily	O
identically	O
distributed	O
.	O
show	O
that	O
there	O
exists	O
a	O
quadratic	O
bayes	O
rule	B
,	O
i.e.	O
,	O
g*	O
(	O
x	O
)	O
is	O
1	O
on	O
the	O
set	O
d	O
ao	O
+	O
l	O
aix	O
(	O
i	O
)	O
+	O
l	O
aijx	O
(	O
i	O
)	O
x	O
(	O
j	O
)	O
>	O
0	O
,	O
d	O
i=	O
]	O
i	O
,	O
j=l	O
where	O
ao	O
,	O
{	O
ai	O
}	O
,	O
and	O
{	O
aij	O
}	O
are	O
some	O
weights	O
(	O
kazmierczak	O
and	O
steinbuch	O
(	O
1963	O
»	O
.	O
problem	O
27.9.	O
let	O
a	O
be	O
the	O
class	O
of	O
all	O
sets	O
on	O
the	O
hypercube	O
{	O
a	O
,	O
1	O
}	O
d	O
of	O
the	O
form	O
xci	O
!	O
)	O
...	O
x	O
(	O
ik	O
)	O
=	O
1	O
,	O
where	O
(	O
x	O
(	O
l	O
)	O
,	O
...	O
,	O
xed	O
»	O
~	O
e	O
{	O
a	O
,	O
l	O
}	O
d	O
,	O
1	O
:	O
:	O
:	O
i1	O
<	O
'	O
''	O
<	O
ik	O
:	O
:	O
:	O
d.	O
(	O
thus	O
,	O
a	O
is	O
the	O
class	O
of	O
all	O
sets	O
carved	O
out	O
by	O
the	O
monomials	O
.	O
)	O
show	O
that	O
the	O
vc	B
dimension	I
of	O
cis	O
d.	O
hint	O
:	O
the	O
set	O
{	O
co	O
,	O
1	O
,	O
1	O
,	O
...	O
,1	O
)	O
,	O
(	O
1	O
,	O
0,1	O
,	O
...	O
,1	O
)	O
,	O
...	O
,	O
(	O
1,1,1	O
,	O
...	O
,	O
a	O
)	O
}	O
is	O
shattered	O
by	O
a.	O
no	O
set	O
of	O
size	O
d	O
+	O
1	O
can	O
be	O
shattered	O
by	O
a	O
by	O
the	O
pigeonhole	B
principle	I
.	O
problem	O
27.10.	O
show	O
that	O
the	O
catalan	O
number	O
1	O
(	O
2n	O
)	O
4n	O
-	O
-	O
n+	O
i	O
n	O
~.	O
''	O
'	O
-	O
'	O
-	O
-	O
(	O
see	O
,	O
e.g.	O
,	O
kemp	O
(	O
1984	O
)	O
.	O
)	O
problem	O
27.11.	O
provide	O
an	O
argument	O
to	O
show	O
that	O
the	O
number	O
of	O
boolean	O
functions	O
with	O
at	O
most	O
k	O
operations	O
``	O
and	O
''	O
or	O
``	O
or	O
''	O
and	O
d	O
operands	O
of	O
the	O
form	O
xu	O
)	O
or	O
i	O
-	O
x	O
(	O
i	O
)	O
,	O
xci	O
)	O
e	O
{	O
a	O
,	O
i	O
}	O
,	O
is	O
not	O
more	O
than	O
k+	O
1	O
k	O
(	O
this	O
is	O
2k	O
times	O
less	O
than	O
the	O
bound	O
given	O
in	O
the	O
text	O
)	O
.	O
2	O
(	O
2di+1	O
_1_	O
(	O
2k	O
)	O
problem	O
27.12.	O
provide	O
upper	O
and	O
lower	O
bounds	O
on	O
the	O
vc	B
dimension	I
of	O
the	O
class	O
of	O
sets	O
a	O
on	O
the	O
hypercube	O
{	O
a	O
,	O
l	O
}	O
d	O
that	O
can	O
be	O
described	O
by	O
a	O
boolean	O
expression	B
with	O
the	O
x	O
(	O
i	O
)	O
's	O
or	O
1	O
-	O
x	O
(	O
i	O
)	O
's	O
as	O
operands	O
and	O
with	O
at	O
most	O
k	O
operations	O
``	O
and	O
''	O
or	O
``	O
or	O
.	O
''	O
problem	O
27.13.	O
linear	O
discrimination	O
on	O
the	O
hypercube	O
.	O
let	O
gn	O
be	O
the	O
rule	B
that	O
mini	O
(	O
cid:173	O
)	O
mizes	O
the	O
empirical	B
error	I
ln	O
(	O
¢	O
)	O
over	O
all	O
linear	O
rules	O
¢	O
,	O
when	O
the	O
data	O
are	O
drawn	O
from	O
any	O
distribution	B
on	O
{	O
a	O
,	O
l	O
}	O
d	O
x	O
{	O
a	O
,	O
i	O
}	O
.	O
let	O
ln	O
be	O
its	O
probability	O
of	O
error	O
,	O
and	O
let	O
l	O
be	O
the	O
minimum	O
error	O
probability	O
over	O
all	O
linear	O
rules	O
.	O
show	O
that	O
for	O
e	O
>	O
0	O
,	O
deduce	O
that	O
e	O
{	O
l	O
n	O
-	O
l	O
}	O
:	O
:	O
:	O
2	O
j	O
1	O
+	O
log4	O
(	O
~	O
)	O
ffn	O
d	O
+	O
1	O
:	O
:	O
:	O
2	O
--	O
.	O
ffn	O
compare	O
this	O
result	O
with	O
the	O
general	O
vapnik-chervonenkis	O
bound	O
for	O
linear	O
rules	O
(	O
theorem	B
13.11	O
)	O
and	O
deduce	O
when	O
the	O
bound	O
given	O
above	O
is	O
better	O
.	O
hint	O
:	O
count	O
the	O
number	O
of	O
possible	O
linear	O
rules	O
.	O
problem	O
27.14.	O
on	O
the	O
hypercube	O
to	O
,	O
l	O
}	O
d	O
,	O
show	O
that	O
the	O
kernel	B
rule	I
of	O
aitchison	O
and	O
aitken	O
(	O
1976	O
)	O
is	O
strongly	O
consistent	O
when	O
limn-+oo	O
h	O
=	O
0.	O
problem	O
27	O
.15.	O
pick	O
h	O
in	O
the	O
kernel	B
estimate	O
by	O
minimizing	O
the	O
resubstitution	B
estimate	O
l~r	O
)	O
,	O
and	O
call	O
it	O
hy	O
)	O
.	O
for	O
l~d	O
)	O
,	O
we	O
call	O
it	O
h~d	O
)	O
.	O
assume	O
that	O
the	O
kernel	B
function	O
is	O
of	O
the	O
form	O
k	O
(	O
ii	O
.	O
iii	O
h	O
)	O
with	O
k	O
~	O
0	O
,	O
k	O
(	O
u	O
)	O
-t	O
°	O
as	O
u	O
t	O
00.	O
let	O
ln	O
be	O
the	O
error	O
estimate	O
for	O
the	O
kernel	B
rule	I
with	O
one	O
of	O
these	O
two	O
choices	O
.	O
is	O
it	O
possible	O
to	O
find	O
a	O
constant	O
c	O
,	O
depending	O
upon	O
d	O
and	O
k	O
only	O
,	O
such	O
that	O
problems	O
and	O
exercises	O
477	O
e	O
{	O
ln	O
-	O
inf	O
ln	O
(	O
h	O
)	O
}	O
:	O
s	O
~	O
?	O
h	O
:	O
:	O
:	O
.o	O
yn	O
if	O
so	O
,	O
give	O
a	O
proof	O
.	O
if	O
not	O
,	O
provide	O
a	O
counterexample	O
.	O
note	O
:	O
if	O
the	O
answer	O
is	O
positive	O
,	O
a	O
minor	O
corollary	O
of	O
this	O
result	O
is	O
tutz	O
's	O
theorem	B
.	O
however	O
,	O
an	O
explicit	O
constant	O
c	O
may	O
aid	O
in	O
determining	O
appropriate	O
sample	O
sizes	O
.	O
it	O
may	O
also	O
be	O
minimized	O
with	O
respect	O
to	O
k.	O
problem	O
27.16	O
.	O
(	O
simon	O
(	O
1991	O
)	O
.	O
)	O
construct	O
a	O
partition	O
of	O
the	O
hypercube	O
{	O
a	O
,	O
l	O
}	O
d	O
in	O
the	O
fol	O
(	O
cid:173	O
)	O
lowing	O
manner	O
,	O
based	O
upon	O
a	O
binary	O
classification	O
tree	B
with	O
perpendicular	O
splits	O
:	O
every	O
node	O
at	O
level	O
i	O
splits	O
the	O
subset	O
according	O
to	O
x	O
(	O
i	O
)	O
=	O
°	O
or	O
x	O
(	O
i	O
)	O
=	O
1	O
,	O
so	O
that	O
there	O
are	O
at	O
most	O
d	O
levels	O
of	O
nodes	O
.	O
(	O
in	O
practice	O
,	O
the	O
most	O
important	O
component	O
should	O
be	O
x	O
(	O
l	O
)	O
.	O
)	O
for	O
example	O
,	O
all	O
possible	O
partitions	O
of	O
{	O
a	O
,	O
1	O
}	O
2	O
obtainable	O
with	O
2	O
cuts	O
are	O
..	O
.	O
•	O
•	O
..	O
.	O
:	O
~	O
~	O
:	O
•	O
i	O
•	O
~	O
assign	O
to	O
each	O
internal	O
node	O
(	O
each	O
region	O
)	O
a	O
class	O
.	O
define	O
the	O
horton-strahler	O
number	O
~	O
of	O
a	O
tree	O
as	O
follows	O
:	O
if	O
a	O
tree	O
has	O
one	O
node	O
,	O
then	O
~	O
=	O
0.	O
if	O
the	O
root	O
of	O
the	O
tree	B
has	O
left	O
and	O
right	O
subtrees	O
with	O
horton-strahler	O
numbers	O
~1	O
and	O
~2	O
'	O
then	O
set	O
let	O
c	O
be	O
the	O
class	O
of	O
classifiers	O
g	O
described	O
above	O
with	O
horton-strahler	O
number	O
:	O
s	O
~	O
.	O
(	O
1	O
)	O
let	O
s	O
=	O
{	O
x	O
e	O
{	O
a	O
,	O
1	O
}	O
d	O
:	O
ilx	O
ii	O
:	O
s	O
n	O
,	O
where	O
ii	O
.	O
ii	O
denotes	O
hamming	O
distance	B
from	O
the	O
all-zero	O
vector	O
.	O
show	O
that	O
s	O
is	O
shattered	O
by	O
the	O
class	O
of	O
sets	O
{	O
g	O
=	O
1	O
:	O
g	O
e	O
c	O
}	O
.	O
(	O
2	O
)	O
show	O
that	O
lsi	O
=	O
ito	O
e	O
)	O
·	O
(	O
3	O
)	O
conclude	O
that	O
the	O
vc	B
dimension	I
of	O
c	O
is	O
at	O
least	O
2	O
:	O
;	O
==0	O
(	O
~	O
)	O
.	O
(	O
simon	O
has	O
shown	O
that	O
the	O
vc	B
dimension	I
of	O
c	O
is	O
exactly	O
this	O
,	O
but	O
that	O
proof	O
is	O
more	O
involved	O
.	O
)	O
(	O
4	O
)	O
assuming	O
l	O
*	O
=	O
0	O
,	O
obtain	O
an	O
upper	O
bound	O
for	O
e	O
{	O
l	O
n	O
}	O
as	O
afunctionof~	O
andd	O
,	O
where	O
ln	O
is	O
the	O
probability	O
of	O
eltor	O
for	O
the	O
rule	B
picked	O
by	O
minimizing	O
the	O
empirical	B
eltor	O
over	O
c.	O
interpret	O
~	O
as	O
the	O
height	B
of	I
the	O
largest	O
complete	O
binary	O
tree	B
that	O
can	O
be	O
embedded	O
in	O
the	O
classification	O
tree	B
.	O
(	O
5	O
)	O
28	O
epsilon	O
entropy	B
and	O
totally	O
bounded	O
sets	O
28.1	O
definitions	O
this	O
chapter	O
deals	O
with	O
discrimination	O
rules	O
that	O
are	O
picked	O
from	O
a	O
certain	O
class	O
of	O
classifiers	O
by	O
minimizing	O
the	O
empirical	B
probability	O
of	O
error	O
over	O
a	O
finite	O
set	O
of	O
carefully	O
selected	O
rules	O
.	O
we	O
begin	O
with	O
a	O
class	O
f	O
of	O
regression	O
functions	O
(	O
i.e.	O
,	O
a	B
posteriori	I
probability	I
functions	O
)	O
tj	O
:	O
nd	O
-+	O
[	O
0	O
,	O
1	O
]	O
from	O
which	O
tjn	O
will	O
be	O
picked	O
by	O
the	O
data	O
.	O
the	O
massiveness	O
of	O
f	O
can	O
be	O
measured	O
in	O
many	O
ways-the	O
route	O
followed	O
here	O
is	O
suggested	O
in	O
the	O
work	O
of	O
kolmogorov	O
and	O
tikhomirov	O
(	O
1961	O
)	O
.	O
we	O
will	O
depart	O
from	O
their	O
work	O
only	O
in	O
details	O
.	O
we	O
suggest	O
comparing	O
the	O
results	O
here	O
with	O
those	O
from	O
chapters	O
12	O
and	O
15.	O
let	O
fe	O
=	O
{	O
tj	O
(	O
l	O
)	O
,	O
...	O
,	O
tj	O
(	O
n	O
)	O
}	O
be	O
a	O
finite	O
collection	O
of	O
functions	O
nd	O
-+	O
[	O
0,1	O
]	O
such	O
that	O
where	O
st	O
)	O
i	O
,	O
e	O
is	O
the	O
ball	O
of	O
all	O
functions	O
~	O
:	O
nd	O
-+	O
[	O
0	O
,	O
1	O
]	O
with	O
ii~	O
-	O
tj'iioo	O
=	O
sup	O
i~	O
(	O
x	O
)	O
-	O
tj	O
'	O
(	O
x	O
)	O
1	O
<	O
e.	O
x	O
in	O
other	O
words	O
,	O
for	O
each	O
tj	O
'	O
e	O
f	O
,	O
there	O
exists	O
an	O
tju	O
)	O
e	O
fe	O
with	O
supx	O
itj	O
'	O
(	O
x	O
)	O
-	O
tj	O
(	O
i	O
)	O
(	O
x	O
)	O
i	O
<	O
e.	O
the	O
fewer	O
tj	O
(	O
i	O
)	O
,	O
s	O
needed	O
to	O
cover	O
f	O
,	O
the	O
smaller	O
f	O
is	O
,	O
in	O
a	O
certain	O
sense	O
.	O
fe	O
is	O
called	O
an	O
e	O
-cove	O
r	O
of	O
f.	O
the	O
minimal	O
value	O
of	O
i	O
fe	O
lover	O
all	O
e	O
-covers	O
is	O
called	O
the	O
e-covering	O
number	O
(	O
ne	O
)	O
.	O
following	O
kolmogorov	O
and	O
tikhomirov	O
(	O
1961	O
)	O
,	O
480	O
28.	O
epsilon	O
entropy	B
and	O
totally	O
bounded	O
sets	O
is	O
called	O
the	O
e	O
-entropy	O
of	O
f.	O
we	O
will	O
also	O
call	O
it	O
the	O
metric	B
entropy	I
.	O
a	O
collection	O
f	O
is	O
totally	O
bounded	O
if	O
ne	O
<	O
00	O
for	O
all	O
e	O
>	O
o.	O
it	O
is	O
with	O
such	O
classes	O
that	O
we	O
are	O
concerned	O
in	O
this	O
chapter	O
.	O
the	O
next	O
section	O
gives	O
a	O
few	O
examples	O
.	O
in	O
the	O
following	O
section	O
,	O
we	O
define	O
the	O
skeleton	B
estimate	I
based	O
upon	O
picking	O
the	O
empirically	O
best	O
member	O
from	O
fe	O
'	O
figure	O
28.1.	O
an	O
e	O
-cover	O
of	O
the	O
unit	O
square	O
.	O
28.2	O
examples	O
:	O
totally	O
bounded	O
classes	O
the	O
simple	O
scalar	O
parametric	O
class	O
f	O
=	O
{	O
e-e1x	O
1	O
,	O
x	O
e	O
r	O
;	O
8	O
>	O
o	O
}	O
is	O
not	O
totally	O
bounded	O
(	O
problem	O
28.1	O
)	O
.	O
this	O
is	O
due	O
simply	O
to	O
the	O
presence	O
of	O
an	O
unrestricted	O
scale	O
factor	O
.	O
it	O
would	O
still	O
fail	O
to	O
be	O
totally	O
bounded	O
if	O
we	O
restricted	O
8	O
to	O
[	O
1	O
,	O
(	O
0	O
)	O
or	O
[	O
0	O
,	O
1	O
]	O
.	O
however	O
,	O
if	O
we	O
force	O
8	O
e	O
[	O
0	O
,	O
1	O
]	O
and	O
change	O
the	O
class	O
f	O
to	O
have	O
functions	O
,	O
1	O
]	O
(	O
x	O
)	O
=	O
0	O
{	O
e-b1x1	O
if	O
ixl	O
:	O
:	O
:	O
:	O
1	O
otherwise	O
,	O
then	O
the	O
class	O
is	O
totally	O
bounded	O
.	O
while	O
it	O
is	O
usually	O
difficult	O
to	O
compute	O
n	O
:	O
exactly	O
,	O
it	O
is	O
often	O
simple	O
to	O
obtain	O
matching	O
upper	O
and	O
lower	O
bounds	O
.	O
here	O
is	O
a	O
simple	O
argument	O
.	O
take	O
8i	O
=	O
2e	O
i	O
,	O
0	O
:	O
:	O
:	O
:	O
i	O
:	O
:	O
:	O
:	O
l	O
i	O
j	O
(	O
2e	O
)	O
j	O
and	O
define	O
8	O
*	O
=	O
1.	O
each	O
of	O
these	O
8-values	O
defines	O
a	O
function	O
1	O
]	O
'	O
.	O
collect	O
these	O
in	O
fe	O
'	O
note	O
that	O
with	O
1	O
]	O
'	O
e	O
f	O
,	O
with	O
parameter	O
8	O
,	O
if	O
e	O
is	O
the	O
nearest	O
value	O
among	O
{	O
8i	O
,	O
0	O
:	O
:	O
:	O
:	O
i	O
:	O
:	O
:	O
:	O
l	O
1	O
j	O
(	O
2e	O
)	O
j	O
}	O
u	O
{	O
8	O
*	O
}	O
,	O
then	O
ie	O
-	O
81	O
:	O
:	O
:	O
:	O
e.	O
but	O
then	O
sup	O
je-e1xl	O
-	O
e-e1xlj	O
:	O
:	O
:	O
:	O
18	O
-	O
el	O
:	O
:	O
:	O
:	O
e.	O
ixisl	O
hence	O
fe	O
is	O
an	O
e-cover	O
of	O
f	O
of	O
cardinality	O
l1j	O
(	O
2e	O
)	O
j	O
+	O
2.	O
we	O
conclude	O
that	O
f	O
is	O
totally	O
bounded	O
,	O
and	O
that	O
(	O
see	O
problem	O
28.2	O
for	O
a	O
d-dimensional	O
generalization	O
.	O
)	O
ne	O
:	O
:	O
:	O
:	O
2	O
+	O
l1j	O
(	O
2e	O
)	O
j	O
.	O
28.2	O
examples	O
:	O
totally	O
bounded	O
classes	O
481	O
for	O
a	O
lower	O
bound	O
,	O
we	O
use	O
once	O
again	O
an	O
idea	O
from	O
kolmogorov	O
and	O
tikhomirov	O
(	O
1961	O
)	O
.	O
let	O
oe	O
=	O
{	O
1	O
}	O
(	O
l	O
)	O
,	O
...	O
,	O
1	O
}	O
(	O
m	O
)	O
}	O
c	O
f	O
be	O
a	O
subset	O
with	O
the	O
property	O
that	O
for	O
every	O
i	O
-	O
:	O
:j	O
j	O
,	O
supx	O
11	O
}	O
(	O
i	O
)	O
(	O
x	O
)	O
-	O
1	O
}	O
(	O
j	O
)	O
(	O
x	O
)	O
i	O
:	O
:	O
:	O
:	O
:	O
e.	O
the	O
set	O
oe	O
is	O
thus	O
e-separated	O
.	O
the	O
maximal	O
cardinality	O
e	O
-separated	O
set	O
is	O
called	O
the	O
e	O
-packing	O
number	O
(	O
or	O
e	O
-separation	O
number	O
)	O
me	O
.	O
it	O
is	O
easy	O
to	O
see	O
that	O
m2e	O
:	O
:	O
:	O
:	O
:	O
ne	O
:	O
:	O
:	O
:	O
:	O
me	O
(	O
problem	O
28.3	O
)	O
.	O
with	O
this	O
in	O
hand	O
,	O
we	O
see	O
that	O
oe	O
may	O
be	O
constructed	O
as	O
follows	O
for	O
our	O
example	O
:	O
begin	O
with	O
80	O
=	O
0.	O
then	O
define	O
81	O
by	O
e-b1	O
=	O
1	O
-	O
e	O
,	O
e-b2	O
=	O
1	O
-	O
2e	O
,	O
etcetera	O
,	O
until	O
8i	O
>	O
1.	O
it	O
is	O
clear	O
that	O
this	O
wayan	O
e	O
-separated	O
set	O
oe	O
may	O
be	O
constructed	O
with	O
ioe	O
i	O
=	O
l	O
(	O
1	O
-	O
1/	O
e	O
)	O
/	O
e	O
j	O
+	O
1.	O
thus	O
,	O
ne	O
:	O
:	O
:	O
:	O
:	O
m2e	O
:	O
:	O
:	O
:	O
:	O
l1-1/ej	O
2e	O
+	O
1.	O
the	O
e-entropy	O
of	O
f	O
grows	O
as	O
10g2	O
(	O
1/e	O
)	O
as	O
e	O
t	O
0.	O
consider	O
next	O
a	O
larger	O
class	O
,	O
not	O
of	O
a	O
parametric	O
nature	O
:	O
let	O
f	O
be	O
a	O
class	O
of	O
functions	O
1	O
}	O
on	O
[	O
0	O
,	O
ll	O
]	O
satisfying	O
the	O
lipschitz	O
condition	O
11	O
}	O
(	O
x	O
)	O
-	O
1	O
}	O
(	O
xi	O
)	O
i	O
:	O
:	O
:	O
:	O
:	O
clx	O
-	O
xii	O
and	O
taking	O
values	O
on	O
[	O
0	O
,	O
1	O
]	O
.	O
kolmogorov	O
and	O
tikhomirov	O
(	O
1961	O
)	O
showed	O
that	O
if	O
e	O
<	O
min	O
(	O
~	O
_1_	O
)	O
4	O
'	O
16llc	O
-	O
'	O
then	O
<	O
<	O
10g2	O
n	O
e	O
llc	O
-	O
e	O
+log2	O
-	O
+3	O
i	O
e	O
(	O
see	O
problem	O
28.5	O
)	O
.	O
observe	O
that	O
the	O
metric	B
entropy	I
is	O
exponentially	O
larger	O
than	O
for	O
the	O
parametric	O
class	O
considered	O
above	O
.	O
this	O
has	O
a	O
major	O
impact	O
on	O
sample	O
sizes	O
needed	O
for	O
similar	O
performances	O
(	O
see	O
the	O
next	O
sections	O
)	O
.	O
another	O
class	O
of	O
functions	O
of	O
interest	O
is	O
that	O
containing	O
functions	O
1	O
}	O
:	O
[	O
0	O
,	O
1	O
]	O
-+	O
[	O
0	O
,	O
1	O
]	O
that	O
are	O
s-times	O
differentiable	O
and	O
for	O
which	O
the	O
s-th	O
derivative	O
1	O
}	O
(	O
s	O
)	O
satisfies	O
a	O
holder	O
condition	O
of	O
order	O
ex	O
,	O
11	O
}	O
(	O
s	O
)	O
(	O
x	O
)	O
-n	O
(	O
s	O
)	O
(	O
xl	O
)	O
1	O
:	O
:	O
:	O
:	O
:	O
clx	O
-	O
xll	O
a	O
,	O
x	O
,	O
xl	O
e	O
[	O
0	O
,	O
1	O
]	O
,	O
where	O
c	O
is	O
a	O
constant	O
.	O
in	O
that	O
case	O
,	O
10g2	O
me	O
and	O
log2ne	O
are	O
both	O
8	O
(	O
e-1/	O
(	O
s+a	O
»	O
)	O
as	O
e	O
t	O
0	O
,	O
where	O
an	O
=	O
8	O
(	O
bn	O
)	O
means	O
that	O
an	O
=	O
o	O
(	O
bn	O
)	O
and	O
bn	O
=	O
o	O
(	O
an	O
)	O
.	O
this	O
result	O
,	O
also	O
due	O
to	O
kolmogorov	O
and	O
tikhomirov	O
(	O
1961	O
)	O
,	O
establishes	O
a	O
continuum	O
of	O
rates	O
of	O
increase	O
of	O
the	O
e-entropy	O
.	O
in	O
n	O
d	O
,	O
with	O
functions	O
n	O
:	O
[	O
0	O
,	O
l	O
]	O
d	O
-+	O
[	O
0	O
,	O
1	O
]	O
,	O
if	O
the	O
holder	O
condition	O
holds	O
for	O
all	O
derivatives	O
of	O
order	O
s	O
,	O
then	O
log2	O
ne	O
=	O
8	O
(	O
e-d/	O
(	O
s+a	O
»	O
)	O
.	O
482	O
28.	O
epsilon	O
entropy	B
and	O
totally	O
bounded	O
sets	O
here	O
we	O
have	O
an	O
interesting	O
interpretation	O
of	O
dimension	O
:	O
doubling	O
the	O
dimension	B
roughly	O
offsets	O
the	O
effect	O
of	O
doubling	O
the	O
number	O
of	O
derivatives	O
(	O
or	O
the	O
degree	O
of	O
smoothness	O
)	O
of	O
the	O
1	O
]	O
's	O
.	O
working	O
with	O
lipschitz	O
functions	O
on	O
rl	O
is	O
roughly	O
equivalent	O
to	O
working	O
with	O
functions	O
on	O
r	O
25	O
for	O
which	O
all	O
24-th	O
order	O
derivatives	O
are	O
lipschitz	O
!	O
as	O
there	O
are	O
2524	O
such	O
derivatives	O
,	O
we	O
note	O
immediately	O
how	O
much	O
we	O
must	O
pay	O
for	O
certain	O
performances	O
in	O
high	O
dimensions	O
.	O
let	O
:	O
f	O
be	O
the	O
class	O
of	O
all	O
entire	O
analytic	O
functions	O
1	O
]	O
:	O
[	O
0	O
,	O
2	O
;	O
r	O
]	O
--	O
--	O
+	O
[	O
0	O
,	O
1	O
]	O
whose	O
periodic	O
continuation	O
satisfies	O
for	O
some	O
constants	O
c	O
,	O
a	O
(	O
z	O
is	O
a	O
complex	O
variable	B
,	O
~	O
(	O
z	O
)	O
is	O
its	O
imaginary	O
part	O
)	O
.	O
for	O
this	O
class	O
,	O
we	O
know	O
that	O
1	O
10g2	O
)	O
\	O
(	O
'	O
''	O
'-	O
'	O
(	O
4laj+2	O
)	O
log-	O
e	O
as	O
e+o	O
(	O
kolmogorov	O
and	O
tikhomirov	O
(	O
1961	O
)	O
)	O
.	O
the	O
class	O
appears	O
to	O
be	O
as	O
small	O
as	O
our	O
parametric	O
class	O
.	O
see	O
also	O
vitushkin	O
(	O
1961	O
)	O
.	O
28.3	O
skeleton	B
estimates	I
the	O
members	O
of	O
:	O
fe	O
form	O
a	O
representative	O
skeleton	O
of	O
f.	O
we	O
assume	O
that	O
fe	O
c	O
f	O
(	O
this	O
condition	O
was	O
not	O
imposed	O
in	O
the	O
definition	B
of	I
an	O
e-cover	O
)	O
.	O
for	O
each	O
1	O
]	O
'	O
e	O
f	O
,	O
we	O
define	O
its	O
discrimination	O
rule	B
as	O
(	O
x	O
)	O
=	O
{	O
1	O
g	O
if	O
1	O
]	O
'	O
(	O
x	O
)	O
.	O
>	O
1/2	O
0	O
otherwise	O
.	O
thus	O
,	O
we	O
will	O
take	O
the	O
liberty	O
of	O
referring	O
to	O
1	O
]	O
'	O
as	O
a	O
rule	O
.	O
for	O
each	O
such	O
1	O
]	O
'	O
,	O
we	O
define	O
the	O
probability	O
of	O
error	O
as	O
usual	O
:	O
l	O
(	O
1	O
]	O
'	O
)	O
=	O
p	O
{	O
g	O
(	O
x	O
)	O
=i	O
y	O
}	O
.	O
the	O
empirical	B
probability	O
of	O
error	O
of	O
1	O
]	O
'	O
is	O
denoted	O
by	O
we	O
define	O
the	O
skeleton	B
estimate	I
1	O
]	O
n	O
by	O
1	O
]	O
n	O
=	O
argminln	O
(	O
l'7	O
'	O
)	O
.	O
r/'efe	O
one	O
of	O
the	O
best	O
rules	O
in	O
:	O
f	O
is	O
denoted	O
by	O
1	O
]	O
*	O
:	O
l	O
(	O
1	O
]	O
*	O
)	O
2	O
:	O
l	O
(	O
1	O
]	O
'	O
)	O
,	O
1	O
]	O
'	O
e	O
f.	O
28.3	O
skeleton	B
estimates	I
483	O
the	O
first	O
objective	O
,	O
as	O
in	O
standard	O
empirical	B
risk	I
minimization	I
,	O
is	O
to	O
ensure	O
that	O
l	O
(	O
1	O
]	O
n	O
)	O
is	O
close	O
to	O
l	O
(	O
rj*	O
)	O
.	O
if	O
the	O
true	O
a	B
posteriori	I
probability	I
function	O
rj	O
is	O
in	O
f	O
(	O
recall	O
that	O
1	O
]	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
11x	O
=	O
x	O
n	O
,	O
then	O
it	O
is	O
clear	O
that	O
l	O
*	O
=	O
l	O
(	O
rj*	O
)	O
.	O
it	O
will	O
be	O
seen	O
from	O
the	O
theorem	B
below	O
that	O
under	O
this	O
condition	O
,	O
the	O
skeleton	B
estimate	I
has	O
nice	O
consistency	B
and	O
rate-of-convergence	O
properties	O
.	O
the	O
result	O
is	O
distribution-free	O
in	O
the	O
sense	O
that	O
no	O
structure	O
on	O
the	O
distribution	B
of	O
x	O
is	O
assumed	O
.	O
problems	O
6.9	O
and	O
28.9	O
show	O
that	O
convergence	O
of	O
l	O
(	O
rjn	O
)	O
to	O
l	O
(	O
rj*	O
)	O
for	O
all	O
rj's-that	O
is	O
,	O
not	O
only	O
for	O
those	O
in	O
f-holds	O
if	O
x	O
has	O
a	O
density	O
.	O
in	O
any	O
case	O
,	O
because	O
the	O
skeleton	B
estimate	I
is	O
selected	O
from	O
a	O
finite	O
deterministic	O
set	O
(	O
that	O
may	O
be	O
constructed	O
before	O
data	O
are	O
collected	O
!	O
)	O
,	O
probability	O
bounding	O
is	O
trivial	O
:	O
for	O
all	O
e	O
>	O
0,8	O
>	O
0	O
,	O
we	O
have	O
p	O
{	O
l	O
(	O
rjn	O
)	O
-	O
inf	O
l	O
(	O
rj	O
!	O
)	O
>	O
<	O
5	O
}	O
r/	O
,	O
efe	O
<	O
<	O
ife	O
i	O
sup	O
p	O
{	O
iln	O
(	O
rj	O
'	O
)	O
-	O
l	O
(	O
rj	O
'	O
)	O
1	O
>	O
8/2	O
}	O
tj'efe	O
(	O
by	O
lemma	O
8.2	O
)	O
21fe	O
le-	O
no2	O
/2	O
(	O
hoeffding	O
's	O
inequality	B
)	O
.	O
theorem	B
28.1.	O
let	O
f	O
be	O
a	O
totally	O
bounded	O
class	O
offunctions	O
rj	O
'	O
:	O
nd	O
-+	O
[	O
0	O
,	O
1	O
]	O
.	O
there	O
is	O
a	O
sequence	O
{	O
en	O
>	O
o	O
}	O
and	O
a	O
sequence	O
of	O
skeletons	O
fen	O
c	O
f	O
such	O
that	O
if	O
rjn	O
is	O
the	O
skeleton	B
estimate	I
drawn	O
from	O
fen	O
'	O
then	O
l	O
(	O
rjn	O
)	O
-+	O
l	O
*	O
with	O
probability	O
one	O
,	O
whenever	O
the	O
true	O
regression	B
function	I
1	O
]	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
11x	O
=	O
x	O
}	O
is	O
in	O
f.	O
it	O
suffices	O
to	O
take	O
fe	O
as	O
an	O
e	O
-cover	O
of	O
f	O
(	O
note	O
that	O
ife	O
i	O
need	O
not	O
equal	O
the	O
e	O
-covering	O
number	O
ne	O
)	O
,	O
and	O
to	O
define	O
en	O
as	O
the	O
smallest	O
positive	O
number	O
for	O
which	O
finally	O
,	O
with	O
en	O
picked	O
in	O
this	O
manner	O
,	O
e	O
{	O
l	O
(	O
ryn	O
)	O
-	O
l	O
'	O
}	O
:	O
s	O
(	O
2	O
+	O
v	O
's	O
)	O
€n	O
+	O
tf	O
;	O
.	O
proof	O
.	O
we	O
note	O
that	O
inftj'efe	O
l	O
(	O
rj	O
'	O
)	O
s	O
l	O
*	O
+	O
2e	O
,	O
because	O
if	O
rj	O
'	O
e	O
stj	O
,	O
e	O
'	O
then	O
e	O
{	O
lrj	O
'	O
(	O
x	O
)	O
-	O
1	O
]	O
(	O
x	O
)	O
i	O
}	O
s	O
sup	O
ir/	O
(	O
x	O
)	O
-	O
1	O
]	O
(	O
x	O
)	O
1	O
s	O
e	O
x	O
and	O
thus	O
,	O
by	O
theorem	B
2.2	O
,	O
l	O
(	O
1	O
]	O
'	O
)	O
-	O
l	O
*	O
s	O
2e	O
.	O
then	O
for	O
any	O
<	O
5	O
:	O
:	O
:	O
2en	O
,	O
p	O
{	O
l	O
(	O
1	O
]	O
n	O
)	O
-	O
l	O
*	O
>	O
8	O
}	O
<	O
p	O
{	O
l	O
(	O
rjn	O
)	O
-	O
inf	O
l	O
(	O
1	O
]	O
!	O
»	O
8-2en	O
}	O
1	O
]	O
'efen	O
<	O
21fen	O
le-	O
n	O
(	O
o-2eni	O
/2	O
(	O
see	O
above	O
)	O
484	O
28.	O
epsilon	O
entropy	B
and	O
totally	O
bounded	O
sets	O
which	O
is	O
summable	O
in	O
n	O
,	O
as	O
en	O
=	O
00	O
)	O
.	O
this	O
shows	O
the	O
first	O
part	O
of	O
the	O
theorem	B
.	O
for	O
the	O
second	O
part	O
,	O
we	O
have	O
e	O
{	O
l	O
(	O
1	O
'	O
}	O
n	O
)	O
}	O
-	O
l	O
*	O
<	O
<	O
<	O
<	O
=	O
2en	O
+	O
e	O
{	O
l	O
(	O
1	O
'	O
}	O
n	O
)	O
}	O
-	O
inf	O
l	O
(	O
1	O
'	O
}	O
'	O
)	O
r	O
(	O
efen	O
2en	O
+	O
100	O
min	O
(	O
2e	O
2	O
''	O
,	O
;	O
-	O
''	O
,	O
'/2	O
,	O
i	O
)	O
dt	O
(	O
2	O
+	O
v	O
's	O
)	O
en	O
+	O
21	O
00	O
e2ne~-nt2	O
!	O
2dt	O
(	O
2	O
+	O
js	O
)	O
en	O
+	O
100	O
(	O
2+	O
js	O
)	O
en	O
+	O
t.	O
e-	O
''	O
,	O
'	O
/4dt	O
,	O
j8	O
''	O
en	O
(	O
since	O
e~	O
-	O
t	O
2/4	O
:	O
:	O
:	O
-t2/8	O
for	O
t	O
:	O
:	O
:	O
:	O
~en	O
)	O
the	O
proof	O
is	O
completed	O
.	O
0	O
observe	O
that	O
the	O
estimate	B
1	O
'	O
}	O
n	O
is	O
picked	O
from	O
a	O
deterministic	O
class	O
.	O
this	O
,	O
of	O
course	O
,	O
requires	O
quite	O
a	O
bit	O
of	O
preparation	O
and	O
knowledge	O
on	O
behalf	O
of	O
the	O
user	O
.	O
knowledge	O
of	O
the	O
e-entropy	O
(	O
or	O
at	O
least	O
an	O
upper	O
bound	O
)	O
is	O
absolutely	O
essential	O
.	O
furthermore	O
,	O
one	O
must	O
be	O
able	O
to	O
construct	O
fe	O
'	O
this	O
is	O
certainly	O
not	O
computationally	O
simple	O
.	O
skeleton	B
estimates	I
should	O
therefore	O
be	O
mainly	O
of	O
theoretical	O
importance	O
.	O
they	O
may	O
be	O
used	O
,	O
for	O
example	O
,	O
to	O
establish	O
the	O
existence	O
of	O
estimates	O
with	O
a	O
guaranteed	O
error	O
bound	O
as	O
given	O
in	O
theorem	O
28.1.	O
a	O
similar	O
idea	O
in	O
nonparametric	O
density	B
estimation	I
was	O
proposed	O
and	O
worked	O
out	O
in	O
devroye	O
(	O
987	O
)	O
.	O
remark	O
.	O
in	O
the	O
first	O
step	O
of	O
the	O
proof	O
,	O
we	O
used	O
the	O
inequality	B
e	O
{	O
i1	O
'	O
}	O
'	O
(	O
x	O
)	O
-	O
17	O
(	O
x	O
)	O
i	O
}	O
:	O
:	O
:	O
;	O
sup	O
i	O
77	O
'	O
(	O
x	O
)	O
-	O
1	O
'	O
}	O
(	O
x	O
)	O
i	O
<	O
e.	O
x	O
it	O
is	O
clear	O
from	O
this	O
that	O
what	O
we	O
really	O
need	O
is	O
not	O
an	O
e	O
-cover	O
of	O
f	O
with	B
respect	I
to	I
the	I
supremum	I
norm	I
,	O
but	O
rather	O
,	O
with	O
respect	O
to	O
the	O
l	O
1	O
(	O
/-l	O
)	O
norm	O
.	O
in	O
other	O
words	O
,	O
the	O
skeleton	B
estimate	I
works	O
equally	O
well	O
if	O
the	O
skeleton	O
is	O
an	O
e	O
-cover	O
of	O
f	O
with	O
respect	O
to	O
the	O
ll	O
(	O
m	O
)	O
norm	O
,	O
that	O
is	O
,	O
it	O
is	O
a	O
list	O
of	O
finitely	O
many	O
candidates	O
with	O
the	O
property	O
that	O
for	O
each	O
1	O
'	O
}	O
'	O
e	O
f	O
there	O
exists	O
an	O
lj	O
(	O
i	O
)	O
in	O
the	O
list	O
such	O
that	O
e	O
{	O
l1	O
'	O
}	O
'	O
(	O
x	O
)	O
-	O
77	O
(	O
i	O
)	O
(	O
x	O
)	O
i	O
}	O
<	O
e.	O
it	O
follows	O
from	O
the	O
inequality	B
above	O
that	O
the	O
smallest	O
such	O
covering	B
has	O
fewer	O
elements	O
than	O
that	O
of	O
any	O
e	O
-cover	O
of	O
f	O
with	B
respect	I
to	I
the	I
supremum	I
norm	I
.	O
therefore	O
,	O
estimates	O
based	O
on	O
such	O
skeletons	O
perform	O
better	O
.	O
in	O
fact	O
,	O
the	O
difference	O
may	O
be	O
essential	O
.	O
as	O
an	O
example	O
,	O
consider	O
the	O
class	O
f	O
of	O
all	O
regression	O
functions	O
on	O
[	O
0	O
,	O
1	O
]	O
that	O
are	O
monotone	O
increasing	O
.	O
for	O
e	O
<	O
1/2	O
,	O
this	O
class	O
does	O
not	O
have	O
a	O
finite	O
e	O
-cover	O
with	B
respect	I
to	I
the	I
supremum	I
norm	I
.	O
however	O
,	O
for	O
any	O
m	O
it	O
is	O
possible	O
to	O
find	O
an	O
e-cover	O
of	O
f	O
,	O
with	O
respect	O
to	O
l1	O
(	O
/-l	O
)	O
,	O
with	O
not	O
more	O
28.4	O
rate	B
of	I
convergence	I
485	O
than	O
4fl/el	O
elements	O
(	O
problem	O
28.6	O
)	O
.	O
unfortunately	O
,	O
an	O
e-cover	O
with	O
respect	O
to	O
ll	O
(	O
ll	O
)	O
depends	O
on	O
il	O
,	O
the	O
distribution	B
of	O
x.	O
since	O
il	O
is	O
not	O
known	O
in	O
advance	O
,	O
we	O
can	O
not	O
construct	O
this	O
better	O
skeleton	O
.	O
however	O
,	O
in	O
some	O
cases	O
,	O
we	O
may	O
have	O
some	O
a	O
priori	O
information	O
about	O
j.l.forexample	O
,	O
if	O
we	O
know	O
that	O
j.l	O
is	O
a	O
member	O
of	O
a	O
known	O
class	O
of	O
distributions	O
,	O
then	O
we	O
may	O
be	O
able	O
to	O
construct	O
a	O
skeleton	O
that	O
is	O
an	O
e	O
-cover	O
for	O
all	O
measures	O
in	O
the	O
class	O
.	O
in	O
the	O
above	O
example	O
,	O
if	O
we	O
know	O
that	O
il	O
has	O
a	O
density	O
bounded	O
by	O
a	O
known	O
number	O
,	O
then	O
there	O
is	O
a	O
finite	O
skeleton	O
with	O
this	O
property	O
(	O
see	O
problem	O
28.7	O
)	O
.	O
we	O
note	O
here	O
that	O
the	O
basic	O
idea	O
behind	O
the	O
empirical	B
covering	I
method	O
of	O
buescher	O
and	O
kumar	O
(	O
l996a	O
)	O
,	O
described	O
in	O
problem	O
12.14	O
,	O
is	O
to	O
find	O
a	O
good	O
skeleton	O
based	O
on	O
a	O
fraction	O
of	O
the	O
data	O
.	O
investigating	O
this	O
question	O
further	O
,	O
we	O
notice	O
that	O
even	O
covering	B
in	O
l	O
i	O
(	O
j.l	O
)	O
is	O
more	O
than	O
what	O
we	O
really	O
need	O
.	O
from	O
the	O
proof	O
of	O
theorem	O
28.1	O
,	O
we	O
see	O
that	O
all	O
we	O
need	O
is	O
a	O
skeleton	O
fe	O
such	O
that	O
infrylefe	O
l	O
(	O
1	O
]	O
'	O
)	O
:	O
:	O
:	O
l	O
*	O
+	O
e	O
for	O
all	O
1	O
]	O
e	O
f.	O
staying	O
with	O
the	O
example	O
of	O
the	O
class	O
of	O
monotonically	O
increasing	O
1	O
]	O
's	O
,	O
we	O
see	O
that	O
we	O
may	O
take	O
in	O
fe	O
the	O
functions	O
1	O
]	O
(	O
i	O
)	O
(	O
x	O
)	O
=	O
i	O
{	O
x	O
?	O
:	O
.q	O
;	O
}	O
'	O
where	O
qi	O
is	O
the	O
i-th	O
e-quantile	O
of	O
j.l	O
,	O
that	O
is	O
,	O
qi	O
is	O
the	O
smallest	O
number	O
z	O
such	O
that	O
p	O
{	O
x	O
:	O
:	O
:	O
z	O
}	O
2	O
:	O
i	O
/	O
e.	O
this	O
collection	O
of	O
functions	O
forms	O
a	O
skeleton	O
in	O
the	O
required	O
sense	O
with	O
about	O
(	O
l/e	O
)	O
elements	O
,	O
instead	O
of	O
the	O
4fl/el	O
obtained	O
by	O
covering	B
in	O
l	O
i	O
(	O
j.l	O
)	O
,	O
a	O
significant	O
improvement	O
.	O
problem	O
28.8	O
illustrates	O
another	O
application	O
of	O
this	O
idea	O
.	O
for	O
more	O
work	O
on	O
this	O
we	O
refer	O
to	O
vapnik	O
(	O
1982	O
)	O
,	O
benedek	O
and	O
itai	O
(	O
1988	O
)	O
,	O
kulkarni	O
(	O
1991	O
)	O
,	O
dudley	O
,	O
kulkarni	O
,	O
richardson	O
,	O
and	O
zeitouni	O
(	O
1994	O
)	O
,	O
and	O
buescher	O
and	O
kumar	O
(	O
1996a	O
)	O
.	O
0	O
28.4	O
rate	B
of	I
convergence	I
in	O
this	O
section	O
,	O
we	O
take	O
a	O
closer	O
look	O
at	O
the	O
distribution-free	O
upper	O
bound	O
e	O
il	O
(	O
~n	O
)	O
-	O
l	O
*	O
}	O
:	O
s	O
(	O
2	O
+	O
vis	O
)	O
cn	O
+	O
tf·	O
for	O
typical	O
parametric	O
classes	O
(	O
such	O
as	O
the	O
one	O
discussed	O
in	O
a	O
section	O
28.2	O
)	O
,	O
we	O
have	O
;	O
v	O
,	O
-e	O
(	O
d	O
if	O
we	O
take	O
ife	O
i	O
close	O
enough	O
to	O
ne	O
,	O
then	O
ell	O
is	O
the	O
solution	O
of	O
10gne	O
``	O
-	O
-	O
-	O
2	O
-	O
=	O
n	O
,	O
e	O
or	O
en	O
=	O
g	O
(	O
log	O
n	O
/	O
~	O
)	O
,	O
and	O
we	O
achieve	O
a	O
guaranteed	O
rate	O
of	O
0	O
(	O
log	O
n	O
/	O
~	O
)	O
.	O
the	O
same	O
is	O
true	O
for	O
the	O
example	O
of	O
the	O
class	O
of	O
analytic	O
functions	O
discussed	O
earlier	O
.	O
the	O
situation	O
is	O
different	O
for	O
massive	O
classes	O
such	O
as	O
the	O
lipschitz	O
functions	O
on	O
[	O
0	O
,	O
l	O
]	O
d.	O
recalling	O
that	O
10gne	O
=	O
e	O
(	O
1/ed	O
)	O
as	O
e	O
~	O
0	O
,	O
we	O
note	O
that	O
ell	O
=	O
g	O
(	O
n-	O
1/	O
(	O
2+d	O
)	O
.	O
for	O
this	O
class	O
,	O
we	O
have	O
486	O
28.	O
epsilon	O
entropy	B
and	O
totally	O
bounded	O
sets	O
here	O
,	O
once	O
again	O
,	O
we	O
encounter	O
the	O
phenomenon	O
called	O
the	O
``	O
curse	O
of	O
dimension	O
(	O
cid:173	O
)	O
ality	O
.	O
''	O
in	O
order	O
to	O
achieve	O
the	O
performance	O
e	O
{	O
l	O
(	O
1	O
'	O
]	O
n	O
)	O
-	O
l	O
*	O
}	O
s	O
e	O
,	O
we	O
need	O
a	O
sample	O
of	O
size	O
n	O
2	O
:	O
(	O
l/e	O
?	O
+d	O
,	O
exponentially	O
large	O
in	O
d.	O
note	O
that	O
the	O
class	O
of	O
classifiers	O
defined	O
by	O
this	O
class	O
of	O
functions	O
has	O
infinite	O
vc	B
dimension	I
.	O
the	O
skeleton	B
estimates	I
thus	O
provide	O
a	O
vehicle	O
for	O
dealing	O
with	O
very	O
large	O
classes	O
.	O
finally	O
,	O
if	O
we	O
take	O
all	O
1	O
'	O
]	O
on	O
[	O
0	O
,	O
1	O
]	O
forwhichs	O
derivatives	O
exist	O
,	O
and	O
1	O
'	O
]	O
(	O
s	O
)	O
is	O
lipschitz	O
with	O
a	O
given	O
constant	O
c	O
,	O
similar	O
considerations	O
show	O
that	O
the	O
rate	B
of	I
convergence	I
is	O
0	O
(	O
n-	O
(	O
s+1	O
)	O
/	O
(	O
2s+3	O
»	O
)	O
,	O
which	O
ranges	O
from	O
0	O
(	O
n-l/3	O
)	O
(	O
at	O
s	O
=	O
0	O
)	O
to	O
0	O
(	O
n-l/2	O
)	O
(	O
as	O
s	O
--	O
-	O
'	O
»	O
(	O
0	O
)	O
.	O
as	O
the	O
class	O
becomes	O
smaller	O
,	O
we	O
can	O
guarantee	O
better	O
rates	O
of	O
convergence	O
.	O
of	O
course	O
,	O
this	O
requires	O
more	O
a	O
priori	O
knowledge	O
about	O
the	O
true	O
1	O
'	O
]	O
.	O
we	O
also	O
note	O
that	O
if	O
log	O
~	O
/	O
e2	O
=	O
2n	O
in	O
theorem	O
28.1	O
,	O
then	O
the	O
bound	O
is	O
~	O
+	O
(	O
2	O
+	O
v	O
's	O
)	O
en	O
=	O
~	O
+	O
(	O
2	O
+	O
v	O
's	O
)	O
log~ll	O
.	O
v	O
-	O
;	O
;	O
v	O
-	O
;	O
;	O
2n	O
the	O
error	O
grows	O
only	O
sub-logarithmically	O
in	O
the	O
size	O
of	O
the	O
skeleton	O
set	O
.	O
it	O
grows	O
as	O
the	O
square	O
root	O
of	O
the	O
e	O
-entropy	O
.	O
roughly	O
speaking	O
(	O
and	O
ignoring	O
the	O
dependence	O
of	O
en	O
upon	O
n	O
)	O
,	O
we	O
may	O
say	O
that	O
for	O
the	O
same	O
performance	O
guarantees	O
,	O
doubling	O
the	O
e	O
-entropy	O
implies	O
that	O
we	O
should	O
double	O
the	O
sample	O
size	O
(	O
to	O
keep	O
log	O
~	O
/	O
n	O
constant	O
)	O
.	O
when	O
referring	O
to	O
e-entropy	O
,	O
it	O
is	O
important	O
to	O
keep	O
this	O
sample	O
size	O
interpretation	O
in	O
mind	O
.	O
problems	O
and	O
exercises	O
problem	O
28.1.	O
show	O
that	O
the	O
class	O
of	O
functions	O
e-8lxl	O
on	O
the	O
real	O
line	O
,	O
with	O
parameter	O
(	O
)	O
>	O
0	O
,	O
is	O
not	O
totally	O
bounded	O
.	O
problem	O
28.2.	O
compute	O
a	O
good	O
upper	O
bound	O
for	O
n	O
''	O
as	O
a	O
function	O
of	O
d	O
and	O
e	O
for	O
the	O
class	O
f	O
of	O
all	O
functions	O
on	O
n	O
d	O
given	O
by	O
1	O
]	O
'	O
(	O
x	O
)	O
=	O
°	O
{	O
e-8l1xll	O
if	O
ilxll	O
:	O
s	O
1	O
otherwise	O
,	O
where	O
(	O
)	O
e	O
[	O
0	O
,	O
1	O
]	O
is	O
a	O
parameter	O
.	O
repeat	O
this	O
question	O
if	O
(	O
)	O
j	O
,	O
...	O
,	O
(	O
)	O
d	O
are	O
in	O
[	O
0	O
,	O
1	O
]	O
and	O
hint	O
:	O
both	O
answers	O
are	O
polynomial	B
in	O
lie	O
as	O
e	O
,	O
j	O
,	O
0.	O
problem	O
28.3.	O
show	O
that	O
m2e	O
:	O
s	O
~	O
:	O
s	O
me	O
for	O
any	O
totally	O
bounded	O
set	O
f	O
(	O
kolmogorov	O
and	O
tikhomirov	O
(	O
1961	O
»	O
.	O
problem	O
28.4.	O
find	O
a	O
class	O
f	O
of	O
functions	O
1	O
]	O
'	O
:	O
rd	O
~	O
[	O
0	O
,	O
1	O
]	O
such	O
that	O
(	O
a	O
)	O
for	O
every	O
e	O
>	O
0	O
,	O
it	O
has	O
a	O
finite	O
e-cover	O
;	O
(	O
b	O
)	O
the	O
vc	B
dimension	I
of	O
a	O
=	O
{	O
{	O
x	O
:	O
7j	O
'	O
(	O
x	O
)	O
>	O
i/2	O
}	O
;	O
1	O
]	O
'	O
e	O
f	O
}	O
is	O
infinite	O
.	O
problems	O
and	O
exercises	O
487	O
problem	O
28.5.	O
show	O
that	O
if	O
f	O
=	O
{	O
ljl	O
:	O
[	O
0,6	O
.	O
]	O
--	O
-+	O
[	O
0	O
,	O
1	O
]	O
,	O
ljl	O
is	O
lipschitz	O
with	O
constant	O
c	O
}	O
,	O
then	O
for	O
e	O
small	O
enough	O
,	O
10g2	O
ne	O
s	O
ale	O
,	O
where	O
a	O
is	O
a	O
constant	O
depending	O
upon	O
6.	O
and	O
c.	O
(	O
this	O
is	O
not	O
as	O
precise	O
as	O
the	O
statement	O
in	O
the	O
text	O
obtained	O
by	O
kolmogorov	O
and	O
tikhomirov	O
,	O
but	O
it	O
will	O
give	O
you	O
excellent	O
practice	O
.	O
)	O
problem	O
28.6.	O
obtain	O
an	O
estimate	B
for	O
the	O
cardinality	O
of	O
the	O
smallest	O
e-cover	O
with	O
respect	O
to	O
the	O
l	O
1	O
(	O
/.1	O
)	O
norm	O
for	O
the	O
class	O
of	O
l	O
}	O
's	O
on	O
(	O
0,1	O
]	O
that	O
are	O
increasing	O
.	O
in	O
particular	O
,	O
show	O
that	O
for	O
any	O
f.l	O
it	O
is	O
possible	O
to	O
find	O
an	O
e-cover	O
with	O
4fl/el	O
elements	O
.	O
can	O
you	O
do	O
something	O
similar	O
for	O
lj	O
's	O
on	O
[	O
0	O
,	O
1	O
]	O
d	O
that	O
are	O
increasing	O
in	O
each	O
coordinate	O
?	O
problem	O
28.7.	O
consider	O
the	O
class	O
of	O
l	O
}	O
's	O
on	O
[	O
0	O
,	O
1	O
]	O
that	O
are	O
increasing	O
.	O
show	O
that	O
for	O
every	O
e	O
>	O
0	O
,	O
there	O
is	O
a	O
finite	O
list	O
l	O
}	O
(	O
l	O
)	O
,	O
...	O
lj	O
(	O
n	O
)	O
such	O
that	O
for	O
all	O
lj	O
in	O
the	O
class	O
,	O
whenever	O
x	O
has	O
a	O
density	O
bounded	O
by	O
b	O
<	O
00.	O
estimate	B
the	O
smallest	O
such	O
n.	O
problem	O
28.8.	O
assume	O
that	O
x	O
has	O
a	O
bounded	O
density	O
on	O
[	O
0	O
,	O
1	O
]	O
2	O
,	O
and	O
that	O
lj	O
is	O
monotonically	O
increasing	O
in	O
both	O
coordinates	O
.	O
(	O
this	O
is	O
a	O
reasonable	O
assumption	O
in	O
many	O
applications	O
.	O
)	O
then	O
the	O
set	O
{	O
x	O
:	O
g*	O
(	O
x	O
)	O
=	O
o	O
}	O
is	O
a	O
monotone	O
layer	O
.	O
consider	O
the	O
following	O
classification	O
rule	B
:	O
take	O
a	O
k	O
x	O
k	O
grid	O
in	O
(	O
0	O
,	O
1	O
]	O
2	O
,	O
and	O
minimize	O
the	O
empirical	B
error	I
over	O
all	O
classifiers	O
¢	O
such	O
that	O
{	O
x	O
:	O
¢	O
(	O
x	O
)	O
=	O
o	O
}	O
is	O
a	O
monotone	O
layer	O
,	O
and	O
it	O
is	O
a	O
union	O
of	O
cells	O
in	O
the	O
k	O
x	O
k	O
grid	O
.	O
what	O
is	O
the	O
optimal	O
choice	O
of	O
k	O
?	O
obtain	O
an	O
upper	O
bound	O
for	O
l	O
(	O
gn	O
)	O
-	O
l	O
*	O
.	O
compare	O
your	O
result	O
with	O
that	O
obtained	O
for	O
empirical	B
error	I
minimization	O
over	O
the	O
class	O
of	O
all	O
monotone	O
layers	O
in	O
section	O
13.4.	O
hint	O
:	O
count	O
the	O
number	O
of	O
different	O
classifiers	O
in	O
the	O
class	O
.	O
use	O
hoeffding	O
's	O
inequality	B
and	O
the	O
union-of-events	O
bound	O
for	O
the	O
estimation	B
error	I
.	O
bound	O
the	O
approximation	B
error	I
using	O
the	O
bounded	O
density	O
assumption	O
.	O
problem	O
28.9.	O
apply	O
problem	O
6.9	O
to	O
extend	O
the	O
consistency	B
result	O
in	O
theorem	O
28.1	O
as	O
follows	O
.	O
let	O
f	O
be	O
a	O
totally	O
bounded	O
class	O
of	O
functions	O
l	O
}	O
'	O
:	O
r/	O
--	O
-+	O
[	O
0	O
,	O
1	O
]	O
such	O
that	O
j..	O
(	O
{	O
x	O
:	O
ljl	O
(	O
x	O
)	O
=	O
1/2	O
}	O
)	O
=	O
°	O
for	O
each	O
ljl	O
e	O
f	O
(	O
j..	O
is	O
the	O
lebesgue	O
measure	B
on	O
r	O
d	O
)	O
.	O
show	O
that	O
there	O
is	O
a	O
sequence	O
{	O
en	O
>	O
o	O
}	O
and	O
a	O
sequence	O
of	O
skeletons	O
fen	O
such	O
that	O
if	O
l	O
}	O
n	O
is	O
the	O
skeleton	B
estimate	I
drawn	O
from	O
fell	O
'	O
then	O
l	O
(	O
lj	O
,	O
j	O
--	O
-+	O
inf1jief	O
l	O
(	O
l	O
}	O
'	O
)	O
with	O
probability	O
one	O
,	O
whenever	O
x	O
has	O
a	O
density	O
.	O
in	O
particular	O
,	O
l	O
(	O
ljn	O
)	O
--	O
-+	O
l	O
*	O
with	O
probability	O
one	O
if	O
the	O
bayes	O
rule	B
takes	O
the	O
i/2	O
}	O
for	O
some	O
l	O
}	O
'	O
e	O
f.	O
note	O
:	O
the	O
true	O
regression	B
function	I
l	O
}	O
is	O
not	O
required	O
form	O
g*	O
(	O
x	O
)	O
=	O
i	O
{	O
1	O
)	O
i	O
(	O
x	O
»	O
to	O
be	O
in	O
f.	O
29	O
uniform	B
laws	I
of	I
large	I
numbers	I
29.1	O
minimizing	O
the	O
empirical	O
squared	O
error	O
in	O
chapter	O
28	O
the	O
data	O
dn	O
were	O
used	O
to	O
select	O
a	O
function	O
1	O
]	O
n	O
from	O
a	O
class	O
f	O
of	O
candidate	O
regression	O
functions	O
1	O
]	O
'	O
:	O
nd	O
-7	O
[	O
0	O
,	O
1	O
]	O
.	O
the	O
corresponding	O
classification	O
rule	B
gn	O
is	O
i	O
{	O
1jn	O
>	O
1/2	O
}	O
'	O
selecting	O
1	O
]	O
n	O
was	O
done	O
in	O
two	O
steps	O
:	O
a	O
skeleton-an	O
e-covering	O
(	O
cid:173	O
)	O
of	O
f	O
was	O
formed	O
,	O
and	O
the	O
empirical	B
error	I
count	O
was	O
minimized	O
over	O
the	O
skeleton	O
.	O
this	O
method	O
is	O
computationally	O
cumbersome	O
.	O
it	O
is	O
tempting	O
to	O
use	O
some	O
other	O
empirical	B
quantity	O
to	O
select	O
a	O
classifier	O
.	O
perhaps	O
the	O
most	O
popular	O
among	O
these	O
measures	O
is	O
the	O
empirical	O
squared	O
error	O
:	O
assume	O
now	O
that	O
the	O
function	O
1	O
]	O
n	O
is	O
selected	O
by	O
minimizing	O
the	O
empirical	O
squared	O
error	O
over	O
f	O
,	O
that	O
is	O
,	O
as	O
always	O
,	O
we	O
are	O
interested	O
in	O
the	O
error	O
probability	O
,	O
of	O
the	O
resulting	O
classifier	B
.	O
if	O
the	O
true	O
regression	B
function	I
1	O
]	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
11	O
x	O
=	O
x	O
}	O
is	O
not	O
in	O
the	O
class	O
f	O
,	O
then	O
it	O
is	O
easy	O
to	O
see	O
that	O
empirical	O
squared	O
error	O
minimization	O
may	O
fail	O
miserably	O
(	O
see	O
problem	O
29.1	O
)	O
.	O
however	O
,	O
if	O
1	O
]	O
e	O
f	O
,	O
then	O
for	O
every	O
1	O
]	O
'	O
e	O
f	O
490	O
29	O
.	O
\	O
uniform	B
laws	I
of	I
large	I
numbers	I
we	O
have	O
l	O
(	O
r	O
!	O
'	O
)	O
-	O
l	O
*	O
:	O
:	O
:	O
2je	O
{	O
(	O
l	O
}	O
/	O
(	O
x	O
)	O
-	O
=	O
2je	O
{	O
(	O
l	O
}	O
/	O
(	O
x	O
)	O
-	O
y	O
)	O
2	O
}	O
-	O
e	O
{	O
(	O
l	O
}	O
(	O
x	O
)	O
-	O
y	O
)	O
2	O
}	O
l	O
}	O
(	O
x	O
»	O
2	O
}	O
(	O
by	O
corollary	O
6.2	O
)	O
=	O
2	O
e	O
{	O
(	O
l	O
}	O
/	O
(	O
x	O
)	O
-	O
y	O
)	O
2	O
}	O
-	O
!	O
nf	O
e	O
{	O
o	O
]	O
(	O
x	O
)	O
-	O
y	O
)	O
2	O
}	O
,	O
i	O
)	O
ef	O
where	O
the	O
two	O
equalities	O
follow	O
from	O
the	O
fact	O
that	O
l	O
}	O
(	O
x	O
)	O
=	O
e	O
{	O
yix	O
}	O
.	O
thus	O
,	O
we	O
have	O
l	O
(	O
l	O
}	O
n	O
)	O
-	O
l	O
*	O
<	O
2	O
e	O
{	O
(	O
l	O
}	O
n	O
(	O
x	O
)	O
-	O
y	O
)	O
2idn	O
}	O
-	O
inf	O
e	O
{	O
(	O
l	O
}	O
/	O
(	O
x	O
)	O
-	O
y	O
)	O
2	O
}	O
i	O
)	O
'ef	O
by	O
an	O
argument	O
as	O
in	O
the	O
proof	O
of	O
lemma	O
8.2.	O
thus	O
,	O
the	O
method	O
is	O
consistent	O
if	O
the	O
supremum	O
above	O
converges	O
to	O
zero	O
.	O
if	O
we	O
define	O
zi	O
=	O
(	O
xi	O
,	O
yd	O
and	O
i	O
(	O
zd	O
=	O
(	O
l	O
}	O
/	O
(	O
xi	O
)	O
-	O
yd2	O
,	O
then	O
we	O
see	O
that	O
we	O
need	O
only	O
to	O
bound	O
where	O
f	O
is	O
a	O
class	O
of	O
bounded	O
functions	O
.	O
in	O
the	O
next	O
four	O
sections	O
we	O
develop	O
upper	O
bounds	O
for	O
such	O
uniform	B
deviations	I
of	O
averages	O
from	O
their	O
expectations	O
.	O
then	O
we	O
apply	O
these	O
techniques	O
to	O
establish	O
consistency	B
of	O
generalized	O
linear	O
classifiers	O
obtained	O
by	O
minimization	O
of	O
the	O
empirical	O
squared	O
error	O
.	O
29.2	O
uniform	B
deviations	I
of	O
averages	O
from	O
expectations	O
let	O
f	O
be	O
a	O
class	O
of	O
real-valued	O
functions	O
defined	O
on	O
n	O
d	O
,	O
and	O
let	O
z	O
1	O
,	O
...	O
,	O
zn	O
be	O
i.i.d	O
.	O
nd-valued	O
random	O
variables	O
.	O
we	O
assume	O
that	O
for	O
each	O
i	O
e	O
f	O
,	O
0	O
:	O
:	O
:	O
i	O
(	O
x	O
)	O
:	O
:	O
:	O
m	O
for	O
all	O
x	O
e	O
nd	O
and	O
some	O
m	O
<	O
00.	O
by	O
hoeffding	O
's	O
inequality	B
,	O
for	O
any	O
i	O
e	O
f.	O
however	O
,	O
it	O
is	O
much	O
less	O
trivial	O
to	O
obtain	O
information	O
about	O
the	O
probabilities	O
p	O
{	O
sup	O
i~	O
t	O
i	O
(	O
zj	O
-	O
e	O
{	O
/	O
(	O
zl	O
)	O
}	O
1	O
>	O
e	O
}	O
.	O
ief	O
n	O
i=l	O
29.2	O
uniform	B
deviations	I
of	O
averages	O
from	O
expectations	O
491	O
vapnik	O
and	O
chervonenkis	O
(	O
1981	O
)	O
were	O
the	O
first	O
to	O
obtain	O
bounds	O
for	O
the	O
probability	O
above	O
.	O
for	O
example	O
,	O
the	O
following	O
simple	O
observation	O
makes	O
theorems	O
12.5	O
,	O
12.8	O
,	O
and	O
12.10	O
easy	O
to	O
apply	O
in	O
the	O
new	O
situation	O
:	O
lemma	O
29.1.	O
sup	O
-	O
l	O
f	O
(	O
zi	O
)	O
-	O
e	O
{	O
f	O
(	O
z	O
)	O
}	O
.-	O
:	O
:	O
:	O
m	O
i	O
1	O
n	O
fe	O
:	O
f	O
n	O
i=l	O
1	O
-	O
l	O
iu	O
(	O
z	O
;	O
»	O
t	O
}	O
-	O
p	O
{	O
f	O
(	O
z	O
)	O
>	O
t	O
}	O
.	O
i	O
11	O
sup	O
n	O
fe	O
:	O
f	O
,	O
t	O
>	O
o	O
n	O
i==l	O
proof	O
.	O
exploiting	O
the	O
identity	O
fooo	O
p	O
{	O
x	O
>	O
t	O
}	O
dt	O
=	O
ex	O
for	O
nonnegative	O
random	O
variables	O
,	O
we	O
have	O
fe	O
:	O
f	O
n	O
i==l	O
sup	O
i	O
~	O
t	O
f	O
(	O
zi	O
)	O
-	O
e	O
{	O
f	O
(	O
z	O
)	O
}	O
1	O
=	O
sup	O
i	O
[	O
00	O
(	O
~	O
t	O
iu	O
(	O
z	O
;	O
»	O
t	O
}	O
-	O
p	O
{	O
f	O
(	O
z	O
)	O
>	O
t	O
}	O
)	O
dtl	O
fe	O
:	O
f	O
10	O
sup	O
i~	O
t	O
iu	O
(	O
z	O
;	O
»	O
t	O
}	O
-	O
p	O
{	O
f	O
(	O
z	O
)	O
>	O
tll·	O
0	O
<	O
m	O
n	O
i==l	O
fe	O
:	O
f	O
,	O
t	O
>	O
o	O
n	O
i=l	O
for	O
example	O
,	O
from	O
theorem	B
12.5	O
and	O
lemma	O
29.1	O
we	O
get	O
corollary	O
29.1.	O
define	O
the	O
collection	O
of	O
sets	O
f	O
=	O
{	O
a	O
f	O
,	O
t	O
:	O
f	O
e	O
f	O
,	O
t	O
e	O
[	O
0	O
,	O
m	O
]	O
}	O
,	O
where	O
for	O
every	O
f	O
e	O
f	O
and	O
t	O
e	O
[	O
0	O
,	O
m	O
]	O
the	O
set	O
a	O
f	O
,	O
t	O
e	O
nd	O
is	O
defined	O
as	O
a	O
f	O
,	O
t	O
=	O
{	O
z	O
:	O
fez	O
)	O
>	O
t	O
}	O
.	O
then	O
example	O
.	O
consider	O
the	O
empirical	O
squared	O
error	O
minimization	O
problem	O
sketched	O
in	O
the	O
previous	O
section	O
.	O
let	O
f	O
be	O
the	O
class	O
of	O
monotone	O
increasing	O
functions	O
1	O
]	O
'	O
:	O
r	O
-+	O
[	O
0	O
,	O
1	O
]	O
,	O
and	O
let	O
1	O
]	O
n	O
be	O
the	O
function	O
selected	O
by	O
minimizing	O
the	O
empirical	O
squared	O
error	O
.	O
by	O
(	O
29.1	O
)	O
,	O
if	O
1	O
]	O
(	O
x	O
)	O
=	O
pry	O
=	O
llx	O
=	O
x	O
}	O
is	O
also	O
monotone	O
increasing	O
,	O
then	O
492	O
29.	O
uniform	B
laws	I
of	I
large	I
numbers	I
if	O
t	O
contains	O
all	O
subsets	O
of	O
r	O
x	O
{	O
o	O
,	O
i	O
}	O
of	O
the	O
form	O
a	O
1	O
)	O
'	O
,	O
t	O
=	O
{	O
(	O
x	O
,	O
y	O
)	O
:	O
(	O
ry	O
'	O
(	O
x	O
)	O
-	O
y	O
)	O
2	O
>	O
t	O
}	O
,	O
ry	O
'	O
e	O
f	O
,	O
t	O
e	O
[	O
0	O
,	O
1	O
]	O
,	O
then	O
it	O
is	O
easy	O
to	O
see	O
that	O
its	O
n-th	O
shatter	B
coefficient	I
satisfies	O
set	O
,	O
n	O
)	O
:	O
:	O
;	O
(	O
n12	O
+	O
1	O
)	O
2.	O
thus	O
,	O
corollary	O
29.1	O
can	O
be	O
applied	O
,	O
and	O
the	O
empirical	O
squared	O
error	O
minimization	O
is	O
consistent	O
.	O
0	O
in	O
many	O
cases	O
,	O
corollary	O
29.1	O
does	O
not	O
provide	O
the	O
best	O
possible	O
bound	O
.	O
to	O
state	O
a	O
similar	O
,	O
but	O
sometimes	O
more	O
useful	O
result	O
,	O
we	O
introduce	O
ii-covering	O
numbers	O
.	O
the	O
notion	O
is	O
very	O
similar	O
to	O
that	O
of	O
covering	O
numbers	O
discussed	O
in	O
chapter	O
28.	O
the	O
main	O
difference	O
is	O
that	O
here	O
the	O
balls	O
are	O
defined	O
in	O
terms	O
of	O
an	O
ii	O
-distance	O
,	O
rather	O
than	O
the	O
supremum	O
norm	O
.	O
definition	O
29.1.	O
let	O
a	O
be	O
a	O
bounded	O
subset	O
of	O
rd	O
.	O
for	O
every	O
e	O
>	O
0	O
,	O
the	O
h	O
(	O
cid:173	O
)	O
covering	B
number	I
,	O
denoted	O
by	O
n	O
(	O
e	O
,	O
a	O
)	O
,	O
is	O
defined	O
as	O
the	O
cardinality	O
of	O
the	O
smallest	O
finite	O
set	O
in	O
rd	O
such	O
that	O
for	O
every	O
z	O
e	O
a	O
there	O
is	O
a	O
point	O
t	O
e	O
rd	O
in	O
the	O
finite	O
till	O
<	O
e.	O
(	O
lixlli	O
=	O
l~=l	O
ixu	O
)	O
1	O
denotes	O
the	O
ii-norm	O
of	O
the	O
set	O
such	O
that	O
(	O
lld	O
)	O
llz	O
-	O
vector	O
x	O
=	O
(	O
x	O
(	O
l	O
)	O
,	O
...	O
,	O
xed	O
)	O
)	O
in	O
rd	O
.	O
)	O
in	O
other	O
words	O
,	O
n	O
(	O
e	O
,	O
a	O
)	O
is	O
the	O
smallest	O
number	O
of	O
ii-balls	O
of	O
radius	O
ed	O
,	O
whose	O
union	O
contains	O
a.	O
logn	O
(	O
e	O
,	O
a	O
)	O
is	O
often	O
called	O
the	O
metric	B
entropy	I
of	O
a.	O
we	O
will	O
mainly	O
be	O
interested	O
in	O
covering	O
numbers	O
of	O
special	O
sets	O
.	O
let	O
zl	O
=	O
(	O
zl	O
,	O
...	O
,	O
zn	O
)	O
be	O
n	O
fixed	O
points	O
in	O
r	O
d	O
,	O
and	O
define	O
the	O
following	O
set	O
:	O
the	O
h	O
-covering	O
number	O
of	O
f	O
(	O
zl	O
)	O
is	O
n	O
(	O
e	O
,	O
f	O
(	O
zl	O
)	O
)	O
'	O
if	O
z	O
?	O
=	O
(	O
zl	O
,	O
...	O
,	O
zn	O
)	O
is	O
a	O
sequence	O
ofi.i.d	O
.	O
random	O
variables	O
,	O
thenn	O
(	O
e	O
,	O
fcz	O
!	O
)	O
)	O
is	O
a	O
random	O
variable	B
,	O
whose	O
expected	O
value	O
plays	O
a	O
central	O
role	O
in	O
our	O
problem	O
:	O
theorem	B
29.1	O
.	O
(	O
pollard	O
(	O
1984	O
)	O
)	O
.	O
for	O
any	O
nand	O
e	O
>	O
0	O
,	O
the	O
proof	O
of	O
the	O
theorem	B
is	O
given	O
in	O
section	O
29.4.	O
remark	O
.	O
theorem	B
29.1	O
is	O
a	O
generalization	O
of	O
the	O
basic	O
vapnik	O
-chervonenkis	O
in	O
(	O
cid:173	O
)	O
equality	O
.	O
to	O
see	O
this	O
,	O
define	O
loo-covering	O
numbers	O
based	O
on	O
the	O
maximum	O
norm	O
(	O
vapnik	O
and	O
chervonenkis	O
(	O
1981	O
)	O
)	O
:	O
noo	O
(	O
e	O
,	O
a	O
)	O
is	O
the	O
cardinality	O
of	O
the	O
smallest	O
finite	O
set	O
in	O
rd	O
such	O
that	O
for	O
every	O
z	O
e	O
a	O
there	O
is	O
a	O
point	O
t	O
e	O
rd	O
in	O
the	O
set	O
such	O
that	O
maxi	O
<	O
i	O
<	O
d	O
iz	O
(	O
i	O
)	O
t	O
(	O
i	O
)	O
1	O
<	O
e.	O
if	O
the	O
functions	O
in	O
f	O
are	O
indicators	O
of	O
sets	O
from	O
a	O
class	O
a	O
of	O
s~bsets	O
of	O
r	O
d	O
,	O
then	O
it	O
is	O
easy	O
to	O
see	O
that	O
for	O
every	O
e	O
e	O
(	O
0	O
,	O
1/2	O
)	O
,	O
29.3	O
empirical	O
squared	O
error	O
minimization	O
493	O
.	O
where	O
n	O
a	O
(	O
zl	O
,	O
...	O
,	O
zn	O
)	O
is	O
the	O
combinatorial	O
quantity	O
that	O
was	O
used	O
in	O
definition	O
12.1	O
of	O
shatter	O
coefficients	O
.	O
since	O
theorem	B
29.1	O
remains	O
true	O
with	O
loo-covering	O
numbers	O
,	O
therefore	O
,	O
it	O
is	O
a	O
general	O
(	O
cid:173	O
)	O
ization	O
of	O
theorem	O
12.5.	O
to	O
see	O
this	O
,	O
notice	O
that	O
if	O
f	O
contains	O
indicators	O
of	O
sets	O
of	O
the	O
class	O
a	O
,	O
then	O
sup	O
i~	O
t	O
f	O
(	O
zi	O
)	O
-	O
e	O
{	O
f	O
(	O
zl	O
)	O
}	O
1	O
=	O
sup	O
i~	O
t	O
i	O
{	O
ziea	O
}	O
-	O
p	O
{	O
zi	O
e	O
a	O
}	O
i·	O
0	O
aea	O
n	O
i=l	O
ief	O
n	O
i=l	O
for	O
inequalities	O
sharper	O
and	O
more	O
general	O
than	O
theorem	B
29.1	O
we	O
refer	O
to	O
vapnik	O
(	O
1982	O
)	O
,	O
pollard	O
(	O
1984	O
;	O
1986	O
)	O
,	O
haussler	O
(	O
1992	O
)	O
,	O
and	O
anthony	O
and	O
shawe-taylor	O
(	O
1990	O
)	O
.	O
29.3	O
empirical	O
squared	O
error	O
minimization	O
we	O
return	O
to	O
the	O
minimization	O
of	O
the	O
empirical	O
squared	O
error	O
.	O
let	O
f	O
be	O
a	O
class	O
of	O
functions	O
r	O
!	O
,	O
:	O
nd	O
-+	O
[	O
0	O
,	O
1	O
]	O
,	O
containing	O
the	O
true	O
a	O
posteriori	O
function	O
'y	O
}	O
.	O
the	O
empirical	O
squared	O
error	O
is	O
minimized	O
over	O
'y	O
}	O
'	O
e	O
f	O
,	O
to	O
obtain	O
the	O
estimate	B
'y	O
}	O
n.	O
the	O
next	O
result	O
shows	O
that	O
empirical	O
squared	O
error	O
minimization	O
is	O
consistent	O
under	O
general	O
conditions	O
.	O
ob	O
(	O
cid:173	O
)	O
serve	O
that	O
these	O
are	O
the	O
same	O
conditions	O
that	O
we	O
assumed	O
in	O
theorem	O
28.1	O
to	O
prove	O
consistency	B
of	O
skeleton	B
estimates	I
.	O
theorem	B
29.2.	O
assume	O
that	O
f	O
is	O
a	O
totally	O
bounded	O
class	O
of	O
functions	O
.	O
(	O
for	O
the	O
definition	O
see	O
chapter	O
28	O
.	O
)	O
if	O
'y	O
}	O
e	O
f	O
,	O
then	O
the	O
classification	O
rule	B
obtained	O
by	O
minimizing	O
the	O
empirical	O
squared	O
error	O
over	O
f	O
is	O
strongly	O
consistent	O
,	O
that	O
is	O
,	O
lim	O
l	O
(	O
17i1	O
)	O
=	O
l	O
*	O
with	O
probability	O
one	O
.	O
n	O
--	O
+oo	O
proof	O
.	O
recall	O
that	O
by	O
(	O
29.1	O
)	O
,	O
we	O
apply	O
theorem	B
29.1	O
to	O
show	O
that	O
for	O
every	O
e	O
>	O
0	O
,	O
the	O
probability	O
on	O
the	O
right	O
(	O
cid:173	O
)	O
hand	O
side	O
converges	O
to	O
zero	O
exponentially	O
as	O
n	O
-+	O
00.	O
to	O
this	O
end	O
,	O
we	O
need	O
to	O
find	O
a	O
suitable	O
upper	O
bound	O
on	O
e	O
{	O
n	O
(	O
e	O
,	O
j	O
(	O
z7	O
)	O
)	O
}	O
,	O
where	O
j	O
is	O
the	O
class	O
of	O
functions	O
494	O
29.	O
uniform	B
laws	I
of	I
large	I
numbers	I
i	O
'	O
(	O
x	O
,	O
y	O
)	O
=	O
(	O
7j	O
'	O
(	O
x	O
)	O
-	O
y	O
?	O
froeu	O
nd	O
x	O
{	O
o	O
,	O
i	O
}	O
to	O
[	O
0,1	O
]	O
,	O
where	O
7j	O
'	O
e	O
f	O
,	O
and	O
zi	O
=	O
(	O
xi	O
,	O
yi	O
)	O
.	O
observe	O
that	O
for	O
any	O
i	O
'	O
,	O
i	O
e	O
.	O
:	O
j	O
,	O
<	O
2	O
sup	O
17j	O
'	O
(	O
x	O
)	O
-	O
~	O
(	O
x	O
)	O
l.	O
x	O
this	O
inequality	B
implies	O
that	O
for	O
every	O
f	O
>	O
0	O
,	O
e	O
{	O
n	O
(	O
f	O
,	O
.	O
:	O
j	O
(	O
z~	O
»	O
}	O
:	O
:s	O
~/2	O
'	O
where	O
~	O
is	O
the	O
covering	B
number	I
of	O
f	O
defined	O
in	O
chapter	O
28.	O
by	O
the	O
assumption	O
of	O
total	O
boundedness	O
,	O
for	O
every	O
e	O
>	O
0	O
,	O
ne/2	O
<	O
00.	O
since	O
~/2	O
does	O
not	O
depend	O
on	O
n	O
,	O
the	O
theorem	B
is	O
proved	O
.	O
0	O
remark	O
.	O
the	O
nonasymptotic	O
exponential	B
nature	O
of	O
the	O
inequality	B
in	O
theorem	B
29.1	O
makes	O
it	O
possible	O
to	O
obtain	O
upper	O
bounds	O
for	O
the	O
rate	B
of	I
convergence	I
of	O
l	O
(	O
7jn	O
)	O
to	O
l	O
*	O
in	O
terms	O
of	O
the	O
covering	B
numbers	O
~	O
of	O
the	O
class	O
f.	O
however	O
,	O
since	O
we	O
started	O
our	O
analysis	O
by	O
the	O
loose	O
inequality	B
l	O
(	O
7j	O
'	O
)	O
-	O
l	O
*	O
:	O
:s	O
2	O
)	O
e	O
{	O
(	O
7j	O
'	O
(	O
x	O
)	O
-	O
7j	O
(	O
x	O
)	O
)	O
2	O
}	O
,	O
the	O
resulting	O
rates	O
are	O
likely	O
to	O
be	O
suboptimal	O
(	O
see	O
theorem	B
6.5	O
)	O
.	O
also	O
,	O
the	O
inequality	B
of	O
theorem	B
29.1	O
may	O
be	O
loose	O
in	O
this	O
case	O
.	O
in	O
a	O
somewhat	O
different	O
setup	O
,	O
barron	O
(	O
1991	O
)	O
developed	O
a	O
proof	O
method	O
based	O
on	O
bernstein	O
's	O
inequality	B
that	O
is	O
useful	O
for	O
obtaining	O
tighter	O
upper	O
bounds	O
for	O
l	O
(	O
7jn	O
)	O
-	O
l	O
*	O
in	O
certain	O
cases	O
.	O
0	O
29.4	O
proof	O
of	O
theorem	O
29.1	O
the	O
main	O
tricks	O
in	O
the	O
proof	O
resemble	O
those	O
of	O
theorem	O
12.5.	O
we	O
can	O
show	O
that	O
for	O
nf	O
2	O
~	O
2m2	O
,	O
p	O
{	O
sup	O
j~	O
t	O
fey	O
:	O
:	O
n	O
i=l	O
i	O
(	O
zj	O
-	O
e	O
{	O
i	O
(	O
zi	O
)	O
}	O
j	O
>	O
f	O
}	O
:	O
:s	O
4p	O
{	O
sup	O
j~	O
taii	O
(	O
zi	O
)	O
j	O
>	O
:	O
.	O
}	O
,	O
fey	O
:	O
:	O
n	O
i=l	O
4	O
where	O
0'1	O
,	O
...	O
,	O
an	O
are	O
i.i.d	O
.	O
{	O
-i	O
,	O
l	O
}	O
-valued	O
random	O
variables	O
,	O
independent	O
of	O
the	O
zi	O
's	O
,	O
with	O
p	O
{	O
ai	O
=	O
i	O
}	O
=	O
p	O
{	O
ai	O
=	O
-i	O
}	O
=	O
1/2	O
.	O
the	O
only	O
minor	O
difference	O
with	O
theorem	O
12.5	O
appears	O
when	O
chebyshev	O
's	O
inequality	B
is	O
applied	O
.	O
we	O
use	O
the	O
fact	O
that	O
by	O
boundedness	O
,	O
var	O
(	O
i	O
(	O
zi	O
»	O
:	O
:s	O
m2/4	O
for	O
every	O
i	O
e	O
f.	O
now	O
,	O
take	O
a	O
minimal	O
f/8-covering	O
of	O
f	O
(	O
z~	O
)	O
,	O
that	O
is	O
,	O
m	O
=	O
n	O
(	O
f/8	O
,	O
f	O
(	O
z	O
'	O
!	O
»	O
)	O
functions	O
gl	O
,	O
...	O
,	O
gm	O
such	O
that	O
for	O
every	O
i	O
e	O
f	O
there	O
is	O
a	O
g*	O
e	O
{	O
gl	O
,	O
...	O
,	O
gm	O
}	O
with	O
1	O
n	O
;	O
;	O
~	O
ii	O
(	O
zi	O
)	O
-	O
g*	O
(	O
zi	O
)	O
1	O
:	O
:s	O
``	O
8.	O
f	O
29.4	O
proof	O
of	O
theorem	O
29.1	O
495	O
''	O
for	O
any	O
function	O
f	O
,	O
we	O
have	O
and	O
thus	O
i~	O
ta	O
;	O
jczi	O
)	O
1	O
~	O
i~	O
taig'czi	O
)	O
h~	O
t	O
<	O
y	O
;	O
(	O
fczi	O
)	O
~	O
g'czi	O
»	O
1	O
1	O
n	O
i	O
l	O
n	O
-	O
l	O
o'ig*	O
(	O
zi	O
)	O
+	O
-	O
l	O
if	O
(	O
zi	O
)	O
-	O
g*	O
(	O
zi	O
)	O
l·	O
n	O
i=l	O
n	O
i=l	O
s	O
1	O
fef	O
n	O
i=l	O
as	O
(	O
lin	O
)	O
l7=1	O
ig*	O
(	O
zi	O
)	O
-	O
!	O
(	O
zi	O
)	O
1	O
s	O
e/8	O
,	O
p	O
{	O
sup	O
i~	O
t	O
o'i	O
!	O
czi	O
)	O
1	O
>	O
:	O
.1	O
zl	O
,	O
...	O
,	O
zn	O
}	O
s	O
p	O
{	O
sup	O
i	O
~	O
t	O
o'ig*	O
(	O
zi	O
)	O
1	O
+	O
~	O
t	O
s	O
p	O
{	O
max	O
i~	O
to'igj	O
(	O
zji	O
>	O
:	O
.1	O
zl	O
,	O
...	O
,	O
zn	O
}	O
.	O
fef	O
n	O
i=l	O
n	O
i=l	O
4	O
8	O
g	O
;	O
n	O
i=l	O
ifczj	O
-	O
g*	O
(	O
zi	O
)	O
1	O
>	O
:	O
.1	O
zl	O
,	O
...	O
,	O
zn	O
}	O
4	O
now	O
that	O
we	O
have	O
been	O
able	O
to	O
convert	O
the	O
``	O
sup	O
''	O
into	O
a	O
``	O
max	O
,	O
''	O
we	O
can	O
use	O
the	O
union	O
bound	O
:	O
we	O
need	O
only	O
find	O
a	O
uniform	O
bound	O
for	O
the	O
probability	O
following	O
the	O
``	O
max	O
.	O
''	O
this	O
,	O
however	O
,	O
is	O
easy	O
,	O
since	O
after	O
conditioning	O
on	O
zl	O
,	O
...	O
,	O
zn	O
,	O
l~1=1	O
o'ig/zi	O
)	O
is	O
the	O
sum	O
of	O
independent	O
bounded	O
random	O
variables	O
whose	O
expected	O
value	O
is	O
zero	O
.	O
therefore	O
,	O
hoeffding	O
's	O
inequality	B
gives	O
1	O
~	O
i	O
;	O
;	O
f	O
:	O
:	O
o'igj	O
(	O
zi	O
)	O
>	O
'8	O
zl	O
,	O
...	O
,	O
zn	O
s	O
2e	O
e	O
i	O
}	O
-ne	O
2	O
/	O
(	O
128m	O
2	O
)	O
.	O
p	O
{	O
1	O
in	O
summary	O
,	O
496	O
29.	O
uniform	B
laws	I
of	I
large	I
numbers	I
~	O
2e	O
{	O
n	O
(	O
i	O
,	O
f	O
(	O
zn	O
)	O
}	O
e-ne2	O
/	O
(	O
l28m	O
2	O
)	O
.	O
the	O
theorem	B
is	O
proved	O
.	O
0	O
properties	O
of	O
n	O
(	O
e	O
,	O
f	O
(	O
z7	O
)	O
)	O
will	O
be	O
studied	O
in	O
the	O
next	O
section	O
.	O
29.5	O
covering	B
numbers	O
and	O
shatter	O
coefficients	O
in	O
this	O
section	O
we	O
study	O
covering	B
numbers	O
,	O
and	O
relate	O
them	O
to	O
shatter	O
coefficients	O
of	O
certain	O
classes	O
of	O
sets	O
.	O
as	O
in	O
chapter	O
28	O
,	O
we	O
introduce	O
ii-packing	O
numbers	O
.	O
let	O
fbe	O
a	O
class	O
of	O
functions	O
on	O
n	O
d	O
,	O
taking	O
their	O
values	O
in	O
[	O
0	O
,	O
mj	O
.	O
let	O
/.1	O
be	O
an	O
arbitrary	O
probability	O
measure	B
on	O
nd	O
.	O
let	O
gl	O
,	O
...	O
,	O
gm	O
be	O
a	O
finite	O
collection	O
of	O
functions	O
from	O
f	O
with	O
the	O
property	O
that	O
for	O
any	O
two	O
of	O
them	O
the	O
largest	O
m	O
for	O
which	O
such	O
a	O
collection	O
exists	O
is	O
called	O
the	O
packing	B
number	I
of	O
f	O
(	O
relative	O
to	O
/.1	O
)	O
,	O
and	O
is	O
denoted	O
by	O
m	O
(	O
e	O
,	O
f	O
)	O
.	O
if	O
t1	O
places	O
probability	O
lin	O
on	O
each	O
of	O
zl	O
,	O
...	O
,	O
zn	O
,	O
then	O
by	O
definition	O
m	O
(	O
e	O
,	O
f	O
)	O
=	O
m	O
(	O
e	O
,	O
f	O
(	O
zi	O
{	O
)	O
)	O
,	O
and	O
it	O
is	O
easy	O
to	O
see	O
(	O
problem	O
28.3	O
)	O
that	O
m	O
(	O
2e	O
,	O
f	O
(	O
zid	O
)	O
~	O
n	O
(	O
e	O
,	O
f	O
(	O
z~	O
)	O
)	O
~	O
m	O
(	O
e	O
,	O
f	O
(	O
z7	O
)	O
)	O
.	O
an	O
important	O
feature	O
of	O
a	O
class	O
of	O
functions	O
f	O
is	O
the	O
vc	B
dimension	I
v.r+	O
of	O
f+	O
=	O
{	O
{	O
(	O
x	O
,	O
t	O
)	O
:	O
t	O
~	O
f	O
(	O
x	O
)	O
}	O
;	O
f	O
e	O
f	O
}	O
.	O
this	O
is	O
clarified	O
by	O
the	O
following	O
theorem	B
,	O
which	O
is	O
a	O
slight	O
refinement	O
of	O
a	O
result	O
by	O
pollard	O
(	O
1984	O
)	O
,	O
which	O
is	O
based	O
on	O
dudley	O
's	O
(	O
1978	O
)	O
work	O
.	O
it	O
connects	O
the	O
packing	B
number	I
of	O
f	O
with	O
the	O
shatter	O
coefficients	O
of	O
f+	O
.	O
see	O
also	O
haussler	O
(	O
1992	O
)	O
for	O
a	O
somewhat	O
different	O
argument	O
.	O
theorem	B
29.3.	O
let	O
f	O
be	O
a	O
class	O
of	O
[	O
0	O
,	O
m	O
]	O
-valued	O
functions	O
on	O
nd	O
.	O
for	O
every	O
e	O
>	O
°	O
and	O
probability	O
measure	B
j.l	O
,	O
eem2	O
(	O
e	O
,	O
f	O
)	O
l	O
m	O
(	O
e	O
,	O
f	O
)	O
~	O
s	O
(	O
f+	O
,	O
k	O
)	O
,	O
where	O
k	O
=	O
-log	O
fm	O
e	O
.	O
2m	O
proof	O
.	O
let	O
{	O
gl	O
,	O
g2	O
,	O
...	O
,	O
gm	O
}	O
be	O
an	O
arbitrary	O
e-packing	O
of	O
f	O
of	O
size	O
m	O
~	O
m	O
(	O
e	O
,	O
f	O
)	O
.	O
the	O
proof	O
is	O
in	O
the	O
spirit	O
of	O
the	O
probabilistic	B
method	I
of	O
combinatorics	O
(	O
see	O
,	O
e.g.	O
,	O
spencer	O
(	O
1987	O
)	O
)	O
.	O
to	O
prove	O
the	O
inequality	B
,	O
we	O
create	O
k	O
random	O
points	O
on	O
nd	O
x	O
[	O
0	O
,	O
m	O
]	O
in	O
the	O
following	O
way	O
,	O
where	O
k	O
is	O
a	O
positive	O
integer	O
specified	O
later	O
.	O
we	O
gener	O
(	O
cid:173	O
)	O
ate	O
k	O
independent	O
random	O
variables	O
sl	O
,	O
...	O
,	O
sk	O
on	O
nd	O
with	O
common	O
distribution	B
29.5	O
covering	B
numbers	O
and	O
shatter	O
coefficients	O
497	O
.	O
jk	O
,	O
and	O
independently	O
of	O
this	O
,	O
we	O
generate	O
another	O
k	O
independent	O
random	O
vari	O
(	O
cid:173	O
)	O
ables	O
tl	O
,	O
..	O
``	O
tb	O
uniformly	O
distributed	O
on	O
[	O
0	O
,	O
m	O
]	O
.	O
this	O
yields	O
k	O
random	O
pairs	O
(	O
s1	O
,	O
tl	O
)	O
,	O
...	O
,	O
(	O
sk	O
,	O
tk	O
)	O
.	O
for	O
any	O
two	O
functions	O
gi	O
and	O
gj	O
in	O
an	O
e-packing	O
,	O
the	O
proba	O
(	O
cid:173	O
)	O
bility	O
that	O
the	O
sets	O
gi	O
=	O
{	O
(	O
x	O
,	O
t	O
)	O
:	O
t	O
:	O
:	O
:	O
gi	O
(	O
x	O
)	O
}	O
and	O
g	O
j	O
=	O
{	O
(	O
x	O
,	O
t	O
)	O
:	O
t	O
:	O
:	O
:	O
:	O
gj	O
(	O
x	O
)	O
}	O
pick	O
the	O
same	O
points	O
from	O
our	O
random	O
set	O
of	O
k	O
points	O
is	O
bounded	O
as	O
follows	O
:	O
p	O
{	O
g	O
i	O
and	O
g	O
j	O
pick	O
the	O
same	O
points	O
}	O
k	O
=	O
ri	O
(	O
i	O
-	O
p	O
{	O
(	O
sf	O
,	O
tr	O
)	O
e	O
gil	O
.	O
g	O
j	O
}	O
)	O
1=1	O
=	O
(	O
1	O
-	O
e	O
{	O
p	O
{	O
(	O
s	O
1	O
,	O
t1	O
)	O
e	O
gil	O
.	O
g	O
j	O
}	O
i	O
s	O
d	O
/	O
(	O
1	O
-	O
~e	O
{	O
lg	O
;	O
(	O
sj	O
)	O
-	O
gj	O
(	O
sjlil	O
)	O
'	O
(	O
1	O
-	O
e/mi	O
<	O
<	O
e-ke	O
/	O
m	O
,	O
where	O
we	O
used	O
the	O
definition	B
of	I
the	O
functions	O
gl	O
,	O
,	O
..	O
,	O
gm	O
'	O
observe	O
that	O
the	O
expected	O
number	O
of	O
pairs	O
(	O
gi	O
,	O
gj	O
)	O
of	O
these	O
functions	O
,	O
such	O
that	O
the	O
corresponding	O
sets	O
g	O
i	O
=	O
{	O
(	O
x	O
,	O
t	O
)	O
:	O
t	O
:	O
:	O
:	O
:	O
gi	O
(	O
x	O
)	O
}	O
andg	O
j	O
=	O
{	O
(	O
x	O
,	O
t	O
)	O
:	O
t	O
:	O
:	O
:	O
:	O
gj	O
(	O
x	O
)	O
}	O
pick	O
the	O
same	O
points	O
,	O
is	O
bounded	O
by	O
e	O
{	O
i	O
{	O
(	O
gi	O
'	O
gj	O
)	O
;	O
g	O
i	O
and	O
g	O
j	O
pick	O
the	O
same	O
points	O
}	O
i	O
}	O
~	O
(	O
~	O
)	O
p	O
{	O
g	O
;	O
and	O
g	O
j	O
pick	O
the	O
same	O
point	O
'	O
)	O
<	O
s	O
(	O
~	O
)	O
e-k	O
,	O
/m	O
since	O
for	O
k	O
randomly	O
chosen	O
points	O
the	O
average	O
number	O
of	O
pairs	O
that	O
pick	O
the	O
same	O
points	O
is	O
bounded	O
by	O
g	O
)	O
e-ke/	O
m	O
,	O
there	O
exist	O
k	O
points	O
in	O
rd	O
x	O
[	O
0	O
,	O
m	O
]	O
,	O
such	O
that	O
the	O
number	O
of	O
pairs	O
(	O
gi	O
,	O
gj	O
)	O
that	O
pick	O
the	O
same	O
points	O
is	O
actually	O
bounded	O
by	O
c	O
;	O
)	O
e-ke/	O
m	O
.	O
for	O
each	O
such	O
pair	O
we	O
can	O
add	O
one	O
more	O
point	O
in	O
rd	O
x	O
[	O
0	O
,	O
m	O
]	O
such	O
that	O
the	O
point	O
is	O
contained	O
in	O
gil	O
.	O
g	O
j.	O
thus	O
,	O
we	O
have	O
obtained	O
a	O
set	O
of	O
no	O
more	O
than	O
k	O
+	O
(	O
~	O
)	O
e-ke/m	O
points	O
such	O
that	O
the	O
sets	O
g	O
i	O
,	O
...	O
,	O
g	O
m	O
pick	O
different	O
subsets	O
of	O
it	O
.	O
since	O
k	O
was	O
arbitrary	O
,	O
we	O
can	O
choose	O
it	O
to	O
minimize	O
this	O
expression	B
.	O
this	O
yields	O
l	O
~	O
log	O
(	O
ee	O
g	O
)	O
/	O
m	O
)	O
j	O
points	O
,	O
so	O
the	O
shatter	B
coefficient	I
of	O
f+	O
corresponding	O
to	O
this	O
number	O
must	O
be	O
greater	O
than	O
m	O
,	O
which	O
proves	O
the	O
statement	O
.	O
0	O
the	O
meaning	O
of	O
theorem	O
29.3	O
is	O
best	O
seen	O
from	O
the	O
following	O
simple	O
corollary	O
:	O
corollary	O
29.2.	O
let	O
f	O
be	O
a	O
class	O
0/	O
[	O
0	O
,	O
m	O
]	O
-valued/unctions	O
on	O
rd	O
.	O
for	O
every	O
e	O
>	O
0	O
and	O
probability	O
measure	B
jk	O
,	O
m	O
(	O
e	O
,	O
f	O
)	O
:	O
:	O
:	O
:	O
-e-	O
1og	O
-e-	O
(	O
4em	O
2em	O
)	O
vf+	O
498	O
29.	O
uniform	B
laws	I
of	I
large	I
numbers	I
proof	O
.	O
recall	O
that	O
theorem	B
13.2	O
implies	O
the	O
inequality	B
follows	O
from	O
theorem	B
29.3	O
by	O
straightforward	O
calculation	O
.	O
the	O
details	O
are	O
left	O
as	O
an	O
exercise	O
(	O
problem	O
29.2	O
)	O
.	O
d	O
recently	O
haussler	O
(	O
1991	O
)	O
was	O
able	O
to	O
get	O
rid	O
of	O
the	O
``	O
log	O
''	O
factor	O
in	O
the	O
above	O
upper	O
bound	O
.	O
he	O
proved	O
that	O
if	O
e	O
=	O
kin	O
for	O
an	O
integer	O
k	O
,	O
then	O
m	O
(	O
e	O
,	O
f	O
)	O
:	O
s	O
e	O
(	O
d	O
+	O
1	O
)	O
2	O
)	O
v.f+	O
e	O
e	O
(	O
the	O
quantity	O
v.	O
:	O
f+	O
is	O
sometimes	O
called	O
the	O
pseudo	B
dimension	I
of	O
f	O
(	O
see	O
problem	O
29.3	O
)	O
.	O
it	O
follows	O
immediately	O
from	O
theorem	B
13.9	O
that	O
if	O
f	O
is	O
a	O
linear	O
space	O
of	O
functions	O
of	O
dimension	O
r	O
,	O
then	O
its	O
pseudo	B
dimension	I
is	O
at	O
most	O
r	O
+	O
1.	O
a	O
few	O
more	O
properties	O
are	O
worth	O
mentioning	O
:	O
theorem	B
29.4	O
.	O
(	O
wenocur	O
and	O
dudley	O
,	O
(	O
1981	O
)	O
)	O
.	O
let	O
g	O
:	O
r	O
d	O
~	O
r	O
be	O
an	O
arbitrary	O
junction	O
,	O
and	O
consider	O
the	O
class	O
ojjunctions	O
9	O
=	O
{	O
g	O
+	O
j	O
;	O
j	O
e	O
f	O
}	O
.	O
then	O
vg+	O
=	O
vf+	O
.	O
proof	O
.	O
if	O
the	O
points	O
(	O
sl	O
'	O
tl	O
)	O
,	O
...	O
,	O
(	O
sk	O
,	O
tk	O
)	O
e	O
rd	O
x	O
r	O
are	O
shattered	O
by	O
f+	O
,	O
then	O
the	O
points	O
(	O
sl	O
'	O
tl	O
+	O
g	O
(	O
sl	O
)	O
)	O
,	O
...	O
,	O
(	O
sk	O
,	O
tk	O
+	O
g	O
(	O
sk	O
)	O
)	O
are	O
shattered	O
by	O
g+	O
.	O
this	O
proves	O
the	O
proof	O
of	O
the	O
other	O
inequality	B
is	O
similar	O
.	O
d	O
theorem	B
29.5	O
.	O
(	O
nolan	O
and	O
pollard	O
(	O
1987	O
)	O
;	O
dudley	O
,	O
(	O
1987	O
)	O
)	O
.	O
let	O
g	O
:	O
[	O
0	O
,	O
m	O
]	O
~	O
r	O
be	O
a	O
fixed	O
nondecreasing	O
junction	O
,	O
and	O
define	O
the	O
class	O
9	O
=	O
{	O
g	O
0	O
j	O
;	O
j	O
e	O
f	O
}	O
.	O
then	O
vg+	O
:	O
s	O
vp	O
.	O
proof	O
.	O
assume	O
that	O
n	O
:	O
s	O
vg+	O
,	O
and	O
let	O
the	O
functions	O
fr	O
,	O
...	O
,12	O
''	O
e	O
f	O
be	O
such	O
that	O
the	O
binary	B
vector	O
takes	O
all	O
2n	O
values	O
if	O
j	O
=	O
1	O
,	O
...	O
,	O
2n	O
.	O
for	O
all	O
1	O
:	O
:	O
;	O
i	O
:	O
:	O
;	O
n	O
define	O
the	O
numbers	O
(	O
i	O
{	O
g	O
(	O
/j	O
(	O
sl	O
»	O
c	O
:	O
:td	O
'	O
...	O
,	O
i	O
{	O
g	O
(	O
/j	O
(	O
sn	O
»	O
?	O
:	O
.tn	O
}	O
)	O
and	O
29.5	O
covering	B
numbers	O
and	O
shatter	O
coefficients	O
499	O
by	O
the	O
monotonicity	O
of	O
g	O
,	O
ui	O
>	O
li	O
.	O
then	O
the	O
binary	B
vector	O
takes	O
the	O
same	O
value	O
as	O
for	O
every	O
j	O
:	O
:	O
:	O
:	O
2n	O
.	O
therefore	O
,	O
the	O
pairs	O
(	O
~	O
)	O
(	O
un	O
+in	O
)	O
'	O
...	O
,	O
sn	O
,	O
sl	O
,	O
2	O
2	O
are	O
shattered	O
by	O
f+	O
,	O
which	O
proves	O
the	O
theorem	B
.	O
0	O
next	O
we	O
present	O
a	O
few	O
results	O
about	O
covering	B
numbers	O
of	O
classes	O
of	O
functions	O
whose	O
members	O
are	O
sums	O
or	O
products	O
of	O
functions	O
from	O
other	O
classes	O
.	O
similar	O
results	O
can	O
be	O
found	O
in	O
nobel	O
(	O
1992	O
)	O
,	O
nolan	O
and	O
pollard	O
(	O
1987	O
)	O
,	O
and	O
pollard	O
(	O
1990	O
)	O
.	O
theorem	B
29.6.	O
let	O
fl	O
,	O
...	O
,	O
fk	O
be	O
classes	O
of	O
real	O
functions	O
on	O
rd	O
.	O
for	O
n	O
arbitrary	O
,	O
fixed	O
points	O
z	O
'	O
{	O
=	O
(	O
zl	O
,	O
...	O
,	O
zn	O
)	O
in	O
r	O
d	O
,	O
define	O
the	O
sets	O
fl	O
(	O
z7	O
)	O
,	O
...	O
,	O
fk	O
(	O
z7	O
)	O
in	O
rn	O
by	O
j	O
=	O
1	O
,	O
...	O
,	O
k.	O
also	O
,	O
introduce	O
f	O
(	O
z7	O
)	O
=	O
{	O
(	O
f	O
(	O
zl	O
)	O
,	O
...	O
,	O
f	O
(	O
zn	O
»	O
;	O
f	O
e	O
f	O
}	O
for	O
the	O
class	O
offunctions	O
thenfor	O
every	O
e	O
>	O
0	O
and	O
z7	O
n	O
(	O
e	O
,	O
f	O
(	O
z7	O
)	O
)	O
:	O
:	O
:	O
:	O
it	O
n	O
(	O
e/	O
k	O
,	O
fj	O
(	O
z7	O
)	O
)	O
·	O
k	O
j=l	O
proof	O
.	O
let	O
sl	O
,	O
...	O
,	O
sk	O
c	O
rn	O
be	O
minimal	O
e	O
/	O
k-coverings	O
of	O
fl	O
(	O
z'j	O
)	O
,	O
...	O
,	O
fk	O
(	O
z'j	O
)	O
,	O
re	O
(	O
cid:173	O
)	O
spectively	O
.	O
this	O
implies	O
that	O
for	O
any	O
/j	O
e	O
fj	O
there	O
is	O
a	O
vector	O
s	O
j	O
=	O
(	O
sy	O
)	O
,	O
...	O
,	O
sjn	O
)	O
e	O
sj	O
such	O
that	O
for	O
every	O
j	O
=	O
1	O
,	O
...	O
,	O
k.	O
moreover	O
,	O
isj	O
i	O
=	O
n	O
(	O
e	O
/	O
k	O
,	O
fj	O
(	O
z'j	O
)	O
.	O
we	O
show	O
that	O
s=	O
{	O
sl+	O
...	O
+sk	O
;	O
sj	O
esj	O
,	O
j=i	O
,	O
...	O
,	O
k	O
}	O
500	O
29.	O
uniform	B
laws	I
of	I
large	I
numbers	I
is	O
an	O
e-covering	O
of	O
:	O
f	O
(	O
z7	O
)	O
.	O
this	O
follows	O
immediately	O
from	O
the	O
triangle	O
inequality	B
,	O
since	O
for	O
any	O
fl	O
,	O
...	O
,	O
!	O
k	O
there	O
is	O
si	O
,	O
...	O
,	O
sk	O
such	O
that	O
<	O
1	O
~	O
i	O
-	O
l	O
...	O
.-	O
h	O
(	O
zl	O
)	O
-	O
sl	O
+	O
...	O
+	O
-	O
l	O
...	O
.-	O
!	O
k	O
(	O
zi	O
)	O
-	O
sk	O
n	O
i=1	O
1	O
~	O
i	O
n	O
i=1	O
ci	O
)	O
i	O
ci	O
)	O
i	O
<	O
k-	O
.	O
0	O
e	O
k	O
theorem	B
29.7	O
.	O
(	O
pollard	O
(	O
1990	O
)	O
)	O
.	O
let	O
:	O
f	O
and	O
9	O
be	O
classes	O
of	O
real	O
functions	O
on	O
r	O
d	O
,	O
bounded	O
by	O
ml	O
and	O
m	O
2	O
,	O
respectively	O
.	O
(	O
that	O
is	O
,	O
e.g.	O
,	O
if	O
(	O
x	O
)	O
1	O
s	O
ml	O
for	O
every	O
x	O
e	O
rd	O
and	O
f	O
e	O
:	O
f.	O
)	O
forarbitraryjixedpoints	O
z7	O
=	O
(	O
zi	O
,	O
...	O
,	O
zn	O
)	O
in	O
rd	O
dejine	O
the	O
sets	O
:	O
f	O
(	O
z7	O
)	O
and	O
g	O
(	O
z~	O
)	O
in	O
rh	O
as	O
in	O
theorem	O
29.6.	O
introduce	O
for	O
the	O
class	O
of	O
functions	O
.	O
:	O
j	O
=	O
{	O
fg	O
;	O
f	O
e	O
:	O
f	O
,	O
g	O
e	O
g	O
}	O
.	O
then	O
for	O
every	O
e	O
>	O
0	O
and	O
z~	O
proof	O
.	O
let	O
s	O
c	O
[	O
-mi	O
,	O
md	O
n	O
be	O
an	O
e/	O
(	O
2m2	O
)	O
-covering	O
of	O
:	O
f	O
(	O
z7	O
)	O
,	O
that	O
is	O
,	O
for	O
any	O
f	O
e	O
:	O
f	O
there	O
is	O
a	O
vector	O
s	O
=	O
(	O
s	O
(	O
1	O
)	O
,	O
...	O
,	O
sen	O
»	O
)	O
e	O
s	O
such	O
that	O
n	O
``	O
\	O
:	O
'	O
i	O
i	O
-	O
l	O
...	O
.-	O
n	O
i=i	O
<	O
-	O
e	O
.	O
2m2	O
f	O
(	O
zt	O
)	O
-	O
s	O
ci	O
)	O
i	O
it	O
is	O
easy	O
to	O
see	O
that	O
s	O
can	O
be	O
chosen	O
such	O
that	O
lsi	O
=	O
n	O
(	O
e/	O
(	O
2m2	O
)	O
,	O
:	O
f	O
(	O
z	O
'd	O
)	O
.	O
similarly	O
,	O
let	O
t	O
c	O
[	O
-m2	O
,	O
m	O
2	O
]	O
be	O
an	O
e/	O
(	O
2md-covering	O
of	O
g	O
(	O
z7	O
)	O
with	O
iti	O
=	O
n	O
(	O
e/	O
(	O
2ml	O
)	O
,	O
g	O
(	O
z	O
'd	O
)	O
such	O
that	O
for	O
any	O
g	O
e	O
9	O
there	O
is	O
at	O
=	O
(	O
t	O
(	O
1	O
)	O
,	O
...	O
,	O
ten	O
»	O
~	O
e	O
t	O
with	O
we	O
show	O
that	O
the	O
set	O
u	O
=	O
{	O
st	O
;	O
s	O
e	O
s	O
,	O
t	O
e	O
t	O
}	O
is	O
an	O
e-covering	O
of	O
.	O
:	O
j	O
(	O
zv	O
.	O
let	O
f	O
e	O
:	O
f	O
and	O
g	O
e	O
9	O
be	O
arbitrary	O
and	O
s	O
e	O
sand	O
t	O
e	O
t	O
the	O
corresponding	O
vectors	O
such	O
that	O
then	O
29.6	O
generalized	O
linear	O
classification	O
501	O
29.6	O
generalized	O
linear	O
classification	O
in	O
this	O
section	O
we	O
use	O
the	O
uniform	B
laws	I
of	I
large	I
numbers	I
discussed	O
in	O
this	O
chapter	O
to	O
prove	O
that	O
squared	B
error	I
minimization	I
over	O
an	O
appropriately	O
chosen	O
class	O
of	O
generalized	O
linear	O
classifiers	O
yields	O
a	O
universally	O
consistent	B
rule	I
.	O
consider	O
the	O
class	O
c	O
(	O
kn	O
)	O
of	O
generalized	O
linear	O
classifiers	O
,	O
whose	O
members	O
are	O
functions	O
¢	O
:	O
nd	O
--	O
-7	O
>	O
(	O
cid:173	O
)	O
{	O
o	O
,	O
1	O
}	O
of	O
the	O
form	O
where	O
0/1	O
,	O
...	O
,	O
1fkn	O
are	O
fixed	O
basis	O
functions	O
,	O
and	O
the	O
coefficients	O
ai	O
,	O
...	O
,	O
akn	O
are	O
arbitrary	O
real	O
numbers	O
.	O
the	O
training	O
sequence	O
dn	O
is	O
used	O
to	O
determine	O
the	O
coeffi	O
(	O
cid:173	O
)	O
cients	O
ai	O
.	O
in	O
chapter	O
17	O
we	O
studied	O
the	O
behavior	O
of	O
the	O
classifier	B
whose	O
coefficients	O
are	O
picked	O
to	O
minimize	O
the	O
empirical	B
error	I
probability	O
instead	O
of	O
minimizing	O
the	O
empirical	B
error	I
probability	O
ln	O
(	O
¢	O
)	O
,	O
several	O
authors	O
sug	O
(	O
cid:173	O
)	O
gested	O
minimizing	O
the	O
empirical	O
squared	O
error	O
(	O
see	O
,	O
e.g.	O
,	O
duda	O
and	O
hart	O
(	O
1973	O
)	O
,	O
vapnik	O
(	O
1982	O
)	O
)	O
.	O
this	O
is	O
rather	O
dangerous	O
.	O
for	O
example	O
,	O
for	O
k	O
=	O
1	O
and	O
d	O
=	O
1	O
it	O
is	O
easy	O
to	O
find	O
a	O
distribution	O
such	O
that	O
the	O
error	O
prob	O
(	O
cid:173	O
)	O
ability	O
of	O
the	O
linear	B
classifier	I
that	O
minimizes	O
the	O
empirical	O
squared	O
error	O
converges	O
to	O
1	O
-	O
e	O
,	O
while	O
the	O
error	O
probability	O
of	O
the	O
best	O
linear	B
classifier	I
is	O
e	O
,	O
where	O
e	O
is	O
an	O
arbitrarily	O
small	O
positive	O
number	O
(	O
theorem	B
4.7	O
)	O
.	O
clearly	O
,	O
similar	O
examples	O
can	O
502	O
29.	O
uniform	B
laws	I
of	I
large	I
numbers	I
be	O
found	O
for	O
any	O
fixed	O
k.	O
this	O
demonstrates	O
powerfully	O
the	O
danger	O
of	O
minimizing	O
squared	B
error	I
instead	O
of	O
error	O
count	O
.	O
minimizing	O
the	O
latter	O
yields	O
a	O
classifier	O
whose	O
average	O
error	O
probability	O
is	O
always	O
within	O
0	O
(	O
jlog	O
n	O
/	O
n	O
)	O
of	O
the	O
optimum	O
in	O
the	O
class	O
,	O
for	O
fixed	O
k.	O
we	O
note	O
here	O
that	O
in	O
some	O
special	O
cases	O
minimization	O
of	O
the	O
two	O
types	O
of	O
error	O
are	O
equivalent	O
(	O
see	O
problem	O
29.5	O
)	O
.	O
interestingly	O
though	O
,	O
if	O
kn	O
-+	O
00	O
as	O
n	O
increases	O
,	O
we	O
can	O
obtain	O
universal	B
consistency	I
by	O
minimizing	O
the	O
empirical	O
squared	O
error	O
.	O
theorem	B
29.8.	O
let	O
vrl	O
,	O
'1f2	O
,	O
..	O
,	O
be	O
a	O
sequence	O
of	O
bounded	O
functions	O
with	O
i	O
vrj	O
(	O
x	O
)	O
1	O
:	O
s	O
1	O
such	O
that	O
the	O
set	O
of	O
all	O
finite	O
linear	O
combinations	O
of	O
the	O
vr	O
j	O
's	O
q	O
{	O
t	O
,	O
aj1fr/x	O
)	O
;	O
aj	O
,	O
a2	O
,	O
·	O
..	O
e	O
r	O
}	O
is	O
dense	O
in	O
l	O
2	O
(	O
/.t	O
)	O
on	O
all	O
balls	O
of	O
the	O
form	O
{	O
x	O
:	O
ilx	O
ii	O
:	O
s	O
m	O
}	O
for	O
any	O
probability	O
measure	B
/.t	O
.	O
let	O
the	O
coefficients	O
at	O
,	O
...	O
,	O
at	O
minimize	O
the	O
empirical	O
squared	O
error	O
under	O
the	O
constraint	O
l~'~l	O
la	O
j	O
i	O
:	O
s	O
bit	O
>	O
bn	O
2	O
:	O
:	O
1.	O
define	O
the	O
generalized	O
linear	O
clas	O
(	O
cid:173	O
)	O
sifier	O
gn	O
by	O
if	O
kn	O
and	O
bn	O
satisfy	O
then	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
-+	O
l	O
*	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
,	O
that	O
is	O
,	O
the	O
rule	B
gn	O
is	O
universally	O
consistent	O
.	O
if	O
we	O
assume	O
additionally	O
that	O
b~	O
log	O
n	O
=	O
o	O
(	O
n	O
)	O
,	O
then	O
gn	O
is	O
strongly	O
universally	O
consistent	O
.	O
proof	O
.	O
let	O
0	O
>	O
0	O
be	O
arbitrary	O
.	O
then	O
there	O
exists	O
a	O
constant	O
m	O
such	O
that	O
p	O
{	O
iixii	O
>	O
m	O
}	O
<	O
o.	O
thus	O
,	O
l	O
(	O
gn	O
)	O
-	O
l	O
*	O
:	O
s	O
0	O
+	O
p	O
{	O
gn	O
(	O
x	O
)	O
i	O
y	O
,	O
iixii	O
:	O
s	O
midn	O
}	O
-	O
p	O
{	O
g*	O
(	O
x	O
)	O
i	O
y	O
,	O
iixii	O
:	O
s	O
m	O
}	O
.	O
it	O
suffices	O
to	O
show	O
that	O
p	O
{	O
gn	O
(	O
x	O
)	O
i	O
y	O
,	O
iixii	O
:	O
s	O
midn	O
}	O
-	O
p	O
{	O
g*	O
(	O
x	O
)	O
i	O
y	O
,	O
iixii	O
:	O
:	O
:	O
m	O
}	O
-+	O
0	O
in	O
the	O
required	O
sense	O
for	O
every	O
m	O
>	O
o.	O
introduce	O
the	O
notation	O
fn*	O
(	O
x	O
)	O
=	O
l~	O
:	O
l	O
ajvrj	O
(	O
x	O
)	O
.	O
by	O
corollary	O
6.2	O
,	O
we	O
see	O
that	O
p	O
{	O
gn	O
(	O
x	O
)	O
i	O
y	O
,	O
iixii	O
:	O
s	O
midn	O
}	O
-	O
p	O
{	O
g*	O
(	O
x	O
)	O
i	O
y	O
,	O
iixii	O
:	O
s	O
m	O
}	O
<	O
(	O
j11xll-	O
:	O
:	O
:	O
:m	O
(	O
f	O
,	O
:	O
(	O
x	O
)	O
-	O
(	O
21j	O
(	O
x	O
)	O
-	O
1	O
)	O
)	O
2/.t	O
(	O
dx	O
)	O
.	O
29.6	O
generalized	O
linear	O
classification	O
503·	O
we	O
prove	O
that	O
the	O
right-hand	O
side	O
converges	O
to	O
zero	O
in	O
probability	O
.	O
observe	O
that	O
since	O
e	O
{	O
2y	O
-	O
llx	O
=	O
x	O
}	O
=	O
217	O
(	O
x	O
)	O
-	O
1	O
,	O
for	O
any	O
function	O
hex	O
)	O
,	O
(	O
h	O
(	O
x	O
)	O
-	O
(	O
217	O
(	O
x	O
)	O
-	O
1	O
»	O
2	O
=	O
e	O
{	O
(	O
h	O
(	O
x	O
)	O
-	O
(	O
2y	O
-	O
1	O
»	O
21x	O
=	O
x	O
}	O
-	O
e	O
{	O
(	O
2y	O
-	O
1	O
)	O
-	O
(	O
217	O
(	O
x	O
)	O
-	O
1	O
)	O
2ix	O
=	O
x	O
}	O
(	O
see	O
chapter	O
2	O
)	O
,	O
therefore	O
,	O
denoting	O
the	O
class	O
of	O
functions	O
over	O
which	O
we	O
minimize	O
by	O
we	O
have	O
[	O
jiixiis	O
:	O
m	O
(	O
fn*	O
(	O
x	O
)	O
-	O
(	O
217	O
(	O
x	O
)	O
-	O
1	O
»	O
)	O
2	O
fj.	O
,	O
(	O
dx	O
)	O
=	O
e	O
{	O
(	O
fn*	O
(	O
x	O
)	O
-	O
(	O
2y	O
-	O
1	O
»	O
)	O
2	O
1	O
{	O
iixiis	O
:	O
m	O
}	O
i	O
dn	O
}	O
-	O
e	O
{	O
(	O
(	O
2y	O
-	O
1	O
)	O
-	O
(	O
217	O
(	O
x	O
)	O
-	O
1	O
»	O
2	O
1	O
{	O
iixiis	O
:	O
m	O
}	O
}	O
(	O
e	O
{	O
(	O
t~	O
(	O
x	O
)	O
-	O
(	O
2y	O
-	O
1	O
»	O
)	O
2	O
1	O
{	O
iixiis	O
:	O
m	O
}	O
i	O
dn	O
}	O
=	O
-	O
inf	O
e	O
{	O
(	O
f	O
(	O
x	O
)	O
-	O
fe	O
:	O
!	O
;	O
,	O
(	O
2y	O
-	O
1	O
»	O
2	O
1	O
{	O
iixiis	O
:	O
m	O
}	O
}	O
)	O
+	O
inf	O
e	O
{	O
(	O
f	O
(	O
x	O
)	O
-	O
fe	O
:	O
fn	O
(	O
2y	O
-	O
1	O
»	O
2	O
i	O
{	O
jlx	O
ii	O
s	O
:	O
m	O
}	O
}	O
-	O
e	O
{	O
(	O
(	O
2y	O
-	O
1	O
)	O
-	O
(	O
217	O
(	O
x	O
)	O
-	O
1	O
»	O
2	O
1	O
{	O
iixiis	O
:	O
m	O
}	O
}	O
.	O
the	O
last	O
two	O
terms	O
may	O
be	O
combined	O
to	O
yield	O
inf	O
[	O
fe	O
:	O
fn	O
jiixiis	O
:	O
m	O
(	O
f	O
(	O
x	O
)	O
-	O
(	O
217	O
(	O
x	O
)	O
-	O
1	O
)	O
i	O
fj.	O
,	O
(	O
dx	O
)	O
,	O
which	O
converges	O
to	O
zero	O
by	O
the	O
denseness	B
assumption	O
.	O
to	O
prove	O
that	O
the	O
first	O
term	O
converges	O
to	O
zero	O
in	O
probability	O
,	O
observe	O
that	O
we	O
may	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
p	O
{	O
ii	O
x	O
ii	O
>	O
m	O
}	O
=	O
o.	O
as	O
in	O
the	O
proof	O
of	O
lemma	O
8.2	O
,	O
it	O
is	O
easy	O
to	O
show	O
that	O
e	O
{	O
(	O
t	O
:	O
(	O
x	O
)	O
-	O
(	O
2y	O
-	O
1	O
)	O
)	O
2	O
1	O
{	O
lixiis	O
:	O
m	O
)	O
i	O
dn	O
}	O
-	O
inf	O
e	O
{	O
(	O
f	O
(	O
x	O
)	O
-	O
fe	O
:	O
!	O
;	O
,	O
(	O
2y	O
-	O
1	O
)	O
2	O
1	O
{	O
iixiis	O
:	O
m	O
}	O
}	O
=	O
e	O
{	O
(	O
fn*	O
(	O
x	O
)	O
-	O
(	O
2y	O
_1	O
)	O
)	O
21	O
dn	O
}	O
-	O
inf	O
e	O
{	O
(	O
f	O
(	O
x	O
)	O
-	O
fe	O
:	O
!	O
;	O
,	O
(	O
2y	O
_l	O
)	O
2	O
}	O
:	O
s	O
2	O
;	O
~~	O
,	O
;	O
;	O
~	O
(	O
f	O
(	O
xi	O
)	O
-	O
1	O
n	O
(	O
2yi	O
-	O
1	O
»	O
)	O
2	O
-	O
e	O
{	O
(	O
f	O
(	O
x	O
)	O
-	O
(	O
2y	O
-	O
1	O
»	O
2	O
}	O
i	O
1	O
2	O
sup	O
i~	O
th	O
(	O
xi	O
,	O
yi	O
)	O
-	O
e	O
{	O
h	O
(	O
x	O
,	O
y	O
)	O
}	O
i	O
,	O
hej	O
n	O
i=l	O
504	O
29.	O
uniform	B
laws	I
of	I
large	I
numbers	I
where	O
the	O
class	O
of	O
functions	O
j	O
is	O
defined	O
by	O
j	O
=	O
{	O
hex	O
,	O
y	O
)	O
=	O
(	O
f	O
(	O
x	O
)	O
-	O
(	O
2y	O
-	O
1	O
)	O
)	O
2	O
;	O
f	O
e	O
fn	O
}	O
.	O
observe	O
that	O
since	O
12y	O
-	O
11	O
=	O
1	O
and	O
i	O
1/i	O
;	O
(	O
x	O
)	O
i	O
:	O
:	O
:	O
:	O
1	O
,	O
we	O
have	O
therefore	O
,	O
theorem	B
29.1	O
asserts	O
that	O
p	O
{	O
e	O
{	O
(	O
f	O
,	O
;	O
(	O
x	O
)	O
-	O
(	O
2y	O
-	O
1	O
)	O
)	O
21	O
dn	O
}	O
-	O
inf	O
e	O
{	O
(	O
f	O
(	O
x	O
)	O
-	O
(	O
2y	O
-	O
1	O
)	O
)	O
2	O
}	O
>	O
e	O
}	O
fe	O
:	O
fn	O
:	O
:	O
:	O
:	O
p	O
{	O
sup	O
i~	O
th	O
(	O
xi	O
,	O
yi	O
)	O
-	O
e	O
{	O
h	O
(	O
x	O
,	O
y	O
)	O
}	O
1	O
>	O
e/2	O
}	O
hej	O
n	O
i==l	O
where	O
z7	O
=	O
(	O
xl	O
,	O
y1	O
)	O
,	O
.·.	O
,	O
(	O
xn	O
,	O
yn	O
)	O
.	O
next	O
,	O
for	O
fixed	O
z7	O
,	O
we	O
estimate	B
the	O
cover	O
(	O
cid:173	O
)	O
ing	O
number	O
n	O
(	O
e	O
116	O
,	O
j	O
(	O
z7	O
)	O
)	O
.	O
for	O
arbitrary	O
fl	O
'	O
12	O
e	O
:01	O
'	O
consider	O
the	O
functions	O
hi	O
(	O
x	O
,	O
y	O
)	O
=	O
(	O
fl	O
(	O
x	O
)	O
-	O
(	O
2y	O
-	O
1	O
)	O
)	O
2.	O
then	O
for	O
any	O
probability	O
measure	B
v	O
on	O
nd	O
x	O
{	O
o	O
,	O
i	O
}	O
,	O
(	O
2y	O
-	O
1	O
)	O
?	O
and	O
h	O
2	O
(	O
x	O
,	O
y	O
)	O
=	O
(	O
h	O
(	O
x	O
)	O
-	O
f	O
ih	O
1	O
(	O
x	O
,	O
y	O
)	O
-	O
h2	O
(	O
x	O
,	O
y	O
)	O
lv	O
(	O
d	O
(	O
x	O
,	O
y	O
)	O
=	O
f	O
i	O
(	O
fl	O
(	O
x	O
)	O
-	O
:	O
:	O
:	O
:	O
f	O
21	O
!	O
i	O
(	O
x	O
)	O
-	O
h	O
(	O
x	O
)	O
l	O
(	O
bn	O
+	O
l	O
)	O
v	O
(	O
d	O
(	O
x	O
,	O
y	O
)	O
)	O
:	O
:	O
:	O
:	O
4bn	O
f	O
i	O
fi	O
(	O
x	O
)	O
-	O
hex	O
)	O
1/j	O
,	O
(	O
dx	O
)	O
,	O
(	O
2y	O
-	O
1	O
)	O
)	O
2	O
-	O
(	O
h	O
(	O
x	O
)	O
-	O
(	O
2y	O
-	O
1	O
)	O
)	O
21	O
v	O
(	O
d	O
(	O
x	O
,	O
y	O
)	O
)	O
where	O
fj.	O
,	O
is	O
the	O
marginal	O
measure	B
for	O
v	O
on	O
nd	O
.	O
thus	O
,	O
for	O
any	O
z7	O
=	O
(	O
xl	O
,	O
yd	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
and	O
e	O
,	O
n	O
(	O
e	O
,	O
j	O
(	O
zl	O
»	O
:5	O
:	O
n	O
(	O
4	O
:	O
n	O
'	O
f	O
,	O
,	O
(	O
x~	O
»	O
)	O
.	O
therefore	O
,	O
it	O
suffices	O
to	O
estimate	B
the	O
covering	B
number	I
corresponding	O
to	O
fn	O
.	O
since	O
:	O
:	O
:	O
:	O
kn	O
+	O
1	O
(	O
theorem	B
13.9	O
)	O
.	O
fn	O
is	O
a	O
subset	O
of	O
a	O
linear	O
space	O
of	O
functions	O
,	O
we	O
have	O
v.1	O
;	O
;	O
-	O
by	O
corollary	O
29.2	O
,	O
n	O
(	O
e	O
4bn	O
:	O
f	O
,	O
(	O
ll	O
)	O
)	O
n	O
xl	O
'	O
8	O
b	O
e	O
n	O
1	O
:	O
:	O
:	O
:	O
e/	O
(	O
4bn	O
4	O
b	O
e	O
n	O
)	O
og	O
e/	O
(	O
4bn	O
(	O
)	O
kn+l	O
)	O
(	O
32	O
b2	O
)	O
2	O
(	O
kll+l	O
)	O
e	O
11	O
:	O
:	O
:	O
:	O
-e	O
-	O
problems	O
and	O
exercises	O
505	O
summarizing	O
,	O
we	O
have	O
p	O
{	O
e	O
{	O
(	O
fn*	O
(	O
x	O
)	O
-	O
(	O
2y	O
-	O
1	O
»	O
)	O
21	O
dn	O
}	O
-	O
inf	O
e	O
{	O
(	O
f	O
(	O
x	O
)	O
-	O
(	O
2y	O
-	O
1	O
»	O
2	O
}	O
>	O
e	O
}	O
fefi1	O
which	O
goes	O
to	O
zero	O
if	O
knb~	O
log	O
(	O
bn	O
)	O
/n	O
~	O
0.	O
the	O
proof	O
of	O
the	O
theorem	B
is	O
completed	O
.	O
it	O
is	O
easy	O
to	O
see	O
that	O
if	O
we	O
assume	O
additionally	O
that	O
b~	O
log	O
n	O
/	O
n	O
~	O
0	O
,	O
then	O
strong	B
universal	I
consistency	I
follows	O
by	O
applying	O
the	O
borel-cantelli	O
lemma	O
to	O
the	O
last	O
probability	O
.	O
0	O
remark	O
.	O
minimization	O
of	O
the	O
squared	B
error	I
is	O
attractive	O
because	O
there	O
are	O
efficient	O
algorithms	O
to	O
find	O
the	O
minimizing	O
coefficients	O
,	O
while	O
minimizing	O
the	O
number	O
of	O
errors	O
committed	O
on	O
the	O
training	O
sequence	O
is	O
computationally	O
more	O
difficult	O
.	O
if	O
the	O
dimension	B
k	O
of	O
the	O
generalized	O
linear	O
classifier	O
is	O
fixed	O
,	O
then	O
stochastic	O
approxi	O
(	O
cid:173	O
)	O
mation	O
asymptotically	O
provides	O
the	O
minimizing	O
coefficients	O
.	O
for	O
more	O
information	O
about	O
this	O
we	O
refer	O
to	O
robbins	O
and	O
monro	O
(	O
1951	O
)	O
,	O
kiefer	O
and	O
wolfowitz	O
(	O
1952	O
)	O
,	O
dvoretzky	O
(	O
1956	O
)	O
,	O
fabian	O
(	O
1971	O
)	O
,	O
tsypkin	O
(	O
1971	O
)	O
,	O
nevelson	O
and	O
khasminskii	O
(	O
1973	O
)	O
,	O
kushner	O
(	O
1984	O
)	O
,	O
ruppert	O
(	O
1991	O
)	O
,	O
and	O
ljung	O
,	O
pflug	O
,	O
and	O
walk	O
(	O
1992	O
)	O
.	O
for	O
example	O
,	O
gyorfi	O
(	O
1984	O
)	O
proved	O
that	O
if	O
(	O
ul	O
,	O
vi	O
)	O
,	O
(	O
u2	O
,	O
v2	O
)	O
,	O
...	O
form	O
a	O
stationary	O
and	O
ergodic	O
sequence	O
,	O
in	O
which	O
each	O
pair	O
is	O
distributed	O
as	O
the	O
bounded	O
random	O
variable	B
pair	O
(	O
u	O
,	O
v	O
)	O
e	O
nk	O
x	O
n	O
,	O
and	O
the	O
vector	O
of	O
coefficients	O
a	O
=	O
(	O
ai	O
,	O
...	O
,	O
ak	O
)	O
minimizes	O
and	O
a	O
(	O
o	O
)	O
e	O
nk	O
is	O
arbitrary	O
,	O
then	O
the	O
sequence	O
of	O
coefficient	O
vectors	O
defined	O
by	O
a	O
(	O
n+1	O
)	O
-	O
a	O
(	O
n	O
)	O
__	O
1_	O
(	O
a	O
(	O
n	O
)	O
t	O
u	O
-	O
n	O
+	O
1	O
-	O
v.	O
)	O
u	O
n+1	O
n+1	O
n+1	O
satisfies	O
lim	O
r	O
(	O
a	O
(	O
n	O
»	O
=	O
r	O
(	O
a	O
)	O
a.s.	O
0	O
n-+oo	O
problems	O
and	O
exercises	O
problem	O
29.1.	O
find	O
a	O
class	O
:	O
f	O
containing	O
two	O
functions	O
1	O
]	O
1	O
,	O
1	O
]	O
2	O
:	O
n	O
-+	O
[	O
0	O
,	O
1	O
]	O
and	O
a	O
distri	O
(	O
cid:173	O
)	O
bution	O
of	O
(	O
x	O
,	O
y	O
)	O
such	O
that	O
min	O
(	O
l	O
(	O
1	O
]	O
i	O
)	O
,	O
l	O
(	O
1	O
]	O
2	O
)	O
)	O
=	O
l	O
*	O
,	O
but	O
as	O
n	O
-+	O
00	O
,	O
the	O
probability	O
converges	O
to	O
one	O
,	O
where	O
1	O
]	O
n	O
is	O
selected	O
from	O
:	O
f	O
by	O
minimizing	O
the	O
empirical	O
squared	O
error	O
.	O
506	O
29.	O
uniform	B
laws	I
of	I
large	I
numbers	I
problem	O
29.2.	O
prove	O
corollary	O
29.2.	O
problem	O
29.3.	O
let	O
fbe	O
a	O
class	O
of	O
functions	O
on	O
n	O
d	O
,	O
taking	O
their	O
values	O
in	O
[	O
0	O
,	O
m	O
]	O
.	O
haussler	O
(	O
1992	O
)	O
defines	O
the	O
pseudo	B
dimension	I
of	O
f	O
as	O
the	O
largest	O
integer	O
n	O
for	O
which	O
there	O
exist	O
n	O
points	O
in	O
n	O
d	O
,	O
zj	O
,	O
...	O
,	O
zn	O
,	O
and	O
a	O
vector	O
v	O
=	O
(	O
v	O
(	O
l	O
)	O
,	O
...	O
,	O
v	O
(	O
n	O
)	O
)	O
e	O
nn	O
such	O
that	O
the	O
binary	B
n-vector	O
takes	O
a1l2n	O
possible	O
values	O
as	O
f	O
ranges	O
through	O
f.	O
prove	O
that	O
the	O
pseudo	B
dimension	I
of	O
f	O
equals	O
the	O
quantity	O
v	O
f+	O
defined	O
in	O
the	O
text	O
.	O
problem	O
29.4.	O
consistency	B
of	O
clustering	B
.	O
let	O
x	O
,	O
xi	O
,	O
...	O
,	O
xn	O
be	O
i.i.d	O
.	O
random	O
variables	O
in	O
nd	O
,	O
and	O
assume	O
that	O
there	O
is	O
a	O
number	O
0	O
<	O
m	O
<	O
00	O
such	O
that	O
p	O
{	O
x	O
e	O
[	O
-m	O
,	O
m	O
]	O
d	O
}	O
=	O
l.	O
take	O
the	O
empirically	O
optimal	O
clustering	O
of	O
xl	O
,	O
...	O
,	O
x	O
n	O
,	O
that	O
is	O
,	O
find	O
the	O
points	O
ai	O
,	O
...	O
,	O
ak	O
that	O
minimize	O
the	O
empirical	O
squared	O
error	O
:	O
the	O
error	O
of	O
the	O
clustering	B
is	O
defined	O
by	O
the	O
mean	O
squared	B
error	I
prove	O
that	O
if	O
aj	O
,	O
...	O
,	O
ak	O
denote	O
the	O
empirically	O
optimal	O
cluster	O
centers	O
,	O
then	O
and	O
that	O
for	O
every	O
e	O
>	O
0	O
p	O
{	O
s	O
p	O
u	O
bj	O
,	O
...	O
,	O
bk	O
erd	O
ie	O
(	O
b	O
ii	O
1	O
,	O
···	O
,	O
k	O
-e	O
i	O
,	O
··	O
''	O
b	O
)	O
(	O
b	O
b	O
)	O
1	O
>	O
e	O
}	O
<	O
_	O
4e8n2k	O
(	O
d+l	O
)	O
e-ne2/	O
(	O
32m4	O
)	O
.	O
k	O
conclude	O
that	O
the	O
error	O
of	O
the	O
empirically	O
optimal	O
clustering	O
converges	O
to	O
that	O
of	O
the	O
truly	O
optimal	O
one	O
as	O
n	O
-+	O
00	O
.	O
(	O
pollard	O
(	O
1981	O
;	O
1982	O
)	O
,	O
linder	O
,	O
lugosi	O
,	O
and	O
zeger	O
(	O
1994	O
»	O
)	O
.	O
hint	O
:	O
for	O
the	O
first	O
part	O
proceed	O
as	O
in	O
the	O
proof	O
of	O
lemma	O
8.2.	O
for	O
the	O
second	O
part	O
use	O
the	O
technique	O
shown	O
in	O
corollary	O
29.1.	O
to	O
compute	O
the	O
vc	B
dimension	I
,	O
exploit	O
corollary	O
13.2.	O
problem	O
29.5.	O
let	O
v	O
!	O
'1	O
,	O
...	O
,	O
v	O
!	O
'k	O
be	O
indicator	O
functions	O
of	O
cells	O
of	O
a	O
k-way	O
partition	B
of	O
nd	O
.	O
consider	O
generalized	O
linear	O
classifiers	O
based	O
on	O
these	O
functions	O
.	O
show	O
that	O
the	O
classifier	B
obtained	O
by	O
minimizing	O
the	O
number	O
of	O
errors	O
made	O
on	O
the	O
training	O
sequence	O
is	O
the	O
same	O
as	O
for	O
the	O
classifier	B
obtained	O
by	O
minimizing	O
the	O
empirical	O
squared	O
error	O
.	O
point	O
out	O
that	O
this	O
is	O
just	O
the	O
histogram	O
classifier	O
based	O
on	O
the	O
partition	B
defined	O
by	O
the	O
v	O
!	O
'i	O
's	O
(	O
csibi	O
(	O
1975	O
»	O
.	O
30	O
neural	O
networks	O
30.1	O
multilayer	B
perceptrons	O
the	O
linear	B
discriminant	I
or	O
perceptron	B
(	O
see	O
chapter	O
4	O
)	O
makes	O
a	O
decision	O
<	O
/j	O
(	O
x	O
)	O
=	O
{	O
o	O
if	O
1/r	O
(	O
x~	O
:	O
:	O
:	O
1/2	O
1	O
otherwlse	O
,	O
based	O
upon	O
a	O
linear	O
combination	O
1/r	O
(	O
x	O
)	O
of	O
the	O
inputs	O
,	O
1/r	O
(	O
x	O
)	O
=	O
co	O
+	O
l	O
cix	O
(	O
i	O
)	O
=	O
co	O
+	O
ct	O
x	O
,	O
d	O
i=l	O
(	O
30.1	O
)	O
where	O
the	O
ci	O
's	O
are	O
weights	O
,	O
x	O
=	O
(	O
x	O
(	O
i	O
)	O
,	O
...	O
,	O
x	O
(	O
d	O
)	O
l	O
,	O
and	O
c	O
=	O
(	O
cl	O
'	O
...	O
,	O
cd	O
)	O
t.	O
this	O
is	O
called	O
a	O
neural	O
network	O
without	O
hidden	O
layers	O
(	O
see	O
figure	O
4.1	O
)	O
.	O
in	O
a	O
(	O
feed-forward	O
)	O
neural	B
network	I
with	O
one	O
hidden	O
layer	O
,	O
one	O
takes	O
1/r	O
(	O
x	O
)	O
=	O
co	O
+	O
l	O
ci	O
cr	O
(	O
1/ri	O
(	O
x	O
)	O
)	O
,	O
k	O
i=l	O
(	O
30.2	O
)	O
where	O
the	O
c/s	O
are	O
as	O
before	O
,	O
and	O
each	O
1/ri	O
is	O
of	O
the	O
form	O
given	O
in	O
(	O
30.1	O
)	O
:	O
1/ri	O
(	O
x	O
)	O
=	O
hi	O
+	O
l~=l	O
aijx	O
(	O
j	O
)	O
for	O
some	O
constants	O
hi	O
and	O
aij	O
.	O
the	O
function	O
cr	O
is	O
called	O
a	O
sigmoid	O
.	O
we	O
define	O
sigmoids	O
to	O
be	O
nondecreasing	O
functions	O
with	O
cr	O
(	O
x	O
)	O
-+	O
-1	O
as	O
x	O
{	O
..	O
-00	O
and	O
cr	O
(	O
x	O
)	O
-+	O
1	O
as	O
x	O
t	O
00.	O
examples	O
include	O
:	O
(	O
1	O
)	O
the	O
threshold	B
sigmoid	O
-i	O
cr	O
(	O
x	O
)	O
=	O
1	O
{	O
if	O
x	O
:	O
:	O
:	O
0	O
if	O
x	O
>	O
0	O
;	O
508	O
30.	O
neural	O
networks	O
(	O
2	O
)	O
the	O
standard	B
,	O
or	O
logistic	B
,	O
sigmoid	B
o-	O
(	O
x	O
)	O
=	O
-	O
-	O
-	O
1+	O
e-	O
x	O
'	O
(	O
3	O
)	O
the	O
arctan	B
sigmoid	O
o-	O
(	O
x	O
)	O
=	O
-	O
arctan	B
(	O
x	O
)	O
;	O
2	O
tc	O
(	O
4	O
)	O
the	O
gaussian	B
sigmoid	O
o-	O
(	O
x	O
)	O
=	O
2	O
1	O
__	O
e-	O
u	O
/2du	O
-	O
1	O
.	O
2	O
j	O
x	O
-oo~	O
oot	O
1	O
figure	O
30.1.	O
a	O
neural	O
network	O
with	B
one	I
hidden	I
layer	I
.	O
the	O
hidden	O
neurons	O
are	O
those	O
within	O
the	O
frame	O
.	O
~~~~	O
a	O
(	O
x	O
)	O
=	O
~	O
f	O
e-v212	O
dv	O
-	O
1	O
i-e-x	O
a	O
(	O
x	O
)	O
=	O
1	O
+e-x	O
a	O
(	O
x	O
)	O
=	O
~	O
arctan	B
(	O
x	O
)	O
-i	O
x	O
2	O
figure	O
30.2.	O
the	O
threshold	B
,	O
standard	B
,	O
arctan	B
,	O
and	O
gaussian	B
sig	O
(	O
cid:173	O
)	O
moids	O
.	O
30.1	O
multilayer	B
perceptrons	O
509	O
for	O
early	O
discussion	O
of	O
multilayer	O
perceptrons	O
,	O
see	O
rosenblatt	O
(	O
1962	O
)	O
,	O
barron	O
(	O
1975	O
)	O
,	O
nilsson	O
(	O
1965	O
)	O
,	O
and	O
minsky	O
and	O
papert	O
(	O
1969	O
)	O
.	O
surveys	O
may	O
be	O
found	O
in	O
barron	O
and	O
barron	O
(	O
1988	O
)	O
,	O
ripley	O
(	O
1993	O
;	O
1994	O
)	O
,	O
hertz	O
,	O
krogh	O
,	O
and	O
palmer	O
(	O
1991	O
)	O
,	O
and	O
weiss	O
and	O
kulikowski	O
(	O
1991	O
)	O
.	O
in	O
the	O
perceptron	B
with	O
one	O
hidden	O
layer	O
,	O
we	O
say	O
that	O
there	O
are	O
k	O
hidden	O
neurons	O
(	O
cid:173	O
)	O
the	O
output	O
of	O
the	O
i-th	O
hidden	O
neuron	O
is	O
ui	O
=	O
cj	O
(	O
o/i	O
(	O
x	O
)	O
)	O
.	O
thus	O
,	O
(	O
30.2	O
)	O
may	O
be	O
rewritten	O
as	O
1jf	O
(	O
x	O
)	O
=	O
co	O
+	O
l	O
ciui	O
,	O
k	O
i=l	O
which	O
is	O
similar	O
in	O
form	O
to	O
(	O
30.1	O
)	O
.	O
we	O
may	O
continue	O
this	O
process	O
and	O
create	O
multi	O
(	O
cid:173	O
)	O
layer	O
feed-forward	O
neural	O
networks	O
.	O
for	O
example	O
,	O
a	O
two-hidden-iayer	O
perceptron	B
uses	O
1jf	O
(	O
x	O
)	O
=	O
co	O
+	O
l	O
cizi	O
,	O
i	O
i=l	O
where	O
and	O
u	O
.	O
=	O
cj	O
(	O
b	O
.	O
+	O
~	O
a	O
..	O
x	O
(	O
i	O
)	O
)	O
}	O
1	O
<	O
j	O
'	O
<	O
k	O
,	O
}	O
~	O
}	O
l	O
,	O
_	O
_	O
and	O
the	O
dij	O
's	O
,	O
b/s	O
,	O
and	O
aj/s	O
are	O
constants	O
.	O
the	O
first	O
hidden	O
layer	O
has	O
k	O
hidden	O
neurons	O
,	O
while	O
the	O
second	O
hidden	O
layer	O
has	O
i	O
hidden	O
neurons	O
.	O
i=l	O
figure	O
30.3.	O
afeed-forward	O
neural	B
network	I
with	O
two	O
hidden	O
lay-	O
ers	O
.	O
510	O
30.	O
neural	O
networks	O
the	O
step	O
from	O
perceptron	B
to	O
a	O
one-hidden-iayer	O
neural	B
network	I
is	O
nontrivial	O
.	O
we	O
know	O
that	O
linear	O
discriminants	O
can	O
not	O
possibly	O
lead	O
to	O
universally	O
consistent	O
rules	O
.	O
fortunately	O
,	O
one-hidden-iayer	O
neural	O
networks	O
yield	O
universally	O
consistent	O
discriminants	O
provided	O
that	O
we	O
allow	O
k	O
,	O
the	O
number	O
of	O
hidden	O
neurons	O
,	O
to	O
grow	O
unboundedly	O
with	O
n.	O
the	O
interest	O
in	O
neural	O
networks	O
is	O
undoubtedly	O
due	O
to	O
the	O
possibility	O
of	O
implementing	O
them	O
directly	O
via	O
processors	O
and	O
circuits	O
.	O
as	O
the	O
hard	O
(	O
cid:173	O
)	O
ware	O
is	O
fixed	O
beforehand	O
,	O
one	O
does	O
not	O
have	O
the	O
luxury	O
to	O
let	O
k	O
become	O
a	O
function	O
of	O
n	O
,	O
and	O
thus	O
,	O
the	O
claimed	O
universal	B
consistency	I
is	O
a	O
moot	O
point	O
.	O
we	O
will	O
deal	O
with	O
both	O
fixed	O
architectures	O
and	O
variable-sized	O
neural	O
networks	O
.	O
because	O
of	O
the	O
universal	B
consistency	I
of	O
one-hidden-iayer	O
neural	O
networks	O
,	O
there	O
is	O
little	O
theoret	O
(	O
cid:173	O
)	O
ical	O
gain	O
in	O
considering	O
neural	O
networks	O
with	O
more	O
than	O
one	O
hidden	O
layer	O
.	O
there	O
may	O
,	O
however	O
,	O
be	O
an	O
information-theoretic	O
gain	O
as	O
the	O
number	O
of	O
hidden	O
neurons	O
needed	O
to	O
achieve	O
the	O
same	O
performance	O
may	O
be	O
substantially	O
reduced	O
.	O
in	O
fact	O
,	O
we	O
will	O
make	O
a	O
case	O
for	O
two	O
hidden	O
layers	O
,	O
and	O
show	O
that	O
after	O
two	O
hidden	O
layers	O
,	O
little	O
is	O
gained	O
for	O
classification	O
.	O
for	O
theoretical	B
analysis	O
,	O
the	O
neural	O
networks	O
are	O
rooted	O
in	O
a	O
classical	O
theorem	B
by	O
kolmogorov	O
(	O
1957	O
)	O
and	O
lorentz	O
(	O
1976	O
)	O
which	O
states	O
that	O
every	O
continuous	O
function	O
f	O
on	O
[	O
0	O
,	O
l	O
]	O
d	O
can	O
be	O
written	O
as	O
where	O
the	O
g	O
ij	O
,	O
s	O
and	O
the	O
fi	O
'	O
s	O
are	O
continuous	O
functions	O
whose	O
form	O
depends	O
on	O
f.	O
we	O
will	O
see	O
that	O
neural	O
networks	O
approximate	O
any	O
measurable	O
function	O
with	O
arbitrary	O
precision	O
,	O
despite	O
the	O
fact	O
that	O
the	O
form	O
of	O
the	O
sigmoids	O
is	O
fixed	O
beforehand	O
.	O
as	O
an	O
example	O
,	O
consider	O
d	O
=	O
2.	O
the	O
function	O
x	O
(	O
1	O
)	O
x	O
(	O
2	O
)	O
is	O
rewritten	O
as	O
which	O
is	O
in	O
the	O
desired	O
form	O
.	O
however	O
,	O
it	O
is	O
much	O
less	O
obvious	O
how	O
one	O
would	O
rewrite	O
more	O
general	O
continuous	O
functions	O
.	O
in	O
fact	O
,	O
in	O
neural	O
networks	O
,	O
we	O
approx	O
(	O
cid:173	O
)	O
imate	O
the	O
gij	O
's	O
and	O
fi	O
's	O
by	O
functions	O
of	O
the	O
form	O
a	O
(	O
b+a	O
t	O
x	O
)	O
and	O
allow	O
the	O
number	O
of	O
tunable	O
coefficients	O
to	O
be	O
high	O
enough	O
such	O
that	O
any	O
continuous	O
function	O
may	O
be	O
represented-though	O
no	O
longer	O
rewritten	O
exactly	O
in	O
the	O
form	O
of	O
kolmogorov	O
and	O
lorentz	O
.	O
we	O
discuss	O
other	O
examples	O
of	O
approximations	O
based	O
upon	O
such	O
rep	O
(	O
cid:173	O
)	O
resentations	O
in	O
a	O
later	O
section	O
.	O
30.2	O
arrangements	O
511	O
.	O
input	O
•	O
•	O
•	O
figure	O
30.4.	O
the	O
general	O
kolmogorov-lorentz	O
representation	O
of	O
a	O
continuous	O
function	O
.	O
30.2	O
arrangements	O
a	O
finite	O
set	O
a	O
of	O
hyperplanes	O
in	O
n	O
d	O
partitions	O
the	O
space	O
into	O
connected	O
convex	O
polyhedral	O
pieces	O
of	O
various	O
dimensions	O
.	O
such	O
a	O
partition	O
p	O
=	O
pea	O
)	O
is	O
called	O
an	O
arrangement	B
.	O
an	O
arrangement	B
is	O
called	O
simple	O
if	O
any	O
d	O
hyperplanes	O
of	O
a	O
have	O
a	O
unique	O
point	O
in	O
common	O
and	O
if	O
d	O
+	O
1	O
hyperplanes	O
have	O
no	O
point	O
in	O
common	O
.	O
figure	O
30.5.	O
an	O
arrangement	B
of	O
five	O
lines	O
in	O
the	O
plane	O
.	O
figure	O
30.6.	O
an	O
arrangement	B
clas	O
(	O
cid:173	O
)	O
sifier	O
.	O
a	O
simple	O
arrangement	B
creates	O
polyhedral	O
cells	O
.	O
interestingly	O
,	O
the	O
number	O
of	O
these	O
cells	O
is	O
independent	O
of	O
the	O
actual	O
configuration	O
of	O
the	O
hyperplanes	O
.	O
in	O
particular	O
,	O
512	O
30.	O
neural	O
networks	O
the	O
number	O
of	O
cells	O
is	O
exactly	O
2k	O
if	O
d	O
:	O
:	O
:	O
:	O
k	O
,	O
and	O
where	O
iai	O
=	O
k.	O
for	O
a	O
proof	O
of	O
this	O
,	O
see	O
problem	O
22.1	O
,	O
or	O
lemma	O
1.2	O
of	O
edelsbrunner	O
(	O
1987	O
)	O
.	O
for	O
general	O
arrangements	O
,	O
this	O
is	O
merely	O
an	O
upper	O
bound	O
.	O
we	O
may	O
of	O
course	O
use	O
arrangements	O
for	O
designing	O
classifiers	O
.	O
we	O
let	O
ga	O
be	O
the	O
natural	B
classifier	I
obtained	O
by	O
taking	O
majority	O
votes	O
over	O
all	O
yi	O
's	O
for	O
which	O
xi	O
is	O
in	O
the	O
same	O
cell	O
of	O
the	O
arrangement	B
p	O
=	O
pea	O
)	O
as	O
x.	O
all	O
classifiers	O
discussed	O
in	O
this	O
section	O
possess	O
the	O
property	O
that	O
they	O
are	O
invariant	O
under	O
linear	O
transformations	O
and	O
universally	O
consistent	O
(	O
in	O
some	O
cases	O
,	O
we	O
assume	O
that	O
x	O
has	O
a	O
density	O
,	O
but	O
that	O
is	O
only	O
done	O
to	O
avoid	O
messy	O
technicalities	O
)	O
.	O
if	O
we	O
fix	O
k	O
and	O
find	O
that	O
a	O
with	O
iai	O
=	O
k	O
for	O
which	O
the	O
empirical	B
error	I
is	O
minimal	O
,	O
we	O
obtain-perhaps	O
at	O
great	O
computational	O
expense-the	O
empirical	B
risk	I
optimized	O
classifier	B
.	O
there	O
is	O
a	O
general	O
theorem	B
for	O
such	O
classifiers-see	O
,	O
for	O
example	O
,	O
corollary	O
23.2-the	O
conditions	O
of	O
which	O
are	O
as	O
follows	O
:	O
(	O
1	O
)	O
it	O
must	O
be	O
possible	O
to	O
select	O
a	O
given	O
sequence	O
of	O
a	O
's	O
for	O
which	O
ln	O
(	O
ga	O
)	O
(	O
the	O
conditional	O
probability	O
of	O
error	O
with	O
ga	O
)	O
tends	O
to	O
l	O
*	O
in	O
probability	O
.	O
but	O
if	O
k	O
-+	O
00	O
,	O
we	O
may	O
align	O
the	O
hyperplanes	O
with	O
the	O
axes	O
,	O
and	O
create	O
a	O
cubic	O
histogram	O
,	O
for	O
which	O
,	O
by	O
theorem	B
6.2	O
,	O
we	O
have	O
consistency	B
if	O
the	O
grid	O
expands	O
to	O
00	O
and	O
the	O
cell	O
sizes	O
in	O
the	O
grid	O
shrink	O
to	O
o.	O
thus	O
,	O
as	O
k	O
-+	O
00	O
,	O
this	O
condition	O
holds	O
trivially	O
.	O
(	O
2	O
)	O
the	O
collection	O
9	O
=	O
{	O
gal	O
is	O
not	O
too	O
rich	O
,	O
in	O
the	O
sense	O
that	O
njlogs	O
(	O
9	O
,	O
n	O
)	O
-+	O
00	O
,	O
where	O
s	O
(	O
9	O
,	O
n	O
)	O
denotes	O
the	O
shatter	B
coefficient	I
of	O
9	O
,	O
that	O
is	O
,	O
the	O
maximal	O
number	O
of	O
ways	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
can	O
be	O
split	O
by	O
sets	O
of	O
the	O
form	O
(	O
u	O
a	O
x	O
{	O
o	O
}	O
)	O
u	O
(	O
u	O
a	O
x	O
{	O
i	O
}	O
)	O
.	O
aep	O
(	O
a	O
)	O
a	O
ep	O
(	O
a	O
)	O
if	O
iai	O
=	O
1	O
,	O
we	O
know	O
that	O
s	O
(	O
9	O
,	O
n	O
)	O
:	O
:	O
:	O
2	O
(	O
n	O
d	O
+	O
1	O
)	O
(	O
see	O
chapter	O
13	O
)	O
.	O
for	O
iai	O
=	O
k	O
,	O
a	O
trivial	O
upper	O
bound	O
is	O
(	O
2	O
(	O
n	O
d	O
+	O
1	O
)	O
)	O
k	O
.	O
the	O
consistency	B
condition	O
is	O
fulfilled	O
if	O
k	O
=	O
o	O
(	O
nj	O
log	O
n	O
)	O
.	O
we	O
have	O
theorem	B
30.1.	O
the	O
empirical-risk-optimized	O
arrangement	B
classifier	I
based	O
upon	O
arrangements	O
with	O
iai	O
:	O
:	O
:	O
k	O
has	O
e	O
{	O
l	O
n	O
}	O
-+	O
l	O
*	O
for	O
all	O
distributions	O
if	O
k	O
-+	O
00	O
and	O
k	O
=	O
o	O
(	O
nj	O
log	O
n	O
)	O
.	O
30.2	O
anangements	O
513	O
arrangements	O
can	O
also	O
be	O
made	O
from	O
the	O
data	O
at	O
hand	O
in	O
a	O
simpler	O
way	O
.	O
fix	O
k	O
points	O
xl	O
,	O
...	O
,	O
x	O
k	O
in	O
general	O
position	O
and	O
look	O
at	O
all	O
possible	O
e	O
)	O
hyperplanes	O
you	O
can	O
form	O
with	O
these	O
points	O
.	O
these	O
form	O
your	O
collection	O
a	O
,	O
which	O
defines	O
your	O
arrangement	B
.	O
no	O
optimization	O
of	O
any	O
kind	O
is	O
performed	O
.	O
we	O
take	O
the	O
nat	O
(	O
cid:173	O
)	O
ural	O
classifier	B
obtained	O
by	O
a	O
majority	O
vote	O
within	O
the	O
cells	O
of	O
the	O
partition	B
over	O
(	O
xk+l	O
,	O
yk+d	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
.	O
figure	O
30.7.	O
arrangement	B
determined	O
by	O
k	O
=	O
4	O
data	O
points	O
on	O
the	O
plane	O
.	O
here	O
we	O
can	O
not	O
apply	O
the	O
powerful	O
consistency	B
theorem	O
mentioned	O
above	O
.	O
also	O
,	O
the	O
arrangement	B
is	O
no	O
longer	O
simple	O
.	O
nevertheless	O
,	O
the	O
partition	B
of	O
space	O
depends	O
on	O
the	O
xi	O
's	O
only	O
,	O
and	O
thus	O
theorem	B
6.1	O
(	O
together	O
with	O
lemma	O
20.1	O
)	O
is	O
useful	O
.	O
the	O
rule	B
thus	O
obtained	O
is	O
consistent	O
if	O
diam	O
(	O
a	O
(	O
x	O
)	O
)	O
-+	O
0	O
in	O
probability	O
and	O
the	O
number	O
of	O
cells	O
is	O
o	O
(	O
n	O
)	O
,	O
where	O
a	O
(	O
x	O
)	O
is	O
the	O
cell	O
to	O
which	O
x	O
belongs	O
in	O
the	O
arrangement	B
.	O
as	O
the	O
number	O
of	O
cells	O
is	O
certainly	O
not	O
more	O
than	O
d	O
(	O
k	O
'	O
)	O
l·	O
,	O
i=o	O
l	O
where	O
k	O
'	O
=	O
e	O
)	O
,	O
we	O
see	O
that	O
the	O
number	O
of	O
cells	O
divided	O
by	O
n	O
tends	O
to	O
zero	O
if	O
this	O
puts	O
a	O
severe	O
restriction	O
on	O
the	O
growth	O
of	O
k.	O
however	O
,	O
it	O
is	O
easy	O
to	O
prove	O
the	O
following	O
:	O
lemma	O
30.1.	O
if	O
k	O
-+	O
00	O
,	O
then	O
diam	O
(	O
a	O
(	O
x	O
)	O
)	O
-+	O
0	O
in	O
probability	O
whenever	O
x	O
has	O
a	O
density	O
.	O
proof	O
.	O
as	O
noted	O
in	O
chapter	O
20	O
(	O
see	O
problem	O
20.6	O
)	O
,	O
the	O
set	O
of	O
all	O
x	O
for	O
which	O
for	O
all	O
e	O
>	O
0	O
,	O
we	O
have	O
jl	O
(	O
x	O
+	O
e	O
qi	O
)	O
>	O
0	O
for	O
all	O
quadrants	O
ql	O
,	O
...	O
,	O
q2d	O
having	O
one	O
vertex	O
at	O
(	O
0	O
,	O
0	O
,	O
...	O
,	O
0	O
)	O
and	O
sides	O
of	O
length	O
one	O
,	O
has	O
jl-measure	O
one	O
.	O
for	O
such	O
x	O
,	O
if	O
at	O
least	O
one	O
of	O
the	O
x/s	O
(	O
i	O
~	O
k	O
)	O
falls	O
in	O
each	O
of	O
the	O
2d	O
quadrants	O
x	O
+	O
eqi	O
,	O
then	O
diam	O
(	O
a	O
(	O
x	O
)	O
)	O
~	O
2de	O
(	O
see	O
figure	O
30.8	O
)	O
.	O
514	O
30.	O
neural	O
networks	O
figure	O
30.8.	O
the	O
diameter	O
of	O
the	O
cell	O
con	O
(	O
cid:173	O
)	O
taining	O
x	O
is	O
less	O
than	O
4e	O
if	O
there	O
is	O
a	O
data	O
point	O
in	O
each	O
of	O
the	O
four	O
quadrants	O
of	O
size	O
e	O
around	O
x.	O
therefore	O
,	O
for	O
arbitrary	O
e	O
>	O
0	O
,	O
p	O
{	O
diam	O
(	O
a	O
(	O
x	O
»	O
>	O
2de	O
}	O
:	O
:	O
:	O
;	O
2d	O
(	O
1	O
-	O
min	O
m	O
(	O
x	O
+	O
eqd	O
)	O
k	O
-+	O
o.	O
i~i~2d	O
thus	O
,	O
by	O
the	O
lebesgue	O
dominated	B
convergence	I
theorem	I
,	O
p	O
{	O
diam	O
(	O
a	O
(	O
x	O
»	O
>	O
2de	O
}	O
-+	O
o	O
.	O
0	O
theorem	B
30.2.	O
the	O
arrangement	B
classifier	I
defined	O
above	O
is	O
consistent	O
whenever	O
x	O
has	O
a	O
density	O
and	O
the	O
theorem	B
points	O
out	O
that	O
empirical	B
error	I
minimization	O
over	O
a	O
finite	O
set	O
of	O
arrangements	O
can	O
also	O
be	O
consistent	O
.	O
such	O
a	O
set	O
may	O
be	O
formed	O
as	O
the	O
collection	O
of	O
arrangements	O
consisting	O
of	O
hyperplanes	O
through	O
d	O
points	O
of	O
xl	O
,	O
...	O
,	O
x	O
k.	O
as	O
nothing	O
new	O
is	O
added	O
here	O
to	O
the	O
discussion	O
,	O
we	O
refer	O
the	O
reader	O
to	O
problem	O
30.1.	O
so	O
how	O
do	O
we	O
deal	O
with	O
arrangements	O
in	O
a	O
computer	O
?	O
clearly	O
,	O
to	O
reach	O
a	O
cell	O
,	O
we	O
find	O
for	O
each	O
hyperplane	B
a	O
e	O
a	O
the	O
side	O
to	O
which	O
x	O
belongs	O
.	O
if	O
f	O
(	O
x	O
)	O
=	O
at	O
x	O
+	O
ao	O
,	O
then	O
f	O
(	O
x	O
)	O
>	O
0	O
in	O
one	O
halfplane	O
,	O
f	O
(	O
x	O
)	O
=	O
0	O
on	O
the	O
hyperplane	B
,	O
and	O
f	O
(	O
x	O
)	O
<	O
0	O
in	O
the	O
other	O
halfplane	O
.	O
if	O
a	O
=	O
{	O
ai	O
,	O
...	O
,	O
ad	O
,	O
the	O
vector	O
(	O
/	O
{	O
hl	O
(	O
x	O
»	O
o	O
}	O
,	O
•••	O
,	O
i	O
{	O
hk	O
(	O
x	O
»	O
o	O
}	O
)	O
describes	O
the	O
cell	O
to	O
which	O
x	O
belongs	O
,	O
where	O
hi	O
(	O
x	O
)	O
is	O
a	O
linear	O
function	O
that	O
is	O
positive	O
if	O
x	O
is	O
on	O
one	O
side	O
of	O
the	O
hyperplane	B
ai	O
,	O
negative	O
if	O
x	O
is	O
on	O
the	O
other	O
side	O
of	O
ai	O
,	O
and	O
0	O
if	O
x	O
e	O
ai	O
.	O
a	O
decision	O
is	O
thus	O
reached	O
in	O
time	O
o	O
(	O
kd	O
)	O
.	O
more	O
importantly	O
,	O
the	O
whole	O
process	O
is	O
easily	O
parallelizable	O
and	O
can	O
be	O
pictured	O
as	O
a	O
battery	O
of	O
perceptrons	O
.	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
classifier	B
depicted	O
in	O
figure	O
30.9	O
is	O
identical	O
to	O
the	O
arrangement	B
classifier	I
.	O
in	O
neural	O
network	O
terminology	O
,	O
the	O
first	O
hidden	O
layer	O
of	O
neurons	O
corresponds	O
to	O
just	O
k	O
perceptrons	O
(	O
and	O
has	O
k	O
(	O
d	O
+	O
1	O
)	O
weights	O
or	O
parameters	O
,	O
if	O
you	O
wish	O
)	O
.	O
the	O
first	O
layer	O
outputs	O
a	O
k-vector	O
of	O
bits	O
that	O
pinpoints	O
the	O
precise	O
location	O
of	O
x	O
in	O
the	O
cells	O
of	O
the	O
arrangement	B
.	O
the	O
second	O
layer	O
only	O
assigns	O
a	O
class	O
(	O
decision	O
)	O
to	O
each	O
cell	O
of	O
the	O
arrangement	B
by	O
firing	O
up	O
one	O
neuron	O
.	O
it	O
has	O
2k	O
neurons	O
(	O
for	O
class	O
assignments	O
)	O
,	O
but	O
of	O
course	O
,	O
in	O
natural	O
classifiers	O
,	O
these	O
neurons	O
do	O
not	O
require	O
training	O
or	O
learning-the	O
majority	B
vote	I
takes	O
care	O
of	O
that	O
.	O
30.2	O
arrangements	O
515	O
~	O
•	O
•	O
i-~o~orljl~	O
,	O
-/	O
±	O
x	O
o	O
or	O
1	O
oor	O
1	O
±	O
figure	O
30.9.	O
arrangement	B
classifier	I
realized	O
by	O
a	O
two-hidden	O
(	O
cid:173	O
)	O
layer	O
neural	B
network	I
.	O
each	O
of	O
the	O
2k	O
cells	O
in	O
the	O
second	O
hidden	O
layer	O
peiforms	O
an	O
``	O
and	O
''	O
operation	O
:	O
the	O
output	O
of	O
node	O
``	O
101	O
''	O
is	O
1	O
ifits	O
three	O
inputs	O
are	O
1,0	O
,	O
and	O
1	O
,	O
respectively	O
.	O
otherwise	O
its	O
output	O
is	O
o.	O
thus	O
,	O
one	O
and	O
only	O
one	O
of	O
the	O
2k	O
outputs	O
is	O
1.	O
if	O
a	O
more	O
classical	O
second	O
layer	O
is	O
needed-without	O
boolean	O
operations-let	O
b	O
=	O
(	O
bi	O
,	O
...	O
,	O
bk	O
)	O
be	O
the	O
k-vector	O
of	O
bits	O
seen	O
at	O
the	O
output	O
of	O
the	O
first	O
layer	O
.	O
assign	O
a	O
perceptron	O
in	O
the	O
second	O
layer	O
to	O
each	O
region	O
of	O
the	O
arrangement	B
and	O
define	O
the	O
output	O
z	O
e	O
{	O
-1	O
,	O
1	O
}	O
to	O
be	O
(	O
j	O
(	O
l~=l	O
c	O
j	O
b	O
j	O
-	O
k	O
+	O
1/2	O
)	O
,	O
where	O
c	O
j	O
e	O
{	O
-1	O
,	O
1	O
}	O
are	O
weights	O
.	O
for	O
each	O
region	O
of	O
the	O
arrangement	B
,	O
we	O
have	O
a	O
description	O
in	O
terms	O
of	O
c	O
=	O
(	O
cl	O
'	O
...	O
,	O
ck	O
)	O
.	O
the	O
argument	O
of	O
the	O
sigmoid	B
function	O
is	O
1/2	O
if	O
2b	O
j	O
-	O
1	O
=	O
c	O
j	O
for	O
all	O
j	O
and	O
is	O
negative	O
otherwise	O
.	O
hence	O
z	O
=	O
1	O
if	O
and	O
only	O
if	O
2b	O
-	O
1	O
=	O
c.	O
assume	O
we	O
now	O
take	O
a	O
decision	O
based	O
upon	O
the	O
sign	O
of	O
lwzzz	O
+	O
wo	O
,	O
z	O
where	O
the	O
wz	O
's	O
are	O
weights	O
and	O
the	O
zz	O
's	O
are	O
the	O
outputs	O
of	O
the	O
second	O
hidden	O
layer	O
.	O
assume	O
that	O
we	O
wish	O
to	O
assign	O
class	O
1	O
to	O
s	O
regions	O
in	O
the	O
arrangement	B
and	O
class	O
0	O
to	O
t	O
other	O
regions	O
.	O
for	O
a	O
class	O
1	O
region	O
l	O
,	O
set	O
wz	O
=	O
1	O
,	O
and	O
for	O
a	O
class	O
0	O
region	O
,	O
set	O
wz	O
=	O
-1.	O
define	O
wo	O
=	O
1	O
+	O
s	O
-	O
t.	O
then	O
,	O
if	O
zj	O
=	O
1	O
,	O
zi	O
=	O
-1	O
,	O
i	O
=i	O
j	O
,	O
l	O
wzzz	O
+	O
wo	O
=	O
w	O
j	O
+	O
wo	O
-	O
l	O
wi	O
=	O
{	O
~	O
1	O
if	O
wj	O
=	O
1	O
if	O
wj	O
=	O
-1.	O
z	O
i=/j	O
516	O
30.	O
neural	O
networks	O
...	O
-	O
...	O
...	O
...	O
-_	O
...	O
...	O
...	O
...	O
...	O
...	O
.	O
-_	O
...	O
...	O
...	O
second	O
hidden	O
layer	O
oor	O
1	O
l	O
...	O
...	O
.	O
...	O
...	O
...	O
...	O
...	O
..	O
__	O
...	O
...	O
...	O
.	O
...	O
...	O
...	O
...	O
..	O
``	O
figure	O
30.10.	O
the	O
second	O
hidden	O
layer	O
of	O
a	O
two-hidden-layer	O
neural	B
network	I
with	O
threshold	B
sigmoids	O
in	O
the	O
first	O
layer	O
.	O
for	O
each	O
k-vector	O
of	O
bits	O
b	O
=	O
(	O
bt	O
,	O
...	O
,	O
bk	O
)	O
at	O
the	O
output	O
of	O
the	O
first	O
layer	O
,	O
we	O
may	O
find	O
a	O
decision	O
g	O
(	O
b	O
)	O
e	O
{	O
o	O
,	O
i	O
}	O
.	O
now	O
return	O
to	O
the	O
two	O
(	O
cid:173	O
)	O
hidden-layer	O
network	O
of	O
figure	O
30.9	O
and	O
assign	O
the	O
values	O
g	O
(	O
b	O
)	O
to	O
the	O
neurons	O
in	O
the	O
second	O
hidden	O
layer	O
to	O
obtain	O
an	O
equivalent	O
network	O
.	O
thus	O
,	O
every	O
arrangement	B
classifier	I
corresponds	O
to	O
a	O
neural	O
network	O
with	B
two	I
hidden	I
layers	I
,	O
and	O
threshold	B
units	O
.	O
the	O
correspondence	O
is	O
also	O
reciprocal	O
.	O
assume	O
someone	O
shows	O
a	O
two-hidden-iayer	O
neural	B
network	I
with	O
the	O
first	O
hidden	O
layer	O
as	O
above-thus	O
,	O
outputs	O
consist	O
of	O
a	O
vector	O
of	O
k	O
bits-and	O
with	O
the	O
second	O
hidden	O
layer	O
consisting	O
once	O
again	O
a	O
battery	O
of	O
perceptrons	O
(	O
see	O
figure	O
30.10	O
)	O
.	O
whatever	O
happens	O
in	O
the	O
second	O
hidden	O
layer	O
,	O
the	O
decision	O
is	O
just	O
a	O
function	O
of	O
the	O
config	O
(	O
cid:173	O
)	O
uration	O
of	O
k	O
input	O
bits	O
.	O
the	O
output	O
of	O
the	O
first	O
hidden	O
layer	O
is	O
constant	O
over	O
each	O
region	O
of	O
the	O
arrangement	B
defined	O
by	O
the	O
hyperplanes	O
given	O
by	O
the	O
input	O
weights	O
of	O
the	O
units	O
of	O
the	O
first	O
layer	O
.	O
thus	O
,	O
the	O
neural	B
network	I
classifier	O
with	O
threshold	O
units	O
in	O
the	O
first	O
hidden	O
layer	O
is	O
equivalent	O
to	O
an	O
arrangement	B
classifier	I
with	O
the	O
same	O
number	O
of	O
hyperplanes	O
as	O
units	O
in	O
the	O
first	O
hidden	O
layer	O
.	O
the	O
equivalence	O
with	O
tree	O
classifiers	O
is	O
described	O
in	O
problem	O
30.2.	O
of	O
course	O
,	O
equivalence	O
is	O
only	O
valid	O
up	O
to	O
a	O
certain	O
point	O
.	O
if	O
the	O
number	O
of	O
neurons	O
in	O
the	O
second	O
layer	O
is	O
small	O
,	O
then	O
neural	O
networks	O
are	O
more	O
restricted	O
.	O
this	O
could	O
be	O
an	O
advantage	O
in	O
training	O
.	O
however	O
,	O
the	O
majority	B
vote	I
in	O
an	O
arrangement	B
classifier	I
avoids	O
training	O
of	O
the	O
second	O
layer	O
's	O
neurons	O
altogether	O
,	O
and	O
offers	O
at	O
the	O
same	O
time	O
an	O
easier	O
interpretation	O
of	O
the	O
classifier	B
.	O
conditions	O
on	O
consistency	B
of	O
general	O
two-hidden-layer	O
neural	O
networks	O
will	O
be	O
given	O
in	O
section	O
30.4	O
.	O
30.3	O
approximation	O
by	O
neural	O
networks	O
517	O
30.3	O
approximation	O
by	O
neural	O
networks	O
consider	O
first	O
the	O
class	O
c	O
(	O
k	O
)	O
of	O
classifiers	O
(	O
30.2	O
)	O
that	O
contains	O
all	O
neural	B
network	I
classifiers	I
with	O
the	O
threshold	B
sigmoid	O
and	O
k	O
hidden	O
nodes	O
in	O
two	O
hidden	O
layers	O
.	O
the	O
training	O
data	O
dn	O
are	O
used	O
to	O
select	O
a	O
classifier	O
from	O
c	O
(	O
k	O
)	O
.	O
for	O
good	O
performance	O
of	O
the	O
selected	O
rule	B
,	O
it	O
is	O
necessary	O
that	O
the	O
best	O
rule	B
in	O
c	O
(	O
k	O
)	O
has	O
probability	O
of	O
error	O
close	O
to	O
l	O
*	O
,	O
that	O
is	O
,	O
that	O
inf	O
l	O
(	O
¢	O
)	O
-	O
l	O
*	O
¢ec	O
(	O
k	O
)	O
is	O
small	O
.	O
we	O
call	O
this	O
quantity	O
the	O
approximation	B
error	I
.	O
naturally	O
,	O
for	O
fixed	O
k	O
,	O
the	O
approximation	B
error	I
is	O
positive	O
for	O
most	O
distributions	O
.	O
however	O
,	O
for	O
large	O
k	O
,	O
it	O
is	O
expected	O
to	O
be	O
small	O
.	O
the	O
question	O
is	O
whether	O
the	O
last	O
statement	O
is	O
true	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
.	O
we	O
showed	O
in	O
the	O
section	O
on	O
arrangements	O
that	O
the	O
class	O
of	O
two-hid	O
den-layer	O
neural	B
network	I
classifiers	I
with	O
m	O
nodes	O
in	O
the	O
first	O
layer	O
and	O
2m	O
nodes	O
in	O
the	O
second	O
layer	O
contains	O
all	O
arrangement	B
classifiers	O
with	O
m	O
hyperplanes	O
.	O
therefore	O
,	O
for	O
k	O
=	O
m	O
+	O
2m	O
,	O
the	O
class	O
of	O
all	O
arrangement	B
classifiers	O
with	O
m	O
hyperplanes	O
is	O
a	O
subclass	O
of	O
c	O
(	O
k	O
)	O
.	O
from	O
this	O
,	O
we	O
easily	O
obtain	O
the	O
following	O
approximation	O
result	O
:	O
theorem	B
30.3.	O
ijc	O
(	O
k	O
)	O
is	O
the	O
class	O
of	O
all	O
neural	B
network	I
classifiers	I
with	O
the	O
thresh	O
(	O
cid:173	O
)	O
old	O
sigmoid	B
and	O
k	O
neurons	O
in	O
two	O
hidden	O
layers	O
,	O
then	O
inf	O
l	O
(	O
¢	O
)	O
-	O
l	O
*	O
=	O
0	O
lim	O
k-+oo	O
¢ec	O
(	O
k	O
)	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
.	O
it	O
is	O
more	O
surprising	O
that	O
the	O
same	O
property	O
holds	O
if	O
c	O
(	O
k	O
)	O
is	O
the	O
class	O
of	O
one	O
(	O
cid:173	O
)	O
hidden-layer	O
neural	O
networks	O
with	O
k	O
hidden	O
neurons	O
,	O
and	O
an	O
arbitrary	O
sigmoid	B
.	O
more	O
precisely	O
,	O
c	O
(	O
k	O
)	O
is	O
the	O
class	O
of	O
classifiers	O
¢	O
(	O
x	O
)	O
=	O
{	O
o	O
if	O
1/i	O
(	O
x	O
!	O
:	O
:	O
:	O
1/2	O
1	O
otherwise	O
,	O
where	O
1/1	O
is	O
as	O
in	O
(	O
30.2	O
)	O
.	O
by	O
theorem	B
2.2	O
,	O
we	O
have	O
l	O
(	O
¢	O
)	O
-	O
l*	O
~	O
2e	O
{	O
11/i	O
(	O
x	O
)	O
-1	O
]	O
(	O
x	O
)	O
i	O
}	O
'	O
where	O
yj	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
iix	O
=	O
x	O
}	O
.	O
thus	O
,	O
inf¢ec	O
(	O
k	O
)	O
l	O
(	O
¢	O
)	O
-	O
l*	O
-+	O
0	O
as	O
k	O
-+	O
00	O
if	O
for	O
some	O
sequence	O
{	O
1/ik	O
}	O
with	O
¢k	O
e	O
c	O
(	O
k	O
)	O
for	O
¢k	O
(	O
x	O
)	O
=	O
h	O
!	O
/ik	O
(	O
x	O
»	O
i/2	O
}	O
'	O
for	O
universal	B
consistency	I
,	O
we	O
need	O
only	O
assure	O
that	O
the	O
family	B
of	I
1/1	O
's	O
can	O
approximate	O
any	O
1	O
]	O
in	O
the	O
li	O
(	O
m	O
)	O
sense	O
.	O
in	O
other	O
words	O
,	O
the	O
approximation	B
error	I
infc/jec	O
(	O
k	O
)	O
l	O
(	O
¢	O
)	O
-	O
l*	O
converges	O
to	O
zero	O
if	O
the	O
class	O
of	O
functions	O
1/1	O
is	O
dense	O
in	O
li	O
(	O
m	O
)	O
for	O
every	O
m.	O
another	O
sufficient	O
condition	O
for	O
this-but	O
of	O
course	O
much	O
too	O
severe-is	O
that	O
the	O
class	O
518	O
30.	O
neural	O
networks	O
:	O
f	O
of	O
functions	O
1/r	O
becomes	O
dense	O
in	O
the	O
loo	O
(	O
supremum-	O
)	O
norm	O
in	O
the	O
space	O
of	O
continuous	O
functions	O
c	O
[	O
a	O
,	O
b	O
]	O
d	O
on	O
[	O
a	O
,	O
b	O
]	O
d	O
,	O
where	O
[	O
a	O
,	O
b	O
]	O
d	O
denotes	O
the	O
hyperrectangle	O
of	O
rd	O
defined	O
by	O
its	O
opposite	O
vertices	O
a	O
and	O
b	O
,	O
for	O
any	O
a	O
and	O
b.	O
lemma	O
30.2.	O
assume	O
that	O
a	O
sequence	O
of	O
classes	O
of	O
functions	O
:	O
fk	O
becomes	O
dense	O
in	O
the	O
loo	O
norm	O
in	O
the	O
space	O
of	O
continuous	O
functions	O
c	O
[	O
a	O
,	O
b	O
]	O
d	O
(	O
where	O
[	O
a	O
,	O
b	O
]	O
d	O
is	O
the	O
hyperrectangle	O
ofrd	O
defined	O
by	O
a	O
,	O
b	O
)	O
.	O
in	O
other	O
words	O
,	O
assume	O
that	O
for	O
every	O
a	O
,	O
b	O
e	O
r	O
d	O
,	O
and	O
every	O
bounded	O
function	O
g	O
,	O
lim	O
inf	O
k-+oo	O
jefk	O
xe	O
[	O
a	O
,	O
b	O
]	O
d	O
sup	O
if	O
(	O
x	O
)	O
-	O
g	O
(	O
x	O
)	O
1	O
=	O
0.	O
then	O
for	O
any	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
lim	O
k-+oo	O
rjjec	O
(	O
k	O
)	O
inf	O
l	O
(	O
¢	O
)	O
-	O
l*	O
=	O
0	O
,	O
where	O
c	O
(	O
k	O
)	O
is	O
the	O
class	O
of	O
classifiers	O
¢	O
(	O
x	O
)	O
=	O
!	O
r1/j	O
(	O
x	O
»	O
ij2	O
}	O
for	O
1/r	O
e	O
:	O
fk	O
.	O
proof	O
.	O
for	O
fixed	O
e	O
>	O
0	O
,	O
find	O
a	O
,	O
b	O
such	O
that	O
fl	O
(	O
[	O
a	O
,	O
b	O
]	O
d	O
)	O
~	O
1	O
-	O
e/3	O
,	O
where	O
fl	O
is	O
the	O
probability	O
measure	B
of	O
x.	O
choose	O
a	O
continuous	O
function	O
1	O
}	O
vanishing	O
off	O
[	O
a	O
,	O
b	O
]	O
d	O
such	O
that	O
e	O
{	O
lry	O
(	O
x	O
)	O
-	O
find	O
k	O
and	O
f	O
e	O
:	O
fk	O
such	O
that	O
____	O
ry	O
(	O
x	O
)	O
1	O
}	O
:	O
:	O
:	O
6	O
.	O
e	O
sup	O
xe	O
[	O
a	O
,	O
bjd	O
___	O
i/	O
(	O
x	O
)	O
-	O
1	O
]	O
(	O
x	O
)	O
i	O
:	O
:	O
:	O
-	O
.	O
e	O
6	O
for	O
¢	O
(	O
x	O
)	O
=	O
iu	O
(	O
x	O
»	O
lj2	O
}	O
,	O
we	O
have	O
,	O
by	O
theorem	B
2.2	O
,	O
l	O
(	O
¢	O
)	O
-l*	O
<	O
e	O
2e	O
{	O
if	O
(	O
x	O
)	O
-	O
ry	O
(	O
x	O
)	O
ii	O
{	O
xe	O
[	O
a	O
,	O
b	O
]	O
dd	O
+	O
3	O
''	O
<	O
2e	O
{	O
if	O
(	O
x	O
)	O
-1	O
}	O
(	O
x	O
)	O
ii	O
{	O
xe	O
[	O
a	O
,	O
b	O
]	O
dd	O
+2e	O
{	O
i1	O
}	O
(	O
x	O
)	O
-	O
ry	O
(	O
x	O
)	O
i	O
}	O
+	O
~	O
<	O
2	O
sup	O
if	O
(	O
x	O
)	O
-	O
xe	O
[	O
a	O
,	O
b	O
]	O
d	O
<	O
e.o	O
____	O
___	O
e	O
ry	O
(	O
x	O
)	O
1	O
+	O
+2e	O
{	O
lry	O
(	O
x	O
)	O
-	O
ry	O
(	O
x	O
)	O
i	O
}	O
+	O
-	O
3	O
this	O
text	O
is	O
basically	O
about	O
all	O
such	O
good	O
families	O
,	O
such	O
as	O
families	O
that	O
are	O
ob	O
(	O
cid:173	O
)	O
tainable	O
by	O
summing	O
kernel	B
functions	O
,	O
and	O
histogram	O
families	O
.	O
the	O
first	O
results	O
for	O
approximation	O
with	O
neural	O
networks	O
with	B
one	I
hidden	I
layer	I
appeared	O
in	O
1989	O
,	O
when	O
cybenko	O
(	O
1989	O
)	O
,	O
hornik	O
,	O
stinchcombe	O
,	O
and	O
white	O
(	O
1989	O
)	O
,	O
and	O
funahashi	O
(	O
1989	O
)	O
proved	O
independently	O
that	O
feedforward	O
neural	O
networks	O
with	B
one	I
hidden	I
layer	I
are	O
dense	O
with	B
respect	I
to	I
the	I
supremum	I
norm	I
on	O
bounded	O
sets	O
in	O
the	O
set	O
of	O
continuous	O
functions	O
.	O
in	O
other	O
words	O
,	O
by	O
taking	O
k	O
large	O
enough	O
,	O
every	O
continuous	O
function	O
30.3	O
approximation	O
by	O
neural	O
networks	O
519	O
on	O
nd	O
can	O
be	O
approximated	O
arbitrarily	O
closely	O
,	O
uniformly	O
over	O
any	O
bounded	O
set	O
by	O
functions	O
realized	O
by	O
neural	O
networks	O
with	B
one	I
hidden	I
layer	I
.	O
for	O
a	O
survey	O
of	O
various	O
denseness	B
results	O
we	O
refer	O
to	O
barron	O
(	O
1989	O
)	O
and	O
hornik	O
(	O
1993	O
)	O
.	O
the	O
proof	O
given	O
here	O
uses	O
ideas	O
of	O
chen	O
,	O
chen	O
,	O
and	O
liu	O
(	O
1990	O
)	O
.	O
it	O
uses	O
the	O
denseness	B
of	O
the	O
class	O
of	O
trigonometric	B
polynomials	O
in	O
the	O
loo	O
sense	O
for	O
c	O
[	O
o	O
,	O
l	O
]	O
d	O
(	O
thisds	O
a	O
special	O
case	O
of	O
the	O
stone-weierstrass	O
theorem	B
;	O
see	O
theorem	B
a.9	O
in	O
the	O
appendix	O
)	O
,	O
that	O
is	O
,	O
by	O
functions	O
of	O
the	O
form	O
l	O
(	O
ai	O
cos	O
(	O
2n	O
at	O
x	O
)	O
+	O
f3i	O
sin	O
(	O
2n	O
bt	O
x	O
)	O
)	O
,	O
i	O
where	O
ai	O
,	O
bi	O
are	O
integer-valued	O
vectors	O
of	O
rd	O
.	O
theorem	B
30.4.	O
for	O
every	O
continuous	O
function	O
f	O
:	O
[	O
a	O
,	O
b	O
]	O
d	O
~	O
nand	O
for	O
every	O
e	O
>	O
0	O
,	O
there	O
exists	O
a	O
neural	O
network	O
with	B
one	I
hidden	I
layer	I
and	O
function	O
t	O
(	O
x	O
)	O
as	O
in	O
(	O
30.2	O
)	O
such	O
that	O
sup	O
if	O
(	O
x	O
)	O
-ljr	O
(	O
x	O
)	O
1	O
<	O
e.	O
xe	O
[	O
a	O
,	O
b	O
]	O
d	O
proof	O
.	O
we	O
prove	O
the	O
theorem	B
for	O
the	O
threshold	B
sigmoid	O
a	O
(	O
x	O
)	O
=	O
{	O
-i	O
1	O
if	O
x	O
:	O
:	O
:	O
;	O
0	O
if	O
x	O
>	O
o.	O
the	O
extension	O
to	O
general	O
non	O
decreasing	O
sigmoids	O
is	O
left	O
as	O
an	O
exercise	O
(	O
prob	O
(	O
cid:173	O
)	O
lem	O
30.3	O
)	O
.	O
fix	O
e	O
>	O
o.	O
we	O
take	O
the	O
fourier	O
series	O
approximation	O
of	O
f	O
(	O
x	O
)	O
.	O
by	O
the	O
stone-weierstrass	O
theorem	B
(	O
theorem	B
a.9	O
)	O
,	O
there	O
exists	O
a	O
large	O
positive	O
inte	O
(	O
cid:173	O
)	O
ger	O
m	O
,	O
nonzero	O
real	O
coefficients	O
al	O
,	O
.	O
'	O
''	O
am	O
,	O
fh	O
,	O
...	O
,	O
13m	O
,	O
and	O
integers	O
mi	O
,	O
}	O
for	O
i	O
=	O
1	O
,	O
...	O
,	O
m	O
,	O
j	O
=	O
1	O
,	O
...	O
,	O
d	O
,	O
such	O
that	O
sup	O
it	O
(	O
a	O
i	O
cos	O
(	O
~mt	O
x	O
)	O
+	O
f3i	O
sin	O
(	O
~mt	O
x	O
)	O
)	O
-	O
f	O
(	O
x	O
)	O
1	O
<	O
~	O
,	O
xe	O
[	O
a	O
,	O
b	O
]	O
d	O
i=l	O
2	O
a	O
a	O
where	O
mi	O
=	O
(	O
mi	O
,	O
l	O
,	O
...	O
,	O
mi	O
,	O
d	O
)	O
,	O
i	O
=	O
1	O
,	O
...	O
,	O
m.	O
it	O
is	O
clear	O
that	O
every	O
continuous	O
func	O
(	O
cid:173	O
)	O
tion	O
on	O
the	O
real	O
line	O
that	O
is	O
zero	O
outside	O
some	O
bounded	O
interval	O
can	O
be	O
arbitrarily	O
closely	O
approximated	O
uniformly	O
on	O
the	O
interval	O
by	O
one-dimensional	O
neural	O
net	O
(	O
cid:173	O
)	O
works	O
,	O
that	O
is	O
,	O
by	O
functions	O
of	O
the	O
form	O
k	O
l	O
cia	O
(	O
ai	O
x	O
+	O
hi	O
)	O
+	O
co·	O
i==l	O
just	O
observe	O
that	O
the	O
indicator	O
function	O
of	O
an	O
interval	O
[	O
b	O
,	O
c	O
]	O
may	O
be	O
written	O
as	O
a	O
(	O
x	O
-	O
b	O
)	O
+	O
a	O
(	O
-x	O
+	O
c	O
)	O
.	O
this	O
implies	O
that	O
bounded	O
functions	O
such	O
as	O
sin	O
and	O
cos	O
can	O
be	O
approximated	O
arbitrarily	O
closely	O
by	O
neural	O
networks	O
.	O
in	O
particular	O
,	O
there	O
exist	O
neural	O
networks	O
uicx	O
)	O
,	O
vi	O
(	O
x	O
)	O
with	O
i	O
=	O
1	O
,	O
...	O
,	O
m	O
,	O
(	O
i.e.	O
,	O
mappings	O
from	O
nd	O
to	O
n	O
)	O
such	O
that	O
sup	O
xe	O
[	O
a	O
,	O
b	O
]	O
d	O
jui	O
(	O
x	O
)	O
-	O
cos	O
(	O
~mt	O
x	O
)	O
j	O
<	O
__	O
e_	O
4ml	O
a	O
i	O
i	O
a	O
520	O
30.	O
neural	O
networks	O
and	O
sup	O
xe	O
[	O
a	O
,	O
bjd	O
!	O
vi	O
(	O
x	O
)	O
-	O
sin	O
(	O
~mt	O
x	O
)	O
!	O
<	O
__	O
e_	O
.	O
4mi	O
,	O
bi	O
i	O
a	O
therefore	O
,	O
applying	O
the	O
triangle	O
inequality	B
we	O
get	O
since	O
the	O
ui	O
's	O
and	O
vi	O
's	O
are	O
neural	O
networks	O
,	O
their	O
linear	O
combination	O
m	O
1/i	O
(	O
x	O
)	O
=	O
l	O
(	O
aiui	O
(	O
x	O
)	O
+	O
,	O
bivi	O
(	O
x	O
)	O
)	O
i=l	O
is	O
a	O
neural	O
network	O
too	O
and	O
,	O
in	O
fact	O
,	O
sup	O
if	O
(	O
x	O
)	O
-1/i	O
(	O
x	O
)	O
1	O
xe	O
[	O
a	O
,	O
b	O
]	O
d	O
<	O
sup	O
.	O
xe	O
[	O
a	O
,	O
b	O
]	O
d	O
if	O
(	O
x	O
)	O
-	O
t	O
(	O
a	O
i	O
cos	O
(	O
~mt	O
x	O
)	O
+	O
,	O
bi	O
sin	O
(	O
~mt	O
x	O
)	O
)	O
i	O
+	O
sup	O
it	O
(	O
ai	O
cos	O
(	O
~mt	O
x	O
)	O
+	O
,	O
bi	O
sin	O
(	O
~mt	O
x	O
)	O
)	O
-1/i	O
(	O
x	O
)	O
1	O
xe	O
[	O
a	O
,	O
b	O
]	O
d	O
i==l	O
i=l	O
a	O
a	O
a	O
a	O
2e	O
''	O
2	O
=e	O
.	O
0	O
<	O
the	O
convergence	O
may	O
be	O
arbitrarily	O
slow	O
for	O
some	O
f.	O
by	O
restricting	O
the	O
class	O
of	O
functions	O
,	O
it	O
is	O
possible	O
to	O
obtain	O
upper	O
bounds	O
for	O
the	O
rate	B
of	I
convergence	I
.	O
for	O
an	O
example	O
,	O
see	O
barron	O
(	O
1993	O
)	O
.	O
the	O
following	O
corollary	O
of	O
theorem	O
30.4	O
is	O
obtained	O
via	O
lemma	O
30.2	O
:	O
corollary	O
30.1.	O
let	O
c	O
(	O
k	O
)	O
contain	O
all	O
neural	B
network	I
classifiers	I
defined	O
by	O
net	O
(	O
cid:173	O
)	O
works	O
of	O
one	O
hidden	O
layer	O
with	O
k	O
hidden	O
nodes	O
,	O
and	O
an	O
arbitrary	O
sigmoid	B
(	O
j.	O
then	O
for	O
any	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
lim	O
k-+oo	O
cpec	O
(	O
k	O
)	O
inf	O
l	O
(	O
¢	O
)	O
-	O
l	O
*	O
=	O
o.	O
the	O
above	O
convergence	O
also	O
holds	O
if	O
the	O
range	O
of	O
the	O
parameters	O
aij	O
,	O
bi	O
,	O
ci	O
is	O
restricted	O
to	O
an	O
interval	O
[	O
-fh	O
,	O
,	O
bk	O
]	O
,	O
where	O
limk-+oo	O
,	O
bk	O
=	O
00.	O
remark	O
.	O
it	O
is	O
also	O
true	O
that	O
the	O
class	O
of	O
one-hidden-iayer	O
neural	O
networks	O
with	O
k	O
hidden	O
neurons	O
becomes	O
dense	O
in	O
l	O
1	O
(	O
j-l	O
)	O
for	O
every	O
probability	O
measure	B
j-l	O
on	O
nd	O
as	O
k	O
-	O
?	O
-	O
00	O
(	O
see	O
problem	O
30.4	O
)	O
.	O
then	O
theorem	B
2.2	O
may	O
be	O
used	O
directly	O
to	O
prove	O
corollary	O
30.l	O
.	O
0	O
30.4	O
vc	B
dimension	I
521	O
.	O
in	O
practice	O
,	O
the	O
network	O
architecture	O
(	O
i.e.	O
,	O
k	O
in	O
our	O
case	O
)	O
is	O
given	O
to	O
the	O
designer	O
,	O
who	O
can	O
only	O
adjust	O
the	O
parameters	O
aij	O
,	O
bi	O
,	O
and	O
ci	O
,	O
depending	O
on	O
the	O
data	O
dn	O
.	O
in	O
this	O
respect	O
,	O
the	O
above	O
results	O
are	O
only	O
of	O
theoretical	O
interest	O
.	O
it	O
is	O
more	O
interesting	O
to	O
find	O
out	O
how	O
far	O
the	O
error	O
probability	O
of	O
the	O
chosen	O
rule	B
is	O
from	O
inf	O
c/	O
>	O
eck	O
l	O
(	O
<	O
p	O
)	O
.	O
we	O
discuss	O
this	O
problem	O
in	O
the	O
next	O
few	O
sections	O
.	O
30.4	O
vc	B
dimension	I
assume	O
now	O
that	O
the	O
data	O
dn	O
=	O
(	O
(	O
xl	O
,	O
yd	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
»	O
are	O
used	O
to	O
tune	O
the	O
parameters	O
of	O
the	O
network	O
.	O
to	O
choose	O
a	O
classifier	O
from	O
c	O
(	O
k	O
)	O
,	O
we	O
focus	O
on	O
the	O
dif	O
(	O
cid:173	O
)	O
ference	O
between	O
the	O
probability	O
of	O
error	O
of	O
the	O
selected	O
rule	B
and	O
that	O
of	O
the	O
best	O
classifier	B
in	O
c	O
(	O
k	O
)	O
.	O
recall	O
from	O
chapters	O
12	O
and	O
14	O
that	O
the	O
vc	B
dimension	I
vc	O
(	O
k	O
)	O
of	O
the	O
class	O
c	O
(	O
k	O
)	O
determines	O
the	O
performance	O
of	O
some	O
learning	B
algorithms	O
.	O
theorem	B
14.5	O
tells	O
us	O
that	O
no	O
method	O
of	O
picking	O
a	O
classifier	O
from	O
c	O
(	O
k	O
)	O
can	O
guarantee	O
better	O
than	O
q	O
(	O
,	O
jvc	O
(	O
k	O
)	O
/	O
n	O
)	O
performance	O
uniformly	O
for	O
all	O
distributions	O
.	O
thus	O
,	O
for	O
meaningful	O
distribution-free	O
performance	O
guarantees	O
,	O
the	O
sample	O
size	O
n	O
has	O
to	O
be	O
significantly	O
larger	O
than	O
the	O
vc	B
dimension	I
.	O
on	O
the	O
other	O
hand	O
,	O
by	O
corollary	O
12.1	O
,	O
there	O
exists	O
a	O
way	O
of	O
choosing	O
the	O
parameters	O
of	O
the	O
network-namely	O
,	O
by	O
minimization	O
of	O
the	O
empirical	B
error	I
probability-such	O
that	O
the	O
obtained	O
classifier	B
<	O
pi~	O
satisfies	O
e	O
{	O
l	O
(	O
<	O
p~	O
)	O
}	O
-	O
inf	O
l	O
(	O
<	O
p	O
)	O
:	O
:	O
:	O
16	O
c/	O
>	O
ec	O
(	O
k	O
)	O
vc	O
(	O
k	O
)	O
log	O
n	O
+	O
4	O
2n	O
for	O
all	O
distributions	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
vc	O
(	O
k	O
)	O
=	O
00	O
,	O
then	O
for	O
any	O
n	O
and	O
any	O
rule	B
,	O
some	O
bad	O
distributions	O
exist	O
that	O
induce	O
very	O
large	O
error	O
probabilities	O
(	O
see	O
theorem	B
14.3	O
)	O
.	O
we	O
start	O
with	O
a	O
universal	O
lower	O
bound	O
on	O
the	O
vc	B
dimension	I
of	O
networks	O
with	B
one	I
hidden	I
layer	I
.	O
theorem	B
30.5	O
.	O
(	O
baum	O
(	O
1988	O
»	O
.	O
let	O
a	O
be	O
an	O
arbitrary	O
sigmoid	B
and	O
consider	O
the	O
class	O
c	O
(	O
k	O
)	O
of	O
neural	O
net	O
classifiers	O
with	O
k	O
nodes	O
in	O
one	O
hidden	O
layer	O
.	O
then	O
proof	O
.	O
we	O
prove	O
the	O
statement	O
for	O
the	O
threshold	B
sigmoid	O
,	O
and	O
leave	O
the	O
extension	O
as	O
an	O
exercise	O
(	O
problem	O
30.7	O
)	O
.	O
we	O
need	O
to	O
show	O
that	O
there	O
is	O
a	O
set	O
of	O
n	O
=	O
2	O
lk/2jd	O
points	O
in	O
rd	O
that	O
can	O
be	O
shattered	O
by	O
sets	O
of	O
the	O
form	O
{	O
x	O
:	O
ljf	O
(	O
x	O
)	O
>	O
1/2	O
}	O
,	O
where	O
ljf	O
is	O
a	O
one-layer	O
neural	B
network	I
of	O
k	O
hidden	O
nodes	O
.	O
clearly	O
,	O
it	O
suffices	O
to	O
prove	O
this	O
for	O
even	O
k.	O
in	O
fact	O
,	O
we	O
prove	O
more	O
:	O
if	O
k	O
is	O
even	O
,	O
any	O
set	O
of	O
n	O
=	O
kd	O
points	O
in	O
general	O
position	O
can	O
be	O
shattered	O
(	O
points	O
are	O
in	O
general	O
position	O
if	O
no	O
d	O
+	O
1	O
points	O
fall	O
on	O
i-dimensional	O
hyperplane	B
)	O
.	O
let	O
{	O
x	O
1	O
,	O
...	O
,	O
xn	O
}	O
be	O
a	O
set	O
of	O
n	O
=	O
kd	O
such	O
the	O
same	O
d	O
-	O
points	O
.	O
for	O
each	O
subset	O
of	O
this	O
set	O
,	O
we	O
construct	O
a	O
neural	O
network	O
ljf	O
with	O
k	O
hidden	O
nodes	O
such	O
that	O
1jj	O
(	O
xi	O
)	O
>	O
1/2	O
if	O
and	O
only	O
if	O
xi	O
is	O
a	O
member	O
of	O
this	O
subset	O
.	O
we	O
may	O
522	O
30.	O
neural	O
networks	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
the	O
cardinality	O
of	O
the	O
subset	O
to	O
be	O
picked	O
out	O
is	O
at	O
most	O
n/2	O
,	O
since	O
otherwise	O
we	O
can	O
use	O
1/2	O
-	O
vr	O
(	O
x	O
)	O
,	O
where	O
vr	O
picks	O
out	O
the	O
complement	O
of	O
the	O
subset	O
.	O
partition	B
the	O
subset	O
into	O
at	O
most	O
n	O
/	O
(	O
2d	O
)	O
=	O
k	O
/2	O
groups	O
,	O
each	O
containing	O
at	O
most	O
d	O
points	O
.	O
for	O
each	O
such	O
group	O
,	O
there	O
exists	O
a	O
hyperplane	O
a	O
t	O
x	O
+	O
b	O
=	O
0	O
that	O
contains	O
these	O
points	O
,	O
but	O
no	O
other	O
point	O
from	O
{	O
xl	O
,	O
...	O
,	O
xn	O
}	O
.	O
moreover	O
,	O
there	O
exists	O
a	O
small	O
positive	O
number	O
h	O
such	O
that	O
at	O
xi	O
+	O
b	O
e	O
[	O
-h	O
,	O
h	O
]	O
if	O
and	O
only	O
if	O
xi	O
is	O
among	O
this	O
group	O
of	O
at	O
most	O
d	O
points	O
.	O
therefore	O
,	O
the	O
simple	O
network	O
a	O
(	O
at	O
x	O
+	O
b	O
+	O
h	O
)	O
+	O
a	O
(	O
-a	O
t	O
x	O
-	O
b	O
+	O
h	O
)	O
is	O
larger	O
than	O
0	O
on	O
xi	O
for	O
exactly	O
these	O
xi	O
's	O
.	O
denote	O
the	O
vectors	O
a	O
,	O
and	O
parameters	O
b	O
,	O
h	O
obtained	O
for	O
the	O
k/2	O
groups	O
by	O
ai	O
,	O
...	O
,	O
ak/2	O
,	O
bt	O
,	O
...	O
,	O
bk/2	O
,	O
and	O
hi	O
,	O
.	O
'	O
''	O
hk/2	O
.	O
let	O
h	O
=	O
min	O
j	O
-s.k/2	O
h	O
j.	O
it	O
is	O
easy	O
to	O
see	O
that	O
kj2	O
vr	O
(	O
x	O
)	O
=	O
l	O
(	O
a	O
(	O
ajx	O
+bj	O
+h	O
)	O
+a	O
(	O
-ajx	O
-	O
b	O
j	O
+h	O
)	O
)	O
+-	O
2	O
j==l	O
1	O
is	O
larger	O
than	O
1/2	O
for	O
exactly	O
the	O
desired	O
xi	O
's	O
.	O
this	O
network	O
has	O
k	O
hidden	O
nodes	O
.	O
0	O
theorem	B
30.5	O
implies	O
that	O
there	O
is	O
no	O
hope	O
for	O
good	O
performance	O
guarantees	O
unless	O
the	O
sample	O
size	O
is	O
much	O
larger	O
than	O
kd	O
.	O
recall	O
chapter	O
14	O
,	O
where	O
we	O
showed	O
that	O
n	O
»	O
vc	O
(	O
k	O
)	O
is	O
necessary	O
for	O
a	O
guaranteed	O
small	O
eltor	O
probability	O
,	O
regardless	O
of	O
the	O
method	O
of	O
tuning	O
the	O
parameters	O
.	O
bartlett	O
(	O
1993	O
)	O
improved	O
theorem	B
30.5	O
in	O
several	O
ways	O
.	O
for	O
example	O
,	O
he	O
proved	O
that	O
vc	O
(	O
k	O
)	O
>	O
d	O
min	O
(	O
k	O
,	O
-	O
2d	O
2	O
d	O
/2	O
+	O
d	O
+	O
1	O
)	O
+	O
1.	O
bartlett	O
also	O
obtained	O
similar	O
lower	B
bounds	I
for	I
not	O
fully	O
connected	O
networks-see	O
problem	O
30.9-and	O
for	O
two-hidden-layer	O
networks	O
.	O
next	O
we	O
show	O
that	O
for	O
the	O
threshold	B
sigmoid	O
,	O
the	O
bound	O
of	O
theorem	O
30.5	O
is	O
tight	O
up	O
to	O
a	O
logarithmic	O
factor	O
,	O
that	O
is	O
,	O
the	O
vc	B
dimension	I
is	O
at	O
most	O
of	O
the	O
order	O
of	O
kdlogk	O
.	O
theorem	B
30.6	O
.	O
(	O
baum	O
and	O
haussler	O
(	O
1989	O
)	O
)	O
.	O
let	O
(	O
5	O
be	O
the	O
threshold	B
sigmoid	O
and	O
let	O
c	O
(	O
k	O
)	O
be	O
the	O
class	O
of	O
neural	O
net	O
classifiers	O
with	O
k	O
nodes	O
in	O
the	O
hidden	O
layer	O
.	O
then	O
the	O
shatter	O
coefficients	O
satisfy	O
<	O
(	O
~	O
)	O
k	O
(	O
d+l	O
)	O
(	O
~	O
)	O
k+l	O
<	O
ne	O
kd+2k+l	O
,	O
-	O
(	O
)	O
k+l	O
d+l	O
which	O
implies	O
that	O
for	O
all	O
k	O
,	O
d	O
:	O
:	O
:	O
:	O
1	O
,	O
vc	O
(	O
k	O
)	O
:	O
:	O
:	O
(	O
2kd	O
+	O
4k	O
+	O
2	O
)	O
10g2	O
(	O
e	O
(	O
kd	O
+	O
2k	O
+	O
1	O
)	O
)	O
.	O
30.4	O
vc	B
dimension	I
523.	O
proof	O
.	O
fix	O
n	O
points	O
xl	O
,	O
...	O
,	O
xn	O
e	O
nd	O
.	O
we	O
bound	O
the	O
number	O
of	O
different	O
values	O
of	O
the	O
vector	O
(	O
¢	O
(	O
xi	O
)	O
,	O
...	O
,	O
¢	O
(	O
xn	O
»	O
as	O
¢	O
ranges	O
through	O
c	O
(	O
k	O
)	O
.	O
a	O
node	O
j	O
in	O
the	O
hidden	O
layer	O
realizes	O
a	O
dichotomy	O
of	O
the	O
n	O
points	O
by	O
a	O
hyperplane	O
split	O
.	O
by	O
theorems	O
13.9	O
and	O
13.3	O
,	O
this	O
can	O
be	O
done	O
at	O
mostef	O
:	O
j	O
(	O
7	O
)	O
:	O
:	O
:	O
:	O
(	O
ne/	O
(	O
d	O
+	O
1	O
»	O
d+1	O
different	O
ways	O
.	O
the	O
different	O
splittings	O
obtained	O
at	O
the	O
k	O
nodes	O
determine	O
the	O
k-dimensional	O
input	O
of	O
the	O
output	O
node	O
.	O
different	O
choices	O
of	O
the	O
parameters	O
co	O
,	O
ci	O
,	O
...	O
,	O
ck	O
of	O
the	O
output	O
node	O
determine	O
different	O
k-dimensionallinear	O
splits	O
of	O
the	O
n	O
input	O
vectors	O
.	O
this	O
can	O
not	O
be	O
done	O
in	O
more	O
than	O
l~	O
:	O
j	O
(	O
7	O
)	O
:	O
:	O
:	O
:	O
(	O
ne/	O
(	O
k	O
+	O
1	O
»	O
k+1	O
different	O
ways	O
for	O
a	O
fixed	O
setting	O
of	O
the	O
au	O
and	O
bi	O
parameters	O
.	O
this	O
altogether	O
yields	O
at	O
most	O
different	O
dichotomies	O
of	O
the	O
n	O
points	O
x	O
i	O
,	O
...	O
,	O
x	O
n	O
,	O
as	O
desired	O
.	O
the	O
bound	O
on	O
the	O
vc	B
dimension	I
follows	O
from	O
the	O
fact	O
that	O
vc	O
(	O
k	O
)	O
:	O
:	O
:	O
:	O
n	O
if	O
s	O
(	O
c	O
(	O
k	O
)	O
,	O
n	O
)	O
2	O
:	O
2n	O
.	O
0	O
for	O
threshold	B
sigmoids	O
,	O
the	O
gap	O
between	O
the	O
lower	O
and	O
upper	O
bounds	O
above	O
is	O
logarithmic	O
in	O
kd	O
.	O
notice	O
that	O
the	O
vc	B
dimension	I
is	O
about	O
the	O
number	O
of	O
weights	O
(	O
or	O
tunable	O
parameters	O
)	O
w	O
=	O
kd	O
+	O
2k	O
+	O
1	O
of	O
the	O
network	O
.	O
surprisingly	O
,	O
maass	O
(	O
1994	O
)	O
proved	O
that	O
for	O
networks	O
with	O
at	O
least	O
two	O
hidden	O
layers	O
,	O
the	O
upper	O
bound	O
has	O
the	O
right	O
order	O
of	O
magnitude	O
,	O
that	O
is	O
,	O
the	O
vc	B
dimension	I
is	O
q	O
(	O
w	O
log	O
w	O
)	O
.	O
a	O
simple	O
application	O
of	O
theorems	O
30.4	O
and	O
30.6	O
provides	O
the	O
next	O
consistency	B
result	O
that	O
was	O
pointed	O
out	O
in	O
farago	O
and	O
lugosi	O
(	O
1993	O
)	O
:	O
theorem	B
30.7.	O
let	O
(	O
j	O
be	O
the	O
threshold	B
sigmoid	O
.	O
let	O
gn	O
be	O
a	O
classifier	O
from	O
c	O
(	O
k	O
)	O
that	O
minimizes	O
the	O
empirical	B
error	I
over	O
¢	O
e	O
c	O
(	O
k	O
)	O
.	O
if	O
k	O
--	O
-+	O
00	O
such	O
that	O
k	O
log	O
n/	O
n	O
--	O
-+	O
0	O
as	O
n	O
--	O
-+	O
00	O
,	O
then	O
gn	O
is	O
strongly	O
universally	O
consistent	O
,	O
that	O
is	O
,	O
with	O
probability	O
one	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
.	O
lim	O
l	O
(	O
gn	O
)	O
=	O
l	O
*	O
n-+oo	O
proof	O
.	O
by	O
the	O
usual	O
decomposition	O
into	O
approximation	O
and	O
estimation	B
errors	O
,	O
l	O
(	O
gn	O
)	O
-	O
l*	O
=	O
(	O
l	O
(	O
gn	O
)	O
-	O
inf	O
l	O
(	O
¢	O
)	O
)	O
+	O
(	O
inf	O
l	O
(	O
¢	O
)	O
-	O
l*	O
)	O
.	O
fjjec	O
(	O
k	O
)	O
fjjec	O
(	O
k	O
)	O
the	O
second	O
term	O
on	O
the	O
right-hand	O
side	O
tends	O
to	O
zero	O
by	O
corollary	O
30.1.	O
for	O
the	O
estimation	B
error	I
,	O
by	O
theorems	O
12.6	O
and	O
30.6	O
,	O
p	O
{	O
l	O
(	O
gn	O
)	O
-	O
inf	O
l	O
(	O
¢	O
)	O
>	O
e	O
}	O
fjjec	O
(	O
k	O
)	O
<	O
8s	O
(	O
c	O
(	O
k	O
)	O
,	O
n	O
)	O
e-ne2/128	O
524	O
30.	O
neural	O
networks	O
which	O
is	O
summable	O
if	O
k	O
=	O
o	O
(	O
nj	O
log	O
n	O
)	O
.	O
0	O
the	O
theorem	B
assures	O
us	O
that	O
if	O
a	O
is	O
the	O
threshold	B
sigmoid	O
,	O
then	O
a	O
sequence	O
of	O
properly	O
sized	O
networks	O
may	O
be	O
trained	O
to	O
asymptotically	O
achieve	O
the	O
optimum	O
probability	O
of	O
error	O
,	O
regardless	O
of	O
what	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
is	O
.	O
for	O
example	O
,	O
k	O
~	O
fn	O
will	O
do	O
the	O
job	O
.	O
however	O
,	O
this	O
is	O
clearly	O
not	O
the	O
optimal	O
choice	O
in	O
the	O
majority	O
of	O
cases	O
.	O
since	O
theorem	B
30.6	O
provides	O
suitable	O
upper	O
bounds	O
on	O
the	O
vc	B
dimension	I
of	O
each	O
class	O
c	O
(	O
k	O
)	O
,	O
one	O
may	O
use	O
complexity	B
regularization	I
as	O
described	O
in	O
chapter	O
18	O
to	O
find	O
a	O
near-optimum	O
size	O
network	O
.	O
unfortunately	O
,	O
the	O
situation	O
is	O
much	O
less	O
clear	O
for	O
more	O
general	O
,	O
continuous	O
sigmoids	O
.	O
the	O
vc	B
dimension	I
then	O
depends	O
on	O
the	O
specific	O
sigmoid	B
.	O
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
the	O
vc	B
dimension	I
of	O
c	O
(	O
k	O
)	O
with	O
an	O
arbitrary	O
nondecreasing	O
sigmoid	B
is	O
always	O
larger	O
than	O
or	O
equal	O
to	O
that	O
with	O
the	O
threshold	B
sigmoid	O
(	O
problem	O
30.8	O
)	O
.	O
typically	O
,	O
the	O
vc	B
dimension	I
of	O
a	O
class	O
of	O
such	O
networks	O
is	O
significantly	O
larger	O
than	O
that	O
for	O
the	O
threshold	B
sigmoid	O
.	O
in	O
fact	O
,	O
it	O
can	O
even	O
be	O
infinite	O
!	O
macintyre	O
and	O
sontag	O
(	O
1993	O
)	O
demonstrated	O
the	O
existence	O
of	O
continuous	O
,	O
infinitely	O
many	O
times	O
differentiable	O
monotone	O
increasing	O
sigmoids	O
such	O
that	O
the	O
vc	B
dimension	I
of	O
c	O
(	O
k	O
)	O
is	O
infinite	O
if	O
k	O
:	O
:	O
:	O
:	O
2.	O
their	O
sigmoids	O
have	O
little	O
squiggles	O
,	O
creating	O
the	O
large	O
variability	O
.	O
it	O
is	O
even	O
more	O
surprising	O
that	O
infinite	O
vc	B
dimension	I
may	O
occur	O
for	O
even	O
smoother	O
sigmoids	O
,	O
whose	O
second	O
derivative	O
is	O
negative	O
for	O
x	O
>	O
0	O
and	O
positive	O
for	O
x	O
<	O
o.	O
in	O
chapter	O
25	O
(	O
see	O
problem	O
25.11	O
)	O
we	O
basically	O
proved	O
the	O
following	O
result	O
.	O
the	O
details	O
are	O
left	O
to	O
the	O
reader	O
(	O
problem	O
30.13	O
)	O
.	O
theorem	B
30.8.	O
there	O
exists	O
a	O
sigmoid	O
a	O
that	O
is	O
monotone	O
increasing	O
,	O
continuous	O
,	O
concave	O
on	O
(	O
0	O
,	O
(	O
0	O
)	O
,	O
and	O
convex	O
on	O
(	O
-00,0	O
)	O
,	O
such	O
that	O
vc	O
(	O
k	O
)	O
=	O
00	O
for	O
each	O
k	O
:	O
:	O
:	O
:	O
8.	O
we	O
recall	O
once	O
again	O
that	O
infinite	O
vc	B
dimension	I
implies	O
that	O
there	O
is	O
no	O
hope	O
of	O
obtaining	O
nontrivial	O
distribution-free	O
upper	O
bounds	O
on	O
no	O
matter	O
how	O
the	O
training	O
sequence	O
dn	O
is	O
used	O
to	O
select	O
the	O
parameters	O
of	O
the	O
neural	B
network	I
.	O
however	O
,	O
as	O
we	O
will	O
see	O
later	O
,	O
it	O
is	O
still	O
possible	O
to	O
obtain	O
universal	B
consistency	I
.	O
finiteness	O
of	O
the	O
vc	B
dimension	I
has	O
been	O
proved	O
for	O
many	O
types	O
of	O
sigmoids	O
.	O
maass	O
(	O
1993	O
)	O
and	O
goldberg	O
and	O
jerrum	O
(	O
1993	O
)	O
obtain	O
upper	O
bounds	O
for	O
piecewise	B
polynomial	I
sigmoids	O
.	O
the	O
results	O
of	O
goldberg	O
and	O
jerrum	O
(	O
1993	O
)	O
apply	O
for	O
general	O
classes	O
parametrized	O
by	O
real	O
numbers	O
,	O
e.g.	O
,	O
for	O
classes	O
of	O
neural	O
networks	O
with	O
the	O
sigmoid	B
a	O
(	O
x	O
)	O
=	O
{	O
1	O
-	O
ij	O
(	O
2x	O
+	O
2	O
)	O
ij	O
(	O
2	O
-	O
2x	O
)	O
if	O
x	O
:	O
:	O
:	O
:	O
0	O
if	O
x	O
<	O
o.	O
macintyre	O
and	O
sontag	O
(	O
1993	O
)	O
prove	O
vc	O
(	O
k	O
)	O
<	O
00	O
for	O
a	O
large	O
class	O
of	O
sigmoids	O
,	O
which	O
includes	O
the	O
standard	B
,	O
arctan	B
,	O
and	O
gaussian	B
sigmoids	O
.	O
while	O
finiteness	O
is	O
useful	O
,	O
the	O
30.4	O
vc	B
dimension	I
525	O
lack	O
of	O
an	O
explicit	O
tight	O
upper	O
bound	O
on	O
vc	O
(	O
k	O
)	O
prevents	O
us	O
from	O
getting	O
meaningful	O
upper	O
bounds	O
on	O
the	O
performance	O
of	O
gn	O
,	O
and	O
also	O
from	O
applying	O
the	O
structural	B
risk	I
minimization	I
of	O
chapter	O
18.	O
for	O
the	O
standard	B
sigmoid	O
,	O
and	O
for	O
networks	O
with	O
k	O
hidden	O
nodes	O
and	O
w	O
tunable	O
weights	O
karpinski	O
and	O
macintyre	O
(	O
1994	O
)	O
recently	O
reported	O
the	O
upper	O
bound	O
v	O
:	O
:	O
:	O
:	O
kw	O
(	O
kw	O
-	O
1	O
)	O
2	O
+	O
w	O
(	O
1	O
+	O
2k	O
)	O
+	O
w	O
(	O
l	O
+	O
3k	O
)	O
10g	O
(	O
3w	O
+	O
6kw	O
+	O
3	O
)	O
.	O
see	O
also	O
shawe-taylor	O
(	O
1994	O
)	O
.	O
unfortunately	O
,	O
the	O
consistency	B
result	O
of	O
theorem	O
30.7	O
is	O
only	O
of	O
theoretical	O
interest	O
,	O
as	O
there	O
is	O
no	O
efficient	O
algorithm	B
to	O
find	O
a	O
classifier	O
that	O
minimizes	O
the	O
empirical	B
error	I
probability	O
.	O
relatively	O
little	O
effort	O
has	O
been	O
made	O
to	O
solve	O
this	O
important	O
problem	O
.	O
farago	O
and	O
lugosi	O
(	O
1993	O
)	O
exhibit	O
an	O
algorithm	B
that	O
finds	O
the	O
empirically	O
optimal	O
network	O
.	O
however	O
,	O
their	O
method	O
takes	O
time	O
exponential	B
in	O
kd	O
,	O
which	O
is	O
intractable	O
even	O
for	O
the	O
smallest	O
toy	O
problems	O
.	O
much	O
more	O
effort	O
has	O
been	O
invested	O
in	O
the	O
tuning	O
of	O
networks	O
by	O
minimizing	O
the	O
empirical	O
squared	O
error	O
,	O
or	O
the	O
empirical	B
l	O
1	O
error	O
.	O
these	O
problems	O
are	O
also	O
computationally	O
demanding	O
,	O
but	O
numerous	O
suboptimal	O
hill-climbing	O
algorithms	O
have	O
been	O
used	O
with	O
some	O
success	O
.	O
most	O
famous	O
among	O
these	O
is	O
the	O
back	B
propagation	I
algorithm	O
of	O
rumelhart	O
,	O
hinton	O
,	O
and	O
williams	O
(	O
1986	O
)	O
.	O
nearly	O
all	O
known	O
algorithms	O
that	O
run	O
in	O
reasonable	O
time	O
may	O
get	O
stuck	O
at	O
local	O
optima	O
,	O
which	O
results	O
in	O
classifiers	O
whose	O
probability	O
of	O
error	O
is	O
hard	O
to	O
predict	O
.	O
in	O
the	O
next	O
section	O
we	O
study	O
the	O
error	O
probability	O
of	O
neural	O
net	O
classifiers	O
obtained	O
by	O
minimizing	O
empirical	B
l	O
p	O
errors	O
.	O
we	O
end	O
this	O
section	O
with	O
a	O
very	O
simple	O
kind	O
of	O
one-layer	O
network	O
.	O
the	O
committee	B
machine	I
(	O
see	O
,	O
e.g.	O
,	O
nilsson	O
(	O
1965	O
)	O
and	O
schmidt	O
(	O
1994	O
)	O
)	O
is	O
a	O
special	O
case	O
of	O
a	O
one	O
(	O
cid:173	O
)	O
hidden-layer	O
neural	B
network	I
of	O
the	O
form	O
(	O
30.2	O
)	O
with	O
co	O
=	O
0	O
,	O
cl	O
=	O
...	O
=	O
ck	O
=	O
1	O
,	O
and	O
the	O
threshold	B
sigmoid	O
.	O
o	O
or	O
1	O
figure	O
30.11.	O
the	O
committee	B
machine	I
has	O
fixed	O
weights	O
at	O
the	O
output	O
of	O
the	O
hidden	O
layer	O
.	O
committee	O
machines	O
thus	O
use	O
a	O
majority	O
vote	O
over	O
the	O
outcomes	O
of	O
the	O
hidden	O
neurons	O
.	O
it	O
is	O
interesting	O
that	O
the	O
lower	O
bound	O
of	O
theorem	O
30.5	O
remains	O
valid	O
when	O
c	O
(	O
k	O
)	O
is	O
the	O
class	O
of	O
all	O
committee	O
machines	O
with	O
k	O
neurons	O
in	O
the	O
hidden	O
526	O
30.	O
neural	O
``	O
networks	O
layer	O
.	O
it	O
is	O
less	O
obvious	O
,	O
however	O
,	O
that	O
the	O
class	O
of	O
committee	O
machines	O
is	O
large	O
enough	O
for	O
the	O
asymptotic	O
property	O
lim	O
k-hx	O
)	O
<	O
/jec	O
(	O
k	O
)	O
inf	O
l	O
(	O
rp	O
)	O
=	O
l	O
*	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
(	O
see	O
problem	O
30.6	O
)	O
.	O
figure	O
30.12.	O
a	O
partition	O
of	O
the	O
plane	O
determined	O
by	O
a	O
committee	O
machine	O
with	O
5	O
hidden	O
neurons	O
.	O
the	O
total	O
vote	O
is	O
shown	O
in	O
each	O
region	O
.	O
the	O
region	O
in	O
which	O
we	O
decide	O
``	O
0	O
''	O
is	O
shaded	O
.	O
30.5	O
ll	O
error	O
minimization	O
in	O
the	O
previous	O
section	O
,	O
we	O
obtained	O
consistency	B
for	O
the	O
standard	B
threshold	O
sigmoid	B
networks	O
by	O
empirical	B
risk	I
minimization	I
.	O
we	O
could	O
not	O
apply	O
the	O
same	O
methodol	O
(	O
cid:173	O
)	O
ogy	O
for	O
general	O
sigmoids	O
simply	O
because	O
the	O
vc	B
dimension	I
for	O
general	O
sigmoidal	O
networks	O
is	O
not	O
bounded	O
.	O
it	O
is	O
bounded	O
for	O
certain	O
classes	O
of	O
sigmoids	O
,	O
and	O
for	O
those	O
,	O
empirical	B
risk	I
minimization	I
yields	O
universally	O
consistent	O
classifiers	O
.	O
even	O
if	O
the	O
vc	B
dimension	I
is	O
infinite	O
,	O
we	O
may	O
get	O
consistency	B
,	O
but	O
this	O
must	O
then	O
be	O
proved	O
by	O
other	O
methods	O
,	O
such	O
as	O
methods	O
based	O
upon	O
metric	B
entropy	I
and	O
covering	B
num	O
(	O
cid:173	O
)	O
bers	O
(	O
see	O
chapters	O
28	O
and	O
29	O
,	O
as	O
well	O
as	O
the	O
survey	O
by	O
haussler	O
(	O
1992	O
)	O
)	O
.	O
one	O
could	O
also	O
train	O
the	O
classifier	B
by	O
minimizing	O
another	O
empirical	B
criterion	O
,	O
which	O
is	O
exactly	O
what	O
we	O
will	O
do	O
in	O
this	O
section	O
.	O
we	O
will	O
be	O
rewarded	O
with	O
a	O
general	O
consistency	B
theorem	O
for	O
all	O
sigmoids	O
.	O
for	O
1	O
:	O
:	O
;	O
p	O
<	O
00	O
,	O
the	O
empirical	B
l	O
p	O
error	O
of	O
a	O
neural	O
network	O
l/f	O
is	O
defined	O
by	O
the	O
most	O
interesting	O
cases	O
are	O
p	O
=	O
1	O
and	O
p	O
=	O
2.	O
for	O
p	O
=	O
2	O
this	O
is	O
just	O
the	O
empirical	O
squared	O
error	O
,	O
while	O
p	O
=	O
1	O
yields	O
the	O
empirical	B
absolute	O
error	O
.	O
often	O
it	O
makes	O
sense	O
to	O
attempt	O
to	O
choose	O
the	O
parameters	O
of	O
the	O
network	O
l/f	O
such	O
that	O
j~p\	O
l/f	O
)	O
is	O
30.5	O
ll	O
error	O
minimization	O
527	O
minimized	O
.	O
in	O
situations	O
where	O
one	O
is	O
not	O
only	O
interested	O
in	O
the	O
number	O
of	O
errors	O
,	O
but	O
also	O
how	O
robust	O
the	O
decision	O
is	O
,	O
such	O
error	O
measures	O
may	O
be	O
meaningful	O
.	O
in	O
other	O
words	O
,	O
these	O
error	O
measures	O
penalize	O
even	O
good	O
decisions	O
if	O
'ljf	O
is	O
close	O
to	O
the	O
threshold	B
value	O
o.	O
minimizing	O
j	O
}	O
~p	O
)	O
is	O
like	O
finding	O
a	O
good	O
regression	B
function	I
esti	O
(	O
cid:173	O
)	O
mate	O
.	O
our	O
concern	O
is	O
primarily	O
with	O
the	O
error	O
probability	O
.	O
in	O
chapter	O
4	O
we	O
already	O
highlighted	O
the	O
dangers	O
of	O
squared	O
error	O
minimization	O
and	O
l	O
p	O
errors	O
in	O
general	O
.	O
here	O
we	O
will	O
concentrate	O
on	O
the	O
consistency	B
properties	O
.	O
we	O
minimize	O
the	O
empirical	B
error	I
over	O
a	O
class	O
of	O
functions	O
,	O
which	O
should	O
not	O
be	O
too	O
large	O
to	O
avoid	O
overfitting	B
.	O
however	O
,	O
the	O
class	O
should	O
be	O
large	O
enough	O
to	O
contain	O
a	O
good	O
approximation	O
of	O
the	O
target	O
function	O
.	O
thus	O
,	O
we	O
let	O
the	O
class	O
of	O
candidate	O
functions	O
grow	O
with	O
the	O
sample	O
size	O
n	O
,	O
as	O
in	O
grenander	O
's	O
``	O
method	O
of	O
sieves	O
''	O
(	O
grenander	O
(	O
1981	O
)	O
)	O
.	O
its	O
consistency	B
and	O
rates	O
of	O
convergence	O
have	O
been	O
widely	O
studied	O
primarily	O
for	O
least	O
squares	O
regression	B
function	I
estimation	O
and	O
nonparametric	O
maximum	B
likelihood	I
density	O
estimation-see	O
geman	O
and	O
hwang	O
(	O
1982	O
)	O
,	O
gallant	O
(	O
1987	O
)	O
,	O
and	O
wong	O
and	O
shen	O
(	O
1992	O
)	O
.	O
remark	O
.	O
regression	B
function	I
estimation	O
.	O
in	O
the	O
regression	B
function	I
estima	O
(	O
cid:173	O
)	O
tion	O
setup	O
,	O
white	O
(	O
1990	O
)	O
proved	O
consistency	B
of	O
neural	B
network	I
estimates	O
based	O
on	O
squared	O
error	O
minimization	O
.	O
barron	O
(	O
1991	O
;	O
1994	O
)	O
used	O
a	O
complexity-regularized	O
modification	O
of	O
these	O
error	O
measures	O
to	O
obtain	O
the	O
fastest	O
possible	O
rate	O
of	O
con	O
(	O
cid:173	O
)	O
vergence	O
for	O
nonparametric	O
neural	B
network	I
estimates	O
.	O
haussler	O
(	O
1992	O
)	O
provides	O
a	O
general	O
framework	O
for	O
empirical	B
error	I
minimization	O
,	O
and	O
provides	O
useful	O
tools	O
for	O
handling	O
neural	O
networks	O
.	O
various	O
consistency	B
properties	O
of	O
nonparametric	O
neu	O
(	O
cid:173	O
)	O
ral	O
network	O
estimates	O
have	O
been	O
proved	O
by	O
white	O
(	O
1991	O
)	O
,	O
mielniczuk	O
and	O
tyrcha	O
(	O
1993	O
)	O
,	O
and	O
lugosi	O
and	O
zeger	O
(	O
1995	O
)	O
.	O
0	O
we	O
only	O
consider	O
the	O
p	O
=	O
1	O
case	O
,	O
as	O
the	O
generalization	O
to	O
other	O
values	O
of	O
p	O
is	O
straightforward	O
.	O
define	O
the	O
ll	O
error	O
of	O
a	O
function	O
'ljf	O
:	O
rd	O
-+	O
r	O
by	O
j	O
(	O
'ljf	O
)	O
=	O
e	O
{	O
i'ljf	O
(	O
x	O
)	O
-	O
yi	O
}	O
.	O
we	O
pointed	O
out	O
in	O
problem	O
2.12	O
that	O
one	O
of	O
the	O
functions	O
minimizing	O
j	O
(	O
'ljf	O
)	O
is	O
the	O
bayes	O
rule	B
g*	O
whose	O
error	O
is	O
denoted	O
by	O
then	O
clearly	O
,	O
j*	O
=	O
l	O
*	O
.	O
we	O
have	O
also	O
seen	O
that	O
if	O
we	O
define	O
a	O
decision	O
by	O
j*	O
=	O
inf	O
j	O
(	O
'ljf	O
)	O
=	O
j	O
(	O
g*	O
)	O
.	O
1jj	O
g	O
(	O
x	O
)	O
=	O
{	O
if	O
'ljf	O
(	O
x	O
)	O
:	O
:	O
:	O
:	O
1/2	O
0	O
1	O
otherwise	O
,	O
then	O
its	O
error	O
probability	O
l	O
(	O
g	O
)	O
=	O
p	O
{	O
g	O
(	O
x	O
)	O
=i	O
y	O
}	O
satisfies	O
the	O
inequality	B
l	O
(	O
g	O
)	O
-	O
l*	O
:	O
:	O
:	O
:	O
j	O
(	O
'ljf	O
)	O
-	O
j*	O
.	O
our	O
approach	O
is	O
to	O
select	O
a	O
neural	O
network	O
from	O
a	O
suitably	O
chosen	O
class	O
of	O
networks	O
by	O
minimizing	O
the	O
empirical	B
error	I
528	O
30.	O
neural	O
networks	O
denoting	O
this	O
function	O
by	O
o/n	O
,	O
according	O
to	O
the	O
inequality	B
above	O
,	O
the	O
classifier	B
gn	O
(	O
x	O
)	O
=	O
{	O
0	O
if	O
o/n	O
(	O
x	O
)	O
:	O
:	O
:	O
1/2	O
h	O
·	O
ot	O
erwlse	O
,	O
is	O
consistent	O
if	O
the	O
lienor	O
converges	O
to	O
j*	O
in	O
probability	O
.	O
convergence	O
with	O
probability	O
one	O
provides	O
strong	B
consistency	I
.	O
for	O
universal	B
convergence	O
,	O
the	O
class	O
over	O
which	O
the	O
minimization	O
is	O
performed	O
has	O
to	O
be	O
defined	O
carefully	O
.	O
the	O
following	O
theorem	B
shows	O
that	O
this	O
may	O
be	O
achieved	O
by	O
neural	O
networks	O
with	O
k	O
nodes	O
,	O
in	O
which	O
the	O
range	O
of	O
the	O
output	O
weights	O
co	O
,	O
ci	O
,	O
...	O
,	O
ck	O
is	O
restricted	O
.	O
theorem	B
30.9	O
.	O
(	O
lugosi	O
and	O
zeger	O
(	O
1995	O
)	O
)	O
.	O
let	O
(	O
j	O
be	O
an	O
arbitrary	O
sigmoid	B
.	O
define	O
the	O
class	O
:01	O
of	O
neural	O
networks	O
by	O
and	O
let	O
o/n	O
be	O
afunction	O
that	O
minimizes	O
the	O
empirical	B
ll	O
error	O
over	O
0/	O
e	O
:01·	O
if	O
kn	O
and	O
f3n	O
satisfy	O
lim	O
k	O
n	O
=	O
00	O
,	O
n	O
--	O
-+oo	O
lim	O
f3n	O
=	O
00	O
,	O
and	O
n	O
--	O
-+oo	O
.	O
lull	O
n	O
--	O
-+oo	O
knf3~	O
log	O
(	O
kn	O
f3n	O
)	O
n	O
=	O
0	O
,	O
then	O
the	O
classification	O
rule	B
gn	O
(	O
x	O
)	O
=	O
{	O
if	O
o/n	O
(	O
x	O
)	O
:	O
:	O
:	O
1/2	O
0	O
1	O
otherwise	O
is	O
universally	O
consistent	O
.	O
remark	O
.	O
strong	B
universal	I
consistency	I
may	O
also	O
be	O
shown	O
by	O
imposing	O
slightly	O
more	O
restrictive	O
conditions	O
on	O
k	O
n	O
and	O
f3n	O
(	O
see	O
problem	O
30.16	O
)	O
.0	O
proof	O
.	O
by	O
the	O
argument	O
preceding	O
the	O
theorem	B
,	O
it	O
suffices	O
to	O
prove	O
that	O
j	O
(	O
't/ln	O
)	O
-	O
j*	O
-+	O
0	O
in	O
probability	O
.	O
write	O
j	O
(	O
ljin	O
)	O
-	O
j*	O
=	O
(	O
j	O
(	O
ljin	O
)	O
-	O
inf	O
j	O
(	O
lji	O
)	O
)	O
+	O
(	O
inf	O
j	O
(	O
o/	O
)	O
-	O
j*	O
)	O
.	O
1jje	O
:	O
f	O
''	O
1jje	O
:	O
f	O
''	O
30.5	O
lj	O
error	O
minimization	O
529	O
to	O
handle	O
the	O
approximation	O
error-the	O
second	O
term	O
on	O
the	O
right-hand	O
side-let	O
t	O
'	O
e	O
:01	O
be	O
a	O
function	O
such	O
that	O
e	O
{	O
lt	O
'	O
(	O
x	O
)	O
-	O
g*	O
(	O
x	O
)	O
1	O
}	O
:	O
:	O
;	O
e	O
{	O
lt	O
(	O
x	O
)	O
-	O
g*	O
(	O
x	O
)	O
1	O
}	O
for	O
each	O
t	O
e	O
:07.	O
the	O
existence	O
of	O
such	O
a	O
function	O
may	O
be	O
seen	O
by	O
noting	O
that	O
e	O
{	O
lt	O
(	O
x	O
)	O
-	O
g*	O
(	O
x	O
)	O
1	O
}	O
is	O
a	O
continuous	O
function	O
of	O
the	O
parameters	O
ai	O
,	O
hi	O
,	O
ci	O
of	O
the	O
neural	B
network	I
t.	O
clearly	O
,	O
inf	O
j	O
(	O
t	O
)	O
-	O
j*	O
<	O
j	O
(	O
t	O
'	O
)	O
-	O
j*	O
1jjefi1	O
=	O
e	O
{	O
lt	O
'	O
(	O
x	O
)	O
-	O
yi	O
}	O
-	O
e	O
{	O
lg*	O
(	O
x	O
)	O
-	O
yi	O
}	O
<	O
e	O
{	O
lt	O
'	O
(	O
x	O
)	O
-	O
g*	O
(	O
x	O
)	O
i	O
}	O
'	O
which	O
converges	O
to	O
zero	O
as	O
n	O
-+	O
00	O
,	O
by	O
problem	O
30.4.	O
we	O
start	O
the	O
analysis	O
of	O
the	O
estimation	B
error	I
by	O
noting	O
that	O
as	O
in	O
lemma	O
8.2	O
,	O
we	O
have	O
j	O
(	O
tn	O
)	O
-	O
inf	O
j	O
(	O
t	O
)	O
1jje	O
:	O
f	O
;	O
l	O
:	O
:	O
;	O
2	O
sup	O
ij	O
(	O
t	O
)	O
-	O
1jjef	O
''	O
in	O
(	O
t	O
)	O
1	O
=	O
2	O
sup	O
e	O
{	O
lt	O
(	O
x	O
)	O
-	O
yi	O
}	O
-	O
1jjefi1	O
l	O
-	O
l	O
it	O
(	O
xd	O
-	O
yil	O
.	O
i	O
in	O
n	O
i=l	O
define	O
the	O
class	O
mn	O
of	O
functions	O
on	O
nd	O
x	O
{	O
o	O
,	O
i	O
}	O
by	O
mil	O
=	O
{	O
m	O
(	O
x	O
,	O
y	O
)	O
=	O
itcig	O
(	O
arx	O
+bi	O
)	O
+co	O
-	O
yl	O
:	O
ai	O
e	O
rd	O
,	O
hi	O
e	O
r	O
,	O
~	O
[	O
cd	O
:	O
s	O
:	O
fjn	O
}	O
.	O
then	O
the	O
previous	O
bound	O
becomes	O
2	O
sup	O
e	O
{	O
m	O
(	O
x	O
,	O
y	O
)	O
}	O
-	O
mem	O
''	O
l	O
i	O
in	O
-	O
lm	O
(	O
xi	O
,	O
yi	O
)	O
n	O
i=l	O
.	O
such	O
quantities	O
may	O
be	O
handled	O
by	O
the	O
uniform	B
law	O
of	O
large	O
numbers	O
of	O
theorem	O
29.1	O
,	O
which	O
applies	O
to	O
classes	O
of	O
uniformly	O
bounded	O
functions	O
.	O
indeed	O
,	O
for	O
each	O
m	O
emn	O
)	O
<	O
2max	O
~	O
lcd	O
,	O
1	O
kl1	O
(	O
<	O
2	O
{	O
3n	O
,	O
530	O
30.	O
neural	O
networks	O
if	O
n	O
is	O
so	O
large	O
that	O
f3n	O
2	O
:	O
1.	O
for	O
such	O
n	O
's	O
,	O
theorem	B
29.1	O
states	O
that	O
p	O
{	O
sup	O
ie	O
{	O
m	O
(	O
x	O
,	O
y	O
)	O
}	O
-	O
~	O
t111	O
(	O
xi	O
,	O
yi	O
)	O
i	O
>	O
e	O
}	O
memn	O
1=1	O
where	O
n	O
(	O
e	O
,	O
mn	O
(	O
dn	O
»	O
denotes	O
the	O
h	O
-covering	O
number	O
of	O
the	O
random	O
set	O
:	O
:	O
:	O
:	O
:	O
8e	O
{	O
n	O
(	O
e/8	O
,	O
mn	O
(	O
dn	O
»	O
}	O
e-ne2/	O
(	O
512f3,7	O
)	O
,	O
defined	O
in	O
chapter	O
29.	O
all	O
we	O
need	O
now	O
is	O
to	O
estimate	B
these	O
covering	B
numbers	O
.	O
observe	O
that	O
for	O
1111,1112	O
e	O
mn	O
with	O
1111	O
(	O
x	O
,	O
y	O
)	O
=	O
11/fi	O
(	O
x	O
)	O
-	O
yl	O
and	O
1112	O
(	O
x	O
,	O
y	O
)	O
=	O
11/f2	O
(	O
x	O
)	O
-	O
yl	O
,	O
for	O
any	O
probability	O
measure	B
v	O
on	O
n	O
d	O
x	O
{	O
o	O
,	O
i	O
}	O
,	O
f	O
1111i	O
(	O
x	O
,	O
y	O
)	O
-	O
1112	O
(	O
x	O
,	O
y	O
)	O
lv	O
(	O
d	O
(	O
x	O
,	O
y	O
»	O
)	O
:	O
:	O
:	O
:	O
:	O
f	O
11/fi	O
(	O
x	O
)	O
-1/f2	O
(	O
x	O
)	O
lfl	O
(	O
dx	O
)	O
,	O
where	O
fl	O
is	O
the	O
marginal	O
of	O
von	O
nd	O
.	O
therefore	O
,	O
it	O
follows	O
thatn	O
(	O
e	O
,	O
mn	O
(	O
dn	O
»	O
:	O
:	O
:	O
:	O
:	O
n	O
(	O
e	O
,	O
:	O
fn	O
(	O
x7	O
»	O
,	O
where	O
x7	O
=	O
(	O
xl	O
,	O
...	O
,	O
xn	O
)	O
.	O
it	O
means	O
that	O
an	O
upper	O
bound	O
on	O
the	O
covering	B
number	I
of	O
the	O
class	O
of	O
neural	O
networks	O
:	O
fn	O
is	O
also	O
an	O
upper	O
bound	O
on	O
the	O
quantity	O
that	O
interests	O
us	O
.	O
this	O
bounding	O
may	O
be	O
done	O
by	O
applying	O
lemmas	O
from	O
chapters	O
13	O
and	O
30.	O
define	O
the	O
following	O
three	O
collections	O
of	O
functions	O
:	O
qi	O
q2	O
q3	O
{	O
at	O
x	O
+	O
b	O
;	O
a	O
end	O
,	O
ben	O
}	O
,	O
{	O
0-	O
(	O
a	O
t	O
x	O
+	O
b	O
)	O
;	O
a	O
end	O
,	O
ben	O
}	O
,	O
{	O
co-cat	O
x	O
+	O
b	O
)	O
;	O
a	O
end	O
,	O
ben	O
,	O
c	O
e	O
[	O
-f3n	O
,	O
f3nj	O
}	O
.	O
by	O
theorem	B
13.9	O
,	O
the	O
vc	B
dimension	I
of	O
the	O
class	O
of	O
sets	O
qt	O
=	O
{	O
(	O
x	O
,	O
t	O
)	O
:	O
t	O
:	O
:	O
:	O
:	O
:	O
1/f	O
(	O
x	O
)	O
,	O
1/f	O
e	O
qd	O
is	O
vgt	O
:	O
:	O
:	O
:	O
:	O
d	O
+	O
2.	O
this	O
implies	O
by	O
lemma	O
29.5	O
that	O
vg	O
;	O
:	O
:	O
:	O
:	O
:	O
d	O
+	O
2	O
,	O
so	O
by	O
corollary	O
29.2	O
,	O
for	O
any	O
xl1	O
=	O
(	O
xl	O
,	O
...	O
,	O
xn	O
)	O
,	O
n	O
(	O
e	O
,	O
q2	O
(	O
x~	O
»	O
:	O
:	O
:	O
:	O
:	O
2	O
(	O
4	O
)	O
2	O
(	O
d+2	O
)	O
e	O
e	O
,	O
where	O
q2	O
(	O
x	O
?	O
)	O
=	O
{	O
z	O
e	O
nn	O
:	O
z	O
=	O
(	O
g	O
(	O
xd	O
,	O
...	O
,	O
g	O
(	O
xn	O
»	O
,	O
g	O
e	O
q2	O
}	O
.	O
now	O
,	O
using	O
similar	O
notations	O
,	O
theorem	B
29.7	O
allows	O
us	O
to	O
estimate	B
covering	O
numbers	O
of	O
q3	O
(	O
xn	O
:	O
n	O
(	O
e	O
,	O
q3	O
(	O
x	O
?	O
)	O
:	O
:	O
:	O
:	O
:	O
;	O
n	O
(	O
e/	O
(	O
2f3n	O
)	O
,	O
q2	O
(	O
x	O
?	O
»	O
)	O
:	O
:	O
:	O
:	O
:	O
4	O
(	O
8	O
f3	O
)	O
2d+s	O
:	O
n	O
if	O
fin	O
>	O
2/e	O
.	O
finally	O
,	O
we	O
can	O
apply	O
lemma	O
29.6	O
to	O
obtain	O
<	O
30.6	O
the	O
adaline	O
and	O
padaline	O
531	O
.	O
thus	O
,	O
substituting	O
this	O
bound	O
into	O
the	O
probability	O
inequality	B
above	O
,	O
we	O
get	O
for	O
n	O
large	O
enough	O
,	O
which	O
tends	O
to	O
zero	O
if	O
concluding	O
the	O
proof	O
.	O
0	O
knf3~	O
10g	O
(	O
knf3n	O
)	O
-	O
...	O
.	O
:	O
.	O
:	O
...	O
.	O
--	O
--	O
-+0	O
,	O
n	O
there	O
are	O
yet	O
other	O
ways	O
to	O
obtain	O
consistency	B
for	O
general	O
sigmoidal	O
networks	O
.	O
we	O
may	O
restrict	O
the	O
network	O
by	O
discretizing	O
the	O
values	O
of	O
the	O
coefficients	O
in	O
some	O
way-thus	O
creating	O
a	O
sieve	O
with	O
a	O
number	O
of	O
members	O
that	O
is	O
easy	O
to	O
enumerate	O
(	O
cid:173	O
)	O
and	O
applying	O
complexity	B
regularization	I
(	O
chapter	O
18	O
)	O
.	O
this	O
is	O
the	O
method	O
followed	O
by	O
barron	O
(	O
1988	O
;	O
1991	O
)	O
.	O
30.6	O
the	O
adaline	O
and	O
padaline	O
widrow	O
(	O
1959	O
)	O
and	O
widrow	O
and	O
hoff	O
(	O
1960	O
)	O
introduced	O
the	O
adaline	O
,	O
and	O
specht	O
(	O
1967	O
;	O
1990	O
)	O
studied	O
polynomial	O
discriminant	O
functions	O
such	O
as	O
the	O
padaline	O
.	O
looked	O
at	O
formally	O
,	O
the	O
discriminant	O
function	O
1/1	O
used	O
in	O
the	O
decision	O
g	O
(	O
x	O
)	O
=	O
i	O
{	O
1/j	O
(	O
x	O
»	O
o	O
}	O
is	O
of	O
a	O
polynomial	O
nature	O
,	O
with	O
1/1	O
consisting	O
of	O
sums	O
of	O
monomials	O
like	O
(	O
(	O
l	O
»	O
)	O
il	O
a	O
x	O
(	O
d	O
»	O
)	O
id	O
'	O
''	O
x	O
,	O
where	O
ii	O
,	O
...	O
,	O
id	O
:	O
:	O
:	O
:	O
0	O
are	O
integers	O
,	O
and	O
a	O
is	O
a	O
coefficient	O
.	O
the	O
order	O
of	O
a	O
monomial	O
is	O
i	O
1	O
+	O
...	O
+	O
id	O
•	O
usually	O
,	O
all	O
terms	O
up	O
to	O
,	O
and	O
including	O
those	O
of	O
order	O
r	O
are	O
included	O
.	O
widrow	O
's	O
adaline	O
(	O
1959	O
)	O
has	O
r	O
=	O
1.	O
the	O
total	O
number	O
of	O
monomials	O
of	O
order	O
r	O
or	O
less	O
does	O
not	O
exceed	O
(	O
r	O
+	O
1	O
)	O
d.	O
the	O
motivation	O
for	O
developing	O
these	O
discriminants	O
is	O
that	O
only	O
up	O
to	O
(	O
r	O
+	O
l	O
)	O
d	O
coefficients	O
need	O
to	O
be	O
trained	O
and	O
stored	O
.	O
in	O
applications	O
in	O
which	O
data	O
continuously	O
arrive	O
,	O
the	O
coefficients	O
may	O
be	O
updated	O
on-line	O
and	O
the	O
data	O
can	O
be	O
discarded	O
.	O
this	O
property	O
is	O
,	O
of	O
course	O
,	O
shared	O
with	O
standard	O
neu	O
(	O
cid:173	O
)	O
ral	O
networks	O
.	O
in	O
most	O
cases	O
,	O
order	O
r	O
polynomial	B
discriminants	O
are	O
not	O
translation	O
invariant	O
.	O
minimizing	O
a	O
given	O
criterion	O
on-line	O
is	O
a	O
phenomenal	O
task	O
,	O
so	O
specht	O
noted	O
that	O
training	O
is	O
not	O
necessary	O
if	O
the	O
a	O
's	O
are	O
chosen	O
so	O
as	O
to	O
give	O
decisions	O
that	O
are	O
close	O
to	O
those	O
of	O
the	O
kernel	B
method	O
with	O
normal	O
kernels	O
.	O
for	O
example	O
,	O
if	O
k	O
(	O
u	O
)	O
=	O
e-	O
u2	O
/	O
2	O
,	O
the	O
kernel	B
method	O
picks	O
a	O
smoothing	O
factor	O
h	O
532	O
30.	O
neural	O
networks	O
(	O
such	O
that	O
h	O
-+	O
0	O
and	O
nh	O
d	O
-+	O
00	O
;	O
see	O
chapter	O
10	O
)	O
and	O
uses	O
1/r	O
(	O
x	O
)	O
~	O
t	O
(	O
2yi	O
-1	O
)	O
k	O
(	O
iix	O
-	O
xiii	O
)	O
n	O
i=l	O
h	O
the	O
same	O
decision	O
is	O
obtained	O
if	O
we	O
use	O
now	O
,	O
approximate	O
this	O
by	O
using	O
taylor	O
's	O
series	O
expansion	O
and	O
truncating	O
to	O
the	O
order	O
r	O
terms	O
.	O
for	O
example	O
,	O
the	O
coefficient	O
of	O
(	O
x	O
(	O
l	O
)	O
)	O
i	O
!	O
...	O
(	O
x	O
(	O
d	O
)	O
)	O
i	O
d	O
in	O
the	O
expansion	B
of	I
1/r	O
(	O
x	O
)	O
would	O
be	O
,	O
if	O
l~=l	O
i	O
j	O
=	O
i	O
,	O
these	O
sums	O
are	O
easy	O
to	O
update	O
on-line	O
,	O
and	O
decisions	O
are	O
based	O
on	O
the	O
sign	O
of	O
the	O
order	O
r	O
truncation	O
1/r	O
r	O
of	O
1/r	O
.	O
the	O
classifier	B
is	O
called	O
the	O
padaline	O
.	O
specht	O
notes	O
that	O
overfitting	B
in	O
1/rr	O
does	O
not	O
occur	O
due	O
to	O
the	O
fact	O
that	O
overfitting	B
does	O
not	O
occur	O
for	O
the	O
kemel	O
method	O
based	O
on	O
1/r	O
.	O
his	O
method	O
interpolates	O
between	O
the	O
latter	O
method	O
,	O
generalized	O
linear	O
discrimination	O
,	O
and	O
generalizations	O
of	O
the	O
perceptron	B
.	O
for	O
fixed	O
r	O
,	O
the	O
pad	O
aline	O
defined	O
above	O
is	O
not	O
universally	O
consistent	O
(	O
for	O
the	O
same	O
reason	O
linear	O
discriminants	O
are	O
not	O
universally	O
consistent	O
)	O
,	O
but	O
if	O
r	O
is	O
allowed	O
to	O
grow	O
with	O
n	O
,	O
the	O
decision	O
based	O
on	O
the	O
sign	O
of	O
1/rr	O
becomes	O
universally	O
consistent	O
(	O
problem	O
30.14	O
)	O
.	O
recall	O
,	O
however	O
,	O
that	O
padaline	O
was	O
not	O
designed	O
with	O
a	O
variable	O
r	O
in	O
mind	O
.	O
30.7	O
polynomial	B
networks	O
besides	O
adaline	O
and	O
padaline	O
,	O
there	O
are	O
several	O
ways	O
of	O
constructing	O
polyno	O
(	O
cid:173	O
)	O
mial	O
many-layered	O
networks	O
in	O
which	O
basic	O
units	O
are	O
of	O
the	O
form	O
(	O
xcl	O
)	O
t	O
...	O
(	O
xck	O
)	O
)	O
ik	O
for	O
inputs	O
xci	O
)	O
,	O
...	O
,	O
x	O
(	O
k	O
)	O
to	O
that	O
level	O
.	O
pioneers	O
in	O
this	O
respect	O
are	O
gabor	O
(	O
1961	O
)	O
,	O
ivakhnenko	O
(	O
1968	O
;	O
1971	O
)	O
who	O
invented	O
the	O
gmdh	O
method-the	O
group	O
method	O
of	O
data	O
handling-and	O
barron	O
(	O
1975	O
)	O
.	O
see	O
also	O
ivakhnenko	O
,	O
konovalenko	O
,	O
tulupchuk	O
,	O
and	O
tymchenko	O
(	O
1968	O
)	O
and	O
ivakhnenko	O
,	O
petrache	O
,	O
and	O
krasyts'kyy	O
(	O
1968	O
)	O
.	O
these	O
networks	O
can	O
be	O
visualized	O
in	O
the	O
following	O
way	O
:	O
30.7	O
polynomial	B
networks	O
533.	O
figure	O
30.13.	O
simple	O
polynomial	O
network	O
:	O
each	O
gi	O
represents	O
a	O
simple	O
polynomialfunction	O
of	O
its	O
input	O
.	O
in	O
barron	O
's	O
work	O
(	O
barron	O
(	O
1975	O
)	O
,	O
barron	O
and	O
barron	O
(	O
1988	O
)	O
)	O
,	O
the	O
gi	O
's	O
are	O
sometimes	O
2-input	O
elements	O
of	O
the	O
form	O
gi	O
(	O
x	O
,	O
y	O
)	O
=	O
ai	O
+	O
bix	O
+	O
ciy	O
+	O
dixy	O
.	O
if	O
the	O
gi	O
's	O
are	O
barron	O
's	O
quadratic	O
elements	O
,	O
then	O
the	O
network	O
shown	O
in	O
figure	O
30.13	O
represents	O
a	O
particular	O
polynomial	B
of	O
order	O
8	O
in	O
which	O
the	O
largest	O
degree	O
of	O
any	O
xci	O
)	O
is	O
at	O
most	O
4.	O
the	O
number	O
of	O
unknown	O
coefficients	O
is	O
36	O
in	O
the	O
example	O
shown	O
in	O
the	O
figure	O
,	O
while	O
in	O
a	O
full-fledged	O
order-8	O
polynomial	B
network	I
,	O
it	O
would	O
be	O
much	O
larger	O
.	O
for	O
training	O
these	O
networks	O
,	O
many	O
strategies	O
have	O
been	O
proposed	O
by	O
barron	O
and	O
his	O
associates	O
.	O
ivakhnenko	O
(	O
1971	O
)	O
,	O
for	O
example	O
,	O
trains	O
one	O
layer	O
at	O
a	O
time	O
and	O
lets	O
only	O
the	O
best	O
neurons	O
in	O
each	O
layer	O
survive	O
for	O
use	O
as	O
input	O
in	O
the	O
next	O
layer	O
.	O
it	O
is	O
easy	O
to	O
see	O
that	O
polynomial	B
networks	O
,	O
even	O
with	O
only	O
two	O
inputs	O
per	O
node	O
,	O
and	O
with	O
degree	O
in	O
each	O
cell	O
restricted	O
to	O
two	O
,	O
but	O
with	O
an	O
unrestricted	O
number	O
of	O
layers	O
,	O
can	O
implement	O
any	O
polynomial	B
in	O
d	O
variables	O
.	O
as	O
the	O
polynomials	O
are	O
dense	O
in	O
the	O
loo	O
sense	O
on	O
c	O
[	O
a	O
,	O
b	O
]	O
d	O
for	O
all	O
a	O
,	O
bend	O
,	O
we	O
note	O
by	O
lemma	O
30.2	O
that	O
such	O
networks	O
include	O
a	O
sequence	O
of	O
classifiers	O
approaching	O
the	O
bayes	O
error	O
for	O
any	O
distribution	B
.	O
consider	O
several	O
classes	O
of	O
polynomials	O
:	O
where	O
0/1	O
,	O
...	O
,	O
o/k	O
are	O
fixed	O
monomials	O
,	O
but	O
the	O
ai	O
's	O
are	O
free	O
coefficients	O
.	O
(	O
'h	O
=	O
{	O
o/	O
e	O
(	O
'h	O
:	O
0/1	O
,	O
...	O
,	O
o/k	O
are	O
monomials	O
of	O
order	O
.	O
:	O
:	O
;	O
r	O
}	O
.	O
(	O
the	O
order	O
of	O
a	O
monomial	O
(	O
x	O
(	O
l	O
»	O
)	O
i	O
1	O
•••	O
(	O
x	O
(	O
d	O
»	O
)	O
i	O
d	O
is	O
il	O
+	O
...	O
+	O
id	O
.	O
)	O
93	O
=	O
{	O
o/	O
e	O
91	O
:	O
0/1	O
,	O
...	O
,	O
o/k	O
are	O
monomials	O
of	O
order	O
~	O
r	O
,	O
but	O
k	O
is	O
not	O
fixed	O
}	O
.	O
as	O
k	O
~	O
00	O
,	O
91	O
does	O
not	O
generally	O
become	O
dense	O
in	O
c	O
[	O
a	O
,	O
b	O
]	O
d	O
in	O
the	O
loo	O
sense	O
unless	O
we	O
add	O
o/k+l	O
,	O
0/k+2	O
,	O
...	O
in	O
a	O
special	O
way	O
.	O
however	O
,	O
93	O
becomes	O
dense	O
as	O
r	O
~	O
00	O
by	O
theorem	B
a.9	O
and	O
92	O
becomes	O
dense	O
as	O
both	O
k	O
~	O
00	O
and	O
r	O
~	O
00	O
(	O
as	O
92	O
contains	O
a	O
subclass	O
of	O
93	O
for	O
a	O
smaller	O
r	O
depending	O
upon	O
k	O
obtained	O
by	O
including	O
in	O
l/ii	O
,	O
...	O
,	O
o/k	O
all	O
monomials	O
in	O
increasing	O
order	O
)	O
.	O
the	O
vc	B
dimension	I
of	O
the	O
class	O
of	O
classifiers	O
based	O
on	O
91	O
is	O
not	O
more	O
than	O
k	O
(	O
see	O
theorem	B
13.9	O
)	O
.	O
the	O
vc	B
dimension	I
of	O
classifiers	O
based	O
upon	O
92	O
does	O
not	O
exceed	O
534	O
30.	O
neural	O
networks	O
those	O
of	O
~h	O
,	O
which	O
in	O
tum	O
is	O
nothing	O
but	O
the	O
number	O
of	O
possible	O
monomials	O
of	O
order	O
:	O
:	O
:	O
;	O
r.	O
a	O
simple	O
counting	O
argument	O
shows	O
that	O
this	O
is	O
bounded	O
by	O
(	O
r	O
+	O
1	O
)	O
d	O
.	O
see	O
also	O
anthony	O
and	O
holden	O
(	O
1993	O
)	O
.	O
these	O
simple	O
bounds	O
may	O
be	O
used	O
to	O
study	O
the	O
consistency	B
of	O
polynomial	B
net	O
(	O
cid:173	O
)	O
works	O
.	O
let	O
us	O
take	O
a	O
fixed	O
structure	O
network	O
in	O
which	O
all	O
nodes	O
are	O
fixed-they	O
have	O
at	O
most	O
k	O
inputs	O
with	O
k	O
:	O
:	O
:	O
:	O
2	O
fixed	O
and	O
represent	O
polynomials	O
of	O
order	O
:	O
:	O
:	O
;	O
r	O
with	O
r	O
:	O
:	O
:	O
:	O
2	O
fixed	O
.	O
for	O
example	O
,	O
with	O
r	O
=	O
2	O
,	O
each	O
cell	O
with	O
input	O
z	O
i	O
,	O
...	O
,	O
zk	O
computes	O
li	O
ai	O
o/i	O
(	O
zl	O
,	O
...	O
,	O
zk	O
)	O
,	O
where	O
the	O
ai	O
's	O
are	O
coefficients	O
and	O
the	O
o/i	O
's	O
are	O
fixed	O
monomi	O
(	O
cid:173	O
)	O
als	O
of	O
order	O
r	O
or	O
less	O
,	O
and	O
all	O
such	O
monomials	O
are	O
included	O
.	O
assume	O
that	O
the	O
layout	O
is	O
fixed	O
and	O
is	O
such	O
that	O
it	O
can	O
realize	O
all	O
polynomials	O
of	O
order	O
:	O
:	O
:	O
;	O
s	O
on	O
the	O
input	O
x	O
(	O
l	O
)	O
,	O
...	O
,	O
x	O
(	O
d	O
)	O
.	O
one	O
way	O
of	O
doing	O
this	O
is	O
to	O
realize	O
all	O
polynomials	O
of	O
order	O
:	O
:	O
:	O
;	O
r	O
by	O
taking	O
all	O
(	O
~	O
)	O
possible	O
input	O
combinations	O
,	O
and	O
to	O
repeat	O
at	O
the	O
second	O
level	O
with	O
(	O
cp	O
)	O
cells	O
of	O
neurons	O
,	O
and	O
so	O
forth	O
for	O
a	O
total	O
of	O
s	O
/	O
r	O
layers	O
of	O
cells	O
.	O
this	O
construc	O
(	O
cid:173	O
)	O
tion	O
is	O
obviously	O
redundant	O
but	O
it	O
will	O
do	O
for	O
now	O
.	O
then	O
note	O
that	O
the	O
vc	B
dimension	I
is	O
not	O
more	O
than	O
(	O
s	O
+	O
1	O
)	O
d	O
,	O
as	O
noted	O
above	O
.	O
if	O
we	O
choose	O
the	O
best	O
coefficients	O
in	O
the	O
cells	O
by	O
empirical	B
risk	I
minimization	I
,	O
then	O
the	O
method	O
is	O
consistent	O
:	O
theorem	B
30.10.	O
in	O
the	O
fixed-structure	O
polynomial	B
network	I
described	O
above	O
,	O
if	O
ln	O
is	O
the	O
probability	O
of	O
error	O
of	O
the	O
empirical	B
risk	I
minimizer	O
,	O
then	O
e	O
{	O
ln	O
}	O
-+	O
l	O
*	O
if	O
s	O
-+	O
(	O
x	O
)	O
and	O
s	O
=	O
o	O
(	O
n	O
lid	O
)	O
.	O
proof	O
.	O
apply	O
lemma	O
30.2	O
and	O
theorem	B
12.6	O
.	O
0	O
assume	O
a	O
fixed-structure	O
network	O
as	O
above	O
such	O
that	O
all	O
polynomials	O
of	O
order	O
:	O
:	O
:	O
;	O
s	O
are	O
realized	O
plus	O
some	O
other	O
ones	O
,	O
while	O
the	O
number	O
of	O
layers	O
of	O
cells	O
is	O
not	O
more	O
than	O
i.	O
then	O
the	O
vc	B
dimension	I
is	O
not	O
more	O
than	O
(	O
r	O
i	O
+	O
l	O
)	O
d	O
because	O
the	O
maximal	O
order	O
is	O
not	O
more	O
than	O
ir	O
.	O
hence	O
,	O
we	O
have	O
consistency	B
under	O
the	O
same	O
conditions	O
as	O
above	O
,	O
that	O
is	O
,	O
s	O
-+	O
00	O
,	O
and	O
i	O
=	O
o	O
(	O
n	O
lid	O
)	O
.	O
similar	O
considerations	O
can	O
now	O
be	O
used	O
in	O
a	O
variety	O
of	O
situations	O
.	O
30.8	O
kolmogorov-lorentz	O
networks	O
and	O
additive	O
models	O
answering	O
one	O
of	O
hilbert	O
's	O
famous	O
questions	O
,	O
kolmogorov	O
(	O
1957	O
)	O
and	O
lorentz	O
(	O
1976	O
)	O
(	O
see	O
also	O
sprecher	O
(	O
1965	O
)	O
and	O
hecht-nielsen	O
(	O
1987	O
)	O
)	O
obtained	O
the	O
following	O
interesting	O
representation	O
of	O
any	O
continuous	O
function	O
on	O
[	O
0	O
,	O
1	O
]	O
d.	O
theorem	B
30.11	O
.	O
(	O
kolmogorov	O
(	O
1957	O
)	O
;	O
lorentz	O
(	O
1976	O
)	O
)	O
.	O
let	O
f	O
be	O
continuous	O
on	O
[	O
0	O
,	O
1	O
]	O
d.	O
then	O
f	O
can	O
be	O
rewritten	O
asfollows	O
:	O
let	O
<	O
5	O
>	O
0	O
be	O
an	O
arbitrary	O
constant	O
,	O
and	O
choose	O
0	O
<	O
e	O
:	O
:	O
:	O
;	O
<	O
5	O
rational	O
.	O
then	O
2d+1	O
f	O
=	O
l	O
g	O
(	O
zk	O
)	O
,	O
k=l	O
30.8	O
kolmogorov-lorentz	O
networks	O
and	O
additive	O
models	O
535	O
where	O
g	O
:	O
r	O
--	O
--	O
'	O
»	O
-	O
r	O
is	O
a	O
continuous	O
function	O
(	O
depending	O
upon	O
f	O
and	O
e	O
)	O
,	O
and	O
each	O
zk	O
is	O
rewritten	O
as	O
d	O
zk	O
=	O
lak	O
1fr	O
(	O
xu	O
)	O
+	O
ek	O
)	O
+	O
k.	O
j=l	O
here	O
a	O
is	O
real	O
and	O
1jf	O
is	O
monotonic	O
and	O
increasing	O
in	O
its	O
argument	O
.	O
also	O
,	O
both	O
a	O
and	O
1jf	O
are	O
universal	B
(	O
independent	O
of	O
f	O
)	O
,	O
and	O
1jf	O
is	O
lipschitz	O
:	O
11jf	O
(	O
x	O
)	O
-1jf	O
(	O
y	O
)	O
1	O
:	O
s	O
clx	O
-	O
y	O
1	O
for	O
some	O
c	O
>	O
0.	O
the	O
kolmogorov-lorentz	O
theorem	B
states	O
that	O
f	O
may	O
be	O
represented	O
by	O
a	O
very	O
simple	O
network	O
that	O
we	O
will	O
call	O
the	O
kolmogorov-lorentz	O
network	O
.	O
what	O
is	O
amaz	O
(	O
cid:173	O
)	O
ing	O
is	O
that	O
the	O
first	O
layer	O
is	O
fixed	O
and	O
known	O
beforehand	O
.	O
only	O
the	O
mapping	O
g	O
depends	O
on	O
f.	O
this	O
representation	O
immediately	O
opens	O
up	O
new	O
revenues	O
of	O
pursuit	O
(	O
cid:173	O
)	O
we	O
need	O
not	O
mix	O
the	O
input	O
variables	O
.	O
simple	O
additive	O
functions	O
of	O
the	O
input	O
variables	O
suffice	O
to	O
represent	O
all	O
continuous	O
functions	O
.	O
input	O
figure	O
30.14.	O
the	O
kolmogorov-lorentz	O
network	O
of	O
theorem	O
30.11.	O
to	O
explain	O
what	O
is	O
happening	O
here	O
,	O
we	O
look	O
at	O
the	O
interleaving	O
of	O
bits	O
to	O
make	O
one-dimensional	O
numbers	O
out	O
of	O
d-dimensional	O
vectors	O
.	O
for	O
the	O
sake	O
of	O
simplicity	O
,	O
let	O
f	O
:	O
[	O
0	O
,	O
1	O
]	O
2	O
--	O
--	O
'	O
»	O
-	O
r.	O
let	O
be	O
the	O
binary	B
expansions	O
of	O
x	O
and	O
y	O
,	O
and	O
consider	O
a	O
representation	O
for	O
the	O
function	O
f	O
(	O
x	O
,	O
y	O
)	O
.	O
the	O
bit-interleaved	O
number	O
z	O
e	O
[	O
0	O
,	O
1	O
]	O
has	O
binary	B
expansion	O
and	O
may	O
thus	O
be	O
written	O
as	O
z	O
=	O
¢	O
(	O
x	O
)	O
+	O
(	O
lj2	O
)	O
¢	O
(	O
y	O
)	O
,	O
where	O
536	O
30.	O
neural	O
networks	O
and	O
thus	O
1	O
2cp	O
(	O
y	O
)	O
=	O
o.oy10y20y3	O
...	O
.	O
we	O
can	O
also	O
retrieve	O
x	O
and	O
y	O
from	O
z	O
by	O
noting	O
that	O
x	O
=	O
0/1	O
(	O
z	O
)	O
,	O
0/1	O
(	O
z	O
)	O
=	O
o.z1z3zsz7	O
...	O
,	O
y	O
=	O
0/2	O
(	O
z	O
)	O
,	O
0/2	O
(	O
z	O
)	O
=	O
o.z2z4z6zs	O
...	O
.	O
and	O
therefore	O
,	O
j	O
(	O
x	O
,	O
y	O
)	O
=	O
j	O
(	O
0/1	O
(	O
z	O
)	O
,0/2	O
(	O
z	O
)	O
)	O
def	O
g	O
(	O
z	O
)	O
(	O
a	O
one-dimensional	O
function	O
of	O
z	O
)	O
=	O
g	O
(	O
1	O
)	O
(	O
x	O
)	O
+	O
~1	O
>	O
(	O
y	O
»	O
)	O
the	O
function	O
cp	O
is	O
strictly	O
monotone	O
increasing	O
.	O
unfortunately	O
,	O
cp	O
,	O
0/1	O
,	O
and	O
0/2	O
are	O
not	O
continuous	O
.	O
kolmogorov	O
's	O
theorem	B
for	O
this	O
special	O
case	O
is	O
as	O
follows	O
:	O
theorem	B
30.12	O
.	O
(	O
kolmogorov	O
(	O
1957	O
)	O
)	O
.	O
there	O
exist	O
jive	O
monotone	O
junctions	O
cpi	O
,	O
...	O
,	O
cps	O
:	O
[	O
0	O
,	O
1	O
]	O
-+	O
rsatisfying	O
icpi	O
(	O
xi	O
)	O
-cpi	O
(	O
x2	O
)	O
1	O
:	O
:	O
;	O
ixi	O
-x21	O
,	O
withthejollowing	O
property	O
:	O
jor	O
every	O
j	O
e	O
e	O
[	O
o	O
,	O
1	O
]	O
2	O
(	O
the	O
continuous	O
junctions	O
on	O
[	O
0	O
,	O
1	O
]	O
2	O
)	O
,	O
there	O
exists	O
a	O
continuous	O
junction	O
g	O
such	O
thatjor	O
all	O
(	O
xl	O
,	O
x2	O
)	O
e	O
[	O
0,1	O
]	O
2	O
,	O
the	O
difference	O
with	O
pure	O
bit-interleaving	O
is	O
that	O
now	O
the	O
cpi	O
's	O
are	O
continuous	O
and	O
g	O
is	O
continuous	O
whenever	O
j	O
is	O
.	O
also	O
,	O
just	O
as	O
in	O
our	O
simle	O
example	O
,	O
kolmogorov	O
gives	O
an	O
explicit	O
construction	O
for	O
cpl	O
,	O
...	O
,	O
cps	O
.	O
kolmogorov	O
's	O
theorem	B
may	O
be	O
used	O
to	O
show	O
the	O
denseness	B
of	O
certain	O
classes	O
of	O
functions	O
that	O
may	O
be	O
described	O
by	O
networks	O
.	O
there	O
is	O
one	O
pitfall	O
however	O
:	O
any	O
such	O
result	O
must	O
involve	O
at	O
least	O
one	O
neuron	O
or	O
cell	O
that	O
has	O
a	O
general	O
function	O
in	O
it	O
,	O
and	O
we	O
are	O
back	O
at	O
square	O
one	O
,	O
because	O
a	O
general	O
function	O
,	O
even	O
on	O
only	O
one	O
input	O
,	O
may	O
be	O
arbitrarily	O
complicated	O
and	O
wild	O
.	O
additive	O
models	O
include	O
,	O
for	O
example	O
,	O
models	O
such	O
as	O
d	O
ex	O
+	O
l	O
o/i	O
(	O
x	O
(	O
i	O
)	O
)	O
,	O
i=l	O
where	O
the	O
o/i	O
's	O
are	O
unspecified	O
univariate	O
functions	O
(	O
friedman	O
and	O
silverman	O
(	O
1989	O
)	O
,	O
hastie	O
and	O
tibshirani	O
(	O
1990	O
)	O
)	O
.	O
these	O
are	O
not	O
powerful	O
enough	O
to	O
approxi	O
(	O
cid:173	O
)	O
mate	O
all	O
functions	O
.	O
a	O
generalized	O
additive	B
model	I
is	O
30.8	O
kolmogorov-lorentz	O
networks	O
and	O
additive	O
models	O
537	O
(	O
hastie	O
and	O
tibshirani	O
(	O
1990	O
»	O
,	O
where	O
(	O
5	O
is	O
now	O
a	O
given	O
or	O
unspecified	O
function	O
.	O
from	O
kolmogorov	O
's	O
theorem	B
,	O
we	O
know	O
that	O
the	O
model	O
with	O
l/fi	O
,	O
k	O
,	O
l/f	O
unspecified	O
functions	O
,	O
includes	O
all	O
continuous	O
functions	O
on	O
compact	O
sets	O
and	O
is	O
thus	O
ideally	O
suited	O
for	O
constructing	O
networks	O
.	O
in	O
fact	O
,	O
we	O
may	O
take	O
ctk	O
=	O
k	O
and	O
take	O
alll/fi	O
,	O
k	O
's	O
as	O
specified	O
in	O
kolmogorov	O
's	O
theorem	B
.	O
this	O
leaves	O
only	O
l/f	O
as	O
the	O
unknown	O
.	O
now	O
,	O
consider	O
the	O
following	O
:	O
any	O
continuous	O
univariate	O
function	O
f	O
can	O
be	O
ap	O
(	O
cid:173	O
)	O
proximated	O
on	O
bounded	O
sets	O
to	O
within	O
e	O
by	O
simple	O
combinations	O
of	O
threshold	O
sigmoids	O
(	O
5	O
of	O
the	O
form	O
k	O
l	O
ai	O
(	O
5	O
(	O
x	O
-	O
ci	O
)	O
,	O
i=l	O
where	O
ai	O
,	O
ci	O
,	O
k	O
are	O
variable	B
.	O
this	O
leads	O
to	O
a	O
two-hidden-layer	O
neural	B
network	I
repre	O
(	O
cid:173	O
)	O
sentation	O
related	O
to	O
that	O
of	O
kurkova	O
(	O
1992	O
)	O
,	O
where	O
only	O
the	O
last	O
layer	O
has	O
unknown	O
coefficients	O
for	O
a	O
total	O
of	O
2k	O
.	O
theorem	B
30.13.	O
consider	O
a	O
network	O
classifier	B
of	O
the	O
form	O
described	O
above	O
in	O
which	O
(	O
5	O
(	O
.	O
)	O
is	O
the	O
threshold	B
sigmoid	O
,	O
and	O
the	O
ai	O
's	O
and	O
ci	O
's	O
are	O
found	O
by	O
empirical	B
error	I
minimization	O
.	O
then	O
e	O
{	O
l	O
n	O
}	O
-7	O
>	O
l	O
*	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
,	O
if	O
k	O
-7	O
>	O
00	O
and	O
klognjn	O
-7	O
>	O
o.	O
proof	O
.	O
we	O
will	O
only	O
outline	O
the	O
proof	O
.	O
first	O
observe	O
that	O
we	O
may	O
approximate	O
all	O
functions	O
on	O
c	O
[	O
a	O
,	O
b	O
]	O
d	O
by	O
selecting	O
k	O
large	O
enough	O
.	O
by	O
lemma	O
30.2	O
and	O
the	O
(	O
cid:173	O
)	O
orem	O
12.6	O
,	O
it	O
suffices	O
to	O
show	O
that	O
the	O
vc	B
dimension	I
of	O
our	O
class	O
of	O
classifiers	O
is	O
o	O
(	O
n	O
j	O
log	O
n	O
)	O
.	O
considering	O
cik	O
+	O
2	O
:	O
.1=1	O
l/fi	O
,	O
k	O
(	O
xci	O
»	O
as	O
new	O
input	O
elements	O
,	O
called	O
yb	O
1	O
s	O
k	O
s	O
2d	O
+	O
1	O
,	O
we	O
note	O
that	O
the	O
vc	B
dimension	I
is	O
not	O
more	O
than	O
that	O
of	O
the	O
classifiers	O
based	O
on	O
2d+l	O
k	O
l	O
l	O
ai	O
(	O
5	O
(	O
yj	O
-	O
ci	O
)	O
,	O
k==l	O
i=l	O
which	O
in	O
turn	O
is	O
not	O
more	O
than	O
that	O
of	O
the	O
classifiers	O
given	O
by	O
k	O
(	O
2d+1	O
)	O
l	O
bz	O
(	O
5	O
(	O
zz	O
-	O
dz	O
)	O
,	O
i=l	O
where	O
{	O
bt	O
}	O
,	O
{	O
dz	O
}	O
are	O
parameters	O
,	O
and	O
{	O
zz	O
}	O
is	O
an	O
input	O
sequence	O
.	O
by	O
theorem	B
13.9	O
,	O
the	O
vc	B
dimension	I
is	O
not	O
more	O
than	O
k	O
(	O
2d	O
+	O
1	O
)	O
.	O
this	O
concludes	O
the	O
proof	O
.	O
0	O
for	O
more	O
results	O
along	O
these	O
lines	O
,	O
we	O
refer	O
to	O
kurkova	O
(	O
1992	O
)	O
.	O
538	O
30.	O
neural	O
networks	O
30.9	O
projection	B
pursuit	I
in	O
projection	B
pursuit	I
(	O
friedman	O
and	O
tukey	O
(	O
1974	O
)	O
,	O
friedman	O
and	O
stuetzle	O
(	O
1981	O
)	O
,	O
friedman	O
,	O
stuetzle	O
,	O
and	O
schroeder	O
(	O
1984	O
)	O
,	O
huber	O
(	O
1985	O
)	O
,	O
hall	O
(	O
1989	O
)	O
,	O
flick	O
,	O
jones	O
,	O
priest	O
,	O
and	O
herman	O
(	O
1990	O
»	O
,	O
one	O
considers	O
functions	O
of	O
the	O
form	O
k	O
o/	O
(	O
x	O
)	O
=	O
l	O
o/j	O
(	O
bj	O
+	O
aj	O
x	O
)	O
,	O
j=l	O
(	O
30.3	O
)	O
where	O
b	O
j	O
e	O
r	O
,	O
a	O
j	O
e	O
rd	O
are	O
constants	O
and	O
0/1	O
,	O
...	O
,	O
o/k	O
are	O
fixed	O
functions	O
.	O
this	O
is	O
related	O
to	O
,	O
but	O
not	O
a	O
special	O
case	O
of	O
,	O
one-hidden-iayer	O
neural	O
networks	O
.	O
based	O
upon	O
the	O
kolmogorov-lorentz	O
representation	O
theorem	B
,	O
we	O
may	O
also	O
consider	O
d	O
o/	O
(	O
x	O
)	O
=	O
l	O
o/j	O
(	O
x	O
(	O
j	O
»	O
,	O
j=l	O
(	O
30.4	O
)	O
for	O
fixed	O
functions	O
o/j	O
(	O
friedman	O
and	O
silverman	O
(	O
1989	O
)	O
,	O
hastie	O
and	O
tibshirani	O
(	O
1990	O
»	O
.	O
in	O
(	O
30.4	O
)	O
and	O
(	O
30.3	O
)	O
,	O
the	O
o/j	O
's	O
may	O
be	O
approximated	O
in	O
tum	O
by	O
spline	O
functions	O
or	O
other	O
nonparametric	O
constructs	O
.	O
this	O
approach	O
is	O
covered	O
in	O
the	O
liter	O
(	O
cid:173	O
)	O
ature	O
on	O
generalized	B
additive	O
models	O
(	O
stone	O
(	O
1985	O
)	O
,	O
hastie	O
and	O
tibshirani	O
(	O
1990	O
»	O
.	O
the	O
class	O
of	O
functions	O
eat	O
x	O
,	O
a	O
e	O
r	O
d	O
,	O
satisfies	O
the	O
conditions	O
of	O
the	O
stone	O
(	O
cid:173	O
)	O
weierstrass	O
theorem	B
(	O
theorem	B
a.9	O
)	O
and	O
is	O
therefore	O
dense	O
in	O
the	O
loo	O
norm	O
on	O
c	O
[	O
a	O
,	O
b	O
]	O
d	O
for	O
any	O
a	O
,	O
b	O
e	O
rd	O
(	O
see	O
,	O
e.g.	O
,	O
diaconis	O
and	O
shahshahani	O
(	O
1984	O
»	O
.	O
as	O
a	O
corollary	O
,	O
we	O
note	O
that	O
the	O
same	O
denseness	B
result	O
applies	O
to	O
the	O
family	O
k	O
l	O
o/i	O
(	O
at	O
x	O
)	O
,	O
i=l	O
(	O
30.5	O
)	O
where	O
k	O
2	O
:	O
1	O
is	O
arbitrary	O
and	O
0/1	O
,	O
0/2	O
,	O
...	O
are	O
general	O
functions	O
.	O
the	O
latter	O
result	O
is	O
at	O
the	O
basis	O
of	O
projection	B
pursuit	I
methods	O
for	O
approximating	O
functions	O
,	O
where	O
one	O
tries	O
to	O
find	O
vectors	O
ai	O
and	O
functions	O
o/i	O
that	O
approximate	O
a	O
given	O
function	O
very	O
well	O
.	O
remark	O
.	O
in	O
some	O
cases	O
,	O
approximations	O
by	O
functions	O
as	O
in	O
(	O
30.5	O
)	O
may	O
be	O
exact	O
.	O
for	O
example	O
,	O
and	O
theorem	B
30.14	O
.	O
(	O
diaconis	O
and	O
shahshahani	O
(	O
1984	O
»	O
.	O
let	O
m	O
be	O
a	O
positive	O
in	O
(	O
cid:173	O
)	O
teger	O
.	O
there	O
are	O
(	O
m+~-l	O
)	O
distinct	O
vectors	O
a	O
j	O
e	O
rd	O
such	O
that	O
any	O
homogeneous	O
polynomial	B
f	O
of	O
order	O
m	O
can	O
be	O
written	O
as	O
30.9	O
projection	B
pursuit	I
539	O
c+	O
!	O
-l	O
)	O
f	O
(	O
x	O
)	O
=	O
l	O
cj	O
(	O
aj	O
x	O
)	O
m	O
j=l	O
for	O
some	O
real	O
numbers	O
c	O
j.	O
every	O
polynomial	B
of	O
order	O
mover	O
n	O
d	O
is	O
a	O
homogeneous	O
polynomial	B
of	O
order	O
m	O
over	O
n	O
d+	O
1	O
by	O
replacing	O
the	O
constant	O
1	O
by	O
a	O
component	O
x	O
(	O
d+	O
1	O
)	O
raised	O
to	O
an	O
appropriate	O
power	O
.	O
thus	O
,	O
any	O
polynomial	B
of	O
order	O
mover	O
nd	O
may	O
be	O
decomposed	O
exactly	O
by	O
(	O
m+d	O
)	O
f	O
(	O
x	O
)	O
=	O
t	O
c	O
j	O
(	O
a	O
j	O
x	O
+	O
b	O
j	O
)	O
m	O
j=l	O
for	O
some	O
real	O
numbers	O
b	O
j	O
,	O
c	O
j	O
and	O
vectors	O
a	O
j	O
e	O
nd	O
.	O
polynomials	O
may	O
thus	O
be	O
represented	O
exactly	O
in	O
the	O
form	O
~~=l	O
cfji	O
(	O
af	O
x	O
)	O
with	O
k	O
=	O
(	O
m~d	O
)	O
.	O
as	O
the	O
polynomials	O
are	O
dense	O
in	O
c	O
[	O
a	O
,	O
b	O
]	O
d	O
,	O
we	O
have	O
yet	O
another	O
proof	O
that	O
{	O
~~=l	O
cfji	O
(	O
af	O
x	O
)	O
}	O
is	O
dense	O
in	O
c	O
[	O
a	O
,	O
b	O
]	O
d.	O
see	O
logan	O
and	O
shepp	O
(	O
1975	O
)	O
or	O
logan	O
(	O
1975	O
)	O
for	O
other	O
proofs	O
.	O
the	O
previous	O
discussion	O
suggests	O
at	O
least	O
two	O
families	O
from	O
which	O
to	O
select	O
a	O
discriminant	O
function	O
.	O
as	O
usual	O
,	O
we	O
let	O
g	O
(	O
x	O
)	O
=	O
i	O
{	O
1/f	O
(	O
x	O
»	O
o	O
}	O
for	O
a	O
discriminant	O
function	O
1jj	O
.	O
here	O
1jj	O
could	O
be	O
picked	O
from	O
or	O
where	O
m	O
is	O
sufficiently	O
large	O
.	O
if	O
we	O
draw	O
1jj	O
by	O
minimizing	O
the	O
empirical	B
error	I
(	O
admittedly	O
at	O
a	O
tremendous	O
computational	O
cost	O
)	O
,	O
then	O
convergence	O
may	O
result	O
if	O
m	O
is	O
not	O
too	O
large	O
.	O
we	O
need	O
to	O
know	O
the	O
vc	B
dimension	I
of	O
the	O
classes	O
of	O
classifiers	O
corresponding	O
to	O
:	O
;	O
:	O
:	O
;	O
n	O
and	O
f~	O
.	O
note	O
that	O
fm	O
coincides	O
with	O
all	O
polynomials	O
of	O
order	O
m	O
and	O
each	O
such	O
polynomial	B
is	O
the	O
sum	O
of	O
at	O
most	O
(	O
m	O
+	O
l	O
)	O
d	O
monomials	O
.	O
if	O
we	O
invoke	O
lemma	O
30.2	O
and	O
theorem	B
12.6	O
,	O
then	O
we	O
get	O
theorem	B
30.15.	O
empirical	B
risk	I
minimization	I
to	O
determine	O
{	O
a	O
j	O
,	O
b	O
j	O
,	O
c	O
j	O
}	O
in	O
fm	O
leads	O
to	O
a	O
universally	O
consistent	O
classifier	O
provided	O
that	O
m	O
-+	O
00	O
and	O
m	O
=	O
o	O
(	O
n	O
lid	O
/	O
log	O
n	O
)	O
.	O
projection	B
pursuit	I
is	O
very	O
powerful	O
and	O
not	O
at	O
all	O
confined	O
to	O
our	O
limited	O
discus	O
(	O
cid:173	O
)	O
sion	O
above	O
.	O
in	O
particular	O
,	O
there	O
are	O
many	O
other	O
ways	O
of	O
constructing	O
good	O
consistent	O
classifiers	O
that	O
do	O
not	O
require	O
extensive	O
computations	O
such	O
as	O
empirical	B
error	I
min	O
(	O
cid:173	O
)	O
imization	O
.	O
540	O
30.	O
neural	O
networks	O
30.10	O
radial	B
basis	I
function	I
networks	O
we	O
may	O
perform	O
discrimination	O
based	O
upon	O
networks	O
with	O
functions	O
of	O
the	O
form	O
(	O
and	O
decision	O
g	O
(	O
x	O
)	O
=	O
i	O
{	O
1/f	O
(	O
x	O
»	O
o	O
}	O
)	O
,	O
where	O
k	O
is	O
an	O
integer	O
,	O
ai	O
,	O
...	O
,	O
ak	O
hi	O
,	O
...	O
,	O
hk	O
are	O
constants	O
,	O
xl	O
,	O
...	O
,	O
xk	O
e	O
rd	O
,	O
and	O
k	O
is	O
a	O
kernel	O
function	O
(	O
such	O
as	O
k	O
(	O
u	O
)	O
=	O
e-liul12	O
or	O
k	O
(	O
u	O
)	O
=	O
i/o	O
+	O
ilui1	O
2	O
)	O
)	O
.	O
in	O
this	O
form	O
,	O
1fj	O
covers	O
several	O
well-known	O
methodologies	O
:	O
0	O
)	O
the	O
kernel	B
rule	I
(	O
chapter	O
10	O
)	O
:	O
take	O
k	O
=	O
n	O
,	O
ai	O
=	O
2yi	O
-1	O
,	O
hi	O
=	O
h	O
,	O
xi	O
=	O
xi	O
.	O
with	O
this	O
choice	O
,	O
for	O
a	O
large	O
class	O
of	O
kernels	O
,	O
we	O
are	O
guaranteed	O
convergence	O
if	O
h	O
~	O
0	O
and	O
nh	O
d	O
~	O
00.	O
this	O
approach	O
is	O
attractive	O
as	O
no	O
difficult	O
optimization	O
problem	O
needs	O
to	O
be	O
solved	O
.	O
(	O
2	O
)	O
the	O
potential	O
function	O
method	O
.	O
in	O
bashkirov	O
,	O
braverman	O
,	O
and	O
muchnik	O
(	O
964	O
)	O
,	O
the	O
parameters	O
are	O
k	O
=	O
n	O
,	O
hi	O
=	O
h	O
,	O
xi	O
=	O
xi	O
.	O
the	O
weights	O
ai	O
are	O
picked	O
to	O
minimize	O
the	O
empirical	B
error	I
on	O
the	O
data	O
,	O
and	O
h	O
is	O
held	O
fixed	O
.	O
the	O
original	O
kernel	B
suggested	O
there	O
is	O
k	O
(	O
u	O
)	O
=	O
i/o	O
+	O
liui1	O
2	O
)	O
.	O
(	O
3	O
)	O
linear	O
discrimination	O
.	O
for	O
k	O
=	O
2	O
,	O
k	O
(	O
u	O
)	O
=	O
e-	O
liuil2	O
,	O
hi	O
==	O
h	O
,	O
al	O
=	O
1	O
,	O
a2	O
=	O
-1	O
,	O
the	O
set	O
{	O
x	O
:	O
1fj	O
(	O
x	O
)	O
>	O
o	O
}	O
is	O
a	O
linear	O
halfspace	O
.	O
this	O
,	O
of	O
course	O
,	O
is	O
not	O
uni	O
(	O
cid:173	O
)	O
versally	O
consistent	O
.	O
observe	O
that	O
the	O
separating	O
hyperplane	B
is	O
the	O
collection	O
of	O
all	O
points	O
x	O
at	O
equal	O
distance	B
from	O
xl	O
and	O
x2	O
.	O
by	O
varying	O
xl	O
and	O
x2	O
,	O
all	O
hyperplanes	O
may	O
be	O
obtained	O
.	O
(	O
4	O
)	O
radial	B
basis	I
function	I
(	O
rbf	O
)	O
neural	O
networks	O
(	O
e.g.	O
,	O
powell	O
(	O
987	O
)	O
,	O
broom	O
(	O
cid:173	O
)	O
head	O
and	O
lowe	O
(	O
988	O
)	O
,	O
moody	O
and	O
darken	O
(	O
989	O
)	O
,	O
poggio	O
and	O
girosi	O
(	O
990	O
)	O
,	O
xu	O
,	O
krzyzak	O
,	O
and	O
oja	O
(	O
993	O
)	O
,	O
xu	O
,	O
krzyzak	O
,	O
and	O
yuille	O
(	O
994	O
)	O
,	O
and	O
krzyzak	O
,	O
linder	O
,	O
and	O
lugosi	O
(	O
1993	O
)	O
)	O
.	O
an	O
even	O
more	O
general	O
function	O
1fj	O
is	O
usually	O
employed	O
here	O
:	O
k	O
1fj	O
(	O
x	O
)	O
=	O
l	O
ci	O
k	O
(	O
(	O
x	O
-	O
xd	O
t	O
ai	O
(	O
x	O
-	O
xi	O
)	O
)	O
+	O
co	O
,	O
i=l	O
where	O
the	O
a/s	O
are	O
tunable	O
d	O
x	O
d	O
matrices	O
.	O
(	O
5	O
)	O
sieve	O
methods	O
.	O
grenander	O
(	O
981	O
)	O
and	O
geman	O
and	O
hwang	O
(	O
982	O
)	O
advocate	O
the	O
use	O
of	O
maximum	O
likelihood	O
methods	O
to	O
find	O
suitable	O
values	O
for	O
the	O
tunable	O
parameters	O
in	O
1fj	O
(	O
for	O
k	O
,	O
k	O
fixed	O
beforehand	O
)	O
subject	O
to	O
certain	O
compactness	O
constraints	O
on	O
these	O
parameters	O
to	O
control	O
the	O
abundance	O
of	O
choices	O
one	O
may	O
have	O
.	O
if	O
we	O
were	O
to	O
use	O
empirical	B
error	I
minimization	O
,	O
we	O
would	O
find	O
,	O
if	O
k	O
:	O
:	O
:	O
n	O
,	O
that	O
all	O
data	O
points	O
can	O
be	O
correctly	O
classified	O
(	O
take	O
the	O
hi	O
's	O
small	O
enough	O
,	O
setai	O
=	O
2yi	O
-1	O
,	O
xi	O
=	O
xi	O
,	O
k	O
=	O
n	O
)	O
,	O
causing	O
overfitting	B
.	O
hence	O
,	O
k	O
must	O
be	O
smaller	O
than	O
n	O
if	O
parameters	O
are	O
picked	O
in	O
this	O
manner	O
.	O
practical	O
ways	O
of	O
choosing	O
the	O
parameters	O
are	O
discussed	O
by	O
kraaijveld	O
and	O
duin	O
(	O
1991	O
)	O
,	O
30.10	O
radial	B
basis	I
function	I
networks	O
541	O
and	O
chou	O
and	O
chen	O
(	O
1992	O
)	O
.	O
in	O
both	O
(	O
4	O
)	O
and	O
(	O
5	O
)	O
,	O
the	O
xi	O
's	O
may	O
be	O
thought	O
of	O
as	O
representative	O
prototypes	O
,	O
the	O
ai	O
's	O
as	O
weights	O
,	O
and	O
the	O
hi	O
's	O
as	O
the	O
radii	O
of	O
influence	O
.	O
as	O
a	O
rule	O
,	O
k	O
is	O
much	O
smaller	O
than	O
n	O
as	O
xl	O
,	O
...	O
,	O
xk	O
summarizes	O
the	O
information	O
present	O
at	O
the	O
data	O
.	O
to	O
design	O
a	O
consistent	O
rbf	O
neural	B
network	I
classifier	O
,	O
we	O
may	O
proceed	O
as	O
in	O
(	O
1	O
)	O
.	O
we	O
may	O
also	O
take	O
k	O
--	O
+	O
00	O
but	O
k	O
=	O
o	O
(	O
n	O
)	O
.	O
just	O
let	O
(	O
xl	O
,	O
...	O
,	O
xd	O
==	O
(	O
xl	O
,	O
...	O
,	O
xk	O
)	O
,	O
(	O
ai	O
,	O
...	O
,	O
ak	O
==	O
2yl	O
-	O
1	O
,	O
...	O
,	O
2yk	O
-	O
1	O
)	O
,	O
and	O
choose	O
ai	O
or	O
hi	O
to	O
minimize	O
a	O
given	O
error	O
criterion	O
based	O
upon	O
x	O
k+	O
i	O
,	O
...	O
,	O
x	O
n	O
,	O
such	O
as	O
a	O
ln	O
(	O
g	O
)	O
=	O
--	O
k	O
l	O
i	O
{	O
g	O
(	O
x	O
;	O
)	O
iy	O
;	O
}	O
,	O
n	O
1	O
n-	O
i=k+l	O
where	O
g	O
(	O
x	O
)	O
=	O
i	O
{	O
1f	O
;	O
(	O
x	O
»	O
o	O
}	O
,	O
and	O
1/1	O
is	O
as	O
in	O
(	O
1	O
)	O
.	O
this	O
is	O
nothing	O
but	O
data	O
splitting	O
(	O
chapter	O
22	O
)	O
.	O
convergence	O
conditions	O
are	O
described	O
in	O
theorem	O
22.1.	O
a	O
more	O
ambitious	O
person	O
might	O
try	O
empirical	B
risk	I
minimization	I
to	O
find	O
the	O
best	O
xl	O
,	O
...	O
,	O
xb	O
ai	O
,	O
''	O
''	O
ab	O
ai	O
,	O
...	O
,	O
ak	O
(	O
d	O
x	O
d	O
matrices	O
as	O
in	O
(	O
4	O
)	O
)	O
based	O
upon	O
1	O
n	O
-l	O
i	O
{	O
g	O
(	O
x	O
;	O
)	O
=	O
!	O
yd	O
'	O
n	O
i=i	O
if	O
k	O
--	O
+	O
00	O
,	O
the	O
class	O
of	O
rules	O
contains	O
a	O
consistent	O
subsequence	O
,	O
and	O
therefore	O
,	O
it	O
suffices	O
only	O
to	O
show	O
that	O
the	O
vc	B
dimension	I
is	O
o	O
(	O
n	O
/	O
log	O
n	O
)	O
.	O
this	O
is	O
a	O
difficult	O
task	O
and	O
some	O
kernels	O
yield	O
infinite	O
vc	B
dimension	I
,	O
even	O
if	O
d	O
=	O
i	O
and	O
k	O
is	O
very	O
small	O
(	O
see	O
chapter	O
25	O
)	O
.	O
however	O
,	O
there	O
is	O
a	O
simple	O
argument	O
if	O
k	O
=	O
ir	O
for	O
a	O
simple	O
set	O
r.	O
let	O
a	O
=	O
{	O
{	O
x	O
:	O
x	O
=	O
a	O
+	O
ay	O
,	O
y	O
e	O
r	O
}	O
:	O
a	O
end	O
,	O
a	O
a	O
d	O
x	O
d	O
matrix	O
}	O
.	O
if	O
r	O
is	O
a	O
sphere	O
,	O
then	O
a	O
is	O
the	O
class	O
of	O
all	O
ellipsoids	O
.	O
the	O
number	O
of	O
ways	O
of	O
shattering	O
a	O
set	O
{	O
x	O
1	O
,	O
...	O
,	O
xn	O
}	O
by	O
intersecting	O
with	O
members	O
from	O
a	O
is	O
not	O
more	O
than	O
nd	O
(	O
d+	O
l	O
)	O
/2+	O
1	O
(	O
see	O
theorem	B
13.9	O
,	O
problem	O
13.10	O
)	O
.	O
the	O
number	O
of	O
ways	O
of	O
shattering	O
a	O
set	O
{	O
xl	O
,	O
...	O
,	O
x	O
l1	O
}	O
by	O
intersecting	O
with	O
sets	O
of	O
the	O
form	O
is	O
not	O
more	O
than	O
the	O
product	B
of	O
all	O
ways	O
of	O
shattering	O
by	O
intersections	O
with	O
ri	O
,	O
with	O
r2	O
,	O
and	O
so	O
forth	O
,	O
that	O
is	O
,	O
nk	O
(	O
d	O
(	O
d+1	O
)	O
/2+1	O
)	O
.	O
the	O
logarithm	O
of	O
the	O
shatter	B
coefficient	I
is	O
o	O
(	O
n	O
)	O
if	O
k	O
=	O
o	O
(	O
n/	O
log	O
n	O
)	O
.	O
thus	O
,	O
by	O
corollary	O
12.1	O
,	O
we	O
have	O
theorem	B
30.16	O
.	O
(	O
krzyzak	O
,	O
linder	O
,	O
and	O
lugosi	O
(	O
1993	O
)	O
)	O
.	O
lfwe	O
take	O
k	O
--	O
+	O
00	O
,	O
k	O
=	O
o	O
(	O
n/	O
log	O
n	O
)	O
in	O
the	O
rbf	O
classifier	B
(	O
4	O
)	O
in	O
which	O
k	O
=	O
ir	O
,	O
r	O
being	O
the	O
unit	O
ball	O
of	O
542	O
30.	O
neural	O
networks	O
r	O
d	O
,	O
and	O
in	O
which	O
all	O
the	O
parameters	O
are	O
chosen	O
by	O
empirical	B
risk	I
minimization	I
,	O
then	O
e	O
{	O
l	O
n	O
}	O
-+	O
l	O
*	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
.	O
furthermore	O
,	O
ijqk	O
is	O
the	O
class	O
of	O
all	O
rbf	O
classifiers	O
with	O
k	O
prototypes	O
,	O
e	O
{	O
l	O
n	O
}	O
-	O
inf	O
l	O
(	O
g	O
)	O
.	O
:	O
:	O
:	O
;	O
16	O
geqk	O
k	O
(	O
d	O
(	O
d	O
+	O
1	O
)	O
/2	O
+	O
1	O
)	O
log	O
n	O
+	O
log	O
(	O
8e	O
)	O
.	O
2n	O
the	O
theorem	B
remains	O
valid	O
(	O
with	O
modified	O
constants	O
in	O
the	O
error	O
estimate	O
)	O
when	O
r	O
is	O
a	O
hyperrectangle	O
or	O
polytope	O
with	O
a	O
bounded	O
number	O
of	O
faces	O
.	O
however	O
,	O
for	O
more	O
general	O
k	O
,	O
the	O
vc	B
dimension	I
is	O
more	O
difficult	O
to	O
evaluate	O
.	O
for	O
general	O
kernels	O
,	O
consistent	O
rbf	O
classifiers	O
can	O
be	O
obtained	O
by	O
empirical	B
l	O
i	O
or	O
l2	O
error	O
minimization	O
(	O
problem	O
30.32	O
)	O
.	O
however	O
,	O
no	O
efficient	O
practical	O
algorithms	O
are	O
known	O
to	O
compute	O
the	O
minima	O
.	O
finally	O
,	O
as	O
suggested	O
by	O
chou	O
and	O
chen	O
(	O
1992	O
)	O
and	O
kraaijveld	O
and	O
duin	O
(	O
1991	O
)	O
,	O
it	O
is	O
a	O
good	O
idea	O
to	O
place	O
xl	O
,	O
...	O
,	O
xk	O
by	O
k-means	B
clustering	I
or	O
another	O
clustering	B
method	O
and	O
to	O
build	O
an	O
rbf	O
classifier	B
with	O
those	O
values	O
or	O
by	O
optimization	O
started	O
at	O
the	O
given	O
cluster	O
centers	O
.	O
problems	O
and	O
exercises	O
problem	O
30.1.	O
let	O
k	O
,	O
i	O
be	O
integers	O
,	O
with	O
d	O
:	O
:	O
:	O
k	O
<	O
n	O
,	O
and	O
i	O
:	O
:	O
:	O
(	O
~	O
)	O
.	O
assume	O
x	O
has	O
a	O
density	O
.	O
let	O
a	O
be	O
a	O
collection	O
of	O
hyperplanes	O
drawn	O
from	O
the	O
e	O
)	O
possible	O
hyperplanes	O
through	O
d	O
points	O
of	O
{	O
xi	O
,	O
...	O
,	O
x	O
k	O
}	O
,	O
and	O
let	O
ga	O
be	O
the	O
corresponding	O
natural	B
classifier	I
based	O
upon	O
the	O
arrangement	B
pea	O
)	O
.	O
take	O
i	O
such	O
collections	O
a	O
at	O
random	O
and	O
with	O
replacement	O
,	O
and	O
pick	O
the	O
best	O
a	O
by	O
minimizing	O
ln	O
(	O
ga	O
)	O
,	O
where	O
show	O
that	O
the	O
selected	O
classifier	B
is	O
consistent	O
if	O
i	O
--	O
+	O
00	O
,	O
ld	O
=	O
o	O
(	O
n	O
)	O
,	O
n	O
/	O
(	O
llog	O
k	O
)	O
--	O
+	O
00	O
.	O
(	O
note	O
:	O
this	O
is	O
applicable	O
with	O
k	O
=	O
l	O
n	O
/2	O
j	O
,	O
that	O
is	O
,	O
half	O
the	O
sample	O
is	O
used	O
to	O
define	O
a	O
,	O
and	O
the	O
other	O
half	O
is	O
used	O
to	O
pick	O
a	O
classifier	O
empilically	O
.	O
)	O
problem	O
30.2.	O
we	O
are	O
given	O
a	O
tree	O
classifier	B
with	O
k	O
internal	O
linear	O
splits	O
and	O
k	O
+	O
i	O
leaf	O
regions	O
(	O
a	O
bsp	O
tree	B
)	O
.	O
show	O
how	O
to	O
combine	O
the	O
neurons	O
in	O
a	O
two-hidden-iayer	O
perceptron	B
with	O
k	O
and	O
k	O
+	O
i	O
hidden	O
neurons	O
in	O
the	O
two	O
hidden	O
layers	O
so	O
as	O
to	O
obtain	O
a	O
decision	O
that	O
is	O
identical	O
to	O
the	O
tree-based	O
classifier	B
(	O
brent	O
(	O
1991	O
)	O
,	O
sethi	O
(	O
1990	O
;	O
1991	O
)	O
)	O
.	O
for	O
more	O
on	O
the	O
equivalence	O
of	O
decision	O
trees	O
and	O
neural	O
networks	O
,	O
see	O
meisel	O
(	O
1990	O
)	O
,	O
koutsougeras	O
and	O
papachristou	O
(	O
1989	O
)	O
,	O
or	O
golea	O
and	O
marchand	O
(	O
1990	O
)	O
.	O
hint	O
:	O
mimic	O
the	O
argument	O
for	O
arrangements	O
in	O
text	O
.	O
problem	O
30.3.	O
extend	O
the	O
proof	O
of	O
theorem	O
30.4	O
so	O
that	O
it	O
includes	O
any	O
nondecreasing	O
sigmoid	B
with	O
limx~_oo	O
o-	O
(	O
x	O
)	O
=	O
-1	O
and	O
limx~oo	O
o-	O
(	O
x	O
)	O
=	O
1.	O
hint	O
:	O
if	O
t	O
is	O
large	O
,	O
o-	O
(	O
tx	O
)	O
approx	O
(	O
cid:173	O
)	O
imates	O
the	O
threshold	B
sigmoid	O
.	O
problem	O
30.4.	O
this	O
exercise	O
states	O
denseness	B
in	O
l	O
i	O
(	O
fj.	O
,	O
)	O
for	O
any	O
probability	O
measure	B
fj.	O
,	O
.	O
show	O
that	O
for	O
every	O
probability	O
measure	B
fj.	O
,	O
on	O
r	O
d	O
,	O
every	O
measurable	O
function	O
f	O
:	O
rd	O
--	O
+	O
r	O
problems	O
and	O
exercises	O
543	O
with	O
j	O
1	O
!	O
(	O
x	O
)	O
i	O
,	O
u	O
(	O
dx	O
)	O
<	O
00	O
,	O
and	O
every	O
e	O
>	O
0	O
,	O
there	O
exists	O
a	O
neural	O
network	O
with	B
one	I
hidden	I
layer	I
and	O
function	O
1/j	O
(	O
x	O
)	O
as	O
in	O
(	O
30.2	O
)	O
such	O
that	O
f	O
i	O
!	O
(	O
x	O
}	O
-1/f	O
(	O
x	O
)	O
i	O
,	O
u	O
(	O
dx	O
)	O
<	O
e	O
(	O
hornik	O
(	O
1991	O
)	O
.	O
hint	O
:	O
proceed	O
as	O
in	O
theorem	O
30.4	O
,	O
considering	O
the	O
following	O
:	O
(	O
1	O
)	O
approximate	O
!	O
in	O
l	O
1	O
(	O
,	O
u	O
)	O
by	O
a	O
continuous	O
function	O
g	O
(	O
x	O
)	O
that	O
is	O
zero	O
outside	O
some	O
bounded	O
set	O
b	O
c	O
1	O
?	O
f	O
since	O
g	O
(	O
x	O
)	O
is	O
bounded	O
,	O
its	O
maximum	O
f3	O
=	O
maxxerd	O
ig	O
(	O
x	O
)	O
1	O
is	O
finite	O
.	O
(	O
2	O
)	O
now	O
,	O
choose	O
a	O
to	O
be	O
a	O
positive	O
number	O
large	O
enough	O
so	O
that	O
both	O
b	O
c	O
[	O
-a	O
,	O
a	O
]	O
d	O
and	O
,	O
u	O
(	O
[	O
-a	O
,	O
a	O
]	O
d	O
)	O
is	O
large	O
.	O
(	O
3	O
)	O
extend	O
the	O
restriction	O
of	O
g	O
(	O
x	O
)	O
to	O
[	O
-a	O
,	O
a	O
)	O
d	O
periodically	O
by	O
tiling	O
over	O
all	O
of	O
nd	O
.	O
the	O
obtained	O
function	O
,	O
i	O
(	O
x	O
)	O
is	O
a	O
good	O
approximation	O
of	O
g	O
(	O
x	O
)	O
in	O
l	O
j	O
(	O
,	O
u	O
)	O
.	O
(	O
4	O
)	O
take	O
the	O
fourier	O
series	O
approximation	O
of	O
i	O
(	O
x	O
)	O
,	O
and	O
use	O
the	O
stone-weierstrass	O
theorem	B
as	O
in	O
theorem	O
30.4	O
.	O
(	O
5	O
)	O
observe	O
that	O
every	O
continuous	O
function	O
on	O
the	O
real	O
line	O
that	O
is	O
zero	O
outside	O
some	O
bounded	O
interval	O
can	O
be	O
arbitrarily	O
closely	O
approximated	O
uniformly	O
over	O
the	O
whole	O
real	O
line	O
by	O
one-dimensional	O
neural	O
networks	O
.	O
thus	O
,	O
bounded	O
functions	O
such	O
as	O
the	O
sine	O
and	O
cosine	O
functions	O
can	O
be	O
approximated	O
arbitrarily	O
closely	O
by	O
neural	O
networks	O
in	O
l	O
1	O
(	O
j	O
)	O
)	O
for	O
any	O
probability	O
measure	B
j	O
)	O
on	O
n.	O
(	O
6	O
)	O
apply	O
the	O
triangle	O
inequality	B
to	O
finish	O
the	O
proof	O
.	O
problem	O
30.5.	O
generalize	O
the	O
previous	O
exercise	O
for	O
denseness	B
in	O
lp	O
(	O
,	O
u	O
)	O
.	O
more	O
precisely	O
,	O
let	O
1	O
:	O
:	O
:	O
:	O
p	O
<	O
00.	O
show	O
that	O
for	O
every	O
probability	O
measure	B
,	O
u	O
on	O
nd	O
,	O
every	O
measurable	O
function	O
!	O
:	O
nd	O
-+	O
nwithjl	O
!	O
(	O
x	O
)	O
ip	O
,	O
u	O
(	O
dx	O
)	O
<	O
oo	O
,	O
andeverye	O
>	O
o	O
,	O
thereexistsaneural	O
network	O
with	B
one	I
hidden	I
layer	I
h	O
(	O
x	O
)	O
such	O
that	O
(	O
1	O
i/	O
(	O
x	O
)	O
-	O
hex	O
)	O
!	O
,	O
/l	O
(	O
dx	O
»	O
)	O
lip	O
<	O
e	O
(	O
hornik	O
(	O
1991	O
)	O
)	O
.	O
problem	O
30.6.	O
committee	O
machines	O
.	O
let	O
c	O
(	O
k	O
)	O
be	O
the	O
class	O
of	O
all	O
committee	O
machines	O
.	O
prove	O
that	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
,	O
lim	O
inf	O
l	O
(	O
¢	O
)	O
=	O
l	O
*	O
.	O
k	O
--	O
-	O
'	O
>	O
oo	O
<	O
jjec	O
(	O
k	O
)	O
hint	O
:	O
for	O
a	O
one-hidden-layer	O
neural	B
network	I
with	O
coefficients	O
ci	O
in	O
(	O
30.2	O
)	O
,	O
approximate	O
the	O
ci	O
's	O
by	O
discretization	O
(	O
truncation	O
to	O
a	O
grid	O
of	O
values	O
)	O
,	O
and	O
note	O
that	O
ci	O
1/ji	O
(	O
x	O
)	O
may	O
thus	O
be	O
approximated	O
in	O
a	O
committee	B
machine	I
by	O
a	O
sufficient	O
number	O
of	O
identical	O
copies	O
of	O
1/ji	O
(	O
x	O
)	O
.	O
this	O
only	O
forces	O
the	O
number	O
of	O
neurons	O
to	O
be	O
a	O
bit	O
larger	O
.	O
problem	O
30.7.	O
prove	O
theorem	B
30.5	O
for	O
arbitrary	O
sigmoids	O
.	O
hint	O
:	O
approximate	O
the	O
thresh	O
(	O
cid:173	O
)	O
old	O
sigmoid	B
by	O
a	O
(	O
t	O
x	O
)	O
for	O
a	O
sufficiently	O
large	O
t.	O
problem	O
30.8.	O
let	O
a	O
be	O
a	O
nondecreasing	O
sigmoid	B
with	O
a	O
(	O
x	O
)	O
-+	O
-1	O
if	O
x	O
-+	O
-00	O
and	O
a	O
(	O
x	O
)	O
-+	O
1	O
if	O
x	O
-+	O
00.	O
denote	O
by	O
c~k	O
)	O
the	O
class	O
of	O
corresponding	O
neural	B
network	I
classifiers	I
with	O
k	O
hidden	O
layers	O
.	O
show	O
that	O
vc~k	O
)	O
~	O
vc	O
(	O
k	O
)	O
,	O
where	O
c	O
(	O
k	O
)	O
is	O
the	O
class	O
corresponding	O
to	O
the	O
threshold	B
sigmoid	O
.	O
544	O
30.	O
neural	O
networks	O
problem	O
30.9.	O
this	O
exercise	O
generalizes	O
theorem	B
30.5	O
for	O
not	O
fully	O
connected	O
neural	O
net	O
(	O
cid:173	O
)	O
works	O
with	B
one	I
hidden	I
layer	I
.	O
consider	O
the	O
class	O
c	O
(	O
k	O
)	O
of	O
one-hidden-iayer	O
neural	O
networks	O
with	O
the	O
threshold	B
sigmoid	O
such	O
that	O
each	O
of	O
the	O
k	O
nodes	O
in	O
the	O
hidden	O
layer	O
are	O
connected	O
to	O
d	O
]	O
,	O
d2	O
,	O
•••	O
,	O
dk	O
inputs	O
,	O
where	O
1	O
:	O
:	O
:	O
:	O
di	O
:	O
:	O
:	O
:	O
d.	O
more	O
precisely	O
,	O
c	O
(	O
k	O
)	O
contains	O
all	O
classifiers	O
based	O
on	O
functions	O
of	O
the	O
form	O
1jf	O
(	O
x	O
)	O
=	O
co	O
+	O
l	O
cio-	O
(	O
1jfi	O
(	O
x	O
»	O
,	O
where	O
1jfi	O
(	O
x	O
)	O
=	O
l	O
ajx	O
(	O
i1l	O
;	O
,	O
j	O
)	O
,	O
d	O
;	O
k	O
i=	O
]	O
j=1	O
where	O
for	O
each	O
i	O
,	O
(	O
mi	O
,	O
]	O
,	O
...	O
,	O
mi	O
,	O
d	O
)	O
is	O
a	O
vector	O
of	O
distinct	O
positive	O
integers	O
not	O
exceeding	O
d.	O
show	O
that	O
if	O
the	O
a/s	O
,	O
c/s	O
and	O
mi	O
,	O
/s	O
are	O
the	O
tunable	O
parameters	O
(	O
bartlett	O
(	O
1993	O
»	O
.	O
i=	O
]	O
problem	O
30.10.	O
let	O
0-	O
be	O
a	O
sigmoid	O
that	O
takes	O
m	O
different	O
values	O
.	O
find	O
upper	O
bounds	O
on	O
the	O
vc	B
dimension	I
of	O
the	O
class	O
c~k	O
)	O
.	O
problem	O
30.11.	O
consider	O
a	O
one-hidden-iayer	O
neural	B
network	I
1jf	O
.	O
if	O
(	O
x	O
]	O
,	O
y	O
]	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
are	O
fixed	O
and	O
all	O
xi	O
's	O
are	O
different	O
,	O
show	O
that	O
with	O
n	O
hidden	O
neurons	O
,	O
we	O
are	O
always	O
able	O
to	O
tune	O
the	O
weights	O
such	O
that	O
yi	O
=	O
1jf	O
(	O
xi	O
)	O
for	O
all	O
i	O
.	O
(	O
this	O
remains	O
true	O
if	O
yen	O
instead	O
of	O
y	O
e	O
{	O
o	O
,	O
i	O
}	O
.	O
)	O
the	O
property	O
above	O
describes	O
a	O
situation	O
of	O
overfitting	O
that	O
occurs	O
when	O
the	O
neural	B
network	I
becomes	O
too	O
``	O
rich	O
''	O
-recall	O
also	O
that	O
the	O
vc	B
dimension	I
,	O
which	O
is	O
at	O
least	O
d	O
times	O
the	O
number	O
of	O
hidden	O
neurons	O
,	O
must	O
remain	O
smaller	O
than	O
n	O
for	O
any	O
meaningful	O
training	O
.	O
problem	O
30.12.	O
the	O
bernstein	O
perceptron	B
.	O
consider	O
the	O
following	O
perceptron	B
for	O
one	O
(	O
cid:173	O
)	O
dimensional	O
data	O
:	O
¢	O
(	O
x	O
)	O
=	O
{	O
~	O
if	O
l7=	O
]	O
aixi	O
(	O
1	O
-	O
x	O
)	O
k-i	O
>	O
1/2	O
otherwise	O
.	O
let	O
us	O
call	O
this	O
the	O
bernstein	O
perceptron	B
since	O
it	O
involves	O
bernstein	O
polynomials	O
.	O
if	O
n	O
data	O
points	O
are	O
collected	O
,	O
how	O
would	O
you	O
choose	O
k	O
(	O
as	O
a	O
function	O
of	O
n	O
)	O
and	O
how	O
would	O
you	O
adjust	O
the	O
weights	O
(	O
the	O
ai	O
's	O
)	O
to	O
make	O
sure	O
that	O
the	O
bernstein	O
perceptron	B
is	O
consistent	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
with	O
pix	O
e	O
[	O
0	O
,	O
in	O
=	O
i	O
?	O
can	O
you	O
make	O
the	O
bernstein	O
perceptron	B
consistent	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
on	O
n	O
x	O
{	O
o	O
,	O
i	O
}	O
?	O
figure	O
30.15.	O
the	O
bernstein	O
per	O
(	O
cid:173	O
)	O
ceptron	O
.	O
problem	O
30.13.	O
use	O
the	O
ideas	O
of	O
section	O
25.5	O
and	O
problem	O
25.11	O
to	O
prove	O
theorem	B
30.8	O
.	O
545	O
problem	O
30.14.	O
consider	O
specht	O
's	O
padaline	O
with	O
r	O
=	O
rn	O
t	O
00.	O
let	O
h	O
=	O
hll	O
+	O
0	O
,	O
and	O
nhd	O
--	O
+	O
00.	O
show	O
that	O
for	O
any	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
e	O
{	O
l	O
,	O
j	O
--	O
+	O
l	O
*	O
.	O
problems	O
and	O
exercises	O
problem	O
30.15.	O
bounded	O
first	O
layers	O
.	O
consider	O
a	O
feed-forward	O
neural	B
network	I
with	O
any	O
number	O
of	O
layers	O
,	O
with	O
only	O
one	O
restriction	O
,	O
that	O
is	O
,	O
the	O
first	O
layer	O
has	O
at	O
most	O
k	O
<	O
d	O
outputs	O
z1	O
,	O
...	O
,	O
zk	O
,	O
where	O
x	O
=	O
(	O
x	O
(	O
l	O
)	O
,	O
...	O
,	O
x	O
(	O
d	O
)	O
)	O
is	O
the	O
input	O
,	O
and	O
(	O
j	O
'	O
is	O
an	O
arbitrary	O
function	O
n	O
--	O
+	O
n	O
(	O
not	O
just	O
a	O
sigmoid	O
)	O
.	O
the	O
integer	O
k	O
remains	O
fixed	O
.	O
let	O
a	O
denote	O
the	O
k	O
x	O
(	O
d	O
+	O
1	O
)	O
matrix	O
of	O
weights	O
aji	O
,	O
bj	O
.	O
(	O
1	O
)	O
if	O
l	O
*	O
(	O
a	O
)	O
is	O
the	O
bayes	O
error	O
for	O
a	O
recognition	O
problem	O
based	O
upon	O
(	O
z	O
,	O
y	O
)	O
,	O
with	O
z	O
=	O
(	O
z1	O
'	O
...	O
,	O
zk	O
)	O
and	O
then	O
show	O
that	O
for	O
some	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
,	O
inf	O
a	O
l	O
*	O
(	O
a	O
)	O
>	O
l	O
*	O
,	O
where	O
l	O
*	O
is	O
the	O
bayes	O
probability	O
of	O
error	O
for	O
(	O
x	O
,	O
y	O
)	O
.	O
if	O
k	O
~	O
d	O
however	O
,	O
show	O
that	O
for	O
any	O
strictly	O
monotonically	O
increasing	O
sigmoid	B
(	O
j	O
'	O
,	O
inf	O
a	O
l	O
*	O
(	O
a	O
)	O
=	O
l	O
*	O
.	O
(	O
2	O
)	O
(	O
3	O
)	O
use	O
(	O
1	O
)	O
to	O
conclude	O
that	O
any	O
neural	B
network	I
based	O
upon	O
a	O
first	O
layer	O
with	O
k	O
<	O
d	O
outputs	O
is	O
not	O
consistent	O
for	O
some	O
distribution	B
,	O
regardless	O
of	O
how	O
many	O
layers	O
it	O
has	O
(	O
note	O
however	O
,	O
that	O
the	O
inputs	O
of	O
each	O
layer	O
are	O
restricted	O
to	O
be	O
the	O
outputs	O
of	O
the	O
previous	O
layer	O
)	O
.	O
figure	O
30.16.	O
the	O
first	O
layer	O
is	O
re	O
(	O
cid:173	O
)	O
stricted	O
to	O
have	O
k	O
outputs	O
.	O
it	O
has	O
ked	O
+	O
1	O
)	O
tunable	O
parameters	O
.	O
zj	O
problem	O
30.16.	O
find	O
conditions	O
on	O
kll	O
and	O
(	O
311	O
that	O
guarantee	O
strong	B
universal	I
consistency	I
in	O
theorem	B
30.9.	O
problem	O
30.17.	O
barron	O
networks	O
.	O
call	O
a	O
barron	O
network	O
a	O
network	O
of	O
any	O
number	O
of	O
layers	O
(	O
as	O
in	O
figure	O
30.13	O
)	O
with	O
2	O
inputs	O
per	O
cell	O
and	O
cells	O
that	O
perform	O
the	O
operation	O
a	O
+	O
(	O
3x	O
+	O
yy	O
+	O
8xy	O
on	O
inputs	O
x	O
,	O
yen	O
,	O
with	O
trainable	O
weights	O
a	O
,	O
(	O
3	O
,	O
y	O
,	O
8.	O
if	O
1jj	O
is	O
the	O
output	O
546	O
30.	O
neural	O
networks	O
of	O
a	O
network	O
with	O
d	O
inputs	O
and	O
k	O
cells	O
(	O
arranged	O
in	O
any	O
way	O
)	O
,	O
compute	O
an	O
upper	O
bound	O
for	O
the	O
vc	B
dimension	I
of	O
the	O
classifier	B
g	O
(	O
x	O
)	O
=	O
i	O
{	O
1/f	O
(	O
x	O
»	O
o	O
}	O
,	O
as	O
a	O
function	O
of	O
d	O
and	O
k.	O
note	O
that	O
the	O
structure	O
of	O
the	O
network	O
(	O
i.e.	O
,	O
the	O
positions	O
of	O
the	O
cells	O
and	O
the	O
connections	O
)	O
is	O
variable	B
.	O
problem	O
30.18.	O
continued	O
.	O
restrict	O
the	O
balton	O
network	O
to	O
llayers	O
and	O
k	O
cells	O
per	O
layer	O
(	O
for	O
kl	O
cells	O
total	O
)	O
,	O
and	O
repeat	O
the	O
previous	O
exercise	O
.	O
problem	O
30.19.	O
continued	O
.	O
find	O
conditions	O
on	O
k	O
and	O
l	O
in	O
the	O
previous	O
exercises	O
that	O
would	O
guarantee	O
the	O
universal	B
consistency	I
of	O
the	O
barron	O
network	O
,	O
if	O
we	O
train	O
to	O
minimize	O
the	O
empirical	B
eltor	O
.	O
problem	O
30.20.	O
consider	O
the	O
family	B
of	I
functions	O
:	O
fit	O
of	O
the	O
form	O
(	O
il	O
)	O
)	O
]	O
i	O
l	O
l	O
wij	O
(	O
1h	O
(	O
x	O
)	O
)	O
j	O
,	O
.1	O
,	O
(	O
'f'i	O
x	O
)	O
_	O
``	O
``	O
i	O
-	O
~~wii'j	O
'	O
x	O
k	O
d	O
i	O
./	O
1	O
:	O
s	O
i	O
:	O
s	O
k	O
,	O
i=1	O
j=1	O
i'=1	O
j'=	O
!	O
where	O
all	O
the	O
wi	O
}	O
's	O
and	O
w~il	O
j	O
'	O
's	O
are	O
tunable	O
parameters	O
.	O
show	O
that	O
for	O
every	O
i	O
e	O
c	O
[	O
o	O
,	O
l	O
]	O
d	O
and	O
e	O
>	O
0	O
,	O
there	O
exist	O
k	O
,	O
llarge	O
enough	O
so	O
that	O
for	O
some	O
g	O
e	O
:	O
fk	O
,	O
/	O
,	O
supxe	O
[	O
o	O
,	O
ljd	O
i/	O
(	O
x	O
)	O
-	O
g	O
(	O
x	O
)	O
1	O
:	O
s	O
e.	O
problem	O
30.21.	O
continued	O
.	O
obtain	O
an	O
upper	O
bound	O
on	O
the	O
vc	B
dimension	I
of	O
the	O
above	O
two-hidden-iayer	O
network	O
.	O
hint	O
:	O
the	O
vc	B
dimension	I
is	O
usually	O
about	O
equal	O
to	O
the	O
number	O
of	O
degrees	O
of	O
freedom	O
(	O
which	O
is	O
(	O
1	O
+	O
d	O
)	O
lk	O
here	O
)	O
.	O
problem	O
30.22.	O
continued	O
.	O
if	O
gn	O
is	O
the	O
rule	B
obtained	O
by	O
empirical	B
error	I
minimization	O
over	O
:	O
fk	O
,	O
z	O
,	O
then	O
show	O
that	O
l	O
(	O
gn	O
)	O
-	O
»	O
l	O
*	O
in	O
probability	O
if	O
k	O
-	O
»	O
00	O
,	O
l	O
-	O
»	O
00	O
,	O
and	O
kl	O
=	O
o	O
(	O
nj	O
log	O
n	O
)	O
.	O
problem	O
30.23.	O
how	O
many	O
different	O
monomials	O
of	O
order	O
r	O
in	O
x	O
(	O
l	O
)	O
,	O
...	O
,	O
xed	O
)	O
are	O
there	O
?	O
how	O
does	O
this	O
grow	O
with	O
r	O
when	O
d	O
is	O
held	O
fixed	O
?	O
problem	O
30.24.	O
show	O
that	O
1/11,1/12	O
,	O
and	O
¢	O
in	O
the	O
bit-interleaving	O
example	O
in	O
the	O
section	O
on	O
kolmogorov-lorentz	O
representations	O
are	O
not	O
continuous	O
.	O
which	O
are	O
the	O
points	O
of	O
disconti	O
(	O
cid:173	O
)	O
nuity	O
?	O
problem	O
30.25.	O
let	O
pick	O
{	O
ai	O
,	O
c	O
j	O
}	O
by	O
empirical	B
risk	I
minimization	I
for	O
the	O
classifier	B
g	O
(	O
x	O
)	O
=	O
i	O
{	O
1/f	O
(	O
x	O
»	O
o	O
}	O
,	O
1/1	O
e	O
:	O
f~l	O
'	O
showthate	O
{	O
ln	O
}	O
-	O
»	O
l*	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
whenm	O
-	O
»	O
ooandm	O
=	O
o	O
(	O
n	O
1	O
/	O
d	O
jlogn	O
)	O
.	O
problem	O
30.26.	O
write	O
(	O
x	O
(	O
l	O
)	O
x	O
(	O
2	O
)	O
)	O
2	O
as	O
2	O
:	O
,	O
;	O
=1	O
(	O
ailx	O
(	O
l	O
)	O
+	O
ai2x	O
(	O
2	O
)	O
)	O
2	O
and	O
identify	O
the	O
coefficients	O
{	O
ail	O
,	O
ai2	O
}	O
.	O
show	O
that	O
there	O
is	O
an	O
entire	O
subspace	O
of	O
solutions	O
(	O
diaconis	O
and	O
shahshahani	O
(	O
1984	O
)	O
)	O
.	O
problem	O
30.27.	O
show	O
that	O
ex	O
(	O
1	O
)	O
x	O
(	O
2	O
)	O
and	O
sin	O
(	O
x	O
(	O
1	O
)	O
x	O
(	O
2	O
)	O
can	O
not	O
be	O
written	O
in	O
the	O
form	O
2	O
:	O
,	O
:	O
=1	O
1/ii	O
(	O
a	O
!	O
x	O
)	O
for	O
any	O
finite	O
k	O
,	O
where	O
x	O
=	O
(	O
x	O
(	O
l	O
)	O
,	O
x	O
(	O
2	O
)	O
)	O
and	O
ai	O
e	O
r2	O
,	O
1	O
:	O
s	O
i	O
:	O
s	O
k	O
(	O
diaconis	O
and	O
shahshahani	O
(	O
1984	O
)	O
)	O
.	O
thus	O
,	O
the	O
projection	B
pursuit	I
representation	O
of	O
functions	O
can	O
only	O
at	O
best	O
approximate	O
all	O
continuous	O
functions	O
on	O
bounded	O
sets	O
.	O
problem	O
30.28.	O
let	O
:	O
f	O
be	O
the	O
class	O
of	O
classifiers	O
of	O
the	O
form	O
g	O
=	O
i	O
{	O
1/f	O
>	O
o	O
}	O
,	O
where	O
1/i	O
(	O
x	O
)	O
=	O
al	O
ii	O
(	O
x	O
(	O
l	O
)	O
)	O
+	O
a2	O
12	O
(	O
x	O
(	O
2	O
»	O
)	O
,	O
for	O
arbitrary	O
functions	O
11	O
,	O
12	O
,	O
and	O
coefficients	O
a1	O
,	O
a2	O
e	O
r.	O
show	O
that	O
for	O
some	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
on	O
r2	O
x	O
{	O
o	O
,	O
1	O
}	O
,	O
infge.f	O
l	O
(	O
g	O
)	O
>	O
l	O
*	O
,	O
so	O
that	O
there	O
is	O
no	O
hope	O
of	O
meaningful	O
distribution-free	O
classification	O
based	O
on	O
additive	O
functions	O
only	O
.	O
problems	O
and	O
exercises	O
547	O
problem	O
30.29.	O
continued	O
.	O
repeat	O
the	O
previous	O
exercise	O
for	O
functions	O
of	O
the	O
form	O
1jr	O
(	O
x	O
)	O
=	O
ai	O
11	O
(	O
x	O
(	O
2	O
)	O
,	O
x	O
(	O
3	O
»	O
)	O
+	O
a2fz	O
(	O
x	O
(	O
1	O
)	O
,	O
x	O
(	O
3	O
»	O
)	O
+a3h	O
(	O
x	O
(	O
l	O
)	O
,	O
x	O
(	O
2	O
»	O
)	O
and	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
on	O
r3	O
x	O
{	O
o	O
,	O
l	O
}	O
.	O
thus	O
,	O
additive	O
functions	O
of	O
pairs	O
do	O
not	O
suffice	O
either	O
.	O
problem	O
30.30.	O
let	O
k	O
:	O
r	O
--	O
+	O
r	O
be	O
a	O
nonnegative	O
bounded	O
kernel	B
with	O
f	O
k	O
(	O
x	O
)	O
dx	O
<	O
00.	O
show	O
that	O
for	O
any	O
e	O
>	O
0	O
,	O
any	O
measurable	O
function	O
i	O
:	O
rd	O
--	O
+	O
r	O
and	O
probability	O
measure	B
fj-	O
such	O
that	O
f	O
ii	O
(	O
x	O
)	O
ifj-	O
(	O
dx	O
)	O
<	O
00	O
,	O
there	O
exists	O
a	O
function	O
1jr	O
of	O
the	O
form	O
1jr	O
(	O
x	O
)	O
=	O
:	O
l	O
;	O
=i	O
ci	O
k	O
(	O
x	O
-	O
bi	O
)	O
t	O
ai	O
(	O
x	O
-	O
bi	O
)	O
)	O
+	O
co	O
such	O
that	O
f	O
ii	O
(	O
x	O
)	O
-	O
1jr	O
(	O
x	O
)	O
fj-	O
(	O
dx	O
)	O
<	O
e	O
(	O
see	O
poggio	O
and	O
girosi	O
(	O
1990	O
)	O
,	O
park	O
and	O
sandberg	O
(	O
1991	O
;	O
1993	O
)	O
,	O
darken	O
,	O
donahue	O
,	O
gurvits	O
,	O
and	O
sontag	O
(	O
1993	O
)	O
,	O
krzyzak	O
,	O
linder	O
,	O
and	O
lugosi	O
(	O
1993	O
)	O
for	O
such	O
denseness	B
results	O
)	O
.	O
hint	O
:	O
relate	O
the	O
problem	O
to	O
a	O
similar	O
result	O
for	O
kernel	B
estimates	O
.	O
problem	O
30.31.	O
let	O
c	O
(	O
k	O
)	O
be	O
the	O
class	O
of	O
classifiers	O
defined	O
by	O
the	O
functions	O
k	O
l	O
ci	O
k	O
(	O
x	O
-	O
bil	O
ai	O
(	O
x	O
-	O
bi	O
)	O
)	O
+	O
co·	O
i=l	O
find	O
upper	O
bounds	O
on	O
its	O
vc	B
dimension	I
when	O
k	O
is	O
an	O
indicator	O
of	O
an	O
interval	O
containing	O
the	O
origin	O
.	O
problem	O
30.32.	O
consider	O
the	O
class	O
of	O
radial	B
basis	I
function	I
networks	O
where	O
k	O
is	O
nonnegative	O
,	O
unimodal	O
,	O
bounded	O
,	O
and	O
continuous	O
.	O
let	O
vr	O
n	O
be	O
a	O
function	O
that	O
minimizes	O
i	O
n	O
(	O
1jr	O
)	O
=	O
n-	O
i	O
:	O
l7=1	O
11jr	O
(	O
xi	O
)	O
-	O
yi	O
lover	O
1jr	O
e	O
f	O
n	O
,	O
and	O
define	O
gn	O
as	O
the	O
corresponding	O
classifier	B
.	O
prove	O
that	O
if	O
kn	O
--	O
+	O
00	O
,	O
fjn	O
--	O
+	O
00	O
,	O
and	O
knfj	O
;	O
;	O
10g	O
(	O
knfjn	O
)	O
=	O
o	O
(	O
n	O
)	O
as	O
n	O
--	O
+	O
00	O
,	O
then	O
gn	O
is	O
universally	O
consistent	O
(	O
krzyzak	O
,	O
linder	O
,	O
and	O
lugosi	O
(	O
1993	O
)	O
)	O
.	O
hint	O
:	O
proceed	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
30.9.	O
use	O
problem	O
30.30	O
to	O
handle	O
the	O
approximation	B
error	I
.	O
bounding	O
the	O
covering	B
numbers	O
needs	O
a	O
little	O
additional	O
work	O
.	O
31	O
other	O
error	O
estimates	O
in	O
this	O
chapter	O
we	O
discuss	O
some	O
alternative	O
error	O
estimates	O
that	O
have	O
been	O
in	O
(	O
cid:173	O
)	O
troduced	O
to	O
improve	O
on	O
the	O
performance	O
of	O
the	O
standard	B
estimates-holdout	O
,	O
re	O
(	O
cid:173	O
)	O
substitution	O
,	O
and	O
deleted-we	O
have	O
encountered	O
so	O
far	O
.	O
the	O
first	O
group	O
of	O
these	O
estimates-smoothed	O
and	O
posterior	B
probability	I
estimates-are	O
used	O
for	O
their	O
small	O
variance	O
.	O
however	O
,	O
we	O
will	O
give	O
examples	O
that	O
show	O
that	O
classifier	B
selection	I
based	O
on	O
the	O
minimization	O
of	O
these	O
estimates	O
may	O
fail	O
even	O
in	O
the	O
simplest	O
situations	O
.	O
among	O
other	O
alternatives	O
,	O
we	O
deal	O
briefly	O
with	O
the	O
rich	O
class	O
of	O
bootstrap	B
estimates	O
.	O
31.1	O
smoothing	O
the	O
error	O
count	O
the	O
resubstitution	B
,	O
deleted	B
,	O
and	O
holdout	B
estimates	O
of	O
the	O
error	O
probability	O
(	O
see	O
chapters	O
22	O
and	O
23	O
)	O
are	O
all	O
based	O
on	O
counting	O
the	O
number	O
of	O
errors	O
committed	O
by	O
the	O
classifier	B
to	O
be	O
tested	O
.	O
this	O
is	O
the	O
reason	O
for	O
the	O
relatively	O
large	O
variance	O
inherently	O
present	O
in	O
these	O
estimates	O
.	O
this	O
intuition	O
is	O
based	O
on	O
the	O
following	O
.	O
most	O
classification	O
rules	O
can	O
be	O
written	O
into	O
the	O
form	O
(	O
x	O
)	O
=	O
{	O
o	O
gn	O
if7jn	O
(	O
x	O
,	O
dn	O
)	O
:	O
:	O
:	O
:	O
1/2	O
1	O
otherwise	O
,	O
where	O
7jn	O
(	O
x	O
,	O
dn	O
)	O
is	O
either	O
an	O
estimate	B
of	O
the	O
a	B
posteriori	I
probability	I
7j	O
(	O
x	O
)	O
,	O
as	O
in	O
the	O
case	O
of	O
histogram	O
,	O
kernel	B
,	O
or	O
nearest	B
neighbor	I
rules	I
;	O
or	O
something	O
else	O
,	O
as	O
for	O
generalized	O
linear	O
,	O
or	O
neural	B
network	I
classifiers	I
.	O
in	O
any	O
case	O
,	O
if	O
7jn	O
(	O
x	O
,	O
dn	O
)	O
is	O
close	O
to	O
1/2	O
,	O
then	O
we	O
feel	O
that	O
the	O
decision	O
is	O
less	O
robust	O
compared	O
to	O
when	O
the	O
value	O
of	O
7jn	O
(	O
x	O
,	O
dn	O
)	O
is	O
far	O
from	O
1/2	O
.	O
in	O
other	O
words	O
,	O
intuitively	O
,	O
inverting	O
the	O
value	O
of	O
the	O
550	O
31.	O
other	O
error	O
estimates	O
decision	O
gn	O
(	O
x	O
)	O
at	O
a	O
point	O
x	O
,	O
where	O
17n	O
(	O
x	O
,	O
dn	O
)	O
is	O
close	O
to	O
1/2	O
makes	O
less	O
difference	O
in	O
the	O
eltor	O
probability	O
,	O
than	O
if	O
l'7n	O
(	O
x	O
,	O
dn	O
)	O
-	O
1/21	O
is	O
large	O
.	O
the	O
eltor	O
estimators	O
based	O
on	O
counting	O
the	O
number	O
of	O
eltors	O
do	O
not	O
take	O
the	O
value	O
of	O
'7n	O
(	O
x	O
)	O
into	O
account	O
:	O
they	O
``	O
penalize	O
''	O
eltofs	O
with	O
the	O
same	O
amount	O
,	O
no	O
matter	O
what	O
the	O
value	O
of	O
'7n	O
is	O
.	O
for	O
example	O
,	O
in	O
the	O
case	O
of	O
the	O
resubstitution	B
estimate	O
each	O
eltor	O
contributes	O
with	O
1/	O
n	O
to	O
the	O
overall	O
count	O
.	O
now	O
,	O
if	O
'7n	O
(	O
xi	O
,	O
dn	O
)	O
is	O
close	O
to	O
1/2	O
,	O
a	O
small	O
perturbation	O
of	O
xi	O
can	O
flip	O
the	O
decision	O
gn	O
(	O
xi	O
)	O
,	O
and	O
therefore	O
change	O
the	O
value	O
of	O
the	O
estimate	B
by	O
1/	O
n	O
,	O
although	O
the	O
eltor	O
probability	O
of	O
the	O
rule	B
gn	O
probably	O
does	O
not	O
change	O
by	O
this	O
much	O
.	O
this	O
phenomenon	O
is	O
what	O
causes	O
the	O
relatively	O
large	O
variance	O
of	O
eltor	O
counting	O
estimators	O
.	O
glick	O
(	O
978	O
)	O
proposed	O
a	O
modification	O
of	O
the	O
counting	O
eltor	O
estimates	O
.	O
the	O
gen	O
(	O
cid:173	O
)	O
eral	O
form	O
of	O
his	O
``	O
smoothed	B
''	O
estimate	B
is	O
where	O
r	O
is	O
a	O
monotone	O
increasing	O
function	O
satisfying	O
r	O
(	O
1	O
/2	O
-	O
u	O
)	O
=	O
1	O
-	O
r	O
(	O
1	O
/2	O
+	O
u	O
)	O
.	O
possible	O
choices	O
of	O
the	O
smoothing	O
function	O
r	O
(	O
u	O
)	O
are	O
r	O
(	O
u	O
)	O
=	O
u	O
,	O
or	O
r	O
(	O
u	O
)	O
=	O
i/o	O
+	O
e1/	O
2-	O
cu	O
)	O
,	O
where	O
the	O
parameter	O
c	O
>	O
0	O
may	O
be	O
adjusted	O
to	O
improve	O
the	O
behavior	O
of	O
the	O
estimate	B
(	O
see	O
also	O
glick	O
(	O
978	O
)	O
,	O
knoke	O
(	O
1986	O
)	O
,	O
or	O
tutz	O
(	O
1985	O
)	O
)	O
.	O
both	O
of	O
these	O
estimates	O
give	O
less	O
penalty	O
to	O
eltofs	O
close	O
to	O
the	O
decision	O
boundary	O
,	O
that	O
is	O
,	O
to	O
eltors	O
where	O
'7n	O
is	O
close	O
to	O
1/2	O
.	O
note	O
that	O
taking	O
r	O
(	O
u	O
)	O
=	O
i	O
{	O
u2	O
:	O
:1/2	O
}	O
coltesponds	O
to	O
the	O
resubstitution	B
estimate	O
.	O
we	O
will	O
see	O
in	O
theorem	O
31.2	O
below	O
that	O
if	O
r	O
is	O
smooth	O
,	O
then	O
l~s	O
)	O
indeed	O
has	O
a	O
very	O
small	O
variance	O
in	O
many	O
situations	O
.	O
just	O
like	O
the	O
resubstitution	B
estimate	O
,	O
the	O
estimate	B
l~s	O
)	O
may	O
be	O
strongly	O
opti	O
(	O
cid:173	O
)	O
mistically	O
biased	O
.	O
just	O
consider	O
the	O
i-nearest	O
neighbor	B
rule	I
,	O
when	O
l~s	O
)	O
=	O
0	O
with	O
probability	O
one	O
,	O
whenever	O
x	O
has	O
a	O
density	O
.	O
to	O
combat	O
this	O
defect	O
,	O
one	O
may	O
define	O
the	O
deleted	B
version	O
of	O
the	O
smoothed	B
estimate	O
,	O
where	O
dn	O
,	O
i	O
is	O
the	O
training	O
sequence	O
with	O
the	O
i-th	O
pair	O
(	O
xi	O
,	O
yi	O
)	O
deleted	B
.	O
the	O
first	O
thing	O
we	O
notice	O
is	O
that	O
this	O
estimate	B
is	O
still	O
biased	O
,	O
even	O
asymptotically	O
.	O
to	O
illustrate	O
this	O
point	O
,	O
consider	O
r	O
(	O
u	O
)	O
=	O
u.	O
in	O
this	O
case	O
,	O
e	O
{	O
l~sd	O
)	O
}	O
e	O
{	O
i	O
{	O
y=o	O
}	O
'7n-l	O
(	O
x	O
,	O
dn-l	O
)	O
+	O
i	O
{	O
y=l	O
}	O
(	O
1	O
-	O
'7n-1	O
(	O
x	O
,	O
d	O
n	O
-	O
1	O
)	O
)	O
}	O
=	O
e	O
{	O
(	O
i	O
-	O
1j	O
(	O
x	O
)	O
)	O
17n-l	O
(	O
x	O
,	O
dn-	O
1	O
)	O
+	O
1j	O
(	O
x	O
)	O
(	O
i	O
-	O
1	O
}	O
n-l	O
(	O
x	O
,	O
dn-	O
1	O
)	O
)	O
}	O
.	O
if	O
the	O
estimate	B
'7n-	O
!	O
(	O
x	O
,	O
dn-l	O
)	O
was	O
perfect	O
,	O
that	O
is	O
,	O
equal	O
to	O
'7	O
(	O
x	O
)	O
for	O
every	O
x	O
,	O
then	O
the	O
expected	O
value	O
above	O
would	O
be	O
2e	O
{	O
n	O
(	O
x	O
)	O
(	O
l	O
-	O
n	O
(	O
x	O
)	O
)	O
}	O
,	O
which	O
is	O
the	O
asymptotic	O
error	O
probability	O
lnn	O
of	O
the	O
i-nn	O
rule	B
.	O
in	O
fact	O
,	O
31.1	O
smoothing	O
the	O
error	O
count	O
551	O
ie	O
{	O
l~sd	O
)	O
}	O
-	O
lnni	O
ie	O
{	O
i	O
{	O
y=o	O
}	O
1	O
]	O
n-1	O
(	O
x	O
,	O
d	O
n-	O
1	O
)	O
+	O
i	O
{	O
y=l	O
}	O
(	O
1	O
-	O
1	O
]	O
n-1	O
(	O
x	O
,	O
dn-	O
1	O
)	O
)	O
-	O
(	O
i	O
{	O
y=o	O
}	O
1	O
]	O
(	O
x	O
)	O
+	O
i	O
{	O
y=l	O
}	O
(	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
)	O
)	O
}	O
\	O
<	O
21e	O
{	O
1	O
]	O
n-1	O
(	O
x	O
,	O
dn-	O
1	O
)	O
-	O
1	O
]	O
(	O
x	O
)	O
}	O
i.	O
this	O
means	O
that	O
when	O
the	O
estimate	B
1	O
]	O
n	O
(	O
x	O
)	O
of	O
the	O
a	B
posteriori	I
probability	I
of	O
1	O
]	O
(	O
x	O
)	O
is	O
consistent	O
in	O
l1	O
(	O
/-l	O
)	O
,	O
then	O
l~sd	O
)	O
converges	O
to	O
l	O
nn	O
,	O
and	O
not	O
to	O
l	O
*	O
!	O
biasedness	O
of	O
an	O
error	O
estimate	O
is	O
not	O
necessarily	O
a	O
bad	O
property	O
.	O
in	O
most	O
ap	O
(	O
cid:173	O
)	O
plications	O
,	O
all	O
we	O
care	O
about	O
is	O
how	O
the	O
classification	O
rule	B
selected	O
by	O
minimizing	O
the	O
error	O
estimate	O
works	O
.	O
unfortunately	O
,	O
in	O
this	O
respect	O
,	O
smoothed	B
estimates	O
per	O
(	O
cid:173	O
)	O
form	O
poorly	O
,	O
even	O
compared	O
to	O
other	O
strongly	O
biased	O
error	O
estimates	O
such	O
as	O
the	O
empirical	O
squared	O
error	O
(	O
see	O
problem	O
31.4	O
)	O
.	O
the	O
next	O
example	O
illustrates	O
our	O
point	O
.	O
theorem	B
31.1.	O
let	O
the	O
distribution	B
of	O
x	O
be	O
concentrated	O
on	O
two	O
values	O
such	O
that	O
p	O
{	O
x	O
=	O
a	O
}	O
=	O
p	O
{	O
x	O
=	O
b	O
}	O
=	O
1/2	O
,	O
and	O
let	O
1	O
]	O
(	O
a	O
)	O
=	O
3/8	O
and	O
1	O
]	O
(	O
b	O
)	O
=	O
5/8	O
.	O
assume	O
that	O
the	O
smoothed	B
error	O
estimate	B
is	O
minimized	O
over	O
1	O
]	O
'	O
e	O
f	O
,	O
to	O
select	O
a	O
classifier	O
from	O
f	O
,	O
where	O
the	O
class	O
f	O
contains	O
two	O
functions	O
,	O
the	O
true	O
a	B
posteriori	I
probability	I
function	O
1	O
]	O
,	O
and	O
ij	O
,	O
where	O
ij	O
(	O
a	O
)	O
=0	O
,	O
3	O
_	O
1	O
]	O
(	O
b	O
)	O
=	O
8	O
'	O
then	O
the	O
probability	O
that	O
ij	O
is	O
selected	O
converges	O
to	O
one	O
as	O
n	O
~	O
00.	O
proof	O
.	O
straightforward	O
calculation	O
shows	O
that	O
the	O
statement	O
follows	O
from	O
the	O
law	O
of	O
large	O
numbers	O
.	O
0	O
remark	O
.	O
the	O
theorem	B
shows	O
that	O
even	O
if	O
the	O
true	O
a	B
posteriori	I
probability	I
function	O
is	O
contained	O
in	O
a	O
finite	O
class	O
of	O
candidates	O
,	O
the	O
smoothed	B
estimate	O
with	O
r	O
(	O
u	O
)	O
=	O
u	O
is	O
unable	O
to	O
select	O
a	O
good	O
discrimination	O
rule	B
.	O
the	O
result	O
may	O
be	O
extended	O
to	O
general	O
smooth	O
r	O
's	O
.	O
as	O
theorems	O
15.1	O
and	O
29.2	O
show	O
,	O
empirical	O
squared	O
error	O
minimization	O
or	O
maximum	B
likelihood	I
never	O
fail	O
in	O
this	O
situation	O
.	O
0	O
finally	O
we	O
demonstrate	O
that	O
if	O
r	O
is	O
smooth	O
,	O
then	O
the	O
variance	B
of	I
l~s	O
)	O
is	O
indeed	O
small	O
.	O
our	O
analysis	O
is	O
based	O
on	O
the	O
work	O
of	O
lugosi	O
and	O
pawlak	O
(	O
1994	O
)	O
.	O
the	O
bounds	O
552	O
31.	O
other	O
error	O
estimates	O
for	O
the	O
variance	B
of	I
l~s	O
)	O
remain	O
valid	O
for	O
l~sd	O
)	O
.	O
consider	O
classification	O
rules	O
of	O
the	O
form	O
(	O
x	O
)	O
=	O
{	O
o	O
ifrjn	O
(	O
x	O
,	O
dn	O
)	O
:	O
:	O
:	O
:	O
1/2	O
gn	O
1	O
otherwise	O
,	O
where	O
1711	O
(	O
x	O
,	O
dn	O
)	O
is	O
an	O
estimate	B
of	O
the	O
a	B
posteriori	I
probability	I
17	O
(	O
x	O
)	O
.	O
examples	O
include	O
the	O
histogram	B
rule	I
,	O
where	O
(	O
see	O
chapters	O
6	O
and	O
9	O
)	O
,	O
the	O
k-nearest	O
neighbor	B
rule	I
,	O
where	O
(	O
chapters	O
5	O
and	O
11	O
)	O
,	O
or	O
the	O
kernel	B
rule	I
,	O
where	O
(	O
chapter	O
10	O
)	O
.	O
in	O
the	O
sequel	O
,	O
we	O
concentrate	O
on	O
the	O
performance	O
of	O
the	O
smoothed	B
estimate	O
of	O
the	O
error	O
probability	O
of	O
these	O
nonparametric	O
rules	O
.	O
the	O
next	O
theorem	B
shows	O
that	O
for	O
these	O
rules	O
,	O
the	O
variance	B
of	I
the	O
smoothed	B
error	O
estimate	B
is	O
0	O
(	O
1/	O
n	O
)	O
,	O
no	O
matter	O
what	O
the	O
distribution	B
is	O
.	O
this	O
is	O
a	O
significant	O
improvement	O
over	O
the	O
variance	B
of	I
the	O
deleted	B
estimate	O
,	O
which	O
,	O
as	O
pointed	O
out	O
in	O
chapter	O
23	O
,	O
can	O
be	O
larger	O
than	O
1/j2nn	O
.	O
theorem	B
31.2.	O
assume	O
that	O
the	O
smoothing	O
function	O
r	O
(	O
u	O
)	O
satisfies	O
°	O
:	O
:	O
:	O
:	O
r	O
(	O
u	O
)	O
:	O
:	O
:	O
:	O
1	O
for	O
u	O
e	O
[	O
0	O
,	O
1	O
]	O
,	O
and	O
is	O
uniformly	O
lipschitz	O
continuous	O
,	O
that	O
is	O
,	O
ir	O
(	O
u	O
)	O
-	O
r	O
(	O
v	O
)	O
1	O
:	O
:	O
:	O
:	O
clu	O
-	O
vi	O
for	O
all	O
u	O
,	O
v	O
,	O
and	O
for	O
some	O
constant	O
c.	O
then	O
the	O
smoothed	B
estimate	O
l~s	O
)	O
of	O
the	O
histogram	O
,	O
k-nearest	O
neighbor	O
,	O
and	O
moving	O
window	O
rules	O
(	O
with	O
kernel	O
k	O
=	O
iso	O
,	O
)	O
satisfies	O
and	O
var	O
{	O
l	O
(	O
s	O
)	O
}	O
<	O
-	O
n	O
c	O
-	O
4n	O
'	O
where	O
c	O
is	O
a	O
constant	O
depending	O
on	O
the	O
rule	B
only	O
.	O
in	O
the	O
case	O
of	O
the	O
histogram	B
rule	I
the	O
value	O
of	O
c	O
is	O
c	O
=	O
(	O
1	O
+	O
4c	O
)	O
2	O
,	O
for	O
the	O
k-nearest	O
neighbor	B
rule	I
c	O
=	O
(	O
1	O
+	O
2cyd	O
)	O
2	O
,	O
and	O
for	O
the	O
moving	B
window	I
rule	I
c	O
=	O
(	O
1	O
+	O
2cf3d	O
)	O
2.	O
here	O
c	O
is	O
the	O
constant	O
in	O
the	O
lipschitz	O
condition	O
,	O
yd	O
is	O
the	O
minimal	O
number	O
of	O
cones	O
centered	O
at	O
the	O
origin	O
of	O
angle	O
n	O
/6	O
that	O
coverrd	O
,	O
and	O
f3d	O
is	O
the	O
minimal	O
number	O
of	O
balls	O
of	O
radius	O
1/2	O
that	O
cover	O
the	O
unit	O
ball	O
in	O
rd	O
.	O
31.1	O
smoothing	O
the	O
error	O
count	O
553	O
remark	O
.	O
notice	O
that	O
the	O
inequalities	O
of	O
theorem	O
31.2	O
are	O
valid	O
for	O
all	O
n	O
,	O
e	O
,	O
and	O
h	O
>	O
0	O
for	O
the	O
histogram	O
and	O
moving	O
window	O
rules	O
,	O
and	O
k	O
for	O
the	O
nearest	B
neighbor	I
rules	I
.	O
interestingly	O
,	O
the	O
constant	O
c	O
does	O
not	O
change	O
with	O
the	O
dimension	B
in	O
the	O
histogram	O
case	O
,	O
but	O
grows	O
exponentially	O
with	O
d	O
for	O
the	O
k-nearest	O
neighbor	O
and	O
moving	O
window	O
rules	O
.	O
0	O
proof	O
.	O
the	O
probability	O
inequalities	O
follow	O
from	O
appropriate	O
applications	O
of	O
mc	O
(	O
cid:173	O
)	O
diarmid	O
's	O
inequality	B
.	O
the	O
upper	O
bound	O
on	O
the	O
variance	O
follows	O
similarly	O
from	O
theorem	B
9.3.	O
we	O
consider	O
each	O
of	O
the	O
three	O
rules	O
in	O
tum	O
.	O
the	O
histogram	B
rule	I
.	O
let	O
(	O
xl	O
,	O
yd	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
be	O
a	O
fixed	O
training	O
sequence	O
.	O
if	O
we	O
can	O
show	O
that	O
by	O
replacing	O
the	O
value	O
of	O
a	O
pair	O
(	O
xi	O
,	O
yi	O
)	O
in	O
the	O
training	O
sequence	O
by	O
some	O
(	O
x	O
;	O
,	O
y	O
;	O
)	O
the	O
value	O
of	O
the	O
estimate	B
l~s	O
)	O
can	O
change	O
by	O
at	O
most	O
(	O
l	O
+	O
2c	O
)	O
ln	O
,	O
then	O
the	O
inequality	B
follows	O
by	O
applying	O
theorem	B
9.2	O
with	O
ci	O
=	O
sin	O
.	O
the	O
i	O
-th	O
term	O
of	O
the	O
sum	O
in	O
l~s	O
)	O
can	O
change	O
by	O
one	O
,	O
causing	O
1	O
in	O
change	O
in	O
the	O
average	O
.	O
obviously	O
,	O
all	O
the	O
other	O
terms	O
in	O
the	O
sum	O
that	O
can	O
change	O
are	O
the	O
ones	O
corresponding	O
to	O
the	O
xj	O
's	O
that	O
are	O
in	O
the	O
same	O
set	O
of	O
the	O
partition	B
as	O
either	O
xi	O
or	O
x	O
;	O
.	O
denoting	O
the	O
number	O
of	O
points	O
in	O
the	O
same	O
set	O
with	O
xi	O
and	O
xi	O
by	O
k	O
and	O
k	O
'	O
respectively	O
,	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
estimate	B
of	O
the	O
a	O
posteriori	O
probabilities	O
in	O
these	O
points	O
can	O
change	O
by	O
at	O
most	O
21	O
k	O
and	O
21	O
k	O
'	O
,	O
respectively	O
.	O
it	O
means	O
that	O
the	O
overall	O
change	O
in	O
the	O
value	O
of	O
the	O
sum	O
can	O
not	O
exceed	O
(	O
1	O
+k	O
¥-	O
+k	O
'	O
f	O
,	O
)	O
in	O
=	O
(	O
1	O
+4c	O
)	O
in	O
.	O
the	O
k-nearest	O
neighbor	B
rule	I
.	O
to	O
avoid	O
difficulties	O
caused	O
by	O
breaking	O
distance	B
ties	O
,	O
assume	O
that	O
x	O
has	O
a	O
density	O
.	O
then	O
recall	O
that	O
the	O
application	O
of	O
lemma	O
11.1	O
for	O
the	O
empirical	B
distribution	O
implies	O
that	O
no	O
x	O
j	O
can	O
be	O
one	O
of	O
the	O
k	O
nearest	O
neighbors	O
of	O
more	O
than	O
kyd	O
points	O
from	O
dn	O
.	O
thus	O
,	O
changing	O
the	O
value	O
of	O
one	O
pair	O
in	O
the	O
training	O
sequence	O
can	O
change	O
at	O
most	O
1	O
+	O
2kyd	O
terms	O
in	O
the	O
expression	B
of	O
l~d	O
)	O
,	O
one	O
of	O
them	O
by	O
at	O
most	O
1	O
,	O
and	O
all	O
the	O
others	O
by	O
at	O
most	O
c	O
i	O
k.	O
theorem	B
9.2	O
yields	O
the	O
result	O
.	O
the	O
moving	B
window	I
rule	I
.	O
again	O
,	O
we	O
only	O
have	O
to	O
check	O
the	O
condition	O
of	O
theorem	O
9.2	O
with	O
ci	O
=	O
(	O
1	O
+	O
2c	O
{	O
3d	O
)	O
/n	O
.	O
fix	O
a	O
training	O
sequence	O
(	O
xl	O
,	O
yl	O
)	O
,	O
'	O
''	O
,	O
(	O
xn	O
,	O
yn	O
)	O
and	O
replace	O
the	O
pair	O
(	O
xi	O
,	O
yi	O
)	O
by	O
(	O
x	O
;	O
,	O
y	O
;	O
>	O
'	O
then	O
the	O
i	O
-th	O
term	O
in	O
the	O
sum	O
of	O
the	O
expression	B
of	O
l~s	O
)	O
can	O
change	O
by	O
at	O
most	O
one	O
.	O
clearly	O
,	O
the	O
j-th	O
term	O
,	O
for	O
which	O
xj	O
fj	O
sxi	O
.	O
h	O
and	O
xj	O
fj	O
sx	O
;	O
.h	O
'	O
keeps	O
its	O
value	O
.	O
it	O
is	O
easy	O
to	O
see	O
that	O
all	O
the	O
other	O
terms	O
can	O
change	O
by	O
at	O
most	O
c	O
.	O
max	O
{	O
i	O
i	O
k	O
j	O
,	O
1	O
i	O
kj	O
}	O
,	O
where	O
k	O
j	O
and	O
kj	O
are	O
the	O
numbers	O
of	O
points	O
xk	O
,	O
k	O
=i	O
i	O
,	O
j	O
,	O
from	O
the	O
training	O
sequence	O
that	O
fall	O
in	O
sxi.h	O
and	O
sx	O
;	O
.h	O
'	O
respectively	O
.	O
thus	O
,	O
the	O
overall	O
change	O
in	O
the	O
sum	O
does	O
not	O
exceed	O
1	O
+	O
``	O
~	O
+	O
``	O
l	O
~	O
.	O
k'.	O
]	O
l..	O
k·	O
]	O
xjesxi.h	O
xjesx	O
;	O
.h	O
it	O
suffices	O
to	O
show	O
by	O
symmetry	O
that	O
lxes	O
i	O
,	O
j	O
:	O
xk	O
e	O
sxi	O
,	O
h	O
n	O
sxj	O
,	O
h	O
}	O
l.	O
then	O
clearly	O
,	O
)	O
xi	O
,	O
h	O
llkj	O
s	O
(	O
3d	O
.	O
let	O
nj	O
=	O
i	O
{	O
xb	O
k	O
=i	O
1	O
lk-sln	O
.	O
xjesxi	O
,	O
h	O
]	O
xjesxj.h	O
]	O
554	O
31.	O
other	O
error	O
estimates	O
to	O
bound	O
the	O
right-hand	O
side	O
from	O
above	O
,	O
cover	O
sxj	O
,	O
h	O
by	O
fjd	O
balls	O
sl	O
,	O
...	O
,	O
sf3d	O
of	O
radius	O
hj2	O
.	O
denote	O
the	O
number	O
of	O
points	O
falling	O
in	O
them	O
by	O
lm	O
(	O
m	O
=	O
1	O
,	O
...	O
,	O
fjd	O
)	O
:	O
lm	O
=	O
i	O
{	O
xk	O
,	O
k	O
i	O
i	O
:	O
xk	O
e	O
sxj	O
,	O
h	O
n	O
sm	O
}	O
l·	O
then	O
and	O
the	O
theorem	B
is	O
proved	O
.	O
0	O
31.2	O
posterior	B
probability	I
estimates	O
the	O
error	O
estimates	O
discussed	O
in	O
this	O
section	O
improve	O
on	O
the	O
biasedness	O
of	O
smoothed	O
estimates	O
,	O
while	O
preserving	O
their	O
small	O
variance	O
.	O
still	O
,	O
these	O
estimates	O
are	O
of	O
ques	O
(	O
cid:173	O
)	O
tionable	O
utility	O
in	O
classifier	O
selection	B
.	O
considering	O
the	O
formula	O
for	O
the	O
error	O
probability	O
of	O
a	O
classification	O
rule	B
gn	O
(	O
x	O
)	O
=	O
1	O
{	O
1jn	O
(	O
x	O
,	O
dn	O
»	O
1/2	O
}	O
,	O
it	O
is	O
plausible	O
to	O
introduce	O
the	O
estimate	B
l	O
(	O
p	O
)	O
=	O
n	O
1	O
n	O
-	O
l	O
(	O
i	O
{	O
1jn	O
(	O
x	O
;	O
,	O
dn	O
)	O
sl/2	O
}	O
17n	O
(	O
xi	O
,	O
dn	O
)	O
n	O
i=l	O
+	O
1	O
{	O
1jn	O
(	O
x	O
;	O
,	O
dn	O
»	O
1/2	O
}	O
(	O
1	O
-	O
17n	O
(	O
xi	O
,	O
dn	O
»	O
)	O
,	O
that	O
is	O
,	O
the	O
expected	O
value	O
is	O
estimated	O
by	O
a	O
sample	O
average	O
,	O
and	O
instead	O
of	O
the	O
(	O
unknown	O
)	O
a	B
posteriori	I
probability	I
yj	O
(	O
x	O
)	O
,	O
its	O
estimate	B
17n	O
(	O
x	O
,	O
dn	O
)	O
is	O
plugged	O
into	O
the	O
formula	O
of	O
ln	O
.	O
the	O
estimate	B
l	O
?	O
)	O
is	O
usually	O
called	O
the	O
posterior	B
probability	I
error	O
estimate	B
.	O
in	O
the	O
case	O
of	O
nonparametric	O
rules	O
such	O
as	O
histogram	O
,	O
kernel	B
,	O
and	O
k-nn	O
rules	O
it	O
is	O
natural	O
to	O
use	O
the	O
corresponding	O
nonparametric	O
estimates	O
of	O
the	O
a	O
poste	O
(	O
cid:173	O
)	O
riori	O
probabilities	O
for	O
plugging	O
in	O
the	O
expression	B
of	O
the	O
error	O
probability	O
.	O
this	O
,	O
and	O
similar	O
estimates	O
of	O
ln	O
have	O
been	O
introduced	O
and	O
studied	O
by	O
fukunaga	O
and	O
kessel	O
(	O
1973	O
)	O
,	O
rora	O
and	O
wilcox	O
(	O
1982	O
)	O
,	O
fitzmaurice	O
and	O
rand	O
(	O
1987	O
)	O
,	O
ganesalingam	O
and	O
mclachlan	O
(	O
1980	O
)	O
,	O
kittler	O
and	O
devijver	O
(	O
1981	O
)	O
,	O
matloff	O
and	O
pruitt	O
(	O
1984	O
)	O
,	O
moore	O
,	O
whitsitt	O
,	O
and	O
landgrebe	O
(	O
1976	O
)	O
,	O
pawlak	O
(	O
1988	O
)	O
,	O
schwemer	O
and	O
dunn	O
(	O
1980	O
)	O
,	O
and	O
lugosi	O
and	O
pawlak	O
(	O
1994	O
)	O
.	O
it	O
is	O
interesting	O
to	O
notice	O
the	O
similarity	O
between	O
the	O
estimates	O
l~s	O
)	O
and	O
l~p	O
)	O
,	O
although	O
they	O
were	O
developed	O
from	O
different	O
scenarios	O
.	O
to	O
reduce	O
the	O
bias	B
,	O
we	O
can	O
use	O
the	O
leave-one-out	B
,	O
(	O
or	O
deleted	B
)	O
version	O
of	O
the	O
estimate	B
,	O
l	O
(	O
pd	O
)	O
=	O
n	O
1	O
n	O
-	O
l	O
(	O
i	O
{	O
1jn-l	O
(	O
x	O
;	O
,	O
d/l	O
,	O
i	O
)	O
sl/2	O
}	O
17n-1	O
(	O
xi	O
,	O
dn	O
,	O
j	O
)	O
n	O
i==l	O
+	O
i	O
{	O
1jn-l	O
(	O
xi	O
,	O
dn	O
,	O
;	O
»	O
1/2	O
}	O
(	O
1	O
-	O
17n-1	O
(	O
xi	O
,	O
dn	O
,	O
i	O
»	O
)	O
'	O
the	O
deleted	B
version	O
l~p	O
d	O
)	O
,	O
has	O
a	O
much	O
better	O
bias	B
than	O
l~sd	O
)	O
.	O
we	O
have	O
the	O
bound	O
31.2	O
posterior	B
probability	I
estimates	O
555.	O
ie	O
{	O
l~pd	O
)	O
}	O
-eln-l	O
!	O
ie	O
{	O
i	O
{	O
gn	O
(	O
x	O
)	O
=o	O
}	O
1	O
]	O
n-l	O
(	O
x	O
,	O
dn-1	O
)	O
+	O
i	O
{	O
gn	O
(	O
x	O
)	O
=l	O
}	O
(	O
1	O
-	O
1	O
]	O
n-1	O
(	O
x	O
,	O
dn-	O
1	O
)	O
)	O
-	O
(	O
i	O
{	O
gll	O
(	O
x	O
)	O
=o	O
}	O
1	O
]	O
(	O
x	O
)	O
+	O
i	O
{	O
g	O
,	O
,	O
(	O
x	O
)	O
=l	O
}	O
(	O
1	O
-	O
17	O
(	O
x	O
)	O
)	O
)	O
}	O
!	O
<	O
21e	O
{	O
1	O
]	O
n-l	O
(	O
x	O
,	O
dn-d	O
-	O
1	O
]	O
(	O
x	O
)	O
}	O
i·	O
this	O
means	O
that	O
if	O
the	O
estimate	B
1	O
]	O
n	O
(	O
x	O
)	O
of	O
the	O
a	B
posteriori	I
probability	I
of	O
1	O
]	O
(	O
x	O
)	O
is	O
con	O
(	O
cid:173	O
)	O
sistent	O
in	O
l	O
1	O
(	O
/1	O
)	O
,	O
then	O
el~p	O
d	O
)	O
converges	O
to	O
l	O
*	O
.	O
this	O
is	O
the	O
case	O
for	O
all	O
distributions	O
for	O
the	O
histogram	O
and	O
moving	O
window	O
rules	O
if	O
h	O
-+	O
0	O
and	O
nh	O
d	O
-+	O
00	O
,	O
and	O
the	O
k-nearest	O
neighbor	B
rule	I
if	O
k	O
-+	O
00	O
and	O
k	O
/	O
n	O
-+	O
0	O
,	O
as	O
it	O
is	O
seen	O
in	O
chapters	O
9	O
,	O
10	O
,	O
and	O
11.	O
for	O
specific	O
cases	O
it	O
is	O
possible	O
to	O
obtain	O
sharper	O
bounds	O
on	O
the	O
bias	B
of	I
l~p	O
d	O
)	O
.	O
for	O
the	O
histogram	B
rule	I
,	O
lugosi	O
and	O
pawlak	O
(	O
1994	O
)	O
carried	O
out	O
such	O
analysis	O
.	O
they	O
showed	O
for	O
example	O
that	O
the	O
estimate	B
l~pd	O
)	O
is	O
optimistically	O
biased	O
(	O
see	O
problem	O
31.2	O
)	O
.	O
posterior	B
probability	I
estimates	O
of	O
ln	O
share	O
the	O
good	O
stability	O
properties	O
of	O
smoothed	O
estimates	O
(	O
problem	O
31.1	O
)	O
.	O
finally	O
,	O
let	O
us	O
select	O
a	O
function	O
1	O
]	O
'	O
:	O
nd	O
-+	O
[	O
0	O
,	O
1	O
]	O
-and	O
a	O
corresponding	O
rule	B
g	O
(	O
x	O
)	O
=	O
ih	O
'	O
(	O
x	O
»	O
lj2	O
}	O
-from	O
a	O
class	O
f	O
based	O
on	O
the	O
minimization	O
of	O
the	O
posterior	B
probability	I
error	O
estimate	B
observe	O
that	O
l~p	O
)	O
(	O
1	O
]	O
'	O
)	O
=	O
0	O
when	O
1	O
]	O
'	O
(	O
x	O
)	O
e	O
{	O
o	O
,	O
i	O
}	O
for	O
all	O
x	O
,	O
that	O
is	O
,	O
rule	B
selection	O
based	O
on	O
this	O
estimate	B
just	O
does	O
not	O
make	O
sense	O
.	O
the	O
reason	O
is	O
that	O
l~p	O
)	O
ignores	O
the	O
yi	O
's	O
of	O
the	O
data	O
sequence	O
!	O
fukunaga	O
and	O
kessel	O
(	O
1973	O
)	O
argued	O
that	O
efficient	O
posterior	B
probability	I
estima	O
(	O
cid:173	O
)	O
tors	O
can	O
be	O
obtained	O
if	O
additional	O
unclassified	O
observations	O
are	O
available	O
.	O
very	O
often	O
in	O
practice	O
,	O
in	O
addition	O
to	O
the	O
training	O
sequence	O
dn	O
,	O
further	O
feature	O
vectors	O
xn+1	O
,	O
...	O
,	O
xn+z	O
are	O
given	O
without	O
their	O
labels	O
yn+1	O
,	O
•••	O
,	O
yn+z	O
,	O
where	O
the	O
xn+i	O
are	O
i.i.d.	O
,	O
independent	O
from	O
x	O
and	O
dn	O
,	O
and	O
they	O
have	O
the	O
same	O
distribution	B
as	O
that	O
of	O
x.	O
this	O
situation	O
is	O
typical	O
in	O
medical	O
applications	O
,	O
when	O
large	O
sets	O
of	O
medical	O
records	O
are	O
available	O
,	O
but	O
it	O
is	O
usually	O
very	O
expensive	O
to	O
get	O
their	O
correct	O
diagnosis	O
.	O
these	O
unclassified	O
samples	O
can	O
be	O
efficiently	O
used	O
for	O
testing	O
the	O
performance	O
of	O
a	O
classifier	O
designed	O
from	O
dn	O
by	O
using	O
the	O
estimate	B
=	O
1	O
z	O
i	O
l	O
(	O
i	O
{	O
7jn	O
(	O
xn+i	O
,	O
dn	O
)	O
~lj2	O
}	O
1	O
]	O
n	O
(	O
xn+i	O
'	O
dn	O
)	O
i=l	O
+	O
i	O
{	O
7jn	O
(	O
xn+i	O
,	O
dn	O
»	O
lj2	O
}	O
(	O
1	O
-	O
1	O
]	O
n	O
(	O
xn+i	O
,	O
dn	O
)	O
)	O
)	O
.	O
again	O
,	O
using	O
l~~	O
)	O
for	O
rule	B
selection	O
is	O
meaningless	O
.	O
556	O
31.	O
other	O
error	O
estimates	O
31.3	O
rotation	B
estimate	O
this	O
method	O
,	O
suggested	O
by	O
toussaint	O
and	O
donaldson	O
(	O
1970	O
)	O
,	O
is	O
a	O
combination	O
of	O
the	O
holdout	B
and	O
deleted	B
estimates	O
.	O
it	O
is	O
sometimes	O
called	O
the	O
n	O
-estimate	O
.	O
let	O
s	O
<	O
n	O
be	O
a	O
positive	O
integer	O
(	O
typically	O
much	O
smaller	O
than	O
n	O
)	O
,	O
and	O
assume	O
,	O
for	O
the	O
sake	O
of	O
simplicity	O
,	O
that	O
q	O
=	O
n	O
/	O
s	O
is	O
an	O
integer	O
.	O
the	O
rotation	B
method	O
forms	O
the	O
holdout	B
estimate	O
,	O
by	O
holding	O
the	O
first	O
s	O
pairs	O
of	O
the	O
training	O
data	O
out	O
,	O
then	O
the	O
second	O
s	O
pairs	O
,	O
and	O
so	O
forth	O
.	O
the	O
estimate	B
is	O
defined	O
by	O
averaging	O
the	O
q	O
numbers	O
obtained	O
this	O
way	O
.	O
to	O
formalize	O
,	O
denote	O
by	O
d~~j	O
the	O
training	O
data	O
,	O
with	O
the	O
j-th	O
s-block	O
held	O
out	O
(	O
j	O
=	O
1	O
,	O
...	O
,	O
q	O
)	O
:	O
d	O
,	O
~~j	O
=	O
(	O
(	O
xl	O
,	O
yd	O
,	O
...	O
,	O
(	O
xs	O
(	O
j-l	O
)	O
,	O
ys	O
(	O
j-l	O
)	O
,	O
(	O
xs	O
)	O
+l	O
,	O
ys	O
)	O
+l	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
»	O
.	O
the	O
estimate	B
is	O
defined	O
by	O
1	O
q	O
1	O
l	O
(	O
d	O
)	O
-	O
l	O
l	O
i	O
s	O
)	O
n	O
s	O
-	O
,	O
q	O
{	O
gll-s	O
(	O
x	O
''	O
dil	O
)	O
=tyi	O
}	O
•	O
.	O
(	O
s	O
)	O
'	O
-	O
s	O
)	O
=1	O
i=s	O
(	O
j-l	O
)	O
+l	O
s	O
=	O
1	O
yields	O
the	O
deleted	B
estimate	O
.	O
if	O
s	O
>	O
1	O
,	O
then	O
the	O
estimate	B
is	O
usually	O
more	O
biased	O
than	O
the	O
deleted	B
estimate	O
,	O
as	O
but	O
usually	O
exhibits	O
smaller	O
variance	O
.	O
el~~j	O
=	O
eln-	O
s	O
,	O
31.4	O
bootstrap	B
bootstrap	O
methods	O
for	O
estimating	O
the	O
misc1assification	O
error	O
became	O
popular	O
fol	O
(	O
cid:173	O
)	O
lowing	O
the	O
revolutionary	O
work	O
of	O
efron	O
(	O
1979	O
;	O
1983	O
)	O
.	O
all	O
bootstrap	B
estimates	O
introduce	O
artificial	O
randomization	O
.	O
the	O
bootstrap	B
sample	O
d	O
(	O
b	O
)	O
=	O
(	O
(	O
x	O
(	O
b	O
)	O
y	O
(	O
b	O
)	O
in	O
1	O
1	O
'	O
,	O
...	O
,	O
(	O
x	O
(	O
b	O
)	O
ycb	O
)	O
)	O
in	O
'	O
in	O
is	O
a	O
sequence	O
of	O
random	O
variable	B
pairs	O
drawn	O
randomly	O
with	O
replacement	O
from	O
the	O
set	O
{	O
(	O
xl	O
,	O
yd	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
)	O
}	O
.	O
in	O
other	O
words	O
,	O
conditionally	O
on	O
the	O
training	O
sample	O
dn	O
=	O
(	O
(	O
xl	O
,	O
yi	O
)	O
,	O
...	O
,	O
(	O
xn	O
,	O
yn	O
»	O
,	O
the	O
pairs	O
(	O
xib	O
)	O
,	O
y	O
?	O
)	O
are	O
drawn	O
independently	O
from	O
vn	O
,	O
the	O
empirical	B
distribution	O
based	O
on	O
dn	O
in	O
n	O
d	O
x	O
{	O
o	O
,	O
i	O
}	O
.	O
one	O
of	O
the	O
standard	B
bootstrap	O
estimates	O
aims	O
to	O
compensate	O
the	O
(	O
usually	O
opti	O
(	O
cid:173	O
)	O
mistic	O
)	O
bias	B
b	O
(	O
gn	O
)	O
=	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
-	O
l	O
?	O
)	O
(	O
gn	O
)	O
of	O
the	O
resubstitution	B
estimate	O
l~r	O
)	O
.	O
to	O
estimate	B
b	O
(	O
gn	O
)	O
,	O
a	O
bootstrap	O
sample	O
of	O
size	O
m	O
=	O
n	O
may	O
be	O
used	O
:	O
often	O
,	O
bootstrap	B
sampling	O
is	O
repeated	O
several	O
times	O
to	O
average	O
out	O
effects	O
of	O
the	O
additional	O
randomization	O
.	O
in	O
our	O
case	O
,	O
31.4	O
bootstrap	B
557	O
yields	O
the	O
estimator	O
l~b	O
)	O
(	O
gn	O
)	O
=	O
l~r	O
)	O
(	O
gn	O
)	O
-	O
bn	O
(	O
gn	O
)	O
.	O
another	O
instance	O
of	O
a	O
bootstrap	O
sample	O
of	O
size	O
n	O
,	O
the	O
so-called	O
eo	O
estimator	O
,	O
uses	O
resubstitution	B
on	O
the	O
training	O
pairs	O
not	O
appearing	O
in	O
the	O
bootstrap	B
sample	O
.	O
the	O
estimator	O
is	O
defined	O
by	O
here	O
,	O
too	O
,	O
averages	O
may	O
be	O
taken	O
after	O
generating	O
the	O
bootstrap	B
sample	O
several	O
times	O
.	O
many	O
other	O
versions	O
of	O
bootstrap	O
estimates	O
have	O
been	O
reported	O
,	O
such	O
as	O
the	O
''	O
0.632	O
estimate	O
,	O
''	O
``	O
double	B
bootstrap	I
,	O
''	O
and	O
``	O
randomized	B
bootstrap	I
''	O
(	O
see	O
hand	O
(	O
1986	O
)	O
,	O
jain	O
,	O
dubes	O
,	O
and	O
chen	O
(	O
1987	O
)	O
,	O
and	O
mclachlan	O
(	O
1992	O
)	O
for	O
surveys	O
and	O
ad	O
(	O
cid:173	O
)	O
ditional	O
references	O
)	O
.	O
clearly	O
,	O
none	O
of	O
these	O
estimates	O
provides	O
a	O
universal	O
remedy	O
,	O
but	O
for	O
several	O
specific	O
classification	O
rules	O
,	O
bootstrap	B
estimates	O
have	O
been	O
experi	O
(	O
cid:173	O
)	O
mentally	O
found	O
to	O
outperform	O
the	O
deleted	B
and	O
resubstitution	B
estimates	O
.	O
however	O
,	O
one	O
point	O
has	O
to	O
be	O
made	O
clear	O
:	O
we	O
always	O
lose	O
information	O
with	O
the	O
additional	O
randomization	O
.	O
we	O
summarize	O
this	O
in	O
the	O
following	O
simple	O
general	O
result	O
:	O
theorem	B
31.3.	O
let	O
x	O
i	O
,	O
...	O
,	O
xn	O
be	O
drawn	O
from	O
an	O
unknown	O
distribution	B
jl	O
,	O
and	O
let	O
a	O
(	O
fl	O
)	O
be	O
afunctional	O
to	O
be	O
estimated	O
.	O
let	O
r	O
(	O
·	O
)	O
be	O
a	O
convex	O
risk	O
function	O
(	O
such	O
as	O
r	O
(	O
u	O
)	O
=	O
u2	O
or	O
r	O
(	O
u	O
)	O
=	O
lui	O
}	O
.	O
let	O
xib	O
)	O
,	O
...	O
xi~	O
)	O
be	O
a	O
bootstrap	O
sample	O
drawn	O
from	O
xl	O
,	O
...	O
,	O
x	O
n	O
.	O
then	O
the	O
theorem	B
states	O
that	O
no	O
matter	O
how	O
large	O
m	O
is	O
,	O
the	O
class	O
of	O
estimators	O
that	O
are	O
functions	O
of	O
the	O
original	O
sample	O
is	O
always	O
at	O
least	O
as	O
good	O
as	O
the	O
class	O
of	O
all	O
estimators	O
that	O
are	O
based	O
upon	O
bootstrap	B
samples	O
.	O
in	O
our	O
case	O
,	O
a	O
(	O
jl	O
)	O
plays	O
the	O
role	O
of	O
the	O
expected	O
error	O
probability	O
eln	O
=	O
p	O
{	O
gn	O
(	O
x	O
)	O
=i	O
y	O
}	O
.	O
if	O
we	O
take	O
r	O
(	O
u	O
)	O
=	O
u2	O
,	O
then	O
it	O
follows	O
from	O
the	O
theorem	B
that	O
there	O
is	O
no	O
estimator	O
ln	O
based	O
on	O
the	O
bootstrap	B
sample	O
d	O
,	O
~	O
)	O
whose	O
squared	B
error	I
e	O
{	O
(	O
eln	O
-	O
lm	O
)	O
2	O
}	O
is	O
smaller	O
than	O
that	O
of	O
some	O
nonbootstrap	O
estimate	B
.	O
in	O
the	O
proof	O
of	O
the	O
theorem	B
we	O
construct	O
such	O
a	O
non-bootstrap	O
estimator	O
.	O
it	O
is	O
clear	O
,	O
however	O
,	O
that	O
,	O
in	O
general	O
,	O
the	O
latter	O
estimator	O
is	O
too	O
complex	O
to	O
have	O
any	O
practical	O
value	O
.	O
the	O
randomization	O
of	O
bootstrap	O
methods	O
may	O
provide	O
a	O
useful	O
tool	O
to	O
overcome	O
the	O
computational	O
difficulties	O
in	O
finding	O
good	O
estimators	O
.	O
558	O
31.	O
other	O
error	O
estimates	O
proof	O
.	O
let	O
1jr	O
be	O
any	O
mapping	O
taking	O
m	O
arguments	O
.	O
then	O
e	O
{	O
r	O
(	O
1jr	O
(	O
xib	O
)	O
,	O
...	O
x	O
}	O
~	O
»	O
-	O
a	O
(	O
jl	O
»	O
)	O
lxi	O
,	O
...	O
,	O
xn	O
}	O
=	O
1	O
m	O
n	O
i	O
:	O
(	O
ij	O
,	O
...	O
.	O
im	O
)	O
c	O
{	O
i	O
,	O
...	O
,	O
n	O
}	O
m	O
r	O
(	O
1jr	O
(	O
xij	O
,	O
...	O
,	O
xi	O
,	O
,	O
)	O
-	O
a	O
(	O
jl	O
»	O
(	O
by	O
jensen	O
's	O
inequality	B
and	O
the	O
convexity	O
of	O
r	O
)	O
r	O
(	O
~	O
i	O
:	O
r	O
(	O
1jr*	O
(	O
x1	O
,	O
...	O
,	O
xn	O
)	O
-	O
a	O
(	O
jl	O
»	O
)	O
(	O
ij	O
,	O
...	O
,	O
im	O
)	O
c	O
{	O
i	O
,	O
...	O
,	O
n	O
}	O
m	O
n	O
1jr	O
(	O
xij	O
,	O
...	O
,	O
xij	O
-	O
a	O
(	O
jl	O
»	O
)	O
=	O
where	O
now	O
,	O
after	O
taking	O
expectations	O
with	O
respect	O
to	O
xl	O
,	O
...	O
,	O
x	O
n	O
,	O
we	O
see	O
that	O
for	O
every	O
1jr	O
we	O
start	O
out	O
with	O
,	O
there	O
is	O
a	O
1jr*	O
that	O
is	O
at	O
least	O
as	O
good	O
.	O
0	O
i	O
'	O
1	O
,	O
...	O
,	O
xm	O
'	O
ym	O
'	O
if	O
m	O
=	O
0	O
(	O
n	O
)	O
,	O
then	O
the	O
bootstrap	B
has	O
an	O
additional	O
problem	O
related	O
to	O
the	O
coupon	O
collector	O
problem	O
.	O
let	O
n	O
be	O
the	O
number	O
of	O
different	O
pairs	O
in	O
the	O
bootstrap	B
sample	O
(	O
x	O
(	O
b	O
)	O
y	O
(	O
b	O
»	O
en	O
,	O
1	O
m	O
``	O
'-	O
'	O
en	O
lor	O
some	O
constant	O
c	O
,	O
t	O
en	O
n	O
n	O
--	O
+	O
1	O
-	O
e-c	O
with	O
probability	O
one	O
.	O
to	O
see	O
this	O
,	O
note	O
that	O
l	O
)	O
cn	O
;	O
;	O
(	O
b	O
»	O
th'f	O
''	O
''	O
ne-c	O
(	O
b	O
)	O
h	O
(	O
e	O
{	O
n	O
-	O
n	O
}	O
=	O
n	O
1	O
-	O
,	O
~	O
/	O
so	O
e	O
{	O
n	O
/	O
n	O
}	O
--	O
+	O
1	O
-	O
e	O
-c.	O
furthermore	O
,	O
if	O
one	O
of	O
the	O
m	O
drawings	O
is	O
varied	O
,	O
n	O
changes	O
by	O
at	O
most	O
one	O
.	O
hence	O
,	O
by	O
mcdiarmid	O
's	O
inequality	B
,	O
for	O
e	O
>	O
0	O
,	O
from	O
which	O
we	O
conclude	O
that	O
n	O
/	O
n	O
--	O
+	O
1-	O
e-c	O
with	O
probability	O
one	O
.	O
as	O
n	O
(	O
1	O
-	O
e-c	O
)	O
of	O
the	O
original	O
data	O
pairs	O
do	O
not	O
appear	O
in	O
the	O
bootstrap	B
sample	O
,	O
a	O
considerable	O
loss	O
of	O
information	O
takes	O
place	O
that	O
will	O
be	O
reflected	O
in	O
the	O
performance	O
.	O
this	O
phenomenon	O
is	O
well-known	O
,	O
and	O
motivated	O
several	O
modifications	O
of	O
the	O
simplest	O
bootstrap	B
estimate	O
.	O
for	O
more	O
information	O
,	O
see	O
the	O
surveys	O
by	O
hand	O
(	O
1986	O
)	O
,	O
jain	O
,	O
dubes	O
,	O
and	O
chen	O
(	O
1987	O
)	O
,	O
and	O
mclachlan	O
(	O
1992	O
)	O
.	O
problems	O
and	O
exercises	O
559	O
problems	O
and	O
exercises	O
problem	O
31.1.	O
show	O
that	O
the	O
posterior	B
probability	I
estimate	O
l~p	O
)	O
of	O
the	O
histogram	O
,	O
k-nearest	O
neighbor	O
,	O
and	O
moving	O
window	O
rules	O
satisfies	O
and	O
var	O
{	O
l	O
(	O
p	O
)	O
}	O
<	O
.£	O
-	O
4n	O
'	O
n	O
where	O
c	O
is	O
a	O
constant	O
,	O
depending	O
on	O
the	O
rule	B
.	O
in	O
the	O
case	O
of	O
the	O
histogram	B
rule	I
the	O
value	O
of	O
c	O
is	O
c	O
=	O
25	O
,	O
for	O
the	O
k-nearest	O
neighbor	B
rule	I
c	O
=	O
(	O
1	O
+	O
2yd	O
)	O
2	O
,	O
and	O
for	O
the	O
moving	B
window	I
rule	I
c	O
=	O
(	O
1	O
+	O
2f3d	O
)	O
2.	O
also	O
show	O
that	O
the	O
deleted	B
version	O
l~p	O
d	O
)	O
of	O
the	O
estimate	B
satisfies	O
the	O
same	O
inequalities	O
(	O
lugosi	O
and	O
pawlak	O
(	O
1994	O
»	O
.	O
hint	O
:	O
proceed	O
as	O
in	O
the	O
proof	O
of	O
theorem	O
31.2.	O
problem	O
31.2.	O
show	O
that	O
the	O
deleted	B
posterior	O
probability	O
estimate	B
of	O
the	O
error	O
probability	O
of	O
a	O
histogram	O
rule	B
is	O
always	O
optimistically	O
biased	O
,	O
that	O
is	O
,	O
for	O
all	O
n	O
,	O
and	O
all	O
distributions	O
,	O
e	O
{	O
l~pd	O
)	O
}	O
:	O
s	O
el	O
n-	O
l	O
.	O
problem	O
31.3.	O
show	O
that	O
for	O
any	O
classification	O
rule	B
and	O
any	O
estimate	B
0	O
:	O
s	O
tjn	O
(	O
x	O
,	O
dn	O
)	O
:	O
s	O
1	O
of	O
the	O
a	O
posteriori	O
probabilities	O
,	O
for	O
all	O
distributions	O
of	O
(	O
x	O
,	O
y	O
)	O
for	O
alli	O
,	O
n	O
,	O
and	O
e	O
>	O
0	O
p	O
{	O
il~~	O
)	O
-	O
e	O
{	O
l~~	O
)	O
}	O
i	O
:	O
:	O
:	O
e	O
i	O
dn	O
}	O
:	O
s	O
2e-2le2	O
,	O
and	O
var	O
{	O
l~~	O
)	O
idn	O
}	O
:	O
:	O
:	O
;	O
1/	O
i.	O
further	O
,	O
show	O
that	O
for	O
alii	O
,	O
e	O
{	O
l~~	O
)	O
}	O
=	O
e	O
{	O
l~~~	O
)	O
}	O
.	O
problem	O
31.4.	O
empirical	O
squared	O
error	O
.	O
consider	O
the	O
deleted	B
empirical	O
squared	B
error	I
show	O
that	O
2	O
}	O
e	O
{	O
en	O
}	O
=	O
2	O
+	O
e	O
(	O
tj	O
(	O
x	O
)	O
-	O
17n-l	O
(	O
x	O
,	O
dn-d	O
)	O
lnn	O
{	O
,	O
where	O
lnn	O
is	O
the	O
asymptotic	O
error	O
probability	O
on	O
the	O
i-nearest	O
neighbor	B
rule	I
.	O
show	O
that	O
if	O
tjn-j	O
is	O
the	O
histogram	O
,	O
kernel	B
,	O
or	O
k-nn	O
estimate	B
,	O
then	O
var	O
{	O
en	O
}	O
:	O
s	O
c/n	O
for	O
some	O
constant	O
depending	O
on	O
the	O
dimension	B
only	O
.	O
we	O
see	O
that	O
en	O
is	O
an	O
asymptotically	O
optimistically	O
biased	O
estimate	B
of	O
l	O
(	O
gn	O
)	O
when	O
tjn-j	O
is	O
an	O
l	O
2	O
(	O
p	O
,	O
)	O
-consistent	O
estimate	B
of	O
tj	O
.	O
still	O
,	O
this	O
estimate	B
is	O
useful	O
in	O
classifier	O
selection	B
(	O
see	O
theorem	B
29.2	O
)	O
.	O
32	O
feature	B
extraction	I
32.1	O
dimensionality	O
reduction	O
so	O
far	O
,	O
we	O
have	O
not	O
addressed	O
the	O
question	O
of	O
how	O
the	O
components	O
of	O
the	O
feature	O
vector	O
x	O
are	O
obtained	O
.	O
in	O
general	O
,	O
these	O
components	O
are	O
based	O
on	O
d	O
measurements	O
of	O
the	O
object	O
to	O
be	O
classified	O
.	O
how	O
many	O
measurements	O
should	O
be	O
made	O
?	O
what	O
should	O
these	O
measurements	O
be	O
?	O
we	O
study	O
these	O
questions	O
in	O
this	O
chapter	O
.	O
general	O
recipes	O
are	O
hard	O
to	O
give	O
as	O
the	O
answers	O
depend	O
on	O
the	O
specific	O
problem	O
.	O
however	O
,	O
there	O
are	O
some	O
rules	O
of	O
thumb	O
that	O
should	O
be	O
followed	O
.	O
one	O
such	O
rule	B
is	O
that	O
noisy	O
measurements	O
,	O
that	O
is	O
,	O
components	O
that	O
are	O
independent	O
of	O
y	O
,	O
should	O
be	O
avoided	O
.	O
also	O
,	O
adding	O
a	O
component	O
that	O
is	O
a	O
function	O
of	O
other	O
components	O
is	O
useless	O
.	O
a	O
nec	O
(	O
cid:173	O
)	O
essary	O
and	O
sufficient	O
condition	O
for	O
measurements	O
providing	O
additional	O
information	O
is	O
given	O
in	O
problem	O
32.1.	O
our	O
goal	O
,	O
of	O
course	O
,	O
is	O
to	O
make	O
the	O
error	O
probability	O
l	O
(	O
gn	O
)	O
as	O
small	O
as	O
possible	O
.	O
this	O
depends	O
on	O
many	O
things	O
,	O
such	O
as	O
the	O
joint	O
distribution	B
of	O
the	O
selected	O
compo	O
(	O
cid:173	O
)	O
nents	O
and	O
the	O
label	O
y	O
,	O
the	O
sample	O
size	O
,	O
and	O
the	O
classification	O
rule	B
gn	O
'	O
to	O
make	O
things	O
a	O
bit	O
simpler	O
,	O
we	O
first	O
investigate	O
the	O
bayes	O
errors	O
corresponding	O
to	O
the	O
selected	O
components	O
.	O
this	O
approach	O
makes	O
sense	O
,	O
since	O
the	O
bayes	O
error	O
is	O
the	O
theoretical	B
limit	O
of	O
the	O
performance	O
of	O
any	O
classifier	B
.	O
as	O
problem	O
2.1	O
indicates	O
,	O
collecting	O
more	O
measurements	O
can	O
not	O
increase	O
the	O
bayes	O
error	O
.	O
on	O
the	O
other	O
hand	O
,	O
having	O
too	O
many	O
components	O
is	O
not	O
desirable	O
.	O
just	O
recall	O
the	O
curse	B
of	I
dimensionality	I
that	O
we	O
often	O
faced	O
:	O
to	O
get	O
good	O
error	O
rates	O
,	O
the	O
number	O
of	O
training	O
samples	O
should	O
be	O
exponentially	O
large	O
in	O
the	O
number	O
of	O
components	O
.	O
also	O
,	O
computational	O
and	O
storage	O
limitations	O
may	O
prohibit	O
us	O
from	O
working	O
with	O
many	O
components	O
.	O
we	O
may	O
formulate	O
the	O
feature	O
selection	O
problem	O
as	O
follows	O
:	O
let	O
x	O
(	O
l	O
)	O
,	O
...	O
,	O
xed	O
)	O
562	O
32.	O
feature	B
extraction	I
be	O
random	O
variables	O
representing	O
d	O
measurements	O
.	O
for	O
a	O
set	O
a	O
~	O
{	O
i	O
,	O
...	O
,	O
d	O
}	O
of	O
indices	O
,	O
let	O
x	O
a	O
denote	O
the	O
i	O
a	O
i-dimensional	O
random	O
vector	O
,	O
whose	O
components	O
are	O
the	O
x	O
(	O
i	O
)	O
's	O
with	O
i	O
e	O
a	O
(	O
in	O
the	O
order	O
of	O
increasing	O
indices	O
)	O
.	O
define	O
l	O
*	O
(	O
a	O
)	O
=	O
inf	O
g	O
:	O
rjai	O
--	O
+	O
{	O
o	O
,	O
i	O
}	O
p	O
{	O
g	O
(	O
xa	O
)	O
=i	O
y	O
}	O
as	O
the	O
bayes	O
error	O
corresponding	O
to	O
the	O
pair	O
(	O
xa	O
'	O
y	O
)	O
.	O
l*	O
(	O
{	O
3	O
}	O
)	O
figure	O
32.1.	O
a	O
possible	O
ar	O
(	O
cid:173	O
)	O
rangement	O
of	O
l*	O
(	O
a	O
)	O
's	O
for	O
d	O
=	O
3.	O
l*	O
(	O
{	O
1,3	O
}	O
)	O
112	O
l*	O
(	O
0	O
)	O
l*	O
(	O
{	O
i	O
}	O
)	O
o	O
obviously	O
,	O
l	O
*	O
(	O
a	O
)	O
:	O
:s	O
l	O
*	O
(	O
b	O
)	O
whenever	O
b	O
c	O
a	O
,	O
and	O
l	O
*	O
(	O
0	O
)	O
=	O
min	O
(	O
p	O
{	O
y	O
=	O
oj	O
,	O
p	O
{	O
y	O
=	O
i	O
}	O
)	O
.	O
the	O
problem	O
is	O
to	O
find	O
an	O
efficient	O
way	O
of	O
selecting	O
an	O
index	O
set	O
a	O
with	O
iai	O
=	O
k	O
,	O
whose	O
corresponding	O
bayes	O
error	O
is	O
the	O
smallest	O
.	O
here	O
k	O
<	O
d	O
is	O
a	O
fixed	O
integer	O
.	O
exhaustive	O
search	O
through	O
the	O
(	O
~	O
)	O
possibilities	O
is	O
often	O
unde	O
(	O
cid:173	O
)	O
sirable	O
because	O
of	O
the	O
imposed	O
computational	O
burden	O
.	O
many	O
attempts	O
have	O
been	O
made	O
to	O
find	O
fast	O
algorithms	O
to	O
obtain	O
the	O
best	O
subset	O
of	O
features	O
.	O
see	O
fu	O
,	O
min	O
,	O
and	O
li	O
(	O
1970	O
)	O
,	O
kanal	O
(	O
1974	O
)	O
,	O
ben-bassat	O
(	O
1982	O
)	O
,	O
and	O
devijver	O
and	O
kittler	O
(	O
1982	O
)	O
for	O
surveys	O
.	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
best	O
k	O
individual	O
features-that	O
is	O
,	O
components	O
corresponding	O
to	O
the	O
k	O
smallest	O
values	O
of	O
l	O
*	O
(	O
{	O
i	O
}	O
)	O
-do	O
not	O
necessarily	O
constitute	O
the	O
best	O
k-dimensional	O
vector	O
:	O
just	O
consider	O
a	O
case	O
in	O
which	O
x	O
(	O
l	O
)	O
==	O
x	O
(	O
2	O
)	O
==	O
...	O
==	O
x	O
(	O
k	O
)	O
.	O
cover	O
and	O
van	O
campenhout	O
(	O
1977	O
)	O
showed	O
that	O
any	O
ordering	O
of	O
the	O
2d	O
subsets	O
of	O
{	O
l	O
,	O
...	O
,	O
d	O
}	O
consistent	O
with	O
the	O
obvious	O
requirement	O
l	O
*	O
(	O
a	O
)	O
:	O
:	O
:	O
l	O
*	O
(	O
b	O
)	O
if	O
b	O
~	O
a	O
is	O
possible	O
.	O
more	O
precisely	O
,	O
they	O
proved	O
the	O
following	O
surprising	O
result	O
:	O
theorem	B
32.1	O
.	O
(	O
cover	O
and	O
van	O
campenhout	O
(	O
1977	O
)	O
)	O
.	O
let	O
ai	O
,	O
a	O
2	O
,	O
...	O
,	O
a2d	O
be	O
an	O
ordering	O
of	O
the	O
2d	O
subsets	O
of	O
{	O
i	O
,	O
...	O
,	O
d	O
}	O
,	O
satisfying	O
the	O
consistency	B
property	O
i	O
<	O
j	O
if	O
ai	O
c	O
a	O
j	O
•	O
(	O
therefore	O
,	O
al	O
=	O
0	O
and	O
a2d	O
=	O
{	O
i	O
,	O
...	O
,	O
d	O
}	O
.	O
)	O
then	O
there	O
exists	O
a	O
distribution	O
of	O
the	O
random	O
variables	O
(	O
x	O
,	O
y	O
)	O
=	O
(	O
x	O
(	O
!	O
)	O
,	O
...	O
,	O
xed	O
)	O
,	O
y	O
)	O
such	O
that	O
the	O
theorem	B
shows	O
that	O
every	O
feature	O
selection	O
algorithm	B
that	O
finds	O
the	O
best	O
k	O
(	O
cid:173	O
)	O
element	O
subset	O
has	O
to	O
search	O
exhaustively	O
through	O
all	O
k-element	O
subsets	O
for	O
some	O
distributions	O
.	O
any	O
other	O
method	O
is	O
doomed	O
to	O
failure	O
for	O
some	O
distribution	B
.	O
many	O
suboptimal	O
,	O
heuristic	O
algorithms	O
have	O
been	O
introduced	O
trying	O
to	O
avoid	O
the	O
compu	O
(	O
cid:173	O
)	O
tational	O
demand	O
of	O
exhaustive	O
search	O
(	O
see	O
,	O
e.g.	O
,	O
sebestyen	O
(	O
1962	O
)	O
,	O
meisel	O
(	O
1972	O
)	O
,	O
32.1	O
dimensionality	O
reduction	O
563.	O
chang	O
(	O
1973	O
)	O
,	O
vilmansen	O
(	O
1973	O
)	O
,	O
and	O
devijver	O
and	O
kittler	O
(	O
1982	O
»	O
.	O
narendra	O
and	O
fukunaga	O
(	O
1977	O
)	O
introduced	O
an	O
efficient	O
branch-and-bound	B
method	I
that	O
finds	O
the	O
optimal	O
set	O
of	O
features	O
.	O
their	O
method	O
avoids	O
searching	O
through	O
all	O
subsets	O
in	O
many	O
cases	O
by	O
making	O
use	O
of	O
the	O
monotonicity	O
of	O
the	O
bayes	O
error	O
with	O
respect	O
to	O
the	O
partial	O
ordering	O
of	O
the	O
subsets	O
.	O
the	O
key	O
of	O
our	O
proof	O
ofthe	O
theorem	B
is	O
the	O
following	O
simple	O
lemma	O
:	O
lemma	O
32.1.	O
let	O
ai	O
,	O
''	O
''	O
a2d	O
be	O
as	O
in	O
theorem	O
32.1.	O
let	O
1	O
<	O
i	O
<	O
2d	O
.	O
assume	O
that	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
on	O
nd	O
x	O
{	O
o	O
,	O
i	O
}	O
is	O
such	O
that	O
the	O
distribution	B
of	O
x	O
is	O
concentrated	O
on	O
a	O
finite	O
set	O
,	O
l	O
*	O
(	O
a2d	O
)	O
=	O
l	O
*	O
(	O
{	O
l	O
,	O
...	O
,	O
2d	O
}	O
)	O
=	O
0	O
,	O
and	O
l	O
*	O
(	O
a	O
j	O
)	O
<	O
1/2	O
for	O
each	O
i	O
<	O
}	O
:	O
:	O
:	O
2d	O
.	O
then	O
there	O
exists	O
another	O
finite	O
distribution	B
such	O
that	O
l	O
*	O
(	O
a	O
j	O
)	O
remains	O
unchanged	O
for	O
each	O
}	O
>	O
i	O
,	O
and	O
*	O
l	O
(	O
a	O
j	O
)	O
<	O
l	O
(	O
ai	O
)	O
<	O
2	O
foreach	O
}	O
>	O
i	O
.	O
*	O
1	O
proof	O
.	O
we	O
denote	O
the	O
original	O
distribution	B
of	O
x	O
by	O
{	O
vi	O
and	O
the	O
a	B
posteriori	I
probability	I
function	O
by	O
fj	O
.	O
we	O
may	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
every	O
atom	O
of	O
the	O
distribution	B
of	O
x	O
is	O
in	O
[	O
0	O
,	O
m	O
)	O
d	O
for	O
some	O
m	O
>	O
0.	O
since	O
l	O
*	O
(	O
{	O
l	O
,	O
...	O
,	O
d	O
}	O
)	O
=	O
0	O
,	O
the	O
value	O
of	O
fj	O
(	O
x	O
)	O
is	O
either	O
zero	O
or	O
one	O
at	O
each	O
atom	O
.	O
we	O
construct	O
the	O
new	O
distribution	B
by	O
duplicating	O
each	O
atom	O
in	O
a	O
special	O
way	O
.	O
we	O
describe	O
the	O
new	O
distribution	B
by	O
defining	O
a	O
measure	O
{	O
vi	O
'	O
on	O
nd	O
and	O
an	O
a	O
posteriori	O
function	O
fj	O
'	O
:	O
nd	O
~	O
{	O
o	O
,	O
i	O
}	O
.	O
define	O
the	O
vector	O
vai	O
e	O
nd	O
such	O
that	O
its	O
m-th	O
component	O
equals	O
m	O
if	O
m	O
1	O
:	O
.	O
ai	O
,	O
and	O
zero	O
if	O
m	O
e	O
ai	O
.	O
the	O
new	O
measure	B
{	O
vi	O
'	O
has	O
twice	O
as	O
many	O
atoms	O
as	O
{	O
vi	O
.	O
for	O
each	O
atom	O
x	O
e	O
nd	O
of	O
{	O
vi	O
,	O
the	O
new	O
measure	B
{	O
vi	O
!	O
has	O
two	O
atoms	O
,	O
namely	O
,	O
xl	O
=	O
x	O
and	O
x2	O
=	O
x	O
+	O
va	O
i	O
•	O
the	O
new	O
distribution	B
is	O
specified	O
by	O
{	O
vi	O
'	O
(	O
xd	O
=	O
q	O
{	O
vi	O
(	O
x	O
)	O
,	O
{	O
vi	O
'	O
(	O
x2	O
)	O
=	O
(	O
1	O
-	O
q	O
)	O
{	O
vi	O
(	O
x	O
)	O
,	O
fj	O
'	O
(	O
xd	O
=	O
fj	O
(	O
x	O
)	O
,	O
and	O
fj	O
'	O
(	O
x2	O
)	O
=	O
1	O
-	O
fj	O
(	O
x	O
)	O
,	O
where	O
q	O
e	O
(	O
0	O
,	O
1/2	O
)	O
is	O
specified	O
later	O
.	O
it	O
remains	O
for	O
us	O
to	O
verify	O
that	O
this	O
distribution	B
satisfies	O
the	O
requirements	O
of	O
the	O
lemma	O
for	O
some	O
q.	O
first	O
observe	O
that	O
the	O
values	O
l	O
*	O
(	O
a	O
j	O
)	O
remain	O
unchanged	O
for	O
all	O
}	O
>	O
i.	O
this	O
follows	O
from	O
the	O
fact	O
that	O
there	O
is	O
at	O
least	O
one	O
component	O
in	O
a	O
j	O
along	O
which	O
the	O
new	O
set	O
of	O
atoms	O
is	O
strictly	O
separated	O
from	O
the	O
old	O
one	O
,	O
leaving	O
the	O
corresponding	O
contribution	O
to	O
the	O
bayes	O
error	O
unchanged	O
.	O
on	O
the	O
other	O
hand	O
,	O
as	O
we	O
vary	O
q	O
from	O
zero	O
to	O
1/2	O
,	O
the	O
new	O
value	O
of	O
l	O
*	O
(	O
ai	O
)	O
grows	O
continuously	O
from	O
the	O
old	O
value	O
of	O
l	O
*	O
(	O
aj	O
to	O
1/2	O
.	O
therefore	O
,	O
since	O
by	O
assumption	O
max	O
j	O
>	O
i	O
l	O
*	O
(	O
a	O
j	O
)	O
<	O
1/2	O
,	O
there	O
exists	O
a	O
value	O
of	O
q	O
such	O
that	O
the	O
new	O
l	O
*	O
(	O
ai	O
)	O
satisfies	O
maxj	O
>	O
i	O
l	O
*	O
(	O
a	O
j	O
)	O
<	O
l	O
*	O
(	O
ai	O
)	O
<	O
1/2	O
as	O
desired	O
.	O
0	O
proof	O
of	O
theorem	O
32.1.	O
we	O
construct	O
the	O
desired	O
distribution	B
in	O
2d	O
-	O
2	O
steps	O
,	O
applying	O
lemma	O
32.1	O
in	O
each	O
step	O
.	O
the	O
procedure	O
for	O
d	O
=	O
3	O
is	O
illustrated	O
in	O
figure	O
32.2	O
.	O
564	O
32.	O
feature	B
extraction	I
cd	O
ag	O
=	O
{	O
1,2,3	O
}	O
cd	O
cd	O
cd	O
cd	O
cd	O
as=	O
{	O
2	O
}	O
!	O
--	O
-	O
<	O
;	O
>	O
i	O
i	O
i	O
i	O
cd	O
figure	O
32.2.	O
construction	O
of	O
a	O
prespecijied	O
ordering	O
.	O
in	O
this	O
three-dimensional	O
example	O
,	O
the	O
first	O
four	O
steps	O
of	O
the	O
procedure	O
are	O
shown	O
,	O
when	O
the	O
desired	O
ordering	O
is	O
l*	O
(	O
{	O
1	O
,	O
2	O
,	O
3	O
}	O
)	O
~	O
l*	O
(	O
{	O
2	O
,	O
3	O
}	O
)	O
~	O
l*	O
(	O
{	O
1	O
,	O
2	O
}	O
)	O
~	O
l*	O
(	O
{	O
2	O
}	O
)	O
~	O
l*	O
(	O
{	O
1	O
,	O
3	O
}	O
)	O
~	O
...	O
.	O
black	O
circles	O
represent	O
atoms	O
with	O
11	O
=	O
1	O
and	O
white	O
circles	O
are	O
those	O
with	O
11	O
=	O
o.	O
we	O
start	O
with	O
a	O
monoatomic	O
distribution	B
concentrated	O
at	O
the	O
origin	O
,	O
with	O
11	O
(	O
0	O
)	O
=	O
1.	O
then	O
clearly	O
,	O
l	O
*	O
(	O
a2d	O
)	O
=	O
o.	O
by	O
lemma	O
32.1	O
,	O
we	O
construct	O
a	O
distribution	O
such	O
that	O
l*	O
(	O
a2d	O
)	O
=oando=	O
l*	O
(	O
a2d	O
)	O
<	O
l*	O
(	O
a2d_d	O
<	O
1/2	O
.	O
by	O
applying	O
the	O
lemma	O
again	O
,	O
we	O
can	O
construct	O
a	O
distribution	O
with	O
0	O
=	O
l	O
*	O
(	O
a2d	O
)	O
<	O
l	O
*	O
(	O
a2d-l	O
)	O
<	O
l	O
*	O
(	O
a2d_2	O
)	O
<	O
1/2	O
.	O
after	O
i	O
steps	O
,	O
we	O
have	O
a	O
distribution	O
satisfying	O
the	O
last	O
i	O
inequalities	O
of	O
the	O
desired	O
ordering	O
.	O
the	O
construction	O
is	O
finished	O
after	O
2d	O
-	O
2	O
steps	O
.	O
0	O
remark	O
.	O
the	O
original	O
example	O
of	O
cover	O
and	O
van	O
campenhout	O
(	O
1977	O
)	O
uses	O
the	O
multidimensional	O
gaussian	B
distribution	I
.	O
van	O
campenhout	O
(	O
1980	O
)	O
developed	O
the	O
idea	O
further	O
by	O
showing	O
that	O
not	O
only	O
all	O
possible	O
orderings	O
,	O
but	O
all	O
possible	O
values	O
of	O
the	O
l	O
*	O
(	O
ai	O
)	O
's	O
can	O
be	O
achieved	O
by	O
some	O
distributions	O
.	O
the	O
distribution	B
constructed	O
in	O
the	O
above	O
proof	O
is	O
discrete	O
.	O
it	O
has	O
22d	O
-2	O
atoms	O
.	O
0	O
one	O
may	O
suspect	O
that	O
feature	B
extraction	I
is	O
much	O
easier	O
if	O
given	O
y	O
,	O
the	O
com	O
(	O
cid:173	O
)	O
ponents	O
x	O
(	O
1	O
)	O
,	O
...	O
,	O
x	O
(	O
d	O
)	O
are	O
conditionally	O
independent	O
.	O
however	O
,	O
three	O
and	O
four	O
(	O
cid:173	O
)	O
dimensional	O
examples	O
given	O
by	O
elashoff	O
,	O
elashoff	O
,	O
and	O
goldman	O
(	O
1967	O
)	O
,	O
toussaint	O
(	O
1971	O
)	O
,	O
and	O
cover	O
(	O
1974	O
)	O
show	O
that	O
even	O
the	O
individually	O
best	O
two	O
independent	O
components	O
are	O
not	O
the	O
best	O
pair	O
of	O
components	O
.	O
we	O
do	O
not	O
know	O
if	O
theorem	B
32.1	O
32.1	O
dimensionality	O
reduction	O
565	O
generalizes	O
to	O
the	O
case	O
when	O
the	O
components	O
are	O
conditionally	O
independent	O
.	O
in	O
the	O
next	O
example	O
,	O
the	O
pair	O
of	O
components	O
consisting	O
of	O
the	O
two	O
worst	O
single	O
features	O
is	O
the	O
best	O
pair	O
,	O
and	O
vice	O
versa	O
.	O
theorem	B
32.2	O
.	O
(	O
toussaint	O
(	O
1971	O
)	O
)	O
.	O
there	O
exist	O
binary-valued	O
random	O
variables	O
xl	O
,	O
x	O
2	O
,	O
x3	O
,	O
y	O
e	O
{	O
o	O
,	O
i	O
}	O
such	O
that	O
xl	O
,	O
x	O
2	O
,	O
and	O
x3	O
are	O
conditionally	O
independent	O
(	O
given	O
y	O
)	O
,	O
and	O
l	O
*	O
(	O
{	O
1	O
}	O
)	O
<	O
l	O
*	O
(	O
{	O
2	O
}	O
)	O
<	O
l	O
*	O
(	O
{	O
3	O
}	O
)	O
,	O
but	O
l	O
*	O
(	O
{	O
1	O
,	O
2	O
}	O
)	O
>	O
l	O
*	O
(	O
{	O
1	O
,	O
3	O
}	O
)	O
>	O
l	O
*	O
(	O
{	O
2	O
,	O
3	O
}	O
)	O
.	O
proof	O
.	O
let	O
p	O
{	O
y	O
=	O
i	O
}	O
=	O
1/2	O
.	O
then	O
the	O
joint	O
distribution	B
of	O
xl	O
,	O
x2	O
,	O
x3	O
,	O
y	O
is	O
spec	O
(	O
cid:173	O
)	O
ified	O
by	O
the	O
conditional	O
probabilities	O
p	O
{	O
xi	O
=	O
11y	O
=	O
o	O
}	O
and	O
p	O
{	O
xi	O
=	O
lly	O
=	O
1	O
}	O
,	O
i	O
=	O
1	O
,	O
2	O
,	O
3.	O
straightforward	O
calculation	O
shows	O
that	O
the	O
values	O
p	O
{	O
xi	O
=	O
lly	O
=	O
o	O
}	O
=	O
0.1	O
,	O
p	O
{	O
x2	O
=	O
iiy	O
=	O
o	O
}	O
=	O
0.05	O
,	O
p	O
{	O
x3	O
=	O
iiy	O
=	O
o	O
}	O
=	O
0.01	O
,	O
p	O
{	O
xi	O
=	O
iiy	O
=	O
i	O
}	O
=	O
0.9	O
,	O
p	O
{	O
x2	O
=	O
iiy	O
=	O
1	O
}	O
=	O
0.8	O
,	O
p	O
{	O
x3	O
=	O
iiy	O
=	O
i	O
}	O
=	O
0.71	O
satisfy	O
the	O
stated	O
inequalities	O
.	O
0	O
as	O
our	O
ultimate	O
goal	O
is	O
to	O
minimize	O
the	O
error	O
probability	O
,	O
finding	O
the	O
feature	O
set	O
minimizing	O
the	O
bayes	O
error	O
is	O
not	O
the	O
best	O
we	O
can	O
do	O
.	O
for	O
example	O
,	O
,	O
if	O
we	O
know	O
that	O
we	O
will	O
use	O
the	O
3-nearest	O
neighbor	B
rule	I
,	O
then	O
it	O
makes	O
more	O
sense	O
to	O
select	O
the	O
set	O
of	O
features	O
that	O
minimizes	O
the	O
asymptotic	O
error	O
probability	O
l3nn	O
of	O
the	O
3-nearest	O
neighbor	B
rule	I
.	O
recall	O
from	O
chapter	O
5	O
that	O
l3nn	O
=	O
e	O
{	O
17	O
(	O
x	O
)	O
(	O
1	O
-	O
17	O
(	O
x	O
)	O
)	O
(	O
1	O
+	O
417	O
(	O
x	O
)	O
(	O
1	O
-	O
17	O
(	O
x	O
)	O
)	O
)	O
}	O
.	O
the	O
situation	O
here	O
is	O
even	O
messier	O
than	O
for	O
the	O
bayes	O
error	O
.	O
as	O
the	O
next	O
example	O
shows	O
,	O
it	O
is	O
not	O
even	O
true	O
that	O
a	O
c	O
b	O
implies	O
l3nn	O
(	O
a	O
)	O
:	O
:	O
:	O
:	O
l3nn	O
(	O
b	O
)	O
,	O
where	O
a	O
,	O
b	O
~	O
{	O
i	O
,	O
...	O
,	O
d	O
}	O
are	O
two	O
subsets	O
of	O
components	O
,	O
and	O
l3nn	O
(	O
a	O
)	O
denotes	O
the	O
asymptotic	O
error	O
probability	O
of	O
the	O
3-nearest	O
neighbor	B
rule	I
for	O
(	O
xa	O
'	O
y	O
)	O
.	O
in	O
other	O
words	O
,	O
adding	O
components	O
may	O
increase	O
l3nn	O
!	O
this	O
can	O
never	O
happen	O
to	O
the	O
bayes	O
error-and	O
in	O
fact	O
,	O
to	O
any	O
f	O
-error	O
(	O
theorem	B
3.3	O
)	O
.	O
the	O
anomaly	O
is	O
due	O
to	O
the	O
fact	O
that	O
the	O
function	O
x	O
(	O
1	O
-	O
x	O
)	O
(	O
1	O
+	O
4x	O
(	O
1	O
-	O
x	O
)	O
)	O
is	O
convex	O
near	O
zero	O
and	O
one	O
.	O
example	O
.	O
let	O
the	O
joint	O
distribution	B
of	O
x	O
=	O
(	O
xl	O
,	O
x2	O
)	O
be	O
uniform	B
on	O
[	O
0,2	O
]	O
2.	O
the	O
joint	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
is	O
defined	O
by	O
the	O
a	O
posteriori	O
probabilities	O
given	O
by	O
17	O
(	O
x	O
)	O
=	O
[	O
0.1	O
~	O
0.9	O
if	O
x	O
e	O
[	O
0	O
,	O
1/2	O
)	O
x	O
[	O
0	O
,	O
1/2	O
)	O
if	O
x	O
e	O
[	O
1/2	O
,	O
1	O
]	O
x	O
[	O
0	O
,	O
1/2	O
)	O
if	O
x	O
e	O
[	O
0	O
,	O
1/2	O
)	O
x	O
[	O
1/2	O
,	O
1	O
]	O
if	O
x	O
e	O
[	O
1/2	O
x	O
1	O
]	O
x	O
[	O
1/2	O
,	O
1	O
]	O
566	O
32.	O
feature	B
extraction	I
(	O
figure	O
32.3	O
)	O
.	O
straightforward	O
calculations	O
show	O
that	O
l3nn	O
(	O
{	O
i	O
,	O
2	O
}	O
)	O
=	O
0.0612	O
,	O
while	O
l3nn	O
(	O
{	O
2	O
}	O
)	O
=	O
0.056525	O
,	O
a	O
smaller	O
value	O
!	O
0	O
figure	O
32.3.	O
anexamplewhen	O
an	O
additionalfeature	O
increases	O
the	O
error	O
probability	O
of	O
the	O
3-	O
nn	O
rule	B
.	O
t\=0.9	O
l3nn	O
(	O
{	O
1,2	O
)	O
)	O
=	O
0.0612	O
l'nn	O
(	O
(	O
2	O
}	O
)	O
=	O
0.056525	O
~~	O
--	O
--	O
--	O
--	O
~	O
--	O
--	O
--	O
~	O
r1'=o.l	O
of	O
course	O
,	O
the	O
real	O
measure	B
of	O
the	O
goodness	O
of	O
the	O
selected	O
feature	O
set	O
is	O
the	O
error	O
probability	O
l	O
(	O
gn	O
)	O
of	O
the	O
classifier	B
designed	O
by	O
using	O
training	O
data	O
dn	O
.	O
if	O
the	O
classification	O
rule	B
gn	O
is	O
not	O
known	O
at	O
the	O
stage	O
of	O
feature	O
selection	B
,	O
then	O
the	O
best	O
one	O
can	O
do	O
is	O
to	O
estimate	B
the	O
bayes	O
errors	O
l	O
*	O
(	O
a	O
)	O
for	O
each	O
set	O
a	O
of	O
features	O
,	O
and	O
select	O
a	O
feature	O
set	O
by	O
minimizing	O
the	O
estimate	B
.	O
unfortunately	O
,	O
as	O
theorem	B
8.5	O
shows	O
,	O
no	O
method	O
of	O
estimating	O
the	O
bayes	O
errors	O
can	O
guarantee	O
good	O
performance	O
.	O
if	O
we	O
know	O
what	O
classifier	B
will	O
be	O
used	O
after	O
feature	O
selection	O
,	O
then	O
the	O
best	O
strategy	O
is	O
to	O
select	O
a	O
set	O
of	O
measurements	O
based	O
on	O
comparing	O
estimates	O
of	O
the	O
error	O
probabilities	O
.	O
we	O
do	O
not	O
pursue	O
this	O
question	O
further	O
.	O
for	O
special	O
cases	O
,	O
we	O
do	O
not	O
need	O
to	O
mount	O
a	O
big	O
search	O
for	O
the	O
best	O
features	O
.	O
here	O
is	O
a	O
simple	O
example	O
:	O
given	O
y	O
=	O
i	O
,	O
let	O
x	O
=	O
(	O
x	O
(	O
1	O
)	O
,	O
...	O
,	O
xed	O
)	O
)	O
have	O
d	O
independent	O
components	O
,	O
where	O
given	O
y	O
=	O
i	O
,	O
x	O
(	O
j	O
)	O
is	O
normal	B
(	O
m	O
ji	O
,	O
a	O
}	O
)	O
.	O
it	O
is	O
easy	O
to	O
verify	O
(	O
problem	O
32.2	O
)	O
that	O
if	O
p	O
{	O
y	O
=	O
i	O
}	O
=	O
1/2	O
,	O
then	O
l*=p	O
{	O
n	O
>	O
~	O
}	O
,	O
where	O
n	O
is	O
a	O
standard	O
normal	B
random	O
variable	B
,	O
and	O
~	O
(	O
m.l-m.o	O
)	O
2	O
]	O
r2	O
=	O
l	O
j=l	O
]	O
cyj	O
is	O
the	O
square	O
of	O
the	O
mahalanobis	O
distance	B
(	O
see	O
also	O
duda	O
and	O
hart	O
(	O
1973	O
,	O
pp	O
.	O
66-67	O
)	O
)	O
.	O
for	O
this	O
case	O
,	O
the	O
quality	O
of	O
the	O
j-th	O
feature	O
is	O
measured	O
by	O
we	O
may	O
as	O
well	O
rank	O
these	O
values	O
,	O
and	O
given	O
that	O
we	O
need	O
only	O
d	O
'	O
<	O
d	O
features	O
,	O
we	O
are	O
best	O
off	O
taking	O
the	O
d	O
'	O
features	O
with	O
the	O
highest	O
quality	O
index	O
.	O
32.2	O
transformations	O
with	O
small	O
distortion	O
567	O
it	O
is	O
possible	O
to	O
come	O
up	O
with	O
analytic	O
solutions	O
in	O
other	O
special	O
cases	O
as	O
well	O
.	O
for	O
example	O
,	O
raudys	O
(	O
1976	O
)	O
and	O
raudys	O
and	O
pikelis	O
(	O
1980	O
)	O
investigated	O
the	O
de	O
(	O
cid:173	O
)	O
pendence	O
of	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
on	O
the	O
dimension	B
of	O
the	O
feature	O
space	O
in	O
the	O
case	O
of	O
certain	O
linear	O
classifiers	O
and	O
normal	B
distributions	O
.	O
they	O
point	O
out	O
that	O
for	O
a	O
fixed	O
n	O
,	O
by	O
increasing	O
the	O
number	O
of	O
features	O
,	O
the	O
expected	O
error	O
probability	O
e	O
{	O
l	O
(	O
gn	O
)	O
}	O
first	O
decreases	O
,	O
and	O
then	O
,	O
after	O
reaching	O
an	O
optimum	O
,	O
grows	O
again	O
.	O
32.2	O
transformations	O
with	O
small	O
distortion	O
one	O
may	O
view	O
the	O
problem	O
of	O
feature	O
extraction	O
in	O
general	O
as	O
the	O
problem	O
of	O
find	O
(	O
cid:173	O
)	O
ing	O
a	O
transformation	O
(	O
i.e.	O
,	O
a	O
function	O
)	O
t	O
:	O
nd	O
-+	O
nd	O
so	O
that	O
the	O
bayes	O
error	O
l~	O
corresponding	O
to	O
the	O
pair	O
(	O
t	O
(	O
x	O
)	O
,	O
y	O
)	O
is	O
close	O
to	O
the	O
bayes	O
error	O
l	O
*	O
corresponding	O
to	O
the	O
pair	O
(	O
x	O
,	O
y	O
)	O
.	O
one	O
typical	O
example	O
of	O
such	O
transformations	O
is	O
fine	O
quantization	B
(	O
i.e.	O
,	O
discretization	O
)	O
of	O
x	O
,	O
when	O
t	O
maps	O
the	O
observed	O
values	O
into	O
a	O
set	O
of	O
finitely	O
,	O
or	O
countably	O
infinitely	O
many	O
values	O
.	O
reduction	B
of	I
dimensionality	O
of	O
the	O
observa	O
(	O
cid:173	O
)	O
tions	O
can	O
be	O
put	O
in	O
this	O
framework	O
as	O
well	O
.	O
in	O
the	O
following	O
result	O
we	O
show	O
that	O
small	O
distortion	O
of	O
the	O
observation	O
can	O
not	O
cause	O
large	O
increase	O
in	O
the	O
bayes	O
error	O
probability	O
.	O
theorem	B
32.3	O
.	O
(	O
farag6	O
and	O
gyorfi	O
(	O
1975	O
)	O
)	O
.	O
assume	O
that	O
for	O
a	O
sequence	O
of	O
transformations	O
tn	O
,	O
n	O
=	O
1,2	O
,	O
...	O
in	O
probability	O
,	O
where	O
ii	O
.	O
ii	O
denotes	O
the	O
euclidean	O
norm	O
in	O
nd	O
.	O
then	O
,	O
if	O
l	O
*	O
is	O
the	O
bayes	O
error	O
for	O
(	O
x	O
,	O
y	O
)	O
and	O
l~n	O
is	O
the	O
bayes	O
error	O
for	O
(	O
tn	O
(	O
x	O
)	O
,	O
y	O
)	O
,	O
l*	O
-+	O
l*	O
.	O
t	O
''	O
proof	O
.	O
for	O
arbitrarily	O
small	O
e	O
>	O
0	O
we	O
can	O
choose	O
a	O
uniformly	O
continuous	O
function	O
o	O
:	O
:	O
:	O
;	O
ij	O
(	O
x	O
)	O
:	O
:	O
:	O
;	O
1	O
such	O
that	O
2e	O
{	O
lry	O
(	O
x	O
)	O
-	O
ij	O
(	O
x	O
)	O
i	O
}	O
<	O
e.	O
for	O
any	O
transformation	O
t	O
,	O
consider	O
now	O
the	O
decision	O
problem	O
of	O
(	O
t	O
(	O
x	O
)	O
,	O
y	O
)	O
,	O
where	O
the	O
random	O
variable	B
y	O
satisfies	O
p	O
{	O
y	O
=	O
11	O
x	O
=	O
x	O
}	O
=	O
ij	O
(	O
x	O
)	O
for	O
every	O
x	O
e	O
nd	O
.	O
denote	O
the	O
corresponding	O
bayes	O
error	O
by	O
i~	O
,	O
and	O
the	O
bayes	O
error	O
corresponding	O
to	O
the	O
pair	O
(	O
x	O
,	O
y	O
)	O
by	O
i	O
*	O
.	O
obviously	O
0	O
:	O
:	O
:	O
;	O
£*	O
-£*	O
:	O
:	O
:	O
;	O
1£*	O
-i*i+li*	O
-i*i+li*-l*i	O
.	O
(	O
32.1	O
)	O
tn	O
t	O
''	O
tn	O
tn	O
to	O
bound	O
the	O
first	O
and	O
third	O
terms	O
on	O
the	O
right-hand	O
side	O
,	O
observe	O
that	O
for	O
any	O
transformation	O
t	O
,	O
p	O
{	O
y	O
=	O
11	O
t	O
(	O
x	O
)	O
}	O
=	O
e	O
{	O
p	O
{	O
y	O
=	O
l1x	O
}	O
1	O
t	O
(	O
x	O
)	O
}	O
=	O
e	O
{	O
ry	O
(	O
x	O
)	O
1	O
t	O
(	O
x	O
)	O
}	O
.	O
568	O
32.	O
feature	B
extraction	I
therefore	O
,	O
the	O
bayes	O
error	O
corresponding	O
to	O
the	O
pair	O
(	O
t	O
(	O
x	O
)	O
,	O
y	O
)	O
equals	O
l~	O
=	O
e	O
{	O
min	O
(	O
e	O
{	O
ry	O
(	O
x	O
)	O
it	O
(	O
x	O
)	O
}	O
,	O
1	O
-	O
e	O
{	O
ry	O
(	O
x	O
)	O
it	O
(	O
x	O
)	O
}	O
)	O
}	O
.	O
thus	O
,	O
il~	O
-	O
i~1	O
ie	O
{	O
min	O
(	O
e	O
{	O
ry	O
(	O
x	O
)	O
1	O
t	O
(	O
x	O
)	O
}	O
,	O
1	O
-	O
e	O
{	O
ry	O
(	O
x	O
)	O
1	O
t	O
(	O
x	O
)	O
}	O
)	O
}	O
-	O
e	O
{	O
min	O
(	O
e	O
{	O
ry	O
(	O
x	O
)	O
it	O
(	O
x	O
)	O
}	O
,	O
1	O
-	O
e	O
{	O
ry	O
(	O
x	O
)	O
it	O
(	O
x	O
)	O
}	O
)	O
}	O
1	O
<	O
e	O
{	O
ie	O
{	O
ry	O
(	O
x	O
)	O
it	O
(	O
x	O
)	O
}	O
-	O
e	O
{	O
ry	O
(	O
x	O
)	O
it	O
(	O
x	O
)	O
}	O
1	O
}	O
<	O
e	O
{	O
lry	O
(	O
x	O
)	O
-	O
ry	O
(	O
x	O
)	O
i	O
}	O
(	O
by	O
jensen	O
's	O
inequality	B
)	O
<	O
e	O
,	O
so	O
the	O
first	O
and	O
third	O
terms	O
of	O
(	O
32.1	O
)	O
are	O
less	O
than	O
e.	O
for	O
the	O
second	O
term	O
,	O
define	O
the	O
decision	O
function	O
gn	O
(	O
x	O
)	O
=	O
{	O
if	O
ry	O
(	O
x	O
)	O
:	O
:	O
:	O
;	O
1/2	O
0	O
1	O
otherwise	O
,	O
which	O
has	O
error	O
probability	O
i	O
(	O
gn	O
)	O
=	O
p	O
{	O
gn	O
(	O
tn	O
(	O
x	O
)	O
)	O
=i	O
y	O
}	O
.	O
then	O
i	O
(	O
gn	O
)	O
~	O
ii	O
''	O
and	O
we	O
have	O
by	O
theorem	B
2.2.	O
all	O
we	O
have	O
to	O
show	O
now	O
is	O
that	O
the	O
limit	O
supremum	O
of	O
the	O
above	O
quantity	O
does	O
not	O
exceed	O
e.	O
let	O
o	O
(	O
e	O
)	O
be	O
the	O
inverse	O
modulus	O
of	O
continuity	O
of	O
the	O
a	B
posteriori	I
probability	I
ry	O
,	O
that	O
is	O
,	O
o	O
(	O
e	O
)	O
=	O
sup	O
{	O
llx	O
yll	O
:2iry	O
(	O
x	O
)	O
-ry	O
(	O
y	O
)	O
1	O
<	O
e	O
}	O
.	O
for	O
every	O
e	O
>	O
0	O
,	O
we	O
have	O
o	O
(	O
e	O
)	O
>	O
0	O
by	O
the	O
uniform	B
continuity	O
of	O
ry	O
.	O
now	O
,	O
we	O
have	O
2e	O
{	O
lry	O
(	O
7	O
;	O
1	O
(	O
x	O
)	O
)	O
-	O
ry	O
(	O
x	O
)	O
1	O
}	O
:	O
:	O
:	O
;	O
2e	O
{	O
i	O
{	O
iix-i	O
;	O
,	O
(	O
x	O
)	O
ii	O
>	O
8	O
(	O
e	O
)	O
}	O
}	O
+	O
2e	O
{	O
i	O
{	O
iix-i	O
;	O
,	O
(	O
x	O
)	O
ii	O
:	O
::8	O
(	O
e	O
)	O
}	O
iry	O
(	O
tn	O
(	O
x	O
)	O
)	O
-	O
ry	O
(	O
x	O
)	O
i	O
}	O
.	O
clearly	O
,	O
the	O
first	O
term	O
on	O
the	O
right-hand	O
side	O
converges	O
to	O
zero	O
by	O
assumption	O
,	O
while	O
the	O
second	O
term	O
does	O
not	O
exceed	O
e	O
by	O
the	O
definition	B
of	I
o	O
(	O
e	O
)	O
.	O
0	O
remark	O
.	O
it	O
is	O
clear	O
from	O
the	O
proof	O
of	O
theorem	O
32.3	O
that	O
everything	O
remains	O
true	O
if	O
the	O
observation	O
x	O
takes	O
its	O
values	O
from	O
a	O
separable	O
metric	B
space	O
with	O
metric	O
p	O
,	O
and	O
the	O
condition	O
of	O
the	O
theorem	B
is	O
modified	O
to	O
p	O
(	O
t	O
(	O
x	O
)	O
,	O
x	O
)	O
--	O
+	O
0	O
in	O
probability	O
.	O
this	O
generalization	O
has	O
significance	O
in	O
curve	O
recognition	O
,	O
when	O
x	O
is	O
a	O
stochastic	O
process	O
.	O
then	O
theorem	B
32.3	O
asserts	O
that	O
one	O
does	O
not	O
lose	O
much	O
information	O
by	O
using	O
usual	O
discretization	O
methods	O
,	O
such	O
as	O
,	O
for	O
example	O
,	O
karhunen-loeve	O
series	O
expansion	O
(	O
see	O
problems	O
32.3	O
to	O
32.5	O
)	O
.	O
0	O
32.3	O
admissible	O
and	O
sufficient	O
transformations	O
569	O
32.3	O
admissible	O
and	O
sufficient	O
transformations	O
sometimes	O
the	O
cost	O
of	O
guessing	O
zero	O
while	O
the	O
true	O
value	O
of	O
y	O
is	O
one	O
is	O
different	O
from	O
the	O
cost	O
of	O
guessing	O
one	O
,	O
while	O
y	O
=	O
o.	O
these	O
situations	O
may	O
be	O
handled	O
as	O
follows	O
.	O
define	O
the	O
costs	O
c	O
(	O
rn	O
,	O
i	O
)	O
,	O
rn	O
,	O
l	O
=	O
0	O
,	O
1.	O
here	O
c	O
(	O
y	O
,	O
g	O
(	O
x	O
»	O
is	O
the	O
cost	O
of	O
deciding	O
on	O
g	O
(	O
x	O
)	O
when	O
the	O
true	O
label	O
is	O
y.	O
the	O
risk	O
of	O
a	O
decision	O
function	O
g	O
is	O
defined	O
as	O
the	O
expected	O
value	O
of	O
the	O
cost	O
:	O
note	O
that	O
if	O
rg	O
=	O
e	O
{	O
c	O
(	O
y	O
,	O
g	O
(	O
x	O
»	O
}	O
.	O
l	O
)	O
=	O
{	O
i	O
if	O
rn	O
=ii	O
0	O
otherwise	O
,	O
c	O
(	O
rn	O
,	O
then	O
the	O
risk	O
is	O
just	O
the	O
probability	O
of	O
error	O
.	O
introduce	O
the	O
notation	O
qm	O
(	O
x	O
)	O
=	O
7	O
]	O
(	O
x	O
)	O
c	O
(	O
1	O
,	O
rn	O
)	O
+	O
(	O
1	O
-	O
7	O
]	O
(	O
x	O
»	O
c	O
(	O
o	O
,	O
rn	O
)	O
,	O
rn	O
=	O
0	O
,	O
1.	O
then	O
we	O
have	O
the	O
following	O
extension	O
of	O
theorem	O
2.1	O
:	O
theorem	B
32.4.	O
define	O
g	O
(	O
x	O
)	O
=	O
{	O
01	O
if	O
ql	O
(	O
x	O
)	O
>	O
qo	O
(	O
x	O
)	O
otherwise	O
.	O
then	O
for	O
all	O
decision	O
functions	O
g	O
we	O
have	O
rg	O
:	O
:	O
:	O
:	O
rg	O
.	O
rg	O
is	O
called	O
the	O
bayes	O
risk	O
.	O
the	O
proof	O
is	O
left	O
as	O
an	O
exercise	O
(	O
see	O
problem	O
32.7	O
)	O
.	O
which	O
transformations	O
preserve	O
all	O
the	O
necessary	O
information	O
in	O
the	O
sense	O
that	O
the	O
bayes	O
error	O
probability	O
corresponding	O
to	O
the	O
pair	O
(	O
t	O
(	O
x	O
)	O
,	O
y	O
)	O
equals	O
that	O
of	O
(	O
x	O
,	O
y	O
)	O
?	O
clearly	O
,	O
every	O
invertible	O
mapping	O
t	O
has	O
this	O
property	O
.	O
however	O
,	O
the	O
practically	O
interesting	O
transformations	O
are	O
the	O
ones	O
that	O
provide	O
some	O
compression	O
of	O
the	O
data	O
.	O
the	O
most	O
efficient	O
of	O
such	O
transformations	O
is	O
the	O
bayes	O
decision	O
g*	O
itself	O
:	O
g*	O
is	O
specifically	O
designed	O
to	O
minimize	O
the	O
error	O
probability	O
.	O
if	O
the	O
goal	O
is	O
to	O
minimize	O
the	O
bayes	O
risk	O
with	O
respect	O
to	O
some	O
other	O
cost	O
function	O
than	O
the	O
error	O
probability	O
,	O
then	O
g*	O
generally	O
fails	O
to	O
preserve	O
the	O
bayes	O
risk	O
.	O
it	O
is	O
natural	O
to	O
ask	O
what	O
transformations	O
preserve	O
the	O
bayes	O
risk	O
for	O
all	O
possible	O
cost	O
functions	O
.	O
this	O
question	O
has	O
a	O
practical	O
significance	O
,	O
when	O
collecting	O
data	O
and	O
construction	O
of	O
the	O
decision	O
are	O
separated	O
in	O
space	O
or	O
in	O
time	O
.	O
in	O
such	O
cases	O
the	O
data	O
should	O
be	O
transmitted	O
via	O
a	O
communication	O
channel	B
(	O
or	O
should	O
be	O
stored	O
)	O
.	O
in	O
both	O
cases	O
there	O
is	O
a	O
need	O
for	O
an	O
efficient	O
data	O
compression	O
rule	B
.	O
in	O
this	O
problem	O
formulation	O
,	O
when	O
getting	O
the	O
data	O
,	O
one	O
may	O
not	O
know	O
the	O
final	O
cost	O
function	O
.	O
therefore	O
a	O
desirable	O
data	O
compression	O
(	O
transformation	O
)	O
does	O
not	O
increase	O
the	O
bayes	O
risk	O
for	O
any	O
cost	O
function	O
c	O
(	O
-	O
,	O
.	O
)	O
.	O
here	O
t	O
is	O
a	O
measurable	O
function	O
mapping	O
from	O
rd	O
to	O
rd	O
'	O
for	O
some	O
positive	O
integer	O
d	O
'	O
.	O
570	O
32.	O
feature	B
extraction	I
definition	O
32.1.	O
let	O
r~	O
t	O
denote	O
the	O
bayes	O
riskfor	O
the	O
costfunction	O
c	O
and	O
trans	O
(	O
cid:173	O
)	O
formed	O
observation	O
t	O
(	O
x	O
)	O
.	O
a	O
transformation	O
t	O
is	O
called	O
admissible	O
iffor	O
any	O
cost	O
function	O
c	O
,	O
r~	O
,	O
t	O
=	O
r~	O
,	O
where	O
r~	O
denotes	O
the	O
bayes	O
risk	O
for	O
the	O
original	O
observation	O
.	O
obviously	O
each	O
invertible	O
transformation	O
t	O
is	O
admissible	O
.	O
a	O
nontrivial	O
example	O
of	O
an	O
admissible	B
transformation	I
is	O
t*	O
(	O
x	O
)	O
=	O
1	O
]	O
(	O
x	O
)	O
,	O
since	O
according	O
to	O
theorem	B
32.4	O
,	O
the	O
bayes	O
decision	O
for	O
any	O
cost	O
function	O
can	O
be	O
constructed	O
by	O
the	O
a	B
posteriori	I
probability	I
17	O
(	O
x	O
)	O
and	O
by	O
the	O
cost	O
function	O
.	O
surpris	O
(	O
cid:173	O
)	O
ingly	O
,	O
this	O
is	O
basically	O
the	O
only	O
such	O
transformation	O
in	O
the	O
following	O
sense	O
:	O
theorem	B
32.5.	O
a	O
transformation	O
t	O
is	O
admissible	O
if	O
and	O
only	O
if	O
there	O
is	O
a	O
mapping	O
g	O
such	O
that	O
g	O
(	O
t	O
(	O
x	O
)	O
)	O
=	O
1	O
]	O
(	O
x	O
)	O
with	O
probability	O
one	O
.	O
proof	O
.	O
the	O
converse	O
is	O
easy	O
since	O
for	O
such	O
g	O
r~	O
:	O
:	O
:	O
r~	O
,	O
t	O
(	O
x	O
)	O
:	O
:	O
:	O
r~	O
,	O
g	O
(	O
t	O
(	O
x	O
»	O
=	O
r~	O
.	O
assume	O
now	O
that	O
t	O
is	O
admissible	O
but	O
such	O
function	O
g	O
does	O
not	O
exist	O
.	O
then	O
there	O
is	O
a	O
set	O
a	O
c	O
nd	O
such	O
that	O
fl	O
(	O
a	O
)	O
>	O
0	O
,	O
t	O
(	O
x	O
)	O
is	O
constant	O
on	O
a	O
,	O
while	O
all	O
values	O
then	O
there	O
are	O
real	O
numbers	O
°	O
<	O
c	O
<	O
1	O
and	O
e	O
>	O
0	O
,	O
and	O
sets	O
b	O
,	O
dca	O
such	O
that	O
of	O
1	O
]	O
(	O
x	O
)	O
are	O
different	O
on	O
a	O
,	O
that	O
is	O
,	O
if	O
x	O
,	O
yea	O
,	O
then	O
x	O
=i	O
y	O
implies	O
1	O
]	O
(	O
x	O
)	O
=l1	O
]	O
(	O
y	O
)	O
.	O
fl	O
(	O
b	O
)	O
,	O
fl	O
(	O
d	O
)	O
>	O
0	O
,	O
and	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
>	O
c	O
+	O
e	O
if	O
x	O
e	O
b	O
,	O
l-1	O
]	O
(	O
x	O
)	O
<	O
c-e	O
if	O
xed	O
.	O
now	O
,	O
choose	O
a	O
cost	O
function	O
with	O
the	O
following	O
values	O
:	O
co	O
,	O
o	O
=	O
ci	O
,	O
i	O
=	O
0	O
,	O
co	O
,	O
i	O
=	O
1	O
,	O
and	O
then	O
,	O
ci	O
,	O
o=	O
-	O
-	O
.	O
1	O
-	O
c	O
c	O
qo	O
(	O
x	O
)	O
=	O
1	O
]	O
(	O
x	O
)	O
,	O
qi	O
(	O
x	O
)	O
ci	O
,	O
o	O
(	O
1	O
-	O
1	O
]	O
(	O
x	O
)	O
)	O
,	O
and	O
the	O
bayes	O
decision	O
on	O
bud	O
is	O
given	O
by	O
*	O
(	O
x	O
)	O
=	O
{	O
o	O
if	O
c	O
<	O
~	O
-	O
1	O
otherwise	O
.	O
g	O
1	O
]	O
(	O
x	O
)	O
32.3	O
admissible	O
and	O
sufficient	O
transformations	O
571	O
now	O
,	O
let	O
get	O
(	O
x	O
»	O
be	O
an	O
arbitrary	O
decision	O
.	O
without	O
loss	O
of	O
generality	O
we	O
can	O
assume	O
that	O
g	O
(	O
t	O
(	O
x	O
»	O
=	O
0	O
if	O
x	O
e	O
a.	O
then	O
the	O
difference	O
between	O
the	O
risk	O
of	O
g	O
(	O
t	O
(	O
x	O
»	O
and	O
the	O
bayes	O
risk	O
is	O
jrd	O
=	O
r	O
(	O
qg	O
(	O
t	O
(	O
x	O
»	O
(	O
x	O
)	O
-	O
qg*	O
(	O
x	O
)	O
(	O
x	O
»	O
m	O
(	O
dx	O
)	O
>	O
i	O
(	O
qo	O
(	O
x	O
)	O
-	O
ql	O
(	O
x	O
»	O
m	O
(	O
dx	O
)	O
=	O
i	O
(	O
7j	O
(	O
x	O
)	O
-	O
cl	O
,	O
o	O
(	O
1	O
-	O
7j	O
(	O
x	O
»	O
m	O
(	O
dx	O
)	O
=	O
i	O
(	O
1	O
-	O
1	O
-	O
c~	O
(	O
x	O
)	O
)	O
jl	O
(	O
dx	O
)	O
2	O
:	O
~jl	O
(	O
d	O
)	O
>	O
o.	O
d	O
we	O
can	O
give	O
another	O
characterization	O
of	O
admissible	O
transformations	O
by	O
virtue	O
of	O
a	O
well-known	O
concept	O
of	O
mathematical	O
statistics	B
:	O
definition	O
32.2.	O
t	O
(	O
x	O
)	O
is	O
called	O
a	O
sufficient	O
statistic	O
if	O
the	O
random	O
variables	O
x	O
,	O
t	O
(	O
x	O
)	O
,	O
and	O
y	O
form	O
a	O
markov	O
chain	O
in	O
this	O
order	O
.	O
that	O
is	O
,	O
for	O
any	O
set	O
a	O
,	O
p	O
{	O
y	O
e	O
ait	O
(	O
x	O
)	O
,	O
x	O
}	O
=	O
pry	O
e	O
ait	O
(	O
x	O
)	O
}	O
.	O
theorem	B
32.6.	O
a	O
transformation	O
t	O
is	O
admissible	O
if	O
and	O
only	O
ift	O
(	O
x	O
)	O
is	O
a	O
sufficient	O
statistic	O
.	O
proof	O
.	O
assume	O
that	O
t	O
is	O
admissible	O
.	O
then	O
according	O
to	O
theorem	B
32.5	O
there	O
is	O
a	O
mapping	O
g	O
such	O
that	O
then	O
and	O
g	O
(	O
t	O
(	O
x	O
»	O
=	O
7j	O
(	O
x	O
)	O
with	O
probability	O
one	O
.	O
p	O
{	O
y	O
=	O
iit	O
(	O
x	O
)	O
,	O
x	O
}	O
=	O
p	O
{	O
y	O
=	O
llx	O
}	O
=	O
l1	O
(	O
x	O
)	O
=	O
g	O
(	O
t	O
(	O
x	O
»	O
,	O
p	O
{	O
y	O
=	O
iit	O
(	O
x	O
)	O
}	O
=	O
e	O
{	O
p	O
{	O
y	O
=	O
lit	O
(	O
x	O
)	O
,	O
x	O
}	O
it	O
(	O
x	O
)	O
}	O
=	O
e	O
{	O
g	O
(	O
t	O
(	O
x	O
»	O
it	O
(	O
x	O
)	O
}	O
=	O
g	O
(	O
t	O
(	O
x	O
»	O
,	O
thus	O
,	O
p	O
{	O
y	O
=	O
iit	O
(	O
x	O
)	O
,	O
x	O
}	O
=	O
p	O
{	O
y	O
=	O
lit	O
(	O
x	O
)	O
}	O
,	O
therefore	O
t	O
(	O
x	O
)	O
is	O
sufficient	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
t	O
(	O
x	O
)	O
is	O
sufficient	O
,	O
then	O
p	O
{	O
y	O
=	O
llx	O
}	O
=	O
p	O
{	O
y	O
=	O
iit	O
(	O
x	O
)	O
,	O
x	O
}	O
=	O
p	O
{	O
y	O
=	O
iit	O
(	O
x	O
)	O
}	O
,	O
572	O
32.	O
feature	B
extraction	I
so	O
for	O
the	O
choice	O
g	O
(	O
t	O
(	O
x	O
)	O
)	O
=	O
p	O
{	O
y	O
=	O
iit	O
(	O
x	O
)	O
}	O
we	O
have	O
the	O
desired	O
function	O
g	O
(	O
·	O
)	O
,	O
and	O
therefore	O
t	O
is	O
admissible	O
.	O
0	O
theorem	B
32.6	O
states	O
that	O
we	O
may	O
replace	O
x	O
by	O
any	O
sufficient	O
statistic	O
t	O
(	O
x	O
)	O
without	O
altering	O
the	O
bayes	O
error	O
.	O
the	O
problem	O
with	O
this	O
,	O
in	O
practice	O
,	O
is	O
that	O
we	O
do	O
not	O
know	O
the	O
sufficient	B
statistics	I
because	O
we	O
do	O
not	O
know	O
the	O
distribution	B
.	O
if	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
is	O
known	O
to	O
some	O
extent	O
,	O
then	O
theorem	B
32.6	O
may	O
be	O
useful	O
.	O
example	O
.	O
assume	O
that	O
it	O
is	O
known	O
that	O
1	O
]	O
(	O
x	O
)	O
=	O
e-cllxll	O
for	O
some	O
unknown	O
c	O
>	O
o.	O
then	O
ii	O
x	O
ii	O
is	O
a	O
sufficient	O
statistic	O
.	O
thus	O
,	O
for	O
discrimination	O
,	O
we	O
may	O
replace	O
the	O
d-dimensional	O
vector	O
x	O
by	O
the	O
i-dimensional	O
random	O
variable	B
iixii	O
without	O
loss	O
of	O
discrimination	O
power	O
.	O
0	O
example	O
.	O
if	O
1	O
]	O
(	O
x	O
(	O
1	O
)	O
,	O
x	O
(	O
2	O
)	O
,	O
x	O
(	O
3	O
»	O
)	O
=	O
1	O
]	O
0	O
(	O
x	O
(	O
1	O
)	O
x	O
(	O
2	O
)	O
,	O
x	O
(	O
2	O
)	O
x	O
(	O
3	O
»	O
)	O
for	O
some	O
function	O
1	O
]	O
0	O
,	O
then	O
(	O
x	O
(	O
1	O
)	O
x	O
(	O
2	O
)	O
,	O
x	O
(	O
2	O
)	O
x	O
(	O
3	O
»	O
)	O
is	O
a	O
2-dimensional	O
sufficient	O
statistic	O
.	O
for	O
discrimination	O
,	O
there	O
is	O
no	O
need	O
to	O
deal	O
with	O
x	O
(	O
l	O
)	O
,	O
x	O
(	O
2	O
)	O
,	O
x	O
(	O
3	O
)	O
.	O
it	O
suffices	O
to	O
extract	O
the	O
features	O
x	O
(	O
1	O
)	O
x	O
(	O
2	O
)	O
and	O
x	O
(	O
2	O
)	O
x	O
(	O
3	O
)	O
•	O
0	O
example	O
.	O
if	O
given	O
y	O
=	O
i	O
,	O
x	O
is	O
normal	B
with	O
unknown	O
mean	O
mi	O
and	O
diagonal	O
covariance	O
matrix	O
a	O
2	O
i	O
(	O
for	O
unknown	O
a	O
)	O
,	O
then	O
1	O
]	O
(	O
x	O
)	O
is	O
a	O
function	O
of	O
iix	O
-	O
m111	O
2	O
ii	O
x	O
-	O
mo	O
112	O
only	O
for	O
unknown	O
mo	O
,	O
mi	O
.	O
here	O
we	O
have	O
no	O
obvious	O
sufficient	O
statistic	O
.	O
however	O
,	O
if	O
mo	O
and	O
m	O
1	O
are	O
both	O
known	O
,	O
then	O
a	O
quick	O
inspection	O
shows	O
that	O
x	O
t	O
(	O
m	O
1	O
-	O
mo	O
)	O
is	O
a	O
i-dimensional	O
sufficient	O
statistic	O
.	O
again	O
,	O
it	O
suffices	O
to	O
look	O
for	O
the	O
simplest	O
possible	O
argument	O
for	O
1	O
]	O
.	O
if	O
mo	O
=	O
m	O
1	O
=	O
0	O
but	O
the	O
covariance	O
matrices	O
are	O
ag	O
i	O
and	O
al	O
i	O
given	O
that	O
y	O
=	O
0	O
or	O
y	O
=	O
1	O
,	O
then	O
iixii	O
2	O
is	O
a	O
sufficient	O
statistic	O
.	O
0	O
-	O
in	O
summary	O
,	O
the	O
results	O
of	O
this	O
section	O
are	O
useful	O
for	O
picking	O
out	O
features	O
when	O
some	O
theoretical	B
information	O
is	O
available	O
regarding	O
the	O
distribution	B
of	O
(	O
x	O
,	O
y	O
)	O
.	O
problems	O
and	O
exercises	O
problem32.1	O
.	O
consider	O
the	O
pair	O
(	O
x	O
,	O
y	O
)	O
e	O
rdx	O
{	O
o	O
,	O
l	O
}	O
ofrandomvariables	O
,	O
andletxcd+l	O
)	O
e	O
r	O
be	O
an	O
additional	O
component	O
.	O
define	O
the	O
augmented	O
random	O
vector	O
x	O
'	O
=	O
(	O
x	O
,	O
x	O
cd+l	O
)	O
.	O
denote	O
the	O
bayes	O
errors	O
corresponding	O
to	O
the	O
pairs	O
(	O
x	O
,	O
y	O
)	O
and	O
(	O
x	O
'	O
,	O
y	O
)	O
by	O
l~	O
and	O
l~	O
,	O
respectively	O
.	O
clearly	O
,	O
l	O
~	O
:	O
:	O
:	O
l	O
~/	O
.	O
prove	O
that	O
equality	O
holds	O
if	O
and	O
only	O
if	O
p	O
{	O
i	O
{	O
ri'cx/	O
»	O
1/2	O
}	O
=i	O
i	O
{	O
t/cx	O
»	O
1/2	O
}	O
}	O
=	O
0	O
,	O
where	O
the	O
a	B
posteriori	I
probability	I
functions	O
r	O
;	O
:	O
rd	O
-+	O
{	O
o	O
,	O
i	O
}	O
and	O
r	O
;	O
'	O
:	O
r	O
d+1	O
-+	O
{	O
o	O
,	O
i	O
}	O
are	O
defined	O
as	O
r	O
;	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
llx	O
=	O
x	O
}	O
and	O
r	O
;	O
'	O
(	O
x	O
'	O
)	O
=	O
p	O
{	O
y	O
=	O
llx	O
'	O
=	O
x	O
'	O
)	O
.	O
hint	O
:	O
consult	O
with	O
the	O
proof	O
of	O
theorem	O
32.5.	O
problem	O
32.2.	O
letp	O
{	O
y	O
=	O
i	O
}	O
=	O
1/2	O
,	O
andgiveny	O
=	O
i	O
,	O
i	O
=	O
0	O
,	O
1	O
,	O
letx	O
=	O
(	O
xcl	O
)	O
,	O
...	O
,	O
xcd	O
»	O
)	O
have	O
d	O
independent	O
components	O
,	O
where	O
x	O
(	O
j	O
)	O
is	O
normal	B
(	O
m	O
ji	O
,	O
a	O
}	O
)	O
.	O
prove	O
that	O
l	O
*	O
=	O
p	O
{	O
n	O
>	O
r/2	O
}	O
,	O
where	O
n	O
is	O
a	O
standard	O
normal	B
random	O
variable	B
,	O
and	O
r2	O
is	O
the	O
square	O
of	O
the	O
mahalanobis	O
distance	B
:	O
r2	O
=	O
l~=l	O
(	O
(	O
m	O
jl	O
-	O
m	O
jo	O
)	O
/aj	O
)	O
2	O
(	O
dud	O
a	O
and	O
hart	O
(	O
1973	O
,	O
pp	O
.	O
66-67	O
»	O
.	O
problems	O
and	O
exercises	O
573	O
problem	O
32.3.	O
sampling	B
of	I
a	O
stochastic	B
process	I
.	O
let	O
x	O
(	O
t	O
)	O
,	O
t	O
e	O
[	O
0	O
,	O
1	O
]	O
,	O
be	O
a	O
stochas	O
(	O
cid:173	O
)	O
tic	O
process	O
(	O
i.e.	O
,	O
a	O
collection	O
of	O
real-valued	O
random	O
variables	O
indexed	O
by	O
t	O
)	O
,	O
and	O
let	O
y	O
be	O
a	O
binary	O
random	O
variable	B
.	O
for	O
integer	O
n	O
>	O
0	O
,	O
define	O
xw	O
=	O
x	O
(	O
i	O
/	O
n	O
)	O
,	O
i	O
:	O
:	O
:	O
1	O
,	O
...	O
,	O
n.	O
find	O
sufficient	O
conditions	O
on	O
the	O
function	O
met	O
)	O
=e	O
{	O
x	O
(	O
t	O
)	O
}	O
and	O
on	O
the	O
covariance	O
function	O
k	O
(	O
t	O
,	O
s	O
)	O
:	O
:	O
:	O
e	O
{	O
(	O
x	O
(	O
t	O
)	O
-	O
e	O
{	O
x	O
(	O
t	O
)	O
}	O
)	O
(	O
x	O
(	O
s	O
)	O
-	O
e	O
{	O
x	O
(	O
s	O
)	O
}	O
)	O
}	O
such	O
that	O
lim	O
l	O
*	O
(	O
xcj	O
)	O
=	O
l	O
*	O
(	O
x	O
(	O
·	O
»	O
,	O
n	O
--	O
+oo	O
where	O
l*	O
(	O
x~	O
)	O
is	O
the	O
bayes	O
error	O
corresponding	O
to	O
(	O
x~	O
)	O
,	O
...	O
,	O
xf/\	O
and	O
l*	O
(	O
x	O
(	O
·	O
)	O
is	O
the	O
infimum	O
of	O
the	O
error	O
probabilities	O
of	O
decision	O
functions	O
that	O
map	O
measurable	O
functions	O
into	O
{	O
o	O
,	O
i	O
}	O
.	O
hint	O
:	O
introduce	O
the	O
stochastic	B
process	I
xn	O
(	O
t	O
)	O
as	O
the	O
linear	O
interpolation	O
of	O
x~l	O
)	O
,	O
...	O
,	O
x	O
(	O
n	O
)	O
.	O
find	O
conditions	O
under	O
which	O
lim	O
e	O
{	O
ll	O
(	O
xn	O
(	O
t	O
)	O
-	O
x	O
(	O
t	O
)	O
)	O
2dt	O
}	O
=	O
0	O
,	O
n	O
--	O
+oo	O
0	O
and	O
use	O
theorem	B
32.3	O
,	O
and	O
the	O
remark	O
following	O
it	O
.	O
problem	O
32.4.	O
expansion	B
of	I
a	O
stochastic	B
process	I
.	O
let	O
x	O
(	O
t	O
)	O
,	O
met	O
)	O
,	O
and	O
k	O
(	O
t	O
,	O
s	O
)	O
be	O
as	O
in	O
the	O
previous	O
problem	O
.	O
let	O
0/1	O
,	O
0/2	O
,	O
...	O
be	O
a	O
complete	O
orthonormal	O
system	O
of	O
functions	O
on	O
[	O
0	O
,	O
1	O
]	O
.	O
define	O
find	O
conditions	O
under	O
which	O
lim	O
l	O
*	O
(	O
x	O
(	O
1	O
)	O
,	O
...	O
,	O
x	O
(	O
n	O
)	O
)	O
=	O
l	O
*	O
(	O
x	O
(	O
·	O
)	O
.	O
n	O
--	O
+oo	O
problem	O
32.5.	O
extend	O
theorem	B
32.3	O
such	O
that	O
the	O
transformations	O
tn	O
(	O
x	O
,	O
d	O
,	O
j	O
are	O
allowed	O
to	O
depend	O
on	O
the	O
training	O
data	O
.	O
this	O
extension	O
has	O
significance	O
,	O
because	O
feature	B
extraction	I
algorithms	O
use	O
the	O
training	O
data	O
.	O
problem	O
32.6.	O
for	O
discrete	O
x	O
prove	O
that	O
t	O
(	O
x	O
)	O
is	O
sufficient	O
iff	O
y	O
,	O
t	O
(	O
x	O
)	O
,	O
x	O
form	O
a	O
markov	O
chain	O
(	O
in	O
this	O
order	O
)	O
.	O
problem	O
32.7.	O
prove	O
theorem	B
32.4.	O
problem	O
32.8.	O
recall	O
the	O
definition	B
of	I
f	O
-errors	O
from	O
chapter	O
3.	O
let	O
f	O
be	O
a	O
strictly	O
con	O
(	O
cid:173	O
)	O
cave	O
function	O
.	O
show	O
that	O
df	O
(	O
x	O
,	O
y	O
)	O
=	O
df	O
(	O
t	O
(	O
x	O
)	O
,	O
y	O
)	O
if	O
and	O
only	O
if	O
the	O
transformation	O
t	O
is	O
admissible	O
.	O
conclude	O
that	O
lnn	O
(	O
x	O
,	O
y	O
)	O
=	O
lnn	O
(	O
t	O
(	O
x	O
)	O
,	O
y	O
)	O
construct	O
a	O
t	O
and	O
a	O
distribution	O
of	O
(	O
x	O
,	O
y	O
)	O
such	O
that	O
l	O
*	O
(	O
x	O
,	O
y	O
)	O
=	O
l	O
*	O
(	O
t	O
(	O
x	O
)	O
,	O
y	O
)	O
,	O
but	O
t	O
is	O
not	O
admissible	O
.	O
problem	O
32.9.	O
find	O
sufficient	B
statistics	I
of	O
minimal	O
dimension	B
for	O
the	O
following	O
discrimi	O
(	O
cid:173	O
)	O
nation	O
problems	O
:	O
(	O
1	O
)	O
it	O
is	O
known	O
that	O
for	O
two	O
given	O
sets	O
a	O
and	O
b	O
with	O
a	O
n	O
b	O
=	O
0	O
,	O
if	O
y	O
=	O
1	O
,	O
we	O
have	O
x	O
e	O
a	O
and	O
if	O
y	O
=	O
0	O
,	O
then	O
x	O
e	O
b	O
,	O
or	O
vice	O
versa	O
.	O
(	O
2	O
)	O
given	O
y	O
,	O
x	O
is	O
a	O
vector	O
of	O
independent	O
gamma	B
random	O
variables	O
with	O
common	O
unknown	O
shape	O
parameter	O
a	O
and	O
common	O
unknown	O
scale	O
parameter	O
b	O
(	O
i.e.	O
,	O
the	O
marginal	O
density	O
of	O
each	O
component	O
of	O
x	O
is	O
of	O
the	O
form	O
xa-je-xjb	O
/	O
(	O
r	O
(	O
a	O
)	O
b	O
a	O
)	O
,	O
x	O
>	O
0	O
)	O
.	O
the	O
parameters	O
a	O
and	O
b	O
depend	O
upon	O
y.	O
problem	O
32.10.	O
let	O
x	O
have	O
support	B
on	O
the	O
surface	O
of	O
a	O
ball	O
of	O
n	O
d	O
centered	O
at	O
the	O
origin	O
of	O
unknown	O
radius	O
.	O
find	O
a	O
sufficient	O
statistic	O
for	O
discrimination	O
of	O
dimension	O
smaller	O
than	O
d.	O
574	O
32.	O
feature	B
extraction	I
problem	O
32.11.	O
assume	O
that	O
the	O
distribution	B
of	O
x	O
=	O
(	O
x	O
(	O
l	O
)	O
,	O
x	O
(	O
2	O
)	O
,	O
x	O
(	O
3	O
)	O
,	O
x	O
(	O
4	O
)	O
)	O
is	O
such	O
that	O
x	O
(	O
2	O
)	O
x	O
(	O
4	O
)	O
=	O
1	O
and	O
x	O
(	O
l	O
)	O
+	O
x	O
(	O
2	O
)	O
+	O
x	O
(	O
3	O
)	O
=	O
0	O
with	O
probability	O
one	O
.	O
find	O
a	O
simple	O
sufficient	O
statistic	O
.	O
appendix	O
in	O
this	O
appendix	O
we	O
summarize	O
some	O
basic	O
definitions	O
and	O
results	O
from	O
the	O
theory	O
of	O
probability	O
.	O
most	O
proofs	O
are	O
omitted	O
as	O
they	O
may	O
be	O
found	O
in	O
standard	O
textbooks	O
on	O
probability	O
,	O
such	O
as	O
ash	O
(	O
1972	O
)	O
,	O
shiryayev	O
(	O
1984	O
)	O
,	O
chow	O
and	O
teicher	O
(	O
1978	O
)	O
,	O
durrett	O
(	O
1991	O
)	O
,	O
grimmett	O
and	O
stirzaker	O
(	O
1992	O
)	O
,	O
and	O
zygmund	O
(	O
1959	O
)	O
.	O
we	O
also	O
give	O
a	O
list	O
of	O
useful	O
inequalities	O
that	O
are	O
used	O
in	O
the	O
text	O
.	O
a.i	O
basics	O
of	O
measure	O
theory	O
definition	O
a.l	O
.	O
let	O
s	O
be	O
a	O
set	O
,	O
and	O
let	O
f	O
be	O
a	O
family	O
of	O
subsets	O
of	O
s.	O
f	O
is	O
called	O
a	O
a	O
-algebra	O
if	O
(	O
i	O
)	O
0	O
e	O
f	O
,	O
(	O
io	O
a	O
e	O
f	O
(	O
iii	O
)	O
ai	O
,	O
a2	O
,	O
...	O
e	O
f	O
implies	O
u~l	O
ai	O
e	O
f.	O
implies	O
a	O
c	O
e	O
f	O
,	O
a	O
a	O
-algebra	O
is	O
closed	O
under	O
complement	O
and	O
union	O
of	O
countably	O
infinitely	O
many	O
sets	O
.	O
conditions	O
(	O
i	O
)	O
and	O
(	O
ii	O
)	O
imply	O
that	O
s	O
e	O
f.	O
moreover	O
,	O
(	O
ii	O
)	O
and	O
(	O
iii	O
)	O
imply	O
that	O
a	O
a-algebra	O
is	O
closed	O
under	O
countably	O
infinite	O
intersections	O
.	O
definition	O
a.2	O
.	O
let	O
s	O
be	O
a	O
set	O
,	O
and	O
let	O
f	O
be	O
a	O
a	O
-algebra	O
of	O
subsets	O
of	O
s.	O
then	O
(	O
s	O
,	O
:f	O
)	O
is	O
called	O
a	O
measurable	O
space	O
.	O
the	O
elements	O
of	O
f	O
are	O
called	O
measurable	O
sets	O
.	O
definition	O
a.3	O
.	O
if	O
s	O
=	O
nd	O
and	O
!	O
3	O
is	O
the	O
smallest	O
a-algebra	O
containing	O
all	O
rect	O
(	O
cid:173	O
)	O
angles	O
,	O
then	O
!	O
3	O
is	O
called	O
the	O
borel	O
a-algebra	O
.	O
the	O
elements	O
of	O
!	O
3	O
are	O
called	O
borel	O
sets	O
.	O
576	O
appendix	O
definition	O
aa	O
.	O
let	O
(	O
s	O
,	O
f	O
)	O
be	O
a	O
measurable	O
space	O
and	O
let	O
f	O
function	O
.	O
f	O
is	O
called	O
measurable	O
if	O
for	O
all	O
b	O
e	O
b	O
s	O
~	O
n	O
be	O
a	O
f-l	O
(	O
b	O
)	O
=	O
{	O
s	O
:	O
f	O
(	O
s	O
)	O
e	O
b	O
}	O
e	O
f	O
,	O
that	O
is	O
,	O
the	O
inverse	O
image	O
of	O
any	O
borel	O
set	O
b	O
is	O
in	O
f.	O
obviously	O
,	O
if	O
a	O
is	O
a	O
measurable	O
set	O
,	O
then	O
the	O
indicator	O
variable	B
ia	O
is	O
a	O
measur	O
(	O
cid:173	O
)	O
able	O
function	O
.	O
moreover	O
,	O
finite	O
linear	O
combinations	O
of	O
indicators	O
of	O
measurable	O
sets	O
(	O
called	O
simple	O
functions	O
)	O
are	O
also	O
measurable	O
functions	O
.	O
it	O
can	O
be	O
shown	O
that	O
the	O
set	O
of	O
measurable	O
functions	O
is	O
closed	O
under	O
addition	O
,	O
subtraction	O
,	O
multiplication	O
,	O
and	O
division	O
.	O
moreover	O
,	O
the	O
supremum	O
and	O
infimum	O
of	O
a	O
sequence	O
of	O
measurable	O
functions	O
,	O
as	O
well	O
as	O
its	O
pointwise	O
limit	O
supremum	O
and	O
limit	O
infimum	O
are	O
measur	O
(	O
cid:173	O
)	O
able	O
.	O
definition	O
a.5	O
.	O
let	O
(	O
s	O
,	O
f	O
)	O
be	O
a	O
measurable	O
space	O
and	O
let	O
v	O
:	O
f	O
~	O
[	O
0	O
,	O
(	O
0	O
)	O
be	O
a	O
function	O
.	O
v	O
is	O
a	O
measure	O
on	O
f	O
if	O
(	O
i	O
)	O
v	O
(	O
0	O
)	O
=	O
0	O
,	O
(	O
ii	O
)	O
v	O
is	O
(	O
j-additive	O
,	O
that	O
is	O
,	O
ai	O
,	O
a2	O
,	O
...	O
e	O
f	O
,	O
and	O
ai	O
n	O
aj	O
=	O
0	O
,	O
i	O
=i	O
j	O
imply	O
that	O
v	O
(	O
u~lai	O
)	O
=	O
2	O
:	O
~1	O
v	O
(	O
ai	O
)	O
.	O
in	O
other	O
words	O
,	O
a	O
measure	O
is	O
a	O
nonnegative	O
,	O
(	O
j	O
-additive	O
set	O
function	O
.	O
definition	O
a.6	O
.	O
v	O
is	O
a	O
finite	O
measure	B
if	O
v	O
(	O
s	O
)	O
<	O
00.	O
v	O
is	O
a	O
(	O
j	O
-finite	O
measure	B
if	O
there	O
are	O
countably	O
many	O
measurable	O
sets	O
ai	O
,	O
a	O
2	O
,	O
...	O
such	O
that	O
u~l	O
ai	O
=	O
sand	O
v	O
(	O
ai	O
)	O
<	O
00	O
,	O
i	O
=	O
1,2	O
,	O
...	O
.	O
definition	O
a.7	O
.	O
the	O
triple	O
(	O
s	O
,	O
f	O
,	O
v	O
)	O
is	O
a	O
measure	O
space	O
if	O
(	O
s	O
,	O
f	O
)	O
is	O
a	O
measurable	O
space	O
and	O
v	O
is	O
a	O
measure	O
on	O
f.	O
definition	O
a.8	O
.	O
the	O
lebesgue	O
measure	B
a	O
on	O
nd	O
is	O
a	O
measure	O
on	O
the	O
borel	O
(	O
j	O
-algebra	O
ofnd	O
such	O
that	O
the	O
a	O
measure	O
of	O
each	O
rectangle	O
equals	O
to	O
its	O
volume	O
.	O
a.2	O
the	O
lebesgue	O
integral	O
definition	O
a.9	O
.	O
let	O
(	O
s	O
,	O
f	O
,	O
v	O
)	O
be	O
a	O
measure	O
space	O
and	O
f	O
=	O
2:7=1	O
xjai	O
a	O
simple	O
function	O
such	O
that	O
the	O
measurable	O
sets	O
ai	O
,	O
...	O
,	O
an	O
are	O
disjoint	O
.	O
then	O
the	O
(	O
lebesgue	O
)	O
integral	O
of	O
f	O
with	O
respect	O
to	O
v	O
is	O
defined	O
by	O
f	O
fdv	O
=	O
is	O
f	O
(	O
s	O
)	O
v	O
(	O
ds	O
)	O
=	O
txiv	O
(	O
ai	O
)	O
.	O
if	O
f	O
is	O
a	O
nonnegative-valued	O
measurable	O
function	O
,	O
then	O
introduce	O
a	O
sequence	O
of	O
simple	O
functions	O
as	O
follows	O
:	O
us	O
)	O
=	O
{	O
~~	O
-	O
l	O
)	O
/n	O
(	O
k	O
-	O
1	O
)	O
ln	O
:	O
:	O
:	O
f	O
(	O
s	O
)	O
<	O
kin	O
,	O
k	O
=	O
1,2	O
,	O
...	O
n2n	O
if	O
if	O
f	O
(	O
s	O
)	O
2	O
:	O
2n	O
.	O
a.2	O
the	O
lebesgue	O
integral	O
577	O
then	O
the	O
fn	O
's	O
are	O
simple	O
functions	O
,	O
and	O
fn	O
(	O
s	O
)	O
-+	O
f	O
(	O
s	O
)	O
in	O
a	O
monotone	O
non	O
(	O
cid:173	O
)	O
decreasing	O
fashion	O
.	O
therefore	O
,	O
the	O
sequence	O
of	O
integrals	O
f	O
fndv	O
is	O
monotone	O
non	O
(	O
cid:173	O
)	O
decreasing	O
,	O
with	O
a	O
limit	O
.	O
the	O
integral	O
f	O
is	O
then	O
defined	O
by	O
f	O
fdv	O
=	O
[	O
f	O
(	O
s	O
)	O
v	O
(	O
ds	O
)	O
=	O
lim	O
f	O
fn	O
dv	O
.	O
j	O
s	O
n	O
--	O
-+oo	O
if	O
f	O
is	O
an	O
arbitrary	O
measurable	O
function	O
,	O
then	O
decompose	O
it	O
as	O
a	O
difference	O
of	O
its	O
positive	O
and	O
negative	O
parts	O
,	O
f	O
(	O
s	O
)	O
=	O
fest	O
-	O
f	O
(	O
s	O
)	O
-	O
=	O
fest	O
-	O
(	O
-	O
f	O
(	O
s	O
)	O
t	O
,	O
where	O
f+	O
and	O
f-	O
are	O
both	O
nonnegative	O
functions	O
.	O
define	O
the	O
integral	O
of	O
f	O
by	O
if	O
at	O
least	O
one	O
term	O
on	O
the	O
right-hand	O
side	O
is	O
finite	O
.	O
then	O
we	O
say	O
that	O
the	O
integral	O
exists	O
.	O
if	O
the	O
integral	O
is	O
finite	O
then	O
f	O
is	O
integrable	O
.	O
definition	O
a.io	O
.	O
iff	O
jdv	O
exists	O
and	O
a	O
is	O
a	O
measurablefun	O
{	O
ji~'n	O
,	O
~then	O
fa	O
fdv	O
is	O
defined	O
by	O
c	O
''	O
'	O
,	O
.j	O
''	O
7	O
i	O
fdv	O
=	O
f	O
fjadv	O
.	O
definition	O
a.ii	O
.	O
we	O
say	O
that	O
fn	O
-+	O
f	O
(	O
mod	O
v	O
)	O
if	O
v	O
(	O
{	O
s	O
:	O
2i~fn	O
(	O
s	O
)	O
i	O
f	O
(	O
s	O
)	O
}	O
)	O
=	O
o.	O
theorem	B
a.i	O
.	O
(	O
beppo-levy	O
theorem	B
)	O
.	O
if	O
fn	O
(	O
s	O
)	O
-+	O
f	O
(	O
s	O
)	O
in	O
a	O
monotone	O
increas	O
(	O
cid:173	O
)	O
ing	O
way	O
for	O
some	O
nonnegative	O
integrable	O
function	O
f	O
,	O
then	O
f	O
lim	O
fn	O
dv	O
=	O
lim	O
f	O
fn	O
dv	O
.	O
17	O
--	O
-+00	O
iz	O
--	O
-+oo	O
theorem	B
a.2	O
.	O
(	O
lebesgue	O
's	O
dominated	B
convergence	I
theorem	I
)	O
.	O
assume	O
that	O
(	O
mod	O
v	O
)	O
andlh7	O
(	O
s	O
)	O
1	O
:	O
:	O
:	O
:	O
g	O
(	O
s	O
)	O
fors	O
e	O
s	O
,	O
n	O
=	O
1,2	O
,	O
...	O
,	O
where	O
f	O
gdv	O
<	O
00.	O
fn	O
-+	O
f	O
then	O
f	O
lim	O
h	O
1dv	O
=	O
lim	O
f	O
h7	O
dv	O
.	O
17	O
--	O
-+00	O
11	O
--	O
-+00	O
theorem	B
a.3	O
.	O
(	O
fatou	O
's	O
lemma	O
)	O
.	O
let	O
fl	O
'	O
12	O
,	O
...	O
be	O
measurable	O
functions	O
.	O
(	O
i	O
)	O
if	O
there	O
exists	O
a	O
measurable	O
function	O
g	O
with	O
f	O
gdv	O
>	O
-00	O
such	O
thatjor	O
every	O
n	O
,	O
hi	O
(	O
s	O
)	O
2	O
:	O
g	O
(	O
s	O
)	O
,	O
then	O
lim	O
inf	O
f	O
hi	O
d	O
v	O
2	O
:	O
f	O
lim	O
inf	O
h1	O
d	O
v	O
.	O
n	O
--	O
-+oo	O
iz	O
--	O
-+oo	O
578	O
appendix	O
(	O
ii	O
)	O
if	O
there	O
is	O
a	O
a	O
measurable	O
function	O
g	O
with	O
j	O
gdv	O
<	O
00	O
,	O
such	O
that	O
fn	O
(	O
s	O
)	O
:	O
:	O
:	O
:	O
:	O
g	O
(	O
s	O
)	O
for	O
every	O
n	O
,	O
then	O
lim	O
sup	O
f	O
fn	O
dv	O
:	O
:	O
:	O
:	O
:	O
f	O
lim	O
sup	O
fn	O
dv	O
.	O
n	O
--	O
-+oo	O
n	O
--	O
-+oo	O
definition	O
a.12	O
.	O
let	O
vi	O
and	O
v2	O
be	O
measures	O
on	O
a	O
measurable	O
space	O
(	O
s	O
,	O
f	O
)	O
.	O
we	O
say	O
that	O
vi	O
is	O
absolutely	O
continuous	O
with	O
respect	O
to	O
v2	O
if	O
and	O
only	O
if	O
v2	O
(	O
a	O
)	O
=	O
0	O
implies	O
vi	O
(	O
a	O
)	O
=	O
0	O
(	O
a	O
e	O
f	O
)	O
.	O
we	O
denote	O
this	O
relation	O
by	O
vi	O
«	O
v2	O
.	O
theorem	B
a.4	O
.	O
(	O
radon-nikodym	O
theorem	B
)	O
.	O
let	O
vi	O
and	O
v2	O
be	O
measures	O
on	O
the	O
measurable	O
space	O
(	O
s	O
,	O
f	O
)	O
such	O
that	O
vi	O
«	O
v2	O
and	O
v2	O
is	O
a-finite	O
.	O
then	O
there	O
exists	O
a	O
measurable	O
function	O
f	O
such	O
that	O
for	O
all	O
a	O
e	O
f	O
vi	O
(	O
a	O
)	O
=	O
1	O
f	O
dv2	O
.	O
f	O
is	O
unique	O
(	O
mod	O
v2	O
)	O
.	O
if	O
vi	O
is	O
finite	O
,	O
then	O
f	O
has	O
a	O
finite	O
integral	O
.	O
definition	O
a.13	O
.	O
f	O
is	O
called	O
the	O
density	O
,	O
or	O
radon-nikodym	O
derivative	O
of	O
vi	O
with	O
respect	O
to	O
v2	O
.	O
we	O
use	O
the	O
notation	O
f	O
=	O
dviidv2	O
.	O
definition	O
a.14	O
.	O
let	O
vi	O
and	O
v2	O
be	O
measures	O
on	O
a	O
measurable	O
space	O
(	O
s	O
,	O
f	O
)	O
.	O
if	O
)	O
=	O
0	O
and	O
v2	O
(	O
a	O
)	O
=	O
0	O
,	O
then	O
vi	O
is	O
singular	O
there	O
exists	O
a	O
set	O
a	O
e	O
f	O
such	O
that	O
vi	O
(	O
a	O
c	O
with	O
respect	O
to	O
v2	O
(	O
and	O
vice	O
versa	O
)	O
.	O
theorem	B
a.s.	O
(	O
lebesgue	O
decomposition	O
theorem	B
)	O
.	O
if	O
fj.	O
,	O
is	O
a	O
a-finite	O
measure	B
on	O
a	O
measurable	O
space	O
(	O
s	O
,	O
f	O
)	O
,	O
then	O
there	O
exist	O
two	O
unique	O
measures	O
vi	O
,	O
v2	O
such	O
that	O
fjv	O
=	O
vi	O
+	O
v2	O
,	O
where	O
vi	O
«	O
fj.	O
,	O
and	O
v2	O
is	O
singular	O
with	O
respect	O
to	O
fj.	O
,	O
.	O
definition	O
a.is	O
.	O
let	O
(	O
s	O
,	O
f	O
,	O
v	O
)	O
be	O
a	O
measure	O
space	O
and	O
let	O
f	O
be	O
a	O
measurable	O
function	O
.	O
then	O
f	O
induces	O
a	O
measure	O
fj.	O
,	O
on	O
the	O
borel	O
a	O
-algebra	O
as	O
follows	O
:	O
fj.	O
,	O
(	O
b	O
)	O
=	O
v	O
(	O
f-i	O
(	O
b	O
)	O
)	O
,	O
b	O
e	O
b.	O
theorem	B
a.6	O
.	O
let	O
v	O
be	O
a	O
measure	O
on	O
the	O
borel	O
a	O
-algebra	O
b	O
ofr	O
,	O
and	O
let	O
f	O
and	O
g	O
be	O
measurable	O
functions	O
.	O
then	O
for	O
all	O
b	O
e	O
b	O
,	O
r	O
g	O
(	O
x	O
)	O
fj.	O
,	O
(	O
dx	O
)	O
=	O
r	O
ib	O
if-icb	O
)	O
g	O
(	O
f	O
(	O
s	O
)	O
)	O
v	O
(	O
ds	O
)	O
,	O
where	O
fjv	O
is	O
induced	O
by	O
f.	O
definition	O
a.16	O
.	O
let	O
vi	O
and	O
v2	O
be	O
measures	O
on	O
the	O
measurable	O
spaces	O
(	O
sl	O
,	O
fd	O
and	O
(	O
s2	O
,	O
f	O
2	O
)	O
,	O
respectively	O
.	O
let	O
(	O
s	O
,	O
f	O
)	O
be	O
a	O
measurable	O
space	O
such	O
that	O
s	O
=	O
sl	O
x	O
s2	O
,	O
and	O
fi	O
x	O
f2	O
e	O
f	O
whenever	O
fi	O
e	O
fi	O
and	O
f2	O
e	O
f	O
2.	O
v	O
is	O
called	O
the	O
product	B
measure	O
of	O
vi	O
and	O
v2	O
on	O
f	O
iffor	O
fi	O
e	O
f1	O
and	O
f2	O
e	O
f2	O
,	O
v	O
(	O
f1	O
x	O
f2	O
)	O
=	O
vi	O
(	O
fi	O
)	O
v2	O
(	O
f2	O
)	O
.	O
the	O
product	B
of	O
more	O
than	O
two	O
measures	O
can	O
be	O
defined	O
similarly	O
.	O
a.3	O
denseness	B
results	O
579	O
theorem	B
a	O
.	O
7	O
.	O
(	O
fubini	O
's	O
theorem	B
)	O
.	O
let	O
h	O
be	O
a	O
measurable	O
function	O
on	O
the	O
product	B
space	O
(	O
s	O
,	O
f	O
)	O
.	O
then	O
is	O
h	O
(	O
u	O
,	O
v	O
)	O
v	O
(	O
d	O
(	O
u	O
,	O
v	O
)	O
)	O
=	O
is	O
,	O
(	O
is	O
,	O
h	O
(	O
u	O
,	O
v	O
)	O
v2	O
(	O
dv	O
)	O
)	O
vl	O
(	O
du	O
)	O
is	O
,	O
(	O
is	O
,	O
h	O
(	O
u	O
,	O
v	O
)	O
v.	O
(	O
du	O
)	O
)	O
v2	O
(	O
d	O
v	O
)	O
,	O
assuming	O
that	O
one	O
of	O
the	O
three	O
integrals	O
is	O
finite	O
.	O
a.3	O
denseness	B
results	O
lemma	O
a.t.	O
(	O
cover	O
and	O
hart	O
(	O
1967	O
)	O
)	O
.	O
let	O
fl	O
be	O
a	O
probability	O
measure	B
on	O
nd	O
and	O
define	O
its	O
support	B
set	O
by	O
,	O
a	O
=	O
support	B
(	O
fl	O
)	O
=	O
{	O
x	O
:	O
for	O
all	O
r	O
>	O
0	O
,	O
fl	O
(	O
sx	O
,	O
r	O
)	O
>	O
o	O
}	O
.	O
then	O
fl	O
(	O
a	O
)	O
=	O
1.	O
proof	O
.	O
by	O
the	O
definition	B
of	I
a	O
,	O
a	O
c	O
=	O
{	O
x	O
:	O
fl	O
(	O
sx	O
,	O
rj	O
=	O
0	O
for	O
some	O
rx	O
>	O
o	O
}	O
.	O
let	O
q	O
denote	O
the	O
set	O
of	O
vectors	O
in	O
nd	O
with	O
rational	O
components	O
(	O
or	O
any	O
countable	O
dense	O
set	O
)	O
.	O
then	O
for	O
each	O
x	O
e	O
ac	O
,	O
there	O
is	O
a	O
yx	O
e	O
q	O
with	O
ilx	O
yxll	O
:	O
:	O
:	O
s	O
rx/3	O
.	O
this	O
implies	O
syx	O
,	O
rx/2	O
c	O
sx	O
,	O
rx	O
'	O
therefore	O
,	O
fl	O
(	O
syx	O
,	O
rx	O
/2	O
)	O
=	O
0	O
,	O
and	O
c	O
c	O
u	O
syx	O
,	O
rx/2	O
.	O
a	O
xeac	O
the	O
right-hand	O
side	O
is	O
a	O
union	O
of	O
countably	O
many	O
sets	O
of	O
zero	O
measure	B
,	O
and	O
therefore	O
fl	O
(	O
a·	O
)	O
=	O
1	O
.	O
0	O
definition	O
a.17	O
.	O
let	O
(	O
s	O
,	O
f	O
,	O
v	O
)	O
be	O
a	O
measure	O
space	O
.	O
for	O
a	O
fixed	O
number	O
p	O
~	O
1	O
,	O
l	O
p	O
(	O
v	O
)	O
denotes	O
the	O
set	O
of	O
all	O
measurable	O
functions	O
satisfying	O
f	O
i	O
f	O
i	O
p	O
d	O
v	O
<	O
00.	O
theorem	B
a.s.	O
for	O
every	O
probability	O
measure	B
v	O
on	O
n	O
d	O
,	O
the	O
set	O
of	O
continuousfunc	O
(	O
cid:173	O
)	O
tions	O
with	O
bounded	O
support	B
is	O
dense	O
in	O
l	O
p	O
(	O
v	O
)	O
.	O
in	O
other	O
words	O
,	O
for	O
every	O
e	O
>	O
0	O
and	O
f	O
e	O
l	O
p	O
there	O
is	O
a	O
continuous	O
function	O
with	O
bounded	O
support	B
gel	O
p	O
such	O
that	O
the	O
following	O
theorem	B
is	O
a	O
rich	O
source	O
of	O
denseness	O
results	O
:	O
580	O
appendix	O
theorem	B
a.9	O
.	O
(	O
stone-weierstrass	O
theorem	B
)	O
.	O
let	O
f	O
be	O
afamily	O
of	O
real-valued	O
continuous	O
functions	O
on	O
a	O
closed	O
bounded	O
subset	O
b	O
of	O
rd	O
.	O
assume	O
that	O
f	O
is	O
an	O
algebra	O
,	O
that	O
is	O
,	O
for	O
any	O
ii	O
,	O
h	O
e	O
f	O
and	O
a	O
,	O
b	O
e	O
r	O
,	O
we	O
have	O
ali	O
+	O
bh	O
e	O
f	O
,	O
and	O
fl	O
h	O
e	O
f.	O
assume	O
furthermore	O
that	O
if	O
x	O
=i	O
y	O
then	O
there	O
is	O
an	O
f	O
e	O
f	O
such	O
that	O
f	O
(	O
x	O
)	O
=i	O
f	O
(	O
y	O
)	O
,	O
and	O
that	O
for	O
each	O
x	O
e	O
b	O
there	O
exists	O
an	O
f	O
e	O
f	O
with	O
f	O
(	O
x	O
)	O
=i	O
o.	O
then	O
for	O
every	O
e	O
>	O
0	O
and	O
continuous	O
function	O
g	O
:	O
b	O
-+	O
r	O
,	O
there	O
exists	O
an	O
f	O
e	O
f	O
such	O
that	O
sup	O
ig	O
(	O
x	O
)	O
-	O
xeb	O
f	O
(	O
x	O
)	O
1	O
<	O
e.	O
the	O
following	O
two	O
theorems	O
concern	O
differentiation	O
of	O
integrals	O
.	O
good	O
general	O
references	O
are	O
whee	O
den	O
and	O
zygmund	O
(	O
1977	O
)	O
and	O
de	O
guzman	O
(	O
1975	O
)	O
:	O
theorem	B
a.io	O
.	O
(	O
the	O
lebesgue	O
density	O
theorem	O
)	O
.	O
let	O
f	O
be	O
a	O
density	O
on	O
rd	O
.	O
let	O
{	O
qk	O
(	O
x	O
)	O
}	O
be	O
a	O
sequence	O
of	O
closed	O
cubes	O
centered	O
at	O
x	O
and	O
contracting	O
to	O
x.	O
then	O
.	O
hm	O
k	O
--	O
+oo	O
fqk	O
(	O
x	O
)	O
if	O
(	O
x	O
)	O
-	O
f	O
(	O
y	O
)	O
ldy	O
a	O
(	O
qk	O
(	O
x	O
)	O
)	O
=0	O
at	O
almost	O
all	O
x	O
,	O
where	O
a	O
denotes	O
the	O
lebesgue	O
measure	B
.	O
note	O
that	O
this	O
implies	O
at	O
almost	O
all	O
x.	O
corollary	O
a.i	O
.	O
let	O
a	O
be	O
a	O
collection	O
of	O
subsets	O
of	O
so	O
,	O
1	O
with	O
the	O
property	O
that	O
for	O
all	O
a	O
e	O
a	O
,	O
a	O
(	O
a	O
)	O
:	O
:	O
:	O
ca	O
(	O
so	O
,	O
l	O
)	O
for	O
some	O
fixed	O
c	O
>	O
o.	O
then	O
for	O
almost	O
all	O
x	O
,	O
if	O
x	O
+	O
r	O
a	O
=	O
{	O
y	O
:	O
(	O
y	O
-	O
x	O
)	O
/	O
rea	O
}	O
,	O
1·	O
1m	O
sup	O
r-+o	O
aea	O
a	O
(	O
x	O
+	O
r	O
a	O
)	O
i	O
fx+ra	O
f	O
(	O
y	O
)	O
dy	O
-	O
f	O
(	O
)	O
i	O
0	O
.	O
x	O
=	O
the	O
lebesgue	O
density	O
theorem	O
also	O
holds	O
if	O
{	O
qk	O
(	O
x	O
)	O
}	O
is	O
replaced	O
by	O
a	O
sequence	O
of	O
contracting	O
balls	O
centered	O
at	O
x	O
,	O
or	O
indeed	O
by	O
any	O
sequence	O
of	O
sets	O
that	O
satisfy	O
x	O
+	O
brks	O
s	O
;	O
qk	O
(	O
x	O
)	O
s	O
;	O
x	O
+	O
arks	O
,	O
where	O
s	O
is	O
the	O
unit	O
ball	O
of	O
rd	O
,	O
rk	O
+	O
0	O
,	O
and	O
0	O
<	O
b	O
:	O
:	O
:	O
:	O
a	O
<	O
00	O
are	O
fixed	O
constants	O
.	O
this	O
follows	O
from	O
the	O
lebesgue	O
density	O
theorem	O
.	O
it	O
does	O
not	O
hold	O
in	O
general	O
when	O
{	O
qk	O
(	O
x	O
)	O
}	O
is	O
a	O
sequence	O
of	O
hyperrectangles	O
containing	O
x	O
and	O
contracting	O
to	O
x.	O
for	O
that	O
,	O
an	O
additional	O
restriction	O
is	O
needed	O
:	O
theorem	B
a.i1	O
.	O
(	O
the	O
jessen-marcinkiewicz-zygmund	O
theorem	B
)	O
.	O
let	O
f	O
be	O
a	O
density	O
on	O
rd	O
with	O
f	O
f	O
logd-l	O
(	O
1	O
+	O
f	O
)	O
dx	O
<	O
00.	O
let	O
{	O
qk	O
(	O
x	O
)	O
}	O
be	O
a	O
sequence	O
of	O
hyperrectangles	O
containing	O
x	O
andfor	O
which	O
diam	O
(	O
qk	O
(	O
x	O
)	O
)	O
-+	O
o.	O
then	O
at	O
almost	O
all	O
x.	O
a.4	O
probability	O
581	O
corollary	O
a.2	O
.	O
iff	O
is	O
a	O
density	O
and	O
{	O
qk	O
(	O
x	O
)	O
}	O
is	O
as	O
in	O
theorema.ll	O
,	O
then	O
~	O
1	O
f	O
(	O
y	O
)	O
dy	O
~	O
f	O
(	O
x	O
)	O
lim	O
inf	O
k-+oo	O
a	O
(	O
qk	O
(	O
x	O
»	O
qk	O
(	O
x	O
)	O
i	O
at	O
almost	O
all	O
x.	O
to	O
see	O
this	O
,	O
take	O
g	O
:	O
:	O
:	O
:	O
min	O
(	O
f	O
,	O
m	O
)	O
for	O
large	O
fixed	O
m.	O
as	O
f	O
g	O
logd-l	O
(	O
l	O
+	O
g	O
)	O
<	O
00	O
,	O
by	O
the	O
lessen-marcinkiewicz-zygmund	O
theorem	B
,	O
i	O
1	O
g	O
(	O
y	O
)	O
dy	O
~	O
g	O
(	O
x	O
)	O
lim	O
inf	O
k-+oo	O
a	O
(	O
qk	O
(	O
x	O
»	O
qk	O
(	O
x	O
)	O
at	O
almost	O
all	O
x.	O
conclude	O
by	O
letting	O
m	O
--	O
-	O
?	O
>	O
-	O
00	O
along	O
the	O
integers	O
.	O
a.4	O
probability	O
definition	O
a.i8	O
.	O
a	O
measure	O
space	O
(	O
q	O
,	O
f	O
,	O
p	O
)	O
is	O
called	O
a	O
probability	O
space	O
if	O
p	O
{	O
q	O
}	O
:	O
:	O
:	O
:	O
1.	O
q	O
is	O
the	O
sample	O
space	O
or	O
sure	O
event	O
,	O
the	O
measurable	O
sets	O
are	O
called	O
events	O
,	O
and	O
the	O
measurable	O
functions	O
are	O
called	O
random	O
variables	O
.	O
if	O
xl	O
,	O
...	O
,	O
xn	O
are	O
random	O
variables	O
then	O
x	O
:	O
:	O
:	O
:	O
(	O
xl	O
,	O
...	O
,	O
xn	O
)	O
is	O
a	O
vector-valued	O
random	O
variable	B
.	O
definition	O
a.19	O
.	O
let	O
x	O
be	O
a	O
random	O
variable	B
,	O
then	O
x	O
induces	O
the	O
measure	B
fl	O
on	O
the	O
borel	O
0	O
'	O
-algebra	O
ofr	O
by	O
fl	O
(	O
b	O
)	O
:	O
:	O
:	O
:	O
p	O
{	O
{	O
w	O
:	O
x	O
(	O
w	O
)	O
e	O
bll	O
:	O
:	O
:	O
:	O
p	O
{	O
x	O
e	O
b	O
}	O
,	O
b	O
e	O
b.	O
the	O
probability	O
measure	B
fl	O
is	O
called	O
the	O
distribution	B
of	O
the	O
random	O
variable	B
x.	O
definition	O
a.20	O
.	O
let	O
x	O
be	O
a	O
random	O
variable	B
.	O
the	O
expectation	O
of	O
x	O
is	O
the	O
integral	O
of	O
x	O
with	O
respect	O
to	O
the	O
distribution	O
fl	O
of	O
x	O
:	O
e	O
{	O
x	O
}	O
:	O
:	O
:	O
:	O
l	O
xfl	O
(	O
dx	O
)	O
if	O
it	O
exists	O
.	O
definition	O
a.21	O
.	O
let	O
x	O
be	O
a	O
random	O
variable	B
.	O
the	O
variance	B
of	I
x	O
is	O
var	O
{	O
x	O
}	O
:	O
:	O
:	O
:	O
e	O
{	O
(	O
x	O
-	O
e	O
{	O
x	O
)	O
)	O
2	O
}	O
ife	O
{	O
x	O
}	O
isjinite	O
,	O
and	O
00	O
ife	O
{	O
x	O
}	O
is	O
notjinite	O
or	O
does	O
not	O
exist	O
.	O
definition	O
a.22	O
.	O
let	O
xl	O
,	O
...	O
,	O
xn	O
be	O
random	O
variables	O
.	O
they	O
induce	O
the	O
measure	B
{	O
l	O
(	O
n	O
)	O
on	O
the	O
borel	O
o'-algebra	O
ofrn	O
with	O
the	O
property	O
582	O
appendix	O
p	O
}	O
n	O
)	O
is	O
called	O
the	O
joint	O
distribution	B
of	O
the	O
random	O
variables	O
x	O
i	O
,	O
...	O
,	O
x	O
n.	O
let	O
fj	O
,	O
i	O
be	O
the	O
distribution	B
of	O
xi	O
(	O
i	O
=	O
1	O
,	O
...	O
,	O
n	O
)	O
.	O
the	O
random	O
variables	O
xl	O
,	O
...	O
,	O
xn	O
are	O
independent	O
if	O
their	O
joint	O
distribution	B
fj	O
,	O
(	O
n	O
)	O
is	O
the	O
product	B
measure	O
of	O
fj,1	O
,	O
...	O
,	O
fj	O
,	O
n	O
.	O
the	O
events	O
ai	O
,	O
...	O
,	O
an	O
e	O
:	O
f	O
are	O
independent	O
if	O
the	O
random	O
variables	O
iaj	O
,	O
...	O
,	O
ian	O
are	O
independent	O
.	O
fubini	O
's	O
theorem	B
implies	O
the	O
following	O
:	O
theorem	B
a.12	O
.	O
if	O
the	O
random	O
variables	O
xl	O
,	O
...	O
,	O
xn	O
are	O
independent	O
and	O
have	O
finite	O
expectations	O
then	O
a.s	O
inequalities	O
theorem	B
a.13	O
.	O
(	O
cauchy-schwarz	O
inequality	B
)	O
.	O
if	O
the	O
random	O
variables	O
x	O
and	O
y	O
have	O
finite	O
second	O
moments	O
(	O
e	O
{	O
x2	O
}	O
<	O
00	O
ande	O
{	O
y2	O
}	O
<	O
(	O
0	O
)	O
,	O
then	O
theorem	B
a.14	O
.	O
(	O
holder	O
's	O
inequality	B
)	O
.	O
let	O
p	O
,	O
q	O
e	O
(	O
1	O
,	O
(	O
0	O
)	O
such	O
that	O
(	O
lip	O
)	O
+	O
(	O
1lq	O
)	O
=	O
1.	O
let	O
x	O
and	O
y	O
be	O
random	O
variables	O
such	O
that	O
(	O
e	O
{	O
ixpi	O
}	O
)	O
ljp	O
<	O
00	O
and	O
(	O
e	O
{	O
lyql	O
}	O
)	O
ljq	O
<	O
00.	O
then	O
theorem	B
a.is	O
.	O
(	O
markov	O
's	O
inequality	B
)	O
.	O
let	O
x	O
be	O
a	O
nonnegative-valued	O
random	O
variable	B
.	O
then	O
for	O
each	O
t	O
>	O
0	O
,	O
p	O
{	O
x	O
:	O
:	O
:	O
t	O
}	O
s	O
e	O
{	O
x	O
}	O
.	O
t	O
theorem	B
a.16	O
.	O
(	O
chebyshev	O
's	O
inequality	B
)	O
.	O
let	O
x	O
be	O
a	O
random	O
variable	B
.	O
then	O
for	O
each	O
t	O
>	O
0	O
,	O
p	O
{	O
ix	O
-	O
e	O
{	O
x	O
}	O
i	O
:	O
:	O
:	O
t	O
}	O
s	O
var	O
{	O
x	O
}	O
.	O
t	O
2	O
theorem	B
a.17	O
.	O
(	O
chebyshev-cantelli	O
inequality	B
)	O
.	O
let	O
t	O
:	O
:	O
:	O
0.	O
then	O
p	O
{	O
x	O
-	O
ex	O
>	O
t	O
}	O
<	O
var	O
{	O
x	O
}	O
2	O
-	O
var	O
{	O
x	O
}	O
+	O
t	O
proof	O
.	O
we	O
may	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
ex	O
=	O
0.	O
then	O
for	O
all	O
t	O
t	O
=	O
e	O
{	O
t	O
-	O
x	O
}	O
s	O
e	O
{	O
(	O
t	O
-	O
x	O
)	O
i	O
{	O
x	O
:	O
:st	O
}	O
}	O
.	O
a.5	O
inequalities	O
583	O
thus	O
for	O
t	O
~	O
0	O
from	O
the	O
cauchy-schwarz	O
inequality	B
,	O
t	O
2	O
:	O
:	O
:	O
:	O
;	O
e	O
{	O
(	O
t	O
-	O
xf	O
}	O
e	O
{	O
ilxst	O
}	O
}	O
e	O
{	O
(	O
t	O
-	O
x	O
)	O
2	O
}	O
p	O
{	O
x	O
:	O
:	O
:	O
:	O
;	O
t	O
}	O
(	O
var	O
{	O
x	O
}	O
+	O
t	O
2	O
)	O
p	O
{	O
x	O
:	O
:	O
:	O
:	O
;	O
t	O
}	O
,	O
that	O
is	O
,	O
and	O
the	O
claim	O
follows	O
.	O
0	O
.	O
t	O
2	O
p	O
{	O
x	O
<	O
t	O
}	O
>	O
--	O
--	O
--	O
,	O
,	O
(	O
cid:173	O
)	O
-	O
var	O
{	O
x	O
}	O
+	O
-	O
theorem	B
a.ls	O
.	O
(	O
jensen	O
's	O
inequality	B
)	O
.	O
if	O
f	O
is	O
a	O
real-valued	O
convex	O
function	O
on	O
afinite	O
or	O
infinite	O
interval	O
ofr	O
,	O
and	O
x	O
is	O
a	O
random	O
variable	B
withfinite	O
expectation	O
,	O
taking	O
its	O
values	O
in	O
this	O
interval	O
,	O
then	O
f	O
(	O
e	O
{	O
x	O
}	O
)	O
:	O
:	O
:	O
;	O
e	O
{	O
f	O
(	O
x	O
)	O
}	O
.	O
theorem	B
a.19	O
.	O
(	O
association	B
inequalities	O
)	O
.	O
let	O
x	O
be	O
a	O
real-valued	O
random	O
variable	B
and	O
let	O
f	O
(	O
x	O
)	O
and	O
g	O
(	O
x	O
)	O
be	O
monotone	O
nondecreasing	O
real-valuedfunctions	O
.	O
then	O
e	O
{	O
f	O
(	O
x	O
)	O
g	O
(	O
x	O
)	O
}	O
~	O
e	O
{	O
f	O
(	O
x	O
)	O
}	O
e	O
{	O
g	O
(	O
x	O
)	O
}	O
,	O
provided	O
that	O
all	O
expectations	O
exist	O
and	O
are	O
finite	O
.	O
if	O
f	O
is	O
monotone	O
increasing	O
and	O
g	O
is	O
monotone	O
decreasing	O
,	O
then	O
e	O
{	O
f	O
(	O
x	O
)	O
g	O
(	O
x	O
)	O
}	O
:	O
:	O
:	O
;	O
e	O
{	O
f	O
(	O
x	O
)	O
}	O
e	O
{	O
g	O
(	O
x	O
)	O
}	O
.	O
proof	O
.	O
we	O
prove	O
the	O
first	O
inequality	B
.	O
the	O
second	O
follows	O
by	O
symmetry	O
.	O
let	O
x	O
have	O
distribution	B
fl	O
.	O
then	O
we	O
write	O
e	O
{	O
f	O
(	O
x	O
)	O
g	O
(	O
x	O
)	O
}	O
-	O
e	O
{	O
f	O
(	O
x	O
)	O
}	O
e	O
{	O
g	O
(	O
x	O
)	O
}	O
f	O
f	O
(	O
x	O
)	O
g	O
(	O
x	O
)	O
fl	O
(	O
dx	O
)	O
-	O
f	O
fey	O
)	O
fl	O
(	O
dy	O
)	O
f	O
g	O
(	O
x	O
)	O
fl	O
(	O
dx	O
)	O
=	O
f	O
(	O
flf	O
(	O
x	O
)	O
-	O
f	O
(	O
y	O
)	O
]	O
g	O
(	O
x	O
)	O
{	O
'	O
(	O
dx	O
»	O
)	O
{	O
'	O
(	O
dy	O
)	O
f	O
(	O
f	O
hex	O
,	O
y	O
)	O
g	O
(	O
x	O
)	O
{	O
'	O
(	O
dx	O
»	O
)	O
{	O
'	O
(	O
dy	O
)	O
,	O
where	O
hex	O
,	O
y	O
)	O
=	O
f	O
(	O
x	O
)	O
-	O
fey	O
)	O
.	O
by	O
fubini	O
's	O
theorem	B
the	O
last	O
integral	O
equals	O
r	O
hex	O
,	O
y	O
)	O
g	O
(	O
x	O
)	O
fl2	O
(	O
dxdy	O
)	O
ir	O
}	O
l	O
>	O
y	O
hex	O
,	O
y	O
)	O
g	O
(	O
x	O
)	O
{	O
'2	O
(	O
dxdy	O
)	O
+	O
l	O
<	O
y	O
hex	O
,	O
y	O
)	O
g	O
(	O
x	O
)	O
{	O
'2	O
(	O
dxdy	O
)	O
,	O
=	O
584	O
appendix	O
since	O
h	O
(	O
x	O
,	O
x	O
)	O
=	O
0	O
for	O
all	O
x.	O
here	O
j1-	O
2	O
(	O
dxdy	O
)	O
=	O
j1-	O
(	O
dx	O
)	O
.	O
i~	O
(	O
dy	O
)	O
.	O
the	O
second	O
integral	O
on	O
the	O
right-hand	O
side	O
is	O
just	O
thus	O
,	O
we	O
have	O
e	O
{	O
f	O
(	O
x	O
)	O
g	O
(	O
x	O
)	O
}	O
-	O
e	O
{	O
f	O
(	O
x	O
)	O
}	O
e	O
{	O
g	O
(	O
x	O
)	O
}	O
l2	O
hex	O
,	O
y	O
)	O
g	O
(	O
x	O
)	O
j1-	O
2	O
(	O
dxdy	O
)	O
=	O
1	O
(	O
1	O
[	O
hex	O
,	O
y	O
)	O
g	O
(	O
x	O
)	O
+	O
hey	O
,	O
x	O
)	O
g	O
(	O
y	O
)	O
]	O
j1-	O
(	O
dx	O
»	O
)	O
j1-	O
(	O
dy	O
)	O
1	O
(	O
1	O
hex	O
,	O
y	O
)	O
[	O
g	O
(	O
x	O
)	O
-	O
g	O
(	O
y	O
)	O
]	O
j1-	O
(	O
dx	O
»	O
)	O
j1-	O
(	O
dy	O
)	O
x	O
>	O
y	O
x	O
>	O
y	O
y	O
y	O
:	O
:	O
:	O
:	O
0	O
,	O
since	O
hey	O
,	O
x	O
)	O
=	O
-hex	O
,	O
y	O
)	O
,	O
and	O
by	O
the	O
fact	O
that	O
hex	O
,	O
y	O
)	O
:	O
:	O
:	O
:	O
0	O
and	O
g	O
(	O
x	O
)	O
-	O
g	O
(	O
y	O
)	O
:	O
:	O
:	O
:	O
0	O
if	O
x	O
>	O
y	O
.	O
0	O
a.6	O
convergence	O
of	O
random	O
variables	O
definition	O
a.23	O
.	O
let	O
{	O
xn	O
}	O
,	O
n	O
=	O
1,2	O
,	O
...	O
,	O
be	O
a	O
sequence	O
of	O
random	O
variables	O
.	O
we	O
say	O
that	O
lim	O
xn	O
=	O
x	O
n-+oo	O
in	O
probability	O
iffor	O
each	O
e	O
>	O
0	O
we	O
say	O
that	O
lim	O
p	O
{	O
ixn	O
-	O
xi	O
:	O
:	O
:	O
:	O
e	O
}	O
=	O
o.	O
n-+oo	O
lim	O
xn	O
=	O
x	O
with	O
probability	O
one	O
(	O
or	O
almost	O
surely	O
)	O
,	O
n-+oo	O
if	O
xn	O
--	O
+	O
x	O
(	O
mod	O
p	O
)	O
,	O
that	O
is	O
,	O
p	O
{	O
w	O
:	O
lim	O
xn	O
(	O
w	O
)	O
=	O
x	O
(	O
w	O
)	O
}	O
=	O
1	O
.	O
11-+00	O
for	O
a	O
fixed	O
number	O
p	O
:	O
:	O
:	O
:	O
1	O
we	O
say	O
that	O
if	O
lim	O
xn	O
=	O
x	O
n-+oo	O
in	O
l	O
p	O
,	O
lim	O
e	O
{	O
ixn	O
-	O
xi	O
p	O
}	O
=	O
o	O
.	O
11-+00	O
a.7	O
conditional	O
expectation	O
585	O
theorem	B
a.20	O
.	O
convergence	O
in	O
l	O
p	O
implies	O
convergence	O
in	O
probability	O
.	O
theorem	B
a.21	O
.	O
limn~oo	O
xn	O
=	O
x	O
with	O
probability	O
one	O
if	O
and	O
only	O
if	O
lim	O
sup	O
ixm	O
-	O
xi	O
=	O
0	O
n~oo	O
ns	O
;	O
m	O
in	O
probability	O
.	O
thus	O
,	O
convergence	O
with	O
probability	O
one	O
implies	O
convergence	O
in	O
probability	O
.	O
theorem	B
a.22	O
.	O
(	O
borel-cantelli	O
lemma	O
)	O
.	O
let	O
an	O
,	O
n	O
=	O
1	O
,	O
2	O
,	O
...	O
,	O
be	O
a	O
sequence	O
of	O
events	O
.	O
introduce	O
the	O
notation	O
[	O
an	O
i.o	O
.	O
]	O
=	O
lim	O
sup	O
an	O
=	O
n~l	O
u~=n	O
am	O
.	O
n~cx	O
)	O
(	O
``	O
i.o	O
.	O
''	O
stands	O
for	O
``	O
infinitely	O
often	O
.	O
''	O
)	O
if	O
then	O
00	O
lp	O
{	O
an	O
}	O
<	O
00	O
n=l	O
p	O
{	O
[	O
an	O
i.o	O
.	O
]	O
}	O
=	O
o.	O
by	O
theorems	O
a.21	O
and	O
a.22	O
,	O
we	O
have	O
theorem	B
a.23	O
.	O
if	O
for	O
each	O
e	O
>	O
0	O
cx	O
)	O
lp	O
{	O
ixn	O
-	O
xi	O
:	O
:	O
:	O
:	O
e	O
}	O
<	O
00	O
,	O
n=l	O
then	O
limn~cx	O
)	O
xn	O
=	O
x	O
with	O
probability	O
one	O
.	O
a.7	O
conditional	O
expectation	O
if	O
y	O
is	O
a	O
random	O
variable	B
with	O
finite	O
expectation	O
and	O
a	O
is	O
an	O
event	O
with	O
positive	O
probability	O
,	O
then	O
the	O
conditional	O
expectation	O
of	O
y	O
given	O
a	O
is	O
defined	O
by	O
e	O
{	O
yia	O
}	O
=	O
e	O
{	O
y	O
ia	O
}	O
.	O
p	O
{	O
a	O
}	O
the	O
conditional	O
probability	O
of	O
an	O
event	O
b	O
given	O
a	O
is	O
p	O
{	O
b	O
ia	O
}	O
=	O
e	O
{	O
lb	O
ia	O
}	O
=	O
p	O
{	O
a	O
}	O
p	O
{	O
a	O
n	O
b	O
}	O
.	O
586	O
appendix	O
definition	O
a.24	O
.	O
let	O
y	O
be	O
a	O
random	O
variable	B
with	O
finite	O
expectation	O
and	O
x	O
be	O
a	O
d	O
-dimensional	O
vector-valued	O
random	O
variable	B
.	O
let	O
f	O
x	O
be	O
the	O
a	O
-algebra	O
generated	O
by	O
x	O
:	O
fx	O
=	O
{	O
x-l	O
(	O
b	O
)	O
;	O
b	O
e	O
sn	O
}	O
.	O
the	O
conditional	O
expectation	O
e	O
{	O
y	O
i	O
x	O
}	O
of	O
y	O
given	O
x	O
is	O
a	O
random	O
variable	B
with	O
the	O
property	O
that	O
for	O
all	O
a	O
e	O
fx	O
i	O
y	O
dp	O
=	O
i	O
e	O
{	O
yix	O
}	O
dp	O
.	O
the	O
existence	O
and	O
uniqueness	O
(	O
with	O
probability	O
one	O
)	O
of	O
e	O
{	O
y	O
i	O
x	O
}	O
is	O
a	O
consequence	O
of	O
the	O
radon-nikodym	O
theorem	B
if	O
we	O
apply	O
it	O
to	O
the	O
measures	O
such	O
that	O
and	O
definition	O
a.2s	O
.	O
let	O
c	O
be	O
an	O
event	O
and	O
x	O
be	O
a	O
d-dimensional	O
vector-valued	O
random	O
variable	B
.	O
then	O
the	O
conditional	O
probability	O
of	O
c	O
given	O
x	O
is	O
p	O
{	O
cix	O
}	O
=	O
e	O
{	O
1ci	O
x	O
}	O
.	O
theorem	B
a.24	O
.	O
let	O
y	O
be	O
a	O
random	O
variable	B
with	O
finite	O
expectation	O
.	O
let	O
c	O
be	O
an	O
event	O
,	O
and	O
let	O
x	O
and	O
z	O
be	O
vector-valued	O
random	O
variables	O
.	O
then	O
(	O
i	O
)	O
there	O
is	O
a	O
measurable	O
function	O
g	O
on	O
nd	O
such	O
that	O
e	O
{	O
yix	O
}	O
=	O
g	O
(	O
x	O
)	O
with	O
probability	O
one	O
.	O
(	O
ii	O
)	O
e	O
{	O
y	O
}	O
=	O
e	O
{	O
e	O
{	O
yix	O
}	O
}	O
,	O
p	O
{	O
c	O
}	O
=	O
e	O
{	O
p	O
{	O
cix	O
}	O
}	O
.	O
(	O
iii	O
)	O
e	O
{	O
yix	O
}	O
=	O
e	O
{	O
e	O
{	O
yix	O
,	O
z	O
}	O
ix	O
}	O
,	O
p	O
{	O
cix	O
}	O
=	O
e	O
{	O
p	O
{	O
cix	O
,	O
y	O
}	O
ix	O
}	O
.	O
(	O
iv	O
)	O
ify	O
is	O
a	O
function	O
of	O
x	O
thene	O
{	O
yix	O
}	O
=	O
y	O
.	O
(	O
v	O
)	O
(	O
vi	O
)	O
ify	O
=	O
f	O
(	O
x	O
,	O
z	O
)	O
forameasurablefunction	O
f	O
,	O
and	O
x	O
and	O
z	O
are	O
independent	O
,	O
if	O
(	O
y	O
,	O
x	O
)	O
and	O
z	O
are	O
independent	O
,	O
then	O
e	O
{	O
yix	O
,	O
z	O
}	O
=	O
e	O
{	O
yix	O
}	O
.	O
then	O
e	O
{	O
yix	O
}	O
=	O
g	O
(	O
x	O
)	O
,	O
where	O
g	O
(	O
x	O
)	O
=	O
e	O
{	O
f	O
(	O
x	O
,	O
z	O
)	O
}	O
.	O
a.8	O
the	O
binomial	B
distribution	I
an	O
integer-valued	O
random	O
variable	B
x	O
is	O
said	O
to	O
be	O
binomially	O
distributed	O
with	O
parameters	O
nand	O
p	O
if	O
p	O
{	O
x	O
=	O
k	O
}	O
=	O
k	O
p	O
(	O
1	O
-	O
p	O
)	O
n-k	O
,	O
k	O
=	O
0,1	O
,	O
...	O
,	O
n.	O
(	O
n	O
)	O
k	O
if	O
ai	O
,	O
...	O
,	O
an	O
are	O
independent	O
events	O
with	O
p	O
{	O
ad	O
=	O
p	O
,	O
then	O
x	O
=	O
2	O
:	O
:7=1ia	O
;	O
is	O
binomial	B
(	O
n	O
,	O
p	O
)	O
.	O
iai	O
is	O
called	O
a	O
bernoulli	O
random	O
variable	B
with	O
parameter	O
p.	O
a.8	O
the	O
binomial	B
distribution	I
587	O
lemma	O
a.2	O
.	O
let	O
the	O
random	O
variable	B
b	O
(	O
n	O
,	O
p	O
)	O
be	O
binomially	O
distributed	O
with	O
pa	O
(	O
cid:173	O
)	O
rameters	O
nand	O
p.	O
then	O
(	O
i	O
)	O
and	O
(	O
ii	O
)	O
proof	O
.	O
(	O
i	O
)	O
follows	O
from	O
the	O
following	O
simple	O
calculation	O
:	O
k=ok+1	O
k	O
t	O
_1_	O
(	O
n	O
)	O
pk	O
(	O
1	O
_	O
p	O
)	O
n-k	O
--	O
-	O
l	O
n	O
(	O
n	O
+	O
1	O
)	O
p	O
k=o	O
1	O
n	O
1	O
~	O
(	O
n	O
+	O
l	O
)	O
pk	O
(	O
1	O
_	O
p	O
)	O
n-k+l	O
=	O
pk+l	O
(	O
1_	O
pt-k	O
(	O
+	O
1	O
)	O
k	O
+	O
1	O
<	O
(	O
n	O
+	O
1	O
)	O
p	O
k=o	O
k	O
1	O
.	O
(	O
n	O
+	O
1	O
)	O
p	O
for	O
(	O
ii	O
)	O
we	O
have	O
e	O
{	O
b	O
(	O
:	O
,	O
p/	O
(	O
b	O
(	O
n	O
,	O
p	O
»	O
o	O
}	O
}	O
s	O
e	O
l	O
+	O
b~n	O
,	O
p	O
)	O
}	O
s	O
(	O
n	O
:	O
l	O
)	O
p	O
by	O
(	O
i	O
)	O
.	O
0	O
lemma	O
a.3	O
.	O
let	O
b	O
be	O
a	O
binomial	O
random	O
variable	B
with	O
parameters	O
nand	O
p.	O
then	O
for	O
every	O
0	O
~	O
p	O
~	O
1	O
,	O
andfor	O
p	O
=	O
1/2	O
p	O
!	O
b	O
:	O
l~j	O
)	O
~j2~rrej~j2	O
'	O
proof	O
.	O
the	O
lemma	O
follows	O
from	O
stirling	O
's	O
formula	O
:	O
j2nn	O
;	O
;	O
(	O
n	O
)	O
n	O
e1/	O
(	O
12n	O
)	O
~	O
n	O
!	O
~	O
-j2nn	O
(	O
n	O
)	O
n	O
;	O
;	O
e	O
1/	O
(	O
l2n+l	O
)	O
(	O
see	O
,	O
e.g.	O
,	O
feller	O
(	O
1968	O
»	O
.	O
0	O
588	O
appendix	O
lemma	O
a.4	O
.	O
(	O
devroye	O
and	O
gyorfi	O
(	O
1985	O
)	O
,	O
p.	O
194	O
)	O
.	O
for	O
any	O
random	O
variable	B
x	O
with	O
finite	O
fourth	O
moment	O
,	O
proof	O
.	O
fix	O
a	O
>	O
o.	O
the	O
function	O
l/x	O
+ax2	O
is	O
minimal	O
on	O
(	O
0	O
,	O
(	O
0	O
)	O
whenx	O
3	O
=	O
1/	O
(	O
2a	O
)	O
.	O
thus	O
,	O
x	O
+	O
ax4	O
___	O
>	O
(	O
2a	O
)	O
1/3	O
+	O
__	O
=	O
_	O
(	O
2a	O
)	O
1/3	O
.	O
a	O
x	O
2	O
-	O
(	O
2a	O
)	O
2/3	O
3	O
2	O
replace	O
x	O
by	O
ixi	O
and	O
take	O
expectations	O
:	O
the	O
lower	O
bound	O
,	O
considered	O
as	O
a	O
function	O
of	O
a	O
,	O
is	O
maximized	O
if	O
we	O
take	O
a	O
=	O
~	O
(	O
e	O
{	O
x2	O
}	O
/e	O
{	O
x4	O
}	O
)	O
3/2	O
.	O
resubstitution	B
yields	O
the	O
given	O
inequality	B
.	O
0	O
lemma	O
a.s.	O
let	O
b	O
be	O
a	O
binomial	O
(	O
n	O
,	O
1/2	O
)	O
random	O
variable	B
.	O
then	O
proof	O
.	O
this	O
bound	O
is	O
a	O
special	O
case	O
of	O
khintchine	O
's	O
inequality	B
(	O
see	O
szarek	O
(	O
1976	O
)	O
,	O
haagerup	O
(	O
1978	O
)	O
,	O
and	O
also	O
devroye	O
and	O
gyorfi	O
(	O
1985	O
)	O
,	O
p.	O
139	O
)	O
.	O
rather	O
than	O
proving	O
the	O
given	O
inequality	B
,	O
we	O
will	O
show	O
how	O
to	O
apply	O
the	O
previous	O
lemma	O
to	O
get	O
(	O
without	O
further	O
work	O
)	O
the	O
inequality	B
indeed	O
,	O
e	O
{	O
(	O
b	O
-	O
n/2	O
)	O
2	O
}	O
=	O
n/4	O
and	O
e	O
{	O
(	O
b	O
-	O
n/2	O
)	O
4	O
}	O
=	O
3n2/16	O
-	O
n/8	O
:	O
:	O
:	O
;	O
3n2/16	O
.	O
thus	O
,	O
n	O
i	O
}	O
2	O
''	O
in	O
~	O
(	O
3n2/16	O
)	O
1/2	O
=	O
v	O
12·	O
0	O
(	O
n/4	O
)	O
3/2	O
e	O
b	O
{	O
i	O
lemma	O
a.6	O
.	O
(	O
slud	O
(	O
1977	O
)	O
)	O
.	O
let	O
b	O
be	O
a	O
binomial	O
(	O
n	O
,	O
p	O
)	O
random	O
variable	B
with	O
p	O
:	O
:	O
:	O
:	O
1/2	O
.	O
thenfor	O
n	O
(	O
1	O
-	O
p	O
)	O
~	O
k	O
~	O
np	O
,	O
p	O
{	O
b	O
>	O
k	O
}	O
>	O
p	O
{	O
n	O
>	O
-	O
-	O
k	O
-	O
np	O
}	O
,	O
-	O
y'np	O
(	O
1-p	O
)	O
where	O
n	O
is	O
normal	B
(	O
0	O
,	O
1	O
)	O
.	O
a.10	O
the	O
multinomial	B
distribution	I
589	O
a.9	O
the	O
hypergeometric	B
distribution	I
let	O
n	O
,	O
b	O
,	O
and	O
n	O
be	O
positive	O
integers	O
with	O
n	O
>	O
n	O
and	O
n	O
>	O
b.	O
a	O
random	O
variable	B
x	O
taking	O
values	O
on	O
the	O
integers	O
0	O
,	O
1	O
,	O
...	O
,	O
b	O
is	O
hypergeometric	B
with	O
parameters	O
n	O
,	O
b	O
and	O
n	O
,	O
if	O
k	O
=	O
1	O
,	O
...	O
,	O
b.	O
x	O
models	O
the	O
number	O
of	O
blue	O
balls	O
in	O
a	O
sample	O
of	O
n	O
balls	O
drawn	O
without	O
replacement	O
from	O
an	O
urn	O
containing	O
b	O
blue	O
and	O
n	O
-	O
b	O
red	O
balls	O
.	O
theorem	B
a.2s	O
.	O
(	O
hoeffding	O
(	O
1963	O
»	O
.	O
let	O
the	O
set	O
a	O
consist	O
of	O
n	O
numbers	O
aj	O
,	O
...	O
,	O
an	O
.	O
let	O
zi	O
,	O
...	O
,	O
zn	O
denote	O
a	O
random	O
sample	O
taken	O
without	O
replacement	O
from	O
a	O
,	O
where	O
n	O
:	O
:	O
:	O
n.	O
denote	O
then	O
for	O
any	O
e	O
>	O
0	O
we	O
have	O
specifically	O
,	O
if	O
x	O
is	O
hypergeometrically	O
distributed	O
with	O
parameters	O
n	O
,	O
b	O
,	O
and	O
n	O
,	O
then	O
for	O
more	O
inequalities	O
of	O
this	O
type	O
,	O
see	O
hoeffding	O
(	O
1963	O
)	O
and	O
serfling	O
(	O
1974	O
)	O
.	O
a.i0	O
the	O
multinomial	B
distribution	I
a	O
vector	O
(	O
ni	O
,	O
•••	O
,	O
nk	O
)	O
of	O
integer-valued	O
random	O
variables	O
is	O
multinomially	O
dis	O
(	O
cid:173	O
)	O
tributed	O
with	O
parameters	O
(	O
n	O
,	O
pi	O
,	O
...	O
,	O
pk	O
)	O
if	O
if	O
l~=l	O
i	O
j	O
=	O
k	O
,	O
otherwise	O
.	O
i	O
j	O
:	O
:	O
:	O
:	O
0	O
lemma	O
a.7	O
.	O
the	O
moment-generating	O
function	O
of	O
a	O
multinomial	O
(	O
k	O
,	O
pi	O
,	O
...	O
,	O
pk	O
)	O
vector	O
is	O
590	O
appendix	O
a.ii	O
the	O
exponential	B
and	O
gamma	B
distributions	O
a	O
nonnegative	O
random	O
variable	B
has	O
exponential	B
distribution	I
with	O
parameter	O
'a	O
>	O
0	O
if	O
it	O
has	O
a	O
density	O
f	O
(	O
x	O
)	O
=	O
'ae-	O
ax	O
,	O
x	O
:	O
:	O
:	O
o.	O
a	O
nonnegative-valued	O
random	O
variable	B
has	O
the	O
gamma	B
distribution	I
with	O
parameters	O
a	O
,	O
b	O
:	O
:	O
:	O
0	O
if	O
it	O
has	O
density	O
the	O
sum	O
of	O
n	O
i.i.d	O
.	O
exponential	B
(	O
)	O
'	O
)	O
random	O
variables	O
has	O
gamma	B
distribution	I
with	O
parameters	O
nand	O
1/	O
)	O
..	O
a.i2	O
the	O
multivariate	B
normal	I
distribution	I
a	O
d-dimensional	O
random	O
variable	B
x	O
=	O
(	O
x	O
(	O
l	O
)	O
,	O
...	O
,	O
xed	O
)	O
)	O
has	O
the	O
multivariate	O
nor	O
(	O
cid:173	O
)	O
mal	O
distribution	B
if	O
it	O
has	O
a	O
density	O
where	O
mend	O
,	O
'e	O
is	O
a	O
positive	O
definite	O
symmetric	O
d	O
x	O
d	O
matrix	O
with	O
entries	O
(	O
5ij	O
,	O
and	O
det	O
(	O
'e	O
)	O
denotes	O
the	O
determinant	O
of	O
'e	O
.	O
then	O
ex	O
=	O
m	O
,	O
and	O
for	O
all	O
i	O
,	O
j	O
=	O
1	O
,	O
...	O
,	O
d	O
,	O
'e	O
is	O
called	O
the	O
covariance	O
matrix	O
of	O
x.	O
notation	O
indicator	O
of	O
an	O
event	O
a.	O
indicator	O
function	O
of	O
a	O
set	O
b	O
.	O
•	O
i	O
a	O
•	O
i	O
b	O
(	O
x	O
)	O
=	O
i	O
{	O
xeb	O
}	O
•	O
iai	O
cardinality	O
of	O
a	O
finite	O
set	O
a	O
.	O
•	O
a	O
c	O
complement	O
of	O
a	O
set	O
a	O
.	O
•	O
alb	O
symmetric	O
difference	O
of	O
sets	O
a	O
,	O
b	O
.	O
•	O
jog	O
composition	O
of	O
functions	O
j	O
,	O
g.	O
•	O
log	O
natural	O
logarithm	O
(	O
base	O
e	O
)	O
.	O
•	O
lx	O
j	O
integer	O
part	O
of	O
the	O
real	O
number	O
x	O
.	O
•	O
i	O
x	O
l	O
upper	O
integer	O
part	O
of	O
the	O
real	O
number	O
x	O
.	O
•	O
x	O
~	O
z	O
•	O
x	O
(	O
i	O
)	O
,	O
...	O
,	O
x	O
(	O
d	O
)	O
components	O
of	O
the	O
d-dimensional	O
column	O
vector	O
x	O
.	O
•	O
ilx	O
ii	O
=	O
jlf=l	O
(	O
x	O
(	O
i	O
)	O
)	O
2	O
l	O
2-norm	O
of	O
x	O
e	O
nd	O
.	O
•	O
x	O
e	O
rd	O
observation	O
,	O
vector-valued	O
random	O
variable	B
.	O
•	O
y	O
e	O
{	O
a	O
,	O
i	O
}	O
•	O
dn	O
=	O
(	O
(	O
xl	O
,	O
yl	O
)	O
,	O
...	O
,	O
(	O
xn	O
'	O
yn	O
»	O
training	O
data	O
,	O
sequence	O
of	O
i.i.d	O
.	O
pairs	O
that	O
are	O
independent	O
of	O
(	O
x	O
,	O
y	O
)	O
,	O
and	O
have	O
the	O
same	O
distribution	B
as	O
that	O
of	O
(	O
x	O
,	O
y	O
)	O
.	O
if	O
x	O
and	O
z	O
have	O
the	O
same	O
distribution	B
.	O
label	O
,	O
binary	B
random	O
variable	B
.	O
•	O
17	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
iix	O
=	O
x	O
}	O
,	O
1	O
-	O
17	O
(	O
x	O
)	O
=	O
p	O
{	O
y	O
=	O
0ix	O
=	O
x	O
}	O
a	O
posteriori	O
probabilities	O
.	O
•	O
p	O
=	O
p	O
{	O
y	O
=	O
i	O
}	O
,	O
1	O
-	O
p	O
=	O
p	O
{	O
y	O
=	O
o	O
}	O
class	O
probabilities	O
.	O
:	O
rd	O
x	O
{	O
rd	O
x	O
{	O
a	O
,	O
l	O
}	O
r	O
--	O
-+	O
{	O
a	O
,	O
l	O
}	O
classification	O
•	O
g*	O
:	O
rd	O
--	O
-+	O
{	O
a	O
,	O
i	O
}	O
bayes	O
decision	O
function	O
.	O
•	O
¢	O
:	O
rd	O
--	O
-+	O
{	O
a	O
,	O
i	O
}	O
,	O
gn	O
functions	O
.	O
the	O
short	O
notation	O
gn	O
(	O
x	O
)	O
=	O
gn	O
(	O
x	O
,	O
dn	O
)	O
is	O
also	O
used	O
.	O
•	O
l	O
*	O
=	O
p	O
{	O
g*	O
(	O
x	O
)	O
=i	O
y	O
}	O
bayes	O
risk	O
,	O
the	O
error	O
probability	O
of	O
the	O
bayes	O
decision	O
.	O
592	O
notation	O
•	O
ln	O
=	O
l	O
(	O
gn	O
)	O
=	O
p	O
{	O
g	O
,	O
jx	O
,	O
dn	O
)	O
=i	O
yidn	O
}	O
error	O
probability	O
of	O
a	O
classification	O
•	O
in	O
(	O
¢	O
)	O
=	O
*	O
l~i=1	O
i	O
{	O
¢	O
(	O
xi	O
)	O
¥yi	O
}	O
empirical	B
error	I
probability	O
of	O
a	O
classifier	O
¢	O
.	O
function	O
gn	O
'	O
•	O
~n	O
(	O
a	O
)	O
=	O
*	O
l~i=1	O
i	O
{	O
xiea	O
}	O
empirical	B
measure	I
corresponding	O
to	O
xl	O
,	O
...	O
,	O
x	O
n.	O
•	O
a	O
lebesgue	O
measure	B
on	O
nd	O
.	O
•	O
i	O
(	O
x	O
)	O
density	O
of	O
x	O
,	O
radon-nikodym	O
derivative	O
of	O
~	O
with	O
respect	O
to	O
a	O
(	O
if	O
it	O
•	O
~	O
(	O
a	O
)	O
=	O
p	O
{	O
x	O
e	O
a	O
}	O
probability	O
measure	B
of	O
x.	O
exists	O
)	O
.	O
•	O
10	O
(	O
x	O
)	O
,	O
11	O
(	O
x	O
)	O
conditional	O
densities	O
of	O
x	O
given	O
y	O
=	O
0	O
and	O
y	O
=	O
1	O
,	O
respectively	O
(	O
if	O
they	O
exist	O
)	O
.	O
•	O
p	O
partition	B
of	O
nd	O
.	O
•	O
x	O
(	O
klx	O
)	O
,	O
x	O
(	O
k	O
)	O
k-th	O
nearest	B
neighbor	I
of	O
x	O
among	O
xl	O
,	O
...	O
,	O
x	O
n	O
•	O
•	O
k	O
:	O
nd	O
-+	O
n	O
kernel	B
function	O
.	O
•	O
h	O
,	O
hn	O
>	O
0	O
smoothing	B
factor	I
for	O
a	O
kernel	O
rule	B
.	O
•	O
kh	O
(	O
x	O
)	O
=	O
(	O
11	O
h	O
)	O
k	O
(	O
xl	O
h	O
)	O
scaled	O
kernel	B
function	O
.	O
•	O
tm	O
=	O
(	O
(	O
xn+l	O
,	O
yn+l	O
)	O
,	O
...	O
,	O
(	O
xn+m	O
'	O
yn+m	O
»	O
testing	O
data	O
,	O
sequence	O
of	O
i.i.d	O
.	O
pairs	O
that	O
are	O
independent	O
of	O
(	O
x	O
,	O
y	O
)	O
and	O
d	O
n	O
,	O
and	O
have	O
the	O
same	O
distribution	B
as	O
that	O
of	O
(	O
x	O
,	O
y	O
)	O
.	O
•	O
a	O
class	O
of	O
sets	O
.	O
•	O
c	O
,	O
cn	O
classes	O
of	O
classification	O
functions	O
.	O
•	O
sea	O
,	O
n	O
)	O
n-th	O
shatter	B
coefficient	I
of	O
the	O
class	O
of	O
sets	O
a	O
.	O
•	O
va	O
vapnik-chervonenkis	O
dimension	B
of	O
the	O
class	O
of	O
sets	O
a	O
.	O
•	O
s	O
(	O
c	O
,	O
n	O
)	O
n-th	O
shatter	B
coefficient	I
of	O
the	O
class	O
of	O
classifiers	O
c.	O
•	O
vc	O
vapnik-chervonenkis	O
dimension	B
of	O
the	O
class	O
of	O
classifiers	O
c.	O
•	O
sx	O
,	O
r	O
=	O
{	O
y	O
e	O
n	O
d	O
:	O
ii	O
y	O
-	O
x	O
ii	O
:	O
:	O
;	O
r	O
}	O
closed	O
euclidean	O
ball	O
in	O
nd	O
centered	O
at	O
x	O
e	O
n	O
d	O
,	O
with	O
radius	O
r	O
>	O
o.	O
references	O
abou-jaoude	O
,	O
s.	O
(	O
1976a	O
)	O
.	O
conditions	O
necessaires	O
et	O
suffisantes	O
de	O
convergence	O
l1	O
en	O
prob	O
(	O
cid:173	O
)	O
abilite	O
de	O
l'histogramme	O
pour	O
une	O
densite	O
.	O
annales	O
de	O
l	O
'	O
institut	O
henri	O
poincare	O
,	O
12:213-	O
231.	O
abou-jaoude	O
,	O
s.	O
(	O
1976b	O
)	O
.	O
la	O
convergence	O
li	O
et	O
loo	O
de	O
l'estimateur	O
de	O
la	O
partition	O
aleatoire	O
pour	O
une	O
densite	O
.	O
annales	O
de	O
l'institut	O
henri	O
poincare	O
,	O
12:299-317.	O
abou-jaoude	O
,	O
s.	O
(	O
1976c	O
)	O
.	O
sur	O
une	O
condition	O
necessaire	O
et	O
suffisante	O
de	O
li-convergence	O
presque	O
complete	O
de	O
l	O
'	O
estimateur	O
de	O
la	O
partition	O
fixe	O
pour	O
une	O
densite	O
.	O
comptes	O
rendus	O
de	O
l	O
'	O
academie	O
des	O
sciences	O
de	O
paris	O
,	O
283	O
:	O
1107-1110.	O
aitchison	O
,	O
j.	O
and	O
aitken	O
,	O
c.	O
(	O
1976	O
)	O
.	O
multivariate	O
binary	O
discrimination	O
by	O
the	O
kernel	B
method	O
.	O
biometrika,63:413-420.	O
aizerman	O
,	O
m.	O
,	O
braverman	O
,	O
e.	O
,	O
and	O
rozonoer	O
,	O
l.	O
(	O
1964a	O
)	O
.	O
the	O
method	O
of	O
potential	O
functions	O
for	O
the	O
problem	O
of	O
restoring	O
the	O
characteristic	O
of	O
a	O
function	O
converter	O
from	O
randomly	O
observed	O
points	O
.	O
automation	O
and	O
remote	O
control	O
,	O
25	O
:	O
1546-1556.	O
aizerman	O
,	O
m.	O
,	O
braverman	O
,	O
e.	O
,	O
and	O
rozonoer	O
,	O
l.	O
(	O
1964b	O
)	O
.	O
the	O
probability	O
problem	O
of	O
pattern	O
recognition	O
learning	B
and	O
the	O
method	O
of	O
potential	O
functions	O
.	O
automation	O
and	O
remote	O
control,25:1307-1323.	O
aizerman	O
,	O
m.	O
,	O
braverman	O
,	O
e.	O
,	O
and	O
rozonoer	O
,	O
l.	O
(	O
1964c	O
)	O
.	O
theoretical	B
foundations	O
of	O
the	O
po	O
(	O
cid:173	O
)	O
tential	O
function	O
method	O
in	O
pattern	O
recognition	O
learning	B
.	O
automation	O
and	O
remote	O
control	O
,	O
25:917-936.	O
aizerman	O
,	O
m.	O
,	O
braverman	O
,	O
e.	O
,	O
and	O
rozonoer	O
,	O
l.	O
(	O
1970	O
)	O
.	O
extrapolative	O
problems	O
in	O
auto	O
(	O
cid:173	O
)	O
matic	O
control	O
and	O
the	O
method	O
of	O
potential	O
functions	O
.	O
american	O
mathematical	O
society	O
translations	O
,	O
87:281-303.	O
akaike	O
,	O
h.	O
(	O
1954	O
)	O
.	O
an	O
approximation	O
to	O
the	O
density	O
function	O
.	O
annals	O
of	O
the	O
institute	O
of	O
statistical	O
mathematics	O
,	O
6:127-132.	O
akaike	O
,	O
h.	O
(	O
1974	O
)	O
.	O
a	O
new	O
look	O
at	O
the	O
statistical	O
model	O
identification	O
.	O
ieee	O
transactions	O
on	O
automatic	B
control	O
,	O
19:716-723	O
.	O
594	O
references	O
alexander	O
,	O
k.	O
(	O
1984	O
)	O
.	O
probability	O
inequalities	O
for	O
empirical	B
processes	O
and	O
a	O
law	O
of	O
the	O
iterated	O
logarithm	O
.	O
annals	O
of	O
probability	O
,	O
4	O
:	O
1041-1067.	O
anderson	O
,	O
a.	O
and	O
fu	O
,	O
k.	O
(	O
1979	O
)	O
.	O
design	O
and	O
development	O
of	O
a	O
linear	O
binary	B
tree	O
classifier	B
for	O
leukocytes	O
.	O
technical	O
report	O
tr-ee-79-31	O
,	O
purdue	O
university	O
,	O
lafayette	O
,	O
in	O
.	O
anderson	O
,	O
j	O
.	O
(	O
1982	O
)	O
.	O
logistic	B
discrimination	I
.	O
in	O
handbook	O
of	O
statistics	O
,	O
krishnaiah	O
,	O
p.	O
and	O
kanal	O
,	O
l.	O
,	O
editors	O
,	O
volume	O
2	O
,	O
pages	O
169-191.	O
north-holland	O
,	O
amsterdam	O
.	O
anderson	O
,	O
m.	O
and	O
benning	O
,	O
r.	O
(	O
1970	O
)	O
.	O
a	O
distribution-free	O
discrimination	O
procedure	O
based	O
on	O
clustering	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
16:541-548.	O
anderson	O
,	O
t.	O
(	O
1958	O
)	O
.	O
an	O
introduction	O
to	O
multivariate	O
statistical	O
analysis	O
.	O
john	O
wiley	O
,	O
new	O
york	O
.	O
anderson	O
,	O
t.	O
(	O
1966	O
)	O
.	O
some	O
nonparametric	O
multivariate	O
procedures	O
based	O
on	O
statistically	O
equivalent	O
blocks	O
.	O
in	O
multivariate	O
analysis	O
,	O
krishnaiah	O
,	O
p.	O
,	O
editor	O
,	O
pages	O
5-27.	O
academic	O
press	O
,	O
new	O
york	O
.	O
angluin	O
,	O
d.	O
and	O
valiant	O
,	O
l.	O
(	O
1979	O
)	O
.	O
fast	O
probabilistic	O
algorithms	O
for	O
hamiltonian	O
circuits	O
and	O
matchings	O
.	O
journal	O
of	O
computing	O
system	O
science	O
,	O
18:155-193.	O
anthony	O
,	O
m.	O
and	O
holden	O
,	O
s.	O
(	O
1993	O
)	O
.	O
on	O
the	O
power	O
of	O
polynomial	O
discriminators	O
and	O
radial	B
basis	I
function	I
networks	O
.	O
in	O
proceedings	O
of	O
the	O
sixth	O
annual	O
acm	O
conference	O
on	O
com	O
(	O
cid:173	O
)	O
putational	O
learning	B
theory	O
,	O
pages	O
158-164.	O
association	B
for	O
computing	O
machinery	O
,	O
new	O
york	O
.	O
anthony	O
,	O
m.	O
and	O
shawe-taylor	O
,	O
j	O
.	O
(	O
1990	O
)	O
.	O
a	O
result	O
ofvapnik	O
with	O
applications	O
.	O
technical	O
report	O
csd-tr-628	O
,	O
university	O
of	O
london	O
,	O
surrey	O
.	O
argentiero	O
,	O
p.	O
,	O
chin	O
,	O
r.	O
,	O
and	O
beaudet	O
,	O
p.	O
(	O
1982	O
)	O
.	O
an	O
automated	O
approach	O
to	O
the	O
design	O
of	O
de	O
(	O
cid:173	O
)	O
cision	O
tree	B
classifiers	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
4:51-57.	O
arkadjew	O
,	O
a.	O
and	O
braverman	O
,	O
e.	O
(	O
1966	O
)	O
.	O
zeichenerkennung	O
und	O
maschinelles	O
lernen	O
.	O
old-	O
enburg	O
verlag	O
,	O
miinchen	O
,	O
wien	O
.	O
ash	O
,	O
r.	O
(	O
1972	O
)	O
.	O
real	O
analysis	O
and	O
probability	O
.	O
academic	O
press	O
,	O
new	O
york	O
.	O
assouad	O
,	O
p.	O
(	O
1983a	O
)	O
.	O
densite	O
et	O
dimension	B
.	O
annales	O
de	O
l'institut	O
fourier	O
,	O
33:233-282.	O
assouad	O
,	O
p.	O
(	O
1983b	O
)	O
.	O
deux	O
remarques	O
sur	O
i	O
'estimation	O
.	O
comptes	O
rendus	O
de	O
l'academie	O
des	O
sciences	O
de	O
paris	O
,	O
296	O
:	O
1021-1024.	O
azuma	O
,	O
k.	O
(	O
1967	O
)	O
.	O
weighted	B
sums	O
of	O
certain	O
dependent	O
random	O
variables	O
.	O
tohoku	O
mathe	O
(	O
cid:173	O
)	O
matical	O
journal	O
,	O
68:357-367.	O
bahadur	O
,	O
r.	O
(	O
1961	O
)	O
.	O
a	O
representation	O
of	O
the	O
joint	O
distribution	B
of	O
responses	O
to	O
n	O
dichotomous	O
items	O
.	O
in	O
studies	O
in	O
item	O
analysis	O
and	O
prediction	O
,	O
solomon	O
,	O
h.	O
,	O
editor	O
,	O
pages	O
158-168.	O
stanford	O
university	O
press	O
,	O
stanford	O
,	O
ca	O
.	O
bailey	O
,	O
t.	O
and	O
jain	O
,	O
a	O
.	O
(	O
1978	O
)	O
.	O
a	O
note	O
on	O
distance-weighted	O
k-nearest	O
neighbor	O
rules	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
8:311-313.	O
barron	O
,	O
a	O
.	O
(	O
1985	O
)	O
.	O
logically	O
smooth	O
density	B
estimation	I
.	O
technical	O
report	O
tr	O
56	O
,	O
depart	O
(	O
cid:173	O
)	O
ment	O
of	O
statistics	O
,	O
stanford	O
university	O
,	O
stanford	O
,	O
ca	O
.	O
barron	O
,	O
a	O
.	O
(	O
1989	O
)	O
.	O
statistical	O
properties	O
of	O
artificial	O
neural	O
networks	O
.	O
in	O
proceedings	O
of	O
the	O
28th	O
conference	O
on	O
decision	O
and	O
control	O
,	O
pages	O
280-285.	O
tampa	O
,	O
fl	O
.	O
barron	O
,	O
a	O
.	O
(	O
1991	O
)	O
.	O
complexity	B
regularization	I
with	O
application	O
to	O
artificial	O
neural	O
networks	O
.	O
in	O
nonparametric	O
functional	O
estimation	B
and	O
related	O
topics	O
,	O
roussas	O
,	O
g.	O
,	O
editor	O
,	O
pages	O
561-576.	O
nato	O
asi	O
series	O
,	O
kluwer	O
academic	O
publishers	O
,	O
dordrecht	O
.	O
barron	O
,	O
a	O
.	O
(	O
1993	O
)	O
.	O
universal	B
approximation	O
bounds	O
for	O
superpositions	O
of	O
a	O
sigmoidal	O
func	O
(	O
cid:173	O
)	O
tion	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
39:930-944.	O
barron	O
,	O
a	O
.	O
(	O
1994	O
)	O
.	O
approximation	O
and	O
estimation	B
bounds	O
for	O
artificial	O
neural	O
networks	O
.	O
machine	O
learning	B
,	O
14:115-133.	O
references	O
595	O
barron	O
,	O
a.	O
and	O
barron	O
,	O
r.	O
(	O
1988	O
)	O
.	O
statistical	O
learning	B
networks	O
:	O
a	O
unifying	O
view	O
.	O
in	O
pro	O
(	O
cid:173	O
)	O
ceedings	O
of	O
the	O
20th	O
symposium	O
on	O
the	O
interface	O
:	O
computing	O
science	O
and	O
statistics	B
,	O
wegman	O
,	O
e.	O
,	O
gantz	O
,	O
d.	O
,	O
and	O
miller	O
,	O
j.	O
,	O
editors	O
,	O
pages	O
192-203.	O
ams	O
,	O
alexandria	O
,	O
va.	O
barron	O
,	O
a.	O
and	O
cover	O
,	O
t.	O
(	O
1991	O
)	O
.	O
minimum	O
complexity	O
density	B
estimation	I
.	O
ieee	O
transac	O
(	O
cid:173	O
)	O
tions	O
on	O
information	O
theory	O
,	O
37	O
:	O
1034-1054.	O
barron	O
,	O
a.	O
,	O
gyorfi	O
,	O
l.	O
,	O
and	O
van	O
der	O
meulen	O
,	O
e.	O
(	O
1992	O
)	O
.	O
distribution	B
estimation	O
consistent	O
in	O
total	B
variation	I
and	O
in	O
two	O
types	O
of	O
information	O
divergence	B
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
38	O
:	O
1437-1454.	O
barron	O
,	O
r.	O
(	O
1975	O
)	O
.	O
learning	B
networks	O
improve	O
computer-aided	O
prediction	O
and	O
control	O
.	O
com	O
(	O
cid:173	O
)	O
puter	O
design	O
,	O
75:65-70.	O
bartlett	O
,	O
p.	O
(	O
1993	O
)	O
.	O
lower	O
bounds	O
on	O
the	O
vapnik-chervonenkis	O
dimension	B
of	O
multi-layer	O
threshold	B
networks	O
.	O
in	O
proceedings	O
of	O
the	O
sixth	O
annual	O
acm	O
conference	O
on	O
computa	O
(	O
cid:173	O
)	O
tional	O
learning	B
theory	O
,	O
pages	O
144-150.	O
association	B
for	O
computing	O
machinery	O
,	O
new	O
york	O
.	O
bashkirov	O
,	O
0.	O
,	O
braverman	O
,	O
e.	O
,	O
and	O
muchnik	O
,	O
i	O
.	O
(	O
1964	O
)	O
.	O
potential	O
function	O
algorithms	O
for	O
pattern	O
recognition	O
learning	B
machines	O
.	O
automation	O
and	O
remote	O
control	O
,	O
25:692-695.	O
baum	O
,	O
e.	O
(	O
1988	O
)	O
.	O
on	O
the	O
capabilities	O
of	O
multilayer	O
perceptrons	O
.	O
journal	O
of	O
complexity	O
,	O
4:193-215.	O
baum	O
,	O
e.	O
and	O
haussler	O
,	O
d.	O
(	O
1989	O
)	O
.	O
what	O
size	O
net	O
gives	O
valid	O
generalization	O
?	O
neural	O
com	O
(	O
cid:173	O
)	O
putation	O
,	O
1:151-160.	O
beakley	O
,	O
g.	O
and	O
tuteur	O
,	O
f.	O
(	O
1972	O
)	O
.	O
distribution-free	O
pattern	O
verification	O
using	O
statistically	B
equivalent	I
blocks	I
.	O
ieee	O
transactions	O
on	O
computers	O
,	O
21	O
:	O
l337-l347	O
.	O
beck	O
,	O
j	O
.	O
(	O
1979	O
)	O
.	O
the	O
exponential	B
rate	O
of	O
convergence	O
of	O
error	O
for	O
kn-nn	O
nonparametric	O
regression	O
and	O
decision	O
.	O
problems	O
of	O
control	O
and	O
information	O
theory	O
,	O
8:303-311.	O
becker	O
,	O
p.	O
(	O
1968	O
)	O
.	O
recognition	O
of	O
patterns	O
.	O
polyteknisk	O
forlag	O
,	O
copenhagen	O
.	O
ben-bassat	O
,	O
m.	O
(	O
1982	O
)	O
.	O
use	O
of	O
distance	O
measures	O
,	O
information	O
measures	O
and	O
error	O
bounds	O
in	O
feature	O
evaluation	O
.	O
in	O
handbook	O
of	O
statistics	O
,	O
krishnaiah	O
,	O
p.	O
and	O
kanal	O
,	O
l.	O
,	O
editors	O
,	O
volume	O
2	O
,	O
pages	O
773-792.	O
north-holland	O
,	O
amsterdam	O
.	O
benedek	O
,	O
g.	O
and	O
itai	O
,	O
a	O
.	O
(	O
1988	O
)	O
.	O
learnability	O
by	O
fixed	O
distributions	O
.	O
in	O
computational	O
learning	B
theory	O
:	O
proceedings	O
of	O
the	O
1988	O
workshop	O
,	O
pages	O
80-90.	O
morgan	O
kaufman	O
,	O
san	O
mateo	O
,	O
ca	O
.	O
benedek	O
,	O
g.	O
and	O
itai	O
,	O
a	O
.	O
(	O
1994	O
)	O
.	O
nonuniform	O
learnability	O
.	O
journal	O
of	O
computer	O
and	O
systems	O
sciences	O
,	O
48:311-323.	O
bennett	O
,	O
g.	O
(	O
1962	O
)	O
.	O
probability	O
inequalities	O
for	O
the	O
sum	O
of	O
independent	O
random	O
variables	O
.	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
,	O
57:33-45.	O
beran	O
,	O
r.	O
(	O
1977	O
)	O
.	O
minimum	O
hellinger	O
distance	B
estimates	O
for	O
parametric	O
models	O
.	O
annals	O
of	O
statistics	O
,	O
5:445-463.	O
beran	O
,	O
r.	O
(	O
1988	O
)	O
.	O
comments	O
on	O
``	O
a	O
new	O
theoretical	B
and	O
algorithmical	O
basis	O
for	O
estimation	B
,	O
identification	O
and	O
control	O
''	O
by	O
p.	O
kovanic	O
.	O
automatica	O
,	O
24:283-287.	O
bernstein	O
,	O
s.	O
(	O
1946	O
)	O
.	O
the	O
theory	O
of	O
probabilities	O
.	O
gastehizdat	O
publishing	O
house	O
,	O
moscow	O
.	O
bhattacharya	O
,	O
p.	O
and	O
mack	O
,	O
y	O
.	O
(	O
1987	O
)	O
.	O
weak	B
convergence	O
of	O
k-nn	O
density	O
and	O
regression	O
estimators	O
with	O
varying	O
k	O
and	O
applications	O
.	O
annals	O
of	O
statistics	O
,	O
15:976-994.	O
bhattacharyya	O
,	O
a	O
.	O
(	O
1946	O
)	O
.	O
on	O
a	O
measure	O
of	O
divergence	O
between	O
two	O
multinomial	B
popula	O
(	O
cid:173	O
)	O
tions	O
.	O
sankhya	O
,	O
series	O
a	O
,	O
7:401-406.	O
bickel	O
,	O
p.	O
and	O
breiman	O
,	O
l.	O
(	O
1983	O
)	O
.	O
sums	O
of	O
functions	O
of	O
nearest	O
neighbor	O
distances	O
,	O
moment	O
bounds	O
,	O
limit	O
theorems	O
and	O
a	O
goodness	O
of	O
fit	O
test	O
.	O
annals	O
of	O
probability	O
,	O
11:185-214.	O
birge	O
,	O
l.	O
(	O
1983	O
)	O
.	O
approximation	O
dans	O
les	O
espaces	O
metriques	O
et	O
theorie	O
de	O
l'estimation	O
.	O
zeitschrijt	O
for	O
wahrscheinlichkeitstheorie	O
und	O
verwandte	O
gebiete	O
,	O
65	O
:	O
181-237	O
.	O
596	O
references	O
birge	O
,	O
l.	O
(	O
1986	O
)	O
.	O
on	O
estimating	O
a	O
density	O
using	O
hellinger	O
distance	B
and	O
some	O
other	O
strange	O
facts	O
.	O
probability	O
theory	O
and	O
related	O
fields	O
,	O
71:271-291.	O
blumer	O
,	O
a.	O
,	O
ehrenfeucht	O
,	O
a.	O
,	O
haussler	O
,	O
d.	O
,	O
and	O
warmuth	O
,	O
m.	O
(	O
1989	O
)	O
.	O
learnability	O
and	O
the	O
vapnik-chervonenkis	O
dimension	B
.	O
journal	O
of	O
the	O
acm	O
,	O
36:929-965.	O
braverman	O
,	O
e.	O
(	O
1965	O
)	O
.	O
the	O
method	O
of	O
potential	O
functions	O
.	O
automation	O
and	O
remote	O
control	O
,	O
26:2130-2138.	O
braverman	O
,	O
e.	O
and	O
pyatniskii	O
,	O
e.	O
(	O
1966	O
)	O
.	O
estimation	B
of	I
the	O
rate	B
of	I
convergence	I
of	O
algorithms	O
based	O
on	O
the	O
potential	O
function	O
method	O
.	O
automation	O
and	O
remote	O
control	O
,	O
27:80-100.	O
breiman	O
,	O
l.	O
,	O
friedman	O
,	O
j.	O
,	O
olshen	O
,	O
r.	O
,	O
and	O
stone	O
,	O
c.	O
(	O
1984	O
)	O
.	O
classification	O
and	O
regression	O
trees	O
.	O
wadsworth	O
international	O
,	O
belmont	O
,	O
ca	O
.	O
breiman	O
,	O
l.	O
,	O
meisel	O
,	O
w.	O
,	O
and	O
purcell	O
,	O
e.	O
(	O
1977	O
)	O
.	O
variable	B
kernel	O
estimates	O
of	O
multivariate	O
densities	O
.	O
technometrics	O
,	O
19	O
:	O
135-144.	O
brent	O
,	O
r.	O
(	O
1991	O
)	O
.	O
fast	O
training	O
algorithms	O
for	O
multilayer	B
neural	O
nets	O
.	O
ieee	O
transactions	O
on	O
neural	O
networks	O
,	O
2:346-354.	O
broder	O
,	O
a	O
.	O
(	O
1990	O
)	O
.	O
strategies	O
for	O
efficient	O
incremental	O
nearest	B
neighbor	I
search	O
.	O
pattern	O
recognition	O
,	O
23:171-178.	O
broornhead	O
,	O
d.	O
and	O
lowe	O
,	O
d.	O
(	O
1988	O
)	O
.	O
multivariable	O
functional	O
interpolation	O
and	O
adaptive	O
networks	O
.	O
complex	O
systems	O
,	O
2:321-323.	O
buescher	O
,	O
k.	O
and	O
kumar	O
,	O
p.	O
(	O
1996a	O
)	O
.	O
learning	B
by	O
canonical	O
smooth	O
estimation	B
,	O
part	O
i	O
:	O
simultaneous	O
estimation	B
.	O
ieee	O
transactions	O
on	O
automatic	B
control	O
,	O
41	O
:545-556.	O
buescher	O
,	O
k.	O
and	O
kumar	O
,	O
p.	O
(	O
1996b	O
)	O
.	O
learning	B
by	O
canonical	O
smooth	O
estimation	B
,	O
part	O
ii	O
:	O
learning	B
and	O
choice	O
of	O
model	O
complexity	O
.	O
ieee	O
transactions	O
on	O
automatic	B
control	O
,	O
41:557-569.	O
burbea	O
,	O
j	O
.	O
(	O
1984	O
)	O
.	O
the	O
convexity	O
with	O
respect	O
to	O
gaussian	O
distributions	O
of	O
divergences	O
of	O
order	O
ex	O
.	O
utilitas	O
mathematica	O
,	O
26	O
:	O
171-192.	O
burbea	O
,	O
j.	O
and	O
rao	O
,	O
c.	O
(	O
1982	O
)	O
.	O
on	O
the	O
convexity	O
of	O
some	O
divergence	B
measures	O
based	O
on	O
entropy	O
functions	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
28:48-495.	O
burshtein	O
,	O
d.	O
,	O
della	O
pietra	O
,	O
v.	O
,	O
kanevsky	O
,	O
d.	O
,	O
and	O
nadas	O
,	O
a	O
.	O
(	O
1992	O
)	O
.	O
minimum	O
impurity	O
partitions	O
.	O
annals	O
of	O
statistics	O
,	O
20	O
:	O
1637-1646.	O
cacoullos	O
,	O
t.	O
(	O
1965	O
)	O
.	O
estimation	B
of	I
a	O
multivariate	O
density	O
.	O
annals	O
of	O
the	O
institute	O
of	O
statis	O
(	O
cid:173	O
)	O
tical	O
mathematics	O
,	O
18	O
:	O
179-190.	O
carnal	O
,	O
h.	O
(	O
1970	O
)	O
.	O
die	O
konvexe	O
hiille	O
von	O
n	O
rotationssymmetrisch	O
verteilten	O
punkten	O
.	O
zeitschrijt	O
for	O
wahrscheinlichkeitstheorie	O
und	O
verwandte	O
gebiete	O
,	O
15	O
:	O
168-176.	O
casey	O
,	O
r.	O
and	O
nagy	O
,	O
g.	O
(	O
1984	O
)	O
.	O
decision	O
tree	O
design	O
using	O
a	O
probabilistic	O
model	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
30:93-99.	O
cencov	O
,	O
n.	O
(	O
1962	O
)	O
.	O
evaluation	O
of	O
an	O
unknown	O
distribution	B
density	O
from	O
observations	O
.	O
soviet	O
math	O
.	O
doklady	O
,	O
3:1559-1562.	O
chang	O
,	O
c.	O
(	O
1974	O
)	O
.	O
finding	O
prototypes	O
for	O
nearest	B
neighbor	I
classifiers	O
.	O
ieee	O
transactions	O
on	O
computers	O
,	O
26:1179-1184.	O
chang	O
,	O
c.	O
(	O
1973	O
)	O
.	O
dynamic	O
programming	O
as	O
applied	O
to	O
feature	O
selection	O
in	O
pattern	O
recog	O
(	O
cid:173	O
)	O
nition	O
systems	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
3:166-171.	O
chen	O
,	O
t.	O
,	O
chen	O
,	O
h.	O
,	O
and	O
liu	O
,	O
r.	O
(	O
1990	O
)	O
.	O
a	O
constructive	O
proof	O
and	O
an	O
extension	O
of	O
cybenko	O
's	O
approximation	O
theorem	O
.	O
in	O
proceedings	O
of	O
the	O
22nd	O
symposium	O
of	O
the	O
interface	O
:	O
com	O
(	O
cid:173	O
)	O
puting	O
science	O
and	O
statistics	B
,	O
pages	O
163-168.	O
american	O
statistical	O
association	B
,	O
alexan	O
(	O
cid:173	O
)	O
dria	O
,	O
va.	O
chen	O
,	O
x.	O
and	O
zhao	O
,	O
l.	O
(	O
1987	O
)	O
.	O
almost	O
sure	O
l	O
i-norm	O
convergence	O
for	O
data-based	B
histogram	O
density	O
estimates	O
.	O
journal	O
of	O
multivariate	O
analysis	O
,	O
21	O
:	O
179-188.	O
references	O
597	O
chen	O
,	O
z.	O
and	O
fu	O
,	O
k.	O
(	O
1973	O
)	O
.	O
nonparametric	O
bayes	O
risk	O
estimation	B
for	O
pattern	O
classification	O
.	O
in	O
proceedings	O
of	O
the	O
ieee	O
conference	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
.	O
boston	O
,	O
ma	O
.	O
chernoff	O
,	O
h.	O
(	O
1952	O
)	O
.	O
a	O
measure	O
of	O
asymptotic	O
efficiency	O
of	O
tests	O
of	O
a	O
hypothesis	O
based	O
on	O
the	O
sum	O
of	O
observations	O
.	O
annals	O
of	O
mathematical	O
statistics	B
,	O
23:493-507.	O
chernoff	O
,	O
h.	O
(	O
1971	O
)	O
.	O
a	O
bound	O
on	O
the	O
classification	O
error	O
for	O
discriminating	O
between	O
popu	O
(	O
cid:173	O
)	O
lations	O
with	O
specified	O
means	O
and	O
variances	O
.	O
in	O
studi	O
di	O
probabilita	O
,	O
statistica	O
e	O
ricerca	O
operativa	O
in	O
onare	O
di	O
giuseppe	O
pompilj	O
,	O
pages	O
203-211.	O
oderisi	O
,	O
gubbio	O
.	O
chou	O
,	O
p.	O
(	O
1991	O
)	O
.	O
optimal	O
partitioning	O
for	O
classification	O
and	O
regression	O
trees	O
.	O
ieee	O
trans	O
(	O
cid:173	O
)	O
actions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
13:340-354.	O
chou	O
,	O
w.	O
and	O
chen	O
,	O
y	O
.	O
(	O
1992	O
)	O
.	O
a	O
new	O
fast	O
algorithm	B
for	O
effective	O
training	O
of	O
neural	O
classi	O
(	O
cid:173	O
)	O
fiers	O
.	O
pattern	O
recognition	O
,	O
25:423-429.	O
chow	O
,	O
c.	O
(	O
1965	O
)	O
.	O
statistical	O
independence	O
and	O
threshold	B
functions	O
.	O
ieee	O
transactions	O
on	O
computers	O
,	O
e-14:66-68.	O
chow	O
,	O
c.	O
(	O
1970	O
)	O
.	O
on	O
optimum	O
recognition	O
error	O
and	O
rejection	O
tradeoff	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
16:41-46.	O
chow	O
,	O
y.	O
and	O
teicher	O
,	O
h.	O
(	O
1978	O
)	O
.	O
probability	O
theory	O
,	O
independence	O
,	O
interchangeability	O
,	O
martingales	O
.	O
springer-verlag	O
,	O
new	O
york	O
.	O
ciampi	O
,	O
a	O
.	O
(	O
1991	O
)	O
.	O
generalized	B
regression	O
trees	O
.	O
computational	O
statistics	B
and	O
data	O
anal	O
(	O
cid:173	O
)	O
ysis	O
,	O
12:57-78.	O
collomb	O
,	O
g.	O
(	O
1979	O
)	O
.	O
estimation	B
de	O
la	O
regression	O
par	O
la	O
methode	O
des	O
k	O
points	O
les	O
plus	O
proches	O
:	O
proprietes	O
de	O
convergence	O
ponctuelle	O
.	O
comptes	O
rendus	O
de	O
l	O
'	O
academie	O
des	O
sciences	O
de	O
paris	O
,	O
289:245-247.	O
collomb	O
,	O
g.	O
(	O
1980	O
)	O
.	O
estimation	B
de	O
la	O
regression	O
par	O
la	O
methode	O
des	O
k	O
points	O
les	O
plus	O
proches	O
avec	O
noyau	O
.	O
lecture	O
notes	O
in	O
mathematics	O
#	O
821	O
,	O
springer-verlag	O
,	O
berlin	O
.	O
159-175.	O
collomb	O
,	O
g.	O
(	O
1981	O
)	O
.	O
estimation	B
non	O
parametrique	O
de	O
la	O
regression	O
:	O
revue	O
bibliographique	O
.	O
international	O
statistical	O
review	O
,	O
49:75-93.	O
conway	O
,	O
j.	O
and	O
sloane	O
,	O
n.	O
(	O
1993	O
)	O
.	O
sphere-packings	O
,	O
lattices	O
and	O
groups	O
.	O
springer-verlag	O
,	O
berlin	O
.	O
coomans	O
,	O
d.	O
and	O
broeckaert	O
,	O
i	O
.	O
(	O
1986	O
)	O
.	O
potential	O
pattern	O
recognition	O
in	O
chemical	O
and	O
medical	O
decision	O
making	O
.	O
research	O
studies	O
press	O
,	O
letchworth	O
,	O
hertfordshire	O
,	O
england	O
.	O
cormen	O
,	O
t.	O
,	O
leiserson	O
,	O
c.	O
,	O
and	O
rivest	O
,	O
r.	O
(	O
1990	O
)	O
.	O
introduction	O
to	O
algorithms	O
.	O
mit	O
press	O
,	O
boston	O
,	O
ma	O
.	O
cover	O
,	O
t.	O
(	O
1965	O
)	O
.	O
geometrical	O
and	O
statistical	O
properties	O
of	O
systems	O
of	O
linear	O
inequalities	O
with	O
applications	O
in	O
pattern	O
recognition	O
.	O
ieee	O
transactions	O
on	O
electronic	O
computers	O
,	O
14:326-334.	O
cover	O
,	O
t.	O
(	O
1968a	O
)	O
.	O
estimation	B
by	O
the	O
nearest	B
neighbor	I
rule	I
.	O
ieee	O
transactions	O
on	O
informa	O
(	O
cid:173	O
)	O
tion	O
theory	O
,	O
14:50-55.	O
cover	O
,	O
t.	O
(	O
1968b	O
)	O
.	O
rates	O
of	O
convergence	O
for	O
nearest	B
neighbor	I
procedures	O
.	O
in	O
proceedings	O
of	O
the	O
hawaii	O
international	O
conference	O
on	O
systems	O
sciences	O
,	O
pages	O
413-415.	O
honolulu	O
.	O
cover	O
,	O
t.	O
(	O
1969	O
)	O
.	O
learning	B
in	O
pattern	O
recognition	O
.	O
in	O
methodologies	O
of	O
pattern	O
recognition	O
,	O
watanabe	O
,	O
s.	O
,	O
editor	O
,	O
pages	O
111-132.	O
academic	O
press	O
,	O
new	O
york	O
.	O
cover	O
,	O
t.	O
(	O
1974	O
)	O
.	O
the	O
best	O
two	O
independent	O
measurements	O
are	O
not	O
the	O
two	O
best	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
4:116-117.	O
cover	O
,	O
t.	O
and	O
hart	O
,	O
p.	O
(	O
1967	O
)	O
.	O
nearest	B
neighbor	I
pattern	O
classification	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
13:21-27.	O
cover	O
,	O
t.	O
and	O
thomas	O
,	O
j	O
.	O
(	O
1991	O
)	O
.	O
elements	O
of	O
information	O
theory	O
.	O
john	O
wiley	O
,	O
new	O
york	O
.	O
cover	O
,	O
t.	O
and	O
van	O
campenhout	O
,	O
j	O
.	O
(	O
1977	O
)	O
.	O
on	O
the	O
possible	O
orderings	O
in	O
the	O
measurement	O
selection	B
problem	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
7:657-661	O
.	O
598	O
references	O
cover	O
,	O
t.	O
and	O
wagner	O
,	O
t.	O
(	O
1975	O
)	O
.	O
topics	O
in	O
statistical	O
pattern	O
recognition	O
.	O
communication	O
and	O
cybernetics	O
,	O
10	O
:	O
15-46.	O
cramer	O
,	O
h.	O
and	O
wold	O
,	O
h.	O
(	O
1936	O
)	O
.	O
some	O
theorems	O
on	O
distribution	B
functions	O
.	O
journal	O
of	O
the	O
london	O
mathematical	O
society	O
,	O
11:290-294.	O
csibi	O
,	O
s.	O
(	O
1971	O
)	O
.	O
simple	O
and	O
compound	O
processes	O
in	O
iterative	O
machine	O
learning	B
.	O
technical	O
report	O
,	O
cism	O
summer	O
course	O
,	O
udine	O
,	O
italy	O
.	O
csibi	O
,	O
s.	O
(	O
1975	O
)	O
.	O
using	O
indicators	O
as	O
a	O
base	O
for	O
estimating	O
optimal	O
decision	O
functions	O
.	O
in	O
colloquia	O
mathematica	O
societatis	O
janos	O
botyai	O
:	O
topics	O
in	O
information	O
theory	O
,	O
pages	O
143-153.	O
keszthely	O
,	O
hungary	O
.	O
csiszar	O
,	O
i	O
.	O
(	O
1967	O
)	O
.	O
information-type	O
measures	O
of	O
difference	O
of	O
probability	O
distributions	O
and	O
indirect	O
observations	O
.	O
studia	O
scientiarium	O
mathematicarum	O
hungarica	O
,	O
2:299-318.	O
csiszar	O
,	O
i	O
.	O
(	O
1973	O
)	O
.	O
generalized	B
entropy	O
and	O
quantization	B
problems	O
.	O
in	O
transactions	O
of	O
the	O
sixth	O
prague	O
conference	O
on	O
information	O
theory	O
,	O
statistical	O
decision	O
functions	O
,	O
ran	O
(	O
cid:173	O
)	O
dom	O
processes	O
,	O
pages	O
159-174.	O
academia	O
,	O
prague	O
.	O
csiszar	O
,	O
i.	O
and	O
korner	O
,	O
j	O
.	O
(	O
1981	O
)	O
.	O
information	O
theory	O
:	O
coding	O
theoremsfor	O
discrete	O
mem	O
(	O
cid:173	O
)	O
oryless	O
systems	O
.	O
academic	O
press	O
,	O
new	O
york	O
.	O
cybenko	O
,	O
g.	O
(	O
1989	O
)	O
.	O
approximations	O
by	O
superpositions	O
of	O
sigmoidal	O
functions	O
.	O
math	O
.	O
con	O
(	O
cid:173	O
)	O
trol	O
,	O
signals	O
,	O
systems	O
,	O
2:303-314.	O
darken	O
,	O
c.	O
,	O
donahue	O
,	O
m.	O
,	O
gurvits	O
,	O
l.	O
,	O
and	O
sontag	O
,	O
e.	O
(	O
1993	O
)	O
.	O
rate	O
of	O
approximation	O
results	O
motivated	O
by	O
robust	O
neural	B
network	I
learning	O
.	O
in	O
proceedings	O
of	O
the	O
sixth	O
acm	O
work	O
(	O
cid:173	O
)	O
shop	O
on	O
computational	O
learning	B
theory	O
,	O
pages	O
303-309.	O
association	B
for	O
computing	O
machinery	O
,	O
new	O
york	O
.	O
das	O
gupta	O
,	O
s.	O
(	O
1964	O
)	O
.	O
nonparametric	O
classification	O
rules	O
.	O
sankhya	O
series	O
a	O
,	O
26:25-30.	O
das	O
gupta	O
,	O
s.	O
and	O
lin	O
,	O
h.	O
(	O
1980	O
)	O
.	O
nearest	B
neighbor	I
rules	I
of	O
statistical	O
classification	O
based	O
on	O
ranks	O
.	O
sankhya	O
series	O
a	O
,	O
42:419-430.	O
dasarathy	O
,	O
b	O
.	O
(	O
1991	O
)	O
.	O
nearest	B
neighbor	I
pattern	O
classification	O
techniques	O
.	O
ieee	O
computer	O
society	O
press	O
,	O
los	O
alamitos	O
,	O
ca	O
.	O
day	O
,	O
n.	O
and	O
kerridge	O
,	O
d.	O
(	O
1967	O
)	O
.	O
a	O
general	O
maximum	B
likelihood	I
discriminant	O
.	O
biometrics	O
,	O
23:313-324.	O
de	O
guzman	O
,	O
m.	O
(	O
1975	O
)	O
.	O
differentiation	O
of	O
integrals	O
in	O
rn	O
.	O
lecture	O
notes	O
in	O
mathematics	O
#	O
481	O
,	O
springer-verlag	O
,	O
berlin	O
.	O
deheuvels	O
,	O
p.	O
(	O
1977	O
)	O
.	O
estimation	B
nonparametrique	O
de	O
la	O
densite	O
par	O
histogrammes	O
gener	O
(	O
cid:173	O
)	O
alises	O
.	O
publications	O
de	O
l'institut	O
de	O
statistique	O
de	O
l	O
'	O
universite	O
de	O
paris	O
,	O
22	O
:	O
1-23.	O
devijver	O
,	O
p.	O
(	O
1978	O
)	O
.	O
a	O
note	O
on	O
ties	O
in	O
voting	O
with	O
the	O
k-nn	O
rule	B
.	O
pattern	O
recognition	O
,	O
10:297-	O
298.	O
devijver	O
,	O
p.	O
(	O
1979	O
)	O
.	O
new	O
error	O
bounds	O
with	O
the	O
nearest	B
neighbor	I
rule	I
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
25:749-753.	O
devijver	O
,	O
p.	O
(	O
1980	O
)	O
.	O
an	O
overview	O
of	O
asymptotic	O
properties	O
of	B
nearest	I
neighbor	I
rules	I
.	O
in	O
pattern	O
recognition	O
in	O
practice	O
,	O
gelsema	O
,	O
e.	O
and	O
kanal	O
,	O
l.	O
,	O
editors	O
,	O
pages	O
343-350.	O
elsevier	O
science	O
publishers	O
,	O
amsterdam	O
.	O
devijver	O
,	O
p.	O
and	O
kittler	O
,	O
j	O
.	O
(	O
1980	O
)	O
.	O
on	O
the	O
edited	B
nearest	O
neighbor	B
rule	I
.	O
in	O
proceedings	O
of	O
the	O
fifth	O
international	O
conference	O
on	O
pattern	O
recognition	O
,	O
pages	O
72-80.	O
pattern	O
recognition	O
society	O
,	O
los	O
alamitos	O
,	O
ca	O
.	O
devijver	O
,	O
p.	O
and	O
kittler	O
,	O
j	O
.	O
(	O
1982	O
)	O
.	O
pattern	O
recognition	O
:	O
a	O
statistical	O
approach	O
.	O
prentice	O
(	O
cid:173	O
)	O
hall	O
,	O
englewood	O
cliffs	O
,	O
nj	O
.	O
devroye	O
,	O
l.	O
(	O
1978	O
)	O
.	O
a	O
universal	O
k-nearest	O
neighbor	O
procedure	O
in	O
discrimination	O
.	O
in	O
pro	O
(	O
cid:173	O
)	O
ceedings	O
of	O
the	O
1978	O
ieee	O
computer	O
society	O
conference	O
on	O
pattern	O
recognition	O
and	O
image	O
processing	O
,	O
pages	O
142-147.	O
ieee	O
computer	O
society	O
,	O
long	O
beach	O
,	O
ca	O
.	O
references	O
599	O
devroye	O
,	O
l.	O
(	O
1981a	O
)	O
.	O
on	O
the	O
almost	O
everywhere	O
convergence	O
of	O
nonparametric	O
regression	B
function	I
estimates	O
.	O
annals	O
of	O
statistics	O
,	O
9	O
:	O
1310-1309.	O
devroye	O
,	O
l.	O
(	O
1981	O
b	O
)	O
.	O
on	O
the	O
asymptotic	O
probability	O
of	O
error	O
in	O
nonparametric	O
discrimina	O
(	O
cid:173	O
)	O
tion	O
.	O
annals	O
of	O
statistics	O
,	O
9	O
:	O
1320-1327.	O
devroye	O
,	O
l.	O
(	O
1981	O
c	O
)	O
.	O
on	O
the	O
inequality	B
of	O
cover	O
and	O
hart	O
in	O
nearest	O
neighbor	O
discrimination	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
3:75-78.	O
devroye	O
,	O
l.	O
(	O
1982a	O
)	O
.	O
bounds	O
for	O
the	O
uniform	B
deviation	O
of	O
empirical	O
measures	O
.	O
journal	O
of	O
multivariate	O
analysis	O
,	O
12:72-79.	O
devroye	O
,	O
l.	O
(	O
1982b	O
)	O
.	O
necessary	O
and	O
sufficient	O
conditions	O
for	O
the	O
almost	O
everywhere	O
con	O
(	O
cid:173	O
)	O
vergence	O
of	O
nearest	O
neighbor	O
regression	O
function	O
estimates	O
.	O
zeitschrijt	O
for	O
wahrschein	O
(	O
cid:173	O
)	O
lichkeitstheorie	O
und	O
verwandte	O
gebiete	O
,	O
61:467-481.	O
devroye	O
,	O
l.	O
(	O
1983	O
)	O
.	O
the	O
equivalence	O
of	O
weak	O
,	O
strong	B
and	O
complete	O
convergence	O
in	O
l1	O
for	O
kernel	O
density	O
estimates	O
.	O
annals	O
of	O
statistics	O
,	O
11:896-904.	O
devroye	O
,	O
l.	O
(	O
1987	O
)	O
.	O
a	O
course	O
in	O
density	O
estimation	B
.	O
birkhauser	O
,	O
boston	O
,	O
ma	O
.	O
devroye	O
,	O
l.	O
(	O
1988a	O
)	O
.	O
applications	O
of	O
the	O
theory	O
of	O
records	O
in	O
the	O
study	O
of	O
random	O
trees	O
.	O
acta	O
informatica	O
,	O
26	O
:	O
123-130.	O
devroye	O
,	O
l.	O
(	O
1988b	O
)	O
.	O
automatic	B
pattern	O
recognition	O
:	O
a	O
study	O
of	O
the	O
probability	O
of	O
error	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
10:530-543.	O
devroye	O
,	O
l.	O
(	O
1988c	O
)	O
.	O
the	O
expected	O
size	O
of	O
some	O
graphs	O
in	O
computational	O
geometry	O
.	O
com	O
(	O
cid:173	O
)	O
puters	O
and	O
mathematics	O
with	O
applications	O
,	O
15:53-64.	O
devroye	O
,	O
l.	O
(	O
1988d	O
)	O
.	O
the	O
kernel	B
estimate	O
is	O
relatively	O
stable	O
.	O
probability	O
theory	O
and	O
related	O
fields	O
,	O
77:521-536.	O
devroye	O
,	O
l.	O
(	O
1991	O
a	O
)	O
.	O
exponential	B
inequalities	O
in	O
nonparametric	O
estimation	B
.	O
in	O
n	O
onparamet	O
(	O
cid:173	O
)	O
ric	O
functional	O
estimation	B
and	O
related	O
topics	O
,	O
roussas	O
,	O
g.	O
,	O
editor	O
,	O
pages	O
31-44.	O
nato	O
asi	O
series	O
,	O
kluwer	O
academic	O
publishers	O
,	O
dordrecht	O
.	O
devroye	O
,	O
l.	O
(	O
1991b	O
)	O
.	O
on	O
the	O
oscillation	O
of	O
the	O
expected	O
number	O
of	O
points	O
on	O
a	O
random	O
convex	B
hull	I
.	O
statistics	B
and	O
probability	O
letters	O
,	O
11	O
:281-286.	O
devroye	O
,	O
l.	O
and	O
gyorfi	O
,	O
l.	O
(	O
1983	O
)	O
.	O
distribution-free	O
exponential	B
bound	O
on	O
the	O
l	O
1	O
error	O
of	O
partitioning	O
estimates	O
of	O
a	O
regression	O
function	O
.	O
in	O
proceedings	O
of	O
the	O
fourth	O
pannonian	O
symposium	O
on	O
mathematical	O
statistics	B
,	O
konecny	O
,	O
e	O
,	O
mogyor6di	O
,	O
j.	O
,	O
and	O
wertz	O
,	O
w.	O
,	O
editors	O
,	O
pages	O
67-76.	O
akademiai	O
kiad6	O
,	O
budapest	O
,	O
hungary	O
.	O
devroye	O
,	O
l.	O
and	O
gyorfi	O
,	O
l.	O
(	O
1985	O
)	O
.	O
nonparametric	O
density	B
estimation	I
:	O
the	O
l1	O
view	O
.	O
john	O
wiley	O
,	O
new	O
york	O
.	O
devroye	O
,	O
l.	O
and	O
gyorfi	O
,	O
l.	O
(	O
1992	O
)	O
.	O
no	O
empirical	B
probability	O
measure	B
can	O
converge	O
in	O
the	O
total	B
variation	I
sense	O
for	O
all	O
distributions	O
.	O
annals	O
of	O
statistics	O
,	O
18:1496-1499.	O
devroye	O
,	O
l.	O
,	O
gyorfi	O
,	O
l.	O
,	O
krzyzak	O
,	O
a.	O
,	O
and	O
lugosi	O
,	O
g.	O
(	O
1994	O
)	O
.	O
on	O
the	O
strong	B
universal	I
consis	O
(	O
cid:173	O
)	O
tency	O
of	O
nearest	O
neighbor	O
regression	O
function	O
estimates	O
.	O
annals	O
of	O
statistics	O
,	O
22	O
:	O
1371-	O
1385.	O
devroye	O
,	O
l.	O
and	O
krzyzak	O
,	O
a	O
.	O
(	O
1989	O
)	O
.	O
an	O
equivalence	O
theorem	B
for	O
l1	O
convergence	O
of	O
the	O
kernel	B
regression	O
estimate	B
.	O
journal	O
of	O
statistical	O
planning	O
and	O
inference	O
,	O
23:71-82.	O
devroye	O
,	O
l.	O
and	O
laforest	O
,	O
l.	O
(	O
1990	O
)	O
.	O
an	O
analysis	O
of	O
random	O
d-dimensional	O
quadtrees	O
.	O
siam	O
journal	O
on	O
computing	O
,	O
19:821-832.	O
devroye	O
,	O
l.	O
and	O
lugosi	O
,	O
g.	O
(	O
1995	O
)	O
.	O
lower	O
bounds	O
in	O
pattern	O
recognition	O
and	O
learning	B
.	O
pattern	O
recognition	O
,	O
28	O
:	O
1011-1018.	O
devroye	O
,	O
l.	O
and	O
machell	O
,	O
e	O
(	O
1985	O
)	O
.	O
data	O
structures	O
in	O
kernel	O
density	B
estimation	I
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
7:360-366.	O
devroye	O
,	O
l.	O
and	O
wagner	O
,	O
t.	O
(	O
1976a	O
)	O
.	O
a	O
distribution-free	O
performance	O
bound	O
in	O
error	O
esti	O
(	O
cid:173	O
)	O
mation	O
.	O
ieee	O
transactions	O
information	O
theory	O
,	O
22:586-587	O
.	O
600	O
references	O
devroye	O
,	O
l.	O
and	O
wagner	O
,	O
t.	O
(	O
1976b	O
)	O
.	O
nonparametric	O
discrimination	O
and	O
density	B
estimation	I
.	O
technical	O
report	O
183	O
,	O
electronics	O
research	O
center	O
,	O
university	O
of	O
texas	O
.	O
devroye	O
,	O
l.	O
and	O
wagner	O
,	O
t.	O
(	O
1979a	O
)	O
.	O
distribution-free	O
inequalities	O
for	O
the	O
deleted	B
and	O
hold	O
(	O
cid:173	O
)	O
out	O
error	O
estimates	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
25:202-207.	O
devroye	O
,	O
l.	O
and	O
wagner	O
,	O
t.	O
(	O
1979b	O
)	O
.	O
distribution-free	O
performance	O
bounds	O
for	O
potential	O
function	O
rules	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
25:601-604.	O
devroye	O
,	O
l.	O
and	O
wagner	O
,	O
t.	O
(	O
1979c	O
)	O
.	O
distribution-free	O
performance	O
bounds	O
with	O
the	O
resub	O
(	O
cid:173	O
)	O
stitution	O
error	O
estimate	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
25:208-210.	O
devroye	O
,	O
l.	O
and	O
wagner	O
,	O
t.	O
(	O
1980a	O
)	O
.	O
distribution-free	O
consistency	B
results	O
in	O
nonparametric	O
discrimination	O
and	O
regression	B
function	I
estimation	O
.	O
annals	O
of	O
statistics	O
,	O
8:231-239.	O
devroye	O
,	O
l.	O
and	O
wagner	O
,	O
t.	O
(	O
1980b	O
)	O
.	O
on	O
the	O
ll	O
convergence	O
ofkemel	O
estimators	O
of	O
reg	O
res	O
(	O
cid:173	O
)	O
sion	O
functions	O
with	O
applications	O
in	O
discrimination	O
.	O
zeitschriji	O
for	O
wahrscheinlichkeits	O
(	O
cid:173	O
)	O
theorie	O
und	O
verwandte	O
gebiete	O
,	O
51:15-21.	O
devroye	O
,	O
l.	O
and	O
wagner	O
,	O
t.	O
(	O
1982	O
)	O
.	O
nearest	B
neighbor	I
methods	O
in	O
discrimination	O
.	O
in	O
hand	O
(	O
cid:173	O
)	O
book	O
of	O
statistics	O
,	O
krishnaiah	O
,	O
p.	O
and	O
kanal	O
,	O
l.	O
,	O
editors	O
,	O
volume	O
2	O
,	O
pages	O
193-197.	O
north	O
holland	O
,	O
amsterdam	O
.	O
devroye	O
,	O
l.	O
and	O
wise	O
,	O
g.	O
(	O
1980	O
)	O
.	O
consistency	B
of	O
a	O
recursive	O
nearest	B
neighbor	I
regression	O
function	O
estimate	B
.	O
journal	O
of	O
multivariate	O
analysis	O
,	O
10:539-550.	O
diaconis	O
,	O
p.	O
and	O
shahshahani	O
,	O
m.	O
(	O
1984	O
)	O
.	O
on	O
nonlinear	O
functions	O
of	O
linear	O
combinations	O
.	O
siam	O
journal	O
on	O
scientific	O
and	O
statistical	O
computing	O
,	O
5	O
:	O
175-191.	O
do-tu	O
,	O
h.	O
and	O
installe	O
,	O
m.	O
(	O
1975	O
)	O
.	O
on	O
adaptive	O
solution	O
of	O
piecewise	O
linear	O
approxima	O
(	O
cid:173	O
)	O
tion	O
problem-application	O
to	O
modeling	O
and	O
identification	O
.	O
in	O
milwaukee	O
symposium	O
on	O
automatic	B
computation	O
and	O
control	O
,	O
pages	O
1-6.	O
milwaukee	O
,	O
wi	O
.	O
duda	O
,	O
r.	O
and	O
hart	O
,	O
p.	O
(	O
1973	O
)	O
.	O
pattern	O
classification	O
and	O
scene	O
analysis	O
.	O
john	O
wiley	O
,	O
new	O
york	O
.	O
dudani	O
,	O
s.	O
(	O
1976	O
)	O
.	O
the	O
distance-weighted	O
k-nearest-neighbor	O
rule	B
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
6:325-327.	O
dudley	O
,	O
r.	O
(	O
1978	O
)	O
.	O
central	O
limit	O
theorems	O
for	O
empirical	B
measures	O
.	O
annals	O
of	O
probability	O
,	O
6:899-929.	O
dudley	O
,	O
r	O
.	O
(	O
1979	O
)	O
.	O
balls	O
in	O
rk	O
do	O
not	O
cut	O
all	O
subsets	O
of	O
k+2	O
points	O
.	O
advances	O
in	O
mathematics	O
,	O
31	O
(	O
3	O
)	O
:306-308.	O
dudley	O
,	O
r.	O
(	O
1984	O
)	O
.	O
empirical	B
processes	O
.	O
in	O
ecole	O
de	O
probabilite	O
de	O
st.	O
flour	O
1982.	O
lecture	O
notes	O
in	O
mathematics	O
#	O
1097	O
,	O
springer-verlag	O
,	O
new	O
york	O
.	O
dudley	O
,	O
r.	O
(	O
1987	O
)	O
.	O
universal	B
donsker	O
classes	O
and	O
metric	B
entropy	I
.	O
annals	O
of	O
probability	O
,	O
15	O
:	O
1306-1326.	O
dudley	O
,	O
r.	O
,	O
kulkarni	O
,	O
s.	O
,	O
richardson	O
,	O
t.	O
,	O
and	O
zeitouni	O
,	O
o	O
.	O
(	O
1994	O
)	O
.	O
a	O
metric	O
entropy	B
bound	O
is	O
not	O
sufficient	O
for	O
learnability	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
40:883-885.	O
durrett	O
,	O
r.	O
(	O
1991	O
)	O
.	O
probability	O
:	O
theory	O
and	O
examples	O
.	O
wadsworth	O
and	O
brooks/cole	O
,	O
pacific	O
grove	O
,	O
ca	O
.	O
dvoretzky	O
,	O
a	O
.	O
(	O
1956	O
)	O
.	O
on	O
stochastic	B
approximation	I
.	O
in	O
proceedings	O
of	O
the	O
first	O
berkeley	O
symposium	O
on	O
mathematical	O
statistics	B
and	O
probability	O
,	O
neyman	O
,	O
j.	O
,	O
editor	O
,	O
pages	O
39-55.	O
university	O
of	O
california	O
press	O
,	O
berkeley	O
,	O
los	O
angeles	O
.	O
dvoretzky	O
,	O
a.	O
,	O
kiefer	O
,	O
j.	O
,	O
and	O
wolfowitz	O
,	O
j	O
.	O
(	O
1956	O
)	O
.	O
asymptotic	O
minimax	O
character	O
of	O
a	O
sample	O
distribution	B
function	I
and	O
of	O
the	O
classical	O
multinomial	B
estimator	O
.	O
annals	O
of	O
math	O
(	O
cid:173	O
)	O
ematical	O
statistics	B
,	O
33:642-669.	O
edelsbrunner	O
,	O
h.	O
(	O
1987	O
)	O
.	O
algorithmsfor	O
computational	O
geometry	O
.	O
springer-verlag	O
,	O
berlin	O
.	O
efron	O
,	O
b	O
.	O
(	O
1979	O
)	O
.	O
bootstrap	B
methods	O
:	O
another	O
look	O
at	O
the	O
jackknife	O
.	O
annals	O
of	O
statistics	O
,	O
7:1-26.	O
references	O
601	O
efron	O
,	O
b	O
.	O
(	O
1983	O
)	O
.	O
estimating	O
the	O
error	O
rate	O
of	O
a	O
prediction	O
rule	B
:	O
improvement	O
on	O
cross	O
validation	O
.	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
,	O
78	O
:	O
316-331.	O
efron	O
,	O
b.	O
and	O
stein	O
,	O
c.	O
(	O
1981	O
)	O
.	O
the	O
jackknife	O
estimate	B
of	O
variance	O
.	O
annals	O
of	O
statistics	O
,	O
9:586-596.	O
ehrenfeucht	O
,	O
a.	O
,	O
haussler	O
,	O
d.	O
,	O
kearns	O
,	O
m.	O
,	O
and	O
valiant	O
,	O
l.	O
(	O
1989	O
)	O
.	O
a	O
general	O
lower	O
bound	O
on	O
the	O
number	O
of	O
examples	O
needed	O
for	O
learning	B
.	O
information	O
and	O
computation	O
,	O
82:247-	O
261.	O
elashoff	O
,	O
j.	O
,	O
elashoff	O
,	O
r.	O
,	O
and	O
goldmann	O
,	O
g.	O
(	O
1967	O
)	O
.	O
on	O
the	O
choice	O
of	O
variables	O
in	O
classifi	O
(	O
cid:173	O
)	O
cation	O
problems	O
with	O
dichotomous	O
variables	O
.	O
biometrika	O
,	O
54:668-670.	O
fabian	O
,	O
v.	O
(	O
1971	O
)	O
.	O
stochastic	B
approximation	I
.	O
in	O
optimizing	O
methods	O
in	O
statistics	O
,	O
rustagi	O
,	O
j.	O
,	O
editor	O
,	O
pages	O
439-470.	O
academic	O
press	O
,	O
new	O
york	O
,	O
london	O
.	O
fano	O
,	O
r.	O
(	O
1952	O
)	O
.	O
class	O
notes	O
for	O
transmission	O
of	O
information	O
.	O
course	O
6.574.	O
mit	O
,	O
cam	O
(	O
cid:173	O
)	O
bridge	O
,	O
ma	O
.	O
farago	O
,	O
a.	O
,	O
linder	O
,	O
t.	O
,	O
and	O
lugosi	O
,	O
g.	O
(	O
1993	O
)	O
.	O
fast	O
nearest	B
neighbor	I
search	O
in	O
dissimilarity	O
spaces	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
15:957-962.	O
farago	O
,	O
a.	O
and	O
lugosi	O
,	O
g.	O
(	O
1993	O
)	O
.	O
strong	B
universal	I
consistency	I
of	O
neural	B
network	I
classifiers	I
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
39	O
:	O
1146-1151.	O
farago	O
,	O
t.	O
and	O
gyorfi	O
,	O
l.	O
(	O
1975	O
)	O
.	O
on	O
the	O
continuity	O
of	O
the	O
error	O
distortion	O
function	O
for	O
multiple	O
hypothesis	O
decisions	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
21	O
:	O
458-460.	O
feinholz	O
,	O
l.	O
(	O
1979	O
)	O
.	O
estimation	B
of	I
the	O
performance	O
of	O
partitioning	O
algorithms	O
in	O
pattern	O
classification	O
.	O
master	O
's	O
thesis	O
,	O
department	O
of	O
mathematics	O
,	O
mcgill	O
university	O
,	O
mon	O
(	O
cid:173	O
)	O
treal	O
.	O
feller	O
,	O
w.	O
(	O
1968	O
)	O
.	O
an	O
introduction	O
to	O
probability	O
theory	O
and	O
its	O
applications	O
,	O
you	O
.	O
john	O
wiley	O
,	O
new	O
york	O
.	O
finkel	O
,	O
r.	O
and	O
bentley	O
,	O
j	O
.	O
(	O
1974	O
)	O
.	O
quad	O
trees	O
:	O
a	O
data	O
structure	O
for	O
retrieval	O
on	O
composite	O
keys	O
.	O
acta	O
informatica	O
,	O
4	O
:	O
1-9.	O
fisher	O
,	O
r.	O
(	O
1936	O
)	O
.	O
the	O
case	O
of	O
multiple	O
measurements	O
in	O
taxonomic	O
problems	O
.	O
annals	O
of	O
eugenics	O
,	O
7	O
,	O
part	O
ii	O
:	O
179-188.	O
fitzmaurice	O
,	O
g.	O
and	O
hand	O
,	O
d.	O
(	O
1987	O
)	O
.	O
a	O
comparison	O
of	O
two	O
average	O
conditional	O
error	O
rate	O
estimators	O
.	O
pattern	O
recognition	O
letters	O
,	O
6:221-224.	O
fix	O
,	O
e.	O
and	O
hodges	O
,	O
j	O
.	O
(	O
1951	O
)	O
.	O
discriminatory	O
analysis	O
.	O
nonparametric	O
discrimination	O
:	O
consistency	B
properties	O
.	O
technical	O
report	O
4	O
,	O
project	O
number	O
21-49-004	O
,	O
usaf	O
school	O
of	O
aviation	O
medicine	O
,	O
randolph	O
field	O
,	O
tx	O
.	O
fix	O
,	O
e.	O
and	O
hodges	O
,	O
j	O
.	O
(	O
1952	O
)	O
.	O
discriminatory	O
analysis	O
:	O
small	O
sample	O
performance	O
.	O
tech	O
(	O
cid:173	O
)	O
nical	O
report	O
21-49-004	O
,	O
usaf	O
school	O
of	O
aviation	O
medicine	O
,	O
randolph	O
field	O
,	O
tx	O
.	O
fix	O
,	O
e.	O
and	O
hodges	O
,	O
j	O
.	O
(	O
1991	O
a	O
)	O
.	O
discriminatory	O
analysis	O
,	O
nonparametric	O
discrimination	O
,	O
con	O
(	O
cid:173	O
)	O
sistency	O
properties	O
.	O
in	O
nearest	O
neighbor	O
pattern	O
classification	O
techniques	O
,	O
dasarathy	O
,	O
b.	O
,	O
editor	O
,	O
pages	O
32-39.	O
ieee	O
computer	O
society	O
press	O
,	O
los	O
alamitos	O
,	O
ca	O
.	O
fix	O
,	O
e.	O
and	O
hodges	O
,	O
j	O
.	O
(	O
1991b	O
)	O
.	O
discriminatory	O
analysis	O
:	O
small	O
sample	O
performance	O
.	O
in	O
nearest	O
neighbor	O
pattern	O
classification	O
techniques	O
,	O
dasarathy	O
,	O
b.	O
,	O
editor	O
,	O
pages	O
40-	O
56.	O
ieee	O
computer	O
society	O
press	O
,	O
los	O
alamitos	O
,	O
ca	O
.	O
flick	O
,	O
t.	O
,	O
jones	O
,	O
l.	O
,	O
priest	O
,	O
r.	O
,	O
and	O
herman	O
,	O
c.	O
(	O
1990	O
)	O
.	O
pattern	O
classification	O
using	O
protection	O
pursuit	O
.	O
pattern	O
recognition	O
,	O
23	O
:	O
1367-1376.	O
forney	O
,	O
g.	O
(	O
1968	O
)	O
.	O
exponential	B
error	O
bounds	O
for	O
erasure	O
,	O
list	O
,	O
and	O
decision	O
feedback	O
schemes	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
14:41-46.	O
friedman	O
,	O
j	O
.	O
(	O
1977	O
)	O
.	O
a	O
recursive	O
partitioning	O
decision	O
rule	B
for	O
nonparametric	O
classification	O
.	O
ieee	O
transactions	O
on	O
computers	O
,	O
26:404-408	O
.	O
602	O
references	O
friedman	O
,	O
j.	O
,	O
baskett	O
,	O
f.	O
,	O
and	O
shustek	O
,	O
l.	O
(	O
1975	O
)	O
.	O
an	O
algorithm	B
for	O
finding	O
nearest	B
neighbor	I
.	O
ieee	O
transactions	O
on	O
computers	O
,	O
24	O
:	O
100~	O
1006.	O
friedman	O
,	O
j.	O
,	O
bentley	O
,	O
j.	O
,	O
and	O
finkel	O
,	O
r.	O
(	O
1977	O
)	O
.	O
an	O
algorithm	B
for	O
finding	O
best	O
matches	O
in	O
logarithmic	O
expected	O
time	O
.	O
acm	O
transactions	O
on	O
mathematical	O
software	O
,	O
3:209-226.	O
friedman	O
,	O
j.	O
and	O
silverman	O
,	O
b	O
.	O
(	O
1989	O
)	O
.	O
flexible	O
parsimonious	O
smoothing	O
and	O
additive	O
mod	O
(	O
cid:173	O
)	O
eling	O
.	O
technometrics	O
,	O
31:3-39.	O
friedman	O
,	O
j.	O
and	O
stuetzle	O
,	O
w.	O
(	O
1981	O
)	O
.	O
projection	B
pursuit	I
regression	O
.	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
,	O
76:817-823.	O
friedman	O
,	O
j.	O
,	O
stuetzle	O
,	O
w.	O
,	O
and	O
schroeder	O
,	O
a	O
.	O
(	O
1984	O
)	O
.	O
projection	B
pursuit	I
density	O
estimation	B
.	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
,	O
79:599-608.	O
friedman	O
,	O
1.	O
and	O
tukey	O
,	O
1	O
.	O
(	O
1974	O
)	O
.	O
a	O
projection	O
pursuit	O
algorithm	B
for	O
exploratory	O
data	O
analysis	O
.	O
ieee	O
transactions	O
on	O
computers	O
,	O
23:881-889.	O
fritz	O
,	O
1.	O
and	O
gyorfi	O
,	O
l.	O
(	O
1976	O
)	O
.	O
on	O
the	O
minimization	O
of	O
classification	O
error	O
probability	O
in	O
statistical	O
pattern	O
recognition	O
.	O
problems	O
of	O
control	O
and	O
information	O
theory	O
,	O
5:371-382.	O
fu	O
,	O
k.	O
,	O
min	O
,	O
p.	O
,	O
and	O
li	O
,	O
t.	O
(	O
1970	O
)	O
.	O
feature	O
selection	O
in	O
pattern	O
recognition	O
.	O
ieee	O
transactions	O
on	O
systems	O
science	O
and	O
cybernetics	O
,	O
6:33-39.	O
fuchs	O
,	O
h.	O
,	O
abram	O
,	O
g.	O
,	O
and	O
grant	O
,	O
e.	O
(	O
1983	O
)	O
.	O
near	O
real-time	O
shaded	O
display	O
of	O
rigid	O
objects	O
.	O
computer	O
graphics	O
,	O
17:65-72.	O
fuchs	O
,	O
h.	O
,	O
kedem	O
,	O
z.	O
,	O
and	O
naylor	O
,	O
b	O
.	O
(	O
1980	O
)	O
.	O
on	O
visible	O
surface	O
generation	O
by	O
a	O
priori	O
tree	B
structures	O
.	O
in	O
proceedings	O
siggraph	O
'	O
80	O
,	O
pages	O
124-133.	O
published	O
as	O
computer	O
graphics	O
,	O
volume	O
14.	O
fukunaga	O
,	O
k.	O
and	O
flick	O
,	O
t.	O
(	O
1984	O
)	O
.	O
an	O
optimal	O
global	O
nearest	B
neighbor	I
metric	O
.	O
ieee	O
trans	O
(	O
cid:173	O
)	O
actions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
6	O
:	O
314-318.	O
fukunaga	O
,	O
k.	O
and	O
hostetler	O
,	O
l.	O
(	O
1973	O
)	O
.	O
optimization	O
of	O
k-nearest	O
neighbor	O
density	O
estimates	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
19:32~326	O
.	O
fukunaga	O
,	O
k.	O
and	O
hummels	O
,	O
d.	O
(	O
1987	O
)	O
.	O
bayes	O
error	B
estimation	I
using	O
parzen	O
and	O
k-nn	O
procedures	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
9:634-	O
643.	O
fukunaga	O
,	O
k.	O
and	O
kessel	O
,	O
d.	O
(	O
1971	O
)	O
.	O
estimation	B
of	I
classification	O
error	O
.	O
ieee	O
transactions	O
on	O
computers	O
,	O
20:1521-1527.	O
fukunaga	O
,	O
k.	O
and	O
kessel	O
,	O
d.	O
(	O
1973	O
)	O
.	O
nonparametric	O
bayes	O
error	B
estimation	I
using	O
unclas	O
(	O
cid:173	O
)	O
sified	O
samples	O
.	O
ieee	O
transactions	O
information	O
theory	O
,	O
19:434-440.	O
fukunaga	O
,	O
k.	O
and	O
mantock	O
,	O
j	O
.	O
(	O
1984	O
)	O
.	O
nonparametric	O
data	O
reduction	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
6:115-118.	O
fukunaga	O
,	O
k.	O
and	O
narendra	O
,	O
p.	O
(	O
1975	O
)	O
.	O
a	O
branch	O
and	O
bound	O
algorithm	B
for	O
computing	O
k	O
(	O
cid:173	O
)	O
nearest	O
neighbors	O
.	O
ieee	O
transactions	O
on	O
computers	O
,	O
24:750-753.	O
funahashi	O
,	O
k.	O
(	O
1989	O
)	O
.	O
on	O
the	O
approximate	O
realization	O
of	O
continuous	O
mappings	O
by	O
neural	O
networks	O
.	O
neural	O
networks	O
,	O
2:183-192.	O
gabor	O
,	O
d.	O
(	O
1961	O
)	O
.	O
a	O
universal	O
nonlinear	O
filter	O
,	O
predictor	O
,	O
and	O
simulator	O
which	O
optimizes	O
itself	O
.	O
proceedings	O
of	O
the	O
institute	O
of	O
electrical	O
engineers	O
,	O
108b:422-438.	O
gabriel	O
,	O
k.	O
and	O
sokal	O
,	O
r.	O
(	O
1969	O
)	O
.	O
a	O
new	O
statistical	O
approach	O
to	O
geographic	O
variation	B
analysis	O
.	O
systematic	O
zoology	O
,	O
18:259-278.	O
gaenssler	O
,	O
p.	O
(	O
1983	O
)	O
.	O
empirical	B
processes	O
.	O
lecture	O
notes-monograph	O
series	O
,	O
institute	O
of	O
mathematical	O
statistics	B
,	O
hayward	O
,	O
ca	O
.	O
gaenssler	O
,	O
p.	O
and	O
stute	O
,	O
w.	O
(	O
1979	O
)	O
.	O
empirical	B
processes	O
:	O
a	O
survey	O
of	O
results	O
for	O
independent	O
identically	O
distributed	O
random	O
variables	O
.	O
annals	O
of	O
probability	O
,	O
7	O
:	O
193-243.	O
gallant	O
,	O
a	O
.	O
(	O
1987	O
)	O
.	O
nonlinear	O
statistical	O
models	O
.	O
john	O
wiley	O
,	O
new	O
york	O
.	O
references	O
603	O
ganesalingam	O
,	O
s.	O
and	O
mclachlan	O
,	O
g.	O
(	O
1980	O
)	O
.	O
error	O
rate	O
estimation	B
on	O
the	O
basis	O
of	O
posterior	O
probabilities	O
.	O
pattern	O
recognition	O
,	O
12:405-413.	O
garnett	O
,	O
j.	O
and	O
yau	O
,	O
s.	O
(	O
1977	O
)	O
.	O
nonparametric	O
estimation	B
of	I
the	O
bayes	O
error	O
offeature	O
extrac	O
(	O
cid:173	O
)	O
tors	O
using	O
ordered	B
nearest	O
neighbour	O
sets	O
.	O
ieee	O
transactions	O
on	O
computers	O
,	O
26:46-54.	O
gates	O
,	O
g.	O
(	O
1972	O
)	O
.	O
the	O
reduced	O
nearest	B
neighbor	I
rule	I
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
18:431-433.	O
gelfand	O
,	O
s.	O
and	O
delp	O
,	O
e.	O
(	O
1991	O
)	O
.	O
on	O
tree	B
structured	O
classifiers	O
.	O
in	O
artificial	O
neural	O
networks	O
and	O
statistical	O
pattern	O
recognition	O
,	O
old	O
and	O
new	O
connections	O
,	O
sethi	O
,	O
i.	O
and	O
jain	O
,	O
a.	O
,	O
editors	O
,	O
pages	O
71-88.	O
elsevier	O
science	O
publishers	O
,	O
amsterdam	O
.	O
gelfand	O
,	O
s.	O
,	O
ravishankar	O
,	O
c.	O
,	O
and	O
delp	O
,	O
e.	O
(	O
1989	O
)	O
.	O
an	O
iterative	O
growing	O
and	O
pruning	O
al	O
(	O
cid:173	O
)	O
gorithm	O
for	O
classification	O
tree	B
design	O
.	O
in	O
proceedings	O
of	O
the	O
1989	O
ieee	O
international	O
conference	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
pages	O
818-823.	O
ieee	O
press	O
,	O
piscataway	O
,	O
nj	O
.	O
gelfand	O
,	O
s.	O
,	O
ravishankar	O
,	O
c.	O
,	O
and	O
delp	O
,	O
e.	O
(	O
1991	O
)	O
.	O
an	O
iterative	O
growing	O
and	O
pruning	O
algo	O
(	O
cid:173	O
)	O
rithm	O
for	O
classification	O
tree	B
design	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
13:163-174.	O
geman	O
,	O
s.	O
and	O
hwang	O
,	O
c.	O
(	O
1982	O
)	O
.	O
nonparametric	O
maximum	B
likelihood	I
estimation	O
by	O
the	O
method	O
of	O
sieves	O
.	O
annals	O
of	O
statistics	O
,	O
10:401-414.	O
gessaman	O
,	O
m.	O
(	O
1970	O
)	O
.	O
a	O
consistent	O
nonparametric	O
multivariate	O
density	O
estimator	O
based	O
on	O
statistically	O
equivalent	O
blocks	O
.	O
annals	O
of	O
mathematical	O
statistics	B
,	O
41	O
:	O
1344-1346.	O
gessaman	O
,	O
m.	O
and	O
gessaman	O
,	O
p.	O
(	O
1972	O
)	O
.	O
a	O
comparison	O
of	O
some	O
multivariate	O
discrimination	O
procedures	O
.	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
,	O
67:468-472.	O
geva	O
,	O
s.	O
and	O
sitte	O
,	O
j	O
.	O
(	O
1991	O
)	O
.	O
adaptive	O
nearest	B
neighbor	I
pattern	O
classification	O
.	O
ieee	O
trans	O
(	O
cid:173	O
)	O
actions	O
on	O
neural	O
networks	O
,	O
2:318-322.	O
gine	O
,	O
e.	O
and	O
zinn	O
,	O
j	O
.	O
(	O
1984	O
)	O
.	O
some	O
limit	O
theorems	O
for	O
empirical	B
processes	O
.	O
annals	O
of	O
prob	O
(	O
cid:173	O
)	O
ability	O
,	O
12:929-989.	O
glick	O
,	O
n.	O
(	O
1973	O
)	O
.	O
sample-based	O
multinomial	B
classification	O
.	O
biometrics	O
,	O
29:241-256.	O
glick	O
,	O
n.	O
(	O
1976	O
)	O
.	O
sample-based	O
classification	O
procedures	O
related	O
to	O
empiric	O
distributions	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
22:454-461.	O
glick	O
,	O
n.	O
(	O
1978	O
)	O
.	O
additive	O
estimators	O
for	O
probabilities	O
of	O
correct	O
classification	O
.	O
pattern	O
recognition	O
,	O
10:211-222.	O
goldberg	O
,	O
p.	O
and	O
jerrum	O
,	O
m.	O
(	O
1993	O
)	O
.	O
bounding	O
the	O
vapnik-chervonenkis	O
dimension	B
of	O
concept	O
classes	O
parametrized	O
by	O
real	O
numbers	O
.	O
in	O
proceedings	O
of	O
the	O
sixth	O
acm	O
work	O
(	O
cid:173	O
)	O
shop	O
on	O
computational	O
learning	B
theory	O
,	O
pages	O
361-369.	O
association	B
for	O
computing	O
machinery	O
,	O
new	O
york	O
.	O
goldstein	O
,	O
m.	O
(	O
1977	O
)	O
.	O
a	O
two-group	O
classification	O
procedure	O
for	O
multivariate	O
dichotomous	O
responses	O
.	O
multivariate	O
behavorial	O
research	O
,	O
12:335-346.	O
goldstein	O
,	O
m.	O
and	O
dillon	O
,	O
w.	O
(	O
1978	O
)	O
.	O
discrete	O
discriminant	O
analysis	O
.	O
john	O
wiley	O
,	O
new	O
york	O
.	O
golea	O
,	O
m.	O
and	O
marchand	O
,	O
m	O
..	O
(	O
1990	O
)	O
.	O
a	O
growth	O
algorithm	B
for	O
neural	B
network	I
decision	O
trees	O
.	O
europhysics	O
letters	O
,	O
12:205-210.	O
goodman	O
,	O
r.	O
and	O
smyth	O
,	O
p.	O
(	O
1988	O
)	O
.	O
decision	O
tree	O
design	O
from	O
a	O
communication	O
theory	O
viewpoint	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
34:979-994.	O
gordon	O
,	O
l.	O
and	O
olshen	O
,	O
r.	O
(	O
1984	O
)	O
.	O
almost	O
surely	O
consistent	O
nonparametric	O
regression	O
from	O
recursive	B
partitioning	O
schemes	O
.	O
]	O
ournal	O
of	O
multivariate	O
analysis	O
,	O
15	O
:	O
147-163.	O
gordon	O
,	O
l.	O
and	O
olshen	O
,	O
r.	O
(	O
1978	O
)	O
.	O
asymptotically	O
efficient	O
solutions	O
to	O
the	O
classification	O
problem	O
.	O
annals	O
of	O
statistics	O
,	O
6:515-533	O
.	O
604	O
references	O
gordon	O
,	O
l.	O
and	O
olshen	O
,	O
r.	O
(	O
1980	O
)	O
.	O
consistent	O
nonparametric	O
regression	O
from	O
recursive	B
partitioning	O
schemes	O
.	O
journal	O
of	O
multivariate	O
analysis	O
,	O
10:611-627.	O
gowda	O
,	O
k.	O
and	O
krishna	O
,	O
g.	O
(	O
1979	O
)	O
.	O
the	O
condensed	B
nearest	O
neighbor	B
rule	I
using	O
the	O
concept	O
of	O
mutual	O
nearest	O
neighborhood	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
25:488-490.	O
greblicki	O
,	O
w.	O
(	O
1974	O
)	O
.	O
asymptotically	O
optimal	O
probabilistic	O
algorithms	O
for	O
pattern	O
recogni	O
(	O
cid:173	O
)	O
tion	O
and	O
identification	O
.	O
technical	O
report	O
,	O
monografie	O
no.3	O
,	O
prace	O
naukowe	O
instytutu	O
cybernetyki	O
technicznej	O
politechniki	O
wroclawsjiej	O
no	O
.	O
18	O
,	O
wroclaw	O
,	O
poland	O
.	O
greblicki	O
,	O
w.	O
(	O
1978a	O
)	O
.	O
asymptotically	O
optimal	O
pattern	O
recognition	O
procedures	O
with	O
density	O
estimates	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
24:250-251.	O
greblicki	O
,	O
w.	O
(	O
1978b	O
)	O
.	O
pattern	O
recognition	O
procedures	O
with	O
nonparametric	O
density	O
estimates	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
and	O
cybernetics	O
,	O
8:809-812.	O
greblicki	O
,	O
w.	O
(	O
1981	O
)	O
.	O
asymptotic	O
efficiency	O
of	O
classifying	O
procedures	O
using	O
the	O
hermite	O
series	O
estimate	B
of	O
multivariate	O
probability	O
densities	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
27:364-366.	O
greblicki	O
,	O
w.	O
,	O
krzyzak	O
,	O
a.	O
,	O
and	O
pawlak	O
,	O
m.	O
(	O
1984	O
)	O
.	O
distribution-free	O
pointwise	O
consistency	B
of	O
kernel	B
regression	O
estimate	B
.	O
annals	O
of	O
statistics	O
,	O
12	O
:	O
1570-1575.	O
greblicki	O
,	O
w.	O
and	O
pawlak	O
,	O
m.	O
(	O
1981	O
)	O
.	O
classification	O
using	O
the	O
fourier	O
series	O
estimate	B
of	O
multivariate	O
density	O
functions	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
and	O
cybernetics	O
,	O
11:726-730.	O
greblicki	O
,	O
w.	O
and	O
pawlak	O
,	O
m.	O
(	O
1982	O
)	O
.	O
a	O
classification	O
procedure	O
using	O
the	O
multiple	O
fourier	O
series	O
.	O
information	O
sciences	O
,	O
26:115-126.	O
greblicki	O
,	O
w.	O
and	O
pawlak	O
,	O
m.	O
(	O
1983	O
)	O
.	O
almost	O
sure	O
convergence	O
of	O
classification	O
procedures	O
using	O
hermite	O
series	O
density	O
estimates	O
.	O
pattern	O
recognition	O
letters	O
,	O
2	O
:	O
13-17.	O
greblicki	O
,	O
w.	O
and	O
pawlak	O
,	O
m.	O
(	O
1985	O
)	O
.	O
pointwise	O
consistency	B
of	O
the	O
hermite	O
series	O
density	O
estimate	O
.	O
statistics	B
and	O
probability	O
letters	O
,	O
3:65-69.	O
greblicki	O
,	O
w.	O
and	O
pawlak	O
,	O
m.	O
(	O
1987	O
)	O
.	O
necessary	O
and	O
sufficient	O
conditions	O
for	O
bayes	O
risk	O
consistency	B
of	O
a	O
recursive	O
kernel	B
classification	O
rule	B
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
33:408-412.	O
grenander	O
,	O
u	O
.	O
(	O
1981	O
)	O
.	O
abstract	O
inference	O
.	O
john	O
wiley	O
,	O
new	O
york	O
.	O
grimmett	O
,	O
g.	O
and	O
stirzaker	O
,	O
d.	O
(	O
1992	O
)	O
.	O
probability	O
and	O
random	O
processes	O
.	O
oxford	O
univer	O
(	O
cid:173	O
)	O
sity	O
press	O
,	O
oxford	O
.	O
guo	O
,	O
h.	O
and	O
gelfand	O
,	O
s.	O
(	O
1992	O
)	O
.	O
classification	O
trees	O
with	O
neural	O
network	O
feature	B
extraction	I
.	O
ieee	O
transactions	O
on	O
neural	O
networks	O
,	O
3:923-933.	O
gyorfi	O
,	O
l.	O
(	O
1975	O
)	O
.	O
an	O
upper	O
bound	O
of	O
error	O
probabilities	O
for	O
multihypothesis	O
testing	O
and	O
its	O
application	O
in	O
adaptive	O
pattern	O
recognition	O
.	O
problems	O
of	O
control	O
and	O
information	O
theory	O
,	O
5:449-457.	O
gyorfi	O
,	O
l.	O
(	O
1978	O
)	O
.	O
on	O
the	O
rate	B
of	I
convergence	I
of	O
nearest	B
neighbor	I
rules	I
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
29:509-512.	O
gyorfi	O
,	O
l.	O
(	O
1981	O
)	O
.	O
recent	O
results	O
on	O
nonparametric	O
regression	O
estimate	O
and	O
multiple	O
clas	O
(	O
cid:173	O
)	O
sification	O
.	O
problems	O
of	O
control	O
and	O
information	O
theory	O
,	O
10:43-52.	O
gyorfi	O
,	O
l.	O
(	O
1984	O
)	O
.	O
adaptive	O
linear	O
procedures	O
under	O
general	O
conditions	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
30:262-267.	O
gyorfi	O
,	O
l.	O
and	O
gyorfi	O
,	O
z	O
.	O
(	O
1975	O
)	O
.	O
on	O
the	O
nonparametric	O
estimate	B
of	O
a	O
posteriori	O
probabilities	O
of	O
simple	O
statistical	O
hypotheses	O
.	O
in	O
colloquia	O
mathematica	O
societatis	O
janos	O
bolyai	O
:	O
topics	O
in	O
information	O
theory	O
,	O
pages	O
299-308.	O
keszthely	O
,	O
hungary	O
.	O
gyorfi	O
,	O
l.	O
and	O
gyorfi	O
,	O
z	O
.	O
(	O
1978	O
)	O
.	O
an	O
upper	O
bound	O
on	O
the	O
asymptotic	O
error	O
probability	O
of	O
the	O
k-nearest	O
neighbor	B
rule	I
for	O
multiple	O
classes	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
24:512-514.	O
references	O
605	O
gyorfi	O
,	O
l.	O
,	O
gyorfi	O
,	O
z.	O
,	O
and	O
vajda	O
,	O
i	O
.	O
(	O
1978	O
)	O
.	O
bayesian	O
decision	B
with	I
rejection	I
.	O
problems	O
of	O
control	O
and	O
information	O
theory	O
,	O
8:445-452.	O
gyorfi	O
,	O
l.	O
and	O
vajda	O
,	O
i	O
.	O
(	O
1980	O
)	O
.	O
upper	O
bound	O
on	O
the	O
error	O
probability	O
of	O
detection	O
in	O
non	O
(	O
cid:173	O
)	O
gaussian	B
noise	I
.	O
problems	O
of	O
control	O
and	O
information	O
theory	O
,	O
9:215-224.	O
haagerup	O
,	O
u	O
.	O
(	O
1978	O
)	O
.	O
les	O
meilleures	O
constantes	O
de	O
l'inegalite	O
de	O
khintchine	O
.	O
comptes	O
rendus	O
de	O
[	O
'	O
academie	O
des	O
sciences	O
de	O
paris	O
a	O
,	O
286:259-262.	O
habbema	O
,	O
j.	O
,	O
hermans	O
,	O
j.	O
,	O
and	O
van	O
den	O
broek	O
,	O
k.	O
(	O
1974	O
)	O
.	O
a	O
stepwise	O
discriminant	O
analysis	O
program	O
using	O
density	B
estimation	I
.	O
in	O
compstat	O
1974	O
,	O
bruckmann	O
,	O
g.	O
,	O
editor	O
,	O
pages	O
101-110.	O
physica	O
verlag	O
,	O
wien	O
.	O
hagerup	O
,	O
t.	O
and	O
riib	O
,	O
c.	O
(	O
1990	O
)	O
.	O
a	O
guided	O
tour	O
of	O
chernoff	O
bounds	O
.	O
information	O
processing	O
letters	O
,	O
33:305-308.	O
hall	O
,	O
p.	O
(	O
1981	O
)	O
.	O
on	O
nonparametric	O
multivariate	O
binary	O
discrimination	O
.	O
biometrika	O
,	O
68:287-	O
.294.	O
hall	O
,	O
p.	O
(	O
1989	O
)	O
.	O
on	O
projection	B
pursuit	I
regression	O
.	O
annals	O
of	O
statistics	O
,	O
17:573-588.	O
hall	O
,	O
p.	O
and	O
wand	O
,	O
m.	O
(	O
1988	O
)	O
.	O
on	O
nonparametric	O
discrimination	O
using	O
density	O
differences	O
.	O
biometrika	O
,	O
75:541-547.	O
hand	O
,	O
d.	O
(	O
1981	O
)	O
.	O
discrimination	O
and	O
classification	O
.	O
john	O
wiley	O
,	O
chichester	O
,	O
u.k.	O
hand	O
,	O
d.	O
(	O
1986	O
)	O
.	O
recent	O
advances	O
in	O
error	O
rate	O
estimation	O
.	O
pattern	O
recognition	O
letters	O
,	O
4:335-346.	O
hardie	O
,	O
w.	O
and	O
marron	O
,	O
j	O
.	O
(	O
1985	O
)	O
.	O
optimal	O
bandwidth	B
selection	O
in	O
nonparametric	O
regression	B
function	I
estimation	O
.	O
annals	O
of	O
statistics	O
,	O
13	O
:	O
1465-1481.	O
hart	O
,	O
p.	O
(	O
1968	O
)	O
.	O
the	O
condensed	B
nearest	O
neighbor	B
rule	I
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
14:515-516.	O
hartigan	O
,	O
j	O
.	O
(	O
1975	O
)	O
.	O
clustering	B
algorithms	O
.	O
john	O
wiley	O
,	O
new	O
york	O
.	O
hartmann	O
,	O
c.	O
,	O
varshney	O
,	O
p.	O
,	O
mehrotra	O
,	O
k.	O
,	O
and	O
gerberich	O
,	O
c.	O
(	O
1982	O
)	O
.	O
application	O
of	O
in	O
(	O
cid:173	O
)	O
formation	O
theory	O
to	O
the	O
construction	O
of	O
efficient	O
decision	O
trees	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
28:565-577.	O
hashlamoun	O
,	O
w.	O
,	O
varshney	O
,	O
p.	O
,	O
and	O
samarasooriya	O
,	O
v.	O
(	O
1994	O
)	O
.	O
a	O
tight	O
upper	O
bound	O
on	O
the	O
bayesian	O
probability	O
of	O
error	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
16:220-224.	O
hastie	O
,	O
t.	O
and	O
tibshirani	O
,	O
r.	O
(	O
1990	O
)	O
.	O
generalized	B
additive	O
models	O
.	O
chapman	O
and	O
hall	O
,	O
london	O
,	O
u.k.	O
haussler	O
,	O
d.	O
(	O
1991	O
)	O
.	O
sphere	B
packing	O
numbers	O
for	O
subsets	O
of	O
the	O
boolean	O
n-cube	O
with	O
bounded	O
v	O
apnik	O
-chervonenkis	O
dimension	B
.	O
technical	O
report	O
,	O
computer	O
research	O
lab	O
(	O
cid:173	O
)	O
oratory	O
,	O
university	O
of	O
california	O
,	O
santa	O
cruz	O
.	O
haussler	O
,	O
d.	O
(	O
1992	O
)	O
.	O
decision	O
theoretic	O
generalizations	O
of	O
the	O
pac	O
model	O
for	O
neural	O
net	O
and	O
other	O
learning	B
applications	O
.	O
information	O
and	O
computation	O
,	O
100:78-150.	O
haussler	O
,	O
d.	O
,	O
littlestone	O
,	O
n.	O
,	O
and	O
warmuth	O
,	O
m.	O
(	O
1988	O
)	O
.	O
predicting	O
{	O
o	O
,	O
1	O
}	O
functions	O
from	O
randomly	O
drawn	O
points	O
.	O
in	O
proceedings	O
of	O
the	O
29th	O
ieee	O
symposium	O
on	O
the	O
foundations	O
of	O
computer	O
science	O
,	O
pages	O
100-109.	O
ieee	O
computer	O
society	O
press	O
,	O
los	O
alamitos	O
,	O
ca	O
.	O
hecht-nielsen	O
,	O
r.	O
(	O
1987	O
)	O
.	O
kolmogorov	O
's	O
mapping	O
network	O
existence	O
theorem	B
.	O
in	O
ieee	O
first	O
international	O
conference	O
on	O
neural	O
networks	O
,	O
volume	O
3	O
,	O
pages	O
11-l3	O
.	O
ieee	O
,	O
piscataway	O
,	O
nj	O
.	O
hellman	O
,	O
m.	O
(	O
1970	O
)	O
.	O
the	O
nearest	B
neighbor	I
classification	O
rule	B
with	O
a	O
reject	O
option	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
and	O
cybernetics	O
,	O
2	O
:	O
179-185.	O
henrichon	O
,	O
e.	O
and	O
fu	O
,	O
k.	O
(	O
1969	O
)	O
.	O
a	O
nonparametric	O
partitioning	O
procedure	O
for	O
pattern	O
clas	O
(	O
cid:173	O
)	O
sification.ieee	O
transactions	O
on	O
computers	O
,	O
18:614	O
--	O
624	O
.	O
606	O
references	O
hertz	O
,	O
j.	O
,	O
krogh	O
,	O
a.	O
,	O
and	O
palmer	O
,	O
r.	O
(	O
1991	O
)	O
.	O
introduction	O
to	O
the	O
theory	O
of	O
neural	O
compu	O
(	O
cid:173	O
)	O
tation	O
.	O
addison-wesley	O
,	O
redwood	O
city	O
,	O
ca	O
.	O
hills	O
,	O
m.	O
(	O
1966	O
)	O
.	O
allocation	O
rules	O
and	O
their	O
error	O
rates	O
.	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
,	O
b28:1-31.	O
hjort	O
,	O
n.	O
(	O
1986a	O
)	O
.	O
contribution	O
to	O
the	O
discussion	O
of	O
a	O
paper	O
by	O
p.	O
diaconis	O
and	O
freeman	O
.	O
annals	O
of	O
statistics	O
,	O
14:49-55.	O
hjort	O
,	O
n.	O
(	O
1986b	O
)	O
.	O
notes	O
on	O
the	O
theory	O
of	O
statistical	O
symbol	O
recognition	O
.	O
technical	O
report	O
778	O
,	O
norwegian	O
computing	O
centre	O
,	O
oslo	O
.	O
hoeffding	O
,	O
w.	O
(	O
1963	O
)	O
.	O
probability	O
inequalities	O
for	O
sums	O
of	O
bounded	O
random	O
variables	O
.	O
jour	O
(	O
cid:173	O
)	O
nal	O
of	O
the	O
american	O
statistical	O
association	B
,	O
58:13-30.	O
holmstrom	O
,	O
l.	O
and	O
klemeui	O
,	O
j	O
.	O
(	O
1992	O
)	O
.	O
asymptotic	O
bounds	O
for	O
the	O
expected	O
11	O
error	O
of	O
a	O
multivariate	O
kernel	O
density	O
estimator	O
.	O
journal	O
of	O
multivariate	O
analysis	O
,	O
40:245-255.	O
hora	O
,	O
s.	O
and	O
wilcox	O
,	O
j	O
.	O
(	O
1982	O
)	O
.	O
estimation	B
of	I
error	O
rates	O
in	O
several-population	O
discriminant	O
analysis	O
.	O
journal	O
of	O
marketing	O
research	O
,	O
19:57-61.	O
horibe	O
,	O
y	O
.	O
(	O
1970	O
)	O
.	O
on	O
zero	O
error	O
probability	O
of	O
binary	O
decisions	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
16:347-348.	O
home	O
,	O
b.	O
and	O
hush	O
,	O
d.	O
(	O
1990	O
)	O
.	O
on	O
the	O
optimality	O
of	O
the	O
sigmoid	B
perceptron	O
.	O
in	O
international	O
joint	O
conference	O
on	O
neural	O
networks	O
,	O
volume	O
1	O
,	O
pages	O
269-272.	O
lawrence	O
erlbaum	O
associates	O
,	O
hillsdale	O
,	O
nj	O
.	O
hornik	O
,	O
k.	O
(	O
1991	O
)	O
.	O
approximation	O
capabilities	O
of	O
multilayer	O
feedforward	O
networks	O
.	O
neural	O
networks	O
,	O
4:251-257.	O
hornik	O
,	O
k.	O
(	O
1993	O
)	O
.	O
some	O
new	O
results	O
on	O
neural	B
network	I
approximation	O
.	O
neural	O
networks	O
,	O
6:1069-1072.	O
hornik	O
,	O
k.	O
,	O
stinchcombe	O
,	O
m.	O
,	O
and	O
white	O
,	O
h.	O
(	O
1989	O
)	O
.	O
multi-layer	O
feedforward	O
networks	O
are	O
universal	B
approximators	O
.	O
neural	O
networks	O
,	O
2:359-366.	O
huber	O
,	O
p.	O
(	O
1985	O
)	O
.	O
projection	B
pursuit	I
.	O
annals	O
of	O
statistics	O
,	O
13:435-525.	O
hudimoto	O
,	O
h.	O
(	O
1957	O
)	O
.	O
a	O
note	O
on	O
the	O
probability	O
of	O
the	O
correct	O
classification	O
when	O
the	O
distributions	O
are	O
not	O
specified	O
.	O
annals	O
of	O
the	O
institute	O
of	O
statistical	O
mathematics	O
,	O
9:31-	O
36.	O
ito	O
,	O
t.	O
(	O
1969	O
)	O
.	O
note	O
on	O
a	O
class	O
of	O
statistical	O
recognition	O
functions	O
.	O
ieee	O
transactions	O
on	O
computers	O
,	O
c-18:76-79.	O
ito	O
,	O
t.	O
(	O
1972	O
)	O
.	O
approximate	O
error	O
bounds	O
in	O
pattern	O
recognition	O
.	O
in	O
machine	O
intelligence	O
,	O
meltzer	O
,	O
b.	O
and	O
mitchie	O
,	O
d.	O
,	O
editors	O
,	O
pages	O
369-376.	O
edinburgh	O
university	O
press	O
,	O
edin	O
(	O
cid:173	O
)	O
burgh	O
.	O
ivakhnenko	O
,	O
a	O
.	O
(	O
1968	O
)	O
.	O
the	O
group	O
method	O
of	O
data	O
handling-a	O
rival	O
of	O
the	O
method	O
of	O
stochastic	O
approximation	O
.	O
soviet	O
automatic	B
control	O
,	O
1:43-55.	O
ivakhnenko	O
,	O
a	O
.	O
(	O
1971	O
)	O
.	O
polynomial	B
theory	O
of	O
complex	O
systems	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
1:364-378.	O
ivakhnenko	O
,	O
a.	O
,	O
konovalenko	O
,	O
v.	O
,	O
tulupchuk	O
,	O
y.	O
,	O
and	O
tymchenko	O
,	O
i	O
.	O
(	O
1968	O
)	O
.	O
the	O
group	B
method	I
of	I
data	I
handling	I
in	O
pattern	O
recognition	O
and	O
decision	O
problems	O
.	O
soviet	O
automatic	B
control	O
,	O
1:31-41.	O
ivakhnenko	O
,	O
a.	O
,	O
petrache	O
,	O
g.	O
,	O
and	O
krasyts'kyy	O
,	O
m.	O
(	O
1968	O
)	O
.	O
a	O
gmdh	O
algorithm	B
with	O
random	O
selection	B
of	O
pairs	O
.	O
soviet	O
automatic	B
control	O
,	O
5:23-30.	O
jain	O
,	O
a.	O
,	O
dubes	O
,	O
r.	O
,	O
and	O
chen	O
,	O
c.	O
(	O
1987	O
)	O
.	O
bootstrap	B
techniques	O
for	O
error	B
estimation	I
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
9:628-633.	O
jeffreys	O
,	O
h.	O
(	O
1948	O
)	O
.	O
theory	O
of	O
probability	O
.	O
clarendon	O
press	O
,	O
oxford	O
.	O
johnson	O
,	O
d.	O
and	O
preparata	O
,	O
f.	O
(	O
1978	O
)	O
.	O
the	O
densest	O
hemisphere	O
problem	O
.	O
theoretical	B
com	O
(	O
cid:173	O
)	O
puter	O
science	O
,	O
6:93-107.	O
references	O
607	O
kurkova	O
,	O
v.	O
(	O
1992	O
)	O
.	O
kolmogorov	O
's	O
theorem	B
and	O
multilayer	B
neural	O
networks	O
.	O
neural	O
net	O
(	O
cid:173	O
)	O
works	O
,	O
5:501-506.	O
kailath	O
,	O
t.	O
(	O
1967	O
)	O
.	O
the	O
divergence	B
and	O
bhattacharyya	O
distance	B
measures	O
in	O
signal	O
detection	O
.	O
ieee	O
transactions	O
on	O
communication	O
technology	O
,	O
15:52-60.	O
kanal	O
,	O
l.	O
(	O
1974	O
)	O
.	O
patterns	O
in	O
pattern	O
recognition	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
20:697-722.	O
kaplan	O
,	O
m.	O
(	O
1985	O
)	O
.	O
the	O
uses	O
of	O
spatial	O
coherence	O
in	O
ray	O
tracing	O
.	O
siggraph	O
'85	O
course	O
notes	O
,	O
11:22-26.	O
karlin	O
,	O
a	O
.	O
(	O
1968	O
)	O
.	O
total	B
positivity	I
,	O
volume	O
1.	O
stanford	O
university	O
press	O
,	O
stanford	O
,	O
ca	O
.	O
karp	O
,	O
r.	O
(	O
1988	O
)	O
.	O
probabilistic	O
analysis	O
of	O
algorithms	O
.	O
class	O
notes	O
,	O
university	O
of	O
california	O
,	O
berkeley	O
.	O
karpinski	O
,	O
m.	O
and	O
macintyre	O
,	O
a	O
.	O
(	O
1994	O
)	O
.	O
quadratic	O
bounds	O
for	O
vc	B
dimension	I
of	O
sigmoidal	O
neural	O
networks	O
.	O
submitted	O
to	O
the	O
acm	O
symposium	O
on	O
theory	O
of	O
computing	O
.	O
kazmierczak	O
,	O
h.	O
and	O
steinbuch	O
,	O
k.	O
(	O
1963	O
)	O
.	O
adaptive	O
systems	O
in	O
pattern	O
recognition	O
.	O
ieee	O
transactions	O
on	O
electronic	O
computers	O
,	O
12:822-835.	O
kemp	O
,	O
r.	O
(	O
1984	O
)	O
.	O
fundamentals	O
of	O
the	O
average	O
case	O
analysis	O
of	O
particular	O
algorithms	O
.	O
b.g	O
.	O
teubner	O
,	O
stuttgart	O
.	O
kemperman	O
,	O
j	O
.	O
(	O
1969	O
)	O
.	O
on	O
the	O
optimum	O
rate	O
of	O
transmitting	O
information	O
.	O
in	O
probability	O
and	O
information	O
theory	O
,	O
pages	O
126-169.	O
springer	O
lecture	O
notes	O
in	O
mathematics	O
,	O
springer	O
(	O
cid:173	O
)	O
verlag	O
,	O
berlin	O
.	O
kiefer	O
,	O
j.	O
and	O
wolfowitz	O
,	O
j	O
.	O
(	O
1952	O
)	O
.	O
stochastic	O
estimation	O
of	O
the	O
maximum	O
of	O
a	O
regression	O
function	O
.	O
annals	O
of	O
mathematical	O
statistics	B
,	O
23:462-466.	O
kim	O
,	O
b.	O
and	O
park	O
,	O
s.	O
(	O
1986	O
)	O
.	O
a	O
fast	O
k-nearest	O
neighbor	O
finding	O
algorithm	B
based	O
on	O
the	O
ordered	B
partition	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
8:761-766.	O
kittler	O
,	O
j.	O
and	O
devijver	O
,	O
p.	O
(	O
1981	O
)	O
.	O
an	O
efficient	O
estimator	O
of	O
pattern	O
recognition	O
system	O
error	O
probability	O
.	O
pattern	O
recognition	O
,	O
13:245-249.	O
knoke	O
,	O
j	O
.	O
(	O
1986	O
)	O
.	O
the	O
robust	O
estimation	B
of	I
classification	O
error	O
rates	O
.	O
computers	O
and	O
math	O
(	O
cid:173	O
)	O
ematics	O
with	O
applications	O
,	O
12a:253-260.	O
kohonen	O
,	O
t.	O
(	O
1988	O
)	O
.	O
self-organization	O
and	O
associative	O
memory	O
.	O
springer-verlag	O
,	O
berlin	O
.	O
kohonen	O
,	O
t.	O
(	O
1990	O
)	O
.	O
statistical	O
pattern	O
recognition	O
revisited	O
.	O
in	O
advanced	O
neural	O
comput	O
(	O
cid:173	O
)	O
ers	O
,	O
eckmiller	O
,	O
r.	O
,	O
editor	O
,	O
pages	O
137-144.	O
north-holland	O
,	O
amsterdam	O
.	O
kolmogorov	O
,	O
a	O
.	O
(	O
1957	O
)	O
.	O
on	O
the	O
representation	O
of	O
continuous	O
functions	O
of	O
many	O
variables	O
by	O
superposition	O
of	O
continuous	O
functions	O
of	O
one	O
variable	B
and	O
addition	O
.	O
doklady	O
akademii	O
nauk	O
ussr	O
,	O
114:953-956.	O
kolmogorov	O
,	O
a.	O
and	O
tikhomirov	O
,	O
v.	O
(	O
1961	O
)	O
.	O
e-entropy	O
and	O
e-capacity	O
of	O
sets	O
in	O
function	O
spaces	O
.	O
translations	O
of	O
the	O
american	O
mathematical	O
society	O
,	O
17:277-364.	O
koutsougeras	O
,	O
c.	O
and	O
papachristou	O
,	O
c.	O
(	O
1989	O
)	O
.	O
training	O
of	O
a	O
neural	O
network	O
for	O
pattern	O
classification	O
based	O
on	O
an	O
entropy	B
measure	O
.	O
in	O
proceedings	O
of	O
ieee	O
international	O
con	O
(	O
cid:173	O
)	O
ference	O
on	O
neural	O
networks	O
,	O
volume	O
1	O
,	O
pages	O
247-254.	O
ieee	O
san	O
diego	O
section	O
,	O
san	O
diego	O
,	O
ca	O
.	O
kraaijveld	O
,	O
m.	O
and	O
duin	O
,	O
r.	O
(	O
1991	O
)	O
.	O
generalization	O
capabilities	O
of	O
minimal	O
kernel-based	O
methods	O
.	O
in	O
international	O
joint	O
conference	O
on	O
neural	O
networks	O
,	O
volume	O
1	O
,	O
pages	O
843-	O
848.	O
piscataway	O
,	O
nj	O
.	O
kronmal	O
,	O
r.	O
and	O
tarter	O
,	O
m.	O
(	O
1968	O
)	O
.	O
the	O
estimation	B
of	I
probability	O
densities	O
and	O
cumulatives	O
by	O
fourier	O
series	O
methods	O
.	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
,	O
63:925-952.	O
krzanowski	O
,	O
w.	O
(	O
1987	O
)	O
.	O
a	O
comparison	O
between	O
two	O
distance-based	O
discriminant	O
principles	O
.	O
journal	O
of	O
classification	O
,	O
4:73-84	O
.	O
608	O
references	O
krzyzak	O
,	O
a	O
.	O
(	O
1983	O
)	O
.	O
classification	O
procedures	O
using	O
multivariate	O
variable	O
kernel	O
density	O
estimate	O
.	O
pattern	O
recognition	O
letters	O
,	O
1:293-298.	O
krzyzak	O
,	O
a	O
.	O
(	O
1986	O
)	O
.	O
the	O
rates	O
of	O
convergence	O
of	O
kernel	O
regression	O
estimates	O
and	O
classifica	O
(	O
cid:173	O
)	O
tion	O
rules	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
32:668-679.	O
krzyzak	O
,	O
a	O
.	O
(	O
1991	O
)	O
.	O
on	O
exponential	B
bounds	O
on	O
the	O
bayes	O
risk	O
of	O
the	O
kernel	B
classification	O
rule	B
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
37:490-499.	O
krzyzak	O
,	O
a.	O
,	O
linder	O
,	O
t.	O
,	O
and	O
lugosi	O
,	O
g.	O
(	O
1993	O
)	O
.	O
nonparametric	O
estimation	B
and	O
classification	O
using	O
radial	B
basis	I
function	I
nets	O
and	O
empirical	B
risk	I
minimization	I
.	O
ieee	O
transactions	O
on	O
neural	O
networks	O
.	O
to	O
appear	O
.	O
krzyzak	O
,	O
a.	O
and	O
pawlak	O
,	O
m.	O
(	O
1983	O
)	O
.	O
universal	B
consistency	I
results	O
for	O
wolverton-wagner	O
regression	B
function	I
estimate	O
with	O
application	O
in	O
discrimination	O
.	O
problems	O
of	O
control	O
and	O
information	O
theory	O
,	O
12:33-42.	O
krzyzak	O
,	O
a.	O
and	O
pawlak	O
,	O
m.	O
(	O
1984a	O
)	O
.	O
almost	O
everywhere	O
convergence	O
of	O
recursive	O
kernel	B
regression	O
function	O
estimates	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
31:91-93.	O
krzyzak	O
,	O
a.	O
and	O
pawlak	O
,	O
m.	O
(	O
1984b	O
)	O
.	O
distribution-free	O
consistency	B
of	O
a	O
nonparametric	O
kernel	B
regression	O
estimate	B
and	O
classification	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
30:78-81.	O
kulkarni	O
,	O
s.	O
(	O
1991	O
)	O
.	O
problems	O
of	O
computational	O
and	O
information	O
complexity	O
in	O
machine	O
vision	O
and	O
learning	B
.	O
phd	O
thesis	O
,	O
department	O
of	O
electrical	O
engineering	O
and	O
computer	O
science	O
,	O
mit	O
,	O
cambridge	O
,	O
ma	O
.	O
kullback	O
,	O
s.	O
(	O
1967	O
)	O
.	O
a	O
lower	O
bound	O
for	O
discrimination	O
information	O
in	O
terms	O
of	O
variation	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
13:126-127.	O
kullback	O
,	O
s.	O
and	O
leibler	O
,	O
a	O
.	O
(	O
1951	O
)	O
.	O
on	O
information	O
and	O
sufficiency	O
.	O
annals	O
of	O
mathematical	O
statistics	B
,	O
22:79-86.	O
kushner	O
,	O
h.	O
(	O
1984	O
)	O
.	O
approximation	O
and	O
weak	B
convergence	O
methods	O
for	O
random	O
processes	O
with	O
applications	O
to	O
stochastic	O
systems	O
theory	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
lachenbruch	O
,	O
p.	O
(	O
1967	O
)	O
.	O
an	O
almost	O
unbiased	O
method	O
of	O
obtaining	O
confidence	B
intervals	O
for	O
the	O
probability	O
of	O
misclassification	O
in	O
discriminant	O
analysis	O
.	O
biometrics	O
,	O
23:639-645.	O
lachenbruch	O
,	O
p.	O
and	O
mickey	O
,	O
m.	O
(	O
1968	O
)	O
.	O
estimation	B
of	I
error	O
rates	O
in	O
discriminant	O
analysis	O
.	O
technometrics	O
,	O
10	O
:	O
1-11.	O
lecam	O
,	O
l.	O
(	O
1970	O
)	O
.	O
on	O
the	O
assumptions	O
used	O
to	O
prove	O
asymptotic	O
normality	O
of	O
maximum	O
likelihood	O
estimates	O
.	O
annals	O
of	O
mathematical	O
statistics	B
,	O
41:802-828.	O
lecam	O
,	O
l.	O
(	O
1973	O
)	O
.	O
convergence	O
of	O
estimates	O
under	O
dimensionality	O
restrictions	O
.	O
annals	O
of	O
statistics	O
,	O
1	O
:38-53.	O
li	O
,	O
x.	O
and	O
dubes	O
,	O
r.	O
(	O
1986	O
)	O
.	O
tree	B
classifier	O
design	O
with	O
a	O
permutation	O
statistic	O
.	O
pattern	O
recognition	O
,	O
19:229-235.	O
lin	O
,	O
y.	O
and	O
fu	O
,	O
k.	O
(	O
1983	O
)	O
.	O
automatic	B
classification	O
of	O
cervical	O
cells	O
using	O
a	O
binary	O
tree	B
classifier	O
.	O
pattern	O
recognition	O
,	O
16:69-80.	O
linde	O
,	O
y.	O
,	O
buzo	O
,	O
a.	O
,	O
and	O
gray	O
,	O
r.	O
(	O
1980	O
)	O
.	O
an	O
algorithm	B
for	O
vector	O
quantizer	O
design	O
.	O
ieee	O
transactions	O
on	O
communications	O
,	O
28:84-95.	O
linder	O
,	O
t.	O
,	O
lugosi	O
,	O
g.	O
,	O
and	O
zeger	O
,	O
k.	O
(	O
1994	O
)	O
.	O
rates	O
of	O
convergence	O
in	O
the	O
source	O
coding	O
theo	O
(	O
cid:173	O
)	O
rem	O
,	O
empirical	B
quantizer	O
design	O
,	O
and	O
universal	B
lossy	O
source	O
coding	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
40:1728-1740.	O
lissack	O
,	O
t.	O
and	O
fu	O
,	O
k.	O
(	O
1976	O
)	O
.	O
error	B
estimation	I
in	O
pattern	O
recognition	O
via	O
za	O
distance	B
between	O
posterior	O
density	O
functions	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
22:34-45.	O
ljung	O
,	O
l.	O
,	O
pflug	O
,	O
g.	O
,	O
and	O
walk	O
,	O
h.	O
(	O
1992	O
)	O
.	O
stochastic	B
approximation	I
and	O
optimization	O
of	O
random	O
systems	O
.	O
birkhauser	O
,	O
basel	O
,	O
boston	O
,	O
berlin	O
.	O
references	O
609	O
lloyd	O
,	O
s.	O
(	O
1982	O
)	O
.	O
least	O
squares	O
quantization	B
in	O
pcm	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
28:129-137.	O
loftsgaarden	O
,	O
d.	O
and	O
quesenberry	O
,	O
c.	O
(	O
1965	O
)	O
.	O
a	O
nonparametric	O
estimate	B
of	O
a	O
multivariate	O
density	O
function	O
.	O
annals	O
of	O
mathematical	O
statistics	B
,	O
.	O
36	O
:	O
1049-1051.	O
logan	O
,	O
b	O
.	O
(	O
1975	O
)	O
.	O
the	O
uncertainty	O
principle	O
in	O
reconstructing	O
functions	O
from	O
projections	O
.	O
duke	O
mathematical	O
journal	O
,	O
42:661-706.	O
logan	O
,	O
b.	O
and	O
shepp	O
,	O
l.	O
(	O
1975	O
)	O
.	O
optimal	O
reconstruction	O
of	O
a	O
function	O
from	O
its	O
projections	O
.	O
duke	O
mathematical	O
journal	O
,	O
42:645-660.	O
loh	O
,	O
w.	O
and	O
vanichsetakul	O
,	O
n.	O
(	O
1988	O
)	O
.	O
tree-structured	O
classification	O
via	O
generalized	B
dis	O
(	O
cid:173	O
)	O
criminant	O
analysis	O
.	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
,	O
83:715-728.	O
loizou	O
,	O
g.	O
and	O
maybank	O
,	O
s.	O
(	O
1987	O
)	O
.	O
the	O
nearest	B
neighbor	I
and	O
the	O
bayes	O
error	O
rates	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
9:254-262.	O
lorentz	O
,	O
g.	O
(	O
1976	O
)	O
.	O
the	O
thirteenth	O
problem	O
of	O
hilbert	O
.	O
in	O
proceedings	O
of	O
symposia	O
in	O
pure	O
mathematics	O
,	O
volume	O
28	O
,	O
pages	O
419-430.	O
providence	O
,	O
ri	O
.	O
lugosi	O
,	O
g.	O
(	O
1992	O
)	O
.	O
learning	B
with	O
an	O
unreliable	O
teacher	O
.	O
pattern	O
recognition	O
,	O
25:79-87.	O
lugosi	O
,	O
g.	O
and	O
nobel	O
,	O
a	O
.	O
(	O
1996	O
)	O
.	O
consistency	B
of	O
data-driven	O
histogram	O
methods	O
for	O
density	B
estimation	I
and	O
classification	O
.	O
annals	O
of	O
statistics	O
,	O
24:687-706.	O
lugosi	O
,	O
g.	O
and	O
pawlak	O
,	O
m.	O
(	O
1994	O
)	O
.	O
on	O
the	O
posterior-probability	O
estimate	B
of	O
the	O
error	O
rate	O
of	O
nonparametric	O
classification	O
rules	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
40:475-	O
481.	O
lugosi	O
,	O
g.	O
and	O
zeger	O
,	O
k.	O
(	O
1995	O
)	O
.	O
nonparametric	O
estimation	B
via	O
empirical	B
risk	I
minimization	I
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
41:677-678.	O
lugosi	O
,	O
g.	O
and	O
zeger	O
,	O
k.	O
(	O
1996	O
)	O
.	O
concept	B
learning	I
using	O
complexity	B
regularization	I
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
42:48-54.	O
lukacs	O
,	O
e.	O
and	O
laha	O
,	O
r.	O
(	O
1964	O
)	O
.	O
applications	O
of	O
characteristic	O
functions	O
in	O
probability	O
theory	O
.	O
griffin	O
,	O
london	O
.	O
lunts	O
,	O
a.	O
and	O
brailovsky	O
,	O
v.	O
(	O
1967	O
)	O
.	O
evaluation	O
of	O
attributes	O
obtained	O
in	O
statistical	O
decision	O
rules	O
.	O
engineering	O
cybernetics	O
,	O
3:98-109.	O
maass	O
,	O
w.	O
(	O
1993	O
)	O
.	O
bounds	O
for	O
the	O
computational	O
power	O
and	O
learning	B
complexity	O
of	O
ana	O
(	O
cid:173	O
)	O
log	O
neural	O
nets	O
.	O
in	O
proceedings	O
of	O
the	O
25th	O
annual	O
acm	O
symposium	O
on	O
the	O
theory	O
of	O
computing	O
,	O
pages	O
335-344.	O
association	B
of	O
computing	O
machinery	O
,	O
new	O
york	O
.	O
maass	O
,	O
w.	O
(	O
1994	O
)	O
.	O
neural	O
nets	O
with	O
superlinear	O
vc-dimension	O
.	O
neural	O
computation	O
,	O
6:875-	O
882.	O
macintyre	O
,	O
a.	O
and	O
sontag	O
,	O
e.	O
(	O
1993	O
)	O
.	O
finiteness	O
results	O
for	O
sigmoidal	O
``	O
neural	O
''	O
networks	O
.	O
in	O
proceedings	O
of	O
the	O
25th	O
annual	O
acm	O
symposium	O
on	O
the	O
theory	O
of	O
computing	O
,	O
pages	O
325-334.	O
association	B
of	O
computing	O
machinery	O
,	O
new	O
york	O
.	O
mack	O
,	O
y	O
.	O
(	O
1981	O
)	O
.	O
local	O
properties	O
of	O
k	O
-nearest	O
neighbor	O
regression	O
estimates	O
.	O
siam	O
journal	O
on	O
algebraic	O
and	O
discrete	O
methods	O
,	O
2:311-323.	O
mahalanobis	O
,	O
p.	O
(	O
1936	O
)	O
.	O
on	O
the	O
generalized	B
distance	O
in	O
statistics	O
.	O
proceedings	O
of	O
the	O
national	O
institute	O
of	O
sciences	O
of	O
india	O
,	O
2:49-55.	O
mahalanobis	O
,	O
p.	O
(	O
1961	O
)	O
.	O
a	O
method	O
offractile	O
graphical	O
analysis	O
.	O
sankhya	O
series	O
a	O
,	O
23:41-	O
64.	O
marron	O
,	O
j	O
.	O
(	O
1983	O
)	O
.	O
optimal	O
rates	O
of	O
convergence	O
to	O
bayes	O
risk	O
in	O
nonparametric	O
discrimi	O
(	O
cid:173	O
)	O
nation	O
.	O
annals	O
of	O
statistics	O
,	O
11:1142-1155.	O
massart	O
,	O
p.	O
(	O
1983	O
)	O
.	O
vitesse	O
de	O
convergence	O
dans	O
le	O
theoreme	O
de	O
la	O
limite	O
centrale	O
pour	O
le	O
processus	O
empirique	O
.	O
phd	O
thesis	O
,	O
universite	O
paris-sud	O
,	O
orsay	O
,	O
france	O
.	O
massart	O
,	O
p.	O
(	O
1990	O
)	O
.	O
the	O
tight	O
constant	O
in	O
the	O
dvoretzky-kiefer-wolfowitz	O
inequality	B
.	O
annals	O
of	O
probability	O
,	O
18	O
:	O
1269-1283	O
.	O
610	O
references	O
mathai	O
,	O
a.	O
and	O
rathie	O
,	O
p.	O
(	O
1975	O
)	O
.	O
basic	O
concepts	O
in	O
information	O
theory	O
and	O
statistics	B
.	O
wiley	O
eastern	O
ltd.	O
,	O
new	O
delhi	O
.	O
matloff	O
,	O
n.	O
and	O
pruitt	O
,	O
r.	O
(	O
1984	O
)	O
.	O
the	O
asymptotic	O
distribution	O
of	O
an	O
estimator	O
of	O
the	O
bayes	O
error	O
rate	O
.	O
pattern	O
recognition	O
letters	O
,	O
2:271-274.	O
matula	O
,	O
d.	O
and	O
sokal	O
,	O
r.	O
(	O
1980	O
)	O
.	O
properties	O
of	O
gabriel	O
graphs	O
relevant	O
to	O
geographic	O
varia	O
(	O
cid:173	O
)	O
tion	O
research	O
and	O
the	O
clustering	B
of	O
points	O
in	O
the	O
plane	O
.	O
geographical	O
analysis	O
,	O
12:205-	O
222.	O
matushita	O
,	O
k.	O
(	O
1956	O
)	O
.	O
decision	O
rule	O
,	O
based	O
on	O
distance	O
,	O
for	O
the	O
classification	O
problem	O
.	O
annals	O
of	O
the	O
institute	O
of	O
statistical	O
mathematics	O
,	O
8:67-77.	O
matushita	O
,	O
k.	O
(	O
1973	O
)	O
.	O
discrimination	O
and	O
the	O
affinity	O
of	O
distributions	O
.	O
in	O
discriminant	O
anal	O
(	O
cid:173	O
)	O
ysis	O
and	O
applications	O
,	O
cacoullos	O
,	O
t.	O
,	O
editor	O
,	O
pages	O
213-223.	O
academic	O
press	O
,	O
new	O
york	O
.	O
max	O
,	O
j	O
.	O
(	O
1960	O
)	O
.	O
quantizing	O
for	O
minimum	O
distortion	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
6:7-12.	O
mcdiarmid	O
,	O
c.	O
(	O
1989	O
)	O
.	O
on	O
the	O
method	B
of	I
bounded	I
differences	I
.	O
in	O
surveys	O
in	O
combinatorics	O
1989	O
,	O
pages	O
148-188.	O
cambridge	O
university	O
press	O
,	O
cambridge	O
.	O
mclachlan	O
,	O
g.	O
(	O
1976	O
)	O
.	O
the	O
bias	B
of	I
the	O
apparent	B
error	I
rate	I
in	O
discriminant	O
analysis	O
.	O
biometrika	O
,	O
63:239-244.	O
mclachlan	O
,	O
g.	O
(	O
1992	O
)	O
.	O
discriminant	O
analysis	O
and	O
statistical	O
pattern	O
recognition	O
.	O
john	O
wiley	O
,	O
new	O
york	O
.	O
meisel	O
,	O
w.	O
(	O
1969	O
)	O
.	O
potential	O
functions	O
in	O
mathematical	O
pattern	O
recognition	O
.	O
ieee	O
transac	O
(	O
cid:173	O
)	O
tions	O
on	O
computers	O
,	O
18:911-918.	O
meisel	O
,	O
w.	O
(	O
1972	O
)	O
.	O
computer	O
oriented	O
approaches	O
to	O
pattern	O
recognition	O
.	O
academic	O
press	O
,	O
new	O
york	O
.	O
meisel	O
,	O
w.	O
(	O
1990	O
)	O
.	O
parsimony	O
in	O
neural	O
networks	O
.	O
in	O
international	O
conference	O
on	O
neural	O
networks	O
,	O
pages	O
443-446.	O
lawrence	O
erlbaum	O
associates	O
,	O
hillsdale	O
,	O
nj	O
.	O
meisel	O
,	O
w.	O
and	O
michalopoulos	O
,	O
d.	O
(	O
1973	O
)	O
.	O
a	O
partitioning	O
algorithm	B
with	O
application	O
in	O
pattern	O
classification	O
and	O
the	O
optimization	O
of	O
decision	O
tree	B
.	O
ieee	O
transactions	O
on	O
com	O
(	O
cid:173	O
)	O
puters	O
,	O
22:93-103.	O
michel-briand	O
,	O
c.	O
and	O
milhaud	O
,	O
x	O
.	O
(	O
1994	O
)	O
.	O
asymptotic	O
behavior	O
of	O
the	O
aid	O
method	O
.	O
tech	O
(	O
cid:173	O
)	O
nical	O
report	O
,	O
universite	O
montpellier	O
2	O
,	O
montpellier	O
.	O
mielniczuk	O
,	O
j.	O
and	O
tyrcha	O
,	O
j	O
.	O
(	O
1993	O
)	O
.	O
consistency	B
of	O
multilayer	B
perceptron	O
regression	O
esti	O
(	O
cid:173	O
)	O
mators	O
.	O
neural	O
networks	O
,	O
to	O
appear	O
.	O
minsky	O
,	O
m.	O
(	O
1961	O
)	O
.	O
steps	O
towards	O
artificial	O
intelligence	O
.	O
in	O
proceedings	O
of	O
the	O
ire	O
,	O
vol	O
(	O
cid:173	O
)	O
ume	O
49	O
,	O
pages	O
8-30.	O
minsky	O
,	O
m.	O
and	O
papert	O
,	O
s.	O
(	O
1969	O
)	O
.	O
perceptrons	O
,	O
'	O
an	O
introduction	O
to	O
computational	O
geometry	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
mitchell	O
,	O
a.	O
and	O
krzanowski	O
,	O
w.	O
(	O
1985	O
)	O
.	O
the	O
mahalanobis	O
distance	B
and	O
elliptic	O
distribu	O
(	O
cid:173	O
)	O
tions	O
.	O
biometrika	O
,	O
72:464-467.	O
mizoguchi	O
,	O
r.	O
,	O
kizawa	O
,	O
m.	O
,	O
and	O
shimura	O
,	O
m.	O
(	O
1977	O
)	O
.	O
piecewise	O
linear	O
discriminant	O
func	O
(	O
cid:173	O
)	O
tions	O
in	O
pattern	O
recognition	O
.	O
systems-computers-controls	O
,	O
8	O
:	O
114-121.	O
moody	O
,	O
j.	O
and	O
darken	O
,	O
j	O
.	O
(	O
1989	O
)	O
.	O
fast	O
learning	B
in	O
networks	O
of	O
locally-tuned	O
processing	O
units	O
.	O
neural	O
computation	O
,	O
1	O
:281-294.	O
moore	O
,	O
d.	O
,	O
whitsitt	O
,	O
s.	O
,	O
and	O
landgrebe	O
,	O
d.	O
(	O
1976	O
)	O
.	O
variance	O
comparisons	O
for	O
unbiased	O
estimators	O
of	O
probability	O
of	O
correct	O
classification	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
22	O
:	O
102-105.	O
morgan	O
,	O
j.	O
and	O
sonquist	O
,	O
j	O
.	O
(	O
1963	O
)	O
.	O
problems	O
in	O
the	O
analysis	O
of	O
survey	O
data	O
,	O
and	O
a	O
proposal	O
.	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
,	O
58:415-434.	O
references	O
611	O
mui	O
,	O
1.	O
and	O
fu	O
,	O
k.	O
(	O
1980	O
)	O
.	O
automated	O
classification	O
of	O
nucleated	O
blood	O
cells	O
using	O
a	O
binary	O
tree	B
classifier	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
2:429-	O
443.	O
myles	O
,	O
1.	O
and	O
hand	O
,	O
d.	O
(	O
1990	O
)	O
.	O
the	O
multi-class	O
metric	B
problem	O
in	O
nearest	O
neighbour	O
dis	O
(	O
cid:173	O
)	O
crimination	O
rules	O
.	O
pattern	O
recognition	O
,	O
23	O
:	O
1291-1297.	O
nadaraya	O
,	O
e.	O
(	O
1964	O
)	O
.	O
on	O
estimating	O
regression	O
.	O
theory	O
of	O
probability	O
and	O
its	O
applications	O
,	O
9	O
:	O
141-142.	O
nadaraya	O
,	O
e.	O
(	O
1970	O
)	O
.	O
remarks	O
on	O
nonparametric	O
estimates	O
for	O
density	O
functions	O
and	O
regres	O
(	O
cid:173	O
)	O
sion	O
curves	O
.	O
theory	O
of	O
probability	O
and	O
its	O
applications	O
,	O
15	O
:	O
134-137.	O
narendra	O
,	O
p.	O
and	O
fukunaga	O
,	O
k.	O
(	O
1977	O
)	O
.	O
a	O
branch	O
and	O
bound	O
algorithm	B
for	O
feature	O
subset	O
selection	B
.	O
ieee	O
transactions	O
on	O
computers	O
,	O
26:917-922.	O
natarajan	O
,	O
b	O
.	O
(	O
1991	O
)	O
.	O
machine	O
learning	B
:	O
a	O
theoretical	O
approach	O
.	O
morgan	O
kaufmann	O
,	O
san	O
mateo	O
,	O
ca	O
.	O
nevelson	O
,	O
m.	O
and	O
khasminskii	O
,	O
r.	O
(	O
1973	O
)	O
.	O
stochastic	B
approximation	I
and	O
recursive	B
es	O
(	O
cid:173	O
)	O
timation	O
.	O
translations	O
of	O
mathematical	O
monographs	O
,	O
vol	O
.	O
47.	O
american	O
mathematical	O
society	O
,	O
providence	O
,	O
ri	O
.	O
niemann	O
,	O
h.	O
and	O
goppert	O
,	O
r.	O
(	O
1988	O
)	O
.	O
an	O
efficient	O
branch-and-bound	O
nearest	O
neighbour	O
classifier	B
.	O
pattern	O
recognition	O
letters	O
,	O
7:67-72.	O
nilsson	O
,	O
n.	O
(	O
1965	O
)	O
.	O
learning	B
machines	O
:	O
foundations	O
of	O
trainable	O
pattern	O
classifying	O
sys	O
(	O
cid:173	O
)	O
tems	O
.	O
mcgraw-hill	O
,	O
new	O
york	O
.	O
nishizeki	O
,	O
t.	O
and	O
chiba	O
,	O
n.	O
(	O
1988	O
)	O
.	O
planar	O
graphs	O
:	O
theory	O
and	O
algorithms	O
.	O
north	O
holland	O
,	O
amsterdam	O
.	O
nobel	O
,	O
a	O
.	O
(	O
1994	O
)	O
.	O
histogram	O
regression	O
estimates	O
using	O
data-dependent	B
partitions	O
.	O
techni	O
(	O
cid:173	O
)	O
cal	O
report	O
,	O
beckman	O
institute	O
,	O
university	O
of	O
illinois	O
,	O
urbana-champaign	O
.	O
nobel	O
,	O
a	O
.	O
(	O
1992	O
)	O
.	O
on	O
uniform	O
laws	O
of	O
averages	O
.	O
phd	O
thesis	O
,	O
department	O
of	O
statistics	O
,	O
stanford	O
university	O
,	O
stanford	O
,	O
ca	O
.	O
nolan	O
,	O
d.	O
and	O
pollard	O
,	O
d.	O
(	O
1987	O
)	O
.	O
u-processes	O
:	O
rates	O
of	O
convergence	O
.	O
annals	O
of	O
statistics	O
,	O
15:780-799.	O
okamoto	O
,	O
m.	O
(	O
1958	O
)	O
.	O
some	O
inequalities	O
relating	O
to	O
the	O
partial	O
sum	O
of	O
binomial	O
probabilities	O
.	O
annals	O
of	O
the	O
institute	O
of	O
statistical	O
mathematics	O
,	O
10:29-35.	O
olshen	O
,	O
r.	O
(	O
1977	O
)	O
.	O
comments	O
on	O
a	O
paper	O
by	O
c.l	O
.	O
stone	O
.	O
annals	O
of	O
statistics	O
,	O
5:632-633.	O
ott	O
,	O
1.	O
and	O
kronmal	O
,	O
r.	O
(	O
1976	O
)	O
.	O
some	O
classification	O
procedures	O
for	O
multivariate	O
binary	O
data	O
using	O
orthogonal	O
functions	O
.	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
,	O
71:391-	O
399.	O
papadimitriou	O
,	O
c.	O
and	O
bentley	O
,	O
1	O
.	O
(	O
1980	O
)	O
.	O
a	O
worst-case	O
analysis	O
of	O
nearest	O
neighbor	O
search	O
(	O
cid:173	O
)	O
ing	O
by	O
projection	O
.	O
in	O
automata	O
,	O
languages	O
and	O
programming	O
1980	O
,	O
pages	O
470-482.	O
lecture	O
notes	O
in	O
computer	O
science	O
#	O
85	O
,	O
springer-verlag	O
,	O
berlin	O
.	O
park	O
,	O
1.	O
and	O
sandberg	O
,	O
i	O
.	O
(	O
1991	O
)	O
.	O
universal	B
approximation	O
using	O
radial-basis-function	O
net	O
(	O
cid:173	O
)	O
works	O
.	O
neural	O
computation	O
,	O
3:246-257.	O
park	O
,	O
1.	O
and	O
sandberg	O
,	O
i	O
.	O
(	O
1993	O
)	O
.	O
approximation	O
and	O
radial-basis-function	O
networks	O
.	O
neural	O
computation	O
,	O
5:305-316.	O
park	O
,	O
y.	O
and	O
sklansky	O
,	O
1	O
.	O
(	O
1990	O
)	O
.	O
automated	O
design	O
of	O
linear	O
tree	B
classifiers	O
.	O
pattern	O
recog	O
(	O
cid:173	O
)	O
nition	O
,	O
23:1393-1412.	O
parthasarathy	O
,	O
k.	O
and	O
bhattacharya	O
,	O
p.	O
(	O
1961	O
)	O
.	O
some	O
limit	O
theorems	O
in	O
regression	O
theory	O
.	O
sankhya	O
series	O
a	O
,	O
23:91-102.	O
parzen	O
,	O
e.	O
(	O
1962	O
)	O
.	O
on	O
the	O
estimation	B
of	I
a	O
probability	O
density	O
function	O
and	O
the	O
mode	O
.	O
annals	O
of	O
mathematical	O
statistics	B
,	O
33	O
:	O
1065-1076	O
.	O
612	O
references	O
patrick	O
,	O
e.	O
(	O
1966	O
)	O
.	O
distribution-free	O
minimum	O
conditional	O
risk	O
learning	B
systems	O
.	O
technical	O
report	O
tr-ee-66-18	O
,	O
purdue	O
university	O
,	O
lafayette	O
,	O
in	O
.	O
patrick	O
,	O
e.	O
and	O
fischer	O
,	O
f.	O
(	O
1970	O
)	O
.	O
a	O
generalized	O
k-nearest	O
neighbor	B
rule	I
.	O
information	O
and	O
control,16:128-152.	O
patrick	O
,	O
e.	O
and	O
fisher	O
,	O
f.	O
(	O
1967	O
)	O
.	O
introduction	O
to	O
the	O
performance	O
of	O
distribution-free	O
conditional	O
risk	O
learning	B
systems	O
.	O
technical	O
report	O
tr-ee-67-12	O
,	O
purdue	O
university	O
,	O
lafayette	O
,	O
in	O
.	O
pawlak	O
,	O
m.	O
(	O
1988	O
)	O
.	O
on	O
the	O
asymptotic	O
properties	O
of	O
smoothed	O
estimators	O
of	O
the	O
classification	O
error	O
rate	O
.	O
pattern	O
recognition	O
,	O
21:515-524.	O
payne	O
,	O
h.	O
and	O
meisel	O
,	O
w.	O
(	O
1977	O
)	O
.	O
an	O
algorithm	B
for	O
constructing	O
optimal	O
binary	B
decision	O
trees	O
.	O
ieee	O
transactions	O
on	O
computers	O
,	O
26:905-916.	O
pearl	O
,	O
j	O
.	O
(	O
1979	O
)	O
.	O
capacity	O
and	O
error	O
estimates	O
for	O
boolean	O
classifiers	O
with	O
limited	O
complexity	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
1:350-355.	O
penrod	O
,	O
c.	O
and	O
wagner	O
,	O
t.	O
(	O
1977	O
)	O
.	O
another	O
look	O
at	O
the	O
edited	B
nearest	O
neighbor	B
rule	I
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
and	O
cybernetics	O
,	O
7:92-94.	O
peterson	O
,	O
d.	O
(	O
1970	O
)	O
.	O
some	O
convergence	O
properties	O
of	O
a	O
nearest	O
neighbor	O
decision	O
rule	B
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
16:26-31.	O
petrov	O
,	O
v.	O
(	O
1975	O
)	O
.	O
sums	O
of	O
independent	O
random	O
variables	O
.	O
springer-verlag	O
,	O
berlin	O
.	O
pippenger	O
,	O
n.	O
(	O
1977	O
)	O
.	O
information	O
theory	O
and	O
the	O
complexity	O
of	O
boolean	O
functions	O
.	O
math	O
(	O
cid:173	O
)	O
ematical	O
systems	O
theory	O
,	O
10	O
:	O
124-162.	O
poggio	O
,	O
t.	O
and	O
girosi	O
,	O
f.	O
(	O
1990	O
)	O
.	O
a	O
theory	O
of	O
networks	O
for	O
approximation	O
and	O
learning	B
.	O
proceedings	O
of	O
the	O
ieee	O
,	O
78	O
:	O
1481-1497.	O
pollard	O
,	O
d.	O
(	O
1981	O
)	O
.	O
strong	B
consistency	I
of	O
k-means	B
clustering	I
.	O
annals	O
of	O
statistics	O
,	O
9	O
:	O
135-	O
140.	O
pollard	O
,	O
d.	O
(	O
1982	O
)	O
.	O
quantization	B
and	O
the	O
method	O
of	O
k-means	O
.	O
ieee	O
transactions	O
on	O
infor	O
(	O
cid:173	O
)	O
mation	O
theory	O
,	O
28	O
:	O
199-205.	O
pollard	O
,	O
d.	O
(	O
1984	O
)	O
.	O
convergence	O
of	O
stochastic	O
processes	O
.	O
springer-verlag	O
,	O
new	O
york	O
.	O
pollard	O
,	O
d.	O
(	O
1986	O
)	O
.	O
rates	O
of	O
uniform	O
almost	O
sure	O
convergence	O
for	O
empirical	B
processes	O
in	O
(	O
cid:173	O
)	O
dexed	O
by	O
unbounded	O
classes	O
of	O
functions	O
.	O
manuscript	O
.	O
pollard	O
,	O
d.	O
(	O
1990	O
)	O
.	O
empirical	B
processes	O
:	O
theory	O
and	O
applications	O
.	O
nsf-cbms	O
regional	O
conference	O
series	O
in	O
probability	O
and	O
statistics	B
,	O
institute	O
of	O
mathematical	O
statistics	B
,	O
hayward	O
,	O
ca	O
.	O
powell	O
,	O
m.	O
(	O
1987	O
)	O
.	O
radial	O
basis	O
functions	O
for	O
multivariable	O
interpolation	O
:	O
a	O
review	O
.	O
algo	O
(	O
cid:173	O
)	O
rithms	O
for	O
approximation	O
.	O
clarendon	O
press	O
,	O
oxford	O
.	O
prep	O
arata	O
,	O
f.	O
and	O
shamos	O
,	O
m.	O
(	O
1985	O
)	O
.	O
computational	O
geometry-an	O
introduction	O
.	O
springer	O
(	O
cid:173	O
)	O
verlag	O
,	O
new	O
york	O
.	O
psaltis	O
,	O
d.	O
,	O
snapp	O
,	O
r.	O
,	O
and	O
venkatesh	O
,	O
s.	O
(	O
1994	O
)	O
.	O
on	O
the	O
finite	O
sample	O
performance	O
of	O
the	O
nearest	B
neighbor	I
classifier	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
40:820-837.	O
qing-yun	O
,	O
s.	O
and	O
fu	O
,	O
k.	O
(	O
1983	O
)	O
.	O
a	O
method	O
for	O
the	O
design	O
of	O
binary	O
tree	B
classifiers	O
.	O
pattern	O
recognition	O
,	O
16:593-603.	O
quesenberry	O
,	O
c.	O
and	O
gessaman	O
,	O
m.	O
(	O
1968	O
)	O
.	O
nonparametric	O
discrimination	O
using	O
tolerance	O
regions	O
.	O
annals	O
of	O
mathematical	O
statistics	B
,	O
39:664-673.	O
quinlan	O
,	O
j	O
.	O
(	O
1993	O
)	O
.	O
c4.5	O
:	O
programs	O
for	O
machine	O
learning	B
.	O
morgan	O
kaufmann	O
,	O
san	O
mateo	O
.	O
rabiner	O
,	O
l.	O
,	O
levinson	O
,	O
s.	O
,	O
rosenberg	O
,	O
a.	O
,	O
and	O
wilson	O
,	O
j	O
.	O
(	O
1979	O
)	O
.	O
speaker-independentrecog	O
(	O
cid:173	O
)	O
nition	O
of	O
isolated	O
words	O
using	O
clustering	B
techniques	O
.	O
ieee	O
transactions	O
on	O
acoustics	O
,	O
speech	O
,	O
and	O
signal	O
processing	O
,	O
27:339-349.	O
references	O
613	O
rao	O
,	O
r.	O
(	O
1962	O
)	O
.	O
relations	O
between	O
weak	B
and	O
uniform	B
convergence	O
of	O
measures	O
with	O
appli	O
(	O
cid:173	O
)	O
cations	O
.	O
annals	O
of	O
mathematical	O
statistics	B
,	O
33:659-680.	O
raudys	O
,	O
s.	O
(	O
1972	O
)	O
.	O
on	O
the	O
amount	O
of	O
a	O
priori	O
information	O
in	O
designing	O
the	O
classification	O
algorithm	B
.	O
technical	O
cybernetics	O
,	O
4	O
:	O
168-174.	O
raudys	O
,	O
s.	O
(	O
1976	O
)	O
.	O
on	O
dimensionality	O
,	O
learning	B
sample	O
size	O
and	O
complexity	O
of	O
classification	O
algorithms	O
.	O
in	O
proceedings	O
of	O
the	O
3rd	O
international	O
conference	O
on	O
pattern	O
recognition	O
,	O
pages	O
166-169.	O
ieee	O
computer	O
society	O
,	O
long	O
beach	O
,	O
ca	O
.	O
raudys	O
,	O
s.	O
and	O
pikelis	O
,	O
v.	O
(	O
1980	O
)	O
.	O
on	O
dimensionality	O
,	O
sample	O
size	O
,	O
classification	O
error	O
,	O
and	O
complexity	O
of	O
classification	O
algorithm	B
in	O
pattern	O
recognition	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
2:242-252.	O
raudys	O
,	O
s.	O
and	O
pikelis	O
,	O
v.	O
(	O
1982	O
)	O
.	O
collective	O
selection	B
of	O
the	O
best	O
version	O
of	O
a	O
pattern	O
recognition	O
system	O
.	O
pattern	O
recognition	O
letters	O
,	O
1	O
:7-13.	O
rejto	O
,	O
l.	O
and	O
revesz	O
,	O
p.	O
(	O
1973	O
)	O
.	O
density	B
estimation	I
and	O
pattern	O
classification	O
.	O
problems	O
of	O
control	O
and	O
information	O
theory	O
,	O
2:67-80.	O
renyi	O
,	O
a	O
.	O
(	O
1961	O
)	O
.	O
on	O
measures	O
of	O
entropy	O
and	O
information	O
.	O
in	O
proceedings	O
of	O
the	O
fourth	O
berkeley	O
symposium	O
,	O
pages	O
547-561.	O
university	O
of	O
california	O
press	O
,	O
berkeley	O
.	O
revesz	O
,	O
p.	O
(	O
1973	O
)	O
.	O
robbins-monroe	O
procedures	O
in	O
a	O
hilbert	O
space	O
and	O
its	O
application	O
in	O
the	O
theory	O
of	O
learning	O
processes	O
.	O
studia	O
scientiarium	O
mathematicarum	O
hungarica	O
,	O
8:391-	O
398.	O
ripley	O
,	O
b	O
.	O
(	O
1993	O
)	O
.	O
statistical	O
aspects	O
of	O
neural	O
networks	O
.	O
in	O
networks	O
and	O
chaos	O
--	O
statistical	O
and	O
probabilistic	O
aspects	O
,	O
barndorff-nielsen	O
,	O
0.	O
,	O
jensen	O
,	O
j.	O
,	O
and	O
kendall	O
,	O
w.	O
,	O
editors	O
,	O
pages	O
40	O
--	O
123.	O
chapman	O
and	O
hall	O
,	O
london	O
,	O
u.k.	O
ripley	O
,	O
b	O
.	O
(	O
1994	O
)	O
.	O
neural	O
networks	O
and	O
related	O
methods	O
for	O
classification	O
.	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
,	O
56:409-456.	O
rissanen	O
,	O
l.	O
(	O
1983	O
)	O
.	O
a	O
universal	O
prior	O
for	O
integers	O
and	O
estimation	B
by	O
minimum	O
description	O
length	O
.	O
annals	O
of	O
statistics	O
,	O
11:416-43l	O
.	O
ritter	O
,	O
g.	O
,	O
woodruff	O
,	O
h.	O
,	O
lowry	O
,	O
s.	O
,	O
and	O
isenhour	O
,	O
t.	O
(	O
1975	O
)	O
.	O
an	O
algorithm	B
for	O
a	O
selective	O
nearest	B
neighbor	I
decision	O
rule	B
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
21	O
:	O
665-669.	O
robbins	O
,	O
h.	O
and	O
monro	O
,	O
s.	O
(	O
1951	O
)	O
.	O
a	O
stochastic	O
approximation	O
method	O
.	O
annals	O
of	O
mathe	O
(	O
cid:173	O
)	O
matical	O
statistics	B
,	O
22	O
:	O
400-407.	O
rogers	O
,	O
w.	O
and	O
wagner	O
,	O
t.	O
(	O
1978	O
)	O
.	O
a	O
finite	O
sample	O
distribution-free	O
performance	O
bound	O
for	O
local	O
discrimination	O
rules	O
.	O
annals	O
of	O
statistics	O
,	O
6:506-514.	O
rosenblatt	O
,	O
f.	O
(	O
1962	O
)	O
.	O
principles	O
ofneurodynamics	O
:	O
perceptrons	O
and	O
the	O
theory	O
of	O
brain	O
mechanisms	O
.	O
spartan	O
books	O
,	O
washington	O
,	O
dc	O
.	O
rosenblatt	O
,	O
m.	O
(	O
1956	O
)	O
.	O
remarks	O
on	O
some	O
nonparametric	O
estimates	O
of	O
a	O
density	O
function	O
.	O
annals	O
of	O
mathematical	O
statistics	B
,	O
27:832-837.	O
rounds	O
,	O
e.	O
(	O
1980	O
)	O
.	O
a	O
combined	O
nonparametric	O
approach	O
to	O
feature	O
selection	O
and	O
binary	B
decision	O
tree	B
design	O
.	O
pattern	O
recognition	O
,	O
12:313-317.	O
royall	O
,	O
r.	O
(	O
1966	O
)	O
.	O
a	O
class	O
of	O
n	O
onparametric	O
estimators	O
of	O
a	O
smooth	O
regression	B
function	I
.	O
phd	O
thesis	O
,	O
stanford	O
university	O
,	O
stanford	O
,	O
ca	O
.	O
rumelhart	O
,	O
d.	O
,	O
hinton	O
,	O
g.	O
,	O
and	O
williams	O
,	O
r.	O
(	O
1986	O
)	O
.	O
learning	B
internal	O
representations	O
by	O
er	O
(	O
cid:173	O
)	O
ror	O
propagation	O
.	O
in	O
parallel	O
distributed	O
processing	O
vol	O
.	O
i	O
,	O
rumelhart	O
,	O
d.	O
,	O
j	O
.l.mccelland	O
,	O
and	O
the	O
pdp	O
research	O
group	O
,	O
editors	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
reprinted	O
in	O
:	O
l.a.	O
anderson	O
and	O
e.	O
rosenfeld	O
,	O
neurocomputing	O
--	O
foundations	O
of	O
research	O
,	O
mit	O
press	O
,	O
cambridge	O
,	O
ma.	O
,	O
pp	O
.	O
673-695,1988.	O
ruppert	O
,	O
d.	O
(	O
1991	O
)	O
.	O
stochastic	B
approximation	I
.	O
in	O
handbook	O
of	O
sequential	O
analysis	O
,	O
ghosh	O
,	O
b.	O
and	O
sen	O
,	O
p.	O
,	O
editors	O
,	O
pages	O
503-529.	O
marcel	O
dekker	O
,	O
new	O
york	O
.	O
614	O
references	O
samet	O
,	O
h.	O
(	O
1984	O
)	O
.	O
the	O
quadtree	B
and	O
related	O
hierarchical	O
data	O
structures	O
.	O
computing	O
surveys	O
,	O
16:187-260.	O
samet	O
,	O
h.	O
(	O
1990a	O
)	O
.	O
applications	O
of	O
spatial	O
data	O
structures	O
.	O
addison-wesley	O
,	O
reading	O
,	O
ma	O
.	O
samet	O
,	O
h.	O
(	O
1990b	O
)	O
.	O
the	O
design	O
and	O
analysis	O
of	O
spatial	O
data	O
structures	O
.	O
addison-wesley	O
,	O
reading	O
,	O
ma	O
.	O
sansone	O
,	O
g.	O
(	O
1969	O
)	O
.	O
orthogonal	O
functions	O
.	O
interscience	O
,	O
new	O
york	O
.	O
sauer	O
,	O
n.	O
(	O
1972	O
)	O
.	O
on	O
the	O
density	O
of	O
families	O
of	O
sets	O
.	O
journal	O
of	O
combinatorial	O
theory	O
series	O
a,13:145-147.	O
scheff6	O
,	O
h.	O
(	O
1947	O
)	O
.	O
a	O
useful	O
convergence	O
theorem	B
for	O
probability	O
distributions	O
.	O
annals	O
of	O
mathematical	O
statistics	B
,	O
18:434-458.	O
schiaffli	O
,	O
l.	O
(	O
1950	O
)	O
.	O
gesammelte	O
mathematische	O
abhandlungen	O
.	O
birkhauser-verlag	O
,	O
basel	O
.	O
schmidt	O
,	O
w.	O
(	O
1994	O
)	O
.	O
neural	O
pattern	O
classifying	O
systems	O
.	O
phd	O
thesis	O
,	O
technical	O
university	O
,	O
delft	O
,	O
the	O
netherlands	O
.	O
schoenberg	O
,	O
i	O
.	O
(	O
1950	O
)	O
.	O
on	O
p6lya	O
frequency	O
functions	O
,	O
ii	O
:	O
variation-diminishing	O
integral	O
operators	O
of	O
the	O
convolution	O
type	O
.	O
acta	O
scientiarium	O
mathematicarum	O
szeged	O
,	O
12:97-	O
106.	O
schwartz	O
,	O
s.	O
(	O
1967	O
)	O
.	O
estimation	B
of	I
probability	O
density	O
by	O
an	O
orthogonal	O
series	O
.	O
annals	O
of	O
mathematical	O
statistics	B
,	O
38:1261-1265.	O
schwemer	O
,	O
g.	O
and	O
dunn	O
,	O
o	O
.	O
(	O
1980	O
)	O
.	O
posterior	B
probability	I
estimators	O
in	O
classification	O
sim	O
(	O
cid:173	O
)	O
ulations	O
.	O
communications	O
in	O
statistics	O
--	O
simulation	O
,	O
b9	O
:	O
133-140.	O
sebestyen	O
,	O
g.	O
(	O
1962	O
)	O
.	O
decision-making	O
processes	O
in	O
pattern	O
recognition	O
.	O
macmillan	O
,	O
new	O
york	O
.	O
serfiing	O
,	O
r.	O
(	O
1974	O
)	O
.	O
probability	O
inequalities	O
for	O
the	O
sum	O
in	O
sampling	O
without	O
replacement	O
.	O
annals	O
of	O
statistics	O
,	O
2:39-48.	O
sethi	O
,	O
i	O
.	O
(	O
1981	O
)	O
.	O
a	O
fast	O
algorithm	B
for	O
recognizing	O
nearest	O
neighbors	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
and	O
cybernetics	O
,	O
11:245-248.	O
sethi	O
,	O
i	O
.	O
(	O
1990	O
)	O
.	O
entropy	B
nets	O
:	O
from	O
decision	O
trees	O
to	O
neural	O
nets	O
.	O
proceedings	O
of	O
the	O
ieee	O
,	O
78:1605-1613.	O
sethi	O
,	O
i	O
.	O
(	O
1991	O
)	O
.	O
decision	O
tree	O
performance	O
enhancement	O
using	O
an	O
artificial	O
neural	B
network	I
interpretation	O
.	O
in	O
artificial	O
neural	O
networks	O
and	O
statistical	O
pattern	O
recognition	O
,	O
old	O
and	O
new	O
connections	O
,	O
sethi	O
,	O
i.	O
and	O
jain	O
,	O
a.	O
,	O
editors	O
,	O
pages	O
71-88.	O
elsevier	O
science	O
publishers	O
,	O
amsterdam	O
.	O
sethi	O
,	O
i.	O
and	O
chatterjee	O
,	O
b	O
.	O
(	O
1977	O
)	O
.	O
efficient	O
decision	O
tree	O
design	O
for	O
discrete	O
variable	B
pattern	O
recognition	O
problems	O
.	O
pattern	O
recognition	O
,	O
9	O
:	O
197-206.	O
sethi	O
,	O
i.	O
and	O
sarvarayudu	O
,	O
g.	O
(	O
1982	O
)	O
.	O
hierarchical	O
classifier	B
design	O
using	O
mutual	O
informa	O
(	O
cid:173	O
)	O
tion	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
4:441-445.	O
shannon	O
,	O
c.	O
(	O
1948	O
)	O
.	O
a	O
mathematical	O
theory	O
of	O
communication	O
.	O
bell	O
systems	O
technical	O
journal,27:379-423.	O
shawe-taylor	O
,	O
j	O
.	O
(	O
1994	O
)	O
.	O
sample	O
sizes	O
for	O
sigmoidal	O
neural	O
networks	O
.	O
technical	O
report	O
,	O
department	O
of	O
computer	O
science	O
,	O
royal	O
holloway	O
,	O
university	O
of	O
london	O
,	O
egham	O
,	O
eng	O
(	O
cid:173	O
)	O
land	O
.	O
shawe-taylor	O
,	O
j.	O
,	O
anthony	O
,	O
m.	O
,	O
and	O
biggs	O
,	O
n.l	O
.	O
(	O
1993	O
)	O
.	O
bounding	O
sample	O
size	O
with	O
the	O
vapnik-chervonenkis	O
dimension	B
.	O
discrete	O
applied	O
mathematics	O
,	O
42:65-73.	O
shiryayev	O
,	O
a	O
.	O
(	O
1984	O
)	O
.	O
probability	O
.	O
springer-verlag	O
,	O
new	O
york	O
.	O
short	O
,	O
r.	O
and	O
fukunaga	O
,	O
k.	O
(	O
1981	O
)	O
.	O
the	O
optimal	O
distance	B
measure	O
for	O
nearest	B
neighbor	I
classification	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
27:622-627.	O
references	O
615	O
simon	O
,	O
h.	O
(	O
1991	O
)	O
.	O
the	O
vapnik-chervonenkis	O
dimension	B
of	O
decision	O
trees	O
with	O
bounded	O
rank	O
.	O
information	O
processing	O
letters	O
,	O
39:137-141.	O
simon	O
,	O
h.	O
(	O
1993	O
)	O
.	O
general	O
lower	O
bounds	O
on	O
the	O
number	O
of	O
examples	O
needed	O
for	O
learning	B
probabilistic	O
concepts	O
.	O
in	O
proceedings	O
of	O
the	O
sixth	O
annual	O
acm	O
conference	O
on	O
compu	O
(	O
cid:173	O
)	O
tational	O
learning	B
theory	O
,	O
pages	O
402-412.	O
association	B
for	O
computing	O
machinery	O
,	O
new	O
york	O
.	O
sklansky	O
,	O
j.	O
and	O
michelotti	O
(	O
1980	O
)	O
.	O
locally	O
trained	O
piecewise	O
linear	O
classifiers	O
.	O
ieee	O
trans	O
(	O
cid:173	O
)	O
actions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
2	O
:	O
101-111.	O
sklansky	O
,	O
j.	O
and	O
wassel	O
,	O
g.	O
(	O
1979	O
)	O
.	O
pattern	O
classifiers	O
and	O
trainable	O
machines	O
.	O
springer	O
(	O
cid:173	O
)	O
verlag	O
,	O
new	O
york	O
.	O
slud	O
,	O
e.	O
(	O
1977	O
)	O
.	O
distribution	B
inequalities	O
for	O
the	O
binomial	B
law	O
.	O
annals	O
of	O
probability	O
,	O
5	O
:	O
404-	O
412.	O
specht	O
,	O
d.	O
(	O
1967	O
)	O
.	O
generation	O
of	O
polynomial	O
discriminant	O
functions	O
for	O
pattern	O
classifica	O
(	O
cid:173	O
)	O
tion	O
.	O
ieee	O
transactions	O
on	O
electronic	O
computers	O
,	O
15:308-319.	O
specht	O
,	O
d.	O
(	O
1971	O
)	O
.	O
series	O
estimation	B
of	I
a	O
probability	O
density	O
function	O
.	O
technometrics	O
,	O
13:409-424.	O
specht	O
,	O
d.	O
(	O
1990	O
)	O
.	O
probabilistic	O
neural	O
networks	O
and	O
the	O
polynomial	B
adaline	O
as	O
complemen	O
(	O
cid:173	O
)	O
tary	O
techniques	O
for	O
classification	O
.	O
ieee	O
transactions	O
on	O
neural	O
networks	O
,	O
1	O
:	O
111-121.	O
spencer	O
,	O
j	O
.	O
(	O
1987	O
)	O
.	O
ten	O
lectures	O
on	O
the	O
probabilistic	B
method	I
.	O
siam	O
,	O
philadelphia	O
,	O
pa.	O
sprecher	O
,	O
d.	O
(	O
1965	O
)	O
.	O
on	O
the	O
structure	O
of	O
continuous	O
functions	O
of	O
several	O
variables	O
.	O
trans	O
(	O
cid:173	O
)	O
actions	O
of	O
the	O
american	O
mathematical	O
society	O
,	O
115:340-355.	O
steele	O
,	O
j	O
.	O
(	O
1975	O
)	O
.	O
combinatorial	O
entropy	B
and	O
uniform	B
limit	O
laws	O
.	O
phd	O
thesis	O
,	O
stanford	O
university	O
,	O
stanford	O
,	O
ca	O
.	O
steele	O
,	O
j	O
.	O
(	O
1986	O
)	O
.	O
an	O
efron-stein	O
inequality	B
for	O
nonsymmetric	O
statistics	B
.	O
annals	O
of	O
statistics	O
,	O
14:753-758.	O
stengle	O
,	O
g.	O
and	O
yukich	O
,	O
j	O
.	O
(	O
1989	O
)	O
.	O
some	O
new	O
vapnik-chervonenkis	O
classes	O
.	O
annals	O
of	O
statistics	O
,	O
17	O
:	O
1441-1446.	O
stoffel	O
,	O
j	O
.	O
(	O
1974	O
)	O
.	O
a	O
classifier	O
design	O
technique	O
for	O
discrete	O
variable	B
pattern	O
recognition	O
problems	O
.	O
ieee	O
transactions	O
on	O
computers	O
,	O
23:428-441.	O
stoller	O
,	O
d.	O
(	O
1954	O
)	O
.	O
univariate	O
two-population	O
distribution-free	O
discrimination	O
.	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
,	O
49:770-777.	O
stone	O
,	O
c.	O
(	O
1977	O
)	O
.	O
consistent	O
nonparametric	O
regression	O
.	O
annals	O
of	O
statistics	O
,	O
5:595-645.	O
stone	O
,	O
c.	O
(	O
1982	O
)	O
.	O
optimal	O
global	O
rates	O
of	O
convergence	O
for	O
nonparametric	O
regression	O
.	O
annals	O
of	O
statistics	O
,	O
10	O
:	O
1040-1053.	O
stone	O
,	O
c.	O
(	O
1985	O
)	O
.	O
additive	O
regression	O
and	O
other	O
nonparametric	O
models	O
.	O
annals	O
of	O
statistics	O
,	O
13:689-705.	O
stone	O
,	O
m.	O
(	O
1974	O
)	O
.	O
cross-validatory	O
choice	O
and	O
assessment	O
of	O
statistical	O
predictions	O
.	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
,	O
36	O
:	O
111-147.	O
stute	O
,	O
w.	O
(	O
1984	O
)	O
.	O
asymptotic	O
normality	O
of	O
nearest	O
neighbor	O
regression	O
function	O
estimates	O
.	O
annals	O
of	O
statistics	O
,	O
12:917-926.	O
sung	O
,	O
k.	O
and	O
shirley	O
,	O
p.	O
(	O
1992	O
)	O
.	O
ray	O
tracing	O
with	O
the	O
bsp	O
tree	B
.	O
in	O
graphics	O
gems	O
iii	O
,	O
kirk	O
,	O
d.	O
,	O
editor	O
,	O
pages	O
271-274.	O
academic	O
press	O
,	O
boston	O
,	O
ma	O
.	O
swonger	O
,	O
c.	O
(	O
1972	O
)	O
.	O
sample	O
set	O
condensation	O
for	O
a	O
condensed	O
nearest	B
neighbor	I
decision	O
rule	B
for	O
pattern	O
recognition	O
.	O
in	O
frontiers	O
of	O
pattern	O
recognition	O
,	O
watanabe	O
,	O
s.	O
,	O
editor	O
,	O
pages	O
511-519.	O
academic	O
press	O
,	O
new	O
york	O
.	O
szarek	O
,	O
s.	O
(	O
1976	O
)	O
.	O
on	O
the	O
best	O
constants	O
in	O
the	O
khintchine	O
inequality	B
.	O
studia	O
mathematica	O
,	O
63:197-208	O
.	O
616	O
references	O
szego	O
,	O
g.	O
(	O
1959	O
)	O
.	O
orthogonal	O
polynomials	O
,	O
volume	O
32.	O
american	O
mathematical	O
society	O
,	O
providence	O
,	O
ri	O
.	O
talagrand	O
,	O
m.	O
(	O
1987	O
)	O
.	O
the	O
glivenko-cantelli	O
problem	O
.	O
annals	O
of	O
probability	O
,	O
15:837-870.	O
talagrand	O
,	O
m.	O
(	O
1994	O
)	O
.	O
sharper	O
bounds	O
for	O
gaussian	B
and	O
empirical	B
processes	O
.	O
annals	O
of	O
probability	O
,	O
22:28-76.	O
talmon	O
,	O
j	O
.	O
(	O
1986	O
)	O
.	O
a	O
multiclass	O
nonparametric	O
partitioning	O
algorithm	O
.	O
in	O
pattern	O
recog	O
(	O
cid:173	O
)	O
nition	O
in	O
practice	O
ii	O
,	O
gelsema	O
,	O
e.	O
and	O
kanal	O
,	O
l.	O
,	O
editors	O
.	O
elsevier	O
science	O
publishers	O
,	O
amsterdam	O
.	O
taneja	O
,	O
1	O
.	O
(	O
1983	O
)	O
.	O
on	O
characterization	O
of	O
j-divergence	O
and	O
its	O
generalizations	O
.	O
journal	O
of	O
combinatorics	O
,	O
information	O
and	O
system	O
sciences	O
,	O
8:206-212.	O
taneja,1	O
.	O
(	O
1987	O
)	O
.	O
statistical	O
aspects	O
of	O
divergence	O
measures	O
.	O
journal	O
of	O
statistical	O
planning	O
and	O
inference	O
,	O
16	O
:	O
137-145.	O
tarter	O
,	O
m.	O
and	O
kronmal	O
,	O
r.	O
(	O
1970	O
)	O
.	O
on	O
multivariate	O
density	O
estimates	O
based	O
on	O
orthogonal	O
expansions	O
.	O
annals	O
of	O
mathematical	O
statistics	B
,	O
41:718-722.	O
tomek	O
,	O
1	O
.	O
(	O
1976a	O
)	O
.	O
a	O
generalization	O
of	O
the	O
k-nn	O
rule	B
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
and	O
cybernetics	O
,	O
6:121-126.	O
tomek	O
,	O
1	O
.	O
(	O
1976b	O
)	O
.	O
two	O
modifications	O
of	O
cnn	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
and	O
cybernetics	O
,	O
6:769-772.	O
toussaint	O
,	O
g.	O
(	O
1971	O
)	O
.	O
note	O
on	O
optimal	O
selection	B
of	O
independent	O
binary-valued	O
features	O
for	O
pattern	O
recognition	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
17:618.	O
toussaint	O
,	O
g.	O
(	O
1974a	O
)	O
.	O
bibliography	O
on	O
estimation	B
ofmisclassification	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
20:472-479.	O
toussaint	O
,	O
g.	O
(	O
1974b	O
)	O
.	O
on	O
the	O
divergence	B
between	O
two	O
distributions	O
and	O
the	O
probability	O
of	O
misclassification	O
of	O
several	O
decision	O
rules	O
.	O
in	O
proceedings	O
of	O
the	O
second	O
international	O
joint	O
conference	O
on	O
pattern	O
recognition	O
,	O
pages	O
27-34.	O
copenhagen	O
.	O
toussaint	O
,	O
g.	O
and	O
donaldson	O
,	O
r.	O
(	O
1970	O
)	O
.	O
algorithms	O
for	O
recognizing	O
contour-traced	O
hand	O
(	O
cid:173	O
)	O
printed	O
characters	O
.	O
ieee	O
transactions	O
on	O
computers	O
,	O
19:541-546.	O
tsypkin	O
,	O
y	O
.	O
(	O
1971	O
)	O
.	O
adaptation	O
and	O
learning	B
in	O
automatic	B
systems	O
.	O
academic	O
press	O
,	O
new	O
york	O
.	O
tutz	O
,	O
g.	O
(	O
1985	O
)	O
.	O
smoothed	B
additive	O
estimators	O
for	O
non-error	O
rates	O
in	O
multiple	O
discriminant	O
analysis	O
.	O
pattern	O
recognition	O
,	O
18:151-159.	O
tutz	O
,	O
g.	O
(	O
1986	O
)	O
.	O
an	O
alternative	O
choice	O
of	O
smoothing	O
for	O
kernel-based	O
density	O
estimates	O
in	O
discrete	O
discriminant	O
analysis	O
.	O
biometrika	O
,	O
73:405-411.	O
tutz	O
,	O
g.	O
(	O
1988	O
)	O
.	O
smoothing	O
for	O
discrete	O
kernels	O
in	O
discrimination	O
.	O
biometrics	O
journal	O
,	O
6:729-739.	O
tutz	O
,	O
g.	O
(	O
1989	O
)	O
.	O
on	O
cross-validation	B
for	O
discrete	O
kernel	B
estimates	O
in	O
discrimination	O
.	O
com	O
(	O
cid:173	O
)	O
munications	O
in	O
statistics-theory	O
and	O
methods	O
,	O
18:4145-4162.	O
ullmann	O
,	O
j	O
.	O
(	O
1974	O
)	O
.	O
automatic	B
selection	O
of	O
reference	O
data	O
for	O
use	O
in	O
a	O
nearest-neighbor	O
method	O
of	O
pattern	O
classification	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
20:541-	O
543.	O
vajda	O
,	O
i	O
.	O
(	O
1968	O
)	O
.	O
the	O
estimation	B
of	I
minimal	O
error	O
probability	O
for	O
testing	O
finite	O
or	O
countable	O
number	O
of	O
hypotheses	O
.	O
problemy	O
peredaci	O
informacii	O
,	O
4:6-14.	O
vajda	O
,	O
1	O
.	O
(	O
1989	O
)	O
.	O
theory	O
of	O
statistical	O
inference	O
and	O
information	O
.	O
kluwer	O
academic	O
pub	O
(	O
cid:173	O
)	O
lishers	O
,	O
dordrecht	O
.	O
valiant	O
,	O
l.	O
(	O
1984	O
)	O
.	O
a	O
theory	O
of	O
the	O
learnable	O
.	O
communications	O
of	O
the	O
acm	O
,	O
27:1134-1142.	O
van	O
campenhout	O
,	O
j	O
.	O
(	O
1980	O
)	O
.	O
the	O
arbitrary	O
relation	O
between	O
probability	O
of	O
error	O
and	O
mea	O
(	O
cid:173	O
)	O
surement	O
subset	O
.	O
journal	O
of	O
the	O
american	O
statistical	O
association	B
,	O
75	O
:	O
104-109.	O
references	O
617	O
van	O
ryzin	O
,	O
j	O
.	O
(	O
1966	O
)	O
.	O
bayes	O
risk	O
consistency	B
of	O
classification	O
procedures	O
using	O
density	B
estimation	I
.	O
sankhya	O
series	O
a	O
,	O
28:161-170.	O
vapnik	O
,	O
v.	O
(	O
1982	O
)	O
.	O
estimation	B
of	I
dependencies	O
based	O
on	O
empirical	O
data	O
.	O
springer-verlag	O
,	O
new	O
york	O
.	O
vapnik	O
,	O
v.	O
and	O
chervonenkis	O
,	O
a	O
.	O
(	O
1971	O
)	O
.	O
on	O
the	O
uniform	B
convergence	O
of	O
relative	O
frequencies	O
of	O
events	O
to	O
their	O
probabilities	O
.	O
theory	O
of	O
probability	O
and	O
its	O
applications	O
,	O
16:264-280.	O
vapnik	O
,	O
v.	O
and	O
chervonenkis	O
,	O
a	O
.	O
(	O
1974a	O
)	O
.	O
ordered	B
risk	O
minimization	O
.	O
i.	O
automation	O
and	O
remote	O
control	O
,	O
35:1226-1235.	O
vapnik	O
,	O
v.	O
and	O
chervonenkis	O
,	O
a	O
.	O
(	O
197	O
4b	O
)	O
.	O
ordered	B
risk	O
minimization	O
.	O
ii	O
.	O
automation	O
and	O
remote	O
control	O
,	O
35	O
:	O
1403-1412.	O
vapnik	O
,	O
v.	O
and	O
chervonenkis	O
,	O
a	O
.	O
(	O
197	O
4c	O
)	O
.	O
theory	O
of	O
pattern	O
recognition	O
.	O
nauka	O
,	O
moscow	O
.	O
(	O
in	O
russian	O
)	O
;	O
german	O
translation	O
:	O
theorie	O
der	O
zeichenerkennung	O
,	O
akademie	O
verlag	O
,	O
berlin	O
,	O
1979.	O
vapnik	O
,	O
v.	O
and	O
chervonenkis	O
,	O
a	O
.	O
(	O
1981	O
)	O
.	O
necessary	O
and	O
sufficient	O
conditions	O
for	O
the	O
uniform	B
convergence	O
of	O
means	O
to	O
their	O
expectations	O
.	O
theory	O
of	O
probability	O
and	O
its	O
applications	O
,	O
26:821-832.	O
vidal	O
,	O
e.	O
(	O
1986	O
)	O
.	O
an	O
algorithm	B
for	O
finding	O
nearest	O
neighbors	O
in	O
(	O
approximately	O
)	O
constant	O
average	O
time	O
.	O
pattern	O
recognition	O
letters	O
,	O
4	O
:	O
145-157.	O
vilmansen	O
,	O
t.	O
(	O
1973	O
)	O
.	O
feature	O
evaluation	O
with	O
measures	O
of	O
probabilistic	O
dependence	O
.	O
ieee	O
transactions	O
on	O
computers	O
,	O
22:381-388.	O
vitushkin	O
,	O
a	O
.	O
(	O
1961	O
)	O
.	O
the	O
absolute	O
e-entropy	O
of	O
metric	O
spaces	O
.	O
translations	O
of	O
the	O
american	O
mathematical	O
society	O
,	O
17:365-367.	O
wagner	O
,	O
t.	O
(	O
1971	O
)	O
.	O
convergence	O
of	O
the	O
nearest	B
neighbor	I
rule	I
.	O
ieee	O
transactions	O
on	O
infor	O
(	O
cid:173	O
)	O
mation	O
theory	O
,	O
17:566-571.	O
wagner	O
,	O
t.	O
(	O
1973	O
)	O
.	O
convergence	O
of	O
the	O
edited	B
nearest	O
neighbor	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
19:696-699.	O
wang	O
,	O
q.	O
and	O
suen	O
,	O
c.	O
(	O
1984	O
)	O
.	O
analysis	O
and	O
design	O
of	O
decision	O
tree	B
based	O
on	O
entropy	B
reduction	O
and	O
its	O
application	O
to	O
large	O
character	O
set	O
recognition	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
6:406-417.	O
warner	O
,	O
h.	O
,	O
toronto	O
,	O
a.	O
,	O
veasey	O
,	O
l.	O
,	O
and	O
stephenson	O
,	O
r.	O
(	O
1961	O
)	O
.	O
a	O
mathematical	O
approach	O
to	O
medical	O
diagnosis	O
.	O
journal	O
of	O
jhe	O
american	O
medical	O
association	B
,	O
177	O
:	O
177-183.	O
wassel	O
,	O
g.	O
and	O
sklansky	O
,	O
j	O
.	O
(	O
1972	O
)	O
.	O
training	O
a	O
one-dimensional	O
classifier	B
to	O
minimize	O
the	O
probability	O
of	O
error	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
2:533-541.	O
watson	O
,	O
g.	O
(	O
1964	O
)	O
.	O
smooth	O
regression	O
analysis	O
.	O
sankhya	O
series	O
a	O
,	O
26:359-372.	O
weiss	O
,	O
s.	O
and	O
kulikowski	O
,	O
c.	O
(	O
1991	O
)	O
.	O
computer	O
systems	O
that	O
learn	O
.	O
morgan	O
kaufmann	O
,	O
san	O
mateo	O
,	O
ca	O
.	O
wenocur	O
,	O
r.	O
and	O
dudley	O
,	O
r.	O
(	O
1981	O
)	O
.	O
some	O
special	O
vapnik-chervonenkis	O
classes	O
.	O
discrete	O
mathematics	O
,	O
33:313-318.	O
wheeden	O
,	O
r.	O
and	O
zygmund	O
,	O
a	O
.	O
(	O
1977	O
)	O
.	O
measure	B
and	O
integral	O
.	O
marcel	O
dekker	O
,	O
new	O
york	O
.	O
white	O
,	O
h.	O
(	O
1990	O
)	O
.	O
connectionist	O
nonparametric	O
regression	O
:	O
multilayer	B
feedforward	O
net	O
(	O
cid:173	O
)	O
works	O
can	O
learn	O
arbitrary	O
mappings	O
.	O
neural	O
networks	O
,	O
3:535-549.	O
white	O
,	O
h.	O
(	O
1991	O
)	O
.	O
nonparametric	O
estimation	B
of	I
conditional	O
quantiles	O
using	O
neural	O
networks	O
.	O
in	O
proceedings	O
of	O
the	O
23rd	O
symposium	O
of	O
the	O
interface	O
:	O
computing	O
science	O
and	O
statis	O
(	O
cid:173	O
)	O
tics	O
,	O
pages	O
190-199.	O
american	O
statistical	O
association	B
,	O
alexandria	O
,	O
va.	O
widrow	O
,	O
b	O
.	O
(	O
1959	O
)	O
.	O
adaptive	O
sampled-data	O
systems-a	O
statistical	O
theory	O
of	O
adaptation	O
.	O
in	O
ire	O
wescon	O
convention	O
record	B
,	O
volume	O
part	O
4	O
,	O
pages	O
74-85	O
.	O
618	O
references	O
widrow	O
,	O
b.	O
and	O
hoff	O
,	O
m.	O
(	O
1960	O
)	O
.	O
adaptive	O
switching	O
circuits	O
.	O
in	O
ire	O
wescon	O
convention	O
record	B
,	O
volume	O
part	O
4	O
,	O
pages	O
96-104.	O
reprinted	O
in	O
j	O
.a	O
.	O
anderson	O
and	O
e.	O
rosenfeld	O
,	O
neurocomputing	O
:	O
foundations	O
of	O
research	O
,	O
mit	O
press	O
,	O
cambridge	O
,	O
ma.	O
,	O
1988.	O
wilson	O
,	O
d.	O
(	O
1972	O
)	O
.	O
asymptotic	O
properties	O
of	B
nearest	I
neighbor	I
rules	I
using	O
edited	B
data	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
and	O
cybernetics	O
,	O
2:408-42l	O
.	O
winder	O
,	O
r.	O
(	O
1963	O
)	O
.	O
threshold	B
logic	O
in	O
artificial	O
intelligence	O
.	O
in	O
artificial	O
intelligence	O
,	O
pages	O
107-128.	O
ieee	O
special	O
publication	O
s-142	O
.	O
wolverton	O
,	O
c.	O
and	O
wagner	O
,	O
t.	O
(	O
1969a	O
)	O
.	O
asymptotically	O
optimal	O
discriminant	O
functions	O
for	O
pattern	O
classification	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
science	O
and	O
cybernetics	O
,	O
15:258-	O
265.	O
wolverton	O
,	O
c.	O
and	O
wagner	O
,	O
t.	O
(	O
1969b	O
)	O
.	O
recursive	B
estimates	O
of	O
probability	O
densities	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
science	O
and	O
cybernetics	O
,	O
5:307.	O
wong	O
,	O
w.	O
and	O
shen	O
,	O
x	O
.	O
(	O
1992	O
)	O
.	O
probability	O
inequalities	O
for	O
likelihood	O
ratios	O
and	O
convergence	O
rates	O
of	O
sieve	O
mle	O
's	O
.	O
technical	O
report	O
346	O
,	O
department	O
of	O
statistics	O
,	O
university	O
of	O
chicago	O
,	O
chicago	O
,	O
il	O
.	O
xu	O
,	O
l.	O
,	O
krzyzak	O
,	O
a.	O
,	O
and	O
oja	O
,	O
e.	O
(	O
1993	O
)	O
.	O
rival	O
penalized	O
competitive	O
learning	B
for	O
clustering	B
analysis	O
,	O
rbf	O
net	O
and	O
curve	O
detection	O
.	O
ieee	O
transactions	O
on	O
neural	O
networks	O
,	O
4:636-	O
649.	O
xu	O
,	O
l.	O
,	O
krzyzak	O
,	O
a.	O
,	O
and	O
yuille	O
,	O
a	O
.	O
(	O
1994	O
)	O
.	O
on	O
radial	B
basis	I
function	I
nets	O
and	O
kernel	B
regres	O
(	O
cid:173	O
)	O
sion	O
:	O
approximation	O
ability	O
,	O
convergence	O
rate	O
and	O
receptive	O
field	O
size	O
.	O
neural	O
networks	O
.	O
to	O
appear	O
.	O
yatracos	O
,	O
y	O
.	O
(	O
1985	O
)	O
.	O
rates	O
of	O
convergence	O
of	O
minimum	O
distance	B
estimators	O
and	O
kol	O
(	O
cid:173	O
)	O
mogorov	O
's	O
entropy	B
.	O
annals	O
of	O
statistics	O
,	O
13:768-774.	O
yau	O
,	O
s.	O
and	O
lin	O
,	O
t.	O
(	O
1968	O
)	O
.	O
on	O
the	O
upper	O
bound	O
of	O
the	O
probability	O
of	O
error	O
of	O
a	O
linear	O
pattern	O
classifier	B
for	O
probabilistic	O
pattern	O
classes	O
.	O
proceedings	O
of	O
the	O
ieee	O
,	O
56:321-322.	O
you	O
,	O
k.	O
and	O
fu	O
,	O
k.	O
(	O
1976	O
)	O
.	O
an	O
approach	O
to	O
the	O
design	O
of	O
a	O
linear	O
binary	B
tree	O
classifier	B
.	O
in	O
proceedings	O
of	O
the	O
symposium	O
of	O
machine	O
processing	O
of	O
remotely	O
sensed	O
data	O
,	O
pages	O
3a-1o	O
.	O
purdue	O
university	O
,	O
lafayette	O
,	O
in	O
.	O
yukich	O
,	O
j	O
.	O
(	O
1985	O
)	O
.	O
laws	O
of	O
large	O
numbers	O
for	O
classes	O
of	O
functions	O
.	O
journal	O
of	O
multivariate	O
analysis	O
,	O
17:245-260.	O
yunck	O
,	O
t.	O
(	O
1976	O
)	O
.	O
a	O
technique	O
to	O
identify	O
nearest	O
neighbors	O
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
6:678-683.	O
zhao	O
,	O
l.	O
(	O
1987	O
)	O
.	O
exponential	B
bounds	O
of	O
mean	O
error	O
for	O
the	O
nearest	B
neighbor	I
estimates	O
of	O
regression	O
functions	O
.	O
journal	O
of	O
multivariate	O
analysis	O
,	O
21	O
:	O
168-178.	O
zhao	O
,	O
l.	O
(	O
1989	O
)	O
.	O
exponential	B
bounds	O
of	O
mean	O
error	O
for	O
the	O
kernel	B
estimates	O
of	O
regression	O
functions	O
.	O
journal	O
of	O
multivariate	O
analysis	O
,	O
29:260-273.	O
zhao	O
,	O
l.	O
,	O
krishnaiah	O
,	O
p.	O
,	O
and	O
chen	O
,	O
x	O
.	O
(	O
1990	O
)	O
.	O
almost	O
sure	O
lr-norm	O
convergence	O
for	O
data	O
(	O
cid:173	O
)	O
based	O
histogram	O
estimates	O
.	O
theory	O
of	O
probability	O
and	O
its	O
applications	O
,	O
35:396-403.	O
zygmund	O
,	O
a	O
.	O
(	O
1959	O
)	O
.	O
trigonometric	B
series	O
i.	O
university	O
press	O
,	O
cambridge	O
.	O
pattern	O
recognition	O
presents	O
one	O
of	O
the	O
most	O
significant	O
challenges	O
for	O
scientists	O
and	O
engineers	O
,	O
and	O
many	O
different	O
approaches	O
have	O
been	O
proposed	O
.	O
the	O
aim	O
of	O
this	O
book	O
is	O
to	O
provide	O
a	O
self-contained	O
account	O
of	O
probabilistic	O
analysis	O
of	O
these	O
approaches	O
.	O
the	O
book	O
includes	O
a	O
discussion	O
of	O
distance	O
measures	O
,	O
nonparametric	O
methods	O
based	O
on	O
kernels	O
or	O
nearest	O
neighbors	O
,	O
vapnik-chervonenkis	O
theo	O
(	O
cid:173	O
)	O
ry	O
,	O
epsilon	O
entropy	B
,	O
parametric	B
classification	I
,	O
error	B
estimation	I
,	O
tree	B
classifiers	O
,	O
and	O
neural	O
networks	O
.	O
wherever	O
possible	O
,	O
distribution-free	O
properties	O
and	O
inequalities	O
are	O
derived	O
.	O
a	O
substantial	O
portion	O
of	O
the	O
results	O
or	O
the	O
analysis	O
is	O
new	O
.	O
over	O
430	O
problems	O
and	O
exercises	O
complement	O
the	O
material	O
.	O
isbn	O
0-387-94618-7	O
isb	O
!	O
\	O
,	O
)	O
0-387-94618-7	O